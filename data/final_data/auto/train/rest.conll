Maoqin	O
@	O
DravidianLangTech	O
-	O
EACL2021	O
:	O
The	O
Application	O
of	O
Transformer	B-MethodName
-	O
Based	O
Model	O

An	O
overall	O
framework	O
and	O
processing	O
pipeline	O
of	O
my	O
solution	O
are	O
shown	O
in	O
Figure	O
1	O
.	O
In	O
my	O
job	O
,	O
I	O
use	O
the	O
ALBERT	B-MethodName
model	O
as	O
my	O
base	O
model	O
and	O
take	O
BiGRU	B-MethodName
-	O
Attention	O
behind	O
it	O
.	O
My	O
model	O
is	O
shown	O
in	O
Figure	O
2	O
.	O

The	O
BiGRU	B-MethodName
-	O
Attention	O
model	O
(	O
Cover	O
and	O
Hart	O
,	O
1967	O
)	O
is	O
divided	O
into	O
three	O
parts	O
:	O
text	O
vector	O
input	O
layer	O
,	O
hidden	O
layer	O
,	O
and	O
output	O
layer	O
.	O
Among	O
them	O
,	O
the	O
hidden	O
layer	O
consists	O
of	O
three	O
layers	O
:	O
the	O
BiGRU	B-MethodName
layer	O
,	O
the	O
attention	O
layer	O
,	O
and	O
the	O
Dense	O
layer	O
(	O
fully	O
connected	O
layer	O
)	O
.	O
I	O
set	O
the	O
output	O
of	O
the	O
ALBERT	B-MethodName
model	O
as	O
the	O
input	O
.	O
After	O
receiving	O
the	O
input	O
,	O
it	O
uses	O
the	O
BiGRU	B-MethodName
neural	O
network	O
layer	O
to	O
extract	O
features	O
of	O
the	O
deep	O
-	O
level	O
information	O
of	O
the	O
text	O
firstly	O
.	O
Secondly	O
,	O
it	O
uses	O
the	O
attention	O
layer	O
to	O
assign	O
corresponding	O
weights	O
to	O
the	O
deep	O
-	O
level	O
information	O
of	O
the	O
extracted	O
text	O
.	O
Finally	O
,	O
the	O
text	O
feature	O
information	O
with	O
different	O
weights	O
is	O
put	O
into	O
the	O
softmax	B-MethodName
function	O
layer	O
for	O
classification	O
.	O
The	O
structure	O
of	O
the	O
BiGRU	B-MethodName
-	O
Attention	O
model	O
is	O
shown	O
in	O
Figure	O
3	O
.	O

In	O
this	O
paper	O
,	O
I	O
present	O
my	O
result	O
on	O
Offensive	O
Language	B-TaskName
Identification	I-TaskName
in	O
Dravidian	O
Languages	O
-	O
EACL	O
2021	O
which	O
includes	O
three	O
tasks	O
of	O
different	O
languages	O
.	O
For	O
this	O
task	O
,	O
I	O
regard	O
it	O
as	O
a	O
multiple	O
classification	O
task	O
,	O
I	O
use	O
the	O
BiGRU	B-MethodName
-	O
Attention	O
based	O
on	O
the	O
ALBERT	B-MethodName
model	O
to	O
complete	O
,	O
and	O
my	O
model	O
works	O
very	O
well	O
.	O
I	O
also	O
summarized	O
the	O
possible	O
reasons	O
for	O
classifying	O
only	O
three	O
types	O
of	O
labels	O
.	O
At	O
the	O
same	O
time	O
,	O
I	O
also	O
use	O
some	O
other	O
neural	O
networks	O
for	O
comparative	O
experiments	O
to	O
prove	O
that	O
my	O
model	O
can	O
obtain	O
excellent	O
performance	O
.	O
The	O
result	O
shows	O
that	O
my	O
model	O
ranks	O
5th	O
in	O
the	O
Malayalam	O
task	O
.	O
Due	O
to	O
the	O
continuous	O
development	O
of	O
the	O
definition	O
of	O
offensive	O
information	O
on	O
the	O
Internet	O
,	O
it	O
is	O
difficult	O
to	O
accurately	O
describe	O
the	O
nature	O
of	O
this	O
information	O
only	O
from	O
the	O
perspective	O
of	O
data	O
mining	O
,	O
which	O
makes	O
it	O
impossible	O
to	O
model	O
this	O
information	O
effectively	O
.	O
In	O
the	O
future	O
,	O
I	O
will	O
use	O
methods	O
based	O
on	O
multidisciplinary	O
discovery	O
to	O
guide	O
model	O
learning	O
.	O
These	O
models	O
are	O
more	O
likely	O
to	O
use	O
limited	O
data	O
to	O
learn	O
more	O
effective	O
models	O
.	O
At	O
the	O
same	O
time	O
,	O
I	O
will	O
also	O
consider	O
whether	O
I	O
can	O
use	O
other	O
transfer	B-TaskName
learning	I-TaskName
models	O
to	O
perform	O
better	O
on	O
multi	O
-	O
classification	O
tasks	O
.	O

Let	O
x	O
=	O
[	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
T	O
]	O
be	O
our	O
input	O
text	O
and	O
y	O
=	O
[	O
y	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
T	O
]	O
be	O
per	O
-	O
token	O
output	O
tags	O
.	O
Let	O
D	O
be	O
the	O
domain	O
size	O
of	O
each	O
y	O
i	O
.	O
We	O
predict	O
the	O
most	O
likely	O
y	O
,	O
given	O
a	O
conditional	O
model	O
P	O
(	O
y	O
|	O
x	O
)	O
.	O
This	O
paper	O
considers	O
two	O
factorizations	O
of	O
the	O
conditional	O
distribution	O
.	O
First	O
,	O
we	O
have	O
P	O
(	O
y	O
|	O
x	O
)	O
=	O
T	O
t=1	O
P	O
(	O
y	O
t	O
|	O
F	O
(	O
x	O
)	O
)	O
,	O
(	O
1	O
)	O
where	O
the	O
tags	O
are	O
conditionally	O
independent	O
given	O
some	O
features	O
for	O
x.	O
Given	O
these	O
features	O
,	O
O	O
(	O
D	O
)	O
prediction	O
is	O
simple	O
and	O
parallelizable	O
across	O
the	O
length	O
of	O
the	O
sequence	O
.	O
However	O
,	O
feature	O
extraction	O
may	O
not	O
necessarily	O
be	O
parallelizable	O
.	O
For	O
example	O
,	O
RNN	O
-	O
based	O
features	O
require	O
iterative	O
passes	O
along	O
the	O
length	O
of	O
x.	O
We	O
also	O
consider	O
a	O
linear	O
-	O
chain	O
CRF	B-MethodName
model	O
that	O
couples	O
all	O
of	O
y	O
together	O
:	O
P	O
(	O
y	O
|	O
x	O
)	O
=	O
1	O
Z	O
x	O
T	O
t=1	O
ψ	O
t	O
(	O
y	O
t	O
|	O
F	O
(	O
x	O
)	O
)	O
ψ	O
p	O
(	O
y	O
t	O
,	O
y	O
t−1	O
)	O
,	O
(	O
2	O
)	O
where	O
ψ	O
t	O
is	O
a	O
local	O
factor	O
,	O
ψ	O
p	O
is	O
a	O
pairwise	O
factor	O
that	O
scores	O
consecutive	O
tags	O
,	O
and	O
Z	O
x	O
is	O
the	O
partition	O
function	O
(	O
Lafferty	O
et	O
al	O
,	O
2001	O
)	O
.	O
To	O
avoid	O
overfitting	O
,	O
ψ	O
p	O
does	O
not	O
depend	O
on	O
the	O
timestep	O
t	O
or	O
the	O
input	O
x	O
in	O
our	O
experiments	O
.	O
Prediction	O
in	O
this	O
model	O
requires	O
global	O
search	O
using	O
the	O
O	O
(	O
D	O
2	O
T	O
)	O
Viterbi	O
algorithm	O
.	O
CRF	B-MethodName
prediction	O
explicitly	O
reasons	O
about	O
interactions	O
among	O
neighboring	O
output	O
tags	O
,	O
whereas	O
prediction	O
in	O
the	O
first	O
model	O
compiles	O
this	O
reasoning	O
into	O
the	O
feature	O
extraction	O
step	O
(	O
Liang	O
et	O
al	O
,	O
2008	O
)	O
.	O
The	O
suitability	O
of	O
such	O
compilation	O
depends	O
on	O
the	O
properties	O
and	O
quantity	O
of	O
the	O
data	O
.	O
While	O
CRF	B-MethodName
prediction	O
requires	O
non	O
-	O
trivial	O
search	O
in	O
output	O
space	O
,	O
it	O
can	O
guarantee	O
that	O
certain	O
output	O
constraints	O
,	O
such	O
as	O
for	O
IOB	O
tagging	O
(	O
Ramshaw	O
and	O
Marcus	O
,	O
1999	O
)	O
,	O
will	O
always	O
be	O
satisfied	O
.	O
It	O
may	O
also	O
have	O
better	O
sample	O
complexity	O
,	O
as	O
it	O
imposes	O
more	O
prior	O
knowledge	O
about	O
the	O
structure	O
of	O
the	O
interactions	O
among	O
the	O
tags	O
(	O
London	O
et	O
al	O
,	O
2016	O
)	O
.	O
However	O
,	O
it	O
has	O
worse	O
computational	O
complexity	O
than	O
independent	O
prediction	O
.	O

We	O
thank	O
Subhransu	O
Maji	O
and	O
Luke	O
Vilnis	O
for	O
helpful	O
discussions	O
,	O
and	O
Brendan	O
O'Connor	O
,	O
Yoav	O
Goldberg	O
,	O
the	O
UMass	O
NLP	O
reading	O
group	O
and	O
many	O
anonymous	O
reviewers	O
for	O
constructive	O
comments	O
on	O
various	O
drafts	O
of	O
the	O
paper	O
.	O
We	O
are	O
also	O
grateful	O
to	O
Guillaume	O
Lample	O
for	O
sharing	O
his	O
pretrained	O
word	B-TaskName
embeddings	I-TaskName
.	O
This	O
work	O
was	O
supported	O
in	O
part	O
by	O
the	O
Center	O
for	O
Intelligent	O
Information	B-TaskName
Retrieval	I-TaskName
,	O
in	O
part	O
by	O
DARPA	B-DatasetName
under	O
agreement	O
number	O
FA8750	O
-	O
13	O
-	O
2	O
-	O
0020	O
,	O
in	O
part	O
by	O
Defense	O
Advanced	O
Research	O
Agency	O
(	O
DARPA	B-DatasetName
)	O
contract	O
number	O
HR0011	O
-	O
15	O
-	O
2	O
-	O
0036	O
,	O
in	O
part	O
by	O
the	O
National	O
Science	O
Foundation	O
(	O
NSF	O
)	O
grant	O
number	O
DMR	O
-	O
1534431	O
,	O
and	O
in	O
part	O
by	O
the	O
National	O
Science	O
Foundation	O
(	O
NSF	O
)	O
grant	O
number	O
IIS	O
-	O
1514053	O
.	O
The	O
U.S.	O
Government	O
is	O
authorized	O
to	O
reproduce	O
and	O
distribute	O
reprints	O
for	O
Governmental	O
purposes	O
notwithstanding	O
any	O
copyright	O
notation	O
thereon	O
.	O
Any	O
opinions	O
,	O
findings	O
and	O
conclusions	O
or	O
recommendations	O
expressed	O
in	O
this	O
material	O
are	O
those	O
of	O
the	O
authors	O
and	O
do	O
not	O
necessarily	O
reflect	O
those	O
of	O
the	O
sponsor	O
.	O

Validating	O
Label	O
Consistency	O
in	O
NER	B-TaskName
Data	O
Annotation	O

Data	O
annotation	O
plays	O
a	O
crucial	O
role	O
in	O
ensuring	O
your	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
projects	O
are	O
trained	O
with	O
the	O
correct	O
information	O
to	O
learn	O
from	O
.	O
Producing	O
the	O
most	O
accurate	O
labels	O
is	O
a	O
challenge	O
due	O
to	O
the	O
complexity	O
involved	O
with	O
annotation	O
.	O
Label	O
inconsistency	O
between	O
multiple	O
subsets	O
of	O
data	O
annotation	O
(	O
e.g.	O
,	O
training	O
set	O
and	O
test	O
set	O
,	O
or	O
multiple	O
training	O
subsets	O
)	O
is	O
an	O
indicator	O
of	O
label	O
mistakes	O
.	O
In	O
this	O
work	O
,	O
we	O
present	O
an	O
empirical	O
method	O
to	O
explore	O
the	O
relationship	O
between	O
label	O
(	O
in	O
-	O
)	O
consistency	O
and	O
NER	B-TaskName
model	O
performance	O
.	O
It	O
can	O
be	O
used	O
to	O
validate	O
the	O
label	O
consistency	O
(	O
or	O
catch	O
the	O
inconsistency	O
)	O
in	O
multiple	O
sets	O
of	O
NER	B-TaskName
data	O
annotation	O
.	O
In	O
experiments	O
,	O
our	O
method	O
identified	O
the	O
label	O
inconsistency	O
of	O
test	O
data	O
in	O
SCIERC	B-DatasetName
and	O
CoNLL03	B-DatasetName
datasets	O
(	O
with	O
26.7	O
%	O
and	O
5.4	O
%	O
label	O
mistakes	O
)	O
.	O
It	O
validated	O
the	O
consistency	O
in	O
the	O
corrected	O
version	O
of	O
both	O
datasets	O
.	O

problem	O
]	O
Task	O
,	O
we	O
present	O
a	O
novel	O
technique	O
...	O
FERRET	O
utilizes	O
a	O
novel	O
approach	O
to	O
[	O
Q	O
/	O
A	O
]	O
Method	O
known	O
as	O
predictive	O
questioning	O
which	O
attempts	O
to	O
identify	O
...	O
FERRET	O
utilizes	O
a	O
novel	O
approach	O
to	O
[	O
Q	O
/	O
A	O
]	O
Task	O
known	O
as	O
predictive	O
questioning	O
which	O
attempts	O
to	O
identify	O
...	O
The	O
goal	O
of	O
this	O
work	O
is	O
the	O
enrichment	O
of	O
[	O
human	O
-	O
machine	O
interactions	O
]	O
Task	O
in	O
a	O
natural	O
language	O
environment	O
.	O
The	O
goal	O
of	O
this	O
work	O
is	O
the	O
[	O
enrichment	O
of	O
human	O
-	O
machine	O
interactions	O
]	O
Task	O
in	O
a	O
natural	O
language	O
environment	O
.	O
We	O
sample	O
three	O
exclusive	O
subsets	O
(	O
of	O
size	O
x	O
)	O
from	O
the	O
training	O
set	O
(	O
orange	O
,	O
green	O
,	O
and	O
blue	O
)	O
.	O
We	O
use	O
one	O
subset	O
as	O
the	O
new	O
test	O
set	O
(	O
orange	O
)	O
.	O
We	O
apply	O
the	O
SCIIE	O
NER	B-TaskName
model	O
on	O
the	O
new	O
test	O
set	O
.	O
We	O
build	O
three	O
new	O
training	O
sets	O
:	O
i	O
)	O
"	O
TrainTest	O
"	O
(	O
blue	O
-	O
red	O
)	O
,	O
ii	O
)	O
"	O
PureTrain	O
"	O
(	O
green	O
-	O
blue	O
)	O
,	O
iii	O
)	O
"	O
TestTrain	O
"	O
(	O
red	O
-	O
blue	O
)	O
.	O
Results	O
on	O
SCIERC	B-DatasetName
show	O
that	O
the	O
test	O
set	O
(	O
red	O
)	O
is	O
less	O
predictive	O
of	O
training	O
samples	O
(	O
orange	O
)	O
than	O
the	O
training	O
set	O
itself	O
(	O
blue	O
or	O
green	O
)	O
.	O
This	O
was	O
not	O
observed	O
on	O
two	O
other	O
datasets	O
.	O
Besides	O
the	O
significant	O
correction	O
on	O
the	O
SCI	O
-	O
ERC	O
dataset	O
,	O
our	O
contributions	O
in	O
this	O
work	O
are	O
as	O
follows	O
:	O
i	O
)	O
an	O
empirical	O
,	O
visual	O
method	O
to	O
identify	O
the	O
label	O
inconsistency	O
between	O
subsets	O
of	O
annotated	O
data	O
(	O
see	O
Figure	O
1	O
)	O
,	O
ii	O
)	O
a	O
method	O
to	O
validate	O
the	O
label	O
consistency	O
of	O
corrected	O
data	O
annotation	O
(	O
see	O
Figure	O
2	O
)	O
.	O
Experiments	O
show	O
that	O
they	O
are	O
effective	O
on	O
the	O
CoNLL03	B-DatasetName
and	O
SCIERC	B-DatasetName
datasets	O
.	O

Suppose	O
the	O
labeling	O
processes	O
on	O
two	O
parts	O
of	O
annotated	O
data	O
were	O
consistent	O
.	O
They	O
are	O
likely	O
to	O
be	O
equivalently	O
predictive	O
of	O
each	O
other	O
.	O
In	O
other	O
words	O
,	O
if	O
we	O
train	O
a	O
model	O
with	O
a	O
set	O
of	O
samples	O
from	O
either	O
part	O
A	O
or	O
part	O
B	O
to	O
predict	O
a	O
different	O
set	O
from	O
part	O
A	O
,	O
the	O
performance	O
should	O
be	O
similar	O
.	O
Take	O
SCIERC	B-DatasetName
as	O
an	O
example	O
.	O
We	O
were	O
wondering	O
whether	O
the	O
labels	O
in	O
the	O
test	O
set	O
were	O
consistent	O
with	O
those	O
in	O
the	O
training	O
set	O
.	O
Our	O
method	O
to	O
identify	O
the	O
inconsistency	O
is	O
presented	O
in	O
Figure	O
1	O
.	O
We	O
sample	O
three	O
exclusive	O
subsets	O
(	O
of	O
size	O
x	O
)	O
from	O
the	O
training	O
set	O
.	O
We	O
set	O
x	O
=	O
550	O
according	O
to	O
the	O
size	O
of	O
the	O
original	O
test	O
set	O
.	O
We	O
use	O
one	O
of	O
the	O
subsets	O
as	O
the	O
new	O
test	O
set	O
.	O
Then	O
we	O
train	O
the	O
SCIIE	O
NER	B-TaskName
model	O
(	O
Luan	O
et	O
al	O
,	O
2018	O
)	O
to	O
perform	O
on	O
the	O
new	O
test	O
set	O
.	O
We	O
build	O
three	O
new	O
training	O
sets	O
to	O
feed	O
into	O
the	O
model	O
:	O
"	O
TrainTest	O
"	O
:	O
first	O
fed	O
with	O
one	O
training	O
subset	O
and	O
then	O
the	O
original	O
test	O
set	O
;	O
"	O
PureTrain	O
"	O
:	O
fed	O
with	O
two	O
training	O
subsets	O
;	O
"	O
TestTrain	O
"	O
:	O
first	O
fed	O
with	O
the	O
original	O
test	O
set	O
and	O
then	O
one	O
of	O
the	O
training	O
subsets	O
.	O
Results	O
show	O
that	O
"	O
TestTrain	O
"	O
performed	O
the	O
worst	O
at	O
the	O
early	O
stage	O
because	O
the	O
quality	O
of	O
the	O
original	O
test	O
set	O
is	O
not	O
reliable	O
.	O
In	O
"	O
TrainTest	O
"	O
the	O
performance	O
no	O
longer	O
improved	O
when	O
the	O
model	O
started	O
being	O
fed	O
with	O
the	O
original	O
test	O
set	O
.	O
"	O
Pure	O
-	O
Train	O
"	O
performed	O
the	O
best	O
.	O
All	O
the	O
observations	O
conclude	O
that	O
the	O
original	O
test	O
set	O
is	O
less	O
predictive	O
of	O
training	O
samples	O
than	O
the	O
training	O
set	O
itself	O
.	O
It	O
may	O
be	O
due	O
to	O
the	O
issue	O
of	O
label	O
inconsistency	O
.	O
Moreover	O
,	O
we	O
do	O
not	O
have	O
such	O
observations	O
on	O
two	O
other	O
datasets	O
,	O
WikiGold	O
and	O
WNUT16	O
.	O

After	O
we	O
corrected	O
the	O
label	O
mistakes	O
,	O
how	O
could	O
we	O
empirically	O
validate	O
the	O
recovery	O
of	O
label	O
consistency	O
?	O
Again	O
,	O
we	O
use	O
a	O
subset	O
of	O
training	O
data	O
as	O
the	O
new	O
test	O
set	O
.	O
We	O
evaluate	O
the	O
predictability	O
of	O
the	O
original	O
wrong	O
test	O
subset	O
,	O
the	O
corrected	O
test	O
subset	O
,	O
and	O
the	O
rest	O
of	O
the	O
training	O
set	O
.	O
We	O
expect	O
to	O
see	O
that	O
the	O
wrong	O
test	O
subset	O
delivers	O
weaker	O
performance	O
and	O
the	O
other	O
two	O
sets	O
make	O
comparable	O
good	O
predictions	O
.	O
Figure	O
2	O
illustrates	O
this	O
idea	O
.	O
Take	O
SCIERC	B-DatasetName
as	O
an	O
example	O
.	O
Suppose	O
we	O
corrected	O
z	O
of	O
y	O
+	O
z	O
sentences	O
in	O
the	O
test	O
set	O
.	O
The	O
original	O
wrong	O
test	O
subset	O
(	O
"	O
Mistake	O
"	O
)	O
and	O
the	O
corrected	O
test	O
subset	O
(	O
"	O
Correct	O
"	O
)	O
are	O
both	O
of	O
size	O
z.	O
Here	O
z	O
=	O
147	O
and	O
the	O
original	O
good	O
test	O
subset	O
y	O
=	O
404	O
(	O
"	O
Test	O
"	O
)	O
.	O
We	O
sampled	O
three	O
exclusive	O
subsets	O
of	O
size	O
x	O
,	O
y	O
,	O
and	O
w	O
=	O
804	O
from	O
the	O
training	O
set	O
(	O
"	O
Train	O
"	O
)	O
.	O
We	O
use	O
the	O
first	O
subset	O
(	O
of	O
size	O
x	O
)	O
as	O
the	O
new	O
test	O
set	O
.	O
We	O
build	O
four	O
new	O
training	O
sets	O
and	O
feed	O
into	O
the	O
SCIIE	O
model	O
.	O
Each	O
new	O
training	O
set	O
has	O
y	O
+	O
w	O
+	O
z	O
=	O
1	O
,	O
355	O
sentences	O
.	O
"	O
TestTrainMistake"/"TestTrainCorrect	O
"	O
:	O
the	O
original	O
good	O
test	O
subset	O
,	O
the	O
third	O
sampled	O
training	O
subset	O
,	O
and	O
the	O
original	O
wrong	O
test	O
subset	O
(	O
or	O
the	O
corrected	O
test	O
subset	O
)	O
;	O
"	O
PureTrainMistake"/"PureTrainCorrect	O
"	O
:	O
the	O
second	O
and	O
third	O
sampled	O
training	O
subsets	O
and	O
the	O
original	O
wrong	O
test	O
subset	O
(	O
or	O
the	O
corrected	O
test	O
subset	O
)	O
;	O
"	O
MistakeTestTrain"/"CorrectTestTrain	O
"	O
:	O
the	O
original	O
wrong	O
test	O
subset	O
(	O
or	O
the	O
corrected	O
test	O
subset	O
)	O
,	O
the	O
original	O
good	O
test	O
subset	O
,	O
and	O
the	O
third	O
sampled	O
training	O
subset	O
;	O
"	O
MistakePureTrain"/"CorrectPureTrain	O
"	O
:	O
the	O
original	O
wrong	O
test	O
subset	O
(	O
or	O
the	O
corrected	O
test	O
subset	O
)	O
and	O
the	O
second	O
and	O
third	O
sampled	O
training	O
subsets	O
.	O
Results	O
show	O
that	O
the	O
label	O
mistakes	O
(	O
i.e.	O
,	O
original	O
wrong	O
test	O
subset	O
)	O
hurt	O
the	O
model	O
performance	O
whenever	O
being	O
fed	O
at	O
the	O
beginning	O
or	O
later	O
.	O
The	O
corrected	O
test	O
subset	O
delivers	O
comparable	O
performance	O
with	O
the	O
original	O
good	O
test	O
subset	O
and	O
the	O
training	O
set	O
.	O
This	O
demonstrates	O
the	O
label	O
consistency	O
of	O
the	O
corrected	O
test	O
set	O
with	O
the	O
training	O
set	O
.	O

The	O
visual	O
results	O
of	O
the	O
proposed	O
methods	O
have	O
been	O
presented	O
in	O
Section	O
2	O
.	O
Here	O
we	O
deploy	O
five	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
NER	B-TaskName
models	O
to	O
investigate	O
their	O
performance	O
on	O
the	O
corrected	O
SCIERC	B-DatasetName
dataset	O
.	O
The	O
NER	B-TaskName
models	O
are	O
BiLSTM	B-MethodName
-	O
CRF	B-MethodName
(	O
Lample	O
et	O
al	O
,	O
2016	O
)	O
,	O
LM	O
-	O
BiLSTM	B-MethodName
-	O
CRF	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2018	O
)	O
,	O
singletask	O
and	O
multi	O
-	O
task	O
SCIIE	O
(	O
Luan	O
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
multi	O
-	O
task	O
DyGIE	O
(	O
Luan	O
et	O
al	O
,	O
2019	O
)	O
.	O
As	O
shown	O
in	O
Table	O
2	O
,	O
all	O
NER	B-TaskName
models	O
deliver	O
better	O
performance	O
on	O
the	O
corrected	O
SCIERC	B-DatasetName
than	O
the	O
original	O
dataset	O
.	O
So	O
the	O
training	O
set	O
is	O
more	O
consistent	O
with	O
the	O
fixed	O
test	O
set	O
than	O
the	O
original	O
wrong	O
test	O
set	O
.	O
In	O
future	O
work	O
,	O
we	O
will	O
explore	O
more	O
baselines	O
in	O
the	O
leaderboard	O
.	O

NER	B-TaskName
is	O
typically	O
cast	O
as	O
a	O
sequence	O
labeling	O
problem	O
and	O
solved	O
by	O
models	O
integrate	O
LSTMs	O
,	O
CRF	B-MethodName
,	O
and	O
language	O
models	O
(	O
Lample	O
et	O
al	O
,	O
2016	O
;	O
Liu	O
et	O
al	O
,	O
2018	O
;	O
Zeng	O
et	O
al	O
,	O
2019	O
.	O
Another	O
idea	O
is	O
to	O
generate	O
span	O
candidates	O
and	O
predict	O
their	O
type	O
.	O
Span	O
-	O
based	O
models	O
have	O
been	O
proposed	O
with	O
multitask	O
learning	O
strategies	O
(	O
Luan	O
et	O
al	O
,	O
2018	O
(	O
Luan	O
et	O
al	O
,	O
,	O
2019	O
.	O
The	O
multiple	O
tasks	O
include	O
concept	O
recognition	O
,	O
relation	B-TaskName
extraction	I-TaskName
,	O
and	O
co	O
-	O
reference	O
resolution	O
.	O
Researchers	O
notice	O
label	O
mistakes	O
in	O
many	O
NLP	O
tasks	O
(	O
Manning	O
,	O
2011	O
;	O
Wang	O
et	O
al	O
,	O
2019	O
;	O
Eskin	O
,	O
2000	O
;	O
Kvȇtoň	O
and	O
Oliva	O
,	O
2002	O
)	O
.	O
For	O
instance	O
,	O
it	O
is	O
reported	O
that	O
the	O
bottleneck	O
of	O
the	O
POS	O
tagging	O
task	O
is	O
the	O
consistency	O
of	O
the	O
annotation	O
result	O
(	O
Manning	O
,	O
2011	O
)	O
.	O
People	O
tried	O
to	O
detect	O
label	O
mistakes	O
automatically	O
and	O
minimize	O
the	O
influence	O
of	O
noise	O
in	O
training	O
.	O
The	O
mistake	O
re	O
-	O
weighting	O
mechanism	O
is	O
effective	O
in	O
the	O
NER	B-TaskName
task	O
(	O
Wang	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
focus	O
on	O
visually	O
evaluating	O
the	O
label	O
consistency	O
.	O

We	O
presented	O
an	O
empirical	O
method	O
to	O
explore	O
the	O
relationship	O
between	O
label	O
consistency	O
and	O
NER	B-TaskName
model	O
performance	O
.	O
It	O
identified	O
the	O
label	O
inconsistency	O
of	O
test	O
data	O
in	O
SCIERC	B-DatasetName
and	O
CoNLL03	B-DatasetName
datasets	O
(	O
with	O
26.7	O
%	O
and	O
5.4	O
%	O
label	O
mistakes	O
)	O
.	O
It	O
validated	O
the	O
label	O
consistency	O
in	O
multiple	O
sets	O
of	O
NER	B-TaskName
data	O
annotation	O
on	O
two	O
benchmarks	O
,	O
CoNLL03	B-DatasetName
and	O
SCIERC	B-DatasetName
.	O

Controllable	O
Text	B-TaskName
Simplification	I-TaskName
with	O
Explicit	O
Paraphrasing	O

Text	B-TaskName
Simplification	I-TaskName
improves	O
the	O
readability	O
of	O
sentences	O
through	O
several	O
rewriting	O
transformations	O
,	O
such	O
as	O
lexical	O
paraphrasing	O
,	O
deletion	O
,	O
and	O
splitting	O
.	O
Current	O
simplification	O
systems	O
are	O
predominantly	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
that	O
are	O
trained	O
end	O
-	O
to	O
-	O
end	O
to	O
perform	O
all	O
these	O
operations	O
simultaneously	O
.	O
However	O
,	O
such	O
systems	O
limit	O
themselves	O
to	O
mostly	O
deleting	O
words	O
and	O
can	O
not	O
easily	O
adapt	O
to	O
the	O
requirements	O
of	O
different	O
target	O
audiences	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
hybrid	O
approach	O
that	O
leverages	O
linguistically	O
-	O
motivated	O
rules	O
for	O
splitting	O
and	O
deletion	O
,	O
and	O
couples	O
them	O
with	O
a	O
neural	O
paraphrasing	O
model	O
to	O
produce	O
varied	O
rewriting	O
styles	O
.	O
We	O
introduce	O
a	O
new	O
data	B-TaskName
augmentation	I-TaskName
method	O
to	O
improve	O
the	O
paraphrasing	O
capability	O
of	O
our	O
model	O
.	O
Through	O
automatic	O
and	O
manual	O
evaluations	O
,	O
we	O
show	O
that	O
our	O
proposed	O
model	O
establishes	O
a	O
new	O
state	O
-	O
ofthe	O
art	O
for	O
the	O
task	O
,	O
paraphrasing	O
more	O
often	O
than	O
the	O
existing	O
systems	O
,	O
and	O
can	O
control	O
the	O
degree	O
of	O
each	O
simplification	O
operation	O
applied	O
to	O
the	O
input	O
texts	O
.	O
1	O

Text	B-TaskName
Simplification	I-TaskName
aims	O
to	O
improve	O
the	O
readability	O
of	O
texts	O
with	O
simpler	O
grammar	O
and	O
word	O
choices	O
while	O
preserving	O
meaning	O
(	O
Saggion	O
,	O
2017	O
)	O
.	O
It	O
provides	O
reading	O
assistance	O
to	O
children	O
(	O
Kajiwara	O
et	O
al	O
,	O
2013	O
)	O
,	O
non	O
-	O
native	O
speakers	O
(	O
Petersen	O
and	O
Ostendorf	O
,	O
2007	O
;	O
Pellow	O
and	O
Eskenazi	O
,	O
2014	O
;	O
Paetzold	O
,	O
2016	O
)	O
,	O
and	O
people	O
with	O
reading	O
disabilities	O
(	O
Rello	O
et	O
al	O
,	O
2013	O
)	O
.	O
It	O
also	O
helps	O
with	O
downstream	O
natural	O
language	O
processing	O
tasks	O
,	O
such	O
as	O
parsing	O
(	O
Chandrasekar	O
et	O
al	O
,	O
1996	O
)	O
,	O
semantic	O
role	O
labelling	O
(	O
Vickrey	O
and	O
Koller	O
,	O
2008	O
)	O
,	O
information	O
extraction	O
(	O
Miwa	O
et	O
al	O
,	O
2010	O
)	O
,	O
and	O
machine	B-TaskName
translation	I-TaskName
(	O
MT	O
,	O
Chen	O
et	O
al	O
,	O
2012	O
;	O
Štajner	O
and	O
Popovic	O
,	O
2016	O
)	O
.	O
Since	O
2016	O
,	O
nearly	O
all	O
text	B-TaskName
simplification	I-TaskName
systems	O
have	O
been	O
sequence	O
-	O
to	O
-	O
sequence	O
(	O
seq2seq	B-MethodName
)	O
Table	O
1	O
:	O
Output	O
statistics	O
of	O
500	O
random	O
sentences	O
from	O
the	O
Newsela	B-DatasetName
test	O
set	O
.	O
Existing	O
systems	O
rely	O
on	O
deletion	O
and	O
do	O
not	O
paraphrase	O
well	O
.	O
OLen	O
,	O
%	O
new	O
,	O
%	O
eq	O
and	O
%	O
split	O
denote	O
the	O
average	O
output	O
length	O
,	O
percentage	O
of	O
new	O
words	O
added	O
,	O
percentage	O
of	O
system	O
outputs	O
that	O
are	O
identical	O
to	O
the	O
inputs	O
,	O
and	O
percentage	O
of	O
sentence	O
splits	O
,	O
respectively	O
.	O
†We	O
used	O
the	O
system	O
outputs	O
shared	O
by	O
their	O
authors	O
.	O
models	O
trained	O
end	O
-	O
to	O
-	O
end	O
,	O
which	O
have	O
greatly	O
increased	O
the	O
fluency	O
of	O
the	O
outputs	O
(	O
Zhang	O
and	O
Lapata	O
,	O
2017	O
;	O
Nisioi	O
et	O
al	O
,	O
2017	O
;	O
Zhao	O
et	O
al	O
,	O
2018	O
;	O
Kriz	O
et	O
al	O
,	O
2019	O
;	O
Dong	O
et	O
al	O
,	O
2019	O
;	O
.	O
However	O
,	O
these	O
systems	O
mostly	O
rely	O
on	O
deletion	O
and	O
tend	O
to	O
generate	O
very	O
short	O
outputs	O
at	O
the	O
cost	O
of	O
meaning	O
preservation	O
(	O
Alva	O
-	O
Manchego	O
et	O
al	O
,	O
2017	O
)	O
.	O
Table	O
1	O
shows	O
that	O
they	O
neither	O
split	O
sentences	O
nor	O
paraphrase	O
well	O
as	O
reflected	O
by	O
the	O
low	O
percentage	O
of	O
splits	O
(	O
<	O
1	O
%	O
)	O
and	O
new	O
words	O
introduced	O
(	O
<	O
11.2	O
%	O
)	O
.	O
While	O
deleting	O
words	O
is	O
a	O
viable	O
(	O
and	O
the	O
simplest	O
)	O
way	O
to	O
reduce	O
the	O
complexity	O
of	O
sentences	O
,	O
it	O
is	O
suboptimal	O
and	O
unsatisfying	O
.	O
Professional	O
editors	O
are	O
known	O
to	O
use	O
a	O
sophisticated	O
combination	O
of	O
deletion	O
,	O
paraphrasing	O
,	O
and	O
sentence	O
splitting	O
to	O
simplify	O
texts	O
(	O
Xu	O
et	O
al	O
,	O
2015	O
)	O
.	O
Another	O
drawback	O
of	O
these	O
end	O
-	O
to	O
-	O
end	O
neural	O
systems	O
is	O
the	O
lack	O
of	O
controllability	O
.	O
Simplification	O
is	O
highly	O
audience	O
dependant	O
,	O
and	O
what	O
constitutes	O
simplified	O
text	O
for	O
one	O
group	O
of	O
users	O
may	O
not	O
be	O
acceptable	O
for	O
other	O
groups	O
(	O
Xu	O
et	O
al	O
,	O
2015	O
;	O
Lee	O
and	O
Yeung	O
,	O
2018	O
)	O
.	O
An	O
ideal	O
simplification	O
system	O
should	O
be	O
able	O
to	O
generate	O
text	O
with	O
varied	O
characteristics	O
,	O
such	O
as	O
different	O
lengths	O
,	O
readability	O
levels	O
,	O
and	O
number	O
of	O
split	O
sentences	O
,	O
which	O
can	O
be	O
difficult	O
to	O
control	O
in	O
end	O
-	O
to	O
-	O
end	O
systems	O
.	O
To	O
address	O
these	O
issues	O
,	O
we	O
propose	O
a	O
novel	O
hybrid	O
approach	O
that	O
combines	O
linguisticallymotivated	O
syntactic	O
rules	O
with	O
data	O
-	O
driven	O
neural	O
models	O
to	O
improve	O
the	O
diversity	O
and	O
controllability	O
of	O
the	O
simplifications	O
.	O
We	O
hypothesize	O
that	O
the	O
seq2seq	B-MethodName
generation	O
model	O
will	O
learn	O
lexical	O
and	O
structural	O
paraphrases	O
more	O
efficiently	O
from	O
the	O
parallel	O
corpus	O
,	O
when	O
we	O
offload	O
some	O
of	O
the	O
burden	O
of	O
sentence	O
splitting	O
(	O
e.g.	O
,	O
split	O
at	O
comma	O
)	O
and	O
deletion	O
(	O
e.g.	O
,	O
remove	O
trailing	O
preposition	O
phrases	O
)	O
decisions	O
to	O
a	O
separate	O
component	O
.	O
Previous	O
hybrid	O
approaches	O
for	O
simplification	O
(	O
Narayan	O
and	O
Gardent	O
,	O
2014	O
;	O
Siddharthan	O
and	O
Mandya	O
,	O
2014	O
;	O
Sulem	O
et	O
al	O
,	O
2018c	O
)	O
used	O
splitting	O
and	O
deletion	O
rules	O
in	O
a	O
deterministic	O
step	O
before	O
applying	O
an	O
MT	O
-	O
based	O
paraphrasing	O
model	O
.	O
In	O
contrast	O
,	O
our	O
approach	O
provides	O
a	O
more	O
flexible	O
and	O
dynamic	O
integration	O
of	O
linguistic	O
rules	O
with	O
the	O
neural	O
models	O
through	O
ranking	O
and	O
data	B-TaskName
augmentation	I-TaskName
(	O
Figure	O
1	O
)	O
.	O
We	O
compare	O
our	O
method	O
to	O
several	O
state	O
-	O
of	O
-	O
theart	O
systems	O
in	O
both	O
automatic	O
and	O
human	O
evaluations	O
.	O
Our	O
model	O
achieves	O
overall	O
better	O
performance	O
measured	O
by	O
SARI	O
(	O
Xu	O
et	O
al	O
,	O
2016	O
)	O
and	O
other	O
metrics	O
,	O
showing	O
that	O
the	O
generated	O
outputs	O
are	O
more	O
similar	O
to	O
those	O
written	O
by	O
human	O
editors	O
.	O
We	O
also	O
demonstrate	O
that	O
our	O
model	O
can	O
control	O
the	O
extent	O
of	O
each	O
simplification	O
operation	O
by	O
:	O
(	O
1	O
)	O
imposing	O
a	O
soft	O
constraint	O
on	O
the	O
percentage	O
of	O
words	O
to	O
be	O
copied	O
from	O
the	O
input	O
in	O
the	O
seq2seq	B-MethodName
model	O
,	O
thus	O
limiting	O
lexical	O
paraphrasing	O
;	O
and	O
(	O
2	O
)	O
selecting	O
candidates	O
that	O
underwent	O
a	O
desired	O
amount	O
of	O
splitting	O
and/or	O
deletion	O
.	O
Finally	O
,	O
we	O
create	O
a	O
new	O
test	O
dataset	O
with	O
multiple	O
human	O
references	O
for	O
Newsela	B-DatasetName
(	O
Xu	O
et	O
al	O
,	O
2015	O
)	O
,	O
the	O
widely	O
used	O
text	B-TaskName
simplification	I-TaskName
corpus	O
,	O
to	O
specifically	O
evaluate	O
lexical	O
paraphrasing	O
.	O

Figure	O
1	O
shows	O
an	O
overview	O
of	O
our	O
hybrid	O
approach	O
.	O
We	O
combine	O
linguistic	O
rules	O
with	O
data	O
-	O
driven	O
neural	O
models	O
to	O
improve	O
the	O
controllability	O
and	O
diversity	O
of	O
the	O
outputs	O
.	O
Given	O
an	O
input	O
complex	O
sentence	O
x	O
,	O
we	O
first	O
generate	O
a	O
set	O
of	O
intermediate	O
simplifications	O
V	O
=	O
{	O
v	O
1	O
,	O
v	O
2	O
,	O
.	O
.	O
.	O
,	O
v	O
n	O
}	O
that	O
have	O
undergone	O
splitting	O
and	O
deletion	O
(	O
2.1	O
)	O
.	O
These	O
intermediate	O
sentences	O
are	O
then	O
used	O
for	O
two	O
purposes	O
:	O
(	O
1	O
)	O
Selected	O
by	O
a	O
pairwise	O
neural	O
ranking	O
model	O
(	O
2.2	O
)	O
based	O
on	O
the	O
simplification	O
quality	O
and	O
then	O
rewritten	O
by	O
the	O
paraphrasing	O
component	O
;	O
(	O
2	O
)	O
Used	O
for	O
data	B-TaskName
augmentation	I-TaskName
to	O
improve	O
the	O
diversity	O
of	O
the	O
paraphrasing	O
model	O
(	O
2.3	O
)	O
.	O

We	O
leverage	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
system	O
for	O
structural	O
simplification	O
,	O
called	O
DisSim	O
(	O
Niklaus	O
et	O
al	O
,	O
2019	O
)	O
,	O
to	O
generate	O
candidate	O
simplifications	O
that	O
focus	O
on	O
splitting	O
and	O
deletion	O
.	O
2	O
The	O
English	O
version	O
of	O
DisSim	O
applies	O
35	O
hand	O
-	O
crafted	O
grammar	O
rules	O
to	O
break	O
down	O
a	O
complex	O
sentence	O
into	O
a	O
set	O
of	O
hierarchically	O
organized	O
sub	O
-	O
sentences	O
(	O
see	O
Figure	O
1	O
for	O
an	O
example	O
)	O
.	O
We	O
choose	O
a	O
rule	O
-	O
based	O
approach	O
for	O
sentence	O
splitting	O
because	O
it	O
works	O
really	O
well	O
.	O
In	O
our	O
pilot	O
experiments	O
,	O
DisSim	O
successfully	O
split	O
92	O
%	O
of	O
100	O
complex	O
sentences	O
from	O
the	O
training	O
data	O
with	O
more	O
than	O
20	O
words	O
,	O
and	O
introduced	O
errors	O
for	O
only	O
6.8	O
%	O
of	O
these	O
splits	O
.	O
We	O
consider	O
these	O
sub	O
-	O
sentences	O
as	O
candidate	O
simplifications	O
for	O
the	O
later	O
steps	O
,	O
except	O
those	O
that	O
are	O
extremely	O
short	O
or	O
long	O
(	O
compression	O
ratio	O
/	O
[	O
0.5	O
,	O
1.5	O
]	O
)	O
.	O
The	O
compression	O
ratio	O
is	O
calculated	O
as	O
the	O
number	O
of	O
words	O
in	O
a	O
candidate	O
simplification	O
v	O
i	O
(	O
which	O
may	O
contain	O
one	O
or	O
more	O
sub	O
-	O
sentences	O
)	O
divided	O
by	O
that	O
of	O
the	O
original	O
sentence	O
x.	O
To	O
further	O
increase	O
the	O
variety	O
of	O
generated	O
candidates	O
,	O
we	O
supplement	O
DisSim	O
with	O
a	O
Neural	O
Deletion	O
and	O
Split	O
module	O
trained	O
on	O
the	O
text	B-TaskName
simplification	I-TaskName
corpus	O
(	O
3.1	O
)	O
.	O
We	O
use	O
a	O
Transformer	B-MethodName
seq2seq	B-MethodName
model	O
with	O
the	O
same	O
configuration	O
as	O
the	O
base	O
model	O
for	O
paraphrasing	O
(	O
2.3	O
)	O
.	O
Given	O
the	O
input	O
sentence	O
x	O
,	O
we	O
constrain	O
the	O
beam	O
search	O
to	O
generate	O
10	O
outputs	O
with	O
splitting	O
and	O
another	O
10	O
outputs	O
without	O
splitting	O
.	O
Then	O
,	O
we	O
select	O
the	O
outputs	O
that	O
do	O
not	O
deviate	O
substantially	O
from	O
x	O
(	O
i.e.	O
,	O
Jaccard	O
similarity	O
>	O
0.5	O
)	O
.	O
We	O
add	O
outputs	O
from	O
the	O
two	O
systems	O
to	O
the	O
candidate	O
pool	O
V	O
.	O

We	O
can	O
control	O
our	O
model	O
to	O
concentrate	O
on	O
specific	O
operations	O
.	O
For	O
split	O
-	O
or	O
delete	O
-	O
focused	O
simplification	O
,	O
we	O
select	O
candidates	O
with	O
desirable	O
length	O
or	O
number	O
of	O
splits	O
during	O
the	O
candidate	O
generation	O
step	O
.	O
We	O
perform	O
only	O
the	O
paraphrase	B-TaskName
generation	I-TaskName
step	O
for	O
paraphrase	O
-	O
focused	O
simplification	O
.	O
The	O
paraphrasing	O
model	O
is	O
designed	O
specifically	O
to	O
paraphrase	O
with	O
minimal	O
deletion	O
and	O
without	O
splitting	O
.	O
It	O
retains	O
the	O
length	O
and	O
the	O
number	O
of	O
split	O
sentences	O
in	O
the	O
output	O
,	O
thus	O
preserving	O
the	O
extent	O
of	O
deletion	O
and	O
splitting	O
controlled	O
in	O
the	O
previous	O
steps	O
.	O
We	O
control	O
the	O
degree	O
of	O
paraphrasing	O
by	O
changing	O
the	O
copy	O
ratio	O
.	O

We	O
use	O
the	O
following	O
simplification	O
approaches	O
as	O
baselines	O
:	O
(	O
i	O
)	O
BERT	B-MethodName
-	O
Initialized	O
Transfomer	O
(	O
?	O
)	O
,	O
where	O
the	O
encoder	O
is	O
initialized	O
with	O
BERT	B-MethodName
base	O
checkpoint	O
and	O
the	O
decoder	O
is	O
randomly	O
initialized	O
.	O
It	O
is	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
for	O
text	B-TaskName
simplification	I-TaskName
.	O
(	O
ii	O
)	O
EditNTS	O
(	O
Dong	O
et	O
al	O
,	O
2019	O
)	O
,	O
6	O
another	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
model	O
that	O
uses	O
a	O
neural	O
programmer	O
-	O
interpreter	O
(	O
Reed	O
and	O
de	O
Freitas	O
,	O
2016	O
)	O
to	O
predict	O
the	O
edit	O
operation	O
on	O
each	O
word	O
,	O
and	O
then	O
generates	O
the	O
simplified	O
sentence	O
.	O
(	O
iii	O
)	O
LSTM	B-MethodName
baseline	O
,	O
a	O
vanilla	O
encoderdecoder	O
model	O
used	O
in	O
Zhang	O
and	O
Lapata	O
(	O
2017	O
)	O
.	O
(	O
iv	O
)	O
Hybrid	O
-	O
NG	O
(	O
Narayan	O
and	O
Gardent	O
,	O
2014	O
)	O
,	O
7	O
one	O
of	O
the	O
best	O
existing	O
hybrid	O
systems	O
that	O
performs	O
splitting	O
and	O
deletion	O
using	O
a	O
probabilistic	O
model	O
and	O
lexical	O
substitution	O
with	O
a	O
phrase	O
-	O
based	O
machine	B-TaskName
translation	I-TaskName
system	O
.	O
We	O
retrained	O
all	O
the	O
models	O
on	O
the	O
NEWSLA	O
-	O
AUTO	O
dataset	O
.	O
Table	O
3	O
:	O
Automatic	O
evaluation	O
results	O
on	O
NEWSELA	B-DatasetName
-	O
TURK	O
that	O
focuses	O
on	O
paraphrasing	O
(	O
500	O
complex	O
sentences	O
with	O
4	O
human	O
written	O
paraphrases	O
)	O
.	O
We	O
control	O
the	O
extent	O
of	O
paraphrasing	O
of	O
our	O
models	O
by	O
specifying	O
the	O
percentage	O
of	O
words	O
to	O
be	O
copied	O
(	O
cp	O
)	O
from	O
the	O
input	O
as	O
a	O
soft	O
constraint	O
.	O

We	O
performed	O
two	O
human	O
evaluations	O
:	O
one	O
to	O
measure	O
the	O
overall	O
simplification	O
quality	O
and	O
the	O
other	O
to	O
specifically	O
capture	O
sentence	O
splitting	O
.	O
11	O
For	O
the	O
first	O
one	O
,	O
we	O
asked	O
five	O
Amazon	O
Mechanical	O
Turk	O
workers	O
to	O
evaluate	O
fluency	O
,	O
adequacy	O
and	O
simplicity	O
of	O
100	O
random	O
simplifications	O
from	O
the	O
NEWSELA	B-DatasetName
-	O
AUTO	O
test	O
set	O
.	O
We	O
supplemented	O
the	O
2	O
-	O
3	O
readability	O
levels	O
in	O
NEWSELA	B-DatasetName
-	O
AUTO	O
,	O
which	O
contained	O
more	O
lexical	O
overlaps	O
and	O
inflated	O
the	O
scores	O
for	O
EditNTS	O
.	O
11	O
We	O
provide	O
instructions	O
in	O
Appendix	O
E.	O
fluency	O
and	O
adequacy	O
ratings	O
with	O
binary	O
questions	O
described	O
in	O
Zhang	O
et	O
al	O
(	O
2020a	O
)	O
for	O
the	O
second	O
evaluation	O
over	O
another	O
100	O
simplifications	O
from	O
the	O
NEWSELA	B-DatasetName
-	O
AUTO	O
split	O
-	O
focused	O
test	O
set	O
.	O
We	O
asked	O
if	O
the	O
output	O
sentence	O
exhibits	O
spitting	O
and	O
if	O
the	O
splitting	O
occurs	O
at	O
the	O
correct	O
place	O
.	O
While	O
fluency	O
measures	O
the	O
grammaticality	O
of	O
the	O
output	O
,	O
adequacy	O
captures	O
the	O
extent	O
of	O
meaning	O
preserved	O
when	O
compared	O
to	O
the	O
input	O
.	O
Simplicity	O
evaluates	O
if	O
the	O
output	O
is	O
simpler	O
than	O
the	O
input	O
.	O
Each	O
sentence	O
was	O
rated	O
on	O
a	O
5	O
-	O
point	O
Likert	O
scale	O
and	O
we	O
averaged	O
the	O
ratings	O
from	O
the	O
five	O
workers	O
.	O
We	O
chose	O
the	O
majority	O
value	O
for	O
the	O
binary	O
ratings	O
.	O
We	O
used	O
the	O
output	O
of	O
our	O
model	O
that	O
is	O
tailored	O
for	O
sentence	O
splitting	O
for	O
the	O
second	O
evaluation	O
.	O
Table	O
6	O
demonstrates	O
that	O
our	O
model	O
achieves	O
the	O
best	O
fluency	O
,	O
simplicity	O
,	O
and	O
overall	O
ratings	O
.	O
The	O
adequacy	O
rating	O
is	O
also	O
very	O
close	O
to	O
that	O
of	O
Transformer	B-MethodName
bert	O
and	O
EditNTS	O
even	O
though	O
our	O
model	O
is	O
performing	O
more	O
paraphrasing	O
(	O
Table	O
2	O
)	O
,	O
which	O
verifies	O
that	O
the	O
changes	O
made	O
by	O
our	O
system	O
are	O
meaningful	O
.	O
Our	O
model	O
achieves	O
the	O
most	O
number	O
of	O
correct	O
sentence	O
splits	O
(	O
90	O
%	O
)	O
,	O
and	O
the	O
highest	O
fluency	O
(	O
4.19	O
)	O
for	O
syntactic	O
simplification	O
,	O
showing	O
that	O
it	O
can	O
generate	O
more	O
number	O
of	O
coherent	O
sentence	O
splits	O
when	O
compared	O
to	O
other	O
models	O
.	O

We	O
evaluate	O
our	O
key	O
design	O
choices	O
,	O
namely	O
candidate	O
ranking	O
that	O
is	O
based	O
on	O
length	O
-	O
penalized	O
BERTScore	O
and	O
paraphrase	B-TaskName
generation	I-TaskName
that	O
uses	O
data	B-TaskName
augmentation	I-TaskName
and	O
copy	O
attention	O
.	O
Table	O
8	O
summarizes	O
the	O
results	O
.	O
Our	O
pairwise	O
ranking	O
model	O
(	O
BERTScore	O
len	O
)	O
achieves	O
an	O
increase	O
of	O
3.2	O
points	O
in	O
SARI	O
when	O
compared	O
to	O
choosing	O
a	O
random	O
(	O
Random	O
)	O
candidate	O
.	O
Randomly	O
selecting	O
a	O
candidate	O
also	O
performs	O
fairly	O
well	O
,	O
indicating	O
that	O
the	O

They	O
float	O
in	O
and	O
out	O
of	O
places	O
that	O
combine	O
stage	O
with	O
the	O
underwater	O
.	O
Compared	O
to	O
our	O
final	O
model	O
(	O
Our	O
Model	O
)	O
,	O
its	O
variants	O
without	O
data	B-TaskName
augmentation	I-TaskName
(	O
−	O
augmentation	O
)	O
and	O
copy	O
mechanism	O
(	O
−	O
copy	O
attn	O
)	O
suffer	O
a	O
drop	O
of	O
1.0	O
and	O
2.6	O
points	O
in	O
SARI	O
respectively	O
and	O
a	O
decrease	O
of	O
at	O
least	O
3.0	O
%	O
of	O
new	O
words	O
,	O
which	O
demonstrates	O
that	O
these	O
components	O
encourage	O
the	O
system	O
to	O
paraphrase	O
.	O
Our	O
model	O
trained	O
on	O
only	O
DisSim	O
(	O
−	O
only	O
DisSim	O
)	O
and	O
Transformer	B-MethodName
(	O
−	O
only	O
Transformer	B-MethodName
)	O
candidates	O
performs	O
close	O
to	O
our	O
best	O
model	O
(	O
Our	O
Model	O
)	O
in	O
terms	O
of	O
SARI	O
.	O

To	O
understand	O
the	O
errors	O
generated	O
by	O
our	O
model	O
,	O
we	O
manually	O
classified	O
200	O
simplifications	O
from	O
the	O
NEWSELA	B-DatasetName
-	O
AUTO	O
test	O
set	O
into	O
the	O
following	O
categories	O
:	O
(	O
a	O
)	O
Good	O
,	O
where	O
the	O
model	O
generated	O
meaningful	O
simplifications	O
,	O
(	O
b	O
)	O
Hallucinations	O
,	O
where	O
the	O
model	O
introduced	O
information	O
not	O
in	O
the	O
input	O
,	O
(	O
c	O
)	O
Fluency	O
Errors	O
,	O
where	O
the	O
model	O
generated	O
ungrammatical	O
output	O
,	O
(	O
d	O
)	O
Anaphora	O
Resolution	O
,	O
where	O
it	O
was	O
difficult	O
to	O
resolve	O
pronouns	O
in	O
the	O
output	O
.	O
(	O
e	O
)	O
Bad	O
substitution	O
,	O
where	O
the	O
model	O
inserted	O
an	O
incorrect	O
simpler	O
phrase	O
,	O
and	O
(	O
e	O
)	O
Human	O
Reference	O
Errors	O
,	O
where	O
the	O
reference	O
does	O
not	O
reflect	O
the	O
source	O
sentence	O
.	O
Note	O
that	O
a	O
simplification	O
can	O
belong	O
to	O
multiple	O
error	O
categories	O
.	O
Table	O
7	O
shows	O
the	O
examples	O
of	O
each	O
category	O
.	O

Before	O
the	O
advent	O
of	O
neural	O
networks	O
,	O
text	B-TaskName
simplification	I-TaskName
approaches	O
performed	O
each	O
operation	O
separately	O
in	O
a	O
pipeline	O
manner	O
using	O
either	O
handcrafted	O
rules	O
(	O
Carroll	O
et	O
al	O
,	O
1999	O
;	O
Siddharthan	O
,	O
2002	O
;	O
Siddharthan	O
et	O
al	O
,	O
2004	O
)	O
or	O
data	O
-	O
driven	O
methods	O
based	O
on	O
parallel	O
corpora	O
(	O
Zhu	O
et	O
al	O
,	O
2010	O
;	O
Woodsend	O
and	O
Lapata	O
,	O
2011	O
;	O
Narayan	O
and	O
Gardent	O
,	O
2014	O
)	O
.	O
Following	O
neural	O
machine	B-TaskName
translation	I-TaskName
,	O
the	O
trend	O
changed	O
to	O
performing	O
all	O
the	O
operations	O
together	O
end	O
-	O
toend	O
(	O
Zhang	O
and	O
Lapata	O
,	O
2017	O
;	O
Nisioi	O
et	O
al	O
,	O
2017	O
;	O
Zhao	O
et	O
al	O
,	O
2018	O
;	O
Alva	O
-	O
Manchego	O
et	O
al	O
,	O
2017	O
they	O
discovered	O
that	O
the	O
ship	O
had	O
been	O
important	O
.	O
EditNTS	O
since	O
2010	O
,	O
project	O
researchers	O
have	O
uncovered	O
documents	O
in	O
portugal	O
.	O
have	O
revealed	O
who	O
owned	O
the	O
ship	O
.	O
Our	O
Model	O
(	O
cp	O
=	O
0.6	O
)	O
scientists	O
have	O
found	O
a	O
secret	O
deal	O
.	O
they	O
have	O
discovered	O
who	O
owned	O
the	O
ship	O
.	O
Our	O
Model	O
(	O
cp	O
=	O
0.7	O
)	O
scientists	O
have	O
found	O
documents	O
in	O
portugal	O
.	O
they	O
have	O
also	O
found	O
out	O
who	O
owned	O
the	O
ship	O
.	O
Our	O
Model	O
(	O
cp	O
=	O
0.8	O
)	O
scientists	O
have	O
found	O
documents	O
in	O
portugal	O
.	O
they	O
have	O
discovered	O
who	O
owned	O
the	O
ship	O
.	O

We	O
proposed	O
a	O
novel	O
hybrid	O
approach	O
for	O
sentence	O
simplification	O
that	O
performs	O
better	O
and	O
produces	O
more	O
diverse	O
outputs	O
than	O
the	O
existing	O
systems	O
.	O
We	O
designed	O
a	O
new	O
data	B-TaskName
augmentation	I-TaskName
method	O
to	O
encourage	O
the	O
model	O
to	O
paraphrase	O
.	O
We	O
created	O
a	O
new	O
dataset	O
,	O
NEWSELA	B-DatasetName
-	O
TURK	O
,	O
to	O
evaluate	O
paraphrasing	O
-	O
focused	O
simplifications	O
.	O
We	O
showed	O
that	O
our	O
model	O
can	O
control	O
various	O
attributes	O
of	O
the	O
simplified	O
text	O
,	O
such	O
as	O
number	O
of	O
sentence	O
splits	O
,	O
length	O
,	O
and	O
number	O
of	O
words	O
copied	O
from	O
the	O
input	O
.	O

This	O
year	O
,	O
it	O
approved	O
dozens	O
of	O
permits	O
for	O
agricultural	O
drone	O
businesses	O
.	O
Hybrid	O
-	O
NG	O
this	O
year	O
,	O
the	O
government	O
has	O
approved	O
dozens	O
of	O
drone	O
permits	O
for	O
agricultural	O
businesses	O
.	O
LSTM	B-MethodName
this	O
year	O
,	O
the	O
faa	O
has	O
approved	O
dozens	O
of	O
permits	O
for	O
agricultural	O
drone	O
businesses	O
.	O
Transformer	B-MethodName
bert	O
this	O
year	O
,	O
the	O
faa	O
has	O
approved	O
dozens	O
of	O
permits	O
for	O
agricultural	O
businesses	O
.	O

As	O
she	O
spoke	O
,	O
the	O
building	O
echoed	O
with	O
music	O
and	O
the	O
beat	O
of	O
drums	O
.	O
Hybrid	O
-	O
NG	O
echoed	O
the	O
room	O
.	O
LSTM	B-MethodName
the	O
room	O
echoed	O
with	O
the	O
sounds	O
of	O
song	O
,	O
the	O
voices	O
of	O
young	O
men	O
.	O
Transformer	B-MethodName
bert	O
the	O
room	O
echoed	O
with	O
the	O
sound	O
of	O
song	O
,	O
the	O
beat	O
of	O
drums	O
,	O
the	O
voices	O
of	O
young	O
men	O
.	O

We	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
valuable	O
feedback	O
.	O
We	O
thank	O
Newsela	B-DatasetName
for	O
sharing	O
the	O
data	O
and	O
NVIDIA	O
for	O
providing	O
GPU	O
computing	O
resources	O
.	O
This	O
research	O
is	O
supported	O
in	O
part	O
by	O
the	O
NSF	O
award	O
IIS	O
-	O
1822754	O
,	O
ODNI	O
and	O
IARPA	O
via	O
the	O
BETTER	O
program	O
contract	O
19051600004	O
.	O
The	O
views	O
and	O
conclusions	O
contained	O
herein	O
are	O
those	O
of	O
the	O
authors	O
and	O
should	O
not	O
be	O
interpreted	O
as	O
necessarily	O
representing	O
the	O
official	O
policies	O
,	O
either	O
expressed	O
or	O
implied	O
,	O
of	O
NSF	O
,	O
ODNI	O
,	O
IARPA	O
,	O
or	O
the	O
U.S.	O
Government	O
.	O
The	O
U.S.	O
Government	O
is	O
authorized	O
to	O
reproduce	O
and	O
distribute	O
reprints	O
for	O
governmental	O
purposes	O
notwithstanding	O
any	O
copyright	O
annotation	O
therein	O
.	O

This	O
paper	O
describes	O
our	O
official	O
entry	O
LearningToQuestion	O
for	O
SemEval	O
2017	O
task	O
3	O
community	O
question	O
answer	O
,	O
subtask	O
B.	O
The	O
objective	O
is	O
to	O
rerank	O
questions	O
obtained	O
in	O
web	O
forum	O
as	O
per	O
their	O
similarity	O
to	O
original	O
question	O
.	O
Our	O
system	O
uses	O
pairwise	O
learning	O
to	O
rank	O
methods	O
on	O
rich	O
set	O
of	O
hand	O
designed	O
and	O
representation	B-TaskName
learning	I-TaskName
features	O
.	O
We	O
use	O
various	O
semantic	O
features	O
that	O
help	O
our	O
system	O
to	O
achieve	O
promising	O
results	O
on	O
the	O
task	O
.	O
The	O
system	O
achieved	O
second	O
highest	O
results	O
on	O
official	O
metrics	O
MAP	B-DatasetName
and	O
good	O
results	O
on	O
other	O
search	O
metrics	O
.	O

In	O
online	O
forums	O
question	B-TaskName
answering	I-TaskName
is	O
one	O
of	O
the	O
most	O
popular	O
way	O
for	O
users	O
to	O
share	O
information	O
between	O
each	O
other	O
.	O
Due	O
to	O
the	O
unstructured	O
nature	O
of	O
these	O
forums	O
,	O
it	O
's	O
a	O
problem	O
to	O
find	O
relevant	O
information	O
from	O
the	O
already	O
existing	O
information	O
for	O
users	O
.	O
One	O
way	O
to	O
solve	O
this	O
problem	O
is	O
to	O
design	O
systems	O
to	O
automatically	O
find	O
similar	O
content	O
(	O
question	O
,	O
answer	O
,	O
comment	O
)	O
to	O
the	O
user	O
's	O
posted	O
question	O
.	O
SemEval	O
-	O
2017	O
task	O
3	O
(	O
Nakov	O
et	O
al	O
,	O
2017	O
)	O
focuses	O
on	O
solving	O
this	O
problem	O
in	O
community	O
question	O
answer	O
by	O
various	O
subtasks	O
of	O
ranking	O
relevant	O
information	O
in	O
Qatar	O
living	O
forums	O
data	O
.	O
The	O
system	O
presented	O
in	O
this	O
paper	O
focuses	O
on	O
subtask	O
B	O
,	O
to	O
re	O
-	O
rank	O
given	O
set	O
of	O
questions	O
retrieved	O
by	O
search	O
engine	O
,	O
in	O
their	O
similarity	O
to	O
original	O
question	O
.	O
The	O
system	O
is	O
mainly	O
designed	O
by	O
employing	O
learning	O
to	O
rank	O
methods	O
on	O
the	O
rich	O
feature	O
set	O
obtained	O
by	O
text	O
processing	O
of	O
the	O
question	O
text	O
.	O

We	O
primarily	O
use	O
the	O
annotated	O
training	O
,	O
development	O
and	O
testing	O
dataset	O
provided	O
by	O
the	O
SemEval	O
-	O
2017	O
task	O
3	O
organizers	O
.	O
The	O
dataset	O
is	O
collected	O
by	O
organizers	O
from	O
Qatar	O
living	O
forum	O
.	O
It	O
's	O
in	O
the	O
form	O
of	O
an	O
original	O
question	O
and	O
set	O
of	O
related	O
questions	O
.	O
Each	O
related	O
question	O
in	O
training	O
and	O
development	O
dataset	O
is	O
annotated	O
with	O
one	O
of	O
the	O
3	O
possible	O
tags	O
,	O
PerfectMatch	O
,	O
Relevant	O
or	O
Irrelevant	O
.	O
A	O
ranking	O
task	O
is	O
required	O
to	O
rank	O
both	O
Per	O
-	O
fectMatch	O
and	O
Relevant	O
above	O
Irrelevant	O
questions	O
without	O
any	O
distinction	O
between	O
the	O
first	O
two	O
.	O
The	O
train	O
dataset	O
for	O
subtask	O
B	O
consists	O
of	O
317	O
original	O
questions	O
and	O
3169	O
retrieved	O
questions	O
by	O
search	O
engine	O
roughly	O
10	O
related	O
questions	O
per	O
original	O
question	O
.	O
The	O
organizers	O
have	O
also	O
provided	O
annotated	O
test	O
dataset	O
from	O
SemEval	O
-	O
2016	O
challenge	O
.	O
Along	O
with	O
these	O
we	O
also	O
used	O
Glove	B-MethodName
embeddings	I-MethodName
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
which	O
were	O
pretrained	O
using	O
6	O
billion	O
tokens	O
from	O
Wikipedia	O
-	O
2014	O
and	O
Gigaword	O
dataset	O
.	O

Since	O
the	O
task	O
is	O
a	O
ranking	O
task	O
,	O
our	O
system	O
uses	O
learning	O
to	O
rank	O
(	O
Trotman	O
,	O
2005	O
)	O
to	O
model	O
the	O
ranking	O
of	O
questions	O
.	O
Learning	O
to	O
rank	O
refers	O
to	O
various	O
machine	O
learning	O
techniques	O
used	O
in	O
ranking	O
tasks	O
.	O
These	O
have	O
been	O
studied	O
in	O
information	B-TaskName
retrieval	I-TaskName
literature	O
and	O
they	O
power	O
many	O
of	O
the	O
industrial	O
search	O
engines	O
.	O
These	O
systems	O
mainly	O
fall	O
into	O
3	O
categories	O
:	O
pointwise	O
,	O
pairwise	O
and	O
listwise	O
as	O
described	O
in	O
.	O
We	O
use	O
pairwise	O
methods	O
for	O
our	O
system	O
with	O
rich	O
feature	O
set	O
.	O
Our	O
feature	O
set	O
is	O
combination	O
of	O
various	O
hand	O
generated	O
features	O
and	O
semantic	O
features	O
learned	O
by	O
neural	O
network	O
.	O
In	O
the	O
following	O
section	O
we	O
first	O
describe	O
these	O
features	O
and	O
then	O
the	O
learning	O
to	O
rank	O
method	O
used	O
.	O

We	O
use	O
rank	O
given	O
by	O
the	O
search	O
engine	O
as	O
a	O
feature	O
in	O
our	O
system	O
.	O
This	O
gives	O
the	O
system	O
the	O
baseline	O
accuracy	B-MetricName
of	O
the	O
search	O
engine	O
.	O

Topic	O
modeling	O
is	O
used	O
to	O
generate	O
the	O
salient	O
topics	O
in	O
the	O
text	O
.	O
We	O
use	O
Latent	O
Dirichlet	O
al	O
ocation	O
(	O
LDA	B-MethodName
)	O
(	O
Blei	O
et	O
al	O
,	O
2003	O
)	O
to	O
compute	O
topic	O
similarity	O
between	O
texts	O
.	O
We	O
train	O
LDA	B-MethodName
topic	O
model	O
using	O
the	O
whole	O
text	O
(	O
body	O
and	O
subject	O
)	O
as	O
corpus	O
.	O
Then	O
a	O
topic	O
distribution	O
over	O
the	O
50	O
topics	O
is	O
computed	O
for	O
both	O
the	O
text	O
and	O
cosine	O
similarity	O
is	O
used	O
as	O
a	O
feature	O
in	O
the	O
system	O
.	O

We	O
use	O
pairwise	O
learning	O
to	O
rank	O
for	O
ranking	O
task	O
which	O
poses	O
the	O
ranking	O
problem	O
as	O
classification	O
problem	O
to	O
minimize	O
the	O
average	O
number	O
of	O
inversions	O
in	O
ranking	O
.	O
This	O
formulation	O
is	O
more	O
closer	O
to	O
ranking	O
task	O
than	O
predicting	O
relevance	O
as	O
regression	O
and	O
also	O
has	O
theoretical	O
guarantees	O
of	O
maximizing	O
the	O
MAP	B-DatasetName
in	O
ranking	O
(	O
Chen	O
et	O
al	O
,	O
2009	O
)	O
.	O
First	O
,	O
we	O
create	O
these	O
pairs	O
by	O
taking	O
original	O
question	O
Q	O
o	O
and	O
two	O
candidate	O
questions	O
of	O
which	O
one	O
was	O
relevant	O
and	O
other	O
one	O
not	O
,	O
Q	O
c1	O
and	O
Q	O
c2	O
.	O
Then	O
we	O
generate	O
above	O
mentioned	O
feature	O
vectors	O
f	O
(	O
Q	O
o	O
,	O
Q	O
c1	O
)	O
,	O
f	O
(	O
Q	O
o	O
,	O
Q	O
c2	O
)	O
and	O
use	O
feature	O
dif	O
-	O
ference	O
f	O
(	O
Q	O
o	O
,	O
Q	O
c1	O
)	O
−	O
f	O
(	O
Q	O
o	O
,	O
Q	O
c2	O
)	O

This	O
paper	O
presented	O
a	O
system	O
which	O
uses	O
sophisticated	O
learning	O
to	O
rank	O
method	O
with	O
semantic	O
features	O
to	O
obtain	O
promising	O
results	O
on	O
ranking	O
similar	O
questions	O
.	O
The	O
paper	O
shows	O
that	O
semantic	O
features	O
and	O
pairwise	O
learning	O
are	O
essential	O
components	O
to	O
the	O
system	O
by	O
ablation	O
tests	O
.	O
In	O
future	O
,	O
we	O
would	O
like	O
to	O
extend	O
our	O
neural	O
architecture	O
to	O
attention	O
based	O
models	O
which	O
have	O
shown	O
success	O
in	O
recent	O
times	O
.	O
We	O
also	O
plan	O
to	O
use	O
Triplet	B-MethodName
loss	I-MethodName
(	O
Hoffer	O
and	O
Ailon	O
,	O
2015	O
)	O
which	O
captures	O
ranking	O
task	O
in	O
better	O
way	O
.	O
Another	O
direction	O
is	O
to	O
use	O
state	O
-	O
of	O
-	O
art	O
listwise	O
learning	O
to	O
rank	O
methods	O
that	O
can	O
directly	O
optimize	O
MAP	B-DatasetName
.	O

Compressing	O
BERT	B-MethodName
:	O
Studying	O
the	O
Effects	O
of	O
Weight	O
Pruning	O
on	O
Transfer	B-TaskName
Learning	I-TaskName

BERT	B-MethodName
is	O
a	O
large	O
Transformer	B-MethodName
encoder	O
;	O
for	O
background	O
,	O
we	O
refer	O
readers	O
to	O
Vaswani	O
et	O
al	O
(	O
2017	O
)	O
or	O
one	O
of	O
these	O
excellent	O
tutorials	O
(	O
Alammar	O
,	O
2018	O
;	O
Klein	O
et	O
al	O
,	O
2017	O
)	O
.	O

Weight	O
Mean	O
Weight	O
STD	O
embeddings	O
word	B-TaskName
embeddings	I-TaskName
-	O
0.0282	O
0.042	O
layer	O
0	B-DatasetName
attention	O
output	O
FC	O
-	O
0.0000	O
0.029	O
layer	O
0	B-DatasetName
self	O
attn	O
key	O
0.0000	O
0.043	O
layer	O
0	B-DatasetName
self	O
attn	O
query	O
0.0000	O
0.043	O
layer	O
0	B-DatasetName
self	O
attn	O
value	O
-	O
0.0000	O
0	B-DatasetName
.	O

Question	B-TaskName
Generation	I-TaskName
for	O
Reading	B-TaskName
Comprehension	I-TaskName
Assessment	O
by	O
Modeling	O
How	O
and	O
What	O
to	O
Ask	O

Reading	O
is	O
integral	O
to	O
everyday	O
life	O
,	O
and	O
yet	O
learning	O
to	O
read	O
is	O
a	O
struggle	O
for	O
many	O
young	O
learners	O
.	O
During	O
lessons	O
,	O
teachers	O
can	O
use	O
comprehension	O
questions	O
to	O
increase	O
engagement	O
,	O
test	O
reading	O
skills	O
,	O
and	O
improve	O
retention	O
.	O
Historically	O
such	O
questions	O
were	O
written	O
by	O
skilled	O
teachers	O
,	O
but	O
recently	O
language	O
models	O
have	O
been	O
used	O
to	O
generate	O
comprehension	O
questions	O
.	O
However	O
,	O
many	O
existing	O
Question	B-TaskName
Generation	I-TaskName
(	O
QG	O
)	O
systems	O
focus	O
on	O
generating	O
literal	O
questions	O
from	O
the	O
text	O
,	O
and	O
have	O
no	O
way	O
to	O
control	O
the	O
type	O
of	O
the	O
generated	O
question	O
.	O
In	O
this	O
paper	O
,	O
we	O
study	O
QG	O
for	O
reading	B-TaskName
comprehension	I-TaskName
where	O
inferential	O
questions	O
are	O
critical	O
and	O
extractive	O
techniques	O
can	O
not	O
be	O
used	O
.	O
We	O
propose	O
a	O
two	O
-	O
step	O
model	O
(	O
HTA	O
-	O
WTA	O
)	O
that	O
takes	O
advantage	O
of	O
previous	O
datasets	O
,	O
and	O
can	O
generate	O
questions	O
for	O
a	O
specific	O
targeted	O
comprehension	O
skill	O
.	O
We	O
propose	O
a	O
new	O
reading	B-TaskName
comprehension	I-TaskName
dataset	O
that	O
contains	O
questions	O
annotated	O
with	O
story	O
-	O
based	O
reading	B-TaskName
comprehension	I-TaskName
skills	O
(	O
SBRCS	O
)	O
,	O
allowing	O
for	O
a	O
more	O
complete	O
reader	O
assessment	O
.	O
Across	O
several	O
experiments	O
,	O
our	O
results	O
show	O
that	O
HTA	O
-	O
WTA	O
outperforms	O
multiple	O
strong	O
baselines	O
on	O
this	O
new	O
dataset	O
.	O
We	O
show	O
that	O
the	O
HTA	O
-	O
WTA	O
model	O
tests	O
for	O
strong	O
SCRS	O
by	O
asking	O
deep	O
inferential	O
questions	O
.	O

Reading	O
is	O
an	O
invaluable	O
skill	O
,	O
and	O
is	O
core	O
to	O
communicating	O
in	O
our	O
digital	O
age	O
.	O
Reading	O
also	O
supports	O
other	O
forms	O
of	O
development	O
;	O
when	O
children	O
read	O
,	O
it	O
sharpens	O
their	O
memory	O
,	O
and	O
improves	O
social	O
skills	O
(	O
Halliday	O
,	O
1973	O
;	O
Mason	O
,	O
2017	O
)	O
.	O
Yet	O
,	O
statistics	O
show	O
that	O
one	O
out	O
of	O
five	O
children	O
in	O
the	O
U.S.	O
face	O
learning	O
difficulties	O
(	O
Shaywitz	O
,	O
2005	O
)	O
,	O
especially	O
in	O
reading	O
(	O
Cornoldi	O
and	O
Oakhill	O
,	O
2013	O
)	O
.	O
The	O
coronavirus	O
pandemic	O
beginning	O
in	O
2020	O
had	O
a	O
huge	O
impact	O
on	O
the	O
early	O
reading	O
skills	O
of	O
many	O
children	O
,	O
and	O
threatens	O
to	O
leave	O
a	O
lasting	O
impact	O
on	O
a	O
whole	O
generation	O
of	O
young	O
readers	O
(	O
Gupta	O
and	O
Jawanda	O
,	O
2020	O
)	O
.	O
The	O
pandemic	O
forced	O
many	O
children	O
to	O
learn	O
online	O
,	O
putting	O
in	O
sharp	O
relief	O
the	O
need	O
for	O
effective	O
online	O
education	O
platforms	O
.	O
In	O
particular	O
,	O
reading	O
games	O
have	O
become	O
popular	O
,	O
and	O
can	O
help	O
fill	O
the	O
gap	O
when	O
teachers	O
can	O
not	O
read	O
in	O
person	O
with	O
students	O
.	O
These	O
platforms	O
present	O
students	O
with	O
short	O
passages	O
and	O
associated	O
comprehension	O
questions	O
.	O
These	O
questions	O
are	O
key	O
to	O
assessing	O
a	O
reader	O
's	O
comprehension	O
of	O
a	O
passage	O
,	O
and	O
can	O
also	O
enhance	O
learning	O
(	O
Chua	O
et	O
al	O
,	O
2017	O
)	O
.	O
But	O
,	O
writing	O
diverse	O
and	O
engaging	O
comprehension	O
questions	O
is	O
a	O
nontrivial	O
task	O
.	O
Teachers	O
need	O
to	O
generate	O
new	O
comprehension	O
questions	O
whenever	O
they	O
incorporate	O
new	O
text	O
into	O
a	O
curriculum	O
.	O
New	O
text	O
helps	O
to	O
keep	O
material	O
fresh	O
and	O
topical	O
,	O
and	O
can	O
allow	O
teachers	O
to	O
customize	O
lessons	O
to	O
the	O
interests	O
of	O
a	O
particular	O
student	O
cohort	O
.	O
After	O
finding	O
such	O
custom	O
reading	O
material	O
,	O
teachers	O
must	O
write	O
new	O
comprehension	O
questions	O
to	O
evaluate	O
several	O
reading	O
aspects	O
of	O
comprehension	O
(	O
e.g.	O
understanding	O
complex	O
words	O
,	O
recalling	O
events	O
,	O
etc	O
.	O
)	O
.	O
Thus	O
,	O
to	O
improve	O
the	O
educational	O
process	O
,	O
and	O
lighten	O
the	O
load	O
on	O
teachers	O
,	O
we	O
need	O
tools	O
to	O
automate	O
Question	B-TaskName
Generation	I-TaskName
(	O
QG	O
)	O
:	O
the	O
task	O
of	O
writing	O
questions	O
for	O
a	O
given	O
passage	O
.	O
Generated	O
questions	O
can	O
be	O
either	O
inferential	O
or	O
literal	O
(	O
extractive	O
)	O
questions	O
.	O
Literal	O
questions	O
can	O
be	O
answered	O
using	O
only	O
information	O
stated	O
in	O
the	O
text	O
,	O
whereas	O
inferential	O
questions	O
require	O
additional	O
information	O
or	O
reasoning	O
.	O
Previous	O
works	O
focused	O
on	O
this	O
aspect	O
of	O
the	O
questions	O
in	O
reading	B-TaskName
comprehension	I-TaskName
and	O
discarded	O
the	O
comprehension	O
skills	O
(	O
e.g.	O
close	O
reading	O
,	O
predicting	O
,	O
figurative	O
language	O
,	O
etc	O
.	O
)	O
(	O
Murakhovs	O
'	O
ka	O
et	O
al	O
,	O
2021	O
)	O
.	O
We	O
take	O
inspiration	O
from	O
continual	B-TaskName
learning	I-TaskName
(	O
Parisi	O
et	O
al	O
,	O
2019	O
)	O
,	O
which	O
orders	O
a	O
set	O
of	O
learning	O
tasks	O
to	O
improve	O
model	O
performance	O
.	O
We	O
begin	O
by	O
training	O
a	O
model	O
on	O
the	O
general	O
task	O
of	O
QG	O
(	O
How	O
to	O
ask	O
:	O
HTA	O
)	O
,	O
and	O
follow	O
with	O
our	O
task	O
of	O
interest	O
:	O
generating	O
a	O
targeted	O
question	O
of	O
a	O
particular	O
type	O
(	O
What	O
to	O
ask	O
:	O
WTA	O
)	O
.	O
This	O
paper	O
focuses	O
on	O
the	O
generation	O
of	O
questions	O
for	O
story	O
-	O
based	O
reading	B-TaskName
comprehension	I-TaskName
skills	O
(	O
SBRCS	O
)	O
,	O
which	O
are	O
varied	O
and	O
cover	O
many	O
aspects	O
of	O
reading	B-TaskName
comprehension	I-TaskName
.	O
We	O
create	O
a	O
QG	O
dataset	O
for	O
SBRCS	O
1	O
.	O
Although	O
our	O
aim	O
in	O
creating	O
this	O
dataset	O
is	O
to	O
enrich	O
educational	O
applications	O
,	O
this	O
dataset	O
can	O
be	O
considered	O
as	O
a	O
source	O
for	O
general	O
QG	O
and	O
question	B-TaskName
answering	I-TaskName
(	O
QA	O
)	O
systems	O
in	O
NLP	O
.	O
Our	O
focus	O
here	O
is	O
to	O
build	O
a	O
question	O
generator	O
without	O
answer	O
supervision	O
as	O
the	O
case	O
in	O
a	O
reallife	O
application	O
,	O
where	O
a	O
story	O
only	O
will	O
be	O
given	O
as	O
input	O
.	O
This	O
is	O
a	O
challenging	O
task	O
,	O
as	O
many	O
different	O
questions	O
can	O
be	O
generated	O
from	O
a	O
story	O
when	O
there	O
is	O
no	O
answer	O
supervision	O
.	O
QG	O
with	O
answer	O
supervision	O
is	O
another	O
prevalent	O
research	O
line	O
in	O
the	O
literature	O
(	O
Zhao	O
et	O
al	O
,	O
2018	O
;	O
Chen	O
and	O
Xu	O
,	O
2021	O
)	O
.	O
The	O
contributions	O
in	O
this	O
work	O
are	O
as	O
follows	O
:	O
We	O
build	O
a	O
novel	O
QG	O
dataset	O
for	O
SBRCS	O
.	O
The	O
dataset	O
contains	O
advanced	O
reading	B-TaskName
comprehension	I-TaskName
skills	O
extracted	O
from	O
stories	O
.	O
We	O
propose	O
a	O
two	O
-	O
steps	O
method	O
to	O
generate	O
skill	O
-	O
related	O
questions	O
from	O
a	O
given	O
story	O
.	O
The	O
method	O
takes	O
advantage	O
of	O
previous	O
datasets	O
to	O
improve	O
generalizability	O
,	O
and	O
then	O
,	O
teaches	O
a	O
model	O
how	O
to	O
ask	O
predefined	O
styles	O
of	O
questions	O
.	O
We	O
demonstrate	O
the	O
efficiency	O
of	O
the	O
proposed	O
method	O
after	O
extensive	O
experiments	O
,	O
and	O
we	O
investigate	O
its	O
performance	O
in	O
a	O
few	B-TaskName
-	I-TaskName
shot	I-TaskName
learning	I-TaskName
setting	O
.	O
The	O
rest	O
of	O
the	O
paper	O
is	O
structured	O
as	O
follows	O
.	O
In	O
the	O
next	O
section	O
,	O
we	O
present	O
an	O
overview	O
of	O
the	O
literature	O
work	O
.	O
In	O
Section	O
3	O
,	O
we	O
describe	O
how	O
we	O
built	O
our	O
dataset	O
.	O
Section	O
4	O
describes	O
the	O
proposed	O
methodology	O
.	O
The	O
experimental	O
setting	O
is	O
presented	O
in	O
Section	O
5	O
.	O
The	O
results	O
and	O
the	O
analysis	O
are	O
presented	O
in	O
Section	O
6	O
.	O
Finally	O
,	O
we	O
draw	O
some	O
conclusions	O
and	O
possible	O
future	O
work	O
for	O
this	O
study	O
.	O

QG	O
has	O
progressed	O
rapidly	O
due	O
to	O
new	O
datasets	O
and	O
model	O
improvements	O
.	O
Many	O
different	O
QG	O
models	O
have	O
been	O
proposed	O
,	O
starting	O
for	O
simple	O
vanilla	O
Sequence	B-MethodName
to	I-MethodName
Sequence	I-MethodName
Neural	O
Networks	O
models	O
(	O
seq2seq	B-MethodName
)	O
Yuan	O
et	O
al	O
,	O
2017	O
)	O
to	O
the	O
more	O
recent	O
transformer	O
-	O
based	O
models	O
(	O
Dong	O
et	O
al	O
,	O
2019	O
;	O
Chan	O
and	O
Fan	O
,	O
2019	O
;	O
Varanasi	O
et	O
al	O
,	O
2020	O
;	O
Narayan	O
et	O
al	O
,	O
2020	O
;	O
Bao	O
et	O
al	O
,	O
2020	O
)	O
.	O
Some	O
QG	O
systems	O
use	O
manual	O
linguistic	O
features	O
in	O
their	O
models	O
(	O
Harrison	O
and	O
Walker	O
,	O
2018	O
;	O
Khullar	O
et	O
al	O
,	O
2018	O
;	O
Liu	O
et	O
al	O
,	O
2019a	O
;	O
Dhole	O
and	O
Manning	O
,	O
2020	O
)	O
,	O
some	O
consider	O
how	O
to	O
select	O
question	O
-	O
worthy	O
content	O
Li	O
et	O
al	O
,	O
2019	O
;	O
Scialom	O
et	O
al	O
,	O
2019	O
;	O
,	O
and	O
some	O
systems	O
explicitly	O
model	O
question	O
types	O
(	O
Duan	O
et	O
al	O
,	O
2017	O
;	O
Sun	O
et	O
al	O
,	O
2018	O
;	O
Kang	O
et	O
al	O
,	O
2019	O
;	O
.	O
The	O
last	O
group	O
focused	O
only	O
on	O
generating	O
questions	O
that	O
start	O
with	O
specific	O
interrogative	O
words	O
(	O
what	O
,	O
how	O
,	O
etc	O
.	O
)	O
.	O
QG	O
has	O
been	O
used	O
to	O
solve	O
many	O
real	O
-	O
life	O
problems	O
.	O
For	O
example	O
,	O
QG	O
in	O
conversational	O
dialogue	O
(	O
Gu	O
et	O
al	O
,	O
2021	O
;	O
Shen	O
et	O
al	O
,	O
2021	O
;	O
Liu	O
et	O
al	O
,	O
2021b	O
)	O
where	O
models	O
were	O
taught	O
to	O
ask	O
a	O
series	O
of	O
coherent	O
questions	O
grounded	O
in	O
a	O
QA	O
style	O
,	O
QG	O
based	O
on	O
visual	O
input	O
(	O
Mostafazadeh	O
et	O
al	O
,	O
2016	O
;	O
Shin	O
et	O
al	O
,	O
2018	O
;	O
Shukla	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
QG	O
for	O
deep	O
questions	O
such	O
as	O
mathematical	O
,	O
curiosity	O
-	O
driven	O
,	O
clinical	O
,	O
and	O
examinationtype	O
questions	O
(	O
Liyanage	O
and	O
Ranathunga	O
,	O
2019	O
;	O
Yue	O
et	O
al	O
,	O
2020	O
;	O
Jia	O
et	O
al	O
,	O
2021	O
)	O
.	O

Despite	O
the	O
recent	O
efforts	O
for	O
building	O
reading	B-TaskName
comprehension	I-TaskName
QA	O
datasets	O
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
none	O
of	O
the	O
available	O
datasets	O
explored	O
SBRCS	O
.	O
Questions	O
in	O
previous	O
datasets	O
ask	O
only	O
either	O
inferential	O
or	O
literal	O
questions	O
from	O
a	O
given	O
passage	O
/	O
story	O
.	O
Rogers	O
et	O
al	O
(	O
2020	O
)	O
,	O
developed	O
questions	O
with	O
general	O
reasoning	O
types	O
based	O
on	O
text	O
from	O
news	O
and	O
blogs	O
(	O
e.g.	O
Quora	O
)	O
.	O
We	O
believe	O
that	O
those	O
texts	O
sources	O
are	O
not	O
rich	O
enough	O
to	O
examine	O
reasoning	O
skills	O
.	O
Advanced	O
reasoning	O
skills	O
(	O
e.g.	O
Figurative	O
Language	O
)	O
are	O
usually	O
used	O
in	O
children	O
's	O
stories	O
to	O
assess	O
comprehension	O
skills	O
.	O
Additionally	O
,	O
we	O
use	O
a	O
extensive	O
set	O
of	O
reading	B-TaskName
comprehension	I-TaskName
skills	O
that	O
deeply	O
evaluates	O
the	O
abilities	O
of	O
the	O
readers	O
(	O
e.g.	O
imagination	O
skill	O
by	O
Visualizing	O
)	O
.	O
In	O
the	O
following	O
,	O
we	O
will	O
show	O
how	O
we	O
built	O
our	O
dataset	O
.	O
Table	O
1	O
gives	O
an	O
overview	O
of	O
the	O
dataset	O
.	O

Our	O
stories	O
(	O
passages	O
)	O
are	O
multi	O
-	O
genre	O
,	O
selfcontained	O
narratives	O
.	O
This	O
content	O
variety	O
leads	O
annotators	O
towards	O
asking	O
non	O
-	O
localized	O
questions	O
that	O
test	O
for	O
more	O
advanced	O
reading	B-TaskName
comprehension	I-TaskName
skills	O
.	O
The	O
stories	O
are	O
generated	O
using	O
several	O
resources	O
:	O
1	O
.	O
acquired	O
from	O
free	O
public	O
domain	O
content	O
(	O
Gutenberg	O
Project	O
2	O
)	O
,	O
2	O
.	O
partnerships	O
with	O
a	O
publishing	O
house	O
(	O
Blue	O
Moon	O
Publishers	O
3	O
)	O
and	O
an	O
educational	O
curriculum	O
development	O
foundation	O
(	O
The	O
Reimagined	O
Classroom	O
4	O
)	O
,	O
and	O
3	O
.	O
authored	O
by	O
two	O
professional	O
writers	O
,	O
(	O
the	O
majority	O
of	O
the	O
stories	O
are	O
from	O
this	O
last	O
category	O
)	O
.	O
To	O
provide	O
good	O
lexical	O
coverage	O
and	O
diverse	O
stories	O
,	O
we	O
choose	O
to	O
write	O
and	O
collect	O
stories	O
that	O
come	O
from	O
a	O
varied	O
set	O
of	O
genres	O
(	O
e.g.	O
science	O
,	O
social	O
studies	O
,	O
fantasy	O
,	O
fairy	O
tale	O
,	O
historical	O
fiction	O
,	O
horror	O
,	O
mystery	O
,	O
adventure	O
,	O
etc	O
.	O
)	O
.	O
In	O
total	O
,	O
we	O
collect	O
726	O
multi	O
-	O
domain	O
stories	O
.	O
The	O
stories	O
'	O
lengths	O
range	O
from	O
a	O
single	O
sentence	O
to	O
113	O
sentences	O
.	O

Given	O
the	O
fact	O
that	O
including	O
more	O
data	O
in	O
a	O
reading	B-TaskName
comprehension	I-TaskName
system	O
is	O
important	O
for	O
gen	O
-	O
eralization	O
(	O
Chung	O
et	O
al	O
,	O
2018	O
;	O
Talmor	O
and	O
Berant	O
,	O
2019	O
)	O
,	O
and	O
given	O
that	O
our	O
created	O
dataset	O
has	O
the	O
SBRCS	O
which	O
are	O
missed	O
in	O
previous	O
datasets	O
,	O
we	O
propose	O
a	O
two	O
-	O
steps	O
method	O
to	O
generate	O
skillrelated	O
questions	O
from	O
a	O
given	O
story	O
:	O
HTA	O
followed	O
by	O
WTA	O
.	O
HTA	O
teaches	O
the	O
model	O
the	O
typical	O
format	O
for	O
comprehension	O
questions	O
using	O
large	O
previously	O
released	O
datasets	O
.	O
We	O
use	O
two	O
well	O
-	O
known	O
datasets	O
,	O
SQuAD	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
and	O
Cos	O
-	O
mosQA	O
(	O
Huang	O
et	O
al	O
,	O
2019	O
)	O
.	O
In	O
Appendix	O
A.3	O
,	O
we	O
add	O
more	O
details	O
on	O
both	O
of	O
these	O
datasets	O
.	O
These	O
previous	O
datasets	O
are	O
not	O
annotated	O
with	O
the	O
question	O
types	O
outlined	O
in	O
Section	O
3.1	O
,	O
so	O
the	O
HTA	O
phase	O
allows	O
us	O
to	O
take	O
advantage	O
of	O
those	O
datasets	O
.	O
WTA	O
guides	O
the	O
model	O
to	O
generate	O
questions	O
to	O
test	O
the	O
specific	O
comprehension	O
skills	O
enumerated	O
in	O
Section	O
3.1	O
.	O
Thus	O
,	O
in	O
HTA	O
,	O
we	O
train	O
(	O
fine	O
-	O
tune	O
)	O
a	O
model	O
on	O
large	O
QG	O
datasets	O
,	O
and	O
then	O
,	O
we	O
further	O
train	O
the	O
model	O
to	O
teach	O
the	O
model	O
what	O
to	O
ask	O
(	O
WTA	O
)	O
.	O
For	O
the	O
generation	O
model	O
,	O
we	O
use	O
the	O
pre	O
-	O
trained	O
Text	O
-	O
to	O
-	O
Text	O
Transfer	O
Transformer	B-MethodName
T5	B-MethodName
(	O
Raffel	O
et	O
al	O
,	O
2020	O
)	O
,	O
which	O
closely	O
follows	O
the	O
encoder	O
-	O
decoder	O
architecture	O
of	O
the	O
transformer	O
model	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
T5	B-MethodName
is	O
a	O
SOTA	O
model	O
on	O
multiple	O
tasks	O
,	O
including	O
QA	O
.	O

Previous	O
works	O
showed	O
that	O
incorporating	O
more	O
data	O
when	O
training	O
a	O
reading	B-TaskName
comprehension	I-TaskName
model	O
improves	O
performance	O
and	O
generalizability	O
(	O
Chung	O
et	O
al	O
,	O
2018	O
;	O
Talmor	O
and	O
Berant	O
,	O
2019	O
)	O
.	O
However	O
,	O
we	O
can	O
not	O
incorporate	O
previously	O
released	O
datasets	O
with	O
our	O
new	O
one	O
,	O
as	O
they	O
do	O
not	O
include	O
compatible	O
question	O
skills	O
information	O
.	O
However	O
,	O
they	O
do	O
contain	O
many	O
well	O
-	O
formed	O
and	O
topical	O
questions	O
.	O
Thus	O
,	O
we	O
train	O
a	O
T5	B-MethodName
model	O
on	O
SQuAD	B-DatasetName
and	O
CosmosQA	B-DatasetName
datasets	O
to	O
teach	O
the	O
model	O
how	O
to	O
ask	O
questions	O
.	O
Previous	O
neural	O
question	B-TaskName
generation	I-TaskName
models	O
take	O
the	O
passage	O
as	O
input	O
,	O
along	O
with	O
the	O
answer	O
.	O
How	O
-	O
ever	O
,	O
encoders	O
can	O
pass	O
all	O
of	O
the	O
information	O
in	O
the	O
input	O
to	O
the	O
decoder	O
,	O
occasionally	O
causing	O
the	O
generated	O
question	O
to	O
contain	O
the	O
target	O
answer	O
.	O
Since	O
the	O
majority	O
of	O
the	O
questions	O
in	O
our	O
created	O
dataset	O
are	O
inferential	O
questions	O
,	O
the	O
answers	O
are	O
not	O
explicitly	O
given	O
in	O
the	O
passages	O
(	O
unlike	O
extractive	O
datasets	O
)	O
.	O
Thus	O
,	O
we	O
feed	O
the	O
stories	O
to	O
the	O
encoder	O
,	O
but	O
withhold	O
the	O
answers	O
.	O
Unlike	O
previous	O
systems	O
,	O
we	O
then	O
train	O
the	O
model	O
to	O
generate	O
the	O
questions	O
and	O
answers	O
.	O
We	O
propose	O
this	O
setting	O
to	O
generate	O
fewer	O
literal	O
questions	O
.	O
During	O
our	O
experiments	O
,	O
we	O
evaluated	O
the	O
effect	O
of	O
excluding	O
the	O
answers	O
,	O
and	O
we	O
found	O
them	O
useful	O
to	O
the	O
system	O
.	O
In	O
Figure	O
1	O
we	O
show	O
the	O
input	O
-	O
output	O
format	O
of	O
the	O
model	O
.	O
The	O
encoder	O
input	O
is	O
structured	O
as	O
<	O
STORY_TEXT	O
>	O
<	O
/s	O
>	O
,	O
where	O
<	O
/s	O
>	O
is	O
the	O
end	O
-	O
of	O
-	O
sentence	O
token	O
.	O
The	O
decoder	O
generates	O
multiple	O
questionanswer	O
pairs	O
as	O
<	O
QUESTION_TOKENS>1	O
<	O
as	O
>	O
<	O
ANSWER_TOKENS>1	O
<	O
sp	O
>	O
...	O
<	O
QUESTION_TOKENS	O
>	O
n	O
<	O
as	O
>	O
<	O
ANSWER_TOKENS	O
>	O
n	O
<	O
/s	O
>	O
,	O
where	O
<	O
as	O
>	O
separates	O
a	O
question	O
from	O
its	O
answer	O
,	O
and	O
<	O
sp	O
>	O
separates	O
a	O
question	O
-	O
answer	O
pair	O
from	O
another	O
.	O
The	O
model	O
can	O
generate	O
more	O
than	O
one	O
question	O
-	O
answer	O
pair	O
.	O
We	O
prepare	O
the	O
data	O
to	O
include	O
all	O
of	O
a	O
passage	O
's	O
question	O
-	O
answer	O
pairs	O
in	O
the	O
decoder	O
.	O
Some	O
passages	O
include	O
single	O
question	O
-	O
answer	O
pair	O
,	O
and	O
some	O
passages	O
have	O
up	O
to	O
fifteen	O
pairs	O
.	O

QG	O
models	O
take	O
a	O
passage	O
/	O
story	O
as	O
input	O
and	O
generate	O
a	O
question	O
.	O
The	O
type	O
of	O
generated	O
question	O
is	O
not	O
controlled	O
and	O
is	O
left	O
for	O
the	O
system	O
to	O
decide	O
it	O
.	O
Thus	O
,	O
the	O
generated	O
question	O
is	O
usually	O
an	O
undesired	O
question	O
.	O
Thus	O
,	O
in	O
order	O
to	O
control	O
the	O
style	O
of	O
the	O
generated	O
question	O
,	O
the	O
system	O
needs	O
an	O
indication	O
about	O
the	O
skill	O
that	O
the	O
system	O
is	O
expected	O
to	O
generate	O
a	O
question	O
for	O
.	O
proposed	O
a	O
way	O
to	O
control	O
the	O
style	O
of	O
the	O
generated	O
questions	O
(	O
e.g.	O
what	O
,	O
how	O
,	O
etc	O
.	O
)	O
.	O
The	O
authors	O
built	O
a	O
rulebased	O
information	O
extractor	O
to	O
sample	O
meaningful	O
inputs	O
from	O
a	O
given	O
text	O
,	O
and	O
then	O
learn	O
a	O
joint	O
distribution	O
of	O
<	O
answer	O
,	O
clue	O
,	O
question	O
style	O
>	O
before	O
asking	O
the	O
GPT2	O
model	O
(	O
Radford	O
et	O
al	O
,	O
2019	O
)	O
to	O
generate	O
questions	O
.	O
However	O
,	O
this	O
distribution	O
can	O
only	O
be	O
learned	O
using	O
an	O
extractive	O
dataset	O
(	O
e.g.	O
SQuAD	B-DatasetName
)	O
;	O
the	O
model	O
can	O
not	O
learn	O
to	O
generate	O
inferential	O
questions	O
.	O
To	O
control	O
the	O
skill	O
of	O
the	O
generated	O
question	O
,	O
we	O
use	O
a	O
specific	O
prompt	O
per	O
skill	O
,	O
by	O
defining	O
a	O
special	O
token	O
<	O
SKILL_NAME	O
>	O
corresponding	O
to	O
the	O
desired	O
target	O
skill	O
,	O
using	O
the	O
collected	O
dataset	O
.	O
This	O
helps	O
us	O
to	O
control	O
what	O
to	O
extract	O
from	O
the	O
pretrained	O
model	O
.	O
Thus	O
,	O
the	O
encoder	O
takes	O
as	O
input	O
<	O
SKILL_NAME	O
>	O
and	O
<	O
STORY_TEXT	O
>	O
,	O
where	O
<	O
SKILL_NAME	O
>	O
indicates	O
to	O
the	O
model	O
for	O
which	O
skill	O
the	O
question	O
should	O
be	O
generated	O
(	O
see	O
Figure	O
2	O
)	O
.	O
The	O
data	O
format	O
in	O
the	O
decoder	O
is	O
similar	O
to	O
the	O
one	O
in	O
the	O
HTA	O
step	O
,	O
but	O
here	O
the	O
model	O
generates	O
a	O
single	O
question	O
-	O
answer	O
pair	O
.	O
As	O
a	O
result	O
,	O
the	O
encoding	O
of	O
the	O
<	O
STORY_TEXT	O
>	O
will	O
be	O
based	O
on	O
the	O
given	O
<	O
SKILL_NAME	O
>	O
.	O
In	O
this	O
way	O
,	O
the	O
model	O
encodes	O
the	O
same	O
story	O
in	O
a	O
different	O
representation	O
when	O
a	O
different	O
<	O
SKILL_NAME	O
>	O
is	O
given	O
.	O
A	O
similar	O
technique	O
was	O
used	O
in	O
the	O
literature	O
to	O
include	O
persona	O
profiles	O
in	O
dialogue	O
agents	O
to	O
produce	O
more	O
coherent	O
and	O
meaningful	O
conversations	O
.	O

To	O
evaluate	O
the	O
performance	O
of	O
our	O
model	O
,	O
we	O
use	O
a	O
set	O
of	O
models	O
that	O
showed	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
several	O
datasets	O
.	O
We	O
obtain	O
the	O
results	O
of	O
those	O
models	O
by	O
running	O
their	O
published	O
GitHub	O
code	O
on	O
our	O
collected	O
dataset	O
.	O
For	O
all	O
of	O
the	O
following	O
baselines	O
,	O
we	O
use	O
SQuAD	B-DatasetName
,	O
CosmosQA	B-DatasetName
,	O
and	O
the	O
collected	O
dataset	O
for	O
training	O
and	O
we	O
test	O
on	O
the	O
test	O
part	O
of	O
the	O
collected	O
dataset	O
:	O
Vanilla	O
Seq2seq	B-MethodName
(	O
Sutskever	O
et	O
al	O
,	O
2014	O
)	O
:	O
a	O
basic	O
encoder	O
-	O
decoder	O
sequence	O
learning	O
system	O
for	O
machine	B-TaskName
translation	I-TaskName
.	O
This	O
model	O
takes	O
the	O
story	O
as	O
input	O
and	O
generates	O
a	O
question	O
.	O
NQG	O
-	O
Seq	O
:	O
another	O
Seq2seq	B-MethodName
that	O
implements	O
an	O
attention	O
layer	O
on	O
top	O
of	O
a	O
bidirectional	O
-	O
LSTM	B-MethodName
encoder	O
.	O
The	O
authors	O
use	O
two	O
encoders	O
,	O
one	O
to	O
encode	O
the	O
sentence	O
that	O
has	O
the	O
answer	O
,	O
and	O
another	O
to	O
encode	O
the	O
whole	O
document	O
.	O
The	O
model	O
then	O
is	O
trained	O
to	O
generate	O
questions	O
.	O
NQG	O
-	O
Max	O
(	O
Zhao	O
et	O
al	O
,	O
2018	O
)	O
8	O
:	O
a	O
QG	O
system	O
with	O
a	O
maxout	B-MethodName
pointer	O
mechanism	O
and	O
gated	O
self	O
-	O
attention	O
LSTM	B-MethodName
-	O
based	O
encoder	O
to	O
address	O
the	O
challenges	O
of	O
processing	O
long	O
text	O
input	O
.	O
This	O
model	O
takes	O
a	O
passage	O
and	O
an	O
answer	O
as	O
input	O
and	O
generate	O
a	O
question	O
.	O
The	O
answer	O
must	O
be	O
a	O
sub	O
span	O
of	O
the	O
passage	O
.	O
CGC	O
-	O
QG	O
(	O
Liu	O
et	O
al	O
,	O
2019a	O
)	O
:	O
a	O
Clue	O
Guided	O
Copy	O
network	O
for	O
Question	B-TaskName
Generation	I-TaskName
,	O
which	O
is	O
a	O
sequence	O
-	O
to	O
-	O
sequence	O
generative	O
model	O
with	O
a	O
copying	O
mechanism	O
that	O
takes	O
a	O
passage	O
and	O
an	O
answer	O
(	O
as	O
a	O
span	O
in	O
the	O
text	O
)	O
and	O
generate	O
the	O
question	O
.	O
The	O
text	O
representation	O
in	O
the	O
encoder	O
(	O
GRU	B-MethodName
network	O
)	O
is	O
represented	O
using	O
a	O
variety	O
of	O
features	O
such	O
as	O
GloVe	B-MethodName
vectors	O
,	O
POS	O
information	O
,	O
answer	O
position	O
,	O
clue	O
word	O
,	O
etc	O
.	O
AnswerQuest	O
(	O
Roemmele	O
et	O
al	O
,	O
2021	O
)	O
:	O
a	O
pipeline	O
model	O
that	O
uses	O
as	O
a	O
first	O
step	O
a	O
previous	O
model	O
to	O
retrieve	O
the	O
relevant	O
sentence	O
that	O
has	O
the	O
answer	O
from	O
a	O
document	O
.	O
And	O
then	O
,	O
the	O
sentence	O
is	O
fed	O
to	O
a	O
transformer	O
-	O
based	O
sequence	O
-	O
to	O
-	O
sequence	O
model	O
that	O
is	O
enhanced	O
with	O
a	O
copy	O
mechanism	O
.	O
One	O
-	O
Step	O
:	O
a	O
baseline	O
that	O
uses	O
T5	B-MethodName
model	O
trained	O
with	O
all	O
data	O
in	O
one	O
step	O
instead	O
of	O
having	O
separate	O
HTA	O
and	O
WTA	O
steps	O
.	O
Because	O
there	O
is	O
only	O
a	O
single	O
step	O
,	O
the	O
skill	O
name	O
is	O
not	O
included	O
in	O
the	O
encoder	O
's	O
input	O
.	O
T5	B-MethodName
-	O
WTA	O
:	O
the	O
WTA	O
model	O
trained	O
using	O
T5	B-MethodName
model	O
as	O
a	O
seed	O
model	O
.	O
The	O
HTA	O
training	O
step	O
is	O
not	O
used	O
here	O
.	O
We	O
use	O
this	O
baseline	O
to	O
evaluate	O
the	O
effect	O
of	O
training	O
WTA	O
using	O
HTA	O
.	O
For	O
all	O
of	O
the	O
previous	O
baselines	O
that	O
require	O
the	O
answer	O
to	O
be	O
a	O
sub	O
-	O
span	O
in	O
the	O
passage	O
,	O
we	O
use	O
the	O
semantic	O
text	B-TaskName
similarity	I-TaskName
method	O
that	O
was	O
proposed	O
in	O
(	O
Ghanem	O
et	O
al	O
,	O
2019	O
)	O
to	O
retrieve	O
the	O
most	O
similar	O
span	O
in	O
the	O
passage	O
.	O
The	O
method	O
extracts	O
several	O
ngrams	O
features	O
from	O
a	O
claim	O
and	O
text	O
spans	O
,	O
and	O
then	O
compute	O
cosine	O
similarity	O
to	O
get	O
the	O
most	O
similar	O
span	O
.	O
In	O
this	O
work	O
,	O
we	O
replace	O
the	O
ngrams	O
features	O
of	O
a	O
text	O
with	O
embeddings	O
extracted	O
from	O
RoBERTa	B-MethodName
model	O
(	O
Liu	O
et	O
al	O
,	O
2019b	O
)	O
.	O
This	O
process	O
has	O
been	O
done	O
on	O
the	O
inferential	O
questions	O
as	O
their	O
answers	O
are	O
not	O
clearly	O
given	O
in	O
the	O
text	O
.	O

In	O
this	O
paper	O
,	O
we	O
presented	O
a	O
new	O
reading	B-TaskName
comprehension	I-TaskName
dataset	O
to	O
assess	O
reading	O
skills	O
using	O
stories	O
.	O
Unlike	O
previous	O
datasets	O
that	O
focused	O
on	O
either	O
inferential	O
or	O
literal	O
questions	O
,	O
our	O
dataset	O
has	O
nine	O
different	O
SBRCS	O
,	O
each	O
contains	O
inferential	O
and	O
literal	O
questions	O
.	O
In	O
addition	O
to	O
that	O
,	O
we	O
proposed	O
HTA	O
-	O
WTA	O
model	O
which	O
uses	O
two	O
-	O
steps	O
fine	O
-	O
tuning	O
processes	O
to	O
take	O
advantage	O
of	O
previous	O
datasets	O
which	O
have	O
different	O
question	O
formats	O
,	O
and	O
to	O
learn	O
how	O
to	O
ask	O
skill	O
-	O
related	O
questions	O
.	O
We	O
evaluated	O
the	O
model	O
on	O
the	O
collected	O
dataset	O
and	O
compared	O
it	O
to	O
several	O
strong	O
baselines	O
.	O
Our	O
extensive	O
experiments	O
showed	O
the	O
effectiveness	O
of	O
the	O
model	O
.	O
Additionally	O
,	O
HTA	O
-	O
WTA	O
is	O
able	O
to	O
generate	O
high	O
quality	O
questions	O
when	O
only	O
10	O
%	O
of	O
the	O
dataset	O
is	O
used	O
(	O
∼240	O
instances	O
)	O
.	O
In	O
future	O
work	O
,	O
we	O
plan	O
to	O
extend	O
our	O
dataset	O
with	O
additional	O
skills	O
,	O
and	O
to	O
investigate	O
how	O
our	O
model	O
can	O
be	O
integrated	O
into	O
online	O
educational	O
platforms	O
.	O

Data	O
collection	O
and	O
Annotation	O
.	O
We	O
made	O
sure	O
that	O
the	O
sources	O
we	O
use	O
to	O
collect	O
stories	O
do	O
not	O
prevent	O
any	O
kind	O
of	O
copyright	O
infringement	O
.	O
The	O
content	O
distribution	O
licenses	O
were	O
checked	O
before	O
any	O
use	O
.	O
Additionally	O
,	O
we	O
manually	O
examined	O
the	O
stories	O
and	O
the	O
created	O
questions	O
to	O
ensure	O
there	O
are	O
no	O
privacy	O
or	O
ethical	O
concerns	O
,	O
e.g.	O
,	O
toxic	O
language	O
,	O
hate	B-DatasetName
speech	I-DatasetName
,	O
or	O
any	O
bias	O
against	O
underrepresented	O
groups	O
.	O
EyeRead	O
has	O
outreach	O
programs	O
in	O
place	O
to	O
recruit	O
writers	O
from	O
diverse	O
populations	O
,	O
incorporate	O
their	O
writing	O
into	O
the	O
online	O
system	O
,	O
and	O
properly	O
compensate	O
them	O
for	O
their	O
work	O
.	O
Writers	O
that	O
created	O
questions	O
earned	O
comparable	O
hourly	O
wages	O
to	O
those	O
earned	O
by	O
salaried	O
teachers	O
in	O
a	O
summer	O
program	O
.	O
We	O
estimated	O
the	O
amount	O
of	O
time	O
AMT	O
workers	O
need	O
to	O
finish	O
a	O
HIT	O
and	O
then	O
we	O
compensated	O
them	O
so	O
that	O
the	O
payment	O
rate	O
was	O
higher	O
than	O
the	O
local	O
living	O
wage	O
per	O
hour	O
.	O
Each	O
AMT	O
worker	O
received	O
$	O
0.41	O
USD	O
for	O
completing	O
one	O
HIT	O
,	O
which	O
we	O
estimated	O
would	O
take	O
1	O
minute	O
.	O
Bias	O
in	O
Language	O
Models	O
.	O
Recently	O
,	O
many	O
research	O
works	O
found	O
that	O
language	O
models	O
have	O
several	O
types	O
of	O
bias	O
,	O
e.g.	O
gender	O
,	O
race	O
,	O
religion	O
,	O
etc	O
.	O
,	O
and	O
this	O
is	O
due	O
to	O
the	O
data	O
used	O
to	O
train	O
them	O
(	O
Liang	O
et	O
al	O
,	O
2021	O
)	O
.	O
Removing	O
bias	O
from	O
language	O
models	O
completely	O
is	O
difficult	O
,	O
if	O
not	O
impossible	O
(	O
Gonen	O
and	O
Goldberg	O
,	O
2019	O
)	O
.	O
Thus	O
,	O
here	O
we	O
acknowledge	O
that	O
the	O
QG	O
model	O
we	O
trained	O
might	O
cause	O
ethical	O
concerns	O
,	O
e.g.	O
generating	O
biased	O
questions	O
about	O
stories	O
'	O
characters	O
.	O
EyeRead	O
is	O
keenly	O
aware	O
of	O
this	O
,	O
and	O
continues	O
to	O
monitor	O
both	O
teacher	O
and	O
modelgenerated	O
questions	O
before	O
they	O
are	O
integrated	O
into	O
their	O
system	O
.	O

Identifying	O
the	O
place	O
in	O
a	O
story	O
where	O
the	O
author	O
best	O
describes	O
or	O
explains	O
a	O
key	O
point	O
.	O
Also	O
,	O
it	O
includes	O
questions	O
to	O
identify	O
the	O
purpose	O
of	O
a	O
quote	O
or	O
a	O
sentence	O
.	O
This	O
skill	O
requires	O
advanced	O
reading	B-TaskName
comprehension	I-TaskName
ability	O
from	O
the	O
reader	O
since	O
its	O
answers	O
can	O
not	O
be	O
extracted	O
directly	O
from	O
the	O
story	O
text	O
,	O
where	O
inferential	O
skills	O
are	O
needed	O
.	O

There	O
are	O
three	O
major	O
approaches	O
within	O
literacy	O
education	O
to	O
which	O
teachers	O
or	O
schools	O
subscribe	O
:	O
the	O
whole	O
-	O
language	O
approach	O
(	O
Froese	O
,	O
1996	O
)	O
(	O
which	O
is	O
the	O
idea	O
that	O
if	O
teachers	O
simply	O
give	O
kids	O
books	O
,	O
kids	O
will	O
learn	O
how	O
to	O
read	O
)	O
,	O
the	O
structural	O
literacy	O
approach	O
(	O
Moats	O
,	O
2019	O
)	O
(	O
which	O
is	O
the	O
theory	O
that	O
letters	O
sounds	O
,	O
words	O
parts	O
,	O
and	O
grammar	O
rules	O
must	O
all	O
be	O
explicitly	O
taught	O
in	O
order	O
for	O
students	O
to	O
be	O
able	O
to	O
read	O
successfully	O
)	O
,	O
and	O
the	O
balanced	O
literacy	O
approach	O
(	O
Asselin	O
,	O
1999	O
)	O
(	O
which	O
basically	O
blends	O
the	O
aforementioned	O
two	O
theories	O
together	O
,	O
in	O
the	O
sense	O
that	O
students	O
read	O
authentic	O
literature	O
while	O
also	O
receiving	O
targeted	O
instruction	O
in	O
skills	O
or	O
strategies	O
)	O
.	O
In	O
this	O
work	O
,	O
we	O
chose	O
to	O
use	O
the	O
balanced	O
literacy	O
approach	O
as	O
it	O
benefits	O
from	O
both	O
approaches	O
and	O
as	O
it	O
is	O
the	O
newest	O
approach	O
.	O
At	O
the	O
beginning	O
,	O
we	O
reviewed	O
some	O
of	O
the	O
most	O
commonly	O
used	O
balanced	O
literacy	O
curricula	O
that	O
were	O
released	O
by	O
publishing	O
houses	O
and	O
universities	O
.	O
In	O
particular	O
,	O
we	O
devoted	O
a	O
lot	O
of	O
focus	O
to	O
the	O
Readers	O
and	O
Writers	O
Workshop	O
Model	O
9	O
which	O
is	O
developed	O
at	O
Columbia	O
University	O
Teachers	O
College	O
,	O
and	O
to	O
the	O
documentations	O
about	O
reading	O
levels	O
that	O
developed	O
by	O
Scholastic	O
publishing	O
house	O
10	O
.	O
The	O
Readers	O
and	O
Writers	O
Workshop	O
curricula	O
were	O
highly	O
instrumental	O
to	O
us	O
in	O
breaking	O
reading	B-TaskName
comprehension	I-TaskName
into	O
sub	O
-	O
skills	O
.	O
Also	O
,	O
it	O
is	O
one	O
of	O
the	O
most	O
commonly	O
used	O
and	O
referenced	O
curricula	O
among	O
teachers	O
.	O
We	O
reviewed	O
the	O
workshop	O
materials	O
to	O
create	O
a	O
list	O
of	O
all	O
of	O
the	O
skills	O
that	O
the	O
workshop	O
program	O
highlighted	O
.	O
Then	O
,	O
we	O
matched	O
those	O
against	O
what	O
was	O
offered	O
by	O
Scholastic	O
.	O
This	O
helped	O
us	O
create	O
our	O
primary	O
list	O
of	O
skills	O
.	O
In	O
this	O
study	O
,	O
we	O
are	O
experimenting	O
with	O
nine	O
skills	O
out	O
of	O
around	O
twenty	O
skills	O
.	O
In	O
this	O
phase	O
of	O
the	O
study	O
,	O
we	O
are	O
focusing	O
on	O
the	O
most	O
comprehensive	O
and	O
common	O
skills	O
.	O
In	O
the	O
future	O
,	O
we	O
will	O
expand	O
our	O
work	O
to	O
include	O
the	O
rest	O
of	O
the	O
skills	O
.	O

In	O
addition	O
to	O
the	O
collected	O
dataset	O
,	O
we	O
use	O
two	O
well	O
-	O
known	O
datasets	O
,	O
SQuAD	B-DatasetName
and	O
CosmosQA	B-DatasetName
.	O
We	O
choose	O
these	O
two	O
datasets	O
because	O
of	O
their	O
large	O
size	O
,	O
and	O
their	O
focus	O
on	O
literal	O
or	O
inferential	O
questions	O
.	O
SQuAD	B-DatasetName
A	O
reading	B-TaskName
comprehension	I-TaskName
dataset	O
,	O
consists	O
of	O
questions	O
created	O
by	O
crowdworkers	O
on	O
a	O
set	O
of	O
Wikipedia	O
articles	O
that	O
cover	O
a	O
large	O
set	O
of	O
topics	O
(	O
from	O
musical	O
celebrities	O
to	O
abstract	O
concepts	O
)	O
,	O
where	O
the	O
answer	O
to	O
every	O
question	O
is	O
a	O
span	O
from	O
the	O
corresponding	O
reading	O
passage	O
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
.	O
This	O
dataset	O
can	O
be	O
considered	O
as	O
an	O
extractive	O
QA	O
dataset	O
.	O
It	O
is	O
one	O
of	O
the	O
largest	O
QA	O
datasets	O
in	O
the	O
literature	O
.	O
In	O
this	O
work	O
,	O
we	O
use	O
SQuAD	B-DatasetName
2.0	O
version	O
with	O
discarding	O
the	O
questions	O
that	O
have	O
no	O
answers	O
.	O
The	O
size	O
of	O
the	O
dataset	O
is	O
100	O
K	O
paragraph	O
/	O
question	O
/	O
answer	O
triplets	O
.	O
CosmosQA	B-DatasetName
It	O
is	O
another	O
reading	B-TaskName
comprehension	I-TaskName
dataset	O
consisting	O
of	O
35.6	O
K	O
paragraph	O
/	O
question	O
pairs	O
that	O
require	O
commonsense	O
-	O
based	O
reading	B-TaskName
comprehension	I-TaskName
.	O
It	O
is	O
a	O
collection	O
of	O
people	O
's	O
everyday	O
narratives	O
,	O
and	O
it	O
asks	O
questions	O
about	O
the	O
likely	O
causes	O
of	O
events	O
that	O
require	O
reasoning	O
(	O
Huang	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
discard	O
questions	O
that	O
have	O
no	O
answers	O
in	O
this	O
dataset	O
,	O
resulting	O
in	O
28	O
K	O
paragraph	O
/	O
question	O
/	O
answer	O
triplets	O
.	O

In	O
the	O
following	O
,	O
we	O
elaborate	O
more	O
on	O
the	O
reading	B-TaskName
comprehension	I-TaskName
skills	O
:	O

We	O
present	O
ADVISER	O
1	O
-	O
an	O
open	O
-	O
source	O
,	O
multi	O
-	O
domain	O
dialog	O
system	O
toolkit	O
that	O
enables	O
the	O
development	O
of	O
multi	O
-	O
modal	O
(	O
incorporating	O
speech	O
,	O
text	O
and	O
vision	O
)	O
,	O
sociallyengaged	O
(	O
e.g.	O
emotion	B-TaskName
recognition	I-TaskName
,	O
engagement	O
level	O
prediction	O
and	O
backchanneling	O
)	O
conversational	O
agents	O
.	O
The	O
final	O
Python	O
-	O
based	O
implementation	O
of	O
our	O
toolkit	O
is	O
flexible	O
,	O
easy	O
to	O
use	O
,	O
and	O
easy	O
to	O
extend	O
not	O
only	O
for	O
technically	O
experienced	O
users	O
,	O
such	O
as	O
machine	O
learning	O
researchers	O
,	O
but	O
also	O
for	O
less	O
technically	O
experienced	O
users	O
,	O
such	O
as	O
linguists	O
or	O
cognitive	O
scientists	O
,	O
thereby	O
providing	O
a	O
flexible	O
platform	O
for	O
collaborative	O
research	O
.	O

Dialog	O
systems	O
or	O
chatbots	O
,	O
both	O
text	O
-	O
based	O
and	O
multi	O
-	O
modal	O
,	O
have	O
received	O
much	O
attention	O
in	O
recent	O
years	O
,	O
with	O
an	O
increasing	O
number	O
of	O
dialog	O
systems	O
in	O
both	O
industrial	O
contexts	O
such	O
as	O
Amazon	O
Alexa	O
,	O
Apple	O
Siri	O
,	O
Microsoft	O
Cortana	O
,	O
Google	B-DatasetName
Duplex	O
,	O
XiaoIce	O
(	O
Zhou	O
et	O
al	O
,	O
2018	O
)	O
and	O
Furhat	O
2	O
,	O
as	O
well	O
as	O
academia	O
such	O
as	O
MuMMER	O
(	O
Foster	O
et	O
al	O
,	O
2016	O
)	O
and	O
Alana	O
(	O
Curry	O
et	O
al	O
,	O
2018	O
)	O
.	O
However	O
,	O
open	O
-	O
source	O
toolkits	O
and	O
frameworks	O
for	O
developing	O
such	O
systems	O
are	O
rare	O
,	O
especially	O
for	O
developing	O
multi	O
-	O
modal	O
systems	O
comprised	O
of	O
speech	O
,	O
text	O
,	O
and	O
vision	O
.	O
Most	O
of	O
the	O
existing	O
toolkits	O
are	O
designed	O
for	O
developing	O
dialog	O
systems	O
focused	O
only	O
on	O
core	O
dialog	O
components	O
,	O
with	O
or	O
without	O
the	O
option	O
to	O
access	O
external	O
speech	O
processing	O
services	O
(	O
Bohus	O
and	O
Rudnicky	O
,	O
2009	O
;	O
Baumann	O
and	O
Schlangen	O
,	O
2012	O
;	O
Lison	O
and	O
Kennington	O
,	O
2016	O
;	O
Ultes	O
et	O
al	O
,	O
2017	O
;	O
Ortega	O
et	O
al	O
,	O
2019	O
;	O
Lee	O
et	O
al	O
,	O
2019	O
)	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
there	O
are	O
only	O
two	O
toolkits	O
,	O
proposed	O
in	O
(	O
Foster	O
et	O
al	O
,	O
2016	O
)	O
and	O
(	O
Bohus	O
et	O
al	O
,	O
2017	O
)	O
,	O
that	O
support	O
developing	O
dialog	O
agents	O
using	O
multi	O
-	O
modal	O
processing	O
and	O
social	O
signals	O
(	O
Wagner	O
et	O
al	O
,	O
2013	O
)	O
.	O
Both	O
provide	O
a	O
decent	O
platform	O
for	O
building	O
systems	O
,	O
however	O
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
the	O
former	O
is	O
not	O
open	O
-	O
source	O
,	O
and	O
the	O
latter	O
is	O
based	O
on	O
the	O
.NET	O
platform	O
,	O
which	O
could	O
be	O
less	O
convenient	O
for	O
non	O
-	O
technical	O
users	O
such	O
as	O
linguists	O
and	O
cognitive	O
scientists	O
,	O
who	O
play	O
an	O
important	O
role	O
in	O
dialog	O
research	O
.	O
In	O
this	O
paper	O
,	O
we	O
introduce	O
a	O
new	O
version	O
of	O
ADVISER	O
-	O
previously	O
a	O
text	O
-	O
based	O
,	O
multi	O
-	O
domain	O
dialog	O
system	O
toolkit	O
(	O
Ortega	O
et	O
al	O
,	O
2019	O
)	O
-	O
that	O
supports	O
multi	O
-	O
modal	O
dialogs	O
,	O
including	O
speech	O
,	O
text	O
and	O
vision	O
information	O
processing	O
.	O
This	O
provides	O
a	O
new	O
option	O
for	O
building	O
dialog	O
systems	O
that	O
is	O
open	O
-	O
source	O
and	O
Python	O
-	O
based	O
for	O
easy	O
use	O
and	O
fast	O
prototyping	O
.	O
The	O
toolkit	O
is	O
designed	O
in	O
such	O
a	O
way	O
that	O
it	O
is	O
modular	O
,	O
flexible	O
,	O
transparent	O
,	O
and	O
user	O
-	O
friendly	O
for	O
both	O
technically	O
experienced	O
and	O
less	O
technically	O
experienced	O
users	O
.	O
Furthermore	O
,	O
we	O
add	O
novel	O
features	O
to	O
AD	O
-	O
VISER	O
,	O
allowing	O
it	O
to	O
process	O
social	O
signals	O
and	O
to	O
incorporate	O
them	O
into	O
the	O
dialog	O
flow	O
.	O
We	O
believe	O
that	O
these	O
features	O
will	O
be	O
key	O
to	O
developing	O
humanlike	O
dialog	O
systems	O
because	O
it	O
is	O
well	O
-	O
known	O
that	O
social	O
signals	O
,	O
such	O
as	O
emotional	O
states	O
and	O
engagement	O
levels	O
,	O
play	O
an	O
important	O
role	O
in	O
human	O
computer	O
interaction	O
(	O
McTear	O
et	O
al	O
,	O
2016	O
)	O
.	O
However	O
in	O
contrast	O
to	O
open	O
-	O
ended	O
dialog	O
systems	O
(	O
Weizenbaum	O
,	O
1966	O
)	O
,	O
our	O
toolkit	O
focuses	O
on	O
task	O
-	O
oriented	O
applications	O
(	O
Bobrow	O
et	O
al	O
,	O
1977	O
)	O
,	O
such	O
as	O
searching	O
for	O
a	O
lecturer	O
at	O
the	O
university	O
(	O
Ortega	O
et	O
al	O
,	O
2019	O
)	O
.	O
The	O
purpose	O
we	O
envision	O
for	O
dialog	O
systems	O
developed	O
using	O
our	O
toolkit	O
is	O
not	O
the	O
same	O
as	O
the	O
objective	O
of	O
a	O
social	O
chatbot	B-TaskName
such	O
as	O
XiaoIce	O
(	O
Zhou	O
et	O
al	O
,	O
2018	O
)	O
.	O
Rather	O
than	O
promoting	O
"	O
an	O
AI	O
companion	O
with	O
an	O
emotional	O
connection	O
to	O
satisfy	O
the	O
human	O
need	O
for	O
communication	O
,	O
affection	O
,	O
and	O
social	O
belonging	O
"	O
(	O
Zhou	O
et	O
al	O
,	O
2018	O
)	O
,	O
ADVISER	O
helps	O
develop	O
dialog	O
systems	O
that	O
support	O
users	O
in	O
efficiently	O
fulfilling	O
concrete	O
goals	O
,	O
while	O
at	O
the	O
same	O
time	O
considering	O
social	O
signals	O
such	O
as	O
emotional	O
states	O
and	O
engagement	O
levels	O
so	O
as	O
to	O
remain	O
friendly	O
and	O
likeable	O
.	O

The	O
main	O
objective	O
of	O
this	O
work	O
is	O
to	O
develop	O
a	O
multi	O
-	O
domain	O
dialog	O
system	O
toolkit	O
that	O
allows	O
for	O
multi	O
-	O
modal	O
information	O
processing	O
and	O
that	O
provides	O
different	O
modules	O
for	O
extracting	O
social	O
signals	O
such	O
as	O
emotional	O
states	O
and	O
for	O
integrating	O
them	O
into	O
the	O
decision	B-TaskName
making	I-TaskName
process	O
.	O
The	O
toolkit	O
should	O
be	O
easy	O
to	O
use	O
and	O
extend	O
for	O
users	O
of	O
all	O
levels	O
of	O
technical	O
experience	O
,	O
providing	O
a	O
flexible	O
collaborative	O
research	O
platform	O
.	O

Multi	O
-	O
modality	O
The	O
main	O
challenges	O
in	O
handling	O
multi	O
-	O
modality	O
are	O
a	O
)	O
the	O
design	O
of	O
a	O
synchronization	O
infrastructure	O
and	O
b	O
)	O
the	O
large	O
range	O
of	O
different	O
latencies	O
from	O
different	O
modalities	O
.	O
To	O
alleviate	O
the	O
former	O
,	O
we	O
use	O
the	O
publisher	O
/	O
subscriber	O
software	O
pattern	O
presented	O
in	O
section	O
4	O
to	O
synchronize	O
signals	O
coming	O
from	O
different	O
sources	O
.	O
This	O
software	O
pattern	O
also	O
allows	O
for	O
services	O
to	O
run	O
in	O
a	O
distributed	O
manner	O
.	O
By	O
assigning	O
computationally	O
heavy	O
tasks	O
such	O
as	O
speech	B-TaskName
recognition	I-TaskName
and	O
speech	B-TaskName
synthesis	I-TaskName
to	O
a	O
more	O
powerful	O
computing	O
node	O
,	O
it	O
is	O
possible	O
to	O
reduce	O
differences	O
in	O
latency	O
when	O
processing	O
different	O
modalities	O
,	O
therefore	O
achieving	O
more	O
natural	O
interactions	O
.	O
Socially	O
-	O
Engaged	O
Systems	O
Determining	O
the	O
ideal	O
scope	O
of	O
a	O
socially	O
-	O
engaged	O
dialog	O
system	O
is	O
a	O
complex	O
issue	O
,	O
that	O
is	O
which	O
information	O
should	O
be	O
extracted	O
from	O
users	O
and	O
how	O
the	O
system	O
can	O
best	O
react	O
to	O
these	O
signals	O
.	O
Here	O
we	O
focus	O
on	O
two	O
major	O
social	O
signals	O
:	O
emotional	O
states	O
and	O
engagement	O
levels	O
(	O
see	O
section	O
3.1	O
)	O
,	O
and	O
maintain	O
an	O
internal	O
user	O
state	O
to	O
track	O
them	O
over	O
the	O
course	O
of	O
a	O
dialog	O
.	O
Note	O
that	O
the	O
toolkit	O
is	O
designed	O
in	O
such	O
a	O
way	O
that	O
any	O
social	O
signal	O
could	O
be	O
extracted	O
and	O
leveraged	O
in	O
the	O
dialog	O
manager	O
.	O
In	O
order	O
to	O
react	O
to	O
social	O
signals	O
extracted	O
from	O
the	O
user	O
,	O
we	O
provide	O
an	O
initial	O
affective	O
policy	O
module	O
(	O
see	O
section	O
3.5	O
)	O
and	O
an	O
initial	O
affective	O
NLG	O
module	O
(	O
see	O
section	O
3.7	O
)	O
,	O
which	O
could	O
be	O
easily	O
extended	O
to	O
more	O
sophisticated	O
behavior	O
.	O
Furthermore	O
,	O
we	O
provide	O
a	O
backchanneling	O
module	O
that	O
enables	O
the	O
dialog	O
system	O
to	O
give	O
feedback	O
to	O
users	O
during	O
conversations	O
.	O
Utilizing	O
these	O
features	O
could	O
lead	O
to	O
increased	O
trust	O
and	O
enhance	O
the	O
impression	O
of	O
an	O
empathetic	O
system	O
.	O

We	O
present	O
the	O
three	O
modules	O
of	O
ADVISER	O
for	O
processing	O
social	O
signals	O
:	O
(	O
a	O
)	O
emotion	B-TaskName
recognition	I-TaskName
,	O
(	O
b	O
)	O
engagement	O
level	O
prediction	O
,	O
and	O
(	O
c	O
)	O
backchanneling	O
.	O
Figure	O
1	O
illustrates	O
an	O
example	O
of	O
our	O
system	O
tracking	O
emotion	B-DatasetName
states	O
and	O
engagement	O
levels	O
.	O

For	O
recognizing	O
a	O
user	O
's	O
emotional	O
state	O
,	O
all	O
three	O
available	O
modalities	O
-	O
text	O
,	O
audio	O
,	O
and	O
vision	O
-	O
can	O
potentially	O
be	O
exploited	O
,	O
as	O
they	O
can	O
deliver	O
complementary	O
information	O
(	O
Zeng	O
et	O
al	O
,	O
2009	O
)	O
.	O
Therefore	O
,	O
the	O
emotion	B-TaskName
recognition	I-TaskName
module	O
can	O
subscribe	O
to	O
the	O
particular	O
input	O
streams	O
of	O
interest	O
(	O
see	O
section	O
4	O
for	O
details	O
)	O
and	O
apply	O
emotion	B-DatasetName
prediction	O
either	O
in	O
a	O
time	O
-	O
continuous	O
fashion	O
or	O
discretely	O
per	O
turn	O
.	O
In	O
our	O
example	O
implementation	O
in	O
the	O
toolkit	O
,	O
we	O
integrate	O
speech	B-TaskName
emotion	I-TaskName
recognition	I-TaskName
,	O
i.e.	O
using	O
the	O
acoustic	O
signal	O
as	O
features	O
.	O
Based	O
on	O
the	O
work	O
presented	O
in	O
(	O
Neumann	O
and	O
Vu	O
,	O
2017	O
)	O
we	O
use	O
log	O
Mel	O
filterbank	O
coefficients	O
as	O
input	O
to	O
convolutional	O
neural	O
networks	O
(	O
CNNs	O
)	O
.	O
For	O
the	O
sake	O
of	O
modularity	O
,	O
three	O
separate	O
models	O
are	O
employed	O
for	O
predicting	O
different	O
types	O
of	O
labels	O
:	O
(	O
a	O
)	O
basic	O
emotions	O
{	O
angry	O
,	O
happy	O
,	O
neutral	O
,	O
sad	O
}	O
,	O
(	O
b	O
)	O
arousal	O
levels	O
{	O
low	O
,	O
medium	O
,	O
high	O
}	O
,	O
and	O
(	O
c	O
)	O
valence	O
levels	O
{	O
negative	O
,	O
neutral	O
,	O
positive	O
}	O
.	O
The	O
models	O
are	O
trained	O
on	O
the	O
IEMOCAP	B-DatasetName
dataset	O
(	O
Busso	O
et	O
al	O
,	O
2008	O
)	O
.	O
The	O
output	O
of	O
the	O
emotion	B-TaskName
recognition	I-TaskName
module	O
consists	O
of	O
three	O
predictions	O
per	O
user	O
turn	O
,	O
which	O
can	O
then	O
be	O
used	O
by	O
the	O
user	O
state	O
tracker	O
(	O
see	O
section	O
3.4	O
)	O
.	O
For	O
future	O
releases	O
,	O
we	O
plan	O
to	O
incorporate	O
multiple	O
training	O
datasets	O
as	O
well	O
as	O
visual	O
features	O
.	O
Engagement	O
Level	O
Prediction	O
User	O
engagement	O
is	O
closely	O
related	O
to	O
states	O
such	O
as	O
boredom	O
and	O
level	O
of	O
interest	O
,	O
with	O
implications	O
for	O
user	O
satisfaction	O
and	O
task	O
success	O
(	O
Forbes	O
-	O
Riley	O
et	O
al	O
,	O
2012	O
;	O
Schuller	O
et	O
al	O
,	O
2009	O
)	O
.	O
In	O
ADVISER	O
,	O
we	O
assume	O
that	O
eye	O
activity	O
serves	O
as	O
an	O
indicator	O
of	O
various	O
mental	O
states	O
(	O
Schuller	O
et	O
al	O
,	O
2009	O
;	O
Niu	O
et	O
al	O
,	O
2018	O
)	O
and	O
implement	O
a	O
gaze	O
tracker	O
that	O
monitors	O
the	O
user	O
's	O
direction	O
of	O
focus	O
via	O
webcam	O
.	O
Using	O
OpenFace	O
2.2.0	O
,	O
a	O
toolkit	O
for	O
facial	O
behavior	O
analysis	O
(	O
Baltrusaitis	O
et	O
al	O
,	O
2018	O
)	O
,	O
we	O
extract	O
the	O
features	O
gaze	O
angle	O
x	O
and	O
gaze	O
angle	O
y	O
,	O
which	O
capture	O
left	O
-	O
right	O
and	O
up	O
-	O
down	O
eye	O
movement	O
,	O
for	O
each	O
frame	O
and	O
compute	O
the	O
deviation	O
from	O
the	O
central	O
point	O
of	O
the	O
screen	O
.	O
If	O
the	O
deviation	O
exceeds	O
a	O
certain	O
threshold	O
for	O
a	O
certain	O
number	O
of	O
seconds	O
,	O
the	O
user	O
is	O
assumed	O
to	O
look	O
away	O
from	O
the	O
screen	O
,	O
thereby	O
disengaging	O
.	O
Thus	O
,	O
the	O
output	O
of	O
our	O
engagement	O
level	O
prediction	O
module	O
is	O
the	O
binary	O
decision	O
{	O
looking	O
,	O
not	O
looking	O
}	O
.	O
Both	O
the	O
spatial	O
and	O
temporal	O
sensitivity	O
can	O
be	O
adjusted	O
,	O
such	O
that	O
developers	O
have	O
the	O
option	O
to	O
decide	O
how	O
far	O
and	O
how	O
long	O
the	O
user	O
's	O
gaze	O
can	O
stray	O
from	O
the	O
central	O
point	O
until	O
they	O
are	O
considered	O
to	O
be	O
disengaged	O
.	O
In	O
an	O
adaptive	O
system	O
,	O
this	O
information	O
could	O
be	O
used	O
to	O
select	O
re	O
-	O
engagement	O
strategies	O
,	O
e.g.	O
using	O
an	O
affective	O
template	O
(	O
see	O
section	O
3.7	O
)	O
.	O
Backchanneling	O
In	O
a	O
conversation	O
,	O
a	O
backchannel	O
(	O
BC	O
)	O
is	O
a	O
soft	O
interjection	O
from	O
the	O
listener	O
to	O
the	O
speaker	O
,	O
with	O
the	O
purpose	O
of	O
signaling	O
acknowledgment	O
or	O
reacting	O
to	O
what	O
was	O
just	O
uttered	O
.	O
Backchannels	O
contribute	O
to	O
a	O
successful	O
conversation	O
flow	O
(	O
Clark	O
and	O
Krych	O
,	O
2004	O
)	O
.	O
Therefore	O
,	O
we	O
add	O
an	O
acoustic	O
backchannel	O
module	O
to	O
create	O
a	O
more	O
human	O
-	O
like	O
dialog	O
experience	O
.	O
For	O
backchannel	O
prediction	O
,	O
we	O
extract	O
13	O
Mel	O
-	O
frequency	O
-	O
cepstral	O
coefficients	O
from	O
the	O
user	O
's	O
speech	O
signal	O
,	O
which	O
form	O
the	O
input	O
to	O
the	O
convolutional	O
neural	O
network	O
based	O
on	O
Ortega	O
et	O
al	O
(	O
2020	O
)	O
.	O
The	O
model	O
assigns	O
one	O
of	O
three	O
categories	O
from	O
the	O
proactive	O
backchanneling	O
theory	O
(	O
Goodwin	O
,	O
1986	O
)	O
to	O
each	O
user	O
utterance	O
{	O
no	O
-	O
backchannel	O
,	O
backchannel	O
-	O
continuer	O
and	O
backchannel	O
-	O
assessment	O
}	O
.	O
The	O
predicted	O
category	O
is	O
used	O
to	O
add	O
the	O
backchannel	O
realization	O
,	O
such	O
as	O
Right	O
or	O
Uh	O
-	O
huh	O
,	O
to	O
the	O
next	O
system	O
response	O
.	O

Automatic	B-TaskName
Speech	I-TaskName
Recognition	I-TaskName
(	O
ASR	O
)	O
The	O
speech	B-TaskName
recognition	I-TaskName
module	O
receives	O
a	O
speech	O
signal	O
as	O
input	O
,	O
which	O
can	O
come	O
from	O
an	O
internal	O
or	O
external	O
microphone	O
,	O
and	O
outputs	O
decoded	O
text	O
.	O
The	O
specific	O
realization	O
of	O
ASR	O
can	O
be	O
interchanged	O
or	O
adapted	O
,	O
for	O
example	O
for	O
new	O
languages	O
or	O
different	O
ASR	O
methods	O
.	O
We	O
provide	O
an	O
end	O
-	O
to	O
-	O
end	O
ASR	O
model	O
for	O
English	O
based	O
on	O
the	O
Transformer	B-MethodName
neural	O
network	O
architecture	O
.	O
We	O
use	O
the	O
end	O
-	O
to	O
-	O
end	O
speech	O
processing	O
toolkit	O
ESPnet	B-MethodName
(	O
Watanabe	O
et	O
al	O
,	O
2018	O
)	O
and	O
the	O
IMS	O
-	O
speech	O
English	O
multi	O
-	O
dataset	O
recipe	O
(	O
Denisov	O
and	O
Vu	O
,	O
2019	O
)	O
,	O
updated	O
to	O
match	O
the	O
LibriSpeech	B-DatasetName
Transformer	B-MethodName
-	O
based	O
system	O
in	O
ESPnet	B-MethodName
(	O
Karita	O
et	O
al	O
,	O
2019	O
)	O
and	O
to	O
include	O
more	O
training	O
data	O
.	O
Training	O
data	O
comprises	O
the	O
LibriSpeech	B-DatasetName
,	O
Switchboard	O
,	O
TED	B-DatasetName
-	I-DatasetName
LIUM	I-DatasetName
3	I-DatasetName
,	O
AMI	O
,	O
WSJ	O
,	O
Common	B-DatasetName
Voice	I-DatasetName
3	O
,	O
SWC	O
,	O
VoxForge	B-DatasetName
and	O
M	O
-	O
AILABS	O
datasets	O
with	O
a	O
total	O
amount	O
of	O
3249	O
hours	O
.	O
As	O
input	O
features	O
,	O
80	O
-	O
dimensional	O
log	O
Mel	O
filterbank	O
coefficients	O
are	O
used	O
.	O
Output	O
of	O
the	O
ASR	O
model	O
is	O
a	O
sequence	O
of	O
subword	O
units	O
,	O
which	O
include	O
single	O
characters	O
as	O
well	O
as	O
combinations	O
of	O
several	O
characters	O
,	O
making	O
the	O
model	O
lexicon	O
independent	O
.	O
Speech	B-TaskName
Synthesis	I-TaskName
For	O
ADVISER	O
's	O
voice	O
output	O
,	O
we	O
use	O
the	O
ESPnet	B-MethodName
-	O
TTS	O
toolkit	O
,	O
which	O
is	O
an	O
extension	O
of	O
the	O
ESPnet	B-MethodName
toolkit	O
mentioned	O
above	O
.	O
We	O
use	O
FastSpeech	O
as	O
the	O
synthesis	O
model	O
speeding	O
up	O
mel	O
-	O
spectrogram	O
generation	O
by	O
a	O
factor	O
of	O
270	O
and	O
voice	O
generation	O
by	O
a	O
factor	O
of	O
38	O
compared	O
to	O
autoregressive	O
Transformer	B-MethodName
TTS	O
(	O
Ren	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
use	O
a	O
Parallel	O
Wave	O
-	O
GAN	B-MethodName
(	O
Yamamoto	O
et	O
al	O
,	O
2020	O
)	O
to	O
generate	O
waveforms	O
that	O
is	O
computationally	O
efficient	O
and	O
achieves	O
a	O
high	O
mean	O
opinion	O
score	O
of	O
4.16	O
.	O
The	O
FastSpeech	O
and	O
WaveGAN	B-MethodName
models	O
were	O
trained	O
with	O
24	O
hours	O
of	O
the	O
LJSpeech	B-DatasetName
dataset	O
from	O
a	O
single	O
speaker	O
(	O
Ito	O
,	O
2017	O
)	O
and	O
are	O
capable	O
of	O
generating	O
voice	O
output	O
in	O
real	O
-	O
time	O
when	O
using	O
a	O
GPU	O
.	O
The	O
synthesis	O
can	O
run	O
on	O
any	O
device	O
in	O
a	O
distributed	O
system	O
.	O
Additionally	O
,	O
we	O
optimize	O
the	O
synthesizer	O
for	O
abbreviations	O
,	O
such	O
as	O
Prof.	O
,	O
Univ	O
.	O
,	O
IMS	O
,	O
NLP	O
,	O
ECTS	O
and	O
PhD	O
,	O
as	O
well	O
as	O
for	O
German	O
proper	O
names	O
,	O
such	O
as	O
street	O
names	O
.	O
These	O
optimizations	O
can	O
be	O
easily	O
extended	O
.	O
Turn	O
Taking	O
To	O
make	O
interacting	O
with	O
the	O
system	O
more	O
natural	O
,	O
we	O
use	O
a	O
naive	O
end	O
-	O
of	O
-	O
utterance	O
detection	O
.	O
Users	O
indicate	O
the	O
start	O
of	O
their	O
turn	O
by	O
pressing	O
a	O
hotkey	O
,	O
so	O
they	O
can	O
choose	O
to	O
pause	O
the	O
interaction	O
.	O
The	O
highest	O
absolute	O
peak	O
of	O
each	O
recording	O
chunk	O
is	O
then	O
compared	O
with	O
a	O
predefined	O
threshold	O
.	O
If	O
a	O
certain	O
number	O
of	O
sequential	O
chunks	O
do	O
not	O
peak	O
above	O
the	O
threshold	O
,	O
the	O
recording	O
stops	O
.	O
We	O
are	O
currenlty	O
in	O
the	O
process	O
of	O
planning	O
more	O
sophisticated	O
turn	O
taking	O
models	O
,	O
such	O
as	O
Skantze	O
et	O
al	O
(	O
2015	O
)	O
.	O

The	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
(	O
NLU	O
)	O
unit	O
parses	O
the	O
textual	O
user	O
input	O
(	O
De	O
Mori	O
et	O
al	O
,	O
2008	O
)	O
-	O
or	O
the	O
output	O
from	O
the	O
speech	B-TaskName
recognition	I-TaskName
systemand	O
extracts	O
the	O
user	O
action	O
type	O
,	O
generally	O
referred	O
to	O
as	O
intent	O
in	O
goal	B-TaskName
-	I-TaskName
oriented	I-TaskName
dialog	I-TaskName
systems	O
(	O
e.g.	O
Inform	O
and	O
Request	O
)	O
,	O
as	O
well	O
as	O
the	O
corresponding	O
slots	O
and	O
values	O
.	O
The	O
domain	O
-	O
independent	O
,	O
rulebased	O
NLU	O
presented	O
in	O
Ortega	O
et	O
al	O
(	O
2019	O
)	O
is	O
integrated	O
into	O
ADVISER	O
and	O
adapted	O
to	O
the	O
new	O
domains	O
presented	O
in	O
section	O
5	O
.	O

Belief	O
State	O
Tracking	O
(	O
BST	O
)	O
:	O
The	O
BST	O
tracks	O
the	O
history	O
of	O
user	O
informs	O
and	O
the	O
user	O
action	O
types	O
,	O
requests	O
,	O
with	O
one	O
BST	O
entry	O
per	O
turn	O
.	O
This	O
information	O
is	O
stored	O
in	O
a	O
dictionary	O
structure	O
that	O
is	O
built	O
up	O
,	O
as	O
the	O
user	O
provides	O
more	O
details	O
and	O
the	O
system	O
has	O
a	O
better	O
understanding	O
of	O
user	O
intent	O
.	O
User	O
State	O
Tracking	O
(	O
UST	O
)	O
:	O
Similar	O
to	O
the	O
BST	O
,	O
the	O
UST	O
tracks	O
the	O
history	O
of	O
the	O
user	O
's	O
state	O
over	O
the	O
course	O
of	O
a	O
dialog	O
,	O
with	O
one	O
entry	O
per	O
turn	O
.	O
In	O
the	O
current	O
implementation	O
,	O
the	O
user	O
state	O
consists	O
of	O
the	O
user	O
's	O
engagement	O
level	O
,	O
valence	O
,	O
arousal	O
,	O
and	O
emotion	B-DatasetName
category	O
(	O
details	O
in	O
section	O
3.1	O
)	O
.	O

Policies	O
To	O
determine	O
the	O
correct	O
system	O
action	O
,	O
we	O
provide	O
three	O
types	O
of	O
policy	O
services	O
:	O
a	O
handcrafted	O
and	O
a	O
reinforcement	O
learning	O
policy	O
for	O
finding	O
entities	O
from	O
a	O
database	O
(	O
Ortega	O
et	O
al	O
,	O
2019	O
)	O
,	O
as	O
well	O
as	O
a	O
handcrafted	O
policy	O
for	O
looking	O
up	O
information	O
through	O
an	O
API	O
call	O
.	O
Both	O
handcrafted	O
policies	O
use	O
a	O
series	O
of	O
rules	O
to	O
help	O
the	O
user	O
find	O
a	O
single	O
entity	O
or	O
,	O
once	O
an	O
entity	O
has	O
been	O
found	O
(	O
or	O
directly	O
provided	O
by	O
the	O
user	O
)	O
,	O
find	O
information	O
about	O
that	O
entity	O
.	O
The	O
reinforcement	O
learning	O
(	O
RL	O
)	O
policy	O
's	O
action	O
-	O
value	O
function	O
is	O
approximated	O
by	O
a	O
neural	O
network	O
which	O
outputs	O
a	O
value	O
for	O
each	O
possible	O
system	O
action	O
,	O
given	O
the	O
vectorized	O
representation	O
of	O
a	O
turn	O
's	O
belief	O
state	O
as	O
input	O
.	O
The	O
neural	O
network	O
is	O
constructed	O
as	O
proposed	O
in	O
following	O
a	O
duelling	O
architecture	O
(	O
Wang	O
et	O
al	O
,	O
2016	O
)	O
.	O
It	O
consists	O
of	O
two	O
separate	O
calculation	O
streams	O
,	O
each	O
with	O
its	O
own	O
layers	O
,	O
where	O
the	O
final	O
layer	O
yields	O
the	O
action	O
-	O
value	O
function	O
.	O
For	O
off	O
-	O
policy	O
batch	O
-	O
training	O
,	O
we	O
make	O
use	O
of	O
prioritized	B-MethodName
experience	I-MethodName
replay	I-MethodName
(	O
Schaul	O
et	O
al	O
,	O
2015	O
)	O
.	O
Affective	O
Policy	O
In	O
addition	O
,	O
we	O
have	O
also	O
implemented	O
a	O
rule	O
-	O
based	O
affective	O
policy	O
service	O
that	O
can	O
be	O
used	O
to	O
determine	O
the	O
system	O
's	O
emotional	O
response	O
.	O
As	O
this	O
policy	O
is	O
domain	O
-	O
agnostic	O
,	O
predicting	O
the	O
next	O
system	O
emotion	B-DatasetName
output	O
rather	O
than	O
the	O
next	O
system	O
action	O
,	O
it	O
can	O
be	O
used	O
alongside	O
any	O
of	O
the	O
previously	O
mentioned	O
policies	O
.	O
User	O
Simulator	O
To	O
support	O
automatic	O
evaluation	O
and	O
to	O
train	O
the	O
RL	O
policy	O
,	O
we	O
provide	O
a	O
user	O
simulator	O
service	O
outputting	O
at	O
the	O
user	O
acts	O
level	O
.	O
As	O
we	O
are	O
concerned	O
with	O
task	O
-	O
oriented	O
dialogs	O
here	O
,	O
the	O
user	O
simulator	O
has	O
an	O
agenda	O
-	O
based	O
(	O
Schatzmann	O
et	O
al	O
,	O
2007	O
)	O
architecture	O
and	O
is	O
randomly	O
assigned	O
a	O
goal	O
at	O
the	O
beginning	O
of	O
the	O
dialog	O
.	O
Each	O
turn	O
,	O
it	O
then	O
works	O
to	O
first	O
respond	O
to	O
the	O
system	O
utterance	O
,	O
and	O
then	O
after	O
to	O
fulfill	O
its	O
own	O
goal	O
.	O
When	O
the	O
system	O
utterance	O
also	O
works	O
toward	O
fulfilling	O
the	O
user	O
goal	O
,	O
the	O
RL	O
policy	O
is	O
rewarded	O
by	O
achieving	O
a	O
shorter	O
total	O
dialog	O
turn	O
count	O
(	O
Ortega	O
et	O
al	O
,	O
2019	O
)	O
.	O

In	O
the	O
NLG	O
service	O
,	O
the	O
semantic	O
representation	O
of	O
the	O
system	O
act	O
is	O
transformed	O
into	O
natural	O
language	O
.	O
ADVISER	O
currently	O
uses	O
a	O
template	O
-	O
based	O
approach	O
to	O
NLG	O
in	O
which	O
each	O
possible	O
system	O
act	O
is	O
mapped	O
to	O
exactly	O
one	O
utterance	O
.	O
A	O
special	O
syntax	O
using	O
placeholders	O
reduces	O
the	O
number	O
of	O
templates	O
needed	O
and	O
accounts	O
for	O
correct	O
morphological	O
inflections	O
(	O
Ortega	O
et	O
al	O
,	O
2019	O
)	O
.	O
Additionally	O
,	O
we	O
developed	O
an	O
affective	O
NLG	O
service	O
,	O
which	O
allows	O
for	O
different	O
templates	O
to	O
be	O
used	O
depending	O
on	O
the	O
user	O
's	O
emotional	O
state	O
.	O
This	O
enables	O
a	O
more	O
sensitive	O
/	O
adaptive	O
system	O
.	O
For	O
example	O
,	O
if	O
the	O
user	O
is	O
sad	O
and	O
the	O
system	O
does	O
not	O
understand	O
the	O
user	O
's	O
input	O
,	O
it	O
might	O
try	O
to	O
establish	O
common	O
ground	O
to	O
prevent	O
their	O
mood	O
from	O
getting	O
worse	O
due	O
to	O
the	O
bad	O
news	O
.	O
An	O
example	O
response	O
would	O
be	O
"	O
As	O
much	O
as	O
I	O
would	O
love	O
to	O
help	O
,	O
I	O
am	O
a	O
bit	O
confused	O
"	O
rather	O
than	O
the	O
more	O
neutral	O
"	O
Sorry	O
I	O
am	O
a	O
bit	O
confused	O
"	O
.	O
One	O
set	O
of	O
NLG	O
templates	O
can	O
be	O
specified	O
for	O
each	O
possible	O
emotional	O
state	O
.	O
At	O
runtime	O
,	O
the	O
utterance	O
is	O
then	O
generated	O
from	O
the	O
template	O
associated	O
with	O
the	O
current	O
system	O
emotion	B-DatasetName
and	O
system	O
action	O
.	O
4	O
Software	O
Architecture	O

Our	O
toolkit	O
allows	O
for	O
easy	O
creation	O
of	O
a	O
dialog	O
system	O
within	O
a	O
few	O
lines	O
of	O
code	O
as	O
follows	O
.	O
As	O
a	O
first	O
step	O
,	O
a	O
dialog	O
system	O
object	O
is	O
initialized	O
,	O
which	O
is	O
responsible	O
for	O
coordinating	O
the	O
initialization	O
and	O
graceful	O
termination	O
of	O
all	O
dialog	O
services	O
.	O
Talking	O
about	O
multiple	O
domains	O
in	O
one	O
dialog	O
is	O
enabled	O
by	O
creating	O
a	O
simple	O
keywordbased	O
domain	O
tracker	O
which	O
is	O
introduced	O
as	O
the	O
first	O
argument	O
to	O
the	O
dialog	O
system	O
.	O
To	O
make	O
the	O
dialog	O
multi	O
-	O
modal	O
,	O
speech	O
and	O
vision	O
modules	O
are	O
introduced	O
next	O
,	O
along	O
with	O
modules	O
to	O
extract	O
engagement	O
and	O
emotion	B-DatasetName
.	O
So	O
far	O
,	O
all	O
of	O
these	O
modules	O
are	O
domain	O
-	O
agnostic	O
and	O
can	O
be	O
used	O
as	O
shared	O
resources	O
between	O
all	O
domains	O
.	O
Next	O
,	O
domaindependent	O
services	O
such	O
as	O
NLUs	O
,	O
BSTs	O
and	O
NLGs	O
for	O
weather	O
and	O
mensa	O
,	O
are	O
added	O
.	O
The	O
following	O
shows	O
an	O
example	O
dialog	O
.	O

Other	O
tools	O
for	O
building	O
dialog	O
systems	O
include	O
ConvLab	O
(	O
Lee	O
et	O
al	O
,	O
2019	O
)	O
,	O
an	O
open	O
-	O
source	O
,	O
textbased	O
dialog	O
system	O
platform	O
that	O
supports	O
both	O
pipelined	O
architectures	O
and	O
an	O
end	O
-	O
to	O
-	O
end	O
neural	O
architecture	O
.	O
ConvLab	O
also	O
provides	O
reusable	O
components	O
and	O
supports	O
multi	O
-	O
domain	O
settings	O
.	O
Other	O
systems	O
are	O
largely	O
text	O
-	O
based	O
,	O
but	O
offer	O
the	O
incorporation	O
of	O
external	O
speech	O
components	O
.	O
In	O
-	O
proTK	O
(	O
Baumann	O
and	O
Schlangen	O
,	O
2012	O
)	O
,	O
for	O
instance	O
,	O
in	O
which	O
modules	O
communicate	O
by	O
networks	O
via	O
configuration	O
files	O
,	O
uses	O
ASR	O
based	O
on	O
Sphinx	O
-	O
4	O
and	O
synthesis	O
based	O
on	O
MaryTTS	O
.	O
Similarly	O
,	O
RavenClaw	O
(	O
Bohus	O
and	O
Rudnicky	O
,	O
2009	O
)	O
provides	O
a	O
framework	O
for	O
creating	O
dialog	O
managers	O
;	O
ASR	O
and	O
synthesis	O
components	O
can	O
be	O
supplied	O
,	O
for	O
example	O
,	O
by	O
connecting	O
to	O
Sphinx	O
and	O
Kalliope	O
.	O
OpenDial	O
(	O
Lison	O
and	O
Kennington	O
,	O
2016	O
)	O
relies	O
on	O
probabilistic	O
rules	O
and	O
provides	O
options	O
to	O
connect	O
to	O
speech	O
components	O
such	O
as	O
Sphinx	O
.	O
Multidomain	O
dialog	O
toolkit	O
-	O
PyDial	O
(	O
Ultes	O
et	O
al	O
,	O
2017	O
)	O
supports	O
connection	O
to	O
DialPort	O
.	O
As	O
mentioned	O
in	O
the	O
introduction	O
,	O
Microsoft	O
Research	O
's	O
\psi	O
is	O
an	O
open	O
and	O
extensible	O
platform	O
that	O
supports	O
the	O
development	O
of	O
multi	O
-	O
modal	O
AI	O
systems	O
(	O
Bohus	O
et	O
al	O
,	O
2017	O
)	O
.	O
It	O
further	O
offers	O
audio	O
and	O
visual	O
processing	O
,	O
such	O
as	O
speech	B-TaskName
recognition	I-TaskName
and	O
face	O
tracking	O
,	O
as	O
well	O
as	O
output	O
,	O
such	O
as	O
synthesis	O
and	O
avatar	O
rendering	O
.	O
And	O
the	O
MuMMER	O
(	O
multimodal	O
Mall	B-DatasetName
Entertainment	O
Robot	O
)	O
project	O
(	O
Foster	O
et	O
al	O
,	O
2016	O
)	O
is	O
based	O
on	O
the	O
SoftBank	O
Robotics	O
Pepper	O
platform	O
,	O
and	O
thereby	O
comprises	O
processing	O
of	O
audio	O
-	O
,	O
visual	O
-	O
and	O
social	O
signals	O
,	O
with	O
the	O
aim	O
to	O
develop	O
a	O
socially	O
engaging	O
robot	O
that	O
can	O
be	O
deployed	O
in	O
public	O
spaces	O
.	O

Dependency	O
treebanks	O
for	O
Latin	O
have	O
a	O
history	O
that	O
goes	O
back	O
to	O
2006	O
.	O
For	O
it	O
was	O
in	O
that	O
year	O
that	O
the	O
first	O
two	O
projects	O
kicked	O
off	O
:	O
the	O
Latin	O
Dependency	O
Treebank	O
(	O
ldt	O
)	O
(	O
Bamman	O
and	O
Crane	O
,	O
2006	O
)	O
,	O
featuring	O
a	O
small	O
selection	O
of	O
texts	O
by	O
Classical	O
authors	O
(	O
currently	O
around	O
50k	O
nodes	O
)	O
,	O
and	O
the	O
Index	O
Thomisticus	O
Treebank	O
(	O
ıt	O
-	O
tb	O
)	O
(	O
Passarotti	O
,	O
2011	O
)	O
,	O
based	O
on	O
works	O
written	O
in	O
the	O
XIIIth	O
century	O
by	O
Thomas	O
Aquinas	O
(	O
approximately	O
400k	O
nodes	O
)	O
.	O
Later	O
on	O
,	O
a	O
third	O
Latin	O
treebank	O
was	O
created	O
in	O
the	O
context	O
of	O
the	O
proıel	O
project	O
(	O
Haug	O
and	O
Jøhndal	O
,	O
2008	O
)	O
,	O
which	O
includes	O
the	O
entire	O
New	O
Testament	O
in	O
Latin	O
(	O
the	O
so	O
called	O
Vulgata	O
by	O
Jerome	O
)	O
and	O
texts	O
from	O
the	O
Classical	O
era	O
(	O
for	O
a	O
total	O
of	O
around	O
250k	O
nodes	O
)	O
.	O
Most	O
recently	O
,	O
a	O
syntactically	O
annotated	O
corpus	O
of	O
original	O
VIIIth	O
-	O
IXth	O
century	O
charters	O
from	O
Central	O
Italy	O
,	O
called	O
Late	O
Latin	O
Charter	O
Treebank	O
(	O
llct	O
;	O
around	O
250k	O
nodes	O
)	O
,	O
was	O
made	O
available	O
(	O
Korkiakangas	O
and	O
Passarotti	O
,	O
2011	O
)	O
.	O
While	O
the	O
ldt	O
,	O
the	O
ıt	O
-	O
tb	O
and	O
the	O
llct	O
have	O
shared	O
the	O
same	O
manual	O
for	O
syntactic	O
annotation	O
since	O
the	O
beginning	O
of	O
their	O
respective	O
projects	O
(	O
Bamman	O
et	O
al	O
,	O
2007	O
)	O
,	O
the	O
proıel	O
treebank	O
follows	O
a	O
slightly	O
different	O
style	O
(	O
Haug	O
,	O
2010	O
)	O
.	O
Currently	O
,	O
all	O
the	O
Latin	O
treebanks	O
except	O
the	O
llct	O
are	O
available	O
also	O
in	O
the	O
Universal	B-DatasetName
Dependencies	I-DatasetName
collection	O
(	O
UD	B-DatasetName
)	O
(	O
Nivre	O
et	O
al	O
,	O
2016	O
)	O
.	O
1	O
The	O
existence	O
of	O
four	O
treebanks	O
for	O
an	O
ancient	O
language	O
like	O
Latin	O
is	O
not	O
surprising	O
,	O
reflecting	O
the	O
large	O
diachronic	O
(	O
as	O
well	O
as	O
diatopic	O
)	O
span	O
of	O
Latin	O
texts	O
,	O
which	O
are	O
spread	O
across	O
a	O
time	O
frame	O
of	O
more	O
than	O
two	O
millennia	O
and	O
in	O
most	O
areas	O
of	O
the	O
Mediterranean	O
and	O
of	O
what	O
is	O
called	O
Europe	O
today	O
.	O
Since	O
Latin	O
has	O
represented	O
for	O
a	O
long	O
time	O
a	O
kind	O
of	O
lingua	O
franca	O
,	O
the	O
variety	O
of	O
its	O
textual	O
typologies	O
is	O
wide	O
,	O
including	O
scientific	O
treaties	O
,	O
literary	O
works	O
,	O
philosophical	O
texts	O
and	O
official	O
documents	O
.	O
This	O
aspect	O
makes	O
it	O
impossible	O
to	O
build	O
one	O
textual	O
corpus	O
that	O
alone	O
can	O
be	O
sufficiently	O
representative	O
of	O
"	O
Latin	O
"	O
,	O
just	O
because	O
there	O
are	O
too	O
many	O
varieties	O
of	O
Latin	O
,	O
which	O
can	O
be	O
even	O
very	O
different	O
from	O
each	O
other	O
.	O
2	O
In	O
order	O
to	O
cope	O
with	O
such	O
a	O
large	O
variety	O
,	O
several	O
collections	O
of	O
Latin	O
texts	O
are	O
today	O
available	O
in	O
digital	O
format	O
,	O
like	O
for	O
instance	O
the	O
Perseus	O
Digital	O
Library	O
3	O
and	O
the	O
collection	O
of	O
Medieval	O
Italian	O
Latinity	O
ALIM	O
.	O
4	O
Besides	O
textual	O
resources	O
,	O
the	O
centuries	O
-	O
old	O
tradition	O
of	O
Latin	O
lexicography	O
resulted	O
in	O
the	O
current	O
availability	O
of	O
several	O
digitized	O
dictionaries	O
,	O
like	O
for	O
instance	O
the	O
Lewis	O
-	O
Short	O
dictionary	O
available	O
at	O
Perseus	O
and	O
the	O
Thesaurus	O
Linguae	O
Latinae	O
by	O
the	O
Bayerische	O
Akademie	O
der	O
Wissenschaften	O
in	O
Munich	O
.	O
5	O
A	O
small	O
Latin	O
WordNet	O
including	O
around	O
9	O
,	O
000	O
lemmas	O
is	O
also	O
available	O
(	O
Minozzi	O
,	O
2010	O
)	O
,	O
as	O
well	O
as	O
a	O
derivational	O
morphology	O
lexicon	O
called	O
Word	O
Formation	O
Latin	O
(	O
wfl	O
)	O
(	O
Litta	O
et	O
al	O
,	O
2016	O
)	O
.	O
Just	O
like	O
for	O
most	O
other	O
(	O
both	O
modern	O
and	O
ancient	O
)	O
languages	O
,	O
the	O
interoperability	O
issues	O
imposed	O
by	O
the	O
different	O
formats	O
,	O
tag	O
sets	O
and	O
annotation	O
criteria	O
of	O
the	O
linguistic	O
resources	O
for	O
Latin	O
severely	O
limit	O
their	O
potential	O
for	O
exploitation	O
and	O
use	O
.	O
Indeed	O
,	O
linking	O
linguistic	O
resources	O
to	O
one	O
another	O
would	O
maximize	O
their	O
contribution	O
to	O
linguistic	O
analysis	O
at	O
multiple	O
levels	O
,	O
be	O
those	O
lexical	O
,	O
morphological	O
,	O
syntactic	O
,	O
semantic	O
or	O
pragmatic	O
.	O
Thus	O
,	O
presently	O
there	O
is	O
a	O
growing	O
interest	O
in	O
the	O
interoperability	O
of	O
(	O
annotated	O
)	O
corpora	O
,	O
lexical	O
resources	O
and	O
Natural	O
Language	O
Processing	O
(	O
NLP	O
)	O
tools	O
(	O
Ide	O
and	O
Pustejovsky	O
,	O
2010	O
)	O
.	O
So	O
far	O
,	O
this	O
was	O
partially	O
approached	O
by	O
building	O
large	O
infrastructures	O
and	O
databases	O
of	O
linguistic	O
resources	O
,	O
like	O
CLARIN	O
,	O
6	O
DARIAH	O
,	O
7	O
META	O
-	O
SHARE	O
,	O
8	O
and	O
EAGLE	O
.	O
9	O
In	O
the	O
treebank	O
area	O
,	O
the	O
UD	B-DatasetName
collection	O
includes	O
more	O
than	O
100	O
treebanks	O
sharing	O
the	O
same	O
annotation	O
guidelines	O
and	O
provides	O
different	O
tools	O
for	O
querying	O
the	O
treebanks	O
on	O
-	O
line	O
.	O
10	O
A	O
relevant	O
initiative	O
of	O
this	O
kind	O
is	O
the	O
Norwegian	O
Infrastructure	O
for	O
the	O
Exploration	O
of	O
Syntax	O
and	O
Semantics	O
(	O
ıness	O
)	O
(	O
Rosén	O
et	O
al	O
,	O
2012	O
)	O
,	O
which	O
offers	O
an	O
open	O
and	O
easy	O
-	O
to	O
-	O
use	O
platform	O
for	O
building	O
,	O
accessing	O
,	O
searching	O
and	O
visualizing	O
treebanks	O
through	O
a	O
web	O
browser	O
.	O
11	O
These	O
collections	O
and	O
infrastructures	O
enable	O
to	O
use	O
and	O
query	O
various	O
resources	O
and	O
tools	O
from	O
one	O
common	O
place	O
on	O
the	O
web	O
,	O
but	O
they	O
do	O
not	O
provide	O
a	O
real	O
interconnection	O
between	O
them	O
,	O
thus	O
failing	O
to	O
achieve	O
their	O
interoperability	O
.	O
Instead	O
,	O
making	O
linguistic	O
resources	O
interoperable	O
requires	O
that	O
all	O
types	O
of	O
annotation	O
applied	O
to	O
a	O
particular	O
word	O
/	O
text	O
get	O
integrated	O
into	O
a	O
common	O
representation	O
that	O
enables	O
access	O
to	O
the	O
linguistic	O
information	O
conveyed	O
in	O
a	O
linguistic	O
resource	O
or	O
produced	O
by	O
an	O
NLP	O
tool	O
(	O
Chiarcos	O
,	O
2012	O
,	O
p.	O
162	O
)	O
.	O
Particularly	O
,	O
by	O
applying	O
the	O
principles	O
of	O
Linked	O
Data	O
to	O
linguistic	O
resources	O
12	O
"	O
it	O
is	O
possible	O
to	O
follow	O
links	O
between	O
existing	O
resources	O
to	O
find	O
other	O
,	O
related	O
data	O
and	O
exploit	O
network	O
effects	O
"	O
(	O
Chiarcos	O
et	O
al	O
,	O
2013	O
,	O
p.	O
iii	O
)	O
.	O
13	O
Despite	O
their	O
rich	O
annotation	O
(	O
ranging	O
from	O
tokenization	O
to	O
syntactic	O
analysis	O
)	O
,	O
treebanks	O
alone	O
can	O
not	O
account	O
for	O
the	O
linguistic	O
complexity	O
of	O
the	O
texts	O
they	O
include	O
,	O
which	O
requires	O
that	O
information	O
provided	O
by	O
different	O
(	O
and	O
currently	O
available	O
)	O
textual	O
and	O
lexical	O
resources	O
is	O
interlinked	O
and	O
,	O
thus	O
,	O
exploited	O
to	O
the	O
best	O
.	O
To	O
this	O
aim	O
,	O
the	O
LiLa	O
:	O
Linking	O
Latin	O
project	O
(	O
2018	O
-	O
2023	O
)	O
14	O
was	O
launched	O
with	O
the	O
objective	O
to	O
interlink	O
the	O
wealth	O
of	O
linguistic	O
resources	O
and	O
NLP	O
tools	O
for	O
Latin	O
developed	O
thus	O
far	O
,	O
in	O
order	O
to	O
bridge	O
the	O
gap	O
between	O
raw	O
language	O
data	O
,	O
NLP	O
and	O
knowledge	O
description	O
(	O
Declerck	O
et	O
al	O
,	O
2012	O
,	O
p.	O
111	O
)	O
.	O
LiLa	O
addresses	O
this	O
challenge	O
by	O
building	O
a	O
collection	O
of	O
several	O
data	O
sets	O
described	O
using	O
the	O
same	O
vocabulary	O
and	O
linked	O
together	O
,	O
namely	O
a	O
Linked	O
(	O
Open	O
)	O
Data	O
Knowledge	O
Base	O
of	O
the	O
linguistic	O
resources	O
(	O
and	O
NLP	O
tools	O
)	O
for	O
Latin	O
currently	O
available	O
from	O
different	O
providers	O
under	O
various	O
licences	O
.	O
After	O
a	O
brief	O
description	O
of	O
the	O
basic	O
architecture	O
of	O
the	O
LiLa	O
Knowledge	O
Base	O
(	O
Section	O
2	O
)	O
,	O
this	O
paper	O
focuses	O
on	O
the	O
inclusion	O
of	O
three	O
dependency	O
treebanks	O
for	O
Latin	O
into	O
LiLa	O
(	O
namely	O
,	O
the	O
ıt	O
-	O
tb	O
in	O
two	O
versions	O
,	O
proıel	O
and	O
the	O
llct	O
)	O
,	O
presenting	O
an	O
example	O
of	O
a	O
complex	O
query	O
crossing	O
the	O
treebanks	O
and	O
the	O
other	O
linguistic	O
resources	O
included	O
so	O
far	O
in	O
the	O
Knowledge	O
Base	O
(	O
Section	O
3	O
)	O
.	O

In	O
order	O
to	O
achieve	O
interoperability	O
between	O
linguistic	O
resources	O
and	O
NLP	O
tools	O
,	O
the	O
LiLa	O
Knowledge	O
Base	O
makes	O
use	O
of	O
a	O
set	O
of	O
Semantic	O
Web	O
and	O
Linguistic	O
Linked	O
Open	O
Data	O
standards	O
.	O
These	O
include	O
ontologies	O
to	O
describe	O
linguistic	O
annotation	O
(	O
OLiA	O
(	O
Chiarcos	O
and	O
Sukhareva	O
,	O
2015	O
)	O
)	O
,	O
corpus	O
annotation	O
(	O
NIF	O
(	O
Hellmann	O
et	O
al	O
,	O
2013	O
)	O
,	O
conll	O
-	O
rdf	O
(	O
Chiarcos	O
and	O
Fäth	O
,	O
2017	O
)	O
)	O
and	O
lexical	O
resources	O
(	O
Lemon	O
(	O
Buitelaar	O
et	O
al	O
,	O
2011	O
)	O
,	O
Ontolex	O
15	O
)	O
.	O
The	O
Resource	O
Description	O
Framework	O
(	O
RDF	O
)	O
(	O
Lassila	O
et	O
al	O
,	O
1998	O
)	O
is	O
used	O
to	O
encode	O
graphbased	O
data	O
structures	O
to	O
represent	O
linguistic	O
annotations	O
in	O
terms	O
of	O
triples	O
,	O
made	O
of	O
a	O
predicate	O
connecting	O
two	O
nodes	O
(	O
a	O
subject	O
and	O
its	O
object	O
)	O
.	O
The	O
SPARQL	O
language	O
is	O
used	O
to	O
query	O
the	O
data	O
recorded	O
in	O
the	O
form	O
of	O
RDF	O
triples	O
(	O
Prud'Hommeaux	O
et	O
al	O
,	O
2008	O
)	O
.	O
The	O
LiLa	O
Knowledge	O
Base	O
is	O
highly	O
lexically	O
-	O
based	O
,	O
striking	O
a	O
balance	O
between	O
feasibility	O
and	O
granularity	O
:	O
its	O
basic	O
assumption	O
is	O
that	O
textual	O
resources	O
are	O
made	O
of	O
(	O
occurrences	O
of	O
)	O
words	O
,	O
lexical	O
resources	O
describe	O
properties	O
of	O
words	O
,	O
and	O
NLP	O
tools	O
process	O
words	O
.	O
Figure	O
1	O
presents	O
the	O
basic	O
architecture	O
of	O
the	O
LiLa	O
Knowledge	O
Base	O
,	O
showing	O
its	O
main	O
components	O
and	O
their	O
relations	O
.	O
The	O
Lemma	B-DatasetName
is	O
the	O
key	O
node	O
type	O
in	O
LiLa	O
.	O
A	O
Lemma	B-DatasetName
is	O
an	O
(	O
inflected	O
)	O
Form	O
conventionally	O
chosen	O
as	O
the	O
citation	O
form	O
of	O
a	O
lexical	O
item	O
.	O
Lemmas	O
occur	O
in	O
Lexical	O
Resources	O
as	O
canonical	O
forms	O
of	O
lexical	O
entries	O
.	O
Forms	O
,	O
too	O
,	O
can	O
occur	O
in	O
lexical	O
resources	O
,	O
like	O
in	O
a	O
lexicon	O
containing	O
all	O
of	O
the	O
forms	O
of	O
a	O
language	O
,	O
as	O
for	O
instance	O
in	O
Tombeur	O
(	O
1998	O
)	O
.	O
The	O
occurrences	O
of	O
Forms	O
in	O
real	O
texts	O
are	O
Tokens	O
,	O
which	O
are	O
provided	O
by	O
Textual	O
Resources	O
.	O
Finally	O
,	O
NLP	O
tools	O
process	O
either	O
Forms	O
regardless	O
of	O
their	O
contextual	O
use	O
(	O
e.g.	O
,	O
a	O
morphological	O
analyzer	O
)	O
,	O
or	O
Tokens	O
(	O
e.g.	O
,	O
a	O
PoS	O
-	O
tagger	O
)	O
,	O
or	O
texts	O
in	O
Textual	O
Resources	O
(	O
e.g.	O
,	O
a	O
tokenizer	O
)	O
.	O
Forms	O
,	O
Lemmas	O
and	O
Tokens	O
can	O
be	O
assigned	O
Morphological	O
Features	O
,	O
like	O
part	O
of	O
speech	O
and	O
gender	O
.	O
Since	O
lemmas	O
serve	O
as	O
the	O
optimal	O
interface	O
between	O
lexical	O
resources	O
,	O
(	O
annotated	O
)	O
corpora	O
and	O
NLP	O
tools	O
,	O
the	O
core	O
of	O
the	O
LiLa	O
Knowledge	O
Base	O
is	O
a	O
collection	O
of	O
citation	O
forms	O
for	O
Latin	O
.	O
Interoperability	O
can	O
be	O
achieved	O
by	O
linking	O
the	O
entries	O
in	O
lexical	O
resources	O
and	O
the	O
corpus	O
tokens	O
pointing	O
to	O
the	O
same	O
lemma	B-DatasetName
.	O
16	O
The	O
collection	O
of	O
citation	O
forms	O
of	O
LiLa	O
is	O
built	O
on	O
top	O
of	O
the	O
set	O
of	O
lemmas	O
used	O
by	O
the	O
morphological	O
analyzer	O
for	O
Latin	O
Lemlat	O
(	O
Passarotti	O
et	O
al	O
,	O
2017	O
)	O
.	O
17	O
Lemlat	O
relies	O
on	O
a	O
lexical	O
basis	O
resulting	O
from	O
the	O
collation	O
of	O
three	O
Latin	O
dictionaries	O
(	O
Georges	O
andGeorges	O
,	O
1913	O
1918	O
;	O
Glare	O
,	O
1982	O
;	O
Gradenwitz	O
,	O
1904	O
)	O
for	O
a	O
total	O
of	O
40	O
,	O
014	O
lexical	O
entries	O
and	O
43	O
,	O
432	O
lemmas	O
,	O
as	O
more	O
than	O
one	O
lemma	B-DatasetName
can	O
be	O
included	O
in	O
one	O
lexical	O
entry	O
.	O
This	O
lexical	O
basis	O
was	O
recently	O
further	O
enlarged	O
by	O
adding	O
the	O
Onomasticon	O
provided	O
by	O
the	O
5th	O
edition	O
of	O
Forcellini	O
dictionary	O
(	O
Budassi	O
and	O
Passarotti	O
,	O
2016	O
)	O
and	O
the	O
entries	O
from	O
a	O
large	O
reference	O
glossary	O
for	O
Medieval	O
Latin	O
,	O
namely	O
the	O
Glossarium	O
Mediae	O
et	O
Infimae	O
Latinitatis	O
(	O
du	O
Cange	O
et	O
al	O
,	O
1883	O
(	O
du	O
Cange	O
et	O
al	O
,	O
1887Cecchini	O
et	O
al	O
,	O
2018	O
)	O
,	O
leading	O
to	O
a	O
total	O
of	O
around	O
150	O
,	O
000	O
lemmas	O
.	O
The	O
linguistic	O
resources	O
currently	O
linked	O
in	O
the	O
LiLa	O
Knowledge	O
Base	O
are	O
stored	O
in	O
a	O
triplestore	O
using	O
the	O
Jena	O
framework	O
.	O
18	O
The	O
Fuseki	O
component	O
exposes	O
the	O
data	O
as	O
a	O
SPARQL	O
end	O
-	O
point	O
accessible	O
over	O
HTTP	O
.	O
The	O
current	O
prototype	O
of	O
the	O
LiLa	O
RDF	O
triplestore	O
database	O
connects	O
the	O
following	O
resources	O
for	O
Latin	O
:	O
(	O
a	O
)	O
the	O
collection	O
of	O
lemmas	O
provided	O
by	O
Lemlat	O
,	O
(	O
b	O
)	O
the	O
wfl	O
lexicon	O
,	O
and	O
(	O
c	O
)	O
three	O
treebanks	O
(	O
four	O
by	O
version	O
)	O
:	O
(	O
c.1	O
)	O
proıel	O
in	O
its	O
UD	B-DatasetName
version	O
(	O
release	O
2.3	O
)	O
,	O
(	O
c.2	O
-	O
3	O
)	O
the	O
ıt	O
-	O
tb	O
in	O
both	O
its	O
UD	B-DatasetName
2.3	O
and	O
original	O
version	O
,	O
and	O
(	O
c.4	O
)	O
a	O
selection	O
of	O
3	O
,	O
900	O
sentences	O
(	O
105	O
,	O
380	O
tokens	O
)	O
of	O
the	O
llct	O
.	O

The	O
Latin	O
treebanks	O
currently	O
integrated	O
into	O
LiLa	O
have	O
been	O
converted	O
into	O
RDF	O
triples	O
.	O
As	O
an	O
example	O
,	O
Figure	O
2	O
represents	O
a	O
first	O
result	O
in	O
the	O
conversion	O
and	O
linking	O
process	O
.	O
The	O
figure	O
shows	O
a	O
three	O
-	O
word	O
sentence	O
from	O
the	O
Vulgata	O
(	O
Matt	O
.	O
6.10	O
)	O
,	O
taken	O
from	O
the	O
UD	B-DatasetName
2.3	O
version	O
of	O
the	O
proıel	O
corpus	O
:	O
veniat	O
regnum	O
tuum	O
(	O
"	O
thy	O
kingdom	O
come	O
"	O
)	O
.	O
The	O
UD	B-DatasetName
2.3	O
tree	O
for	O
this	O
sentence	O
is	O
shown	O
in	O
Figure	O
3	O
.	O
19	O
Tokens	O
and	O
sentences	O
are	O
defined	O
using	O
the	O
NIF	O
vocabulary	O
.	O
In	O
the	O
current	O
,	O
preliminary	O
stage	O
of	O
the	O
Knowledge	O
Base	O
,	O
some	O
information	O
on	O
the	O
tokens	O
,	O
such	O
as	O
the	O
list	O
of	O
morphological	O
features	O
,	O
is	O
still	O
registered	O
as	O
a	O
simple	O
string	O
of	O
text	O
.	O
For	O
instance	O
,	O
in	O
Figure	O
2	O
this	O
is	O
the	O
case	O
of	O
the	O
string	O
"	O
Case	O
=	O
Nom	O
|	O
Gen	O
-	O
der	O
=	O
Neut	O
|	O
Number	O
=	O
Sing	O
"	O
,	O
which	O
is	O
linked	O
to	O
the	O
proıel	O
token	O
with	O
ID	O
s15924_2	O
(	O
for	O
the	O
word	O
regnum	O
"	O
kingdom	O
"	O
)	O
via	O
the	O
relation	O
conll	O
:	O
FEAT	O
,	O
linking	O
the	O
morphological	O
features	O
taken	O
from	O
files	O
in	O
the	O
CoNLL	O
-	O
U	O
format	O
of	O
UD	B-DatasetName
.	O
20	O
Other	O
types	O
of	O
tagging	O
(	O
such	O
as	O
syntactic	O
dependencies	O
,	O
or	O
sentence	O
boundaries	O
)	O
are	O
expressed	O
by	O
links	O
between	O
the	O
nodes	O
for	O
tokens	O
or	O
sentences	O
.	O
For	O
example	O
,	O
in	O
Figure	O
2	O
,	O
this	O
is	O
represented	O
by	O
the	O
linking	O
between	O
the	O
token	O
s15924_2	O
(	O
regnum	O
)	O
and	O
the	O
token	O
s15924_1	O
(	O
veniat	O
"	O
come	O
"	O
)	O
via	O
the	O
relation	O
conll	O
:	O
HEAD	O
,	O
representing	O
that	O
in	O
the	O
sentence	O
the	O
word	O
veniat	O
is	O
the	O
head	O
of	O
the	O
word	O
regnum	O
,	O
as	O
can	O
be	O
seen	O
from	O
the	O
tree	O
in	O
Figure	O
3	O
.	O
Finally	O
,	O
a	O
third	O
group	O
of	O
linguistic	O
annotations	O
,	O
like	O
the	O
part	O
of	O
speech	O
,	O
directly	O
relate	O
tokens	O
to	O
concepts	O
from	O
an	O
ontology	B-MethodName
of	O
linguistic	O
data	O
(	O
OLiA	O
)	O
.	O
21	O
In	O
Figure	O
2	O
,	O
this	O
is	O
shown	O
by	O
the	O
edge	O
connecting	O
the	O
token	O
s15924_2	O
(	O
regnum	O
)	O
to	O
the	O
concept	O
node	O
olia	O
:	O
CommonNoun	O
.	O
Tokens	O
are	O
connected	O
to	O
the	O
appropriate	O
Lemma	B-DatasetName
nodes	O
recorded	O
in	O
the	O
LiLa	O
Knowledge	O
Base	O
.	O
In	O
Figure	O
2	O
,	O
for	O
instance	O
,	O
the	O
token	O
s15924_2	O
(	O
regnum	O
)	O
is	O
linked	O
to	O
lemma	B-DatasetName
34146	O
,	O
which	O
has	O
written	O
representation	O
regnum	O
.	O
Via	O
this	O
connection	O
,	O
it	O
becomes	O
possible	O
to	O
access	O
all	O
the	O
other	O
information	O
that	O
is	O
also	O
pointing	O
to	O
that	O
lemma	B-DatasetName
.	O
In	O
the	O
figure	O
,	O
the	O
lemma	B-DatasetName
34146	O
is	O
connected	O
to	O
a	O
node	O
for	O
a	O
lexical	O
base	O
(	O
1133	O
)	O
,	O
the	O
same	O
to	O
which	O
also	O
lemmas	O
rex	O
"	O
king	O
"	O
(	O
34799	O
)	O
and	O
regno	O
"	O
to	O
rule	O
,	O
to	O
be	O
king	O
"	O
(	O
34145	O
)	O
are	O
attached	O
.	O
This	O
means	O
that	O
lemmas	O
regno	O
,	O
regnum	O
and	O
rex	O
belong	O
to	O
the	O
same	O
"	O
word	O
formation	O
family	O
"	O
,	O
i.e.	O
a	O
set	O
of	O
lemmas	O
sharing	O
the	O
same	O
lexical	O
base	O
.	O
The	O
lemma	B-DatasetName
regnum	O
is	O
also	O
formed	O
with	O
the	O
suffix	O
"	O
-	O
n	O
"	O
(	O
represented	O
by	O
the	O
node	O
affix:111	O
in	O
Figure	O
2	O
)	O
,	O
the	O
same	O
found	O
in	O
e.g.	O
fanum	O
"	O
shrine	O
"	O
(	O
not	O
shown	O
here	O
for	O
reasons	O
of	O
space	O
)	O
.	O
In	O
the	O
collection	O
of	O
citation	O
forms	O
included	O
in	O
LiLa	O
,	O
all	O
the	O
lemmas	O
formed	O
with	O
the	O
suffix	O
"	O
-	O
n	O
"	O
are	O
linked	O
to	O
affix:111	O
via	O
the	O
relation	O
lemlat_base	O
:	O
hasSuffix	O
,	O
thus	O
allowing	O
to	O
retrieve	O
them	O
in	O
the	O
Knowledge	O
Base	O
.	O
The	O
information	O
about	O
lexical	O
bases	O
and	O
affixes	O
is	O
available	O
thanks	O
to	O
the	O
connection	O
of	O
the	O
wfl	O
lexicon	O
in	O
LiLa	O
.	O

In	O
this	O
section	O
,	O
we	O
provide	O
an	O
example	O
of	O
the	O
types	O
of	O
queries	O
that	O
the	O
LiLa	O
Knowledge	O
Base	O
can	O
already	O
support	O
.	O
As	O
mentioned	O
,	O
one	O
single	O
query	O
can	O
extract	O
data	O
from	O
all	O
the	O
multiple	O
corpora	O
and	O
lexical	O
resources	O
linked	O
to	O
LiLa	O
's	O
collection	O
,	O
and	O
can	O
also	O
combine	O
syntactic	O
,	O
lexical	O
and	O
morphological	O
information	O
beyond	O
the	O
type	O
of	O
annotation	O
explicitly	O
recorded	O
in	O
a	O
single	O
corpus	O
.	O
Consider	O
,	O
for	O
instance	O
,	O
the	O
case	O
of	O
a	O
researcher	O
interested	O
in	O
the	O
relation	O
between	O
the	O
syntactic	O
role	O
of	O
subject	O
and	O
the	O
semantic	O
role	O
of	O
agent	B-DatasetName
in	O
Latin	O
.	O
One	O
possible	O
approach	O
to	O
study	O
the	O
question	O
would	O
be	O
to	O
start	O
by	O
collecting	O
and	O
analyzing	O
the	O
sentences	O
where	O
nouns	O
formed	O
with	O
a	O
typical	O
morpheme	O
for	O
agent	B-DatasetName
nouns	O
like	O
"	O
-	O
(	O
t	O
)	O
or	O
"	O
(	O
common	O
to	O
several	O
Indo	O
-	O
European	O
languages	O
)	O
are	O
attested	O
as	O
subject	O
of	O
an	O
active	O
verb	O
.	O
Though	O
the	O
number	O
of	O
linguistic	O
resources	O
currently	O
interlinked	O
in	O
LiLa	O
is	O
still	O
small	O
,	O
it	O
is	O
already	O
possible	O
to	O
design	O
a	O
single	O
SPARQL	O
query	O
to	O
extract	O
this	O
information	O
from	O
our	O
RDF	O
versions	O
of	O
proıel	O
,	O
ıt	O
-	O
tb	O
(	O
UD	B-DatasetName
version	O
)	O
and	O
llct	O
.	O
In	O
what	O
follows	O
,	O
we	O
illustrate	O
the	O
results	O
of	O
a	O
query	O
that	O
asks	O
for	O
an	O
active	O
(	O
or	O
deponent	O
)	O
verb	O
governing	O
a	O
noun	O
with	O
the	O
syntactic	O
relation	O
of	O
subject	O
in	O
the	O
three	O
treebanks	O
.	O
By	O
leveraging	O
the	O
connection	O
between	O
lemmas	O
and	O
the	O
affixes	O
in	O
wfl	O
,	O
we	O
add	O
the	O
additional	O
constraint	O
that	O
the	O
noun	O
must	O
be	O
formed	O
with	O
the	O
suffix	O
"	O
-	O
(	O
t	O
)	O
or	O
"	O
.	O
This	O
information	O
,	O
which	O
is	O
not	O
encoded	O
into	O
the	O
original	O
treebanks	O
,	O
is	O
now	O
accessible	O
thanks	O
to	O
the	O
architecture	O
based	O
on	O
Linked	O
Open	O
Data	O
that	O
LiLa	O
adopts	O
.	O
The	O
query	O
allows	O
us	O
to	O
extract	O
143	O
passages	O
,	O
with	O
80	O
different	O
verbs	O
and	O
58	O
agent	B-DatasetName
nouns	O
.	O
One	O
sample	O
of	O
the	O
results	O
,	O
a	O
sentence	O
from	O
Cicero	O
's	O
Letters	O
to	O
Atticus	O
(	O
4.4a.2	O
)	O
retrieved	O
from	O
proıel	O
,	O
is	O
reported	O
in	O
Example	O
(	O
1	O
)	O
.	O
(	O
1	O
)	O
gladiatores	O
audio	O
pugnare	O
mirifice	O
.	O
'	O
I	O
hear	O
that	O
your	O
gladiators	O
fight	O
superbly	O
.	O
'	O
The	O
subject	O
-	O
verb	O
bigrams	O
resulting	O
from	O
the	O
query	O
highlight	O
interest	O
lexical	O
aspects	O
in	O
the	O
language	O
of	O
the	O
three	O
corpora	O
.	O
As	O
it	O
is	O
to	O
be	O
expected	O
from	O
the	O
documentary	O
nature	O
of	O
the	O
texts	O
provided	O
by	O
the	O
llct	O
treebank	O
,	O
the	O
10	O
occurrences	O
found	O
in	O
this	O
corpus	O
all	O
involve	O
legal	O
actors	O
and	O
events	O
:	O
the	O
most	O
frequent	O
subject	O
(	O
4	O
occurrences	O
)	O
is	O
rector	O
,	O
the	O
priest	O
responsible	O
for	O
a	O
rural	O
church	O
.	O
The	O
other	O
actors	O
are	O
:	O
dispensator	O
"	O
treasurer	O
"	O
,	O
fideiussor	O
"	O
bail	O
"	O
,	O
genitor	O
"	O
parent	O
"	O
and	O
imperator	O
"	O
emperor	O
"	O
.	O
In	O
the	O
ıttb	O
,	O
on	O
the	O
other	O
hand	O
,	O
the	O
most	O
frequent	O
couplet	O
is	O
the	O
one	O
formed	O
by	O
the	O
noun	O
commentator	O
"	O
interpreter	O
"	O
and	O
the	O
verb	O
dico	O
"	O
to	O
say	O
"	O
(	O
21	O
cases	O
)	O
,	O
where	O
the	O
assertions	O
of	O
a	O
scholar	O
are	O
reported	O
and	O
discussed	O
.	O
Indeed	O
,	O
the	O
verbs	O
pointing	O
to	O
intellectual	O
activities	O
of	O
scholars	O
dominate	O
in	O
the	O
results	O
from	O
the	O
corpus	O
of	O
Thomas	O
Aquinas	O
:	O
in	O
addition	O
to	O
the	O
most	O
frequent	O
dico	O
(	O
22	O
)	O
,	O
other	O
intellectual	O
verbs	O
include	O
respondeo	O
"	O
to	O
reply	O
"	O
(	O
3	O
instances	O
)	O
,	O
fingo	O
"	O
to	O
imagine	O
"	O
(	O
2	O
)	O
,	O
and	O
intendo	O
"	O
to	O
mean	O
"	O
(	O
2	O
)	O
.	O
Finally	O
,	O
proıel	O
,	O
which	O
is	O
more	O
balanced	O
between	O
different	O
genres	O
,	O
offers	O
a	O
more	O
varied	O
set	O
of	O
subject	O
-	O
verb	O
couplets	O
in	O
its	O
57	O
results	O
.	O
As	O
in	O
Example	O
(	O
1	O
)	O
,	O
where	O
the	O
noun	O
gladiator	O
"	O
gladiator	O
"	O
is	O
coupled	O
with	O
the	O
verb	O
pugnare	O
"	O
to	O
fight	O
"	O
,	O
we	O
find	O
several	O
nouns	O
and	O
verbs	O
from	O
everyday	O
life	O
,	O
or	O
from	O
the	O
domain	O
of	O
the	O
professions	O
and	O
human	O
activities	O
.	O
Thus	O
,	O
for	O
instance	O
,	O
we	O
find	O
4	O
cases	O
of	O
fossor	O
"	O
digger	O
,	O
ditcher	O
"	O
joined	O
with	O
verbs	O
like	O
includo	O
"	O
to	O
shut	O
in	O
"	O
and	O
incumbo	O
"	O
to	O
press	O
upon	O
"	O
,	O
or	O
6	O
cases	O
of	O
pastor	O
"	O
herdsman	O
,	O
shepherd	O
"	O
with	O
verbs	O
like	O
fugio	O
"	O
to	O
flee	O
"	O
and	O
secludo	O
"	O
to	O
shut	O
off	O
"	O
.	O

Improving	O
Adversarial	B-TaskName
Text	I-TaskName
Generation	O
by	O
Modeling	O
the	O
Distant	O
Future	O

Auto	O
-	O
regressive	O
text	B-TaskName
generation	I-TaskName
models	O
usually	O
focus	O
on	O
local	O
fluency	O
,	O
and	O
may	O
cause	O
inconsistent	O
semantic	O
meaning	O
in	O
long	O
text	B-TaskName
generation	I-TaskName
.	O
Further	O
,	O
automatically	O
generating	O
words	O
with	O
similar	O
semantics	O
is	O
challenging	O
,	O
and	O
hand	O
-	O
crafted	O
linguistic	O
rules	O
are	O
difficult	O
to	O
apply	O
.	O
We	O
consider	O
a	O
text	O
planning	O
scheme	O
and	O
present	O
a	O
model	O
-	O
based	O
imitation	O
-	O
learning	O
approach	O
to	O
alleviate	O
the	O
aforementioned	O
issues	O
.	O
Specifically	O
,	O
we	O
propose	O
a	O
novel	O
guider	O
network	O
to	O
focus	O
on	O
the	O
generative	O
process	O
over	O
a	O
longer	O
horizon	O
,	O
which	O
can	O
assist	O
next	O
-	O
word	O
prediction	O
and	O
provide	O
intermediate	O
rewards	O
for	O
generator	O
optimization	O
.	O
Extensive	O
experiments	O
demonstrate	O
that	O
the	O
proposed	O
method	O
leads	O
to	O
improved	O
performance	O
.	O

Text	B-TaskName
generation	I-TaskName
is	O
an	O
important	O
area	O
of	O
investigation	O
within	O
machine	O
learning	O
.	O
Recent	O
work	O
has	O
shown	O
excellent	O
performance	O
on	O
a	O
number	O
of	O
tasks	O
,	O
by	O
combining	O
reinforcement	O
learning	O
(	O
RL	O
)	O
and	O
generative	O
models	O
.	O
Example	O
applications	O
include	O
image	B-TaskName
captioning	I-TaskName
(	O
Ren	O
et	O
al	O
,	O
2017	O
;	O
Rennie	O
et	O
al	O
,	O
2016	O
)	O
,	O
text	B-TaskName
summarization	I-TaskName
(	O
Li	O
et	O
al	O
,	O
2018b	O
;	O
Paulus	O
et	O
al	O
,	O
2017	O
;	O
Rush	O
et	O
al	O
,	O
2015	O
)	O
,	O
and	O
adversarial	B-TaskName
text	I-TaskName
generation	O
(	O
Guo	O
et	O
al	O
,	O
2017	O
;	O
Lin	O
et	O
al	O
,	O
2017	O
;	O
Zhu	O
et	O
al	O
,	O
2018	O
)	O
.	O
The	O
sequence	O
-	O
to	O
-	O
sequence	O
framework	O
(	O
Seq2Seq	B-MethodName
)	O
(	O
Sutskever	O
et	O
al	O
,	O
2014	O
)	O
is	O
a	O
popular	O
technique	O
for	O
text	B-TaskName
generation	I-TaskName
.	O
However	O
,	O
models	O
from	O
such	O
a	O
setup	O
are	O
typically	O
trained	O
to	O
predict	O
the	O
next	O
token	O
given	O
previous	O
ground	O
-	O
truth	O
tokens	O
as	O
input	O
,	O
causing	O
what	O
is	O
termed	O
exposure	O
bias	O
(	O
Ranzato	O
et	O
al	O
,	O
2016	O
)	O
.	O
By	O
contrast	O
,	O
sequence	O
-	O
level	O
training	O
with	O
RL	O
provides	O
an	O
effective	O
means	O
of	O
solving	O
this	O
challenge	O
,	O
by	O
treating	O
text	B-TaskName
generation	I-TaskName
as	O
a	O
sequential	O
decision	O
-	O
making	O
problem	O
.	O
By	O
directly	O
optimizing	O
an	O
evaluation	O
score	O
(	O
cumulative	O
rewards	O
)	O
(	O
Ranzato	O
et	O
al	O
,	O
2016	O
)	O
,	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
have	O
been	O
ob	O
-	O
tained	O
in	O
many	O
text	O
-	O
generation	O
tasks	O
(	O
Paulus	O
et	O
al	O
,	O
2017	O
;	O
Rennie	O
et	O
al	O
,	O
2016	O
)	O
.	O
However	O
,	O
one	O
problem	O
in	O
such	O
a	O
framework	O
is	O
that	O
rewards	O
in	O
RL	O
training	O
are	O
particularly	O
sparse	O
,	O
since	O
a	O
scalar	O
reward	O
is	O
typically	O
only	O
available	O
after	O
an	O
entire	O
sequence	O
has	O
been	O
generated	O
.	O
Furthermore	O
,	O
the	O
recurrent	O
models	O
focus	O
more	O
on	O
local	O
fluency	O
,	O
and	O
may	O
cause	O
inconsistent	O
semantic	O
meanings	O
for	O
long	O
text	B-TaskName
generation	I-TaskName
.	O
For	O
RL	O
-	O
based	O
text	B-TaskName
generation	I-TaskName
,	O
most	O
existing	O
works	O
rely	O
on	O
a	O
model	O
-	O
free	O
framework	O
,	O
which	O
has	O
been	O
criticized	O
for	O
its	O
high	O
variance	O
and	O
poor	O
sample	O
efficiency	O
(	O
Sutton	O
and	O
Barto	O
,	O
1998	O
)	O
.	O
On	O
the	O
other	O
hand	O
,	O
while	O
model	O
-	O
based	O
RL	O
methods	O
do	O
not	O
suffer	O
from	O
these	O
issues	O
,	O
they	O
are	O
usually	O
difficult	O
to	O
train	O
in	O
complex	O
environments	O
.	O
Further	O
,	O
a	O
learned	O
policy	O
is	O
usually	O
restricted	O
by	O
the	O
capacity	O
of	O
an	O
environment	O
model	O
.	O
Recent	O
developments	O
on	O
model	O
-	O
based	O
RL	O
(	O
Gu	O
et	O
al	O
,	O
2016	O
;	O
Kurutach	O
et	O
al	O
,	O
2018	O
;	O
Nagabandi	O
et	O
al	O
,	O
2017	O
)	O
combine	O
the	O
advantages	O
of	O
these	O
two	O
approaches	O
,	O
and	O
have	O
achieved	O
improved	O
performance	O
by	O
learning	O
a	O
model	O
-	O
free	O
policy	O
,	O
assisted	O
by	O
an	O
environment	O
model	O
.	O
In	O
addition	O
,	O
model	O
-	O
based	O
RL	O
has	O
been	O
employed	O
recently	O
to	O
solve	O
problems	O
with	O
extremely	O
sparse	O
rewards	O
,	O
with	O
curiosity	O
-	O
driven	O
methods	O
(	O
Pathak	O
et	O
al	O
,	O
2017	O
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
model	O
-	O
based	O
imitation	O
-	O
learning	O
method	O
to	O
overcome	O
the	O
aforementioned	O
issues	O
in	O
text	O
-	O
generation	O
tasks	O
.	O
Our	O
main	O
idea	O
is	O
to	O
employ	O
an	O
explicit	O
guider	O
network	O
to	O
model	O
the	O
generation	O
environment	O
in	O
the	O
feature	O
space	O
of	O
sentence	O
tokens	O
,	O
used	O
to	O
emit	O
intermediate	O
rewards	O
by	O
matching	O
the	O
predicted	O
features	O
from	O
the	O
guider	O
network	O
and	O
features	O
from	O
generated	O
sentences	O
.	O
The	O
guider	O
network	O
is	O
trained	O
to	O
encode	O
global	O
structural	O
information	O
of	O
training	O
sentences	O
,	O
and	O
thus	O
is	O
useful	O
to	O
guide	O
next	O
-	O
token	O
prediction	O
in	O
the	O
generative	O
process	O
.	O
Within	O
the	O
proposed	O
framework	O
,	O
to	O
assist	O
the	O
guider	O
network	O
,	O
we	O
also	O
develop	O
a	O
new	O
type	O
of	O
self	O
-	O
attention	O
mechanism	O
to	O
provide	O
high	O
-	O
level	O
planning	O
-	O
ahead	O
information	O
and	O
maintain	O
consistent	O
semantic	O
meaning	O
.	O
Our	O
experimental	O
results	O
demonstrate	O
the	O
effectiveness	O
of	O
proposed	O
methods	O
.	O

The	O
model	O
is	O
illustrated	O
in	O
Figure	O
1	O
,	O
with	O
an	O
autoeocoder	O
(	O
AE	B-MethodName
)	O
structure	O
for	O
sentence	O
feature	O
extraction	O
and	O
generation	O
.	O
The	O
encoder	O
is	O
shared	O
for	O
sentences	O
from	O
both	O
training	O
data	O
and	O
generated	O
data	O
,	O
as	O
explained	O
in	O
detail	O
below	O
.	O
Overall	O
,	O
text	B-TaskName
generation	I-TaskName
can	O
be	O
formulated	O
as	O
an	O
imitationlearning	O
problem	O
.	O
At	O
each	O
timestep	O
t	O
,	O
the	O
agent	B-DatasetName
,	O
also	O
called	O
a	O
generator	O
(	O
which	O
corresponds	O
to	O
the	O
LSTM	B-MethodName
decoder	O
)	O
,	O
takes	O
the	O
current	O
LSTM	B-MethodName
state	O
as	O
input	O
,	O
denoted	O
as	O
s	O
t	O
.	O
The	O
policy	O
π	O
φ	O
(	O
|	O
s	O
t	O
)	O
parameterized	O
by	O
φ	O
is	O
a	O
conditional	O
generator	O
,	O
to	O
generate	O
the	O
next	O
token	O
(	O
action	O
)	O
given	O
s	O
t	O
,	O
the	O
observation	O
representing	O
the	O
current	O
generated	O
sentence	O
.	O
The	O
objective	O
of	O
text	B-TaskName
generation	I-TaskName
is	O
to	O
maximize	O
the	O
total	O
reward	O
as	O
in	O
(	O
4	O
)	O
.	O
We	O
detail	O
the	O
components	O
for	O
our	O
proposed	O
model	O
in	O
the	O
following	O
subsections	O
.	O

As	O
illustrated	O
in	O
Figure	O
2	O
,	O
our	O
framework	O
naturally	O
provides	O
a	O
way	O
for	O
style	B-TaskName
transfer	I-TaskName
,	O
where	O
the	O
guider	O
network	O
plays	O
the	O
role	O
of	O
style	O
selection	O
,	O
and	O
the	O
generator	O
only	O
focuses	O
on	O
maintaining	O
content	O
without	O
considering	O
the	O
styles	O
.	O
To	O
make	O
the	O
guider	O
network	O
focus	O
on	O
the	O
guidance	O
of	O
styles	O
,	O
we	O
assign	O
the	O
label	O
l	O
as	O
the	O
initial	O
state	O
s	O
G	O
0	B-DatasetName
of	O
the	O
guider	O
network	O
.	O
Specifically	O
,	O
at	O
each	O
step	O
t	O
,	O
we	O
feed	O
the	O
current	O
sentence	O
representation	O
f	O
t	O
and	O
label	O
l	O
into	O
the	O
guider	O
network	O
:	O
O	O
t	O
=	O
g	O
(	O
s	O
t−1	O
)	O
,	O
w	O
t	O
=	O
ϕ	O
(	O
G	O
ψ	O
(	O
s	O
G	O
t−1	O
,	O
[	O
f	O
t	O
,	O
l	O
]	O
)	O
)	O
,	O
(	O
9	O
)	O
y	O
t	O
∼	O
Multi	O
(	O
1	O
,	O
softmax	B-MethodName
(	O
O	O
t	O
w	O
t	O
)	O
)	O
.	O
(	O
10	O
)	O
For	O
the	O
generator	O
,	O
we	O
put	O
an	O
adversarial	O
regularizer	O
on	O
the	O
encoded	O
latent	O
s	O
0	B-DatasetName
(	O
X	O
)	O
and	O
penalize	O
it	O
if	O
it	O
contains	O
the	O
sentiment	O
information	O
,	O
by	O
maximizing	O
the	O
entropy	O
,	O
i.e.	O
,	O
max	O
l	O
p	O
(	O
l	O
|	O
s	O
0	B-DatasetName
(	O
X	O
)	O
)	O
log	O
p	O
(	O
l	O
|	O
s	O
0	B-DatasetName
(	O
X	O
)	O
)	O
,	O
where	O
p	O
is	O
a	O
pre	O
-	O
trained	O
classifier	O
.	O
Intuitively	O
,	O
the	O
generator	O
gives	O
candidate	O
words	O
represented	O
by	O
O	O
t	O
,	O
while	O
the	O
guider	O
makes	O
a	O
choice	O
implicitly	O
by	O
w	O
t	O
based	O
on	O
the	O
sentiment	O
information	O
.	O
The	O
sentiment	O
information	O
is	O
contained	O
in	O
w	O
t	O
,	O
while	O
the	O
content	O
of	O
the	O
original	O
sentence	O
is	O
represented	O
by	O
O	O
t	O
.	O
To	O
achieve	O
styletransfer	O
,	O
one	O
feeds	O
the	O
original	O
sentence	O
X	O
with	O
the	O
target	O
style	O
label	O
l	O
to	O
get	O
the	O
transferred	O
sentence	O
Y	O
with	O
style	O
l.	O
Following	O
previous	O
work	O
(	O
Hu	O
et	O
al	O
,	O
2017	O
;	O
Yang	O
et	O
al	O
,	O
2018	O
;	O
Cheng	O
et	O
al	O
,	O
2020	O
)	O
,	O
we	O
adopt	O
a	O
classifier	O
as	O
the	O
discriminator	O
and	O
the	O
soft	O
-	O
argmax	O
approach	O
(	O
Kusner	O
and	O
Miguel	O
,	O
2016	O
)	O
for	O
the	O
update	O
of	O
generator	O
instead	O
of	O
policy	O
gradient	O
(	O
Sutton	O
and	O
Barto	O
,	O
1998	O
)	O
.	O

We	O
conduct	O
ablation	O
studies	O
on	O
long	O
text	B-TaskName
generation	I-TaskName
to	O
investigate	O
the	O
improvements	O
brought	O
by	O
each	O
part	O
of	O
our	O
proposed	O
method	O
.	O
We	O
first	O
test	O
the	O
benefits	O
of	O
using	O
the	O
guider	O
network	O
.	O
Among	O
the	O
methods	O
compared	O
,	O
Guider	O
is	O
the	O
standard	O
MLE	O
model	O
with	O
the	O
guider	O
network	O
.	O
We	O
further	O
compare	O
RL	O
training	O
with	O
i	O
)	O
only	O
final	O
rewards	O
,	O
ii	O
)	O
only	O
feature	O
-	O
matching	O
rewards	O
,	O
and	O
iii	O
)	O
combining	O
both	O
rewards	O
,	O
namely	O
GMGAN	O
.	O
The	O
results	O
are	O
shown	O
in	O
Table	O
6	O
.	O
We	O
observe	O
that	O
guider	O
network	O
plays	O
an	O
important	O
role	O
in	O
improving	O
the	O
performance	O
.	O
RL	O
training	O
with	O
final	O
rewards	O
given	O
by	O
a	O
discriminator	O
typically	O
damages	O
the	O
generation	O
quality	O
,	O
but	O
feature	O
-	O
matching	O
reward	O
produces	O
sentences	O
with	O
much	O
better	O
diversity	O
due	O
to	O
the	O
ability	O
of	O
exploration	O
.	O

(	O
1	O
)	O
A	O
person	O
and	O
black	O
wooden	O
table	O
.	O
(	O
2	O
)	O
A	O
closeup	O
of	O
a	O
window	O
at	O
night	O
.	O
(	O
1	O
)	O
She	O
added	O
on	O
a	O
page	O
where	O
it	O
was	O
made	O
clear	O
more	O
old	O
but	O
public	O
got	O
said	O
.	O
(	O
2	O
)	O
I	O
think	O
she're	O
guys	O
in	O
four	O
years	O
,	O
and	O
more	O
after	O
it	O
played	O
well	O
enough	O
.	O
LeakGAN	O
(	O
1	O
)	O
A	O
bathroom	O
with	O
a	O
black	O
sink	O
and	O
a	O
white	O
toilet	O
next	O
to	O
a	O
tub	O
.	O
(	O
2	O
)	O
A	O
man	O
throws	O
a	O
Frisbee	O
across	O
the	O
grass	O
covered	O
yard	O
.	O
(	O
1	O
)	O
"	O
I	O
'm	O
a	O
fan	O
of	O
all	O
the	O
game	O
,	O
I	O
think	O
if	O
that	O
's	O
something	O
that	O
I	O
've	O
not	O
,	O
"	O
she	O
said	O
,	O
adding	O
that	O
he	O
would	O
not	O
be	O
decided	O
.	O
(	O
2	O
)	O
The	O
UK	O
is	O
Google	O
'	O
s	B-DatasetName
largest	O
non	O
-	O
US	O
market	O
,	O
he	O
has	O
added	O
"	O
20	O
,	O
before	O
the	O
best	O
team	O
is	O
amount	O
of	O
fewer	O
than	O
one	O
or	O
the	O
closest	O
home	O
or	O
two	O
years	O
ago	O
.	O

Original	O
:	O
the	O
service	O
was	O
slow	O
.	O
Transferred	O
:	O
the	O
service	O
was	O
fast	O
and	O
friendly	O
.	O
Original	O
:	O
i	O
would	O
never	O
eat	O
there	O
again	O
and	O
would	O
probably	O
not	O
stay	O
there	O
either	O
.	O
Transferred	O
:	O
i	O
would	O
definitely	O
eat	O
this	O
place	O
and	O
i	O
would	O
recommend	O
them	O
.	O
Table	O
8	O
:	O
Generated	O
samples	O
of	O
guided	O
style	B-TaskName
transfer	I-TaskName
.	O

We	O
have	O
proposed	O
a	O
model	O
-	O
based	O
imitationlearning	O
framework	O
for	O
adversarial	B-TaskName
text	I-TaskName
generation	O
,	O
by	O
introducing	O
a	O
guider	O
network	O
to	O
model	O
the	O
generation	O
environment	O
.	O
The	O
guider	O
network	O
provides	O
a	O
plan	O
-	O
ahead	O
mechanism	O
for	O
next	O
-	O
word	O
selection	O
.	O
Furthermore	O
,	O
this	O
framework	O
can	O
alleviate	O
the	O
sparse	O
-	O
reward	O
issue	O
,	O
as	O
the	O
intermediate	O
rewards	O
are	O
used	O
to	O
optimize	O
the	O
generator	O
.	O
Our	O
proposed	O
models	O
are	O
validated	O
on	O
both	O
unconditional	O
and	O
conditional	B-TaskName
text	I-TaskName
generation	I-TaskName
,	O
including	O
adversarial	B-TaskName
text	I-TaskName
generation	O
and	O
non	O
-	O
parallel	O
style	B-TaskName
transfer	I-TaskName
.	O
We	O
achieve	O
improved	O
performance	O
in	O
terms	O
of	O
generation	O
quality	O
and	O
diversity	O
for	O
unconditional	O
and	O
conditional	O
generation	O
tasks	O
.	O

More	O
Generated	O
Samples	O
of	O
Text	B-TaskName
Generation	I-TaskName
Table	O
13	O
lists	O
more	O
generated	O
samples	O
on	O
the	O
proposed	O
GMGAN	O
and	O
its	O
baselines	O
.	O
From	O
the	O
experiments	O
,	O
we	O
can	O
see	O
,	O
(	O
i	O
)	O
SeqGAN	O
tends	O
to	O
generate	O
shorter	O
sentences	O
,	O
and	O
the	O
readability	O
and	O
fluency	O
is	O
very	O
poor	O
.	O
(	O
ii	O
)	O
LeakGAN	O
tends	O
to	O
generate	O
very	O
long	O
sentences	O
,	O
and	O
usually	O
longer	O
than	O
the	O
original	O
sentences	O
.	O
However	O
,	O
even	O
with	O
good	O
locality	O
fluency	O
,	O
its	O
sentences	O
usually	O
are	O
not	O
semantically	O
consistent	O
.	O
By	O
contrast	O
,	O
our	O
proposed	O
GMGAN	O
can	O
generate	O
sentences	O
with	O
similar	O
length	O
to	O
the	O
original	O
sentences	O
,	O
and	O
has	O
good	O
readability	O
and	O
fluency	O
.	O
This	O
is	O
also	O
validated	O
in	O
the	O
Human	O
evaluation	O
experiment	O
.	O

at	O
time	O
t	O
+	O
t.	O
In	O
the	O
text	B-TaskName
generation	I-TaskName
setting	O
,	O
when	O
t	O
=	O
1	O
,	O
we	O
can	O
exactly	O
get	O
the	O
feature	O
representation	O
of	O
the	O
current	O
generated	O
sentence	O
if	O
the	O
guider	O
does	O
not	O
help	O
the	O
word	O
selection	O
.	O
If	O
not	O
,	O
we	O
can	O
not	O
exactly	O
get	O
this	O
feature	O
extraction	O
since	O
the	O
guider	O
's	O
prediction	O
partly	O
determine	O
next	O
token	O
.	O
In	O
practice	O
,	O
we	O
use	O
t	O
=	O
c	O
=	O
4	O
,	O
to	O
give	O
the	O
guider	O
planning	O
ability	O
,	O
to	O
help	O
for	O
word	O
selection	O
and	O
guide	O
sentence	O
generation	O
.	O

The	O
LSTM	B-MethodName
state	O
of	O
dimension	O
for	O
the	O
generator	O
is	O
300	O
,	O
and	O
the	O
LSTM	B-MethodName
state	O
of	O
dimension	O
for	O
the	O
guider	O
is	O
300	O
.	O
The	O
dimension	O
of	O
word	O
-	O
embedding	O
is	O
300	O
.	O

Acknowledgement	O
The	O
authors	O
would	O
like	O
to	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
insightful	O
comments	O
.	O
The	O
research	O
was	O
supported	O
in	O
part	O
by	O
DARPA	B-DatasetName
,	O
DOE	O
,	O
NIH	O
,	O
NSF	O
and	O
ONR	O
.	O

O	O
t	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
y	O
l	O
D	O
Y	O
v	O
N	O
J	O
9	O
i	O
T	O
d	O
4	O
H	O
B	O
V	O
D	O
R	O
Q	O
f	O
3	O
c	O
g	O
H	O
t	O
3	O
s	O
w	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
U	O
G	O
9	O
F	O
L	O
9	O
6	O
s	O
a	O
G	O
y	O
h	O
D	O
W	O
W	O
z	O
3	O
b	O
R	O
L	O
N	O
5	O
u	O
w	O
O	O
x	O
F	O
K	O
6	O
E	O
/	O
w	O
4	O
k	O
H	O
F	O
q	O
/	O
/	O
I	O
m	O
/	O
/	O
G	O
b	O
Z	O
u	O
D	O
V	O
h	O
8	O
M	O
P	O
N	O
6	O
b	O
Y	O
W	O
Z	O
e	O
m	O
E	O
p	O
h	O
0	B-DatasetName
H	O
W	O
/	O
n	O
N	O
L	O
S	O
8	O
s	O
r	O
q	O
W	O
n	O
m	O
9	O
s	O
r	O
G	O
5	O
t	O
b	O
1	O
T	O
3	O
d	O
1	O
7	O
M	O
E	O
m	O
m	O
G	O
f	O
d	O
Z	O
I	O
h	O
P	O
d	O
D	O
q	O
n	O
h	O
U	O
i	O
j	O
u	O
o	O
0	B-DatasetName
D	O
J	O
2	O
6	O
n	O
m	O
N	O
A	O
4	O
l	O
b	O
4	O
W	O
j	O
q	O
6	O
n	O
f	O
e	O
u	O
T	O
a	O
i	O
E	O
T	O
d	O
4	O
z	O
j	O
l	O
Q	O
U	O
w	O
H	O
S	O
k	O
S	O
C	O
U	O
b	O
T	O
S	O
3	O
U	O
0	B-DatasetName
P	O
e	O
9	O
W	O
a	O
W	O
3	O
d	O
n	O
I	O
H	O
+	O
J	O
V	O
5	O
A	O
a	O
F	O
G	O
j	O
2	O
q	O
p	O
/	O
d	O
f	O
s	O
K	O
y	O
m	O
C	O
t	O
k	O
k	O
h	O
r	O
T	O
8	O
d	O
w	O
U	O
g	O
5	O
x	O
q	O
F	O
E	O
z	O
y	O
S	O
a	O
W	O
b	O
G	O
Z	O
5	O
S	O
N	O
q	O
I	O
D	O
3	O
r	O
F	O
U	O
0	B-DatasetName
Z	O
i	O
b	O
I	O
J	O
+	O
d	O
O	O
i	O
F	O
H	O
V	O
u	O
m	O
T	O
K	O
N	O
G	O
2	O
F	O
J	O
K	O
Z	O
+	O
n	O
M	O
i	O
p	O
7	O
E	O
x	O
4	O
z	O
i	O
0	B-DatasetName
n	O
T	O
H	O
F	O
o	O
V	O
n	O
0	B-DatasetName
p	O
u	O
J	O
/	O
X	O
i	O
f	O
D	O
6	O
D	O
z	O
I	O
h	O
U	O
o	O
z	O
5	O
I	O
r	O
N	O
F	O
0	B-DatasetName
W	O
Z	O
J	O
J	O
i	O
Q	O
6	O
d	O
+	O
k	O
L	O
z	O
R	O
n	O
K	O
M	O
e	O
W	O
U	O
K	O
a	O
F	O
v	O
Z	O
W	O
w	O
I	O
d	O
W	O
U	O
o	O
U	O
2	O
n	O
Y	O
k	O
P	O
w	O
F	O
l	O
/	O
+	O
S	O
/	O
y	O
T	O
+	O
k	O
X	O
d	O
u	O
z	O
2	O
t	O
N	O
S	O
6	O
L	O
N	O
M	O
p	O
w	O
A	O
I	O
d	O
w	O
D	O
B	O
6	O
c	O
Q	O
Q	O
O	O
u	O
o	O
Q	O
k	O
+	O
M	O
B	O
j	O
A	O
E	O
7	O
z	O
A	O
q	O
y	O
O	O
d	O
Z	O
+	O
f	O
N	O
e	O
Z	O
+	O
3	O
l	O
p	O
x	O
i	O
Z	O
h	O
9	O
+	O
w	O
f	O
n	O
4	O
B	O
q	O
B	O
x	O
j	O
Y	O
4	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
y	O
l	O
D	O
Y	O
v	O
N	O
J	O
9	O
i	O
T	O
d	O
4	O
H	O
B	O
V	O
D	O
R	O
Q	O
f	O
3	O
c	O
g	O
H	O
t	O
3	O
s	O
w	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
U	O
G	O
9	O
F	O
L	O
9	O
6	O
s	O
a	O
G	O
y	O
h	O
D	O
W	O
W	O
z	O
3	O
b	O
R	O
L	O
N	O
5	O
u	O
w	O
O	O
x	O
F	O
K	O
6	O
E	O
/	O
w	O
4	O
k	O
H	O
F	O
q	O
/	O
/	O
I	O
m	O
/	O
/	O
G	O
b	O
Z	O
u	O
D	O
V	O
h	O
8	O
M	O
P	O
N	O
6	O
b	O
Y	O
W	O
Z	O
e	O
m	O
E	O
p	O
h	O
0	B-DatasetName
H	O
W	O
/	O
n	O
N	O
L	O
S	O
8	O
s	O
r	O
q	O
W	O
n	O
m	O
9	O
s	O
r	O
G	O
5	O
t	O
b	O
1	O
T	O
3	O
d	O
1	O
7	O
M	O
E	O
m	O
m	O
G	O
f	O
d	O
Z	O
I	O
h	O
P	O
d	O
D	O
q	O
n	O
h	O
U	O
i	O
j	O
u	O
o	O
0	B-DatasetName
D	O
J	O
2	O
6	O
n	O
m	O
N	O
A	O
4	O
l	O
b	O
4	O
W	O
j	O
q	O
6	O
n	O
f	O
e	O
u	O
T	O
a	O
i	O
E	O
T	O
d	O
4	O
z	O
j	O
l	O
Q	O
U	O
w	O
H	O
S	O
k	O
S	O
C	O
U	O
b	O
T	O
S	O
3	O
U	O
0	B-DatasetName
P	O
e	O
9	O
W	O
a	O
W	O
3	O
d	O
n	O
I	O
H	O
+	O
J	O
V	O
5	O
A	O
a	O
F	O
G	O
j	O
2	O
q	O
p	O
/	O
d	O
f	O
s	O
K	O
y	O
m	O
C	O
t	O
k	O
k	O
h	O
r	O
T	O
8	O
d	O
w	O
U	O
g	O
5	O
x	O
q	O
F	O
E	O
z	O
y	O
S	O
a	O
W	O
b	O
G	O
Z	O
5	O
S	O
N	O
q	O
I	O
D	O
3	O
r	O
F	O
U	O
0	B-DatasetName
Z	O
i	O
b	O
I	O
J	O
+	O
d	O
O	O
i	O
F	O
H	O
V	O
u	O
m	O
T	O
K	O
N	O
G	O
2	O
F	O
J	O
K	O
Z	O
+	O
n	O
M	O
i	O
p	O
7	O
E	O
x	O
4	O
z	O
i	O
0	B-DatasetName
n	O
T	O
H	O
F	O
o	O
V	O
n	O
0	B-DatasetName
p	O
u	O
J	O
/	O
X	O
i	O
f	O
D	O
6	O
D	O
z	O
I	O
h	O
U	O
o	O
z	O
5	O
I	O
r	O
N	O
F	O
0	B-DatasetName
W	O
Z	O
J	O
J	O
i	O
Q	O
6	O
d	O
+	O
k	O
L	O
z	O
R	O
n	O
K	O
M	O
e	O
W	O
U	O
K	O
a	O
F	O
v	O
Z	O
W	O
w	O
I	O
d	O
W	O
U	O
o	O
U	O
2	O
n	O
Y	O
k	O
P	O
w	O
F	O
l	O
/	O
+	O
S	O
/	O
y	O
T	O
+	O
k	O
X	O
d	O
u	O
z	O
2	O
t	O
N	O
S	O
6	O
L	O
N	O
M	O
p	O
w	O
A	O
I	O
d	O
w	O
D	O
B	O
6	O
c	O
Q	O
Q	O
O	O
u	O
o	O
Q	O
k	O
+	O
M	O
B	O
j	O
A	O
E	O
7	O
z	O
A	O
q	O
y	O
O	O
d	O
Z	O
+	O
f	O
N	O
e	O
Z	O
+	O
3	O
l	O
p	O
x	O
i	O
Z	O
h	O
9	O
+	O
w	O
f	O
n	O
4	O
B	O
q	O
B	O
x	O
j	O
Y	O
4	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
y	O
l	O
D	O
Y	O
v	O
N	O
J	O
9	O
i	O
T	O
d	O
4	O
H	O
B	O
V	O
D	O
R	O
Q	O
f	O
3	O
c	O
g	O
H	O
t	O
3	O
s	O
w	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
U	O
G	O
9	O
F	O
L	O
9	O
6	O
s	O
a	O
G	O
y	O
h	O
D	O
W	O
W	O
z	O
3	O
b	O
R	O
L	O
N	O
5	O
u	O
w	O
O	O
x	O
F	O
K	O
6	O
E	O
/	O
w	O
4	O
k	O
H	O
F	O
q	O
/	O
/	O
I	O
m	O
/	O
/	O
G	O
b	O
Z	O
u	O
D	O
V	O
h	O
8	O
M	O
P	O
N	O
6	O
b	O
Y	O
W	O
Z	O
e	O
m	O
E	O
p	O
h	O
0	B-DatasetName
H	O
W	O
/	O
n	O
N	O
L	O
S	O
8	O
s	O
r	O
q	O
W	O
n	O
m	O
9	O
s	O
r	O
G	O
5	O
t	O
b	O
1	O
T	O
3	O
d	O
1	O
7	O
M	O
E	O
m	O
m	O
G	O
f	O
d	O
Z	O
I	O
h	O
P	O
d	O
D	O
q	O
n	O
h	O
U	O
i	O
j	O
u	O
o	O
0	B-DatasetName
D	O
J	O
2	O
6	O
n	O
m	O
N	O
A	O
4	O
l	O
b	O
4	O
W	O
j	O
q	O
6	O
n	O
f	O
e	O
u	O
T	O
a	O
i	O
E	O
T	O
d	O
4	O
z	O
j	O
l	O
Q	O
U	O
w	O
H	O
S	O
k	O
S	O
C	O
U	O
b	O
T	O
S	O
3	O
U	O
0	B-DatasetName
P	O
e	O
9	O
W	O
a	O
W	O
3	O
d	O
n	O
I	O
H	O
+	O
J	O
V	O
5	O
A	O
a	O
F	O
G	O
j	O
2	O
q	O
p	O
/	O
d	O
f	O
s	O
K	O
y	O
m	O
C	O
t	O
k	O
k	O
h	O
r	O
T	O
8	O
d	O
w	O
U	O
g	O
5	O
x	O
q	O
F	O
E	O
z	O
y	O
S	O
a	O
W	O
b	O
G	O
Z	O
5	O
S	O
N	O
q	O
I	O
D	O
3	O
r	O
F	O
U	O
0	B-DatasetName
Z	O
i	O
b	O
I	O
J	O
+	O
d	O
O	O
i	O
F	O
H	O
V	O
u	O
m	O
T	O
K	O
N	O
G	O
2	O
F	O
J	O
K	O
Z	O
+	O
n	O
M	O
i	O
p	O
7	O
E	O
x	O
4	O
z	O
i	O
0	B-DatasetName
n	O
T	O
H	O
F	O
o	O
V	O
n	O
0	B-DatasetName
p	O
u	O
J	O
/	O
X	O
i	O
f	O
D	O
6	O
D	O
z	O
I	O
h	O
U	O
o	O
z	O
5	O
I	O
r	O
N	O
F	O
0	B-DatasetName
W	O
Z	O
J	O
J	O
i	O
Q	O
6	O
d	O
+	O
k	O
L	O
z	O
R	O
n	O
K	O
M	O
e	O
W	O
U	O
K	O
a	O
F	O
v	O
Z	O
W	O
w	O
I	O
d	O
W	O
U	O
o	O
U	O
2	O
n	O
Y	O
k	O
P	O
w	O
F	O
l	O
/	O
+	O
S	O
/	O
y	O
T	O
+	O
k	O
X	O
d	O
u	O
z	O
2	O
t	O
N	O
S	O
6	O
L	O
N	O
M	O
p	O
w	O
A	O
I	O
d	O
w	O
D	O
B	O
6	O
c	O
Q	O
Q	O
O	O
u	O
o	O
Q	O
k	O
+	O
M	O
B	O
j	O
A	O
E	O
7	O
z	O
A	O
q	O
y	O
O	O
d	O
Z	O
+	O
f	O
N	O
e	O
Z	O
+	O
3	O
l	O
p	O
x	O
i	O
Z	O
h	O
9	O
+	O
w	O
f	O
n	O
4	O
B	O
q	O
B	O
x	O
j	O
Y	O
4	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
X	O
/	O
B	O
b	O
P	O
P	O
Q	O
R	O
M	O
1	O
p	O
m	O
B	O
h	O
x	O
d	O
K	O
1	O
e	O
n	O
S	O
b	O
L	O
+	O
g	O
J	O
w	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
2	O
H	O
i	O
c	O
b	O
Z	O
D	O
N	O
S	O
g	O
M	O
x	O
F	O
I	O
X	O
v	O
1	O
L	O
8	O
6	O
V	O
q	O
1	O
r	O
N	O
8	O
E	O
i	O
u	O
C	O
o	O
z	O
b	O
t	O
S	O
d	O
4	O
M	O
Z	O
l	O
B	O
c	O
c	O
W	O
2	O
q	O
F	O
k	O
M	O
n	O
f	O
a	O
0	B-DatasetName
E	O
x	O
m	O
S	O
O	O
4	O
I	O
p	O
f	O
Q	O
F	O
X	O
L	O
h	O
R	O
f	O
D	O
B	O
3	O
v	O
o	O
3	O
p	O
z	O
0	B-DatasetName
K	O
t	O
B	O
w	O
I	O
f	O
5	O
y	O
T	O
k	O
3	O
p	O
O	O
U	O
S	O
l	O
o	O
K	O
g	O
i	O
+	O
v	O
t	O
r	O
W	O
9	O
s	O
7	O
t	O
X	O
3	O
/	O
c	O
P	O
G	O
v	O
7	O
h	O
0	B-DatasetName
X	O
G	O
z	O
8	O
W	O
S	O
L	O
y	O
g	O
i	O
M	O
R	O
K	O
E	O
K	O
0	B-DatasetName
0	B-DatasetName
u	O
4	O
R	O
S	O
U	O
1	O
R	O
i	O
R	O
J	O
Y	O
a	O
8	O
0	B-DatasetName
y	O
P	O
N	O
E	O
Y	O
T	O
e	O
Z	O
3	O
C	O
3	O
y	O
7	O
j	O
M	O
a	O
K	O
w	O
v	O
9	O
S	O
N	O
M	O
S	O
4	O
5	O
y	O
P	O
t	O
M	O
y	O
k	O
4	O
O	O
S	O
s	O
z	O
r	O
D	O
Z	O
C	O
t	O
r	O
B	O
U	O
m	O
w	O
T	O
w	O
j	O
W	O
0	B-DatasetName
Y	O
K	O
1	O
h	O
8	O
3	O
O	O
Q	O
F	O
q	O
L	O
K	O
U	O
Z	O
N	O
Q	O
3	O
N	O
p	O
+	O
G	O
J	O
Q	O
U	O
z	O
7	O
g	O
h	O
K	O
R	O
T	O
O	O
/	O
U	O
F	O
l	O
s	O
e	O
R	O
i	O
w	O
k	O
f	O
Y	O
d	O
6	O
h	O
5	O
j	O
j	O
a	O
e	O
L	O
c	O
e	O
c	O
s	O
3	O
P	O
n	O
p	O
C	O
w	O
r	O
j	O
D	O
u	O
a	O
2	O
N	O
L	O
9	O
+	O
W	O
L	O
G	O
c	O
2	O
u	O
n	O
e	O
e	O
J	O
u	O
5	O
p	O
z	O
G	O
9	O
m	O
+	O
2	O
M	O
P	O
/	O
L	O
+	O
h	O
V	O
l	O
1	O
/	O
F	O
M	O
6	O
r	O
I	O
i	O
1	O
G	O
L	O
1	O
U	O
V	O
Y	O
p	O
R	O
g	O
V	O
b	O
7	O
M	O
x	O
S	O
a	O
V	O
C	O
Q	O
m	O
j	O
r	O
g	O
w	O
k	O
g	O
3	O
K	O
x	O
N	O
j	O
b	O
r	O
g	O
g	O
1	O
4	O
z	O
v	O
O	O
g	O
j	O
/	O
b	O
r	O
w	O
J	O
0	B-DatasetName
W	O
X	O
7	O
p	O
h	O
0	B-DatasetName
+	O
B	O
F	O
C	O
H	O
U	O
z	O
i	O
D	O
C	O
w	O
j	O
h	O
C	O
m	O
7	O
h	O
H	O
j	O
o	O
Q	O
g	O
Y	O
A	O
U	O
X	O
u	O
D	O
N	O
G	O
3	O
u	O
v	O
3	O
v	O
u	O
q	O
q	O
p	O
q	O
3	O
7	O
u	O
w	O
E	O
f	O
s	O
n	O
7	O
+	O
A	O
a	O
q	O
K	O
Y	O
o	O
N	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
L	O
Q	O
y	O
c	O
e	O
l	O
W	O
j	O
n	O
/	O
7	O
H	O
2	O
e	O
E	O
g	O
a	O
t	O
v	O
n	O
P	O
8	O
L	O
r	O
W	O
o	O
I	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
3	O
n	O
i	O
c	O
b	O
Z	O
D	O
N	O
S	O
g	O
M	O
x	O
F	O
I	O
X	O
v	O
+	O
F	O
t	O
r	O
1	O
e	O
r	O
W	O
T	O
b	O
A	O
I	O
r	O
s	O
q	O
M	O
G	O
3	O
U	O
n	O
u	O
H	O
F	O
n	O
R	O
c	O
c	O
W	O
2	O
q	O
F	O
k	O
0	B-DatasetName
j	O
t	O
t	O
a	O
C	O
Y	O
z	O
J	O
H	O
e	O
E	O
U	O
v	O
o	O
I	O
b	O
l	O
y	O
o	O
+	O
F	O
j	O
u	O
f	O
B	O
v	O
T	O
n	O
4	O
W	O
2	O
H	O
g	O
h	O
8	O
n	O
J	O
O	O
Q	O
e	O
0	B-DatasetName
+	O
c	O
K	O
2	O
n	O
J	O
9	O
7	O
+	O
9	O
t	O
f	O
W	O
N	O
z	O
a	O
3	O
t	O
0	B-DatasetName
k	O
5	O
5	O
t	O
7	O
K	O
3	O
f	O
1	O
A	O
9	O
r	O
D	O
z	O
Z	O
r	O
D	O
A	O
C	O
Q	O
5	O
G	O
p	O
z	O
L	O
R	O
i	O
b	O
l	O
F	O
J	O
j	O
S	O
F	O
J	O
U	O
t	O
j	O
K	O
D	O
f	O
I	O
0	B-DatasetName
V	O
t	O
i	O
M	O
h	O
z	O
f	O
T	O
v	O
P	O
m	O
M	O
x	O
s	O
p	O
M	O
P	O
9	O
I	O
o	O
x	O
y	O
j	O
l	O
f	O
S	O
0	B-DatasetName
T	O
K	O
T	O
g	O
5	O
6	O
+	O
G	O
u	O
S	O
9	O
1	O
q	O
z	O
a	O
/	O
7	O
M	O
7	O
F	O
V	O
C	O
B	O
Z	O
Q	O
g	O
4	O
U	O
a	O
3	O
e	O
p	O
X	O
p	O
5	O
e	O
J	O
I	O
k	O
V	O
N	O
Q	O
n	O
F	O
r	O
2	O
4	O
G	O
f	O
U	O
z	O
T	O
m	O
h	O
q	O
R	O
Q	O
O	O
C	O
l	O
3	O
C	O
o	O
s	O
5	O
F	O
0	B-DatasetName
P	O
e	O
x	O
7	O
Z	O
D	O
z	O
V	O
O	O
0	B-DatasetName
0	B-DatasetName
X	O
g	O
2	O
6	O
o	O
S	O
d	O
O	O
q	O
f	O
H	O
k	O
s	O
y	O
4	O
o	O
4	O
n	O
N	O
3	O
N	O
8	O
v	O
x	O
j	O
y	O
1	O
d	O
p	O
T	O
G	O
7	O
m	O
b	O
K	O
a	O
W	O
C	O
X	O
s	O
6	O
n	O
5	O
X	O
9	O
Y	O
u	O
K	O
L	O
m	O
M	O
x	O
l	O
L	O
n	O
B	O
a	O
E	O
W	O
8	O
4	O
+	O
S	O
Q	O
j	O
H	O
K	O
2	O
H	O
R	O
v	O
1	O
p	O
M	O
G	O
B	O
a	O
m	O
R	O
A	O
y	O
6	O
M	O
d	O
L	O
M	O
y	O
M	O
e	O
C	O
G	O
C	O
3	O
L	O
t	O
l	O
F	O
0	B-DatasetName
J	O
w	O
f	O
L	O
K	O
q	O
x	O
C	O
e	O
1	O
6	O
/	O
q	O
w	O
b	O
0	B-DatasetName
P	O
J	O
T	O
i	O
G	O
E	O
z	O
i	O
D	O
A	O
C	O
7	O
g	O
G	O
m	O
6	O
h	O
A	O
S	O
E	O
I	O
6	O
M	O
M	O
L	O
v	O
M	O
G	O
7	O
p	O
7	O
x	O
X	O
7	O
2	O
P	O
e	O
1	O
p	O
q	O
3	O
q	O
O	O
0	B-DatasetName
I	O
/	O
s	O
j	O
7	O
/	O
A	O
G	O
F	O
6	O
Y	O
w	O
6	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
L	O
Q	O
y	O
c	O
e	O
l	O
W	O
j	O
n	O
/	O
7	O
H	O
2	O
e	O
E	O
g	O
a	O
t	O
v	O
n	O
P	O
8	O
L	O
r	O
W	O
o	O
I	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
3	O
n	O
i	O
c	O
b	O
Z	O
D	O
N	O
S	O
g	O
M	O
x	O
F	O
I	O
X	O
v	O
+	O
F	O
t	O
r	O
1	O
e	O
r	O
W	O
T	O
b	O
A	O
I	O
r	O
s	O
q	O
M	O
G	O
3	O
U	O
n	O
u	O
H	O
F	O
n	O
R	O
c	O
c	O
W	O
2	O
q	O
F	O
k	O
0	B-DatasetName
j	O
t	O
t	O
a	O
C	O
Y	O
z	O
J	O
H	O
e	O
E	O
U	O
v	O
o	O
I	O
b	O
l	O
y	O
o	O
+	O
F	O
j	O
u	O
f	O
B	O
v	O
T	O
n	O
4	O
W	O
2	O
H	O
g	O
h	O
8	O
n	O
J	O
O	O
Q	O
e	O
0	B-DatasetName
+	O
c	O
K	O
2	O
n	O
J	O
9	O
7	O
+	O
9	O
t	O
f	O
W	O
N	O
z	O
a	O
3	O
t	O
0	B-DatasetName
k	O
5	O
5	O
t	O
7	O
K	O
3	O
f	O
1	O
A	O
9	O
r	O
D	O
z	O
Z	O
r	O
D	O
A	O
C	O
Q	O
5	O
G	O
p	O
z	O
L	O
R	O
i	O
b	O
l	O
F	O
J	O
j	O
S	O
F	O
J	O
U	O
t	O
j	O
K	O
D	O
f	O
I	O
0	B-DatasetName
V	O
t	O
i	O
M	O
h	O
z	O
f	O
T	O
v	O
P	O
m	O
M	O
x	O
s	O
p	O
M	O
P	O
9	O
I	O
o	O
x	O
y	O
j	O
l	O
f	O
S	O
0	B-DatasetName
T	O
K	O
T	O
g	O
5	O
6	O
+	O
G	O
u	O
S	O
9	O
1	O
q	O
z	O
a	O
/	O
7	O
M	O
7	O
F	O
V	O
C	O
B	O
Z	O
Q	O
g	O
4	O
U	O
a	O
3	O
e	O
p	O
X	O
p	O
5	O
e	O
J	O
I	O
k	O
V	O
N	O
Q	O
n	O
F	O
r	O
2	O
4	O
G	O
f	O
U	O
z	O
T	O
m	O
h	O
q	O
R	O
Q	O
O	O
C	O
l	O
3	O
C	O
o	O
s	O
5	O
F	O
0	B-DatasetName
P	O
e	O
x	O
7	O
Z	O
D	O
z	O
V	O
O	O
0	B-DatasetName
0	B-DatasetName
X	O
g	O
2	O
6	O
o	O
S	O
d	O
O	O
q	O
f	O
H	O
k	O
s	O
y	O
4	O
o	O
4	O
n	O
N	O
3	O
N	O
8	O
v	O
x	O
j	O
y	O
1	O
d	O
p	O
T	O
G	O
7	O
m	O
b	O
K	O
a	O
W	O
C	O
X	O
s	O
6	O
n	O
5	O
X	O
9	O
Y	O
u	O
K	O
L	O
m	O
M	O
x	O
l	O
L	O
n	O
B	O
a	O
E	O
W	O
8	O
4	O
+	O
S	O
Q	O
j	O
H	O
K	O
2	O
H	O
R	O
v	O
1	O
p	O
M	O
G	O
B	O
a	O
m	O
R	O
A	O
y	O
6	O
M	O
d	O
L	O
M	O
y	O
M	O
e	O
C	O
G	O
C	O
3	O
L	O
t	O
l	O
F	O
0	B-DatasetName
J	O
w	O
f	O
L	O
K	O
q	O
x	O
C	O
e	O
1	O
6	O
/	O
q	O
w	O
b	O
0	B-DatasetName
P	O
J	O
T	O
i	O
G	O
E	O
z	O
i	O
D	O
A	O
C	O
7	O
g	O
G	O
m	O
6	O
h	O
A	O
S	O
E	O
I	O
6	O
M	O
M	O
L	O
v	O
M	O
G	O
7	O
p	O
7	O
x	O
X	O
7	O
2	O
P	O
e	O
1	O
p	O
q	O
3	O
q	O
O	O
0	B-DatasetName
I	O
/	O
s	O
j	O
7	O
/	O
A	O
G	O
F	O
6	O
Y	O
w	O
6	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
k	O
l	O
y	O
C	O
Z	O
f	O
z	O
i	O
j	O
r	O
C	O
U	O
f	O
s	O
q	O
x	O
X	O
J	O
j	O
p	O
o	O
q	O
f	O
K	O
G	O
D	O
A	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
m	O
8	O
q	O
L	O
e	O
i	O
F	O
2	O
9	O
W	O
N	O
L	O
b	O
Q	O
h	O
r	O
L	O
Z	O
b	O
t	O
q	O
l	O
m	O
0	B-DatasetName
3	O
Y	O
n	O
Q	O
g	O
l	O
9	O
C	O
d	O
4	O
8	O
a	O
D	O
i	O
1	O
X	O
/	O
k	O
z	O
X	O
/	O
j	O
t	O
s	O
1	O
B	O
W	O
x	O
8	O
M	O
P	O
N	O
6	O
b	O
Y	O
W	O
Z	O
e	O
m	O
E	O
p	O
h	O
0	B-DatasetName
H	O
W	O
/	O
n	O
d	O
L	O
K	O
6	O
t	O
r	O
6	O
R	O
n	O
m	O
z	O
s	O
r	O
W	O
9	O
s	O
7	O
t	O
X	O
3	O
T	O
9	O
4	O
N	O
E	O
m	O
m	O
G	O
f	O
d	O
Z	O
I	O
h	O
P	O
d	O
D	O
q	O
n	O
h	O
U	O
i	O
j	O
u	O
o	O
0	B-DatasetName
D	O
J	O
2	O
6	O
n	O
m	O
N	O
A	O
4	O
l	O
b	O
4	O
W	O
j	O
6	O
6	O
n	O
f	O
e	O
u	O
L	O
a	O
i	O
E	O
Q	O
9	O
4	O
D	O
j	O
l	O
Q	O
U	O
w	O
H	O
S	O
k	O
S	O
C	O
U	O
b	O
T	O
S	O
/	O
W	O
0	B-DatasetName
P	O
e	O
9	O
W	O
a	O
W	O
3	O
d	O
n	O
I	O
M	O
v	O
E	O
K	O
0	B-DatasetName
g	O
N	O
C	O
j	O
R	O
7	O
1	O
a	O
9	O
u	O
P	O
2	O
F	O
Z	O
z	O
B	O
U	O
y	O
S	O
Y	O
3	O
p	O
e	O
G	O
6	O
K	O
Q	O
U	O
4	O
1	O
C	O
i	O
b	O
5	O
p	O
N	O
L	O
N	O
D	O
E	O
8	O
p	O
G	O
9	O
E	O
B	O
7	O
1	O
i	O
q	O
a	O
M	O
x	O
N	O
k	O
M	O
9	O
O	O
n	O
Z	O
A	O
T	O
q	O
/	O
R	O
J	O
l	O
G	O
h	O
b	O
C	O
s	O
l	O
M	O
/	O
T	O
2	O
R	O
0	B-DatasetName
9	O
i	O
Y	O
c	O
R	O
z	O
a	O
z	O
p	O
j	O
i	O
0	B-DatasetName
C	O
x	O
6	O
U	O
/	O
E	O
/	O
r	O
5	O
N	O
h	O
d	O
B	O
H	O
k	O
Q	O
q	O
U	O
Z	O
c	O
s	O
X	O
m	O
i	O
6	O
J	O
M	O
E	O
k	O
z	O
I	O
9	O
G	O
/	O
S	O
F	O
5	O
o	O
z	O
l	O
G	O
N	O
L	O
K	O
N	O
P	O
C	O
3	O
k	O
r	O
Y	O
k	O
G	O
r	O
K	O
0	B-DatasetName
K	O
Z	O
T	O
s	O
S	O
F	O
4	O
i	O
y	O
8	O
v	O
E	O
/	O
+	O
s	O
f	O
l	O
n	O
3	O
7	O
t	O
x	O
a	O
4	O
6	O
p	O
I	O
o	O
w	O
x	O
H	O
c	O
A	O
y	O
n	O
4	O
M	O
E	O
5	O
N	O
O	O
A	O
G	O
m	O
u	O
A	O
D	O
g	O
w	O
E	O
8	O
w	O
y	O
u	O
8	O
O	O
d	O
J	O
5	O
c	O
d	O
6	O
d	O
j	O
3	O
l	O
r	O
y	O
S	O
l	O
m	O
D	O
u	O
E	O
P	O
n	O
M	O
8	O
f	O
n	O
z	O
G	O
N	O
i	O
g	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
y	O
l	O
D	O
Y	O
v	O
N	O
J	O
9	O
i	O
T	O
d	O
4	O
f	O
N	O
e	O
Z	O
+	O
3	O
l	O
p	O
x	O
i	O
Z	O
h	O
9	O
+	O
w	O
f	O
n	O
4	O
B	O
q	O
B	O
x	O
j	O
Y	O
4	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
f	O
t	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
z	O
U	O
m	O
7	O
9	O
l	O
o	O
9	O
e	O
Y	O
U	O
U	O
g	O
i	O
5	O
D	O
W	O
6	O
o	O
p	O
C	O
U	O
a	O
4	O
f	O
s	O
s	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
U	O
G	O
9	O
F	O
L	O
x	O
4	O
r	O
G	O
l	O
t	O
o	O
Q	O
9	O
l	O
s	O
N	O
+	O
3	O
S	O
z	O
S	O
b	O
s	O
T	O
o	O
Q	O
S	O
+	O
h	O
O	O
8	O
e	O
F	O
D	O
x	O
6	O
j	O
/	O
y	O
5	O
r	O
9	O
x	O
2	O
+	O
a	O
g	O
r	O
Q	O
8	O
G	O
H	O
u	O
/	O
N	O
M	O
D	O
M	O
v	O
T	O
K	O
U	O
w	O
6	O
L	O
r	O
f	O
T	O
m	O
l	O
l	O
d	O
W	O
1	O
9	O
o	O
7	O
x	O
Z	O
2	O
d	O
r	O
e	O
2	O
d	O
2	O
r	O
7	O
h	O
8	O
8	O
m	O
i	O
T	O
T	O
j	O
P	O
s	O
s	O
k	O
Y	O
l	O
u	O
h	O
9	O
R	O
w	O
K	O
R	O
T	O
3	O
U	O
a	O
D	O
k	O
7	O
V	O
R	O
z	O
G	O
o	O
e	O
S	O
t	O
8	O
L	O
R	O
z	O
d	O
R	O
v	O
P	O
X	O
F	O
t	O
R	O
K	O
I	O
e	O
c	O
J	O
z	O
y	O
I	O
K	O
Y	O
D	O
J	O
S	O
L	O
B	O
K	O
F	O
r	O
p	O
P	O
u	O
p	O
h	O
r	O
1	O
p	O
z	O
6	O
+	O
4	O
M	O
Z	O
J	O
l	O
4	O
B	O
a	O
l	O
B	O
g	O
W	O
a	O
v	O
+	O
t	O
X	O
t	O
J	O
y	O
y	O
L	O
u	O
U	O
I	O
m	O
q	O
T	O
E	O
d	O
z	O
0	B-DatasetName
0	B-DatasetName
7	O
y	O
1	O
5	O
B	O
Q	O
z	O
h	O
/	O
A	O
H	O
z	O
u	O
c	O
P	O
w	O
2	O
S	O
N	O
p	O
Q	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
z	O
U	O
m	O
7	O
9	O
l	O
o	O
9	O
e	O
Y	O
U	O
U	O
g	O
i	O
5	O
D	O
W	O
6	O
o	O
p	O
C	O
U	O
a	O
4	O
f	O
s	O
s	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
U	O
G	O
9	O
F	O
L	O
x	O
4	O
r	O
G	O
l	O
t	O
o	O
Q	O
9	O
l	O
s	O
N	O
+	O
3	O
S	O
z	O
S	O
b	O
s	O
T	O
o	O
Q	O
S	O
+	O
h	O
O	O
8	O
e	O
F	O
D	O
x	O
6	O
j	O
/	O
y	O
5	O
r	O
9	O
x	O
2	O
+	O
a	O
g	O
r	O
Q	O
8	O
G	O
H	O
u	O
/	O
N	O
M	O
D	O
M	O
v	O
T	O
K	O
U	O
w	O
6	O
L	O
r	O
f	O
T	O
m	O
l	O
l	O
d	O
W	O
1	O
9	O
o	O
7	O
x	O
Z	O
2	O
d	O
r	O
e	O
2	O
d	O
2	O
r	O
7	O
h	O
8	O
8	O
m	O
i	O
T	O
T	O
j	O
P	O
s	O
s	O
k	O
Y	O
l	O
u	O
h	O
9	O
R	O
w	O
K	O
R	O
T	O
3	O
U	O
a	O
D	O
k	O
7	O
V	O
R	O
z	O
G	O
o	O
e	O
S	O
t	O
8	O
L	O
R	O
z	O
d	O
R	O
v	O
P	O
X	O
F	O
t	O
R	O
K	O
I	O
e	O
c	O
J	O
z	O
y	O
I	O
K	O
Y	O
D	O
J	O
S	O
L	O
B	O
K	O
F	O
r	O
p	O
P	O
u	O
p	O
h	O
r	O
1	O
p	O
z	O
6	O
+	O
4	O
M	O
Z	O
J	O
l	O
4	O
B	O
a	O
l	O
B	O
g	O
W	O
a	O
v	O
+	O
t	O
X	O
t	O
J	O
y	O
y	O
L	O
u	O
U	O
I	O
m	O
q	O
T	O
E	O
d	O
z	O
0	B-DatasetName
0	B-DatasetName
7	O
y	O
1	O
5	O
B	O
Q	O
z	O
h	O
/	O
A	O
H	O
z	O
u	O
c	O
P	O
w	O
2	O
S	O
N	O
p	O
Q	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
z	O
U	O
m	O
7	O
9	O
l	O
o	O
9	O
e	O
Y	O
U	O
U	O
g	O
i	O
5	O
D	O
W	O
6	O
o	O
p	O
C	O
U	O
a	O
4	O
f	O
s	O
s	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
U	O
G	O
9	O
F	O
L	O
x	O
4	O
r	O
G	O
l	O
t	O
o	O
Q	O
9	O
l	O
s	O
N	O
+	O
3	O
S	O
z	O
S	O
b	O
s	O
T	O
o	O
Q	O
S	O
+	O
h	O
O	O
8	O
e	O
F	O
D	O
x	O
6	O
j	O
/	O
y	O
5	O
r	O
9	O
x	O
2	O
+	O
a	O
g	O
r	O
Q	O
8	O
G	O
H	O
u	O
/	O
N	O
M	O
D	O
M	O
v	O
T	O
K	O
U	O
w	O
6	O
L	O
r	O
f	O
T	O
m	O
l	O
l	O
d	O
W	O
1	O
9	O
o	O
7	O
x	O
Z	O
2	O
d	O
r	O
e	O
2	O
d	O
2	O
r	O
7	O
h	O
8	O
8	O
m	O
i	O
T	O
T	O
j	O
P	O
s	O
s	O
k	O
Y	O
l	O
u	O
h	O
9	O
R	O
w	O
K	O
R	O
T	O
3	O
U	O
a	O
D	O
k	O
7	O
V	O
R	O
z	O
G	O
o	O
e	O
S	O
t	O
8	O
L	O
R	O
z	O
d	O
R	O
v	O
P	O
X	O
F	O
t	O
R	O
K	O
I	O
e	O
c	O
J	O
z	O
y	O
I	O
K	O
Y	O
D	O
J	O
S	O
L	O
B	O
K	O
F	O
r	O
p	O
P	O
u	O
p	O
h	O
r	O
1	O
p	O
z	O
6	O
+	O
4	O
M	O
Z	O
J	O
l	O
4	O
B	O
a	O
l	O
B	O
g	O
W	O
a	O
v	O
+	O
t	O
X	O
t	O
J	O
y	O
y	O
L	O
u	O
U	O
I	O
m	O
q	O
T	O
E	O
d	O
z	O
0	B-DatasetName
0	B-DatasetName
7	O
y	O
1	O
5	O
B	O
Q	O
z	O
h	O
/	O
A	O
H	O
z	O
u	O
c	O
P	O
w	O
2	O
S	O
N	O
p	O
Q	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
X	O
/	O
B	O
b	O
P	O
o	O
3	O
p	O
z	O
0	B-DatasetName
K	O
t	O
B	O
w	O
I	O
f	O
5	O
y	O
T	O
k	O
3	O
p	O
O	O
U	O
S	O
l	O
o	O
K	O
g	O
i	O
+	O
v	O
t	O
r	O
W	O
9	O
s	O
7	O
t	O
X	O
3	O
/	O
c	O
P	O
G	O
v	O
7	O
h	O
0	B-DatasetName
X	O
G	O
z	O
8	O
W	O
S	O
L	O
y	O
g	O
i	O
M	O
R	O
K	O
E	O
K	O
0	B-DatasetName
0	B-DatasetName
u	O
4	O
R	O
S	O
U	O
1	O
R	O
i	O
R	O
J	O
Y	O
a	O
8	O
0	B-DatasetName
y	O
P	O
N	O
E	O
Y	O
T	O
e	O
Z	O
3	O
C	O
3	O
y	O
7	O
j	O
M	O
a	O
K	O
w	O
v	O
9	O
S	O
N	O
M	O
S	O
4	O
5	O
y	O
P	O
t	O
M	O
y	O
k	O
4	O
O	O
S	O
s	O
z	O
r	O
D	O
Z	O
C	O
t	O
r	O
B	O
U	O
m	O
w	O
T	O
w	O
j	O
W	O
0	B-DatasetName
Y	O
K	O
1	O
h	O
8	O
3	O
O	O
Q	O
F	O
q	O
L	O
K	O
U	O
Z	O
N	O
Q	O
3	O
N	O
p	O
+	O
G	O
J	O
Q	O
U	O
z	O
7	O
g	O
h	O
K	O
R	O
T	O
O	O
/	O
U	O
F	O
l	O
s	O
e	O
R	O
i	O
w	O
k	O
f	O
Y	O
d	O
6	O
h	O
5	O
j	O
j	O
a	O
e	O
L	O
c	O
e	O
c	O
s	O
3	O
P	O
n	O
p	O
C	O
w	O
r	O
j	O
D	O
u	O
a	O
2	O
N	O
L	O
9	O
+	O
W	O
L	O
G	O
c	O
2	O
u	O
n	O
e	O
e	O
J	O
u	O
5	O
p	O
z	O
G	O
9	O
m	O
+	O
2	O
M	O
P	O
/	O
L	O
+	O
h	O
V	O
l	O
1	O
/	O
F	O
M	O
6	O
r	O
I	O
i	O
1	O
G	O
L	O
1	O
U	O
V	O
Y	O
p	O
R	O
g	O
V	O
b	O
7	O
M	O
x	O
S	O
a	O
V	O
C	O
Q	O
m	O
j	O
r	O
g	O
w	O
k	O
g	O
3	O
K	O
x	O
N	O
j	O
b	O
r	O
g	O
g	O
1	O
4	O
z	O
v	O
O	O
g	O
j	O
/	O
b	O
r	O
w	O
J	O
0	B-DatasetName
W	O
X	O
7	O
p	O
h	O
0	B-DatasetName
+	O
B	O
F	O
C	O
H	O
U	O
z	O
i	O
D	O
C	O
w	O
j	O
h	O
C	O
m	O
7	O
h	O
H	O
j	O
o	O
Q	O
g	O
Y	O
A	O
U	O
X	O
u	O
D	O
N	O
G	O
3	O
u	O
v	O
3	O
v	O
u	O
q	O
q	O
p	O
q	O
3	O
7	O
u	O
w	O
E	O
f	O
s	O
n	O
7	O
+	O
A	O
a	O
q	O
K	O
Y	O
o	O
N	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
Y	O
t	O
v	O
F	O
5	O
e	O
c	O
6	O
C	O
w	O
p	O
4	O
w	O
r	O
M	O
A	O
A	O
D	O
n	O
k	O
U	O
2	O
D	O
S	O
P	O
P	O
I	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
3	O
n	O
i	O
c	O
b	O
Z	O
D	O
N	O
S	O
g	O
M	O
x	O
F	O
I	O
X	O
v	O
1	O
L	O
9	O
a	O
q	O
1	O
a	O
3	O
b	O
o	O
J	O
F	O
c	O
F	O
V	O
m	O
3	O
K	O
g	O
7	O
w	O
Y	O
3	O
L	O
i	O
o	O
4	O
t	O
t	O
E	O
P	O
J	O
p	O
H	O
f	O
a	O
0	B-DatasetName
E	O
x	O
m	O
S	O
O	O
4	O
I	O
p	O
f	O
Q	O
R	O
3	O
L	O
h	O
Q	O
8	O
b	O
H	O
c	O
+	O
T	O
a	O
m	O
P	O
w	O
t	O
t	O
P	O
R	O
D	O
4	O
O	O
C	O
c	O
h	O
9	O
5	O
4	O
4	O
V	O
9	O
K	O
S	O
7	O
3	O
9	O
7	O
p	O
Y	O
3	O
N	O
r	O
e	O
2	O
d	O
8	O
m	O
5	O
l	O
r	O
7	O
p	O
/	O
c	O
F	O
g	O
7	O
q	O
j	O
7	O
Z	O
r	O
D	O
A	O
C	O
Q	O
5	O
G	O
p	O
z	O
L	O
R	O
j	O
b	O
l	O
F	O
J	O
j	O
S	O
F	O
J	O
U	O
t	O
j	O
O	O
D	O
f	O
I	O
0	B-DatasetName
V	O
t	O
i	O
K	O
R	O
7	O
e	O
z	O
v	O
P	O
W	O
M	O
x	O
s	O
p	O
M	O
P	O
9	O
I	O
4	O
x	O
y	O
j	O
l	O
A	O
y	O
0	B-DatasetName
T	O
K	O
T	O
g	O
5	O
6	O
y	O
H	O
p	O
U	O
a	O
9	O
W	O
9	O
x	O
v	O
+	O
X	O
G	O
w	O
d	O
g	O
i	O
X	O
U	O
Y	O
a	O
l	O
m	O
r	O
/	O
b	O
V	O
7	O
W	O
e	O
i	O
S	O
F	O
G	O
T	O
U	O
N	O
z	O
a	O
T	O
u	O
D	O
n	O
F	O
E	O
2	O
4	O
I	O
S	O
k	O
U	O
T	O
i	O
v	O
d	O
w	O
m	O
L	O
O	O
x	O
Y	O
g	O
P	O
s	O
O	O
N	O
Q	O
8	O
x	O
R	O
t	O
N	O
J	O
m	O
P	O
O	O
m	O
V	O
n	O
z	O
u	O
m	O
z	O
J	O
D	O
P	O
u	O
a	O
G	O
J	O
z	O
9	O
/	O
e	O
L	O
C	O
U	O
+	O
t	O
H	O
a	O
e	O
x	O
u	O
5	O
l	O
y	O
G	O
t	O
r	O
V	O
b	O
G	O
b	O
+	O
l	O
3	O
U	O
K	O
S	O
q	O
6	O
i	O
i	O
d	O
R	O
5	O
Q	O
a	O
j	O
F	O
4	O
q	O
O	O
k	O
U	O
I	O
w	O
y	O
N	O
t	O
u	O
b	O
9	O
a	O
V	O
B	O
Q	O
W	O
r	O
s	O
g	O
A	O
s	O
j	O
3	O
a	O
x	O
M	O
D	O
L	O
n	O
h	O
g	O
l	O
w	O
7	O
F	O
V	O
d	O
C	O
s	O
L	O
r	O
y	O
O	O
o	O
Q	O
X	O
j	O
e	O
t	O
G	O
c	O
O	O
9	O
D	O
G	O
U	O
7	O
g	O
F	O
M	O
4	O
h	O
g	O
E	O
u	O
4	O
g	O
T	O
t	O
o	O
Q	O
g	O
g	O
C	O
B	O
v	O
A	O
C	O
b	O
/	O
D	O
u	O
K	O
e	O
/	O
V	O
+	O
1	O
i	O
0	B-DatasetName
V	O
f	O
K	O
W	O
t	O
R	O
3	O
D	O
H	O
3	O
m	O
f	O
P	O
6	O
f	O
f	O
j	O
F	O
E	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
Y	O
t	O
v	O
F	O
5	O
e	O
c	O
6	O
C	O
w	O
p	O
4	O
w	O
r	O
M	O
A	O
A	O
D	O
n	O
k	O
U	O
2	O
D	O
S	O
P	O
P	O
I	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
3	O
n	O
i	O
c	O
b	O
Z	O
D	O
N	O
S	O
g	O
M	O
x	O
F	O
I	O
X	O
v	O
1	O
L	O
9	O
a	O
q	O
1	O
a	O
3	O
b	O
o	O
J	O
F	O
c	O
F	O
V	O
m	O
3	O
K	O
g	O
7	O
w	O
Y	O
3	O
L	O
i	O
o	O
4	O
t	O
t	O
E	O
P	O
J	O
p	O
H	O
f	O
a	O
0	B-DatasetName
E	O
x	O
m	O
S	O
O	O
4	O
I	O
p	O
f	O
Q	O
R	O
3	O
L	O
h	O
Q	O
8	O
b	O
H	O
c	O
+	O
T	O
a	O
m	O
P	O
w	O
t	O
t	O
P	O
R	O
D	O
4	O
O	O
C	O
c	O
h	O
9	O
5	O
4	O
4	O
V	O
9	O
K	O
S	O
7	O
3	O
9	O
7	O
p	O
Y	O
3	O
N	O
r	O
e	O
2	O
d	O
8	O
m	O
5	O
l	O
r	O
7	O
p	O
/	O
c	O
F	O
g	O
7	O
q	O
j	O
7	O
Z	O
r	O
D	O
A	O
C	O
Q	O
5	O
G	O
p	O
z	O
L	O
R	O
j	O
b	O
l	O
F	O
J	O
j	O
S	O
F	O
J	O
U	O
t	O
j	O
O	O
D	O
f	O
I	O
0	B-DatasetName
V	O
t	O
i	O
K	O
R	O
7	O
e	O
z	O
v	O
P	O
W	O
M	O
x	O
s	O
p	O
M	O
P	O
9	O
I	O
4	O
x	O
y	O
j	O
l	O
A	O
y	O
0	B-DatasetName
T	O
K	O
T	O
g	O
5	O
6	O
y	O
H	O
p	O
U	O
a	O
9	O
W	O
9	O
x	O
v	O
+	O
X	O
G	O
w	O
d	O
g	O
i	O
X	O
U	O
Y	O
a	O
l	O
m	O
r	O
/	O
b	O
V	O
7	O
W	O
e	O
i	O
S	O
F	O
G	O
T	O
U	O
N	O
z	O
a	O
T	O
u	O
D	O
n	O
F	O
E	O
2	O
4	O
I	O
S	O
k	O
U	O
T	O
i	O
v	O
d	O
w	O
m	O
L	O
O	O
x	O
Y	O
g	O
P	O
s	O
O	O
N	O
Q	O
8	O
x	O
R	O
t	O
N	O
J	O
m	O
P	O
O	O
m	O
V	O
n	O
z	O
u	O
m	O
z	O
J	O
D	O
P	O
u	O
a	O
G	O
J	O
z	O
9	O
/	O
e	O
L	O
C	O
U	O
+	O
t	O
H	O
a	O
e	O
x	O
u	O
5	O
l	O
y	O
G	O
t	O
r	O
V	O
b	O
G	O
b	O
+	O
l	O
3	O
U	O
K	O
S	O
q	O
6	O
i	O
i	O
d	O
R	O
5	O
Q	O
a	O
j	O
F	O
4	O
q	O
O	O
k	O
U	O
I	O
w	O
y	O
N	O
t	O
u	O
b	O
9	O
a	O
V	O
B	O
Q	O
W	O
r	O
s	O
g	O
A	O
s	O
j	O
3	O
a	O
x	O
M	O
D	O
L	O
n	O
h	O
g	O
l	O
w	O
7	O
F	O
V	O
d	O
C	O
s	O
L	O
r	O
y	O
O	O
o	O
Q	O
X	O
j	O
e	O
t	O
G	O
c	O
O	O
9	O
D	O
G	O
U	O
7	O
g	O
F	O
M	O
4	O
h	O
g	O
E	O
u	O
4	O
g	O
T	O
t	O
o	O
Q	O
g	O
g	O
C	O
B	O
v	O
A	O
C	O
b	O
/	O
D	O
u	O
K	O
e	O
/	O
V	O
+	O
1	O
i	O
0	B-DatasetName
V	O
f	O
K	O
W	O
t	O
R	O
3	O
D	O
H	O
3	O
m	O
f	O
P	O
6	O
f	O
f	O
j	O
F	O
E	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
R	O
o	O
g	O
s	O
9	O
b	O
w	O
4	O
p	O
c	O
z	O
S	O
b	O
P	O
x	O
9	O
x	O
Q	O
I	O
s	O
8	O
j	O
H	O
X	O
A	O
R	O
A	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
T	O
8	O
J	O
A	O
E	O
J	O
3	O
i	O
F	O
+	O
I	O
X	O
6	O
t	O
H	O
L	O
R	O
m	O
L	O
i	O
i	O
b	O
R	O
e	O
x	O
B	O
v	O
R	O
i	O
0	B-DatasetName
e	O
M	O
V	O
k	O
i	O
g	O
I	O
d	O
t	O
l	O
C	O
x	O
u	O
2	O
2	O
2	O
Z	O
3	O
a	O
k	O
I	O
a	O
f	O
o	O
I	O
X	O
D	O
2	O
q	O
8	O
+	O
o	O
+	O
8	O
+	O
W	O
9	O
c	O
o	O
A	O
c	O
F	O
X	O
z	O
L	O
J	O
y	O
3	O
s	O
z	O
m	O
Z	O
k	O
X	O
p	O
l	O
I	O
Y	O
d	O
N	O
1	O
v	O
p	O
7	O
S	O
2	O
v	O
r	O
G	O
5	O
V	O
d	O
6	O
u	O
7	O
O	O
z	O
u	O
7	O
R	O
9	O
U	O
D	O
4	O
8	O
e	O
T	O
Z	O
J	O
p	O
x	O
n	O
2	O
W	O
y	O
E	O
R	O
3	O
Q	O
m	O
q	O
4	O
F	O
I	O
r	O
7	O
K	O
F	O
D	O
y	O
T	O
q	O
o	O
5	O
j	O
U	O
P	O
J	O
2	O
+	O
H	O
4	O
Z	O
u	O
a	O
3	O
n	O
7	O
g	O
6	O
L	O
8	O
+	O
5	O
8	O
L	O
F	O
p	O
L	O
T	O
j	O
F	O
z	O
D	O
H	O
/	O
g	O
f	O
P	O
4	O
A	O
w	O
i	O
S	O
N	O
o	O
Q	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
z	O
U	O
m	O
7	O
9	O
l	O
o	O
9	O
e	O
Y	O
U	O
U	O
g	O
i	O
5	O
D	O
W	O
6	O
o	O
p	O
C	O
U	O
a	O
4	O
f	O
s	O
s	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
U	O
G	O
9	O
F	O
L	O
x	O
4	O
r	O
G	O
l	O
t	O
o	O
Q	O
9	O
l	O
s	O
N	O
+	O
3	O
S	O
z	O
S	O
b	O
s	O
T	O
o	O
Q	O
S	O
+	O
h	O
O	O
8	O
e	O
F	O
D	O
x	O
6	O
j	O
/	O
y	O
5	O
r	O
9	O
x	O
2	O
+	O
a	O
g	O
r	O
Q	O
8	O
G	O
H	O
u	O
/	O
N	O
M	O
D	O
M	O
v	O
T	O
K	O
U	O
w	O
6	O
L	O
r	O
f	O
T	O
m	O
l	O
l	O
d	O
W	O
1	O
9	O
o	O
7	O
x	O
Z	O
2	O
d	O
r	O
e	O
2	O
d	O
2	O
r	O
7	O
h	O
8	O
8	O
m	O
i	O
T	O
T	O
j	O
P	O
s	O
s	O
k	O
Y	O
l	O
u	O
h	O
9	O
R	O
w	O
K	O
R	O
T	O
3	O
U	O
a	O
D	O
k	O
7	O
V	O
R	O
z	O
G	O
o	O
e	O
S	O
t	O
8	O
L	O
R	O
z	O
d	O
R	O
v	O
P	O
X	O
F	O
t	O
R	O
K	O
I	O
e	O
c	O
J	O
z	O
y	O
I	O
K	O
Y	O
D	O
J	O
S	O
L	O
B	O
K	O
F	O
r	O
p	O
P	O
u	O
p	O
h	O
r	O
1	O
p	O
z	O
6	O
+	O
4	O
M	O
Z	O
J	O
l	O
4	O
B	O
a	O
l	O
B	O
g	O
W	O
a	O
v	O
+	O
t	O
X	O
t	O
J	O
y	O
y	O
L	O
u	O
U	O
I	O
m	O
q	O
T	O
E	O
d	O
z	O
0	B-DatasetName
0	B-DatasetName
7	O
y	O
1	O
5	O
B	O
Q	O
z	O
h	O
/	O
A	O
H	O
z	O
u	O
c	O
P	O
w	O
2	O
S	O
N	O
p	O
Q	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
z	O
U	O
m	O
7	O
9	O
l	O
o	O
9	O
e	O
Y	O
U	O
U	O
g	O
i	O
5	O
D	O
W	O
6	O
o	O
p	O
C	O
U	O
a	O
4	O
f	O
s	O
s	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
U	O
G	O
9	O
F	O
L	O
x	O
4	O
r	O
G	O
l	O
t	O
o	O
Q	O
9	O
l	O
s	O
N	O
+	O
3	O
S	O
z	O
S	O
b	O
s	O
T	O
o	O
Q	O
S	O
+	O
h	O
O	O
8	O
e	O
F	O
D	O
x	O
6	O
j	O
/	O
y	O
5	O
r	O
9	O
x	O
2	O
+	O
a	O
g	O
r	O
Q	O
8	O
G	O
H	O
u	O
/	O
N	O
M	O
D	O
M	O
v	O
T	O
K	O
U	O
w	O
6	O
L	O
r	O
f	O
T	O
m	O
l	O
l	O
d	O
W	O
1	O
9	O
o	O
7	O
x	O
Z	O
2	O
d	O
r	O
e	O
2	O
d	O
2	O
r	O
7	O
h	O
8	O
8	O
m	O
i	O
T	O
T	O
j	O
P	O
s	O
s	O
k	O
Y	O
l	O
u	O
h	O
9	O
R	O
w	O
K	O
R	O
T	O
3	O
U	O
a	O
D	O
k	O
7	O
V	O
R	O
z	O
G	O
o	O
e	O
S	O
t	O
8	O
L	O
R	O
z	O
d	O
R	O
v	O
P	O
X	O
F	O
t	O
R	O
K	O
I	O
e	O
c	O
J	O
z	O
y	O
I	O
K	O
Y	O
D	O
J	O
S	O
L	O
B	O
K	O
F	O
r	O
p	O
P	O
u	O
p	O
h	O
r	O
1	O
p	O
z	O
6	O
+	O
4	O
M	O
Z	O
J	O
l	O
4	O
B	O
a	O
l	O
B	O
g	O
W	O
a	O
v	O
+	O
t	O
X	O
t	O
J	O
y	O
y	O
L	O
u	O
U	O
I	O
m	O
q	O
T	O
E	O
d	O
z	O
0	B-DatasetName
0	B-DatasetName
7	O
y	O
1	O
5	O
B	O
Q	O
z	O
h	O
/	O
A	O
H	O
z	O
u	O
c	O
P	O
w	O
2	O
S	O
N	O
p	O
Q	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
z	O
U	O
m	O
7	O
9	O
l	O
o	O
9	O
e	O
Y	O
U	O
U	O
g	O
i	O
5	O
D	O
W	O
6	O
o	O
p	O
C	O
U	O
a	O
4	O
f	O
s	O
s	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
U	O
G	O
9	O
F	O
L	O
x	O
4	O
r	O
G	O
l	O
t	O
o	O
Q	O
9	O
l	O
s	O
N	O
+	O
3	O
S	O
z	O
S	O
b	O
s	O
T	O
o	O
Q	O
S	O
+	O
h	O
O	O
8	O
e	O
F	O
D	O
x	O
6	O
j	O
/	O
y	O
5	O
r	O
9	O
x	O
2	O
+	O
a	O
g	O
r	O
Q	O
8	O
G	O
H	O
u	O
/	O
N	O
M	O
D	O
M	O
v	O
T	O
K	O
U	O
w	O
6	O
L	O
r	O
f	O
T	O
m	O
l	O
l	O
d	O
W	O
1	O
9	O
o	O
7	O
x	O
Z	O
2	O
d	O
r	O
e	O
2	O
d	O
2	O
r	O
7	O
h	O
8	O
8	O
m	O
i	O
T	O
T	O
j	O
P	O
s	O
s	O
k	O
Y	O
l	O
u	O
h	O
9	O
R	O
w	O
K	O
R	O
T	O
3	O
U	O
a	O
D	O
k	O
7	O
V	O
R	O
z	O
G	O
o	O
e	O
S	O
t	O
8	O
L	O
R	O
z	O
d	O
R	O
v	O
P	O
X	O
F	O
t	O
R	O
K	O
I	O
e	O
c	O
J	O
z	O
y	O
I	O
K	O
Y	O
D	O
J	O
S	O
L	O
B	O
K	O
F	O
r	O
p	O
P	O
u	O
p	O
h	O
r	O
1	O
p	O
z	O
6	O
+	O
4	O
M	O
Z	O
J	O
l	O
4	O
B	O
a	O
l	O
B	O
g	O
W	O
a	O
v	O
+	O
t	O
X	O
t	O
J	O
y	O
y	O
L	O
u	O
U	O
I	O
m	O
q	O
T	O
E	O
d	O
z	O
0	B-DatasetName
0	B-DatasetName
7	O
y	O
1	O
5	O
B	O
Q	O
z	O
h	O
/	O
A	O
H	O
z	O
u	O
c	O
P	O
w	O
2	O
S	O
N	O
p	O
Q	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
z	O
U	O
m	O
7	O
9	O
l	O
o	O
9	O
e	O
Y	O
U	O
U	O
g	O
i	O
5	O
D	O
W	O
6	O
o	O
p	O
C	O
U	O
a	O
4	O
f	O
s	O
s	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
U	O
G	O
9	O
F	O
L	O
x	O
4	O
r	O
G	O
l	O
t	O
o	O
Q	O
9	O
l	O
s	O
N	O
+	O
3	O
S	O
z	O
S	O
b	O
s	O
T	O
o	O
Q	O
S	O
+	O
h	O
O	O
8	O
e	O
F	O
D	O
x	O
6	O
j	O
/	O
y	O
5	O
r	O
9	O
x	O
2	O
+	O
a	O
g	O
r	O
Q	O
8	O
G	O
H	O
u	O
/	O
N	O
M	O
D	O
M	O
v	O
T	O
K	O
U	O
w	O
6	O
L	O
r	O
f	O
T	O
m	O
l	O
l	O
d	O
W	O
1	O
9	O
o	O
7	O
x	O
Z	O
2	O
d	O
r	O
e	O
2	O
d	O
2	O
r	O
7	O
h	O
8	O
8	O
m	O
i	O
T	O
T	O
j	O
P	O
s	O
s	O
k	O
Y	O
l	O
u	O
h	O
9	O
R	O
w	O
K	O
R	O
T	O
3	O
U	O
a	O
D	O
k	O
7	O
V	O
R	O
z	O
G	O
o	O
e	O
S	O
t	O
8	O
L	O
R	O
z	O
d	O
R	O
v	O
P	O
X	O
F	O
t	O
R	O
K	O
I	O
e	O
c	O
J	O
z	O
y	O
I	O
K	O
Y	O
D	O
J	O
S	O
L	O
B	O
K	O
F	O
r	O
p	O
P	O
u	O
p	O
h	O
r	O
1	O
p	O
z	O
6	O
+	O
4	O
M	O
Z	O
J	O
l	O
4	O
B	O
a	O
l	O
B	O
g	O
W	O
a	O
v	O
+	O
t	O
X	O
t	O
J	O
y	O
y	O
L	O
u	O
U	O
I	O
m	O
q	O
T	O
E	O
d	O
z	O
0	B-DatasetName
0	B-DatasetName
7	O
y	O
1	O
5	O
B	O
Q	O
z	O
h	O
/	O
A	O
H	O
z	O
u	O
c	O
P	O
w	O
2	O
S	O
N	O
p	O
Q	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
z	O
U	O
m	O
7	O
9	O
l	O
o	O
9	O
e	O
Y	O
U	O
U	O
g	O
i	O
5	O
D	O
W	O
6	O
o	O
p	O
C	O
U	O
a	O
4	O
f	O
s	O
s	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
U	O
G	O
9	O
F	O
L	O
x	O
4	O
r	O
G	O
l	O
t	O
o	O
Q	O
9	O
l	O
s	O
N	O
+	O
3	O
S	O
z	O
S	O
b	O
s	O
T	O
o	O
Q	O
S	O
+	O
h	O
O	O
8	O
e	O
F	O
D	O
x	O
6	O
j	O
/	O
y	O
5	O
r	O
9	O
x	O
2	O
+	O
a	O
g	O
r	O
Q	O
8	O
G	O
H	O
u	O
/	O
N	O
M	O
D	O
M	O
v	O
T	O
K	O
U	O
w	O
6	O
L	O
r	O
f	O
T	O
m	O
l	O
l	O
d	O
W	O
1	O
9	O
o	O
7	O
x	O
Z	O
2	O
d	O
r	O
e	O
2	O
d	O
2	O
r	O
7	O
h	O
8	O
8	O
m	O
i	O
T	O
T	O
j	O
P	O
s	O
s	O
k	O
Y	O
l	O
u	O
h	O
9	O
R	O
w	O
K	O
R	O
T	O
3	O
U	O
a	O
D	O
k	O
7	O
V	O
R	O
z	O
G	O
o	O
e	O
S	O
t	O
8	O
L	O
R	O
z	O
d	O
R	O
v	O
P	O
X	O
F	O
t	O
R	O
K	O
I	O
e	O
c	O
J	O
z	O
y	O
I	O
K	O
Y	O
D	O
J	O
S	O
L	O
B	O
K	O
F	O
r	O
p	O
P	O
u	O
p	O
h	O
r	O
1	O
p	O
z	O
6	O
+	O
4	O
M	O
Z	O
J	O
l	O
4	O
B	O
a	O
l	O
B	O
g	O
W	O
a	O
v	O
+	O
t	O
X	O
t	O
J	O
y	O
y	O
L	O
u	O
U	O
I	O
m	O
q	O
T	O
E	O
d	O
z	O
0	B-DatasetName
0	B-DatasetName
7	O
y	O
1	O
5	O
B	O
Q	O
z	O
h	O
/	O
A	O
H	O
z	O
u	O
c	O
P	O
w	O
2	O
S	O
N	O
p	O
Q	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
z	O
U	O
m	O
7	O
9	O
l	O
o	O
9	O
e	O
Y	O
U	O
U	O
g	O
i	O
5	O
D	O
W	O
6	O
o	O
p	O
C	O
U	O
a	O
4	O
f	O
s	O
s	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
U	O
G	O
9	O
F	O
L	O
x	O
4	O
r	O
G	O
l	O
t	O
o	O
Q	O
9	O
l	O
s	O
N	O
+	O
3	O
S	O
z	O
S	O
b	O
s	O
T	O
o	O
Q	O
S	O
+	O
h	O
O	O
8	O
e	O
F	O
D	O
x	O
6	O
j	O
/	O
y	O
5	O
r	O
9	O
x	O
2	O
+	O
a	O
g	O
r	O
Q	O
8	O
G	O
H	O
u	O
/	O
N	O
M	O
D	O
M	O
v	O
T	O
K	O
U	O
w	O
6	O
L	O
r	O
f	O
T	O
m	O
l	O
l	O
d	O
W	O
1	O
9	O
o	O
7	O
x	O
Z	O
2	O
d	O
r	O
e	O
2	O
d	O
2	O
r	O
7	O
h	O
8	O
8	O
m	O
i	O
T	O
T	O
j	O
P	O
s	O
s	O
k	O
Y	O
l	O
u	O
h	O
9	O
R	O
w	O
K	O
R	O
T	O
3	O
U	O
a	O
D	O
k	O
7	O
V	O
R	O
z	O
G	O
o	O
e	O
S	O
t	O
8	O
L	O
R	O
z	O
d	O
R	O
v	O
P	O
X	O
F	O
t	O
R	O
K	O
I	O
e	O
c	O
J	O
z	O
y	O
I	O
K	O
Y	O
D	O
J	O
S	O
L	O
B	O
K	O
F	O
r	O
p	O
P	O
u	O
p	O
h	O
r	O
1	O
p	O
z	O
6	O
+	O
4	O
M	O
Z	O
J	O
l	O
4	O
B	O
a	O
l	O
B	O
g	O
W	O
a	O
v	O
+	O
t	O
X	O
t	O
J	O
y	O
y	O
L	O
u	O
U	O
I	O
m	O
q	O
T	O
E	O
d	O
z	O
0	B-DatasetName
0	B-DatasetName
x	O
H	O
a	O
t	O
E	O
r	O
x	O
f	O
+	O
8	O
f	O
q	O
7	O
j	O
C	O
7	O
9	O
g	O
I	O
s	O
s	O
1	O
F	O
W	O
T	O
+	O
U	O
J	O
x	O
z	O
p	O
F	O
N	O
U	O
t	O
o	O
A	O
i	O
J	O
i	O
n	O
R	O
f	O
G	O
I	O
I	O
J	O
p	O
K	O
Z	O
r	O
I	O
i	O
M	O
s	O
M	O
R	O
E	O
m	O
6	O
4	O
a	O
p	O
g	O
R	O
3	O
8	O
c	O
v	O
L	O
x	O
D	O
t	O
t	O
X	O
7	O
b	O
d	O
2	O
7	O
N	O
W	O
5	O
6	O
p	O
q	O
o	O
w	O
6	O
H	O
c	O
A	O
Q	O
n	O
4	O
M	O
I	O
5	O
d	O
O	O
A	O
G	O
u	O
u	O
A	O
B	O
g	O
R	O
y	O
e	O
4	O
R	O
X	O
e	O
r	O
C	O
f	O
r	O
x	O
X	O
q	O
3	O
P	O
u	O
a	O
j	O
N	O
a	O
v	O
a	O
O	O
Y	O
A	O
/	O
s	O
D	O
5	O
/	O
A	O
O	O
d	O
+	O
k	O
s	O
o	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
X	O
/	O
B	O
b	O
P	O
U	O
F	O
l	O
s	O
e	O
R	O
i	O
w	O
k	O
f	O
Y	O
d	O
6	O
h	O
5	O
j	O
j	O
a	O
e	O
L	O
c	O
e	O
c	O
s	O
3	O
P	O
n	O
p	O
C	O
w	O
r	O
j	O
D	O
u	O
a	O
2	O
N	O
L	O
9	O
+	O
W	O
L	O
G	O
c	O
2	O
u	O
n	O
e	O
e	O
J	O
u	O
5	O
p	O
z	O
G	O
9	O
m	O
s	O
y	O
I	O
y	O
w	O
Q	O
o	O
T	O
Y	O
7	O
t	O
q	O
2	O
B	O
L	O
8	O
9	O
Z	O
U	O
3	O
S	O
X	O
D	O
Z	O
6	O
X	O
b	O
8	O
e	O
w	O
/	O
q	O
c	O
A	O
p	O
n	O
c	O
A	O
E	O
+	O
X	O
M	O
E	O
N	O
3	O
E	O
E	O
P	O
A	O
i	O
B	O
Q	O
w	O
A	O
u	O
8	O
w	O
b	O
v	O
z	O
7	O
L	O
w	O
6	O
H	O
8	O
u	O
2	O
a	O
s	O
6	O
q	O
t	O
h	O
P	O
4	O
A	O
+	O
f	O
z	O
B	O
5	O
X	O
m	O
k	O
W	O
g	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
c	O
X	O
m	O
s	O
y	O
I	O
y	O
w	O
Q	O
o	O
T	O
Y	O
7	O
t	O
q	O
2	O
B	O
L	O
8	O
9	O
Z	O
U	O
3	O
S	O
X	O
D	O
Z	O
6	O
X	O
b	O
8	O
e	O
w	O
/	O
q	O
c	O
A	O
p	O
n	O
c	O
A	O
E	O
+	O
X	O
M	O
E	O
N	O
3	O
E	O
E	O
P	O
A	O
i	O
B	O
Q	O
w	O
A	O
u	O
8	O
w	O
b	O
v	O
z	O
7	O
L	O
w	O
6	O
H	O
8	O
u	O
2	O
a	O
s	O
6	O
q	O
t	O
h	O
P	O
4	O
A	O
+	O
f	O
z	O
B	O
5	O
X	O
m	O
k	O
W	O
g	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
f	O
s	O
H	O
x	O
Z	O
T	O
9	O
a	O
L	O
9	O
W	O
5	O
9	O
L	O
E	O
Z	O
r	O
V	O
r	O
V	O
z	O
B	O
H	O
9	O
g	O
f	O
f	O
4	O
A	O
5	O
j	O
6	O
S	O
x	O
g	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
v	O
8	O
T	O
3	O
v	O
g	O
o	O
R	O
2	O
Q	O
P	O
y	O
V	O
G	O
+	O
h	O
5	O
W	O
x	O
O	O
e	O
l	O
G	O
p	O
n	O
5	O
I	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
9	O
3	O
i	O
c	O
b	O
V	O
B	O
P	O
S	O
8	O
M	O
w	O
H	O
P	O
1	O
1	O
/	O
p	O
v	O
z	O
z	O
6	O
o	O
e	O
v	O
Q	O
S	O
H	O
4	O
G	O
m	O
0	B-DatasetName
I	O
q	O
i	O
3	O
o	O
R	O
e	O
P	O
E	O
6	O
w	O
O	O
t	O
l	O
L	O
S	O
N	O
N	O
3	O
C	O
0	B-DatasetName
r	O
Q	O
k	O
q	O
T	O
D	O
L	O
P	O
o	O
k	O
X	O
D	O
y	O
p	O
e	O
/	O
S	O
r	O
e	O
/	O
D	O
a	O
m	O
W	O
w	O
+	O
6	O
+	O
S	O
D	O
k	O
8	O
G	O
B	O
R	O
3	O
i	O
G	O
V	O
3	O
i	O
z	O
n	O
q	O
w	O
X	O
6	O
9	O
3	O
6	O
m	O
I	O
9	O
W	O
r	O
H	O
L	O
n	O
A	O
P	O
7	O
A	O
+	O
v	O
w	O
B	O
y	O
q	O
i	O
T	O
5	O
A	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
U	O
0	B-DatasetName
G	O
B	O
R	O
3	O
i	O
G	O
V	O
3	O
i	O
z	O
n	O
q	O
w	O
X	O
6	O
9	O
3	O
6	O
m	O
I	O
9	O
W	O
r	O
H	O
L	O
n	O
A	O
P	O
7	O
A	O
+	O
v	O
w	O
B	O
y	O
q	O
i	O
T	O
5	O
A	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
U	O
0	B-DatasetName
g	O
M	O
h	O
Y	O
y	O
t	O
y	O
h	O
J	O
9	O
O	O
i	O
N	O
I	O
M	O
Q	O
F	O
4	O
c	O
M	O
z	O
J	O
C	O
Y	O
s	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
+	O
X	O
i	O
c	O
b	O
V	O
C	O
9	O
T	O
s	O
M	O
w	O
G	O
P	O
x	O
S	O
/	O
k	O
r	O
5	O
S	O
2	O
F	O
k	O
s	O
a	O
i	O
Q	O
m	O
K	O
o	O
E	O
I	O
Q	O
F	O
b	O
B	O
Q	O
t	O
j	O
k	O
Q	O
i	O
t	O
1	O
E	O
S	O
V	O
4	O
7	O
i	O
t	O
V	O
c	O
e	O
J	O
b	O
A	O
d	O
U	O
h	O
T	O
4	O
K	O
C	O
w	O
M	O
g	O
V	O
t	O
6	O
E	O
j	O
b	O
f	O
B	O
a	O
T	O
N	O
A	O
y	O
0	B-DatasetName
m	O
W	O
T	O
3	O
f	O
f	O
J	O
5	O
8	O
v	O
T	O
D	O
l	O
T	O
2	O
n	O
G	O
+	O
r	O
c	O
r	O
K	O
6	O
t	O
r	O
6	O
R	O
n	O
W	O
z	O
t	O
r	O
W	O
9	O
s	O
7	O
t	O
n	O
1	O
/	O
f	O
v	O
V	O
Z	O
J	O
J	O
Q	O
j	O
2	O
S	O
8	O
E	O
R	O
2	O
Q	O
6	O
w	O
o	O
Z	O
4	O
J	O
6	O
m	O
m	O
l	O
O	O
u	O
6	O
m	O
k	O
O	O
A	O
4	O
5	O
7	O
Y	O
T	O
j	O
6	O
8	O
L	O
v	O
P	O
F	O
C	O
p	O
W	O
C	O
L	O
u	O
9	O
C	O
S	O
l	O
Q	O
Y	O
y	O
H	O
g	O
g	O
0	B-DatasetName
Y	O
w	O
d	O
p	O
I	O
f	O
b	O
v	O
u	O
5	O
3	O
6	O
Y	O
8	O
E	O
h	O
N	O
Y	O
n	O
O	O
h	O
r	O
j	O
/	O
t	O
2	O
w	O
2	O
n	O
6	O
c	O
y	O
A	O
l	O
o	O
l	O
b	O
k	O
g	O
a	O
U	O
a	O
P	O
f	O
t	O
L	O
z	O
9	O
K	O
S	O
B	O
Z	O
T	O
o	O
Q	O
n	O
H	O
S	O
v	O
V	O
c	O
J	O
9	O
V	O
B	O
j	O
q	O
V	O
m	O
h	O
N	O
N	O
p	O
z	O
c	O
8	O
U	O
T	O
T	O
E	O
Z	O
4	O
y	O
H	O
t	O
G	O
S	O
p	O
w	O
T	O
F	O
W	O
Q	O
z	O
6	O
J	O
P	O
0	B-DatasetName
b	O
F	O
R	O
I	O
j	O
R	O
I	O
p	O
D	O
l	O
C	O
o	O
5	O
n	O
6	O
e	O
y	O
P	O
H	O
s	O
S	O
q	O
y	O
m	O
c	O
k	O
Y	O
6	O
5	O
F	O
a	O
9	O
A	O
r	O
x	O
P	O
6	O
+	O
X	O
6	O
c	O
F	O
F	O
k	O
D	O
O	O
R	O
Z	O
p	O
o	O
K	O
M	O
n	O
9	O
o	O
k	O
H	O
G	O
k	O
E	O
1	O
T	O
0	B-DatasetName
g	O
C	O
I	O
m	O
K	O
d	O
F	O
8	O
Y	O
g	O
g	O
m	O
k	O
p	O
m	O
s	O
i	O
I	O
y	O
w	O
x	O
E	O
S	O
b	O
t	O
m	O
q	O
m	O
B	O
H	O
f	O
x	O
y	O
8	O
v	O
E	O
O	O
2	O
1	O
e	O
N	O
t	O
3	O
b	O
s	O
0	B-DatasetName
b	O
r	O
q	O
m	O
y	O
j	O
C	O
o	O
d	O
w	O
B	O
C	O
f	O
g	O
w	O
j	O
m	O
0	B-DatasetName
4	O
A	O
b	O
a	O
4	O
A	O
G	O
B	O
R	O
3	O
i	O
G	O
V	O
3	O
i	O
z	O
n	O
q	O
w	O
X	O
6	O
9	O
3	O
6	O
m	O
I	O
9	O
W	O
r	O
H	O
L	O
n	O
A	O
P	O
7	O
A	O
+	O
v	O
w	O
B	O
y	O
q	O
i	O
T	O
5	O
A	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
X	O
/	O
B	O
b	O
P	O
o	O
3	O
p	O
z	O
0	B-DatasetName
K	O
t	O
B	O
w	O
I	O
f	O
5	O
y	O
T	O
k	O
3	O
p	O
O	O
U	O
S	O
l	O
o	O
K	O
g	O
i	O
+	O
v	O
t	O
r	O
W	O
9	O
s	O
7	O
t	O
X	O
3	O
/	O
c	O
P	O
G	O
v	O
7	O
h	O
0	B-DatasetName
X	O
G	O
z	O
8	O
W	O
S	O
L	O
y	O
g	O
i	O
M	O
R	O
K	O
E	O
K	O
0	B-DatasetName
0	B-DatasetName
u	O
4	O
R	O
S	O
U	O
1	O
R	O
i	O
R	O
J	O
Y	O
a	O
8	O
0	B-DatasetName
y	O
P	O
N	O
E	O
Y	O
T	O
e	O
Z	O
3	O
C	O
3	O
y	O
7	O
j	O
M	O
a	O
K	O
w	O
v	O
9	O
S	O
N	O
M	O
S	O
4	O
5	O
y	O
P	O
t	O
M	O
y	O
k	O
4	O
O	O
S	O
s	O
z	O
r	O
D	O
Z	O
C	O
t	O
r	O
B	O
U	O
m	O
w	O
T	O
w	O
j	O
W	O
0	B-DatasetName
Y	O
K	O
1	O
h	O
8	O
3	O
O	O
Q	O
F	O
q	O
L	O
K	O
U	O
Z	O
N	O
Q	O
3	O
N	O
p	O
+	O
G	O
J	O
Q	O
U	O
z	O
7	O
g	O
h	O
K	O
R	O
T	O
O	O
/	O
U	O
F	O
l	O
s	O
e	O
R	O
i	O
w	O
k	O
f	O
Y	O
d	O
6	O
h	O
5	O
j	O
j	O
a	O
e	O
L	O
c	O
e	O
c	O
s	O
3	O
P	O
n	O
p	O
C	O
w	O
r	O
j	O
D	O
u	O
a	O
2	O
N	O
L	O
9	O
+	O
W	O
L	O
G	O
c	O
2	O
u	O
n	O
e	O
e	O
J	O
u	O
5	O
p	O
z	O
G	O
9	O
m	O
+	O
2	O
M	O
P	O
/	O
L	O
+	O
h	O
V	O
l	O
1	O
/	O
F	O
M	O
6	O
r	O
I	O
i	O
1	O
G	O
L	O
1	O
U	O
V	O
Y	O
p	O
R	O
g	O
V	O
b	O
7	O
M	O
x	O
S	O
a	O
V	O
C	O
Q	O
m	O
j	O
r	O
g	O
w	O
k	O
g	O
3	O
K	O
x	O
N	O
j	O
b	O
r	O
g	O
g	O
1	O
4	O
z	O
v	O
O	O
g	O
j	O
/	O
b	O
r	O
w	O
J	O
0	B-DatasetName
W	O
X	O
7	O
p	O
h	O
0	B-DatasetName
+	O
B	O
F	O
C	O
H	O
U	O
z	O
i	O
D	O
C	O
w	O
j	O
h	O
C	O
m	O
7	O
h	O
H	O
j	O
o	O
Q	O
g	O
Y	O
A	O
U	O
X	O
u	O
D	O
N	O
G	O
3	O
u	O
v	O
3	O
v	O
u	O
q	O
q	O
p	O
q	O
3	O
7	O
u	O
w	O
E	O
f	O
s	O
n	O
7	O
+	O
A	O
a	O
q	O
K	O
Y	O
o	O
N	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
k	O
v	O
u	O
1	O
4	O
B	O
G	O
D	O
B	O
I	O
h	O
R	O
l	O
F	O
R	O
I	O
G	O
u	O
9	O
Z	O
3	O
v	O
M	O
Y	O
U	O
y	O
g	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
7	O
n	O
i	O
c	O
b	O
V	O
C	O
9	O
T	O
s	O
M	O
w	O
G	O
P	O
x	O
S	O
/	O
k	O
o	O
p	O
k	O
L	O
K	O
y	O
W	O
F	O
R	O
I	O
T	O
F	O
X	O
C	O
A	O
m	O
x	O
I	O
L	O
I	O
x	O
F	O
I	O
r	O
R	O
S	O
E	O
1	O
W	O
O	O
4	O
7	O
R	O
W	O
H	O
T	O
u	O
y	O
H	O
V	O
A	O
V	O
+	O
i	O
g	O
s	O
D	O
I	O
B	O
4	O
H	O
D	O
b	O
e	O
B	O
q	O
f	O
t	O
A	O
C	O
0	B-DatasetName
n	O
f	O
f	O
L	O
p	O
z	O
p	O
b	O
v	O
u	O
z	O
j	O
n	O
T	O
B	O
v	O
P	O
+	O
3	O
Z	O
q	O
G	O
5	O
t	O
b	O
2	O
z	O
v	O
1	O
3	O
c	O
Z	O
e	O
c	O
/	O
/	O
g	O
0	B-DatasetName
G	O
0	B-DatasetName
1	O
H	O
7	O
Q	O
s	O
F	O
K	O
E	O
B	O
k	O
V	O
y	O
q	O
f	O
o	O
w	O
1	O
5	O
U	O
z	O
Q	O
w	O
D	O
D	O
D	O
a	O
T	O
9	O
X	O
F	O
G	O
c	O
x	O
p	O
7	O
1	O
4	O
c	O
l	O
P	O
5	O
v	O
U	O
e	O
q	O
N	O
J	O
P	O
i	O
3	O
k	O
x	O
z	O
G	O
m	O
V	O
4	O
J	O
F	O
j	O
K	O
C	O
D	O
Z	O
W	O
G	O
r	O
q	O
t	O
s	O
A	O
x	O
j	O
y	O
R	O
M	O
9	O
z	O
e	O
y	O
B	O
+	O
u	O
F	O
s	O
6	O
L	O
a	O
9	O
j	O
j	O
c	O
H	O
W	O
i	O
f	O
+	O
k	O
r	O
R	O
h	O
i	O
e	O
7	O
Q	O
/	O
Q	O
o	O
T	O
S	O
Y	O
q	O
M	O
C	O
k	O
M	O
4	O
1	O
n	O
r	O
g	O
e	O
7	O
m	O
J	O
S	O
q	O
w	O
M	O
I	O
5	O
z	O
O	O
G	O
m	O
G	O
h	O
a	O
Y	O
7	O
J	O
B	O
I	O
/	O
o	O
w	O
F	O
K	O
B	O
M	O
6	O
q	O
j	O
c	O
h	O
5	O
9	O
h	O
k	O
6	O
t	O
k	O
q	O
B	O
U	O
K	O
j	O
v	O
C	O
o	O
L	O
n	O
6	O
+	O
0	B-DatasetName
W	O
J	O
M	O
1	O
1	O
l	O
s	O
z	O
c	O
z	O
b	O
M	O
Z	O
6	O
1	O
a	O
v	O
E	O
/	O
7	O
x	O
B	O
Y	O
d	O
L	O
L	O
q	O
G	O
Q	O
i	O
L	O
w	O
w	O
V	O
Z	O
P	O
F	O
R	O
W	O
n	O
B	O
k	O
J	O
K	O
p	O
6	O
Q	O
A	O
l	O
T	O
l	O
B	O
g	O
+	O
t	O
Q	O
Q	O
T	O
x	O
W	O
x	O
W	O
R	O
M	O
Z	O
Y	O
Y	O
W	O
J	O
s	O
W	O
w	O
1	O
b	O
g	O
r	O
+	O
6	O
8	O
j	O
o	O
J	O
z	O
j	O
t	O
X	O
H	O
f	O
/	O
O	O
g	O
z	O
o	O
c	O
w	O
w	O
m	O
c	O
g	O
Q	O
8	O
X	O
c	O
A	O
2	O
3	O
0	B-DatasetName
I	O
U	O
A	O
C	O
D	O
z	O
B	O
C	O
7	O
z	O
B	O
u	O
/	O
P	O
s	O
v	O
D	O
o	O
f	O
i	O
7	O
Z	O
q	O
z	O
r	O
K	O
2	O
I	O
/	O
g	O
D	O
5	O
/	O
M	O
H	O
b	O
U	O
6	O
S	O
g	O
A	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
k	O
v	O
u	O
1	O
4	O
B	O
G	O
D	O
B	O
I	O
h	O
R	O
l	O
F	O
R	O
I	O
G	O
u	O
9	O
Z	O
3	O
v	O
M	O
Y	O
U	O
y	O
g	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
7	O
n	O
i	O
c	O
b	O
V	O
C	O
9	O
T	O
s	O
M	O
w	O
G	O
P	O
x	O
S	O
/	O
k	O
o	O
p	O
k	O
L	O
K	O
y	O
W	O
F	O
R	O
I	O
T	O
F	O
X	O
C	O
A	O
m	O
x	O
I	O
L	O
I	O
x	O
F	O
I	O
r	O
R	O
S	O
E	O
1	O
W	O
O	O
4	O
7	O
R	O
W	O
H	O
T	O
u	O
y	O
H	O
V	O
A	O
V	O
+	O
i	O
g	O
s	O
D	O
I	O
B	O
4	O
H	O
D	O
b	O
e	O
B	O
q	O
f	O
t	O
A	O
C	O
0	B-DatasetName
n	O
f	O
f	O
L	O
p	O
z	O
p	O
b	O
v	O
u	O
z	O
j	O
n	O
T	O
B	O
v	O
P	O
+	O
3	O
Z	O
q	O
G	O
5	O
t	O
b	O
2	O
z	O
v	O
1	O
3	O
c	O
Z	O
e	O
c	O
/	O
/	O
g	O
0	B-DatasetName
G	O
0	B-DatasetName
1	O
H	O
7	O
Q	O
s	O
F	O
K	O
E	O
B	O
k	O
V	O
y	O
q	O
f	O
o	O
w	O
1	O
5	O
U	O
z	O
Q	O
w	O
D	O
D	O
D	O
a	O
T	O
9	O
X	O
F	O
G	O
c	O
x	O
p	O
7	O
1	O
4	O
c	O
l	O
P	O
5	O
v	O
U	O
e	O
q	O
N	O
J	O
P	O
i	O
3	O
k	O
x	O
z	O
G	O
m	O
V	O
4	O
J	O
F	O
j	O
K	O
C	O
D	O
Z	O
W	O
G	O
r	O
q	O
t	O
s	O
A	O
x	O
j	O
y	O
R	O
M	O
9	O
z	O
e	O
y	O
B	O
+	O
u	O
F	O
s	O
6	O
L	O
a	O
9	O
j	O
j	O
c	O
H	O
W	O
i	O
f	O
+	O
k	O
r	O
R	O
h	O
i	O
e	O
7	O
Q	O
/	O
Q	O
o	O
T	O
S	O
Y	O
q	O
M	O
C	O
k	O
M	O
4	O
1	O
n	O
r	O
g	O
e	O
7	O
m	O
J	O
S	O
q	O
w	O
M	O
I	O
5	O
z	O
O	O
G	O
m	O
G	O
h	O
a	O
Y	O
7	O
J	O
B	O
I	O
/	O
o	O
w	O
F	O
K	O
B	O
M	O
6	O
q	O
j	O
c	O
h	O
5	O
9	O
h	O
k	O
6	O
t	O
k	O
q	O
B	O
U	O
K	O
j	O
v	O
C	O
o	O
L	O
n	O
6	O
+	O
0	B-DatasetName
W	O
J	O
M	O
1	O
1	O
l	O
s	O
z	O
c	O
z	O
b	O
M	O
Z	O
6	O
1	O
a	O
v	O
E	O
/	O
7	O
x	O
B	O
Y	O
d	O
L	O
L	O
q	O
G	O
Q	O
i	O
L	O
w	O
w	O
V	O
Z	O
P	O
F	O
R	O
W	O
n	O
B	O
k	O
J	O
K	O
p	O
6	O
Q	O
A	O
l	O
T	O
l	O
B	O
g	O
+	O
t	O
Q	O
Q	O
T	O
x	O
W	O
x	O
W	O
R	O
M	O
Z	O
Y	O
Y	O
W	O
J	O
s	O
W	O
w	O
1	O
b	O
g	O
r	O
+	O
6	O
8	O
j	O
o	O
J	O
z	O
j	O
t	O
X	O
H	O
f	O
/	O
O	O
g	O
z	O
o	O
c	O
w	O
w	O
m	O
c	O
g	O
Q	O
8	O
X	O
c	O
A	O
2	O
3	O
0	B-DatasetName
I	O
U	O
A	O
C	O
D	O
z	O
B	O
C	O
7	O
z	O
B	O
u	O
/	O
P	O
s	O
v	O
D	O
o	O
f	O
i	O
7	O
Z	O
q	O
z	O
r	O
K	O
2	O
I	O
/	O
g	O
D	O
5	O
/	O
M	O
H	O
b	O
U	O
6	O
S	O
g	O
A	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
S	O
d	O
O	O
L	O
O	O
h	O
y	O
v	O
h	O
a	O
t	O
7	O
G	O
h	O
d	O
S	O
U	O
z	O
X	O
L	O
f	O
g	O
4	O
p	O
i	O
J	O
4	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
+	O
X	O
i	O
c	O
b	O
V	O
C	O
9	O
T	O
s	O
M	O
w	O
G	O
P	O
z	O
C	O
b	O
y	O
l	O
/	O
K	O
Y	O
w	O
s	O
F	O
h	O
U	O
S	O
U	O
5	O
W	O
w	O
A	O
F	O
s	O
F	O
C	O
2	O
O	O
R	O
C	O
K	O
3	O
U	O
R	O
J	O
X	O
j	O
O	O
K	O
1	O
V	O
J	O
4	O
5	O
s	O
B	O
1	O
S	O
F	O
P	O
g	O
o	O
L	O
A	O
y	O
B	O
W	O
3	O
o	O
S	O
N	O
t	O
8	O
F	O
p	O
M	O
0	B-DatasetName
D	O
L	O
S	O
Z	O
Z	O
P	O
d	O
9	O
8	O
n	O
n	O
y	O
/	O
M	O
O	O
F	O
P	O
a	O
c	O
b	O
6	O
t	O
l	O
d	O
W	O
1	O
9	O
Y	O
3	O
N	O
2	O
l	O
Z	O
9	O
e	O
2	O
d	O
3	O
b	O
9	O
9	O
u	O
H	O
N	O
w	O
r	O
k	O
U	O
t	O
C	O
P	O
S	O
K	O
4	O
k	O
L	O
0	B-DatasetName
Q	O
K	O
8	O
p	O
Z	O
S	O
j	O
3	O
N	O
N	O
K	O
e	O
9	O
T	O
F	O
K	O
c	O
h	O
J	O
x	O
2	O
w	O
/	O
F	O
1	O
6	O
X	O
c	O
f	O
q	O
F	O
R	O
M	O
p	O
H	O
d	O
6	O
k	O
t	O
E	O
g	O
w	O
c	O
O	O
U	O
x	O
Y	O
x	O
g	O
b	O
a	O
S	O
B	O
3	O
f	O
A	O
L	O
P	O
x	O
Q	O
8	O
U	O
p	O
P	O
E	O
X	O
K	O
j	O
n	O
T	O
w	O
d	O
2	O
0	B-DatasetName
2	O
k	O
5	O
M	O
6	O
B	O
l	O
4	O
l	O
a	O
k	O
C	O
R	O
U	O
6	O
A	O
/	O
v	O
L	O
j	O
w	O
T	O
J	O
E	O
5	O
p	O
q	O
w	O
r	O
F	O
S	O
f	O
d	O
f	O
J	O
d	O
F	O
B	O
g	O
q	O
R	O
n	O
h	O
d	O
F	O
r	O
3	O
c	O
0	B-DatasetName
U	O
z	O
T	O
M	O
Z	O
4	O
S	O
P	O
u	O
G	O
p	O
j	O
i	O
h	O
K	O
i	O
h	O
m	O
0	B-DatasetName
a	O
f	O
o	O
x	O
C	O
g	O
R	O
i	O
o	O
U	O
0	B-DatasetName
J	O
9	O
V	O
o	O
p	O
v	O
7	O
e	O
K	O
H	O
C	O
i	O
y	O
m	O
x	O
m	O
M	O
s	O
F	O
6	O
p	O
B	O
a	O
9	O
U	O
v	O
z	O
P	O
6	O
+	O
c	O
6	O
v	O
g	O
g	O
K	O
l	O
m	O
a	O
5	O
p	O
i	O
m	O
Z	O
P	O
d	O
g	O
7	O
h	O
D	O
6	O
z	O
P	O
H	O
8	O
l	O
o	O
k	O
+	O
A	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
U	O
0	B-DatasetName
g	O
M	O
h	O
Y	O
y	O
t	O
y	O
h	O
J	O
9	O
O	O
i	O
N	O
I	O
M	O
Q	O
F	O
4	O
c	O
M	O
z	O
J	O
C	O
Y	O
s	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
+	O
X	O
i	O
c	O
b	O
V	O
C	O
9	O
T	O
s	O
M	O
w	O
G	O
P	O
x	O
S	O
/	O
k	O
r	O
5	O
S	O
2	O
F	O
k	O
s	O
a	O
i	O
Q	O
m	O
K	O
o	O
E	O
I	O
Q	O
F	O
b	O
B	O
Q	O
t	O
j	O
k	O
Q	O
i	O
t	O
1	O
E	O
S	O
V	O
4	O
7	O
i	O
t	O
V	O
c	O
e	O
J	O
b	O
A	O
d	O
U	O
h	O
T	O
4	O
K	O
C	O
w	O
M	O
g	O
V	O
t	O
6	O
E	O
j	O
b	O
f	O
B	O
a	O
T	O
N	O
A	O
y	O
0	B-DatasetName
m	O
W	O
T	O
3	O
f	O
f	O
J	O
5	O
8	O
v	O
T	O
D	O
l	O
T	O
2	O
n	O
G	O
+	O
r	O
c	O
r	O
K	O
6	O
t	O
r	O
6	O
R	O
n	O
W	O
z	O
t	O
r	O
W	O
9	O
s	O
7	O
t	O
n	O
1	O
/	O
f	O
v	O
V	O
Z	O
J	O
J	O
Q	O
j	O
2	O
S	O
8	O
E	O
R	O
2	O
Q	O
6	O
w	O
o	O
Z	O
4	O
J	O
6	O
m	O
m	O
l	O
O	O
u	O
6	O
m	O
k	O
O	O
A	O
4	O
5	O
7	O
Y	O
T	O
j	O
6	O
8	O
L	O
v	O
P	O
F	O
C	O
p	O
W	O
C	O
L	O
u	O
9	O
C	O
S	O
l	O
Q	O
Y	O
y	O
H	O
g	O
g	O
0	B-DatasetName
Y	O
w	O
d	O
p	O
I	O
f	O
b	O
v	O
u	O
5	O
3	O
6	O
Y	O
8	O
E	O
h	O
N	O
Y	O
n	O
O	O
h	O
r	O
j	O
/	O
t	O
2	O
w	O
2	O
n	O
6	O
c	O
y	O
A	O
l	O
o	O
l	O
b	O
k	O
g	O
a	O
U	O
a	O
P	O
f	O
t	O
L	O
z	O
9	O
K	O
S	O
B	O
Z	O
T	O
o	O
Q	O
n	O
H	O
S	O
v	O
V	O
c	O
J	O
9	O
V	O
B	O
j	O
q	O
V	O
m	O
h	O
N	O
N	O
p	O
z	O
c	O
8	O
U	O
T	O
T	O
E	O
Z	O
4	O
y	O
H	O
t	O
G	O
S	O
p	O
w	O
T	O
F	O
W	O
Q	O
z	O
6	O
J	O
P	O
0	B-DatasetName
b	O
F	O
R	O
I	O
j	O
R	O
I	O
p	O
D	O
l	O
C	O
o	O
5	O
n	O
6	O
e	O
y	O
P	O
H	O
s	O
S	O
q	O
y	O
m	O
c	O
k	O
Y	O
6	O
5	O
F	O
a	O
9	O
A	O
r	O
x	O
P	O
6	O
+	O
X	O
6	O
c	O
F	O
F	O
k	O
D	O
O	O
R	O
Z	O
p	O
o	O
K	O
M	O
n	O
9	O
o	O
k	O
H	O
G	O
k	O
E	O
1	O
T	O
0	B-DatasetName
g	O
C	O
I	O
m	O
K	O
d	O
F	O
8	O
Y	O
g	O
g	O
m	O
k	O
p	O
m	O
s	O
i	O
I	O
y	O
w	O
x	O
E	O
S	O
b	O
t	O
m	O
q	O
m	O
B	O
H	O
f	O
x	O
y	O
8	O
v	O
E	O
O	O
2	O
1	O
e	O
N	O
t	O
3	O
b	O
s	O
0	B-DatasetName
b	O
r	O
q	O
m	O
y	O
j	O
C	O
o	O
d	O
w	O
B	O
C	O
f	O
g	O
w	O
j	O
m	O
0	B-DatasetName
4	O
A	O
b	O
a	O
4	O
A	O
G	O
B	O
R	O
3	O
i	O
G	O
V	O
3	O
i	O
z	O
n	O
q	O
w	O
X	O
6	O
9	O
3	O
6	O
m	O
I	O
9	O
W	O
r	O
H	O
L	O
n	O
A	O
P	O
7	O
A	O
+	O
v	O
w	O
B	O
y	O
q	O
i	O
T	O
5	O
A	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
U	O
0	B-DatasetName
g	O
M	O
h	O
Y	O
y	O
t	O
y	O
h	O
J	O
9	O
O	O
i	O
N	O
I	O
M	O
Q	O
F	O
4	O
c	O
M	O
z	O
J	O
C	O
Y	O
s	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
+	O
X	O
i	O
c	O
b	O
V	O
C	O
9	O
T	O
s	O
M	O
w	O
G	O
P	O
x	O
S	O
/	O
k	O
r	O
5	O
S	O
2	O
F	O
k	O
s	O
a	O
i	O
Q	O
m	O
K	O
o	O
E	O
I	O
Q	O
F	O
b	O
B	O
Q	O
t	O
j	O
k	O
Q	O
i	O
t	O
1	O
E	O
S	O
V	O
4	O
7	O
i	O
t	O
V	O
c	O
e	O
J	O
b	O
A	O
d	O
U	O
h	O
T	O
4	O
K	O
C	O
w	O
M	O
g	O
V	O
t	O
6	O
E	O
j	O
b	O
f	O
B	O
a	O
T	O
N	O
A	O
y	O
0	B-DatasetName
m	O
W	O
T	O
3	O
f	O
f	O
J	O
5	O
8	O
v	O
T	O
D	O
l	O
T	O
2	O
n	O
G	O
+	O
r	O
c	O
r	O
K	O
6	O
t	O
r	O
6	O
R	O
n	O
W	O
z	O
t	O
r	O
W	O
9	O
s	O
7	O
t	O
n	O
1	O
/	O
f	O
v	O
V	O
Z	O
J	O
J	O
Q	O
j	O
2	O
S	O
8	O
E	O
R	O
2	O
Q	O
6	O
w	O
o	O
Z	O
4	O
J	O
6	O
m	O
m	O
l	O
O	O
u	O
6	O
m	O
k	O
O	O
A	O
4	O
5	O
7	O
Y	O
T	O
j	O
6	O
8	O
L	O
v	O
P	O
F	O
C	O
p	O
W	O
C	O
L	O
u	O
9	O
C	O
S	O
l	O
Q	O
Y	O
y	O
H	O
g	O
g	O
0	B-DatasetName
Y	O
w	O
d	O
p	O
I	O
f	O
b	O
v	O
u	O
5	O
3	O
6	O
Y	O
8	O
E	O
h	O
N	O
Y	O
n	O
O	O
h	O
r	O
j	O
/	O
t	O
2	O
w	O
2	O
n	O
6	O
c	O
y	O
A	O
l	O
o	O
l	O
b	O
k	O
g	O
a	O
U	O
a	O
P	O
f	O
t	O
L	O
z	O
9	O
K	O
S	O
B	O
Z	O
T	O
o	O
Q	O
n	O
H	O
S	O
v	O
V	O
c	O
J	O
9	O
V	O
B	O
j	O
q	O
V	O
m	O
h	O
N	O
N	O
p	O
z	O
c	O
8	O
U	O
T	O
T	O
E	O
Z	O
4	O
y	O
H	O
t	O
G	O
S	O
p	O
w	O
T	O
F	O
W	O
Q	O
z	O
6	O
J	O
P	O
0	B-DatasetName
b	O
F	O
R	O
I	O
j	O
R	O
I	O
p	O
D	O
l	O
C	O
o	O
5	O
n	O
6	O
e	O
y	O
P	O
H	O
s	O
S	O
q	O
y	O
m	O
c	O
k	O
Y	O
6	O
5	O
F	O
a	O
9	O
A	O
r	O
x	O
P	O
6	O
+	O
X	O
6	O
c	O
F	O
F	O
k	O
D	O
O	O
R	O
Z	O
p	O
o	O
K	O
M	O
n	O
9	O
o	O
k	O
H	O
G	O
k	O
E	O
1	O
T	O
0	B-DatasetName
g	O
C	O
I	O
m	O
K	O
d	O
F	O
8	O
Y	O
g	O
g	O
m	O
k	O
p	O
m	O
s	O
i	O
I	O
y	O
w	O
x	O
E	O
S	O
b	O
t	O
m	O
q	O
m	O
B	O
H	O
f	O
x	O
y	O
8	O
v	O
E	O
O	O
2	O
1	O
e	O
N	O
t	O
3	O
b	O
s	O
0	B-DatasetName
b	O
r	O
q	O
m	O
y	O
j	O
C	O
o	O
d	O
w	O
B	O
C	O
f	O
g	O
w	O
j	O
m	O
0	B-DatasetName
4	O
A	O
b	O
a	O
4	O
A	O
G	O
B	O
R	O
3	O
i	O
G	O
V	O
3	O
i	O
z	O
n	O
q	O
w	O
X	O
6	O
9	O
3	O
6	O
m	O
I	O
9	O
W	O
r	O
H	O
L	O
n	O
A	O
P	O
7	O
A	O
+	O
v	O
w	O
B	O
y	O
q	O
i	O
T	O
5	O
A	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
U	O
0	B-DatasetName
g	O
M	O
h	O
Y	O
y	O
t	O
y	O
h	O
J	O
9	O
O	O
i	O
N	O
I	O
M	O
Q	O
F	O
4	O
c	O
M	O
z	O
J	O
C	O
Y	O
s	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
+	O
X	O
i	O
c	O
b	O
V	O
C	O
9	O
T	O
s	O
M	O
w	O
G	O
P	O
x	O
S	O
/	O
k	O
r	O
5	O
S	O
2	O
F	O
k	O
s	O
a	O
i	O
Q	O
m	O
K	O
o	O
E	O
I	O
Q	O
F	O
b	O
B	O
Q	O
t	O
j	O
k	O
Q	O
i	O
t	O
1	O
E	O
S	O
V	O
4	O
7	O
i	O
t	O
V	O
c	O
e	O
J	O
b	O
A	O
d	O
U	O
h	O
T	O
4	O
K	O
C	O
w	O
M	O
g	O
V	O
t	O
6	O
E	O
j	O
b	O
f	O
B	O
a	O
T	O
N	O
A	O
y	O
0	B-DatasetName
m	O
W	O
T	O
3	O
f	O
f	O
J	O
5	O
8	O
v	O
T	O
D	O
l	O
T	O
2	O
n	O
G	O
+	O
r	O
c	O
r	O
K	O
6	O
t	O
r	O
6	O
R	O
n	O
W	O
z	O
t	O
r	O
W	O
9	O
s	O
7	O
t	O
n	O
1	O
/	O
f	O
v	O
V	O
Z	O
J	O
J	O
Q	O
j	O
2	O
S	O
8	O
E	O
R	O
2	O
Q	O
6	O
w	O
o	O
Z	O
4	O
J	O
6	O
m	O
m	O
l	O
O	O
u	O
6	O
m	O
k	O
O	O
A	O
4	O
5	O
7	O
Y	O
T	O
j	O
6	O
8	O
L	O
v	O
P	O
F	O
C	O
p	O
W	O
C	O
L	O
u	O
9	O
C	O
S	O
l	O
Q	O
Y	O
y	O
H	O
g	O
g	O
0	B-DatasetName
Y	O
w	O
d	O
p	O
I	O
f	O
b	O
v	O
u	O
5	O
3	O
6	O
Y	O
8	O
E	O
h	O
N	O
Y	O
n	O
O	O
h	O
r	O
j	O
/	O
t	O
2	O
w	O
2	O
n	O
6	O
c	O
y	O
A	O
l	O
o	O
l	O
b	O
k	O
g	O
a	O
U	O
a	O
P	O
f	O
t	O
L	O
z	O
9	O
K	O
S	O
B	O
Z	O
T	O
o	O
Q	O
n	O
H	O
S	O
v	O
V	O
c	O
J	O
9	O
V	O
B	O
j	O
q	O
V	O
m	O
h	O
N	O
N	O
p	O
z	O
c	O
8	O
U	O
T	O
T	O
E	O
Z	O
4	O
y	O
H	O
t	O
G	O
S	O
p	O
w	O
T	O
F	O
W	O
Q	O
z	O
6	O
J	O
P	O
0	B-DatasetName
b	O
F	O
R	O
I	O
j	O
R	O
I	O
p	O
D	O
l	O
C	O
o	O
5	O
n	O
6	O
e	O
y	O
P	O
H	O
s	O
S	O
q	O
y	O
m	O
c	O
k	O
Y	O
6	O
5	O
F	O
a	O
9	O
A	O
r	O
x	O
P	O
6	O
+	O
X	O
6	O
c	O
F	O
F	O
f	O
g	O
w	O
j	O
m	O
0	B-DatasetName
4	O
A	O
b	O
a	O
4	O
A	O
G	O
B	O
R	O
3	O
i	O
G	O
V	O
3	O
i	O
z	O
n	O
q	O
w	O
X	O
6	O
9	O
3	O
6	O
m	O
I	O
9	O
W	O
r	O
H	O
L	O
n	O
A	O
P	O
7	O
A	O
+	O
v	O
w	O
B	O
y	O
q	O
i	O
T	O
5	O
A	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
U	O
0	B-DatasetName
g	O
M	O
h	O
Y	O
y	O
t	O
y	O
h	O
J	O
9	O
O	O
i	O
N	O
I	O
M	O
Q	O
F	O
4	O
c	O
M	O
z	O
J	O
C	O
Y	O
s	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
+	O
X	O
i	O
c	O
b	O
V	O
C	O
9	O
T	O
s	O
M	O
w	O
G	O
P	O
x	O
S	O
/	O
k	O
r	O
5	O
S	O
2	O
F	O
k	O
s	O
a	O
i	O
Q	O
m	O
K	O
o	O
E	O
I	O
Q	O
F	O
b	O
B	O
Q	O
t	O
j	O
k	O
Q	O
i	O
t	O
1	O
E	O
S	O
V	O
4	O
7	O
i	O
t	O
V	O
c	O
e	O
J	O
b	O
A	O
d	O
U	O
h	O
T	O
4	O
K	O
C	O
w	O
M	O
g	O
V	O
t	O
6	O
E	O
j	O
b	O
f	O
B	O
a	O
T	O
N	O
A	O
y	O
0	B-DatasetName
m	O
W	O
T	O
3	O
f	O
f	O
J	O
5	O
8	O
v	O
T	O
D	O
l	O
T	O
2	O
n	O
G	O
+	O
r	O
c	O
r	O
K	O
6	O
t	O
r	O
6	O
R	O
n	O
W	O
z	O
t	O
r	O
W	O
9	O
s	O
7	O
t	O
n	O
1	O
/	O
f	O
v	O
V	O
Z	O
J	O
J	O
Q	O
j	O
2	O
S	O
8	O
E	O
R	O
2	O
Q	O
6	O
w	O
o	O
Z	O
4	O
J	O
6	O
m	O
m	O
l	O
O	O
u	O
6	O
m	O
k	O
O	O
A	O
4	O
5	O
7	O
Y	O
T	O
j	O
6	O
8	O
L	O
v	O
P	O
F	O
C	O
p	O
W	O
C	O
L	O
u	O
9	O
C	O
S	O
l	O
Q	O
Y	O
y	O
H	O
g	O
g	O
0	B-DatasetName
Y	O
w	O
d	O
p	O
I	O
f	O
b	O
v	O
u	O
5	O
3	O
6	O
Y	O
8	O
E	O
h	O
N	O
Y	O
n	O
O	O
h	O
r	O
j	O
/	O
t	O
2	O
w	O
2	O
n	O
6	O
c	O
y	O
A	O
l	O
o	O
l	O
b	O
k	O
g	O
a	O
U	O
a	O
P	O
f	O
t	O
L	O
z	O
9	O
K	O
S	O
B	O
Z	O
T	O
o	O
Q	O
n	O
H	O
S	O
v	O
V	O
c	O
J	O
9	O
V	O
B	O
j	O
q	O
V	O
m	O
h	O
N	O
N	O
p	O
z	O
c	O
8	O
U	O
T	O
T	O
E	O
Z	O
4	O
y	O
H	O
t	O
G	O
S	O
p	O
w	O
T	O
F	O
W	O
Q	O
z	O
6	O
J	O
P	O
0	B-DatasetName
b	O
F	O
R	O
I	O
j	O
R	O
I	O
p	O
D	O
l	O
C	O
o	O
5	O
n	O
6	O
e	O
y	O
P	O
H	O
s	O
S	O
q	O
y	O
m	O
c	O
k	O
Y	O
6	O
5	O
F	O
a	O
9	O
A	O
r	O
x	O
P	O
6	O
+	O
X	O
6	O
c	O
F	O
F	O
f	O
g	O
w	O
j	O
m	O
0	B-DatasetName
4	O
A	O
b	O
a	O
4	O
A	O
G	O
B	O
R	O
3	O
i	O
G	O
V	O
3	O
i	O
z	O
n	O
q	O
w	O
X	O
6	O
9	O
3	O
6	O
m	O
I	O
9	O
W	O
r	O
H	O
L	O
n	O
A	O
P	O
7	O
A	O
+	O
v	O
w	O
B	O
y	O
q	O
i	O
T	O
5	O
A	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
U	O
0	B-DatasetName
g	O
M	O
h	O
Y	O
y	O
t	O
y	O
h	O
J	O
9	O
O	O
i	O
N	O
I	O
M	O
Q	O
F	O
4	O
c	O
M	O
z	O
J	O
C	O
Y	O
s	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
+	O
X	O
i	O
c	O
b	O
V	O
C	O
9	O
T	O
s	O
M	O
w	O
G	O
P	O
x	O
S	O
/	O
k	O
r	O
5	O
S	O
2	O
F	O
k	O
s	O
a	O
i	O
Q	O
m	O
K	O
o	O
E	O
I	O
Q	O
F	O
b	O
B	O
Q	O
t	O
j	O
k	O
Q	O
i	O
t	O
1	O
E	O
S	O
V	O
4	O
7	O
i	O
t	O
V	O
c	O
e	O
J	O
b	O
A	O
d	O
U	O
h	O
T	O
4	O
K	O
C	O
w	O
M	O
g	O
V	O
t	O
6	O
E	O
j	O
b	O
f	O
B	O
a	O
T	O
N	O
A	O
y	O
0	B-DatasetName
m	O
W	O
T	O
3	O
f	O
f	O
J	O
5	O
8	O
v	O
T	O
D	O
l	O
T	O
2	O
n	O
G	O
+	O
r	O
c	O
r	O
K	O
6	O
t	O
r	O
6	O
R	O
n	O
W	O
z	O
t	O
r	O
W	O
9	O
s	O
7	O
t	O
n	O
1	O
/	O
f	O
v	O
V	O
Z	O
J	O
J	O
Q	O
j	O
2	O
S	O
8	O
E	O
R	O
2	O
Q	O
6	O
w	O
o	O
Z	O
4	O
J	O
6	O
m	O
m	O
l	O
O	O
u	O
6	O
m	O
k	O
O	O
A	O
4	O
5	O
7	O
Y	O
T	O
j	O
6	O
8	O
L	O
v	O
P	O
F	O
C	O
p	O
W	O
C	O
L	O
u	O
9	O
C	O
S	O
l	O
Q	O
Y	O
y	O
H	O
g	O
g	O
0	B-DatasetName
Y	O
w	O
d	O
p	O
I	O
f	O
b	O
v	O
u	O
5	O
3	O
6	O
Y	O
8	O
E	O
h	O
N	O
Y	O
n	O
O	O
h	O
r	O
j	O
/	O
t	O
2	O
w	O
2	O
n	O
6	O
c	O
y	O
A	O
l	O
o	O
l	O
b	O
k	O
g	O
a	O
U	O
a	O
P	O
f	O
t	O
L	O
z	O
9	O
K	O
S	O
B	O
Z	O
T	O
o	O
Q	O
n	O
H	O
S	O
v	O
V	O
c	O
J	O
9	O
V	O
B	O
j	O
q	O
V	O
m	O
h	O
N	O
N	O
p	O
z	O
c	O
8	O
U	O
T	O
T	O
E	O
Z	O
4	O
y	O
H	O
t	O
G	O
S	O
p	O
w	O
T	O
F	O
W	O
Q	O
z	O
6	O
J	O
P	O
0	B-DatasetName
b	O
F	O
R	O
I	O
j	O
R	O
I	O
p	O
D	O
l	O
C	O
o	O
5	O
n	O
6	O
e	O
y	O
P	O
H	O
s	O
S	O
q	O
y	O
m	O
c	O
k	O
Y	O
6	O
5	O
F	O
a	O
9	O
A	O
r	O
x	O
P	O
6	O
+	O
X	O
6	O
c	O
F	O
F	O
f	O
g	O
w	O
j	O
m	O
0	B-DatasetName
4	O
A	O
b	O
a	O
4	O
A	O
G	O
B	O
R	O
3	O
i	O
G	O
V	O
3	O
i	O
z	O
n	O
q	O
w	O
X	O
6	O
9	O
3	O
6	O
m	O
I	O
9	O
W	O
r	O
H	O
L	O
n	O
A	O
P	O
7	O
A	O
+	O
v	O
w	O
B	O
y	O
q	O
i	O
T	O
5	O
A	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
U	O
0	B-DatasetName
g	O
M	O
h	O
Y	O
y	O
t	O
y	O
h	O
J	O
9	O
O	O
i	O
N	O
I	O
M	O
Q	O
F	O
4	O
c	O
M	O
z	O
J	O
C	O
Y	O
s	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
+	O
X	O
i	O
c	O
b	O
V	O
C	O
9	O
T	O
s	O
M	O
w	O
G	O
P	O
x	O
S	O
/	O
k	O
r	O
5	O
S	O
2	O
F	O
k	O
s	O
a	O
i	O
Q	O
m	O
K	O
o	O
E	O
I	O
Q	O
F	O
b	O
B	O
Q	O
t	O
j	O
k	O
Q	O
i	O
t	O
1	O
E	O
S	O
V	O
4	O
7	O
i	O
t	O
V	O
c	O
e	O
J	O
b	O
A	O
d	O
U	O
h	O
T	O
4	O
K	O
C	O
w	O
M	O
g	O
V	O
t	O
6	O
E	O
j	O
b	O
f	O
B	O
a	O
T	O
N	O
A	O
y	O
0	B-DatasetName
m	O
W	O
T	O
3	O
f	O
f	O
J	O
5	O
8	O
v	O
T	O
D	O
l	O
T	O
2	O
n	O
G	O
+	O
r	O
c	O
r	O
K	O
6	O
t	O
r	O
6	O
R	O
n	O
W	O
z	O
t	O
r	O
W	O
9	O
s	O
7	O
t	O
n	O
1	O
/	O
f	O
v	O
V	O
Z	O
J	O
J	O
Q	O
j	O
2	O
S	O
8	O
E	O
R	O
2	O
Q	O
6	O
w	O
o	O
Z	O
4	O
J	O
6	O
m	O
m	O
l	O
O	O
u	O
6	O
m	O
k	O
O	O
A	O
4	O
5	O
7	O
Y	O
T	O
j	O
6	O
8	O
L	O
v	O
P	O
F	O
C	O
p	O
W	O
C	O
L	O
u	O
9	O
C	O
S	O
l	O
Q	O
Y	O
y	O
H	O
g	O
g	O
0	B-DatasetName
Y	O
w	O
d	O
p	O
I	O
f	O
b	O
v	O
u	O
5	O
3	O
6	O
Y	O
8	O
E	O
h	O
N	O
Y	O
n	O
O	O
h	O
r	O
j	O
/	O
t	O
2	O
w	O
2	O
n	O
6	O
c	O
y	O
A	O
l	O
o	O
l	O
b	O
k	O
g	O
a	O
U	O
a	O
P	O
f	O
t	O
L	O
z	O
9	O
K	O
S	O
B	O
Z	O
T	O
o	O
Q	O
n	O
H	O
S	O
v	O
V	O
c	O
J	O
9	O
V	O
B	O
j	O
q	O
V	O
m	O
h	O
N	O
N	O
p	O
z	O
c	O
8	O
U	O
T	O
T	O
E	O
Z	O
4	O
y	O
H	O
t	O
G	O
S	O
p	O
w	O
T	O
F	O
W	O
Q	O
z	O
6	O
J	O
P	O
0	B-DatasetName
b	O
F	O
R	O
I	O
j	O
R	O
I	O
p	O
D	O
l	O
C	O
o	O
5	O
n	O
6	O
e	O
y	O
P	O
H	O
s	O
S	O
q	O
y	O
m	O
c	O
k	O
Y	O
6	O
5	O
F	O
a	O
9	O
A	O
r	O
x	O
P	O
6	O
+	O
X	O
6	O
c	O
F	O
F	O
f	O
g	O
w	O
j	O
m	O
0	B-DatasetName
4	O
A	O
b	O
a	O
4	O
A	O
G	O
B	O
R	O
3	O
i	O
G	O
V	O
3	O
i	O
z	O
n	O
q	O
w	O
X	O
6	O
9	O
3	O
6	O
m	O
I	O
9	O
W	O
r	O
H	O
L	O
n	O
A	O
P	O
7	O
A	O
+	O
v	O
w	O
B	O
y	O
q	O
i	O
T	O
5	O
A	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
s	O
t	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
5	O
n	O
C	O
j	O
d	O
F	O
J	O
8	O
J	O
R	O
B	O
I	O
R	O
q	O
G	O
7	O
y	O
f	O
D	O
E	O
S	O
c	O
q	O
x	O
v	O
i	O
U	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
U	O
G	O
9	O
F	O
L	O
x	O
4	O
r	O
G	O
l	O
t	O
o	O
Q	O
9	O
l	O
s	O
N	O
+	O
3	O
S	O
z	O
S	O
b	O
s	O
T	O
o	O
Q	O
S	O
+	O
h	O
O	O
8	O
e	O
F	O
D	O
x	O
6	O
j	O
/	O
y	O
5	O
r	O
9	O
x	O
2	O
+	O
a	O
g	O
r	O
Q	O
8	O
G	O
H	O
u	O
/	O
N	O
M	O
D	O
M	O
v	O
T	O
K	O
U	O
w	O
6	O
L	O
r	O
f	O
T	O
m	O
l	O
l	O
d	O
W	O
1	O
9	O
o	O
7	O
x	O
Z	O
2	O
d	O
r	O
e	O
2	O
d	O
2	O
r	O
7	O
h	O
8	O
8	O
m	O
i	O
T	O
T	O
j	O
P	O
s	O
s	O
k	O
Y	O
l	O
u	O
h	O
9	O
R	O
w	O
K	O
R	O
T	O
3	O
U	O
a	O
D	O
k	O
7	O
V	O
R	O
z	O
G	O
o	O
e	O
S	O
t	O
8	O
L	O
R	O
z	O
d	O
R	O
v	O
P	O
X	O
F	O
t	O
R	O
K	O
I	O
e	O
c	O
J	O
z	O
y	O
I	O
K	O
Y	O
D	O
J	O
S	O
L	O
B	O
K	O
F	O
r	O
p	O
3	O
v	O
S	O
w	O
V	O
6	O
2	O
5	O
d	O
X	O
c	O
G	O
s	O
k	O
y	O
8	O
g	O
t	O
S	O
g	O
Q	O
L	O
N	O
X	O
/	O
e	O
r	O
2	O
E	O
5	O
b	O
F	O
X	O
C	O
G	O
T	O
1	O
J	O
i	O
O	O
5	O
6	O
Y	O
Y	O
5	O
F	O
S	O
j	O
Y	O
J	O
J	O
P	O
K	O
t	O
3	O
M	O
8	O
J	O
S	O
y	O
E	O
R	O
3	O
w	O
j	O
q	O
W	O
K	O
x	O
t	O
w	O
E	O
+	O
e	O
z	O
U	O
C	O
T	O
m	O
x	O
S	O
p	O
9	O
p	O
e	O
H	O
A	O
B	O
D	O
b	O
i	O
F	O
J	O
v	O
j	O
A	O
Y	O
A	O
D	O
P	O
8	O
A	O
p	O
v	O
j	O
n	O
R	O
e	O
n	O
H	O
f	O
n	O
Y	O
9	O
5	O
a	O
c	O
o	O
q	O
Z	O
Q	O
/	O
g	O
D	O
5	O
/	O
M	O
H	O
1	O
y	O
W	O
N	O
s	O
g	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
5	O
n	O
C	O
j	O
d	O
F	O
J	O
8	O
J	O
R	O
B	O
I	O
R	O
q	O
G	O
7	O
y	O
f	O
D	O
E	O
S	O
c	O
q	O
x	O
v	O
i	O
U	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
U	O
G	O
9	O
F	O
L	O
x	O
4	O
r	O
G	O
l	O
t	O
o	O
Q	O
9	O
l	O
s	O
N	O
+	O
3	O
S	O
z	O
S	O
b	O
s	O
T	O
o	O
Q	O
S	O
+	O
h	O
O	O
8	O
e	O
F	O
D	O
x	O
6	O
j	O
/	O
y	O
5	O
r	O
9	O
x	O
2	O
+	O
a	O
g	O
r	O
Q	O
8	O
G	O
H	O
u	O
/	O
N	O
M	O
D	O
M	O
v	O
T	O
K	O
U	O
w	O
6	O
L	O
r	O
f	O
T	O
m	O
l	O
l	O
d	O
W	O
1	O
9	O
o	O
7	O
x	O
Z	O
2	O
d	O
r	O
e	O
2	O
d	O
2	O
r	O
7	O
h	O
8	O
8	O
m	O
i	O
T	O
T	O
j	O
P	O
s	O
s	O
k	O
Y	O
l	O
u	O
h	O
9	O
R	O
w	O
K	O
R	O
T	O
3	O
U	O
a	O
D	O
k	O
7	O
V	O
R	O
z	O
G	O
o	O
e	O
S	O
t	O
8	O
L	O
R	O
z	O
d	O
R	O
v	O
P	O
X	O
F	O
t	O
R	O
K	O
I	O
e	O
c	O
J	O
z	O
y	O
I	O
K	O
Y	O
D	O
J	O
S	O
L	O
B	O
K	O
F	O
r	O
p	O
3	O
v	O
S	O
w	O
V	O
6	O
2	O
5	O
d	O
X	O
c	O
G	O
s	O
k	O
y	O
8	O
g	O
t	O
S	O
g	O
Q	O
L	O
N	O
X	O
/	O
e	O
r	O
2	O
E	O
5	O
b	O
F	O
X	O
C	O
G	O
T	O
1	O
J	O
i	O
O	O
5	O
6	O
Y	O
Y	O
5	O
F	O
S	O
j	O
Y	O
J	O
J	O
P	O
K	O
t	O
3	O
M	O
8	O
J	O
S	O
y	O
E	O
R	O
3	O
w	O
j	O
q	O
W	O
K	O
x	O
t	O
w	O
E	O
+	O
e	O
z	O
U	O
C	O
T	O
m	O
x	O
S	O
p	O
9	O
p	O
e	O
H	O
A	O
B	O
D	O
b	O
i	O
F	O
J	O
v	O
j	O
A	O
Y	O
A	O
D	O
P	O
8	O
A	O
p	O
v	O
j	O
n	O
R	O
e	O
n	O
H	O
f	O
n	O
Y	O
9	O
5	O
a	O
c	O
o	O
q	O
Z	O
Q	O
/	O
g	O
D	O
5	O
/	O
M	O
H	O
1	O
y	O
W	O
N	O
s	O
g	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
5	O
n	O
C	O
j	O
d	O
F	O
J	O
8	O
J	O
R	O
B	O
I	O
R	O
q	O
G	O
7	O
y	O
f	O
D	O
E	O
S	O
c	O
q	O
x	O
v	O
i	O
U	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
U	O
G	O
9	O
F	O
L	O
x	O
4	O
r	O
G	O
l	O
t	O
o	O
Q	O
9	O
l	O
s	O
N	O
+	O
3	O
S	O
z	O
S	O
b	O
s	O
T	O
o	O
Q	O
S	O
+	O
h	O
O	O
8	O
e	O
F	O
D	O
x	O
6	O
j	O
/	O
y	O
5	O
r	O
9	O
x	O
2	O
+	O
a	O
g	O
r	O
Q	O
8	O
G	O
H	O
u	O
/	O
N	O
M	O
D	O
M	O
v	O
T	O
K	O
U	O
w	O
6	O
L	O
r	O
f	O
T	O
m	O
l	O
l	O
d	O
W	O
1	O
9	O
o	O
7	O
x	O
Z	O
2	O
d	O
r	O
e	O
2	O
d	O
2	O
r	O
7	O
h	O
8	O
8	O
m	O
i	O
T	O
T	O
j	O
P	O
s	O
s	O
k	O
Y	O
l	O
u	O
h	O
9	O
R	O
w	O
K	O
R	O
T	O
3	O
U	O
a	O
D	O
k	O
7	O
V	O
R	O
z	O
G	O
o	O
e	O
S	O
t	O
8	O
L	O
R	O
z	O
d	O
R	O
v	O
P	O
X	O
F	O
t	O
R	O
K	O
I	O
e	O
c	O
J	O
z	O
y	O
I	O
K	O
Y	O
D	O
J	O
S	O
L	O
B	O
K	O
F	O
r	O
p	O
3	O
v	O
S	O
w	O
V	O
6	O
2	O
5	O
d	O
X	O
c	O
G	O
s	O
k	O
y	O
8	O
g	O
t	O
S	O
g	O
Q	O
L	O
N	O
X	O
/	O
e	O
r	O
2	O
E	O
5	O
b	O
F	O
X	O
C	O
G	O
T	O
1	O
J	O
i	O
O	O
5	O
6	O
Y	O
Y	O
5	O
F	O
S	O
j	O
Y	O
J	O
J	O
P	O
K	O
t	O
3	O
M	O
8	O
J	O
S	O
y	O
E	O
R	O
3	O
w	O
j	O
q	O
W	O
K	O
x	O
t	O
w	O
E	O
+	O
e	O
z	O
U	O
C	O
T	O
m	O
x	O
S	O
p	O
9	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
g	O
t	O
a	O
G	O
d	O
G	O
s	O
R	O
i	O
N	O
S	O
q	O
c	O
f	O
e	O
a	O
q	O
5	O
f	O
3	O
G	O
N	O
U	O
B	O
Z	O
c	O
0	B-DatasetName
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
3	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
4	O
W	O
e	O
t	O
X	O
1	O
a	O
O	O
X	O
x	O
S	O
J	O
4	O
K	O
o	O
k	O
I	O
6	O
q	O
3	O
o	O
Q	O
Y	O
8	O
V	O
j	O
C	O
2	O
0	B-DatasetName
s	O
W	O
y	O
2	O
m	O
3	O
b	O
p	O
Z	O
h	O
N	O
2	O
J	O
0	B-DatasetName
I	O
J	O
/	O
Q	O
1	O
e	O
P	O
K	O
h	O
4	O
9	O
Q	O
9	O
5	O
8	O
9	O
+	O
4	O
b	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
g	O
t	O
a	O
G	O
d	O
G	O
s	O
R	O
i	O
N	O
S	O
q	O
c	O
f	O
e	O
a	O
q	O
5	O
f	O
3	O
G	O
N	O
U	O
B	O
Z	O
c	O
0	B-DatasetName
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
3	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
4	O
W	O
e	O
t	O
X	O
1	O
a	O
O	O
X	O
x	O
S	O
J	O
4	O
K	O
o	O
k	O
I	O
6	O
q	O
3	O
o	O
Q	O
Y	O
8	O
V	O
j	O
C	O
2	O
0	B-DatasetName
s	O
W	O
y	O
2	O
m	O
3	O
b	O
p	O
Z	O
h	O
N	O
2	O
J	O
0	B-DatasetName
I	O
J	O
/	O
Q	O
1	O
e	O
P	O
K	O
h	O
4	O
9	O
Q	O
9	O
5	O
8	O
9	O
+	O
4	O
b	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
g	O
t	O
a	O
G	O
d	O
G	O
s	O
R	O
i	O
N	O
S	O
q	O
c	O
f	O
e	O
a	O
q	O
5	O
f	O
3	O
G	O
N	O
U	O
B	O
Z	O
c	O
0	B-DatasetName
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
3	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
4	O
W	O
e	O
t	O
X	O
1	O
a	O
O	O
X	O
x	O
S	O
J	O
4	O
K	O
o	O
k	O
I	O
6	O
q	O
3	O
o	O
Q	O
Y	O
8	O
V	O
j	O
C	O
2	O
0	B-DatasetName
s	O
W	O
y	O
2	O
m	O
3	O
b	O
p	O
Z	O
h	O
N	O
2	O
J	O
0	B-DatasetName
I	O
J	O
/	O
Q	O
1	O
e	O
P	O
K	O
h	O
4	O
9	O
Q	O
9	O
5	O
8	O
9	O
+	O
4	O
b	O
y	O
5	O
a	O
J	O
h	O
J	O
o	O
h	O
J	O
y	O
P	O
x	O
z	O
M	O
u	O
A	O
K	O
m	O
R	O
F	O
T	O
S	O
y	O
h	O
T	O
3	O
N	O
5	O
K	O
2	O
J	O
g	O
q	O
y	O
o	O
z	O
N	O
p	O
2	O
x	O
D	O
8	O
F	O
Z	O
f	O
X	O
i	O
f	O
t	O
e	O
s	O
2	O
7	O
r	O
t	O
U	O
f	O
b	O
q	O
r	O
N	O
R	O
h	O
F	O
H	O
C	O
c	O
7	O
h	O
A	O
q	O
7	O
A	O
g	O
1	O
t	O
o	O
w	O
j	O
2	O
0	B-DatasetName
w	O
A	O
c	O
G	O
H	O
J	O
7	O
h	O
F	O
d	O
4	O
c	O
6	O
b	O
w	O
4	O
7	O
8	O
7	O
H	O
s	O
n	O
X	O
D	O
K	O
W	O
b	O
O	O
4	O
A	O
+	O
c	O
z	O
x	O
/	O
4	O
8	O
4	O
7	O
C	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O

We	O
test	O
the	O
proposed	O
framework	O
on	O
unconditional	O
and	O
conditional	B-TaskName
text	I-TaskName
generation	I-TaskName
tasks	O
,	O
and	O
analyze	O
the	O
results	O
to	O
understand	O
the	O
performance	O
gained	O
by	O
the	O
guider	O
network	O
.	O
We	O
also	O
perform	O
an	O
ablation	O
investigation	O
on	O
the	O
improvements	O
brought	O
by	O
each	O
part	O
of	O
our	O
proposed	O
method	O
,	O
and	O
consider	O
non	O
-	O
parallel	O
style	B-TaskName
transfer	I-TaskName
.	O
All	O
experiments	O
are	O
conducted	O
on	O
a	O
single	O
Tesla	O
P100	O
GPU	O
and	O
implemented	O
with	O
TensorFlow	O
and	O
Theano	O
.	O
Details	O
of	O
the	O
datasets	O
,	O
the	O
experimental	O
setup	O
and	O
model	O
architectures	O
are	O
provided	O
in	O
the	O
Appendix	O
.	O

Multi	O
-	O
Style	B-TaskName
Transfer	I-TaskName
with	O
Discriminative	O
Feedback	O
on	O
Disjoint	O
Corpus	O

Style	B-TaskName
transfer	I-TaskName
has	O
been	O
widely	O
explored	O
in	O
natural	O
language	O
generation	O
with	O
non	O
-	O
parallel	O
corpus	O
by	O
directly	O
or	O
indirectly	O
extracting	O
a	O
notion	O
of	O
style	O
from	O
source	O
and	O
target	O
domain	O
corpus	O
.	O
A	O
common	O
shortcoming	O
of	O
existing	O
approaches	O
is	O
the	O
prerequisite	O
of	O
joint	O
annotations	O
across	O
all	O
the	O
stylistic	O
dimensions	O
under	O
consideration	O
.	O
Availability	O
of	O
such	O
dataset	O
across	O
a	O
combination	O
of	O
styles	O
limits	O
the	O
extension	O
of	O
these	O
setups	O
to	O
multiple	O
style	O
dimensions	O
.	O
While	O
cascading	O
single	O
-	O
dimensional	O
models	O
across	O
multiple	O
styles	O
is	O
a	O
possibility	O
,	O
it	O
suffers	O
from	O
content	O
loss	B-MetricName
,	O
especially	O
when	O
the	O
style	O
dimensions	O
are	O
not	O
completely	O
independent	O
of	O
each	O
other	O
.	O
In	O
our	O
work	O
,	O
we	O
relax	O
this	O
requirement	O
of	O
jointly	O
annotated	O
data	O
across	O
multiple	O
styles	O
by	O
using	O
independently	O
acquired	O
data	O
across	O
different	O
style	O
dimensions	O
without	O
any	O
additional	O
annotations	O
.	O
We	O
initialize	O
an	O
encoder	O
-	O
decoder	O
setup	O
with	O
transformerbased	O
language	O
model	O
pre	O
-	O
trained	O
on	O
a	O
generic	O
corpus	O
and	O
enhance	O
its	O
re	O
-	O
writing	O
capability	O
to	O
multiple	O
target	O
style	O
dimensions	O
by	O
employing	O
multiple	O
style	O
-	O
aware	O
language	O
models	O
as	O
discriminators	O
.	O
Through	O
quantitative	O
and	O
qualitative	O
evaluation	O
,	O
we	O
show	O
the	O
ability	O
of	O
our	O
model	O
to	O
control	O
styles	O
across	O
multiple	O
style	O
dimensions	O
while	O
preserving	O
content	O
of	O
the	O
input	O
text	O
.	O
We	O
compare	O
it	O
against	O
baselines	O
involving	O
cascaded	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
uni	O
-	O
dimensional	O
style	B-TaskName
transfer	I-TaskName
models	O
.	O

Style	B-TaskName
transfer	I-TaskName
is	O
a	O
popular	O
task	O
in	O
natural	O
language	O
processing	O
and	O
has	O
been	O
studied	O
on	O
attributes	O
like	O
age	O
or	O
gender	O
(	O
Subramanian	O
et	O
al	O
,	O
2018	O
)	O
,	O
styles	O
emanating	O
from	O
social	O
construct	O
like	O
formality	O
(	O
Rao	O
and	O
Tetreault	O
,	O
2018	O
)	O
and	O
politeness	O
(	O
Madaan	O
et	O
al	O
,	O
2020	O
)	O
,	O
linguistic	O
styles	O
based	O
on	O
author	O
writing	O
style	O
(	O
Syed	O
et	O
al	O
,	O
2020	O
)	O
,	O
or	O
psycho	O
-	O
linguistic	O
styles	O
based	O
on	O
personality	O
types	O
(	O
Mairesse	O
and	O
Walker	O
,	O
2011	O
)	O
.	O
While	O
early	O
style	B-TaskName
transfer	I-TaskName
frameworks	O
were	O
modeled	O
as	O
a	O
supervised	O
learning	O
task	O
on	O
a	O
parallel	O
corpus	O
,	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
are	O
semi	O
-	O
supervised	O
/	O
unsupervised	O
and	O
operate	O
on	O
nonparallel	O
corpus	O
.	O
These	O
models	O
achieve	O
style	B-TaskName
transfer	I-TaskName
by	O
aligning	O
source	O
and	O
target	O
distribution	O
of	O
sentences	O
from	O
non	O
-	O
parallel	O
corpus	O
(	O
Shen	O
et	O
al	O
,	O
2017	O
)	O
,	O
disentangling	O
content	O
space	O
from	O
style	O
space	O
in	O
latent	O
representation	O
(	O
Hu	O
et	O
al	O
,	O
2017	O
)	O
or	O
employing	O
self	O
-	O
reconstruction	O
(	O
Dai	O
et	O
al	O
,	O
2019	O
)	O
and	O
back	O
translation	O
(	O
Lample	O
et	O
al	O
,	O
2018	O
)	O
objectives	O
to	O
achieve	O
pseudo	O
-	O
supervision	O
with	O
non	O
-	O
parallel	O
corpus	O
.	O
Recent	O
works	O
have	O
also	O
modeled	O
this	O
in	O
a	O
self	O
-	O
supervised	O
manner	O
where	O
rewriting	O
(	O
transfer	O
)	O
is	O
achieved	O
by	O
utilizing	O
corpus	O
from	O
the	O
target	O
style	O
alone	O
(	O
Syed	O
et	O
al	O
,	O
2020	O
)	O
.	O
These	O
wide	O
studies	O
have	O
also	O
led	O
to	O
the	O
curation	O
and	O
benchmarking	O
of	O
non	O
-	O
parallel	O
dataset	O
for	O
various	O
style	O
dimensions	O
,	O
such	O
as	O
sentiment	O
(	O
Li	O
et	O
al	O
,	O
2018	O
)	O
,	O
formality	O
(	O
Rao	O
and	O
Tetreault	O
,	O
2018	O
)	O
,	O
politeness	O
(	O
Danescu	O
-	O
Niculescu	O
-	O
Mizil	O
et	O
al	O
,	O
2013	O
)	O
,	O
excitement	O
(	O
Sancheti	O
et	O
al	O
,	O
2020	O
)	O
,	O
etc	O
.	O
But	O
availability	O
of	O
data	O
with	O
joint	O
tagging	O
across	O
multiple	O
styles	O
is	O
limited	O
and	O
has	O
restricted	O
the	O
ability	O
of	O
existing	O
approaches	O
to	O
scale	O
from	O
single	O
-	O
dimensional	O
transfer	O
to	O
multiple	O
style	O
dimensions	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
multidimensional	O
style	B-TaskName
transfer	I-TaskName
approach	O
that	O
can	O
work	O
off	O
partially	O
labelled	O
data	O
for	O
style	B-TaskName
transfer	I-TaskName
across	O
multiple	O
dimensions	O
simultaneously	O
.	O
The	O
work	O
by	O
Subramanian	O
et	O
al	O
(	O
2018	O
)	O
attempts	O
style	B-TaskName
transfer	I-TaskName
with	O
multiple	O
attributes	O
such	O
as	O
age	O
,	O
gender	O
,	O
and	O
sentiment	O
simultaneously	O
.	O
However	O
,	O
their	O
approach	O
avails	O
corpus	O
tagged	O
with	O
each	O
of	O
these	O
three	O
style	O
dimensions	O
.	O
In	O
contrast	O
to	O
this	O
and	O
other	O
similar	O
explorations	O
in	O
multi	O
-	O
style	B-TaskName
transfer	I-TaskName
,	O
our	O
approach	O
does	O
not	O
require	O
jointly	O
labelled	O
data	O
across	O
all	O
the	O
stylistic	O
dimensions	O
in	O
source	O
and/or	O
target	O
corpus	O
.	O
We	O
focus	O
on	O
the	O
problem	O
where	O
independent	O
corpus	O
is	O
available	O
across	O
different	O
stylistic	O
dimensions	O
(	O
say	O
sentiment	O
and	O
formality	O
)	O
and	O
we	O
achieve	O
style	B-TaskName
transfer	I-TaskName
spanning	O
different	O
stylistic	O
dimensions	O
(	O
say	O
make	O
a	O
sentence	O
more	O
positive	O
and	O
formal	O
)	O
.	O
While	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
approaches	O
can	O
be	O
extended	O
to	O
achieve	O
this	O
by	O
sequentially	O
transferring	O
one	O
style	O
after	O
another	O
,	O
it	O
is	O
limited	O
as	O
different	O
style	O
dimensions	O
are	O
not	O
necessarily	O
independent	O
of	O
each	O
other	O
.	O
In	O
aspects	O
that	O
are	O
not	O
independent	O
,	O
changing	O
one	O
style	O
aspect	O
of	O
the	O
text	O
might	O
affect	O
another	O
aspect	O
considered	O
,	O
making	O
a	O
sequential	O
brute	O
-	O
force	O
approach	O
non	O
-	O
ideal	O
.	O
As	O
we	O
show	O
in	O
our	O
experiments	O
later	O
,	O
the	O
cascaded	O
setup	O
also	O
lacks	O
common	O
grounding	O
between	O
the	O
content	O
from	O
different	O
styles	O
leading	O
to	O
erratic	O
changes	O
in	O
content	O
.	O
We	O
circumvent	O
this	O
by	O
grounding	O
our	O
framework	O
on	O
the	O
linguistic	O
understanding	O
of	O
a	O
large	O
language	O
model	O
.	O
Our	O
model	O
builds	O
understanding	O
of	O
interplay	O
between	O
the	O
different	O
styles	O
by	O
incorporating	O
multiple	O
discriminative	O
language	O
models	O
(	O
LM	O
)	O
with	O
language	O
model	O
-	O
based	O
encoder	O
-	O
decoder	O
setup	O
.	O
The	O
key	O
contributions	O
of	O
this	O
paper	O
are	O
:	O
1	O
)	O
An	O
encoder	O
-	O
decoder	O
setup	O
with	O
multiple	O
language	O
models	O
as	O
discriminator	O
,	O
with	O
each	O
entity	O
harnessing	O
the	O
language	O
understanding	O
from	O
a	O
large	O
pre	O
-	O
trained	O
transformer	O
model	O
.	O
2	O
)	O
Relaxing	O
the	O
requirement	O
of	O
jointly	O
labelled	O
data	O
for	O
multi	O
-	O
style	B-TaskName
transfer	I-TaskName
,	O
by	O
leveraging	O
independently	O
acquired	O
disjoint	O
corpus	O
for	O
different	O
styles	O
.	O
3	O
)	O
Achieving	O
better	O
style	O
control	O
with	O
better	O
content	O
preservation	O
in	O
multi	O
-	O
dimensional	O
style	B-TaskName
transfer	I-TaskName
than	O
a	O
cascaded	O
setup	O
of	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
unidimensional	O
style	B-TaskName
transfer	I-TaskName
models	O
.	O

An	O
intelligent	O
,	O
rewarding	O
film	O
that	O
I	O
look	O
forward	O
to	O
watching	O
again	O
.	O
ludicrous	O
,	O
shallow	O
film	O
that	O
look	O
forward	O
to	O
watching	O
again	O
.	O
An	O
unintelligent	O
,	O
poor	O
film	O
that	O
I	O
would	O
not	O
look	O
forward	O
to	O
watching	O
again	O
.	O
super	O
friendly	O
staff	O
,	O
quick	O
service	O
and	O
amazing	O
and	O
simple	O
food	O
was	O
done	O
right	O
!	O
says	O
wait	O
staff	O
,	O
quick	O
not	O
amazing	O
before	O
overcooked	O
food	O
done	O
were	O
okay	O
.	O
dirty	O
staff	O
and	O
slow	O
service	O
and	O
simple	O
food	O
was	O
not	O
done	O
right	O
.	O
Positive+Informal	O
You	O
need	O
to	O
separate	O
the	O
bad	O
thing	O
and	O
move	O
on	O
.	O
need	O
to	O
the	O
great	O
thing	O
and	O
move	O
on	O
.	O
You	O
need	O
to	O
enjoy	O
the	O
good	O
stuff	O
and	O
move	O
on	O
.	O
The	O
evening	O
started	O
out	O
slow	O
.	O
The	O
evening	O
spent	O
in	O
professional	O
show	O
.	O
The	O
evening	O
began	O
amazing	O
.	O
Negative+Informal	O
Great	O
food	O
recommendations	O
steak	O
and	O
tuna	O
were	O
both	O
great	O
.	O
terrible	O
food	O
9	O
am	O
steak	O
and	O
were	O
both	O
terrible	O
.	O
Disappointing	O
food	O
recommendations	O
steak	O
and	O
tuna	O
were	O
horrible	O
.	O
That	O
person	O
in	O
hilarious	O
.	O
You	O
person	O
in	O
worse	O
!	O
That	O
guy	O
in	O
so	O
boring	O
.	O
(	O
Syed	O
et	O
al	O
(	O
2020	O
)	O
and	O
our	O
proposed	O
approach	O
)	O
,	O
we	O
note	O
that	O
content	O
preservation	O
is	O
marginally	O
better	O
for	O
Syed	O
et	O
al	O
(	O
2020	O
)	O
's	O
model	O
,	O
however	O
,	O
our	O
model	O
is	O
able	O
to	O
yield	O
much	O
better	O
style	B-TaskName
transfer	I-TaskName
owing	O
to	O
feedback	O
on	O
style	O
control	O
by	O
multiple	O
discriminators	O
.	O

We	O
propose	O
an	O
approach	O
to	O
extend	O
currently	O
existing	O
style	B-TaskName
transfer	I-TaskName
work	O
to	O
multiple	O
style	O
setting	O
without	O
imposing	O
any	O
extra	O
constraints	O
on	O
availability	O
of	O
dataset	O
.	O
Our	O
method	O
makes	O
use	O
of	O
disjoint	O
corpus	O
from	O
separate	O
styles	O
to	O
enable	O
one	O
step	O
transfer	O
across	O
multiple	O
target	O
styles	O
.	O
We	O
exploit	O
multiple	O
discriminative	O
language	O
models	O
with	O
an	O
encoder	O
-	O
decoder	O
framework	O
,	O
all	O
emerging	O
from	O
large	O
transformer	O
-	O
based	O
language	O
models	O
pretrained	O
on	O
Masked	B-TaskName
Language	I-TaskName
Modeling	I-TaskName
objective	O
and	O
fine	O
-	O
tuned	O
separately	O
for	O
transfer	O
and	O
discriminative	O
purposes	O
.	O
We	O
show	O
that	O
unified	O
single	O
step	O
transfer	O
approach	O
is	O
able	O
to	O
achieve	O
better	O
transfer	O
while	O
offering	O
much	O
better	O
content	O
preservation	O
which	O
is	O
paramount	O
to	O
any	O
style	B-TaskName
transfer	I-TaskName
task	O
.	O
Further	O
improvements	O
are	O
in	O
scope	O
for	O
adding	O
modularity	O
to	O
the	O
proposed	O
transfer	O
module	O
.	O
In	O
the	O
current	O
setup	O
,	O
each	O
version	O
of	O
model	O
is	O
trained	O
for	O
a	O
specific	O
combination	O
of	O
target	O
style	O
(	O
s	O
)	O
.	O
The	O
utility	O
of	O
such	O
a	O
model	O
increases	O
manifold	O
with	O
added	O
ease	O
of	O
transfer	O
across	O
multiple	O
style	O
combinations	O
within	O
a	O
single	O
model	O
.	O
This	O
could	O
be	O
attempted	O
by	O
employing	O
a	O
controlled	O
language	O
model	O
as	O
a	O
unified	O
discriminator	O
for	O
multiple	O
styles	O
,	O
which	O
would	O
be	O
the	O
subject	O
of	O
further	O
research	O
.	O
Ethics	B-DatasetName
Statement	O
.	O
We	O
recognise	O
the	O
ethical	O
implication	O
of	O
employing	O
large	O
language	O
models	O
trained	O
on	O
data	O
infused	O
with	O
unchecked	O
biases	O
.	O
As	O
with	O
any	O
generative	O
task	O
,	O
style	B-TaskName
transfer	I-TaskName
too	O
suffers	O
from	O
the	O
potential	O
misuse	O
for	O
fact	O
distortion	O
,	O
plagiarism	O
and	O
more	O
.	O
The	O
paper	O
aims	O
at	O
establishing	O
academic	O
utility	O
of	O
proposed	O
framework	O
.	O
To	O
meet	O
ethical	O
standards	O
,	O
this	O
solution	O
has	O
to	O
coupled	O
with	O
strict	O
misrepresentation	O
,	O
offensiveness	O
and	O
bias	O
checks	O
.	O

Improving	O
Numerical	O
Reasoning	O
Skills	O
in	O
the	O
Modular	O
Approach	O
for	O
Complex	O
Question	B-TaskName
Answering	I-TaskName
on	O
Text	O

Complex	O
Question	B-TaskName
Answering	I-TaskName
focuses	O
on	O
questions	O
that	O
require	O
capabilities	O
beyond	O
multi	O
-	O
hop	O
reasoning	O
.	O
These	O
capabilities	O
include	O
numerical	O
,	O
logical	O
and	O
discrete	O
reasoning	O
.	O
A	O
number	O
of	O
neural	O
models	O
were	O
recently	O
proposed	O
to	O
address	O
the	O
CQA	O
task	O
,	O
such	O
as	O
BiDAF	O
(	O
Seo	O
et	O
al	O
,	O
2017	O
)	O
,	O
QANet	O
(	O
Yu	O
et	O
al	O
,	O
2018	O
)	O
,	O
NMNs	O
(	O
Gupta	O
et	O
al	O
,	O
2020	O
)	O
and	O
NumNet	O
(	O
Ran	O
et	O
al	O
,	O
2019	O
)	O
,	O
which	O
achieved	O
high	O
performance	O
on	O
benchmark	O
datasets	O
such	O
as	O
DROP	B-DatasetName
(	O
Dua	O
et	O
al	O
,	O
2019	O
)	O
.	O
Numerical	O
Reasoning	O
is	O
an	O
essential	O
capability	O
for	O
the	O
CQA	O
task	O
,	O
which	O
is	O
a	O
challenging	O
problem	O
since	O
the	O
numbers	O
and	O
computation	O
procedures	O
are	O
separately	O
extracted	O
and	O
generated	O
from	O
raw	O
text	O
.	O
Dua	O
et	O
al	O
(	O
2019	O
)	O
modified	O
the	O
output	O
layer	O
of	O
QANet	O
(	O
Yu	O
et	O
al	O
,	O
2018	O
)	O
and	O
proposed	O
a	O
number	O
-	O
aware	O
model	O
NAQANet	O
that	O
can	O
deal	O
with	O
numerical	O
questions	O
for	O
which	O
the	O
answer	O
can	O
not	O
be	O
directly	O
extracted	O
from	O
the	O
paragraph	O
.	O
In	O
addition	O
to	O
NAQANet	O
,	O
NumNet	O
(	O
Ran	O
et	O
al	O
,	O
2019	O
)	O
leveraged	O
Graph	O
Neural	O
Network	O
(	O
GNN	O
)	O
to	O
design	O
a	O
number	O
-	O
aware	O
deep	O
learning	O
model	O
.	O
Also	O
leveraging	O
GNN	O
,	O
Chen	O
et	O
al	O
(	O
2020a	O
)	O
distinguished	O
number	O
types	O
more	O
precisely	O
by	O
adding	O
the	O
connection	O
with	O
entities	O
and	O
obtained	O
better	O
performance	O
.	O
Chen	O
et	O
al	O
(	O
2020b	O
)	O
searched	O
possible	O
programs	O
exhaustively	O
based	O
on	O
answer	O
numbers	O
and	O
employed	O
these	O
programs	O
as	O
weak	O
supervision	O
to	O
train	O
the	O
whole	O
model	O
.	O
Using	O
dependency	B-TaskName
parsing	I-TaskName
of	O
questions	O
,	O
Saha	O
et	O
al	O
(	O
2021	O
)	O
focused	O
on	O
the	O
numerical	O
part	O
and	O
obtained	O
excellent	O
results	O
on	O
different	O
kinds	O
of	O
numerical	O
reasoning	O
questions	O
.	O
Neural	O
Module	O
Networks	O
(	O
NMNs	O
)	O
(	O
Gupta	O
et	O
al	O
,	O
2020	O
)	O
adopts	O
the	O
programmer	O
-	O
interpreter	O
paradigm	O
and	O
is	O
a	O
fully	O
end	O
-	O
to	O
-	O
end	O
differentiable	O
model	O
,	O
in	O
which	O
the	O
programmer	O
(	O
responsible	O
for	O
composing	O
programs	O
)	O
and	O
the	O
interpreter	O
(	O
responsible	O
for	O
soft	O
execution	O
)	O
are	O
jointly	O
learned	O
.	O
Specialised	O
modules	O
,	O
such	O
as	O
find	O
and	O
find	O
-	O
num	O
,	O
are	O
predefined	O
to	O
perform	O
different	O
types	O
of	O
reasoning	O
over	O
text	O
and	O
numbers	O
.	O
Compared	O
with	O
those	O
techniques	O
that	O
employ	O
GNNs	O
(	O
Ran	O
et	O
al	O
,	O
2019	O
;	O
Yu	O
et	O
al	O
,	O
2018	O
)	O
,	O
NMNs	O
is	O
highly	O
interpretable	O
while	O
achieving	O
competitive	O
performance	O
.	O
More	O
details	O
can	O
be	O
found	O
in	O
Appendix	O
A.	O

It	O
is	O
highly	O
likely	O
for	O
a	O
paragraph	O
to	O
contain	O
multiple	O
numbers	O
and	O
entities	O
,	O
as	O
shown	O
in	O
Figure	O
1	O
.	O
For	O
such	O
paragraphs	O
,	O
the	O
original	O
NMNs	O
allows	O
all	O
numbers	O
to	O
interact	O
with	O
all	O
entities	O
in	O
the	O
computation	O
of	O
number	O
-	O
related	O
modules	O
such	O
as	O
"	O
find	O
-	O
num	O
"	O
.	O
This	O
is	O
detrimental	O
to	O
performance	O
as	O
,	O
intuitively	O
,	O
a	O
number	O
far	O
away	O
from	O
an	O
entity	O
is	O
less	O
likely	O
to	O
be	O
related	O
to	O
the	O
entity	O
.	O
As	O
the	O
second	O
example	O
in	O
Figure	O
1	O
shows	O
,	O
NMNs	O
connects	O
"	O
December	O
1997	O
"	O
to	O
the	O
entity	O
"	O
PUK	O
and	O
KDP	O
"	O
since	O
"	O
2003	O
"	O
is	O
far	O
away	O
from	O
it	O
,	O
resulting	O
in	O
wrong	O
predictions	O
eventually	O
.	O
To	O
tackle	O
this	O
issue	O
,	O
we	O
add	O
another	O
computational	O
component	O
,	O
the	O
relation	O
matrix	O
U	O
n	O
,	O
into	O
numberrelated	O
modules	O
.	O
Taking	O
the	O
"	O
find	O
-	O
num	O
"	O
module	O
as	O
an	O
example	O
,	O
the	O
following	O
step	O
is	O
added	O
before	O
Equation	O
2	O
when	O
computing	O
S	O
n	O
ij	O
:	O
S	O
n	O
ij	O
=	O
U	O
n	O
ij	O
S	O
n	O
ij	O
,	O
(	O
8	O
)	O
where	O
is	O
element	O
-	O
wise	O
multiplication	O
.	O
In	O
the	O
above	O
equation	O
,	O
the	O
value	O
of	O
S	O
n	O
ij	O
is	O
updated	O
with	O
the	O
relation	O
matrix	O
U	O
n	O
,	O
which	O
constrains	O
the	O
relationship	O
between	O
the	O
i	O
th	O
paragraph	O
token	O
and	O
j	O
th	O
number	O
token	O
.	O
More	O
specifically	O
,	O
let	O
s	O
t	O
be	O
the	O
token	O
index	O
set	O
for	O
the	O
t	O
th	O
sentence	O
in	O
the	O
paragraph	O
.	O
Thus	O
,	O
if	O
both	O
the	O
i	O
th	O
paragraph	O
token	O
and	O
the	O
j	O
th	O
number	O
token	O
belong	O
to	O
the	O
same	O
sentence	O
,	O
element	O
U	O
n	O
ij	O
,	O
in	O
row	O
i	O
and	O
column	O
j	O
,	O
is	O
set	O
to	O
1	O
,	O
otherwise	O
0	B-DatasetName
:	O
U	O
n	O
ij	O
=	O
1	O
,	O
(	O
i	O
s	O
t	O
)	O
(	O
n	O
j	O
s	O
t	O
)	O
0	B-DatasetName
,	O
otherwise	O
(	O
9	O
)	O
By	O
adding	O
this	O
matrix	O
,	O
the	O
module	O
only	O
keeps	O
the	O
attention	O
values	O
of	O
tokens	O
in	O
close	O
vicinity	O
within	O
a	O
sentence	O
,	O
and	O
learns	O
to	O
find	O
the	O
related	O
numbers	O
that	O
directly	O
interact	O
with	O
entities	O
.	O
Similarly	O
,	O
this	O
relation	O
matrix	O
U	O
n	O
is	O
also	O
applied	O
to	O
other	O
number	O
-	O
related	O
modules	O
to	O
improve	O
performance	O
.	O

In	O
order	O
to	O
solve	O
the	O
complex	O
question	B-TaskName
answering	I-TaskName
problem	O
,	O
Gupta	O
et	O
al	O
(	O
2020	O
)	O
proposed	O
a	O
Neural	O
Module	O
Networks	O
(	O
NMNs	O
)	O
model	O
.	O
Consisting	O
of	O
a	O
programmer	O
and	O
an	O
interpreter	O
,	O
NMNs	O
can	O
be	O
more	O
interpretable	O
as	O
shown	O
in	O
Figure	O
2	O
.	O
As	O
Figure	O
2	O
shows	O
,	O
NMNs	O
takes	O
the	O
question	O
and	O
the	O
paragraph	O
as	O
inputs	O
.	O
The	O
programmer	O
firstly	O
maps	O
the	O
question	O
into	O
corresponding	O
"	O
discrete	O
"	O
modules	O
in	O
order	O
.	O
Then	O
,	O
the	O
interpreter	O
executes	O
these	O
generated	O
modules	O
against	O
the	O
corresponding	O
paragraph	O
to	O
produce	O
the	O
final	O
answer	O
.	O
Moreover	O
,	O
all	O
modules	O
are	O
differentiable	O
so	O
that	O
the	O
whole	O
NMNs	O
can	O
be	O
trained	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
way	O
.	O
For	O
hyper	O
-	O
parameters	O
in	O
our	O
model	O
,	O
we	O
do	O
n't	O
conduct	O
experiments	O
on	O
their	O
search	O
trials	O
since	O
we	O
employ	O
the	O
same	O
settings	O
as	O
Gupta	O
et	O
al	O
(	O
2020	O
)	O
did	O
,	O
which	O
can	O
be	O
found	O
in	O
Table	O
3	O
.	O
Note	O
that	O
they	O
are	O
also	O
the	O
configuration	O
to	O
obtain	O
the	O
best	O
performance	O
.	O
For	O
the	O
added	O
parameter	O
λ	O
in	O
Equation	O
7	O
,	O
we	O
leverage	O
an	O
empirical	O
value	O
λ=0.5	O
without	O
any	O
fine	O
-	O
tuning	O
.	O

This	O
research	O
was	O
supported	O
in	O
part	O
by	O
the	O
Future	O
Fellowship	O
FT190100039	O
from	O
the	O
Australian	O
Research	O
Council	O
.	O
The	O
computational	O
resources	O
for	O
this	O
work	O
were	O
provided	O
by	O
the	O
Multi	O
-	O
modal	O
Australian	O
Sci	O
-	O
enceS	O
Imaging	O
and	O
Visualisation	O
Environment	O
(	O
MAS	B-MethodName
-	O
SIVE	O
)	O
(	O
www.massive.org.au	O
)	O
.	O
We	O
would	O
like	O
to	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
useful	O
comments	O
to	O
improve	O
the	O
manuscript	O
.	O

More	O
Identifiable	O
yet	O
Equally	O
Performant	O
Transformers	O
for	O
Text	B-TaskName
Classification	I-TaskName

Interpretability	O
is	O
an	O
important	O
aspect	O
of	O
the	O
trustworthiness	O
of	O
a	O
model	O
's	O
predictions	O
.	O
Transformer	B-MethodName
's	O
predictions	O
are	O
widely	O
explained	O
by	O
the	O
attention	O
weights	O
,	O
i.e.	O
,	O
a	O
probability	O
distribution	O
generated	O
at	O
its	O
self	O
-	O
attention	O
unit	O
(	O
head	O
)	O
.	O
Current	O
empirical	O
studies	O
provide	O
shreds	O
of	O
evidence	O
that	O
attention	O
weights	O
are	O
not	O
explanations	O
by	O
proving	O
that	O
they	O
are	O
not	O
unique	O
.	O
A	O
recent	O
study	O
showed	O
theoretical	O
justifications	O
to	O
this	O
observation	O
by	O
proving	O
the	O
non	O
-	O
identifiability	O
of	O
attention	O
weights	O
.	O
For	O
a	O
given	O
input	O
to	O
a	O
head	O
and	O
its	O
output	O
,	O
if	O
the	O
attention	O
weights	O
generated	O
in	O
it	O
are	O
unique	O
,	O
we	O
call	O
the	O
weights	O
identifiable	O
.	O
In	O
this	O
work	O
,	O
we	O
provide	O
deeper	O
theoretical	O
analysis	O
and	O
empirical	O
observations	O
on	O
the	O
identifiability	O
of	O
attention	O
weights	O
.	O
Ignored	O
in	O
the	O
previous	O
works	O
,	O
we	O
find	O
the	O
attention	O
weights	O
are	O
more	O
identifiable	O
than	O
we	O
currently	O
perceive	O
by	O
uncovering	O
the	O
hidden	O
role	O
of	O
the	O
key	O
vector	O
.	O
However	O
,	O
the	O
weights	O
are	O
still	O
prone	O
to	O
be	O
non	O
-	O
unique	O
attentions	O
that	O
make	O
them	O
unfit	O
for	O
interpretation	O
.	O
To	O
tackle	O
this	O
issue	O
,	O
we	O
provide	O
a	O
variant	O
of	O
the	O
encoder	O
layer	O
that	O
decouples	O
the	O
relationship	O
between	O
key	O
and	O
value	O
vector	O
and	O
provides	O
identifiable	O
weights	O
up	O
to	O
the	O
desired	O
length	O
of	O
the	O
input	O
.	O
We	O
prove	O
the	O
applicability	O
of	O
such	O
variations	O
by	O
providing	O
empirical	O
justifications	O
on	O
varied	O
text	B-TaskName
classification	I-TaskName
tasks	O
.	O
The	O
implementations	O
are	O
available	O
at	O
https://github.com/declare	O
-	O
lab/	O
identifiable	O
-	O
transformers	O
.	O

The	O
output	O
of	O
an	O
attention	O
head	O
H	O
is	O
the	O
product	O
of	O
A	O
and	O
T	O
(	O
eq	O
.	O
(	O
2	O
)	O
)	O
.	O
Formally	O
,	O
we	O
define	O
identifiability	O
of	O
attention	O
in	O
a	O
head	O
:	O
Definition	O
3.1	O
.	O
For	O
an	O
attention	O
head	O
's	O
output	O
H	O
,	O
attention	O
weights	O
A	O
are	O
identifiable	O
if	O
there	O
exists	O
a	O
unique	O
solution	O
of	O
A	O
T	O
=	O
H.	O
The	O
above	O
definition	O
can	O
be	O
reformulated	O
as	O
Definition	O
3.2	O
.	O
A	O
is	O
unidentifiable	O
if	O
there	O
exist	O
añ	O
A	O
,	O
(	O
Ã	O
=	O
0	B-DatasetName
)	O
,	O
such	O
that	O
(	O
A	O
+	O
Ã	O
)	O
is	O
obtainable	O
from	O
phase	O
-	O
1	O
of	O
head	O
computations	O
and	O
satisfy	O
(	O
A	O
+	O
Ã	O
)	O
T	O
=	O
A	O
T	O
=	O
⇒Ã	O
T	O
=	O
0	B-DatasetName
.	O
(	O
constraint	O
-	O
R1	O
)	O
Under	O
this	O
constraint	O
,	O
we	O
getã	O
i	O
T	O
=	O
0	B-DatasetName
whereã	O
i	O
is	O
the	O
i	O
th	O
row	O
ofÃ.	O
The	O
set	O
of	O
vectors	O
which	O
when	O
multiplied	O
to	O
T	O
gets	O
mapped	O
to	O
zero	O
describes	O
the	O
left	O
null	O
space	O
of	O
T	O
denoted	O
by	O
LN	O
(	O
T	O
)	O
.	O
The	O
dimension	O
of	O
the	O
left	O
null	O
space	O
of	O
T	O
can	O
be	O
obtained	O
by	O
taking	O
the	O
difference	O
of	O
the	O
total	O
number	O
of	O
rows	O
(	O
d	O
s	O
)	O
and	O
the	O
number	O
of	O
linearly	O
independent	O
rows	O
,	O
i.e	O
,	O
rank	O
of	O
the	O
matrix	O
T	O
denoted	O
by	O
rank	O
(	O
T	O
)	O
.	O
Let	O
dim	O
(	O
)	O
denotes	O
the	O
dimension	O
of	O
a	O
vector	O
space	O
,	O
then	O
LN	O
(	O
T	O
)	O
=	O
{	O
v	O
|	O
v	O
T	O
T	O
=	O
0	B-DatasetName
}	O
(	O
3	O
)	O
dim	O
LN	O
(	O
T	O
)	O
=	O
d	O
s	O
−	O
rank	O
(	O
T	O
)	O
.	O
(	O
4	O
)	O
3.1	O
"	O
A	O
"	O
is	O
Identifiable	O
for	O
d	O
s	O
≤	O
d	O
v	O
If	O
dim	O
(	O
LN	O
(	O
T	O
)	O
)	O
=	O
0	B-DatasetName
then	O
LN	O
(	O
T	O
)	O
=	O
{	O
0	B-DatasetName
}	O
,	O
it	O
leads	O
to	O
the	O
only	O
solution	O
of	O
constraint	O
-	O
R1	O
that	O
isÃ	O
=	O
0	B-DatasetName
.	O
Therefore	O
,	O
the	O
unidentifiabilty	O
condition	O
does	O
not	O
hold	O
.	O
Now	O
we	O
will	O
prove	O
such	O
a	O
situation	O
exists	O
when	O
the	O
number	O
of	O
tokens	O
is	O
not	O
more	O
than	O
the	O
size	O
of	O
value	O
vector	O
.	O
The	O
matrix	O
T	O
in	O
eq	O
.	O
(	O
2	O
)	O
is	O
product	O
of	O
d	O
s	O
×	O
d	O
v	O
value	O
matrix	O
V	O
and	O
d	O
v	O
×	O
d	O
e	O
transformation	O
D.	O
We	O
utilize	O
the	O
fact	O
that	O
the	O
rank	O
of	O
product	O
of	O
two	O
matrices	O
P	O
and	O
Q	O
is	O
upper	O
bounded	O
by	O
the	O
minimum	O
of	O
rank	O
(	O
P	O
)	O
and	O
rank	O
(	O
Q	O
)	O
,	O
i.e.	O
,	O
rank	O
(	O
P	O
Q	O
)	O
≤	O
min	O
rank	O
(	O
P	O
)	O
,	O
rank	O
(	O
Q	O
)	O
.	O
Thus	O
,	O
the	O
upper	O
bound	O
on	O
rank	O
(	O
T	O
)	O
in	O
eq	O
.	O
(	O
4	O
)	O
can	O
be	O
determined	O
by	O
rank	O
(	O
T	O
)	O
≤	O
min	O
rank	O
(	O
V	O
)	O
,	O
rank	O
(	O
D	O
)	O
≤	O
min	O
min	O
(	O
ds	O
,	O
dv	O
)	O
,	O
min	O
(	O
dv	O
,	O
de	O
)	O
≤	O
min	O
ds	O
,	O
dv	O
,	O
dv	O
,	O
de	O
≤	O
min	O
ds	O
,	O
dv	O
(	O
as	O
de	O
>	O
dv	O
)	O
=	O
min	O
ds	O
,	O
64	O
where	O
the	O
last	O
inequality	O
is	O
obtained	O
for	O
a	O
head	O
in	O
the	O
regular	O
Transformer	B-MethodName
for	O
which	O
d	O
v	O
=	O
64	O
.	O
Numerical	O
rank	O
.	O
To	O
substantiate	O
the	O
bounds	O
on	O
rank	O
(	O
T	O
)	O
as	O
derived	O
above	O
,	O
we	O
set	O
up	O
a	O
model	O
with	O
a	O
single	O
encoder	O
layer	O
(	O
6	O
)	O
.	O
The	O
model	O
is	O
trained	O
to	O
predict	O
the	O
sentiment	O
of	O
IMDB	B-DatasetName
reviews	O
(	O
5	O
)	O
.	O
We	O
feed	O
the	O
review	O
tokens	O
to	O
the	O
model	O
and	O
store	O
the	O
values	O
generated	O
in	O
T	O
of	O
the	O
first	O
head	O
.	O
A	O
standard	O
technique	O
for	O
calculating	O
the	O
rank	O
of	O
a	O
matrix	O
with	O
floating	O
-	O
point	O
values	O
and	O
computations	O
is	O
to	O
use	O
singular	O
value	O
decomposition	O
.	O
The	O
rank	O
of	O
the	O
matrix	O
will	O
be	O
computed	O
as	O
the	O
number	O
of	O
singular	O
values	O
larger	O
than	O
the	O
predefined	O
threshold	O
4	O
.	O
The	O
fig	O
.	O
2	O
illustrates	O
how	O
the	O
rank	O
changes	O
with	O
the	O
sequence	O
length	O
d	O
s	O
.	O
The	O
numerical	O
rank	O
provides	O
experimental	O
support	O
to	O
the	O
theoretical	O
analysis	O
.	O
rank	O
(	O
T	O
)	O
=	O
d	O
s	O
if	O
d	O
s	O
≤	O
d	O
v	O
,	O
d	O
v	O
if	O
d	O
s	O
>	O
d	O
v	O
.	O
(	O
6	O
)	O
Thus	O
,	O
dim	O
LN	O
(	O
T	O
)	O
=	O
d	O
s	O
−	O
rank	O
(	O
T	O
)	O
=	O
0	B-DatasetName
if	O
d	O
s	O
≤	O
d	O
v	O
,	O
(	O
d	O
s	O
−	O
d	O
v	O
)	O
if	O
d	O
s	O
>	O
d	O
v	O
.	O
=	O
max	O
(	O
d	O
s	O
−	O
d	O
v	O
,	O
0	B-DatasetName
)	O
(	O
7	O
)	O
With	O
this	O
,	O
we	O
infer	O
A	O
is	O
identifiable	O
if	O
d	O
s	O
≤	O
d	O
v	O
=	O
64	O
.	O
For	O
the	O
identifiability	O
study	O
,	O
since	O
we	O
focus	O
on	O
a	O
model	O
's	O
capability	O
of	O
learning	O
unique	O
attention	O
weights	O
,	O
we	O
will	O
assume	O
T	O
has	O
the	O
maximum	O
obtainable	O
rank	O
set	O
by	O
its	O
upper	O
bound	O
.	O

(	O
the	O
hidden	O
role	O
of	O
d	O
k	O
)	O
In	O
this	O
case	O
,	O
from	O
eq	O
.	O
(	O
7	O
)	O
,	O
we	O
obtain	O
a	O
non	O
zero	O
value	O
of	O
dim	O
LN	O
(	O
T	O
)	O
.	O
It	O
allows	O
us	O
to	O
find	O
infi	O
-	O
niteÃ	O
's	O
satisfying	O
(	O
A	O
+	O
Ã	O
)	O
T	O
=	O
A	O
T.	O
However	O
,	O
constraint	O
-	O
R1	O
demandsÃ	O
to	O
be	O
obtainable	O
from	O
the	O
first	O
phase	O
of	O
self	O
-	O
attention	O
.	O
As	O
a	O
first	O
step	O
,	O
we	O
focus	O
our	O
analysis	O
on	O
the	O
attention	O
matrix	O
without	O
applying	O
softmax	B-MethodName
non	O
-	O
linearity	O
,	O
i.e.	O
,	O
A	O
=	O
Q	O
K	O
T	O
√	O
dq	O
.	O
The	O
analysis	O
is	O
crucial	O
to	O
identify	O
constraints	O
coming	O
from	O
the	O
first	O
phase	O
of	O
self	O
-	O
attention	O
in	O
Transformer	B-MethodName
that	O
impact	O
identifiability	O
.	O
Insights	O
from	O
this	O
will	O
help	O
us	O
analyse	O
softmax	B-MethodName
version	O
of	O
A.	O

For	O
the	O
empirical	O
analysis	O
of	O
our	O
proposed	O
solutions	O
as	O
mentioned	O
in	O
4	O
,	O
we	O
conduct	O
our	O
experiments	O
on	O
the	O
following	O
varied	O
text	B-TaskName
classification	I-TaskName
tasks	O
:	O

IMDB	B-DatasetName
(	O
Maas	O
et	O
al	O
,	O
2011	O
)	O
.	O
The	O
dataset	O
for	O
the	O
task	O
of	O
sentiment	O
classification	O
consist	O
of	O
IMDB	B-DatasetName
movie	I-DatasetName
reviews	I-DatasetName
with	O
their	O
sentiment	O
as	O
positive	O
or	O
negative	O
.	O
Each	O
of	O
the	O
train	O
and	O
test	O
sets	O
contain	O
25	O
,	O
000	O
data	O
samples	O
equally	O
distributed	O
in	O
both	O
the	O
sentiment	O
polarities	O
.	O
TREC	B-DatasetName
(	O
Voorhees	O
and	O
Tice	O
,	O
2000	O
)	O
.	O
We	O
use	O
the	O
6	O
-	O
class	O
version	O
of	O
the	O
dataset	O
for	O
the	O
task	O
of	O
question	O
classification	O
consisting	O
of	O
open	O
-	O
domain	O
,	O
facet	O
-	O
based	O
questions	O
.	O
There	O
are	O
5	O
,	O
452	O
and	O
500	O
samples	O
for	O
training	O
and	O
testing	O
,	O
respectively	O
.	O
SST	B-DatasetName
(	O
Socher	O
et	O
al	O
,	O
2013	O
)	O
.	O
Stanford	O
sentiment	B-TaskName
analysis	I-TaskName
dataset	O
consist	O
of	O
11	O
,	O
855	O
sentences	O
obtained	O
from	O
movie	O
reviews	O
.	O
We	O
use	O
the	O
3	O
-	O
class	O
version	O
of	O
the	O
dataset	O
for	O
the	O
task	O
of	O
sentiment	O
classification	O
.	O
Each	O
review	O
is	O
labeled	O
as	O
positive	O
,	O
neutral	O
,	O
or	O
negative	O
.	O
The	O
provided	O
train	O
/	O
test	O
/	O
valid	O
split	O
is	O
8	O
,	O
544/2	O
,	O
210/1	O
,	O
101	O
.	O
8	O
ds	O
-	O
max	O
<	O
de	O
as	O
in	O
the	O
regular	O
Transformer	B-MethodName
setting	O
.	O

SNLI	B-DatasetName
(	O
Bowman	O
et	O
al	O
,	O
2015	O
)	O
.	O
The	O
dataset	O
contain	O
549	O
,	O
367	O
samples	O
in	O
the	O
training	O
set	O
,	O
9	O
,	O
842	O
samples	O
in	O
the	O
validation	O
set	O
,	O
and	O
9	O
,	O
824	O
samples	O
in	O
the	O
test	O
set	O
.	O
For	O
the	O
task	O
of	O
recognizing	O
textual	O
entailment	O
,	O
each	O
sample	O
consists	O
of	O
a	O
premisehypothesis	O
sentence	O
pair	O
and	O
a	O
label	O
indicating	O
whether	O
the	O
hypothesis	O
entails	O
the	O
premise	O
,	O
contradicts	O
it	O
,	O
or	O
neutral	O
.	O
Please	O
refer	O
to	O
Zhang	O
et	O
al	O
(	O
2015	O
)	O
for	O
more	O
details	O
about	O
the	O
following	O
datasets	O
:	O
Yelp	O
.	O
We	O
use	O
the	O
large	O
-	O
scale	O
Yelp	O
review	O
dataset	O
for	O
the	O
task	O
of	O
binary	O
sentiment	O
classification	O
.	O
There	O
are	O
560	O
,	O
000	O
samples	O
for	O
training	O
and	O
38	O
,	O
000	O
samples	O
for	O
testing	O
,	O
equally	O
split	O
into	O
positive	O
and	O
negative	O
polarities	O
.	O
DBPedia	B-DatasetName
.	O
The	O
Ontology	B-MethodName
dataset	O
for	O
topic	B-TaskName
classification	I-TaskName
consist	O
of	O
14	O
non	O
-	O
overlapping	O
classes	O
each	O
with	O
40	O
,	O
000	O
samples	O
for	O
training	O
and	O
5	O
,	O
000	O
samples	O
for	O
testing	O
.	O
Sogou	O
News	O
.	O
The	O
dataset	O
for	O
news	O
article	O
classification	O
consist	O
of	O
450	O
,	O
000	O
samples	O
for	O
training	O
and	O
60	O
,	O
000	O
for	O
testing	O
.	O
Each	O
article	O
is	O
labeled	O
in	O
one	O
of	O
the	O
5	O
news	O
categories	O
.	O
The	O
dataset	O
is	O
perfectly	O
balanced	O
.	O
AG	B-DatasetName
News	I-DatasetName
.	O
The	O
dataset	O
for	O
the	O
news	O
articles	O
classification	O
partitioned	O
into	O
four	O
categories	O
.	O
The	O
balanced	O
train	O
and	O
test	O
set	O
consist	O
of	O
120	O
,	O
000	O
and	O
7	O
,	O
600	O
samples	O
,	O
respectively	O
.	O
Yahoo	B-DatasetName
!	I-DatasetName
Answers	I-DatasetName
.	O
The	O
balanced	O
dataset	O
for	O
10class	O
topic	B-TaskName
classification	I-TaskName
contain	O
1	O
,	O
400	O
,	O
000	O
samples	O
for	O
training	O
and	O
50	O
,	O
000	O
samples	O
for	O
testing	O
.	O
Amazon	O
Reviews	O
.	O
For	O
the	O
task	O
of	O
sentiment	O
classification	O
,	O
the	O
dataset	O
contain	O
3	O
,	O
600	O
,	O
000	O
samples	O
for	O
training	O
and	O
400	O
,	O
000	O
samples	O
for	O
testing	O
.	O
The	O
samples	O
are	O
equally	O
divided	O
into	O
positive	O
and	O
negative	O
sentiment	O
labels	O
.	O
Except	O
for	O
the	O
SST	B-DatasetName
and	O
SNLI	B-DatasetName
,	O
where	O
the	O
validation	O
split	O
is	O
already	O
provided	O
,	O
we	O
flag	O
30	O
%	O
of	O
the	O
train	O
set	O
as	O
part	O
of	O
the	O
validation	O
set	O
and	O
the	O
rest	O
70	O
%	O
were	O
used	O
for	O
model	O
parameter	O
learning	O
.	O

This	O
work	O
probed	O
Transformer	B-MethodName
for	O
identifiability	O
of	O
self	O
-	O
attention	O
,	O
i.e.	O
,	O
the	O
attention	O
weights	O
can	O
be	O
uniquely	O
identified	O
from	O
the	O
head	O
's	O
output	O
.	O
With	O
theoretical	O
analysis	O
and	O
supporting	O
empirical	O
evidence	O
,	O
we	O
were	O
able	O
to	O
identify	O
the	O
limitations	O
of	O
the	O
existing	O
study	O
by	O
Brunner	O
et	O
al	O
(	O
2019	O
)	O
.	O
We	O
found	O
the	O
study	O
largely	O
ignored	O
the	O
constraint	O
coming	O
from	O
the	O
first	O
phase	O
of	O
self	O
-	O
attention	O
in	O
the	O
encoder	O
,	O
i.e.	O
,	O
the	O
size	O
of	O
the	O
key	O
vector	O
.	O
Later	O
,	O
we	O
proved	O
how	O
we	O
can	O
utilize	O
d	O
k	O
to	O
make	O
the	O
attention	O
weights	O
more	O
identifiable	O
.	O
To	O
give	O
a	O
more	O
concrete	O
solution	O
,	O
we	O
propose	O
encoder	O
variants	O
that	O
are	O
more	O
identifiable	O
,	O
theoretically	O
as	O
well	O
as	O
experimentally	O
,	O
for	O
a	O
large	O
range	O
of	O
input	O
sequence	O
lengths	O
.	O
The	O
identifiable	O
variants	O
do	O
not	O
show	O
any	O
performance	O
drop	O
when	O
experiments	O
are	O
done	O
on	O
varied	O
text	B-TaskName
classification	I-TaskName
tasks	O
.	O
Future	O
works	O
may	O
analyse	O
the	O
critical	O
impact	O
of	O
identifiability	O
on	O
the	O
explainability	O
and	O
interpretability	O
of	O
the	O
Transformer	B-MethodName
.	O

The	O
left	O
null	O
space	O
of	O
a	O
m	O
p	O
×	O
n	O
p	O
matrix	O
P	O
can	O
be	O
defined	O
as	O
the	O
set	O
of	O
vectors	O
v	O
-	O
LN	O
P	O
=	O
{	O
v	O
T	O
R	O
1×mp	O
|	O
v	O
T	O
P	O
=	O
0	B-DatasetName
}	O
(	O
10	O
)	O
If	O
the	O
rows	O
of	O
P	O
are	O
linearly	O
independent	O
(	O
P	O
is	O
full	O
-	O
row	O
rank	O
)	O
the	O
left	O
null	O
space	O
of	O
P	O
is	O
zero	O
dimensional	O
.	O
The	O
only	O
solution	O
to	O
the	O
system	O
of	O
equations	O
v	O
P	O
=	O
0	B-DatasetName
is	O
trivial	O
,	O
i.e.	O
,	O
v=0	O
.	O
The	O
dimensions	O
of	O
the	O
null	O
space	O
,	O
known	O
as	O
nullity	O
,	O
of	O
P	O
can	O
be	O
calculated	O
as	O
dim	O
LN	O
(	O
P	O
)	O
=	O
m	O
p	O
−	O
rank	O
(	O
P	O
)	O
.	O
The	O
nullity	O
of	O
P	O
sets	O
the	O
dimensions	O
of	O
the	O
space	O
v	O
lies	O
in	O
.	O
In	O
3	O
,	O
we	O
utilize	O
our	O
knowledge	O
of	O
appendix	O
A.2	O
and	O
appendix	O
A.3	O
to	O
analyse	O
identifiability	O
in	O
a	O
Transformer	B-MethodName
.	O

NLQuAD	O
:	O
A	O
Non	O
-	O
Factoid	O
Long	O
Question	B-TaskName
Answering	I-TaskName
Data	O
Set	O

SQuAD	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
is	O
a	O
factoid	O
span	O
detection	O
data	O
set	O
with	O
short	O
answers	O
.	O
Crowdworkers	O
generated	O
the	O
questions	O
given	O
a	O
set	O
of	O
articles	O
.	O
DROP	B-DatasetName
(	O
Dua	O
et	O
al	O
,	O
2019	O
)	O
makes	O
the	O
problem	O
more	O
challenging	O
by	O
adversarially	O
-	O
created	O
questions	O
requiring	O
discrete	O
reasoning	O
over	O
the	O
text	O
.	O
SQuAD	B-DatasetName
and	O
DROP	B-DatasetName
use	O
Wikipedia	O
pages	O
as	O
context	O
passages	O
whereas	O
SearchQA	B-DatasetName
(	O
Dunn	O
et	O
al	O
,	O
2017	O
)	O
uses	O
IR	O
approaches	O
to	O
collect	O
context	O
passages	O
.	O
Answer	B-TaskName
generation	I-TaskName
based	O
on	O
a	O
set	O
of	O
passages	O
is	O
another	O
approach	O
to	O
address	O
this	O
task	O
.	O
MS	B-DatasetName
MARCO	I-DatasetName
(	O
Bajaj	O
et	O
al	O
,	O
2016	O
)	O
consists	O
of	O
real	O
-	O
world	O
search	O
queries	O
and	O
retrieved	O
documents	O
corresponding	O
to	O
the	O
queries	O
.	O
There	O
are	O
also	O
different	O
types	O
of	O
QA	O
data	O
sets	O
such	O
as	O
Antique	O
(	O
Hashemi	O
et	O
al	O
,	O
2020	O
)	O
,	O
which	O
is	O
a	O
data	O
set	O
for	O
answer	O
retrieval	O
for	O
non	O
-	O
factoid	O
ques	O
-	O
tions	O
.	O
There	O
is	O
also	O
a	O
range	O
of	O
multiple	O
-	O
choice	O
QA	O
tasks	O
such	O
as	O
RACE	B-DatasetName
(	O
Lai	O
et	O
al	O
,	O
2017	O
)	O
,	O
ARC	B-DatasetName
(	O
Clark	O
et	O
al	O
,	O
2018	O
)	O
,	O
SWAQ	O
(	O
Zellers	O
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
COS	O
-	O
MOS	O
QA	O
(	O
Huang	O
et	O
al	O
,	O
2019	O
)	O
that	O
are	O
clustered	O
together	O
with	O
the	O
short	O
-	O
context	O
QA	O
data	O
sets	O
.	O

Factoid	O
QA	O
has	O
been	O
applied	O
to	O
longer	O
documents	O
,	O
however	O
,	O
the	O
nature	O
of	O
factoid	O
questions	O
limits	O
answers	O
to	O
short	O
texts	O
.	O
NewsQA	B-DatasetName
(	O
Trischler	O
et	O
al	O
,	O
2017	O
)	O
,	O
TriviaQA	B-DatasetName
(	O
Joshi	O
et	O
al	O
,	O
2017	O
)	O
,	O
NarrativeQA	B-DatasetName
(	O
Kočiský	O
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
DuoRC	B-DatasetName
(	O
Saha	O
et	O
al	O
,	O
2018	O
)	O
fall	O
into	O
this	O
category	O
and	O
their	O
documents	O
are	O
extracted	O
from	O
news	O
articles	O
,	O
stories	O
,	O
and	O
movie	O
plots	O
,	O
respectively	O
.	O
On	O
the	O
other	O
hand	O
,	O
DQA	O
(	O
ter	O
Hoeve	O
et	O
al	O
,	O
2020	O
)	O
is	O
a	O
document	O
-	O
centred	O
QA	O
data	O
set	O
aimed	O
at	O
document	O
assistance	O
systems	O
.	O
Along	O
with	O
Yes	O
/	O
No	O
questions	O
,	O
it	O
also	O
includes	O
non	O
-	O
factoid	O
questions	O
with	O
relatively	O
long	O
answers	O
.	O
However	O
,	O
the	O
questions	O
are	O
generated	O
by	O
crowd	O
-	O
workers	O
based	O
on	O
a	O
small	O
set	O
of	O
documents	O
.	O
DuReader	B-DatasetName
(	O
He	O
et	O
al	O
,	O
2018	O
)	O
consists	O
of	O
real	O
-	O
word	O
Chinese	O
queries	O
and	O
corresponding	O
retrieved	O
documents	O
.	O
It	O
contains	O
both	O
factoid	O
and	O
non	O
-	O
factoid	O
(	O
40	O
%	O
)	O
questions	O
and	O
consequently	O
has	O
longer	O
average	O
answer	O
length	O
than	O
pure	O
factoid	O
datasets	O
.	O
The	O
multi	O
-	O
hop	O
QA	O
task	O
,	O
requiring	O
multi	O
-	O
hop	O
reasoning	O
over	O
multiple	O
paragraphs	O
,	O
can	O
also	O
be	O
considered	O
as	O
long	O
-	O
context	O
QA	O
if	O
models	O
process	O
paragraphs	O
together	O
.	O
HotpotQA	B-DatasetName
(	O
Yang	O
et	O
al	O
,	O
2018	O
)	O
is	O
a	O
multi	O
-	O
hop	O
data	O
set	O
,	O
but	O
the	O
answer	O
length	O
of	O
its	O
factoid	O
questions	O
is	O
as	O
limited	O
as	O
that	O
of	O
short	O
-	O
context	O
QA	O
data	O
sets	O
.	O
Natural	B-DatasetName
Questions	I-DatasetName
(	O
Kwiatkowski	O
et	O
al	O
,	O
2019	O
)	O
is	O
a	O
factoid	O
QA	O
task	O
with	O
much	O
longer	O
documents	O
and	O
two	O
types	O
of	O
answer	O
lengths	O
.	O
answers	O
(	O
yes	O
/	O
no	O
,	O
entities	O
)	O
as	O
well	O
as	O
long	O
answers	O
(	O
bounding	O
boxes	O
with	O
the	O
information	O
to	O
infer	O
the	O
answer	O
)	O
.	O
However	O
,	O
due	O
to	O
the	O
nature	O
of	O
factoid	O
questions	O
,	O
the	O
majority	O
of	O
long	O
answers	O
are	O
sections	O
containing	O
exactly	O
the	O
short	O
answer	O
or	O
simple	O
facts	O
.	O
ELI5	B-DatasetName
(	O
Fan	O
et	O
al	O
,	O
2019	O
)	O
consists	O
of	O
real	O
-	O
world	O
questions	O
with	O
answers	O
provided	O
by	O
the	O
Reddit	B-DatasetName
community	O
.	O
The	O
task	O
is	O
to	O
generate	O
answers	O
given	O
a	O
set	O
of	O
documents	O
retrieved	O
from	O
the	O
Web	O
.	O
However	O
,	O
the	O
documents	O
are	O
not	O
guaranteed	O
to	O
completely	O
address	O
the	O
questions	O
.	O
Furthermore	O
,	O
evaluation	O
metrics	O
for	O
sequence	O
generation	O
tasks	O
such	O
as	O
the	O
ROUGE	O
score	O
(	O
Lin	O
and	O
Och	O
,	O
2004	O
)	O
are	O
far	O
from	O
perfect	O
to	O
assess	O
the	O
quality	O
of	O
generated	O
answers	O
.	O
Table	O
1	O
compares	O
existing	O
long	O
-	O
context	O
question	B-TaskName
answering	I-TaskName
data	O
sets	O
along	O
with	O
SQuAD	B-DatasetName
and	O
MS	B-DatasetName
MARCO	I-DatasetName
.	O
We	O
report	O
the	O
average	O
length	O
for	O
data	O
sets	O
with	O
different	O
types	O
of	O
answers	O
.	O

To	O
investigate	O
the	O
difficulty	O
level	O
of	O
NLQuAD	O
for	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
QA	O
systems	O
and	O
to	O
establish	O
baseline	O
results	O
,	O
we	O
evaluate	O
the	O
performance	O
of	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
,	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
Longformer	B-MethodName
(	O
Beltagy	O
et	O
al	O
,	O
2020	O
)	O
.	O
Longformer	B-MethodName
is	O
a	O
scalable	O
model	O
for	O
processing	O
long	O
documents	O
and	O
has	O
been	O
used	O
for	O
long	O
sequences	O
such	O
as	O
document	B-TaskName
classification	I-TaskName
(	O
Beltagy	O
et	O
al	O
,	O
2020	O
)	O
and	O
document	O
re	O
-	O
ranking	O
(	O
Sekulić	O
et	O
al	O
,	O
2020	O
)	O
.	O
We	O
refer	O
readers	O
to	O
Tay	O
et	O
al	O
(	O
2020b	O
)	O
for	O
a	O
detailed	O
survey	O
on	O
efficient	O
transformers	O
.	O
We	O
train	O
these	O
Transformerbased	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
models	O
to	O
predict	O
the	O
span	O
of	O
the	O
answer	O
in	O
a	O
context	O
document	O
given	O
a	O
question	O
and	O
document	O
.	O

The	O
BERT	B-MethodName
QA	O
model	O
concatenates	O
question	O
and	O
document	O
pairs	O
into	O
a	O
single	O
sequence	O
and	O
predicts	O
the	O
answer	O
span	O
by	O
a	O
dot	O
product	O
between	O
the	O
final	O
hidden	O
vectors	O
,	O
a	O
start	O
vector	O
and	O
an	O
end	O
vector	O
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
.	O
Due	O
to	O
the	O
memory	O
and	O
computational	O
requirements	O
,	O
BERT	B-MethodName
can	O
encode	O
sequences	O
with	O
a	O
maximum	O
length	O
of	O
512	O
tokens	O
that	O
is	O
less	O
than	O
the	O
average	O
sample	O
length	O
in	O
NLQuAD	O
.	O
Therefore	O
,	O
we	O
adopt	O
a	O
sliding	O
window	O
approach	O
.	O
We	O
split	O
the	O
samples	O
into	O
segments	O
using	O
a	O
sliding	O
window	O
of	O
512	O
tokens	O
and	O
a	O
stride	O
of	O
128	O
tokens	O
.	O
Each	O
segment	O
is	O
augmented	O
with	O
its	O
corresponding	O
question	O
.	O
The	O
segments	O
can	O
include	O
no	O
answer	O
,	O
a	O
portion	O
of	O
the	O
answer	O
,	O
or	O
the	O
entire	O
answer	O
.	O
We	O
train	O
BERT	B-MethodName
on	O
the	O
segments	O
independently	O
.	O
Finally	O
,	O
the	O
predicted	O
spans	O
corresponding	O
to	O
a	O
single	O
sample	O
are	O
aggregated	O
to	O
predict	O
the	O
final	O
span	O
that	O
is	O
the	O
span	O
between	O
the	O
earliest	O
start	O
position	O
and	O
the	O
latest	O
end	O
position	O
.	O
The	O
output	O
is	O
considered	O
empty	O
when	O
all	O
segments	O
have	O
empty	O
spans	O
.	O
RoBERTa	B-MethodName
has	O
the	O
same	O
model	O
architecture	O
and	O
input	O
length	O
limitation	O
as	O
BERT	B-MethodName
but	O
with	O
a	O
robustly	O
optimized	O
pre	O
-	O
training	O
scheme	O
allowing	O
it	O
to	O
generalize	O
better	O
to	O
downstream	O
tasks	O
such	O
as	O
QA	O
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
apply	O
the	O
same	O
sliding	O
window	O
approach	O
for	O
RoBERTa	B-MethodName
.	O

In	O
order	O
to	O
process	O
the	O
question	O
and	O
entire	O
documents	O
at	O
the	O
same	O
time	O
,	O
we	O
use	O
the	O
Longformer	B-MethodName
model	O
.	O
It	O
employs	O
an	O
attention	O
mechanism	O
scaling	O
linearly	O
with	O
the	O
sequence	O
length	O
which	O
enables	O
Longformer	B-MethodName
to	O
process	O
up	O
to	O
4	O
,	O
096	O
tokens	O
.	O
It	O
uses	O
multiple	O
attention	O
heads	O
with	O
different	O
dilation	O
configurations	O
to	O
attend	O
to	O
the	O
entire	O
sequence	O
and	O
includes	O
global	O
attention	O
to	O
question	O
tokens	O
in	O
the	O
sequence	O
.	O
Question	O
and	O
document	O
pairs	O
are	O
packed	O
together	O
into	O
a	O
single	O
sequence	O
without	O
having	O
to	O
use	O
sliding	O
windows	O
and	O
the	O
answer	O
span	O
is	O
calculated	O
by	O
a	O
dot	O
product	O
(	O
Beltagy	O
et	O
al	O
,	O
2020	O
)	O
.	O

We	O
study	O
the	O
extent	O
to	O
which	O
emoji	O
can	O
be	O
used	O
to	O
add	O
interpretability	O
to	O
embeddings	O
of	O
text	O
and	O
emoji	O
.	O
To	O
do	O
so	O
,	O
we	O
extend	O
the	O
POLAR	O
-	O
framework	O
that	O
transforms	O
word	B-TaskName
embeddings	I-TaskName
to	O
interpretable	O
counterparts	O
and	O
apply	O
it	O
to	O
word	O
-	O
emoji	O
embeddings	O
trained	O
on	O
four	O
years	O
of	O
messaging	O
data	O
from	O
the	O
Jodel	O
social	O
network	O
.	O
We	O
devise	O
a	O
crowdsourced	O
human	O
judgement	O
experiment	O
to	O
study	O
six	O
usecases	O
,	O
evaluating	O
against	O
words	O
only	O
,	O
what	O
role	O
emoji	O
can	O
play	O
in	O
adding	O
interpretability	O
to	O
word	B-TaskName
embeddings	I-TaskName
.	O
That	O
is	O
,	O
we	O
use	O
a	O
revised	O
POLAR	O
approach	O
interpreting	O
words	O
and	O
emoji	O
with	O
words	O
,	O
emoji	O
or	O
both	O
according	O
to	O
human	O
judgement	O
.	O
We	O
find	O
statistically	O
significant	O
trends	O
demonstrating	O
that	O
emoji	O
can	O
be	O
used	O
to	O
interpret	O
other	O
emoji	O
very	O
well	O
.	O

Word	B-TaskName
embeddings	I-TaskName
create	O
a	O
vector	O
-	O
space	O
representation	O
in	O
which	O
words	O
with	O
a	O
similar	O
meaning	O
are	O
in	O
close	O
proximity	O
.	O
Existing	O
approaches	O
to	O
make	O
embeddings	O
interpretable	O
,	O
e.g.	O
,	O
via	O
contextual	O
(	O
Subramanian	O
et	O
al	O
,	O
2018	O
)	O
,	O
sparse	O
embeddings	O
(	O
Panigrahi	O
et	O
al	O
,	O
2019	O
)	O
,	O
or	O
learned	O
(	O
Senel	O
et	O
al	O
,	O
2018	O
)	O
transformations	O
(	O
Mathew	O
et	O
al	O
,	O
2020	O
)	O
-	O
all	O
focus	O
on	O
text	O
only	O
.	O
Yet	O
,	O
emoji	O
are	O
widely	O
used	O
in	O
casual	O
communication	O
,	O
e.g.	O
,	O
Online	O
Social	O
Networks	O
(	O
OSN	O
)	O
,	O
and	O
are	O
known	O
to	O
extend	O
textual	O
expressiveness	O
,	O
demonstrated	O
to	O
benefit	O
,	O
e.g.	O
,	O
sentiment	B-TaskName
analysis	I-TaskName
(	O
Novak	O
et	O
al	O
,	O
2015	O
;	O
Hu	O
et	O
al	O
,	O
2017	O
)	O
.	O

We	O
raise	O
the	O
question	O
if	O
we	O
can	O
leverage	O
the	O
expressiveness	O
of	O
emoji	O
to	O
make	O
word	B-TaskName
embeddings	I-TaskName
-	O
and	O
thus	O
also	O
emoji	O
-	O
interpretable	O
.	O
I.e.	O
,	O
can	O
we	O
adopt	O
word	O
embedding	O
interpretability	O
via	O
leveraging	O
semantic	O
polar	O
opposites	O
(	O
e.g.	O
,	O
cold	O
/	O
hot	O
)	O
to	O
emoji	O
(	O
e.g.	O
,	O
/	O
,	O
or	O
/	O
)	O
for	O
interpreting	O
words	O
or	O
emoji	O
w.r.t	O
.	O
human	O
judgement	O
.	O
*	O
Timon	O
Mohaupt	O
performed	O
this	O
work	O
during	O
his	O
master	O
thesis	O
at	O
Brandenburg	O
University	O
of	O
Technology	O
and	O
RWTH	O
Aachen	O
University	O
.	O
Approach	O
.	O
Motivated	O
and	O
based	O
upon	O
POLAR	O
(	O
Mathew	O
et	O
al	O
,	O
2020	O
)	O
,	O
we	O
deploy	O
a	O
revised	O
variant	O
POLAR	O
ρ	O
that	O
transforms	O
arbitrary	O
word	B-TaskName
embeddings	I-TaskName
into	O
interpretable	O
counterparts	O
.	O
The	O
key	O
idea	O
is	O
to	O
leverage	O
semantic	O
differentials	O
as	O
a	O
psychometric	O
tool	O
to	O
align	O
embedded	O
terms	O
on	O
a	O
scale	O
between	O
two	O
polar	O
opposites	O
.	O
Employing	O
a	O
projection	O
-	O
based	O
transformation	O
in	O
POLAR	O
ρ	O
,	O
we	O
provide	O
embedding	O
dimensions	O
with	O
semantic	O
information	O
.	O
I.e.	O
,	O
the	O
resulting	O
interpretable	O
embedding	O
space	O
values	O
directly	O
estimate	O
a	O
term	O
's	O
position	O
on	O
a	O
-	O
priori	O
provided	O
polar	O
opposite	O
scales	O
,	O
while	O
approximately	O
preserving	O
in	O
-	O
embedding	O
structures	O
(	O
2	O
)	O
.	O
The	O
main	O
contribution	O
of	O
this	O
work	O
is	O
the	O
largescale	O
application	O
of	O
this	O
approach	O
to	O
a	O
social	O
media	O
corpus	O
and	O
especially	O
its	O
evaluation	O
in	O
a	O
crowdsourced	O
human	O
judgement	O
experiment	O
.	O
For	O
studying	O
the	O
role	O
of	O
emoji	O
in	O
interpretability	O
,	O
we	O
create	O
a	O
word	O
-	O
emoji	O
input	O
embedding	O
from	O
on	O
a	O
large	O
social	O
media	O
corpus	O
.	O
The	O
dataset	O
comprises	O
four	O
years	O
of	O
complete	O
data	O
in	O
a	O
single	O
country	O
from	O
the	O
online	O
social	O
network	O
provider	O
Jodel	O
(	O
48	O
M	O
posts	O
of	O
which	O
11	O
M	O
contain	O
emoji	O
)	O
.	O
For	O
subsequent	O
main	O
evaluation	O
,	O
we	O
make	O
this	O
embedding	O
interpretable	O
with	O
word	O
and	O
emoji	O
opposites	O
by	O
deploying	O
our	O
adopted	O
tool	O
POLAR	O
ρ	O
(	O
3	O
)	O
.	O
Given	O
different	O
expressiveness	O
of	O
emoji	O
,	O
we	O
ask	O
RQ1	O
)	O
How	O
does	O
adding	O
emoji	O
to	O
POLAR	O
ρ	O
impact	O
interpretability	O
w.r.t	O
.	O
to	O
human	O
judgement	O
?	O
I.e.	O
,	O
do	O
humans	O
agree	O
on	O
best	O
interpretable	O
dimensions	O
for	O
describing	O
words	O
or	O
emoji	O
with	O
word	O
or	O
emoji	O
opposites	O
?	O
And	O
RQ2	O
)	O
How	O
well	O
do	O
POLAR	O
ρ	O
-	O
semantic	O
dimensions	O
reflect	O
a	O
term	O
's	O
position	O
on	O
a	O
scale	O
between	O
word	O
or	O
emoji	O
polar	O
opposites	O
?	O
Human	O
judgement	O
.	O
We	O
design	O
a	O
crowdsourced	O
human	O
judgement	O
experiment	O
(	O
4	O
)	O
to	O
study	O
if	O
adding	O
emoji	O
to	O
word	B-TaskName
embeddings	I-TaskName
and	O
POLAR	O
ρ	O
in	O
particular	O
increases	O
the	O
interpretability	O
-	O
while	O
also	O
answering	O
how	O
to	O
describe	O
emoji	O
best	O
.	O
Our	O
human	O
judgement	O
experiment	O
involves	O
six	O
campaigns	O
explaining	O
Words	O
(	O
W/	O
*	O
)	O
or	O
Emoji	O
(	O
E/	O
*	O
)	O
with	O
Words	O
,	O
Figure	O
1	O
:	O
The	O
POLAR	O
-	O
framework	O
(	O
Mathew	O
et	O
al	O
,	O
2020	O
)	O
makes	O
word	B-TaskName
embeddings	I-TaskName
interpretable	O
leveraging	O
polar	O
opposites	O
.	O
It	O
provides	O
a	O
new	O
interpretable	O
embedding	O
subspace	O
with	O
systematic	O
polar	O
opposite	O
scales	O
:	O
Along	O
six	O
use	O
-	O
cases	O
,	O
we	O
evaluate	O
which	O
role	O
emoji	O
expressiveness	O
plays	O
in	O
adding	O
interpretability	O
to	O
word	B-TaskName
embeddings	I-TaskName
.	O
I.e.	O
,	O
how	O
well	O
can	O
our	O
adopted	O
POLAR	O
ρ	O
interpret	O
(	O
W/	O
*	O
)	O
words	O
or	O
(	O
E/	O
*	O
)	O
emoji	O
with	O
words	O
,	O
emoji	O
or	O
both	O
(	O
*	O
/M	O
)	O
,	O
Mixed	O
.	O
We	O
test	O
POLAR	O
ρ	O
alignment	O
with	O
human	O
judgement	O
as	O
represented	O
in	O
shown	O
semantic	O
profiles	O
above	O
.	O
Emoji	O
,	O
or	O
both	O
Mixed	O
.	O
We	O
evaluate	O
two	O
test	O
conditions	O
to	O
answer	O
both	O
research	O
questions	O
:	O
(	O
RQ1	O
)	O
a	O
selection	O
test	O
studies	O
if	O
human	O
subjects	O
agree	O
to	O
the	O
POLAR	O
ρ	O
identified	O
differentials	O
(	O
e.g.	O
,	O
how	O
do	O
emoji	O
affect	O
POLAR	O
ρ	O
interpretability	O
?	O
)	O
,	O
and	O
(	O
RQ2	O
)	O
a	O
preference	O
test	O
that	O
studies	O
if	O
the	O
direction	O
on	O
a	O
given	O
differential	O
scale	O
is	O
in	O
line	O
with	O
human	O
judgement	O
(	O
e.g.	O
,	O
how	O
well	O
does	O
POLAR	O
ρ	O
interpret	O
scales	O
)	O
.	O
Results	O
.	O
POLAR	O
ρ	O
identifies	O
the	O
best	O
interpretable	O
opposites	O
for	O
describing	O
emoji	O
with	O
emoji	O
,	O
yet	O
generally	O
aligning	O
well	O
with	O
human	O
judgement	O
.	O
Except	O
interpreting	O
words	O
with	O
emoji	O
only	O
probably	O
due	O
to	O
lack	O
of	O
emoji	O
expressiveness	O
indicated	O
by	O
coder	O
agreement	O
.	O
Further	O
,	O
POLAR	O
ρ	O
estimates	O
an	O
embedded	O
terms	O
'	O
position	O
on	O
a	O
scale	O
between	O
opposites	O
successfully	O
,	O
especially	O
for	O
interpreting	O
emoji	O
.	O
Broader	O
application	O
.	O
Not	O
all	O
emoji	O
have	O
a	O
universally	O
agreed	O
on	O
meaning	O
.	O
Prior	O
work	O
showed	O
that	O
differences	O
in	O
the	O
meaning	O
of	O
emoji	O
exist	O
between	O
cultures	O
(	O
Guntuku	O
et	O
al	O
,	O
2019	O
;	O
Gupta	O
et	O
al	O
,	O
2021	O
)	O
.	O
Even	O
within	O
the	O
same	O
culture	O
,	O
ambiguity	O
and	O
double	O
meanings	O
of	O
emoji	O
exist	O
(	O
Reelfs	O
et	O
al	O
,	O
2020	O
)	O
.	O
Currently	O
,	O
no	O
data	O
-	O
driven	O
approach	O
exists	O
to	O
infer	O
the	O
meaning	O
of	O
emoji	O
-	O
to	O
make	O
them	O
interpretable	O
.	O
Our	O
proposed	O
approach	O
can	O
be	O
used	O
to	O
tackle	O
this	O
challenge	O
since	O
it	O
makes	O
emoji	O
interpretable	O
.	O

Semantic	O
Differentials	O
.	O
Based	O
upon	O
the	O
idea	O
of	O
semantic	O
differentials	O
as	O
a	O
psychometric	O
tool	O
to	O
align	O
a	O
word	O
on	O
a	O
scale	O
between	O
two	O
polar	O
opposites	O
(	O
Fig	O
.	O
1	O
)	O
,	O
POLAR	O
(	O
Mathew	O
et	O
al	O
,	O
2020	O
)	O
takes	O
a	O
word	O
embedding	O
as	O
input	O
and	O
creates	O
a	O
new	O
interpretable	O
embedding	O
on	O
a	O
polar	O
subspace	O
.	O
This	O
subspace	O
,	O
i.e.	O
,	O
the	O
opposites	O
used	O
for	O
the	O
interpretable	O
embedding	O
are	O
defined	O
by	O
an	O
external	O
source	O
.	O
That	O
is	O
,	O
starting	O
with	O
a	O
corpus	O
and	O
its	O
vocabulary	O
V	O
,	O
a	O
word	O
embedding	O
created	O
by	O
an	O
algorithm	O
a	O
(	O
e.g.	O
,	O
Word2Vec	O
or	O
GloVe	B-MethodName
)	O
assigns	O
vectors	O
−	O
W	O
a	O
v	O
R	O
d	O
on	O
d	O
dimensions	O
to	O
all	O
words	O
v	O
V	O
according	O
to	O
an	O
optimization	O
function	O
(	O
usually	O
word	O
co	O
-	O
occurrence	O
)	O
.	O
This	O
pretraining	O
results	O
in	O
an	O
embedding	O
D	O
=	O
−	O
W	O
a	O
v	O
,	O
v	O
V	O
R	O
|	O
V	O
|	O
×d	O
.	O
Such	O
embedding	O
spaces	O
carry	O
a	O
semantic	O
structure	O
between	O
embedded	O
words	O
,	O
whereas	O
the	O
dimensions	O
do	O
not	O
have	O
any	O
specific	O
meaning	O
.	O
However	O
,	O
we	O
can	O
leverage	O
the	O
semantic	O
structure	O
between	O
words	O
to	O
transform	O
the	O
embedding	O
space	O
to	O
carrying	O
over	O
meaning	O
into	O
the	O
dimensions	O
:	O
POLAR	O
uses	O
N	O
semantic	O
differentials	O
/	O
opposites	O
that	O
are	O
itself	O
items	O
within	O
the	O
embedding	O
,	O
i.e.	O
,	O
P	O
=	O
(	O
p	O
i	O
z	O
,	O
p	O
i	O
−z	O
)	O
,	O
i	O
[	O
1	O
..	O
N	O
]	O
,	O
(	O
p	O
i	O
z	O
,	O
p	O
i	O
−z	O
)	O
⊆	O
V	O
2	O
.	O
As	O
shown	O
in	O
Fig	O
.	O
2a	O
,	O
given	O
two	O
anchor	O
points	O
for	O
each	O
polar	O
opposite	O
,	O
a	O
line	O
between	O
them	O
represents	O
a	O
differential	O
-	O
which	O
we	O
name	O
POLAR	O
direction	O
)	O
onto	O
this	O
subspace	O
(	O
e.g.	O
,	O
left	O
:	O
/	O
,	O
right	O
:	O
/	O
)	O
yields	O
a	O
direct	O
scale	O
measure	O
between	O
both	O
opposites	O
in	O
the	O
adjacent	O
leg	O
(	O
green	O
vectors	O
,	O
directed	O
alike	O
the	O
differential	O
)	O
.	O
(	O
c	O
)	O
The	O
resulting	O
interpretable	O
embedding	O
now	O
contains	O
a	O
tangible	O
position	O
estimation	O
along	O
employed	O
polar	O
dimensions	O
for	O
each	O
embedded	O
term	O
(	O
here	O
:	O
)	O
.	O
(	O
red	O
dashed	O
vectors	O
)	O
:	O
−	O
−	O
dir	O
i	O
=	O
−	O
−	O
W	O
a	O
p	O
i	O
z	O
−	O
−	O
−−	O
W	O
a	O
p	O
i	O
−z	O
R	O
d	O
Base	O
Change	O
.	O
Naturally	O
,	O
we	O
can	O
use	O
these	O
differentials	O
as	O
a	O
new	O
basis	O
for	O
the	O
interpretable	O
embedding	O
E.	O
Gathering	O
all	O
directions	O
in	O
a	O
matrix	O
dir	O
R	O
N	O
×d	O
,	O
we	O
obtain	O
for	O
all	O
embedded	O
terms	O
v	O
V	O
:	O
dir	O
T	O
−	O
E	O
v	O
=	O
−	O
W	O
a	O
v	O
,	O

We	O
next	O
propose	O
an	O
approach	O
to	O
improve	O
the	O
interpretability	O
of	O
word	B-TaskName
embeddings	I-TaskName
by	O
adding	O
emoji	O
.	O
It	O
uses	O
our	O
extended	O
version	O
POLAR	O
ρ	O
and	O
adds	O
emoji	O
to	O
the	O
POLAR	O
space	O
by	O
creating	O
word	B-TaskName
embeddings	I-TaskName
that	O
include	O
emoji	O
.	O

We	O
create	O
a	O
word	O
embedding	O
out	O
of	O
a	O
social	O
media	O
text	O
corpus	O
,	O
since	O
emoji	O
are	O
prominent	O
in	O
communication	O
within	O
Online	O
Social	O
Networks	O
.	O
We	O
decided	O
to	O
use	O
a	O
corpus	O
from	O
the	O
Jodel	O
network	O
,	O
where	O
about	O
one	O
out	O
of	O
four	O
sentences	O
contain	O
emoji	O
(	O
see	O
(	O
Reelfs	O
et	O
al	O
,	O
2020	O
)	O
)	O
.	O
The	O
Jodel	O
Network	O
.	O
We	O
base	O
our	O
study	O
on	O
a	O
country	O
-	O
wide	O
complete	O
dataset	O
of	O
posts	O
in	O
the	O
online	O
social	O
network	O
Jodel	O
,	O
a	O
mobile	O
-	O
only	O
messaging	O
application	O
.	O
It	O
is	O
location	O
-	O
based	O
and	O
establishes	O
local	O
communities	O
relative	O
to	O
the	O
users	O
'	O
location	O
.	O
Within	O
these	O
communities	O
,	O
users	O
can	O
anonymously	O
post	O
photos	O
from	O
the	O
camera	O
app	O
or	O
content	O
of	O
up	O
to	O
250	O
characters	O
length	O
,	O
i.e.	O
,	O
microblogging	O
,	O
and	O
reply	O
to	O
posts	O
forming	O
discussion	O
threads	O
.	O
Corpus	O
.	O
The	O
network	O
operators	O
provided	O
us	O
with	O
data	O
of	O
content	O
created	O
in	O
Germany	O
from	O
2014	O
to	O
2017	O
.	O
It	O
contains	O
48	O
M	O
sentences	O
,	O
of	O
which	O
11	O
M	O
contain	O
emoji	O
(	O
1.76	O
emoji	O
per	O
sentence	O
on	O
average	O
)	O
.	O
Ethics	B-DatasetName
.	O
The	O
dataset	O
contains	O
no	O
personal	O
informa	O
-	O
tion	O
and	O
can	O
not	O
be	O
used	O
to	O
personally	O
identify	O
users	O
except	O
for	O
data	O
that	O
they	O
willingly	O
have	O
posted	O
on	O
the	O
platform	O
.	O
We	O
synchronize	O
with	O
the	O
Jodel	O
operator	O
on	O
analyses	O
we	O
perform	O
on	O
their	O
data	O
.	O

No	O
universal	O
meaning	O
of	O
emoji	O
.	O
Prior	O
work	O
showed	O
that	O
the	O
interpretation	O
of	O
emoji	O
varies	O
(	O
Miller	O
et	O
al	O
,	O
2016	O
;	O
Kimura	O
-	O
Thollander	O
and	O
Kumar	B-DatasetName
,	O
2019	O
)	O
,	O
also	O
between	O
cultures	O
(	O
Guntuku	O
et	O
al	O
,	O
2019	O
;	O
Gupta	O
et	O
al	O
,	O
2021	O
)	O
.	O
Even	O
within	O
the	O
same	O
culture	O
,	O
ambiguity	O
and	O
double	O
meanings	O
of	O
emoji	O
exist	O
(	O
Reelfs	O
et	O
al	O
,	O
2020	O
)	O
and	O
differences	O
exists	O
on	O
the	O
basis	O
of	O
an	O
individual	O
usage	O
(	O
Wiseman	O
and	O
Gould	O
,	O
2018	O
)	O
.	O
These	O
observations	O
motivate	O
the	O
need	O
to	O
better	O
understand	O
the	O
meaning	O
of	O
emoji	O
.	O
Currently	O
,	O
no	O
data	O
-	O
driven	O
approach	O
exists	O
to	O
make	O
emoji	O
interpretable	O
-	O
a	O
gap	O
that	O
we	O
aim	O
to	O
close	O
.	O
Interpretable	O
word	B-TaskName
embeddings	I-TaskName
.	O
Word	B-TaskName
embeddings	I-TaskName
are	O
a	O
common	O
approach	O
to	O
capture	O
meaning	O
;	O
they	O
are	O
a	O
learned	O
vector	O
space	O
representation	O
of	O
text	O
that	O
carries	O
semantic	O
relationships	O
as	O
distances	O
between	O
the	O
embedded	O
words	O
.	O
A	O
rich	O
body	O
of	O
work	O
aims	O
at	O
making	O
word	B-TaskName
embeddings	I-TaskName
interpretable	O
,	O
e.g.	O
,	O
via	O
contextual	O
(	O
Subramanian	O
et	O
al	O
,	O
2018	O
)	O
,	O
sparse	O
embeddings	O
(	O
Panigrahi	O
et	O
al	O
,	O
2019	O
)	O
,	O
or	O
learned	O
(	O
Senel	O
et	O
al	O
,	O
2018	O
transformations	O
(	O
Mathew	O
et	O
al	O
,	O
2020	O
)	O
-	O
all	O
focus	O
on	O
text	O
only	O
.	O
Recently	O
,	O
(	O
Mathew	O
et	O
al	O
,	O
2020	O
)	O
proposed	O
the	O
PO	O
-	O
LAR	O
that	O
takes	O
a	O
word	O
embedding	O
as	O
input	O
and	O
creates	O
a	O
new	O
interpretable	O
embedding	O
on	O
a	O
polar	O
subspace	O
.	O
The	O
POLAR	O
approach	O
is	O
similar	O
to	O
SEMCAT	B-DatasetName
(	O
Senel	O
et	O
al	O
,	O
2018	O
)	O
,	O
but	O
is	O
based	O
on	O
the	O
concept	O
of	O
semantic	O
differentials	O
(	O
Osgood	O
et	O
al	O
,	O
1957	O
)	O
for	O
creating	O
a	O
polar	O
subspace	O
.	O
It	O
measures	O
the	O
meaning	O
of	O
abstract	O
concepts	O
by	O
relying	O
on	O
opposing	O
dimensions	O
associated	O
(	O
good	O
vs.	O
bad	O
,	O
hot	O
vs.	O
cold	O
,	O
conservative	O
vs.	O
liberal	O
)	O
.	O
In	O
this	O
work	O
,	O
we	O
extend	O
and	O
use	O
POLAR	O
.	O
Emoji	O
embeddings	O
.	O
Few	O
works	O
focused	O
on	O
using	O
word	B-TaskName
embeddings	I-TaskName
for	O
creating	O
emoji	O
representations	O
,	O
e.g.	O
,	O
(	O
Eisner	O
et	O
al	O
,	O
2016	O
)	O
or	O
(	O
Reelfs	O
et	O
al	O
,	O
2020	O
)	O
.	O
(	O
Barbieri	O
et	O
al	O
,	O
2016	O
)	O
used	O
a	O
vector	O
space	O
skip	O
-	O
gram	O
model	O
to	O
infer	O
the	O
meaning	O
of	O
emoji	O
in	O
Twitter	O
data	O
(	O
Barbieri	O
et	O
al	O
,	O
2016	O
)	O
.	O
Yet	O
,	O
the	O
general	O
question	O
if	O
the	O
interpretability	O
of	O
word	B-TaskName
embeddings	I-TaskName
can	O
be	O
improved	O
by	O
adding	O
emoji	O
and	O
if	O
different	O
meaning	O
of	O
emoji	O
can	O
be	O
captured	O
remains	O
still	O
open	O
.	O
In	O
this	O
work	O
,	O
we	O
adapt	O
the	O
POLAR	O
interpretability	O
approach	O
to	O
emoji	O
and	O
study	O
in	O
a	O
human	O
subject	O
experiment	O
if	O
word	B-TaskName
embeddings	I-TaskName
can	O
be	O
made	O
interpretable	O
by	O
adding	O
emoji	O
and	O
how	O
emoji	O
can	O
be	O
interpretated	O
by	O
emoji	O
.	O

We	O
raise	O
the	O
question	O
whether	O
we	O
can	O
leverage	O
the	O
expressiveness	O
of	O
emoji	O
to	O
make	O
word	B-TaskName
embeddings	I-TaskName
interpretable	O
.	O
Thus	O
,	O
we	O
use	O
the	O
POLAR	O
framework	O
(	O
Mathew	O
et	O
al	O
,	O
2020	O
)	O
that	O
creates	O
interpretable	O
word	B-TaskName
embeddings	I-TaskName
through	O
semantic	O
differentials	O
,	O
polar	O
opposites	O
.	O
We	O
employ	O
a	O
revised	O
POLAR	O
ρ	O
method	O
that	O
transforms	O
arbitrary	O
word	B-TaskName
embeddings	I-TaskName
to	O
interpretable	O
counterparts	O
to	O
which	O
we	O
added	O
emoji	O
.	O
We	O
base	O
our	O
evaluation	O
on	O
an	O
off	O
the	O
shelf	O
word	O
-	O
emoji	O
embedding	O
from	O
a	O
large	O
social	O
media	O
corpus	O
,	O
resulting	O
in	O
an	O
interpretable	O
embedding	O
based	O
on	O
semantic	O
differentials	O
,	O
i.e.	O
,	O
antonym	O
lists	O
and	O
polar	O
emoji	O
opposites	O
.	O
Via	O
crowdsourced	O
campaigns	O
,	O
we	O
investigate	O
the	O
interpretable	O
word	O
-	O
emoji	O
embedding	O
quality	O
along	O
six	O
use	O
-	O
cases	O
(	O
cf	O
.	O
Fig	O
.	O
1	O
)	O
:	O
Using	O
word	O
-	O
&	O
emoji	O
-	O
polar	O
opposites	O
(	O
or	O
both	O
Mixed	O
)	O
,	O
to	O
interpret	O
words	O
(	O
W	O
/	O
W	O
,	O
W	O
/	O
E	O
,	O
W	O
/	O
M	O
)	O
and	O
emoji	O
(	O
E	O
/	O
W	O
,	O
E	O
/	O
E	O
,	O
E	O
/	O
M	O
)	O
,	O
w.r.t	O
.	O
human	O
interpretability	O
.	O
Overall	O
,	O
we	O
find	O
POLAR	O
ρ	O
's	O
interpretations	O
w	O
/	O
wo	O
emoji	O
being	O
well	O
in	O
line	O
with	O
human	O
judgement	O
.	O
We	O
show	O
that	O
explaining	O
emoji	O
with	O
emoji	O
(	O
E	O
/	O
E	O
)	O
works	O
statistically	O
significantly	O
best	O
,	O
whereas	O
describing	O
words	O
with	O
emoji	O
(	O
W	O
/	O
E	O
)	O
systematically	O
yields	O
the	O
worst	O
performance	O
.	O
We	O
also	O
find	O
good	O
alignment	O
to	O
human	O
judgement	O
estimating	O
a	O
term	O
's	O
position	O
on	O
differential	O
scales	O
,	O
using	O
the	O
POLAR	O
ρ	O
-	O
projection	O
.	O
That	O
is	O
,	O
emoji	O
can	O
improve	O
POLAR	O
ρ	O
's	O
capability	O
in	O
identifying	O
most	O
interpretable	O
semantic	O
differentials	O
.	O
We	O
have	O
demonstrated	O
how	O
emoji	O
can	O
be	O
used	O
to	O
interpret	O
other	O
emoji	O
using	O
POLAR	O
ρ	O
.	O

We	O
thank	O
Felix	O
Dommes	O
,	O
who	O
was	O
instrumental	O
for	O
this	O
work	O
by	O
developing	O
and	O
implementing	O
the	O
POLAR	O
ρ	O
projection	O
approach	O
and	O
the	O
Extremal	O
Word	O
Score	B-MetricName
in	O
his	O
Master	O
Thesis	O
.	O

NoahNMT	O
at	O
WMT	O
2021	O
:	O
Dual	O
Transfer	O
for	O
Very	O
Low	O
Resource	O
Supervised	O
Machine	B-TaskName
Translation	I-TaskName

We	O
reproduced	O
the	O
illustration	O
of	O
dual	O
transfer	O
from	O
the	O
original	O
paper	O
(	O
Zhang	O
et	O
al	O
,	O
2021	O
)	O
,	O
as	O
shown	O
in	O
Figure	O
1	O
.	O
This	O
illustration	O
shows	O
the	O
case	O
of	O
general	O
transfer	O
,	O
where	O
the	O
high	O
resource	O
translation	O
direction	O
is	O
A	O
B	O
,	O
and	O
the	O
low	O
resource	O
translation	O
direction	O
is	O
P	O
Q.	O
As	O
discussed	O
in	O
the	O
original	O
paper	O
,	O
in	O
many	O
cases	O
,	O
it	O
is	O
possible	O
to	O
use	O
shared	O
target	O
transfer	O
(	O
B	O
=	O
Q	O
)	O
or	O
shared	O
source	O
transfer	O
(	O
A	O
=	O
P	O
)	O
.	O
Taking	O
chv	O
ru	O
as	O
an	O
example	O
,	O
we	O
can	O
choose	O
en	O
ru	O
as	O
the	O
high	O
resource	O
translation	O
direction	O
,	O
resulting	O
in	O
an	O
instance	O
of	O
shared	O
target	O
transfer	O
.	O
In	O
this	O
shared	O
task	O
,	O
when	O
training	O
the	O
high	O
resource	O
translation	O
model	O
,	O
we	O
always	O
initialize	O
the	O
shared	O
language	O
side	O
with	O
the	O
pretrained	O
language	O
model	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
.	O

In	O
our	O
preliminary	O
experiments	O
,	O
we	O
found	O
it	O
beneficial	O
to	O
use	O
a	O
closely	O
related	O
language	O
as	O
the	O
parent	O
language	O
.	O
It	O
is	O
clear	O
that	O
there	O
are	O
several	O
factors	O
that	O
should	O
be	O
taken	O
into	O
account	O
,	O
such	O
as	O
the	O
degree	O
of	O
closeness	O
,	O
and	O
the	O
amount	O
of	O
resource	O
for	O
training	O
the	O
parent	O
model	O
.	O
For	O
Upper	O
Sorbian	O
,	O
Czech	O
(	O
cs	O
)	O
is	O
closely	O
related	O
to	O
it	O
,	O
and	O
Czech	O
-	O
German	O
has	O
a	O
good	O
amount	O
of	O
parallel	O
data	O
,	O
so	O
we	O
directly	O
choose	O
Czech	O
as	O
the	O
parent	O
language	O
.	O
Chuvash	O
,	O
however	O
,	O
is	O
a	O
rather	O
isolated	O
language	O
in	O
the	O
Turkic	O
family	O
.	O
The	O
closest	O
language	O
with	O
usable	O
data	O
is	O
Kazakh	O
(	O
kk	O
)	O
,	O
but	O
the	O
amount	O
of	O
parallel	O
data	O
for	O
Kazakh	O
-	O
Russian	O
is	O
relatively	O
small	O
,	O
and	O
we	O
found	O
it	O
to	O
be	O
quite	O
noisy	O
.	O
Therefore	O
,	O
we	O
considered	O
using	O
English	O
(	O
en	O
)	O
as	O
the	O
parent	O
language	O
of	O
Chuvash	O
.	O
Even	O
though	O
English	O
is	O
unrelated	O
to	O
Chuvash	O
and	O
they	O
use	O
different	O
scripts	O
,	O
English	O
-	O
Russian	O
has	O
more	O
parallel	O
data	O
that	O
can	O
guarantee	O
the	O
quality	O
of	O
the	O
parent	O
model	O
.	O
We	O
conducted	O
an	O
experiment	O
with	O
Transformer	B-MethodName
base	O
.	O
Results	O
in	O
Table	O
2	O
indicate	O
that	O
English	O
can	O
serve	O
as	O
an	O
eligible	O
parent	O
for	O
Chuvash	O
.	O
Considering	O
that	O
we	O
plan	O
to	O
use	O
Transformer	B-MethodName
big	O
for	O
which	O
data	O
amount	O
is	O
likely	O
to	O
play	O
a	O
more	O
important	O
role	O
,	O
we	O
decided	O
to	O
use	O
English	O
as	O
the	O
parent	O
language	O
for	O
Chuvash	O
.	O

The	O
original	O
paper	O
(	O
Zhang	O
et	O
al	O
,	O
2021	O
)	O
evaluated	O
dual	O
transfer	O
only	O
with	O
Transformer	B-MethodName
base	O
.	O
In	O
this	O
shared	O
task	O
,	O
we	O
scale	O
up	O
to	O
Transformer	B-MethodName
big	O
.	O
We	O
also	O
face	O
a	O
more	O
realistic	O
setting	O
where	O
the	O
monolingual	O
data	O
for	O
the	O
low	O
resource	O
languages	O
(	O
chv	O
and	O
hsb	O
)	O
are	O
quite	O
scarce	O
.	O
Therefore	O
it	O
is	O
worth	O
testing	O
the	O
effect	O
of	O
scaling	O
up	O
.	O
Results	O
in	O
Table	O
3	O
show	O
that	O
Transformer	B-MethodName
big	O
brings	O
consistent	O
improvements	O
.	O
We	O
also	O
report	O
the	O
runtime	O
of	O
each	O
step	O
in	O
dual	O
transfer	O
for	O
NMT	O
chv	O
ru	O
with	O
Transformer	B-MethodName
big	O
in	O
Table	O
4	O
for	O
reference	O
,	O
but	O
the	O
numbers	O
can	O
vary	O
depending	O
on	O
implementation	O
and	O
data	O
size	O
.	O
In	O
the	O
following	O
experiments	O
and	O
our	O
final	O
submission	O
,	O
we	O
use	O
Transformer	B-MethodName
big	O
models	O
.	O

For	O
chv	O
ru	O
and	O
ru	O
chv	O
,	O
we	O
perform	O
selected	O
finetuning	O
starting	O
from	O
the	O
best	O
models	O
from	O
iterative	O
back	O
-	O
translation	O
(	O
Iteration	O
2	O
for	O
chv	O
ru	O
,	O
Iteration	O
3	O
for	O
ru	O
chv	O
)	O
.	O
Note	O
that	O
the	O
selected	O
training	O
subsets	O
are	O
different	O
from	O
those	O
in	O
Section	O
4.4	O
because	O
the	O
selection	O
is	O
based	O
on	O
the	O
source	O
side	O
of	O
the	O
blind	O
test	O
sets	O
.	O
We	O
finetune	O
five	O
times	O
with	O
different	O
random	O
seeds	B-DatasetName
for	O
model	O
ensemble	O
.	O
For	O
hsb	O
de	O
and	O
de	O
hsb	O
,	O
we	O
ensemble	O
the	O
five	O
models	O
from	O
iterative	O
back	O
-	O
translation	O
.	O

In	O
this	O
paper	O
,	O
we	O
describe	O
a	O
series	O
of	O
experiments	O
that	O
contribute	O
to	O
our	O
submission	O
to	O
the	O
WMT	O
2021	O
shared	O
task	O
of	O
Very	O
Low	O
Resource	O
Supervised	O
Machine	B-TaskName
Translation	I-TaskName
.	O
These	O
experiments	O
,	O
as	O
well	O
as	O
the	O
good	O
results	O
of	O
the	O
final	O
submission	O
,	O
show	O
that	O
dual	O
transfer	O
can	O
work	O
in	O
synergy	O
with	O
several	O
widely	O
used	O
techniques	O
in	O
realistic	O
scenarios	O
.	O

Towards	O
Generative	O
Aspect	B-TaskName
-	I-TaskName
Based	I-TaskName
Sentiment	I-TaskName
Analysis	I-TaskName
*	O

Aspect	B-TaskName
-	I-TaskName
based	I-TaskName
sentiment	I-TaskName
analysis	I-TaskName
(	O
ABSA	O
)	O
has	O
received	O
increasing	O
attention	O
recently	O
.	O
Most	O
existing	O
work	O
tackles	O
ABSA	O
in	O
a	O
discriminative	O
manner	O
,	O
designing	O
various	O
task	O
-	O
specific	O
classification	O
networks	O
for	O
the	O
prediction	O
.	O
Despite	O
their	O
effectiveness	O
,	O
these	O
methods	O
ignore	O
the	O
rich	O
label	O
semantics	O
in	O
ABSA	O
problems	O
and	O
require	O
extensive	O
task	O
-	O
specific	O
designs	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
to	O
tackle	O
various	O
ABSA	O
tasks	O
in	O
a	O
unified	O
generative	O
framework	O
.	O
Two	O
types	O
of	O
paradigms	O
,	O
namely	O
annotation	O
-	O
style	O
and	O
extraction	O
-	O
style	O
modeling	O
,	O
are	O
designed	O
to	O
enable	O
the	O
training	O
process	O
by	O
formulating	O
each	O
ABSA	O
task	O
as	O
a	O
text	B-TaskName
generation	I-TaskName
problem	O
.	O
We	O
conduct	O
experiments	O
on	O
four	O
ABSA	O
tasks	O
across	O
multiple	O
benchmark	O
datasets	O
where	O
our	O
proposed	O
generative	O
approach	O
achieves	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
in	O
almost	O
all	O
cases	O
.	O
This	O
also	O
validates	O
the	O
strong	O
generality	O
of	O
the	O
proposed	O
framework	O
which	O
can	O
be	O
easily	O
adapted	O
to	O
arbitrary	O
ABSA	O
task	O
without	O
additional	O
taskspecific	O
model	O
design	O
.	O
1	O

Aspect	B-TaskName
-	I-TaskName
based	I-TaskName
sentiment	I-TaskName
analysis	I-TaskName
(	O
ABSA	O
)	O
,	O
aiming	O
at	O
mining	O
fine	O
-	O
grained	O
opinion	O
information	O
towards	O
specific	O
aspects	O
,	O
has	O
attracted	O
increasing	O
attention	O
in	O
recent	O
years	O
(	O
Liu	O
,	O
2012	O
)	O
.	O
Multiple	O
fundamental	O
sentiment	O
elements	O
are	O
involved	O
in	O
ABSA	O
,	O
including	O
the	O
aspect	O
term	O
,	O
opinion	O
term	O
,	O
aspect	O
category	O
,	O
and	O
sentiment	O
polarity	O
.	O
Given	O
a	O
simple	O
example	O
sentence	O
"	O
The	O
pizza	O
is	O
delicious	O
.	O
"	O
,	O
the	O
corresponding	O
elements	O
are	O
"	O
pizza	O
"	O
,	O
"	O
delicious	O
"	O
,	O
"	O
food	O
quality	O
"	O
and	O
"	O
positive	O
"	O
,	O
respectively	O
.	O
The	O
main	O
research	O
line	O
of	O
ABSA	O
focuses	O
on	O
the	O
identification	O
of	O
those	O
sentiment	O
elements	O
such	O
as	O
extracting	O
the	O
aspect	O
term	O
(	O
Liu	O
et	O
al	O
,	O
2015	O
;	O
Yin	O
et	O
al	O
,	O
2016	O
;	O
Li	O
et	O
al	O
,	O
2018	O
;	O
Ma	O
et	O
al	O
,	O
2019	O
)	O
or	O
classifying	O
the	O
sentiment	O
polarity	O
for	O
a	O
given	O
aspect	O
(	O
Wang	O
et	O
al	O
,	O
2016	O
;	O
Chen	O
et	O
al	O
,	O
2017	O
;	O
Jiang	O
et	O
al	O
,	O
2019	O
;	O
Zhang	O
and	O
Qian	O
,	O
2020	O
)	O
.	O
To	O
provide	O
more	O
detailed	O
information	O
,	O
many	O
recent	O
studies	O
propose	O
to	O
jointly	O
predict	O
multiple	O
elements	O
simultaneously	O
(	O
Li	O
et	O
al	O
,	O
2019a	O
;	O
Wan	O
et	O
al	O
,	O
2020	O
;	O
Peng	O
et	O
al	O
,	O
2020	O
;	O
Zhao	O
et	O
al	O
,	O
2020	O
)	O
.	O
Taking	O
the	O
Unified	O
ABSA	O
(	O
UABSA	O
,	O
also	O
called	O
End	O
-	O
to	O
-	O
End	O
ABSA	O
)	O
task	O
as	O
an	O
example	O
,	O
it	O
tries	O
to	O
simultaneously	O
predict	O
the	O
mentioned	O
aspect	O
terms	O
and	O
the	O
corresponding	O
sentiment	O
polarities	O
(	O
Luo	O
et	O
al	O
,	O
2019	O
;	O
He	O
et	O
al	O
,	O
2019	O
)	O
.	O
In	O
general	O
,	O
most	O
ABSA	O
tasks	O
are	O
formulated	O
as	O
either	O
sequence	O
-	O
level	O
or	O
token	O
-	O
level	O
classification	O
problems	O
(	O
Li	O
et	O
al	O
,	O
2019b	O
)	O
.	O
By	O
designing	O
taskspecific	O
classification	O
networks	O
,	O
the	O
prediction	O
is	O
made	O
in	O
a	O
discriminative	O
manner	O
,	O
using	O
the	O
class	O
index	O
as	O
labels	O
for	O
training	O
(	O
Huang	O
and	O
Carley	O
,	O
2018	O
;	O
Wan	O
et	O
al	O
,	O
2020	O
)	O
.	O
However	O
,	O
these	O
methods	O
ignore	O
the	O
label	O
semantics	O
,	O
i.e.	O
,	O
the	O
meaning	O
of	O
the	O
natural	O
language	O
labels	O
,	O
during	O
the	O
training	O
process	O
.	O
Intuitively	O
,	O
knowing	O
the	O
meaning	O
of	O
"	O
food	O
quality	O
"	O
and	O
"	O
restaurant	O
ambiance	O
"	O
,	O
it	O
can	O
be	O
much	O
easier	O
to	O
identify	O
that	O
the	O
former	O
one	O
is	O
more	O
likely	O
to	O
be	O
the	O
correct	O
aspect	O
category	O
for	O
the	O
concerned	O
aspect	O
"	O
pizza	O
"	O
.	O
Such	O
semantics	O
of	O
the	O
label	O
can	O
be	O
more	O
helpful	O
for	O
the	O
joint	O
extraction	O
of	O
multiple	O
sentiment	O
elements	O
,	O
due	O
to	O
the	O
complicated	O
interactions	O
of	O
those	O
involved	O
elements	O
.	O
For	O
example	O
,	O
understanding	O
"	O
delicious	O
"	O
is	O
an	O
adjective	O
for	O
describing	O
the	O
food	O
such	O
as	O
"	O
pizza	O
"	O
could	O
better	O
lead	O
to	O
the	O
prediction	O
of	O
aspect	O
opinion	O
pair	O
(	O
"	O
pizza	O
"	O
,	O
"	O
delicious	O
"	O
)	O
.	O
Another	O
issue	O
is	O
that	O
different	O
classification	O
models	O
are	O
proposed	O
to	O
suit	O
the	O
need	O
of	O
different	O
ABSA	O
problems	O
,	O
making	O
it	O
difficult	O
to	O
adapt	O
the	O
model	O
from	O
one	O
to	O
another	O
.	O
Motivated	O
by	O
recent	O
success	O
in	O
formulating	O
sev	O
-	O
eral	O
language	O
understanding	O
problems	O
such	O
as	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
,	O
question	B-TaskName
answering	I-TaskName
,	O
and	O
text	B-TaskName
classification	I-TaskName
as	O
generation	O
tasks	O
(	O
Raffel	O
et	O
al	O
,	O
2020	O
;	O
Athiwaratkun	O
et	O
al	O
,	O
2020	O
)	O
,	O
we	O
propose	O
to	O
tackle	O
various	O
ABSA	O
problems	O
in	O
a	O
unified	O
generative	O
approach	O
in	O
this	O
paper	O
.	O
It	O
can	O
fully	O
utilize	O
the	O
rich	O
label	O
semantics	O
by	O
encoding	O
the	O
natural	O
language	O
label	O
into	O
the	O
target	O
output	O
.	O
Moreover	O
,	O
this	O
unified	O
generative	O
model	O
can	O
be	O
seamlessly	O
adapted	O
to	O
multiple	O
tasks	O
without	O
introducing	O
additional	O
task	O
-	O
specific	O
model	O
designs	O
.	O
In	O
order	O
to	O
enable	O
the	O
Generative	O
Aspect	B-TaskName
-	I-TaskName
based	I-TaskName
Sentiment	I-TaskName
analysis	I-TaskName
(	O
GAS	O
)	O
,	O
we	O
tailor	O
-	O
make	O
two	O
paradigms	O
,	O
namely	O
annotation	O
-	O
style	O
and	O
extractionstyle	O
modeling	O
to	O
transform	O
the	O
original	O
task	O
as	O
a	O
generation	O
problem	O
.	O
Given	O
a	O
sentence	O
,	O
the	O
former	O
one	O
adds	O
annotations	O
on	O
it	O
to	O
include	O
the	O
label	O
information	O
when	O
constructing	O
the	O
target	O
sentence	O
;	O
while	O
the	O
latter	O
directly	O
adopts	O
the	O
desired	O
natural	O
language	O
label	O
of	O
the	O
input	O
sentence	O
as	O
the	O
target	O
.	O
The	O
original	O
sentence	O
and	O
the	O
target	O
sentence	O
produced	O
by	O
either	O
paradigm	O
can	O
then	O
be	O
paired	O
as	O
a	O
training	O
instance	O
of	O
the	O
generation	O
model	O
.	O
Furthermore	O
,	O
we	O
propose	O
a	O
prediction	O
normalization	O
strategy	O
to	O
handle	O
the	O
issue	O
that	O
the	O
generated	O
sentiment	O
element	O
falls	O
out	O
of	O
its	O
corresponding	O
label	O
vocabulary	O
set	O
.	O
We	O
investigate	O
four	O
ABSA	O
tasks	O
including	O
Aspect	O
Opinion	O
Pair	O
Extraction	O
(	O
AOPE	O
)	O
,	O
Unified	O
ABSA	O
(	O
UABSA	O
)	O
,	O
Aspect	B-TaskName
Sentiment	I-TaskName
Triplet	I-TaskName
Extraction	I-TaskName
(	O
ASTE	O
)	O
,	O
and	O
Target	O
Aspect	O
Sentiment	O
Detection	O
(	O
TASD	O
)	O
with	O
the	O
proposed	O
unified	O
GAS	O
framework	O
to	O
verify	O
its	O
effectiveness	O
and	O
generality	O
.	O
Our	O
main	O
contributions	O
are	O
1	O
)	O
We	O
tackle	O
various	O
ABSA	O
tasks	O
in	O
a	O
novel	O
generative	O
manner	O
;	O
2	O
)	O
We	O
propose	O
two	O
paradigms	O
to	O
formulate	O
each	O
task	O
as	O
a	O
generation	O
problem	O
and	O
a	O
prediction	O
normalization	O
strategy	O
to	O
refine	O
the	O
generated	O
outputs	O
;	O
3	O
)	O
We	O
conduct	O
experiments	O
on	O
multiple	O
benchmark	O
datasets	O
across	O
four	O
ABSA	O
tasks	O
and	O
our	O
approach	O
surpasses	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
almost	O
all	O
cases	O
.	O
Specifically	O
,	O
we	O
obtain	O
7.6	O
and	O
3.7	O
averaged	O
gains	O
on	O
the	O
challenging	O
ASTE	O
and	O
TASD	O
task	O
respectively	O
.	O
2	O
Generative	O
ABSA	O
(	O
GAS	O
)	O

In	O
this	O
section	O
,	O
we	O
describe	O
the	O
investigated	O
ABSA	O
tasks	O
and	O
the	O
proposed	O
two	O
paradigms	O
,	O
namely	O
,	O
annotation	O
-	O
style	O
and	O
extraction	O
-	O
style	O
modeling	O
.	O
Aspect	O
Opinion	O
Pair	O
Extraction	O
(	O
AOPE	O
)	O
aims	O
to	O
extract	O
aspect	O
terms	O
and	O
their	O
corresponding	O
opinion	O
terms	O
as	O
pairs	O
(	O
Zhao	O
et	O
al	O
,	O
2020	O
;	O
.	O
Here	O
is	O
an	O
illustrative	O
example	O
of	O
our	O
generative	O
formulations	O
for	O
the	O
AOPE	O
task	O
:	O
Input	O
:	O
Salads	O
were	O
fantastic	O
,	O
our	O
server	O
was	O
also	O
very	O
helpful	O
.	O
Target	O
(	O
Annotation	O
-	O
style	O
)	O
:	O
[	O
Salads	O
|	O
fantastic	O
]	O
were	O
fantastic	O
here	O
,	O
our	O
[	O
server	O
|	O
helpful	O
]	O
was	O
also	O
very	O
helpful	O
.	O
Target	O
(	O
Extraction	O
-	O
style	O
)	O
:	O
(	O
Salads	O
,	O
fantastic	O
)	O
;	O
(	O
server	O
,	O
helpful	O
)	O
In	O
the	O
annotation	O
-	O
style	O
paradigm	O
,	O
to	O
indicate	O
the	O
pair	O
relations	O
between	O
the	O
aspect	O
and	O
opinion	O
terms	O
,	O
we	O
append	O
the	O
associated	O
opinion	O
modifier	O
to	O
each	O
aspect	O
term	O
in	O
the	O
form	O
of	O
[	O
aspect	O
|	O
opinion	O
]	O
for	O
constructing	O
the	O
target	O
sentence	O
,	O
as	O
shown	O
in	O
the	O
above	O
example	O
.	O
The	O
prediction	O
of	O
the	O
coupled	O
aspect	O
and	O
opinion	O
term	O
is	O
thus	O
achieved	O
by	O
including	O
them	O
in	O
the	O
same	O
bracket	O
.	O
For	O
the	O
extraction	O
-	O
style	O
paradigm	O
,	O
we	O
treat	O
the	O
desired	O
pairs	O
as	O
the	O
target	O
,	O
which	O
resembles	O
direct	O
extraction	O
of	O
the	O
expected	O
sentiment	O
elements	O
but	O
in	O
a	O
generative	O
manner	O
.	O
Unified	O
ABSA	O
(	O
UABSA	O
)	O
is	O
the	O
task	O
of	O
extracting	O
aspect	O
terms	O
and	O
predicting	O
their	O
sentiment	O
polarities	O
at	O
the	O
same	O
time	O
(	O
Li	O
et	O
al	O
,	O
2019a	O
;	O
Chen	O
and	O
Qian	O
,	O
2020	O
)	O
.	O
We	O
also	O
formulate	O
it	O
as	O
an	O
(	O
aspect	O
,	O
sentiment	O
polarity	O
)	O
pair	O
extraction	O
problem	O
.	O
For	O
the	O
same	O
example	O
given	O
above	O
,	O
we	O
aim	O
to	O
extract	O
two	O
pairs	O
:	O
(	O
Salads	O
,	O
positive	O
)	O
and	O
(	O
server	O
,	O
positive	O
)	O
.	O
Similarly	O
,	O
we	O
replace	O
each	O
aspect	O
term	O
as	O
[	O
aspect	O
|	O
sentiment	O
polarity	O
]	O
under	O
the	O
annotation	O
-	O
style	O
formulation	O
and	O
treat	O
the	O
desired	O
pairs	O
as	O
the	O
target	O
output	O
in	O
the	O
extraction	O
-	O
style	O
paradigm	O
to	O
reformulate	O
the	O
UABSA	O
task	O
as	O
a	O
text	B-TaskName
generation	I-TaskName
problem	O
.	O
As	O
shown	O
above	O
,	O
we	O
annotate	O
each	O
aspect	O
term	O
with	O
its	O
corresponding	O
sentiment	O
triplet	O
wrapped	O
in	O
the	O
bracket	O
,	O
i.e.	O
,	O
[	O
aspect	O
|	O
opinion	O
|	O
sentiment	O
polarity	O
]	O
for	O
the	O
annotation	O
-	O
style	O
modeling	O
.	O
Note	O
that	O
we	O
will	O
include	O
all	O
the	O
opinion	O
modifiers	O
of	O
the	O
same	O
aspect	O
term	O
within	O
the	O
same	O
bracket	O
to	O
predict	O
the	O
sentiment	O
polarities	O
more	O
accurately	O
.	O
For	O
the	O
extraction	O
-	O
style	O
paradigm	O
,	O
we	O
just	O
concatenate	O
all	O
triplets	O
as	O
the	O
target	O
output	O
.	O
Target	O
Aspect	O
Sentiment	O
Detection	O
(	O
TASD	O
)	O
is	O
the	O
task	O
to	O
detect	O
all	O
(	O
aspect	O
term	O
,	O
aspect	O
category	O
,	O
sentiment	O
polarity	O
)	O
triplets	O
for	O
a	O
given	O
sentence	O
(	O
Wan	O
et	O
al	O
,	O
2020	O
)	O
,	O
where	O
the	O
aspect	O
category	O
belongs	O
to	O
a	O
pre	O
-	O
defined	O
category	O
set	O
.	O
For	O
example	O
,	O
Input	O
:	O
A	O
big	O
disappointment	O
,	O
all	O
around	O
.	O
The	O
pizza	O
was	O
cold	O
and	O
the	O
cheese	O
was	O
n't	O
even	O
fully	O
melted	O
.	O
Similarly	O
,	O
we	O
pack	O
each	O
aspect	O
term	O
,	O
the	O
aspect	O
category	O
it	O
belongs	O
to	O
,	O
and	O
its	O
sentiment	O
polarity	O
into	O
a	O
bracket	O
to	O
build	O
the	O
target	O
sentence	O
for	O
the	O
annotation	O
-	O
style	O
method	O
.	O
Note	O
that	O
we	O
use	O
a	O
bigram	O
expression	O
for	O
the	O
aspect	O
category	O
instead	O
of	O
the	O
original	O
uppercase	O
form	O
"	O
FOOD#QUALITY	O
"	O
to	O
make	O
the	O
annotated	O
target	O
sentence	O
more	O
natural	O
.	O
As	O
presented	O
in	O
the	O
example	O
,	O
some	O
triplets	O
may	O
not	O
have	O
explicitly	O
-	O
mentioned	O
aspect	O
terms	O
,	O
we	O
thus	O
use	O
"	O
null	O
"	O
to	O
represent	O
it	O
and	O
put	O
such	O
triplets	O
at	O
the	O
end	O
of	O
the	O
target	O
output	O
.	O
For	O
the	O
extraction	O
-	O
style	O
paradigm	O
,	O
we	O
concatenate	O
all	O
the	O
desired	O
triplets	O
,	O
including	O
those	O
with	O
implicit	O
aspect	O
terms	O
,	O
as	O
the	O
target	O
sentence	O
for	O
sequence	O
-	O
to	O
-	O
sequence	O
learning	O
.	O

Given	O
the	O
input	O
sentence	O
x	O
,	O
we	O
generate	O
a	O
target	O
sequence	O
y	O
,	O
which	O
is	O
either	O
based	O
on	O
the	O
annotationstyle	O
or	O
extraction	O
-	O
style	O
paradigm	O
as	O
described	O
in	O
the	O
last	O
section	O
,	O
with	O
a	O
text	B-TaskName
generation	I-TaskName
model	O
f	O
(	O
)	O
.	O
Then	O
the	O
desired	O
sentiment	O
pairs	O
or	O
triplets	O
s	O
can	O
be	O
decoded	O
from	O
the	O
generated	O
sequence	O
y	O
.	O
Specifically	O
,	O
for	O
the	O
annotation	O
-	O
style	O
modeling	O
,	O
we	O
extract	O
the	O
contents	O
included	O
in	O
the	O
bracket	O
"	O
[	O
]	O
"	O
from	O
y	O
,	O
and	O
separate	O
different	O
sentiment	O
elements	O
with	O
the	O
vertical	O
bar	O
"	O
|	O
"	O
.	O
If	O
such	O
decoding	O
fails	O
,	O
e.g.	O
,	O
we	O
can	O
not	O
find	O
any	O
bracket	O
in	O
the	O
output	O
sentence	O
or	O
the	O
number	O
of	O
vertical	O
bars	O
is	O
not	O
as	O
expected	O
,	O
we	O
ignore	O
such	O
predictions	O
.	O
For	O
the	O
extractionstyle	O
paradigm	O
,	O
we	O
separate	O
the	O
generated	O
pairs	O
or	O
triplets	O
from	O
the	O
sequence	O
y	O
and	O
ignore	O
those	O
invalid	O
generations	O
in	O
a	O
similar	O
way	O
.	O
We	O
adopt	O
the	O
pre	O
-	O
trained	O
T5	B-MethodName
model	O
(	O
Raffel	O
et	O
al	O
,	O
2020	O
)	O
as	O
the	O
generation	O
model	O
f	O
(	O
)	O
,	O
which	O
closely	O
follows	O
the	O
encoder	O
-	O
decoder	O
architecture	O
of	O
the	O
original	O
Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
Therefore	O
,	O
by	O
formulating	O
these	O
ABSA	O
tasks	O
as	O
a	O
text	B-TaskName
generation	I-TaskName
problem	O
,	O
we	O
can	O
tackle	O
them	O
in	O
a	O
unified	O
sequence	O
-	O
to	O
-	O
sequence	O
framework	O
without	O
taskspecific	O
model	O
design	O
.	O

We	O
adopt	O
F1	B-MetricName
scores	O
as	O
the	O
main	O
evaluation	O
metrics	O
for	O
all	O
tasks	O
.	O
A	O
prediction	O
is	O
correct	O
if	O
and	O
only	O
if	O
all	O
its	O
predicted	O
sentiment	O
elements	O
in	O
the	O
pair	O
or	O
triplet	O
are	O
correct	O
.	O

We	O
adopt	O
the	O
T5	B-MethodName
base	O
model	O
from	O
huggingface	O
Transformer	B-MethodName
library	O
2	O
for	O
2	O
https://github.com/huggingface/	O
transformers	O

To	O
better	O
understand	O
the	O
effectiveness	O
of	O
the	O
proposed	O
prediction	O
normalization	O
strategy	O
,	O
we	O
randomly	O
sample	O
some	O
instances	O
from	O
the	O
ASTE	O
task	O
that	O
have	O
different	O
raw	O
prediction	O
and	O
normalized	O
prediction	O
(	O
i.e.	O
,	O
corrected	O
by	O
our	O
strategy	O
)	O
.	O
The	O
predicted	O
sentiment	O
elements	O
before	O
and	O
after	O
the	O
normalization	O
,	O
as	O
well	O
as	O
the	O
gold	O
label	O
of	O
some	O
example	O
cases	O
are	O
shown	O
in	O
Table	O
5	O
.	O
We	O
find	O
that	O
the	O
normalization	O
mainly	O
helps	O
on	O
two	O
occasions	O
:	O
The	O
first	O
one	O
is	O
the	O
morphology	O
shift	O
where	O
two	O
words	O
have	O
minor	O
lexical	O
differences	O
.	O
For	O
example	O
,	O
the	O
method	O
fixes	O
"	O
Bbq	B-DatasetName
rib	O
"	O
to	O
"	O
BBQ	B-DatasetName
rib	O
"	O
(	O
#	O
1	O
)	O
and	O
"	O
repeat	O
"	O
to	O
"	O
repeats	O
"	O
(	O
#	O
2	O
)	O
.	O
Another	O
case	O
is	O
orthographic	O
alternatives	O
where	O
the	O
model	O
might	O
generate	O
words	O
with	O
the	O
same	O
etyma	O
but	O
different	O
word	O
types	O
,	O
e.g.	O
,	O
it	O
outputs	O
"	O
vegetarian	O
"	O
rather	O
than	O
"	O
vegan	O
"	O
(	O
#	O
6	O
)	O
.	O
Our	O
proposed	O
prediction	O
normalization	O
,	O
which	O
finds	O
the	O
replacement	O
from	O
the	O
corresponding	O
vocabulary	O
set	O
via	O
Levenshtein	O
distance	O
,	O
is	O
a	O
simple	O
yet	O
effective	O
strategy	O
to	O
alleviate	O
this	O
issue	O
.	O
We	O
also	O
observe	O
that	O
our	O
prediction	O
strategy	O
may	O
fail	O
if	O
the	O
raw	O
predictions	O
are	O
quite	O
lexically	O
different	O
or	O
even	O
semantically	O
different	O
from	O
the	O
goldstandard	O
labels	O
(	O
see	O
Case	O
#	O
4	O
,	O
#	O
7	O
and	O
#	O
8	O
)	O
.	O
In	O
these	O
cases	O
,	O
the	O
difficulty	O
does	O
not	O
come	O
from	O
the	O
way	O
of	O
performing	O
prediction	O
normalization	O
but	O
the	O
generation	O
of	O
labels	O
close	O
to	O
the	O
ground	O
truths	O
,	O
especially	O
for	O
the	O
examples	O
containing	O
implicit	O
aspects	O
or	O
opinions	O
(	O
Case	O
#	O
4	O
)	O
.	O

We	O
tackle	O
various	O
ABSA	O
tasks	O
in	O
a	O
novel	O
generative	O
framework	O
in	O
this	O
paper	O
.	O
By	O
formulating	O
the	O
target	O
sentences	O
with	O
our	O
proposed	O
annotation	O
-	O
style	O
and	O
extraction	O
-	O
style	O
paradigms	O
,	O
we	O
solve	O
multiple	O
sentiment	O
pair	O
or	O
triplet	O
extraction	O
tasks	O
with	O
a	O
unified	O
generation	O
model	O
.	O
Extensive	O
experiments	O
on	O
multiple	O
benchmarks	O
across	O
four	O
ABSA	O
tasks	O
show	O
the	O
effectiveness	O
of	O
our	O
proposed	O
method	O
.	O
Our	O
work	O
is	O
an	O
initial	O
attempt	O
on	O
transforming	O
ABSA	O
tasks	O
,	O
which	O
are	O
typically	O
treated	O
as	O
classification	O
problems	O
,	O
into	O
text	B-TaskName
generation	I-TaskName
problems	O
.	O
Experimental	O
results	O
indicate	O
that	O
such	O
transformation	O
is	O
an	O
effective	O
solution	O
to	O
tackle	O
various	O
ABSA	O
tasks	O
.	O
Following	O
this	O
direction	O
,	O
designing	O
more	O
effective	O
generation	O
paradigms	O
and	O
extending	O
such	O
ideas	O
to	O
other	O
tasks	O
can	O
be	O
interesting	O
research	O
problems	O
for	O
future	O
work	O
.	O

Role	O
-	O
Playing	O
in	O
Open	O
-	O
Domain	O
Dialogue	O
Much	O
recent	O
work	O
has	O
explored	O
training	O
open	O
-	O
domain	O
dialogue	O
models	O
on	O
large	O
and	O
small	O
dialogue	O
corpora	O
,	O
with	O
the	O
former	O
imbuing	O
raw	O
conversational	O
ability	O
and	O
the	O
latter	O
providing	O
necessary	O
conversational	O
skills	O
.	O
Most	O
crowd	O
-	O
sourced	O
datasets	O
require	O
acting	O
out	O
a	O
role	O
to	O
some	O
capacity	O
in	O
conversation	O
(	O
though	O
indeed	O
Mazaré	O
et	O
al	O
(	O
2018	O
)	O
study	O
extraction	O
of	O
roles	O
from	O
raw	O
data	O
)	O
.	O
Some	O
involve	O
providing	O
persona	O
lines	O
that	O
a	O
model	O
must	O
assume	O
throughout	O
the	O
conversation	O
(	O
Zhang	O
et	O
al	O
,	O
2018	O
;	O
;	O
others	O
require	O
more	O
subtle	O
"	O
roles	O
"	O
,	O
such	O
as	O
a	O
listener	O
(	O
Rashkin	O
et	O
al	O
,	O
2019	O
)	O
,	O
or	O
a	O
teacher	O
and	O
student	O
(	O
Dinan	O
et	O
al	O
,	O
2019b	O
;	O
Gopalakrishnan	O
et	O
al	O
,	O
2019	O
;	O
Zhou	O
et	O
al	O
,	O
2018	O
;	O
Komeili	O
et	O
al	O
,	O
2021	O
)	O
.	O
Zheng	O
et	O
al	O
(	O
2020	O
)	O
explore	O
using	O
a	O
discriminative	O
model	O
to	O
predict	O
whether	O
model	O
responses	O
contain	O
similarity	O
with	O
their	O
persona	O
,	O
similar	O
to	O
methods	O
we	O
employ	O
in	O
our	O
work	O
.	O
Consistency	O
in	O
Open	O
-	O
Domain	O
Dialogue	O
A	O
common	O
paradigm	O
in	O
the	O
state	O
of	O
the	O
art	O
of	O
open	O
-	O
domain	O
dialogue	O
involves	O
concatenating	O
all	O
relevant	O
contextual	O
information	O
as	O
input	O
to	O
a	O
sequence	B-MethodName
to	I-MethodName
sequence	I-MethodName
neural	O
model	O
(	O
e.g.	O
,	O
transformers	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
)	O
to	O
obtain	O
a	O
conditioned	O
response	O
.	O
Such	O
models	O
can	O
yield	O
human	O
-	O
like	O
and	O
engaging	O
responses	O
(	O
Adiwardana	O
et	O
al	O
,	O
2020	O
;	O
Roller	O
et	O
al	O
,	O
2021	O
)	O
.	O
Nevertheless	O
,	O
various	O
consistency	O
issues	O
still	O
plague	O
such	O
models	O
.	O
Recent	O
studies	O
have	O
indicated	O
that	O
hallucination	O
of	O
incorrect	O
knowledge	O
is	O
still	O
far	O
from	O
a	O
solved	O
issue	O
Santhanam	O
et	O
al	O
,	O
2021	O
)	O
,	O
with	O
some	O
proposing	O
specific	O
datasets	O
and	O
tools	O
for	O
measuring	O
precisely	O
the	O
levels	O
of	O
this	O
undesired	O
attribute	O
.	O
Another	O
clear	O
example	O
of	O
failure	O
is	O
the	O
short	O
-	O
term	O
memory	O
of	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
,	O
sometimes	O
due	O
to	O
the	O
lack	O
of	O
long	O
-	O
form	O
training	O
data	O
or	O
long	O
-	O
context	O
models	O
but	O
often	O
due	O
to	O
simply	O
the	O
modeling	O
itself	O
.	O
To	O
address	O
consistency	O
issues	O
,	O
a	O
variety	O
of	O
methods	O
have	O
been	O
explored	O
.	O
In	O
the	O
context	O
of	O
knowledge	O
-	O
grounded	O
dialogue	O
,	O
different	O
ways	O
to	O
attend	O
most	O
effectively	O
over	O
provided	O
contextual	O
information	O
have	O
been	O
explored	O
(	O
Zheng	O
and	O
Zhou	O
,	O
2019	O
;	O
Ye	O
et	O
al	O
,	O
2020	O
;	O
Prabhumoye	O
et	O
al	O
,	O
2021	O
;	O
Wang	O
et	O
al	O
,	O
2019	O
)	O
.	O
These	O
works	O
find	O
that	O
considering	O
factual	O
documents	O
separately	O
(	O
in	O
some	O
capacity	O
)	O
improves	O
model	O
grounding	O
.	O
We	O
explore	O
such	O
methods	O
,	O
but	O
in	O
the	O
context	O
of	O
character	O
identity	O
.	O
Another	O
general	O
problem	O
is	O
that	O
of	O
contradictions	O
.	O
Nie	O
et	O
al	O
(	O
2021	O
)	O
collect	O
a	O
dataset	O
of	O
contradictions	O
in	O
dialogue	O
,	O
and	O
train	O
classifiers	O
that	O
help	O
re	O
-	O
rank	O
model	O
outputs	O
at	O
inference	O
time	O
;	O
explore	O
unlikelihood	O
training	O
to	O
reduce	O
repetition	O
and	O
contradiction	O
,	O
among	O
other	O
undesired	O
traits	O
,	O
in	O
model	O
generations	O
.	O
The	O
character	O
identity	O
issue	O
we	O
study	O
in	O
this	O
work	O
can	O
be	O
seen	O
as	O
an	O
important	O
class	O
of	O
contradictions	O
,	O
but	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
has	O
not	O
been	O
explicitly	O
focused	O
on	O
.	O

We	O
consider	O
a	O
two	O
-	O
party	O
chat	O
setting	O
.	O
The	O
context	O
provided	O
to	O
a	O
model	O
includes	O
:	O
(	O
i	O
)	O
the	O
name	O
of	O
its	O
character	O
and	O
the	O
partner	O
's	O
character	O
;	O
(	O
ii	O
)	O
an	O
extended	O
description	O
of	O
its	O
own	O
character	O
;	O
(	O
iii	O
)	O
and	O
,	O
information	O
about	O
the	O
area	O
in	O
which	O
the	O
conversation	O
takes	O
place	O
.	O
The	O
responsibility	O
of	O
the	O
model	O
is	O
to	O
engage	O
its	O
conversational	O
partner	O
,	O
with	O
no	O
other	O
goal	O
prescribed	O
;	O
however	O
,	O
it	O
should	O
stay	O
within	O
character	O
and	O
within	O
the	O
bounds	O
of	O
the	O
defined	O
setting	O
.	O
We	O
operate	O
in	O
the	O
context	O
of	O
LIGHT	O
(	O
Urbanek	O
et	O
al	O
,	O
2019	O
)	O
,	O
consisting	O
of	O
grounded	O
fantasy	O
roleplaying	O
game	O
conversations	O
.	O
The	O
LIGHT	O
environment	O
involves	O
humans	O
and	O
models	O
interacting	O
with	O
thousands	O
of	O
objects	O
in	O
hundreds	O
of	O
locations	O
,	O
all	O
while	O
assuming	O
the	O
roles	O
of	O
one	O
of	O
hundreds	O
of	O
characters	O
.	O
The	O
dataset	O
consists	O
of	O
roughly	O
8.5k	O
dialogues	O
spanning	O
111k	O
utterances	O
.	O
It	O
is	O
an	O
ideal	O
setting	O
for	O
this	O
study	O
because	O
of	O
the	O
rich	O
and	O
varied	O
personas	O
with	O
explicit	O
backstories	O
.	O
To	O
quantify	O
the	O
character	O
identity	O
problem	O
,	O
we	O
take	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
dialogue	O
agent	B-DatasetName
(	O
specifically	O
,	O
BlenderBot	O
(	O
Roller	O
et	O
al	O
,	O
2021	O
)	O
)	O
fine	O
-	O
tuned	O
on	O
the	O
LIGHT	O
dialogue	O
dataset	O
and	O
ask	O
human	O
annotators	O
if	O
the	O
agent	B-DatasetName
mistakes	O
its	O
identity	O
based	O
on	O
its	O
utterances	O
in	O
context	O
.	O
The	O
agent	B-DatasetName
conditions	O
its	O
response	O
on	O
the	O
LIGHT	O
context	O
and	O
prior	O
utterances	O
in	O
the	O
dialogue	O
history	O
.	O
We	O
see	O
in	O
Table	O
4	O
that	O
in	O
roughly	O
6.5	O
percent	O
of	O
utterances	O
the	O
model	O
mistakes	O
its	O
identity	O
;	O
this	O
corresponds	O
to	O
a	O
mistake	O
in	O
approximately	O
35	O
percent	O
of	O
conversations	O
.	O
BlenderBot	O
uses	O
a	O
Byte	O
-	O
Level	O
BPE	B-MethodName
tokenizer	O
(	O
Radford	O
et	O
al	O
,	O
2019	O
)	O
;	O
an	O
artifact	O
from	O
the	O
Blender	B-MethodName
-	O
Bot	O
pre	O
-	O
training	O
is	O
that	O
it	O
only	O
considers	O
128	O
such	O
tokens	O
in	O
the	O
past	O
,	O
and	O
thus	O
has	O
no	O
mechanism	O
for	O
recovering	O
truncated	O
information	O
about	O
the	O
LIGHT	O
context	O
in	O
later	O
conversational	O
turns	O
.	O
Our	O
second	O
baseline	O
lengthens	O
the	O
input	O
context	O
to	O
1024	O
BPE	B-MethodName
tokens	O
,	O
which	O
allows	O
the	O
entire	O
context	O
for	O
every	O
example	O
to	O
fit	O
into	O
the	O
truncation	O
length	O
of	O
the	O
model	O
;	O
we	O
follow	O
methods	O
employed	O
in	O
to	O
extend	O
the	O
positional	O
embeddings	O
of	O
the	O
model	O
.	O
We	O
see	O
in	O
Table	O
that	O
this	O
actually	O
makes	O
the	O
problem	O
worse	O
,	O
resulting	O
in	O
7.4	O
percent	O
of	O
utterances	O
with	O
mistaken	O
identity	O
(	O
corresponding	O
to	O
a	O
failure	O
in	O
approximately	O
38	O
percent	O
of	O
conversations	O
)	O
.	O

In	O
this	O
section	O
we	O
describe	O
several	O
strategies	O
for	O
improving	O
the	O
role	O
-	O
playing	O
accuracy	B-MetricName
of	O
dialogue	O
agents	O
,	O
specifically	O
ways	O
to	O
improve	O
our	O
transformer	O
baselines	O
.	O

We	O
explore	O
utilizing	O
an	O
unlikelihood	O
(	O
UL	O
)	O
loss	B-MetricName
While	O
training	O
on	O
the	O
LIGHT	O
dataset	O
with	O
standard	O
NLL	B-MetricName
loss	B-MetricName
,	O
with	O
some	O
fixed	O
probability	O
we	O
consider	O
a	O
candidate	O
model	O
generation	O
for	O
UL	O
loss	B-MetricName
.	O
The	O
full	O
generation	O
is	O
sent	O
to	O
the	O
RPA	O
classifier	O
;	O
if	O
the	O
generation	O
is	O
classified	O
as	O
coming	O
from	O
the	O
incorrect	O
character	O
,	O
we	O
examine	O
each	O
partial	O
generated	O
sequence	O
of	O
the	O
output	O
,	O
and	O
send	O
these	O
sequences	O
to	O
the	O
LTR	O
RPA	O
classifier	O
to	O
determine	O
whether	O
the	O
candidate	O
partial	O
sequences	O
match	O
the	O
model	O
's	O
character	O
.	O
We	O
apply	O
UL	O
loss	B-MetricName
to	O
tokens	O
that	O
yield	O
the	O
wrong	O
character	O
classification	O
.	O

The	O
RPA	O
classifiers	O
utilize	O
the	O
LIGHT	O
setting	O
and	O
prior	O
utterances	O
of	O
dialogue	O
history	O
to	O
determine	O
which	O
character	O
generates	O
a	O
candidate	O
response	O
.	O
We	O
hypothesize	O
that	O
the	O
generation	O
models	O
themselves	O
should	O
be	O
able	O
to	O
pick	O
out	O
and	O
utilize	O
these	O
components	O
as	O
well	O
.	O
However	O
,	O
the	O
RPA	O
classifier	O
models	O
are	O
trained	O
explicitly	O
for	O
this	O
task	O
,	O
whereas	O
the	O
seq2seq	B-MethodName
models	O
are	O
trained	O
only	O
to	O
generate	O
a	O
plausible	O
continuation	O
of	O
a	O
dialogue	O
history	O
.	O
We	O
thus	O
explore	O
a	O
setup	O
in	O
which	O
the	O
generation	O
models	O
are	O
trained	O
to	O
identify	O
the	O
speaker	O
of	O
an	O
utterance	O
as	O
well	O
.	O
To	O
do	O
this	O
,	O
we	O
use	O
the	O
output	O
representations	O
from	O
the	O
model	O
(	O
either	O
encoder	O
+	O
decoder	O
,	O
or	O
decoder	O
only	O
)	O
as	O
inputs	O
to	O
n	O
M	O
O	O
additional	O
transformer	O
layers	O
,	O
where	O
we	O
vary	O
n	O
M	O
O	O
{	O
0	B-DatasetName
,	O
2	O
}	O
.	O
The	O
final	O
outputs	O
are	O
used	O
to	O
compute	O
a	O
character	O
score	O
,	O
similarly	O
to	O
the	O
RPA	O
classifier	O
.	O
The	O
model	O
can	O
then	O
be	O
trained	O
piece	O
-	O
wise	O
.	O
After	O
initializing	O
the	O
model	O
weights	O
with	O
those	O
trained	O
on	O
the	O
LIGHT	O
response	B-TaskName
generation	I-TaskName
task	O
,	O
we	O
then	O
train	O
only	O
the	O
extra	O
layers	O
with	O
only	O
the	O
character	O
classification	O
objective	O
;	O
once	O
the	O
classifier	O
achieves	O
suitable	O
performance	O
on	O
the	O
task	O
,	O
we	O
can	O
begin	O
to	O
back	O
-	O
propagate	O
the	O
character	O
classification	O
objective	O
multi	O
-	O
tasking	O
with	O
the	O
dialogue	O
task	O
itself	O
to	O
the	O
generation	O
model	O
directly	O
,	O
in	O
the	O
hope	O
that	O
the	O
model	O
learns	O
to	O
update	O
its	O
internal	O
representations	O
of	O
the	O
context	O
and/or	O
the	O
decoded	O
response	O
.	O

Maintaining	O
identity	O
relies	O
on	O
the	O
model	O
's	O
capacity	O
to	O
understand	O
which	O
inputs	O
from	O
the	O
conversational	O
history	O
are	O
pertinent	O
when	O
generating	O
a	O
continuation	O
of	O
the	O
preceding	O
dialogue	O
.	O
In	O
a	O
standard	O
,	O
opendomain	O
chit	O
-	O
chat	O
scenario	O
,	O
the	O
model	O
has	O
free	O
reign	O
to	O
decide	O
which	O
elements	O
of	O
the	O
context	O
it	O
would	O
like	O
to	O
condition	O
on	O
when	O
generating	O
a	O
response	O
,	O
as	O
we	O
are	O
dealing	O
with	O
a	O
nearly	O
unconstrained	O
output	O
space	O
(	O
so	O
long	O
as	O
the	O
output	O
follows	O
plausibly	O
from	O
the	O
input	O
)	O
.	O
In	O
LIGHT	O
,	O
however	O
,	O
we	O
want	O
to	O
emphasize	O
certain	O
components	O
of	O
the	O
context	O
more	O
so	O
than	O
others	O
;	O
specifically	O
,	O
when	O
role	O
-	O
playing	O
as	O
a	O
character	O
,	O
we	O
want	O
the	O
model	O
to	O
always	O
be	O
reminded	O
of	O
its	O
role	O
,	O
so	O
that	O
it	O
can	O
conditionally	O
generate	O
an	O
optimal	O
response	O
while	O
staying	O
in	O
character	O
.	O
In	O
this	O
lens	O
,	O
one	O
can	O
view	O
the	O
task	O
as	O
"	O
grounding	O
"	O
on	O
one	O
's	O
character	O
information	O
when	O
conversing	O
.	O
Profile	O
Grounding	O
Inspired	B-DatasetName
by	O
models	O
demonstrating	O
good	O
performance	O
in	O
knowledge	O
-	O
grounded	O
dialogue	O
(	O
Zheng	O
and	O
Zhou	O
,	O
2019	O
;	O
Ye	O
et	O
al	O
,	O
2020	O
;	O
Prabhumoye	O
et	O
al	O
,	O
2021	O
;	O
Wang	O
et	O
al	O
,	O
2019	O
)	O
,	O
we	O
propose	O
a	O
simple	O
extension	O
to	O
the	O
transformer	O
seq2seq	B-MethodName
architecture	O
,	O
specifically	O
the	O
decoder	O
,	O
to	O
ensure	O
the	O
model	O
knows	O
to	O
condition	O
on	O
the	O
pro	O
-	O
file	O
.	O
The	O
standard	O
transformer	B-MethodName
decoder	I-MethodName
first	O
uses	O
self	O
-	O
attention	O
over	O
the	O
decoded	O
response	O
,	O
and	O
then	O
cross	O
-	O
attention	O
over	O
the	O
encoder	O
outputs	O
.	O
We	O
add	O
a	O
third	O
attention	O
step	O
,	O
expanded	O
attention	O
,	O
that	O
attends	O
again	O
over	O
an	O
extracted	O
subset	O
of	O
the	O
input	O
context	O
(	O
encoded	O
separately	O
from	O
the	O
normal	O
context	O
)	O
.	O
We	O
explore	O
various	O
subsets	O
of	O
the	O
context	O
to	O
determine	O
which	O
are	O
most	O
important	O
for	O
both	O
RPA	O
and	O
other	O
automated	O
metrics	O
,	O
and	O
call	O
this	O
method	O
"	O
Profile	O
"	O
grounding	O
as	O
the	O
subsets	O
generally	O
include	O
the	O
character	O
and	O
role	O
description	O
.	O
We	O
utilize	O
the	O
exact	O
same	O
(	O
shared	O
)	O
parameters	O
for	O
both	O
the	O
normal	O
cross	O
-	O
attention	O
and	O
the	O
expanded	O
attention	O
;	O
thus	O
,	O
model	O
size	O
is	O
not	O
affected	O
.	O
Automated	O
Grounding	O
Instead	O
of	O
directly	O
telling	O
the	O
model	O
what	O
to	O
re	O
-	O
attend	O
to	O
,	O
we	O
also	O
explore	O
whether	O
the	O
model	O
can	O
learn	O
to	O
do	O
this	O
automatically	O
,	O
based	O
on	O
its	O
own	O
(	O
or	O
other	O
)	O
representations	O
of	O
the	O
context	O
.	O
The	O
first	O
method	O
we	O
consider	O
is	O
examining	O
the	O
decoder	O
attention	O
weights	O
.	O
Specifically	O
,	O
we	O
use	O
the	O
attention	O
weights	O
from	O
the	O
decoder	O
over	O
the	O
full	O
context	O
to	O
choose	O
k	O
tokens	O
to	O
re	O
-	O
attend	O
to	O
.	O
This	O
operation	O
is	O
done	O
on	O
a	O
per	O
-	O
layer	O
basis	O
,	O
and	O
thus	O
allows	O
different	O
decoder	O
layers	O
to	O
re	O
-	O
attend	O
to	O
(	O
potentially	O
different	O
)	O
components	O
of	O
the	O
input	O
.	O
The	O
second	O
method	O
we	O
consider	O
is	O
a	O
trainable	O
mask	O
;	O
this	O
involves	O
feeding	O
the	O
encoded	O
context	O
through	O
a	O
"	O
mask	O
"	O
layer	O
to	O
select	O
various	O
tokens	O
to	O
re	O
-	O
attend	O
to	O
.	O
Specifically	O
,	O
we	O
feed	O
the	O
context	O
through	O
a	O
linear	O
projection	O
layer	O
followed	O
by	O
a	O
softmax	B-MethodName
to	O
select	O
the	O
top	O
-	O
k	O
tokens	O
.	O
This	O
set	O
of	O
tokens	O
is	O
then	O
re	O
-	O
encoded	O
by	O
the	O
encoder	O
and	O
fed	O
to	O
the	O
decoder	O
as	O
the	O
expanded	O
attention	O
context	O
.	O
Finally	O
,	O
we	O
explore	O
using	O
the	O
classifier	O
attention	O
weights	O
over	O
the	O
context	O
from	O
the	O
RPA	O
classifier	O
itself	O
.	O
Intuitively	O
,	O
the	O
RPA	O
classifier	O
has	O
learned	O
what	O
components	O
of	O
the	O
input	O
are	O
necessary	O
for	O
determining	O
which	O
character	O
is	O
speaking	O
;	O
if	O
we	O
look	O
at	O
these	O
attention	O
weights	O
when	O
considering	O
the	O
model	O
's	O
character	O
,	O
we	O
know	O
what	O
the	O
classifier	O
thinks	O
is	O
important	O
to	O
use	O
.	O

We	O
first	O
assess	O
the	O
quality	O
of	O
our	O
RPA	O
classifiers	O
.	O
We	O
measure	O
hits@1/427	O
,	O
where	O
the	O
model	O
must	O
correctly	O
identify	O
the	O
character	O
speaking	O
out	O
of	O
427	O
characters	O
from	O
the	O
validation	O
set	O
,	O
comparing	O
the	O
standard	O
and	O
left	O
-	O
to	O
-	O
right	O
(	O
LTR	O
)	O
models	O
in	O
Table	O
2	O
.	O
We	O
experiment	O
with	O
either	O
0	B-DatasetName
,	O
4	O
,	O
or	O
All	O
prior	O
context	O
utterances	O
.	O
The	O
LTR	O
classifiers	O
perform	O
nearly	O
as	O
well	O
as	O
the	O
full	O
classifiers	O
on	O
the	O
full	O
datasplit	O
,	O
and	O
outperform	O
them	O
on	O
the	O
LTR	O
split	O
.	O
Given	O
the	O
robustness	O
of	O
the	O
LTR	O
RPA	O
classifiers	O
,	O
we	O
use	O
this	O
model	O
for	O
computing	O
RPA	O
throughout	O
the	O
remaining	O
results	O
,	O
unless	O
otherwise	O
specified	O
.	O
Further	O
results	O
are	O
given	O
in	O
Appendix	O
Table	O
10	O
.	O

We	O
next	O
train	O
baseline	O
models	O
for	O
the	O
dialogue	B-TaskName
generation	I-TaskName
task	O
itself	O
.	O
Performance	O
on	O
the	O
LIGHT	O
dataset	O
test	O
split	O
for	O
our	O
baseline	O
models	O
can	O
be	O
found	O
in	O
detailed	O
training	O
and	O
optimization	O
specifications	O
are	O
given	O
in	O
Appendix	O
A.	O

All	O
models	O
are	O
trained	O
with	O
the	O
ParlAI	O
2	O
framework	O
(	O
Miller	O
et	O
al	O
,	O
2017	O
)	O
.	O
Due	O
to	O
the	O
large	O
number	O
of	O
experimental	O
setups	O
and	O
computational	O
cost	O
,	O
we	O
do	O
not	O
consider	O
multiple	O
training	O
runs	O
.	O
Base	O
Models	O
RPA	O
classifier	O
Poly	O
-	O
encoders	O
are	O
initialized	O
with	O
the	O
622	O
M	O
parameter	O
models	O
from	O
Roller	O
et	O
al	O
(	O
2021	O
)	O
;	O
we	O
also	O
use	O
this	O
architecture	O
for	O
dialogue	O
response	O
(	O
retrieval	O
)	O
models	O
which	O
we	O
also	O
evaluate	O
(	O
see	O
Table	O
19	O
)	O
.	O
All	O
generative	O
models	O
are	O
initialized	O
with	O
BlenderBot	O
,	O
also	O
from	O
Roller	O
et	O
al	O
(	O
2021	O
)	O
,	O
a	O
2.7B	O
parameter	O
transformer	O
encoder	O
/	O
decoder	O
model	O
.	O
Each	O
model	O
was	O
pre	O
-	O
trained	O
on	O
1.5B	O
training	O
examples	O
from	O
pushshift.io	O
Reddit	B-DatasetName
(	O
Baumgartner	O
et	O
al	O
,	O
2020	O
)	O
,	O
with	O
BlenderBot	O
additionally	O
fine	O
-	O
tuned	O
on	O
the	O
BST	O
tasks	O
(	O
see	O
Roller	O
et	O
al	O
(	O
2021	O
)	O
for	O
more	O
details	O
)	O
,	O
before	O
training	O
on	O
LIGHT	O
.	O

Train	O
Valid	O
Test	O
LIGHT	O
(	O
Urbanek	O
et	O
al	O
,	O
nation	O
of	O
(	O
1	O
)	O
the	O
LIGHT	O
context	O
(	O
set	O
of	O
characters	O
,	O
setting	O
,	O
etc	O
.	O
)	O
;	O
(	O
2	O
)	O
a	O
fixed	O
number	O
of	O
previous	O
utterances	O
in	O
the	O
conversation	O
;	O
and	O
(	O
3	O
)	O
a	O
candidate	O
utterance	O
from	O
any	O
point	O
later	O
in	O
the	O
conversation	O
(	O
a	O
special	O
token	O
separates	O
the	O
candidate	O
utterance	O
from	O
the	O
prior	O
context	O
)	O
.	O
We	O
experiment	O
with	O
either	O
0	B-DatasetName
,	O
4	O
,	O
or	O
N	O
−	O
2	O
prior	O
utterances	O
(	O
dubbed	O
"	O
All	O
"	O
in	O
relevant	O
tables	O
)	O
,	O
where	O
N	O
is	O
the	O
total	O
number	O
of	O
utterances	O
(	O
N	O
−	O
2	O
allows	O
the	O
last	O
turn	O
for	O
each	O
speaker	O
to	O
be	O
a	O
candidate	O
utterance	O
)	O
.	O
The	O
left	O
-	O
toright	O
(	O
LTR	O
)	O
data	O
split	O
is	O
built	O
similarly	O
,	O
except	O
each	O
example	O
i	O
becomes	O
w	O
i	O
examples	O
,	O
where	O
w	O
i	O
is	O
the	O
number	O
of	O
tokens	O
in	O
the	O
candidate	O
utterance	O
for	O
example	O
i.	O
Statistics	O
of	O
the	O
training	O
dataset	O
are	O
given	O
in	O
Table	O
8	O
.	O
Suppose	O
we	O
choose	O
n	O
as	O
the	O
number	O
of	O
prior	O
utterances	O
to	O
include	O
in	O
the	O
input	O
,	O
and	O
let	O
us	O
denote	O
D	O
=	O
8538	O
to	O
represent	O
all	O
the	O
dialogues	O
in	O
the	O
LIGHT	O
train	O
split	O
,	O
and	O
U	O
=	O
110877	O
to	O
represent	O
all	O
the	O
utterances	O
in	O
those	O
dialogues	O
.	O
For	O
the	O
RPA	O
classification	O
dataset	O
,	O
each	O
dialogue	O
is	O
presented	O
twice	O
,	O
once	O
from	O
each	O
character	O
's	O
POV	O
.	O
For	O
any	O
value	O
0	B-DatasetName
<	O
n	O
<	O
N	O
−	O
1	O
,	O
we	O
build	O
out	O
several	O
examples	O
from	O
several	O
slices	O
of	O
each	O
conversation	O
.	O
Suppose	O
we	O
have	O
dialogue	O
d	O
i	O
with	O
N	O
utterances	O
{	O
u	O
0	B-DatasetName
,	O
u	O
1	O
,	O
...	O
,	O
u	O
N	O
}	O
.	O
To	O
build	O
the	O
training	O
data	O
from	O
dialogue	O
d	O
i	O
,	O
we	O
select	O
all	O
continuous	O
subsets	O
of	O
n	O
utterances	O
within	O
d	O
i	O
,	O
forming	O
contexts	O
c	O
i	O
=	O
{	O
u	O
i	O
,	O
...	O
,	O
u	O
i+n	O
}	O
∀	O
0	B-DatasetName
≤	O
i	O
≤	O
N	O
−	O
i	O
Then	O
,	O
we	O
look	O
at	O
all	O
N	O
−	O
i	O
utterances	O
following	O
utterance	O
u	O
i+n	O
,	O
and	O
use	O
these	O
as	O
target	O
utterances	O
in	O
the	O
task	O
.	O
The	O
goal	O
of	O
this	O
is	O
to	O
build	O
the	O
model	O
to	O
be	O
robust	O
to	O
dataset	O
artifacts	O
;	O
without	O
this	O
modification	O
,	O
the	O
model	O
could	O
trivially	O
pick	O
out	O
the	O
character	O
just	O
by	O
looking	O
at	O
the	O
number	O
of	O
alternating	O
utterances	O
.	O
These	O
measures	O
force	O
the	O
model	O
to	O
fully	O
understand	O
the	O
task	O
and	O
react	O
accordingly	O
.	O

Dynamic	O
Sentence	O
Boundary	B-TaskName
Detection	I-TaskName
for	O
Simultaneous	O
Translation	B-TaskName

Simultaneous	O
Translation	B-TaskName
is	O
a	O
great	O
challenge	O
in	O
which	O
translation	O
starts	O
before	O
the	O
source	O
sentence	O
finished	O
.	O
Most	O
studies	O
take	O
transcription	O
as	O
input	O
and	O
focus	O
on	O
balancing	O
translation	O
quality	O
and	O
latency	O
for	O
each	O
sentence	O
.	O
However	O
,	O
most	O
ASR	O
systems	O
can	O
not	O
provide	O
accurate	O
sentence	O
boundaries	O
in	O
realtime	O
.	O
Thus	O
it	O
is	O
a	O
key	O
problem	O
to	O
segment	O
sentences	O
for	O
the	O
word	O
streaming	O
before	O
translation	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
method	O
for	O
sentence	O
boundary	B-TaskName
detection	I-TaskName
that	O
takes	O
it	O
as	O
a	O
multi	B-TaskName
-	I-TaskName
class	I-TaskName
classification	I-TaskName
task	O
under	O
the	O
endto	O
-	O
end	O
pre	O
-	O
training	O
framework	O
.	O
Experiments	O
show	O
significant	O
improvements	O
both	O
in	O
terms	O
of	O
translation	O
quality	O
and	O
latency	O
.	O

Recent	O
studies	O
show	O
that	O
the	O
pre	O
-	O
training	O
and	O
finetuning	O
framework	O
achieves	O
significant	O
improvements	O
in	O
various	O
NLP	O
tasks	O
.	O
Generally	O
,	O
a	O
model	O
is	O
first	O
pre	O
-	O
trained	O
on	O
large	O
unlabeled	O
data	O
.	O
After	O
that	O
,	O
on	O
the	O
fine	O
-	O
tuning	O
step	O
,	O
the	O
model	O
is	O
initialized	O
by	O
the	O
parameters	O
obtained	O
by	O
the	O
pre	O
-	O
training	O
step	O
and	O
fine	O
-	O
tuned	O
using	O
labeled	O
data	O
for	O
specific	O
tasks	O
.	O
Devlin	O
et	O
al	O
(	O
2019	O
)	O
proposed	O
a	O
generalized	O
framework	O
BERT	B-MethodName
,	O
to	O
learn	O
language	O
representations	O
based	O
on	O
a	O
deep	O
Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
encoder	O
.	O
Rather	O
than	O
traditionally	O
train	O
a	O
language	O
model	O
from	O
-	O
left	O
-	O
to	O
-	O
right	O
or	O
from	O
-	O
rightto	O
-	O
left	O
,	O
they	O
proposed	O
a	O
masked	O
language	O
model	O
(	O
MLM	B-DatasetName
)	O
that	O
randomly	O
replace	O
some	O
tokens	O
in	O
a	O
sequence	O
by	O
a	O
placeholder	O
(	O
mask	O
)	O
and	O
trained	O
the	O
model	O
to	O
predict	O
the	O
original	O
tokens	O
.	O
They	O
also	O
pre	O
-	O
train	O
the	O
model	O
for	O
the	O
next	O
sentence	O
prediction	O
(	O
NSP	O
)	O
task	O
that	O
is	O
to	O
predict	O
whether	O
a	O
sentence	O
is	O
the	O
subsequent	O
sentence	O
of	O
the	O
first	O
sentence	O
.	O
Sun	O
et	O
al	O
(	O
2019	O
)	O
proposed	O
a	O
pre	O
-	O
training	O
framework	O
ERNIE	O
,	O
by	O
integrating	O
more	O
knowledge	O
.	O
Rather	O
than	O
masking	O
single	O
tokens	O
,	O
they	O
proposed	O
to	O
mask	O
a	O
group	O
of	O
words	O
on	O
different	O
levels	O
,	O
such	O
as	O
entities	O
,	O
phrases	O
,	O
etc	O
.	O
The	O
model	O
achieves	O
state	O
-	O
of	O
-	O
theart	O
performances	O
on	O
many	O
NLP	O
tasks	O
.	O
In	O
this	O
paper	O
,	O
we	O
train	O
our	O
model	O
under	O
the	O
ERNIE	O
framework	O
.	O

For	O
a	O
streaming	O
input	O
x	O
=	O
{	O
x	O
1	O
,	O
...	O
,	O
x	O
t	O
}	O
,	O
our	O
goal	O
is	O
to	O
detect	O
whether	O
there	O
is	O
a	O
sentence	O
boundary	O
till	O
the	O
current	O
word	O
x	O
t	O
from	O
last	O
sentence	O
boundary	O
.	O
Rather	O
than	O
a	O
binary	O
classification	O
that	O
detects	O
whether	O
x	O
t	O
is	O
a	O
sentence	O
boundary	O
,	O
we	O
propose	O
a	O
multi	O
-	O
class	O
method	O
.	O
The	O
classes	O
are	O
as	O
follows	O
:	O
0	B-DatasetName
1	O
2	O
…	O
−2	O
−1	O
…	O
ℎ	O
ℎ	O
0	B-DatasetName
1	O
2	O
…	O
−2	O
−1	O
…	O
…	O
…	O
ϕ	O
0	B-DatasetName
−	O
1	O
−	O
2	O
Classes	O
Masked	O
Language	O
Model	O
…	O
ℎ	O
ℎ	O
…	O
ℎ	O
ℎ	O
"	O
.	O
"	O
…	O
ℎ	O
ℎ	O
"	O
.	O
"	O
…	O
ℎ	O
"	O
.	O
"	O
ℎ	O
y	O
=	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
φ	O
,	O
no	O
sentence	O
boundary	O
detected	O
0	B-DatasetName
,	O
x	O
t	O
is	O
the	O
end	O
of	O
a	O
sentence	O
−1	O
,	O
x	O
t−1	O
is	O
the	O
end	O
of	O
a	O
sentence	O
...	O
−M	O
,	O
x	O
t−M	O
is	O
the	O
end	O
of	O
a	O
sentence	O
where	O
M	O
is	O
the	O
maximum	O
offset	O
size	O
to	O
the	O
current	O
state	O
.	O
Thus	O
,	O
we	O
have	O
M	O
+	O
2	O
classes	O
.	O
See	O
Figure	O
2	O
for	O
illustration	O
.	O
We	O
set	O
M	O
=	O
2	O
,	O
indicating	O
that	O
the	O
model	O
predicts	O
4	O
classes	O
for	O
the	O
input	O
stream	O
.	O
If	O
the	O
output	O
class	O
is	O
φ	O
,	O
meaning	O
that	O
the	O
model	O
does	O
not	O
detect	O
any	O
sentence	O
boundary	O
.	O
Thus	O
the	O
model	O
will	O
continue	O
receiving	O
new	O
words	O
.	O
If	O
the	O
output	O
class	O
is	O
0	B-DatasetName
,	O
indicating	O
that	O
the	O
current	O
word	O
x	O
t	O
is	O
the	O
end	O
of	O
a	O
sentence	O
and	O
we	O
put	O
a	O
period	O
after	O
the	O
word	O
.	O
Similarly	O
,	O
class	O
−m	O
denotes	O
to	O
add	O
a	O
sentence	O
boundary	O
after	O
x	O
t−m	O
.	O
While	O
a	O
sentence	O
boundary	O
is	O
detected	O
,	O
the	O
sentence	O
will	O
be	O
extracted	O
from	O
the	O
stream	O
and	O
sent	O
to	O
the	O
MT	O
system	O
as	O
an	O
input	O
for	O
translation	O
.	O
The	O
sentence	O
detection	O
then	O
continues	O
from	O
x	O
t−m+1	O
.	O
Each	O
time	O
our	O
system	O
receives	O
a	O
new	O
word	O
x	O
t	O
,	O
the	O
classifier	O
predicts	O
probabilities	O
for	O
the	O
last	O
M	O
+1	O
words	O
as	O
sentence	O
boundaries	O
.	O
If	O
the	O
output	O
class	O
is	O
φ	O
,	O
the	O
classifier	O
receives	O
a	O
new	O
word	O
x	O
t+1	O
,	O
and	O
recompute	O
the	O
probabilities	O
for	O
x	O
t+1	O
,	O
x	O
t	O
,	O
x	O
t−1	O
,	O
...	O
,	O
x	O
t−M	O
+1	O
.	O
Generally	O
,	O
more	O
contextual	O
information	O
will	O
help	O
the	O
classifier	O
improve	O
the	O
precision	O
(	O
Section	O
4.5	O
)	O
.	O

Sentence	O
boundary	B-TaskName
detection	I-TaskName
has	O
been	O
explored	O
for	O
years	O
,	O
but	O
the	O
majority	O
of	O
these	O
work	O
focuses	O
on	O
offline	O
punctuation	O
restoration	O
,	O
instead	O
of	O
applied	O
in	O
simultaneous	O
translation	O
.	O
Existing	O
work	O
can	O
be	O
divided	O
into	O
two	O
classes	O
according	O
to	O
the	O
model	O
input	O
.	O

Nominal	O
ellipsis	O
has	O
been	O
a	O
topic	O
of	O
interest	O
in	O
theoretical	O
linguistics	O
for	O
a	O
very	O
long	O
time	O
(	O
Halliday	O
and	O
Hasan	O
,	O
1976	O
;	O
Dalrymple	O
et	O
al	O
,	O
1991	O
;	O
Lobeck	O
,	O
1995	O
;	O
Lappin	O
,	O
1996	O
;	O
Hobbs	O
and	O
Kehler	O
,	O
1997	O
;	O
Hardt	O
,	O
1999	O
;	O
Johnson	O
,	O
2001	O
;	O
Wijnen	O
et	O
al	O
,	O
2003	O
;	O
Merchant	O
,	O
2004	O
;	O
Frazier	O
,	O
2008	O
;	O
Chung	O
et	O
al	O
,	O
2010	O
;	O
Mer	O
-	O
chant	O
,	O
2010	O
;	O
Goksun	O
et	O
al	O
,	O
2010	O
;	O
Gunther	O
,	O
2011	O
;	O
Rouveret	O
,	O
2012	O
;	O
Lindenbergh	O
et	O
al	O
,	O
2015	O
;	O
van	O
Craenenbroeck	O
and	O
Merchant	O
,	O
2013	O
;	O
Park	O
,	O
2017	O
;	O
Hyams	O
et	O
al	O
,	O
2017	O
;	O
Kim	O
et	O
al	O
,	O
2019	O
)	O
.	O
Computational	O
approaches	O
to	O
the	O
ellipsis	O
phenomenon	O
majorly	O
focus	O
on	O
the	O
Verb	O
Phrase	O
Ellipsis	O
(	O
VPE	O
)	O
along	O
with	O
a	O
few	O
related	O
phenomenon	O
such	O
as	O
gapping	O
,	O
sluicing	O
and	O
do	O
-	O
so	O
anaphora	O
,	O
for	O
instance	O
,	O
the	O
detection	O
of	O
VPE	O
in	O
the	O
Penn	B-DatasetName
Treebank	I-DatasetName
using	O
pattern	O
match	O
(	O
Hardt	O
,	O
1992	O
)	O
,	O
a	O
transformation	O
learning	O
-	O
based	O
approach	O
to	O
generated	O
patterns	O
for	O
VPE	O
resolution	O
(	O
Hardt	O
,	O
1998	O
)	O
,	O
the	O
domain	O
independent	O
VPE	O
detection	O
and	O
resolution	O
using	O
machine	O
learning	O
(	O
Nielsen	O
,	O
2003	O
)	O
,	O
automatically	O
parsed	O
text	O
(	O
Nielsen	O
,	O
2004b	O
)	O
,	O
sentence	O
trimming	O
methods	O
(	O
McShane	O
et	O
al	O
,	O
2015	O
)	O
,	O
linguistic	O
principles	O
(	O
McShane	O
and	O
Babkin	O
,	O
2016	O
)	O
,	O
improved	O
parsing	O
techniques	O
that	O
encode	O
elided	O
material	O
dependencies	O
for	O
reconstruction	O
of	O
sentences	O
containing	O
gapping	O
(	O
Schuster	O
et	O
al	O
,	O
2018	O
)	O
,	O
discriminative	O
and	O
margin	O
infused	O
algorithms	O
(	O
Dean	O
et	O
al	O
,	O
2016	O
)	O
,	O
Multilayer	O
Perceptrons	O
(	O
MLP	B-DatasetName
)	O
and	O
Transformers	O
(	O
Zhang	O
et	O
al	O
,	O
2019	O
)	O
.	O
In	O
recent	O
times	O
,	O
there	O
has	O
been	O
a	O
surge	O
in	O
the	O
computational	O
research	O
on	O
nominal	O
ellipsis	O
and	O
closely	O
related	O
phenomena	O
(	O
Khullar	O
et	O
al	O
,	O
2020	O
(	O
Khullar	O
et	O
al	O
,	O
,	O
2019Lapshinova	O
-	O
Koltunski	O
et	O
al	O
,	O
2018	O
;	O
Menzel	O
,	O
2017	O
;	O
Menzel	O
and	O
Lapshinova	O
-	O
Koltunski	O
,	O
2014	O
)	O
.	O
For	O
the	O
resolution	O
process	O
,	O
we	O
previously	O
proposed	O
a	O
rule	O
based	O
system	O
(	O
Khullar	O
et	O
al	O
,	O
2019	O
)	O
that	O
detects	O
noun	O
ellipsis	O
using	O
syntactic	O
constraints	O
on	O
licensors	O
of	O
ellipsis	O
and	O
resolves	O
them	O
by	O
matching	O
Part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
Speech	O
(	O
POS	O
)	O
tag	O
similarity	O
between	O
the	O
licensor	O
of	O
ellipsis	O
and	O
the	O
modifier	O
of	O
the	O
antecedent	O
.	O
It	O
later	O
fine	O
tunes	O
these	O
syntactic	O
rules	O
on	O
a	O
small	O
curated	O
dataset	O
that	O
contains	O
234	O
instances	O
of	O
noun	O
ellipsis	O
along	O
with	O
some	O
negative	O
samples	O
(	O
Khullar	O
et	O
al	O
,	O
2019	O
)	O
.	O
For	O
the	O
present	O
paper	O
,	O
we	O
further	O
the	O
research	O
on	O
noun	O
ellipses	O
by	O
using	O
the	O
NoEl	O
corpus	O
annotated	O
by	O
us	O
previously	O
(	O
Khullar	O
et	O
al	O
,	O
2020	O
)	O
to	O
experiment	O
with	O
state	O
-	O
ofthe	O
-	O
art	O
ML	O
models	O
.	O

Following	O
the	O
VPE	O
resolution	O
framework	O
presented	O
by	O
(	O
Zhang	O
et	O
al	O
,	O
2019	O
)	O
,	O
we	O
investigate	O
a	O
similar	O
framework	O
for	O
noun	O
ellipsis	O
resolution	O
in	O
English	O
and	O
present	O
alternative	O
choices	O
of	O
the	O
models	O
at	O
each	O
step	O
as	O
shown	O
in	O
Figure	O
2	O
.	O
We	O
use	O
the	O
NoEl	O
corpus	O
(	O
Khullar	O
et	O
al	O
,	O
2020	O
)	O
that	O
marks	O
noun	O
ellipsis	O
instances	O
as	O
a	O
separate	O
layer	O
(	O
using	O
the	O
stand	O
-	O
off	O
annotation	O
scheme	O
)	O
on	O
the	O
Cornell	B-DatasetName
Movie	O
Dialogs	O
corpus	O
(	O
Danescu	O
-	O
Niculescu	O
-	O
Mizil	O
and	O
Lee	O
,	O
2011	O
)	O
.	O
The	O
corpus	O
marks	O
a	O
total	O
of	O
946	O
annotations	O
,	O
of	O
which	O
438	O
are	O
described	O
as	O
endophoric	O
,	O
i.e.	O
with	O
a	O
textual	O
antecedent	O
,	O
and	O
508	O
exophoric	O
,	O
i.e.	O
without	O
a	O
textual	O
antecedent	O
.	O

From	O
a	O
given	O
sentence	O
,	O
we	O
first	O
select	O
all	O
words	O
belonging	O
to	O
the	O
syntactic	O
categories	O
that	O
can	O
license	O
noun	O
ellipsis	O
in	O
English	O
,	O
i.e.	O
cardinal	O
and	O
ordinal	O
numbers	O
,	O
determiners	O
and	O
adjectives	O
(	O
Ross	O
,	O
1967	O
;	O
Lobeck	O
,	O
1995	O
;	O
Mitkov	O
,	O
1999	O
;	O
Saito	O
et	O
al	O
,	O
2008	O
;	O
Kim	O
et	O
al	O
,	O
2019	O
;	O
Khullar	O
et	O
al	O
,	O
2019	O
)	O
using	O
a	O
POS	O
tag	O
filter	O
.	O
The	O
POS	O
tags	O
are	O
obtained	O
from	O
state	O
-	O
ofthe	O
-	O
art	O
spaCy	O
parser	O
(	O
Honnibal	O
and	O
Johnson	O
,	O
2015	O
)	O
.	O
For	O
simplicity	O
,	O
we	O
refer	O
to	O
words	O
with	O
these	O
categories	O
as	O
noun	O
modifiers	O
(	O
although	O
in	O
strict	O
linguistic	O
terms	O
,	O
this	O
might	O
be	O
problematic	O
)	O
.	O
For	O
each	O
of	O
these	O
selected	O
noun	O
modifiers	O
,	O
we	O
follow	O
the	O
task	O
specification	O
for	O
VPE	O
detection	O
used	O
by	O
(	O
Nielsen	O
,	O
2004a	O
;	O
Bos	O
and	O
Spenader	O
,	O
2011	O
;	O
Liu	O
et	O
al	O
,	O
2016a	O
;	O
Dean	O
et	O
al	O
,	O
2016	O
)	O
and	O
present	O
noun	O
ellipsis	O
detection	O
as	O
a	O
binary	O
classification	O
task	O
,	O
where	O
given	O
a	O
noun	O
modifier	O
and	O
the	O
sentence	O
in	O
which	O
it	O
occurs	O
as	O
the	O
input	O
,	O
the	O
goal	O
of	O
the	O
classifier	O
is	O
to	O
predict	O
whether	O
the	O
noun	O
modifier	O
licenses	O
a	O
noun	O
ellipsis	O
or	O
not	O
.	O
Formally	O
,	O
for	O
a	O
given	O
licensor	O
word	O
l	O
i	O
is	O
a	O
licensor	O
in	O
a	O
sentence	O
s	O
,	O
the	O
task	O
is	O
represented	O
as	O
follows	O
:	O
f	O
(	O
l	O
i	O
,	O
s	O
)	O
−	O
{	O
0	B-DatasetName
,	O
1	O
}	O
where	O
1	O
denotes	O
that	O
l	O
i	O
is	O
a	O
licensor	O
in	O
s	O
,	O
and	O
0	B-DatasetName
otherwise	O
.	O
We	O
experiment	O
with	O
both	O
static	O
and	O
contextualised	O
word	B-TaskName
embeddings	I-TaskName
for	O
word	O
and	O
context	O
representation	O
.	O
For	O
the	O
former	O
,	O
we	O
choose	O
pretrained	O
fastText	B-MethodName
(	O
FT	O
)	O
word	B-TaskName
embeddings	I-TaskName
(	O
Bojanowski	O
et	O
al	O
,	O
2016	O
)	O
as	O
they	O
provide	O
representations	O
for	O
rare	O
and	O
unknown	O
words	O
that	O
might	O
be	O
frequent	O
in	O
the	O
movie	O
dialogues	O
.	O
For	O
the	O
latter	O
,	O
we	O
use	O
pretrained	O
BERT	B-MethodName
embeddings	O
from	O
the	O
BERT	B-MethodName
base	O
uncased	O
wordpiece	B-MethodName
model	O
for	O
English	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
as	O
these	O
currently	O
offer	O
the	O
most	O
powerful	O
embeddings	O
taking	O
into	O
account	O
a	O
large	O
left	O
and	O
right	O
context	O
.	O
fastText	B-MethodName
We	O
take	O
pretrained	O
FT	O
word	B-TaskName
embeddings	I-TaskName
for	O
the	O
noun	O
modifier	O
and	O
sentence	O
in	O
which	O
it	O
is	O
present	O
and	O
sum	O
pool	O
to	O
obtain	O
a	O
single	O
vector	O
that	O
we	O
use	O
to	O
train	O
our	O
classifiers	O
.	O
For	O
the	O
statistical	O
models	O
,	O
we	O
choose	O
Naive	O
Bayes	O
and	O
Linear	O
Support	B-MethodName
Vector	I-MethodName
Machine	I-MethodName
(	O
SVM	B-MethodName
)	O
,	O
and	O
use	O
scikit	O
learn	O
(	O
Pedregosa	O
et	O
al	O
,	O
2011	O
)	O
with	O
5	O
-	O
fold	O
cross	O
validation	O
for	O
training	O
and	O
testing	O
.	O
We	O
choose	O
a	O
BERT	B-MethodName
We	O
separate	O
the	O
sentence	O
and	O
the	O
licensor	O
with	O
a	O
[	O
SEP	O
]	O
token	O
and	O
keep	O
the	O
sequence	O
length	O
to	O
300	O
as	O
this	O
is	O
the	O
maximum	O
sentence	O
length	O
in	O
the	O
training	O
data	O
.	O
After	O
creating	O
the	O
concatenated	O
set	O
of	O
tokens	O
,	O
if	O
the	O
number	O
of	O
tokens	O
are	O
greater	O
than	O
300	O
,	O
we	O
clip	O
it	O
to	O
300	O
,	O
otherwise	O
we	O
add	O
[	O
PAD	B-DatasetName
]	O
tokens	O
which	O
correspond	O
to	O
the	O
embedding	O
of	O
768	O
dimensional	O
zero	O
-	O
vector	O
.	O
The	O
[	O
CLS	O
]	O
output	O
of	O
the	O
BERT	B-MethodName
model	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
is	O
then	O
fed	O
into	O
Naive	O
Bayes	O
,	O
Linear	O
SVM	B-MethodName
,	O
MLP	B-DatasetName
and	O
bi	O
-	O
LSTM	B-MethodName
networks	O
as	O
above	O
.	O
Manual	O
Syntactic	O
Features	O
For	O
each	O
of	O
these	O
models	O
,	O
we	O
additionally	O
experiment	O
with	O
manual	O
syntactic	O
features	O
.	O
We	O
use	O
the	O
lexical	O
features	O
proposed	O
by	O
(	O
Dean	O
et	O
al	O
,	O
2016	O
)	O
and	O
extended	O
lexical	O
features	O
by	O
(	O
Zhang	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
take	O
the	O
five	O
syntactic	O
constraints	O
on	O
licensors	O
of	O
ellipsis	O
explored	O
by	O
(	O
Khullar	O
et	O
al	O
,	O
2019	O
)	O
for	O
their	O
rulebased	O
approach	O
as	O
our	O
slot	O
pattern	O
features	O
.	O
We	O
concatenate	O
all	O
these	O
features	O
to	O
the	O
embeddings	O
from	O
the	O
previous	O
step	O
and	O
check	O
if	O
they	O
improve	O
the	O
classification	O
decision	O
.	O

We	O
define	O
noun	O
ellipsis	O
resolution	O
as	O
a	O
binary	O
classification	O
task	O
where	O
given	O
a	O
licensor	O
,	O
antecedent	O
candidate	O
and	O
their	O
context	O
,	O
the	O
goal	O
of	O
the	O
classifier	O
is	O
to	O
predict	O
whether	O
the	O
antecedent	O
candidate	O
is	O
the	O
resolution	O
of	O
the	O
ellipsis	O
licensed	O
by	O
the	O
licensor	O
.	O
Formally	O
,	O
given	O
a	O
sentence	O
s	O
,	O
the	O
licensor	O
l	O
i	O
from	O
the	O
detection	O
step	O
,	O
and	O
the	O
antecedent	O
candidate	O
a	O
j	O
;	O
the	O
noun	O
ellipsis	O
resolution	O
task	O
can	O
be	O
defined	O
as	O
follows	O
:	O
f	O
(	O
a	O
j	O
,	O
l	O
i	O
,	O
s	O
)	O
−	O
{	O
0	B-DatasetName
,	O
1	O
}	O
where	O
1	O
denotes	O
that	O
the	O
antecedent	O
candidate	O
a	O
j	O
is	O
the	O
actual	O
resolution	O
of	O
the	O
ellipsis	O
licensed	O
by	O
l	O
i	O
,	O
and	O
0	B-DatasetName
otherwise	O
.	O
Embeddings	O
Similar	O
to	O
the	O
detection	O
step	O
,	O
we	O
take	O
pretrained	O
fastText	B-MethodName
word	B-TaskName
embeddings	I-TaskName
for	O
the	O
licensor	O
,	O
antecedent	O
candidate	O
and	O
context	O
,	O
and	O
sum	O
pool	O
to	O
obtain	O
a	O
single	O
vector	O
.	O
In	O
case	O
of	O
BERT	B-MethodName
,	O
we	O
separate	O
the	O
sentence	O
,	O
the	O
licensor	O
and	O
the	O
antecedent	O
candidate	O
with	O
a	O
[	O
SEP	O
]	O
token	O
and	O
follow	O
the	O
same	O
steps	O
as	O
in	O
the	O
detection	O
step	O
.	O

We	O
explored	O
statistical	O
and	O
neural	O
models	O
for	O
noun	O
ellipsis	O
detection	O
and	O
resolution	O
,	O
presenting	O
a	O
strong	O
results	O
for	O
this	O
task	O
.	O
As	O
expected	O
,	O
neural	O
classifiers	O
perform	O
significantly	O
better	O
than	O
the	O
statistical	O
with	O
the	O
same	O
input	O
representation	O
.	O
As	O
with	O
several	O
other	O
NLP	O
tasks	O
,	O
the	O
contextual	O
nature	O
of	O
BERT	B-MethodName
is	O
useful	O
for	O
noun	O
ellipsis	O
resolution	O
too	O
,	O
making	O
robust	O
predictions	O
with	O
simple	O
neural	O
classifiers	O
.	O
Finally	O
,	O
addition	O
of	O
manual	O
features	O
boosts	O
the	O
performance	O
of	O
all	O
classifiers	O
including	O
those	O
that	O
use	O
BERT	B-MethodName
,	O
highlighting	O
that	O
ellipsis	O
is	O
a	O
syntactically	O
constrained	O
phenomenon	O
.	O

Attention	O
mechanisms	O
(	O
Bahdanau	O
et	O
al	O
,	O
2014	O
)	O
have	O
achieved	O
great	O
success	O
in	O
various	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
tasks	O
.	O
They	O
are	O
introduced	O
to	O
mimic	O
the	O
human	O
eye	O
focusing	O
on	O
important	O
parts	O
in	O
the	O
inputs	O
when	O
predicting	O
labels	O
.	O
The	O
existing	O
studies	O
show	O
attention	O
mechanisms	O
can	O
improve	O
not	O
only	O
the	O
performance	O
but	O
also	O
the	O
interpretability	O
of	O
the	O
models	O
(	O
Mullenbach	O
et	O
al	O
,	O
2018	O
;	O
Xie	O
et	O
al	O
,	O
2017	O
;	O
Xu	O
et	O
al	O
,	O
2015	O
)	O
.	O
Li	O
et	O
al	O
(	O
2016	O
)	O
pointed	O
the	O
view	O
:	O
"	O
Attention	O
provides	O
an	O
important	O
way	O
to	O
explain	O
the	O
workings	O
of	O
neural	O
models	O
"	O
.	O
Additionally	O
,	O
Wiegreffe	O
and	O
Pinter	O
(	O
2019	O
)	O
showed	O
that	O
attention	O
mechanisms	O
could	O
help	O
understand	O
the	O
inner	O
workings	O
of	O
a	O
model	O
.	O
The	O
basic	O
assumption	O
of	O
understanding	O
of	O
models	O
with	O
attention	O
scores	O
is	O
that	O
the	O
inputs	O
(	O
e.g.	O
,	O
words	O
)	O
with	O
high	O
attentive	O
weights	O
are	O
essential	O
for	O
making	O
decisions	O
.	O
However	O
,	O
as	O
far	O
as	O
we	O
know	O
,	O
it	O
has	O
not	O
been	O
formally	O
verified	O
.	O
Existing	O
research	O
(	O
Jain	O
and	O
Wallace	O
,	O
2019	O
)	O
also	O
shows	O
that	O
attention	O
is	O
not	O
explicable	O
,	O
and	O
there	O
are	O
a	O
lot	O
of	O
controversy	O
regarding	O
to	O
the	O
result	O
explanations	O
(	O
Wiegreffe	O
and	O
Pinter	O
,	O
2019	O
;	O
Jain	O
and	O
Wallace	O
,	O
2019	O
)	O
.	O
Moreover	O
,	O
we	O
find	O
that	O
though	O
the	O
attention	O
mechanism	O
can	O
help	O
improve	O
the	O
performance	O
for	O
text	B-TaskName
classification	I-TaskName
in	O
our	O
experiments	O
,	O
it	O
may	O
focus	O
on	O
the	O
irrelevant	O
information	O
.	O
For	O
example	O
,	O
in	O
the	O
sentence	O
"	O
A	O
very	O
funny	O
movie	O
.	O
"	O
,	O
the	O
long	B-MethodName
short	I-MethodName
-	I-MethodName
term	I-MethodName
memory	I-MethodName
model	O
with	O
standard	O
attention	O
(	O
LSTM	B-MethodName
-	O
ATT	O
)	O
infers	O
a	O
correct	O
sentiment	O
label	O
while	O
pays	O
more	O
attention	O
to	O
the	O
irrelevant	O
word	O
"	O
movie	O
"	O
,	O
making	O
the	O
result	O
difficult	O
to	O
explain	O
.	O
In	O
general	O
,	O
the	O
attention	O
weights	O
are	O
only	O
optimized	O
to	O
encode	O
the	O
task	O
-	O
relevant	O
information	O
while	O
are	O
not	O
restricted	O
to	O
imitate	O
human	O
behavior	O
.	O
In	O
order	O
to	O
enhance	O
the	O
interpretability	O
of	O
the	O
attention	O
mechanism	O
,	O
recent	O
studies	O
turn	O
to	O
integrate	O
the	O
human	O
provided	O
explanation	O
signals	O
into	O
the	O
attention	O
models	O
.	O
regularized	O
the	O
attention	O
weights	O
with	O
a	O
small	O
amount	O
of	O
word	O
-	O
level	O
annotations	O
.	O
Barrett	O
et	O
al	O
(	O
2018	O
)	O
;	O
Bao	O
et	O
al	O
(	O
2018	O
)	O
improved	O
the	O
explanation	O
of	O
attention	O
by	O
aligning	O
explanations	O
with	O
human	O
-	O
provided	O
rationales	O
.	O
These	O
methods	O
rely	O
on	O
additional	O
labour	O
consuming	O
labelling	O
for	O
enhancing	O
explanations	O
,	O
which	O
is	O
hard	O
to	O
extend	O
to	O
other	O
datasets	O
or	O
tasks	O
.	O
In	O
this	O
paper	O
,	O
we	O
aim	O
to	O
train	O
a	O
more	O
efficient	O
and	O
effective	O
interpretable	O
attention	O
model	O
without	O
any	O
pre	O
-	O
defined	O
annotations	O
or	O
pre	O
-	O
collected	O
explanations	O
.	O
Specifically	O
,	O
we	O
propose	O
a	O
framework	O
consisting	O
of	O
a	O
learner	O
and	O
a	O
compressor	O
,	O
which	O
enhances	O
the	O
performance	O
and	O
interpretability	O
of	O
the	O
attention	O
model	O
for	O
text	B-TaskName
classification	I-TaskName
1	O
.	O
The	O
learner	O
learns	O
text	O
representations	O
by	O
fine	O
-	O
tuning	O
the	O
encoder	O
.	O
Regarding	O
to	O
the	O
compressor	O
,	O
we	O
are	O
motivated	O
by	O
the	O
effectiveness	O
of	O
the	O
information	O
bottleneck	O
(	O
IB	O
)	O
(	O
Tishby	O
et	O
al	O
,	O
1999	O
)	O
to	O
enhance	O
performance	O
(	O
Li	O
and	O
Eisner	O
,	O
2019	O
)	O
or	O
detect	O
important	O
features	O
(	O
Bang	O
et	O
al	O
,	O
2019	O
;	O
Chen	O
and	O
Ji	O
,	O
2020	O
;	O
Jiang	O
et	O
al	O
,	O
2020	O
;	O
Schulz	O
et	O
al	O
,	O
2020	O
)	O
,	O
and	O
present	O
a	O
Variational	O
information	O
bottleneck	O
ATtention	O
(	O
VAT	O
)	O
mechanism	O
using	O
IB	O
to	O
keep	O
the	O
most	O
relevant	O
clues	O
and	O
forget	O
the	O
irrelevant	O
ones	O
for	O
better	O
attention	O
explanations	O
.	O
In	O
particular	O
,	O
IB	O
is	O
integrated	O
into	O
attention	O
to	O
minimize	O
the	O
mutual	O
information	O
(	O
MI	O
)	O
with	O
the	O
input	O
while	O
preserving	O
as	O
much	O
MI	O
as	O
possible	O
with	O
the	O
output	O
,	O
which	O
provides	O
more	O
accurate	O
and	O
reliable	O
explanations	O
by	O
controlling	O
the	O
information	O
flow	O
.	O
To	O
evaluate	O
the	O
effectiveness	O
of	O
our	O
proposed	O
approach	O
,	O
we	O
adapt	O
two	O
advanced	O
neural	O
models	O
(	O
LSTM	B-MethodName
and	O
BERT	B-MethodName
)	O
within	O
the	O
framework	O
and	O
conduct	O
experiments	O
on	O
eight	O
benchmark	O
datasets	O
.	O
The	O
experimental	O
results	O
show	O
that	O
our	O
adapted	O
models	O
outperform	O
the	O
standard	O
attention	O
-	O
based	O
models	O
over	O
all	O
the	O
datasets	O
.	O
Moreover	O
,	O
they	O
exhibit	O
great	O
advantages	O
with	O
respect	O
to	O
interpretability	O
by	O
both	O
qualitative	O
and	O
quantitative	O
analyses	O
.	O
Specifically	O
,	O
we	O
obtain	O
significant	O
improvements	O
by	O
applying	O
our	O
model	O
to	O
the	O
semi	O
-	O
supervised	O
word	O
-	O
level	O
sentiment	O
detection	O
task	O
,	O
which	O
detects	O
the	O
sentiment	O
words	O
based	O
on	O
attention	O
weights	O
via	O
only	O
sentencelevel	O
sentiment	O
label	O
.	O
In	O
addition	O
,	O
we	O
provide	O
the	O
case	O
studies	O
and	O
text	O
representation	O
visualization	O
to	O
have	O
an	O
insight	O
into	O
how	O
our	O
model	O
works	O
.	O
The	O
main	O
contributions	O
of	O
this	O
work	O
are	O
summarized	O
as	O
follows	O
.	O
We	O
propose	O
a	O
novel	O
framework	O
to	O
enhance	O
the	O
performance	O
and	O
interpretability	O
of	O
the	O
attention	O
models	O
,	O
where	O
a	O
learner	O
is	O
used	O
to	O
learn	O
good	O
representations	O
by	O
fine	O
-	O
tuning	O
and	O
a	O
compressor	O
is	O
used	O
to	O
obtain	O
good	O
attentive	O
weights	O
by	O
compressing	O
iteratively	O
.	O
We	O
present	O
a	O
Variational	O
information	O
bottleneck	O
ATtention	O
(	O
VAT	O
)	O
mechanism	O
for	O
the	O
compressor	O
,	O
which	O
performs	O
compression	O
over	O
the	O
text	O
representation	O
to	O
keep	O
the	O
task	O
related	O
information	O
while	O
reduce	O
the	O
irrelevant	O
noise	O
via	O
information	O
bottleneck	O
.	O
Extensive	O
experiments	O
show	O
the	O
great	O
advantages	O
of	O
our	O
models	O
within	O
the	O
proposed	O
framework	O
,	O
and	O
we	O
perform	O
various	O
qualitative	O
and	O
quantitative	O
analyses	O
to	O
shed	O
light	O
on	O
why	O
our	O
models	O
work	O
in	O
both	O
performance	O
and	O
interpretability	O
.	O

For	O
LSTM	B-MethodName
-	O
based	O
models	O
,	O
we	O
use	O
GloVe	B-MethodName
embedding	O
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
with	O
300	O
-	O
dimension	O
to	O
initialize	O
the	O
word	O
embedding	O
and	O
fine	O
-	O
tune	O
it	O
during	O
the	O
training	O
.	O
We	O
randomly	O
initialize	O
all	O
outof	O
-	O
vocabulary	O
words	O
and	O
weights	O
with	O
the	O
uniform	O
distribution	O
U	O
p´0.1	O
,	O
0.1q	O
.	O
For	O
the	O
BERT	B-MethodName
-	O
based	O
models	O
,	O
we	O
fine	O
-	O
tune	O
pre	O
-	O
trained	O
BERT	B-MethodName
-	O
base	O
model	O
.	O

To	O
understand	O
why	O
our	O
proposed	O
VAT	O
model	O
is	O
more	O
effective	O
than	O
the	O
standard	O
attention	O
-	O
based	O
model	O
,	O
we	O
visualize	O
two	O
examples	O
of	O
LSTM	B-MethodName
-	O
based	O
models	O
using	O
attention	O
heatmaps	O
(	O
Figure	O
7	O
)	O
.	O
First	O
,	O
the	O
standard	O
attention	O
-	O
based	O
LSTM	B-MethodName
model	O
focuses	O
on	O
the	O
wrong	O
words	O
(	O
e.g.	O
,	O
"	O
this	O
"	O
,	O
"	O
work	O
"	O
)	O
even	O
though	O
it	O
predicts	O
the	O
right	O
sentiment	O
while	O
our	O
VAT	O
model	O
finds	O
the	O
correct	O
words	O
(	O
e.g.	O
,	O
"	O
admired	O
"	O
,	O
"	O
lot	O
"	O
)	O
.	O
It	O
indicates	O
integrating	O
IB	O
into	O
attention	O
can	O
help	O
it	O
focus	O
on	O
the	O
key	O
words	O
and	O
reduce	O
the	O
noisy	O
information	O
.	O
Second	O
,	O
our	O
proposed	O
model	O
can	O
also	O
improve	O
the	O
attention	O
's	O
performance	O
by	O
capturing	O
the	O
critical	O
words	O
accurately	O
.	O
For	O
example	O
,	O
in	O
the	O
sentence	O
"	O
That	O
sucks	O
if	O
you	O
have	O
to	O
take	O
the	O
sats	O
tomorrow	O
.	O
"	O
,	O
our	O
model	O
predicts	O
the	O
right	O
class	O
label	O
by	O
attending	O
the	O
words	O
"	O
sucks	O
"	O
and	O
"	O
have	O
to	O
.	O
"	O

This	O
paper	O
proposes	O
a	O
VAT	O
-	O
based	O
framework	O
to	O
improve	O
the	O
performance	O
and	O
interpretability	O
of	O
attentions	O
via	O
both	O
fine	O
-	O
tuning	O
and	O
compressing	O
.	O
The	O
experimental	O
results	O
on	O
eight	O
benchmark	O
datasets	O
for	O
text	B-TaskName
classification	I-TaskName
verify	O
the	O
effectiveness	O
of	O
our	O
models	O
within	O
this	O
framework	O
.	O
In	O
addition	O
,	O
we	O
apply	O
the	O
framework	O
for	O
sentiment	O
detection	O
,	O
which	O
further	O
demonstrates	O
the	O
superiority	O
in	O
terms	O
of	O
interpretability	O
.	O
It	O
is	O
also	O
interesting	O
to	O
find	O
that	O
training	O
the	O
models	O
by	O
fine	O
-	O
tuning	O
and	O
compressing	O
iteratively	O
is	O
effective	O
to	O
improve	O
the	O
text	O
representations	O
.	O
In	O
the	O
future	O
,	O
we	O
will	O
investigate	O
the	O
effectiveness	O
of	O
our	O
proposed	O
attention	O
framework	O
for	O
other	O
tasks	O
and	O
areas	O
,	O
such	O
as	O
machine	B-TaskName
translation	I-TaskName
and	O
visual	B-DatasetName
question	I-DatasetName
answering	I-DatasetName
.	O

Current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
machine	B-TaskName
translation	I-TaskName
systems	O
are	O
based	O
on	O
encoder	O
-	O
decoder	O
architectures	O
,	O
that	O
first	O
encode	O
the	O
input	O
sequence	O
,	O
and	O
then	O
generate	O
an	O
output	O
sequence	O
based	O
on	O
the	O
input	O
encoding	O
.	O
Both	O
are	O
interfaced	O
with	O
an	O
attention	O
mechanism	O
that	O
recombines	O
a	O
fixed	O
encoding	O
of	O
the	O
source	O
tokens	O
based	O
on	O
the	O
decoder	O
state	O
.	O
We	O
propose	O
an	O
alternative	O
approach	O
which	O
instead	O
relies	O
on	O
a	O
single	O
2D	O
convolutional	O
neural	O
network	O
across	O
both	O
sequences	O
.	O
Each	O
layer	O
of	O
our	O
network	O
recodes	O
source	O
tokens	O
on	O
the	O
basis	O
of	O
the	O
output	O
sequence	O
produced	O
so	O
far	O
.	O
Attention	O
-	O
like	O
properties	O
are	O
therefore	O
pervasive	O
throughout	O
the	O
network	O
.	O
Our	O
model	O
yields	O
excellent	O
results	O
,	O
outperforming	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
encoderdecoder	O
systems	O
,	O
while	O
being	O
conceptually	O
simpler	O
and	O
having	O
fewer	O
parameters	O
.	O

Deep	O
neural	O
networks	O
have	O
made	O
a	O
profound	O
impact	O
on	O
natural	O
language	O
processing	O
technology	O
in	O
general	O
,	O
and	O
machine	B-TaskName
translation	I-TaskName
in	O
particular	O
(	O
Blunsom	O
,	O
2013	O
;	O
Cho	O
et	O
al	O
,	O
2014	O
;	O
Jean	O
et	O
al	O
,	O
2015	O
;	O
LeCun	O
et	O
al	O
,	O
2015	O
)	O
.	O
Machine	B-TaskName
translation	I-TaskName
(	O
MT	O
)	O
can	O
be	O
seen	O
as	O
a	O
sequenceto	O
-	O
sequence	O
prediction	O
problem	O
,	O
where	O
the	O
source	O
and	O
target	O
sequences	O
are	O
of	O
different	O
and	O
variable	O
length	O
.	O
Current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
approaches	O
are	O
based	O
on	O
encoder	O
-	O
decoder	O
architectures	O
(	O
Blunsom	O
,	O
2013	O
;	O
Cho	O
et	O
al	O
,	O
2014	O
;	O
Bahdanau	O
et	O
al	O
,	O
2015	O
)	O
.	O
The	O
encoder	O
"	O
reads	O
"	O
the	O
variable	O
-	O
length	O
source	O
sequence	O
and	O
maps	O
it	O
into	O
a	O
vector	O
representation	O
.	O
The	O
decoder	O
takes	O
this	O
vector	O
as	O
input	O
and	O
"	O
writes	O
"	O
the	O
target	O
sequence	O
,	O
updating	O
its	O
state	O
each	O
step	O
with	O
the	O
most	O
recent	O
word	O
that	O
it	O
generated	O
.	O
The	O
basic	O
encoder	O
-	O
decoder	O
model	O
is	O
generally	O
equipped	O
with	O
an	O
attention	O
model	O
(	O
Bahdanau	O
et	O
al	O
,	O
2015	O
)	O
,	O
which	O
repetitively	O
re	O
-	O
accesses	O
the	O
source	O
sequence	O
during	O
the	O
decoding	O
process	O
.	O
Given	O
the	O
current	O
state	O
of	O
the	O
decoder	O
,	O
a	O
probability	O
distribution	O
over	O
the	O
elements	O
in	O
the	O
source	O
sequence	O
is	O
computed	O
,	O
which	O
is	O
then	O
used	O
to	O
select	O
or	O
aggregate	O
features	O
of	O
these	O
elements	O
into	O
a	O
single	O
"	O
context	O
"	O
vector	O
that	O
is	O
used	O
by	O
the	O
decoder	O
.	O
Rather	O
than	O
relying	O
on	O
the	O
global	O
representation	O
of	O
the	O
source	O
sequence	O
,	O
the	O
attention	O
mechanism	O
allows	O
the	O
decoder	O
to	O
"	O
look	O
back	O
"	O
into	O
the	O
source	O
sequence	O
and	O
focus	O
on	O
salient	O
positions	O
.	O
Besides	O
this	O
inductive	O
bias	O
,	O
the	O
attention	O
mechanism	O
bypasses	O
the	O
problem	O
of	O
vanishing	O
gradients	O
that	O
most	O
recurrent	O
architectures	O
encounter	O
.	O
However	O
,	O
the	O
current	O
attention	O
mechanisms	O
have	O
limited	O
modeling	O
abilities	O
and	O
are	O
generally	O
a	O
simple	O
weighted	O
sum	O
of	O
the	O
source	O
representations	O
(	O
Bahdanau	O
et	O
al	O
,	O
2015	O
;	O
Luong	O
et	O
al	O
,	O
2015	O
)	O
,	O
where	O
the	O
weights	O
are	O
the	O
result	O
of	O
a	O
shallow	O
matching	O
between	O
source	O
and	O
target	O
elements	O
.	O
The	O
attention	O
module	O
re	O
-	O
combines	O
the	O
same	O
source	O
token	O
codes	O
and	O
is	O
unable	O
to	O
re	O
-	O
encode	O
or	O
re	O
-	O
interpret	O
the	O
source	O
sequence	O
while	O
decoding	O
.	O
To	O
address	O
these	O
limitations	O
,	O
we	O
propose	O
an	O
alternative	O
neural	O
MT	O
architecture	O
,	O
based	O
on	O
deep	O
2D	O
convolutional	O
neural	O
networks	O
(	O
CNNs	O
)	O
.	O
The	O
product	O
space	O
of	O
the	O
positions	O
in	O
source	O
and	O
target	O
sequences	O
defines	O
the	O
2D	O
grid	O
over	O
which	O
the	O
network	O
is	O
defined	O
.	O
The	O
convolutional	O
filters	O
are	O
masked	O
to	O
prohibit	O
accessing	O
information	O
derived	O
from	O
future	O
tokens	O
in	O
the	O
target	O
sequence	O
,	O
obtaining	O
an	O
autoregressive	O
model	O
akin	O
to	O
generative	O
models	O
for	O
images	O
and	O
audio	O
waveforms	O
(	O
Oord	O
et	O
al	O
,	O
2016a	O
,	O
b	O
)	O
.	O
See	O
Figure	O
1	O
for	O
an	O
illustration	O
.	O
This	O
approach	O
allows	O
us	O
to	O
learn	O
deep	O
feature	O
hierarchies	O
based	O
on	O
a	O
stack	O
of	O
2D	O
convolutional	O
layers	O
,	O
and	O
benefit	O
from	O
parallel	O
computation	O
during	O
training	O
.	O
Every	O
layer	O
of	O
our	O
network	O
computes	O
features	O
of	O
the	O
the	O
source	O
tokens	O
,	O
based	O
on	O
the	O
target	O
sequence	O
produced	O
so	O
far	O
,	O
and	O
uses	O
these	O
to	O
predict	O
the	O
next	O
output	O
token	O
.	O
Our	O
model	O
therefore	O
has	O
attention	O
-	O
like	O
capabilities	O
by	O
construction	O
,	O
that	O
are	O
pervasive	O
throughout	O
the	O
layers	O
of	O
the	O
network	O
,	O
Convolutional	O
layers	O
in	O
our	O
model	O
use	O
masked	O
3×3	O
filters	O
so	O
that	O
features	O
are	O
only	O
computed	O
from	O
previous	O
output	O
symbols	O
.	O
Illustration	O
of	O
the	O
receptive	O
fields	O
after	O
one	O
(	O
dark	O
blue	O
)	O
and	O
two	O
layers	O
(	O
light	O
blue	O
)	O
,	O
together	O
with	O
the	O
masked	O
part	O
of	O
the	O
field	O
of	O
view	O
of	O
a	O
normal	O
3×3	O
filter	O
(	O
gray	O
)	O
.	O
rather	O
than	O
using	O
an	O
"	O
add	O
-	O
on	O
"	O
attention	O
model	O
.	O
We	O
validate	O
our	O
model	O
with	O
experiments	O
on	O
the	O
IWSLT	O
2014	O
German	O
-	O
to	O
-	O
English	O
(	O
De	O
-	O
En	O
)	O
and	O
English	O
-	O
to	O
-	O
German	O
(	O
En	O
-	O
De	O
)	O
tasks	O
.	O
We	O
improve	O
on	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
encoder	O
-	O
decoder	O
models	O
with	O
attention	O
,	O
while	O
being	O
conceptually	O
simpler	O
and	O
having	O
fewer	O
parameters	O
.	O
In	O
the	O
next	O
section	O
we	O
will	O
discuss	O
related	O
work	O
,	O
before	O
presenting	O
our	O
approach	O
in	O
detail	O
in	O
Section	O
3	O
.	O
We	O
present	O
our	O
experimental	O
evaluation	O
results	O
in	O
Section	O
4	O
,	O
and	O
conclude	O
in	O
Section	O
5	O
.	O

In	O
this	O
section	O
we	O
present	O
our	O
2D	O
CNN	O
translation	O
model	O
in	O
detail	O
.	O
Input	O
source	O
-	O
target	O
tensor	O
.	O
Given	O
the	O
source	O
and	O
target	O
pair	O
(	O
s	O
,	O
t	O
)	O
of	O
lengths	O
|	O
s	O
|	O
and	O
|	O
t	O
|	O
respectively	O
,	O
we	O
first	O
embed	O
the	O
tokens	O
in	O
d	O
s	O
and	O
d	O
t	O
dimensional	O
spaces	O
via	O
look	O
-	O
up	O
tables	O
.	O
The	O
word	B-TaskName
embeddings	I-TaskName
{	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
|	O
s	O
|	O
}	O
and	O
{	O
y	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
|	O
t	O
|	O
}	O
are	O
then	O
concatenated	O
to	O
form	O
a	O
3D	O
tensor	O
X	O
R	O
|	O
t	O
|	O
×	O
|	O
s	O
|	O
×f	O
0	B-DatasetName
,	O
with	O
f	O
0	B-DatasetName
=	O
d	O
t	O
+	O
d	O
s	O
,	O
where	O
X	O
ij	O
=	O
[	O
y	O
i	O
x	O
j	O
]	O
.	O
(	O
1	O
)	O
This	O
joint	O
unigram	O
encoding	O
is	O
the	O
input	O
to	O
our	O
convolutional	O
network	O
.	O

Besides	O
pooling	O
we	O
can	O
collapse	O
the	O
source	O
dimension	O
of	O
the	O
feature	O
tensor	O
with	O
an	O
attention	O
mechanism	O
.	O
This	O
mechanism	O
will	O
generate	O
a	O
tensor	O
H	O
att	O
that	O
can	O
be	O
used	O
instead	O
of	O
,	O
or	O
concatenated	O
with	O
,	O
H	O
Pool	O
.	O
We	O
use	O
the	O
self	O
-	O
attention	O
approach	O
of	O
,	O
which	O
for	O
output	O
token	O
i	O
computes	O
the	O
attention	O
vector	O
ρ	O
i	O
R	O
|	O
s	O
|	O
from	O
the	O
activations	O
H	O
L	O
i	O
:	O
ρ	O
i	O
=	O
SoftMax	B-MethodName
H	O
L	O
i	O
w	O
+	O
b1	O
|	O
s	O
|	O
,	O
(	O
8	O
)	O
H	O
att	O
i	O
=	O
|	O
s	O
|	O
ρ	O
i	O
H	O
L	O
i	O
,	O
(	O
9	O
)	O
where	O
w	O
R	O
f	O
L	O
and	O
b	O
R	O
are	O
parameters	O
of	O
the	O
attention	O
mechanism	O
.	O
Scaling	O
of	O
attention	O
vectors	O
with	O
the	O
square	O
-	O
root	O
of	O
the	O
source	O
length	O
was	O
also	O
used	O
by	O
Gehring	O
et	O
al	O
(	O
2017b	O
)	O
,	O
and	O
we	O
found	O
it	O
effective	O
here	O
as	O
well	O
as	O
in	O
the	O
average	O
-	O
pooling	O
case	O
.	O

RNNsearch	O
*	O
(	O
Bahdanau	O
et	O
al	O
,	O
2015	O
)	O
31.02	O
1.79	O
6	O
M	O
25.92	O
7	O
M	O
Varational	O
attention	O
33.10	O
Transformer	B-MethodName
*	O
*	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
32.83	O
3.53	O
59	O
M	O
27.68	O
61	O
M	O
ConvS2S	O
*	O
*	O
(	O
MLE	O
)	O
(	O
Gehring	O
et	O
al	O
,	O
2017b	O
)	O
32	O
(	O
Gehring	O
et	O
al	O
,	O
2017b	O
)	O
.	O
phrase	O
,	O
and	O
progressively	O
from	O
the	O
part	O
of	O
the	O
source	O
phrase	O
that	O
remains	O
to	O
be	O
decoded	O
.	O

We	O
presented	O
a	O
novel	O
neural	O
machine	B-TaskName
translation	I-TaskName
architecture	O
that	O
departs	O
from	O
the	O
encoder	O
-	O
decoder	O
paradigm	O
.	O
Our	O
model	O
jointly	O
encodes	O
the	O
source	O
and	O
target	O
sequence	O
into	O
a	O
deep	O
feature	O
hierarchy	O
in	O
which	O
the	O
source	O
tokens	O
are	O
embedded	O
in	O
the	O
context	O
of	O
a	O
partial	O
target	O
sequence	O
.	O
Max	O
-	O
pooling	O
over	O
this	O
joint	O
-	O
encoding	O
along	O
the	O
source	O
dimension	O
is	O
used	O
to	O
map	O
the	O
features	O
to	O
a	O
prediction	O
for	O
the	O
next	O
target	O
token	O
.	O
The	O
model	O
is	O
implemented	O
as	O
2D	O
CNN	O
based	O
on	O
DenseNet	B-MethodName
,	O
with	O
masked	O
convolutions	O
to	O
ensure	O
a	O
proper	O
autoregressive	O
factorization	O
of	O
the	O
conditional	O
probabilities	O
.	O
Since	O
each	O
layer	O
of	O
our	O
model	O
re	O
-	O
encodes	O
the	O
input	O
tokens	O
in	O
the	O
context	O
of	O
the	O
target	O
sequence	O
generated	O
so	O
far	O
,	O
the	O
model	O
has	O
attention	O
-	O
like	O
properties	O
in	O
every	O
layer	O
of	O
the	O
network	O
by	O
construction	O
.	O
Adding	O
an	O
explicit	O
self	O
-	O
attention	O
module	O
therefore	O
has	O
a	O
very	O
limited	O
,	O
but	O
positive	O
,	O
effect	O
.	O
Nevertheless	O
,	O
the	O
max	O
-	O
pooling	O
operator	O
in	O
our	O
model	O
generates	O
implicit	O
sentence	O
alignments	O
that	O
are	O
qualitatively	O
similar	O
to	O
the	O
ones	O
generated	O
by	O
attention	O
mechanisms	O
.	O
We	O
evaluate	O
our	O
model	O
on	O
the	O
IWSLT'14	O
dataset	O
,	O
translation	O
German	O
to	O
English	O
and	O
vice	O
-	O
versa	O
.	O
We	O
obtain	O
excellent	O
BLEU	B-MetricName
scores	O
that	O
compare	O
favorably	O
with	O
the	O
state	O
of	O
the	O
art	O
,	O
while	O
using	O
a	O
conceptually	O
simpler	O
model	O
with	O
fewer	O
parameters	O
.	O
We	O
hope	O
that	O
our	O
alternative	O
joint	O
source	O
-	O
target	O
encoding	O
sparks	O
interest	O
in	O
other	O
alternatives	O
to	O
the	O
encoder	O
-	O
decoder	O
model	O
.	O
In	O
the	O
future	O
,	O
we	O
plan	O
to	O
explore	O
hybrid	O
approaches	O
in	O
which	O
the	O
input	O
to	O
our	O
joint	O
encoding	O
model	O
is	O
not	O
provided	O
by	O
tokenembedding	O
vectors	O
,	O
but	O
the	O
output	O
of	O
1D	O
source	O
and	O
target	O
embedding	O
networks	O
,	O
e.g.	O
(	O
bi	O
-	O
)	O
LSTM	B-MethodName
or	O
1D	O
convolutional	O
.	O
We	O
also	O
want	O
to	O
explore	O
how	O
our	O
model	O
can	O
be	O
used	O
to	O
translate	O
across	O
multiple	O
language	O
pairs	O
.	O
Our	O
PyTorch	O
-	O
based	O
implementation	O
is	O
available	O
at	O
https://github.com/elbayadm/	O
attn2d	O
.	O

In	O
this	O
paper	O
,	O
we	O
describe	O
our	O
participation	O
in	O
the	O
2021	O
Workshop	O
on	O
Asian	O
Translation	B-TaskName
(	O
team	O
ID	O
:	O
tpt_wat	O
)	O
.	O
We	O
submitted	O
results	O
for	O
all	O
six	O
directions	O
of	O
the	O
JPC2	O
patent	O
task	O
.	O
As	O
a	O
first	O
-	O
time	O
participant	O
in	O
the	O
task	O
,	O
we	O
attempted	O
to	O
identify	O
a	O
single	O
configuration	O
that	O
provided	O
the	O
best	O
overall	O
results	O
across	O
all	O
language	O
pairs	O
.	O
All	O
our	O
submissions	O
were	O
created	O
using	O
single	O
base	O
transformer	O
models	O
,	O
trained	O
on	O
only	O
the	O
task	O
-	O
specific	O
data	O
,	O
using	O
a	O
consistent	O
configuration	O
of	O
hyperparameters	O
.	O
In	O
contrast	O
to	O
the	O
uniformity	O
of	O
our	O
methods	O
,	O
our	O
results	O
vary	O
widely	O
across	O
the	O
six	O
language	O
pairs	O
.	O

The	O
field	O
of	O
machine	B-TaskName
translation	I-TaskName
has	O
seen	O
rapid	O
innovation	O
in	O
the	O
last	O
few	O
years	O
,	O
with	O
new	O
model	O
architectures	O
,	O
pre	O
-	O
training	O
regimens	O
,	O
and	O
computational	O
algorithms	O
emerging	O
at	O
a	O
dizzying	O
pace	O
.	O
However	O
,	O
translation	O
of	O
these	O
techniques	O
into	O
industry	O
practice	O
occurs	O
more	O
slowly	O
.	O
Companies	O
utilizing	O
these	O
techniques	O
must	O
take	O
into	O
account	O
considerations	O
such	O
as	O
deployment	O
costs	O
(	O
model	O
speed	O
and	O
size	O
)	O
,	O
scalability	O
,	O
explainability	O
,	O
the	O
complexity	O
of	O
training	O
regimens	O
(	O
resource	O
constraints	O
limiting	O
independent	O
hyperparameter	B-TaskName
optimization	I-TaskName
for	O
all	O
language	O
pairs	O
)	O
,	O
and	O
risk	O
management	O
,	O
against	O
which	O
advances	O
yielding	O
performance	O
gains	O
must	O
be	O
weighed	O
.	O
For	O
our	O
participation	O
in	O
the	O
2021	O
Workshop	O
on	O
Asian	O
Translation	B-TaskName
shared	O
task	O
on	O
patent	O
translation	O
,	O
we	O
have	O
applied	O
a	O
single	O
,	O
standardized	O
data	O
preparation	O
and	O
model	O
training	O
pipeline	O
as	O
a	O
way	O
of	O
benchmarking	O
the	O
performance	O
of	O
this	O
process	O
.	O
We	O
conducted	O
limited	O
experiments	O
to	O
test	O
different	O
parameters	O
,	O
before	O
1	O
http://lotus.kuee.kyoto	O
-	O
u.ac.jp/WAT/patent/	O
settling	O
on	O
the	O
approach	O
which	O
provided	O
the	O
best	O
overall	O
results	O
across	O
all	O
language	O
pairs	O
.	O
Our	O
NMT	O
systems	O
are	O
standard	O
base	O
Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
models	O
,	O
which	O
were	O
trained	O
using	O
only	O
the	O
data	O
resources	O
provided	O
by	O
the	O
task	O
organizers	O
.	O
These	O
models	O
used	O
shared	O
subword	O
vocabularies	O
created	O
with	O
SentencePiece	B-MethodName
(	O
Kudo	O
and	O
Richardson	O
,	O
2018	O
)	O
.	O
In	O
contrast	O
to	O
the	O
uniformity	O
of	O
our	O
methods	O
,	O
our	O
results	O
varied	O
widely	O
across	O
the	O
six	O
language	O
pairs	O
.	O
Different	O
scoring	O
metrics	O
prevent	O
the	O
direct	O
comparison	O
of	O
scores	O
from	O
different	O
language	O
pairs	O
,	O
but	O
relative	O
to	O
the	O
top	O
performing	O
model	O
in	O
each	O
language	O
pair	O
,	O
our	O
scores	O
ranged	O
from	O
98.84	O
%	O
of	O
the	O
top	O
score	O
for	O
the	O
English	O
Japanese	O
language	O
pair	O
,	O
to	O
83.89	O
%	O
of	O
the	O
top	O
score	O
for	O
Korean	O
Japanese	O
.	O
Below	O
,	O
we	O
describe	O
in	O
detail	O
our	O
system	O
architecture	O
,	O
hyperparameter	O
configuration	O
,	O
hardware	O
resources	O
,	O
and	O
results	O
.	O

The	O
data	O
were	O
encoded	O
using	O
subword	O
encodings	O
learned	O
from	O
the	O
corpora	O
using	O
the	O
unigram	O
model	O
trainer	O
provided	O
by	O
SentencePiece	B-MethodName
(	O
Kudo	O
and	O
Richardson	O
,	O
2018	O
)	O
.	O
To	O
avoid	O
the	O
added	O
complexity	O
of	O
using	O
different	O
pre	O
-	O
tokenization	O
strategies	O
for	O

Wiktor	O
Stribiżew	O
and	O
Fred	O
Bane	O
and	O
José	O
Conceição	O
and	O
Anna	O
Zaretskaya	O
Transperfect	O
Translations	O
{	O
wstribizew	O
,	O
fbane	O
,	O
jconceicao	O
,	O
azaretskaya	O
}	O
@translations.com	O
different	O
languages	O
,	O
we	O
did	O
not	O
pre	O
-	O
tokenize	O
the	O
data	O
prior	O
to	O
learning	O
the	O
subword	O
model	O
.	O
We	O
tested	O
vocabulary	O
sizes	O
of	O
8000	O
and	O
32000	O
,	O
as	O
well	O
as	O
using	O
shared	O
or	O
split	O
vocabularies	O
for	O
the	O
source	O
and	O
target	O
languages	O
.	O
Character	O
coverage	O
was	O
set	O
to	O
0.9995	O
,	O
the	O
recommended	O
value	O
for	O
languages	O
with	O
extensive	O
character	O
sets	O
such	O
as	O
Chinese	O
and	O
Japanese	O
.	O
For	O
the	O
English	O
Japanese	O
,	O
Korean	O
Japanese	O
,	O
and	O
Chinese	O
Japanese	O
language	O
pairs	O
,	O
we	O
supplemented	O
the	O
corpora	O
with	O
back	O
translation	O
(	O
from	O
Japanese	O
into	O
each	O
language	O
)	O
,	O
which	O
is	O
a	O
common	O
data	B-TaskName
augmentation	I-TaskName
technique	O
in	O
NMT	O
(	O
Sennrich	O
et	O
al	O
,	O
2016	O
)	O
.	O
The	O
back	O
translations	O
were	O
produced	O
by	O
the	O
NMT	O
systems	O
trained	O
for	O
the	O
other	O
three	O
directions	O
(	O
Japanese	O
English	O
,	O
Korean	O
,	O
and	O
Chinese	O
)	O
.	O

In	O
this	O
shared	O
task	O
,	O
we	O
set	O
out	O
to	O
identify	O
a	O
single	O
configuration	O
of	O
hyperparameters	O
that	O
provided	O
the	O
best	O
overall	O
performance	O
across	O
all	O
six	O
language	O
pairs	O
.	O
While	O
this	O
approach	O
precluded	O
the	O
possibility	O
of	O
obtaining	O
optimal	O
performance	O
for	O
all	O
language	O
pairs	O
,	O
it	O
afforded	O
the	O
opportunity	O
to	O
investigate	O
which	O
hyperparameters	O
have	O
similar	O
effects	O
on	O
different	O
language	O
pairs	O
,	O
and	O
which	O
have	O
varied	O
effects	O
on	O
different	O
language	O
pairs	O
.	O
As	O
different	O
language	O
pairs	O
require	O
different	O
hyperparameters	O
,	O
any	O
parameter	O
that	O
can	O
be	O
held	O
fixed	O
during	O
the	O
experimentation	O
stage	O
can	O
create	O
significant	O
savings	O
for	O
companies	O
training	O
their	O
own	O
machine	B-TaskName
translation	I-TaskName
models	O
.	O
For	O
instance	O
,	O
variation	O
in	O
parameters	O
such	O
as	O
learning	B-HyperparameterName
rate	I-HyperparameterName
,	O
dropout	O
,	O
embedding	O
dimensions	O
,	O
and	O
tying	O
the	O
weights	O
of	O
the	O
source	O
and	O
target	O
embedding	O
layers	O
seemed	O
to	O
have	O
similar	O
effects	O
on	O
performance	O
across	O
all	O
language	O
pairs	O
that	O
we	O
tested	O
.	O
Using	O
back	O
translated	O
data	O
to	O
augment	O
the	O
training	O
sets	O
also	O
appeared	O
to	O
be	O
universally	O
beneficial	O
.	O
However	O
,	O
the	O
size	O
of	O
the	O
vocabulary	O
seemed	O
to	O
have	O
quite	O
different	O
effects	O
in	O
different	O
language	O
pairs	O
.	O
We	O
are	O
not	O
aware	O
of	O
any	O
theoretical	O
framework	O
for	O
explaining	O
how	O
the	O
various	O
hyperparameters	O
interact	O
to	O
produce	O
such	O
different	O
results	O
,	O
nor	O
do	O
we	O
know	O
of	O
any	O
way	O
of	O
predicting	O
the	O
optimal	O
hyperparameters	O
for	O
a	O
given	O
language	O
pair	O
other	O
than	O
iterative	O
experimentation	O
.	O
If	O
additional	O
resources	O
are	O
used	O
,	O
several	O
additional	O
steps	O
have	O
also	O
been	O
shown	O
to	O
be	O
effective	O
at	O
boosting	O
performance	O
,	O
but	O
were	O
not	O
employed	O
in	O
these	O
experiments	O
in	O
order	O
to	O
maintain	O
maximum	O
simplicity	O
.	O
These	O
additional	O
steps	O
include	O
using	O
an	O
ensemble	O
of	O
models	O
for	O
decoding	O
,	O
using	O
larger	O
model	O
sizes	O
,	O
performing	O
word	O
segmentation	O
prior	O
to	O
creating	O
the	O
vocabularies	O
,	O
ordering	O
the	O
training	O
data	O
using	O
the	O
output	O
of	O
a	O
language	O
model	O
(	O
a	O
technique	O
referred	O
to	O
as	O
curriculum	O
learning	O
)	O
,	O
and	O
employing	O
an	O
additional	O
model	O
for	O
right	O
-	O
to	O
-	O
left	O
re	O
-	O
ranking	O
.	O
With	O
minimal	O
manual	O
intervention	O
,	O
our	O
models	O
achieved	O
results	O
ranging	O
from	O
fair	O
to	O
excellent	O
.	O
The	O
large	O
variance	O
in	O
the	O
relative	O
performance	O
of	O
these	O
systems	O
shows	O
that	O
no	O
"	O
onesize	O
-	O
fits	O
-	O
all	O
"	O
yet	O
exists	O
for	O
the	O
problem	O
of	O
machine	B-TaskName
translation	I-TaskName
.	O
Despite	O
monumental	O
advances	O
in	O
the	O
field	O
over	O
the	O
past	O
several	O
years	O
,	O
achieving	O
optimal	O
performance	O
requires	O
careful	O
selection	O
of	O
hyperparameters	O
,	O
and	O
different	O
configurations	O
are	O
required	O
for	O
different	O
languages	O
.	O

A	O
Transition	O
-	O
based	O
System	O
for	O
Universal	O
Dependency	B-TaskName
Parsing	I-TaskName

In	O
the	O
pipeline	O
of	O
dealing	O
known	O
languages	O
,	O
the	O
second	O
step	O
is	O
to	O
provide	O
several	O
light	O
-	O
weighted	O
syntactical	O
and	O
morphological	O
features	O
for	O
the	O
tokenized	O
texts	O
,	O
which	O
will	O
be	O
utilized	O
as	O
the	O
input	O
features	O
in	O
the	O
final	O
parsing	O
step	O
.	O
In	O
our	O
system	O
,	O
we	O
adopt	O
the	O
tagger	O
in	O
UDPipe	O
,	O
whose	O
tagging	O
method	O
is	O
based	O
on	O
MorphoDita	O
(	O
Straková	O
et	O
al	O
,	O
2014	O
)	O
and	O
the	O
training	O
method	O
is	O
the	O
classical	O
Averaged	O
Perceptron	O
(	O
Collins	O
,	O
2002	O
)	O
,	O
and	O
the	O
training	O
parameters	O
of	O
UDPipe	O
Tagger	O
are	O
provided	O
in	O
Table	O
2	O
.	O
In	O
this	O
step	O
,	O
the	O
tagger	O
will	O
provide	O
the	O
following	O
outputs	O
:	O
1	O
.	O
Lemma	B-DatasetName
:	O
Lemma	B-DatasetName
or	O
stem	O
of	O
word	O
forms	O
.	O
2	O
.	O
UPOS	O
:	O
Universal	O
POS	O
tags	O
.	O
3	O
.	O
XPOS	O
:	O
Language	O
-	O
specific	O
POS	O
tags	O
.	O

For	O
the	O
final	O
step	O
,	O
we	O
generate	O
the	O
final	O
dependency	O
outputs	O
with	O
the	O
tokens	O
and	O
features	O
generated	O
by	O
the	O
pre	O
-	O
trained	O
POS	O
taggers	O
.	O
The	O
parser	O
uses	O
Parsito	O
(	O
Straka	O
et	O
al	O
,	O
2015b	O
)	O
.	O
Parsito	O
4	O
is	O
a	O
transition	O
-	O
based	O
parser	O
with	O
neural	O
network	O
classifier	O
,	O
which	O
is	O
similar	O
to	O
the	O
one	O
of	O
(	O
Chen	O
and	O
Manning	O
,	O
2014	O
)	O
.	O
The	O
inputs	O
to	O
the	O
model	O
represent	O
the	O
current	O
configuration	O
of	O
the	O
stack	O
and	O
buffer	O
,	O
including	O
features	O
of	O
the	O
top	O
three	O
nodes	O
on	O
both	O
of	O
them	O
and	O
child	O
nodes	O
of	O
the	O
nodes	O
on	O
the	O
stack	O
.	O
After	O
we	O
projected	O
features	O
to	O
embeddings	O
and	O
concatenated	O
the	O
generated	O
embeddings	O
to	O
representations	O
of	O
features	O
,	O
the	O
vector	O
representations	O
of	O
the	O
input	O
are	O
fed	O
to	O
a	O
hidden	O
layer	O
activated	O
with	O
tanh	O
,	O
and	O
the	O
output	O
layer	O
is	O
softmax	B-MethodName
indicating	O
the	O
probabilities	O
of	O
each	O
possible	O
transition	O
actions	O
.	O
The	O
parser	O
supports	O
projective	O
and	O
nonprojective	O
dependency	B-TaskName
parsing	I-TaskName
,	O
which	O
is	O
configured	O
by	O
the	O
option	O
transition	O
system	O
.	O
In	O
Universal	B-DatasetName
Dependencies	I-DatasetName
release	O
2.0	O
,	O
only	O
UD	B-DatasetName
Japanese	O
and	O
UD	B-DatasetName
Galician	O
have	O
no	O
non	O
-	O
projective	O
dependency	O
trees	O
;	O
while	O
UD	B-DatasetName
Chinese	O
,	O
UD	B-DatasetName
Polish	O
and	O
UD	B-DatasetName
Hebrew	O
have	O
a	O
few	O
non	O
-	O
projective	O
trees	O
,	O
around	O
1	O
%	O
in	O
the	O
treebanks	O
.	O
According	O
to	O
the	O
projective	O
tree	O
quantities	O
of	O
the	O
whole	O
treebanks	O
5	O
,	O
we	O
train	O
non	O
-	O
projective	O
parsing	O
for	O
most	O
treebanks	O
except	O
UD	B-DatasetName
Japanese	O
and	O
UD	B-DatasetName
Galician	O
.	O
In	O
projective	O
parsing	O
,	O
we	O
use	O
dynamic	O
oracle	O
which	O
usually	O
performs	O
better	O
but	O
more	O
slowly	O
.	O
In	O
non	O
-	O
projective	O
parsing	O
,	O
we	O
use	O
static	O
lazy	O
and	O
search	O
-	O
based	O
oracle	O
(	O
Straka	O
et	O
al	O
,	O
2015a	O
)	O
.	O
Except	O
transition	O
system	O
option	O
,	O
other	O
configurations	O
of	O
Parsito	O
are	O
the	O
same	O
in	O
all	O
the	O
training	O
of	O
different	O
treebanks	O
.	O
For	O
the	O
structured	O
interval	O
option	O
,	O
we	O
kept	O
the	O
default	O
value	O
8	O
.	O
To	O
make	O
sure	O
that	O
there	O
is	O
a	O
only	O
single	O
root	O
when	O
parsing	O
,	O
single	O
root	O
option	O
is	O
set	O
to	O
1	O
.	O
3	O
.	O

This	O
sub	O
-	O
system	O
deals	O
with	O
the	O
surprise	O
languages	O
without	O
enough	O
training	O
data	O
.	O
We	O
use	O
a	O
simple	O
delexicalized	O
and	O
cross	O
-	O
lingual	O
method	O
,	O
that	O
is	O
,	O
parsing	O
these	O
low	O
resource	O
languages	O
based	O
on	O
the	O
models	O
learned	O
from	O
other	O
languages	O
.	O
This	O
follows	O
the	O
method	O
of	O
(	O
Zeman	O
and	O
Resnik	O
,	O
2008	O
)	O
,	O
which	O
shows	O
that	O
transfer	B-TaskName
learning	I-TaskName
for	O
another	O
language	O
based	O
on	O
delexicalized	O
parser	O
can	O
perform	O
well	O
.	O
Although	O
different	O
languages	O
may	O
have	O
different	O
word	O
forms	O
,	O
the	O
underlying	O
syntactic	O
information	O
could	O
overlap	O
and	O
the	O
universal	O
POS	O
tags	O
could	O
be	O
utilized	O
to	O
explore	O
the	O
correlations	O
.	O
To	O
achieve	O
this	O
,	O
we	O
train	O
a	O
dependency	O
parser	O
in	O
a	O
close	O
-	O
relation	O
language	O
(	O
source	O
language	O
)	O
for	O
a	O
surprise	O
language	O
,	O
and	O
then	O
feed	O
the	O
delexicalized	O
POS	O
tag	O
sequence	O
of	O
the	O
surprise	O
language	O
to	O
the	O
source	O
language	O
parser	O
.	O
We	O
consider	O
language	O
family	O
and	O
close	O
area	O
to	O
find	O
the	O
source	O
language	O
for	O
surprise	O
language	O
.	O

The	O
Strength	O
of	O
the	O
Weakest	O
Supervision	O
:	O
Topic	B-TaskName
Classification	I-TaskName
Using	O
Class	O
Labels	O

When	O
developing	O
topic	O
classifiers	O
for	O
realworld	O
applications	O
,	O
we	O
begin	O
by	O
defining	O
a	O
set	O
of	O
meaningful	O
topic	O
labels	O
.	O
Ideally	O
,	O
an	O
intelligent	O
classifier	O
can	O
understand	O
these	O
labels	O
right	O
away	O
and	O
start	O
classifying	O
documents	O
.	O
Indeed	O
,	O
a	O
human	O
can	O
confidently	O
tell	O
if	O
a	O
news	O
article	O
is	O
about	O
science	O
,	O
politics	O
,	O
sports	O
,	O
or	O
none	O
of	O
the	O
above	O
,	O
after	O
knowing	O
just	O
the	O
class	O
labels	O
.	O
We	O
study	O
the	O
problem	O
of	O
training	O
an	O
initial	O
topic	O
classifier	O
using	O
only	O
class	O
labels	O
.	O
We	O
investigate	O
existing	O
techniques	O
for	O
solving	O
this	O
problem	O
and	O
propose	O
a	O
simple	O
but	O
effective	O
approach	O
.	O
Experiments	O
on	O
a	O
variety	O
of	O
topic	B-TaskName
classification	I-TaskName
data	O
sets	O
show	O
that	O
learning	O
from	O
class	O
labels	O
can	O
save	O
significant	O
initial	O
labeling	O
effort	O
,	O
essentially	O
providing	O
a	O
"	O
free	O
"	O
warm	O
start	O
to	O
the	O
topic	O
classifier	O
.	O

When	O
developing	O
topic	O
classifiers	O
for	O
real	O
-	O
world	O
tasks	O
,	O
such	O
as	O
news	O
categorization	O
,	O
query	O
intent	B-TaskName
detection	I-TaskName
,	O
and	O
user	O
-	O
generated	O
content	O
analysis	O
,	O
practitioners	O
often	O
begin	O
by	O
crafting	O
a	O
succinct	O
definition	O
,	O
or	O
a	O
class	O
label	O
,	O
to	O
define	O
each	O
class	O
.	O
Unfortunately	O
,	O
these	O
carefully	O
written	O
class	O
labels	O
are	O
completely	O
ignored	O
by	O
supervised	O
topic	B-TaskName
classification	I-TaskName
models	O
.	O
Given	O
a	O
new	O
task	O
,	O
these	O
models	O
typically	O
require	O
a	O
significant	O
amount	O
of	O
labeled	O
documents	O
to	O
reach	O
even	O
a	O
modest	O
initial	O
performance	O
.	O
In	O
contrast	O
,	O
a	O
human	O
can	O
readily	O
understand	O
new	O
topic	O
categories	O
by	O
reading	O
the	O
class	O
definitions	O
and	O
making	O
connections	O
to	O
prior	O
knowledge	O
.	O
Labeling	O
initial	O
examples	O
for	O
every	O
new	O
task	O
can	O
be	O
time	O
-	O
consuming	O
and	O
laborintensive	O
,	O
especially	O
in	O
resource	O
-	O
constrained	O
domains	O
like	O
medicine	O
and	O
law	O
.	O
Therefore	O
it	O
is	O
desirable	O
if	O
a	O
topic	O
classifier	O
can	O
proactively	O
interpret	O
class	O
labels	O
before	O
the	O
training	O
starts	O
,	O
giving	O
itself	O
a	O
"	O
warm	O
start	O
"	O
.	O
An	O
imperfect	O
initial	O
model	O
can	O
always	O
be	O
fine	O
-	O
tuned	O
with	O
more	O
labeled	O
documents	O
.	O
As	O
conceptually	O
shown	O
in	O
Figure	O
1	O
,	O
a	O
warm	O
start	O
can	O
reduce	O
the	O
total	O
number	O
of	O
training	O
labels	O
for	O
a	O
classifier	O
to	O
reach	O
certain	O
performance	O
level	O
.	O
In	O
this	O
work	O
,	O
we	O
study	O
algorithms	O
that	O
can	O
initialize	O
a	O
topic	O
classifier	O
using	O
class	O
labels	O
only	O
.	O
Since	O
class	O
labels	O
are	O
the	O
starting	O
point	O
of	O
any	O
topic	B-TaskName
classification	I-TaskName
task	O
,	O
they	O
can	O
be	O
viewed	O
as	O
the	O
earliest	O
hence	O
weakest	O
supervision	O
signal	O
.	O
We	O
propose	O
a	O
simple	O
and	O
effective	O
approach	O
that	O
combines	O
word	O
embedding	O
and	O
naive	O
Bayes	O
classification	O
.	O
On	O
six	O
topic	B-TaskName
classification	I-TaskName
data	O
sets	O
,	O
we	O
evaluate	O
a	O
suite	O
of	O
existing	O
approaches	O
and	O
the	O
proposed	O
approach	O
.	O
Experimental	O
results	O
show	O
that	O
class	O
labels	O
can	O
train	O
a	O
topic	O
classifier	O
that	O
generalizes	O
as	O
well	O
as	O
a	O
classifier	O
trained	O
on	O
hundreds	O
to	O
thousands	O
of	O
labeled	O
documents	O
.	O

Text	O
retrieval	O
.	O
Classifying	O
documents	O
by	O
short	O
labels	O
can	O
be	O
viewed	O
as	O
evaluating	O
textual	O
similarity	O
between	O
a	O
document	O
and	O
a	O
label	O
.	O
Baeza	O
-	O
Yates	O
et	O
al	O
(	O
2011	O
)	O
called	O
this	O
approach	O
"	O
naive	O
text	B-TaskName
classification	I-TaskName
"	O
.	O
Treating	O
labels	O
as	O
search	O
queries	O
,	O
we	O
can	O
classify	O
a	O
document	O
into	O
a	O
class	O
if	O
it	O
best	O
matches	O
the	O
label	O
of	O
that	O
class	O
.	O
Well	O
-	O
studied	O
text	O
retrieval	O
methods	O
,	O
such	O
as	O
vector	O
space	O
models	O
and	O
probabilistic	O
models	O
(	O
Croft	O
et	O
al	O
,	O
2010	O
)	O
,	O
can	O
produce	O
matching	O
scores	O
.	O
To	O
mitigate	O
vocabulary	O
mismatch	O
,	O
such	O
a	O
classifier	O
can	O
be	O
further	O
enhanced	O
by	O
self	O
-	O
training	O
:	O
the	O
classifier	O
assigns	O
pseudo	O
labels	O
to	O
top	O
-	O
ranked	O
documents	O
as	O
done	O
in	O
pseudo	O
relevance	O
feedback	O
(	O
Rocchio	O
,	O
1965	O
)	O
,	O
and	O
updates	O
itself	O
using	O
those	O
labels	O
.	O
Semi	O
-	O
supervised	O
learning	O
.	O
Our	O
problem	O
setting	O
can	O
be	O
seen	O
as	O
an	O
extreme	O
case	O
of	O
weak	O
supervision	O
:	O
we	O
only	O
use	O
class	O
labels	O
as	O
the	O
(	O
noisy	O
)	O
supervision	O
signal	O
,	O
and	O
nothing	O
else	O
.	O
If	O
we	O
view	O
class	O
labels	O
as	O
"	O
labeled	O
documents	O
"	O
,	O
one	O
from	O
each	O
class	O
,	O
and	O
to	O
-	O
be	O
-	O
classified	O
documents	O
as	O
unlabeled	O
documents	O
,	O
then	O
we	O
cast	O
the	O
problem	O
as	O
semisupervised	O
learning	O
(	O
Zhu	O
,	O
2006	O
)	O
.	O
Self	O
-	O
training	O
is	O
one	O
such	O
technique	O
:	O
a	O
generative	O
classifier	O
is	O
trained	O
using	O
only	O
class	O
labels	O
,	O
and	O
then	O
teaches	O
itself	O
using	O
its	O
own	O
predictions	O
on	O
unlabeled	O
data	O
.	O
If	O
we	O
view	O
class	O
labels	O
as	O
"	O
labeled	O
features	O
"	O
,	O
then	O
we	O
expect	O
the	O
classifier	O
to	O
predict	O
a	O
class	O
when	O
a	O
document	O
contains	O
the	O
class	O
label	O
words	O
.	O
For	O
instance	O
,	O
Druck	O
et	O
al	O
(	O
2008	O
)	O
proposed	O
generalized	O
expectation	O
criteria	O
that	O
uses	O
feature	O
words	O
(	O
class	O
labels	O
)	O
to	O
train	O
a	O
discriminative	O
classifier	O
.	O
Jagarlamudi	O
et	O
al	O
(	O
2012	O
)	O
and	O
Hingmire	O
and	O
Chakraborti	O
(	O
2014	O
)	O
proposed	O
Seeded	O
LDA	B-MethodName
to	O
incorporate	O
labeled	O
words	O
/	O
topics	O
into	O
statistical	O
topic	O
modeling	O
.	O
The	O
inferred	O
document	O
-	O
topic	O
mixture	O
probabilities	O
can	O
be	O
used	O
to	O
classify	O
documents	O
.	O
Zero	B-TaskName
-	I-TaskName
shot	I-TaskName
learning	I-TaskName
aims	O
to	O
classify	O
visual	O
objects	O
from	O
a	O
new	O
class	O
using	O
only	O
word	O
descriptions	O
of	O
that	O
class	O
(	O
Socher	O
et	O
al	O
,	O
2013	O
)	O
.	O
It	O
first	O
learns	O
visual	O
features	O
and	O
their	O
correspondence	O
with	O
word	O
descriptions	O
,	O
and	O
then	O
constructs	O
a	O
new	O
classifier	O
by	O
composing	O
learned	O
features	O
.	O
Most	O
research	O
on	O
zero	B-TaskName
-	I-TaskName
shot	I-TaskName
learning	I-TaskName
focuses	O
on	O
image	B-TaskName
classification	I-TaskName
,	O
but	O
the	O
same	O
principle	O
applies	O
to	O
text	B-TaskName
classification	I-TaskName
as	O
well	O
(	O
Pushp	O
and	O
Srivastava	O
,	O
2017	O
)	O
.	O
Our	O
proposed	O
method	O
constructs	O
a	O
new	O
classifier	O
by	O
composing	O
learned	O
word	B-TaskName
embeddings	I-TaskName
in	O
a	O
probabilistic	O
manner	O
.	O
Since	O
the	O
new	O
classifier	O
transfers	O
semantic	O
knowledge	O
in	O
word	O
embedding	O
to	O
topic	B-TaskName
classification	I-TaskName
tasks	O
,	O
it	O
is	O
broadly	O
related	O
to	O
transfer	B-TaskName
learning	I-TaskName
(	O
Pan	O
and	O
Yang	O
,	O
2010	O
)	O
.	O
The	O
main	O
difference	O
is	O
that	O
in	O
transfer	B-TaskName
learning	I-TaskName
the	O
information	O
about	O
the	O
new	O
task	O
is	O
in	O
the	O
form	O
of	O
labeled	O
data	O
,	O
not	O
class	O
definition	O
words	O
.	O

We	O
compare	O
a	O
variety	O
of	O
methods	O
on	O
six	O
topic	B-TaskName
classification	I-TaskName
data	O
sets	O
.	O
The	O
goals	O
are	O
(	O
1	O
)	O
to	O
study	O
the	O
best	O
classification	O
performance	O
achievable	O
using	O
class	O
labels	O
only	O
,	O
and	O
(	O
2	O
)	O
to	O
estimate	O
the	O
equivalent	O
amount	O
of	O
true	O
labels	O
needed	O
to	O
achieve	O
the	O
same	O
warm	O
-	O
start	O
performance	O
.	O

We	O
consider	O
six	O
topic	B-TaskName
classification	I-TaskName
data	O
sets	O
with	O
different	O
document	O
lengths	O
and	O
application	O
domains	O
.	O
Table	O
1	O
summarizes	O
basic	O
statistics	O
of	O
these	O
data	O
sets	O
.	O
Table	O
4	O
and	O
Three	O
short	O
text	O
data	O
sets	O
are	O
(	O
1	O
)	O
Wiki	O
Titles	O
:	O
Wikipedia	O
article	O
titles	O
sampled	O
from	O
15	O
main	O
categories	O
(	O
Wikipedia	O
Main	O
Topic	O
)	O
.	O
(	O
2	O
)	O
News	O
Titles	O
:	O
The	O
UCI	O
news	O
title	O
data	O
set	O
(	O
Lichman	O
,	O
2013	O
)	O
.	O
(	O
3	O
)	O
Y	O
Questions	O
:	O
User	O
-	O
posted	O
questions	O
in	O
Yahoo	O
Answers	O
(	O
Yahoo	O
Language	O
Data	O
,	O
2007	O
)	O
.	O
Three	O
long	O
text	O
data	O
sets	O
are	O
(	O
1	O
)	O
20	O
News	O
:	O
The	O
well	O
-	O
known	O
20	O
newsgroup	O
data	O
set	O
.	O
(	O
2	O
)	O
Reuters	O
.	O
The	O
Reuters	O
-	O
21578	O
data	O
set	O
(	O
Lewis	O
)	O
.	O
We	O
take	O
the	O
articles	O
from	O
the	O
10	O
largest	O
topics	O
.	O
(	O
3	O
)	O
Med	O
WSD	O
:	O
The	O
MeSH	O
word	B-TaskName
sense	I-TaskName
disambiguation	I-TaskName
(	O
WSD	O
)	O
data	O
set	O
(	O
Jimeno	O
-	O
Yepes	O
et	O
al	O
,	O
2011	O
)	O
.	O
Each	O
WSD	O
task	O
aims	O
to	O
tell	O
the	O
sense	O
(	O
meaning	O
)	O
of	O
an	O
ambiguous	O
term	O
in	O
a	O
MEDLINE	O
abstract	O
.	O
For	O
instance	O
,	O
the	O
term	O
"	O
cold	O
"	O
may	O
refer	O
to	O
Low	O
Temperature	O
,	O
Common	O
Cold	O
,	O
or	O
Chronic	O
Obstructive	O
Lung	O
Disease	O
,	O
depending	O
on	O
its	O
context	O
.	O
These	O
senses	O
are	O
used	O
as	O
the	O
class	O
labels	O
.	O
We	O
use	O
198	O
ambiguous	O
words	O
with	O
at	O
least	O
100	O
labeled	O
abstracts	O
in	O
the	O
data	O
set	O
,	O
and	O
report	O
the	O
average	O
statistics	O
over	O
198	O
independent	O
classification	O
tasks	O
.	O
Although	O
no	O
true	O
labels	O
are	O
used	O
for	O
training	O
,	O
some	O
methods	O
require	O
unlabeled	O
data	O
for	O
retrieval	O
,	O
pseudo	O
-	O
labeling	O
,	O
and	O
re	O
-	O
training	O
.	O
We	O
split	O
unlabeled	O
data	O
into	O
5	O
folds	O
,	O
using	O
4	O
folds	O
to	O
"	O
train	O
"	O
a	O
classifier	O
and	O
1	O
fold	O
for	O
test	O
.	O
We	O
use	O
macroaveraged	O
F	O
1	O
as	O
the	O
performance	O
metric	O
because	O
not	O
all	O
data	O
sets	O
have	O
a	O
balanced	O
class	O
distribution	O
.	O

We	O
studied	O
the	O
problem	O
of	O
training	O
topic	O
classifiers	O
using	O
only	O
class	O
labels	O
.	O
Experiments	O
on	O
six	O
data	O
sets	O
show	O
that	O
class	O
labels	O
can	O
save	O
a	O
significant	O
amount	O
of	O
labeled	O
examples	O
in	O
the	O
beginning	O
.	O
Retrieval	O
-	O
based	O
and	O
semi	O
-	O
supervised	O
methods	O
tend	O
to	O
perform	O
better	O
on	O
long	O
documents	O
,	O
while	O
the	O
proposed	O
method	O
performs	O
better	O
on	O
short	O
documents	O
.	O
This	O
study	O
opens	O
up	O
many	O
interesting	O
avenues	O
for	O
future	O
work	O
.	O
First	O
,	O
we	O
introduce	O
a	O
new	O
perspective	O
on	O
text	B-TaskName
classification	I-TaskName
:	O
can	O
we	O
build	O
a	O
text	O
classifier	O
by	O
just	O
providing	O
a	O
short	O
description	O
of	O
each	O
class	O
?	O
This	O
is	O
a	O
more	O
challenging	O
(	O
but	O
more	O
user	O
-	O
friendly	O
)	O
setup	O
than	O
standard	O
supervised	O
classification	O
.	O
Second	O
,	O
future	O
work	O
can	O
investigate	O
tasks	O
such	O
as	O
sentiment	O
and	O
emotion	B-TaskName
classification	I-TaskName
,	O
which	O
are	O
more	O
challenging	O
than	O
topic	B-TaskName
classification	I-TaskName
tasks	O
.	O
Third	O
,	O
the	O
two	O
approaches	O
-	O
leveraging	O
unlabeled	O
data	O
(	O
retrievalbased	O
and	O
semi	O
-	O
supervised	O
methods	O
)	O
and	O
leveraging	O
pretrained	O
models	O
(	O
the	O
proposed	O
method	O
)	O
could	O
be	O
combined	O
to	O
give	O
robust	O
performance	O
on	O
both	O
short	O
and	O
long	O
documents	O
.	O
Finally	O
,	O
we	O
can	O
invite	O
users	O
into	O
the	O
training	O
loop	O
:	O
in	O
addition	O
to	O
labeling	O
documents	O
,	O
users	O
can	O
also	O
revise	O
the	O
class	O
definitions	O
to	O
improve	O
the	O
classifier	O
.	O

STIL	O
-	O
Simultaneous	O
Slot	B-TaskName
Filling	I-TaskName
,	O
Translation	B-TaskName
,	O
Intent	B-TaskName
Classification	I-TaskName
,	O
and	O
Language	B-TaskName
Identification	I-TaskName
:	O
Initial	O
Results	O
using	O
mBART	B-MethodName
on	O
MultiATIS++	O

Multilingual	O
Natural	B-TaskName
Language	I-TaskName
Understanding	I-TaskName
(	O
NLU	O
)	O
,	O
also	O
called	O
cross	O
-	O
lingual	O
NLU	O
,	O
is	O
a	O
technique	O
by	O
which	O
an	O
NLU	O
-	O
based	O
system	O
can	O
scale	O
to	O
multiple	O
languages	O
.	O
A	O
single	O
model	O
is	O
trained	O
on	O
more	O
than	O
one	O
language	O
,	O
and	O
it	O
can	O
accept	O
input	O
from	O
more	O
than	O
one	O
language	O
during	O
inference	O
.	O
In	O
most	O
recent	O
high	O
-	O
performing	O
systems	O
,	O
a	O
model	O
is	O
first	O
pre	O
-	O
trained	O
using	O
unlabeled	O
data	O
for	O
all	O
supported	O
languages	O
and	O
then	O
fine	O
tuned	O
for	O
a	O
specific	O
task	O
using	O
a	O
small	O
set	O
of	O
labeled	O
data	O
(	O
Conneau	O
and	O
Lample	O
,	O
2019	O
;	O
Pires	O
et	O
al	O
,	O
2019	O
)	O
.	O
Two	O
typical	O
tasks	O
for	O
goal	O
-	O
based	O
systems	O
,	O
such	O
as	O
virtual	O
assistants	O
and	O
chatbots	O
,	O
are	O
intent	B-TaskName
classification	I-TaskName
and	O
slot	B-TaskName
filling	I-TaskName
(	O
Gupta	O
et	O
al	O
,	O
2006	O
)	O
.	O
Though	O
intent	B-TaskName
classification	I-TaskName
creates	O
a	O
language	O
agnostic	O
output	O
(	O
the	O
intent	O
of	O
the	O
user	O
)	O
,	O
slot	B-TaskName
filling	I-TaskName
does	O
not	O
.	O
Instead	O
,	O
a	O
slot	O
-	O
filling	O
model	O
outputs	O
the	O
labels	O
for	O
each	O
of	O
input	O
tokens	O
from	O
the	O
user	O
.	O
Suppose	O
the	O
slot	O
-	O
filling	O
model	O
can	O
handle	O
L	O
languages	O
.	O
Downstream	O
components	O
must	O
therefore	O
handle	O
all	O
L	O
languages	O
for	O
the	O
full	O
system	O
to	O
be	O
multilingual	O
across	O
L	O
languages	O
.	O
Machine	B-TaskName
translation	I-TaskName
could	O
be	O
performed	O
before	O
the	O
slot	B-TaskName
filling	I-TaskName
model	O
at	O
system	O
runtime	O
,	O
though	O
the	O
latency	O
would	O
be	O
fully	O
additive	O
,	O
and	O
some	O
amount	O
of	O
information	O
useful	O
to	O
the	O
slotfilling	O
model	O
may	O
be	O
lost	O
.	O
Similarly	O
,	O
translation	O
could	O
occur	O
after	O
the	O
slot	O
-	O
filling	O
model	O
at	O
runtime	O
,	O
but	O
slot	O
alignment	O
between	O
the	O
source	O
and	O
target	O
language	O
is	O
a	O
non	O
-	O
trivial	O
task	O
(	O
Jain	O
et	O
al	O
,	O
2019	O
;	O
Xu	O
et	O
al	O
,	O
2020	O
)	O
.	O
Instead	O
,	O
the	O
goal	O
of	O
this	O
work	O
was	O
to	O
build	O
a	O
single	O
model	O
that	O
can	O
simultaneously	O
translate	O
the	O
input	O
,	O
output	O
slotted	O
text	O
in	O
a	O
single	O
language	O
(	O
English	O
)	O
,	O
classify	O
the	O
intent	O
,	O
and	O
classify	O
the	O
input	O
language	O
(	O
See	O
Table	O
1	O
)	O
.	O
The	O
STIL	O
task	O
is	O
defined	O
such	O
that	O
the	O
input	O
language	O
tag	O
is	O
not	O
given	O
to	O
the	O
model	O
as	O
input	O
.	O
Thus	O
,	O
language	B-TaskName
identification	I-TaskName
is	O
necessary	O
so	O
that	O
the	O
system	O
can	O
communicate	O
back	O
to	O
the	O
user	O
in	O
the	O
correct	O
language	O
.	O
In	O
all	O
STIL	O
cases	O
,	O
the	O
output	O
is	O
in	O
English	O
.	O
Each	O
token	O
is	O
followed	O
by	O
its	O
BIO	O
-	O
tagged	O
slot	O
label	O
.	O
The	O
sequence	O
of	O
tokens	O
and	O
slots	O
are	O
followed	O
by	O
the	O
intent	O
and	O
then	O
the	O
language	O
.	O
sification	O
,	O
and	O
Language	B-TaskName
identification	I-TaskName
(	O
STIL	O
)	O
;	O
(	O
2	O
)	O
both	O
non	O
-	O
translated	O
and	O
STIL	O
results	O
using	O
the	O
mBART	B-MethodName
model	O
(	O
Liu	O
et	O
al	O
,	O
2020	O
)	O
trained	O
using	O
a	O
fully	O
text	O
-	O
to	O
-	O
text	O
data	O
format	O
;	O
and	O
(	O
3	O
)	O
public	O
release	O
of	O
source	O
code	O
used	O
in	O
this	O
study	O
,	O
with	O
a	O
goal	O
toward	O
reproducibility	O
and	O
future	O
work	O
on	O
the	O
STIL	O
task	O
1	O
.	O

The	O
Airline	O
Travel	O
Information	O
System	O
(	O
ATIS	B-DatasetName
)	O
dataset	O
is	O
a	O
classic	O
benchmark	O
for	O
goal	O
-	O
oriented	O
NLU	O
(	O
Price	O
,	O
1990	O
;	O
Tur	O
et	O
al	O
,	O
2010	O
)	O
.	O
It	O
contains	O
utterances	O
focused	O
on	O
airline	O
travel	O
,	O
such	O
as	O
how	O
much	O
is	O
the	O
cheapest	O
flight	O
from	O
Boston	O
to	O
New	O
York	O
tomorrow	O
morning	O
?	O
The	O
dataset	O
is	O
annotated	O
with	O
17	O
intents	O
,	O
though	O
the	O
distribution	O
is	O
skewed	O
,	O
with	O
70	O
%	O
of	O
intents	O
being	O
the	O
flight	O
intent	O
.	O
Slots	O
are	O
labeled	O
using	O
the	O
Beginning	O
Inside	O
Outside	O
(	O
BIO	O
)	O
format	O
.	O
ATIS	B-DatasetName
was	O
localized	O
to	O
Turkish	O
and	O
Hindi	O
in	O
2018	O
,	O
forming	O
MultiATIS	O
(	O
Upadhyay	O
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
then	O
to	O
Spanish	O
,	O
Portuguese	O
,	O
German	O
,	O
French	O
,	O
Chinese	O
,	O
and	O
Japanese	O
in	O
2020	O
,	O
forming	O
Multi	O
-	O
ATIS++	O
(	O
Xu	O
et	O
al	O
,	O
2020	O
)	O
.	O
In	O
this	O
work	O
,	O
Portuguese	O
was	O
excluded	O
due	O
to	O
a	O
lack	O
of	O
Portuguese	O
pretraining	O
in	O
the	O
publicly	O
available	O
mBART	B-MethodName
model	O
,	O
and	O
Japanese	O
was	O
excluded	O
due	O
to	O
a	O
current	O
lack	O
of	O
alignment	O
between	O
Japanese	O
and	O
English	O
samples	O
in	O
MultiATIS++	O
.	O
Hindi	O
and	O
Turkish	O
data	O
were	O
taken	O
from	O
Multi	O
-	O
ATIS	B-DatasetName
,	O
and	O
the	O
training	O
data	O
were	O
upsampled	O
by	O
3x	O
for	O
Hindi	O
and	O
7x	O
for	O
Turkish	O
.	O
Prior	O
to	O
any	O
upsampling	O
,	O
there	O
were	O
4	O
,	O
488	O
training	O
samples	O
for	O
English	O
,	O
Spanish	O
,	O
German	O
,	O
French	O
,	O
and	O
Chinese	O
.	O
The	O
test	O
sets	O
contained	O
893	O
samples	O
for	O
all	O
languages	O
except	O
Turkish	O
,	O
which	O
had	O
715	O
samples	O
.	O
For	O
English	O
,	O
Spanish	O
,	O
German	O
,	O
French	O
,	O
and	O
Chinese	O
,	O
validation	O
sets	O
of	O
490	O
samples	O
were	O
used	O
in	O
all	O
cases	O
.	O
Given	O
the	O
smaller	O
data	O
quantities	O
for	O
Hindi	O
and	O
Turkish	O
,	O
two	O
training	O
and	O
validation	O
set	O
configurations	O
were	O
considered	O
.	O
The	O
first	O
configuration	O

Previous	O
approaches	O
for	O
intent	B-TaskName
classification	I-TaskName
and	O
slot	B-TaskName
filling	I-TaskName
have	O
used	O
either	O
(	O
1	O
)	O
separate	O
models	O
for	O
slot	B-TaskName
filling	I-TaskName
,	O
including	O
support	O
vector	O
machines	O
(	O
Moschitti	O
et	O
al	O
,	O
2007	O
)	O
,	O
conditional	O
random	O
fields	O
(	O
Xu	O
and	O
Sarikaya	O
,	O
2014	O
)	O
,	O
and	O
recurrent	O
neural	O
networks	O
of	O
various	O
types	O
(	O
Kurata	O
et	O
al	O
,	O
2016	O
)	O
or	O
(	O
2	O
)	O
joint	O
models	O
that	O
diverge	O
into	O
separate	O
decoders	O
or	O
layers	O
for	O
intent	B-TaskName
classification	I-TaskName
and	O
slot	B-TaskName
filling	I-TaskName
(	O
Xu	O
and	O
Sarikaya	O
,	O
2013	O
;	O
Guo	O
et	O
al	O
,	O
2014	O
;	O
Liu	O
and	O
Lane	O
,	O
2016	O
;	O
Hakkani	O
-	O
Tür	O
et	O
al	O
,	O
2016	O
)	O
or	O
that	O
share	O
hidden	O
states	O
(	O
Wang	O
et	O
al	O
,	O
2018	O
)	O
.	O
In	O
this	O
work	O
,	O
a	O
fully	O
text	O
-	O
to	O
-	O
text	O
approach	O
similar	O
to	O
that	O
of	O
the	O
T5	B-MethodName
model	O
was	O
used	O
,	O
such	O
that	O
the	O
model	O
would	O
have	O
maximum	O
information	O
sharing	O
across	O
the	O
four	O
STIL	O
sub	O
-	O
tasks	O
.	O
Encoder	O
-	O
decoder	O
models	O
,	O
first	O
introduced	O
in	O
2014	O
(	O
Sutskever	O
et	O
al	O
,	O
2014	O
)	O
,	O
are	O
a	O
mainstay	O
of	O
neural	O
machine	B-TaskName
translation	I-TaskName
.	O
The	O
original	O
transformer	O
model	O
included	O
both	O
an	O
encoder	O
and	O
a	O
decoder	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
Since	O
then	O
,	O
much	O
of	O
the	O
work	O
on	O
transformers	O
focuses	O
on	O
models	O
with	O
only	O
an	O
encoder	O
pretrained	O
with	O
autoencoding	O
techniques	O
(	O
e.g.	O
BERT	B-MethodName
by	O
Devlin	O
et	O
al	O
(	O
2018	O
)	O
)	O
or	O
auto	O
-	O
regressive	O
models	O
with	O
only	O
a	O
decoder	O
(	O
e.g.	O
GPT	B-MethodName
by	O
Radford	O
(	O
2018	O
)	O
)	O
.	O
In	O
this	O
work	O
,	O
it	O
was	O
assumed	O
that	O
encoder	O
-	O
decoder	O
models	O
,	O
such	O
as	O
BART	B-MethodName
(	O
Lewis	O
et	O
al	O
,	O
2019	O
)	O
and	O
T5	B-MethodName
(	O
Raffel	O
et	O
al	O
,	O
2019	O
)	O
,	O
are	O
the	O
best	O
architectural	O
candidates	O
given	O
the	O
translation	O
component	O
of	O
the	O
STIL	O
task	O
,	O
as	O
well	O
as	O
past	O
state	O
of	O
the	O
art	O
advancement	O
by	O
encoder	O
-	O
decoder	O
models	O
on	O
ATIS	B-DatasetName
,	O
cited	O
above	O
.	O
Rigorous	O
architectural	O
comparisons	O
are	O
left	O
to	O
future	O
work	O
.	O

This	O
preliminary	O
work	O
demonstrates	O
that	O
a	O
single	O
NLU	O
model	O
can	O
perform	O
simultaneous	O
slot	B-TaskName
filling	I-TaskName
,	O
translation	O
,	O
intent	B-TaskName
classification	I-TaskName
,	O
and	O
language	B-TaskName
identification	I-TaskName
across	O
7	O
languages	O
using	O
MultiATIS++	O
.	O
Such	O
an	O
NLU	O
model	O
would	O
negate	O
the	O
need	O
for	O
multiple	O
-	O
language	O
support	O
in	O
some	O
portion	O
of	O
downstream	O
system	O
components	O
.	O
Performance	O
is	O
not	O
irreconcilably	O
worse	O
than	O
traditional	O
slot	O
-	O
filling	O
models	O
,	O
and	O
performance	O
is	O
statistically	O
equivalent	O
with	O
a	O
small	O
amount	O
of	O
additional	O
training	O
data	O
.	O
Looking	O
forward	O
,	O
a	O
more	O
challenging	O
dataset	O
is	O
needed	O
to	O
further	O
develop	O
the	O
translation	O
compo	O
-	O
nent	O
of	O
the	O
STIL	O
task	O
.	O
The	O
English	O
MultiATIS++	O
test	O
set	O
only	O
contains	O
455	O
unique	O
entity	O
-	O
slot	O
pairs	O
.	O
An	O
ideal	O
future	O
dataset	O
would	O
include	O
freeform	O
and	O
varied	O
content	O
,	O
such	O
as	O
text	O
messages	O
,	O
song	O
titles	O
,	O
or	O
open	O
-	O
domain	O
questions	O
.	O
Until	O
then	O
,	O
work	O
remains	O
to	O
achieve	O
parity	O
with	O
English	O
-	O
only	O
ATIS	B-DatasetName
models	O
.	O

We	O
choose	O
the	O
parameterization	O
of	O
the	O
distribution	O
of	O
diplomatic	O
words	O
in	O
order	O
to	O
capture	O
two	O
types	O
of	O
spelling	O
preference	O
:	O
(	O
1	O
)	O
general	O
preferences	O
for	O
certain	O
character	O
groups	O
(	O
such	O
as	O
-	O
ie	O
)	O
and	O
(	O
2	O
)	O
preferences	O
that	O
only	O
pertain	O
to	O
a	O
particular	O
word	O
and	O
do	O
not	O
indicate	O
a	O
larger	O
pattern	O
.	O
Since	O
it	O
is	O
unknown	O
which	O
of	O
the	O
two	O
behaviors	O
is	O
dominant	O
,	O
we	O
let	O
the	O
model	O
describe	O
both	O
and	O
learn	O
to	O
separate	O
their	O
effects	O
.	O
Using	O
a	O
log	O
-	O
linear	O
parameterization	O
,	O
P	O
(	O
d	O
|	O
m	O
,	O
c	O
;	O
w	O
)	O
∝	O
exp	O
(	O
w	O
c	O
f	O
(	O
m	O
,	O
d	O
)	O
)	O
we	O
introduce	O
features	O
to	O
capture	O
both	O
effects	O
.	O
Here	O
,	O
f	O
(	O
m	O
,	O
d	O
)	O
is	O
a	O
feature	O
function	O
defined	O
on	O
modern	O
word	O
m	O
paired	O
with	O
diplomatic	O
word	O
d	O
,	O
while	O
w	O
c	O
is	O
a	O
weight	O
vector	O
corresponding	O
to	O
compositor	O
c.	O
To	O
capture	O
word	O
-	O
specific	O
preferences	O
we	O
add	O
an	O
indicator	O
feature	O
for	O
each	O
pair	O
of	O
modern	O
word	O
m	O
and	O
diplomatic	O
spelling	O
d.	O
We	O
refer	O
to	O
these	O
as	O
WORD	B-DatasetName
features	O
below	O
.	O
To	O
capture	O
general	O
orthographic	O
preferences	O
we	O
introduce	O
an	O
additional	O
set	O
of	O
features	O
based	O
on	O
the	O
edit	O
operations	O
involved	O
in	O
the	O
computation	O
of	O
Levenshtein	O
distance	O
between	O
m	O
and	O
d.	O
In	O
particular	O
,	O
each	O
operation	O
is	O
added	O
as	O
a	O
separate	O
feature	O
,	O
both	O
with	O
and	O
without	O
local	O
context	O
(	O
previous	O
or	O
next	O
character	O
of	O
the	O
modern	O
word	O
)	O
.	O
We	O
refer	O
to	O
this	O
group	O
as	O
EDIT	O
features	O
.	O
The	O
weight	O
vector	O
for	O
each	O
compositor	O
represents	O
their	O
unique	O
biases	O
,	O
as	O
shown	O
in	O
the	O
depiction	O
of	O
these	O
parameters	O
in	O
Figure	O
2	O
.	O

The	O
media	O
environment	O
has	O
grown	O
increasingly	O
polarized	O
in	O
recent	O
years	O
,	O
creating	O
social	O
,	O
cultural	O
and	O
political	O
divisions	O
(	O
Prior	O
,	O
2013	O
;	O
Fiorina	O
and	O
Abrams	O
,	O
2008	O
)	O
.	O
Although	O
a	O
diversity	O
of	O
opinions	O
is	O
healthy	O
,	O
and	O
even	O
necessary	O
for	O
democratic	O
discourse	O
,	O
unchecked	O
polarization	O
can	O
paralyze	O
society	O
by	O
suppressing	O
consensus	O
required	O
for	O
effective	O
governance	O
(	O
Tworzecki	O
,	O
2019	O
)	O
.	O
In	O
more	O
extreme	O
cases	O
,	O
polarization	O
leads	O
to	O
disagreement	O
,	O
conflict	O
and	O
even	O
violence	O
.	O
The	O
COVID	O
-	O
19	O
pandemic	O
has	O
exposed	O
many	O
of	O
our	O
vulnerabilities	O
to	O
the	O
pernicious	O
effects	O
of	O
polarization	O
.	O
Public	O
opinions	O
about	O
COVID	O
-	O
19	O
(	O
Jiang	O
et	O
al	O
,	O
2020	O
)	O
,	O
as	O
well	O
as	O
messaging	O
by	O
political	O
elites	O
(	O
Green	O
et	O
al	O
,	O
2020	O
;	O
Bhanot	O
1	O
Code	O
and	O
data	O
are	O
publicly	O
available	O
at	O
https://github.com/ZagHe568/	O
pacte	O
-	O
polarized	O
-	O
topics	O
-	O
detection	O
.	O
and	O
Hopkins	O
,	O
2020	O
)	O
,	O
are	O
sharply	O
divided	O
along	O
partisan	O
lines	O
.	O
According	O
to	O
a	O
Pew	O
Report	O
(	O
Jurkowitz	O
et	O
al	O
,	O
2020	O
)	O
,	O
partisanship	O
significantly	O
explains	O
attitudes	O
about	O
the	O
costs	O
and	O
benefits	O
of	O
various	O
mitigation	O
strategies	O
,	O
including	O
non	O
-	O
pharmaceutical	O
interventions	O
and	O
lockdowns	O
,	O
and	O
even	O
explains	O
regional	O
differences	O
in	O
the	O
pandemic	O
's	O
toll	O
in	O
the	O
US	O
(	O
Gollwitzer	O
et	O
al	O
,	O
2020	O
)	O
.	O
In	O
mass	O
media	O
a	O
variety	O
of	O
topics	O
is	O
discussed	O
every	O
day	O
,	O
and	O
polarization	O
can	O
form	O
on	O
different	O
topics	O
.	O
Therefore	O
,	O
identifying	O
nascent	O
disagreements	O
and	O
growing	O
controversies	O
of	O
different	O
topics	O
in	O
news	O
media	O
and	O
public	O
discourse	O
would	O
help	O
journalists	O
craft	O
more	O
balanced	O
news	O
coverage	O
(	O
Lorenz	O
-	O
Spreen	O
et	O
al	O
,	O
2020	O
;	O
.	O
Different	O
from	O
previous	O
works	O
that	O
study	O
polarization	O
from	O
a	O
more	O
coarse	O
-	O
grained	O
perspective	O
,	O
Demszky	O
et	O
al	O
(	O
2019	O
)	O
were	O
the	O
first	O
to	O
study	O
polarized	O
topics	O
using	O
tweets	O
about	O
21	O
mass	O
shootings	O
to	O
show	O
that	O
some	O
topics	O
were	O
more	O
polarized	O
than	O
others	O
.	O
However	O
,	O
their	O
approach	O
to	O
represent	O
semantic	O
information	O
with	O
word	O
frequencies	O
is	O
less	O
expressive	O
than	O
modern	O
methods	O
allow	O
.	O
To	O
better	O
capture	O
the	O
topical	O
polarization	O
among	O
partisan	O
(	O
liberal	O
vs.	O
conservative	O
)	O
media	O
sources	O
,	O
we	O
propose	O
Partisanship	O
-	O
aware	O
Contextualized	O
Topic	O
Embeddings	O
(	O
PaCTE	O
)	O
.	O
Specifically	O
,	O
given	O
a	O
text	O
corpus	O
containing	O
news	O
articles	O
from	O
both	O
sides	O
,	O
we	O
first	O
extract	O
a	O
set	O
of	O
topics	O
utilizing	O
LDA	B-MethodName
topic	O
modeling	O
(	O
Blei	O
et	O
al	O
,	O
2003	O
)	O
.	O
Next	O
,	O
we	O
finetune	O
a	O
pretrained	O
language	O
model	O
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
to	O
recognize	O
the	O
partisanship	O
of	O
the	O
news	O
articles	O
so	O
as	O
to	O
render	O
it	O
partisanship	O
-	O
aware	O
.	O
Then	O
for	O
each	O
article	O
,	O
we	O
represent	O
its	O
ideology	O
on	O
a	O
topic	O
by	O
a	O
vector	O
,	O
called	O
document	O
-	O
contextualized	O
2	O
(	O
DC	O
)	O
topic	O
embedding	O
,	O
by	O
aggregating	O
language	O
model	O
representations	O
of	O
the	O
topic	O
keywords	O
contextualized	O
by	O
the	O
article	O
.	O
Such	O
a	O
representation	O
sheds	O
light	O
primarily	O
on	O
the	O
tokens	O
that	O
appear	O
in	O
the	O
topic	O
keywords	O
and	O
thus	O
concentrates	O
on	O
the	O
topic	O
-	O
oriented	O
local	O
semantics	O
in	O
the	O
context	O
of	O
the	O
article	O
,	O
instead	O
of	O
the	O
global	O
semantics	O
from	O
the	O
article	O
that	O
might	O
contain	O
irrelevant	O
and	O
noisy	O
information	O
.	O
We	O
further	O
represent	O
the	O
ideology	O
of	O
the	O
news	O
corpus	O
on	O
the	O
topic	O
,	O
what	O
we	O
call	O
corpus	O
-	O
contextualized	O
(	O
CC	O
)	O
topic	O
embedding	O
,	O
by	O
aggregating	O
the	O
DC	O
topic	O
embeddings	O
.	O
As	O
a	O
result	O
,	O
the	O
ideology	O
of	O
the	O
news	O
corpus	O
on	O
a	O
topic	O
is	O
represented	O
by	O
a	O
single	O
vector	O
.	O
Finally	O
,	O
we	O
measure	O
the	O
polarization	O
between	O
two	O
news	O
sources	O
on	O
the	O
topic	O
using	O
the	O
cosine	O
distance	O
between	O
such	O
vectors	O
.	O
For	O
evaluation	O
,	O
we	O
create	O
ground	O
truth	O
by	O
annotating	O
the	O
polarization	O
of	O
pairs	O
of	O
partisan	O
news	O
sources	O
on	O
a	O
variety	O
of	O
topics	O
.	O
We	O
evaluate	O
the	O
topic	O
polarization	O
scores	O
produced	O
by	O
PaCTE	O
against	O
the	O
ground	O
truth	O
on	O
the	O
task	O
of	O
polarized	O
topics	O
retrieval	O
.	O
Experiments	O
on	O
nine	O
pairs	O
of	O
partisan	O
news	O
sources	O
demonstrate	O
that	O
compared	O
to	O
baselines	O
,	O
PaCTE	O
is	O
more	O
effective	O
in	O
capturing	O
topic	O
polarization	O
and	O
retrieving	O
polarized	O
topics	O
.	O
We	O
argue	O
that	O
public	O
media	O
watchdogs	O
and	O
social	O
media	O
platforms	O
can	O
utilize	O
such	O
a	O
simple	O
-	O
yet	O
-	O
effective	O
tool	O
to	O
flag	O
discussions	O
that	O
have	O
grown	O
divisive	O
so	O
that	O
action	O
could	O
be	O
taken	O
to	O
reduce	O
partisan	O
divisions	O
and	O
improve	O
civil	O
discourse	O
.	O

The	O
partisan	O
polarization	O
in	O
the	O
US	O
media	O
is	O
a	O
widely	O
studied	O
topic	O
(	O
Hollander	O
,	O
2008	O
;	O
Stroud	O
,	O
2011	O
)	O
.	O
During	O
the	O
onset	O
of	O
the	O
COVID	O
-	O
19	O
pandemic	O
,	O
the	O
polarization	O
among	O
the	O
political	O
elites	O
and	O
the	O
news	O
media	O
causes	O
a	O
lot	O
of	O
confusion	O
.	O
For	O
example	O
,	O
Hart	O
et	O
al	O
(	O
2020	O
)	O
show	O
that	O
COVID	O
-	O
19	O
media	O
coverage	O
is	O
politicized	O
and	O
polarized	O
.	O
Other	O
works	O
have	O
been	O
studying	O
the	O
polarization	O
in	O
media	O
from	O
different	O
perspectives	O
.	O
Focusing	O
on	O
the	O
differences	O
in	O
the	O
languages	O
of	O
liberals	O
and	O
conservatives	O
,	O
KhudaBukhsh	O
et	O
al	O
(	O
2020	O
)	O
analyze	O
political	O
polarization	O
on	O
YouTube	O
using	O
machine	B-TaskName
translation	I-TaskName
tools	O
.	O
To	O
analyze	O
how	O
the	O
news	O
outlets	O
frame	O
the	O
events	O
differently	O
,	O
Fan	O
et	O
al	O
(	O
2019	O
)	O
have	O
collected	O
and	O
labeled	O
100	O
triplets	O
of	O
news	O
articles	O
each	O
discussing	O
the	O
same	O
event	O
from	O
three	O
news	O
sources	O
bearing	O
different	O
political	O
ideologies	O
.	O
In	O
addition	O
to	O
qualitatively	O
analyzing	O
polarization	O
,	O
different	O
approaches	O
to	O
quantifying	O
polarization	O
have	O
also	O
been	O
proposed	O
.	O
propose	O
two	O
different	O
ways	O
,	O
namely	O
the	O
leave	O
-	O
out	O
estimator	O
and	O
the	O
multinomial	O
regression	O
,	O
to	O
measure	O
the	O
trends	O
of	O
partisanship	O
in	O
congressional	O
speech	O
.	O
Green	O
et	O
al	O
(	O
2020	O
)	O
define	O
the	O
po	O
-	O
larization	O
as	O
one	O
's	O
ability	O
to	O
identify	O
the	O
partisanship	O
of	O
a	O
tweet	O
's	O
author	O
based	O
on	O
the	O
contents	O
of	O
tweets	O
and	O
investigate	O
the	O
polarization	O
regarding	O
COVID	O
-	O
19	O
among	O
political	O
elites	O
on	O
Twitter	O
.	O
Demszky	O
et	O
al	O
(	O
2019	O
)	O
first	O
measure	O
topic	O
-	O
wise	O
polarization	O
using	O
the	O
leave	O
-	O
out	O
estimator	O
proposed	O
by	O
;	O
however	O
,	O
they	O
use	O
a	O
token	O
frequency	O
vector	O
to	O
represent	O
an	O
article	O
,	O
which	O
is	O
less	O
expressive	O
and	O
fails	O
to	O
make	O
use	O
of	O
the	O
rich	O
semantics	O
in	O
the	O
context	O
and	O
the	O
pre	O
-	O
knowledge	O
in	O
pretrained	B-TaskName
language	I-TaskName
models	I-TaskName
(	O
Devlin	O
et	O
al	O
,	O
2018	O
;	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
or	O
pretrained	O
word	B-TaskName
embeddings	I-TaskName
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
;	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
;	O
furthermore	O
,	O
they	O
represent	O
the	O
topic	O
using	O
the	O
token	O
frequency	O
vector	O
of	O
the	O
entire	O
document	O
,	O
thus	O
incurring	O
noisy	O
information	O
that	O
might	O
smooth	O
over	O
the	O
target	O
semantics	O
in	O
the	O
locality	O
of	O
topic	O
keywords	O
.	O
In	O
contrast	O
,	O
our	O
method	O
represents	O
the	O
topic	O
embedding	O
in	O
the	O
context	O
of	O
a	O
document	O
,	O
thus	O
generating	O
topic	O
representations	O
with	O
more	O
attention	O
to	O
the	O
target	O
topic	O
keywords	O
as	O
well	O
as	O
making	O
use	O
of	O
the	O
contextualized	O
semantics	O
from	O
the	O
document	O
,	O
as	O
captured	O
by	O
the	O
contextualized	O
embeddings	O
.	O
Some	O
works	O
have	O
proposed	O
contextualized	O
embeddings	O
to	O
enhance	O
the	O
quality	O
of	O
neural	O
topic	B-TaskName
models	I-TaskName
(	O
Bianchi	O
et	O
al	O
,	O
2020	O
;	O
Chaudhary	O
et	O
al	O
,	O
2020	O
)	O
.	O
However	O
,	O
the	O
scope	O
of	O
this	O
work	O
is	O
to	O
generate	O
better	O
contextualize	O
topic	O
embeddings	O
for	O
articles	O
to	O
capture	O
topic	O
polarization	O
,	O
with	O
a	O
given	O
topic	O
model	O
;	O
the	O
exploration	O
of	O
other	O
topic	O
modeling	O
techniques	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
work	O
.	O

The	O
proposed	O
PaCTE	O
framework	O
consists	O
of	O
four	O
components	O
:	O
1	O
)	O
LDA	B-MethodName
Topic	O
Modeling	O
,	O
2	O
)	O
Partisanship	O
Learning	O
,	O
3	O
)	O
Partisanship	O
-	O
aware	O
Contextualized	O
Topic	O
Embedding	O
Generation	O
,	O
and	O
4	O
)	O
Measuring	O
Polarization	O
and	O
Ranking	O
Topics	O
.	O
The	O
overall	O
framework	O
is	O
illustrated	O
in	O
Figure	O
1	O
.	O
In	O
this	O
section	O
we	O
elaborate	O
on	O
each	O
component	O
in	O
detail	O
.	O

We	O
train	O
an	O
LDA	B-MethodName
topic	O
model	O
using	O
the	O
the	O
combined	O
corpus	O
D	O
C	O
=	O
D	O
L	O
∪D	O
R	O
and	O
extract	O
K	O
topics	O
T	O
=	O
{	O
t	O
i	O
}	O
K	O
i=1	O
,	O
where	O
t	O
i	O
is	O
a	O
topic	O
.	O
The	O
modeled	O
topics	O
T	O
apply	O
to	O
both	O
D	O
L	O
and	O
D	O
R	O
.	O
An	O
example	O
is	O
given	O
in	O
Figure	O
1	O
(	O
a	O
)	O
.	O
Representing	O
a	O
topic	O
by	O
keywords	O
.	O
A	O
topic	O
t	O
i	O
is	O
represented	O
as	O
a	O
distribution	O
of	O
keywords	O
from	O
the	O
global	O
vocabulary	O
of	O
D	O
C	O
and	O
we	O
only	O
keep	O
the	O
top	O
-	O
m	O
keywords	O
:	O
t	O
i	O
=	O
(	O
(	O
p	O
ij	O
,	O
w	O
j	O
)	O
)	O
m	O
j=1	O
,	O
p	O
ij	O
>	O
p	O
ik	O
⇔	O
j	O
<	O
k	O
,	O
(	O
2	O
)	O
where	O
p	O
ij	O
is	O
the	O
probability	O
of	O
observing	O
keyword	O
w	O
j	O
given	O
topic	O
t	O
i	O
.	O
Representing	O
a	O
topic	O
by	O
documents	O
.	O
A	O
document	O
d	O
D	O
C	O
is	O
represented	O
as	O
a	O
distribution	O
over	O
the	O
K	O
topics	O
.	O
Accordingly	O
,	O
we	O
renormalize	O
the	O
probabilities	O
and	O
represent	O
each	O
topic	O
t	O
i	O
as	O
an	O
(	O
inverse	O
)	O
distribution	O
of	O
documents	O
in	O
D	O
C	O
and	O
only	O
keep	O
the	O
top	O
-	O
n	O
most	O
relevant	O
documents	O
,	O
such	O
that	O
t	O
D	O
C	O
i	O
=	O
(	O
(	O
q	O
D	O
C	O
ij	O
,	O
d	O
j	O
)	O
)	O
n	O
j=1	O
,	O
q	O
D	O
C	O
ij	O
>	O
q	O
D	O
C	O
ik	O
⇔	O
j	O
<	O
k	O
,	O
(	O
3	O
)	O
where	O
q	O
D	O
ij	O
is	O
the	O
probability	O
of	O
observing	O
document	O
d	O
j	O
D	O
C	O
given	O
topic	O
t	O
i	O
.	O
Because	O
our	O
goal	O
is	O
to	O
study	O
the	O
polarization	O
between	O
D	O
L	O
and	O
D	O
R	O
,	O
instead	O
of	O
using	O
the	O
global	O
documents	O
in	O
D	O
C	O
,	O
we	O
represent	O
a	O
topic	O
by	O
the	O
top	O
-	O
n	O
documents	O
in	O
D	O
L	O
and	O
D	O
R	O
separately	O
and	O
thus	O
obtain	O
t	O
D	O
L	O
i	O
and	O
t	O
D	O
R	O
i	O
accordingly	O
.	O

Denote	O
the	O
ideology	O
embedding	O
of	O
A	O
on	O
B	O
as	O
H	O
A	O
(	O
B	O
)	O
,	O
where	O
A	O
represents	O
a	O
news	O
corpus	O
or	O
a	O
document	O
and	O
B	O
represents	O
a	O
topic	O
or	O
a	O
topic	O
keyword	O
.	O
We	O
then	O
represent	O
the	O
ideology	O
of	O
a	O
corpus	O
D	O
on	O
a	O
topic	O
t	O
as	O
corpus	O
-	O
contextualized	O
(	O
CC	O
)	O
topic	O
embedding	O
H	O
D	O
(	O
t	O
)	O
,	O
the	O
ideology	O
of	O
a	O
document	O
d	O
on	O
a	O
topic	O
t	O
as	O
document	O
-	O
contextualized	O
(	O
DC	O
)	O
topic	O
embedding	O
H	O
d	O
(	O
t	O
)	O
,	O
and	O
the	O
ideology	O
of	O
a	O
document	O
d	O
on	O
a	O
topic	O
keyword	O
w	O
as	O
DC	O
keyword	O
embedding	O
H	O
d	O
(	O
w	O
)	O
.	O
We	O
will	O
elaborate	O
on	O
how	O
the	O
CC	O
topic	O
embedding	O
is	O
obtained	O
from	O
a	O
top	O
-	O
down	O
perspective	O
.	O
According	O
to	O
Equation	O
3	O
,	O
in	O
order	O
to	O
compute	O
the	O
CC	O
topic	O
embedding	O
H	O
D	O
(	O
t	O
i	O
)	O
,	O
we	O
can	O
rewrite	O
it	O
as	O
H	O
D	O
(	O
t	O
i	O
)	O
=	O
n	O
j=1	O
q	O
D	O
ij	O
H	O
d	O
j	O
(	O
t	O
i	O
)	O
.	O
(	O
4	O
)	O
Hence	O
,	O
we	O
decompose	O
a	O
CC	O
topic	O
embedding	O
into	O
DC	O
topic	O
embeddings	O
from	O
the	O
top	O
-	O
n	O
most	O
relevant	O
documents	O
.	O
To	O
obtain	O
the	O
DC	O
topic	O
embedding	O
,	O
Demszky	O
et	O
al	O
(	O
2019	O
)	O
use	O
word	O
frequency	O
vectors	O
;	O
Grootendorst	O
(	O
2020	O
)	O
takes	O
the	O
[	O
CLS	O
]	O
embedding	O
of	O
a	O
pretrained	O
language	O
model	O
that	O
gives	O
a	O
holistic	O
document	B-TaskName
embedding	I-TaskName
without	O
encoding	O
the	O
context	O
of	O
a	O
topic	O
.	O
However	O
,	O
while	O
word	O
frequency	O
vectors	O
encode	O
statistical	O
features	O
of	O
words	O
in	O
the	O
document	O
,	O
they	O
neglect	O
their	O
context	O
.	O
In	O
addition	O
,	O
a	O
document	O
is	O
likely	O
to	O
be	O
associated	O
with	O
multiple	O
topics	O
according	O
to	O
the	O
LDA	B-MethodName
topic	O
model	O
,	O
and	O
therefore	O
using	O
the	O
holistic	O
document	B-TaskName
embedding	I-TaskName
as	O
the	O
topic	O
embedding	O
regardless	O
of	O
the	O
specific	O
topic	O
results	O
in	O
identical	O
embeddings	O
for	O
different	O
topics	O
on	O
the	O
same	O
document	O
;	O
moreover	O
,	O
even	O
if	O
a	O
document	O
is	O
only	O
associated	O
with	O
one	O
topic	O
,	O
it	O
might	O
contain	O
information	O
not	O
relevant	O
to	O
that	O
topic	O
and	O
thus	O
the	O
holistic	O
document	B-TaskName
embedding	I-TaskName
will	O
encode	O
noisy	O
information	O
.	O
Therefore	O
,	O
we	O
argue	O
that	O
the	O
DC	O
topic	O
embedding	O
should	O
be	O
both	O
contextualized	O
and	O
topic	O
-	O
specific	O
.	O
In	O
this	O
regard	O
,	O
according	O
to	O
Equation	O
2	O
,	O
we	O
rewrite	O
the	O
DC	O
topic	O
embedding	O
as	O
the	O
weighted	O
sum	O
of	O
DC	O
keyword	O
embeddings	O
where	O
only	O
top	O
-	O
m	O
topic	O
keywords	O
are	O
used	O
instead	O
of	O
all	O
the	O
words	O
in	O
the	O
document	O
,	O
as	O
H	O
d	O
j	O
(	O
t	O
i	O
)	O
=	O
m	O
k=1	O
p	O
ik	O
H	O
d	O
j	O
(	O
w	O
k	O
)	O
.	O
(	O
5	O
)	O
Finally	O
,	O
in	O
terms	O
of	O
the	O
DC	O
keyword	O
embedding	O
H	O
d	O
j	O
(	O
w	O
k	O
)	O
,	O
as	O
can	O
be	O
told	O
from	O
its	O
name	O
,	O
it	O
is	O
precisely	O
what	O
a	O
pretrained	O
language	O
model	O
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
is	O
designed	O
for	O
.	O
Therefore	O
,	O
we	O
take	O
the	O
corresponding	O
final	O
-	O
layer	O
token	O
embedding	O
of	O
w	O
k	O
when	O
the	O
input	O
to	O
the	O
language	O
model	O
is	O
d	O
j	O
.	O
Due	O
to	O
the	O
self	O
-	O
attention	O
mechanism	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
in	O
the	O
pretrained	O
language	O
model	O
,	O
H	O
d	O
j	O
(	O
t	O
i	O
)	O
encodes	O
the	O
global	O
context	O
of	O
the	O
document	O
,	O
but	O
since	O
it	O
only	O
takes	O
the	O
sum	O
of	O
topic	O
keyword	O
embeddings	O
,	O
the	O
encoded	O
information	O
is	O
more	O
oriented	O
towards	O
this	O
specific	O
topic	O
t	O
i	O
,	O
which	O
elegantly	O
resonates	O
with	O
its	O
name	O
"	O
document	O
-	O
contextualized	O
topic	O
embedding	O
"	O
.	O
The	O
step	O
-	O
by	O
-	O
step	O
illustration	O
of	O
the	O
generation	O
of	O
H	O
d	O
(	O
w	O
)	O
,	O
H	O
d	O
(	O
t	O
)	O
and	O
H	O
D	O
(	O
t	O
)	O
is	O
shown	O
in	O
Figure	O
1	O
(	O
c	O
)	O
.	O
Because	O
the	O
language	O
model	O
used	O
to	O
generate	O
the	O
embeddings	O
is	O
finetuned	O
to	O
encode	O
partisanship	O
,	O
the	O
generated	O
H	O
D	O
(	O
t	O
i	O
)	O
also	O
contains	O
this	O
information	O
and	O
is	O
more	O
precisely	O
called	O
partisanshipaware	O
corpus	O
-	O
contextualized	O
topic	O
embedding	O
.	O
For	O
brevity	O
we	O
call	O
it	O
corpus	O
-	O
contextualized	O
(	O
CC	O
)	O
topic	O
embedding	O
.	O

We	O
compare	O
PaCTE	O
to	O
the	O
following	O
three	O
baselines	O
.	O
Leave	O
-	O
out	O
estimator	O
(	O
LOE	O
)	O
.	O
For	O
a	O
pair	O
of	O
news	O
corpora	O
D	O
L	O
and	O
D	O
R	O
and	O
a	O
given	O
topic	O
t	O
,	O
we	O
take	O
the	O
top	O
-	O
10	O
most	O
relevant	O
documents	O
from	O
each	O
corpus	O
and	O
feed	O
the	O
token	O
frequency	O
vectors	O
of	O
the	O
documents	O
into	O
the	O
leave	O
-	O
out	O
estimator	O
(	O
Demszky	O
et	O
al	O
,	O
2019	O
)	O
,	O
from	O
which	O
we	O
use	O
estimated	O
partisanship	O
as	O
the	O
polarization	O
score	O
(	O
[	O
0	B-DatasetName
,	O
1	O
]	O
)	O
of	O
topic	O
t	O
between	O
D	O
L	O
and	O
D	O
R	O
,	O
following	O
the	O
idea	O
of	O
measuring	O
within	O
-	O
topic	O
polarization	O
in	O
their	O
paper	O
.	O
Note	O
that	O
different	O
from	O
their	O
method	O
that	O
extracts	O
topic	O
using	O
embedding	O
-	O
based	O
topic	O
assignment	O
,	O
we	O
use	O
the	O
same	O
LDA	B-MethodName
topic	O
model	O
in	O
PaCTE	O
to	O
extract	O
topics	O
,	O
so	O
as	O
to	O
ensure	O
fair	O
comparison	O
between	O
PaCTE	O
and	O
LOE	O
.	O
PaCTE¬FT	O
.	O
A	O
variant	O
of	O
PaCTE	O
without	O
finetuning	O
the	O
language	O
model	O
.	O
We	O
compare	O
to	O
it	O
to	O
show	O
the	O
effect	O
of	O
finetuning	O
the	O
language	O
model	O
.	O
PaCTE	O
-	O
PLS	O
.	O
A	O
variant	O
of	O
PaCTE	O
where	O
the	O
language	O
model	O
is	O
finetuned	O
on	O
news	O
articles	O
with	O
partisanship	O
labels	O
shuffled	O
and	O
thus	O
is	O
confused	O
about	O
the	O
partisanship	O
.	O
We	O
compare	O
to	O
it	O
in	O
order	O
to	O
show	O
the	O
effect	O
of	O
rendering	O
the	O
language	O
model	O
partisanship	O
-	O
aware	O
.	O

Breit	O
NYP	O
CNN	O
1	O
,	O
9	O
,	O
10	O
9	O
,	O
1	O
,	O
11	O
9	O
,	O
10	O
,	O
2	O
Huff	O
10	O
,	O
1	O
,	O
8	O
1	O
,	O
11	O
,	O
9	O
10	O
,	O
12	O
,	O
30	O
NYT	O
10	O
,	O
33	O
,	O
1	O
11	O
,	O
1	O
,	O
33	O
11	O
,	O
9	O
,	O
10	O
Analysis	O
of	O
results	O
.	O
The	O
results	O
of	O
polarized	O
topics	O
retrieval	O
using	O
different	O
methods	O
in	O
nine	O
news	O
corpus	O
pairs	O
are	O
shown	O
in	O
Table	O
3	O
.	O
The	O
average	O
recall@3	O
over	O
the	O
nine	O
news	O
source	O
pairs	O
is	O
0.26	O
,	O
0.04	O
,	O
0.26	O
,	O
and	O
0.52	O
on	O
LOE	O
,	O
PaCTE¬FT	O
,	O
PaCTE	O
-	O
PLS	O
,	O
and	O
PaCTE	O
respectively	O
,	O
where	O
PaCTE	O
outperforms	O
all	O
other	O
baselines	O
.	O
Comparing	O
the	O
results	O
of	O
LOE	O
and	O
PaCTE	O
,	O
we	O
see	O
that	O
in	O
most	O
pairs	O
PaCTE	O
outperforms	O
or	O
ties	O
with	O
LOE	O
.	O
We	O
argue	O
that	O
the	O
inferior	O
performance	O
of	O
LOE	O
stems	O
from	O
its	O
inability	O
to	O
capture	O
document	O
semantics	O
due	O
to	O
the	O
use	O
of	O
word	O
frequency	O
vectors	O
.	O
For	O
example	O
,	O
in	O
Huff	O
vs.	O
NYP	O
,	O
topic	O
12	O
is	O
one	O
of	O
the	O
target	O
polarized	O
topics	O
,	O
where	O
documents	O
from	O
both	O
stances	O
spend	O
the	O
bulk	O
of	O
the	O
content	O
on	O
the	O
fact	O
about	O
the	O
primaries	O
and	O
then	O
use	O
a	O
few	O
words	O
to	O
explicitly	O
or	O
implicitly	O
endorse	O
Biden	O
or	O
Sanders	O
.	O
Based	O
on	O
the	O
use	O
of	O
words	O
it	O
is	O
difficult	O
to	O
differentiate	O
documents	O
from	O
the	O
two	O
stances	O
,	O
leading	O
to	O
the	O
failure	O
of	O
LOE	O
.	O
In	O
contrast	O
,	O
PaCTE	O
is	O
able	O
to	O
capture	O
the	O
contextual	O
semantics	O
in	O
addition	O
to	O
the	O
statistics	O
of	O
word	O
usages	O
.	O
Therefore	O
,	O
even	O
when	O
word	O
usages	O
are	O
statistically	O
similar	O
,	O
PaCTE	O
manages	O
to	O
discern	O
the	O
semantic	O
difference	O
and	O
capture	O
polarization	O
.	O
However	O
,	O
in	O
Huff	O
vs.	O
Breit	O
,	O
compared	O
to	O
LOE	O
,	O
PaCTE	O
fails	O
to	O
retrieve	O
topic	O
1	O
regarding	O
"	O
black	O
lives	O
matter	O
"	O
,	O
which	O
is	O
in	O
the	O
target	O
polarized	O
topics	O
.	O
On	O
topic	O
1	O
Huff	O
stresses	O
"	O
justice	O
"	O
where	O
the	O
news	O
articles	O
suggest	O
"	O
police	O
knelt	O
on	O
a	O
black	O
man	O
"	O
,	O
while	O
Breit	O
stresses	O
"	O
riot	O
"	O
where	O
the	O
articles	O
suggest	O
"	O
the	O
protesters	O
loot	O
stores	O
and	O
attack	O
police	O
"	O
.	O
As	O
a	O
result	O
,	O
the	O
word	O
usages	O
of	O
the	O
articles	O
from	O
two	O
stances	O
are	O
significantly	O
different	O
,	O
which	O
is	O
trivial	O
for	O
LOE	O
to	O
capture	O
,	O
and	O
thus	O
LOE	O
ranks	O
topic	O
1	O
in	O
a	O
high	O
place	O
in	O
the	O
output	O
list	O
.	O
Despite	O
the	O
difference	O
in	O
word	O
usages	O
,	O
articles	O
from	O
both	O
sources	O
mention	O
"	O
protests	O
"	O
and	O
"	O
violence	O
"	O
a	O
lot	O
and	O
their	O
"	O
negative	O
"	O
semantics	O
is	O
captured	O
by	O
PaCTE	O
,	O
leading	O
to	O
the	O
perceived	O
less	O
polarization	O
by	O
PaCTE	O
.	O
The	O
worst	O
-	O
performing	O
method	O
is	O
PaCTE¬FT	O
where	O
the	O
language	O
model	O
is	O
not	O
finetuned	O
.	O
On	O
all	O
topics	O
and	O
in	O
all	O
partisan	O
news	O
source	O
pairs	O
,	O
the	O
polarization	O
scores	O
given	O
by	O
PaCTE¬FT	O
are	O
below	O
0.1	O
(	O
the	O
full	O
range	O
is	O
[	O
0	B-DatasetName
,	O
1	O
]	O
)	O
which	O
indicates	O
significant	O
alignment	O
.	O
However	O
,	O
this	O
is	O
contradictory	O
to	O
the	O
well	O
-	O
known	O
polarization	O
in	O
news	O
media	O
.	O
Such	O
a	O
phenomenon	O
demonstrates	O
the	O
necessity	O
of	O
fitting	O
a	O
language	O
model	O
on	O
the	O
target	O
corpus	O
before	O
apply	O
cosine	O
similarity	O
between	O
learned	O
embeddings	O
as	O
a	O
measure	O
of	O
word	O
and	O
topic	O
similarities	O
.	O
In	O
PaCTE	O
-	O
PLS	O
the	O
language	O
model	O
is	O
finetuned	O
on	O
shuffled	O
partisan	O
labels	O
that	O
do	O
not	O
represent	O
real	O
partisanship	O
.	O
Compared	O
to	O
PaCTE¬FT	O
where	O
the	O
model	O
is	O
not	O
finetuned	O
at	O
all	O
,	O
the	O
performance	O
of	O
PaCTE	O
-	O
PLS	O
improves	O
significantly	O
,	O
achieving	O
the	O
performance	O
on	O
a	O
par	O
with	O
LOE	O
.	O
However	O
,	O
neither	O
PaCTE¬FT	O
nor	O
LOE	O
makes	O
use	O
of	O
information	O
about	O
news	O
partisanship	O
,	O
and	O
compared	O
to	O
PaCTE	O
0	B-DatasetName
1/3	O
1/3	O
0	B-DatasetName
1/3	O
1/3	O
0	B-DatasetName
1/3	O
1/3	O
2/3	O
Huff	O
1/3	O
1/3	O
1/3	O
2/3	O
2/3	O
0	B-DatasetName
1/3	O
1/3	O
0	B-DatasetName
0	B-DatasetName
1/3	O
2/3	O
NYT	O
1/3	O
0	B-DatasetName
1/3	O
1	O
1/3	O
0	B-DatasetName
1/3	O
1/3	O
0	B-DatasetName
1/3	O
0	B-DatasetName
1/3	O
Table	O
3	O
:	O
Recall@3	O
on	O
polarized	O
topics	O
retrieval	O
in	O
nine	O
partisan	O
news	O
source	O
pairs	O
using	O
different	O
methods	O
,	O
where	O
we	O
use	O
the	O
polarization	O
-	O
based	O
topic	O
ranked	O
list	O
from	O
a	O
model	O
predictions	O
f	O
pred	O
(	O
D	O
L	O
,	O
D	O
R	O
,	O
T	O
labeled	O
)	O
to	O
retrieve	O
the	O
top	O
-	O
3	O
topics	O
from	O
the	O
ground	O
-	O
truth	O
ranked	O
list	O
l	O
gt	O
(	O
D	O
L	O
,	O
D	O
R	O
,	O
T	O
labeled	O
)	O
.	O
The	O
row	O
represents	O
the	O
liberal	O
source	O
and	O
the	O
column	O
represents	O
the	O
conservative	O
source	O
in	O
the	O
news	O
source	O
pair	O
.	O
where	O
partisanship	O
information	O
is	O
leveraged	O
,	O
they	O
are	O
still	O
outperformed	O
.	O
Insights	O
into	O
partisanship	O
learning	O
.	O
We	O
observe	O
that	O
PaCTE	O
,	O
which	O
is	O
finetuned	O
on	O
partisanship	O
labels	O
,	O
outperforms	O
PaCTE¬FT	O
and	O
PaCTE	O
-	O
PLS	O
.	O
We	O
hypothesize	O
that	O
during	O
the	O
finetuning	O
process	O
of	O
PaCTE	O
,	O
whereas	O
the	O
direct	O
objective	O
is	O
to	O
separate	O
documents	O
based	O
on	O
partisanship	O
labels	O
,	O
the	O
model	O
implicitly	O
learns	O
the	O
two	O
political	O
stances	O
on	O
each	O
topic	O
in	O
an	O
automatic	O
manner	O
;	O
just	O
like	O
in	O
human	O
annotating	O
,	O
the	O
annotators	O
were	O
given	O
two	O
groups	O
of	O
documents	O
from	O
two	O
partisan	O
lines	O
,	O
and	O
the	O
annotators	O
were	O
able	O
to	O
discover	O
the	O
two	O
political	O
stances	O
after	O
reading	O
the	O
documents	O
.	O
Therefore	O
,	O
after	O
finetuning	O
,	O
while	O
the	O
model	O
differentiates	O
document	O
embeddings	O
based	O
on	O
partisan	O
divisions	O
,	O
it	O
separates	O
DC	O
topic	O
embeddings	O
according	O
to	O
the	O
implicitly	O
and	O
automatically	O
learned	O
political	O
stances	O
,	O
bearing	O
resemblance	O
to	O
human	O
annotators	O
'	O
defining	O
two	O
political	O
stances	O
for	O
topics	O
.	O
As	O
a	O
result	O
,	O
we	O
can	O
use	O
the	O
partisanship	O
-	O
aware	O
model	O
to	O
capture	O
topic	O
polarization	O
arising	O
from	O
the	O
partisan	O
divisions	O
.	O

In	O
Section	O
3.4	O
we	O
propose	O
to	O
use	O
the	O
DC	O
topic	O
embedding	O
to	O
represent	O
the	O
ideology	O
of	O
a	O
document	O
on	O
a	O
topic	O
,	O
instead	O
of	O
using	O
the	O
holistic	O
document	B-TaskName
embedding	I-TaskName
.	O
In	O
this	O
section	O
we	O
study	O
the	O
difference	O
between	O
them	O
.	O
We	O
denote	O
the	O
variant	O
of	O
PaCTE	O
that	O
uses	O
document	O
embeddings	O
(	O
[	O
CLS	O
]	O
token	O
em	O
-	O
beddings	O
)	O
as	O
PaCTE	O
-	O
DE	O
.	O
First	O
,	O
we	O
show	O
the	O
results	O
of	O
polarized	O
topics	O
retrieval	O
using	O
PaCTE	O
-	O
DE	O
and	O
PaCTE	O
in	O
three	O
partisan	O
news	O
source	O
pairs	O
in	O
Table	O
4	O
.	O
Method	O
CNN	O
vs.	O
Fox	O
Huff	O
vs.	O
Breit	O
NYT	O
vs.	O
NYP	O
PaCTE	O
-	O
DE	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
We	O
observe	O
that	O
PaCTE	O
-	O
DE	O
fails	O
to	O
retrieve	O
any	O
polarized	O
topics	O
in	O
all	O
three	O
pairs	O
of	O
news	O
sources	O
,	O
significantly	O
outperformed	O
by	O
PaCTE	O
.	O
We	O
provide	O
more	O
explanations	O
on	O
the	O
advantages	O
of	O
DC	O
topic	O
embeddings	O
over	O
document	O
embeddings	O
from	O
another	O
perspective	O
,	O
in	O
addition	O
to	O
the	O
capability	O
of	O
DC	O
topic	O
embedding	O
to	O
focus	O
more	O
on	O
the	O
topicspecific	O
semantics	O
in	O
a	O
document	O
.	O
We	O
observe	O
that	O
the	O
polarization	O
scores	O
given	O
by	O
PaCTE	O
-	O
DE	O
in	O
three	O
source	O
pairs	O
on	O
all	O
topics	O
are	O
above	O
0.98	O
(	O
the	O
range	O
is	O
[	O
0	B-DatasetName
,	O
1	O
]	O
)	O
,	O
suggesting	O
that	O
all	O
topics	O
are	O
highly	O
polarized	O
.	O
Therefore	O
,	O
as	O
the	O
polarization	O
scores	O
cluster	O
within	O
the	O
interval	O
of	O
[	O
0.98	O
,	O
1	O
]	O
,	O
the	O
gaps	O
between	O
different	O
scores	O
are	O
barely	O
discernible	O
,	O
in	O
which	O
case	O
the	O
output	O
ranked	O
list	O
is	O
more	O
susceptible	O
to	O
random	O
noise	O
during	O
the	O
language	O
model	O
finetuning	O
and	O
is	O
thus	O
more	O
unstable	O
and	O
erratic	O
.	O
However	O
,	O
the	O
output	O
polarization	O
scores	O
from	O
PaCTE	O
are	O
more	O
evenly	O
distributed	O
in	O
[	O
0	B-DatasetName
,	O
1	O
]	O
,	O
and	O
thus	O
are	O
more	O
robust	O
to	O
perturbations	O
during	O
partisanship	O
learning	O
;	O
a	O
small	O
perturbation	O
on	O
a	O
polarization	O
score	O
does	O
not	O
affect	O
the	O
output	O
ranking	O
.	O
As	O
a	O
result	O
,	O
PaCTE	O
enjoys	O
a	O
better	O
chance	O
to	O
outperform	O
PaCTE	O
-	O
DE	O
.	O
PaCTE	O
1/3	O
1/3	O
1/3	O
As	O
a	O
matter	O
of	O
fact	O
,	O
the	O
large	O
polarization	O
scores	O
from	O
PaCTE	O
-	O
DE	O
on	O
all	O
topics	O
are	O
expected	O
,	O
because	O
the	O
language	O
model	O
is	O
finetuned	O
to	O
directly	O
separate	O
the	O
document	O
embeddings	O
according	O
to	O
partisan	O
line	O
divisions	O
,	O
resulting	O
in	O
low	O
cosine	O
similarities	O
between	O
document	O
embeddings	O
on	O
every	O
topic	O
,	O
as	O
shown	O
in	O
Figure	O
2	O
(	O
Left	O
)	O
.	O
However	O
,	O
despite	O
the	O
prominent	O
separation	O
of	O
document	O
embeddings	O
,	O
the	O
corresponding	O
DC	O
topic	O
embeddings	O
that	O
are	O
used	O
in	O
PaCTE	O
display	O
more	O
alignment	O
,	O
as	O
shown	O
in	O
Figure	O
2	O
(	O
Right	O
)	O
,	O
where	O
we	O
see	O
on	O
some	O
topics	O
the	O
DC	O
topic	O
embeddings	O
are	O
separated	O
while	O
on	O
other	O
topics	O
the	O
embeddings	O
are	O
more	O
close	O
.	O
Thus	O
,	O
we	O
argue	O
that	O
during	O
the	O
finetuning	O
process	O
,	O
on	O
a	O
given	O
topic	O
,	O
DC	O
topic	O
embeddings	O
retain	O
their	O
similarity	O
if	O
the	O
two	O
partisan	O
news	O
articles	O
agree	O
on	O
this	O
topic	O
,	O
because	O
in	O
these	O
articles	O
the	O
topic	O
-	O
related	O
se	O
-	O
mantics	O
does	O
not	O
contribute	O
to	O
the	O
forming	O
of	O
the	O
partisanship	O
and	O
thus	O
maintains	O
its	O
position	O
during	O
partisanship	O
learning	O
,	O
while	O
the	O
non	O
-	O
topical	O
semantics	O
(	O
not	O
captured	O
by	O
DC	O
topic	O
embeddings	O
but	O
captured	O
by	O
document	O
embeddings	O
)	O
that	O
contribute	O
to	O
the	O
document	O
partisanship	O
keeps	O
moving	O
apart	O
in	O
the	O
embedding	O
space	O
.	O

We	O
use	O
MALLET	O
5	O
topic	O
modeling	O
.	O
The	O
top	O
-	O
10	O
keywords	O
of	O
all	O
39	O
topics	O
are	O
shown	O
in	O
Table	O
6	O
.	O
Among	O
them	O
topic	O
0	B-DatasetName
,	O
3	O
,	O
4	O
,	O
14	O
,	O
16	O
,	O
26	O
,	O
35	O
,	O
36	O
,	O
37	O
are	O
not	O
used	O
in	O
further	O
analysis	O
because	O
after	O
reading	O
relevant	O
articles	O
we	O
find	O
that	O
they	O
are	O
more	O
about	O
advertisements	O
,	O
sport	O
events	O
,	O
gossip	O
news	O
and	O
recipes	O
and	O
etc	O
.	O
,	O
which	O
are	O
more	O
factual	O
and	O
convey	O
limited	O
media	O
ideologies	O
.	O
30	O
topics	O
are	O
left	O
after	O
removing	O
the	O
9	O
topics	O
.	O
Table	O
6	O
lists	O
the	O
top	O
-	O
10	O
keywords	O
of	O
the	O
30	O
topics	O
.	O

This	O
project	O
was	O
funded	O
in	O
part	O
by	O
DARPA	B-DatasetName
under	O
contract	O
HR001121C0168	O
.	O
The	O
authors	O
are	O
also	O
grateful	O
to	O
Ves	O
Stoyanov	O
for	O
a	O
productive	O
discussion	O
.	O

We	O
recruit	O
3	O
annotators	O
that	O
work	O
as	O
academic	O
researchers	O
in	O
the	O
areas	O
of	O
NLP	O
and	O
social	O
science	O
.	O
For	O
each	O
one	O
of	O
the	O
30	O
topics	O
,	O
the	O
annotators	O
are	O
provided	O
with	O
the	O
top	O
-	O
10	O
topic	O
keywords	O
and	O
the	O
summaries	O
of	O
top	O
-	O
10	O
most	O
relevant	O
documents	O
from	O
each	O
news	O
corpus	O
(	O
as	O
a	O
total	O
of	O
60	O
documents	O
)	O
.	O
First	O
,	O
the	O
annotators	O
select	O
15	O
topics	O
on	O
which	O
they	O
feel	O
it	O
is	O
straightforward	O
to	O
find	O
two	O
polarized	O
political	O
stances	O
by	O
reading	O
the	O
relevant	O
documents	O
.	O
For	O
example	O
,	O
on	O
topic	O
12	O
about	O
Democratic	O
primaries	O
,	O
it	O
is	O
intuitive	O
to	O
perceive	O
the	O
two	O
political	O
stances	O
are	O
"	O
endorsing	O
Biden	O
"	O
and	O
"	O
endorsing	O
Sanders	O
"	O
after	O
reading	O
relevant	O
articles	O
,	O
and	O
then	O
this	O
topic	O
is	O
likely	O
to	O
be	O
selected	O
.	O
We	O
take	O
the	O
overlap	O
of	O
the	O
15	O
selected	O
topics	O
from	O
3	O
annotators	O
and	O
obtain	O
10	O
topics	O
:	O
T	O
labeled	O
=	O
{	O
t	O
1	O
,	O
t	O
2	O
,	O
t	O
8	O
,	O
t	O
9	O
,	O
t	O
10	O
,	O
t	O
11	O
,	O
t	O
12	O
,	O
t	O
27	O
,	O
t	O
30	O
,	O
t	O
33	O
}	O
with	O
defined	O
polarized	O
political	O
stances	O
.	O
In	O
other	O
words	O
,	O
the	O
annotators	O
reach	O
an	O
agreement	O
that	O
it	O
is	O
more	O
clear	O
on	O
these	O
10	O
topics	O
that	O
there	O
are	O
two	O
political	O
stances	O
.	O
We	O
find	O
that	O
on	O
each	O
of	O
these	O
10	O
topics	O
,	O
the	O
two	O
stances	O
defined	O
by	O
3	O
annotators	O
reach	O
a	O
complete	O
agreement	O
.	O
We	O
do	O
not	O
annotate	O
all	O
topics	O
because	O
1	O
)	O
it	O
is	O
difficult	O
for	O
humans	O
to	O
discern	O
the	O
two	O
political	O
stances	O
on	O
some	O
topics	O
,	O
especially	O
when	O
such	O
two	O
stances	O
do	O
not	O
exist	O
at	O
all	O
;	O
2	O
)	O
we	O
use	O
the	O
vanilla	O
LDA	B-MethodName
topic	O
modeling	O
which	O
is	O
not	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
,	O
so	O
the	O
modeled	O
topics	O
will	O
change	O
using	O
different	O
topic	B-TaskName
models	I-TaskName
,	O
in	O
which	O
case	O
the	O
annotating	O
step	O
should	O
be	O
repeated	O
.	O
Nevertheless	O
,	O
we	O
argue	O
that	O
annotating	O
10	O
topics	O
is	O
sufficient	O
to	O
quantitatively	O
evaluate	O
the	O
effectiveness	O
of	O
PaCTE	O
.	O
Given	O
a	O
topic	O
t	O
from	O
T	O
labeled	O
,	O
the	O
defined	O
two	O
stances	O
,	O
and	O
its	O
60	O
most	O
relevant	O
documents	O
(	O
10	O
from	O
each	O
of	O
the	O
six	O
news	O
sources	O
)	O
,	O
for	O
each	O
document	O
,	O
we	O
ask	O
the	O
annotators	O
to	O
label	O
which	O
stance	O
it	O
belongs	O
to	O
and	O
label	O
it	O
as	O
0	B-DatasetName
or	O
1	O
;	O
if	O
the	O
annotator	O
is	O
not	O
able	O
to	O
perceive	O
a	O
clear	O
political	O
stance	O
,	O
then	O
the	O
annotator	O
will	O
label	O
it	O
as	O
-	O
1	O
.	O
For	O
each	O
document	O
,	O
the	O
majority	O
vote	O
of	O
the	O
three	O
labels	O
with	O
be	O
used	O
as	O
the	O
final	O
annotation	O
.	O
If	O
no	O
majority	O
vote	O
is	O
achieved	O
,	O
in	O
other	O
words	O
,	O
the	O
three	O
annotators	O
give	O
three	O
different	O
labels	O
to	O
a	O
document	O
,	O
then	O
a	O
fourth	O
annotator	O
will	O
read	O
the	O
document	O
again	O
and	O
decide	O
the	O
final	O
label	O
.	O
For	O
a	O
complete	O
list	O
of	O
all	O
document	O
labels	O
on	O
the	O
10	O
selected	O
topics	O
,	O
please	O
refer	O
to	O
our	O
public	O
repository	O
.	O

Beyond	O
Laurel	O
/	O
Yanny	O
:	O
An	O
Autoencoder	B-MethodName
-	O
Enabled	O
Search	O
for	O
Polyperceivable	O
Audio	O

How	O
robust	O
is	O
human	O
sensory	O
perception	O
,	O
and	O
to	O
what	O
extent	O
do	O
perceptions	O
differ	O
between	O
individuals	O
?	O
In	O
May	O
2018	O
,	O
an	O
audio	O
clip	O
of	O
a	O
man	O
speaking	O
the	O
word	O
"	O
laurel	O
"	O
received	O
widespread	O
attention	O
because	O
a	O
significant	O
proportion	O
of	O
listeners	O
confidently	O
reported	O
hearing	O
not	O
the	O
word	O
"	O
laurel	O
,	O
"	O
but	O
rather	O
the	O
quite	O
different	O
sound	O
"	O
yanny	O
"	O
(	O
Salam	O
and	O
Victor	O
,	O
2018	O
)	O
.	O
At	O
first	O
glance	O
,	O
this	O
suggests	O
that	O
the	O
decision	O
boundaries	O
for	O
speech	O
perception	O
vary	O
considerably	O
among	O
individuals	O
.	O
The	O
reality	O
is	O
more	O
surprising	O
:	O
almost	O
everyone	O
has	O
a	O
decision	O
boundary	O
between	O
the	O
sounds	O
"	O
laurel	O
"	O
and	O
"	O
yanny	O
,	O
"	O
without	O
a	O
significant	O
"	O
dead	O
zone	O
"	O
separating	O
these	O
classes	O
.	O
The	O
audio	O
clip	O
in	O
question	O
lies	O
close	O
to	O
this	O
decision	O
boundary	O
,	O
so	O
that	O
if	O
the	O
clip	O
is	O
slightly	O
perturbed	O
(	O
e.g.	O
by	O
damping	O
certain	O
frequencies	O
or	O
slowing	O
down	O
the	O
playback	O
rate	O
)	O
,	O
individuals	O
switch	O
from	O
confidently	O
perceiving	O
"	O
laurel	O
"	O
to	O
confidently	O
perceiving	O
"	O
yanny	O
,	O
"	O
with	O
the	O
exact	O
point	O
of	O
switching	O
varying	O
slightly	O
from	O
person	O
to	O
person	O
.	O
How	O
common	O
is	O
this	O
phenomenon	O
?	O
Specifically	O
,	O
what	O
fraction	O
of	O
spoken	O
language	O
is	O
"	O
polyperceivable	O
"	O
in	O
the	O
sense	O
of	O
evoking	O
a	O
multimodal	O
response	O
in	O
a	O
population	O
of	O
listeners	O
?	O
In	O
this	O
work	O
,	O
we	O
provide	O
initial	O
results	O
suggesting	O
a	O
significant	O
density	O
of	O
spoken	O
words	O
that	O
,	O
like	O
the	O
original	O
"	O
laurel	O
/	O
yanny	O
"	O
clip	O
,	O
lie	O
close	O
to	O
unexpected	O
decision	O
boundaries	O
between	O
seemingly	O
unrelated	O
pairs	O
of	O
words	O
or	O
sounds	O
,	O
such	O
that	O
individual	O
listeners	O
can	O
switch	O
between	O
perceptual	O
modes	O
via	O
a	O
slight	O
perturbation	O
.	O
The	O
clips	O
we	O
consider	O
consist	O
of	O
audio	O
signals	O
synthesized	O
by	O
the	O
Amazon	O
Polly	O
speech	B-TaskName
synthesis	I-TaskName
system	O
with	O
a	O
slightly	O
perturbed	O
playback	O
rate	O
(	O
i.e.	O
a	O
slight	O
slowing	O
-	O
down	O
of	O
the	O
clip	O
)	O
.	O
Though	O
the	O
resulting	O
audio	O
signals	O
are	O
not	O
"	O
natural	O
"	O
stimuli	O
,	O
in	O
the	O
sense	O
that	O
they	O
are	O
very	O
different	O
from	O
the	O
result	O
of	O
asking	O
a	O
human	O
to	O
speak	O
slower	O
(	O
see	O
Section	O
5	O
)	O
,	O
we	O
find	O
that	O
they	O
are	O
easy	O
to	O
compute	O
and	O
reliably	O
yield	O
compelling	O
polyperceivable	O
instances	O
.	O
We	O
encourage	O
future	O
work	O
to	O
investigate	O
the	O
power	O
of	O
more	O
sophisticated	O
perturbations	O
,	O
as	O
well	O
as	O
to	O
consider	O
natural	O
,	O
ecologically	O
-	O
plausible	O
perturbations	O
.	O
To	O
find	O
our	O
polyperceivable	O
instances	O
,	O
we	O
(	O
1	O
)	O
devise	O
a	O
metric	O
that	O
correlates	O
with	O
polyperceivability	O
,	O
(	O
2	O
)	O
use	O
this	O
metric	O
to	O
efficiently	O
sample	O
candidate	O
audio	O
clips	O
,	O
and	O
(	O
3	O
)	O
evaluate	O
these	O
candidates	O
on	O
human	O
subjects	O
via	O
Amazon	O
Mechanical	O
Turk	O
.	O
We	O
present	O
several	O
compelling	O
new	O
examples	O
of	O
the	O
"	O
laurel	O
/	O
yanny	O
"	O
effect	O
,	O
and	O
we	O
encourage	O
readers	O
to	O
listen	O
to	O
the	O
examples	O
included	O
in	O
the	O
supplementary	O
materials	O
(	O
also	O
available	O
online	O
at	O
https://theory.stanford.edu/	O
valiant	O
/	O
polyperceivable	O
/	O
index.html	O
)	O
.	O
Finally	O
,	O
we	O
estimate	O
that	O
polyperceivable	O
clips	O
can	O
be	O
made	O
for	O
>	O
2	O
%	O
of	O
English	O
words	O
.	O

Are	O
the	O
words	O
we	O
found	O
polyperceivable	O
?	O
To	O
identify	O
cases	O
where	O
words	O
had	O
multiple	O
perceptual	O
"	O
modes	O
,	O
"	O
we	O
looked	O
for	O
clusters	O
in	O
the	O
distribution	O
of	O
responses	O
for	O
each	O
of	O
the	O
29	O
candidate	O
words	O
.	O
Concretely	O
,	O
we	O
treated	O
responses	O
as	O
"	O
bags	O
of	O
phonemes	O
"	O
and	O
then	O
applied	O
K	O
-	O
means	O
.	O
Though	O
this	O
rough	O
heuristic	O
discards	O
information	O
about	O
the	O
order	O
of	O
phonemes	O
within	O
a	O
word	O
,	O
it	O
works	O
sufficiently	O
well	O
for	O
clustering	O
,	O
especially	O
since	O
most	O
of	O
our	O
words	O
have	O
very	O
few	O
syllables	O
(	O
more	O
sophisticated	O
models	O
of	O
phonetic	O
similarity	O
exist	O
,	O
but	O
they	O
would	O
not	O
change	O
our	O
results	O
)	O
.	O
We	O
found	O
that	O
the	O
largest	O
cluster	O
typically	O
contained	O
the	O
original	O
word	O
and	O
rhymes	O
,	O
whereas	O
other	O
clusters	O
represented	O
significantly	O
different	O
perceptual	O
modes	O
.	O
Some	O
examples	O
of	O
clusters	O
and	O
their	O
relative	O
frequency	O
are	O
available	O
in	O
the	O
prevalance	O
of	O
alternate	O
modes	O
among	O
our	O
clips	O
increases	O
.	O
How	O
prevalent	O
are	O
polyperceivable	O
words	O
?	O
Of	O
our	O
initial	O
sample	O
of	O
200	O
words	O
,	O
11	O
ultimately	O
yielded	O
compelling	O
demonstrations	O
.	O
To	O
compute	O
the	O
prevalence	O
of	O
polyperceivable	O
words	O
in	O
the	O
population	O
of	O
the	O
top	O
10k	O
words	O
,	O
we	O
have	O
to	O
account	O
for	O
the	O
importance	O
sampling	O
weights	O
we	O
used	O
when	O
sampling	O
in	O
Section	O
2.1	O
.	O
After	O
scaling	O
each	O
word	O
's	O
contribution	O
by	O
the	O
inverse	O
of	O
the	O
probability	O
of	O
including	O
that	O
word	O
in	O
our	O
nonuniform	O
sample	O
of	O
200	O
,	O
we	O
conclude	O
that	O
polyperceivable	O
clips	O
exist	O
for	O
at	O
least	O
2	O
%	O
of	O
the	O
population	O
:	O
that	O
is	O
,	O
of	O
the	O
16	O
voices	O
under	O
consideration	O
,	O
at	O
least	O
one	O
yields	O
a	O
polyperceivable	O
clip	O
for	O
>	O
2	O
%	O
of	O
the	O
top	O
10k	O
English	O
words	O
.	O
We	O
emphasize	O
that	O
this	O
is	O
a	O
conservative	O
lower	O
bound	O
,	O
because	O
it	O
assumes	O
that	O
there	O
were	O
no	O
other	O
polyperceivable	O
words	O
in	O
the	O
200	O
words	O
we	O
sampled	O
,	O
besides	O
the	O
11	O
that	O
we	O
selected	O
for	O
the	O
second	O
round	O
.	O
We	O
did	O
not	O
conduct	O
an	O
exhaustive	O
search	O
among	O
those	O
200	O
words	O
,	O
instead	O
focusing	O
our	O
Mechanical	O
Turk	O
resources	O
on	O
only	O
the	O
most	O
promising	O
candidates	O
.	O
Is	O
S	O
a	O
good	O
metric	O
?	O
We	O
consider	O
the	O
metric	O
S	O
to	O
be	O
successful	O
because	O
it	O
allowed	O
us	O
to	O
efficiently	O
find	O
several	O
new	O
polyperceivable	O
instances	O
.	O
If	O
the	O
200	O
words	O
were	O
sampled	O
uniformly	O
instead	O
of	O
being	O
importance	O
-	O
sampled	O
based	O
on	O
S	O
,	O
we	O
would	O
only	O
have	O
found	O
4	O
polyperceivable	O
words	O
in	O
expectation	O
(	O
2	O
%	O
of	O
200	O
)	O
.	O
Thus	O
,	O
importance	O
sampling	O
increased	O
our	O
procedure	O
's	O
recall	O
by	O
almost	O
3×.	O
For	O
a	O
more	O
quantitative	O
understanding	O
,	O
we	O
analyzed	O
the	O
relationship	O
between	O
"	O
autoencoder	B-MethodName
path	O
length	O
"	O
S	O
and	O
"	O
perceptual	O
path	O
length	O
"	O
T	O
.	O
Our	O
measure	O
T	O
of	O
"	O
perceptual	O
path	O
length	O
"	O
for	O
a	O
clip	O
is	O
change	O
in	O
average	O
distance	O
between	O
source	O
word	O
and	O
response	O
as	O
we	O
slow	O
the	O
clip	O
down	O
from	O
0.75×	O
to	O
0.6×.	O
As	O
with	O
clustering	O
above	O
,	O
distance	O
is	O
measured	O
in	O
bag	O
-	O
of	O
-	O
phonemes	O
space	O
.	O
For	O
each	O
word	O
,	O
we	O
computed	O
the	O
correlation	O
between	O
S	O
and	O
T	O
among	O
the	O
16	O
voices	O
(	O
both	O
S	O
and	O
T	O
vary	O
significantly	O
across	O
voices	O
)	O
.	O
For	O
all	O
but	O
5	O
of	O
our	O
29	O
words	O
these	O
metrics	O
correlated	O
positively	O
,	O
though	O
with	O
varying	O
strength	O
(	O
Figure	O
3	O
)	O
.	O
This	O
suggests	O
that	O
S	O
indeed	O
correlates	O
with	O
polyperceivability	O
.	O
4	O
Discussion	O
:	O
Why	O
study	O
quirks	O
of	O
human	O
perception	O
in	O
an	O
ACL	O
paper	O
?	O
Perceptual	O
instability	O
in	O
human	O
sensory	O
systems	O
offers	O
insight	O
into	O
ML	O
systems	O
.	O
The	O
question	O
of	O
what	O
fraction	O
of	O
natural	O
inputs	O
lie	O
close	O
to	O
decision	O
boundaries	O
for	O
trained	O
ML	O
systems	O
has	O
received	O
enormous	O
attention	O
.	O
The	O
surprising	O
punchline	O
that	O
has	O
emerged	O
over	O
the	O
past	O
decade	O
is	O
that	O
most	O
natural	O
examples	O
(	O
including	O
points	O
in	O
the	O
training	O
set	O
)	O
actually	O
lie	O
extremely	O
close	O
to	O
unexpected	O
decision	O
boundaries	O
.	O
For	O
most	O
of	O
these	O
points	O
,	O
a	O
tiny	O
but	O
carefully	O
-	O
crafted	O
perturbation	O
can	O
lead	O
the	O
ML	O
system	O
to	O
change	O
the	O
label	O
.	O
Such	O
perturbations	O
are	O
analogous	O
to	O
the	O
slight	O
perturbation	O
in	O
playback	O
speed	O
for	O
the	O
polyperceivable	O
clips	O
we	O
consider	O
.	O
In	O
the	O
ML	O
literature	O
,	O
these	O
perturbations	O
,	O
referred	O
to	O
as	O
"	O
adversarial	O
examples	O
"	O
seem	O
pervasive	O
across	O
complex	O
ML	O
systems	O
(	O
Szegedy	O
et	O
al	O
,	O
2013	O
;	O
Goodfellow	O
et	O
al	O
,	O
2014	O
;	O
Nguyen	O
et	O
al	O
,	O
2015	O
;	O
Moosavi	O
-	O
Dezfooli	O
et	O
al	O
,	O
2016	O
;	O
Madry	O
et	O
al	O
,	O
2017	O
;	O
Raghunathan	O
et	O
al	O
,	O
2018	O
;	O
Athalye	O
et	O
al	O
,	O
2017	O
)	O
.	O
While	O
the	O
initial	O
work	O
on	O
adversarial	O
examples	O
focused	O
on	O
computer	O
vision	O
,	O
more	O
recent	O
work	O
shows	O
the	O
presence	O
of	O
such	O
examples	O
across	O
other	O
settings	O
,	O
including	O
reinforcement	O
learning	O
(	O
Huang	O
et	O
al	O
,	O
2017	O
)	O
,	O
reading	B-TaskName
comprehension	I-TaskName
(	O
Jia	O
and	O
Liang	O
,	O
2017	O
)	O
,	O
and	O
speech	B-TaskName
recognition	I-TaskName
(	O
Carlini	O
and	O
Wag	O
-	O
ner	O
,	O
2018	O
;	O
Qin	O
et	O
al	O
,	O
2019	O
)	O
.	O
Studying	O
perceptual	O
illusions	O
would	O
provide	O
a	O
much	O
-	O
needed	O
reference	O
when	O
evaluating	O
ML	O
systems	O
in	O
these	O
domains	O
.	O
For	O
vision	O
tasks	O
,	O
for	O
example	O
,	O
human	O
vision	O
provides	O
the	O
only	O
evidence	O
that	O
current	O
ML	O
models	O
are	O
far	O
from	O
optimal	O
in	O
terms	O
of	O
robustness	O
to	O
adversarial	O
examples	O
.	O
However	O
,	O
while	O
humans	O
are	O
certainly	O
not	O
as	O
susceptible	O
to	O
adversarial	O
examples	O
as	O
ML	O
systems	O
,	O
we	O
lack	O
quantified	O
bounds	O
on	O
human	O
robustness	O
.	O
More	O
broadly	O
,	O
understanding	O
which	O
systems	O
(	O
both	O
biological	O
and	O
ML	O
)	O
have	O
decision	O
boundaries	O
that	O
lie	O
surprisingly	O
close	O
to	O
many	O
natural	O
inputs	O
may	O
inform	O
our	O
sense	O
of	O
what	O
settings	O
are	O
amenable	O
to	O
adversarially	O
robust	O
models	O
,	O
and	O
what	O
settings	O
inherently	O
lead	O
to	O
vulnerable	O
classifiers	O
.	O
Perceptual	O
instability	O
in	O
ML	O
systems	O
offers	O
insight	O
into	O
human	O
sensory	O
systems	O
.	O
Recent	O
research	O
on	O
adversarial	B-TaskName
robustness	I-TaskName
of	O
ML	O
models	O
has	O
provided	O
a	O
trove	O
of	O
new	O
tools	O
and	O
perspectives	O
for	O
probing	O
classifiers	O
and	O
exploring	O
the	O
geometry	O
of	O
decision	O
boundaries	O
.	O
These	O
tools	O
can	O
not	O
directly	O
be	O
applied	O
to	O
study	O
the	O
decision	O
boundaries	O
of	O
biological	O
classifiers	O
(	O
e.g.	O
we	O
can	O
not	O
reasonably	O
do	O
"	O
gradient	O
descent	O
"	O
on	O
human	O
subjects	O
)	O
.	O
However	O
,	O
using	O
standard	O
data	O
-	O
driven	O
deep	O
learning	O
techniques	O
to	O
model	O
human	O
perceptual	O
systems	O
can	O
allow	O
us	O
to	O
apply	O
these	O
techniques	O
by	O
proxy	O
.	O
An	O
example	O
can	O
be	O
found	O
in	O
the	O
study	O
of	O
"	O
transferability	O
.	O
"	O
Adversarial	O
examples	O
crafted	O
to	O
fool	O
a	O
specific	O
model	O
often	O
also	O
fool	O
other	O
models	O
,	O
even	O
those	O
trained	O
on	O
disjoint	O
training	O
sets	O
(	O
Papernot	O
et	O
al	O
,	O
2016a	O
;	O
Tramèr	O
et	O
al	O
,	O
2017	O
;	O
Liu	O
et	O
al	O
,	O
2016	O
)	O
.	O
This	O
prompts	O
the	O
question	O
of	O
whether	O
adversarial	O
examples	O
crafted	O
for	O
an	O
ML	O
model	O
might	O
also	O
transfer	O
to	O
humans	O
.	O
Recent	O
surprising	O
work	O
by	O
Elsayed	O
et	O
al	O
(	O
2018	O
)	O
explores	O
this	O
question	O
for	O
vision	O
.	O
Humans	O
were	O
shown	O
adversarial	O
examples	O
trained	O
for	O
an	O
image	O
classifier	O
for	O
≈	O
70ms	O
,	O
and	O
asked	O
to	O
choose	O
between	O
the	O
correct	O
label	O
and	O
the	O
classifier	O
's	O
(	O
incorrect	O
)	O
predicted	O
label	O
.	O
Humans	O
selected	O
the	O
incorrect	O
label	O
more	O
frequently	O
when	O
shown	O
adversarial	O
examples	O
than	O
when	O
shown	O
unperturbed	O
images	O
.	O
Similarly	O
,	O
Hong	O
et	O
al	O
(	O
2014	O
)	O
trained	O
a	O
lowdimensional	O
representation	O
of	O
"	O
perceptual	O
space	O
,	O
"	O
and	O
used	O
the	O
decision	O
boundaries	O
of	O
the	O
model	O
to	O
find	O
images	O
that	O
confused	O
human	O
subjects	O
.	O

Priming	O
effects	O
It	O
is	O
possible	O
to	O
use	O
additional	O
stimuli	O
to	O
alter	O
perceptions	O
of	O
the	O
"	O
laurel	O
/	O
yanny	O
"	O
audio	O
clip	O
.	O
For	O
example	O
,	O
Bosker	O
(	O
2018	O
)	O
demonstrates	O
the	O
ability	O
to	O
control	O
a	O
listener	O
's	O
perception	O
by	O
"	O
priming	O
"	O
them	O
with	O
a	O
carefully	O
crafted	O
recording	O
before	O
the	O
polyperceivable	O
clip	O
is	O
played	O
.	O
Similarly	O
,	O
Guan	O
and	O
Valiant	O
(	O
2019	O
)	O
investigated	O
the	O
"	O
McGurk	O
effect	O
"	O
(	O
McGurk	O
and	O
MacDonald	O
,	O
1976	O
)	O
,	O
where	O
what	O
one	O
"	O
sees	O
"	O
affects	O
what	O
one	O
"	O
hears	O
.	O
"	O
The	O
work	O
estimated	O
the	O
fraction	O
of	O
spoken	O
words	O
that	O
,	O
when	O
accompanied	O
by	O
a	O
carefully	O
designed	O
video	O
of	O
a	O
human	O
speaker	O
,	O
would	O
be	O
perceived	O
as	O
significantly	O
different	O
words	O
by	O
listeners	O
.	O
Such	O
phenomena	O
raise	O
questions	O
about	O
how	O
our	O
autoencoder	B-MethodName
-	O
based	O
method	O
can	O
be	O
extended	O
to	O
search	O
for	O
"	O
priming	O
-	O
sensitive	O
"	O
polyperceivability	O
.	O
Security	O
implications	O
Just	O
as	O
adversarial	O
examples	O
for	O
DNNs	O
have	O
security	O
implications	O
(	O
Papernot	O
et	O
al	O
,	O
2016b	O
;	O
Carlini	O
and	O
Wagner	O
,	O
2017	O
;	O
Liu	O
et	O
al	O
,	O
2016	O
)	O
,	O
so	O
too	O
might	O
adversarial	O
examples	O
for	O
sensory	O
systems	O
.	O
For	O
example	O
,	O
if	O
a	O
video	O
clip	O
of	O
a	O
politician	O
happens	O
to	O
be	O
polyperceivable	O
,	O
an	O
adversary	O
could	O
lightly	O
edit	O
it	O
with	O
potentially	O
significant	O
ramifications	O
.	O
A	O
thorough	O
treatment	O
of	O
such	O
security	O
implications	O
is	O
left	O
to	O
future	O
work	O
.	O

In	O
this	O
paper	O
,	O
we	O
leveraged	O
ML	O
techniques	O
to	O
study	O
polyperceivability	O
in	O
humans	O
.	O
By	O
modeling	O
perceptual	O
space	O
as	O
the	O
latent	O
space	O
of	O
an	O
autoencoder	B-MethodName
,	O
we	O
were	O
able	O
to	O
discover	O
dozens	O
of	O
new	O
polyper	O
-	O
ceivable	O
instances	O
,	O
which	O
were	O
validated	O
with	O
Mechanical	O
Turk	O
experiments	O
.	O
Our	O
results	O
indicate	O
that	O
polyperceivability	O
is	O
surprisingly	O
prevalent	O
in	O
spoken	O
language	O
.	O
More	O
broadly	O
,	O
we	O
suggest	O
that	O
the	O
study	O
of	O
perceptual	O
illusions	O
can	O
offer	O
insight	O
into	O
machine	O
learning	O
systems	O
,	O
and	O
vice	O
-	O
versa	O
.	O

Synonym	O
,	O
antonym	O
and	O
their	O
relations	O
from	O
unstructured	O
text	O
are	O
fundamental	O
problems	O
in	O
information	O
classification	O
field	O
.	O
These	O
problems	O
can	O
be	O
decomposed	O
into	O
three	O
subtasks	O
:	O
word	O
extraction	O
using	O
regrex	O
,	O
relation	B-TaskName
extraction	I-TaskName
(	O
Zelenko	O
et	O
al	O
,	O
2003	O
)	O
,	O
(	O
Bunescu	O
and	O
Mooney	O
,	O
2005	O
)	O
,	O
and	O
classifying	O
the	O
logistics	O
between	O
them	O
.	O
However	O
,	O
an	O
end	O
-	O
to	O
-	O
end	O
model	O
,	O
i.e.	O
ERNIE	O
-	O
M	O
model	O
(	O
Ouyang	O
et	O
al	O
,	O
2020	O
)	O
,	O
is	O
proposed	O
to	O
solve	O
the	O
three	O
tasks	O
.	O
Presupposed	O
Taxonomies	O
-	O
Evaluating	O
Neuralnetwork	O
Semantics	O
(	O
PreTENS	O
)	O
(	O
Zamparelli	O
et	O
al	O
,	O
2022	O
)	O
is	O
a	O
task	O
to	O
predict	O
the	O
acceptability	O
of	O
simple	O
sentences	O
containing	O
constructions	O
whose	O
two	O
arguments	O
are	O
presupposed	O
to	O
be	O
or	O
not	O
to	O
be	O
in	O
an	O
ordered	O
taxonomic	O
relation	O
.	O
In	O
this	O
paper	O
,	O
we	O
first	O
present	O
a	O
simple	O
approach	O
with	O
the	O
ERNIE	O
-	O
M	O
model	O
to	O
solve	O
the	O
task	O
.	O
Although	O
the	O
ERNIE	O
-	O
M	O
model	O
performs	O
unexpectedly	O
impressive	O
,	O
the	O
model	O
has	O
poor	O
robustness	O
.	O
Hence	O
,	O
the	O
additional	O
pre	O
-	O
trained	O
model	O
is	O
introduced	O
to	O
solve	O
the	O
robustness	O
problem	O
.	O
The	O
latest	O
model	O
DeBERTaV3	O
(	O
He	O
et	O
al	O
,	O
2021	O
)	O
has	O
outstanding	O
performance	O
on	O
cross	O
-	O
linguistic	O
tasks	O
,	O
which	O
outperforms	O
BERT	B-MethodName
and	O
DeBERTa	B-MethodName
on	O
many	O
tasks	O
.	O
The	O
proposed	O
model	O
consists	O
of	O
two	O
parts	O
:	O
the	O
basic	O
ERNIE	O
-	O
M	O
model	O
and	O
the	O
pre	O
-	O
trained	O
model	O
DeBERTaV3	O
.	O
The	O
De	O
-	O
BERTaV3	O
model	O
shares	O
the	O
same	O
pre	O
-	O
trained	O
data	O
with	O
ERNIE	O
-	O
M	O
called	O
XNLI	B-DatasetName
(	O
Conneau	O
et	O
al	O
,	O
2018	O
)	O
,	O
which	O
can	O
improve	O
the	O
performance	O
and	O
robustness	O
as	O
well	O
.	O
The	O
DeBERTaV3	O
model	O
is	O
trained	O
independently	O
,	O
which	O
has	O
significant	O
improvement	O
for	O
English	O
but	O
somehow	O
brought	O
no	O
improvement	O
for	O
other	O
languages	O
.	O
Based	O
on	O
the	O
above	O
conclusion	O
,	O
we	O
employ	O
the	O
DeBERTaV3	O
model	O
for	O
English	O
-	O
task	O
only	O
.	O
To	O
better	O
understand	O
the	O
effectiveness	O
of	O
the	O
proposed	O
model	O
,	O
we	O
started	O
a	O
bunch	O
of	O
analyses	O
.	O
The	O
first	O
problem	O
is	O
the	O
data	O
-	O
set	O
limitation	O
.	O
Two	O
additional	O
datasets	O
were	O
imported	O
,	O
i.e.	O
,	O
the	O
translated	O
dataset	O
from	O
Google	B-DatasetName
translation	O
which	O
is	O
translated	O
from	O
three	O
languages	O
,	O
and	O
the	O
XNLI	B-DatasetName
dataset	O
.	O
However	O
,	O
larger	O
datasets	O
do	O
n't	O
lead	O
to	O
better	O
performance	O
.	O
We	O
compared	O
the	O
performance	O
of	O
the	O
ERNIE	O
-	O
M	O
model	O
on	O
four	O
sets	O
of	O
data	O
:	O
the	O
given	O
data	O
,	O
the	O
given	O
data	O
with	O
translated	O
data	O
,	O
the	O
given	O
data	O
with	O
XNLI	B-DatasetName
augmentation	O
,	O
and	O
the	O
given	O
data	O
with	O
both	O
the	O
translated	O
data	O
and	O
XNLI	B-DatasetName
data	O
.	O
We	O
do	O
the	O
same	O
experiments	O
with	O
the	O
DeBERTaV3	O
model	O
as	O
well	O
.	O
The	O
results	O
show	O
that	O
the	O
combination	O
of	O
ERNIE	O
-	O
M	O
with	O
all	O
the	O
three	O
datasets	O
and	O
DeBERTaV3	O
with	O
the	O
given	O
English	O
data	O
perform	O
the	O
best	O
.	O
Cross	O
-	O
Attention	O
masked	B-TaskName
language	I-TaskName
modeling	I-TaskName
(	O
CAMLM	O
)	O
is	O
to	O
align	O
cross	O
-	O
language	O
semantic	O
representations	O
on	O
parallel	O
corpora	O
.	O
Then	O
,	O
the	O
multilingual	O
representation	O
is	O
enhanced	O
with	O
transferability	O
learned	O
from	O
parallel	O
corpora	O
.	O

Back	O
-	O
Translation	B-TaskName
masked	B-TaskName
language	I-TaskName
modeling	I-TaskName
(	O
BTMLM	O
)	O
is	O
trained	O
to	O
generate	O
pseudo	O
-	O
parallel	O
sentences	O
from	O
monolingual	O
sentences	O
.	O
The	O
generated	O
pairs	O
are	O
then	O
used	O
as	O
the	O
input	O
of	O
the	O
model	O
to	O
further	O
align	O
the	O
cross	O
-	O
lingual	O
semantics	O
,	O
thus	O
enhancing	O
the	O
multilingual	O
representation	O
.	O
DeBERTaV3	O
presents	O
a	O
new	O
pre	O
-	O
trained	O
language	O
model	O
,	O
which	O
improves	O
the	O
original	O
De	O
-	O
BERTa	O
model	O
by	O
replacing	O
mask	O
language	O
modeling	O
(	O
MLM	B-DatasetName
)	O
with	O
replaced	O
token	O
detection	O
(	O
RTD	O
)	O
,	O
a	O
more	O
sample	O
-	O
efficient	O
pre	O
-	O
training	O
task	O
.	O
They	O
all	O
come	O
from	O
an	O
important	O
field	O
,	O
multilingual	O
models	O
.	O
Since	O
the	O
related	O
paper	O
was	O
published	O
at	O
the	O
end	O
of	O
2021	O
,	O
there	O
are	O
no	O
similar	O
tasks	O
have	O
been	O
done	O
and	O
published	O
.	O

In	O
this	O
section	O
,	O
we	O
first	O
introduce	O
the	O
methods	O
to	O
solving	O
the	O
multi	O
-	O
language	O
problem	O
and	O
then	O
present	O
our	O
work	O
about	O
improving	O
the	O
performance	O
on	O
uni	O
-	O
language	O
.	O
To	O
extenuate	O
over	O
-	O
fitting	O
for	O
a	O
specific	O
language	O
,	O
our	O
team	O
uses	O
a	O
multi	O
-	O
language	O
ensemble	B-TaskName
learning	I-TaskName
strategy	O
that	O
includes	O
a	O
pre	O
-	O
trained	O
language	O
model	O
and	O
a	O
multilingual	O
language	O
model	O
.	O
Based	O
on	O
the	O
approach	O
above	O
,	O
it	O
makes	O
the	O
learned	O
representation	O
generalizable	O
across	O
languages	O
and	O
improves	O
the	O
performance	O
in	O
finding	O
the	O
suitable	O
taxonomic	O
relations	O
in	O
two	O
nominal	O
arguments	O
.	O

Our	O
key	O
idea	O
of	O
solving	O
multilingual	O
language	O
tasks	O
is	O
to	O
learn	O
the	O
language	O
invariant	O
feature	O
space	O
shared	O
among	O
multiple	O
languages	O
.	O
We	O
tried	O
multilingual	O
masked	B-TaskName
language	I-TaskName
modeling	I-TaskName
(	O
MMLM	O
)	O
,	O
translation	O
language	O
modeling	O
(	O
TLM	O
)	O
,	O
and	O
crossattention	O
masked	B-TaskName
language	I-TaskName
modeling	I-TaskName
(	O
CAMLM	O
)	O
have	O
been	O
tried	O
.	O
However	O
,	O
the	O
scale	O
of	O
the	O
parallel	O
corpus	O
is	O
quite	O
limited	O
,	O
which	O
limits	O
the	O
performance	O
of	O
the	O
model	O
.	O
However	O
,	O
we	O
found	O
that	O
using	O
the	O
transferability	O
learned	O
from	O
parallel	O
corpora	O
to	O
enhance	O
the	O
model	O
's	O
learning	O
of	O
large	O
-	O
scale	O
monolingual	O
corpora	O
to	O
enhance	O
multilingual	O
semantic	O
representation	O
can	O
achieve	O
a	O
good	O
effect	O
.	O
ERNIE	O
-	O
M	O
does	O
this	O
by	O
making	O
the	O
predictions	O
of	O
tokens	O
depending	O
on	O
tokens	O
in	O
another	O
language	O
,	O
but	O
not	O
on	O
other	O
tokens	O
in	O
this	O
language	O
.	O
Therefore	O
,	O
we	O
choose	O
ERNIE	O
-	O
M	O
as	O
the	O
baseline	O
model	O
for	O
this	O
task	O
and	O
explore	O
on	O
this	O
basis	O
to	O
improve	O
the	O
prediction	O
effect	O
.	O
In	O
the	O
process	O
of	O
using	O
multilingual	O
language	O
models	O
,	O
we	O
mainly	O
adopt	O
random	B-MethodName
search	I-MethodName
to	O
finetune	O
the	O
ERNIE	O
-	O
M	O
model	O
and	O
data	B-TaskName
augmentation	I-TaskName
methods	O
are	O
used	O
for	O
model	O
training	O
.	O
Cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
natural	I-TaskName
language	I-TaskName
inference	I-TaskName
(	O
XNLI	B-DatasetName
)	O
dataset	O
is	O
used	O
and	O
the	O
English	O
training	O
set	O
is	O
translated	O
to	O
Italian	O
(	O
E2I	O
set	O
)	O
.	O
Firstly	O
,	O
the	O
English	O
training	O
set	O
is	O
combined	O
with	O
the	O
French	O
and	O
E2I	O
set	O
.	O
Then	O
,	O
the	O
model	O
is	O
fine	O
-	O
tuned	O
with	O
the	O
combined	O
training	O
set	O
.	O
Finally	O
,	O
the	O
augmented	O
task	O
training	O
set	O
in	O
three	O
languages	O
is	O
adopted	O
for	O
fine	O
-	O
tune	O
process	O
.	O

To	O
enhance	O
the	O
effect	O
in	O
a	O
single	O
language	O
subtask	O
,	O
we	O
consider	O
using	O
an	O
enhanced	O
mask	O
decoder	O
and	O
a	O
disentangled	B-MethodName
attention	I-MethodName
mechanism	I-MethodName
to	O
improve	O
the	O
effect	O
.	O
DeBERTaV3	O
meets	O
our	O
needs	O
by	O
using	O
Electra	B-MethodName
-	O
style	O
pre	O
-	O
training	O
and	O
gradient	O
unwrapping	O
embedding	O
sharing	O
.	O
We	O
have	O
tried	O
to	O
use	O
DeBER	O
-	O
TaV3	O
for	O
training	O
in	O
each	O
single	O
language	O
subtask	O
respectively	O
.	O

By	O
using	O
the	O
multilingual	O
language	O
model	O
and	O
pretrained	O
language	O
model	O
respectively	O
,	O
we	O
have	O
two	O
groups	O
of	O
validation	O
set	O
results	O
for	O
each	O
language	O
.	O
We	O
adopt	O
the	O
mean	O
of	O
the	O
best	O
-	O
saved	O
models	O
from	O
ERNIE	O
-	O
M	O
and	O
DeBERTaV3	O
after	O
making	O
predictions	O
on	O
the	O
validation	O
set	O
.	O
After	O
comparing	O
the	O
Figure	O
1	O
:	O
The	O
process	O
of	O
10	O
-	O
fold	O
cross	O
-	O
validation	O
and	O
ensemble	O
.	O
The	O
training	O
set	O
which	O
includes	O
all	O
three	O
languages	O
is	O
divided	O
randomly	O
10	O
times	O
by	O
setting	O
different	O
random	O
seeds	B-DatasetName
.	O
In	O
each	O
division	O
,	O
the	O
training	O
set	O
is	O
divided	O
into	O
10	O
parts	O
,	O
of	O
which	O
9	O
parts	O
are	O
respectively	O
used	O
as	O
the	O
training	O
set	O
and	O
the	O
remaining	O
1	O
part	O
is	O
used	O
as	O
the	O
validation	O
set	O
.	O
And	O
finally	O
,	O
the	O
average	O
of	O
all	O
saved	O
best	O
models	O
predicted	O
on	O
the	O
test	O
set	O
is	O
the	O
final	O
results	O
.	O
combination	O
result	O
,	O
we	O
finally	O
used	O
different	O
strategies	O
in	O
different	O
languages	O
.	O
For	O
the	O
English	O
subtask	O
,	O
we	O
retain	O
the	O
strategy	O
of	O
merging	O
the	O
two	O
types	O
of	O
models	O
.	O
For	O
French	O
and	O
Italian	O
subtasks	O
,	O
the	O
result	O
from	O
cross	O
-	O
validation	O
of	O
the	O
multilingual	O
language	O
model	O
is	O
used	O
directly	O
.	O

As	O
the	O
total	O
number	O
of	O
labeled	O
data	O
in	O
each	O
language	O
is	O
only	O
5840	O
,	O
it	O
's	O
liable	O
to	O
overfit	O
the	O
training	O
data	O
even	O
with	O
pre	O
-	O
trained	O
models	O
.	O
The	O
overfitting	O
phenomenon	O
may	O
be	O
more	O
significant	O
than	O
expected	O
because	O
the	O
data	O
is	O
generated	O
programmatically	O
through	O
manually	O
verified	O
templates	O
.	O
To	O
increase	O
the	O
size	O
of	O
training	O
data	O
,	O
we	O
use	O
the	O
following	O
data	B-TaskName
augmentation	I-TaskName
methods	O
:	O
1	O
)	O
translate	O
English	O
data	O
into	O
French	O
and	O
Italian	O
by	O
using	O
Baidu	O
translate	O
2	O
)	O
translate	O
English	O
data	O
into	O
French	O
and	O
Italian	O
by	O
using	O
Google	B-DatasetName
translate	O
3	O
)	O
translate	O
French	O
and	O
Italian	O
data	O
into	O
English	O
by	O
using	O
Google	B-DatasetName
translate	O
.	O
We	O
find	O
that	O
the	O
augmentation	O
can	O
help	O
delay	O
the	O
overfitting	O
occurrence	O
slightly	O
,	O
especially	O
for	O
large	O
models	O
.	O

Our	O
dataset	O
comes	O
from	O
two	O
parts	O
.	O
The	O
first	O
part	O
is	O
the	O
trial	O
dataset	O
released	O
by	O
organizers	O
,	O
which	O
is	O
composed	O
of	O
English	O
,	O
French	O
and	O
Italian	O
.	O
Each	O
language	O
contains	O
5838	O
sentences	O
.	O
Because	O
the	O
trail	O
dataset	O
provided	O
by	O
organizers	O
is	O
only	O
5838	O
in	O
each	O
language	O
,	O
to	O
increase	O
the	O
amount	O
of	O
data	O
and	O
make	O
the	O
model	O
better	O
,	O
we	O
use	O
Google	B-DatasetName
translator	O
and	O
Baidu	O
translator	O
to	O
translate	O
the	O
English	O
dataset	O
into	O
French	O
and	O
Italian	O
again	O
.	O
The	O
use	O
of	O
two	O
different	O
translators	O
also	O
increases	O
the	O
diversity	O
of	O
data	O
.	O
The	O
other	O
part	O
is	O
that	O
we	O
use	O
the	O
public	O
dataset	O
-	O
XNLI	B-DatasetName
.	O
We	O
use	O
XNLI	B-DatasetName
dataset	O
because	O
it	O
is	O
often	O
used	O
in	O
similar	O
cross	O
-	O
language	O
tasks	O
.	O
The	O
XNLI	B-DatasetName
dataset	O
contains	O
a	O
total	O
of	O
15	O
languages	O
,	O
and	O
each	O
language	O
contains	O
7500	O
pairs	O
of	O
data	O
.	O
We	O
used	O
the	O
English	O
and	O
French	O
datasets	O
in	O
this	O
competition	O
.	O
Because	O
the	O
XNLI	B-DatasetName
dataset	O
itself	O
does	O
not	O
contain	O
Italian	O
datasets	O
,	O
we	O
translated	O
the	O
English	O
dataset	O
into	O
Italian	O
and	O
then	O
used	O
the	O
three	O
languages	O
in	O
ERNIE	O
-	O
M	O
model	O
training	O
.	O

In	O
this	O
task	O
,	O
we	O
mainly	O
use	O
the	O
ERNIE	O
-	O
M	O
model	O
and	O
DeBERTaV3	O
model	O
.	O
The	O
ERNIE	O
-	O
M	O
model	O
is	O
composed	O
of	O
24	O
layers	O
,	O
1024	O
hidden	O
,	O
and	O
16	O
heads	O
.	O
In	O
terms	O
of	O
parameter	O
selection	O
,	O
we	O
set	O
a	O
set	O
of	O
parameters	O
,	O
as	O
Table	O
2	O
shows	O
.	O
We	O
set	O
up	O
10000	O
times	O
of	O
ERNIE	O
-	O
M	O
model	O
training	O
,	O
in	O
which	O
the	O
specific	O
values	O
of	O
the	O
above	O
parameters	O
are	O
randomly	O
selected	O
according	O
to	O
the	O
ta	O
-	O
For	O
the	O
comparative	O
analysis	O
of	O
the	O
results	O
of	O
using	O
only	O
ERNIE	O
-	O
M	O
as	O
the	O
baseline	O
model	O
and	O
the	O
ensemble	O
model	O
,	O
we	O
can	O
see	O
that	O
the	O
improvement	O
of	O
the	O
ensemble	O
model	O
in	O
English	O
is	O
relatively	O
obvious	O
,	O
but	O
the	O
improvement	O
in	O
Italian	O
and	O
French	O
is	O
very	O
weak	O
.	O
We	O
think	O
this	O
is	O
due	O
to	O
the	O
following	O
reasons	O
:	O
Firstly	O
,	O
Italian	O
is	O
not	O
included	O
in	O
the	O
original	O
XNLI	B-DatasetName
dataset	O
.	O
In	O
this	O
task	O
,	O
we	O
translate	O
English	O
into	O
Italian	O
.	O
So	O
to	O
a	O
certain	O
extent	O
,	O
the	O
understanding	O
of	O
English	O
by	O
the	O
ERNIE	O
-	O
M	O
model	O
is	O
increased	O
.	O
Secondly	O
,	O
because	O
DeBERTaV3	O
performs	O
well	O
in	O
English	O
,	O
we	O
only	O
use	O
its	O
results	O
in	O
English	O
,	O
So	O
the	O
results	O
for	O
Italian	O
and	O
French	O
did	O
not	O
get	O
a	O
big	O
boost	O
.	O
This	O
also	O
shows	O
that	O
using	O
the	O
ensemble	O
model	O
can	O
indeed	O
improve	O
the	O
prediction	O
.	O
In	O
the	O
future	O
,	O
we	O
will	O
explore	O
ensemble	O
models	O
that	O
can	O
improve	O
predictions	O
in	O
Italian	O
and	O
French	O
.	O

To	O
solve	O
the	O
problem	O
of	O
judging	O
whether	O
the	O
meaning	O
of	O
a	O
sentence	O
is	O
self	O
-	O
consistent	O
in	O
multilingual	O
language	O
tasks	O
,	O
that	O
is	O
,	O
the	O
problem	O
raised	O
in	O
task	O
3	O
,	O
we	O
propose	O
an	O
ensemble	O
model	O
using	O
ERNIE	O
-	O
M	O
and	O
DeBERTaV3	O
,	O
and	O
regard	O
this	O
problem	O
as	O
a	O
binary	O
classification	O
problem	O
.	O
Furthermore	O
,	O
to	O
solve	O
the	O
issue	O
of	O
the	O
small	O
dataset	O
,	O
we	O
use	O
various	O
strategies	O
,	O
such	O
as	O
K	O
-	O
ford	O
cross	O
-	O
validation	O
,	O
translating	O
the	O
dataset	O
using	O
different	O
translators	O
,	O
and	O
introducing	O
an	O
external	O
dataset	O
-	O
XNLI	B-DatasetName
,	O
a	O
dataset	O
commonly	O
used	O
in	O
multilingual	O
problems	O
.	O
In	O
future	O
efforts	O
,	O
we	O
plan	O
to	O
further	O
improve	O
our	O
model	O
from	O
these	O
aspects	O
.	O
The	O
first	O
is	O
to	O
enrich	O
the	O
data	O
,	O
especially	O
Italian	O
and	O
French	O
,	O
to	O
help	O
the	O
model	O
learn	O
better	O
.	O
The	O
second	O
is	O
that	O
we	O
could	O
train	O
more	O
models	O
on	O
standard	O
fine	O
-	O
tuning	O
,	O
multi	O
-	O
step	O
fine	O
-	O
tuning	O
,	O

multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
,	O
or	O
adversarial	O
training	O
.	O
Then	O
try	O
to	O
ensemble	O
different	O
models	O
to	O
gain	O
a	O
better	O
performance	O
.	O

Multilingual	O
Code	O
-	O
Switching	O
for	O
Zero	O
-	O
Shot	O
Cross	O
-	O
Lingual	O
Intent	O
Prediction	O
and	O
Slot	B-TaskName
Filling	I-TaskName

A	O
cross	O
-	O
lingual	O
setting	O
is	O
typically	O
described	O
as	O
a	O
scenario	O
in	O
which	O
a	O
model	O
trained	O
for	O
a	O
particular	O
task	O
in	O
one	O
source	O
language	O
(	O
e.g.	O
English	O
)	O
should	O
be	O
able	O
to	O
generalize	O
well	O
to	O
a	O
different	O
target	O
language	O
(	O
e.g.	O
Japanese	O
)	O
.	O
While	O
semi	O
-	O
supervised	O
solutions	O
(	O
Muis	O
et	O
al	O
,	O
2018	O
;	O
FitzGerald	O
,	O
2020	O
,	O
inter	O
alia	O
)	O
assume	O
some	O
target	O
language	O
data	O
or	O
translators	O
are	O
available	O
,	O
a	O
zero	O
-	O
shot	O
solution	O
(	O
Eriguchi	O
et	O
al	O
,	O
2018	O
;	O
Srivastava	O
et	O
al	O
,	O
2018	O
;	O
assumes	O
none	O
is	O
available	O
at	O
training	O
time	O
.	O
Having	O
models	O
that	O
generalize	O
well	O
even	O
to	O
unseen	O
languages	O
is	O
crucial	O
for	O
tackling	O
real	O
world	O
problems	O
such	O
as	O
extracting	O
relevant	O
information	O
during	O
a	O
new	O
disaster	O
(	O
Nguyen	O
et	O
al	O
,	O
2017	O
;	O
Krishnan	O
et	O
al	O
,	O
2020	O
)	O
or	O
detecting	O
hate	B-DatasetName
speech	I-DatasetName
(	O
Pamungkas	O
and	O
Patti	O
,	O
2019	O
;	O
Stappen	O
et	O
al	O
,	O
2020	O
)	O
,	O
where	O
the	O
target	O
language	O
might	O
be	O
of	O
low	O
-	O
resource	O
or	O
unknown	O
.	O
Intent	O
prediction	O
and	O
slot	B-TaskName
filling	I-TaskName
are	O
two	O
NLU	O
tasks	O
,	O
usually	O
solved	O
jointly	O
,	O
which	O
learn	O
to	O
model	O
the	O
intent	O
(	O
sentence	O
-	O
level	O
)	O
and	O
slot	O
(	O
word	O
-	O
level	O
)	O
labels	O
.	O
Such	O
models	O
are	O
currently	O
used	O
extensively	O
for	O
goal	B-TaskName
-	I-TaskName
oriented	I-TaskName
dialogue	I-TaskName
systems	I-TaskName
,	O
such	O
as	O
Amazon	O
's	O
Alexa	O
,	O
Apple	O
's	O
Siri	O
,	O
Google	B-DatasetName
Assistant	O
,	O
and	O
Microsoft	O
's	O
Cortana	O
.	O
Finding	O
the	O
'	O
intent	O
'	O
behind	O
the	O
user	O
's	O
query	O
and	O
identifying	O
relevant	O
'	O
slots	O
'	O
in	O
the	O
sentence	O
to	O
engage	O
in	O
a	O
dialogue	O
are	O
essential	O
for	O
effective	O
conversational	O
assistance	O
.	O
For	O
example	O
,	O
users	O
might	O
want	O
to	O
'	O
play	O
music	O
'	O
given	O
the	O
slot	O
labels	O
'	O
year	O
'	O
and	O
'	O
artist	O
'	O
(	O
Coucke	O
et	O
al	O
,	O
2018	O
)	O
,	O
or	O
they	O
may	O
want	O
to	O
'	O
book	O
a	O
flight	O
'	O
given	O
the	O
'	O
airport	O
'	O
and	O
'	O
locations	O
'	O
slot	O
labels	O
(	O
Price	O
,	O
1990	O
)	O
.	O
A	O
strong	O
correlation	O
between	O
the	O
two	O
tasks	O
has	O
made	O
jointly	O
trained	O
models	O
successful	O
(	O
Goo	O
et	O
al	O
,	O
2018	O
;	O
Haihong	O
et	O
al	O
,	O
2019	O
;	O
Hardalov	O
et	O
al	O
,	O
2020	O
;	O
.	O
In	O
a	O
cross	O
-	O
lingual	O
setting	O
,	O
the	O
model	O
should	O
be	O
able	O
to	O
learn	O
this	O
joint	O
task	O
in	O
one	O
language	O
and	O
transfer	O
knowledge	O
to	O
another	O
(	O
Upadhyay	O
et	O
al	O
,	O
2018	O
;	O
Schuster	O
et	O
al	O
,	O
2019	O
;	O
.	O
This	O
is	O
the	O
premise	O
of	O
our	O
work	O
.	O
Highly	O
effective	O
transformer	O
-	O
based	O
multilingual	O
models	O
such	O
as	O
mBERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
and	O
XLM	B-MethodName
-	O
R	O
(	O
Conneau	O
et	O
al	O
,	O
2020a	O
)	O
have	O
found	O
success	O
across	O
several	O
multilingual	O
tasks	O
in	O
recent	O
years	O
.	O
In	O
the	O
zero	B-TaskName
-	I-TaskName
shot	I-TaskName
cross	I-TaskName
-	I-TaskName
lingual	I-TaskName
transfer	I-TaskName
setting	O
with	O
an	O
unknown	O
target	O
language	O
,	O
a	O
typical	O
solution	O
is	O
to	O
use	O
pre	O
-	O
trained	O
transformer	O
models	O
and	O
fine	O
-	O
tune	O
to	O
the	O
downstream	O
task	O
using	O
the	O
monolingual	O
source	O
data	O
.	O
However	O
,	O
Pires	O
et	O
al	O
(	O
2019	O
)	O
showed	O
that	O
existing	O
transformer	O
-	O
based	O
represen	O
-	O
Figure	O
1	O
:	O
t	O
-	O
SNE	O
plot	O
of	O
embeddings	O
across	O
the	O
12	O
multi	B-MethodName
-	I-MethodName
head	I-MethodName
attention	I-MethodName
layers	O
of	O
multilingual	O
BERT	B-MethodName
.	O
Parallelly	O
translated	O
sentences	O
of	O
MutiATIS++	O
dataset	O
are	O
still	O
clustered	O
according	O
to	O
the	O
languages	O
:	O
English	O
(	O
black	O
)	O
,	O
Chinese	O
(	O
cyan	O
)	O
,	O
French	O
(	O
blue	O
)	O
,	O
German	O
(	O
green	O
)	O
,	O
and	O
Japanese	O
(	O
red	O
)	O
.	O
Figure	O
2	O
:	O
An	O
original	O
example	O
in	O
English	O
from	O
MultiATIS++	O
dataset	O
and	O
its	O
multilingually	O
code	O
-	O
switched	O
version	O
.	O
In	O
the	O
above	O
code	O
-	O
switching	O
example	O
,	O
the	O
chunks	O
are	O
in	O
Chinese	O
,	O
Punjabi	B-DatasetName
,	O
Spanish	O
,	O
English	O
,	O
Arabic	O
,	O
and	O
Russian	O
.	O
'	O
atis_airfare	O
'	O
represents	O
an	O
intent	O
class	O
where	O
the	O
user	O
seeks	O
price	O
of	O
a	O
ticket	O
.	O
tations	O
may	O
exhibit	O
systematic	O
deficiencies	O
for	O
certain	O
language	O
pairs	O
.	O
Figure	O
1	O
also	O
verifies	O
that	O
the	O
representations	O
across	O
the	O
12	O
multi	B-MethodName
-	I-MethodName
head	I-MethodName
attention	I-MethodName
layers	O
of	O
mBERT	B-MethodName
are	O
still	O
not	O
shared	O
across	O
languages	O
,	O
instead	O
forming	O
clearly	O
distinguishable	O
clusters	O
per	O
language	O
.	O
This	O
leads	O
to	O
a	O
fundamental	O
challenge	O
that	O
we	O
address	O
in	O
this	O
work	O
:	O
enhancing	O
the	O
language	O
neutrality	O
so	O
that	O
the	O
fine	O
-	O
tuned	O
model	O
is	O
generalizable	O
across	O
languages	O
for	O
the	O
downstream	O
task	O
.	O
To	O
this	O
goal	O
,	O
we	O
introduce	O
a	O
data	B-TaskName
augmentation	I-TaskName
method	O
via	O
multilingual	O
codeswitching	O
,	O
where	O
the	O
original	O
sentence	O
in	O
English	O
is	O
code	O
-	O
switched	O
into	O
randomly	O
selected	O
languages	O
.	O
For	O
example	O
,	O
chunk	O
-	O
level	O
code	O
-	O
switching	O
creates	O
sentences	O
with	O
phrases	O
in	O
multiple	O
languages	O
as	O
shown	O
in	O
Figure	O
2	O
.	O
We	O
show	O
that	O
mBERT	B-MethodName
can	O
be	O
fine	O
-	O
tuned	O
for	O
many	O
languages	O
starting	O
only	O
with	O
monolingual	O
source	O
-	O
language	O
data	O
,	O
leading	O
to	O
better	O
performance	O
in	O
zero	O
-	O
shot	O
settings	O
.	O
Further	O
,	O
we	O
show	O
how	O
code	O
-	O
switching	O
with	O
languages	O
from	O
different	O
language	O
families	O
impacts	O
the	O
model	O
's	O
performance	O
on	O
individual	O
target	O
languages	O
,	O
even	O
finding	O
some	O
counter	O
-	O
intuitive	O
results	O
.	O
For	O
instance	O
,	O
training	O
on	O
data	O
code	O
-	O
switched	O
between	O
English	O
and	O
Sino	O
-	O
Tibetan	O
languages	O
is	O
as	O
helpful	O
for	O
Hindi	O
(	O
an	O
Indo	O
-	O
Aryan	O
Indo	O
-	O
European	O
language	O
)	O
as	O
code	O
-	O
switching	O
with	O
other	O
Indo	O
-	O
Aryan	O
languages	O
,	O
and	O
Turkic	O
languages	O
can	O
be	O
helpful	O
for	O
both	O
Chinese	O
and	O
Japanese	O
.	O

We	O
present	O
a	O
data	B-TaskName
augmentation	I-TaskName
method	O
via	O
multilingual	O
code	O
-	O
switching	O
to	O
enhance	O
the	O
language	O
neutrality	O
of	O
transformerbased	O
language	O
models	O
such	O
as	O
mBERT	B-MethodName
for	O
finetuning	O
to	O
a	O
downstream	O
NLU	O
task	O
of	O
intent	O
prediction	O
and	O
slot	B-TaskName
filling	I-TaskName
.	O
b	O
)	O
By	O
studying	O
different	O
language	O
families	O
,	O
we	O
show	O
how	O
code	O
-	O
switching	O
can	O
be	O
used	O
to	O
aid	O
zero	O
-	O
shot	O
cross	O
-	O
lingual	O
learning	O
for	O
low	O
-	O
resource	O
languages	O
.	O
c	O
)	O
We	O
release	O
a	O
new	O
human	O
-	O
annotated	O
tweet	O
dataset	O
,	O
collected	O
during	O
Haiti	O
earthquake	O
disaster	O
,	O
for	O
intent	O
prediction	O
and	O
slot	B-TaskName
filling	I-TaskName
in	O
English	O
and	O
Haitian	O
Creole	O
.	O

Multilingual	O
masked	O
language	O
models	O
,	O
such	O
as	O
mBERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
are	O
trained	O
using	O
large	O
datasets	O
of	O
publicly	O
available	O
unlabeled	O
corpora	O
such	O
as	O
Wikipedia	O
.	O
Such	O
corpora	O
largely	O
remain	O
monolingual	O
at	O
the	O
sentence	O
level	O
because	O
the	O
presence	O
of	O
intra	O
-	O
sentence	O
code	O
-	O
switched	O
data	O
in	O
written	O
texts	O
is	O
likely	O
scarce	O
.	O
The	O
masked	O
words	O
that	O
needed	O
to	O
be	O
predicted	O
usually	O
are	O
in	O
the	O
same	O
language	O
as	O
their	O
surrounding	O
words	O
.	O
We	O
study	O
how	O
code	O
-	O
switching	O
can	O
enhance	O
the	O
language	O
neutrality	O
of	O
such	O
language	O
models	O
by	O
augmenting	O
it	O
with	O
artificially	O
code	O
-	O
switched	O
data	O
for	O
fine	O
-	O
tuning	O
it	O
to	O
a	O
downstream	O
task	O
.	O
Algorithm	O
1	O
explains	O
this	O
codeswitching	O
process	O
at	O
the	O
chunk	O
-	O
level	O
.	O
When	O
using	O
slot	B-TaskName
filling	I-TaskName
datasets	O
,	O
slot	O
labels	O
that	O
are	O
grouped	O
by	O
BIO	O
(	O
Ramshaw	O
and	O
Marcus	O
,	O
1999	O
)	O
tags	O
constitute	O
natural	O
chunks	O
,	O
as	O
shown	O
in	O
Figure	O
2	O
.	O
To	O
summarize	O
the	O
algorithm	O
,	O
we	O
take	O
a	O
sentence	O
,	O
take	O
each	O
chunk	O
from	O
that	O
sentence	O
,	O
perform	O
a	O
translation	O
into	O
a	O
random	O
language	O
using	O
Google	B-DatasetName
's	O
NMT	O
system	O
(	O
Wu	O
et	O
al	O
,	O
2016	O
)	O
,	O
and	O
align	O
the	O
slot	O
labels	O
to	O
fit	O
the	O
translation	O
,	O
i.e.	O
,	O
label	O
propagation	O
through	O
alignment	O
as	O
the	O
translated	O
sentence	O
do	O
not	O
preserve	O
the	O

Afro	O
-	O
Asiatic	O
Arabic	O
(	O
ar	O
)	O
,	O
Amharic	O
(	O
am	O
)	O
,	O
Hebrew	O
(	O
he	O
)	O
,	O
Somali	O
(	O
so	O
)	O
Germanic	O
German	O
(	O
de	O
)	O
,	O
Dutch	O
(	O
nl	O
)	O
,	O
Danish	O
(	O
da	O
)	O
,	O
Swedish	O
(	O
sv	O
)	O
,	O
Norwegian	O
(	O
no	O
)	O
Indo	O
-	O
Aryan	O
Hindi	O
(	O
hi	O
)	O
,	O
Bengali	O
(	O
bn	O
)	O
,	O
Marathi	O
(	O
mr	O
)	O
,	O
Nepali	O
(	O
ne	O
)	O
,	O
Gujarati	O
(	O
gu	O
)	O
,	O
Punjabi	B-DatasetName
(	O
pa	O
)	O
Romance	O
Spanish	O
(	O
es	O
)	O
,	O
Portuguese	O
(	O
pt	O
)	O
,	O
French	O
(	O
fr	O
)	O
,	O
Italian	O
(	O
it	O
)	O
,	O
Romanian	O
(	O
ro	O
)	O
Sino	O
-	O
Tibetan	O
,	O
Koreanic	O
,	O
&	O
Japonic	O
Chinese	O
(	O
zh	O
-	O
cn	O
)	O
,	O
Japanese	O
(	O
ja	O
)	O
,	O
Korean	O
(	O
ko	O
)	O
Turkic	O
Turkish	O
(	O
tr	O
)	O
,	O
Azerbaijani	O
(	O
az	O
)	O
,	O
Uyghur	O
(	O
ug	O
)	O
,	O
Kazakh	O
(	O
kk	O
)	O
number	O
and	O
order	O
of	O
words	O
in	O
the	O
original	O
sentence	O
.	O
At	O
the	O
chunk	O
-	O
level	O
,	O
we	O
use	O
a	O
direct	O
alignment	O
.	O
The	O
BIO	O
-	O
tagged	O
labels	O
are	O
recreated	O
for	O
the	O
translated	O
phrase	O
based	O
on	O
the	O
word	O
tokens	O
.	O
More	O
complex	O
methods	O
could	O
be	O
applied	O
here	O
to	O
improve	O
the	O
alignment	O
of	O
the	O
slot	O
labels	O
such	O
as	O
fast	O
-	O
align	O
(	O
Dyer	O
et	O
al	O
,	O
2013	O
)	O
or	O
soft	O
-	O
align	O
,	O
but	O
we	O
leave	O
this	O
for	O
future	O
work	O
.	O
Code	O
-	O
Switching	O
at	O
the	O
word	O
-	O
level	O
essentially	O
translates	O
every	O
word	O
randomly	O
,	O
while	O
at	O
the	O
sentence	O
-	O
level	O
translates	O
the	O
entire	O
sentence	O
.	O
During	O
the	O
experimental	O
evaluation	O
process	O
,	O
to	O
build	O
a	O
language	O
-	O
neutral	O
model	O
using	O
monolingual	O
source	O
(	O
English	O
)	O
data	O
,	O
all	O
eight	O
target	O
languages	O
are	O
excluded	O
from	O
the	O
code	O
-	O
switching	O
procedure	O
to	O
avoid	O
unfair	O
model	O
comparisons	O
,	O
i.e.	O
removing	O
target	O
languages	O
(	O
l	O
T	O
)	O
from	O
lset	O
in	O
Algorithm	O
1	O
.	O
Complexity	O
.	O
The	O
augmentation	O
process	O
is	O
repeated	O
k	O
times	O
per	O
sentence	O
producing	O
a	O
new	O
augmented	O
dataset	O
of	O
size	O
k	O
×	O
n	O
,	O
where	O
n	O
is	O
the	O
size	O
of	O
the	O
original	O
dataset	O
,	O
i.e.	O
space	O
complexity	O
of	O
O	O
(	O
k	O
×	O
n	O
)	O
.	O
For	O
T	O
translations	O
per	O
sentence	O
,	O
Algorithm	O
1	O
has	O
a	O
runtime	O
complexity	O
of	O
O	O
(	O
k	O
×	O
n	O
×	O
T	O
)	O
assuming	O
constant	O
time	O
for	O
alignment	O
.	O
Word	O
-	O
level	O
requires	O
as	O
many	O
translations	O
as	O
the	O
number	O
of	O
words	O
but	O
sentence	O
-	O
level	O
requires	O
only	O
one	O
.	O
An	O
increase	O
in	O
the	O
dataset	O
size	O
also	O
increases	O
the	O
training	O
time	O
,	O
but	O
the	O
advantage	O
is	O
one	O
model	O
appropriate	O
for	O
many	O
languages	O
.	O

Since	O
we	O
assume	O
that	O
target	O
language	O
is	O
not	O
known	O
before	O
hand	O
,	O
Translate	O
-	O
Train	O
(	O
TT	O
)	O
method	O
is	O
not	O
a	O
suitable	O
baseline	O
.	O
Rather	O
,	O
we	O
set	O
this	O
to	O
be	O
an	O
upper	O
bound	O
,	O
i.e.	O
translating	O
to	O
the	O
target	O
language	O
and	O
fine	O
-	O
tuning	O
the	O
model	O
should	O
intuitively	O
outperform	O
a	O
generic	O
model	O
.	O
Additionally	O
,	O
we	O
add	O
code	O
-	O
switching	O
to	O
this	O
TT	O
model	O
to	O
assess	O
if	O
augmentation	O
negatively	O
impacts	O
its	O
performance	O
.	O
The	O
zero	O
-	O
shot	O
baselines	O
for	O
the	O
codeswitching	O
experiments	O
use	O
an	O
English	O
-	O
Only	O
)	O
model	O
,	O
which	O
is	O
fine	O
-	O
tuned	O
over	O
the	O
pre	O
-	O
trained	O
mBERT	B-MethodName
separately	O
for	O
each	O
task	O
and	O
an	O
English	O
-	O
only	O
Joint	O
model	O
.	O

Cross	B-TaskName
-	I-TaskName
Lingual	I-TaskName
Transfer	I-TaskName
.	O
Researchers	O
have	O
studied	O
cross	O
-	O
lingual	O
tasks	O
in	O
various	O
settings	O
such	O
as	O
sentiment	O
/	O
sequence	O
classification	O
(	O
Wan	O
,	O
2009	O
;	O
Eriguchi	O
et	O
al	O
,	O
2018	O
;	O
,	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
Zirikly	O
and	O
Hagiwara	O
,	O
2015	O
;	O
Tsai	O
et	O
Xie	O
et	O
al	O
,	O
2018	O
)	O
,	O
parts	O
-	O
of	O
-	O
speech	O
tagging	O
(	O
Yarowsky	O
et	O
al	O
,	O
2001	O
;	O
Täckström	O
et	O
al	O
,	O
2013	O
;	O
Plank	O
and	O
Agić	O
,	O
2018	O
)	O
,	O
and	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
(	O
He	O
et	O
al	O
,	O
2013	O
;	O
Upadhyay	O
et	O
al	O
,	O
2018	O
;	O
.	O
The	O
methodology	O
for	O
most	O
of	O
the	O
current	O
approaches	O
for	O
cross	O
-	O
lingual	O
tasks	O
can	O
be	O
categorizes	O
as	O
:	O
a	O
)	O
multilingual	O
representations	O
from	O
pre	O
-	O
trained	O
or	O
fine	O
-	O
tuned	O
models	O
such	O
as	O
mBERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
or	O
XLM	B-MethodName
-	O
R	O
(	O
Conneau	O
et	O
al	O
,	O
2020a	O
)	O
,	O
b	O
)	O
machine	B-TaskName
translation	I-TaskName
followed	O
by	O
alignment	O
(	O
Shah	O
et	O
al	O
,	O
2010	O
;	O
Yarowsky	O
et	O
al	O
,	O
2001	O
;	O
Ni	O
et	O
al	O
,	O
2017	O
)	O
,	O
or	O
c	O
)	O
a	O
combination	O
of	O
both	O
.	O
Before	O
transformer	O
models	O
,	O
effective	O
approaches	O
included	O
domain	O
adversarial	O
training	O
to	O
extract	O
language	O
-	O
agnostic	O
features	O
(	O
Ganin	O
et	O
al	O
,	O
2016	O
;	O
and	O
word	B-TaskName
alignment	I-TaskName
methods	O
such	O
as	O
MUSE	B-DatasetName
(	O
Conneau	O
et	O
al	O
,	O
2017	O
)	O
to	O
align	O
fastText	B-MethodName
word	O
vectors	O
(	O
Bojanowski	O
et	O
al	O
,	O
2017	O
)	O
.	O
Recently	O
,	O
Conneau	O
et	O
al	O
,	O
2020b	O
show	O
that	O
having	O
shared	O
parameters	O
in	O
the	O
top	O
layers	O
of	O
the	O
multilingual	O
encoders	O
can	O
be	O
used	O
to	O
align	O
different	O
languages	O
quite	O
effectively	O
on	O
tasks	O
such	O
as	O
XNLI	B-DatasetName
(	O
Conneau	O
et	O
al	O
,	O
2018	O
)	O
.	O
Monolingual	O
models	O
for	O
joint	O
slot	B-TaskName
filling	I-TaskName
and	O
intent	O
prediction	O
have	O
used	O
attention	O
-	O
based	O
RNN	O
(	O
Liu	O
and	O
Lane	O
,	O
2016	O
)	O
and	O
attention	O
-	O
based	O
BiLSTM	B-MethodName
with	O
a	O
slot	O
gate	O
(	O
Goo	O
et	O
al	O
,	O
2018	O
)	O
on	O
benchmark	O
datasets	O
(	O
Price	O
,	O
1990	O
;	O
Coucke	O
et	O
al	O
,	O
2018	O
)	O
.	O
These	O
methods	O
have	O
shown	O
that	O
a	O
joint	O
method	O
can	O
enhance	O
both	O
tasks	O
and	O
slot	B-TaskName
filling	I-TaskName
can	O
be	O
conditioned	O
on	O
the	O
learned	O
intent	O
.	O
A	O
related	O
approach	O
iteratively	O
learns	O
the	O
relationship	O
between	O
the	O
two	O
tasks	O
(	O
Haihong	O
et	O
al	O
,	O
2019	O
)	O
.	O
Recently	O
,	O
BERT	B-MethodName
-	O
based	O
approaches	O
(	O
Hardalov	O
et	O
al	O
,	O
2020	O
;	O
have	O
improved	O
results	O
.	O
On	O
the	O
other	O
hand	O
,	O
cross	O
-	O
lingual	O
versions	O
of	O
this	O
joint	O
task	O
include	O
a	O
low	O
-	O
supervision	O
based	O
approach	O
for	O
Hindi	O
and	O
Turkish	O
(	O
Upadhyay	O
et	O
al	O
,	O
2018	O
)	O
,	O
new	O
datasets	O
for	O
Spanish	O
and	O
Thai	O
(	O
Schuster	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
recently	O
creating	O
MultiATIS++	O
,	O
a	O
comprehensive	O
dataset	O
in	O
9	O
languages	O
.	O
The	O
joint	O
task	O
mentioned	O
above	O
in	O
a	O
pure	O
zero	O
-	O
shot	O
setting	O
is	O
one	O
of	O
the	O
motivations	O
for	O
our	O
work	O
.	O
A	O
Zero	O
-	O
shot	O
is	O
the	O
setting	O
where	O
the	O
model	O
sees	O
a	O
new	O
distribution	O
of	O
examples	O
only	O
during	O
test	O
(	O
prediction	O
)	O
time	O
(	O
Xian	O
et	O
al	O
,	O
2017	O
;	O
Srivastava	O
et	O
al	O
,	O
2018	O
;	O
Romera	O
-	O
Paredes	O
and	O
Torr	O
,	O
2015	O
)	O
.	O
Thus	O
,	O
in	O
our	O
setting	O
,	O
we	O
assume	O
that	O
target	O
language	O
is	O
unknown	O
during	O
training	O
,	O
so	O
that	O
our	O
model	O
is	O
generalizable	O
across	O
multiple	O
languages	O
.	O
Code	O
-	O
Switching	O
.	O
Linguistic	O
code	O
-	O
switching	O
is	O
a	O
phenomenon	O
where	O
multilingual	O
speakers	O
alternate	O
between	O
languages	O
.	O
Recently	O
,	O
monolingual	O
models	O
have	O
been	O
adapted	O
to	O
code	O
-	O
switched	O
text	O
in	O
entity	O
recognition	O
(	O
Aguilar	O
and	O
Solorio	O
,	O
2019	O
)	O
,	O
part	O
-	O
ofspeech	O
tagging	O
(	O
Soto	O
and	O
Hirschberg	O
,	O
2018	O
;	O
Ball	O
and	O
Garrette	O
,	O
2018	O
)	O
,	O
sentiment	B-TaskName
analysis	I-TaskName
(	O
Joshi	O
et	O
al	O
,	O
2016	O
)	O
and	O
language	B-TaskName
identification	I-TaskName
(	O
Mave	O
et	O
al	O
,	O
2018	O
;	O
Yirmibeşoglu	O
and	O
Eryigit	O
,	O
2018	O
;	O
Mager	O
et	O
al	O
,	O
2019	O
)	O
.	O
Recently	O
,	O
KhudaBukhsh	O
et	O
al	O
,	O
2020	O
have	O
proposed	O
an	O
approach	O
to	O
sample	O
code	O
-	O
mixed	O
documents	O
using	O
minimal	O
supervision	O
.	O
Qin	O
et	O
al	O
,	O
2020	O
allows	O
randomized	O
code	O
-	O
switching	O
to	O
include	O
the	O
target	O
language	O
,	O
as	O
shown	O
in	O
their	O
Figure	O
3	O
.	O
In	O
our	O
context	O
for	O
example	O
,	O
if	O
the	O
target	O
language	O
is	O
German	O
,	O
we	O
ensure	O
that	O
there	O
is	O
no	O
code	O
-	O
switching	O
to	O
German	O
during	O
training	O
.	O
We	O
consider	O
this	O
distinction	O
essential	O
to	O
evaluate	O
a	O
true	O
zero	B-TaskName
-	I-TaskName
shot	I-TaskName
learning	I-TaskName
scenario	O
and	O
prevent	O
any	O
bias	O
when	O
comparing	O
with	O
translate	O
-	O
and	O
-	O
train	O
.	O
present	O
a	O
non	O
-	O
zero	O
-	O
shot	O
approach	O
that	O
performs	O
code	O
-	O
switching	O
to	O
target	O
languages	O
,	O
and	O
Jiang	O
et	O
al	O
(	O
2020	O
)	O
present	O
a	O
code	O
-	O
switching	O
based	O
method	O
to	O
improve	O
the	O
ability	O
of	O
multilingual	O
language	O
mod	O
-	O
els	O
for	O
factual	O
knowledge	O
retrieval	O
.	O
Contemporary	O
work	O
by	O
Tan	O
and	O
Joty	O
,	O
2021	O
makes	O
use	O
of	O
both	O
word	O
and	O
phrase	O
-	O
level	O
code	O
-	O
mixing	O
to	O
switch	O
to	O
a	O
set	O
of	O
languages	O
to	O
perform	O
adversarial	O
training	O
for	O
XNLI	B-DatasetName
.	O
Code	O
-	O
switching	O
and	O
other	O
data	B-TaskName
augmentation	I-TaskName
techniques	O
have	O
been	O
applied	O
to	O
the	O
pre	O
-	O
training	O
stage	O
in	O
recent	O
works	O
(	O
Chaudhary	O
et	O
al	O
,	O
2020	O
;	O
Kale	O
and	O
Siddhant	O
,	O
2021	O
;	O
Dufter	O
and	O
Schütze	O
,	O
2020	O
)	O
.	O
However	O
,	O
pre	O
-	O
training	O
is	O
outside	O
the	O
scope	O
of	O
this	O
work	O
.	O
In	O
addition	O
to	O
studying	O
cross	O
-	O
lingual	O
slot	B-TaskName
filling	I-TaskName
and	O
language	O
families	O
,	O
another	O
key	O
distinction	O
of	O
our	O
method	O
is	O
that	O
we	O
completely	O
ignore	O
the	O
target	O
language	O
during	O
training	O
to	O
represent	O
a	O
fully	O
zero	O
-	O
shot	O
scenario	O
.	O
The	O
main	O
advantage	O
is	O
that	O
with	O
enhanced	O
cross	O
-	O
lingual	O
generalizability	O
,	O
it	O
can	O
be	O
deployed	O
out	O
-	O
of	O
-	O
the	O
-	O
box	O
,	O
as	O
our	O
training	O
is	O
conducted	O
independently	O
of	O
the	O
target	O
language	O
.	O

Our	O
study	O
shows	O
that	O
augmenting	O
the	O
monolingual	O
input	O
data	O
with	O
multilingual	O
code	O
-	O
switching	O
via	O
random	O
translations	O
at	O
the	O
chunk	O
-	O
level	O
helps	O
a	O
zeroshot	O
model	O
to	O
be	O
language	O
neutral	O
when	O
evaluated	O
on	O
unseen	O
languages	O
.	O
This	O
approach	O
enhanced	O
the	O
generalizability	O
of	O
pre	O
-	O
trained	O
language	O
models	O
such	O
as	O
mBERT	B-MethodName
when	O
fine	O
-	O
tuning	O
for	O
downstream	O
tasks	O
of	O
intent	B-TaskName
detection	I-TaskName
and	O
slot	B-TaskName
filling	I-TaskName
.	O
Additionally	O
,	O
we	O
presented	O
an	O
application	O
of	O
this	O
method	O
using	O
a	O
new	O
annotated	O
dataset	O
of	O
disaster	O
tweets	O
.	O
Further	O
,	O
we	O
studied	O
code	O
-	O
switching	O
with	O
language	O
families	O
and	O
their	O
impact	O
on	O
specific	O
target	O
languages	O
.	O
Addressing	O
code	O
-	O
switching	O
with	O
language	O
families	O
during	O
the	O
pre	O
-	O
training	O
phase	O
and	O
releasing	O
a	O
larger	O
dataset	O
of	O
annotated	O
disaster	O
tweets	O
in	O
more	O
languages	O
are	O
planned	O
for	O
future	O
work	O
.	O

Automatic	O
Error	B-MetricName
Analysis	O
for	O
Document	O
-	O
level	O
Information	O
Extraction	O

Figure	O
1	O
:	O
The	O
document	O
-	O
level	O
extraction	O
task	O
from	O
the	O
ProMED	O
dataset	O
on	O
disease	O
outbreaks	O
(	O
left	O
)	O
and	O
the	O
automatic	O
error	O
analysis	O
process	O
(	O
right	O
)	O
.	O
Our	O
system	O
performs	O
a	O
set	O
of	O
transformations	O
on	O
the	O
predicted	O
templates	O
to	O
convert	O
them	O
into	O
the	O
corresponding	O
gold	O
standard	O
templates	O
.	O
Transformation	O
steps	O
are	O
mapped	O
to	O
corresponding	O
error	O
types	O
to	O
produce	O
informative	O
error	O
statistics	O
.	O
newly	O
developed	O
neural	O
IE	O
methods	O
with	O
that	O
of	O
the	O
largely	O
hand	O
-	O
crafted	O
systems	O
of	O
the	O
1990s	O
.	O
In	O
this	O
work	O
,	O
we	O
first	O
introduce	O
a	O
framework	O
for	O
automating	O
error	O
analysis	O
for	O
document	O
-	O
level	O
event	O
and	O
relation	B-TaskName
extraction	I-TaskName
,	O
casting	O
both	O
as	O
instances	O
of	O
a	O
general	O
role	O
-	O
filling	O
,	O
or	O
template	O
-	O
filling	O
task	O
(	O
Jurafsky	O
and	O
Martin	O
,	O
2021	O
)	O
.	O
Our	O
approach	O
converts	O
predicted	O
system	O
outputs	O
into	O
their	O
gold	O
standard	O
counterparts	O
through	O
a	O
series	O
of	O
template	O
-	O
level	O
transformations	O
(	O
Figure	O
2	O
)	O
and	O
then	O
maps	O
combinations	O
of	O
transformations	O
into	O
a	O
collection	O
of	O
IE	O
-	O
based	O
error	O
types	O
.	O
Examples	O
of	O
errors	O
include	O
duplicates	O
,	O
missing	O
and	O
spurious	O
role	O
fillers	O
,	O
missing	O
and	O
spurious	O
templates	O
,	O
and	O
incorrect	O
role	O
and	O
template	O
assignments	O
for	O
fillers	O
.	O
(	O
See	O
Figure	O
3	O
for	O
the	O
full	O
set	O
)	O
.	O
Next	O
,	O
we	O
employ	O
the	O
error	O
analysis	O
framework	O
in	O
a	O
comparison	O
of	O
two	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
documentlevel	O
neural	O
template	O
-	O
filling	O
approaches	O
,	O
DyGIE++	O
and	O
GTT	O
(	O
Du	O
et	O
al	O
,	O
2021b	O
)	O
,	O
across	O
three	O
template	O
-	O
filling	O
datasets	O
(	O
SciREX	B-DatasetName
,	O
ProMED	O
(	O
Patwardhan	O
and	O
Riloff	O
,	O
2009	O
)	O
3	O
,	O
and	O
MUC	O
-	O
4	O
)	O
.	O
Finally	O
,	O
in	O
an	O
attempt	O
to	O
gauge	O
progress	O
in	O
the	O
information	O
extraction	O
field	O
over	O
the	O
past	O
30	O
years	O
,	O
we	O
employ	O
the	O
framework	O
to	O
compare	O
the	O
performance	O
of	O
four	O
of	O
the	O
original	O
MUC	O
-	O
4	O
systems	O
with	O
the	O
two	O
newer	O
deep	O
-	O
learning	O
approaches	O
to	O
documentlevel	O
IE	O
.	O
4	O
We	O
find	O
that	O
(	O
1	O
)	O
the	O
best	O
of	O
the	O
early	O
IE	O
models	O
-	O
which	O
strikes	O
a	O
better	O
balance	O
between	O
precision	O
and	O
recall	O
-	O
outperforms	O
modern	O
models	O
that	O
exhibit	O
much	O
higher	O
precision	O
and	O
much	O
lower	O
recall	O
;	O
(	O
2	O
)	O
the	O
modern	O
neural	O
models	O
make	O
more	O
mistakes	O
on	O
scientific	O
vs.	O
news	O
-	O
oriented	O
texts	O
,	O
and	O
missing	O
role	O
fillers	O
is	O
universally	O
the	O
largest	O
source	O
of	O
errors	O
;	O
and	O
(	O
3	O
)	O
modern	O
models	O
have	O
clear	O
advantages	O
over	O
the	O
early	O
IE	O
systems	O
in	O
terms	O
of	O
accurate	O
span	O
extraction	O
,	O
while	O
the	O
early	O
systems	O
make	O
fewer	O
mistakes	O
assigning	O
role	O
fillers	O
to	O
their	O
roles	O
.	O

Aside	O
from	O
the	O
original	O
MUC	O
-	O
4	O
evaluation	O
scoring	O
reports	O
(	O
Chinchor	O
,	O
1991	O
)	O
,	O
which	O
included	O
counts	O
of	O
missing	O
and	O
spurious	O
role	O
filler	O
errors	O
,	O
there	O
have	O
been	O
very	O
few	O
attempts	O
at	O
understanding	O
the	O
types	O
of	O
errors	O
made	O
by	O
IE	O
systems	O
and	O
grounding	O
those	O
errors	O
linguistically	O
.	O
Valls	O
-	O
Vargas	O
et	O
al	O
(	O
2017	O
)	O
proposed	O
a	O
framework	O
for	O
studying	O
how	O
different	O
errors	O
propagate	O
through	O
an	O
IE	O
system	O
;	O
however	O
,	O
the	O
framework	O
can	O
only	O
be	O
used	O
for	O
pipelined	O
systems	O
,	O
not	O
end	O
-	O
to	O
-	O
end	O
ones	O
.	O
On	O
the	O
other	O
hand	O
,	O
automated	O
error	O
analysis	O
with	O
linguistically	O
motivated	O
error	O
types	O
has	O
been	O
used	O
in	O
other	O
sub	O
-	O
fields	O
of	O
NLP	O
such	O
as	O
machinetranslation	O
(	O
Vilar	O
et	O
al	O
,	O
2006	O
;	O
Zhou	O
et	O
al	O
,	O
2008	O
;	O
Farrús	O
et	O
al	O
,	O
2010	O
;	O
Kholy	O
and	O
Habash	O
,	O
2011	O
;	O
Zeman	O
et	O
al	O
,	O
2011	O
;	O
Popović	O
and	O
Ney	O
,	O
2011	O
)	O
,	O
coreference	B-TaskName
resolution	I-TaskName
(	O
Uryupina	O
,	O
2008	O
;	O
Kummerfeld	O
and	O
Klein	O
,	O
2013	O
;	O
Martschat	O
and	O
Strube	O
,	O
2014	O
;	O
Martschat	O
et	O
al	O
,	O
2015	O
)	O
and	O
parsing	O
(	O
Kummerfeld	O
et	O
al	O
,	O
2012	O
)	O
.	O
Recently	O
,	O
generalized	O
automated	O
error	O
analysis	O
frameworks	O
involving	O
human	O
-	O
in	O
-	O
the	O
-	O
loop	O
testing	O
like	O
Errudite	O
(	O
Wu	O
et	O
al	O
,	O
2019	O
)	O
,	O
CHECK	O
-	O
LIST	O
(	O
Ribeiro	O
et	O
al	O
,	O
2020	O
)	O
,	O
CrossCheck	O
(	O
Arendt	B-DatasetName
et	O
al	O
,	O
2021	O
)	O
,	O
and	O
AllenNLP	O
Interpret	O
(	O
Wallace	O
et	O
al	O
,	O
2019	O
)	O
have	O
successfully	O
been	O
applied	O
to	O
tasks	O
like	O
machine	O
comprehension	O
and	O
relation	B-TaskName
extraction	I-TaskName
(	O
Alt	O
et	O
al	O
,	O
2020	O
)	O
.	O
Closest	O
to	O
our	O
work	O
are	O
Kummerfeld	O
et	O
al	O
(	O
2012	O
)	O
and	O
Kummerfeld	O
and	O
Klein	O
(	O
2013	O
)	O
,	O
which	O
use	O
model	O
-	O
agnostic	O
transformationbased	O
mapping	O
approaches	O
to	O
automatically	O
obtain	O
error	O
information	O
in	O
the	O
predicted	O
structured	O
output	O
.	O

Our	O
experiments	O
employ	O
three	O
document	O
-	O
level	O
information	O
extraction	O
datasets	O
.	O
We	O
briefly	O
describe	O
each	O
below	O
.	O
Dataset	O
statistics	O
are	O
summarized	O
in	O
Table	O
1	O
.	O
MUC	O
-	O
4	O
(	O
MUC	O
-	O
4	O
,	O
1992	O
)	O
consists	O
of	O
newswire	O
describing	O
terrorist	O
incidents	O
in	O
Latin	O
America	O
provided	O
by	O
the	O
FBIS	O
(	O
Federal	O
Broadcast	O
Information	O
Services	O
)	O
.	O
We	O
converted	O
the	O
optional	O
templates	O
to	O
required	O
templates	O
and	O
removed	O
the	O
subtypes	O
of	O
the	O
incidents	O
as	O
done	O
in	O
previous	O
work	O
(	O
Chambers	O
,	O
2013	O
;	O
Du	O
et	O
al	O
,	O
2021b	O
)	O
so	O
that	O
the	O
dataset	O
is	O
transformed	O
into	O
standardized	O
templates	O
.	O
The	O
roles	O
chosen	O
from	O
the	O
MUC	O
-	O
4	O
dataset	O
are	O
PERPIND	O
(	O
individual	O
perpetrator	O
)	O
,	O
PERPORG	O
(	O
organization	O
perpetrator	O
)	O
,	O
TARGET	O
(	O
physical	O
target	O
)	O
,	O
VICTIM	O
(	O
human	O
target	O
)	O
,	O
and	O
WEAPON	O
which	O
are	O
all	O
string	O
-	O
fill	O
roles	O
,	O
as	O
well	O
as	O
INCIDENT	O
We	O
focus	O
specifically	O
on	O
its	O
4	B-TaskName
-	I-TaskName
ary	I-TaskName
relation	I-TaskName
extraction	I-TaskName
subtask	O
.	O
The	O
roles	O
present	O
in	O
each	O
relation	O
are	O
MATERIAL	O
(	O
DATASET	O
)	O
,	O
METRIC	O
,	O
TASK	O
,	O
and	O
METHOD	O
which	O
are	O
all	O
string	O
-	O
fills	O
.	O
We	O
convert	O
the	O
dataset	O
from	O
its	O
original	O
format	O
to	O
templates	O
for	O
our	O
models	O
,	O
and	O
remove	O
individual	O
role	O
fillers	O
(	O
entities	O
)	O
that	O
have	O
no	O
mentions	O
in	O
the	O
text	O
.	O
11	O
We	O
also	O
remove	O
any	O
duplicate	O
templates	O
.	O
12	O
During	O
preprocessing	O
,	O
we	O
remove	O
malformed	O
words	O
longer	O
than	O
25	O
characters	O
,	O
as	O
the	O
majority	O
of	O
these	O
consist	O
of	O
concatenated	O
words	O
that	O
are	O
not	O
present	O
in	O
the	O
corresponding	O
text	O
.	O

In	O
our	O
experiments	O
,	O
we	O
train	O
and	O
test	O
two	O
neuralbased	O
IE	O
models	O
,	O
described	O
briefly	O
below	O
,	O
on	O
the	O
MUC	O
-	O
4	O
,	O
ProMED	O
,	O
and	O
SciREX	B-DatasetName
datasets	O
.	O
Note	O
that	O
(	O
2019	O
)	O
for	O
the	O
SciREX	B-DatasetName
dataset	O
and	O
11	O
tokens	O
for	O
the	O
ProMED	O
dataset	O
.	O
We	O
use	O
bert	O
-	O
base	O
-	O
cased	O
and	O
allenai	O
/	O
scibert_scivocab_uncased	O
for	O
the	O
base	O
BERT	B-MethodName
and	O
SciBERT	O
models	O
respectively	O
,	O
which	O
both	O
have	O
a	O
maximum	O
input	O
sequence	O
length	O
of	O
512	O
tokens	O
.	O
To	O
aggregate	O
entities	O
detected	O
by	O
DyGIE++	O
into	O
templates	O
,	O
we	O
use	O
a	O
clustering	O
algorithm	O
.	O
For	O
the	O
SciREX	B-DatasetName
dataset	O
,	O
we	O
adopt	O
a	O
heuristic	O
approach	O
that	O
assumes	O
there	O
is	O
only	O
one	O
template	O
per	O
document	O
,	O
and	O
in	O
that	O
template	O
,	O
we	O
assign	O
the	O
named	O
entities	O
predicted	O
by	O
DyGIE++	O
for	O
a	O
document	O
to	O
the	O
predicted	O
role	O
types	O
.	O
For	O
the	O
ProMED	O
dataset	O
,	O
we	O
use	O
a	O
different	O
clustering	O
heuristic	O
that	O
ensures	O
that	O
each	O
template	O
has	O
exactly	O
one	O
role	O
filler	O
for	O
the	O
COUNTRY	O
and	O
DISEASE	O
roles	O
,	O
as	O
detailed	O
in	O
the	O
dataset	O
annotation	O
guidelines	O
.	O
Also	O
,	O
since	O
STATUS	O
has	O
the	O
value	O
confirmed	O
in	O
the	O
majority	O
of	O
the	O
templates	O
,	O
every	O
template	O
predicted	O
has	O
its	O
STATUS	O
assigned	O
as	O
confirmed	O
.	O
GTT	O
is	O
an	O
end	O
-	O
to	O
-	O
end	O
document	O
-	O
level	O
templategenerating	O
model	O
.	O
For	O
the	O
MUC	O
-	O
4	O
and	O
SciREX	B-DatasetName
datasets	O
,	O
GTT	O
is	O
run	O
for	O
20	O
epochs	O
,	O
while	O
for	O
ProMED	O
it	O
is	O
run	O
for	O
36	O
epochs	O
,	O
to	O
adjust	O
for	O
the	O
smaller	O
size	O
of	O
the	O
dataset	O
.	O
All	O
other	O
hyperparameters	O
are	O
set	O
as	O
in	O
Du	O
et	O
al	O
(	O
2021b	O
)	O
.	O
We	O
use	O
the	O
same	O
BERT	B-MethodName
and	O
SciBERT	O
base	O
models	O
as	O
described	O
in	O
the	O
DyGIE++	O
architecture	O
above	O
,	O
both	O
with	O
a	O
maximum	O
input	O
sequence	O
length	O
of	O
512	O
tokens	O
.	O
The	O
computational	O
budget	O
and	O
optimal	O
hyperparameters	O
for	O
these	O
models	O
can	O
be	O
found	O
in	O
Ap	O
-	O
pendix	O
sections	O
D	O
and	O
E	O
,	O
respectively	O
.	O

We	O
first	O
discuss	O
the	O
results	O
of	O
DyGIE++	O
and	O
GTT	O
on	O
SciREX	B-DatasetName
,	O
ProMED	O
,	O
and	O
MUC	O
-	O
4	O
;	O
and	O
then	O
examine	O
the	O
performance	O
of	O
these	O
newer	O
neural	O
models	O
on	O
the	O
1992	O
MUC	O
-	O
4	O
dataset	O
vs.	O
a	O
few	O
of	O
the	O
bestperforming	O
IE	O
systems	O
at	O
the	O
time	O
.	O

The	O
GTT	O
(	O
BERT	B-MethodName
)	O
model	O
on	O
the	O
MUC	O
-	O
4	O
dataset	O
took	O
1	O
hour	O
and	O
21	O
minutes	O
to	O
train	O
and	O
around	O
11	O
minutes	O
to	O
test	O
on	O
Google	B-DatasetName
Colab	O
(	O
GPU	O
)	O
.	O
The	O
GTT	O
(	O
BERT	B-MethodName
)	O
model	O
on	O
the	O
ProMED	O
dataset	O
took	O
around	O
24	O
minutes	O
to	O
train	O
and	O
4	O
minutes	O
to	O
test	O
,	O
while	O
the	O
GTT	O
(	O
SciBERT	O
)	O
model	O
on	O
the	O
ProMED	O
dataset	O
took	O
around	O
13	O
minutes	O
to	O
train	O
and	O
4	O
minutes	O
to	O
test	O
,	O
both	O
on	O
Google	B-DatasetName
Colab	O
(	O
GPU	O
)	O
.	O
The	O
DyGIE++	O
(	O
BERT	B-MethodName
)	O
model	O
on	O
the	O
ProMED	O
dataset	O
took	O
around	O
50	O
minutes	O
to	O
train	O
,	O
while	O
the	O
DyGIE++	O
(	O
SciBERT	O
)	O
model	O
on	O
the	O
ProMED	O
dataset	O
took	O
around	O
1	O
hour	O
and	O
30	O
minutes	O
to	O
train	O
,	O
both	O
on	O
a	O
NVIDIA	O
V100	O
GPU	O
.	O
For	O
the	O
SciREX	B-DatasetName
dataset	O
,	O
it	O
took	O
around	O
10	O
-	O
20	O
minutes	O
to	O
run	O
the	O
GTT	O
(	O
BERT	B-MethodName
)	O
and	O
GTT	O
(	O
SciBERT	O
)	O
models	O
on	O
a	O
NVIDIA	O
V100	O
GPU	O
.	O
It	O
is	O
worth	O
noting	O
that	O
since	O
the	O
GTT	O
model	O
embeds	O
all	O
inputs	O
before	O
training	O
and	O
SciREX	B-DatasetName
documents	O
are	O
extremely	O
long	O
,	O
more	O
than	O
25	O
GB	O
of	O
memory	O
needs	O
to	O
be	O
allocated	O
at	O
the	O
embedding	O
phrase	O
.	O
The	O
training	O
process	O
has	O
normal	O
memory	O
usage	O
.	O
The	O
DyGIE++	O
(	O
BERT	B-MethodName
)	O
model	O
took	O
around	O
2	O
hours	O
to	O
train	O
,	O
while	O
the	O
DyGIE++	O
(	O
SciBERT	O
)	O
model	O
took	O
around	O
4	O
hours	O
to	O
train	O
,	O
both	O
on	O
a	O
NVIDIA	O
V100	O
GPU	O
.	O
Our	O
error	O
analysis	O
tool	O
can	O
be	O
run	O
completely	O
on	O
a	O
CPU	O
and	O
takes	O
a	O
couple	O
of	O
minutes	O
to	O
run	O
,	O
depending	O
on	O
the	O
size	O
of	O
the	O
dataset	O
and	O
the	O
predicted	O
outputs	O
.	O

We	O
thank	O
the	O
anonymous	O
reviewers	O
and	O
Ellen	O
Riloff	O
for	O
their	O
helpful	O
comments	O
(	O
!	O
)	O
and	O
Sienna	O
Hu	O
for	O
converting	O
the	O
1992	O
model	O
outputs	O
to	O
a	O
format	O
compatible	O
with	O
our	O
error	O
analysis	O
tool	O
.	O
Our	O
research	O
was	O
supported	O
,	O
in	O
part	O
,	O
by	O
NSF	O
CISE	O
Grant	O
1815455	O
and	O
the	O
Cornell	B-DatasetName
CS	B-DatasetName
Department	O
CSURP	O
grants	O
for	O
undergraduate	O
research	O
.	O

Intrinsic	O
Evaluation	O
of	O
Summarization	B-TaskName
Datasets	O

High	O
quality	O
data	O
forms	O
the	O
bedrock	O
for	O
building	O
meaningful	O
statistical	O
models	O
in	O
NLP	O
.	O
Consequently	O
,	O
data	O
quality	O
must	O
be	O
evaluated	O
either	O
during	O
dataset	O
construction	O
or	O
post	O
hoc	O
.	O
Almost	O
all	O
popular	O
summarization	B-TaskName
datasets	O
are	O
drawn	O
from	O
natural	O
sources	O
and	O
do	O
not	O
come	O
with	O
inherent	O
quality	O
assurance	O
guarantees	O
.	O
In	O
spite	O
of	O
this	O
,	O
data	O
quality	O
has	O
gone	O
largely	O
unquestioned	O
for	O
many	O
recent	O
summarization	B-TaskName
datasets	O
.	O
We	O
perform	O
the	O
first	O
large	O
-	O
scale	O
evaluation	O
of	O
summarization	B-TaskName
datasets	O
by	O
introducing	O
5	O
intrinsic	O
metrics	O
and	O
applying	O
them	O
to	O
10	O
popular	O
datasets	O
.	O
We	O
find	O
that	O
data	O
usage	O
in	O
recent	O
summarization	B-TaskName
research	O
is	O
sometimes	O
inconsistent	O
with	O
the	O
underlying	O
properties	O
of	O
the	O
datasets	O
employed	O
.	O
Further	O
,	O
we	O
discover	O
that	O
our	O
metrics	O
can	O
serve	O
the	O
additional	O
purpose	O
of	O
being	O
inexpensive	O
heuristics	O
for	O
detecting	O
generically	O
low	O
quality	O
examples	O
.	O

Data	O
understanding	O
is	O
fundamentally	O
important	O
in	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
;	O
for	O
data	O
-	O
driven	O
learning	O
-	O
based	O
methods	O
(	O
e.g.	O
neural	O
networks	O
)	O
,	O
the	O
quality	O
of	O
the	O
training	O
data	O
bounds	O
the	O
quality	O
of	O
models	O
learned	O
using	O
it	O
.	O
Therefore	O
,	O
understanding	O
this	O
data	O
is	O
necessary	O
in	O
order	O
to	O
ensure	O
that	O
models	O
learn	O
to	O
perform	O
a	O
given	O
task	O
correctly	O
.	O
Understanding	O
data	O
is	O
a	O
multidimensional	O
problem	O
.	O
One	O
line	O
of	O
inquiry	O
has	O
demonstrated	O
why	O
prominent	O
datasets	O
are	O
insufficiently	O
challenging	O
:	O
many	O
data	O
examples	O
can	O
be	O
solved	O
by	O
alternative	O
heuristics	O
that	O
do	O
not	O
encode	O
an	O
approach	O
that	O
is	O
faithful	O
to	O
the	O
task	O
(	O
McCoy	O
et	O
al	O
,	O
2019	O
)	O
.	O
From	O
the	O
perspective	O
of	O
datasets	O
,	O
several	O
works	O
have	O
shown	O
that	O
standard	O
datasets	O
in	O
areas	O
such	O
as	O
visual	B-DatasetName
question	I-DatasetName
answering	I-DatasetName
Kafle	O
and	O
Kanan	O
,	O
2017	O
)	O
,	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
(	O
Gururangan	O
et	O
al	O
,	O
2018	O
;	O
Poliak	O
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
reading	B-TaskName
comprehension	I-TaskName
(	O
Kaushik	O
and	O
Lipton	O
,	O
2018	O
)	O
contain	O
annotation	O
artifacts	O
that	O
often	O
give	O
rise	O
to	O
these	O
spurious	O
correlations	O
or	O
reasoning	O
shortcuts	O
.	O
Data	O
understanding	O
can	O
also	O
inform	O
scientific	O
and	O
ethical	O
decision	O
-	O
making	O
(	O
Bender	O
and	O
Friedman	O
,	O
2018	O
;	O
Gebru	O
et	O
al	O
,	O
2018	O
;	O
Mitchell	O
et	O
al	O
,	O
2019	O
)	O
with	O
recent	O
work	O
studying	O
how	O
social	O
biases	O
encoded	O
in	O
training	O
data	O
propagate	O
to	O
learned	O
models	O
(	O
Zhao	O
et	O
al	O
,	O
2019	O
;	O
Tan	O
and	O
Celis	O
,	O
2019	O
)	O
.	O
In	O
this	O
work	O
,	O
we	O
extend	O
these	O
efforts	O
towards	O
the	O
setting	O
of	O
summarization	B-TaskName
.	O
We	O
find	O
this	O
to	O
be	O
particularly	O
timely	O
since	O
several	O
summarization	B-TaskName
datasets	O
have	O
been	O
released	O
in	O
recent	O
years	O
with	O
little	O
discussion	O
of	O
data	O
quality	O
.	O
While	O
prior	O
work	O
on	O
evaluating	O
NLP	O
datasets	O
has	O
focused	O
on	O
their	O
difficulty	O
,	O
transparency	O
,	O
or	O
bias	O
,	O
we	O
consider	O
broadly	O
the	O
overall	O
quality	O
of	O
the	O
dataset	O
-	O
in	O
our	O
case	O
,	O
for	O
the	O
task	O
of	O
summarization	B-TaskName
.	O
1	O
Our	O
central	O
insight	O
is	O
that	O
desirable	O
properties	O
of	O
a	O
summary	O
can	O
be	O
readily	O
estimated	O
by	O
adapting	O
and	O
applying	O
existing	O
NLP	O
methods	O
.	O
With	O
this	O
in	O
mind	O
,	O
we	O
present	O
a	O
multiaspect	O
large	O
-	O
scale	O
study	O
of	O
summarization	B-TaskName
datasets	O
that	O
dissects	O
summarization	B-TaskName
into	O
5	O
properties	O
that	O
are	O
evaluated	O
across	O
10	O
datasets	O
spanning	O
multiple	O
summarization	B-TaskName
domains	O
.	O
Our	O
analysis	O
reveals	O
that	O
our	O
metrics	O
can	O
serve	O
as	O
lightweight	O
detectors	O
of	O
generically	O
low	O
quality	O
examples	O
.	O
Most	O
strikingly	O
,	O
we	O
show	O
that	O
quantifiable	O
aspects	O
of	O
summarization	B-TaskName
datasets	O
are	O
inconsistent	O
with	O
their	O
use	O
by	O
the	O
NLP	O
community	O
in	O
several	O
instances	O
.	O

Compression	O
scores	O
quantitatively	O
disambiguate	O
summarization	B-TaskName
tasks	O
.	O
Concretely	O
,	O
we	O
observe	O
GW	O
has	O
the	O
lowest	O
compression	O
scores	O
and	O
while	O
GW	O
is	O
sometimes	O
described	O
as	O
a	O
summarization	B-TaskName
dataset	O
(	O
Rush	O
et	O
al	O
,	O
2015	O
;	O
Chopra	O
et	O
al	O
,	O
2016	O
)	O
,	O
it	O
is	O
better	O
seen	O
as	O
a	O
headline	O
generation	O
dataset	O
that	O
is	O
more	O
in	O
the	O
style	O
of	O
sentence	B-DatasetName
compression	I-DatasetName
(	O
as	O
is	O
suggested	O
by	O
S	O
i	O
=	O
D	O
i	O
=	O
1	O
)	O
.	O
Conversely	O
,	O
AMI	O
and	O
Movi	B-DatasetName
-	O
eScript	O
achieve	O
the	O
highest	O
scores	O
by	O
a	O
substantial	O
margin	O
and	O
are	O
long	O
-	O
document	B-TaskName
summarization	I-TaskName
datasets	O
.	O
Classifying	O
new	O
summarization	B-TaskName
datasets	O
accurately	O
may	O
prove	O
useful	O
given	O
that	O
successful	O
methods	O
from	O
one	O
domain	O
often	O
do	O
not	O
extend	O
to	O
another	O
and	O
this	O
shortcoming	O
in	O
generalization	O
can	O
be	O
attributed	O
to	O
the	O
differences	O
in	O
compression	O
requirements	O
(	O
Cohan	O
et	O
al	O
,	O
2018	O
)	O
.	O
Given	O
the	O
goals	O
stated	O
in	O
the	O
XSum	B-DatasetName
dataset	O
paper	O
,	O
TL	O
;	O
DR	O
may	O
be	O
a	O
better	O
choice	O
than	O
XSum	B-DatasetName
.	O
In	O
particular	O
,	O
Narayan	O
et	O
al	O
(	O
2018	O
)	O
introduce	O
XSum	B-DatasetName
as	O
a	O
large	O
dataset	O
that	O
legitimately	O
requires	O
abstraction	O
.	O
While	O
XSum	B-DatasetName
is	O
more	O
abstractive	O
than	O
other	O
News	O
datasets	O
(	O
barring	O
GW	O
)	O
and	O
is	O
relatively	O
large	O
,	O
TL	O
;	O
DR	O
displays	O
greater	O
abstractivity	O
,	O
similar	O
length	O
summaries	O
,	O
and	O
is	O
15	O
times	O
larger	O
.	O
That	O
said	O
,	O
Narayan	O
et	O
al	O
(	O
2018	O
)	O
explore	O
topic	O
-	O
oriented	O
strategies	O
in	O
their	O
work	O
and	O
such	O
methods	O
may	O
be	O
better	O
suited	O
to	O
XSum	B-DatasetName
given	O
the	O
TS	B-MethodName
scores	O
.	O
CNN	B-DatasetName
-	I-DatasetName
DM	I-DatasetName
and	O
NYT	O
are	O
suboptimal	O
for	O
studying	O
abstractive	O
/	O
extractive	O
systems	O
respectively	O
.	O
Several	O
recent	O
works	O
(	O
See	O
et	O
al	O
,	O
2017	O
;	O
Paulus	O
et	O
al	O
,	O
2018	O
;	O
Li	O
et	O
al	O
,	O
2018	O
)	O
have	O
used	O
CNN	B-DatasetName
-	I-DatasetName
DM	I-DatasetName
to	O
build	O
and	O
evaluate	O
abstractive	O
systems	O
.	O
Conversely	O
,	O
NYT	O
has	O
been	O
used	O
to	O
build	O
extractive	O
systems	O
(	O
Hong	O
and	O
Nenkova	O
,	O
2014	O
;	O
Li	O
et	O
al	O
,	O
2016	O
)	O
.	O
Given	O
our	O
findings	O
,	O
we	O
find	O
both	O
of	O
these	O
trends	O
to	O
be	O
inconsistent	O
with	O
dataset	O
properties	O
and	O
suboptimal	O
given	O
other	O
preferable	O
datasets	O
for	O
these	O
purposes	O
:	O
CNN	B-DatasetName
-	I-DatasetName
DM	I-DatasetName
is	O
one	O
of	O
the	O
least	O
abstractive	O
datasets	O
and	O
there	O
are	O
larger	O
and	O
more	O
extractive	O
alternatives	O
to	O
NYT	O
such	O
as	O
NWS	O
.	O
Especially	O
in	O
the	O
case	O
of	O
CNN	B-DatasetName
-	I-DatasetName
DM	I-DatasetName
,	O
we	O
note	O
that	O
training	O
learning	O
-	O
based	O
systems	O
(	O
e.g.	O
neural	O
methods	O
)	O
using	O
data	O
with	O
limited	O
abstractivity	O
implies	O
the	O
resulting	O
summarizers	O
will	O
be	O
limited	O
in	O
their	O
ability	O
to	O
generate	O
genuinely	O
abstractive	O
text	O
.	O
This	O
is	O
validated	O
by	O
empirical	O
findings	O
as	O
both	O
See	O
et	O
al	O
(	O
2017	O
)	O
and	O
Zhang	O
et	O
al	O
(	O
2018	O
)	O
observe	O
limited	O
abstractivity	O
in	O
abstractive	O
systems	O
trained	O
on	O
CNN	B-DatasetName
-	I-DatasetName
DM	I-DatasetName
.	O
In	O
light	O
of	O
this	O
,	O
we	O
argue	O
systems	O
should	O
be	O
characterized	O
as	O
abstractive	O
or	O
not	O
based	O
on	O
their	O
empirical	O
behavior	O
rather	O
than	O
their	O
theoretical	O
capability	O
.	O
9	O
CNN	B-DatasetName
-	I-DatasetName
DM	I-DatasetName
is	O
not	O
a	O
representative	O
benchmark	O
for	O
summarization	B-TaskName
as	O
a	O
whole	O
.	O
Recent	O
work	O
(	O
Kryscinski	O
et	O
al	O
,	O
2019	O
;	O
Raffel	O
et	O
al	O
,	O
2019	O
)	O
has	O
explicitly	O
portrayed	O
CNN	B-DatasetName
-	I-DatasetName
DM	I-DatasetName
as	O
the	B-DatasetName
benchmark	I-DatasetName
dataset	O
for	O
summarization	B-TaskName
;	O
the	O
field	O
has	O
implicitly	O
done	O
this	O
for	O
several	O
years	O
(	O
Kryscinski	O
et	O
al	O
,	O
2019	O
)	O
.	O
While	O
there	O
is	O
clear	O
value	O
in	O
evaluating	O
pretrained	O
representations	O
on	O
summarization	B-TaskName
datasets	O
,	O
we	O
caution	O
against	O
using	O
CNN	B-DatasetName
-	I-DatasetName
DM	I-DatasetName
as	O
a	O
stand	O
-	O
in	O
for	O
the	O
entire	O
summarization	B-TaskName
subfield	O
.	O
Instead	O
,	O
we	O
suggest	O
using	O
a	O
diverse	O
group	O
of	O
datasets	O
and	O
not	O
reducing	O
a	O
highly	O
heterogeneous	O
subfield	O
to	O
a	O
single	O
dataset	O
.	O
While	O
this	O
adds	O
additional	O
overhead	O
,	O
this	O
cost	O
is	O
necessary	O
to	O
draw	O
meaningful	O
conclusions	O
about	O
the	O
impact	O
of	O
advances	O
on	O
summarization	B-TaskName
broadly	O
given	O
the	O
pronounced	O
diversity	O
in	O
summarization	B-TaskName
datasets	O
(	O
Table	O
1	O
)	O
.	O
Post	O
-	O
processing	O
methods	O
for	O
mitigating	O
redundancy	O
may	O
be	O
needed	O
for	O
practical	O
systems	O
.	O
While	O
evaluation	O
on	O
standard	O
datasets	O
using	O
ROUGE	O
may	O
not	O
penalize	O
for	O
this	O
,	O
redundancy	O
is	O
clearly	O
undesirable	O
(	O
Carbonell	O
and	O
Goldstein	O
,	O
1998	O
;	O
Peyrard	O
,	O
2019a	O
)	O
and	O
existing	O
datasets	O
(	O
and	O
thereby	O
systems	O
learned	O
using	O
that	O
data	O
)	O
display	O
significant	O
amounts	O
of	O
redundancy	O
in	O
their	O
gold	O
-	O
standard	O
summaries	O
(	O
exceptions	O
are	O
datasets	O
with	O
short	O
summaries	O
where	O
cross	O
-	O
sentence	O
redundancy	O
is	O
constrained	O
to	O
be	O
low	O
)	O
.	O
Specifically	O
,	O
Nenkova	O
(	O
2006	O
)	O
argues	O
that	O
redundancy	O
is	O
a	O
clear	O
inhibitor	O
for	O
practical	O
application	O
of	O
summarization	B-TaskName
systems	O
.	O
Consequently	O
,	O
post	O
hoc	O
methods	O
that	O
reduce	O
redundancy	O
after	O
initial	O
evaluation	O
may	O
be	O
useful	O
in	O
generating	O
summaries	O
that	O
are	O
suitable	O
for	O
human	O
users	O
.	O
Semantic	O
coherence	O
captures	O
observable	O
variation	O
in	O
summary	O
coherence	O
.	O
We	O
observe	O
that	O
the	O
Scientific	O
summaries	O
(	O
which	O
are	O
abstracts	O
of	O
published	O
papers	O
)	O
are	O
clearly	O
more	O
coherent	O
than	O
the	O
author	O
-	O
generated	O
summaries	O
in	O
TL	O
;	O
DR	O
,	O
the	O
fragmented	O
summaries	O
in	O
AMI	O
,	O
and	O
the	O
concatenated	O
bullet	O
-	O
point	O
summaries	O
in	O
CNN	B-DatasetName
-	I-DatasetName
DM	I-DatasetName
.	O
We	O
find	O
that	O
this	O
distinction	O
is	O
captured	O
by	O
the	O
SC	O
measure	O
using	O
BERT	B-MethodName
.	O
Quantifying	O
semantic	O
coherence	O
is	O
especially	O
important	O
given	O
that	O
the	O
coherence	O
of	O
reference	O
summaries	O
will	O
inform	O
the	O
coherence	O
of	O
system	O
summaries	O
,	O
especially	O
for	O
learning	O
-	O
based	O
approaches	O
.	O
Akin	O
to	O
what	O
we	O
discuss	O
for	O
abstractivity	O
,	O
See	O
et	O
al	O
(	O
2017	O
)	O
and	O
Paulus	O
et	O
al	O
(	O
2018	O
)	O
both	O
demonstrate	O
that	O
neural	O
summarizers	O
generate	O
incoherent	O
summaries	O
despite	O
achieving	O
high	O
ROUGE	O
scores	O
.	O

While	O
the	O
properties	O
we	O
evaluate	O
for	O
do	O
not	O
exhaust	O
all	O
aspects	O
of	O
summarization	B-TaskName
that	O
may	O
be	O
of	O
interest	O
,	O
it	O
is	O
unclear	O
to	O
what	O
extent	O
different	O
measures	O
overlap	O
in	O
judgments	O
.	O
To	O
quantify	O
this	O
,	O
in	O
we	O
report	O
pairwise	O
correlations	O
for	O
every	O
pair	O
of	O
metrics	O
.	O
In	O
each	O
case	O
,	O
the	O
value	O
reported	O
is	O
the	O
Spearman	O
rank	O
correlation	O
coefficient	O
ρ	O
computed	O
between	O
the	O
length	O
10	O
vectors	O
containing	O
the	O
scores	O
for	O
each	O
dataset	O
.	O
10	O
ρ	O
=	O
1	O
indicates	O
perfect	O
positive	O
correlation	O
(	O
which	O
is	O
why	O
we	O
see	O
this	O
for	O
all	O
diagonal	O
entries	O
)	O
and	O
ρ	O
<	O
0	B-DatasetName
indicates	O
the	O
metrics	O
are	O
anti	O
-	O
correlated	O
.	O
Unsurprisingly	O
,	O
the	O
compression	O
metrics	O
are	O
strongly	O
correlated	O
with	O
each	O
other	O
.	O
We	O
further	O
observe	O
that	O
redundancy	O
and	O
topic	O
similarity	O
are	O
correlated	O
whereas	O
abstractivity	O
is	O
anti	O
-	O
correlated	O
with	O
both	O
.	O
In	O
particular	O
,	O
when	O
summaries	O
are	O
considerably	O
redundant	O
,	O
we	O
qualitatively	O
observe	O
that	O
the	O
repeated	O
content	O
in	O
the	O
summary	O
was	O
both	O
important	O
and	O
repeated	O
in	O
the	O
context	O
of	O
the	O
reference	O
document	O
.	O
As	O
a	O
result	O
,	O
this	O
may	O
explain	O
why	O
redundancy	O
and	O
abstractivity	O
are	O
anti	O
-	O
correlated	O
as	O
this	O
would	O
suggest	O
that	O
highly	O
redundant	O
summaries	O
are	O
highly	O
extractive	O
.	O
Additionally	O
,	O
since	O
we	O
measure	O
topic	O
similarity	O
using	O
LDA	B-MethodName
and	O
unigram	O
count	O
statistics	O
,	O
it	O
is	O
not	O
surprising	O
that	O
extractions	O
may	O
correlate	O
with	O
high	O
topic	O
similarity	O
.	O
In	O
part	O
,	O
this	O
may	O
suggest	O
a	O
deficiency	O
of	O
our	O
measure	O
of	O
topic	O
similarity	O
to	O
accurately	O
consider	O
references	O
to	O
the	O
same	O
topic	O
using	O
substantially	O
different	O
words	O
.	O
We	O
also	O
observe	O
that	O
semantic	O
coherence	O
patterns	O
similarly	O
to	O
redundancy	O
.	O
In	O
particular	O
,	O
while	O
we	O
find	O
the	O
semantic	O
coherence	O
scores	O
are	O
appropriate	O
for	O
most	O
examples	O
we	O
manually	O
inspected	O
,	O
this	O
suggests	O
that	O
BERT	B-MethodName
relies	O
upon	O
word	O
-	O
level	O
overlaps	O
in	O
making	O
next	O
-	O
sentence	O
judgments	O
(	O
similar	O
to	O
behaviors	O
seen	O
in	O
other	O
sentence	O
-	O
pair	O
tasks	O
such	O
as	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
,	O
c.f	O
Gururangan	O
et	O
al	O
,	O
2018	O
)	O

To	O
complement	O
our	O
quantitative	O
dataset	O
-	O
level	O
analysis	O
,	O
we	O
conduct	O
a	O
qualitative	O
study	O
of	O
individual	O
examples	O
by	O
examining	O
outliers	O
.	O
For	O
each	O
(	O
dataset	O
,	O
metric	O
)	O
pair	O
,	O
we	O
sample	O
10	O
examples	O
from	O
both	O
the	O
top	O
and	O
bottom	O
10	O
%	O
of	O
examples	O
for	O
that	O
metric	O
and	O
in	O
that	O
dataset	O
.	O
Since	O
manually	O
considering	O
all	O
of	O
the	O
1080	O
examples	O
was	O
not	O
feasible	O
,	O
we	O
began	O
by	O
examining	O
the	O
sampled	O
examples	O
for	O
topic	O
similarity	O
,	O
redundancy	O
,	O
and	O
semantic	O
coherence	O
.	O
Our	O
hypothesis	O
was	O
that	O
example	O
quality	O
would	O
positively	O
correlate	O
with	O
coherence	O
and	O
topic	O
similarity	O
and	O
negatively	O
correlate	O
with	O
redundancy	O
.	O
We	O
found	O
this	O
hypothesis	O
to	O
be	O
validated	O
by	O
our	O
observations	O
as	O
we	O
found	O
that	O
examples	O
with	O
low	O
coherence	O
,	O
low	O
topic	O
similarity	O
,	O
or	O
high	O
redundancy	O
scores	O
were	O
generally	O
low	O
quality	O
examples	O
.	O
Every	O
example	O
which	O
we	O
judged	O
to	O
be	O
low	O
quality	O
demonstrated	O
at	O
least	O
one	O
of	O
the	O
following	O
defects	O
:	O
The	O
summary	O
contains	O
critical	O
disfluencies	O
that	O
severely	O
hinder	O
accurate	O
processing	O
.	O
11	O
The	O
summary	O
excludes	O
unambiguously	O
critical	O
information	O
from	O
the	O
reference	O
document	O
.	O
Crucial	O
information	O
in	O
the	O
summary	O
does	O
not	O
appear	O
in	O
the	O
reference	O
document	O
and	O
is	O
not	O
general	B-TaskName
knowledge	I-TaskName
.	O
Substantial	O
fractions	O
of	O
the	O
summary	O
involve	O
entities	O
,	O
relations	O
,	O
or	O
events	O
that	O
are	O
ambiguous	O
and	O
that	O
we	O
could	O
not	O
resolve	O
from	O
the	O
11	O
We	O
invoked	O
this	O
condition	O
fairly	O
judiciously	O
as	O
we	O
observed	O
that	O
the	O
domain	O
of	O
summaries	O
also	O
could	O
influence	O
the	O
fluency	O
of	O
summaries	O
in	O
terms	O
of	O
grammaticality	O
.	O
In	O
particular	O
,	O
we	O
unsurprisingly	O
found	O
that	O
academic	O
papers	O
in	O
the	O
Science	O
domain	O
generally	O
have	O
highly	O
grammatical	O
summaries	O
whereas	O
the	O
bullet	O
-	O
point	O
summaries	O
in	O
CNN	B-DatasetName
-	I-DatasetName
DM	I-DatasetName
and	O
the	O
author	O
-	O
written	O
summaries	O
in	O
TL	O
;	O
DR	O
often	O
were	O
ungrammatical	O
but	O
still	O
sufficiently	O
clear	O
to	O
be	O
interpreted	O
correctly	O
.	O
summary	O
alone	O
.	O
In	O
particular	O
,	O
accurate	O
interpretation	O
of	O
the	O
summary	O
would	O
require	O
also	O
reading	O
the	O
reference	O
document	O
to	O
resolve	O
various	O
coreferring	O
expressions	O
;	O
the	O
summary	O
is	O
not	O
self	O
-	O
contained	O
.	O
12	O
The	O
summary	O
is	O
entirely	O
inappropriate	O
as	O
a	O
summary	O
of	O
the	O
reference	O
document	O
.	O
For	O
example	O
,	O
the	O
summary	O
only	O
discusses	O
an	O
event	O
with	O
no	O
obvious	O
relationship	O
to	O
the	O
contents	O
of	O
the	O
reference	O
document	O
.	O
The	O
summary	O
includes	O
an	O
entire	O
sentence	O
or	O
long	O
phrase	O
describing	O
something	O
that	O
appears	O
in	O
the	O
main	O
document	O
but	O
that	O
is	O
clearly	O
an	O
auxiliary	O
detail	O
.	O
We	O
flagged	O
examples	O
as	O
low	O
quality	O
due	O
to	O
this	O
condition	O
quite	O
conservatively	O
,	O
only	O
using	O
it	O
when	O
we	O
could	O
come	O
to	O
no	O
basis	O
for	O
why	O
the	O
sentence	O
/	O
phrase	O
should	O
appear	O
in	O
the	O
summary	O
.	O
On	O
the	O
other	O
hand	O
,	O
we	O
did	O
not	O
find	O
any	O
systematic	O
defects	O
in	O
examples	O
with	O
high	O
coherence	O
,	O
high	O
topic	O
similarity	O
,	O
or	O
low	O
redundancy	O
scores	O
.	O
Instead	O
,	O
almost	O
all	O
of	O
these	O
examples	O
were	O
satisfactory	O
.	O
For	O
the	O
remaining	O
two	O
properties	O
(	O
compression	O
measured	O
by	O
CMP	O
w	O
,	O
abstractivity	O
measured	O
by	O
ABS	O
1	O
)	O
,	O
we	O
analyzed	O
all	O
of	O
the	O
associated	O
400	O
examples	O
.	O
What	O
we	O
observed	O
is	O
that	O
many	O
of	O
these	O
examples	O
tended	O
to	O
be	O
generically	O
low	O
quality	O
and	O
we	O
quantify	O
this	O
in	O
Table	O
3	O
.	O
Since	O
this	O
analysis	O
may	O
be	O
difficult	O
to	O
replicate	O
and	O
involves	O
subjective	O
decisions	O
about	O
example	O
quality	O
,	O
we	O
comprehensively	O
enumerate	O
all	O
example	O
IDs	O
we	O
use	O
in	O
Table	O
8	O
.	O
Table	O
4	O
shows	O
a	O
representative	O
subset	O
of	O
the	O
low	O
quality	O
examples	O
we	O
found	O
in	O
our	O
analysis	O
.	O
We	O
provide	O
further	O
examples	O
in	O
Appendix	O
C	O
and	O
Figures	O
1	O
-	O
9	O
.	O
Compression	O
.	O
Minimally	O
compressed	O
summaries	O
in	O
NYT	O
,	O
NWS	O
,	O
TL	O
;	O
DR	O
,	O
and	O
PubMed	O
often	O
are	O
supplementary	O
information	O
to	O
the	O
document	O
rather	O
than	O
a	O
summary	O
of	O
it	O
;	O
in	O
some	O
cases	O
,	O
we	O
believe	O
this	O
is	O
due	O
to	O
errors	O
in	O
alignment	O
in	O
dataset	O
construction	O
/	O
release	O
.	O
On	O
the	O
other	O
hand	O
,	O
heavily	O
compressed	O
summaries	O
in	O
NWS	O
and	O
XSum	B-DatasetName
often	O
are	O
just	O
category	O
labels	O
(	O
e.g.	O
Sports	O
)	O
,	O
in	O
TL	O
;	O
DR	O
are	O
usually	O
attention	O
-	O
grabbers	O
,	O
and	O
in	O
NYT	O
are	O
nearexact	O
duplicates	O
of	O
reference	O
documents	O
,	O
which	O
themselves	O
are	O
letters	O
to	O
the	O
editor	O
.	O
Abstractivity	O
.	O
Manual	O
inspection	O
reveals	O
highly	O
abstractive	O
summaries	O
in	O
NYT	O
and	O
NWS	O
generally	O
are	O
exceedingly	O
vague	O
or	O
are	O
entirely	O
unrelated	O
to	O
the	O
original	O
document	O
.	O
Highly	O
abstractive	O
summaries	O
in	O
PeerRead	B-DatasetName
are	O
often	O
translated	O
to	O
English	O
from	O
the	O
reference	O
document	O
's	O
language	O
and	O
discuss	O
results	O
that	O
do	O
not	O
appear	O
in	O
the	O
introduction	O
but	O
likely	O
appear	O
later	O
in	O
the	O
paper	O
.	O
Conversely	O
,	O
extremely	O
extractive	O
summaries	O
in	O
NWS	O
and	O
NYT	O
often	O
are	O
just	O
the	O
lede	O
and	O
can	O
not	O
be	O
understood	O
without	O
the	O
reference	O
document	O
.	O
However	O
,	O
in	O
most	O
other	O
instances	O
,	O
the	O
lede	O
is	O
an	O
effective	O
summary	O
for	O
examples	O
drawn	O
from	O
the	O
News	O
domain	O
.	O
Within	O
the	O
context	O
of	O
our	O
sample	O
of	O
examples	O
,	O
we	O
find	O
that	O
eight	O
of	O
the	O
ten	O
summarization	B-TaskName
datasets	O
(	O
all	O
but	O
AMI	O
,	O
MovieScript	O
)	O
contain	O
at	O
least	O
8	O
%	O
low	O
quality	O
examples	O
,	O
the	O
majority	O
contain	O
at	O
least	O
14	O
%	O
low	O
quality	O
examples	O
,	O
and	O
that	O
these	O
low	O
quality	O
examples	O
can	O
be	O
detected	O
using	O
our	O
compression	O
and	O
abstractivity	O
metrics	O
.	O
For	O
the	O
worst	O
-	O
offending	O
TL	O
;	O
DR	O
dataset	O
,	O
we	O
conservatively	O
estimate	O
at	O
least	O
20	O
%	O
of	O
examples	O
are	O
of	O
substantially	O
subpar	O
quality	O
.	O
In	O
general	O
,	O
we	O
find	O
that	O
the	O
low	O
quality	O
TL	O
;	O
DR	O
"	O
summaries	O
"	O
we	O
detect	O
often	O
serve	O
a	O
different	O
rhetorical	O
purpose	O
than	O
summarization	B-TaskName
(	O
e.g.	O
attention	O
grabbing	O
,	O
responding	O
to	O
a	O
previous	O
post	O
that	O
is	O
not	O
available	O
in	O
the	O
dataset	O
,	O
sarcasm	O
/	O
humor	O
)	O
.	O

Dataset	O
Analysis	O
.	O
As	O
an	O
alternative	O
to	O
automated	O
evaluation	O
,	O
Chen	O
et	O
al	O
(	O
2016	O
)	O
and	O
Yatskar	O
(	O
2019	O
)	O
conduct	O
human	O
evaluations	O
of	O
standard	O
datasets	O
in	O
reading	B-TaskName
comprehension	I-TaskName
and	O
question	B-TaskName
answering	I-TaskName
.	O
In	O
some	O
cases	O
,	O
dataset	O
creators	O
perform	O
manual	O
analyses	O
of	O
the	O
data	O
they	O
introduce	O
(	O
e.g.	O
Sandhaus	O
(	O
2008	O
)	O
and	O
Grusky	O
et	O
al	O
(	O
2018	O
)	O
for	O
the	O
NYT	O
and	O
Newsroom	O
corpora	O
,	O
respectively	O
)	O
.	O
Automated	O
and	O
human	O
evaluation	O
provide	O
complementary	O
benefits	O
with	O
respect	O
to	O
their	O
scalability	O
and	O
reliability	O
.	O
Even	O
in	O
the	O
context	O
of	O
human	O
evaluations	O
,	O
we	O
advocate	O
that	O
automatic	O
metrics	O
can	O
be	O
useful	O
in	O
guiding	O
the	O
exploration	O
of	O
data	O
and	O
informing	O
subsampling	O
procedures	O
that	O
provide	O
fine	O
-	O
grained	O
insights	O
.	O
Quality	O
Estimation	O
.	O
Our	O
work	O
bears	O
resemblance	O
both	O
in	O
name	O
and	O
structure	O
to	O
work	O
on	O
quality	O
estimation	O
.	O
Quality	O
estimation	O
,	O
often	O
centered	O
on	O
natural	O
language	O
generation	O
,	O
is	O
the	O
task	O
of	O
measuring	O
system	O
-	O
generated	O
output	O
quality	O
(	O
Paetzold	O
and	O
Specia	O
,	O
2016	O
;	O
Yuan	O
and	O
Sharoff	O
,	O
2020	O
)	O
.	O
It	O
is	O
closely	O
related	O
to	O
work	O
on	O
unsupervised	O
or	O
reference	O
-	O
free	O
evaluation	O
(	O
Napoles	O
et	O
al	O
,	O
2016	O
;	O
Ethayarajh	O
and	O
Sadigh	O
,	O
2020	O
)	O
.	O
Within	O
the	O
context	O
of	O
summarization	B-TaskName
,	O
the	O
special	O
case	O
of	O
quality	O
estimation	O
regarding	O
factual	O
consistency	O
/	O
faithfulness	O
has	O
been	O
of	O
recent	O
interest	O
(	O
Wang	O
et	O
al	O
,	O
2020	O
;	O
Maynez	O
et	O
al	O
,	O
2020	O
;	O
Durmus	O
et	O
al	O
,	O
2020	O
)	O
since	O
neural	O
abstractive	O
summarizers	O
have	O
been	O
shown	O
to	O
hallucinate	O
/	O
misrepresent	O
facts	O
(	O
See	O
et	O
al	O
,	O
2017	O
)	O
.	O
In	O
comparison	O
to	O
these	O
settings	O
,	O
our	O
metrics	O
make	O
no	O
use	O
of	O
labelled	O
data	O
(	O
even	O
in	O
training	O
)	O
and	O
are	O
entirely	O
intrinsic	O
/	O
unsupervised	O
.	O
Summarization	B-TaskName
Practices	O
.	O
Several	O
analyses	O
and	O
critiques	O
exist	O
for	O
different	O
aspects	O
of	O
the	O
summarization	B-TaskName
pipeline	O
.	O
From	O
a	O
modelling	O
perspective	O
,	O
Zhang	O
et	O
al	O
(	O
2018	O
)	O
assess	O
whether	O
abstractive	O
systems	O
are	O
truly	O
abstractive	O
,	O
Kedzie	O
et	O
al	O
(	O
2018	O
)	O
evaluate	O
content	O
selection	O
policies	O
in	O
a	O
variety	O
of	O
methods	O
,	O
and	O
Mao	O
et	O
al	O
(	O
2020	O
)	O
assess	O
the	O
facetlevel	O
performance	O
of	O
extractive	O
summarizers	O
.	O
From	O
an	O
evaluation	O
perspective	O
,	O
several	O
works	O
have	O
discussed	O
the	O
shortcomings	O
of	O
ROUGE	O
/	O
automated	O
evaluation	O
(	O
Liu	O
and	O
Liu	O
,	O
2008	O
;	O
Chaganty	O
et	O
al	O
,	O
2018	O
;	O
Hashimoto	O
et	O
al	O
,	O
2019	O
;	O
Peyrard	O
,	O
2019b	O
)	O
as	O
well	O
proposed	O
alternative	O
metrics	O
for	O
summarization	B-TaskName
or	O
natural	O
language	O
generation	O
more	O
broadly	O
(	O
Clark	O
et	O
al	O
,	O
2019	O
;	O
Zhang	O
et	O
al	O
,	O
2020	O
;	O
Sellam	O
et	O
al	O
,	O
2020	O
)	O
.	O
Two	O
recent	O
works	O
are	O
highly	O
related	O
to	O
our	O
own	O
.	O
Kryscinski	O
et	O
al	O
(	O
2019	O
)	O
provide	O
a	O
critical	O
reevaluation	O
of	O
summarization	B-TaskName
research	O
.	O
Most	O
relevant	O
to	O
our	O
work	O
,	O
they	O
show	O
that	O
web	O
-	O
scraped	O
datasets	O
,	O
specifically	O
CNN	B-DatasetName
-	I-DatasetName
DM	I-DatasetName
and	O
NWS	O
,	O
contain	O
a	O
nontrivial	O
fraction	O
of	O
examples	O
(	O
approx	O
.	O
3.5	O
%	O
)	O
with	O
HTML	O
artifacts	O
(	O
which	O
can	O
be	O
easily	O
detected	O
/	O
removed	O
)	O
.	O
Jung	O
et	O
al	O
(	O
2019	O
)	O
provide	O
an	O
aspect	O
-	O
level	O
evaluation	O
of	O
both	O
summarization	B-TaskName
datasets	O
and	O
systems	O
.	O
In	O
their	O
work	O
,	O
the	O
dataset	O
analyses	O
center	O
on	O
biases	O
in	O
the	O
data	O
(	O
e.g.	O
positional	O
biases	O
,	O
which	O
are	O
often	O
seen	O
in	O
news	B-TaskName
summarization	I-TaskName
)	O
,	O
which	O
is	O
reminiscent	O
of	O
the	O
annotation	O
artifacts	O
seen	O
in	O
other	O
NLP	O
tasks	O
(	O
Gururangan	O
et	O
al	O
,	O
2018	O
;	O
Niven	O
and	O
Kao	O
,	O
2019	O
)	O
.	O

Open	O
Problems	O
and	O
Future	O
Directions	O
.	O
Our	O
results	O
demonstrate	O
that	O
a	O
sizeable	O
fraction	O
of	O
examples	O
in	O
most	O
summarization	B-TaskName
datasets	O
are	O
low	O
quality	O
.	O
However	O
,	O
it	O
remains	O
open	O
whether	O
modellers	O
should	O
simply	O
prune	O
these	O
examples	O
,	O
manually	O
/	O
automatically	O
attempt	O
to	O
correct	O
them	O
,	O
or	O
model	O
them	O
without	O
change	O
.	O
We	O
do	O
note	O
that	O
research	O
in	O
the	O
machine	O
learning	O
and	O
learning	O
theory	O
communities	O
shows	O
that	O
models	O
both	O
theoretically	O
and	O
empirically	O
do	O
substantially	O
worse	O
when	O
trained	O
using	O
low	O
quality	O
examples	O
,	O
even	O
when	O
the	O
examples	O
are	O
not	O
strictly	O
adversarially	O
chosen	O
(	O
Klivans	O
et	O
al	O
,	O
2009	O
;	O
Biggio	O
et	O
al	O
,	O
2012	O
;	O
Koh	O
et	O
al	O
,	O
2018	O
)	O
.	O
These	O
concerns	O
are	O
further	O
compounded	O
by	O
the	O
evidence	O
of	O
Belinkov	O
and	O
Bisk	O
(	O
2018	O
)	O
that	O
neural	O
models	O
for	O
natural	O
language	O
generation	O
are	O
not	O
robust	O
to	O
naturally	O
noisy	O
data	O
.	O
Our	O
metrics	O
may	O
be	O
repurposed	O
to	O
rank	O
examples	O
in	O
designing	O
curricula	O
for	O
curriculum	O
learning	O
ap	O
-	O
proaches	O
(	O
Bengio	O
et	O
al	O
,	O
2009	O
)	O
.	O
Alternatively	O
,	O
they	O
can	O
serve	O
as	O
additional	O
metrics	O
for	O
the	O
(	O
possibly	O
unsupervised	O
)	O
evaluation	O
of	O
summarization	B-TaskName
systems	O
,	O
potentially	O
mitigating	O
deficiencies	O
in	O
standard	O
metrics	O
,	O
such	O
as	O
ROUGE	O
,	O
by	O
directly	O
penalizing	O
redundancy	O
and	O
semantic	O
incoherence	O
.	O
Limitations	O
.	O
In	O
this	O
work	O
,	O
we	O
restrict	O
ourselves	O
to	O
single	O
-	O
document	O
single	O
-	O
reference	O
English	O
language	O
summarization	B-TaskName
datasets	O
.	O
While	O
the	O
datasets	O
we	O
study	O
constitute	O
a	O
considerable	O
fraction	O
of	O
dataset	O
usage	O
in	O
the	O
summarization	B-TaskName
community	O
,	O
several	O
multi	B-TaskName
-	I-TaskName
document	I-TaskName
summarization	I-TaskName
datasets	O
have	O
been	O
introduced	O
(	O
e.g.	O
Fabbri	O
et	O
al	O
,	O
2019	O
;	O
Antognini	O
and	O
Faltings	O
,	O
2020	O
)	O
and	O
multi	O
-	O
reference	O
summarization	B-TaskName
datasets	O
have	O
often	O
been	O
argued	O
to	O
be	O
desirable	O
due	O
to	O
under	O
-	O
constrained	O
nature	O
of	O
the	O
summarization	B-TaskName
task	O
(	O
Kryscinski	O
et	O
al	O
,	O
2019	O
)	O
and	O
the	O
ideal	O
evaluation	O
paradigm	O
for	O
ROUGE	O
(	O
Lin	O
,	O
2004	O
)	O
.	O
Beyond	O
English	O
,	O
both	O
large	O
summarization	B-TaskName
datasets	O
(	O
Nguyen	O
and	O
Daumé	O
III	O
,	O
2019	O
;	O
Varab	O
and	O
Schluter	O
,	O
2020	O
)	O
and	O
more	O
general	O
language	O
resources	O
/	O
technologies	O
(	O
Joshi	O
et	O
al	O
,	O
2020	O
)	O
are	O
less	O
available	O
,	O
which	O
may	O
heighten	O
the	O
need	O
for	O
data	O
quality	O
assurance	O
.	O
More	O
broadly	O
,	O
the	O
measures	O
that	O
we	O
introduce	O
are	O
automated	O
,	O
and	O
therefore	O
non	O
-	O
human	O
,	O
judgments	O
of	O
the	O
quality	O
of	O
summarization	B-TaskName
data	O
.	O
Therefore	O
,	O
we	O
only	O
envision	O
these	O
measures	O
to	O
be	O
useful	O
as	O
inexpensive	O
first	O
-	O
order	O
approximations	O
of	O
aspectlevel	O
summary	O
quality	O
rather	O
than	O
bona	O
fide	O
replacements	O
for	O
human	O
evaluation	O
.	O
Additionally	O
,	O
since	O
we	O
principally	O
envision	O
applying	O
these	O
metrics	O
to	O
datasets	O
,	O
we	O
make	O
no	O
efforts	O
to	O
make	O
these	O
metrics	O
robust	O
to	O
adversarially	O
-	O
crafted	O
data	O
and	O
they	O
are	O
likely	O
quite	O
susceptible	O
to	O
adversarial	B-TaskName
attack	I-TaskName
.	O

In	O
this	O
work	O
,	O
we	O
demonstrate	O
that	O
various	O
aspects	O
of	O
summarization	B-TaskName
datasets	O
can	O
be	O
intrinsically	O
evaluated	O
for	O
.	O
We	O
specifically	O
show	O
this	O
for	O
5	O
properties	O
across	O
10	O
popular	O
datasets	O
,	O
uncovering	O
that	O
dataset	O
use	O
is	O
sometimes	O
incongruous	O
with	O
the	O
attributes	O
of	O
the	O
underlying	O
data	O
.	O
We	O
also	O
find	O
that	O
some	O
aspectlevel	O
estimators	O
may	O
be	O
surprisingly	O
effective	O
at	O
detecting	O
low	O
quality	O
dataset	O
examples	O
.	O
Our	O
findings	O
suggest	O
that	O
more	O
intentional	O
and	O
deliberate	O
decisions	O
should	O
be	O
made	O
in	O
selecting	O
summarization	B-TaskName
datasets	O
for	O
downstream	O
modelling	O
research	O
and	O
that	O
further	O
scrutiny	O
should	O
be	O
placed	O
upon	O
summarization	B-TaskName
datasets	O
released	O
in	O
the	O
future	O
.	O

We	O
evaluate	O
for	O
semantic	O
coherence	O
between	O
successive	O
pairs	O
of	O
sentences	O
,	O
exploiting	O
the	O
auxiliary	O
training	O
objective	O
of	O
BERT	B-MethodName
beyond	O
its	O
masked	B-TaskName
language	I-TaskName
modeling	I-TaskName
objective	O
.	O
In	O
particular	O
,	O
we	O
were	O
especially	O
interested	O
in	O
this	O
given	O
that	O
many	O
systems	O
are	O
designed	O
with	O
explicit	O
handling	O
of	O
sentence	O
boundaries	O
(	O
e.g.	O
more	O
extractive	O
systems	O
first	O
rank	O
extractive	O
sentences	O
and	O
then	O
order	O
a	O
thresholded	O
subset	O
)	O
and	O
datasets	O
such	O
as	O
CNN	B-DatasetName
-	I-DatasetName
DM	I-DatasetName
,	O
which	O
are	O
artificially	O
concatenated	O
,	O
may	O
not	O
be	O
inherently	O
coherent	O
across	O
sentence	O
-	O
boundaries	O
.	O
Our	O
observations	O
regarding	O
the	O
measure	O
of	O
coherence	O
provided	O
by	O
BERT	B-MethodName
's	O
next	O
-	O
sentence	O
predictions	O
seem	O
to	O
contradict	O
existing	O
findings	O
.	O
In	O
particular	O
,	O
introduce	O
RoBERTa	B-MethodName
as	O
a	O
direct	O
followup	B-DatasetName
study	O
to	O
BERT	B-MethodName
and	O
find	O
that	O
the	O
next	O
-	O
sentence	O
prediction	O
objective	O
is	O
not	O
an	O
effective	O
pretraining	O
objective	O
for	O
improving	O
representations	O
for	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
;	O
Yang	O
et	O
al	O
(	O
2019	O
)	O
also	O
provide	O
similar	O
evidence	O
.	O
However	O
,	O
our	O
findings	O
do	O
not	O
contest	O
these	O
conclusions	O
but	O
instead	O
suggest	O
that	O
,	O
nonetheless	O
,	O
BERT	B-MethodName
is	O
a	O
strong	O
next	O
-	O
sentence	O
predictor	O
and	O
that	O
these	O
predictions	O
are	O
still	O
useful	O
for	O
measuring	O
coherence	O
across	O
sentences	O
.	O
While	O
we	O
considered	O
word	O
or	O
subword	O
measures	O
of	O
coherence	O
,	O
we	O
did	O
not	O
consider	O
alternative	O
pretrained	O
models	O
that	O
are	O
pretrained	O
on	O
other	O
objectives	O
related	O
to	O
inter	O
-	O
sentence	O
coherence	O
such	O
as	O
ALBERT	B-MethodName
(	O
Lan	O
et	O
al	O
,	O
2020	O
)	O
.	O
Given	O
the	O
findings	O
of	O
Lan	O
et	O
al	O
(	O
2020	O
,	O
4.6	O
)	O
,	O
it	O
seems	O
likely	O
that	O
the	O
sentence	O
order	O
prediction	O
task	O
they	O
use	O
may	O
be	O
more	O
effective	O
for	O
measuring	O
semantic	O
coherence	O
.	O
Concurrent	O
work	O
by	O
Prabhumoye	O
et	O
al	O
(	O
2020	O
)	O
also	O
substantiates	O
the	O
usefulness	O
of	O
BERT	B-MethodName
-	O
based	O
nextsentence	O
prediction	O
for	O
measuring	O
coherence	O
and	O
ranking	O
sentences	O
orders	O
.	O
That	O
said	O
,	O
semantic	O
coherence	O
could	O
also	O
be	O
evaluated	O
using	O
(	O
neural	O
)	O
language	O
models	O
,	O
especially	O
in	O
light	O
of	O
results	O
suggest	O
they	O
may	O
be	O
consistent	O
with	O
human	O
judgments	O
regarding	O
grammaticality	O
and	O
acceptability	O
(	O
Chowdhury	O
and	O
Zamparelli	O
,	O
2018	O
;	O
Warstadt	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
did	O
consider	O
this	O
and	O
found	O
language	O
modeling	O
scores	O
(	O
e.g.	O
surprisal	O
)	O
assigned	O
via	O
a	O
pretrained	O
high	O
-	O
quality	O
causal	O
lan	O
-	O
guage	O
model	O
(	O
GPT	B-MethodName
-	O
2	O
)	O
to	O
be	O
inconsistent	O
with	O
our	O
human	O
judgments	O
.	O
We	O
believe	O
language	O
modeling	O
scores	O
in	O
this	O
sense	O
are	O
likely	O
highly	O
sensitive	O
to	O
the	O
domain	O
(	O
and	O
even	O
within	O
-	O
domain	O
effects	O
,	O
e.g.	O
lexical	O
variation	O
for	O
XSum	B-DatasetName
which	O
is	O
fairly	O
limited	O
given	O
all	O
articles	O
are	O
sourced	O
from	O
the	O
BBC	O
whereas	O
for	O
Newsroom	O
the	O
variation	O
is	O
greater	O
given	O
the	O
heterogeneous	O
group	O
of	O
publishers	O
with	O
more	O
diversified	O
writing	O
styles	O
)	O
.	O

We	O
use	O
the	O
versions	O
of	O
GW	O
and	O
CNN	B-DatasetName
-	I-DatasetName
DM	I-DatasetName
dataset	O
released	O
by	O
Gehrmann	O
et	O
al	O
(	O
2018	O
)	O
.	O
14	O
Sentence	O
boundary	O
tokens	O
inserted	O
by	O
Gehrmann	O
et	O
al	O
(	O
2018	O
)	O
to	O
improve	O
summarization	B-TaskName
quality	O
were	O
removed	O
to	O
ensure	O
fair	O
comparison	O
in	O
our	O
work	O
.	O
An	O
important	O
distinction	O
in	O
the	O
use	O
of	O
the	O
CNN	B-DatasetName
-	I-DatasetName
DM	I-DatasetName
dataset	O
for	O
modeling	O
is	O
whether	O
the	O
entity	O
-	O
anonymized	O
or	O
non	O
-	O
anonymized	O
version	O
was	O
used	O
.	O
This	O
copy	O
is	O
non	O
-	O
anonymized	O
and	O
it	O
is	O
important	O
to	O
consider	O
the	O
stability	O
of	O
our	O
metrics	O
under	O
this	O
anonymization	O
.	O
We	O
used	O
the	O
released	O
version	O
of	O
the	O
NYT	O
dataset	O
directly	O
as	O
it	O
was	O
released	O
via	O
LDC	O
.	O
We	O
use	O
the	O
released	O
version	O
of	O
the	O
TL	O
;	O
DR	O
dataset	O
provided	O
by	O
the	O
authors	O
of	O
Völske	O
et	O
al	O
(	O
2017	O
)	O
.	O
16	O
We	O
use	O
a	O
version	O
of	O
the	O
NWS	O
dataset	O
that	O
was	O
released	O
via	O
private	O
communication	O
with	O
the	O
authors	O
of	O
Grusky	O
et	O
al	O
(	O
2018	O
)	O
.	O
We	O
have	O
verified	O
with	O
the	O
authors	O
that	O
the	O
data	O
can	O
be	O
requested	O
with	O
the	O
platform	O
they	O
released	O
in	O
their	O
original	O
work	O
.	O
17	O
For	O
all	O
remaining	O
datasets	O
,	O
we	O
use	O
the	O
version	O
released	O
by	O
Jung	O
et	O
al	O
(	O
2019	O
)	O
.	O
18	O
All	O
of	O
our	O
conventions	O
in	O
using	O
these	O
five	O
datasets	O
follow	O
their	O
work	O
.	O

All	O
datasets	O
were	O
first	O
filtered	O
to	O
remove	O
examples	O
where	O
either	O
the	O
document	O
or	O
summary	O
was	O
empty	O
.	O
We	O
found	O
only	O
examples	O
in	O
CNN	B-DatasetName
-	I-DatasetName
DM	I-DatasetName
failed	O
this	O
criterion	O
and	O
this	O
constituted	O
less	O
than	O
0.1	O
%	O
114	O
287227	O
of	O
the	O
dataset	O
.	O
All	O
results	O
were	O
reported	O
then	O
on	O
the	O
standard	O
training	O
set	O
if	O
we	O
were	O
aware	O
of	O
a	O
standard	O
split	O
used	O
consistently	O
in	O
the	O
summarization	B-TaskName
system	O
literature	O
.	O
Splits	O
in	O
the	O
case	O
of	O
datasets	O
sourced	O
from	O
the	O
work	O
of	O
Jung	O
et	O
al	O
(	O
2019	O
)	O
followed	O
their	O
work	O
.	O
In	O
all	O
cases	O
,	O
the	O
training	O
set	O
was	O
at	O
least	O
80	O
%	O
of	O
the	O
full	O
data	O
collection	O
,	O
so	O
we	O
expect	O
results	O
to	O
generalize	O
to	O
the	O
portions	O
of	O
the	O
collection	O
that	O
were	O
not	O
considered	O
assuming	O
splits	O
were	O
constructed	O
by	O
sampling	O
uniformly	O
at	O
random	O
(	O
we	O
did	O
not	O
verify	O
this	O
)	O
.	O
Sentence	O
-	O
level	O
tokenization	O
was	O
performed	O
using	O
NLTK	O
(	O
Loper	O
and	O
Bird	O
,	O
2002	O
)	O
.	O
Word	O
-	O
level	O
tokenization	O
was	O
performed	O
using	O
SpaCy	O
(	O
Honnibal	O
and	O
Montani	O
,	O
2017	O
)	O
.	O

We	O
compute	O
semantic	O
coherence	O
by	O
predicting	O
the	O
probability	O
of	O
a	O
sentence	O
conditional	O
on	O
the	O
preceding	O
sentence	O
using	O
BERT	B-MethodName
.	O
BERT	B-MethodName
was	O
pretrained	O
with	O
exactly	O
this	O
objective	O
(	O
beyond	O
its	O
masked	B-TaskName
language	I-TaskName
modeling	I-TaskName
objective	O
)	O
and	O
we	O
use	O
the	O
released	O
model	O
as	O
-	O
is	O
with	O
no	O
further	O
fine	O
-	O
tuning	O
.	O
We	O
use	O
the	O
bert	O
-	O
base	O
-	O
uncased	O
model	O
along	O
with	O
the	O
associated	O
tokenizer	O
that	O
was	O
implemented	O
in	O
PyTorch	O
(	O
Paszke	O
et	O
al	O
,	O
2017	O
)	O
by	O
HuggingFace	O
in	O
the	O
transformers	O
repository	O
.	O
20	O

Figure	O
3	O
:	O
Dataset	O
:	O
PeerRead	B-DatasetName
.	O
This	O
summary	O
simply	O
is	O
not	O
in	O
the	O
same	O
language	O
and	O
hence	O
achieves	O
a	O
very	O
high	O
abstractivity	O
.	O
Original	O
Text	O
(	O
truncated	O
)	O
:	O
from	O
russia	O
with	O
love"screenplay	O
byrichard	O
maibaumadapted	O
byjohanna	O
harwoodbased	O
on	O
the	O
novel	O
byian	O
fleming	O
.	O
.	O
.	O

Figure	O
8	O
:	O
Dataset	O
:	O
TL	O
;	O
DR	O
.	O
We	O
observe	O
this	O
trend	O
quite	O
frequently	O
in	O
TL	O
;	O
DR	O
.	O
Specifically	O
,	O
since	O
authors	O
on	O
the	O
social	O
discussion	O
platform	O
Reddit	B-DatasetName
choose	O
to	O
provide	O
these	O
summaries	O
at	O
their	O
discretion	O
,	O
we	O
often	O
find	O
the	O
"	O
summaries	O
"	O
are	O
attention	O
-	O
grabbing	O
and	O
serve	O
a	O
starkly	O
different	O
rhetorical	O
purpose	O
from	O
how	O
summaries	O
are	O
generally	O
conceived	O
.	O
The	O
mutual	O
information	O
between	O
random	O
variables	O
X	O
and	O
Y	O
is	O
defined	O
as	O
:	O
I	O
(	O
X	O
;	O
Y	O
)	O
H	O
(	O
X	O
)	O
−	O
H	O
(	O
X	O
|	O
Y	O
)	O
The	O
entropy	O
measures	O
the	O
uncertainty	O
in	O
the	O
probability	O
mass	O
/	O
density	O
function	O
of	O
a	O
random	O
variable	O
.	O
As	O
such	O
,	O
the	O
mutual	O
information	O
measures	O
how	O
Original	O
Text	O
(	O
truncated	O
)	O
:	O
these	O
are	O
external	O
links	O
and	O
will	O
open	O
in	O
a	O
new	O
window1908	O
-	O
king	O
carlos	O
and	O
eldest	O
son	O
assassinated	O
in	O
lisbon	O
.	O
second	O
son	O
manuel	O
becomes	O
king	O
.	O
1910	O
-	O
king	O
manuel	O
ii	O
abdicates	O
amid	O
revolution	O
.	O
.	O
.	O

Detector	O
:	O
Extremely	O
High	O
Compression	O
Figure	O
9	O
:	O
Dataset	O
:	O
XSum	B-DatasetName
.	O
We	O
observe	O
this	O
trend	O
quite	O
frequently	O
in	O
XSum	B-DatasetName
.	O
For	O
articles	O
that	O
are	O
essentially	O
timelines	O
or	O
other	O
types	O
of	O
chronologies	O
discussing	O
historic	O
events	O
diachronically	O
(	O
which	O
forms	O
a	O
small	O
but	O
distinctive	O
section	O
of	O
the	O
writing	O
style	O
of	O
BBC	O
from	O
our	O
analysis	O
)	O
,	O
the	O
summary	O
extracted	O
to	O
accompany	O
it	O
is	O
generally	O
this	O
string	O
or	O
a	O
slightly	O
altered	O
version	O
.	O
We	O
argue	O
this	O
summary	O
is	O
fairly	O
unhelpful	O
(	O
and	O
is	O
likely	O
fairly	O
uninteresting	O
to	O
test	O
models	O
on	O
;	O
simple	O
rule	O
-	O
based	O
filtering	O
made	O
be	O
preferable	O
to	O
avoid	O
overestimating	O
performance	O
on	O
this	O
dataset	O
because	O
of	O
these	O
examples	O
)	O
.	O
much	O
the	O
entropy	O
of	O
X	O
is	O
reduced	O
by	O
(	O
on	O
average	O
)	O
due	O
to	O
the	O
observation	O
of	O
Y	O
.	O
Intuitively	O
,	O
the	O
claim	O
is	O
that	O
the	O
uncertainty	O
about	O
the	O
summarization	B-TaskName
task	O
that	O
is	O
reduced	O
by	O
the	O
model	O
(	O
which	O
is	O
uniquely	O
determined	O
by	O
its	O
training	O
data	O
,	O
pretraining	O
data	O
,	O
and	O
architecture	O
)	O
is	O
at	O
most	O
what	O
can	O
be	O
cumulatively	O
reduced	O
by	O
the	O
training	O
data	O
,	O
pretraining	O
data	O
,	O
and	O
inductive	O
biases	O
encoded	O
in	O
the	O
model	O
's	O
architecture	O
.	O
Our	O
hypothesis	O
is	O
that	O
I	O
(	O
S	O
;	O
A	O
)	O
is	O
small	O
for	O
learning	O
-	O
based	O
models	O
with	O
minimal	O
inductive	O
biases	O
,	O
such	O
as	O
neural	O
networks	O
.	O
Further	O
,	O
we	O
hypothesize	O
that	O
while	O
I	O
(	O
S	O
;	O
P	O
)	O
is	O
likely	O
nontrivial	O
for	O
popular	O
pretraining	O
regimes	O
,	O
the	O
dominant	O
term	O
on	O
the	O
right	O
-	O
hand	O
side	O
is	O
likely	O
I	O
(	O
S	O
;	O
T	O
)	O
.	O
We	O
do	O
note	O
that	O
this	O
second	O
hypothesis	O
may	O
be	O
false	O
given	O
the	O
partial	O
evidence	O
of	O
GPT	B-MethodName
-	O
3	O
(	O
Brown	O
et	O
al	O
,	O
2020	O
)	O
and	O
the	O
successes	O
it	O
enjoys	O
in	O
few	B-TaskName
-	I-TaskName
shot	I-TaskName
learning	I-TaskName
due	O
to	O
pretraining	O
at	O
unprecedented	O
scale	O
.	O
However	O
,	O
no	O
evaluation	O
is	O
conducted	O
on	O
summarization	B-TaskName
data	O
in	O
that	O
work	O
.	O

We	O
thank	O
Anna	O
Huang	O
for	O
her	O
help	O
with	O
analyzing	O
the	O
data	O
.	O
We	O
thank	O
Ge	O
Gao	O
,	O
Esin	O
Durmus	O
,	O
and	O
members	O
of	O
the	O
Cornell	B-DatasetName
and	O
Stanford	O
NLP	O
groups	O
for	O
their	O
valuable	O
advice	O
.	O
We	O
especially	O
thank	O
the	O
reviewers	O
and	O
area	O
chairs	O
for	O
their	O
articulate	O
and	O
constructive	O
feedback	O
.	O

CUNI	O
-	O
KIT	O
System	O
for	O
Simultaneous	O
Speech	O
Translation	B-TaskName
Task	O
at	O
IWSLT	O
2022	O

In	O
this	O
paper	O
,	O
we	O
describe	O
our	O
submission	O
to	O
the	O
Simultaneous	O
Speech	O
Translation	B-TaskName
at	O
IWSLT	O
2022	O
.	O
We	O
explore	O
strategies	O
to	O
utilize	O
an	O
offline	O
model	O
in	O
a	O
simultaneous	O
setting	O
without	O
the	O
need	O
to	O
modify	O
the	O
original	O
model	O
.	O
In	O
our	O
experiments	O
,	O
we	O
show	O
that	O
our	O
onlinization	O
algorithm	O
is	O
almost	O
on	O
par	O
with	O
the	O
offline	O
setting	O
while	O
being	O
3×	O
faster	O
than	O
offline	O
in	O
terms	O
of	O
latency	O
on	O
the	O
test	O
set	O
.	O
We	O
also	O
show	O
that	O
the	O
onlinized	O
offline	O
model	O
outperforms	O
the	O
best	O
IWSLT2021	O
simultaneous	O
system	O
in	O
medium	O
and	O
high	O
latency	O
regimes	O
and	O
is	O
almost	O
on	O
par	O
in	O
the	O
low	O
latency	O
regime	O
.	O
We	O
make	O
our	O
system	O
publicly	O
available	O
.	O
1	O

This	O
paper	O
describes	O
the	O
CUNI	O
-	O
KIT	O
submission	O
to	O
the	O
Simultaneous	O
Speech	O
Translation	B-TaskName
task	O
at	O
IWSLT	O
2022	O
(	O
Anastasopoulos	O
et	O
al	O
,	O
2022	O
)	O
by	O
Charles	O
University	O
(	O
CUNI	O
)	O
and	O
Karlsruhe	O
Institute	O
of	O
Technology	O
(	O
KIT	O
)	O
.	O
Recent	O
work	O
on	O
end	O
-	O
to	O
-	O
end	O
(	O
E2E	B-DatasetName
)	O
simultaneous	O
speech	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
text	I-TaskName
translation	I-TaskName
(	O
ST	O
)	O
is	O
focused	O
on	O
training	O
specialized	O
models	O
specifically	O
for	O
this	O
task	O
.	O
The	O
disadvantage	O
is	O
the	O
need	O
of	O
storing	O
an	O
extra	O
model	O
,	O
usually	O
a	O
more	O
difficult	O
training	O
and	O
inference	O
setup	O
,	O
increased	O
computational	O
complexity	O
Liu	O
et	O
al	O
,	O
2021	O
)	O
and	O
risk	O
of	O
performance	O
degradation	O
if	O
used	O
in	O
offline	O
setting	O
(	O
Liu	O
et	O
al	O
,	O
2020a	O
)	O
.	O
In	O
this	O
work	O
,	O
we	O
base	O
our	O
system	O
on	O
a	O
robust	O
multilingual	O
offline	O
ST	O
model	O
that	O
leverages	O
pretrained	O
wav2vec	O
2.0	O
(	O
Baevski	O
et	O
al	O
,	O
2020	O
)	O
and	O
mBART	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2020b	O
)	O
.	O
We	O
revise	O
the	O
onlinization	O
approach	O
by	O
Liu	O
et	O
al	O
(	O
2020a	O
)	O
and	O
propose	O
an	O
improved	O
technique	O
with	O
a	O
fully	O
controllable	O
qualitylatency	O
trade	O
-	O
off	O
.	O
We	O
demonstrate	O
that	O
without	O
any	O
change	O
to	O
the	O
offline	O
model	O
,	O
our	O
simultaneous	O
system	O
in	O
the	O
mid	O
-	O
and	O
high	O
-	O
latency	O
regimes	O
is	O
on	O
par	O
with	O
the	O
offline	O
performance	O
.	O
At	O
the	O
same	O
time	O
,	O
the	O
model	O
outperforms	O
previous	O
IWSLT	O
systems	O
in	O
medium	O
and	O
high	O
latency	O
regimes	O
and	O
is	O
almost	O
on	O
par	O
in	O
the	O
low	O
latency	O
regime	O
.	O
Finally	O
,	O
we	O
observe	O
a	O
problematic	O
behavior	O
of	O
the	O
average	O
lagging	O
metric	O
for	O
speech	O
translation	O
when	O
dealing	O
with	O
long	O
hypotheses	O
,	O
resulting	O
in	O
negative	O
values	O
.	O
We	O
propose	O
a	O
minor	O
change	O
to	O
the	O
metric	O
formula	O
to	O
prevent	O
this	O
behavior	O
.	O
Our	O
contribution	O
is	O
as	O
follows	O
:	O
We	O
revise	O
and	O
generalize	O
onlinization	O
proposed	O
by	O
Liu	O
et	O
al	O
(	O
2020a	O
)	O
;	O
Nguyen	O
et	O
al	O
(	O
2021	O
)	O
and	O
discover	O
parameter	O
enabling	O
quality	O
-	O
latency	O
trade	O
-	O
off	O
,	O
We	O
demonstrate	O
that	O
one	O
multilingual	O
offline	O
model	O
can	O
serve	O
as	O
simultaneous	O
ST	O
for	O
three	O
language	O
pairs	O
,	O
We	O
demonstrate	O
that	O
an	O
improvement	O
in	O
the	O
offline	O
model	O
leads	O
also	O
to	O
an	O
improvement	O
in	O
the	O
online	O
regime	O
,	O
We	O
propose	O
a	O
change	O
to	O
the	O
average	O
lagging	O
metric	O
that	O
avoids	O
negative	O
values	O
.	O

Simultaneous	O
speech	O
translation	O
can	O
be	O
implemented	O
either	O
as	O
a	O
(	O
hybrid	O
)	O
cascaded	O
system	O
(	O
Kolss	O
et	O
al	O
,	O
2008	O
;	O
Elbayad	O
et	O
al	O
,	O
2020	O
;	O
Liu	O
et	O
al	O
,	O
2020a	O
;	O
Bahar	O
et	O
al	O
,	O
2021	O
)	O
or	O
an	O
end	O
-	O
to	O
-	O
end	O
model	O
Liu	O
et	O
al	O
,	O
2021	O
)	O
.	O
Unlike	O
for	O
the	O
offline	O
speech	O
translation	O
where	O
cascade	O
seems	O
to	O
have	O
the	O
best	O
quality	O
,	O
the	O
end	O
-	O
to	O
-	O
end	O
speech	O
translation	O
offers	O
a	O
better	O
qualitylatency	O
trade	O
-	O
off	O
(	O
Ansari	O
et	O
al	O
,	O
2020	O
;	O
Liu	O
et	O
al	O
,	O
2021	O
;	O
Anastasopoulos	O
et	O
al	O
,	O
2021	O
)	O
.	O
End	O
-	O
to	O
-	O
end	O
systems	O
use	O
different	O
techniques	O
to	O
perform	O
simultaneous	O
speech	O
translation	O
.	O
Han	O
et	O
al	O
(	O
2020	O
)	O
uses	O
wait	O
-	O
k	O
(	O
Ma	O
et	O
al	O
,	O
2019	O
)	O
model	O
and	O
metalearning	O
to	O
alleviate	O
the	O
data	O
scarcity	O
.	O
Liu	O
et	O
al	O
(	O
2020a	O
)	O
uses	O
a	O
unidirectional	O
encoder	O
with	O
monotonic	O
cross	O
-	O
attention	O
to	O
limit	O
the	O
dependence	O
on	O
future	O
context	O
.	O
Other	O
work	O
(	O
Liu	O
et	O
al	O
,	O
2021	O
)	O
proposes	O
Cross	O
Attention	O
augmented	O
Transducer	O
(	O
CAAT	O
)	O
as	O
an	O
extension	O
of	O
RNN	O
-	O
T	O
(	O
Graves	O
,	O
2012	O
)	O
.	O
Nguyen	O
et	O
al	O
(	O
2021	O
)	O
proposed	O
a	O
hypothesis	O
stability	O
detection	O
for	O
automatic	B-TaskName
speech	I-TaskName
recognition	I-TaskName
(	O
ASR	O
)	O
.	O
The	O
shared	O
prefix	O
strategy	O
finds	O
the	O
longest	O
common	O
prefix	O
in	O
all	O
beams	O
.	O
Liu	O
et	O
al	O
(	O
2020a	O
)	O
explore	O
such	O
strategies	O
in	O
the	O
context	O
of	O
speech	B-TaskName
recognition	I-TaskName
and	O
translation	O
.	O
The	O
most	O
promising	O
is	O
the	O
longest	O
common	O
prefix	O
of	O
two	O
consecutive	O
chunks	O
.	O
The	O
downside	O
of	O
this	O
approach	O
is	O
the	O
inability	O
to	O
parametrize	O
the	O
quality	O
-	O
latency	O
trade	O
-	O
off	O
.	O
We	O
directly	O
address	O
this	O
in	O
our	O
work	O
.	O

Speech	B-TaskName
recognition	I-TaskName
and	O
translation	O
use	O
chunking	B-TaskName
for	O
simultaneous	O
inference	O
with	O
various	O
chunk	O
sizes	O
ranging	O
from	O
300	O
ms	O
to	O
2	O
seconds	O
(	O
Liu	O
,	O
2020	O
;	O
Nguyen	O
et	O
al	O
,	O
2021	O
)	O
although	O
the	O
literature	O
suggests	O
that	O
the	O
turn	O
-	O
taking	O
in	O
conversational	O
speech	O
is	O
shorter	O
,	O
around	O
200	O
ms	O
(	O
Levinson	O
and	O
Torreira	O
,	O
2015	O
)	O
.	O
We	O
investigate	O
different	O
chunk	O
sizes	O
in	O
combination	O
with	O
various	O
stable	O
hypothesis	O
detection	O
strategies	O
.	O
As	O
we	O
document	O
later	O
,	O
the	O
chunk	O
size	O
is	O
the	O
principal	O
factor	O
that	O
controls	O
the	O
quality	O
-	O
latency	O
trade	O
-	O
off	O
.	O

Committing	O
hypotheses	O
from	O
incomplete	O
input	O
presents	O
a	O
possible	O
risk	O
of	O
introducing	O
errors	O
.	O
To	O
reduce	O
the	O
instability	O
and	O
trade	O
time	O
for	O
quality	O
,	O
we	O
employ	O
a	O
stable	O
hypothesis	O
detection	O
.	O
Formally	O
,	O
we	O
define	O
a	O
function	O
pref	O
ix	O
(	O
W	O
)	O
that	O
,	O
given	O
a	O
set	O
of	O
hypotheses	O
(	O
i.e.	O
,	O
W	O
c	O
all	O
if	O
we	O
want	O
to	O
consider	O
the	O
whole	O
beam	O
or	O
W	O
c	O
best	O
for	O
the	O
single	O
best	O
hypothesis	O
obtained	O
during	O
the	O
beam	O
search	O
decoding	O
of	O
the	O
c	O
-	O
th	O
chunk	O
)	O
,	O
outputs	O
a	O
stable	O
prefix	O
.	O
We	O
investigate	O
several	O
functions	O
:	O
Hold	O
-	O
n	O
(	O
Liu	O
et	O
al	O
,	O
2020a	O
)	O
Hold	O
-	O
n	O
strategy	O
selects	O
the	O
best	O
hypothesis	O
in	O
the	O
beam	O
and	O
deletes	O
the	O
last	O
n	O
tokens	O
from	O
it	O
:	O
prefix	O
(	O
W	O
c	O
best	O
)	O
=	O
W	O
0	B-DatasetName
:	O
max	O
(	O
0	B-DatasetName
,	O
|	O
W	O
|	O
−n	O
)	O
,	O
(	O
1	O
)	O
where	O
W	O
c	O
best	O
is	O
the	O
best	O
hypothesis	O
obtained	O
in	O
the	O
beam	O
search	O
of	O
c	O
-	O
th	O
chunk	O
.	O
If	O
the	O
hypothesis	O
has	O
only	O
n	O
or	O
fewer	O
tokens	O
,	O
we	O
return	O
an	O
empty	O
string	O
.	O
LA	O
-	O
n	O
Local	O
agreement	O
(	O
Liu	O
et	O
al	O
,	O
2020a	O
)	O
displays	O
the	O
agreeing	O
prefixes	O
of	O
the	O
two	O
consecutive	O
chunks	O
.	O
Unlike	O
the	O
hold	O
-	O
n	O
strategy	O
,	O
the	O
local	O
agreement	O
does	O
not	O
offer	O
any	O
explicit	O
quality	O
-	O
latency	O
trade	O
-	O
off	O
.	O
We	O
generalize	O
the	O
strategy	O
to	O
take	O
the	O
agreeing	O
prefixes	O
of	O
n	O
consecutive	O
chunks	O
.	O
During	O
the	O
first	O
n	O
−	O
1	O
chunks	O
,	O
we	O
do	O
not	O
output	O
any	O
tokens	O
.	O
From	O
the	O
n	O
-	O
th	O
chunk	O
on	O
,	O
we	O
identify	O
the	O
longest	O
common	O
prefix	O
of	O
the	O
best	O
hypothesis	O
of	O
the	O
n	O
consecutive	O
chunks	O
:	O
prefix	O
(	O
W	O
c	O
best	O
)	O
=	O
,	O
if	O
c	O
<	O
n	O
,	O
LCP	O
(	O
W	O
c−n+1	O
best	O
,	O
...	O
,	O
W	O
c	O
best	O
)	O
,	O
otherwise	O
,	O
(	O
2	O
)	O
where	O
LCP	O
(	O
)	O
is	O
longest	O
common	O
prefix	O
of	O
the	O
arguments	O
.	O
SP	O
-	O
n	O
Shared	O
prefix	O
(	O
Nguyen	O
et	O
al	O
,	O
2021	O
)	O
strategy	O
displays	O
the	O
longest	O
common	O
prefix	O
of	O
all	O
the	O
items	O
in	O
the	O
beam	O
of	O
a	O
chunk	O
.	O
Similarly	O
to	O
the	O
LA	O
-	O
n	O
strategy	O
,	O
we	O
propose	O
a	O
generalization	O
to	O
the	O
longest	O
common	O
prefix	O
of	O
all	O
items	O
in	O
the	O
beams	O
of	O
the	O
n	O
consecutive	O
chunks	O
:	O
prefix	O
(	O
W	O
c	O
all	O
)	O
=	O
,	O
if	O
c	O
<	O
n	O
,	O
LCP	O
(	O
W	O
c−n+1	O
beam	O
1	O
...	O
B	O
,	O
...	O
,	O
W	O
c	O
beam	O
1	O
...	O
B	O
)	O
,	O
otherwise	O
,	O
(	O
3	O
)	O
i.e.	O
,	O
all	O
beam	O
hypotheses	O
1	O
,	O
...	O
,	O
B	O
(	O
where	O
B	O
is	O
the	O
beam	O
size	O
)	O
of	O
all	O
chunks	O
c	O
−	O
n	O
+	O
1	O
,	O
...	O
,	O
c.	O

In	O
our	O
experiments	O
,	O
we	O
use	O
two	O
different	O
models	O
.	O
First	O
,	O
we	O
do	O
experiments	O
with	O
a	O
monolingual	O
Model	O
A	O
,	O
then	O
for	O
the	O
submission	O
,	O
we	O
use	O
a	O
multilingual	O
and	O
more	O
robust	O
Model	O
B.	O
4	O
Model	O
A	O
is	O
the	O
KIT	O
IWSLT	O
2020	O
model	O
for	O
the	O
Offline	O
Speech	O
Translation	B-TaskName
task	O
.	O
Specifically	O
,	O
it	O
is	O
an	O
end	O
-	O
to	O
-	O
end	O
English	O
to	O
German	O
Transformer	B-MethodName
model	O
with	O
relative	O
attention	O
.	O
For	O
more	O
described	O
description	O
,	O
refer	O
to	O
Pham	O
et	O
al	O
(	O
2020b	O
)	O
.	O

For	O
the	O
submission	O
,	O
we	O
use	O
a	O
multilingual	O
Model	O
B.	O
We	O
construct	O
the	O
SLT	O
architecture	O
with	O
the	O
encoder	O
based	O
on	O
the	O
wav2vec	O
2.0	O
(	O
Baevski	O
et	O
al	O
,	O
2020	O
)	O
and	O
the	O
decoder	O
based	O
on	O
the	O
autoregressive	O
language	O
model	O
pretrained	O
with	O
mBART50	O
(	O
Tang	O
et	O
al	O
,	O
2020	O
)	O
.	O
wav2vec	O
2.0	O
is	O
a	O
Transformer	B-MethodName
encoder	O
model	O
which	O
receives	O
raw	O
waveforms	O
as	O
input	O
and	O
generates	O
high	O
-	O
level	O
representations	O
.	O
The	O
architecture	O
consists	O
of	O
two	O
main	O
components	O
:	O
first	O
,	O
a	O
convolution	B-MethodName
-	O
based	O
feature	O
extractor	O
downsamples	O
long	O
audio	O
waveforms	O
into	O
features	O
that	O
have	O
similar	O
lengths	O
with	O
spectrograms	O
.	O
After	O
that	O
,	O
a	O
deep	O
Transformer	B-MethodName
encoder	O
uses	O
self	O
-	O
attention	O
and	O
feedforward	O
neural	O
network	O
blocks	O
to	O
transform	O
the	O
features	O
without	O
further	O
downsampling	O
.	O
During	O
the	O
self	O
-	O
supervised	O
training	O
process	O
,	O
the	O
network	O
is	O
trained	O
with	O
a	O
contrastive	B-MethodName
learning	I-MethodName
strategy	O
(	O
Baevski	O
et	O
al	O
,	O
2020	O
)	O
,	O
in	O
which	O
the	O
already	O
downsampled	O
features	O
are	O
randomly	O
masked	O
and	O
the	O
model	O
learns	O
to	O
predict	O
the	O
quantized	O
latent	O
representation	O
of	O
the	O
masked	O
time	O
step	O
.	O
During	O
the	O
supervised	O
learning	O
step	O
,	O
we	O
freeze	O
the	O
feature	O
extraction	O
weights	O
to	O
save	O
memory	O
since	O
the	O
first	O
layers	O
are	O
among	O
the	O
largest	O
ones	O
.	O
We	O
fine	O
-	O
tune	O
all	O
of	O
the	O
weights	O
in	O
the	O
Transformer	B-MethodName
encoder	O
.	O
Moreover	O
,	O
to	O
make	O
the	O
model	O
more	O
robust	O
to	O
the	O
fluctuation	O
in	O
absolute	O
positions	O
and	O
durations	O
when	O
it	O
comes	O
to	O
audio	O
signals	O
,	O
we	O
added	O
the	O
relative	B-MethodName
position	I-MethodName
encodings	I-MethodName
(	O
Dai	O
et	O
al	O
,	O
2019	O
;	O
Pham	O
et	O
al	O
,	O
2020a	O
)	O
to	O
alleviate	O
this	O
problem	O
.	O
5	O
Here	O
we	O
used	O
the	O
same	O
pretrained	O
model	O
with	O
the	O
speech	O
recognizer	O
,	O
with	O
the	O
large	O
architecture	O
pretrained	O
with	O
53k	O
hours	O
of	O
unlabeled	O
data	O
.	O
mBART50	O
is	O
an	O
encoder	O
-	O
decoder	O
Transformerbased	O
language	O
model	O
.	O
During	O
training	O
,	O
instead	O
of	O
the	O
typical	O
language	O
modeling	O
setting	O
of	O
predicting	O
the	O
next	O
word	O
in	O
the	O
sequence	O
,	O
this	O
model	O
is	O
trained	O
to	O
reconstruct	O
a	O
sequence	O
from	O
its	O
noisy	O
version	O
(	O
Lewis	O
et	O
al	O
,	O
2019	O
)	O
and	O
later	O
extended	O
to	O
a	O
multilingual	O
version	O
(	O
Liu	O
et	O
al	O
,	O
2020b	O
;	O
Tang	O
et	O
al	O
,	O
2020	O
)	O
in	O
which	O
the	O
corpora	O
from	O
multiple	O
languages	O
are	O
combined	O
during	O
training	O
.	O
mBART50	O
is	O
the	O
version	O
that	O
is	O
pretrained	O
on	O
50	O
languages	O
.	O
The	O
mBART50	O
model	O
follows	O
the	O
Transformer	B-MethodName
encoder	O
and	O
decoder	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
During	O
fine	O
-	O
tuning	O
,	O
we	O
combine	O
the	O
mBART50	O
decoder	O
with	O
the	O
wav2vec	O
2.0	O
encoder	O
,	O
where	O
both	O
encoder	O
and	O
decoder	O
know	O
one	O
modality	O
.	O
The	O
crossattention	O
layers	O
connecting	O
the	O
decoder	O
with	O
the	O
encoder	O
are	O
the	O
parts	O
that	O
require	O
extensive	O
finetuning	O
in	O
this	O
case	O
,	O
due	O
to	O
the	O
modality	O
mismatch	O
between	O
pretraining	O
and	O
fine	O
-	O
tuning	O
.	O
Finally	O
,	O
we	O
use	O
the	O
model	O
in	O
a	O
multilingual	O
setting	O
,	O
i.e.	O
,	O
for	O
English	O
to	O
Chinese	O
,	O
German	O
,	O
and	O
Japanese	O
language	O
pairs	O
by	O
training	O
on	O
the	O
combination	O
of	O
the	O
datasets	O
.	O
The	O
mBART50	O
vocabulary	O
contains	O
language	O
tokens	O
for	O
all	O
three	O
languages	O
and	O
can	O
be	O
used	O
to	O
control	O
the	O
language	O
output	O
.	O
For	O
more	O
details	O
on	O
the	O
model	O
refer	O
to	O
Pham	O
et	O
al	O
(	O
2022	O
)	O
.	O

For	O
the	O
onlinization	O
experiments	O
,	O
we	O
use	O
MuST	B-DatasetName
-	I-DatasetName
C	I-DatasetName
(	O
Cattoni	O
et	O
al	O
,	O
2021	O
)	O
tst	O
-	O
COMMON	O
from	O
the	O
v2.0	O
release	O
.	O
We	O
conduct	O
all	O
the	O
experiments	O
on	O
the	O
English	O
-	O
German	O
language	O
pair	O
.	O

In	O
this	O
paper	O
,	O
we	O
do	O
not	O
report	O
any	O
computationally	O
aware	O
metrics	O
,	O
as	O
our	O
implementation	O
of	O
Transformers	O
is	O
slow	O
.	O
Later	O
,	O
we	O
implemented	O
the	O
same	O
onlinization	O
approach	O
using	O
wav2vec	O
2.0	O
and	O
mBART	B-MethodName
from	O
Huggingface	O
Transformers	O
(	O
Wolf	O
et	O
al	O
,	O
2020	O
)	O
.	O
The	O
new	O
implementation	O
reaches	O
faster	O
than	O
realtime	O
inference	O
speed	O
.	O
A	O
)	O
and	O
the	O
submitted	O
system	O
(	O
Model	O
B	O
)	O
on	O
the	O
MuST	B-DatasetName
-	I-DatasetName
C	I-DatasetName
v2	O
tst	O
-	O
COMMON	O
.	O
We	O
also	O
include	O
the	O
best	O
IWSLT	O
2021	O
system	O
(	O
USTC	O
-	O
NELSLIP	O
(	O
Liu	O
et	O
al	O
,	O
2021	O
)	O
)	O
.	O

This	O
work	O
has	O
received	O
support	O
from	O
the	O
project	O
"	O
Grant	O
Schemes	O
at	O
CU	O
"	O
(	O
reg	O
.	O
no	O
.	O
CZ.02.2.69/0.0/0.0/19_073/0016935	O
)	O
,	O
the	O
grant	O
19	O
-	O
26934X	O
(	O
NEUREM3	O
)	O
of	O
the	O
Czech	O
Science	O
Foundation	O
,	O
the	O
European	O
Union	O
's	O
Horizon	O
2020	O
Research	O
and	O
Innovation	O
Programme	O
under	O
Grant	O
Agreement	O
No	O
825460	O
(	O
ELITR	O
)	O
,	O
and	O
partly	O
supported	O
by	O
a	O
Facebook	O
Sponsored	O
Research	O
Agreement	O
"	O
Language	O
Similarity	O
in	O
Machine	B-TaskName
Translation	I-TaskName
"	O
.	O

Neural	O
machine	B-TaskName
translation	I-TaskName
(	O
NMT	O
)	O
witnessed	O
a	O
lot	O
of	O
success	O
in	O
the	O
past	O
few	O
years	O
especially	O
for	O
high	O
resource	O
languages	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
Improving	O
the	O
quality	O
of	O
low	O
resource	O
languages	O
is	O
still	O
challenging	O
.	O
Some	O
of	O
the	O
popular	O
techniques	O
are	O
adding	O
high	O
resource	O
helper	O
languages	O
as	O
in	O
multilingual	O
neural	O
machine	B-TaskName
translation	I-TaskName
(	O
MNMT	O
)	O
(	O
Dong	O
et	O
al	O
,	O
2015	O
;	O
Firat	O
et	O
al	O
,	O
2016	O
;	O
Ha	O
et	O
al	O
,	O
2016	O
;	O
Johnson	O
et	O
al	O
,	O
2017	O
;	O
Arivazhagan	O
et	O
al	O
,	O
2019	O
)	O
,	O
using	O
monolingual	O
data	O
including	O
pre	O
-	O
training	O
(	O
Liu	O
et	O
al	O
,	O
2020	O
)	O
,	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
(	O
Wang	O
et	O
al	O
,	O
2020	O
)	O
,	O
back	O
translation	O
(	O
Sennrich	O
et	O
al	O
,	O
2016	O
)	O
or	O
any	O
combination	O
of	O
these	O
methods	O
(	O
Barrault	O
et	O
al	O
,	O
2020	O
)	O
and	O
system	O
combination	O
of	O
multiple	O
systems	O
(	O
Liu	O
et	O
al	O
,	O
2018	O
)	O
.	O
This	O
paper	O
describes	O
the	O
Microsoft	O
Egypt	O
Development	O
Center	O
(	O
EgDC	O
)	O
submission	O
to	O
the	O
WMT21	O
shared	O
news	O
translation	O
task	O
for	O
three	O
low	O
resource	O
language	O
pairs	O
(	O
six	O
directions	O
)	O
,	O
Bengali	O
↔	O
Hindi	O
(	O
Bn	O
↔	O
Hi	O
)	O
,	O
English	O
↔	O
Hausa	O
(	O
En	O
↔	O
Ha	O
)	O
and	O
Xhosa	O
↔	O
Zulu	O
(	O
Xh	O
↔	O
Zu	O
)	O
.	O
We	O
focus	O
on	O
the	O
constrained	O
track	O
because	O
it	O
is	O
easier	O
to	O
compare	O
different	O
systems	O
and	O
it	O
is	O
always	O
possible	O
to	O
improve	O
performance	O
by	O
adding	O
more	O
data	O
.	O
The	O
main	O
features	O
of	O
our	O
approach	O
are	O
as	O
follows	O
:	O
Using	O
a	O
recently	O
proposed	O
multitask	O
and	O
multilingual	O
learning	O
framework	O
to	O
benefit	O
from	O
monolingual	O
data	O
in	O
both	O
the	O
source	O
and	O
target	O
languages	O
(	O
Wang	O
et	O
al	O
,	O
2020	O
)	O
.	O
Using	O
knowledge	B-MethodName
distillation	I-MethodName
(	O
Freitag	O
et	O
al	O
,	O
2017	O
)	O
to	O
create	O
bilingual	O
baselines	O
from	O
the	O
original	O
multilingual	O
model	O
and	O
combining	O
it	O
with	O
the	O
multilingual	O
model	O
.	O
The	O
paper	O
is	O
organized	O
as	O
follows	O
.	O
Section	O
2	O
gives	O
an	O
overview	O
of	O
the	O
data	O
used	O
in	O
the	O
constrained	O
scenario	O
,	O
followed	O
by	O
section	O
3	O
that	O
gives	O
a	O
detailed	O
description	O
of	O
our	O
approach	O
.	O
Section	O
4	O
presents	O
our	O
experimental	O
evaluation	O
.	O
Finally	O
,	O
our	O
findings	O
are	O
summarized	O
in	O
Section	O
5	O
.	O

For	O
Bengali	O
,	O
English	O
,	O
Hindi	O
and	O
German	O
,	O
we	O
apply	O
fastText	B-MethodName
1	O
language	B-TaskName
identification	I-TaskName
on	O
the	O
monolingual	O
data	O
to	O
remove	O
sentences	O
which	O
are	O
not	O
predicted	O
as	O
the	O
expected	O
language	O
.	O
We	O
do	O
the	O
same	O
for	O
Hausa	O
,	O
Xhosa	O
and	O
Zulu	O
using	O
Polyglot	O
2	O
because	O
fastText	B-MethodName
does	O
not	O
cover	O
these	O
three	O
languages	O
.	O
The	O
resulting	O
size	O
of	O
the	O
monolingual	O
data	O
of	O
each	O
language	O
is	O
shown	O
in	O
Table	O
2	O
.	O

This	O
subsection	O
describes	O
the	O
individual	O
systems	O
and	O
their	O
training	O
leading	O
to	O
the	O
proposed	O
system	O
combination	O
strategy	O
in	O
the	O
following	O
subsection	O
.	O
We	O
first	O
build	O
bilingual	O
models	O
for	O
the	O
six	O
primary	O
directions	O
using	O
the	O
data	O
shown	O
in	O
Table	O
1	O
except	O
the	O
English	O
↔	O
German	O
.	O
These	O
serve	O
as	O
baselines	O
to	O
compare	O
to	O
the	O
developed	O
systems	O
.	O
The	O
models	O
use	O
a	O
transformer	O
base	O
architecture	O
comprising	O
6	O
encoder	O
and	O
6	O
decoder	O
layers	O
and	O
a	O
24	O
K	O
joint	O
vocabulary	O
built	O
for	O
Bengali	O
↔	O
Hindi	O
,	O
a	O
8	O
K	O
joint	O
vocabulary	O
built	O
for	O
English	O
↔	O
Hausa	O
and	O
a	O
4	O
K	O
joint	O
vocabulary	O
built	O
for	O
Xhosa	O
↔	O
Zulu	O
using	O
sentencepiece	B-MethodName
(	O
Kudo	O
and	O
Richardson	O
,	O
2018	O
)	O
to	O
learn	O
these	O
subword	O
units	O
to	O
tokenize	O
the	O
sentences	O
.	O
In	O
addition	O
to	O
the	O
baseline	O
bilingual	O
models	O
,	O
we	O
use	O
knowledge	O
distilled	O
(	O
KD	O
)	O
data	O
and	O
back	O
-	O
translated	O
(	O
BT	O
)	O
data	O
generated	O
from	O
a	O
multilingual	O
model	O
to	O
build	O
another	O
set	O
of	O
bilingual	O
models	O
for	O
each	O
of	O
the	O
six	O
primary	O
directions	O
.	O
This	O
multilingual	O
model	O
is	O
described	O
below	O
.	O
The	O
purpose	O
of	O
these	O
models	O
is	O
to	O
participate	O
in	O
the	O
ensemble	O
along	O
with	O
the	O
multilingual	O
models	O
.	O
The	O
latter	O
bilingual	O
models	O
follow	O
the	O
same	O
transformer	O
base	O
architecture	O
and	O
joint	O
vocabulary	O
used	O
in	O
the	O
baseline	O
bilingual	O
models	O
.	O
The	O
multilingual	O
model	O
combines	O
the	O
8	O
translation	O
directions	O
shown	O
in	O
Table	O
1	O
.	O
These	O
are	O
the	O
six	O
primary	O
directions	O
plus	O
English	O
↔	O
German	O
as	O
a	O
helper	O
.	O
The	O
latter	O
is	O
mainly	O
used	O
to	O
improve	O
generation	O
on	O
the	O
English	O
centric	O
directions	O
.	O
The	O
model	O
uses	O
a	O
64	O
K	O
joint	O
vocabulary	O
constructed	O
using	O
sentencepiece	B-MethodName
(	O
Kudo	O
and	O
Richardson	O
,	O
2018	O
)	O
from	O
a	O
subset	O
of	O
the	O
monolingual	O
data	O
of	O
each	O
language	O
as	O
described	O
in	O
Section	O
2	O
.	O
The	O
transformer	O
model	O
has	O
12	O
encoder	O
and	O
6	O
decoder	O
layers	O
.	O
In	O
addition	O
,	O
a	O
multitask	O
objective	O
is	O
used	O
during	O
training	O
to	O
make	O
use	O
of	O
monolingual	O
data	O
.	O
The	O
objective	O
comprises	O
the	O
usual	O
parallel	O
data	O
likelihood	O
referred	O
to	O
as	O
MT	O
,	O
a	O
masked	O
language	O
model	O
(	O
MLM	B-DatasetName
)	O
at	O
the	O
encoder	O
and	O
a	O
denoising	B-TaskName
auto	O
-	O
encoder	O
(	O
DAE	O
)	O
(	O
similar	O
to	O
mBART	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2020	O
)	O
)	O
at	O
the	O
decoder	O
side	O
.	O
The	O
latter	O
two	O
objectives	O
help	O
leverage	O
monolingual	O
data	O
for	O
both	O
the	O
encoder	O
and	O
the	O
decoder	O
sides	O
.	O
The	O
three	O
objectives	O
are	O
combined	O
using	O
different	O
proportions	O
according	O
to	O
a	O
schedule	O
during	O
the	O
training	O
.	O
Please	O
refer	O
to	O
(	O
Wang	O
et	O
al	O
,	O
2020	O
)	O
for	O
details	O
.	O
To	O
summarize	O
we	O
build	O
the	O
following	O
models	O
:	O
Bilingual	O
models	O
trained	O
using	O
parallel	O
data	O
in	O
Table	O
1	O
for	O
the	O
6	O
primary	O
directions	O
.	O
These	O
are	O
mainly	O
used	O
as	O
baselines	O
.	O
Multilingual	O
models	O
trained	O
using	O
a	O
multitask	O
objective	O
using	O
parallel	O
and	O
monolingual	O
data	O
and	O
comprising	O
8	O
directions	O
.	O
Bilingual	O
models	O
trained	O
using	O
KD	O
and	O
BT	O
data	O
generated	O
using	O
our	O
best	O
multilingual	O
model	O
.	O
These	O
are	O
combined	O
with	O
the	O
best	O
multilingual	O
model	O
as	O
described	O
in	O
3.2	O
.	O

Knowledge	O
-	O
grounded	O
generation	O
models	O
that	O
utilize	O
retrieved	O
results	O
(	O
e.g.	O
,	O
relevant	O
documents	O
from	O
Wikipedia	O
)	O
to	O
generate	O
informative	O
responses	O
have	O
been	O
proposed	O
to	O
perform	O
knowledge	O
-	O
intensive	O
NLP	O
tasks	O
(	O
e.g.	O
,	O
open	B-TaskName
-	I-TaskName
domain	I-TaskName
question	I-TaskName
answering	I-TaskName
)	O
.	O
The	O
knowledge	O
-	O
grounded	O
generation	O
has	O
a	O
similar	O
form	O
with	O
the	O
exemplar	O
-	O
based	O
generation	O
.	O
However	O
,	O
the	O
main	O
difference	O
is	O
that	O
knowledgegrounded	O
generative	O
models	O
extract	O
the	O
knowledge	O
from	O
external	O
resources	O
to	O
generate	O
the	O
informative	O
response	O
.	O
Guu	O
et	O
al	O
(	O
2020	O
)	O
show	O
the	O
effectiveness	O
of	O
pre	O
-	O
training	O
a	O
knowledge	O
retriever	O
with	O
the	O
largescale	O
language	O
model	O
for	O
open	B-TaskName
-	I-TaskName
domain	I-TaskName
question	I-TaskName
answering	I-TaskName
,	O
and	O
Lewis	O
et	O
al	O
(	O
2020	O
)	O
demonstrate	O
that	O
knowledge	O
-	O
grounded	O
generative	O
models	O
produce	O
more	O
informative	O
and	O
diverse	O
sentences	O
than	O
vanilla	O
generative	O
models	O
on	O
a	O
wide	O
range	O
of	O
knowledgeintensive	O
NLP	O
tasks	O
.	O
Fan	O
et	O
al	O
(	O
2021	O
)	O
similarly	O
propose	O
a	O
knowledge	O
-	O
grounded	O
generative	O
model	O
for	O
response	B-TaskName
generation	I-TaskName
,	O
but	O
they	O
do	O
not	O
focus	O
on	O
the	O
open	O
-	O
domain	O
conversation	O
.	O
In	O
Method	O
Section	O
,	O
we	O
demonstrate	O
the	O
difference	O
between	O
our	O
approach	O
and	O
knowledge	O
-	O
grounded	O
generative	O
models	O
,	O
and	O
we	O
show	O
that	O
existing	O
knowledge	O
-	O
grounded	O
generative	O
models	O
are	O
not	O
directly	O
applicable	O
to	O
the	O
open	O
-	O
domain	O
conversation	O
in	O
Experiments	O
Section	O
.	O

As	O
mentioned	O
in	O
Roller	O
et	O
al	O
(	O
2021	O
)	O
,	O
the	O
primitive	O
exemplar	O
-	O
based	O
generative	O
model	O
tends	O
to	O
ignore	O
the	O
retrieved	O
exemplar	O
dur	O
-	O
ing	O
response	B-TaskName
generation	I-TaskName
due	O
to	O
the	O
one	O
-	O
to	O
-	O
many	O
problem	O
in	O
open	O
-	O
domain	O
conversation	O
(	O
Li	O
et	O
al	O
,	O
2016	O
)	O
.	O
Since	O
its	O
retriever	O
searches	O
an	O
exemplar	O
based	O
on	O
a	O
given	O
context	O
,	O
the	O
retrieved	O
exemplar	O
is	O
often	O
significantly	O
different	O
from	O
a	O
gold	O
response	O
of	O
the	O
generator	O
,	O
although	O
both	O
of	O
the	O
retrieved	O
exemplar	O
and	O
gold	O
response	O
are	O
relevant	O
to	O
the	O
given	O
context	O
,	O
which	O
is	O
shown	O
in	O
Figure	O
2	O
(	O
a	O
)	O
.	O
As	O
the	O
retrieved	O
exemplar	O
is	O
not	O
helpful	O
for	O
generating	O
the	O
gold	O
response	O
,	O
the	O
generator	O
is	O
trained	O
to	O
ignore	O
the	O
retrieved	O
exemplar	O
and	O
to	O
produce	O
a	O
response	O
using	O
only	O
the	O
given	O
context	O
.	O
To	O
induce	O
the	O
generator	O
to	O
utilize	O
retrieved	O
exemplars	O
more	O
actively	O
,	O
Roller	O
et	O
al	O
(	O
2021	O
)	O
make	O
use	O
of	O
the	O
gold	O
response	O
,	O
and	O
Cai	O
et	O
al	O
(	O
2019b	O
)	O
use	O
perturbed	O
gold	O
response	O
as	O
an	O
exemplar	O
rather	O
than	O
using	O
retrieved	O
exemplars	O
during	O
the	O
model	O
training	O
.	O
However	O
,	O
since	O
the	O
exemplar	O
z	O
i	O
and	O
the	O
gold	O
response	O
r	O
i	O
are	O
too	O
similar	O
(	O
as	O
shown	O
in	O
Figure	O
2	O
(	O
b	O
)	O
)	O
,	O
the	O
exemplar	O
-	O
based	O
generative	O
model	O
learns	O
to	O
rely	O
overly	O
on	O
the	O
exemplar	O
.	O
Eventually	O
,	O
the	O
generator	O
produces	O
a	O
highly	O
over	O
-	O
fitted	O
response	O
to	O
the	O
exemplar	O
by	O
directly	O
copying	O
the	O
tokens	O
of	O
the	O
exemplar	O
.	O

Relevant	O
but	O
Lexically	O
Distanced	O
to	O
the	O
Gold	O
Response	O
We	O
describe	O
how	O
CORGE	O
selects	O
semantically	O
relevant	O
but	O
lexically	O
distanced	O
exemplars	O
to	O
the	O
gold	O
response	O
.	O
Conventionally	O
,	O
the	O
retriever	O
selects	O
the	O
exemplars	O
z	O
based	O
on	O
the	O
relevance	O
score	O
S	O
R	O
(	O
z	O
,	O
c	O
i	O
)	O
for	O
the	O
given	O
context	O
c	O
i	O
.	O
However	O
,	O
this	O
searching	O
process	O
could	O
return	O
a	O
significantly	O
different	O
exemplar	O
z	O
from	O
the	O
gold	O
response	O
r	O
i	O
,	O
and	O
it	O
induces	O
the	O
generator	O
G	O
to	O
ignore	O
the	O
retrieved	O
exemplar	O
during	O
response	B-TaskName
generation	I-TaskName
.	O
Therefore	O
,	O
we	O
select	O
exemplars	O
based	O
on	O
the	O
gold	O
response	O
r	O
i	O
to	O
ensure	O
that	O
the	O
generator	O
G	O
utilizes	O
the	O
exemplars	O
inspired	O
by	O
Wu	O
et	O
al	O
.	O
We	O
select	O
top	O
-	O
k	O
scoring	O
exemplars	O
based	O
on	O
the	O
score	O
S	O
R	O
′	O
(	O
z	O
,	O
r	O
i	O
)	O
,	O
which	O
we	O
call	O
k	O
-	O
Nearest	O
Exemplars	O
(	O
kNE	O
)	O
.	O
1	O
These	O
kNE	O
are	O
more	O
semantically	O
related	O
to	O
the	O
gold	O
response	O
r	O
i	O
than	O
the	O
exemplar	O
obtained	O
by	O
using	O
S	O
R	O
(	O
z	O
,	O
c	O
i	O
)	O
.	O
However	O
,	O
some	O
of	O
the	O
selected	O
kNE	O
are	O
lexically	O
identical	O
or	O
too	O
close	O
to	O
the	O
gold	O
response	O
r	O
unintentionally	O
since	O
the	O
retriever	O
searches	O
the	O
exemplars	O
based	O
on	O
the	O
gold	O
response	O
.	O
We	O
observe	O
that	O
using	O
these	O
exemplars	O
also	O
causes	O
the	O
overfitting	O
problem	O
of	O
generated	O
responses	O
;	O
therefore	O
,	O
the	O
generator	O
excessively	O
copies	O
tokens	O
from	O
the	O
exemplars	O
.	O
From	O
this	O
,	O
we	O
are	O
motivated	O
to	O
filter	O
out	O
the	O
exemplars	O
which	O
are	O
lexically	O
too	O
close	O
to	O
the	O
gold	O
response	O
and	O
preserve	O
the	O
exemplars	O
properly	O
distanced	O
to	O
the	O
gold	O
response	O
to	O
mitigate	O
the	O
over	O
-	O
fitting	O
problem	O
.	O
Here	O
,	O
we	O
employ	O
Jaccard	O
similarity	O
to	O
measure	O
the	O
lexical	O
similarity	O
(	O
Guu	O
et	O
al	O
,	O
2018	O
;	O
Cai	O
et	O
al	O
,	O
2019a	O
;	O
Wu	O
et	O
al	O
,	O
2019	O
)	O
between	O
the	O
exemplar	O
and	O
the	O
gold	O
response	O
.	O
Exemplars	O
are	O
filtered	O
out	O
when	O
their	O
Jaccard	O
distance	O
with	O
the	O
gold	O
response	O
r	O
is	O
larger	O
than	O
0.6	O
,	O
and	O
we	O
replace	O
them	O
with	O
the	O
randomly	O
chosen	O
responses	O
from	O
the	O
pre	O
-	O
defined	O
response	O
set	O
R.	O
The	O
threshold	O
of	O
filtering	O
is	O
empirically	O
chosen	O
as	O
0.6	O
.	O
The	O
set	O
of	O
the	O
final	O
exemplars	O
z	O
obtained	O
through	O
these	O
steps	O
is	O
referred	O
to	O
as	O
Z	O
i	O
=	O
{	O
z	O
i	O
,	O
1	O
,	O
z	O
i	O
,	O
2	O
,	O
,	O
z	O
i	O
,	O
k	O
}	O
.	O

We	O
utilize	O
the	O
following	O
four	O
datasets	O
used	O
in	O
Roller	O
et	O
al	O
(	O
2021	O
)	O
,	O
which	O
are	O
Blended	B-DatasetName
Skill	I-DatasetName
Talk	I-DatasetName
(	O
BST	O
)	O
(	O
Smith	O
et	O
al	O
,	O
2020	O
)	O
,	O
ConvAI2	B-DatasetName
(	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
,	O
Empathetic	O
Dialogues	O
(	O
ED	O
)	O
(	O
Rashkin	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
Wizard	B-DatasetName
of	I-DatasetName
Wikipedia	I-DatasetName
(	O
WoW	O
)	O
.	O
To	O
simplify	O
the	O
notation	O
,	O
we	O
denote	O
the	O
concatenated	O
version	O
of	O
these	O
four	O
datasets	O
as	O
BST+	O
.	O
We	O
split	O
BST+	O
into	O
train	O
,	O
validation	O
,	O
and	O
test	O
sets	O
following	O
Smith	O
et	O
al	O
(	O
2020	O
)	O
.	O

Our	O
retriever	O
follows	O
the	O
architecture	O
of	O
Biencoder	O
(	O
Mazare	O
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
the	O
score	O
S	O
R	O
(	O
z	O
,	O
c	O
)	O
and	O
S	O
R	O
′	O
(	O
z	O
,	O
r	O
)	O
are	O
calculated	O
as	O
follows	O
:	O
S	O
R	O
(	O
z	O
,	O
c	O
)	O
=	O
d	O
(	O
z	O
)	O
q	O
(	O
c	O
)	O
,	O
S	O
R	O
′	O
(	O
z	O
,	O
r	O
)	O
=	O
d	O
(	O
z	O
)	O
d	O
(	O
r	O
)	O
,	O
d	O
(	O
z	O
)	O
=	O
BERT	B-MethodName
r	O
(	O
z	O
)	O
,	O
d	O
(	O
r	O
)	O
=	O
BERT	B-MethodName
r	O
(	O
r	O
)	O
,	O
q	O
(	O
c	O
)	O
=	O
BERT	B-MethodName
c	O
(	O
c	O
)	O
,	O
(	O
3	O
)	O
where	O
d	O
(	O
z	O
)	O
and	O
d	O
(	O
r	O
)	O
are	O
encoded	O
vectors	O
produced	O
by	O
response	O
encoder	O
BERT	B-MethodName
r	O
and	O
q	O
(	O
c	O
)	O
is	O
an	O
encoded	O
vector	O
produced	O
by	O
context	O
encoder	O
BERT	B-MethodName
c	O
.	O
The	O
notation	O
R	O
′	O
indicates	O
that	O
it	O
only	O
uses	O
the	O
response	O
encoder	O
instead	O
of	O
using	O
the	O
context	O
encoder	O
together	O
.	O
CORGE	O
is	O
not	O
limited	O
to	O
use	O
Bi	O
-	O
encoder	O
as	O
a	O
retriever	O
and	O
can	O
be	O
applied	O
to	O
other	O
types	O
of	O
a	O
retriever	O
(	O
e.g.	O
Poly	O
-	O
encoder	O
(	O
Humeau	O
et	O
al	O
,	O
2019	O
)	O
)	O
.	O

As	O
we	O
mentioned	O
in	O
Section	O
5.2	O
,	O
we	O
employ	O
Biencoder	O
256	O
M	O
and	O
Blender	B-MethodName
90	O
M	O
as	O
a	O
retriever	O
and	O
a	O
generator	O
of	O
each	O
exemplar	O
-	O
based	O
generative	O
model	O
,	O
respectively	O
.	O
For	O
MatToGen	O
,	O
additional	O
MLP	B-DatasetName
layers	O
are	O
added	O
to	O
the	O
retriever	O
,	O
as	O
follows	O
the	O
details	O
in	O
Cai	O
et	O
al	O
(	O
2019b	O
)	O
.	O
When	O
training	O
the	O
models	O
,	O
weights	O
of	O
the	O
retriever	O
and	O
the	O
generator	O
are	O
initialized	O
with	O
the	O
pre	O
-	O
trained	O
Bi	O
-	O
encoder	O
256	O
M	O
and	O
Blender	B-MethodName
90	O
M	O
,	O
respectively	O
,	O
For	O
Blender	B-MethodName
90	O
M	O
,	O
we	O
use	O
the	O
model	O
released	O
by	O
ParlAI	O
(	O
Miller	O
et	O
al	O
,	O
2017	O
)	O

When	O
we	O
generate	O
samples	O
using	O
generative	O
model	O
,	O
exemplar	O
-	O
based	O
generative	O
models	O
,	O
and	O
knowledgegrounded	O
generative	O
models	O
,	O
we	O
adopt	O
a	O
beam	O
decoding	O
strategy	O
which	O
is	O
widely	O
used	O
in	O
generative	O
models	O
(	O
Graves	O
,	O
2012	O
)	O
.	O
Following	O
(	O
Roller	O
et	O
al	O
,	O
2021	O
)	O
,	O
we	O
choose	O
a	O
minimum	O
beam	O
length	O
and	O
a	O
beam	O
size	O
as	O
20	O
BPE	B-MethodName
tokens	O
and	O
10	O
,	O
respectively	O
,	O
and	O
use	O
tri	O
-	O
gram	O
beam	O
blocking	O
on	O
context	O
and	O
response	O
blocks	O
.	O
During	O
the	O
inference	O
phase	O
,	O
both	O
exemplar	O
-	O
based	O
generative	O
models	O
and	O
knowledgegrounded	O
generative	O
models	O
use	O
the	O
top	O
-	O
1	O
scoring	O
candidate	O
as	O
an	O
exemplar	O
chosen	O
from	O
utilizing	O
the	O
relevance	O
score	O
S	O
R	O
(	O
z	O
,	O
c	O
)	O
.	O

We	O
measure	O
how	O
much	O
time	O
spend	O
when	O
the	O
model	O
generates	O
the	O
responses	O
.	O
When	O
generating	O
the	O
response	O
,	O
Blender	B-MethodName
90	O
M	O
takes	O
0.481	O
seconds	O
,	O
and	O
Ret	O
-	O
NRef	O
+	O
CORGE	O
takes	O
0.523	O
seconds	O
per	O
instance	O
.	O
There	O
is	O
only	O
an	O
8.7	O
%	O
amount	O
of	O
inference	O
time	O
gap	O
between	O
Blender	B-MethodName
90	O
M	O
and	O
RetNRef	O
+	O
CORGE	O
.	O
This	O
tells	O
us	O
that	O
exemplar	O
-	O
based	O
generation	O
can	O
significantly	O
improve	O
the	O
quality	O
of	O
responses	O
regarding	O
appropriateness	O
,	O
informativeness	O
,	O
and	O
diversity	O
without	O
increasing	O
the	O
amount	O
of	O
time	O
to	O
generate	O
answers	O
.	O
We	O
test	O
our	O
model	O
on	O
NVIDIA	O
DGX	O
Station	O
A100	O
with	O
PyTorch	O
1.7.1	O
,	O
CUDA	O
11.0	O
,	O
CuDNN	O
8.0	O
,	O
and	O
here	O
we	O
adopt	O
the	O
generation	O
strategy	O
we	O
describe	O
above	O
.	O
When	O
we	O
measure	O
the	O
inference	O
time	O
,	O
we	O
only	O
use	O
a	O
single	O
GPU	O
(	O
NVIDIA	O
A100	O
GPU	O
,	O
40	O
GB	O
Memory	O
)	O
,	O
and	O
the	O
inference	O
time	O
is	O
measured	O
as	O
the	O
average	O
inference	O
time	O
of	O
100	O
response	O
generations	O
.	O

Any	O
friends	O
?	O
My	O
dog	O
is	O
one	O
of	O
mine	O
.	O
Blender	B-MethodName
90	O
M	O
That	O
's	O
good	O
to	O
hear	O
,	O
I	O
'	O
ve	O
a	O
dog	O
too	O
,	O
he	O
'	O
s	O
my	O
best	O
friend	O
.	O

Are	O
you	O
doing	O
anything	O
cool	O
for	O
halloween	O
RAG	B-MethodName
No	O
,	O
I	O
don	O
'	O
t	O
think	O
I	O
'	O
m	O
going	O
to	O
.	O
I	O
'	O
ve	O
never	O
been	O
a	O
fan	O
of	O
halloween	O
.	O

Understanding	O
procedural	O
language	O
requires	O
reasoning	O
about	O
both	O
hierarchical	O
and	O
temporal	O
relations	O
between	O
events	O
.	O
For	O
example	O
,	O
"	O
boiling	O
pasta	O
"	O
is	O
a	O
sub	O
-	O
event	O
of	O
"	O
making	O
a	O
pasta	O
dish	O
"	O
,	O
typically	O
happens	O
before	O
"	O
draining	O
pasta	O
,	O
"	O
and	O
requires	O
the	O
use	O
of	O
omitted	O
tools	O
(	O
e.g.	O
a	O
strainer	O
,	O
sink	O
...	O
)	O
.	O
While	O
people	O
are	O
able	O
to	O
choose	O
when	O
and	O
how	O
to	O
use	O
abstract	O
versus	O
concrete	O
instructions	O
,	O
the	O
NLP	O
community	O
lacks	O
corpora	O
and	O
tasks	O
for	O
evaluating	O
if	O
our	O
models	O
can	O
do	O
the	O
same	O
.	O
In	O
this	O
paper	O
,	O
we	O
introduce	O
KIDSCOOK	O
,	O
a	O
parallel	O
script	O
corpus	O
,	O
as	O
well	O
as	O
a	O
cloze	O
task	O
which	O
matches	O
video	O
captions	O
with	O
missing	O
procedural	O
details	O
.	O
Experimental	O
results	O
show	O
that	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
struggle	O
at	O
this	O
task	O
,	O
which	O
requires	O
inducing	O
functional	O
commonsense	O
knowledge	O
not	O
explicitly	O
stated	O
in	O
text	O
.	O
*	O
Author	O
now	O
at	O
Google	B-DatasetName
.	O
Work	O
done	O
while	O
unaffiliated	O
.	O
1	O
.	O
Take	O
the	O
strainer	O
with	O
the	O
pasta	O
and	O
pour	O
the	O
pasta	O
into	O
the	O
sauce	O
.	O
2	O
.	O
Stir	O
the	O
pasta	O
into	O
sauce	O
while	O
it	O
is	O
in	O
the	O
pan	O
.	O
3	O
.	O
Let	O
the	O
pasta	O
and	O
sauce	O
simmer	O
for	O
a	O
few	O
minutes	O
.	O

The	O
level	O
of	O
detail	O
used	O
in	O
natural	O
language	O
communication	O
varies	O
:	O
descriptive	O
or	O
instructive	O
text	O
for	O
experts	O
may	O
elide	O
over	O
details	O
the	O
reader	O
can	O
seamlessly	O
infer	O
,	O
while	O
text	O
for	O
more	O
novice	O
audiences	O
may	O
be	O
more	O
verbose	O
.	O
A	O
given	O
document	O
typically	O
adheres	O
to	O
a	O
single	O
level	O
of	O
verbosity	O
suited	O
to	O
its	O
presumed	O
audience	O
(	O
Grice	O
,	O
1975	O
)	O
,	O
so	O
learning	O
correspondences	O
between	O
abstract	O
and	O
detailed	O
descriptions	O
of	O
similar	O
concepts	O
from	O
text	O
is	O
a	O
challenging	O
problem	O
.	O
Commonsense	O
knowledge	O
of	O
how	O
complex	O
events	O
decompose	O
into	O
stereotypical	O
sequences	O
of	O
simpler	O
events	O
is	O
a	O
necessary	O
component	O
of	O
a	O
system	O
that	O
can	O
automatically	O
understand	O
and	O
reason	O
about	O
different	O
types	O
of	O
discourse	O
.	O
Hierarchical	O
correspondences	O
between	O
abstract	O
and	O
detailed	O
representations	O
of	O
concepts	O
and	O
events	O
were	O
an	O
important	O
aspect	O
of	O
the	O
original	O
formulation	O
of	O
scripts	O
for	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
(	O
Schank	O
and	O
1	O
.	O
Put	O
a	O
large	O
pot	O
half	O
full	O
of	O
water	O
on	O
the	O
stove	O
.	O
2	O
.	O
Turn	O
the	O
heat	O
on	O
under	O
the	O
pot	O
and	O
wait	O
for	O
the	O
water	O
to	O
boil	O
hard	O
.	O
3	O
.	O
Pour	O
the	O
pasta	O
into	O
the	O
boiling	O
water	O
.	O
Cook	O
the	O
pasta	O
4	O
.	O
Pick	O
up	O
the	O
strainer	O
and	O
shake	O
it	O
a	O
little	O
bit	O
so	O
more	O
water	O
comes	O
out	O
.	O

<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
F	O
V	O
a	O
Q	O
d	O
j	O
U	O
W	O
Z	O
V	O
L	O
y	O
U	O
U	O
a	O
l	O
b	O
b	O
T	O
R	O
1	O
N	O
M	O
D	O
o	O
K	O
M	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
o	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
0	B-DatasetName
G	O
P	O
R	O
i	O
8	O
e	O
K	O
9	O
g	O
P	O
a	O
U	O
D	O
b	O
b	O
T	O
b	O
t	O
0	B-DatasetName
s	O
w	O
m	O
7	O
E	O
6	O
G	O
E	O
/	O
g	O
Q	O
v	O
H	O
h	O
T	O
x	O
6	O
i	O
/	O
y	O
5	O
r	O
9	O
x	O
2	O
+	O
a	O
g	O
r	O
Q	O
8	O
G	O
H	O
u	O
/	O
N	O
M	O
D	O
M	O
v	O
T	O
K	O
U	O
w	O
6	O
H	O
n	O
f	O
T	O
m	O
l	O
t	O
f	O
W	O
N	O
z	O
q	O
7	O
x	O
d	O
2	O
d	O
n	O
d	O
2	O
z	O
9	O
w	O
D	O
4	O
9	O
a	O
J	O
s	O
k	O
0	B-DatasetName
4	O
0	B-DatasetName
2	O
W	O
y	O
E	O
R	O
3	O
Q	O
m	O
q	O
4	O
F	O
I	O
o	O
3	O
U	O
a	O
D	O
k	O
n	O
V	O
R	O
z	O
G	O
o	O
e	O
S	O
t	O
8	O
P	O
x	O
7	O
c	O
x	O
v	O
P	O
3	O
F	O
t	O
R	O
K	O
I	O
e	O
c	O
Z	O
L	O
y	O
I	O
K	O
Z	O
D	O
J	O
S	O
L	O
B	O
K	O
F	O
r	O
p	O
A	O
f	O
t	O
+	O
3	O
6	O
1	O
6	O
N	O
W	O
8	O
O	O
s	O
k	O
r	O
8	O
g	O
l	O
S	O
h	O
Q	O
K	O
P	O
v	O
f	O
v	O
U	O
G	O
C	O
c	O
t	O
i	O
r	O
p	O
B	O
J	O
a	O
k	O
z	O
X	O
9	O
1	O
I	O
M	O
c	O
q	O
p	O
R	O
M	O
M	O
m	O
n	O
l	O
V	O
5	O
m	O
e	O
E	O
r	O
Z	O
m	O
A	O
5	O
5	O
1	O
1	O
J	O
F	O
Y	O
2	O
6	O
C	O
f	O
H	O
7	O
q	O
l	O
J	O
x	O
Z	O
Z	O
U	O
C	O
i	O
R	O
N	O
t	O
S	O
S	O
O	O
b	O
q	O
7	O
4	O
m	O
c	O
x	O
s	O
Z	O
M	O
4	O
t	O
B	O
2	O
x	O
h	O
R	O
H	O
Z	O
t	O
m	O
b	O
i	O
f	O
9	O
5	O
3	O
Q	O
y	O
j	O
6	O
y	O
A	O
X	O
K	O
s	O
2	O
Q	O
K	O
7	O
Z	O
Y	O
F	O
G	O
W	O
S	O
Y	O
E	O
J	O
m	O
f	O
5	O
O	O
B	O
0	B-DatasetName
J	O
y	O
h	O
n	O
F	O
h	O
C	O
m	O
R	O
b	O
2	O
V	O
s	O
J	O
G	O
V	O
F	O
O	O
G	O
N	O
p	O
2	O
K	O
D	O
c	O
F	O
f	O
f	O
n	O
m	O
V	O
t	O
C	O
5	O
q	O
v	O
l	O
f	O
z	O
7	O
y	O
+	O
r	O
9	O
Z	O
s	O
i	O
j	O
j	O
K	O
c	O
w	O
C	O
m	O
c	O
g	O
w	O
9	O
X	O
U	O
I	O
c	O
7	O
a	O
E	O
A	O
T	O
G	O
A	O
z	O
h	O
G	O
V	O
7	O
h	O
z	O
Z	O
H	O
O	O
i	O
/	O
P	O
u	O
f	O
C	O
x	O
a	O
S	O
0	B-DatasetName
4	O
x	O
c	O
w	O
x	O
/	O
4	O
H	O
z	O
+	O
A	O
A	O
W	O
Y	O
j	O
Z	O
w	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
F	O
V	O
a	O
Q	O
d	O
j	O
U	O
W	O
Z	O
V	O
L	O
y	O
U	O
U	O
a	O
l	O
b	O
b	O
T	O
R	O
1	O
N	O
M	O
D	O
o	O
K	O
M	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
o	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
0	B-DatasetName
G	O
P	O
R	O
i	O
8	O
e	O
K	O
9	O
g	O
P	O
a	O
U	O
D	O
b	O
b	O
T	O
b	O
t	O
0	B-DatasetName
s	O
w	O
m	O
7	O
E	O
6	O
G	O
E	O
/	O
g	O
Q	O
v	O
H	O
h	O
T	O
x	O
6	O
i	O
/	O
y	O
5	O
r	O
9	O
x	O
2	O
+	O
a	O
g	O
r	O
Q	O
8	O
G	O
H	O
u	O
/	O
N	O
M	O
D	O
M	O
v	O
T	O
K	O
U	O
w	O
6	O
H	O
n	O
f	O
T	O
m	O
l	O
t	O
f	O
W	O
N	O
z	O
q	O
7	O
x	O
d	O
2	O
d	O
n	O
d	O
2	O
z	O
9	O
w	O
D	O
4	O
9	O
a	O
J	O
s	O
k	O
0	B-DatasetName
4	O
0	B-DatasetName
2	O
W	O
y	O
E	O
R	O
3	O
Q	O
m	O
q	O
4	O
F	O
I	O
o	O
3	O
U	O
a	O
D	O
k	O
n	O
V	O
R	O
z	O
G	O
o	O
e	O
S	O
t	O
8	O
P	O
x	O
7	O
c	O
x	O
v	O
P	O
3	O
F	O
t	O
R	O
K	O
I	O
e	O
c	O
Z	O
L	O
y	O
I	O
K	O
Z	O
D	O
J	O
S	O
L	O
B	O
K	O
F	O
r	O
p	O
A	O
f	O
t	O
+	O
3	O
6	O
1	O
6	O
N	O
W	O
8	O
O	O
s	O
k	O
r	O
8	O
g	O
l	O
S	O
h	O
Q	O
K	O
P	O
v	O
f	O
v	O
U	O
G	O
C	O
c	O
t	O
i	O
r	O
p	O
B	O
J	O
a	O
k	O
z	O
X	O
9	O
1	O
I	O
M	O
c	O
q	O
p	O
R	O
M	O
M	O
m	O
n	O
l	O
V	O
5	O
m	O
e	O
E	O
r	O
Z	O
m	O
A	O
5	O
5	O
1	O
1	O
J	O
F	O
Y	O
2	O
6	O
C	O
f	O
H	O
7	O
q	O
l	O
J	O
x	O
Z	O
Z	O
U	O
C	O
i	O
R	O
N	O
t	O
S	O
S	O
O	O
b	O
q	O
7	O
4	O
m	O
c	O
x	O
s	O
Z	O
M	O
4	O
t	O
B	O
2	O
x	O
h	O
R	O
H	O
Z	O
t	O
m	O
b	O
i	O
f	O
9	O
5	O
3	O
Q	O
y	O
j	O
6	O
y	O
A	O
X	O
K	O
s	O
2	O
Q	O
K	O
7	O
Z	O
Y	O
F	O
G	O
W	O
S	O
Y	O
E	O
J	O
m	O
f	O
5	O
O	O
B	O
0	B-DatasetName
J	O
y	O
h	O
n	O
F	O
h	O
C	O
m	O
R	O
b	O
2	O
V	O
s	O
J	O
G	O
V	O
F	O
O	O
G	O
N	O
p	O
2	O
K	O
D	O
c	O
F	O
f	O
f	O
n	O
m	O
V	O
t	O
C	O
5	O
q	O
v	O
l	O
f	O
z	O
7	O
y	O
+	O
r	O
9	O
Z	O
s	O
i	O
j	O
j	O
K	O
c	O
w	O
C	O
m	O
c	O
g	O
w	O
9	O
X	O
U	O
I	O
c	O
7	O
a	O
E	O
A	O
T	O
G	O
A	O
z	O
h	O
G	O
V	O
7	O
h	O
z	O
Z	O
H	O
O	O
i	O
/	O
P	O
u	O
f	O
C	O
x	O
a	O
S	O
0	B-DatasetName
4	O
x	O
c	O
w	O
x	O
/	O
4	O
H	O
z	O
+	O
A	O
A	O
W	O
Y	O
j	O
Z	O
w	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
F	O
V	O
a	O
Q	O
d	O
j	O
U	O
W	O
Z	O
V	O
L	O
y	O
U	O
U	O
a	O
l	O
b	O
b	O
T	O
R	O
1	O
N	O
M	O
D	O
o	O
K	O
M	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
o	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
0	B-DatasetName
G	O
P	O
R	O
i	O
8	O
e	O
K	O
9	O
g	O
P	O
a	O
U	O
D	O
b	O
b	O
T	O
b	O
t	O
0	B-DatasetName
s	O
w	O
m	O
7	O
E	O
6	O
G	O
E	O
/	O
g	O
Q	O
v	O
H	O
h	O
T	O
x	O
6	O
i	O
/	O
y	O
5	O
r	O
9	O
x	O
2	O
+	O
a	O
g	O
r	O
Q	O
8	O
G	O
H	O
u	O
/	O
N	O
M	O
D	O
M	O
v	O
T	O
K	O
U	O
w	O
6	O
H	O
n	O
f	O
T	O
m	O
l	O
t	O
f	O
W	O
N	O
z	O
q	O
7	O
x	O
d	O
2	O
d	O
n	O
d	O
2	O
z	O
9	O
w	O
D	O
4	O
9	O
a	O
J	O
s	O
k	O
0	B-DatasetName
4	O
0	B-DatasetName
2	O
W	O
y	O
E	O
R	O
3	O
Q	O
m	O
q	O
4	O
F	O
I	O
o	O
3	O
U	O
a	O
D	O
k	O
n	O
V	O
R	O
z	O
G	O
o	O
e	O
S	O
t	O
8	O
P	O
x	O
7	O
c	O
x	O
v	O
P	O
3	O
F	O
t	O
R	O
K	O
I	O
e	O
c	O
Z	O
L	O
y	O
I	O
K	O
Z	O
D	O
J	O
S	O
L	O
B	O
K	O
F	O
r	O
p	O
A	O
f	O
t	O
+	O
3	O
6	O
1	O
6	O
N	O
W	O
8	O
O	O
s	O
k	O
r	O
8	O
g	O
l	O
S	O
h	O
Q	O
K	O
P	O
v	O
f	O
v	O
U	O
G	O
C	O
c	O
t	O
i	O
r	O
p	O
B	O
J	O
a	O
k	O
z	O
X	O
9	O
1	O
I	O
M	O
c	O
q	O
p	O
R	O
M	O
M	O
m	O
n	O
l	O
V	O
5	O
m	O
e	O
E	O
r	O
Z	O
m	O
A	O
5	O
5	O
1	O
1	O
J	O
F	O
Y	O
2	O
6	O
C	O
f	O
H	O
7	O
q	O
l	O
J	O
x	O
Z	O
Z	O
U	O
C	O
i	O
R	O
N	O
t	O
S	O
S	O
O	O
b	O
q	O
7	O
4	O
m	O
c	O
x	O
s	O
Z	O
M	O
4	O
t	O
B	O
2	O
x	O
h	O
R	O
H	O
Z	O
t	O
m	O
b	O
i	O
f	O
9	O
5	O
3	O
Q	O
y	O
j	O
6	O
y	O
A	O
X	O
K	O
s	O
2	O
Q	O
K	O
7	O
Z	O
Y	O
F	O
G	O
W	O
S	O
Y	O
E	O
J	O
m	O
f	O
5	O
O	O
B	O
0	B-DatasetName
J	O
y	O
h	O
n	O
F	O
h	O
C	O
m	O
R	O
b	O
2	O
V	O
s	O
J	O
G	O
V	O
F	O
O	O
G	O
N	O
p	O
2	O
K	O
D	O
c	O
F	O
f	O
f	O
n	O
m	O
V	O
t	O
C	O
5	O
q	O
v	O
l	O
f	O
z	O
7	O
y	O
+	O
r	O
9	O
Z	O
s	O
i	O
j	O
j	O
K	O
c	O
w	O
C	O
m	O
c	O
g	O
w	O
9	O
X	O
U	O
I	O
c	O
7	O
a	O
E	O
A	O
T	O
G	O
A	O
z	O
h	O
G	O
V	O
7	O
h	O
z	O
Z	O
H	O
O	O
i	O
/	O
P	O
u	O
f	O
C	O
x	O
a	O
S	O
0	B-DatasetName
4	O
x	O
c	O
w	O
x	O
/	O
4	O
H	O
z	O
+	O
A	O
A	O
W	O
Y	O
j	O
Z	O
w	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
F	O
V	O
a	O
Q	O
d	O
j	O
U	O
W	O
Z	O
V	O
L	O
y	O
U	O
U	O
a	O
l	O
b	O
b	O
T	O
R	O
1	O
N	O
M	O
D	O
o	O
K	O
M	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
o	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
0	B-DatasetName
G	O
P	O
R	O
i	O
8	O
e	O
K	O
9	O
g	O
P	O
a	O
U	O
D	O
b	O
b	O
T	O
b	O
t	O
0	B-DatasetName
s	O
w	O
m	O
7	O
E	O
6	O
G	O
E	O
/	O
g	O
Q	O
v	O
H	O
h	O
T	O
x	O
6	O
i	O
/	O
y	O
5	O
r	O
9	O
x	O
2	O
+	O
a	O
g	O
r	O
Q	O
8	O
G	O
H	O
u	O
/	O
N	O
M	O
D	O
M	O
v	O
T	O
K	O
U	O
w	O
6	O
H	O
n	O
f	O
T	O
m	O
l	O
t	O
f	O
W	O
N	O
z	O
q	O
7	O
x	O
d	O
2	O
d	O
n	O
d	O
2	O
z	O
9	O
w	O
D	O
4	O
9	O
a	O
J	O
s	O
k	O
0	B-DatasetName
4	O
0	B-DatasetName
2	O
W	O
y	O
E	O
R	O
3	O
Q	O
m	O
q	O
4	O
F	O
I	O
o	O
3	O
U	O
a	O
D	O
k	O
n	O
V	O
R	O
z	O
G	O
o	O
e	O
S	O
t	O
8	O
P	O
x	O
7	O
c	O
x	O
v	O
P	O
3	O
F	O
t	O
R	O
K	O
I	O
e	O
c	O
Z	O
L	O
y	O
I	O
K	O
Z	O
D	O
J	O
S	O
L	O
B	O
K	O
F	O
r	O
p	O
A	O
f	O
t	O
+	O
3	O
6	O
1	O
6	O
N	O
W	O
8	O
O	O
s	O
k	O
r	O
8	O
g	O
l	O
S	O
h	O
Q	O
K	O
P	O
v	O
f	O
v	O
U	O
G	O
C	O
c	O
t	O
i	O
r	O
p	O
B	O
J	O
a	O
k	O
z	O
X	O
9	O
1	O
I	O
M	O
c	O
q	O
p	O
R	O
M	O
M	O
m	O
n	O
l	O
V	O
5	O
m	O
e	O
E	O
r	O
Z	O
m	O
A	O
5	O
5	O
1	O
1	O
J	O
F	O
Y	O
2	O
6	O
C	O
f	O
H	O
7	O
q	O
l	O
J	O
x	O
Z	O
Z	O
U	O
C	O
i	O
R	O
N	O
t	O
S	O
S	O
O	O
b	O
q	O
7	O
4	O
m	O
c	O
x	O
s	O
Z	O
M	O
4	O
t	O
B	O
2	O
x	O
h	O
R	O
H	O
Z	O
t	O
m	O
b	O
i	O
f	O
9	O
5	O
3	O
Q	O
y	O
j	O
6	O
y	O
A	O
X	O
K	O
s	O
2	O
Q	O
K	O
7	O
Z	O
Y	O
F	O
G	O
W	O
S	O
Y	O
E	O
J	O
m	O
f	O
5	O
O	O
B	O
0	B-DatasetName
J	O
y	O
h	O
n	O
F	O
h	O
C	O
m	O
R	O
b	O
2	O
V	O
s	O
J	O
G	O
V	O
F	O
O	O
G	O
N	O
p	O
2	O
K	O
D	O
c	O
F	O
f	O
f	O
n	O
m	O
V	O
t	O
C	O
5	O
q	O
v	O
l	O
f	O
z	O
7	O
y	O
+	O
r	O
9	O
Z	O
s	O
i	O
j	O
j	O
K	O
c	O
w	O
C	O
m	O
c	O
g	O
w	O
9	O
X	O
U	O
I	O
c	O
7	O
a	O
E	O
A	O
T	O
G	O
A	O
z	O
h	O
G	O
V	O
7	O
h	O
z	O
Z	O
H	O
O	O
i	O
/	O
P	O
u	O
f	O
C	O
x	O
a	O
S	O
0	B-DatasetName
4	O
x	O
c	O
w	O
x	O
/	O
4	O
H	O
z	O
+	O
A	O
A	O
W	O
Y	O
j	O
Z	O
w	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
Figure	O
1	O
:	O
An	O
example	O
KIDSCOOK	O
sequence	O
with	O
multiple	O
types	O
of	O
hierarchy	O
and	O
abstraction	O
:	O
the	O
example	O
contains	O
sequences	O
of	O
complex	O
instructions	O
,	O
given	O
both	O
as	O
sentences	O
and	O
sequences	O
of	O
simpler	O
instructions	O
.	O
Abelson	O
,	O
1977	O
;	O
DeJong	O
,	O
1981	O
)	O
but	O
required	O
handwritten	O
data	O
structures	O
encoding	O
world	O
knowledge	O
.	O
However	O
,	O
the	O
automatic	O
induction	O
of	O
such	O
commonsense	O
knowledge	O
from	O
open	O
-	O
domain	O
noisy	O
text	O
corpora	O
remains	O
an	O
open	O
problem	O
(	O
Chambers	O
,	O
2013	O
;	O
Weber	O
et	O
al	O
,	O
2018	O
;	O
Zellers	O
et	O
al	O
,	O
2018	O
)	O
.	O
As	O
a	O
step	O
towards	O
solving	O
this	O
problem	O
we	O
consider	O
textual	O
descriptions	O
of	O
actions	O
in	O
a	O
cooking	O
domain	O
.	O
We	O
introduce	O
a	O
dataset	O
,	O
KIDSCOOK	O
,	O
targeted	O
at	O
exploring	O
the	O
automatic	O
acquisition	O
of	O
correspondences	O
between	O
abstract	O
and	O
concrete	O
descriptions	O
of	O
actions	O
.	O
The	O
dataset	O
consists	O
of	O
higher	O
-	O
level	O
single	O
-	O
sentence	O
imperative	O
descriptions	O
paired	O
with	O
lower	O
-	O
level	O
descriptions	O
with	O
elided	O
details	O
included	O
.	O
Descriptions	O
come	O
from	O
real	O
grounded	O
actions	O
,	O
built	O
on	O
top	O
of	O
the	O
YouCookII	O
video	O
caption	O
dataset	O
.	O
Figure	O
1	O
gives	O
an	O
example	O
annotation	O
from	O
the	O
dataset	O
:	O
the	O
phrase	O
"	O
drain	O
the	O
pasta	O
,	O
"	O
presented	O
to	O
an	O
annotator	O
with	O
its	O
corresponding	O
video	O
clip	O
,	O
was	O
annotated	O
as	O
corresponding	O
to	O
four	O
constituent	O
steps	O
appropriate	O
as	O
instruction	O
for	O
a	O
child	O
.	O
The	O
constituent	O
steps	O
are	O
"	O
simpler	O
"	O
in	O
the	O
sense	O
that	O
they	O
correspond	O
to	O
more	O
atomic	O
actions	O
,	O
but	O
not	O
necessarily	O
in	O
their	O
linguistic	O
complexity	O
.	O
We	O
identify	O
over	O
1	O
,	O
500	O
procedures	O
and	O
tools	O
which	O
KIDSCOOK	O
makes	O
explicit	O
but	O
are	O
assumed	O
as	O
commonsense	O
world	O
knowledge	O
by	O
YouCookII	O
.	O
The	O
KIDSCOOK	O
dataset	O
al	O
ows	O
us	O
to	O
learn	O
mappings	O
between	O
abstract	O
and	O
concrete	O
descriptions	O
via	O
sequence	O
-	O
to	O
-	O
sequence	O
prediction	O
.	O
We	O
apply	O
several	O
standard	O
neural	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
;	O
however	O
,	O
since	O
these	O
models	O
do	O
not	O
expose	O
explicit	O
,	O
interpretable	O
correspondences	O
between	O
abstract	O
and	O
concrete	O
descriptions	O
,	O
we	O
also	O
propose	O
the	O
application	O
of	O
neural	O
transduction	O
models	O
which	O
capture	O
correspondences	O
with	O
latent	O
hard	O
alignment	O
variables	O
.	O
We	O
define	O
a	O
cloze	O
-	O
style	O
evaluation	O
to	O
complement	O
our	O
dataset	O
,	O
in	O
which	O
models	O
must	O
predict	O
the	O
values	O
of	O
held	O
-	O
out	O
tokens	O
which	O
target	O
knowledge	O
of	O
tool	O
usage	O
,	O
temporal	O
ordering	O
,	O
and	O
kitchen	O
commonsense	O
.	O
We	O
find	O
that	O
our	O
neural	O
transduction	O
models	O
are	O
able	O
to	O
match	O
the	O
predictive	O
power	O
of	O
traditional	O
neural	O
sequence	O
models	O
while	O
providing	O
interpretable	O
alignments	O
between	O
abstract	O
and	O
concrete	O
subsequences	O
useful	O
for	O
our	O
primary	O
goal	O
of	O
analysis	O
of	O
implicit	O
hierarchical	O
script	O
knowledge	O
.	O

We	O
construct	O
a	O
task	O
on	O
Amazon	O
's	O
Mechanical	O
Turk	O
,	O
where	O
workers	O
are	O
asked	O
to	O
explain	O
a	O
video	O
action	O
caption	O
to	O
a	O
child	O
.	O
2	O
Every	O
instruction	O
is	O
paired	O
with	O
the	O
original	O
YouTube	O
video	O
and	O
YouCook	B-DatasetName
caption	O
so	O
the	O
annotator	O
could	O
see	O
how	O
1	O
Notable	O
exceptions	O
include	O
the	O
hierarchical	O
instructions	O
of	O
(	O
Regneri	O
et	O
al	O
,	O
2013	O
)	O
and	O
(	O
Bisk	O
et	O
al	O
,	O
2016	O
the	O
action	O
was	O
performed	O
,	O
rather	O
than	O
hallucinating	O
additional	O
details	O
.	O
All	O
captions	O
received	O
three	O
simplifications	O
.	O
The	O
instructions	O
ask	O
users	O
to	O
focus	O
on	O
missing	O
information	O
and	O
allow	O
them	O
up	O
to	O
five	O
steps	O
.	O
Finally	O
,	O
we	O
explicitly	O
asked	O
annotators	O
to	O
simplify	O
complex	O
actions	O
(	O
e.g.	O
dice	O
)	O
that	O
can	O
be	O
defined	O
by	O
a	O
series	O
of	O
more	O
basic	O
actions	O
(	O
e.g.	O
cut	O
)	O
.	O
Our	O
KIDSCOOK	O
corpus	O
statistics	O
are	O
shown	O
in	O
Table	O
1	O
.	O
In	O
total	O
we	O
collected	O
over	O
10	O
K	O
action	O
sequences	O
(	O
∼400	O
K	O
tokens	O
)	O
.	O
The	O
average	O
caption	O
is	O
approximately	O
4x	O
longer	O
than	O
a	O
YouCook	B-DatasetName
caption	O
.	O
Most	O
importantly	O
1	O
,	O
536	O
lemmas	O
and	O
2	O
,	O
316	O
lexical	O
types	O
from	O
KIDSCOOK	O
's	O
vocabulary	O
do	O
not	O
appear	O
in	O
any	O
of	O
the	O
original	O
captions	O
.	O
This	O
indicates	O
that	O
there	O
are	O
over	O
1	O
,	O
500	O
new	O
concepts	O
,	O
tools	O
,	O
and	O
procedures	O
that	O
were	O
assumed	O
by	O
YouCookII	O
but	O
are	O
now	O
explicit	O
in	O
KIDSCOOK	O
.	O

wrap	O
the	O
pizza	O
.	O
Pred	O
find	O
a	O
large	O
piece	O
to	O
put	O
the	O
pizza	O
om	O
.	O
place	O
the	O
pizza	O
in	O
the	O
center	O
for	O
it	O
not	O
to	O
stick	O
around	O
.	O
grab	O
the	O
plastic	O
wrap	O
and	O
start	O
wrapping	O
the	O
entire	O
thing	O
and	O
pizza	O
.	O
wrap	O
all	O
around	O
until	O
completely	O
covered	O
on	O
all	O
corners	O
.	O
put	O
in	O
freezer	O
on	O
a	O
cold	O
water	O
and	O
freeze	O
overnight	O
Gold	O
find	O
a	O
hard	O
surface	O
to	O
put	O
the	O
pizza	O
om	O
.	O
place	O
the	O
pizza	O
in	O
the	O
center	O
for	O
it	O
not	O
to	O
slide	O
around	O
.	O
grab	O
the	O
plastic	O
wrap	O
and	O
start	O
wrapping	O
the	O
hard	O
surface	O
and	O
pizza	O
.	O
wrap	O
all	O
around	O
until	O
fully	O
covered	O
on	O
all	O
corners	O
.	O
put	O
in	O
freezer	O
on	O
a	O
flat	O
surface	O
and	O
freeze	O
overnight	O
of	O
the	O
phenomena	O
we	O
would	O
hope	O
to	O
see	O
(	O
Table	O
3	O
)	O
.	O
The	O
left	O
-	O
hand	O
side	O
of	O
the	O
table	O
shows	O
words	O
from	O
the	O
abstract	O
YouCook	B-DatasetName
annotations	O
and	O
corresponding	O
phrases	O
in	O
the	O
concrete	O
annotation	O
.	O
For	O
the	O
righthand	O
side	O
we	O
searched	O
for	O
common	O
concrete	O
terms	O
that	O
may	O
be	O
preceded	O
or	O
followed	O
by	O
other	O
terms	O
,	O
and	O
present	O
the	O
abstract	O
terms	O
they	O
were	O
most	O
often	O
generated	O
by	O
.	O
Finally	O
,	O
Table	O
5	O
shows	O
three	O
randomly	O
chosen	O
examples	O
(	O
from	O
the	O
validation	O
set	O
)	O
of	O
greedy	O
decodings	O
for	O
slot	B-TaskName
filling	I-TaskName
with	O
GPT	B-MethodName
fine	O
-	O
tuned	O
on	O
our	O
dataset	O
.	O
These	O
examples	O
demonstrate	O
that	O
,	O
first	O
,	O
there	O
are	O
cases	O
where	O
GPT	B-MethodName
is	O
successful	O
or	O
produces	O
a	O
semantically	O
valid	O
answer	O
(	O
e.g.	O
fully	O
vs	O
completely	O
)	O
.	O
Second	O
,	O
as	O
is	O
common	O
with	O
greedy	O
decoding	O
,	O
the	O
model	O
can	O
get	O
stuck	O
in	O
a	O
loop	O
(	O
e.g.	O
cut	O
,	O
cutting	O
,	O
cutting	O
,	O
...	O
)	O
.	O
Finally	O
,	O
note	O
there	O
are	O
nonsensical	O
cases	O
where	O
the	O
model	O
appears	O
to	O
have	O
discarded	O
the	O
abstract	O
context	O
(	O
e.g.	O
knife	O
to	O
add	O
tomato	O
sauce	O
or	O
freezer	O
on	O
a	O
cold	O
water	O
)	O
.	O

Many	O
script	O
learning	O
systems	O
are	O
based	O
on	O
event	O
co	O
-	O
occurrence	O
and	O
language	O
modeling	O
in	O
large	O
text	O
corpora	O
,	O
and	O
can	O
infer	O
implicit	O
events	O
without	O
creating	O
explicit	O
situation	O
-	O
specific	O
frame	O
structures	O
(	O
Chambers	O
and	O
Jurafsky	O
,	O
2008	O
;	O
Rudinger	O
et	O
al	O
,	O
2015	O
;	O
Pichotta	O
and	O
Mooney	O
,	O
2016	O
)	O
.	O
Other	O
systems	O
induce	O
situation	O
-	O
specific	O
frames	O
from	O
text	O
(	O
Cheung	O
et	O
al	O
,	O
2013	O
;	O
Balasubramanian	O
et	O
al	O
,	O
2013	O
)	O
.	O
However	O
,	O
these	O
methods	O
do	O
not	O
explicitly	O
target	O
the	O
commonsense	O
correspondence	O
between	O
differing	O
levels	O
of	O
detail	O
of	O
complex	O
events	O
.	O
Most	O
relevant	O
to	O
this	O
paper	O
is	O
the	O
pioneering	O
work	O
of	O
Regneri	O
et	O
al	O
(	O
2013	O
)	O
as	O
extended	O
by	O
Senina	O
et	O
al	O
(	O
2014	O
)	O
and	O
.	O
These	O
papers	O
present	O
the	O
TACOS	O
corpus	O
,	O
consisting	O
of	O
natural	O
language	O
descriptions	O
of	O
activities	O
in	O
videos	O
paired	O
with	O
low	O
-	O
level	O
activity	O
labels	O
.	O
Senina	O
et	O
al	O
(	O
2014	O
)	O
collect	O
an	O
additional	O
level	O
of	O
multi	O
-	O
sentence	O
annotations	O
on	O
the	O
corpus	O
,	O
which	O
allowing	O
for	O
video	O
caption	O
generation	O
at	O
multiple	O
levels	O
of	O
detail	O
.	O
describe	O
a	O
similar	O
corpus	O
of	O
natural	O
descriptions	O
of	O
composite	O
actions	O
,	O
useful	O
for	O
activity	B-TaskName
recognition	I-TaskName
in	O
video	O
.	O
These	O
corpora	O
differ	O
in	O
a	O
number	O
of	O
important	O
ways	O
from	O
KIDSCOOK	O
;	O
in	O
particular	O
,	O
the	O
language	O
has	O
somewhat	O
limited	O
complexity	O
and	O
"	O
naturalness	O
"	O
when	O
describing	O
complex	O
scenarios	O
,	O
a	O
phenomenon	O
also	O
observed	O
in	O
the	O
robotics	O
literature	O
(	O
Scalise	O
et	O
al	O
,	O
2018	O
)	O
.	O
Our	O
data	O
collection	O
process	O
avoids	O
more	O
formulaic	O
language	O
by	O
eliciting	O
"	O
child	O
-	O
directed	O
"	O
descriptions	O
.	O

We	O
model	O
the	O
conditional	O
probability	O
of	O
a	O
concrete	O
sequence	O
y	O
given	O
abstract	O
sequence	O
x	O
through	O
a	O
latent	O
alignment	O
variable	O
a	O
between	O
x	O
and	O
y	O
,	O
which	O
is	O
a	O
sequence	O
of	O
variables	O
a	O
j	O
,	O
with	O
a	O
j	O
=	O
i	O
signifying	O
that	O
y	O
j	O
is	O
aligned	O
to	O
x	O
i	O
.	O
The	O
marginal	O
probability	O
of	O
y	O
given	O
x	O
is	O
p	O
(	O
y	O
|	O
x	O
)	O
=	O
a	O
p	O
(	O
y	O
,	O
a	O
|	O
x	O
)	O
.	O
(	O
1	O
)	O
In	O
the	O
following	O
,	O
we	O
use	O
m	O
to	O
denote	O
the	O
length	O
of	O
x	O
and	O
n	O
to	O
denote	O
the	O
length	O
of	O
y.	O
The	O
model	O
formulation	O
restricts	O
alignments	O
to	O
be	O
monotonic	O
,	O
i.e.	O
a	O
j+1	O
≥	O
a	O
j	O
for	O
all	O
j.	O
The	O
model	O
factorizes	O
over	O
timesteps	O
into	O
alignment	O
and	O
word	O
prediction	O
probabilities	O
,	O
such	O
that	O
the	O
word	O
prediction	O
at	O
each	O
timestep	O
is	O
informed	O
by	O
its	O
alignment	O
:	O
p	O
(	O
y	O
,	O
a	O
|	O
x	O
)	O
=	O
j	O
p	O
(	O
a	O
j	O
|	O
a	O
j−1	O
,	O
x	O
1	O
:	O
a	O
j−1	O
,	O
y	O
1	O
:	O
j−1	O
)	O
×	O
p	O
(	O
y	O
j	O
|	O
a	O
j	O
,	O
x	O
1	O
:	O
a	O
j	O
,	O
y	O
1	O
:	O
j−1	O
)	O
(	O
2	O
)	O
The	O
abstract	O
and	O
concrete	O
sequences	O
are	O
both	O
encoded	O
with	O
LSTM	B-MethodName
Recurrent	O
Neural	O
Networks	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
.	O
In	O
contrast	O
to	O
standard	O
attention	O
-	O
based	O
models	O
,	O
the	O
aligned	O
encoder	O
representation	O
is	O
not	O
fed	O
into	O
the	O
decoder	O
RNN	O
state	O
,	O
but	O
only	O
used	O
to	O
make	O
next	O
word	O
predictions	O
.	O
Due	O
to	O
the	O
small	O
size	O
of	O
the	O
training	O
data	O
,	O
words	O
in	O
both	O
sequences	O
are	O
embedded	O
using	O
fixed	O
GloVe	B-MethodName
embeddings	I-MethodName
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
.	O
The	O
word	O
emission	O
probability	O
is	O
then	O
defined	O
as	O
p	O
(	O
yj	O
|	O
aj	O
,	O
x1	O
:	O
a	O
j	O
,	O
y1	O
:	O
j−1	O
)	O
=	O
softmax	B-MethodName
(	O
MLP	B-DatasetName
(	O
ea	O
j	O
,	O
dj	O
)	O
)	O
(	O
3	O
)	O
with	O
e	O
the	O
encoder	O
hidden	O
states	O
and	O
d	O
the	O
decoder	O
hidden	O
states	O
.	O
The	O
alignment	O
probability	O
factorizes	O
into	O
shift	O
and	O
emit	O
probabilities	O
,	O
where	O
a	O
shift	O
action	O
increments	O
the	O
alignment	O
to	O
the	O
next	O
word	O
in	O
the	O
input	O
sequence	O
,	O
and	O
an	O
emit	O
action	O
generates	O
the	O
next	O
output	O
word	O
.	O
We	O
refer	O
to	O
these	O
as	O
transition	O
probabilities	O
.	O
This	O
formulation	O
enables	O
us	O
to	O
restrict	O
the	O
hard	O
alignment	O
to	O
be	O
monotonic	O
.	O
We	O
consider	O
two	O
parameterizations	O
of	O
this	O
distribution	O
.	O
In	O
the	O
first	O
,	O
the	O
probabilities	O
are	O
parameterized	O
by	O
the	O
neural	O
network	O
,	O
using	O
the	O
encoder	O
and	O
decoder	O
hidden	O
state	O
in	O
a	O
similar	O
manner	O
to	O
how	O
the	O
word	O
emission	O
probability	O
was	O
computed	O
.	O
The	O
alignment	O
probability	O
at	O
a	O
given	O
timestep	O
is	O
therefore	O
parameterized	O
as	O
p	O
(	O
aj	O
|	O
aj−1	O
,	O
x1	O
:	O
a	O
j−1	O
,	O
y1	O
:	O
j−1	O
)	O
=	O
p	O
(	O
emit	O
|	O
aj	O
,	O
x1	O
:	O
a	O
j	O
,	O
y1	O
:	O
j−1	O
)	O
×	O
a	O
j	O
−1	O
i	O
=	O
a	O
j−1	O
p	O
(	O
shift	O
|	O
i	O
,	O
x1	O
:	O
i	O
,	O
y1	O
:	O
j−1	O
)	O
,	O
(	O
4	O
)	O
where	O
p	O
(	O
shift	O
|	O
i	O
,	O
x1	O
:	O
i	O
,	O
y1	O
:	O
j−1	O
)	O
=	O
σ	O
(	O
M	O
LP	O
(	O
ei	O
,	O
dj	O
)	O
)	O
,	O
(	O
5	O
)	O
p	O
(	O
emit	O
|	O
i	O
,	O
x1	O
:	O
i	O
,	O
y1	O
:	O
j−1	O
)	O
=	O
1	O
−	O
p	O
(	O
shift	O
|	O
i	O
,	O
x1	O
:	O
i	O
,	O
y1	O
:	O
j−1	O
)	O
.	O
(	O
6	O
)	O
We	O
also	O
consider	O
using	O
the	O
simpler	O
,	O
fixed	O
alignment	O
parameterization	O
in	O
Yu	O
,	O
Buys	O
,	O
and	O
Blunsom	O
(	O
2016	O
)	O
,	O
where	O
the	O
transition	O
probability	O
is	O
conditioned	O
only	O
on	O
sequence	O
length	O
,	O
not	O
on	O
x	O
or	O
y	O
,	O
and	O
can	O
therefore	O
be	O
estimated	O
using	O
the	O
ratio	O
between	O
input	O
and	O
output	O
sentence	O
lengths	O
.	O
The	O
alignment	O
probabilities	O
are	O
not	O
updated	O
during	O
training	O
,	O
and	O
consequently	O
the	O
posterior	O
distribution	O
over	O
the	O
alignments	O
is	O
biased	O
towards	O
this	O
prior	O
,	O
favoring	O
alignments	O
close	O
to	O
the	O
diagonal	O
.	O
The	O
parameterized	O
alignment	O
model	O
contains	O
as	O
special	O
cases	O
two	O
degenerate	O
solutions	O
:	O
(	O
1	O
)	O
an	O
unconditional	O
language	O
model	O
and	O
(	O
2	O
)	O
a	O
seq2seq	B-MethodName
model	O
.	O
These	O
occur	O
if	O
the	O
model	O
performs	O
all	O
emits	O
before	O
shifting	O
or	O
all	O
shifts	O
before	O
emitting	O
,	O
respectively	O
.	O
To	O
prevent	O
the	O
creation	O
of	O
a	O
language	O
model	O
we	O
force	O
the	O
last	O
output	O
word	O
to	O
be	O
aligned	O
to	O
the	O
last	O
word	O
in	O
the	O
abstract	O
sequence	O
,	O
similar	O
to	O
Yu	O
et	O
al	O
(	O
2017	O
)	O
.	O
However	O
,	O
the	O
parameterized	O
transition	O
model	O
could	O
still	O
in	O
practice	O
revert	O
to	O
a	O
pure	O
sequence	O
-	O
to	O
-	O
sequence	O
model	O
.	O

Improving	O
Joint	O
Training	O
of	O
Inference	O
Networks	O
and	O
Structured	B-TaskName
Prediction	I-TaskName
Energy	O
Networks	O

We	O
now	O
discuss	O
several	O
methods	O
that	O
simplify	O
and	O
stabilize	O
training	O
SPENs	O
with	O
inference	O
networks	O
.	O
When	O
describing	O
them	O
,	O
we	O
will	O
illustrate	O
their	O
impact	O
by	O
showing	O
training	O
trajectories	O
for	O
the	O
Twitter	O
part	B-TaskName
-	I-TaskName
of	I-TaskName
-	I-TaskName
speech	I-TaskName
tagging	I-TaskName
task	O
.	O

We	O
contributed	O
several	O
strategies	O
to	O
stabilize	O
and	O
improve	O
joint	O
training	O
of	O
SPENs	O
and	O
inference	O
networks	O
.	O
Our	O
use	O
of	O
joint	O
parameterizations	O
mitigates	O
the	O
need	O
for	O
inference	O
network	O
fine	O
-	O
tuning	O
,	O
leads	O
to	O
complementarity	O
in	O
the	O
learned	O
inference	O
networks	O
,	O
and	O
yields	O
improved	O
performance	O
overall	O
.	O
These	O
developments	O
offer	O
promise	O
for	O
SPENs	O
to	O
be	O
more	O
easily	O
applied	O
to	O
a	O
broad	O
range	O
of	O
NLP	O
tasks	O
.	O
Future	O
work	O
will	O
explore	O
other	O
structured	B-TaskName
prediction	I-TaskName
tasks	O
,	O
such	O
as	O
parsing	O
and	O
generation	O
.	O
We	O
have	O
taken	O
initial	O
steps	O
in	O
this	O
direction	O
,	O
considering	O
constituency	B-TaskName
parsing	I-TaskName
with	O
the	O
sequence	O
-	O
to	O
-	O
sequence	O
model	O
of	O
Tran	O
et	O
al	O
(	O
2018	O
)	O
.	O
Preliminary	O
experiments	O
are	O
positive	O
,	O
5	O
but	O
significant	O
challenges	O
remain	O
,	O
specifically	O
in	O
defining	O
appropriate	O
inference	O
network	O
architectures	O
to	O
enable	O
efficient	O
learning	O
.	O

YNU	O
-	O
HPCC	O
at	O
SemEval	O
-	O
2021	O
Task	O
6	O
:	O
Combining	O
ALBERT	B-MethodName
and	O
Text	O
-	O
CNN	O
for	O
Persuasion	O
Detection	O
in	O
Texts	O
and	O
Images	O

In	O
recent	O
years	O
,	O
memes	O
combining	O
image	O
and	O
text	O
have	O
been	O
widely	O
used	O
in	O
social	O
media	O
,	O
and	O
memes	O
are	O
one	O
of	O
the	O
most	O
popular	O
types	O
of	O
content	O
used	O
in	O
online	O
disinformation	O
campaigns	O
.	O
In	O
this	O
paper	O
,	O
our	O
study	O
on	O
the	O
detection	O
of	O
persuasion	O
techniques	O
in	O
texts	O
and	O
images	O
in	O
SemEval	O
-	O
2021	O
Task	O
6	O
is	O
summarized	O
.	O
For	O
propaganda	O
technology	O
detection	O
in	O
text	O
,	O
we	O
propose	O
a	O
combination	O
model	O
of	O
both	O
AL	O
-	O
BERT	B-MethodName
and	O
Text	O
-	O
CNN	O
for	O
text	B-TaskName
classification	I-TaskName
,	O
as	O
well	O
as	O
a	O
BERT	B-MethodName
-	O
based	O
multi	O
-	O
task	O
sequence	O
labeling	O
model	O
for	O
propaganda	O
technology	O
coverage	O
span	O
detection	O
.	O
For	O
the	O
meme	B-TaskName
classification	I-TaskName
task	O
involved	O
in	O
text	O
understanding	O
and	O
visual	O
feature	O
extraction	O
,	O
we	O
designed	O
a	O
parallel	O
channel	O
model	O
divided	O
into	O
text	O
and	O
image	O
channels	O
.	O
Our	O
method	O
1	O
achieved	O
a	O
good	O
performance	O
on	O
subtasks	O
1	O
and	O
3	O
.	O
The	O
micro	O
F	O
1scores	O
of	O
0.492	O
,	O
0.091	O
,	O
and	O
0.446	O
achieved	O
on	O
the	O
test	O
sets	O
of	O
the	O
three	O
subtasks	O
ranked	O
12th	O
,	O
7th	O
,	O
and	O
11th	O
,	O
respectively	O
,	O
and	O
all	O
are	O
higher	O
than	O
the	O
baseline	O
model	O
.	O

The	O
intentional	O
shaping	O
of	O
information	O
to	O
promote	O
a	O
predetermined	O
agenda	O
is	O
called	O
propaganda	O
.	O
Propaganda	O
uses	O
psychological	O
and	O
rhetorical	O
techniques	O
to	O
achieve	O
its	O
purpose	O
.	O
Propaganda	O
techniques	O
generally	O
include	O
the	O
use	O
of	O
logical	B-TaskName
fallacies	I-TaskName
and	O
appeal	O
to	O
the	O
emotions	O
of	O
the	O
audience	O
.	O
In	O
recent	O
years	O
,	O
memes	O
combining	O
images	O
and	O
text	O
have	O
been	O
widely	O
used	O
in	O
social	O
media	O
,	O
and	O
the	O
use	O
of	O
memes	O
can	O
easily	O
and	O
effectively	O
attract	O
a	O
large	O
number	O
of	O
users	O
on	O
social	O
platforms	O
.	O
Memes	O
are	O
one	O
of	O
the	O
most	O
popular	O
types	O
of	O
content	O
used	O
in	O
online	O
disinformation	O
campaigns	O
,	O
and	O
memes	O
applied	O
in	O
a	O
disinformation	O
campaign	O
achieve	O
their	O
purpose	O
of	O
influencing	O
users	O
through	O
rhetorical	O
and	O
psychological	O
techniques	O
.	O
Therefore	O
,	O
it	O
is	O
meaningful	O
to	O
research	O
computational	O
techniques	O
for	O
automatically	O
detecting	O
propaganda	O
in	O
particular	O
content	O
.	O
The	O
SemEval	O
2021	O
Task	O
6	O
(	O
Dimitrov	O
et	O
al	O
,	O
2021	O
)	O
consists	O
of	O
three	O
subtasks	O
:	O
Subtask	O
1	O
-	O
Given	O
only	O
the	O
"	O
textual	O
content	O
"	O
of	O
a	O
meme	O
,	O
identify	O
which	O
of	O
the	O
20	O
techniques	O
are	O
used	O
.	O
The	O
20	O
techniques	O
include	O
appeal	O
to	O
authority	O
,	O
loaded	O
language	O
,	O
and	O
name	O
calling	O
or	O
labeling	O
.	O
Subtask	O
2	O
:	O
Given	O
only	O
the	O
"	O
textual	O
content	O
"	O
of	O
a	O
meme	O
,	O
identify	O
which	O
of	O
the	O
20	O
techniques	O
are	O
used	O
along	O
with	O
the	O
span	O
(	O
s	O
)	O
of	O
the	O
text	O
covered	O
by	O
each	O
technique	O
.	O
Subtask	O
3	O
:	O
Given	O
a	O
meme	O
,	O
identify	O
which	O
of	O
the	O
22	O
techniques	O
are	O
used	O
for	O
both	O
the	O
textual	O
and	O
visual	O
content	O
of	O
the	O
meme	O
.	O
These	O
22	O
technologies	O
include	O
the	O
20	O
technologies	O
in	O
subtasks	O
1	O
and	O
2	O
,	O
and	O
2	O
technologies	O
,	O
i.e.	O
,	O
transfer	O
and	O
appeal	O
to	O
(	O
strong	O
)	O
emotions	O
,	O
are	O
added	O
.	O
The	O
detection	O
of	O
propaganda	O
techniques	O
in	O
texts	O
is	O
similar	O
to	O
a	O
text	O
sentiment	B-TaskName
analysis	I-TaskName
,	O
and	O
both	O
can	O
be	O
attributed	O
to	O
text	B-TaskName
classification	I-TaskName
tasks	O
.	O
In	O
a	O
previous	O
study	O
,	O
Peng	O
et	O
al	O
(	O
2020	O
)	O
used	O
the	O
adversarial	O
learning	O
of	O
sentiment	O
word	O
representations	O
for	O
a	O
sentiment	B-TaskName
analysis	I-TaskName
.	O
A	O
tree	O
-	O
structured	O
regional	O
CNN	O
-	O
LSTM	B-MethodName
and	O
dynamic	O
routing	O
in	O
a	O
tree	O
-	O
structured	O
LSTM	B-MethodName
(	O
Wang	O
et	O
al	O
,	O
2019	O
)	O
were	O
used	O
for	O
a	O
dimensional	O
sentiment	B-TaskName
analysis	I-TaskName
.	O
In	O
previous	O
SemEval	O
competitions	O
,	O
Dao	O
et	O
al	O
(	O
2020	O
)	O
used	O
GloVe	B-MethodName
-	O
LSTM	B-MethodName
and	O
BERT	B-MethodName
-	O
LSTM	B-MethodName
models	O
,	O
and	O
Paraschiv	O
et	O
al	O
(	O
2020	O
)	O
used	O
an	O
ensemble	O
model	O
containing	O
BERT	B-MethodName
and	O
BiLSTM	B-MethodName
to	O
detect	O
both	O
spans	O
and	O
categories	O
of	O
propaganda	O
techniques	O
in	O
news	O
articles	O
.	O
In	O
addition	O
,	O
in	O
multimodal	O
analysis	O
combining	O
images	O
and	O
text	O
,	O
Yuan	O
et	O
al	O
(	O
2020	O
)	O
proposed	O
a	O
parallel	O
channel	O
ensemble	O
model	O
combining	O
BERT	B-MethodName
embedding	O
,	O
BiLSTM	B-MethodName
,	O
attention	O
and	O
CNN	O
,	O
and	O
ResNet	B-MethodName
for	O
a	O
sentiment	B-TaskName
analysis	I-TaskName
of	O
memes	O
.	O
Li	O
et	O
al	O
(	O
2019	O
)	O
proposed	O
a	O
Visual	O
BERT	B-MethodName
model	O
that	O
aligns	O
and	O
fuses	O
text	O
and	O
image	O
information	O
using	O
transformers	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
three	O
different	O
systems	O
for	O
the	O
three	O
subtasks	O
in	O
SemEval	O
-	O
2021	O
Task	O
6	O
.	O
For	O
subtask	O
1	O
,	O
we	O
added	O
a	O
Text	O
-	O
CNN	O
layer	O
after	O
the	O
pre	O
-	O
trained	O
model	O
ALBERT	B-MethodName
to	O
fine	O
-	O
tune	O
it	O
for	O
a	O
multi	B-TaskName
-	I-TaskName
label	I-TaskName
classification	I-TaskName
of	O
text	O
.	O
For	O
subtask	O
2	O
,	O
we	O
used	O
the	O
idea	O
of	O
partitioning	O
to	O
transform	O
the	O
problem	O
into	O
the	O
detection	O
of	O
20	O
techniques	O
for	O
each	O
text	O
separately	O
.	O
BERT	B-MethodName
was	O
used	O
in	O
the	O
model	O
for	O
text	O
feature	O
extraction	O
followed	O
by	O
multi	O
-	O
task	O
sequence	O
labeling	O
,	O
and	O
the	O
results	O
of	O
each	O
task	O
were	O
combined	O
to	O
obtain	O
the	O
final	O
results	O
.	O
For	O
subtask	O
3	O
,	O
we	O
built	O
the	O
system	O
using	O
a	O
parallel	O
channel	O
model	O
containing	O
text	O
and	O
image	O
channels	O
.	O
The	O
text	O
channel	O
used	O
both	O
the	O
ALBERT	B-MethodName
and	O
Text	O
-	O
CNN	O
models	O
to	O
extract	O
features	O
of	O
text	O
in	O
the	O
meme	O
,	O
and	O
the	O
image	O
channel	O
used	O
ResNet	B-MethodName
and	O
VGGNet	O
for	O
image	O
feature	O
extraction	O
.	O
The	O
information	O
extracted	O
by	O
the	O
two	O
parallel	O
channels	O
was	O
then	O
combined	O
through	O
a	O
fully	O
connected	O
layer	O
after	O
concatenation	O
.	O
Using	O
micro	O
F	O
1	O
-	O
scores	O
as	O
metrics	O
,	O
the	O
results	O
of	O
the	O
proposed	O
model	O
in	O
subtasks	O
1	O
,	O
2	O
,	O
and	O
3	O
were	O
0.625	O
,	O
0.215	O
,	O
and	O
0.636	O
,	O
respectively	O
,	O
on	O
the	O
dev	O
set	O
.	O
The	O
remainder	O
of	O
this	O
paper	O
is	O
organized	O
as	O
follows	O
.	O
First	O
,	O
section	O
2	O
describes	O
the	O
details	O
of	O
the	O
w1	O
,	O
w2	O
,	O
w3	O
,	O
w4	O
,	O
…	O
…	O
,	O
wn	O
-	O
1	O
,	O
wn	O
ALBERT	B-MethodName
and	O
Text	O
-	O
CNN	O
used	O
in	O
our	O
system	O
.	O
Section	O
3	O
then	O
presents	O
the	O
experimental	O
results	O
.	O
Finally	O
,	O
some	O
concluding	O
remarks	O
are	O
presented	O
in	O
section	O
4	O
.	O

Subtask	O
2	O
was	O
a	O
multi	O
-	O
label	O
sequence	O
-	O
labeling	O
task	O
.	O
We	O
built	O
the	O
model	O
by	O
converting	O
the	O
problem	O
to	O
detect	O
the	O
coverage	O
of	O
each	O
propagation	O
technique	O
separately	O
for	O
the	O
input	O
sequence	O
,	O
and	O
built	O
a	O
multi	O
-	O
task	O
sequence	O
labeling	O
model	O
based	O
on	O
a	O
fine	O
-	O
tuning	O
of	O
BERT	B-MethodName
.	O
As	O
illustrated	O
in	O
Figure	O
3	O
,	O
the	O
input	O
sequence	O
was	O
first	O
obtained	O
using	O
the	O
pre	O
-	O
trained	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
model	O
with	O
a	O
hidden	O
representation	O
matrix	O
with	O
dimensions	O
of	O
512	O
×	O
768	O
.	O
Subsequently	O
,	O
20	O
parallel	O
fully	O
connected	O
layers	O
were	O
input	O
separately	O
for	O
the	O
detection	O
of	O
each	O
propaganda	O
technique	O
coverage	O
span	O
(	O
For	O
each	O
propagation	O
technique	O
,	O
the	O
sequence	O
labeling	O
task	O
is	O
performed	O
separately	O
for	O
the	O
input	O
text	O
)	O
.	O
For	O
each	O
technique	O
,	O
the	O
intermediate	O
result	O
of	O
each	O
parallel	O
channel	O
output	O
is	O
a	O
512	O
×	O
41	O
matrix	O
,	O
and	O
the	O
ensemble	O
layer	O
represents	O
the	O
stacking	O
of	O
20	O
matrices	O
from	O
20	O
parallel	O
channels	O
,	O
the	O
dimensions	O
of	O
the	O
final	O
output	O
were	O
20	O
×	O
512	O
×	O
41	O
,	O
which	O
denote	O
the	O
propaganda	O
technique	O
category	O
,	O
maximum	O
sentence	O
length	O
,	O
and	O
code	O
corresponding	O
to	O
each	O
technique	O
,	O
respectively	O
.	O

For	O
subtask	O
3	O
,	O
we	O
modeled	O
the	O
problem	O
as	O
a	O
multilabel	O
classification	O
task	O
of	O
the	O
meme	O
text	O
and	O
image	O
content	O
.	O
We	O
used	O
a	O
parallel	O
channel	O
model	O
of	O
text	O
and	O
image	O
channels	O
,	O
and	O
then	O
concatenated	O
the	O
text	O
and	O
image	O
features	O
extracted	O
by	O
the	O
two	O
parallel	O
channels	O
to	O
apply	O
multi	O
-	O
label	O
meme	B-TaskName
classification	I-TaskName
.	O
The	O
architecture	O
of	O
the	O
proposed	O
model	O
is	O
shown	O
in	O
Figure	O
4	O
.	O
Text	O
Channel	O
.	O
In	O
the	O
text	O
channel	O
,	O
we	O
used	O
the	O
ALBERT	B-MethodName
-	O
Text	O
-	O
CNN	O
model	O
used	O
in	O
subtask	O
1	O
,	O
taking	O
the	O
text	O
part	O
of	O
the	O
meme	O
content	O
as	O
an	O
input	O
to	O
obtain	O
a	O
768	O
-	O
dimensional	O
text	O
feature	O
vector	O
as	O
the	O
output	O
.	O
Image	O
Channel	O
.	O
In	O
the	O
image	O
channel	O
,	O
we	O
used	O
ResNet	B-MethodName
and	O
VGGNet	O
,	O
taking	O
the	O
image	O
part	O
of	O
the	O
meme	O
content	O
as	O
input	O
to	O
obtain	O
a	O
512	O
-	O
dimensional	O
image	O
feature	O
vector	O
as	O
the	O
output	O
.	O
The	O
ResNet	B-MethodName
model	O
(	O
He	O
et	O
al	O
,	O
2016	O
)	O
is	O
a	O
deep	O
residual	O
learning	O
model	O
for	O
image	B-TaskName
recognition	I-TaskName
,	O
and	O
presents	O
the	O
interlayer	O
residual	O
jump	O
connection	O
and	O
solves	O
the	O
deep	O
vanishing	O
gradient	O
problem	O
.	O
VGGNet	O
(	O
Simonyan	O
and	O
Zisserman	O
,	O
2015	O
)	O
is	O
a	O
deep	O
convolutional	O
neural	O
network	O
with	O
small	O
-	O
sized	O
convolutional	O
kernels	O
and	O
a	O
regular	O
network	O
structure	O
,	O
in	O
which	O
the	O
size	O
of	O
the	O
convolution	B-MethodName
kernels	O
used	O
in	O
VGG16	O
in	O
our	O
experiment	O
is	O
3	O
×	O
3	O
,	O
and	O
the	O
pooling	O
kernels	O
is	O
2	O
×	O
2	O
.	O
Furthermore	O
,	O
only	O
the	O
structures	O
of	O
the	O
ResNet	B-MethodName
and	O
VGGNet	O
were	O
used	O
in	O
our	O
experiment	O
,	O
and	O
the	O
pre	O
-	O
training	O
weights	O
were	O
not	O
applied	O
.	O

In	O
this	O
paper	O
,	O
we	O
presented	O
our	O
system	O
for	O
the	O
SemEval	O
-	O
2021	O
Task	O
6	O
,	O
the	O
experimental	O
results	O
in	O
subtasks	O
1	O
and	O
3	O
show	O
that	O
our	O
proposed	O
ALBERT	B-MethodName
-	O
Text	O
-	O
CNN	O
model	O
and	O
the	O
parallel	O
channel	O
model	O
achieved	O
a	O
good	O
performance	O
in	O
the	O
detection	O
of	O
persuasion	O
techniques	O
in	O
texts	O
and	O
images	O
.	O
We	O
participated	O
in	O
all	O
three	O
subtasks	O
and	O
achieved	O
the	O
12th	O
,	O
7th	O
,	O
and	O
11th	O
places	O
in	O
the	O
test	O
set	O
,	O
respectively	O
.	O
In	O
a	O
future	O
study	O
,	O
to	O
improve	O
the	O
generalization	O
ability	O
of	O
the	O
model	O
,	O
we	O
will	O
focus	O
on	O
how	O
to	O
deal	O
with	O
the	O
problems	O
caused	O
by	O
unbalanced	O
training	O
data	O
.	O

PubMedQA	B-DatasetName
:	O
A	O
Dataset	O
for	O
Biomedical	O
Research	O
Question	B-TaskName
Answering	I-TaskName

A	O
long	O
-	O
term	O
goal	O
of	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
is	O
to	O
build	O
intelligent	O
systems	O
that	O
can	O
reason	O
and	O
infer	O
over	O
natural	O
language	O
.	O
The	O
question	B-TaskName
answering	I-TaskName
(	O
QA	O
)	O
task	O
,	O
in	O
which	O
models	O
learn	O
how	O
to	O
answer	O
questions	O
,	O
is	O
often	O
used	O
as	O
a	O
benchmark	O
for	O
quantitatively	O
measuring	O
the	O
reasoning	O
and	O
inferring	O
abilities	O
of	O
such	O
intelligent	O
systems	O
.	O
While	O
many	O
large	O
-	O
scale	O
annotated	O
general	O
domain	O
QA	O
datasets	O
have	O
been	O
introduced	O
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
;	O
Lai	O
et	O
al	O
,	O
2017	O
;	O
Kočiskỳ	O
Question	O
:	O
Do	O
preoperative	O
statins	O
reduce	O
atrial	O
fibrillation	O
after	O
coronary	O
artery	O
bypass	O
grafting	O
?	O
Context	O
:	O
(	O
Objective	O
)	O
Recent	O
studies	O
have	O
demonstrated	O
that	O
statins	O
have	O
pleiotropic	O
effects	O
,	O
including	O
anti	O
-	O
inflammatory	O
effects	O
and	O
atrial	O
fibrillation	O
(	O
AF	O
)	O
preventive	O
effects	O
[	O
...	O
]	O
(	O
Methods	O
)	O
221	O
patients	O
underwent	O
CABG	O
in	O
our	O
hospital	O
from	O
2004	O
to	O
2007	O
.	O
14	O
patients	O
with	O
preoperative	O
AF	O
and	O
4	O
patients	O
with	O
concomitant	O
valve	O
surgery	O
[	O
...	O
]	O
(	O
Results	O
)	O
The	O
overall	O
incidence	O
of	O
postoperative	O
AF	O
was	O
26	O
%	O
.	O
Postoperative	O
AF	O
was	O
significantly	O
lower	O
in	O
the	O
Statin	O
group	O
compared	O
with	O
the	O
Non	O
-	O
statin	O
group	O
(	O
16	O
%	O
versus	O
33	O
%	O
,	O
p=0.005	O
)	O
.	O
Multivariate	O
analysis	O
demonstrated	O
that	O
independent	O
predictors	O
of	O
AF	O
[	O
...	O
]	O
Long	O
Answer	O
:	O
(	O
Conclusion	O
)	O
Our	O
study	O
indicated	O
that	O
preoperative	O
statin	O
therapy	O
seems	O
to	O
reduce	O
AF	O
development	O
after	O
CABG	O
.	O
Answer	O
:	O
yes	O
Figure	O
1	O
:	O
An	O
instance	O
(	O
Sakamoto	O
et	O
al	O
,	O
2011	O
)	O
of	O
Pub	O
-	O
MedQA	O
dataset	O
:	O
Question	O
is	O
the	O
original	O
question	O
title	O
;	O
Context	O
includes	O
the	O
structured	O
abstract	O
except	O
its	O
conclusive	O
part	O
,	O
which	O
serves	O
as	O
the	O
Long	O
Answer	O
;	O
Human	O
experts	O
annotated	O
the	O
Answer	O
yes	O
.	O
Supporting	O
fact	O
for	O
the	O
answer	O
is	O
highlighted	O
.	O
Yang	O
et	O
al	O
,	O
2018	O
;	O
,	O
the	O
largest	O
annotated	O
biomedical	O
QA	O
dataset	O
,	O
BioASQ	B-DatasetName
(	O
Tsatsaronis	O
et	O
al	O
,	O
2015	O
)	O
has	O
less	O
than	O
3k	O
training	O
instances	O
,	O
most	O
of	O
which	O
are	O
simple	O
factual	O
questions	O
.	O
Some	O
works	O
proposed	O
automatically	O
constructed	O
biomedical	O
QA	O
datasets	O
(	O
Pampari	O
et	O
al	O
,	O
2018	O
;	O
Pappas	O
et	O
al	O
,	O
2018	O
;	O
Kim	O
et	O
al	O
,	O
2018	O
)	O
,	O
which	O
have	O
much	O
larger	O
sizes	O
.	O
However	O
,	O
questions	O
of	O
these	O
datasets	O
are	O
mostly	O
factoid	O
,	O
whose	O
answers	O
can	O
be	O
extracted	O
in	O
the	O
contexts	O
without	O
much	O
reasoning	O
.	O
In	O
this	O
paper	O
,	O
we	O
aim	O
at	O
building	O
a	O
biomedical	O
QA	O
dataset	O
which	O
(	O
1	O
)	O
has	O
substantial	O
instances	O
with	O
some	O
expert	O
annotations	O
and	O
(	O
2	O
)	O
requires	O
reasoning	O
over	O
the	O
contexts	O
to	O
answer	O
the	O
questions	O
.	O
For	O
this	O
,	O
we	O
turn	O
to	O
the	O
PubMed	O
1	O
,	O
a	O
search	O
engine	O
providing	O
access	O
to	O
over	O
25	O
million	O
references	O
of	O
biomedical	O
articles	O
.	O
We	O
found	O
that	O
around	O
760k	O
articles	O
in	O
PubMed	O
use	O
questions	O
as	O
their	O
titles	O
.	O
Among	O
them	O
,	O
the	O
abstracts	O
of	O
about	O
120k	O
articles	O
are	O
written	O
in	O
a	O
structured	O
style	O
-	O
meaning	O
they	O
have	O
subsections	O
of	O
"	O
Introduction	O
"	O
,	O
"	O
Results	O
"	O
etc	O
.	O
Conclusive	O
parts	O
of	O
the	O
abstracts	O
,	O
often	O
in	O
"	O
Conclusions	O
"	O
,	O
are	O
the	O
authors	O
'	O
answers	O
to	O
the	O
question	O
title	O
.	O
Other	O
abstract	O
parts	O
can	O
be	O
viewed	O
as	O
the	O
contexts	O
for	O
giving	O
such	O
answers	O
.	O
This	O
pattern	O
perfectly	O
fits	O
the	O
scheme	O
of	O
QA	O
,	O
but	O
modeling	O
it	O
as	O
abstractive	O
QA	O
,	O
where	O
models	O
learn	O
to	O
generate	O
the	O
conclusions	O
,	O
will	O
result	O
in	O
an	O
extremely	O
hard	O
task	O
due	O
to	O
the	O
variability	O
of	O
writing	O
styles	O
.	O
Interestingly	O
,	O
more	O
than	O
half	O
of	O
the	O
question	O
titles	O
of	O
PubMed	O
articles	O
can	O
be	O
briefly	O
answered	O
by	O
yes	O
/	O
no	O
/	O
maybe	O
,	O
which	O
is	O
significantly	O
higher	O
than	O
the	O
proportions	O
of	O
such	O
questions	O
in	O
other	O
datasets	O
,	O
e.g.	O
:	O
just	O
1	O
%	O
in	O
Natural	B-DatasetName
Questions	I-DatasetName
and	O
6	O
%	O
in	O
HotpotQA	B-DatasetName
(	O
Yang	O
et	O
al	O
,	O
2018	O
)	O
.	O
Instead	O
of	O
using	O
conclusions	O
to	O
answer	O
the	O
questions	O
,	O
we	O
explore	O
answering	O
them	O
with	O
yes	O
/	O
no	O
/	O
maybe	O
and	O
treat	O
the	O
conclusions	O
as	O
a	O
long	O
answer	O
for	O
additional	O
supervision	O
.	O
To	O
this	O
end	O
,	O
we	O
present	O
PubMedQA	B-DatasetName
,	O
a	O
biomedical	O
QA	O
dataset	O
for	O
answering	O
research	O
questions	O
using	O
yes	O
/	O
no	O
/	O
maybe	O
.	O
We	O
collected	O
all	O
PubMed	O
articles	O
with	O
question	O
titles	O
,	O
and	O
manually	O
labeled	O
1k	O
of	O
them	O
for	O
cross	O
-	O
validation	O
and	O
testing	O
.	O
An	O
example	O
is	O
shown	O
in	O
Fig	O
.	O
1	O
.	O
The	O
rest	O
of	O
yes	O
/	O
no	O
/	O
answerable	O
QA	O
instances	O
compose	O
of	O
the	O
unlabeled	O
subset	O
which	O
can	O
be	O
used	O
for	O
semisupervised	O
learning	O
.	O
Further	O
,	O
we	O
automatically	O
convert	O
statement	O
titles	O
of	O
211.3k	O
PubMed	O
articles	O
to	O
questions	O
and	O
label	O
them	O
with	O
yes	O
/	O
no	O
answers	O
using	O
a	O
simple	O
heuristic	O
.	O
These	O
artificially	O
generated	O
instances	O
can	O
be	O
used	O
for	O
pre	O
-	O
training	O
.	O
Unlike	O
other	O
QA	O
datasets	O
in	O
which	O
questions	O
are	O
asked	O
by	O
crowd	O
-	O
workers	O
for	O
existing	O
contexts	O
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
;	O
Yang	O
et	O
al	O
,	O
2018	O
;	O
Kočiskỳ	O
et	O
al	O
,	O
2018	O
)	O
,	O
in	O
PubMedQA	B-DatasetName
contexts	O
are	O
generated	O
to	O
answer	O
the	O
questions	O
and	O
both	O
are	O
written	O
by	O
the	O
same	O
authors	O
.	O
This	O
consistency	O
assures	O
that	O
contexts	O
are	O
perfectly	O
related	O
to	O
the	O
questions	O
,	O
thus	O
making	O
PubMedQA	B-DatasetName
an	O
ideal	O
benchmark	O
for	O
testing	O
scientific	O
reasoning	O
abilities	O
.	O
As	O
an	O
attempt	O
to	O
solve	O
PubMedQA	B-DatasetName
and	O
provide	O
a	O
strong	O
baseline	O
,	O
we	O
fine	O
-	O
tune	O
BioBERT	O
on	O
different	O
subsets	O
in	O
a	O
multi	O
-	O
phase	O
style	O
with	O
additional	O
supervision	O
of	O
long	O
answers	O
.	O
Though	O
this	O
model	O
generates	O
decent	O
results	O
and	O
vastly	O
outperforms	O
other	O
baselines	O
,	O
it	O
's	O
still	O
much	O
worse	O
than	O
the	O
single	O
-	O
human	O
performance	O
,	O
leaving	O
significant	O
room	O
for	O
future	O
improvements	O
.	O

Biomedical	O
QA	O
:	O
Expert	O
-	O
annotated	O
biomedical	O
QA	O
datasets	O
are	O
limited	O
by	O
scale	O
due	O
to	O
the	O
difficulty	O
of	O
annotations	O
.	O
In	O
2006	O
and	O
2007	O
,	O
TREC	B-DatasetName
2	O
held	O
QA	O
challenges	O
on	O
genomics	O
corpus	O
(	O
Hersh	O
et	O
al	O
,	O
2006	O
(	O
Hersh	O
et	O
al	O
,	O
,	O
2007	O
,	O
where	O
the	O
task	O
is	O
to	O
retrieve	O
relevant	O
documents	O
for	O
36	O
and	O
38	O
topic	O
questions	O
,	O
respectively	O
.	O
QA4MRE	O
(	O
Peñas	O
et	O
al	O
,	O
2013	O
)	O
included	O
a	O
QA	O
task	O
about	O
Alzheimer	O
's	O
disease	O
(	O
Morante	O
et	O
al	O
,	O
2012	O
)	O
.	O
This	O
dataset	O
has	O
40	O
QA	O
instances	O
and	O
the	O
task	O
is	O
to	O
answer	O
a	O
question	O
related	O
to	O
a	O
given	O
document	O
using	O
one	O
of	O
five	O
answer	O
choices	O
.	O
The	O
QA	O
task	O
of	O
BioASQ	B-DatasetName
(	O
Tsatsaronis	O
et	O
al	O
,	O
2015	O
)	O
has	O
phases	O
of	O
(	O
a	O
)	O
retrieve	O
question	O
-	O
related	O
documents	O
and	O
(	O
b	O
)	O
using	O
related	O
documents	O
as	O
contexts	O
to	O
answer	O
yes	O
/	O
no	O
,	O
factoid	O
,	O
list	O
or	O
summary	O
questions	O
.	O
BioASQ	B-DatasetName
2019	O
has	O
a	O
training	O
set	O
of	O
2	O
,	O
747	O
QA	O
instances	O
and	O
a	O
test	O
set	O
of	O
500	O
instances	O
.	O
Several	O
large	O
-	O
scale	O
automatically	O
collected	O
biomedical	O
QA	O
datasets	O
have	O
been	O
introduced	O
:	O
emrQA	B-DatasetName
(	O
Pampari	O
et	O
al	O
,	O
2018	O
)	O
is	O
an	O
extractive	O
QA	O
dataset	O
for	O
electronic	O
medical	O
records	O
(	O
EHR	O
)	O
built	O
by	O
re	O
-	O
purposing	O
existing	O
annotations	O
on	O
EHR	O
corpora	O
.	O
BioRead	O
(	O
Pappas	O
et	O
al	O
,	O
2018	O
)	O
and	O
BMKC	O
(	O
Kim	O
et	O
al	O
,	O
2018	O
)	O
both	O
collect	O
cloze	O
-	O
style	O
QA	O
instances	O
by	O
masking	O
biomedical	O
named	O
entities	O
in	O
sentences	O
of	O
research	O
articles	O
and	O
using	O
other	O
parts	O
of	O
the	O
same	O
article	O
as	O
context	O
.	O
Yes	O
/	O
No	O
QA	O
:	O
Datasets	O
such	O
as	O
HotpotQA	B-DatasetName
(	O
Yang	O
et	O
al	O
,	O
2018	O
)	O
,	O
Natural	B-DatasetName
Questions	I-DatasetName
,	O
ShARC	B-DatasetName
(	O
Saeidi	O
et	O
al	O
,	O
2018	O
)	O
and	O
BioASQ	B-DatasetName
(	O
Tsatsaronis	O
et	O
al	O
,	O
2015	O
)	O
contain	O
yes	O
/	O
no	O
questions	O
as	O
well	O
as	O
other	O
types	O
of	O
questions	O
.	O
BoolQ	B-DatasetName
(	O
Clark	O
et	O
al	O
,	O
2019	O
)	O
specifically	O
focuses	O
on	O
naturally	O
occurring	O
yes	O
/	O
no	O
questions	O
,	O
and	O
those	O
questions	O
are	O
shown	O
to	O
be	O
surprisingly	O
difficult	O
to	O
answer	O
.	O
We	O
add	O
a	O
"	O
maybe	O
"	O
choice	O
in	O
PubMedQA	B-DatasetName
to	O
cover	O
uncertain	O
instances	O
.	O
Typical	O
neural	O
approaches	O
to	O
answering	O
yes	O
/	O
no	O
questions	O
involve	O
encoding	O
both	O
the	O
question	O
and	O
context	O
,	O
and	O
decoding	O
the	O
encoding	O
to	O
a	O
class	O
output	O
,	O
which	O
is	O
similar	O
to	O
the	O
well	O
-	O
studied	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
(	O
NLI	O
)	O
task	O
.	O
Recent	O
breakthroughs	O
of	O
pre	O
-	O
trained	O
language	O
models	O
like	O
ELMo	B-MethodName
(	O
Peters	O
et	O
al	O
,	O
2018	O
)	O
and	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
show	O
significant	O
performance	O
i	O
m	O
-	O
provements	O
on	O
NLI	O
tasks	O
.	O
In	O
this	O
work	O
,	O
we	O
use	O
domain	O
specific	O
versions	O
of	O
them	O
to	O
set	O
baseline	O
performance	O
on	O
PubMedQA	B-DatasetName
.	O
3	O
PubMedQA	B-DatasetName
Dataset	O

PubMedQA	B-DatasetName
is	O
split	O
into	O
three	O
subsets	O
:	O
labeled	O
,	O
unlabeled	O
and	O
artificially	O
generated	O
.	O
They	O
are	O
denoted	O
as	O
PQA	O
-	O
L	O
(	O
abeled	O
)	O
,	O
PQA	O
-	O
U	O
(	O
nlabeled	O
)	O
and	O
PQA	O
-	O
A	O
(	O
rtificial	O
)	O
,	O
respectively	O
.	O
We	O
show	O
the	O
architecture	O
of	O
PubMedQA	B-DatasetName
dataset	O
in	O
Fig	O
.	O
2	O
.	O
Collection	O
of	O
PQA	O
-	O
L	O
and	O
PQA	O
-	O
U	O
:	O
PubMed	O
articles	O
which	O
have	O
i	O
)	O
a	O
question	O
mark	O
in	O
the	O
titles	O
and	O
ii	O
)	O
a	O
structured	O
abstract	O
with	O
conclusive	O
part	O
are	O
collected	O
and	O
denoted	O
as	O
pre	O
-	O
PQA	O
-	O
U.	O
Now	O
each	O
instance	O
has	O
1	O
)	O
a	O
question	O
which	O
is	O
the	O
original	O
title	O
2	O
)	O
a	O
context	O
which	O
is	O
the	O
structured	O
abstract	O
without	O
the	O
conclusive	O
part	O
and	O
3	O
)	O
a	O
long	O
answer	O
which	O
is	O
the	O
conclusive	O
part	O
of	O
the	O
abstract	O
.	O
Two	O
annotators	O
3	O
labeled	O
1k	O
instances	O
from	O
pre	O
-	O
PQA	O
-	O
U	O
with	O
yes	O
/	O
no	O
/	O
maybe	O
to	O
build	O
PQA	O
-	O
L	O
using	O
Algorithm	O
1	O
.	O
The	O
annotator	O
1	O
does	O
n't	O
need	O
to	O
do	O
much	O
reasoning	O
to	O
annotate	O
since	O
the	O
long	O
answer	O
is	O
available	O
.	O
We	O
denote	O
this	O
reasoning	O
-	O
free	O
setting	O
.	O
However	O
,	O
the	O
annotator	O
2	O
can	O
not	O
use	O
the	O
long	O
answer	O
,	O
so	O
reasoning	O
over	O
the	O
context	O
is	O
required	O
for	O
3	O
Both	O
are	O
qualified	O
M.D.	O
candidates	O
.	O
Algorithm	O
1	O
PQA	O
-	O
L	O
data	O
collection	O
procedure	O
Input	O
:	O
pre	O
-	O
PQA	O
-	O
U	O
ReasoningFreeAnnotation	O
{	O
}	O
ReasoningRequiredAnnotation	O
{	O
}	O
GroundTruthLabel	O
{	O
}	O
while	O
not	O
finished	O
do	O
Randomly	O
sample	O
an	O
instance	O
inst	O
from	O
pre	O
-	O
PQA	O
-	O
U	O
if	O
inst	O
is	O
not	O
yes	O
/	O
no	O
/	O
maybe	O
answerable	O
then	O
Remove	O
inst	O
and	O
continue	O
to	O
next	O
iteration	O
end	O
if	O
Annotator	O
1	O
annotates	O
inst	O
with	O
l1	O
{	O
yes	O
,	O
no	O
,	O
maybe	O
}	O
using	O
question	O
,	O
context	O
and	O
long	O
answer	O
Annotator	O
2	O
annotates	O
inst	O
with	O
l2	O
{	O
yes	O
,	O
no	O
,	O
maybe	O
}	O
using	O
question	O
and	O
context	O
if	O
l1	O
=	O
l2	O
then	O
la	O
l1	O
else	O
Annotator	O
1	O
and	O
Annotator	O
2	O
discuss	O
for	O
an	O
agreement	O
annotation	O
la	O
if	O
not	O
la	O
then	O
Remove	O
inst	O
and	O
continue	O
to	O
next	O
iteration	O
end	O
if	O
end	O
if	O
ReasoningFreeAnnotation	O
[	O
inst	O
]	O
l1	O
ReasoningRequiredAnnotation	O
[	O
inst	O
]	O
l2	O
GroundTruthLabel	O
[	O
inst	O
]	O
la	O
end	O
while	O
annotation	O
.	O
We	O
denote	O
such	O
setting	O
as	O
reasoningrequired	O
setting	O
.	O
Note	O
that	O
the	O
annotation	O
process	O
might	O
assign	O
wrong	O
labels	O
when	O
both	O
annotator	O
1	O
and	O
annotator	O
2	O
make	O
a	O
same	O
mistake	O
,	O
but	O
considering	O
human	O
performance	O
in	O
5.1	O
,	O
such	O
error	O
rate	O
could	O
be	O
as	O
low	O
as	O
1	O
%	O
4	O
.	O
500	O
randomly	O
sampled	O
PQA	O
-	O
L	O
instances	O
are	O
used	O
for	O
10	O
-	O
fold	O
cross	O
validation	O
and	O
the	O
rest	O
500	O
instances	O
consist	O
of	O
Pub	O
-	O
MedQA	O
test	O
set	O
.	O
Further	O
,	O
we	O
include	O
the	O
unlabeled	O
instances	O
in	O
pre	O
-	O
PQA	O
-	O
U	O
with	O
yes	O
/	O
no	O
/	O
maybe	O
answerable	O
questions	O
to	O
build	O
PQA	O
-	O
U.	O
For	O
this	O
,	O
we	O
use	O
a	O
simple	O
rule	O
-	O
based	O
method	O
which	O
removes	O
all	O
questions	O
started	O
with	O
interrogative	O
words	O
(	O
i.e.	O
wh	O
-	O
words	O
)	O
or	O
involving	O
selections	O
from	O
multiple	O
entities	O
.	O
This	O
results	O
in	O
over	O
93	O
%	O
agreement	O
with	O
annotator	O
1	O
in	O
identifying	O
the	O
questions	O
that	O
can	O
be	O
answered	O
by	O
yes	O
/	O
no	O
/	O
maybe	O
.	O
statement	O
titles	O
are	O
converted	O
to	O
questions	O
by	O
simply	O
moving	O
or	O
adding	O
copulas	O
(	O
"	O
is	O
"	O
,	O
"	O
are	O
"	O
)	O
or	O
auxiliary	O
verbs	O
(	O
"	O
does	O
"	O
,	O
"	O
do	O
"	O
)	O
in	O
the	O
front	O
and	O
further	O
revising	O
for	O
coherence	O
(	O
e.g.	O
:	O
adding	O
a	O
question	O
mark	O
)	O
.	O
We	O
generate	O
the	O
yes	O
/	O
no	O
answer	O
according	O
to	O
negation	O
status	O
of	O
the	O
VB	O
.	O
Several	O
examples	O
are	O
shown	O
in	O
Table	O
2	O
.	O
We	O
collected	O
211.3k	O
instances	O
for	O
PQA	O
-	O
A	O
,	O
of	O
which	O
200k	O
randomly	O
sampled	O
instances	O
are	O
for	O
training	O
and	O
the	O
rest	O
11.3k	O
instances	O
are	O
for	O
validation	O
.	O

We	O
show	O
the	O
basic	O
statistics	O
of	O
three	O
PubMedQA	B-DatasetName
subsets	O
in	O
Table	O
1	O
.	O
Instance	O
Topics	O
:	O
PubMed	O
abstracts	O
are	O
manually	O
annotated	O
by	O
medical	O
librarians	O
with	O
Medical	O
Subject	O
Headings	O
(	O
MeSH	O
)	O
6	O
,	O
which	O
is	O
a	O
controlled	O
vocabulary	O
designed	O
to	O
describe	O
the	O
topics	O
of	O
biomedical	O
texts	O
.	O
We	O
use	O
MeSH	O
terms	O
to	O
represent	O
abstract	O
topics	O
,	O
and	O
visualize	O
their	O
distribution	O
in	O
Fig	O
.	O
3	O
.	O
Nearly	O
all	O
instances	O
are	O
human	O
studies	O
and	O
they	O
cover	O
a	O
wide	O
variety	O
of	O
topics	O
,	O
including	O
retrospective	O
,	O
prospective	O
,	O
and	O
cohort	O
studies	O
,	O
different	O
age	O
groups	O
,	O
and	O
healthcare	O
-	O
related	O
subjects	O
like	O
treatment	O
outcome	O
,	O
prognosis	O
and	O
risk	O
factors	O
of	O
diseases	O
.	O
We	O
use	O
a	O
Sankey	O
diagram	O
to	O
show	O
the	O
proportional	O
relationships	O
between	O
corresponded	O
question	O
type	O
and	O
reasoning	O
type	O
,	O
as	O
well	O
as	O
corresponded	O
reasoning	O
type	O
and	O
whether	O
there	O
are	O
text	O
interpretations	O
of	O
numbers	O
in	O
Fig	O
.	O
4	O
.	O

The	O
main	O
metrics	O
of	O
PubMedQA	B-DatasetName
are	O
accuracy	B-MetricName
and	O
macro	B-MetricName
-	I-MetricName
F1	I-MetricName
on	O
PQA	O
-	O
L	O
test	O
set	O
using	O
question	O
and	O
context	O
as	O
input	O
.	O
We	O
denote	O
prediction	O
using	O
question	O
and	O
context	O
as	O
a	O
reasoning	O
-	O
required	O
setting	O
,	O
because	O
under	O
this	O
setting	O
answers	O
are	O
not	O
directly	O
expressed	O
in	O
the	O
input	O
and	O
reasoning	O
over	O
the	O
contexts	O
is	O
required	O
to	O
answer	O
the	O
question	O
.	O
Additionally	O
,	O
long	O
answers	O
are	O
available	O
at	O
training	O
time	O
,	O
so	O
generation	O
or	O
prediction	O
of	O
them	O
can	O
be	O
used	O
as	O
an	O
auxiliary	O
task	O
in	O
this	O
setting	O
.	O
A	O
parallel	O
setting	O
,	O
where	O
models	O
can	O
use	O
question	O
and	O
long	O
answer	O
to	O
predict	O
yes	O
/	O
no	O
/	O
maybe	O
answer	O
,	O
is	O
denoted	O
as	O
reasoning	O
-	O
free	O
setting	O
since	O
yes	O
/	O
no	O
/	O
maybe	O
are	O
usually	O
explicitly	O
expressed	O
in	O
the	O
long	O
answers	O
(	O
i.e.	O
:	O
conclusions	O
of	O
the	O
abstracts	O
)	O
.	O
Obviously	O
,	O
it	O
's	O
a	O
much	O
easier	O
setting	O
which	O
can	O
be	O
exploited	O
for	O
bootstrapping	O
PQA	O
-	O
U.	O

Majority	O
:	O
The	O
majority	O
(	O
about	O
55	O
%	O
)	O
of	O
the	O
instances	O
have	O
the	O
label	O
"	O
yes	O
"	O
.	O
We	O
use	O
a	O
trivial	O
baseline	O
denoted	O
as	O
Majority	O
where	O
we	O
simply	O
predict	O
"	O
yes	O
"	O
for	O
all	O
instances	O
,	O
regardless	O
of	O
the	O
question	O
and	O
context	O
.	O
Shallow	O
Features	O
:	O
For	O
each	O
instance	O
,	O
we	O
include	O
the	O
following	O
shallow	O
features	O
:	O
1	O
)	O
TF	O
-	O
IDF	O
statistics	O
of	O
the	O
question	O
2	O
)	O
TF	O
-	O
IDF	O
statistics	O
of	O
the	O
context	O
/	O
long	O
answer	O
and	O
3	O
)	O
sum	O
of	O
IDF	O
of	O
the	O
overlapping	O
non	O
-	O
stop	O
words	O
between	O
the	O
question	O
and	O
the	O
context	O
/	O
long	O
answer	O
.	O
To	O
allow	O
multi	O
-	O
phase	O
fine	O
-	O
tuning	O
,	O
we	O
apply	O
a	O
feed	O
-	O
forward	O
neural	O
network	O
on	O
the	O
shallow	O
features	O
instead	O
of	O
using	O
a	O
logistic	O
classifier	O
.	O
BiLSTM	B-MethodName
:	O
We	O
simply	O
concatenate	O
the	O
question	O
and	O
context	O
/	O
long	O
answer	O
with	O
learnable	O
segment	O
embeddings	O
appended	O
to	O
the	O
biomedical	O
word2vec	O
embeddings	O
(	O
Pyysalo	O
et	O
al	O
,	O
2013	O
)	O
of	O
each	O
token	O
.	O
The	O
concatenated	O
sentence	O
is	O
then	O
fed	O
to	O
a	O
biL	O
-	O
STM	O
,	O
and	O
the	O
final	O
hidden	O
states	O
of	O
the	O
forward	O
and	O
backward	O
network	O
are	O
used	O
for	O
classifying	O
the	O
yes	O
/	O
no	O
/	O
maybe	O
label	O
.	O
ESIM	B-MethodName
with	O
BioELMo	O
:	O
Following	O
the	O
state	O
-	O
ofthe	O
-	O
art	O
recurrent	O
architecture	O
of	O
NLI	O
(	O
Peters	O
et	O
al	O
,	O
2018	O
)	O
,	O
we	O
use	O
pre	O
-	O
trained	O
biomedical	O
contextualized	O
embeddings	O
BioELMo	O
(	O
Jin	O
et	O
al	O
,	O
2019	O
)	O
for	O
word	O
representations	O
.	O
Then	O
we	O
apply	O
the	O
ESIM	B-MethodName
model	O
(	O
Chen	O
et	O
al	O
,	O
2016	O
)	O
,	O
where	O
a	O
biLSTM	B-MethodName
is	O
used	O
to	O
encode	O
the	O
question	O
and	O
context	O
/	O
long	O
answer	O
,	O
followed	O
by	O
an	O
attentional	O
local	O
inference	O
layer	O
and	O
a	O
biLSTM	B-MethodName
inference	O
composition	O
layer	O
.	O
After	O
pooling	O
,	O
a	O
softmax	B-MethodName
output	O
unit	O
is	O
applied	O
for	O
predicting	O
the	O
yes	O
/	O
no	O
/	O
maybe	O
label	O
.	O

We	O
present	O
PubMedQA	B-DatasetName
,	O
a	O
novel	O
dataset	O
aimed	O
at	O
biomedical	O
research	O
question	B-TaskName
answering	I-TaskName
using	O
yes	O
/	O
no	O
/	O
maybe	O
,	O
where	O
complex	O
quantitative	O
reasoning	O
is	O
required	O
to	O
solve	O
the	O
task	O
.	O
PubMedQA	B-DatasetName
has	O
substantial	O
automatically	O
collected	O
instances	O
as	O
well	O
as	O
the	O
largest	O
size	O
of	O
expert	O
annotated	O
yes	O
/	O
no	O
/	O
maybe	O
questions	O
in	O
biomedical	O
domain	O
.	O
We	O
provide	O
a	O
strong	O
baseline	O
using	O
multi	O
-	O
phase	O
fine	O
-	O
tuning	O
of	O
BioBERT	O
with	O
long	O
answer	O
as	O
additional	O
supervision	O
,	O
but	O
it	O
's	O
still	O
much	O
worse	O
than	O
just	O
single	O
human	O
performance	O
.	O
There	O
are	O
several	O
interesting	O
future	O
directions	O
to	O
explore	O
on	O
PubMedQA	B-DatasetName
,	O
e.g.	O
:	O
(	O
1	O
)	O
about	O
21	O
%	O
of	O
PubMedQA	B-DatasetName
contexts	O
contain	O
no	O
natural	O
language	O
descriptions	O
of	O
numbers	O
,	O
so	O
how	O
to	O
properly	O
handle	O
these	O
numbers	O
is	O
worth	O
studying	O
;	O
(	O
2	O
)	O
we	O
use	O
binary	O
BoW	O
statistics	O
prediction	O
as	O
a	O
simple	O
demonstration	O
for	O
additional	O
supervision	O
of	O
long	O
answers	O
.	O
Learning	O
a	O
harder	O
but	O
more	O
informative	O
auxiliary	O
task	O
of	O
long	O
answer	B-TaskName
generation	I-TaskName
might	O
lead	O
to	O
further	O
improvements	O
.	O
Articles	O
of	O
PubMedQA	B-DatasetName
are	O
biased	O
towards	O
clinical	O
study	O
-	O
related	O
topics	O
(	O
described	O
in	O
Appendix	O
B	O
)	O
,	O
so	O
PubMedQA	B-DatasetName
has	O
the	O
potential	O
to	O
assist	O
evidence	O
-	O
based	O
medicine	O
,	O
which	O
seeks	O
to	O
make	O
clinical	O
decisions	O
based	O
on	O
evidence	O
of	O
high	O
quality	O
clinical	O
studies	O
.	O
Generally	O
,	O
PubMedQA	B-DatasetName
can	O
serve	O
as	O
a	O
benchmark	O
for	O
testing	O
scientific	O
reasoning	O
abilities	O
of	O
machine	B-TaskName
reading	I-TaskName
comprehension	I-TaskName
models	O
.	O
7	O
Acknowledgement	O

Clinical	O
study	O
-	O
related	O
topics	O
are	O
over	O
-	O
represented	O
in	O
PubMedQA	B-DatasetName
:	O
we	O
found	O
proportions	O
of	O
MeSH	O
terms	O
like	O
:	O
"	O
Pregnancy	O
Outcome	O
"	O
"	O
Socioeconomic	O
Factors	O
"	O
"	O
Risk	O
Assessment	O
"	O
"	O
Survival	B-TaskName
Analysis	I-TaskName
"	O
"	O
Prospective	O
Studies	O
"	O
"	O
Case	O
-	O
Control	O
Studies	O
"	O
"	O
Reference	O
Values	O
"	O
are	O
significantly	O
higher	O
in	O
the	O
PubMedQA	B-DatasetName
articles	O
than	O
those	O
in	O
200k	O
most	O
recent	O
general	O
PubMed	O
articles	O
(	O
significance	O
is	O
defined	O
by	O
p	O
<	O
0.05	O
in	O
twoproportion	O
z	O
-	O
test	O
)	O
.	O

Misogyny	O
is	O
a	O
problem	O
in	O
many	O
online	O
spaces	O
,	O
making	O
them	O
less	O
welcoming	O
,	O
safe	O
,	O
and	O
accessible	O
for	O
women	O
.	O
Women	O
have	O
been	O
shown	O
to	O
be	O
twice	O
as	O
likely	O
as	O
men	O
to	O
experience	O
gender	O
-	O
based	O
online	O
harassment	O
(	O
Duggan	O
,	O
2017	O
)	O
.	O
This	O
misogyny	O
can	O
inflict	O
serious	O
psychological	O
harm	O
on	O
women	O
and	O
produce	O
a	O
'	O
silencing	O
effect	O
'	O
,	O
whereby	O
women	O
selfcensor	O
or	O
withdraw	O
from	O
online	O
spaces	O
entirely	O
,	O
thus	O
limiting	O
their	O
freedom	O
of	O
expression	O
(	O
Mantilla	O
,	O
2013	O
;	O
International	O
,	O
2017	O
)	O
.	O
Tackling	O
such	O
content	O
is	O
increasingly	O
a	O
priority	O
for	O
social	O
media	O
platforms	O
and	O
civil	O
society	O
organisations	O
.	O
However	O
,	O
detecting	O
online	O
misogyny	O
remains	O
a	O
difficult	O
task	O
(	O
Hewitt	O
et	O
al	O
,	O
2016	O
;	O
Nozza	O
et	O
al	O
,	O
2019	O
)	O
.	O
One	O
problem	O
is	O
the	O
lack	O
of	O
high	O
-	O
quality	O
datasets	O
to	O
train	O
machine	O
learning	O
models	O
,	O
which	O
would	O
enable	O
the	O
creation	O
of	O
efficient	O
and	O
scalable	O
automated	O
detection	O
systems	O
.	O
Previous	O
research	O
has	O
primarily	O
used	O
Twitter	O
data	O
and	O
there	O
is	O
a	O
pressing	O
need	O
for	O
other	O
platforms	O
to	O
be	O
researched	O
Lynn	O
et	O
al	O
(	O
2019a	O
)	O
.	O
Notably	O
,	O
de	O
-	O
spite	O
social	O
scientific	O
studies	O
that	O
show	O
online	O
misogyny	O
is	O
pervasive	O
on	O
some	O
Reddit	B-DatasetName
communities	O
,	O
to	O
date	O
a	O
training	O
dataset	O
for	O
misogyny	O
has	O
not	O
been	O
created	O
with	O
Reddit	B-DatasetName
data	O
.	O
In	O
this	O
paper	O
we	O
seek	O
to	O
address	O
the	O
limitations	O
of	O
previous	O
research	O
by	O
presenting	O
a	O
dataset	O
of	O
Reddit	B-DatasetName
content	O
with	O
expert	O
labels	O
for	O
misogyny	O
that	O
can	O
be	O
used	O
to	O
develop	O
more	O
accurate	O
and	O
nuanced	O
classification	O
models	O
.	O
Our	O
contributions	O
are	O
four	O
-	O
fold	O
.	O
First	O
,	O
we	O
develop	O
a	O
detailed	O
hierarchical	O
taxonomy	O
based	O
on	O
existing	O
literature	O
on	O
online	O
misogyny	O
.	O
Second	O
,	O
we	O
create	O
and	O
share	O
a	O
detailed	O
codebook	O
used	O
to	O
train	O
annotators	O
to	O
identify	O
different	O
types	O
of	O
misogyny	O
.	O
Third	O
,	O
we	O
present	O
a	O
dataset	O
of	O
6	O
,	O
383	O
entries	O
from	O
Reddit	B-DatasetName
.	O
Fourth	O
,	O
we	O
create	O
baseline	O
classification	O
models	O
based	O
on	O
these	O
datasets	O
.	O
All	O
of	O
the	O
research	O
artefacts	O
are	O
made	O
freely	O
available	O
via	O
a	O
public	O
repository	O
for	O
future	O
researchers	O
.	O
1	O
The	O
dataset	O
itself	O
has	O
several	O
innovations	O
which	O
differentiate	O
it	O
from	O
previous	O
training	O
datasets	O
for	O
misogyny	O
.	O
First	O
,	O
we	O
use	O
chronological	O
and	O
structured	O
conversation	O
threads	O
,	O
which	O
mean	O
annotators	O
take	O
into	O
account	O
the	O
previous	O
context	O
of	O
each	O
entry	O
before	O
labelling	O
.	O
Second	O
,	O
we	O
distinguish	O
between	O
conceptually	O
distinct	O
types	O
of	O
misogynistic	O
abuse	O
,	O
including	O
gendered	O
personal	O
attacks	O
,	O
use	O
of	O
misogynistic	O
pejoratives	O
,	O
and	O
derogatory	O
and	O
threatening	O
language	O
.	O
Third	O
,	O
we	O
highlight	O
the	O
specific	O
section	O
of	O
text	O
,	O
also	O
known	O
as	O
a	O
'	O
span	O
'	O
,	O
on	O
which	O
each	O
label	O
is	O
based	O
.	O
This	O
helps	O
differentiate	O
between	O
multiple	O
labels	O
on	O
one	O
piece	O
of	O
text	O
.	O
Fourth	O
,	O
we	O
use	O
trained	O
annotators	O
,	O
rather	O
than	O
crowd	O
-	O
sourced	O
workers	O
.	O
We	O
also	O
use	O
facilitated	O
meetings	O
to	O
decide	O
the	O
final	O
labels	O
rather	O
than	O
just	O
a	O
majority	O
decision	O
.	O
Both	O
of	O
these	O
factors	O
lead	O
to	O
a	O
high	O
-	O
quality	O
dataset	O
.	O
Additionally	O
,	O
we	O
provide	O
a	O
second	O
dataset	O
with	O
the	O
original	O
labels	O
made	O
by	O
annotators	O
before	O
the	O
final	O
labels	O
were	O
decided	O
.	O

We	O
collected	O
conversation	O
threads	O
from	O
Reddit	B-DatasetName
.	O
Given	O
that	O
a	O
very	O
small	O
amount	O
of	O
content	O
on	O
social	O
media	O
is	O
hateful	O
,	O
a	O
key	O
difficulty	O
when	O
creating	O
datasets	O
for	O
annotation	O
is	O
collecting	O
enough	O
instances	O
of	O
the	O
'	O
positive	O
'	O
class	O
to	O
be	O
useful	O
for	O
machine	O
learning	O
(	O
Schmidt	O
and	O
Wiegand	O
,	O
2017	O
;	O
Fortuna	O
and	O
Nunes	O
,	O
2018	O
)	O
.	O
However	O
,	O
sampling	O
strategies	O
can	O
introduce	O
biases	O
in	O
the	O
composition	O
and	O
focus	O
of	O
the	O
datasets	O
if	O
overly	O
simplistic	O
methods	O
are	O
used	O
,	O
such	O
as	O
searching	O
for	O
explicitly	O
misogynistic	O
terms	O
(	O
Wiegand	O
et	O
al	O
,	O
2019	O
)	O
.	O
To	O
ensure	O
that	O
our	O
dataset	O
contains	O
enough	O
misogynistic	O
abuse	O
we	O
began	O
with	O
targeted	O
sampling	O
,	O
taking	O
content	O
from	O
12	O
subreddits	O
that	O
were	O
identified	O
as	O
misogynistic	O
in	O
previous	O
research	O
.	O
This	O
includes	O
subreddits	O
such	O
as	O
r	O
/	O
MensRights	O
,	O
r	O
/	O
seduction	O
,	O
and	O
r	O
/	O
TheRedPill	O
.	O
The	O
sources	O
used	O
to	O
identify	O
these	O
subreddits	O
are	O
available	O
in	O
Table	O
9	O
in	O
the	O
Appendix	O
.	O
We	O
then	O
identified	O
22	O
additional	O
subreddits	O
which	O
had	O
been	O
recommended	O
by	O
the	O
moderators	O
/	O
owners	O
of	O
the	O
original	O
12	O
subreddits	O
in	O
the	O
'	O
sidebar	O
'	O
.	O
Some	O
of	O
these	O
are	O
not	O
misogynistic	O
but	O
discuss	O
women	O
(	O
e.g.	O
r	O
/	O
AskFeminists	O
)	O
and/or	O
are	O
otherwise	O
related	O
to	O
misogyny	O
.	O
For	O
example	O
,	O
r	O
/	O
exredpill	O
is	O
a	O
support	O
group	O
for	O
former	O
members	O
of	O
the	O
misogynistic	O
subreddit	O
r	O
/	O
TheRedPill	O
.	O
Table	O
9	O
in	O
the	O
Appendix	O
lists	O
the	O
34	O
targeted	O
subreddits	O
and	O
the	O
number	O
of	O
entries	O
and	O
threads	O
for	O
each	O
in	O
the	O
dataset	O
.	O
Over	O
11	O
weeks	O
,	O
for	O
each	O
subreddit	O
,	O
we	O
collected	O
the	O
entire	O
threads	O
of	O
the	O
20	O
most	O
popular	O
posts	O
that	O
week	O
.	O
Using	O
subreddits	O
to	O
target	O
the	O
sampling	O
rather	O
than	O
keywords	O
should	O
ensure	O
that	O
more	O
linguistic	O
variety	O
is	O
captured	O
,	O
minimising	O
the	O
amount	O
of	O
bias	O
as	O
keywords	O
such	O
as	O
'	O
slut	O
'	O
are	O
associated	O
with	O
more	O
explicit	O
and	O
less	O
subtle	O
forms	O
of	O
abuse	O
.	O
Nonetheless	O
,	O
only	O
sampling	O
from	O
suspected	O
misogynistic	O
communities	O
could	O
still	O
lead	O
to	O
classifiers	O
which	O
only	O
identify	O
the	O
forms	O
of	O
misogyny	O
found	O
in	O
those	O
targeted	O
contexts	O
Wiegand	O
et	O
al	O
,	O
2019	O
;	O
Sap	O
et	O
al	O
,	O
2019	O
)	O
.	O
To	O
account	O
for	O
this	O
potential	O
bias	O
,	O
and	O
to	O
enable	O
greater	O
generalisabilty	O
,	O
we	O
sampled	O
content	O
from	O
71	O
randomly	O
selected	O
subreddits	O
.	O
They	O
accounted	O
for	O
18	O
%	O
of	O
threads	O
and	O
16	O
%	O
of	O
entries	O
in	O
our	O
dataset	O
.	O
For	O
each	O
randomly	O
selected	O
subreddit	O
,	O
we	O
collected	O
the	O
thread	O
of	O
the	O
most	O
popular	O
post	O
.	O
All	O
threads	O
were	O
in	O
English	O
with	O
the	O
exception	O
of	O
one	O
thread	O
from	O
the	O
subreddit	O
r	O
/	O
Romania	O
.	O
Posts	O
and	O
comments	O
were	O
collected	O
from	O
February	O
to	O
May	O
2020	O
using	O
the	O
python	O
package	O
PRAW	O
,	O
a	O
wrapper	O
for	O
the	O
Reddit	B-DatasetName
API	O
(	O
Boe	O
,	O
2020	O
)	O
.	O
Posts	O
on	O
Reddit	B-DatasetName
have	O
a	O
text	O
title	O
and	O
a	O
body	O
which	O
can	O
be	O
text	O
,	O
an	O
image	O
,	O
or	O
a	O
link	O
.	O
For	O
posts	O
with	O
a	O
text	O
body	O
we	O
combined	O
this	O
with	O
the	O
post	O
title	O
to	O
create	O
a	O
single	O
unit	O
of	O
text	O
.	O
For	O
the	O
29	O
%	O
of	O
posts	O
where	O
the	O
body	O
was	O
an	O
image	O
we	O
also	O
collected	O
the	O
image	O
.	O

Content	O
which	O
does	O
not	O
contain	O
misogynistic	O
abuse	O
,	O
pejoratives	O
,	O
or	O
related	O
counter	O
speech	O
as	O
defined	O
in	O
the	O
previous	O
categories	O
.	O
This	O
content	O
is	O
often	O
not	O
related	O
to	O
abuse	O
or	O
to	O
women	O
in	O
general	O
.	O
That	O
said	O
,	O
it	O
can	O
include	O
other	O
forms	O
of	O
abusive	B-TaskName
language	I-TaskName
which	O
are	O
not	O
misogynistic	O
.	O

A	O
key	O
difficulty	O
in	O
the	O
formation	O
of	O
abusive	B-TaskName
language	I-TaskName
training	O
datasets	O
is	O
producing	O
high	O
quality	O
annotations	O
.	O
Several	O
factors	O
affect	O
this	O
.	O
Deciding	O
between	O
similar	O
categories	O
,	O
such	O
as	O
'	O
hate	B-DatasetName
speech	I-DatasetName
'	O
versus	O
'	O
offensive	O
language	O
'	O
can	O
be	O
difficult	O
(	O
Waseem	O
et	O
al	O
,	O
2017	O
)	O
.	O
Determining	O
the	O
right	O
category	O
often	O
requires	O
close	O
scrutiny	O
and	O
sustained	O
critical	O
thinking	O
from	O
annotators	O
.	O
Annotators	O
may	O
face	O
information	O
overload	O
if	O
asked	O
to	O
work	O
with	O
too	O
many	O
categories	O
,	O
both	O
in	O
terms	O
of	O
breadth	O
(	O
e.g.	O
annotating	O
for	O
different	O
types	O
of	O
abuse	O
)	O
and	O
depth	O
(	O
e.g.	O
working	O
with	O
numerous	O
subcategories	O
)	O
.	O
Further	O
,	O
annotators	O
may	O
have	O
different	O
values	O
and	O
experiences	O
and	O
so	O
make	O
different	O
assessments	O
of	O
the	O
content	O
they	O
observe	O
,	O
especially	O
when	O
context	O
plays	O
a	O
large	O
role	O
.	O
Annotators	O
will	O
also	O
have	O
unconscious	O
social	O
biases	O
which	O
may	O
mean	O
they	O
interpret	O
coding	O
instructions	O
differently	O
to	O
each	O
other	O
,	O
and	O
to	O
how	O
they	O
were	O
intended	O
by	O
the	O
research	O
authors	O
.	O
For	O
instance	O
,	O
found	O
that	O
crowdsourced	O
annotators	O
were	O
more	O
likely	O
to	O
label	O
sexist	O
content	O
as	O
merely	O
'	O
offensive	O
'	O
while	O
racist	O
and	O
homophobic	O
content	O
was	O
considered	O
'	O
hate	B-DatasetName
speech	I-DatasetName
'	O
.	O
To	O
mitigate	O
such	O
annotator	O
biases	O
,	O
we	O
used	O
expert	O
annotators	O
specifically	O
trained	O
in	O
identifying	O
misogynistic	O
content	O
,	O
as	O
well	O
as	O
a	O
group	O
-	O
based	O
facilitation	O
process	O
to	O
decide	O
final	O
labels	O
.	O
Due	O
to	O
time	O
and	O
resource	O
constraints	O
,	O
the	O
final	O
dataset	O
is	O
smaller	O
than	O
if	O
we	O
had	O
used	O
crowdsourced	O
workers	O
but	O
captures	O
more	O
nuanced	O
and	O
detailed	O
cases	O
of	O
misogyny	O
.	O
Six	O
annotators	O
worked	O
on	O
the	O
dataset	O
.	O
Annotators	O
were	O
trained	O
in	O
the	O
use	O
of	O
a	O
codebook	O
detailing	O
the	O
taxonomy	O
and	O
annotation	O
guidelines	O
.	O
The	O
codebook	O
was	O
updated	O
over	O
time	O
based	O
on	O
feedback	O
from	O
the	O
annotators	O
.	O
Demographic	O
information	O
on	O
the	O
annotators	O
is	O
available	O
in	O
Appendix	O
A.2	O

Annotators	O
independently	O
marked	O
up	O
each	O
entry	O
for	O
the	O
three	O
levels	O
presented	O
in	O
Section	O
4	O
.	O
For	O
all	O
level	O
two	O
categories	O
other	O
than	O
'	O
None	O
'	O
,	O
they	O
also	O
highlighted	O
the	O
specific	O
part	O
of	O
the	O
entry	O
which	O
was	O
relevant	O
to	O
the	O
labelled	O
category	O
(	O
the	O
'	O
span	O
'	O
)	O
.	O
This	O
is	O
particularly	O
important	O
information	O
for	O
long	O
posts	O
which	O
can	O
contain	O
multiple	O
forms	O
of	O
abuse	O
.	O
Each	O
entry	O
was	O
annotated	O
by	O
either	O
two	O
(	O
43	O
%	O
)	O
or	O
three	O
(	O
57	O
%	O
)	O
annotators	O
.	O
If	O
all	O
annotators	O
made	O
the	O
exact	O
same	O
annotation	O
(	O
including	O
all	O
three	O
levels	O
and	O
highlighting	O
)	O
this	O
was	O
accepted	O
as	O
the	O
final	O
annotation	O
.	O
All	O
other	O
entries	O
were	O
flagged	O
as	O
disagreements	O
.	O
Annotators	O
reviewed	O
the	O
disagreements	O
in	O
weekly	O
meetings	O
which	O
were	O
overseen	O
by	O
an	O
expert	O
facilitator	O
,	O
a	O
PhD	O
researcher	O
who	O
had	O
developed	O
the	O
annotation	O
taxonomy	O
and	O
was	O
familiar	O
with	O
the	O
literature	O
on	O
online	O
misogyny	O
and	O
hate	B-DatasetName
speech	I-DatasetName
classification	O
.	O
The	O
role	O
of	O
the	O
facilitator	O
was	O
to	O
promote	O
discussion	O
between	O
annotators	O
and	O
ensure	O
the	O
final	O
labels	O
reflected	O
the	O
taxonomy	O
.	O
Each	O
disagreement	O
was	O
discussed	O
until	O
the	O
annotators	O
reached	O
a	O
consensus	O
on	O
the	O
final	O
agreed	O
label	O
or	O
labels	O
.	O

We	O
make	O
use	O
of	O
the	O
more	O
granular	O
secondary	O
labels	O
in	O
our	O
taxonomy	O
to	O
conduct	O
an	O
error	O
analysis	O
for	O
the	O
weighted	O
BERT	B-MethodName
model	O
.	O
Table	O
7	O
shows	O
the	O
confusion	O
matrix	O
for	O
the	O
1	O
,	O
277	O
entries	O
in	O
the	O
test	O
set	O
.	O
Overall	O
,	O
137	O
entries	O
(	O
10.7	O
%	O
)	O
were	O
misclassified	O
.	O
86	O
(	O
63	O
%	O
)	O
of	O
these	O
errors	O
were	O
false	O
positives	O
and	O
51	O
(	O
37	O
%	O
)	O
were	O
false	O
negatives	O
.	O

Precision	B-MetricName
Recall	B-MetricName
F1	B-MetricName
score	I-MetricName
Accuracy	B-MetricName
In	O
many	O
cases	O
,	O
the	O
derogation	O
depends	O
on	O
the	O
context	O
of	O
the	O
earlier	O
conversation	O
thread	O
,	O
thus	O
the	O
BERT	B-MethodName
-	O
model	O
,	O
which	O
does	O
not	O
explicitly	O
take	O
into	O
account	O
prior	O
entries	O
in	O
the	O
thread	O
,	O
can	O
not	O
recognise	O
the	O
misogyny	O
in	O
isolation	O
.	O
"	O
It	O
's	O
funny	O
to	O
see	O
the	O
hamster	O
that	O
starts	O
to	O
act	O
up	O
in	O
their	O
little	O
widdle	O
tiny	O
brains	O
after	O
saying	O
that	O
too	O
.	O
"	O
is	O
an	O
explicit	O
statement	O
that	O
women	O
are	O
intellectually	O
inferior	O
,	O
but	O
understanding	O
that	O
it	O
refers	O
to	O
women	O
depends	O
on	O
having	O
seen	O
previous	O
entries	O
in	O
the	O
conversation	O
.	O
The	O
next	O
most	O
common	O
source	O
of	O
false	O
negatives	O
is	O
Pejoratives	O
(	O
n=19	O
)	O
.	O
The	O
classifier	O
misses	O
six	O
counts	O
each	O
of	O
'	O
whore	O
'	O
and	O
'	O
stacy	O
'	O
and	O
five	O
of	O
'	O
bitch	O
'	O
.	O
There	O
are	O
seven	O
missed	O
labels	O
for	O
Treatment	O
,	O
five	O
Disrespectful	O
actions	O
and	O
two	O
Threatening	O
language	O
.	O
However	O
,	O
due	O
to	O
the	O
low	O
prevalence	O
of	O
the	O
category	O
in	O
the	O
training	O
data	O
we	O
anticipate	O
some	O
errors	O
.	O
For	O
example	O
,	O
"	O
I	O
am	O
waiting	O
for	O
such	O
incident	O
to	O
happen	O
to	O
me	O
so	O
that	O
I	O
can	O
beat	O
the	O
shit	O
out	O
of	O
her	O
,	O
and	O
of	O
course	O
it	O
will	O
be	O
all	O
revenge	O
"	O
details	O
a	O
specific	O
form	O
of	O
violence	O
(	O
i.e.	O
'	O
beat	O
the	O
shit	O
out	O
of	O
her	O
'	O
)	O
which	O
the	O
model	O
can	O
not	O
know	O
to	O
identify	O
as	O
misogyny	O
without	O
being	O
trained	O
on	O
other	O
uses	O
of	O
the	O
term	O
.	O
The	O
final	O
two	O
errors	O
are	O
for	O
Personal	O
attacks	O
.	O
For	O
example	O
,	O
"	O
Yeah	O
there	O
s	O
women	O
that	O
I	O
as	O
an	O
Incel	O
would	O
nt	O
even	O
acknowledge	O
and	O
this	O
is	O
one	O
of	O
em	O
[	O
sic	O
]	O
"	O
.	O
This	O
is	O
an	O
implicit	O
attack	O
which	O
requires	O
understanding	O
that	O
considering	O
a	O
woman	O
unworthy	O
of	O
the	O
attention	O
of	O
an	O
incel	O
is	O
a	O
gendered	O
insult	O
.	O
As	O
we	O
can	O
see	O
from	O
these	O
examples	O
the	O
main	O
classification	O
errors	O
are	O
due	O
to	O
context	O
limitations	O
.	O
For	O
false	O
negatives	O
there	O
is	O
usually	O
not	O
enough	O
infor	O
-	O
mation	O
in	O
the	O
entry	O
alone	O
or	O
in	O
the	O
training	O
dataset	O
to	O
identify	O
the	O
misogyny	O
.	O
Conversely	O
,	O
for	O
false	O
positives	O
the	O
classifier	O
appears	O
to	O
overly	O
associate	O
content	O
about	O
women	O
with	O
content	O
that	O
abuses	O
women	O
.	O
These	O
limitations	O
can	O
be	O
addressed	O
by	O
future	O
work	O
drawing	O
on	O
the	O
taxonomy	O
and	O
annotation	O
process	O
presented	O
here	O
to	O
develop	O
larger	O
datasets	O
which	O
can	O
cover	O
a	O
greater	O
range	O
of	O
forms	O
of	O
discourse	O
,	O
including	O
both	O
non	O
-	O
misogynistic	O
discussions	O
of	O
women	O
and	O
a	O
wider	O
variety	O
of	O
misogynistic	O
speech	O
.	O

In	O
this	O
paper	O
we	O
have	O
presented	O
a	O
hierarchical	O
granular	O
taxonomy	O
for	O
misogyny	O
and	O
have	O
described	O
a	O
dataset	O
containing	O
high	O
quality	O
,	O
expert	O
labels	O
of	O
misogynistic	O
content	O
from	O
Reddit	B-DatasetName
.	O
We	O
have	O
also	O
provided	O
the	O
detailed	O
coding	O
book	O
we	O
created	O
and	O
a	O
dataset	O
with	O
all	O
of	O
the	O
original	O
labels	O
.	O
The	O
final	O
dataset	O
is	O
small	O
compared	O
to	O
other	O
annotated	O
datasets	O
used	O
for	O
classification	O
.	O
However	O
it	O
benefits	O
from	O
a	O
detailed	O
taxonomy	O
based	O
on	O
the	O
existing	O
literature	O
focused	O
on	O
just	O
one	O
form	O
of	O
online	O
abuse	O
-	O
misogyny	O
.	O
The	O
use	O
of	O
trained	O
annotators	O
and	O
an	O
adjudication	O
process	O
also	O
ensures	O
the	O
quality	O
of	O
the	O
labels	O
.	O
The	O
more	O
granular	O
subcategories	O
in	O
the	O
taxonomy	O
may	O
be	O
too	O
small	O
to	O
classify	O
separately	O
,	O
but	O
they	O
provide	O
insights	O
into	O
the	O
relative	O
frequency	O
of	O
different	O
forms	O
of	O
misogynistic	O
content	O
on	O
Reddit	B-DatasetName
and	O
enable	O
detailed	O
error	O
analysis	O
.	O
They	O
are	O
also	O
useful	O
for	O
other	O
researchers	O
aiming	O
to	O
create	O
larger	O
datasets	O
,	O
who	O
can	O
build	O
on	O
the	O
taxonomic	O
work	O
conducted	O
here	O
.	O
A	O
Short	O
form	O
data	O
statement	O
Following	O
the	O
recommendation	O
of	O
Bender	O
and	O
Friedman	O
(	O
2018	O
)	O
we	O
include	O
the	O
following	O
short	O
form	O
data	O
statement	O
to	O
summarise	O
the	O
main	O
features	O
of	O
the	O
datasets	O
.	O
Further	O
details	O
on	O
the	O
creation	O
of	O
the	O
datasets	O
are	O
in	O
in	O
Sections	O
3	O
and	O
5	O
in	O
the	O
main	O
paper	O
.	O

The	O
two	O
datasets	O
include	O
labels	O
for	O
6	O
,	O
383	O
unique	O
Reddit	B-DatasetName
entries	O
(	O
i.e.	O
posts	O
or	O
comments	O
)	O
across	O
672	O
conversation	O
threads	O
collected	O
.	O
One	O
dataset	O
is	O
of	O
the	O
15	O
,	O
816	O
original	O
labels	O
selected	O
by	O
annotators	O
and	O
the	O
second	O
is	O
of	O
the	O
6	O
,	O
567	O
agreed	O
labels	O
.	O
Table	O
8	O
provides	O
a	O
description	O
of	O
each	O
of	O
the	O
variables	O
in	O
the	O
datasets	O
.	O
We	O
also	O
include	O
the	O
accompanying	O
set	O
of	O
images	O
associated	O
with	O
some	O
original	O
post	O
entries	O
.	O
All	O
threads	O
except	O
one	O
are	O
in	O
English	O
.	O
The	O
majority	O
of	O
threads	O
were	O
sampled	O
from	O
a	O
set	O
of	O
34	O
subreddits	O
selected	O
for	O
the	O
expected	O
prevalence	O
of	O
misogynistic	O
content	O
,	O
or	O
non	O
-	O
misogynistic	O
discussions	O
about	O
women	O
.	O
Paid	O
annotators	O
received	O
extensive	O
training	O
to	O
apply	O
the	O
taxonomy	O
presented	O
in	O
this	O
paper	O
to	O
label	O
entries	O
.	O
The	O
majority	O
of	O
annotators	O
were	O
White	O
-	O
British	O
,	O
spoke	O
English	O
as	O
a	O
first	O
language	O
,	O
and	O
had	O
or	O
were	O
pursuing	O
a	O
University	O
degree	O
.	O
Two	O
-	O
thirds	O
of	O
annotators	O
were	O
women	O
.	O

Logistic	B-MethodName
regression	I-MethodName
with	O
l1	O
-	O
regularisation	O
is	O
implemented	O
in	O
R	O
using	O
the	O
'	O
glmnet	O
'	O
package	O
(	O
Friedman	O
et	O
al	O
,	O
2010	O
)	O
on	O
a	O
unigram	O
representation	O
of	O
the	O
data	O
.	O
Lambda	O
is	O
selected	O
using	O
cross	O
-	O
validation	O
and	O
set	O
to	O
0.015	O
.	O

Model	O
Architecture	O
We	O
implement	O
uncased	O
BERT	B-MethodName
-	O
base	O
models	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
using	O
the	O
transformers	O
Python	O
library	O
(	O
Wolf	O
et	O
al	O
,	O
2020	O
)	O
.	O
For	O
sequence	O
classification	O
,	O
we	O
add	O
a	O
linear	B-MethodName
layer	I-MethodName
with	O
softmax	B-MethodName
output	O
.	O

Variable	O
Description	O
entry	O
i	O
d	O
A	O
unique	O
string	O
assigned	O
to	O
every	O
comment	O
and	O
post	O
by	O
Reddit	B-DatasetName
.	O
link	O
i	O
d	O
The	O
i	O
d	O
number	O
of	O
the	O
original	O
post	O
of	O
a	O
thread	O
.	O
parent	O
i	O
d	O
The	O
i	O
d	O
number	O
of	O
parent	O
entry	O
(	O
i.e.	O
the	O
post	O
or	O
comment	O
this	O
entry	O
responds	O
to	O
)	O
.	O
subreddit	O
The	O
subreddit	O
community	O
where	O
the	O
entry	O
was	O
made	O
.	O
author	O
The	O
Reddit	B-DatasetName
username	O
of	O
the	O
entry	O
author	O
.	O
body	O
The	O
text	O
body	O
of	O
the	O
entry	O
.	O
For	O
the	O
original	O
posts	O
of	O
threads	O
the	O
title	O
and	O
post	O
body	O
were	O
combined	O
.	O
image	O
Whether	O
the	O
entry	O
has	O
an	O
accompanying	O
image	O
.	O
Only	O
applicable	O
to	O
posts	O
.	O
Images	O
are	O
provided	O
as	O
jpg	O
files	O
.	O
They	O
are	O
named	O
as	O
'	O
X	O
Y	O
Z	O
'	O
corresponding	O
to	O
the	O
week	O
(	O
X	O
)	O
,	O
group	O
(	O
Y	O
)	O
,	O
and	O
thread	O
i	O
d	O
(	O
Z	O
)	O
.	O
label	O
date	O
The	O
week	O
commencing	O
date	O
of	O
when	O
the	O
entry	O
was	O
labelled	O
.	O
week	O
The	O
week	O
in	O
the	O
annotation	O
process	O
when	O
the	O
entry	O
as	O
assigned	O
(	O
1	O
to	O
11	O
)	O
.	O
group	O
The	O
weekly	O
group	O
the	O
entry	O
was	O
assigned	O
to	O
.	O
All	O
weeks	O
had	O
two	O
groups	O
except	O
week	O
7	O
which	O
only	O
had	O
1	O
.	O
sheet	O
order	O
The	O
order	O
of	O
the	O
entry	O
in	O
the	O
weekly	O
annotation	O
sheet	O
.	O
This	O
is	O
a	O
list	O
of	O
numbers	O
referring	O
to	O
the	O
nested	O
structure	O
of	O
comments	O
in	O
threads	O
.	O
It	O
shows	O
the	O
i	O
d	O
number	O
of	O
each	O
level	O
of	O
the	O
thread	O
from	O
the	O
original	O
post	O
to	O
the	O
relevant	O
entry	O
.	O
For	O
example	O
,	O
if	O
an	O
entry	O
has	O
the	O
sheet	O
order	O
(	O
1	O
,	O
2	O
,	O
3	O
)	O
it	O
belongs	O
to	O
the	O
first	O
thread	O
(	O
1	O
)	O
,	O
and	O
replied	O
to	O
the	O
second	O
comment	O
(	O
2	O
)	O
,	O
to	O
which	O
it	O
is	O
the	O
third	O
reply	O
(	O
3	O
)	O
.	O
See	O
Fig	O
.	O
3	O

Discourse	O
parsers	O
recognize	O
the	O
intentional	O
and	O
inferential	O
relationships	O
that	O
organize	O
extended	O
texts	O
.	O
They	O
have	O
had	O
a	O
great	O
influence	O
on	O
a	O
variety	O
of	O
NLP	O
tasks	O
as	O
well	O
as	O
theoretical	O
studies	O
in	O
linguistics	O
and	O
cognitive	O
science	O
.	O
However	O
it	O
is	O
often	O
difficult	O
to	O
achieve	O
good	O
results	O
from	O
current	O
discourse	O
models	O
,	O
largely	O
due	O
to	O
the	O
difficulty	O
of	O
the	O
task	O
,	O
particularly	O
recognizing	O
implicit	O
discourse	O
relations	O
.	O
Recent	O
developments	O
in	O
transformer	O
-	O
based	O
models	O
have	O
shown	O
great	O
promise	O
on	O
these	O
analyses	O
,	O
but	O
challenges	O
still	O
remain	O
.	O
We	O
present	O
a	O
position	O
paper	O
which	O
provides	O
a	O
systematic	O
analysis	O
of	O
the	O
state	O
of	O
the	O
art	O
discourse	O
parsers	O
.	O
We	O
aim	O
to	O
examine	O
the	O
performance	O
of	O
current	O
discourse	B-TaskName
parsing	I-TaskName
models	O
via	O
gradual	O
domain	O
shift	O
:	O
within	O
the	O
same	O
corpus	O
,	O
on	O
in	O
-	O
domain	O
texts	O
,	O
and	O
on	O
out	O
-	O
of	O
-	O
domain	O
texts	O
,	O
and	O
discuss	O
the	O
differences	O
between	O
the	O
transformer	O
-	O
based	O
models	O
and	O
the	O
previous	O
models	O
in	O
predicting	O
different	O
types	O
of	O
implicit	B-TaskName
relations	I-TaskName
both	O
interand	O
intra	O
-	O
sentential	O
.	O
We	O
conclude	O
by	O
describing	O
several	O
shortcomings	O
of	O
the	O
existing	O
models	O
and	O
a	O
discussion	O
of	O
how	O
future	O
work	O
should	O
approach	O
this	O
problem	O
.	O

There	O
are	O
various	O
frameworks	O
for	O
studying	O
inferential	O
links	O
between	O
discourse	O
segments	O
,	O
from	O
local	O
shallow	O
relations	O
between	O
discourse	O
segments	O
in	O
PDTB	O
(	O
Rashmi	O
Prasad	O
,	O
2008	O
)	O
to	O
hierarchical	O
constituent	O
structures	O
in	O
RST	O
(	O
Carlson	O
et	O
al	O
,	O
2003	O
)	O
or	O
discourse	O
graphs	O
in	O
Segmented	O
Discourse	O
Representation	O
Theory	O
(	O
SDRT	O
)	O
(	O
Asher	O
et	O
al	O
,	O
2003	O
)	O
and	O
the	O
Discourse	O
Graphbank	O
(	O
Wolf	O
and	O
Gibson	O
,	O
2005	O
)	O
.	O
Rhetorical	O
Structure	O
Theory	O
(	O
RST	O
)	O
(	O
Mann	O
and	O
Thompson	O
,	O
1987	O
)	O
provides	O
a	O
hierarchical	O
structure	O
for	O
analyzing	O
text	O
that	O
describes	O
relations	O
between	O
text	O
spans	O
known	O
as	O
elementary	O
discourse	O
units	O
(	O
EDUs	O
)	O
.	O
The	O
RST	O
Discourse	O
Treebank	O
(	O
Carlson	O
et	O
al	O
,	O
2003	O
)	O
contains	O
385	O
Wall	O
Street	O
Journal	O
articles	O
from	O
the	O
Penn	B-DatasetName
Treebank	I-DatasetName
(	O
Marcus	O
et	O
al	O
,	O
1993	O
)	O
which	O
have	O
been	O
split	O
into	O
elementary	O
discourse	O
units	O
and	O
annotated	O
according	O
to	O
Rhetorical	O
Structure	O
Theory	O
,	O
where	O
discourse	O
relations	O
are	O
annotated	O
in	O
a	O
tree	O
structure	O
across	O
the	O
whole	O
document	O
.	O
A	O
full	O
list	O
of	O
these	O
relations	O
can	O
be	O
found	O
in	O
Carlson	O
and	O
Marcu	O
(	O
2001	O
)	O
.	O
The	O
Penn	O
Discourse	O
Treebank	O
(	O
PDTB	O
)	O
(	O
Eleni	O
Miltsakaki	O
,	O
2004	O
;	O
Rashmi	O
Prasad	O
,	O
2008	O
;	O
Prasad	O
et	O
al	O
,	O
2018	O
)	O
,	O
which	O
also	O
uses	O
Penn	B-DatasetName
Treebank	I-DatasetName
Wall	O
Street	O
Journal	O
articles	O
,	O
contains	O
discourse	O
relations	O
annotated	O
in	O
a	O
shallow	O
,	O
non	O
-	O
hierarchical	O
manner	O
.	O
For	O
each	O
relation	O
between	O
two	O
arguments	O
,	O
each	O
argument	O
and	O
the	O
discourse	O
connective	O
(	O
word	O
or	O
phrase	O
that	O
indicates	O
the	O
discourse	O
relation	O
)	O
are	O
labeled	O
.	O
The	O
PDTB	O
also	O
annotates	O
whether	O
a	O
relation	O
is	O
explicit	O
or	O
non	O
-	O
explicit	O
,	O
the	O
latter	O
type	O
of	O
which	O
has	O
three	O
subtypes	O
:	O
Implicit	O
,	O
AltLex	O
,	O
and	O
EntRel	O
.	O
In	O
this	O
paper	O
,	O
we	O
focus	O
on	O
implicit	B-TaskName
relations	I-TaskName
,	O
where	O
a	O
connective	O
can	O
be	O
inserted	O
between	O
the	O
two	O
arguments	O
that	O
indicates	O
a	O
discourse	O
relation	O
.	O
These	O
relations	O
are	O
considered	O
extremely	O
challenging	O
for	O
discourse	O
parsers	O
to	O
automatically	O
identify	O
.	O
There	O
is	O
a	O
need	O
to	O
examine	O
the	O
performance	O
of	O
the	O
proposed	O
discourse	O
parsers	O
,	O
their	O
representational	O
choices	O
,	O
their	O
generalizability	O
,	O
and	O
interpretability	O
both	O
across	O
domains	O
,	O
distributions	O
,	O
and	O
frameworks	O
.	O
One	O
recently	O
developed	O
framework	O
is	O
the	O
PDTB	O
-	O
3	O
.	O
Since	O
its	O
release	O
in	O
2019	O
,	O
several	O
papers	O
have	O
evaluated	O
the	O
performance	O
of	O
implicit	O
sense	O
classifiers	O
on	O
this	O
new	O
corpus	O
,	O
which	O
includes	O
newly	O
annotated	O
intra	O
-	O
sentential	O
implicit	O
discourse	O
relations	O
.	O
In	O
addition	O
to	O
proposing	O
a	O
new	O
evaluation	O
framework	O
for	O
PDTB	O
,	O
Kim	O
et	O
al	O
(	O
2020	O
)	O
evaluate	O
the	O
performance	O
of	O
pretrained	O
encoders	O
for	O
implicit	O
sense	O
classification	O
on	O
the	O
PDTB	O
-	O
2	O
and	O
the	O
PDTB	O
-	O
3	O
.	O
Liang	O
et	O
al	O
(	O
2020	O
)	O
identify	O
locating	O
the	O
position	O
of	O
relations	O
as	O
a	O
new	O
challenge	O
in	O
the	O
PDTB	O
-	O
3	O
,	O
due	O
to	O
the	O
significantly	O
increased	O
number	O
of	O
intra	O
-	O
sentential	O
implicit	B-TaskName
relations	I-TaskName
annotated	O
.	O
Techniques	O
of	O
discourse	B-TaskName
parsing	I-TaskName
range	O
from	O
supervised	O
Mabona	O
et	O
al	O
,	O
2019	O
;	O
Lin	O
et	O
al	O
,	O
2019	O
;	O
Zhang	O
et	O
al	O
,	O
2020	O
;	O
Kobayashi	O
et	O
al	O
,	O
2020	O
)	O
and	O
weakly	O
supervised	O
and	O
unsupervised	O
approaches	O
(	O
Lee	O
et	O
al	O
,	O
2020	O
;	O
Nishida	O
and	O
Nakayama	O
,	O
2020	O
;	O
Kurfalı	O
andÖstling	O
,	O
2019	O
)	O
;	O
recent	O
developments	O
such	O
as	O
word	O
/	O
contextual	O
embeddings	O
have	O
improved	O
parser	O
performance	O
,	O
although	O
not	O
as	O
significantly	O
as	O
other	O
tasks	O
(	O
Shi	O
and	O
Demberg	O
,	O
2019	O
;	O
Chen	O
et	O
al	O
,	O
2019	O
)	O
Yet	O
most	O
works	O
have	O
made	O
simplifying	O
assumptions	O
concerning	O
the	O
linguistic	O
annotations	O
for	O
practical	O
purposes	O
that	O
affect	O
their	O
evaluation	O
and	O
generality	O
.	O
For	O
instance	O
,	O
most	O
shallow	O
discourse	O
parsers	O
use	O
only	O
the	O
argument	O
pairs	O
to	O
determine	O
the	O
discourse	O
sense	O
without	O
considering	O
further	O
context	O
.	O
Additionally	O
,	O
in	O
RST	O
parsing	O
,	O
standard	O
practice	O
involves	O
classifying	O
only	O
the	O
18	O
top	O
-	O
level	O
RST	O
classes	O
(	O
Hernault	O
et	O
al	O
,	O
2010	O
;	O
Feng	O
and	O
Hirst	O
,	O
2014	O
;	O
Morey	O
et	O
al	O
,	O
2017	O
)	O
.	O
Thus	O
,	O
all	O
Elaboration	O
relations	O
are	O
lumped	O
together	O
,	O
making	O
it	O
a	O
huge	O
class	O
.	O
We	O
reveal	O
findings	O
about	O
these	O
assumptions	O
in	O
Section	O
4	O
.	O
Other	O
works	O
evaluating	O
discourse	O
parsers	O
include	O
DiscoEval	O
(	O
Chen	O
et	O
al	O
,	O
2019	O
)	O
,	O
a	O
test	O
suite	O
of	O
evaluation	O
tasks	O
that	O
test	O
the	O
effectiveness	O
of	O
different	O
sentence	O
encoders	O
for	O
discourse	O
parsers	O
,	O
and	O
an	O
i	O
m	O
-	O
proved	O
evaluation	O
protocol	O
for	O
the	O
PDTB	O
-	O
2	O
(	O
Kim	O
et	O
al	O
,	O
2020	O
)	O
.	O
In	O
contrast	O
,	O
our	O
work	O
aims	O
to	O
analyze	O
and	O
evaluate	O
existing	O
discourse	O
parsers	O
via	O
gradual	O
domain	O
shift	O
.	O
We	O
provide	O
a	O
comparative	O
genrebased	O
analysis	O
on	O
distributionally	O
shifted	O
text	O
data	O
and	O
present	O
a	O
qualitative	O
analysis	O
of	O
the	O
impact	O
of	O
the	O
practical	O
choices	O
that	O
these	O
models	O
make	O
while	O
doing	O
discourse	B-TaskName
parsing	I-TaskName
across	O
frameworks	O
.	O
3	O
Where	O
are	O
we	O
in	O
discourse	B-TaskName
parsing	I-TaskName
?	O

Data	O
.	O
We	O
start	O
by	O
focusing	O
on	O
possible	O
distributional	O
shifts	O
in	O
a	O
shallow	O
parser	O
's	O
application	O
,	O
by	O
considering	O
different	O
linguistic	O
types	O
of	O
implicit	O
discourse	O
relations	O
(	O
inter	O
-	O
vs	O
intra	O
-	O
sentential	O
)	O
(	O
Liang	O
et	O
al	O
,	O
2020	O
)	O
.	O
To	O
do	O
this	O
,	O
we	O
evaluate	O
performance	O
on	O
the	O
PDTB	O
-	O
2	O
and	O
PDTB	O
-	O
3	O
,	O
as	O
well	O
as	O
the	O
intrasentential	O
relations	O
in	O
the	O
PDTB	O
-	O
3	O
specifically	O
.	O
We	O
then	O
evaluate	O
the	O
performance	O
of	O
three	O
widely	O
used	O
or	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
under	O
gradual	O
shift	O
of	O
the	O
domain	O
of	O
texts	O
,	O
noting	O
that	O
users	O
who	O
would	O
want	O
to	O
use	O
a	O
parser	O
will	O
be	O
applying	O
it	O
on	O
data	O
that	O
varies	O
linguistically	O
to	O
different	O
degrees	O
from	O
the	O
parser	O
's	O
training	O
data	O
(	O
a	O
fixed	O
3	O
-	O
year	O
window	O
of	O
WSJ	O
articles	O
)	O
.	O
The	O
data	O
we	O
examine	O
is	O
:	O
WSJ	O
texts	O
outside	O
of	O
the	O
Penn	B-DatasetName
Treebank	I-DatasetName
,	O
other	O
news	O
texts	O
,	O
and	O
the	O
GUM	B-DatasetName
corpus	O
(	O
Zeldes	O
,	O
2017	O
)	O
.	O
Note	O
that	O
none	O
of	O
these	O
texts	O
contain	O
gold	O
PDTB	O
annotations	O
,	O
and	O
only	O
the	O
GUM	B-DatasetName
corpus	O
contains	O
gold	O
RST	O
annotations	O
.	O
Setup	O
.	O
To	O
examine	O
the	O
impact	O
of	O
changing	O
the	O
linguistic	O
distribution	O
by	O
introducing	O
intra	O
-	O
sentential	O
discourse	O
relations	O
,	O
we	O
run	O
the	O
model	O
developed	O
by	O
Chen	O
et	O
al	O
(	O
2019	O
)	O
using	O
the	O
same	O
train	O
-	O
test	O
split	O
as	O
the	O
authors	O
and	O
training	O
/	O
testing	O
on	O
discourse	O
senses	O
which	O
contain	O
10	O
or	O
more	O
examples	O
.	O
To	O
get	O
results	O
for	O
the	O
PDTB	O
-	O
2	O
,	O
we	O
train	O
and	O
test	O
the	O
model	O
on	O
the	O
PDTB	O
-	O
2	O
;	O
to	O
get	O
results	O
for	O
the	O
PDTB	O
-	O
3	O
and	O
intrasentential	O
relations	O
in	O
the	O
PDTB	O
-	O
3	O
,	O
we	O
train	O
the	O
model	O
on	O
the	O
PDTB	O
-	O
3	O
and	O
evaluate	O
its	O
performance	O
on	O
both	O
of	O
these	O
sets	O
.	O
To	O
parse	O
plain	O
-	O
text	O
documents	O
for	O
PDTB	O
relations	O
,	O
we	O
use	O
the	O
Wang	O
and	O
Lan	O
(	O
2015	O
)	O
parser	O
as	O
our	O
end	O
-	O
to	O
-	O
end	O
parser	O
and	O
the	O
Chen	O
et	O
al	O
(	O
2019	O
)	O
DiscoEval	O
parser	O
as	O
our	O
implicit	O
sense	O
classifier	O
.	O
The	O
former	O
is	O
needed	O
in	O
order	O
to	O
parse	O
unlabeled	O
text	O
,	O
and	O
the	O
latter	O
is	O
a	O
more	O
accurate	O
BERT	B-MethodName
-	O
based	O
implicit	O
sense	O
classifier	O
(	O
implicit	O
sense	O
classification	O
is	O
the	O
most	O
difficult	O
PDTB	O
parsing	O
task	O
)	O
.	O
To	O
evaluate	O
these	O
parsers	O
,	O
we	O
look	O
at	O
quantitative	O
as	O
-	O
pects	O
of	O
their	O
output	O
(	O
e.g.	O
the	O
distributions	O
)	O
and	O
qualitative	O
aspects	O
(	O
manual	O
annotation	O
and	O
inspection	O
of	O
parser	O
output	O
)	O
.	O
For	O
our	O
RST	O
experiments	O
,	O
we	O
use	O
the	O
state	O
-	O
ofthe	O
-	O
art	O
(	O
Wang	O
et	O
al	O
,	O
2017	O
)	O
parser	O
.	O
We	O
evaluate	O
the	O
performance	O
of	O
this	O
parser	O
on	O
the	O
standard	O
RST	O
Discourse	O
Treebank	O
test	O
set	O
with	O
a	O
90	O
-	O
10	O
split	O
(	O
347	O
training	O
documents	O
and	O
38	O
test	O
documents	O
)	O
.	O
We	O
also	O
evaluate	O
it	O
on	O
the	O
gold	O
labels	O
from	O
the	O
GUM	B-DatasetName
corpus	O
(	O
but	O
trained	O
on	O
the	O
RST	O
)	O
.	O
Because	O
GUM	B-DatasetName
is	O
annotated	O
with	O
20	O
different	O
discourse	O
relations	O
which	O
do	O
not	O
precisely	O
map	O
to	O
the	O
conventional	O
18	O
types	O
used	O
in	O
the	O
Wang	O
et	O
al	O
(	O
2017	O
)	O
parser	O
,	O
we	O
map	O
the	O
ones	O
that	O
do	O
n't	O
match	O
these	O
types	O
or	O
the	O
more	O
fine	O
-	O
grained	O
relations	O
in	O
the	O
following	O
manner	O
,	O
following	O
Braud	O
et	O
al	O
(	O
2017	O
)	O
:	O
preparation	O
to	O
BACK	O
-	O
GROUND	O
,	O
justify	O
and	O
motivation	O
to	O
EXPLANA	O
-	O
TION	O
,	O
and	O
solutionhood	O
to	O
TOPIC	O
-	O
COMMENT	O
.	O
For	O
the	O
plain	O
-	O
text	O
news	O
articles	O
from	O
outside	O
of	O
the	O
PDTB	O
corpus	O
,	O
we	O
mirror	O
the	O
PDTB	O
experiments	O
on	O
these	O
documents	O
by	O
parsing	O
them	O
with	O
the	O
(	O
Wang	O
et	O
al	O
,	O
2017	O
)	O
parser	O
,	O
then	O
examining	O
the	O
resulting	O
distributions	O
and	O
manually	O
inspecting	O
the	O
parser	O
output	O
.	O

While	O
inspecting	O
the	O
results	O
of	O
the	O
annotations	O
,	O
we	O
found	O
several	O
helpful	O
phenomena	O
for	O
developing	O
future	O
models	O
,	O
including	O
observations	O
regarding	O
the	O
role	O
of	O
context	O
in	O
shallow	O
discourse	B-TaskName
parsing	I-TaskName
and	O
errors	O
that	O
current	O
RST	O
parsers	O
are	O
making	O
.	O

For	O
the	O
qualitative	O
analysis	O
,	O
we	O
ask	O
two	O
annotators	O
(	O
a	O
faculty	O
member	O
and	O
a	O
graduate	O
student	O
from	O
linguistics	O
departments	O
)	O
to	O
provide	O
annotations	O
for	O
the	O
data	O
,	O
as	O
none	O
of	O
the	O
texts	O
contain	O
gold	O
PDTB	O
labels	O
and	O
only	O
the	O
GUM	B-DatasetName
corpus	O
contains	O
gold	O
RST	O
labels	O
.	O
The	O
annotators	O
were	O
trained	O
on	O
,	O
and	O
provided	O
with	O
,	O
the	O
PDTB	O
2.0	O
annotation	O
manual	O
(	O
Prasad	O
et	O
al	O
,	O
2007	O
)	O
.	O
In	O
order	O
for	O
the	O
annotators	O
to	O
annotate	O
this	O
corpus	O
,	O
discourse	O
relations	O
were	O
randomly	O
chosen	O
from	O
Wall	O
Street	O
Journal	O
articles	O
,	O
other	O
news	O
articles	O
,	O
and	O
the	O
GUM	B-DatasetName
corpus	O
.	O
64	O
of	O
these	O
discourse	O
relations	O
were	O
implicit	O
,	O
and	O
are	O
the	O
only	O
ones	O
reported	O
in	O
this	O
paper	O
.	O
The	O
annotators	O
were	O
given	O
the	O
sentence	O
(	O
s	O
)	O
containing	O
both	O
arguments	O
,	O
with	O
the	O
arguments	O
labeled	O
,	O
and	O
they	O
also	O
had	O
access	O
to	O
the	O
article	O
text	O
if	O
they	O
ever	O
needed	O
to	O
reference	O
back	O
to	O
it	O
.	O
To	O
assess	O
the	O
inter	O
-	O
rater	O
agreement	O
,	O
we	O
determine	O
Cohen	O
's	O
κ	O
value	O
(	O
Cohen	O
,	O
1960	O
)	O
.	O
We	O
randomly	O
selected	O
25	O
samples	O
from	O
the	O
PDTB	O
and	O
assigned	O
each	O
to	O
the	O
annotators	O
.	O
We	O
obtained	O
a	O
Cohen	O
's	O
κ	O
of	O
0.88	O
,	O
which	O
indicates	O
almost	O
perfect	O
agreement	O
.	O

Discourse	B-TaskName
parsing	I-TaskName
for	O
text	O
has	O
seen	O
a	O
recent	O
surge	O
in	O
experimental	O
approaches	O
.	O
In	O
this	O
work	O
we	O
presented	O
a	O
detailed	O
analysis	O
of	O
the	O
performance	O
of	O
the	O
state	O
of	O
the	O
art	O
discourse	O
parsers	O
and	O
analysed	O
their	O
weaknesses	O
and	O
strength	O
.	O
The	O
conclusions	O
drawn	O
above	O
from	O
these	O
experiments	O
make	O
it	O
clear	O
that	O
discourse	B-TaskName
parsing	I-TaskName
,	O
though	O
it	O
has	O
come	O
a	O
long	O
way	O
in	O
the	O
past	O
decade	O
or	O
so	O
,	O
still	O
has	O
a	O
long	O
way	O
to	O
go	O
,	O
particularly	O
with	O
respect	O
to	O
parsing	O
on	O
out	O
-	O
ofdomain	O
texts	O
and	O
addressing	O
issues	O
of	O
class	O
imbalances	O
,	O
although	O
the	O
BERT	B-MethodName
-	O
based	O
model	O
has	O
made	O
some	O
improvements	O
in	O
this	O
area	O
.	O
Additionally	O
,	O
we	O
investigated	O
how	O
and	O
when	O
PDTB	O
-	O
3	O
can	O
help	O
in	O
improving	O
the	O
prediction	O
of	O
intra	O
-	O
sentential	O
implicit	B-TaskName
relations	I-TaskName
.	O
There	O
are	O
several	O
promising	O
future	O
directions	O
for	O
the	O
area	O
of	O
discourse	B-TaskName
parsing	I-TaskName
.	O
A	O
model	O
that	O
detects	O
intra	O
-	O
sentential	O
implicit	B-TaskName
relations	I-TaskName
is	O
necessary	O
in	O
order	O
to	O
be	O
able	O
to	O
parse	O
on	O
the	O
PDTB	O
-	O
3	O
.	O
Exploring	O
new	O
neural	O
parsing	O
strategies	O
is	O
also	O
a	O
must	O
.	O
We	O
observed	O
that	O
neural	O
parsers	O
are	O
ignorant	O
about	O
what	O
they	O
do	O
not	O
know	O
and	O
overconfident	O
when	O
they	O
make	O
uninformed	O
predictions	O
.	O
Quantifying	O
prediction	O
uncertainty	O
directly	O
by	O
training	O
the	O
model	O
to	O
output	O
high	O
uncertainty	O
for	O
the	O
data	O
samples	O
close	O
to	O
class	O
boundaries	O
can	O
results	O
in	O
parsers	O
that	O
can	O
make	O
better	O
decisions	O
.	O
One	O
takeaway	O
of	O
our	O
empirical	O
analysis	O
was	O
the	O
importance	O
of	O
the	O
role	O
of	O
context	O
in	O
identifying	O
the	O
correct	O
discourse	O
relations	O
.	O
This	O
observation	O
suggests	O
the	O
need	O
for	O
new	O
computational	O
experiments	O
that	O
can	O
identify	O
the	O
right	O
context	O
window	O
that	O
is	O
required	O
for	O
the	O
model	O
to	O
accurately	O
predict	O
relations	O
.	O
Another	O
useful	O
direction	O
is	O
designing	O
models	O
that	O
can	O
learn	O
discourse	O
relations	O
on	O
their	O
own	O
without	O
the	O
help	O
of	O
annotated	O
corpora	O
.	O
There	O
are	O
several	O
unsupervised	O
models	O
(	O
Kobayashi	O
et	O
al	O
,	O
2019	O
;	O
Nishida	O
and	O
Nakayama	O
,	O
2020	O
)	O
that	O
are	O
used	O
for	O
determining	O
the	O
structure	O
of	O
discourse	O
parse	O
trees	O
but	O
few	O
that	O
infer	O
the	O
relations	O
themselves	O
.	O

Importance	O
-	O
based	O
Neuron	O
Allocation	O
for	O
Multilingual	O
Neural	O
Machine	B-TaskName
Translation	I-TaskName

Multilingual	O
neural	O
machine	B-TaskName
translation	I-TaskName
with	O
a	O
single	O
model	O
has	O
drawn	O
much	O
attention	O
due	O
to	O
its	O
capability	O
to	O
deal	O
with	O
multiple	O
languages	O
.	O
However	O
,	O
the	O
current	O
multilingual	O
translation	O
paradigm	O
often	O
makes	O
the	O
model	O
tend	O
to	O
preserve	O
the	O
general	B-TaskName
knowledge	I-TaskName
,	O
but	O
ignore	O
the	O
language	O
-	O
specific	O
knowledge	O
.	O
Some	O
previous	O
works	O
try	O
to	O
solve	O
this	O
problem	O
by	O
adding	O
various	O
kinds	O
of	O
language	O
-	O
specific	O
modules	O
to	O
the	O
model	O
,	O
but	O
they	O
suffer	O
from	O
the	O
parameter	O
explosion	O
problem	O
and	O
require	O
specialized	O
manual	O
design	O
.	O
To	O
solve	O
these	O
problems	O
,	O
we	O
propose	O
to	O
divide	O
the	O
model	O
neurons	O
into	O
general	O
and	O
language	O
-	O
specific	O
parts	O
based	O
on	O
their	O
importance	O
across	O
languages	O
.	O
The	O
general	O
part	O
is	O
responsible	O
for	O
preserving	O
the	O
general	B-TaskName
knowledge	I-TaskName
and	O
participating	O
in	O
the	O
translation	O
of	O
all	O
the	O
languages	O
,	O
while	O
the	O
language	O
-	O
specific	O
part	O
is	O
responsible	O
for	O
preserving	O
the	O
languagespecific	O
knowledge	O
and	O
participating	O
in	O
the	O
translation	O
of	O
some	O
specific	O
languages	O
.	O
Experimental	O
results	O
on	O
several	O
language	O
pairs	O
,	O
covering	O
IWSLT	O
and	O
Europarl	O
corpus	O
datasets	O
,	O
demonstrate	O
the	O
effectiveness	O
and	O
universality	O
of	O
the	O
proposed	O
method	O
.	O

In	O
this	O
section	O
,	O
we	O
will	O
give	O
a	O
brief	O
introduction	O
to	O
the	O
Transformer	B-MethodName
model	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
and	O
the	O
Multilingual	O
translation	O
.	O

We	O
denote	O
the	O
input	O
sequence	O
of	O
symbols	O
as	O
x	O
=	O
(	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
J	O
)	O
,	O
the	O
ground	O
-	O
truth	O
sequence	O
as	O
y	O
*	O
=	O
(	O
y	O
*	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
*	O
K	O
*	O
)	O
and	O
the	O
translation	O
as	O
y	O
=	O
(	O
y	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
K	O
)	O
.	O
Transformer	B-MethodName
is	O
a	O
stacked	O
network	O
with	O
N	O
identical	O
layers	O
containing	O
two	O
or	O
three	O
basic	O
blocks	O
in	O
each	O
layer	O
.	O
For	O
a	O
single	O
layer	O
in	O
the	O
encoder	O
,	O
it	O
consists	O
of	O
a	O
multi	O
-	O
head	O
self	O
-	O
attention	O
and	O
a	O
position	O
-	O
wise	O
feed	O
-	O
forward	O
network	O
.	O
For	O
a	O
single	O
decoder	O
layer	O
,	O
besides	O
the	O
above	O
two	O
basic	O
blocks	O
,	O
a	O
multi	O
-	O
head	O
cross	O
-	O
attention	O
follows	O
multi	O
-	O
head	O
selfattention	O
.	O
The	O
input	O
sequence	O
x	O
will	O
be	O
first	O
converted	O
to	O
a	O
sequence	O
of	O
vectors	O
and	O
fed	O
into	O
the	O
encoder	O
.	O
Then	O
the	O
output	O
of	O
the	O
N	O
-	O
th	O
encoder	O
layer	O
will	O
be	O
taken	O
as	O
source	O
hidden	O
states	O
and	O
fed	O
into	O
decoder	O
.	O
The	O
final	O
output	O
of	O
the	O
N	O
-	O
th	O
decoder	O
layer	O
gives	O
the	O
target	O
hidden	O
states	O
and	O
translate	O
the	O
target	O
sentences	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
the	O
datasets	O
using	O
in	O
our	O
experiments	O
on	O
many	O
-	O
to	O
-	O
many	O
and	O
one	O
-	O
to	O
-	O
many	O
multilingual	O
translation	O
scenarios	O
.	O
Many	O
-	O
to	O
-	O
Many	O
For	O
this	O
translation	O
scenario	O
,	O
we	O
test	O
our	O
approach	O
on	O
IWSLT	O
-	O
17	O
1	O
translation	O
datasets	O
,	O
including	O
English	O
,	O
Italian	O
,	O
Romanian	O
,	O
Dutch	O
(	O
briefly	O
,	O
En	O
,	O
It	O
,	O
Ro	O
,	O
Nl	O
)	O
.	O
We	O
experimented	O
in	O
eight	O
directions	O
,	O
including	O
It↔En	O
,	O
Ro↔En	O
,	O
Nl↔En	O
,	O
and	O
It↔Ro	O
,	O
with	O
231.6k	O
,	O
220.5k	O
,	O
237.2k	O
,	O
and	O
217.5k	O
data	O
for	O
each	O
language	O
pair	O
.	O
We	O
choose	O
test2016	O
and	O
test2017	O
as	O
our	O
development	O
and	O
test	O
set	O
,	O
respectively	O
.	O
Sentences	O
of	O
all	O
languages	O
were	O
tokenized	O
by	O
the	O
Moses	O
scripts	O
2	O
and	O
further	O
segmented	O
into	O
subword	O
symbols	O
using	O
Byte	O
-	O
Pair	O
Encoding	O
(	O
BPE	B-MethodName
)	O
rules	O
(	O
Sennrich	O
et	O
al	O
,	O
2016	O
)	O
with	O
40	O
K	O
merge	O
operations	O
for	O
all	O
languages	O
jointly	O
.	O
One	O
-	O
to	O
-	O
Many	O
We	O
evaluate	O
the	O
quality	O
of	O
our	O
multilingual	O
translation	O
models	O
using	O
training	O
data	O
from	O
the	O
Europarl	O
Corpus	O
3	O
,	O
Release	O
V7	O
.	O
Our	O
experiments	O
focus	O
on	O
English	O
to	O
twelve	O
primary	O
languages	O
:	O
Czech	O
,	O
Finnish	O
,	O
Greek	O
,	O
Hungarian	O
,	O
Lithuanian	O
,	O
Latvian	O
,	O
Polish	O
,	O
Portuguese	O
,	O
Slovak	O
,	O
Slovene	O
,	O
Swedish	O
,	O
Spanish	O
(	O
briefly	O
,	O
Cs	O
,	O
Fi	O
,	O
El	O
,	O
Hu	O
,	O
Lt	O
,	O
Lv	O
,	O
Pl	O
,	O
Pt	O
,	O
Sk	O
,	O
Sl	O
,	O
Sv	O
,	O
Es	O
)	O
.	O
For	O
each	O
language	O
pair	O
,	O
we	O
randomly	O
sampled	O
0.6	O
M	O
parallel	O
sentences	O
as	O
training	O
corpus	O
(	O
7.2	O
M	O
in	O
all	O
)	O
.	O
The	O
Europarl	O
evaluation	O
data	O
set	O
dev2006	O
is	O
used	O
as	O
our	O
validation	O
set	O
,	O
while	O
devtest2006	O
is	O
our	O
test	O
set	O
.	O
For	O
language	O
pairs	O
without	O
available	O
development	O
and	O
test	O
set	O
,	O
we	O
randomly	O
split	O
1	O
K	O
unseen	O
sentence	O
pairs	O
from	O
the	O
corresponding	O
training	O
set	O
as	O
the	O
development	O
and	O
test	O
data	O
respectively	O
.	O
We	O
tokenize	O
and	O
truecase	O
the	O
sentences	O
with	O
Moses	O
scripts	O
and	O
apply	O
a	O
jointly	O
-	O
learned	O
set	O
of	O
90k	O
BPE	B-MethodName
obtained	O
from	O
the	O
merged	O
source	O
and	O
target	O
sides	O
of	O
the	O
training	O
data	O
for	O
all	O
twelve	O
language	O
pairs	O
.	O

To	O
make	O
the	O
evaluation	O
convincing	O
,	O
we	O
reimplement	O
and	O
compare	O
our	O
method	O
with	O
four	O
baseline	O
systems	O
,	O
which	O
can	O
be	O
divided	O
into	O
two	O
categories	O
with	O
respect	O
to	O
the	O
number	O
of	O
models	O
.	O
The	O
multiple	O
-	O
model	O
approach	O
requires	O
maintaining	O
a	O
dedicated	O
NMT	O
model	O
for	O
each	O
language	O
:	O
Individual	O
A	O
NMT	O
model	O
is	O
trained	O
for	O
each	O
language	O
pair	O
.	O
Therefore	O
,	O
there	O
are	O
N	O
different	O
models	O
for	O
N	O
language	O
pairs	O
.	O
The	O
unified	O
model	O
-	O
based	O
methods	O
handle	O
multiple	O
languages	O
within	O
a	O
single	O
unified	O
NMT	O
model	O
:	O
Multilingual	O
(	O
Johnson	O
et	O
al	O
,	O
2017	O
)	O
Handling	O
multiple	O
languages	O
in	O
a	O
single	O
transformer	O
model	O
which	O
contains	O
one	O
encoder	O
and	O
one	O
decoder	O
with	O
a	O
special	O
language	O
indicator	O
lang	O
added	O
to	O
the	O
input	O
sentence	O
.	O
+	O
TS	B-MethodName
(	O
Blackwood	O
et	O
al	O
,	O
2018	O
)	O
This	O
method	O
assigns	O
language	O
-	O
specific	O
attention	O
modules	O
to	O
each	O
language	O
pair	O
.	O
We	O
implement	O
the	O
target	O
-	O
specific	O
attention	O
mechanism	O
because	O
of	O
its	O
excellent	O
performance	O
in	O
the	O
original	O
paper	O
.	O
+	O
Adapter	B-MethodName
This	O
method	O
injects	O
tiny	O
adapter	O
layers	O
for	O
specific	O
language	O
pairs	O
into	O
the	O
original	O
MNMT	O
model	O
.	O
We	O
set	O
the	O
dimension	O
of	O
projection	O
layer	O
to	O
128	O
and	O
train	O
the	O
model	O
from	O
scratch	O
.	O
Our	O
Method	O
-	O
AV	O
Our	O
model	O
is	O
trained	O
just	O
as	O
the	O
Approach	O
section	O
describes	O
.	O
In	O
this	O
system	O
,	O
we	O
adopt	O
the	O
absolute	O
value	O
based	O
method	O
to	O
evaluate	O
the	O
importance	O
of	O
neurons	O
across	O
languages	O
.	O
Our	O
Method	O
-	O
TE	O
This	O
system	O
is	O
implemented	O
the	O
same	O
as	O
the	O
system	O
Our	O
Method	O
-	O
AV	O
except	O
that	O
we	O
adopt	O
the	O
Taylor	O
Expansion	O
based	O
evaluation	O
method	O
as	O
shown	O
in	O
Equation	O
7	O
.	O
+	O
Expansion	O
To	O
make	O
a	O
fair	O
comparison	O
,	O
we	O
set	O
the	O
size	O
of	O
Feed	O
Forward	O
Network	O
to	O
3000	O
to	O
expand	O
the	O
model	O
capacity	O
up	O
to	O
the	O
level	O
of	O
other	O
baselines	O
,	O
and	O
then	O
apply	O
our	O
Taylor	O
Expansion	O
based	O
method	O
to	O
this	O
model	O
.	O

Our	O
work	O
closely	O
relates	O
to	O
language	O
-	O
specific	O
modeling	O
for	O
MNMT	O
and	O
model	O
pruning	O
which	O
we	O
will	O
recap	O
both	O
here	O
.	O
Early	O
MNMT	O
studies	O
focus	O
on	O
improving	O
the	O
sharing	O
capability	O
of	O
individual	O
bilingual	O
models	O
to	O
handle	O
multiple	O
languages	O
,	O
which	O
includes	O
sharing	O
encoders	O
(	O
Dong	O
et	O
al	O
,	O
2015	O
)	O
,	O
sharing	O
decoders	O
(	O
Zoph	O
et	O
al	O
,	O
2016	O
)	O
,	O
and	O
sharing	O
sublayers	O
(	O
Firat	O
et	O
al	O
,	O
2016	O
)	O
.	O
Later	O
,	O
Ha	O
et	O
al	O
(	O
2016	O
)	O
and	O
Johnson	O
et	O
al	O
(	O
2017	O
)	O
propose	O
an	O
universal	O
MNMT	O
model	O
with	O
a	O
target	O
language	O
token	O
to	O
indicate	O
the	O
translation	O
direction	O
.	O
While	O
this	O
paradigm	O
fully	O
explores	O
the	O
general	B-TaskName
knowledge	I-TaskName
between	O
languages	O
and	O
hard	O
to	O
obtain	O
the	O
specific	O
knowledge	O
of	O
each	O
language	O
(	O
Tan	O
et	O
al	O
,	O
2019	O
;	O
Aharoni	O
et	O
al	O
,	O
2019	O
)	O
,	O
the	O
subsequent	O
researches	O
resort	O
to	O
Language	O
-	O
specific	O
modeling	O
,	O
trying	O
to	O
find	O
a	O
better	O
trade	O
-	O
off	O
between	O
sharing	O
and	O
specific	O
.	O
Such	O
approaches	O
involve	O
inserting	O
conditional	O
languagespecific	O
routing	O
layer	O
(	O
Zhang	O
et	O
al	O
,	O
2021	O
)	O
,	O
specific	O
attention	O
networks	O
(	O
Blackwood	O
et	O
al	O
,	O
2018	O
;	O
Sachan	O
and	O
Neubig	O
,	O
2018	O
)	O
,	O
adding	O
task	O
adapters	O
,	O
and	O
training	O
model	O
with	O
different	O
language	O
clusters	O
(	O
Tan	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
so	O
on	O
.	O
However	O
,	O
these	O
methods	O
increase	O
the	O
capacity	O
of	O
the	O
model	O
which	O
makes	O
the	O
model	O
bloated	O
.	O
Moreover	O
,	O
our	O
method	O
is	O
also	O
related	O
to	O
model	O
pruning	O
,	O
which	O
usually	O
aims	O
to	O
reduce	O
the	O
model	O
size	O
or	O
improve	O
the	O
inference	O
efficiency	O
.	O
Model	O
pruning	O
has	O
been	O
widely	O
investigated	O
for	O
both	O
computer	O
vision	O
(	O
CV	O
)	O
(	O
Luo	O
et	O
al	O
,	O
2017	O
)	O
and	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
tasks	O
.	O
For	O
example	O
,	O
See	O
et	O
al	O
(	O
2016	O
)	O
examines	O
three	O
magnitude	O
-	O
based	O
pruning	O
schemes	O
,	O
Zhu	O
and	O
Gupta	O
(	O
2018	O
)	O
demonstrates	O
that	O
large	O
-	O
sparse	O
models	O
outperform	O
comparablysized	O
small	O
-	O
dense	O
models	O
,	O
and	O
Wang	O
et	O
al	O
(	O
2020a	O
)	O
improves	O
the	O
utilization	O
efficiency	O
of	O
parameters	O
by	O
introducing	O
a	O
rejuvenation	O
approach	O
.	O
Besides	O
,	O
Lan	O
et	O
al	O
(	O
2020	O
)	O
presents	O
two	O
parameter	O
reduction	O
techniques	O
to	O
lower	O
memory	O
consumption	O
and	O
increase	O
the	O
training	O
speed	O
of	O
BERT	B-MethodName
.	O

The	O
current	O
standard	O
models	O
of	O
multilingual	O
neural	O
machine	B-TaskName
translation	I-TaskName
fail	O
to	O
capture	O
the	O
characteristics	O
of	O
specific	O
languages	O
,	O
while	O
the	O
latest	O
researches	O
focus	O
on	O
the	O
pursuit	O
of	O
specific	O
knowledge	O
while	O
increasing	O
the	O
capacity	O
of	O
the	O
model	O
and	O
requiring	O
fine	O
manual	O
design	O
.	O
To	O
solve	O
the	O
problem	O
,	O
we	O
propose	O
an	O
importance	O
-	O
based	O
neuron	O
allocation	O
method	O
.	O
We	O
divide	O
neurons	O
to	O
general	O
neurons	O
and	O
language	O
-	O
specific	O
neurons	O
to	O
retain	O
general	B-TaskName
knowledge	I-TaskName
and	O
capture	O
language	O
-	O
specific	O
knowledge	O
without	O
model	O
capacity	O
incremental	O
and	O
specialized	O
design	O
.	O
The	O
experiments	O
prove	O
that	O
our	O
method	O
can	O
get	O
superior	O
translation	O
results	O
with	O
better	O
general	O
and	O
language	O
-	O
specific	O
knowledge	O
.	O

Graph	O
Neural	O
Networks	O
(	O
GNNs	O
)	O
that	O
capture	O
the	O
relationships	O
between	O
graph	O
nodes	O
via	O
message	O
passing	O
have	O
been	O
a	O
hot	O
research	O
direction	O
in	O
the	O
natural	O
language	O
processing	O
community	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
Graph	O
Topic	O
Model	O
(	O
GTM	O
)	O
,	O
a	O
GNN	O
based	O
neural	O
topic	O
model	O
that	O
represents	O
a	O
corpus	O
as	O
a	O
document	O
relationship	O
graph	O
.	O
Documents	O
and	O
words	O
in	O
the	O
corpus	O
become	O
nodes	O
in	O
the	O
graph	O
and	O
are	O
connected	O
based	O
on	O
document	O
-	O
word	O
cooccurrences	O
.	O
By	O
introducing	O
the	O
graph	O
structure	O
,	O
the	O
relationships	O
between	O
documents	O
are	O
established	O
through	O
their	O
shared	O
words	O
and	O
thus	O
the	O
topical	O
representation	O
of	O
a	O
document	O
is	O
enriched	O
by	O
aggregating	O
information	O
from	O
its	O
neighboring	O
nodes	O
using	O
graph	O
convolution	B-MethodName
.	O
Extensive	O
experiments	O
on	O
three	O
datasets	O
were	O
conducted	O
and	O
the	O
results	O
demonstrate	O
the	O
effectiveness	O
of	O
the	O
proposed	O
approach	O
.	O

Probabilistic	O
topic	B-TaskName
models	I-TaskName
(	O
Blei	O
,	O
2012	O
)	O
are	O
tools	O
for	O
discovering	O
main	O
themes	O
from	O
large	O
corpora	O
.	O
The	O
popular	O
Latent	O
Dirichlet	O
Allocation	O
(	O
LDA	B-MethodName
)	O
(	O
Blei	O
et	O
al	O
,	O
2003	O
)	O
and	O
its	O
variants	O
(	O
Lin	O
and	O
He	O
,	O
2009	O
;	O
Zhao	O
et	O
al	O
,	O
2010	O
;	O
Zhou	O
et	O
al	O
,	O
2014	O
)	O
are	O
effective	O
in	O
extracting	O
coherent	O
topics	O
in	O
an	O
interpretable	O
manner	O
,	O
but	O
usually	O
at	O
the	O
cost	O
of	O
designing	O
sophisticated	O
and	O
model	O
-	O
specific	O
learning	O
algorithm	O
.	O
Recently	O
,	O
neural	O
topic	O
modeling	O
that	O
utilizes	O
neuralnetwork	O
-	O
based	O
black	O
-	O
box	O
inference	O
has	O
been	O
the	O
main	O
research	O
direction	O
in	O
this	O
field	O
.	O
Notably	O
,	O
NVDM	O
(	O
Miao	O
et	O
al	O
,	O
2016	O
)	O
employs	O
variational	B-MethodName
autoencoder	I-MethodName
(	O
VAE	B-MethodName
)	O
(	O
Kingma	O
and	O
Welling	O
,	O
2013	O
)	O
to	O
model	O
topic	O
inference	O
and	O
document	O
generation	O
.	O
Specifically	O
,	O
NVDM	O
consists	O
of	O
an	O
encoder	O
inferring	O
topics	O
from	O
documents	O
and	O
a	O
decoder	O
generating	O
documents	O
from	O
topics	O
,	O
where	O
the	O
latent	O
topics	O
are	O
constrained	O
by	O
a	O
Gaussian	O
prior	O
.	O
Srivastava	O
and	O
Sutton	O
(	O
2017	O
)	O
argued	O
that	O
Dirichlet	O
distribution	O
is	O
a	O
more	O
appropriate	O
prior	O
for	O
topic	O
modeling	O
than	O
Gaussian	O
in	O
NVDM	O
and	O
proposed	O
ProdLDA	O
that	O
approximates	O
the	O
Dirichlet	O
prior	O
with	O
logistic	O
normal	O
.	O
There	O
are	O
also	O
attempts	O
that	O
directly	O
enforced	O
a	O
Dirichlet	O
prior	O
on	O
the	O
document	O
topics	O
.	O
W	O
-	O
LDA	B-MethodName
(	O
Nan	O
et	O
al	O
,	O
2019	O
)	O
models	O
topics	O
in	O
the	O
Wasserstein	O
autoencoders	B-MethodName
(	O
Tolstikhin	O
et	O
al	O
,	O
2017	O
)	O
framework	O
and	O
achieves	O
distribution	O
matching	O
by	O
minimizing	O
their	O
Maximum	O
Mean	O
Discrepancy	O
(	O
MMD	B-DatasetName
)	O
(	O
Gretton	O
et	O
al	O
,	O
2012	O
)	O
,	O
while	O
adversarial	O
topic	O
model	O
(	O
Wang	O
et	O
al	O
,	O
2019a	O
(	O
Wang	O
et	O
al	O
,	O
,	O
b	O
,	O
2020	O
directly	O
generates	O
documents	O
from	O
the	O
Dirichlet	O
prior	O
and	O
such	O
a	O
process	O
is	O
adversarially	O
trained	O
with	O
a	O
discriminator	O
under	O
the	O
framework	O
of	O
Generative	B-MethodName
Adversarial	I-MethodName
Network	I-MethodName
(	O
GAN	B-MethodName
)	O
(	O
Goodfellow	O
et	O
al	O
,	O
2014	O
)	O
.	O
Recently	O
,	O
due	O
to	O
the	O
effectiveness	O
of	O
Graph	O
Neural	O
Networks	O
(	O
GNNs	O
)	O
(	O
Li	O
et	O
al	O
,	O
2015	O
;	O
Kipf	O
and	O
Welling	O
,	O
2016	O
;	O
Zhou	O
et	O
al	O
,	O
2018	O
)	O
in	O
embedding	O
graph	O
structures	O
,	O
there	O
is	O
a	O
surge	O
of	O
interests	O
of	O
applying	O
GNN	O
to	O
natural	O
language	O
processing	O
tasks	O
(	O
Yasunaga	O
et	O
al	O
,	O
2017	O
;	O
Song	O
et	O
al	O
,	O
2018	O
;	O
Yao	O
et	O
al	O
,	O
2019	O
)	O
.	O
For	O
example	O
,	O
GraphBTM	O
(	O
Zhu	O
et	O
al	O
,	O
2018	O
)	O
is	O
a	O
neural	O
topic	O
model	O
that	O
incorporates	O
the	O
graph	O
representation	O
of	O
a	O
document	O
to	O
capture	O
biterm	O
cooccurrences	O
in	O
the	O
document	O
.	O
To	O
construct	O
the	O
graph	O
,	O
a	O
sliding	O
window	O
over	O
the	O
document	O
is	O
employed	O
and	O
all	O
word	O
pairs	O
in	O
the	O
window	O
are	O
connected	O
.	O
A	O
limitation	O
of	O
GraphBTM	O
is	O
that	O
only	O
word	O
relationships	O
are	O
considered	O
while	O
ignoring	O
document	O
relationships	O
.	O
Since	O
a	O
topic	O
is	O
possessed	O
by	O
a	O
subset	O
of	O
documents	O
in	O
the	O
corpus	O
,	O
we	O
believe	O
that	O
the	O
topical	O
neighborhood	O
of	O
a	O
document	O
,	O
i.e.	O
,	O
documents	O
with	O
similar	O
topics	O
,	O
would	O
help	O
determine	O
the	O
topics	O
of	O
a	O
document	O
.	O
To	O
this	O
end	O
,	O
we	O
propose	O
Graph	O
Topic	O
Model	O
(	O
GTM	O
)	O
,	O
a	O
neural	O
topic	O
model	O
that	O
a	O
corpus	O
is	O
represented	O
as	O
a	O
document	O
relationship	O
graph	O
where	O
documents	O
and	O
words	O
in	O
the	O
corpus	O
are	O
nodes	O
and	O
they	O
are	O
connected	O
based	O
on	O
document	O
-	O
word	O
co	O
-	O
occurrences	O
.	O
In	O
GTM	O
,	O
the	O
topical	O
representation	O
of	O
a	O
document	O
node	O
is	O
aggregated	O
from	O
its	O
multi	O
-	O
hop	O
neighborhood	O
,	O
including	O
both	O
document	O
and	O
word	O
nodes	O
,	O
using	O
Graph	B-MethodName
Convolutional	I-MethodName
Network	I-MethodName
(	O
GCN	B-MethodName
)	O
(	O
Kipf	O
and	O
Welling	O
,	O
2016	O
)	O
.	O
As	O
GCN	B-MethodName
is	O
able	O
to	O
capture	O
high	O
-	O
order	O
neighborhood	O
relationships	O
,	O
GTM	O
is	O
essentially	O
capable	O
of	O
modeling	O
both	O
word	O
-	O
word	O
and	O
doc	O
-	O
doc	O
relationships	O
.	O
In	O
specific	O
,	O
the	O
relationships	O
between	O
relevant	O
documents	O
are	O
established	O
by	O
their	O
shared	O
words	O
,	O
which	O
is	O
desirable	O
for	O
topic	O
modeling	O
as	O
documents	O
belonging	O
to	O
one	O
topic	O
typically	O
have	O
similar	O
word	O
distributions	O
.	O
The	O
main	O
contributions	O
of	O
the	O
paper	O
are	O
:	O
We	O
propose	O
GTM	O
,	O
a	O
novel	O
topic	O
model	O
that	O
incorporates	O
document	O
relationship	O
graph	O
to	O
enrich	O
document	O
and	O
word	O
representations	O
.	O
We	O
extensively	O
experimented	O
on	O
three	O
datasets	O
and	O
the	O
results	O
demonstrate	O
the	O
effectiveness	O
of	O
the	O
proposed	O
approach	O
.	O
2	O
Graph	O
Topic	O
Model	O

We	O
have	O
introduced	O
Graph	O
Topic	O
Model	O
,	O
a	O
neural	O
topic	O
model	O
that	O
incorporates	O
corpus	O
-	O
level	O
neighboring	O
context	O
using	O
graph	O
convolutions	O
to	O
enrich	O
document	O
representations	O
and	O
facilitate	O
the	O
topic	O
inference	O
.	O
Both	O
quantitative	O
and	O
qualitative	O
results	O
are	O
presented	O
in	O
the	O
experiments	O
to	O
demonstrate	O
the	O
effectiveness	O
of	O
the	O
proposed	O
approach	O
.	O
In	O
the	O
future	O
,	O
we	O
would	O
like	O
to	O
extend	O
GTM	O
to	O
corpora	O
with	O
explicit	O
doc	O
-	O
doc	O
interactions	O
,	O
e.g.	O
,	O
scientific	O
documents	O
with	O
citations	O
or	O
social	O
media	O
posts	O
with	O
user	O
relationships	O
.	O
Replacing	O
GCN	B-MethodName
in	O
GTM	O
with	O
more	O
advanced	O
graph	O
neural	O
networks	O
is	O
another	O
promising	O
research	O
direction	O
.	O

With	O
the	O
introduction	O
of	O
connotative	O
valid	O
facts	O
,	O
knowledge	O
inference	O
on	O
knowledge	O
graph	O
improves	O
the	O
performance	O
of	O
many	O
downstream	O
applications	O
,	O
such	O
as	O
vertical	O
search	O
and	O
question	B-TaskName
answering	I-TaskName
(	O
Dong	O
et	O
al	O
,	O
2015	O
;	O
Lukovnikov	O
et	O
al	O
,	O
2017	O
)	O
.	O
Existing	O
studies	O
(	O
Nickel	O
et	O
al	O
,	O
2016	O
;	O
Wang	O
et	O
al	O
,	O
2017	O
)	O
mainly	O
focus	O
on	O
knowledge	O
inference	O
on	O
binary	O
facts	O
with	O
two	O
entities	O
connected	O
with	O
a	O
certain	O
binary	O
relation	O
,	O
represented	O
as	O
triples	O
,	O
(	O
head	O
entity	O
,	O
relation	O
,	O
tail	O
entity	O
)	O
.	O
They	O
attempt	O
to	O
infer	O
the	O
unknown	O
head	O
/	O
tail	O
entity	O
or	O
the	O
unknown	O
relation	O
of	O
a	O
given	O
binary	O
fact	O
.	O
However	O
,	O
n	O
-	O
ary	O
facts	O
involving	O
more	O
than	O
two	O
entities	O
are	O
also	O
ubiquitous	O
.	O
For	O
example	O
,	O
in	O
Freebase	O
,	O
more	O
than	O
1/3	O
entities	O
participate	O
in	O
n	O
-	O
ary	O
facts	O
(	O
Wen	O
et	O
al	O
,	O
2016	O
)	O
.	O
The	O
fact	O
that	O
John	O
Bardeen	O
received	O
N	O
obel	O
P	O
rize	O
in	O
P	O
hysics	O
in	O
1956	O
together	O
with	O
W	O
alter	O
Houser	O
Brattain	O
and	O
W	O
illiam	O
Shockley	O
1	O
is	O
a	O
typical	O
5ary	O
fact	O
.	O
So	O
far	O
,	O
only	O
a	O
few	O
studies	O
(	O
Wen	O
et	O
al	O
,	O
2016	O
;	O
Zhang	O
et	O
al	O
,	O
2018	O
;	O
Guan	O
et	O
al	O
,	O
2019	O
)	O
have	O
tried	O
to	O
address	O
knowledge	O
inference	O
on	O
n	O
-	O
ary	O
facts	O
.	O
In	O
existing	O
studies	O
for	O
knowledge	O
inference	O
on	O
nary	O
facts	O
,	O
each	O
n	O
-	O
ary	O
fact	O
is	O
represented	O
as	O
a	O
group	O
of	O
peer	O
attributes	O
and	O
attribute	O
values	O
.	O
In	O
practice	O
,	O
for	O
each	O
n	O
-	O
ary	O
fact	O
,	O
there	O
is	O
usually	O
a	O
primary	O
triple	O
(	O
the	O
main	O
focus	O
of	O
the	O
n	O
-	O
ary	O
fact	O
)	O
,	O
and	O
other	O
attributes	O
along	O
with	O
the	O
corresponding	O
attribute	O
values	O
are	O
its	O
auxiliary	O
descriptions	O
.	O
Take	O
the	O
above	O
5	O
-	O
ary	O
fact	O
for	O
example	O
,	O
the	O
primary	O
triple	O
is	O
(	O
John	O
Bardeen	O
,	O
award	O
-	O
received	O
,	O
N	O
obel	O
P	O
rize	O
in	O
P	O
hysics	O
)	O
,	O
and	O
other	O
attribute	O
-	O
value	O
pairs	O
including	O
point	O
-	O
in	O
-	O
time	O
:	O
1956	O
,	O
together	O
-	O
with	O
:	O
W	O
alter	O
Houser	O
Brattain	O
and	O
together	O
-	O
with	O
:	O
W	O
illiam	O
Shockley	O
are	O
its	O
auxiliary	O
descriptions	O
.	O
Actually	O
,	O
in	O
YAGO	B-DatasetName
(	O
Suchanek	O
et	O
al	O
,	O
2007	O
)	O
and	O
Wikidata	O
(	O
Vrandečić	O
and	O
Krötzsch	O
,	O
2014	O
)	O
,	O
a	O
primary	O
triple	O
is	O
identified	O
for	O
each	O
n	O
-	O
ary	O
fact	O
.	O
The	O
above	O
5	O
-	O
ary	O
fact	O
is	O
a	O
relatively	O
complete	O
example	O
.	O
In	O
the	O
real	O
-	O
world	O
scenario	O
,	O
many	O
n	O
-	O
ary	O
facts	O
appear	O
as	O
only	O
partial	O
ones	O
,	O
each	O
consisting	O
of	O
a	O
primary	O
triple	O
and	O
a	O
subset	O
of	O
its	O
auxiliary	O
description	O
(	O
s	O
)	O
,	O
due	O
to	O
incomplete	O
knowledge	O
acquisition	O
.	O
For	O
example	O
,	O
(	O
John	O
Bardeen	O
,	O
awardreceived	O
,	O
N	O
obel	O
P	O
rize	O
in	O
P	O
hysics	O
)	O
with	O
pointin	O
-	O
time	O
:	O
1956	O
and	O
it	O
with	O
{	O
together	O
-	O
with	O
:	O
W	O
alter	O
Houser	O
Brattain	O
,	O
together	O
-	O
with	O
:	O
W	O
illiam	O
Shockley	O
}	O
are	O
two	O
typical	O
partial	O
facts	O
corresponding	O
to	O
the	O
above	O
5	O
-	O
ary	O
fact	O
.	O
For	O
differentiation	O
,	O
we	O
call	O
those	O
relatively	O
complete	O
facts	O
as	O
whole	O
ones	O
.	O
We	O
noticed	O
that	O
existing	O
studies	O
on	O
n	O
-	O
ary	O
facts	O
infer	O
an	O
unknown	O
element	O
in	O
a	O
welldefined	O
whole	O
fact	O
and	O
have	O
not	O
paid	O
attention	O
to	O
knowledge	O
inference	O
on	O
partial	O
facts	O
.	O
Later	O
on	O
,	O
we	O
refer	O
the	O
former	O
as	O
simple	O
knowledge	O
inference	O
,	O
while	O
the	O
latter	O
as	O
flexible	O
knowledge	O
inference	O
.	O
With	O
these	O
considerations	O
in	O
mind	O
,	O
in	O
this	O
paper	O
,	O
by	O
discriminating	O
the	O
information	O
in	O
the	O
same	O
n	O
-	O
ary	O
fact	O
,	O
we	O
propose	O
a	O
neural	O
network	O
model	O
,	O
called	O
NeuInfer	O
,	O
to	O
conduct	O
both	O
simple	O
and	O
flexible	O
knowledge	O
inference	O
on	O
n	O
-	O
ary	O
facts	O
.	O
Our	O
specific	O
contributions	O
are	O
summarized	O
as	O
:	O
We	O
treat	O
the	O
information	O
in	O
the	O
same	O
n	O
-	O
ary	O
fact	O
discriminatingly	O
and	O
represent	O
each	O
n	O
-	O
ary	O
fact	O
as	O
a	O
primary	O
triple	O
coupled	O
with	O
a	O
set	O
of	O
its	O
auxiliary	O
descriptive	O
attribute	O
-	O
value	O
pair	O
(	O
s	O
)	O
.	O
We	O
propose	O
a	O
neural	O
network	O
model	O
,	O
NeuInfer	O
,	O
for	O
knowledge	O
inference	O
on	O
n	O
-	O
ary	O
facts	O
.	O
NeuInfer	O
can	O
particularly	O
handle	O
the	O
new	O
type	O
of	O
task	O
,	O
flexible	O
knowledge	O
inference	O
,	O
which	O
infers	O
an	O
unknown	O
element	O
in	O
a	O
partial	O
fact	O
consisting	O
of	O
a	O
primary	O
triple	O
and	O
any	O
number	O
of	O
its	O
auxiliary	O
description	O
(	O
s	O
)	O
.	O
Experimental	O
results	O
validate	O
the	O
significant	O
effectiveness	O
and	O
superiority	O
of	O
NeuInfer	O
.	O
2	O
Related	O
Works	O

They	O
can	O
be	O
divided	O
into	O
tensor	O
/	O
matrix	O
based	O
methods	O
,	O
translation	O
based	O
methods	O
,	O
and	O
neural	O
network	O
based	O
ones	O
.	O
The	O
quintessential	O
one	O
of	O
tensor	O
/	O
matrix	O
based	O
methods	O
is	O
RESCAL	B-MethodName
(	O
Nickel	O
et	O
al	O
,	O
2011	O
)	O
.	O
It	O
relates	O
a	O
knowledge	O
graph	O
to	O
a	O
three	O
-	O
way	O
tensor	O
of	O
head	O
entities	O
,	O
relations	O
,	O
and	O
tail	O
entities	O
.	O
The	O
learned	O
embeddings	O
of	O
entities	O
and	O
relations	O
via	O
minimizing	O
the	O
reconstruction	O
error	O
of	O
the	O
tensor	O
are	O
used	O
to	O
reconstruct	O
the	O
tensor	O
.	O
And	O
binary	O
facts	O
corresponding	O
to	O
entries	O
of	O
large	O
values	O
are	O
treated	O
as	O
valid	O
.	O
Similarly	O
,	O
ComplEx	O
(	O
Trouillon	O
et	O
al	O
,	O
2016	O
)	O
relates	O
each	O
relation	O
to	O
a	O
matrix	O
of	O
head	O
and	O
tail	O
entities	O
,	O
which	O
is	O
decomposed	O
and	O
learned	O
like	O
RESCAL	B-MethodName
.	O
To	O
improve	O
the	O
embeddings	O
and	O
thus	O
the	O
performance	O
of	O
inference	O
,	O
researchers	O
further	O
introduce	O
the	O
constraints	O
of	O
entities	O
and	O
relations	O
(	O
Ding	O
et	O
al	O
,	O
2018	O
;	O
Jain	O
et	O
al	O
,	O
2018	O
)	O
.	O
Translation	B-TaskName
based	O
methods	O
date	O
back	O
to	O
TransE	B-MethodName
(	O
Bordes	O
et	O
al	O
,	O
2013	O
)	O
.	O
It	O
views	O
each	O
valid	O
binary	O
fact	O
as	O
the	O
translation	O
from	O
the	O
head	O
entity	O
to	O
the	O
tail	O
entity	O
via	O
their	O
relation	O
.	O
Thus	O
,	O
the	O
score	O
function	O
indicating	O
the	O
validity	O
of	O
the	O
fact	O
is	O
defined	O
based	O
on	O
the	O
similarity	O
between	O
the	O
translation	O
result	O
and	O
the	O
tail	O
entity	O
.	O
Then	O
,	O
a	O
flurry	O
of	O
methods	O
spring	O
up	O
(	O
Wang	O
et	O
al	O
,	O
2014	O
;	O
Lin	O
et	O
al	O
,	O
2015b	O
;	O
Ji	O
et	O
al	O
,	O
2015	O
;	O
Guo	O
et	O
al	O
,	O
2015	O
;	O
Lin	O
et	O
al	O
,	O
2015a	O
;	O
Xiao	O
et	O
al	O
,	O
2016	O
;	O
Jia	O
et	O
al	O
,	O
2016	O
;	O
Tay	O
et	O
al	O
,	O
2017	O
;	O
Ebisu	O
and	O
Ichise	O
,	O
2018	O
;	O
Chen	O
et	O
al	O
,	O
2019	O
)	O
.	O
They	O
modify	O
the	O
above	O
translation	O
assumption	O
or	O
introduce	O
additional	O
information	O
and	O
constraints	O
.	O
Among	O
them	O
,	O
TransH	O
(	O
Wang	O
et	O
al	O
,	O
2014	O
)	O
translates	O
on	O
relationspecific	O
hyperplanes	O
.	O
Entities	O
are	O
projected	O
into	O
the	O
hyperplanes	O
of	O
relations	O
before	O
translating	O
.	O
Neural	O
network	O
based	O
methods	O
model	O
the	O
validity	O
of	O
binary	O
facts	O
or	O
the	O
inference	O
processes	O
.	O
For	O
example	O
,	O
ConvKB	O
(	O
Nguyen	O
et	O
al	O
,	O
2018	O
)	O
treats	O
each	O
binary	O
fact	O
as	O
a	O
three	O
-	O
column	O
matrix	O
.	O
This	O
matrix	O
is	O
fed	O
into	O
a	O
convolution	B-MethodName
layer	O
,	O
followed	O
by	O
a	O
concatenation	O
layer	O
and	O
a	O
fully	O
-	O
connected	O
layer	O
to	O
generate	O
a	O
validity	O
score	O
.	O
Nathani	O
et	O
al	O
(	O
2019	O
)	O
further	O
proposes	O
a	O
generalized	O
graph	O
attention	O
model	O
as	O
the	O
encoder	O
to	O
capture	O
neighborhood	O
features	O
and	O
applies	O
ConvKB	O
as	O
the	O
decoder	O
.	O
ConvE	O
(	O
Dettmers	O
et	O
al	O
,	O
2018	O
)	O
models	O
entity	O
inference	O
process	O
via	O
2D	O
convolution	B-MethodName
over	O
the	O
reshaped	O
then	O
concatenated	O
embedding	O
of	O
the	O
known	O
entity	O
and	O
relation	O
.	O
ConvR	O
(	O
Jiang	O
et	O
al	O
,	O
2019	O
)	O
further	O
adaptively	O
constructs	O
convolution	B-MethodName
filters	O
from	O
relation	O
embedding	O
and	O
applies	O
these	O
filters	O
across	O
entity	O
embedding	O
to	O
generate	O
convolutional	O
features	O
.	O
SENN	O
(	O
Guan	O
et	O
al	O
,	O
2018	O
)	O
models	O
the	O
inference	O
processes	O
of	O
head	O
entities	O
,	O
tail	O
entities	O
,	O
and	O
relations	O
via	O
fullyconnected	O
neural	O
networks	O
,	O
and	O
integrates	O
them	O
into	O
a	O
unified	O
framework	O
.	O

This	O
component	O
estimates	O
the	O
validity	O
of	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
,	O
including	O
the	O
acquisition	O
of	O
its	O
interaction	O
vector	O
and	O
the	O
assessment	O
of	O
its	O
validity	O
,	O
corresponding	O
to	O
"	O
hrt	O
-	O
FCNs	O
"	O
and	O
"	O
FCN	B-MethodName
1	O
"	O
in	O
Figure	O
1	O
,	O
respectively	O
.	O
Detailedly	O
,	O
the	O
embeddings	O
of	O
h	O
,	O
r	O
,	O
and	O
t	O
are	O
concatenated	O
and	O
fed	O
into	O
a	O
fully	O
-	O
connected	O
neural	O
network	O
.	O
After	O
layer	O
-	O
by	O
-	O
layer	O
learning	O
,	O
the	O
last	O
layer	O
outputs	O
the	O
interaction	O
vector	O
o	O
hrt	O
of	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
:	O
o	O
hrt	O
=	O
f	O
(	O
f	O
(	O
f	O
(	O
f	O
(	O
[	O
h	O
;	O
r	O
;	O
t	O
]	O
W	O
1	O
,	O
1	O
+	O
b	O
1	O
,	O
1	O
)	O
W	O
1	O
,	O
2	O
+	O
b	O
1	O
,	O
2	O
)	O
)	O
W	O
1	O
,	O
n	O
1	O
+	O
b	O
1	O
,	O
n	O
1	O
)	O
,	O
(	O
1	O
)	O
where	O
f	O
(	O
)	O
is	O
the	O
ReLU	B-MethodName
function	O
;	O
n	O
1	O
is	O
the	O
number	O
of	O
the	O
neural	O
network	O
layers	O
;	O
{	O
W	O
1	O
,	O
1	O
,	O
W	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
W	O
1	O
,	O
n	O
1	O
}	O
and	O
{	O
b	O
1	O
,	O
1	O
,	O
b	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
b	O
1	O
,	O
n	O
1	O
}	O
are	O
their	O
weight	O
matrices	O
and	O
bias	O
vectors	O
,	O
respectively	O
.	O
With	O
o	O
hrt	O
as	O
the	O
input	O
,	O
the	O
validity	O
score	O
val	O
hrt	O
of	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
is	O
computed	O
via	O
a	O
fully	O
-	O
connected	O
layer	O
and	O
then	O
the	O
sigmoid	O
operation	O
:	O
val	O
hrt	O
=	O
σ	O
(	O
o	O
hrt	O
W	O
val	O
+	O
b	O
val	O
)	O
,	O
(	O
2	O
)	O
where	O
W	O
val	O
and	O
b	O
val	O
are	O
the	O
weight	O
matrix	O
and	O
bias	O
variable	O
,	O
respectively	O
;	O
σ	O
(	O
x	O
)	O
=	O
1	O
1+e	O
−x	O
is	O
the	O
sigmoid	O
function	O
,	O
which	O
constrains	O
val	O
hrt	O
(	O
0	B-DatasetName
,	O
1	O
)	O
.	O
For	O
simplicity	O
,	O
the	O
number	O
of	O
hidden	O
nodes	O
in	O
each	O
fully	O
-	O
connected	O
layer	O
of	O
"	O
hrt	O
-	O
FCNs	O
"	O
and	O
"	O
FCN	B-MethodName
1	O
"	O
gradually	O
reduces	O
with	O
the	O
same	O
difference	O
between	O
layers	O
.	O

This	O
component	O
estimates	O
the	O
compatibility	O
of	O
F	O
ct	O
.	O
It	O
contains	O
three	O
sub	O
-	O
processes	O
,	O
i.e.	O
,	O
the	O
capture	O
of	O
the	O
interaction	O
vector	O
between	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
and	O
each	O
auxiliary	O
description	O
a	O
i	O
:	O
v	O
i	O
(	O
i	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
m	O
)	O
,	O
the	O
acquisition	O
of	O
the	O
overall	O
interaction	O
vector	O
,	O
and	O
the	O
assessment	O
of	O
the	O
compatibility	O
of	O
F	O
ct	O
,	O
corresponding	O
to	O
"	O
hrtav	O
-	O
FCNs	O
"	O
,	O
"	O
min	O
"	O
and	O
"	O
FCN	B-MethodName
2	O
"	O
in	O
Figure	O
1	O
,	O
respectively	O
.	O
Similar	O
to	O
"	O
hrt	O
-	O
FCNs	O
"	O
,	O
we	O
obtain	O
the	O
interaction	O
vector	O
o	O
hrta	O
i	O
v	O
i	O
of	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
and	O
a	O
i	O
:	O
v	O
i	O
:	O
o	O
hrta	O
i	O
v	O
i	O
=	O
f	O
(	O
f	O
(	O
f	O
(	O
f	O
(	O
[	O
h	O
;	O
r	O
;	O
t	O
;	O
a	O
i	O
;	O
v	O
i	O
]	O
W	O
2	O
,	O
1	O
+	O
b	O
2	O
,	O
1	O
)	O
W	O
2	O
,	O
2	O
+	O
b	O
2	O
,	O
2	O
)	O
)	O
W	O
2	O
,	O
n	O
2	O
+	O
b	O
2	O
,	O
n	O
2	O
)	O
,	O
(	O
3	O
)	O
where	O
n	O
2	O
is	O
the	O
number	O
of	O
the	O
neural	O
network	O
layers	O
;	O
{	O
W	O
2	O
,	O
1	O
,	O
W	O
2	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
W	O
2	O
,	O
n	O
2	O
}	O
and	O
{	O
b	O
2	O
,	O
1	O
,	O
b	O
2	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
b	O
2	O
,	O
n	O
2	O
}	O
are	O
their	O
weight	O
matrices	O
and	O
bias	O
vectors	O
,	O
respectively	O
.	O
The	O
number	O
of	O
hidden	O
nodes	O
in	O
each	O
fully	O
-	O
connected	O
layer	O
also	O
gradually	O
reduces	O
with	O
the	O
same	O
difference	O
between	O
layers	O
.	O
And	O
the	O
dimension	O
of	O
the	O
resulting	O
o	O
hrta	O
i	O
v	O
i	O
is	O
d.	O
All	O
the	O
auxiliary	O
descriptions	O
share	O
the	O
same	O
parameters	O
in	O
this	O
sub	O
-	O
process	O
.	O
The	O
overall	O
interaction	O
vector	O
o	O
hrtav	O
of	O
F	O
ct	O
is	O
generated	O
based	O
on	O
o	O
hrta	O
i	O
v	O
i	O
.	O
Before	O
introducing	O
this	O
sub	O
-	O
process	O
,	O
let	O
us	O
see	O
the	O
principle	O
behind	O
first	O
.	O
Straightforwardly	O
,	O
if	O
F	O
ct	O
is	O
valid	O
,	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
should	O
be	O
compatible	O
with	O
any	O
of	O
its	O
auxiliary	O
description	O
.	O
Then	O
,	O
the	O
values	O
of	O
their	O
interaction	O
vector	O
,	O
measuring	O
the	O
compatibility	O
in	O
many	O
different	O
views	O
,	O
are	O
all	O
encouraged	O
to	O
be	O
large	O
.	O
Therefore	O
,	O
for	O
each	O
dimension	O
,	O
the	O
minimum	O
over	O
it	O
of	O
all	O
the	O
interaction	O
vectors	O
is	O
not	O
allowed	O
to	O
be	O
too	O
small	O
.	O
Thus	O
,	O
the	O
overall	O
interaction	O
vector	O
o	O
hrtav	O
of	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
and	O
its	O
auxiliary	O
description	O
(	O
s	O
)	O
is	O
:	O
o	O
hrtav	O
=	O
min	O
m	O
i=1	O
(	O
o	O
hrta	O
i	O
v	O
i	O
)	O
,	O
(	O
4	O
)	O
where	O
min	O
(	O
)	O
is	O
the	O
element	O
-	O
wise	O
minimizing	O
function	O
.	O
Then	O
,	O
similar	O
to	O
"	O
FCN	B-MethodName
1	O
"	O
,	O
we	O
obtain	O
the	O
compatibility	O
score	O
comp	O
F	O
ct	O
of	O
F	O
ct	O
:	O
comp	O
F	O
ct	O
=	O
σ	O
(	O
o	O
hrtav	O
W	O
comp	O
+	O
b	O
comp	O
)	O
,	O
(	O
5	O
)	O
where	O
W	O
comp	O
of	O
dimension	O
d	O
×	O
1	O
and	O
b	O
comp	O
are	O
the	O
weight	O
matrix	O
and	O
bias	O
variable	O
,	O
respectively	O
.	O

Simple	O
knowledge	O
inference	O
includes	O
simple	O
entity	O
inference	O
and	O
simple	O
relation	O
inference	O
.	O
For	O
an	O
nary	O
fact	O
,	O
they	O
infer	O
one	O
of	O
the	O
entities	O
/	O
the	O
relation	O
in	O
Method	O
JF17	O
K	O
WikiPeople	O
MRR	B-MetricName
Hits@1	B-MetricName
Hits@3	B-MetricName
Hits@10	B-MetricName
MRR	B-MetricName
Hits@1	B-MetricName
Hits@3	B-MetricName
Hits@10	B-MetricName
the	O
primary	O
triple	O
or	O
the	O
attribute	O
value	O
/	O
attribute	O
in	O
an	O
auxiliary	O
description	O
,	O
given	O
its	O
other	O
information	O
.	O

Knowledge	O
inference	O
methods	O
on	O
n	O
-	O
ary	O
facts	O
are	O
scarce	O
.	O
The	O
representative	O
methods	O
are	O
m	O
-	O
TransH	O
(	O
Wen	O
et	O
al	O
,	O
2016	O
)	O
and	O
its	O
modified	O
version	O
RAE	B-MethodName
(	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
one	O
is	O
NaLP	O
(	O
Guan	O
et	O
al	O
,	O
2019	O
)	O
.	O
As	O
m	O
-	O
TransH	O
is	O
worse	O
than	O
RAE	B-MethodName
,	O
following	O
NaLP	O
,	O
we	O
do	O
not	O
adopt	O
it	O
as	O
a	O
baseline	O
.	O

Since	O
RAE	B-MethodName
is	O
deliberately	O
developed	O
only	O
for	O
simple	O
entity	O
inference	O
,	O
we	O
compare	O
NeuInfer	O
only	O
with	O
NaLP	O
on	O
simple	O
relation	O
inference	O
.	O

Transformer	B-MethodName
based	O
Natural	O
Language	O
Generation	O
for	O
Question	O
-	O
Answering	O

This	O
paper	O
explores	O
Natural	O
Language	O
Generation	O
within	O
the	O
context	O
of	O
Question	O
-	O
Answering	O
task	O
.	O
The	O
several	O
works	O
addressing	O
this	O
task	O
only	O
focused	O
on	O
generating	O
a	O
short	O
answer	O
or	O
a	O
long	O
text	O
span	O
that	O
contains	O
the	O
answer	O
,	O
while	O
reasoning	O
over	O
a	O
Web	O
page	O
or	O
processing	O
structured	O
data	O
.	O
Such	O
answers	O
'	O
length	O
are	O
usually	O
not	O
appropriate	O
as	O
the	O
answer	O
tend	O
to	O
be	O
perceived	O
as	O
too	O
brief	O
or	O
too	O
long	O
to	O
be	O
read	O
out	O
loud	O
by	O
an	O
intelligent	O
assistant	O
.	O
In	O
this	O
work	O
,	O
we	O
aim	O
at	O
generating	O
a	O
concise	O
answer	O
for	O
a	O
given	O
question	O
using	O
an	O
unsupervised	O
approach	O
that	O
does	O
not	O
require	O
annotated	O
data	O
.	O
Tested	O
over	O
English	O
and	O
French	O
datasets	O
,	O
the	O
proposed	O
approach	O
shows	O
very	O
promising	O
results	O
.	O
25	O
50	O
rank	O
A1	O
/	O
Bert	O
/	O
Cmbert	O
-	O
base	O
A2	O
/	O
Bert	O
/	O
flaubert	O
-	O
small	O
A1	O
/	O
Bert	O
/	O
xlm	B-MethodName
-	O
roberta	O
-	O
base	O
A1	O
/	O
Bert	O
/	O
flaubert	O
-	O
base	O
-	O
unc	O
A1	O
/	O
Bert	O
/	O
xlm	B-MethodName
-	O
mlm	B-DatasetName
-	O
enfr	O
-	O
1024	O
A1	O
/	O
Bert	O
/	O
xlm	B-MethodName
-	O
roberta	O
-	O
large	O
A2	O
/	O
Bert	O
/	O
gpt2	O
A1	O
/	O
Bert	O
/	O
bert	O
-	O
base	O
-	O
mlg	O
-	O
unc	O
A1	O
/	O
Bert	O
/	O
bert	O
-	O
base	O
-	O
mlg	O
A2	O
/	O
Bert	O
/	O
xlm	B-MethodName
-	O
clm	O
-	O
enfr	O
-	O
1024	O
A1	O
/	O
Bert	O
/	O
xlm	B-MethodName
-	O
clm	O
-	O
enfr	O
-	O
1024	O
A2	O
/	O
Bert	O
/	O
xlm	B-MethodName
-	O
mlm	B-DatasetName
-	O
enfr	O
-	O
1024	O
A2	O
/	O
Bert	O
/	O
flaubert	O
-	O
large	O
A2	O
/	O
Bert	O
/	O
xlm	B-MethodName
-	O
roberta	O
-	O
base	O
A1	O
/	O
Bert	O
/	O
flaubert	O
-	O
large	O
A1	O
/	O
Bert	O
/	O
flaubert	O
-	O
small	O
A1	O
/	O
Bert	O
/	O
openai	O
-	O
gpt	B-MethodName
A2	O
/	O
Bert	O
/	O
Cmbert	O
-	O
base	O
A2	O
/	O
Bert	O
/	O
flaubert	O
-	O
base	O
A2	O
/	O
Bert	O
/	O
flaubert	O
-	O
base	O
-	O
unc	O
A2	O
/	O
Bert	O
/	O
xlm	B-MethodName
-	O
roberta	O
-	O
large	O
A1	O
/	O
Bert	O
/	O
gpt2	O
A2	O
/	O
Bert	O
/	O
bert	O
-	O
base	O
-	O
mlg	O
-	O
unc	O
A2	O
/	O
Bert	O
/	O
gpt2	O
-	O
medium	O
A1	O
/	O
Bert	O
/	O
gpt2	O
-	O
large	O
A2	O
/	O
Bert	O
/	O
bert	O
-	O
base	O
-	O
mlg	O
A1	O
/	O
Bert	O
/	O
gpt2	O
-	O
medium	O

Albert	O
Einstein	O
is	O
a	O
German	O
-	O
born	O
theoretical	O
physicist	O
who	O
developed	O
the	O
theory	O
of	O
relativity	O
,	O
one	O
of	O
the	O
two	O
pillars	O
of	O
modern	O
physics	O
.	O
Given	O
the	O
specificity	O
of	O
QAS	O
which	O
extract	O
answers	O
from	O
structured	O
data	O
,	O
users	O
generally	O
receive	O
only	O
a	O
short	O
and	O
limited	O
answer	O
to	O
their	O
questions	O
as	O
illustrated	O
by	O
the	O
example	O
above	O
.	O
This	O
type	O
of	O
answer	O
representation	O
might	O
not	O
meet	O
the	O
user	O
expectations	O
.	O
Indeed	O
,	O
the	O
type	O
of	O
answer	O
given	O
by	O
the	O
first	O
system	O
can	O
be	O
perceived	O
as	O
too	O
brief	O
not	O
recalling	O
the	O
context	O
of	O
the	O
question	O
.	O
The	O
second	O
system	O
returns	O
a	O
passage	O
which	O
contains	O
information	O
that	O
are	O
out	O
of	O
the	O
question	O
's	O
scope	O
and	O
might	O
be	O
deemed	O
by	O
the	O
user	O
as	O
irrelevant	O
.	O
It	O
is	O
within	O
this	O
framework	O
that	O
we	O
propose	O
in	O
this	O
article	O
an	O
approach	O
which	O
allows	O
to	O
generate	O
a	O
concise	O
answer	O
in	O
natural	O
language	O
(	O
e.g.	O
The	O
thesis	O
superviser	O
of	O
Albert	O
Einstein	O
was	O
Alfred	O
Kleiner	O
)	O
that	O
shows	O
very	O
promising	O
results	O
tested	O
over	O
French	O
and	O
English	O
questions	O
.	O
This	O
approach	O
is	O
a	O
component	O
of	O
a	O
QAS	O
that	O
we	O
proposed	O
in	O
Rojas	O
Barahona	O
et	O
al	O
(	O
2019	O
)	O
and	O
that	O
we	O
will	O
briefly	O
present	O
in	O
this	O
article	O
.	O
In	O
what	O
follows	O
,	O
we	O
detail	O
in	O
section	O
3	O
the	O
approach	O
we	O
propose	O
for	O
answer	B-TaskName
generation	I-TaskName
in	O
Natural	O
Language	O
and	O
we	O
briefly	O
discuss	O
the	O
QAS	O
developed	O
.	O
We	O
present	O
in	O
section	O
4	O
the	O
experiments	O
that	O
we	O
have	O
conducted	O
to	O
evaluate	O
this	O
approach	O
.	O

The	O
huge	O
amount	O
of	O
information	O
available	O
nowadays	O
makes	O
the	O
task	O
of	O
retrieving	O
relevant	O
informa	O
-	O
tion	O
complex	O
and	O
time	O
consuming	O
.	O
This	O
complexity	O
has	O
prompted	O
the	O
development	O
of	O
QAS	O
which	O
help	O
spare	O
the	O
user	O
the	O
search	O
and	O
the	O
information	O
filtering	O
tasks	O
,	O
as	O
it	O
is	O
often	O
the	O
case	O
with	O
search	O
engines	O
,	O
and	O
directly	O
return	O
the	O
exact	O
answer	O
to	O
a	O
question	O
asked	O
in	O
natural	O
language	O
.	O
The	O
QAS	O
cover	O
mainly	O
three	O
tasks	O
:	O
question	O
analysis	O
,	O
information	B-TaskName
retrieval	I-TaskName
and	O
answer	O
extraction	O
(	O
Lopez	O
et	O
al	O
,	O
2011	O
)	O
.	O
These	O
tasks	O
have	O
been	O
tackled	O
in	O
different	O
ways	O
,	O
considering	O
the	O
knowledge	O
bases	O
used	O
,	O
the	O
types	O
of	O
questions	O
addressed	O
(	O
Iida	O
et	O
al	O
,	O
2019	O
;	O
Zayaraz	O
et	O
al	O
,	O
2015	O
;	O
Dwivedi	O
and	O
Singh	O
,	O
2013	O
;	O
Lopez	O
et	O
al	O
,	O
2011	O
)	O
and	O
the	O
way	O
in	O
which	O
the	O
answer	O
is	O
presented	O
.	O
In	O
this	O
article	O
,	O
we	O
particularly	O
focus	O
on	O
the	O
answer	B-TaskName
generation	I-TaskName
process	O
.	O
We	O
generally	O
notice	O
two	O
forms	O
of	O
representation	O
addressed	O
in	O
literature	O
.	O
The	O
answer	O
can	O
take	O
the	O
form	O
of	O
a	O
paragraph	O
selected	O
from	O
a	O
set	O
of	O
text	O
passages	O
retrieved	O
from	O
the	O
web	O
(	O
Asai	O
et	O
al	O
,	O
2018	O
;	O
Du	O
and	O
Cardie	O
,	O
2018	O
;	O
Wang	O
and	O
Jiang	O
,	O
2016	O
;	O
Wang	O
et	O
al	O
,	O
2017	O
;	O
Oh	O
et	O
al	O
,	O
2016	O
)	O
,	O
as	O
it	O
can	O
also	O
be	O
the	O
exact	O
answer	O
to	O
the	O
question	O
extracted	O
from	O
a	O
knowledge	O
base	O
(	O
Wu	O
et	O
al	O
,	O
2003	O
;	O
Bhaskar	O
et	O
al	O
,	O
2013	O
;	O
Le	O
et	O
al	O
,	O
2016	O
)	O
.	O
Despite	O
the	O
abundance	O
of	O
work	O
in	O
the	O
field	O
of	O
QAS	O
,	O
the	O
answers	O
generation	O
issue	O
has	O
received	O
little	O
attention	O
.	O
A	O
first	O
approach	O
indirectly	O
addressing	O
this	O
task	O
has	O
been	O
proposed	O
in	O
Brill	O
et	O
al	O
(	O
2001Brill	O
et	O
al	O
(	O
,	O
2002	O
.	O
Indeed	O
,	O
the	O
authors	O
aimed	O
at	O
diversifying	O
the	O
possible	O
answer	O
patterns	O
by	O
permuting	O
the	O
question	O
's	O
words	O
in	O
order	O
to	O
maximise	O
the	O
number	O
of	O
retrieved	O
documents	O
that	O
may	O
contain	O
the	O
answer	O
to	O
the	O
given	O
question	O
.	O
Another	O
answer	O
representation	O
approach	O
based	O
on	O
rephrasing	O
rules	O
has	O
also	O
been	O
proposed	O
in	O
Agichtein	O
and	O
Gravano	O
(	O
2000	O
)	O
;	O
Lawrence	O
and	O
Giles	O
(	O
1998	O
)	O
within	O
the	O
context	O
of	O
query	O
expansion	O
task	O
for	O
document	O
retrieval	O
and	O
not	O
purposely	O
for	O
the	O
question	O
-	O
answering	O
task	O
.	O
The	O
few	O
works	O
that	O
have	O
considered	O
this	O
task	O
within	O
the	O
QAS	O
framework	O
have	O
approached	O
it	O
from	O
a	O
text	O
summary	O
generation	O
perspective	O
(	O
Ishida	O
et	O
al	O
,	O
2018	O
;	O
Iida	O
et	O
al	O
,	O
2019	O
;	O
Rush	O
et	O
al	O
,	O
2015	O
;	O
Chopra	O
et	O
al	O
,	O
2016	O
;	O
Nallapati	O
et	O
al	O
,	O
2016	O
;	O
Miao	O
and	O
Blunsom	O
,	O
2016	O
;	O
See	O
et	O
al	O
,	O
2017	O
;	O
Oh	O
et	O
al	O
,	O
2016	O
;	O
Sharp	O
et	O
al	O
,	O
2016	O
;	O
.	O
These	O
works	O
consist	O
in	O
generating	O
a	O
summary	O
of	O
a	O
single	O
or	O
various	O
text	O
spans	O
that	O
contain	O
the	O
answer	O
to	O
a	O
question	O
.	O
Most	O
of	O
these	O
works	O
have	O
only	O
considered	O
causality	O
questions	O
like	O
the	O
ones	O
starting	O
with	O
"	O
why	O
"	O
and	O
whose	O
answers	O
are	O
para	O
-	O
graphs	O
.	O
To	O
make	O
these	O
answers	O
more	O
concise	O
,	O
the	O
extracted	O
paragraphs	O
are	O
summed	O
up	O
.	O
Other	O
approaches	O
(	O
Kruengkrai	O
et	O
al	O
,	O
2017	O
;	O
Girju	O
,	O
2003	O
;	O
Verberne	O
et	O
al	O
,	O
2011	O
;	O
Oh	O
et	O
al	O
,	O
2013	O
)	O
have	O
explored	O
this	O
task	O
as	O
a	O
classification	O
problem	O
that	O
consists	O
in	O
predicting	O
whether	O
a	O
text	O
passage	O
can	O
be	O
considered	O
as	O
an	O
answer	O
to	O
a	O
given	O
question	O
.	O
It	O
should	O
be	O
noted	O
that	O
these	O
approaches	O
only	O
intend	O
to	O
diversify	O
as	O
much	O
as	O
possible	O
the	O
answer	O
representation	O
patterns	O
to	O
a	O
given	O
question	O
in	O
order	O
to	O
increase	O
the	O
probability	O
of	O
extracting	O
the	O
correct	O
answer	O
from	O
the	O
Web	O
and	O
do	O
not	O
focus	O
on	O
the	O
answer	O
's	O
representation	O
itself	O
.	O
It	O
should	O
also	O
be	O
noted	O
that	O
these	O
approaches	O
are	O
only	O
applicable	O
for	O
QAS	O
which	O
extract	O
answers	O
as	O
a	O
text	O
snippet	O
and	O
can	O
not	O
be	O
applied	O
to	O
short	O
answers	O
usually	O
extracted	O
from	O
knowledge	O
bases	O
.	O
The	O
work	O
presented	O
in	O
Pal	O
et	O
al	O
(	O
2019	O
)	O
tried	O
to	O
tackle	O
this	O
issue	O
by	O
proposing	O
a	O
supervised	O
approach	O
that	O
was	O
trained	O
on	O
a	O
small	O
dataset	O
whose	O
questions	O
/	O
answers	O
pairs	O
were	O
extracted	O
from	O
machine	O
comprehension	O
datasets	O
and	O
augmented	O
manually	O
which	O
make	O
generalization	O
and	O
capturing	O
variation	O
very	O
limited	O
.	O
Our	O
answer	B-TaskName
generation	I-TaskName
approach	O
differs	O
from	O
these	O
works	O
as	O
it	O
is	O
unsupervised	O
,	O
can	O
be	O
adapted	O
to	O
any	O
type	O
of	O
factual	O
question	O
(	O
except	O
for	O
why	O
)	O
and	O
is	O
based	O
only	O
on	O
easily	O
accessible	O
and	O
unannotated	O
data	O
.	O
Indeed	O
,	O
we	O
build	O
upon	O
the	O
intuitive	O
hypothesis	O
that	O
a	O
concise	O
answer	O
and	O
easily	O
pronounced	O
by	O
an	O
intelligent	O
assistant	O
can	O
in	O
fact	O
consist	O
of	O
a	O
reformulation	O
of	O
the	O
question	O
asked	O
.	O
This	O
approach	O
is	O
a	O
part	O
of	O
a	O
QAS	O
that	O
we	O
have	O
developed	O
in	O
Rojas	O
Barahona	O
et	O
al	O
(	O
2019	O
)	O
that	O
extracts	O
the	O
answer	O
to	O
a	O
question	O
from	O
structured	O
data	O
.	O
In	O
what	O
follows	O
,	O
we	O
detail	O
in	O
section	O
3	O
the	O
approach	O
we	O
propose	O
for	O
answer	B-TaskName
generation	I-TaskName
in	O
Natural	O
Language	O
and	O
we	O
briefly	O
discuss	O
the	O
QAS	O
developed	O
.	O
We	O
present	O
in	O
section	O
4	O
the	O
experiments	O
that	O
we	O
have	O
conducted	O
to	O
evaluate	O
this	O
approach	O
.	O
and	O
we	O
conclude	O
in	O
section	O
5	O
with	O
the	O
limitations	O
noted	O
and	O
the	O
perspectives	O
considered	O
.	O

The	O
answer	B-TaskName
generation	I-TaskName
approach	O
proposed	O
is	O
a	O
component	O
of	O
a	O
system	O
which	O
was	O
developed	O
in	O
Rojas	O
Barahona	O
et	O
al	O
(	O
2019	O
)	O
and	O
which	O
consists	O
in	O
a	O
spoken	O
conversational	O
question	O
-	O
answering	O
system	O
which	O
analyses	O
and	O
translates	O
a	O
question	O
in	O
natural	O
language	O
(	O
French	O
or	O
English	O
)	O
in	O
a	O
formal	O
representation	O
that	O
is	O
transformed	O
into	O
a	O
Sparql	O
query	O
1	O
.	O
The	O
Sparql	O
query	O
helps	O
extracting	O
the	O
answer	O
to	O
the	O
given	O
question	O
from	O
an	O
RDF	O
knowledge	O
base	O
,	O
in	O
our	O
case	O
Wikidata	O
2	O
.	O
The	O
extracted	O
answer	O
takes	O
the	O
form	O
of	O
a	O
list	O
of	O
URIs	O
or	O
values	O
.	O
Although	O
the	O
QAS	O
that	O
we	O
have	O
developed	O
(	O
Rojas	O
Barahona	O
et	O
al	O
,	O
2019	O
)	O
is	O
able	O
to	O
find	O
the	O
correct	O
answer	O
to	O
a	O
question	O
,	O
we	O
have	O
noticed	O
that	O
its	O
short	O
representation	O
is	O
not	O
user	O
-	O
friendly	O
.	O
Therefore	O
,	O
we	O
propose	O
an	O
unsupervised	O
approach	O
which	O
integrates	O
the	O
use	O
of	O
Transformer	B-MethodName
models	O
such	O
as	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
and	O
GPT	B-MethodName
(	O
Radford	O
et	O
al	O
,	O
2018	O
)	O
.	O
The	O
choice	O
of	O
an	O
unsupervised	O
approach	O
arises	O
from	O
the	O
fact	O
that	O
there	O
is	O
no	O
available	O
training	O
dataset	O
associating	O
a	O
question	O
with	O
an	O
exhaustive	O
and	O
concise	O
answer	O
at	O
the	O
same	O
time	O
.	O
such	O
dataset	O
could	O
have	O
helped	O
use	O
an	O
End	O
-	O
to	O
-	O
End	O
learning	O
neural	O
architecture	O
that	O
can	O
generate	O
an	O
elaborated	O
answer	O
to	O
a	O
question	O
.	O
This	O
approach	O
builds	O
upon	O
the	O
fact	O
that	O
we	O
have	O
already	O
extracted	O
the	O
short	O
answer	O
to	O
a	O
given	O
question	O
and	O
assumes	O
that	O
a	O
user	O
-	O
friendly	O
answer	O
can	O
consist	O
in	O
rephrasing	O
the	O
question	O
words	O
along	O
with	O
the	O
short	O
answer	O
.	O
This	O
approach	O
is	O
composed	O
of	O
two	O
fundamental	O
phases	O
:	O
The	O
dependency	O
analysis	O
of	O
the	O
input	O
question	O
and	O
the	O
answer	B-TaskName
generation	I-TaskName
using	O
Transformer	B-MethodName
models	O
.	O

During	O
this	O
phase	O
,	O
we	O
first	O
carry	O
out	O
a	O
first	O
test	O
of	O
the	O
set	O
Q	O
to	O
check	O
whether	O
the	O
text	O
fragment	O
which	O
contains	O
a	O
question	O
marker	O
(	O
exp	O
:	O
what	O
,	O
when	O
,	O
who	O
etc	O
.	O
)	O
represents	O
the	O
subject	O
nsubj	O
in	O
the	O
analysed	O
question	O
.	O
If	O
so	O
,	O
we	O
simply	O
replace	O
that	O
text	O
fragment	O
with	O
the	O
answer	O
we	O
identified	O
earlier	O
.	O
Let	O
us	O
take	O
the	O
previous	O
example	O
What	O
is	O
the	O
political	O
party	O
of	O
the	O
mayor	O
of	O
Paris	O
?	O
,	O
the	O
system	O
automatically	O
detects	O
that	O
the	O
text	O
fragment	O
containing	O
the	O
question	O
marker	O
What	O
represents	O
the	O
subject	O
and	O
will	O
therefore	O
be	O
replaced	O
directly	O
by	O
the	O
exact	O
answer	O
The	O
Socialist	O
Party	O
.	O
Therefore	O
,	O
the	O
concise	O
answer	O
generated	O
will	O
be	O
The	O
Socialist	O
Party	O
is	O
the	O
political	O
party	O
of	O
the	O
mayor	O
of	O
Paris	O
.	O
Otherwise	O
,	O
we	O
remove	O
the	O
text	O
fragment	O
containing	O
the	O
question	O
marker	O
that	O
we	O
detected	O
and	O
we	O
add	O
the	O
short	O
answer	O
R	O
to	O
Q	O
:	O
Q	O
=	O
{	O
c	O
1	O
,	O
c	O
2	O
,	O
.	O
.	O
.	O
,	O
c	O
n−1	O
,	O
R	O
}	O
Using	O
the	O
text	O
fragments	O
set	O
Q	O
,	O
we	O
proceed	O
with	O
a	O
permutation	O
based	O
generation	O
of	O
all	O
possible	O
answer	O
structures	O
that	O
can	O
form	O
the	O
sentence	O
answering	O
the	O
question	O
asked	O
:	O
S	O
=	O
{	O
s	O
1	O
(	O
R	O
,	O
c	O
1	O
,	O
c	O
2	O
,	O
.	O
.	O
.	O
,	O
c	O
n−1	O
)	O
,	O
s	O
2	O
(	O
c	O
1	O
,	O
R	O
,	O
c	O
2	O
,	O
.	O
.	O
.	O
,	O
c	O
n−1	O
)	O
,	O
.	O
.	O
.	O
,	O
s	O
m	O
(	O
c	O
1	O
,	O
c	O
2	O
,	O
.	O
.	O
.	O
,	O
c	O
n−1	O
,	O
R	O
)	O
}	O
These	O
structures	O
will	O
be	O
evaluated	O
by	O
a	O
Language	O
Model	O
(	O
LM	O
)	O
based	O
on	O
Transformer	B-MethodName
models	O
which	O
will	O
extract	O
the	O
most	O
probable	O
sequence	O
of	O
text	O
fragments	O
that	O
can	O
account	O
for	O
the	O
answer	O
to	O
be	O
sent	O
to	O
the	O
user	O
:	O
structure	O
*	O
=	O
s	O
S	O
;	O
p	O
(	O
s	O
)	O
=	O
argmax	O
s	O
i	O
S	O
p	O
(	O
s	O
i	O
)	O
Once	O
the	O
best	O
structure	O
is	O
identified	O
,	O
we	O
initiate	O
the	O
generation	O
process	O
of	O
possible	O
missing	O
words	O
.	O
Indeed	O
,	O
we	O
suppose	O
that	O
there	O
could	O
be	O
some	O
terms	O
which	O
do	O
not	O
necessarily	O
appear	O
in	O
the	O
question	O
or	O
in	O
the	O
short	O
answer	O
but	O
which	O
are	O
,	O
on	O
the	O
other	O
hand	O
,	O
necessary	O
to	O
the	O
generation	O
of	O
a	O
correct	O
grammatical	O
structure	O
of	O
the	O
final	O
answer	O
.	O
This	O
process	O
requires	O
that	O
we	O
set	O
two	O
parameters	O
,	O
the	O
number	O
of	O
possible	O
missing	O
words	O
and	O
their	O
positions	O
within	O
the	O
selected	O
structure	O
.	O
In	O
this	O
paper	O
,	O
we	O
experiment	O
the	O
assumption	O
that	O
one	O
word	O
could	O
be	O
missing	O
and	O
that	O
it	O
is	O
located	O
before	O
the	O
short	O
answer	O
within	O
the	O
identified	O
structure	O
,	O
as	O
it	O
could	O
be	O
the	O
case	O
for	O
a	O
missing	O
article	O
(	O
the	O
,	O
a	O
,	O
etc	O
.	O
)	O
or	O
a	O
preposition	O
(	O
in	O
,	O
at	O
,	O
etc	O
.	O
)	O
for	O
example	O
.	O
Therefore	O
,	O
to	O
predict	O
this	O
missing	O
word	O
,	O
we	O
use	O
BERT	B-MethodName
as	O
the	O
generation	O
model	O
(	O
GM	O
)	O
for	O
its	O
ability	O
to	O
capture	O
bidirectionally	O
the	O
context	O
of	O
a	O
given	O
word	O
within	O
a	O
sentence	O
.	O
In	O
case	O
when	O
BERT	B-MethodName
returns	O
a	O
non	O
-	O
alphabetic	O
character	O
sequence	O
,	O
we	O
assume	O
that	O
the	O
optimal	O
structure	O
,	O
as	O
predicted	O
by	O
the	O
LM	O
,	O
does	O
not	O
need	O
to	O
be	O
completed	O
by	O
an	O
additional	O
word	O
.	O
The	O
following	O
example	O
illustrates	O
the	O
different	O
steps	O
of	O
the	O
proposed	O
approach	O
:	O
Question	O
:	O
When	O
did	O
princess	O
Diana	O
die	O
?	O
1	O
.	O
Question	O
parsing	O
and	O
answer	O
extraction	O
using	O
the	O
system	O
proposed	O
in	O
Rojas	O
Barahona	O
et	O
al	O
(	O
2019	O
)	O
:	O
short	O
answer	O
=	O
{	O
August	O
31	O
,	O
1997	O
}	O

Long	O
-	O
form	O
answers	O
,	O
consisting	O
of	O
multiple	O
sentences	O
,	O
can	O
provide	O
nuanced	O
and	O
comprehensive	O
answers	O
to	O
a	O
broader	O
set	O
of	O
questions	O
.	O
To	O
better	O
understand	O
this	O
complex	O
and	O
understudied	O
task	O
,	O
we	O
study	O
the	O
functional	O
structure	O
of	O
long	O
-	O
form	O
answers	O
collected	O
from	O
three	O
datasets	O
,	O
ELI5	B-DatasetName
(	O
Fan	O
et	O
al	O
,	O
2019	O
)	O
,	O
We	O
-	O
bGPT	O
(	O
Nakano	O
et	O
al	O
,	O
2021	O
)	O
and	O
Natural	B-DatasetName
Questions	I-DatasetName
(	O
Kwiatkowski	O
et	O
al	O
,	O
2019	O
)	O
.	O
Our	O
main	O
goal	O
is	O
to	O
understand	O
how	O
humans	O
organize	O
information	O
to	O
craft	O
complex	O
answers	O
.	O
We	O
develop	O
an	O
ontology	B-MethodName
of	O
six	O
sentence	O
-	O
level	O
functional	O
roles	O
for	O
long	O
-	O
form	O
answers	O
,	O
and	O
annotate	O
3.9k	O
sentences	O
in	O
640	O
answer	O
paragraphs	O
.	O
Different	O
answer	O
collection	O
methods	O
manifest	O
in	O
different	O
discourse	O
structures	O
.	O
We	O
further	O
analyze	O
model	O
-	O
generated	O
answers	O
-	O
finding	O
that	O
annotators	O
agree	O
less	O
with	O
each	O
other	O
when	O
annotating	O
model	O
-	O
generated	O
answers	O
compared	O
to	O
annotating	O
human	O
-	O
written	O
answers	O
.	O
Our	O
annotated	O
data	O
enables	O
training	O
a	O
strong	O
classifier	O
that	O
can	O
be	O
used	O
for	O
automatic	O
analysis	O
.	O
We	O
hope	O
our	O
work	O
can	O
inspire	O
future	O
research	O
on	O
discourselevel	O
modeling	O
and	O
evaluation	O
of	O
long	O
-	O
form	O
QA	O
systems	O
.	O
1	O

While	O
many	O
information	O
seeking	O
questions	O
can	O
be	O
answered	O
by	O
a	O
short	O
text	O
span	O
,	O
requiring	O
a	O
short	O
span	O
answer	O
significantly	O
limits	O
the	O
types	O
of	O
questions	O
that	O
can	O
be	O
addressed	O
as	O
well	O
as	O
the	O
extent	O
of	O
information	O
that	O
can	O
be	O
conveyed	O
.	O
Recent	O
work	O
(	O
Fan	O
et	O
al	O
,	O
2019	O
;	O
Krishna	O
et	O
al	O
,	O
2021	O
;	O
Nakano	O
et	O
al	O
,	O
2021	O
)	O
explored	O
long	O
-	O
form	O
answers	O
,	O
where	O
answers	O
are	O
free	O
-	O
form	O
texts	O
consisting	O
of	O
multiple	O
sentences	O
.	O
Such	O
long	O
-	O
form	O
answers	O
provide	O
flexible	O
space	O
where	O
the	O
answerer	O
can	O
provide	O
a	O
nuanced	O
answer	O
,	O
incorporating	O
their	O
confidence	O
and	O
sources	O
of	O
their	O
knowledge	O
.	O
Thus	O
the	O
answer	O
sentences	O
form	O
a	O
discourse	O
where	O
the	O
answerers	O
provide	O
information	O
,	O
hedge	O
,	O
explain	O
,	O
provide	O
examples	O
,	O
point	O
to	O
other	O
sources	O
,	O
and	O
more	O
;	O
these	O
elements	O
need	O
to	O
be	O
structured	O
and	O
organized	O
coherently	O
.	O
We	O
take	O
a	O
linguistically	O
informed	O
approach	O
to	O
understand	O
the	O
structure	O
of	O
long	O
-	O
form	O
answers	O
,	O
designing	O
six	O
communicative	O
functions	O
of	O
sentences	O
in	O
long	O
-	O
form	O
answers	O
(	O
which	O
we	O
call	O
roles	O
)	O
.	O
2	O
Our	O
framework	O
combines	O
functional	O
structures	O
with	O
the	O
notion	O
of	O
information	O
salience	O
by	O
designating	O
a	O
role	O
for	O
sentences	O
that	O
convey	O
the	O
main	O
message	O
of	O
an	O
answer	O
.	O
Other	O
roles	O
include	O
signaling	O
the	O
organization	O
of	O
the	O
answer	O
,	O
directly	O
answering	O
the	O
question	O
,	O
giving	O
an	O
example	O
,	O
providing	O
background	O
information	O
,	O
and	O
so	O
on	O
.	O
About	O
a	O
half	O
of	O
the	O
sentences	O
in	O
long	O
-	O
form	O
answers	O
we	O
study	O
serve	O
roles	O
other	O
than	O
providing	O
an	O
answer	O
to	O
the	O
question	O
.	O
We	O
collect	O
discourse	O
annotations	O
on	O
three	O
long	O
-	O
form	O
question	B-TaskName
answering	I-TaskName
(	O
LFQA	O
)	O
datasets	O
,	O
ELI5	B-DatasetName
(	O
Fan	O
et	O
al	O
,	O
2019	O
)	O
,	O
WebGPT	O
(	O
Nakano	O
et	O
al	O
,	O
2021	O
)	O
and	O
Natural	B-DatasetName
Questions	I-DatasetName
(	O
NQ	B-DatasetName
)	O
(	O
Kwiatkowski	O
et	O
al	O
,	O
2019	O
)	O
.	O
Figure	O
1	O
contains	O
an	O
example	O
annotation	O
on	O
each	O
dataset	O
.	O
While	O
all	O
three	O
contain	O
paragraph	O
-	O
length	O
answers	O
needed	O
for	O
complex	O
queries	O
,	O
they	O
are	O
collected	O
in	O
distinct	O
mannersanswers	O
in	O
ELI5	B-DatasetName
are	O
written	O
by	O
Reddit	B-DatasetName
users	O
;	O
answers	O
in	O
WebGPT	O
are	O
written	O
by	O
annotators	O
who	O
searched	O
documents	O
on	O
a	O
web	O
interface	O
and	O
heavily	O
quoted	O
those	O
documents	O
to	O
form	O
an	O
answer	O
,	O
and	O
answers	O
in	O
NQ	B-DatasetName
are	O
pre	O
-	O
existing	O
paragraphs	O
from	O
Wikipedia	O
corpus	O
.	O
We	O
collect	O
three	O
-	O
way	O
annotations	O
for	O
3.9k	O
sentences	O
(	O
∼700	O
question	O
-	O
answer	O
pairs	O
across	O
three	O
datasets	O
)	O
.	O
We	O
also	O
annotate	O
a	O
small	O
number	O
of	O
model	O
-	O
generated	O
answers	O
from	O
a	O
recent	O
long	O
-	O
form	O
question	B-TaskName
answering	I-TaskName
(	O
LFQA	O
)	O
system	O
(	O
Krishna	O
et	O
al	O
,	O
2021	O
)	O
and	O
provide	O
rich	O
analysis	O
of	O
their	O
discourse	O
structure	O
.	O
In	O
all	O
three	O
datasets	O
,	O
we	O
observe	O
appearance	O
of	O
most	O
proposed	O
functional	O
roles	O
,	O
but	O
with	O
different	O
proportions	O
.	O
Answers	O
in	O
ELI5	B-DatasetName
contains	O
more	O
examples	O
and	O
elaborations	O
,	O
while	O
answers	O
extracted	O
WebGPT	O
:	O
How	O
much	O
money	O
is	O
needed	O
in	O
order	O
to	O
not	O
have	O
to	O
work	O
for	O
the	O
rest	O
of	O
your	O
life	O
?	O
I	O
worked	O
on	O
a	O
window	O
-	O
washing	O
robot	O
that	O
cleaned	O
acres	O
of	O
rooftops	O
over	O
a	O
huge	O
commercial	O
greenhouse	O
.	O
Worked	O
great	O
,	O
except	O
when	O
it	O
did	O
n't	O
,	O
and	O
would	O
either	O
break	O
down	O
completely	O
or	O
just	O
get	O
lost	O
and	O
start	O
climbing	O
the	O
wrong	O
parts	O
of	O
the	O
structure	O
.	O
Then	O
repair	O
techs	O
and	O
manual	O
window	O
washers	O
still	O
have	O
to	O
be	O
employed	O
.	O
I	O
think	O
this	O
ends	O
up	O
being	O
a	O
cost	O
/	O
benefit	O
problem	O
where	O
the	O
reliability	O
of	O
our	O
robots	O
and	O
price	O
of	O
implementation	O
is	O
n't	O
quite	O
at	O
the	O
point	O
where	O
it	O
m	O
a	O
k	O
e	O
s	O
t	O
h	O
i	O
s	O
c	O
o	O
m	O
m	O
e	O
r	O
c	O
i	O
a	O
l	O
l	O
y	O
v	O
i	O
a	O
b	O
l	O
e	O
f	O
o	O
r	O
skyscrapers	O
.	O
For	O
what	O
it	O
's	O
worth	O
,	O
I	O
think	O
the	O
Twin	O
Towers	O
actually	O
used	O
a	O
washer	O
robot	O
on	O
the	O
upper	O
floors	O
to	O
limited	O
success	O
.	O
To	O
determine	O
how	O
much	O
money	O
you	O
need	O
to	O
never	O
have	O
to	O
work	O
again	O
for	O
the	O
rest	O
of	O
your	O
life	O
,	O
some	O
calculation	O
is	O
needed	O
to	O
arrive	O
at	O
a	O
dollar	O
number	O
tailored	O
to	O
you	O
[	O
2	O
]	O
.	O
You	O
need	O
to	O
consider	O
the	O
amount	O
you	O
spend	O
yearly	O
,	O
the	O
effect	O
of	O
inflation	O
on	O
your	O
savings	O
,	O
and	O
the	O
income	O
you	O
need	O
from	O
an	O
investment	O
portfolio	O
to	O
keep	O
ahead	O
of	O
inflation	O
.	O
[	O
1	O
]	O
[	O
3	O
]	O
A	O
reliable	O
savings	O
range	O
is	O
your	O
current	O
yearly	O
spending	O
multiplied	O
by	O
28	O
to	O
36	O
,	O
with	O
more	O
security	O
and	O
comfort	O
the	O
higher	O
the	O
number	O
is	O
.	O
from	O
Wikipedia	O
passages	O
(	O
NQ	B-DatasetName
)	O
contain	O
more	O
auxiliary	O
information	O
.	O
Analyzing	O
a	O
subset	O
of	O
ELI5	B-DatasetName
and	O
WebGPT	O
,	O
we	O
also	O
identify	O
a	O
big	O
gap	O
in	O
lexical	O
overlap	O
between	O
long	O
-	O
form	O
answer	O
and	O
evidence	O
passages	O
across	O
all	O
functional	O
roles	O
.	O
Lastly	O
,	O
we	O
found	O
that	O
human	O
agreement	O
of	O
the	O
discourse	O
roles	O
of	O
model	O
-	O
generated	O
answers	O
are	O
much	O
lower	O
than	O
human	O
-	O
written	O
ones	O
,	O
reflecting	O
the	O
difficulty	O
for	O
humans	O
to	O
process	O
model	O
-	O
generated	O
answers	O
.	O
With	O
the	O
data	O
collected	O
,	O
we	O
present	O
a	O
competitive	O
role	O
classifier	O
,	O
which	O
performs	O
on	O
par	O
with	O
human	O
when	O
trained	O
with	O
our	O
annotated	O
data	O
and	O
can	O
be	O
used	O
for	O
automatic	O
discourse	O
analysis	O
.	O
We	O
further	O
envision	O
using	O
functional	O
roles	O
for	O
controllable	O
long	O
-	O
form	O
generations	O
,	O
concise	O
answer	B-TaskName
generation	I-TaskName
,	O
and	O
improved	O
evaluation	O
metrics	O
for	O
LFQA	O
.	O

We	O
study	O
the	O
discourse	O
structure	O
of	O
long	O
-	O
form	O
answers	O
based	O
on	O
functional	O
roles	O
of	O
sentences	O
in	O
the	O
paragraph	O
.	O
Functional	O
structures	O
characterize	O
the	O
communicative	O
role	O
a	O
linguistic	O
unit	O
plays	O
;	O
as	O
such	O
,	O
they	O
vary	O
across	O
genres	O
as	O
the	O
goals	O
of	O
communication	O
also	O
vary	O
.	O
In	O
scientific	O
or	O
technical	O
articles	O
,	O
these	O
roles	O
can	O
be	O
background	O
,	O
method	O
,	O
findings	O
(	O
Kircz	O
,	O
1991	O
;	O
Liddy	O
,	O
1991	O
;	O
Mizuta	O
et	O
al	O
,	O
2006	O
)	O
,	O
while	O
in	O
news	O
,	O
they	O
can	O
be	O
main	O
event	O
or	O
anecdotes	O
(	O
Van	O
Dijk	O
,	O
2013	O
;	O
Choubey	O
et	O
al	O
,	O
2020	O
)	O
.	O
These	O
structures	O
are	O
related	O
to	O
,	O
though	O
distinct	O
from	O
,	O
coherence	O
discourse	O
structures	O
(	O
Hobbs	O
,	O
1985	O
)	O
.	O
The	O
latter	O
characterizes	O
how	O
each	O
unit	O
(	O
e.g.	O
,	O
adjacent	O
clauses	O
or	O
sentences	O
)	O
relates	O
to	O
others	O
through	O
semantic	O
relations	O
such	O
as	O
temporal	O
,	O
causal	O
,	O
etc	O
.	O
;	O
such	O
structures	O
can	O
be	O
trees	O
that	O
hierarchically	O
relate	O
adjacent	O
units	O
(	O
Mann	O
and	O
Thompson	O
,	O
1988	O
)	O
or	O
graphs	O
(	O
Lascarides	O
and	O
Asher	O
,	O
2008	O
)	O
.	O
In	O
contrast	O
,	O
functional	O
roles	O
describe	O
how	O
information	O
is	O
organized	O
to	O
serve	O
the	O
communication	O
goal	O
,	O
in	O
our	O
case	O
,	O
providing	O
the	O
answer	O
.	O
We	O
developed	O
our	O
ontology	B-MethodName
by	O
examining	O
longform	O
answers	O
in	O
online	O
community	O
forums	O
(	O
subreddit	O
Explain	O
Like	O
I	O
'm	O
Five	O
(	O
ELI5	O
)	B-DatasetName
)	O
and	O
Wikipedia	O
passages	O
,	O
hence	O
answers	O
derived	O
from	O
different	O
domains	O
(	O
e.g.	O
,	O
textbooks	O
)	O
can	O
contain	O
roles	O
beyond	O
our	O
ontology	O
.	B-MethodName
We	O
describe	O
our	O
six	O
sentence	O
-	O
level	O
discourse	O
roles	O
for	O
long	O
-	O
form	O
answers	O
here	O
:	O
Answer	O
-	O
Summary	O
(	O
Sum	O
)	O
,	O
Answer	O
(	O
Ans	O
)	O
.	O
An	O
answer	O
sentence	O
directly	O
addresses	O
the	O
question	O
.	O
Here	O
we	O
distinguish	O
between	O
the	O
the	O
main	O
content	O
of	O
the	O
answer	O
(	O
henceforth	O
answer	O
summary	O
)	O
vs.	O
sentences	O
which	O
explain	O
or	O
elaborate	O
on	O
the	O
summary	O
.	O
The	O
summaries	O
play	O
a	O
more	O
salient	O
role	O
than	O
non	O
-	O
summary	O
answer	O
sentences	O
,	O
and	O
can	O
often	O
suffice	O
by	O
themselves	O
as	O
the	O
answer	O
to	O
the	O
question	O
.	O
This	O
is	O
akin	O
to	O
argumentation	O
structure	O
that	O
hierarchically	O
arranges	O
main	O
claims	O
and	O
supporting	O
arguments	O
(	O
Peldszus	O
and	O
Stede	O
,	O
2013	O
)	O
,	O
and	O
news	O
structure	O
that	O
differentiates	O
between	O
main	O
vs.	O
supporting	O
events	O
(	O
Van	O
Dijk	O
,	O
2013	O
)	O
.	O
Organizational	O
sentences	O
(	O
Org	O
.	O
)	O
Rather	O
than	O
conveying	O
information	O
of	O
the	O
answer	O
,	O
the	O
major	O
role	O
of	O
an	O
organizational	O
sentence	O
is	O
to	O
inform	O
the	O
reader	O
how	O
the	O
answer	O
will	O
be	O
structured	O
.	O
We	O
found	O
two	O
main	O
types	O
of	O
such	O
sentences	O
;	O
the	O
first	O
signals	O
an	O
upcoming	O
set	O
of	O
items	O
of	O
parallel	O
importance	O
:	O
[	O
A	O
]	O
:	O
There	O
are	O
a	O
few	O
reasons	O
candidates	O
with	O
"	O
no	O
chance	O
"	O
to	O
win	O
keep	O
running	O
.	O
1	O
)	O
They	O
enjoy	O
campaigning	O
[	O
...	O
]	O
The	O
other	O
type	O
indicates	O
that	O
part	O
of	O
the	O
answer	O
is	O
upcoming	O
amidst	O
an	O
established	O
flow	O
;	O
in	O
the	O
example	O
below	O
,	O
the	O
answerer	O
used	O
a	O
hypophora	O
:	O
Examples	O
(	O
Ex	O
.	O
)	O
Often	O
people	O
provide	O
examples	O
in	O
answers	O
;	O
these	O
are	O
linguistically	O
distinct	O
from	O
other	O
answer	O
sentences	O
in	O
the	O
sense	O
that	O
they	O
are	O
more	O
specific	O
towards	O
a	O
particular	O
entity	O
,	O
concept	O
,	O
or	O
situation	O
.	O
This	O
pattern	O
of	O
language	O
specificity	O
can	O
also	O
be	O
found	O
in	O
example	O
-	O
related	O
discourse	O
relations	O
(	O
Louis	O
and	O
Nenkova	O
,	O
2011	O
;	O
Li	O
and	O
Nenkova	O
,	O
2015	O
)	O
,	O
or	O
through	O
entity	O
instantiation	O
(	O
MacKinlay	O
and	O
Markert	O
,	O
2011	O
)	O
:	O
[	O
Q	O
]	O
:	O
What	O
is	O
it	O
about	O
electricity	O
that	O
kills	O
you	O
?	O
[	O
A	O
]	O
:	O
[	O
...	O
]	O
For	O
example	O
,	O
static	O
electricity	O
consists	O
of	O
tens	O
of	O
thousands	O
of	O
volts	O
,	O
but	O
basically	O
no	O
amps	O
.	O
[	O
...	O
]	O
We	O
found	O
that	O
examples	O
in	O
human	O
answers	O
are	O
often	O
not	O
signaled	O
explicitly	O
,	O
and	O
often	O
contain	O
hypothetical	O
situations	O
:	O
[	O
Q	O
]	O
:	O
Were	O
major	O
news	O
outlets	O
established	O
with	O
political	O
bias	O
or	O
was	O
it	O
formed	O
over	O
time	O
?	O
[	O
A	O
]	O
:	O
[	O
...	O
]	O
This	O
is	O
impossible	O
due	O
to	O
the	O
problem	O
of	O
"	O
anchoring	O
.	O
"	O
Consider	O
a	O
world	O
where	O
people	O
on	O
the	O
right	O
want	O
the	O
tax	O
rate	O
to	O
be	O
1	O
%	O
lower	O
and	O
people	O
on	O
the	O
left	O
want	O
the	O
tax	O
rate	O
to	O
be	O
1	O
%	O
higher	O
[	O
...	O
]	O
Auxiliary	O
information	O
(	O
Aux	O
.	O
)	O
These	O
sentences	O
provide	O
information	O
that	O
are	O
related	O
to	O
what	O
is	O
discussed	O
in	O
the	O
answer	O
,	O
but	O
not	O
asked	O
in	O
the	O
question	O
.	O
It	O
could	O
be	O
background	O
knowledge	O
that	O
the	O
answerer	O
deemed	O
necessary	O
or	O
helpful	O
,	O
e.g.	O
,	O
or	O
related	O
content	O
that	O
extends	O
the	O
question	O
,	O
e.g.	O
,	O
[	O
Q	O
]	O
:	O
what	O
is	O
the	O
difference	O
between	O
mandi	O
and	O
kabsa	O
?	O
[	O
A	O
]	O
:	O
[	O
...	O
]	O
A	O
popular	O
way	O
of	O
preparing	O
meat	O
is	O
called	O
mandi	O
.	O
[	O
...	O
]	O
Another	O
way	O
of	O
preparing	O
and	O
serving	O
meat	O
for	O
kabsa	O
is	O
mathbi	O
,	O
where	O
seasoned	O
meat	O
is	O
grilled	O
on	O
flat	O
stones	O
that	O
are	O
placed	O
on	O
top	O
of	O
burning	O
embers	O
.	O
Notably	O
,	O
the	O
removal	O
of	O
auxiliary	O
information	O
would	O
still	O
leave	O
the	O
answer	O
itself	O
intact	O
.	O

We	O
observe	O
various	O
roles	O
that	O
,	O
although	O
less	O
frequent	O
,	O
show	O
up	O
consistently	O
in	O
human	O
answers	O
.	O
We	O
group	O
them	O
into	O
a	O
miscellaneous	O
role	O
and	O
list	O
them	O
below	O
.	O
(	O
a	O
)	O
Some	O
sentences	O
specify	O
the	O
limitation	O
of	O
the	O
answer	O
by	O
narrowing	O
down	O
the	O
scope	O
of	O
the	O
answer	O
to	O
an	O
open	O
-	O
ended	O
question	O
.	O
[	O
Q	O
]	O
:	O
Why	O
are	O
there	O
such	O
drastic	O
differences	O
in	O
salaries	O
between	O
different	O
countries	O
?	O
(	O
b	O
)	O
Some	O
sentences	O
state	O
where	O
the	O
answer	O
came	O
from	O
and	O
thus	O
put	O
the	O
answer	O
into	O
context	O
.	O
[	O
Q	O
]	O
:	O
Why	O
Does	O
a	O
thermostat	O
require	O
the	O
user	O
to	O
switch	O
between	O
heat	O
and	O
cool	O
modes	O
,	O
as	O
opposed	O
to	O
just	O
setting	O
the	O
desired	O
temperature	O
?	O
[	O
A	O
]	O
:	O
The	O
person	O
who	O
installed	O
my	O
heat	O
pump	O
(	O
which	O
has	O
all	O
three	O
modes	O
)	O
explained	O
this	O
to	O
me	O
.	O
[	O
...	O
]	O
(	O
c	O
)	O
Some	O
sentences	O
point	O
to	O
other	O
resources	O
that	O
might	O
contain	O
the	O
answers	O
.	O
As	O
our	O
ontology	B-MethodName
does	O
not	O
provide	O
an	O
exhaustive	O
list	O
of	O
the	O
functional	O
roles	O
,	O
we	O
instructed	O
our	O
annotators	O
to	O
annotate	O
other	O
roles	O
not	O
covered	O
by	O
our	O
ontology	B-MethodName
as	O
Miscellaneous	B-TaskName
as	O
well	O
.	O

We	O
randomly	O
sample	O
examples	O
from	O
three	O
LFQA	O
datasets	O
and	O
filter	O
answers	O
with	O
more	O
than	O
15	O
sentences	O
and	O
those	O
with	O
less	O
than	O
3	O
sentences	O
.	O
3	O
We	O
briefly	O
describe	O
each	O
dataset	O
below	O
.	O
4	O
ELI5	B-DatasetName
/	O
ELI5	B-DatasetName
-	O
model	O
ELI5	B-DatasetName
consists	O
of	O
QA	O
pairs	O
where	O
the	O
questions	O
and	O
answers	O
are	O
retrieved	O
from	O
the	O
subreddit	O
r	O
/	O
explainlikeimfive	O
.	O
The	O
answers	O
in	O
ELI5	B-DatasetName
are	O
of	O
varying	O
quality	O
and	O
style	O
.	O
While	O
the	O
original	O
dataset	O
consists	O
of	O
(	O
question	O
,	O
answer	O
)	O
pairs	O
,	O
recent	O
benchmark	O
(	O
Petroni	O
et	O
al	O
,	O
2021	O
)	O
annotated	O
a	O
subset	O
of	O
examples	O
with	O
relevant	O
Wikipedia	O
paragraphs	O
,	O
which	O
we	O
used	O
for	O
analysis	O
in	O
Section	O
4	O
.	O
In	O
addition	O
to	O
answers	O
in	O
the	O
original	O
datasets	O
,	O
we	O
annotate	O
a	O
small	O
number	O
of	O
model	O
-	O
generated	O
answers	O
from	O
Krishna	O
et	O
al	O
(	O
2021	O
)	O
(	O
we	O
refer	O
this	O
set	O
as	O
ELI5	B-DatasetName
-	O
model	O
)	O
,	O
a	O
state	O
-	O
ofthe	O
art	O
LFQA	O
system	O
on	O
ELI5	B-DatasetName
.	O
WebGPT	O
Nakano	O
et	O
al	O
(	O
2021	O
)	O
presented	O
a	O
new	O
LFQA	O
dataset	O
and	O
model	O
;	O
with	O
the	O
goal	O
of	O
building	O
a	O
model	O
that	O
can	O
search	O
and	O
navigate	O
the	O
web	O
to	O
compose	O
a	O
long	O
-	O
form	O
answer	O
.	O
While	O
they	O
reuse	O
questions	O
from	O
ELI5	B-DatasetName
,	O
they	O
newly	O
collect	O
answers	O
from	O
trained	O
human	O
annotators	O
who	O
were	O
instructed	O
to	O
first	O
search	O
for	O
related	O
documents	O
using	O
a	O
search	O
engine	O
and	O
then	O
construct	O
the	O
answers	O
with	O
reference	O
to	O
those	O
documents	O
.	O
The	O
collected	O
data	O
(	O
denoted	O
as	O
"	O
human	O
demonstration	O
"	O
consisting	O
of	O
question	O
,	O
answer	O
,	O
a	O
set	O
of	O
evidence	O
documents	O
,	O
and	O
mapping	O
from	O
the	O
answer	O
to	O
the	O
evidence	O
document	O
)	O
are	O
used	O
to	O
finetune	O
GPT	B-MethodName
-	O
3	O
(	O
Brown	O
et	O
al	O
,	O
2020	O
)	O
to	O
generate	O
long	O
-	O
form	O
answers	O
.	O
Natural	B-DatasetName
Questions	I-DatasetName
(	O
NQ	B-DatasetName
)	O
NQ	B-DatasetName
contains	O
questions	O
from	O
Google	B-DatasetName
search	O
queries	O
,	O
which	O
is	O
paired	O
with	O
a	O
relevant	O
Wikipedia	O
article	O
and	O
an	O
answer	O
in	O
the	O
article	O
if	O
the	O
article	O
answers	O
the	O
question	O
.	O
They	O
annotate	O
paragraph	O
-	O
level	O
answer	O
as	O
well	O
as	O
short	O
span	O
answer	O
inside	O
the	O
paragraph	O
answer	O
if	O
it	O
exists	O
.	O
In	O
open	O
retrieval	O
QA	O
,	O
researchers	O
filtered	O
questions	O
with	O
paragraph	O
level	O
answers	O
for	O
its	O
3	O
We	O
used	O
Stanza	O
(	O
Qi	O
et	O
al	O
,	O
2020	O
)	O
to	O
split	O
long	O
-	O
form	O
answers	O
into	O
sentences	O
.	O
This	O
process	O
removes	O
42	O
%	O
,	O
28	O
%	O
and	O
34	O
%	O
from	O
ELI5	B-DatasetName
,	O
WebGPT	O
and	O
NQ	B-DatasetName
respectively	O
.	O
4	O
Our	O
data	O
is	O
sourced	O
from	O
the	O
validation	O
split	O
of	O
ELI5	B-DatasetName
from	O
the	O
KILT	B-DatasetName
(	O
Petroni	O
et	O
al	O
,	O
2021	O
)	O
benchmark	O
,	O
the	O
testing	O
portion	O
from	O
WebGPT	O
(	O
their	O
samples	O
are	O
publicly	O
hosted	O
at	O
https://openaipublic.blob.core.windows	O
.	O
net	O
/	O
webgpt	O
-	O
answer	O
-	O
viewer	O
/	O
index.html	O
,	O
which	O
answers	O
questions	O
from	O
the	O
ELI5	B-DatasetName
test	O
set	O
)	O
,	O
and	O
the	O
validation	O
split	O
from	O
Natural	B-DatasetName
Questions	I-DatasetName
.	O
difficulty	O
of	O
evaluation	O
and	O
only	O
look	O
at	O
questions	O
with	O
short	O
span	O
answer	O
.	O
We	O
create	O
a	O
filtered	O
set	O
of	O
NQ	B-DatasetName
that	O
focuses	O
on	O
paragraph	O
-	O
level	O
answers	O
containing	O
complex	O
queries	O
.	O
5	O
While	O
many	O
NQ	B-DatasetName
questions	O
can	O
be	O
answered	O
with	O
a	O
short	O
entity	O
(	O
e.g.	O
,	O
how	O
many	O
episodes	O
in	O
season	O
2	O
breaking	O
bad	O
?	O
)	O
,	O
many	O
others	O
questions	O
require	O
paragraph	O
length	O
answer	O
(	O
e.g.	O
,	O
what	O
does	O
the	O
word	O
china	O
mean	O
in	O
chinese	O
?	O
)	O
.	O
This	O
provides	O
a	O
complementary	O
view	O
compared	O
to	O
the	O
other	O
two	O
datasets	O
,	O
as	O
the	O
answers	O
are	O
not	O
written	O
specifically	O
for	O
the	O
questions	O
but	O
harvested	O
from	O
pre	O
-	O
written	O
Wikipedia	O
paragraphs	O
.	O
Thus	O
,	O
this	O
simulates	O
scenarios	O
where	O
model	O
retrieves	O
paragraphs	O
instead	O
of	O
generating	O
them	O
.	O

With	O
our	O
annotated	O
data	O
,	O
we	O
study	O
the	O
differences	O
between	O
the	O
three	O
types	O
of	O
long	O
-	O
form	O
answers	O
,	O
namely	O
answers	O
provided	O
by	O
users	O
in	O
online	O
community	O
(	O
ELI5	B-DatasetName
)	O
,	O
answers	O
written	O
by	O
trained	O
annotators	O
through	O
web	O
search	O
(	O
WebGPT	O
)	O
,	O
and	O
answers	O
identified	O
in	O
Wikipedia	O
passages	O
(	O
NQ	B-DatasetName
)	O
.	O
Q	O
/	O
A	O
Validity	O
Table	O
2	O
summarizes	O
the	O
portion	O
of	O
valid	O
answers	O
in	O
the	O
three	O
datasets	O
and	O
the	O
distribution	O
of	O
invalid	O
reasons	O
.	O
NQ	B-DatasetName
has	O
the	O
highest	O
rate	O
of	O
invalid	O
answer	O
(	O
15	O
%	O
)	O
.	O
Upon	O
manual	O
inspection	O
,	O
we	O
find	O
that	O
passages	O
from	O
Wikipedia	O
written	O
independently	O
of	O
the	O
question	O
often	O
only	O
partially	O
address	O
complex	O
questions	O
.	O
This	O
demonstrates	O
the	O
limitation	O
of	O
a	O
fully	O
extractive	O
approach	O
.	O
Around	O
10	O
%	O
of	O
the	O
answers	O
from	O
ELI5	B-DatasetName
reject	O
presupposition	O
in	O
the	O
question	O
,	O
which	O
is	O
a	O
common	O
phenomena	O
in	O
information	O
-	O
seeking	O
questions	O
.	O
WebGPT	O
boasts	O
the	O
lowest	O
invalid	O
rate	O
,	O
showing	O
the	O
high	O
quality	O
of	O
their	O
collected	O
answers	O
.	O

We	O
study	O
the	O
distribution	O
of	O
roles	O
in	O
three	O
datasets	O
(	O
Table	O
3	O
)	O
.	O
NQ	B-DatasetName
shows	O
the	O
highest	O
proportion	O
of	O
auxiliary	O
information	O
,	O
as	O
the	O
paragraphs	O
are	O
written	O
independent	O
of	O
the	O
questions	O
.	O
In	O
contrast	O
,	O
ELI5	B-DatasetName
contains	O
more	O
answer	O
sentences	O
and	O
examples	O
which	O
provide	O
explanation	O
.	O
Both	O
ELI5	B-DatasetName
and	O
WebGPT	O
contain	O
organizational	O
sentences	O
,	O
demonstrating	O
that	O
it	O
is	O
commonly	O
used	O
when	O
answerers	O
assemble	O
answers	O
that	O
cover	O
more	O
than	O
one	O
aspects	O
.	O
In	O
all	O
datasets	O
,	O
around	O
half	O
of	O
the	O
sentences	O
serve	O
roles	O
other	O
than	O
directly	O
answering	O
the	O
questions	O
,	O
such	O
as	O
providing	O
auxiliary	O
information	O
or	O
giving	O
an	O
example	O
,	O
which	O
reflects	O
the	O
wide	O
spectrum	O
of	O
information	O
presented	O
in	O
a	O
long	O
-	O
form	O
answer	O
.	O
sentences	O
.	O
This	O
is	O
partially	O
because	O
both	O
datasets	O
are	O
more	O
extractive	O
and	O
less	O
personal	O
,	O
without	O
sentences	O
which	O
serve	O
the	O
role	O
of	O
various	O
kinds	O
of	O
communication	O
from	O
answerers	O
to	O
question	O
askers	O
(	O
e.g.	O
expressing	O
sentiments	O
,	O
pointing	O
to	O
other	O
resources	O
)	O
that	O
are	O
commonly	O
seen	O
in	O
online	O
community	O
forum	O
.	O
Discourse	O
Structure	O
Figure	O
3	O
presents	O
the	O
distribution	O
of	O
each	O
role	O
per	O
its	O
relative	O
location	O
in	O
the	O
answer	O
.	O
Despite	O
the	O
significant	O
differences	O
in	O
the	O
proportion	O
of	O
different	O
discourse	O
roles	O
,	O
the	O
positioning	O
of	O
the	O
roles	O
is	O
similar	O
across	O
the	O
datasets	O
.	O
Answer	O
summary	O
and	O
organizational	O
sentences	O
typically	O
locate	O
at	O
the	O
beginning	O
of	O
the	O
paragraph	O
,	O
examples	O
and	O
answers	O
often	O
in	O
the	O
middle	O
,	O
with	O
an	O
increasing	O
portion	O
of	O
auxiliary	O
information	O
towards	O
the	O
end	O
.	O
The	O
sentences	O
belonging	O
to	O
miscellaneous	O
role	O
frequently	O
position	O
at	O
the	O
beginning	O
or	O
the	O
end	O
of	O
the	O
paragraph	O
,	O
instead	O
of	O
intervening	O
in	O
the	O
middle	O
.	O
WebGPT	O
contains	O
a	O
higher	O
portion	O
of	O
auxiliary	O
information	O
locating	O
at	O
the	O
beginning	O
of	O
the	O
passage	O
,	O
followed	O
by	O
the	O
answer	O
summary	O
sentences	O
.	O
Answer	O
Extractiveness	O
One	O
important	O
aspect	O
for	O
long	O
-	O
form	O
answer	O
is	O
whether	O
the	O
answer	O
can	O
be	O
attributed	O
to	O
an	O
external	O
evidence	O
document	O
.	O
While	O
answers	O
from	O
NQ	B-DatasetName
are	O
directly	O
extracted	O
from	O
Wikipedia	O
passages	O
,	O
both	O
ELI5	B-DatasetName
and	O
WebGPT	O
are	O
written	O
specifically	O
for	O
the	O
question	O
.	O
To	O
help	O
with	O
verification	O
,	O
both	O
datasets	O
provide	O
evidence	O
documents	O
paired	O
with	O
the	O
answer	O
,	O
and	O
yet	O
there	O
are	O
design	O
differences	O
between	O
the	O
two	O
.	O
Answerer	O
(	O
annotators	O
)	O
of	O
WebGPT	O
were	O
instructed	O
to	O
answer	O
the	O
question	O
based	O
on	O
the	O
evidence	O
documents	O
returned	O
by	O
a	O
search	O
engine	O
,	O
while	O
answers	O
from	O
ELI5	B-DatasetName
were	O
written	O
first	O
independently	O
and	O
later	O
paired	O
with	O
relevant	O
Wikipedia	O
passages	O
(	O
Petroni	O
et	O
al	O
,	O
2021	O
)	O
.	O
We	O
found	O
that	O
such	O
difference	O
leads	O
to	O
different	O
level	O
of	O
extractiveness	O
of	O
the	O
answer	O
,	O
by	O
calculating	O
sentence	O
-	O
level	O
lexical	O
overlap	O
(	O
after	O
removing	O
stopwords	O
)	O
with	O
the	O
evidence	O
document	O
.	O
Overall	O
,	O
WebGPT	O
answers	O
exhibit	O
more	O
lexical	O
overlap	O
(	O
unigram	O
:	O
0.64	O
,	O
bigram	O
:	O
0.36	O
)	O
with	O
evidence	O
document	O
than	O
ELI5	B-DatasetName
answers	O
(	O
unigram	O
:	O
0.09	O
,	O
bigram	O
:	O
0.01	O
)	O
.	O
Answer	O
sentences	O
with	O
different	O
roles	O
also	O
exhibit	O
different	O
levels	O
of	O
extractiveness	O
(	O
detailed	O
role	O
-	O
level	O
overlap	O
can	O
be	O
found	O
in	O
Table	O
8	O
in	O
the	O
appendix	O
)	O
.	O
For	O
ELI5	B-DatasetName
answers	O
,	O
sentences	O
belonging	O
to	O
answer	O
and	O
summary	O
roles	O
have	O
the	O
highest	O
overlap	O
while	O
example	O
,	O
auxiliary	O
information	O
and	O
miscellaneous	O
sentences	O
are	O
less	O
grounded	O
to	O
external	O
sources	O
.	O
For	O
WebGPT	O
,	O
organizational	O
sentences	O
are	O
the	O
least	O
extractive	O
among	O
all	O
the	O
roles	O
.	O

Table	O
4	O
reports	O
the	O
results	O
on	O
ELI5	B-DatasetName
test	O
set	O
.	O
11	O
All	O
models	O
outperform	O
the	O
majority	O
and	O
summarylead	O
baselines	O
.	O
The	O
sequential	O
prediction	O
model	O
(	O
T5	B-MethodName
)	O
significantly	O
outperform	O
classification	O
model	O
(	O
RoBERTa	B-MethodName
)	O
which	O
makes	O
a	O
prediction	O
per	O
sentence	O
.	O
The	O
roles	O
with	O
lower	O
human	O
agreement	O
(	O
auxiliary	O
,	O
organizational	O
sentence	O
,	O
answer	O
)	O
also	O
exhibit	O
low	O
model	O
performances	O
,	O
reflecting	O
the	O
subjectivity	O
and	O
ambiguity	O
of	O
roles	O
for	O
some	O
sentences	O
.	O
Overall	O
,	O
with	O
a	O
moderate	O
amount	O
of	O
in	O
-	O
domain	O
annotated	O
data	O
,	O
our	O
best	O
model	O
(	O
T5	B-MethodName
-	O
large	O
)	O
can	O
reliably	O
classify	O
functional	O
roles	O
of	O
sentences	O
in	O
the	O
long	O
-	O
form	O
answers	O
,	O
showing	O
comparable	O
performances	O
to	O
human	O
lower	O
bound	O
.	O
Table	O
5	O
reports	O
the	O
results	O
on	O
the	O
three	O
out	O
-	O
ofdomain	O
datasets	O
,	O
WebGPT	O
,	O
NQ	B-DatasetName
and	O
ELI5	B-DatasetName
-	O
model	O
(	O
model	O
-	O
generated	O
answers	O
)	O
.	O
Human	O
agreement	O
numbers	O
are	O
comparable	O
across	O
all	O
datasets	O
(	O
0.53	O
-	O
0.59	O
for	O
lower	O
bound	O
,	O
0.73	O
-	O
0.78	O
for	O
upper	O
bound	O
)	O
.	O
While	O
T5	B-MethodName
-	O
large	O
still	O
exhibits	O
the	O
best	O
overall	O
performance	O
,	O
all	O
learned	O
models	O
perform	O
worse	O
,	O
partially	O
as	O
the	O
role	O
distribution	O
has	O
changed	O
.	O
Despite	O
trained	O
on	O
the	O
ELI5	B-DatasetName
dataset	O
,	O
role	O
classification	O
model	O
also	O
perform	O
worse	O
on	O
model	O
-	O
generated	O
answers	O
(	O
ELI5model	O
)	O
,	O
echoing	O
our	O
observation	O
that	O
human	O
annotators	O
find	O
it	O
challenging	O
to	O
process	O
the	O
discourse	O
structure	O
of	O
model	O
-	O
generated	O
answers	O
.	O
Our	O
pilot	O
showed	O
that	O
training	O
with	O
in	O
-	O
domain	O
data	O
improved	O
the	O
performances	O
consistently	O
,	O
but	O
the	O
evaluation	O
is	O
on	O
a	O
small	O
subset	O
(	O
after	O
setting	O
apart	O
some	O
for	O
training	O
)	O
,	O
so	O
we	O
do	O
not	O
report	O
it	O
here	O
.	O
We	O
anticipate	O
that	O
automatic	O
role	O
classification	O
is	O
feasible	O
given	O
moderate	O
amount	O
of	O
annotation	O
for	O
all	O
three	O
humanwritten	O
long	O
-	O
form	O
answer	O
datasets	O
we	O
study	O
.	O

Discourse	O
structure	O
.	O
Our	O
work	O
is	O
closely	O
related	O
to	O
functional	O
structures	O
defined	O
through	O
content	O
types	O
explored	O
in	O
other	O
domains	O
;	O
prior	O
work	O
has	O
affirmed	O
the	O
usefulness	O
of	O
these	O
structures	O
in	O
downstream	O
NLP	O
tasks	O
.	O
In	O
news	O
,	O
Choubey	O
et	O
al	O
(	O
2020	O
)	O
adopted	O
Van	O
Dijk	O
(	O
2013	O
)	O
's	O
content	O
schema	O
cataloging	O
events	O
(	O
e.g.	O
,	O
main	O
event	O
,	O
anecdotal	O
)	O
,	O
which	O
they	O
showed	O
to	O
improve	O
the	O
performance	O
of	O
event	B-TaskName
coreference	I-TaskName
resolution	I-TaskName
.	O
In	O
scientific	O
writing	O
,	O
content	O
types	O
(	O
e.g.	O
,	O
background	O
,	O
methodology	O
)	O
are	O
shown	O
to	O
be	O
useful	O
for	O
summarization	B-TaskName
(	O
Teufel	O
and	O
Moens	O
,	O
2002	O
;	O
Cohan	O
et	O
al	O
,	O
2018	O
)	O
,	O
information	O
extraction	O
(	O
Mizuta	O
et	O
al	O
,	O
2006	O
;	O
Liakata	O
et	O
al	O
,	O
2012	O
)	O
,	O
and	O
information	B-TaskName
retrieval	I-TaskName
(	O
Kircz	O
,	O
1991	O
;	O
Liddy	O
,	O
1991	O
)	O
.	O
The	O
discourse	O
structure	O
of	O
argumentative	O
texts	O
(	O
e.g.	O
,	O
support	O
,	O
rebuttal	O
)	O
(	O
Peldszus	O
and	O
Stede	O
,	O
2013	O
;	O
Becker	O
et	O
al	O
,	O
2016	O
;	O
Stab	O
and	O
Gurevych	O
,	O
2017	O
)	O
has	O
also	O
been	O
applied	O
on	O
argumentation	O
min	O
-	O
ing	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
no	O
prior	O
work	O
has	O
studied	O
the	O
discourse	O
structure	O
of	O
long	O
-	O
form	O
answers	O
.	O
Question	B-TaskName
Answering	I-TaskName
.	O
Recent	O
work	O
(	O
Cao	O
and	O
Wang	O
,	O
2021	O
)	O
have	O
investigated	O
the	O
ontology	B-MethodName
of	O
questions	O
,	O
which	O
includes	O
comparison	O
questions	O
,	O
verification	O
questions	O
,	O
judgement	O
questions	O
,	O
etc	O
.	O
We	O
construct	O
the	O
ontology	B-MethodName
of	O
functional	O
roles	O
of	O
answer	O
sentences	O
.	O
One	O
of	O
the	O
roles	O
in	O
our	O
ontology	B-MethodName
is	O
summary	O
,	O
yielding	O
an	O
extractive	B-TaskName
summarization	I-TaskName
dataset	O
.	O
This	O
shares	O
motivation	O
with	O
a	O
line	O
of	O
work	O
studying	O
query	O
-	O
focused	O
summarization	B-TaskName
(	O
Xu	O
and	O
Lapata	O
,	O
2020	O
)	O
.	O
Concurrent	O
to	O
our	O
work	O
,	O
Su	O
et	O
al	O
(	O
2022	O
)	O
studies	O
improving	O
faithfulness	O
of	O
long	O
-	O
form	O
answer	O
through	O
predicting	O
and	O
focusing	O
on	O
salient	O
information	O
in	O
retrieved	O
evidence	O
document	O
.	O
Lastly	O
,	O
our	O
work	O
build	O
up	O
on	O
three	O
datasets	O
containing	O
longform	O
answers	O
(	O
Kwiatkowski	O
et	O
al	O
,	O
2019	O
;	O
Fan	O
et	O
al	O
,	O
2019	O
;	O
Nakano	O
et	O
al	O
,	O
2021	O
)	O
and	O
extends	O
the	O
analysis	O
of	O
long	O
-	O
form	O
answers	O
from	O
earlier	O
studies	O
(	O
Krishna	O
et	O
al	O
,	O
2021	O
)	O
.	O

We	O
annotate	O
existing	O
,	O
publicly	O
available	O
long	O
-	O
form	O
question	B-TaskName
answering	I-TaskName
datasets	O
which	O
might	O
contain	O
incorrect	O
and	O
outdated	O
information	O
and	O
societal	O
biases	O
.	O
We	O
collected	O
annotations	O
through	O
crowdsourcing	O
platform	O
and	O
also	O
by	O
recruiting	O
undergraduate	O
annotators	O
at	O
our	O
educational	O
institution	O
.	O
We	O
paid	O
a	O
reasonable	O
hourly	O
wage	O
(	O
$	O
13	O
/	O
hour	O
)	O
to	O
annotators	O
and	O
documented	O
our	O
data	O
collection	O
process	O
with	O
datasheet	O
(	O
Gebru	O
et	O
al	O
,	O
2021	O
)	O
.	O
We	O
include	O
studies	O
on	O
the	O
extractiveness	O
of	O
long	O
-	O
form	O
answers	O
(	O
how	O
much	O
content	O
can	O
be	O
grounded	O
to	O
evidence	O
document	O
)	O
through	O
a	O
coarse	O
measure	O
of	O
lexical	O
overlap	O
.	O
This	O
is	O
connected	O
to	O
faithfulness	O
and	O
reducing	O
hallucination	O
of	O
QA	O
system	O
.	O
Our	O
study	O
is	O
limited	O
to	O
English	O
sources	O
,	O
and	O
we	O
hope	O
future	O
work	O
can	O
address	O
analysis	O
in	O
other	O
languages	O
.	O
I	O
think	O
one	O
of	O
the	O
biggest	O
ones	O
is	O
that	O
your	O
spouse	O
becomes	O
your	O
legal	O
'	O
next	O
of	O
kin	O
'	O
,	O
meaning	O
you	O
can	O
make	O
medical	O
decisions	O
for	O
them	O
,	O
own	O
their	O
property	O
after	O
they	O
die	O
,	O
etc	O
.	O

We	O
use	O
pytorch	O
-	O
transformers	O
Wolf	O
et	O
al	O
(	O
2019	O
)	O
to	O
implement	O
our	O
models	O
.	O
The	O
hyperparameters	O
are	O
manually	O
searched	O
by	O
the	O
authors	O
.	O
Table	O
9	O
:	O
Different	O
reasons	O
for	O
invalid	O
question	O
answer	O
pairs	O
for	O
ELI5	B-DatasetName
-	O
model	O
and	O
annotator	O
agreement	O
.	O
We	O
report	O
both	O
Fleiss	O
kappa	O
and	O
pairwise	O
agreement	O
after	O
reannotation	O
.	O
For	O
reference	O
,	O
we	O
also	O
report	O
agreement	O
for	O
human	O
-	O
written	O
answers	O
annotated	O
.	O
Question	O
:	O
Do	O
animals	O
know	O
they	O
're	O
going	O
to	O
die	O
?	O
Role	O
I	O
read	O
an	O
article	O
about	O
this	O
once	O
,	O
I	O
ca	O
n't	O
find	O
it	O
now	O
,	O
but	O
I	O
remember	O
reading	O
about	O
a	O
dog	O
that	O
had	O
been	O
put	O
into	O
a	O
room	O
with	O
a	O
vacuum	O
cleaner	O
,	O
and	O
it	O
did	O
n't	O
notice	O
it	O
was	O
sucking	O
in	O
air	O
,	O
it	O
just	O
started	O
sucking	O
in	O
air	O
as	O
normal	O
.	O

It	O
was	O
pretty	O
amazing	O
to	O
watch	O
.	O
Disagreed	O
So	O
it	O
was	O
just	O
sucking	O
in	O
air	O
.	O
Example	O
Then	O
,	O
the	O
dog	O
got	O
out	O
of	O
the	O
room	O
and	O
began	O
running	O
around	O
the	O
house	O
,	O
running	O
into	O
things	O
and	O
being	O
hurt	O
.	O
Example	O
It	O
eventually	O
just	O
died	O
of	O
exhaustion	O
.	O
Example	O
So	O
,	O
no	O
,	O
they	O
do	O
n't	O
know	O
.	O
Answer	O
But	O
it	O
is	O
interesting	O
to	O
think	O
about	O
.	O
Miscellaneous	B-TaskName
It	O
might	O
have	O
just	O
been	O
a	O
part	O
of	O
their	O
routine	O
,	O
or	O
it	O
might	O
have	O
been	O
a	O
learned	O
behavior	O
,	O
or	O
it	O
might	O
have	O
been	O
something	O
they	O
did	O
because	O
it	O
was	O
the	O
only	O
way	O
they	O
could	O
do	O
it	O
,	O
and	O
they	O
figured	O
it	O
out	O
,	O
and	O
it	O
was	O
just	O
a	O
part	O
of	O
their	O
routine	O
,	O
and	O
they	O
thought	O
it	O
was	O
cool	O
.	O

Sentiment	O
Lexicon	O
Construction	O
with	O
Representation	B-TaskName
Learning	I-TaskName
Based	O
on	O
Hierarchical	O
Sentiment	O
Supervision	O

Sentiment	O
lexicon	O
is	O
an	O
important	O
tool	O
for	O
identifying	O
the	O
sentiment	O
polarity	O
of	O
words	O
and	O
texts	O
.	O
How	O
to	O
automatically	O
construct	O
sentiment	O
lexicons	O
has	O
become	O
a	O
research	O
topic	O
in	O
the	O
field	O
of	O
sentiment	B-TaskName
analysis	I-TaskName
and	O
opinion	B-TaskName
mining	I-TaskName
.	O
Recently	O
there	O
were	O
some	O
attempts	O
to	O
employ	O
representation	B-TaskName
learning	I-TaskName
algorithms	O
to	O
construct	O
a	O
sentiment	O
lexicon	O
with	O
sentiment	O
-	O
aware	O
word	O
embedding	O
.	O
However	O
,	O
these	O
methods	O
were	O
normally	O
trained	O
under	O
documentlevel	O
sentiment	O
supervision	O
.	O
In	O
this	O
paper	O
,	O
we	O
develop	O
a	O
neural	O
architecture	O
to	O
train	O
a	O
sentiment	O
-	O
aware	O
word	O
embedding	O
by	O
integrating	O
the	O
sentiment	O
supervision	O
at	O
both	O
document	O
and	O
word	O
levels	O
,	O
to	O
enhance	O
the	O
quality	O
of	O
word	O
embedding	O
as	O
well	O
as	O
the	O
sentiment	O
lexicon	O
.	O
Experiments	O
on	O
the	O
SemEval	B-DatasetName
2013	I-DatasetName
-	O
2016	O
datasets	O
indicate	O
that	O
the	O
sentiment	O
lexicon	O
generated	O
by	O
our	O
approach	O
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
both	O
supervised	O
and	O
unsupervised	O
sentiment	O
classification	O
,	O
in	O
comparison	O
with	O
several	O
strong	O
sentiment	O
lexicon	O
construction	O
methods	O
.	O

Sentiment	O
lexicon	O
is	O
a	O
set	O
of	O
words	O
(	O
or	O
phrases	O
)	O
each	O
of	O
which	O
is	O
assigned	O
with	O
a	O
sentiment	O
polarity	O
score	O
.	O
Sentiment	O
lexicon	O
plays	O
an	O
important	O
role	O
in	O
many	O
practical	O
sentiment	B-TaskName
analysis	I-TaskName
and	O
opinion	B-TaskName
mining	I-TaskName
tasks	O
.	O
There	O
were	O
some	O
manually	O
annotated	O
universal	O
sentiment	O
lexicons	O
such	O
as	O
General	B-DatasetName
Inquireer	O
(	O
GI	O
)	O
and	O
HowNet	O
.	O
However	O
,	O
due	O
to	O
the	O
ubiquitous	O
domain	O
diversity	O
and	O
absence	O
of	O
domain	O
prior	O
knowledge	O
,	O
the	O
automatic	O
construction	O
technique	O
for	O
domain	O
-	O
specific	O
sentiment	O
lex	O
-	O
*	O
The	O
corresponding	O
author	O
of	O
this	O
paper	O
.	O
icons	O
has	O
become	O
a	O
challenging	O
research	O
topic	O
in	O
the	O
field	O
of	O
sentiment	B-TaskName
analysis	I-TaskName
and	O
opinion	B-TaskName
mining	I-TaskName
(	O
Wang	O
and	O
Xia	O
,	O
2016	O
)	O
.	O
The	O
early	O
work	O
employed	O
unsupervised	O
learning	O
for	O
sentiment	O
lexicon	O
construction	O
.	O
They	O
normally	O
labelled	O
a	O
set	O
of	O
seed	O
words	O
at	O
first	O
,	O
and	O
then	O
learned	O
the	O
polarity	O
of	O
each	O
candidate	O
word	O
,	O
based	O
on	O
either	O
word	O
conjunction	O
relations	O
(	O
e.g.	O
,	O
constellation	O
and	O
transition	O
in	O
texts	O
)	O
(	O
Hatzivassiloglou	O
and	O
McKeown	O
,	O
1997	O
)	O
,	O
or	O
the	O
word	O
co	O
-	O
occurrence	O
information	O
(	O
such	O
as	O
pointwise	O
mutual	O
information	O
,	O
PMI	O
)	O
(	O
Turney	O
,	O
2002	O
)	O
,	O
between	O
the	O
candidate	O
word	O
and	O
the	O
seed	O
words	O
.	O
However	O
,	O
the	O
unsupervised	O
manner	O
showed	O
limited	O
effect	O
in	O
sentiment	O
prediction	O
,	O
and	O
the	O
performance	O
greatly	O
depends	O
on	O
the	O
quality	O
of	O
the	O
seed	O
words	O
.	O
To	O
fully	O
exploit	O
the	O
sentiment	O
labeling	O
information	O
in	O
texts	O
,	O
a	O
series	O
of	O
supervised	O
learning	O
methods	O
was	O
further	O
proposed	O
to	O
learn	O
the	O
sentiment	O
lexicons	O
.	O
For	O
example	O
,	O
Mohammad	O
et	O
al	O
(	O
2013	O
)	O
proposed	O
to	O
construct	O
sentiment	O
lexicons	O
by	O
calculating	O
PMI	O
between	O
the	O
word	O
and	O
the	O
distantly	O
supervised	O
sentiment	O
labels	O
(	O
such	O
as	O
emoticons	O
)	O
in	O
tweets	O
and	O
the	O
word	O
's	O
sentiment	O
orientation	O
(	O
SO	O
)	O
.	O
The	O
resulting	O
lexicons	O
obtained	O
the	O
best	O
results	O
in	O
SemEval	B-DatasetName
2013	I-DatasetName
.	O
More	O
advanced	O
representation	B-TaskName
learning	I-TaskName
models	O
were	O
also	O
utilized	O
,	O
with	O
the	O
aim	O
to	O
construct	O
the	O
sentiment	O
lexicons	O
with	O
efficient	O
word	B-TaskName
embeddings	I-TaskName
(	O
Tang	O
et	O
al	O
,	O
2014a	O
;	O
Hamilton	O
et	O
al	O
,	O
2016	O
;	O
Vo	O
and	O
Zhang	O
,	O
2016	O
)	O
.	O
The	O
traditional	O
representation	B-TaskName
learning	I-TaskName
framework	O
such	O
as	O
Word2Vec	O
only	O
captures	O
the	O
syntactic	O
information	O
in	O
the	O
texts	O
,	O
but	O
ignores	O
the	O
sentiment	O
relations	O
between	O
words	O
.	O
Therefore	O
,	O
some	O
researchers	O
attempted	O
to	O
add	O
sentiment	O
supervision	O
into	O
the	O
network	O
structure	O
,	O
in	O
order	O
to	O
train	O
a	O
sentimentaware	O
word	O
embedding	O
.	O
For	O
example	O
,	O
Tang	O
et	O
al	O
(	O
2014a	O
)	O
exploited	O
a	O
dedicated	O
neural	O
architecture	O
to	O
integrate	O
document	O
-	O
level	O
sentiment	O
supervision	O
and	O
the	O
syntactic	O
knowledge	O
for	O
representation	B-TaskName
learning	I-TaskName
.	O
The	O
sentiment	O
-	O
aware	O
word	O
embedding	O
is	O
then	O
used	O
to	O
construct	O
a	O
sentiment	O
lexicon	O
.	O
Vo	O
and	O
Zhang	O
(	O
2016	O
)	O
proposed	O
to	O
learn	O
a	O
two	O
-	O
dimensional	O
sentiment	O
representation	O
based	O
on	O
a	O
simple	O
neural	O
network	O
.	O
The	O
sentiment	O
lexicons	O
generated	O
by	O
their	O
approach	O
obtained	O
better	O
performance	O
to	O
predict	O
the	O
tweet	O
sentiment	O
labels	O
,	O
in	O
comparison	O
with	O
the	O
PMI	O
-	O
based	O
method	O
(	O
Mohammad	O
et	O
al	O
,	O
2013	O
)	O
.	O
Although	O
these	O
supervised	O
learning	O
methods	O
can	O
to	O
some	O
extent	O
exploit	O
the	O
sentiment	O
labeling	O
information	O
in	O
the	O
texts	O
and	O
can	O
learn	O
a	O
sentiment	O
-	O
aware	O
word	O
embedding	O
,	O
the	O
manner	O
of	O
using	O
document	O
-	O
level	O
sentiment	O
supervision	O
suffers	O
from	O
some	O
complex	O
linguistic	O
phenomena	O
such	O
as	O
negation	O
,	O
transition	O
and	O
comparative	O
degree	O
,	O
and	O
hence	O
unable	O
to	O
capture	O
the	O
fine	O
-	O
grained	O
sentiment	O
information	O
in	O
the	O
text	O
.	O
For	O
example	O
,	O
in	O
the	O
following	O
tweet	O
"	O
Four	O
more	O
fake	O
people	O
added	O
me	O
.	O
Is	O
this	O
why	O
people	O
do	O
n't	O
like	O
Twitter	O
?	O
:	O
(	O
"	O
,	O
the	O
document	O
-	O
level	O
sentiment	O
label	O
is	O
negative	O
,	O
but	O
there	O
is	O
a	O
positive	O
word	O
"	O
like	O
"	O
in	O
the	O
text	O
.	O
In	O
representation	B-TaskName
learning	I-TaskName
,	O
the	O
embeddings	O
of	O
words	O
are	O
summed	O
up	O
to	O
represent	O
the	O
document	O
,	O
and	O
the	O
word	O
"	O
like	O
"	O
will	O
be	O
falsely	O
associated	O
with	O
the	O
negative	O
sentiment	O
label	O
.	O
Such	O
linguistic	O
phenomena	O
occur	O
frequently	O
in	O
review	O
texts	O
,	O
and	O
makes	O
sentiment	O
-	O
aware	O
word	O
representation	B-TaskName
learning	I-TaskName
less	O
effective	O
.	O
To	O
address	O
this	O
problem	O
,	O
in	O
this	O
paper	O
,	O
we	O
propose	O
a	O
new	O
representation	B-TaskName
learning	I-TaskName
framework	O
called	O
HSSWE	O
,	O
to	O
learn	O
sentiment	O
-	O
aware	O
word	B-TaskName
embeddings	I-TaskName
based	O
on	O
hierarchical	O
sentiment	O
supervision	O
.	O
In	O
HSSWE	O
,	O
the	O
learning	O
algorithm	O
is	O
supervised	O
under	O
both	O
document	O
-	O
level	O
sentiment	O
labels	O
and	O
word	O
-	O
level	O
sentiment	O
annotations	O
(	O
e.g.	O
,	O
labeling	O
"	O
like	O
"	O
as	O
a	O
positive	O
word	O
)	O
.	O
By	O
leveraging	O
the	O
sentiment	O
supervision	O
at	O
both	O
document	O
and	O
word	O
level	O
,	O
our	O
approach	O
can	O
avoid	O
the	O
sentiment	O
learning	O
flaws	O
caused	O
by	O
coarse	O
-	O
grained	O
document	O
-	O
level	O
supervision	O
by	O
incorporating	O
finegrained	O
word	O
-	O
level	O
supervision	O
,	O
and	O
improve	O
the	O
quality	O
of	O
sentiment	O
-	O
aware	O
word	O
embedding	O
.	O
Finally	O
,	O
following	O
Tang	O
et	O
al	O
(	O
2014a	O
)	O
,	O
a	O
simple	O
classifier	O
was	O
constructed	O
to	O
obtain	O
the	O
domainspecific	O
sentiment	O
lexicon	O
by	O
using	O
word	B-TaskName
embeddings	I-TaskName
as	O
inputs	O
.	O
The	O
main	O
contributions	O
of	O
this	O
work	O
are	O
as	O
follows	O
:	O
1	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
work	O
that	O
learns	O
the	O
sentiment	O
-	O
aware	O
word	O
representation	O
under	O
supervision	O
at	O
both	O
document	O
and	O
word	O
levels	O
.	O
2	O
.	O
Our	O
approach	O
supports	O
several	O
kinds	O
of	O
wordlevel	O
sentiment	O
annotations	O
such	O
as	O
1	O
)	O
predefined	O
sentiment	O
lexicon	O
;	O
2	O
)	O
PMI	O
-	O
SO	O
lexicon	O
with	O
hard	O
sentiment	O
annotation	O
;	O
3	O
)	O
PMI	O
-	O
SO	O
lexicon	O
with	O
soft	O
sentiment	O
annotation	O
.	O
By	O
using	O
PMI	O
-	O
SO	O
dictionary	O
as	O
word	O
-	O
level	O
sentiment	O
annotation	O
,	O
our	O
approach	O
is	O
totally	O
corpus	O
-	O
based	O
,	O
without	O
any	O
external	O
resource	O
.	O
3	O
.	O
Our	O
approach	O
obtains	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
comparison	O
with	O
several	O
strong	O
sentiment	O
lexicon	O
construction	O
methods	O
,	O
on	O
the	B-DatasetName
benchmark	I-DatasetName
SemEval	B-DatasetName
2013	I-DatasetName
-	O
2016	O
datasets	O
for	O
twitter	O
sentiment	O
classification	O
.	O

In	O
general	O
,	O
sentiment	O
lexicons	O
construction	O
can	O
be	O
classified	O
into	O
two	O
categories	O
,	O
dictionary	O
-	O
based	O
methods	O
and	O
corpus	O
-	O
based	O
methods	O
.	O
Dictionary	O
-	O
based	O
methods	O
generally	O
integrate	O
predefined	O
resources	O
,	O
such	O
as	O
WordNet	O
,	O
to	O
construct	O
sentiment	O
lexicons	O
.	O
Hu	O
and	O
Liu	O
(	O
2004	O
)	O
exploited	O
WordNet	O
for	O
sentiment	O
lexicon	O
construction	O
.	O
They	O
first	O
labelled	O
two	O
sets	O
of	O
seed	O
words	O
by	O
polarities	O
,	O
then	O
extended	O
the	O
sets	O
by	O
adding	O
the	O
synonyms	O
for	O
each	O
word	O
to	O
the	O
same	O
set	O
and	O
antonyms	O
to	O
the	O
other	O
.	O
For	O
a	O
given	O
new	O
word	O
,	O
Kim	O
and	O
Hovy	O
(	O
2004	O
)	O
introduced	O
a	O
Naive	O
Bayes	O
model	O
to	O
predict	O
the	O
polarities	O
with	O
.the	O
synonym	O
set	O
obtained	O
from	O
WordNet	O
as	O
features	O
.	O
Kamps	O
et	O
al	O
(	O
2004	O
)	O
investigated	O
a	O
graph	O
-	O
theoretic	O
model	O
of	O
WordNet	O
's	O
synonymy	O
relation	O
and	O
measured	O
the	O
sentiment	O
orientation	O
by	O
distance	O
between	O
each	O
candidate	O
word	O
and	O
the	O
seed	O
words	O
with	O
different	O
polarities	O
.	O
Heerschop	O
et	O
al	O
(	O
2011	O
)	O
proposed	O
a	O
method	O
to	O
propagate	O
the	O
sentiment	O
of	O
seed	O
set	O
words	O
through	O
semantic	O
relations	O
of	O
WordNet	O
.	O
Corpus	O
-	O
based	O
approaches	O
originate	O
from	O
the	O
latent	O
relation	O
hypothesis	O
:	O
"	O
Pairs	O
of	O
words	O
that	O
cooccur	O
in	O
similar	O
patterns	O
tend	O
to	O
have	O
similar	O
semantic	O
and	O
sentiment	O
relations	O
"	O
(	O
Turney	O
,	O
2008	O
)	O
.	O
The	O
primary	O
corpus	O
-	O
based	O
method	O
made	O
the	O
use	O
of	O
PMI	O
.	O
Turney	O
(	O
2002	O
)	O
built	O
a	O
sentiment	O
lexicon	O
by	O
calculating	O
PMI	O
between	O
the	O
candidate	O
word	O
and	O
seed	O
words	O
.	O
The	O
difference	O
of	O
the	O
PMI	O
score	O
between	O
positive	O
and	O
negative	O
seed	O
words	O
is	O
finally	O
used	O
as	O
the	O
sentiment	O
orientation	O
(	O
SO	O
)	O
of	O
each	O
candidate	O
word	O
(	O
Turney	O
,	O
2002	O
)	O
.	O
Many	O
variants	O
of	O
PMI	O
were	O
proposed	O
afterwards	O
,	O
for	O
example	O
,	O
positive	O
pointwise	O
mutual	O
information	O
(	O
PPMI	B-DatasetName
)	O
,	O
second	O
order	O
co	O
-	O
occurrence	O
PMI	O
(	O
SOC	B-DatasetName
-	O
PMI	O
)	O
,	O
etc	O
.	O
Hamilton	O
et	O
al	O
(	O
2016	O
)	O
proposed	O
to	O
build	O
a	O
sentiment	O
lexicon	O
by	O
a	O
propagation	O
method	O
.	O
The	O
key	O
of	O
this	O
method	O
is	O
to	O
build	O
a	O
lexical	O
graph	O
by	O
calculating	O
the	O
PPMI	B-DatasetName
between	O
words	O
.	O
Instead	O
of	O
calculating	O
the	O
PMI	O
between	O
words	O
,	O
Mohammad	O
et	O
al	O
(	O
2013	O
)	O
proposed	O
to	O
use	O
emoticons	O
as	O
distant	O
supervision	O
and	O
calculate	O
the	O
PMI	O
between	O
words	O
and	O
the	O
distant	O
class	O
labels	O
,	O
and	O
obtained	O
sound	O
performance	O
for	O
tweet	O
sentiment	O
classification	O
.	O
The	O
latest	O
corpus	O
-	O
based	O
approaches	O
normally	O
utilize	O
the	O
up	O
-	O
to	O
-	O
date	O
machine	O
learning	O
models	O
(	O
e.g.	O
neural	O
networks	O
)	O
to	O
first	O
learn	O
a	O
sentimentaware	O
distributed	O
representation	O
of	O
words	O
,	O
based	O
on	O
which	O
the	O
sentiment	O
lexicon	O
is	O
then	O
constructed	O
.	O
There	O
were	O
many	O
word	O
representation	B-TaskName
learning	I-TaskName
methods	O
such	O
as	O
NNLM	O
(	O
Bengio	O
et	O
al	O
,	O
2003	O
)	O
and	O
Word2Vec	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
.	O
However	O
,	O
they	O
mainly	O
consider	O
the	O
syntactic	O
relation	O
of	O
words	O
in	O
the	O
context	O
but	O
ignore	O
the	O
sentiment	O
information	O
.	O
Some	O
work	O
were	O
later	O
proposed	O
to	O
deal	O
with	O
this	O
problem	O
by	O
incorporating	O
the	O
sentiment	O
information	O
during	O
representation	B-TaskName
learning	I-TaskName
.	O
For	O
example	O
,	O
Tang	O
et	O
al	O
(	O
2014a	O
)	O
adapted	O
a	O
variant	O
of	O
skip	O
-	O
gram	O
model	O
,	O
which	O
can	O
learn	O
the	O
sentiment	O
information	O
based	O
on	O
distant	O
supervision	O
.	O
Furthermore	O
,	O
Tang	O
et	O
al	O
(	O
2014b	O
)	O
proposed	O
a	O
new	O
neural	O
network	O
approach	O
called	O
SSWE	O
to	O
train	O
sentimentaware	O
word	O
representation	O
.	O
Vo	O
and	O
Zhang	O
(	O
2016	O
)	O
exploited	O
a	O
simple	O
and	O
fast	O
neural	O
network	O
to	O
train	O
a	O
2	O
-	O
dimensional	O
representation	O
.	O
Each	O
dimension	O
is	O
explicitly	O
associated	O
with	O
a	O
sentiment	O
polarity	O
.	O
The	O
sentiment	O
-	O
aware	O
word	O
representation	O
in	O
these	O
methods	O
was	O
normally	O
trained	O
based	O
on	O
only	O
document	O
-	O
level	O
sentiment	O
supervision	O
.	O
In	O
contrast	O
,	O
the	O
learning	O
algorithm	O
in	O
our	O
approach	O
is	O
supervised	O
under	O
both	O
document	O
-	O
level	O
and	O
wordlevel	O
sentiment	O
supervision	O
.	O

Our	O
approach	O
is	O
comprised	O
of	O
three	O
base	O
modules	O
:	O
(	O
1	O
)	O
Word	O
-	O
level	O
sentiment	O
learning	O
and	O
annotation	O
;	O
(	O
2	O
)	O
Sentiment	O
-	O
aware	O
word	O
embedding	O
learning	O
;	O
(	O
3	O
)	O
Sentiment	O
lexicon	O
construction	O
.	O
Our	O
approach	O
depends	O
on	O
document	O
-	O
level	O
sentiment	O
labels	O
.	O
The	O
tweet	O
corpus	O
provides	O
a	O
cheap	O
way	O
to	O
get	O
document	O
-	O
level	O
sentiment	O
annotation	O
,	O
owing	O
to	O
the	O
distant	O
sentiment	O
supervision	O
.	O
But	O
it	O
should	O
be	O
noted	O
that	O
our	O
approach	O
is	O
feasible	O
for	O
any	O
corpus	O
provided	O
with	O
document	O
-	O
level	O
sentiment	O
labels	O
(	O
not	O
merely	O
tweets	O
)	O
.	O
The	O
first	O
module	O
of	O
our	O
method	O
aims	O
to	O
learn	O
the	O
pseudo	O
sentiment	O
distribution	O
for	O
each	O
word	O
and	O
use	O
it	O
as	O
word	O
-	O
level	O
sentiment	O
annotations	O
to	O
supervise	O
word	O
embedding	O
learning	O
.	O
In	O
the	O
second	O
module	O
,	O
we	O
learn	O
the	O
sentimentaware	O
embeddings	O
for	O
each	O
word	O
in	O
corpus	O
,	O
based	O
on	O
hierarchical	O
sentiment	O
supervision	O
.	O
In	O
the	O
last	O
module	O
,	O
we	O
construct	O
a	O
sentiment	O
lexicon	O
by	O
using	O
the	O
sentiment	O
-	O
aware	O
word	B-TaskName
embeddings	I-TaskName
as	O
the	O
basis	O
.	O

In	O
this	O
part	O
,	O
we	O
follow	O
the	O
method	O
proposed	O
by	O
Tang	O
et	O
al	O
(	O
2014a	O
)	O
to	O
build	O
a	O
classifier	O
to	O
convert	O
the	O
sentiment	O
-	O
aware	O
word	O
representation	O
learned	O
in	O
Section	O
3.2	O
to	O
a	O
sentiment	O
lexicon	O
.	O
The	O
word	O
representation	O
is	O
the	O
input	O
of	O
the	O
classifier	O
and	O
word	O
sentiment	O
polarity	O
is	O
the	O
output	O
.	O
Firstly	O
,	O
we	O
utilize	O
the	O
embedding	O
of	O
125	O
positive	O
and	O
109	O
negative	O
seed	O
words	O
manually	O
labelled	O
by	O
Tang	O
et	O
al	O
(	O
2014a	O
)	O
Thirdly	O
,	O
a	O
traditional	O
logistic	B-MethodName
regression	I-MethodName
classifier	O
is	O
trained	O
by	O
using	O
the	O
embeddings	O
of	O
extended	O
sentiment	O
words	O
as	O
the	O
inputs	O
.	O
The	O
sentiment	O
score	O
of	O
a	O
word	O
is	O
the	O
difference	O
between	O
its	O
positive	O
and	O
negative	O
probabilities	O
.	O
Finally	O
,	O
the	O
sentiment	O
lexicon	O
can	O
be	O
collected	O
by	O
using	O
the	O
classifier	O
to	O
predict	O
the	O
other	O
words	O
'	O
sentiment	O
score	O
.	O

In	O
this	O
paper	O
,	O
we	O
proposed	O
to	O
construct	O
sentiment	O
lexicons	O
based	O
on	O
a	O
sentiment	O
-	O
aware	O
word	O
representation	B-TaskName
learning	I-TaskName
approach	O
.	O
In	O
contrast	O
to	O
traditional	O
methods	O
normally	O
learned	O
based	O
on	O
only	O
the	O
document	O
-	O
level	O
sentiment	O
supervision	O
.	O
We	O
proposed	O
word	O
representation	B-TaskName
learning	I-TaskName
via	O
hierarchical	O
sentiment	O
supervision	O
,	O
i.e.	O
,	O
under	O
the	O
supervi	O
-	O
sion	O
at	O
both	O
word	O
and	O
document	O
levels	O
.	O
The	O
wordlevel	O
supervision	O
can	O
be	O
provided	O
based	O
on	O
either	O
predefined	O
sentiment	O
lexicons	O
or	O
the	O
learned	O
PMI	O
-	O
SO	O
based	O
sentiment	O
annotation	O
of	O
words	O
.	O
A	O
wide	O
range	O
of	O
experiments	O
were	O
conducted	O
on	O
several	O
benchmark	O
sentiment	O
classification	O
datasets	O
.	O
The	O
results	O
indicate	O
that	O
our	O
method	O
is	O
quite	O
effective	O
for	O
sentiment	O
-	O
aware	O
word	O
representation	O
,	O
and	O
the	O
sentiment	O
lexicon	O
generated	O
by	O
our	O
approach	O
beats	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
sentiment	O
lexicon	O
construction	O
approaches	O
.	O

Multi	O
-	O
Domain	O
Named	B-TaskName
Entity	I-TaskName
Recognition	I-TaskName
with	O
Genre	O
-	O
Aware	O
and	O
Agnostic	O
Inference	O

Accurately	O
identifying	O
named	O
entities	O
and	O
their	O
type	O
in	O
texts	O
is	O
a	O
key	O
processing	O
step	O
for	O
many	O
NLP	O
applications	O
.	O
Named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
is	O
an	O
important	O
component	O
in	O
several	O
tasks	O
including	O
named	O
entity	B-TaskName
linking	I-TaskName
(	O
Cucerzan	O
,	O
2007	O
)	O
,	O
co	O
-	O
reference	O
resolution	O
(	O
Ng	O
and	O
Cardie	O
,	O
2002	O
)	O
,	O
question	B-TaskName
answering	I-TaskName
(	O
Krishnamurthy	O
and	O
Mitchell	O
,	O
2015	O
)	O
,	O
relation	B-TaskName
extraction	I-TaskName
(	O
Culotta	O
and	O
Sorensen	O
,	O
2004	O
)	O
and	O
usually	O
sits	O
upstream	O
of	O
analytics	O
such	O
as	O
sentiment	O
(	O
Pang	O
and	O
Lee	O
,	O
2004	O
)	O
or	O
stance	O
(	O
Mohammad	O
et	O
al	O
,	O
2016	O
)	O
.	O
Building	O
robust	O
NER	B-TaskName
models	O
to	O
accurately	O
tag	O
and	O
adapt	O
to	O
heterogeneous	O
types	O
of	O
text	O
is	O
thus	O
paramount	O
.	O
Recent	O
research	O
focused	O
on	O
improving	O
the	O
overall	O
performance	O
of	O
NER	B-TaskName
models	O
on	O
specific	O
data	O
sets	O
.	O
Yet	O
NER	B-TaskName
models	O
show	O
relatively	O
high	O
variance	O
even	O
when	O
trained	O
on	O
the	O
same	O
data	O
(	O
Reimers	O
and	O
Gurevych	O
,	O
2017	O
)	O
and	O
poorly	O
generalize	O
when	O
tested	O
on	O
data	O
from	O
different	O
genres	O
1	O
,	O
especially	O
if	O
these	O
contain	O
entity	O
mentions	O
unseen	O
in	O
the	O
test	O
data	O
(	O
Augenstein	O
et	O
al	O
,	O
2017	O
;	O
Agarwal	O
et	O
al	O
,	O
2020	O
)	O
.	O
Despite	O
this	O
,	O
research	O
on	O
NER	B-TaskName
models	O
robust	O
to	O
different	O
types	O
of	O
input	O
is	O
usually	O
limited	O
to	O
the	O
standard	O
domain	B-TaskName
adaptation	I-TaskName
scenario	O
:	O
a	O
single	O
source	O
domain	O
rich	O
in	O
training	O
data	O
and	O
a	O
single	O
target	O
domain	O
with	O
limited	O
or	O
no	O
training	O
data	O
(	O
Lin	O
and	O
Lu	O
,	O
2018	O
)	O
.	O
We	O
argue	O
that	O
this	O
is	O
an	O
over	O
-	O
simplified	O
experimental	O
setup	O
that	O
is	O
not	O
typical	O
for	O
how	O
NER	B-TaskName
models	O
are	O
used	O
in	O
real	O
-	O
world	O
applications	O
.	O
Ideally	O
,	O
NER	B-TaskName
models	O
use	O
all	O
available	O
data	O
,	O
regardless	O
of	O
genre	O
,	O
and	O
perform	O
inference	O
on	O
data	O
from	O
any	O
genre	O
,	O
even	O
if	O
this	O
was	O
not	O
encountered	O
in	O
training	O
.	O
In	O
this	O
scenario	O
,	O
simply	O
pooling	O
all	O
the	O
available	O
data	O
is	O
likely	O
sub	O
-	O
optimal	O
as	O
genre	O
-	O
specific	O
differences	O
in	O
named	O
entity	O
mentions	O
are	O
useful	O
to	O
model	O
.	O
Conversely	O
,	O
models	O
limited	O
to	O
only	O
data	O
from	O
the	O
same	O
genre	O
as	O
the	O
test	O
set	O
are	O
likely	O
to	O
underperform	O
,	O
as	O
using	O
more	O
data	O
is	O
usually	O
beneficial	O
.	O
This	O
work	O
introduces	O
three	O
experimental	O
setups	O
for	O
the	O
NER	B-TaskName
task	O
where	O
models	O
are	O
trained	O
on	O
data	O
from	O
multiple	O
genres	O
and	O
evaluated	O
as	O
follows	O
:	O
a	O
)	O
Multi	O
-	O
Domain	O
-	O
evaluation	O
is	O
performed	O
across	O
multiple	O
genres	O
,	O
all	O
seen	O
in	O
training	O
.	O
b	O
)	O
Multi	O
-	O
Domain	O
with	O
Unknown	O
Domain	O
Labels	O
-	O
evaluation	O
is	O
carried	O
out	O
across	O
multiple	O
genres	O
,	O
all	O
seen	O
in	O
training	O
,	O
but	O
the	O
genre	O
label	O
for	O
each	O
document	O
is	O
unknown	O
at	O
inference	O
time	O
.	O
c	O
)	O
Zero	O
-	O
shot	O
Domain	O
-	O
evaluation	O
is	O
performed	O
on	O
documents	O
from	O
genres	O
unseen	O
in	O
training	O
.	O
We	O
propose	O
a	O
neural	O
architecture	O
for	O
NER	B-TaskName
tailored	O
to	O
these	O
three	O
experimental	O
setups	O
,	O
based	O
on	O
the	O
popular	O
BiLSTM	B-MethodName
-	O
CRF	B-MethodName
architecture	O
(	O
Lample	O
et	O
al	O
,	O
2016	O
)	O
.	O
We	O
augment	O
the	O
base	O
architecture	O
to	O
learn	O
both	O
domain	O
-	O
specific	O
and	O
independent	O
features	O
through	O
shared	O
and	O
private	O
domain	O
components	O
including	O
projections	O
and	O
CRFs	O
.	O
Further	O
,	O
we	O
add	O
a	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
objective	O
for	O
domain	O
prediction	O
to	O
guide	O
this	O
separation	O
.	O
This	O
model	O
can	O
perform	O
inference	O
on	O
a	O
text	O
without	O
knowledge	O
of	O
its	O
corresponding	O
domain	O
label	O
by	O
using	O
the	O
shared	O
components	O
.	O
We	O
compare	O
this	O
model	O
with	O
several	O
competitive	O
methods	O
that	O
use	O
a	O
similar	O
base	O
architecture	O
while	O
holding	O
the	O
embeddings	O
constant	O
(	O
i.e.	O
GloVe	B-MethodName
embeddings	I-MethodName
)	O
.	O
These	O
include	O
models	O
trained	O
on	O
data	O
from	O
each	O
domain	O
independently	O
,	O
models	O
that	O
pool	O
all	O
data	O
and	O
models	O
that	O
use	O
domain	O
identities	O
as	O
features	O
through	O
to	O
source	O
-	O
target	O
domain	B-TaskName
adaptation	I-TaskName
methods	O
.	O
Extensive	O
results	O
on	O
all	O
three	O
experimental	O
setups	O
on	O
a	O
collection	O
of	O
data	O
from	O
a	O
total	O
of	O
twelve	O
genres	O
demonstrate	O
that	O
our	O
proposed	O
architecture	O
outperforms	O
all	O
others	O
by	O
a	O
respectable	O
margin	O
.	O
Finally	O
,	O
through	O
an	O
error	O
analysis	O
of	O
our	O
results	O
,	O
we	O
aim	O
to	O
understand	O
the	O
contributions	O
of	O
each	O
proposed	O
component	O
and	O
the	O
margins	O
for	O
future	O
improvements	O
.	O

Setups	O
for	O
Domain	B-TaskName
Adaptation	I-TaskName
Domain	B-TaskName
adaptation	I-TaskName
,	O
formulated	O
as	O
learning	O
a	O
single	O
model	O
for	O
the	O
same	O
task	O
across	O
multiple	O
domains	O
,	O
is	O
a	O
wellstudied	O
research	O
area	O
in	O
NLP	O
(	O
Chelba	O
and	O
Acero	O
,	O
2004	O
;	O
Florian	O
et	O
al	O
,	O
2004	O
;	O
Blitzer	O
et	O
al	O
,	O
2006	O
;	O
Daumé	O
III	O
,	O
2007	O
)	O
.	O
The	O
standard	O
setup	O
for	O
domain	B-TaskName
adaptation	I-TaskName
is	O
to	O
maximize	O
performance	O
on	O
data	O
from	O
a	O
single	O
low	O
-	O
resource	O
(	O
target	O
)	O
domain	O
,	O
by	O
using	O
data	O
from	O
a	O
single	O
high	O
-	O
resource	O
(	O
source	O
)	O
domain	O
(	O
Blitzer	O
et	O
al	O
,	O
2007	O
;	O
Peng	O
and	O
Dredze	O
,	O
2017	O
)	O
.	O
Extensions	O
consider	O
a	O
single	O
source	O
and	O
multiple	O
different	O
target	O
domains	O
(	O
Yang	O
and	O
Eisenstein	O
,	O
2015	O
)	O
or	O
multiple	O
sources	O
and	O
a	O
single	O
target	O
domain	O
(	O
Mansour	O
et	O
al	O
,	O
2009	O
)	O
.	O
The	O
multi	O
-	O
domain	O
text	B-TaskName
classification	I-TaskName
task	O
studied	O
in	O
(	O
Li	O
and	O
Zong	O
,	O
2008	O
;	O
Wu	O
and	O
Huang	O
,	O
2015	O
;	O
Chen	O
and	O
Cardie	O
,	O
2018	O
)	O
is	O
the	O
analogous	O
setup	O
for	O
the	O
text	B-TaskName
classification	I-TaskName
task	O
to	O
the	O
first	O
experimental	O
setup	O
we	O
propose	O
for	O
NER	B-TaskName
.	O
Under	O
this	O
setup	O
,	O
training	O
and	O
evaluation	O
is	O
done	O
across	O
data	O
from	O
multiple	O
domains	O
.	O
Multi	O
-	O
Domain	B-TaskName
Adaptation	I-TaskName
Methods	O
for	O
multidomain	O
text	B-TaskName
classification	I-TaskName
use	O
data	O
fusion	O
either	O
at	O
the	O
feature	O
or	O
classifier	O
level	O
(	O
Li	O
and	O
Zong	O
,	O
2008	O
)	O
,	O
decomposing	O
the	O
classifier	O
into	O
a	O
shared	O
one	O
and	O
multiple	O
domain	O
-	O
specific	O
ones	O
(	O
Wu	O
and	O
Huang	O
,	O
2015	O
)	O
,	O
further	O
guided	O
by	O
a	O
domain	O
discriminator	O
(	O
Chen	O
and	O
Cardie	O
,	O
2018	O
)	O
which	O
is	O
also	O
used	O
in	O
multi	O
-	O
lingual	O
NER	B-TaskName
(	O
Chen	O
et	O
al	O
,	O
2019	O
)	O
.	O
Further	O
,	O
Mc	O
-	O
Closky	O
et	O
al	O
(	O
2010	O
)	O
explored	O
sequence	O
tagging	O
tasks	O
on	O
data	O
from	O
unknown	O
domains	O
and	O
Chen	O
and	O
Cardie	O
(	O
2018	O
)	O
experiment	O
with	O
sentiment	O
classification	O
on	O
data	O
from	O
unknown	O
domains	O
,	O
similar	O
to	O
our	O
third	O
experimental	O
setup	O
for	O
NER	B-TaskName
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
our	O
second	O
setup	O
where	O
the	O
domain	O
label	O
is	O
not	O
available	O
at	O
inference	O
time	O
was	O
never	O
explicitly	O
studied	O
.	O
We	O
note	O
that	O
most	O
of	O
these	O
approaches	O
make	O
use	O
of	O
additional	O
unlabeled	O
data	O
from	O
each	O
domain	O
to	O
learn	O
domain	O
-	O
specific	O
representations	O
.	O
We	O
do	O
not	O
use	O
these	O
resources	O
in	O
our	O
methods	O
,	O
as	O
we	O
assume	O
the	O
end	O
-	O
user	O
of	O
the	O
model	O
is	O
agnostic	O
to	O
the	O
data	O
used	O
in	O
training	O
and	O
wants	O
to	O
run	O
inference	O
without	O
having	O
to	O
provide	O
entire	O
comparable	O
corpora	O
.	O
Domain	B-TaskName
Adaptation	I-TaskName
for	O
NER	B-TaskName
Models	O
for	O
domain	B-TaskName
adaptation	I-TaskName
in	O
NER	B-TaskName
using	O
neural	O
architectures	O
were	O
studied	O
recently	O
,	O
albeit	O
mostly	O
for	O
covering	O
the	O
single	O
-	O
source	O
and	O
single	O
-	O
target	O
setup	O
.	O
The	O
INIT	O
method	O
trains	O
a	O
model	O
using	O
the	O
source	O
domain	O
data	O
,	O
and	O
its	O
parameters	O
are	O
used	O
to	O
initialize	O
a	O
target	O
model	O
which	O
is	O
fine	O
-	O
tuned	O
on	O
the	O
target	O
data	O
(	O
Mou	O
et	O
al	O
,	O
2016	O
)	O
.	O
The	O
MULT	O
method	O
trains	O
jointly	O
one	O
model	O
for	O
each	O
domain	O
with	O
shared	O
parameters	O
.	O
For	O
sequence	O
tagging	O
,	O
one	O
CRF	B-MethodName
for	O
each	O
of	O
the	O
two	O
domains	O
is	O
used	O
to	O
obtain	O
the	O
predictions	O
(	O
Yang	O
et	O
al	O
,	O
2017	O
)	O
.	O
Adaptation	O
can	O
also	O
be	O
made	O
at	O
the	O
embeddings	O
stage	O
(	O
Lin	O
and	O
Lu	O
,	O
2018	O
)	O
or	O
by	O
using	O
additional	O
unlabeled	O
data	O
from	O
the	O
source	O
domain	O
and	O
out	O
-	O
of	O
-	O
domain	O
annotated	O
data	O
(	O
He	O
and	O
Sun	O
,	O
2017	O
)	O
.	O
However	O
,	O
as	O
mentioned	O
above	O
,	O
this	O
assumes	O
that	O
unlabeled	O
training	O
data	O
can	O
be	O
provided	O
for	O
each	O
domain	O
,	O
which	O
may	O
not	O
be	O
realistic	O
.	O
The	O
model	O
adds	O
layers	O
between	O
embeddings	O
and	O
the	O
BiLSTM	B-MethodName
layers	O
,	O
between	O
the	O
BiL	O
-	O
STM	O
and	O
the	O
CRF	B-MethodName
for	O
the	O
target	O
domain	O
and	O
separate	O
CRF	B-MethodName
layers	O
,	O
the	O
latter	O
two	O
of	O
which	O
we	O
adapt	O
to	O
our	O
proposed	O
architecture	O
for	O
multi	O
-	O
domain	B-TaskName
adaptation	I-TaskName
.	O
A	O
hierarchical	O
Bayesian	O
prior	O
approach	O
is	O
used	O
in	O
(	O
Finkel	O
and	O
Manning	O
,	O
2009	O
)	O
to	O
tie	O
feature	O
weights	O
across	O
domains	O
when	O
information	O
is	O
sparse	O
and	O
also	O
allow	O
the	O
model	O
to	O
take	O
advantage	O
if	O
substantial	O
data	O
is	O
available	O
in	O
one	O
domain	O
.	O
Their	O
experiments	O
on	O
NER	B-TaskName
focused	O
only	O
on	O
three	O
data	O
sets	O
:	O
CoNLL	O
,	O
MUC	O
-	O
6	O
and	O
MUC	O
-	O
7	O
and	O
only	O
the	O
first	O
of	O
our	O
three	O
setups	O
.	O
A	O
multi	O
-	O
task	O
domain	B-TaskName
adaptation	I-TaskName
method	O
for	O
NER	B-TaskName
and	O
word	O
segmentation	O
is	O
used	O
in	O
(	O
Peng	O
and	O
Dredze	O
,	O
2017	O
)	O
.	O
The	O
proposed	O
architecture	O
learns	O
a	O
shared	O
representation	O
across	O
domains	O
and	O
experiments	O
with	O
linear	O
domain	O
projections	O
for	O
each	O
domain	O
to	O
guide	O
learning	O
of	O
shared	O
representations	O
.	O
The	O
output	O
of	O
these	O
linear	O
layers	O
is	O
fed	O
to	O
a	O
CRF	B-MethodName
.	O
We	O
adopt	O
the	O
linear	O
domain	O
projection	O
method	O
,	O
but	O
extend	O
this	O
to	O
also	O
include	O
a	O
shared	O
projection	O
,	O
followed	O
by	O
domain	O
-	O
specific	O
CRFs	O
and	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
.	O
Finally	O
,	O
another	O
type	O
of	O
domain	B-TaskName
adaptation	I-TaskName
is	O
temporal	O
adaptation	O
of	O
models	O
tested	O
on	O
data	O
that	O
is	O
more	O
recent	O
than	O
the	O
training	O
data	O
,	O
when	O
each	O
temporal	O
slice	O
can	O
be	O
considered	O
as	O
a	O
different	O
domain	O
(	O
Rijwhani	O
and	O
Preoţiuc	O
-	O
Pietro	O
,	O
2020	O
)	O
.	O

This	O
section	O
describes	O
the	O
proposed	O
NER	B-TaskName
architecture	O
tailored	O
the	O
architecture	O
to	O
our	O
multi	O
-	O
domain	O
experimental	O
setups	O
,	O
which	O
is	O
independent	O
of	O
input	O
embedding	O
representation	O
.	O

The	O
basic	O
component	O
of	O
our	O
NER	B-TaskName
models	O
is	O
an	O
architecture	O
which	O
has	O
reached	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
several	O
times	O
over	O
the	O
last	O
few	O
years	O
(	O
Lample	O
et	O
al	O
,	O
2016	O
;	O
Peters	O
et	O
al	O
,	O
2018	O
;	O
Akbik	O
et	O
al	O
,	O
2018	O
)	O
.	O
Named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
task	O
is	O
a	O
structured	B-TaskName
prediction	I-TaskName
task	O
and	O
earlier	O
statistical	O
approaches	O
are	O
based	O
models	O
like	O
Conditional	O
Random	O
Fields	O
(	O
Lafferty	O
et	O
al	O
,	O
2001	O
)	O
,	O
which	O
rely	O
on	O
features	O
often	O
designed	O
based	O
on	O
domain	O
-	O
specific	O
knowledge	O
(	O
Luo	O
et	O
al	O
,	O
2015	O
)	O
.	O
The	O
current	O
dominant	O
approach	O
to	O
the	O
NER	B-TaskName
task	O
consists	O
of	O
neural	O
architectures	O
based	O
on	O
recurrent	O
neural	O
networks	O
with	O
different	O
choices	O
of	O
input	O
representations	O
Ma	O
and	O
Hovy	O
,	O
2016	O
;	O
Lample	O
et	O
al	O
,	O
2016	O
;	O
Peters	O
et	O
al	O
,	O
2018	O
;	O
Akbik	O
et	O
al	O
,	O
2018Akbik	O
et	O
al	O
,	O
,	O
2019	O
.	O
The	O
input	O
consists	O
of	O
a	O
concatenation	O
of	O
pretrained	O
word	B-TaskName
embeddings	I-TaskName
and	O
character	O
embeddings	O
.	O
Character	O
embeddings	O
are	O
trained	O
using	O
an	O
LSTM	B-MethodName
from	O
randomly	O
initialized	O
vectors	O
as	O
in	O
(	O
Lample	O
et	O
al	O
,	O
2016	O
)	O
.	O
Word	B-TaskName
embeddings	I-TaskName
are	O
derived	O
from	O
a	O
combination	O
GloVe	B-MethodName
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
and	O
FastText	B-MethodName
(	O
Bojanowski	O
et	O
al	O
,	O
2017	O
)	O
pre	O
-	O
trained	O
word	B-TaskName
embeddings	I-TaskName
,	O
as	O
used	O
in	O
(	O
Ma	O
and	O
Hovy	O
,	O
2016	O
)	O
.	O
The	O
choice	O
of	O
embeddings	O
is	O
orthogonal	O
to	O
the	O
architecture	O
and	O
thus	O
,	O
we	O
hold	O
these	O
constant	O
in	O
all	O
experiments	O
.	O
This	O
representation	O
is	O
passed	O
through	O
two	O
LSTM	B-MethodName
layers	O
that	O
process	O
the	O
input	O
sequence	O
in	O
differ	O
-	O
.	O
The	O
outputs	O
of	O
these	O
layers	O
are	O
concatenated	O
and	O
,	O
in	O
order	O
to	O
map	O
the	O
word	O
representation	O
obtained	O
from	O
the	O
LSTM	B-MethodName
module	O
into	O
the	O
label	O
distribution	O
,	O
passed	O
to	O
a	O
one	O
-	O
layer	O
feed	O
-	O
forward	O
network	O
.	O
A	O
Conditional	B-MethodName
Random	I-MethodName
Field	I-MethodName
is	O
applied	O
to	O
the	O
class	O
predictions	O
to	O
jointly	O
assign	O
the	O
sequence	O
tags	O
using	O
a	O
transition	O
matrix	O
.	O
This	O
CRF	B-MethodName
layer	O
improves	O
performance	O
of	O
the	O
model	O
(	O
Lample	O
et	O
al	O
,	O
2016	O
)	O
as	O
it	O
ensures	O
the	O
output	O
sequence	O
takes	O
into	O
account	O
dependencies	O
between	O
the	O
tags	O
and	O
also	O
models	O
the	O
constraints	O
the	O
output	O
sequence	O
adheres	O
to	O
(	O
e.g.	O
I	O
-	O
PER	O
can	O
not	O
follow	O
B	O
-	O
LOC	O
)	O
.	O

We	O
use	O
a	O
collection	O
of	O
data	O
sets	O
spanning	O
eight	O
genres	O
to	O
evaluate	O
our	O
methods	O
.	O
In	O
addition	O
,	O
in	O
order	O
to	O
test	O
the	O
feasibility	O
of	O
NER	B-TaskName
tagging	O
in	O
a	O
zero	O
-	O
shot	O
domain	O
setup	O
,	O
we	O
present	O
additional	O
data	O
covering	O
four	O
other	O
genres	O
.	O
Each	O
genre	O
of	O
documents	O
is	O
considered	O
a	O
domain	O
in	O
modelling	O
.	O

The	O
data	O
set	O
collection	O
used	O
in	O
learning	O
the	O
multidomain	O
models	O
(	O
denoted	O
as	O
'	O
Open	O
Data	O
'	O
in	O
the	O
rest	O
of	O
the	O
paper	O
)	O
includes	O
the	O
following	O
three	O
data	O
sets	O
:	O
CoNLL	B-DatasetName
2003	I-DatasetName
We	O
use	O
the	O
data	O
set	O
released	O
as	O
part	O
of	O
CoNLL	B-DatasetName
2003	I-DatasetName
shared	O
task	O
for	O
English	O
(	O
Tjong	O
Kim	O
Sang	O
and	O
De	O
Meulder	O
,	O
2003	O
)	O
,	O
which	O
is	O
arguably	O
the	O
most	O
popular	O
data	O
set	O
for	O
NER	B-TaskName
and	O
is	O
regularly	O
used	O
as	O
a	O
benchmark	O
for	O
this	O
task	O
.	O
This	O
data	O
is	O
a	O
collection	O
of	O
news	O
articles	O
from	O
the	O
Reuters	O
Corpus	O
.	O
Twitter	O
The	O
Twitter	O
data	O
set	O
consists	O
of	O
22	O
,	O
000	O
tweets	O
representative	O
of	O
multiple	O
English	O
-	O
speaking	O
locales	O
and	O
a	O
variety	O
of	O
topics	O
that	O
span	O
11	O
years	O
of	O
Twitter	O
posts	O
(	O
2009	O
)	O
(	O
2010	O
)	O
(	O
2011	O
)	O
(	O
2012	O
)	O
(	O
2013	O
)	O
(	O
2014	O
)	O
(	O
2015	O
)	O
(	O
2016	O
)	O
(	O
2017	O
)	O
(	O
2018	O
)	O
(	O
2019	O
)	O
.	O
This	O
data	O
was	O
annotated	O
with	O
Organizations	O
(	O
ORG	O
)	O
,	O
Persons	O
(	O
PER	O
)	O
and	O
Locations	O
(	O
LOC	O
)	O
,	O
using	O
the	O
annotation	O
guidelines	O
used	O
in	O
annotating	O
past	O
data	O
sets	O
(	O
Tjong	O
Kim	O
Sang	O
and	O
De	O
Meulder	O
,	O
2003	O
)	O
supplemented	O
with	O
examples	O
that	O
are	O
specific	O
to	O
Twitter	O
data	O
.	O
OntoNotes	B-DatasetName
(	O
six	O
genres	O
)	O
The	O
OntoNotes	B-DatasetName
data	O
set	O
(	O
Hovy	O
et	O
al	O
,	O
2006	O
)	O
(	O
Augenstein	O
et	O
al	O
,	O
2017	O
)	O
.	O
Zero	O
Shot	O
Genres	O
Finally	O
,	O
for	O
zero	O
-	O
shot	O
genre	O
NER	B-TaskName
,	O
we	O
use	O
a	O
collection	O
of	O
internal	O
data	O
sets	O
from	O
four	O
different	O
genres	O
spanning	O
news	O
,	O
closed	O
captions	O
and	O
other	O
documents	O
.	O
All	O
four	O
genres	O
were	O
annotated	O
with	O
the	O
same	O
entity	O
types	O
and	O
using	O
similar	O
guidelines	O
.	O

In	O
order	O
to	O
present	O
comparable	O
results	O
across	O
all	O
different	O
data	O
sets	O
,	O
we	O
limit	O
our	O
experiments	O
to	O
three	O
different	O
types	O
of	O
entities	O
that	O
are	O
present	O
in	O
all	O
the	O
above	O
data	O
sets	O
and	O
annotated	O
using	O
similar	O
guidelines	O
:	O
organizations	O
(	O
including	O
geo	O
-	O
political	O
entities	O
and	O
facilities	O
)	O
,	O
persons	O
and	O
locations	O
.	O
In	O
case	O
other	O
types	O
of	O
entities	O
exist	O
in	O
the	O
data	O
(	O
e.g.	O
MISC	O
for	O
CoNLL	O
,	O
dates	O
for	O
OntoNotes	B-DatasetName
)	O
,	O
these	O
are	O
considered	O
to	O
be	O
not	O
an	O
entity	O
,	O
similar	O
to	O
(	O
Augenstein	O
et	O
al	O
,	O
2017	O
)	O
.	O
We	O
used	O
the	O
BIO	O
tagging	O
scheme	O
in	O
all	O
our	O
experiments	O
,	O
as	O
this	O
is	O
arguably	O
the	O
most	O
popular	O
and	O
differences	O
in	O
results	O
between	O
this	O
tagging	O
scheme	O
and	O
others	O
,	O
such	O
as	O
the	O
BILOU	O
scheme	O
,	O
are	O
very	O
small	O
in	O
practice	O
(	O
Ratinov	O
and	O
Roth	O
,	O
2009	O
)	O
.	O

We	O
train	O
our	O
models	O
using	O
the	O
open	O
data	O
sets	O
from	O
CoNLL	O
,	O
Twitter	O
and	O
OntoNotes	B-DatasetName
.	O
The	O
training	O
,	O
development	O
and	O
test	O
splits	O
of	O
CoNLL	O
and	O
OntoNotes	B-DatasetName
follows	O
the	O
standard	O
splits	O
.	O
Similarly	O
,	O
we	O
randomly	O
split	O
the	O
Twitter	O
data	O
set	O
randomly	O
into	O
70	O
%	O
for	O
training	O
,	O
10	O
%	O
for	O
development	O
and	O
20	O
%	O
for	O
testing	O
.	O
The	O
final	O
train	O
,	O
dev	O
and	O
test	O
sets	O
are	O
obtained	O
by	O
joining	O
all	O
the	O
respective	O
splits	O
across	O
the	O
individual	O
data	O
sets	O
.	O

We	O
evaluate	O
several	O
baseline	O
methods	O
and	O
other	O
competitive	O
methods	O
introduced	O
in	O
past	O
research	O
and	O
compare	O
to	O
our	O
proposed	O
architecture	O
(	O
MultDomain	O
-	O
SP	O
-	O
Aux	O
)	O
described	O
in	O
Section	O
3.2	O
.	O
These	O
methods	O
focus	O
on	O
different	O
variations	O
of	O
the	O
neural	O
model	O
architecture	O
,	O
while	O
holding	O
the	O
input	O
embeddings	O
constant	O
.	O
InDomain	O
trains	O
an	O
individual	O
NER	B-TaskName
model	O
using	O
the	O
base	O
architecture	O
for	O
each	O
of	O
the	O
known	O
domains	O
.	O
In	O
inference	O
,	O
the	O
corresponding	O
in	O
-	O
domain	O
model	O
is	O
used	O
.	O
This	O
allows	O
us	O
to	O
establish	O
the	O
baseline	O
individual	O
domain	O
performance	O
when	O
no	O
information	O
is	O
shared	O
between	O
the	O
domains	O
in	O
training	O
.	O
InDomain	O
-	O
DomainClassifier	O
uses	O
the	O
same	O
NER	B-TaskName
models	O
as	O
the	O
InDomain	O
model	O
.	O
The	O
In	O
-	O
Domain	O
approach	O
is	O
however	O
unable	O
to	O
directly	O
perform	O
inference	O
on	O
sentences	O
where	O
the	O
domain	O
label	O
is	O
unknown	O
at	O
inference	O
time	O
.	O
We	O
thus	O
build	O
a	O
separate	O
domain	O
classifier	O
using	O
a	O
Bi	O
-	O
LSTM	B-MethodName
recurrent	O
neural	O
network	O
that	O
feeds	O
the	O
final	O
hidden	O
state	O
into	O
a	O
feed	O
-	O
forward	O
network	O
to	O
recognize	O
the	O
domain	O
of	O
a	O
given	O
input	O
sentence	O
and	O
route	O
it	O
to	O
the	O
appropriate	O
InDomain	O
NER	B-TaskName
model	O
.	O
PoolDomain	O
naively	O
pools	O
all	O
available	O
data	O
,	O
disregarding	O
the	O
domain	O
information	O
and	O
trains	O
a	O
model	O
using	O
the	O
base	O
architecture	O
.	O
This	O
model	O
thus	O
ignores	O
the	O
domain	O
information	O
when	O
training	O
,	O
albeit	O
uses	O
all	O
available	O
training	O
data	O
.	O
Data	O
pooling	O
is	O
the	O
standard	O
baseline	O
in	O
most	O
domain	B-TaskName
adaptation	I-TaskName
experiments	O
.	O
PoolDomain	O
-	O
Init	O
uses	O
all	O
available	O
data	O
and	O
uses	O
the	O
domain	O
information	O
to	O
train	O
models	O
on	O
data	O
from	O
one	O
domain	O
at	O
once	O
.	O
After	O
training	O
on	O
data	O
from	O
each	O
domain	O
,	O
the	O
model	O
uses	O
the	O
weights	O
as	O
initialization	O
for	O
training	O
on	O
next	O
domain	O
.	O
This	O
is	O
similar	O
to	O
the	O
INIT	O
strategy	O
for	O
domain	B-TaskName
adaptation	I-TaskName
used	O
in	O
(	O
Mou	O
et	O
al	O
,	O
2016	O
;	O
.	O
We	O
perform	O
this	O
weight	O
initialization	O
and	O
fine	O
-	O
tuning	O
process	O
over	O
all	O
the	O
domains	O
consecutively	O
,	O
where	O
the	O
order	O
is	O
defined	O
by	O
the	O
density	O
of	O
entities	O
,	O
starting	O
with	O
the	O
highest	O
one	O
.	O
PoolDomain	O
-	O
GradRev	O
trains	O
the	O
base	O
architecture	O
using	O
a	O
gradient	O
reversal	O
layer	O
(	O
Ganin	O
and	O
Lempitsky	O
,	O
2014	O
)	O
.	O
The	O
gradient	O
reversal	O
technique	O
aims	O
to	O
confuse	O
the	O
domain	O
discriminator	O
while	O
learning	O
NER	B-TaskName
with	O
the	O
combination	O
of	O
the	O
training	O
data	O
from	O
all	O
domains	O
.	O
PoolDomain+DomainFeat	O
trains	O
a	O
base	O
architecture	O
model	O
over	O
all	O
available	O
data	O
and	O
,	O
in	O
addition	O
to	O
the	O
text	O
-	O
based	O
features	O
,	O
the	O
domain	O
information	O
is	O
explicitly	O
represented	O
by	O
passing	O
it	O
through	O
a	O
domain	O
embedding	O
.	O
This	O
is	O
appended	O
to	O
the	O
word	O
-	O
level	O
features	O
that	O
are	O
used	O
as	O
input	O
to	O
the	O
BiLSTM	B-MethodName
layers	O
.	O
The	O
domain	O
embeddings	O
are	O
randomly	O
initialized	O
.	O
MultDomain	O
-	O
SP	O
extends	O
the	O
MULT	O
method	O
(	O
Yang	O
et	O
al	O
,	O
2017	O
)	O
to	O
the	O
multi	O
-	O
domain	O
setup	O
.	O
This	O
method	O
uses	O
a	O
domain	O
-	O
specific	O
CRF	B-MethodName
for	O
each	O
domain	O
and	O
a	O
shared	O
CRF	B-MethodName
for	O
all	O
domains	O
.	O
Both	O
the	O
BiLSTM	B-MethodName
and	O
the	O
feed	O
-	O
forward	O
layers	O
are	O
shared	O
across	O
all	O
domains	O
.	O
Inference	O
can	O
be	O
done	O
either	O
through	O
the	O
private	O
layer	O
corresponding	O
to	O
the	O
domain	O
of	O
the	O
input	O
-	O
denoted	O
as	O
MultDomain	O
-	O
MultCRF	O
(	O
P	O
)	O
-	O
or	O
through	O
the	O
shared	O
layer	O
-	O
denoted	O
as	O
MultDomain	O
-	O
MultCRF	O
(	O
S	O
)	O
-	O
in	O
which	O
case	O
this	O
can	O
be	O
used	O
when	O
the	O
domain	O
label	O
is	O
unknown	O
in	O
inference	O
.	O

In	O
this	O
section	O
,	O
we	O
present	O
and	O
compare	O
the	O
results	O
of	O
all	O
the	O
methods	O
introduced	O
previously	O
.	O
Experiments	O
are	O
conducted	O
first	O
on	O
the	O
open	O
data	O
collection	O
introduced	O
in	O
Section	O
4.1	O
in	O
the	O
Multi	O
-	O
Domain	O
and	O
Multi	O
-	O
Domain	O
with	O
Unknown	O
Label	O
setups	O
.	O
Following	O
,	O
we	O
evaluate	O
the	O
performance	O
of	O
our	O
model	O
on	O
the	O
data	O
used	O
for	O
zero	O
-	O
shot	O
genre	O
NER	B-TaskName
.	O
The	O
goal	O
of	O
these	O
experiments	O
is	O
to	O
examine	O
the	O
NER	B-TaskName
performance	O
across	O
the	O
three	O
proposed	O
experimental	O
setups	O
which	O
focus	O
on	O
model	O
generalizability	O
across	O
multiple	O
domains	O
.	O
We	O
note	O
that	O
the	O
results	O
below	O
can	O
not	O
be	O
directly	O
compared	O
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
each	O
data	O
set	O
,	O
as	O
we	O
restrict	O
the	O
entity	O
types	O
to	O
PER	O
,	O
ORG	O
,	O
LOC	O
,	O
such	O
that	O
these	O
types	O
are	O
constant	O
across	O
all	O
data	O
sets	O
.	O

We	O
first	O
focus	O
on	O
understanding	O
the	O
impact	O
of	O
each	O
component	O
added	O
to	O
our	O
proposed	O
method	O
over	O
the	O
base	O
architecture	O
through	O
an	O
ablation	O
study	O
.	O
Table	O
4	O
shows	O
results	O
using	O
the	O
private	O
layer	O
(	O
MultDomain	O
-	O
SP	O
-	O
Aux	O
(	O
P	O
)	O
)	O
when	O
each	O
of	O
the	O
three	O
components	O
are	O
alternatively	O
turned	O
off	O
:	O
Shared	O
-	O
Private	O
Linear	B-MethodName
layer	I-MethodName
,	O
Shared	O
-	O
Private	O
CRF	B-MethodName
and	O
the	O
domain	O
prediction	O
auxiliary	O
task	O
.	O
Shared	O
vs.	O
Shared	O
-	O
Private	O
CRF	B-MethodName
With	O
the	O
rest	O
of	O
the	O
architecture	O
fixed	O
,	O
the	O
results	O
show	O
that	O
the	O
shared	O
-	O
private	O
CRF	B-MethodName
performs	O
close	O
to	O
the	O
shared	O
CRF	B-MethodName
when	O
the	O
shared	O
linear	B-MethodName
layer	I-MethodName
is	O
used	O
(	O
80.08	O
vs.	O
80.16	O
;	O
82.04	O
vs.	O
82.74	O
;	O
all	O
comparisons	O
in	O
this	O
section	O
are	O
on	O
macro	O
-	O
average	O
)	O
.	O
However	O
,	O
once	O
we	O
use	O
a	O
separate	O
linear	B-MethodName
layer	I-MethodName
between	O
the	O
BiLSTM	B-MethodName
and	O
each	O
CRF	B-MethodName
,	O
the	O
difference	O
between	O
having	O
the	O
shared	O
and	O
the	O
shared	O
-	O
private	O
CRFs	O
increases	O
drastically	O
(	O
81.36	O
vs.	O
83.11	O
;	O
82.30	O
vs.	O
84.68	O
)	O
.	O
With	O
only	O
this	O
late	O
separation	O
,	O
the	O
inputs	O
to	O
CRF	B-MethodName
decoders	O
are	O
still	O
domain	O
-	O
independent	O
features	O
,	O
which	O
makes	O
it	O
hard	O
for	O
the	O
linear	O
CRF	B-MethodName
to	O
adapt	O
.	O
When	O
the	O
inputs	O
are	O
already	O
domain	O
-	O
dependent	O
,	O
the	O
linear	O
CRF	B-MethodName
can	O
better	O
use	O
this	O
information	O
in	O
performing	O
the	O
joint	O
inference	O
of	O
the	O
sequence	O
.	O
We	O
note	O
that	O
only	O
using	O
shared	O
-	O
private	O
CRF	B-MethodName
with	O
the	O
base	O
architecture	O
is	O
equivalent	O
to	O
the	O
MultDomain	O
-	O
SP	O
method	O
(	O
Yang	O
et	O
al	O
,	O
2017	O
)	O
.	O

The	O
results	O
show	O
that	O
regardless	O
of	O
the	O
other	O
parameters	O
,	O
adding	O
shared	O
and	O
private	O
linear	O
layers	O
between	O
the	O
BiLSTM	B-MethodName
layers	O
and	O
the	O
CRF	B-MethodName
(	O
s	O
)	O
is	O
always	O
beneficial	O
(	O
80.08	O
vs.	O
81.36	O
;	O
80.16	O
vs.	O
83.11	O
;	O
82.04	O
vs.	O
82.30	O
;	O
82.74	O
vs.	O
84.68	O
)	O
.	O
The	O
improvements	O
are	O
relatively	O
larger	O
when	O
combined	O
with	O
shared	O
and	O
private	O
CRF	B-MethodName
,	O
as	O
previously	O
seen	O
.	O
Multi	B-TaskName
-	I-TaskName
Task	I-TaskName
Learning	I-TaskName
of	O
Domain	O
Labels	O
Finally	O
,	O
we	O
compare	O
the	O
impact	O
of	O
adding	O
the	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
objective	O
.	O
We	O
find	O
that	O
,	O
similar	O
to	O
the	O
linear	O
layers	O
,	O
adding	O
the	O
domain	O
prediction	O
task	O
is	O
always	O
beneficial	O
for	O
the	O
model	O
with	O
the	O
increase	O
being	O
larger	O
if	O
is	O
only	O
a	O
shared	O
linear	B-MethodName
layer	I-MethodName
.	O
We	O
expect	O
that	O
the	O
two	O
tasks	O
at	O
different	O
levels	O
of	O
granularity	O
rely	O
on	O
shared	O
structure	O
in	O
the	O
original	O
semantic	O
space	O
.	O
The	O
document	O
-	O
level	O
domain	O
labels	O
can	O
help	O
regularize	O
the	O
training	O
,	O
providing	O
generic	O
information	O
about	O
which	O
low	O
-	O
level	O
features	O
are	O
valuable	O
to	O
entity	O
-	O
level	O
recognition	O
.	O

We	O
further	O
study	O
the	O
domains	O
that	O
are	O
selected	O
by	O
the	O
methods	O
above	O
by	O
creating	O
confusion	O
matrices	O
between	O
the	O
domain	O
predictions	O
of	O
three	O
setups	O
:	O
domain	O
classification	O
,	O
domain	O
prediction	O
in	O
the	O
proposed	O
MultDomain	O
-	O
SP	O
-	O
Aux	O
model	O
and	O
the	O
oracle	O
in	O
-	O
domain	O
choice	O
on	O
gold	O
data	O
.	O
Figure	O
2	O
shows	O
that	O
the	O
Oracle	O
model	O
relies	O
on	O
the	O
corresponding	O
InDomain	O
model	O
to	O
only	O
a	O
limited	O
extent	O
for	O
each	O
model	O
.	O
In	O
uniformly	O
many	O
cases	O
,	O
predictions	O
from	O
other	O
in	O
-	O
domain	O
models	O
are	O
better	O
than	O
the	O
existing	O
in	O
-	O
domain	O
one	O
,	O
showing	O
the	O
variability	O
of	O
the	O
NER	B-TaskName
models	O
.	O
The	O
domain	O
classifier	O
predictions	O
align	O
closer	O
to	O
the	O
actual	O
domains	O
.	O
The	O
MultDomain	O
-	O
SP	O
-	O
Aux	O
model	O
also	O
tends	O
to	O
predict	O
the	O
domain	O
correctly	O
,	O
but	O
we	O
see	O
that	O
it	O
better	O
learns	O
the	O
NW	O
,	O
WB	O
and	O
BN	O
domains	O
.	O
Note	O
noting	O
that	O
the	O
MultDomain	O
-	O
SP	O
-	O
Aux	O
model	O
does	O
not	O
use	O
these	O
domain	O
predictions	O
in	O
inference	O
and	O
the	O
model	O
uses	O
the	O
shared	O
components	O
for	O
unknown	O
domains	O
or	O

Analysis	O
of	O
Zero	O
-	O
Shot	O
Crosslingual	O
Learning	O
between	O
English	O
and	O
Korean	O
for	O
Named	B-TaskName
Entity	I-TaskName
Recognition	I-TaskName

Crosslingual	O
representation	B-TaskName
learning	I-TaskName
aims	O
to	O
derive	O
embeddings	O
for	O
words	O
(	O
or	O
sentences	O
)	O
from	O
multiple	O
languages	O
that	O
can	O
be	O
projected	O
into	O
a	O
shared	O
vector	O
space	O
(	O
Conneau	O
et	O
al	O
,	O
2018	O
;	O
Schuster	O
et	O
al	O
,	O
2019b	O
;	O
Conneau	O
and	O
Lample	O
,	O
2019	O
)	O
.	O
One	O
important	O
application	O
of	O
crosslingual	O
embeddings	O
has	O
been	O
found	O
for	O
transferring	O
models	O
trained	O
on	O
a	O
high	O
-	O
resource	O
language	O
to	O
a	O
low	O
-	O
resource	O
one	O
(	O
Lin	O
et	O
al	O
,	O
2019	O
;	O
Schuster	O
et	O
al	O
,	O
2019a	O
;	O
Artetxe	O
and	O
Schwenk	O
,	O
2019	O
)	O
.	O
The	O
latest	O
multilingual	O
transformer	O
encoders	O
such	O
as	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
and	O
XLM	B-MethodName
(	O
Conneau	O
et	O
al	O
,	O
2020	O
)	O
have	O
made	O
it	O
possible	O
to	O
develop	O
robust	O
crosslingual	O
models	O
through	O
zero	B-TaskName
-	I-TaskName
shot	I-TaskName
learning	I-TaskName
that	O
requires	O
no	O
labeled	O
training	O
data	O
on	O
the	O
target	O
side	O
(	O
Jebbara	O
and	O
Cimiano	O
,	O
2019	O
;	O
Chidambaram	O
et	O
al	O
,	O
2019	O
;	O
Chi	O
et	O
al	O
,	O
2020	O
)	O
.	O
However	O
,	O
these	O
approaches	O
tend	O
not	O
to	O
work	O
as	O
well	O
for	O
languages	O
whose	O
words	O
can	O
not	O
be	O
easily	O
aligned	O
.	O
Our	O
team	O
is	O
motivated	O
to	O
create	O
a	O
rich	O
crosslingual	O
resource	O
between	O
English	O
and	O
Korean	O
,	O
which	O
are	O
largely	O
different	O
in	O
nature	O
as	O
English	O
is	O
known	O
to	O
be	O
rigid	O
-	O
order	O
,	O
morphologically	O
-	O
poor	O
,	O
and	O
head	O
-	O
initial	O
whereas	O
Korean	O
is	O
flexible	O
-	O
order	O
,	O
morphologicallyrich	O
,	O
and	O
head	O
-	O
final	O
(	O
Choi	O
et	O
al	O
,	O
1994	O
;	O
Han	O
et	O
al	O
,	O
2002	O
;	O
Hong	O
,	O
2009	O
)	O
.	O
Creation	O
of	O
a	O
high	O
quality	O
parallel	O
dataset	O
to	O
facilitate	O
crosslingual	O
research	O
can	O
reduce	O
the	O
gap	O
between	O
these	O
two	O
languages	O
,	O
and	O
advance	O
NLP	O
techniques	O
in	O
both	O
languages	O
.	O
This	O
paper	O
provides	O
a	O
comprehensive	O
analysis	O
of	O
crosslingual	O
zero	B-TaskName
-	I-TaskName
shot	I-TaskName
learning	I-TaskName
in	O
English	O
and	O
Korean	O
.	O
We	O
first	O
create	O
a	O
new	O
dataset	O
comprising	O
a	O
large	O
number	O
of	O
parallel	O
sentences	O
and	O
annotate	O
them	O
for	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
;	O
Sec	O
.	O
3	O
)	O
.	O
We	O
then	O
adapt	O
the	O
crosslingual	O
approaches	O
and	O
build	O
NER	B-TaskName
models	O
in	O
Korean	O
through	O
zeroshot	O
learning	O
(	O
Sec	O
.	O
4	O
)	O
.	O
All	O
models	O
are	O
experimented	O
on	O
our	O
dataset	O
and	O
thoroughly	O
compared	O
to	O
evaluate	O
the	O
feasibility	O
of	O
this	O
work	O
(	O
Sec	O
.	O
5	O
)	O
.	O
Our	O
results	O
are	O
promising	O
although	O
depicting	O
few	O
challenges	O
in	O
zero	B-TaskName
-	I-TaskName
shot	I-TaskName
learning	I-TaskName
for	O
English	O
and	O
Korean	O
(	O
Sec	O
.	O
6	O
)	O
.	O
The	O
contributions	O
of	O
this	O
work	O
can	O
be	O
summarized	O
as	O
follows	O
:	O
To	O
create	O
a	O
crosslingual	O
dataset	O
that	O
enables	O
to	O
develop	O
robust	O
zero	O
-	O
shot	O
NER	B-TaskName
models	O
in	O
Korean	O
.	O
To	O
present	O
a	O
new	O
data	O
selection	O
scheme	O
that	O
can	O
notably	O
improve	O
zero	O
-	O
shot	O
model	O
performance	O
.	O
To	O
provide	O
a	O
comparative	O
analysis	O
among	O
several	O
crosslingual	O
approaches	O
and	O
establish	O
the	O
initial	O
foundation	O
of	O
this	O
research	O
.	O

For	O
crosslingual	O
representation	O
alignment	O
,	O
Artetxe	O
et	O
al	O
(	O
2016	O
)	O
and	O
Smith	O
et	O
al	O
(	O
2017	O
)	O
suggested	O
orthogonality	O
constraints	O
on	O
the	O
embedding	O
transformation	O
that	O
led	O
to	O
better	O
quality	O
translation	O
.	O
Aldarmaki	O
and	O
Diab	O
(	O
2019	O
)	O
derived	O
a	O
context	O
-	O
aware	O
crosslingual	O
mapping	O
from	O
a	O
parallel	O
corpus	O
using	O
word	B-TaskName
alignment	I-TaskName
.	O
Schuster	O
et	O
al	O
(	O
2019b	O
)	O
aligned	O
word	B-TaskName
embeddings	I-TaskName
from	O
multilingual	O
transformer	O
encoders	O
using	O
context	O
independent	O
embedding	O
anchors	O
.	O
Recent	O
works	O
based	O
on	O
multilingual	O
pretrained	O
language	O
model	O
aligns	O
representations	O
between	O
languages	O
in	O
a	O
unsupervised	O
fashion	O
.	O
Devlin	O
et	O
al	O
(	O
2019	O
)	O
proposed	O
multilingual	O
BERT	B-MethodName
that	O
generates	O
contextualized	O
word	B-TaskName
embeddings	I-TaskName
for	O
multiple	O
languages	O
in	O
one	O
vector	O
space	O
by	O
simply	O
sharing	O
all	O
languages	O
'	O
vocabulary	O
.	O
Conneau	O
and	O
Lample	O
(	O
2019	O
)	O
extends	O
mBERT	B-MethodName
by	O
introducing	O
bilingual	O
data	O
and	O
an	O
extra	O
pretraining	O
task	O
(	O
Translation	B-TaskName
Language	O
Modeling	O
)	O
.	O
Luo	O
et	O
al	O
(	O
2021	O
)	O
adds	O
a	O
crossattention	O
module	O
into	O
the	O
Transformer	B-MethodName
encoder	O
to	O
explicitly	O
build	O
the	O
interdependence	O
between	O
langauges	O
.	O
For	O
cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
NER	I-TaskName
,	O
Ni	O
et	O
al	O
(	O
2017	O
)	O
presented	O
weakly	O
supervised	O
crosslingual	O
models	O
using	O
annotation	O
and	O
representation	O
projection	O
.	O
Huang	O
et	O
al	O
(	O
2019	O
)	O
made	O
an	O
empirical	O
analysis	O
of	O
how	O
sequential	O
order	O
and	O
multilingual	O
embeddings	O
are	O
used	O
in	O
crosslingual	O
NER	B-TaskName
.	O
Artetxe	O
and	O
Schwenk	O
(	O
2019	O
)	O
presented	O
multilingual	O
transfer	O
models	O
that	O
used	O
few	B-TaskName
-	I-TaskName
shot	I-TaskName
learning	I-TaskName
adapting	O
supervising	O
BEA	O
,	O
ranking	O
and	O
retraining	O
for	O
massive	O
transfer	O
.	O
Wu	O
and	O
Dredze	O
(	O
2019	O
)	O
and	O
Wu	O
et	O
al	O
(	O
2020	O
)	O
directly	O
transfers	O
the	O
NER	B-TaskName
model	O
trained	O
on	O
the	O
source	O
language	O
to	O
the	O
target	O
language	O
using	O
crosslingual	O
representations	O
from	O
multilingual	O
encoders	O
(	O
Direct	O
model	O
transfer	O
)	O
.	O
3	O
English	O
-	O
Korean	O
Crosslingual	O
Dataset	O
3.1	O
Data	O
Collection	O
AI	O
Open	O
Innovation	O
Hub	O
(	O
AI	O
Hub	O
)	O
is	O
an	O
integration	O
platform	O
operated	O
by	O
the	O
Korea	O
National	O
Information	O
Society	O
Agency	O
that	O
provides	O
data	O
,	O
software	O
,	O
and	O
computing	O
resources	O
for	O
AI	O
research	O
.	O
It	O
has	O
released	O
the	O
Korean	O
-	O
English	O
AI	O
Training	O
Text	O
Corpus	O
(	O
KEAT	O
)	O
1	O
containing	O
1.6	O
M	O
English	O
-	O
Korean	O
parallel	O
sentences	O
from	O
various	O
sources	O
such	O
as	O
news	O
media	O
,	O
government	O
website	O
/	O
journal	O
,	O
law	O
&	O
administration	O
,	O
conversation	O
and	O
etc	O
.	O
For	O
the	O
present	O
study	O
,	O
800	O
K	O
parallel	O
sentences	O
from	O
the	O
news	O
portion	O
of	O
this	O
corpus	O
are	O
extracted	O
.	O

Since	O
KEAT	O
is	O
not	O
organized	O
into	O
documents	O
,	O
each	O
sentence	O
is	O
composed	O
independently	O
although	O
it	O
comes	O
with	O
the	O
URL	O
of	O
its	O
original	O
source	O
.	O
Thus	O
,	O
we	O
group	O
all	O
sentences	O
into	O
news	O
articles	O
based	O
on	O
the	O
URLs	O
.	O
Although	O
there	O
exist	O
news	O
articles	O
with	O
single	O
sentence	O
after	O
the	O
grouping	O
process	O
,	O
we	O
still	O
include	O
them	O
in	O
the	O
train	O
set	O
in	O
order	O
to	O
make	O
full	O
use	O
of	O
the	O
parallel	O
sentences	O
provided	O
,	O
which	O
will	O
be	O
used	O
to	O
train	O
the	O
word	B-TaskName
alignment	I-TaskName
model	O
and	O
the	O
transformation	O
matrix	O
in	O
Section	O
5	O
.	O
As	O
a	O
result	O
,	O
757	O
,	O
697	O
sentences	O
are	O
selected	O
,	O
that	O
are	O
composed	O
into	O
381	O
,	O
173	O
news	O
articles	O
,	O
to	O
create	O
our	O
English	O
-	O
Korean	O
crosslingual	O
dataset	O
.	O
The	O
news	O
articles	O
can	O
be	O
categorized	O
into	O
9	O
sections	O
:	O
Business	O
,	O
Lifestyle	O
,	O
Science	O
/	O
Technology	O
,	O
Society	O
,	O
Sports	O
,	O
World	O
,	O
Regional	O
,	O
and	O
Others	O
.	O
Among	O
those	O
,	O
200	O
articles	O
are	O
randomly	O
sampled	O
from	O
each	O
of	O
the	O
first	O
7	O
categories	O
for	O
our	O
annotation	O
in	O
Section	O
3.4	O
and	O
they	O
are	O
split	O
into	O
50/50	O
to	O
create	O
the	O
development	O
and	O
test	O
sets	O
for	O
our	O
experiments	O
in	O
Section	O
5	O
.	O
Table	O
1	O
describes	O
the	O
statistics	O
of	O
our	O
dataset	O
.	O
All	O
sections	O
are	O
uniformly	O
distributed	O
in	O
DEV	O
and	O
TST	O
,	O
enabling	O
to	O
conduct	O
comparative	O
studies	O
among	O
these	O
sections	O
.	O
Table	O
3	O
:	O
The	O
statistics	O
of	O
manually	O
annotated	O
named	O
entities	O
on	O
the	O
parallel	O
sentences	O
in	O
the	O
DEV	O
and	O
TST	O
sets	O
.	O
The	O
numbers	O
in	O
the	O
parentheses	O
indicate	O
the	O
percentages	O
of	O
the	O
corresponding	O
tags	O
for	O
each	O
set	O
.	O
EN	O
/	O
KR	O
:	O
#	O
of	O
entities	O
in	O
the	O
English	O
/	O
Korean	O
sentences	O
respectively	O
,	O
E	O
∩	O
K	O
:	O
#	O
of	O
entities	O
existing	O
in	O
both	O
English	O
and	O
Korean	O
sentences	O
.	O

ELIT	O
2	O
using	O
the	O
Flair	O
model	O
trained	O
on	O
OntoNotes	B-DatasetName
(	O
Pradhan	O
et	O
al	O
,	O
2013	O
)	O
.	O
Korean	O
sentences	O
are	O
tagged	O
by	O
a	O
CRF	B-MethodName
-	O
based	O
model	O
adapting	O
KoBERT	O
(	O
Korean	O
BERT	B-MethodName
)	O
3	O
trained	O
on	O
the	O
corpus	O
distributed	O
by	O
Cheon	O
and	O
Kim	O
(	O
2018	O
)	O
.	O
Note	O
that	O
the	O
named	O
entity	O
types	O
pseudo	O
-	O
annotated	O
on	O
the	O
Korean	O
sentences	O
do	O
n't	O
match	O
with	O
those	O
of	O
the	O
English	O
sentences	O
for	O
now	O
,	O
which	O
will	O
be	O
matched	O
in	O
Section	O
3.4	O
in	O
the	O
case	O
of	O
DEV	O
and	O
TST	O
.	O
In	O
addition	O
,	O
Korean	O
sentences	O
are	O
processed	O
by	O
the	O
Mecab	O
morphological	O
analyzer	O
4	O
that	O
produces	O
more	O
linguistically	O
sounding	O
tokens	O
than	O
SentencePiece	B-MethodName
(	O
Kudo	O
and	O
Richardson	O
,	O
2018	O
)	O
in	O
KoBERT	O
.	O
All	O
named	O
entities	O
from	O
the	O
CRF	B-MethodName
tagger	O
are	O
then	O
remapped	O
to	O
the	O
tokens	O
produced	O
by	O
the	O
Mecab	O
analyzer	O
using	O
heuristics	O
so	O
they	O
can	O
better	O
reflect	O
the	O
previous	O
morphology	O
work	O
in	O
Korean	O
(	O
Hong	O
,	O
2009	O
)	O
.	O
Words	O
in	O
every	O
parallel	O
sentence	O
pair	O
,	O
tokenized	O
by	O
the	O
ELIT	O
and	O
Mecab	O
analyzers	O
,	O
are	O
aligned	O
by	O
GIZA++	O
,	O
that	O
has	O
been	O
adapted	O
by	O
many	O
prior	O
crosslingual	O
studies	O
(	O
Och	O
and	O
Ney	O
,	O
2003	O
)	O
.	O
Table	O
2	O
shows	O
the	O
statistics	O
of	O
pseudo	O
-	O
annotated	O
named	O
entities	O
in	O
our	O
dataset	O
.	O
The	O
detailed	O
descriptions	O
of	O
these	O
tags	O
are	O
provided	O
in	O
Appendix	O
A.1	O
.	O
The	O
overall	O
statistics	O
are	O
comparable	O
between	O
English	O
and	O
Korean	O
,	O
2.5	O
and	O
2.3	O
entities	O
per	O
sentence	O
,	O
respectively	O
.	O
GPE	O
e	O
,	O
the	O
3rd	O
most	O
frequent	O
tag	O
in	O
English	O
,	O
is	O
not	O
supported	O
by	O
the	O
Korean	O
tagger	O
but	O
rather	O
tagged	O
as	O
ORG	O
k	O
or	O
LOC	O
k	O
,	O
explaining	O
why	O
the	O
numbers	O
of	O
these	O
two	O
tags	O
in	O
Korean	O
are	O
much	O
greater	O
than	O
those	O
of	O
ORG	O
e	O
and	O
LOC	O
e	O
,	O
respectively	O
.	O

Three	O
crosslingual	O
learning	O
approaches	O
are	O
adapted	O
to	O
develop	O
zero	O
-	O
shot	O
Korean	O
models	O
.	O
One	O
is	O
direct	O
model	O
transfer	O
method	O
following	O
Wu	O
and	O
Dredze	O
(	O
2019	O
)	O
.	O
We	O
reproduce	O
the	O
previous	O
work	O
which	O
finetunes	O
mBERT	B-MethodName
on	O
English	O
NER	B-TaskName
dataset	O
and	O
transfers	O
the	O
trained	O
model	O
to	O
a	O
target	O
language	O
,	O
in	O
our	O
case	O
,	O
Korean	O
.	O
We	O
fine	O
-	O
tune	O
on	O
OntoNotes	B-DatasetName
,	O
whereas	O
the	O
previous	O
work	O
fine	O
-	O
tuned	O
on	O
CoNLL	B-DatasetName
2003	I-DatasetName
NER	B-TaskName
dataset	O
.	O
The	O
other	O
two	O
approaches	O
that	O
will	O
be	O
experimented	O
are	O
embedding	O
preojection	O
and	O
annotation	O
projection	O
following	O
Ni	O
et	O
al	O
(	O
2017	O
)	O
,	O
although	O
some	O
modules	O
in	O
the	O
implementation	O
are	O
updated	O
or	O
added	O
:	O
the	O
encoders	O
used	O
to	O
derive	O
the	O
embeddings	O
from	O
sentences	O
,	O
the	O
word	B-TaskName
alignment	I-TaskName
tool	O
,	O
the	O
training	O
data	O
selection	O
scheme	O
heuristics	O
.	O
Figure	O
1	O
illustrates	O
an	O
overview	O
of	O
two	O
crosslingual	O
learning	O
approaches	O
adapted	O
to	O
develop	O
zero	O
-	O
shot	O
Korean	O
NER	B-TaskName
models	O
.	O
One	O
is	O
embedding	O
projection	O
(	O
R1	O
)	O
that	O
takes	O
a	O
labeled	O
English	O
sentence	O
(	O
R2	O
)	O
and	O
generates	O
English	O
embeddings	O
,	O
(	O
R3	O
)	O
which	O
are	O
fed	O
into	O
an	O
orthogonal	O
mapping	O
(	O
R4	O
)	O
then	O
transformed	O
into	O
Korean	O
embeddings	O
(	O
Section	O
4.2	O
)	O
.	O
The	O
other	O
is	O
annotation	O
projection	O
(	O
A1	O
)	O
that	O
aligns	O
words	O
across	O
the	O
two	O
languages	O
and	O
pseudo	O
-	O
annotates	O
the	O
Korean	O
sentence	O
,	O
(	O
A2	O
)	O
which	O
are	O
fed	O
into	O
an	O
encoder	O
(	O
A3	O
)	O
to	O
generate	O
Korean	O
embeddings	O
(	O
Section	O
4.3	O
)	O
.	O
The	O
Korean	O
embeddings	O
generated	O
by	O
individual	O
approaches	O
are	O
fed	O
into	O
a	O
trainer	O
to	O
build	O
the	O
Korean	O
NER	B-TaskName
models	O
.	O
No	O
manual	O
annotation	O
is	O
added	O
to	O
the	O
Korean	O
data	O
;	O
thus	O
they	O
both	O
are	O
zero	B-TaskName
-	I-TaskName
shot	I-TaskName
learning	I-TaskName
.	O

Let	O
X	O
,	O
Y	O
R	O
n×d	O
be	O
parallel	O
matrices	O
between	O
the	O
source	O
and	O
target	O
languages	O
,	O
where	O
n	O
is	O
the	O
number	O
of	O
parallel	O
terms	O
(	O
words	O
or	O
sentences	O
)	O
in	O
those	O
two	O
languages	O
.	O
Let	O
x	O
i	O
,	O
y	O
i	O
R	O
1×d	O
be	O
the	O
i'th	O
rows	O
in	O
X	O
and	O
Y	O
,	O
which	O
are	O
the	O
embeddings	O
of	O
the	O
i'th	O
terms	O
in	O
the	O
source	O
and	O
target	O
languages	O
respectively	O
,	O
that	O
refer	O
to	O
the	O
same	O
content	O
.	O
Then	O
,	O
the	O
transformation	O
matrix	O
W	O
R	O
d×d	O
can	O
be	O
found	O
by	O
minimizing	O
the	O
distance	O
between	O
XW	O
and	O
Y	O
as	O
follows	O
:	O
argmin	O
W	O
XW	O
−	O
Y	O
s.t	O
.	O
W	O
T	O
W	O
=	O
I	O
This	O
optimization	O
can	O
be	O
achieved	O
by	O
singular	O
value	O
decomposition	O
as	O
proposed	O
by	O
Artetxe	O
et	O
al	O
(	O
2016	O
)	O
,	O
where	O
U	O
,	O
V	O
R	O
d×p	O
,	O
Σ	O
R	O
p×p	O
:	O
W	O
=	O
U	O
V	O
T	O
s.t	O
.	O
X	O
T	O
Y	O
=	O
U	O
ΣV	O
T	O
The	O
transformation	O
matrix	O
W	O
is	O
used	O
to	O
convert	O
any	O
English	O
embedding	O
e	O
i	O
into	O
a	O
Korean	O
embedding	O
k	O
i	O
in	O
Figure	O
1	O
such	O
that	O
e	O
i	O
W	O
=	O
k	O
i	O
≈	O
k	O
j	O
where	O
k	O
j	O
is	O
the	O
embedding	O
from	O
the	O
Korean	O
encoder	O
that	O
can	O
be	O
aligned	O
with	O
e	O
i	O
.	O
The	O
NER	B-TaskName
model	O
is	O
trained	O
on	O
only	O
English	O
sentences	O
represented	O
by	O
the	O
transformed	O
embeddings	O
k	O
*	O
and	O
a	O
pseudo	O
-	O
label	O
annotated	O
with	O
an	O
existing	O
English	O
NER	B-TaskName
model	O
.	O
During	O
decoding	O
,	O
the	O
model	O
takes	O
Korean	O
sentences	O
represented	O
by	O
the	O
encoded	O
embeddings	O
k	O
*	O
and	O
makes	O
the	O
predictions	O
.	O
Given	O
the	O
latest	O
contextualized	O
encoders	O
that	O
generate	O
different	O
embeddings	O
for	O
the	O
same	O
word	O
type	O
by	O
contexts	O
(	O
Peters	O
et	O
al	O
,	O
2018	O
;	O
Devlin	O
et	O
al	O
,	O
2019	O
;	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
,	O
the	O
size	O
of	O
X	O
and	O
Y	O
is	O
as	O
large	O
as	O
the	O
number	O
of	O
all	O
aligned	O
words	O
in	O
the	O
training	O
data	O
.	O
It	O
is	O
worth	O
saying	O
that	O
the	O
transformed	O
embedding	O
space	O
may	O
be	O
similar	O
to	O
the	O
actual	O
encoded	O
space	O
in	O
the	O
target	O
language	O
;	O
however	O
,	O
the	O
word	O
order	O
is	O
still	O
preserved	O
as	O
in	O
the	O
source	O
language	O
.	O
Therefore	O
,	O
the	O
model	O
is	O
limited	O
to	O
learn	O
sequence	O
information	O
of	O
the	O
target	O
language	O
,	O
which	O
can	O
be	O
an	O
issue	O
for	O
languages	O
with	O
very	O
different	O
word	O
orderings	O
.	O

For	O
embedding	O
projection	O
and	O
annotation	O
projection	O
,	O
two	O
types	O
of	O
transformer	O
encoders	O
,	O
mBERT	B-MethodName
(	O
Devlin	O
et	O
al	O
2019	O
)	O

Two	O
types	O
of	O
transformation	O
matrices	O
are	O
derived	O
by	O
the	O
embedding	O
projection	O
method	O
(	O
Section	O
4.2	O
)	O
.	O
One	O
is	O
a	O
word	O
-	O
level	O
matrix	O
and	O
the	O
other	O
is	O
a	O
sentence	O
-	O
level	O
matrix	O
.	O
To	O
evaluate	O
the	O
zero	O
-	O
shot	O
Korean	O
NER	B-TaskName
model	O
performance	O
(	O
Table	O
6	O
)	O
when	O
different	O
size	O
of	O
parallel	O
sentences	O
are	O
available	O
,	O
we	O
use	O
different	O
subsets	O
of	O
sentences	O
of	O
increasing	O
sizes	O
(	O
0	B-DatasetName
,	O
1	O
K	O
,	O
10	O
K	O
,	O
100	O
K	O
,	O
200	O
K	O
,	O
400	O
K	O
,	O
747	O
K	O
;	O
0	B-DatasetName
to	O
total	O
#	O
of	O
sentences	O
in	O
TRN	B-DatasetName
)	O
.	O
Size	O
0	B-DatasetName
means	O
the	O
embeddings	O
from	O
source	O
language	O
are	O
not	O
transformed	O
when	O
fed	O
into	O
the	O
NER	B-TaskName
model	O
for	O
training	O
.	O
Word	B-TaskName
embeddings	I-TaskName
from	O
the	O
last	O
hidden	O
layer	O
of	O
each	O
transformer	O
encoder	O
are	O
extracted	O
.	O
For	O
every	O
parallel	O
sentence	O
pair	O
,	O
let	O
X	O
i	O
and	O
Y	O
i	O
be	O
lists	O
of	O
word	B-TaskName
embeddings	I-TaskName
of	O
the	O
i'th	O
sentence	O
extracted	O
from	O
the	O
last	O
layer	O
in	O
the	O
source	O
and	O
target	O
encoders	O
,	O
respectively	O
.	O
Only	O
embeddings	O
for	O
words	O
that	O
find	O
alignments	O
are	O
included	O
in	O
X	O
i	O
and	O
Y	O
i	O
.	O
If	O
multiple	O
words	O
in	O
the	O
source	O
language	O
,	O
s	O
i	O
and	O
s	O
j	O
,	O
are	O
aligned	O
to	O
one	O
word	O
,	O
t	O
k	O
,	O
in	O
the	O
target	O
language	O
(	O
e.g.	O
,	O
United	O
States	O
미국	O
in	O
Figure	O
1	O
)	O
,	O
the	O
embeddings	O
of	O
t	O
k	O
are	O
duplicated	O
and	O
added	O
to	O
Y	O
i	O
and	O
vice	O
versa	O
s.t	O
.	O
|	O
X	O
*	O
|	O
=	O
|	O
Y	O
*	O
|	O
.	O
Let	O
x	O
ij	O
and	O
y	O
ij	O
be	O
the	O
j'th	O
embeddings	O
in	O
X	O
i	O
and	O
Y	O
i	O
that	O
are	O
guaranteed	O
to	O
be	O
the	O
embeddings	O
of	O
aligned	O
words	O
;	O
thus	O
,	O
X	O
i	O
and	O
Y	O
i	O
are	O
completely	O
in	O
parallel	O
.	O
For	O
the	O
word	O
-	O
level	O
transformation	O
matrix	O
W	O
w	O
,	O
X	O
i	O
and	O
Y	O
i	O
from	O
all	O
parallel	O
sentences	O
are	O
appended	O
together	O
to	O
create	O
X	O
w	O
i	O
and	O
Y	O
w	O
i	O
respectively	O
such	O
that	O
X	O
w	O
i	O
W	O
w	O
≈	O
Y	O
w	O
i	O
.	O
Sentence	B-TaskName
embeddings	I-TaskName
are	O
simply	O
created	O
by	O
averaging	O
the	O
word	B-TaskName
embeddings	I-TaskName
of	O
parallel	O
source	O
and	O
target	O
sentences	O
.	O
Let	O
X	O
i	O
and	O
Y	O
i	O
be	O
lists	O
of	O
word	B-TaskName
embeddings	I-TaskName
of	O
the	O
i'th	O
sentence	O
extracted	O
from	O
the	O
last	O
layer	O
in	O
the	O
source	O
and	O
target	O
encoders	O
.	O
Note	O
that	O
words	O
in	O
X	O
i	O
and	O
Y	O
i	O
are	O
not	O
aligned	O
,	O
thus	O
no	O
duplications	O
of	O
word	O
embedding	O
unlike	O
X	O
i	O
and	O
Y	O
i	O
.	O
For	O
the	O
sentence	O
-	O
level	O
matrix	O
W	O
s	O
,	O
the	O
average	O
em	O
-	O
beddings	O
of	O
X	O
i	O
,	O
Y	O
i	O
are	O
appended	O
to	O
create	O
X	O
s	O
i	O
and	O
Y	O
s	O
i	O
such	O
that	O
X	O
s	O
i	O
W	O
s	O
≈	O
Y	O
s	O
i	O
.	O
For	O
each	O
sentence	O
in	O
the	O
source	O
language	O
,	O
embeddings	O
from	O
last	O
hidden	O
layer	O
are	O
transformed	O
by	O
W	O
w	O
|	O
s	O
and	O
fed	O
into	O
the	O
NER	B-TaskName
model	O
for	O
training	O
(	O
Section	O
5.5	O
)	O
.	O

For	O
embedding	O
and	O
annotation	O
projections	O
,	O
a	O
bidirectional	B-MethodName
LSTM	I-MethodName
-	O
based	O
NER	B-TaskName
tagger	O
using	O
a	O
CRF	B-MethodName
decoder	O
is	O
adapted	O
to	O
build	O
our	O
NER	B-TaskName
models	O
(	O
Lample	O
et	O
al	O
,	O
2016	O
)	O
.	O
Details	O
of	O
the	O
hyperparameters	O
are	O
described	O
in	O
Appendix	O
A.3	O

Given	O
the	O
results	O
of	O
the	O
best	O
models	O
for	O
embedding	O
and	O
annotation	O
projection	O
approaches	O
(	O
Section	O
5.6	O
)	O
,	O
a	O
total	O
of	O
105	O
parallel	O
sentences	O
(	O
15	O
pairs	O
per	O
news	O
section	O
)	O
are	O
randomly	O
selected	O
for	O
error	O
analysis	O
.	O
7	O
Table	O
8	O
shows	O
the	O
distributions	O
of	O
the	O
5	O
error	O
types	O
.	O
error	O
types	O
in	O
both	O
models	O
.	O
For	O
example	O
,	O
the	O
Korean	O
entity	O
"	O
10개	O
(	O
10	O
things	O
)	O
"	O
comprises	O
the	O
quantity	O
"	O
10	O
"	O
and	O
the	O
metric	O
"	O
개	O
(	O
things	O
)	O
"	O
that	O
is	O
a	O
generic	O
measure	O
word	O
in	O
Korean	O
,	O
whereas	O
in	O
English	O
just	O
write	O
"	O
10	O
"	O
.	O
The	O
grammatical	O
difference	O
between	O
English	O
and	O
Korean	O
,	O
where	O
Korean	O
uses	O
measure	O
words	O
for	O
quantifying	O
the	O
classes	O
of	O
objects	O
while	O
English	O
does	O
not	O
in	O
general	O
,	O
makes	O
it	O
difficult	O
to	O
accurately	O
predict	O
under	O
the	O
zero	B-TaskName
-	I-TaskName
shot	I-TaskName
learning	I-TaskName
setting	O
.	O
Wrong	O
Label	O
occurs	O
frequently	O
across	O
all	O
models	O
when	O
dealing	O
with	O
entities	O
referring	O
to	O
nationality	O
.	O
As	O
mentioned	O
in	O
Section	O
3.5	O
,	O
a	O
single	O
word	O
in	O
Korean	O
can	O
entail	O
the	O
meaning	O
of	O
both	O
nationality	O
and	O
country	O
.	O
This	O
overloaded	O
word	O
-	O
sense	O
characteristic	O
makes	O
entities	O
that	O
actually	O
refer	O
to	O
nationality	O
be	O
mislabeled	O
as	O
GPE	O
,	O
which	O
should	O
have	O
been	O
labeled	O
as	O
NORP	O
.	O

This	O
paper	O
presents	O
a	O
multilingual	O
dataset	O
that	O
allows	O
researchers	O
to	O
conduct	O
crosslingual	O
research	O
between	O
English	O
and	O
Korean	O
.	O
Our	O
dataset	O
contains	O
high	O
-	O
quality	O
annotation	O
of	O
named	O
entities	O
on	O
parallel	O
sentences	O
from	O
seven	O
popular	O
news	O
sections	O
.	O
Given	O
this	O
dataset	O
,	O
Korean	O
NER	B-TaskName
models	O
are	O
built	O
by	O
zero	B-TaskName
-	I-TaskName
shot	I-TaskName
learning	I-TaskName
using	O
multilingual	O
encoders	O
.	O
Our	O
data	O
selection	O
scheme	O
for	O
annotation	O
projection	O
significantly	O
improves	O
the	O
NER	B-TaskName
performance	O
although	O
it	O
is	O
still	O
suboptimal	O
.	O
Our	O
error	O
analysis	O
depicts	O
unique	O
characteristics	O
in	O
Korean	O
that	O
make	O
it	O
hard	O
for	O
zero	B-TaskName
-	I-TaskName
shot	I-TaskName
learning	I-TaskName
,	O
challenges	O
that	O
we	O
need	O
to	O
overcome	O
in	O
the	O
future	O
work	O
.	O
8	O

There	O
are	O
18	O
named	O
entity	O
tags	O
annotated	O
in	O
the	O
OntoNotes	B-DatasetName
5.0	I-DatasetName
as	O
follows	O
(	O
Pradhan	O
et	O
al	O
,	O
2013	O
)	O
:	O
9	O
CARDINAL	O
:	O
Numerical	O
terms	O
not	O
categorized	O
in	O
other	O
categorizations	O
.	O
Numbers	O
that	O
indicate	O
ages	O
are	O
included	O
.	O
DATE	O
:	O
Absolute	O
or	O
relative	O
dates	O
or	O
periods	O
.	O
The	O
period	O
should	O
last	O
longer	O
than	O
'	O
TIME	O
'	O
.	O
General	B-DatasetName
expressions	O
of	O
dates	O
are	O
included	O
too	O
such	O
as	O
'	O
few	O
months	O
'	O
,	O
'	O
that	O
day	O
'	O
,	O
'	O
Next	O
season	O
'	O
and	O
'	O
First	O
quarter	O
'	O
.	O
EVENT	O
:	O
It	O
means	O
an	O
official	O
or	O
widely	O
known	O
event	O
,	O
war	O
,	O
exhibition	O
.	O
Official	O
events	O
include	O
ministerial	O
meetings	O
,	O
general	O
elections	O
,	O
presidential	O
elections	O
,	O
exams	O
(	O
SAT	O
)	O
,	O
and	O
prayers	O
(	O
U.S.	O
national	O
breakfast	O
prayer	O
)	O
.	O
Social	O
phenomena	O
also	O
include	O
(	O
Brexit	O
)	O
for	O
widely	O
known	O
events	O
.	O
FAC	O
:	O
Objectives	O
referring	O
to	O
facilities	O
include	O
buildings	O
,	O
airports	O
,	O
highways	O
and	O
bridge	O
names	O
.	O
GPE	O
:	O
An	O
object	O
referring	O
to	O
a	O
place	O
or	O
location	O
,	O
including	O
the	O
name	O
of	O
a	O
country	O
and	O
the	O
name	O
of	O
an	O
administrative	O
district	O
,	O
such	O
as	O
a	O
city	O
or	O
state	O
.	O
LANGUAGE	O
:	O
Any	O
named	O
language	O
.	O
LAW	B-DatasetName
:	O
Named	O
documents	O
made	O
into	O
laws	O
.	O
LOC	O
:	O
Refers	O
to	O
the	O
name	O
of	O
a	O
place	O
or	O
location	O
that	O
does	O
not	O
belong	O
to	O
GPE	O
.	O
It	O
also	O
includes	O
expressions	O
covering	O
the	O
entire	O
location	O
of	O
mountains	O
,	O
rivers	O
,	O
ocean	O
names	O
and	O
Europe	O
,	O
Asia	O
,	O
etc	O
.	O
MONEY	O
:	O
Monetary	O
values	O
including	O
units	O
.	O
NORP	O
:	O
It	O
refers	O
to	O
nationality	O
,	O
religious	O
groups	O
and	O
political	O
groups	O
(	O
party	O
)	O
.	O
ORDINAL	O
:	O
All	O
ordinal	O
numbers	O
such	O
as	O
first	O
and	O
second	O
.	O
ORG	O
:	O
It	O
refers	O
a	O
community	O
/	O
group	O
of	O
people	O
gathered	O
together	O
.	O
For	O
example	O
,	O
the	O
name	O
of	O
the	O
company	O
,	O
the	O
name	O
of	O
the	O
school	O
,	O
and	O
the	O
name	O
of	O
the	O
sports	O
team	O
.	O
9	O
https://catalog.ldc.upenn.edu/docs/	O
LDC2013T19	O
/	O
OntoNotes	B-DatasetName
-	O
Release	O
-	O
5.0.pdf	O
PERCENT	O
:	O
Percentage	O
expressions	O
with	O
%	O
symbol	O
or	O
the	O
word	O
'	O
percent	O
'	O
.	O
PERSON	O
:	O
Referring	O
to	O
a	O
last	O
name	O
or	O
full	O
name	O
of	O
a	O
particular	O
person	O
.	O
It	O
also	O
includes	O
nicknames	O
for	O
non	O
-	O
human	O
creatures	O
and	O
characters	O
in	O
cartoons	O
,	O
dramas	O
and	O
movies	O
.	O
PRODUCT	O
:	O
Vehicles	O
,	O
Weapons	O
,	O
foods	O
.	O
IT	O
services	O
(	O
including	O
SNS	O
)	O
and	O
medicine	O
names	O
are	O
included	O
.	O
QUANTITY	O
:	O
Measurements	O
as	O
of	O
weights	O
or	O
distances	O
such	O
as	O
km	O
,	O
kg	O
and	O
etc	O
.	O
TIME	O
:	O
This	O
tag	O
indicates	O
time	O
expressions	O
smaller	O
than	O
a	O
day	O
.	O
This	O
tag	O
includes	O
certain	O
time	O
indication	O
,	O
amount	O
of	O
time	O
or	O
any	O
other	O
expressions	O
related	O
to	O
time	O
.	O
Even	O
though	O
an	O
entity	O
does	O
not	O
have	O
numeral	O
expressions	O
but	O
only	O
words	O
related	O
to	O
time	O
(	O
for	O
instance	O
,	O
'	O
noon	O
'	O
)	O
,	O
the	O
words	O
are	O
tagged	O
as	O
'	O
Time	O
'	O
.	O
WORK_OF_ART	O
:	O
Titles	O
of	O
books	O
,	O
songs	O
,	O
TV	O
programs	O
and	O
art	O
pieces	O
.	O
Title	O
of	O
games	O
,	O
awards	O
,	O
theories	O
,	O
records	O
are	O
included	O
.	O

There	O
are	O
10	O
tags	O
annotated	O
in	O
the	O
copus	O
distributed	O
by	O
the	O
Korea	O
Maritime	O
and	O
Ocean	O
University	O
:	O
10	O
DAT	O
:	O
Absolute	O
dates	O
.	O
Public	O
holidays	O
and	O
day	O
of	O
the	O
week	O
is	O
included	O
.	O
DUR	O
:	O
Duration	O
of	O
incidents	O
.	O
Academically	O
clarified	O
periods	O
such	O
as	O
Cretaceous	O
period	O
are	O
also	O
included	O
.	O
LOC	O
:	O
The	O
name	O
of	O
a	O
country	O
and	O
the	O
name	O
of	O
an	O
administrative	O
district	O
,	O
such	O
as	O
a	O
city	O
or	O
state	O
.	O
Words	O
representing	O
certain	O
locations	O
such	O
as	O
tour	O
spot	O
and	O
stadium	O
is	O
also	O
included	O
.	O
When	O
location	O
word	O
becomes	O
compound	O
nouns	O
with	O
other	O
words	O
,	O
it	O
is	O
not	O
included	O
.	O
MNY	O
:	O
Monetary	O
values	O
including	O
units	O
.	O
Bitcoin	O
is	O
not	O
included	O
.	O
NOH	O
:	O
Any	O
numerical	O
expressions	O
such	O
as	O
measurements	O
of	O
heights	O
,	O
temperatures	O
,	O
weights	O
.	O
Ordinal	O
numbers	O
are	O
included	O
.	O
ORG	O
:	O
A	O
group	O
consisting	O
of	O
2	O
or	O
more	O
people	O
.	O
The	O
name	O
of	O
the	O
company	O
,	O
the	O
name	O
of	O
the	O
school	O
,	O
and	O
the	O
name	O
of	O
the	O
sports	O
team	O
.	O
PER	O
:	O
Personal	O
name	O
including	O
first	O
and	O
last	O
name	O
.	O
Any	O
name	O
referring	O
to	O
living	O
things	O
and	O
nicknames	O
for	O
non	O
-	O
human	O
creatures	O
and	O
characters	O
in	O
cartoons	O
,	O
dramas	O
and	O
movies	O
are	O
included	O
.	O
PNT	B-DatasetName
:	O
Percentage	O
expressions	O
with	O
%	O
symbol	O
or	O
the	O
word	O
'	O
percent	O
'	O
.	O
POH	O
:	O
Product	O
name	O
,	O
medicine	O
,	O
game	O
,	O
event	O
,	O
meeting	O
,	O
movies	O
,	O
songs	O
,	O
drama	O
series	O
,	O
TV	O
channels	O
,	O
daily	O
and	O
weekly	O
magazines	O
,	O
emails	O
,	O
phone	O
numbers	O
are	O
included	O
.	O
TIM	O
:	O
This	O
tag	O
indicates	O
time	O
expressions	O
smaller	O
than	O
a	O
day	O
.	O
This	O
tag	O
includes	O
certain	O
time	O
indication	O
,	O
amount	O
of	O
time	O
or	O
any	O
other	O
expressions	O
related	O
to	O
time	O
.	O
Even	O
though	O
an	O
entity	O
does	O
not	O
have	O
numeral	O
expressions	O
but	O
only	O
words	O
related	O
to	O
time	O
(	O
for	O
instance	O
,	O
'	O
noon	O
'	O
)	O
,	O
the	O
words	O
are	O
tagged	O
as	O
'	O
Time	O
'	O
.	O

Table	O
9	O
shows	O
the	O
performance	O
of	O
the	O
NER	B-TaskName
model	O
in	O
the	O
ELIT	O
toolkit	O
on	O
the	O
English	O
development	O
and	O
evaluation	O
sets	O
in	O
our	O
dataset	O
.	O

Our	O
zero	O
-	O
shot	O
model	O
yields	O
a	O
better	O
performance	O
than	O
the	O
existing	O
model	O
although	O
it	O
may	O
be	O
difficult	O
to	O
directly	O
compare	O
the	O
two	O
models	O
.	O
In	O
the	O
case	O
of	O
the	O
existing	O
Korean	O
model	O
,	O
the	O
low	O
performance	O
may	O
be	O
caused	O
by	O
the	O
different	O
annotation	O
scheme	O
between	O
the	O
datasets	O
.	O
In	O
the	O
case	O
of	O
our	O
zero	O
-	O
shot	O
model	O
,	O
the	O
improvement	O
of	O
the	O
performance	O
are	O
seen	O
due	O
to	O
the	O
coarse	O
-	O
grained	O
named	O
entities	O
after	O
the	O
mapping	O
.	O
Table	O
11b	O
describes	O
the	O
proportions	O
of	O
news	O
sections	O
per	O
entity	O
type	O
.	O
CARDINAL	O
,	O
ORDINAL	O
,	O
and	O
EVENT	O
appear	O
the	O
most	O
in	O
Sports	O
that	O
involves	O
many	O
game	O
events	O
and	O
statistics	O
.	O
DATE	O
,	O
ORG	O
,	O
and	O
QUANTITY	O
show	O
fairly	O
even	O
proportions	O
in	O
every	O
section	O
as	O
they	O
are	O
elemental	O
to	O
a	O
variety	O
of	O
topics	O
.	O
GPE	O
,	O
LOC	O
,	O
and	O
NORP	O
give	O
high	O
proportions	O
to	O
both	O
Politics	O
and	O
World	O
.	O
MONEY	O
and	O
PERCENT	O
appear	O
the	O
most	O
in	O
Business	O
that	O
often	O
deals	O
with	O
monetary	O
issues	O
.	O
PERSON	O
show	O
high	O
proportions	O
in	O
Sports	O
and	O
Politics	O
as	O
discussed	O
above	O
.	O
FAC	O
takes	O
good	O
portions	O
in	O
Lifestyle	O
,	O
Business	O
,	O
and	O
World	O
,	O
which	O
often	O
mention	O
facilities	O
that	O
people	O
encounter	O
daily	O
(	O
e.g.	O
,	O
airports	O
,	O
bridges	O
)	O
.	O
TIME	O
appears	O
the	O
most	O
in	O
Sports	O
and	O
Society	O
that	O
are	O
full	O
of	O
dynamic	O
events	O
and	O
issues	O
.	O
LANGUAGE	O
is	O
mostly	O
found	O
in	O
Society	O
although	O
the	O
sample	O
size	O
is	O
too	O
small	O
to	O
generalize	O
.	O
LAW	B-DatasetName
,	O
PRODUCT	O
,	O
and	O
WORK_OF_ART	O
appear	O
the	O
most	O
in	O
Politics	O
,	O
Sci	O
/	O
Tech	O
,	O
and	O
Lifestyle	O
,	O
that	O
focus	O
on	O
legal	O
issues	O
,	O
tech	O
products	O
,	O
and	O
entertainment	O
(	O
e.g.	O
,	O
music	O
,	O
movies	O
,	O
shows	O
)	O
,	O
respectively	O
.	O

This	O
work	O
was	O
partly	O
supported	O
by	O
the	O
Institute	O
of	O
Information	O
and	O
Communications	O
Technology	O
Planning	O
and	O
Evaluation	O
(	O
IITP	O
)	O
grant	O
funded	O
by	O
the	O
Korean	O
government	O
(	O
MSIT	O
)	O
(	O
No	O
.	O
2020	O
-	O
0	B-DatasetName
-	O
01361	O
,	O
Artificial	O
Intelligence	O
Graduate	O
School	O
Program	O
(	O
Yonsei	O
University	O
)	O
)	O
.	O

Dissecting	O
offensive	O
language	O
detection	O
:	O
does	O
it	O
work	O
,	O
and	O
what	O
can	O
we	O
do	O
with	O
it	O
?	O
Social	O
media	O
messages	O
are	O
often	O
written	O
to	O
attack	O
specific	O
groups	O
of	O
users	O
based	O
on	O
their	O
religion	O
,	O
ethnicity	O
or	O
social	O
status	O
,	O
and	O
they	O
can	O
be	O
particularly	O
threatening	O
to	O
vulnerable	O
users	O
such	O
as	O
teenagers	O
.	O
It	O
is	O
therefore	O
very	O
important	O
to	O
develop	O
reliable	O
,	O
unbiased	O
and	O
robust	O
detection	O
systems	O
to	O
support	O
stakeholders	O
in	O
fighting	O
online	O
hatred	O
.	O
Although	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
systems	O
yield	O
very	O
good	O
classification	O
results	O
,	O
the	O
problem	O
is	O
far	O
from	O
being	O
solved	O
.	O
In	O
my	O
talk	O
,	O
I	O
will	O
discuss	O
which	O
issues	O
still	O
affect	O
the	O
development	O
of	O
abusive	B-TaskName
language	I-TaskName
detection	O
systems	O
,	O
for	O
example	O
the	O
problem	O
of	O
dealing	O
with	O
annotators	O
'	O
disagreement	O
in	O
the	O
creation	O
of	O
training	O
data	O
,	O
and	O
the	O
issues	O
related	O
to	O
contextual	O
information	O
in	O
threads	O
.	O
On	O
the	O
other	O
hand	O
,	O
I	O
will	O
show	O
how	O
the	O
output	O
of	O
offensive	O
language	O
detection	O
systems	O
can	O
be	O
integrated	O
with	O
network	O
-	O
based	O
information	O
to	O
study	O
the	O
behavioral	O
patterns	O
of	O
different	O
types	O
of	O
users	O
,	O
also	O
in	O
relation	O
to	O
misinformation	O
.	O

Sara	O
Tonelli	O
holds	O
a	O
PhD	O
in	O
Language	O
Sciences	O
from	O
Università	O
Ca	O
'	O
Foscari	O
,	O
Venice	O
.	O
Since	O
2013	O
she	O
has	O
been	O
the	O
head	O
of	O
the	O
Digital	O
Humanities	O
research	O
group	O
at	O
Fondazione	O
Bruno	O
Kessler	O
in	O
Trento	O
,	O
Italy	O
.	O
Among	O
many	O
projects	O
in	O
digital	O
humanities	O
,	O
she	O
is	O
currently	O
involved	O
in	O
the	O
H2020	O
ODEUROPA	O
project	O
(	O
focusing	O
on	O
olfactory	O
information	O
extraction	O
)	O
,	O
and	O
in	O
the	O
H2020	O
PERCEPTIONS	O
project	O
(	O
online	O
perception	O
and	O
migration	O
narratives	O
related	O
to	O
EU	O
)	O
.	O
Since	O
January	O
2021	O
she	O
is	O
also	O
the	O
scientific	O
coordinator	O
of	O
the	O
KID	O
ACTIONS	O
European	O
project	O
(	O
addressing	O
cyberbullying	O
among	O
children	O
and	O
adolescents	O
)	O
.	O
Sara	O
's	O
main	O
research	O
interests	O
are	O
related	O
to	O
temporal	O
and	O
event	O
-	O
based	O
processing	O
of	O
texts	O
,	O
especially	O
in	O
the	O
historical	O
domain	O
,	O
and	O
social	O
media	O
processing	O
,	O
including	O
the	O
detection	O
of	O
abusive	B-TaskName
language	I-TaskName
.	O

Blocking	O
has	O
been	O
widely	O
studied	O
for	O
record	O
linkage	O
and	O
entity	B-TaskName
disambiguation	I-TaskName
.	O
Standard	O
blocking	O
is	O
the	O
simplest	O
but	O
most	O
widely	O
used	O
method	O
(	O
Fellegi	O
and	O
Sunter	O
,	O
1969	O
)	O
.	O
It	O
is	O
done	O
by	O
considering	O
only	O
pairs	O
that	O
meet	O
al	O
blocking	O
predicates	O
.	O
Another	O
is	O
the	O
sorted	O
neighborhood	O
approach	O
(	O
Hernández	O
and	O
Stolfo	O
,	O
1995	O
)	O
which	O
sorts	O
the	O
data	O
by	O
a	O
certain	O
blocking	O
predicate	O
,	O
and	O
forms	O
blocks	O
with	O
pairs	O
of	O
those	O
records	O
within	O
a	O
certain	O
window	O
.	O
Yan	B-DatasetName
et	I-DatasetName
al	I-DatasetName
(	O
2007	O
)	O
further	O
improved	O
this	O
method	O
to	O
adaptively	O
select	O
the	O
size	O
of	O
the	O
window	O
.	O
Aizawa	O
and	O
Oyama	O
(	O
2005	O
)	O
introduced	O
a	O
suffix	O
array	O
-	O
based	O
indexing	O
method	O
,	O
which	O
uses	O
an	O
inverted	O
index	O
of	O
suffixes	O
to	O
generate	O
candidate	O
pairs	O
.	O
Canopy	O
clustering	O
(	O
McCallum	O
et	O
al	O
,	O
2000	O
)	O
generates	O
blocks	O
by	O
clustering	O
with	O
a	O
simple	O
similarity	O
measure	O
and	O
use	O
loose	O
&	O
tight	O
thresholds	O
to	O
generate	O
overlapping	O
clusters	O
.	O
Recent	O
surveys	O
(	O
Christen	O
,	O
2012	O
;	O
Papadakis	O
et	O
al	O
,	O
2016Papadakis	O
et	O
al	O
,	O
,	O
2020	O
imply	O
that	O
there	O
are	O
no	O
clear	O
winners	O
and	O
proper	O
parameter	O
tuning	O
is	O
required	O
for	O
a	O
specific	O
task	O
.	O
Much	O
work	O
optimized	O
the	O
blocking	O
function	O
for	O
standard	O
blocking	O
.	O
The	O
blocking	O
function	O
is	O
typically	O
presented	O
with	O
a	O
logical	O
formula	O
with	O
blocking	O
predicates	O
.	O
Two	O
studies	O
focused	O
on	O
learning	O
a	O
disjunctive	O
normal	O
form	O
(	O
DNF	O
)	O
blocking	O
(	O
Bilenko	O
et	O
al	O
,	O
2006	O
;	O
Michelson	O
and	O
Knoblock	O
,	O
2006	O
)	O
were	O
published	O
in	O
the	O
same	O
year	O
.	O
Making	O
use	O
of	O
manually	O
labeled	O
record	O
pairs	O
,	O
they	O
used	O
a	O
sequential	O
covering	O
algorithm	O
to	O
find	O
the	O
optimal	O
blocking	O
predicates	O
in	O
a	O
greedy	O
manner	O
.	O
Additional	O
unlabeled	O
data	O
was	O
used	O
to	O
estimate	O
the	O
reduction	O
ratio	O
of	O
their	O
cost	O
function	O
(	O
Cao	O
et	O
al	O
,	O
2011	O
)	O
while	O
an	O
unsupervised	O
algorithm	O
was	O
used	O
to	O
automatically	O
generate	O
labeled	O
pairs	O
with	O
rule	O
-	O
based	O
heuristics	O
used	O
to	O
learn	O
DNF	O
blocking	O
(	O
Kejriwal	O
and	O
Miranker	O
,	O
2013	O
)	O
.	O
All	O
the	O
work	O
above	O
proposed	O
to	O
learn	O
nondisjoint	O
blocking	O
because	O
of	O
the	O
logical	O
OR	O
terms	O
in	O
the	O
DNF	O
.	O
However	O
,	O
other	O
work	O
learns	O
the	O
blocking	O
function	O
with	O
a	O
pure	O
conjunction	O
,	O
to	O
ensure	O
the	O
generation	O
of	O
disjoint	O
blocks	O
.	O
Das	O
et	O
al	O
(	O
2012	O
)	O
learns	O
a	O
conjunctive	O
blocking	O
tree	O
,	O
which	O
has	O
different	O
blocking	O
predicates	O
for	O
each	O
branch	O
of	O
the	O
tree	O
.	O
Fisher	O
et	O
al	O
(	O
2015	O
)	O
produces	O
blocks	O
with	O
respect	O
to	O
a	O
size	O
restriction	O
,	O
by	O
generating	O
candidate	O
blocks	O
with	O
a	O
list	O
of	O
predefined	O
blocking	O
predicates	O
and	O
then	O
performs	O
a	O
merge	O
and	O
split	O
to	O
generate	O
the	O
block	O
with	O
the	O
desired	O
size	O
.	O
Our	O
work	O
proposes	O
a	O
method	O
for	O
learning	O
a	O
nondisjoint	O
blocking	O
function	O
in	O
a	O
conjunctive	O
normal	O
form	O
(	O
CNF	O
)	O
.	O
Our	O
method	O
is	O
based	O
on	O
a	O
previous	O
CNF	O
learner	O
(	O
Mooney	O
,	O
1995	O
)	O
,	O
which	O
uses	O
the	O
fact	O
that	O
a	O
CNF	O
can	O
be	O
a	O
logical	O
dual	O
of	O
a	O
DNF	O
.	O

CNF	O
blocking	O
can	O
be	O
learned	O
with	O
a	O
small	O
modification	O
to	O
DNF	O
blocking	O
.	O
CNF	O
can	O
be	O
presented	O
as	O
the	O
entire	O
negation	O
of	O
a	O
corresponding	O
DNF	O
and	O
vice	O
versa	O
based	O
on	O
De	O
Morgan	O
's	O
laws	O
.	O
Using	O
this	O
,	O
Mooney	O
proposed	O
CNF	O
learning	O
(	O
Mooney	O
,	O
1995	O
)	O
,	O
which	O
is	O
a	O
logical	O
dual	O
of	O
DNF	O
learning	O
.	O
This	O
motivated	O
our	O
CNF	O
blocking	O
method	O
.	O
Algorithm	O
2	O
illustrates	O
the	O
proposed	O
CNF	O
blocking	O
and	O
has	O
a	O
similar	O
structure	O
to	O
algorithm	O
1	O
.	O
Instead	O
of	O
running	O
a	O
sequential	O
covering	O
algorithm	O
to	O
cover	O
all	O
positive	O
samples	O
,	O
CNF	O
blocking	O
tries	O
to	O
cover	O
all	O
negative	O
samples	O
using	O
negated	O
blocking	O
predicates	O
.	O
In	O
other	O
words	O
,	O
a	O
DNF	O
formula	O
is	O
learned	O
that	O
is	O
consistent	O
with	O
a	O
negated	O
predicate	O
,	O
which	O
we	O
designate	O
negated	O
DNF	O
(	O
N	O
egDN	O
F	O
)	O
.	O
N	O
egP	O
is	O
the	O
negation	O
of	O
each	O
predicate	O
p	O
in	O
P	O
.	O
LEARNCNF	O
gets	O
3	O
inputs	O
,	O
where	O
L	O
are	O
labeled	O
Find	O
T	O
CandN	O
egT	B-MethodName
erms	O
that	O
maximizes	O
gain	O
function	O
CALCNEG	O
-	O
GAIN	O
(	O
P	O
os	O
,	O
N	O
eg	O
,	O
T	O
erm	O
)	O
28	O
:	O
if	O
CALCNEGGAIN	O
(	O
P	O
os	O
,	O
N	O
eg	O
,	O
T	O
)	O
>	O
0	B-DatasetName
then	O
29	O
:	O
N	O
egDN	O
F	O
N	O
egDN	O
F	O
∨	O
T	O
30	O
:	O
Let	O
P	O
osCov	O
be	O
all	O
l	O
in	O
P	O
os	O
that	O
satisfies	O
T	O

Consider	O
pairs	O
in	O
L	O
only	O
for	O
clustering	O
9	O
:	O
end	O
for	O
10	O
:	O
end	O
function	O
they	O
filter	O
out	O
all	O
terms	O
with	O
pairwise	O
completeness	O
(	O
PC	O
)	O
below	O
threshold	O
t.	O
We	O
use	O
the	O
dual	O
of	O
the	O
original	O
function	O
used	O
as	O
a	O
CNF	O
,	O
which	O
is	O
now	O
gain	O
CN	O
F	O
=	O
p+n	O
P	O
+	O
N	O
if	O
n	O
N	O
>	O
t	O
0	B-DatasetName
otherwise	O
.	O
(	O
4	O
)	O

Figure	O
1	O
shows	O
the	O
PC	O
-	O
RR	B-DatasetName
curve	O
tested	O
on	O
three	O
different	O
gain	O
functions	O
.	O
Blocking	O
usually	O
requires	O
a	O
high	O
PC	O
,	O
so	O
that	O
we	O
do	O
not	O
lose	O
matched	O
pairs	O
after	O
it	O
is	O
applied	O
.	O
As	O
such	O
,	O
we	O
focused	O
on	O
experiments	O
with	O
high	O
PC	O
values	O
.	O
As	O
we	O
can	O
see	O
from	O
the	O
results	O
,	O
information	O
gain	O
has	O
highest	O
RR	B-DatasetName
overall	O
.	O
Thus	O
,	O
we	O
use	O
it	O
as	O
the	O
gain	O
function	O
for	O
the	O
rest	O
of	O
the	O
experiments	O
.	O

We	O
compare	O
non	O
-	O
disjoint	O
CNF	O
blocking	O
with	O
the	O
DNF	O
blocking	O
(	O
Bilenko	O
et	O
al	O
,	O
2006	O
;	O
Michelson	O
and	O
Knoblock	O
,	O
2006	O
)	O
and	O
canopy	O
clustering	O
(	O
McCallum	O
et	O
al	O
,	O
2000	O
)	O
.	O
We	O
used	O
the	O
set	O
of	O
Jaro	O
-	O
Winkler	O
distance	O
attributes	O
for	O
canopy	O
clustering	O
.	O
Figure	O
2	O
shows	O
the	O
PC	O
-	O
RR	B-DatasetName
curve	O
for	O
each	O
method	O
.	O
Both	O
CNF	O
and	O
DNF	O
were	O
better	O
than	O
canopy	O
clustering	O
,	O
as	O
was	O
shown	O
in	O
Bilenko	O
et	O
al	O
(	O
2006	O
in	O
(	O
Khabsa	O
et	O
al	O
,	O
2015	O
)	O
.	O
For	O
PC=0.99	O
,	O
RR	B-DatasetName
for	O
CNF	O
blocking	O
was	O
0.882	O
while	O
DNF	O
blocking	O
was	O
0.745	O
.	O
We	O
believe	O
this	O
is	O
due	O
to	O
certain	O
characteristics	O
of	O
scholarly	O
databases	O
.	O
As	O
discussed	O
on	O
the	O
previous	O
section	O
,	O
some	O
attributes	O
are	O
empty	O
for	O
some	O
records	O
.	O
DNF	O
learns	O
a	O
blocking	O
function	O
by	O
adding	O
conjunction	O
terms	O
to	O
gradually	O
cover	O
positive	O
pairs	O
.	O
Although	O
the	O
proposed	O
similarity	O
criterion	O
compatible	O
could	O
catch	O
positive	O
pairs	O
with	O
empty	O
attributes	O
,	O
it	O
allows	O
many	O
negative	O
pairs	O
to	O
pass	O
the	O
criterion	O
,	O
which	O
makes	O
the	O
RR	B-DatasetName
low	O
.	O
On	O
the	O
other	O
hand	O
,	O
CNF	O
learns	O
a	O
blocking	O
function	O
to	O
cover	O
(	O
and	O
filter	O
out	O
)	O
negative	O
pairs	O
gradually	O
.	O
Negative	O
pairs	O
are	O
much	O
more	O
obvious	O
to	O
define	O
(	O
pairs	O
with	O
different	O
values	O
)	O
,	O
which	O
makes	O
the	O
CNF	O
more	O
effective	O
.	O
Another	O
advantage	O
of	O
using	O
CNF	O
is	O
the	O
processing	O
time	O
.	O
Fast	O
processing	O
time	O
to	O
apply	O
blocking	O
is	O
important	O
for	O
some	O
applications	O
,	O
one	O
example	O
is	O
when	O
we	O
do	O
a	O
online	O
disambiguation	O
(	O
Khabsa	O
et	O
al	O
,	O
2015	O
)	O
,	O
another	O
is	O
to	O
do	O
an	O
author	O
search	O
which	O
requires	O
to	O
find	O
the	O
relevant	O
cluster	O
quickly	O
(	O
Kim	O
et	O
al	O
,	O
2018	O
)	O
.	O
We	O
measured	O
the	O
average	O
processing	O
time	O
of	O
applying	O
each	O
blocking	O
method	O
at	O
high	O
PC	O
(	O
PC=0.99	O
)	O
,	O
CNF	O
blocking	O
,	O
DNF	O
blocking	O
,	O
canopy	O
clustering	O
took	O
1.39s	O
,	O
2.09s	O
,	O
0.44s	O
respectively	O
.	O
Canopy	O
clustering	O
was	O
the	O
fastest	O
but	O
generally	O
we	O
saw	O
from	O
the	O
Figure	O
2	O
that	O
its	O
RR	B-DatasetName
is	O
much	O
lower	O
in	O
high	O
PC	O
.	O
CNF	O
blocking	O
has	O
a	O
faster	O
processing	O
time	O
compared	O
to	O
DNF	O
blocking	O
.	O
This	O
is	O
because	O
CNF	O
is	O
composed	O
with	O
conjunctions	O
,	O
so	O
it	O
can	O
quickly	O
reject	O
pairs	O
that	O
are	O
not	O
consistent	O
with	O
any	O
terms	O
.	O
On	O
the	O
other	O
hand	O
,	O
DNF	O
consists	O
of	O
disjunction	O
terms	O
,	O
so	O
each	O
pair	O
should	O
check	O
all	O
terms	O
to	O
make	O
the	O
decision	O
.	O
Learned	O
CNF	O
is	O
also	O
simpler	O
than	O
DNF	O
.	O
Learned	O
CNF	O
at	O
this	O
level	O
is	O
as	O
below	O
(	O
fn	O
,	O
mn	O
,	O
ln	O
is	O
first	O
,	O
middle	O
,	O
last	O
name	O
respectively	O
)	O
:	O
In	O
addition	O
,	O
we	O
observed	O
that	O
proposed	O
compatible	O
predicate	O
was	O
frequently	O
used	O
in	O
our	O
result	O
.	O
This	O
shows	O
the	O
effectiveness	O
of	O
compatible	O
in	O
dealing	O
with	O
the	O
empty	O
value	O
.	O

We	O
evaluate	O
our	O
extension	O
to	O
disjoint	O
blocks	O
with	O
CNF	O
blocking	O
.	O
We	O
compare	O
the	O
blocking	O
learned	O
with	O
a	O
pure	O
conjunction	O
,	O
our	O
proposed	O
method	O
,	O
and	O
the	O
method	O
of	O
Fisher	O
et	O
al	O
(	O
2015	O
)	O
.	O
Figure	O
3	O
shows	O
the	O
reduction	O
ratio	O
pair	O
completion	O
(	O
RR	B-DatasetName
-	O
PC	O
)	O
curve	O
for	O
each	O
method	O
.	O
We	O
also	O
plot	O
the	O
original	O
non	O
-	O
disjoint	O
CNF	O
blocking	O
for	O
comparison	O
.	O
We	O
see	O
that	O
our	O
proposed	O
disjoint	O
CNF	O
blocking	O
is	O
the	O
best	O
amongst	O
all	O
disjoint	O
methods	O
.	O
Fisher	O
's	O
method	O
produced	O
nearly	O
uniformsized	O
blocks	O
,	O
but	O
had	O
limitations	O
in	O
reaching	O
a	O
high	O
PC	O
and	O
had	O
a	O
generally	O
lower	O
RR	B-DatasetName
compared	O
to	O
our	O
method	O
.	O
Disjoint	O
CNF	O
did	O
n't	O
perform	O
as	O
well	O
when	O
compared	O
to	O
non	O
-	O
disjoint	O
CNF	O
because	O
it	O
is	O
forced	O
to	O
use	O
a	O
pure	O
conjunction	O
on	O
its	O
first	O
step	O
.	O
However	O
,	O
this	O
simple	O
extension	O
easily	O
helps	O
parallelize	O
the	O
clustering	O
process	O
,	O
so	O
that	O
the	O
algorithm	O
scales	O
better	O
.	O
Testing	O
our	O
method	O
to	O
all	O
of	O
PubMed	O
,	O
82.17	O
%	O
of	O
the	O
pairs	O
are	O
created	O
in	O
10.5	O
min	O
with	O
24	O
threads	O
.	O
Parallelization	O
is	O
important	O
for	O
disambiguation	O
algorithms	O
to	O
scale	O
to	O
PubMed	O
size	O
scholarly	O
databases	O
(	O
Khabsa	O
et	O
al	O
,	O
2014	O
)	O
.	O
Processing	O
time	O
for	O
disjoint	O
CNF	O
blocking	O
comparable	O
to	O
the	O
original	O
non	O
-	O
disjoint	O
CNF	O
blocking	O
.	O
The	O
learned	O
disjoint	O
CNF	O
is	O
:	O
{	O
(	O
fn	O
,	O
first	O
(	O
1	O
)	O
)	O
}	O
{	O
(	O
ln	O
,	O
exact	O
)	O
}	O
{	O
(	O
fn	O
,	O
compatible	O
)	O
∨	O
(	O
coauth	O
,	O
cos	O
(	O
0.8	O
)	O
)	O
}	O
{	O
(	O
mn	O
,	O
compatible	O
)	O
}	O
First	O
two	O
terms	O
are	O
from	O
1	O
-	O
CNF	O
,	O
and	O
others	O
from	O
3	O
-	O
CNF	O
learner	O
.	O
We	O
also	O
tested	O
this	O
function	O
to	O
the	O
whole	O
PubMed	O
.	O

Morphological	B-TaskName
Inflection	I-TaskName
Generation	O
with	O
Multi	O
-	O
space	O
Variational	O
Encoder	O
-	O
Decoders	O

In	O
morphologically	O
rich	O
languages	O
,	O
different	O
affixes	O
(	O
i.e.	O
prefixes	O
,	O
infixes	O
,	O
suffixes	O
)	O
can	O
be	O
combined	O
with	O
the	O
lemma	B-DatasetName
to	O
reflect	O
various	O
syntactic	O
and	O
semantic	O
features	O
of	O
a	O
word	O
.	O
In	O
many	O
areas	O
of	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
it	O
is	O
important	O
that	O
systems	O
are	O
able	O
to	O
correctly	O
analyze	O
and	O
generate	O
different	O
morphological	O
forms	O
,	O
including	O
previously	O
unseen	O
forms	O
.	O
The	O
ability	O
to	O
accurately	O
analyze	O
and	O
generate	O
morphological	O
forms	O
is	O
crucial	O
to	O
creating	O
applications	O
such	O
as	O
machine	B-TaskName
translation	I-TaskName
(	O
Chahuneau	O
et	O
al	O
,	O
2013	O
)	O
and	O
information	B-TaskName
retrieval	I-TaskName
(	O
Darwish	O
and	O
Oard	O
,	O
2007	O
)	O
.	O
Accordingly	O
,	O
learning	O
morphological	O
reinflection	O
patterns	O
from	O
labeled	O
data	O
is	O
an	O
important	O
challenge	O
.	O
The	O
Universal	O
Morphological	O
Reinflection	O
task	O
at	O
SIGMORPHON	O
2017	O
(	O
Cotterell	O
and	O
Schütze	O
,	O
2017	O
)	O
is	O
an	O
evaluation	O
campaign	O
aimed	O
at	O
systems	O
that	O
tackle	O
the	O
task	O
of	O
morphological	B-TaskName
inflection	I-TaskName
.	O
It	O
extends	O
the	O
SIGMORPHON	O
2016	O
Morphological	O
Reinflection	O
by	O
conducting	O
tasks	O
in	O
52	O
languages	O
instead	O
of	O
10	O
Cotterell	O
et	O
al	O
(	O
2016	O
)	O
.	O
In	O
our	O
system	O
submission	O
,	O
we	O
utilize	O
multispace	O
variational	O
encoder	O
-	O
decoders	O
(	O
MSVEDs	O
)	O
,	O
which	O
are	O
a	O
varitional	O
encoder	O
-	O
decoder	O
with	O
both	O
continuous	O
and	O
discrete	O
latent	O
variables	O
(	O
Zhou	O
and	O
Neubig	O
,	O
2017	O
)	O
.	O
The	O
continuous	O
latent	O
variable	O
is	O
expected	O
to	O
reflect	O
the	O
lemma	B-DatasetName
form	O
of	O
a	O
word	O
and	O
the	O
discrete	O
variables	O
are	O
used	O
to	O
induce	O
the	O
desired	O
labels	O
of	O
the	O
inflected	O
word	O
.	O
The	O
whole	O
model	O
is	O
trained	O
in	O
a	O
semi	O
-	O
supervised	O
fashion	O
.	O
For	O
the	O
supervised	O
part	O
we	O
are	O
reducing	O
the	O
reconstruction	O
error	O
of	O
generating	O
the	O
inflected	O
word	O
given	O
the	O
lemma	B-DatasetName
and	O
corresponding	O
tags	O
.	O
For	O
the	O
unsupervised	O
part	O
,	O
we	O
introduce	O
the	O
discrete	O
latent	O
variables	O
representing	O
the	O
morphological	O
tags	O
,	O
and	O
train	O
an	O
auto	O
-	O
encoder	O
over	O
unlabeled	O
corpora	O
.	O
Thus	O
,	O
the	O
training	O
objective	O
includes	O
both	O
the	O
variational	O
lower	O
bound	O
on	O
the	O
marginal	O
log	O
likelihood	O
of	O
the	O
observed	O
parallel	O
training	O
data	O
and	O
the	O
monolingual	O
data	O
.	O
There	O
are	O
two	O
tasks	O
in	O
SIGMORPHON	O
2017	O
,	O
which	O
are	O
morphology	O
inflection	O
(	O
task	O
1	O
)	O
and	O
paradigm	O
completion	O
(	O
task	O
2	O
)	O
respectively	O
.	O
We	O
participated	O
in	O
task	O
1	O
,	O
inflection	O
generation	O
,	O
in	O
which	O
the	O
goal	O
is	O
to	O
output	O
the	O
inflected	O
form	O
of	O
a	O
lemma	B-DatasetName
given	O
a	O
set	O
of	O
desired	O
morphological	O
tags	O
.	O
1	O
Experimental	O
results	O
found	O
that	O
our	O
model	O
works	O
relatively	O
well	O
on	O
the	O
shared	O
task	O
1	O
without	O
extensive	O
tuning	O
of	O
hyper	O
-	O
parameters	O
and	O
languagespecific	O
features	O
.	O

One	O
challenge	O
in	O
training	O
our	O
model	O
is	O
that	O
discrete	O
random	O
variables	O
in	O
a	O
stochastic	O
computation	O
graph	O
prevent	O
the	O
gradient	O
from	O
being	O
backpropagated	O
due	O
to	O
their	O
non	O
-	O
differentiability	O
,	O
and	O
marginalizing	O
over	O
all	O
label	O
combinations	O
is	O
also	O
infeasible	O
in	O
our	O
case	O
.	O
To	O
alleviate	O
this	O
problem	O
,	O
we	O
use	O
the	O
recently	O
proposed	O
Gumbel	O
-	O
Softmax	B-MethodName
trick	O
(	O
Maddison	O
et	O
al	O
,	O
2014	O
;	O
Gumbel	O
and	O
Lieblein	O
,	O
1954	O
)	O
to	O
create	O
a	O
differentiable	O
estimator	O
for	O
categorical	O
variables	O
.	O
In	O
experiments	O
,	O
we	O
start	O
with	O
a	O
relatively	O
large	O
temperature	O
and	O
decrease	O
it	O
gradually	O
.	O

Creating	O
morphosyntactic	O
tag	O
maps	O
:	O
In	O
our	O
model	O
,	O
we	O
treat	O
the	O
inference	O
model	O
on	O
discrete	O
labels	O
in	O
the	O
form	O
of	O
discriminator	O
,	O
thus	O
we	O
need	O
to	O
know	O
which	O
label	O
belongs	O
to	O
which	O
morphosyntactic	O
dimension	O
.	O
For	O
example	O
,	O
V	O
is	O
a	O
label	O
of	O
Part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
-	O
tagging	O
.	O
To	O
obtain	O
such	O
mapping	O
from	O
a	O
specific	O
label	O
to	O
the	O
morphosyntactic	O
dimension	O
,	O
we	O
leverage	O
the	O
Universal	O
Morphological	O
Feature	O
Schema	O
(	O
Sylak	O
-	O
Glassman	O
,	O
2016	O
)	O
and	O
also	O
add	O
the	O
missing	O
schema	O
from	O
the	O
training	O
data	O
to	O
create	O
the	O
key	O
-	O
value	O
pairs	O
of	O
morphosysntactic	O
dimension	O
and	O
label	O
.	O
Then	O
we	O
reformat	O
the	O
labels	O
provided	O
in	O
the	O
data	O
set	O
into	O
the	O
key	O
-	O
value	O
pairs	O
to	O
train	O
a	O
classifier	O
for	O
each	O
morphosyntactic	O
dimension	O
.	O
Data	B-TaskName
Augmentation	I-TaskName
:	O
We	O
augment	O
the	O
data	O
set	O
in	O
the	O
similar	O
way	O
as	O
Kann	O
and	O
Schütze	O
(	O
2016	O
)	O
.	O
By	O
doing	O
so	O
,	O
the	O
training	O
data	O
is	O
not	O
limited	O
to	O
the	O
form	O
of	O
lemma	B-DatasetName
to	O
inflected	O
word	O
but	O
can	O
also	O
be	O
any	O
word	O
pairs	O
that	O
share	O
the	O
same	O
lemma	B-DatasetName
.	O
This	O
helps	O
our	O
model	O
generalize	O
better	O
and	O
learn	O
the	O
latent	O
continuous	O
representations	O
more	O
effectively	O
.	O
The	O
size	O
of	O
training	O
data	O
set	O
after	O
augmentation	O
scales	O
with	O
a	O
factor	O
of	O
2	O
to	O
20	O
times	O
compared	O
with	O
the	O
original	O
one	O
.	O

One	O
potential	O
reason	O
for	O
the	O
lack	O
of	O
effectiveness	O
of	O
semi	O
-	O
supervised	O
training	O
is	O
that	O
the	O
semi	O
-	O
supervised	O
data	O
that	O
we	O
used	O
for	O
training	O
was	O
not	O
appropriate	O
for	O
the	O
task	O
at	O
hand	O
,	O
or	O
that	O
we	O
were	O
not	O
able	O
to	O
use	O
it	O
in	O
the	O
most	O
effective	O
way	O
.	O
In	O
order	O
to	O
do	O
so	O
,	O
we	O
analyze	O
the	O
distribution	O
of	O
linguistic	O
tags	O
for	O
words	O
from	O
the	O
training	O
data	O
in	O
the	O
shared	O
task	O
and	O
the	O
Wiki	O
Data	O
provided	O
by	O
the	O
organizer	O
,	O
with	O
the	O
hypothesis	O
that	O
if	O
the	O
distribution	O
of	O
tags	O
for	O
the	O
Wiki	O
Data	O
is	O
very	O
different	O
from	O
the	O
training	O
and	O
test	O
data	O
for	O
the	O
shared	O
task	O
,	O
our	O
predictions	O
may	O
be	O
biased	O
away	O
from	O
the	O
testing	O
distribution	O
by	O
incorporating	O
the	O
unsupervised	O
Wiki	O
data	O
.	O
To	O
perform	O
this	O
examination	O
,	O
we	O
use	O
the	O
tag	O
classifier	O
trained	O
in	O
our	O
model	O
to	O
predict	O
the	O
labels	O
for	O
each	O
word	O
in	O
the	O
Wiki	O
Data	O
.	O
The	O
percentages	O
of	O
each	O
label	O
within	O
each	O
morphosyntactic	O
dimension	O
for	O
Arabic	O
and	O
Persian	O
are	O
listed	O
in	O
Tab	O
.	O
4	O
and	O
Tab	O
.	O
5	O
.	O
We	O
found	O
that	O
the	O
distribution	O
of	O
the	O
linguistic	O
tags	O
for	O
the	O
Wiki	O
Data	O
and	O
the	O
training	O
data	O
in	O
the	O
shared	O
task	O
are	O
not	O
always	O
consistent	O
.	O
For	O
example	O
,	O
in	O
Arabic	O
,	O
the	O
distributions	O
of	O
predicted	O
tags	O
with	O
respect	O
to	O
case	O
,	O
possession	O
,	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
,	O
and	O
several	O
other	O
classes	O
differ	O
significantly	O
from	O
the	O
original	O
training	O
data	O
.	O
Such	O
difference	O
suggests	O
that	O
either	O
the	O
words	O
in	O
the	O
unlabeled	O
Wiki	O
Data	O
have	O
very	O
different	O
characteristics	O
than	O
our	O
training	O
set	O
,	O
or	O
our	O
tag	O
classifier	O
is	O
not	O
functioning	O
properly	O
to	O
identify	O
the	O
tags	O
.	O
Either	O
case	O
would	O
be	O
detrimental	O
to	O
semi	O
-	O
supervised	O
learning	O
.	O
The	O
problem	O
is	O
even	O
more	O
stark	O
for	O
Persian	O
:	O
in	O
Persian	O
the	O
only	O
labeled	O
words	O
in	O
the	O
training	O
data	O
are	O
verbs	O
,	O
so	O
all	O
nonverb	O
words	O
in	O
the	O
Wiki	O
Data	O
will	O
receive	O
an	O
incorrect	O
analysis	O
,	O
which	O
is	O
obviously	O
not	O
conducive	O
to	O
learning	O
anything	O
useful	O
.	O
As	O
a	O
recommendation	O
for	O
the	O
future	O
,	O
when	O
performing	O
semi	O
-	O
supervised	O
learning	O
for	O
morphology	O
where	O
the	O
labeled	O
data	O
only	O
represents	O
a	O
subset	O
of	O
the	O
phenomena	O
in	O
the	O
language	O
,	O
it	O
is	O
likely	O
necessary	O
to	O
first	O
identify	O
which	O
of	O
the	O
available	O
unlabeled	O
data	O
is	O
appropriate	O
for	O
semi	O
-	O
supervised	O
learning	O
before	O
applying	O
such	O
methods	O
.	O

In	O
Tab	O
.	O
1	O
,	O
we	O
notice	O
that	O
the	O
performance	O
on	O
Latin	O
is	O
relatively	O
poor	O
compared	O
with	O
other	O
languages	O
.	O
Latin	O
is	O
a	O
highly	O
inflected	O
languages	O
with	O
three	O
distinct	O
genders	O
,	O
seven	O
noun	O
cases	O
,	O
four	O
verb	O
conjugations	O
,	O
four	O
verb	O
principal	O
parts	O
,	O
six	O
tenses	O
,	O
three	O
persons	O
,	O
three	O
moods	O
,	O
two	O
voices	O
,	O
two	O
aspects	O
and	O
two	O
numbers	O
.	O
In	O
addition	O
to	O
this	O
,	O
we	O
found	O
that	O
the	O
data	O
set	O
size	O
after	O
augmentation	O
was	O
only	O
enlarged	O
2	O
times	O
.	O
We	O
examine	O
some	O
errors	O
made	O
by	O
our	O
system	O
on	O
two	O
worst	O
performed	O
languages	O
Latin	O
and	O
Icelandic	O
in	O
Tab	O
.	O
2	O
.	O
As	O
shown	O
in	O
the	O
table	O
,	O
we	O
found	O
that	O
the	O
inflections	O
of	O
Latin	O
and	O
Icelandic	O
have	O
more	O
suffix	O
variations	O
from	O
the	O
lemma	B-DatasetName
.	O
We	O
guess	O
our	O
model	O
still	O
lacks	O
the	O
ability	O
to	O
capture	O
more	O
complicated	O
inflections	O
for	O
such	O
languages	O
.	O
We	O
might	O
consider	O
adding	O
the	O
dependencies	O
between	O
different	O
inflections	O
for	O
multiple	O
target	O
labels	O
in	O
our	O
future	O
work	O
.	O

Non	O
-	O
Autoregressive	O
Text	B-TaskName
Generation	I-TaskName
with	O
Pre	O
-	O
trained	O
Language	O
Models	O

Non	O
-	O
autoregressive	O
generation	O
(	O
NAG	O
)	O
has	O
recently	O
attracted	O
great	O
attention	O
due	O
to	O
its	O
fast	O
inference	O
speed	O
.	O
However	O
,	O
the	O
generation	O
quality	O
of	O
existing	O
NAG	O
models	O
still	O
lags	O
behind	O
their	O
autoregressive	O
counterparts	O
.	O
In	O
this	O
work	O
,	O
we	O
show	O
that	O
BERT	B-MethodName
can	O
be	O
employed	O
as	O
the	O
backbone	O
of	O
a	O
NAG	O
model	O
to	O
greatly	O
improve	O
performance	O
.	O
Additionally	O
,	O
we	O
devise	O
mechanisms	O
to	O
alleviate	O
the	O
two	O
common	O
problems	O
of	O
vanilla	O
NAG	O
models	O
:	O
the	O
inflexibility	O
of	O
prefixed	O
output	O
length	O
and	O
the	O
conditional	O
independence	O
of	O
individual	O
token	O
predictions	O
.	O
Lastly	O
,	O
to	O
further	O
increase	O
the	O
speed	O
advantage	O
of	O
the	O
proposed	O
model	O
,	O
we	O
propose	O
a	O
new	O
decoding	O
strategy	O
,	O
ratio	O
-	O
first	O
,	O
for	O
applications	O
where	O
the	O
output	O
lengths	O
can	O
be	O
approximately	O
estimated	O
beforehand	O
.	O
For	O
a	O
comprehensive	O
evaluation	O
,	O
we	O
test	O
the	O
proposed	O
model	O
on	O
three	O
text	B-TaskName
generation	I-TaskName
tasks	O
,	O
including	O
text	B-TaskName
summarization	I-TaskName
,	O
sentence	B-DatasetName
compression	I-DatasetName
and	O
machine	B-TaskName
translation	I-TaskName
.	O
Experimental	O
results	O
show	O
that	O
our	O
model	O
significantly	O
outperforms	O
existing	O
non	O
-	O
autoregressive	O
baselines	O
and	O
achieves	O
competitive	O
performance	O
with	O
many	O
strong	O
autoregressive	O
models	O
.	O
In	O
addition	O
,	O
we	O
also	O
conduct	O
extensive	O
analysis	O
experiments	O
to	O
reveal	O
the	O
effect	O
of	O
each	O
proposed	O
component	O
.	O
1	O

In	O
this	O
section	O
,	O
we	O
give	O
a	O
detailed	O
explanation	O
of	O
the	O
proposed	O
model	O
.	O
First	O
,	O
we	O
describe	O
how	O
to	O
utilize	O
BERT	B-MethodName
as	O
a	O
non	O
-	O
autoregressive	O
generation	O
model	O
.	O
Then	O
we	O
discuss	O
the	O
decoding	O
mechanism	O
which	O
allows	O
the	O
model	O
to	O
determine	O
the	O
output	O
length	O
dynamically	O
.	O
Finally	O
,	O
we	O
introduce	O
the	O
new	O
ratiofirst	O
decoding	O
strategy	O
which	O
further	O
improves	O
the	O
model	O
's	O
decoding	O
efficiency	O
.	O

The	O
architecture	O
of	O
the	O
proposed	O
model	O
is	O
presented	O
in	O
Figure	O
2	O
,	O
in	O
which	O
the	O
embedding	O
layer	O
and	O
the	O
stack	O
of	O
transformer	O
layers	O
are	O
initialized	O
with	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
.	O
Input	O
Representation	O
Following	O
the	O
setup	O
of	O
BERT	B-MethodName
,	O
we	O
first	O
append	O
a	O
[	O
cls	O
]	O
and	O
a	O
[	O
sep	O
]	O
token	O
on	O
both	O
sides	O
of	O
the	O
source	O
sequence	O
.	O
Then	O
we	O
attach	O
a	O
number	O
of	O
[	O
pad	O
]	O
tokens	O
at	O
the	O
end	O
of	O
source	O
sequence	O
to	O
make	O
its	O
length	O
equal	O
to	O
the	O
predefined	O
maximum	O
size	O
(	O
e.g.	O
,	O
256	O
)	O
.	O
Thus	O
we	O
can	O
make	O
sure	O
the	O
source	O
length	O
is	O
longer	O
than	O
or	O
equal	O
to	O
the	O
output	O
length	O
.	O
As	O
a	O
special	O
case	O
,	O
for	O
tasks	O
like	O
text	B-TaskName
summarization	I-TaskName
where	O
the	O
source	O
is	O
known	O
to	O
be	O
longer	O
than	O
the	O
target	O
,	O
we	O
do	O
not	O
attach	O
the	O
[	O
pad	O
]	O
tokens	O
when	O
constructing	O
the	O
input	O
.	O
Transformer	B-MethodName
Layers	O
Given	O
the	O
source	O
sequence	O
X	O
,	O
it	O
is	O
processed	O
by	O
a	O
stack	O
of	O
N	O
transformer	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
layers	O
.	O
Formally	O
,	O
the	O
Multi	B-MethodName
-	I-MethodName
Head	I-MethodName
Attention	I-MethodName
is	O
defined	O
as	O
MultiHead	O
(	O
Q	O
,	O
K	O
,	O
V	O
)	O
,	O
where	O
Q	O
,	O
K	O
,	O
V	O
denotes	O
the	O
query	O
,	O
key	O
and	O
value	O
respectively	O
.	O
The	O
computation	O
of	O
the	O
first	O
transformer	O
layer	O
is	O
then	O
defined	O
as	O
:	O
V	O
(	O
1	O
)	O
=	O
MultiHead	O
(	O
E	O
(	O
X	O
)	O
,	O
E	O
(	O
X	O
)	O
,	O
E	O
(	O
X	O
)	O
)	O
,	O
(	O
3	O
)	O
O	O
(	O
1	O
)	O
=	O
FFN	O
(	O
V	O
(	O
1	O
)	O
)	O
,	O
(	O
4	O
)	O
FFN	O
(	O
x	O
)	O
=	O
max	O
(	O
0	B-DatasetName
,	O
xW	O
1	O
+	O
b	O
1	O
)	O
W	O
2	O
+	O
b	O
2	O
,	O
(	O
5	O
)	O
where	O
E	O
(	O
X	O
)	O
=	O
T	O
E	O
(	O
X	O
)	O
+	O
P	O
E	O
(	O
X	O
)	O
in	O
which	O
T	O
E	O
(	O
)	O
denotes	O
the	O
token	O
embedding	O
and	O
P	O
E	O
(	O
)	O
denotes	O
the	O
position	O
embedding	O
.	O
For	O
other	O
layers	O
:	O
V	O
(	O
n	O
)	O
=	O
MultiHead	O
(	O
O	O
(	O
n−1	O
)	O
,	O
O	O
(	O
n−1	O
)	O
,	O
O	O
(	O
n−1	O
)	O
)	O
,	O
(	O
6	O
)	O
O	O
(	O
n	O
)	O
=	O
FFN	O
(	O
V	O
(	O
n	O
)	O
)	O
,	O
(	O
7	O
)	O
where	O
n	O
=	O
2	O
,	O
...	O
,	O
N	O
and	O
N	O
is	O
the	O
total	O
number	O
of	O
transformer	O
layers	O
.	O
The	O
final	O
sequence	O
representation	O
H	O
R	O
T	O
×d	O
model	O
is	O
the	O
output	O
states	O
of	O
BERT	B-MethodName
from	O
the	O
last	O
layer	O
,	O
where	O
T	O
is	O
the	O
source	O
sequence	O
length	O
and	O
d	O
model	O
is	O
the	O
model	O
size	O
.	O
CRF	B-MethodName
Layer	O
Then	O
,	O
H	O
is	O
passed	O
through	O
a	O
linearchain	O
CRF	B-MethodName
(	O
Lafferty	O
et	O
al	O
,	O
2001	O
)	O
.	O
Under	O
the	O
CRF	B-MethodName
framework	O
,	O
the	O
likelihood	O
of	O
the	O
target	O
sequence	O
Y	O
with	O
length	O
T	O
is	O
then	O
modelled	O
as	O
:	O
P	O
CRF	B-MethodName
(	O
Y	O
|	O
X	O
)	O
=	O
e	O
S	O
(	O
X	O
,	O
Y	O
)	O
Y	O
e	O
S	O
(	O
X	O
,	O
Y	O
)	O
=	O
1	O
Z	O
(	O
X	O
)	O
exp	O
(	O
T	O
i=1	O
Φ	O
y	O
i	O
(	O
h	O
i	O
)	O
+	O
T	O
i=2	O
t	O
(	O
y	O
i−1	O
,	O
y	O
i	O
)	O
)	O
,	O
(	O
8	O
)	O
where	O
Z	O
(	O
X	O
)	O
is	O
the	O
normalizing	O
factor	O
and	O
Φ	O
y	O
i	O
(	O
h	O
i	O
)	O
denotes	O
the	O
label	O
score	O
of	O
y	O
i	O
at	O
position	O
i.	O
In	O
practice	O
,	O
Φ	O
is	O
parameterized	O
by	O
a	O
neural	O
network	O
that	O
maps	O
the	O
BERT	B-MethodName
output	O
state	O
h	O
i	O
into	O
the	O
label	O
(	O
vocabulary	O
)	O
space	O
.	O
The	O
t	O
(	O
y	O
i−1	O
,	O
y	O
i	O
)	O
=	O
T	O
y	O
i−1	O
,	O
y	O
i	O
denotes	O
the	O
transition	O
score	O
from	O
label	O
y	O
i−1	O
to	O
y	O
i	O
where	O
T	O
R	O
|	O
V	O
|	O
×	O
|	O
V	O
|	O
is	O
the	O
transition	O
matrix	O
.	O
Approximation	O
In	O
the	O
context	O
of	O
text	B-TaskName
generation	I-TaskName
,	O
the	O
size	O
of	O
the	O
label	O
space	O
(	O
vocabulary	O
size	O
)	O
|	O
V	O
|	O
is	O
typically	O
large	O
,	O
e.g.	O
,	O
32k	O
.	O
Therefore	O
,	O
it	O
is	O
intractable	O
to	O
directly	O
model	O
the	O
transition	O
matrix	O
T	O
and	O
the	O
normalizing	O
factor	O
Z	O
(	O
X	O
)	O
.	O
To	O
this	O
end	O
,	O
we	O
adopt	O
the	O
techniques	O
proposed	O
by	O
to	O
approximate	O
these	O
two	O
terms	O
.	O
Specifically	O
,	O
the	O
full	O
transition	O
matrix	O
is	O
approximated	O
by	O
the	O
product	O
of	O
two	O
low	O
-	O
rank	O
matrices	O
T	O
=	O
E	O
1	O
E	O
T	O
2	O
,	O
where	O
E	O
1	O
,	O
E	O
2	O
R	O
|	O
V	O
|	O
×d	O
and	O
d	O
is	O
much	O
smaller	O
than	O
|	O
V	O
|	O
.	O
To	O
compute	O
the	O
normalizing	O
factor	O
Z	O
(	O
X	O
)	O
,	O
at	O
each	O
time	O
step	O
,	O
instead	O
of	O
searching	O
through	O
all	O
possible	O
paths	O
,	O
the	O
number	O
of	O
candidates	O
is	O
heuristically	O
truncated	O
to	O
a	O
predefined	O
beam	O
size	O
k.	O
We	O
refer	O
readers	O
to	O
the	O
original	O
paper	O
for	O
further	O
details	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
how	O
to	O
let	O
the	O
model	O
determine	O
the	O
output	O
sequence	O
length	O
by	O
itself	O
.	O
Our	O
basic	O
idea	O
is	O
that	O
we	O
want	O
the	O
model	O
to	O
dynamically	O
stop	O
generation	O
via	O
emitting	O
a	O
special	O
[	O
eos	O
]	O
token	O
.	O
To	O
achieve	O
this	O
,	O
during	O
training	O
,	O
we	O
manually	O
append	O
two	O
consecutive	O
[	O
eos	O
]	O
tokens	O
to	O
the	O
end	O
of	O
the	O
target	O
sequence	O
,	O
as	O
shown	O
in	O
the	O
top	O
left	O
part	O
of	O
Figure	O
2	O
.	O
In	O
this	O
way	O
,	O
the	O
model	O
can	O
learn	O
a	O
deterministic	O
transition	O
behaviour	O
between	O
two	O
[	O
eos	O
]	O
states	O
,	O
meaning	O
that	O
t	O
(	O
[	O
eos	O
]	O
,	O
[	O
eos	O
]	O
)	O
=	O
max	O
v	O
V	O
t	O
(	O
[	O
eos	O
]	O
,	O
v	O
)	O
.	O
This	O
is	O
because	O
,	O
during	O
training	O
,	O
the	O
model	O
never	O
sees	O
a	O
transition	O
(	O
[	O
eos	O
]	O
,	O
v	O
)	O
,	O
where	O
v	O
=	O
[	O
eos	O
]	O
.	O
During	O
inference	O
,	O
the	O
resultỸ	O
is	O
acquired	O
as	O
Y	O
=	O
arg	O
max	O
Y	O
S	O
(	O
X	O
,	O
Y	O
)	O
,	O
where	O
the	O
CRF	B-MethodName
scoring	O
function	O
S	O
(	O
X	O
,	O
Y	O
)	O
in	O
Equation	O
(	O
8	O
)	O
can	O
be	O
decomposed	O
as	O
:	O
S	O
(	O
X	O
,	O
Y	O
)	O
=	O
T	O
i=1	O
Φ	O
y	O
i	O
(	O
h	O
i	O
)	O
+	O
T	O
i=2	O
t	O
(	O
y	O
i−1	O
,	O
y	O
i	O
)	O
=	O
Φ	O
y	O
1	O
(	O
h	O
1	O
)	O
initial	O
state	O
+	O
T	O
i=2	O
{	O
label	O
score	O
Φ	O
y	O
i	O
(	O
h	O
i	O
)	O
+	O
transition	O
score	O
t	O
(	O
y	O
i−1	O
,	O
y	O
i	O
)	O
state	O
transition	O
}	O
.	O
(	O
9	O
)	O
Once	O
the	O
decoded	O
trajectory	O
enters	O
the	O
[	O
eos	O
]	O
state	O
,	O
the	O
state	O
transition	O
term	O
in	O
S	O
(	O
X	O
,	O
Y	O
)	O
will	O
be	O
dominated	O
by	O
the	O
transition	O
score	O
term	O
t	O
(	O
[	O
eos	O
]	O
,	O
[	O
eos	O
]	O
)	O
.	O
As	O
a	O
result	O
,	O
the	O
model	O
will	O
keep	O
transitioning	O
to	O
[	O
eos	O
]	O
in	O
the	O
remaining	O
steps	O
.	O
An	O
example	O
is	O
provided	O
in	O
the	O
right	O
part	O
of	O
Figure	O
2	O
,	O
from	O
which	O
we	O
can	O
see	O
that	O
,	O
at	O
step	O
5	O
,	O
the	O
decoded	O
trajectory	O
enters	O
the	O
[	O
eos	O
]	O
state	O
and	O
remains	O
at	O
it	O
in	O
the	O
rest	O
of	O
the	O
generation	O
process	O
.	O
In	O
this	O
way	O
,	O
our	O
model	O
can	O
dynamically	O
control	O
the	O
length	O
of	O
output	O
sequence	O
by	O
entering	O
the	O
[	O
eos	O
]	O
state	O
during	O
the	O
generation	O
process	O
.	O
After	O
the	O
entire	O
generation	O
process	O
is	O
completed	O
,	O
the	O
final	O
output	O
sequence	O
can	O
be	O
obtained	O
by	O
removing	O
all	O
generated	O
[	O
eos	O
]	O
tokens	O
.	O

Non	O
-	O
Autoregressive	O
generation	O
was	O
first	O
introduced	O
by	O
Gu	O
et	O
al	O
(	O
2018	O
)	O
to	O
reduce	O
the	O
inference	O
latency	O
in	O
machine	B-TaskName
translation	I-TaskName
.	O
Recent	O
works	O
in	O
this	O
area	O
have	O
investigated	O
ways	O
to	O
mitigate	O
the	O
tradeoff	O
between	O
the	O
decoding	O
speed	O
and	O
generation	O
quality	O
.	O
Gu	O
et	O
al	O
(	O
2018	O
)	O
utilized	O
fertility	O
as	O
latent	O
variables	O
for	O
better	O
translation	O
performance	O
.	O
Wang	O
et	O
al	O
(	O
2019b	O
)	O
proposed	O
two	O
auxiliary	O
objectives	O
for	O
better	O
modelling	O
the	O
output	O
states	O
and	O
solving	O
the	O
under	O
-	O
translation	O
problem	O
.	O
To	O
better	O
model	O
the	O
intermediate	O
alignments	O
between	O
source	O
and	O
target	O
sides	O
,	O
Ma	O
et	O
al	O
(	O
2019	O
)	O
proposed	O
a	O
model	O
based	O
on	O
the	O
generative	O
flow	O
framework	O
.	O
Ghazvininejad	O
et	O
al	O
(	O
2019	O
)	O
proposed	O
to	O
use	O
a	O
masked	O
language	O
objective	O
to	O
train	O
the	O
NAG	O
model	O
.	O
During	O
inference	O
,	O
starting	O
from	O
a	O
fully	O
masked	O
sequence	O
,	O
the	O
output	O
is	O
generated	O
in	O
an	O
iterative	O
refinement	O
manner	O
.	O
Recently	O
,	O
proposed	O
to	O
incorporate	O
a	O
conditional	B-MethodName
random	I-MethodName
field	I-MethodName
into	O
the	O
decoder	O
of	O
a	O
NAG	O
model	O
for	O
better	O
modelling	O
the	O
outputside	O
dependencies	O
.	O
Our	O
work	O
is	O
different	O
from	O
prior	O
works	O
in	O
two	O
aspects	O
:	O
(	O
1	O
)	O
we	O
directly	O
utilize	O
a	O
pretrained	O
language	O
model	O
(	O
BERT	B-MethodName
)	O
to	O
perform	O
nonautoregressive	O
generation	O
;	O
(	O
2	O
)	O
our	O
model	O
can	O
dynamically	O
generate	O
the	O
output	O
sequence	O
without	O
the	O
need	O
of	O
prespecified	O
output	O
length	O
.	O

We	O
evaluate	O
the	O
proposed	O
model	O
on	O
three	O
typical	O
text	B-TaskName
generation	I-TaskName
tasks	O
:	O
(	O
1	O
)	O
text	B-TaskName
summarization	I-TaskName
;	O
(	O
2	O
)	O
sentence	B-DatasetName
compression	I-DatasetName
and	O
(	O
3	O
)	O
machine	B-TaskName
translation	I-TaskName
.	O

In	O
this	O
section	O
,	O
we	O
present	O
further	O
discussions	O
and	O
empirical	O
analysis	O
of	O
the	O
proposed	O
model	O
.	O
nents	O
,	O
the	O
overall	O
performance	O
decreases	O
.	O
By	O
removing	O
BERT	B-MethodName
from	O
the	O
model	O
,	O
we	O
observe	O
notable	O
drop	O
across	O
all	O
metrics	O
.	O
This	O
shows	O
that	O
the	O
knowledge	O
of	O
BERT	B-MethodName
is	O
an	O
important	O
factor	O
of	O
the	O
model	O
's	O
strong	O
performance	O
.	O
Comparing	O
with	O
results	O
in	O
Table	O
1	O
,	O
it	O
still	O
outperforms	O
vanilla	O
NAG	O
-	O
CRF	B-MethodName
and	O
performs	O
comparably	O
with	O
NAG	O
-	O
CRF	B-MethodName
using	O
LPD	O
decoding	O
,	O
which	O
demonstrates	O
the	O
merit	O
of	O
the	O
proposed	O
dynamic	O
length	O
decoding	O
mechanism	O
.	O
Another	O
interesting	O
finding	O
is	O
that	O
,	O
by	O
only	O
removing	O
the	O
CRF	B-MethodName
layer	O
,	O
the	O
most	O
notable	O
drop	O
is	O
observed	O
on	O
the	O
bigram	O
-	O
level	O
metric	O
.	O
This	O
shows	O
that	O
the	O
bigram	O
-	O
level	O
dependencies	O
on	O
the	O
output	O
side	O
are	O
mainly	O
captured	O
by	O
the	O
CRF	B-MethodName
module	O
.	O
In	O
addition	O
,	O
by	O
removing	O
both	O
BERT	B-MethodName
and	O
CRF	B-MethodName
,	O
all	O
metrics	O
further	O
decrease	O
.	O
This	O
confirms	O
that	O
each	O
of	O
these	O
two	O
components	O
positively	O
contributes	O
to	O
the	O
model	O
's	O
overall	O
performance	O
.	O

In	O
this	O
work	O
,	O
we	O
explored	O
the	O
potential	O
of	O
BERT	B-MethodName
in	O
various	O
text	B-TaskName
generation	I-TaskName
tasks	O
under	O
the	O
NAG	O
framework	O
.	O
To	O
address	O
problems	O
from	O
NAG	O
models	O
previously	O
having	O
a	O
prefixed	O
output	O
length	O
,	O
we	O
devised	O
a	O
decoding	O
mechanism	O
which	O
enables	O
the	O
model	O
to	O
determine	O
the	O
output	O
length	O
dynamically	O
.	O
To	O
reduce	O
errors	O
stemming	O
from	O
the	O
assumption	O
of	O
conditional	O
independence	O
of	O
output	O
tokens	O
,	O
we	O
proposed	O
a	O
context	O
-	O
aware	O
objective	O
as	O
well	O
as	O
using	O
a	O
CRF	B-MethodName
decoding	O
.	O
Furthermore	O
,	O
to	O
maximize	O
the	O
inference	O
speed	O
advantage	O
of	O
our	O
model	O
,	O
we	O
introduced	O
a	O
ratio	O
-	O
first	O
decoding	O
strategy	O
.	O
We	O
evaluated	O
our	O
model	O
on	O
three	O
benchmark	O
datasets	O
and	O
the	O
results	O
show	O
that	O
our	O
model	O
significantly	O
outperforms	O
many	O
strong	O
NAG	O
baselines	O
and	O
performs	O
comparably	O
to	O
many	O
strong	O
AG	O
models	O
.	O

Revision	O
is	O
an	O
essential	O
part	O
of	O
the	O
human	O
writing	O
process	O
.	O
It	O
tends	O
to	O
be	O
strategic	O
,	O
adaptive	O
,	O
and	O
,	O
more	O
importantly	O
,	O
iterative	O
in	O
nature	O
.	O
Despite	O
the	O
success	O
of	O
large	O
language	O
models	O
on	O
text	O
revision	O
tasks	O
,	O
they	O
are	O
limited	O
to	O
non	O
-	O
iterative	O
,	O
one	O
-	O
shot	O
revisions	O
.	O
Examining	O
and	O
evaluating	O
the	O
capability	O
of	O
large	O
language	O
models	O
for	O
making	O
continuous	O
revisions	O
and	O
collaborating	O
with	O
human	O
writers	O
is	O
a	O
critical	O
step	O
towards	O
building	O
effective	O
writing	O
assistants	O
.	O
In	O
this	O
work	O
,	O
we	O
present	O
a	O
human	O
-	O
inthe	O
-	O
loop	O
iterative	O
text	O
revision	O
system	O
,	O
Read	O
,	O
Revise	O
,	O
Repeat	O
(	O
R3	O
)	O
,	O
which	O
aims	O
at	O
achieving	O
high	O
quality	O
text	O
revisions	O
with	O
minimal	O
human	O
efforts	O
by	O
reading	O
model	O
-	O
generated	O
revisions	O
and	O
user	O
feedbacks	O
,	O
revising	O
documents	O
,	O
and	O
repeating	O
human	O
-	O
machine	O
interactions	O
.	O
In	O
R3	O
,	O
a	O
text	O
revision	O
model	O
provides	O
text	O
editing	O
suggestions	O
for	O
human	O
writers	O
,	O
who	O
can	O
accept	O
or	O
reject	O
the	O
suggested	O
edits	O
.	O
The	O
accepted	O
edits	O
are	O
then	O
incorporated	O
into	O
the	O
model	O
for	O
the	O
next	O
iteration	O
of	O
document	O
revision	O
.	O
Writers	O
can	O
therefore	O
revise	O
documents	O
iteratively	O
by	O
interacting	O
with	O
the	O
system	O
and	O
simply	O
accepting	O
/	O
rejecting	O
its	O
suggested	O
edits	O
until	O
the	O
text	O
revision	O
model	O
stops	O
making	O
further	O
revisions	O
or	O
reaches	O
a	O
predefined	O
maximum	O
number	O
of	O
revisions	O
.	O
Empirical	O
experiments	O
show	O
that	O
R3	O
can	O
generate	O
revisions	O
with	O
comparable	O
acceptance	O
rate	O
to	O
human	O
writers	O
at	O
early	O
revision	O
depths	O
,	O
and	O
the	O
human	O
-	O
machine	O
interaction	O
can	O
get	O
higher	O
quality	O
revisions	O
with	O
fewer	O
iterations	O
and	O
edits	O
.	O
The	O
collected	O
human	O
-	O
model	O
interaction	B-DatasetName
dataset	I-DatasetName
and	O
system	O
code	O
are	O
available	O
at	O
https://github	O
.	O
com	O
/	O
vipulraheja	O
/	O
IteraTeR.	O
Our	O
system	O
demonstration	O
is	O
available	O
at	O
https://	O
youtu.be/lK08tIpEoaE.	O

Text	O
revision	O
is	O
a	O
crucial	O
part	O
of	O
writing	O
.	O
Specifically	O
,	O
text	O
revision	O
involves	O
identifying	O
discrepan	O
-	O
cies	O
between	O
intended	O
and	O
instantiated	O
text	O
,	O
deciding	O
what	O
edits	O
to	O
make	O
,	O
and	O
how	O
to	O
make	O
those	O
desired	O
edits	O
(	O
Flower	O
and	O
Hayes	O
,	O
1981	O
;	O
Faigley	O
and	O
Witte	O
,	O
1981	O
;	O
Fitzgerald	O
,	O
1987	O
)	O
.	O
It	O
enables	O
writers	O
to	O
deliberate	O
over	O
and	O
organize	O
their	O
thoughts	O
,	O
find	O
a	O
better	O
line	O
of	O
argument	O
,	O
learn	O
afresh	O
,	O
and	O
discover	O
what	O
was	O
not	O
known	O
before	O
(	O
Sommers	O
,	O
1980	O
;	O
Scardamalia	O
,	O
1986	O
)	O
.	O
Previous	O
studies	O
(	O
Flower	O
,	O
1980	O
;	O
Collins	O
and	O
Gentner	O
,	O
1980	O
;	O
Vaughan	O
and	O
McDonald	O
,	O
1986	O
)	O
have	O
shown	O
that	O
text	O
revision	O
is	O
an	O
iterative	O
process	O
since	O
human	O
writers	O
are	O
unable	O
to	O
simultaneously	O
comprehend	O
multiple	O
demands	O
and	O
constraints	O
of	O
the	O
task	O
when	O
producing	O
well	O
-	O
written	O
texts	O
-	O
for	O
instance	O
,	O
covering	O
the	O
content	O
,	O
following	O
linguistic	O
norms	O
and	O
discourse	O
conventions	O
of	O
written	O
prose	O
,	O
etc	O
.	O
Therefore	O
,	O
writers	O
resort	O
to	O
performing	O
text	O
revisions	O
on	O
their	O
drafts	O
iteratively	O
to	O
reduce	O
the	O
number	O
of	O
considerations	O
at	O
each	O
time	O
.	O
Computational	O
modeling	O
of	O
the	O
iterative	O
text	O
revision	O
process	O
is	O
essential	O
for	O
building	O
intelligent	O
and	O
interactive	O
writing	O
assistants	O
.	O
Most	O
prior	O
works	O
on	O
the	O
development	O
of	O
neural	O
text	O
revision	O
systems	O
Botha	O
et	O
al	O
,	O
2018	O
;	O
Ito	O
et	O
al	O
,	O
2019	O
;	O
Faltings	O
et	O
al	O
,	O
2021	O
)	O
do	O
not	O
take	O
the	O
iterative	O
nature	O
of	O
text	O
revision	O
and	O
human	O
feedback	O
on	O
suggested	O
revisions	O
into	O
consideration	O
.	O
The	O
direct	O
application	O
of	O
such	O
revision	O
systems	O
in	O
an	O
iterative	O
way	O
,	O
however	O
,	O
could	O
generate	O
some	O
"	O
noisy	O
"	O
edits	O
and	O
require	O
much	O
burden	O
on	O
human	O
writers	O
to	O
fix	O
the	O
noise	O
.	O
Therefore	O
,	O
we	O
propose	O
to	O
collect	O
human	O
feedback	O
at	O
each	O
iteration	O
of	O
revision	O
to	O
filter	O
out	O
those	O
harmful	O
noisy	O
edits	O
and	O
produce	O
revised	O
documents	O
of	O
higher	O
quality	O
.	O
In	O
this	O
work	O
,	O
we	O
present	O
a	O
novel	O
human	O
-	O
in	O
-	O
theloop	O
iterative	O
text	O
revision	O
system	O
,	O
Read	O
,	O
Revise	O
,	O
Repeat	O
(	O
R3	O
)	O
,	O
which	O
reads	O
model	O
-	O
generated	O
revisions	O
and	O
user	O
feedbacks	O
,	O
revises	O
documents	O
,	O
and	O
repeats	O
human	O
-	O
machine	O
interactions	O
in	O
an	O
iterative	O
way	O
,	O
as	O
depicted	O
in	O
Figure	O
1	O
.	O
First	O
,	O
users	O
write	O
a	O
document	O
as	O
input	O
to	O
the	O
system	O
or	O
choose	O
one	O
from	O
a	O
candidate	O
document	O
set	O
to	O
edit	O
.	O
Then	O
,	O
the	O
text	O
revision	O
system	O
provides	O
multiple	O
editing	O
suggestions	O
with	O
their	O
edits	O
and	O
intents	O
.	O
Users	O
can	O
accept	O
or	O
reject	O
the	O
editing	O
suggestions	O
in	O
an	O
iterative	O
way	O
and	O
stop	O
revision	O
when	O
no	O
editing	O
suggestions	O
are	O
provided	O
or	O
the	O
model	O
reaches	O
the	O
maximum	O
revision	O
limit	O
.	O
The	O
overall	O
model	O
performance	O
can	O
be	O
estimated	O
by	O
calculating	O
the	O
acceptance	O
rate	O
throughout	O
all	O
editing	O
suggestions	O
.	O
R3	O
provides	O
numerous	O
benefits	O
over	O
existing	O
writing	O
assistants	O
for	O
text	O
revision	O
.	O
First	O
,	O
R3	O
improves	O
the	O
overall	O
writing	O
experience	O
for	O
writers	O
by	O
making	O
it	O
more	O
interpretable	O
,	O
controllable	O
,	O
and	O
productive	O
:	O
on	O
the	O
one	O
hand	O
,	O
writers	O
do	O
n't	O
have	O
to	O
(	O
re	O
-	O
)	O
read	O
the	O
parts	O
of	O
the	O
text	O
that	O
are	O
already	O
high	O
quality	O
,	O
and	O
this	O
,	O
in	O
turn	O
,	O
helps	O
them	O
focus	O
on	O
larger	O
writing	O
goals	O
(	O
4.2	O
)	O
;	O
on	O
the	O
other	O
hand	O
,	O
by	O
showing	O
edit	O
intentions	O
for	O
every	O
suggested	O
edit	O
,	O
which	O
users	O
can	O
further	O
decide	O
to	O
accept	O
or	O
reject	O
,	O
R3	O
provides	O
them	O
with	O
more	O
fine	O
-	O
grained	O
control	O
over	O
the	O
text	O
revision	O
process	O
compared	O
to	O
other	O
one	O
-	O
shot	O
based	O
text	O
revision	O
systems	O
(	O
Lee	O
et	O
al	O
,	O
2022	O
)	O
,	O
and	O
are	O
limited	O
in	O
both	O
interpretability	O
and	O
controllability	O
.	O
Second	O
,	O
R3	O
improves	O
the	O
revision	O
efficiency	O
.	O
The	O
human	O
-	O
machine	O
interaction	O
can	O
help	O
the	O
system	O
produce	O
higher	O
quality	O
revisions	O
with	O
fewer	O
iterations	O
and	O
edits	O
,	O
and	O
the	O
empirical	O
experiments	O
in	O
4.2	O
validate	O
this	O
claim	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
R3	O
is	O
the	O
first	O
text	O
revision	O
system	O
in	O
literature	O
that	O
can	O
perform	O
iterative	O
text	O
revision	O
in	O
collaboration	O
by	O
human	O
writers	O
and	O
revision	O
models	O
.	O
In	O
this	O
paper	O
,	O
we	O
make	O
three	O
major	O
contributions	O
:	O
We	O
present	O
a	O
novel	O
human	O
-	O
in	O
-	O
the	O
-	O
loop	O
text	O
revision	O
system	O
R3	O
to	O
make	O
text	O
revision	O
models	O
more	O
accessible	O
;	O
and	O
to	O
make	O
the	O
process	O
of	O
iterative	O
text	O
revision	O
efficient	O
,	O
productive	O
,	O
and	O
cognitively	O
less	O
challenging	O
.	O
From	O
an	O
HCI	O
perspective	O
,	O
we	O
conduct	O
experiments	O
to	O
measure	O
the	O
effectiveness	O
of	O
the	O
proposed	O
system	O
for	O
the	O
iterative	O
text	O
revision	O
task	O
.	O
Empirical	O
experiments	O
show	O
that	O
R3	O
can	O
generate	O
edits	O
with	O
comparable	O
acceptance	O
rate	O
to	O
human	O
writers	O
at	O
early	O
revision	O
depths	O
.	O
We	O
analyze	O
the	O
data	O
collected	O
from	O
humanmodel	O
interactions	O
for	O
text	O
revision	O
and	O
provide	O
insights	O
and	O
future	O
directions	O
for	O
building	O
high	O
-	O
quality	O
and	O
efficient	O
human	O
-	O
in	O
-	O
the	O
-	O
loop	O
text	O
revision	O
systems	O
.	O
We	O
release	O
our	O
code	O
,	O
revision	O
interface	O
,	O
and	O
collected	O
human	O
-	O
model	O
interaction	B-DatasetName
dataset	I-DatasetName
to	O
promote	O
future	O
research	O
on	O
collaborative	O
text	O
revision	O
.	O

Iterativeness	O
.	O
The	O
human	O
-	O
in	O
-	O
the	O
-	O
loop	O
iterative	O
text	O
revision	O
evaluation	O
results	O
are	O
reported	O
in	O
Table	O
2	O
.	O
Each	O
document	O
is	O
evaluated	O
by	O
at	O
least	O
2	O
users	O
.	O
We	O
find	O
that	O
R3	O
achieves	O
comparable	O
performances	O
with	O
ground	O
-	O
truth	O
human	O
revisions	O
at	O
revision	O
depth	O
1	O
and	O
2	O
,	O
and	O
tends	O
to	O
generate	O
less	O
favorable	O
edits	O
at	O
revision	O
depth	O
3	O
.	O
At	O
revision	O
depth	O
1	O
,	O
R3	O
is	O
able	O
to	O
generate	O
more	O
edits	O
than	O
ground	O
-	O
truth	O
human	O
edits	O
for	O
each	O
document	O
,	O
and	O
gets	O
more	O
edits	O
accepted	O
by	O
users	O
on	O
average	O
.	O
This	O
shows	O
the	O
potential	O
of	O
R3	O
in	O
generating	O
appropriate	O
text	O
revisions	O
that	O
are	O
more	O
favorable	O
to	O
users	O
.	O
At	O
revision	O
depth	O
2	O
,	O
while	O
R3	O
generates	O
less	O
edits	O
than	O
human	O
writers	O
on	O
average	O
,	O
it	O
gets	O
a	O
higher	O
acceptance	O
rate	O
than	O
human	O
writers	O
.	O
This	O
result	O
suggests	O
that	O
for	O
the	O
end	O
users	O
,	O
more	O
edits	O
may	O
not	O
necessarily	O
lead	O
to	O
a	O
higher	O
acceptance	O
ratio	O
,	O
and	O
shows	O
that	O
R3	O
is	O
able	O
to	O
make	O
high	O
-	O
quality	O
edits	O
for	O
effective	O
iterative	O
text	O
revisions	O
.	O
At	O
revision	O
depth	O
3	O
,	O
R3	O
generates	O
even	O
less	O
edits	O
compared	O
both	O
to	O
human	O
writers	O
and	O
its	O
previous	O
revision	O
depths	O
.	O
This	O
result	O
can	O
be	O
attributed	O
to	O
the	O
fact	O
that	O
our	O
models	O
are	O
only	O
trained	O
on	O
static	O
human	O
revision	O
data	O
,	O
while	O
at	O
testing	O
time	O
they	O
have	O
to	O
make	O
predictions	O
conditioned	O
on	O
their	O
revisions	O
generated	O
at	O
the	O
previous	O
depth	O
,	O
which	O
may	O
have	O
a	O
very	O
different	O
distribution	O
of	O
edits	O
than	O
the	O
training	O
data	O
.	O
Table	O
7	O
shows	O
an	O
example	O
of	O
iterative	O
text	O
revision	O
in	O
ArXiv	B-DatasetName
domain	O
generated	O
by	O
R3	O
.	O
We	O
also	O
provide	O
some	O
other	O
iterative	O
revision	O
examples	O
generated	O
by	O
R3	O
in	O
Appendix	O
A.	O
Edit	O
Intentions	O
.	O
Table	O
3	O
demonstrates	O
the	O
distribution	O
of	O
different	O
edit	O
intentions	O
,	O
which	O
can	O
help	O
us	O
further	O
analyze	O
the	O
which	O
type	O
of	O
edits	O
are	O
more	O
likely	O
to	O
be	O
accepted	O
by	O
end	O
users	O
.	O
For	O
humangenerated	O
revisions	O
,	O
we	O
find	O
that	O
FLUENCY	O
edits	O
are	O
most	O
likely	O
to	O
be	O
accepted	O
since	O
they	O
are	O
mainly	O
fixing	O
grammatical	O
errors	O
.	O
For	O
system	O
-	O
generated	O
revisions	O
,	O
we	O
observe	O
that	O
CLARITY	O
edits	O
are	O
the	O
most	O
frequent	O
edits	O
but	O
end	O
users	O
only	O
accept	O
58.73	O
%	O
of	O
them	O
,	O
which	O
suggests	O
that	O
our	O
system	O
needs	O
further	O
improvements	O
in	O
learning	O
CLARITY	O
edits	O
.	O
Another	O
interesting	O
observation	O
is	O
that	O
STYLE	O
edits	O
are	O
rarely	O
generated	O
by	O
human	O
writers	O
(	O
1.2	O
%	O
)	O
and	O
also	O
gets	O
the	O
lowest	O
acceptance	O
rate	O
(	O
33.33	O
%	O
)	O
than	O
other	O
intentions	O
,	O
while	O
they	O
are	O
frequently	O
generated	O
by	O
our	O
system	O
(	O
16.7	O
%	O
)	O
and	O
surprisingly	O
gets	O
the	O
highest	O
acceptance	O
rate	O
(	O
64.6	O
%	O
)	O
than	O
other	O
intentions	O
.	O
This	O
observation	O
indicates	O
that	O
R3	O
is	O
capable	O
for	O
generating	O
favorable	O
stylistic	O
edits	O
.	O
Table	O
4	O
shows	O
some	O
examples	O
of	O
edit	O
suggestions	O
generated	O
by	O
R3	O
.	O
Role	O
of	O
Human	O
Feedback	O
in	O
Revision	O
Quality	O
.	O
Table	O
3	O
:	O
The	O
distribution	O
of	O
different	O
edit	O
intentions	O
.	O
#	O
Edits	O
indicates	O
the	O
total	O
number	O
of	O
applied	O
edits	O
under	O
the	O
current	O
edit	O
intention	O
,	O
#	O
Accepts	O
means	O
the	O
total	O
number	O
of	O
edits	O
accepted	O
by	O
users	O
under	O
the	O
current	O
edit	O
intention	O
,	O
and	O
%	O
Accepts	O
is	O
calculated	O
by	O
dividing	O
the	O
total	O
accepted	O
edits	O
with	O
the	O
total	O
applied	O
edits	O
.	O
final	O
revised	O
documents	O
with	O
and	O
without	O
humanin	O
-	O
the	O
-	O
loop	O
for	O
R3	O
.	O
We	O
asked	O
another	O
group	O
of	O
three	O
annotators	O
(	O
English	O
L2	O
,	O
bachelor	O
's	O
or	O
higher	O
degree	O
in	O
Computer	O
Science	O
)	O
to	O
judge	O
whether	O
the	O
overall	O
quality	O
of	O
system	O
-	O
generated	O
final	O
document	O
is	O
better	O
than	O
the	O
ground	O
-	O
truth	O
reference	O
final	O
document	O
.	O
The	O
quality	O
score	O
ranges	O
between	O
0	B-DatasetName
and	O
1	O
.	O
We	O
evaluated	O
10	O
unique	O
documents	O
in	O
ArXiv	B-DatasetName
domain	O
,	O
and	O
took	O
the	O
average	O
score	O
from	O
all	O
3	O
annotators	O
.	O
As	O
shown	O
in	O
Table	O
5	O
,	O
SYSTEM	O
-	O
HUMAN	O
produces	O
better	O
overall	O
quality	O
score	O
for	O
the	O
final	O
system	O
-	O
generated	O
documents	O
with	O
fewer	O
iterations	O
of	O
revision	O
and	O
fewer	O
edits	O
,	O
which	O
validates	O
the	O
effectiveness	O
of	O
the	O
human	O
-	O
machine	O
interaction	O
proposed	O
in	O
R3	O
.	O
User	O
Feedback	O
.	O
We	O
also	O
collected	O
qualitative	O
feedback	O
about	O
R3	O
from	O
the	O
linguistic	O
experts	O
through	O
a	O
questionnaire	O
.	O
The	O
first	O
part	O
of	O
our	O
questionnaire	O
asks	O
participants	O
to	O
recall	O
their	O
experience	O
with	O
the	O
system	O
,	O
and	O
evaluate	O
various	O
aspects	O
of	O
the	O
system	O
(	O
in	O
Table	O
6	O
)	O
.	O
They	O
were	O
asked	O
to	O
rate	O
how	O
easy	O
it	O
was	O
to	O
get	O
onboarded	O
and	O
use	O
the	O
system	O
(	O
convenience	O
)	O
,	O
whether	O
they	O
were	O
satisfied	O
with	O
the	O
system	O
(	O
revision	O
quality	O
and	O
usage	O
experience	O
)	O
(	O
satisfaction	O
)	O
,	O
whether	O
they	O
felt	O
it	O
improved	O
their	O
productivity	O
for	O
text	O
revision	O
(	O
productivity	O
)	O
,	O
and	O
whether	O
they	O
would	O
like	O
to	O
use	O
the	O
system	O
again	O
(	O
retention	O
)	O
for	O
performing	O
revisions	O
on	O
their	O
documents	O
.	O
In	O
general	O
,	O
the	O
users	O
gave	O
positive	O
feedback	O
towards	O
the	O
ease	O
of	O
use	O
of	O
the	O
system	O
.	O
However	O
,	O
they	O
were	O
neutral	O
on	O
the	O
potential	O
productivity	O
impact	O
,	O
owing	O
to	O
the	O
lack	O
of	O
domain	O
knowledge	O
of	O
the	O
documents	O
they	O
were	O
evaluating	O
.	O
This	O
issue	O
could	O
be	O
mitigated	O
by	O
asking	O
users	O
to	O
revise	O
their	O
own	O
documents	O
of	O
interest	O
.	O
The	O
retention	O
and	O
satisfaction	O
scores	O
were	O
leaning	O
slightly	O
negative	O
,	O
which	O
was	O
explained	O
as	O
primarily	O
attributed	O
to	O
gaps	O
in	O
the	O
user	O
interface	O
design	O
(	O
eg	O
.	O
improperly	O
aligned	O
diffs	O
,	O
suboptimal	O
presentation	O
of	O
word	O
-	O
level	O
edits	O
,	O
etc	O
.	O
)	O
.	O
We	O
also	O
asked	O
them	O
to	O
provide	O
detailed	O
comments	O
on	O
their	O
experience	O
,	O
and	O
the	O
potential	O
impact	O
of	O
the	O
system	O
on	O
their	O
text	O
revision	O
experience	O
.	O
Specifically	O
,	O
upon	O
asking	O
the	O
users	O
whether	O
using	O
the	O
system	O
to	O
evaluate	O
the	O
model	O
-	O
suggested	O
edits	O
would	O
be	O
more	O
time	O
-	O
efficient	O
compared	O
to	O
actually	O
revising	O
the	O
document	O
themselves	O
,	O
we	O
received	O
many	O
useful	O
insights	O
that	O
help	O
better	O
design	O
better	O
interfaces	O
and	O
features	O
of	O
our	O
system	O
in	O
future	O
work	O
,	O
as	O
some	O
users	O
noted	O
:	O
I	O
think	O
it	O
would	O
be	O
faster	O
using	O
the	O
system	O
,	O
but	O
I	O
would	O
still	O
be	O
checking	O
the	O
text	O
myself	O
in	O
case	O
edits	O
were	O
missed	O
.	O
The	O
system	O
made	O
some	O
edits	O
where	O
there	O
were	O
letters	O
and	O
parts	O
of	O
words	O
being	O
added	O
/	O
re	O
-	O
moved	O
/	O
replaced	O
,	O
which	O
sometimes	O
took	O
some	O
time	O
to	O
figure	O
out	O
.	O
That	O
would	O
n't	O
be	O
the	O
case	O
if	O
I	O
were	O
editing	O
a	O
document	O
.	O
Ultimately	O
,	O
I	O
would	O
use	O
the	O
system	O
for	O
grammar	O
/	O
coherence	O
/	O
clarity	O
edits	O
,	O
and	O
then	O
still	O
research	O
(	O
a	O
lot	O
)	O
to	O
ensure	O
that	O
meaning	O
was	O
preserved	O
throughout	O
the	O
document	O
.	O
For	O
topics	O
that	O
I	O
was	O
more	O
familiar	O
with	O
/	O
more	O
general	O
topics	O
,	O
using	O
the	O
system	O
would	O
probably	O
reduce	O
my	O
time	O
by	O
a	O
third	O
or	O
so	O
.	O
For	O
topics	O
that	O
required	O
more	O
in	O
-	O
depth	O
research	O
for	O
me	O
,	O
the	O
time	O
saved	O
by	O
using	O
the	O
system	O
might	O
be	O
minimal	O
.	O

In	O
this	O
work	O
,	O
we	O
develop	O
an	O
interactive	O
iterative	O
text	O
revision	O
system	O
R3	O
that	O
is	O
able	O
to	O
effectively	O
assist	O
users	O
to	O
make	O
revisions	O
and	O
improve	O
the	O
quality	O
of	O
existing	O
documents	O
.	O
R3	O
can	O
generate	O
higher	O
quality	O
revisions	O
while	O
minimizing	O
the	O
human	O
efforts	O
.	O
Users	O
are	O
provided	O
with	O
a	O
reviewing	O
interface	O
to	O
accept	O
or	O
reject	O
system	O
suggesting	O
edits	O
.	O
The	O
user	O
-	O
validated	O
edits	O
are	O
then	O
propagated	O
to	O
the	O
next	O
revision	O
depth	O
to	O
get	O
further	O
improved	O
revisions	O
.	O
Empirical	O
results	O
show	O
that	O
R3	O
can	O
generate	O
iterative	O
text	O
revisions	O
with	O
acceptance	O
rates	O
comparable	O
or	O
even	O
better	O
than	O
human	O
writers	O
at	O
early	O
revision	O
depths	O
.	O
0	B-DatasetName
Due	O
to	O
its	O
high	O
lethality	O
amongst	O
the	O
elderly	O
,	O
nursing	O
homes	O
are	O
in	O
the	O
eye	O
of	O
the	O
COVID	O
-	O
19	O
storm	O
.	O
Emerging	O
new	O
test	O
procedures	O
,	O
such	O
as	O
antigen	O
or	O
RT	O
-	O
LAMP	O
tests	O
,	O
might	O
enable	O
us	O
to	O
protect	O
nursing	O
home	O
residents	O
by	O
means	O
of	O
preventive	O
screening	O
strategies	O
.	O
Here	O
,	O
we	O
develop	O
a	O
novel	O
agent	B-DatasetName
-	O
based	O
epidemiological	O
model	O
for	O
the	O
spread	O
of	O
SARS	O
-	O
CoV	O
-	O
2	O
in	O
nursing	O
homes	O
to	O
identify	O
optimal	O
preventive	O
testing	O
strategiesto	O
curb	O
this	O
spread	O
.	O
The	O
model	O
is	O
microscopically	O
calibrated	O
to	O
high	O
-	O
resolution	O
data	O
from	O
actual	O
nursing	O
homes	O
in	O
Austria	O
,	O
including	O
the	O
detailed	O
networks	O
of	O
social	O
contacts	O
of	O
their	O
residents	O
and	O
information	O
on	O
past	O
outbreaks	O
.	O
Due	O
to	O
its	O
high	O
lethality	O
amongst	O
the	O
elderly	O
,	O
nursing	O
homes	O
are	O
in	O
the	O
eye	O
of	O
the	O
COVID	O
-	O
19	O
storm	O
.	O
Emerging	O
new	O
test	O
procedures	O
,	O
such	O
as	O
antigen	O
or	O
RT	O
-	O
LAMP	O
tests	O
,	O
might	O
enable	O
us	O
to	O
protect	O
nursing	O
home	O
residents	O
by	O
means	O
of	O
preventive	O
screening	O
strategies	O
.	O
Here	O
,	O
we	O
develop	O
a	O
novel	O
agent	B-DatasetName
-	O
based	O
epidemiological	O
model	O
for	O
the	O
spread	O
of	O
SARS	O
-	O
CoV	O
-	O
2	O
in	O
nursing	O
homes	O
to	O
identify	O
optimal	O
preventive	O
testing	O
strategiesto	O
curb	O
this	O
spread	O
.	O
The	O
model	O
is	O
microscopically	O
calibrated	O
to	O
high	O
-	O
resolution	O
data	O
from	O
actual	O
nursing	O
homes	O
in	O
Austria	O
,	O
including	O
the	O
detailed	O
networks	O
of	O
social	O
contacts	O
of	O
their	O
residents	O
and	O
information	O
on	O
past	O
outbreaks	O
.	O
Here	O
,	O
we	O
develop	O
a	O
novel	O
detailed	O
agent	B-DatasetName
-	O
based	O
epidemiological	O
model	O
for	O
the	O
spread	O
of	O
SARS	O
-	O
CoV	O
-	O
2	O
in	O
nursing	O
homes	O
to	O
identify	O
optimal	O
preventive	O
testing	O
strategiesto	O
curb	O
this	O
spread	O
.	O
The	O
model	O
is	O
microscopically	O
calibrated	O
to	O
high	O
-	O
resolution	O
data	O
from	O
actual	O
nursing	O
homes	O
in	O
Austria	O
,	O
including	O
the	O
detailed	O
networks	O
of	O
social	O
contacts	O
of	O
their	O
resident	O
detailed	O
social	O
contact	O
networks	O
and	O
information	O
on	O
past	O
outbreaks	O
.	O
Due	O
to	O
its	O
high	O
lethality	O
amongst	O
the	O
elderly	O
,	O
n	O
N	O
ursing	O
homes	O
are	O
in	O
the	O
eye	O
of	O
the	O
COVID	O
-	O
19	O
storm	O
.	O
Emerging	O
new	O
test	O
procedures	O
might	O
enable	O
us	O
to	O
protect	O
nursing	O
home	O
residents	O
by	O
means	O
of	O
preventive	O
screening	O
strategies	O
.	O
Here	O
,	O
we	O
develop	O
a	O
novel	O
agent	B-DatasetName
-	O
based	O
epidemiological	O
model	O
for	O
the	O
spread	O
of	O
SARS	O
-	O
CoV	O
-	O
2	O
in	O
nursing	O
homes	O
to	O
identify	O
optimal	O
preventive	O
testing	O
strategies	O
.	O
The	O
model	O
is	O
calibrated	O
to	O
high	O
-	O
resolution	O
data	O
from	O
actual	O
nursing	O
homes	O
in	O
Austria	O
,	O
including	O
the	O
detailed	O
networks	O
of	O
social	O
contacts	O
of	O
their	O
residents	O
and	O
information	O
on	O
past	O
outbreaks	O
.	O

Due	O
to	O
its	O
high	O
lethality	O
amongst	O
the	O
elderly	O
,	O
nursing	O
homes	O
are	O
in	O
the	O
eye	O
of	O
the	O
COVID	O
-	O
19	O
storm	O
.	O
Emerging	O
new	O
test	O
procedures	O
might	O
enable	O
us	O
to	O
protect	O
nursing	O
home	O
residents	O
by	O
means	O
of	O
preventive	O
screening	O
.	O
Here	O
,	O
we	O
develop	O
a	O
novel	O
n	O
agent	B-DatasetName
-	O
based	O
epidemiological	O
model	O
for	O
the	O
spread	O
of	O
SARS	O
-	O
CoV	O
-	O
2	O
in	O
nursing	O
homes	O
to	O
identify	O
optimal	O
preventive	O
testing	O
strategies	O
.	O
The	O
model	O
is	O
calibrated	O
to	O
high	O
-	O
resolution	O
data	O
from	O
actual	O
nursing	O
homes	O
in	O
Austria	O
,	O
including	O
detailed	O
networks	O
of	O
social	O
contacts	O
of	O
their	O
residents	O
and	O
information	O
on	O
past	O
outbreaks	O
.	O
A	O
Canadian	O
Forces	O
statement	O
said	O
Cpl	O
.	O
Hornburg	O
was	O
killed	O
during	O
Operation	O
Sadiq	O
Sarbaaz	O
(	O
Honest	O
Soldier	O
)	O
approximately	O
47	O
kilometres	O
west	O
of	O
Kandahar	O
City	O
in	O
Panjwaii	O
District	O
,	O
a	O
joint	O
Afghan	O
-	O
NATO	O
mission	O
designed	O
to	O
"	O
set	O
the	O
conditions	O
for	O
a	O
continuous	O
security	O
presence	O
and	O
the	O
establishment	O
of	O
a	O
new	O
police	O
sub	O
-	O
station	O
in	O
the	O
northern	O
part	O
of	O
(	O
Panjwaii	O
)	O
.	O
"	O
.	O
Media	O
reports	O
indicated	O
he	O
died	O
from	O
mortar	O
fire	O
at	O
around	O
4	O
:	O
30	O
p.m.	O
local	O
time	O
(	O
12:00	O
UTC	O
)	O
while	O
he	O
was	O
repairing	O
the	O
track	O
on	O
a	O
Canadian	O
Leopard	O
tank	O
near	O
a	O
cluster	O
of	O
villages	O
known	O
as	O
Zangabad	O
.	O
A	O
Canadian	O
soldier	O
serving	O
with	O
the	O
Canadian	O
Forces	O
in	O
Afghanistanwas	O
killed	O
on	O
September	O
24	O
,	O
2007	O
.	O
Four	O
others	O
were	O
injured	O
in	O
the	O
incident	O
which	O
killed	O
24	O
-	O
year	O
-	O
old	O
Corporal	O
Nathan	O
Hornburg	O
of	O
Calgary	O
,	O
Alberta	O
.	O
Nathan	O
Hornburg	O
was	O
killed	O
during	O
Operation	O
Sadiq	O
Sarbaaz	O
(	O
Honest	O
Soldier	O
)	O
,	O
approximately	O
47	O
kilometres	O
west	O
of	O
Kandahar	O
City	O
in	O
Panjwaii	O
District	O
.	O
Media	O
reports	O
indicated	O
he	O
died	O
from	O
mortar	O
fire	O
at	O
around	O
4	O
:	O
30	O
p.m.	O
local	O
time	O
(	O
12:00	O
UTC	O
)	O
while	O
he	O
was	O
repairing	O
the	O
track	O
on	O
a	O
Canadian	O
Leopard	O
tank	O
near	O
a	O
cluster	O
of	O
villages	O
known	O
as	O
Zangabad	O
.	O

We	O
present	O
more	O
iterative	O
revision	O
examples	O
generated	O
by	O
R3	O
in	O
Table	O
8	O
and	O
Table	O
9	O
.	O
t	O
HUMAN	O
-	O
HUMAN	O
SYSTEM	O
-	O
HUMAN	O
(	O
ours	O
)	O
0	B-DatasetName
Jecon	O
Gregory	O
is	O
or	O
was	O
a	O
nomadic	O
artist	O
,	O
whose	O
autobiographical	O
fragments	O
and	O
poems	O
,	O
dictated	O
to	O
an	O
acquaintance	O
,	O
were	O
published	O
as	O
the	O
book	O
"	O
History	O
of	O
a	O
Nation	O
of	O
One	O
"	O
(	O
Harcourt	O
Brace	O
,	O
New	O
York	O
,	O
1969	O
,	O
andMichael	O
Joseph	O
,	O
London	O
,	O
1971	O
)	O
.	O
Jecon	O
apparently	O
did	O
not	O
know	O
his	O
place	O
,	O
date	O
,	O
language	O
or	O
even	O
name	O
of	O
birth	O
,	O
began	O
his	O
wanderings	O
as	O
a	O
child	O
in	O
Malta	O
;	O
walked	O
through	O
many	O
lands	O
,	O
barefoot	O
,	O
tall	O
and	O
thin	O
,	O
pulling	O
all	O
his	O
possessions	O
in	O
a	O
basket	O
on	O
wheels	O
,	O
sleeping	O
on	O
the	O
ground	O
,	O
and	O
making	O
a	O
living	O
by	O
drawing	O
portraits	O
.	O
Jecon	O
Gregory	O
is	O
or	O
was	O
a	O
nomadic	O
artist	O
,	O
whose	O
autobiographical	O
fragments	O
and	O
poems	O
,	O
dictated	O
to	O
an	O
acquaintance	O
,	O
were	O
published	O
as	O
the	O
book	O
"	O
History	O
of	O
a	O
Nation	O
of	O
One	O
"	O
(	O
Harcourt	O
Brace	O
,	O
New	O
York	O
,	O
1969	O
,	O
andMichael	O
Joseph	O
,	O
London	O
,	O
1971	O
)	O
.	O
Jecon	O
apparently	O
did	O
not	O
know	O
his	O
place	O
,	O
date	O
,	O
language	O
or	O
even	O
name	O
of	O
birth	O
,	O
began	O
his	O
wanderings	O
as	O
a	O
child	O
in	O
Malta	O
;	O
walked	O
through	O
many	O
lands	O
,	O
barefoot	O
,	O
tall	O
and	O
thin	O
,	O
pulling	O
all	O
his	O
possessions	O
in	O
a	O
basket	O
on	O
wheels	O
,	O
sleeping	O
on	O
the	O
ground	O
,	O
and	O
making	O
a	O
living	O
by	O
drawing	O
portraits	O
.	O
1	O
Jecon	O
Gregory	O
is	O
or	O
was	O
a	O
nomadic	O
artist	O
,	O
whose	O
autobiographical	O
fragments	O
and	O
poems	O
,	O
dictated	O
to	O
an	O
acquaintance	O
,	O
were	O
published	O
as	O
the	O
book	O
"	O
History	O
of	O
a	O
Nation	O
of	O
One	O
:	O
An	O
Unlikely	O
Memoir	O
"	O
(	O
Harcourt	O
Brace	O
,	O
New	O
York	O
,	O
1969	O
,	O
andMichael	O
Joseph	O
,	O
London	O
,	O
1971	O
)	O
.	O
..	O
Jecon	O
apparently	O
did	O
not	O
know	O
his	O
place	O
,	O
date	O
,	O
language	O
or	O
even	O
name	O
of	O
birth	O
,	O
began	O
his	O
wanderings	O
as	O
a	O
child	O
in	O
Malta	O
;	O
walked	O
through	O
many	O
lands	O
,	O
barefoot	O
,	O
tall	O
and	O
thin	O
,	O
pulling	O
all	O
his	O
possessions	O
in	O
a	O
basket	O
on	O
wheels	O
,	O
sleeping	O
on	O
the	O
ground	O
,	O
and	O
making	O
a	O
living	O
by	O
drawing	O
portraits	O
.	O
Jecon	O
Gregory	O
is	O
or	O
was	O
a	O
nomadic	O
artist	O
,	O
whose	O
autobiographical	O
fragments	O
and	O
poems	O
,	O
dictated	O
to	O
an	O
acquaintance	O
,	O
were	O
published	O
as	O
the	O
book	O
"	O
History	O
of	O
a	O
Nation	O
of	O
One	O
"	O
(	O
Harcourt	O
Brace	O
,	O
New	O
York	O
,	O
1969	O
,	O
andMichael	O
Joseph	O
,	O
London	O
,	O
1971	O
)	O
.	O
Jecon	O
apparently	O
did	O
not	O
know	O
his	O
place	O
,	O
date	O
,	O
language	O
or	O
even	O
name	O
of	O
birth	O
,	O
began	O
his	O
wanderings	O
as	O
a	O
child	O
in	O
Malta	O
;	O
walked	O
through	O
many	O
lands	O
,	O
barefoot	O
,	O
tall	O
and	O
thin	O
,	O
pulling	O
all	O
his	O
possessions	O
in	O
a	O
basket	O
on	O
wheels	O
,	O
sleeping	O
on	O
the	O
ground	O
,	O
and	O
making	O
a	O
living	O
by	O
drawing	O
portraits	O
.	O

Semantic	O
Modelling	O
of	O
Adjective	O
-	O
Noun	O
Collocations	O
Using	O
FrameNet	B-DatasetName

In	O
this	O
paper	O
we	O
argue	O
that	O
Frame	O
Semantics	O
(	O
Fillmore	O
,	O
1982	O
)	O
provides	O
a	O
good	O
framework	O
for	O
semantic	O
modelling	O
of	O
adjective	O
-	O
noun	O
collocations	O
.	O
More	O
specifically	O
,	O
the	O
notion	O
of	O
a	O
frame	O
is	O
rich	O
enough	O
to	O
account	O
for	O
nouns	O
from	O
different	O
semantic	O
classes	O
and	O
to	O
model	O
semantic	O
relations	O
that	O
hold	O
between	O
an	O
adjective	O
and	O
a	O
noun	O
in	O
terms	O
of	O
Frame	O
Elements	O
.	O
We	O
have	O
substantiated	O
these	O
findings	O
by	O
considering	O
a	O
sample	O
of	O
adjectivenoun	O
collocations	O
from	O
German	O
such	O
as	O
enger	O
Freund	O
'	O
close	O
friend	O
'	O
and	O
starker	O
Regen	O
'	O
heavy	O
rain	O
'	O
.	O
The	O
data	O
sample	O
is	O
taken	O
from	O
different	O
semantic	O
fields	O
identified	O
in	O
the	O
German	O
wordnet	O
GermaNet	O
(	O
Hamp	O
and	O
Feldweg	O
,	O
1997	O
;	O
Henrich	O
and	O
Hinrichs	O
,	O
2010	O
)	O
.	O
The	O
study	O
is	O
based	O
on	O
the	O
electronic	O
dictionary	O
DWDS	O
(	O
Klein	O
and	O
Geyken	O
,	O
2010	O
)	O
and	O
uses	O
the	O
collocation	O
extraction	O
tool	O
Wortprofil	O
(	O
Geyken	O
et	O
al	O
,	O
2009	O
)	O
.	O
The	O
FrameNet	B-DatasetName
modelling	O
is	O
based	O
on	O
the	O
online	O
resource	O
available	O
at	O
http://framenet.icsi.berkeley.edu	O
.	O
Since	O
FrameNets	O
are	O
available	O
for	O
a	O
range	O
of	O
typologically	O
different	O
languages	O
,	O
it	O
is	O
feasible	O
to	O
extend	O
the	O
current	O
case	O
study	O
to	O
other	O
languages	O
.	O

Collocations	O
such	O
as	O
to	O
make	O
a	O
mistake	O
and	O
black	O
coffee	O
are	O
multi	O
-	O
word	O
expressions	O
(	O
MWEs	O
)	O
in	O
which	O
the	O
choice	O
of	O
one	O
constituent	O
(	O
base	O
)	O
is	O
free	O
,	O
and	O
the	O
choice	O
of	O
the	O
other	O
one	O
(	O
collocate	O
)	O
is	O
restricted	O
and	O
depends	O
on	O
the	O
base	O
(	O
Wanner	O
et	O
al	O
,	O
2006	O
)	O
.	O
Collocations	O
are	O
in	O
the	O
grey	O
area	O
between	O
free	O
phrases	O
like	O
black	O
car	O
and	O
idiomatic	O
MWEs	O
such	O
as	O
black	O
sheep	O
,	O
and	O
in	O
some	O
cases	O
it	O
is	O
challenging	O
to	O
draw	O
the	O
line	O
between	O
those	O
concepts	O
.	O
As	O
opposed	O
to	O
mere	O
co	O
-	O
occurrences	O
of	O
words	O
based	O
on	O
their	O
frequencies	O
,	O
collocations	O
show	O
a	O
certain	O
degree	O
of	O
lexical	O
rigidity	O
which	O
results	O
in	O
their	O
partial	O
lexicalization	O
.	O
This	O
creates	O
difficulties	O
for	O
the	O
non	O
-	O
native	O
speakers	O
when	O
interpreting	O
and	O
especially	O
producing	O
such	O
expressions	O
because	O
a	O
substitution	O
of	O
the	O
restricted	O
component	O
with	O
a	O
synonymous	O
word	O
is	O
not	O
allowed	O
by	O
the	O
language	O
(	O
Bartsch	O
,	O
2004	O
)	O
.	O
Therefore	O
,	O
combinations	O
such	O
as	O
*	O
to	O
do	O
a	O
mistake	O
or	O
*	O
dark	O
coffee	O
are	O
not	O
acceptable	O
and	O
sound	O
unnatural	O
to	O
the	O
native	O
speakers	O
,	O
but	O
they	O
still	O
can	O
be	O
interpreted	O
correctly	O
.	O
Idiomatic	O
MWEs	O
such	O
as	O
black	O
sheep	O
are	O
semantically	O
opaque	O
and	O
belong	O
to	O
the	O
domain	O
of	O
figurative	O
language	O
.	O
In	O
spite	O
of	O
the	O
fact	O
that	O
collocations	O
have	O
been	O
getting	O
more	O
attention	O
in	O
the	O
recent	O
decades	O
,	O
there	O
is	O
a	O
lack	O
of	O
systematic	O
empirical	O
studies	O
on	O
their	O
semantic	O
properties	O
.	O
Most	O
of	O
the	O
previous	O
corpus	O
studies	O
of	O
collocations	O
are	O
concerned	O
with	O
their	O
statistical	O
properties	O
and	O
the	O
ways	O
to	O
improve	O
methods	O
of	O
automatic	O
collocation	O
extraction	O
(	O
Church	O
et	O
al	O
,	O
1991	O
;	O
Smadja	O
,	O
1993	O
;	O
Evert	O
,	O
2004	O
;	O
Pecina	O
,	O
2008	O
;	O
Bouma	O
,	O
2009	O
)	O
.	O
These	O
authors	O
have	O
shown	O
that	O
automatic	O
and/or	O
manual	O
extraction	O
of	O
collocations	O
is	O
not	O
an	O
easy	O
task	O
.	O
Our	O
research	O
does	O
not	O
attempt	O
to	O
contribute	O
to	O
this	O
growing	O
body	O
of	O
research	O
.	O
Rather	O
,	O
we	O
focus	O
on	O
the	O
classification	O
and	O
modelling	O
of	O
semantic	O
relations	O
that	O
hold	O
between	O
a	O
base	O
and	O
its	O
collocate	O
,	O
e.g.	O
the	O
relation	O
of	O
degree	O
that	O
holds	O
between	O
the	O
collocate	O
heavy	O
and	O
its	O
nominal	O
base	O
rain	O
.	O
More	O
specifically	O
,	O
we	O
will	O
focus	O
on	O
the	O
semantic	O
relations	O
that	O
hold	O
in	O
adjective	O
-	O
noun	O
collocations	O
,	O
since	O
such	O
collocations	O
have	O
received	O
considerably	O
less	O
attention	O
than	O
verb	O
-	O
noun	O
collocations	O
.	O
In	O
our	O
research	O
,	O
we	O
utilize	O
existing	O
lexical	O
resources	O
that	O
reliably	O
identify	O
adjective	O
-	O
noun	O
collocations	O
.	O
For	O
purely	O
opportunistic	O
reasons	O
,	O
we	O
have	O
chosen	O
German	O
as	O
our	O
language	O
of	O
investigation	O
since	O
there	O
are	O
a	O
number	O
of	O
digital	O
resources	O
for	O
German	O
,	O
including	O
the	O
DWDS	O
(	O
short	O
for	O
the	O
Digitales	O
Wörterbuch	O
der	O
deutschen	O
Sprache	O
)	O
(	O
Klein	O
and	O
Geyken	O
,	O
2010	O
)	O
and	O
GermaNet	O
(	O
Hamp	O
and	O
Feldweg	O
,	O
1997	O
;	O
Henrich	O
and	O
Hinrichs	O
,	O
2010	O
)	O
,	O
that	O
offer	O
a	O
broad	O
coverage	O
of	O
adjectives	O
and	O
nouns	O
as	O
the	O
two	O
word	O
classes	O
under	O
investigation	O
.	O
The	O
remainder	O
of	O
this	O
paper	O
is	O
structured	O
as	O
follows	O
:	O
Section	O
2	O
introduces	O
the	O
notion	O
of	O
collocation	O
in	O
more	O
detail	O
and	O
describes	O
the	O
related	O
work	O
on	O
the	O
semantic	O
classification	O
of	O
collocations	O
.	O
Section	O
3	O
presents	O
our	O
own	O
proposal	O
of	O
how	O
to	O
deal	O
with	O
semantics	O
of	O
collocations	O
;	O
we	O
argue	O
that	O
the	O
notion	O
of	O
a	O
semantic	O
frame	O
in	O
the	O
sense	O
of	O
FrameNet	B-DatasetName
(	O
Ruppenhofer	O
et	O
al	O
,	O
2016	O
)	O
provides	O
a	O
suitably	O
general	O
semantic	O
framework	O
that	O
is	O
applicable	O
to	O
a	O
wide	O
range	O
of	O
semantic	O
fields	O
.	O
Furthermore	O
,	O
we	O
argue	O
that	O
collocations	O
offer	O
an	O
interesting	O
empirical	O
domain	O
for	O
validating	O
the	O
structure	O
of	O
semantic	O
frames	O
and	O
for	O
further	O
developing	O
the	O
FrameNet	B-DatasetName
framework	O
itself	O
.	O
The	O
paper	O
concludes	O
with	O
summary	O
of	O
our	O
approach	O
and	O
with	O
the	O
discussion	O
of	O
different	O
directions	O
for	O
future	O
work	O
.	O

Following	O
the	O
logic	O
of	O
Nesselhauf	O
(	O
2003	O
)	O
and	O
Mel'čuk	O
(	O
1998	O
)	O
,	O
we	O
consider	O
the	O
following	O
types	O
of	O
statistical	O
co	O
-	O
occurrences	O
true	O
collocations	O
:	O
1	O
.	O
the	O
collocate	O
has	O
a	O
specific	O
sense	O
with	O
a	O
limited	O
number	O
of	O
words	O
from	O
different	O
semantic	O
fields	O
,	O
e.g.	O
'	O
heavy	O
'	O
as	O
intensifier	O
:	O
heavy	O
smoker	O
,	O
heavy	O
rain	O
,	O
heavy	O
traffic	O
.	O
The	O
adjective	O
's	O
sense	O
is	O
not	O
prototypical	O
,	O
since	O
it	O
does	O
not	O
refer	O
to	O
the	O
physical	O
weight	O
,	O
but	O
to	O
intensity	O
.	O
2	O
.	O
the	O
collocate	O
has	O
a	O
specific	O
sense	O
only	O
with	O
one	O
or	O
very	O
few	O
semantically	O
related	O
bases	O
,	O
e.g.	O
black	O
coffee	O
.	O
The	O
adjective	O
's	O
sense	O
here	O
is	O
not	O
prototypical	O
,	O
since	O
it	O
does	O
not	O
refer	O
to	O
the	O
colour	O
,	O
but	O
to	O
the	O
fact	O
,	O
that	O
no	O
dairy	O
products	O
are	O
added	O
to	O
the	O
coffee	O
.	O
3	O
.	O
the	O
sense	O
of	O
the	O
collocate	O
is	O
so	O
specific	O
that	O
it	O
can	O
be	O
used	O
with	O
only	O
one	O
or	O
very	O
few	O
semantically	O
closely	O
related	O
bases	O
,	O
e.g.	O
aquiline	O
nose	O
/	O
face	O
(	O
Mel'čuk	O
,	O
1998	O
)	O
.	O
That	O
is	O
the	O
adjective	O
's	O
only	O
sense	O
.	O
As	O
our	O
empirical	O
basis	O
we	O
rely	O
on	O
the	O
electronic	O
dictionary	O
DWDS	O
.	O
The	O
DWDS	O
contains	O
a	O
rich	O
lexicographic	O
treatment	O
of	O
collocations	O
on	O
the	O
basis	O
of	O
the	O
collocation	O
extraction	O
tool	O
Wortprofil	O
(	O
Geyken	O
et	O
al	O
,	O
2009	O
)	O
.	O
Figure	O
1	O
shows	O
an	O
excerpt	O
of	O
the	O
Wortprofil	O
for	O
the	O
German	O
noun	O
Freund	O
'	O
friend	O
'	O
.	O
1	O
It	O
illustrates	O
the	O
information	O
contained	O
in	O
such	O
a	O
word	O
profile	O
.	O
As	O
Wanner	O
(	O
2006	O
)	O
emphasizes	O
,	O
collocation	O
extraction	O
typically	O
only	O
results	O
in	O
lists	O
of	O
collocations	O
that	O
are	O
classified	O
according	O
to	O
their	O
morphosyntactic	O
structure	O
,	O
but	O
that	O
do	O
not	O
provide	O
any	O
semantic	O
information	O
about	O
the	O
combinations	O
.	O
Semantic	O
modelling	O
of	O
collocations	O
requires	O
a	O
theoretical	O
framework	O
with	O
a	O
rich	O
inventory	O
that	O
can	O
be	O
used	O
for	O
describing	O
the	O
relations	O
between	O
the	O
base	O
and	O
its	O
collocate	O
.	O
Such	O
an	O
inventory	O
is	O
offered	O
in	O
the	O
form	O
of	O
Lexical	O
Functions	O
(	O
LFs	O
)	O
in	O
Mel'čuk	O
's	O
Meaning	O
↔Text	O
Theory	O
(	O
Mel'čuk	O
,	O
1996	O
)	O
.	O
A	O
LF	O
is	O
a	O
function	O
in	O
the	O
mathematical	O
sense	O
:	O
f	O
(	O
x	O
)	O
=	O
y	O
,	O
where	O
a	O
general	O
and	O
abstract	O
sense	O
f	O
is	O
expressed	O
by	O
a	O
certain	O
lexical	O
unit	O
y	O
depending	O
on	O
the	O
lexical	O
unit	O
x	O
it	O
is	O
associated	O
with	O
(	O
Mel'čuk	O
,	O
1995	O
)	O
.	O
The	O
number	O
of	O
standard	O
LFs	O
is	O
limited	O
to	O
about	O
60	O
,	O
and	O
they	O
have	O
fixed	O
names	O
,	O
e.g.	O
for	O
intensifiers	O
the	O
LF	O
Magn	O
is	O
suggested	O
:	O
Magn	O
[	O
RAIN	O
]	O
=	O
heavy	O
.	O
For	O
other	O
cases	O
the	O
non	O
-	O
standard	O
LFs	O
are	O
suggested	O
.	O
They	O
are	O
very	O
specific	O
,	O
and	O
their	O
names	O
are	O
formulated	O
in	O
a	O
natural	O
language	O
:	O
e.g.	O
obtained	O
in	O
an	O
illegal	O
way	O
[	O
MONEY	O
]	O
=	O
dirty	O
.	O
LFs	O
have	O
been	O
widely	O
used	O
in	O
lexicographic	O
projects	O
on	O
describing	O
French	O
semantic	O
derivations	O
and	O
collocations	O
(	O
Polguere	O
,	O
2000	O
)	O
,	O
and	O
have	O
also	O
been	O
implemented	O
in	O
the	O
Spanish	O
online	O
dictionary	O
of	O
collocations	O
(	O
DiCE	O
)	O
that	O
focuses	O
on	O
describing	O
emotion	B-DatasetName
lexemes	O
(	O
Vincze	O
et	O
al	O
,	O
2011	O
)	O
.	O
Mel'čuk	O
and	O
Wanner	O
(	O
1994	O
)	O
employ	O
LFs	O
to	O
represent	O
collocation	O
information	O
for	O
German	O
lexemes	O
from	O
the	O
semantic	O
field	O
of	O
emotions	O
.	O
Wanner	O
(	O
2004	O
)	O
conducts	O
experiments	O
on	O
automatic	O
classification	O
of	O
Spanish	O
verb	O
-	O
noun	O
collocations	O
based	O
on	O
the	O
typology	O
of	O
LFs	O
,	O
and	O
continues	O
to	O
work	O
on	O
this	O
problem	O
using	O
different	O
algorithms	O
(	O
Wanner	O
et	O
al	O
,	O
2006	O
)	O
.	O
The	O
works	O
by	O
Wanner	O
(	O
2004	O
;	O
2006	O
)	O
mostly	O
concentrate	O
on	O
verbal	O
collocations	O
,	O
for	O
which	O
the	O
Meaning	O
-	O
Text	O
Theory	O
provides	O
at	O
least	O
24	O
simple	O
verbal	O
LFs	O
that	O
can	O
further	O
be	O
combined	O
into	O
complex	O
LFs	O
.	O
By	O
comparison	O
,	O
adjective	O
-	O
noun	O
collocations	O
have	O
received	O
less	O
attention	O
and	O
the	O
set	O
of	O
proposed	O
adjectival	O
LFs	O
is	O
relatively	O
small	O
:	O
there	O
are	O
six	O
simple	O
adjectival	O
LFs	O
(	O
Mel'čuk	O
,	O
2015	O
)	O
.	O
Thus	O
,	O
our	O
main	O
objective	O
is	O
to	O
find	O
a	O
suitable	O
framework	O
for	O
describing	O
adjectival	O
collocations	O
.	O
Jousse	O
(	O
2007	O
)	O
proposes	O
a	O
way	O
of	O
formalizing	O
non	O
-	O
standard	O
adjectival	O
LFs	O
through	O
assign	O
-	O
ing	O
attributes	O
to	O
the	O
base	O
word	O
,	O
e.g.	O
shape	O
,	O
size	O
,	O
colour	O
,	O
function	O
.	O
These	O
attributes	O
can	O
be	O
compared	O
to	O
Frame	O
Elements	O
in	O
Frame	O
Semantics	O
(	O
Fillmore	O
,	O
1982	O
)	O
and	O
to	O
the	O
Qualia	O
Roles	O
in	O
the	O
theory	O
of	O
Generative	O
Lexicon	O
by	O
J.	O
Pustejovsky	O
(	O
1991	O
)	O
.	O
Qualia	O
roles	O
have	O
been	O
implemented	O
as	O
the	O
underlying	O
framework	O
in	O
the	O
construction	O
of	O
SIMPLE	O
lexicon	O
(	O
Bel	O
et	O
al	O
,	O
2000	O
)	O
.	O
While	O
they	O
are	O
easily	O
applicable	O
for	O
the	O
treatment	O
of	O
concrete	O
nouns	O
,	O
they	O
fail	O
to	O
suitably	O
generalize	O
the	O
semantics	O
of	O
abstract	O
nouns	O
.	O
By	O
contrast	O
,	O
the	O
concept	O
of	O
semantic	O
roles	O
in	O
Frame	O
Semantics	O
is	O
not	O
restricted	O
to	O
concrete	O
nouns	O
,	O
but	O
applies	O
equally	O
well	O
to	O
other	O
semantic	O
fields	O
as	O
well	O
(	O
for	O
details	O
see	O
section	O
3	O
below	O
)	O
.	O
The	O
main	O
idea	O
of	O
Frame	O
Semantics	O
is	O
that	O
word	O
meanings	O
are	O
defined	O
relative	O
to	O
a	O
set	O
of	O
semantic	O
frames	O
,	O
which	O
represent	O
non	O
-	O
linguistic	O
entities	O
such	O
as	O
events	O
,	O
states	O
of	O
affairs	O
,	O
beliefs	O
,	O
and	O
emotions	O
,	O
and	O
which	O
are	O
evoked	O
by	O
the	O
use	O
of	O
corresponding	O
words	O
in	O
a	O
particular	O
language	O
.	O
Semantic	O
Frames	O
for	O
English	O
are	O
described	O
in	O
the	O
lexical	O
database	O
FrameNet	B-DatasetName
(	O
FrameNet	B-DatasetName
-	O
Database	O
)	O
in	O
terms	O
of	O
Frame	O
Elements	O
(	O
FEs	O
)	O
(	O
Ruppenhofer	O
et	O
al	O
,	O
2016	O
)	O
.	O
The	O
database	O
provides	O
a	O
rich	O
coverage	O
of	O
nouns	O
and	O
adjectives	O
from	O
different	O
semantic	O
fields	O
,	O
currently	O
there	O
are	O
5558	O
nouns	O
and	O
2396	O
adjectives	O
,	O
and	O
the	O
resource	O
is	O
under	O
further	O
development	O
.	O
The	O
further	O
advantage	O
of	O
FrameNet	B-DatasetName
is	O
that	O
it	O
can	O
be	O
adapted	O
for	O
other	O
languages	O
.	O
As	O
demonstrated	O
by	O
Boas	O
(	O
2005	O
)	O
and	O
Padó	O
(	O
2007	O
)	O
,	O
a	O
transfer	O
of	O
existing	O
frame	O
annotations	O
from	O
English	O
to	O
other	O
languages	O
is	O
possible	O
:	O
there	O
is	O
a	O
high	O
degree	O
of	O
cross	O
-	O
lingual	O
parallelism	O
both	O
for	O
frames	O
(	O
70	O
%	O
)	O
and	O
for	O
Frame	O
Elements	O
(	O
90	O
%	O
)	O
(	O
Padó	O
,	O
2007	O
)	O
.	O
For	O
the	O
reasons	O
outlined	O
above	O
,	O
we	O
will	O
use	O
Frame	O
El	O
-	O
ements	O
in	O
the	O
sense	O
of	O
FrameNet	B-DatasetName
for	O
the	O
semantic	O
modelling	O
of	O
adjective	O
-	O
noun	O
collocations	O
.	O

As	O
motivated	O
in	O
the	O
previous	O
section	O
,	O
the	O
main	O
objective	O
of	O
this	O
study	O
is	O
to	O
develop	O
a	O
framework	O
for	O
semantic	O
modelling	O
of	O
German	O
adjectivenoun	O
collocations	O
.	O
To	O
assess	O
the	O
applicability	O
of	O
FrameNet	B-DatasetName
for	O
modelling	O
of	O
collocations	O
,	O
we	O
have	O
investigated	O
eleven	O
frames	O
for	O
nouns	O
from	O
various	O
semantic	O
fields	O
(	O
see	O
Table	O
1	O
)	O
.	O
The	O
corresponding	O
semantic	O
fields	O
were	O
assigned	O
according	O
to	O
the	O
information	O
from	O
the	O
German	O
wordnet	O
GermaNet	O
,	O
and	O
the	O
estimates	O
about	O
the	O
degree	O
of	O
concreteness	O
of	O
the	O
chosen	O
nouns	O
are	O
provided	O
by	O
the	O
MRC	O
Psycholinguistic	O
Database	O
(	O
Wilson	O
,	O
1988	O
)	O
.	O
The	O
nominal	O
bases	O
have	O
been	O
chosen	O
on	O
the	O
basis	O
of	O
frequency	O
and	O
richness	O
of	O
collocates	O
.	O
The	O
stage	O
of	O
choosing	O
the	O
candidates	O
for	O
modelling	O
showed	O
that	O
there	O
are	O
significant	O
differences	O
in	O
the	O
behaviour	O
of	O
concrete	O
and	O
abstract	O
nouns	O
:	O
the	O
latter	O
ones	O
have	O
a	O
greater	O
number	O
and	O
a	O
richer	O
variety	O
of	O
collocates	O
(	O
see	O
Table	O
2	O
)	O
.	O
As	O
explained	O
in	O
the	O
previous	O
section	O
,	O
we	O
employ	O
English	O
FrameNet	B-DatasetName
for	O
German	O
collocations	O
.	O
Semantic	O
Frames	O
in	O
FrameNet	B-DatasetName
describe	O
non	O
-	O
linguistic	O
concepts	O
and	O
deal	O
with	O
meanings	O
rather	O
than	O
with	O
particular	O
lexical	O
units	O
in	O
a	O
language	O
.	O
Thus	O
,	O
a	O
correct	O
translation	O
of	O
the	O
target	O
German	O
word	O
into	O
English	O
makes	O
it	O
possible	O
to	O
apply	O
the	O
information	O
contained	O
in	O
the	O
English	O
FrameNet	B-DatasetName
to	O
German	O
data	O
.	O
In	O
collocations	O
,	O
it	O
is	O
only	O
the	O
collocate	O
(	O
the	O
adjective	O
)	O
that	O
is	O
language	O
specific	O
,	O
and	O
thus	O
is	O
problematic	O
to	O
translate	O
.	O
However	O
,	O
we	O
consider	O
the	O
semantically	O
transparent	O
base	O
(	O
noun	O
)	O
to	O
be	O
the	O
frame	O
-	O
evoking	O
word	O
,	O
and	O
such	O
words	O
do	O
not	O
cause	O
any	O
difficulties	O
for	O
translation	O
.	O

The	O
number	O
of	O
true	O
collocates	O
for	O
concrete	O
nouns	O
is	O
relatively	O
small	O
due	O
to	O
several	O
reasons	O
.	O
First	O
of	O
all	O
,	O
when	O
combined	O
with	O
concrete	O
nouns	O
,	O
most	O
adjectives	O
retain	O
their	O
prototypical	O
meaning	O
:	O
enge	O
Straße	O
'	O
narrow	O
street	O
'	O
,	O
großes	O
Haus	O
'	O
big	O
house	O
'	O
,	O
hoher	O
Turm	O
'	O
tall	O
tower	O
'	O
,	O
such	O
expressions	O
are	O
considered	O
free	O
phrases	O
.	O
In	O
addition	O
,	O
there	O
are	O
a	O
lot	O
of	O
cases	O
where	O
a	O
concrete	O
noun	O
is	O
part	O
of	O
an	O
idiomatic	O
expression	O
.	O
2	O
(	O
Wilson	O
,	O
1988	O
)	O
indicate	O
the	O
level	O
of	O
concreteness	O
of	O
the	O
nouns	O
(	O
in	O
the	O
range	O
100	O
to	O
700	O
)	O
.	O
When	O
concrete	O
nouns	O
do	O
form	O
true	O
collocations	O
,	O
the	O
sense	O
of	O
their	O
collocates	O
is	O
not	O
prototypical	O
,	O
yet	O
it	O
is	O
highly	O
conventionalized	O
.	O
Consider	O
the	O
following	O
collocates	O
of	O
the	O
word	O
Schokolade	O
'	O
chocolate	O
'	O
:	O
schwarz	O
lit	O
.	O
'black	O
'	O
,	O
dunkel	O
'	O
dark	O
'	O
,	O
weiß	O
'	O
white	O
'	O
.	O
In	O
FrameNet	O
the	B-DatasetName
lexical	O
unit	O
(	O
LU	O
)	O
'	O
chocolate	O
'	O
evokes	O
the	O
frame	O
"	O
Food	O
"	O
with	O
Frame	O
Elements	O
(	O
FEs	O
)	O
FOOD	O
,	O
CONSTITUENT	O
PARTS	O
,	O
DESCRIPTOR	O
,	O
and	O
TYPE	O
.	O
Although	O
it	O
is	O
true	O
that	O
dark	O
chocolate	O
has	O
a	O
darker	O
colour	O
than	O
milk	O
chocolate	O
,	O
when	O
we	O
use	O
the	O
expression	O
dunkle	O
Schokolade	O
,	O
we	O
do	O
not	O
refer	O
to	O
the	O
colour	O
of	O
the	O
product	O
,	O
but	O
to	O
the	O
fact	O
that	O
it	O
contains	O
a	O
high	O
percentage	O
of	O
cocoa	O
and	O
little	O
or	O
no	O
milk	O
.	O
The	O
same	O
is	O
true	O
for	O
weiße	O
Schokolade	O
'	O
white	O
chocolate	O
'	O
:	O
it	O
indeed	O
has	O
a	O
very	O
light	O
colour	O
,	O
but	O
it	O
is	O
due	O
to	O
the	O
fact	O
that	O
such	O
type	O
of	O
chocolate	O
is	O
made	O
of	O
cocoa	O
butter	O
and	O
does	O
not	O
contain	O
cocoa	O
powder	O
.	O
FrameNet	O
offers	B-DatasetName
a	O
suitable	O
FE	O
TYPE	O
for	O
describing	O
the	O
relation	O
that	O
holds	O
between	O
these	O
adjectives	O
and	O
the	O
noun	O
.	O
It	O
is	O
defined	O
in	O
FrameNet	O
as	B-DatasetName
follows	O
:	O
"	O
This	O
FE	O
identifies	O
a	O
particular	O
Type	O
of	O
the	O
food	O
item	O
"	O
(	O
FrameNet	O
-	B-DatasetName
Database	O
)	O
.	O
A	O
similar	O
logic	O
is	O
applied	O
to	O
the	O
collocates	O
of	O
the	O
noun	O
Droge	O
'	O
drug	O
'	O
:	O
the	O
collocates	O
hart	O
'	O
hard	O
'	O
,	O
weich	O
'	O
soft	O
'	O
,	O
and	O
leicht	O
'	O
light	O
'	O
are	O
accommodated	O
by	O
the	O
FE	O
TYPE	O
within	O
the	O
frame	O
"	O
Intoxicants	O
"	O
.	O
3	O
In	O
the	O
case	O
of	O
the	O
artefact	O
Schuh	O
'	O
shoe	O
'	O
,	O
there	O
are	O
only	O
two	O
collocates	O
(	O
hochhackig	O
'	O
high	O
-	O
heeled	O
'	O
,	O
flach	O
'	O
flat	O
'	O
)	O
and	O
the	O
corresponding	O
frame	O
"	O
Clothing	O
"	O
offers	O
a	O
suit	O
-	O
When	O
a	O
noun	O
is	O
less	O
concrete	O
,	O
e.g.	O
Regen	O
'	O
rain	O
'	O
that	O
is	O
a	O
natural	O
phenomenon	O
and	O
thus	O
is	O
a	O
process	O
,	O
the	O
list	O
of	O
its	O
collocates	O
is	O
longer	O
.	O
The	O
noun	O
evokes	O
the	O
frame	O
"	O
Precipitation	O
"	O
and	O
all	O
the	O
collocates	O
are	O
accommodated	O
by	O
the	O
suitable	O
frame	O
elements	O
.	O
For	O
example	O
,	O
under	O
QUANTITY	O
the	O
following	O
attributes	O
are	O
found	O
:	O
sintflutartig	O
'	O
torrential	O
'	O
,	O
stark	O
'	O
heavy	O
'	O
,	O
kräftig	O
'	O
heavy	O
'	O
,	O
leicht	O
'	O
light	O
'	O
.	O
All	O
those	O
adjectives	O
describe	O
rain	O
in	O
terms	O
of	O
the	O
amount	O
of	O
water	O
that	O
falls	O
in	O
the	O
process	O
.	O
The	O
same	O
is	O
true	O
for	O
the	O
modifier	O
strömend	O
'	O
pouring	O
'	O
,	O
however	O
,	O
it	O
carries	O
an	O
extra	O
meaning	O
of	O
the	O
manner	O
in	O
which	O
it	O
can	O
rain	O
and	O
is	O
therefore	O
assigned	O
to	O
the	O
FE	O
MANNER	O
.	O

Abstract	O
concepts	O
have	O
a	O
complex	O
meaning	O
which	O
is	O
reflected	O
in	O
the	O
amount	O
of	O
semantic	O
roles	O
describing	O
the	O
corresponding	O
frame	O
and	O
in	O
the	O
amount	O
of	O
attributes	O
through	O
which	O
the	O
semantic	O
roles	O
are	O
realised	O
in	O
the	O
language	O
.	O
For	O
instance	O
,	O
according	O
to	O
the	O
FrameNet	B-DatasetName
Database	O
(	O
FrameNet	B-DatasetName
-	O
Database	O
)	O
,	O
the	O
frame	O
"	O
Personal	O
relationship	O
"	O
evoked	O
by	O
the	O
noun	O
Freund	O
'	O
friend	O
'	O
has	O
the	O
following	O
non	O
-	O
core	O
FEs	O
:	O
Relationship	O
:	O
The	O
Relationship	O
between	O
Partners	O
.	O
Source	O
of	O
relationship	O
:	O
The	O
source	O
of	O
the	O
relationship	O
.	O
The	O
semantic	O
roles	O
as	O
well	O
as	O
the	O
name	O
of	O
the	O
frame	O
suggest	O
that	O
,	O
in	O
many	O
contexts	O
,	O
the	O
word	O
'	O
friend	O
'	O
does	O
not	O
refer	O
to	O
a	O
person	O
as	O
a	O
human	O
being	O
of	O
certain	O
age	O
,	O
appearance	O
,	O
ethnicity	O
,	O
etc	O
.	O
,	O
but	O
to	O
the	O
relationship	O
people	O
are	O
engaged	O
in	O
.	O
In	O
German	O
,	O
the	O
adjectives	O
eng	O
lit	O
.	O
'	O
narrow	O
'	O
or	O
dick	O
lit	O
.	O
'	O
thick	O
'	O
are	O
both	O
used	O
with	O
Freund	O
in	O
the	O
sense	O
'	O
close	O
'	O
,	O
thus	O
describing	O
the	O
DEGREE	O
of	O
friendship	O
.	O
The	O
collocate	O
alt	O
'	O
old	O
'	O
implies	O
that	O
the	O
friendship	O
has	O
lasted	O
for	O
some	O
time	O
to	O
the	O
moment	O
of	O
speaking	O
and	O
can	O
therefore	O
be	O
accommodate	O
by	O
the	O
FE	O
DURATION	O
.	O
When	O
using	O
wahr	O
'	O
true	O
'	O
,	O
echt	O
'	O
real	O
'	O
,	O
falsch	O
'	O
fake	O
'	O
in	O
connection	O
with	O
friendship	O
,	O
we	O
refer	O
to	O
its	O
quality	O
,	O
the	O
most	O
suitable	O
FE	O
of	O
that	O
kind	O
in	O
this	O
case	O
is	O
MANNER	O
.	O
There	O
are	O
also	O
borderline	O
cases	O
,	O
when	O
the	O
suitable	O
FE	O
is	O
not	O
obvious	O
,	O
as	O
in	O
the	O
case	O
of	O
the	O
word	O
fest	O
'	O
steady	O
'	O
(	O
lit	O
.	O
'	O
solid	O
'	O
)	O
.	O
At	O
first	O
glance	O
,	O
the	O
modifier	O
characterizes	O
MAN	O
-	O
NER	B-TaskName
;	O
however	O
,	O
in	O
German	O
,	O
the	O
expression	O
fester	O
Freund	O
means	O
'	O
boyfriend	O
'	O
that	O
actually	O
refers	O
to	O
the	O
nature	O
of	O
the	O
relationship	O
between	O
the	O
partners	O
.	O
Therefore	O
,	O
the	O
most	O
suitable	O
FE	O
for	O
that	O
adjective	O
is	O
RELATIONSHIP	O
.	O
All	O
the	O
adjectival	O
modifiers	O
find	O
corresponding	O
semantic	O
roles	O
,	O
however	O
,	O
not	O
all	O
the	O
FEs	O
are	O
realised	O
through	O
adjectives	O
and	O
some	O
of	O
the	O
slots	O
such	O
as	O
MEANS	O
or	O
DEPICTIVE	O
are	O
left	O
empty	O
.	O
Such	O
unrealised	O
FEs	O
are	O
not	O
listed	O
in	O
Table	O
2	O
.	O
An	O
accurate	O
mapping	O
of	O
collocates	O
to	O
corresponding	O
FEs	O
is	O
possible	O
for	O
other	O
semantic	O
fields	O
as	O
well	O
.	O
Consider	O
an	O
example	O
from	O
the	O
field	O
of	O
cognition	O
:	O
Interesse	O
'	O
interest	O
'	O
.	O
In	O
FrameNet	B-DatasetName
it	O
evokes	O
the	O
frame	O
"	O
Emotion	O
directed	O
"	O
.	O
It	O
has	O
an	O
EXPERIENCER	O
referred	O
to	O
by	O
the	O
adjectives	O
ureigen	O
'	O
vested	O
'	O
and	O
widerstreitend	O
'	O
conflicting	O
'	O
;	O
MANNER	O
(	O
rege	O
'	O
active	O
'	O
,	O
lebhaft	O
'	O
lively	O
'	O
,	O
vital	O
'	O
lively	O
'	O
,	O
echt	O
'	O
genuine	O
'	O
,	O
and	O
wahr	O
'	O
genuine	O
'	O
)	O
;	O
TOPIC	O
(	O
materiell	O
'	O
material	O
'	O
)	O
;	O
PARAMETER	O
(	O
breit	O
'	O
wide	O
'	O
,	O
handfest	O
'	O
concrete	O
'	O
,	O
elementar	O
'	O
fundamental	O
'	O
,	O
and	O
vital	O
'	O
vital	O
'	O
)	O
;	O
and	O
CIRCUMSTANCES	O
(	O
unmittelbar	O
'	O
direct	O
'	O
)	O
.	O
It	O
also	O
has	O
a	O
property	O
of	O
intensity	O
described	O
in	O
the	O
frame	O
as	O
DEGREE	O
.	O
This	O
FE	O
accommodates	O
the	O
collocates	O
groß	O
'	O
strong	O
'	O
,	O
stark	O
'	O
strong	O
'	O
,	O
hoch	O
'	O
strong	O
'	O
,	O
and	O
massiv	O
'	O
massive	O
'	O
.	O
A	O
similar	O
pattern	O
is	O
found	O
for	O
the	O
emotion	B-DatasetName
noun	O
Angst	O
'	O
fear	O
'	O
.	O
Consider	O
its	O
collocates	O
:	O
groß	O
'	O
strong	O
'	O
,	O
nackt	O
'	O
pure	O
'	O
,	O
höllisch	O
'	O
hellish	O
'	O
,	O
panisch	O
'	O
panic	O
'	O
,	O
pur	O
'	O
pure	O
'	O
,	O
unterschwellig	O
'	O
subconscious	O
'	O
,	O
blank	O
'	O
sheer	O
'	O
,	O
diffus	O
'	O
vague	O
'	O
,	O
tief	O
'	O
deep	O
'	O
,	O
dumpf	O
'	O
vague	O
'	O
,	O
existenziell	O
'	O
existential	O
'	O
,	O
krankhaft	O
'	O
pathological	O
'	O
The	O
identified	O
relevant	O
FEs	O
are	O
as	O
follows	O
(	O
FrameNet	B-DatasetName
-	O
Database	O
)	O
:	O
Degree	O
:	O
The	O
extent	O
to	O
which	O
the	O
Experiencer	O
's	O
emotion	B-DatasetName
deviates	O
from	O
the	O
norm	O
for	O
the	O
emotion	B-DatasetName
.	O
Circumstances	O
:	O
The	O
Circumstances	O
is	O
the	O
condition	O
(	O
s	O
)	O
under	O
which	O
the	O
Stimulus	O
evokes	O
its	O
response	O
.	O
In	O
some	O
cases	O
it	O
may	O
appear	O
without	O
an	O
explicit	O
Stimulus	O
.	O
Quite	O
often	O
in	O
such	O
cases	O
,	O
the	O
Stimulus	O
can	O
be	O
inferred	O
from	O
the	O
Circumstances	O
.	O
Manner	O
:	O
Any	O
description	O
of	O
the	O
way	O
in	O
which	O
the	O
Experiencer	O
experiences	O
the	O
Stimulus	O
which	O
is	O
not	O
covered	O
by	O
more	O
specific	O
FEs	O
,	O
including	O
secondary	O
effects	O
(	O
quietly	O
,	O
loudly	O
)	O
,	O
and	O
general	O
descriptions	O
comparing	O
events	O
(	O
the	O
same	O
way	O
)	O
.	O
Manner	O
may	O
also	O
describe	O
a	O
state	O
of	O
the	O
Experiencer	O
that	O
affects	O
the	O
details	O
of	O
the	O
emotional	O
experience	O
.	O
The	O
interpretation	O
of	O
some	O
collocates	O
is	O
straightforward	O
:	O
the	O
adjective	O
existenziell	O
'	O
existential	O
'	O
indicates	O
the	O
area	O
of	O
the	O
stimulus	O
and	O
is	O
modelled	O
as	O
TOPIC	O
.	O
The	O
collocates	O
groß	O
'	O
strong	O
'	O
and	O
tief	O
'	O
deep	O
'	O
are	O
used	O
as	O
intensifiers	O
and	O
are	O
,	O
therefore	O
,	O
assigned	O
to	O
the	O
FE	O
DEGREE	O
.	O
The	O
word	O
höllisch	O
'	O
hellish	O
'	O
is	O
frequently	O
used	O
as	O
an	O
intensifier	O
with	O
Schmerz	O
'	O
pain	O
'	O
and	O
carries	O
the	O
same	O
meaning	O
with	O
'	O
fear	O
'	O
,	O
thus	O
it	O
is	O
also	O
assigned	O
to	O
DE	O
-	O
GREE	O
.	O
The	O
other	O
adjectives	O
do	O
not	O
reveal	O
any	O
information	O
about	O
the	O
intensity	O
of	O
the	O
experienced	O
emotion	B-DatasetName
:	O
blank	O
'	O
sheer	O
'	O
,	O
pur	O
'	O
pure	O
'	O
,	O
and	O
nackt	O
'	O
pure	O
'	O
rather	O
imply	O
that	O
,	O
at	O
a	O
particular	O
moment	O
,	O
fear	O
is	O
the	O
only	O
emotion	B-DatasetName
guiding	O
the	O
behaviour	O
of	O
a	O
person	O
.	O
This	O
interpretation	O
fits	O
the	O
definition	O
of	O
MANNER	O
,	O
and	O
so	O
do	O
the	O
collocates	O
diffus	O
'	O
vague	O
'	O
and	O
dumpf	O
'	O
vague	O
'	O
.	O
The	O
remaining	O
three	O
adjectives	O
(	O
panisch	O
,	O
unterschwellig	O
,	O
krankhaft	O
)	O
could	O
also	O
be	O
assigned	O
to	O
MANNER	O
,	O
however	O
,	O
there	O
is	O
more	O
information	O
in	O
their	O
meaning	O
than	O
it	O
may	O
seem	O
.	O
These	O
collocations	O
are	O
very	O
close	O
to	O
psychological	O
terms	O
,	O
as	O
well	O
as	O
'	O
existential	O
'	O
,	O
but	O
they	O
refer	O
to	O
certain	O
conditions	O
under	O
which	O
fear	O
might	O
be	O
experienced	O
rather	O
than	O
to	O
the	O
area	O
of	O
the	O
stimulus	O
.	O
In	O
such	O
cases	O
context	O
is	O
helpful	O
;	O
consider	O
the	O
following	O
examples	O
from	O
the	O
DWDS	O
-	O
Wortprofil	O
for	O
the	O
noun	O
Angst	O
4	O
:	O
1	O
.	O
Deshalb	O
habe	O
die	O
Frau	O
panische	O
Angst	O
vor	O
ihrem	O
sehr	O
dominanten	O
Mann	O
gehabt	O
.	O
eng	O
.	O
'	O
That	O
is	O
why	O
the	O
woman	O
had	O
a	O
panic	O
fear	O
of	O
her	O
dominant	O
husband	O
'	O
.	O
2	O
.	O
Dann	O
spricht	O
man	O
von	O
Erythrophobie	O
,	O
der	O
krankhaften	O
Angst	O
zu	O
erröten	O
.	O
eng	O
.	O
'	O
This	O
is	O
referred	O
to	O
as	O
erythrophobia	O
,	O
a	O
pathological	O
fear	O
of	O
blushing	O
'	O
.	O
3	O
.	O
Es	O
ist	O
eine	O
unterschwellige	O
,	O
alltägliche	O
Angst	O
,	O
mit	O
der	O
die	O
Bürger	O
leben	O
.	O
eng	O
.	O
'	O
It	O
is	O
a	O
subconscious	O
everyday	O
fear	O
the	O
citizens	O
live	O
with	O
'	O
.	O
The	O
examples	O
illustrate	O
that	O
these	O
three	O
collocates	O
describe	O
a	O
certain	O
kind	O
of	O
fear	O
triggered	O
by	O
a	O
particular	O
stimulus	O
,	O
but	O
the	O
stimulus	O
itself	O
can	O
only	O
be	O
derived	O
from	O
the	O
context	O
.	O
Thus	O
,	O
the	O
most	O
suitable	O
semantic	O
role	O
for	O
accommodating	O
the	O
collocates	O
is	O
CIRCUMSTANCES	O
.	O
All	O
the	O
above	O
described	O
cases	O
demonstrate	O
that	O
semantic	O
roles	O
present	O
in	O
abstract	O
collocations	O
are	O
quite	O
diverse	O
,	O
and	O
the	O
relations	O
can	O
well	O
be	O
generalized	O
using	O
FrameNet	B-DatasetName
's	O
inventory	O
of	O
frame	O
elements	O
.	O
There	O
are	O
,	O
however	O
,	O
nouns	O
,	O
that	O
seem	O
to	O
be	O
less	O
diverse	O
when	O
in	O
comes	O
to	O
the	O
number	O
of	O
attributes	O
realized	O
through	O
adjectives	O
.	O
This	O
is	O
the	O
case	O
when	O
a	O
noun	O
has	O
a	O
certain	O
kind	O
of	O
scale	O
at	O
the	O
core	O
of	O
its	O
meaning	O
.	O
For	O
instance	O
,	O
the	O
noun	O
Strafe	O
'	O
punishment	O
/	O
penalty	O
'	O
is	O
mostly	O
modified	O
in	O
terms	O
of	O
how	O
strict	O
the	O
inflicted	O
punishment	O
is	O
:	O
drakonisch	O
'	O
draconian	O
'	O
,	O
mild	O
'	O
mild	O
'	O
,	O
hart	O
'	O
harsh	O
'	O
,	O
empfindlich	O
'	O
severe	O
'	O
,	O
hoch	O
'	O
high	O
'	O
,	O
niedrig	O
'	O
weak	O
'	O
,	O
saftig	O
'	O
stiff	O
'	O
,	O
streng	O
'	O
strict	O
'	O
,	O
scharf	O
'	O
harsh	O
'	O
,	O
unmenschlich	O
'	O
inhumane	O
'	O
,	O
schwer	O
'	O
heavy	O
'	O
,	O
symbolisch	O
'	O
symbolic	O
'	O
,	O
deftig	O
'	O
severe	O
'	O
They	O
can	O
all	O
be	O
accomodated	O
by	O
the	O
FE	O
DEGREE	O
.	O
However	O
,	O
two	O
adjectives	O
from	O
this	O
list	O
stand	O
out	O
in	O
their	O
meaning	O
:	O
symbolisch	O
'	O
symbolic	O
'	O
and	O
unmenschlich	O
'	O
inhumane	O
'	O
,	O
they	O
carry	O
an	O
extra	O
meaning	O
describing	O
a	O
kind	O
of	O
penalty	O
,	O
which	O
is	O
reflected	O
in	O
the	O
FE	O
INSTRUMENT	O
(	O
"	O
The	O
Instrument	O
with	O
which	O
the	O
reward	O
or	O
punishment	O
is	O
carried	O
out	O
"	O
(	O
FrameNet	B-DatasetName
-	O
Database	O
)	O
)	O
.	O
A	O
similar	O
situation	O
holds	O
for	O
nouns	O
from	O
other	O
semantic	O
fields	O
.	O
Consider	O
the	O
noun	O
'	O
price	O
'	O
:	O
it	O
is	O
defined	O
in	O
FrameNet	B-DatasetName
as	O
"	O
the	O
amount	O
of	O
money	O
expected	O
,	O
required	O
,	O
or	O
given	O
in	O
payment	O
for	O
something	O
"	O
(	O
FrameNet	B-DatasetName
-	O
Database	O
)	O
.	O
The	O
list	O
of	O
its	O
collocates	O
contains	O
the	O
following	O
adjectives	O
:	O
horrend	O
'	O
horrendous	O
'	O
,	O
vernünftig	O
'	O
reasonable	O
'	O
,	O
erschwinglich	O
'	O
affordable	O
'	O
,	O
stolz	O
'	O
stiff	O
'	O
,	O
hoch	O
'	O
high	O
'	O
,	O
niedrig	O
'	O
low	O
'	O
,	O
fest	O
'	O
fixed	O
'	O
,	O
stabil	O
'	O
stable	O
'	O
They	O
all	O
refer	O
to	O
the	O
scale	O
"	O
the	O
amount	O
of	O
money	O
"	O
,	O
the	O
latter	O
two	O
emphasize	O
that	O
there	O
are	O
no	O
changes	O
on	O
the	O
scale	O
,	O
whereas	O
the	O
others	O
show	O
the	O
degree	O
of	O
how	O
high	O
the	O
certain	O
amount	O
is	O
from	O
the	O
point	O
of	O
view	O
of	O
the	O
customer	O
.	O
The	O
noun	O
'	O
price	O
'	O
evokes	O
the	O
frame	O
'	O
Commerce	O
scenario	O
"	O
with	O
the	O
following	O
FEs	O
:	O
BUYER	O
,	O
SELLER	O
,	O
GOODS	O
,	O
MONEY	O
,	O
MEANS	O
,	O
PURPOSE	O
,	O
RATE	O
,	O
UNIT	O
.	O
The	O
most	O
suitable	O
FE	O
in	O
this	O
case	O
is	O
RATE	O
that	O
according	O
to	O
FrameNet	B-DatasetName
describes	O
price	O
or	O
payment	O
per	O
unit	O
of	O
Goods	O
and	O
is	O
therefore	O
the	O
closest	O
to	O
the	O
concept	O
of	O
a	O
scale	O
in	O
this	O
frame	O
.	O
The	O
examples	O
illustrate	O
that	O
frame	O
semantics	O
offers	O
a	O
varied	O
inventory	O
for	O
modelling	O
semantic	O
relations	O
between	O
the	O
constituents	O
of	O
collocations	O
independently	O
of	O
the	O
semantic	O
field	O
of	O
the	O
noun	O
,	O
either	O
concrete	O
or	O
abstract	O
.	O
FrameNet	B-DatasetName
provides	O
frame	O
semantic	O
information	O
about	O
many	O
lexical	O
units	O
;	O
however	O
,	O
it	O
is	O
still	O
under	O
development	O
and	O
there	O
are	O
cases	O
,	O
when	O
the	O
frame	O
evoked	O
by	O
a	O
noun	O
does	O
not	O
reflect	O
all	O
the	O
aspects	O
of	O
its	O
meaning	O
.	O
This	O
issue	O
is	O
discussed	O
in	O
more	O
detail	O
in	O
the	O
next	O
subsection	O
.	O

More	O
than	O
one	O
thousand	O
frames	O
are	O
described	O
in	O
FrameNet	B-DatasetName
,	O
thus	O
providing	O
a	O
rich	O
coverage	O
of	O
the	O
lexicon	O
.	O
However	O
,	O
there	O
is	O
always	O
the	O
fundamental	O
issue	O
of	O
granularity	O
that	O
affects	O
the	O
groupings	O
of	O
LUs	O
into	O
frames	O
.	O
There	O
are	O
cases	O
when	O
adjectival	O
collocates	O
provide	O
additional	O
information	O
about	O
a	O
word	O
's	O
semantics	O
,	O
but	O
where	O
there	O
are	O
no	O
suitable	O
FEs	O
to	O
accommodate	O
this	O
additional	O
aspect	O
of	O
a	O
word	O
's	O
meaning	O
.	O
The	O
following	O
examples	O
illustrate	O
the	O
issue	O
.	O
Consider	O
the	O
collocates	O
of	O
the	O
noun	O
Zukunft	O
'	O
future	O
'	O
:	O
nah	O
'	O
near	O
'	O
,	O
unmittelbar	O
'	O
immediate	O
'	O
,	O
fern	O
'	O
distant	O
'	O
,	O
weit	O
'	O
distant	O
'	O
,	O
entfernt	O
'	O
distant	O
'	O
,	O
rosig	O
'	O
rosy	O
'	O
,	O
glänzend	O
'	O
bright	O
'	O
,	O
licht	O
'	O
bright	O
'	O
,	O
golden	O
'	O
golden	O
'	O
,	O
strahlend	O
'	O
bright	O
'	O
,	O
hell	O
'	O
bright	O
'	O
,	O
blühend	O
'	O
prosper	O
-	O
ous	O
'	O
,	O
leuchtend	O
'	O
bright	O
'	O
,	O
groß	O
'	O
great	O
'	O
,	O
glanzvoll	O
'	O
bright	O
'	O
,	O
dunkel	O
'	O
dark	O
'	O
,	O
düster	O
'	O
dark	O
'	O
,	O
stabil	O
'	O
stable	O
'	O
Some	O
of	O
them	O
refer	O
to	O
the	O
temporal	O
proximity	O
of	O
future	O
,	O
the	O
others	O
are	O
evaluative	O
descriptors	O
(	O
mostly	O
positive	O
ones	O
)	O
.	O
The	O
frame	O
evoked	O
by	O
'	O
future	O
'	O
in	O
FrameNet	B-DatasetName
is	O
"	O
Alternatives	O
"	O
with	O
the	O
following	O
FEs	O
(	O
FrameNet	B-DatasetName
-	O
Database	O
)	O
:	O
Agent	B-DatasetName
:	O
An	O
individual	O
involved	O
in	O
the	O
Event	O
.	O
Salient	O
entity	O
:	O
An	O
entity	O
intimately	O
involved	O
in	O
the	O
Event	O
.	O
Situation	O
:	O
Something	O
that	O
may	O
happen	O
in	O
the	O
future	O
,	O
or	O
at	O
least	O
whose	O
factual	O
status	O
is	O
unresolved	O
.	O
-	O
Number	O
of	O
possibilities	O
:	O
The	O
number	O
of	O
different	O
future	O
Events	O
under	O
consideration	O
.	O
Purpose	O
:	O
The	O
state	O
-	O
of	O
-	O
affairs	O
that	O
the	O
Agent	B-DatasetName
hopes	O
to	O
bring	O
about	O
which	O
is	O
associated	O
with	O
some	O
of	O
the	O
possible	O
Events	O
but	O
not	O
others	O
.	O
None	O
of	O
the	O
FEs	O
reflects	O
the	O
evaluative	O
or	O
the	O
temporal	O
aspect	O
of	O
the	O
meaning	O
of	O
the	O
noun	O
'	O
future	O
'	O
expressed	O
by	O
the	O
collocates	O
above	O
.	O
This	O
means	O
that	O
additional	O
FEs	O
need	O
to	O
be	O
inserted	O
into	O
the	O
frame	O
"	O
Alternatives	O
"	O
.	O
The	O
most	O
appropriate	O
FEs	O
appear	O
to	O
be	O
DESCRIPTOR	O
which	O
in	O
FrameNet	B-DatasetName
refers	O
to	O
descriptive	O
characteristics	O
and	O
properties	O
,	O
and	O
TIME	O
.	O
Consider	O
another	O
example	O
:	O
the	O
frame	O
"	O
Calendric	O
unit	O
"	O
is	O
evoked	O
by	O
LUs	O
denoting	O
seasons	O
,	O
days	O
of	O
the	O
week	O
,	O
months	O
,	O
times	O
of	O
the	O
day	O
,	O
etc	O
.	O
The	O
FEs	O
describing	O
this	O
frame	O
refer	O
to	O
different	O
aspects	O
of	O
time	O
.	O
However	O
,	O
some	O
,	O
but	O
not	O
all	O
of	O
the	O
LUs	O
that	O
evoke	O
this	O
frame	O
have	O
collocates	O
referring	O
to	O
the	O
weather	O
or	O
the	O
state	O
of	O
nature	O
:	O
winter	O
can	O
be	O
'	O
mild	O
'	O
or	O
'	O
harsh	O
'	O
(	O
in	O
the	O
sense	O
of	O
temperature	O
/	O
weather	O
)	O
,	O
autumn	O
,	O
and	O
September	O
or	O
October	O
are	O
'	O
golden	O
'	O
.	O
Such	O
LUs	O
should	O
be	O
accommodated	O
by	O
a	O
subframe	O
that	O
inherits	O
from	O
the	O
frame	O
"	O
Calendric	O
unit	O
"	O
and	O
contains	O
additional	O
FEs	O
referring	O
to	O
weather	O
and/or	O
state	O
of	O
nature	O
.	O

In	O
this	O
paper	O
we	O
have	O
argued	O
that	O
Frame	O
Semantics	O
provides	O
a	O
good	O
framework	O
for	O
semantic	O
modelling	O
of	O
adjective	O
-	O
noun	O
collocations	O
.	O
More	O
specifically	O
,	O
the	O
notion	O
of	O
a	O
frame	O
is	O
rich	O
enough	O
to	O
account	O
for	O
nouns	O
from	O
different	O
semantic	O
classes	O
and	O
to	O
model	O
semantic	O
relations	O
that	O
hold	O
between	O
an	O
adjective	O
and	O
a	O
noun	O
in	O
terms	O
of	O
Frame	O
Elements	O
.	O
We	O
have	O
substantiated	O
these	O
findings	O
by	O
considering	O
a	O
sample	O
of	O
adjective	O
-	O
noun	O
collocations	O
from	O
German	O
that	O
are	O
taken	O
from	O
different	O
semantic	O
fields	O
identified	O
in	O
the	O
German	O
wordnet	O
GermaNet	O
.	O
We	O
are	O
grateful	O
to	O
the	O
anonymous	O
reviewer	O
for	O
raising	O
an	O
interesting	O
question	O
concerning	O
the	O
applicability	O
of	O
FrameNet	B-DatasetName
's	O
semantic	O
relations	O
to	O
adjective	O
-	O
noun	O
free	O
phrases	O
as	O
well	O
.	O
In	O
future	O
research	O
,	O
we	O
plan	O
to	O
perform	O
the	O
modelling	O
on	O
a	O
larger	O
scale	O
.	O
For	O
this	O
purpose	O
,	O
we	O
are	O
currently	O
preparing	O
a	O
large	O
dataset	O
containing	O
more	O
than	O
2000	O
German	O
adjective	O
-	O
noun	O
collocations	O
.	O
We	O
will	O
continue	O
to	O
use	O
the	O
dictionary	O
DWDS	O
and	O
its	O
collocation	O
extraction	O
tool	O
Wortprofil	O
as	O
the	O
empirical	O
basis	O
for	O
obtaining	O
the	O
data	O
.	O
The	O
resulting	O
data	O
sample	O
will	O
cover	O
nouns	O
and	O
adjectives	O
from	O
all	O
the	O
semantic	O
classes	O
identified	O
in	O
GermaNet	O
.	O
We	O
will	O
use	O
this	O
dataset	O
to	O
examine	O
FrameNet	B-DatasetName
's	O
coverage	O
of	O
lexical	O
units	O
from	O
different	O
semantic	O
fields	O
.	O
But	O
even	O
if	O
a	O
lexical	O
frame	O
exists	O
for	O
a	O
given	O
noun	O
,	O
the	O
Frame	O
Elements	O
included	O
in	O
the	O
lexical	O
frame	O
may	O
not	O
suffice	O
.	O
As	O
described	O
in	O
the	O
previous	O
subsection	O
,	O
the	O
structure	O
of	O
some	O
semantic	O
frames	O
lacks	O
important	O
FEs	O
,	O
which	O
therefore	O
need	O
to	O
be	O
added	O
.	O
Therefore	O
,	O
the	O
overall	O
objective	O
in	O
the	O
future	O
work	O
is	O
to	O
examine	O
various	O
semantic	O
frames	O
and	O
their	O
Frame	O
Elements	O
in	O
terms	O
of	O
their	O
comprehensiveness	O
and	O
applicability	O
for	O
modelling	O
diverse	O
relations	O
that	O
hold	O
between	O
collocation	O
constituents	O
.	O
A	O
second	O
important	O
objective	O
of	O
our	O
future	O
research	O
will	O
be	O
to	O
address	O
the	O
question	O
of	O
reliability	O
of	O
annotations	O
for	O
the	O
semantics	O
of	O
collocations	O
on	O
the	O
basis	O
of	O
FrameNet	B-DatasetName
.	O
To	O
this	O
end	O
,	O
we	O
plan	O
to	O
conduct	O
an	O
inter	O
-	O
annotator	O
agreement	O
study	O
.	O
This	O
study	O
will	O
be	O
informed	O
by	O
detailed	O
instructions	O
to	O
the	O
annotators	O
in	O
the	O
form	O
of	O
written	O
guidelines	O
on	O
how	O
to	O
identify	O
the	O
correct	O
Frame	O
Elements	O
for	O
a	O
given	O
collocation	O
.	O
As	O
mentioned	O
in	O
Section	O
2	O
,	O
one	O
of	O
the	O
advantages	O
of	O
FrameNet	B-DatasetName
is	O
that	O
it	O
can	O
be	O
adapted	O
for	O
other	O
languages	O
.	O
Therefore	O
,	O
it	O
is	O
worthwhile	O
to	O
conduct	O
a	O
comparative	O
study	O
on	O
semantic	O
annotation	O
of	O
collocations	O
based	O
on	O
FrameNet	B-DatasetName
for	O
languages	O
other	O
than	O
German	O
.	O
We	O
plan	O
to	O
conduct	O
such	O
a	O
study	O
for	O
Russian	O
and	O
English	O
,	O
since	O
relevant	O
resources	O
and	O
points	O
of	O
comparison	O
are	O
available	O
for	O
each	O
of	O
those	O
two	O
languages	O
.	O
For	O
Russian	O
,	O
the	O
Explanatory	O
Combinatorial	O
Dictionary	O
of	O
Russian	O
(	O
Mel'cuk	O
and	O
Zholkovsky	O
,	O
1984	O
)	O
describes	O
collocations	O
in	O
terms	O
of	O
Lexical	O
Functionsà	O
la	O
Mel'čuk	O
.	O
The	O
Macmillan	O
Collocations	O
Dictionary	O
for	O
Learners	O
of	O
English	O
(	O
Macmillan	O
,	O
2010	O
)	O
provides	O
a	O
rich	O
coverage	O
of	O
English	O
lexicon	O
with	O
semantic	O
grouping	O
of	O
collocates	O
for	O
each	O
base	O
word	O
and	O
uses	O
short	O
definitions	O
to	O
describe	O
such	O
semantic	O
sets	O
.	O
We	O
plan	O
to	O
evaluate	O
the	O
relative	O
merits	O
of	O
different	O
annotation	O
schemes	O
and	O
expect	O
that	O
it	O
will	O
be	O
of	O
further	O
benefit	O
for	O
our	O
research	O
on	O
collocations	O
as	O
MWEs	O
.	O
Extending	O
the	O
present	O
study	O
to	O
Russian	O
will	O
also	O
provide	O
an	O
opportunity	O
to	O
compare	O
the	O
present	O
approach	O
that	O
classifies	O
collocations	O
in	O
terms	O
of	O
Frame	O
Elements	O
with	O
Mel'čuk	O
's	O
classification	O
according	O
to	O
Lexical	O
Functions	O
.	O
One	O
noteworthy	O
difference	O
that	O
is	O
apparent	O
already	O
at	O
this	O
point	O
is	O
that	O
FrameNet	B-DatasetName
's	O
semantic	O
relations	O
can	O
also	O
be	O
applied	O
to	O
describe	O
free	O
phrases	O
,	O
whereas	O
the	O
application	O
of	O
LFs	O
is	O
limited	O
to	O
lexically	O
restricted	O
combinations	O
(	O
Mel'čuk	O
,	O
1995	O
;	O
Mel'čuk	O
,	O
2015	O
)	O
.	O
5	O

As	O
social	O
distancing	O
,	O
self	O
-	O
quarantines	O
,	O
and	O
travel	O
restrictions	O
have	O
shifted	O
a	O
lot	O
of	O
pandemic	O
conversations	O
to	O
social	O
media	O
so	O
does	O
the	O
spread	O
of	O
hate	B-DatasetName
speech	I-DatasetName
.	O
While	O
recent	O
machine	O
learning	O
solutions	O
for	O
automated	O
hate	O
and	O
offensive	O
speech	O
identification	O
are	O
available	O
on	O
Twitter	O
,	O
there	O
are	O
issues	O
with	O
their	O
interpretability	O
.	O
We	O
propose	O
a	O
novel	O
use	O
of	O
learned	O
feature	B-TaskName
importance	I-TaskName
which	O
improves	O
upon	O
the	O
performance	O
of	O
prior	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
text	B-TaskName
classification	I-TaskName
techniques	O
,	O
while	O
producing	O
more	O
easily	O
interpretable	O
decisions	O
.	O
We	O
also	O
discuss	O
both	O
technical	O
and	O
practical	O
challenges	O
that	O
remain	O
for	O
this	O
task	O
.	O

In	O
the	O
day	O
and	O
age	O
of	O
social	O
media	O
,	O
a	O
person	O
's	O
thoughts	O
and	O
feelings	O
can	O
enter	O
the	O
public	O
discourse	O
at	O
the	O
click	O
of	O
a	O
mouse	O
or	O
tap	O
of	O
a	O
screen	O
.	O
With	O
billions	O
of	O
individuals	O
active	O
on	O
social	O
media	O
,	O
the	O
task	O
of	O
finding	O
reviewing	O
and	O
classifying	O
hate	B-DatasetName
speech	I-DatasetName
online	O
quickly	O
grows	O
to	O
a	O
scale	O
not	O
achievable	O
without	O
the	O
use	O
of	O
machine	O
learning	O
.	O
Additionally	O
,	O
the	O
definition	O
of	O
hate	O
-	O
speech	O
can	O
be	O
broad	O
and	O
include	O
many	O
nuances	O
,	O
but	O
in	O
general	O
hate	B-DatasetName
speech	I-DatasetName
is	O
defined	O
as	O
communication	O
which	O
disparages	O
or	O
incites	O
violence	O
towards	O
an	O
individual	O
or	O
group	O
based	O
on	O
that	O
person	O
or	O
groups	O
'	O
cultural	O
/	O
ethnic	O
background	O
,	O
gender	O
or	O
sexual	O
orientation	O
.	O
(	O
Schmidt	O
and	O
Wiegand	O
,	O
2017	O
)	O
.	O
In	O
the	O
context	O
of	O
Covid	O
-	O
19	O
,	O
the	O
United	O
Nations	B-DatasetName
has	O
released	O
guidelines	O
on	O
Covid	O
-	O
19	O
related	O
hatespeech	O
Guidance	O
on	O
COVID	O
-	O
19	O
related	O
Hate	B-DatasetName
Speech	I-DatasetName
cautioning	O
that	O
Member	O
States	O
and	O
Social	O
Media	O
companies	O
that	O
with	O
the	O
rise	O
of	O
Covid	O
-	O
19	O
cases	O
there	O
has	O
also	O
been	O
an	O
increase	O
of	O
hate	B-DatasetName
speech	I-DatasetName
.	O
The	O
UN	O
warns	O
that	O
such	O
communication	O
could	O
be	O
used	O
for	O
scapegoating	O
,	O
stereotyping	O
,	O
racist	O
and	O
xenophobic	O
purposes	O
.	O
The	O
tweets	O
above	O
displays	O
an	O
example	O
of	O
hate	B-DatasetName
speech	I-DatasetName
used	O
for	O
scapegoating	O
by	O
@realDonaldTrump	O
and	O
the	O
response	O
of	O
@ajRAFAEL	O
highlighting	O
the	O
impact	O
this	O
hate	B-DatasetName
speech	I-DatasetName
has	O
on	O
Asian	O
Americans	O
.	O
The	O
importance	O
of	O
identifying	O
hate	B-DatasetName
speech	I-DatasetName
combined	O
with	O
the	O
magnitude	O
of	O
the	O
data	O
makes	O
this	O
an	O
area	O
in	O
which	O
innovations	O
achieved	O
in	O
NLP	O
and	O
AI	O
research	O
can	O
make	O
an	O
impact	O
.	O
However	O
,	O
the	O
datasets	O
we	O
use	O
reflect	O
their	O
environments	O
and	O
even	O
their	O
annotators	O
(	O
Waseem	O
,	O
2016	O
)	O
(	O
Sap	O
et	O
al	O
,	O
2019	O
)	O
,	O
there	O
are	O
inherent	O
cues	O
contained	O
by	O
the	O
data	O
which	O
can	O
bias	O
the	O
predictions	O
of	O
models	O
developed	O
from	O
these	O
data	O
(	O
Davidson	O
et	O
al	O
,	O
2019	O
)	O
.	O
In	O
the	O
context	O
of	O
detecting	O
hate	B-TaskName
speech	I-TaskName
detection	I-TaskName
,	O
this	O
can	O
lead	O
to	O
predictions	O
be	O
largely	O
the	O
outcome	O
of	O
a	O
few	O
key	O
terms	O
(	O
Davidson	O
et	O
al	O
,	O
2017	O
)	O
.	O
Being	O
able	O
to	O
explain	O
how	O
underlying	O
data	O
impacts	O
AI	O
decision	O
outcomes	O
has	O
real	O
world	O
applications	O
,	O
and	O
social	O
media	O
companies	O
ignorant	O
to	O
this	O
fact	O
could	O
face	O
a	O
multitude	O
of	O
ethical	O
and	O
legal	O
repercussions	O
(	O
Samek	O
et	O
al	O
,	O
2017	O
)	O
.	O
Our	O
Contribution	O
:	O
In	O
this	O
research	O
,	O
we	O
merge	O
feature	B-TaskName
importance	I-TaskName
with	O
text	B-TaskName
classification	I-TaskName
to	O
help	O
decrease	O
false	O
positives	O
.	O
Our	O
method	O
combines	O
the	O
global	O
representation	O
of	O
a	O
term	O
's	O
feature	B-TaskName
importance	I-TaskName
to	O
a	O
predicted	O
class	O
with	O
the	O
local	O
term	O
feature	B-TaskName
importance	I-TaskName
of	O
an	O
individual	O
observation	O
.	O
Each	O
term	O
's	O
Figure	O
2	O
:	O
This	O
is	O
an	O
example	O
of	O
a	O
Covid	O
-	O
19	O
Tweet	O
incorrectly	O
classified	O
by	O
our	O
baseline	O
model	O
as	O
"	O
hate	B-DatasetName
speech	I-DatasetName
towards	O
immigrants	O
"	O
.	O
After	O
the	O
applying	O
our	O
prediction	O
enhancement	O
method	O
,	O
the	O
tweet	O
was	O
correctly	O
classified	O
as	O
"	O
not	O
hate	B-DatasetName
speech	I-DatasetName
"	O
.	O
The	O
first	O
two	O
sentence	O
combinations	O
show	O
differences	O
in	O
local	O
and	O
global	O
term	O
importance	O
impacting	O
the	O
Term	O
Difference	O
Multiplier	O
.	O
The	O
intensity	O
of	O
grey	O
represents	O
the	O
importance	O
of	O
each	O
term	O
to	O
the	O
denoted	O
label	O
.	O
The	O
last	O
sentence	O
pair	O
provides	O
the	O
term	O
difference	O
for	O
each	O
local	O
and	O
global	O
term	O
pair	O
as	O
described	O
in	O
our	O
experimental	O
design	O
.	O
global	O
feature	B-TaskName
importance	I-TaskName
is	O
collected	O
from	O
our	O
training	O
dataset	O
and	O
baseline	O
model	O
.	O
Then	O
local	O
feature	B-TaskName
importance	I-TaskName
is	O
calculated	O
for	O
each	O
observation	O
on	O
which	O
our	O
trained	O
model	O
makes	O
a	O
prediction	O
.	O
Our	O
algorithm	O
,	O
uses	O
the	O
term	O
level	O
global	O
feature	B-TaskName
importance	I-TaskName
to	O
penalize	O
model	O
predictions	O
when	O
an	O
observation	O
's	O
local	O
term	O
feature	B-TaskName
importance	I-TaskName
differs	O
from	O
the	O
global	O
feature	B-TaskName
importance	I-TaskName
.	O

In	O
the	O
same	O
vein	O
of	O
our	O
research	O
,	O
others	O
have	O
leveraged	O
explainability	O
derived	O
with	O
integrated	O
gradients	O
(	O
Sundararajan	O
et	O
al	O
,	O
2017	O
)	O
and	O
subject	O
matter	O
experts	O
to	O
create	O
priors	O
for	O
use	O
in	O
text	B-TaskName
classification	I-TaskName
.	O
In	O
this	O
research	O
,	O
they	O
showed	O
a	O
decrease	O
in	O
undesired	O
model	O
bias	O
and	O
an	O
increase	O
in	O
model	O
performance	O
when	O
using	O
scarce	O
data	O
(	O
Liu	O
and	O
Avci	O
,	O
2019	O
)	O
.	O
Overall	O
,	O
our	O
method	O
appears	O
to	O
have	O
similar	O
results	O
and	O
lessens	O
the	O
impacts	O
of	O
specific	O
key	O
terms	O
to	O
the	O
the	O
overall	O
model	O
prediction	O
.	O
One	O
of	O
the	O
more	O
commonly	O
utilized	O
explainability	O
methods	O
,	O
SHAP	B-MethodName
provides	O
a	O
framework	O
within	O
the	O
feature	O
contrubutions	O
to	O
a	O
a	O
model	O
's	O
output	O
can	O
be	O
derived	O
by	O
borrowing	O
Aultman	O
-	O
Shapely	O
values	O
from	O
cooperative	O
game	O
theory	O
(	O
Lundberg	O
and	O
Lee	O
,	O
2017	O
)	O
.	O
While	O
there	O
are	O
several	O
"	O
explainer	O
"	O
implementations	O
included	O
with	O
SHAP	B-MethodName
,	O
Gradint	O
Explainer	O
allowed	O
us	O
to	O
leverage	O
our	O
entire	O
training	O
dataset	O
as	O
the	O
background	O
dataset	O
which	O
allows	O
our	O
global	O
average	O
term	O
values	O
described	O
in	O
our	O
experimental	O
design	O
to	O
represent	O
all	O
terms	O
in	O
the	O
training	O
corpus	O
.	O
SHAP	B-MethodName
's	O
Gradient	O
Explainer	O
builds	O
off	O
of	O
integrated	O
gradients	O
and	O
leverages	O
what	O
are	O
called	O
expected	O
gradients	O
.	O
This	O
feature	O
attribution	O
method	O
takes	O
the	O
integral	O
from	O
integrated	O
gradients	O
and	O
reformulates	O
it	O
as	O
an	O
expectation	O
usable	O
in	O
calculating	O
the	O
Shapely	O
values	O
.	O
The	O
resulting	O
attributions	O
sum	O
to	O
the	O
difference	O
between	O
the	O
expected	O
and	O
current	O
model	O
output	O
.	O
However	O
,	O
this	O
method	O
does	O
assume	O
independence	O
of	O
the	O
input	O
features	O
,	O
so	O
it	O
would	O
violate	O
this	O
assumption	O
if	O
we	O
were	O
to	O
leverage	O
any	O
sequence	O
models	O
in	O
classification	O
.	O

For	O
this	O
research	O
,	O
our	O
intent	O
to	O
score	O
unlabeled	O
tweets	O
called	O
for	O
a	O
robust	O
dataset	O
which	O
could	O
be	O
generalize	O
to	O
Covid	O
-	O
19	O
tweets	O
.	O
This	O
lead	O
us	O
to	O
combining	O
three	O
datasets	O
in	O
the	O
domain	O
of	O
hate	O
and	O
offensive	O
speech	O
:	O
the	O
collection	O
of	O
racist	O
and	O
sexist	O
tweets	O
presented	O
by	O
Waseem	O
and	O
Hovy	O
(	O
Waseem	O
and	O
Hovy	O
,	O
2016	O
)	O
,	O
the	O
Offensive	O
Language	B-TaskName
Identification	I-TaskName
Dataset	O
(	O
OLID	B-DatasetName
)	O
(	O
Zampieri	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
Multilingual	O
Detection	O
of	O
Hate	B-DatasetName
Speech	I-DatasetName
Against	O
Immigrants	O
and	O
Women	O
in	O
Twitter	O
(	O
HatEval	O
)	O
(	O
Basile	O
et	O
al	O
,	O
2019	O
)	O
.	O
All	O
hate	O
or	O
offensive	O
labels	O
contained	O
within	O
these	O
three	O
datasets	O
were	O
combined	O
and	O
given	O
a	O
sub	O
-	O
classification	O
based	O
on	O
terms	O
contained	O
within	O
each	O
example	O
.	O
The	O
sub	O
-	O
classes	O
focus	O
on	O
hate	O
and	O
offensive	O
speech	O
targeting	O
or	O
directed	O
towards	O
immigrants	O
,	O
sexist	O
,	O
or	O
political	O
topics	O
and/or	O
individuals	O
.	O
The	O
terms	O
which	O
divided	O
positive	O
labeled	O
data	O
into	O
these	O
three	O
sub	O
classes	O
were	O
derived	O
through	O
analysis	O
of	O
the	O
terms	O
contained	O
in	O
positive	O
labeled	O
tweets	O
and	O
through	O
the	O
terms	O
used	O
in	O
extracting	O
the	O
tweets	O
for	O
the	O
original	O
data	O
datasets	O
.	O

In	O
order	O
to	O
leverage	O
this	O
research	O
in	O
the	O
context	O
of	O
Covid	O
-	O
19	O
,	O
we	O
collected	O
tweets	O
using	O
two	O
different	O
sources	O
.	O
First	O
,	O
we	O
leveraged	O
data	O
collected	O
by	O
the	O
Texas	B-DatasetName
Advanced	O
Computing	O
Center	O
(	O
TAAC	O
)	O
at	O
the	O
University	O
of	O
Texas	B-DatasetName
at	O
Austin	O
.	O
This	O
dataset	O
was	O
important	O
for	O
us	O
to	O
use	O
due	O
to	O
"	O
Chinese	O
Virus	O
"	O
being	O
one	O
of	O
the	O
term	O
pairs	O
the	O
TACC	O
team	O
used	O
to	O
collect	O
data	O
.	O
In	O
the	O
context	O
of	O
hate	B-DatasetName
speech	I-DatasetName
in	O
Covid	O
-	O
19	O
,	O
terms	O
which	O
target	O
countries	O
or	O
ethnicity	O
's	O
in	O
the	O
labeling	O
of	O
the	O
virus	O
clearly	O
disregard	O
the	O
aforementioned	O
UN	O
guidance	O
on	O
hate	B-DatasetName
speech	I-DatasetName
.	O
The	O
second	O
dataset	O
we	O
leveraged	O
was	O
provided	O
by	O
Georgia	O
State	O
University	O
's	O
Panacea	O
Lab	O
(	O
Banda	O
et	O
al	O
,	O
2020	O
)	O
.	O
For	O
this	O
dataset	O
,	O
we	O
specifically	O
hydrated	O
tweets	O
from	O
the	O
days	O
following	O
the	O
murder	O
of	O
George	O
Floyd	O
and	O
begging	O
of	O
civil	O
unrest	O
in	O
America	O
.	O
The	O
intent	O
behind	O
limiting	O
to	O
these	O
dates	O
was	O
to	O
increase	O
the	O
chance	O
of	O
capturing	O
tweets	O
containing	O
racial	O
or	O
ethnic	O
terms	O
.	O

To	O
achieve	O
this	O
,	O
we	O
apply	O
SHAP	B-MethodName
's	O
Gradient	O
Explainer	O
to	O
our	O
baseline	O
model	O
.	O
The	O
Gradient	O
Explainer	O
output	O
has	O
the	O
same	O
dimensions	O
as	O
our	O
Glove	O
embedded	O
data	O
,	O
so	O
to	O
reduce	O
the	O
dimensionality	O
to	O
that	O
of	O
the	O
input	O
text	O
sequence	O
,	O
we	O
sum	O
the	O
expected	O
gradients	O
across	O
the	O
axis	O
corresponding	O
to	O
a	O
term	O
in	O
each	O
sequence	O
.	O
s	O
1	O
xn	O
x	O
1	O
x	O
i	O
=	O
x	O
1	O
+	O
...	O
+	O
x	O
n	O
s	O
n	O
xn	O
x	O
1	O
Where	O
s	O
is	O
a	O
token	O
in	O
each	O
sequence	O
and	O
x	O
i	O
is	O
the	O
summation	O
of	O
all	O
expected	O
gradients	O
for	O
the	O
embedding	O
dimensions	O
.	O
The	O
values	O
of	O
these	O
summations	O
can	O
be	O
both	O
positive	O
and	O
negative	O
.	O
Since	O
our	O
method	O
requires	O
positive	O
inputs	O
to	O
measure	O
the	O
percentage	O
difference	O
,	O
these	O
values	O
for	O
every	O
s	O
step	O
across	O
all	O
sequences	O
in	O
the	O
training	O
dataset	O
are	O
scaled	O
between	O
0	B-DatasetName
and	O
1	O
via	O
min	O
/	O
max	O
scaling	O
.	O
We	O
then	O
create	O
a	O
dictionary	O
of	O
terms	O
from	O
the	O
training	O
corpus	O
and	O
store	O
the	O
"	O
global	O
average	O
"	O
feature	B-TaskName
importance	I-TaskName
of	O
each	O
term	O
.	O
This	O
dictionary	O
of	O
global	O
average	O
feature	B-TaskName
importance	I-TaskName
values	O
is	O
used	O
to	O
calculate	O
how	O
far	O
a	O
particular	O
prediction	O
strays	O
from	O
the	O
feature	B-TaskName
importance	I-TaskName
represented	O
in	O
our	O
training	O
data	O
.	O

Difference	O
Multiplier	O
Now	O
that	O
we	O
have	O
the	O
global	O
importance	O
of	O
each	O
term	O
(	O
feature	O
token	O
)	O
to	O
each	O
class	O
,	O
we	O
calculate	O
the	O
percentage	O
difference	O
of	O
each	O
term	O
's	O
local	O
feature	B-TaskName
importance	I-TaskName
to	O
that	O
term	O
's	O
global	O
feature	B-TaskName
importance	I-TaskName
by	O
each	O
class	O
.	O
1	O
−	O
	O
	O
	O
	O
|	O
s	O
g	O
−	O
s	O
l	O
|	O
(	O
s	O
g	O
+	O
s	O
l	O
)	O
2	O
	O
	O
	O
	O
As	O
you	O
can	O
see	O
above	O
the	O
percentage	O
difference	O
between	O
each	O
local	O
(	O
s	O
l	O
)	O
and	O
global	O
(	O
s	O
g	O
)	O
feature	B-TaskName
importance	I-TaskName
is	O
subtracted	O
from	O
1	O
.	O
This	O
outputs	O
the	O
difference	O
multiplier	O
for	O
each	O
term	O
in	O
a	O
sequence	O
.	O
These	O
values	O
are	O
averaged	O
for	O
each	O
sequence	O
,	O
and	O
the	O
predicted	O
probability	O
for	O
each	O
class	O
is	O
multiplied	O
by	O
it	O
's	O
local	O
Term	O
Difference	O
Multiplier	O
for	O
each	O
sequence	O
.	O
This	O
outputs	O
a	O
new	O
predicted	O
probability	O
score	O
which	O
has	O
been	O
penalized	O
based	O
on	O
how	O
much	O
it	O
's	O
local	O
attribution	O
values	O
differ	O
from	O
the	O
global	O
mean	O
of	O
each	O
term	O
in	O
the	O
input	O
sequence	O
.	O

We	O
found	O
that	O
the	O
enhanced	O
predictions	O
predominately	O
help	O
to	O
correct	O
false	O
positive	O
classifications	O
and	O
shift	O
predictions	O
towards	O
the	O
negative	O
class	O
.	O
We	O
hypothesize	O
this	O
is	O
due	O
to	O
the	O
diversity	O
of	O
language	O
and	O
relatively	O
neutral	O
feature	B-TaskName
importance	I-TaskName
most	O
terms	O
have	O
on	O
the	O
negative	O
class	O
.	O
The	O
relative	O
neutrality	O
in	O
both	O
global	O
and	O
local	O
feature	B-TaskName
importance	I-TaskName
scores	O
can	O
be	O
seen	O
in	O
figure	O
2	O
,	O
and	O
this	O
results	O
in	O
a	O
higher	O
overall	O
average	O
for	O
the	O
aggregation	O
of	O
the	O
Term	O
Difference	O
Multiplier	O
.	O
When	O
we	O
applied	O
our	O
model	O
to	O
Covid	O
-	O
19	O
tweets	O
we	O
found	O
similar	O
results	O
as	O
described	O
above	O
.	O
Quite	O
often	O
,	O
we	O
found	O
that	O
tweets	O
providing	O
information	O
about	O
specific	O
ethnic	O
groups	O
or	O
migrants	O
were	O
labeled	O
as	O
hateful	O
or	O
toxic	O
towards	O
immigrants	O
by	O
our	O
baseline	O
model	O
and	O
then	O
correctly	O
labeled	O
as	O
not	O
hateful	O
or	O
offensive	O
speech	O
when	O
we	O
applied	O
the	O
Term	O
Difference	O
Multiplier	O
.	O
An	O
example	O
of	O
this	O
exact	O
scenario	O
is	O
provided	O
in	O
figure	O
2	O
.	O

Here	O
we	O
have	O
experimented	O
with	O
a	O
novel	O
method	O
to	O
leverage	O
the	O
global	O
feature	B-TaskName
importance	I-TaskName
from	O
a	O
model	O
's	O
training	O
dataset	O
to	O
reinforce	O
or	O
even	O
penalize	O
new	O
predictions	O
when	O
their	O
local	O
feature	B-TaskName
importance	I-TaskName
varies	O
from	O
this	O
learned	O
global	O
value	O
.	O
This	O
novel	O
algorithm	O
marries	O
the	O
field	O
of	O
XAI	O
and	O
NLP	O
in	O
a	O
manner	O
which	O
allows	O
prior	O
knowledge	O
obtained	O
in	O
model	O
training	O
to	O
impact	O
present	O
predictions	O
.	O
Overall	O
,	O
we	O
believe	O
this	O
technique	O
is	O
especially	O
applicable	O
in	O
scenarios	O
like	O
Covid	O
-	O
19	O
where	O
little	O
to	O
no	O
pre	O
-	O
existing	O
labeled	O
data	O
are	O
available	O
.	O
By	O
training	O
this	O
method	O
on	O
a	O
similar	O
corpus	O
it	O
can	O
be	O
used	O
to	O
detract	O
from	O
incorrect	O
predictions	O
made	O
due	O
to	O
a	O
few	O
highly	O
influential	O
terms	O
in	O
Covid	O
-	O
19	O
datasets	O
.	O
At	O
present	O
due	O
to	O
this	O
method	O
's	O
ability	O
to	O
decrease	O
false	O
positives	O
,	O
we	O
believe	O
one	O
application	O
of	O
this	O
research	O
is	O
increasing	O
the	O
efficiency	O
of	O
systems	O
monitoring	O
for	O
hateful	O
and	O
toxic	O
communication	O
.	O
However	O
,	O
this	O
research	O
is	O
ongoing	O
.	O
We	O
intend	O
to	O
explore	O
further	O
scenarios	O
such	O
as	O
altering	O
the	O
equation	O
used	O
in	O
our	O
Term	O
Difference	O
Multiplier	O
and	O
the	O
datasets	O
used	O
since	O
the	O
global	O
feature	B-TaskName
importance	I-TaskName
can	O
greatly	O
influence	O
the	O
multiplier	O
combined	O
with	O
our	O
model	O
's	O
original	O
predicted	O
probabilities	O
.	O

Bi	O
-	O
Directional	O
Recurrent	O
Neural	O
Ordinary	O
Differential	O
Equations	O
for	O
Social	O
Media	O
Text	B-TaskName
Classification	I-TaskName

Classification	B-TaskName
of	O
posts	O
in	O
social	O
media	O
such	O
as	O
Twitter	O
is	O
difficult	O
due	O
to	O
the	O
noisy	O
and	O
short	O
nature	O
of	O
texts	O
.	O
Sequence	O
classification	O
models	O
based	O
on	O
recurrent	O
neural	O
networks	O
(	O
RNN	O
)	O
are	O
popular	O
for	O
classifying	O
posts	O
that	O
are	O
sequential	O
in	O
nature	O
.	O
RNNs	O
assume	O
the	O
hidden	O
representation	O
dynamics	O
to	O
evolve	O
in	O
a	O
discrete	O
manner	O
and	O
do	O
not	O
consider	O
the	O
exact	O
time	O
of	O
the	O
posting	O
.	O
In	O
this	O
work	O
,	O
we	O
propose	O
to	O
use	O
recurrent	O
neural	O
ordinary	O
differential	O
equations	O
(	O
RN	O
-	O
ODE	O
)	O
for	O
social	O
media	O
post	O
classification	O
which	O
consider	O
the	O
time	O
of	O
posting	O
and	O
allow	O
the	O
computation	O
of	O
hidden	O
representation	O
to	O
evolve	O
in	O
a	O
time	O
-	O
sensitive	O
continuous	O
manner	O
.	O
In	O
addition	O
,	O
we	O
propose	O
a	O
novel	O
model	O
,	O
Bi	O
-	O
directional	O
RNODE	O
(	O
Bi	O
-	O
RNODE	O
)	O
,	O
which	O
can	O
consider	O
the	O
information	O
flow	O
in	O
both	O
the	O
forward	O
and	O
backward	O
directions	O
of	O
posting	O
times	O
to	O
predict	O
the	O
post	O
label	O
.	O
Our	O
experiments	O
demonstrate	O
that	O
RNODE	O
and	O
Bi	O
-	O
RNODE	O
are	O
effective	O
for	O
the	O
problem	O
of	O
stance	B-TaskName
classification	I-TaskName
of	O
rumours	O
in	O
social	O
media	O
.	O

Information	O
disseminated	O
in	O
social	O
media	O
such	O
as	O
Twitter	O
can	O
be	O
useful	O
for	O
addressing	O
several	O
realworld	O
problems	O
like	O
rumour	B-TaskName
detection	I-TaskName
,	O
disaster	O
management	O
,	O
and	O
opinion	B-TaskName
mining	I-TaskName
.	O
Most	O
of	O
these	O
problems	O
involve	O
classifying	O
social	O
media	O
posts	O
into	O
different	O
categories	O
based	O
on	O
their	O
textual	O
content	O
.	O
For	O
example	O
,	O
classifying	O
the	O
veracity	O
of	O
tweets	O
as	O
False	O
,	O
True	O
,	O
or	O
unverified	O
allows	O
one	O
to	O
debunk	O
the	O
rumours	O
evolving	O
in	O
social	O
media	O
(	O
Zubiaga	O
et	O
al	O
,	O
2018a	O
)	O
.	O
However	O
,	O
social	O
media	O
text	O
is	O
extremely	O
noisy	O
with	O
informal	O
grammar	O
,	O
typographical	O
errors	O
,	O
and	O
irregular	O
vocabulary	O
.	O
In	O
addition	O
,	O
the	O
character	O
limit	O
(	O
240	O
characters	O
)	O
imposed	O
by	O
social	O
media	O
such	O
as	O
Twitter	O
make	O
it	O
even	O
harder	O
to	O
perform	O
text	B-TaskName
classification	I-TaskName
.	O
Social	O
media	O
text	B-TaskName
classification	I-TaskName
,	O
such	O
as	O
rumour	O
stance	B-TaskName
classification	I-TaskName
1	O
(	O
Qazvinian	O
et	O
al	O
,	O
1	O
Rumour	O
stance	B-TaskName
classification	I-TaskName
helps	O
to	O
identify	O
the	O
veracity	O
2011	O
;	O
Zubiaga	O
et	O
al	O
,	O
2016	O
;	O
Lukasik	O
et	O
al	O
,	O
2019	O
)	O
can	O
be	O
addressed	O
effectively	O
using	O
sequence	O
labelling	O
models	O
such	O
as	O
long	O
short	O
term	O
memory	O
(	O
LSTM	B-MethodName
)	O
networks	O
(	O
Zubiaga	O
et	O
al	O
,	O
2016	O
;	O
Augenstein	O
et	O
al	O
,	O
2016	O
;	O
Kochkina	O
et	O
al	O
,	O
2017	O
;	O
Zubiaga	O
et	O
al	O
,	O
2018b	O
,	O
a	O
;	O
Dey	O
et	O
al	O
,	O
2018	O
;	O
Liu	O
et	O
al	O
,	O
2019	O
;	O
Tian	O
et	O
al	O
,	O
2020	O
)	O
.	O
Though	O
they	O
consider	O
the	O
sequential	O
nature	O
of	O
tweets	O
,	O
they	O
ignore	O
the	O
temporal	O
aspects	O
associated	O
with	O
the	O
tweets	O
.	O
The	O
time	O
gap	O
between	O
tweets	O
varies	O
a	O
lot	O
and	O
LSTMs	O
ignore	O
this	O
irregularity	O
in	O
tweet	O
occurrences	O
.	O
They	O
are	O
discrete	O
state	O
space	O
models	O
where	O
hidden	O
representation	O
changes	O
from	O
one	O
tweet	O
to	O
another	O
without	O
considering	O
the	O
time	O
difference	O
between	O
the	O
tweets	O
.	O
Considering	O
the	O
exact	O
times	O
at	O
which	O
tweets	O
occur	O
can	O
play	O
an	O
important	O
role	O
in	O
determining	O
the	O
label	O
.	O
If	O
the	O
time	O
gap	O
between	O
tweets	O
is	O
large	O
,	O
then	O
the	O
corresponding	O
labels	O
may	O
not	O
influence	O
each	O
other	O
but	O
can	O
have	O
a	O
very	O
high	O
influence	O
if	O
they	O
are	O
closer	O
.	O
We	O
propose	O
to	O
use	O
recurrent	O
neural	O
ordinary	O
differential	O
equations	O
(	O
RNODE	O
)	O
(	O
Rubanova	O
et	O
al	O
,	O
2019	O
)	O
and	O
developed	O
a	O
novel	O
approach	O
bidirectional	O
RNODE	O
(	O
Bi	O
-	O
RNODE	O
)	O
,	O
which	O
can	O
naturally	O
consider	O
the	O
temporal	O
information	O
to	O
perform	O
time	O
sensitive	O
classification	O
of	O
social	O
media	O
posts	O
.	O
NODE	B-MethodName
(	O
Chen	O
et	O
al	O
,	O
2018	O
)	O
is	O
a	O
continuous	O
depth	O
deep	O
learning	O
model	O
that	O
performs	O
transformation	O
of	O
feature	O
vectors	O
in	O
a	O
continuous	O
manner	O
using	O
ordinary	O
differential	O
equation	O
solvers	O
.	O
NODEs	O
bring	O
parameter	O
efficiency	O
and	O
address	O
model	B-TaskName
selection	I-TaskName
in	O
deep	O
learning	O
to	O
a	O
great	O
extent	O
.	O
RNODE	O
generalizes	O
RNN	O
by	O
extending	O
NODE	B-MethodName
for	O
time	O
-	O
series	O
data	O
by	O
considering	O
temporal	O
information	O
associated	O
with	O
the	O
sequential	O
data	O
.	O
Hidden	O
representations	O
are	O
changed	O
continuously	O
by	O
considering	O
the	O
temporal	O
information	O
.	O
We	O
propose	O
to	O
use	O
RNODE	O
for	O
the	O
task	O
of	O
sequence	O
labeling	O
of	O
posts	O
,	O
which	O
considers	O
arrival	O
times	O
of	O
the	O
posts	O
for	O
updating	O
hidden	O
representaof	O
a	O
rumour	O
post	O
by	O
classifying	O
the	O
reply	O
tweets	O
into	O
different	O
stance	O
classes	O
such	O
as	O
Support	O
,	O
Deny	O
,	O
Question	O
,	O
Comment	O
tions	O
and	O
for	O
classifying	O
the	O
post	O
.	O
In	O
addition	O
,	O
we	O
propose	O
a	O
novel	O
model	O
,	O
Bi	O
-	O
RNODE	O
,	O
which	O
considers	O
not	O
only	O
information	O
from	O
the	O
past	O
but	O
also	O
from	O
the	O
future	O
in	O
predicting	O
the	O
label	O
of	O
the	O
post	O
.	O
Here	O
,	O
continuously	O
evolving	O
hidden	O
representations	O
in	O
the	O
forward	O
and	O
backward	O
directions	O
in	O
time	O
are	O
combined	O
and	O
used	O
to	O
predict	O
the	O
post	O
label	O
.	O
We	O
show	O
the	O
effectiveness	O
of	O
the	O
proposed	O
models	O
on	O
the	O
rumour	O
stance	B-TaskName
classification	I-TaskName
problem	O
in	O
Twitter	O
using	O
the	O
RumourEval	O
-	O
2019	O
(	O
Derczynski	B-DatasetName
et	O
al	O
,	O
2019	O
)	O
dataset	O
.	O
We	O
found	O
RNODE	O
and	O
Bi	O
-	O
RNODE	O
can	O
improve	O
the	O
social	O
media	O
text	B-TaskName
classification	I-TaskName
by	O
effectively	O
making	O
use	O
of	O
the	O
temporal	O
information	O
and	O
is	O
better	O
than	O
LSTMs	O
and	O
gated	O
recurrent	O
units	O
(	O
GRU	B-MethodName
)	O
with	O
temporal	O
features	O
.	O

We	O
consider	O
the	O
problem	O
of	O
classifying	O
social	O
media	O
posts	O
into	O
different	O
classes	O
.	O
Let	O
D	O
be	O
a	O
collection	O
of	O
N	O
posts	O
,	O
D	O
=	O
{	O
p	O
i	O
}	O
N	O
i=1	O
.	O
Each	O
post	O
p	O
i	O
is	O
assumed	O
to	O
be	O
a	O
tuple	O
containing	O
information	O
such	O
as	O
textual	O
and	O
contextual	O
features	O
x	O
i	O
,	O
time	O
of	O
the	O
post	O
t	O
i	O
and	O
the	O
label	O
associated	O
with	O
the	O
post	O
y	O
i	O
,	O
thus	O
p	O
i	O
=	O
{	O
(	O
x	O
i	O
,	O
t	O
i	O
,	O
y	O
i	O
)	O
}	O
.	O
Our	O
aim	O
is	O
to	O
develop	O
a	O
sequence	O
classification	O
model	O
which	O
considers	O
the	O
temporal	O
information	O
t	O
i	O
along	O
with	O
x	O
i	O
for	O
classifying	O
a	O
social	O
media	O
post	O
.	O
In	O
particular	O
,	O
we	O
consider	O
the	O
rumour	O
stance	B-TaskName
classification	I-TaskName
problem	O
in	O
Twitter	O
where	O
one	O
classifies	O
tweets	O
into	O
Support	O
,	O
Query	O
,	O
Deny	O
,	O
and	O
Comment	O
class	O
,	O
thus	O
y	O
i	O
Y=	O
{	O
Support	O
,	O
Query	O
,	O
Deny	O
,	O
Comment	O
}	O
.	O

To	O
demonstrate	O
the	O
effectiveness	O
of	O
the	O
proposed	O
approaches	O
,	O
we	O
consider	O
the	O
stance	B-TaskName
classification	I-TaskName
problem	O
in	O
Twitter	O
and	O
RumourEval	O
-	O
2019	O
(	O
Derczynski	B-DatasetName
et	O
al	O
,	O
2019	O
data	O
set	O
.	O
This	O
Twitter	O
data	O
set	O
consists	O
of	O
rumours	O
associated	O
with	O
eight	O
events	O
.	O
Each	O
event	O
has	O
a	O
collection	O
of	O
tweets	O
labelled	O
with	O
one	O
of	O
the	O
four	O
labels	O
-	O
Support	O
,	O
Query	O
,	O
Deny	O
and	O
Comment	O
.	O
We	O
picked	O
four	O
major	O
events	O
Charliehebdo	O
,	O
Ferguson	O
,	O
Ottawashooting	O
and	O
Sydneysiege	O
(	O
each	O
with	O
approximately	O
1000	O
tweets	O
per	O
event	O
)	O
from	O
RumourEval	O
-	O
2019	O
to	O
perform	O
experiments	O
.	O
Features	O
:	O
For	O
dataset	O
preparation	O
,	O
each	O
data	O
point	O
x	O
i	O
associated	O
with	O
a	O
Tweet	O
includes	O
text	O
embedding	O
,	O
retweet	B-DatasetName
count	O
,	O
favourites	O
count	O
,	O
punctuation	O
features	O
,	O
negative	O
and	O
positive	O
word	O
count	O
,	O
presence	O
of	O
hashtags	O
,	O
user	O
mentions	O
,	O
URLs	O
etc	O
.	O
obtained	O
from	O
the	O
tweet	O
.	O
The	O
text	O
embedding	O
of	O
the	O
tweet	O
is	O
obtained	O
by	O
concatenating	O
the	O
word	B-TaskName
embeddings	I-TaskName
2	O
.	O
Each	O
tweet	O
timestamp	O
is	O
converted	O
to	O
epoch	O
time	O
and	O
Min	O
-	O
Max	O
normalization	O
is	O
applied	O
over	O
the	O
time	O
stamps	O
associated	O
with	O
each	O
event	O
to	O
keep	O
the	O
duration	O
of	O
the	O
event	O
in	O
the	O
interval	O
[	O
0	B-DatasetName
,	O
1	O
]	O
.	O

We	O
proposed	O
RNODE	O
,	O
Bi	O
-	O
RNODE	O
models	O
for	O
sequence	O
classification	O
of	O
social	O
media	O
posts	O
.	O
These	O
models	O
consider	O
temporal	O
information	O
of	O
the	O
posts	O
and	O
hidden	O
representation	O
are	O
evolved	O
as	O
solution	O
to	O
ODE	O
.	O
Through	O
experiments	O
,	O
we	O
show	O
these	O
models	O
perform	O
better	O
than	O
LSTMs	O
on	O
rumour	O
stance	B-TaskName
classification	I-TaskName
problem	O
in	O
Twitter	O

AMR	B-TaskName
Parsing	I-TaskName
via	O
Graph	O
Sequence	O
Iterative	O
Inference	O
*	O

On	O
a	O
coarse	O
-	O
grained	O
level	O
,	O
we	O
can	O
categorize	O
existing	O
AMR	B-TaskName
parsing	I-TaskName
approaches	O
into	O
two	O
main	O
classes	O
:	O
Two	O
-	O
stage	O
parsing	O
(	O
Flanigan	O
et	O
al	O
,	O
2014	O
;	O
Lyu	O
and	O
Titov	O
,	O
2018	O
;	O
Zhang	O
et	O
al	O
,	O
2019a	O
)	O
uses	O
a	O
pipeline	O
design	O
for	O
concept	O
identification	O
and	O
relation	O
prediction	O
,	O
where	O
the	O
concept	O
decisions	O
precede	O
all	O
relation	O
decisions	O
;	O
One	O
-	O
stage	O
parsing	O
constructs	O
a	O
parse	O
graph	O
incrementally	O
.	O
For	O
more	O
fine	O
-	O
grained	O
analysis	O
,	O
those	O
one	O
-	O
stage	O
parsing	O
methods	O
can	O
be	O
further	O
categorized	O
into	O
three	O
types	O
:	O
Transitionbased	O
parsing	O
(	O
Wang	O
et	O
al	O
,	O
2016	O
;	O
Damonte	O
et	O
al	O
,	O
2017	O
;	O
Ballesteros	O
and	O
Al	O
-	O
Onaizan	O
,	O
2017	O
;	O
Peng	O
et	O
al	O
,	O
2017	O
;	O
Guo	O
and	O
Lu	O
,	O
2018	O
;	O
Liu	O
et	O
al	O
,	O
2018	O
;	O
Naseem	O
et	O
al	O
,	O
2019	O
)	O
processes	O
a	O
sentence	O
from	O
left	O
-	O
to	O
-	O
right	O
and	O
constructs	O
the	O
graph	O
incrementally	O
by	O
alternately	O
inserting	O
a	O
new	O
node	O
or	O
building	O
a	O
new	O
edge	O
.	O
Seq2seq	B-MethodName
-	O
based	O
parsing	O
(	O
Barzdins	O
and	O
Gosko	O
,	O
2016	O
;	O
Konstas	O
et	O
al	O
,	O
2017	O
;	O
van	O
Noord	O
and	O
Bos	O
,	O
2017	O
;	O
Peng	O
et	O
al	O
,	O
2018	O
)	O
views	O
parsing	O
as	O
sequence	O
-	O
to	O
-	O
sequence	O
transduction	O
by	O
some	O
linearization	O
of	O
the	O
AMR	O
graph	O
.	O
The	O
concept	O
and	O
relation	O
prediction	O
are	O
then	O
treated	O
equally	O
with	O
a	O
shared	O
vocabulary	O
.	O
The	O
third	O
class	O
is	O
graph	O
-	O
based	O
parsing	O
(	O
Cai	O
and	O
Lam	O
,	O
2019	O
;	O
Zhang	O
et	O
al	O
,	O
2019b	O
)	O
,	O
where	O
at	O
each	O
time	O
step	O
,	O
a	O
new	O
node	O
along	O
with	O
its	O
connections	O
to	O
existing	O
nodes	O
are	O
jointly	O
decided	O
,	O
either	O
in	O
order	O
(	O
Cai	O
and	O
Lam	O
,	O
2019	O
)	O
or	O
in	O
parallel	O
(	O
Zhang	O
et	O
al	O
,	O
2019b	O
)	O
.	O
So	O
far	O
,	O
the	O
recip	O
-	O
The	O
boy	O
must	O
not	O
go	O
The	O
current	O
partial	O
(	O
solid	O
)	O
and	O
full	O
(	O
solid	O
+	O
dashed	O
)	O
AMR	O
graphs	O
for	O
the	O
sentence	O
"	O
The	O
boy	O
must	O
no	O
go	O
"	O
rocal	O
causation	O
of	O
relation	O
prediction	O
and	O
concept	O
prediction	O
has	O
not	O
been	O
closely	O
-	O
studied	O
and	O
wellutilized	O
.	O
There	O
are	O
also	O
some	O
exceptions	O
staying	O
beyond	O
the	O
above	O
categorization	O
.	O
Peng	O
et	O
al	O
(	O
2015	O
)	O
introduce	O
a	O
synchronous	O
hyperedge	O
replacement	O
grammar	O
solution	O
.	O
Pust	O
et	O
al	O
(	O
2015	O
)	O
regard	O
the	O
task	O
as	O
a	O
machine	B-TaskName
translation	I-TaskName
problem	O
,	O
while	O
Artzi	O
et	O
al	O
(	O
2015	O
)	O
adapt	O
combinatory	O
categorical	O
grammar	O
.	O
Groschwitz	O
et	O
al	O
(	O
2018	O
)	O
;	O
Lindemann	O
et	O
al	O
(	O
2019	O
)	O
view	O
AMR	O
graphs	O
as	O
the	O
structure	O
AM	O
algebra	O
.	O

Our	O
approach	O
is	O
inspired	O
by	O
the	O
deliberation	O
process	O
when	O
a	O
human	O
expert	O
is	O
deducing	O
a	O
semantic	O
graph	O
from	O
a	O
sentence	O
.	O
The	O
output	O
graph	O
starts	O
from	O
an	O
empty	O
graph	O
and	O
spans	O
incrementally	O
in	O
a	O
node	O
-	O
by	O
-	O
node	O
manner	O
.	O
At	O
any	O
time	O
step	O
of	O
this	O
process	O
,	O
we	O
are	O
distilling	O
the	O
information	O
for	O
the	O
next	O
expansion	O
.	O
We	O
call	O
it	O
expansion	O
because	O
the	O
new	O
node	O
,	O
as	O
an	O
abstract	O
concept	O
of	O
some	O
specific	O
text	O
fragments	O
in	O
the	O
input	O
sentence	O
,	O
is	O
derived	O
to	O
complete	O
some	O
missing	B-TaskName
elements	I-TaskName
in	O
the	O
current	O
semantic	O
graph	O
.	O
Specifically	O
,	O
given	O
the	O
input	O
sentence	O
and	O
the	O
current	O
partially	O
constructed	O
graph	O
,	O
we	O
are	O
answering	O
two	O
critical	O
questions	O
:	O
which	O
part	O
of	O
the	O
input	O
sequence	O
to	O
abstract	O
,	O
and	O
where	O
in	O
the	O
output	O
graph	O
to	O
construct	O
the	O
new	O
concept	O
.	O
For	O
instance	O
,	O
Figure	O
1	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
show	O
two	O
possible	O
choices	O
for	O
the	O
next	O
expansion	O
.	O
In	O
Figure	O
1	O
(	O
a	O
)	O
,	O
the	O
word	O
"	O
boy	O
"	O
is	O
abstracted	O
to	O
the	O
concept	O
boy	O
to	O
complement	O
the	O
subject	O
information	O
of	O
the	O
event	O
go	O
-	O
02	O
.	O
On	O
the	O
(	O
Current	O
Graph	O
)	O
(	O
Input	O
Sequence	O
)	O
The	O
boy	O
wants	O
the	O
girl	O
to	O
believe	O
him	O
.	O

The	O
boy	O
wants	O
the	O
girl	O
to	O
believe	O
him	O
.	O
attention	O
x	O
t	O
y	O
t+	O
1	O
…	O
initial	O
state	O
x	O
0	B-DatasetName
f	O
(	O
G	O
i	O
,	O
x	O
0	B-DatasetName
)	O
f	O
(	O
G	O
i	O
,	O
x	O
1	O
)	O
g	O
(	O
W	O
,	O
y	O
1	O
)	O
g	O
(	O
W	O
,	O
y	O
2	O
)	O
y	O
1	O
x	O
1	O
y	O
2	O
G	O
i	O
W	O
graph	O
memory	O
Figure	O
2	O
:	O
Overview	O
of	O
the	O
dual	O
graph	O
-	O
sequence	O
iterative	O
inference	O
for	O
AMR	B-TaskName
parsing	I-TaskName
.	O
Given	O
the	O
current	O
graph	O
G	O
i	O
and	O
input	O
sequence	O
W	O
.	O
The	O
inference	O
starts	O
with	O
an	O
initial	O
concept	O
decision	O
x	O
0	B-DatasetName
and	O
follows	O
the	O
inference	O
chain	O
x	O
0	B-DatasetName
f	O
(	O
G	O
i	O
,	O
x	O
0	B-DatasetName
)	O
y	O
1	O
g	O
(	O
W	O
,	O
y	O
1	O
)	O
x	O
1	O
f	O
(	O
G	O
i	O
,	O
x	O
1	O
)	O
y	O
2	O
g	O
(	O
W	O
,	O
y	O
2	O
)	O
.	O
The	O
details	O
of	O
f	O
and	O
g	O
are	O
shown	O
in	O
red	O
and	O
blue	O
boxes	O
,	O
where	O
nodes	O
in	O
graph	O
and	O
tokens	O
in	O
sequence	O
are	O
selected	O
via	O
attention	O
mechanisms	O
.	O
other	O
hand	O
,	O
in	O
Figure	O
1	O
(	O
b	O
)	O
,	O
a	O
polarity	O
attribute	O
of	O
the	O
event	O
go	O
-	O
2	O
is	O
constructed	O
,	O
which	O
is	O
triggered	O
by	O
the	O
word	O
"	O
not	O
"	O
in	O
the	O
sentence	O
.	O
We	O
note	O
that	O
the	O
answer	O
to	O
one	O
of	O
the	O
questions	O
can	O
help	O
answer	O
the	O
other	O
.	O
For	O
instance	O
,	O
if	O
we	O
have	O
decided	O
to	O
render	O
the	O
word	O
"	O
not	O
"	O
to	O
the	O
graph	O
,	O
then	O
we	O
will	O
consider	O
adding	O
an	O
edge	O
labeled	O
as	O
polarity	O
,	O
and	O
finally	O
determine	O
its	O
attachment	O
to	O
the	O
existing	O
event	O
go	O
-	O
2	O
(	O
rather	O
than	O
an	O
edge	O
labeled	O
ARG0	O
to	O
the	O
same	O
event	O
go	O
-	O
2	O
,	O
though	O
it	O
is	O
also	O
present	O
in	O
the	O
golden	O
graph	O
)	O
.	O
On	O
the	O
other	O
hand	O
,	O
if	O
we	O
have	O
decided	O
to	O
find	O
the	O
subject	O
(	O
ARG0	O
relation	O
)	O
of	O
the	O
action	O
go	O
-	O
02	O
,	O
we	O
are	O
confident	O
to	O
locate	O
the	O
word	O
"	O
boy	O
"	O
instead	O
of	O
function	O
words	O
like	O
"	O
not	O
"	O
or	O
"	O
must	O
"	O
,	O
thus	O
unambiguously	O
predict	O
the	O
right	O
concept	O
boy	O
.	O
Another	O
possible	O
circumstance	O
is	O
that	O
we	O
may	O
make	O
a	O
mistake	O
trying	O
to	O
ask	O
something	O
that	O
is	O
not	O
present	O
in	O
the	O
sentence	O
(	O
e.g.	O
,	O
the	O
destination	O
of	O
the	O
go	O
-	O
02	O
action	O
)	O
.	O
This	O
attempt	O
will	O
be	O
rejected	O
by	O
a	O
review	O
of	O
the	O
sentence	O
.	O
The	O
rationale	O
is	O
that	O
literally	O
we	O
can	O
not	O
find	O
the	O
destination	O
information	O
in	O
the	O
sentence	O
.	O
Similarly	O
,	O
if	O
we	O
mistakenly	O
propose	O
to	O
abstract	O
some	O
parts	O
of	O
the	O
sentence	O
that	O
are	O
not	O
ready	O
for	O
construction	O
yet	O
,	O
the	O
proposal	O
will	O
be	O
rejected	O
by	O
another	O
inspection	O
on	O
the	O
graph	O
since	O
that	O
there	O
is	O
nowhere	O
to	O
place	O
such	O
a	O
new	O
concept	O
.	O
We	O
believe	O
the	O
mutual	O
causalities	O
,	O
as	O
described	O
above	O
,	O
are	O
useful	O
for	O
action	O
disambiguation	O
and	O
harmonious	O
decision	B-TaskName
making	I-TaskName
,	O
which	O
eventually	O
result	O
in	O
more	O
accurate	O
parses	O
.	O
We	O
formulate	O
AMR	B-TaskName
parsing	I-TaskName
as	O
a	O
series	O
of	O
dual	O
graph	O
-	O
sequence	O
decisions	O
and	O
design	O
an	O
iterative	O
inference	O
approach	O
to	O
tackle	O
each	O
of	O
them	O
.	O
It	O
is	O
sort	O
of	O
analogous	O
to	O
the	O
cognition	O
procedure	O
of	O
a	O
person	O
,	O
who	O
might	O
first	O
notice	O
part	O
of	O
the	O
important	O
information	O
in	O
one	O
side	O
(	O
graph	O
or	O
sequence	O
)	O
,	O
then	O
try	O
to	O
confirm	O
her	O
decision	O
at	O
the	O
other	O
side	O
,	O
which	O
could	O
just	O
refute	O
her	O
former	O
hypothesis	O
and	O
propose	O
a	O
new	O
one	O
,	O
and	O
finally	O
converge	O
to	O
a	O
conclusion	O
after	O
multiple	O
rounds	O
of	O
reasoning	O
.	O

Formally	O
,	O
the	O
parsing	O
model	O
consists	O
of	O
a	O
series	O
of	O
graph	O
expansion	O
procedures	O
{	O
G	O
0	B-DatasetName
.	O
.	O
.	O
G	O
i	O
.	O
.	O
.	O
}	O
,	O
starting	O
from	O
an	O
empty	O
graph	O
G	O
0	B-DatasetName
.	O
In	O
each	O
turn	O
of	O
expansion	O
,	O
the	O
following	O
iterative	O
inference	O
process	O
is	O
performed	O
:	O
y	O
i	O
t	O
=	O
g	O
(	O
G	O
i	O
,	O
x	O
i	O
t	O
)	O
,	O
x	O
i	O
t+1	O
=	O
f	O
(	O
W	O
,	O
y	O
i	O
t	O
)	O
,	O
where	O
W	O
,	O
G	O
i	O
are	O
the	O
input	O
sequence	O
and	O
the	O
current	O
semantic	O
graph	O
respectively	O
.	O
g	O
(	O
)	O
,	O
f	O
(	O
)	O
seek	O
where	O
to	O
construct	O
(	O
edge	O
prediction	O
)	O
and	O
what	O
to	O
abstract	O
(	O
node	O
prediction	O
)	O
respectively	O
,	O
and	O
x	O
i	O
t	O
,	O
y	O
i	O
t	O
are	O
the	O
t	O
-	O
th	O
graph	O
hypothesis	O
(	O
where	O
to	O
construct	O
)	O
and	O
t	O
-	O
th	O
sequence	O
hypothesis	O
(	O
what	O
to	O
abstract	O
)	O
for	O
the	O
i	O
-	O
th	O
expansion	O
step	O
respectively	O
.	O
For	O
clarity	O
,	O
we	O
may	O
drop	O
the	O
superscript	O
i	O
in	O
the	O
following	O
descriptions	O
.	O
Figure	O
2	O
depicts	O
an	O
overview	O
of	O
the	O
graphsequence	O
iterative	O
inference	O
process	O
.	O
Our	O
model	O
has	O
four	O
main	O
components	O
:	O
(	O
1	O
)	O
Sequence	O
Encoder	O
,	O
which	O
generates	O
a	O
set	O
of	O
text	O
memories	O
(	O
per	O
token	O
)	O
to	O
provide	O
grounding	O
for	O
concept	O
alignment	O
and	O
abstraction	O
;	O
(	O
2	O
)	O
Graph	O
Encoder	O
,	O
which	O
generates	O
a	O
set	O
of	O
graph	O
memories	O
(	O
per	O
node	O
)	O
to	O
provide	O
grounding	O
for	O
relation	O
reasoning	O
;	O
(	O
3	O
)	O
Concept	O
Solver	O
,	O
where	O
a	O
previous	O
graph	O
hypothesis	O
is	O
used	O
for	O
concept	O
prediction	O
;	O
and	O
(	O
4	O
)	O
Graph	O
Solver	O
,	O
where	O
a	O
previous	O
concept	O
hypothesis	O
is	O
used	O
for	O
relation	O
prediction	O
.	O
The	O
last	O
two	O
components	O
correspond	O
to	O
the	O
reasoning	O
functions	O
g	O
(	O
)	O
and	O
f	O
(	O
)	O
respectively	O
.	O
The	O
text	O
memories	O
can	O
be	O
computed	O
by	O
Sentence	O
Encoder	O
at	O
the	O
beginning	O
of	O
the	O
whole	O
parsing	O
while	O
the	O
graph	O
memories	O
are	O
constructed	O
by	O
Graph	O
Encoder	O
incrementally	O
as	O
the	O
parsing	O
progresses	O
.	O
During	O
the	O
iterative	O
inference	O
,	O
a	O
semantic	O
representation	O
of	O
current	O
state	O
is	O
used	O
to	O
attend	O
to	O
both	O
graph	O
and	O
text	O
memories	O
(	O
blue	O
and	O
red	O
arrows	O
)	O
in	O
order	O
to	O
locate	O
the	O
new	O
concept	O
and	O
obtain	O
its	O
relations	O
to	O
the	O
existing	O
graph	O
,	O
both	O
of	O
which	O
subsequently	O
refine	O
each	O
other	O
.	O
Intuitively	O
,	O
after	O
a	O
first	O
glimpse	O
of	O
the	O
input	O
sentence	O
and	O
the	O
current	O
graph	O
,	O
specific	O
sub	O
-	O
areas	O
of	O
both	O
sequence	O
and	O
graph	O
are	O
revisited	O
to	O
obtain	O
a	O
better	O
understanding	O
of	O
the	O
current	O
situation	O
.	O
Later	O
steps	O
typically	O
read	O
the	O
text	O
in	O
detail	O
with	O
specific	O
learning	O
aims	O
,	O
either	O
confirming	O
or	O
overturning	O
a	O
previous	O
hypothesis	O
.	O
Finally	O
,	O
after	O
several	O
iterations	O
of	O
reasoning	O
steps	O
,	O
the	O
refined	O
sequence	O
/	O
graph	O
decisions	O
are	O
used	O
for	O
graph	O
expansion	O
.	O

As	O
mentioned	O
above	O
,	O
we	O
employ	O
a	O
sequence	O
encoder	O
to	O
convert	O
the	O
input	O
sentence	O
into	O
vector	O
representations	O
.	O
The	O
sequence	O
encoder	O
follows	O
the	O
multi	O
-	O
layer	O
Transformer	B-MethodName
architecture	O
described	O
in	O
Vaswani	O
et	O
al	O
(	O
2017	O
)	O
.	O
At	O
the	O
bottom	O
layer	O
,	O
each	O
token	O
is	O
firstly	O
transformed	O
into	O
the	O
concatenation	O
of	O
features	O
learned	O
by	O
a	O
character	O
-	O
level	O
convolutional	O
neural	O
network	O
(	O
charCNN	O
,	O
Kim	O
et	O
al	O
,	O
2016	O
)	O
and	O
randomly	O
initialized	O
embeddings	O
for	O
its	O
lemma	B-DatasetName
,	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
tag	O
,	O
and	O
named	O
entity	O
tag	O
.	O
Additionally	O
,	O
we	O
also	O
include	O
features	O
learned	O
by	O
pre	O
-	O
trained	O
language	O
model	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
.	O
2	O
Formally	O
,	O
for	O
an	O
input	O
sequence	O
w	O
1	O
,	O
w	O
2	O
,	O
.	O
.	O
.	O
,	O
w	O
n	O
with	O
length	O
n	O
,	O
we	O
insert	O
a	O
special	O
token	O
BOS	O
at	O
the	O
beginning	O
of	O
the	O
sequence	O
.	O
For	O
clarity	O
,	O
we	O
omit	O
the	O
detailed	O
transformations	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
and	O
denote	O
the	O
final	O
output	O
from	O
our	O
sequence	O
encoder	O
as	O
{	O
h	O
0	B-DatasetName
,	O
h	O
1	O
,	O
.	O
.	O
.	O
,	O
h	O
n	O
}	O
R	O
d	O
,	O
where	O
h	O
0	B-DatasetName
corresponds	O
the	O
special	O
token	O
BOS	O
and	O
serves	O
as	O
an	O
overall	O
rep	O
-	O
resentation	O
while	O
others	O
are	O
considered	O
as	O
contextualized	O
word	O
representations	O
.	O
Note	O
that	O
the	O
sequence	O
encoder	O
only	O
needs	O
to	O
be	O
invoked	O
once	O
,	O
and	O
the	O
produced	O
text	O
memories	O
are	O
used	O
for	O
the	O
whole	O
parsing	O
procedure	O
.	O

We	O
use	O
a	O
similar	O
idea	O
in	O
Cai	O
and	O
Lam	O
(	O
2019	O
)	O
to	O
encode	O
the	O
incrementally	O
expanding	O
graph	O
.	O
Specifically	O
,	O
a	O
graph	O
is	O
simply	O
treated	O
as	O
a	O
sequence	O
of	O
nodes	O
(	O
concepts	O
)	O
in	O
the	O
chronological	O
order	O
of	O
when	O
they	O
are	O
inserted	O
into	O
the	O
graph	O
.	O
We	O
employ	O
multi	O
-	O
layer	O
Transformer	B-MethodName
architecture	O
with	O
masked	O
self	O
-	O
attention	O
and	O
source	O
-	O
attention	O
,	O
which	O
only	O
allows	O
each	O
position	O
in	O
the	O
node	O
sequence	O
to	O
attend	O
to	O
all	O
positions	O
up	O
to	O
and	O
including	O
that	O
position	O
,	O
and	O
every	O
position	O
in	O
the	O
node	O
sequence	O
to	O
attend	O
over	O
all	O
positions	O
in	O
the	O
input	O
sequence	O
.	O
3	O
While	O
this	O
design	O
allows	O
for	O
significantly	O
more	O
parallelization	O
during	O
training	O
and	O
computation	O
-	O
saving	O
incrementality	O
during	O
testing	O
,	O
4	O
it	O
inherently	O
neglects	O
the	O
edge	O
information	O
.	O
We	O
attempted	O
to	O
alleviate	O
this	O
problem	O
by	O
incorporating	O
the	O
idea	O
of	O
Strubell	O
et	O
al	O
(	O
2018	O
)	O
that	O
applies	O
auxiliary	O
supervision	O
at	O
attention	O
heads	O
to	O
encourage	O
them	O
to	O
attend	O
to	O
each	O
node	O
's	O
parents	O
in	O
the	O
AMR	O
graph	O
.	O
However	O
,	O
we	O
did	O
not	O
see	O
performance	O
improvement	O
.	O
We	O
attribute	O
the	O
failure	O
to	O
the	O
fact	O
that	O
the	O
neural	O
attention	O
mechanisms	O
on	O
their	O
own	O
are	O
already	O
capable	O
of	O
learning	O
to	O
attend	O
to	O
useful	O
graph	O
elements	O
,	O
and	O
the	O
auxiliary	O
supervision	O
is	O
likely	O
to	O
disturb	O
the	O
ultimate	O
parsing	O
goal	O
.	O
Consequently	O
,	O
for	O
the	O
current	O
graph	O
G	O
with	O
m	O
nodes	O
,	O
we	O
take	O
its	O
output	O
concept	O
sequence	O
c	O
1	O
,	O
c	O
2	O
,	O
.	O
.	O
.	O
,	O
c	O
m	O
as	O
input	O
.	O
Similar	O
to	O
the	O
sequence	O
encoder	O
,	O
we	O
insert	O
a	O
special	O
token	O
BOG	O
at	O
the	O
beginning	O
of	O
the	O
concept	O
sequence	O
.	O
Each	O
concept	O
is	O
firstly	O
transformed	O
into	O
the	O
concatenation	O
of	O
feature	O
vector	O
learned	O
by	O
a	O
char	O
-	O
CNN	O
and	O
randomly	O
initialized	O
embedding	O
.	O
Then	O
,	O
a	O
multi	O
-	O
layer	O
Transformer	B-MethodName
encoder	O
with	O
masked	O
self	O
-	O
attention	O
and	O
sourceattention	O
is	O
applied	O
,	O
resulting	O
in	O
vector	O
representations	O
{	O
s	O
0	B-DatasetName
,	O
s	O
1	O
,	O
.	O
.	O
.	O
,	O
s	O
m	O
}	O
R	O
d	O
,	O
where	O
s	O
0	B-DatasetName
represents	O
the	O
special	O
concept	O
BOG	O
and	O
serves	O
as	O
a	O
dummy	O
node	O
while	O
others	O
are	O
considered	O
as	O
contextualized	O
node	O
representations	O
.	O

Datasets	O
Our	O
evaluation	O
is	O
conducted	O
on	O
two	O
AMR	O
public	O
releases	O
:	O
AMR	O
2.0	O
(	O
LDC0217T10	O
)	O
and	O
AMR	O
1.0	O
(	O
LDC2014T12	O
)	O
.	O
AMR	O
2.0	O
is	O
the	O
latest	O
and	O
largest	O
AMR	O
sembank	O
that	O
was	O
extensively	O
used	O
in	O
recent	O
works	O
.	O
AMR	O
1.0	O
shares	O
the	O
same	O
development	O
and	O
test	O
set	O
with	O
AMR	O
,	O
while	O
the	O
size	O
of	O
its	O
training	O
set	O
is	O
only	O
about	O
one	O
-	O
third	O
of	O
AMR	O
2.0	O
,	O
making	O
it	O
a	O
good	O
testbed	O
to	O
evaluate	O
our	O
model	O
's	O
sensitivity	O
for	O
data	O
size	O
.	O
6	O
Implementation	O
Details	O
We	O
use	O
Stanford	O
CoreNLP	O
(	O
Manning	O
et	O
al	O
,	O
2014	O
)	O
for	O
tokenization	O
,	O
lemmatization	B-TaskName
,	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
,	O
and	O
named	O
entity	O
tagging	O
.	O
The	O
hyper	O
-	O
parameters	O
of	O
our	O
models	O
are	O
chosen	O
on	O
the	O
development	O
set	O
of	O
AMR	O
2.0	O
.	O
Without	O
explicit	O
specification	O
,	O
we	O
perform	O
N	O
=	O
4	O
steps	O
of	O
iterative	O
inference	O
.	O
Other	O
hyper	O
-	O
parameter	O
settings	O
can	O
be	O
found	O
in	O
the	O
Appendix	O
.	O
Our	O
models	O
are	O
trained	O
using	O
ADAM	B-DatasetName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
for	O
up	O
to	O
60	O
K	O
steps	O
(	O
first	O
50	O
K	O
with	O
the	O
random	O
sibling	O
order	O
and	O
last	O
10	O
K	O
with	O
deterministic	O
order	O
)	O
,	O
with	O
early	B-MethodName
stopping	I-MethodName
based	O
on	O
development	O
set	O
performance	O
.	O
We	O
fix	O
BERT	B-MethodName
parameters	O
similar	O
to	O
Zhang	O
et	O
al	O
(	O
2019a	O
,	O
b	O
)	O
due	O
to	O
the	O
GPU	O
memory	O
limit	O
.	O
During	O
testing	O
,	O
we	O
use	O
a	O
beam	O
size	O
of	O
8	O
for	O
the	O
highest	O
-	O
scored	O
graph	O
approximation	O
.	O
7	O
AMR	O
Pre	O
-	O
and	O
Post	O
-	O
processing	O
We	O
remove	O
senses	O
as	O
done	O
in	O
Lyu	O
and	O
Titov	O
(	O
2018	O
)	O
;	O
Zhang	O
et	O
al	O
(	O
2019a	O
,	O
b	O
)	O
and	O
simply	O
assign	O
the	O
most	O
frequent	O
sense	O
for	O
nodes	O
in	O
post	O
-	O
processing	O
.	O
Notably	O
,	O
most	O
existing	O
methods	O
including	O
the	O
state	O
-	O
the	O
-	O
ofart	O
parsers	O
(	O
Zhang	O
et	O
al	O
,	O
2019a	O
,	O
b	O
;	O
Lyu	O
and	O
Titov	O
,	O
2018	O
;	O
Guo	O
and	O
Lu	O
,	O
2018	O
,	O
inter	O
alia	O
)	O
often	O
rely	O
on	O
heavy	O
graph	O
re	O
-	O
categorization	O
for	O
reducing	O
the	O
complexity	O
and	O
sparsity	O
of	O
the	O
original	O
AMR	O
graphs	O
.	O
For	O
graph	O
re	O
-	O
categorization	O
,	O
specific	O
subgraphs	O
of	O
AMR	O
are	O
grouped	O
together	O
and	O
assigned	O
to	O
a	O
single	O
node	O
with	O
a	O
new	O
compound	O
category	O
,	O
which	O
usually	O
involves	O
non	O
-	O
trivial	O
expert	O
-	O
level	O
manual	O
efforts	O
for	O
hand	O
-	O
crafting	O
rules	O
.	O
We	O
follow	O
the	O
exactly	O
same	O
pre	O
-	O
and	O
post	O
-	O
processing	O
steps	O
of	O
those	O
of	O
Zhang	O
et	O
al	O
(	O
2019a	O
,	O
b	O
)	O
for	O
graph	O
re	O
-	O
categorization	O
.	O
More	O
details	O
can	O
be	O
found	O
in	O
the	O
Appendix	O
.	O
Ablated	O
Models	O
As	O
pointed	O
out	O
by	O
Cai	O
and	O
Lam	O
(	O
2019	O
)	O
,	O
the	O
precise	O
set	O
of	O
graph	O
re	O
-	O
categorization	O
rules	O
differs	O
among	O
different	O
works	O
,	O
making	O
it	O
difficult	O
to	O
distinguish	O
the	O
performance	O
improvement	O
from	O
model	O
optimization	O
and	O
carefully	O
designed	O
rules	O
.	O
In	O
addition	O
,	O
only	O
recent	O
works	O
(	O
Zhang	O
et	O
al	O
,	O
2019a	O
,	O
b	O
;	O
Lindemann	O
et	O
al	O
,	O
2019	O
;	O
Naseem	O
et	O
al	O
,	O
2019	O
)	O
have	O
started	O
to	O
utilize	O
the	O
large	O
-	O
scale	O
pretrained	O
language	O
model	O
,	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
;	O
Wolf	O
et	O
al	O
,	O
2019	O
)	O
.	O
Therefore	O
,	O
we	O
also	O
include	O
ablated	O
models	O
for	O
addressing	O
two	O
questions	O
:	O
(	O
1	O
)	O
How	O
dependent	O
is	O
our	O
model	O
on	O
performance	O
from	O
handcrafted	O
graph	O
re	O
-	O
categorization	O
rules	O
?	O
(	O
2	O
)	O
How	O
much	O
does	O
BERT	B-MethodName
help	O
?	O
We	O
accordingly	O
implement	O
three	O
ablated	O
models	O
by	O
removing	O
either	O
one	O
of	O
them	O
or	O
removing	O
both	O
.	O
The	O
ablation	O
study	O
not	O
only	O
reveals	O
the	O
individual	O
effect	O
of	O
two	O
model	O
components	O
but	O
also	O
helps	O
facilitate	O
fair	O
comparisons	O
with	O
prior	O
works	O
.	O

In	O
order	O
to	O
investigate	O
how	O
our	O
parser	O
performs	O
on	O
individual	O
sub	O
-	O
tasks	O
,	O
we	O
also	O
use	O
the	O
fine	O
-	O
grained	O
evaluation	O
tool	O
(	O
Damonte	O
et	O
al	O
,	O
2017	O
)	O
and	O
compare	O
to	O
systems	O
which	O
reported	O
these	O
scores	O
.	O
8	O
As	O
shown	O
in	O
the	O
right	O
block	O
of	O
Table	O
1	O
,	O
our	O
best	O
model	O
obtains	O
the	O
highest	O
scores	O
on	O
almost	O
all	O
sub	O
-	O
tasks	O
.	O
The	O
improvements	O
in	O
all	O
sub	O
-	O
tasks	O
are	O
consistent	O
and	O
uniform	O
(	O
around	O
2%∼3	O
%	O
)	O
compared	O
to	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
(	O
Zhang	O
et	O
al	O
,	O
2019b	O
)	O
,	O
partly	O
confirming	O
that	O
our	O
model	O
boosts	O
performance	O
via	O
consolidated	O
and	O
harmonious	O
decisions	O
rather	O
than	O
fixing	O
particular	O
phenomena	O
.	O
By	O
our	O
ablation	O
study	O
,	O
8	O
We	O
only	O
list	O
the	O
results	O
on	O
AMR	O
2.0	O
since	O
there	O
are	O
few	O
results	O
on	O
AMR	O
1.0	O
to	O
compare	O
.	O
it	O
is	O
worth	O
noting	O
that	O
the	O
NER	B-TaskName
scores	O
are	O
much	O
lower	O
when	O
using	O
graph	O
re	O
-	O
categorization	O
.	O
This	O
is	O
because	O
the	O
rule	O
-	O
based	O
system	O
for	O
NER	B-TaskName
in	O
graph	O
recategorization	O
does	O
not	O
generalize	O
well	O
to	O
unseen	O
entities	O
,	O
which	O
suggest	O
a	O
potential	O
improvement	O
by	O
adapting	O
better	O
NER	B-TaskName
taggers	O
.	O

We	O
presented	O
the	O
dual	O
graph	O
-	O
sequence	O
iterative	O
inference	O
method	O
for	O
AMR	B-TaskName
Parsing	I-TaskName
.	O
Our	O
method	O
constructs	O
an	O
AMR	O
graph	O
incrementally	O
in	O
a	O
nodeby	O
-	O
node	O
fashion	O
.	O
Each	O
spanning	O
step	O
is	O
explicitly	O
characterized	O
as	O
answering	O
two	O
questions	O
:	O
which	O
parts	O
of	O
the	O
sequence	O
to	O
abstract	O
,	O
and	O
where	O
in	O
the	O
graph	O
to	O
construct	O
.	O
We	O
leverage	O
the	O
mutual	O
causalities	O
between	O
the	O
two	O
and	O
design	O
an	O
iterative	O
inference	O
algorithm	O
.	O
Our	O
model	O
significantly	O
advances	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
two	O
AMR	O
corpora	O
.	O
An	O
interesting	O
future	O
work	O
is	O
to	O
make	O
the	O
number	O
of	O
inference	O
steps	O
adaptive	O
to	O
input	O
sentences	O
.	O
Also	O
,	O
the	O
idea	O
proposed	O
in	O
this	O
paper	O
may	O
be	O
applied	O
to	O
a	O
broad	O
range	O
of	O
structured	B-TaskName
prediction	I-TaskName
tasks	O
(	O
not	O
only	O
restricted	O
to	O
other	O
semantic	B-TaskName
parsing	I-TaskName
tasks	O
)	O
where	O
the	O
complex	O
output	O
space	O
can	O
be	O
divided	O
into	O
two	O
interdependent	O
parts	O
with	O
a	O
similar	O
iterative	O
inference	O
process	O
to	O
achieve	O
harmonious	O
predictions	O
and	O
better	O
performance	O
.	O

We	O
follow	O
exactly	O
the	O
same	O
pre	O
-	O
and	O
postprocessing	O
steps	O
of	O
those	O
of	O
Zhang	O
et	O
al	O
(	O
2019a	O
,	O
b	O
)	O
for	O
graph	O
re	O
-	O
categorization	O
.	O
In	O
preprocessing	O
,	O
we	O
anonymize	O
entities	O
,	O
remove	O
wiki	O
links	O
and	O
polarity	O
attributes	O
,	O
and	O
convert	O
the	O
resultant	O
AMR	O
graphs	O
into	O
a	O
compact	O
format	O
by	O
compressing	O
certain	O
subgraphs	O
.	O
In	O
post	O
-	O
processing	O
,	O
we	O
recover	O
the	O
original	O
AMR	O
format	O
from	O
the	O
compact	O
format	O
,	O
restore	O
Wikipedia	O
links	O
using	O
the	O
DBpedia	B-DatasetName
Spotlight	O
API	O
(	O
Daiber	O
et	O
al	O
,	O
2013	O
)	O
,	O
add	O
polarity	O
attributes	O
based	O
on	O
rules	O
observed	O
from	O
the	O
training	O
data	O
.	O
More	O
details	O
can	O
be	O
found	O
in	O
Zhang	O
et	O
al	O
(	O
2019a	O
)	O
.	O

Using	O
Type	O
Information	O
to	O
Improve	O
Entity	O
Coreference	B-TaskName
Resolution	I-TaskName

Coreference	B-TaskName
resolution	I-TaskName
(	O
CR	O
)	O
is	O
an	O
extensively	O
studied	O
problem	O
in	O
computational	O
linguistics	O
and	O
NLP	O
(	O
Hobbs	O
,	O
1978	O
;	O
Lappin	O
and	O
Leass	O
,	O
1994	O
;	O
Mitkov	O
,	O
1999	O
;	O
Ng	O
,	O
2017	O
;	O
Clark	O
and	O
Manning	O
,	O
2016	O
;	O
Lee	O
et	O
al	O
,	O
2017	O
)	O
.	O
Solutions	O
to	O
this	O
problem	O
allow	O
us	O
to	O
make	O
meaningful	O
links	O
between	O
concepts	O
and	O
entities	O
within	O
a	O
discourse	O
and	O
therefore	O
serves	O
as	O
a	O
valuable	O
pre	O
-	O
processing	O
step	O
for	O
downstream	O
tasks	O
like	O
summarization	B-TaskName
and	O
questionanswering	O
(	O
Steinberger	O
et	O
al	O
,	O
2007	O
;	O
Dasigi	O
et	O
al	O
,	O
2019	O
;	O
Sukthanker	O
et	O
al	O
,	O
2020a	O
)	O
.	O
Recently	O
,	O
multiple	O
datasets	O
including	O
Ontonotes	B-DatasetName
(	O
Pradhan	O
et	O
al	O
,	O
2012	O
)	O
,	O
Litbank	B-DatasetName
(	O
Bamman	O
et	O
al	O
,	O
2020	O
)	O
,	O
EmailCoref	O
(	O
Dakle	O
et	O
al	O
,	O
2020	O
)	O
,	O
and	O
WikiCoref	B-DatasetName
(	O
Ghaddar	O
and	O
Langlais	O
,	O
2016	O
)	O
have	O
been	O
proposed	O
as	O
benchmark	O
datasets	O
for	O
CR	O
,	O
especially	O
in	O
the	O
sub	O
-	O
area	O
of	O
entity	O
anaphora	O
(	O
Sukthanker	O
et	O
al	O
,	O
2020b	O
)	O
.	O
Entity	O
anaphora	O
is	O
a	O
simpler	O
starting	O
place	O
for	O
work	O
on	O
anaphora	O
because	O
unlike	O
abstract	O
anaphora	O
(	O
Webber	O
,	O
1991	O
)	O
,	O
entity	O
anaphora	O
are	O
pronouns	O
or	O
noun	O
phrases	O
that	O
refer	O
to	O
an	O
explicitly	O
mentioned	O
entity	O
in	O
the	O
discourse	O
rather	O
than	O
an	O
abstract	O
idea	O
that	O
must	O
be	O
constructed	O
from	O
a	O
repackaging	O
of	O
information	O
revealed	O
over	O
an	O
extended	O
text	O
.	O
An	O
affordance	O
of	O
entity	O
anaphora	O
is	O
that	O
they	O
have	O
easily	O
articulated	O
semantic	O
types	O
.	O
Most	O
of	O
the	O
entity	O
CR	O
datasets	O
are	O
extensively	O
annotated	O
for	O
syntactic	O
features	O
(	O
like	O
constituency	O
parse	O
etc	O
.	O
)	O
and	O
semantic	O
features	O
(	O
like	O
entity	O
-	O
types	O
)	O
.	O
However	O
,	O
none	O
of	O
the	O
published	O
SOTA	O
methods	O
(	O
Lee	O
et	O
al	O
,	O
2017	O
;	O
Joshi	O
et	O
al	O
,	O
2019Joshi	O
et	O
al	O
,	O
,	O
2020	O
)	O
explicitly	O
leverage	O
the	O
type	O
information	O
.	O
In	O
this	O
paper	O
,	O
we	O
present	O
a	O
proof	O
of	O
concept	O
to	O
portray	O
the	O
benefits	O
of	O
using	O
type	O
information	O
in	O
neural	O
approaches	O
for	O
CR	O
.	O
Named	O
entities	O
are	O
generally	O
divided	O
generically	O
(	O
e.g.	O
person	O
,	O
organization	O
etc	O
.	O
)	O
or	O
in	O
a	O
domain	O
-	O
specific	O
manner	O
(	O
e.g.	O
symptom	O
,	O
drug	O
,	O
test	O
etc	O
.	O
)	O
.	O
In	O
this	O
work	O
,	O
we	O
consider	O
CR	O
datasets	O
that	O
contain	O
generic	O
entitytypes	O
.	O
One	O
challenge	O
is	O
that	O
the	O
different	O
corpora	O
do	O
not	O
utilize	O
the	O
same	O
set	O
of	O
type	O
tags	O
.	O
For	O
example	O
,	O
OntoNotes	B-DatasetName
includes	O
18	O
types	O
while	O
EmailCoref	O
includes	O
only	O
4	O
.	O
Thus	O
,	O
we	O
evaluate	O
the	O
performance	O
of	O
the	O
proposed	O
modeling	O
approach	O
on	O
each	O
dataset	O
both	O
with	O
the	O
set	O
of	O
type	O
tags	O
germaine	O
to	O
the	O
dataset	O
as	O
well	O
as	O
a	O
common	O
set	O
of	O
four	O
basic	O
types	O
(	O
person	O
,	O
org	O
,	O
location	O
,	O
facility	O
)	O
inspired	O
from	O
research	O
on	O
Named	B-TaskName
Entity	I-TaskName
Recognition	I-TaskName
(	O
NER	B-TaskName
)	O
(	O
Tjong	O
Kim	O
Sang	O
,	O
2002	O
;	O
Tjong	O
Kim	O
Sang	O
and	O
De	O
Meulder	O
,	O
2003	O
)	O
.	O
Our	O
motivation	O
is	O
similar	O
to	O
(	O
Durrett	O
and	O
Klein	O
,	O
2014	O
)	O
,	O
which	O
used	O
a	O
structured	O
CRF	B-MethodName
with	O
handcurated	O
features	O
to	O
jointly	O
-	O
model	O
the	O
tasks	O
of	O
CR	O
,	O
entity	B-TaskName
typing	I-TaskName
,	O
and	O
entity	B-TaskName
linking	I-TaskName
.	O
Their	O
joint	O
architecture	O
showed	O
an	O
improved	O
performance	O
on	O
CR	O
over	O
the	O
independent	O
baseline	O
.	O
However	O
,	O
our	O
work	O
differs	O
from	O
there	O
's	O
as	O
we	O
show	O
the	O
benefits	O
of	O
entity	O
-	O
type	O
information	O
in	O
neural	O
models	O
that	O
use	O
contextualized	O
representations	O
like	O
BERT	B-MethodName
(	O
Peters	O
et	O
al	O
,	O
2018	O
)	O
.	O
Some	O
prior	O
art	O
(	O
Petroni	O
et	O
al	O
,	O
2019	O
;	O
Roberts	O
et	O
al	O
,	O
2020	O
)	O
argues	O
that	O
contextual	O
-	O
Figure	O
1	O
:	O
We	O
improve	O
Bamman	O
et	O
al	O
(	O
2020	O
)	O
for	O
entity	O
coreference	B-TaskName
resolution	I-TaskName
by	O
incorporating	O
type	O
information	O
at	O
two	O
levels	O
.	O
(	O
1	O
)	O
Type	O
information	O
is	O
concatenated	O
with	O
the	O
mention	O
span	O
representation	O
created	O
by	O
their	O
model	O
;	O
and	O
(	O
2	O
)	O
A	O
consistency	O
check	O
is	O
incorporated	O
that	O
compares	O
the	O
types	O
of	O
two	O
mentions	O
under	O
consideration	O
to	O
calculate	O
the	O
coreference	O
score	O
.	O
Please	O
refer	O
to	O
Section	O
3	O
for	O
details	O
.	O
ized	O
embeddings	O
implicitly	O
capture	O
facts	O
and	O
relationships	O
between	O
real	O
-	O
world	O
entities	O
.	O
However	O
,	O
in	O
this	O
work	O
,	O
we	O
empirically	O
show	O
that	O
access	O
to	O
explicit	O
knowledge	O
about	O
entity	O
-	O
types	O
benefits	O
neural	O
models	O
that	O
use	O
BERT	B-MethodName
for	O
CR	O
.	O
We	O
show	O
a	O
consistent	O
improvement	O
in	O
performance	O
on	O
four	O
different	O
coreference	O
datasets	O
from	O
varied	O
domains	O
.	O
Our	O
contribution	O
is	O
that	O
we	O
evaluate	O
the	O
impact	O
of	O
the	O
introduction	O
of	O
type	O
information	O
in	O
neural	O
entity	O
coreference	O
at	O
two	O
different	O
levels	O
of	O
granularity	O
(	O
which	O
we	O
refer	O
to	O
as	O
original	O
vs	O
common	O
)	O
,	O
demonstrating	O
their	O
utility	O
both	O
in	O
the	O
case	O
where	O
gold	O
standard	O
type	O
information	O
is	O
available	O
,	O
and	O
the	O
more	O
typical	O
case	O
where	O
it	O
is	O
predicted	O
.	O

Neural	O
Coreference	B-TaskName
Resolution	I-TaskName
:	O
Recently	O
,	O
neural	O
approaches	O
to	O
coreference	O
(	O
Joshi	O
et	O
al	O
,	O
2020	O
(	O
Joshi	O
et	O
al	O
,	O
,	O
2019Lee	O
et	O
al	O
,	O
,	O
2017	O
have	O
begun	O
to	O
show	O
their	O
prowess	O
.	O
The	O
SOTA	O
models	O
show	O
impressive	O
performance	O
on	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
datasets	O
like	O
OntoNotes	B-DatasetName
(	O
Pradhan	O
et	O
al	O
,	O
2012	O
)	O
and	O
GAP	B-DatasetName
(	O
Webster	O
et	O
al	O
,	O
2018	O
)	O
.	O
The	O
notable	O
architecture	O
proposed	O
by	O
Lee	O
et	O
al	O
(	O
2017	O
)	O
scores	O
pairs	O
of	O
entity	O
mentions	O
independently	O
and	O
later	O
uses	O
a	O
clustering	O
algorithm	O
to	O
find	O
coreference	O
clusters	O
.	O
On	O
the	O
other	O
hand	O
,	O
improve	O
upon	O
this	O
foundation	O
by	O
introducing	O
an	O
approximated	O
higher	O
-	O
order	O
inference	O
that	O
iteratively	O
updates	O
the	O
existing	O
span	O
representation	O
using	O
its	O
antecedent	O
distribution	O
.	O
Moreover	O
,	O
they	O
propose	O
a	O
coarseto	O
-	O
fine	O
grained	O
approach	O
to	O
pairwise	O
scoring	O
for	O
tackling	O
the	O
computational	O
challenges	O
caused	O
due	O
to	O
the	O
iterative	O
higher	O
-	O
order	O
inference	O
.	O
More	O
recently	O
,	O
Joshi	O
et	O
al	O
(	O
2019Joshi	O
et	O
al	O
(	O
,	O
2020	O
showed	O
that	O
use	O
of	O
contextual	O
representations	O
instead	O
of	O
wordembeddings	O
like	O
GloVe	B-MethodName
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
can	O
further	O
boost	O
the	O
results	O
over	O
and	O
above	O
those	O
just	O
mentioned	O
.	O
Our	O
work	O
offers	O
additional	O
improvement	O
by	O
building	O
on	O
the	O
model	O
proposed	O
in	O
Bamman	O
et	O
al	O
(	O
2020	O
)	O
,	O
which	O
is	O
based	O
on	O
Lee	O
et	O
al	O
(	O
2017	O
)	O
,	O
and	O
adds	O
additional	O
nuanced	O
information	O
grounded	O
in	O
semantic	O
types	O
.	O
Type	O
Information	O
:	O
Named	B-TaskName
Entity	I-TaskName
Recognition	I-TaskName
datasets	O
(	O
Tjong	O
Kim	O
Sang	O
,	O
2002	O
;	O
Tjong	O
Kim	O
Sang	O
and	O
De	O
Meulder	O
,	O
2003	O
;	O
Li	O
et	O
al	O
,	O
2016	O
)	O
often	O
group	O
entity	O
mentions	O
into	O
different	O
types	O
(	O
or	O
categories	O
)	O
depending	O
on	O
the	O
domain	O
and	O
the	O
potential	O
downstream	O
applications	O
of	O
the	O
corpus	O
.	O
For	O
example	O
,	O
the	O
medical	O
corpus	O
used	O
in	O
the	O
i2b2	O
Challenge	O
2010	O
(	O
Uzuner	O
et	O
al	O
,	O
2011	O
)	O
annotates	O
domain	O
-	O
specific	O
types	O
like	O
problem	O
,	O
test	O
,	O
symptom	O
etc	O
.	O
,	O
whereas	O
,	O
a	O
more	O
general	O
-	O
domain	O
dataset	O
like	O
CoNLL	O
-	O
2002	O
(	O
Tjong	O
Kim	O
Sang	O
,	O
2002	O
uses	O
generic	O
types	O
like	O
person	O
,	O
organization	O
,	O
and	O
location	O
.	O
Type	O
information	O
as	O
a	O
predictive	O
signal	O
has	O
been	O
shown	O
to	O
be	O
beneficial	O
for	O
NLP	O
tasks	O
like	O
relation	B-TaskName
extraction	I-TaskName
(	O
Soares	O
et	O
al	O
,	O
2019	O
)	O
and	O
entitylinking	O
.	O
It	O
affords	O
some	O
level	O
of	O
disambiguation	O
,	O
which	O
assists	O
models	O
with	O
filtering	O
out	O
some	O
incorrect	O
predictions	O
in	O
order	O
to	O
increase	O
the	O
probability	O
of	O
a	O
correct	O
prediction	O
.	O
In	O
this	O
work	O
,	O
we	O
evaluate	O
the	O
benefits	O
of	O
using	O
explicit	O
type	O
information	O
for	O
CR	O
.	O
We	O
show	O
that	O
a	O
model	O
that	O
leverages	O
entity	O
types	O
associated	O
with	O
the	O
anaphoric/	O
antecedent	O
mentions	O
significantly	O
reduces	O
the	O
problem	O
of	O
type	O
inconsistency	O
in	O
the	O
output	O
coreference	O
clusters	O
and	O
thus	O
improves	O
the	O
overall	O
performance	O
of	O
the	O
neural	O
baseline	O
on	O
four	O
datasets	O
.	O
Type	O
Information	O
for	O
CR	O
:	O
Multiple	O
prior	O
works	O
have	O
shown	O
type	O
-	O
information	O
to	O
be	O
a	O
useful	O
feature	O
for	O
shallow	O
coreference	B-TaskName
resolution	I-TaskName
classifiers	O
(	O
Soon	O
et	O
al	O
,	O
2001	O
;	O
Bengtson	O
and	O
Roth	O
,	O
2008	O
;	O
Ponzetto	O
and	O
Strube	O
,	O
2006	O
;	O
Haghighi	O
and	O
Klein	O
,	O
2010	O
;	O
Durrett	O
and	O
Klein	O
,	O
2014	O
)	O
.	O
(	O
Soon	O
et	O
al	O
,	O
2001	O
)	O
take	O
the	O
most	O
frequent	O
sense	O
for	O
each	O
noun	O
in	O
WordNet	O
as	O
the	O
semantic	O
class	O
for	O
that	O
noun	O
and	O
use	O
a	O
decision	O
-	O
tree	O
for	O
pairwise	O
classification	O
of	O
whether	O
two	O
samples	O
co	O
-	O
refer	O
each	O
other	O
.	O
(	O
Bengtson	O
and	O
Roth	O
,	O
2008	O
)	O
use	O
a	O
hypernym	O
tree	O
to	O
extract	O
the	O
type	O
information	O
for	O
different	O
common	O
nouns	O
,	O
and	O
compare	O
the	O
proper	O
names	O
against	O
a	O
predefined	O
list	O
to	O
determine	O
if	O
the	O
mention	O
is	O
a	O
person	O
.	O
They	O
,	O
then	O
,	O
pass	O
this	O
and	O
many	O
other	O
features	O
(	O
like	O
distance	O
,	O
agreement	O
,	O
etc	O
.	O
)	O
through	O
a	O
regularized	O
average	O
perceptron	O
for	O
pairwise	O
classification	O
.	O
This	O
paper	O
expands	O
on	O
these	O
studies	O
to	O
show	O
that	O
entity	O
-	O
type	O
information	O
is	O
also	O
beneficial	O
for	O
neural	O
models	O
that	O
use	O
contextualized	O
representations	O
like	O
BERT	B-MethodName
(	O
Peters	O
et	O
al	O
,	O
2018	O
)	O
,	O
which	O
have	O
been	O
argued	O
to	O
implicitly	O
capture	O
facts	O
and	O
relationships	O
between	O
real	O
-	O
world	O
entities	O
(	O
Petroni	O
et	O
al	O
,	O
2019	O
;	O
Roberts	O
et	O
al	O
,	O
2020	O
)	O
.	O

We	O
use	O
the	O
model	O
proposed	O
in	O
Bamman	O
et	O
al	O
(	O
2020	O
)	O
as	O
our	O
baseline	O
.	O
The	O
model	O
gives	O
stateof	O
-	O
the	O
-	O
art	O
scores	O
on	O
the	O
LitBank	B-DatasetName
corpus	O
(	O
Bamman	O
et	O
al	O
,	O
2020	O
)	O
and	O
is	O
an	O
end	O
-	O
to	O
-	O
end	O
mention	O
ranking	O
system	O
based	O
on	O
Lee	O
et	O
al	O
(	O
2017	O
)	O
,	O
which	O
has	O
shown	O
competitive	O
performance	O
on	O
the	O
OntoNotes	B-DatasetName
dataset	O
.	O
However	O
,	O
this	O
model	O
differs	O
from	O
Lee	O
et	O
al	O
(	O
2017	O
)	O
as	O
it	O
uses	O
BERT	B-MethodName
embeddings	O
,	O
omits	O
author	O
and	O
genre	O
information	O
,	O
and	O
only	O
focuses	O
on	O
the	O
task	O
of	O
mention	O
-	O
linking	O
.	O
Since	O
our	O
main	O
goal	O
is	O
to	O
evaluate	O
the	O
benefits	O
of	O
type	O
information	O
,	O
we	O
too	O
separate	O
mention	O
-	O
linking	O
from	O
mentionidentification	O
and	O
only	O
show	O
results	O
computed	O
over	O
gold	O
-	O
standard	O
mentions	O
.	O
This	O
controls	O
for	O
the	O
effects	O
of	O
the	O
mention	O
-	O
identification	O
module	O
's	O
performance	O
on	O
our	O
experiments	O
.	O
Impact	O
of	O
typeinformation	O
incorporation	O
in	O
the	O
real	O
-	O
world	O
endto	O
-	O
end	O
CR	O
setting	O
(	O
mention	O
identification	O
+	O
linking	O
)	O
is	O
left	O
as	O
future	O
work	O
.	O
The	O
BERT	B-MethodName
embeddings	O
for	O
each	O
token	O
i	O
are	O
passed	O
through	O
a	O
bi	O
-	O
directional	O
LSTM	B-MethodName
(	O
x	O
i	O
)	O
.	O
To	O
represent	O
a	O
mention	O
m	O
with	O
start	O
and	O
end	O
positions	O
s	O
,	O
e	O
respectively	O
,	O
x	O
s	O
,	O
x	O
e	O
,	O
attention	O
over	O
x	O
s	O
,	O
...	O
,	O
x	O
e	O
,	O
and	O
features	O
to	O
represent	O
the	O
width	O
(	O
wi	O
)	O
and	O
inclusion	O
within	O
quotations	O
(	O
qu	O
)	O
are	O
concatenated	O
.	O
m	O
=	O
[	O
x	O
s	O
;	O
x	O
e	O
;	O
Att	O
(	O
x	O
s	O
,	O
..	O
,	O
x	O
e	O
)	O
;	O
wi	O
;	O
qu	O
]	O
(	O
1	O
)	O
Finally	O
,	O
given	O
the	O
representation	O
of	O
two	O
mentions	O
m	O
j	O
and	O
m	O
k	O
,	O
their	O
coreference	O
score	O
S	O
(	O
m	O
j	O
,	O
m	O
k	O
)	O
is	O
computed	O
by	O
concatenating	O
m	O
j	O
,	O
m	O
k	O
,	O
m	O
j	O
m	O
k	O
,	O
distance	O
(	O
d	O
)	O
between	O
the	O
mentions	O
and	O
whether	O
one	O
mention	O
is	O
nested	O
(	O
n	O
)	O
within	O
the	O
other	O
,	O
which	O
are	O
then	O
passed	O
through	O
fully	O
-	O
connected	O
layers	O
(	O
FC	O
)	O
.	O
S	O
(	O
m	O
j	O
,	O
m	O
k	O
)	O
=	O
FC	O
(	O
[	O
m	O
j	O
;	O
m	O
k	O
;	O
m	O
j	O
m	O
k	O
;	O
d	O
;	O
n	O
]	O
)	O
(	O
2	O
)	O
We	O
refer	O
the	O
reader	O
to	O
(	O
Bamman	O
et	O
al	O
,	O
2020	O
;	O
Lee	O
et	O
al	O
,	O
2017	O
)	O
for	O
more	O
details	O
about	O
the	O
architecture	O
.	O

We	O
improve	O
the	O
above	O
model	O
by	O
including	O
entitytype	O
information	O
on	O
two	O
levels	O
(	O
Figure	O
1	O
)	O
.	O
First	O
,	O
we	O
concatenate	O
the	O
entity	O
-	O
type	O
t	O
of	O
the	O
mention	O
to	O
m	O
(	O
in	O
Eq	O
.	O
1	O
)	O
to	O
improve	O
the	O
mention	O
representation	O
.	O
m	O
=	O
[	O
m	O
;	O
t	O
]	O
(	O
3	O
)	O
This	O
allows	O
the	O
model	O
access	O
to	O
the	O
entity	O
type	O
of	O
the	O
mention	O
as	O
an	O
additional	O
feature	O
.	O
We	O
call	O
this	O
+	O
ET	O
-	O
self	O
.	O
Second	O
,	O
to	O
check	O
the	O
type	O
consistency	O
(	O
softly	O
)	O
between	O
any	O
two	O
mentions	O
under	O
consideration	O
as	O
possibly	O
coreferent	O
,	O
we	O
append	O
a	O
feature	O
(	O
tc	O
)	O
in	O
Eq	O
.	O
2	O
,	O
which	O
takes	O
the	O
value	O
0	B-DatasetName
if	O
both	O
mentions	O
have	O
the	O
same	O
type	O
,	O
and	O
1	O
otherwise	O
.	O
For	O
example	O
,	O
in	O
Figure	O
1	O
,	O
since	O
Los	O
Angeles	O
and	O
it	O
have	O
the	O
same	O
entity	O
-	O
type	O
PLACE	O
,	O
tc	O
jk	O
=	O
0	B-DatasetName
.	O
S	O
(	O
m	O
j	O
,	O
m	O
k	O
)	O
=	O
FC	O
(	O
[	O
m	O
j	O
;	O
m	O
k	O
;	O
m	O
j	O
m	O
k	O
;	O
d	O
;	O
n	O
;	O
tc	O
jk	O
]	O
)	O
(	O
4	O
)	O
This	O
part	O
of	O
the	O
approach	O
is	O
referred	O
to	O
as	O
+	O
ETcross	O
throughout	O
the	O
remainder	O
of	O
the	O
paper	O
.	O
We	O
decide	O
against	O
the	O
use	O
of	O
a	O
hard	O
consistency	O
check	O
(	O
which	O
would	O
filter	O
out	O
mentions	O
which	O
do	O
not	O
have	O
the	O
same	O
type	O
)	O
as	O
it	O
might	O
not	O
generalize	O
well	O
to	O
bridging	O
anaphora	O
(	O
Clark	O
,	O
1975	O
)	O
where	O
the	O
anaphor	O
refers	O
to	O
an	O
object	O
that	O
is	O
associated	O
with	O
,	O
but	O
not	O
identical	O
to	O
,	O
the	O
antecedent	O
(	O
Poesio	O
et	O
al	O
,	O
2018	O
)	O
.	O
In	O
such	O
cases	O
,	O
the	O
type	O
of	O
the	O
anaphora	O
and	O
its	O
antecedent	O
may	O
not	O
match	O
.	O
Finally	O
,	O
our	O
architecture	O
combines	O
both	O
components	O
together	O
as	O
+	O
ET	O
(	O
ET	O
=	O
ET	O
-	O
self	O
+	O
ET	O
-	O
cross	O
)	O
.	O

We	O
gauge	O
the	O
benefits	O
of	O
using	O
entity	O
-	O
type	O
information	O
on	O
the	O
four	O
datasets	O
discussed	O
below	O
.	O
LitBank	B-DatasetName
.	O
This	O
dataset	O
(	O
Bamman	O
et	O
al	O
,	O
2020	O
)	O
contains	O
coreference	O
annotations	O
for	O
100	O
literary	O
texts	O
.	O
1	O
This	O
dataset	O
limits	O
the	O
markable	O
mentions	O
to	O
six	O
entity	O
-	O
types	O
,	O
where	O
majority	O
of	O
the	O
mentions	O
(	O
83.1	O
%	O
)	O
point	O
to	O
a	O
person	O
.	O
EmailCoref	O
.	O
This	O
dataset	O
(	O
Dakle	O
et	O
al	O
,	O
2020	O
)	O
comprises	O
of	O
46	O
email	O
threads	O
with	O
a	O
total	O
of	O
245	O
email	O
messages	O
.	O
2	O
Similar	O
to	O
LitBank	B-DatasetName
,	O
it	O
considers	O
a	O
mention	O
to	O
be	O
a	O
span	O
of	O
text	O
that	O
refers	O
to	O
a	O
real	O
-	O
world	O
entity	O
.	O
In	O
this	O
work	O
,	O
we	O
filter	O
out	O
pronouns	O
that	O
point	O
towards	O
multiple	O
entities	O
in	O
the	O
email	O
(	O
e.g.	O
we	O
,	O
they	O
)	O
thus	O
only	O
focusing	O
on	O
singular	O
mentions	O
.	O
Ontonotes	B-DatasetName
.	O
From	O
this	O
multi	O
-	O
lingual	O
dataset	O
,	O
we	O
evaluate	O
on	O
the	O
subset	O
(	O
english	O
)	O
from	O
OntoNotes	B-DatasetName
that	O
was	O
used	O
in	O
the	O
CoNLL	O
-	O
2012	O
shared	O
task	O
(	O
Pradhan	O
et	O
al	O
,	O
2012	O
)	O
.	O
3	O
It	O
contains	O
2802	O
training	O
,	O
343	O
development	O
,	O
and	O
348	O
test	O
documents	O
.	O
The	O
dataset	O
differs	O
from	O
LitBank	B-DatasetName
in	O
its	O
annotation	O
scheme	O
with	O
the	O
biggest	O
difference	O
being	O
the	O
fact	O
that	O
it	O
does	O
not	O
annotate	O
singletons	O
.	O
It	O
contains	O
annotations	O
for	O
18	O
different	O
entitytypes	O
.	O
However	O
,	O
unlike	O
LitBank	B-DatasetName
and	O
EmailCoref	O
,	O
not	O
all	O
mentions	O
have	O
an	O
associated	O
entity	O
-	O
type	O
.	O
For	O
example	O
,	O
none	O
of	O
the	O
pronoun	O
mentions	O
are	O
given	O
a	O
type	O
even	O
if	O
they	O
act	O
as	O
anaphors	O
to	O
typed	O
entities	O
.	O
We	O
partially	O
ameliorate	O
this	O
issue	O
by	O
extracting	O
gold	O
coreference	O
clusters	O
that	O
contain	O
at	O
least	O
one	O
typed	O
mention	O
and	O
assigning	O
the	O
majority	O
type	O
in	O
that	O
cluster	O
to	O
all	O
of	O
its	O
elements	O
.	O
For	O
example	O
,	O
in	O
Figure	O
1	O
,	O
if	O
Los	O
Angeles	O
is	O
typed	O
PLACE	O
,	O
and	O
it	O
is	O
in	O
the	O
gold	O
coreference	O
cluster	O
of	O
Los	O
Angeles	O
(	O
no	O
other	O
element	O
in	O
the	O
cluster	O
)	O
,	O
then	O
it	O
is	O
also	O
assigned	O
the	O
type	O
PLACE	O
.	O
WikiCoref	B-DatasetName
.	O
This	O
corpus	O
,	O
released	O
by	O
(	O
Ghaddar	O
and	O
Langlais	O
,	O
2016	O
)	O
,	O
comprises	O
30	O
documents	O
from	O
wikipedia	O
annotated	O
for	O
coreference	B-TaskName
resolution	I-TaskName
.	O
4	O
The	O
annotations	O
contain	O
additional	O
metadata	O
,	O
like	O
the	O
associated	O
freebase	O
rdf	O
link	O
for	O
each	O
mention	O
(	O
if	O
available	O
)	O
.	O
We	O
use	O
this	O
rdf	O
entry	O
to	O
extract	O
the	O
mention	O
's	O
entity	O
types	O
from	O
freebase	O
dump	O
.	O
Mentions	O
that	O
do	O
not	O
get	O
any	O
type	O
are	O
marked	O
NA	O
.	O
The	O
first	O
24	O
documents	O
are	O
chosen	O
for	O
training	O
,	O
the	O
next	O
3	O
for	O
development	O
,	O
and	O
the	O
rest	O
for	O
testing	O
.	O
The	O
above	O
-	O
discussed	O
datasets	O
differ	O
in	O
the	O
number	O
as	O
well	O
as	O
the	O
categories	O
of	O
entity	O
-	O
types	O
they	O
originally	O
annotate	O
(	O
Table	O
1	O
)	O
.	O
Apart	O
from	O
a	O
common	O
list	O
of	O
types	O
(	O
like	O
PER	O
,	O
ORG	O
,	O
LOC	O
)	O
,	O
they	O
also	O
include	O
corpus	O
-	O
specific	O
categories	O
like	O
DIGital	O
(	O
EmailCoref	O
)	O
,	O
MONey	O
,	O
and	O
LANG	O
(	O
OntoNotes	B-DatasetName
)	O
.	O
We	O
carry	O
out	O
experiments	O
with	O
two	O
sets	O
of	O
types	O
-	O
original	O
and	O
common	O
-	O
for	O
each	O
dataset	O
.	O
The	O
common	O
set	O
of	O
types	O
include	O
the	O
following	O
5	O
categories	O
:	O
PER	O
,	O
ORG	O
,	O
LOC	O
,	O
FAC	O
,	O
OTHER	O
.	O

The	O
previous	O
experiment	O
leverages	O
the	O
original	O
entity	O
-	O
types	O
assigned	O
by	O
dataset	O
annotators	O
.	O
Due	O
to	O
the	O
differences	O
in	O
domain	O
and	O
annotation	O
guidelines	O
among	O
these	O
datasets	O
,	O
the	O
annotators	O
introduce	O
several	O
domain	O
-	O
specific	O
entity	O
types	O
(	O
e.g.	O
DIGital	O
,	O
Work	O
Of	O
Art	O
etc	O
.	O
)	O
apart	O
from	O
the	O
common	O
four	O
(	O
PERson	O
,	O
ORGanization	O
,	O
LOCation	O
,	O
FACility	O
)	O
that	O
are	O
often	O
used	O
in	O
the	O
Named	B-TaskName
Entity	I-TaskName
Recognition	I-TaskName
literature	O
(	O
Tjong	O
Kim	O
Sang	O
,	O
2002	O
)	O
.	O
The	O
former	O
can	O
prove	O
to	O
be	O
much	O
more	O
difficult	O
to	O
obtain/	O
learn	O
due	O
to	O
dearth	O
of	O
relevant	O
data	O
.	O
Therefore	O
,	O
to	O
assess	O
the	O
worth	O
of	O
using	O
a	O
common	O
entitytype	O
list	O
for	O
all	O
datasets	O
,	O
we	O
map	O
the	O
original	O
types	O
(	O
Table	O
1	O
)	O
to	O
the	O
above	O
-	O
mentioned	O
four	O
common	O
types	O
.	O
6	O
Categories	O
that	O
do	O
not	O
map	O
to	O
any	O
common	O
type	O
are	O
assigned	O
Other	O
.	O
+	O
ET	O
(	O
com	O
)	O
rows	O
in	O
Table	O
2	O
show	O
the	O
results	O
for	O
this	O
experiment	O
.	O
Models	O
trained	O
with	O
common	O
types	O
as	O
features	O
perform	O
worse	O
than	O
+	O
ET	O
(	O
orig	O
)	O
which	O
was	O
expected	O
as	O
several	O
original	O
types	O
are	O
now	O
clubbed	O
into	O
a	O
single	O
category	O
(	O
e.g.	O
LAW	B-DatasetName
-	O
>	O
OTHER	O
,	O
LANG	O
-	O
>	O
OTHER	O
)	O
thus	O
somewhat	O
reducing	O
the	O
effectiveness	O
of	O
the	O
feature	O
.	O
One	O
surprising	O
observation	O
is	O
the	O
small	O
difference	O
between	O
the	O
performance	O
on	O
OntoNotes	B-DatasetName
dataset	O
,	O
despite	O
the	O
fact	O
that	O
the	O
number	O
of	O
type	O
categories	O
reduce	O
from	O
18	O
+	O
Other	O
(	O
+	O
ET	O
(	O
orig	O
)	O
)	O
to	O
4	O
+	O
Other	O
(	O
+	O
ET	O
(	O
com	O
)	O
)	O
.	O
This	O
could	O
either	O
be	O
because	O
(	O
1	O
)	O
the	O
entities	O
with	O
corpus	O
-	O
specific	O
types	O
occur	O
less	O
frequently	O
in	O
Ontonotes	B-DatasetName
,	O
or	O
(	O
2	O
)	O
the	O
baseline	O
model	O
does	O
a	O
good	O
job	O
in	O
resolving	O
them	O
.	O
Further	O
research	O
is	O
required	O
to	O
understand	O
this	O
case	O
which	O
is	O
out	O
of	O
scope	O
for	O
this	O
work	O
.	O
Figure	O
2	O
:	O
Type	O
-	O
prediction	O
model	O
.	O

Results	O
shown	O
in	O
the	O
previous	O
section	O
assume	O
the	O
presence	O
of	O
gold	O
standard	O
types	O
during	O
training	O
as	O
well	O
as	O
inference	O
,	O
which	O
is	O
often	O
impractical	O
in	O
the	O
real	O
-	O
world	O
.	O
Most	O
of	O
the	O
new	O
samples	O
that	O
a	O
CR	O
model	O
would	O
encounter	O
would	O
not	O
include	O
type	O
information	O
about	O
the	O
candidate	O
mentions	O
.	O
Therefore	O
,	O
we	O
set	O
up	O
an	O
additional	O
experiment	O
to	O
gauge	O
the	O
benefits	O
of	O
type	O
information	O
using	O
predicted	O
types	O
.	O
We	O
introduce	O
a	O
baseline	O
approach	O
to	O
infer	O
the	O
type	O
of	O
the	O
mentions	O
and	O
then	O
use	O
these	O
predictions	O
in	O
the	O
+	O
ET	O
models	O
,	O
in	O
place	O
of	O
the	O
gold	O
types	O
,	O
for	O
coreference	B-TaskName
resolution	I-TaskName
.	O

Given	O
the	O
mention	O
and	O
its	O
immediate	O
context	O
,	O
i.e.	O
the	O
sentence	O
it	O
occurs	O
in	O
(	O
S	O
=	O
...	O
,	O
c	O
−2	O
,	O
c	O
−1	O
,	O
e	O
1	O
,	O
e	O
2	O
,	O
...	O
,	O
e	O
n	O
,	O
c	O
1	O
,	O
c	O
2	O
,	O
...	O
)	O
,	O
we	O
add	O
markers	O
<	O
ENT_START>/	O
<	O
ENT_END	O
>	O
before/	O
after	O
the	O
beginning/	O
ending	O
of	O
the	O
mention	O
in	O
the	O
sentence	O
.	O
The	O
new	O
sequence	O
(	O
S	O
=	O
...	O
,	O
c	O
−2	O
,	O
c	O
−1	O
,	O
<	O
ENT_START	O
>	O
,	O
e	O
1	O
,	O
e	O
2	O
,	O
...	O
,	O
e	O
n	O
,	O
<	O
ENT_END	O
>	O
,	O
c	O
1	O
,	O
c	O
2	O
,	O
...	O
)	O
is	O
tokenized	O
using	O
BERT	B-MethodName
tokenizer	O
and	O
passed	O
through	O
the	O
BERT	B-MethodName
encoder	O
.	O
The	O
output	O
from	O
which	O
is	O
then	O
mean	O
-	O
pooled	O
and	O
passed	O
through	O
a	O
fully	O
-	O
connected	O
layer	O
for	O
classification	O
.	O
This	O
architecture	O
is	O
motivated	O
from	O
(	O
Soares	O
et	O
al	O
,	O
2019	O
)	O
who	O
show	O
that	O
adding	O
markers	O
around	O
entities	O
before	O
passing	O
the	O
sentence	O
through	O
BERT	B-MethodName
performs	O
better	O
for	O
relation	B-TaskName
extraction	I-TaskName
.	O

Table	O
6	O
shows	O
the	O
most	O
frequently	O
occurring	O
entity	O
-	O
types	O
for	O
each	O
of	O
the	O
genres	O
in	O
OntoNotes	B-DatasetName
.	O
In	O
line	O
with	O
our	O
intuition	O
,	O
we	O
find	O
that	O
enity	O
-	O
type	O
information	O
helps	O
the	O
baseline	O
in	O
bc	O
,	O
bn	O
,	O
wb	O
,	O
and	O
mz	O
genres	O
which	O
have	O
less	O
skew	O
in	O
their	O
entity	O
-	O
type	O
distribution	O
.	O
Genres	O
like	O
bc	O
,	O
bn	O
and	O
wb	O
,	O
although	O
dominated	O
by	O
PER	O
entities	O
,	O
contain	O
a	O
substantial	O
minority	O
of	O
other	O
entity	O
-	O
types	O
like	O
ORG	O
and	O
GPE	O
.	O
Along	O
the	O
same	O
lines	O
,	O
mz	O
contains	O
a	O
majority	O
of	O
GPE	O
entities	O
but	O
also	O
enough	O
entities	O
with	O
type	O
PER	O
and	O
ORG	O
to	O
make	O
type	O
information	O
a	O
potentially	O
useful	O
feature	O
for	O
CR	O
.	O
However	O
,	O
two	O
exceptions	O
to	O
this	O
are	O
the	O
improved	O
performance	O
of	O
+	O
ET	O
(	O
orig	O
)	O
on	O
tc	O
(	O
highest	O
skew	O
)	O
and	O
no	O
significant	O
improvement	O
on	O
nw	O
(	O
lowest	O
skew	O
)	O
.	O
These	O
findings	O
prompt	O
further	O
research	O
in	O
the	O
future	O
.	O

Entity	O
coreference	O
in	O
discourse	O
often	O
takes	O
the	O
surface	O
form	O
of	O
pronouns	O
(	O
PRP	O
)	O
(	O
like	O
she	O
,	O
they	O
,	O
that	O
,	O
it	O
etc	O
.	O
)	O
or	O
noun	O
phrases	O
(	O
NP	O
)	O
(	O
like	O
LA	O
,	O
John	O
's	O
brother	O
etc	O
.	O
)	O
In	O
Table	O
4	O
,	O
we	O
compare	O
the	O
performance	O
of	O
our	O
type	B-TaskName
prediction	I-TaskName
model	O
on	O
different	O
types	O
of	O
pronouns	O
,	O
and	O
noun	O
phrases	O
of	O
varying	O
length	O
.	O
We	O
find	O
that	O
the	O
model	O
does	O
well	O
in	O
predicting	O
types	O
for	O
personal	O
pronouns	O
(	O
PRP	O
(	O
pers	O
.	O
)	O
)	O
like	O
she	O
,	O
he	O
and	O
noun	O
phrases	O
(	O
NP	O
)	O
.	O
However	O
,	O
it	O
consistently	O
underperforms	O
on	O
demonstrative	O
pro	O
-	O
nouns	O
(	O
PRP	O
(	O
dem	O
.	O
)	O
)	O
like	O
this	O
,	O
that	O
,	O
and	O
it	O
across	O
all	O
datasets	O
.	O
This	O
reduced	O
performance	O
could	O
be	O
due	O
to	O
the	O
fact	O
that	O
demonstrative	O
pronouns	O
do	O
not	O
contain	O
any	O
signal	O
about	O
the	O
type	O
of	O
the	O
entity	O
they	O
refer	O
to	O
.	O
Therefore	O
,	O
the	O
type	B-TaskName
prediction	I-TaskName
model	O
has	O
to	O
solely	O
rely	O
on	O
the	O
context	O
to	O
make	O
that	O
decision	O
.	O
However	O
,	O
this	O
is	O
not	O
the	O
case	O
with	O
PRPs	O
(	O
pers	O
.	O
)	O
and	O
NPs	O
where	O
the	O
mention	O
string	O
is	O
usually	O
a	O
strong	O
indicator	O
of	O
the	O
type	O
.	O
This	O
problem	O
is	O
worsened	O
by	O
the	O
imbalance	O
due	O
to	O
the	O
small	O
presence	O
of	O
PRP	O
(	O
dem	O
.	O
)	O
mentions	O
in	O
difference	O
CR	O
datasets	O
.	O
Since	O
,	O
the	O
model	O
does	O
not	O
encounter	O
enough	O
PRPs	O
(	O
dem	O
.	O
)	O
,	O
it	O
might	O
not	O
be	O
able	O
to	O
learn	O
to	O
give	O
high	O
importance	O
to	O
context	O
in	O
these	O
cases	O
.	O
This	O
could	O
be	O
partially	O
alleviated	O
by	O
creating	O
a	O
separate	O
type	O
-	O
prediction	O
path	O
for	O
PRP	O
(	O
dem	O
.	O
)	O
where	O
the	O
mention	O
span	O
is	O
masked	O
before	O
it	O
is	O
passed	O
through	O
the	O
model	O
.	O
A	O
model	O
that	O
is	O
trained	O
with	O
masked	O
mentions	O
would	O
focus	O
more	O
on	O
the	O
context	O
for	O
type	B-TaskName
prediction	I-TaskName
and	O
thus	O
could	O
lead	O
to	O
better	O
performance	O
on	O
PRPs	O
(	O
dem	O
.	O
)	O
.	O
One	O
could	O
also	O
experiment	O
with	O
training	O
the	O
type	O
-	O
prediction	O
model	O
on	O
all	O
of	O
the	O
mentions	O
across	O
the	O
four	O
datasets	O
.	O
The	O
common	O
list	O
of	O
types	O
introduced	O
in	O
this	O
work	O
would	O
allow	O
for	O
the	O
creation	O
of	O
a	O
larger	O
training	O
-	O
set	O
that	O
includes	O
mentions	O
from	O
multiple	O
corpora	O
(	O
including	O
external	O
NER	B-TaskName
datasets	O
)	O
which	O
could	O
provide	O
enough	O
signal	O
for	O
the	O
model	O
to	O
better	O
learn	O
the	O
common	O
types	O
for	O
PRPs	O
(	O
dem	O
.	O
)	O
.	O
Both	O
these	O
approaches	O
could	O
further	O
boost	O
the	O
results	O
for	O
CR	O
with	O
predicted	O
entity	O
-	O
types	O
,	O
ultimately	O
,	O
reducing	O
the	O
gap	O
between	O
the	O
scores	O
in	O
Table	O
2	O
and	O
5	O
.	O
However	O
,	O
they	O
are	O
left	O
as	O
future	O
work	O
as	O
they	O
are	O
out	O
of	O
scope	O
for	O
this	O
paper	O
.	O

Table	O
7	O
provides	O
an	O
excerpt	O
of	O
an	O
email	O
from	O
EmailCoref	O
corpus	O
.	O
As	O
shown	O
,	O
the	O
baseline	O
model	O
predicts	O
the	O
coreference	O
clusters	O
for	O
an	O
organizer	O
(	O
DIG	O
)	O
and	O
PricewaterhouseCoopers	O
Calgary	O
(	O
ORG	O
)	O
incorrectly	O
.	O
For	O
the	O
former	O
,	O
the	O
model	O
mistakes	O
it	O
as	O
a	O
reference	O
to	O
your	O
current	O
home	O
address	O
(	O
LOC	O
)	O
which	O
is	O
corrected	O
by	O
the	O
entity	O
-	O
type	O
aware	O
models	O
.	O
For	O
the	O
latter	O
,	O
the	O
baseline	O
considers	O
PricewaterhouseCoopers	O
Calgary	O
(	O
PCC	B-DatasetName
)	O
as	O
part	O
of	O
a	O
new	O
coreference	O
cluster	O
,	O
even	O
though	O
it	O
refers	O
to	O
the	O
organization	O
of	O
the	O
email	O
's	O
sender	O
which	O
was	O
previously	O
referred	O
to	O
as	O
we	O
in	O
the	O
email	O
.	O
Models	O
with	O
access	O
to	O
gold	O
type	O
information	O
(	O
+	O
ET	O
(	O
orig	O
)	O
and	O
+	O
ET	O
(	O
com	O
)	O
)	O
are	O
able	O
to	O
make	O
that	O
connection	O
.	O
+	O
ET	O
-	O
pred	O
(	O
orig	O
)	O
,	O
however	O
,	O
is	O
unable	O
to	O
cluster	O
PCC	B-DatasetName
correctly	O
which	O
could	O
be	O
due	O
to	O
the	O
fact	O
that	O
the	O
type	O
-	O
prediction	O
model	O
incorrectly	O
classifies	O
the	O
type	O
of	O
we	O
as	O
PER	O
rather	O
than	O
ORG	O
.	O
This	O
could	O
lead	O
to	O
the	O
CR	O
model	O
considering	O
PCC	B-DatasetName
(	O
ORG	O
)	O
as	O
a	O
new	O
entity	O
in	O
the	O
discourse	O
rather	O
than	O
a	O
postcedent	O
of	O
we	O
.	O
This	O
example	O
demonstrates	O
that	O
sentencelevel	O
context	O
might	O
not	O
be	O
sufficient	O
in	O
some	O
cases	O
for	O
mention	O
type	O
-	O
disambiguation	O
.	O
We	O
intend	O
to	O
experiment	O
with	O
models	O
that	O
capture	O
long	O
-	O
term	O
context	O
and	O
leverage	O
external	O
knowledge	O
in	O
the	O
future	O
.	O

In	O
this	O
work	O
,	O
we	O
show	O
the	O
importance	O
of	O
using	O
entity	O
-	O
type	O
information	O
in	O
neural	O
coreference	B-TaskName
resolution	I-TaskName
(	O
CR	O
)	O
models	O
with	O
contextualized	O
embeddings	O
like	O
BERT	B-MethodName
.	O
Models	O
which	O
leverage	O
type	O
information	O
,	O
annotated	O
in	O
the	O
corpus	O
,	O
substantially	O
outperform	O
the	O
baseline	O
on	O
four	O
CR	O
datasets	O
by	O
reducing	O
the	O
number	O
of	O
type	O
mismatches	O
in	O
detected	O
coreference	O
clusters	O
.	O
Since	O
,	O
these	O
datasets	O
vary	O
in	O
number	O
and	O
categories	O
of	O
the	O
types	O
they	O
define	O
,	O
we	O
also	O
experiment	O
with	O
mapping	O
the	O
original	O
corpus	O
types	O
to	O
four	O
common	O
types	O
(	O
PER	O
,	O
ORG	O
,	O
LOC	O
,	O
FAC	O
)	O
based	O
on	O
previous	O
NER	B-TaskName
research	O
that	O
can	O
be	O
learnt	O
more	O
easily	O
through	O
large	O
NER	B-TaskName
datasets	O
.	O
Models	O
which	O
use	O
these	O
common	O
types	O
perform	O
slightly	O
worse	O
than	O
original	O
types	O
but	O
still	O
show	O
significant	O
improvements	O
over	O
the	O
baseline	O
systems	O
.	O
The	O
presence	O
of	O
gold	O
standard	O
types	O
during	O
CR	O
inference	O
is	O
unlikely	O
in	O
practice	O
.	O
Therefore	O
,	O
we	O
propose	O
a	O
model	O
that	O
infers	O
the	O
type	O
of	O
a	O
mention	O
given	O
the	O
mention	O
span	O
and	O
its	O
immediate	O
context	O
to	O
use	O
along	O
side	O
the	O
proposed	O
CR	O
approach	O
.	O
In	O
our	O
evaluation	O
,	O
we	O
find	O
that	O
using	O
types	O
predicted	O
by	O
our	O
model	O
for	O
CR	O
still	O
performs	O
significantly	O
better	O
than	O
the	O
baseline	O
,	O
thus	O
offering	O
stronger	O
evidence	O
that	O
type	O
information	O
holds	O
the	O
potential	O
for	O
practical	O
improvements	O
for	O
CR	O
.	O
Dropout	B-MethodName
0.2	O
(	O
com	O
)	O
experiments	O
.	O
These	O
types	O
are	O
annotated	O
in	O
most	O
of	O
the	O
named	O
-	O
entity	O
recognition	O
datasets	O
and	O
therefore	O
are	O
easier	O
to	O
model	O
and	O
learn	O
via	O
machine	O
learning	O
approaches	O
.	O
Tables	O
A2	O
,	O
A3	O
,	O
A4	O
,	O
A5	O
show	O
the	O
mapping	O
between	O
the	O
original	O
types	O
of	O
each	O
coreference	O
dataset	O
used	O
in	O
our	O
study	O
to	O
the	O
reduced	O
common	O
types	O
.	O
The	O
most	O
drastic	O
difference	O
occurs	O
for	O
OntoNotes	B-DatasetName
(	O
19	O
-	O
>	O
5	O
)	O
and	O
Wi	O
-	O
kiCoref	O
(	O
8	O
-	O
>	O
5	O
)	O
.	O
OTHER	O
type	O
in	O
WikiCoref	B-DatasetName
is	O
for	O
freebase	O
links	O
that	O
did	O
not	O
have	O
an	O
associated	O
type	O
stored	O
in	O
freebase	O
,	O
whereas	O
NA	O
is	O
used	O
for	O
mentions	O
which	O
do	O
not	O
have	O
a	O
freebase	O
link	O
.	O
For	O
OntoNotes	B-DatasetName
,	O
NA	O
refers	O
to	O
the	O
mentions	O
that	O
did	O
not	O
get	O
any	O
type	O
assigned	O
to	O
them	O
even	O
after	O
the	O
use	O
of	O
our	O
cluster	O
based	O
type	O
-	O
propagation	O
approach	O
(	O
explained	O
in	O
Section	O
4	O
)	O
.	O

A	O
Position	O
-	O
aware	O
Bidirectional	O
Attention	O
Network	O
for	O
Aspect	O
-	O
level	O
Sentiment	B-TaskName
Analysis	I-TaskName

Aspect	O
-	O
level	O
sentiment	B-TaskName
analysis	I-TaskName
aims	O
to	O
distinguish	O
the	O
sentiment	O
polarity	O
of	O
each	O
specific	O
aspect	O
term	O
in	O
a	O
given	O
sentence	O
.	O
Both	O
industry	O
and	O
academia	O
have	O
realized	O
the	O
importance	O
of	O
the	O
relationship	O
between	O
aspect	O
term	O
and	O
sentence	O
,	O
and	O
made	O
attempts	O
to	O
model	O
the	O
relationship	O
by	O
designing	O
a	O
series	O
of	O
attention	O
models	O
.	O
However	O
,	O
most	O
existing	O
methods	O
usually	O
neglect	O
the	O
fact	O
that	O
the	O
position	O
information	O
is	O
also	O
crucial	O
for	O
identifying	O
the	O
sentiment	O
polarity	O
of	O
the	O
aspect	O
term	O
.	O
When	O
an	O
aspect	O
term	O
occurs	O
in	O
a	O
sentence	O
,	O
its	O
neighboring	O
words	O
should	O
be	O
given	O
more	O
attention	O
than	O
other	O
words	O
with	O
long	O
distance	O
.	O
Therefore	O
,	O
we	O
propose	O
a	O
position	O
-	O
aware	O
bidirectional	O
attention	O
network	O
(	O
PBAN	O
)	O
based	O
on	O
bidirectional	B-MethodName
GRU	I-MethodName
.	O
PBAN	O
not	O
only	O
concentrates	O
on	O
the	O
position	O
information	O
of	O
aspect	O
terms	O
,	O
but	O
also	O
mutually	O
models	O
the	O
relation	O
between	O
aspect	O
term	O
and	O
sentence	O
by	O
employing	O
bidirectional	O
attention	O
mechanism	O
.	O
The	O
experimental	O
results	O
on	O
SemEval	O
2014	O
Datasets	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
proposed	O
PBAN	O
model	O
.	O

Sentiment	B-TaskName
analysis	I-TaskName
,	O
also	O
known	O
as	O
opinion	B-TaskName
mining	I-TaskName
(	O
Liu	O
,	O
2012	O
;	O
Pang	O
et	O
al	O
,	O
2008	O
)	O
,	O
is	O
a	O
vital	O
task	O
in	O
Natural	O
Language	O
Processing	O
(	O
NLP	O
)	O
.	O
It	O
divides	O
the	O
text	O
into	O
two	O
or	O
more	O
classes	O
according	O
to	O
the	O
affective	O
states	O
and	O
the	O
subjective	O
information	O
of	O
the	O
text	O
,	O
and	O
has	O
received	O
plenty	O
of	O
attention	O
from	O
both	O
industry	O
and	O
academia	O
.	O
In	O
this	O
paper	O
,	O
we	O
address	O
the	O
aspect	O
-	O
level	O
sentiment	B-TaskName
analysis	I-TaskName
,	O
which	O
is	O
a	O
fine	O
-	O
grained	O
task	O
in	O
the	O
field	O
of	O
sentiment	B-TaskName
analysis	I-TaskName
.	O
For	O
instance	O
,	O
given	O
the	O
mentioned	O
aspect	O
terms	O
{	O
menu	O
,	O
server	O
,	O
specials	O
}	O
,	O
and	O
the	O
sentence	O
is	O
"	O
The	O
menu	O
looked	O
good	O
,	O
except	O
for	O
offering	O
the	O
Chilean	O
Sea	O
Bass	O
,	O
but	O
the	O
server	O
does	O
not	O
offer	O
up	O
the	O
specials	O
that	O
were	O
written	O
on	O
the	O
board	O
outside	O
.	O
"	O
.	O
For	O
aspect	O
term	O
menu	O
,	O
the	O
sentiment	O
polarity	O
is	O
positive	O
,	O
but	O
for	O
aspect	O
term	O
server	O
,	O
the	O
polarity	O
is	O
negative	O
while	O
for	O
specials	O
,	O
the	O
polarity	O
is	O
neutral	O
.	O
One	O
important	O
challenge	O
in	O
aspect	O
-	O
level	O
sentiment	B-TaskName
analysis	I-TaskName
is	O
how	O
to	O
model	O
the	O
semantic	O
relationship	O
between	O
aspect	O
terms	O
and	O
sentences	O
.	O
Traditional	O
approaches	O
have	O
defined	O
rich	O
features	O
about	O
content	O
and	O
syntactic	O
structures	O
so	O
as	O
to	O
capture	O
the	O
sentiment	O
polarity	O
(	O
Jiang	O
et	O
al	O
,	O
2011	O
)	O
.	O
However	O
this	O
kind	O
of	O
feature	O
-	O
based	O
method	O
is	O
labor	O
-	O
intensive	O
and	O
highly	O
depends	O
on	O
the	O
quality	O
of	O
the	O
features	O
.	O
Compared	O
with	O
these	O
methods	O
,	O
neural	O
network	O
architectures	O
are	O
capable	O
of	O
learning	O
features	O
without	O
feature	B-TaskName
engineering	I-TaskName
,	O
and	O
have	O
been	O
widely	O
used	O
in	O
a	O
variety	O
of	O
NLP	O
tasks	O
such	O
as	O
machine	B-TaskName
translation	I-TaskName
,	O
question	B-TaskName
answering	I-TaskName
(	O
Andreas	O
et	O
al	O
,	O
2016	O
)	O
and	O
text	B-TaskName
classification	I-TaskName
(	O
Lai	O
et	O
al	O
,	O
2015	O
)	O
.	O
Recently	O
,	O
with	O
the	O
development	O
of	O
the	O
neural	O
networks	O
,	O
they	O
are	O
also	O
applied	O
to	O
target	O
-	O
dependent	O
sentiment	B-TaskName
analysis	I-TaskName
1	O
,	O
such	O
as	O
Target	O
-	O
Dependent	O
LSTM	B-MethodName
(	O
TD	O
-	O
LSTM	B-MethodName
)	O
(	O
Tang	O
et	O
al	O
,	O
2015	O
)	O
and	O
Target	O
-	O
Connection	O
LSTM	B-MethodName
(	O
TC	O
-	O
LSTM	B-MethodName
)	O
(	O
Tang	O
et	O
al	O
,	O
2015	O
)	O
.	O
However	O
,	O
these	O
neural	O
network	O
-	O
based	O
methods	O
can	O
not	O
effectively	O
identify	O
which	O
words	O
in	O
the	O
sentence	O
are	O
more	O
important	O
.	O
Fortunately	O
,	O
attention	O
mechanisms	O
are	O
an	O
effective	O
way	O
to	O
solve	O
this	O
problem	O
.	O
Attention	O
,	O
which	O
is	O
widely	O
applied	O
to	O
Computer	O
Vision	O
(	O
CV	O
)	O
and	O
NLP	O
fields	O
,	O
is	O
an	O
effective	O
mechanism	O
and	O
has	O
been	O
demonstrated	O
in	O
image	B-TaskName
recognition	I-TaskName
(	O
Mnih	O
et	O
al	O
,	O
2014	O
)	O
,	O
machine	B-TaskName
translation	I-TaskName
Luong	O
et	O
al	O
,	O
2015	O
)	O
and	O
reading	B-TaskName
comprehension	I-TaskName
(	O
Hermann	O
et	O
al	O
,	O
2015	O
;	O
Cui	O
et	O
al	O
,	O
2016	O
)	O
.	O
Therefore	O
,	O
some	O
researchers	O
have	O
designed	O
attention	O
networks	O
to	O
address	O
the	O
aspect	O
-	O
level	O
sentiment	B-TaskName
analysis	I-TaskName
and	O
have	O
obtained	O
comparable	O
results	O
,	O
such	O
as	O
AE	B-MethodName
-	O
LSTM	B-MethodName
,	O
ATAE	O
-	O
LSTM	B-MethodName
and	O
IAN	B-MethodName
(	O
Ma	O
et	O
al	O
,	O
2017	O
)	O
.	O
However	O
,	O
these	O
existing	O
work	O
ignores	O
or	O
does	O
not	O
explicitly	O
model	O
the	O
position	O
information	O
of	O
the	O
aspect	O
term	O
in	O
a	O
sentence	O
,	O
which	O
has	O
been	O
studied	O
for	O
improving	O
performance	O
in	O
information	B-TaskName
retrieval	I-TaskName
(	O
IR	O
)	O
.	O
In	O
,	O
the	O
occurrence	O
positions	O
of	O
the	O
query	O
terms	O
were	O
modeled	O
via	O
kernel	O
functions	O
and	O
then	O
integrated	O
into	O
traditional	O
IR	O
models	O
to	O
boost	O
the	O
retrieval	O
performance	O
.	O
By	O
analyzing	O
this	O
aspect	O
-	O
level	O
sentiment	B-TaskName
analysis	I-TaskName
task	O
and	O
the	O
corresponding	O
dataset	O
,	O
we	O
find	O
that	O
when	O
an	O
aspect	O
term	O
occurs	O
in	O
a	O
sentence	O
,	O
its	O
neighboring	O
words	O
in	O
the	O
sentence	O
should	O
be	O
given	O
more	O
attention	O
than	O
other	O
words	O
with	O
long	O
distance	O
.	O
Let	O
us	O
take	O
"	O
It	O
's	O
a	O
perfect	O
place	O
to	O
have	O
an	O
amazing	O
indian	O
food	O
.	O
"	O
as	O
an	O
example	O
,	O
when	O
the	O
aspect	O
term	O
is	O
indian	O
food	O
,	O
its	O
corresponding	O
sentiment	O
polarity	O
is	O
positive	O
.	O
Intuitively	O
,	O
we	O
can	O
see	O
that	O
the	O
neighboring	O
word	O
of	O
the	O
indian	O
food	O
(	O
i.e.	O
"	O
amazing	O
"	O
)	O
has	O
a	O
greater	O
contribution	O
to	O
judge	O
the	O
sentiment	O
polarity	O
of	O
the	O
aspect	O
term	O
than	O
other	O
words	O
with	O
long	O
distance	O
such	O
as	O
"	O
to	O
"	O
and	O
"	O
have	O
"	O
.	O
Sometimes	O
this	O
intuitive	O
idea	O
of	O
judging	O
the	O
sentiment	O
polarity	O
may	O
be	O
interpreted	O
as	O
a	O
cognitive	O
activity	O
,	O
which	O
also	O
can	O
be	O
rephrased	O
in	O
a	O
quantum	O
-	O
like	O
language	O
model	O
(	O
Niu	O
et	O
al	O
,	O
2017	O
)	O
.	O
To	O
be	O
specific	O
,	O
sentiment	O
polarity	O
may	O
be	O
interpreted	O
as	O
a	O
quantum	O
-	O
like	O
cognition	O
state	O
.	O
Inspired	B-DatasetName
by	O
this	O
,	O
we	O
go	O
one	O
step	O
further	O
and	O
propose	O
a	O
position	O
-	O
aware	O
bidirectional	O
attention	O
network	O
(	O
PBAN	O
)	O
based	O
on	O
bidirectional	O
Gated	O
Recurrent	O
Units	O
(	O
Bi	O
-	O
GRU	B-MethodName
)	O
.	O
In	O
addition	O
to	O
utilizing	O
the	O
position	O
information	O
,	O
PBAN	O
also	O
mutually	O
models	O
the	O
relationship	O
between	O
the	O
sentence	O
and	O
different	O
words	O
in	O
the	O
aspect	O
term	O
by	O
adopting	O
a	O
bidirectional	O
attention	O
mechanism	O
.	O
To	O
be	O
specific	O
,	O
our	O
model	O
consists	O
of	O
three	O
components	O
:	O
1	O
)	O
Obtaining	O
position	O
information	O
of	O
each	O
word	O
in	O
corresponding	O
sentence	O
based	O
on	O
the	O
current	O
aspect	O
term	O
,	O
then	O
converting	O
the	O
position	O
information	O
into	O
position	O
embedding	O
.	O
2	O
)	O
The	O
PBAN	O
composes	O
of	O
two	O
Bi	O
-	O
GRU	B-MethodName
networks	O
focusing	O
on	O
extracting	O
the	O
aspectlevel	O
features	O
and	O
sentence	O
-	O
level	O
features	O
respectively	O
.	O
3	O
)	O
Using	O
the	O
bidirectional	O
attention	O
mechanism	O
to	O
model	O
the	O
mutual	O
relation	O
between	O
aspect	O
term	O
and	O
its	O
corresponding	O
sentence	O
.	O
We	O
evaluate	O
our	O
models	O
on	O
SemEval	O
2014	O
Datasets	O
,	O
and	O
the	O
results	O
show	O
that	O
our	O
models	O
are	O
more	O
effective	O
than	O
other	O
previous	O
methods	O
.	O
The	O
main	O
contributions	O
of	O
our	O
work	O
can	O
be	O
summarized	O
as	O
follows	O
:	O
(	O
1	O
)	O
We	O
attempt	O
to	O
explicitly	O
investigate	O
the	O
effectiveness	O
of	O
the	O
position	O
information	O
of	O
aspect	O
term	O
for	O
aspect	O
-	O
level	O
sentiment	B-TaskName
analysis	I-TaskName
.	O
(	O
2	O
)	O
We	O
propose	O
a	O
position	O
-	O
aware	O
bidirectional	O
attention	O
network	O
(	O
PBAN	O
)	O
based	O
on	O
Bi	O
-	O
GRU	B-MethodName
,	O
which	O
has	O
been	O
proved	O
to	O
be	O
effective	O
to	O
improve	O
the	O
sentiment	B-TaskName
analysis	I-TaskName
performance	O
.	O
(	O
3	O
)	O
We	O
apply	O
a	O
bidirectional	O
attention	O
mechanism	O
,	O
which	O
can	O
enhance	O
the	O
mutual	O
relation	O
between	O
the	O
aspect	O
term	O
and	O
its	O
corresponding	O
sentence	O
,	O
and	O
prevent	O
the	O
irrelevant	O
words	O
from	O
getting	O
more	O
attention	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
the	O
proposed	O
model	O
position	O
-	O
aware	O
bidirectional	O
attention	O
network	O
(	O
PBAN	O
)	O
for	O
aspect	O
-	O
level	O
sentiment	B-TaskName
analysis	I-TaskName
and	O
PBAN	O
is	O
shown	O
in	O
Figure	O
1	O
.	O
In	O
this	O
paper	O
,	O
the	O
set	O
of	O
sentiment	O
polarity	O
of	O
the	O
aspect	O
term	O
is	O
{	O
positive	O
,	O
negative	O
,	O
neutral	O
}	O
.	O

As	O
for	O
how	O
to	O
model	O
the	O
position	O
information	O
of	O
the	O
aspect	O
term	O
in	O
its	O
corresponding	O
sentence	O
,	O
inspired	O
by	O
the	O
position	O
encoding	O
vectors	O
used	O
in	O
(	O
Collobert	O
et	O
al	O
,	O
2011	O
;	O
Zeng	O
et	O
al	O
,	O
2014	O
)	O
,	O
we	O
define	O
a	O
position	O
index	O
sequence	O
whose	O
length	O
is	O
equal	O
to	O
the	O
length	O
of	O
corresponding	O
sentence	O
.	O
Suppose	O
that	O
if	O
a	O
word	O
in	O
the	O
aspect	O
term	O
occurs	O
in	O
the	O
sentence	O
,	O
then	O
its	O
position	O
index	O
will	O
be	O
marked	O
as	O
"	O
0	B-DatasetName
"	O
,	O
and	O
the	O
position	O
index	O
of	O
other	O
words	O
will	O
be	O
represented	O
as	O
the	O
relative	O
distance	O
to	O
the	O
current	O
aspect	O
term	O
.	O
p	O
i	O
=	O
	O
	O
	O
	O
|	O
i	O
−	O
j	O
s	O
|	O
,	O
i	O
<	O
j	O
s	O
0	B-DatasetName
,	O
j	O
s	O
≤	O
i	O
≤	O
j	O
e	O
|	O
i	O
−	O
j	O
e	O
|	O
,	O
i	O
>	O
j	O
e	O
(	O
1	O
)	O
…	O
Word	O
embedding	O
Position	O
embedding	O
…	O
…	O
N	O
w	O
1	O
w	O
1	O
p	O
N	O
p	O
Bi	O
-	O
GRU	B-MethodName
1	O
h	O
2	O
h	O
…	O
N	O
h	O
Bi	O
-	O
GRU	B-MethodName
1	O
t	O
h	O
2	O
t	O
h	O
…	O
M	O
t	O
h	O
M	O
t	O
1	O
t	O
Mean	O
Pool	O
Attention	O
Mechanism	O
1	O
	O
R	O
h	O
11	O
	O
12	O
	O
1N	O
	O
1	O
M	O
	O
2	O
M	O
	O
MN	O
	O
2	O
	O
M	O
	O
…	O
…	O
…	O
…	O
⨀	O
…	O
…	O
⨀	O
dot	O
product	O
dot	O
product	O
1	O
s	O
2	O
s	O
M	O
s	O
Term	O
embedding	O
	O
Figure	O
1	O
:	O
The	O
architecture	O
of	O
position	O
-	O
aware	O
bidirectional	O
attention	O
network	O
for	O
aspect	O
-	O
level	O
sentiment	B-TaskName
analysis	I-TaskName
(	O
PBAN	O
)	O
.	O
{	O
w	O
1	O
,	O
w	O
2	O
,	O
...	O
,	O
w	O
N	O
}	O
represents	O
the	O
word	O
embedding	O
in	O
a	O
sentence	O
whose	O
length	O
is	O
N	O
,	O
and	O
{	O
t	O
1	O
,	O
t	O
2	O
,	O
...	O
,	O
t	O
M	O
}	O
represents	O
the	O
aspect	O
term	O
embedding	O
whose	O
length	O
is	O
M	O
.	O
{	O
p	O
1	O
,	O
p	O
2	O
,	O
...	O
,	O
p	O
N	O
}	O
is	O
the	O
position	O
embedding	O
of	O
the	O
aspect	O
term	O
,	O
which	O
is	O
concatenated	O
to	O
the	O
word	O
embedding	O
.	O
{	O
h	O
1	O
,	O
h	O
2	O
,	O
...	O
,	O
h	O
N	O
}	O
denotes	O
the	O
hidden	O
representation	O
of	O
inputs	O
and	O
{	O
h	O
t	O
1	O
,	O
h	O
t	O
2	O
,	O
...	O
,	O
h	O
t	O
M	O
}	O
indicates	O
the	O
hidden	O
representation	O
of	O
aspect	O
term	O
.	O
where	O
,	O
j	O
s	O
and	O
j	O
e	O
denote	O
the	O
starting	O
and	O
ending	O
indices	O
of	O
the	O
aspect	O
term	O
respectively	O
,	O
and	O
p	O
i	O
can	O
be	O
viewed	O
as	O
the	O
relative	O
distance	O
of	O
the	O
i	O
-	O
th	O
word	O
in	O
sentence	O
to	O
the	O
aspect	O
term	O
.	O
For	O
example	O
,	O
given	O
a	O
sentence	O
"	O
not	O
only	O
was	O
the	O
food	O
outstanding	O
but	O
the	O
little	O
perks	O
were	O
great	O
.	O
"	O
,	O
and	O
the	O
aspect	O
term	O
is	O
food	O
,	O
then	O
the	O
position	O
index	O
sequence	O
is	O
represented	O
as	O
p	O
=	O
[	O
4	O
,	O
3	O
,	O
2	O
,	O
1	O
,	O
0	B-DatasetName
,	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
,	O
5	O
,	O
6	O
,	O
7	O
]	O
.	O
And	O
its	O
corresponding	O
position	O
embedding	O
are	O
obtained	O
by	O
looking	O
up	O
a	O
position	O
embedding	O
matrix	O
P	O
R	O
dp×N	O
,	O
which	O
is	O
randomly	O
initialized	O
,	O
and	O
updated	O
during	O
the	O
training	O
process	O
.	O
Here	O
,	O
d	O
p	O
denotes	O
the	O
dimension	O
of	O
position	O
embedding	O
and	O
N	O
indicates	O
the	O
length	O
of	O
the	O
sentence	O
.	O
After	O
the	O
position	O
index	O
is	O
converted	O
to	O
the	O
embedding	O
,	O
the	O
position	O
embedding	O
can	O
model	O
the	O
different	O
weights	O
of	O
words	O
with	O
different	O
distance	O
.	O
From	O
this	O
example	O
,	O
it	O
is	O
obvious	O
that	O
the	O
words	O
with	O
smaller	O
relative	O
distances	O
(	O
such	O
as	O
"	O
outstanding	O
"	O
)	O
play	O
an	O
more	O
important	O
role	O
in	O
judging	O
the	O
sentiment	O
polarity	O
of	O
food	O
.	O
We	O
can	O
find	O
that	O
this	O
process	O
is	O
basically	O
consistent	O
with	O
the	O
way	O
people	O
judge	O
the	O
sentiment	O
polarity	O
of	O
the	O
aspect	O
term	O
.	O
Because	O
we	O
usually	O
first	O
observe	O
the	O
neighboring	O
words	O
of	O
the	O
aspect	O
term	O
,	O
judging	O
whether	O
the	O
neighboring	O
words	O
can	O
show	O
its	O
sentiment	O
polarity	O
,	O
after	O
that	O
we	O
will	O
focus	O
on	O
those	O
words	O
with	O
long	O
distance	O
.	O

Bidirectional	O
LSTMs	O
have	O
been	O
successfully	O
applied	O
to	O
various	O
NLP	O
tasks	O
,	O
and	O
it	O
models	O
the	O
context	O
dependency	O
with	O
the	O
forward	O
LSTM	B-MethodName
and	O
the	O
backward	O
LSTM	B-MethodName
.	O
The	O
forward	O
L	O
-	O
STM	O
handles	O
the	O
sentence	O
from	O
left	O
to	O
right	O
,	O
and	O
the	O
backward	O
LSTM	B-MethodName
processes	O
it	O
in	O
the	O
reverse	O
order	O
.	O
Therefore	O
,	O
we	O
can	O
obtain	O
two	O
hidden	O
representation	O
,	O
and	O
then	O
concatenate	O
the	O
forward	O
hidden	O
state	O
and	O
backward	O
hidden	O
state	O
of	O
each	O
word	O
.	O
In	O
this	O
paper	O
,	O
we	O
choose	O
to	O
use	O
bidirectional	B-MethodName
GRU	I-MethodName
since	O
it	O
performs	O
similarly	O
to	O
bidirectional	B-MethodName
LSTM	I-MethodName
but	O
has	O
fewer	O
parameters	O
and	O
lower	O
computational	O
complexity	O
.	O
Concretely	O
,	O
we	O
firstly	O
obtain	O
the	O
representation	O
of	O
each	O
word	O
in	O
aspect	O
term	O
and	O
sentence	O
,	O
and	O
formalize	O
the	O
notations	O
in	O
our	O
work	O
.	O
We	O
suppose	O
that	O
a	O
sentence	O
consists	O
of	O
N	O
words	O
[	O
w	O
1	O
,	O
w	O
2	O
,	O
...	O
,	O
w	O
N	O
]	O
and	O
an	O
aspect	O
term	O
contains	O
M	O
words	O
[	O
t	O
1	O
,	O
t	O
2	O
,	O
...	O
,	O
t	O
M	O
]	O
,	O
then	O
we	O
get	O
sentence	B-TaskName
embedding	I-TaskName
and	O
aspect	O
term	O
embedding	O
by	O
looking	O
up	O
a	O
word	O
embedding	O
matrix	O
E	O
R	O
d×v	O
respectively	O
,	O
where	O
d	O
denotes	O
the	O
dimension	O
of	O
the	O
embedding	O
,	O
and	O
v	O
indicates	O
the	O
vocabulary	O
size	O
.	O
Then	O
we	O
input	O
aspect	O
term	O
embeddings	O
into	O
the	O
left	O
Bi	O
-	O
GRU	B-MethodName
to	O
get	O
the	O
hidden	O
contextual	O
representa	O
-	O
tion	O
,	O
which	O
consists	O
of	O
forward	O
hidden	O
state	O
−	O
h	O
t	O
i	O
R	O
d	O
h	O
and	O
backward	O
hidden	O
state	O
−	O
h	O
t	O
i	O
R	O
d	O
h	O
,	O
where	O
d	O
h	O
denotes	O
the	O
number	O
of	O
hidden	O
units	O
.	O
Finally	O
,	O
the	O
hidden	O
contextual	O
representation	O
of	O
aspect	O
term	O
h	O
t	O
i	O
is	O
obtained	O
by	O
concatenating	O
−	O
h	O
t	O
i	O
and	O
−	O
h	O
t	O
i	O
,	O
i.e.	O
,	O
h	O
t	O
i	O
=	O
[	O
−	O
h	O
t	O
i	O
;	O
−	O
h	O
t	O
i	O
]	O
R	O
2d	O
h	O
.	O
For	O
the	O
right	O
Bi	O
-	O
GRU	B-MethodName
structure	O
,	O
we	O
take	O
the	O
concatenation	O
of	O
the	O
position	O
embedding	O
and	O
word	O
embedding	O
as	O
the	O
inputs	O
,	O
then	O
we	O
can	O
obtain	O
the	O
final	O
hidden	O
contextual	O
representation	O
of	O
the	O
inputs	O
,	O
i.e.	O
,	O
h	O
i	O
=	O
[	O
−	O
h	O
i	O
;	O
−	O
h	O
i	O
]	O
R	O
2d	O
h	O
.	O

In	O
this	O
section	O
,	O
we	O
design	O
a	O
series	O
of	O
models	O
to	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
PBAN	O
model	O
.	O
Firstly	O
,	O
we	O
design	O
an	O
ATAE	O
-	O
Bi	O
-	O
GRU	B-MethodName
model	O
,	O
whose	O
structure	O
is	O
similar	O
with	O
ATAE	O
-	O
LSTM	B-MethodName
.	O
The	O
only	O
difference	O
between	O
these	O
two	O
models	O
is	O
that	O
ATAE	O
-	O
Bi	O
-	O
GRU	B-MethodName
uses	O
the	O
Bi	O
-	O
GRU	B-MethodName
structure	O
rather	O
than	O
LSTM	B-MethodName
,	O
and	O
other	O
design	O
is	O
the	O
same	O
as	O
ATAE	O
-	O
LSTM	B-MethodName
.	O
Next	O
we	O
design	O
a	O
BAN	O
model	O
without	O
modeling	O
position	O
embedding	O
,	O
and	O
it	O
just	O
utilizes	O
the	O
representation	O
of	O
aspect	O
term	O
and	O
sentence	O
.	O
In	O
BAN	O
,	O
we	O
still	O
adopt	O
bidirectional	O
attention	O
mechanism	O
to	O
model	O
the	O
relation	O
between	O
aspect	O
term	O
and	O
sentence	O
as	O
PBAN	O
does	O
.	O
The	O
only	O
difference	O
between	O
BAN	O
and	O
PBAN	O
is	O
that	O
BAN	O
without	O
taking	O
the	O
position	O
embedding	O
as	O
a	O
part	O
of	O
inputs	O
.	O
Moreover	O
,	O
we	O
also	O
design	O
a	O
PAN	O
model	O
,	O
whose	O
structure	O
is	O
similar	O
with	O
the	O
ATAE	O
-	O
Bi	O
-	O
GRU	B-MethodName
model	O
.	O
PAN	O
takes	O
the	O
concatenation	O
of	O
the	O
aspect	O
term	O
embedding	O
and	O
the	O
word	O
embedding	O
as	O
the	O
inputs	O
of	O
the	O
Bi	O
-	O
GRU	B-MethodName
structure	O
to	O
obtain	O
the	O
hidden	O
contextual	O
representation	O
,	O
and	O
then	O
PAN	O
utilizes	O
this	O
representation	O
and	O
the	O
position	O
embedding	O
of	O
the	O
aspect	O
term	O
to	O
calculate	O
the	O
attention	O
weights	O
,	O
so	O
as	O
to	O
effectively	O
judge	O
the	O
sentiment	O
polarity	O
of	O
an	O
aspect	O
term	O
.	O
From	O
Table	O
3	O
,	O
we	O
can	O
find	O
that	O
PBAN	O
achieves	O
the	O
best	O
performance	O
among	O
these	O
models	O
.	O
Because	O
Bi	O
-	O
GRU	B-MethodName
structure	O
has	O
a	O
big	O
advantage	O
over	O
LSTM	B-MethodName
,	O
it	O
is	O
obvious	O
that	O
ATAE	O
-	O
Bi	O
-	O
GRU	B-MethodName
model	O
performs	O
better	O
than	O
ATAE	O
-	O
LSTM	B-MethodName
model	O
.	O
For	O
PAN	O
model	O
,	O
it	O
outperforms	O
ATAE	O
-	O
LSTM	B-MethodName
and	O
ATAE	O
-	O
Bi	O
-	O
GRU	B-MethodName
models	O
,	O
but	O
it	O
is	O
worse	O
than	O
BAN	O
model	O
.	O
Compared	O
with	O
ATAE	O
-	O
Bi	O
-	O
GRU	B-MethodName
,	O
the	O
most	O
difference	O
is	O
that	O
PAN	O
utilizes	O
the	O
position	O
embedding	O
to	O
calculate	O
the	O
attention	O
weights	O
rather	O
than	O
the	O
aspect	O
term	O
embedding	O
like	O
ATAE	O
-	O
Bi	O
-	O
GRU	B-MethodName
.	O
Therefore	O
,	O
according	O
to	O
these	O
three	O
experimental	O
results	O
,	O
we	O
can	O
prove	O
the	O
importance	O
of	O
the	O
position	O
information	O
in	O
aspect	O
-	O
level	O
sentiment	B-TaskName
analysis	I-TaskName
task	O
.	O
As	O
for	O
BAN	O
model	O
,	O
it	O
outperforms	O
IAN	B-MethodName
model	O
while	O
performs	O
worse	O
than	O
PBAN	O
model	O
.	O
Because	O
Aspect	O
term	O
Sentence	O
Polarity	O
pizza	O
This	O
is	O
one	O
great	O
place	O
to	O
eat	O
pizza	O
more	O
out	O
but	O
not	O
a	O
good	O
place	O
for	O
take	O
-	O
out	O
pizza	O
.	O
positive	O
take	O
-	O
out	O
pizza	O
This	O
is	O
one	O
great	O
place	O
to	O
eat	O
pizza	O
more	O
out	O
but	O
not	O
a	O
good	O
place	O
for	O
take	O
-	O
out	O
pizza	O
.	O
negative	O
compared	O
with	O
IAN	B-MethodName
model	O
,	O
BAN	O
model	O
can	O
learn	O
more	O
semantic	O
relationship	O
between	O
aspect	O
term	O
and	O
sentence	O
via	O
bidirectional	O
attention	O
mechanism	O
.	O
However	O
,	O
it	O
ignores	O
the	O
position	O
information	O
of	O
aspect	O
term	O
when	O
compared	O
with	O
PBAN	O
model	O
.	O
As	O
we	O
expect	O
,	O
PBAN	O
achieves	O
the	O
best	O
performance	O
among	O
all	O
these	O
models	O
.	O
This	O
is	O
because	O
in	O
addition	O
to	O
fully	O
considering	O
the	O
position	O
information	O
of	O
the	O
aspect	O
term	O
in	O
its	O
corresponding	O
sentence	O
,	O
PBAN	O
also	O
considers	O
the	O
mutual	O
relationship	O
between	O
aspect	O
term	O
and	O
sentence	O
,	O
which	O
is	O
mainly	O
achieved	O
by	O
a	O
bidirectional	O
attention	O
mechanism	O
.	O

In	O
this	O
section	O
,	O
we	O
will	O
briefly	O
review	O
some	O
research	O
on	O
sentiment	B-TaskName
analysis	I-TaskName
in	O
recent	O
years	O
.	O
The	O
previous	O
research	O
can	O
be	O
divided	O
into	O
three	O
directions	O
:	O
traditional	O
machine	O
learning	O
methods	O
,	O
neural	O
network	O
methods	O
and	O
attention	O
network	O
methods	O
.	O

With	O
the	O
successful	O
application	O
of	O
the	O
attention	O
mechanism	O
in	O
machine	B-TaskName
translation	I-TaskName
and	O
reading	B-TaskName
comprehension	I-TaskName
,	O
it	O
is	O
also	O
applied	O
to	O
aspect	O
-	O
level	O
sentiment	B-TaskName
analysis	I-TaskName
in	O
recent	O
years	O
.	O
examined	O
the	O
latent	O
relatedness	O
of	O
the	O
aspect	O
term	O
and	O
sentiment	O
polarity	O
for	O
aspect	O
-	O
level	O
sentiment	B-TaskName
analysis	I-TaskName
.	O
They	O
designed	O
an	O
attention	O
-	O
based	O
LSTM	B-MethodName
to	O
learn	O
aspect	O
term	O
embedding	O
,	O
and	O
let	O
the	O
aspect	O
term	O
embedding	O
participate	O
in	O
calculating	O
the	O
attention	O
weights	O
.	O
Ma	O
et	O
al	O
(	O
2017	O
)	O
proposed	O
a	O
new	O
attention	O
model	O
IAN	B-MethodName
,	O
which	O
considered	O
the	O
separate	O
modeling	O
of	O
aspect	O
terms	O
and	O
could	O
interactively	O
learn	O
attention	O
in	O
the	O
contexts	O
and	O
aspect	O
terms	O
.	O
Despite	O
the	O
effectiveness	O
of	O
these	O
attention	O
mechanisms	O
,	O
they	O
are	O
coarse	O
-	O
grained	O
and	O
it	O
is	O
still	O
challenging	O
to	O
identify	O
different	O
sentiment	O
polarity	O
at	O
a	O
fine	O
-	O
grained	O
aspect	O
level	O
.	O
However	O
,	O
our	O
PBAN	O
model	O
makes	O
full	O
use	O
of	O
the	O
position	O
information	O
of	O
the	O
aspect	O
term	O
,	O
and	O
PBAN	O
uses	O
a	O
fine	O
-	O
grained	O
bidirectional	O
attention	O
mechanism	O
to	O
model	O
the	O
mutual	O
relationship	O
between	O
the	O
sentence	O
and	O
each	O
word	O
in	O
the	O
aspect	O
term	O
,	O
identifying	O
the	O
importance	O
of	O
the	O
word	O
in	O
the	O
aspect	O
term	O
to	O
obtain	O
a	O
more	O
effective	O
sentence	O
representation	O
as	O
described	O
in	O
Section	O
1	O
.	O

In	O
this	O
paper	O
,	O
we	O
have	O
proposed	O
a	O
position	O
-	O
aware	O
bidirectional	O
network	O
(	O
PBAN	O
)	O
based	O
on	O
Bi	O
-	O
GRU	B-MethodName
for	O
aspect	O
-	O
level	O
sentiment	B-TaskName
analysis	I-TaskName
.	O
The	O
main	O
idea	O
of	O
PBAN	O
is	O
to	O
utilize	O
the	O
position	O
embedding	O
of	O
aspect	O
term	O
for	O
calculating	O
the	O
attention	O
weights	O
.	O
Moreover	O
,	O
PBAN	O
adopts	O
a	O
bidirectional	O
attention	O
mechanism	O
,	O
which	O
is	O
not	O
only	O
capable	O
of	O
mutually	O
modeling	O
the	O
relation	O
between	O
sentence	O
and	O
different	O
words	O
in	O
aspect	O
term	O
,	O
but	O
also	O
takes	O
advantage	O
of	O
the	O
position	O
information	O
to	O
better	O
judge	O
the	O
sentiment	O
polarity	O
of	O
aspect	O
term	O
.	O
Experimental	O
results	O
on	O
SemEval	O
2014	O
Datasets	O
demonstrate	O
that	O
our	O
proposed	O
models	O
can	O
learn	O
effective	O
features	O
and	O
obtain	O
superior	O
performance	O
over	O
the	O
baseline	O
models	O
.	O

Adversarial	O
Multi	O
-	O
lingual	O
Neural	O
Relation	B-TaskName
Extraction	I-TaskName

Multi	O
-	O
lingual	O
relation	B-TaskName
extraction	I-TaskName
aims	O
to	O
find	O
unknown	O
relational	O
facts	O
from	O
text	O
in	O
various	O
languages	O
.	O
Existing	O
models	O
can	O
not	O
well	O
capture	O
the	O
consistency	O
and	O
diversity	O
of	O
relation	O
patterns	O
in	O
different	O
languages	O
.	O
To	O
address	O
these	O
issues	O
,	O
we	O
propose	O
an	O
adversarial	O
multi	O
-	O
lingual	O
neural	O
relation	B-TaskName
extraction	I-TaskName
(	O
AMNRE	O
)	O
model	O
,	O
which	O
builds	O
both	O
consistent	O
and	O
individual	O
representations	O
for	O
each	O
sentence	O
to	O
consider	O
the	O
consistency	O
and	O
diversity	O
among	O
languages	O
.	O
Further	O
,	O
we	O
adopt	O
an	O
adversarial	O
training	O
strategy	O
to	O
ensure	O
those	O
consistent	O
sentence	O
representations	O
could	O
effectively	O
extract	O
the	O
language	O
-	O
consistent	O
relation	O
patterns	O
.	O
The	O
experimental	O
results	O
on	O
real	O
-	O
world	O
datasets	O
demonstrate	O
that	O
our	O
AMNRE	O
model	O
significantly	O
outperforms	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
.	O
The	O
source	O
code	O
of	O
this	O
paper	O
can	O
be	O
obtained	O
from	O
https://github.com/thunlp/AMNRE	O
.	O

Relation	B-TaskName
extraction	I-TaskName
(	O
RE	O
)	O
is	O
a	O
crucial	O
task	O
in	O
NLP	O
,	O
which	O
aims	O
to	O
extract	O
semantic	O
relations	O
between	O
entity	O
pairs	O
from	O
the	O
sentences	O
containing	O
them	O
.	O
For	O
example	O
,	O
given	O
an	O
entity	O
pair	O
(	O
Bill	O
Gates	O
,	O
Microsoft	O
)	O
and	O
a	O
sentence	O
"	O
Bill	O
Gates	O
is	O
the	O
co	O
-	O
founder	O
and	O
CEO	O
of	O
Microsoft	O
"	O
,	O
we	O
want	O
to	O
figure	O
out	O
the	O
relation	O
Founder	O
between	O
the	O
two	O
entities	O
.	O
RE	O
can	O
potentially	O
benefit	O
many	O
applications	O
,	O
such	O
as	O
knowledge	O
base	O
construction	O
(	O
Zhong	O
et	O
al	O
,	O
2015	O
;	O
Han	O
et	O
al	O
,	O
2018	O
)	O
and	O
question	B-TaskName
answering	I-TaskName
(	O
Xiang	O
et	O
al	O
,	O
2017	O
)	O
.	O
Recently	O
,	O
neural	O
models	O
have	O
shown	O
their	O
great	O
abilities	O
in	O
RE	O
.	O
Zeng	O
et	O
al	O
(	O
2014	O
)	O
introduce	O
a	O
convolutional	O
neural	O
network	O
(	O
CNN	O
)	O
to	O
extract	O
relational	O
facts	O
with	O
automatically	O
learning	O
features	O
from	O
text	O
.	O
To	O
address	O
the	O
issue	O
of	O
lack	O
of	O
data	O
,	O
Zeng	O
et	O
al	O
(	O
2015	O
)	O
incorporate	O
multi	O
-	O
instance	O
learning	O
with	O
a	O
piece	O
-	O
wise	O
convolutional	O
neural	O
network	O
(	O
PCNN	O
)	O
to	O
extract	O
relations	O
in	O
distantly	O
supervised	O
data	O
.	O
Because	O
distant	O
supervision	O
suffer	O
from	O
wrong	O
labeling	O
problems	O
,	O
Lin	O
et	O
al	O
(	O
2016	O
)	O
further	O
employ	O
a	O
sentence	O
-	O
level	O
selective	O
attention	O
to	O
filter	O
out	O
those	O
noisy	O
sentences	O
in	O
distantly	O
supervised	O
data	O
and	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
.	O
All	O
these	O
neural	O
relation	B-TaskName
extraction	I-TaskName
(	O
NRE	O
)	O
models	O
merely	O
focus	O
on	O
extracting	O
relational	O
facts	O
from	O
mono	O
-	O
lingual	O
data	O
,	O
ignoring	O
the	O
rich	O
information	O
in	O
multi	O
-	O
lingual	O
data	O
.	O
propose	O
a	O
multi	O
-	O
lingual	O
attention	O
-	O
based	O
neural	O
relation	B-TaskName
extraction	I-TaskName
(	O
MNRE	O
)	O
model	O
,	O
which	O
considers	O
the	O
consistency	O
and	O
complementarity	O
in	O
multi	O
-	O
lingual	O
data	O
.	O
MNRE	O
builds	O
a	O
sentence	O
representation	O
for	O
each	O
sentence	O
in	O
various	O
languages	O
and	O
employs	O
a	O
multi	O
-	O
lingual	O
attention	O
to	O
capture	O
the	O
pattern	O
consistency	O
and	O
complementarity	O
among	O
languages	O
.	O
Although	O
MNRE	O
achieves	O
great	O
success	O
in	O
multi	O
-	O
lingual	O
RE	O
,	O
it	O
still	O
has	O
some	O
problems	O
.	O
MNRE	O
learns	O
a	O
single	O
representation	O
for	O
each	O
sentence	O
in	O
various	O
languages	O
,	O
which	O
can	O
not	O
well	O
capture	O
both	O
the	O
consistency	O
and	O
diversity	O
of	O
relation	O
patterns	O
in	O
different	O
languages	O
.	O
Moreover	O
,	O
MNRE	O
simply	O
utilizes	O
a	O
multi	O
-	O
lingual	O
attention	O
mechanism	O
and	O
a	O
global	O
relation	O
predictor	O
to	O
capture	O
the	O
consistent	O
relation	O
patterns	O
among	O
multiple	O
languages	O
.	O
From	O
the	O
experimental	O
data	O
,	O
we	O
find	O
that	O
the	O
sentence	O
representations	O
in	O
different	O
languages	O
are	O
still	O
far	O
from	O
each	O
other	O
and	O
linearly	O
separable	O
.	O
Therefore	O
,	O
it	O
is	O
hard	O
for	O
the	O
multi	O
-	O
To	O
address	O
these	O
issues	O
,	O
we	O
propose	O
an	O
adversarial	O
multi	O
-	O
lingual	O
NRE	O
(	O
AMNRE	O
)	O
model	O
.	O
As	O
shown	O
in	O
Figure	O
1	O
,	O
for	O
an	O
entity	O
pair	O
,	O
we	O
encode	O
its	O
corresponding	O
sentences	O
in	O
various	O
languages	O
through	O
neural	O
sentence	O
encoders	O
.	O
For	O
each	O
sentence	O
,	O
we	O
build	O
an	O
individual	O
representation	O
to	O
grasp	O
its	O
individual	O
language	O
features	O
and	O
a	O
consistent	O
representation	O
to	O
encode	O
its	O
substantially	O
consistent	O
features	O
among	O
languages	O
.	O
Further	O
,	O
we	O
adopt	O
an	O
adversarial	O
training	O
strategy	O
to	O
ensure	O
AMNRE	O
can	O
extract	O
the	O
language	O
-	O
consistent	O
relation	O
patterns	O
from	O
the	O
consistent	O
representations	O
.	O
Orthogonality	O
constraints	O
are	O
also	O
adopted	O
to	O
enhance	O
differences	O
between	O
individual	O
representations	O
and	O
consistent	O
representations	O
for	O
each	O
language	O
.	O
x1	O
x	O
2	O
x1	O
x	O
1	O
1	O
E	O
I	O
1	O
E	O
I	O
1	O
E	O
C	O
1	O
E	O
C	O
x	O
2	O
x2	O
x	O
1	O
x2	O
E	O
I	O
2	O
E	O
C	O
2	O
E	O
C	O
2	O
E	O
I	O
2	O
1	O
2	O
1	O
1	O
1	O
2	O
2	O
2	O
2	O
1	O
s1	O
s2	O
In	O
experiments	O
,	O
we	O
take	O
Chinese	O
and	O
English	O
to	O
show	O
the	O
effectiveness	O
of	O
AMNRE	O
.	O
The	O
experimental	O
results	O
show	O
that	O
AMNRE	O
outperforms	O
all	O
baseline	O
models	O
significantly	O
by	O
explicitly	O
encoding	O
the	O
consistency	O
and	O
diversity	O
among	O
languages	O
.	O
And	O
we	O
further	O
give	O
a	O
case	O
study	O
and	O
an	O
ablation	O
study	O
to	O
demonstrate	O
the	O
adversarial	O
training	O
strategy	O
could	O
help	O
AMNRE	O
to	O
capture	O
language	O
-	O
consistent	O
relation	O
patterns	O
.	O

Traditional	O
supervised	O
RE	O
models	O
(	O
Zelenko	O
et	O
al	O
,	O
2003	O
;	O
Socher	O
et	O
al	O
,	O
2012	O
;	O
Santos	O
et	O
al	O
,	O
2015	O
)	O
heavily	O
rely	O
on	O
abundant	O
amounts	O
of	O
high	O
-	O
quality	O
annotated	O
data	O
.	O
Hence	O
,	O
Mintz	O
et	O
al	O
(	O
2009	O
)	O
propose	O
a	O
distantly	O
supervised	O
model	O
for	O
RE	O
.	O
Distant	O
supervision	O
aligns	O
knowledge	O
bases	O
(	O
KBs	O
)	O
and	O
text	O
to	O
automatically	O
annotate	O
data	O
,	O
and	O
thus	O
distantly	O
supervised	O
models	O
inevitably	O
suffer	O
from	O
wrong	O
labeling	O
problems	O
.	O
To	O
alleviate	O
the	O
noise	O
issue	O
,	O
Riedel	O
et	O
al	O
(	O
2010	O
)	O
and	O
Hoffmann	O
et	O
al	O
(	O
2011	O
)	O
propose	O
multi	O
-	O
instance	O
learning	O
(	O
MIL	O
)	O
mechanisms	O
for	O
single	O
-	O
label	O
and	O
multi	O
-	O
label	O
problems	O
respectively	O
.	O
Then	O
,	O
Zeng	O
et	O
al	O
(	O
2015	O
)	O
attempt	O
to	O
integrate	O
neural	O
models	O
into	O
distant	O
supervision	O
.	O
Lin	O
et	O
al	O
(	O
2016	O
)	O
further	O
propose	O
a	O
sentence	O
-	O
level	O
attention	O
to	O
jointly	O
consider	O
all	O
sentences	O
containing	O
same	O
entity	O
pairs	O
for	O
RE	O
.	O
The	O
attention	O
-	O
based	O
neural	O
relation	B-TaskName
extraction	I-TaskName
(	O
NRE	O
)	O
model	O
has	O
become	O
a	O
foundation	O
for	O
some	O
recent	O
works	O
(	O
Ji	O
et	O
al	O
,	O
2017	O
;	O
Zeng	O
et	O
al	O
,	O
2017	O
;	O
Liu	O
et	O
al	O
,	O
2017b	O
;	O
Wu	O
et	O
al	O
,	O
2017	O
;	O
Feng	O
et	O
al	O
,	O
2018	O
;	O
Zeng	O
et	O
al	O
,	O
2018	O
)	O
.	O
Most	O
existing	O
RE	O
models	O
are	O
devoted	O
to	O
extracting	O
relations	O
from	O
mono	O
-	O
lingual	O
data	O
and	O
ignore	O
information	O
lying	O
in	O
text	O
of	O
multiple	O
languages	O
.	O
Faruqui	O
and	O
Kumar	B-DatasetName
(	O
2015	O
)	O
and	O
Verga	O
et	O
al	O
(	O
2016	O
)	O
first	O
attempt	O
to	O
adopt	O
multi	O
-	O
lingual	O
transfer	B-TaskName
learning	I-TaskName
for	O
RE	O
.	O
However	O
,	O
both	O
of	O
these	O
works	O
learn	O
predictive	O
models	O
on	O
a	O
new	O
language	O
for	O
existing	O
KBs	O
,	O
without	O
fully	O
leveraging	O
semantic	O
information	O
in	O
text	O
.	O
Then	O
,	O
construct	O
a	O
multi	O
-	O
lingual	O
NRE	O
(	O
MNRE	O
)	O
model	O
to	O
jointly	O
represent	O
text	O
of	O
multiple	O
languages	O
to	O
enhance	O
RE	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
multi	O
-	O
lingual	O
NRE	O
framework	O
to	O
explicitly	O
encode	O
language	O
consistency	O
and	O
diversity	O
into	O
different	O
semantic	O
spaces	O
,	O
which	O
can	O
achieve	O
more	O
effective	O
representations	O
for	O
RE	O
.	O
et	O
al	O
(	O
2015	O
)	O
propose	O
adversarial	O
training	O
for	O
image	B-TaskName
classification	I-TaskName
tasks	O
.	O
Afterwards	O
,	O
Goodfellow	O
et	O
al	O
(	O
2014	O
)	O
propose	O
a	O
mature	O
adversarial	O
training	O
framework	O
and	O
use	O
the	O
framework	O
to	O
train	O
generative	O
models	O
.	O
Adversarial	O
networks	O
have	O
recently	O
been	O
used	O
as	O
methods	O
to	O
narrow	O
probability	O
distributions	O
and	O
proven	O
effective	O
in	O
some	O
tasks	O
.	O
In	O
domain	B-TaskName
adaptation	I-TaskName
,	O
Ganin	O
et	O
al	O
(	O
2016	O
)	O
and	O
Bousmalis	O
et	O
al	O
(	O
2016	O
)	O
adopt	O
adverarial	O
training	O
strategies	O
to	O
transfer	O
the	O
features	O
of	O
one	O
source	O
domain	O
to	O
its	O
corresponding	O
target	O
domain	O
.	O

Inspired	B-DatasetName
by	O
Ganin	O
et	O
al	O
(	O
2016	O
)	O
,	O
adversarial	O
training	O
has	O
also	O
been	O
explored	O
in	O
some	O
typical	O
NLP	O
tasks	O
for	O
multi	O
-	O
feature	O
fusion	O
.	O
Park	O
and	O
I	O
m	O
(	O
2016	O
)	O
propose	O
a	O
multi	O
-	O
modal	O
representation	B-TaskName
learning	I-TaskName
model	O
based	O
on	O
adversarial	O
training	O
.	O
Then	O
,	O
Liu	O
et	O
al	O
(	O
2017a	O
)	O
employ	O
adversarial	O
training	O
to	O
construct	O
a	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
model	O
for	O
text	B-TaskName
classification	I-TaskName
by	O
extending	O
the	O
original	O
binary	O
adversarial	O
training	O
to	O
the	O
multiclass	O
version	O
.	O
And	O
a	O
similar	O
adversarial	O
framework	O
is	O
also	O
adapted	O
by	O
to	O
learn	O
features	O
from	O
different	O
datasets	O
for	O
chinese	B-TaskName
word	I-TaskName
segmentation	I-TaskName
.	O
In	O
this	O
paper	O
,	O
we	O
adopt	O
adversarial	O
training	O
to	O
boost	O
feature	O
fusion	O
to	O
grasp	O
the	O
consistency	O
among	O
different	O
languages	O
.	O

The	O
input	O
layer	O
transforms	O
all	O
input	O
words	O
in	O
the	O
sentence	O
into	O
corresponding	O
input	O
embeddings	O
by	O
concatenating	O
their	O
word	B-TaskName
embeddings	I-TaskName
and	O
position	O
embeddings	O
.	O
The	O
word	B-TaskName
embeddings	I-TaskName
are	O
pre	O
-	O
trained	O
by	O
Skip	O
-	O
Gram	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
.	O
The	O
position	O
embeddings	O
are	O
a	O
widely	O
-	O
used	O
technique	O
in	O
RE	O
proposed	O
by	O
Zeng	O
et	O
al	O
(	O
2014	O
)	O
,	O
representing	O
each	O
word	O
's	O
relative	O
distances	O
to	O
the	O
two	O
entities	O
into	O
two	O
k	O
p	O
-	O
dimensional	O
vectors	O
.	O
The	O
input	O
layer	O
represents	O
the	O
input	O
sentence	O
as	O
a	O
k	O
i	O
-	O
dimensional	O
embedding	O
sequence	O
x	O
=	O
{	O
w	O
1	O
,	O
w	O
2	O
,	O
.	O
.	O
.	O
}	O
,	O
where	O
k	O
i	O
=	O
k	O
w	O
+	O
k	O
p	O
×2	O
,	O
k	O
w	O
and	O
k	O
p	O
are	O
the	O
dimensions	O
of	O
word	B-TaskName
embeddings	I-TaskName
and	O
position	O
embeddings	O
respectively	O
.	O

After	O
representing	O
the	O
input	O
sentence	O
as	O
a	O
k	O
i	O
-	O
dimensional	O
embedding	O
sequence	O
,	O
we	O
select	O
both	O
CNN	O
(	O
Zeng	O
et	O
al	O
,	O
2014	O
)	O
and	O
RNN	O
to	O
encode	O
the	O
input	O
embedding	O
sequence	O
x	O
=	O
{	O
w	O
1	O
,	O
w	O
2	O
,	O
.	O
.	O
.	O
}	O
to	O
its	O
sentence	B-TaskName
embedding	I-TaskName
.	O
CNN	O
slides	O
a	O
convolution	B-MethodName
kernel	O
with	O
the	O
window	O
size	O
m	O
to	O
extract	O
the	O
k	O
h	O
-	O
dimensional	O
local	O
features	O
,	O
hi	O
=	O
CNN	O
w	O
i−	O
m−1	O
2	O
,	O
.	O
.	O
.	O
,	O
w	O
i+	O
m−1	O
2	O
.	O
(	O
1	O
)	O
A	O
max	O
-	O
pooling	O
is	O
then	O
adopted	O
to	O
obtain	O
the	O
final	O
sentence	B-TaskName
embedding	I-TaskName
y	O
as	O
follows	O
,	O
[	O
y	O
]	O
j	O
=	O
max	O
{	O
[	O
h1	O
]	O
j	O
,	O
.	O
.	O
.	O
,	O
[	O
hn	O
]	O
j	O
}	O
.	O
(	O
2	O
)	O
RNN	O
is	O
mainly	O
designed	O
for	O
modeling	O
sequential	O
data	O
.	O
In	O
this	O
paper	O
,	O
we	O
adopt	O
bidirectional	O
RNN	O
(	O
Bi	O
-	O
RNN	O
)	O
to	O
incorporate	O
information	O
from	O
both	O
sides	O
of	O
the	O
sentence	O
sequence	O
as	O
follows	O
,	O
−	O
h	O
i	O
=	O
RNN	O
f	O
(	O
xi	O
,	O
−	O
h	O
i−1	O
)	O
,	O
−	O
h	O
i	O
=	O
RNN	O
b	O
(	O
xi	O
,	O
−	O
h	O
i+1	O
)	O
,	O
(	O
3	O
)	O
where	O
−	O
h	O
i	O
and	O
−	O
h	O
i	O
are	O
the	O
k	O
h	O
-	O
dimensional	O
hidden	O
states	O
at	O
the	O
position	O
i	O
of	O
the	O
forward	O
and	O
backward	O
RNN	O
respectively	O
.	O
RNN	O
(	O
)	O
is	O
the	O
recurrent	O
unit	O
and	O
we	O
select	O
gated	B-MethodName
recurrent	I-MethodName
unit	I-MethodName
(	O
GRU	B-MethodName
)	O
(	O
Cho	O
et	O
al	O
,	O
2014	O
)	O
as	O
the	O
recurrent	O
unit	O
in	O
this	O
paper	O
.	O
We	O
concatenate	O
both	O
the	O
forward	O
and	O
backward	O
hidden	O
states	O
as	O
the	O
sentence	B-TaskName
embedding	I-TaskName
y	O
,	O
y	O
=	O
[	O
−	O
h	O
n	O
;	O
−	O
h	O
1	O
]	O
.	O
(	O
4	O
)	O
For	O
simplicity	O
,	O
we	O
denote	O
such	O
a	O
sentence	O
encoding	O
operation	O
as	O
the	O
following	O
equation	O
,	O
y	O
=	O
E	O
(	O
x	O
)	O
.	O
(	O
5	O
)	O
For	O
each	O
sentence	O
x	O
i	O
j	O
S	O
j	O
,	O
we	O
adopt	O
the	O
individual	O
sentence	O
encoder	O
E	O
I	O
j	O
and	O
the	O
consistent	O
sentence	O
encoder	O
E	O
C	O
j	O
to	O
embed	O
the	O
sentence	O
into	O
its	O
individual	O
and	O
consistent	O
representations	O
respectively	O
,	O
{	O
y	O
1	O
j	O
,	O
y	O
2	O
j	O
,	O
.	O
.	O
.	O
}	O
=	O
{	O
E	O
I	O
j	O
(	O
x	O
1	O
j	O
)	O
,	O
E	O
I	O
j	O
(	O
x	O
2	O
j	O
)	O
,	O
.	O
.	O
.	O
}	O
,	O
{	O
ȳ	O
1	O
j	O
,	O
ȳ	O
2	O
j	O
,	O
.	O
.	O
.	O
}	O
=	O
{	O
E	O
C	O
j	O
(	O
x	O
1	O
j	O
)	O
,	O
E	O
C	O
j	O
(	O
x	O
2	O
j	O
)	O
,	O
.	O
.	O
.	O
}	O
.	O
(	O
6	O
)	O

Following	O
the	O
settings	O
of	O
previous	O
works	O
,	O
we	O
use	O
the	O
pre	O
-	O
trained	O
word	B-TaskName
embeddings	I-TaskName
learned	O
by	O
Skip	O
-	O
Gram	O
as	O
the	O
initial	O
word	B-TaskName
embeddings	I-TaskName
.	O
We	O
implement	O
the	O
MNRE	O
framework	O
proposed	O
by	O
by	O
ourselves	O
.	O
For	O
fair	O
comparision	O
,	O
we	O
set	O
most	O
of	O
the	O
hyperparameters	O
following	O
.	O
We	O
list	O
the	O
best	O
setting	O
of	O
hyperparameters	O
in	O
Table	O
2	O
.	O

To	O
further	O
show	O
the	O
effectiveness	O
of	O
our	O
proposed	O
model	O
to	O
extract	O
the	O
language	O
-	O
consistent	O
semantic	O
information	O
,	O
we	O
give	O
an	O
example	O
in	O
Table	O
6	O
.	O
We	O
adopt	O
the	O
cosine	O
similarity	O
to	O
measure	O
the	O
similarity	O
between	O
sentence	B-TaskName
embeddings	I-TaskName
encoded	O
by	O
consistent	O
encoders	O
.	O
The	O
first	O
sentence	O
in	O
the	O
middle	O
column	O
is	O
the	O
standard	O
Chinese	O
translation	O
of	O
the	O
left	O
sentence	O
,	O
thus	O
they	O
share	O
the	O
same	O
semantic	O
information	O
.	O
We	O
observe	O
that	O
in	O
our	O
proposed	O
model	O
,	O
the	O
feature	O
embedding	O
similarity	O
between	O
these	O
two	O
sentences	O
are	O
significantly	O
higher	O
than	O
the	O
other	O
English	O
sentences	O
sharing	O
entity	O
pair	O
and	O
relational	O
fact	O
but	O
differing	O
in	O
semantics	O
.	O
It	O
indicates	O
that	O
sentences	O
in	O
different	O
languages	O
containing	O
similar	O
semantics	O
can	O
be	O
indeed	O
encoded	O
into	O
adjacent	O
places	O
of	O
the	O
consistent	O
space	O
in	O
our	O
framework	O
.	O

In	O
this	O
paper	O
,	O
we	O
introduce	O
a	O
novel	O
adversarial	O
multi	O
-	O
lingual	O
neural	O
relation	B-TaskName
extraction	I-TaskName
model	O
(	O
AMNRE	O
)	O
.	O
AMNRE	O
builds	O
both	O
individual	O
and	O
consistent	O
representations	O
for	O
each	O
sentence	O
to	O
consider	O
the	O
consistency	O
and	O
diversity	O
of	O
relation	O
patterns	O
among	O
languages	O
.	O
It	O
also	O
employs	O
an	O
adversarial	O
training	O
strategy	O
and	O
orthogonality	O
constraints	O
to	O
ensure	O
the	O
consistent	O
representations	O
could	O
extract	O
the	O
languageconsistent	O
features	O
to	O
extract	O
relations	O
.	O
The	O
experimental	O
results	O
on	O
real	O
-	O
world	O
datasets	O
demonstrate	O
that	O

our	O
AMNRE	O
could	O
effectively	O
encode	O
the	O
consistency	O
and	O
diversity	O
among	O
languages	O
,	O
and	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
relation	B-TaskName
extraction	I-TaskName
.	O
We	O
will	O
explore	O
the	O
following	O
directions	O
as	O
our	O
future	O
work	O
:	O
(	O
1	O
)	O
AMNRE	O
can	O
be	O
also	O
implemented	O
in	O
the	O
scenario	O
of	O
multiple	O
languages	O
,	O
and	O
this	O
paper	O
shows	O
the	O
effectiveness	O
of	O
AMNRE	O
on	O
the	O
dataset	O
with	O
two	O
languages	O
(	O
English	O
and	O
Chinese	O
)	O
.	O
In	O
the	O
future	O
,	O
we	O
will	O
explore	O
AMNRE	O
in	O
much	O
more	O
other	O
languages	O
such	O
as	O
French	O
,	O
Spanish	O
,	O
and	O
so	O
on	O
.	O
(	O
2	O
)	O
AMNRE	O
simply	O
aligns	O
the	O
sentences	O
with	O
similar	O
semantics	O
in	O
different	O
languages	O
with	O
an	O
adversarial	O
training	O
strategy	O
.	O
In	O
fact	O
,	O
machine	B-TaskName
translation	I-TaskName
is	O
a	O
typical	O
approach	O
to	O
align	O
sentences	O
in	O
various	O
languages	O
.	O
In	O
the	O
future	O
,	O
we	O
will	O
combine	O
machine	B-TaskName
translation	I-TaskName
with	O
our	O
model	O
to	O
further	O
improve	O
the	O
extraction	O
performance	O
.	O

It	O
is	O
well	O
understood	O
that	O
recognizing	O
whether	O
a	O
speaker	O
is	O
ironic	O
or	O
sarcastic	O
is	O
essential	O
to	O
understanding	O
their	O
actual	O
sentiments	O
and	O
beliefs	O
.	O
For	O
instance	O
,	O
the	O
utterance	O
"	O
pictures	O
of	O
holding	O
animal	O
carcasses	O
are	O
so	O
flattering	O
"	O
is	O
an	O
expression	O
of	O
verbal	O
irony	O
,	O
where	O
the	O
speaker	O
has	O
a	O
negative	O
sentiment	O
towards	O
"	O
pictures	O
of	O
holding	O
animal	O
carcasses	O
"	O
,	O
but	O
uses	O
the	O
positive	O
sentiment	O
word	O
"	O
flattering	O
"	O
.	O
This	O
inherent	O
characteristic	O
of	O
verbal	O
irony	O
is	O
called	O
semantic	O
incongruity	O
-	O
incongruity	O
between	O
the	O
literal	O
evaluation	O
and	O
the	O
context	O
(	O
e.g.	O
,	O
between	O
the	O
positive	O
sentiment	O
words	O
and	O
the	O
negative	O
situation	O
in	O
this	O
example	O
)	O
.	O
Most	O
NLP	O
research	O
on	O
verbal	O
irony	O
or	O
sarcasm	O
has	O
focused	O
on	O
the	O
task	O
of	O
sarcasm	B-TaskName
detection	I-TaskName
treating	O
it	O
as	O
a	O
binary	O
classification	O
task	O
using	O
either	O
the	O
utterance	O
in	O
isolation	O
or	O
adding	O
contextual	O
information	O
such	O
as	O
conversation	O
context	O
,	O
author	O
context	O
,	O
visual	O
context	O
,	O
or	O
cognitive	O
features	O
(	O
Davidov	O
et	O
al	O
,	O
2010	O
;	O
Maynard	O
and	O
Greenwood	O
,	O
2014	O
;	O
Wallace	O
et	O
al	O
,	O
2014	O
;	O
Joshi	O
et	O
al	O
,	O
2015	O
;	O
Bamman	O
and	O
Smith	O
,	O
2015	O
;	O
Muresan	O
et	O
al	O
,	O
2016	O
;	O
Amir	O
et	O
al	O
,	O
2016	O
;	O
Mishra	O
et	O
al	O
,	O
2016	O
;	O
Ghosh	O
and	O
Veale	O
,	O
2017	O
;	O
Felbo	O
et	O
al	O
,	O
2017	O
;	O
Hazarika	O
et	O
al	O
,	O
2018	O
;	O
Tay	O
et	O
al	O
,	O
2018	O
;	O
Oprea	O
and	O
Magdy	O
,	O
2019	O
)	O
.	O
Such	O
approaches	O
have	O
focused	O
their	O
analysis	O
on	O
the	O
speakers	O
'	O
beliefs	O
and	O
intentions	O
for	O
using	O
irony	O
(	O
Attardo	O
,	O
2000	O
)	O
.	O
However	O
,	O
sarcasm	O
and	O
verbal	O
irony	O
are	O
types	O
of	O
interactional	O
phenomena	O
with	O
specific	O
perlocutionary	O
effects	O
on	O
the	O
hearer	O
(	O
Haverkate	O
,	O
1990	O
)	O
.	O
Thus	O
,	O
we	O
argue	O
that	O
,	O
besides	O
recognizing	O
the	O
speaker	O
's	O
sarcastic	O
/	O
ironic	O
intent	O
,	O
it	O
is	O
equally	O
important	O
to	O
understand	O
how	O
the	O
hearer	O
interprets	O
the	O
speaker	O
's	O
sarcastic	O
/	O
ironic	O
message	O
.	O
For	O
the	O
above	O
utterance	O
,	O
the	O
strength	O
of	O
negative	O
sentiment	O
perceived	O
by	O
the	O
hearer	O
depends	O
on	O
whether	O
they	O
interpret	O
the	O
speaker	O
's	O
actual	O
meaning	O
as	O
"	O
picture	O
.	O
.	O
.	O
are	O
not	O
flattering	O
"	O
vs.	O
"	O
pictures	O
.	O
.	O
.	O
are	O
so	O
gross	O
"	O
(	O
Table	O
1	O
)	O
.	O
The	O
intensity	O
of	O
negative	O
sentiment	O
is	O
higher	O
in	O
the	O
latter	O
interpretation	O
than	O
in	O
the	O
former	O
.	O
Kreuz	O
(	O
2000	O
)	O
noted	O
that	O
most	O
studies	O
in	O
linguistics	O
and	O
psychology	O
have	O
conducted	O
experiments	O
analyzing	O
reaction	O
times	O
(	O
Gibbs	O
,	O
1986	O
;	O
Katz	O
et	O
al	O
,	O
2004	O
)	O
or	O
situational	O
context	O
(	O
Ivanko	O
and	O
Pexman	O
,	O
2003	O
)	O
,	O
featuring	O
a	O
setup	O
with	O
in	O
vitro	O
data	O
aimed	O
at	O
testing	O
the	O
validity	O
of	O
specific	O
theories	O
of	O
irony	O
.	O
Instead	O
,	O
our	O
study	O
adopts	O
a	O
naturalistic	O
approach	O
to	O
understand	O
hearers	O
'	O
reception	O
of	O
irony	O
looking	O
at	O
what	O
linguistic	O
strategies	O
are	O
recurrently	O
used	O
by	O
hearers	O
to	O
interpret	O
the	O
non	O
-	O
literal	O
meaning	O
underlying	O
ironic	O
utterances	O
.	O
We	O
leverage	O
the	O
crowdsourcing	O
task	O
introduced	O
by	O
Ghosh	O
et	O
al	O
(	O
2015	O
)	O
for	O
their	O
work	O
on	O
detecting	O
whether	O
a	O
word	O
has	O
a	O
literal	O
or	O
sarcastic	O
in	O
-	O
terpretation	O
,	O
later	O
adopted	O
by	O
Peled	O
and	O
Reichart	O
(	O
2017	O
)	O
.	O
The	O
task	O
is	O
framed	O
as	O
follows	O
:	O
given	O
a	O
speaker	O
's	O
ironic	O
message	O
,	O
five	O
annotators	O
(	O
e.g.	O
,	O
Turkers	O
on	O
Amazon	O
Mechanical	O
Turk	O
(	O
MTurk	O
)	O
)	O
are	O
asked	O
to	O
verbalize	O
their	O
interpretation	O
of	O
the	O
speaker	O
's	O
ironic	O
message	O
(	O
i.e.	O
,	O
their	O
understanding	O
of	O
the	O
speaker	O
's	O
intended	O
meaning	O
)	O
(	O
see	O
Table	O
1	O
;	O
S	O
i	O
m	O
denotes	O
the	O
speaker	O
's	O
ironic	O
message	O
,	O
while	O
H	O
int	O
denotes	O
the	O
hearer	O
's	O
interpretation	O
of	O
that	O
ironic	O
message	O
)	O
.	O
The	O
crowdsourcing	O
experiments	O
are	O
reported	O
in	O
Section	O
2	O
.	O
The	O
paper	O
makes	O
three	O
contributions	O
.	O
First	O
,	O
we	O
propose	O
a	O
data	O
-	O
driven	O
typology	O
of	O
linguistic	O
strategies	O
that	O
hearers	O
use	O
to	O
interpret	O
ironic	O
messages	O
and	O
discuss	O
its	O
relevance	O
in	O
verifying	O
theoretical	O
frameworks	O
of	O
irony	O
(	O
Section	O
4	O
)	O
.	O
Second	O
,	O
we	O
propose	O
computational	O
models	O
to	O
capture	O
these	O
strategies	O
(	O
Section	O
5	O
)	O
.	O
Third	O
,	O
we	O
present	O
two	O
studies	O
that	O
aim	O
to	O
answer	O
two	O
questions	O
:	O
(	O
1	O
)	O
does	O
the	O
type	O
of	O
semantic	O
incongruity	O
in	O
the	O
ironic	O
message	O
(	O
explicit	O
vs.	O
implicit	O
;	O
see	O
Section	O
3	O
)	O
influence	O
the	O
choice	O
of	O
interpretation	O
strategies	O
by	O
the	O
hearers	O
?	O
(	O
Section	O
6.2	O
)	O
;	O
(	O
2	O
)	O
do	O
interpretation	O
strategies	O
of	O
verbal	O
irony	O
vary	O
by	O
hearers	O
?	O
We	O
make	O
all	O
datasets	O
and	O
code	O
available	O
.	O
1	O

To	O
generate	O
a	O
parallel	O
dataset	O
of	O
speakers	O
'	O
ironic	O
messages	O
and	O
hearers	O
'	O
interpretations	O
we	O
conduct	O
a	O
crowdsourcing	O
experiment	O
.	O
Given	O
a	O
speaker	O
's	O
ironic	O
message	O
(	O
S	O
i	O
m	O
)	O
,	O
five	O
Turkers	O
(	O
hearers	O
)	O
on	O
MTurk	O
are	O
asked	O
to	O
verbalize	O
their	O
interpretation	O
of	O
the	O
speaker	O
's	O
ironic	O
message	O
(	O
i.e.	O
,	O
their	O
understanding	O
of	O
the	O
speaker	O
's	O
intended	O
meaning	O
)	O
(	O
H	O
int	O
)	O
.	O
The	O
design	O
of	O
the	O
MTurk	O
task	O
was	O
first	O
introduced	O
by	O
Ghosh	O
et	O
al	O
(	O
2015	O
)	O
,	O
who	O
use	O
the	O
resulting	O
dataset	O
to	O
identify	O
words	O
that	O
can	O
have	O
both	O
a	O
literal	O
and	O
a	O
sarcastic	O
sense	O
.	O
Peled	O
and	O
Reichart	O
(	O
2017	O
)	O
employed	O
similar	O
design	O
to	O
generate	O
a	O
parallel	O
dataset	O
to	O
use	O
for	O
generating	O
interpretations	O
of	O
sarcastic	O
messages	O
using	O
machine	B-TaskName
translation	I-TaskName
approaches	O
.	O
They	O
use	O
skilled	O
annotators	O
in	O
comedy	O
writing	O
and	O
literature	O
paraphrasing	O
and	O
give	O
them	O
the	O
option	O
not	O
to	O
rephrase	O
(	O
we	O
refer	O
to	O
Peled	O
and	O
Reichart	O
(	O
2017	O
)	O
's	O
dataset	O
as	O
SIGN	O
)	O
.	O
We	O
perform	O
this	O
new	O
crowdsourcing	O
task	O
and	O
do	O
not	O
rely	O
entirely	O
on	O
the	O
above	O
two	O
datasets	O
for	O
two	O
reasons	O
:	O
(	O
1	O
)	O
we	O
focus	O
on	O
verbal	O
irony	O
,	O
and	O
(	O
2	O
)	O
we	O
always	O
require	O
an	O
interpretation	O
from	O
the	O
Turkers	O
.	O
Un	O
-	O
like	O
the	O
above	O
two	O
studies	O
,	O
the	O
main	O
goal	O
of	O
our	O
research	O
is	O
to	O
analyze	O
the	O
linguistics	O
strategies	O
employed	O
by	O
hearers	O
in	O
interpreting	O
verbal	O
irony	O
.	O
We	O
collected	O
messages	O
that	O
express	O
verbal	O
irony	O
from	O
Twitter	O
using	O
the	O
hashtags	O
#	O
irony	O
,	O
#	O
sarcastic	O
,	O
and	O
#	O
sarcasm	O
.	O
We	O
chose	O
Twitter	O
as	O
a	O
source	O
since	O
the	O
presence	O
of	O
the	O
hashtags	O
allows	O
us	O
to	O
select	O
sentences	O
where	O
the	O
speaker	O
's	O
intention	O
was	O
to	O
be	O
ironic	O
.	O
Furthermore	O
,	O
even	O
though	O
Twitter	O
users	O
can	O
not	O
be	O
considered	O
representative	O
of	O
the	O
entire	O
population	O
,	O
they	O
are	O
unlikely	O
to	O
be	O
skewed	O
with	O
respect	O
to	O
topics	O
or	O
gender	O
.	O
We	O
manually	O
checked	O
and	O
kept	O
1	O
,	O
000	O
tweets	O
that	O
express	O
verbal	O
irony	O
.	O
We	O
do	O
not	O
draw	O
any	O
theoretical	O
distinction	O
between	O
sarcasm	O
and	O
irony	O
since	O
we	O
can	O
not	O
assume	O
that	O
Twitter	O
users	O
also	O
differentiate	O
between	O
#	O
irony	O
and	O
#	O
sarcasm	O
,	O
blurred	O
even	O
in	O
scholarly	O
literature	O
.	O
The	O
Turkers	O
were	O
provided	O
with	O
detailed	O
instructions	O
and	O
examples	O
of	O
the	O
task	O
including	O
the	O
standard	O
definition	O
of	O
verbal	O
irony	O
taken	O
from	O
the	O
Merriam	O
-	O
Webster	O
dictionary	O
(	O
"	O
use	O
of	O
words	O
to	O
express	O
something	O
other	O
than	O
and	O
especially	O
the	O
opposite	O
of	O
the	O
literal	O
meaning	O
"	O
)	O
.	O
We	O
decided	O
to	O
suggest	O
them	O
a	O
guiding	O
definition	O
for	O
two	O
reasons	O
.	O
First	O
,	O
hearers	O
do	O
not	O
usually	O
focus	O
on	O
literal	O
vs.	O
non	O
literal	O
meaning	O
,	O
as	O
shown	O
by	O
studies	O
measuring	O
processing	O
times	O
for	O
both	O
types	O
of	O
statements	O
(	O
Inhoff	O
et	O
al	O
,	O
1984	O
)	O
.	O
Therefore	O
,	O
when	O
asked	O
to	O
rephrase	O
the	O
speakers	O
'	O
intended	O
meaning	O
,	O
hearers	O
would	O
have	O
probably	O
come	O
up	O
with	O
sentences	O
expressing	O
the	O
speaker	O
's	O
imagined	O
discursive	O
goals	O
,	O
rather	O
than	O
disclosing	O
their	O
perceived	O
literal	O
meaning	O
.	O
Second	O
,	O
it	O
is	O
reasonable	O
to	O
assume	O
that	O
Turkers	O
would	O
have	O
looked	O
up	O
the	O
standard	O
meaning	O
of	O
ironic	O
utterance	O
given	O
by	O
an	O
online	O
dictionary	O
to	O
ease	O
up	O
their	O
task	O
,	O
possibly	O
coming	O
up	O
with	O
biased	O
definitions	O
.	O
The	O
Turkers	O
were	O
instructed	O
to	O
consider	O
the	O
entire	O
message	O
in	O
their	O
verbalization	O
to	O
avoid	O
asymmetry	O
in	O
length	O
between	O
the	O
S	O
i	O
m	O
and	O
H	O
int	O
.	O
We	O
obtained	O
a	O
dataset	O
of	O
5	O
,	O
000	O
S	O
i	O
m	O
-	O
H	O
int	O
pairs	O
where	O
five	O
Turkers	O
rephrase	O
each	O
S	O
i	O
m	O
.	O
A	O
total	O
of	O
184	O
Turkers	O
participated	O
in	O
the	O
rephrasing	O
task	O
.	O
Table	O
1	O
shows	O
examples	O
of	O
speaker	O
's	O
ironic	O
messages	O
(	O
S	O
i	O
m	O
)	O
and	O
their	O
corresponding	O
hearers	O
'	O
interpretations	O
(	O
H	O
i	O
int	O
)	O
.	O
Next	O
,	O
we	O
ran	O
a	O
second	O
MTurk	O
task	O
to	O
verify	O
whether	O
the	O
generated	O
H	O
int	O
messages	O
are	O
plausible	O
interpretations	O
of	O
the	O
ironic	O
messages	O
.	O
This	O
time	O
we	O
employ	O
three	O
Turkers	O
per	O
task	O
and	O
only	O
Turkers	O
who	O
were	O
not	O
involved	O
in	O
the	O
content	O
generation	O
task	O
were	O
allowed	O
to	O
perform	O
this	O
task	O
.	O
We	O
observe	O
that	O
Turkers	O
labeled	O
5	O
%	O
(	O
i.e.	O
,	O
238	O
verbalizations	O
)	O
of	O
H	O
int	O
s	O
as	O
invalid	O
and	O
low	O
quality	O
(	O
e.g.	O
,	O
wrong	O
interpretation	O
)	O
.	O
For	O
both	O
tasks	O
,	O
we	O
allowed	O
only	O
qualified	O
Turkers	O
(	O
i.e.	O
,	O
at	O
least	O
95	O
%	O
approval	O
rate	O
and	O
5	O
,	O
000	O
approved	O
HITs	O
)	O
,	O
paid	O
7	O
cents	O
/	O
task	O
and	O
gave	O
sixty	O
minutes	O
to	O
complete	O
each	O
task	O
.	O
The	O
final	O
dataset	O
contains	O
4	O
,	O
762	O
pairs	O
S	O
i	O
m	O
-	O
H	O
int	O
.	O
Sim	O
H	O
1	O
int	O
H	O
2	O
int	O
H	O
3	O
int	O
1	O
.	O

Most	O
NLP	O
research	O
on	O
verbal	O
irony	O
or	O
sarcasm	O
has	O
focused	O
on	O
the	O
task	O
of	O
sarcasm	B-TaskName
detection	I-TaskName
treating	O
it	O
as	O
a	O
binary	O
classification	O
task	O
using	O
either	O
the	O
utterance	O
in	O
isolation	O
or	O
adding	O
contextual	O
information	O
such	O
as	O
conversation	O
context	O
,	O
author	O
context	O
,	O
visual	O
context	O
,	O
or	O
cognitive	O
features	O
(	O
González	O
-	O
Ibáñez	O
et	O
al	O
,	O
2011	O
;	O
Liebrecht	O
et	O
al	O
,	O
2013	O
;	O
Wallace	O
et	O
al	O
,	O
2014	O
;	O
Zhang	O
et	O
al	O
,	O
2016	O
;	O
Ghosh	O
and	O
Veale	O
,	O
2016	O
;	O
Schifanella	O
et	O
al	O
,	O
2016	O
;	O
Xiong	O
et	O
al	O
,	O
2019	O
;	O
Castro	O
et	O
al	O
,	O
2019	O
)	O
.	O
Unlike	O
this	O
line	O
of	O
work	O
,	O
our	O
research	O
focuses	O
on	O
how	O
the	O
hearer	O
interprets	O
an	O
ironic	O
message	O
.	O
The	O
findings	O
from	O
our	O
study	O
could	O
have	O
multiple	O
impacts	O
on	O
the	O
sarcasm	B-TaskName
detection	I-TaskName
task	O
.	O
First	O
,	O
interpretation	O
strategies	O
open	O
up	O
a	O
scope	O
of	O
"	O
graded	O
interpretation	O
"	O
of	O
irony	O
instead	O
of	O
only	O
a	O
binary	O
decision	O
(	O
i.e.	O
,	O
predicting	O
the	O
strength	O
of	O
irony	O
)	O
.	O
Second	O
,	O
nature	O
of	O
semantic	O
incongruence	O
and	O
stereotype	O
irony	O
situations	O
can	O
be	O
useful	O
features	O
in	O
irony	O
detection	O
.	O
Recently	O
,	O
Peled	O
and	O
Reichart	O
(	O
2017	O
)	O
proposed	O
a	O
computational	O
model	O
based	O
on	O
SMT	O
to	O
generate	O
interpretations	O
of	O
sarcastic	O
messages	O
.	O
We	O
aim	O
to	O
deepen	O
our	O
understanding	O
of	O
such	O
interpretations	O
by	O
introducing	O
a	O
typology	O
of	O
linguistic	O
strategies	O
.	O
We	O
study	O
the	O
distribution	O
of	O
these	O
strategies	O
via	O
both	O
hearer	O
-	O
dependent	O
and	O
messagedependent	O
interpretations	O
.	O
Psycholinguistics	O
studies	O
that	O
have	O
dealt	O
with	O
the	O
hearers	O
'	O
perception	O
,	O
have	O
mainly	O
focused	O
on	O
how	O
ironic	O
messages	O
are	O
processed	O
:	O
through	O
the	O
analysis	O
of	O
reaction	O
times	O
(	O
Gibbs	O
,	O
1986	O
;	O
Katz	O
et	O
al	O
,	O
2004	O
)	O
,	O
the	O
role	O
of	O
situational	O
context	O
(	O
Ivanko	O
and	O
Pexman	O
,	O
2003	O
)	O
and	O
in	O
tackling	O
speaker	O
-	O
hearer	O
social	O
relations	O
by	O
annotating	O
ironic	O
texts	O
from	O
different	O
genres	O
(	O
Burgers	O
,	O
2010	O
)	O
.	O
However	O
,	O
no	O
attention	O
has	O
been	O
paid	O
to	O
correlations	O
between	O
how	O
ironic	O
message	O
is	O
expressed	O
and	O
how	O
it	O
is	O
interpreted	O
by	O
the	O
hearer	O
,	O
including	O
what	O
linguistic	O
strategies	O
the	O
hearers	O
employ	O
.	O

We	O
thank	O
Rituparna	O
Mukherjee	O
,	O
Daniel	O
Chaparro	O
,	O
Pedro	O
Pérez	O
Sánchez	O
,	O
and	O
Renato	O
Augusto	O
Vieira	O
Nishimori	O
who	O
helped	O
us	O
in	O
annotating	O
as	O
well	O
as	O
in	O
running	O
experiments	O
.	O
This	O
paper	O
partially	O
based	O
on	O
the	O
work	O
supported	O
by	O
the	O
DARPA	B-DatasetName
-	O
DEFT	O
program	O
.	O
The	O
views	O
expressed	O
are	O
those	O
of	O
the	O
authors	O
and	O
do	O
not	O
reflect	O
the	O
official	O
policy	O
or	O
position	O
of	O
the	O
Department	O
of	O
Defense	O
or	O
the	O
U.S.	O
Government	O
.	O

The	O
current	O
automated	O
event	O
understanding	O
task	O
has	O
been	O
overly	O
simplified	O
to	O
be	O
local	O
and	O
sequential	O
.	O
Real	O
world	O
events	O
,	O
such	O
as	O
disease	O
outbreaks	O
and	O
terrorist	O
attacks	O
,	O
have	O
multiple	O
actors	O
,	O
complex	O
timelines	O
,	O
intertwined	O
relations	O
and	O
multiple	O
possible	O
outcomes	O
.	O
Understanding	O
such	O
events	O
requires	O
knowledge	O
in	O
the	O
form	O
of	O
a	O
library	O
of	O
event	O
schemas	O
,	O
capturing	O
the	O
progress	O
of	O
time	O
,	O
and	O
performing	O
global	O
inference	O
for	O
event	O
prediction	O
.	O
For	O
example	O
,	O
regarding	O
the	O
2019	O
protest	O
in	O
Hong	O
Kong	O
International	O
Airport	B-DatasetName
,	O
a	O
typical	O
question	O
from	O
analysts	O
would	O
be	O
"	O
How	O
long	O
will	O
the	O
flights	O
being	O
canceled	O
?	O
"	O
This	O
requires	O
an	O
event	O
understanding	O
system	O
to	O
match	O
events	O
to	O
schema	O
representations	O
and	O
reason	O
about	O
what	O
might	O
happen	O
next	O
.	O
The	O
airport	O
protest	O
schema	O
would	O
be	O
triggered	O
by	O
"	O
protest	O
"	O
and	O
"	O
flight	O
cancellation	O
"	O
,	O
and	O
evidence	O
of	O
protesters	O
(	O
e.g.	O
,	O
the	O
number	O
of	O
protesters	O
,	O
the	O
instruments	O
being	O
used	O
,	O
etc	O
)	O
will	O
suggest	O
a	O
CEO	O
resignation	O
event	O
,	O
or	O
a	O
flight	O
rescheduling	O
event	O
,	O
or	O
continuous	O
flight	O
cancellation	O
events	O
with	O
respective	O
probabilities	O
.	O
Comprehending	O
such	O
a	O
news	O
story	O
requires	O
following	O
a	O
timeline	O
,	O
identifying	O
key	O
events	O
and	O
tracking	O
characters	O
.	O
We	O
refer	O
to	O
such	O
a	O
"	O
story	O
"	O
as	O
a	O
complex	O
event	O
,	O
e.g.	O
,	O
the	O
Kabul	O
ambulance	O
bombing	O
event	O
.	O
Its	O
complexity	O
comes	O
from	O
the	O
inclusion	O
of	O
multiple	O
atomic	O
events	O
(	O
and	O
their	O
arguments	O
)	O
,	O
relations	O
and	O
temporal	O
order	O
.	O
A	O
complex	O
event	O
schema	O
can	O
be	O
used	O
to	O
define	O
the	O
typical	O
structure	O
of	O
a	O
particular	O
type	O
of	O
complex	O
event	O
,	O
e.g.	O
,	O
carbombing	O
.	O
This	O
leads	O
us	O
to	O
the	O
new	O
task	O
that	O
we	O
address	O
in	O
this	O
paper	O
:	O
temporal	O
complex	O
event	O
schema	O
induction	O
.	O
Figure	O
1	O
shows	O
an	O
example	O
schema	O
about	O
car	O
-	O
bombing	O
with	O
multiple	O
temporal	O
dependencies	O
between	O
events	O
.	O
Namely	O
,	O
the	O
occurrence	O
of	O
one	O
event	O
may	O
depend	O
on	O
multiple	O
events	O
.	O
For	O
example	O
,	O
the	O
ASSEMBLE	O
event	O
happens	O
after	O
buying	O
both	O
the	O
bomb	O
materials	O
and	O
the	O
vehicle	O
.	O
Also	O
,	O
there	O
may	O
be	O
multiple	O
events	O
following	O
an	O
event	O
,	O
such	O
as	O
the	O
multiple	O
consequences	O
of	O
the	O
ATTACK	O
event	O
in	O
Figure	O
1	O
.	O
That	O
is	O
to	O
say	O
,	O
"	O
the	O
future	O
is	O
not	O
one	O
-	O
dimensional	O
"	O
.	O
Our	O
automatically	O
induced	O
probabilistic	O
complex	O
event	O
schema	O
can	O
be	O
used	O
to	O
forecast	O
event	O
abstractions	O
into	O
the	O
future	O
and	O
thus	O
provide	O
a	O
comprehensive	O
understanding	O
of	O
evolving	O
situations	O
,	O
events	O
,	O
and	O
trends	O
.	O
For	O
each	O
type	O
of	O
complex	O
event	O
,	O
we	O
aim	O
to	O
induce	O
a	O
schema	O
library	O
that	O
is	O
probabilistic	O
,	O
temporally	O
organized	O
and	O
semantically	O
coherent	O
.	O
Low	O
level	O
atomic	O
event	O
schemas	O
are	O
abundant	O
,	O
and	O
can	O
be	O
part	O
of	O
multiple	O
,	O
sparsely	O
occurring	O
,	O
higher	O
-	O
level	O
schemas	O
.	O
We	O
propose	O
a	O
Temporal	O
Event	O
Graph	O
Model	O
,	O
an	O
auto	O
-	O
regressive	O
graph	B-TaskName
generation	I-TaskName
model	O
,	O
to	O
reach	O
this	O
goal	O
.	O
Given	O
a	O
currently	O
extracted	O
event	O
graph	O
,	O
we	O
generate	O
the	O
next	O
event	O
type	O
node	O
with	O
its	O
potential	O
arguments	O
,	O
such	O
as	O
the	O
ARREST	O
event	O
in	O
Figure	O
2	O
,	O
and	O
then	O
propagate	O
edge	O
-	O
aware	O
information	O
following	O
temporal	O
orders	O
.	O
After	O
that	O
,	O
we	O
employ	O
a	O
copy	O
mechanism	O
to	O
generate	O
coreferential	O
arguments	O
,	O
such	O
as	O
the	O
DETAINEE	O
argument	O
is	O
the	O
ATTACKER	O
of	O
the	O
previous	O
ATTACK	O
event	O
,	O
and	O
build	O
relation	O
edges	O
for	O
them	O
,	O
e.g.	O
,	O
PART	O
WHOLE	O
relation	O
between	O
the	O
PLACE	O
arguments	O
.	O
Finally	O
,	O
temporal	O
dependencies	O
are	O
determined	O
with	O
argument	O
connections	O
considered	O
,	O
such	O
as	O
the	O
temporal	O
edge	O
showing	O
that	O
ARREST	O
is	O
after	O
ATTACK	O
.	O
Our	O
generative	O
model	O
serves	O
as	O
both	O
a	O
schema	O
library	O
and	O
a	O
predictive	O
model	O
.	O
Specifically	O
,	O
we	O
can	O
probe	O
the	O
model	O
to	O
generate	O
event	O
graphs	O
unconditionally	O
to	O
obtain	O
a	O
set	O
of	O
schemas	O
.	O
We	O
can	O
also	O
pass	O
partially	O
instantiated	O
graphs	O
to	O
the	O
model	O
and	O
"	O
grow	O
"	O
the	O
graph	O
either	O
forward	O
or	O
backward	O
in	O
time	O
to	O
predict	O
missing	O
events	O
,	O
arguments	O
or	O
relations	O
,	O
both	O
from	O
the	O
past	O
and	O
in	O
the	O
future	O
.	O
We	O
propose	O
a	O
set	O
of	O
schema	O
matching	O
metrics	O
to	O
evaluate	O
the	O
induced	O
schemas	O
by	O
comparing	O
with	O
human	O
-	O
created	O
schemas	O
and	O
show	O
the	O
power	O
of	O
the	O
probabilistic	O
schema	O
in	O
the	O
task	O
of	O
future	O
event	O
prediction	O
as	O
an	O
extrinsic	O
evaluation	O
,	O
to	O
predict	O
event	O
types	O
that	O
are	O
likely	O
to	O
happen	O
next	O
.	O
We	O
make	O
the	O
following	O
novel	O
contributions	O
:	O
This	O
is	O
the	O
first	O
work	O
to	O
induce	O
probabilistic	O
temporal	O
graph	O
schemas	O
for	O
complex	O
events	O
Symbol	O
Meaning	O
G	O
G	O
Instance	O
graph	O
of	O
a	O
complex	O
event	O
S	O
S	O
Schema	O
graph	O
of	O
a	O
complex	O
event	O
type	O
e	O
E	O
Event	O
node	O
in	O
an	O
instance	O
graph	O
v	O
V	O
Entity	O
node	O
in	O
an	O
instance	O
graph	O
ei	O
,	O
e	O
l	O
Temporal	O
ordering	O
edge	O
between	O
events	O
ei	O
and	O
e	O
l	O
,	O
indicating	O
ei	O
is	O
before	O
e	O
l	O
ei	O
,	O
a	O
,	O
vj	O
Argument	O
edge	O
,	O
indicating	O
vj	O
plays	O
argument	O
role	O
a	O
in	O
the	O
event	O
ei	O
vj	O
,	O
r	O
,	O
v	O
k	O
Relation	O
edge	O
between	O
entities	O
vj	O
and	O
v	O
k	O
,	O
and	O
r	O
is	O
the	O
relation	O
type	O
A	O
(	O
e	O
)	O
Argument	O
role	O
set	O
of	O
event	O
e	O
,	O
defined	O
by	O
the	O
IE	O
ontology	B-MethodName
ΦE	O
The	O
type	O
set	O
of	O
events	O
ΦV	O
The	O
type	O
set	O
of	O
entities	O
φ	O
(	O
)	O
A	O
mapping	O
function	O
from	O
a	O
node	O
to	O
its	O
type	O

To	O
induce	O
schemas	O
for	O
a	O
complex	O
event	O
type	O
,	O
such	O
as	O
car	O
-	O
bombing	O
,	O
we	O
construct	O
a	O
set	O
of	O
instance	O
graphs	O
,	O
where	O
each	O
instance	O
graph	O
is	O
about	O
one	O
complex	O
event	O
,	O
such	O
as	O
Kabul	O
ambulance	O
bombing	O
.	O
We	O
first	O
identify	O
a	O
cluster	O
of	O
documents	O
that	O
describes	O
the	O
same	O
complex	O
event	O
.	O
In	O
this	O
paper	O
,	O
we	O
treat	O
all	O
documents	O
linked	O
to	O
a	O
single	O
Wikipedia	O
page	O
as	O
belonging	O
to	O
the	O
same	O
complex	O
event	O
,	O
detailed	O
in	O
4.1	O
.	O
We	O
use	O
OneIE	O
,	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
Information	O
Extraction	O
system	O
,	O
to	O
extract	O
entities	O
,	O
relations	O
and	O
events	O
,	O
and	O
then	O
perform	O
crossdocument	O
entity	O
(	O
Pan	O
et	O
al	O
,	O
2015	O
(	O
Pan	O
et	O
al	O
,	O
,	O
2017	O
and	O
event	B-TaskName
coreference	I-TaskName
resolution	I-TaskName
(	O
Lai	O
et	O
al	O
,	O
2021	O
)	O
over	O
the	O
document	O
cluster	O
of	O
each	O
complex	O
event	O
.	O
We	O
further	O
conduct	O
event	O
-	O
event	O
temporal	B-TaskName
relation	I-TaskName
extraction	I-TaskName
(	O
Ning	O
et	O
al	O
,	O
2019	O
;	O
Wen	O
et	O
al	O
,	O
2021b	O
)	O
to	O
determine	O
the	O
order	O
of	O
event	O
pairs	O
.	O
We	O
run	O
the	O
entire	O
pipeline	O
following	O
(	O
Wen	O
et	O
al	O
,	O
2021a	O
)	O
3	O
,	O
and	O
the	O
detailed	O
extraction	O
performance	O
is	O
reported	O
in	O
the	O
paper	O
.	O
After	O
extraction	O
,	O
we	O
construct	O
one	O
instance	O
graph	O
for	O
each	O
complex	O
event	O
,	O
where	O
coreferential	O
events	O
or	O
entities	O
are	O
merged	O
.	O
We	O
consider	O
the	O
isolated	O
events	O
as	O
irrelevant	O
nodes	O
in	O
schema	O
induction	O
,	O
so	O
they	O
are	O
excluded	O
from	O
the	O
instance	O
graphs	O
during	O
graph	B-TaskName
construction	I-TaskName
.	O
Considering	O
schema	O
graphs	O
focus	O
on	O
type	O
-	O
level	O
abstraction	O
,	O
we	O
use	O
type	O
label	O
and	O
node	O
index	O
to	O
represent	O
each	O
node	O
,	O
ignoring	O
the	O
mention	O
level	O
information	O
in	O
these	O
instance	O
graphs	O
.	O

Given	O
an	O
instance	O
graph	O
G	O
,	O
we	O
regard	O
the	O
schema	O
as	O
the	O
hidden	O
knowledge	O
to	O
guide	O
the	O
generation	O
of	O
these	O
graphs	O
.	O
To	O
this	O
end	O
,	O
we	O
propose	O
a	O
temporal	O
event	O
graph	O
model	O
that	O
maximizes	O
the	O
probability	O
of	O
each	O
instance	O
graph	O
,	O
parameterized	O
by	O
G	O
G	O
p	O
(	O
G	O
)	O
.	O
At	O
each	O
step	O
,	O
based	O
on	O
the	O
previous	O
graph	O
G	O
<	O
i	O
,	O
we	O
predict	O
one	O
event	O
node	O
e	O
i	O
with	O
its	O
arguments	O
to	O
generate	O
the	O
next	O
graph	O
G	O
i	O
,	O
p	O
(	O
G	O
)	O
=	O
|	O
E	O
|	O
i=0	O
p	O
(	O
G	O
i	O
|	O
G	O
<	O
i	O
)	O
.	O
We	O
factorize	O
the	O
probability	O
of	O
generating	O
new	O
nodes	O
and	O
edges	O
as	O
:	O
p	O
(	O
G	O
i	O
|	O
G	O
<	O
i	O
)	O
=	O
p	O
(	O
e	O
i	O
|	O
G	O
<	O
i	O
)	O
a	O
j	O
A	O
(	O
e	O
i	O
)	O
p	O
(	O
e	O
i	O
,	O
a	O
j	O
,	O
v	O
j	O
|	O
e	O
i	O
,	O
a	O
j	O
)	O
v	O
k	O
G	O
<	O
i	O
p	O
(	O
v	O
j	O
,	O
r	O
,	O
v	O
k	O
|	O
v	O
j	O
,	O
v	O
k	O
)	O
e	O
l	O
G	O
<	O
i	O
p	O
(	O
e	O
i	O
,	O
e	O
l	O
|	O
e	O
i	O
,	O
e	O
l	O
)	O
.	O
(	O
1	O
)	O
As	O
shown	O
in	O
Figure	O
2	O
,	O
an	O
event	O
node	O
e	O
i	O
is	O
generated	O
first	O
according	O
to	O
the	O
probability	O
p	O
(	O
e	O
i	O
|	O
G	O
<	O
i	O
)	O
.	O
We	O
then	O
add	O
argument	O
nodes	O
based	O
on	O
the	O
IE	O
ontology	B-MethodName
.	O
We	O
also	O
predict	O
relation	O
v	O
j	O
,	O
r	O
,	O
v	O
k	O
between	O
the	O
newly	O
generated	O
node	O
v	O
j	O
and	O
the	O
existing	O
nodes	O
v	O
k	O
G	O
<	O
i	O
.	O
After	O
knowing	O
the	O
shared	O
and	O
related	O
arguments	O
,	O
we	O
add	O
a	O
final	O
step	O
to	O
predict	O
the	O
temporal	O
relations	O
between	O
the	O
new	O
event	O
e	O
i	O
and	O
the	O
existing	O
events	O
e	O
l	O
G	O
<	O
i	O
.	O
In	O
the	O
traditional	O
graph	B-TaskName
generation	I-TaskName
setting	O
,	O
the	O
order	O
of	O
node	O
generation	O
can	O
be	O
arbitrary	O
.	O
However	O
,	O
in	O
our	O
instance	O
graphs	O
,	O
event	O
nodes	O
are	O
connected	O
through	O
temporal	O
relations	O
.	O
We	O
order	O
events	O
as	O
a	O
directed	O
acyclic	O
graph	O
(	O
DAG	O
)	O
.	O
Considering	O
each	O
event	O
may	O
have	O
multiple	O
events	O
both	O
"	O
before	O
"	O
and	O
"	O
after	O
"	O
,	O
we	O
obtain	O
the	O
generation	O
order	O
by	O
traversing	O
the	O
graph	O
using	O
Breadth	O
-	O
First	O
Search	O
.	O
We	O
also	O
add	O
dummy	O
START	O
/	O
END	O
event	O
nodes	O
to	O
indicate	O
the	O
starting	O
/	O
ending	O
of	O
the	O
graph	B-TaskName
generation	I-TaskName
.	O
At	O
the	O
beginning	O
of	O
the	O
generation	O
process	O
,	O
the	O
graph	O
G	O
0	B-DatasetName
has	O
a	O
single	O
start	O
event	O
node	O
e	O
[	O
SOG	O
]	O
.	O
We	O
generate	O
e	O
[	O
EOG	O
]	O
to	O
signal	O
the	O
end	O
of	O
the	O
graph	O
.	O

To	O
determine	O
the	O
event	O
type	O
of	O
the	O
newly	O
generated	O
event	O
node	O
e	O
i	O
,	O
we	O
apply	O
a	O
graph	O
pooling	O
over	O
all	O
events	O
to	O
get	O
the	O
current	O
graph	O
representation	O
g	O
i	O
,	O
g	O
i	O
=	O
Pooling	O
(	O
{	O
e	O
0	B-DatasetName
,	O
,	O
e	O
i−1	O
}	O
)	O
.	O
We	O
use	O
bold	O
to	O
denote	O
the	O
latent	O
representations	O
of	O
nodes	O
and	O
edges	O
,	O
which	O
will	O
be	O
initialized	O
as	O
zeros	O
and	O
updated	O
at	O
each	O
generation	O
step	O
via	O
message	O
passing	O
in	O
3.4	O
.	O
We	O
adopt	O
a	O
mean	O
-	O
pooling	O
operation	O
in	O
this	O
paper	O
.	O
After	O
that	O
,	O
the	O
event	O
type	O
is	O
predicted	O
through	O
a	O
fully	O
connected	O
layer	O
,	O
p	O
(	O
e	O
i	O
|	O
G	O
<	O
i	O
)	O
=	O
exp	O
(	O
W	O
φ	O
(	O
e	O
i	O
)	O
g	O
i	O
)	O
φ	O
Φ	O
E	O
∪	O
[	O
EOG	O
]	O
exp	O
(	O
W	O
φ	O
g	O
i	O
)	O
.	O
Once	O
we	O
know	O
the	O
event	O
type	O
of	O
e	O
i	O
,	O
we	O
add	O
all	O
of	O
its	O
arguments	O
in	O
A	O
(	O
e	O
i	O
)	O
defined	O
in	O
the	O
IE	O
ontology	B-MethodName
as	O
new	O
entity	O
nodes	O
.	O
For	O
example	O
,	O
in	O
Figure	O
2	O
,	O
the	O
new	O
event	O
e	O
i	O
is	O
an	O
ARREST	O
event	O
,	O
so	O
we	O
add	O
three	O
argument	O
nodes	O
for	O
DETAINEE	O
,	O
JAILOR	O
,	O
and	O
PLACE	O
respectively	O
.	O
The	O
edges	O
between	O
these	O
arguments	O
and	O
event	O
e	O
i	O
are	O
also	O
added	O
into	O
the	O
graph	O
.	O

After	O
updating	O
the	O
node	O
representations	O
,	O
we	O
detect	O
the	O
entity	O
type	O
of	O
each	O
argument	O
,	O
and	O
also	O
predict	O
whether	O
the	O
argument	O
is	O
coreferential	O
to	O
existing	O
entities	O
.	O
Inspired	B-DatasetName
by	O
copy	O
mechanism	O
(	O
Gu	O
et	O
al	O
,	O
2016	O
)	O
,	O
we	O
classify	O
each	O
argument	O
node	O
v	O
j	O
to	O
either	O
a	O
new	O
entity	O
with	O
entity	O
type	O
φ	O
(	O
v	O
j	O
)	O
,	O
or	O
an	O
existing	O
entity	O
node	O
in	O
the	O
previous	O
graph	O
G	O
<	O
i	O
.	O
For	O
example	O
,	O
in	O
Figure	O
2	O
,	O
the	O
DETAINEE	O
should	O
be	O
classified	O
to	O
the	O
existing	O
ATTACKER	O
node	O
,	O
while	O
JAILOR	O
node	O
is	O
classified	O
as	O
PERSON	O
.	O
Namely	O
,	O
p	O
(	O
e	O
i	O
,	O
a	O
j	O
,	O
v	O
j	O
|	O
e	O
i	O
,	O
a	O
j	O
)	O
=	O
p	O
(	O
e	O
i	O
,	O
a	O
j	O
,	O
v	O
j	O
,	O
g	O
|	O
e	O
i	O
,	O
a	O
j	O
)	O
if	O
v	O
j	O
is	O
new	O
,	O
p	O
(	O
e	O
i	O
,	O
a	O
j	O
,	O
v	O
j	O
,	O
c	O
|	O
e	O
i	O
,	O
a	O
j	O
)	O
otherwise	O
,	O
where	O
p	O
(	O
e	O
i	O
,	O
a	O
j	O
,	O
v	O
j	O
,	O
g	O
|	O
e	O
i	O
,	O
a	O
j	O
)	O
is	O
the	O
generation	O
probability	O
,	O
classifying	O
the	O
new	O
node	O
to	O
its	O
entity	O
type	O
φ	O
(	O
v	O
j	O
)	O
:	O
p	O
(	O
e	O
i	O
,	O
a	O
j	O
,	O
v	O
j	O
,	O
g	O
|	O
e	O
i	O
,	O
a	O
j	O
)	O
=	O
exp	O
(	O
W	O
φ	O
(	O
v	O
j	O
)	O
v	O
j	O
)	O
Z	O
The	O
copy	O
probability	O
p	O
(	O
e	O
i	O
,	O
a	O
j	O
,	O
v	O
j	O
,	O
c	O
|	O
e	O
i	O
,	O
a	O
j	O
)	O
selects	O
the	O
coreferential	O
entity	O
v	O
from	O
the	O
entities	O
in	O
existing	O
graph	O
,	O
denoted	O
by	O
V	O
<	O
i	O
,	O
p	O
(	O
e	O
i	O
,	O
a	O
j	O
,	O
v	O
j	O
,	O
c	O
|	O
e	O
i	O
,	O
a	O
j	O
)	O
=	O
exp	O
(	O
W	O
v	O
v	O
j	O
)	O
Z.	O
Here	O
,	O
Z	O
is	O
the	O
shared	O
normalization	O
term	O
,	O
Z	O
=	O
φ	O
Φ	O
V	O
exp	O
(	O
W	O
φ	O
v	O
j	O
)	O
+	O
v	O
V	O
<	O
i	O
exp	O
(	O
W	O
v	O
v	O
j	O
)	O
If	O
determined	O
to	O
copy	O
,	O
we	O
merge	O
coreferential	O
entities	O
in	O
the	O
graph	O
.	O

In	O
this	O
phase	O
,	O
we	O
determine	O
the	O
virtual	O
edges	O
to	O
be	O
kept	O
and	O
assign	O
relation	O
types	O
to	O
them	O
,	O
such	O
as	O
PARTWHOLE	O
relation	O
in	O
Figure	O
2	O
.	O
We	O
model	O
the	O
relation	O
edge	O
generation	O
probability	O
as	O
a	O
categorical	O
distribution	O
over	O
relation	O
types	O
,	O
and	O
add	O
[	O
O	O
]	O
(	O
OTHER	O
)	O
to	O
the	O
typeset	O
R	O
to	O
represent	O
that	O
there	O
is	O
no	O
relation	O
edge	O
:	O
p	O
(	O
v	O
j	O
,	O
r	O
,	O
v	O
k	O
|	O
v	O
j	O
,	O
v	O
k	O
)	O
=	O
exp	O
(	O
MLP	B-DatasetName
r	O
(	O
v	O
j	O
−	O
v	O
k	O
)	O
)	O
r	O
R∪	O
[	O
O	O
]	O
exp	O
(	O
MLP	B-DatasetName
r	O
(	O
v	O
j	O
−	O
v	O
k	O
)	O
)	O
We	O
use	O
two	O
hidden	O
layers	O
with	O
ReLU	B-MethodName
activation	O
functions	O
to	O
implement	O
the	O
MLP	B-DatasetName
.	O

We	O
conduct	O
experiments	O
on	O
two	O
datasets	O
for	O
both	O
the	O
general	O
scenario	O
and	O
a	O
more	O
specific	O
scenario	O
.	O
We	O
adopt	O
the	O
DARPA	B-DatasetName
KAIROS	O
6	O
ontology	B-MethodName
,	O
a	O
newly	O
defined	O
fine	O
-	O
grained	O
ontology	B-MethodName
for	O
Schema	O
Learning	O
,	O
with	O
24	O
entity	O
types	O
,	O
46	O
relation	O
types	O
,	O
67	O
event	O
types	O
,	O
and	O
85	O
argument	O
roles	O
.	O
7	O
Our	O
schema	O
induction	O
method	O
does	O
not	O
rely	O
on	O
any	O
specific	O
ontology	B-MethodName
,	O
only	O
the	O
IE	O
system	O
is	O
trained	O
on	O
a	O
given	O
ontology	B-MethodName
to	O
create	O
the	O
instance	O
event	O
graphs	O
.	O
General	B-DatasetName
Schema	O
Learning	O
Corpus	O
:	O
The	O
Schema	O
Learning	O
Corpus	O
,	O
released	O
by	O
LDC	O
(	O
LDC2020E25	O
)	O
,	O
includes	O
82	O
types	O
of	O
complex	O
events	O
,	O
such	O
as	O
Disease	O
Outbreak	O
,	O
Presentations	O
and	O
Shop	O
Online	O
.	O
Each	O
complex	O
event	O
is	O
associated	O
with	O
a	O
set	O
of	O
source	O
documents	O
.	O
This	O
data	O
set	O
al	O
o	O
includes	O
ground	O
-	O
truth	O
schemas	O
created	O
by	O
LDC	O
annotators	O
,	O
which	O
were	O
used	O
for	O
our	O
intrinsic	O
evaluation	O
.	O
IED	O
Schema	O
Learning	O
Corpus	O
:	O
The	O
same	O
type	O
of	O
complex	O
events	O
may	O
have	O
many	O
variants	O
,	O
which	O
depends	O
on	O
the	O
different	O
types	O
of	O
conditions	O
and	O
participants	O
.	O
In	O
order	O
to	O
evaluate	O
our	O
model	O
's	O
capability	O
at	O
capturing	O
uncertainty	O
and	O
multiple	O
hypotheses	O
,	O
we	O
decided	O
to	O
dive	O
deeper	O
into	O
one	O
scenario	O
and	O
chose	O
the	O
improvised	O
explosive	O
device	O
(	O
IED	O
)	O
as	O
our	O
case	O
study	O
.	O
We	O
first	O
collected	O
Wikipedia	O
articles	O
that	O
describe	O
4	O
types	O
of	O
complex	O
events	O
,	O
i.e.	O
,	O
Car	O
-	O
bombing	O
IED	O
,	O
Drone	O
Strikes	O
IED	O
,	O
Suicide	O
IED	O
and	O
General	B-DatasetName
IED	O
.	O
Then	O
we	O
followed	O
(	O
Li	O
et	O
al	O
,	O
2021	O
)	O
to	O
exploit	O
the	O
external	O
links	O
to	O
collect	O
the	O
additional	O
news	O
documents	O
with	O
the	O
corresponding	O
complex	O
event	O
type	O
.	O
The	O
ground	O
-	O
truth	O
schemas	O
for	O
this	O
IED	O
corpus	O
are	O
created	O
manually	O
,	O
through	O
a	O
schema	O
curation	O
tool	O
(	O
Mishra	O
et	O
al	O
,	O
2021	O
)	O
.	O
Only	O
one	O
human	O
schema	O
graph	O
was	O
created	O
for	O
each	O
complex	O
event	O
type	O
,	O
resulting	O
in	O
4	O
schemas	O
.	O
In	O
detail	O
,	O
for	O
each	O
complex	O
event	O
type	O
,	O
we	O
presented	O
example	O
instance	O
graphs	O
and	O
the	O
ranked	O
event	O
sequences	O
to	O
annotators	O
to	O
create	O
human	O
(	O
ground	O
truth	O
)	O
schemas	O
.	O
The	O
event	O
sequences	O
are	O
generated	O
by	O
traversing	O
the	O
instance	O
graphs	O
,	O
and	O
then	O
sorted	O
by	O
frequency	O
and	O
the	O
number	O
of	O
arguments	O
.	O
Initially	O
we	O
assigned	O
three	O
annotators	O
(	O
IE	O
experts	O
)	O
to	O
each	O
create	O
a	O
version	O
of	O
the	O
schema	O
and	O
then	O
the	O
final	O
schema	O
was	O
merged	O
through	O
discussion	O
.	O
After	O
that	O
,	O
two	O
annotators	O
(	O
linguists	O
)	O
performed	O
a	O
two	O
-	O
pass	O
revision	O
.	O
Human	O
curation	O
focuses	O
on	O
merging	O
and	O
trimming	O
steps	O
by	O
validating	O
them	O
using	O
the	O
reference	O
instance	O
graphs	O
.	O
Also	O
,	O
temporal	O
dependencies	O
between	O
steps	O
were	O
further	O
refined	O
,	O
and	O
coreferential	O
entities	O
and	O
their	O
relations	O
were	O
added	O
during	O
the	O
curation	O
process	O
.	O
To	O
avoid	O
bias	O
from	O
the	O
event	O
sequences	O
,	O
linguists	O
in	O
the	O
second	O
round	O
revision	O
were	O
not	O
presented	O
with	O
the	O
event	O
sequences	O
.	O
All	O
annotators	O
were	O
trained	O
and	O
disagreements	O
were	O
resolved	O
through	O
discussion	O
.	O

We	O
compare	O
the	O
generated	O
schemas	O
with	O
the	O
ground	O
truth	O
schemas	O
based	O
on	O
the	O
overlap	O
between	O
them	O
.	O
The	O
following	O
evaluation	O
metrics	O
were	O
employed	O
:	O
8	O
Event	O
Match	O
:	O
A	O
good	O
schema	O
must	O
contain	O
the	O
events	O
crucial	O
to	O
the	O
complex	O
event	O
scenario	O
.	O
Fscore	O
is	O
used	O
to	O
compute	O
the	O
overlap	O
of	O
event	O
nodes	O
.	O
Event	O
Sequence	O
Match	O
:	O
A	O
good	O
schema	O
is	O
able	O
to	O
track	O
events	O
through	O
a	O
timeline	O
.	O
So	O
we	O
obtain	O
event	O
sequences	O
following	O
temporal	O
order	O
,	O
and	O
evaluate	O
F	O
-	O
score	O
on	O
the	O
overlapping	O
sequences	O
of	O
lengths	O
l	O
=	O
2	O
and	O
l	O
=	O
3	O
.	O
Event	O
Argument	O
Connection	O
Match	O
:	O
Our	O
complex	O
event	O
graph	O
schema	O
includes	O
entities	O
and	O
their	O
relations	O
and	O
captures	O
how	O
events	O
are	O
connected	O
through	O
arguments	O
,	O
in	O
addition	O
to	O
their	O
temporal	O
order	O
.	O
We	O
categorize	O
these	O
connections	O
into	O
three	O
categories	O
:	O
(	O
1	O
)	O
two	O
events	O
are	O
connected	O
by	O
shared	O
arguments	O
;	O
(	O
2	O
)	O
two	O
events	O
have	O
related	O
arguments	O
,	O
i.e.	O
,	O
their	O
arguments	O
are	O
connected	O
through	O
entity	O
relations	O
;	O
(	O
3	O
)	O
there	O
are	O
no	O
direct	O
connections	O
between	O
two	O
events	O
.	O
For	O
every	O
pair	O
of	O
overlapped	O
events	O
,	O
we	O
calculate	O
F	O
-	O
score	O
based	O
on	O
whether	O
these	O
connections	O
are	O
predicted	O
correctly	O
.	O
The	O
human	O
schemas	O
of	O
the	O
General	B-DatasetName
dataset	O
do	O
not	O
contain	O
arguments	O
and	O
the	O
relations	O
between	O
arguments	O
,	O
so	O
we	O
only	O
compute	O
this	O
metric	O
for	O
the	O
IED	O
dataset	O
.	O

To	O
explore	O
schema	O
-	O
guided	O
probabilistic	O
reasoning	O
and	O
prediction	O
,	O
we	O
perform	O
an	O
extrinsic	O
evaluation	O
of	O
event	O
prediction	O
.	O
Different	O
from	O
traditional	O
event	O
prediction	O
tasks	O
,	O
the	O
temporal	O
event	O
graphs	O
contain	O
arguments	O
with	O
relations	O
,	O
and	O
there	O
are	O
type	O
labels	O
assigned	O
to	O
nodes	O
and	O
edges	O
.	O
We	O
create	O
a	O
graph	O
-	O
based	O
event	O
prediction	O
dataset	O
using	O
our	O
testing	O
graphs	O
.	O
The	O
task	O
aims	O
to	O
predict	O
ending	O
events	O
of	O
each	O
graph	O
,	O
i.e.	O
,	O
events	O
that	O
have	O
no	O
future	O
events	O
after	O
it	O
.	O
An	O
event	O
is	O
predicted	O
correctly	O
if	O
its	O
event	O
type	O
matches	O
one	O
of	O
the	O
ending	O
events	O
in	O
the	O
graph	O
.	O
Considering	O
that	O
there	O
can	O
be	O
multiple	O
ending	O
events	O
in	O
one	O
instance	O
graph	O
,	O
we	O
rank	O
event	O
type	B-TaskName
prediction	I-TaskName
scores	O
and	O
adopt	O
MRR	B-MetricName
(	O
Mean	O
Reciprocal	O
Rank	O
)	O
and	O
HITS@1	B-MetricName
as	O
evaluation	O
metrics	O
.	O

The	O
definition	O
of	O
a	O
complex	O
event	O
schema	O
separates	O
us	O
from	O
related	O
lines	O
of	O
work	O
,	O
namely	O
schema	O
induction	O
and	O
script	O
learning	O
.	O
Previous	O
work	O
on	O
schema	O
induction	O
aims	O
to	O
characterize	O
event	O
triggers	O
and	O
participants	O
of	O
individual	O
atomic	O
events	O
(	O
Chambers	O
,	O
2013	O
;	O
Cheung	O
et	O
al	O
,	O
2013	O
;	O
Nguyen	O
et	O
al	O
,	O
2015	O
;	O
Sha	O
et	O
al	O
,	O
2016	O
;	O
Yuan	O
et	O
al	O
,	O
2018	O
)	O
,	O
ignoring	O
inter	O
-	O
event	O
relations	O
.	O
Work	O
on	O
script	O
learning	O
,	O
on	O
the	O
other	O
hand	O
,	O
originally	O
limited	O
attention	O
to	O
event	O
chains	O
with	O
a	O
single	O
protagonist	O
(	O
Chambers	O
andJurafsky	O
,	O
2008	O
,	O
2009	O
;	O
Rudinger	O
et	O
al	O
,	O
2015	O
;	O
Jans	O
et	O
al	O
,	O
2012	O
;	O
Granroth	O
-	O
Wilding	O
and	O
Clark	O
,	O
2016	O
)	O
and	O
later	O
extended	O
to	O
multiple	O
participants	O
Mooney	O
,	O
2014	O
,	O
2016	O
;	O
Weber	O
et	O
al	O
,	O
2018	O
)	O
.	O
Recent	O
efforts	O
rely	O
on	O
distributed	O
representations	O
encoded	O
from	O
the	O
compositional	O
nature	O
of	O
events	O
(	O
Modi	O
,	O
2016	O
;	O
Granroth	O
-	O
Wilding	O
and	O
Clark	O
,	O
2016	O
;	O
Weber	O
et	O
al	O
,	O
2018Weber	O
et	O
al	O
,	O
,	O
2020	O
,	O
and	O
language	O
modeling	O
(	O
Rudinger	O
et	O
al	O
,	O
2015	O
;	O
Pichotta	O
and	O
Mooney	O
,	O
2016	O
;	O
Peng	O
and	O
Roth	O
,	O
2016	O
)	O
.	O
All	O
of	O
these	O
methods	O
still	O
assume	O
that	O
events	O
follow	O
linear	O
order	O
in	O
a	O
single	O
chain	O
.	O
They	O
also	O
overlook	O
the	O
relations	O
between	O
participants	O
which	O
are	O
critical	O
for	O
understanding	O
the	O
complex	O
event	O
.	O
However	O
,	O
we	O
induce	O
a	O
comprehensive	O
event	O
graph	O
schema	O
,	O
capturing	O
both	O
the	O
temporal	O
dependency	O
and	O
the	O
multi	O
-	O
hop	O
argument	O
dependency	O
across	O
events	O
.	O
Recent	O
work	O
on	O
event	O
graph	O
schema	O
induction	O
(	O
Li	O
et	O
al	O
,	O
2020	O
)	O
only	O
considers	O
the	O
connections	O
between	O
a	O
pair	O
of	O
two	O
events	O
.	O
Similarly	O
,	O
their	O
event	O
prediction	O
task	O
is	O
designed	O
to	O
automatically	O
generate	O
a	O
missing	O
event	O
(	O
e.g.	O
,	O
a	O
word	O
sequence	O
)	O
given	O
a	O
single	O
or	O
a	O
sequence	O
of	O
prerequisite	O
events	O
(	O
Nguyen	O
et	O
al	O
,	O
2017	O
;	O
Hu	O
et	O
al	O
,	O
2017	O
;	O
Li	O
et	O
al	O
,	O
2018b	O
;	O
Kiyomaru	O
et	O
al	O
,	O
2019	O
;	O
Lv	O
et	O
al	O
,	O
2019	O
)	O
,	O
or	O
predict	O
a	O
pre	O
-	O
condition	O
event	O
given	O
the	O
current	O
events	O
(	O
Kwon	O
et	O
al	O
,	O
2020	O
)	O
.	O
In	O
contrast	O
,	O
we	O
leverage	O
the	O
automatically	O
discovered	O
temporal	O
event	O
schema	O
as	O
guidance	O
to	O
forecast	O
the	O
future	O
events	O
.	O
Existing	O
script	O
annotations	O
(	O
Chambers	O
andJurafsky	O
,	O
2008	O
,	O
2010	O
;	O
Wanzare	O
et	O
al	O
,	O
2016	O
;	O
Mostafazadeh	O
et	O
al	O
,	O
2016a	O
,	O
b	O
;	O
Kwon	O
et	O
al	O
,	O
2020	O
)	O
can	O
not	O
support	O
a	O
comprehensive	O
graph	O
schema	O
induction	O
due	O
to	O
the	O
missing	O
of	O
critical	O
event	O
graph	O
structures	O
,	O
such	O
as	O
argument	O
relations	O
.	O
Furthermore	O
,	O
in	O
real	O
-	O
world	O
applications	O
,	O
complex	O
event	O
schemas	O
are	O
expected	O
to	O
be	O
induced	O
from	O
large	O
-	O
scale	O
historical	O
data	O
,	O
which	O
is	O
not	O
feasible	O
to	O
annotate	O
manually	O
.	O
We	O
propose	O
a	O
data	O
-	O
driven	O
schema	O
induction	O
approach	O
,	O
and	O
choose	O
to	O
use	O
IE	O
systems	O
instead	O
of	O
using	O
manual	O
annotation	O
,	O
to	O
induce	O
schemas	O
that	O
are	O
robust	O
and	O
can	O
tolerate	O
extraction	O
errors	O
.	O
Our	O
work	O
is	O
also	O
related	O
to	O
recent	O
advances	O
in	O
modeling	O
and	O
generation	O
of	O
graphs	O
(	O
Li	O
et	O
al	O
,	O
2018a	O
;	O
Jin	O
et	O
al	O
,	O
2018	O
;	O
Grover	O
et	O
al	O
,	O
2019	O
;	O
Simonovsky	O
and	O
Komodakis	O
,	O
2018	O
;	O
Liu	O
et	O
al	O
,	O
2019	O
;	O
Fu	O
et	O
al	O
,	O
2020	O
;	O
Dai	O
et	O
al	O
,	O
2020	O
;	O
You	O
et	O
al	O
,	O
2018	O
;	O
Liao	O
et	O
al	O
,	O
2019	O
;	O
Yoo	O
et	O
al	O
,	O
2020	O
;	O
Shi	O
et	O
al	O
,	O
2020	O
)	O
.	O
We	O
are	O
the	O
first	O
to	O
perform	O
graph	B-TaskName
generation	I-TaskName
on	O
event	O
graphs	O
.	O

This	O
research	O
is	O
based	O
upon	O
work	O
supported	O
by	O
U.S.	O
DARPA	B-DatasetName
KAIROS	O
Program	O
Nos	O
.	O
FA8750	O
-	O
19	O
-	O
2	O
-	O
1004	O
and	O
Air	O
Force	O
No	O
.	O
FA8650	O
-	O
17	O
-	O
C	O
-	O
7715	O
.	O
The	O
views	O
and	O
conclusions	O
contained	O
herein	O
are	O
those	O
of	O
the	O
authors	O
and	O
should	O
not	O
be	O
interpreted	O
as	O
necessarily	O
representing	O
the	O
official	O
policies	O
,	O
either	O
expressed	O
or	O
implied	O
,	O
of	O
DARPA	B-DatasetName
,	O
or	O
the	O
U.S.	O
Government	O
.	O
The	O
U.S.	O
Government	O
is	O
authorized	O
to	O
reproduce	O
and	O
distribute	O
reprints	O
for	O
governmental	O
purposes	O
notwithstanding	O
any	O
copyright	O
annotation	O
therein	O
.	O

An	O
important	O
desideratum	O
of	O
natural	O
language	O
generation	O
(	O
NLG	O
)	O
systems	O
is	O
to	O
produce	O
outputs	O
that	O
are	O
not	O
only	O
correct	O
,	O
but	O
also	O
diverse	O
.	O
For	O
example	O
,	O
a	O
dialog	O
system	O
(	O
Adiwardana	O
et	O
al	O
,	O
2020	O
)	O
should	O
permit	O
many	O
responses	O
for	O
the	O
prompt	O
"	O
How	O
are	O
you	O
today	O
?	O
"	O
.	O
Similarly	O
,	O
we	O
expect	O
diverse	O
responses	O
in	O
tasks	O
such	O
as	O
story	B-TaskName
generation	I-TaskName
(	O
Li	O
et	O
al	O
,	O
2018	O
)	O
,	O
question	B-TaskName
generation	I-TaskName
(	O
Pan	O
et	O
al	O
,	O
2019	O
)	O
and	O
question	B-TaskName
answering	I-TaskName
(	O
Fan	O
et	O
al	O
,	O
2019	O
)	O
.	O
Despite	O
growing	O
effort	O
to	O
produce	O
more	O
diverse	O
models	O
(	O
Li	O
et	O
al	O
,	O
2016c	O
,	O
a	O
;	O
Holtzman	O
et	O
al	O
,	O
2019	O
;	O
Du	O
and	O
Black	O
,	O
2019	O
)	O
,	O
there	O
is	O
no	O
standard	O
evaluation	O
metric	O
for	O
measuring	O
diversity	O
.	O
Thus	O
,	O
different	O
papers	O
evaluate	O
diversity	O
differently	O
(	O
if	O
at	O
Set	O
A	O
Pretty	O
much	O
everything	O
.	O
Nothing	O
,	O
really	O
.	O
You	O
wo	O
n't	O
believe	O
what	O
happened	O
!	O
Why	O
do	O
you	O
even	O
care	O
?	O
What	O
were	O
you	O
doing	O
that	O
was	O
more	O
important	O
than	O
this	O
?	O

We	O
now	O
describe	O
our	O
framework	O
for	O
evaluating	O
diversity	O
metrics	O
.	O
Diversity	O
has	O
many	O
facets	O
:	O
for	O
in	O
-	O
stance	O
,	O
a	O
set	O
of	O
sentences	O
can	O
be	O
diverse	O
in	O
terms	O
of	O
their	O
content	O
,	O
while	O
another	O
may	O
have	O
similar	O
content	O
,	O
but	O
diverse	O
form	O
(	O
Figure	O
1	O
)	O
.	O
Our	O
framework	O
provides	O
a	O
way	O
to	O
evaluate	O
metrics	O
for	O
different	O
aspects	O
of	O
diversity	O
under	O
moderate	O
assumptions	O
.	O
We	O
define	O
a	O
diversity	O
metric	O
m	O
div	O
(	O
S	O
c	O
)	O
R	O
as	O
a	O
function	O
that	O
takes	O
a	O
set	O
of	O
generated	O
responses	O
S	O
c	O
as	O
an	O
input	O
,	O
and	O
outputs	O
a	O
diversity	O
score	O
.	O
Each	O
response	O
s	O
S	O
c	O
is	O
generated	O
for	O
the	O
same	O
input	O
context	O
c	O
,	O
hence	O
S	O
c	O
is	O
a	O
sample	O
from	O
a	O
generative	O
distribution	O
P	O
gen	O
(	O
s	O
|	O
c	O
)	O
.	O
The	O
overall	O
diversity	O
score	O
of	O
a	O
generative	O
model	O
can	O
be	O
obtained	O
by	O
averaging	O
m	O
div	O
over	O
sets	O
S	O
c	O
sampled	O
from	O
the	O
model	O
given	O
multiple	O
contexts	O
c	O
C.	O
To	O
evaluate	O
m	O
div	O
(	O
)	O
,	O
we	O
assume	O
access	O
to	O
some	O
deterministic	O
diversity	O
parameter	O
d	O
that	O
controls	O
an	O
aspect	O
of	O
diversity	O
in	O
S	O
c	O
.	O
We	O
test	O
the	O
relation	O
between	O
m	O
div	O
and	O
the	O
parameter	O
d.	O
By	O
varying	O
d	O
and	O
measuring	O
m	O
div	O
,	O
we	O
can	O
compute	O
the	O
correlation	O
ρ	O
between	O
m	O
div	O
and	O
an	O
aspect	O
of	O
diversity	O
represented	O
by	O
d.	O
Because	O
our	O
goal	O
is	O
to	O
have	O
metrics	O
that	O
rank	O
the	O
diversity	O
of	O
generated	O
texts	O
,	O
we	O
use	O
Spearman	O
's	O
ρ	O
rank	O
correlation	O
as	O
our	O
test	O
score	O
.	O
Figure	O
2	O
illustrates	O
the	O
flow	O
of	O
a	O
test	O
in	O
our	O
framework	O
.	O
In	O
practice	O
,	O
to	O
control	O
the	O
diversity	O
level	O
of	O
S	O
c	O
using	O
d	O
,	O
we	O
use	O
a	O
tester	O
:	O
a	O
generative	O
model	O
that	O
takes	O
a	O
context	O
c	O
and	O
a	O
diversity	O
parameter	O
d	O
as	O
input	O
,	O
and	O
outputs	O
a	O
response	O
set	O
S	O
c	O
,	O
d	O
.	O
We	O
stress	O
that	O
the	O
tester	O
can	O
be	O
either	O
a	O
neural	O
model	O
or	O
a	O
human	O
.	O
A	O
good	O
tester	O
should	O
reliably	O
represent	O
the	O
diversity	O
level	O
quantified	O
by	O
d.	O
As	O
a	O
hypothetical	O
example	O
,	O
c	O
can	O
be	O
a	O
movie	O
name	O
and	O
d	O
represent	O
sentiment	O
diversity	O
,	O
that	O
is	O
,	O
the	O
number	O
of	O
different	O
sentiments	O
in	O
a	O
collection	O
of	O
reviews	O
S	O
c	O
.	O
A	O
human	O
tester	O
can	O
observe	O
c	O
and	O
d	O
,	O
and	O
produce	O
reviews	O
accordingly	O
(	O
such	O
data	O
can	O
be	O
easily	O
mined	O
from	O
IMDB	B-DatasetName
)	O
.	O
A	O
collection	O
of	O
such	O
(	O
d	O
,	O
S	O
c	O
,	O
d	O
)	O
makes	O
a	O
test	O
,	O
in	O
which	O
the	O
correlation	O
between	O
m	O
div	O
(	O
S	O
c	O
,	O
d	O
)	O
and	O
d	O
measures	O
the	O
sensitivity	O
of	O
m	O
div	O
to	O
sentiment	O
diversity	O
.	O
We	O
now	O
describe	O
two	O
tests	O
that	O
instantiate	O
this	O
framework	O
,	O
roughly	O
corresponding	O
to	O
the	O
two	O
main	O
aspects	O
of	O
diversity	O
:	O
form	O
diversity	O
and	O
content	O
diversity	O
.	O

The	O
diversity	O
of	O
a	O
NLG	O
system	O
constructed	O
from	O
a	O
LM	O
depends	O
on	O
both	O
the	O
LM	O
but	O
also	O
the	O
decoding	O
algorithm	O
on	O
top	O
of	O
it	O
.	O
For	O
example	O
,	O
beam	O
search	O
approximates	O
the	O
most	O
probable	O
output	O
,	O
and	O
dramatically	O
reduces	O
diversity	O
.	O
Conversely	O
,	O
sampling	O
from	O
the	O
LM	O
leads	O
to	O
high	O
diversity	O
,	O
but	O
low	O
quality	O
output	O
(	O
Holtzman	O
et	O
al	O
,	O
2019	O
)	O
.	O
A	O
popular	O
method	O
to	O
control	O
diversity	O
in	O
NLG	O
systems	O
is	O
to	O
vary	O
some	O
decoding	O
parameter	O
.	O
Variations	O
include	O
(	O
a	O
)	O
softmax	B-MethodName
temperature	O
(	O
Ackley	O
et	O
al	O
,	O
1985	O
)	O
,	O
where	O
a	O
parameter	O
τ	O
controls	O
the	O
skewness	O
of	O
the	O
softmax	B-MethodName
distribution	O
at	O
each	O
step	O
,	O
(	O
b	O
)	O
Nucleus	O
(	O
Top	O
-	O
p	O
)	O
sampling	O
(	O
Holtzman	O
et	O
al	O
,	O
2019	O
)	O
,	O
where	O
one	O
samples	O
at	O
each	O
step	O
from	O
the	O
minimal	O
set	O
of	O
most	O
probable	O
tokens	O
whose	O
cumulative	O
probability	O
is	O
at	O
least	O
p	O
,	O
and	O
(	O
c	O
)	O
Top	O
-	O
k	O
sampling	O
,	O
which	O
samples	O
from	O
the	O
top	O
-	O
k	O
most	O
probable	O
tokens	O
at	O
each	O
step	O
.	O
All	O
methods	O
skew	O
the	O
LM	O
distribution	O
in	O
a	O
way	O
that	O
avoids	O
low	O
-	O
probability	O
tokens	O
and	O
leads	O
to	O
higher	O
quality	O
(	O
Holtzman	O
et	O
al	O
,	O
2019	O
)	O
,	O
providing	O
a	O
decoding	O
parameter	O
that	O
trades	O
off	O
quality	O
and	O
diversity	O
(	O
Caccia	O
et	O
al	O
,	O
2018	O
)	O
.	O
In	O
the	O
decoding	O
test	O
(	O
decTest	O
)	O
,	O
we	O
define	O
the	O
tester	O
to	O
be	O
a	O
LM	O
,	O
such	O
as	O
GPT	B-MethodName
-	O
2	O
(	O
Radford	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
the	O
diversity	O
parameter	O
d	O
to	O
be	O
a	O
decoding	O
parameter	O
such	O
as	O
temperature	O
.	O
We	O
check	O
how	O
different	O
diversity	O
metrics	O
m	O
div	O
correlate	O
with	O
decoding	O
parameters	O
.	O
This	O
can	O
shed	O
light	O
on	O
the	O
quality	O
of	O
the	O
metrics	O
,	O
but	O
also	O
on	O
how	O
decoding	O
parameters	O
affect	O
the	O
output	O
of	O
a	O
NLG	O
system	O
.	O
The	O
decoding	O
test	O
uses	O
automatically	O
-	O
generated	O
data	O
that	O
is	O
cheap	O
to	O
produce	O
,	O
and	O
decoding	O
parameters	O
that	O
are	O
well	O
-	O
known	O
to	O
control	O
diversity	O
.	O
Thus	O
,	O
we	O
view	O
this	O
test	O
as	O
a	O
warm	O
-	O
up	O
test	O
to	O
explore	O
the	O
strengths	O
of	O
our	O
framework	O
.	O

We	O
expand	O
the	O
idea	O
from	O
Zhu	O
et	O
al	O
(	O
2018	O
)	O
and	O
suggest	O
a	O
method	O
to	O
construct	O
a	O
diversity	O
metric	O
from	O
any	O
2	O
-	O
sentence	O
similarity	O
metric	O
.	O
Given	O
m	O
sim	O
(	O
s	O
1	O
,	O
s	O
2	O
)	O
R	O
,	O
a	O
symmetric	O
similarity	O
metric	O
that	O
gets	O
a	O
pair	O
of	O
input	O
sentences	O
(	O
s	O
1	O
,	O
s	O
2	O
)	O
and	O
returns	O
a	O
similarity	O
score	O
,	O
we	O
can	O
define	O
a	O
diversity	O
metricm	O
div	O
as	O
the	O
negation	O
of	O
the	O
mean	O
similarity	O
score	O
across	O
all	O
(	O
unordered	O
)	O
pairs	O
of	O
S	O
c	O
:	O
m	O
div	O
(	O
S	O
c	O
)	O
=	O
−	O
1	O
|	O
Sc	O
|	O
2	O
s	O
i	O
,	O
s	O
j	O
Sc	O
,	O
i	O
>	O
j	O
m	O
sim	O
(	O
s	O
i	O
,	O
s	O
j	O
)	O
.	O
This	O
reduction	O
allows	O
us	O
to	O
easily	O
define	O
new	O
diversity	O
metrics	O
based	O
on	O
past	O
work	O
on	O
sentence	O
similarity	O
(	O
Gomaa	O
et	O
al	O
,	O
2013	O
;	O
Devlin	O
et	O
al	O
,	O
2019	O
;	O
Zhang	O
et	O
al	O
,	O
2019a	O
;	O
Reimers	O
and	O
Gurevych	O
,	O
2019	O
)	O
.	O
In	O
6	O
we	O
show	O
that	O
both	O
n	O
-	O
gram	O
-	O
based	O
similarity	O
metrics	O
and	O
neural	O
semantic	B-TaskName
similarity	I-TaskName
metrics	O
provide	O
useful	O
diversity	O
metrics	O
.	O
6	O
Experiments	O

We	O
apply	O
our	O
evaluation	O
procedure	O
on	O
three	O
different	O
English	O
NLG	O
tasks	O
that	O
require	O
diversity	O
.	O
Story	B-TaskName
completion	I-TaskName
(	O
storyGen	O
)	O
;	O
We	O
use	O
the	O
ROC	O
Stories	O
dataset	O
(	O
Mostafazadeh	O
et	O
al	O
,	O
2016	O
)	O
,	O
in	O
which	O
the	O
context	O
c	O
is	O
the	O
first	O
four	O
sentences	O
of	O
a	O
story	O
,	O
and	O
the	O
response	O
s	O
is	O
a	O
single	O
sentence	O
that	O
ends	O
the	O
story	O
.	O
We	O
use	O
the	O
contexts	O
C	O
from	O
this	O
data	O
and	O
generate	O
response	O
sets	O
S	O
c	O
for	O
each	O
context	O
using	O
our	O
testers	O
.	O
The	O
long	O
contexts	O
characterizing	O
this	O
data	O
narrow	O
down	O
the	O
space	O
of	O
possible	O
responses	O
,	O
making	O
this	O
a	O
"	O
low	O
-	O
entropy	O
"	O
generation	O
task	O
,	O
where	O
the	O
output	O
is	O
constrained	O
,	O
but	O
diversity	O
is	O
still	O
essential	O
.	O
Dialog	O
response	B-TaskName
generation	I-TaskName
(	O
respGen	O
)	O
;	O
A	O
comment	O
-	O
response	O
pairs	O
dataset	O
extracted	O
from	O
the	O
website	O
reddit.com	O
and	O
pre	O
-	O
processed	O
by	O
Hashimoto	O
et	O
al	O
(	O
2019	O
)	O
.	O
We	O
use	O
the	O
comments	O
from	O
their	O
data	O
as	O
contexts	O
C	O
and	O
generate	O
response	O
sets	O
S	O
c	O
for	O
each	O
context	O
using	O
our	O
testers	O
.	O
Since	O
comments	O
are	O
single	O
sentences	O
the	O
response	O
is	O
less	O
constrained	O
,	O
making	O
this	O
a	O
"	O
medium	O
-	O
entropy	O
"	O
generation	O
task	O
.	O
3	O
-	O
words	O
prompt	O
completion	O
(	O
promptGen	O
)	O
;	O
Contexts	O
C	O
are	O
3	O
-	O
words	O
prompts	O
,	O
extracted	O
from	O
the	O
Cornell	B-DatasetName
Movie	I-DatasetName
-	I-DatasetName
Dialogs	I-DatasetName
Corpus	I-DatasetName
(	O
Danescu	O
-	O
Niculescu	O
-	O
Mizil	O
and	O
Lee	O
,	O
2011	O
)	O
by	O
taking	O
the	O
first	O
three	O
words	O
from	O
each	O
original	O
context	O
.	O
The	O
response	O
sets	O
S	O
c	O
are	O
completions	O
of	O
the	O
prompts	O
,	O
generated	O
by	O
our	O
testers	O
.	O
This	O
context	O
provides	O
minimal	O
constraints	O
,	O
making	O
this	O
a	O
"	O
highentropy	O
"	O
generation	O
task	O
.	O
Samples	O
of	O
the	O
contexts	O
extracted	O
for	O
each	O
task	O
,	O
along	O
with	O
generated	O
response	O
sets	O
,	O
are	O
presented	O
in	O
Appendix	O
B.	O
We	O
intentionally	O
avoid	O
NLG	O
tasks	O
where	O
diversity	O
is	O
not	O
necessarily	O
desired	O
,	O
such	O
as	O
summarization	B-TaskName
and	O
machine	B-TaskName
translation	I-TaskName
.	O

In	O
decTest	O
we	O
measure	O
the	O
correlation	O
between	O
diversity	O
metrics	O
(	O
m	O
div	O
)	O
and	O
the	O
softmax	B-MethodName
temperature	O
decoding	O
parameter	O
(	O
d	O
)	O
.	O
The	O
tester	O
generating	O
the	O
response	O
sets	O
(	O
S	O
c	O
)	O
is	O
a	O
neural	O
NLG	O
model	O
.	O
Response	O
set	O
(	O
τ	O
=	O
0.25	O
)	O
It	O
was	O
a	O
minor	O
fire	O
and	O
they	O
put	O
it	O
out	O
.	O
It	O
was	O
a	O
fire	O
.	O
It	O
was	O
a	O
fire	O
.	O
It	O
was	O
a	O
fire	O
.	O
It	O
was	O
a	O
fire	O
.	O
Response	O
set	O
(	O
τ	O
=	O
0.8	O
)	O
They	O
arrived	O
and	O
put	O
out	O
the	O
fire	O
.	O
It	O
was	O
a	O
fire	O
.	O
It	O
was	O
a	O
fire	O
.	O
It	O
turned	O
out	O
to	O
be	O
a	O
fire	O
.	O
It	O
was	O
a	O
minor	O
fire	O
night	O
.	O
Response	O
set	O
(	O
τ	O
=	O
1.1	O
)	O
It	O
turned	O
out	O
to	O
be	O
a	O
mechanic	O
.	O
Before	O
the	O
fire	O
was	O
put	O
out	O
it	O
was	O
a	O
fire	O
.	O
It	O
was	O
a	O
fire	O
.	O
They	O
co	O
-	O
worker	O
matter	O
how	O
bad	O
the	O
fire	O
was	O
.	O
Several	O
shells	O
,	O
the	O
fire	O
department	O
came	O
just	O
in	O
time	O
.	O
deviation	O
.	O
HDS	O
metrics	O
are	O
computed	O
over	O
one	O
experiment	O
of	O
200	O
sets	O
,	O
due	O
to	O
their	O
high	O
cost	O
.	O
Data	O
for	O
storyGen	O
and	O
respGen	O
was	O
generated	O
by	O
the	O
MASS	O
model	O
,	O
fine	O
-	O
tuned	O
on	O
each	O
dataset	O
.	O
Data	O
for	O
promptGen	O
was	O
generated	O
by	O
GPT	B-MethodName
-	O
2	O
-	O
large	O
(	O
Radford	O
et	O
al	O
,	O
2019	O
)	O
without	O
fine	O
-	O
tuning	O
.	O
We	O
provide	O
examples	O
for	O
how	O
story	O
endings	O
change	O
as	O
a	O
function	O
of	O
temperature	O
in	O
Table	O
1	O
.	O
Examples	O
for	O
all	O
tasks	O
along	O
with	O
additional	O
reproducibility	O
details	O
are	O
in	O
the	O
Appendix	O
B.	O
For	O
each	O
HDS	O
metric	O
,	O
we	O
collected	O
10	O
ratings	O
per	O
query	O
from	O
Amazon	O
Mechanical	O
Turk	O
(	O
AMT	O
)	O
workers	O
.	O
While	O
absHDS	O
demands	O
one	O
query	O
per	O
response	O
set	O
,	O
in	O
order	O
to	O
perform	O
simHDS	O
at	O
a	O
reasonable	O
cost	O
,	O
we	O
chose	O
|	O
S	O
c	O
|	O
=	O
5	O
,	O
resulting	O
in	O
5	O
2	O
=	O
10	O
crowdsourcing	O
queries	O
instead	O
of	O
10	O
2	O
=	O
45	O
per	O
set	O
.	O
We	O
evaluate	O
simHDS	O
only	O
for	O
respGen	O
due	O
to	O
the	O
metric	O
's	O
high	O
cost	O
and	O
low	O
performance	O
.	O
Results	O
Table	O
2	O
presents	O
results	O
of	O
absHDS	O
,	O
simHDS	O
,	O
and	O
all	O
automatic	O
metrics	O
.	O
In	O
general	O
,	O
ngram	O
based	O
metrics	O
capture	O
the	O
diversity	O
induced	O
by	O
a	O
temperature	O
sweep	O
,	O
beating	O
HDS	O
and	O
neural	O
metrics	O
.	O
Figure	O
3	O
provides	O
a	O
more	O
detailed	O
analysis	O
.	O
Each	O
point	O
represents	O
a	O
single	O
set	O
of	O
responses	O
generated	O
at	O
some	O
temperature	O
.	O
While	O
rank	O
correlation	O
for	O
cosine	O
similarity	O
is	O
high	O
,	O
it	O
is	O
While	O
our	O
framework	O
is	O
meant	O
to	O
evaluate	O
diversity	O
metrics	O
,	O
the	O
results	O
of	O
the	O
test	O
let	O
us	O
reflect	O
on	O
the	O
decoding	O
parameters	O
themselves	O
.	O
This	O
result	O
shows	O
that	O
humans	O
perform	O
worse	O
than	O
automatic	O
metrics	O
in	O
this	O
experimental	O
setup	O
,	O
hinting	O
that	O
temperature	O
mostly	O
controls	O
superficial	O
changes	O
to	O
the	O
generated	O
text	O
.	O
Additionally	O
,	O
simHDS	O
performs	O
worse	O
than	O
absHDS	O
although	O
it	O
is	O
3x	O
more	O
expensive	O
,	O
showing	O
that	O
rating	O
the	O
entire	O
set	O
rather	O
than	O
averaging	O
over	O
pairs	O
is	O
useful	O
.	O
Other	O
decoding	O
parameters	O
To	O
compare	O
the	O
robustness	O
of	O
our	O
conclusions	O
to	O
other	O
decoding	O
parameters	O
,	O
we	O
repeat	O
it	O
with	O
two	O
additional	O
decoding	O
methods	O
:	O
(	O
a	O
)	O
in	O
Nucleus	O
(	O
Top	O
-	O
p	O
)	O
sampling	O
we	O
swept	O
linearly	O
over	O
100	O
values	O
of	O
p	O
in	O
the	O
range	O
[	O
0.1	O
,	O
1.0	O
]	O
;	O
(	O
b	O
)	O
In	O
Top	O
-	O
k	O
sampling	O
we	O
swept	O
k	O
in	O
logarithmic	O
scale	O
over	O
100	O
values	O
in	O
the	O
range	O
[	O
1	O
,	O
30	O
K	O
]	O
and	O
present	O
the	O
correlation	O
between	O
the	O
metrics	O
and	O
log	O
10	O
(	O
k	O
)	O
.	O
While	O
softmax	B-MethodName
temperature	O
enables	O
skewing	O
P	O
LM	O
to	O
a	O
more	O
diverse	O
P	O
gen	O
using	O
τ	O
>	O
1	O
,	O
both	O
Top	O
-	O
p	O
and	O
Top	O
-	O
k	O
enable	O
only	O
skewing	O
P	O
LM	O
to	O
a	O
more	O
sharp	O
(	O
hence	O
less	O
diverse	O
)	O
P	O
gen	O
.	O
Table	O
3	O
presents	O
results	O
for	O
all	O
automatic	O
metrics	O
using	O
the	O
three	O
decoding	O
methods	O
over	O
prompt	O
-	O
Gen.	O
Results	O
for	O
other	O
tasks	O
are	O
in	O
Appendix	O
C.	O
We	O
find	O
that	O
Top	O
-	O
p	O
correlates	O
well	O
with	O
temperature	O
along	O
all	O
three	O
generation	O
tasks	O
,	O
whereas	O
Topk	O
does	O
not	O
correlate	O
with	O
any	O
of	O
them	O
.	O

In	O
conTest	O
,	O
we	O
measure	O
the	O
correlation	O
between	O
diversity	O
metrics	O
(	O
m	O
div	O
)	O
and	O
content	O
diversity	O
,	O
represented	O
by	O
a	O
binary	O
parameter	O
d	O
{	O
0	B-DatasetName
,	O
1	O
}	O
.	O
The	O
testers	O
are	O
AMT	O
workers	O
,	O
guided	O
to	O
create	O
sets	O
with	O
high	O
level	O
of	O
form	O
diversity	O
and	O
high	O
or	O
low	O
content	O
diversity	O
according	O
to	O
d.	O

Tables	O
11	O
to	O
19	O
present	O
data	O
samples	O
from	O
sto	O
-	O
ryGen	O
,	O
respGen	O
and	O
promptGen	O
with	O
the	O
neural	O
testers	O
of	O
decTest	O
,	O
as	O
detailed	O
in	O
6	O
.	O
Each	O
table	O
presents	O
two	O
contexts	O
and	O
three	O
response	O
sets	O
per	O
context	O
.	O
Each	O
response	O
set	O
was	O
generated	O
with	O
a	O
different	O
value	O
of	O
decoding	O
parameter	O
for	O
the	O
three	O
decoding	O
methods	O
:	O
softmax	B-MethodName
temperature	O
,	O
Nucleus	O
sampling	O
,	O
and	O
Top	O
-	O
k.	O

We	O
applied	O
conTest	O
for	O
all	O
the	O
collected	O
data	O
for	O
each	O
of	O
the	O
three	O
NLG	O
tasks	O
(	O
see	O
Tables	O
8	O
and	O
9	O
10	O
)	O
.	O
Compared	O
to	O
Table	O
4	O
,	O
The	O
gap	O
between	O
the	O
best	O
performing	O
neural	O
metrics	O
(	O
sent	O
-	O
BERT	B-MethodName
)	O
and	O
absHDS	O
was	O
increased	O
in	O
favor	O
to	O
HDS	O
(	O
0.04	O
compared	O
to	O
0.1	O
difference	O
in	O
Spearman	O
's	O
ρ	O
)	O
.	O

Collected	O
data	O
and	O
code	O
All	O
the	O
collected	O
data	O
,	O
metric	O
scores	O
per	O
samples	O
for	O
each	O
of	O
decTest	O
and	O
conTest	O
,	O
as	O
well	O
as	O
code	O
for	O
running	O
and	O
visualizing	O
the	O
tests	O
,	O
are	O
publicly	O
available	O
8	O
.	O
The	O
collection	O
methods	O
are	O
elaborated	O
in	O
Section	O
6	O
.	O
Original	O
data	O
We	O
provide	O
additional	O
data	O
for	O
the	O
original	O
three	O
datasets	O
used	O
in	O
Section	O
6	O
.	O
ROC	O
Stories	O
dataset	O
9	O
(	O
Mostafazadeh	O
et	O
al	O
,	O
2016	O
)	O
used	O
for	O
storyGen	O
task	O
contains	O
96K/1K/1	O
K	O
train	O
/	O
validation	O
/	O
test	O
titles	O
and	O
five	O
-	O
sentence	O
stories	O
.	O
We	O
used	O
the	O
samples	O
without	O
pre	O
-	O
processing	O
for	O
both	O
fine	O
-	O
tuning	O
MASS	O
model	O
and	O
generate	O
samples	O
for	O
our	O
tests	O
.	O
Reddit	B-DatasetName
comment	O
-	O
response	O
dataset	O
used	O
for	O
respGen	O
task	O
contains	O
37	O
M	O
/1	O
M	O
/1	O
M	O
train	O
/	O
validation	O
/	O
test	O
comment	O
-	O
response	O
pairs	O
,	O
extracted	O
from	O
the	O
social	O
website	O
reddit.com	O
scraped	O
by	O
pushshift.io	O
followed	O
by	O
the	O
pre	O
-	O
process	O
described	O
in	O
(	O
Hashimoto	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
used	O
the	O
samples	O
without	O
further	O
processing	O
for	O
both	O
fine	O
-	O
tuning	O
MASS	O
model	O
and	O
generate	O
samples	O
for	O
our	O
tests	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
this	O
dataset	O
is	O
not	O
publicly	O
available	O
at	O
the	O
moment	O
.	O
CMDC	O
dataset	O
10	O
(	O
Danescu	O
-	O
Niculescu	O
-	O
Mizil	O
and	O
Lee	O
,	O
2011	O
)	O
contains	O
108K/30	O
K	O
train	O
/	O
test	O
sentence	O
-	O
response	O
pairs	O
extracted	O
from	O
movie	O
scripts	O
.	O
We	O
extracted	O
the	O
first	O
three	O
words	O
from	O
the	O
sentences	O
(	O
used	O
as	O
contexts	O
for	O
the	O
original	O
task	O
)	O
to	O
be	O
the	O
context	O
of	O
our	O
task	O
.	O
We	O
did	O
not	O
use	O
this	O
data	O
for	O
training	O
since	O
we	O
used	O
GPT	B-MethodName
-	O
2	O
without	O
fine	O
-	O
tuning	O
for	O
promptGen	O
.	O
Auto	O
-	O
generated	O
data	O
For	O
decTest	O
,	O
we	O
used	O
two	O
pre	O
-	O
trained	O
generative	O
models	O
for	O
generating	O
responses	O
given	O
the	O
contexts	O
:	O
For	O
storyGen	O
and	O
respGen	O
tasks	O
,	O
we	O
used	O
MASS	O
11	O
He	O
missed	O
his	O
meeting	O
.	O
He	O
missed	O
his	O
meeting	O
.	O
He	O
missed	O
his	O
meeting	O
.	O
He	O
missed	O
his	O
meeting	O
.	O
He	O
missed	O
his	O
meeting	O
.	O
He	O
missed	O
his	O
meeting	O
.	O
He	O
missed	O
his	O
meeting	O
.	O
He	O
missed	O
his	O
meeting	O
.	O
He	O
missed	O
his	O
meeting	O
.	O
He	O
missed	O
his	O
meeting	O
.	O
He	O
missed	O
his	O
meeting	O
.	O
He	O
missed	O
his	O
meeting	O
.	O
He	O
missed	O
his	O
meeting	O
.	O
He	O
missed	O
his	O
meeting	O
.	O
He	O
missed	O
his	O
meeting	O
.	O
He	O
missed	O
his	O
meeting	O
.	O
He	O
missed	O
his	O
meeting	O
.	O
He	O
missed	O
his	O
meeting	O
.	O
He	O
missed	O
his	O
meeting	O
.	O
He	O
missed	O
his	O
meeting	O
.	O
He	O
did	O
not	O
get	O
to	O
the	O
meeting	O
anymore	O
.	O
He	O
missed	O
his	O
meeting	O
.	O
He	O
passed	O
out	O
and	O
failing	O
the	O
meeting	O
He	O
missed	O
his	O
meeting	O
.	O
He	O
missed	O
his	O
meeting	O
.	O
He	O
missed	O
his	O
meeting	O
.	O
He	O
missed	O
his	O
meeting	O
.	O
He	O
passed	O
out	O
and	O
was	O
kicked	O
out	O
of	O
the	O
meeting	O
.	O
He	O
missed	O
his	O
meeting	O
.	O
He	O
missed	O
his	O
meeting	O
.	O
Family	O
Night	O
Food	O
.	O
Tonight	O
,	O
my	O
mom	O
ordered	O
Mexican	O
food	O
for	O
family	O
night	O
.	O
She	O
got	O
it	O
from	O
my	O
favorite	O
Mexican	O
place	O
in	O
town	O
.	O
When	O
it	O
arrived	O
,	O
it	O
was	O
hot	O
and	O
smelled	O
wonderful	O
.	O
We	O
devoured	O
it	O
with	O
gusto	O
.	O
After	O
a	O
few	O
hours	O
of	O
take	O
it	O
home	O
we	O
all	O
enjoyed	O
its	O
night	O
.	O
After	O
a	O
few	O
hours	O
of	O
take	O
it	O
home	O
we	O
all	O
enjoyed	O
its	O
night	O
.	O
After	O
a	O
few	O
hours	O
of	O
take	O
it	O
home	O
we	O
all	O
enjoyed	O
its	O
night	O
.	O
After	O
a	O
few	O
hours	O
of	O
eating	O
everyone	O
was	O
satisfied	O
.	O
After	O
a	O
few	O
hours	O
of	O
take	O
it	O
home	O
we	O
all	O
enjoyed	O
its	O
night	O
.	O
After	O
a	O
few	O
hours	O
of	O
eating	O
everyone	O
was	O
satisfied	O
.	O
After	O
a	O
few	O
hours	O
of	O
take	O
it	O
home	O
we	O
all	O
enjoyed	O
its	O
night	O
.	O
After	O
a	O
few	O
hours	O
of	O
take	O
it	O
home	O
we	O
all	O
enjoyed	O
its	O
night	O
.	O
After	O
a	O
few	O
hours	O
of	O
take	O
it	O
home	O
we	O
all	O
enjoyed	O
its	O
night	O
.	O
After	O
a	O
few	O
hours	O
of	O
eating	O
everyone	O
was	O
satisfied	O
.	O
After	O
dinner	O
,	O
we	O
all	O
went	O
home	O
to	O
cook	O
Mexican	O
food	O
.	O
After	O
a	O
few	O
hours	O
of	O
cooking	O
she	O
was	O
tired	O
and	O
ready	O
to	O
eat	O
.	O
After	O
dinner	O
,	O
I	O
always	O
put	O
got	O
ready	O
for	O
Christmas	O
.	O
After	O
dinner	O
,	O
I	O
helped	O
her	O
do	O
the	O
dishes	O
.	O
After	O
a	O
few	O
hours	O
of	O
dinner	O
,	O
the	O
food	O
was	O
amazing	O
.	O
After	O
a	O
few	O
hours	O
of	O
take	O
it	O
home	O
we	O
all	O
enjoyed	O
margaria	O
.	O
After	O
dinner	O
,	O
I	O
was	O
ready	O
to	O
take	O
on	O
work	O
the	O
next	O
day	O
of	O
After	O
dinner	O
,	O
I	O
was	O
sad	O
to	O
say	O
goodbye	O
to	O
her	O
After	O
a	O
few	O
hours	O
of	O
take	O
it	O
home	O
we	O
all	O
enjoyed	O
one	O
bite	O
.	O
After	O
a	O
few	O
hours	O
of	O
eating	O
everyone	O
was	O
satisfied	O
.	O
Even	O
though	O
my	O
stomach	O
was	O
gone	O
,	O
I	O
was	O
sad	O
it	O
was	O
finally	O
pockets	O
After	O
dinner	O
,	O
I	O
alone	O
.	O
All	O
in	O
all	O
the	O
family	O
while	O
my	O
mom	O
finished	O
the	O
food	O
.	O
After	O
a	O
few	O
hours	O
of	O
dancing	O
,	O
she	O
pianed	O
.	O
Afterwards	O
I	O
'd	O
never	O
finish	O
single	O
night	O
a	O
week	O
.	O
Afterwards	O
we	O
all	O
went	O
to	O
sleep	O
in	O
my	O
woods	O
.	O
Afterwards	O
I	O
helped	O
her	O
do	O
much	O
better	O
than	O
my	O
wife	O
.	O
Afterwards	O
my	O
mom	O
helped	O
me	O
finish	O
my	O
dinner	O
.	O
After	O
a	O
few	O
hours	O
we	O
all	O
enjoyed	O
storm	O
blood	O
from	O
the	O
Italy	O
.	O
After	O
dinner	O
,	O
we	O
all	O
enjoyed	O
some	O
good	O
food	O
together	O
.	O
The	O
worker	O
is	O
asked	O
to	O
generate	O
response	O
of	O
hers	O
/	O
his	O
own	O
and	O
rate	O
the	O
quality	O
of	O
the	O
tester	O
's	O
response	O
.	O
So	O
that	O
's	O
the	O
first	O
time	O
you	O
want	O
to	O
punch	O
somebody	O
,	O
not	O
miss	O
before	O
.	O
"	O
The	O
Seahawks	O
would	O
!	O
So	O
that	O
's	O
the	O
science	O
behind	O
the	O
Broadwell	O
-	O
E	O
processors	O
from	O
Intel	O
that	O
Intel	O
launched	O
last	O
fall	O
!	O
So	O
that	O
's	O
the	O
instinct	O
from	O
other	O
teams	O
,	O
that	O
they	O
're	O
a	O
headache	O
.	O
-	O
Ramsay	O
MacDonald	O
,	O
!	O
So	O
that	O
's	O
the	O
white	O
whale	O
right	O
there	O
about	O
too	O
much	O
debt	O
.	O
And	O
then	O
what	O
you	O
!	O
So	O
that	O
's	O
the	O
end	O
of	O
our	O
discussion	O
about	O
the	O
causes	O
.	O
What	O
happens	O
when	O
we	O
look	O
at	O
the	O
!	O
So	O
that	O
's	O
the	O
cover	O
of	O
inhibition	O
against	O
"	O
chronic	O
"	O
or	O
"	O
adaptive	O
"	O
stimulants	O
!	O
So	O
that	O
's	O
the	O
way	O
the	O
story	O
goes	O
,	O
but	O
exactly	O
how	O
is	O
cloud	O
providers	O
going	O
to	O
restrict	O
Their	O
!	O
So	O
that	O
's	O
the	O
beginning	O
,	O
the	O
beginning	O
of	O
the	O
show	O
,	O
I	O
guess	O
five	O
minutes	O
.	O
"	O
!	O
So	O
that	O
's	O
the	O
Indie	O
Mobile	O
Game	O
Week	O
Honoring	O
Winners	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
So	O
that	O
's	O
the	O
reason	O
I	O
'm	O
writing	O
,	O
that	O
's	O
why	O
you	O
do	O
n't	O
understand	O
why	O
people	O
know	O
!	O
do	O
you	O
listen	O
to	O
the	O
music	O
?	O
"	O
"	O
I	O
do	O
n't	O
know	O
.	O
I	O
do	O
n't	O
listen	O
!	O
do	O
you	O
listen	O
to	O
them	O
?	O
"	O
"	O
I	O
do	O
,	O
"	O
he	O
said	O
.	O
"	O
I	O
'm	O
not	O
!	O
do	O
you	O
listen	O
to	O
the	O
voices	O
of	O
the	O
people	O
?	O
"	O
"	O
I	O
do	O
,	O
"	O
said	O
the	O
king	O
!	O
do	O
you	O
listen	O
to	O
the	O
song	O
?	O
"	O
"	O
I	O
do	O
n't	O
know	O
.	O
I	O
do	O
n't	O
know	O
!	O
do	O
you	O
listen	O
to	O
the	O
music	O
?	O
"	O
"	O
I	O
do	O
.	O
"	O
"	O
You	O
're	O
not	O
!	O
do	O
you	O
listen	O
to	O
the	O
news	O
?	O
I	O
do	O
.	O
I	O
'm	O
a	O
big	O
fan	O
of	O
the	O
!	O
do	O
you	O
listen	O
to	O
me	O
?	O
"	O
"	O
Yes	O
,	O
I	O
do	O
.	O
"	O
"	O
I	O
'm	O
!	O
do	O
you	O
listen	O
to	O
the	O
other	O
side	O
?	O
"	O
"	O
I	O
do	O
n't	O
know	O
.	O
I	O
do	O
n't	O
!	O
do	O
you	O
listen	O
to	O
the	O
other	O
side	O
?	O
"	O
"	O
I	O
do	O
,	O
"	O
said	O
the	O
boy	O
.	O
"	O
!	O
do	O
you	O
listen	O
to	O
the	O
news	O
?	O
No	O
,	O
I	O
do	O
n't	O
.	O
I	O
do	O
n't	O
listen	O
!	O
do	O
you	O
listen	O
to	O
the	O
current	O
draft	O
?	O
I	O
listen	O
to	O
the	O
current	O
draft	O
.	O
I	O
'm	O
!	O
do	O
you	O
listen	O
to	O
it	O
?	O
"	O
It	O
's	O
easy	O
to	O
hear	O
the	O
"	O
why	O
?	O
"	O
but	O
when	O
!	O
do	O
you	O
listen	O
to	O
the	O
people	O
that	O
come	O
here	O
?	O
"	O
"	O
No	O
,	O
I	O
'm	O
too	O
busy	O
!	O
do	O
you	O
listen	O
to	O
the	O
thing	O
?	O
"	O
"	O
Of	O
course	O
I	O
do	O
.	O

Automatic	O
Generation	O
of	O
Contrast	O
Sets	O
from	O
Scene	O
Graphs	O
:	O
Probing	O
the	O
Compositional	O
Consistency	O
of	O
GQA	B-DatasetName

NLP	O
benchmarks	O
typically	O
evaluate	O
in	O
-	O
distribution	O
generalization	O
,	O
where	O
test	O
sets	O
are	O
drawn	O
i.i.d	O
from	O
a	O
distribution	O
similar	O
to	O
the	O
training	O
set	O
.	O
Recent	O
works	O
showed	O
that	O
high	O
performance	O
on	O
test	O
sets	O
sampled	O
in	O
this	O
manner	O
is	O
often	O
achieved	O
by	O
exploiting	O
systematic	O
gaps	O
,	O
annotation	O
artifacts	O
,	O
lexical	O
cues	O
and	O
other	O
heuristics	O
,	O
rather	O
than	O
learning	O
meaningful	O
task	O
-	O
related	O
signal	O
.	O
As	O
a	O
result	O
,	O
1	O
Our	O
contrast	O
sets	O
and	O
code	O
are	O
available	O
at	O
https://github.com/yonatanbitton/	O
AutoGenOfContrastSetsFromSceneGraphs	O
.	O
Figure	O
1	O
:	O
Illustration	O
of	O
our	O
approach	O
based	O
on	O
an	O
example	O
from	O
the	O
GQA	B-DatasetName
dataset	O
.	O
Top	O
:	O
QA	O
pairs	O
and	O
an	O
image	O
annotated	O
with	O
bounding	O
boxes	O
from	O
the	O
scene	O
graph	O
.	O
Bottom	O
:	O
relations	O
among	O
the	O
objects	O
in	O
the	O
scene	O
graph	O
.	O
First	O
line	O
at	O
the	O
top	O
is	O
the	O
original	O
QA	O
pair	O
,	O
while	O
the	O
following	O
3	O
lines	O
show	O
our	O
pertubated	O
questions	O
:	O
replacing	O
a	O
single	O
element	O
in	O
the	O
question	O
(	O
a	O
fence	O
)	O
with	O
other	O
options	O
(	O
a	O
wall	O
,	O
men	O
,	O
an	O
elephant	O
)	O
,	O
leading	O
to	O
a	O
change	O
in	O
the	O
output	O
label	O
.	O
For	O
each	O
QA	O
pair	O
,	O
the	O
LXMERT	B-MethodName
predicted	O
output	O
is	O
shown	O
.	O
the	O
out	O
-	O
of	O
-	O
domain	O
performance	O
of	O
these	O
models	O
is	O
often	O
severely	O
deteriorated	O
(	O
Jia	O
and	O
Liang	O
,	O
2017	O
;	O
Ribeiro	O
et	O
al	O
,	O
2018	O
;	O
Gururangan	O
et	O
al	O
,	O
2018	O
;	O
Geva	O
et	O
al	O
,	O
2019	O
;	O
McCoy	O
et	O
al	O
,	O
2019	O
;	O
Feng	O
et	O
al	O
,	O
2019	O
;	O
Stanovsky	O
et	O
al	O
,	O
2019	O
)	O
.	O
Recently	O
,	O
Kaushik	O
et	O
al	O
(	O
2019	O
)	O
and	O
introduced	O
the	O
contrast	O
sets	O
approach	O
to	O
probe	O
out	O
-	O
of	O
-	O
domain	B-TaskName
generalization	I-TaskName
.	O
Contrast	O
sets	O
are	O
constructed	O
via	O
minimal	O
modifications	O
to	O
test	O
inputs	O
,	O
such	O
that	O
their	O
label	O
is	O
modified	O
.	O
For	O
example	O
,	O
in	O
Fig	O
.	O
1	O
,	O
replacing	O
"	O
a	O
fence	O
"	O
with	O
"	O
a	O
wall	O
"	O
,	O
changes	O
the	O
answer	O
from	O
"	O
Yes	O
"	O
to	O
"	O
No	O
"	O
.	O
Since	O
such	O
perturbations	O
introduce	O
minimal	O
additional	O
semantic	O
complexity	O
,	O
robust	O
models	O
are	O
expected	O
to	O
perform	O
similarly	O
on	O
the	O
test	O
and	O
contrast	O
sets	O
.	O
However	O
,	O
a	O
range	O
of	O
NLP	O
models	O
severely	O
degrade	O
in	O
performance	O
on	O
contrast	O
sets	O
,	O
hinting	O
that	O
they	O
do	O
not	O
generalize	O
well	O
.	O
Except	O
two	O
recent	O
exceptions	O
for	O
textual	O
datasets	O
(	O
Li	O
et	O
al	O
,	O
2020	O
;	O
Rosenman	O
et	O
al	O
,	O
2020	O
)	O
,	O
contrast	O
sets	O
have	O
so	O
far	O
been	O
built	O
manually	O
,	O
requiring	O
extensive	O
human	O
effort	O
and	O
expertise	O
.	O
In	O
this	O
work	O
,	O
we	O
propose	O
a	O
method	O
for	O
automatic	O
generation	O
of	O
large	O
contrast	O
sets	O
for	O
visual	B-DatasetName
question	I-DatasetName
answering	I-DatasetName
(	O
VQA	B-TaskName
)	O
.	O
We	O
experiment	O
with	O
the	O
GQA	B-DatasetName
dataset	O
(	O
Hudson	O
and	O
Manning	O
,	O
2019	O
)	O
.	O
GQA	B-DatasetName
includes	O
semantic	O
scene	O
graphs	O
(	O
Krishna	O
et	O
al	O
,	O
2017	O
)	O
representing	O
the	O
spatial	O
relations	O
between	O
objects	O
in	O
the	O
image	O
,	O
as	O
exemplified	O
in	O
Fig	O
.	O
1	O
.	O
The	O
scene	O
graphs	O
,	O
along	O
with	O
functional	O
programs	O
that	O
represent	O
the	O
questions	O
,	O
are	O
used	O
to	O
balance	O
the	O
dataset	O
,	O
thus	O
aiming	O
to	O
mitigate	O
spurious	O
dataset	O
correlations	O
.	O
We	O
leverage	O
the	O
GQA	B-DatasetName
scene	O
graphs	O
to	O
create	O
contrast	O
sets	O
,	O
by	O
automatically	O
computing	O
the	O
answers	O
to	O
question	O
perturbations	O
,	O
e.g.	O
,	O
verifying	O
that	O
there	O
is	O
no	O
wall	O
near	O
the	O
puddle	O
in	O
Fig	O
.	O
1	O
.	O
We	O
create	O
automatic	O
contrast	O
sets	O
for	O
29	O
K	O
samples	O
or	O
≈22	O
%	O
of	O
the	O
validation	O
set	O
.	O
We	O
manually	O
verify	O
the	O
correctness	O
of	O
1	O
,	O
106	O
of	O
these	O
samples	O
on	O
Mechanical	O
Turk	O
.	O
Following	O
,	O
we	O
evaluate	O
two	O
leading	O
models	O
,	O
LXMERT	B-MethodName
(	O
Tan	O
and	O
Bansal	O
,	O
2019	O
)	O
and	O
MAC	O
(	O
Hudson	O
and	O
Manning	O
,	O
2019	O
)	O
on	O
our	O
contrast	O
sets	O
,	O
and	O
find	O
a	O
13	O
-	O
17	O
%	O
reduction	O
in	O
performance	O
compared	O
to	O
the	O
original	O
validation	O
set	O
.	O
Finally	O
,	O
we	O
show	O
that	O
our	O
automatic	O
method	O
for	O
contrast	O
set	O
construction	O
can	O
be	O
used	O
to	O
improve	O
performance	O
by	O
employing	O
it	O
during	O
training	O
.	O
We	O
augment	O
the	O
GQA	B-DatasetName
training	O
set	O
with	O
automatically	O
constructed	O
training	O
contrast	O
sets	O
(	O
adding	O
80	O
K	O
samples	O
to	O
the	O
existing	O
943	O
K	O
in	O
GQA	B-DatasetName
)	O
,	O
and	O
observe	O
that	O
when	O
trained	O
with	O
it	O
,	O
both	O
LXMERT	B-MethodName
and	O
MAC	O
improve	O
by	O
about	O
14	O
%	O
on	O
the	O
contrast	O
sets	O
,	O
while	O
maintaining	O
their	O
original	O
validation	O
performance	O
.	O
Our	O
key	O
contributions	O
are	O
:	O
(	O
1	O
)	O
We	O
present	O
an	O
automatic	O
method	O
for	O
creating	O
contrast	O
sets	O
for	O
VQA	B-TaskName
datasets	O
with	O
structured	O
input	O
representations	O
;	O
(	O
2	O
)	O
We	O
automatically	O
create	O
contrast	O
sets	O
for	O
GQA	B-DatasetName
,	O
and	O
find	O
that	O
for	O
two	O
strong	O
models	O
,	O
performance	O
on	O
the	O
contrast	O
sets	O
is	O
lower	O
than	O
on	O
the	O
original	O
validation	O
set	O
;	O
and	O
(	O
3	O
)	O
We	O
apply	O
our	O
method	O
to	O
augment	O
the	O
training	O
data	O
,	O
improving	O
both	O
models	O
'	O
performance	O
on	O
the	O
contrast	O
sets	O
.	O

To	O
construct	O
automatic	O
contrast	O
sets	O
for	O
GQA	B-DatasetName
we	O
first	O
identify	O
a	O
large	O
subset	O
of	O
questions	O
requiring	O
specific	O
reasoning	O
skills	O
(	O
2.1	O
)	O
.	O
Using	O
the	O
scene	O
graph	O
representation	O
,	O
we	O
perturb	O
each	O
question	O
in	O
a	O
manner	O
which	O
changes	O
its	O
gold	O
answer	O
(	O
2.2	O
)	O
.	O
Finally	O
,	O
we	O
validate	O
the	O
automatic	O
process	O
via	O
crowdsourcing	O
(	O
2.3	O
)	O
.	O

The	O
questions	O
in	O
the	O
GQA	B-DatasetName
dataset	O
present	O
a	O
diverse	O
set	O
of	O
modelling	O
challenges	O
,	O
as	O
exemplified	O
in	O
Table	O
1	O
,	O
including	O
object	O
identification	O
and	O
grounding	O
,	O
spatial	O
reasoning	O
and	O
color	O
identification	O
.	O
Following	O
the	O
contrast	O
set	O
approach	O
,	O
we	O
create	O
perturbations	O
testing	O
whether	O
models	O
are	O
capable	O
of	O
solving	O
questions	O
which	O
require	O
this	O
skill	O
set	O
,	O
but	O
that	O
diverge	O
from	O
their	O
training	O
distribution	O
.	O
To	O
achieve	O
this	O
,	O
we	O
identify	O
commonly	O
recurring	O
question	O
templates	O
which	O
specifically	O
require	O
such	O
skills	O
.	O
For	O
example	O
,	O
to	O
answer	O
the	O
question	O
"	O
Are	O
there	O
any	O
cats	O
near	O
the	O
boat	O
?	O
"	O
a	O
model	O
needs	O
to	O
identify	O
objects	O
in	O
the	O
image	O
(	O
cats	O
,	O
boat	O
)	O
,	O
link	O
them	O
to	O
the	O
question	O
,	O
and	O
identify	O
their	O
relative	O
position	O
.	O
We	O
identify	O
six	O
question	O
templates	O
,	O
testing	O
various	O
skills	O
(	O
Table	O
1	O
)	O
.	O
We	O
abstract	O
each	O
question	O
template	O
with	O
a	O
regular	O
expression	O
which	O
identifies	O
the	O
question	O
types	O
as	O
well	O
as	O
the	O
physical	O
objects	O
,	O
their	O
attributes	O
(	O
e.g.	O
,	O
colors	O
)	O
,	O
and	O
spatial	O
relations	O
.	O
Overall	O
,	O
these	O
regular	O
expressions	O
match	O
29	O
K	O
questions	O
in	O
the	O
validation	O
set	O
(	O
≈22	O
%	O
)	O
,	O
and	O
80	O
K	O
questions	O
in	O
the	O
training	O
set	O
(	O
≈8	O
%	O
)	O
.	O

To	O
verify	O
the	O
correctness	O
of	O
our	O
automatic	O
process	O
,	O
we	O
sampled	O
553	O
images	O
,	O
each	O
one	O
with	O
an	O
original	O
and	O
perturbed	O
QA	O
pair	O
for	O
a	O
total	O
of	O
1	O
,	O
106	O
instances	O
(	O
≈4	O
%	O
of	O
the	O
validation	O
contrast	O
pairs	O
)	O
.	O
The	O
(	O
image	O
,	O
question	O
)	O
pairs	O
were	O
answered	O
independently	O
by	O
human	O
annotators	O
on	O
Amazon	O
Mechanical	O
Turk	O
(	O
see	O
Fig	O
.	O
3	O
in	O
Appendix	O
A.4	O
)	O
,	O
oblivious	O
to	O
whether	O
the	O
question	O
originated	O
from	O
GQA	B-DatasetName
or	O
from	O
our	O
automatic	O
contrast	O
set	O
.	O
We	O
found	O
that	O
the	O
workers	O
were	O
able	O
to	O
correctly	O
answer	O
72.3	O
%	O
of	O
the	O
perturbed	O
questions	O
,	O
slightly	O
lower	O
than	O
their	O
performance	O
on	O
the	O
original	O
questions	O
(	O
76.6	O
%	O
)	O
.	O
2	O
We	O
observed	O
high	O
agreement	O
between	O
annotators	O
(	O
κ	O
=	O
0.679	O
)	O
.	O
Our	O
analysis	O
shows	O
that	O
the	O
human	O
performance	O
difference	O
between	O
the	O
perturbed	O
questions	O
and	O
the	O
original	O
questions	O
can	O
be	O
attributed	O
to	O
the	O
scene	O
Figure	O
2	O
:	O
GQA	B-DatasetName
image	O
(	O
left	O
)	O
with	O
example	O
perturbations	O
for	O
different	O
question	O
templates	O
(	O
right	O
)	O
.	O
Each	O
perturbation	O
aims	O
to	O
change	O
the	O
label	O
in	O
a	O
predetermined	O
manner	O
,	O
e.g.	O
,	O
from	O
"	O
yes	O
"	O
to	O
"	O
no	O
"	O
.	O

Training	O
graph	O
annotation	O
errors	O
in	O
the	O
GQA	B-DatasetName
dataset	O
:	O
3.5	O
%	O
of	O
the	O
4	O
%	O
difference	O
is	O
caused	O
by	O
a	O
discrepancy	O
between	O
image	O
and	O
scene	O
graph	O
(	O
objects	O
appearing	O
in	O
the	O
image	O
and	O
not	O
in	O
the	O
graph	O
,	O
and	O
vice	O
versa	O
)	O
.	O
Examples	O
are	O
available	O
in	O
Fig	O
.	O
5	O
in	O
Appendix	O
A.5	O
.	O

Our	O
results	O
suggest	O
that	O
both	O
MAC	O
and	O
LXMERT	B-MethodName
under	O
-	O
perform	O
when	O
tested	O
out	O
of	O
distribution	O
.	O
A	O
remaining	O
question	O
is	O
whether	O
this	O
is	O
due	O
to	O
model	O
architecture	O
or	O
dataset	O
design	O
.	O
claim	O
that	O
both	O
of	O
these	O
models	O
are	O
prone	O
to	O
fail	O
on	O
compositional	O
generalization	O
because	O
they	O
do	O
not	O
decompose	O
the	O
problem	O
into	O
smaller	O
sub	O
-	O
tasks	O
.	O
Our	O
results	O
support	O
this	O
claim	O
.	O
On	O
the	O
other	O
hand	O
,	O
it	O
is	O
possible	O
that	O
a	O
different	O
dataset	O
could	O
prevent	O
these	O
models	O
from	O
finding	O
shortcuts	O
.	O
Is	O
there	O
a	O
dataset	O
that	O
can	O
prevent	O
all	O
shortcuts	O
?	O
Our	O
automatic	O
method	O
for	O
creating	O
contrast	O
sets	O
allows	O
us	O
to	O
ask	O
those	O
questions	O
,	O
while	O
we	O
believe	O
that	O
future	O
work	O
in	O
better	O
training	O
mechanisms	O
,	O
as	O
suggested	O
in	O
and	O
Jin	O
et	O
al	O
(	O
2020	O
)	O
,	O
could	O
help	O
in	O
making	O
more	O
robust	O
models	O
.	O
We	O
proposed	O
an	O
automatic	O
method	O
for	O
creating	O
contrast	O
sets	O
for	O
VQA	B-TaskName
datasets	O
that	O
use	O
annotated	O
scene	O
graphs	O
.	O
We	O
created	O
contrast	O
sets	O
for	O
the	O
GQA	B-DatasetName
dataset	O
,	O
which	O
is	O
designed	O
to	O
be	O
compositional	O
,	O
balanced	O
,	O
and	O
robust	O
against	O
statistical	O
biases	O
.	O
We	O
observed	O
a	O
large	O
performance	O
drop	O
between	O
the	O
original	O
and	O
augmented	O
sets	O
.	O
As	O
our	O
contrast	O
sets	O
can	O
be	O
generated	O
cheaply	O
,	O
we	O
further	O
augmented	O
the	O
GQA	B-DatasetName
training	O
data	O
with	O
additional	O
perturbed	O
questions	O
,	O
and	O
showed	O
that	O
this	O
improves	O
models	O
'	O
performance	O
on	O
the	O
contrast	O
set	O
.	O
Our	O
proposed	O
method	O
can	O
be	O
extended	O
to	O
other	O
VQA	B-TaskName
datasets	O
.	O

We	O
created	O
contrast	O
sets	O
automatically	O
,	O
and	O
verified	O
their	O
correctness	O
via	O
the	O
crowdsourcing	O
annotation	O
of	O
a	O
sample	O
of	O
roughly	O
1	O
K	O
instances	O
.	O
Section	O
2.3	O
describes	O
the	O
annotation	O
process	O
on	O
Amazon	O
Mechanical	O
Turk	O
.	O
The	O
images	O
and	O
original	O
questions	O
were	O
sampled	O
from	O
the	O
public	O
GQA	B-DatasetName
dataset	O
(	O
Hudson	O
and	O
Manning	O
,	O
2019	O
)	O
,	O
in	O
the	O
English	O
language	O
.	O
Fig	O
.	O
3	O
in	O
Appendix	O
A.4	O
provides	O
example	O
of	O
the	O
annotation	O
task	O
.	O
Overall	O
,	O
the	O
crowdsourcing	O
task	O
resulted	O
in	O
≈6	O
hours	O
of	O
work	O
,	O
which	O
paid	O
an	O
average	O
of	O
11USD	O
per	O
hour	O
per	O
annotator	O
.	O
Reproducibility	O
The	O
augmentations	O
were	O
performed	O
with	O
a	O
MacBook	O
Pro	O
laptop	O
.	O
Augmentations	O
for	O
the	O
validation	O
data	O
takes	O
<	O
1	O
hour	O
per	O
question	O
template	O
,	O
and	O
for	O
the	O
training	O
data	O
<	O
3	O
hours	O
per	O
question	O
template	O
.	O
Overall	O
process	O
,	O
<	O
24	O
hours	O
.	O
The	O
experiments	O
have	O
been	O
performed	O
with	O
the	O
public	O
implementations	O
of	O
MAC	O
(	O
Hudson	O
and	O
Manning	O
,	O
2018	O
)	O
and	O
LXMERT	B-MethodName
(	O
Tan	O
and	O
Bansal	O
,	O
2019	O
)	O
,	O
models	O
:	O
https	O
:	O
//github.com	O
/	O
airsplay	O
/	O
lxmert	B-MethodName
,	O
https://github.com/stanfordnlp/	O
mac	O
-	O
network/.	O
The	O
configurations	O
were	O
modified	O
to	O
not	O
include	O
the	O
validation	O
set	O
in	O
the	O
training	O
process	O
.	O
The	O
experiments	O
were	O
performed	O
with	O
a	O
Linux	B-DatasetName
virtual	O
machine	O
with	O
a	O
NVIDIA	O
's	O
Tesla	O
V100	O
GPU	O
.	O
The	O
training	O
took	O
∼1	O
-	O
2	O
days	O
in	O
each	O
model	O
.	O
Validation	O
took	O
∼	O
30	O
minutes	O
.	O

Table	O
5	O
reports	O
the	O
basic	O
statistics	O
of	O
automatic	O
contrast	O
sets	O
generation	O
method	O
when	O
applied	O
on	O
the	O
GQA	B-DatasetName
validation	B-DatasetName
dataset	I-DatasetName
.	O
It	O
shows	O
the	O
overall	O
number	O
of	O
images	O
and	O
QA	O
pairs	O
that	O
matched	O
the	O
6	O
question	O
types	O
we	O
identified	O
.	O
Tables	O
6	O
shows	O
the	O
statistics	O
per	O
question	O
type	O
,	O
indicating	O
how	O
productive	O
each	O
augmentation	O
method	O
is	O
.	O
Tables	O
7	O
and	O
8	O

We	O
thank	O
the	O
reviewers	O
for	O
the	O
helpful	O
comments	O
and	O
feedback	O
.	O
We	O
thank	O
the	O
authors	O
of	O
GQA	B-DatasetName
for	O
building	O
the	O
dataset	O
,	O
and	O
the	O
authors	O
of	O
LXMERT	B-MethodName
and	O
MAC	O
for	O
sharing	O
their	O
code	O
and	O
making	O
it	O
usable	O
.	O
This	O
work	O
was	O
supported	O
in	O
part	O
by	O
the	O
Center	O
for	O
Interdisciplinary	O
Data	O
Science	O
Research	O
at	O
the	O
Hebrew	O
University	O
of	O
Jerusalem	O
,	O
and	O
research	O
gifts	O
from	O
the	O
Allen	O
Institute	O
for	O
AI	O
.	O

Received	O
views	O
of	O
REG	O
suggest	O
that	O
the	O
process	O
contains	O
two	O
steps	O
(	O
Reiter	O
and	O
Dale	O
,	O
2000	O
)	O
:	O
Step	O
1	O
decides	O
what	O
general	O
syntactic	O
type	O
of	O
RE	O
to	O
use	O
(	O
e.g.	O
,	O
a	O
full	O
description	O
,	O
a	O
PN	O
,	O
a	O
pronoun	O
,	O
or	O
some	O
other	O
type	O
)	O
;	O
once	O
this	O
decision	O
is	O
taken	O
,	O
Step	O
2	O
(	O
discussed	O
in	O
section	O
1	O
above	O
)	O
makes	O
more	O
fine	O
-	O
grained	O
decisions	O
,	O
for	O
example	O
,	O
in	O
case	O
of	O
a	O
full	O
description	O
,	O
this	O
step	O
decides	O
what	O
properties	O
should	O
be	O
expressed	O
in	O
the	O
description	O
.	O
The	O
observations	O
of	O
the	O
previous	O
section	O
make	O
this	O
two	O
-	O
step	O
approach	O
problematic	O
,	O
for	O
example	O
because	O
(	O
in	O
some	O
situations	O
)	O
no	O
PN	O
may	O
be	O
available	O
for	O
a	O
given	O
referent	O
,	O
or	O
because	O
PNs	O
and	O
descriptions	O
must	O
be	O
combined	O
(	O
in	O
other	O
situations	O
)	O
.	O
In	O
what	O
follows	O
,	O
we	O
explore	O
a	O
radical	O
alternative	O
,	O
showing	O
that	O
if	O
a	O
suitable	O
representation	O
scheme	O
is	O
used	O
,	O
it	O
is	O
possible	O
to	O
incorporate	O
all	O
decisions	O
related	O
to	O
PNs	O
within	O
Step	O
2	O
.	O
Suppose	O
each	O
individual	O
in	O
the	O
KB	O
comes	O
not	O
just	O
with	O
a	O
number	O
of	O
descriptive	O
properties	O
but	O
with	O
0	B-DatasetName
or	O
more	O
PNs	O
as	O
well	O
,	O
where	O
a	O
PN	O
is	O
regarded	O
as	O
a	O
property	O
that	O
is	O
true	O
of	O
all	O
individuals	O
who	O
bear	O
this	O
name	O
.	O
-	O
(	O
being	O
named	O
)	O
Joe	O
Klein	O
is	O
a	O
property	O
of	O
all	O
individuals	O
named	O
Joe	O
Klein	O
-	O
(	O
being	O
named	O
)	O
Joe	O
is	O
a	O
property	O
of	O
all	O
those	O
individuals	O
named	O
Joe	O
-	O
(	O
being	O
named	O
)	O
Klein	O
is	O
a	O
property	O
of	O
all	O
those	O
individuals	O
named	O
Klein	O
The	O
idea	O
that	O
a	O
PN	O
can	O
be	O
viewed	O
as	O
a	O
property	O
of	O
its	O
bearer	O
deviates	O
from	O
a	O
long	O
tradition	O
of	O
work	O
in	O
philosophy	O
and	O
logic	O
that	O
regards	O
PNs	O
as	O
rigid	O
designators	O
(	O
Kripke	O
,	O
1980	O
)	O
,	O
yet	O
it	O
enjoys	O
considerable	O
support	O
.	O
(	O
Burge	O
,	O
1973	O
)	O
,	O
for	O
example	O
,	O
observes	O
that	O
PNs	O
can	O
behave	O
like	O
common	O
nouns	O
,	O
as	O
in	O
"	O
There	O
are	O
relatively	O
few	O
Alfreds	O
in	O
P	O
"	O
,	O
and	O
"	O
An	O
Alfred	O
joined	O
the	O
club	O
today	O
"	O
(	O
see	O
(	O
Larson	O
and	O
Segal	O
,	O
1995	O
)	O
and	O
(	O
Elbourne	O
,	O
2005	O
)	O
for	O
further	O
support	O
)	O
.	O
A	O
simple	O
KB	O
containing	O
PNs	O
as	O
well	O
as	O
ordinary	O
properties	O
could	O
look	O
like	O
this	O
:	O
JOB	O
:	O
political	O
commentator	O
,	O
commentator	O
NATIONALITY	O
:	O
American	O
NAMES	O
:	O
Mr	O
Joe	O
Klein	O
,	O
Joe	O
Klein	O
,	O
Joe	O
,	O
Klein	O
Because	O
longer	O
versions	O
of	O
a	O
person	O
's	O
name	O
are	O
applicable	O
to	O
only	O
some	O
of	O
the	O
individuals	O
to	O
whom	O
a	O
shorter	O
version	O
is	O
applicable	O
,	O
the	O
values	O
of	O
the	O
NAMES	O
attribute	O
often	O
subsume	B-DatasetName
each	O
other	O
:	O
all	O
people	O
who	O
are	O
called	O
Mr	O
Joe	O
Klein	O
are	O
also	O
called	O
Joe	O
Klein	O
,	O
and	O
so	O
on	O
.	O
These	O
properties	O
can	O
be	O
dealt	O
with	O
using	O
the	O
mechanism	O
for	O
subsumption	O
in	O
the	O
Incremental	O
Algorithm	O
(	O
which	O
would	O
also	O
state	O
that	O
all	O
political	O
commentators	O
are	O
commentators	O
,	O
for	O
instance	O
)	O
(	O
Dale	O
and	O
Reiter	O
,	O
1996	O
)	O
.	O
Of	O
course	O
if	O
Joe	O
Klein	O
is	O
the	O
only	O
Joe	O
in	O
the	O
room	O
,	O
we	O
can	O
refer	O
unambiguously	O
to	O
him	O
saying	O
"	O
Joe	O
"	O
.	O
This	O
is	O
accounted	O
for	O
by	O
making	O
the	O
REG	O
algorithm	O
that	O
operates	O
on	O
the	O
KB	O
above	O
salience	O
aware	O
in	O
one	O
of	O
the	O
standard	O
ways	O
,	O
e.g.	O
,	O
(	O
Krahmer	O
and	O
Theune	O
,	O
2002	O
)	O
.	O
Salience	O
also	O
suggests	O
a	O
way	O
in	O
which	O
REG	O
can	O
extend	O
beyond	O
one	O
-	O
shot	O
REs	O
to	O
cover	O
reference	O
in	O
extended	O
discourse	O
or	O
dialogue	O
:	O
if	O
x	O
is	O
introduced	O
by	O
means	O
of	O
the	O
PN	O
"	O
Joe	O
Klein	O
"	O
in	O
a	O
text	O
,	O
then	O
if	O
x	O
is	O
the	O
only	O
Joe	O
so	O
far	O
mentioned	O
,	O
then	O
this	O
makes	O
x	O
the	O
most	O
salient	O
of	O
all	O
Joe	O
's	O
,	O
licencing	O
the	O
short	O
RE	O
"	O
Joe	O
"	O
.	O
In	O
short	O
:	O
-	O
Each	O
object	O
has	O
an	O
attribute	O
NAMES	O
.	O
-	O
The	O
set	O
of	O
values	O
of	O
NAMES	O
can	O
be	O
empty	O
(	O
no	O
name	O
is	O
available	O
)	O
,	O
singleton	O
(	O
one	O
name	O
)	O
,	O
or	O
neither	O
(	O
several	O
names	O
)	O
.	O
-	O
A	O
subsumption	O
(	O
i.e.	O
,	O
subset	O
)	O
relation	O
can	O
be	O
defined	O
among	O
these	O
values	O
.	O
-	O
Different	O
objects	O
can	O
share	O
some	O
or	O
all	O
of	O
their	O
names	O
.	O
If	O
names	O
are	O
the	O
"	O
canonical	O
"	O
way	O
of	O
referring	O
to	O
an	O
entity	O
,	O
then	O
standard	O
mechanisms	O
could	O
be	O
invoked	O
to	O
favour	O
names	O
at	O
the	O
expense	O
of	O
other	O
properties	O
.	O
One	O
option	O
is	O
to	O
Dale	O
and	O
Reiter	O
's	O
Preference	O
Order	O
(	O
Dale	O
and	O
Reiter	O
,	O
1996	O
)	O
,	O
making	O
NAMES	O
the	O
most	O
highly	O
preferred	O
attribute	O
in	O
an	O
Incremental	O
Algorithm	O
.	O
Alternatively	O
,	O
a	O
new	O
type	O
of	O
brevity	O
-	O
based	O
algorithm	O
might	O
be	O
used	O
that	O
generates	O
the	O
RE	O
that	O
contains	O
the	O
smallest	O
number	O
of	O
syllables	O
.	O
3	O
Assuming	O
that	O
PNs	O
are	O
brief	O
(	O
as	O
they	O
often	O
are	O
)	O
,	O
this	O
type	O
of	O
approach	O
would	O
favour	O
PNs	O
,	O
and	O
it	O
would	O
favour	O
shorter	O
PNs	O
over	O
longer	O
ones	O
(	O
e.g.	O
,	O
"	O
Klein	O
"	O
over	O
"	O
Joe	O
Klein	O
"	O
)	O
.	O
It	O
would	O
also	O
predict	O
that	O
PNs	O
are	O
avoided	O
where	O
large	O
sets	O
are	O
enumerated	O
(	O
compare	O
the	O
RE	O
"	O
the	O
citizens	O
of	O
China	O
"	O
with	O
an	O
enumeration	O
of	O
all	O
the	O
elements	O
of	O
this	O
set	O
)	O
.	O
To	O
see	O
how	O
REG	O
could	O
work	O
in	O
an	O
Incremental	O
Algorithm	O
,	O
consider	O
a	O
simple	O
KB	O
,	O
where	O
each	O
individual	O
has	O
1	O
name	O
:	O
TYPE	O
:	O
woman	O
{	O
w	O
1	O
,	O
w	O
2	O
,	O
w	O
3	O
}	O
,	O
man	O
{	O
m1	O
}	O
,	O
dog	O
{	O
d	O
1	O
,	O
d	O
2	O
}	O
NAMES	O
:	O
mary	O
{	O
w	O
1	O
}	O
,	O
shona	O
{	O
w	O
2	O
,	O
w	O
3	O
}	O
,	O
rover	O
{	O
d	O
1	O
}	O
,	O
max	O
{	O
m	O
1	O
,	O
d	O
2	O
}	O
ACTION	O
:	O
feed	O
{	O
(	O
w	O
1	O
,	O
d	O
1	O
)	O
,	O
(	O
w	O
2	O
,	O
d	O
2	O
)	O
,	O
(	O
w	O
2	O
,	O
d	O
1	O
)	O
}	O
AFFECTION	O
:	O
love	O
{	O
(	O
w	O
1	O
,	O
d	O
1	O
)	O
,	O
(	O
w	O
3	O
,	O
d	O
1	O
)	O
}	O
This	O
approach	O
generates	O
REs	O
such	O
as	O
:	O
d	O
1	O
:	O
"	O
Rover	O
"	O
d	O
2	O
:	O
"	O
The	O
dog	O
called	O
Max	O
"	O
w	O
3	O
:	O
"	O
Shona	O
,	O
who	O
loves	O
a	O
dog	O
"	O
With	O
the	O
above	O
representation	O
scheme	O
in	O
place	O
,	O
classic	O
REG	O
algorithms	O
can	O
be	O
applied	O
without	O
modifications	O
.	O
However	O
,	O
the	O
scheme	O
does	O
not	O
allow	O
PNs	O
to	O
have	O
properties	O
(	O
e.g.	O
,	O
"	O
is	O
a	O
posh	O
name	O
"	O
,	O
"	O
has	O
5	O
characters	O
"	O
,	O
"	O
is	O
common	O
in	O
Scotland	O
"	O
)	O
.	O
If	O
names	O
are	O
reified	O
,	O
then	O
this	O
becomes	O
possible	O
;	O
what	O
's	O
more	O
,	O
PNs	O
themselves	O
could	O
be	O
referred	O
to	O
(	O
e.g.	O
,	O
"	O
the	O
name	O
his	O
friends	O
call	O
him	O
"	O
)	O
:	O
a	O
name	O
is	O
just	O
another	O
object	O
linked	O
(	O
on	O
the	O
one	O
hand	O
)	O
to	O
the	O
things	O
it	O
names	O
and	O
(	O
on	O
the	O
other	O
hand	O
)	O
to	O
the	O
ways	O
in	O
which	O
it	O
manifests	O
itself	O
in	O
spelling	O
,	O
pronunciation	O
,	O
etc	O
.	O
For	O
example	O
,	O
n	O
2	O
may	O
name	O
both	O
a	O
man	O
and	O
a	O
dog	O
,	O
and	O
it	O
may	O
be	O
written	O
as	O
"	O
Max	O
"	O
:	O
Type	O
:	O
woman	O
{	O
w	O
1	O
,	O
w	O
2	O
,	O
w	O
3	O
}	O
,	O
man	O
{	O
m	O
1	O
}	O
,	O
dog	O
{	O
d	O
1	O
,	O
d	O
2	O
}	O
,	O
name	O
{	O
n	O
1	O
,	O
n	O
2	O
,	O
n	O
3	O
,	O
n	O
4	O
}	O
Action	O
:	O
feed	O
{	O
(	O
w	O
1	O
,	O
d	O
1	O
)	O
,	O
(	O
w	O
2	O
,	O
d	O
2	O
)	O
,	O
(	O
w	O
2	O
,	O
d	O
1	O
)	O
}	O
Affection	O
:	O
love	O
{	O
(	O
w	O
1	O
,	O
d	O
1	O
)	O
,	O
(	O
w	O
3	O
,	O
d	O
1	O
)	O
}	O
Naming	O
:	O
name	O
{	O
(	O
d	O
1	O
,	O
n	O
1	O
)	O
,	O
(	O
d	O
2	O
,	O
n	O
2	O
)	O
,	O
(	O
w	O
1	O
,	O
n	O
3	O
)	O
,	O
(	O
w	O
2	O
,	O
n	O
4	O
)	O
,	O
(	O
w	O
3	O
,	O
n	O
4	O
)	O
,	O
(	O
m	O
1	O
,	O
n	O
2	O
)	O
}	O
Spelling	O
:	O
written	O
{	O
(	O
n	O
1	O
,	O
Rover	O
)	O
,	O
(	O
n	O
2	O
,	O
M	O
ax	O
)	O
,	O
(	O
n	O
3	O
,	O
M	O
ary	O
)	O
,	O
(	O
n	O
4	O
,	O
Shona	O
)	O
}	O
Standard	O
REG	O
algorithms	O
can	O
use	O
this	O
KB	O
to	O
generate	O
"	O
The	O
name	O
shared	O
by	O
a	O
man	O
and	O
a	O
dog	O
"	O
(	O
i.e.	O
,	O
"	O
Max	O
"	O
)	O
.	O
If	O
n	O
4	O
is	O
Scottish	O
,	O
we	O
obtain	O
"	O
women	O
with	O
a	O
Scottish	O
name	O
"	O
as	O
well	O
.	O
A	O
slight	O
drawback	O
of	O
this	O
approach	O
,	O
which	O
treats	O
names	O
as	O
objects	O
,	O
is	O
that	O
subsumption	O
can	O
no	O
longer	O
be	O
used	O
to	O
compare	O
names	O
.	O

BRIO	O
:	O
Bringing	O
Order	O
to	O
Abstractive	O
Summarization	B-TaskName

Abstractive	O
summarization	B-TaskName
models	O
are	O
commonly	O
trained	O
using	O
maximum	O
likelihood	O
estimation	O
,	O
which	O
assumes	O
a	O
deterministic	O
(	O
onepoint	O
)	O
target	O
distribution	O
in	O
which	O
an	O
ideal	O
model	O
will	O
assign	O
all	O
the	O
probability	O
mass	O
to	O
the	O
reference	O
summary	O
.	O
This	O
assumption	O
may	O
lead	O
to	O
performance	O
degradation	O
during	O
inference	O
,	O
where	O
the	O
model	O
needs	O
to	O
compare	O
several	O
system	O
-	O
generated	O
(	O
candidate	O
)	O
summaries	O
that	O
have	O
deviated	O
from	O
the	O
reference	O
summary	O
.	O
To	O
address	O
this	O
problem	O
,	O
we	O
propose	O
a	O
novel	O
training	O
paradigm	O
which	O
assumes	O
a	O
non	O
-	O
deterministic	O
distribution	O
so	O
that	O
different	O
candidate	O
summaries	O
are	O
assigned	O
probability	O
mass	O
according	O
to	O
their	O
quality	O
.	O
Our	O
method	O
achieves	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
result	O
on	O
the	O
CNN	O
/	O
DailyMail	O
(	O
47.78	O
ROUGE	O
-	O
1	O
)	O
and	O
XSum	B-DatasetName
(	O
49.07	O
ROUGE	O
-	O
1	O
)	O
datasets	O
.	O
Further	O
analysis	O
also	O
shows	O
that	O
our	O
model	O
can	O
estimate	O
probabilities	O
of	O
candidate	O
summaries	O
that	O
are	O
more	O
correlated	O
with	O
their	O
level	O
of	O
quality	O
.	O
1	O

The	O
results	O
are	O
shown	O
in	O
Tab	O
2	O
.	O
For	O
CNNDM	O
and	O
NYT	O
we	O
use	O
BART	B-MethodName
as	O
the	O
backbone	O
model	O
while	O
for	O
XSum	B-DatasetName
we	O
use	O
the	O
pre	O
-	O
trained	O
PEGASUS	O
model	O
as	O
our	O
base	O
model	O
since	O
it	O
achieves	O
better	O
performance	O
than	O
BART	B-MethodName
.	O
We	O
have	O
the	O
following	O
observations	O
:	O
(	O
1	O
)	O
BRIO	O
-	O
Ctr	O
outperforms	O
SimCLS	O
,	O
its	O
counterpart	O
as	O
an	O
evaluation	O
model	O
in	O
a	O
two	O
-	O
stage	O
summarization	B-TaskName
framework	O
.	O
Specifically	O
,	O
both	O
BRIO	O
-	O
Ctr	O
and	O
SimCLS	O
are	O
used	O
to	O
score	O
the	O
candidate	O
summaries	O
generated	O
by	O
a	O
Seq2Seq	B-MethodName
abstractive	O
model	O
(	O
BART	B-MethodName
)	O
.	O
The	O
final	O
outputs	O
are	O
selected	O
based	O
on	O
those	O
scores	O
.	O
We	O
attribute	O
BRIO	O
-	O
Ctr	O
's	O
superior	O
performance	O
to	O
its	O
use	O
of	O
the	O
same	O
model	O
architecture	O
(	O
BART	B-MethodName
)	O
for	O
both	O
candidate	O
generation	O
and	O
scoring	O
,	O
while	O
SimCLS	O
uses	O
RoBERTa	B-MethodName
as	O
the	O
evaluation	O
model	O
.	O
As	O
a	O
result	O
,	O
BRIO	O
-	O
Ctr	O
maximizes	O
the	O
parameter	O
sharing	O
between	O
the	O
two	O
stages	O
,	O
and	O
preserves	O
the	O
power	O
of	O
the	O
Seq2Seq	B-MethodName
model	O
pre	O
-	O
trained	O
on	O
the	O
same	O
dataset	O
.	O
(	O
2	O
)	O
BRIO	O
-	O
Mul	O
is	O
able	O
to	O
establish	O
the	O
new	O
stare	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
CNNDM	O
.	O
Notably	O
,	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
model	O
,	O
GSum	O
,	O
takes	O
additional	O
guidance	O
as	O
input	O
and	O
needs	O
a	O
separate	O
encoder	O
to	O
encode	O
the	O
guidance	O
information	O
,	O
while	O
BRIO	O
-	O
Mul	O
uses	O
the	O
same	O
parameterization	O
of	O
BART	B-MethodName
.	O
Compared	O
to	O
other	O
methods	O
(	O
ConSum	O
,	O
SeqCo	O
,	O
GOLD	O
)	O
that	O
aim	O
to	O
improve	O
upon	O
BART	B-MethodName
,	O
BRIO	O
-	O
Mul	O
performs	O
much	O
better	O
,	O
showing	O
the	O
effectiveness	O
of	O
our	O
training	O
method	O
.	O
(	O
3	O
)	O
Since	O
on	O
XSum	B-DatasetName
we	O
use	O
PEGASUS	O
instead	O
of	O
BART	B-MethodName
as	O
the	O
base	O
model	O
,	O
the	O
result	O
shows	O
that	O
our	O
method	O
is	O
not	O
restricted	O
to	O
the	O
specific	O
choice	O
of	O
the	O
base	O
model	O
.	O

The	O
training	O
paradigm	O
proposed	O
in	O
this	O
paper	O
may	O
be	O
extended	O
to	O
any	O
Seq2Seq	B-MethodName
model	O
.	O
However	O
,	O
it	O
can	O
be	O
a	O
non	O
-	O
trivial	O
overhead	O
to	O
generate	O
the	O
candidate	O
summaries	O
using	O
large	O
neural	O
models	O
on	O
the	O
entire	O
training	O
set	O
.	O
On	O
the	O
other	O
hand	O
,	O
recent	O
work	O
(	O
Raffel	O
et	O
al	O
,	O
2020	O
;	O
Schick	O
and	O
Schütze	O
,	O
System	O
Summary	O
Reference	O
chelsea	O
forward	O
tammy	O
abraham	O
nets	O
first	O
-	O
half	O
double	O
for	O
chelsea	O
.	O
dominic	O
solanke	O
adds	O
a	O
third	O
late	O
on	O
as	O
chelsea	O
look	O
set	O
to	O
win	O
trophy	O
.	O
manchester	O
city	O
struggle	O
without	O
injured	O
star	O
thierry	O
ambrose	O
.	O
read	O
:	O
mourinho	O
warns	O
his	O
young	O
chelsea	O
players	O
he	O
can	O
not	O
play	O
them	O
all	O
.	O
click	O
here	O
to	O
read	O
our	O
match	O
report	O
from	O
man	O
city	O
's	O
academy	O
stadium	O
.	O
BART	B-MethodName
tammy	O
abraham	O
scored	O
twice	O
in	O
the	O
first	O
half	O
to	O
give	O
chelsea	O
the	O
lead	O
.	O
isaac	O
buckley	O
-	O
ricketts	O
levelled	O
the	O
game	O
for	O
manchester	O
city	O
.	O
dominic	O
solanke	O
scored	O
late	O
on	O
to	O
put	O
a	O
gloss	O
on	O
the	O
scoreline	O
.	O
click	O
here	O
to	O
read	O
sportsmail	O
's	O
player	O
ratings	O
from	O
the	O
youth	O
cup	O
final	O
.	O
BRIO	O
-	O
Mul	O
chelsea	O
beat	O
manchester	O
city	O
3	O
-	O
1	O
in	O
the	O
youth	O
cup	O
final	O
at	O
the	O
etihad	O
stadium	O
.	O
tammy	O
abraham	O
scored	O
twice	O
in	O
the	O
first	O
half	O
to	O
give	O
chelsea	O
the	O
lead	O
.	O
dominic	O
solanke	O
scored	O
late	O
on	O
to	O
seal	O
the	O
win	O
for	O
the	O
home	O
side	O
.	O
Reference	O
alejandro	O
valverde	O
won	O
ahead	O
of	O
julian	O
alaphilippe	O
and	O
michael	O
albasini	O
.	O
chris	O
froome	O
finished	O
123rd	O
after	O
a	O
crash	O
during	O
the	O
final	O
12	O
kilometres	O
.	O
team	O
sky	O
's	O
sports	O
director	O
gabriel	O
rasch	O
praised	O
froome	O
for	O
finishing	O
.	O
rasch	O
said	O
froome	O
was	O
'	O
banged	O
up	O
'	O
but	O
expects	O
to	O
ride	O
tour	O
de	O
romandie	O
.	O
BART	B-MethodName
movistar	O
rider	O
alejandro	O
valverde	O
won	O
fleche	O
wallonne	O
on	O
wednesday	O
.	O
team	O
sky	O
's	O
chris	O
froome	O
fell	O
in	O
the	O
final	O
12	O
km	O
but	O
finished	O
the	O
race	O
.	O
philippe	O
gilbert	O
pulled	O
out	O
of	O
the	O
race	O
after	O
a	O
bad	O
crash	O
50	O
km	O
from	O
the	O
end	O
.	O
click	O
here	O
for	O
more	O
cycling	O
news	O
.	O
BRIO	O
-	O
Mul	O
alejandro	O
valverde	O
defended	O
his	O
fleche	O
wallonne	O
title	O
in	O
belgium	O
on	O
wednesday	O
.	O
movistar	O
rider	O
finished	O
ahead	O
of	O
julian	O
alaphilippe	O
and	O
michael	O
albasini	O
.	O
team	O
sky	O
's	O
chris	O
froome	O
fell	O
in	O
the	O
final	O
12	O
km	O
of	O
the	O
race	O
but	O
finished	O
in	O
123rd	O
.	O
froome	O
was	O
involved	O
in	O
a	O
crash	O
but	O
finished	O
the	O
race	O
despite	O
being	O
'	O
banged	O
up	O
'	O
Reference	O
manuel	O
pellegrini	O
won	O
the	O
premier	O
league	O
and	O
capital	O
one	O
cup	O
last	O
season	O
.	O
city	O
currently	O
sit	O
fourth	O
in	O
the	O
league	O
table	O
-	O
12	O
points	O
behind	O
chelsea	O
.	O
pellegrini	O
's	O
contract	O
expires	O
at	O
the	O
end	O
of	O
the	O
2015	O
-	O
16	O
season	O
.	O
city	O
players	O
have	O
been	O
impressed	O
with	O
vieira	O
's	O
work	O
with	O
the	O
youth	O
team	O
.	O
pep	O
guardiola	O
is	O
city	O
's	O
first	O
-	O
choice	O
to	O
succeed	O
pellegrini	O
at	O
the	O
etihad	O
.	O
BART	B-MethodName
manuel	O
pellegrini	O
's	O
future	O
at	O
manchester	O
city	O
is	O
under	O
scrutiny	O
.	O
patrick	O
vieira	O
is	O
highly	O
-	O
respected	O
among	O
the	O
city	O
players	O
.	O
city	O
's	O
first	O
-	O
choice	O
managerial	O
option	O
is	O
bayern	O
munich	O
boss	O
pep	O
guardiola	O
.	O
click	O
here	O
for	O
all	O
the	O
latest	O
manchester	O
city	O
news	O
.	O
click	O
here	O
for	O
more	O
premier	O
league	O
news	O
.	O
BRIO	O
-	O
Mul	O
manchester	O
city	O
players	O
have	O
backed	O
patrick	O
vieira	O
to	O
replace	O
manuel	O
pellegrini	O
as	O
manager	O
of	O
the	O
club	O
.	O
the	O
frenchman	O
is	O
highly	O
-	O
respected	O
among	O
the	O
players	O
at	O
the	O
etihad	O
stadium	O
.	O
pellegrini	O
's	O
future	O
at	O
the	O
club	O
is	O
under	O
scrutiny	O
after	O
a	O
disappointing	O
season	O
.	O
city	O
's	O
first	O
-	O
choice	O
manager	O
is	O
current	O
bayern	O
munich	O
boss	O
pep	O
guardiola	O
.	O

Tab	O
.	O
10	O
presents	O
an	O
interesting	O
pattern	O
we	O
observed	O
when	O
comparing	O
the	O
results	O
of	O
BRIO	O
-	O
Mul	O
and	O
BART	B-MethodName
,	O
which	O
demonstrates	O
that	O
our	O
method	O
helps	O
the	O
abstractive	O
model	O
to	O
filter	O
out	O
noise	O
patterns	O
in	O
the	O
original	O
data	O
.	O
Specifically	O
,	O
some	O
of	O
the	O
reference	O
summaries	O
(	O
331/11490	O
)	O
in	O
CNNDM	O
contains	O
the	O
phrase	O
"	O
click	O
here	O
"	O
,	O
pointing	O
to	O
a	O
hyperlink	O
,	O
and	O
103	O
source	O
documents	O
also	O
contain	O
this	O
phrase	O
.	O
BART	B-MethodName
picked	O
up	O
this	O
pattern	O
,	O
and	O
generates	O
this	O
phrase	O
in	O
96	O
output	O
summaries	O
.	O
On	O
the	O
contrary	O
,	O
our	O
model	O
learns	O
to	O
ignore	O
this	O
noise	O
pattern	O
and	O
never	O
generated	O
it	O
across	O
the	O
whole	O
test	O
set	O
,	O
likely	O
because	O
it	O
identified	O
that	O
generated	O
candidates	O
with	O
this	O
pattern	O
rarely	O
achieve	O
a	O
high	O
ROUGE	O
score	O
,	O
and	O
downweighted	O
the	O
probability	O
accordingly	O
.	O

In	O
this	O
work	O
,	O
we	O
presented	O
a	O
new	O
training	O
paradigm	O
that	O
assigns	O
candidate	O
outputs	O
probability	O
mass	O
according	O
to	O
their	O
quality	O
using	O
contrastive	B-MethodName
learning	I-MethodName
.	O
While	O
our	O
method	O
has	O
achieved	O
significant	O
improvement	O
on	O
abstractive	O
summarization	B-TaskName
,	O
we	O
note	O
several	O
directions	O
for	O
the	O
future	O
work	O
to	O
explore	O
.	O
First	O
,	O
since	O
our	O
method	O
makes	O
no	O
assumptions	O
specifically	O
about	O
the	O
summarization	B-TaskName
task	O
,	O
it	O
can	O
be	O
extended	O
to	O
other	O
conditional	B-TaskName
text	I-TaskName
generation	I-TaskName
tasks	O
such	O
as	O
machine	B-TaskName
translation	I-TaskName
.	O
Second	O
,	O
it	O
is	O
possible	O
to	O
apply	O
our	O
method	O
in	O
a	O
reinforcement	O
learning	O
setting	O
,	O
where	O
the	O
candidate	O
summaries	O
are	O
dynamically	O
generated	O
.	O
Finally	O
,	O
in	O
experiments	O
we	O
only	O
used	O
diverse	O
beam	O
search	O
to	O
generate	O
the	O
candidate	O
summaries	O
,	O
but	O
it	O
is	O
likely	O
that	O
other	O
candidate	O
generation	O
methods	O
could	O
yield	O
further	O
improvements	O
.	O

Extremely	O
Small	O
BERT	B-MethodName
Models	O
from	O
Mixed	O
-	O
Vocabulary	O
Training	O

Pretrained	B-TaskName
language	I-TaskName
models	I-TaskName
like	O
BERT	B-MethodName
have	O
achieved	O
good	O
results	O
on	O
NLP	O
tasks	O
,	O
but	O
are	O
impractical	O
on	O
resource	O
-	O
limited	O
devices	O
due	O
to	O
memory	O
footprint	O
.	O
A	O
large	O
fraction	O
of	O
this	O
footprint	O
comes	O
from	O
the	O
input	O
embeddings	O
with	O
large	O
input	O
vocabulary	O
and	O
embedding	O
dimensions	O
.	O
Existing	O
knowledge	B-MethodName
distillation	I-MethodName
methods	O
used	O
for	O
model	B-TaskName
compression	I-TaskName
can	O
not	O
be	O
directly	O
applied	O
to	O
train	O
student	O
models	O
with	O
reduced	O
vocabulary	O
sizes	O
.	O
To	O
this	O
end	O
,	O
we	O
propose	O
a	O
distillation	O
method	O
to	O
align	O
the	O
teacher	O
and	O
student	O
embeddings	O
via	O
mixed	O
-	O
vocabulary	O
training	O
.	O
Our	O
method	O
compresses	O
BERT	B-MethodName
LARGE	O
to	O
a	O
task	O
-	O
agnostic	O
model	O
with	O
smaller	O
vocabulary	O
and	O
hidden	O
dimensions	O
,	O
which	O
is	O
an	O
order	O
of	O
magnitude	O
smaller	O
than	O
other	O
distilled	O
BERT	B-MethodName
models	O
and	O
offers	O
a	O
better	O
size	O
-	O
accuracy	B-MetricName
trade	O
-	O
off	O
on	O
language	O
understanding	O
benchmarks	O
as	O
well	O
as	O
a	O
practical	O
dialogue	O
task	O
.	O

Recently	O
,	O
pre	O
-	O
trained	O
context	O
-	O
aware	O
language	O
models	O
like	O
ELMo	B-MethodName
(	O
Peters	O
et	O
al	O
,	O
2018	O
)	O
,	O
GPT	B-MethodName
(	O
Radford	O
et	O
al	O
,	O
2019	O
)	O
,	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
and	O
XLNet	B-MethodName
(	O
Yang	O
et	O
al	O
,	O
2019	O
)	O
have	O
outperformed	O
traditional	O
word	O
embedding	O
models	O
like	O
Word2Vec	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
and	O
GloVe	B-MethodName
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
,	O
and	O
achieved	O
strong	O
results	O
on	O
a	O
number	O
of	O
language	O
understanding	O
tasks	O
.	O
However	O
,	O
these	O
models	O
are	O
typically	O
too	O
huge	O
to	O
host	O
on	O
mobile	O
/	O
edge	O
devices	O
,	O
especially	O
for	O
real	O
-	O
time	O
inference	O
.	O
Recent	O
work	O
has	O
explored	O
,	O
inter	O
alia	O
,	O
knowledge	B-MethodName
distillation	I-MethodName
(	O
Ba	O
and	O
Caruana	O
,	O
2014	O
;	O
Hinton	O
et	O
al	O
,	O
2015	O
)	O
to	O
train	O
small	O
-	O
footprint	O
student	O
models	O
by	O
implicit	O
transfer	O
of	O
knowledge	O
from	O
a	O
teacher	O
model	O
.	O
Most	O
distillation	O
methods	O
,	O
however	O
,	O
need	O
the	O
student	O
and	O
teacher	O
output	O
spaces	O
to	O
be	O
aligned	O
.	O
This	O
complicates	O
task	O
-	O
agnostic	O
distillation	O
of	O
BERT	B-MethodName
to	O
Asterisk	O
(	O
*	O
)	O
denotes	O
equal	O
contribution	O
.	O
Research	O
conducted	O
when	O
all	O
authors	O
were	O
at	O
Google	B-DatasetName
.	O
smaller	O
-	O
vocabulary	O
student	O
BERT	B-MethodName
models	O
since	O
the	O
input	O
vocabulary	O
is	O
also	O
the	O
output	O
space	O
for	O
the	O
masked	B-TaskName
language	I-TaskName
modeling	I-TaskName
(	O
MLM	B-DatasetName
)	O
task	O
used	O
in	O
BERT	B-MethodName
.	O
This	O
in	O
turn	O
limits	O
these	O
distillation	O
methods	O
'	O
ability	O
to	O
compress	O
the	O
input	O
embedding	O
matrix	O
,	O
that	O
makes	O
up	O
a	O
major	O
proportion	O
of	O
model	O
parameters	O
e.g.	O
the	O
∼30	O
K	O
input	O
WordPiece	B-MethodName
embeddings	O
of	O
the	O
BERT	B-MethodName
BASE	B-MethodName
model	O
make	O
up	O
over	O
21	O
%	O
of	O
the	O
model	O
size	O
.	O
This	O
proportion	O
is	O
even	O
higher	O
for	O
most	O
distilled	O
BERT	B-MethodName
models	O
,	O
owing	O
to	O
these	O
distilled	O
models	O
typically	O
having	O
fewer	O
layers	O
than	O
their	O
teacher	O
BERT	B-MethodName
counterparts	O
.	O
We	O
present	O
a	O
task	O
and	O
model	O
-	O
agnostic	O
distillation	O
approach	O
for	O
training	O
small	O
,	O
reduced	O
-	O
vocabulary	O
BERT	B-MethodName
models	O
running	O
into	O
a	O
few	O
megabytes	O
.	O
In	O
our	O
setup	O
,	O
the	O
teacher	O
and	O
student	O
models	O
have	O
incompatible	O
vocabularies	O
and	O
tokenizations	O
for	O
the	O
same	O
sequence	O
.	O
We	O
therefore	O
align	O
the	O
student	O
and	O
teacher	O
WordPiece	B-MethodName
embeddings	O
by	O
training	O
the	O
teacher	O
on	O
the	O
MLM	B-DatasetName
task	O
with	O
a	O
mix	O
of	O
teacher	O
-	O
tokenized	O
and	O
student	O
-	O
tokenized	O
words	O
in	O
a	O
sequence	O
,	O
and	O
then	O
using	O
these	O
student	O
embeddings	O
to	O
train	O
smaller	O
student	O
models	O
.	O
Using	O
our	O
method	O
,	O
we	O
train	O
compact	O
6	O
and	O
12	O
-	O
layer	O
reducedvocabulary	O
student	O
models	O
which	O
achieve	O
competitive	O
performance	O
in	O
addition	O
to	O
high	O
compression	O
for	O
benchmark	O
datasets	O
as	O
well	O
as	O
a	O
real	O
-	O
world	O
application	O
in	O
language	O
understanding	O
for	O
dialogue	O
.	O

Work	O
in	O
NLP	O
model	B-TaskName
compression	I-TaskName
falls	O
broadly	O
into	O
four	O
classes	O
:	O
matrix	O
approximation	O
,	O
weight	O
quantization	B-TaskName
,	O
pruning	O
/	O
sharing	O
,	O
and	O
knowledge	B-MethodName
distillation	I-MethodName
.	O
The	O
former	O
two	O
seek	O
to	O
map	O
model	O
parameters	O
to	O
low	O
-	O
rank	O
approximations	O
(	O
Tulloch	O
and	O
Jia	O
,	O
2017	O
)	O
and	O
lower	O
-	O
precision	O
integers	O
/	O
floats	O
(	O
Chen	O
et	O
al	O
,	O
2015	O
;	O
Zhou	O
et	O
al	O
,	O
2018	O
;	O
Shen	O
et	O
al	O
,	O
2019	O
)	O
respectively	O
.	O
In	O
contrast	O
,	O
pruning	O
aims	O
to	O
remove	O
/	O
share	O
redundant	O
model	O
weights	O
(	O
Li	O
et	O
al	O
,	O
2016	O
;	O
Lan	O
et	O
al	O
,	O
2019	O
)	O
.	O
More	O
recently	O
,	O
dropout	O
(	O
Srivastava	O
et	O
al	O
,	O
2014	O
)	O
has	O
been	O
used	O
to	O
cut	O
inference	O
latency	O
by	O
early	O
exit	O
(	O
Fan	O
et	O
al	O
,	O
2019	O
;	O
Xin	O
et	O
al	O
,	O
2020	O
)	O
.	O
Knowledge	B-MethodName
distillation	I-MethodName
focuses	O
on	O
implicit	O
transfer	O
of	O
knowledge	O
as	O
soft	O
teacher	O
predictions	O
(	O
Tang	O
et	O
al	O
,	O
2019	O
)	O
,	O
attention	O
distributions	O
(	O
Zagoruyko	O
and	O
Komodakis	O
,	O
2016	O
)	O
and	O
intermediate	O
outputs	O
(	O
Romero	O
et	O
al	O
,	O
2014	O
)	O
.	O
Approaches	O
close	O
to	O
our	O
work	O
rely	O
on	O
similar	O
methods	O
(	O
Sanh	O
et	O
al	O
,	O
2019	O
;	O
Sun	O
et	O
al	O
,	O
2019	O
)	O
,	O
while	O
others	O
involve	O
combinations	O
of	O
layer	O
-	O
wise	O
transfer	O
(	O
Sun	O
et	O
al	O
,	O
2020	O
)	O
,	O
taskspecific	O
distillation	O
(	O
Jiao	O
et	O
al	O
,	O
2019	O
)	O
,	O
architecture	O
search	O
(	O
Chen	O
et	O
al	O
,	O
2020	O
)	O
and	O
layer	O
dropout	O
(	O
Xu	O
et	O
al	O
,	O
2020	O
)	O
;	O
many	O
of	O
these	O
are	O
specific	O
to	O
the	O
transformer	O
layer	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
Another	O
highly	O
relevant	O
line	O
of	O
work	O
focuses	O
on	O
reducing	O
the	O
size	O
of	O
the	O
embedding	O
matrix	O
,	O
either	O
via	O
factorization	O
(	O
Shu	O
and	O
Nakayama	O
,	O
2018	O
;	O
Lan	O
et	O
al	O
,	O
2019	O
)	O
or	O
vocabulary	O
selection	O
/	O
pruning	O
(	O
Provilkov	O
et	O
al	O
,	O
2019	O
;	O
Chen	O
et	O
al	O
,	O
2019b	O
)	O
.	O

WordPiece	B-MethodName
(	O
WP	O
)	O
tokens	O
(	O
Wu	O
et	O
al	O
,	O
2016	O
)	O
are	O
subword	O
units	O
obtained	O
by	O
applying	O
greedy	O
segmentation	O
to	O
a	O
training	O
corpus	O
.	O
Given	O
such	O
a	O
corpus	O
and	O
a	O
number	O
of	O
desired	O
tokens	O
D	O
,	O
a	O
WordPiece	B-MethodName
vocabulary	O
is	O
generated	O
by	O
selecting	O
D	O
subword	O
tokens	O
such	O
that	O
the	O
resulting	O
corpus	O
is	O
minimal	O
in	O
the	O
number	O
of	O
WordPiece	B-MethodName
when	O
segmented	O
according	O
to	O
the	O
chosen	O
WordPiece	B-MethodName
model	O
.	O
The	O
greedy	O
algorithm	O
for	O
this	O
optimization	O
problem	O
is	O
described	O
in	O
more	O
detail	O
in	O
Sennrich	O
et	O
al	O
(	O
2016	O
)	O
.	O
Most	O
published	O
BERT	B-MethodName
models	O
use	O
a	O
vocabulary	O
of	O
30522	O
Word	O
-	O
Pieces	O
,	O
obtained	O
by	O
running	O
the	O
above	O
algorithm	O
on	O
the	O
Wikipedia	O
and	O
BooksCorpus	O
(	O
Zhu	O
et	O
al	O
,	O
2015	O
)	O
corpora	O
with	O
a	O
desired	O
vocabulary	O
size	O
D	O
of	O
30000	O
.	O
For	O
our	O
student	O
model	O
,	O
we	O
chose	O
a	O
target	O
vocabulary	O
size	O
D	O
of	O
5000	O
WordPiece	B-MethodName
tokens	O
.	O
Using	O
the	O
same	O
WordPiece	B-MethodName
vocabulary	O
generation	O
algorithm	O
and	O
corpus	O
as	O
above	O
,	O
we	O
obtain	O
a	O
4928	O
-	O
WordPiece	B-MethodName
vocabulary	O
for	O
the	O
student	O
model	O
.	O
This	O
student	O
vocabulary	O
includes	O
all	O
ASCII	O
characters	O
as	O
separate	O
tokens	O
,	O
ensuring	O
no	O
out	O
-	O
of	O
-	O
vocabulary	O
words	O
upon	O
tokenization	O
with	O
this	O
vocabulary	O
.	O
Additionally	O
,	O
the	O
30	O
K	O
teacher	O
BERT	B-MethodName
vocabulary	O
includes	O
93.9	O
%	O
of	O
the	O
WP	O
tokens	O
in	O
this	O
5	O
K	O
student	O
vocabulary	O
but	O
does	O
not	O
subsume	B-DatasetName
it	O
.	O
We	O
explore	O
other	O
strategies	O
to	O
obtain	O
a	O
small	O
student	O
vocabulary	O
in	O
Section	O
6	O
.	O
For	O
task	O
-	O
agnostic	O
student	O
models	O
,	O
we	O
reuse	O
BERT	B-MethodName
's	O
masked	B-TaskName
language	I-TaskName
modeling	I-TaskName
(	O
MLM	B-DatasetName
)	O
task	O
:	O
words	B-DatasetName
in	I-DatasetName
context	I-DatasetName
are	O
randomly	O
masked	O
and	O
predicted	O
given	O
the	O
context	O
via	O
softmax	B-MethodName
over	O
the	O
model	O
's	O
WP	O
vocabulary	O
.	O
Thus	O
,	O
the	O
output	O
spaces	O
for	O
our	O
teacher	O
(	O
30	O
K	O
)	O
and	O
student	O
(	O
5	O
K	O
)	O
models	O
are	O
unaligned	O
.	O
This	O
,	O
coupled	O
with	O
both	O
vocabularies	O
tokenizing	O
the	O
same	O
words	O
differently	O
,	O
means	O
existing	O
distillation	O
methods	O
do	O
not	O
apply	O
to	O
our	O
setting	O
.	O

For	O
evaluation	O
,	O
we	O
finetune	O
the	O
student	O
model	O
just	O
as	O
one	O
would	O
finetune	O
the	O
original	O
BERT	B-MethodName
model	O
i.e.	O
,	O
without	O
using	O
the	O
teacher	O
model	O
or	O
any	O
taskspecific	O
distillation	O
.	O
We	O
describe	O
our	O
experiments	O
below	O
,	O
with	O
dataset	O
details	O
left	O
to	O
the	O
appendix	O
.	O

We	O
fine	O
-	O
tune	O
and	O
evaluate	O
the	O
distilled	O
student	O
models	O
on	O
two	O
classes	O
of	O
language	O
understanding	O
tasks	O
:	O
MNLI	B-DatasetName
:	O
Multi	O
-	O
Genre	O
Natural	B-TaskName
Language	I-TaskName
Inference	I-TaskName
(	O
Williams	O
et	O
al	O
,	O
2018	O
)	O
,	O
a	O
3	O
-	O
way	O
sentence	O
pair	O
classification	O
task	O
with	O
393	O
K	O
training	O
instances	O
.	O
SST	B-DatasetName
-	O
2	O
:	O
Stanford	O
Sentiment	O
Treebank	O
(	O
Socher	O
et	O
al	O
,	O
2013	O
)	O
,	O
a	O
2	O
-	O
way	O
sentence	B-TaskName
classification	I-TaskName
task	O
with	O
67	O
K	O
training	O
instances	O
.	O
Spoken	B-TaskName
Language	I-TaskName
Understanding	I-TaskName
:	O
Since	O
we	O
are	O
also	O
keen	O
on	O
edge	O
device	O
applications	O
,	O
we	O
also	O
evaluate	O
on	O
spoken	B-TaskName
language	I-TaskName
understanding	I-TaskName
,	O
a	O
practical	O
task	O
in	O
dialogue	O
systems	O
.	O
We	O
use	O
the	O
SNIPS	B-DatasetName
dataset	O
(	O
Coucke	O
et	O
al	O
,	O
2018	O
)	O
of	O
∼14	O
K	O
virtual	O
assistant	O
queries	O
,	O
each	O
comprising	O
one	O
of	O
7	O
intents	O
and	O
values	O
for	O
one	O
or	O
more	O
of	O
the	O
39	O
pre	O
-	O
defined	O
slots	O
.	O
The	O
intent	B-TaskName
detection	I-TaskName
and	O
slot	B-TaskName
filling	I-TaskName
subtasks	O
are	O
modeled	O
respectively	O
as	O
7	O
-	O
way	O
sentence	B-TaskName
classification	I-TaskName
and	O
sequence	O
tagging	O
with	O
IOB	O
slot	O
labels	O
.	O

For	O
GLUE	B-DatasetName
,	O
we	O
train	O
student	O
models	O
with	O
6	O
and	O
12	O
layers	O
,	O
4	O
attention	O
heads	O
,	O
and	O
embedding	O
/	O
hidden	O
dimensions	O
fixed	O
to	O
256	O
,	O
each	O
using	O
a	O
compact	O
5	O
K	O
-	O
WP	O
vocabulary	O
.	O
We	O
also	O
evaluate	O
baselines	O
without	O
knowledge	B-MethodName
distillation	I-MethodName
(	O
NoKD	O
)	O
,	O
parameterized	O
identically	O
to	O
the	O
distilled	O
student	O
models	O
(	O
incl	O
.	O
the	O
5	O
K	O
vocabulary	O
)	O
,	O
trained	O
on	O
the	O
MLM	B-DatasetName
teacher	O
objective	O
from	O
scratch	O
.	O
We	O
also	O
compare	O
our	O
models	O
on	O
GLUE	B-DatasetName
with	O
the	O
following	O
approaches	O
:	O
DistilBERT	B-MethodName
(	O
Sanh	O
et	O
al	O
,	O
2019	O
)	O
structures	O
for	O
an	O
optimized	O
student	O
model	O
.	O
For	O
SNIPS	B-DatasetName
,	O
we	O
shift	O
our	O
focus	O
to	O
smaller	O
,	O
lowlatency	O
models	O
for	O
on	O
-	O
device	O
use	O
cases	O
.	O
Here	O
,	O
we	O
train	O
student	O
models	O
with	O
6	O
layers	O
and	O
embedding	O
/	O
hidden	O
dimensions	O
{	O
96	O
,	O
192	O
,	O
256	O
}	O
.	O
The	O
smaller	O
models	O
here	O
may	O
not	O
be	O
competitive	O
on	O
GLUE	B-DatasetName
but	O
are	O
adequate	O
for	O
practical	O
tasks	O
such	O
as	O
spoken	O
LU	O
.	O
We	O
compare	O
with	O
two	O
strong	O
baselines	O
:	O
BERT	B-MethodName
BASE	B-MethodName
(	O
Chen	O
et	O
al	O
,	O
2019a	O
)	O
with	O
intent	O
and	O
IOB	O
slot	O
tags	O
predicted	O
using	O
the	O
[	O
CLS	O
]	O
and	O
the	O
first	O
WP	O
tokens	O
of	O
each	O
word	O
respectively	O
,	O
and	O
StackProp	O
(	O
Qin	O
et	O
al	O
,	O
2019	O
)	O
,	O
which	O
uses	O
a	O
series	O
of	O
smaller	O
recurrent	O
and	O
self	O
-	O
attentive	O
encoders	O
.	O

We	O
propose	O
a	O
novel	O
approach	O
to	O
knowledge	B-MethodName
distillation	I-MethodName
for	O
BERT	B-MethodName
,	O
focusing	O
on	O
using	O
a	O
significantly	O
smaller	O
vocabulary	O
for	O
the	O
student	O
BERT	B-MethodName
models	O
.	O
Our	O
mixed	O
-	O
vocabulary	O
training	O
method	O
encourages	O
implicit	O
alignment	O
of	O
the	O
teacher	O
and	O
student	O
Word	O
-	O
Piece	O
embeddings	O
.	O
Our	O
highly	O
-	O
compressed	O
6	O
and	O
12	O
-	O
layer	O
distilled	O
student	O
models	O
are	O
optimized	O
for	O
on	O
-	O
device	O
use	O
cases	O
and	O
demonstrate	O
competitive	O
performance	O
on	O
both	O
benchmark	O
datasets	O
and	O
practical	O
tasks	O
.	O
Our	O
technique	O
is	O
unique	O
in	O
targeting	O
the	O
student	O
vocabulary	O
size	O
,	O
enabling	O
easy	O
combination	O
with	O
most	O
BERT	B-MethodName
distillation	O
methods	O
.	O

In	O
the	O
last	O
decade	O
,	O
natural	O
language	O
processing	O
and	O
machine	O
learning	O
-	O
in	O
particular	O
deep	O
learning	O
-	O
have	O
come	O
a	O
long	O
way	O
towards	O
building	O
an	O
automated	O
dialogue	O
system	O
.	O
In	O
a	O
fully	O
automated	O
dialogue	O
system	O
,	O
the	O
goal	O
is	O
to	O
predict	O
an	O
appropriate	O
response	O
given	O
the	O
dialogue	O
history	O
.	O
This	O
problem	O
of	O
response	O
prediction	O
can	O
be	O
formulated	O
in	O
two	O
ways	O
.	O
One	O
is	O
purely	O
generative	O
,	O
where	O
the	O
task	O
is	O
to	O
generate	O
a	O
text	O
response	O
,	O
i.e.	O
generating	O
a	O
sentence	O
or	O
utterance	O
from	O
scratch	O
,	O
whereas	O
the	O
other	O
is	O
Next	O
Utterance	O
Selection	O
,	O
where	O
the	O
task	O
is	O
to	O
select	O
an	O
appropriate	O
response	O
from	O
a	O
set	O
of	O
given	O
candidates	O
.	O
Despite	O
significant	O
research	O
in	O
text	B-TaskName
generation	I-TaskName
,	O
a	O
pure	O
generative	O
model	O
capable	O
of	O
generating	O
syntactically	O
and	O
semantically	O
correct	O
text	O
still	O
remains	O
a	O
distant	O
reality	O
.	O
There	O
have	O
been	O
several	O
efforts	O
such	O
as	O
(	O
Vinyals	O
and	O
Le	O
,	O
2015	O
;	O
Serban	O
et	O
al	O
,	O
2016a	O
;	O
Serban	O
et	O
al	O
,	O
2016b	O
;	O
Serban	O
et	O
al	O
,	O
2017b	O
)	O
for	O
the	O
task	O
of	O
dialogue	B-TaskName
generation	I-TaskName
,	O
however	O
these	O
models	O
still	O
do	O
not	O
seem	O
to	O
work	O
in	O
practice	O
(	O
Liu	O
et	O
al	O
,	O
2016	O
)	O
.	O
This	O
is	O
particularly	O
true	O
for	O
open	O
domain	O
dialogue	O
systems	O
.	O
Dialogue	B-TaskName
generation	I-TaskName
in	O
a	O
task	O
-	O
oriented	O
oriented	O
dialogue	O
system	O
,	O
such	O
as	O
flight	O
-	O
booking	O
and	O
troubleshooting	O
,	O
is	O
much	O
easier	O
than	O
in	O
a	O
non	O
-	O
task	O
oriented	O
dialogue	O
system	O
.	O
This	O
level	O
of	O
difficulty	O
arises	O
because	O
a	O
non	O
-	O
task	O
-	O
oriented	O
dialogue	O
system	O
has	O
no	O
predefined	O
goal	O
(	O
or	O
domain	O
)	O
,	O
and	O
the	O
vocabulary	O
and	O
possibilities	O
of	O
the	O
dialogues	O
could	O
be	O
endless	O
.	O
Given	O
these	O
challenges	O
,	O
researchers	O
have	O
defined	O
a	O
simpler	O
problem	O
for	O
conversation	O
modeling	O
based	O
on	O
retrieval	O
,	O
i.e.	O
next	O
utterance	O
selection	O
.	O
In	O
this	O
paper	O
we	O
use	O
this	O
second	O
formulation	O
of	O
the	O
problem	O
,	O
and	O
show	O
that	O
using	O
additional	O
information	O
available	O
in	O
the	O
form	O
of	O
dialogue	O
acts	O
help	O
in	O
improving	O
the	O
performance	O
of	O
the	O
underlying	O
model	O
.	O
Dialogue	O
acts	O
(	O
DA	O
)	O
are	O
higher	O
level	O
semantic	O
abstractions	O
assigned	O
to	O
utterances	O
in	O
a	O
conversation	O
.	O
An	O
example	O
of	O
a	O
dialogue	O
act	O
for	O
an	O
utterance	O
i	O
'll	O
give	O
you	O
a	O
call	O
tonight	O
is	O
Inform	O
since	O
speaker	O
is	O
providing	O
information	O
.	O
In	O
a	O
traditional	O
dialogue	O
system	O
,	O
where	O
dialogues	O
are	O
formulated	O
by	O
first	O
sentence	O
planning	O
and	O
then	O
by	O
surface	O
realization	O
,	O
the	O
first	O
step	O
is	O
to	O
understand	O
the	O
dialogue	O
act	O
of	O
the	O
utterance	O
that	O
needs	O
to	O
be	O
generated	O
,	O
and	O
then	O
plan	O
and	O
realize	O
the	O
dialogue	O
accordingly	O
.	O
To	O
better	O
understand	O
the	O
importance	O
of	O
dialogue	O
acts	O
,	O
consider	O
an	O
example	O
of	O
a	O
simple	O
conversation	O
,	O
where	O
if	O
the	O
previous	O
utterance	O
is	O
of	O
type	O
Question	O
then	O
the	O
next	O
utterance	O
is	O
most	O
likely	O
going	O
to	O
be	O
of	O
the	O
type	O
,	O
i.e.	O
Inform	O
,	O
providing	O
information	O
to	O
that	O
question	O
.	O
Knowing	O
that	O
the	O
next	O
utterance	O
is	O
of	O
type	O
Inform	O
,	O
a	O
conversation	O
system	O
with	O
support	O
of	O
dialogue	O
act	O
information	O
can	O
filter	O
a	O
set	O
of	O
candidate	O
responses	O
,	O
and	O
select	O
the	O
most	O
appropriate	O
one	O
.	O
Driven	O
by	O
this	O
intuition	O
,	O
we	O
hypothesize	O
that	O
understanding	O
dialogue	O
acts	O
and	O
using	O
them	O
in	O
the	O
task	O
of	O
next	O
utterance	O
selection	O
should	O
improve	O
the	O
performance	O
irrespective	O
of	O
the	O
underlying	O
model	O
.	O
Driven	O
by	O
this	O
intuition	O
,	O
we	O
hypothesize	O
that	O
understanding	O
dialogue	O
acts	O
and	O
using	O
them	O
in	O
the	O
task	O
of	O
next	O
utterance	O
selection	O
should	O
improve	O
the	O
performance	O
irrespective	O
of	O
the	O
underlying	O
model	O
.	O
Most	O
of	O
the	O
existing	O
literature	O
for	O
the	O
task	O
of	O
next	O
utterance	O
selection	O
can	O
be	O
classified	O
into	O
two	O
categories	O
.	O
First	O
is	O
based	O
on	O
Sequence	O
-	O
to	O
-	O
sequence	O
models	O
(	O
generative	O
models	O
)	O
(	O
Serban	O
et	O
al	O
,	O
2016a	O
;	O
Serban	O
et	O
al	O
,	O
2017a	O
;	O
Vinyals	O
and	O
Le	O
,	O
2015	O
)	O
,	O
where	O
a	O
model	O
is	O
trained	O
to	O
generate	O
a	O
response	O
given	O
context	O
;	O
and	O
the	O
other	O
is	O
Siamese	O
models	O
(	O
discriminative	O
models	O
)	O
(	O
Lowe	O
et	O
al	O
,	O
2017	O
)	O
,	O
where	O
a	O
model	O
is	O
trained	O
to	O
discriminate	O
between	O
positive	O
and	O
negative	O
responses	O
for	O
a	O
similar	O
context	O
.	O
In	O
both	O
types	O
of	O
models	O
,	O
at	O
test	O
time	O
,	O
a	O
set	O
of	O
candidate	O
responses	O
is	O
provided	O
consisting	O
of	O
one	O
correct	O
response	O
and	O
several	O
incorrect	O
responses	O
,	O
and	O
the	O
model	O
is	O
evaluated	O
on	O
its	O
ability	O
to	O
assign	O
a	O
higher	O
rank	O
to	O
the	O
true	O
response	O
.	O
In	O
this	O
paper	O
,	O
through	O
the	O
experimentation	O
with	O
both	O
generative	O
and	O
discriminative	O
types	O
of	O
models	O
,	O
we	O
validate	O
the	O
hypothesis	O
that	O
additional	O
information	O
available	O
in	O
the	O
form	O
of	O
dialogue	O
act	O
significantly	O
improves	O
the	O
performance	O
irrespective	O
of	O
the	O
underlying	O
model	O
.	O
In	O
addition	O
to	O
showing	O
the	O
utility	O
of	O
dialogue	O
acts	O
,	O
we	O
propose	O
a	O
novel	O
model	O
that	O
can	O
use	O
the	O
sequential	O
dialogue	O
act	O
information	O
in	O
a	O
natural	O
way	O
.	O
More	O
specifically	O
,	O
we	O
propose	O
a	O
dialogue	O
-	O
act	O
-	O
driven	O
hierarchical	O
Siamese	O
model	O
.	O
Hierarchical	O
models	O
have	O
shown	O
to	O
perform	O
better	O
than	O
non	O
-	O
hierarchical	O
models	O
for	O
the	O
task	O
of	O
dialogue	B-TaskName
generation	I-TaskName
,	O
whereas	O
Siamese	O
models	O
have	O
been	O
shown	O
to	O
outperform	O
the	O
encoder	O
-	O
decoder	O
based	O
models	O
for	O
the	O
task	O
of	O
next	O
utterance	O
selection	O
.	O
In	O
this	O
paper	O
,	O
we	O
combine	O
both	O
of	O
these	O
models	O
,	O
and	O
further	O
enhance	O
them	O
with	O
a	O
dialogue	O
act	O
encoder	O
.	O
The	O
proposed	O
model	O
has	O
a	O
hierarchical	O
encoder	O
which	O
encodes	O
the	O
past	O
utterances	O
,	O
and	O
combine	O
them	O
with	O
the	O
representation	O
of	O
additional	O
contextual	O
information	O
,	O
obtained	O
from	O
the	O
dialogue	O
acts	O
associated	O
with	O
the	O
past	O
utterances	O
,	O
to	O
discriminate	O
the	O
correct	O
response	O
from	O
the	O
incorrect	O
ones	O
.	O
Our	O
proposed	O
model	O
provides	O
us	O
the	O
best	O
of	O
both	O
worlds	O
and	O
outperforms	O
the	O
baseline	O
models	O
by	O
a	O
significant	O
margin	O
.	O
Among	O
others	O
,	O
a	O
key	O
contribution	O
of	O
this	O
paper	O
is	O
that	O
we	O
do	O
a	O
deeper	O
analysis	O
of	O
the	O
reasons	O
for	O
the	O
performance	O
improvement	O
due	O
to	O
inclusion	O
of	O
dialogue	O
act	O
and	O
draw	O
several	O
important	O
key	O
insights	O
such	O
as	O
,	O
dialogue	O
acts	O
induce	O
uniformity	O
in	O
the	O
data	O
,	O
they	O
aid	O
in	O
learning	O
the	O
right	O
patterns	O
.	O
We	O
believe	O
that	O
these	O
insights	O
would	O
inspire	O
new	O
research	O
in	O
this	O
field	O
and	O
push	O
the	O
boundary	O
even	O
further	O
.	O
The	O
main	O
contributions	O
of	O
this	O
paper	O
are	O
as	O
follows	O
:	O
1	O
.	O
For	O
the	O
task	O
of	O
next	O
utterance	O
selection	O
,	O
we	O
validate	O
the	O
hypothesis	O
that	O
additional	O
information	O
available	O
in	O
the	O
form	O
of	O
dialogue	O
acts	O
improves	O
the	O
performance	O
irrespective	O
of	O
the	O
underlying	O
models	O
.	O
2	O
.	O
We	O
propose	O
a	O
novel	O
model	O
that	O
combines	O
the	O
strength	O
of	O
Siamese	B-MethodName
network	I-MethodName
with	O
strengths	O
of	O
hierarchical	O
structure	O
inherent	O
in	O
the	O
conversations	O
and	O
dialogue	O
act	O
information	O
.	O
The	O
model	O
gives	O
us	O
the	O
best	O
of	O
all	O
,	O
and	O
outperforms	O
the	O
baseline	O
models	O
by	O
a	O
significant	O
margin	O
on	O
the	O
DailyDialog	B-DatasetName
Dataset	O
.	O
3	O
.	O
We	O
perform	O
a	O
deeper	O
analysis	O
of	O
the	O
utility	O
of	O
the	O
dialogue	O
act	O
information	O
and	O
draw	O
three	O
key	O
insights	O
:	O
models	O
learn	O
dominant	O
dialogue	O
act	O
patterns	O
;	O
dialogue	O
acts	O
induce	O
uniformity	O
;	O
dialogue	O
acts	O
reinforce	O
correct	O
dialogue	O
act	O
patterns	O
.	O
4	O
.	O
We	O
modify	O
the	O
DailyDialog	B-DatasetName
(	O
Li	O
et	O
al	O
,	O
2017b	O
)	O
dataset	O
for	O
the	O
task	O
of	O
next	O
utterance	O
selection	O
,	O
and	O
release	O
it	O
publicly	O
along	O
with	O
the	O
code	O
-	O
base	O
of	O
the	O
proposed	O
model	O
1	O
.	O
We	O
believe	O
that	O
this	O
dataset	O
will	O
work	O
as	O
a	O
benchmark	O
dataset	O
for	O
further	O
research	O
on	O
this	O
problem	O
.	O
Similar	O
benchmark	O
datasets	O
have	O
been	O
released	O
earlier	O
,	O
however	O
they	O
do	O
not	O
come	O
with	O
dialogue	O
act	O
information	O
.	O

A	O
simple	O
encoder	O
-	O
decoder	O
treats	O
the	O
first	O
K	O
utterances	O
as	O
a	O
single	O
long	O
chain	O
of	O
words	O
,	O
and	O
therefore	O
fails	O
to	O
leverage	O
the	O
hierarchical	O
structure	O
,	O
which	O
is	O
an	O
inherent	O
part	O
of	O
a	O
conversation	O
.	O
Hierarchy	O
is	O
important	O
for	O
conversation	O
modeling	O
since	O
it	O
captures	O
the	O
natural	O
dependency	O
among	O
utterances	O
.	O
Several	O
researchers	O
(	O
Sordoni	O
et	O
al	O
,	O
2015	O
;	O
Serban	O
et	O
al	O
,	O
2016b	O
;	O
Serban	O
et	O
al	O
,	O
2017b	O
;	O
Dehghani	O
et	O
al	O
,	O
2017	O
;	O
Kumar	B-DatasetName
et	O
al	O
,	O
2017	O
)	O
have	O
shown	O
that	O
hierarchical	O
models	O
outperform	O
standard	O
non	O
-	O
hierarchical	O
models	O
.	O
Hierarchical	O
models	O
use	O
two	O
encoders	O
to	O
capture	O
the	O
hierarchical	O
structure	O
.	O
The	O
first	O
encoder	O
,	O
referred	O
as	O
utterance	O
encoder	O
,	O
operates	O
at	O
the	O
utterance	O
level	O
,	O
encoding	O
each	O
word	O
in	O
each	O
utterance	O
.	O
The	O
second	O
encoder	O
,	O
referred	O
as	O
conversation	O
encoder	O
,	O
operates	O
at	O
the	O
conversation	O
level	O
,	O
encoding	O
each	O
utterance	O
in	O
the	O
conversation	O
,	O
based	O
on	O
the	O
representations	O
of	O
the	O
previous	O
encoder	O
.	O
These	O
two	O
encoders	O
make	O
sure	O
that	O
the	O
output	O
of	O
the	O
conversation	O
encoder	O
captures	O
the	O
dependencies	O
among	O
utterances	O
.	O
For	O
a	O
given	O
conversation	O
,	O
each	O
word	O
w	O
k	O
of	O
each	O
utterance	O
u	O
j	O
is	O
processed	O
by	O
an	O
embedding	O
layer	O
,	O
followed	O
by	O
an	O
RNN	O
which	O
serves	O
as	O
the	O
utterance	O
encoder	O
.	O
Similar	O
to	O
the	O
encoder	O
in	O
equation	O
(	O
1	O
)	O
,	O
an	O
utterance	O
encoder	O
gives	O
us	O
a	O
sequence	O
of	O
representations	O
v	O
1	O
,	O
v	O
2	O
,	O
.	O
.	O
.	O
v	O
K	O
,	O
corresponding	O
to	O
the	O
first	O
K	O
utterances	O
u	O
1	O
,	O
u	O
2	O
,	O
.	O
.	O
.	O
u	O
K	O
in	O
a	O
conversation	O
.	O
These	O
representations	O
are	O
passed	O
on	O
to	O
the	O
conversation	O
encoder	O
,	O
another	O
RNN	O
,	O
which	O
transforms	O
v	O
j	O
to	O
another	O
representation	O
g	O
j	O
.	O
The	O
representation	O
obtained	O
from	O
the	O
last	O
time	O
-	O
step	O
of	O
the	O
conversation	O
-	O
level	O
encoder	O
i.e.	O
g	O
K	O
is	O
considered	O
as	O
the	O
representation	O
of	O
the	O
entire	O
conversation	O
and	O
used	O
to	O
initialize	O
the	O
decoder	O
which	O
works	O
in	O
the	O
same	O
way	O
as	O
Equation	O
2	O
.	O

In	O
our	O
problem	O
setting	O
,	O
we	O
require	O
a	O
dataset	O
that	O
is	O
of	O
reasonable	O
size	O
2	O
and	O
has	O
utterances	O
annotated	O
with	O
the	O
corresponding	O
dialogue	O
acts	O
.	O
Although	O
there	O
are	O
several	O
available	O
datasets	O
,	O
such	O
as	O
SwDA	O
(	O
Switchboard	O
Dialogue	O
Act	O
Corpus	O
(	O
Jurafsky	O
,	O
1997	O
)	O
)	O
,	O
MRDA	B-DatasetName
(	O
Meeting	O
Recorder	O
Dialogue	O
Act	O
corpus	O
(	O
Janin	O
et	O
al	O
,	O
2003	O
)	O
)	O
,	O
Ubuntu	O
,	O
OpenSubtitles	B-DatasetName
(	O
Tiedemann	O
,	O
2009	O
)	O
,	O
etc	O
.	O
,	O
they	O
are	O
not	O
really	O
suitable	O
for	O
our	O
problem	O
setting	O
.	O
Most	O
of	O
these	O
datasets	O
do	O
not	O
come	O
with	O
dialogue	O
acts	O
,	O
and	O
the	O
ones	O
which	O
do	O
(	O
i.e.	O
SWDA	O
and	O
MRDA	B-DatasetName
)	O
are	O
small	O
in	O
size	O
.	O
Note	O
that	O
the	O
SwDA	O
and	O
MRDA	B-DatasetName
datasets	O
contain	O
1003	O
and	O
51	O
conversations	O
,	O
respectively	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
a	O
recently	O
released	O
dataset	O
,	O
DailyDialog	B-DatasetName
(	O
Li	O
et	O
al	O
,	O
2017b	O
)	O
,	O
is	O
the	O
only	O
dataset	O
that	O
has	O
utterances	O
annotated	O
with	O
dialogue	O
acts	O
and	O
is	O
large	O
enough	O
for	O
conversation	O
modeling	O
methods	O
to	O
work	O
.	O
Furthermore	O
,	O
in	O
this	O
dataset	O
,	O
conversations	O
are	O
non	O
-	O
task	O
oriented	O
,	O
and	O
each	O
conversation	O
focuses	O
on	O
one	O
topic	O
.	O
Each	O
utterance	O
is	O
annotated	O
with	O
four	O
dialogue	O
acts	O
as	O
described	O
in	O
Table	O
1	O
.	O
The	O
dataset	O
has	O
train	O
,	O
validation	O
,	O
and	O
test	O
splits	O
of	O
11118	O
,	O
1000	O
,	O
and	O
1000	O
conversations	O
,	O
respectively	O
.	O
We	O
evaluate	O
and	O
report	O
our	O
results	O
on	O
the	O
DailyDialog	B-DatasetName
dataset	O
.	O
In	O
this	O
paper	O
,	O
we	O
hypothesize	O
that	O
dialogue	O
acts	O
improve	O
conversation	O
modeling	O
.	O
However	O
,	O
it	O
is	O
not	O
always	O
possible	O
that	O
such	O
dialogue	O
acts	O
are	O
available	O
in	O
practice	O
,	O
and	O
it	O
would	O
be	O
ideal	O
to	O
predict	O
dialogue	O
acts	O
first	O
(	O
Kumar	B-DatasetName
et	O
al	O
,	O
2017	O
)	O
,	O
and	O
then	O
use	O
them	O
for	O
next	O
utterance	O
generation	O
/	O
retrieval	O
;	O
having	O
a	O
model	O
where	O
both	O
tasks	O
,	O
i.e.	O
prediction	O
and	O
generation	O
,	O
are	O
performed	O
simultaneously	O
may	O
not	O
be	O
ideal	O
for	O
validating	O
the	O
hypothesis	O
.	O
Note	O
that	O
the	O
error	O
from	O
the	O
dialogue	O
act	O
prediction	O
may	O
propagate	O
to	O
the	O
next	O
utterance	O
generation	O
/	O
retrieval	O
.	O
Therefore	O
,	O
we	O
intentionally	O
did	O
not	O
use	O
the	O
predicted	O
dialogue	O
acts	O
(	O
rather	O
used	O
the	O
available	O
dialogue	O
acts	O
)	O
to	O
make	O
sure	O
that	O
the	O
insights	O
about	O
the	O
usefulness	O
of	O
the	O
dialogue	O
acts	O
are	O
not	O
corrupted	O
due	O
to	O
the	O
error	O
in	O
the	O
upstream	O
prediction	O
model	O
.	O

A	O
speaker	O
is	O
providing	O
information	O
by	O
means	O
of	O
a	O
question	O
or	O
statement	O
Question	O
A	O
speaker	O
intends	O
to	O
obtain	O
information	O
by	O
asking	O
a	O
question	O
Directive	O
A	O
speaker	O
is	O
requesting	O
,	O
accept	O
/	O
reject	O
offer	O
,	O
or	O
making	O
a	O
suggestion	O
Comissive	O
A	O
speaker	O
accept	O
/	O
reject	O
a	O
request	O
or	O
suggestion	O
Table	O
1	O
:	O
Dialogue	O
Acts	O
and	O
their	O
description	O
available	O
in	O
the	O
DailyDialog	B-DatasetName
Dataset	O
.	O

The	O
DailyDialog	B-DatasetName
dataset	O
in	O
its	O
original	O
form	O
is	O
not	O
directly	O
useful	O
for	O
the	O
task	O
of	O
next	O
utterance	O
selection	O
,	O
and	O
hence	O
requires	O
preparation	O
.	O
The	O
dataset	O
has	O
the	O
dialogues	O
from	O
both	O
the	O
speakers	O
.	O
Owing	O
to	O
the	O
different	O
conversational	O
style	O
of	O
human	O
and	O
conversation	O
agent	B-DatasetName
,	O
our	O
objective	O
is	O
to	O
build	O
a	O
model	O
that	O
is	O
specific	O
to	O
the	O
agent	B-DatasetName
,	O
i.e.	O
bot	O
.	O
Therefore	O
,	O
we	O
need	O
to	O
modify	O
the	O
dataset	O
in	O
such	O
a	O
way	O
that	O
we	O
only	O
consider	O
those	O
turns	O
where	O
we	O
need	O
to	O
predict	O
the	O
bot	O
's	O
utterance	O
.	O
To	O
clarify	O
further	O
,	O
consider	O
the	O
example	O
conversation	O
given	O
in	O
Table	O
2	O
.	O
The	O
conversation	O
has	O
8	O
utterances	O
,	O
and	O
each	O
utterances	O
is	O
marked	O
with	O
the	O
speaker	O
,	O
i.e.	O
human	O
(	O
H	O
)	O
and	O
bot	O
(	O
B	O
)	O
.	O
Since	O
we	O
are	O
only	O
interested	O
in	O
building	O
bot	O
-	O
specific	O
model	O
,	O
we	O
only	O
pick	O
those	O
subsequences	O
from	O
this	O
conversation	O
where	O
the	O
last	O
utterance	O
is	O
"	O
B	O
"	O
.	O
This	O
gives	O
us	O
three	O
subsequences	O
:	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
;	O
3	O
,	O
4	O
,	O
5	O
,	O
6	O
;	O
5	O
,	O
6	O
,	O
7	O
,	O
8	O
for	O
a	O
context	O
of	O
size	O
3	O
.	O
In	O
each	O
of	O
these	O
sub	O
-	O
conversations	O
,	O
the	O
first	O
three	O
utterances	O
constitute	O
the	O
context	O
,	O
while	O
the	O
last	O
utterance	O
is	O
the	O
true	O
response	O
.	O
Our	O
training	O
data	O
consists	O
of	O
such	O
subsequences	O
made	O
up	O
of	O
4	O
utterances	O
.	O
In	O
the	O
test	O
data	O
,	O
each	O
subsequence	O
,	O
in	O
addition	O
to	O
these	O
4	O
utterances	O
,	O
has	O
9	O
more	O
utterances	O
selected	O
randomly	O
from	O
the	O
test	O
pool	O
,	O
therefore	O
a	O
total	O
of	O
13	O
utterances	O
.	O
These	O
9	O
utterances	O
along	O
with	O
the	O
4	O
th	O
response	O
(	O
i.	O

ED	O
-	O
It	O
is	O
a	O
vanilla	O
sequence	B-MethodName
to	I-MethodName
sequence	I-MethodName
model	O
that	O
uses	O
an	O
utterance	O
encoder	O
to	O
obtain	O
a	O
representation	O
of	O
first	O
K	O
utterances	O
which	O
is	O
then	O
used	O
in	O
a	O
decoder	O
to	O
generate	O
next	O
utterance	O
.	O
HRED	O
-	O
An	O
extension	O
of	O
sequence	B-MethodName
to	I-MethodName
sequence	I-MethodName
model	O
that	O
uses	O
a	O
hierarchical	O
encoder	O
to	O
obtain	O
a	O
representation	O
of	O
first	O
K	O
utterances	O
,	O
which	O
is	O
then	O
used	O
in	O
decoder	O
to	O
generate	O
next	O
utterance	O
.	O
ED	O
-	O
DA	O
-	O
An	O
extension	O
of	O
the	O
ED	O
model	O
which	O
uses	O
dialogue	O
act	O
information	O
.	O
It	O
has	O
a	O
conditional	O
decoder	O
,	O
that	O
conditions	O
the	O
generation	O
of	O
each	O
word	O
on	O
the	O
dialogue	O
acts	O
representation	O
.	O
HRED	O
-	O
DA	O
-	O
An	O
extension	O
of	O
the	O
HRED	O
model	O
which	O
uses	O
dialogue	O
act	O
information	O
.	O
Similar	O
to	O
ED	O
-	O
DA	O
,	O
it	O
also	O
has	O
a	O
conditional	O
decoder	O
that	O
conditions	O
the	O
generation	O
of	O
each	O
word	O
on	O
the	O
dialogue	O
acts	O
representation	O
.	O

In	O
conversation	O
modeling	O
,	O
the	O
most	O
basic	O
problem	O
is	O
to	O
generate	O
a	O
response	O
given	O
a	O
context	O
.	O
Several	O
efforts	O
have	O
been	O
made	O
towards	O
solving	O
the	O
problem	O
of	O
dialogue	B-TaskName
generation	I-TaskName
(	O
Vinyals	O
and	O
Le	O
,	O
2015	O
;	O
Liu	O
et	O
al	O
,	O
2016	O
;	O
Li	O
et	O
al	O
,	O
2015	O
)	O
,	O
however	O
,	O
due	O
to	O
the	O
inherent	O
difficulty	O
of	O
the	O
problem	O
,	O
these	O
efforts	O
have	O
only	O
had	O
limited	O
success	O
and	O
are	O
known	O
to	O
have	O
issues	O
like	O
generating	O
repetitive	O
and	O
generalized	O
responses	O
such	O
as	O
I	O
do	O
n't	O
know	O
or	O
Ok	O
.	O
For	O
the	O
task	O
of	O
Next	O
Utterance	O
Selection	O
,	O
which	O
is	O
a	O
relatively	O
simpler	O
problem	O
than	O
generation	O
,	O
though	O
existing	O
generative	O
models	O
can	O
be	O
easily	O
adopted	O
,	O
their	O
counterpart	O
discriminative	O
models	O
have	O
shown	O
to	O
have	O
better	O
performance	O
.	O
In	O
generative	O
models	O
,	O
the	O
most	O
notable	O
work	O
is	O
from	O
(	O
Vinyals	O
and	O
Le	O
,	O
2015	O
)	O
,	O
however	O
this	O
work	O
considers	O
the	O
context	O
as	O
a	O
flat	O
long	O
string	O
of	O
words	O
and	O
ignores	O
the	O
hierarchical	O
structure	O
.	O
Researchers	O
have	O
proposed	O
hierarchical	O
model	O
(	O
Serban	O
et	O
al	O
,	O
2016b	O
)	O
and	O
their	O
variations	O
(	O
Serban	O
et	O
al	O
,	O
2017b	O
;	O
Serban	O
et	O
al	O
,	O
2017a	O
;	O
Li	O
et	O
al	O
,	O
2017a	O
)	O
but	O
none	O
of	O
these	O
models	O
take	O
into	O
account	O
the	O
dialogue	O
act	O
information	O
.	O
In	O
Discriminative	O
models	O
,	O
such	O
as	O
Siamese	O
,	O
a	O
very	O
notable	O
work	O
by	O
(	O
Kannan	O
et	O
al	O
,	O
2016	O
)	O
,	O
smart	O
reply	O
,	O
retrieves	O
the	O
most	O
likely	O
response	O
from	O
a	O
set	O
of	O
candidate	O
response	O
clusters	O
.	O
(	O
Lowe	O
et	O
al	O
,	O
2017	O
)	O
has	O
used	O
a	O
retrieval	O
based	O
Siamese	O
model	O
and	O
shown	O
its	O
results	O
on	O
the	O
Ubuntu	O
corpus	O
.	O
Our	O
proposed	O
model	O
builds	O
upon	O
the	O
strengths	O
of	O
generative	O
and	O
discriminative	O
models	O
,	O
and	O
uses	O
hierarchy	O
along	O
with	O
the	O
dialogue	O
act	O
information	O
to	O
achieve	O
the	O
best	O
performance	O
.	O
A	O
recent	O
work	O
by	O
(	O
Zhao	O
et	O
al	O
,	O
2017	O
)	O
has	O
used	O
dialogue	O
acts	O
for	O
the	O
task	O
of	O
dialogue	B-TaskName
generation	I-TaskName
.	O
Our	O
work	O
complements	O
their	O
findings	O
,	O
and	O
further	O
show	O
that	O
dialogue	O
acts	O
improve	O
the	O
model	O
performance	O
across	O
the	O
board	O
irrespective	O
of	O
underlying	O
model	O
(	O
i.e.	O
generative	O
or	O
discriminative	O
models	O
)	O
and	O
for	O
the	O
task	O
of	O
next	O
utterance	O
selection	O
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
our	O
systems	O
for	O
the	O
MADAR	O
Shared	O
Task	O
:	O
Arabic	O
Fine	O
-	O
Grained	O
Dialect	B-TaskName
Identification	I-TaskName
.	O
The	O
shared	O
task	O
consists	O
of	O
two	O
subtasks	O
.	O
The	O
goal	O
of	O
Subtask	O
-	O
1	O
(	O
S	O
-	O
1	O
)	O
is	O
to	O
detect	O
an	O
Arabic	O
city	O
dialect	O
in	O
a	O
given	O
text	O
and	O
the	O
goal	O
of	O
Subtask	O
-	O
2	O
(	O
S	O
-	O
2	O
)	O
is	O
to	O
predict	O
the	O
country	O
of	O
origin	O
of	O
a	O
Twitter	O
user	O
by	O
using	O
tweets	O
posted	O
by	O
the	O
user	O
.	O
In	O
S	O
-	O
1	O
,	O
our	O
proposed	O
systems	O
are	O
based	O
on	O
language	B-TaskName
modelling	I-TaskName
.	O
We	O
use	O
language	O
models	O
to	O
extract	O
features	O
that	O
are	O
later	O
used	O
as	O
an	O
input	O
for	O
other	O
machine	O
learning	O
algorithms	O
.	O
We	O
also	O
experiment	O
with	O
recurrent	O
neural	O
networks	O
(	O
RNN	O
)	O
,	O
but	O
these	O
experiments	O
showed	O
that	O
simpler	O
machine	O
learning	O
algorithms	O
are	O
more	O
successful	O
.	O
Our	O
system	O
achieves	O
0.658	O
macro	O
F	O
1	O
-	O
score	O
and	O
our	O
rank	O
is	O
6	O
th	O
out	O
of	O
19	O
teams	O
in	O
S	O
-	O
1	O
and	O
7	O
th	O
in	O
S	O
-	O
2	O
with	O
0.475	O
macro	O
F	O
1	O
-	O
score	O
.	O

In	O
S	O
-	O
1	O
,	O
both	O
of	O
our	O
systems	O
used	O
for	O
the	O
official	O
submission	O
take	O
as	O
an	O
input	O
language	O
model	O
features	O
.	O
In	O
our	O
case	O
the	O
objective	O
of	O
a	O
language	O
model	O
in	O
its	O
simplest	O
form	O
is	O
to	O
predict	O
probability	O
p	O
(	O
S	O
)	O
of	O
sentence	O
S	O
which	O
is	O
composed	O
from	O
strings	O
(	O
words	O
or	O
character	O
n	O
-	O
grams	O
)	O
s	O
1	O
,	O
s	O
2	O
.	O
.	O
.	O
s	O
N	O
,	O
where	O
N	O
is	O
a	O
number	O
of	O
strings	O
in	O
the	O
sentence	O
.	O
The	O
probability	O
estimation	O
of	O
p	O
(	O
S	O
)	O
can	O
be	O
computed	O
as	O
a	O
product	O
of	O
conditional	O
probabilities	O
p	O
(	O
s	O
i	O
|	O
h	O
i	O
)	O
of	O
its	O
strings	O
s	O
1	O
,	O
s	O
2	O
.	O
.	O
.	O
s	O
N	O
,	O
where	O
h	O
i	O
is	O
a	O
history	O
of	O
a	O
string	O
s	O
i	O
.	O
The	O
probability	O
of	O
string	O
s	O
i	O
is	O
conditioned	O
by	O
history	O
h	O
i	O
i.e.	O
n	O
−	O
1	O
preceding	O
strings	O
s	O
i−n+1	O
,	O
s	O
i−n+2	O
,	O
.	O
.	O
.	O
s	O
i−1	O
which	O
can	O
be	O
rewritten	O
as	O
s	O
i−1	O
i−n+1	O
.	O
The	O
resulting	O
formula	O
for	O
the	O
p	O
(	O
S	O
)	O
estimation	O
looks	O
as	O
follows	O
:	O
p	O
(	O
S	O
)	O
=	O
N	O
i=1	O
p	O
(	O
s	O
i	O
|	O
h	O
i	O
)	O
=	O
N	O
i=1	O
p	O
(	O
s	O
i	O
|	O
s	O
i−1	O
i−n+1	O
)	O
(	O
1	O
)	O
The	O
conditioned	O
probability	O
p	O
(	O
s	O
i	O
|	O
h	O
i	O
)	O
can	O
be	O
estimated	O
with	O
Maximum	O
Likelihood	O
Estimate	O
(	O
MLE	O
)	O
which	O
is	O
defined	O
as	O
:	O
p	O
M	O
LE	O
(	O
s	O
i	O
|	O
h	O
i	O
)	O
=	O
c	O
(	O
s	O
i−n+1	O
,	O
s	O
i−n+2	O
.	O
.	O
.	O
s	O
i	O
)	O
c	O
(	O
s	O
i−n+1	O
,	O
s	O
i−n+2	O
.	O
.	O
.	O
s	O
i−1	O
)	O
(	O
2	O
)	O
where	O
c	O
(	O
s	O
i−n+1	O
,	O
s	O
i−n+2	O
.	O
.	O
.	O
s	O
i	O
)	O
is	O
a	O
number	O
of	O
occurrences	O
of	O
string	O
s	O
i	O
with	O
history	O
h	O
i	O
and	O
c	O
(	O
s	O
i−n+1	O
,	O
s	O
i−n+2	O
.	O
.	O
.	O
s	O
i−1	O
)	O
is	O
a	O
number	O
of	O
occurrences	O
of	O
history	O
h	O
i	O
.	O
These	O
counts	O
are	O
taken	O
from	O
a	O
training	O
corpus	O
.	O
We	O
followed	O
Salameh	O
)	O
in	O
using	O
the	O
kenlm	O
language	B-TaskName
modelling	I-TaskName
tool	O
(	O
Heafield	O
et	O
al	O
,	O
2013	O
)	O
.	O
kenlm	O
does	O
n't	O
have	O
an	O
option	O
to	O
use	O
character	O
n	O
-	O
grams	O
instead	O
of	O
words	O
,	O
so	O
in	O
order	O
to	O
get	O
character	O
-	O
based	O
language	O
models	O
,	O
we	O
prepared	O
input	O
files	O
with	O
characters	O
separated	O
by	O
spaces	O
.	O
Instead	O
of	O
encoding	O
space	O
as	O
a	O
special	O
word	O
,	O
we	O
surrounded	O
words	O
with	O
a	O
<	O
w></w	O
>	O
pair	O
.	O
This	O
enables	O
noticing	O
strings	O
which	O
occur	O
at	O
the	O
beginning	O
or	O
end	O
of	O
a	O
word	O
(	O
as	O
would	O
a	O
special	O
sequence	O
for	O
space	O
)	O
but	O
reduces	O
the	O
possible	O
amount	O
of	O
inter	O
-	O
word	O
information	O
which	O
the	O
language	O
model	O
can	O
keep	O
for	O
a	O
given	O
order	O
,	O
the	O
parameter	O
which	O
indicates	O
to	O
kenlm	O
the	O
largest	O
n	O
-	O
gram	O
to	O
index	O
.	O
We	O
used	O
order	O
5	O
for	O
all	O
our	O
kenlm	O
language	O
models	O
.	O
We	O
prebuilt	O
models	O
for	O
each	O
dialect	O
.	O
We	O
prepared	O
six	O
directories	O
,	O
each	O
containing	O
word	O
or	O
character	O
models	O
for	O
each	O
dialect	O
in	O
one	O
of	O
the	O
three	O
corpora	O
.	O
We	O
wrote	O
a	O
LangModel	O
class	O
which	O
quacks	O
like	O
a	O
sklearn	O
classifier	O
,	O
that	O
is	O
,	O
it	O
supports	O
fit	O
(	O
)	O
,	O
predict	O
(	O
)	O
,	O
and	O
predict	O
proba	O
(	O
)	O
,	O
but	O
its	O
choices	O
are	O
based	O
on	O
a	O
directory	O
of	O
language	O
models	O
.	O
predict	O
(	O
)	O
returns	O
the	O
dialect	O
name	O
whose	O
model	O
gives	O
the	O
highest	O
score	O
.	O
predict	O
proba	O
(	O
)	O
provides	O
a	O
list	O
of	O
languagemodel	O
-	O
score	O
features	O
,	O
adjusted	O
to	O
probabilities	O
.	O

This	O
submission	O
uses	O
a	O
jumble	O
of	O
features	O
and	O
classifiers	O
,	O
most	O
from	O
the	O
sklearn	O
module	O
(	O
Buitinck	O
et	O
al	O
,	O
2013	O
)	O
.	O
The	O
final	O
classifier	O
is	O
a	O
hard	O
voting	O
classifier	O
with	O
three	O
input	O
streams	O
:	O
1	O
.	O
Soft	O
voting	O
classifier	O
on	O
:	O
on	O
language	O
-	O
model	O
-	O
scores	O
for	O
character	O
and	O
language	O
models	O
on	O
the	O
corpus	O
-	O
6	O
language	O
models	O
and	O
character	O
language	O
models	O
for	O
the	O
corpus	O
-	O
26	O
language	O
models	O
.	O
2	O
.	O
Support	B-MethodName
vector	I-MethodName
machine	I-MethodName
,	O
svm	B-MethodName
.	O
SVC	O
(	O
gamma='scale	O
'	O
,	O
kernel	O
=	O
'	O
poly	O
'	O
,	O
degree	O
=	O
2	O
)	O
with	O
the	O
same	O
features	O
as	O
item	O
1e	O
.	O

Fully	O
Quantized	O
Transformer	B-MethodName
for	O
Machine	B-TaskName
Translation	I-TaskName

In	O
this	O
section	O
,	O
we	O
review	O
a	O
broad	O
spectrum	O
of	O
quantization	B-TaskName
and	O
pruning	O
methods	O
for	O
neural	B-TaskName
network	I-TaskName
compression	I-TaskName
.	O

Unlike	O
Jacob	O
et	O
al	O
(	O
2017	O
)	O
,	O
we	O
do	O
not	O
nudge	O
the	O
domain	O
so	O
that	O
the	O
zero	O
value	O
gets	O
perfectly	O
mapped	O
.	O
The	O
only	O
zero	O
values	O
which	O
we	O
have	O
to	O
deal	O
with	O
are	O
the	O
padding	O
,	O
the	O
Softmax	B-MethodName
numerator	O
and	O
output	O
,	O
the	O
output	O
of	O
ReLU	B-MethodName
layers	O
and	O
dropouts	O
.	O
Since	O
padding	O
has	O
no	O
effect	O
on	O
the	O
final	O
output	O
,	O
we	O
completely	O
ignore	O
these	O
values	O
when	O
quantizing	O
.	O
For	O
ReLUs	O
and	O
the	O
Softmax	B-MethodName
's	O
numerator	O
and	O
output	O
,	O
we	O
fix	O
their	O
x	O
min	O
to	O
0	B-DatasetName
,	O
which	O
guarantees	O
the	O
perfect	O
mapping	O
of	O
the	O
value	O
.	O
Finally	O
,	O
quantization	B-TaskName
is	O
applied	O
before	O
any	O
dropout	O
operation	O
.	O
Indeed	O
,	O
even	O
though	O
the	O
zeros	O
added	O
to	O
the	O
output	O
of	O
the	O
quantization	B-TaskName
layer	O
might	O
not	O
be	O
part	O
of	O
the	O
domain	O
,	O
this	O
only	O
happens	O
during	O
training	O
.	O

In	O
this	O
section	O
,	O
we	O
present	O
the	O
results	O
of	O
our	O
full	O
quantization	B-TaskName
scheme	O
on	O
various	O
tasks	O
.	O
We	O
first	O
compare	O
our	O
method	O
on	O
a	O
machine	B-TaskName
translation	I-TaskName
setup	O
.	O
Then	O
we	O
present	O
the	O
results	O
of	O
numerous	O
ablation	O
studies	O
.	O
We	O
also	O
compare	O
the	O
impact	O
of	O
delaying	O
quantization	B-TaskName
on	O
translation	O
quality	O
.	O
Finally	O
,	O
we	O
evaluate	O
our	O
method	O
on	O
two	O
language	O
model	O
tasks	O
and	O
experiment	O
with	O
node	O
pruning	O
.	O

TOD	O
-	O
BERT	B-MethodName
:	O
Pre	O
-	O
trained	O
Natural	B-TaskName
Language	I-TaskName
Understanding	I-TaskName
for	O
Task	O
-	O
Oriented	O
Dialogue	O

The	O
underlying	O
difference	O
of	O
linguistic	O
patterns	O
between	O
general	O
text	O
and	O
task	O
-	O
oriented	O
dialogue	O
makes	O
existing	O
pre	O
-	O
trained	O
language	O
models	O
less	O
useful	O
in	O
practice	O
.	O
In	O
this	O
work	O
,	O
we	O
unify	O
nine	O
human	O
-	O
human	O
and	O
multi	O
-	O
turn	O
task	O
-	O
oriented	O
dialogue	O
datasets	O
for	O
language	O
modeling	O
.	O
To	O
better	O
model	O
dialogue	O
behavior	O
during	O
pre	O
-	O
training	O
,	O
we	O
incorporate	O
user	O
and	O
system	O
tokens	O
into	O
the	O
masked	B-TaskName
language	I-TaskName
modeling	I-TaskName
.	O
We	O
propose	O
a	O
contrastive	O
objective	O
function	O
to	O
simulate	O
the	O
response	O
selection	O
task	O
.	O
Our	O
pre	O
-	O
trained	O
task	O
-	O
oriented	O
dialogue	O
BERT	B-MethodName
(	O
TOD	O
-	O
BERT	B-MethodName
)	O
outperforms	O
strong	O
baselines	O
like	O
BERT	B-MethodName
on	O
four	O
downstream	O
taskoriented	O
dialogue	O
applications	O
,	O
including	O
intention	O
recognition	O
,	O
dialogue	B-TaskName
state	I-TaskName
tracking	I-TaskName
,	O
dialogue	O
act	O
prediction	O
,	O
and	O
response	O
selection	O
.	O
We	O
also	O
show	O
that	O
TOD	O
-	O
BERT	B-MethodName
has	O
a	O
stronger	O
few	O
-	O
shot	O
ability	O
that	O
can	O
mitigate	O
the	O
data	O
scarcity	O
problem	O
for	O
task	O
-	O
oriented	O
dialogue	O
.	O

Pre	O
-	O
trained	O
models	O
with	O
self	O
-	O
attention	O
encoder	O
architectures	O
(	O
Devlin	O
et	O
al	O
,	O
2018	O
;	O
have	O
been	O
commonly	O
used	O
in	O
many	O
NLP	O
applications	O
.	O
Such	O
models	O
are	O
self	O
-	O
supervised	O
based	O
on	O
a	O
massive	O
scale	O
of	O
general	O
text	O
corpora	O
,	O
such	O
as	O
English	O
Wikipedia	O
or	O
books	O
(	O
Zhu	O
et	O
al	O
,	O
2015	O
)	O
.	O
By	O
further	O
fine	O
-	O
tuning	O
these	O
representations	O
,	O
breakthroughs	O
have	O
been	O
continuously	O
reported	O
for	O
various	O
downstream	O
tasks	O
,	O
especially	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
.	O
However	O
,	O
previous	O
work	O
(	O
Rashkin	O
et	O
al	O
,	O
2018	O
;	O
Wolf	O
et	O
al	O
,	O
2019	O
)	O
shows	O
that	O
there	O
are	O
some	O
deficiencies	O
in	O
the	O
performance	O
to	O
apply	O
fine	O
-	O
tuning	O
on	O
conversational	O
corpora	O
directly	O
.	O
One	O
possible	O
reason	O
could	O
be	O
the	O
intrinsic	O
difference	O
of	O
linguistic	O
patterns	O
between	O
human	O
conversations	O
and	O
writing	O
text	O
,	O
resulting	O
in	O
a	O
large	O
gap	O
of	O
data	O
distributions	O
(	O
Bao	O
et	O
al	O
,	O
2019	O
)	O
.	O
Therefore	O
,	O
pre	O
-	O
training	O
dialogue	O
language	O
models	O
using	O
chit	O
-	O
chat	O
corpora	O
from	O
social	O
media	O
,	O
such	O
as	O
Twitter	O
or	O
Reddit	B-DatasetName
,	O
has	O
been	O
recently	O
investigated	O
,	O
especially	O
for	O
dialogue	O
response	B-TaskName
generation	I-TaskName
(	O
Zhang	O
et	O
al	O
,	O
2019	O
)	O
and	O
retrieval	O
(	O
Henderson	O
et	O
al	O
,	O
2019b	O
)	O
.	O
Although	O
these	O
opendomain	O
dialogues	O
are	O
diverse	O
and	O
easy	O
-	O
to	O
-	O
get	O
,	O
they	O
are	O
usually	O
short	O
,	O
noisy	O
,	O
and	O
without	O
specific	O
chatting	O
goals	O
.	O
On	O
the	O
other	O
hand	O
,	O
a	O
task	O
-	O
oriented	O
dialogue	O
has	O
explicit	O
goals	O
(	O
e.g.	O
restaurant	O
reservation	O
or	O
ticket	O
booking	O
)	O
and	O
many	O
conversational	O
interactions	O
.	O
But	O
each	O
dataset	O
is	O
usually	O
small	O
and	O
scattered	O
because	O
obtaining	O
and	O
labeling	O
such	O
data	O
is	O
time	O
-	O
consuming	O
.	O
Moreover	O
,	O
a	O
task	O
-	O
oriented	O
dialogue	O
has	O
explicit	O
user	O
and	O
system	O
behaviors	O
where	O
a	O
user	O
has	O
his	O
/	O
her	O
goal	O
,	O
and	O
a	O
system	O
has	O
its	O
belief	O
and	O
database	O
information	O
,	O
which	O
makes	O
the	O
language	O
understanding	O
component	O
and	O
dialogue	O
policy	O
learning	O
more	O
important	O
than	O
those	O
chit	O
-	O
chat	O
scenarios	O
.	O
This	O
paper	O
aims	O
to	O
prove	O
this	O
hypothesis	O
:	O
selfsupervised	O
language	O
model	O
pre	O
-	O
training	O
using	O
taskoriented	O
corpora	O
can	O
learn	O
better	O
representations	O
than	O
existing	O
pre	O
-	O
trained	O
models	O
for	O
task	O
-	O
oriented	O
downstream	O
tasks	O
.	O
We	O
emphasize	O
that	O
what	O
we	O
care	O
about	O
the	O
most	O
is	O
not	O
whether	O
our	O
pre	O
-	O
trained	O
model	O
can	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
each	O
downstream	O
task	O
since	O
most	O
of	O
the	O
current	O
best	O
models	O
are	O
built	O
on	O
top	O
of	O
pre	O
-	O
trained	O
models	O
,	O
and	O
ours	O
can	O
easily	O
replace	O
them	O
.	O
We	O
avoid	O
adding	O
too	O
many	O
additional	O
components	O
on	O
top	O
of	O
the	O
pre	O
-	O
training	O
architecture	O
when	O
fine	O
-	O
tuning	O
in	O
our	O
experiments	O
.	O
We	O
collect	O
and	O
combine	O
nine	O
human	O
-	O
human	O
and	O
multi	O
-	O
turn	O
task	O
-	O
oriented	O
dialogue	O
corpora	O
to	O
train	O
a	O
task	O
-	O
oriented	O
dialogue	O
BERT	B-MethodName
(	O
TOD	O
-	O
BERT	B-MethodName
)	O
.	O
In	O
total	O
,	O
there	O
are	O
around	O
100k	O
dialogues	O
with	O
1.4	O
M	O
utterances	O
across	O
over	O
60	O
different	O
domains	O
.	O
Like	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
,	O
TOD	O
-	O
BERT	B-MethodName
is	O
formulated	O
as	O
a	O
masked	O
language	O
model	O
and	O
uses	O
a	O
deep	O
bidirectional	O
Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
encoder	O
as	O
its	O
model	O
architecture	O
.	O
Unlike	O
BERT	B-MethodName
,	O
TOD	O
-	O
BERT	B-MethodName
incorporates	O
two	O
special	O
tokens	O
for	O
user	O
and	O
system	O
to	O
model	O
the	O
corresponding	O
dialogue	O
behavior	O
.	O
A	O
contrastive	O
objective	O
function	O
of	O
response	O
selection	O
task	O
is	O
combined	O
during	O
pretraining	O
stage	O
to	O
capture	O
response	O
similarity	O
.	O
We	O
select	O
BERT	B-MethodName
because	O
it	O
is	O
the	O
most	O
widely	O
used	O
model	O
in	O
NLP	O
research	O
recently	O
,	O
and	O
our	O
unified	O
datasets	O
can	O
be	O
easily	O
applied	O
to	O
pre	O
-	O
train	O
any	O
existing	O
language	O
models	O
.	O
We	O
test	O
TOD	O
-	O
BERT	B-MethodName
on	O
task	B-TaskName
-	I-TaskName
oriented	I-TaskName
dialogue	I-TaskName
systems	I-TaskName
on	O
four	O
core	O
downstream	O
tasks	O
,	O
including	O
intention	O
recognition	O
,	O
dialogue	B-TaskName
state	I-TaskName
tracking	I-TaskName
,	O
dialogue	O
act	O
prediction	O
,	O
and	O
response	O
selection	O
.	O
What	O
we	O
observe	O
is	O
:	O
TOD	O
-	O
BERT	B-MethodName
outperforms	O
BERT	B-MethodName
and	O
other	O
strong	O
baselines	O
such	O
as	O
GPT	B-MethodName
-	O
2	O
(	O
Radford	O
et	O
al	O
,	O
2019	O
)	O
and	O
DialoGPT	O
(	O
Zhang	O
et	O
al	O
,	O
2019	O
)	O
on	O
all	O
the	O
selected	O
downstream	O
tasks	O
,	O
which	O
further	O
confirms	O
its	O
effectiveness	O
for	O
improving	O
dialogue	O
language	O
understanding	O
.	O
We	O
find	O
that	O
response	O
contrastive	B-MethodName
learning	I-MethodName
is	O
beneficial	O
,	O
but	O
it	O
is	O
currently	O
overlooked	O
not	O
well	O
-	O
investigated	O
in	O
dialogue	O
pretraining	O
research	O
.	O
More	O
importantly	O
,	O
TOD	O
-	O
BERT	B-MethodName
has	O
a	O
stronger	O
few	O
-	O
shot	O
ability	O
than	O
BERT	B-MethodName
on	O
each	O
task	O
,	O
suggesting	O
that	O
it	O
can	O
reduce	O
the	O
need	O
for	O
expensive	O
human	O
-	O
annotated	O
labels	O
.	O
TOD	O
-	O
BERT	B-MethodName
can	O
be	O
easily	O
leveraged	O
and	O
adapted	O
to	O
a	O
new	O
taskoriented	O
dialogue	O
dataset	O
.	O
Our	O
source	O
code	O
and	O
data	O
processing	O
are	O
released	O
to	O
facilitate	O
future	O
research	O
on	O
pre	O
-	O
training	O
and	O
fine	O
-	O
tuning	O
of	O
task	O
-	O
oriented	O
dialogue	O
1	O
.	O

General	B-DatasetName
Pre	O
-	O
trained	O
Language	O
Models	O
,	O
which	O
are	O
trained	O
on	O
massive	O
general	O
text	O
such	O
as	O
Wikipedia	O
and	O
BookCorpus	B-DatasetName
,	O
can	O
be	O
roughly	O
divided	O
into	O
two	O
categories	O
:	O
uni	O
-	O
directional	O
or	O
bidirectional	O
attention	O
mechanisms	O
.	O
GPT	B-MethodName
(	O
Radford	O
et	O
al	O
,	O
2018	O
)	O
and	O
GPT	B-MethodName
-	O
2	O
(	O
Radford	O
et	O
al	O
,	O
2019	O
)	O
are	O
representatives	O
of	O
uni	O
-	O
directional	O
language	O
models	O
using	O
a	O
Transformer	B-MethodName
decoder	I-MethodName
,	O
where	O
the	O
objective	O
is	O
to	O
maximize	O
left	O
-	O
to	O
-	O
right	O
generation	O
likelihood	O
.	O
These	O
models	O
are	O
commonly	O
applied	O
in	O
natural	O
language	O
generation	O
tasks	O
.	O
On	O
the	O
other	O
hand	O
,	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
,	O
RoBERTa	B-MethodName
,	O
and	O
their	O
variances	O
are	O
pre	O
-	O
trained	O
using	O
a	O
Transformer	B-MethodName
encoder	O
with	O
bi	O
-	O
directional	O
token	O
prediction	O
.	O
These	O
models	O
are	O
usually	O
evaluated	O
on	O
classification	O
tasks	O
such	O
as	O
GLUE	B-DatasetName
benchmark	O
(	O
Wang	O
et	O
al	O
,	O
2018	O
)	O
or	O
span	O
-	O
based	O
question	B-TaskName
answering	I-TaskName
tasks	O
(	O
Ra	O
-	O
1	O
github.com/jasonwu0731/ToD	O
-	O
BERT	B-MethodName
jpurkar	O
et	O
al	O
,	O
2016	O
)	O
.	O
Some	O
language	O
models	O
can	O
support	O
both	O
unidirectional	O
and	O
bi	O
-	O
directional	O
attention	O
,	O
such	O
as	O
UniLM	O
(	O
Dong	O
et	O
al	O
,	O
2019	O
)	O
.	O
Conditional	O
language	O
model	O
pre	O
-	O
training	O
is	O
also	O
proposed	O
.	O
For	O
example	O
,	O
CTRL	B-MethodName
(	O
Keskar	O
et	O
al	O
,	O
2019	O
)	O
is	O
a	O
conditional	O
Transformer	B-MethodName
model	O
,	O
trained	O
to	O
condition	O
on	O
control	O
codes	O
that	O
govern	O
style	O
,	O
content	O
,	O
and	O
task	O
-	O
specific	O
behavior	O
.	O
Recently	O
,	O
multi	O
-	O
task	O
language	O
model	O
pretraining	O
with	O
unified	O
sequence	O
-	O
to	O
-	O
sequence	O
generation	O
is	O
proposed	O
.	O
Text	O
-	O
to	O
-	O
text	O
Transformer	B-MethodName
(	O
T5	B-MethodName
)	O
(	O
Raffel	O
et	O
al	O
,	O
2019	O
)	O
unifies	O
multiple	O
text	O
modeling	O
tasks	O
and	O
achieves	O
the	O
promising	O
results	O
in	O
various	O
NLP	O
benchmarks	O
.	O
Dialogue	O
Pre	O
-	O
trained	O
Language	O
Models	O
are	O
mostly	O
trained	O
on	O
open	O
-	O
domain	O
conversational	O
data	O
from	O
Reddit	B-DatasetName
or	O
Twitter	O
for	O
dialogue	O
response	B-TaskName
generation	I-TaskName
.	O
Transfertransfo	O
(	O
Wolf	O
et	O
al	O
,	O
2019	O
)	O
achieves	O
good	O
performance	O
on	O
ConvAI	O
-	O
2	O
dialogue	O
competition	O
using	O
GPT	B-MethodName
-	O
2	O
.	O
DialoGPT	O
(	O
Zhang	O
et	O
al	O
,	O
2019	O
)	O
is	O
an	O
extension	O
of	O
GPT	B-MethodName
-	O
2	O
that	O
is	O
pre	O
-	O
trained	O
on	O
Reddit	B-DatasetName
data	O
for	O
open	O
-	O
domain	O
response	B-TaskName
generation	I-TaskName
.	O
Con	O
-	O
veRT	O
(	O
Henderson	O
et	O
al	O
,	O
2019a	O
)	O
pre	O
-	O
trained	O
a	O
dual	O
transformer	O
encoder	O
for	O
response	O
selection	O
task	O
on	O
large	O
-	O
scale	O
Reddit	B-DatasetName
(	O
input	O
,	O
response	O
)	O
pairs	O
.	O
PLATO	O
(	O
Bao	O
et	O
al	O
,	O
2019	O
)	O
uses	O
both	O
Twitter	O
and	O
Reddit	B-DatasetName
data	O
to	O
pre	O
-	O
trained	O
a	O
dialogue	B-TaskName
generation	I-TaskName
model	O
with	O
discrete	O
latent	O
variables	O
.	O
All	O
of	O
them	O
are	O
designed	O
to	O
cope	O
with	O
the	O
response	B-TaskName
generation	I-TaskName
task	O
for	O
opendomain	O
chatbots	O
.	O
Pretraining	O
for	O
task	O
-	O
oriented	O
dialogues	O
,	O
on	O
the	O
other	O
hand	O
,	O
has	O
few	O
related	O
works	O
.	O
Budzianowski	O
and	O
Vulić	O
(	O
2019	O
)	O
first	O
apply	O
the	O
GPT	B-MethodName
-	O
2	O
model	O
to	O
train	O
on	O
response	B-TaskName
generation	I-TaskName
task	O
,	O
which	O
takes	O
system	O
belief	O
,	O
database	O
result	O
,	O
and	O
last	O
dialogue	O
turn	O
as	O
input	O
to	O
predict	O
next	O
system	O
responses	O
.	O
It	O
only	O
uses	O
one	O
dataset	O
to	O
train	O
its	O
model	O
because	O
few	O
public	O
datasets	O
have	O
database	O
information	O
available	O
.	O
Henderson	O
et	O
al	O
(	O
2019b	O
)	O
pre	O
-	O
trained	O
a	O
response	O
selection	O
model	O
for	O
task	O
-	O
oriented	O
dialogues	O
.	O
They	O
first	O
pre	O
-	O
train	O
on	O
Reddit	B-DatasetName
corpora	O
and	O
then	O
fine	O
-	O
tune	O
on	O
target	O
dialogue	O
domains	O
,	O
but	O
their	O
training	O
and	O
fine	O
-	O
tuning	O
code	O
is	O
not	O
released	O
.	O
Peng	O
et	O
al	O
(	O
2020	O
)	O
focus	O
on	O
the	O
natural	O
language	O
generation	O
(	O
NLG	O
)	O
task	O
,	O
which	O
assumes	O
dialogue	O
acts	O
and	O
slot	O
-	O
tagging	O
results	O
are	O
given	O
to	O
generate	O
a	O
natural	O
language	O
response	O
.	O
Pre	O
-	O
training	O
on	O
a	O
set	O
of	O
annotated	O
NLG	O
corpora	O
can	O
improve	O
conditional	O
generation	O
quality	O
using	O
a	O
GPT	B-MethodName
-	O
2	O
model	O
.	O
Name	O
#	O
Dialogue	O
#	O
Utterance	O
Avg	O
.	O
Turn	O
#	O
Domain	O
MetaLWOZ	B-DatasetName
37	O
,	O
884	O
432	O
,	O
036	O
11.4	O
47	O
Schema	O
(	O
Rastogi	O
et	O
al	O
,	O
2019	O
)	O
22	O
,	O
825	O
463	O
,	O
284	O
20.3	O
17	O
Taskmaster	O
(	O
Byrne	O
et	O
al	O
,	O
2019	O
)	O
13	O
,	O
215	O
303	O
,	O
066	O
22.9	O
6	O
MWOZ	O
(	O
Budzianowski	O
et	O
al	O
,	O
2018	O
)	O
10	O
,	O
420	O
71	O
,	O
410	O
6.9	O
7	O
MSR	O
-	O
E2E	B-DatasetName
10	O
,	O
087	O
74	O
,	O
686	O
7.4	O
3	O
SMD	B-DatasetName
(	O
Eric	O
and	O
Manning	O
,	O
2017	O
)	O
3	O
,	O
031	O
15	O
,	O
928	O
5.3	O
3	O
Frames	O
(	O
Asri	O
et	O
al	O
,	O
2017	O
)	O
1	O
,	O
369	O
19	O
,	O
986	O
14.6	O
3	O
WOZ	O
(	O
Mrkšić	O
et	O
al	O
,	O
2016	O
)	O
1	O
,	O
200	O
5	O
,	O
012	O
4.2	O
1	O
CamRest676	O
676	O
2	O
,	O
744	O
4.1	O
1	O

We	O
collect	O
nine	O
different	O
task	O
-	O
oriented	O
datasets	O
which	O
are	O
English	O
,	O
human	O
-	O
human	O
and	O
multi	O
-	O
turn	O
.	O
In	O
total	O
,	O
there	O
are	O
100	O
,	O
707	O
dialogues	O
,	O
which	O
dialogue	O
competition	O
.	O
Schema	O
(	O
Rastogi	O
et	O
al	O
,	O
2019	O
)	O
:	O
Schema	O
-	O
guided	O
dialogue	O
has	O
22	O
,	O
825	O
dialogues	O
and	O
provides	O
a	O
challenging	O
testbed	O
for	O
several	O
tasks	O
,	O
in	O
particular	O
,	O
dialogue	B-TaskName
state	I-TaskName
tracking	I-TaskName
.	O
Each	O
schema	O
is	O
a	O
set	O
of	O
tracking	O
slots	O
,	O
and	O
each	O
domain	O
could	O
have	O
multiple	O
possible	O
schemas	O
.	O
This	O
allows	O
a	O
single	O
dialogue	O
system	O
to	O
support	O
many	O
services	O
and	O
facilitates	O
the	O
simple	O
integration	O
of	O
new	O
services	O
without	O
requiring	O
much	O
training	O
data	O
.	O
The	O
Schema	O
dataset	O
is	O
used	O
as	O
the	O
dialogue	B-TaskName
state	I-TaskName
tracking	I-TaskName
task	O
for	O
DSTC8	O
dialogue	O
competition	O
.	O
Taskmaster	O
(	O
Byrne	O
et	O
al	O
,	O
2019	O
)	O
:	O
This	O
dataset	O
includes	O
13	O
,	O
215	O
dialogues	O
comprising	O
six	O
do	O
-	O
mains	O
,	O
including	O
5	O
,	O
507	O
spoken	O
and	O
7	O
,	O
708	O
written	O
dialogs	O
created	O
with	O
two	O
distinct	O
procedures	O
.	O
One	O
is	O
a	O
two	O
-	O
person	O
Wizard	O
of	O
Oz	O
approach	O
that	O
one	O
person	O
acts	O
like	O
a	O
robot	O
,	O
and	O
the	O
other	O
is	O
a	O
self	O
-	O
dialogue	O
approach	O
in	O
which	O
crowdsourced	O
workers	O
wrote	O
the	O
entire	O
dialog	O
themselves	O
.	O
It	O
has	O
22.9	O
average	O
conversational	O
turns	O
in	O
a	O
single	O
dialogue	O
,	O
which	O
is	O
the	O
longest	O
among	O
all	O
taskoriented	O
datasets	O
listed	O
.	O
MWOZ	O
(	O
Budzianowski	O
et	O
al	O
,	O
2018	O
)	O
:	O
Multi	O
-	O
Domain	O
Wizard	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	I-DatasetName
Oz	I-DatasetName
dataset	O
contains	O
10	O
,	O
420	O
dialogues	O
over	O
seven	O
domains	O
,	O
and	O
it	O
has	O
multiple	O
domains	O
in	O
a	O
single	O
dialogue	O
.	O
It	O
has	O
a	O
detailed	O
description	O
of	O
the	O
data	O
collection	O
procedure	O
,	O
user	O
goal	O
,	O
system	O
act	O
,	O
and	O
dialogue	O
state	O
labels	O
.	O
Different	O
from	O
most	O
of	O
the	O
existing	O
corpora	O
,	O
it	O
also	O
provides	O
full	O
database	O
information	O
.	O
MSR	O
-	O
E2E	B-DatasetName
:	O
Microsoft	O
end	O
-	O
toend	O
dialogue	O
challenge	O
has	O
10	O
,	O
087	O
dialogues	O
in	O
three	O
domains	O
,	O
movie	O
-	O
ticket	O
booking	O
,	O
restaurant	O
reservation	O
,	O
and	O
taxi	O
booking	O
.	O
It	O
also	O
includes	O
an	O
experiment	O
platform	O
with	O
built	O
-	O
in	O
simulators	O
in	O
each	O
domain	O
.	O
SMD	B-DatasetName
(	O
Eric	O
and	O
Manning	O
,	O
2017	O
)	O
:	O
Stanford	O
multidomain	O
dialogue	O
is	O
an	O
in	O
-	O
car	O
personal	O
assistant	O
dataset	O
,	O
comprising	O
3	O
,	O
301	O
dialogues	O
and	O
three	O
domains	O
:	O
calendar	O
scheduling	O
,	O
weather	O
information	B-TaskName
retrieval	I-TaskName
,	O
and	O
point	O
-	O
of	O
-	O
interest	O
navigation	O
.	O
It	O
is	O
designed	O
to	O
smoothly	O
interface	O
with	O
knowledge	O
bases	O
,	O
where	O
a	O
knowledge	O
snippet	O
is	O
attached	O
with	O
each	O
dialogue	O
as	O
a	O
piece	O
of	O
simplified	O
database	O
information	O
.	O
Frames	O
(	O
Asri	O
et	O
al	O
,	O
2017	O
)	O
:	O
This	O
dataset	O
comprises	O
1	O
,	O
369	O
human	O
-	O
human	O
dialogues	O
with	O
an	O
average	O
of	O
14.6	O
turns	O
per	O
dialogue	O
,	O
where	O
users	O
are	O
given	O
some	O
constraints	O
to	O
book	O
a	O
trip	O
and	O
assistants	O
who	O
search	O
a	O
database	O
to	O
find	O
appropriate	O
trips	O
.	O
Unlike	O
other	O
datasets	O
,	O
it	O
has	O
labels	O
to	O
keep	O
track	O
of	O
different	O
semantic	O
frames	O
,	O
which	O
is	O
the	O
decision	O
-	O
making	O
behavior	O
of	O
users	O
throughout	O
each	O
dialogue	O
.	O
WOZ	O
(	O
Mrkšić	O
et	O
al	O
,	O
2016	O
)	O
and	O
Cam	O
-	O
Rest676	O
(	O
Wen	O
et	O
al	O
,	O
2016	O
)	O
:	O
These	O
two	O
corpora	O
use	O
the	O
same	O
data	O
collection	O
procedure	O
and	O
same	O
ontology	B-MethodName
from	O
DSTC2	O
(	O
Henderson	O
et	O
al	O
,	O
2014	O
)	O
.	O
They	O
are	O
one	O
of	O
the	O
first	O
task	O
-	O
oriented	O
dialogue	O
datasets	O
that	O
use	O
Wizard	O
of	O
Oz	O
style	O
with	O
text	O
input	O
instead	O
of	O
speech	O
input	O
,	O
which	O
improves	O
the	O
model	O
's	O
capacity	O
for	O
the	O
semantic	O
understanding	O
instead	O
of	O
its	O
robustness	O
to	O
automatic	B-TaskName
speech	I-TaskName
recognition	I-TaskName
errors	O
.	O

We	O
pick	O
up	O
several	O
datasets	O
,	O
OOS	O
,	O
DSTC2	O
,	O
GSIM	O
,	O
and	O
MWOZ	O
,	O
for	O
downstream	O
evaluation	O
.	O
The	O
first	O
three	O
corpora	O
are	O
not	O
included	O
in	O
the	O
pre	O
-	O
trained	O
task	O
-	O
oriented	O
datasets	O
.	O
For	O
MWOZ	O
,	O
to	O
be	O
fair	O
,	O
we	O
do	O
not	O
include	O
its	O
test	O
set	O
dialogues	O
during	O
the	O
pretraining	O
stage	O
.	O
Details	O
of	O
each	O
evaluation	O
dataset	O
are	O
discussed	O
in	O
the	O
following	O
:	O
OOS	O
(	O
Larson	O
et	O
al	O
,	O
2019	O
)	O
:	O
The	O
out	O
-	O
of	O
-	O
scope	O
intent	O
dataset	O
is	O
one	O
of	O
the	O
largest	O
annotated	O
intent	O
datasets	O
,	O
including	O
15	O
,	O
100/3	O
,	O
100/5	O
,	O
500	O
samples	O
for	O
the	O
train	O
,	O
validation	O
,	O
and	O
test	O
sets	O
,	O
respectively	O
.	O
It	O
covers	O
151	O
intent	O
classes	O
over	O
ten	O
domains	O
,	O
including	O
150	O
in	O
-	O
scope	O
intent	O
and	O
one	O
outof	O
-	O
scope	O
intent	O
.	O
The	O
out	O
-	O
of	O
-	O
scope	O
intent	O
means	O
that	O
a	O
user	O
utterance	O
that	O
does	O
not	O
fall	O
into	O
any	O
of	O
the	O
predefined	O
intents	O
.	O
Each	O
of	O
the	O
intents	O
has	O
100	O
training	O
samples	O
.	O
DSTC2	O
(	O
Henderson	O
et	O
al	O
,	O
2014	O
)	O
:	O
DSTC2	O
is	O
a	O
human	O
-	O
machine	O
task	O
-	O
oriented	O
dataset	O
that	O
may	O
include	O
a	O
certain	O
system	O
response	O
noise	O
.	O
It	O
has	O
1	O
,	O
612/506/1117	O
dialogues	O
for	O
train	O
,	O
validation	O
,	O
and	O
test	O
sets	O
,	O
respectively	O
.	O
We	O
follow	O
to	O
map	O
the	O
original	O
dialogue	O
act	O
labels	O
to	O
universal	O
dialogue	O
acts	O
,	O
which	O
results	O
in	O
9	O
different	O
system	O
dialogue	O
acts	O
.	O
GSIM	O
(	O
Shah	O
et	O
al	O
,	O
2018a	O
)	O
:	O
GSIM	O
is	O
a	O
humanrewrote	O
machine	O
-	O
machine	O
task	O
-	O
oriented	O
corpus	O
,	O
including	O
1500/469/1039	O
dialogues	O
for	O
the	O
train	O
,	O
validation	O
,	O
and	O
test	O
sets	O
,	O
respectively	O
.	O
We	O
combine	O
its	O
two	O
domains	O
,	O
movie	O
and	O
restaurant	O
domains	O
,	O
into	O
one	O
single	O
corpus	O
.	O
It	O
is	O
collected	O
by	O
Machines	O
Talking	O
To	O
Machines	O
(	O
M2	O
M	O
)	O
(	O
Shah	O
et	O
al	O
,	O
2018b	O
)	O
approach	O
,	O
a	O
functionality	O
-	O
driven	O
process	O
combining	O
a	O
dialogue	O
self	O
-	O
play	O
step	O
and	O
a	O
crowdsourcing	O
step	O
.	O
We	O
map	O
its	O
dialogue	O
act	O
labels	O
to	O
universal	O
dialogue	O
acts	O
,	O
resulting	O
in	O
6	O
different	O
system	O
dialogue	O
acts	O
.	O
MWOZ	O
(	O
Budzianowski	O
et	O
al	O
,	O
2018	O
)	O
:	O
MWOZ	O
is	O
the	O
most	O
common	O
benchmark	O
for	O
task	O
-	O
oriented	O
dialogues	O
,	O
especially	O
for	O
dialogue	B-TaskName
state	I-TaskName
tracking	I-TaskName
.	O
It	O
has	O
8420/1000/1000	O
dialogues	O
for	O
train	O
,	O
validation	O
,	O
and	O
test	O
sets	O
,	O
respectively	O
.	O
Across	O
seven	O
different	O
domains	O
,	O
in	O
total	O
,	O
it	O
has	O
30	O
(	O
domain	O
,	O
slot	O
)	O
pairs	O
that	O
need	O
to	O
be	O
tracked	O
in	O
the	O
test	O
set	O
.	O
We	O
use	O
its	O
revised	O
version	O
MWOZ	O
2.1	O
,	O
which	O
has	O
the	O
same	O
dialogue	O
transcripts	O
but	O
with	O
cleaner	O
state	O
label	O
annotation	O
.	O

Before	O
fine	O
-	O
tuning	O
each	O
pre	O
-	O
trained	O
models	O
,	O
we	O
first	O
investigate	O
their	O
feature	O
extraction	O
ability	O
by	O
probing	O
their	O
output	O
representations	O
.	O
Probing	O
methods	O
are	O
proposed	O
to	O
determine	O
what	O
information	O
is	O
carried	O
intrinsically	O
by	O
the	O
learned	O
embeddings	O
(	O
Tenney	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
probe	O
the	O
output	O
representation	O
using	O
one	O
single	O
-	O
layer	O
perceptron	O
on	O
top	O
of	O
a	O
"	O
fixed	O
"	O
pre	O
-	O
trained	O
language	O
model	O
and	O
only	O
finetune	O
that	O
layer	O
for	O
a	O
downstream	O
task	O
with	O
the	O
same	O
hyper	O
-	O
parameters	O
.	O
Table	O
3	O
shows	O
the	O
probing	O
results	O
of	O
domain	O
classification	O
on	O
MWOZ	O
,	O
intent	O
identification	O
on	O
OOS	O
,	O
and	O
dialogue	O
act	O
prediction	O
on	O
MWOZ	O
.	O
TOD	O
-	O
BERT	B-MethodName
-	O
jnt	O
achieves	O
the	O
highest	O
performance	O
in	O
this	O
setting	O
,	O
suggesting	O
its	O
representation	O
contains	O
the	O
most	O
useful	O
information	O
.	O

We	O
propose	O
task	O
-	O
oriented	O
dialogue	O
BERT	B-MethodName
(	O
TOD	O
-	O
BERT	B-MethodName
)	O
trained	O
on	O
nine	O
human	O
-	O
human	O
and	O
multiturn	O
task	O
-	O
oriented	O
datasets	O
across	O
over	O
60	O
domains	O
.	O
TOD	O
-	O
BERT	B-MethodName
outperforms	O
BERT	B-MethodName
on	O
four	O
dialogue	O
downstream	O
tasks	O
,	O
including	O
intention	O
classification	O
,	O
dialogue	B-TaskName
state	I-TaskName
tracking	I-TaskName
,	O
dialogue	O
act	O
prediction	O
,	O
and	O
response	O
selection	O
.	O
It	O
also	O
has	O
a	O
clear	O
advantage	O
in	O
the	O
few	O
-	O
shot	O
experiments	O
when	O
only	O
limited	O
labeled	O
data	O
is	O
available	O
.	O
TOD	O
-	O
BERT	B-MethodName
is	O
easy	O
-	O
to	O
-	O
deploy	O
and	O
will	O
be	O
open	O
-	O
sourced	O
,	O
allowing	O
the	O
NLP	O
research	O
community	O
to	O
apply	O
or	O
fine	O
-	O
tune	O
any	O
task	O
-	O
oriented	O
conversational	O
problem	O
.	O

Transformers	O
are	O
being	O
used	O
extensively	O
across	O
several	O
sequence	O
modeling	O
tasks	O
.	O
Significant	O
research	O
effort	O
has	O
been	O
devoted	O
to	O
experimentally	O
probe	O
the	O
inner	O
workings	O
of	O
Transformers	O
.	O
However	O
,	O
our	O
conceptual	O
and	O
theoretical	O
understanding	O
of	O
their	O
power	O
and	O
inherent	O
limitations	O
is	O
still	O
nascent	O
.	O
In	O
particular	O
,	O
the	O
roles	O
of	O
various	O
components	O
in	O
Transformers	O
such	O
as	O
positional	O
encodings	O
,	O
attention	O
heads	O
,	O
residual	O
connections	O
,	O
and	O
feedforward	O
networks	O
,	O
are	O
not	O
clear	O
.	O
In	O
this	O
paper	O
,	O
we	O
take	O
a	O
step	O
towards	O
answering	O
these	O
questions	O
.	O
We	O
analyze	O
the	O
computational	O
power	O
as	O
captured	O
by	O
Turing	O
-	O
completeness	O
.	O
We	O
first	O
provide	O
an	O
alternate	O
and	O
simpler	O
proof	O
to	O
show	O
that	O
vanilla	O
Transformers	O
are	O
Turing	O
-	O
complete	O
and	O
then	O
we	O
prove	O
that	O
Transformers	O
with	O
only	O
positional	O
masking	O
and	O
without	O
any	O
positional	O
encoding	O
are	O
also	O
Turing	O
-	O
complete	O
.	O
We	O
further	O
analyze	O
the	O
necessity	O
of	O
each	O
component	O
for	O
the	O
Turing	O
-	O
completeness	O
of	O
the	O
network	O
;	O
interestingly	O
,	O
we	O
find	O
that	O
a	O
particular	O
type	O
of	O
residual	B-MethodName
connection	I-MethodName
is	O
necessary	O
.	O
We	O
demonstrate	O
the	O
practical	O
implications	O
of	O
our	O
results	O
via	O
experiments	O
on	O
machine	B-TaskName
translation	I-TaskName
and	O
synthetic	O
tasks	O
.	O

Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
is	O
a	O
recent	O
selfattention	O
based	O
sequence	O
-	O
to	O
-	O
sequence	O
architecture	O
which	O
has	O
led	O
to	O
state	O
of	O
the	O
art	O
results	O
across	O
various	O
NLP	O
tasks	O
including	O
machine	B-TaskName
translation	I-TaskName
(	O
Ott	O
et	O
al	O
,	O
2018	O
)	O
,	O
language	O
modeling	O
(	O
Radford	O
et	O
al	O
,	O
2018	O
)	O
and	O
question	B-TaskName
answering	I-TaskName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
.	O
Although	O
a	O
number	O
of	O
variants	O
of	O
Transformers	O
have	O
been	O
proposed	O
,	O
the	O
original	O
architecture	O
still	O
underlies	O
these	O
variants	O
.	O
While	O
the	O
training	O
and	O
generalization	O
of	O
machine	O
learning	O
models	O
such	O
as	O
Transformers	O
are	O
the	O
central	O
goals	O
in	O
their	O
analysis	O
,	O
an	O
essential	O
prerequisite	O
to	O
this	O
end	O
is	O
characterization	O
of	O
the	O
computational	O
power	O
of	O
the	O
model	O
:	O
training	O
a	O
model	O
for	O
a	O
certain	O
task	O
can	O
not	O
succeed	O
if	O
the	O
model	O
is	O
computationally	O
incapable	O
of	O
carrying	O
out	O
the	O
task	O
.	O
While	O
the	O
computational	O
capabilities	O
of	O
recurrent	O
networks	O
(	O
RNNs	O
)	O
have	O
been	O
studied	O
for	O
decades	O
(	O
Kolen	O
and	O
Kremer	O
,	O
2001	O
;	O
Siegelmann	O
,	O
2012	O
)	O
,	O
for	O
Transformers	O
we	O
are	O
still	O
in	O
the	O
early	O
stages	O
.	O
The	O
celebrated	O
work	O
of	O
Siegelmann	O
and	O
Sontag	O
(	O
1992	O
)	O
showed	O
,	O
assuming	O
arbitrary	O
precision	O
,	O
that	O
RNNs	O
are	O
Turing	O
-	O
complete	O
,	O
meaning	O
that	O
they	O
are	O
capable	O
of	O
carrying	O
out	O
any	O
algorithmic	O
task	O
formalized	O
by	O
Turing	O
machines	O
.	O
Recently	O
,	O
Pérez	O
et	O
al	O
(	O
2019	O
)	O
have	O
shown	O
that	O
vanilla	O
Transformers	O
with	O
hard	O
-	O
attention	O
can	O
also	O
simulate	O
Turing	O
machines	O
given	O
arbitrary	O
precision	O
.	O
However	O
,	O
in	O
contrast	O
to	O
RNNs	O
,	O
Transformers	O
consist	O
of	O
several	O
components	O
and	O
it	O
is	O
unclear	O
which	O
components	O
are	O
necessary	O
for	O
its	O
Turing	O
-	O
completeness	O
and	O
thereby	O
crucial	O
to	O
its	O
computational	O
expressiveness	O
.	O
The	O
role	O
of	O
various	O
components	O
of	O
the	O
Transformer	B-MethodName
in	O
its	O
efficacy	O
is	O
an	O
important	O
question	O
for	O
further	O
improvements	O
.	O
Since	O
the	O
Transformer	B-MethodName
does	O
not	O
process	O
the	O
input	O
sequentially	O
,	O
it	O
requires	O
some	O
form	O
of	O
positional	O
information	O
.	O
Various	O
positional	O
encoding	O
schemes	O
have	O
been	O
proposed	O
to	O
capture	O
order	O
information	O
(	O
Shaw	O
et	O
al	O
,	O
2018	O
;	O
Dai	O
et	O
al	O
,	O
2019	O
;	O
Huang	O
et	O
al	O
,	O
2018	O
)	O
.	O
At	O
the	O
same	O
time	O
,	O
on	O
machine	B-TaskName
translation	I-TaskName
,	O
showed	O
that	O
the	O
performance	O
of	O
Transformers	O
with	O
only	O
positional	O
masking	O
(	O
Shen	O
et	O
al	O
,	O
2018	O
)	O
is	O
comparable	O
to	O
that	O
with	O
positional	O
encodings	O
.	O
In	O
case	O
of	O
positional	O
masking	O
(	O
Fig	O
.	O
1	O
)	O
,	O
as	O
opposed	O
to	O
explicit	O
encodings	O
,	O
the	O
model	O
is	O
only	O
allowed	O
to	O
attend	O
over	O
preceding	O
inputs	O
and	O
no	O
additional	O
positional	O
encoding	O
vector	O
is	O
combined	O
with	O
the	O
input	O
vector	O
.	O
Tsai	O
et	O
al	O
(	O
2019	O
)	O
raised	O
the	O
question	O
of	O
whether	O
explicit	O
encoding	O
is	O
necessary	O
if	O
positional	O
masking	O
is	O
used	O
.	O
Additionally	O
,	O
since	O
Pérez	O
et	O
al	O
(	O
2019	O
)	O
's	O
Turingcompleteness	O
proof	O
relied	O
heavily	O
on	O
residual	O
connections	O
,	O
they	O
asked	O
whether	O
these	O
connections	O
are	O
essential	O
for	O
Turing	O
-	O
completeness	O
.	O
In	O
this	O
paper	O
,	O
we	O
take	O
a	O
step	O
towards	O
answering	O
such	O
questions	O
.	O
Below	O
,	O
we	O
list	O
the	O
main	O
contributions	O
of	O
the	O
paper	O
,	O
We	O
provide	O
an	O
alternate	O
and	O
arguably	O
simpler	O
proof	O
to	O
show	O
that	O
Transformers	O
are	O
Turingcomplete	O
by	O
directly	O
relating	O
them	O
to	O
RNNs	O
.	O
More	O
importantly	O
,	O
we	O
prove	O
that	O
Transformers	O
with	O
positional	O
masking	O
and	O
without	O
positional	O
encoding	O
are	O
also	O
Turing	O
-	O
complete	O
.	O
We	O
analyze	O
the	O
necessity	O
of	O
various	O
components	O
such	O
as	O
self	O
-	O
attention	O
blocks	O
,	O
residual	O
connections	O
and	O
feedforward	O
networks	O
for	O
Turing	O
-	O
completeness	O
.	O
Figure	O
2	O
provides	O
an	O
overview	O
.	O
We	O
explore	O
implications	O
of	O
our	O
results	O
on	O
machine	B-TaskName
translation	I-TaskName
and	O
synthetic	O
tasks	O
.	O
1	O

Computational	O
Power	O
of	O
neural	O
networks	O
has	O
been	O
studied	O
since	O
the	O
foundational	O
paper	O
Mc	O
-	O
Culloch	O
and	O
Pitts	O
(	O
1943	O
)	O
;	O
in	O
particular	O
,	O
among	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
,	O
this	O
aspect	O
of	O
RNNs	O
has	O
long	O
been	O
studied	O
(	O
Kolen	O
and	O
Kremer	O
,	O
2001	O
)	O
.	O
The	O
seminal	O
work	O
by	O
Siegelmann	O
and	O
Sontag	O
(	O
1992	O
)	O
showed	O
that	O
RNNs	O
can	O
simulate	O
a	O
Turing	O
machine	O
by	O
using	O
unbounded	O
precision	O
.	O
Chen	O
et	O
al	O
(	O
2018	O
)	O
showed	O
that	O
RNNs	O
with	O
ReLU	B-MethodName
activations	O
are	O
also	O
Turing	O
-	O
complete	O
.	O
Many	O
recent	O
works	O
have	O
explored	O
the	O
computational	O
power	O
of	O
RNNs	O
in	O
practical	O
settings	O
.	O
Several	O
works	O
(	O
Merrill	O
et	O
al	O
,	O
2020	O
)	O
,	O
(	O
Weiss	O
et	O
al	O
,	O
2018	O
)	O
recently	O
studied	O
the	O
ability	O
of	O
RNNs	O
to	O
recognize	O
counter	O
-	O
like	O
languages	O
.	O
The	O
capability	O
of	O
RNNs	O
to	O
recognize	O
strings	O
of	O
balanced	O
parantheses	O
has	O
also	O
been	O
studied	O
(	O
Sennhauser	O
and	O
Berwick	O
,	O
2018	O
;	O
Skachkova	O
et	O
al	O
,	O
2018	O
)	O
.	O
However	O
,	O
such	O
analysis	O
on	O
Transformers	O
has	O
been	O
scarce	O
.	O
Theoretical	O
work	O
on	O
Transformers	O
was	O
initiated	O
by	O
Pérez	O
et	O
al	O
(	O
2019	O
)	O
who	O
formalized	O
the	O
notion	O
of	O
Transformers	O
and	O
showed	O
that	O
it	O
can	O
simulate	O
a	O
Turing	O
machine	O
given	O
arbitrary	O
precision	O
.	O
Concurrent	O
to	O
our	O
work	O
,	O
there	O
have	O
been	O
several	O
efforts	O
to	O
understand	O
self	O
-	O
attention	O
based	O
models	O
(	O
Levine	O
et	O
al	O
,	O
2020	O
;	O
Kim	O
et	O
al	O
,	O
2020	O
)	O
.	O
Hron	O
et	O
al	O
(	O
2020	O
)	O
show	O
that	O
Transformers	O
behave	O
as	O
Gaussian	B-TaskName
processes	I-TaskName
when	O
the	O
number	O
of	O
heads	O
tend	O
to	O
infinity	O
.	O
Hahn	O
(	O
2020	O
)	O
showed	O
some	O
limitations	O
of	O
Transformer	B-MethodName
encoders	O
in	O
modeling	O
regular	O
and	O
context	O
-	O
free	O
languages	O
.	O
It	O
has	O
been	O
recently	O
shown	O
that	O
Transformers	O
are	O
universal	O
approximators	O
of	O
sequence	O
-	O
tosequence	O
functions	O
given	O
arbitrary	O
precision	O
(	O
Yun	O
et	O
al	O
,	O
2020	O
)	O
.	O
However	O
,	O
these	O
are	O
not	O
applicable	O
2	O
to	O
the	O
complete	O
Transformer	B-MethodName
architecture	O
.	O
With	O
a	O
goal	O
similar	O
to	O
ours	O
,	O
Tsai	O
et	O
al	O
(	O
2019	O
)	O
attempted	O
to	O
study	O
the	O
attention	O
mechanism	O
via	O
a	O
kernel	O
formulation	O
.	O
However	O
,	O
a	O
systematic	O
study	O
of	O
various	O
components	O
of	O
Transformers	O
has	O
not	O
been	O
done	O
.	O

All	O
the	O
numbers	O
used	O
in	O
our	O
computations	O
will	O
be	O
from	O
the	O
set	O
of	O
rational	O
numbers	O
denoted	O
Q.	O
For	O
a	O
sequence	O
X	O
=	O
(	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
n	O
)	O
,	O
we	O
set	O
X	O
j	O
:	O
=	O
(	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
j	O
)	O
for	O
1	O
≤	O
j	O
≤	O
n.	O
We	O
will	O
work	O
with	O
an	O
alphabet	O
Σ	O
of	O
size	O
m	O
,	O
with	O
special	O
symbols	O
#	O
and	O
$	O
signifying	O
the	O
beginning	O
and	O
end	O
of	O
the	O
input	O
sequence	O
,	O
respectively	O
.	O
The	O
symbols	O
are	O
mapped	O
to	O
vectors	O
via	O
a	O
given	O
'	O
base	O
'	O
embedding	O
f	O
b	O
:	O
Σ	O
Q	O
d	O
b	O
,	O
where	O
d	O
b	O
is	O
the	O
dimension	O
of	O
the	O
embedding	O
.	O
E.g.	O
,	O
this	O
embedding	O
could	O
be	O
the	O
one	O
used	O
for	O
processing	O
the	O
symbols	O
by	O
the	O
RNN	O
.	O
We	O
set	O
f	O
b	O
(	O
#	O
)	O
=	O
0	B-DatasetName
d	O
b	O
and	O
f	O
b	O
(	O
$	O
)	O
=	O
0	B-DatasetName
d	O
b	O
.	O
Posi	O
-	O
tional	O
encoding	O
is	O
a	O
function	O
pos	O
:	O
N	O
Q	O
d	O
b	O
.	O
Together	O
,	O
these	O
provide	O
embedding	O
for	O
a	O
symbol	O
s	O
at	O
position	O
i	O
given	O
by	O
f	O
(	O
f	O
b	O
(	O
s	O
)	O
,	O
pos	O
(	O
i	O
)	O
)	O
,	O
often	O
taken	O
to	O
be	O
simply	O
f	O
b	O
(	O
s	O
)	O
+	O
pos	O
(	O
i	O
)	O
.	O
Vector	O
s	O
Q	O
m	O
denotes	O
one	O
-	O
hot	O
encoding	O
of	O
a	O
symbol	O
s	O
Σ.	O

We	O
follow	O
Siegelmann	O
and	O
Sontag	O
(	O
1992	O
)	O
in	O
our	O
definition	O
of	O
RNNs	O
.	O
To	O
feed	O
the	O
sequences	O
s	O
1	O
s	O
2	O
.	O
.	O
.	O
s	O
n	O
Σ	O
*	O
to	O
the	O
RNN	O
,	O
these	O
are	O
converted	O
to	O
the	O
vectors	O
x	O
1	O
,	O
x	O
2	O
,	O
.	O
.	O
.	O
,	O
x	O
n	O
where	O
x	O
i	O
=	O
f	O
b	O
(	O
s	O
i	O
)	O
.	O
The	O
RNN	O
is	O
given	O
by	O
the	O
recurrence	O
h	O
t	O
=	O
g	O
(	O
W	O
h	O
h	O
t−1	O
+	O
W	O
x	O
x	O
t	O
+	O
b	O
)	O
,	O
where	O
t	O
≥	O
1	O
,	O
function	O
g	O
(	O
)	O
is	O
a	O
multilayer	O
feedforward	B-MethodName
network	I-MethodName
(	O
FFN	O
)	O
with	O
activation	O
σ	O
,	O
bias	O
vector	O
b	O
Q	O
d	O
h	O
,	O
matrices	O
W	O
h	O
Q	O
d	O
h	O
×d	O
h	O
and	O
W	O
x	O
Q	O
d	O
h	O
×d	O
b	O
,	O
and	O
h	O
t	O
Q	O
d	O
h	O
is	O
the	O
hidden	O
state	O
with	O
given	O
initial	O
hidden	O
state	O
h	O
0	B-DatasetName
;	O
d	O
h	O
is	O
the	O
hidden	O
state	O
dimension	O
.	O
After	O
the	O
last	O
symbol	O
s	O
n	O
has	O
been	O
fed	O
,	O
we	O
continue	O
to	O
feed	O
the	O
RNN	O
with	O
the	O
terminal	O
symbol	O
f	O
b	O
(	O
$	O
)	O
until	O
it	O
halts	O
.	O
This	O
allows	O
the	O
RNN	O
to	O
carry	O
out	O
computation	O
after	O
having	O
read	O
the	O
input	O
.	O
A	O
class	O
of	O
seq	O
-	O
to	O
-	O
seq	O
neural	O
networks	O
is	O
Turingcomplete	O
if	O
the	O
class	O
of	O
languages	O
recognized	O
by	O
the	O
networks	O
is	O
exactly	O
the	O
class	O
of	O
languages	O
recognized	O
by	O
Turing	O
machines	O
.	O
Theorem	O
3.1	O
.	O
(	O
Siegelmann	O
and	O
Sontag	O
,	O
1992	O
)	O
Any	O
seq	O
-	O
to	O
-	O
seq	O
function	O
Σ	O
*	O
Σ	O
*	O
computable	O
by	O
a	O
Turing	O
machine	O
can	O
also	O
be	O
computed	O
by	O
an	O
RNN	O
.	O
For	O
details	O
please	O
see	O
section	O
B.1	O
in	O
appendix	O
.	O

We	O
showed	O
that	O
the	O
class	O
of	O
languages	O
recognized	O
by	O
Transformers	O
and	O
RNNs	O
are	O
exactly	O
the	O
same	O
.	O
This	O
implies	O
that	O
the	O
difference	O
in	O
performance	O
of	O
both	O
the	O
networks	O
across	O
different	O
tasks	O
can	O
be	O
attributed	O
only	O
to	O
their	O
learning	O
abilities	O
.	O
In	O
contrast	O
to	O
RNNs	O
,	O
Transformers	O
are	O
composed	O
of	O
multiple	O
components	O
which	O
are	O
not	O
essential	O
for	O
their	O
com	O
-	O
putational	O
expressiveness	O
.	O
However	O
,	O
in	O
practice	O
they	O
may	O
play	O
a	O
crucial	O
role	O
.	O
Recently	O
,	O
Voita	O
et	O
al	O
(	O
2019	O
)	O
showed	O
that	O
the	O
decoder	O
-	O
decoder	O
attention	O
heads	O
in	O
the	O
lower	O
layers	O
of	O
the	O
decoder	O
do	O
play	O
a	O
significant	O
role	O
in	O
the	O
NMT	O
task	O
and	O
suggest	O
that	O
they	O
may	O
be	O
helping	O
in	O
language	O
modeling	O
.	O
This	O
indicates	O
that	O
components	O
which	O
are	O
not	O
essential	O
for	O
the	O
computational	O
power	O
may	O
play	O
a	O
vital	O
role	O
in	O
improving	O
the	O
learning	O
and	O
generalization	O
ability	O
.	O
Take	O
-	O
Home	O
Messages	O
.	O
We	O
showed	O
that	O
the	O
order	O
information	O
can	O
be	O
provided	O
either	O
in	O
the	O
form	O
of	O
explicit	O
encodings	O
or	O
masking	O
without	O
affecting	O
computational	O
power	O
of	O
Transformers	O
.	O
The	O
decoder	O
-	O
encoder	O
attention	O
block	O
plays	O
a	O
necessary	O
role	O
in	O
conditioning	O
the	O
computation	O
on	O
the	O
input	O
sequence	O
while	O
the	O
residual	B-MethodName
connection	I-MethodName
around	O
it	O
is	O
necessary	O
to	O
keep	O
track	O
of	O
previous	O
computations	O
.	O
The	O
feedforward	B-MethodName
network	I-MethodName
in	O
the	O
decoder	O
is	O
the	O
only	O
component	O
capable	O
of	O
performing	O
computations	O
based	O
on	O
the	O
input	O
and	O
prior	O
computations	O
.	O
Our	O
experimental	O
results	O
show	O
that	O
removing	O
components	O
essential	O
for	O
computational	O
power	O
inhibit	O
the	O
model	O
's	O
ability	O
to	O
perform	O
certain	O
tasks	O
.	O
At	O
the	O
same	O
time	O
,	O
the	O
components	O
which	O
do	O
not	O
play	O
a	O
role	O
in	O
the	O
computational	O
power	O
may	O
be	O
vital	O
to	O
the	O
learning	O
ability	O
of	O
the	O
network	O
.	O
Although	O
our	O
proofs	O
rely	O
on	O
arbitrary	O
precision	O
,	O
which	O
is	O
common	O
practice	O
while	O
studying	O
the	O
computational	O
power	O
of	O
neural	O
networks	O
in	O
theory	O
(	O
Siegelmann	O
and	O
Sontag	O
,	O
1992	O
;	O
Pérez	O
et	O
al	O
,	O
2019	O
;	O
Hahn	O
,	O
2020	O
;	O
Yun	O
et	O
al	O
,	O
2020	O
)	O
,	O
implementations	O
in	O
practice	O
work	O
over	O
fixed	O
precision	O
settings	O
.	O
However	O
,	O
our	O
construction	O
provides	O
a	O
starting	O
point	O
to	O
analyze	O
Transformers	O
under	O
finite	O
precision	O
.	O
Since	O
RNNs	O
can	O
recognize	O
all	O
regular	O
languages	O
in	O
finite	O
precision	O
(	O
Korsky	O
and	O
Berwick	O
,	O
2019	O
)	O
,	O
it	O
follows	O
from	O
our	O
construction	O
that	O
Transformer	B-MethodName
can	O
also	O
recognize	O
a	O
large	O
class	O
of	O
regular	O
languages	O
in	O
finite	O
precision	O
.	O
At	O
the	O
same	O
time	O
,	O
it	O
does	O
not	O
imply	O
that	O
it	O
can	O
recognize	O
all	O
regular	O
languages	O
given	O
the	O
limitation	O
due	O
to	O
the	O
precision	O
required	O
to	O
encode	O
positional	O
information	O
.	O
We	O
leave	O
the	O
study	O
of	O
Transformers	O
in	O
finite	O
precision	O
for	O
future	O
work	O
.	O

We	O
begin	O
with	O
various	O
definitions	O
and	O
results	O
.	O
We	O
define	O
simulation	O
of	O
Turing	O
machines	O
by	O
RNNs	O
and	O
state	O
the	O
Turing	O
-	O
completeness	O
result	O
for	O
RNNs	O
.	O
We	O
define	O
vanilla	O
and	O
directional	O
Transformers	O
and	O
what	O
it	O
means	O
for	O
Transformers	O
to	O
simulate	O
RNNs	O
.	O
Many	O
of	O
the	O
definitions	O
from	O
the	O
main	O
paper	O
are	O
reproduced	O
here	O
,	O
but	O
in	O
more	O
detail	O
.	O
In	O
Sec	O
.	O
C.1	O
we	O
discuss	O
the	O
effect	O
of	O
removing	O
a	O
residual	B-MethodName
connection	I-MethodName
on	O
computational	O
power	O
of	O
Transformers	O
.	O
Sec	O
.	O
C.2	O
contains	O
the	O
proof	O
of	O
Turing	O
completeness	O
of	O
vanilla	O
Transformers	O
and	O
Sec	O
.	O
D	O
the	O
corresponding	O
proof	O
for	O
directional	O
Transformers	O
.	O
Finally	O
,	O
Sec	O
.	O
5	O
has	O
further	O
details	O
of	O
experiments	O
.	O

Here	O
we	O
summarize	O
,	O
somewhat	O
informally	O
,	O
the	O
Turing	O
-	O
completeness	O
result	O
for	O
RNNs	O
due	O
to	O
(	O
Siegelmann	O
and	O
Sontag	O
,	O
1992	O
)	O
.	O
We	O
recall	O
basic	O
notions	O
from	O
computability	O
theory	O
.	O
In	O
the	O
main	O
paper	O
,	O
for	O
simplicity	O
we	O
stated	O
the	O
results	O
for	O
total	O
recursive	O
functions	O
φ	O
:	O
{	O
0	B-DatasetName
,	O
1	O
}	O
*	O
{	O
0	B-DatasetName
,	O
1	O
}	O
*	O
,	O
i.e.	O
a	O
function	O
that	O
is	O
defined	O
on	O
every	O
s	O
{	O
0	B-DatasetName
,	O
1	O
}	O
*	O
and	O
whose	O
values	O
can	O
be	O
computed	O
by	O
a	O
Turing	O
machine	O
.	O
While	O
total	O
recursive	O
functions	O
form	O
a	O
satisfactory	O
formalization	O
of	O
seq	O
-	O
to	O
-	O
seq	O
tasks	O
,	O
here	O
we	O
state	O
the	O
more	O
general	O
result	O
for	O
partial	O
recursive	O
functions	O
.	O
Let	O
φ	O
:	O
{	O
0	B-DatasetName
,	O
1	O
}	O
*	O
{	O
0	B-DatasetName
,	O
1	O
}	O
*	O
be	O
partial	O
recursive	O
.	O
A	O
partial	O
recursive	O
function	O
is	O
one	O
that	O
need	O
not	O
be	O
defined	O
for	O
every	O
s	O
{	O
0	B-DatasetName
,	O
1	O
}	O
*	O
,	O
and	O
there	O
exists	O
a	O
Turing	O
Machine	O
M	O
with	O
the	O
following	O
property	O
.	O
The	O
input	O
s	O
is	O
initially	O
written	O
on	O
the	O
tape	O
of	O
the	O
Turing	O
Machine	O
M	O
and	O
the	O
output	O
φ	O
(	O
s	O
)	O
is	O
the	O
content	O
of	O
the	O
tape	O
upon	O
acceptance	O
which	O
is	O
indicated	O
by	O
halting	O
in	O
a	O
designated	O
accept	O
state	O
.	O
On	O
s	O
for	O
which	O
φ	O
is	O
undefined	O
,	O
M	O
does	O
not	O
halt	O
.	O
We	O
now	O
specify	O
how	O
Turing	O
machine	O
M	O
is	O
simulated	O
by	O
RNN	O
R	O
(	O
M	O
)	O
.	O
In	O
the	O
RNNs	O
in	O
(	O
Siegelmann	O
and	O
Sontag	O
,	O
1992	O
)	O
the	O
hidden	O
state	O
h	O
t	O
has	O
the	O
form	O
h	O
t	O
=	O
[	O
q	O
t	O
,	O
Ψ	O
1	O
,	O
Ψ	O
2	O
]	O
,	O
where	O
q	O
t	O
=	O
[	O
q	O
1	O
,	O
.	O
.	O
.	O
,	O
q	O
s	O
]	O
denotes	O
the	O
state	O
of	O
M	O
one	O
-	O
hot	O
form	O
.	O
Numbers	O
Ψ	O
1	O
,	O
Ψ	O
2	O
Q	O
,	O
called	O
stacks	O
,	O
store	O
the	O
contents	O
of	O
the	O
tape	O
in	O
a	O
certain	O
Cantor	O
set	O
like	O
encoding	O
(	O
which	O
is	O
similar	O
to	O
,	O
but	O
slightly	O
more	O
involved	O
,	O
than	O
binary	O
representation	O
)	O
at	O
each	O
step	O
.	O
The	O
simulating	O
RNN	O
R	O
(	O
M	O
)	O
,	O
gets	O
as	O
input	O
encodings	O
of	O
s	O
1	O
s	O
2	O
...	O
s	O
n	O
in	O
the	O
first	O
n	O
steps	O
,	O
and	O
from	O
then	O
on	O
receives	O
the	O
vector	O
0	B-DatasetName
as	O
input	O
in	O
each	O
step	O
.	O
If	O
φ	O
is	O
defined	O
on	O
s	O
,	O
then	O
M	O
halts	O
and	O
accepts	O
with	O
the	O
output	O
φ	O
(	O
s	O
)	O
the	O
content	O
of	O
the	O
tape	O
.	O
In	O
this	O
case	O
,	O
R	O
(	O
M	O
)	O
enters	O
a	O
special	O
accept	O
state	O
,	O
and	O
Ψ	O
1	O
encodes	O
φ	O
(	O
s	O
)	O
and	O
Ψ	O
2	O
=	O
0	B-DatasetName
.	O
If	O
M	O
does	O
not	O
halt	O
then	O
R	O
(	O
M	O
)	O
also	O
does	O
not	O
enter	O
the	O
accept	O
state	O
.	O
Siegelmann	O
and	O
Sontag	O
(	O
1992	O
)	O
further	O
show	O
that	O
from	O
R	O
(	O
M	O
)	O
one	O
can	O
further	O
explicitly	O
produce	O
the	O
φ	O
(	O
s	O
)	O
as	O
its	O
output	O
.	O
In	O
the	O
present	O
paper	O
,	O
we	O
will	O
not	O
deal	O
with	O
explicit	O
production	O
of	O
the	O
output	O
but	O
rather	O
work	O
with	O
the	O
definition	O
of	O
simulation	O
in	O
the	O
previous	O
paragraph	O
.	O
This	O
is	O
for	O
simplicity	O
of	O
exposition	O
,	O
and	O
the	O
main	O
ideas	O
are	O
already	O
contained	O
in	O
our	O
results	O
.	O
If	O
the	O
Turing	O
machine	O
computes	O
φ	O
(	O
s	O
)	O
in	O
time	O
T	O
(	O
s	O
)	O
,	O
the	O
simulation	O
takes	O
O	O
(	O
|	O
s	O
|	O
)	O
time	O
to	O
encode	O
the	O
input	O
sequence	O
s	O
and	O
4	O
T	O
(	O
s	O
)	O
to	O
compute	O
φ	O
(	O
s	O
)	O
.	O
Theorem	O
B.1	O
(	O
(	O
Siegelmann	O
and	O
Sontag	O
,	O
1992	O
)	O
)	O
.	O
Given	O
any	O
partial	O
recursive	O
function	O
φ	O
:	O
{	O
0	B-DatasetName
,	O
1	O
}	O
*	O
{	O
0	B-DatasetName
,	O
1	O
}	O
*	O
computed	O
by	O
Turing	O
machine	O
M	O
φ	O
,	O
there	O
exists	O
a	O
simulating	O
RNN	O
R	O
(	O
M	O
φ	O
)	O
.	O
In	O
view	O
of	O
the	O
above	O
theorem	O
,	O
for	O
establishing	O
Turing	O
-	O
completeness	O
of	O
Transformers	O
,	O
it	O
suffices	O
to	O
show	O
that	O
RNNs	O
can	O
be	O
simulated	O
by	O
Transformers	O
.	O
Thus	O
,	O
in	O
the	O
sequel	O
we	O
will	O
only	O
talk	O
about	O
simulating	O
RNNs	O
.	O

Theorem	O
C.2	O
.	O
RNNs	O
can	O
be	O
simulated	O
by	O
vanilla	O
Transformers	O
and	O
hence	O
the	O
class	O
of	O
vanilla	O
Transformers	O
is	O
Turing	O
-	O
complete	O
.	O
Proof	O
.	O
The	O
construction	O
of	O
the	O
simulating	O
transformer	O
is	O
simple	O
:	O
it	O
uses	O
a	O
single	O
head	O
and	O
both	O
the	O
encoder	O
and	O
decoder	O
have	O
one	O
layer	O
.	O
Moreover	O
,	O
the	O
encoder	O
does	O
very	O
little	O
and	O
most	O
of	O
the	O
action	O
happens	O
in	O
the	O
decoder	O
.	O
The	O
main	O
task	O
for	O
the	O
simulation	O
is	O
to	O
design	O
the	O
input	O
embedding	O
(	O
building	O
on	O
the	O
given	O
base	O
embedding	O
f	O
b	O
)	O
,	O
the	O
feedforward	B-MethodName
network	I-MethodName
O	O
(	O
)	O
and	O
the	O
matrices	O
corresponding	O
to	O
functions	O
Q	O
(	O
)	O
,	O
K	O
(	O
)	O
,	O
V	O
(	O
)	O
.	O
Input	O
embedding	O
.	O
The	O
input	O
embedding	O
is	O
obtained	O
by	O
summing	O
the	O
symbol	O
and	O
positional	O
encodings	O
which	O
we	O
next	O
describe	O
.	O
These	O
encodings	O
have	O
dimension	O
d	O
=	O
2d	O
h	O
+	O
d	O
b	O
+	O
2	O
,	O
where	O
d	O
h	O
is	O
the	O
dimension	O
of	O
the	O
hidden	O
state	O
of	O
the	O
RNN	O
and	O
d	O
b	O
is	O
the	O
dimension	O
of	O
the	O
given	O
encoding	O
f	O
b	O
of	O
the	O
input	O
symbols	O
.	O
We	O
will	O
use	O
the	O
symbol	O
encoding	O
f	O
symb	O
:	O
Σ	O
Q	O
d	O
which	O
is	O
essentially	O
the	O
same	O
as	O
f	O
b	O
except	O
that	O
the	O
dimension	O
is	O
now	O
larger	O
:	O
f	O
symb	O
(	O
s	O
)	O
=	O
[	O
0	B-DatasetName
d	O
h	O
,	O
f	O
e	O
(	O
s	O
)	O
;	O
0	B-DatasetName
d	O
h	O
,	O
0	B-DatasetName
,	O
0	B-DatasetName
]	O
.	O
The	O
positional	O
encoding	O
pos	O
:	O
N	O
Q	O
d	O
is	O
simply	O
pos	O
(	O
i	O
)	O
=	O
[	O
0	B-DatasetName
d	O
h	O
,	O
0	B-DatasetName
d	O
b	O
,	O
0	B-DatasetName
d	O
h	O
,	O
i	O
,	O
1	O
]	O
.	O
Together	O
,	O
these	O
define	O
the	O
combined	O
embedding	O
f	O
for	O
a	O
given	O
input	O
sequence	O
s	O
0	B-DatasetName
s	O
1	O
s	O
n	O
Σ	O
*	O
by	O
f	O
(	O
s	O
i	O
)	O
=	O
f	O
symb	O
(	O
s	O
i	O
)	O
+	O
pos	O
(	O
i	O
)	O
=	O
[	O
0	B-DatasetName
d	O
h	O
,	O
f	O
b	O
(	O
s	O
i	O
)	O
,	O
0	B-DatasetName
d	O
h	O
,	O
i	O
,	O
1	O
]	O
.	O
The	O
vectors	O
v	O
Q	O
d	O
used	O
in	O
the	O
computation	O
of	O
our	O
transformer	O
are	O
of	O
the	O
form	O
v	O
=	O
[	O
h	O
1	O
,	O
s	O
;	O
h	O
2	O
,	O
x	O
1	O
,	O
x	O
2	O
]	O
,	O
where	O
h	O
1	O
,	O
h	O
2	O
Q	O
d	O
h	O
,	O
s	O
Q	O
de	O
,	O
and	O
x	O
1	O
,	O
x	O
2	O
Q.	O
The	O
coordinates	O
corresponding	O
to	O
the	O
h	O
i	O
's	O
are	O
reserved	O
for	O
computation	O
related	O
to	O
hidden	O
states	O
of	O
the	O
RNN	O
,	O
the	O
coordinates	O
corresponding	O
to	O
s	O
are	O
reserved	O
for	O
base	O
embeddings	O
,	O
and	O
those	O
for	O
x	O
1	O
and	O
x	O
2	O
are	O
reserved	O
for	O
scalar	O
values	O
related	O
to	O
positional	O
operations	O
.	O
The	O
first	O
two	O
blocks	O
,	O
corresponding	O
to	O
h	O
1	O
and	O
s	O
are	O
reserved	O
for	O
computation	O
of	O
the	O
RNN	O
.	O
During	O
the	O
computation	O
of	O
the	O
Transformer	B-MethodName
,	O
the	O
underlying	O
RNN	O
will	O
get	O
the	O
input	O
st	O
at	O
step	O
t	O
for	O
t	O
=	O
0	B-DatasetName
,	O
1	O
,	O
.	O
.	O
.	O
,	O
where	O
recall	O
thatt	O
=	O
min	O
{	O
t	O
,	O
n	O
}	O
.	O
This	O
sequence	O
leads	O
to	O
the	O
RNN	O
getting	O
the	O
embedding	O
of	O
the	O
input	O
sequence	O
s	O
0	B-DatasetName
,	O
.	O
.	O
.	O
,	O
s	O
n	O
in	O
the	O
first	O
n	O
+	O
1	O
steps	O
followed	O
by	O
the	O
embedding	O
of	O
the	O
symbol	O
$	O
for	O
the	O
subsequent	O
steps	O
,	O
which	O
is	O
in	O
accordance	O
with	O
the	O
requirements	O
of	O
(	O
Siegelmann	O
and	O
Sontag	O
,	O
1992	O
)	O
.	O
Similar	O
to	O
(	O
Pérez	O
et	O
al	O
,	O
2019	O
)	O
we	O
use	O
the	O
following	O
scoring	O
function	O
in	O
the	O
attention	O
mechanism	O
in	O
our	O
construction	O
,	O
f	O
att	O
(	O
q	O
i	O
,	O
k	O
j	O
)	O
=	O
−	O
|	O
q	O
i	O
,	O
k	O
j	O
|	O
(	O
8	O
)	O
Construction	O
of	O
TEnc	O
.	O
As	O
previously	O
mentioned	O
,	O
our	O
transformer	O
encoder	O
has	O
only	O
one	O
layer	O
,	O
and	O
the	O
computation	O
in	O
the	O
encoder	O
is	O
very	O
simple	O
:	O
the	O
attention	O
mechanism	O
is	O
not	O
utilized	O
,	O
only	O
the	O
residual	O
connections	O
are	O
.	O
This	O
is	O
done	O
by	O
setting	O
the	O
matrix	O
for	O
V	O
(	O
)	O
to	O
the	O
all	O
-	O
zeros	O
matrix	O
,	O
and	O
the	O
feedforward	O
networks	O
to	O
always	O
output	O
0	B-DatasetName
.	O
The	O
application	O
of	O
appropriately	O
chosen	O
linear	O
transformations	O
for	O
the	O
final	O
K	O
(	O
)	O
and	O
V	O
(	O
)	O
give	O
the	O
following	O
lemma	B-DatasetName
about	O
the	O
output	O
of	O
the	O
encoder	O
.	O
Lemma	B-DatasetName
C.3	O
.	O
There	O
exists	O
a	O
single	O
layer	O
encoder	O
denoted	O
by	O
TEnc	O
that	O
takes	O
as	O
input	O
the	O
sequence	O
(	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
n	O
,	O
$	O
)	O
and	O
generates	O
the	O
tuple	O
(	O
K	O
e	O
,	O
V	O
e	O
)	O
where	O
K	O
e	O
=	O
(	O
k	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
n	O
)	O
and	O
V	O
e	O
=	O
(	O
v	O
1	O
,	O
.	O
.	O
.	O
,	O
v	O
n	O
)	O
such	O
that	O
,	O
k	O
i	O
=	O
[	O
0	B-DatasetName
h	O
,	O
0	B-DatasetName
s	O
;	O
0	B-DatasetName
h	O
,	O
−1	O
,	O
i	O
]	O
,	O
v	O
i	O
=	O
[	O
0	B-DatasetName
h	O
,	O
s	O
i	O
;	O
0	B-DatasetName
h	O
,	O
0	B-DatasetName
,	O
0	B-DatasetName
]	O
.	O
Construction	O
of	O
TDec	O
.	O
As	O
in	O
the	O
construction	O
of	O
TEnc	O
,	O
our	O
TDec	O
has	O
only	O
one	O
layer	O
.	O
Also	O
like	O
TEnc	O
,	O
the	O
decoder	O
-	O
decoder	O
attention	O
block	O
just	O
computes	O
the	O
identity	O
:	O
we	O
set	O
V	O
(	O
1	O
)	O
(	O
)	O
=	O
0	B-DatasetName
identically	O
,	O
and	O
use	O
the	O
residual	B-MethodName
connection	I-MethodName
so	O
that	O
p	O
t	O
=	O
y	O
t	O
.	O
For	O
t	O
≥	O
0	B-DatasetName
,	O
at	O
the	O
t	O
-	O
th	O
step	O
we	O
denote	O
the	O
input	O
to	O
the	O
decoder	O
as	O
y	O
t	O
=	O
ỹ	O
t	O
+	O
pos	O
(	O
t	O
)	O
.	O
Let	O
h	O
0	B-DatasetName
=	O
0	B-DatasetName
h	O
andỹ	O
0	B-DatasetName
=	O
0	B-DatasetName
.	O
We	O
will	O
show	O
by	O
induction	O
that	O
at	O
the	O
t	O
-	O
th	O
timestep	O
we	O
have	O
y	O
t	O
=	O
[	O
h	O
t	O
,	O
0	B-DatasetName
s	O
;	O
0	B-DatasetName
h	O
,	O
t	O
+	O
1	O
,	O
1	O
]	O
.	O
(	O
9	O
)	O
By	O
construction	O
,	O
this	O
is	O
true	O
for	O
t	O
=	O
0	B-DatasetName
:	O
y	O
0	B-DatasetName
=	O
[	O
0	B-DatasetName
h	O
,	O
0	B-DatasetName
s	O
;	O
0	B-DatasetName
h	O
,	O
1	O
,	O
1	O
]	O
.	O
Assuming	O
that	O
it	O
holds	O
for	O
t	O
,	O
we	O
show	O
it	O
for	O
t	O
+	O
1	O
.	O
By	O
Lemma	B-DatasetName
C.5	O
Att	O
(	O
p	O
t	O
,	O
K	O
e	O
,	O
V	O
e	O
)	O
=	O
[	O
0	B-DatasetName
h	O
,	O
v	O
t+1	O
;	O
0	B-DatasetName
h	O
,	O
0	B-DatasetName
,	O
0	B-DatasetName
]	O
.	O
(	O
10	O
)	O
Lemma	B-DatasetName
C.5	O
basically	O
shows	O
how	O
we	O
retrieve	O
the	O
input	O
s	O
t+1	O
at	O
the	O
relevant	O
step	O
for	O
further	O
computation	O
in	O
the	O
decoder	O
.	O
It	O
follows	O
that	O
a	O
t	O
=	O
Att	O
(	O
p	O
t	O
,	O
K	O
e	O
,	O
V	O
e	O
)	O
+	O
p	O
t	O
=	O
[	O
h	O
t	O
,	O
s	O
t+1	O
,	O
0	B-DatasetName
h	O
,	O
t	O
+	O
1	O
,	O
1	O
]	O
.	O
In	O
the	O
final	O
block	O
of	O
the	O
decoder	O
,	O
the	O
computation	O
for	O
RNN	O
takes	O
place	O
:	O
Lemma	B-DatasetName
C.4	O
.	O
There	O
exists	O
a	O
function	O
O	O
(	O
)	O
defined	O
by	O
feed	O
-	O
forward	O
network	O
such	O
that	O
,	O
O	O
(	O
a	O
t	O
)	O
=	O
[	O
(	O
h	O
t+1	O
−	O
h	O
t	O
)	O
,	O
−s	O
t+1	O
,	O
0	B-DatasetName
h	O
,	O
−	O
(	O
t	O
+	O
1	O
)	O
,	O
−1	O
]	O
,	O
where	O
W	O
h	O
,	O
W	O
x	O
and	O
b	O
denote	O
the	O
parameters	O
of	O
the	O
RNN	O
under	O
consideration	O
.	O

z	O
t	O
=	O
O	O
(	O
a	O
t	O
)	O
+	O
a	O
t	O
=	O
[	O
h	O
t+1	O
,	O
0	B-DatasetName
s	O
;	O
0	B-DatasetName
h	O
,	O
0	B-DatasetName
,	O
0	B-DatasetName
]	O
.	O
We	O
choose	O
the	O
function	O
F	O
for	O
our	O
decoder	O
to	O
be	O
the	O
identity	O
function	O
,	O
thereforeỹ	O
t+1	O
=	O
[	O
h	O
t+1	O
,	O
0	B-DatasetName
s	O
;	O
0	B-DatasetName
h	O
,	O
0	B-DatasetName
,	O
0	B-DatasetName
]	O
,	O
which	O
means	O
y	O
t+1	O
=	O
ỹ	O
t+1	O
+	O
pos	O
(	O
i	O
+	O
1	O
)	O
=	O
[	O
h	O
t+1	O
,	O
0	B-DatasetName
s	O
;	O
0	B-DatasetName
h	O
,	O
t	O
+	O
2	O
,	O
1	O
]	O
,	O
proving	O
our	O
induction	O
hypothesis	O
.	O

There	O
are	O
a	O
few	O
changes	O
in	O
the	O
architecture	O
of	O
the	O
Transformer	B-MethodName
to	O
obtain	O
directional	O
Transformer	B-MethodName
.	O
The	O
first	O
change	O
is	O
that	O
there	O
are	O
no	O
positional	O
encodings	O
and	O
thus	O
the	O
input	O
vector	O
x	O
i	O
only	O
consists	O
of	O
s	O
i	O
.	O
Similarly	O
,	O
there	O
are	O
no	O
positional	O
encodings	O
in	O
the	O
decoder	O
inputs	O
and	O
hence	O
y	O
t	O
=	O
ỹ	O
t	O
.	O
The	O
vectorỹ	O
is	O
the	O
output	O
representation	O
produced	O
at	O
the	O
previous	O
step	O
and	O
the	O
first	O
input	O
vector	O
to	O
the	O
decoderỹ	O
0	B-DatasetName
=	O
0	B-DatasetName
.	O
Instead	O
of	O
using	O
positional	O
encodings	O
,	O
we	O
apply	O
positional	O
masking	O
to	O
the	O
inputs	O
and	O
outputs	O
of	O
the	O
encoder	O
.	O
Thus	O
the	O
encoder	O
-	O
encoder	O
attention	O
in	O
(	O
5	O
)	O
is	O
redefined	O
as	O
a	O
(	O
+1	O
)	O
i	O
=	O
Att	O
(	O
Q	O
(	O
z	O
(	O
)	O
i	O
)	O
,	O
K	O
(	O
Z	O
(	O
)	O
i	O
)	O
,	O
V	O
(	O
Z	O
(	O
)	O
i	O
)	O
)	O
+	O
z	O
(	O
)	O
i	O
,	O
where	O
Z	O
(	O
0	B-DatasetName
)	O
=	O
X.	O
Similarly	O
the	O
decoder	O
-	O
encoder	O
attention	O
in	O
(	O
7	O
)	O
is	O
redefined	O
by	O
a	O
(	O
)	O
t	O
=	O
Att	O
(	O
p	O
(	O
)	O
t	O
,	O
K	O
e	O
t	O
,	O
V	O
e	O
t	O
)	O
+	O
p	O
(	O
)	O
t	O
,	O
where	O
in	O
a	O
(	O
)	O
t	O
denotes	O
the	O
layer	O
and	O
we	O
use	O
v	O
(	O
,	O
b	O
)	O
to	O
denote	O
any	O
intermediate	O
vector	O
being	O
used	O
in	O
-	O
th	O
layer	O
and	O
b	O
-	O
th	O
block	O
in	O
cases	O
where	O
the	O
same	O
symbol	O
is	O
used	O
in	O
multiple	O
blocks	O
in	O
the	O
same	O
layer	O
.	O
Theorem	O
D.1	O
.	O
RNNs	O
can	O
be	O
simulated	O
by	O
vanilla	O
Transformers	O
and	O
hence	O
the	O
class	O
of	O
vanilla	O
Transformers	O
is	O
Turing	O
-	O
complete	O
.	O
Proof	O
.	O
The	O
Transformer	B-MethodName
network	O
in	O
this	O
case	O
will	O
be	O
more	O
complex	O
than	O
the	O
construction	O
for	O
the	O
vanilla	O
case	O
.	O
The	O
encoder	O
remains	O
very	O
similar	O
,	O
but	O
the	O
decoder	O
is	O
different	O
and	O
has	O
two	O
layers	O
.	O
Embedding	O
.	O
We	O
will	O
construct	O
our	O
Transformer	B-MethodName
to	O
simulate	O
an	O
RNN	O
of	O
the	O
form	O
given	O
in	O
the	O
definition	O
with	O
the	O
recurrence	O
where	O
h	O
i	O
Q	O
d	O
h	O
,	O
s	O
Q	O
de	O
and	O
x	O
i	O
Q.	O
These	O
blocks	O
reserved	O
for	O
different	O
types	O
of	O
objects	O
.	O
The	O
where	O
W	O
i	O
Q	O
d×d	O
and	O
b	O
1	O
Q	O
d	O
.	O
Define	O
W	O
1	O
as	O
h	O
t	O
=	O
g	O
(	O
W	O
h	O
h	O
t−1	O
+	O
W	O
x	O
x	O
t	O
+	O
b	O
)	O
.	O
2d	O
h	O
d	O
e	O
d	O
ω	O
1	O
d	O
ω	O
d	O
ω	O
d	O
ω	O
2d	O
h	O
d	O
e	O
d	O
ω	O
−	O
1	O
1	O
1	O
d	O
ω	O
d	O
ω	O
d	O
ω	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
I	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
I	O
−I	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
1	O
2	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
1	O
2	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
I	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
I	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
I	O
Proof	O
.	O
Proof	O
is	O
very	O
similar	O
to	O
proof	O
of	O
lemma	B-DatasetName
C.4	O
.	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
and	O
b	O
1	O
=	O
0	B-DatasetName
,	O
then	O
σ	O
(	O
W	O
1	O
a	O
(	O
1	O
)	O
t	O
+	O
b	O
1	O
)	O
=	O
[	O
0	B-DatasetName
h	O
,	O
0	B-DatasetName
h	O
,	O
s	O
0	B-DatasetName
:	O
t	O
,	O
∆	O
t	O
,	O
1	O
2	O
t+1	O
,	O
ω	O
t	O
,	O
ω	O
t−1	O
,	O
ω	O
t−1	O
]	O
We	O
define	O
W	O
2	O
as	O
2d	O
h	O
d	O
e	O
d	O
ω−1	O
2	O
d	O
ω	O
d	O
ω	O
d	O
ω	O
2d	O
h	O
d	O
e	O
d	O
ω	O
−	O
1	O
1	O
1	O
d	O
ω	O
d	O
ω	O
d	O
ω	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
0	O

Lemma	B-DatasetName
D.2	O
.	O
There	O
exists	O
a	O
function	O
O	O
(	O
1	O
)	O
(	O
.	O
)	O
defined	O
by	O
feed	O
-	O
forward	O
network	O
such	O
that	O
,	O
Proof	O
.	O
We	O
define	O
the	O
feed	O
-	O
forward	O
network	O
O	O
(	O
1	O
)	O
(	O
.	O
)	O
such	O
that	O
We	O
define	O
the	O
feed	O
-	O
forward	O
network	O
O	O
(	O
a	O
t	O
)	O
as	O
follows	O
,	O

Sentiment	O
Word	O
Aware	O
Multimodal	O
Refinement	O
for	O
Multimodal	B-TaskName
Sentiment	I-TaskName
Analysis	I-TaskName
with	O
ASR	O
Errors	O

Multimodal	B-TaskName
sentiment	I-TaskName
analysis	I-TaskName
has	O
attracted	O
increasing	O
attention	O
and	O
lots	O
of	O
models	O
have	O
been	O
proposed	O
.	O
However	O
,	O
the	O
performance	O
of	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
decreases	O
sharply	O
when	O
they	O
are	O
deployed	O
in	O
the	O
real	O
world	O
.	O
We	O
find	O
that	O
the	O
main	O
reason	O
is	O
that	O
real	O
-	O
world	O
applications	O
can	O
only	O
access	O
the	O
text	O
outputs	O
by	O
the	O
automatic	B-TaskName
speech	I-TaskName
recognition	I-TaskName
(	O
ASR	O
)	O
models	O
,	O
which	O
may	O
be	O
with	O
errors	O
because	O
of	O
the	O
limitation	O
of	O
model	O
capacity	O
.	O
Through	O
further	O
analysis	O
of	O
the	O
ASR	O
outputs	O
,	O
we	O
find	O
that	O
in	O
some	O
cases	O
the	O
sentiment	O
words	O
,	O
the	O
key	O
sentiment	O
elements	O
in	O
the	O
textual	O
modality	O
,	O
are	O
recognized	O
as	O
other	O
words	O
,	O
which	O
makes	O
the	O
sentiment	O
of	O
the	O
text	O
change	O
and	O
hurts	O
the	O
performance	O
of	O
multimodal	B-TaskName
sentiment	I-TaskName
analysis	I-TaskName
models	O
directly	O
.	O
To	O
address	O
this	O
problem	O
,	O
we	O
propose	O
the	O
sentiment	O
word	O
aware	O
multimodal	O
refinement	O
model	O
(	O
SWRM	O
)	O
,	O
which	O
can	O
dynamically	O
refine	O
the	O
erroneous	O
sentiment	O
words	O
by	O
leveraging	O
multimodal	O
sentiment	O
clues	O
.	O
Specifically	O
,	O
we	O
first	O
use	O
the	O
sentiment	O
word	O
position	O
detection	O
module	O
to	O
obtain	O
the	O
most	O
possible	O
position	O
of	O
the	O
sentiment	O
word	O
in	O
the	O
text	O
and	O
then	O
utilize	O
the	O
multimodal	O
sentiment	O
word	O
refinement	O
module	O
to	O
dynamically	O
refine	O
the	O
sentiment	O
word	B-TaskName
embeddings	I-TaskName
.	O
The	O
refined	O
embeddings	O
are	O
taken	O
as	O
the	O
textual	O
inputs	O
of	O
the	O
multimodal	O
feature	O
fusion	O
module	O
to	O
predict	O
the	O
sentiment	O
labels	O
.	O
We	O
conduct	O
extensive	O
experiments	O
on	O
the	O
real	O
-	O
world	O
datasets	O
including	O
MOSI	B-DatasetName
-	O
Speechbrain	O
,	O
MOSI	B-DatasetName
-	O
IBM	O
,	O
and	O
MOSI	B-DatasetName
-	O
iFlytek	B-DatasetName
and	O
the	O
results	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
model	O
,	O
which	O
surpasses	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
on	O
three	O
datasets	O
.	O
Furthermore	O
,	O
our	O
approach	O
can	O
be	O
adapted	O
for	O
other	O
multimodal	O
feature	O
fusion	O
models	O
easily	O
1	O
.	O

Multimodal	B-TaskName
sentiment	I-TaskName
analysis	I-TaskName
has	O
gained	O
increasing	O
attention	O
from	O
the	O
community	O
recently	O
and	O
some	O
process	O
has	O
been	O
made	O
.	O
In	O
general	O
,	O
there	O
are	O
three	O
findings	O
presented	O
by	O
previous	O
work	O
.	O
Performing	O
the	O
cross	O
-	O
modal	O
alignment	O
is	O
helpful	O
for	O
multimodal	O
feature	O
fusion	O
.	O
considered	O
that	O
the	O
holistic	O
features	O
mainly	O
contain	O
global	O
information	O
,	O
which	O
may	O
fail	O
to	O
capture	O
local	O
information	O
.	O
Therefore	O
,	O
they	O
applied	O
the	O
force	O
-	O
alignment	O
to	O
align	O
the	O
visual	O
and	O
acoustic	O
features	O
with	O
the	O
words	O
and	O
further	O
obtained	O
the	O
word	O
-	O
level	O
features	O
.	O
To	O
effectively	O
fuse	O
them	O
,	O
they	O
proposed	O
the	O
GME	O
-	O
LSTM	B-MethodName
(	O
A	O
)	O
model	O
,	O
which	O
consists	O
of	O
two	O
modules	O
,	O
the	O
gated	O
multimodal	O
embedding	O
and	O
the	O
LSTM	B-MethodName
with	O
the	O
temporal	B-MethodName
attention	I-MethodName
.	O
However	O
,	O
obtaining	O
the	O
word	O
-	O
level	O
features	O
needs	O
to	O
perform	O
the	O
force	O
-	O
alignment	O
,	O
which	O
is	O
time	O
-	O
consuming	O
.	O
To	O
address	O
it	O
,	O
proposed	O
the	O
MulT	O
model	O
,	O
which	O
uses	O
the	O
crossmodal	O
attention	O
to	O
align	O
different	O
modal	O
features	O
implicitly	O
.	O
Instead	O
of	O
performing	O
the	O
alignment	O
in	O
the	O
time	O
dimension	O
,	O
some	O
works	O
focusing	O
on	O
semantic	O
alignment	O
.	O
Hazarika	O
et	O
al	O
(	O
2020	O
)	O
considered	O
that	O
the	O
semantic	O
gaps	O
between	O
heterogeneous	O
data	O
could	O
hurt	O
the	O
model	O
performance	O
and	O
proposed	O
the	O
MISA	O
model	O
,	O
which	O
maps	O
the	O
different	O
modal	O
data	O
into	O
a	O
shared	O
space	O
before	O
multimodal	O
feature	O
fusion	O
.	O
first	O
utilized	O
the	O
cross	O
-	O
modal	O
prediction	O
task	O
to	O
distinguish	O
the	O
shared	O
and	O
private	O
semantics	O
of	O
non	O
-	O
textual	O
modalities	O
compared	O
to	O
the	O
textual	O
modality	O
and	O
then	O
fuse	O
them	O
.	O
The	O
above	O
works	O
show	O
that	O
performing	O
the	O
cross	O
-	O
modal	O
alignment	O
is	O
helpful	O
for	O
multimodal	O
feature	O
fusion	O
.	O
Training	O
the	O
MSA	O
models	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
manner	O
is	O
more	O
effective	O
.	O
Most	O
of	O
the	O
previous	O
studies	O
adopt	O
a	O
two	O
-	O
phase	O
pipeline	O
,	O
first	O
extracting	O
unimodal	O
features	O
and	O
then	O
fusing	O
them	O
.	O
Dai	O
et	O
al	O
(	O
2021	O
)	O
considered	O
that	O
it	O
may	O
lead	O
to	O
suboptimal	O
performance	O
since	O
the	O
extracted	O
unimodal	O
features	O
are	O
fixed	O
and	O
can	O
not	O
be	O
further	O
improved	O
benefiting	O
from	O
the	O
downstream	O
supervisory	O
signals	O
.	O
Therefore	O
,	O
they	O
proposed	O
the	O
multimodal	O
endto	O
-	O
end	O
sparse	O
model	O
,	O
which	O
can	O
optimize	O
the	O
unimodal	O
feature	O
extraction	O
and	O
multimodal	O
feature	O
fusion	O
jointly	O
.	O
The	O
experimental	O
results	O
on	O
the	O
multimodal	O
emotion	B-DatasetName
detection	O
task	O
show	O
that	O
training	O
the	O
models	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
manner	O
can	O
obtain	O
better	O
results	O
than	O
the	O
pipeline	O
models	O
.	O
Leveraging	O
the	O
unimodal	O
sentiment	O
labels	O
to	O
learn	O
more	O
informative	O
unimodal	O
representations	O
is	O
useful	O
for	O
multimodal	O
feature	O
fusion	O
.	O
Yu	O
et	O
al	O
(	O
2020	O
)	O
considered	O
that	O
introducing	O
the	O
unimodal	O
sentiment	O
labels	O
can	O
help	O
the	O
model	O
capture	O
the	O
unimodal	O
sentiment	O
information	O
and	O
model	O
the	O
difference	O
between	O
modalities	O
.	O
Motivated	O
by	O
it	O
,	O
they	O
built	O
the	O
CH	B-DatasetName
-	I-DatasetName
SIMS	I-DatasetName
dataset	O
,	O
which	O
contains	O
not	O
only	O
the	O
multimodal	O
sentiment	O
labels	O
but	O
also	O
unimodal	O
sentiment	O
labels	O
.	O
And	O
based	O
on	O
it	O
,	O
they	O
proposed	O
a	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
framework	O
to	O
leverage	O
two	O
types	O
of	O
sentiment	O
labels	O
simultaneously	O
.	O
However	O
,	O
this	O
method	O
needs	O
unimodal	O
labels	O
,	O
which	O
is	O
absent	O
for	O
most	O
of	O
the	O
existing	O
datasets	O
.	O
To	O
address	O
it	O
,	O
Yu	O
et	O
al	O
(	O
2021	O
)	O
proposed	O
the	O
Self	O
-	O
MM	O
model	O
,	O
which	O
first	O
generates	O
the	O
unimodal	O
labels	O
by	O
utilizing	O
the	O
relationship	O
between	O
the	O
unimodal	O
and	O
multimodal	O
labels	O
and	O
then	O
uses	O
the	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
to	O
train	O
the	O
model	O
.	O
These	O
two	O
works	O
both	O
address	O
the	O
usefulness	O
of	O
introducing	O
unimodal	O
labels	O
.	O
However	O
,	O
even	O
though	O
lots	O
of	O
models	O
are	O
proposed	O
and	O
obtain	O
promising	O
results	O
on	O
the	B-DatasetName
benchmark	I-DatasetName
datasets	O
,	O
there	O
are	O
few	O
works	O
considering	O
the	O
noisy	O
inputs	O
when	O
the	O
MSA	O
models	O
are	O
deployed	O
in	O
the	O
real	O
world	O
.	O
presented	O
the	O
Gated	O
Multimodal	O
Embedding	O
to	O
filter	O
out	O
the	O
noises	O
from	O
the	O
acoustic	O
and	O
visual	O
data	O
.	O
Pham	O
et	O
al	O
(	O
2019	O
)	O
considered	O
that	O
visual	O
and	O
acoustic	O
data	O
may	O
be	O
absent	O
and	O
proposed	O
the	O
MCTN	O
model	O
to	O
handle	O
it	O
.	O
and	O
Mittal	O
et	O
al	O
(	O
2020	O
)	O
also	O
mainly	O
focused	O
on	O
dealing	O
with	O
the	O
noises	O
introduced	O
by	O
the	O
visual	O
and	O
acoustic	O
data	O
,	O
and	O
their	O
models	O
are	O
based	O
on	O
the	O
word	O
-	O
level	O
features	O
,	O
which	O
are	O
obtained	O
by	O
aligning	O
the	O
audios	O
with	O
the	O
gold	O
texts	O
.	O
There	O
is	O
only	O
one	O
work	O
(	O
Dumpala	O
et	O
al	O
,	O
2018	O
)	O
considering	O
that	O
the	O
texts	O
are	O
output	O
by	O
the	O
ASR	O
models	O
,	O
which	O
may	O
be	O
erroneous	O
.	O
But	O
this	O
work	O
does	O
not	O
study	O
how	O
do	O
the	O
ASR	O
errors	O
affect	O
the	O
MSA	O
models	O
and	O
does	O
not	O
evaluate	O
the	O
SOTA	O
MSA	O
models	O
on	O
the	O
datasets	O
.	O
Besides	O
,	O
the	O
proposed	O
model	O
needs	O
the	O
gold	O
texts	O
when	O
training	O
,	O
which	O
is	O
time	O
-	O
consuming	O
and	O
labor	O
-	O
consuming	O
.	O
Comparing	O
to	O
the	O
above	O
works	O
,	O
we	O
evaluate	O
the	O
SOTA	O
MSA	O
models	O
on	O
the	O
real	O
-	O
world	O
datasets	O
and	O
observe	O
that	O
the	O
performance	O
of	O
models	O
decreases	O
sharply	O
because	O
of	O
the	O
erroneous	O
ASR	O
texts	O
.	O
Through	O
in	O
-	O
depth	O
analysis	O
of	O
the	O
ASR	O
outputs	O
,	O
we	O
find	O
the	O
sentiment	O
word	O
substitution	O
error	O
in	O
the	O
ASR	O
texts	O
could	O
hurt	O
the	O
MSA	O
models	O
directly	O
.	O
To	O
address	O
it	O
,	O
we	O
propose	O
the	O
sentiment	O
word	O
aware	O
multimodal	O
refinement	O
model	O
,	O
which	O
only	O
uses	O
the	O
ASR	O
texts	O
in	O
the	O
training	O
and	O
testing	O
phrases	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
the	O
sentiment	O
word	O
aware	O
multimodal	O
refinement	O
model	O
in	O
detail	O
.	O
An	O
illustration	O
of	O
our	O
proposed	O
model	O
is	O
given	O
in	O
Figure	O
2	O
.	O
Our	O
model	O
consists	O
of	O
three	O
modules	O
including	O
the	O
sentiment	O
word	O
location	O
module	O
,	O
multimodal	O
sentiment	O
word	O
refinement	O
module	O
,	O
and	O
multimodal	O
feature	O
fusion	O
module	O
.	O
We	O
first	O
use	O
the	O
sentiment	O
word	O
location	O
module	O
to	O
detect	O
the	O
possible	O
positions	O
of	O
sentiment	O
words	O
and	O
then	O
utilize	O
the	O
multimodal	O
sentiment	O
word	O
refinement	O
module	O
to	O
dynamically	O
refine	O
the	O
word	B-TaskName
embeddings	I-TaskName
in	O
the	O
detected	O
positions	O
.	O
Finally	O
,	O
the	O
refined	O
word	B-TaskName
embeddings	I-TaskName
are	O
fed	O
into	O
the	O
multimodal	O
feature	O
fusion	O
module	O
to	O
predict	O
the	O
final	O
sentiment	O
labels	O
.	O

The	O
core	O
idea	O
of	O
the	O
sentiment	O
word	O
position	O
detection	O
module	O
is	O
to	O
find	O
out	O
the	O
possible	O
positions	O
of	O
sentiment	O
words	O
in	O
the	O
ASR	O
texts	O
.	O
Note	O
that	O
,	O
it	O
is	O
different	O
from	O
locating	O
sentiment	O
words	O
depending	O
on	O
the	O
word	O
semantics	O
,	O
since	O
the	O
ASR	O
models	O
may	O
recognize	O
a	O
sentiment	O
word	O
as	O
a	O
neutral	O
word	O
,	O
which	O
makes	O
it	O
hard	O
to	O
locate	O
correctly	O
.	O
For	O
example	O
,	O
given	O
a	O
gold	O
text	O
"	O
And	O
I	O
was	O
really	O
upset	O
about	O
it	O
"	O
,	O
the	O
ASR	O
model	O
recognizes	O
it	O
as	O
"	O
And	O
I	O
was	O
really	O
set	O
about	O
it	O
"	O
.	O
It	O
is	O
easy	O
for	O
the	O
model	O
to	O
label	O
the	O
word	O
"	O
set	O
"	O
as	O
a	O
neutral	O
word	O
.	O
Therefore	O
,	O
we	O
choose	O
to	O
detect	O
the	O
position	O
of	O
the	O
sentiment	O
words	O
instead	O
of	O
locating	O
them	O
.	O
To	O
achieve	O
it	O
,	O
we	O
consider	O
adopting	O
a	O
powerful	O
language	O
model	O
,	O
since	O
the	O
language	O
model	O
can	O
model	O
the	O
context	O
information	O
of	O
the	O
sentiment	O
words	O
such	O
as	O
syntactic	O
and	O
grammatical	O
information	O
and	O
predict	O
the	O
appropriate	O
words	O
for	O
the	O
target	O
position	O
.	O
Specifically	O
,	O
we	O
choose	O
the	O
BERT	B-MethodName
model	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
as	O
our	O
language	O
model	O
since	O
the	O
masked	B-TaskName
language	I-TaskName
modeling	I-TaskName
pretraining	O
objective	O
meets	O
our	O
needs	O
perfectly	O
.	O
Given	O
the	O
sentence	O
{	O
w	O
1	O
,	O
w	O
2	O
,	O
...	O
,	O
w	O
n	O
l	O
}	O
,	O
we	O
first	O
mask	O
each	O
word	O
w	O
i	O
in	O
the	O
sentence	O
sequentially	O
,	O
and	O
in	O
practice	O
,	O
we	O
replace	O
the	O
word	O
with	O
the	O
special	O
word	O
[	O
MASK	O
]	O
.	O
For	O
example	O
,	O
we	O
mask	O
the	O
first	O
word	O
in	O
the	O
sentence	O
and	O
obtain	O
{	O
[	O
MASK	O
]	O
,	O
w	O
2	O
,	O
...	O
,	O
w	O
n	O
l	O
}	O
.	O
And	O
then	O
we	O
use	O
the	O
BERT	B-MethodName
model	O
to	O
predict	O
the	O
possible	O
words	O
in	O
the	O
position	O
of	O
the	O
masked	O
word	O
.	O
We	O
sort	O
the	O
predicted	O
candidate	O
words	O
by	O
the	O
prediction	O
probabilities	O
and	O
get	O
the	O
Top	O
-	O
k	O
candidate	O
words	O
C	O
i	O
=	O
{	O
c	O
i	O
1	O
,	O
c	O
i	O
2	O
,	O
...	O
,	O
c	O
i	O
k	O
}	O
.	O
Next	O
,	O
we	O
distinguish	O
the	O
sentiment	O
words	O
from	O
the	O
candidates	O
using	O
the	O
sentiment	O
lexicons	O
(	O
Hu	O
and	O
Liu	O
,	O
2004	O
;	O
Wilson	O
et	O
al	O
,	O
2005	O
)	O
and	O
k	O
i	O
is	O
the	O
number	O
of	O
selected	O
sentiment	O
words	O
corresponding	O
to	O
the	O
position	O
i.	O
The	O
larger	O
the	O
number	O
is	O
,	O
the	O
more	O
possible	O
the	O
position	O
is	O
.	O
And	O
we	O
obtain	O
the	O
most	O
possible	O
position	O
of	O
sentiment	O
word	O
,	O
s	O
=	O
arg	O
max	O
(	O
{	O
k	O
1	O
,	O
k	O
2	O
,	O
...	O
,	O
k	O
n	O
l	O
}	O
)	O
.	O
Considering	O
that	O
in	O
some	O
cases	O
there	O
is	O
not	O
a	O
sentiment	O
word	O
in	O
the	O
sentence	O
,	O
we	O
use	O
a	O
sentiment	O
threshold	O
to	O
filter	O
out	O
the	O
impossible	O
ones	O
.	O
In	O
practice	O
,	O
we	O
use	O
the	O
gate	O
mask	O
p	O
to	O
record	O
it	O
,	O
and	O
p	O
is	O
1	O
if	O
k	O
s	O
is	O
larger	O
than	O
k/2	O
and	O
0	B-DatasetName
otherwise	O
.	O

In	O
order	O
to	O
reduce	O
the	O
negative	O
effects	O
of	O
the	O
ASR	O
errors	O
,	O
we	O
propose	O
the	O
multimodal	O
sentiment	O
word	O
refinement	O
module	O
,	O
in	O
which	O
we	O
refine	O
the	O
word	B-TaskName
embeddings	I-TaskName
of	O
sentiment	O
words	O
from	O
two	O
aspects	O
.	O
One	O
is	O
that	O
we	O
uses	O
the	O
multimodal	O
gating	O
network	O
to	O
filter	O
out	O
the	O
useless	O
information	O
from	O
the	O
input	O
word	B-TaskName
embeddings	I-TaskName
.	O
The	O
other	O
one	O
is	O
that	O
we	O
design	O
the	O
multimodal	O
sentiment	O
attention	O
network	O
to	O
incorporate	O
the	O
useful	O
information	O
from	O
candidate	O
words	O
generated	O
by	O
the	O
BERT	B-MethodName
model	O
.	O
Given	O
an	O
utterance	O
,	O
which	O
includes	O
three	O
modal	O
unaligned	O
features	O
,	O
word	B-TaskName
embeddings	I-TaskName
,	O
acoustic	O
features	O
,	O
and	O
visual	O
features	O
,	O
we	O
denote	O
them	O
as	O
x	O
i	O
=	O
{	O
x	O
i	O
t	O
:	O
1	O
≤	O
t	O
≤	O
n	O
i	O
,	O
x	O
i	O
t	O
R	O
d	O
i	O
x	O
}	O
,	O
i	O
{	O
l	O
,	O
v	O
,	O
a	O
}	O
.	O
To	O
obtain	O
the	O
multimodal	O
information	O
corresponding	O
to	O
each	O
word	O
,	O
We	O
utilize	O
the	O
pseudo	O
-	O
alignment	O
method	O
to	O
align	O
the	O
features	O
.	O
We	O
split	O
the	O
the	O
acoustic	O
and	O
visual	O
features	O
into	O
non	O
-	O
overlapping	O
feature	O
groups	O
,	O
of	O
which	O
lengths	O
are	O
na	O
n	O
l	O
and	O
nv	O
n	O
l	O
respectively	O
,	O
and	O
average	O
the	O
features	O
in	O
each	O
group	O
and	O
obtain	O
the	O
aligned	O
features	O
,	O
u	O
i	O
=	O
{	O
u	O
i	O
t	O
:	O
1	O
≤	O
t	O
≤	O
n	O
l	O
,	O
u	O
i	O
t	O
R	O
d	O
i	O
x	O
}	O
,	O
i	O
{	O
v	O
,	O
a	O
}	O
.	O
To	O
obtain	O
the	O
context	O
-	O
aware	O
representations	O
,	O
we	O
apply	O
the	O
BERT	B-MethodName
model	O
and	O
LSTM	B-MethodName
networks	O
to	O
encode	O
the	O
features	O
,	O
producing	O
h	O
i	O
=	O
{	O
h	O
i	O
t	O
:	O
1	O
≤	O
t	O
≤	O
n	O
l	O
,	O
h	O
i	O
t	O
R	O
d	O
i	O
h	O
}	O
,	O
i	O
{	O
v	O
,	O
a	O
,	O
l	O
}	O
.	O
Besides	O
,	O
we	O
also	O
use	O
an	O
LSTM	B-MethodName
network	O
to	O
fuse	O
the	O
acoustic	O
and	O
visual	O
features	O
for	O
capturing	O
high	O
-	O
level	O
sentiment	O
semantics	O
and	O
obtain	O
h	O
va	O
=	O
{	O
h	O
va	O
.	O
network	O
to	O
filter	O
the	O
word	O
embedding	O
,	O
which	O
is	O
implemented	O
by	O
a	O
non	O
-	O
linear	B-MethodName
layer	I-MethodName
.	O
The	O
motivation	O
is	O
that	O
the	O
ASR	O
model	O
may	O
recognize	O
incorrectly	O
the	O
sentiment	O
word	O
resulting	O
in	O
the	O
corrupted	O
sentiment	O
semantics	O
of	O
the	O
text	O
.	O
Therefore	O
,	O
we	O
leverage	O
the	O
multimodal	O
sentiment	O
information	O
to	O
decide	O
how	O
much	O
information	O
of	O
the	O
input	O
word	O
embedding	O
to	O
pass	O
.	O
Specifically	O
,	O
we	O
concatenate	O
the	O
unimodal	O
context	O
-	O
aware	O
representations	O
,	O
h	O
l	O
s	O
,	O
h	O
v	O
s	O
,	O
h	O
a	O
s	O
,	O
and	O
bimodal	O
representation	O
h	O
va	O
s	O
in	O
the	O
detected	O
position	O
s	O
and	O
feed	O
them	O
into	O
a	O
non	O
-	O
linear	O
neural	O
network	O
,	O
producing	O
the	O
gate	O
value	O
g	O
v	O
.	O
And	O
then	O
the	O
gate	O
value	O
is	O
used	O
to	O
filter	O
out	O
the	O
useless	O
information	O
from	O
the	O
word	O
embedding	O
.	O
To	O
make	O
the	O
model	O
ignore	O
the	O
impossible	O
one	O
,	O
we	O
use	O
the	O
gate	O
mask	O
p	O
to	O
achieve	O
it	O
.	O
t	O
:	O
1	O
≤	O
t	O
≤	O
n	O
l	O
,	O
h	O
va	O
t	O
R	O
d	O
va	O
h	O
}	O
.	O
h	O
l	O
=	O
BERT	B-MethodName
(	O
x	O
l	O
)	O
h	O
v	O
=	O
LSTM	B-MethodName
v	O
(	O
u	O
v	O
)	O
h	O
a	O
=	O
LSTM	B-MethodName
a	O
(	O
u	O
a	O
)	O
h	O
va	O
=	O
LSTM	B-MethodName
va	O
(	O
[	O
u	O
v	O
;	O
u	O
a	O
]	O
)	O
(	O
1	O
g	O
v	O
=	O
Sigmoid	O
(	O
W	O
1	O
(	O
[	O
h	O
l	O
s	O
;	O
h	O
v	O
s	O
;	O
h	O
a	O
s	O
;	O
h	O
va	O
s	O
]	O
)	O
+	O
b	O
1	O
)	O
r	O
v	O
=	O
(	O
1	O
−	O
g	O
v	O
p	O
)	O
x	O
l	O
s	O
(	O
2	O
)	O
where	O
W	O
1	O
R	O
1×	O
i	O
{	O
l	O
,	O
v	O
,	O
a	O
,	O
va	O
}	O
d	O
i	O
h	O
,	O
b	O
1	O
R	O
1	O
are	O
the	O
parameters	O
of	O
the	O
multimodal	O
gating	O
network	O
.	O
Furthermore	O
,	O
we	O
propose	O
a	O
novel	O
multimodal	O
sentiment	O
word	O
attention	O
network	O
to	O
leverage	O
the	O
sentiment	O
-	O
related	O
information	O
from	O
the	O
candidate	O
words	O
,	O
more	O
than	O
half	O
of	O
which	O
are	O
sentiment	O
words	O
,	O
generated	O
by	O
the	O
BERT	B-MethodName
model	O
to	O
complement	O
the	O
word	B-TaskName
embeddings	I-TaskName
.	O
For	O
example	O
,	O
the	O
ASR	O
model	O
recognizes	O
the	O
"	O
upset	O
"	O
as	O
"	O
set	O
"	O
,	O
we	O
first	O
want	O
to	O
remove	O
the	O
useless	O
information	O
of	O
"	O
set	O
"	O
and	O
then	O
incorporate	O
the	O
information	O
of	O
negative	O
sentiment	O
words	O
to	O
reconstruct	O
the	O
original	O
sentiment	O
semantics	O
.	O
We	O
use	O
a	O
linear	B-MethodName
layer	I-MethodName
to	O
implement	O
the	O
multimodal	O
sentiment	O
word	O
attention	O
network	O
.	O
We	O
first	O
concatenate	O
the	O
word	O
embedding	O
x	O
c	O
s	O
t	O
of	O
the	O
candidate	O
word	O
c	O
s	O
t	O
and	O
multimodal	O
representations	O
,	O
h	O
v	O
s	O
,	O
h	O
a	O
s	O
,	O
and	O
h	O
va	O
s	O
at	O
the	O
most	O
possible	O
time	O
step	O
s.	O
Then	O
,	O
we	O
pass	O
them	O
to	O
the	O
linear	B-MethodName
layer	I-MethodName
and	O
obtain	O
the	O
attention	O
score	O
g	O
e	O
t	O
.	O
The	O
attention	O
scores	O
are	O
fed	O
into	O
a	O
softmax	B-MethodName
function	O
to	O
obtain	O
the	O
attention	O
weights	O
.	O
Finally	O
,	O
we	O
apply	O
the	O
weights	O
to	O
the	O
candidate	O
word	B-TaskName
embeddings	I-TaskName
and	O
get	O
the	O
sentiment	O
embedding	O
r	O
e	O
.	O
where	O
W	O
2	O
R	O
1×	O
(	O
d	O
l	O
x	O
+	O
i	O
{	O
v	O
,	O
a	O
,	O
va	O
}	O
d	O
i	O
h	O
)	O
,	O
b	O
2	O
R	O
1	O
are	O
the	O
parameters	O
of	O
the	O
multimodal	O
sentiment	O
word	O
attention	O
network	O
.	O
In	O
addition	O
,	O
there	O
may	O
not	O
be	O
suitable	O
words	O
in	O
the	O
candidate	O
words	O
.	O
Hence	O
,	O
we	O
incorporate	O
the	O
embedding	O
of	O
the	O
special	O
word	O
[	O
MASK	O
]	O
,	O
x	O
mask	O
,	O
to	O
let	O
the	O
BERT	B-MethodName
model	O
handle	O
this	O
problem	O
based	O
on	O
the	O
context	O
.	O
We	O
then	O
design	O
an	O
aggregation	O
network	O
to	O
balance	O
the	O
contributions	O
of	O
the	O
special	O
word	O
embedding	O
x	O
mask	O
and	O
the	O
sentiment	O
embedding	O
r	O
e	O
.	O
Finally	O
,	O
we	O
add	O
the	O
r	O
add	O
to	O
the	O
filtered	O
word	O
embedding	O
u	O
l	O
s	O
and	O
obtain	O
the	O
refined	O
word	O
embedding	O
r	O
l	O
for	O
the	O
target	O
word	O
.	O
g	O
mask	O
=	O
Sigmoid	O
(	O
W	O
3	O
(	O
[	O
r	O
e	O
;	O
x	O
mask	O
]	O
)	O
+	O
b	O
3	O
)	O
r	O
add	O
=	O
g	O
mask	O
r	O
e	O
+	O
(	O
1	O
−	O
g	O
mask	O
)	O
x	O
mask	O
r	O
l	O
=	O
(	O
g	O
v	O
p	O
)	O
r	O
add	O
+	O
r	O
v	O
(	O
4	O
)	O
where	O
W	O
3	O
R	O
1×2d	O
l	O
x	O
,	O
b	O
3	O
R	O
1	O
are	O
the	O
trainable	O
parameters	O
.	O

We	O
describe	O
our	O
multimodal	O
feature	O
fusion	O
module	O
in	O
the	O
section	O
and	O
it	O
is	O
noted	O
that	O
our	O
proposed	O
refinement	O
approach	O
only	O
modifies	O
the	O
textual	O
input	O
token	O
embeddings	O
,	O
which	O
makes	O
it	O
easy	O
to	O
be	O
adapted	O
for	O
other	O
multimodal	O
feature	O
fusion	O
models	O
based	O
on	O
BERT	B-MethodName
,	O
such	O
as	O
MISA	O
(	O
Hazarika	O
et	O
al	O
,	O
2020	O
)	O
.	O
We	O
first	O
use	O
the	O
BERT	B-MethodName
model	O
to	O
encode	O
the	O
refined	O
word	B-TaskName
embeddings	I-TaskName
z	O
l	O
=	O
{	O
x	O
l	O
1	O
,	O
x	O
l	O
2	O
,	O
...	O
,	O
r	O
l	O
,	O
..	O
,	O
x	O
l	O
n	O
l	O
}	O
and	O
take	O
the	O
representation	O
of	O
[	O
CLS	O
]	O
as	O
the	O
textual	O
representation	O
,	O
which	O
is	O
denoted	O
as	O
v	O
l	O
.	O
And	O
then	O
we	O
use	O
two	O
LSTM	B-MethodName
networks	O
to	O
encode	O
the	O
visual	O
and	O
acoustic	O
features	O
and	O
take	O
the	O
representations	O
of	O
the	O
first	O
words	O
as	O
the	O
visual	O
representation	O
v	O
v	O
and	O
acoustic	O
representation	O
v	O
a	O
.	O
Finally	O
,	O
we	O
fuse	O
them	O
using	O
a	O
non	O
-	O
linear	B-MethodName
layer	I-MethodName
to	O
capture	O
the	O
interactions	O
between	O
them	O
.	O
v	O
l	O
=	O
BERT	B-MethodName
textual	O
(	O
z	O
l	O
)	O
v	O
v	O
=	O
LSTM	B-MethodName
visual	O
(	O
x	O
v	O
)	O
v	O
a	O
=	O
LSTM	B-MethodName
acoustic	O
(	O
x	O
a	O
)	O
v	O
f	O
=	O
Relu	B-MethodName
(	O
W	O
4	O
(	O
[	O
v	O
l	O
;	O
v	O
v	O
;	O
v	O
a	O
]	O
)	O
+	O
b	O
4	O
)	O
(	O
5	O
)	O
where	O
W	O
4	O
R	O
d	O
f	O
v	O
×	O
(	O
d	O
l	O
v	O
+	O
d	O
a	O
v	O
+	O
d	O
v	O
v	O
)	O
,	O
b	O
4	O
R	O
d	O
f	O
v	O
are	O
the	O
trainable	O
parameters	O
of	O
the	O
fusion	O
network	O
.	O
We	O
utilize	O
a	O
linear	B-MethodName
layer	I-MethodName
to	O
predict	O
the	O
final	O
sentiment	O
regression	O
labels	O
.	O
p	O
f	O
=	O
W	O
5	O
v	O
f	O
+	O
b	O
5	O
(	O
6	O
)	O
where	O
W	O
5	O
R	O
1×d	O
f	O
v	O
,	O
b	O
5	O
R	O
1	O
are	O
the	O
trainable	O
parameters	O
of	O
the	O
prediction	O
network	O
.	O
Besides	O
,	O
to	O
enhance	O
the	O
model	O
to	O
capture	O
unimodal	O
sentiment	O
information	O
,	O
we	O
use	O
the	O
Unimodal	O
Label	O
Generation	O
Module	O
(	O
ULGM	O
)	O
(	O
Yu	O
et	O
al	O
,	O
2021	O
)	O
to	O
generate	O
pseudo	O
unimodal	O
sentiment	O
labels	O
and	O
adopt	O
them	O
to	O
train	O
our	O
model	O
in	O
a	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
manner	O
.	O
For	O
more	O
details	O
,	O
we	O
refer	O
you	O
to	O
Yu	O
et	O
al	O
(	O
2021	O
)	O
.	O

We	O
compare	O
our	O
proposed	O
model	O
with	O
the	O
following	O
baselines	O
4	O
.	O
TFN	O
uses	O
the	O
three	O
-	O
fold	O
Cartesian	O
product	O
to	O
capture	O
unimodal	O
,	O
bimodal	O
,	O
and	O
trimodal	O
interactions	O
.	O
LMF	O
(	O
Liu	O
et	O
al	O
,	O
2018	O
)	O
uses	O
the	O
low	O
-	O
rank	O
tensors	O
to	O
accelerate	O
the	O
multimodal	O
feature	O
fusion	O
process	O
.	O
MulT	O
uses	O
the	O
cross	O
-	O
modal	O
transformers	O
to	O
fuse	O
multimodal	O
features	O
.	O
MISA	O
(	O
Hazarika	O
et	O
al	O
,	O
2020	O
)	O
adopts	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
to	O
map	O
different	O
modal	O
features	O
into	O
a	O
shared	O
subspace	O
.	O
Self	O
-	O
MM	O
(	O
Yu	O
et	O
al	O
,	O
2021	O
)	O
first	O
generates	O
the	O
pseudo	O
unimodal	O
sentiment	O
labels	O
and	O
then	O
adopts	O
them	O
to	O
train	O
the	O
model	O
in	O
a	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
manner	O
.	O
5	O
Results	O
and	O
Analysis	O

In	O
And	O
we	O
also	O
list	O
the	O
results	O
of	O
the	O
SOTA	O
model	O
,	O
Self	O
-	O
MM	O
,	O
on	O
the	O
original	O
MOSI	B-DatasetName
dataset	O
in	O
the	O
last	O
row	O
of	O
the	O
table	O
for	O
the	O
performance	O
comparison	O
between	O
Self	O
-	O
MM	O
in	O
the	O
ideal	O
world	O
and	O
real	O
world	O
.	O
As	O
we	O
can	O
see	O
from	O
the	O
results	O
,	O
Self	O
-	O
MM	O
obtains	O
the	O
best	O
results	O
on	O
the	O
MOSI	B-DatasetName
-	O
Gold	O
dataset	O
than	O
the	O
other	O
datasets	O
,	O
which	O
demonstrates	O
that	O
the	O
ASR	O
errors	O
hurt	O
the	O
MSA	O
models	O
.	O
We	O
also	O
observe	O
that	O
the	O
better	O
ASR	O
model	O
can	O
help	O
the	O
MSA	O
models	O
achieve	O
better	O
performance	O
.	O
But	O
it	O
should	O
be	O
noted	O
that	O
,	O
according	O
to	O
the	O
analysis	O
in	O
the	O
previous	O
section	O
,	O
current	O
ASR	O
models	O
still	O
can	O
not	O
produce	O
satisfactory	O
results	O
for	O
the	O
MSA	O
models	O
in	O
the	O
real	O
world	O
.	O
Comparison	O
between	O
the	O
feature	O
-	O
based	O
models	O
including	O
TFN	O
,	O
LMF	O
,	O
and	O
MulT	O
and	O
finetuningbased	O
baselines	O
such	O
as	O
MISA	O
and	O
Self	O
-	O
MM	O
,	O
we	O
can	O
find	O
that	O
finetuning	O
-	O
based	O
models	O
obtain	O
better	O
results	O
.	O
We	O
consider	O
that	O
the	O
finetuning	O
-	O
based	O
models	O
can	O
adapt	O
the	O
BERT	B-MethodName
encoder	O
to	O
the	O
target	O
task	O
and	O
learning	O
more	O
informative	O
textual	O
representations	O
,	O
which	O
also	O
makes	O
them	O
benefit	O
more	O
as	O
the	O
quality	O
of	O
texts	O
increases	O
.	O
Comparing	O
to	O
the	O
baselines	O
especially	O
Self	O
-	O
MM	O
,	O
our	O
model	O
achieves	O
better	O
performance	O
in	O
all	O
evaluation	O
metrics	O
since	O
our	O
model	O
can	O
detect	O
the	O
substitution	O
error	O
of	O
the	O
sentiment	O
words	O
and	O
then	O
refine	O
the	O
word	B-TaskName
embeddings	I-TaskName
to	O
reconstruct	O
the	O
sentiment	O
semantics	O
in	O
the	O
textual	O
modality	O
by	O
filtering	O
out	O
useless	O
information	O
from	O
the	O
input	O
words	O
and	O
incorporating	O
useful	O
information	O
from	O
the	O
candidate	O
words	O
generated	O
by	O
the	O
language	O
model	O
.	O
We	O
also	O
observe	O
that	O
the	O
improvement	O
of	O
our	O
model	O
compared	O
with	O
Self	O
-	O
MM	O
on	O
MOSI	B-DatasetName
-	O
iFlytek	B-DatasetName
is	O
smaller	O
.	O
We	O
consider	O
that	O
the	O
main	O
reason	O
is	O
fewer	O
sentiment	O
word	O
substitution	O
errors	O
on	O
MOSI	B-DatasetName
-	O
iFlytek	B-DatasetName
.	O

To	O
have	O
an	O
intuitive	O
understanding	O
of	O
our	O
proposed	O
model	O
,	O
we	O
show	O
a	O
case	O
in	O
Figure	O
3	O
.	O
We	O
can	O
see	O
that	O
our	O
model	O
first	O
detects	O
the	O
most	O
possible	O
position	O
based	O
on	O
the	O
context	O
and	O
then	O
finds	O
that	O
the	O
input	O
word	O
in	O
the	O
position	O
may	O
be	O
recognized	O
incorrectly	O
since	O
there	O
is	O
a	O
mismatch	O
between	O
the	O
negative	O
word	O
"	O
cruel	O
"	O
and	O
either	O
the	O
smile	O
or	O
the	O
excited	O
tone	O
.	O
Hence	O
our	O
model	O
decides	O
to	O
incorporate	O
the	O
related	O
sentiment	O
information	O
from	O
the	O
candidate	O
words	O
to	O
refine	O
the	O
word	O
embedding	O
.	O
As	O
shown	O
in	O
Figure	O
3	O
,	O
our	O
model	O
pays	O
more	O
attention	O
to	O
the	O
candidate	O
words	O
"	O
special	O
"	O
,	O
"	O
cool	O
"	O
,	O
and	O
"	O
awesome	O
"	O
.	O
The	O
word	O
"	O
cool	O
"	O
is	O
exactly	O
the	O
gold	O
word	O
and	O
the	O
others	O
have	O
the	O
same	O
sentiment	O
polarity	O
as	O
it	O
.	O
Beneficial	O
from	O
the	O
attended	O
candidate	O
words	O
,	O
our	O
model	O
refines	O
the	O
input	O
word	O
and	O
reconstructs	O
its	O
sentiment	O
semantics	O
.	O
Finally	O
,	O
the	O
refined	O
word	B-TaskName
embeddings	I-TaskName
are	O
fed	O
into	O
the	O
multimodal	O
feature	O
fusion	O
module	O
to	O
predict	O
the	O
sentiment	O
label	O
.	O

In	O
this	O
paper	O
,	O
we	O
observe	O
an	O
obvious	O
performance	O
drop	O
when	O
the	O
SOTA	O
MSA	O
model	O
is	O
deployed	O
in	O
the	O
real	O
world	O
,	O
and	O
through	O
in	O
-	O
depth	O
analysis	O
,	O
we	O
find	O
that	O
the	O
sentiment	O
word	O
substitution	O
error	O
is	O
a	O
very	O
important	O
factor	O
causing	O
it	O
.	O
To	O
address	O
it	O
,	O
we	O
propose	O
the	O
sentiment	O
word	O
aware	O
multimodal	O
refinement	O
model	O
,	O
which	O
can	O
dynamically	O
refine	O
the	O
word	B-TaskName
embeddings	I-TaskName
and	O
reconstruct	O
the	O
corrupted	O
sentiment	O
semantics	O
by	O
incorporating	O
the	O
multimodal	O
sentiment	O
information	O
.	O
We	O
evaluate	O
our	O
model	O
on	O
MOSI	B-DatasetName
-	O
SpeechBrain	O
,	O
MOSI	B-DatasetName
-	O
IBM	O
,	O
and	O
MOSI	B-DatasetName
-	O
iFlytek	B-DatasetName
and	O
the	O
results	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
approach	O
.	O
For	O
future	O
work	O
,	O
we	O
will	O
explore	O
leveraging	O
the	O
multimodal	O
information	O
to	O
detect	O
the	O
sentiment	O
word	O
positions	O
.	O

GTI	O
at	O
SemEval	O
-	O
2016	O
Task	O
5	O
:	O
SVM	B-MethodName
and	O
CRF	B-MethodName
for	O
Aspect	O
Detection	O
and	O
Unsupervised	O
Aspect	B-TaskName
-	I-TaskName
Based	I-TaskName
Sentiment	I-TaskName
Analysis	I-TaskName

This	O
paper	O
describes	O
in	O
detail	O
the	O
approach	O
carried	O
out	O
by	O
the	O
GTI	O
research	O
group	O
for	O
Se	O
-	O
mEval	O
2016	O
Task	O
5	O
:	O
Aspect	B-TaskName
-	I-TaskName
Based	I-TaskName
Sentiment	I-TaskName
Analysis	I-TaskName
,	O
for	O
the	O
different	O
subtasks	O
proposed	O
,	O
as	O
well	O
as	O
languages	O
and	B-DatasetName
dataset	I-DatasetName
contexts	O
.	O
In	O
particular	O
,	O
we	O
developed	O
a	O
system	O
for	O
category	O
detection	O
based	O
on	O
SVM	B-MethodName
.	O
Then	O
for	O
the	O
opinion	O
target	O
detection	O
task	O
we	O
developed	O
a	O
system	O
based	O
on	O
CRFs	O
.	O
Both	O
are	O
built	O
for	O
restaurants	O
domain	O
in	O
English	O
and	O
Spanish	O
languages	O
.	O
Finally	O
for	O
aspect	B-TaskName
-	I-TaskName
based	I-TaskName
sentiment	I-TaskName
analysis	I-TaskName
we	O
carried	O
out	O
an	O
unsupervised	O
approach	O
based	O
on	O
lexicons	O
and	O
syntactic	O
dependencies	O
,	O
in	O
English	O
language	O
for	O
laptops	O
and	O
restaurants	O
domains	O
.	O

In	O
the	O
last	O
years	O
,	O
with	O
the	O
growth	O
of	O
Internet	O
,	O
people	O
use	O
it	O
as	O
a	O
means	O
of	O
expressing	O
their	O
opinions	O
and	O
experiences	O
about	O
several	O
subjects	O
.	O
That	O
is	O
the	O
reason	O
why	O
there	O
is	O
a	O
great	O
amount	O
of	O
user	O
generated	O
information	O
available	O
online	O
,	O
through	O
many	O
different	O
platforms	O
,	O
such	O
as	O
blogs	O
,	O
social	O
networks	O
,	O
etc	O
.	O
This	O
information	O
became	O
very	O
valuable	O
for	O
companies	O
,	O
politicians	O
,	O
etc	O
.	O
,	O
who	O
are	O
interested	O
in	O
what	O
users	O
say	O
about	O
them	O
or	O
their	O
products	O
.	O
Due	O
to	O
this	O
,	O
Sentiment	B-TaskName
Analysis	I-TaskName
(	O
SA	O
)	O
techniques	O
have	O
attracted	O
the	O
interest	O
of	O
researches	O
,	O
trying	O
to	O
process	O
all	O
this	O
amount	O
of	O
information	O
by	O
means	O
of	O
usually	O
supervised	O
methods	O
based	O
on	O
classifiers	O
.	O
Most	O
of	O
these	O
researches	O
focus	O
on	O
extracting	O
the	O
sentiment	O
of	O
a	O
whole	O
review	O
or	O
text	O
(	O
Liu	O
,	O
2012	O
)	O
.	O
This	O
is	O
enough	O
for	O
many	O
applications	O
and	O
purposes	O
.	O
However	O
,	O
sometimes	O
there	O
is	O
a	O
need	O
for	O
analysing	O
the	O
text	O
in	O
a	O
deeper	O
way	O
,	O
at	O
entity	O
or	O
aspect	O
level	O
.	O
For	O
example	O
,	O
a	O
review	O
in	O
the	O
restaurants	O
domain	O
can	O
include	O
different	O
opinions	O
about	O
different	O
aspects	O
,	O
such	O
as	O
the	O
service	O
or	O
the	O
food	O
quality	O
,	O
so	O
it	O
is	O
interesting	O
to	O
distinguish	O
the	O
different	O
opinions	O
for	O
each	O
of	O
these	O
aspects	O
.	O
This	O
is	O
the	O
reason	O
why	O
some	O
studies	O
emerged	O
about	O
the	O
so	O
-	O
called	O
aspect	B-TaskName
-	I-TaskName
based	I-TaskName
sentiment	I-TaskName
analysis	I-TaskName
(	O
Marcheggiani	O
et	O
al	O
,	O
2014	O
;	O
Lu	O
et	O
al	O
,	O
2011	O
)	O
.	O
Hence	O
this	O
is	O
the	O
subject	O
of	O
the	O
task	O
5	O
of	O
the	O
Se	O
-	O
mEval	O
2016	O
(	O
Pontiki	O
et	O
al	O
,	O
2016	O
)	O
,	O
divided	O
into	O
different	O
subtasks	O
.	O
Groups	O
are	O
asked	O
to	O
detect	O
aspect	O
categories	O
in	O
a	O
review	O
or	O
sentence	O
,	O
which	O
are	O
predefined	O
for	O
each	O
domain	O
and	O
formed	O
by	O
an	O
entity	O
and	O
an	O
attribute	O
.	O
Then	O
,	O
there	O
is	O
a	O
subtask	O
which	O
consists	O
of	O
detecting	O
the	O
opinion	O
target	O
expression	O
,	O
which	O
are	O
related	O
to	O
the	O
categories	O
found	O
.	O
Finally	O
,	O
aspect	B-TaskName
-	I-TaskName
based	I-TaskName
sentiment	I-TaskName
analysis	I-TaskName
is	O
required	O
for	O
one	O
of	O
the	O
subtasks	O
,	O
associating	O
a	O
polarity	O
,	O
which	O
can	O
be	O
positive	O
,	O
negative	O
or	O
neutral	O
,	O
to	O
each	O
of	O
the	O
categories	O
found	O
in	O
the	O
sentence	O
or	O
review	O
.	O
Datasets	O
in	O
different	O
languages	O
and	O
domains	O
are	O
available	O
for	O
proving	O
the	O
approaches	O
.	O
The	O
remainder	O
of	O
this	O
paper	O
is	O
structured	O
as	O
follows	O
.	O
In	O
Section	O
2	O
we	O
make	O
a	O
description	O
of	O
the	O
system	O
developed	O
for	O
all	O
the	O
subtasks	O
.	O
Section	O
3	O
contains	O
the	O
results	O
of	O
all	O
the	O
different	O
subtasks	O
,	O
as	O
well	O
as	O
detailed	O
scores	O
for	O
each	O
slot	O
.	O
Finally	O
,	O
in	O
section	O
4	O
we	O
summarize	O
the	O
main	O
aspects	O
of	O
our	O
system	O
and	O
extract	O
some	O
final	O
conclusions	O
.	O

As	O
a	O
first	O
step	O
for	O
all	O
the	O
subtasks	O
,	O
each	O
preprocessed	O
social	O
media	O
review	O
must	O
first	O
be	O
broken	O
into	O
tokens	O
,	O
in	O
order	O
to	O
derive	O
the	O
syntactic	O
context	O
.	O
Partof	O
-	O
speech	O
(	O
POS	O
)	O
tagging	O
and	O
lemmatization	B-TaskName
are	O
performed	O
to	O
ensure	O
that	O
all	O
the	O
inflected	O
forms	O
of	O
a	O
word	O
are	O
covered	O
.	O
In	O
the	O
case	O
of	O
English	O
,	O
Stanford	O
Tagger	O
is	O
applied	O
due	O
to	O
its	O
better	O
results	O
,	O
however	O
it	O
does	O
not	O
provide	O
lemmatization	B-TaskName
.	O
That	O
is	O
why	O
using	O
the	O
resulting	O
form	O
and	O
tag	O
,	O
lemma	B-DatasetName
is	O
extracted	O
by	O
means	O
of	O
Freeling	O
Tagger	O
(	O
Atserias	O
et	O
al	O
,	O
2006	O
;	O
Padró	O
and	O
Stanilovsky	O
,	O
2012	O
)	O
.	O
On	O
the	O
other	O
hand	O
,	O
for	O
Spanish	O
language	O
only	O
Freeling	O
Tagger	O
is	O
used	O
.	O
Freeling	O
is	O
a	O
library	O
that	O
provides	O
multiple	O
languages	O
among	O
which	O
are	O
English	O
and	O
Spanish	O
.	O
Food	O
and	O
drinks	O
recognition	O
is	O
also	O
performed	O
,	O
based	O
on	O
dictionaries	O
1	O
,	O
in	O
order	O
to	O
identify	O
words	O
referring	O
to	O
those	O
topics	O
for	O
the	O
subsequent	O
processing	O
of	O
the	O
sentences	O
.	O
POS	O
tagging	O
allows	O
the	O
identification	O
of	O
lexical	O
items	O
that	O
can	O
contribute	O
to	O
the	O
correct	O
recognition	O
of	O
targets	O
in	O
a	O
message	O
.	O
These	O
items	O
are	O
namely	O
adjectives	O
,	O
adverbs	O
,	O
verbs	O
and	O
nouns	O
.	O
The	O
lemmatized	O
and	O
POS	O
-	O
annotated	O
messages	O
are	O
fed	O
to	O
a	O
parser	O
that	O
transforms	O
the	O
output	O
of	O
the	O
tagger	O
into	O
a	O
full	O
parse	O
tree	O
.	O
Finally	O
,	O
the	O
tree	O
is	O
converted	O
to	O
dependencies	O
,	O
and	O
the	O
functions	O
are	O
annotated	O
.	O
The	O
entire	O
process	O
is	O
performed	O
by	O
means	O
of	O
Freeling	O
Parser	O
(	O
Padró	O
and	O
Stanilovsky	O
,	O
2012	O
)	O
.	O

Sentiment	B-TaskName
Analysis	I-TaskName
(	O
ABSA	O
)	O
This	O
subtask	O
contains	O
different	O
slots	O
,	O
having	O
participated	O
in	O
three	O
of	O
them	O
,	O
which	O
are	O
slot	O
1	O
,	O
slot	O
2	O
and	O
slot	O
3	O
.	O
The	O
system	O
for	O
Spanish	O
and	O
English	O
language	O
is	O
exactly	O
the	O
same	O
for	O
both	O
slots	O
1	O
and	O
2	O
.	O
1	O
Taken	O
from	O
the	O
lists	O
available	O
at	O
https://es.speaklanguages.com/inglés/vocabulario/comidas	O

The	O
aim	O
of	O
this	O
task	O
is	O
to	O
assign	O
to	O
each	O
sentence	O
a	O
category	O
,	O
which	O
is	O
a	O
tuple	O
(	O
entity	O
,	O
attribute	O
)	O
,	O
from	O
a	O
given	O
set	O
of	O
12	O
different	O
predefined	O
categories	O
.	O
To	O
do	O
this	O
,	O
we	O
used	O
a	O
linear	O
SVM	B-MethodName
classifier	O
combined	O
with	O
word	O
lists	O
.	O
These	O
word	O
lists	O
are	O
created	O
from	O
the	O
training	O
file	O
provided	O
by	O
the	O
organization	O
,	O
which	O
was	O
composed	O
of	O
2000	O
sentences	O
,	O
grouped	O
in	O
350	O
reviews	O
.	O
Different	O
datasets	O
were	O
provided	O
for	O
several	O
languages	O
and	O
topics	O
.	O
Our	O
system	O
was	O
developed	O
for	O
restaurants	O
dataset	O
,	O
both	O
in	O
English	O
and	O
Spanish	O
.	O
The	O
library	O
libsvm	O
(	O
Chang	O
and	O
Lin	O
,	O
2011	O
)	O
was	O
used	O
to	O
implement	O
the	O
SVM	B-MethodName
classifier	O
,	O
using	O
the	O
following	O
features	O
for	O
each	O
sentence	O
:	O
Words	O
:	O
those	O
words	O
appearing	O
in	O
the	O
sentence	O
,	O
which	O
are	O
nouns	O
,	O
verbs	O
or	O
adjectives	O
are	O
extracted	O
.	O
Lemmas	O
:	O
lemmas	O
from	O
nouns	O
,	O
verbs	O
and	O
adjectives	O
are	O
selected	O
.	O
POS	O
tags	O
:	O
part	O
of	O
speech	O
from	O
nouns	O
,	O
verbs	O
and	O
adjectives	O
in	O
the	O
sentence	O
.	O
Bigrams	O
:	O
all	O
the	O
bigrams	O
found	O
in	O
the	O
sentence	O
.	O
We	O
developed	O
12	O
different	O
binary	O
classifiers	O
,	O
one	O
for	O
each	O
possible	O
category	O
.	O
If	O
the	O
output	O
of	O
one	O
classifier	O
for	O
a	O
particular	O
sentence	O
is	O
"	O
1	O
"	O
,	O
then	O
we	O
add	O
the	O
related	O
category	O
to	O
the	O
sentence	O
.	O
If	O
more	O
than	O
one	O
category	O
is	O
found	O
for	O
the	O
same	O
sentence	O
,	O
we	O
add	O
all	O
of	O
them	O
to	O
the	O
list	O
of	O
categories	O
.	O
After	O
this	O
,	O
the	O
outputs	O
are	O
improved	O
by	O
means	O
of	O
our	O
word	O
lists	O
,	O
as	O
we	O
can	O
see	O
in	O
Algorithm	O
1	O
,	O
executed	O
for	O
each	O
sentence	O
.	O
The	O
word	O
lists	O
were	O
created	O
automatically	O
from	O
the	O
training	O
file	O
,	O
extracting	O
all	O
the	O
nouns	O
and	O
adjectives	O
appearing	O
in	O
sentences	O
from	O
the	O
same	O
category	O
,	O
and	O
manually	O
filtered	O
later	O
in	O
order	O
to	O
remove	O
noisy	O
items	O
.	O
Six	O
different	O
lists	O
are	O
composed	O
,	O
containing	O
terms	O
related	O
to	O
:	O
ambience	O
,	O
service	O
,	O
prices	O
,	O
quality	O
,	O
style	O
options	O
and	O
location	O
.	O
The	O
inputs	O
defined	O
for	O
the	O
following	O
algorithm	O
are	O
the	O
list	O
of	O
categories	O
obtained	O
from	O
SVM	B-MethodName
for	O
each	O
sentence	O
(	O
CList	O
(	O
s	O
)	O
)	O
and	O
the	O
six	O
word	O
lists	O
created	O
previously	O
.	O
The	O
output	O
is	O
the	O
new	O
list	O
per	O
sentence	O
,	O
containing	O
the	O
old	O
categories	O
from	O
SVM	B-MethodName
and	O
the	O
new	O
ones	O
added	O
.	O

For	O
this	O
slot	O
,	O
teams	O
were	O
asked	O
to	O
extract	O
the	O
exact	O
expressions	O
or	O
words	O
in	O
the	O
sentence	O
,	O
in	O
which	O
an	O
opinion	O
is	O
expressed	O
.	O
The	O
implementation	O
for	O
this	O
slot	O
is	O
made	O
by	O
means	O
of	O
CRFs	O
,	O
using	O
CRF++	O
tool	O
(	O
Kudo	O
,	O
2005	O
)	O
and	O
the	O
training	O
file	O
provided	O
for	O
building	O
the	O
model	O
.	O
A	O
training	O
file	O
is	O
needed	O
to	O
build	O
as	O
input	O
for	O
the	O
CRF	B-MethodName
,	O
whose	O
structure	O
is	O
as	O
follows	O
.	O
In	O
the	O
first	O
column	O
,	O
all	O
the	O
words	O
for	O
every	O
sentence	O
are	O
written	O
,	O
then	O
in	O
the	O
second	O
column	O
,	O
the	O
corresponding	O
lemma	B-DatasetName
.	O
The	O
third	O
column	O
represents	O
the	O
tag	O
and	O
the	O
last	O
one	O
represents	O
if	O
the	O
word	O
is	O
an	O
aspect	O
or	O
not	O
or	O
if	O
it	O
is	O
included	O
in	O
a	O
multiword	O
aspect	O
.	O
Then	O
for	O
creating	O
the	O
model	O
we	O
take	O
into	O
account	O
all	O
these	O
features	O
,	O
as	O
well	O
as	O
all	O
the	O
possible	O
bigrams	O
in	O
each	O
sentence	O
.	O
In	O
the	O
output	O
,	O
if	O
no	O
target	O
is	O
found	O
,	O
no	O
opinion	O
is	O
returned	O
for	O
that	O
sentence	O
.	O

This	O
slot	O
is	O
implemented	O
only	O
for	O
English	O
language	O
,	O
both	O
restaurants	O
and	O
laptops	O
datasets	O
.	O
Our	O
system	O
is	O
fully	O
unsupervised	O
,	O
this	O
can	O
explain	O
the	O
low	O
results	O
obtained	O
for	O
this	O
slot	O
.	O
An	O
adjustment	O
was	O
made	O
to	O
the	O
system	O
already	O
implemented	O
for	O
sentiment	B-TaskName
analysis	I-TaskName
in	O
the	O
whole	O
sentence	O
,	O
which	O
was	O
presented	O
in	O
Semeval	O
2015	O
,	O
task	O
10	O
:	O
sentiment	B-TaskName
analysis	I-TaskName
in	O
Twitter	O
(	O
Fernández	O
-	O
Gavilanes	O
et	O
al	O
,	O
2015	O
)	O
,	O
which	O
was	O
also	O
unsupervised	O
.	O
For	O
this	O
dataset	O
,	O
a	O
new	O
polarity	O
lexicon	O
was	O
generated	O
automatically	O
from	O
the	O
training	O
dataset	O
,	O
applying	O
a	O
polarity	O
rank	O
algorithm	O
,	O
as	O
explained	O
in	O
the	O
mentioned	O
article	O
.	O
Then	O
,	O
it	O
was	O
merged	O
with	O
SOCAL	O
(	O
Taboada	O
et	O
al	O
,	O
2011	O
)	O
and	O
AFINN	O
(	O
Nielsen	O
,	O
2011	O
)	O
lexicons	O
,	O
which	O
are	O
general	O
context	O
ones	O
,	O
by	O
applying	O
an	O
average	O
for	O
those	O
words	O
which	O
appeared	O
in	O
more	O
than	O
one	O
of	O
them	O
.	O
Our	O
system	O
for	O
the	O
restaurant	O
dataset	O
implements	O
the	O
following	O
syntactic	O
rules	O
:	O
If	O
there	O
is	O
no	O
opinion	O
or	O
only	O
one	O
target	O
expression	O
in	O
the	O
sentence	O
,	O
the	O
system	O
automatically	O
takes	O
the	O
polarity	O
of	O
the	O
whole	O
sentence	O
and	O
assign	O
it	O
to	O
all	O
the	O
categories	O
which	O
appear	O
in	O
this	O
sentence	O
.	O
If	O
there	O
is	O
only	O
one	O
different	O
target	O
expression	O
but	O
appearing	O
more	O
than	O
once	O
,	O
we	O
check	O
if	O
there	O
is	O
an	O
adversative	O
clause	O
in	O
the	O
sentence	O
built	O
with	O
"	O
but	O
"	O
particle	O
.	O
If	O
not	O
,	O
we	O
also	O
take	O
the	O
polarity	O
of	O
the	O
whole	O
sentence	O
for	O
all	O
the	O
opinions	O
.	O
If	O
the	O
previous	O
condition	O
is	O
fulfilled	O
,	O
we	O
will	O
take	O
the	O
polarity	O
of	O
the	O
first	O
clause	O
of	O
the	O
sentence	O
,	O
which	O
is	O
the	O
piece	O
of	O
sentence	O
placed	O
before	O
the	O
"	O
but	O
"	O
and	O
then	O
apply	O
a	O
polarity	O
linear	O
system	O
,	O
which	O
consists	O
of	O
summing	O
up	O
all	O
the	O
polarities	O
found	O
in	O
the	O
dictionary	O
created	O
.	O
For	O
the	O
next	O
opinions	O
which	O
have	O
the	O
same	O
target	O
,	O
we	O
will	O
follow	O
the	O
same	O
procedure	O
but	O
with	O
the	O
piece	O
of	O
sentence	O
after	O
the	O
"	O
but	O
"	O
.	O
For	O
this	O
linear	O
approach	O
,	O
we	O
take	O
negations	O
in	O
account	O
only	O
for	O
adjectives	O
,	O
flipping	O
the	O
polarity	O
of	O
the	O
adjectives	O
which	O
come	O
inmediately	O
after	O
a	O
negation	O
particle	O
,	O
as	O
"	O
no	O
"	O
or	O
"	O
not	O
"	O
.	O
When	O
there	O
are	O
several	O
different	O
opinion	O
targets	O
,	O
we	O
split	O
the	O
sentence	O
to	O
detect	O
the	O
scope	O
of	O
each	O
target	O
and	O
apply	O
the	O
same	O
linear	O
polarity	O
algorithm	O
explained	O
in	O
the	O
previous	O
point	O
.	O
To	O
detect	O
the	O
scope	O
of	O
the	O
target	O
,	O
we	O
take	O
the	O
words	O
which	O
appear	O
before	O
and	O
after	O
the	O
target	O
,	O
splitting	O
by	O
punctuation	O
marks	O
(	O
"	O
;	O
"	O
,	O
"	O
,	O
"	O
,	O
"	O
.	O
"	O
,	O
"	O
?	O
"	O
,	O
"	O
!	O
"	O
,	O
"	O
-	O
"	O
)	O
.	O
For	O
the	O
laptops	O
dataset	O
,	O
since	O
there	O
are	O
no	O
opinion	O
target	O
expressions	O
,	O
we	O
take	O
the	O
polarity	O
of	O
the	O
whole	O
sentence	O
to	O
assign	O
the	O
polarity	O
of	O
each	O
category	O
.	O

Once	O
we	O
performed	O
aspect	B-TaskName
category	I-TaskName
detection	I-TaskName
at	O
sentence	O
-	O
level	O
,	O
we	O
use	O
this	O
output	O
as	O
input	O
for	O
textlevel	O
detection	O
.	O
All	O
the	O
categories	O
found	O
are	O
grouped	O
at	O
sentence	O
-	O
level	O
and	O
added	O
all	O
of	O
them	O
at	O
reviewlevel	O
.	O
Besides	O
this	O
,	O
if	O
RESTAURANT#GENERAL	O
is	O
not	O
explicitly	O
assigned	O
to	O
any	O
sentence	O
of	O
the	O
review	O
,	O
we	O
add	O
it	O
anyway	O
.	O

This	O
paper	O
describes	O
the	O
participation	O
of	O
the	O
GTI	O
group	O
,	O
AtlantTIC	O
Research	O
Center	O
,	O
University	O
of	O
Vigo	O
,	O
in	O
the	O
SemEval	O
2016	O
,	O
Task	O
5	O
:	O
Aspect	B-TaskName
-	I-TaskName
Based	I-TaskName
Sentiment	I-TaskName
Analysis	I-TaskName
.	O
We	O
developed	O
a	O
supervised	O
system	O
based	O
on	O
SVM	B-MethodName
classifiers	O
for	O
category	O
detection	O
,	O
and	O
CRFs	O
for	O
opinion	O
target	O
detection	O
.	O
Then	O
,	O
for	O
the	O
aspect	B-TaskName
-	I-TaskName
based	I-TaskName
sentiment	I-TaskName
analysis	I-TaskName
we	O
submitted	O
a	O
fully	O
unsupervised	O
system	O
,	O
based	O
on	O
syntactic	O
dependencies	O
and	O
context	O
-	O
based	O
polarity	O
lexicons	O
.	O
As	O
we	O
can	O
see	O
in	O
Table	O
4	O
,	O
competitive	O
results	O
were	O
obtained	O
for	O
aspect	O
and	O
category	O
detection	O
,	O
being	O
in	O
first	O
position	O
for	O
Spanish	O
language	O
,	O
both	O
in	O
subtask	O
1	O
and	O
subtask	O
2	O
.	O
Moreover	O
,	O
in	O
subtask	O
2	O
,	O
which	O
is	O
aspect	O
detection	O
at	O
review	O
level	O
,	O
we	O
also	O
achieved	O
the	O
first	O
position	O
for	O
English	O
language	O
in	O
restaurants	O
datasets	O
.	O
However	O
,	O
our	O
system	O
did	O
not	O
perform	O
as	O
well	O
as	O
expected	O
in	O
slot	O
3	O
,	O
maybe	O
due	O
to	O
the	O
fact	O
of	O
the	O
lack	O
of	O
supervision	O
for	O
our	O
model	O
.	O
It	O
results	O
not	O
competitive	O
against	O
other	O
supervised	O
approaches	O
,	O
although	O
its	O
main	O
advantage	O
is	O
that	O
there	O
is	O
no	O
need	O
of	O
training	O
sets	O
,	O
which	O
is	O
time	O
and	O
resource	O
consuming	O
in	O
order	O
to	O
manually	O
tag	O
them	O
.	O

Welcome	O
to	O
the	O
first	O
Workshop	O
on	O
Human	O
-	O
Computer	O
Question	B-TaskName
Answering	I-TaskName
(	O
HCQA	O
)	O
!	O
Question	B-TaskName
answering	I-TaskName
is	O
a	O
central	O
task	O
in	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
.	O
Unlike	O
other	O
NLP	O
tasks	O
,	O
it	O
also	O
is	O
easy	O
for	O
non	O
-	O
experts	O
to	O
understand	O
when	O
question	B-TaskName
answering	I-TaskName
systems	O
perform	O
well	O
(	O
or	O
fail	O
)	O
.	O
The	O
goal	O
of	O
this	O
workshop	O
is	O
to	O
bring	O
the	O
community	O
together	O
to	O
discuss	O
the	O
state	O
of	O
the	O
art	O
of	O
question	B-TaskName
answering	I-TaskName
and	O
interactively	O
compete	O
with	O
top	O
human	O
trivia	O
masters	O
.	O
This	O
workshop	O
highlights	O
question	B-TaskName
answering	I-TaskName
on	O
the	O
real	O
-	O
world	O
task	O
of	O
quiz	O
bowl	O
,	O
a	O
trivia	O
game	O
in	O
which	O
competitors	O
are	O
asked	O
to	O
identify	O
entities	O
such	O
as	O
battles	O
,	O
novels	O
,	O
and	O
scientific	O
terms	O
.	O
In	O
quiz	O
bowl	O
,	O
a	O
moderator	O
reads	O
a	O
paragraph	O
-	O
long	O
question	O
to	O
two	O
teams	O
,	O
and	O
players	O
are	O
permitted	O
to	O
interrupt	O
the	O
moderator	O
(	O
or	O
"	O
buzz	O
in	O
"	O
)	O
with	O
a	O
guess	O
if	O
they	O
feel	O
confident	O
.	O
This	O
setting	O
is	O
especially	O
interesting	O
because	O
acquiring	O
more	O
features	O
(	O
clues	O
)	O
comes	O
with	O
an	O
added	O
cost	O
(	O
the	O
other	O
team	O
may	O
buzz	O
in	O
before	O
you	O
)	O
.	O
While	O
computerized	O
question	B-TaskName
answering	I-TaskName
systems	O
have	O
previously	O
had	O
success	O
against	O
humans	O
,	O
this	O
workshop	O
will	O
be	O
the	O
first	O
to	O
pit	O
different	O
systems	O
against	O
each	O
other	O
and	O
then	O
have	O
that	O
winner	O
face	O
off	O
against	O
a	O
top	O
human	O
team	O
.	O
Question	B-TaskName
answering	I-TaskName
is	O
a	O
task	O
interesting	O
to	O
both	O
academia	O
and	O
industry	O
.	O
This	O
workshop	O
brings	O
people	O
from	O
both	O
sides	O
to	O
discuss	O
recent	O
progress	O
in	O
QA	O
.	O
We	O
will	O
have	O
a	O
presentation	O
from	O
the	O
IBM	O
Watson	O
team	O
talking	O
about	O
their	O
new	O
Watson	O
Discovery	O
Advisor	O
and	O
the	O
challenges	O
of	O
QA	O
in	O
the	O
industrial	O
setting	O
.	O
Peter	O
Clark	O
will	O
talk	O
about	O
new	O
types	O
of	O
question	B-TaskName
answering	I-TaskName
problems	O
that	O
he	O
and	O
his	O
team	O
are	O
solving	O
at	O
the	O
Allen	O
Institute	O
for	O
AI	O
.	O
We	O
also	O
have	O
Zhengdong	O
Lu	O
,	O
Jason	O
Weston	O
,	O
and	O
Richard	O
Socher	O
talking	O
about	O
recent	O
neural	O
network	O
approaches	O
to	O
QA	O
.	O
This	O
year	O
we	O
have	O
nine	O
papers	O
covering	O
a	O
variety	O
of	O
approaches	O
to	O
QA	O
,	O
including	O
neural	O
networks	O
,	O
crowdsourcing	O
,	O
knowledge	O
graph	O
search	O
,	O
and	O
paraphrasing	O
.	O
Besides	O
common	O
QA	O
tasks	O
such	O
as	O
machine	O
comprehension	O
,	O
(	O
open	O
-	O
domain	O
)	O
factoid	O
QA	O
,	O
we	O
are	O
also	O
excited	O
to	O
see	O
new	O
topics	O
on	O
error	O
analysis	O
of	O
QA	O
systems	O
by	O
crowdsourcing	O
and	O
alignment	O
between	O
text	O
description	O
and	O
paintings	O
for	O
art	O
questions	O
.	O
At	O
the	O
end	O
of	O
the	O
workshop	O
,	O
we	O
will	O
have	O
a	O
dual	O
computer	O
-	O
human	O
tournament	O
to	O
test	O
entrants	O
'	O
question	B-TaskName
answering	I-TaskName
systems	O
against	O
each	O
other	O
and	O
against	O
the	O
top	O
human	O
trivia	O
masters	O
.	O
Enjoy	O
the	O
match	O
!	O
Finally	O
,	O
we	O
invite	O
you	O
to	O
enjoy	O
this	O
volume	O
and	O
we	O
are	O
looking	O
forward	O
to	O
seeing	O
you	O
in	O
San	O
Diego	O
!	O

As	O
a	O
prominent	O
attribution	O
-	O
based	O
explanation	O
algorithm	O
,	O
Integrated	O
Gradients	O
(	O
IG	O
)	O
is	O
widely	O
adopted	O
due	O
to	O
its	O
desirable	O
explanation	O
axioms	O
and	O
the	O
ease	O
of	O
gradient	O
computation	O
.	O
It	O
measures	O
feature	B-TaskName
importance	I-TaskName
by	O
averaging	O
the	O
model	O
's	O
output	O
gradient	O
interpolated	O
along	O
a	O
straight	O
-	O
line	O
path	O
in	O
the	O
input	O
data	O
space	O
.	O
However	O
,	O
such	O
straight	O
-	O
line	O
interpolated	O
points	O
are	O
not	O
representative	O
of	O
text	O
data	O
due	O
to	O
the	O
inherent	O
discreteness	O
of	O
the	O
word	O
embedding	O
space	O
.	O
This	O
questions	O
the	O
faithfulness	O
of	O
the	O
gradients	O
computed	O
at	O
the	O
interpolated	O
points	O
and	O
consequently	O
,	O
the	O
quality	O
of	O
the	O
generated	O
explanations	O
.	O
Here	O
we	O
propose	O
Discretized	O
Integrated	O
Gradients	O
(	O
DIG	O
)	O
,	O
which	O
allows	O
effective	O
attribution	O
along	O
non	O
-	O
linear	O
interpolation	O
paths	O
.	O
We	O
develop	O
two	O
interpolation	O
strategies	O
for	O
the	O
discrete	O
word	O
embedding	O
space	O
that	O
generates	O
interpolation	O
points	O
that	O
lie	O
close	O
to	O
actual	O
words	O
in	O
the	O
embedding	O
space	O
,	O
yielding	O
more	O
faithful	O
gradient	O
computation	O
.	O
We	O
demonstrate	O
the	O
effectiveness	O
of	O
DIG	O
over	O
IG	O
through	O
experimental	O
and	O
human	O
evaluations	O
on	O
multiple	O
sentiment	O
classification	O
datasets	O
.	O
We	O
provide	O
the	O
source	O
code	O
of	O
DIG	O
to	O
encourage	O
reproducible	O
research	O
1	O
.	O

In	O
the	O
past	O
few	O
years	O
,	O
natural	O
language	O
processing	O
has	O
seen	O
tremendous	O
progress	O
,	O
largely	O
due	O
to	O
strong	O
performances	O
yielded	O
by	O
pre	O
-	O
trained	O
language	O
models	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
;	O
Radford	O
et	O
al	O
,	O
2019	O
;	O
Brown	O
et	O
al	O
,	O
2020	O
)	O
.	O
But	O
even	O
with	O
this	O
impressive	O
performance	O
,	O
it	O
can	O
still	O
be	O
difficult	O
to	O
understand	O
the	O
underlying	O
reasoning	O
for	O
the	O
preferred	O
predictions	O
leading	O
to	O
distrust	O
among	O
end	O
-	O
users	O
(	O
Lipton	O
,	O
2018	O
)	O
.	O
Hence	O
,	O
improving	O
model	O
interpretability	O
has	O
become	O
a	O
central	O
focus	O
in	O
the	O
community	O
with	O
an	O
increasing	O
effort	O
in	O
developing	O
methods	O
that	O
can	O
explain	O
model	O
behaviors	O
(	O
Ribeiro	O
et	O
al	O
,	O
2016	O
;	O
Binder	O
et	O
al	O
,	O
2016	O
;	O
Li	O
et	O
al	O
,	O
2016	O
;	O
Sundararajan	O
et	O
2017	O
;	O
Shrikumar	O
et	O
al	O
,	O
2017	O
;	O
Lundberg	O
and	O
Lee	O
,	O
2017	O
;	O
Murdoch	O
et	O
al	O
,	O
2018	O
)	O
.	O
Explanations	O
in	O
NLP	O
are	O
typically	O
represented	O
at	O
a	O
word	O
-	O
level	O
or	O
phrase	O
-	O
level	O
by	O
quantifying	O
the	O
contributions	O
of	O
the	O
words	O
or	O
phrases	O
to	O
the	O
model	O
's	O
prediction	O
by	O
a	O
scalar	O
score	O
.	O
These	O
explanation	O
methods	O
are	O
commonly	O
referred	O
as	O
attributionbased	O
methods	O
(	O
Murdoch	O
et	O
al	O
,	O
2018	O
;	O
Ancona	O
et	O
al	O
,	O
2018	O
)	O
.	O
Integrated	O
Gradients	O
(	O
IG	O
)	O
(	O
Sundararajan	O
et	O
al	O
,	O
2017	O
)	O
is	O
a	O
prominent	O
attribution	O
-	O
based	O
explanation	O
method	O
used	O
due	O
to	O
the	O
many	O
desirable	O
explanation	O
axioms	O
and	O
ease	O
of	O
gradient	O
computation	O
.	O
It	O
computes	O
the	O
partial	O
derivatives	O
of	O
the	O
model	O
output	O
with	O
respect	O
to	O
each	O
input	O
feature	O
as	O
the	O
features	O
are	O
interpolated	O
along	O
a	O
straight	O
-	O
line	O
path	O
from	O
the	O
given	O
input	O
to	O
a	O
baseline	O
value	O
.	O
For	O
example	O
,	O
say	O
we	O
want	O
to	O
compute	O
the	O
attribution	O
for	O
the	O
word	O
"	O
good	O
"	O
in	O
the	O
sentence	O
"	O
the	O
movie	O
was	O
good	O
!	O
"	O
using	O
IG	O
.	O
The	O
straight	O
-	O
line	O
interpolation	O
path	O
used	O
by	O
IG	O
is	O
depicted	O
in	O
green	O
in	O
Figure	O
1	O
.	O
Here	O
,	O
the	O
baseline	O
word	O
is	O
defined	O
as	O
the	O
"	O
<	O
pad	O
>	O
"	O
embedding	O
and	O
the	O
green	O
squares	O
are	O
the	O
intermediate	O
interpolation	O
points	O
in	O
the	O
embedding	O
space	O
.	O
While	O
this	O
method	O
can	O
be	O
used	O
for	O
attributing	O
inputs	O
in	O
both	O
continuous	O
(	O
e.g.	O
,	O
image	O
,	O
audio	O
,	O
etc	O
.	O
)	O
and	O
discrete	O
(	O
e.g.	O
,	O
text	O
,	O
molecules	O
,	O
etc	O
.	O
)	O
domains	O
(	O
Sundararajan	O
et	O
al	O
,	O
2017	O
)	O
,	O
their	O
usage	O
in	O
the	O
dis	O
-	O
crete	O
domain	O
has	O
some	O
limitations	O
.	O
Since	O
the	O
interpolation	O
is	O
done	O
along	O
a	O
straight	O
-	O
line	O
path	O
joining	O
the	O
input	O
word	O
embedding	O
and	O
the	O
baseline	O
embedding	O
(	O
"	O
<	O
pad	O
>	O
"	O
in	O
Figure	O
1	O
)	O
,	O
the	O
interpolated	O
points	O
are	O
not	O
necessarily	O
representative	O
of	O
the	O
discrete	O
word	O
embedding	O
distribution	O
.	O
Specifically	O
,	O
let	O
a	O
dummy	O
word	O
embedding	O
space	O
be	O
defined	O
by	O
the	O
words	O
represented	O
by	O
black	O
dots	O
in	O
Figure	O
1	O
.	O
Then	O
we	O
can	O
see	O
that	O
some	O
of	O
the	O
green	O
squares	O
can	O
be	O
very	O
far	O
-	O
off	O
from	O
any	O
original	O
word	O
in	O
the	O
embedding	O
space	O
.	O
Since	O
the	O
underlying	O
language	O
model	O
is	O
trained	O
to	O
effectively	O
work	O
with	O
the	O
specific	O
word	O
embedding	O
space	O
as	O
input	O
,	O
using	O
these	O
out	O
-	O
of	O
-	O
distribution	O
green	O
interpolated	O
samples	O
as	O
intermediate	O
inputs	O
to	O
calculate	O
gradients	O
can	O
lead	O
to	O
sub	O
-	O
optimal	O
attributions	O
.	O
To	O
mitigate	O
these	O
limitations	O
,	O
we	O
propose	O
a	O
Discretized	O
integrated	O
gradients	O
(	O
DIG	O
)	O
formulation	O
by	O
relaxing	O
the	O
constraints	O
of	O
searching	O
for	O
interpolation	O
points	O
along	O
a	O
straight	O
-	O
line	O
path	O
.	O
Relaxing	O
this	O
linear	O
-	O
path	O
constraint	O
leads	O
to	O
a	O
new	O
constraint	O
on	O
the	O
interpolation	O
paths	O
in	O
DIG	O
that	O
points	O
along	O
the	O
path	O
should	O
be	O
monotonically	O
situated	O
between	O
the	O
input	O
word	O
embedding	O
and	O
the	O
baseline	O
embedding	O
.	O
Hence	O
,	O
in	O
DIG	O
,	O
our	O
main	O
objective	O
is	O
to	O
monotonically	O
interpolate	O
between	O
the	O
input	O
word	O
embedding	O
and	O
baseline	O
such	O
that	O
the	O
intermediate	O
points	O
are	O
close	O
to	O
real	O
data	O
samples	O
.	O
This	O
would	O
ensure	O
that	O
the	O
interpolated	O
points	O
are	O
more	O
representative	O
of	O
the	O
word	O
embedding	O
distribution	O
,	O
enabling	O
more	O
faithful	O
model	O
gradient	O
computations	O
.	O
To	O
this	O
end	O
,	O
we	O
propose	O
two	O
interpolation	O
strategies	O
that	O
search	O
for	O
an	O
optimal	O
anchor	O
word	O
embedding	O
in	O
the	O
real	O
data	O
space	O
and	O
then	O
modify	O
it	O
such	O
that	O
it	O
lies	O
monotonically	O
between	O
the	O
input	O
word	O
and	O
baseline	O
(	O
see	O
Fig	O
.	O
1	O
for	O
an	O
illustration	O
)	O
.	O
We	O
apply	O
DIG	O
using	O
our	O
proposed	O
interpolation	O
algorithms	O
to	O
generate	O
attributions	O
for	O
three	O
pre	O
-	O
trained	O
language	O
models	O
-	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
DistilBERT	B-MethodName
(	O
Sanh	O
et	O
al	O
,	O
2020	O
)	O
,	O
and	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
,	O
each	O
fine	O
-	O
tuned	O
separately	O
on	O
three	O
sentiment	O
classification	O
datasets	O
-	O
SST2	B-DatasetName
(	O
Socher	O
et	O
al	O
,	O
2013	O
)	O
,	O
IMDB	B-DatasetName
(	O
Maas	O
et	O
al	O
,	O
2011	O
)	O
,	O
and	O
Rotten	O
Tomatoes	O
(	O
Pang	O
and	O
Lee	O
,	O
2005	O
)	O
.	O
We	O
find	O
that	O
our	O
proposed	O
interpolation	O
strategies	O
achieve	O
a	O
superior	O
performance	O
compared	O
to	O
integrated	O
gradients	O
and	O
other	O
gradient	O
-	O
based	O
baselines	O
on	O
eight	O
out	O
of	O
the	O
nine	O
settings	O
across	O
different	O
metrics	O
.	O
Further	O
,	O
we	O
also	O
observe	O
that	O
on	O
average	O
,	O
end	O
-	O
users	O
find	O
explanations	O
provided	O
by	O
DIG	O
to	O
be	O
more	O
plausible	O
justifications	O
of	O
model	O
behavior	O
than	O
the	O
explanations	O
from	O
other	O
baselines	O
.	O

As	O
described	O
in	O
prior	O
works	O
(	O
Sundararajan	O
et	O
al	O
,	O
2017	O
;	O
Shrikumar	O
et	O
al	O
,	O
2017	O
)	O
,	O
a	O
good	O
explanation	O
w	O
w2	O
w6	O
w1	O
w5	O
w3	O
w4	O
w	O
'	O
c	O
(	O
a	O
)	O
DIG	O
-	O
GREEDY	O
w	O
w2	O
[	O
7	O
]	O
w6	O
[	O
8	O
]	O
w1	O
[	O
5	O
]	O
w5	O
[	O
12	O
]	O
w3	O
[	O
10	O
]	O
w4	O
[	O
20	O
]	O
w	O
'	O
c	O
(	O
b	O
)	O
DIG	O
-	O
MAXCOUNT	O
Figure	O
2	O
:	O
Overview	O
of	O
paths	O
used	O
in	O
DIG	O
and	O
IG	O
.	O
The	O
gray	O
region	O
is	O
the	O
neighborhood	O
of	O
w.	O
Green	O
line	O
depicts	O
the	O
straight	O
-	O
line	O
path	O
used	O
by	O
IG	O
.	O
Left	O
:	O
In	O
DIG	O
-	O
GREEDY	O
,	O
we	O
first	O
monotonize	O
each	O
word	O
in	O
the	O
neighborhood	O
(	O
red	O
arrow	O
)	O
and	O
the	O
word	O
closest	O
to	O
its	O
corresponding	O
monotonic	O
point	O
is	O
selected	O
as	O
the	O
anchor	O
(	O
w	O
5	O
since	O
the	O
red	O
arrow	O
of	O
w	O
5	O
has	O
the	O
smallest	O
magnitude	O
)	O
.	O
Right	O
:	O
In	O
DIG	O
-	O
MAXCOUNT	O
we	O
select	O
the	O
word	O
with	O
the	O
highest	O
number	O
of	O
monotonic	O
dimensions	O
(	O
count	O
shown	O
in	O
[	O
.	O
]	O
)	O
as	O
the	O
anchor	O
word	O
(	O
w	O
4	O
)	O
,	O
followed	O
by	O
changing	O
the	O
non	O
-	O
monotonic	O
dimensions	O
of	O
w	O
4	O
(	O
red	O
arrow	O
to	O
c	O
)	O
.	O
Repeating	O
this	O
iteratively	O
gives	O
the	O
non	O
-	O
linear	O
blue	O
path	O
for	O
DIG	O
with	O
the	O
red	O
stars	O
as	O
interpolation	O
points	O
.	O
Please	O
refer	O
to	O
Section	O
2.1	O
for	O
more	O
details	O
.	O
Figure	O
best	O
viewed	O
in	O
color	O
.	O
algorithm	O
should	O
satisfy	O
certain	O
desirable	O
axioms	O
which	O
justify	O
the	O
use	O
of	O
the	O
algorithm	O
for	O
generating	O
model	O
explanations	O
.	O
Similar	O
to	O
IG	O
,	O
DIG	O
also	O
satisfies	O
many	O
such	O
desirable	O
axioms	O
.	O
First	O
,	O
DIG	O
satisfies	O
Implementation	O
Invariance	O
which	O
states	O
that	O
attributions	O
should	O
be	O
identical	O
for	O
two	O
functionally	O
equivalent	O
models	O
.	O
Two	O
models	O
are	O
functionally	O
equivalent	O
if	O
they	O
have	O
the	O
same	O
output	O
for	O
the	O
same	O
input	O
,	O
irrespective	O
of	O
any	O
differences	O
in	O
the	O
model	O
's	O
internal	O
implementation	O
design	O
.	O
Further	O
,	O
DIG	O
satisfies	O
Completeness	O
which	O
states	O
that	O
the	O
sum	O
of	O
the	O
attributions	O
for	O
an	O
input	O
should	O
add	O
up	O
to	O
the	O
difference	O
between	O
the	O
output	O
of	O
the	O
model	O
at	O
the	O
input	O
and	O
the	O
baseline	O
,	O
i.e.	O
,	O
i	O
DIG	O
i	O
(	O
x	O
)	O
=	O
F	O
(	O
x	O
)	O
−F	O
(	O
x	O
)	O
.	O
This	O
ensures	O
that	O
if	O
F	O
(	O
x	O
)	O
≈	O
0	B-DatasetName
then	O
the	O
output	O
is	O
completely	O
attributed	O
to	O
the	O
inputs	O
.	O
Thirdly	O
,	O
DIG	O
satisfies	O
Sensitivity	O
which	O
states	O
that	O
attributions	O
of	O
inputs	O
should	O
be	O
zero	O
if	O
the	O
model	O
does	O
not	O
depend	O
(	O
mathematically	O
)	O
on	O
the	O
input	O
.	O
Please	O
refer	O
to	O
Appendix	O
B	O
for	O
further	O
comparisons	O
of	O
DIG	O
with	O
IG	O
.	O

To	O
further	O
understand	O
the	O
impact	O
of	O
our	O
algorithm	O
on	O
end	O
users	O
,	O
we	O
conduct	O
human	O
evaluations	O
of	O
explanations	O
from	O
our	O
method	O
and	O
the	O
two	O
top	O
baselines	O
-	O
IG	O
and	O
GradShap	O
.	O
We	O
perform	O
the	O
study	O
on	O
the	O
DistilBERT	B-MethodName
model	O
fine	O
-	O
tuned	O
on	O
SST2	B-DatasetName
dataset	O
and	O
the	O
BERT	B-MethodName
model	O
fine	O
-	O
tuned	O
on	O
Rotten	O
Tomatoes	O
dataset	O
.	O
Further	O
,	O
we	O
select	O
the	O
best	O
variant	O
of	O
DIG	O
on	O
each	O
dataset	O
for	O
explanation	O
comparisons	O
.	O
First	O
,	O
we	O
pick	O
50	O
sample	O
sentences	O
from	O
each	O
dataset	O
with	O
lengths	O
between	O
5	O
and	O
25	O
words	O
for	O
easier	O
visualizations	O
.	O
Then	O
,	O
we	O
convert	O
the	O
attributions	O
from	O
each	O
method	O
into	O
word	O
highlights	O
,	O
whose	O
intensity	O
is	O
determined	O
by	O
the	O
magnitude	O
of	O
the	O
attributions	O
.	O
Finally	O
,	O
we	O
show	O
the	O
highlighted	O
sentence	O
and	O
the	O
model	O
's	O
predicted	O
label	O
to	O
the	O
annotators	O
and	O
ask	O
them	O
to	O
rank	O
the	O
explanations	O
on	O
a	O
scale	O
of	O
1	O
-	O
3	O
,	O
"	O
1	O
"	O
being	O
the	O
most	O
comprehensive	O
explanation	O
that	O
best	O
justifies	O
the	O
prediction	O
.	O
Figure	O
3	O
shows	O
the	O
mean	O
rank	O
of	O
each	O
explanation	O
algorithm	O
across	O
the	O
two	O
datasets	O
.	O
We	O
find	O
that	O
DIG	O
has	O
a	O
significantly	O
lower	O
mean	O
rank	O
compared	O
to	O
IG	O
(	O
p	O
<	O
.001	O
on	O
both	O
SST2	B-DatasetName
and	O
Rotten	O
Tomatoes	O
5	O
)	O
.	O
Thus	O
,	O
we	O
conclude	O
that	O
explanations	O
generated	O
by	O
DIG	O
are	O
also	O
trustworthy	O
according	O
to	O
humans	O
.	O
Please	O
refer	O
to	O
Appendix	O
G	O
for	O
visualizations	O
and	O
discussion	O
on	O
explanations	O
generated	O
by	O
our	O
methods	O
.	O

In	O
this	O
section	O
,	O
we	O
report	O
the	O
ablation	O
of	O
AN	O
-	O
CHORSEARCH	O
and	O
the	O
effect	O
of	O
path	O
density	O
on	O
DIG	O
.	O
Please	O
refer	O
to	O
Appendix	O
F	O
for	O
ablations	O
on	O
neighborhood	O
size	O
and	O
discussions	O
on	O
computational	O
complexity	O
.	O
Ablation	O
Study	O
on	O
ANCHORSEARCH	O
.	O
We	O
ablate	O
our	O
methods	O
with	O
two	O
random	O
variants	O
-	O
DIG	O
-	O
RANDOMANCHOR	O
and	O
DIG	O
-	O
5	O
We	O
compute	O
the	O
p−value	O
using	O
Wilcoxon	O
signed	O
-	O
rank	O
test	O
.	O
RANDOMNEIGHBOR	O
,	O
in	O
which	O
the	O
AN	O
-	O
CHORSEARCH	O
step	O
uses	O
a	O
random	O
anchor	O
selection	O
heuristic	O
.	O
Specifically	O
,	O
in	O
DIG	O
-	O
RANDOMANCHOR	O
,	O
the	O
anchor	O
is	O
selected	O
randomly	O
from	O
the	O
complete	O
vocabulary	O
.	O
Thus	O
,	O
this	O
variant	O
just	O
ensures	O
that	O
the	O
selected	O
anchor	O
is	O
close	O
to	O
some	O
word	O
in	O
the	O
vocabulary	O
which	O
is	O
not	O
necessarily	O
in	O
the	O
neighborhood	O
.	O
In	O
contrast	O
,	O
the	O
DIG	O
-	O
RANDOMNEIGHBOR	O
selects	O
the	O
anchor	O
randomly	O
from	O
the	O
neighborhood	O
without	O
using	O
our	O
proposed	O
heuristics	O
MAXCOUNT	O
or	O
GREEDY	O
.	O
The	O
log	O
-	O
odds	O
metrics	O
of	O
IG	O
,	O
the	O
two	O
ablations	O
,	O
and	O
our	O
best	O
variant	O
of	O
DIG	O
for	O
DistilBERT	B-MethodName
fine	O
-	O
tuned	O
individually	O
on	O
all	O
three	O
datasets	O
are	O
reported	O
in	O
Table	O
4	O
.	O
We	O
report	O
5	O
-	O
seed	O
average	O
for	O
the	O
randomized	O
baselines	O
.	O
We	O
observe	O
that	O
DIG	O
-	O
RANDOMANCHOR	O
improves	O
upon	O
IG	O
on	O
all	O
three	O
datasets	O
.	O
This	O
shows	O
that	O
generating	O
interpolation	O
points	O
close	O
to	O
the	O
words	O
in	O
the	O
vocabulary	O
improve	O
the	O
explanation	O
quality	O
.	O
Further	O
,	O
we	O
observe	O
that	O
DIG	O
-	O
RANDOMNEIGHBOR	O
improves	O
upon	O
DIG	O
-	O
RANDOMANCHOR	O
on	O
log	O
-	O
odds	O
metric	O
.	O
One	O
reason	O
could	O
be	O
that	O
the	O
words	O
in	O
a	O
neighborhood	O
are	O
more	O
semantically	O
relevant	O
to	O
the	O
original	O
word	O
,	O
leading	O
to	O
more	O
coherent	O
perturbations	O
for	O
evaluating	O
model	O
gradients	O
.	O
Finally	O
,	O
we	O
observe	O
that	O
,	O
on	O
average	O
,	O
our	O
proposed	O
method	O
is	O
better	O
compared	O
to	O
selecting	O
a	O
random	O
anchor	O
in	O
the	O
neighborhood	O
.	O
This	O
shows	O
that	O
our	O
search	O
strategies	O
MAXCOUNT	O
and	O
GREEDY	O
are	O
indeed	O
helpful	O
.	O
Effect	O
of	O
Increasing	O
Path	O
Density	O
.	O
In	O
integrated	O
gradients	O
,	O
the	O
completeness	O
axiom	O
(	O
Section	O
2.2	O
)	O
is	O
used	O
to	O
estimate	O
if	O
the	O
integral	O
approximation	O
(	O
Equation	O
6	O
)	O
error	O
is	O
low	O
enough	O
.	O
This	O
error	O
is	O
denoted	O
as	O
the	O
Delta	O
%	O
error	O
.	O
If	O
the	O
error	O
is	O
high	O
,	O
users	O
can	O
increase	O
the	O
number	O
of	O
interpolation	O
points	O
m.	O
While	O
DIG	O
also	O
satisfies	O
the	O
completeness	O
axiom	O
,	O
error	O
reduction	O
by	O
increasing	O
m	O
is	O
infeasible	O
.	O
This	O
is	O
because	O
increasing	O
m	O
in	O
Equation	O
3	O
implicitly	O
changes	O
the	O
integral	O
path	O
rather	O
than	O
increasing	O
the	O
density	O
.	O
Hence	O
,	O
to	O
achieve	O
an	O
error	O
reduction	O
in	O
DIG	O
,	O
we	O
up	O
-	O
sample	O
the	O
interpolation	O
path	O
P	O
=	O
{	O
w	O
,	O
w	O
1	O
,	O
w	O
2	O
,	O
.	O
.	O
.	O
,	O
w	O
m−2	O
,	O
w	O
}	O
with	O
an	O
up	O
-	O
sampling	O
factor	O
(	O
f	O
)	O
of	O
one	O
as	O
follows	O
:	O
P	O
1	O
=	O
{	O
w	O
,	O
w+w	O
1	O
2	O
,	O
w	O
1	O
,	O
w	O
1	O
+	O
w	O
2	O
2	O
,	O
.	O
.	O
.	O
,	O
w	O
m−2	O
+	O
w	O
2	O
,	O
w	O
}	O
,	O
i.e.	O
,	O
we	O
insert	O
the	O
mean	O
of	O
two	O
consecutive	O
points	O
to	O
the	O
path	O
.	O
This	O
essentially	O
doubles	O
the	O
density	O
of	O
points	O
in	O
the	O
path	O
.	O
Similarly	O
,	O
P	O
2	O
can	O
be	O
obtained	O
by	O
up	O
-	O
sampling	O
P	O
1	O
,	O
etc	O
.	O
DIG	O
(	O
m	O
,	O
f	O
=	O
0	B-DatasetName
)	O
refers	O
to	O
the	O
standard	O
DIG	O
with	O
no	O
up	O
-	O
sampling	O
.	O
Given	O
that	O
we	O
have	O
two	O
hyperparameters	O
m	O
and	O
f	O
that	O
determine	O
the	O
overall	O
path	O
density	O
,	O
we	O
analyze	O
the	O
effect	O
of	O
each	O
of	O
these	O
in	O
Figure	O
4	O
and	O
Table	O
5	O
respectively	O
.	O
The	O
results	O
are	O
shown	O
for	O
DIG	O
-	O
MAXCOUNT	O
applied	O
on	O
DistilBERT	B-MethodName
model	O
finetuned	O
on	O
SST2	B-DatasetName
dataset	O
.	O
In	O
Figure	O
4	O
,	O
we	O
observe	O
that	O
as	O
m	O
increases	O
,	O
the	O
Delta	O
%	O
of	O
IG	O
decreases	O
as	O
expected	O
.	O
But	O
the	O
trend	O
is	O
opposite	O
for	O
DIG	O
.	O
As	O
discussed	O
above	O
,	O
for	O
DIG	O
,	O
the	O
path	O
length	O
increases	O
with	O
increasing	O
m	O
,	O
and	O
hence	O
,	O
we	O
attribute	O
this	O
trend	O
to	O
increasing	O
difficulty	O
in	O
effectively	O
approximating	O
the	O
integral	O
for	O
longer	O
paths	O
.	O
Next	O
,	O
in	O
Table	O
5	O
,	O
we	O
observe	O
that	O
as	O
the	O
up	O
-	O
sampling	O
factor	O
f	O
increases	O
,	O
the	O
Delta	O
%	O
consistently	O
decreases	O
.	O
We	O
also	O
find	O
that	O
our	O
up	O
-	O
sampling	O
strategy	O
does	O
not	O
increase	O
the	O
WAE	O
by	O
a	O
significant	O
amount	O
with	O
increasing	O
f	O
,	O
which	O
is	O
desirable	O
.	O
Thus	O
,	O
this	O
confirms	O
that	O
our	O
up	O
-	O
sampling	O
strategy	O
is	O
a	O
good	O
substitute	O
of	O
increasing	O
m	O
for	O
IG	O
to	O
effectively	O
reduce	O
the	O
integral	O
approximation	O
error	O
Delta	O
%	O
.	O
Following	O
Sundararajan	O
et	O
al	O
(	O
2017	O
)	O
,	O
we	O
choose	O
a	O
threshold	O
of	O
5	O
%	O
average	O
Delta	O
to	O
select	O
the	O
hyperparameters	O
.	O
For	O
more	O
discussions	O
,	O
please	O
refer	O
to	O
Appendix	O
F.1	O
.	O

There	O
has	O
been	O
an	O
increasing	O
effort	O
in	O
developing	O
interpretability	O
algorithms	O
that	O
can	O
help	O
understand	O
a	O
neural	O
network	O
model	O
's	O
behavior	O
by	O
explaining	O
their	O
predictions	O
(	O
Doshi	O
-	O
Velez	O
and	O
Kim	O
,	O
2017	O
;	O
Gilpin	O
et	O
al	O
,	O
2019	O
)	O
.	O
Attributions	O
are	O
a	O
post	O
-	O
hoc	O
explanation	O
class	O
where	O
input	O
features	O
are	O
quantified	O
by	O
scalar	O
scores	O
indicating	O
the	O
magnitude	O
of	O
contribution	O
of	O
the	O
features	O
toward	O
the	O
predicted	O
label	O
.	O
Explanation	O
algorithms	O
that	O
generate	O
attributions	O
can	O
be	O
broadly	O
classified	O
into	O
two	O
categories	O
-	O
model	O
-	O
agnostic	O
algorithms	O
,	O
like	O
LIME	B-MethodName
(	O
Ribeiro	O
et	O
al	O
,	O
2016	O
)	O
,	O
Input	O
occlusion	O
(	O
Li	O
et	O
al	O
,	O
2016	O
)	O
,	O
Integrated	O
gradients	O
6	O
(	O
Sundararajan	O
et	O
al	O
,	O
2017	O
)	O
,	O
SHAP	B-MethodName
(	O
Lundberg	O
and	O
Lee	O
,	O
2017	O
)	O
,	O
etc	O
.	O
and	O
model	O
-	O
dependent	O
algorithms	O
,	O
like	O
LRP	O
(	O
Binder	O
et	O
al	O
,	O
2016	O
)	O
,	O
DeepLIFT	O
(	O
Shrikumar	O
et	O
al	O
,	O
2017	O
)	O
,	O
CD	O
(	O
Murdoch	O
et	O
al	O
,	O
2018	O
)	O
,	O
ACD	O
(	O
Singh	O
et	O
al	O
,	O
2019	O
)	O
,	O
SOC	B-DatasetName
(	O
Jin	O
et	O
al	O
,	O
2020	O
)	O
,	O
etc	O
.	O
While	O
the	O
model	O
-	O
agnostic	O
algorithms	O
can	O
be	O
used	O
as	O
blackbox	O
explanation	O
tools	O
that	O
can	O
work	O
for	O
any	O
neural	O
network	O
architecture	O
,	O
for	O
the	O
latter	O
,	O
one	O
needs	O
to	O
understand	O
the	O
network	O
's	O
architectural	O
details	O
to	O
implement	O
the	O
explanation	O
algorithm	O
.	O
Typically	O
,	O
model	O
-	O
dependent	O
algorithms	O
require	O
specific	O
layer	O
decomposition	O
rules	O
(	O
Ancona	O
et	O
al	O
,	O
2018	O
;	O
Murdoch	O
et	O
al	O
,	O
2018	O
)	O
which	O
needs	O
to	O
be	O
defined	O
for	O
all	O
the	O
components	O
in	O
the	O
model	O
.	O
Model	O
-	O
agnostic	O
methods	O
usually	O
work	O
directly	O
with	O
the	O
model	O
outputs	O
and	O
gradients	O
which	O
are	O
universally	O
available	O
.	O
Due	O
to	O
the	O
many	O
desirable	O
explanation	O
axioms	O
and	O
ease	O
of	O
gradient	O
computation	O
,	O
there	O
has	O
been	O
several	O
extensions	O
of	O
integrated	O
gradients	O
.	O
For	O
example	O
,	O
Miglani	O
et	O
al	O
(	O
2020	O
)	O
study	O
the	O
effect	O
of	O
saturation	O
in	O
the	O
saliency	O
maps	O
generated	O
by	O
integrated	O
gradients	O
.	O
Merrill	O
et	O
al	O
(	O
2019	O
)	O
extend	O
integrated	O
gradients	O
to	O
certain	O
classes	O
of	O
discontinuous	O
functions	O
in	O
financial	O
domains	O
.	O
Further	O
,	O
Jha	O
et	O
al	O
(	O
2020	O
)	O
use	O
KNNs	O
and	O
auto	O
-	O
encoders	O
to	O
learn	O
latent	O
paths	O
for	O
RNAs	O
.	O
Different	O
from	O
prior	O
work	O
,	O
our	O
focus	O
here	O
is	O
to	O
improve	O
integrated	O
gradients	O
specifically	O
for	O
the	O
discrete	O
textual	O
domain	O
.	O
While	O
the	O
idea	O
of	O
learning	O
latent	O
paths	O
for	O
text	O
data	O
is	O
quite	O
interesting	O
,	O
it	O
brings	O
a	O
significant	O
amount	O
of	O
challenge	O
in	O
successfully	O
modeling	O
such	O
a	O
complex	O
latent	O
space	O
and	O
hence	O
,	O
we	O
leave	O
this	O
for	O
future	O
work	O
.	O

In	O
Figure	O
5	O
,	O
we	O
visualize	O
the	O
effect	O
of	O
changing	O
top	O
-	O
k%	O
on	O
log	O
-	O
odds	O
,	O
comprehensiveness	O
,	O
and	O
sufficiency	O
metrics	O
for	O
DistilBERT	B-MethodName
model	O
fine	O
-	O
tuned	O
on	O
the	O
SST2	B-DatasetName
dataset	O
.	O
We	O
compare	O
the	O
two	O
variants	O
of	O
our	O
method	O
:	O
DIG	O
-	O
GREEDY	O
and	O
DIG	O
-	O
MAXCOUNT	O
with	O
Integrated	O
Gradients	O
.	O
We	O
observe	O
that	O
our	O
method	O
outperforms	O
IG	O
for	O
all	O
values	O
of	O
k.	O
Specifically	O
,	O
we	O
note	O
that	O
the	O
gap	O
between	O
DIG	O
and	O
IG	O
is	O
initially	O
non	O
-	O
existent	O
but	O
then	O
gradually	O
increases	O
with	O
increasing	O
k	O
in	O
Figure	O
5	O
(	O
a	O
)	O
and	O
eventually	O
saturates	O
.	O
This	O
shows	O
that	O
although	O
IG	O
might	O
be	O
equally	O
good	O
as	O
DIG	O
at	O
finding	O
the	O
top	O
-	O
5	O
%	O
important	O
words	O
,	O
the	O
explanations	O
from	O
IG	O
are	O
significantly	O
misaligned	O
from	O
true	O
model	O
behavior	O
for	O
higher	O
top	O
-	O
k	O
values	O
.	O

In	O
this	O
section	O
,	O
we	O
present	O
some	O
interesting	O
sentence	O
visualizations	O
based	O
on	O
explanations	O
from	O
DIG	O
and	O
IG	O
for	O
SST2	B-DatasetName
dataset	O
in	O
Figure	O
6	O
.	O
We	O
show	O
the	O
sentence	O
visualization	O
and	O
the	O
model	O
's	O
predicted	O
sentiment	O
for	O
the	O
sentence	O
for	O
each	O
explanation	O
algorithm	O
.	O
In	O
the	O
visualizations	O
,	O
the	O
red	O
highlighted	O
words	O
denote	O
positive	O
attributions	O
and	O
blue	O
denotes	O
negative	O
attributions	O
.	O
That	O
is	O
,	O
the	O
explanation	O
model	O
suggests	O
that	O
the	O
red	O
highlighted	O
words	O
support	O
the	O
predicted	O
label	O
whereas	O
the	O
blue	O
ones	O
oppose	O
(	O
or	O
undermine	O
)	O
the	O
prediction	O
.	O
We	O
observe	O
that	O
in	O
many	O
cases	O
,	O
DIG	O
is	O
able	O
to	O
highlight	O
more	O
plausible	O
explanations	O
.	O
For	O
example	O
,	O
in	O
sentence	O
pairs	O
1	O
-	O
7	O
,	O
clearly	O
the	O
DIG	O
highlights	O
are	O
more	O
inline	O
with	O
the	O
model	O
prediction	O
.	O
But	O
we	O
want	O
to	O
emphasize	O
that	O
it	O
does	O
not	O
mean	O
that	O
our	O
method	O
always	O
produces	O
more	O
plausible	O
highlights	O
.	O
For	O
example	O
,	O
for	O
sentences	O
8	O
-	O
10	O
,	O
we	O
observe	O
that	O
highlights	O
from	O
IG	O
are	O
more	O
plausible	O
than	O
those	O
of	O
DIG	O
.	O
Hence	O
,	O
this	O
shows	O
that	O
,	O
while	O
it	O
could	O
be	O
a	O
good	O
exercise	O
to	O
visualize	O
the	O
attributions	O
as	O
a	O
sanity	O
check	O
,	O
we	O
should	O
rely	O
more	O
on	O
automated	O
metrics	O
and	O
human	O
evaluations	O
to	O
correctly	O
compare	O
explanation	O
algorithms	O
.	O
6	O
:	O
Some	O
example	O
visualizations	O
of	O
attributions	O
from	O
DIG	O
and	O
IG	O
for	O
the	O
DistilBERT	B-MethodName
model	O
fine	O
-	O
tuned	O
on	O
SST2	B-DatasetName
dataset	O
.	O
The	O
sentence	O
visualization	O
is	O
followed	O
by	O
model	O
's	O
sentiment	O
prediction	O
for	O
the	O
sentence	O
.	O
Here	O
,	O
the	O
red	O
highlighted	O
words	O
denote	O
positive	O
attributions	O
and	O
blue	O
denotes	O
negative	O
attributions	O
.	O
For	O
more	O
details	O
,	O
please	O
refer	O
to	O
Appendix	O
G	O

Evaluating	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
event	B-TaskName
detection	I-TaskName
systems	O
on	O
determining	O
spatio	O
-	O
temporal	O
distribution	O
of	O
the	O
events	O
on	O
the	O
ground	O
is	O
performed	O
unfrequently	O
.	O
But	O
,	O
the	O
ability	O
to	O
both	O
(	O
1	O
)	O
extract	O
events	O
"	O
in	O
the	O
wild	O
"	O
from	O
text	O
and	O
(	O
2	O
)	O
properly	O
evaluate	O
event	B-TaskName
detection	I-TaskName
systems	O
has	O
potential	O
to	O
support	O
a	O
wide	O
variety	O
of	O
tasks	O
such	O
as	O
monitoring	O
the	O
activity	O
of	O
sociopolitical	O
movements	O
,	O
examining	O
media	O
coverage	O
and	O
public	O
support	O
of	O
these	O
movements	O
,	O
and	O
informing	O
policy	O
decisions	O
.	O
Therefore	O
,	O
we	O
study	O
performance	O
of	O
the	O
best	O
event	B-TaskName
detection	I-TaskName
systems	O
on	O
detecting	O
Black	O
Lives	O
Matter	O
(	O
BLM	O
)	O
events	O
from	O
tweets	O
and	O
news	O
articles	O
.	O
The	O
murder	O
of	O
George	O
Floyd	O
,	O
an	O
unarmed	O
Black	O
man	O
,	O
at	O
the	O
hands	O
of	O
police	O
officers	O
received	O
global	O
attention	O
throughout	O
the	O
second	O
half	O
of	O
2020	O
.	O
Protests	O
against	O
police	O
violence	O
emerged	O
worldwide	O
and	O
the	O
BLM	O
movement	O
,	O
which	O
was	O
once	O
mostly	O
regulated	O
to	O
the	O
United	O
States	O
,	O
was	O
now	O
seeing	O
activity	O
globally	O
.	O
This	O
shared	O
task	O
asks	O
participants	O
to	O
identify	O
BLM	O
related	O
events	O
from	O
large	O
unstructured	O
data	O
sources	O
,	O
using	O
systems	O
pretrained	O
to	O
extract	O
socio	O
-	O
political	O
events	O
from	O
text	O
.	O
We	O
evaluate	O
several	O
metrics	O
,	O
assessing	O
each	O
system	O
's	O
ability	O
to	O
evolution	O
of	O
protest	O
events	O
both	O
temporally	O
and	O
spatially	O
.	O
Results	O
show	O
that	O
identifying	O
daily	O
protest	O
counts	O
is	O
an	O
easier	O
task	O
than	O
classifying	O
spatial	O
and	O
temporal	O
protest	O
trends	O
simultaneously	O
,	O
with	O
maximum	O
performance	O
of	O
0.745	O
(	O
Spearman	O
)	O
and	O
0.210	O
(	O
Pearson	O
r	O
)	O
,	O
respectively	O
.	O
Additionally	O
,	O
all	O
baselines	O
and	O
participant	O
systems	O
suffered	O
from	O
low	O
recall	O
(	O
max.5.08	O
)	O
,	O
confirming	O
the	O
high	O
impact	O
of	O
media	O
sourcing	O
in	O
the	O
modelling	O
of	O
protest	O
movements	O
.	O

The	O
goal	O
of	O
this	O
task	O
is	O
to	O
evaluate	O
the	O
performance	O
of	O
automatic	O
event	B-TaskName
detection	I-TaskName
systems	O
on	O
modeling	O
the	O
spatial	O
and	O
temporal	O
pattern	O
of	O
a	O
social	O
protest	O
movement	O
.	O
We	O
evaluate	O
the	O
capability	O
of	O
participant	O
systems	O
to	O
reproduce	O
a	O
manually	O
curated	O
BLM	O
-	O
related	O
protest	O
event	O
data	O
set	O
,	O
by	O
detecting	O
BLM	O
event	O
reports	O
,	O
enriched	O
with	O
location	O
and	O
date	O
attributes	O
,	O
from	O
a	O
news	O
corpus	O
collection	O
,	O
a	O
Twitter	O
collection	O
,	O
and	O
from	O
the	O
union	O
of	O
the	O
two	O
.	O

For	O
the	O
Gold	O
Standard	O
data	O
(	O
i.e.	O
,	O
the	O
BLM	O
events	O
list	O
we	O
wish	O
to	O
automatically	O
detect	O
)	O
we	O
considered	O
two	O
online	O
sources	O
of	O
Black	O
Lives	O
Matter	O
protest	O
events	O
:	O
Creosote	O
Maps	O
2	O
and	O
Race	O
and	O
Policing	O
3	O
.	O
Starting	O
with	O
these	O
two	O
data	O
sets	O
,	O
we	O
first	O
checked	O
if	O
the	O
source	O
URL	O
link	O
was	O
still	O
active	O
.	O
If	O
not	O
,	O
we	O
referenced	O
other	O
data	O
sets	O
for	O
the	O
event	O
in	O
question	O
:	O
Wikipedia	O
(	O
a	O
list	O
of	O
George	O
Floyd	O
protests	O
in	O
and	O
outside	O
of	O
the	O
U.S.	O
)	O
and	O
the	O
New	O
York	O
Times	O
.	O
If	O
a	O
valid	O
article	O
was	O
not	O
found	O
matching	O
this	O
protest	O
date	O
and	O
location	O
,	O
then	O
we	O
performed	O
a	O
Google	B-DatasetName
search	O
for	O
the	O
specific	O
event	O
.	O
If	O
still	O
nothing	O
was	O
found	O
,	O
then	O
the	O
event	O
was	O
removed	O
from	O
the	O
data	O
set	O
.	O
If	O
at	O
any	O
point	O
,	O
we	O
discovered	O
a	O
valid	O
URL	O
for	O
the	O
event	O
,	O
we	O
ran	O
a	O
validation	O
check	O
.	O
This	O
check	O
asked	O
:	O
(	O
1	O
)	O
is	O
the	O
source	O
a	O
tweet	O
or	O
Facebook	O
post	O
;	O
(	O
2	O
)	O
does	O
the	O
source	O
describe	O
an	O
upcoming	O
event	O
;	O
(	O
3	O
)	O
is	O
the	O
source	O
irrelevant	O
to	O
the	O
protest	O
at	O
the	O
location	O
;	O
(	O
4	O
)	O
does	O
the	O
source	O
have	O
enough	O
information	O
;	O
and	O
(	O
5	O
)	O
is	O
the	O
source	O
not	O
accessible	O
because	O
of	O
a	O
paywall	O
.	O
If	O
the	O
source	O
passed	O
this	O
check	O
,	O
we	O
then	O
scraped	O
the	O
source	O
for	O
the	O
publication	O
date	O
and	O
days	O
of	O
the	O
week	O
in	O
the	O
article	O
text	O
.	O
If	O
the	O
publication	O
date	O
and	O
the	O
day	O
of	O
the	O
week	O
do	O
not	O
match	O
,	O
we	O
then	O
inferred	O
the	O
date	O
of	O
the	O
protest	O
by	O
the	O
mention	O
of	O
the	O
day	O
of	O
the	O
week	O
closest	O
to	O
the	O
publication	O
date	O
.	O
Finally	O
,	O
we	O
manually	O
checked	O
the	O
scraped	O
or	O
inferred	O
dates	O
and	O
record	O
this	O
as	O
the	O
event	O
date	O
.	O
In	O
the	O
end	O
,	O
this	O
produced	O
3	O
,	O
463	O
distinct	O
U.S.	O
events	O
between	O
May	O
25	O
and	O
June	O
30	O
,	O
2020	O
with	O
date	O
,	O
city	O
,	O
and	O
state	O
information	O
.	O
Of	O
these	O
events	O
,	O
only	O
537	O
(	O
approximately	O
15	O
%	O
of	O
the	O
events	O
)	O
occurred	O
after	O
the	O
first	O
week	O
of	O
June	O
.	O
To	O
compensate	O
for	O
the	O
lack	O
of	O
coverage	O
across	O
all	O
of	O
June	O
,	O
we	O
used	O
the	O
open	O
source	O
data	O
set	O
from	O
the	O
The	O
Crowd	B-TaskName
Counting	I-TaskName
Consortium	O
(	O
CCC	O
)	O
4	O
.	O
From	O
our	O
original	O
data	O
set	O
of	O
3	O
,	O
463	O
events	O
,	O
754	O
events	O
also	O
occurred	O
in	O
the	O
CCC	O
data	O
,	O
matching	O
on	O
(	O
1	O
)	O
URL	O
or	O
(	O
2	O
)	O
both	O
date	O
and	O
city	O
.	O
We	O
then	O
combined	O
the	O
two	O
data	O
sets	O
(	O
i.e.	O
,	O
the	O
CCC	O
events	O
with	O
our	O
original	O
list	O
)	O
and	O
removed	O
duplicates	O
.	O
This	O
resulted	O
in	O
7	O
,	O
976	O
protest	O
events	O
in	O
our	O
final	O
Gold	O
Standard	O
data	O
.	O
The	O
U.S.	O
map	O
in	O
Figure	O
1	O
shows	O
the	O
spatial	O
distribution	O
of	O
these	O
events	O
(	O
yellow	O
dots	O
)	O
.	O

To	O
this	O
purpose	O
,	O
we	O
also	O
measure	O
the	O
correlation	O
coefficients	O
on	O
the	O
absolute	O
event	O
counts	O
with	O
respect	O
to	O
Gold	O
Standard	O
,	O
over	O
each	O
single	O
cell	O
-	O
day	O
.	O
For	O
both	O
analyses	O
,	O
we	O
use	O
two	O
types	O
of	O
correlation	O
coefficients	O
to	O
assess	O
variable	O
's	O
relationship	O
:	O
Pearson	O
coefficient	O
r	O
and	O
Spearman	O
's	O
rank	O
correlation	O
coefficient	O
ρ	O
.	O
Moreover	O
,	O
we	O
used	O
Root	O
Mean	O
Squared	O
Error	B-MetricName
(	O
RMSE	B-MetricName
)	O
to	O
measure	O
the	O
absolute	O
value	O
of	O
the	O
error	O
on	O
estimating	O
cell	O
/	O
event	O
counts	O
from	O
the	O
Gold	O
Standard	O
.	O

This	O
system	O
,	O
developed	O
by	O
the	O
Task	O
organizers	O
and	O
denoted	O
NexusDdpl	O
,	O
is	O
an	O
extension	O
of	O
the	O
Baseline	O
system	O
,	O
where	O
an	O
event	O
deduplication	O
has	O
been	O
integrated	O
as	O
a	O
post	O
-	O
processing	O
module	O
.	O
The	O
algorithm	O
uses	O
two	O
metrics	O
based	O
on	O
geographical	O
distance	O
between	O
two	O
event	O
points	O
and	O
semantic	O
distance	O
,	O
respectively	O
.	O
The	O
semantic	O
distance	O
is	O
computed	O
using	O
the	O
cosine	O
between	O
the	O
projections	O
of	O
the	O
sentence	B-TaskName
embeddings	I-TaskName
of	O
the	O
texts	O
of	O
the	O
events	O
records	O
.	O
The	O
LASER	O
embeddings	O
(	O
Schwenk	O
and	O
Douze	O
,	O
2017	O
)	O
were	O
used	O
for	O
that	O
purpose	O
.	O
Twitter	O
data	O
has	O
been	O
cleaned	O
of	O
hashtags	O
,	O
URLs	O
,	O
and	O
accounts	O
names	O
,	O
as	O
these	O
have	O
a	O
negative	O
impact	O
on	O
the	O
semantic	B-TaskName
similarity	I-TaskName
measure	O
.	O
In	O
order	O
to	O
be	O
considered	O
duplicate	O
two	O
events	O
must	O
have	O
both	O
distance	O
measures	O
under	O
a	O
fixed	O
threshold	O
,	O
which	O
were	O
set	O
to	O
2	O
km	O
for	O
spatial	O
distance	O
,	O
0.20	O
for	O
semantic	O
distance	O
on	O
NYT	O
data	O
,	O
0.30	O
for	O
semantic	O
distance	O
on	O
Twitter	O
data	O
.	O
The	O
reason	O
of	O
these	O
different	O
threshold	O
depending	O
on	O
the	O
data	O
sets	O
is	O
that	O
Twitter	O
data	O
are	O
noisier	O
than	O
NYT	O
data	O
,	O
with	O
higher	O
variations	O
in	O
text	O
size	O
and	O
style	O
when	O
describing	O
a	O
single	O
event	O
.	O
As	O
such	O
looser	O
threshold	O
was	O
required	O
.	O
When	O
applying	O
on	O
the	O
combination	O
of	O
both	O
data	O
sets	O
,	O
we	O
use	O
a	O
compromise	O
threshold	O
of	O
0.35	O
was	O
used	O
.	O

Four	O
teams	O
participated	O
in	O
this	O
event	O
:	O
DaDeFrNi	O
,	O
EventMiner	O
,	O
Handshakes	O
,	O
and	O
NoConflict	O
.	O
We	O
briefly	O
describe	O
the	O
systems	O
below	O
and	O
ask	O
the	O
reader	O
to	O
refer	O
to	O
their	O
systems	O
papers	O
for	O
additional	O
details	O
.	O
DaDeFrNi	O
This	O
team	O
considered	O
two	O
slightly	O
different	O
procedures	O
for	O
this	O
task	O
.	O
For	O
the	O
NYT	O
data	O
set	O
,	O
they	O
first	O
extracted	O
geo	O
-	O
entities	O
from	O
each	O
article	O
using	O
the	O
Python	O
library	O
geography	O
,	O
which	O
was	O
used	O
to	O
classify	O
each	O
entity	O
in	O
one	O
of	O
the	O
three	O
categories	O
"	O
city	O
"	O
,	O
"	O
country	O
"	O
,	O
and	O
"	O
region	O
"	O
.	O
For	O
the	O
cases	O
where	O
an	O
article	O
contained	O
the	O
name	O
of	O
a	O
city	O
but	O
did	O
not	O
provide	O
any	O
region	O
or	O
country	O
reference	O
,	O
DaDeFrNi	O
retrieved	O
the	O
necessary	O
information	O
by	O
checking	O
the	O
city	O
name	O
against	O
a	O
worldwide	O
cities	O
database	O
.	O
When	O
the	O
name	O
of	O
a	O
city	O
was	O
associated	O
with	O
several	O
locations	O
,	O
we	O
filtered	O
the	O
city	O
with	O
the	O
highest	O
population	O
,	O
along	O
with	O
its	O
corresponding	O
"	O
region	O
"	O
and	O
"	O
country	O
"	O
.	O
For	O
the	O
Twitter	O
data	O
set	O
,	O
given	O
the	O
large	O
size	O
of	O
the	O
data	O
,	O
the	O
above	O
procedure	O
was	O
computationally	O
expensive	O
.	O
Thus	O
,	O
the	O
Python	O
library	O
spaCy	O
(	O
Honnibal	O
et	O
al	O
,	O
2020	O
)	O
for	O
retrieving	O
NER	B-TaskName
/	O
GPE	O
entities	O
,	O
given	O
its	O
much	O
smaller	O
computational	O
cost	O
.	O
The	O
complete	O
system	O
details	O
can	O
be	O
found	O
in	O
Ignazio	O
Re	O
et	O
al	O
(	O
2021	O
)	O
.	O
EventMiner	O
Team	O
EventMiner	O
's	O
approach	O
for	O
Task	O
3	O
is	O
mainly	O
based	O
on	O
transformer	O
models	O
(	O
Hettiarachchi	O
et	O
al	O
,	O
2021	O
)	O
.	O
This	O
approach	O
involved	O
three	O
steps	O
:	O
(	O
1	O
)	O
event	O
document	O
identification	O
,	O
(	O
2	O
)	O
location	O
detail	O
extraction	O
,	O
(	O
3	O
)	O
and	O
event	O
filtering	O
to	O
identify	O
the	O
spatial	O
and	O
temporal	O
pattern	O
of	O
the	O
targeted	O
social	O
protest	O
movement	O
.	O
Event	O
documents	O
are	O
identified	O
using	O
the	O
winning	O
solution	O
submitted	O
to	O
CASE	O
2021	O
Task	O
1	O
-	O
Subtask	O
1	O
:	O
event	O
document	B-TaskName
classification	I-TaskName
(	O
Hettiarachchi	O
et	O
al	O
,	O
2021	O
)	O
.	O
Next	O
,	O
the	O
location	O
details	O
in	O
event	O
described	O
tweets	O
are	O
extracted	O
.	O
Since	O
this	O
team	O
only	O
focused	O
on	O
the	O
Twitter	O
corpus	O
,	O
they	O
used	O
tweet	O
metadata	O
to	O
extract	O
location	O
details	O
.	O
However	O
,	O
since	O
the	O
majority	O
of	O
the	O
tweets	O
are	O
not	O
geotagged	O
and	O
to	O
extract	O
the	O
location	O
details	O
mentioned	O
in	O
the	O
text	O
,	O
they	O
used	O
a	O
NER	B-TaskName
approach	O
too	O
.	O
For	O
NER	B-TaskName
,	O
a	O
transformer	O
model	O
is	O
fine	O
-	O
tuned	O
for	O
token	B-TaskName
classification	I-TaskName
using	O
the	O
data	O
set	O
released	O
with	O
the	O
WNUT	B-DatasetName
2017	I-DatasetName
Shared	O
Task	O
on	O
Novel	O
and	O
Emerging	O
Entity	O
Recognition	O
(	O
Derczynski	B-DatasetName
et	O
al	O
,	O
2017	O
)	O
.	O
The	O
BERTweet	O
model	O
is	O
used	O
since	O
it	O
is	O
pretrained	O
on	O
Tweets	O
(	O
Nguyen	O
et	O
al	O
,	O
2020	O
)	O
.	O
To	O
convert	O
the	O
location	O
details	O
into	O
an	O
unique	O
format	O
and	O
fill	O
the	O
missing	O
details	O
(	O
e.g.	O
region	O
,	O
country	O
)	O
,	O
locations	O
are	O
geocoded	O
using	O
the	O
GeoPy	O
library	O
9	O
.	O
For	O
the	O
final	O
step	O
,	O
event	O
tweets	O
with	O
location	O
details	O
are	O
grouped	O
based	O
on	O
their	O
created	O
dates	O
and	O
locations	O
and	O
removed	O
the	O
groups	O
with	O
fewer	O
tweets	O
assuming	O
that	O
important	O
events	O
generate	O
a	O
high	O
number	O
of	O
tweets	O
.	O
Three	O
systems	O
were	O
submitted	O
.	O
For	O
the	O
first	O
system	O
,	O
denoted	O
by	O
†	O
,	O
only	O
the	O
new	O
events	O
are	O
included	O
(	O
i.e.	O
,	O
events	O
with	O
locations	O
which	O
are	O
identified	O
in	O
the	O
previous	O
day	O
are	O
removed	O
)	O
.	O
The	O
second	O
system	O
†	O
†	O
,	O
includes	O
all	O
the	O
extracted	O
events	O
(	O
i.e.	O
,	O
no	O
filtering	O
as	O
in	O
†	O
)	O
.	O
Finally	O
,	O
the	O
third	O
system	O
†	O
†	O
†	O
further	O
filters	O
the	O
events	O
from	O
†	O
to	O
include	O
U.S.	O
events	O
only	O
.	O
Please	O
see	O
Hettiarachchi	O
et	O
al	O
(	O
2021	O
)	O
for	O
more	O
details	O
Handshakes	O
This	O
model	O
is	O
a	O
pretrained	O
XLM	B-MethodName
-	O
RoBERTa	B-MethodName
model	O
,	O
fine	O
-	O
tuned	O
on	O
the	O
multi	O
-	O
language	O
article	O
data	O
from	O
Task	O
1	O
Subtask	O
1	O
and	O
sentence	O
data	O
from	O
Subtask	O
2	O
,	O
with	O
a	O
classification	O
head	O
that	O
predicts	O
if	O
the	O
input	O
text	O
is	O
a	O
protest	O
or	O
not	O
.	O
We	O
make	O
use	O
of	O
the	O
provided	O
location	O
data	O
in	O
the	O
data	O
sets	O
,	O
where	O
available	O
.	O
Please	O
see	O
Kalyan	O
et	O
al	O
(	O
2021	O
)	O
for	O
further	O
details	O
.	O
NoConflict	O
Team	O
NoConflict	O
used	O
their	O
model	O
of	O
protest	O
event	O
sentence	B-TaskName
classification	I-TaskName
from	O
the	O
winning	O
submission	O
of	O
the	O
English	O
version	O
of	O
Task	O
1	O
Subtask	O
2	O
.	O
Their	O
model	O
is	O
based	O
on	O
a	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
backbone	O
with	O
a	O
second	O
pretraining	O
(	O
Gururangan	O
et	O
al	O
,	O
2020	O
)	O
stage	O
done	O
on	O
the	O
POLUSA	B-DatasetName
(	O
Gebhard	O
and	O
Hamborg	O
,	O
2020	O
)	O
data	O
set	O
before	O
finetuned	O
on	O
Subtask	O
2	O
data	O
.	O
For	O
the	O
NYT	O
data	O
set	O
,	O
they	O
first	O
filtered	O
the	O
articles	O
based	O
on	O
the	O
section	O
name	O
.	O
They	O
then	O
ran	O
their	O
model	O
on	O
the	O
abstract	O
of	O
each	O
article	O
to	O
identify	O
ones	O
containing	O
protest	O
events	O
.	O
For	O
each	O
remaining	O
article	O
,	O
they	O
run	O
a	O
transformer	O
-	O
based	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
from	O
spaCy	O
(	O
Honnibal	O
et	O
al	O
,	O
2020	O
)	O
to	O
identify	O
the	O
location	O
and	O
date	O
of	O
the	O
events	O
.	O
They	O
covert	O
the	O
location	O
to	O
absolute	O
location	O
using	O
the	O
Geocoder	O
library	O
and	O
convert	O
the	O
date	O
of	O
the	O
event	O
to	O
the	O
absolute	O
date	O
based	O
on	O
the	O
article	O
's	O
publication	O
date	O
.	O
If	O
the	O
relative	O
location	O
or	O
date	O
is	O
unavailable	O
,	O
they	O
default	O
to	O
those	O
included	O
in	O
the	O
metadata	O
.	O
The	O
event	O
sentence	B-TaskName
classification	I-TaskName
system	O
details	O
can	O
be	O
found	O
in	O
Hu	O
and	O
Stoehr	O
(	O
2021	O
)	O
.	O
Three	O
systems	O
were	O
submitted	O
for	O
the	O
NYT	O
data	O
,	O
denoted	O
,	O
,	O
and	O
.	O
Each	O
system	O
used	O
a	O
set	O
of	O
manually	O
curated	O
keywords	O
applied	O
to	O
different	O
parts	O
of	O
each	O
data	O
point	O
.	O
Theses	O
rules	O
are	O
included	O
in	O
the	O
Appendix	O
.	O
For	O
the	O
Twitter	O
data	O
set	O
,	O
Team	O
NoConflict	O
ran	O
their	O
model	O
on	O
the	O
full	O
text	O
of	O
each	O
tweet	O
to	O
identify	O
protest	O
events	O
.	O
For	O
each	O
potential	O
event	O
tweet	O
,	O
they	O
identify	O
the	O
location	O
and	O
time	O
based	O
on	O
the	O
metadata	O
of	O
the	O
tweet	O
itself	O
and	O
the	O
main	O
tweet	O
if	O
it	O
is	O
a	O
retweet	B-DatasetName
.	O

Honey	O
or	O
Poison	O
?	O
Solving	O
the	O
Trigger	O
Curse	O
in	O
Few	O
-	O
shot	O
Event	B-TaskName
Detection	I-TaskName
via	O
Causal	O
Intervention	O

Event	B-TaskName
detection	I-TaskName
has	O
long	O
been	O
troubled	O
by	O
the	O
trigger	O
curse	O
:	O
overfitting	O
the	O
trigger	O
will	O
harm	O
the	O
generalization	O
ability	O
while	O
underfitting	O
it	O
will	O
hurt	O
the	O
detection	O
performance	O
.	O
This	O
problem	O
is	O
even	O
more	O
severe	O
in	O
few	O
-	O
shot	O
scenario	O
.	O
In	O
this	O
paper	O
,	O
we	O
identify	O
and	O
solve	O
the	O
trigger	O
curse	O
problem	O
in	O
few	O
-	O
shot	O
event	B-TaskName
detection	I-TaskName
(	O
FSED	O
)	O
from	O
a	O
causal	O
view	O
.	O
By	O
formulating	O
FSED	O
with	O
a	O
structural	O
causal	O
model	O
(	O
SCM	O
)	O
,	O
we	O
found	O
that	O
the	O
trigger	O
is	O
a	O
confounder	O
of	O
the	O
context	O
and	O
the	O
result	O
,	O
which	O
makes	O
previous	O
FSED	O
methods	O
much	O
easier	O
to	O
overfit	O
triggers	O
.	O
To	O
resolve	O
this	O
problem	O
,	O
we	O
propose	O
to	O
intervene	O
on	O
the	O
context	O
via	O
backdoor	O
adjustment	O
during	O
training	O
.	O
Experiments	O
show	O
that	O
our	O
method	O
significantly	O
improves	O
the	O
FSED	O
on	O
ACE05	O
,	O
MAVEN	B-DatasetName
and	O
KBP17	O
datasets	O
.	O

Event	B-TaskName
detection	I-TaskName
(	O
ED	O
)	O
aims	O
to	O
identify	O
and	O
classify	O
event	O
triggers	O
in	O
a	O
sentence	O
,	O
e.g.	O
,	O
detecting	O
an	O
Attack	O
event	O
triggered	O
by	O
fire	O
in	O
"	O
They	O
killed	O
by	O
hostile	O
fire	O
in	O
Iraqi	O
"	O
.	O
Recently	O
,	O
supervised	O
ED	O
approaches	O
have	O
achieved	O
promising	O
performance	O
(	O
Chen	O
et	O
al	O
,	O
2015	O
;	O
Nguyen	O
and	O
Grishman	O
,	O
2015	O
;	O
Nguyen	O
et	O
al	O
,	O
2016	O
;	O
Lin	O
et	O
al	O
,	O
2018Lin	O
et	O
al	O
,	O
,	O
2019bDu	O
and	O
Cardie	O
,	O
2020	O
;	O
Liu	O
et	O
al	O
,	O
2020a	O
;	O
Lu	O
et	O
al	O
,	O
2021	O
)	O
,	O
but	O
when	O
adapting	O
to	O
new	O
event	O
types	O
and	O
domains	O
,	O
a	O
large	O
number	O
of	O
manually	O
annotated	O
event	O
data	O
is	O
required	O
which	O
is	O
expensive	O
.	O
By	O
contrast	O
,	O
fewshot	O
event	B-TaskName
detection	I-TaskName
(	O
FSED	O
)	O
aims	O
to	O
build	O
effective	O
event	O
detectors	O
that	O
are	O
able	O
to	O
detect	O
new	O
events	O
from	O
instances	O
(	O
query	O
)	O
with	O
a	O
few	O
labeled	O
instances	O
(	O
support	O
set	O
)	O
.	O
Due	O
to	O
their	O
ability	O
to	O
classify	O
novel	O
types	O
,	O
many	O
few	O
-	O
shot	O
algorithms	O
have	O
been	O
used	O
in	O
FSED	O
,	O
e.g.	O
,	O
metric	O
-	O
based	O
methods	O
like	O
Prototypical	O
Network	O
(	O
Lai	O
et	O
al	O
,	O
2020	O
;	O
Deng	B-DatasetName
et	I-DatasetName
al	I-DatasetName
,	O
2020	O
;	O
Cong	O
et	O
al	O
,	O
2021	O
)	O
.	O
Unfortunately	O
,	O
there	O
has	O
long	O
been	O
a	O
"	O
trigger	O
curse	O
"	O
which	O
troubles	O
the	O
learning	O
of	O
event	O
detec	O
-	O
They	O
were	O
killed	O
by	O
hostile	O
[	O
MASK	O
]	O
in	O
Iraqi	O
.	O
They	O
were	O
killed	O
by	O
hostile	O
fire	O
in	O
Iraqi	O
.	O
1	O
or	O
0	B-DatasetName
tion	O
models	O
,	O
especially	O
in	O
few	O
-	O
shot	O
scenario	O
(	O
Bronstein	O
et	O
al	O
,	O
2015	O
;	O
Liu	O
et	O
al	O
,	O
2017	O
;	O
Chen	O
et	O
al	O
,	O
2018	O
;	O
Liu	O
et	O
al	O
,	O
2019	O
;	O
Ji	O
et	O
al	O
,	O
2019	O
)	O
.	O
For	O
many	O
event	O
types	O
,	O
their	O
triggers	O
are	O
dominated	O
by	O
several	O
popular	O
words	O
,	O
e.g.	O
,	O
the	O
Attack	O
event	O
type	O
is	O
dominated	O
by	O
war	O
,	O
attack	O
,	O
fight	O
,	O
fire	O
,	O
bomb	O
in	O
ACE05	O
.	O
And	O
we	O
found	O
the	O
top	O
5	O
triggers	O
of	O
each	O
event	O
type	O
cover	O
78	O
%	O
of	O
event	O
occurrences	O
in	O
ACE05	O
.	O
Due	O
to	O
the	O
trigger	O
curse	O
,	O
event	B-TaskName
detection	I-TaskName
models	O
nearly	O
degenerate	O
to	O
a	O
trigger	O
matcher	O
,	O
ignore	O
the	O
majority	O
of	O
contextual	O
information	O
and	O
mainly	O
rely	O
on	O
whether	O
the	O
candidate	O
word	O
matches	O
the	O
dominant	O
triggers	O
.	O
This	O
problem	O
is	O
more	O
severe	O
in	O
FSED	O
:	O
since	O
the	O
given	O
support	O
instances	O
are	O
very	O
sparse	O
and	O
lack	O
diversity	O
,	O
it	O
is	O
much	O
easier	O
to	O
overfit	O
the	O
trigger	O
of	O
the	O
support	O
instances	O
.	O
An	O
intuitive	O
solution	O
for	O
the	O
trigger	O
curse	O
is	O
to	O
erase	O
the	O
trigger	O
information	O
in	O
instances	O
and	O
forces	O
the	O
model	O
to	O
focus	O
more	O
on	O
the	O
context	O
.	O
Unfortunately	O
,	O
due	O
to	O
the	O
decisive	O
role	O
of	O
triggers	O
,	O
directly	O
wiping	O
out	O
the	O
trigger	O
information	O
commonly	O
hurts	O
the	O
performance	O
(	O
Lu	O
et	O
al	O
,	O
2019	O
;	O
Liu	O
et	O
al	O
,	O
2020b	O
)	O
.	O
Some	O
previous	O
approaches	O
try	O
to	O
tackle	O
this	O
problem	O
by	O
introducing	O
more	O
di	O
-	O
versified	O
context	O
information	O
like	O
event	O
argument	O
information	O
(	O
Liu	O
et	O
al	O
,	O
2017	O
(	O
Liu	O
et	O
al	O
,	O
,	O
2019Ji	O
et	O
al	O
,	O
2019	O
)	O
and	O
document	O
-	O
level	O
information	O
(	O
Ji	O
and	O
Grishman	O
,	O
2008	O
;	O
Liao	O
and	O
Grishman	O
,	O
2010	O
;	O
Duan	O
et	O
al	O
,	O
2017	O
;	O
Chen	O
et	O
al	O
,	O
2018	O
)	O
.	O
However	O
,	O
rich	O
context	O
information	O
is	O
commonly	O
not	O
available	O
for	O
FSED	O
,	O
and	O
therefore	O
these	O
methods	O
can	O
not	O
be	O
directly	O
applied	O
.	O
Query	O
E	O
T	O
C	O
S	O
Y	O
E	O
C	O
T	O
S	O
Y	O
Q	O
(	O
In	O
this	O
paper	O
,	O
we	O
revisit	O
the	O
trigger	O
curse	O
in	O
FSED	O
from	O
a	O
causal	O
view	O
.	O
Specifically	O
,	O
we	O
formulate	O
the	O
data	O
distribution	O
of	O
FSED	O
using	O
a	O
trigger	O
-	O
centric	O
structural	O
causal	O
model	O
(	O
SCM	O
)	O
(	O
Pearl	O
et	O
al	O
,	O
2016	O
)	O
shown	O
in	O
Figure	O
1	O
(	O
a	O
)	O
.	O
Such	O
trigger	O
-	O
centric	O
formulation	O
is	O
based	O
on	O
the	O
fact	O
that	O
,	O
given	O
the	O
event	O
type	O
,	O
contexts	O
have	O
a	O
much	O
lower	O
impact	O
on	O
triggers	O
,	O
compared	O
with	O
the	O
impact	O
of	O
triggers	O
on	O
contexts	O
.	O
This	O
results	O
in	O
the	O
decisive	O
role	O
of	O
triggers	O
in	O
event	B-TaskName
extraction	I-TaskName
,	O
and	O
therefore	O
conventional	O
event	B-TaskName
extraction	I-TaskName
approaches	O
commonly	O
follow	O
the	O
triggercentric	O
procedure	O
(	O
i.e.	O
,	O
identifying	O
triggers	O
first	O
and	O
then	O
using	O
triggers	O
as	O
an	O
indicator	O
to	O
find	O
arguments	O
in	O
contexts	O
)	O
.	O
Furthermore	O
,	O
the	O
case	O
grammar	O
theory	O
in	O
linguistics	O
(	O
Fillmore	O
,	O
1967	O
)	O
also	O
formulate	O
the	O
language	O
using	O
such	O
trigger	O
/	O
predicate	O
-	O
centric	O
assumption	O
,	O
and	O
have	O
been	O
widely	O
exploited	O
in	O
many	O
NLP	O
tasks	O
like	O
semantic	B-TaskName
role	I-TaskName
labeling	I-TaskName
(	O
Gildea	O
and	O
Jurafsky	O
,	O
2002	O
)	O
and	O
abstract	O
meaning	O
representation	O
(	O
Banarescu	O
et	O
al	O
,	O
2013	O
)	O
.	O
From	O
the	O
SCM	O
,	O
we	O
found	O
that	O
T	O
(	O
trigger	O
set	O
)	O
is	O
a	O
confounder	O
of	O
the	O
C	O
(	O
context	O
set	O
)	O
and	O
the	O
Y	O
(	O
result	O
)	O
,	O
and	O
therefore	O
there	O
exists	O
a	O
backdoor	O
path	O
C	O
T	O
Y	O
.	O
The	O
backdoor	O
path	O
explains	O
why	O
previous	O
FSED	O
models	O
disregard	O
contextual	O
information	O
:	O
it	O
misleads	O
the	O
conventional	O
learning	O
procedure	O
to	O
mistakenly	O
regard	O
effects	O
of	O
triggers	O
as	O
the	O
effects	O
of	O
contexts	O
.	O
Consequently	O
,	O
the	O
learning	O
criteria	O
of	O
conventional	O
FSED	O
methods	O
are	O
optimized	O
towards	O
spurious	O
correlation	O
,	O
rather	O
than	O
capturing	O
causality	O
between	O
C	O
and	O
Y	O
.	O
To	O
address	O
this	O
issue	O
,	O
we	O
propose	O
to	O
intervene	O
on	O
context	O
to	O
block	O
the	O
information	O
from	O
trigger	O
to	O
context	O
.	O
Specifically	O
,	O
we	O
apply	O
backdoor	O
adjustment	O
to	O
estimate	O
the	O
interventional	O
distribution	O
that	O
is	O
used	O
for	O
optimizing	O
causality	O
.	O
Furthermore	O
,	O
because	O
backdoor	O
adjustment	O
relies	O
on	O
the	O
unknown	O
prior	O
confounder	O
(	O
trigger	O
)	O
distribution	O
,	O
we	O
also	O
propose	O
to	O
estimate	O
it	O
based	O
on	O
contextualized	O
word	O
prediction	O
.	O
We	O
conducted	O
experiments	O
on	O
ACE05	O
1	O
,	O
MAVEN	B-DatasetName
2	O
and	O
KBP17	O
3	O
datasets	O
.	O
Experiments	O
show	O
that	O
causal	O
intervention	O
can	O
significantly	O
alleviate	O
trigger	O
curse	O
,	O
and	O
therefore	O
the	O
proposed	O
method	O
significantly	O
outperforms	O
previous	O
FSED	O
methods	O
.	O

Causal	B-MethodName
Inference	I-MethodName
.	O
Causal	B-MethodName
inference	I-MethodName
aims	O
to	O
make	O
reliable	O
predictions	O
using	O
the	O
causal	O
effect	O
between	O
variables	O
(	O
Pearl	O
,	O
2009	O
)	O
.	O
Many	O
studies	O
have	O
used	O
causal	O
theory	O
to	O
improve	O
model	O
robustness	O
(	O
Wang	O
et	O
al	O
,	O
2020a	O
,	O
b	O
;	O
Qi	O
et	O
al	O
,	O
2020	O
;	O
Tang	O
et	O
al	O
,	O
2020b	O
;	O
Zeng	O
et	O
al	O
,	O
2020	O
)	O
.	O
Recently	O
,	O
backdoor	O
adjustment	O
has	O
been	O
used	O
to	O
remove	O
the	O
spurious	O
association	O
brought	O
by	O
the	O
confounder	O
(	O
Tang	O
et	O
al	O
,	O
2020a	O
;	O
Yue	O
et	O
al	O
,	O
2020	O
;	O
Zhang	O
et	O
al	O
,	O
2021	O
)	O
.	O
Few	O
-	O
shot	O
Event	B-TaskName
Detection	I-TaskName
.	O
Few	O
-	O
shot	O
event	B-TaskName
detection	I-TaskName
has	O
been	O
studied	O
in	O
many	O
different	O
settings	O
.	O
Bronstein	O
et	O
al	O
(	O
2015	O
)	O
collect	O
some	O
seed	O
triggers	O
,	O
then	O
detect	O
unseen	O
event	O
with	O
feature	O
-	O
based	O
method	O
.	O
Deng	B-DatasetName
et	I-DatasetName
al	I-DatasetName
(	O
2020	O
)	O
decompose	O
FSED	O
into	O
two	O
subtasks	O
:	O
trigger	O
identification	O
and	O
few	O
-	O
shot	O
classification	O
.	O
Feng	O
et	O
al	O
(	O
2020	O
)	O
adopt	O
a	O
sentence	O
-	O
level	O
few	O
-	O
shot	O
classification	O
without	O
triggers	O
.	O
Lai	O
et	O
al	O
(	O
2020	O
)	O
and	O
Cong	O
et	O
al	O
(	O
2021	O
)	O
adopt	O
N+1	O
-	O
way	O
fewshot	O
setting	O
that	O
is	O
closest	O
to	O
our	O
setting	O
.	O

We	O
prove	O
L	O
SG	O
(	O
θ	B-HyperparameterName
)	O
is	O
equivalent	O
to	O
L	O
(	O
θ	B-HyperparameterName
)	O
,	O
which	O
indicates	O
that	O
minimizing	O
L	O
SG	O
(	O
θ	B-HyperparameterName
)	O
is	O
equivalent	O
to	O
minimizing	O
L	O
(	O
θ	B-HyperparameterName
)	O
.	O
At	O
first	O
,	O
we	O
define	O
a	O
function	O
φ	O
(	O
s	O
,	O
q	O
)	O
∝	O
P	O
(	O
Y	O
|	O
s	O
,	O
q	O
;	O
θ	B-HyperparameterName
)	O
and	O
then	O
we	O
need	O
to	O
prove	O
that	O
g	O
(	O
t	O
T	O
s	O
S	O
P	O
(	O
t	O
|	O
e	O
)	O
p	O
(	O
s	O
|	O
C	O
,	O
t	O
)	O
r	O
s	O
,	O
q	O
)	O
=	O
f	O
(	O
t	O
T	O
s	O
S	O
P	O
(	O
t	O
|	O
e	O
)	O
P	O
(	O
s	O
|	O
C	O
,	O
t	O
)	O
φ	O
(	O
s	O
,	O
q	O
)	O
)	O
.	O
From	O
Appendix	O
-	O
A	O
,	O
we	O
can	O
obtain	O
:	O

All	O
of	O
our	O
experiments	O
are	O
implemented	O
on	O
one	O
Nvidia	O
TITAN	B-DatasetName
RTX	O
.	O
Our	O
implementation	O
is	O
based	O
on	O
HuggingFace	O
's	O
Transformers	O
(	O
Wolf	O
et	O
al	O
,	O
2019	O
)	O
and	O
Allennlp	O
(	O
Gardner	O
et	O
al	O
,	O
2018	O
)	O
.	O
We	O
tune	O
the	O
hyperparameters	O
based	O
on	O
the	O
dev	O
performance	O
.	O
We	O
train	O
each	O
model	O
5	O
times	O
with	O
different	O
random	O
seed	O
,	O
and	O
when	O
evaluating	O
,	O
we	O
sample	O
4	O
different	O
support	O
sets	O
.	O

The	O
hyperparameter	O
is	O
shown	O
in	O
Table	O
6	O
.	O
For	O
pretraining	O
,	O
we	O
train	O
a	O
supervised	O
event	B-TaskName
detection	I-TaskName
model	O
using	O
the	O
training	O
set	O
.	O
For	O
finetuning	O
,	O
we	O
use	O
the	O
support	O
set	O
to	O
finetune	O
the	O
parameters	O
of	O
the	O
event	B-TaskName
detection	I-TaskName
model	O
and	O
then	O
detect	O
the	O
event	O
in	O
query	O
.	O

In	O
recent	O
times	O
,	O
multi	O
-	O
modal	O
analysis	O
has	O
been	O
an	O
emerging	O
and	O
highly	O
sought	O
-	O
after	O
field	O
at	O
the	O
intersection	O
of	O
natural	O
language	O
processing	O
,	O
computer	O
vision	O
,	O
and	O
speech	O
processing	O
.	O
The	O
prime	O
objective	O
of	O
such	O
studies	O
is	O
to	O
leverage	O
the	O
diversified	O
information	O
,	O
(	O
e.g.	O
,	O
textual	O
,	O
acoustic	O
and	O
visual	O
)	O
,	O
for	O
learning	O
a	O
model	O
.	O
The	O
effective	O
interaction	O
among	O
these	O
modalities	O
often	O
leads	O
to	O
a	O
better	O
system	O
in	O
terms	O
of	O
performance	O
.	O
In	O
this	O
paper	O
,	O
we	O
introduce	O
a	O
recurrent	O
neural	O
network	O
based	O
approach	O
for	O
the	O
multi	O
-	O
modal	O
sentiment	O
and	O
emotion	B-DatasetName
analysis	O
.	O
The	O
proposed	O
model	O
learns	O
the	O
inter	O
-	O
modal	O
interaction	O
among	O
the	O
participating	O
modalities	O
through	O
an	O
auto	O
-	O
encoder	O
mechanism	O
.	O
We	O
employ	O
a	O
context	O
-	O
aware	O
attention	O
module	O
to	O
exploit	O
the	O
correspondence	O
among	O
the	O
neighboring	O
utterances	O
.	O
We	O
evaluate	O
our	O
proposed	O
approach	O
for	O
five	O
standard	O
multi	O
-	O
modal	O
affect	O
analysis	O
datasets	O
.	O
Experimental	O
results	O
suggest	O
the	O
efficacy	O
of	O
the	O
proposed	O
model	O
for	O
both	O
sentiment	O
and	O
emotion	B-DatasetName
analysis	O
over	O
various	O
existing	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
systems	O
.	O

In	O
recent	O
past	O
,	O
the	O
world	O
has	O
witnessed	O
tremendous	O
growth	O
of	O
various	O
social	O
media	O
platforms	O
,	O
e.g.	O
,	O
YouTube	O
,	O
Instagram	O
,	O
Twitter	O
,	O
Facebook	O
,	O
etc	O
.	O
People	O
treat	O
these	O
platforms	O
as	O
a	O
communication	O
medium	O
and	O
freely	O
express	O
themselves	O
with	O
the	O
help	O
of	O
a	O
diverse	O
set	O
of	O
input	O
sources	O
,	O
e.g.	O
videos	O
,	O
images	O
,	O
audio	O
,	O
text	O
etc	O
.	O
The	O
amount	O
of	O
information	O
produced	O
daily	O
through	O
these	O
mediums	O
are	O
enormous	O
,	O
and	O
hence	O
,	O
the	O
research	O
on	O
multi	O
-	O
modal	O
information	O
processing	O
has	O
attracted	O
attention	O
to	O
the	O
researchers	O
and	O
developers	O
.	O
A	O
video	O
is	O
a	O
multimodal	O
input	O
which	O
provides	O
visual	O
,	O
acoustic	O
,	O
and	O
textual	O
information	O
.	O
The	O
motivation	O
of	O
multi	O
-	O
modal	O
sentiment	O
and	O
emotion	B-DatasetName
analysis	O
lies	O
in	O
fact	O
to	O
leverage	O
the	O
varieties	O
of	O
(	O
often	O
distinct	O
)	O
information	O
from	O
multiple	O
sources	O
for	O
building	O
more	O
efficient	O
systems	O
.	O
For	O
some	O
cases	O
,	O
text	O
can	O
provide	O
a	O
better	O
clue	O
for	O
the	O
prediction	O
,	O
whereas	O
for	O
the	O
others	O
,	O
acoustic	O
or	O
visual	O
sources	O
can	O
be	O
more	O
informative	O
.	O
Similarly	O
,	O
in	O
some	O
situations	O
,	O
a	O
combination	O
of	O
two	O
or	O
more	O
information	O
sources	O
together	O
ensures	O
better	O
and	O
unambiguous	O
classification	O
decision	O
.	O
For	O
example	O
,	O
only	O
text	O
"	O
shut	O
up	O
"	O
can	O
not	O
decide	O
the	O
mood	O
of	O
a	O
person	O
but	O
acoustic	O
(	O
tone	O
of	O
a	O
person	O
)	O
and	O
visual	O
(	O
expression	O
of	O
a	O
person	O
)	O
can	O
reveal	O
the	O
exact	O
mood	O
.	O
Similarly	O
,	O
for	O
some	O
instances	O
visual	O
features	O
such	O
as	O
gesture	O
,	O
postures	O
,	O
facial	O
expression	O
etc	O
.	O
have	O
important	O
roles	O
to	O
play	O
in	O
determining	O
the	O
correctness	O
of	O
the	O
system	O
.	O
However	O
,	O
effectively	O
combining	O
this	O
information	O
is	O
a	O
nontrivial	O
task	O
that	O
researchers	O
often	O
have	O
to	O
face	O
(	O
Poria	O
et	O
al	O
,	O
2016	O
;	O
Ranganathan	O
et	O
al	O
,	O
2016	O
;	O
Lee	O
et	O
al	O
,	O
2018	O
)	O
.	O
Traditionally	O
,	O
'	O
text	O
'	O
has	O
been	O
the	O
key	O
factor	O
in	O
any	O
Natural	O
Language	O
Processing	O
(	O
NLP	O
)	O
tasks	O
,	O
including	O
sentiment	O
and	O
emotion	B-DatasetName
analysis	O
.	O
However	O
,	O
with	O
the	O
recent	O
emergence	O
of	O
social	O
media	O
platforms	O
,	O
an	O
interdisciplinary	O
study	O
involving	O
text	O
,	O
visual	O
and	O
acoustic	O
features	O
have	O
drawn	O
a	O
great	O
interest	O
among	O
the	O
research	O
community	O
.	O
Expressing	O
the	O
feelings	O
and	O
emotions	O
through	O
a	O
video	O
is	O
much	O
convenient	O
than	O
the	O
text	O
for	O
a	O
user	O
,	O
and	O
it	O
is	O
the	O
best	O
source	O
to	O
extract	O
all	O
multi	O
-	O
modal	O
information	O
.	O
Not	O
only	O
the	O
visual	O
,	O
it	O
also	O
provides	O
other	O
information	O
such	O
as	O
acoustic	O
and	O
textual	O
representation	O
of	O
spoken	O
language	O
.	O
Additionally	O
,	O
a	O
single	O
video	O
can	O
have	O
multiple	O
utterances	O
based	O
on	O
a	O
speaker	O
's	O
pause	O
(	O
speech	O
bounded	O
by	O
breaths	O
)	O
with	O
different	O
sentiments	O
and	O
emotions	O
.	O
The	O
sentiments	O
and	O
emotions	O
of	O
an	O
utterance	O
often	O
have	O
interdependence	O
on	O
the	O
other	O
contextual	O
utterances	O
.	O
Independently	O
classifying	O
such	O
an	O
utterance	O
poses	O
several	O
challenges	O
to	O
the	O
underlying	O
problem	O
.	O
In	O
contrast	O
,	O
multi	O
-	O
modal	O
sentiment	O
and	O
emotion	B-DatasetName
analysis	O
take	O
inputs	O
from	O
more	O
than	O
one	O
sources	O
e.g.	O
text	O
,	O
visual	O
,	O
acoustic	O
for	O
the	O
analysis	O
.	O
Effectively	O
fusing	O
this	O
diverse	O
information	O
is	O
non	O
-	O
trivial	O
and	O
poses	O
several	O
challenges	O
to	O
the	O
underlying	O
problem	O
.	O
In	O
our	O
current	O
work	O
,	O
we	O
propose	O
an	O
end	O
-	O
to	O
-	O
end	O
Context	O
-	O
aware	O
Interactive	O
Attention	O
(	O
CIA	O
)	O
based	O
recurrent	O
neural	O
network	O
for	O
sentiment	O
and	O
emotion	B-DatasetName
analysis	O
.	O
We	O
aim	O
to	O
leverage	O
the	O
interaction	O
between	O
the	O
modalities	O
to	O
increase	O
the	O
confidence	O
of	O
individual	O
task	O
in	O
prediction	O
.	O
The	O
main	O
contributions	O
of	O
our	O
current	O
research	O
are	O
as	O
follows	O
:	O
(	O
1	O
)	O
We	O
propose	O
an	O
Inter	O
-	O
modal	O
Interactive	O
Module	O
(	O
IIM	O
)	O
that	O
aims	O
to	O
learn	O
the	O
interaction	O
among	O
the	O
diverse	O
and	O
distinct	O
features	O
of	O
the	O
input	O
modalities	O
,	O
i.e.	O
,	O
text	O
,	O
acoustic	O
and	O
visual	O
;	O
(	O
2	O
)	O
We	O
employ	O
a	O
Context	O
-	O
aware	O
Attention	O
Module	O
(	O
CAM	B-MethodName
)	O
that	O
identifies	O
and	O
assigns	O
the	O
weights	O
to	O
the	O
neighboring	O
utterances	O
based	O
on	O
their	O
contributing	O
features	O
.	O
It	O
exploits	O
the	O
interactive	O
representations	O
of	O
pairwise	O
modalities	O
to	O
learn	O
the	O
attention	O
weights	O
,	O
and	O
(	O
3	O
)	O
We	O
present	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
arts	O
for	O
five	O
benchmark	O
datasets	O
for	O
both	O
sentiment	O
and	O
emotion	B-DatasetName
predictions	O
.	O

Since	O
the	O
utterances	O
in	O
a	O
video	O
are	O
the	O
split	O
units	O
of	O
the	O
break	O
/	O
pause	O
of	O
the	O
speech	O
,	O
their	O
emotions	O
(	O
or	O
sentiments	O
)	O
often	O
have	O
relations	O
with	O
their	O
neighboring	O
utterances	O
.	O
Therefore	O
,	O
knowledge	O
of	O
the	O
emotions	O
(	O
or	O
,	O
sentiments	O
)	O
of	O
the	O
neighboring	O
utterances	O
is	O
an	O
important	O
piece	O
of	O
information	O
and	O
has	O
the	O
capability	O
to	O
derive	O
the	O
prediction	O
of	O
an	O
utterance	O
,	O
if	O
the	O
available	O
inputs	O
are	O
insufficient	O
for	O
the	O
correct	O
prediction	O
.	O
Our	O
proposed	O
context	O
-	O
aware	O
attention	O
module	O
leverages	O
the	O
contextual	O
information	O
.	O
For	O
each	O
utterance	O
in	O
a	O
video	O
,	O
we	O
compute	O
the	O
attention	O
weights	O
of	O
all	O
the	O
neighboring	O
utterances	O
based	O
on	O
their	O
contributions	O
in	O
predicting	O
the	O
current	O
utterance	O
.	O
It	O
ensures	O
that	O
the	O
network	O
properly	O
utilizes	O
the	O
local	O
contextual	O
information	O
of	O
an	O
utterance	O
as	O
well	O
as	O
the	O
global	O
contextual	O
information	O
of	O
a	O
video	O
together	O
.	O
The	O
aim	O
is	O
to	O
compute	O
the	O
interactive	O
attention	O
weights	O
utilizing	O
a	O
softmax	B-MethodName
activation	O
for	O
each	O
utterance	O
in	O
the	O
video	O
.	O
Next	O
,	O
we	O
apply	O
a	O
multiplicative	O
gating	O
mechanism	O
following	O
the	O
work	O
of	O
Dhingra	O
et	O
al	O
(	O
2016	O
)	O
.	O
The	O
attentive	O
representation	O
is	O
,	O
then	O
,	O
forwarded	O
to	O
the	O
upper	O
layers	O
for	O
further	O
processing	O
.	O
We	O
summarize	O
the	O
process	O
of	O
CAM	B-MethodName
in	O
Algorithm3	O
.	O

One	O
of	O
the	O
key	O
objectives	O
of	O
the	O
multi	O
-	O
modal	O
analysis	O
is	O
to	O
fuse	O
the	O
available	O
input	O
modalities	O
effectively	O
.	O
In	O
general	O
,	O
different	O
modalities	O
represent	O
distinct	O
features	O
despite	O
serving	O
a	O
common	O
goal	O
.	O
For	O
example	O
,	O
in	O
multi	O
-	O
modal	O
sentiment	B-TaskName
analysis	I-TaskName
all	O
the	O
three	O
modalities	O
,	O
i.e.	O
,	O
text	O
,	O
acoustic	O
,	O
and	O
visual	O
,	O
aim	O
to	O
predict	O
the	O
expressed	O
polarity	O
of	O
an	O
utterance	O
.	O
The	O
distinctive	O
features	O
in	O
isolation	O
might	O
create	O
an	O
ambiguous	O
scenario	O
for	O
a	O
network	O
to	O
learn	O
effectively	O
.	O
Therefore	O
,	O
we	O
introduce	O
an	O
auto	O
-	O
encoder	O
based	O
inter	O
-	O
modal	O
interactive	O
module	O
whose	O
objective	O
is	O
to	O
learn	O
the	O
interaction	O
between	O
two	O
distinct	O
modalities	O
to	O
serve	O
a	O
common	O
goal	O
.	O
The	O
IIM	O
encodes	O
the	O
feature	O
representation	O
of	O
one	O
modality	O
(	O
say	O
,	O
text	O
)	O
,	O
and	O
aims	O
to	O
decode	O
it	O
into	O
the	O
feature	O
representation	O
of	O
another	O
modality	O
(	O
say	O
,	O
acoustic	O
)	O
.	O
Similar	O
to	O
an	O
auto	O
-	O
encoder	O
where	O
the	O
input	O
and	O
output	O
are	O
conceptually	O
the	O
same	O
(	O
or	O
closely	O
related	O
)	O
,	O
in	O
our	O
case	O
the	O
input	O
and	O
output	O
feature	O
representations	O
of	O
two	O
modalities	O
also	O
intuitively	O
serve	O
a	O
common	O
goal	O
.	O
After	O
training	O
of	O
IIM	O
,	O
the	O
encoded	O
vector	O
signifies	O
a	O
joint	O
representation	O
of	O
the	O
two	O
modalities	O
,	O
which	O
can	O
be	O
further	O
utilized	O
in	O
the	O
network	O
.	O
As	O
the	O
proposed	O
architecture	O
in	O
Figure	O
1	O
depicts	O
,	O
our	O
proposed	O
model	O
is	O
an	O
end	O
-	O
to	O
-	O
end	O
system	O
,	O
which	O
takes	O
multi	O
-	O
modal	O
raw	O
features	O
for	O
each	O
utterance	O
in	O
a	O
video	O
and	O
predicts	O
an	O
output	O
.	O
We	O
also	O
train	O
our	O
proposed	O
IIM	O
in	O
the	O
combined	O
framework	O
.	O
For	O
any	O
pair	O
of	O
modalities	O
,	O
e.g.	O
,	O
text	O
-	O
visual	O
,	O
the	O
encoded	O
vector	O
in	O
IIM	O
receives	O
two	O
gradients	O
of	O
errors	O
,	O
i.e.	O
,	O
one	O
error	O
from	O
the	O
IIM	O
output	O
(	O
visual	O
)	O
l	O
1	O
and	O
another	O
from	O
the	O
task	O
-	O
specific	O
label	O
l	O
2	O
.	O
We	O
aggregate	O
the	O
errors	O
(	O
l	O
1	O
+	O
l	O
2	O
)	O
at	O
the	O
encoded	O
vector	O
and	O
backpropagate	O
it	O
to	O
the	O
input	O
(	O
text	O
)	O
.	O
Thus	O
,	O
the	O
weights	O
in	O
the	O
encoder	O
part	O
will	O
adjust	O
according	O
to	O
the	O
desired	O
task	O
-	O
specific	O
label	O
as	O
well	O
.	O
However	O
,	O
in	O
contrast	O
,	O
the	O
decoder	O
part	O
does	O
not	O
have	O
such	O
information	O
.	O
Therefore	O
,	O
we	O
employ	O
another	O
IIM	O
to	O
capture	O
the	O
interaction	O
between	O
the	O
visual	O
-	O
text	O
.	O
This	O
time	O
,	O
the	O
visual	O
features	O
are	O
aware	O
of	O
the	O
desired	O
label	O
during	O
the	O
interaction	O
with	O
textual	O
features	O
.	O
A	O
conceptual	O
diagram	O
,	O
depicting	O
the	O
gradient	O
flow	O
in	O
IIM	O
for	O
the	O
text	O
and	O
visual	O
modalities	O
,	O
is	O
shown	O
in	O
Figure	O
2	O
.	O

The	O
above	O
datasets	O
offer	O
different	O
dimension	O
of	O
sentiment	B-TaskName
analysis	I-TaskName
.	O
We	O
define	O
the	O
following	O
setups	O
for	O
our	O
experiments	O
.	O
Two	O
-	O
class	O
(	O
pos	O
and	O
neg	O
)	O
classification	O
:	O
MO	O
-	O
SEI	O
,	O
MOSI	B-DatasetName
,	O
ICT	O
-	O
MMMO	O
,	O
and	O
MOUD	O
.	O
Three	O
-	O
class	O
(	O
pos	O
,	O
neu	O
,	O
and	O
neg	O
)	O
classification	O
:	O
YouTube	O
.	O
Five	O
-	O
class	O
(	O
strong	O
pos	O
,	O
weak	O
pos	O
,	O
neu	O
,	O
weak	O
neg	O
,	O
and	O
strong	O
neg	O
)	O
classification	O
:	O
MOSEI	O
.	O
Seven	O
-	O
class	O
(	O
strong	O
pos	O
,	O
moderate	O
pos	O
,	O
weak	O
pos	O
,	O
neu	O
,	O
weak	O
neg	O
,	O
moderate	O
neg	O
,	O
and	O
strong	O
neg	O
)	O
classification	O
:	O
MOSEI	O
and	O
MOSI	B-DatasetName
.	O
Intensity	O
prediction	O
:	O
MOSEI	O
and	O
MOSI	B-DatasetName
.	O

We	O
analyze	O
our	O
proposed	O
CIA	O
model	O
to	O
understand	O
the	O
importance	O
of	O
the	O
baseline	O
framework	O
CIA	O
-	O
IIM	O
.	O
We	O
study	O
the	O
predictions	O
of	O
both	O
the	O
models	O
and	O
observe	O
that	O
the	O
proposed	O
CIA	O
framework	O
improves	O
the	O
predictions	O
of	O
the	O
baseline	O
CIA	O
-	O
IIM	O
model	O
.	O
It	O
indicates	O
that	O
the	O
CIA	O
framework	O
,	O
indeed	O
,	O
learns	O
the	O
interaction	O
among	O
the	O
input	O
modalities	O
,	O
and	O
the	O
model	O
effectively	O
exploits	O
this	O
interaction	O
for	O
better	O
judgment	O
.	O
In	O
Table	O
6	O
,	O
we	O
list	O
the	O
utterances	O
of	O
a	O
CMU	B-DatasetName
-	I-DatasetName
MOSEI	I-DatasetName
video	O
along	O
with	O
their	O
correct	O
and	O
predicted	O
labels	O
for	O
both	O
the	O
proposed	O
and	O
baseline	O
systems	O
.	O
The	O
video	O
in	O
Table	O
6	O
has	O
4	O
utterances	O
,	O
out	O
of	O
which	O
the	O
correct	O
sentiments	O
of	O
three	O
utterances	O
(	O
i.e.	O
,	O
u	O
1	O
,	O
u	O
3	O
,	O
and	O
u	O
4	O
)	O
are	O
positive	O
,	O
while	O
one	O
utterance	O
(	O
i.e.	O
,	O
u	O
2	O
)	O
is	O
negative	O
.	O
We	O
observe	O
that	O
our	O
proposed	O
CIA	O
model	O
predicts	O
all	O
the	O
4	O
utterances	O
correctly	O
,	O
while	O
the	O
CIA	O
-	O
IIM	O
mis	O
-	O
classify	O
the	O
sentiments	O
of	O
the	O
utterances	O
,	O
u	O
2	O
and	O
u	O
3	O
.	O
We	O
also	O
analyze	O
the	O
context	O
-	O
aware	O
attention	O
module	O
(	O
CAM	B-MethodName
)	O
with	O
the	O
help	O
of	O
heatmaps	O
of	O
the	O
attention	O
weights	O
.	O
The	O
heatmaps	O
,	O
as	O
depicted	O
in	O
Figure	O
3	O
,	O
represent	O
the	O
contributing	O
utterances	O
in	O
the	O
neighbourhood	O
for	O
the	O
classification	O
of	O
each	O
utterance	O
.	O
Figures	O
3a	O
,	O
3b	O
and	O
3c	O
show	O
the	O
heatmaps	O
of	O
the	O
pair	O
-	O
wise	O
modality	O
interaction	O
of	O
the	O
proposed	O
model	O
CIA	O
.	O
In	O
Figure	O
3a	O
,	O
each	O
cell	O
(	O
i	O
,	O
j	O
)	O
of	O
the	O
heatmap	B-MethodName
signifies	O
the	O
weights	O
of	O
utterance	O
'	O
j	O
'	O
for	O
the	O
classification	O
of	O
utterance	O
'	O
i	O
'	O
.	O
For	O
the	O
utterance	O
u	O
4	O
,	O
the	O
model	O
puts	O
more	O
attention	O
weights	O
on	O
the	O
u	O
2	O
and	O
u	O
3	O
of	O
the	O
text	O
-	O
visual	O
interactions	O
,	O
while	O
for	O
the	O
text	O
-	O
acoustic	O
interaction	O
the	O
model	O
assigns	O
higher	O
weights	O
to	O
the	O
u	O
4	O
utterance	O
itself	O
.	O
Similarly	O
,	O
the	O
model	O
assigns	O
the	O
least	O
weight	O
to	O
the	O
u	O
1	O
utterance	O
,	O
whereas	O
the	O
utterance	O
u	O
3	O
gets	O
the	O
highest	O
weights	O
.	O
We	O
argue	O
that	O
the	O
proposed	O
CAM	B-MethodName
module	O
captures	O
the	O
diversity	O
in	O
the	O
input	O
modalities	O
of	O
the	O
contextual	O
utterances	O
for	O
the	O
correct	O
prediction	O
.	O
For	O
emotion	B-DatasetName
prediction	O
,	O
the	O
CIA	O
model	O
captures	O
all	O
the	O
emotions	O
correctly	O
,	O
while	O
the	O
CIA	O
-	O
IIM	O
framework	O
fails	O
to	O
predict	O
the	O
correct	O
emotions	O
of	O
the	O
utterances	O
,	O
u	O
2	O
and	O
u	O
3	O
.	O
For	O
the	O
same	O
video	O
,	O
we	O
also	O
show	O
the	O
attention	O
heatmaps	O
for	O
emotion	B-DatasetName
in	O
Figure	O
3	O
.	O
For	O
the	O
utterance	O
u	O
2	O
,	O
our	O
proposed	O
model	O
(	O
CIA	O
)	O
captures	O
the	O
emotion	B-DatasetName
class	O
'	O
sad	O
'	O
as	O
the	O
CAM	B-MethodName
module	O
assigns	O
higher	O
attention	O
weights	O
on	O
the	O
utterances	O
u	O
2	O
and	O
u	O
3	O
in	O
Figure	O
3d	O
,	O
u	O
4	O
in	O
Figure	O
3e	O
,	O
and	O
u	O
2	O
in	O
Figure	O
3f	O
.	O
Since	O
the	O
system	O
finds	O
the	O
contributing	O
neighbours	O
as	O
utterances	O
u	O
2	O
,	O
u	O
3	O
and	O
u	O
4	O
for	O
various	O
combinations	O
,	O
we	O
argue	O
that	O
it	O
utilizes	O
the	O
information	O
of	O
these	O
utterances	O
-	O
which	O
all	O
express	O
the	O
'	O
sad	O
'	O
emotion	B-DatasetName
-	O
for	O
the	O
correct	O
prediction	O
of	O
utterance	O
u	O
2	O
as	O
'	O
sad	O
'	O
.	O

In	O
this	O
paper	O
,	O
we	O
have	O
proposed	O
a	O
Context	O
-	O
aware	O
Interactive	O
Attention	O
framework	O
that	O
aims	O
to	O
capture	O
the	O
interaction	O
between	O
the	O
input	O
modalities	O
for	O
the	O
multi	O
-	O
modal	O
sentiment	O
and	O
emotion	B-DatasetName
prediction	O
.	O
We	O
employed	O
a	O
contextual	O
attention	O
module	O
to	O
learn	O
the	O
contributing	O
utterances	O
in	O
the	O
neighborhood	O
by	O
exploiting	O
the	O
interaction	O
among	O
the	O
input	O
modalities	O
.	O
We	O
evaluate	O
our	O
proposed	O
approach	O
on	O
five	O
standard	O
multi	O
-	O
modal	O
datasets	O
.	O
Experiments	O
suggest	O
the	O
effectiveness	O
of	O
the	O
proposed	O
model	O
over	O
various	O
existing	O
systems	O
,	O
for	O
both	O
sentiment	O
and	O
emotion	B-DatasetName
analysis	O
,	O
as	O
we	O
obtained	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
for	O
all	O
five	O
datasets	O
.	O
In	O
current	O
work	O
,	O
we	O
undertook	O
the	O
problem	O
of	O
sentiment	O
and	O
emotion	B-DatasetName
analysis	O
for	O
a	O
single	O
-	O
party	O
utterances	O
.	O
In	O
future	O
,	O
we	O
would	O
like	O
to	O
extend	O
our	O
work	O
towards	O
the	O
multi	O
-	O
party	O
dialogue	O
.	O
6	O
Acknowledgment	O

Improving	O
Graph	O
-	O
based	O
Sentence	B-TaskName
Ordering	I-TaskName
with	O
Iteratively	O
Predicted	O
Pairwise	O
Orderings	O

Dominant	O
sentence	B-TaskName
ordering	I-TaskName
models	O
can	O
be	O
classified	O
into	O
pairwise	O
ordering	O
models	O
and	O
set	O
-	O
to	O
-	O
sequence	O
models	O
.	O
However	O
,	O
there	O
is	O
little	O
attempt	O
to	O
combine	O
these	O
two	O
types	O
of	O
models	O
,	O
which	O
inituitively	O
possess	O
complementary	O
advantages	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
sentence	B-TaskName
ordering	I-TaskName
framework	O
which	O
introduces	O
two	O
classifiers	O
to	O
make	O
better	O
use	O
of	O
pairwise	O
orderings	O
for	O
graph	O
-	O
based	O
sentence	B-TaskName
ordering	I-TaskName
(	O
Yin	O
et	O
al	O
,	O
2019	O
(	O
Yin	O
et	O
al	O
,	O
,	O
2021	O
.	O
Specially	O
,	O
given	O
an	O
initial	O
sentence	O
-	O
entity	O
graph	O
,	O
we	O
first	O
introduce	O
a	O
graph	O
-	O
based	O
classifier	O
to	O
predict	O
pairwise	O
orderings	O
between	O
linked	O
sentences	O
.	O
Then	O
,	O
in	O
an	O
iterative	O
manner	O
,	O
based	O
on	O
the	O
graph	O
updated	O
by	O
previously	O
predicted	O
highconfident	O
pairwise	O
orderings	O
,	O
another	O
classifier	O
is	O
used	O
to	O
predict	O
the	O
remaining	O
uncertain	O
pairwise	O
orderings	O
.	O
At	O
last	O
,	O
we	O
adapt	O
a	O
GRN	O
-	O
based	O
sentence	B-TaskName
ordering	I-TaskName
model	O
(	O
Yin	O
et	O
al	O
,	O
2019	O
(	O
Yin	O
et	O
al	O
,	O
,	O
2021	O
on	O
the	O
basis	O
of	O
final	O
graph	O
.	O
Experiments	O
on	O
five	O
commonly	O
-	O
used	O
datasets	O
demonstrate	O
the	O
effectiveness	O
and	O
generality	O
of	O
our	O
model	O
.	O
Particularly	O
,	O
when	O
equipped	O
with	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
and	O
FHDecoder	O
(	O
Yin	O
et	O
al	O
,	O
2020	O
)	O
,	O
our	O
model	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
.	O
Our	O
code	O
is	O
available	O
at	O
https://	O
github.com/DeepLearnXMU/IRSEG	O
.	O

With	O
the	O
rapid	O
development	O
and	O
increasing	O
applications	O
of	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
,	O
modeling	O
text	O
coherence	O
has	O
become	O
a	O
significant	O
task	O
,	O
since	O
it	O
can	O
provide	O
beneficial	O
information	O
for	O
understanding	O
,	O
evaluating	O
and	O
generating	O
multi	O
-	O
sentence	O
texts	O
.	O
As	O
an	O
important	O
subtask	O
,	O
sentence	B-TaskName
ordering	I-TaskName
aims	O
at	O
recovering	O
unordered	O
sentences	O
back	O
to	O
naturally	O
coherent	O
paragraphs	O
.	O
It	O
is	O
required	O
to	O
deal	O
with	O
logic	O
and	O
syntactic	O
consistency	O
,	O
and	O
has	O
increasingly	O
attracted	O
attention	O
due	O
to	O
its	O
wide	O
applications	O
on	O
several	O
tasks	O
such	O
as	O
text	B-TaskName
generation	I-TaskName
(	O
Konstas	O
and	O
Lapata	O
,	O
2012	O
;	O
Holtzman	O
et	O
al	O
,	O
2018	O
)	O
Recently	O
,	O
inspired	O
by	O
the	O
great	O
success	O
of	O
deep	O
learning	O
in	O
other	O
NLP	O
tasks	O
,	O
researchers	O
have	O
resorted	O
to	O
neural	O
sentence	B-TaskName
ordering	I-TaskName
models	O
,	O
which	O
can	O
be	O
classified	O
into	O
:	O
pairwise	O
ordering	O
models	O
Agrawal	O
et	O
al	O
,	O
2016	O
;	O
Li	O
and	O
Jurafsky	O
,	O
2017	O
;	O
Moon	O
et	O
al	O
,	O
2019	O
;	O
Kumar	B-DatasetName
et	O
al	O
,	O
2020	O
;	O
Prabhumoye	O
et	O
al	O
,	O
2020	O
;	O
Zhu	O
et	O
al	O
,	O
2021	O
)	O
and	O
set	O
-	O
to	O
-	O
sequence	O
models	O
(	O
Gong	O
et	O
al	O
,	O
2016	O
;	O
Nguyen	O
and	O
Joty	O
,	O
2017	O
;	O
Logeswaran	O
et	O
al	O
,	O
2018	O
;	O
Mohiuddin	O
et	O
al	O
,	O
2018	O
;	O
Cui	O
et	O
al	O
,	O
2018	O
;	O
Yin	O
et	O
al	O
,	O
2019	O
;	O
Oh	O
et	O
al	O
,	O
2019	O
;	O
Yin	O
et	O
al	O
,	O
2020	O
;	O
Cui	O
et	O
al	O
,	O
2020	O
;	O
Yin	O
et	O
al	O
,	O
2021	O
)	O
.	O
Generally	O
,	O
the	O
former	O
predicts	O
the	O
relative	O
orderings	O
between	O
pairwise	O
sentences	O
,	O
which	O
are	O
then	O
leveraged	O
to	O
produce	O
the	O
final	O
ordered	O
sentence	O
sequence	O
.	O
Its	O
advantage	O
lies	O
in	O
the	O
lightweight	O
pairwise	O
ordering	O
predictions	O
,	O
since	O
the	O
predictions	O
only	O
depend	O
on	O
the	O
semantic	O
representations	O
of	O
involved	O
sentences	O
.	O
By	O
contrast	O
,	O
the	O
latter	O
is	O
mainly	O
based	O
on	O
an	O
encoder	O
-	O
decoder	O
framework	O
,	O
where	O
an	O
encoder	O
is	O
first	O
used	O
to	O
learn	O
contexualized	O
sentence	O
representations	O
by	O
considering	O
other	O
sentences	O
,	O
and	O
then	O
a	O
decoder	O
,	O
such	O
as	O
pointer	B-MethodName
network	I-MethodName
(	O
Vinyals	O
et	O
al	O
,	O
2015a	O
)	O
,	O
outputs	O
ordered	O
sentences	O
.	O
Overall	O
,	O
these	O
two	O
kinds	O
of	O
models	O
have	O
their	O
own	O
strengths	O
,	O
which	O
are	O
complementary	O
to	O
each	O
other	O
.	O
To	O
combine	O
their	O
advantages	O
,	O
Yin	O
et	O
al	O
(	O
2020	O
)	O
propose	O
FHDecoder	O
that	O
is	O
equipped	O
with	O
three	O
pairwise	O
ordering	O
prediction	O
modules	O
to	O
enhance	O
the	O
pointer	B-MethodName
network	I-MethodName
decoder	O
.	O
Along	O
this	O
line	O
,	O
Cui	O
et	O
al	O
(	O
2020	O
)	O
introduce	O
BERT	B-MethodName
to	O
exploit	O
the	O
deep	O
semantic	O
connection	O
and	O
relative	O
orderings	O
between	O
sentences	O
and	O
achieve	O
SOTA	O
performance	O
when	O
equipped	O
with	O
FHDecoder	O
.	O
However	O
,	O
there	O
still	O
exist	O
two	O
drawbacks	O
:	O
1	O
)	O
their	O
pairwise	O
ordering	O
predictions	O
only	O
depend	O
on	O
involved	O
sentence	O
pairs	O
,	O
without	O
considering	O
other	O
sentences	O
in	O
the	O
same	O
set	O
;	O
2	O
)	O
their	O
one	O
-	O
pass	O
pairwise	O
ordering	O
predictions	O
are	O
relatively	O
rough	O
,	O
ignoring	O
distinct	O
difficulties	O
in	O
predicting	O
different	O
sentence	O
pairs	O
.	O
Therefore	O
,	O
we	O
believe	O
that	O
the	O
potential	O
of	O
pairwise	O
orderings	O
in	O
neural	O
sentence	B-TaskName
ordering	I-TaskName
models	O
has	O
not	O
been	O
fully	O
exploited	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
iterative	O
pairwise	O
ordering	O
prediction	O
framework	O
which	O
introduces	O
two	O
classifiers	O
to	O
make	O
better	O
use	O
of	O
pairwise	O
orderings	O
for	O
graph	O
-	O
based	O
sentence	B-TaskName
ordering	I-TaskName
(	O
Yin	O
et	O
al	O
,	O
2019	O
(	O
Yin	O
et	O
al	O
,	O
,	O
2021	O
.	O
As	O
an	O
extension	O
of	O
Sentence	O
-	O
Enity	O
Graph	O
Recurrent	O
Network	O
(	O
SE	O
-	O
GRN	O
)	O
(	O
Yin	O
et	O
al	O
,	O
2019	O
(	O
Yin	O
et	O
al	O
,	O
,	O
2021	O
,	O
our	O
framework	O
enriches	O
the	O
graph	O
representation	O
with	O
iteratively	O
predicted	O
orderings	O
between	O
pairwise	O
sentences	O
,	O
which	O
further	O
benefits	O
the	O
subsequent	O
generation	O
of	O
ordered	O
sentences	O
.	O
The	O
basic	O
intuitions	O
behind	O
our	O
work	O
are	O
two	O
-	O
fold	O
.	O
First	O
,	O
learning	O
contextual	O
sentence	O
representations	O
is	O
helpful	O
to	O
predict	O
pairwise	O
orderings	O
.	O
Second	O
,	O
difficulties	O
of	O
predicting	O
ordering	O
vary	O
with	O
respect	O
to	O
different	O
sentence	O
pairs	O
.	O
Thus	O
,	O
it	O
is	O
more	O
reasonable	O
to	O
first	O
predict	O
the	O
orderings	O
of	O
pairwise	O
sentences	O
easily	O
to	O
be	O
predicted	O
,	O
and	O
then	O
leverage	O
these	O
predicted	O
orderings	O
to	O
refine	O
the	O
predictions	O
for	O
other	O
pairwise	O
sentences	O
.	O
Concretely	O
,	O
we	O
propose	O
two	O
graph	O
-	O
based	O
classifiers	O
to	O
iteratively	O
conduct	O
ordering	O
predictions	O
for	O
pairwise	O
sentences	O
.	O
The	O
first	O
classifier	O
takes	O
the	O
sentence	O
-	O
entity	O
graph	O
(	O
SE	O
-	O
Graph	O
)	O
(	O
Yin	O
et	O
al	O
,	O
2019	O
(	O
Yin	O
et	O
al	O
,	O
,	O
2021	O
as	O
input	O
and	O
yields	O
relative	O
orderings	O
of	O
linked	O
sentences	O
via	O
corresponding	O
probabilities	O
.	O
Next	O
,	O
in	O
an	O
iterative	O
manner	O
,	O
the	O
second	O
classifier	O
enriches	O
the	O
previous	O
graph	O
representation	O
by	O
converting	O
high	O
-	O
value	O
probabilities	O
into	O
the	O
weights	O
of	O
the	O
corresponding	O
edges	O
,	O
and	O
then	O
reconduct	O
graph	O
encoding	O
to	O
predict	O
orderings	O
for	O
the	O
other	O
pairwise	O
sentences	O
.	O
Based	O
on	O
the	O
final	O
weighted	O
graph	O
representation	O
,	O
we	O
adapt	O
SE	O
-	O
GRN	O
to	O
construct	O
a	O
graph	O
-	O
based	O
sentence	B-TaskName
ordering	I-TaskName
model	O
,	O
of	O
which	O
the	O
decoder	O
is	O
also	O
a	O
pointer	B-MethodName
network	I-MethodName
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
our	O
work	O
is	O
the	O
first	O
to	O
exploit	O
pairwise	O
orderings	O
to	O
enhance	O
the	O
graph	O
encoding	O
for	O
graph	O
-	O
based	O
set	O
-	O
to	O
-	O
squence	O
sentence	B-TaskName
ordering	I-TaskName
.	O
To	O
investigate	O
the	O
effectiveness	O
of	O
our	O
framework	O
,	O
we	O
conduct	O
extensive	O
experiments	O
on	O
several	O
commonly	O
-	O
used	O
datasets	O
.	O
Experimental	O
results	O
and	O
in	O
-	O
depth	O
analyses	O
show	O
that	O
our	O
model	O
enhanced	O
with	O
some	O
proposed	O
technologies	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
;	O
Yin	O
et	O
al	O
,	O
2020	O
)	O
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
.	O

Early	O
studies	O
mainly	O
focused	O
on	O
exploring	O
humandesigned	O
features	O
for	O
sentence	B-TaskName
ordering	I-TaskName
(	O
Lapata	O
,	O
2003	O
;	O
Barzilay	O
and	O
Lee	O
,	O
2004	O
;	O
Lapata	O
,	O
2005	O
,	O
2008	O
;	O
Elsner	O
and	O
Charniak	O
,	O
2011	O
;	O
Guinaudeau	O
and	O
Strube	O
,	O
2013	O
)	O
.	O
Recently	O
,	O
neural	O
network	O
based	O
sentence	B-TaskName
ordering	I-TaskName
models	O
have	O
become	O
dominant	O
,	O
consisting	O
of	O
the	O
following	O
two	O
kinds	O
of	O
models	O
:	O
1	O
)	O
Pairwise	O
models	O
.	O
Generally	O
,	O
they	O
first	O
predict	O
the	O
pairwise	O
orderings	O
between	O
sentences	O
and	O
then	O
use	O
them	O
to	O
produce	O
the	O
final	O
sentence	O
order	O
via	O
ranking	O
algorithms	O
Agrawal	O
et	O
al	O
,	O
2016	O
;	O
Li	O
and	O
Jurafsky	O
,	O
2017	O
;	O
Kumar	B-DatasetName
et	O
al	O
,	O
2020	O
;	O
Prabhumoye	O
et	O
al	O
,	O
2020	O
;	O
Zhu	O
et	O
al	O
,	O
2021	O
)	O
.	O
For	O
example	O
,	O
first	O
framed	O
sentence	B-TaskName
ordering	I-TaskName
as	O
a	O
ranking	O
task	O
conditioned	O
on	O
pairwise	O
scores	O
.	O
Agrawal	O
et	O
al	O
(	O
2016	O
)	O
conducted	O
the	O
same	O
experiments	O
as	O
in	O
the	O
task	O
of	O
image	O
caption	O
storytelling	O
.	O
Similarly	O
,	O
Li	O
and	O
Jurafsky	O
(	O
2017	O
)	O
2	O
)	O
Set	O
-	O
to	O
-	O
sequence	O
Models	O
.	O
Basically	O
,	O
these	O
models	O
are	O
based	O
on	O
an	O
encoder	O
-	O
decoder	O
framework	O
,	O
where	O
the	O
encoder	O
is	O
used	O
to	O
obtain	O
sentence	O
representations	O
and	O
then	O
the	O
decoder	O
produces	O
ordered	O
sentences	O
progressively	O
.	O
Among	O
them	O
,	O
both	O
Gong	O
et	O
al	O
(	O
2016	O
)	O
and	O
Logeswaran	O
et	O
al	O
(	O
2018	O
)	O
Cui	O
et	O
al	O
(	O
2018	O
)	O
proposed	O
ATTOrderNet	O
that	O
uses	O
self	O
-	O
attention	O
mechanism	O
to	O
learn	O
sentence	O
representations	O
.	O
Inspired	B-DatasetName
by	O
the	O
successful	O
applications	O
of	O
graph	O
neural	O
network	O
(	O
GNN	O
)	O
in	O
many	O
NLP	O
tasks	O
Xue	O
et	O
al	O
,	O
2019	O
;	O
,	O
Yin	O
et	O
al	O
(	O
2019Yin	O
et	O
al	O
(	O
,	O
2021	O
represented	O
input	O
sentences	O
with	O
a	O
unified	O
SE	O
-	O
Graph	O
and	O
then	O
applied	O
GRN	O
to	O
learn	O
sentence	O
representations	O
.	O
Very	O
recently	O
,	O
we	O
notice	O
that	O
Chowdhury	O
et	O
al	O
(	O
2021	O
)	O
proposes	O
a	O
BART	B-MethodName
-	O
based	O
sentence	B-TaskName
ordering	I-TaskName
model	O
.	O
Please	O
note	O
that	O
our	O
porposed	O
framework	O
is	O
compatible	O
with	O
BART	B-MethodName
(	O
Lewis	O
et	O
al	O
,	O
2020	O
)	O
.	O
For	O
(	O
Yin	O
et	O
al	O
,	O
2019	O
(	O
Yin	O
et	O
al	O
,	O
,	O
2021	O
.	O
example	O
,	O
we	O
can	O
easily	O
adapt	O
the	O
BART	B-MethodName
encoder	O
as	O
our	O
sentence	O
encoder	O
.	O
With	O
similar	O
motivation	O
with	O
ours	O
,	O
that	O
is	O
,	O
to	O
combine	O
advantages	O
of	O
above	O
-	O
mentioned	O
two	O
kinds	O
of	O
models	O
,	O
Yin	O
et	O
al	O
(	O
2020	O
)	O
introduced	O
three	O
pairwise	O
ordering	O
predicting	O
modules	O
(	O
FHDecoder	O
)	O
to	O
enhance	O
the	O
pointer	B-MethodName
network	I-MethodName
decoder	O
of	O
ATTOrder	O
-	O
Net	O
.	O
Recently	O
,	O
Cui	O
et	O
al	O
(	O
2020	O
)	O
proposed	O
BERSON	O
that	O
is	O
also	O
equipped	O
with	O
FHDecoder	O
and	O
utilizes	O
BERT	B-MethodName
to	O
exploit	O
the	O
deep	O
semantic	O
connection	O
and	O
relative	O
ordering	O
between	O
sentences	O
.	O
However	O
,	O
significantly	O
different	O
from	O
them	O
,	O
we	O
borrow	O
the	O
idea	O
from	O
the	O
mask	O
-	O
predict	O
framework	O
(	O
Gu	O
et	O
al	O
,	O
2018	O
;	O
Ghazvininejad	O
et	O
al	O
,	O
2019	O
;	O
Deng	B-DatasetName
et	I-DatasetName
al	I-DatasetName
,	O
2020	O
)	O
to	O
progressively	O
incorporate	O
pairwise	O
ordering	O
information	O
into	O
SE	O
-	O
Graph	O
,	O
which	O
is	O
the	O
basis	O
of	O
our	O
graph	O
-	O
based	O
sentence	B-TaskName
ordering	I-TaskName
model	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
our	O
work	O
is	O
the	O
first	O
attempt	O
to	O
explore	O
iteratively	O
refined	O
GNN	O
for	O
sentence	B-TaskName
ordering	I-TaskName
.	O

In	O
this	O
section	O
,	O
we	O
give	O
a	O
brief	O
introduction	O
to	O
the	O
SE	O
-	O
GRN	O
(	O
Yin	O
et	O
al	O
,	O
2019	O
(	O
Yin	O
et	O
al	O
,	O
,	O
2021	O
,	O
which	O
is	O
selected	O
as	O
our	O
baseline	O
due	O
to	O
its	O
competitive	O
performance	O
.	O
As	O
shown	O
in	O
Figure	O
1	O
,	O
SE	O
-	O
GRN	O
is	O
composed	O
of	O
a	O
Bi	O
-	O
LSTM	B-MethodName
sentence	O
encoder	O
,	O
GRN	O
paragraph	O
encoder	O
,	O
and	O
a	O
pointer	B-MethodName
network	I-MethodName
(	O
Vinyals	O
et	O
al	O
,	O
2015b	O
)	O
decoder	O
.	O
o	O
i	O
.	O
As	O
illustrated	O
in	O
the	O
middle	O
of	O
Figure	O
1	O
,	O
each	O
input	O
sentence	O
set	O
is	O
rep	O
-	O
resented	O
as	O
an	O
undirected	O
sentence	O
-	O
entity	O
graph	O
G	O
=	O
(	O
V	O
,	O
E	O
)	O
,	O
where	O
V	O
=	O
{	O
v	O
i	O
}	O
I	O
i=1	O
∪	O
{	O
v	O
j	O
}	O
J	O
j=1	O
and	O
E	O
=	O
{	O
e	O
i	O
,	O
i	O
}	O
I	O
,	O
I	O
i=1	O
,	O
i	O
=	O
1	O
∪	O
{	O
ē	O
i	O
,	O
j	O
}	O
I	O
,	O
J	O
i=1	O
,	O
j=1	O
∪	O
{	O
ê	O
j	O
,	O
j	O
}	O
J	O
,	O
J	O
j=1	O
,	O
j	O
=	O
1	O
represent	O
the	O
nodes	O
and	O
edges	O
respectively	O
.	O
Here	O
,	O
nodes	O
include	O
sentence	O
nodes	O
(	O
such	O
as	O
v	O
i	O
)	O
and	O
entity	O
nodes	O
(	O
such	O
asv	O
j	O
)	O
,	O
and	O
each	O
edge	O
is	O
1	O
)	O
sentencesentence	O
edge	O
(	O
ss	O
-	O
edge	O
,	O
such	O
as	O
e	O
i	O
,	O
i	O
)	O
linking	O
two	O
sentences	O
having	O
the	O
same	O
entity	O
;	O
or	O
2	O
)	O
sentenceentity	O
edge	O
(	O
se	O
-	O
edge	O
,	O
such	O
asē	O
i	O
,	O
j	O
)	O
connecting	O
an	O
entity	O
to	O
a	O
sentence	O
that	O
contains	O
it	O
.	O
Each	O
se	O
-	O
edge	O
is	O
assigned	O
with	O
a	O
label	O
including	O
subject	O
,	O
object	O
or	O
other	O
,	O
based	O
on	O
the	O
syntactic	O
role	O
of	O
its	O
involved	O
entity	O
;	O
or	O
3	O
)	O
entity	O
-	O
entity	O
edge	O
(	O
ee	O
-	O
edge	O
,	O
such	O
asê	O
j	O
,	O
j	O
)	O
connecting	O
two	O
semantic	O
related	O
entities	O
.	O
Besides	O
,	O
a	O
virtual	O
global	O
node	O
connecting	O
to	O
all	O
nodes	O
is	O
introduced	O
to	O
capture	O
global	O
information	O
effectively	O
.	O

Node	O
representations	O
of	O
each	O
sentence	O
and	O
each	O
entity	O
are	O
first	O
initialized	O
with	O
the	O
concatenation	O
of	O
bidirectional	O
last	O
states	O
of	O
the	O
Bi	O
-	O
LSTM	B-MethodName
sentence	O
encoder	O
and	O
the	O
corresponding	O
GloVe	B-MethodName
word	O
embedding	O
,	O
respectively	O
.	O
Then	O
,	O
a	O
GRN	O
is	O
adapted	O
to	O
encode	O
the	O
above	O
sentence	O
-	O
entity	O
graph	O
,	O
where	O
node	O
states	O
are	O
updated	O
iteratively	O
.	O
During	O
the	O
process	O
of	O
updating	O
hidden	O
states	O
,	O
the	O
messages	O
for	O
each	O
node	O
are	O
aggregated	O
from	O
its	O
adjacent	O
nodes	O
.	O
Specifically	O
,	O
the	O
sentence	O
-	O
level	O
message	O
m	O
(	O
l	O
)	O
i	O
and	O
entity	O
-	O
level	O
messagem	O
(	O
l	O
)	O
i	O
for	O
a	O
sentence	O
s	O
i	O
are	O
defined	O
as	O
follows	O
:	O
m	O
(	O
l	O
)	O
i	O
=	O
v	O
i	O
N	O
i	O
w	O
(	O
κ	O
(	O
l	O
-	O
1	O
)	O
i	O
,	O
κ	O
(	O
l	O
-	O
1	O
)	O
i	O
)	O
κ	O
(	O
l	O
-	O
1	O
)	O
i	O
,	O
m	O
(	O
l	O
)	O
i	O
=	O
v	O
j	O
N	O
iw	O
(	O
κ	O
(	O
l	O
-	O
1	O
)	O
i	O
,	O
(	O
l	O
-	O
1	O
)	O
j	O
,	O
rij	O
)	O
(	O
l	O
-	O
1	O
)	O
j	O
,	O
(	O
1	O
)	O
where	O
κ	O
(	O
l	O
-	O
1	O
)	O
i	O
and	O
(	O
l	O
-	O
1	O
)	O
j	O
stand	O
for	O
the	O
neighboring	O
sentence	O
and	O
entity	O
representations	O
of	O
the	O
i	O
-	O
th	O
sentence	O
node	O
v	O
i	O
at	O
the	O
(	O
l	O
−	O
1	O
)	O
-	O
th	O
layer	O
,	O
N	O
i	O
andN	O
i	O
denote	O
the	O
sets	O
of	O
neighboring	O
sentences	O
and	O
entities	O
of	O
v	O
i	O
,	O
and	O
both	O
w	O
(	O
*	O
)	O
andw	O
(	O
*	O
)	O
are	O
gating	O
functions	O
with	O
single	O
-	O
layer	O
networks	O
,	O
involving	O
associated	O
node	O
states	O
and	O
edge	O
label	O
r	O
ij	O
(	O
if	O
any	O
)	O
.	O
Afterwards	O
,	O
κ	O
(	O
l	O
-	O
1	O
)	O
i	O
is	O
updated	O
by	O
concatenating	O
its	O
original	O
representation	O
κ	O
(	O
0	B-DatasetName
)	O
i	O
,	O
the	O
messages	O
from	O
neighbours	O
(	O
m	O
(	O
l	O
)	O
i	O
andm	O
(	O
l	O
)	O
i	O
)	O
and	O
the	O
global	O
state	O
g	O
(	O
l	O
-	O
1	O
)	O
via	O
GRU	B-MethodName
:	O
ξ	O
(	O
l	O
)	O
i	O
=	O
[	O
κ	O
(	O
0	B-DatasetName
)	O
i	O
;	O
m	O
(	O
l	O
)	O
i	O
;	O
m	O
(	O
l	O
)	O
i	O
;	O
g	O
(	O
l	O
-	O
1	O
)	O
]	O
,	O
κ	O
(	O
l	O
)	O
i	O
=	O
GRU	B-MethodName
(	O
ξ	O
(	O
l	O
)	O
i	O
,	O
κ	O
(	O
l	O
-	O
1	O
)	O
i	O
)	O
.	O
(	O
)	O
2	O
Similar	O
to	O
updating	O
sentence	O
nodes	O
,	O
each	O
entity	O
state	O
(	O
l	O
-	O
1	O
)	O
j	O
is	O
updated	O
based	O
on	O
its	O
word	O
embedding	O
emb	O
j	O
,	O
hidden	O
states	O
of	O
its	O
connected	O
sentence	O
nodes	O
(	O
such	O
as	O
κ	O
(	O
l	O
-	O
1	O
)	O
i	O
)	O
,	O
and	O
g	O
(	O
l	O
-	O
1	O
)	O
:	O
m	O
(	O
l	O
)	O
j	O
=	O
v	O
i	O
N	O
jw	O
(	O
(	O
l	O
-	O
1	O
)	O
j	O
,	O
κ	O
(	O
l	O
-	O
1	O
)	O
i	O
,	O
rij	O
)	O
κ	O
(	O
l	O
-	O
1	O
)	O
i	O
,	O
m	O
(	O
l	O
)	O
j	O
=	O
v	O
j	O
N	O
jw	O
(	O
(	O
l	O
-	O
1	O
)	O
j	O
,	O
(	O
l	O
-	O
1	O
)	O
j	O
)	O
(	O
l	O
-	O
1	O
)	O
j	O
,	O
ξ	O
(	O
l	O
)	O
j	O
=	O
[	O
embj	O
;	O
m	O
(	O
l	O
)	O
j	O
;	O
m	O
(	O
l	O
)	O
j	O
;	O
g	O
(	O
l	O
-	O
1	O
)	O
]	O
,	O
(	O
l	O
)	O
j	O
=	O
GRU	B-MethodName
(	O
ξ	O
(	O
l	O
)	O
j	O
,	O
(	O
l	O
-	O
1	O
)	O
j	O
)	O
.	O
(	O
3	O
)	O
Finally	O
,	O
the	O
messages	O
from	O
both	O
sentence	O
and	O
entity	O
states	O
are	O
used	O
to	O
update	O
global	O
state	O
g	O
(	O
l	O
-	O
1	O
)	O
via	O
g	O
(	O
l	O
)	O
=	O
GRU	B-MethodName
(	O
1	O
|	O
V	O
|	O
v	O
i	O
V	O
κ	O
(	O
l	O
-	O
1	O
)	O
i	O
,	O
1	O
|	O
V	O
|	O
v	O
j	O
V	O
(	O
l	O
-	O
1	O
)	O
j	O
,	O
g	O
(	O
l	O
-	O
1	O
)	O
)	O
.	O
(	O
4	O
)	O
The	O
above	O
updating	O
process	O
is	O
iterated	O
for	O
L	O
times	O
.	O
Usually	O
,	O
the	O
top	O
hidden	O
states	O
are	O
considered	O
as	O
fine	O
-	O
grained	O
graph	O
representations	O
,	O
which	O
will	O
provide	O
dynamical	O
context	O
for	O
the	O
decoder	O
via	O
attention	O
mechanism	O
.	O

Given	O
the	O
learned	O
hidden	O
states	O
{	O
κ	O
(	O
L	O
)	O
i	O
}	O
and	O
g	O
(	O
L	O
)	O
,	O
the	O
prediction	O
procedure	O
for	O
order	O
o	O
can	O
be	O
formalized	O
as	O
follows	O
:	O
P	O
(	O
o	O
|	O
K	O
(	O
L	O
)	O
)	O
=	O
I	O
t=1	O
P	O
(	O
o	O
t	O
|	O
o	O
<	O
t	O
,	O
K	O
(	O
L	O
)	O
o	O
t−1	O
)	O
,	O
P	O
(	O
o	O
t	O
|	O
o	O
<	O
t	O
,	O
K	O
(	O
L	O
)	O
o	O
t−1	O
)	O
=	O
softmax	B-MethodName
(	O
q	O
T	O
tanh	O
(	O
W	O
h	O
d	O
t	O
+	O
U	O
K	O
(	O
L	O
)	O
o	O
t−1	O
)	O
)	O
,	O
h	O
d	O
t	O
=	O
LSTM	B-MethodName
(	O
h	O
d	O
t−1	O
,	O
κ	O
(	O
0	B-DatasetName
)	O
o	O
t−1	O
)	O
.	O
Here	O
,	O
q	O
,	O
W	O
and	O
U	O
are	O
learnable	O
parameters	O
,	O
K	O
and	O
the	O
decoder	O
hidden	O
state	O
at	O
the	O
t	O
-	O
th	O
time	O
step	O
,	O
which	O
is	O
initialized	O
by	O
g	O
(	O
L	O
)	O
as	O
t=0	O
,	O
respectively	O
.	O

In	O
this	O
section	O
,	O
we	O
give	O
a	O
detailed	O
description	O
to	O
our	O
framework	O
.	O
As	O
shown	O
in	O
Figure	O
2	O
,	O
we	O
first	O
introduce	O
two	O
graph	O
-	O
based	O
classifiers	O
to	O
construct	O
an	O
iteratively	O
refined	O
sentence	O
-	O
entity	O
graph	O
(	O
IRSE	O
-	O
Graph	O
)	O
.	O
It	O
is	O
a	O
weighted	O
version	O
of	O
SE	O
-	O
Graph	O
,	O
where	O
pairwise	O
ordering	O
inforamtion	O
is	O
iteratively	O
incorporated	O
to	O
update	O
ss	O
-	O
edge	O
weights	O
.	O
Then	O
,	O
we	O
adapt	O
the	O
conventional	O
GRN	O
to	O
establish	O
a	O
neural	O
sentence	B-TaskName
ordering	I-TaskName
model	O
based	O
on	O
the	O
final	O
IRSE	O
-	O
Graph	O
.	O

Finally	O
,	O
following	O
the	O
conventional	O
SE	O
-	O
GRN	O
(	O
Yin	O
et	O
al	O
,	O
2019	O
(	O
Yin	O
et	O
al	O
,	O
,	O
2021	O
,	O
we	O
construct	O
a	O
graph	O
-	O
based	O
sentence	B-TaskName
ordering	I-TaskName
model	O
.	O
Note	O
that	O
the	O
above	O
two	O
classifiers	O
and	O
our	O
sentence	B-TaskName
ordering	I-TaskName
model	O
are	O
all	O
based	O
on	O
IRSE	O
-	O
Graph	O
rather	O
than	O
the	O
conventional	O
SE	O
-	O
Graph	O
,	O
which	O
makes	O
the	O
standard	O
GRN	O
unable	O
to	O
be	O
applied	O
directly	O
.	O
To	O
deal	O
with	O
this	O
issue	O
,	O
we	O
slightly	O
adapt	O
GRN	O
to	O
utilize	O
pairwise	O
ordering	O
information	O
for	O
graph	O
encoding	O
.	O
Specifically	O
,	O
we	O
adapt	O
Equation	O
1	O
to	O
incorporate	O
ss	O
-	O
edge	O
weights	O
into	O
the	O
message	O
aggregation	O
of	O
sentence	O
-	O
level	O
nodes	O
:	O
m	O
(	O
l	O
)	O
i	O
=	O
v	O
i	O
N	O
i	O
w	O
i	O
,	O
i	O
w	O
(	O
κ	O
(	O
l	O
-	O
1	O
)	O
i	O
,	O
κ	O
(	O
l	O
-	O
1	O
)	O
i	O
)	O
κ	O
(	O
l	O
-	O
1	O
)	O
i	O
,	O
w	O
(	O
κ	O
(	O
l	O
-	O
1	O
)	O
i	O
,	O
κ	O
(	O
l	O
-	O
1	O
)	O
i	O
)	O
=	O
σ	O
(	O
Wg	O
[	O
κ	O
(	O
l	O
-	O
1	O
)	O
i	O
;	O
κ	O
(	O
l	O
-	O
1	O
)	O
i	O
]	O
)	O
.	O
(	O
6	O
)	O
Here	O
σ	O
denotes	O
sigmoid	O
function	O
and	O
W	O
g	O
is	O
learnable	O
parameter	O
matrix	O
.	O
Equation	O
6	O
expresses	O
that	O
the	O
sentence	O
-	O
level	O
aggregation	O
should	O
consider	O
not	O
only	O
the	O
semantic	O
representations	O
of	O
the	O
two	O
involved	O
sentences	O
,	O
but	O
also	O
the	O
relative	O
ordering	O
between	O
them	O
.	O
In	O
addition	O
,	O
other	O
Equations	O
are	O
the	O
same	O
as	O
those	O
of	O
conventional	O
GRN	O
,	O
which	O
have	O
been	O
described	O
in	O
Section	O
3.2	O
.	O

Table	O
1	O
reports	O
the	O
overall	O
experimental	O
results	O
of	O
sentence	B-TaskName
ordering	I-TaskName
.	O
When	O
incorporating	O
BERT	B-MethodName
and	O
FHDecoder	O
into	O
IRSE	O
-	O
GRN	O
,	O
our	O
model	O
achieves	O
SOTA	O
performance	O
on	O
most	O
of	O
datasets	O
.	O
Besides	O
,	O
we	O
arrive	O
at	O
the	O
following	O
conclusions	O
:	O
First	O
,	O
IRSE	O
-	O
GRN	O
significantly	O
surpasses	O
SE	O
-	O
GRN	O
on	O
all	O
datasets	O
(	O
bootstrapping	O
test	O
,	O
p<0.01	O
)	O
,	O
indicating	O
that	O
iteratively	O
refining	O
graph	O
representations	O
indeed	O
benefit	O
the	O
ordering	O
of	O
input	O
sentences	O
.	O
Second	O
,	O
IRSE	O
-	O
GRN+FHDecoder	O
exhibits	O
better	O
performance	O
than	O
IRSE	O
-	O
GRN	O
and	O
all	O
non	O
-	O
BERT	B-MethodName
baselines	O
,	O
which	O
are	O
shown	O
above	O
the	O
upper	O
dotted	O
line	O
of	O
Table	O
1	O
,	O
across	O
datasets	O
in	O
different	O
domains	O
.	O
Therefore	O
,	O
we	O
confirm	O
that	O
our	O
framework	O
is	O
orthogonal	O
to	O
the	O
current	O
approach	O
exploiting	O
pairwise	O
ordering	O
information	O
for	O
decoder	O
.	O
Third	O
,	O
when	O
constructing	O
our	O
model	O
based	O
on	O
BERT	B-MethodName
,	O
IRSE	O
-	O
GRN+BERT+FHDecoder	O
also	O
outperforms	O
all	O
BERT	B-MethodName
-	O
based	O
baselines	O
,	O
such	O
as	O
Cons	O
-	O
Graph	O
,	O
BERSON	O
,	O
achieving	O
SOTA	O
performance	O
.	O
It	O
can	O
be	O
known	O
that	O
our	O
proposed	O
framework	O
is	O
also	O
effective	O
when	O
combining	O
with	O
pretrained	O
language	O
model	O
.	O
Finally	O
,	O
we	O
note	O
that	O
IRSE	O
-	O
GRN+BERT+FH	O
-	O
Decoder	O
gains	O
relatively	O
marginal	O
improvement	O
on	O
SIND	B-DatasetName
and	O
ROCStory	O
,	O
and	O
performs	O
worse	O
than	O
BERSON	O
in	O
PMR	O
on	O
SIND	B-DatasetName
.	O
We	O
speculate	O
that	O
there	O
exist	O
less	O
ss	O
-	O
edges	O
on	O
these	O
two	O
datasets	O
,	O
resulting	O
in	O
that	O
our	O
proposed	O
framework	O
can	O
not	O
achieve	O
its	O
full	O
potential	O
.	O
Specifically	O
,	O
average	O
edge	O
numbers	O
of	O
SIND	B-DatasetName
and	O
ROCStory	O
are	O
2.85	O
and	O
5.66	O
respectively	O
,	O
far	O
fewer	O
than	O
16.60	O
,	O
10.86	O
and	O
16.73	O
on	O
NIPS	O
Abstract	O
,	O
ANN	O
Abstract	O
and	O
arXiv	B-DatasetName
Abstract	O
.	O
Besides	O
,	O
since	O
it	O
is	O
a	O
challenge	O
to	O
order	O
longer	O
paragraphs	O
,	O
we	O
investigate	O
the	O
Kendall	O
's	O
τ	O
of	O
our	O
models	O
and	O
SE	O
-	O
GRN	O
with	O
respect	O
to	O
different	O
sentence	O
numbers	O
,	O
as	O
shown	O
in	O
Figure	O
4	O
.	O
Overall	O
,	O
all	O
models	O
degrade	O
with	O
the	O
increase	O
of	O
sentence	O
number	O
.	O
However	O
,	O
our	O
model	O
and	O
its	O
two	O
enhanced	O
versions	O
always	O
exhibit	O
better	O
performance	O
than	O
SE	O
-	O
GRN	O
.	O

As	O
mentioned	O
in	O
previous	O
studies	O
(	O
Gong	O
et	O
al	O
,	O
2016	O
;	O
Cui	O
et	O
al	O
,	O
2018	O
;	O
Oh	O
et	O
al	O
,	O
2019	O
)	O
,	O
the	O
first	O
and	O
last	O
sentences	O
are	O
very	O
important	O
in	O
a	O
paragraph	O
.	O
Following	O
these	O
studies	O
,	O
we	O
compare	O
models	O
by	O
conducting	O
experiments	O
to	O
predict	O
the	O
first	O
and	O
last	O
sentences	O
.	O
As	O
displayed	O
in	O
Table	O
3	O
,	O
IRSE	O
-	O
GRN	O
surpasses	O
all	O
non	O
-	O
BERT	B-MethodName
baselines	O
,	O
and	O
IRSE	O
-	O
GRN+BERT+	O
FHDecoder	O
wins	O
against	O
BERTSON	O
.	O
These	O
results	O
are	O
consistent	O
with	O
those	O
reported	O
in	O
Table	O
1	O
,	O
further	O
demonstrating	O
the	O
effectiveness	O
of	O
our	O
model	O
.	O

We	O
conduct	O
several	O
experiments	O
to	O
investigate	O
the	O
impacts	O
of	O
our	O
proposed	O
components	O
on	O
ROCstory	O
dataset	O
and	O
arXiv	B-DatasetName
dataset	O
which	O
are	O
the	O
two	O
largest	O
datasets	O
.	O
All	O
results	O
are	O
provided	O
in	O
Table	O
4	O
,	O
where	O
we	O
draw	O
the	O
following	O
conclusions	O
:	O
First	O
,	O
using	O
only	O
iterative	O
classifier	O
,	O
IRSE	O
-	O
GRN	O
(	O
w/o	O
initial	O
classifier	O
)	O
performs	O
worse	O
than	O
IRSE	O
-	O
GRN	O
.	O
This	O
result	O
proves	O
that	O
iterative	O
classifier	O
fails	O
to	O
predict	O
well	O
from	O
scratch	O
and	O
the	O
pairwise	O
ordering	O
predicted	O
by	O
initial	O
classifier	O
is	O
beneficial	O
to	O
construct	O
a	O
well	O
-	O
formed	O
graph	O
representation	O
for	O
iterative	O
classifier	O
.	O
Second	O
,	O
when	O
the	O
iteration	O
number	O
k	O
is	O
set	O
as	O
1	O
,	O
the	O
performance	O
of	O
IRSE	O
-	O
GRN	O
decreases	O
.	O
Moreover	O
,	O
if	O
we	O
remove	O
iterative	O
classifier	O
,	O
the	O
performance	O
of	O
IRSE	O
-	O
GRN	O
becomes	O
even	O
worse	O
.	O
Therefore	O
,	O
we	O
confirm	O
that	O
the	O
iterative	O
predictions	O
of	O
pairwise	O
ordering	O
indeed	O
benefit	O
the	O
learning	O
of	O
graph	O
representations	O
.	O
Finally	O
,	O
the	O
result	O
in	O
the	O
last	O
line	O
indicates	O
that	O
removing	O
noisy	O
weights	O
leads	O
to	O
a	O
significant	O
performance	O
drop	O
.	O
It	O
suggests	O
that	O
the	O
utilization	O
of	O
noisy	O
weights	O
is	O
useful	O
for	O
the	O
training	O
of	O
iterative	O
classifier	O
,	O
which	O
makes	O
our	O
model	O
more	O
robust	O
.	O

Following	O
previous	O
studies	O
(	O
Barzilay	O
and	O
Lapata	O
,	O
2005	O
;	O
Nayeem	O
and	O
Chali	O
,	O
2017	O
)	O
spect	O
the	O
validity	O
of	O
our	O
proposed	O
framework	O
via	O
multi	B-TaskName
-	I-TaskName
document	I-TaskName
summarization	I-TaskName
.	O
Concretely	O
,	O
we	O
train	O
different	O
neural	O
sentence	B-TaskName
ordering	I-TaskName
models	O
on	O
a	O
large	O
-	O
scale	O
summarization	B-TaskName
corpus	O
(	O
Fabbri	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
then	O
individually	O
use	O
them	O
to	O
reorder	O
the	O
small	O
-	O
scale	O
summarization	B-TaskName
data	O
of	O
DUC2004	O
(	O
Task2	O
)	O
.	O
Finally	O
,	O
we	O
use	O
coherence	O
probability	O
proposed	O
by	O
(	O
Nayeem	O
and	O
Chali	O
,	O
2017	O
)	O
to	O
evaluate	O
the	O
coherence	O
of	O
summaries	O
.	O
In	O
this	O
group	O
of	O
experiments	O
,	O
we	O
conduct	O
experiments	O
using	O
different	O
weights	O
:	O
0.5	O
and	O
0.8	O
,	O
as	O
implemented	O
in	O
(	O
Nayeem	O
and	O
Chali	O
,	O
2017	O
)	O
and	O
(	O
Yin	O
et	O
al	O
,	O
2020	O
)	O
respectively	O
.	O
The	O
results	O
are	O
reported	O
in	O
Table	O
5	O
.	O
We	O
can	O
observe	O
that	O
the	O
summaries	O
reordered	O
by	O
IRSE	O
-	O
GRN	O
and	O
its	O
variants	O
achieve	O
higher	O
coherence	O
probabilities	O
than	O
baseline	O
,	O
verifying	O
the	O
effectiveness	O
of	O
our	O
proposed	O
framework	O
in	O
the	O
downstream	O
task	O
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
a	O
novel	O
sentence	B-TaskName
ordering	I-TaskName
framework	O
that	O
makes	O
better	O
use	O
of	O
pairwise	O
orderings	O
for	O
graph	O
-	O
based	O
sentence	B-TaskName
ordering	I-TaskName
.	O
Specifically	O
,	O
we	O
introduce	O
two	O
classifiers	O
to	O
iteratively	O
predict	O
pairwise	O
orderings	O
,	O
which	O
are	O
gradually	O
incorporated	O
into	O
the	O
graph	O
as	O
edge	O
weights	O
.	O
Then	O
,	O
based	O
on	O
this	O
refined	O
graph	O
,	O
we	O
construct	O
a	O
graph	O
-	O
based	O
sentence	B-TaskName
ordering	I-TaskName
model	O
.	O
Experiments	O
on	O
five	O
datasets	O
demonstrate	O
not	O
only	O
the	O
superiority	O
of	O
our	O
model	O
over	O
baselines	O
,	O
but	O
also	O
the	O
compatibility	O
to	O
other	O
modules	O
utilizing	O
pairwise	O
ordering	O
information	O
.	O
Moreover	O
,	O
when	O
equipped	O
with	O
BERT	B-MethodName
and	O
FHDecoder	O
,	O
our	O
enhanced	O
model	O
achieves	O
SOTA	O
performance	O
across	O
datasets	O
.	O
In	O
the	O
future	O
,	O
we	O
plan	O
to	O
explore	O
more	O
effective	O
GNN	O
for	O
sentence	B-TaskName
ordering	I-TaskName
.	O
In	O
particular	O
,	O
we	O
will	O
improve	O
our	O
model	O
by	O
iteratively	O
merging	O
nodes	O
to	O
refine	O
the	O
graph	O
representation	O
.	O

In	O
order	O
to	O
train	O
and	O
evaluate	O
machine	O
learning	O
systems	O
to	O
match	O
or	O
correct	O
authors	O
'	O
names	O
,	O
a	O
dataset	O
of	O
name	O
en	O
-	O
tities	O
containing	O
the	O
different	O
surface	O
forms	O
(	O
or	O
variants	O
)	O
of	O
authors	O
'	O
names	O
is	O
required	O
.	O
The	O
entities	O
should	O
reflect	O
as	O
well	O
as	O
possible	O
the	O
variability	O
that	O
can	O
be	O
found	O
in	O
the	O
RFR	O
dataset	O
,	O
as	O
was	O
illustrated	O
in	O
the	O
case	O
of	O
F.	O
Scott	O
Fitzgerald	O
in	O
Section	O
1	O
.	O
For	O
each	O
entity	O
,	O
a	O
canonical	O
name	O
should	O
be	O
elected	O
and	O
correspond	O
to	O
the	O
name	O
that	O
should	O
be	O
preferred	O
for	O
the	O
purpose	O
of	O
e	O
-	O
commerce	O
.	O
Instead	O
of	O
setting	O
these	O
gold	O
spellings	O
by	O
following	O
some	O
predefined	O
rules	O
(	O
i.e.	O
family	O
name	O
in	O
the	O
first	O
position	O
,	O
initial	O
of	O
first	O
name	O
,	O
etc	O
.	O
)	O
,	O
for	O
e	O
-	O
commerce	O
applications	O
it	O
is	O
more	O
appropriate	O
that	O
the	O
displayed	O
authors	O
names	O
have	O
the	O
most	O
popular	O
spellings	O
among	O
readers	O
.	O
In	O
agreement	O
with	O
Rakuten	O
catalog	O
analysts	O
we	O
set	O
the	O
most	O
popular	O
spelling	O
of	O
an	O
author	O
name	O
as	O
the	O
one	O
found	O
on	O
Wikipedia	O
4	O
or	O
DBpedia	B-DatasetName
(	O
Lehmann	O
et	O
al	O
,	O
2015	O
)	O
.	O
While	O
Wikipedia	O
seems	O
more	O
pertinent	O
to	O
select	O
canonical	O
names	O
matching	O
the	O
e	O
-	O
commerce	O
user	O
expectations	O
,	O
specialized	O
librarian	O
data	O
services	O
,	O
such	O
as	O
the	O
Library	O
of	O
Congress	O
Name	O
Authority	O
5	O
,	O
could	O
be	O
used	O
in	O
future	O
research	O
to	O
enrich	O
the	O
dataset	O
of	O
name	O
entities	O
.	O
Name	O
entities	O
are	O
collected	O
in	O
three	O
distinct	O
ways	O
:	O
1	O
.	O
ISBN	O
matching	O
:	O
for	O
each	O
book	O
the	O
different	O
author	O
names	O
found	O
via	O
ISBN	O
search	O
on	O
external	O
sources	O
and	O
the	O
RFR	O
author	O
name	O
field	O
build	O
up	O
an	O
entity	O
.	O
The	O
canonical	O
form	O
is	O
the	O
one	O
that	O
is	O
matched	O
with	O
Wikipedia	O
or	O
DBpedia	B-DatasetName
;	O
else	O
the	O
one	O
provided	O
by	O
the	O
greatest	O
number	O
of	O
sources	O
.	O
2	O
.	O
Matching	O
of	O
Rakuten	O
authors	O
:	O
we	O
build	O
entities	O
using	O
fuzzy	O
search	O
on	O
the	O
author	O
name	O
field	O
on	O
DBpedia	B-DatasetName
and	O
consider	O
the	O
DBpedia	B-DatasetName
value	O
to	O
be	O
canonical	O
.	O
We	O
limit	O
the	O
number	O
of	O
false	O
positives	O
in	O
fuzzy	O
search	O
by	O
tokenizing	O
both	O
names	O
,	O
and	O
keeping	O
only	O
the	O
names	O
where	O
at	O
least	O
one	O
token	O
from	O
the	O
name	O
on	O
RFR	O
is	O
approximately	O
found	O
in	O
the	O
external	O
resource	O
(	O
Levenshtein	O
distance	O
<	O
2	O
)	O
.	O
3	O
.	O
Name	O
variants	O
:	O
DBpedia	B-DatasetName
,	O
BnF	O
,	O
and	O
JRCnames	O
(	O
Steinberger	O
et	O
al	O
,	O
2011	O
;	O
Maud	O
et	O
al	O
,	O
2016	O
)	O
directly	O
provide	O
data	O
about	O
people	O
(	O
not	O
limited	O
to	O
book	O
authors	O
)	O
and	O
their	O
name	O
variants	O
.	O
As	O
an	O
example	O
,	O
by	O
using	O
the	O
wikiPageRedirects	O
field	O
in	O
DBpedia	B-DatasetName
we	O
can	O
build	O
a	O
large	O
entity	O
for	O
the	O
canonical	O
name	O
"	O
Anton	O
Tchekhov	O
"	O
,	O
containing	O
"	O
Anton	O
Tchechov	O
"	O
,	O
"	O
Antòn	O
Pàvlovič	O
Chéchov	O
"	O
,	O
"	O
Checkhov	O
"	O
,	O
"	O
Anton	O
Chekov	O
"	O
,	O
and	O
many	O
more	O
.	O
After	O
creating	O
the	O
name	O
entity	O
dataset	O
,	O
we	O
normalize	O
all	O
names	O
to	O
latin	O
-	O
1	O
.	O
We	O
obtain	O
about	O
750	O
,	O
000	O
entities	O
,	O
for	O
a	O
total	O
of	O
2.1	O
million	O
names	O
.	O

For	O
any	O
given	O
book	O
with	O
an	O
ISBN	O
and	O
an	O
author	O
's	O
name	O
,	O
all	O
three	O
techniques	O
shown	O
in	O
Fig	O
.	O
1	O
provide	O
one	O
or	O
several	O
candidate	O
canonical	O
names	O
.	O
As	O
we	O
aim	O
at	O
providing	O
an	O
automated	O
tool	O
to	O
enhance	O
the	O
quality	O
of	O
the	O
book	O
products	O
,	O
the	O
final	O
system	O
should	O
provide	O
a	O
ranked	O
list	O
of	O
candidates	O
with	O
a	O
calibrated	O
confidence	O
level	O
.	O
For	O
this	O
purpose	O
we	O
train	O
a	O
logistic	B-MethodName
regression	I-MethodName
to	O
estimate	O
the	O
probability	O
that	O
a	O
proposal	O
is	O
the	O
canonical	O
form	O
for	O
an	O
author	O
's	O
name	O
.	O
This	O
information	O
is	O
then	O
used	O
as	O
a	O
confidence	O
score	O
to	O
rank	O
the	O
different	O
candidate	O
names	O
returned	O
by	O
the	O
three	O
normalization	O
approaches	O
.	O
Specifically	O
,	O
we	O
represent	O
a	O
proposal	O
with	O
a	O
set	O
of	O
12	O
features	O
:	O
11	O
indicating	O
whether	O
it	O
is	O
found	O
in	O
the	O
bibliographic	O
sources	O
,	O
generated	O
from	O
the	O
seq2seq	B-MethodName
model	O
,	O
matched	O
with	O
the	O
Siamese	B-MethodName
network	I-MethodName
or	O
equal	O
to	O
the	O
input	O
name	O
,	O
and	O
one	O
last	O
feature	O
corresponding	O
to	O
the	O
cosine	O
distance	O
between	O
the	O
representation	O
of	O
the	O
proposal	O
and	O
that	O
of	O
the	O
input	O
name	O
.	O
The	O
selected	O
features	O
reflect	O
that	O
the	O
confidence	O
of	O
the	O
global	O
system	O
should	O
increase	O
with	O
(	O
i	O
)	O
the	O
consensus	O
among	O
the	O
different	O
sources	O
,	O
and	O
(	O
ii	O
)	O
the	O
similarity	O
of	O
the	O
candidate	O
to	O
the	O
input	O
name	O
.	O
For	O
this	O
component	O
we	O
use	O
the	O
annotated	O
dataset	O
introduced	O
in	O
Section	O
2.4	O
,	O
splitting	O
the	O
books	O
between	O
training	O
and	O
test	O
sets	O
,	O
with	O
a	O
ratio	O
of	O
50	O
%	O
:	O
50	O
%	O
,	O
generating	O
a	O
total	O
of	O
11185	O
proposals	O
.	O

For	O
each	O
category	O
of	O
constraints	O
introduced	O
in	O
Section	O
2	O
,	O
we	O
discuss	O
best	O
practices	O
for	O
both	O
human	O
and	O
automatic	O
evaluation	O
.	O
We	O
leave	O
out	O
overlap	O
due	O
to	O
ease	O
of	O
automatic	O
evaluation	O
.	O
Additionally	O
,	O
we	O
perform	O
a	O
case	O
study	O
,	O
evaluating	O
how	O
well	O
black	O
-	O
box	O
synonym	O
substitution	O
attacks	O
GENETICATTACK	O
and	O
TEXTFOOLER	O
fulfill	O
constraints	O
.	O
Both	O
attacks	O
find	O
adversarial	O
examples	O
by	O
swapping	O
out	O
words	O
for	O
their	O
synonyms	O
until	O
the	O
classifier	O
is	O
fooled	O
.	O
GENETICATTACK	O
uses	O
a	O
genetic	O
algorithm	O
to	O
attack	O
an	O
LSTM	B-MethodName
trained	O
on	O
the	O
IMDB	B-DatasetName
6	O
document	O
-	O
level	O
sentiment	O
classification	O
dataset	O
.	O
TEXTFOOLER	O
uses	O
a	O
greedy	O
approach	O
to	O
attack	O
an	O
LSTM	B-MethodName
,	O
CNN	O
,	O
and	O
BERT	B-MethodName
trained	O
on	O
five	O
classification	O
datasets	O
.	O
We	O
chose	O
these	O
attacks	O
because	O
:	O
They	O
claim	O
to	O
create	O
perturbations	O
that	O
preserve	O
semantics	O
,	O
maintain	O
grammaticality	O
,	O
and	O
are	O
not	O
suspicious	O
to	O
readers	O
.	O
However	O
,	O
our	O
inspection	O
of	O
the	O
perturbations	O
revealed	O
that	O
many	O
violated	O
these	O
constraints	O
.	O
They	O
report	O
high	O
attack	O
success	O
rates	O
.	O
7	O
They	O
successfully	O
attack	O
two	O
of	O
the	O
most	O
effective	O
models	O
for	O
text	B-TaskName
classification	I-TaskName
:	O
LSTM	B-MethodName
and	O
BERT	B-MethodName
.	O
To	O
generate	O
examples	O
for	O
evaluation	O
,	O
we	O
attacked	O
BERT	B-MethodName
using	O
TEXTFOOLER	O
and	O
attacked	O
an	O
LSTM	B-MethodName
using	O
GENETICATTACK	O
.	O
We	O
evaluate	O
both	O
methods	O
on	O
the	O
IMDB	B-DatasetName
dataset	O
.	O
In	O
addition	O
,	O
we	O
evaluate	O
TEXTFOOLER	O
on	O
the	O
Yelp	O
polarity	O
document	O
-	O
level	O
sentiment	O
classification	O
dataset	O
and	O
the	O
Movie	O
Review	O
(	O
MR	B-DatasetName
)	O
sentence	O
-	O
level	O
sentiment	O
classification	O
dataset	O
(	O
Pang	O
and	O
Lee	O
,	O
2005	O
;	O
Zhang	O
et	O
al	O
,	O
2015	O
)	O
.	O
We	O
use	O
1	O
,	O
000	O
examples	O
from	O
each	O
dataset	O
.	O
Table	O
3	O
shows	O
example	O
violations	O
of	O
each	O
constraint	O
.	O

Semantics	O
Grammaticality	O
Edit	O
Distance	O
Non	O
-	O
Suspicion	O
Synonym	O
Substitution	O
.	O
(	O
Alzantot	O
et	O
al	O
,	O
2018	O
;	O
Kuleshov	O
et	O
al	O
,	O
2018	O
;	O
Jin	O
et	O
al	O
,	O
2019	O
;	O
Ren	O
et	O
al	O
,	O
2019	O
)	O
3	O
3	O
3	O
3	O
Character	O
Substitution	O
.	O
(	O
Ebrahimi	O
et	O
al	O
,	O
2017	O
;	O
Gao	O
et	O
al	O
,	O
2018	O
;	O
Li	O
et	O
al	O
,	O
2018	O
)	O
3	O
5	O
3	O
3	O
Word	O
Insertion	O
or	O
Removal	O
.	O
(	O
Liang	O
et	O
al	O
,	O
2017	O
;	O
Samanta	O
and	O
Mehta	O
,	O
2017	O
)	O
3	O
3	O
3	O
3	O
General	B-DatasetName
Paraphrase	O
.	O
(	O
Zhao	O
et	O
al	O
,	O
2017	O
;	O
Ribeiro	O
et	O
al	O
,	O
2018	O
;	O
Iyyer	O
et	O
al	O
,	O
2018	O
)	O
3	O
3	O
5	O
3	O
A	O
"	O
3	O
"	O
indicates	O
that	O
the	O
respective	O
attack	O
is	O
supposed	O
to	O
meet	O
the	O
constraint	O
,	O
and	O
a	O
"	O
5	O
"	O
means	O
the	O
attack	O
is	O
not	O
supposed	O
to	O
meet	O
the	O
constraint	O
.	O

To	O
quantify	O
semantic	B-TaskName
similarity	I-TaskName
of	O
x	O
and	O
x	O
adv	O
,	O
we	O
asked	O
users	O
whether	O
they	O
agreed	O
that	O
the	O
changes	O
between	O
the	O
two	O
passages	O
preserved	O
meaning	O
on	O
a	O
scale	O
of	O
1	O
(	O
Strongly	O
Disagree	O
)	O
to	O
5	O
(	O
Strongly	O
Agree	O
)	O
.	O
We	O
averaged	O
scores	O
for	O
each	O
attack	O
method	O
to	O
determine	O
if	O
the	O
method	O
generally	O
preserves	O
semantics	O
.	O
Perturbations	O
generated	O
by	O
TEXTFOOLER	O
were	O
rated	O
an	O
average	O
of	O
3.28	O
,	O
while	O
perturbations	O
generated	O
by	O
GENETICATTACK	O
were	O
rated	O
on	O
average	O
2.70	O
.	O
8	O
The	O
average	O
rating	O
given	O
for	O
both	O
methods	O
was	O
significantly	O
less	O
than	O
our	O
proposed	O
✏	O
sem	O
of	O
4	O
.	O
Using	O
a	O
clear	O
survey	O
question	O
illustrates	O
that	O
humans	O
,	O
on	O
average	O
,	O
do	O
n't	O
assess	O
these	O
perturbations	O
as	O
semantics	O
-	O
preserving	O
.	O

We	O
propose	O
evaluation	O
of	O
non	O
-	O
suspicion	O
by	O
having	O
judges	O
view	O
a	O
shuffled	O
mix	O
of	O
real	O
and	O
adversarial	O
inputs	O
and	O
guess	O
whether	O
each	O
is	O
real	O
or	O
computer	O
-	O
altered	O
.	O
This	O
is	O
similar	O
to	O
the	O
human	O
evaluation	O
done	O
by	O
Ren	O
et	O
al	O
(	O
2019	O
)	O
,	O
but	O
we	O
formulate	O
it	O
as	O
a	O
binary	O
classification	O
task	O
rather	O
than	O
on	O
a	O
1	O
-	O
5	O
scale	O
.	O
A	O
perturbed	O
example	O
x	O
adv	O
is	O
not	O
suspicious	O
if	O
the	O
percentage	O
of	O
judges	O
who	O
identify	O
x	O
adv	O
as	O
computer	O
-	O
altered	O
is	O
at	O
most	O
✏	O
ns	O
,	O
where	O
0	B-DatasetName
	O
✏	O
ns	O
	O
1	O
.	O

In	O
Section	O
4	O
,	O
we	O
evaluated	O
how	O
well	O
generated	O
examples	O
met	O
constraints	O
.	O
We	O
found	O
that	O
although	O
attacks	O
in	O
NLP	O
aspire	O
to	O
meet	O
linguistic	O
constraints	O
,	O
in	O
practice	O
,	O
they	O
frequently	O
violate	O
them	O
.	O
Now	O
,	O
we	O
adjust	O
automatic	O
constraints	O
applied	O
during	O
the	O
course	O
of	O
the	O
attack	O
to	O
produce	O
better	O
quality	O
adversarial	O
examples	O
.	O
We	O
set	O
out	O
to	O
find	O
if	O
a	O
set	O
of	O
constraint	O
application	O
methods	O
with	O
appropriate	O
thresholds	O
could	O
produce	O
adversarial	O
examples	O
that	O
are	O
semanticspreserving	O
,	O
grammatical	O
and	O
non	O
-	O
suspicious	O
.	O
We	O
modified	O
TEXTFOOLER	O
to	O
produce	O
TFADJUSTED	O
,	O
a	O
new	O
attack	O
with	O
stricter	O
constraint	O
application	O
.	O
To	O
enforce	O
grammaticality	O
,	O
we	O
added	O
Language	O
-	O
Tool	O
.	O
To	O
enforce	O
semantic	O
preservation	O
,	O
we	O
tuned	O
two	O
thresholds	O
which	O
filter	O
out	O
invalid	O
word	O
substitutions	O
:	O
(	O
a	O
)	O
minimum	O
cosine	O
similarity	O
between	O
counter	O
-	O
fitted	O
word	B-TaskName
embeddings	I-TaskName
and	O
(	O
b	O
)	O
minimum	O
cosine	O
similarity	O
between	O
sentence	B-TaskName
embeddings	I-TaskName
.	O
Through	O
human	O
studies	O
,	O
we	O
found	O
threshold	O
values	O
of	O
0.9	O
for	O
(	O
a	O
)	O
and	O
0.98	O
for	O
(	O
b	O
)	O
9	O
.	O
We	O
implemented	O
TFADJUSTED	O
using	O
TextAttack	O
,	O
a	O
Python	O
framework	O
for	O
implementing	O
adversarial	O
attacks	O
in	O
NLP	O
(	O
Morris	O
et	O
al	O
,	O
2020	O
)	O
.	O

We	O
tested	O
TFADJUSTED	O
to	O
determine	O
the	O
effect	O
of	O
tightening	O
constraint	O
application	O
.	O
We	O
used	O
the	O
IMDB	B-DatasetName
,	O
Yelp	O
,	O
and	O
MR	B-DatasetName
datasets	O
for	O
classifcation	O
as	O
in	O
Section	O
4	O
.	O
We	O
added	O
the	O
SNLI	B-DatasetName
and	O
MNLI	B-DatasetName
entailment	O
datasets	O
(	O
Bowman	O
et	O
al	O
,	O
2015	O
;	O
Williams	O
et	O
al	O
,	O
2018	O
)	O
for	O
the	O
portions	O
not	O
requring	O
human	O
evaluation	O
.	O
Table	O
5	O
shows	O
the	O
results	O
.	O
Semantics	O
.	O
TEXTFOOLER	O
generates	O
perturbations	O
for	O
which	O
human	O
judges	O
are	O
on	O
average	O
"	O
Not	O
sure	O
"	O
if	O
semantics	O
are	O
preserved	O
.	O
With	O
perturbations	O
generated	O
by	O
TFADJUSTED	O
,	O
human	O
judges	O
on	O
average	O
"	O
Agree	O
"	O
that	O
semantics	O
are	O
preserved	O
.	O

When	O
an	O
attack	O
's	O
success	O
rate	O
improves	O
,	O
it	O
may	O
be	O
the	O
result	O
of	O
either	O
(	O
a	O
)	O
improvement	O
of	O
the	O
search	O
method	O
for	O
finding	O
adversarial	O
perturbations	O
or	O
(	O
b	O
)	O
more	O
lenient	O
constraint	O
definitions	O
or	O
constraint	O
application	O
.	O
TEXTFOOLER	O
achieves	O
a	O
higher	O
success	O
rate	O
than	O
GENETICATTACK	O
,	O
but	O
Jin	O
et	O
al	O
(	O
2019	O
)	O
did	O
not	O
identify	O
whether	O
the	O
improvement	O
was	O
due	O
to	O
(	O
a	O
)	O
or	O
(	O
b	O
)	O
.	O
Since	O
TEXTFOOLER	O
uses	O
both	O
a	O
different	O
search	O
method	O
and	O
different	O
constraint	O
application	O
methods	O
than	O
GENETICATTACK	O
,	O
the	O
source	O
of	O
the	O
difference	O
in	O
attack	O
success	O
rates	O
is	O
unclear	O
.	O
To	O
determine	O
which	O
search	O
method	O
is	O
more	O
effective	O
,	O
we	O
used	O
TextAttack	O
to	O
compose	O
attacks	O
from	O
the	O
search	O
method	O
of	O
GENETICATTACK	O
and	O
the	O
constraint	O
application	O
methods	O
of	O
each	O
of	O
TEXTFOOLER	O
and	O
TFADJUSTED	O
(	O
Morris	O
et	O
al	O
,	O
2020	O
)	O
.	O
With	O
the	O
constraint	O
application	O
held	O
constant	O
,	O
we	O
can	O
identify	O
the	O
source	O
of	O
the	O
difference	O
in	O
attack	O
success	O
rate	O
.	O
Table	O
7	O
reveals	O
that	O
the	O
genetic	O
algorithm	O
of	O
GENETICATTACK	O
is	O
more	O
successful	O
than	O
the	O
greedy	O
search	O
of	O
TEXTFOOLER	O
at	O
both	O
constraint	O
application	O
levels	O
.	O
This	O
reveals	O
the	O
source	O
of	O
improvement	O
in	O
attack	O
success	O
rate	O
between	O
GENETICATTACK	O
and	O
TEXTFOOLER	O
to	O
be	O
more	O
lenient	O
constraint	O
application	O
.	O
However	O
,	O
GE	O
-	O
NETICATTACK	O
's	O
genetic	O
algorithm	O
is	O
far	O
more	O
computationally	O
expensive	O
,	O
requiring	O
over	O
40x	O
more	O
model	O
queries	O
.	O
Table	O
7	O
:	O
Comparison	O
of	O
the	O
search	O
methods	O
from	O
GENETICATTACK	O
and	O
TEXTFOOLER	O
with	O
two	O
sets	O
of	O
constraints	O
(	O
TEXTFOOLER	O
and	O
TFADJUSTED	O
)	O
.	O
Attacks	O
were	O
run	O
on	O
1000	O
samples	O
against	O
BERT	B-MethodName
fine	O
-	O
tuned	O
on	O
the	O
MR	B-DatasetName
dataset	O
.	O
GENETICATTACK	O
's	O
genetic	O
algorithm	O
is	O
more	O
successful	O
than	O
TEXTFOOLER	O
's	O
greedy	O
strategy	O
,	O
albeit	O
much	O
less	O
efficient	O
.	O
that	O
preserve	O
semantics	O
and	O
grammaticality	O
,	O
NLP	O
models	O
are	O
relatively	O
robust	O
to	O
current	O
synonym	O
substitution	O
attacks	O
.	O
Note	O
that	O
our	O
set	O
of	O
constraints	O
is	O
n't	O
necessarily	O
optimal	O
for	O
every	O
attack	O
scenario	O
.	O
Some	O
contexts	O
may	O
require	O
fewer	O
constraints	O
or	O
less	O
strict	O
constraint	O
application	O
.	O
Decoupling	O
search	O
methods	O
and	O
constraints	O
.	O
It	O
is	O
critical	O
that	O
researchers	O
decouple	O
new	O
search	O
methods	O
from	O
new	O
constraint	O
evaluation	O
and	O
constraint	O
application	O
methods	O
.	O
Demonstrating	O
the	O
performance	O
of	O
a	O
new	O
attack	O
that	O
simultaneously	O
introduces	O
a	O
new	O
search	O
method	O
and	O
new	O
constraints	O
makes	O
it	O
unclear	O
whether	O
empirical	O
gains	O
indicate	O
a	O
more	O
effective	O
attack	O
or	O
a	O
more	O
relaxed	O
set	O
of	O
constraints	O
.	O
This	O
mirrors	O
a	O
broader	O
trend	O
in	O
machine	O
learning	O
where	O
researchers	O
report	O
differences	O
that	O
come	O
from	O
changing	O
multiple	O
independent	O
variables	O
,	O
making	O
the	O
sources	O
of	O
empirical	O
gains	O
unclear	O
(	O
Lipton	O
and	O
Steinhardt	O
,	O
2018	O
)	O
.	O
This	O
is	O
especially	O
relevant	O
in	O
adversarial	O
NLP	O
,	O
where	O
each	O
experiment	O
depends	O
on	O
many	O
parameters	O
.	O
Towards	O
improved	O
methods	O
for	O
generating	O
textual	O
adversarial	O
examples	O
.	O
As	O
models	O
improve	O
at	O
paraphrasing	O
inputs	O
,	O
we	O
will	O
be	O
able	O
to	O
explore	O
the	O
space	O
of	O
adversarial	O
examples	O
beyond	O
synonym	O
substitutions	O
.	O
As	O
models	O
improve	O
at	O
measuring	O
semantic	B-TaskName
similarity	I-TaskName
,	O
we	O
will	O
be	O
able	O
to	O
more	O
rigorously	O
ensure	O
that	O
adversarial	O
perturbations	O
preserve	O
semantics	O
.	O
It	O
remains	O
to	O
be	O
seen	O
how	O
robust	O
BERT	B-MethodName
is	O
when	O
subject	O
to	O
paraphrase	O
attacks	O
that	O
rigorously	O
preserve	O
semantics	O
and	O
grammaticality	O
.	O

The	O
goal	O
of	O
creating	O
adversarial	O
examples	O
that	O
preserve	O
semantics	O
and	O
grammaticality	O
is	O
common	O
in	O
the	O
NLP	O
attack	O
literature	O
(	O
Zhang	O
et	O
al	O
,	O
2019	O
)	O
.	O
However	O
,	O
previous	O
works	O
use	O
different	O
definitions	O
of	O
adversarial	O
examples	O
,	O
making	O
it	O
difficult	O
to	O
compare	O
methods	O
.	O
We	O
provide	O
a	O
unified	O
definition	O
of	O
an	O
adversarial	O
example	O
based	O
on	O
a	O
goal	O
function	O
and	O
a	O
set	O
of	O
linguistic	O
constraints	O
.	O
Gilmer	O
et	O
al	O
(	O
2018	O
)	O
laid	O
out	O
a	O
set	O
of	O
potential	O
constraints	O
for	O
the	O
attack	O
space	O
when	O
generating	O
adversarial	O
examples	O
,	O
which	O
are	O
each	O
useful	O
in	O
different	O
real	O
-	O
world	O
scenarios	O
.	O
However	O
,	O
they	O
did	O
not	O
discuss	O
NLP	O
attacks	O
in	O
particular	O
.	O
Michel	O
et	O
al	O
(	O
2019	O
)	O
defined	O
a	O
framework	O
for	O
evaluating	O
attacks	O
on	O
machine	B-TaskName
translation	I-TaskName
models	O
,	O
focusing	O
on	O
meaning	O
preservation	O
constraints	O
,	O
but	O
restricted	O
their	O
definitions	O
to	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
.	O
Other	O
research	O
on	O
NLP	O
attacks	O
has	O
suggested	O
various	O
constraints	O
but	O
has	O
not	O
introduced	O
a	O
shared	O
vocabulary	O
and	O
categorization	O
that	O
allows	O
for	O
effective	O
comparisons	O
between	O
attacks	O
.	O

Probabilistic	O
Case	O
-	O
based	O
Reasoning	O
for	O
Open	O
-	O
World	O
Knowledge	B-TaskName
Graph	I-TaskName
Completion	I-TaskName

Given	O
a	O
query	O
,	O
our	O
approach	O
gathers	O
KG	O
path	O
types	O
from	O
entities	O
that	O
are	O
similar	O
to	O
the	O
query	O
entity	O
.	O
Each	O
path	O
type	O
is	O
weighed	O
with	O
respect	O
to	O
an	O
estimate	O
of	O
both	O
its	O
frequency	O
and	O
precision	O
(	O
2.2.1	O
)	O
.	O
By	O
clustering	O
similar	O
entities	O
together	O
(	O
2.2.2	O
)	O
,	O
our	O
model	O
obtains	O
robust	O
estimate	O
of	O
the	O
path	O
statistics	O
(	O
2.2.3	O
)	O
.	O
Our	O
approach	O
is	O
non	O
-	O
parametric	O
because	O
-	O
(	O
a	O
)	O
Instead	O
of	O
storing	O
reasoning	O
rules	O
in	O
parameters	O
(	O
Das	O
et	O
al	O
,	O
2018	O
;	O
Minervini	O
et	O
al	O
,	O
2020	O
)	O
,	O
it	O
derives	O
them	O
dynamically	O
from	O
k	O
-	O
similar	O
entities	O
(	O
like	O
a	O
non	O
-	O
parametric	O
k	B-MethodName
-	I-MethodName
nn	I-MethodName
classifier	O
(	O
Cover	O
and	O
Hart	O
,	O
1967	O
)	O
)	O
.	O
(	O
b	O
)	O
We	O
cluster	O
entities	O
together	O
using	O
a	O
non	O
-	O
parametric	O
clustering	O
approach	O
and	O
provide	O
an	O
efficient	O
way	O
of	O
adding	O
/	O
estimating	O
parameters	O
when	O
entities	O
are	O
added	O
to	O
the	O
KG	O
(	O
2.3	O
)	O
.	O

Our	O
approach	O
first	O
finds	O
k	O
similar	O
entities	O
to	O
the	O
query	O
entity	O
that	O
have	O
atleast	O
an	O
edge	O
of	O
type	O
r	O
q	O
.	O
For	O
example	O
,	O
for	O
the	O
query	O
(	O
MELINDA	O
GATES	O
,	O
WORKS	O
IN	O
CITY	O
,	O
?	O
)	O
,	O
we	O
would	O
consider	O
WAR	O
-	O
REN	O
BUFFET	O
if	O
we	O
observe	O
(	O
WARREN	O
BUFFET	O
,	O
WORKS	O
IN	O
CITY	O
,	O
OMAHA	O
)	O
.	O
We	O
refer	O
to	O
these	O
entities	O
as	O
'	O
contextual	O
entities	O
'	O
.	O
Each	O
entity	O
is	O
represented	O
as	O
a	O
sparse	O
vector	O
of	O
its	O
outgoing	O
edge	O
types	O
,	O
i.e.	O
e	O
i	O
{	O
0	B-DatasetName
,	O
1	O
}	O
|	O
R	O
|	O
.	O
If	O
entity	O
e	O
i	O
has	O
m	O
distinct	O
outgoing	O
edge	O
types	O
,	O
then	O
the	O
dimension	O
corresponding	O
to	O
those	O
types	O
are	O
set	O
to	O
1	O
.	O
This	O
is	O
an	O
extremely	O
simple	O
and	O
flexible	O
way	O
of	O
representing	O
entities	O
which	O
we	O
find	O
to	O
work	O
well	O
.	O
Also	O
note	O
that	O
,	O
as	O
more	O
data	O
is	O
added	O
about	O
an	O
entity	O
,	O
this	O
sparse	O
representation	O
makes	O
it	O
trivial	O
to	O
update	O
the	O
embeddings	O
.	O
Let	O
E	O
c	O
,	O
q	O
denote	O
the	O
set	O
of	O
contextual	O
entities	O
for	O
the	O
query	O
q.	O
To	O
compute	O
E	O
c	O
,	O
q	O
,	O
we	O
first	O
sort	O
entities	O
with	O
respect	O
to	O
their	O
cosine	O
distance	O
with	O
respect	O
to	O
query	O
entity	O
and	O
select	O
the	O
k	O
entities	O
with	O
the	O
least	O
distance	O
and	O
which	O
have	O
the	O
query	O
relation	O
r	O
q	O
.	O
For	O
each	O
contextual	O
entity	O
e	O
c	O
,	O
we	O
gather	O
the	O
path	O
types	O
(	O
up	O
to	O
length	O
n	O
)	O
that	O
connect	O
e	O
c	O
to	O
the	O
entities	O
it	O
is	O
connected	O
by	O
the	O
edge	O
r	O
q	O
(	O
i.e.	O
P	O
n	O
(	O
e	O
c	O
,	O
r	O
q	O
)	O
in	O
2.1	O
)	O
.	O
These	O
extracted	O
path	O
types	O
will	O
be	O
used	O
to	O
reason	O
about	O
the	O
query	O
entity	O
.	O
Let	O
P	O
n	O
(	O
E	O
c	O
,	O
q	O
,	O
r	O
q	O
)	O
=	O
e	O
c	O
E	O
c	O
,	O
q	O
P	O
n	O
(	O
e	O
c	O
,	O
r	O
q	O
)	O
represent	O
the	O
set	O
of	O
unique	O
path	O
types	O
from	O
the	O
contextual	O
entities	O
.	O
The	O
probability	O
of	O
finding	O
the	O
answer	O
entity	O
e	O
2	O
given	O
the	O
query	O
is	O
given	O
by	O
:	O
P	O
(	O
e	O
2	O
|	O
e	O
1q	O
,	O
r	O
q	O
)	O
=	O
∑	O
p	O
P	O
n	O
(	O
E	O
(	O
c	O
,	O
q	O
)	O
,	O
r	O
q	O
)	O
P	O
(	O
e	O
2	O
,	O
p	O
|	O
e	O
1q	O
,	O
r	O
q	O
)	O
=	O
∑	O
p	O
P	O
(	O
p	O
|	O
e	O
1q	O
,	O
r	O
q	O
)	O
P	O
(	O
e	O
2	O
|	O
p	O
,	O
e	O
1q	O
,	O
r	O
q	O
)	O
(	O
1	O
)	O
We	O
marginalize	O
the	O
random	O
variable	O
representing	O
the	O
path	O
types	O
obtained	O
from	O
E	O
c	O
,	O
q	O
.	O
P	O
(	O
p	O
|	O
e	O
1q	O
,	O
r	O
q	O
)	O
denotes	O
the	O
probability	O
of	O
finding	O
a	O
path	O
type	O
given	O
the	O
query	O
.	O
This	O
term	O
captures	O
how	O
frequently	O
each	O
path	O
type	O
co	O
-	O
occurs	O
with	O
a	O
query	O
and	O
represents	O
the	O
prior	O
probability	O
for	O
a	O
path	O
type	O
.	O
On	O
the	O
other	O
hand	O
,	O
P	O
(	O
e	O
2	O
|	O
p	O
,	O
e	O
1q	O
,	O
r	O
q	O
)	O
captures	O
the	O
proportion	O
of	O
times	O
,	O
when	O
a	O
path	O
type	O
p	O
is	O
traversed	O
starting	O
from	O
the	O
query	O
entity	O
,	O
we	O
reach	O
the	O
correct	O
answer	O
instead	O
of	O
some	O
other	O
entity	O
.	O
This	O
term	O
can	O
be	O
understood	O
as	O
capturing	O
the	O
likelihood	O
of	O
reaching	O
the	O
right	O
answer	O
or	O
the	O
'	O
precision	O
'	O
of	O
a	O
reasoning	O
path	O
type	O
.	O
This	O
is	O
crucial	O
in	O
penalizing	O
'	O
spurious	O
'	O
path	O
types	O
that	O
sometimes	O
coincidentally	O
find	O
the	O
right	O
answer	O
entity	O
.	O
For	O
example	O
,	O
for	O
the	O
query	O
relation	O
WORKS	O
IN	O
CITY	O
,	O
the	O
path	O
type	O
(	O
FRIEND	O
LIVES	O
IN	O
CITY	O
)	O
might	O
have	O
a	O
high	O
prior	O
probability	O
(	O
since	O
people	O
often	O
have	O
many	O
friends	O
in	O
the	O
city	O
where	O
they	O
work	O
)	O
.	O
However	O
,	O
this	O
path	O
is	O
'	O
spurious	O
'	O
with	O
respect	O
to	O
WORKS	O
IN	O
CITY	O
,	O
since	O
they	O
might	O
have	O
friends	O
living	O
in	O
various	O
cities	O
and	O
hence	O
this	O
path	O
type	O
will	O
not	O
necessarily	O
return	O
the	O
correct	O
answer	O
.	O

Equation	O
1	O
has	O
parameters	O
for	O
each	O
entity	O
in	O
the	O
KG	O
.	O
For	O
large	O
KGs	O
,	O
this	O
can	O
quickly	O
lead	O
to	O
parameter	O
explosion	O
.	O
Also	O
,	O
estimating	O
per	O
-	O
entity	O
parameter	O
leads	O
to	O
noisy	O
estimates	O
due	O
to	O
sparsity	O
.	O
Instead	O
,	O
we	O
choose	O
to	O
cluster	O
similar	O
entities	O
together	O
.	O
Let	O
c	O
be	O
a	O
random	O
variable	O
representing	O
the	O
cluster	O
assignment	O
of	O
the	O
query	O
entity	O
.	O
Then	O
for	O
the	O
pathprior	O
term	O
,	O
we	O
have	O
P	O
(	O
p	O
|	O
e	O
1q	O
,	O
r	O
q	O
)	O
=	O
∑	O
c	O
P	O
(	O
c	O
|	O
e	O
1q	O
,	O
r	O
q	O
)	O
P	O
(	O
p	O
|	O
c	O
,	O
e	O
1q	O
,	O
r	O
q	O
)	O
We	O
assume	O
that	O
each	O
entity	O
is	O
assigned	O
to	O
one	O
cluster	O
,	O
so	O
P	O
(	O
c	O
|	O
e	O
1q	O
,	O
r	O
q	O
)	O
is	O
zero	O
for	O
all	O
clusters	O
except	O
the	O
cluster	O
in	O
which	O
the	O
query	O
entity	O
belongs	O
to	O
.	O
Secondly	O
we	O
assume	O
,	O
that	O
the	O
prior	O
probability	O
of	O
a	O
path	O
given	O
the	O
entity	O
and	O
cluster	O
can	O
be	O
determined	O
from	O
the	O
cluster	O
alone	O
and	O
is	O
independent	O
of	O
each	O
entity	O
in	O
the	O
cluster	O
.	O
In	O
other	O
words	O
,	O
if	O
c	O
e	O
1q	O
is	O
the	O
cluster	O
in	O
which	O
the	O
e	O
1	O
,	O
q	O
has	O
been	O
assigned	O
,	O
then	O
P	O
(	O
p	O
|	O
c	O
e	O
1q	O
,	O
e	O
1q	O
,	O
r	O
q	O
)	O
=	O
P	O
(	O
p	O
|	O
c	O
e	O
1q	O
,	O
r	O
q	O
)	O
.	O
Instead	O
of	O
perentity	O
parameters	O
,	O
we	O
now	O
aggregate	O
statistics	O
over	O
entities	O
in	O
the	O
same	O
cluster	O
and	O
have	O
per	O
-	O
cluster	O
parameters	O
.	O
We	O
also	O
show	O
that	O
this	O
leads	O
to	O
significantly	O
better	O
performance	O
(	O
3.3	O
)	O
.	O
A	O
similar	O
argument	O
applies	O
for	O
the	O
path	O
-	O
precision	O
term	O
in	O
which	O
we	O
calculate	O
the	O
proportion	O
of	O
times	O
,	O
a	O
path	O
leads	O
to	O
the	O
correct	O
answer	O
entity	O
starting	O
from	O
each	O
entity	O
in	O
the	O
cluster	O
.	O
To	O
perform	O
clustering	O
,	O
we	O
use	O
hierarchical	O
agglomerative	O
clustering	O
with	O
average	O
linkage	O
with	O
the	O
entity	O
-	O
entity	O
similarity	O
defined	O
in	O
2.2.1	O
.	O
We	O
extract	O
a	O
non	O
-	O
parameteric	O
number	O
of	O
clusters	O
from	O
the	O
hierarchy	O
using	O
a	O
threshold	O
on	O
the	O
linkage	O
function	O
.	O
Agglomerative	O
clustering	O
has	O
been	O
shown	O
to	O
be	O
effective	O
in	O
many	O
knowledge	O
-	O
base	O
related	O
tasks	O
such	O
as	O
entity	B-TaskName
resolution	I-TaskName
(	O
Lee	O
et	O
al	O
,	O
2012	O
;	O
Vashishth	O
et	O
al	O
,	O
2018	O
)	O
and	O
in	O
general	O
has	O
shown	O
to	O
outperform	O
flat	O
clustering	O
methods	O
such	O
as	O
K	O
-	O
means	O
(	O
Green	O
et	O
al	O
,	O
2012	O
;	O
Kobren	O
et	O
al	O
,	O
2017	O
)	O
.	O
A	O
flat	O
clustering	O
is	O
extracted	O
from	O
the	O
hierarchical	O
clustering	O
by	O
using	O
a	O
threshold	O
on	O
the	O
linkage	O
function	O
score	O
.	O
We	O
perform	O
a	O
breadth	O
first	O
search	O
from	O
the	O
root	O
of	O
the	O
tree	O
stopping	O
at	O
nodes	O
for	O
which	O
the	O
linkage	O
is	O
above	O
the	O
given	O
threshold	O
.	O
The	O
nodes	O
where	O
the	O
search	O
stops	O
give	O
a	O
flat	O
clustering	O
(	O
refer	O
to	O
A.2	O
for	O
more	O
detail	O
on	O
this	O
)	O
.	O

Knowledge	B-TaskName
Base	I-TaskName
Completion	I-TaskName
.	O
Given	O
an	O
entity	O
e	O
1	O
and	O
a	O
relation	O
r	O
,	O
our	O
task	O
is	O
retrieve	O
all	O
entities	O
e	O
2	O
such	O
that	O
(	O
e	O
1	O
,	O
r	O
,	O
e	O
2	O
)	O
belongs	O
in	O
the	O
edges	O
E	O
in	O
a	O
KG	O
G.	O
This	O
task	O
is	O
known	O
as	O
tail	O
prediction	O
.	O
If	O
the	O
relation	O
is	O
instead	O
the	O
inverse	O
relation	O
r	O
−1	O
,	O
we	O
assume	O
that	O
we	O
are	O
given	O
an	O
e	O
2	O
and	O
asked	O
to	O
predict	O
entities	O
e	O
1	O
such	O
that	O
(	O
e	O
1	O
,	O
r	O
−1	O
,	O
e	O
2	O
)	O
belongs	O
in	O
the	O
edges	O
E	O
(	O
head	O
prediction	O
)	O
.	O
To	O
be	O
exactly	O
comparable	O
to	O
baselines	O
,	O
we	O
report	O
an	O
average	O
of	O
head	O
and	O
tail	O
prediction	O
results	O
3	O
.	O
We	O
are	O
given	O
a	O
knowledge	O
graph	O
with	O
three	O
partitions	O
of	O
edges	O
,	O
E	O
train	O
,	O
E	O
dev	O
,	O
E	O
test	O
.	O
For	O
this	O
task	O
,	O
we	O
evaluate	O
against	O
several	O
stateof	O
-	O
the	O
-	O
art	O
embeddings	O
based	O
models	O
such	O
as	O
Dist	O
-	O
Mult	O
(	O
Yang	O
et	O
al	O
,	O
2015	O
)	O
,	O
ComplEx	O
(	O
Trouillon	O
et	O
al	O
,	O
2016	O
)	O
,	O
ConvE	O
(	O
Dettmers	O
et	O
al	O
,	O
2018	O
)	O
,	O
RotatE	B-MethodName
(	O
Sun	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
also	O
compare	O
against	O
several	O
parametric	O
rule	O
learning	O
methods	O
-	O
NTP	O
,	O
NeuralLP	O
,	O
MINERVA	O
(	O
Das	O
et	O
al	O
,	O
2018	O
)	O
,	O
GNTP	O
(	O
Minervini	O
et	O
al	O
,	O
2020	O
)	O
and	O
also	O
the	O
closely	O
related	O
CBR	O
approach	O
of	O
Das	O
et	O
al	O
(	O
2020	O
)	O
.	O
Open	O
-	O
world	O
Knowledge	B-TaskName
Base	I-TaskName
Completion	I-TaskName
.	O
In	O
this	O
setting	O
,	O
we	O
begin	O
with	O
the	O
top	O
10	O
%	O
of	O
the	O
most	O
popular	O
nodes	O
(	O
with	O
several	O
edges	O
going	O
out	O
from	O
them	O
)	O
and	O
add	O
more	O
randomly	O
selected	O
nodes	O
such	O
that	O
the	O
initial	O
seed	O
KB	O
contains	O
50	O
%	O
of	O
all	O
the	O
entities	O
in	O
V	O
.	O
This	O
is	O
to	O
ensure	O
,	O
that	O
the	O
seed	O
KB	O
is	O
not	O
too	O
sparse	O
and	O
the	O
initial	O
models	O
trained	O
on	O
them	O
are	O
meaningful	O
.	O
Next	O
,	O
any	O
edges	O
between	O
the	O
nodes	O
selected	O
are	O
added	O
to	O
the	O
seed	O
KB	O
.	O
We	O
divide	O
the	O
rest	O
of	O
the	O
entities	O
randomly	O
into	O
10	O
batches	O
.	O
Each	O
batch	O
of	O
entities	O
is	O
incrementally	O
added	O
to	O
the	O
KB	O
along	O
with	O
the	O
edges	O
contained	O
in	O
it	O
.	O
The	O
validation	O
and	O
test	O
set	O
are	O
also	O
divided	O
in	O
the	O
same	O
way	O
,	O
i.e.	O
if	O
both	O
the	O
head	O
and	O
tail	O
entity	O
of	O
a	O
triple	O
are	O
present	O
in	O
the	O
KB	O
,	O
only	O
then	O
the	O
triple	O
is	O
put	O
in	O
the	O
corresponding	O
splits	O
.	O
Parametric	O
models	O
for	O
KBC	O
that	O
learn	O
representations	O
for	O
a	O
fixed	O
set	O
of	O
entities	O
can	O
not	O
handle	O
'	O
open	O
-	O
world	O
'	O
setting	O
out	O
-	O
of	O
-	O
the	O
-	O
box	O
.	O
We	O
extend	O
the	O
most	O
competitive	O
embedding	O
based	O
model	O
-	O
RotatE	B-MethodName
(	O
Sun	O
et	O
al	O
,	O
2019	O
)	O
for	O
this	O
task	O
.	O
For	O
every	O
new	O
entity	O
arriving	O
in	O
a	O
batch	O
,	O
we	O
initialize	O
a	O
new	O
entity	O
embedding	O
for	O
it	O
.	O
We	O
explore	O
two	O
ways	O
of	O
initial	O
-	O
Here	O
,	O
represents	O
the	O
Hadamard	O
(	O
or	O
element	O
-	O
wise	O
)	O
product	O
.	O
This	O
initialization	O
minimizes	O
the	O
RotatE	B-MethodName
objective	O
for	O
the	O
new	O
embedding	O
ensuring	O
that	O
it	O
is	O
"	O
well	O
-	O
placed	O
"	O
according	O
to	O
the	O
model	O
in	O
the	O
previous	O
time	O
step	O
.	O
Embeddings	O
for	O
new	O
relations	O
are	O
initialized	O
randomly	O
.	O
Next	O
,	O
the	O
model	O
is	O
further	O
trained	O
on	O
the	O
new	O
batch	O
of	O
triples	O
so	O
that	O
the	O
new	O
entity	B-TaskName
embeddings	I-TaskName
get	O
trained	O
.	O
Note	O
,	O
for	O
massive	O
KGs	O
,	O
it	O
might	O
be	O
impractical	O
to	O
re	O
-	O
train	O
on	O
the	O
entire	O
data	O
as	O
new	O
batches	O
of	O
data	O
arrive	O
frequently	O
,	O
however	O
to	O
still	O
prevent	O
the	O
model	O
to	O
forget	O
what	O
it	O
had	O
learned	O
before	O
,	O
we	O
also	O
sample	O
m%	O
of	O
triples	O
that	O
it	O
had	O
already	O
been	O
trained	O
on	O
and	O
re	O
-	O
train	O
on	O
them	O
.	O
We	O
ensure	O
that	O
triples	O
in	O
the	O
neighborhood	O
of	O
the	O
newly	O
added	O
entities	O
are	O
ten	O
times	O
likely	O
to	O
be	O
sampled	O
more	O
than	O
other	O
triples	O
.	O
We	O
also	O
try	O
a	O
setting	O
where	O
we	O
try	O
freezing	O
the	O
initially	O
trained	O
entity	B-TaskName
embeddings	I-TaskName
and	O
only	O
training	O
the	O
new	O
entity	O
and	O
relation	O
embeddings	O
.	O

The	O
results	O
for	O
KBC	O
tasks	O
are	O
presented	O
in	O
Table	O
2	O
and	O
3	O
4	O
.	O
Our	O
method	O
does	O
significantly	O
better	O
than	O
parametric	O
rule	O
learning	O
approaches	O
such	O
as	O
MIN	O
-	O
ERVA	O
,	O
GNTPs	O
and	O
the	O
recent	O
case	O
-	O
based	O
approach	O
of	O
Das	O
et	O
al	O
(	O
2020	O
)	O
.	O
We	O
would	O
like	O
to	O
highlight	O
the	O
difference	O
between	O
the	O
performance	O
of	O
our	O
model	O
and	O
that	O
of	O
Das	O
et	O
al	O
(	O
2020	O
)	O
on	O
the	O
test	O
-	O
II	O
evaluation	O
of	O
FB122	B-DatasetName
where	O
triples	O
can	O
be	O
answered	O
by	O
learning	O
logical	O
rules	O
.	O
This	O
results	O
emphasizes	O
the	O
importance	O
of	O
our	O
probabilistic	O
weighing	O
of	O
paths	O
.	O
We	O
also	O
perform	O
comparably	O
to	O
most	O
embedding	O
based	O
models	O
and	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
the	O
overall	O
test	O
sets	O
of	O
FB122	B-DatasetName
and	O
NELL	B-DatasetName
-	O
995	O
.	O
We	O
report	O
the	O
mean	O
over	O
3	O
runs	O
for	O
our	O
model	O
.	O
We	O
perform	O
an	O
ablation	O
where	O
we	O
do	O
not	O
cluster	O
entities	O
(	O
i.e.	O
every	O
entity	O
has	O
its	O
own	O
cluster	O
)	O
and	O
have	O
per	O
-	O
entity	O
parameters	O
.	O
Table	O
4	O
notes	O
the	O
drop	O
in	O
performance	O
due	O
to	O
the	O
noisy	O
estimates	O
of	O
path	O
prior	O
and	O
precision	O
parameters	O
because	O
of	O
sparsity	O
.	O
Table	O
6	O
shows	O
an	O
example	O
where	O
our	O
model	O
learns	O
to	O
score	O
different	O
paths	O
based	O
on	O
the	O
type	O
of	O
entities	O
present	O
in	O
the	O
cluster	O
.	O
Effect	O
of	O
path	O
length	O
on	O
WN18RR	B-DatasetName
:	O
On	O
the	O
dev	O
set	O
of	O
WN18RR	B-DatasetName
,	O
out	O
of	O
2985	O
queries	O
where	O
our	O
method	O
does	O
not	O
rank	O
the	O
answer	O
in	O
the	O
top	O
-	O
10	O
,	O
2030	O
queries	O
require	O
a	O
minimum	O
path	O
length	O
greater	O
than	O
3	O
.	O
Path	O
-	O
based	O
reasoning	O
models	O
have	O
no	O
power	O
to	O
answer	O
these	O
queries	O
.	O
To	O
correct	O
for	O
this	O
,	O
we	O
perform	O
an	O
experiment	O
with	O
the	O
path	O
length	O
n	O
=	O
5	O
(	O
950	O
of	O
2030	O
answers	O
are	O
reachable	O
)	O
.	O
The	O
results	O
in	O
Table	O
5	O
show	O
that	O
our	O
method	O
recovers	O
a	O
significant	O
portion	O
of	O
performance	O
when	O
allowed	O
to	O
use	O
longer	O
reasoning	O
paths	O
.	O

Open	O
-	O
world	O
KG	O
completion	O
.	O
Shi	O
and	O
Weninger	O
(	O
2018	O
)	O
consider	O
the	O
task	O
of	O
open	O
-	O
world	O
KG	O
completion	O
.	O
However	O
,	O
they	O
use	O
text	O
descriptions	O
to	O
learn	O
entity	O
representations	O
using	O
convolutional	O
neural	O
networks	O
.	O
Our	O
model	O
does	O
not	O
use	O
additional	O
text	O
data	O
and	O
we	O
use	O
very	O
simple	O
entity	O
representations	O
that	O
helps	O
us	O
to	O
perform	O
well	O
.	O
learns	O
to	O
update	O
a	O
KG	O
with	O
new	O
links	O
by	O
reading	O
news	O
.	O
Even	O
though	O
they	O
handle	O
adding	O
or	O
deleting	O
new	O
edges	O
,	O
they	O
do	O
not	O
observe	O
new	O
entities	O
.	O
Lastly	O
,	O
none	O
of	O
them	O
learn	O
from	O
similar	O
entities	O
using	O
a	O
CBR	O
approach	O
.	O
Inductive	O
representation	B-TaskName
learning	I-TaskName
on	O
KGs	O
.	O
Recent	O
works	O
(	O
Teru	O
et	O
al	O
,	O
2020	O
;	O
Wang	O
et	O
al	O
,	O
2020	O
)	O
learn	O
entity	O
independent	O
relation	O
representations	O
and	O
hence	O
allow	O
them	O
to	O
handle	O
unseen	O
entities	O
.	O
However	O
,	O
they	O
do	O
not	O
perform	O
contextual	O
reasoning	O
by	O
gathering	O
reasoning	O
paths	O
from	O
similar	O
entities	O
.	O
Moreoever	O
,	O
in	O
our	O
open	O
-	O
world	O
setting	O
,	O
we	O
consider	O
the	O
more	O
challenging	O
setting	O
,	O
where	O
new	O
facts	O
and	O
entities	O
are	O
arriving	O
in	O
a	O
streaming	O
fashion	O
and	O
we	O
give	O
an	O
efficient	O
way	O
of	O
updating	O
parameters	O
using	O
online	O
hierarchical	O
clustering	O
.	O
This	O
allows	O
our	O
method	O
to	O
be	O
applicable	O
in	O
settings	O
where	O
the	O
initial	O
KG	O
is	O
small	O
and	O
it	O
grows	O
continuously	O
.	O
Rule	O
induction	O
in	O
knowledge	B-TaskName
graphs	I-TaskName
.	O
Classic	O
work	O
in	O
inductive	B-TaskName
logic	I-TaskName
programming	I-TaskName
(	O
ILP	O
)	O
(	O
Muggleton	O
et	O
al	O
,	O
1992	O
;	O
Quinlan	O
,	O
1990	O
)	O
induce	O
rules	O
from	O
grounded	O
facts	O
.	O
However	O
,	O
they	O
need	O
explicit	O
counter	O
-	O
examples	O
which	O
are	O
not	O
present	O
in	O
KBs	O
and	O
they	O
do	O
not	O
scale	O
to	O
large	O
KBs	O
.	O
Recent	O
ILP	O
approaches	O
(	O
Galárraga	O
et	O
al	O
,	O
2013	O
(	O
Galárraga	O
et	O
al	O
,	O
,	O
2015	O
try	O
to	O
fix	O
Figure	O
3	O
:	O
Results	O
for	O
open	O
-	O
world	O
setting	O
when	O
trained	O
with	O
10	O
%	O
(	O
top	O
row	O
)	O
and	O
30	O
%	O
(	O
bottom	O
row	O
)	O
of	O
already	O
seen	O
edges	O
.	O
Our	O
online	O
method	O
matches	O
the	O
offline	O
version	O
of	O
our	O
approach	O
and	O
outperforms	O
the	O
online	O
variants	O
of	O
RotatE.	O
After	O
all	O
data	O
is	O
observed	O
our	O
online	O
method	O
achieves	O
results	O
closest	O
to	O
the	O
best	O
offline	O
method	O
's	O
results	O
.	O

(	O
politician	O
-	O
us	O
-	O
member	O
-	O
of	O
-	O
political	O
-	O
group	O
,	O
person	O
-	O
belongs	O
-	O
to	O
-	O
organization	O
−1	O
,	O
agent	B-DatasetName
-	O
belongs	O
-	O
to	O
-	O
organization	O
)	O
(	O
agent	B-DatasetName
-	O
collaborates	O
-	O
with	O
-	O
agent	B-DatasetName
,	O
agent	B-DatasetName
-	O
belongs	O
-	O
to	O
-	O
organization	O
)	O
(	O
Getoor	O
and	O
Taskar	O
,	O
2007	O
;	O
Kok	O
and	O
Domingos	O
,	O
2007	O
;	O
Schoenmackers	O
et	O
al	O
,	O
2010	O
)	O
and	O
probabilistic	O
logic	O
approaches	O
(	O
Richardson	O
and	O
Domingos	O
,	O
2006	O
;	O
Broecheler	O
et	O
al	O
,	O
2010	O
;	O
Wang	O
et	O
al	O
,	O
2013	O
)	O
combine	O
machine	O
learning	O
and	O
logic	O
to	O
learn	O
rules	O
.	O
However	O
,	O
none	O
of	O
these	O
work	O
derive	O
reasoning	O
rules	O
dynamically	O
from	O
similar	O
entities	O
in	O
the	O
knowledge	O
graph	O
.	O
Bayesian	O
non	O
-	O
parametric	O
approaches	O
for	O
linkprediction	O
.	O
There	O
is	O
a	O
rich	O
body	O
of	O
work	O
in	O
bayesian	O
non	O
-	O
parametrics	O
to	O
automatically	O
learn	O
the	O
latent	O
dimension	O
of	O
entities	O
(	O
Kemp	O
et	O
al	O
,	O
2006	O
;	O
Xu	O
et	O
al	O
,	O
2006	O
)	O
.	O
Our	O
method	O
does	O
not	O
learn	O
latent	O
dimension	O
of	O
entities	O
,	O
instead	O
our	O
work	O
is	O
nonparametric	O
because	O
it	O
gathers	O
reasoning	O
paths	O
from	O
nearest	O
neighbors	O
and	O
can	O
seamlessly	O
reason	O
with	O
new	O
entities	O
by	O
efficiently	O
updating	O
parameters	O
using	O
online	O
non	O
-	O
parametric	O
hierarchical	O
clustering	O
.	O
Embedding	O
-	O
based	O
approach	O
for	O
link	B-TaskName
prediction	I-TaskName
.	O
We	O
also	O
compare	O
to	O
the	O
more	O
popular	O
embeddings	O
based	O
models	O
based	O
on	O
tensor	O
factorization	O
or	O
neural	O
approaches	O
(	O
Nickel	O
et	O
al	O
,	O
2011	O
;	O
Bordes	O
et	O
al	O
,	O
2013	O
;	O
Dettmers	O
et	O
al	O
,	O
2018	O
;	O
Sun	O
et	O
al	O
,	O
2019	O
)	O
.	O
Our	O
simple	O
approach	O
which	O
needs	O
no	O
iterative	O
opti	O
-	O
mization	O
outperforms	O
most	O
of	O
them	O
and	O
performs	O
comparably	O
to	O
the	O
latest	O
RotatE	B-MethodName
model	O
.	O
Moreover	O
we	O
outperform	O
RotatE	B-MethodName
in	O
the	O
online	O
experiments	O
.	O
CBR	O
for	O
KG	O
completion	O
.	O
There	O
has	O
been	O
few	O
attempts	O
to	O
apply	O
CBR	O
for	O
knowledge	O
management	O
(	O
Dubitzky	O
et	O
al	O
,	O
1999	O
;	O
Bartlmae	O
and	O
Riemenschneider	O
,	O
2000	O
)	O
,	O
however	O
they	O
do	O
not	O
do	O
contextualized	O
reasoning	O
or	O
consider	O
online	O
settings	O
.	O
Our	O
work	O
is	O
most	O
closely	O
related	O
to	O
the	O
recent	O
work	O
of	O
Das	O
et	O
al	O
(	O
2020	O
)	O
.	O
However	O
,	O
since	O
it	O
does	O
not	O
take	O
in	O
to	O
account	O
the	O
importance	O
of	O
each	O
path	O
,	O
it	O
suffers	O
from	O
low	O
performance	O
,	O
with	O
our	O
model	O
outperforming	O
it	O
in	O
several	O
benchmarks	O
.	O

We	O
thank	O
anonymous	O
reviewers	O
and	O
members	O
of	O
UMass	O
IESL	O
and	O
NLP	O
groups	O
for	O
helpful	O
discussion	O
and	O
feedback	O
.	O
This	O
work	O
is	O
funded	O
in	O
part	O
by	O
the	O
Center	O
for	O
Data	O
Science	O
and	O
the	O
Center	O
for	O
Intelligent	O
Information	B-TaskName
Retrieval	I-TaskName
,	O
and	O
in	O
part	O
by	O
the	O
National	O
Science	O
Foundation	O
under	O
Grants	O
No	O
.	O
IIS	O
-	O
1514053	O
and	O
No	O
.	O
1763618	O
,	O
and	O
in	O
part	O
by	O
the	O
Chan	O
Zuckerberg	O
Initiative	O
under	O
the	O
project	O
Scientific	O
Knowledge	O
Base	O
Construction	O
.	O
Any	O
opinions	O
,	O
findings	O
and	O
conclusions	O
or	O
recommendations	O
expressed	O
in	O
this	O
material	O
are	O
those	O
of	O
the	O
authors	O
and	O
do	O
not	O
necessarily	O
reflect	O
those	O
of	O
the	O
sponsor	O
.	O

Incorporating	O
related	O
text	O
information	O
has	O
proven	O
successful	O
in	O
stock	B-TaskName
market	I-TaskName
prediction	I-TaskName
.	O
However	O
,	O
it	O
is	O
a	O
huge	O
challenge	O
to	O
utilize	O
texts	O
in	O
the	O
enormous	O
forex	O
(	O
foreign	O
currency	O
exchange	O
)	O
market	O
because	O
the	O
associated	O
texts	O
are	O
too	O
redundant	O
.	O
In	O
this	O
work	O
,	O
we	O
propose	O
a	O
BERT	B-MethodName
-	O
based	O
Hierarchical	O
Aggregation	O
Model	O
to	O
summarize	O
a	O
large	O
amount	O
of	O
finance	O
news	O
to	O
predict	O
forex	O
movement	O
.	O
We	O
firstly	O
group	O
news	O
from	O
different	O
aspects	O
:	O
time	O
,	O
topic	O
and	O
category	O
.	O
Then	O
we	O
extract	O
the	O
most	O
crucial	O
news	O
in	O
each	O
group	O
by	O
the	O
SOTA	O
extractive	B-TaskName
summarization	I-TaskName
method	O
.	O
Finally	O
,	O
we	O
conduct	O
interaction	O
between	O
the	O
news	O
and	O
the	O
trade	O
data	O
with	O
attention	O
to	O
predict	O
the	O
forex	O
movement	O
.	O
The	O
experimental	O
results	O
show	O
that	O
the	O
category	O
based	O
method	O
performs	O
best	O
among	O
three	O
grouping	O
methods	O
and	O
outperforms	O
all	O
the	O
baselines	O
.	O
Besides	O
,	O
we	O
study	O
the	O
influence	O
of	O
essential	O
news	O
attributes	O
(	O
category	O
and	O
region	O
)	O
by	O
statistical	O
analysis	O
and	O
summarize	O
the	O
influence	O
patterns	O
for	O
different	O
currency	O
pairs	O
.	O
*	O
This	O
work	O
is	O
done	O
when	O
Deli	O
Chen	O
is	O
a	O
intern	O
at	O
Mizuho	O
Securities	O
.	O

Deep	O
learning	O
and	O
Natural	O
Language	O
Processing	O
technologies	O
have	O
been	O
widely	O
applied	O
in	O
market	O
prediction	O
tasks	O
(	O
Strauß	O
et	O
al	O
,	O
2018	O
;	O
Alostad	O
and	O
Davulcu	O
,	O
2017	O
;	O
Li	O
et	O
al	O
,	O
2015	O
;	O
Ni	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
the	O
market	O
related	O
finance	O
news	O
has	O
proven	O
very	O
useful	O
for	O
the	O
prediction	O
(	O
Ding	O
et	O
al	O
,	O
2016	O
;	O
Xu	O
and	O
Cohen	O
,	O
2018	O
)	O
.	O
However	O
,	O
the	O
studies	O
of	O
prediction	O
in	O
forex	O
market	O
,	O
which	O
is	O
the	O
largest	O
market	O
in	O
the	O
world	O
with	O
the	O
highest	O
daily	O
trading	O
volume	O
,	O
is	O
much	O
less	O
than	O
that	O
in	O
the	O
stock	O
market	O
.	O
Figure	O
1	O
shows	O
the	O
average	O
numbers	O
per	O
hour	O
of	O
forex	O
related	O
news	O
.	O
There	O
is	O
a	O
large	O
amount	O
of	O
finance	O
news	O
related	O
to	O
forex	O
trading	O
with	O
different	O
influence	O
,	O
so	O
it	O
is	O
a	O
huge	O
challenge	O
to	O
extract	O
the	O
useful	O
semantic	O
information	O
from	O
news	O
.	O
Most	O
of	O
previous	O
works	O
(	O
Bakhach	O
et	O
al	O
,	O
2016	O
;	O
Shen	O
and	O
Liang	O
,	O
2016	O
;	O
Pradeepkumar	O
and	O
Ravi	O
,	O
2016	O
;	O
Contreras	O
et	O
al	O
,	O
2018	O
;	O
Weeraddana	O
et	O
al	O
,	O
2018	O
)	O
on	O
forex	O
prediction	O
ignore	O
related	O
text	O
totally	O
and	O
focus	O
on	O
the	O
forex	O
trade	O
data	O
only	O
,	O
which	O
loses	O
the	O
important	O
semantic	O
information	O
.	O
Yet	O
existing	O
works	O
(	O
Seifollahi	O
and	O
Shajari	O
,	O
2019	O
;	O
Nassirtoussi	O
et	O
al	O
,	O
2015	O
)	O
applying	O
finance	O
news	O
in	O
forex	O
prediction	O
mainly	O
rely	O
on	O
manual	O
rules	O
to	O
build	O
feature	O
vectors	O
,	O
which	O
can	O
hardly	O
access	O
the	O
semantic	O
information	O
effectively	O
.	O
To	O
make	O
better	O
use	O
of	O
finance	O
news	O
,	O
we	O
propose	O
a	O
novel	O
neural	O
model	O
:	O
Bert	O
-	O
based	O
Hierarchical	O
Aggregation	O
Model	O
(	O
BHAM	O
)	O
to	O
summarize	O
a	O
large	O
amount	O
of	O
finance	O
news	O
for	O
forex	O
movement	O
prediction	O
.	O
We	O
suppose	O
that	O
the	O
finance	O
news	O
is	O
redundant	O
and	O
only	O
a	O
small	O
amount	O
of	O
news	O
plays	O
a	O
crucial	O
role	O
in	O
forex	O
trading	O
.	O
So	O
the	O
key	O
point	O
is	O
how	O
to	O
extract	O
the	O
most	O
important	O
news	O
.	O
In	O
BHAM	O
,	O
we	O
design	O
a	O
hierarchical	O
structure	O
to	O
extract	O
essential	O
news	O
at	O
the	O
group	O
level	O
first	O
and	O
then	O
aggregate	O
the	O
semantic	O
information	O
across	O
all	O
groups	O
.	O
We	O
expect	O
the	O
news	O
is	O
more	O
related	O
intragroup	O
and	O
less	O
related	O
inter	O
-	O
groups	O
to	O
make	O
the	O
extraction	O
more	O
effective	O
.	O
We	O
design	O
three	O
grouping	O
methods	O
from	O
different	O
aspects	O
:	O
time	O
,	O
topic	O
or	O
category	O
.	O
At	O
the	O
group	O
level	O
,	O
we	O
concatenate	O
news	O
headlines	O
in	O
the	O
same	O
group	O
and	O
regard	O
news	O
extraction	O
in	O
each	O
group	O
as	O
an	O
extractive	B-TaskName
summarization	I-TaskName
task	O
.	O
We	O
modify	O
the	O
SOTA	O
extractive	B-TaskName
summarization	I-TaskName
model	O
proposed	O
in	O
(	O
Liu	O
,	O
2019	O
)	O
to	O
select	O
the	O
most	O
important	O
news	O
.	O
The	O
connection	O
process	O
can	O
let	O
the	O
selected	O
news	O
both	O
content	O
aware	O
and	O
context	O
aware	O
.	O
Followingly	O
,	O
we	O
conduct	O
multimodal	O
interaction	O
between	O
news	O
data	O
and	O
trade	O
data	O
through	O
attention	O
mechanism	O
to	O
predict	O
the	O
forex	O
prediction	O
.	O
The	O
trade	O
data	O
represents	O
the	O
history	O
movement	O
of	O
the	O
forex	O
,	O
and	O
the	O
news	O
data	O
represents	O
the	O
environment	O
variable	O
.	O
These	O
two	O
types	O
of	O
information	O
are	O
highly	O
related	O
.	O
We	O
conduct	O
experiments	O
on	O
four	O
major	O
currency	O
pairs	O
(	O
USD	O
-	O
EUR	O
,	O
USD	O
-	O
JPY	O
,	O
USD	O
-	O
RMB	O
,	O
USD	O
-	O
GBP	O
)	O
,	O
and	O
the	O
experimental	O
results	O
show	O
that	O
the	O
category	O
-	O
based	O
BHAM	O
performs	O
best	O
among	O
all	O
the	O
baselines	O
and	O
proposed	O
methods	O
in	O
all	O
currency	O
pairs	O
.	O
Based	O
on	O
this	O
method	O
,	O
we	O
analyze	O
the	O
influence	O
of	O
input	O
time	O
and	O
prediction	O
time	O
on	O
forex	O
trading	O
.	O
We	O
also	O
analyze	O
the	O
influence	O
of	O
news	O
category	O
and	O
news	O
region	O
and	O
find	O
various	O
influence	O
patterns	O
for	O
different	O
currency	O
pairs	O
,	O
which	O
may	O
be	O
enlightening	O
to	O
the	O
forex	O
investors	O
.	O
The	O
main	O
contributions	O
of	O
this	O
works	O
are	O
summarized	O
as	O
follows	O
:	O
We	O
design	O
a	O
novel	O
neural	O
model	O
to	O
incorporate	O
finance	O
news	O
in	O
forex	O
movement	O
prediction	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
work	O
to	O
use	O
the	O
neural	O
model	O
to	O
summarize	O
a	O
large	O
amount	O
of	O
news	O
for	O
forex	O
movement	O
prediction	O
.	O
We	O
propose	O
three	O
news	O
grouping	O
methods	O
from	O
different	O
aspects	O
:	O
time	O
,	O
topic	O
and	O
category	O
.	O
Experiments	O
show	O
that	O
the	O
category	O
based	O
method	O
performs	O
best	O
and	O
outperforms	O
all	O
the	O
baselines	O
.	O
Based	O
on	O
our	O
experiments	O
,	O
we	O
study	O
the	O
effect	O
of	O
time	O
parameters	O
on	O
forex	O
trading	O
.	O
We	O
also	O
analyze	O
and	O
summarize	O
different	O
influence	O
patterns	O
of	O
finance	O
news	O
(	O
both	O
category	O
and	O
region	O
)	O
on	O
different	O
currency	O
pairs	O
.	O

BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
is	O
a	O
potent	O
pretrained	O
contextualized	O
sentence	O
representation	O
and	O
has	O
proven	O
obvious	O
improvement	O
for	O
many	O
NLP	O
tasks	O
(	O
Sun	O
et	O
al	O
,	O
2019	O
;	O
Xu	O
et	O
al	O
,	O
2019	O
)	O
.	O
Liu	O
(	O
2019	O
)	O
proposes	O
a	O
modified	O
BERT	B-MethodName
for	O
extractive	B-TaskName
summarization	I-TaskName
and	O
achieve	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
result	O
in	O
extractive	B-TaskName
document	I-TaskName
summarization	I-TaskName
task	O
.	O
There	O
have	O
been	O
many	O
studies	O
applying	O
the	O
related	O
text	O
in	O
market	O
prediction	O
tasks	O
.	O
Moreover	O
,	O
the	O
text	O
assisted	O
stock	O
movement	O
prediction	O
has	O
attracted	O
many	O
researchers	O
'	O
interest	O
.	O
Most	O
of	O
these	O
works	O
predict	O
stock	O
movement	O
based	O
on	O
single	O
news	O
:	O
Si	O
et	O
al	O
(	O
2014	O
)	O
utilize	O
the	O
sentiment	B-TaskName
analysis	I-TaskName
to	O
help	O
the	O
prediction	O
.	O
Duan	O
et	O
al	O
(	O
2018	O
)	O
adopt	O
the	O
summarization	B-TaskName
of	O
news	O
body	O
instead	O
of	O
headline	O
to	O
predict	O
.	O
Ding	O
et	O
al	O
(	O
2016	O
)	O
propose	O
the	O
knowledgedriven	O
event	O
embedding	O
method	O
to	O
make	O
the	O
forecast	O
.	O
Yet	O
some	O
others	O
choose	O
multi	B-DatasetName
-	I-DatasetName
news	I-DatasetName
:	O
Hu	O
et	O
al	O
(	O
2018	O
)	O
propose	O
a	O
hybrid	O
attention	O
network	O
to	O
combine	O
news	O
in	O
different	O
days	O
.	O
However	O
,	O
the	O
number	O
of	O
combined	O
news	O
is	O
still	O
limited	O
and	O
much	O
smaller	O
than	O
that	O
of	O
forex	O
news	O
.	O
Compared	O
to	O
stock	B-TaskName
prediction	I-TaskName
,	O
works	O
about	O
forex	O
prediction	O
is	O
much	O
scarce	O
,	O
and	O
most	O
of	O
these	O
works	O
(	O
Carapuço	O
et	O
al	O
,	O
2018	O
;	O
Bakhach	O
et	O
al	O
,	O
2016	O
;	O
Yong	O
et	O
al	O
,	O
2018	O
;	O
Roledene	O
et	O
al	O
,	O
2016	O
;	O
Contreras	O
et	O
al	O
,	O
2018	O
;	O
Weeraddana	O
et	O
al	O
,	O
2018	O
)	O
do	O
not	O
consider	O
the	O
text	O
information	O
.	O
Shen	O
and	O
Liang	O
(	O
2016	O
)	O
employ	O
stacked	O
autoencoder	B-MethodName
to	O
get	O
the	O
trade	O
data	O
representation	O
and	O
adopt	O
support	O
vector	O
regression	O
to	O
predict	O
.	O
de	O
Almeida	O
et	O
al	O
(	O
2018	O
)	O
combine	O
SVM	B-MethodName
with	O
genetic	B-MethodName
algorithms	I-MethodName
to	O
optimize	O
investments	O
in	O
Forex	O
markets	O
based	O
on	O
history	O
price	O
.	O
Tsai	O
et	O
al	O
(	O
2018	O
)	O
choose	O
the	O
convolutional	O
neural	O
network	O
to	O
process	O
the	O
trading	O
data	O
.	O
Besides	O
,	O
only	O
limited	O
works	O
utilize	O
the	O
forex	O
related	O
text	O
in	O
the	O
prediction	O
process	O
.	O
Nassirtoussi	O
et	O
al	O
(	O
2015	O
)	O
adopt	O
the	O
WordNet	O
(	O
Miller	O
,	O
1995	O
)	O
and	O
SentiWordNet	O
(	O
Baccianella	O
et	O
al	O
,	O
2010	O
)	O
to	O
extract	O
the	O
text	O
semantic	O
and	O
sentiment	O
information	O
and	O
build	O
the	O
text	O
feature	O
vector	O
to	O
forecast	O
forex	O
movement	O
.	O
Following	O
this	O
work	O
,	O
Seifollahi	O
and	O
Shajari	O
(	O
2019	O
)	O
add	O
word	B-TaskName
sense	I-TaskName
disambiguation	I-TaskName
in	O
the	O
sentiment	B-TaskName
analysis	I-TaskName
of	O
news	O
headlines	O
.	O
Vijayan	O
and	O
Potey	O
(	O
2016	O
)	O
apply	O
the	O
J48	O
algorithm	O
in	O
analyzing	O
text	O
.	O
This	O
kind	O
of	O
method	O
pays	O
more	O
attention	O
to	O
access	O
a	O
fixed	O
feature	O
vector	O
from	O
news	O
and	O
can	O
only	O
represent	O
news	O
on	O
a	O
shallow	O
level	O
.	O
In	O
this	O
work	O
,	O
we	O
propose	O
a	O
selection	O
and	O
aggregation	O
neural	O
framework	O
to	O
process	O
the	O
larger	O
amount	O
of	O
finance	O
news	O
and	O
employ	O
the	O
powerful	O
pre	O
-	O
trained	O
BERT	B-MethodName
as	O
text	O
encoder	O
,	O
which	O
can	O
learn	O
the	O
deep	O
semantic	O
information	O
effectively	O
.	O

Each	O
sample	O
in	O
the	O
dataset	O
(	O
x	O
,	O
y	O
,	O
f	O
)	O
contains	O
the	O
set	O
of	O
news	O
text	O
x	O
,	O
the	O
forex	O
trade	O
data	O
y	O
,	O
and	O
the	O
forex	O
movement	O
label	O
f	O
.	O
x	O
and	O
y	O
happen	O
in	O
the	O
same	O
input	O
time	O
window	O
.	O
To	O
be	O
more	O
specific	O
,	O
x	O
is	O
a	O
list	O
of	O
news	O
groups	O
x	O
=	O
C	O
1	O
,	O
C	O
2	O
,	O
,	O
C	O
L	O
.	O
L	O
is	O
the	O
number	O
of	O
groups	O
.	O
The	O
methods	O
for	O
dividing	O
groups	O
are	O
introduced	O
in	O
Section	O
3.5	O
.	O
Each	O
news	O
group	O
is	O
a	O
sequence	O
of	O
finance	O
news	O
[	O
news	O
1	O
,	O
news	O
2	O
,	O
,	O
news	O
K	O
]	O
in	O
chronological	O
order	O
.	O
y	O
is	O
the	O
trade	O
data	O
embedding	O
accessed	O
by	O
the	O
method	O
introduced	O
in	O
Section	O
3.6	O
.	O
And	O
f	O
{	O
1	O
,	O
0	B-DatasetName
}	O
is	O
the	O
forex	O
movement	O
label	O
telling	O
whether	O
the	O
forex	O
trade	O
price	O
is	O
up	O
or	O
down	O
after	O
a	O
certain	O
time	O
(	O
we	O
call	O
it	O
prediction	O
delay	O
)	O
.	O
The	O
forex	O
movement	O
prediction	O
task	O
can	O
be	O
defined	O
as	O
assigning	O
movement	O
label	O
for	O
the	O
news	O
input	O
and	O
trade	O
data	O
input	O
.	O

The	O
overview	O
of	O
the	O
Bert	O
-	O
based	O
Hierarchical	O
Aggregation	O
Model	O
(	O
BHAM	O
)	O
is	O
displayed	O
in	O
Figure	O
2	O
.	O
The	O
model	O
can	O
be	O
generally	O
divided	O
into	O
two	O
steps	O
:	O
(	O
1	O
)	O
Intra	O
-	O
group	O
extraction	O
and	O
(	O
2	O
)	O
Intergroups	O
aggregation	O
.	O
In	O
the	O
Intra	O
-	O
group	O
extraction	O
step	O
,	O
news	O
in	O
the	O
same	O
group	O
is	O
connected	O
as	O
a	O
continuous	O
paragraph	O
,	O
and	O
we	O
conduct	O
extractive	B-TaskName
summarization	I-TaskName
on	O
this	O
paragraph	O
to	O
select	O
the	O
most	O
important	O
news	O
.	O
Specifically	O
,	O
we	O
employ	O
BERT	B-MethodName
as	O
the	O
encoder	O
to	O
get	O
the	O
contextualized	O
paragraph	O
representation	O
and	O
compute	O
the	O
importance	O
score	O
for	O
each	O
news	O
.	O
Then	O
we	O
select	O
and	O
aggregate	O
the	O
top	O
-	O
k	O
(	O
k	O
is	O
a	O
hyper	O
-	O
parameters	O
)	O
news	O
to	O
get	O
the	O
final	O
group	O
representation	O
.	O
In	O
the	O
Inter	O
-	O
groups	O
aggregation	O
step	O
,	O
we	O
first	O
access	O
the	O
trade	O
data	O
representation	O
by	O
a	O
3	O
-	O
layer	O
perceptron	O
and	O
then	O
employ	O
the	O
trade	O
data	O
representation	O
as	O
a	O
query	O
to	O
calculate	O
the	O
attention	O
scores	O
of	O
all	O
the	O
news	O
group	O
and	O
obtain	O
the	O
final	O
news	O
representation	O
.	O
Finally	O
,	O
we	O
fuse	O
the	O
final	O
news	O
representation	O
and	O
the	O
trade	O
data	O
representation	O
to	O
predict	O
the	O
forex	O
movement	O
.	O

There	O
will	O
be	O
lots	O
of	O
news	O
in	O
the	O
same	O
group	O
,	O
and	O
we	O
suppose	O
that	O
only	O
a	O
small	O
amount	O
of	O
news	O
has	O
the	O
greatest	O
influence	O
on	O
the	O
forex	O
movement	O
.	O
The	O
purpose	O
of	O
this	O
step	O
is	O
to	O
select	O
the	O
essential	O
news	O
from	O
all	O
news	O
in	O
group	O
,	O
which	O
is	O
redundant	O
and	O
full	O
of	O
noise	O
.	O
Inspired	B-DatasetName
by	O
the	O
BERT	B-MethodName
-	O
based	O
extractive	B-TaskName
summarization	I-TaskName
model	O
proposed	O
in	O
(	O
Liu	O
,	O
2019	O
)	O
,	O
we	O
modify	O
this	O
method	O
to	O
select	O
the	O
most	O
crucial	O
news	O
in	O
each	O
group	O
.	O
All	O
the	O
news	O
in	O
the	O
same	O
group	O
is	O
related	O
to	O
the	O
subject	O
of	O
this	O
group	O
,	O
and	O
the	O
connection	O
of	O
them	O
in	O
chronological	O
order	O
can	O
be	O
regarded	O
as	O
the	O
continuous	O
description	O
of	O
the	O
group	O
subject	O
.	O
The	O
connection	O
can	O
make	O
the	O
news	O
representations	O
realize	O
the	O
context	O
information	O
of	O
this	O
group	O
by	O
passing	O
information	O
among	O
different	O
news	O
.	O
We	O
suppose	O
the	O
context	O
information	O
can	O
help	O
select	O
better	O
news	O
in	O
group	O
.	O
The	O
form	O
of	O
group	O
news	O
input	O
for	O
BERT	B-MethodName
encoder	O
is	O
illustrated	O
in	O
Figure	O
3	O
.	O
We	O
insert	O
a	O
[	O
CLS	O
]	O
token	O
before	O
each	O
news	O
and	O
a	O
[	O
SEP	O
]	O
token	O
after	O
each	O
news	O
.	O
For	O
the	O
segment	O
embedding	O
,	O
we	O
use	O
the	O
loop	O
of	O
[	O
E	O
A	O
,	O
E	O
B	O
]	O
to	O
extend	O
the	O
raw	O
segment	O
embedding	O
of	O
BERT	B-MethodName
to	O
multi	O
-	O
sentences	O
.	O
After	O
the	O
BERT	B-MethodName
encoding	O
,	O
all	O
the	O
[	O
CLS	O
]	O
tokens	O
cls	O
are	O
regarded	O
as	O
the	O
semantic	O
representations	O
of	O
the	O
corresponding	O
news	O
.	O
The	O
importance	O
score	O
for	O
each	O
news	O
is	O
calculated	O
base	O
on	O
these	O
[	O
CLS	O
]	O
tokens	O
:	O
score	O
i	O
=	O
sigmoid	O
(	O
W	O
0	B-DatasetName
*	O
cls	O
i	O
+	O
b	O
0	B-DatasetName
)	O
(	O
1	O
)	O
t	O
i	O
=	O
TOP	O
k	O
(	O
score	O
i	O
)	O
(	O
2	O
)	O
s	O
i	O
=	O
softmax	B-MethodName
(	O
t	O
i	O
)	O
(	O
3	O
)	O
Where	O
i	O
{	O
1	O
,	O
2	O
,	O
,	O
L	O
}	O
,	O
L	O
is	O
the	O
number	O
of	O
groups	O
.	O
cls	O
i	O
is	O
the	O
list	O
of	O
[	O
CLS	O
]	O
tokens	O
in	O
the	O
ith	O
group	O
.	O
W	O
0	B-DatasetName
and	O
b	O
0	B-DatasetName
are	O
the	O
trainable	O
parameters	O
.	O
score	O
i	O
is	O
a	O
list	O
of	O
values	O
indicating	O
the	O
important	O
scores	O
of	O
news	O
.	O
TOP	O
k	O
is	O
an	O
operation	O
to	O
select	O
the	O
top	O
-	O
k	O
pieces	O
of	O
news	O
with	O
the	O
highest	O
scores	O
.	O
Then	O
the	O
group	O
representation	O
is	O
calculated	O
by	O
the	O
weighted	O
sum	O
of	O
the	O
top	O
-	O
k	O
[	O
CLS	O
]	O
tokens	O
:	O
G	O
i	O
=	O
k	O
j=1	O
cls	O
i	O
j	O
*	O
s	O
i	O
j	O
(	O
4	O
)	O
The	O
G	O
i	O
is	O
the	O
final	O
representation	O
of	O
the	O
i	O
-	O
th	O
news	O
group	O
which	O
contains	O
the	O
semantic	O
information	O
from	O
the	O
most	O
important	O
news	O
in	O
this	O
group	O
.	O

In	O
this	O
method	O
,	O
news	O
is	O
divided	O
into	O
groups	O
by	O
category	O
.	O
The	O
news	O
categories	O
1	O
are	O
{	O
Business	O
Sectors	O
,	O
Business	O
General	B-DatasetName
,	O
Business	O
Assets	O
,	O
Business	O
Commodities	O
,	O
Business	O
Organizations	O
,	O
Politics&International	O
Affairs	O
,	O
Arts&Culture&Entertainment&Sports	O
,	O
Science	O
&	O
Technology	O
,	O
Other	O
}	O
.	O
This	O
method	O
supposes	O
that	O
news	O
in	O
the	O
same	O
category	O
is	O
close	O
to	O
each	O
other	O
.	O

The	O
raw	O
record	O
of	O
forex	O
data	O
includes	O
the	O
open/	O
close	O
/	O
high	O
/	O
low	O
trade	O
prices	O
for	O
each	O
minute	O
.	O
In	O
order	O
to	O
extract	O
all	O
the	O
possible	O
features	O
,	O
we	O
build	O
the	O
trade	O
data	O
embedding	O
y	O
containing	O
multi	O
aspects	O
:	O
Raw	O
Number	O
:	O
open	O
/	O
close	O
/	O
high	O
/	O
low	O
trade	O
price	O
for	O
each	O
trade	O
minute	O
.	O
Change	O
Rate	O
:	O
change	O
rate	O
of	O
open	O
/	O
close/	O
high	O
/	O
low	O
price	O
compared	O
to	O
last	O
trade	O
minute	O
.	O
Trade	O
Statistics	O
:	O
mean	O
value	O
,	O
max	O
value	O
,	O
min	O
value	O
,	O
median	O
,	O
variance	O
of	O
all	O
the	O
trade	O
prices	O
in	O
input	O
minutes	O
.	O
The	O
min	O
-	O
max	O
scale	O
is	O
applied	O
for	O
each	O
currency	O
pair	O
's	O
samples	O
to	O
scale	O
the	O
raw	O
numbers	O
in	O
y	O
to	O
[	O
0	B-DatasetName
,	O
1	O
]	O
according	O
to	O
the	O
maximum	O
and	O
minimum	O
value	O
of	O
each	O
feature	O
.	O

Here	O
,	O
we	O
introduce	O
the	O
baselines	O
in	O
this	O
work	O
.	O
Since	O
there	O
are	O
few	O
existing	O
works	O
,	O
we	O
modify	O
two	O
advanced	O
models	O
from	O
stock	B-TaskName
prediction	I-TaskName
field	O
which	O
adopt	O
multi	B-DatasetName
-	I-DatasetName
news	I-DatasetName
as	O
input	O
for	O
this	O
task	O
.	O
Besides	O
,	O
we	O
design	O
some	O
ablation	O
variations	O
of	O
the	O
proposed	O
model	O
to	O
check	O
the	O
effects	O
of	O
different	O
modules	O
.	O
The	O
baselines	O
are	O
shown	O
below	O
:	O
NoNews	O
:	O
This	O
method	O
considers	O
the	O
forex	O
trade	O
data	O
only	O
and	O
use	O
a	O
3	O
-	O
layer	O
perceptron	O
(	O
the	O
setting	O
is	O
same	O
as	O
full	O
model	O
)	O
to	O
encode	O
the	O
trade	O
data	O
and	O
make	O
prediction	O
.	O
This	O
is	O
a	O
baseline	O
to	O
check	O
the	O
improvement	O
by	O
adding	O
text	O
information	O
.	O
SVM	B-MethodName
:	O
This	O
method	O
chooses	O
the	O
support	B-MethodName
vector	I-MethodName
machine	I-MethodName
to	O
predict	O
the	O
result	O
based	O
on	O
the	O
feature	O
vectors	O
extracted	O
by	O
the	O
method	O
introduced	O
in	O
(	O
Seifollahi	O
and	O
Shajari	O
,	O
2019	O
)	O
.	O
HAN	O
:	O
This	O
method	O
is	O
proposed	O
in	O
(	O
Hu	O
et	O
al	O
,	O
2018	O
)	O
for	O
stock	O
movement	O
prediction	O
.	O
It	O
includes	O
a	O
hybrid	O
attention	O
mechanism	O
and	O
Gated	B-MethodName
Recurrent	I-MethodName
Unit	I-MethodName
to	O
combine	O
multi	O
-	O
day	O
's	O
stock	O
news	O
to	O
predict	O
movement	O
.	O
We	O
use	O
every	O
5	O
minutes	O
instead	O
of	O
each	O
day	O
as	O
time	O
unit	O
for	O
this	O
method	O
and	O
the	O
StockNet	B-DatasetName
method	O
because	O
there	O
is	O
too	O
much	O
news	O
for	O
forex	O
trading	O
and	O
the	O
experiments	O
show	O
that	O
the	O
latest	O
news	O
has	O
the	O
most	O
influence	O
.	O
StockNet	B-DatasetName
:	O
This	O
method	O
is	O
proposed	O
in	O
(	O
Xu	O
and	O
Cohen	O
,	O
2018	O
)	O
.	O
It	O
treats	O
the	O
prediction	O
task	O
as	O
a	O
generation	O
task	O
and	O
designs	O
a	O
modified	O
variational	O
auto	O
encoder	O
to	O
process	O
multidays	O
'	O
tweets	O
to	O
predict	O
stock	O
movement	O
.	O
NoGroup	O
:	O
This	O
method	O
does	O
not	O
group	O
news	O
and	O
select	O
key	O
news	O
directly	O
from	O
all	O
news	O
.	O
NoConnect	O
:	O
This	O
method	O
does	O
not	O
connect	O
news	O
in	O
the	O
same	O
group	O
.	O
Instead	O
,	O
it	O
gets	O
the	O
representation	O
for	O
each	O
news	O
independently	O
using	O
BERT	B-MethodName
.	O
This	O
method	O
groups	O
news	O
by	O
category	O
.	O
LSTM+Attention	O
:	O
This	O
method	O
uses	O
the	O
bidirectional	B-MethodName
LSTM	I-MethodName
and	O
self	O
-	O
attention	O
to	O
replace	O
the	O
BERT	B-MethodName
as	O
text	O
encoder	O
.	O
The	O
number	O
of	O
LSTM	B-MethodName
hidden	O
states	O
is	O
256	O
,	O
and	O
the	O
hidden	O
-	O
layer	O
is	O
3	O
.	O
This	O
method	O
groups	O
news	O
by	O
category	O
.	O
methods	O
perform	O
well	O
,	O
and	O
both	O
BHAM	O
-	O
Topic	O
and	O
BHAM	O
-	O
Category	O
methods	O
outperform	O
all	O
the	O
baselines	O
.	O
The	O
BHAM	O
-	O
Category	O
performs	O
best	O
among	O
these	O
methods	O
,	O
which	O
shows	O
that	O
the	O
semantic	O
information	O
of	O
finance	O
news	O
is	O
mostly	O
aggregated	O
by	O
category	O
.	O
All	O
the	O
methods	O
get	O
improved	O
after	O
introducing	O
the	O
text	O
information	O
,	O
which	O
proves	O
the	O
related	O
finance	O
news	O
is	O
helpful	O
for	O
the	O
prediction	O
.	O
The	O
performance	O
of	O
NoGroup	O
method	O
decreases	O
by	O
a	O
large	O
margin	O
compared	O
to	O
BHAM	O
-	O
Category	O
,	O
which	O
demonstrates	O
that	O
the	O
hierarchical	O
structure	O
works	O
well	O
.	O
Without	O
hierarchical	O
structure	O
,	O
selecting	O
essential	O
news	O
directly	O
from	O
all	O
news	O
has	O
more	O
noise	O
and	O
requires	O
the	O
model	O
to	O
have	O
a	O
stronger	O
fitting	O
ability	O
for	O
a	O
longer	O
paragraph	O
.	O
After	O
removing	O
the	O
news	O
connection	O
,	O
the	O
performance	O
of	O
NoConnect	O
method	O
drops	O
sharply	O
compared	O
to	O
BHAM	O
-	O
Category	O
.	O
Accessing	O
the	O
news	O
representation	O
from	O
the	O
connected	O
paragraph	O
helps	O
the	O
news	O
representation	O
realize	O
the	O
context	O
information	O
in	O
the	O
group	O
.	O
The	O
LSTM+Attention	O
method	O
performs	O
worse	O
than	O
the	O
BERT	B-MethodName
-	O
based	O
method	O
,	O
which	O
proves	O
that	O
BERT	B-MethodName
has	O
stronger	O
power	O
of	O
sentence	O
encoding	O
.	O
The	O
two	O
methods	O
borrowed	O
from	O
stock	O
movement	O
prediction	O
are	O
designed	O
to	O
consider	O
all	O
news	O
's	O
information	O
,	O
but	O
the	O
forex	O
related	O
news	O
is	O
redundant	O
,	O
which	O
can	O
explain	O
the	O
poor	O
performance	O
of	O
these	O
two	O
methods	O
.	O

Polyglot	O
Semantic	B-TaskName
Parsing	I-TaskName
in	O
APIs	O

Traditional	O
approaches	O
to	O
semantic	B-TaskName
parsing	I-TaskName
(	O
SP	O
)	O
work	O
by	O
training	O
individual	O
models	O
for	O
each	O
available	O
parallel	O
dataset	O
of	O
text	O
-	O
meaning	O
pairs	O
.	O
In	O
this	O
paper	O
,	O
we	O
explore	O
the	O
idea	O
of	O
polyglot	O
semantic	O
translation	O
,	O
or	O
learning	O
semantic	B-TaskName
parsing	I-TaskName
models	O
that	O
are	O
trained	O
on	O
multiple	O
datasets	O
and	O
natural	O
languages	O
.	O
In	O
particular	O
,	O
we	O
focus	O
on	O
translating	O
text	O
to	O
code	O
signature	O
representations	O
using	O
the	O
software	O
component	O
datasets	O
of	O
Richardson	O
and	O
Kuhn	O
(	O
2017a	O
,	O
b	O
)	O
.	O
The	O
advantage	O
of	O
such	O
models	O
is	O
that	O
they	O
can	O
be	O
used	O
for	O
parsing	O
a	O
wide	O
variety	O
of	O
input	O
natural	O
languages	O
and	O
output	O
programming	O
languages	O
,	O
or	O
mixed	O
input	O
languages	O
,	O
using	O
a	O
single	O
unified	O
model	O
.	O
To	O
facilitate	O
modeling	O
of	O
this	O
type	O
,	O
we	O
develop	O
a	O
novel	O
graph	O
-	O
based	O
decoding	O
framework	O
that	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
the	O
above	O
datasets	O
,	O
and	O
apply	O
this	O
method	O
to	O
two	O
other	O
benchmark	O
SP	O
tasks	O
.	O
*	O
Returns	O
the	O
greater	O
of	O
two	O
long	O
values	O
public	O
static	O
long	O
max	O
(	O
long	O
a	O
,	O
long	O
b	O
)	O
2	O
.	O
(	O
en	O
,	O
Python	O
)	O
Documentation	O
max	O
(	O
self	O
,	O
a	O
,	O
b	O
)	O
:	O
"	O
"	O
"	O
Compares	O
two	O
values	O
numerically	O
and	O
returns	O
the	O
maximum	O
"	O
"	O
"	O
3	O
.	O
(	O
en	O
,	O
Haskell	O
)	O
Documentation	O
-	O
-	O
|	O
"	O
The	O
largest	O
element	O
of	O
a	O
non	O
-	O
empty	O
structure	O
"	O
maximum	O
:	O
:	O
forall	O
z.	O
Ord	O
a	O
a	O
=	O
>	O
t	O
a	O
-	O
>	O
a	O
4	O
.	O
(	O
de	O
,	O
PHP	O
)	O
Documentation	O
*	O
gibt	O
den	O
größeren	O
dieser	O
Werte	O
zurück	O
.	O

Recent	O
work	O
by	O
Richardson	O
and	O
Kuhn	O
(	O
2017a	O
,	O
b	O
)	O
;	O
Miceli	O
Barone	O
and	O
Sennrich	O
(	O
2017	O
)	O
considers	O
the	O
problem	O
of	O
translating	O
source	O
code	O
documentation	O
to	O
lower	O
-	O
level	O
code	O
template	O
representations	O
as	O
part	O
of	O
an	O
effort	O
to	O
model	O
the	O
meaning	O
of	O
such	O
documentation	O
.	O
Example	O
documentation	O
for	O
a	O
number	O
of	O
programming	O
languages	O
is	O
shown	O
in	O
Figure	O
1	O
,	O
where	O
each	O
docstring	O
description	O
in	O
red	O
describes	O
a	O
given	O
function	O
(	O
blue	O
)	O
in	O
the	O
library	O
.	O
While	O
capturing	O
the	O
semantics	O
of	O
docstrings	O
is	O
in	O
general	O
a	O
difficult	O
task	O
,	O
learning	O
the	O
translation	O
from	O
descriptions	O
to	O
formal	O
code	O
representations	O
(	O
e.g.	O
,	O
formal	O
representations	O
of	O
functions	O
)	O
is	O
proposed	O
as	O
a	O
reasonable	O
first	O
step	O
towards	O
learning	O
more	O
general	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
models	O
in	O
the	O
software	O
domain	O
.	O
Under	O
this	O
approach	O
,	O
one	O
can	O
view	O
a	O
software	O
library	O
,	O
or	O
API	O
,	O
as	O
a	O
kind	O
of	O
parallel	O
translation	O
corpus	O
for	O
studying	O
text	O
code	O
or	O
code	O
text	O
translation	O
.	O
Richardson	O
and	O
Kuhn	O
(	O
2017b	O
)	O
extracted	O
the	O
standard	O
library	O
documentation	O
for	O
10	O
popular	O
programming	O
languages	O
across	O
a	O
number	O
of	O
natural	O
languages	O
to	O
study	O
the	O
problem	O
of	O
text	O
to	O
function	O
signature	O
translation	O
.	O
Initially	O
,	O
these	O
datasets	O
were	O
proposed	O
as	O
a	O
resource	O
for	O
studying	O
semantic	O
parser	O
induction	O
(	O
Mooney	O
,	O
2007	O
)	O
,	O
or	O
for	O
building	O
models	O
that	O
learn	O
to	O
translate	O
text	O
to	O
formal	O
meaning	O
representations	O
from	O
parallel	O
data	O
.	O
In	O
followup	B-DatasetName
work	O
(	O
Richardson	O
and	O
Kuhn	O
,	O
2017a	O
)	O
,	O
they	O
proposed	O
using	O
the	O
resulting	O
models	O
to	O
do	O
automated	O
question	O
-	O
answering	O
(	O
QA	O
)	O
and	O
code	O
retrieval	O
on	O
target	O
APIs	O
,	O
and	O
experimented	O
with	O
an	O
additional	O
set	O
of	O
software	O
datasets	O
built	O
from	O
27	O
open	O
-	O
source	O
Python	O
projects	O
.	O
As	O
traditionally	O
done	O
in	O
SP	O
(	O
Zettlemoyer	O
and	O
Collins	O
,	O
2012	O
)	O
,	O
their	O
approach	O
involves	O
learning	O
individual	O
models	O
for	O
each	O
parallel	O
dataset	O
or	O
language	O
pair	O
,	O
e.g.	O
,	O
(	O
en	O
,	O
Java	O
)	O
,	O
(	O
de	O
,	O
PHP	O
)	O
,	O
and	O
(	O
en	O
,	O
Haskell	O
)	O
.	O
Looking	O
again	O
at	O
Figure	O
1	O
,	O
we	O
notice	O
that	O
while	O
programming	O
languages	O
differ	O
in	O
terms	O
of	O
representation	O
conventions	O
,	O
there	O
is	O
often	O
overlap	O
between	O
the	O
functionality	O
implemented	O
and	O
naming	O
in	O
these	O
different	O
languages	O
(	O
e.g.	O
,	O
the	O
max	O
function	O
)	O
,	O
and	O
redundancy	O
in	O
the	O
associated	O
linguistic	O
descriptions	O
.	O
In	O
addition	O
,	O
each	O
English	O
description	O
(	O
Figure	O
1.1	O
-	O
1.3	O
)	O
describes	O
max	O
differently	O
using	O
the	O
synonyms	O
greater	O
,	O
maximum	O
,	O
largest	O
.	O
In	O
this	O
case	O
,	O
it	O
would	O
seem	O
that	O
training	O
models	O
on	O
multiple	O
datasets	O
,	O
as	O
opposed	O
to	O
single	O
language	O
pairs	O
,	O
might	O
make	O
learning	O
more	O
robust	O
,	O
and	O
help	O
to	O
capture	O
various	O
linguistic	O
alternatives	O
.	O
With	O
the	O
software	O
QA	O
application	O
in	O
mind	O
,	O
an	O
additional	O
limitation	O
is	O
that	O
their	O
approach	O
does	O
not	O
allow	O
one	O
to	O
freely	O
translate	O
a	O
given	O
description	O
to	O
multiple	O
output	O
languages	O
,	O
which	O
would	O
be	O
useful	O
for	O
comparing	O
how	O
different	O
programming	O
languages	O
represent	O
the	O
same	O
functionality	O
.	O
The	O
model	O
also	O
can	O
not	O
translate	O
between	O
natural	O
languages	O
and	O
programming	O
languages	O
that	O
are	O
not	O
observed	O
during	O
training	O
.	O
While	O
software	O
documentation	O
is	O
easy	O
to	O
find	O
in	O
bulk	O
,	O
if	O
a	O
particular	O
API	O
is	O
not	O
already	O
documented	O
in	O
a	O
language	O
other	O
than	O
English	O
(	O
e.g.	O
,	O
Haskell	O
in	O
de	O
)	O
,	O
it	O
is	O
unlikely	O
that	O
such	O
a	O
translation	O
will	O
appear	O
without	O
considerable	O
effort	O
by	O
experienced	O
translators	O
.	O
Similarly	O
,	O
many	O
individual	O
APIs	O
may	O
be	O
too	O
small	O
or	O
poorly	O
documented	O
to	O
build	O
individual	O
models	O
or	O
QA	O
applications	O
,	O
and	O
will	O
in	O
some	O
way	O
need	O
to	O
bootstrap	O
off	O
of	O
more	O
general	O
models	O
or	O
resources	O
.	O
To	O
deal	O
with	O
these	O
issues	O
,	O
we	O
aim	O
to	O
learn	O
more	O
general	O
text	O
-	O
to	O
-	O
code	B-TaskName
translation	I-TaskName
models	O
that	O
are	O
trained	O
on	O
multiple	O
datasets	O
simultaneously	O
.	O
Our	O
ultimate	O
goal	O
is	O
to	O
build	O
polyglot	O
translation	O
models	O
(	O
cf	O
.	O
Johnson	O
et	O
al	O
(	O
2016	O
)	O
)	O
,	O
or	O
models	O
with	O
shared	O
representations	O
that	O
can	O
translate	O
any	O
input	O
text	O
to	O
any	O
output	O
programming	O
language	O
,	O
regardless	O
of	O
whether	O
such	O
language	O
pairs	O
were	O
encountered	O
explicitly	O
during	O
training	O
.	O
Inherent	O
in	O
this	O
task	O
is	O
the	O
challenge	O
of	O
building	O
an	O
efficient	O
polyglot	O
decoder	O
,	O
or	O
a	O
translation	O
mechanism	O
that	O
allows	O
such	O
crossing	O
between	O
input	O
and	O
output	O
languages	O
.	O
A	O
key	O
challenge	O
is	O
ensuring	O
that	O
such	O
a	O
decoder	O
generates	O
well	O
-	O
formed	O
code	O
representations	O
,	O
which	O
is	O
not	O
guaranteed	O
when	O
one	O
simply	O
applies	O
standard	O
decoding	O
strategies	O
from	O
SMT	O
and	O
neural	O
MT	O
(	O
cf	O
.	O
Cheng	O
et	O
al	O
(	O
2017	O
)	O
)	O
.	O
Given	O
our	O
ultimate	O
interest	O
in	O
API	O
QA	O
,	O
such	O
a	O
decoder	O
must	O
also	O
facilitate	O
monolingual	O
translation	O
,	O
or	O
being	O
able	O
to	O
translate	O
to	O
specific	O
output	O
languages	O
as	O
needed	O
.	O
To	O
solve	O
the	O
decoding	O
problem	O
,	O
we	O
introduce	O
a	O
new	O
graph	O
-	O
based	O
decoding	O
and	O
representation	O
framework	O
that	O
reduces	O
to	O
solving	O
shortest	O
path	O
problems	O
in	O
directed	O
graphs	O
.	O
We	O
investigate	O
several	O
translation	O
models	O
that	O
work	O
within	O
this	O
framework	O
,	O
including	O
traditional	O
SMT	O
models	O
and	O
models	O
based	O
on	O
neural	O
networks	O
,	O
and	O
report	O
stateof	O
-	O
the	O
-	O
art	O
results	O
on	O
the	O
technical	O
documentation	O
task	O
of	O
Richardson	O
and	O
Kuhn	O
(	O
2017b	O
,	O
a	O
)	O
.	O
To	O
show	O
the	O
applicability	O
of	O
our	O
approach	O
to	O
more	O
conventional	O
SP	O
tasks	O
,	O
we	O
apply	O
our	O
methods	O
to	O
the	O
Geo	O
-	O
Query	O
domain	O
(	O
Zelle	O
and	O
Mooney	O
,	O
1996	O
)	O
and	O
the	O
Sportscaster	O
corpus	O
(	O
Chen	O
et	O
al	O
,	O
2010	O
)	O
.	O
These	O
experiments	O
also	O
provide	O
insight	O
into	O
the	O
main	O
technical	O
documentation	O
task	O
and	O
highlight	O
the	O
strengths	O
and	O
weaknesses	O
of	O
the	O
various	O
translation	O
models	O
being	O
investigated	O
.	O

Our	O
approach	O
builds	O
on	O
the	O
baseline	O
models	O
introduced	O
in	O
Richardson	O
and	O
Kuhn	O
(	O
2017b	O
)	O
(	O
see	O
also	O
Deng	O
and	O
Chrupała	O
(	O
2014	O
)	O
)	O
.	O
Their	O
work	O
is	O
positioned	O
within	O
the	O
broader	O
SP	O
literature	O
,	O
where	O
traditionally	O
SMT	O
(	O
Wong	O
and	O
Mooney	O
,	O
2006a	O
)	O
and	O
parsing	O
(	O
Zettlemoyer	O
and	O
Collins	O
,	O
2009	O
)	O
methods	O
are	O
used	O
to	O
study	O
the	O
problem	O
of	O
translating	O
text	O
to	O
formal	O
meaning	O
representations	O
,	O
usually	O
centering	O
around	O
QA	O
applications	O
(	O
Berant	O
et	O
al	O
,	O
2013	O
)	O
.	O
More	O
recently	O
,	O
there	O
has	O
been	O
interest	O
in	O
using	O
neural	O
network	O
approaches	O
either	O
in	O
place	O
of	O
(	O
Dong	O
and	O
Lapata	O
,	O
2016	O
;	O
Kočiský	O
et	O
al	O
,	O
2016	O
)	O
or	O
in	O
combination	O
with	O
(	O
Misra	O
and	O
Artzi	O
,	O
2016	O
;	O
Jia	O
and	O
Liang	O
,	O
2016	O
;	O
Cheng	O
et	O
al	O
,	O
2017	O
)	O
these	O
traditional	O
models	O
,	O
the	O
latter	O
idea	O
we	O
look	O
at	O
in	O
this	O
paper	O
.	O
Work	O
in	O
NLP	O
on	O
software	O
documentation	O
has	O
accelerated	O
in	O
recent	O
years	O
due	O
in	O
large	O
part	O
to	O
the	O
availability	O
of	O
new	O
data	O
resources	O
through	O
websites	O
such	O
as	O
StackOverflow	O
and	O
Github	O
(	O
cf	O
.	O
Allamanis	O
et	O
al	O
(	O
2017	O
)	O
)	O
.	O
Most	O
of	O
this	O
recent	O
work	O
focuses	O
on	O
processing	O
large	O
amounts	O
of	O
API	O
data	O
in	O
bulk	O
(	O
Gu	O
et	O
al	O
,	O
2016	O
;	O
Miceli	O
Barone	O
and	O
Sennrich	O
,	O
2017	O
)	O
,	O
either	O
for	O
learning	O
longer	O
executable	O
programs	O
from	O
text	O
(	O
Yin	O
and	O
Neubig	O
,	O
2017	O
;	O
Rabinovich	O
et	O
al	O
,	O
2017	O
)	O
,	O
or	O
solving	O
the	O
inverse	O
problem	O
of	O
code	O
to	O
text	B-TaskName
generation	I-TaskName
(	O
Iyer	O
et	O
al	O
,	O
2016	O
;	O
.	O
In	O
contrast	O
to	O
our	O
work	O
,	O
these	O
studies	O
do	O
not	O
look	O
explicitly	O
at	O
translating	O
to	O
target	O
APIs	O
,	O
or	O
at	O
non	O
-	O
English	O
documentation	O
.	O
The	O
idea	O
of	O
polyglot	O
modeling	O
has	O
gained	O
some	O
traction	O
in	O
recent	O
years	O
for	O
a	O
variety	O
of	O
problems	O
(	O
Tsvetkov	O
et	O
al	O
,	O
2016	O
)	O
and	O
has	O
appeared	O
within	O
work	O
in	O
SP	O
under	O
the	O
heading	O
of	O
multilingual	O
SP	O
(	O
Jie	O
and	O
Lu	O
,	O
2014	O
;	O
Duong	O
et	O
al	O
,	O
2017	O
)	O
.	O
A	O
related	O
topic	O
is	O
learning	O
from	O
multiple	O
knowledge	O
sources	O
or	O
domains	O
(	O
Herzig	O
and	O
Berant	O
,	O
2017	O
)	O
,	O
which	O
is	O
related	O
to	O
our	O
idea	O
of	O
learning	O
from	O
multiple	O
APIs	O
.	O
When	O
building	O
models	O
that	O
can	O
translate	O
between	O
unobserved	O
language	O
pairs	O
,	O
we	O
use	O
the	O
term	O
zeroshot	O
translation	O
from	O
Johnson	O
et	O
al	O
(	O
2016	O
)	O
.	O

To	O
improve	O
the	O
baseline	O
translation	O
approach	O
used	O
previously	O
(	O
Section	O
3.1	O
)	O
,	O
we	O
pursue	O
a	O
graph	O
based	O
approach	O
.	O
Given	O
the	O
formulation	O
above	O
and	O
the	O
finiteness	O
of	O
our	O
prediction	O
space	O
C	O
,	O
our	O
approach	O
exploits	O
the	O
fact	O
that	O
we	O
can	O
represent	O
the	O
complete	O
component	O
search	O
space	O
for	O
any	O
set	O
of	O
APIs	O
as	O
a	O
directed	O
acyclic	O
finite	O
-	O
state	O
automaton	O
(	O
DAFSA	O
)	O
,	O
such	O
as	O
the	O
one	O
shown	O
graphically	O
in	O
Figure	O
2	O
.	O
The	O
underlying	O
graph	O
is	O
constructed	O
by	O
concatenating	O
all	O
of	O
the	O
component	O
representations	O
for	O
each	O
API	O
of	O
interest	O
and	O
applying	O
standard	O
finite	O
-	O
state	O
construction	O
and	O
minimization	O
techniques	O
(	O
Mohri	O
,	O
1996	O
)	O
.	O
Each	O
path	O
in	O
the	O
resulting	O
compact	O
automaton	O
is	O
therefore	O
a	O
well	O
-	O
formed	O
component	O
representation	O
.	O
Using	O
an	O
idea	O
from	O
Johnson	O
et	O
al	O
(	O
2016	O
)	O
,	O
we	O
add	O
to	O
each	O
component	O
representation	O
an	O
artificial	O
token	O
that	O
identifies	O
the	O
output	O
programming	O
language	O
or	O
library	O
.	O
For	O
example	O
,	O
the	O
two	O
edges	O
from	O
the	O
initial	O
state	O
0	B-DatasetName
in	O
Figure	O
2	O
are	O
labeled	O
as	O
2C	O
and	O
2Clojure	O
,	O
which	O
identify	O
the	O
C	O
and	O
Clojure	O
programming	O
languages	O
respectively	O
.	O
All	O
paths	O
starting	O
from	O
the	O
right	O
of	O
these	O
edges	O
are	O
therefore	O
valid	O
paths	O
in	O
each	O
respective	O
programming	O
language	O
.	O
The	O
paths	O
starting	O
from	O
the	O
initial	O
state	O
0	B-DatasetName
,	O
in	O
contrast	O
,	O
correspond	O
to	O
all	O
valid	O
component	O
representations	O
in	O
all	O
languages	O
.	O
Decoding	O
reduces	O
to	O
the	O
problem	O
of	O
finding	O
a	O
path	O
for	O
a	O
given	O
text	O
input	O
x.	O
For	O
example	O
,	O
given	O
the	O
input	O
the	O
ceiling	O
of	O
a	O
number	O
,	O
we	O
would	O
want	O
to	O
find	O
the	O
paths	O
corresponding	O
to	O
the	O
component	O
translations	O
numeric	O
math	O
ceil	O
arg	O
(	O
in	O
C	O
)	O
and	O
algo	O
math	O
ceil	O
x	O
(	O
in	O
Clojure	O
)	O
in	O
the	O
graph	O
shown	O
in	O
Figure	O
2	O
.	O
Using	O
the	O
trick	O
above	O
,	O
our	O
setup	O
facilitates	O
both	O
monolingual	O
decoding	O
,	O
i.e.	O
,	O
generating	O
components	O
specific	O
to	O
a	O
particular	O
output	O
language	O
(	O
e.g.	O
,	O
the	O
C	O
language	O
via	O
the	O
path	O
shown	O
in	O
bold	O
)	O
,	O
and	O
polyglot	O
decoding	O
,	O
i.e.	O
,	O
generating	O
any	O
output	O
language	O
by	O
starting	O
at	O
the	O
initial	O
state	O
0	B-DatasetName
(	O
e.g.	O
,	O
C	O
and	O
Clojure	O
)	O
.	O
We	O
formulate	O
the	O
decoding	O
problem	O
using	O
a	O
variant	O
of	O
the	O
well	O
-	O
known	O
single	O
source	O
shortest	O
path	O
(	O
SSSP	O
)	O
algorithm	O
for	O
directed	O
acyclic	O
graphs	O
(	O
DAGs	O
)	O
(	O
Johnson	O
(	O
1977	O
)	O
)	O
.	O
This	O
involves	O
a	O
graph	O
G	O
=	O
(	O
V	O
,	O
E	O
)	O
(	O
nodes	O
V	O
and	O
labeled	O
edges	O
E	O
,	O
see	O
graph	O
in	O
Figure	O
2	O
)	O
,	O
and	O
taking	O
an	O
off	O
-	O
line	O
topological	O
sort	O
of	O
the	O
graph	O
's	O
vertices	O
.	O
Using	O
a	O
data	O
structure	O
d	O
R	O
|	O
V	O
|	O
(	O
initialized	O
as	O
|	O
V	O
|	O
,	O
as	O
shown	O
in	O
Figure	O
2	O
)	O
,	O
the	O
standard	O
SSSP	O
algorithm	O
(	O
which	O
is	O
the	O
forward	O
update	O
variant	O
of	O
the	O
Viterbi	O
algorithm	O
(	O
Huang	O
,	O
2008	O
)	O
)	O
works	O
by	O
searching	O
forward	O
through	O
the	O
graph	O
in	O
sorted	O
order	O
and	O
finding	O
for	O
each	O
node	O
v	O
an	O
incoming	O
labeled	O
edge	O
u	O
,	O
with	O
label	O
z	O
,	O
that	O
solves	O
the	O
following	O
recurrence	O
:	O
d	O
(	O
v	O
)	O
=	O
min	O
(	O
u	O
,	O
z	O
)	O
:	O
(	O
u	O
,	O
v	O
,	O
z	O
)	O
E	O
d	O
(	O
u	O
)	O
+	O
w	O
(	O
u	O
,	O
v	O
,	O
z	O
)	O
(	O
2	O
)	O
where	O
d	O
(	O
u	O
)	O
is	O
shortest	O
path	O
score	O
from	O
a	O
unique	O
source	O
node	O
b	O
to	O
the	O
incoming	O
node	O
u	O
(	O
computed	O
recursively	O
)	O
and	O
w	O
(	O
u	O
,	O
v	O
,	O
z	O
)	O
is	O
the	O
weight	O
of	O
the	O
particular	O
labeled	O
edge	O
.	O
The	O
weight	O
of	O
the	O
resulting	O
shortest	O
path	O
is	O
commonly	O
taken	O
to	O
be	O
the	O
sum	O
of	O
the	O
path	O
edge	O
weights	O
as	O
given	O
by	O
w	O
,	O
and	O
the	O
output	O
translation	O
is	O
the	O
sequence	O
of	O
labels	O
associated	O
with	O
each	O
edge	O
.	O
This	O
algorithm	O
runs	O
in	O
linear	O
time	O
over	O
the	O
size	O
of	O
the	O
graph	O
's	O
adjacency	O
matrix	O
(	O
Adj	O
)	O
and	O
can	O
be	O
extended	O
to	O
find	O
k	O
SSSPs	O
.	O
In	O
the	O
standard	O
case	O
,	O
a	O
weighting	O
function	O
w	O
is	O
pro	O
-	O
d	O
[	O
V	O
[	O
G	O
]	O
]	O
,	O
π	O
[	O
V	O
[	O
G	O
]	O
]	O
N	O
il	O
,	O
d	O
[	O
b	O
]	O
o	O
2	O
:	O
s	O
[	O
V	O
[	O
G	O
]	O
,	O
n	O
]	O
0.0	O
Shortest	O
path	O
sums	O
at	O
each	O
node	O
3	O
:	O
for	O
each	O
vertex	O
u	O
≥	O
b	O
V	O
[	O
G	O
]	O
in	O
sorted	O
order	O
do	O
4	O
:	O
for	O
each	O
vertex	O
and	O
label	O
(	O
v	O
,	O
z	O
)	O
Adj	O
[	O
u	O
]	O
do	O
5	O
:	O
score	O
−log	O
n	O
i	O
pt	O
(	O
xi	O
|	O
z	O
)	O
+	O
s	O
[	O
u	O
,	O
i	O
]	O
6	O
:	O
if	O
d	O
[	O
v	O
]	O
>	O
score	O
then	O
7	O
:	O
d	O
[	O
v	O
]	O
score	O
,	O
π	O
[	O
v	O
]	O
u	O
8	O
:	O
for	O
i	O
in	O
1	O
,	O
..	O
,	O
n	O
do	O
Update	O
scores	O
9	O
:	O
s	O
[	O
v	O
,	O
i	O
]	O
pt	O
(	O
xi	O
|	O
z	O
)	O
+	O
s	O
[	O
u	O
,	O
i	O
]	O
10	O
:	O
return	O
FINDPATH	O
(	O
π	O
,	O
|	O
V	O
|	O
,	O
b	O
)	O
vided	O
by	O
assuming	O
a	O
static	O
weighted	O
graph	O
.	O
In	O
our	O
translation	O
context	O
,	O
we	O
replace	O
w	O
with	O
a	O
translation	O
model	O
,	O
which	O
is	O
used	O
to	O
dynamically	O
generate	O
edge	O
weights	O
during	O
the	O
SSSP	O
search	O
for	O
each	O
input	O
x	O
by	O
scoring	O
the	O
translation	O
between	O
x	O
and	O
each	O
edge	O
label	O
z	O
encountered	O
.	O
Given	O
this	O
general	O
framework	O
,	O
many	O
different	O
translation	O
models	O
can	O
be	O
used	O
for	O
scoring	O
.	O
In	O
what	O
follows	O
,	O
we	O
describe	O
two	O
types	O
of	O
decoders	O
based	O
on	O
lexical	O
translation	O
(	O
or	O
unigram	O
)	O
and	O
neural	O
sequence	O
models	O
.	O
Technically	O
,	O
each	O
decoding	O
algorithm	O
involves	O
modifying	O
the	O
standard	O
SSSP	O
search	O
procedure	O
by	O
adding	O
an	O
additional	O
data	O
structure	O
s	O
to	O
each	O
node	O
(	O
see	O
Figure	O
2	O
)	O
,	O
which	O
is	O
used	O
to	O
store	O
information	O
about	O
translations	O
(	O
e.g.	O
,	O
running	O
lexical	O
translation	O
scores	O
,	O
RNN	O
state	O
information	O
)	O
associated	O
with	O
particular	O
shortest	O
paths	O
.	O
By	O
using	O
these	O
two	O
very	O
different	O
models	O
,	O
we	O
can	O
get	O
insight	O
into	O
the	O
challenges	O
associated	O
with	O
the	O
technical	O
documentation	O
translation	O
task	O
.	O
As	O
we	O
show	O
in	O
Section	O
6	O
,	O
each	O
model	O
achieves	O
varying	O
levels	O
of	O
success	O
when	O
subjected	O
to	O
a	O
wider	O
range	O
of	O
SP	O
tasks	O
,	O
which	O
reveals	O
differences	O
between	O
our	O
task	O
and	O
other	O
SP	O
tasks	O
.	O

Our	O
second	O
set	O
of	O
models	O
use	O
neural	O
networks	O
to	O
compute	O
the	O
weighting	O
function	O
in	O
Equation	O
2	O
.	O
We	O
use	O
an	O
encoder	O
-	O
decoder	O
model	O
with	O
global	O
attention	O
(	O
Bahdanau	O
et	O
al	O
,	O
2014	O
;	O
Luong	O
et	O
al	O
,	O
2015	O
)	O
,	O
which	O
has	O
the	O
following	O
two	O
components	O
:	O
Encoder	O
Model	O
The	O
first	O
is	O
an	O
encoder	O
network	O
,	O
which	O
uses	O
a	O
bi	O
-	O
directional	O
recurrent	O
neural	O
network	O
architecture	O
with	O
LSTM	B-MethodName
units	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
to	O
compute	O
a	O
sequence	O
of	O
forward	O
annotations	O
or	O
hidden	O
states	O
(	O
−	O
h	O
1	O
,	O
...	O
,	O
−	O
h	O
|	O
x	O
|	O
)	O
and	O
a	O
sequence	O
of	O
backward	O
hid	O
-	O
1	O
Details	O
about	O
the	O
approx	O
.	O
are	O
provided	O
as	O
supp	O
.	O
material	O
.	O
den	O
states	O
(	O
−	O
h	O
,	O
...	O
,	O
−	O
h	O
|	O
x	O
|	O
)	O
for	O
the	O
input	O
sequence	O
(	O
x	O
1	O
,	O
...	O
,	O
x	O
|	O
x	O
|	O
)	O
.	O
Standardly	O
,	O
each	O
word	O
is	O
then	O
represented	O
as	O
the	O
concatenation	O
of	O
its	O
forward	O
and	O
backward	O
states	O
:	O
h	O
j	O
=	O
[	O
−	O
h	O
j	O
,	O
−	O
h	O
j	O
]	O
.	O

Technical	O
API	O
Docs	O
The	O
first	O
dataset	O
includes	O
the	O
Stdlib	O
and	O
Py27	O
datasets	O
of	O
Richardson	O
and	O
Kuhn	O
(	O
2017b	O
,	O
a	O
)	O
,	O
which	O
are	O
publicly	O
available	O
via	O
Richardson	O
(	O
2017	O
)	O
.	O
Stdlib	O
consists	O
of	O
short	O
description	O
and	O
function	O
signature	O
pairs	O
for	O
10	O
programming	O
languages	O
in	O
7	O
languages	O
,	O
and	O
Py27	O
contains	O
the	O
same	O
type	O
of	O
data	O
for	O
27	O
popular	O
Python	O
projects	O
in	O
English	O
mined	O
from	O
Github	O
.	O
We	O
also	O
built	O
new	O
datasets	O
from	O
the	O
Japanese	O
translation	O
of	O
the	O
Python	O
2.7	O
standard	O
library	O
,	O
as	O
well	O
as	O
the	O
Lua	O
stdlib	O
documentation	O
in	O
a	O
mixture	O
of	O
Russian	O
,	O
Portuguese	O
,	O
German	O
,	O
Spanish	O
and	O
English	O
.	O
Taken	O
together	O
,	O
these	O
resources	O
consist	O
of	O
79	O
,	O
885	O
training	O
pairs	O
,	O
and	O
we	O
experiment	O
with	O
training	O
models	O
on	O
Stdlib	O
and	O
Py27	O
separately	O
as	O
well	O
as	O
together	O
(	O
shown	O
as	O
+	O
more	O
in	O
Table	O
1	O
)	O
.	O
We	O
use	O
a	O
BPE	B-MethodName
subword	O
encoding	O
(	O
Sennrich	O
et	O
al	O
,	O
2015	O
)	O
of	O
both	O
input	O
and	O
output	O
words	O
to	O
make	O
the	O
representations	O
more	O
similar	O
and	O
transliterated	O
all	O
datasets	O
(	O
excluding	O
Japanese	O
datasets	O
)	O
to	O
an	O
8	O
-	O
bit	O
latin	O
encoding	O
.	O
Graphs	O
were	O
built	O
by	O
concatenating	O
all	O
function	O
representations	O
into	O
a	O
single	O
word	O
list	O
and	O
compiling	O
this	O
list	O
into	O
a	O
minimized	O
DAFSA	O
.	O
For	O
our	O
global	O
polyglot	O
dataset	O
,	O
this	O
resulted	O
in	O
a	O
graph	O
with	O
218	O
,	O
505	O
nodes	O
,	O
313	O
,	O
288	O
edges	O
,	O
and	O
112	O
,	O
107	O
paths	O
or	O
component	O
representations	O
over	O
an	O
output	O
vocabulary	O
of	O
9	O
,	O
324	O
words	O
.	O

We	O
run	O
experiments	O
on	O
the	O
GeoQuery	O
880	O
corpus	O
using	O
the	O
splits	O
from	O
Andreas	O
et	O
al	O
(	O
2013	O
)	O
,	O
which	O
includes	O
geography	O
queries	O
for	O
English	O
,	O
Greek	O
,	O
Thai	O
,	O
and	O
German	O
paired	O
with	O
formal	O
database	O
queries	O
,	O
as	O
well	O
as	O
a	O
seed	O
lexicon	O
or	O
NP	O
list	O
for	O
each	O
language	O
.	O
In	O
addition	O
to	O
training	O
models	O
on	O
each	O
individual	O
dataset	O
,	O
we	O
also	O
learn	O
polyglot	O
models	O
trained	O
on	O
all	O
datasets	O
concatenated	O
together	O
.	O
We	O
also	O
created	O
a	O
new	O
mixed	O
language	O
test	O
set	O
that	O
was	O
built	O
by	O
re	O
-	O
placing	O
NPs	O
in	O
803	O
test	O
examples	O
with	O
one	O
or	O
more	O
NPs	O
from	O
a	O
different	O
language	O
using	O
the	O
NP	O
lists	O
mentioned	O
above	O
(	O
see	O
examples	O
in	O
Figure	O
4	O
)	O
.	O
The	O
goal	O
in	O
the	O
last	O
case	O
is	O
to	O
test	O
our	O
model	O
's	O
ability	O
to	O
handle	O
mixed	O
language	O
input	O
.	O
We	O
also	O
ran	O
monolingual	O
experiments	O
on	O
the	O
English	O
Sportscaster	O
corpus	O
,	O
which	O
contains	O
human	O
generated	O
soccer	O
commentary	O
paired	O
with	O
symbolic	O
meaning	O
representation	O
produced	O
by	O
a	O
simulation	O
of	O
four	O
games	O
.	O
For	O
GeoQuery	O
graph	B-TaskName
construction	I-TaskName
,	O
we	O
built	O
a	O
single	O
graph	O
for	O
all	O
languages	O
by	O
extracting	O
general	O
rule	O
templates	O
from	O
all	O
representations	O
in	O
the	O
dataset	O
,	O
and	O
exploited	O
additional	O
information	O
and	O
patterns	O
using	O
the	O
Geobase	O
database	O
and	O
the	O
semantic	O
grammars	O
used	O
in	O
(	O
Wong	O
and	O
Mooney	O
,	O
2006b	O
)	O
.	O
This	O
resulted	O
in	O
a	O
graph	O
with	O
2	O
,	O
419	O
nodes	O
,	O
4	O
,	O
936	O
edges	O
and	O
39	O
,	O
482	O
paths	O
over	O
an	O
output	O
vocabulary	O
of	O
164	O
.	O
For	O
Sportscaster	O
,	O
we	O
directly	O
translated	O
the	O
semantic	O
grammar	O
provided	O
in	O
Chen	O
and	O
Mooney	O
(	O
2008	O
)	O
to	O
a	O
DAFSA	O
,	O
which	O
resulted	O
in	O
a	O
graph	O
with	O
98	O
nodes	O
,	O
86	O
edges	O
and	O
830	O
paths	O
.	O

Mixed	O
GeoQuery	O
(	O
de	O
/	O
gr	O
)	O
Input	O
:	O
Wie	O
hoch	O
liegt	O
der	O
höchstgelegene	O
punkt	O
in	O
Αλαμπάμα	O
?	O
Logical	O
Form	O
Translation	B-TaskName
:	O
answer	O
(	O
elevation	O
1	O
(	O
highest	O
(	O
place	O
(	O
loc	O
2	O
(	O
stateid	O
(	O
'	O
alabama	O
'	O
)	O
)	O
)	O
)	O
)	O
)	O
Figure	O
4	O
:	O
Examples	O
of	O
zero	O
-	O
shot	O
translation	O
when	O
running	O
in	O
polyglot	O
mode	O
(	O
1	O
-	O
3	O
,	O
function	O
representations	O
shown	O
in	O
a	O
conventionalized	O
format	O
)	O
,	O
and	O
mixed	O
language	O
parsing	O
(	O
4	O
)	O
.	O
Semantic	B-TaskName
Parsing	I-TaskName
Results	O
SP	O
results	O
are	O
summarized	O
in	O
Table	O
2	O
.	O
In	O
contrast	O
,	O
the	O
neural	O
models	O
,	O
especially	O
those	O
with	O
biasing	O
and	O
copying	O
,	O
strongly	O
outperform	O
all	O
other	O
models	O
and	O
are	O
competitive	O
with	O
related	O
work	O
.	O
In	O
the	O
GeoQuery	O
case	O
,	O
we	O
compare	O
against	O
two	O
classic	O
grammar	O
-	O
based	O
models	O
,	O
UBL	O
and	O
TreeTrans	O
,	O
as	O
well	O
as	O
a	O
feature	O
rich	O
,	O
neural	O
hybrid	O
tree	O
model	O
(	O
nHT	O
)	O
.	O
We	O
also	O
see	O
that	O
the	O
polyglot	O
Geo	O
achieves	O
the	O
best	O
performance	O
,	O
demonstrating	O
that	O
training	O
on	O
multiple	O
datasets	O
helps	O
in	O
this	O
domain	O
as	O
well	O
.	O
In	O
the	O
Sportscaster	O
case	O
we	O
compare	O
against	O
two	O
PCFG	O
learning	O
approaches	O
,	O
where	O
the	O
second	O
model	O
(	O
wo	O
-	O
PCFG	O
)	O
involves	O
a	O
grammar	O
with	O
complex	O
wordorder	O
constraints	O
.	O
The	O
advantage	O
of	O
training	O
a	O
polyglot	O
model	O
is	O
shown	O
on	O
the	O
results	O
related	O
to	O
mixed	O
language	O
parsing	O
(	O
i.e.	O
,	O
the	O
middle	O
set	O
of	O
results	O
)	O
.	O
Here	O
we	O
compared	O
against	O
the	O
best	O
performing	O
monolingual	O
English	O
model	O
(	O
Best	O
Mono	O
.	O
Model	O
)	O
,	O
which	O
does	O
not	O
have	O
a	O
way	O
to	O
deal	O
with	O
multilingual	O
NPs	O
.	O
We	O
also	O
find	O
the	O
neural	O
model	O
to	O
be	O
more	O
robust	O
than	O
the	O
lexical	O
models	O
with	O
reranking	O
.	O
While	O
the	O
lexical	O
models	O
overall	O
perform	O
poorly	O
on	O
both	O
tasks	O
,	O
the	O
weakness	O
of	O
this	O
model	O
is	O
particularly	O
acute	O
in	O
the	O
Sportscaster	O
case	O
.	O
We	O
found	O
that	O
mistakes	O
are	O
largely	O
related	O
to	O
the	O
ordering	O
of	O
arguments	O
,	O
which	O
these	O
lexical	O
(	O
unigram	O
)	O
models	O
are	O
blind	O
to	O
.	O
That	O
these	O
models	O
still	O
perform	O
reasonably	O
well	O
on	O
the	O
Geo	O
task	O
shows	O
that	O
such	O
ordering	O
issues	O
are	O
less	O
of	O
a	O
factor	O
in	O
this	O
domain	O
.	O

Since	O
the	O
time	O
of	O
the	O
Index	O
Thomisticus	O
by	O
father	O
Roberto	O
Busa	O
(	O
Busa	O
,	O
1974	O
(	O
Busa	O
,	O
-	O
1980	O
,	O
which	O
is	O
usually	O
mentioned	O
among	O
the	O
first	O
electronic	O
(	O
nowadays	O
called	O
"	O
digital	O
"	O
)	O
annotated	O
corpora	O
available	O
,	O
NLP	O
tools	O
for	O
automatic	O
morphological	B-TaskName
analysis	I-TaskName
and	O
lemmatisation	O
of	O
a	O
richly	O
inflected	O
language	O
like	O
Latin	O
were	O
needed	O
.	O
Over	O
the	O
last	O
decades	O
,	O
this	O
need	O
was	O
fulfilled	O
by	O
a	O
number	O
of	O
morphological	O
analysers	O
for	O
Latin	O
.	O
Among	O
the	O
most	O
widespread	O
ones	O
are	O
Morpheus	O
(	O
Crane	O
,	O
1991	O
)	O
,	O
Whitaker	O
's	O
Words	O
(	O
http://archives.nd.edu/words.html	O
)	O
and	O
Lemlat	O
(	O
Passarotti	O
,	O
2004	O
)	O
.	O
Over	O
the	O
past	O
ten	O
years	O
,	O
such	O
tools	O
have	O
become	O
essential	O
,	O
in	O
light	O
of	O
a	O
number	O
of	O
projects	O
aimed	O
at	O
developing	O
advanced	O
language	O
resources	O
for	O
Latin	O
,	O
like	O
treebanks	O
.	O
1	O
The	O
most	O
recent	O
advances	O
in	O
linguistic	O
annotation	O
of	O
Latin	O
treebanks	O
are	O
moving	O
beyond	O
the	O
level	O
of	O
syntax	O
,	O
by	O
performing	O
semantic	O
-	O
based	O
tasks	O
like	O
semantic	O
role	O
labelling	O
and	O
anaphora	O
and	O
ellipsis	O
resolution	O
(	O
Passarotti	O
,	O
2014	O
)	O
.	O
In	O
particular	O
,	O
in	O
the	O
area	O
of	O
Digital	O
Humanities	O
there	O
is	O
growing	O
interest	O
in	O
Named	B-TaskName
Entity	I-TaskName
Recognition	I-TaskName
(	O
NER	B-TaskName
)	O
,	O
especially	O
for	O
purposes	O
of	O
geographicalbased	O
analysis	O
of	O
texts	O
.	O
NER	B-TaskName
is	O
a	O
sub	O
-	O
branch	O
of	O
Information	O
Extraction	O
,	O
whose	O
inception	O
goes	O
back	O
to	O
the	O
Sixth	O
Message	O
Understanding	O
Conference	O
(	O
MUC	O
-	O
6	O
)	O
(	O
Grishman	O
and	O
Sundheim	O
,	O
1996	O
)	O
.	O
NER	B-TaskName
aims	O
at	O
recognising	O
and	O
labelling	O
(	O
multi	O
)	O
words	O
,	O
as	O
names	O
of	O
people	O
,	O
things	O
,	O
places	O
,	O
etc	O
.	O
Since	O
MUC	O
-	O
6	O
,	O
NER	B-TaskName
has	O
largely	O
expanded	O
,	O
with	O
several	O
applications	O
also	O
on	O
ancient	O
languages	O
(	O
see	O
,	O
for	O
example	O
,	O
Depauw	O
and	O
Van	O
Beek	O
,	O
2009	O
)	O
.	O
Although	O
Lemlat	O
provides	O
quite	O
a	O
large	O
coverage	O
of	O
the	O
Latin	O
lexicon	O
,	O
its	O
performance	O
is	O
limited	O
by	O
the	O
absence	O
of	O
an	O
Onomasticon	O
in	O
its	O
lexical	O
basis	O
,	O
which	O
would	O
be	O
helpful	O
for	O
tasks	O
like	O
NER	B-TaskName
.	O
Given	O
that	O
in	O
Latin	O
proper	O
names	O
undergo	O
morphological	B-TaskName
inflection	I-TaskName
,	O
in	O
this	O
paper	O
we	O
describe	O
our	O
work	O
of	O
enhancing	O
Lemlat	O
with	O
an	O
Onomasticon	O
.	O
The	O
paper	O
is	O
organised	O
as	O
follows	O
.	O
Section	O
2	O
presents	O
the	O
basic	O
features	O
of	O
Lemlat	O
.	O
Section	O
3	O
describes	O
our	O
method	O
to	O
enhance	O
Lemlat	O
with	O
an	O
Onomasticon	O
,	O
by	O
detailing	O
the	O
rules	O
for	O
the	O
automatic	O
enhancement	O
and	O
discussing	O
the	O
most	O
problematic	O
kinds	O
of	O
words	O
.	O
Section	O
4	O
evaluates	O
the	O
rules	O
and	O
presents	O
one	O
experiment	O
run	O
on	O
four	O
Latin	O
texts	O
.	O
Section	O
5	O
is	O
a	O
short	O
conclusion	O
and	O
sketches	O
the	O
future	O
work	O
.	O

The	O
lexical	O
basis	O
of	O
Lemlat	O
results	O
from	O
the	O
collation	O
of	O
three	O
Latin	O
dictionaries	O
(	O
Georges	O
andGeorges	O
,	O
1913	O
-	O
1918	O
;	O
Glare	O
,	O
1982	O
;	O
Gradenwitz	O
,	O
1904	O
)	O
.	O
It	O
counts	O
40	O
,	O
014	O
lexical	O
entries	O
and	O
43	O
,	O
432	O
lemmas	O
,	O
as	O
more	O
than	O
one	O
lemma	B-DatasetName
can	O
be	O
included	O
into	O
the	O
same	O
lexical	O
entry	O
.	O
Given	O
an	O
input	O
wordform	O
that	O
is	O
recognised	O
by	O
Lemlat	O
,	O
the	O
tool	O
produces	O
in	O
output	O
the	O
corresponding	O
lemma	B-DatasetName
(	O
s	O
)	O
and	O
a	O
number	O
of	O
tags	O
conveying	O
(	O
a	O
)	O
the	O
inflectional	O
paradigm	O
of	O
the	O
lemma	B-DatasetName
(	O
s	O
)	O
(	O
e.g.	O
first	O
declension	O
noun	O
)	O
and	O
(	O
b	O
)	O
the	O
morphological	O
features	O
of	O
the	O
input	O
wordform	O
(	O
e.g.	O
singular	O
nominative	O
)	O
,	O
as	O
well	O
as	O
the	O
identification	O
number	O
(	O
ID	O
)	O
of	O
the	O
lemma	B-DatasetName
(	O
s	O
)	O
in	O
the	O
lexical	O
basis	O
of	O
Lemlat	O
.	O
No	O
contextual	O
disambiguation	O
is	O
performed	O
.	O
For	O
instance	O
,	O
receiving	O
in	O
input	O
the	O
wordform	O
abamitae	O
(	O
"	O
great	O
-	O
aunt	O
"	O
)	O
,	O
Lemlat	O
outputs	O
the	O
corresponding	O
lemma	B-DatasetName
(	O
abamita	O
,	O
ID	O
:	O
A0019	O
)	O
,	O
the	O
tags	O
for	O
its	O
inflectional	O
paradigm	O
(	O
N1	O
:	O
first	O
declension	O
noun	O
)	O
and	O
those	O
for	O
the	O
morphological	O
features	O
of	O
the	O
input	O
wordform	O
(	O
feminine	O
singular	O
genitive	O
and	O
dative	O
;	O
feminine	O
plural	O
nominative	O
and	O
vocative	O
)	O
.	O
The	O
basic	O
component	O
of	O
the	O
lexical	O
look	O
-	O
up	O
table	O
used	O
by	O
Lemlat	O
to	O
analyse	O
input	O
wordforms	O
is	O
the	O
so	O
-	O
called	O
LES	O
(	O
"	O
LExical	O
Segment	O
"	O
)	O
.	O
The	O
LES	O
is	O
defined	O
as	O
the	O
invariable	O
part	O
of	O
the	O
inflected	O
form	O
(	O
e.g.	O
abamit	O
for	O
abamit	O
-	O
ae	B-MethodName
)	O
.	O
In	O
other	O
words	O
,	O
the	O
LES	O
is	O
the	O
sequence	O
(	O
or	O
one	O
of	O
the	O
sequences	O
)	O
of	O
characters	O
that	O
remains	O
the	O
same	O
in	O
the	O
inflectional	O
paradigm	O
of	O
a	O
lemma	B-DatasetName
(	O
hence	O
,	O
the	O
LES	O
does	O
not	O
necessarily	O
correspond	O
to	O
the	O
word	O
stem	O
)	O
.	O
Lemlat	O
includes	O
a	O
LES	O
archive	O
,	O
in	O
which	O
LES	O
are	O
assigned	O
an	O
ID	O
and	O
a	O
number	O
of	O
inflectional	O
features	O
among	O
which	O
are	O
a	O
tag	O
for	O
the	O
gender	O
of	O
the	O
lemma	B-DatasetName
(	O
for	O
nouns	O
only	O
)	O
and	O
a	O
code	O
(	O
called	O
CODLES	O
)	O
for	O
its	O
inflectional	O
category	O
.	O
According	O
to	O
the	O
CODLES	O
,	O
the	O
LES	O
is	O
compatible	O
with	O
the	O
endings	O
of	O
its	O
inflectional	O
paradigm	O
.	O
For	O
instance	O
,	O
the	O
CODLES	O
for	O
the	O
LES	O
abamit	O
is	O
N1	O
(	O
first	O
declension	O
nouns	O
)	O
and	O
its	O
gender	O
is	O
F	O
(	O
feminine	O
)	O
.	O
The	O
wordform	O
abamitae	O
is	O
thus	O
analysed	O
as	O
belonging	O
to	O
the	O
LES	O
abamit	O
because	O
the	O
segmentae	O
is	O
recognised	O
as	O
an	O
ending	O
compatible	O
with	O
a	O
LES	O
with	O
CODLES	O
N1	O
.	O

The	O
bedrock	O
of	O
our	O
work	O
is	O
Busa	O
's	O
(	O
1988	O
)	O
Totius	O
Latinitatis	O
Lemmata	O
,	O
which	O
contains	O
the	O
list	O
of	O
the	O
lemmas	O
(	O
92	O
,	O
052	O
)	O
from	O
the	O
5	O
th	O
edition	O
of	O
Lexicon	O
Totius	O
Latinitatis	O
(	O
Forcellini	O
,	O
1940	O
)	O
.	O
In	O
Busa	O
(	O
1988	O
)	O
,	O
three	O
kinds	O
of	O
metadata	O
are	O
assigned	O
to	O
each	O
lemma	B-DatasetName
:	O
(	O
a	O
)	O
a	O
code	O
for	O
the	O
section	O
of	O
the	O
dictionary	O
in	O
which	O
the	O
lemma	B-DatasetName
occurs	O
(	O
e.g.	O
ON	O
:	O
the	O
lemma	B-DatasetName
occurs	O
in	O
the	O
Onomasticon	O
)	O
,	O
(	O
b	O
)	O
a	O
code	O
for	O
the	O
inflectional	O
paradigm	O
the	O
lemma	B-DatasetName
belongs	O
to	O
and	O
its	O
gender	O
(	O
e.g.	O
BM	O
:	O
second	O
declension	O
masculine	O
nouns	O
)	O
and	O
(	O
c	O
)	O
the	O
number	O
of	O
lines	O
of	O
the	O
lexical	O
entry	O
for	O
the	O
lemma	B-DatasetName
in	O
Forcellini	O
.	O
In	O
order	O
to	O
enhance	O
Lemlat	O
with	O
Forcellini	O
's	O
Onomasticon	O
,	O
we	O
first	O
extracted	O
from	O
Busa	O
(	O
1988	O
)	O
the	O
list	O
of	O
those	O
lemmas	O
that	O
occur	O
in	O
the	O
ON	O
section	O
.	O
This	O
list	O
counts	O
28	O
,	O
178	O
lemmas	O
.	O
Then	O
,	O
we	O
built	O
a	O
number	O
of	O
rules	O
to	O
automatically	O
include	O
the	O
lemmas	O
of	O
the	O
Onomasticon	O
into	O
the	O
lexical	O
basis	O
of	O
Lemlat	O
.	O

Including	O
the	O
Onomasticon	O
of	O
Forcellini	O
into	O
Lemlat	O
means	O
converting	O
the	O
list	O
of	O
proper	O
names	O
provided	O
by	O
Busa	O
(	O
1988	O
)	O
into	O
the	O
same	O
format	O
of	O
the	O
LES	O
archive	O
.	O
In	O
order	O
to	O
perform	O
this	O
task	O
as	O
automatically	O
as	O
possible	O
,	O
we	O
built	O
a	O
number	O
of	O
rules	O
to	O
extract	O
the	O
relevant	O
information	O
for	O
each	O
lemma	B-DatasetName
in	O
the	O
list	O
,	O
namely	O
its	O
LES	O
,	O
CODLES	O
and	O
gender	O
.	O
By	O
exploiting	O
the	O
morphological	B-TaskName
tagging	I-TaskName
of	O
Busa	O
(	O
1988	O
)	O
,	O
which	O
groups	O
sets	O
of	O
lemmas	O
showing	O
common	O
inflectional	O
features	O
,	O
our	O
rules	O
treat	O
automatically	O
such	O
inflectionally	O
regular	O
groups	O
.	O
In	O
total	O
,	O
we	O
wrote	O
122	O
rules	O
,	O
which	O
fall	O
into	O
four	O
types	O
.	O
The	O
first	O
type	O
(	O
60	O
rules	O
)	O
builds	O
the	O
LES	O
by	O
removing	O
one	O
or	O
more	O
characters	O
from	O
the	O
right	O
side	O
of	O
the	O
lemma	B-DatasetName
.	O
Such	O
a	O
removal	O
is	O
constrained	O
by	O
the	O
code	O
for	O
the	O
inflectional	O
paradigm	O
of	O
the	O
lemma	B-DatasetName
,	O
which	O
is	O
then	O
used	O
to	O
create	O
both	O
the	O
CODLES	O
and	O
the	O
tag	O
for	O
the	O
gender	O
.	O
For	O
instance	O
,	O
the	O
lemma	B-DatasetName
marcus	O
(	O
"	O
Mark	O
"	O
)	O
is	O
assigned	O
the	O
inflectional	O
paradigm	O
BM	O
in	O
Busa	O
(	O
1988	O
)	O
.	O
One	O
rule	O
states	O
that	O
the	O
LES	O
for	O
BM	O
lemmas	O
ending	O
in	O
-	O
us	O
is	O
built	O
by	O
removing	O
the	O
last	O
two	O
characters	O
from	O
the	O
lemma	B-DatasetName
(	O
marcus	O
>	O
marc	O
)	O
The	O
inflectional	O
code	O
BM	O
stands	O
for	O
second	O
declension	O
(	O
B	O
)	O
masculine	O
(	O
M	O
)	O
nouns	O
:	O
this	O
is	O
converted	O
into	O
the	O
CODLES	O
of	O
Lemlat	O
for	O
second	O
declension	O
nouns	O
(	O
B	O
>	O
N2	O
)	O
and	O
into	O
the	O
tag	O
for	O
masculine	O
gender	O
(	O
M	O
>	O
m	O
)	O
.	O
The	O
second	O
type	O
of	O
rules	O
(	O
19	O
)	O
adds	O
one	O
or	O
more	O
characters	O
on	O
the	O
right	O
side	O
of	O
the	O
lemma	B-DatasetName
to	O
build	O
the	O
LES	O
.	O
Again	O
,	O
this	O
is	O
done	O
according	O
both	O
to	O
the	O
inflectional	O
paradigm	O
and	O
to	O
the	O
ending	O
of	O
the	O
lemma	B-DatasetName
in	O
Busa	O
(	O
1988	O
)	O
.	O
For	O
instance	O
,	O
the	O
LES	O
for	O
lemmas	O
with	O
inflectional	O
code	O
CM	O
(	O
third	O
declension	O
masculine	O
nouns	O
)	O
and	O
ending	O
in	O
-	O
o	O
is	O
built	O
by	O
adding	O
an	O
-	O
n	O
after	O
the	O
last	O
character	O
.	O
One	O
example	O
is	O
the	O
lemma	B-DatasetName
bappo	O
(	O
"	O
Bappo	O
"	O
)	O
,	O
whose	O
LES	O
is	O
bappon	O
,	O
as	O
third	O
declension	O
imparisyllable	O
nouns	O
are	O
analysed	O
by	O
Lemlat	O
by	O
using	O
the	O
basis	O
for	O
their	O
singular	O
genitive	O
(	O
bappon	O
-	O
is	O
)	O
.	O
The	O
third	O
type	O
of	O
rules	O
(	O
19	O
)	O
replaces	O
one	O
or	O
more	O
characters	O
on	O
the	O
right	O
side	O
of	O
the	O
lemma	B-DatasetName
with	O
others	O
.	O
For	O
instance	O
,	O
the	O
LES	O
of	O
clemens	O
(	O
"	O
Clement	O
"	O
,	O
third	O
declension	O
masculine	O
noun	O
ending	O
in	O
-	O
s	O
,	O
with	O
singular	O
genitive	O
clement	O
-	O
is	O
)	O
is	O
built	O
by	O
replacing	O
the	O
final	O
-	O
s	O
with	O
a	O
-	O
t	O
(	O
clement	O
)	O
.	O
The	O
last	O
type	O
of	O
rules	O
(	O
24	O
)	O
deals	O
with	O
those	O
lemmas	O
that	O
are	O
equal	O
to	O
their	O
LES	O
(	O
no	O
change	O
is	O
needed	O
)	O
.	O
These	O
are	O
uninflected	O
nouns	O
,	O
(	O
like	O
hamilcar	O
-	O
"	O
Hamilcar	O
"	O
)	O
,	O
which	O
can	O
be	O
easily	O
retrieved	O
because	O
they	O
are	O
assigned	O
a	O
specific	O
inflectional	O
code	O
in	O
Busa	O
(	O
1988	O
)	O
.	O

Not	O
all	O
inflectional	O
paradigms	O
are	O
as	O
much	O
regular	O
as	O
to	O
allow	O
for	O
a	O
fully	O
automatic	O
rule	O
-	O
based	O
treatment	O
.	O
For	O
instance	O
,	O
third	O
declension	O
feminine	O
nouns	O
represent	O
an	O
entangled	O
class	O
.	O
The	O
lemma	B-DatasetName
charybdis	O
,	O
-	O
is	O
(	O
"	O
Charybdis	O
"	O
)	O
is	O
a	O
third	O
declension	O
parisyllable	O
feminine	O
noun	O
ending	O
in	O
-	O
is	O
.	O
Instead	O
,	O
phegis	O
,	O
-	O
gidis	O
(	O
"	O
daughter	O
of	O
Phegeus	O
"	O
)	O
is	O
a	O
third	O
declension	O
imparisyllable	O
feminine	O
noun	O
ending	O
in	O
-	O
is	O
.	O
One	O
common	O
rule	O
can	O
not	O
be	O
used	O
for	O
these	O
two	O
kinds	O
of	O
words	O
.	O
We	O
overcome	O
such	O
problem	O
by	O
building	O
two	O
more	O
specific	O
rules	O
:	O
one	O
accounting	O
for	O
third	O
declension	O
feminine	O
nouns	O
ending	O
in	O
-	O
dis	O
and	O
one	O
for	O
third	O
declension	O
feminine	O
nouns	O
ending	O
in	O
-	O
gis	O
.	O
However	O
,	O
there	O
are	O
sub	O
-	O
groups	O
of	O
nouns	O
for	O
which	O
such	O
a	O
solution	O
does	O
not	O
work	O
,	O
like	O
third	O
declension	O
feminine	O
nouns	O
ending	O
in	O
-	O
mis	O
,	O
which	O
can	O
be	O
both	O
imparisyllable	O
nouns	O
(	O
e.g.	O
salamis	O
,	O
-	O
minis	O
,	O
"	O
Salamis	O
"	O
)	O
and	O
parisyllable	O
nouns	O
(	O
e.g.	O
tomis	O
,	O
-	O
is	O
,	O
"	O
Tomis	O
"	O
)	O
.	O
For	O
these	O
lemmas	O
we	O
checked	O
manually	O
their	O
inflection	O
in	O
Forcellini	O
and	O
assigned	O
LES	O
and	O
CODLES	O
accordingly	O
.	O
Another	O
group	O
of	O
tricky	O
words	O
includes	O
those	O
lemmas	O
that	O
show	O
two	O
(	O
or	O
even	O
more	O
)	O
different	O
inflectional	O
paradigms	O
.	O
For	O
instance	O
,	O
apollonides	O
(	O
"	O
Apollonides	O
"	O
)	O
shows	O
both	O
a	O
singular	O
genitive	O
of	O
the	O
second	O
declension	O
(	O
in	O
-	O
i	O
)	O
and	O
one	O
of	O
the	O
first	O
declension	O
(	O
in	O
-	O
ae	B-MethodName
)	O
.	O
We	O
treated	O
these	O
cases	O
manually	O
by	O
checking	O
their	O
lexical	O
entries	O
in	O
Forcellini	O
.	O
A	O
further	O
problem	O
is	O
represented	O
by	O
graphical	O
variants	O
,	O
which	O
are	O
managed	O
by	O
Lemlat	O
through	O
so	O
-	O
called	O
"	O
exceptional	O
forms	O
"	O
.	O
These	O
are	O
wordforms	O
that	O
are	O
hard	O
-	O
coded	O
in	O
the	O
LES	O
archive	O
and	O
are	O
assigned	O
the	O
same	O
ID	O
of	O
the	O
LES	O
used	O
to	O
build	O
their	O
base	O
lemma	B-DatasetName
.	O
For	O
instance	O
,	O
the	O
nominative	O
singular	O
of	O
the	O
lemma	B-DatasetName
jesus	O
(	O
"	O
Jesus	B-TaskName
"	O
)	O
is	O
attested	O
also	O
as	O
hiesus	O
,	O
ihesus	O
and	O
zesus	O
.	O
Beside	O
the	O
LES	O
jes	O
(	O
used	O
for	O
the	O
base	O
lemma	B-DatasetName
jesus	O
)	O
,	O
in	O
the	O
LES	O
archive	O
also	O
the	O
wordforms	O
hiesus	O
,	O
ihesus	O
and	O
zesus	O
are	O
recorded	O
and	O
assigned	O
the	O
same	O
ID	O
of	O
the	O
LES	O
jes	O
.	O

We	O
evaluated	O
the	O
enhancement	O
of	O
Lemlat	O
with	O
the	O
Onomasticon	O
of	O
Forcellini	O
in	O
two	O
steps	O
.	O
First	O
,	O
we	O
focused	O
on	O
the	O
accuracy	B-MetricName
of	O
the	O
rules	O
for	O
automatic	O
enhancement	O
.	O
Then	O
,	O
we	O
compared	O
the	O
new	O
version	O
of	O
Lemlat	O
with	O
the	O
previous	O
one	O
by	O
the	O
lexical	O
coverage	O
they	O
provide	O
for	O
four	O
Latin	O
texts	O
.	O

We	O
evaluated	O
the	O
enhancement	O
of	O
Lemlat	O
with	O
the	O
Onomasticon	O
of	O
Forcellini	O
by	O
comparing	O
the	O
lexical	O
coverage	O
provided	O
by	O
the	O
two	O
versions	O
of	O
the	O
tool	O
for	O
four	O
Latin	O
texts	O
of	O
similar	O
size	O
and	O
different	O
genre	O
(	O
prose	O
and	O
poetry	O
)	O
and	O
era	O
(	O
Classical	O
and	O
Late	O
Latin	O
)	O
.	O
3	O
The	O
coverage	O
of	O
Lemlat	O
on	O
the	O
four	O
test	O
texts	O
improved	O
of	O
4.86	O
%	O
on	O
average	O
after	O
the	O
enhancement	O
with	O
Forcellini	O
's	O
Onomasticon	O
.	O
The	O
highest	O
improvement	O
is	O
on	O
Virgil	O
(	O
+5.7	O
%	O
)	O
.	O
Most	O
of	O
the	O
words	O
not	O
analysed	O
by	O
LemlatON	O
are	O
graphical	O
variants	O
(	O
e.g.	O
creüsa	O
for	O
creusa	O
-	O
"	O
Creusa	O
"	O
)	O
or	O
part	O
of	O
the	O
inflectional	O
paradigm	O
of	O
lemmas	O
not	O
available	O
in	O
its	O
lexical	O
basis	O
.	O
Beside	O
these	O
words	O
,	O
there	O
are	O
Roman	O
numbers	O
(	O
e.g.	O
XV	O
,	O
"	O
fifteen	O
"	O
)	O
,	O
abbreviations	O
(	O
e.g.	O
kal	O
for	O
kalendae	O
,	O
"	O
calends	O
"	O
)	O
and	O
foreign	O
words	O
(	O
e.g.	O
µητέρα	O
,	O
"	O
mother	O
"	O
)	O
.	O
4	O
Table	O
3	O
shows	O
the	O
results	O
by	O
category	O
of	O
unknown	O
words	O
(	O
types	O
)	O
.	O
Roman	O
numbers	O
are	O
frequent	O
in	O
Caesar	O
's	O
text	O
(	O
1	O
)	O
.	O
The	O
fact	O
that	O
Lemlat	O
does	O
not	O
analyse	O
Roman	O
numbers	O
is	O
not	O
a	O
major	O
concern	O
,	O
as	O
their	O
form	O
is	O
regular	O
,	O
easily	O
predictable	O
and	O
interpretable	O
.	O
Only	O
a	O
few	O
of	O
them	O
can	O
raise	O
ambiguity	O
when	O
written	O
lowercase	O
.	O
For	O
instance	O
,	O
vi	O
(	O
"	O
six	O
"	O
)	O
is	O
homograph	O
with	O
the	O
singular	O
ablative	O
of	O
the	O
third	O
declension	O
noun	O
vis	O
(	O
"	O
power	O
"	O
)	O
.	O
Homography	O
can	O
hold	O
also	O
between	O
items	O
of	O
of	O
the	O
Onomasticon	O
and	O
the	O
original	O
lexical	O
basis	O
of	O
Lemlat	O
.	O
For	O
instance	O
,	O
the	O
lemma	B-DatasetName
augustus	O
occurs	O
both	O
in	O
the	O
original	O
Lemlat	O
(	O
a	O
first	O
class	O
adjective	O
,	O
"	O
solemn	O
"	O
)	O
and	O
in	O
the	O
Onomasticon	O
(	O
a	O
proper	O
name	O
,	O
"	O
Augustus	O
"	O
)	O
.	O
If	O
we	O
look	O
at	O
tokens	O
instead	O
of	O
types	O
,	O
coverage	O
rates	O
remain	O
quite	O
similar	O
,	O
as	O
it	O
is	O
shown	O
by	O
Table	O
4	O
It	O
is	O
worth	O
noting	O
that	O
,	O
while	O
the	O
text	O
of	O
Virgil	O
shows	O
the	O
highest	O
improvement	O
in	O
type	O
-	O
based	O
evaluation	O
(	O
+5.7	O
%	O
)	O
,	O
Caesar	O
's	O
De	O
Bello	O
Gallico	O
is	O
the	O
one	O
that	O
mostly	O
benefits	O
from	O
the	O
extension	O
of	O
Lemlat	O
with	O
the	O
Onomasticon	O
in	O
token	O
-	O
based	O
evaluation	O
(	O
+6.64	O
%	O
)	O
.	O
This	O
is	O
due	O
to	O
the	O
higher	O
number	O
of	O
occurrences	O
of	O
proper	O
names	O
in	O
Caesar	O
than	O
in	O
Virgil	O
.	O
Indeed	O
,	O
although	O
the	O
number	O
of	O
new	O
word	O
types	O
analysed	O
by	O
LemlatON	O
in	O
comparison	O
to	O
Lemlat	O
is	O
lower	O
for	O
Caesar	O
than	O
for	O
Virgil	O
,	O
the	O
opposite	O
holds	O
when	O
tokens	O
are	O
concerned	O
.	O
6	O
In	O
more	O
detail	O
,	O
the	O
average	O
number	O
of	O
occurrences	O
(	O
tokens	O
)	O
of	O
the	O
new	O
word	O
types	O
analysed	O
by	O
LemlatON	O
for	O
Caesar	O
is	O
3.59	O
(	O
542/151	O
)	O
,	O
while	O
it	O
is	O
1.71	O
for	O
Virgil	O
(	O
493/288	O
)	O
.	O

A	O
prevailing	O
paradigm	O
in	O
neural	O
text	B-TaskName
generation	I-TaskName
is	O
one	O
-	O
shot	O
generation	O
,	O
where	O
text	O
is	O
produced	O
in	O
a	O
single	O
step	O
.	O
The	O
one	O
-	O
shot	O
setting	O
is	O
inadequate	O
,	O
however	O
,	O
when	O
the	O
constraints	O
the	O
user	O
wishes	O
to	O
impose	O
on	O
the	O
generated	O
text	O
are	O
dynamic	O
,	O
especially	O
when	O
authoring	O
longer	O
documents	O
.	O
We	O
address	O
this	O
limitation	O
with	O
an	O
interactive	O
text	B-TaskName
generation	I-TaskName
setting	O
in	O
which	O
the	O
user	O
interacts	O
with	O
the	O
system	O
by	O
issuing	O
commands	O
to	O
edit	O
existing	O
text	O
.	O
To	O
this	O
end	O
,	O
we	O
propose	O
a	O
novel	O
text	O
editing	O
task	O
,	O
and	O
introduce	O
WikiDocEdits	B-DatasetName
,	O
a	O
dataset	O
of	O
singlesentence	O
edits	O
extracted	O
from	O
Wikipedia	O
revision	O
histories	O
.	O
We	O
show	O
that	O
our	O
Interactive	O
Editor	O
,	O
a	O
transformer	O
-	O
based	O
model	O
trained	O
on	O
this	O
dataset	O
,	O
outperforms	O
baselines	O
and	O
obtains	O
positive	O
results	O
in	O
both	O
automatic	O
and	O
human	O
evaluations	O
.	O
We	O
present	O
empirical	O
and	O
qualitative	O
analyses	O
of	O
this	O
model	O
's	O
performance	O
.	O
1	O

A	O
long	O
-	O
standing	O
goal	O
of	O
natural	O
language	O
processing	O
research	O
has	O
been	O
to	O
generate	O
long	O
-	O
form	O
text	O
(	O
Lebowitz	O
,	O
1985	O
;	O
Rashkin	O
et	O
al	O
,	O
2020	O
)	O
.	O
Recent	O
large	O
generative	O
language	O
models	O
such	O
as	O
GPT	B-MethodName
-	O
2	O
(	O
Radford	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
GPT	B-MethodName
-	O
3	O
(	O
Brown	O
et	O
al	O
,	O
2020	O
)	O
,	O
demonstrate	O
an	O
impressive	O
ability	O
to	O
generate	O
fluent	O
text	O
,	O
but	O
their	O
outputs	O
are	O
difficult	O
to	O
control	O
beyond	O
a	O
prompt	O
,	O
and	O
they	O
manifest	O
a	O
tendency	O
to	O
hallucinate	O
facts	O
(	O
Wiseman	O
et	O
al	O
,	O
2017	O
)	O
.	O
Much	O
recent	O
work	O
has	O
thus	O
focused	O
on	O
making	O
such	O
models	O
more	O
controllable	O
(	O
Keskar	O
et	O
al	O
,	O
2019	O
;	O
Hu	O
et	O
al	O
,	O
2017	O
;	O
Zhang	O
et	O
al	O
,	O
2020	O
;	O
Dathathri	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
factually	O
grounded	O
(	O
Guu	O
et	O
al	O
,	O
2020	O
;	O
Liu	O
et	O
al	O
,	O
2018b	O
)	O
.	O
*	O
Work	O
done	O
at	O
Microsoft	O
Research	O
.	O
1	O
All	O
our	O
code	O
(	O
including	O
code	O
to	O
recreate	O
our	O
data	O
)	O
and	O
pre	O
-	O
trained	O
models	O
will	O
be	O
made	O
available	O
at	O
:	O
http://microsoft.com/research/project/	O
interactive	O
-	O
document	O
-	O
generation	O
Most	O
such	O
work	O
only	O
considers	O
a	O
one	O
-	O
shot	O
generation	O
setting	O
.	O
Given	O
a	O
set	O
of	O
inputs	O
,	O
which	O
may	O
be	O
a	O
prompt	O
,	O
a	O
control	O
code	O
(	O
Keskar	O
et	O
al	O
,	O
2019	O
)	O
,	O
or	O
a	O
table	O
of	O
data	O
(	O
Liu	O
et	O
al	O
,	O
2018b	O
)	O
for	O
example	O
,	O
the	O
system	O
generates	O
text	O
in	O
a	O
single	O
step	O
.	O
Humans	O
,	O
though	O
,	O
often	O
produce	O
text	O
through	O
an	O
evolutionary	O
process	O
involving	O
multiple	O
draft	O
-	O
edit	O
cycles	O
.	O
This	O
is	O
not	O
simply	O
because	O
they	O
make	O
mistakes	O
when	O
writing	O
,	O
but	O
because	O
they	O
may	O
require	O
multiple	O
iterations	O
to	O
help	O
them	O
shape	O
and	O
even	O
make	O
sense	O
of	O
what	O
they	O
want	O
to	O
express	O
(	O
Pirolli	O
and	O
Card	O
,	O
2005	O
)	O
.	O
For	O
example	O
,	O
consider	O
a	O
user	O
writing	O
an	O
article	O
about	O
Barack	O
Obama	O
.	O
They	O
might	O
start	O
with	O
a	O
simple	O
sentence	O
such	O
as	O
"	O
Barack	O
Obama	O
was	O
the	O
44th	O
President	O
of	O
the	O
United	O
States	O
"	O
.	O
Next	O
,	O
they	O
may	O
wish	O
to	O
expand	O
on	O
that	O
sentence	O
,	O
adding	O
information	O
,	O
or	O
rephrasing	O
it	O
to	O
integrate	O
it	O
better	O
with	O
the	O
text	O
.	O
Replicating	O
this	O
process	O
in	O
software	O
will	O
mean	O
allowing	O
users	O
to	O
adjust	O
their	O
requirements	O
in	O
response	O
to	O
model	O
outputs	O
.	O
Even	O
an	O
error	O
-	O
free	O
system	O
that	O
meets	O
all	O
of	O
a	O
user	O
's	O
initial	O
requirements	O
does	O
not	O
obviate	O
the	O
need	O
for	O
iteration	O
,	O
since	O
those	O
constraints	O
are	O
themselves	O
dynamic	O
.	O
While	O
this	O
work	O
focuses	O
on	O
text	O
,	O
we	O
also	O
note	O
that	O
these	O
arguments	O
extend	O
to	O
other	O
settings	O
where	O
a	O
system	O
must	O
generate	O
a	O
complex	O
,	O
structured	O
object	O
for	O
a	O
user	O
,	O
such	O
as	O
image	O
or	O
code	B-TaskName
generation	I-TaskName
.	O
The	O
purpose	O
of	O
this	O
paper	O
is	O
to	O
bring	O
into	O
view	O
the	O
task	O
of	O
controllable	O
text	O
editing	O
,	O
as	O
a	O
step	O
beyond	O
one	O
-	O
shot	O
generation	O
towards	O
interactive	O
document	O
generation	O
.	O
A	O
full	O
interactive	O
document	O
generation	O
system	O
will	O
likely	O
comprise	O
multiple	O
components	O
,	O
possibly	O
including	O
one	O
-	O
shot	O
generation	O
to	O
create	O
a	O
first	O
draft	O
.	O
Editing	O
is	O
crucial	O
to	O
interactivity	O
because	O
it	O
allows	O
users	O
to	O
change	O
previously	O
generated	O
text	O
to	O
fit	O
their	O
dynamic	O
constraints	O
.	O
This	O
is	O
a	O
stateful	O
operation	O
,	O
where	O
the	O
state	O
is	O
the	O
current	O
version	O
of	O
the	O
document	O
,	O
as	O
opposed	O
to	O
stateless	O
recasting	O
of	O
text	O
from	O
scratch	O
using	O
a	O
one	O
-	O
shot	O
model	O
.	O
While	O
services	O
like	O
Grammarly	O
or	O
MS	O
Word	O
already	O
offer	O
rewriting	O
suggestions	O
,	O
they	O
mainly	O
focus	O
on	O
syntactic	O
or	O
stylistic	O
edits	O
such	O
as	O
paraphrases	O
(	O
Gupta	O
et	O
al	O
,	O
2018	O
)	O
.	O
In	O
this	O
work	O
,	O
we	O
are	O
interested	O
in	O
a	O
broader	O
range	O
of	O
edits	O
,	O
particularly	O
those	O
that	O
add	O
or	O
remove	O
content	O
,	O
or	O
change	O
the	O
meaning	O
of	O
text	O
.	O
Figure	O
1	O
illustrates	O
this	O
editing	O
setting	O
with	O
an	O
example	O
from	O
our	O
trained	O
model	O
,	O
where	O
a	O
user	O
produces	O
a	O
sentence	O
about	O
Barack	O
Obama	O
over	O
multiple	O
edits	O
.	O
In	O
sum	O
,	O
we	O
make	O
the	O
following	O
contributions	O
:	O
We	O
introduce	O
a	O
challenging	O
new	O
text	O
editing	O
task	O
,	O
wherein	O
a	O
model	O
must	O
learn	O
to	O
edit	O
text	O
in	O
response	O
to	O
a	O
user	O
command	O
,	O
while	O
drawing	O
on	O
grounding	O
to	O
avoid	O
problems	O
of	O
hallucination	O
(	O
Wiseman	O
et	O
al	O
,	O
2017	O
)	O
.	O
To	O
accompany	O
this	O
task	O
,	O
we	O
release	O
an	O
open	O
-	O
source	O
dataset	O
of	O
sentence	O
-	O
level	O
edits	O
extracted	O
from	O
Wikipedia	O
,	O
including	O
editor	O
comments	O
,	O
which	O
we	O
leverage	O
as	O
natural	O
language	O
commands	O
,	O
together	O
with	O
pre	O
-	O
retrieved	O
grounding	O
documents	O
.	O
We	O
show	O
that	O
a	O
transformer	O
-	O
based	O
editing	O
model	O
trained	O
on	O
our	O
data	O
outperforms	O
"	O
parrot	B-MethodName
"	O
and	O
GPT	B-MethodName
-	O
2	O
baselines	O
,	O
and	O
obtains	O
competitive	O
results	O
compared	O
to	O
gold	O
-	O
standard	O
edits	O
in	O
human	O
evaluations	O
.	O
We	O
then	O
perform	O
an	O
empirical	O
analysis	O
of	O
our	O
model	O
's	O
performance	O
,	O
showing	O
the	O
importance	O
of	O
the	O
command	O
and	O
grounding	O
,	O
and	O
the	O
varying	O
difficulty	O
of	O
edits	O
in	O
our	O
dataset	O
.	O

We	O
now	O
formalize	O
our	O
text	O
editing	O
task	O
.	O
Let	O
D	O
be	O
a	O
document	O
,	O
q	O
a	O
user	O
command	O
2	O
,	O
and	O
G	O
some	O
appropriate	O
form	O
of	O
grounding	O
.	O
Moreover	O
,	O
let	O
D	O
be	O
an	O
edited	O
version	O
of	O
D.	O
Then	O
our	O
task	O
is	O
,	O
given	O
a	O
dataset	O
of	O
edits	O
D	O
=	O
{	O
(	O
D	O
0	B-DatasetName
,	O
q	O
0	B-DatasetName
,	O
G	O
0	B-DatasetName
,	O
D	O
0	B-DatasetName
)	O
,	O
...	O
,	O
(	O
D	O
N	O
,	O
q	O
N	O
,	O
G	O
N	O
,	O
D	O
N	O
)	O
}	O
,	O
learn	O
to	O
produce	O
document	O
D	O
,	O
given	O
D	O
,	O
q	O
,	O
and	O
G.	O
Note	O
that	O
while	O
previous	O
work	O
on	O
text	O
editing	O
usually	O
only	O
considers	O
D	O
as	O
input	O
,	O
we	O
include	O
both	O
a	O
form	O
of	O
control	O
q	O
and	O
grounding	O
G.	O
The	O
command	O
is	O
needed	O
because	O
otherwise	O
the	O
type	O
of	O
edit	O
to	O
be	O
made	O
is	O
undefined	O
,	O
while	O
the	O
grounding	O
provides	O
external	O
knowledge	O
needed	O
to	O
make	O
an	O
edit	O
.	O
In	O
our	O
specific	O
instance	O
of	O
this	O
task	O
,	O
we	O
will	O
only	O
consider	O
sentence	O
-	O
level	O
edits	O
.	O
More	O
formally	O
,	O
we	O
consider	O
edits	O
D	O
−	O
D	O
,	O
where	O
D	O
and	O
D	O
differ	O
only	O
on	O
a	O
single	O
sentence	O
s	O
D	O
,	O
respectively	O
s	O
D	O
.	O
While	O
,	O
in	O
general	O
,	O
edits	O
can	O
vary	O
in	O
complexity	O
from	O
document	O
-	O
level	O
to	O
character	O
-	O
level	O
changes	O
,	O
sentences	O
are	O
a	O
natural	O
way	O
to	O
break	O
down	O
text	O
into	O
relatively	O
independent	O
units	O
of	O
meaning	O
,	O
so	O
it	O
makes	O
sense	O
to	O
edit	O
text	O
one	O
sentence	O
at	O
a	O
time	O
.	O
More	O
complex	O
,	O
document	O
-	O
level	O
edits	O
can	O
be	O
seen	O
as	O
a	O
composition	O
of	O
multiple	O
sentence	O
-	O
level	O
edits	O
.	O
Additionally	O
,	O
we	O
will	O
consider	O
user	O
commands	O
q	O
written	O
in	O
natural	O
language	O
,	O
e.g.	O
,	O
"	O
add	O
years	O
in	O
office	O
"	O
.	O
The	O
command	O
could	O
also	O
take	O
other	O
forms	O
,	O
such	O
as	O
a	O
categorical	O
variable	O
,	O
but	O
natural	O
language	O
allows	O
for	O
the	O
greatest	O
flexibility	O
in	O
specifying	O
what	O
the	O
edit	O
should	O
accomplish	O
.	O
Moreover	O
,	O
natural	O
language	O
commands	O
are	O
a	O
good	O
fit	O
for	O
our	O
model	O
,	O
which	O
we	O
will	O
initialize	O
with	O
pretrained	O
language	O
model	O
weights	O
.	O
For	O
similar	O
reasons	O
,	O
we	O
will	O
also	O
consider	O
corpora	O
of	O
text	O
snippets	O
as	O
our	O
grounding	O
G.	O
Alternatively	O
,	O
the	O
grounding	O
could	O
also	O
consist	O
of	O
structured	O
data	O
such	O
as	O
tables	O
or	O
graphs	O
.	O
In	O
a	O
real	O
user	O
scenario	O
,	O
this	O
grounding	O
might	O
be	O
supplied	O
by	O
the	O
user	O
,	O
or	O
retrieved	O
on	O
the	O
fly	O
.	O
For	O
our	O
dataset	O
,	O
we	O
pre	O
-	O
retrieve	O
groundings	O
by	O
querying	O
a	O
commercial	O
search	O
engine	O
.	O

We	O
now	O
provide	O
an	O
overview	O
of	O
our	O
dataset	O
.	O
From	O
667	O
dump	O
files	O
in	O
the	O
February	O
1	O
st	O
2020	O
dump	O
of	O
Wikipedia	O
,	O
we	O
extract	O
11	O
,	O
850	O
,	O
786	O
edits	O
,	O
and	O
take	O
a	O
1	O
%	O
sample	O
of	O
118	O
,	O
818	O
edits	O
to	O
run	O
our	O
analyses	O
.	O
Table	O
1	O
presents	O
summary	O
statistics	O
for	O
our	O
data	O
,	O
and	O
in	O
the	O
following	O
,	O
we	O
break	O
down	O
the	O
edits	O
by	O
edit	O
type	O
,	O
and	O
present	O
some	O
examples	O
.	O
See	O
also	O
appendix	O
D	O
for	O
an	O
analysis	O
of	O
the	O
quality	O
of	O
the	O
retrieved	O
grounding	O
.	O
Fluency	O
and	O
Content	O
Edits	O
We	O
are	O
interested	O
in	O
the	O
distribution	O
of	O
different	O
edit	O
types	O
within	O
our	O
dataset	O
.	O
In	O
particular	O
,	O
we	O
want	O
to	O
distinguish	O
between	O
fluency	O
edits	O
,	O
which	O
only	O
affect	O
the	O
grammar	O
or	O
structure	O
of	O
a	O
sentence	O
,	O
and	O
content	O
edits	O
,	O
which	O
change	O
the	O
meaning	O
of	O
a	O
sentence	O
.	O
We	O
can	O
lean	O
on	O
previous	O
work	O
to	O
categorize	O
edits	O
on	O
Wikipedia	O
.	O
create	O
13	O
edit	O
intention	O
categories	O
,	O
and	O
train	O
a	O
classifier	O
to	O
label	O
revisions	O
according	O
to	O
the	O
categories	O
.	O
We	O
apply	O
their	O
classifier	O
to	O
our	O
data	O
,	O
and	O
group	O
their	O
13	O
categories	O
into	O
"	O
fluency	O
"	O
,	O
"	O
content	O
"	O
,	O
or	O
"	O
other	O
"	O
edits	O
,	O
as	O
reported	O
in	O
table	O
2	O
.	O
With	O
the	O
caveat	O
that	O
the	O
edits	O
were	O
labelled	O
automatically	O
using	O
a	O
trained	O
classifier	O
,	O
we	O
see	O
that	O
,	O
while	O
fluency	O
edits	O
make	O
up	O
the	O
majority	O
of	O
the	O
edits	O
in	O
our	O
data	O
,	O
a	O
large	O
proportion	O
are	O
content	O
edits	O
.	O
Examples	O
Table	O
3	O
presents	O
some	O
examples	O
from	O
our	O
data	O
.	O
These	O
were	O
chosen	O
to	O
illustrate	O
a	O
variety	O
of	O
edits	O
.	O
The	O
first	O
example	O
shows	O
an	O
elaboration	O
edit	O
,	O
appending	O
new	O
information	O
to	O
the	O
end	O
of	O
a	O
sentence	O
.	O
The	O
second	O
example	O
is	O
a	O
simple	O
typo	O
fix	O
,	O
while	O
the	O
third	O
is	O
changing	O
a	O
fact	O
.	O
Finally	O
,	O
the	O
last	O
example	O
is	O
a	O
more	O
complex	O
edit	O
to	O
reword	O
a	O
sentence	O
.	O
We	O
can	O
see	O
that	O
there	O
is	O
a	O
large	O
variety	O
of	O
edits	O
in	O
our	O
dataset	O
.	O
See	O
T5	B-MethodName
Encoder	O
<	O
bos	O
>	O
0	B-DatasetName
′	O
1	O
′	O
T5	B-MethodName
Decoder	O
0	B-DatasetName
′	O
1	O
′	O
2	O
′	O
…	O
…	O

We	O
conducted	O
two	O
rounds	O
of	O
human	O
evaluations	O
,	O
each	O
time	O
across	O
200	O
examples	O
from	O
our	O
test	O
set	O
.	O
Annotators	O
were	O
crowd	O
sourced	O
,	O
and	O
each	O
example	O
was	O
rated	O
by	O
seven	O
judges	O
for	O
a	O
total	O
of	O
1400	O
judgements	O
.	O
8	O
Command	O
and	O
Grounding	O
In	O
our	O
first	O
round	O
of	O
human	O
evaluations	O
we	O
compared	O
our	O
model	O
's	O
top	O
output	O
from	O
beam	O
search	O
to	O
the	O
reference	O
edit	O
.	O
There	O
were	O
two	O
tasks	O
.	O
In	O
the	O
first	O
task	O
,	O
we	O
asked	O
judges	O
to	O
choose	O
which	O
system	O
better	O
accomplished	O
the	O
command	O
q.	O
In	O
the	O
second	O
,	O
we	O
asked	O
which	O
system	O
was	O
more	O
faithful	O
to	O
the	O
grounding	O
G.	O
8	O
:	O
Human	O
Evaluation	O
:	O
comparisons	O
between	O
absolute	O
evaluations	O
of	O
different	O
settings	O
.	O
Raters	O
were	O
asked	O
whether	O
edits	O
were	O
satisfactory	O
.	O
0	B-DatasetName
corresponds	O
to	O
strong	O
disagreement	O
,	O
and	O
5	O
to	O
strong	O
agreement	O
.	O
Systems	O
are	O
given	O
by	O
model	O
(	O
full	O
or	O
with	O
the	O
comment	O
ablated	O
)	O
,	O
and	O
whether	O
the	O
command	O
was	O
shown	O
to	O
the	O
raters	O
(	O
+	O
or	O
-	O
)	O
.	O
Bolded	O
numbers	O
indicate	O
significant	O
difference	O
with	O
p	O
<	O
0.0125	O
.	O
than	O
the	O
reference	O
.	O
9	O
In	O
the	O
grounding	O
task	O
,	O
Interactive	O
Editor	O
demonstrates	O
good	O
correspondence	O
with	O
the	O
background	O
material	O
.	O
10	O
Judges	O
were	O
further	O
asked	O
whether	O
the	O
retrieved	O
grounding	O
was	O
relevant	O
to	O
the	O
context	O
D	O
:	O
92.86	O
%	O
of	O
judgments	O
recorded	O
the	O
grounding	O
as	O
either	O
"	O
Somewhat	O
relevant	O
"	O
or	O
"	O
Very	O
relevant	O
"	O
.	O

Grounded	O
Generation	O
Large	O
language	O
models	O
can	O
generate	O
fluent	O
text	O
(	O
Radford	O
et	O
al	O
,	O
2019	O
;	O
Brown	O
et	O
al	O
,	O
2020	O
;	O
Raffel	O
et	O
al	O
,	O
2020	O
)	O
,	O
but	O
they	O
have	O
a	O
tendency	O
to	O
hallucinate	O
facts	O
(	O
Wiseman	O
et	O
al	O
,	O
2017	O
)	O
.	O
Thus	O
,	O
several	O
works	O
have	O
explored	O
using	O
various	O
forms	O
of	O
grounding	O
to	O
enable	O
models	O
to	O
generate	O
factually	O
consistent	O
texts	O
(	O
Koncel	O
-	O
Kedziorski	O
et	O
al	O
,	O
2019	O
;	O
Liu	O
et	O
al	O
,	O
2018b	O
;	O
Prabhumoye	O
et	O
al	O
,	O
2019	O
;	O
Liu	O
et	O
al	O
,	O
2018a	O
;	O
Guu	O
et	O
al	O
,	O
2020	O
)	O
.	O
Our	O
work	O
uses	O
grounding	O
to	O
ensure	O
that	O
edits	O
are	O
factually	O
correct	O
,	O
although	O
our	O
task	O
differs	O
from	O
previous	O
work	O
because	O
of	O
the	O
user	O
command	O
,	O
which	O
requires	O
specific	O
information	O
to	O
be	O
retrieved	O
from	O
the	O
grounding	O
during	O
generation	O
.	O
Controllable	O
Generation	O
While	O
grounding	O
can	O
be	O
seen	O
as	O
a	O
way	O
to	O
implicitly	O
control	O
the	O
contents	O
of	O
generated	O
text	O
,	O
other	O
works	O
have	O
explored	O
more	O
explicit	O
forms	O
of	O
control	O
.	O
Hokamp	O
and	O
Liu	O
(	O
2017	O
)	O
and	O
Zhang	O
et	O
al	O
(	O
2020	O
)	O
use	O
lexical	O
constraints	O
,	O
while	O
Keskar	O
et	O
al	O
(	O
2019	O
)	O
and	O
Dathathri	O
et	O
al	O
(	O
2019	O
)	O
control	O
higher	O
level	O
attributes	O
of	O
text	O
,	O
such	O
as	O
style	O
,	O
tone	O
,	O
or	O
topic	O
.	O
Our	O
task	O
instead	O
uses	O
natural	O
language	O
commands	O
,	O
which	O
can	O
flexibly	O
express	O
different	O
types	O
of	O
constraints	O
,	O
ranging	O
from	O
low	O
-	O
level	O
lexical	O
ones	O
,	O
to	O
high	O
-	O
level	O
topical	O
ones	O
.	O
In	O
this	O
sense	O
,	O
we	O
can	O
also	O
draw	O
the	O
parallel	O
to	O
dialog	O
response	B-TaskName
generation	I-TaskName
Dinan	O
et	O
al	O
,	O
2018	O
)	O
,	O
task	O
-	O
oriented	O
dialog	O
,	O
or	O
open	O
domain	O
question	B-TaskName
answering	I-TaskName
(	O
Min	O
et	O
al	O
,	O
2019	O
;	O
Chen	O
et	O
al	O
,	O
2017	O
)	O
,	O
that	O
also	O
involve	O
user	O
responses	O
or	O
queries	O
,	O
although	O
these	O
tasks	O
are	O
not	O
concerned	O
with	O
text	B-TaskName
generation	I-TaskName
in	O
the	O
context	O
of	O
document	O
creation	O
.	O

The	O
task	O
of	O
Document	O
Generation	O
considered	O
in	O
our	O
work	O
bears	O
similarity	O
with	O
work	O
on	O
generating	O
long	O
-	O
form	O
narratives	O
(	O
Jain	O
et	O
al	O
,	O
2017	O
)	O
.	O
While	O
earlier	O
work	O
in	O
Story	B-TaskName
Generation	I-TaskName
focused	O
more	O
on	O
plan	O
-	O
based	O
architectures	O
(	O
Lebowitz	O
,	O
1985	O
)	O
,	O
more	O
recent	O
work	O
moved	O
towards	O
end	O
-	O
to	O
-	O
end	O
approaches	O
allowing	O
generation	O
to	O
be	O
unconstrained	O
and	O
creative	O
.	O
As	O
narratives	O
are	O
often	O
aimed	O
at	O
particular	O
goals	O
expressed	O
in	O
terms	O
of	O
outlines	O
and	O
plans	O
,	O
much	O
of	O
the	O
literature	O
in	O
Story	B-TaskName
Generation	I-TaskName
is	O
framed	O
as	O
a	O
form	O
of	O
controllable	O
generation	O
,	O
using	O
storylines	O
(	O
Peng	O
et	O
al	O
,	O
2018	O
)	O
,	O
events	O
(	O
Martin	O
et	O
al	O
,	O
2017	O
,	O
plot	O
words	O
or	O
word	O
skeletons	O
(	O
Xu	O
et	O
al	O
,	O
2018	O
;	O
Ippolito	O
et	O
al	O
,	O
2019	O
)	O
,	O
plans	O
(	O
Yao	O
et	O
al	O
,	O
2019	O
)	O
,	O
story	O
ending	O
(	O
Tambwekar	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
outlines	O
(	O
Rashkin	O
et	O
al	O
,	O
2020	O
)	O
as	O
various	O
forms	O
of	O
constraints	O
.	O
Our	O
work	O
takes	O
a	O
significantly	O
different	O
approach	O
,	O
as	O
we	O
treat	O
document	O
or	O
story	B-TaskName
generation	I-TaskName
as	O
an	O
iterative	O
process	O
that	O
allows	O
a	O
human	O
to	O
generate	O
a	O
full	O
document	O
from	O
scratch	O
,	O
but	O
also	O
allows	O
constraints	O
to	O
be	O
more	O
dynamic	O
(	O
e.g.	O
,	O
add	O
nationality	O
in	O
Table	O
9	O
only	O
if	O
the	O
system	O
missed	O
that	O
the	O
first	O
time	O
)	O
.	O
Text	O
Editing	O
Several	O
previous	O
works	O
have	O
focused	O
on	O
text	O
editing	O
.	O
Guu	O
et	O
al	O
(	O
2018	O
)	O
generate	O
sentences	O
by	O
editing	O
prototypes	O
taken	O
from	O
their	O
training	O
corpus	O
,	O
although	O
they	O
use	O
editing	O
only	O
as	O
a	O
means	O
for	O
language	O
modeling	O
.	O
expand	O
upon	O
Guu	O
et	O
al	O
(	O
2018	O
)	O
's	O
setting	O
,	O
but	O
for	O
dialog	O
.	O
More	O
related	O
to	O
our	O
own	O
setting	O
,	O
Faruqui	O
et	O
al	O
(	O
2018	O
)	O
propose	O
WikiAtomicEdits	B-DatasetName
,	O
a	O
dataset	O
of	O
edits	O
crawled	O
from	O
Wikipedia	O
.	O
However	O
,	O
they	O
consider	O
a	O
much	O
narrower	O
definition	O
of	O
edits	O
than	O
our	O
data	O
does	O
.	O
Yin	O
et	O
al	O
(	O
2018	O
)	O
use	O
WikiAtomicEdits	B-DatasetName
and	O
propose	O
the	O
task	O
of	O
learning	O
to	O
represent	O
edits	O
,	O
which	O
Marrese	O
-	O
Taylor	O
et	O
al	O
(	O
2020	O
)	O
expand	O
using	O
a	O
variational	O
approach	O
.	O
In	O
contrast	O
,	O
we	O
are	O
more	O
interested	O
in	O
generating	O
edits	O
rather	O
than	O
repre	O
-	O
senting	O
them	O
.	O
Related	O
to	O
Wikipedia	O
data	O
,	O
Pryzant	O
et	O
al	O
(	O
2020	O
)	O
also	O
used	O
Wikipedia	O
revision	O
histories	O
to	O
learn	O
to	O
debias	O
text	O
,	O
whereas	O
we	O
considered	O
general	O
edits	O
.	O
Iso	O
et	O
al	O
(	O
2020	O
)	O
propose	O
a	O
factbased	O
text	O
editing	O
task	O
,	O
but	O
they	O
do	O
not	O
consider	O
control	O
or	O
other	O
types	O
of	O
edits	O
.	O
Another	O
related	O
task	O
to	O
text	O
editing	O
is	O
text	O
paraphrasing	O
(	O
Gupta	O
et	O
al	O
,	O
2018	O
)	O
,	O
however	O
paraphrasing	O
usually	O
conserves	O
the	O
meaning	O
of	O
a	O
sentence	O
.	O
While	O
the	O
edits	O
we	O
consider	O
include	O
meaning	O
-	O
preserving	O
edits	O
,	O
we	O
are	O
mostly	O
interested	O
in	O
edits	O
that	O
affect	O
meaning	O
.	O

In	O
this	O
work	O
we	O
argued	O
that	O
text	B-TaskName
generation	I-TaskName
should	O
be	O
interactive	O
,	O
and	O
,	O
as	O
a	O
means	O
towards	O
that	O
end	O
,	O
we	O
proposed	O
a	O
general	O
text	O
editing	O
task	O
,	O
where	O
a	O
system	O
must	O
edit	O
a	O
document	O
in	O
response	O
to	O
a	O
user	O
command	O
.	O
In	O
our	O
specific	O
instance	O
of	O
the	O
task	O
we	O
considered	O
single	O
-	O
sentence	O
edits	O
,	O
and	O
we	O
crawled	O
a	O
dataset	O
of	O
several	O
million	O
edits	O
from	O
Wikipedia	O
that	O
included	O
commands	O
,	O
in	O
the	O
form	O
of	O
editor	O
comments	O
,	O
as	O
well	O
as	O
grounding	O
documents	O
.	O
We	O
then	O
showed	O
that	O
training	O
a	O
transformer	O
-	O
based	O
model	O
on	O
our	O
data	O
,	O
while	O
initializing	O
with	O
pretrained	O
language	O
model	O
weights	O
,	O
yields	O
encouraging	O
results	O
on	O
both	O
automatic	O
and	O
human	O
evaluations	O
.	O
Additionally	O
,	O
our	O
ablation	O
studies	O
showed	O
the	O
crucial	O
role	O
played	O
by	O
the	O
user	O
command	O
and	O
grounding	O
.	O
Breaking	O
down	O
our	O
results	O
by	O
types	O
of	O
edits	O
,	O
we	O
saw	O
that	O
our	O
model	O
not	O
only	O
performs	O
well	O
on	O
easier	O
fluency	O
edits	O
,	O
but	O
also	O
on	O
much	O
harder	O
content	O
edits	O
.	O
Finally	O
,	O
we	O
discussed	O
future	O
research	O
directions	O
for	O
interactive	O
document	O
generation	O
,	O
as	O
well	O
as	O
possible	O
extensions	O
to	O
other	O
domains	O
such	O
as	O
images	O
or	O
code	O
.	O
found	O
that	O
this	O
helps	O
remedy	O
the	O
shortfalls	O
of	O
the	O
markup	O
removal	O
step	O
,	O
since	O
it	O
often	O
leaves	O
behind	O
markup	O
symbols	O
.	O
While	O
there	O
may	O
be	O
valid	O
sentences	O
that	O
use	O
markup	O
punctuation	O
,	O
we	O
do	O
not	O
expect	O
them	O
to	O
make	O
up	O
a	O
significant	O
part	O
of	O
the	O
data	O
,	O
nor	O
do	O
we	O
expect	O
them	O
to	O
be	O
significantly	O
different	O
from	O
regular	O
sentences	O
,	O
except	O
for	O
their	O
use	O
of	O
unusual	O
punctuation	O
.	O

We	O
are	O
also	O
interested	O
in	O
knowing	O
how	O
well	O
edits	O
in	O
the	O
data	O
are	O
covered	O
by	O
the	O
inputs	O
(	O
i.e.	O
D	O
,	O
s	O
,	O
q	O
,	O
or	O
G	O
)	O
,	O
where	O
an	O
edit	O
is	O
well	O
covered	O
if	O
the	O
information	O
necessary	O
to	O
produce	O
the	O
edit	O
appears	O
somewhere	O
in	O
the	O
inputs	O
.	O
To	O
measure	O
coverage	O
we	O
use	O
word	O
recall	O
:	O
how	O
many	O
words	O
that	O
were	O
inserted	O
in	O
an	O
edit	O
also	O
appear	O
in	O
the	O
grounding	O
?	O
However	O
,	O
because	O
simple	O
recall	O
fails	O
to	O
account	O
for	O
synonyms	O
,	O
or	O
the	O
context	O
in	O
which	O
words	O
appear	O
,	O
we	O
use	O
the	O
BERTScore	O
(	O
Zhang	O
et	O
al	O
,	O
2019a	O
)	O
recall	O
.	O
This	O
allows	O
for	O
fuzzy	O
matching	O
between	O
BERT	B-MethodName
embeddings	O
instead	O
of	O
requiring	O
exact	O
word	O
matches	O
.	O
We	O
also	O
use	O
idf	O
scores	O
to	O
weigh	O
words	O
,	O
since	O
we	O
are	O
mostly	O
interested	O
in	O
covering	O
rare	O
words	O
,	O
which	O
are	O
more	O
likely	O
to	O
be	O
meaning	O
-	O
carrying	O
.	O
We	O
can	O
define	O
the	O
BERT	B-MethodName
recall	O
,	O
R	O
BERT	B-MethodName
,	O
for	O
a	O
sentence	O
edit	O
The	O
BERT	B-MethodName
embeddings	O
used	O
to	O
compute	O
R	O
BERT	B-MethodName
were	O
produced	O
using	O
a	O
pretrained	O
BERT	B-MethodName
base	O
model	O
.	O
The	O
idf	O
weights	O
were	O
computed	O
from	O
a	O
sample	O
of	O
500	O
,	O
000	O
Wikipedia	O
pages	O
.	O
In	O
all	O
rows	O
,	O
the	O
considered	O
corpus	O
C	O
corresponds	O
to	O
the	O
grounding	O
.	O
s	O
−	O
s	O
,	O
with	O
respect	O
to	O
some	O
text	O
corpus	O
C	O
as	O
w	O
s	O
\s	O
idf	O
(	O
w	O
)	O
max	O
w	O
C	O
BERT	B-MethodName
(	O
w	O
)	O
T	O
BERT	B-MethodName
(	O
w	O
)	O
w	O
s	O
\s	O
idf	O
(	O
w	O
)	O
,	O
where	O
s	O
\s	O
=	O
{	O
w	O
s	O
|	O
w	O
/	O
s	O
}	O
,	O
and	O
idf	O
(	O
w	O
)	O
are	O
the	O
inverse	O
document	O
frequency	O
scores	O
computed	O
on	O
a	O
random	O
sample	O
of	O
500	O
K	O
Wikipedia	O
pages	O
.	O
Table	O
13	O
reports	O
the	O
coverage	O
statistics	O
for	O
our	O
subsample	O
of	O
the	O
data	O
.	O
We	O
used	O
an	O
uncased	O
BERT	B-MethodName
base	O
model	O
to	O
compute	O
the	O
embeddings	O
.	O
The	O
first	O
row	O
reports	O
the	O
coverage	O
of	O
the	O
target	O
by	O
all	O
of	O
the	O
inputs	O
,	O
namely	O
the	O
command	O
,	O
grounding	O
,	O
context	O
,	O
and	O
source	O
sentence	O
.	O
The	O
second	O
row	O
shows	O
the	O
coverage	O
by	O
the	O
grounding	O
alone	O
.	O
Note	O
that	O
,	O
even	O
with	O
just	O
the	O
grounding	O
,	O
coverage	O
is	O
already	O
fairly	O
high	O
.	O
Finally	O
,	O
the	O
last	O
row	O
presents	O
the	O
coverage	O
by	O
the	O
command	O
alone	O
,	O
which	O
shows	O
that	O
it	O
also	O
provides	O
grounding	O
.	O

Neural	O
machine	B-TaskName
translation	I-TaskName
(	O
NMT	O
)	O
has	O
achieved	O
impressive	O
performance	O
recently	O
by	O
using	O
large	O
-	O
scale	O
parallel	O
corpora	O
.	O
However	O
,	O
it	O
struggles	O
in	O
the	O
low	O
-	O
resource	O
and	O
morphologically	O
-	O
rich	O
scenarios	O
of	O
agglutinative	O
language	O
translation	O
task	O
.	O
Inspired	B-DatasetName
by	O
the	O
finding	O
that	O
monolingual	O
data	O
can	O
greatly	O
improve	O
the	O
NMT	O
performance	O
,	O
we	O
propose	O
a	O
multi	O
-	O
task	O
neural	O
model	O
that	O
jointly	O
learns	O
to	O
perform	O
bi	O
-	O
directional	O
translation	O
and	O
agglutinative	O
language	O
stemming	O
.	O
Our	O
approach	O
employs	O
the	O
shared	O
encoder	O
and	O
decoder	O
to	O
train	O
a	O
single	O
model	O
without	O
changing	O
the	O
standard	O
NMT	O
architecture	O
but	O
instead	O
adding	O
a	O
token	O
before	O
each	O
source	O
-	O
side	O
sentence	O
to	O
specify	O
the	O
desired	O
target	O
outputs	O
of	O
the	O
two	O
different	O
tasks	O
.	O
Experimental	O
results	O
on	O
Turkish	O
-	O
English	O
and	O
Uyghur	O
-	O
Chinese	O
show	O
that	O
our	O
proposed	O
approach	O
can	O
significantly	O
improve	O
the	O
translation	O
performance	O
on	O
agglutinative	O
languages	O
by	O
using	O
a	O
small	O
amount	O
of	O
monolingual	O
data	O
.	O

Neural	O
machine	B-TaskName
translation	I-TaskName
(	O
NMT	O
)	O
has	O
achieved	O
impressive	O
performance	O
on	O
many	O
high	O
-	O
resource	O
machine	B-TaskName
translation	I-TaskName
tasks	O
(	O
Bahdanau	O
et	O
al	O
,	O
2015	O
;	O
Luong	O
et	O
al	O
,	O
2015a	O
;	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
The	O
standard	O
NMT	O
model	O
uses	O
the	O
encoder	O
to	O
map	O
the	O
source	O
sentence	O
to	O
a	O
continuous	O
representation	O
vector	O
,	O
and	O
then	O
it	O
feeds	O
the	O
resulting	O
vector	O
to	O
the	O
decoder	O
to	O
produce	O
the	O
target	O
sentence	O
.	O
However	O
,	O
the	O
NMT	O
model	O
still	O
suffers	O
from	O
the	O
low	O
-	O
resource	O
and	O
morphologically	O
-	O
rich	O
scenarios	O
of	O
agglutinative	O
language	O
translation	O
tasks	O
,	O
such	O
as	O
Turkish	O
-	O
English	O
and	O
Uyghur	O
-	O
Chinese	O
.	O
Both	O
Turkish	O
and	O
Uyghur	O
are	O
agglutinative	O
languages	O
with	O
complex	O
morphology	O
.	O
The	O
morpheme	O
structure	O
of	O
the	O
word	O
can	O
be	O
denoted	O
as	O
:	O
prefix1	O
+	O
…	O
+	O
prefixN	O
+	O
stem	O
+	O
suffix1	O
+	O
…	O
+	O
suffixN	O
(	O
Ablimit	O
et	O
al	O
,	O
2010	O
)	O
.	O
Since	O
the	O
suffixes	O
have	O
many	O
inflected	O
and	O
morphological	O
variants	O
,	O
the	O
vocabulary	O
size	O
of	O
an	O
agglutinative	O
language	O
is	O
considerable	O
even	O
in	O
small	O
-	O
scale	O
training	O
data	O
.	O
Moreover	O
,	O
many	O
words	O
have	O
different	O
morphemes	O
and	O
meanings	O
in	O
different	O
context	O
,	O
which	O
leads	O
to	O
inaccurate	O
translation	O
results	O
.	O
Recently	O
,	O
researchers	O
show	O
their	O
great	O
interest	O
in	O
utilizing	O
monolingual	O
data	O
to	O
further	O
improve	O
the	O
NMT	O
model	O
performance	O
(	O
Cheng	O
et	O
al	O
,	O
2016	O
;	O
Ramachandran	O
et	O
al	O
,	O
2017	O
;	O
Currey	O
et	O
al	O
,	O
2017	O
)	O
.	O
Sennrich	O
et	O
al	O
(	O
2016	O
)	O
pair	O
the	O
target	O
-	O
side	O
monolingual	O
data	O
with	O
automatic	O
back	O
-	O
translation	O
as	O
additional	O
training	O
data	O
to	O
train	O
the	O
NMT	O
model	O
.	O
Zhang	O
and	O
Zong	O
(	O
2016	O
)	O
use	O
the	O
source	O
-	O
side	O
monolingual	O
data	O
and	O
employ	O
the	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
framework	O
for	O
translation	O
and	O
source	O
sentence	O
reordering	O
.	O
modify	O
the	O
decoder	O
to	O
enable	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
for	O
translation	O
and	O
language	O
modeling	O
.	O
However	O
,	O
the	O
above	O
works	O
mainly	O
focus	O
on	O
boosting	O
the	O
translation	O
fluency	O
,	O
and	O
lack	O
the	O
consideration	O
of	O
morphological	O
and	O
linguistic	O
knowledge	O
.	O
Stemming	O
is	O
a	O
morphological	B-TaskName
analysis	I-TaskName
method	O
,	O
which	O
is	O
widely	O
used	O
for	O
information	B-TaskName
retrieval	I-TaskName
tasks	O
(	O
Kishida	O
,	O
2005	O
)	O
.	O
By	O
removing	O
the	O
suffixes	O
in	O
the	O
word	O
,	O
stemming	O
allows	O
the	O
variants	O
of	O
the	O
same	O
word	O
to	O
share	O
representations	O
and	O
reduces	O
data	O
sparseness	O
.	O
We	O
consider	O
that	O
stemming	O
can	O
lead	O
to	O
better	O
generalization	O
on	O
agglutinative	O
languages	O
,	O
which	O
helps	O
NMT	O
to	O
capture	O
the	O
in	O
-	O
depth	O
semantic	O
information	O
.	O
Thus	O
we	O
use	O
stemming	O
as	O
an	O
auxiliary	O
task	O
for	O
agglutinative	O
language	O
translation	O
.	O
In	O
this	O
paper	O
,	O
we	O
investigate	O
a	O
method	O
to	O
exploit	O
the	O
monolingual	O
data	O
of	O
the	O
agglutinative	O
language	O
to	O
enhance	O
the	O
representation	O
ability	O
of	O
the	O
encoder	O
.	O
This	O
is	O
achieved	O
by	O
training	O
a	O
multi	O
-	O
task	O
neural	O
model	O
to	O
jointly	O
perform	O
bi	O
-	O
directional	O
translation	O
and	O
agglutinative	O
language	O
stemming	O
,	O
which	O
utilizes	O
the	O
shared	O
encoder	O
and	O
decoder	O
.	O
We	O
treat	O
stemming	O
as	O
a	O
sequence	O
generation	O
task	O
.	O
Figure	O
1	O
:	O
The	O
architecture	O
of	O
the	O
multi	O
-	O
task	O
neural	O
model	O
that	O
jointly	O
learns	O
to	O
perform	O
bi	O
-	O
directional	O
translation	O
between	O
Turkish	O
and	O
English	O
,	O
and	O
stemming	O
for	O
Turkish	O
sentence	O
.	O

Multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
(	O
MTL	O
)	O
aims	O
to	O
improve	O
the	O
generalization	O
performance	O
of	O
a	O
main	O
task	O
by	O
using	O
the	O
other	O
related	O
tasks	O
,	O
which	O
has	O
been	O
successfully	O
applied	O
to	O
various	O
research	O
fields	O
ranging	O
from	O
language	O
(	O
Liu	O
et	O
al	O
,	O
2015	O
;	O
Luong	O
et	O
al	O
,	O
2015a	O
)	O
,	O
vision	O
(	O
Yim	O
et	O
al	O
,	O
2015	O
;	O
Misra	O
et	O
al	O
,	O
2016	O
)	O
,	O
and	O
speech	O
(	O
Chen	O
and	O
Mak	O
,	O
2015	O
;	O
Kim	O
et	O
al	O
,	O
2016	O
)	O
.	O
Many	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
tasks	O
have	O
been	O
chosen	O
as	O
auxiliary	O
task	O
to	O
deal	O
with	O
the	O
increasingly	O
complex	O
tasks	O
.	O
Luong	O
et	O
al	O
(	O
2015b	O
)	O
employ	O
a	O
small	O
amount	O
of	O
data	O
of	O
syntactic	O
parsing	O
and	O
image	O
caption	O
for	O
English	O
-	O
German	O
translation	O
.	O
Hashimoto	O
et	O
al	O
(	O
2017	O
)	O
present	O
a	O
joint	O
MTL	O
model	O
to	O
handle	O
the	O
tasks	O
of	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
(	O
POS	O
)	O
tagging	O
,	O
dependency	B-TaskName
parsing	I-TaskName
,	O
semantic	O
relatedness	O
,	O
and	O
textual	O
entailment	O
for	O
English	O
.	O
Kiperwasser	O
and	O
Ballesteros	O
(	O
2018	O
)	O
utilize	O
the	O
POS	O
tagging	O
and	O
dependency	B-TaskName
parsing	I-TaskName
for	O
English	O
-	O
German	O
machine	B-TaskName
translation	I-TaskName
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
we	O
are	O
the	O
first	O
to	O
incorporate	O
stemming	O
task	O
into	O
MTL	O
framework	O
to	O
further	O
improve	O
the	O
translation	O
performance	O
on	O
agglutinative	O
languages	O
.	O
Recently	O
,	O
several	O
works	O
have	O
combined	O
the	O
MTL	O
method	O
with	O
sequence	O
-	O
to	O
-	O
sequence	O
NMT	O
model	O
for	O
machine	B-TaskName
translation	I-TaskName
tasks	O
.	O
Dong	O
et	O
al	O
(	O
2015	O
)	O
follow	O
a	O
one	O
-	O
to	O
-	O
many	O
setting	O
that	O
utilizes	O
a	O
shared	O
encoder	O
for	O
all	O
the	O
source	O
languages	O
with	O
respective	O
attention	O
mechanisms	O
and	O
multiple	O
decoders	O
for	O
the	O
different	O
target	O
languages	O
.	O
Luong	O
et	O
al	O
(	O
2015b	O
)	O
follow	O
a	O
many	O
-	O
to	O
-	O
many	O
setting	O
that	O
uses	O
multiple	O
encoders	O
and	O
decoders	O
with	O
two	O
separate	O
unsupervised	O
objective	O
functions	O
.	O
Zoph	O
and	O
Knight	O
(	O
2016	O
)	O
follow	O
a	O
many	O
-	O
to	O
-	O
one	O
setting	O
that	O
employs	O
multiple	O
encoders	O
for	O
all	O
the	O
source	O
languages	O
and	O
one	O
decoder	O
for	O
the	O
desired	O
target	O
language	O
.	O
Johnson	O
et	O
al	O
(	O
2017	O
)	O
propose	O
a	O
more	O
simple	O
method	O
in	O
one	O
-	O
to	O
-	O
one	O
setting	O
,	O
which	O
trains	O
a	O
single	O
NMT	O
model	O
with	O
the	O
shared	O
encoder	O
and	O
decoder	O
in	O
order	O
to	O
enable	O
multilingual	O
translation	O
.	O
The	O
method	O
requires	O
no	O
changes	O
to	O
the	O
standard	O
NMT	O
architecture	O
but	O
instead	O
requires	O
adding	O
a	O
token	O
at	O
the	O
beginning	O
of	O
each	O
source	O
sentence	O
to	O
specify	O
the	O
desired	O
target	O
sentence	O
.	O
Inspired	B-DatasetName
by	O
their	O
work	O
,	O
we	O
employ	O
the	O
standard	O
NMT	O
model	O
with	O
one	O
encoder	O
and	O
one	O
decoder	O
for	O
parameter	O
sharing	O
and	O
model	O
generalization	O
.	O
In	O
addition	O
,	O
we	O
build	O
a	O
joint	O
vocabulary	O
on	O
the	O
concatenation	O
of	O
the	O
source	O
-	O
side	O
and	O
target	O
-	O
side	O
words	O
.	O
Several	O
works	O
on	O
morphologically	O
-	O
rich	O
NMT	O
have	O
focused	O
on	O
using	O
morphological	B-TaskName
analysis	I-TaskName
to	O
pre	O
-	O
process	O
the	O
training	O
data	O
(	O
Luong	O
et	O
al	O
,	O
2016	O
;	O
Huck	O
et	O
al	O
,	O
2017	O
;	O
Tawfik	O
et	O
al	O
,	O
2019	O
)	O
.	O
Gulcehre	O
et	O
al	O
(	O
2015	O
)	O
segment	O
each	O
Turkish	O
sentence	O
into	O
a	O
sequence	O
of	O
morpheme	O
units	O
and	O
remove	O
any	O
nonsurface	O
morphemes	O
for	O
Turkish	O
-	O
English	O
translation	O
.	O
Ataman	O
et	O
al	O
(	O
2017	O
)	O
propose	O
a	O
vocabulary	O
reduction	O
method	O
that	O
considers	O
the	O
morphological	O
properties	O
of	O
the	O
agglutinative	O
language	O
,	O
which	O
is	O
based	O
on	O
the	O
unsupervised	O
morphology	O
learning	O
.	O
This	O
work	O
takes	O
inspiration	O
from	O
our	O
previously	O
proposed	O
segmentation	O
method	O
(	O
Pan	O
et	O
al	O
,	O
2020	O
)	O
that	O
segments	O
the	O
word	O
into	O
a	O
sequence	O
of	O
subword	O
units	O
with	O
morpheme	O
structure	O
,	O
which	O
can	O
effectively	O
reduce	O
language	O
complexity	O
.	O

We	O
propose	O
a	O
multi	O
-	O
task	O
neural	O
model	O
for	O
machine	B-TaskName
translation	I-TaskName
from	O
and	O
into	O
a	O
low	O
-	O
resource	O
and	O
morphologically	O
-	O
rich	O
agglutinative	O
language	O
.	O
We	O
train	O
the	O
model	O
to	O
jointly	O
learn	O
to	O
perform	O
both	O
the	O
bi	O
-	O
directional	O
translation	O
task	O
and	O
the	O
stemming	O
task	O
on	O
an	O
agglutinative	O
language	O
by	O
using	O
the	O
standard	O
NMT	O
framework	O
.	O
Moreover	O
,	O
we	O
add	O
an	O
artificial	O
token	O
before	O
each	O
source	O
sentence	O
to	O
specify	O
the	O
desired	O
target	O
outputs	O
for	O
different	O
tasks	O
.	O
The	O
architecture	O
of	O
the	O
proposed	O
model	O
is	O
shown	O
in	O
Figure	O
1	O
.	O
We	O
take	O
the	O
Turkish	O
-	O
English	O
translation	O
task	O
as	O
example	O
.	O
The	O
"	O
<	O
MT	O
>	O
"	O
token	O
denotes	O
the	O
bilingual	O
translation	O
task	O
and	O
the	O
"	O
<	O
ST	O
>	O
"	O
token	O
denotes	O
the	O
stemming	O
task	O
on	O
Turkish	O
sentence	O
.	O

Our	O
proposed	O
multi	O
-	O
task	O
neural	O
model	O
on	O
using	O
the	O
source	O
-	O
side	O
monolingual	O
data	O
for	O
agglutinative	O
language	O
translation	O
task	O
can	O
be	O
applied	O
in	O
any	O
NMT	O
structures	O
with	O
encoder	O
-	O
decoder	O
framework	O
.	O
In	O
this	O
work	O
,	O
we	O
follow	O
the	O
NMT	O
model	O
proposed	O
by	O
Vaswani	O
et	O
al	O
(	O
2017	O
)	O
,	O
which	O
is	O
implemented	O
as	O
Transformer	B-MethodName
.	O
We	O
will	O
briefly	O
summarize	O
it	O
here	O
.	O
Firstly	O
,	O
the	O
Transformer	B-MethodName
model	O
maps	O
the	O
source	O
sequence	O
=	O
(	O
1	O
,	O
…	O
,	O
)	O
and	O
the	O
target	O
sentence	O
=	O
(	O
1	O
,	O
…	O
,	O
)	O
into	O
a	O
word	O
embedding	O
matrix	O
,	O
respectively	O
.	O
Secondly	O
,	O
in	O
order	O
to	O
make	O
use	O
of	O
the	O
word	O
order	O
in	O
the	O
sequence	O
,	O
the	O
above	O
word	O
embedding	O
matrices	O
sum	O
with	O
their	O
positional	O
encoding	O
matrices	O
to	O
generate	O
the	O
source	O
-	O
side	O
and	O
target	O
-	O
side	O
positional	O
embedding	O
matrices	O
.	O
The	O
encoder	O
is	O
composed	O
of	O
a	O
stack	O
of	O
N	O
identical	O
layers	O
.	O
Each	O
layer	O
has	O
two	O
sub	O
-	O
layers	O
consisting	O
of	O
the	O
multi	O
-	O
head	O
self	O
-	O
attention	O
and	O
the	O
fully	O
connected	O
feed	O
-	O
forward	O
network	O
,	O
which	O
maps	O
the	O
source	O
-	O
side	O
positional	O
embedding	O
matrix	O
into	O
a	O
representation	O
vector	O
.	O
The	O
decoder	O
is	O
also	O
composed	O
of	O
a	O
stack	O
of	O
N	O
identical	O
layers	O
.	O
Each	O
layer	O
has	O
three	O
sub	O
-	O
layers	O
:	O
the	O
multi	O
-	O
head	O
self	O
-	O
attention	O
,	O
the	O
multi	B-MethodName
-	I-MethodName
head	I-MethodName
attention	I-MethodName
,	O
and	O
the	O
fully	O
connected	O
feed	O
-	O
forward	O
network	O
.	O
The	O
multi	B-MethodName
-	I-MethodName
head	I-MethodName
attention	I-MethodName
attends	O
to	O
the	O
outputs	O
of	O
the	O
encoder	O
and	O
decoder	O
to	O
generate	O
a	O
context	O
vector	O
.	O
The	O
feed	O
-	O
forward	O
network	O
followed	O
by	O
a	O
linear	B-MethodName
layer	I-MethodName
maps	O
the	O
context	O
vector	O
into	O
a	O
vector	O
with	O
the	O
original	O
space	O
dimension	O
.	O
Finally	O
,	O
the	O
softmax	B-MethodName
function	O
is	O
applied	O
on	O
the	O
vector	O
to	O
predict	O
the	O
target	O
word	O
sequence	O
.	O

The	O
statistics	O
of	O
the	O
training	O
,	O
validation	O
,	O
and	O
test	O
datasets	O
on	O
Turkish	O
-	O
English	O
and	O
Uyghur	O
-	O
Chinese	O
machine	B-TaskName
translation	I-TaskName
tasks	O
are	O
shown	O
in	O
Table	O
1	O
.	O
For	O
the	O
Turkish	O
-	O
English	O
machine	B-TaskName
translation	I-TaskName
,	O
following	O
(	O
Sennrich	O
et	O
al	O
,	O
2015a	O
)	O
,	O
we	O
use	O
the	O
WIT	B-DatasetName
corpus	O
(	O
Cettolo	O
et	O
al	O
,	O
2012	O
)	O
and	O
the	O
SETimes	O
corpus	O
(	O
Tyers	O
and	O
Alperen	O
,	O
2010	O
)	O
as	O
the	O
training	O
dataset	O
,	O
merge	O
the	O
dev2010	O
and	O
tst2010	O
as	O
the	O
validation	B-DatasetName
dataset	I-DatasetName
,	O
and	O
use	O
tst2011	O
,	O
tst2012	O
,	O
tst2013	O
,	O
tst2014	O
from	O
the	O
IWSLT	O
as	O
the	O
test	O
datasets	O
.	O
We	O
also	O
use	O
the	O
talks	O
data	O
from	O
the	O
IWSLT	O
evaluation	O
campaign	O
1	O
in	O
2018	O
and	O
the	O
news	O
data	O
from	O
News	O
Crawl	O
corpora	O
2	O
in	O
2017	O
as	O
external	O
monolingual	O
data	O
for	O
the	O
stemming	O
task	O
on	O
Turkish	O
sentences	O
.	O
For	O
the	O
Uyghur	O
-	O
Chinese	O
machine	B-TaskName
translation	I-TaskName
,	O
we	O
use	O
the	O
news	O
data	O
from	O
the	O
China	O
Workshop	O
on	O
Machine	B-TaskName
Translation	I-TaskName
in	O
2017	O
(	O
CWMT2017	O
)	O
as	O
the	O
training	O
dataset	O
and	O
validation	B-DatasetName
dataset	I-DatasetName
,	O
use	O
the	O
news	O
data	O
from	O
CWMT2015	O
as	O
the	O
test	O
dataset	O
.	O
Each	O
Uyghur	O
sentence	O
has	O
four	O
Chinese	O
reference	O
sentences	O
.	O
Moreover	O
,	O
we	O
use	O
the	O
news	O
data	O
from	O
the	O
Tianshan	O
website	O
3	O
as	O
external	O
monolingual	O
data	O
for	O
the	O
stemming	O
task	O
on	O
Uyghur	O
sentences	O
.	O

We	O
normalize	O
and	O
tokenize	O
the	O
experimental	O
data	O
.	O
We	O
utilize	O
the	O
jieba	O
toolkit	O
4	O
to	O
segment	O
the	O
Chinese	O
sentences	O
,	O
we	O
utilize	O
the	O
Zemberek	O
toolkit	O
5	O
with	O
morphological	B-TaskName
disambiguation	I-TaskName
(	O
Sak	O
et	O
al	O
,	O
2007	O
)	O
and	O
the	O
morphological	B-TaskName
analysis	I-TaskName
tool	O
(	O
Tursun	O
et	O
al	O
,	O
2016	O
)	O
to	O
annotate	O
the	O
morpheme	O
structure	O
of	O
the	O
words	O
in	O
Turkish	O
and	O
Uyghur	O
,	O
respectively	O
.	O
We	O
use	O
our	O
previously	O
proposed	O
morphological	O
segmentation	O
method	O
(	O
Pan	O
et	O
al	O
,	O
2020	O
)	O
,	O
which	O
segments	O
the	O
word	O
into	O
smaller	O
subword	O
units	O
with	O
morpheme	O
structure	O
.	O
Since	O
Turkish	O
and	O
Uyghur	O
only	O
have	O
a	O
few	O
prefixes	O
,	O
we	O
combine	O
the	O
prefixes	O
with	O
stem	O
into	O
the	O
stem	O
unit	O
.	O
As	O
shown	O
in	O
Figure	O
2	O
,	O
the	O
morpheme	O
structure	O
of	O
the	O
Turkish	O
word	O
"	O
hecelerini	O
"	O
(	O
syllables	O
)	O
is	O
:	O
hece	O
+	O
lerini	O
.	O
Then	O
the	O
byte	B-MethodName
pair	I-MethodName
encoding	I-MethodName
(	O
BPE	B-MethodName
)	O
technique	O
(	O
Sennrich	O
et	O
al	O
,	O
2015b	O
)	O
is	O
applied	O
on	O
the	O
stem	O
unit	O
"	O
hece	O
"	O
to	O
segment	O
it	O
into	O
"	O
he@@	O
"	O
and	O
"	O
ce@@	O
"	O
.	O
Thus	O
the	O
Turkish	O
word	O
is	O
segmented	O
into	O
a	O
sequence	O
of	O
subword	O
units	O
:	O
he@@	O
+	O
ce@@	O
+	O
lerini	O
.	O

Training	O
Sentence	O
Samples	O
En	O
-	O
Tr	O
Translation	B-TaskName
<	O
MT	O
>	O
We	O
go	O
through	O
initiation	O
rit@@	O
es	O
.	O
Başla@@	O
ma	O
ritüel@@	O
lerini	O
yaş@@	O
ıyoruz	O
.	O
Tr	O
-	O
En	O
Translation	B-TaskName
<	O
MT	O
>	O
Başla@@	O
ma	O
ritüel@@	O
lerini	O
yaş@@	O
ıyoruz	O
.	O
We	O
go	O
through	O
initiation	O
rit@@	O
es	O
.	O
Turkish	O
Stemming	O
<	O
ST	O
>	O
Başla@@	O
ma	O
ritüel@@	O
lerini	O
yaş@@	O
ıyoruz	O
.	O
Başla@@	O
ritüel@@	O
yaş@@	O
In	O
this	O
paper	O
,	O
we	O
utilize	O
the	O
above	O
morphological	O
segmentation	O
method	O
for	O
our	O
experiments	O
by	O
applying	O
BPE	B-MethodName
on	O
the	O
stem	O
units	O
with	O
15	O
K	O
merge	O
operations	O
for	O
the	O
Turkish	O
words	O
and	O
10	O
K	O
merge	O
operations	O
for	O
the	O
Uyghur	O
words	O
.	O
The	O
standard	O
NMT	O
model	O
trained	O
on	O
this	O
experimental	O
data	O
is	O
denoted	O
as	O
"	O
baseline	O
NMT	O
model	O
"	O
.	O
Moreover	O
,	O
we	O
employ	O
BPE	B-MethodName
to	O
segment	O
the	O
words	O
in	O
English	O
and	O
Chinese	O
by	O
learning	O
separate	O
vocabulary	O
with	O
32	O
K	O
merge	O
operations	O
.	O
Table	O
2	O
shows	O
the	O
training	O
sentence	O
samples	O
for	O
multi	O
-	O
task	O
neural	O
model	O
on	O
Turkish	O
-	O
English	O
machine	B-TaskName
translation	I-TaskName
task	O
.	O
In	O
addition	O
,	O
to	O
certify	O
the	O
effectiveness	O
of	O
the	O
morphological	O
segmentation	O
method	O
,	O
we	O
employ	O
the	O
pure	O
BPE	B-MethodName
to	O
segment	O
the	O
words	O
in	O
Turkish	O
and	O
Uyghur	O
by	O
learning	O
a	O
separate	O
vocabulary	O
with	O
36	O
K	O
and	O
38	O
K	O
merge	O
operations	O
,	O
respectively	O
.	O
The	O
standard	O
NMT	O
model	O
trained	O
on	O
this	O
experimental	O
data	O
is	O
denoted	O
as	O
"	O
general	O
NMT	O
model	O
"	O
.	O
Table	O
3	O
shows	O
the	O
detailed	O
statistics	O
of	O
using	O
different	O
word	O
segmentation	O
methods	O
on	O
Turkish	O
,	O
English	O
,	O
Uyghur	O
,	O
and	O
Chinese	O
.	O
The	O
"	O
Vocab	O
"	O
token	O
denotes	O
the	O
vocabulary	O
size	O
after	O
data	O
preprocessing	O
.	O
The	O
"	O
Avg	O
.	O
Len	O
"	O
token	O
denotes	O
the	O
average	O
sentence	O
length	O
.	O

In	O
this	O
paper	O
,	O
we	O
select	O
4	O
neural	O
translation	O
models	O
for	O
comparison	O
.	O
More	O
details	O
about	O
the	O
models	O
are	O
shown	O
below	O
:	O
General	B-DatasetName
NMT	O
Model	O
:	O
The	O
standard	O
NMT	O
model	O
trained	O
on	O
the	O
experimental	O
data	O
segmented	O
by	O
BPE	B-MethodName
.	O
Baseline	O
NMT	O
Model	O
:	O
The	O
standard	O
NMT	O
model	O
trained	O
on	O
the	O
experimental	O
data	O
segmented	O
by	O
morphological	O
segmentation	O
.	O
The	O
following	O
models	O
also	O
use	O
this	O
word	O
segmentation	O
method	O
.	O
Bi	O
-	O
Directional	O
NMT	O
Model	O
:	O
Following	O
Niu	O
et	O
al	O
(	O
2018b	O
)	O
,	O
we	O
train	O
a	O
single	O
NMT	O
model	O
to	O
perform	O
bi	O
-	O
directional	O
machine	B-TaskName
translation	I-TaskName
.	O
We	O
concatenate	O
the	O
bilingual	O
parallel	O
sentences	O
in	O
both	O
directions	O
.	O
Since	O
the	O
source	O
and	O
target	O
sentences	O
come	O
from	O
the	O
same	O
language	O
pairs	O
,	O
we	O
share	O
the	O
source	O
and	O
target	O
vocabulary	O
,	O
and	O
tie	O
their	O
word	O
embedding	O
during	O
model	O
training	O
.	O
Multi	O
-	O
Task	O
Neural	O
Model	O
:	O
We	O
simply	O
use	O
the	O
monolingual	O
data	O
of	O
the	O
agglutinative	O
language	O
from	O
the	O
bilingual	O
parallel	O
sentences	O
.	O
We	O
use	O
a	O
joint	O
vocabulary	O
,	O
tie	O
the	O
word	O
embedding	O
as	O
well	O
as	O
the	O
output	O
layer	O
's	O
weight	O
matrix	O
.	O

Many	O
areas	O
of	O
natural	O
language	O
processing	O
have	O
benefited	O
from	O
the	O
existence	O
of	O
tools	O
and	O
frameworks	O
that	O
can	O
be	O
customized	O
to	O
develop	O
specific	O
applications	O
.	O
In	O
the	O
area	O
of	O
dialogue	O
systems	O
,	O
there	O
are	O
few	O
such	O
tools	O
and	O
frameworks	O
and	O
they	O
mostly	O
remain	O
focused	O
on	O
simple	O
tasks	O
that	O
can	O
be	O
encoded	O
in	O
a	O
state	O
-	O
based	O
dialogue	O
model	O
(	O
see	O
,	O
e.g.	O
,	O
Williams	O
et	O
al	O
,	O
2016	O
and	O
the	O
Dialogue	B-DatasetName
State	I-DatasetName
Tracking	I-DatasetName
Challenge	I-DatasetName
1	O
)	O
.	O
In	O
this	O
category	O
some	O
of	O
the	O
more	O
expressive	O
approaches	O
to	O
dialogue	O
modeling	O
are	O
based	O
on	O
the	O
information	O
state	O
(	O
Cooper	O
,	O
1997	O
)	O
;	O
notable	O
toolkits	O
include	O
TrindiKit	O
(	O
Larsson	O
and	O
Traum	O
,	O
2000	O
)	O
and	O
its	O
open	O
-	O
source	O
successor	O
trindikit.py	O
(	O
Ljunglöf	O
,	O
2009	O
)	O
,	O
and	O
OpenDial	O
(	O
Lison	O
and	O
Kennington	O
,	O
2016	O
)	O
.	O
Unfortunately	O
,	O
there	O
is	O
a	O
dearth	O
of	O
tools	O
for	O
developing	O
mixed	O
-	O
initiative	O
dialogue	O
systems	O
that	O
involve	O
complex	O
back	O
-	O
end	O
reasoning	O
systems	O
.	O
Early	O
theoretical	O
work	O
of	O
SharedPlans	O
(	O
Grosz	O
and	O
Kraus	O
,	O
1996	O
;	O
Lochbaum	O
et	O
al	O
,	O
1990	O
)	O
and	O
planbased	O
dialogue	O
systems	O
(	O
e.g.	O
,	O
Allen	O
and	O
Perrault	O
,	O
1980	O
;	O
Litman	O
and	O
Allen	O
,	O
1987	O
)	O
laid	O
good	O
foundations	O
.	O
The	O
Collaborative	O
Problem	O
Solving	O
(	O
CPS	O
)	O
model	O
(	O
Allen	O
et	O
al	O
,	O
2002	O
)	O
seemed	O
to	O
promise	O
a	O
solution	O
but	O
that	O
model	O
has	O
never	O
been	O
implemented	O
in	O
a	O
truly	O
domain	O
-	O
independent	O
way	O
.	O
Ravenclaw	O
(	O
Bohus	O
and	O
Rudnicky	O
,	O
2009	O
)	O
is	O
a	O
plan	O
-	O
based	O
dialog	O
management	O
framework	O
that	O
has	O
been	O
used	O
to	O
develop	O
a	O
number	O
of	O
dialogue	O
systems	O
.	O
Its	O
dialogue	O
engine	O
is	O
task	O
-	O
independent	O
and	O
includes	O
a	O
number	O
of	O
generic	O
conversational	O
skills	O
;	O
however	O
,	O
its	O
behavior	O
is	O
driven	O
by	O
task	O
-	O
specific	O
dialogue	O
trees	O
,	O
which	O
have	O
to	O
be	O
implemented	O
anew	O
for	O
every	O
application	O
.	O
Dialogue	B-TaskName
management	I-TaskName
involves	O
understanding	O
the	O
intention	O
of	O
the	O
user	O
's	O
contributions	O
to	O
the	O
dialogue	O
,	O
and	O
deciding	O
what	O
to	O
do	O
or	O
say	O
next	O
.	O
It	O
is	O
the	O
core	O
component	O
of	O
a	O
dialogue	O
system	O
,	O
and	O
typically	O
requires	O
significant	O
development	O
effort	O
for	O
every	O
new	O
application	O
domain	O
.	O
We	O
believe	O
that	O
dialogue	O
managers	O
based	O
on	O
models	O
of	O
the	O
collaborative	O
problem	O
solving	O
process	O
offer	O
the	O
highest	O
potential	O
for	O
flexibility	O
and	O
portability	O
.	O
Flexibility	O
refers	O
to	O
the	O
ability	O
to	O
cover	O
the	O
full	O
range	O
of	O
natural	O
dialogues	O
users	O
may	O
want	O
to	O
engage	O
in	O
,	O
and	O
portability	O
refers	O
to	O
how	O
easy	O
it	O
is	O
to	O
customize	O
or	O
modify	O
a	O
system	O
to	O
work	O
in	O
new	O
domains	O
(	O
Blaylock	O
,	O
2007	O
)	O
.	O
In	O
this	O
paper	O
we	O
describe	O
a	O
new	O
,	O
domainindependent	O
dialogue	O
manager	O
based	O
on	O
the	O
CPS	O
model	O
,	O
and	O
its	O
implementation	O
in	O
an	O
open	O
-	O
source	O
dialog	O
system	O
shell	O
(	O
Cogent	O
2	O
)	O
.	O
To	O
demonstrate	O
its	O
flexibility	O
,	O
we	O
also	O
describe	O
briefly	O
a	O
few	O
dialogue	O
systems	O
for	O
different	O
domains	O
.	O

When	O
agents	O
are	O
engaged	O
in	O
solving	O
problems	O
together	O
,	O
they	O
need	O
to	O
communicate	O
to	O
agree	O
on	O
what	O
goals	O
to	O
pursue	O
and	O
what	O
steps	O
to	O
take	O
to	O
achieve	O
those	O
goals	O
,	O
negotiate	O
roles	O
,	O
resources	O
,	O
etc	O
.	O
To	O
underscore	O
its	O
collaborative	O
aspect	O
,	O
this	O
type	O
of	O
joint	O
activity	O
has	O
been	O
called	O
Collaborative	O
Problem	O
Solving	O
(	O
CPS	O
)	O
.	O
Modeling	O
the	O
type	O
of	O
dialogue	O
agents	O
engage	O
in	O
during	O
CPS	O
must	O
,	O
therefore	O
,	O
take	O
into	O
account	O
the	O
nature	O
of	O
the	O
joint	O
activity	O
itself	O
.	O
In	O
the	O
early	O
2000s	O
,	O
Allen	O
and	O
colleagues	O
described	O
a	O
preliminary	O
plan	O
-	O
based	O
CPS	O
model	O
of	O
dialogue	O
based	O
on	O
an	O
analysis	O
of	O
an	O
agent	B-DatasetName
's	O
collaborative	O
behavior	O
at	O
various	O
levels	O
:	O
	O
An	O
individual	O
problem	O
-	O
solving	O
level	O
,	O
where	O
each	O
agent	B-DatasetName
manages	O
its	O
own	O
problemsolving	O
state	O
,	O
plans	O
and	O
executes	O
individual	O
actions	O
,	O
etc	O
.	O
;	O
	O
A	O
collaborative	O
problem	O
-	O
solving	O
level	O
,	O
which	O
models	O
and	O
manages	O
the	O
joint	O
or	O
collaborative	O
problem	O
-	O
solving	O
state	O
(	O
shared	O
goals	O
,	O
resources	O
,	O
situations	O
)	O
;	O
	O
An	O
interaction	O
level	O
,	O
where	O
individual	O
agents	O
negotiate	O
changes	O
in	O
the	O
joint	O
problem	O
-	O
solving	O
state	O
;	O
and	O
,	O
finally	O
,	O
	O
A	O
communication	O
level	O
,	O
where	O
speech	O
acts	O
realize	O
the	O
interaction	O
level	O
acts	O
.	O
This	O
model	O
was	O
refined	O
in	O
a	O
series	O
of	O
publications	O
,	O
and	O
several	O
prototype	O
systems	O
were	O
developed	O
for	O
illustration	O
(	O
Allen	O
et	O
al	O
,	O
2002	O
;	O
Blaylock	O
and	O
Allen	O
,	O
2005	O
;	O
,	O
all	O
based	O
on	O
the	O
TRIPS	O
system	O
(	O
Allen	O
et	O
al	O
,	O
2000	O
)	O
.	O
One	O
of	O
the	O
main	O
benefits	O
of	O
this	O
model	O
is	O
that	O
linguistic	O
interpretation	O
and	O
high	O
-	O
level	O
intention	O
recognition	O
could	O
be	O
performed	O
independently	O
of	O
the	O
individual	O
problem	O
-	O
solving	O
level	O
,	O
whose	O
contribution	O
to	O
interpretation	O
would	O
be	O
to	O
specialize	O
the	O
higher	O
-	O
level	O
intentions	O
into	O
concrete	O
problemsolving	O
actions	O
and	O
verify	O
that	O
such	O
actions	O
make	O
sense	O
.	O
The	O
corollary	O
is	O
that	O
in	O
this	O
model	O
the	O
back	O
-	O
end	O
problem	O
solvers	O
would	O
be	O
insulated	O
from	O
the	O
need	O
to	O
worry	O
about	O
linguistic	O
issues	O
.	O
On	O
this	O
basis	O
,	O
it	O
should	O
be	O
possible	O
to	O
create	O
a	O
generic	O
dialogue	O
system	O
shell	O
with	O
only	O
domainindependent	O
components	O
.	O
Other	O
developers	O
,	O
not	O
necessarily	O
specialists	O
in	O
NLU	O
or	O
dialogue	O
systems	O
,	O
could	O
use	O
this	O
shell	O
to	O
build	O
,	O
relatively	O
quickly	O
,	O
intelligent	O
dialogue	O
systems	O
for	O
collaborative	O
tasks	O
in	O
various	O
domains	O
.	O
The	O
various	O
prototypes	O
of	O
TRIPS	O
CPS	O
-	O
based	O
systems	O
referenced	O
above	O
did	O
not	O
fulfill	O
this	O
promise	O
.	O
In	O
each	O
,	O
the	O
CPS	O
level	O
was	O
integrated	O
fairly	O
tightly	O
with	O
the	O
individual	O
problem	O
-	O
solving	O
level	O
for	O
the	O
application	O
domain	O
,	O
and	O
they	O
were	O
all	O
developed	O
by	O
the	O
same	O
team	O
.	O
Thus	O
,	O
even	O
though	O
each	O
such	O
prototype	O
implemented	O
(	O
a	O
version	O
of	O
)	O
the	O
CPS	O
model	O
and	O
used	O
the	O
same	O
platform	O
for	O
NLU	O
,	O
the	O
ultimate	O
goal	O
of	O
creating	O
a	O
domain	O
-	O
independent	O
dialogue	O
shell	O
that	O
others	O
could	O
customize	O
to	O
develop	O
independently	O
dialogue	O
systems	O
has	O
so	O
far	O
remained	O
elusive	O
.	O
Similarly	O
,	O
the	O
CPS	O
-	O
based	O
dialogue	O
manager	O
in	O
SAMMIE	O
(	O
Becker	O
et	O
al	O
,	O
2006	O
)	O
also	O
aimed	O
for	O
domain	O
independence	O
but	O
never	O
quite	O
realized	O
it	O
(	O
Blaylock	O
,	O
2007	O
)	O
.	O
In	O
the	O
rest	O
of	O
the	O
paper	O
we	O
will	O
report	O
on	O
our	O
attempt	O
to	O
develop	O
a	O
generic	O
dialogue	O
shell	O
based	O
on	O
the	O
CPS	O
model	O
.	O
We	O
start	O
with	O
a	O
description	O
of	O
the	O
general	O
architecture	O
of	O
a	O
dialogue	O
system	O
based	O
on	O
the	O
CPS	O
model	O
.	O
Then	O
,	O
we	O
will	O
describe	O
our	O
dialogue	O
manager	O
,	O
with	O
a	O
focus	O
on	O
its	O
interface	O
with	O
the	O
domain	O
-	O
specific	O
problem	O
solving	O
agent	B-DatasetName
.	O
Finally	O
,	O
we	O
give	O
some	O
details	O
on	O
six	O
prototype	O
dialogue	O
systems	O
developed	O
using	O
our	O
dialogue	O
shell	O
,	O
five	O
of	O
which	O
were	O
developed	O
by	O
independent	O
teams	O
of	O
researchers	O
.	O

A	O
collaborative	O
conversational	O
agent	B-DatasetName
must	O
understand	O
a	O
user	O
's	O
utterances	O
,	O
that	O
is	O
,	O
obtain	O
a	O
representation	O
of	O
the	O
meaning	O
of	O
the	O
utterance	O
,	O
recognize	O
its	O
intention	O
,	O
and	O
then	O
reason	O
with	O
this	O
intention	O
to	O
decide	O
what	O
to	O
do	O
and/or	O
say	O
next	O
.	O
Finally	O
,	O
the	O
system	O
must	O
convert	O
its	O
own	O
intentions	O
into	O
language	O
and	O
communicate	O
them	O
to	O
the	O
user	O
.	O
Figure	O
1	O
shows	O
a	O
conceptual	O
diagram	O
of	O
the	O
dialogue	O
system	O
we	O
envision	O
.	O
This	O
follows	O
the	O
common	O
separation	O
of	O
a	O
conversational	O
agent	B-DatasetName
's	O
functionality	O
into	O
interpretation	O
,	O
behavior	O
and	O
generation	O
,	O
but	O
where	O
the	O
separation	O
lines	O
are	O
is	O
critical	O
for	O
realizing	O
the	O
idea	O
of	O
isolating	O
domainindependent	O
from	O
domain	O
-	O
specific	O
processing	O
.	O
We	O
take	O
the	O
output	O
of	O
NL	O
Understanding	O
(	O
assumed	O
here	O
to	O
have	O
broad	O
lexical	O
,	O
syntactic	O
and	O
semantic	O
coverage	O
)	O
to	O
be	O
a	O
domain	O
-	O
independent	O
semantic	O
representation	O
of	O
the	O
user	O
's	O
utterance	O
(	O
a	O
communicative	O
act	O
)	O
,	O
expressed	O
in	O
terms	O
of	O
a	O
domainindependent	O
ontology	B-MethodName
.	O
Intention	O
recognition	O
is	O
performed	O
by	O
the	O
CPS	O
agent	B-DatasetName
,	O
which	O
takes	O
into	O
account	O
the	O
discourse	O
context	O
and	O
converts	O
communicative	O
acts	O
into	O
abstract	O
communicative	O
intentions	O
.	O
These	O
communicative	O
intentions	O
need	O
to	O
be	O
further	O
evaluated	O
with	O
respect	O
to	O
the	O
actual	O
problem	O
-	O
solving	O
state	O
,	O
so	O
they	O
are	O
not	O
fully	O
interpreted	O
until	O
they	O
reach	O
the	O
problem	O
solving	O
agent	B-DatasetName
.	O
This	O
agent	B-DatasetName
is	O
responsible	O
for	O
the	O
domain	O
-	O
specific	O
behaviorhereafter	O
we	O
will	O
refer	O
to	O
it	O
as	O
the	O
Behavioral	O
Agent	B-DatasetName
(	O
BA	B-DatasetName
)	O
and	O
for	O
operationalizing	O
the	O
communicative	O
intentions	O
into	O
actions	O
(	O
which	O
may	O
involve	O
planning	O
,	O
acting	O
on	O
the	O
world	O
,	O
updating	O
its	O
knowledge	O
of	O
the	O
situation	O
,	O
etc	O
.	O
)	O
.	O
An	O
autonomous	O
BA	B-DatasetName
should	O
be	O
able	O
to	O
plan	O
and	O
act	O
on	O
its	O
own	O
,	O
but	O
neither	O
the	O
BA	B-DatasetName
nor	O
the	O
user	O
can	O
singlehandedly	O
decide	O
on	O
the	O
status	O
of	O
collaborative	O
goals	O
without	O
a	O
commitment	O
from	O
the	O
other	O
party	O
.	O
The	O
BA	B-DatasetName
expresses	O
its	O
attitude	O
towards	O
shared	O
goals	O
by	O
sending	O
to	O
the	O
CPS	O
agent	B-DatasetName
its	O
own	O
communicative	O
intentions	O
,	O
which	O
the	O
CPS	O
agent	B-DatasetName
will	O
use	O
to	O
update	O
the	O
collaborative	O
state	O
and	O
generate	O
communicative	O
acts	O
for	O
NL	O
generation	O
(	O
such	O
as	O
accepting	O
or	O
rejecting	O
a	O
goal	O
,	O
or	O
proposing	O
a	O
new	O
one	O
)	O
.	O
Customization	O
:	O
Figure	O
1	O
includes	O
,	O
on	O
the	O
left	O
side	O
,	O
a	O
number	O
of	O
resources	O
needed	O
by	O
our	O
ideal	O
dialogue	O
system	O
:	O
(	O
1	O
)	O
a	O
broad	O
lexicon	O
for	O
NL	O
understanding	O
;	O
(	O
2	O
)	O
a	O
general	O
-	O
purpose	O
(	O
upper	O
-	O
level	O
)	O
ontology	B-MethodName
;	O
and	O
,	O
optionally	O
,	O
(	O
3	O
)	O
a	O
domain	O
ontology	B-MethodName
.	O
Even	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
broad	O
coverage	O
parser	O
,	O
with	O
an	O
extensive	O
domain	O
-	O
independent	O
high	O
-	O
level	O
ontology	B-MethodName
and	O
lexicon	O
,	O
will	O
not	O
contain	O
all	O
the	O
word	O
senses	O
and	O
concepts	O
needed	O
for	O
every	O
application	O
domain	O
.	O
Additionally	O
,	O
the	O
general	O
ontology	B-MethodName
concepts	O
need	O
to	O
be	O
mapped	O
onto	O
the	O
domain	O
ontology	B-MethodName
used	O
by	O
the	O
back	O
-	O
end	O
problem	O
solvers	O
.	O
Lastly	O
,	O
NL	O
generation	O
from	O
semantic	O
representations	O
of	O
communicative	O
acts	O
is	O
a	O
difficult	O
problem	O
,	O
with	O
no	O
general	O
solutions	O
.	O
Many	O
taskoriented	O
dialogue	O
systems	O
employ	O
template	O
-	O
based	O
techniques	O
,	O
which	O
can	O
lead	O
to	O
satisfactory	O
,	O
if	O
somewhat	O
repetitive	O
text	O
realizations	O
.	O
Such	O
templates	O
are	O
tailored	O
for	O
the	O
application	O
domain	O
.	O
It	O
may	O
appear	O
that	O
customizing	O
a	O
generic	O
dialogue	O
shell	O
to	O
specific	O
applications	O
involves	O
a	O
considerable	O
amount	O
of	O
work	O
.	O
Nevertheless	O
,	O
we	O
believe	O
these	O
customization	O
tasks	O
are	O
easier	O
to	O
accomplish	O
and	O
require	O
less	O
linguistic	O
expertise	O
than	O
building	O
a	O
dialogue	O
manager	O
for	O
every	O
application	O
,	O
let	O
al	O
ne	O
building	O
domain	O
-	O
specific	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
components	O
.	O

Let	O
us	O
now	O
turn	O
to	O
the	O
details	O
of	O
our	O
new	O
instantiation	O
of	O
the	O
CPS	O
model	O
.	O
Unlike	O
prior	O
work	O
on	O
CPSbased	O
dialogue	B-TaskName
management	I-TaskName
,	O
we	O
focus	O
on	O
the	O
interface	O
between	O
the	O
CPS	O
agent	B-DatasetName
(	O
CPSA	O
)	O
and	O
the	O
BA	B-DatasetName
.	O
This	O
allows	O
us	O
to	O
directly	O
address	O
the	O
issue	O
of	O
domain	O
-	O
independence	O
that	O
posed	O
difficulties	O
in	O
other	O
approaches	O
(	O
e.g.	O
,	O
Blaylock	O
,	O
2007	O
)	O
.	O
The	O
CPSA	O
computes	O
communicative	O
intentions	O
based	O
on	O
the	O
communicative	O
acts	O
resulting	O
from	O
the	O
NLU	O
component	O
.	O
These	O
communicative	O
intentions	O
are	O
realized	O
in	O
our	O
model	O
as	O
CPS	O
Acts	O
,	O
represented	O
as	O
a	O
pair	O
<	O
ACI	O
,	O
CONTEXT	O
>	O
,	O
where	O
ACI	O
represents	O
the	O
abstract	O
communicative	O
intention	O
and	O
CONTEXT	O
represents	O
the	O
semantic	O
content	O
of	O
the	O
act	O
in	O
a	O
knowledge	O
representation	O
language	O
.	O
Where	O
there	O
is	O
no	O
ambiguity	O
we	O
will	O
omit	O
CONTEXT	O
and	O
denote	O
CPS	O
acts	O
by	O
their	O
ACI	O
only	O
.	O
In	O
the	O
following	O
subsections	O
we	O
will	O
describe	O
the	O
set	O
of	O
CPS	O
acts	O
we	O
have	O
devised	O
so	O
far	O
,	O
grouped	O
by	O
the	O
manner	O
in	O
which	O
they	O
affect	O
the	O
collaborative	O
state	O
.	O

The	O
CPS	O
Model	O
defines	O
an	O
objective	O
as	O
an	O
intention	O
that	O
is	O
driving	O
the	O
agent	B-DatasetName
's	O
current	O
behavior	O
(	O
Allen	O
et	O
al	O
,	O
2002	O
)	O
.	O
An	O
objective	O
can	O
be	O
proposed	O
by	O
either	O
agent	B-DatasetName
,	O
provided	O
they	O
are	O
ready	O
to	O
commit	O
to	O
it	O
.	O
We	O
represent	O
the	O
intention	O
to	O
commit	O
to	O
an	O
objective	O
via	O
the	O
CPS	O
act	O
ADOPT	O
.	O
For	O
example	O
,	O
if	O
the	O
user	O
starts	O
a	O
conversation	O
with	O
"	O
Let	O
's	O
build	O
a	O
tower	O
"	O
,	O
this	O
results	O
in	O
the	O
following	O
CPS	O
act	O
:	O
(	O
ADOPT	O
:	O
i	O
d	O
O1	O
:	O
what	O
C1	O
:	O
as	O
(	O
GOAL	O
)	O
)	O
Here	O
,	O
O1	O
represents	O
a	O
unique	O
,	O
persistent	O
identifier	O
for	O
the	O
shared	O
objective	O
proposed	O
via	O
this	O
act	O
(	O
all	O
objectives	O
are	O
assigned	O
an	O
identifier	O
)	O
.	O
C1	O
is	O
an	O
identifier	O
indexed	O
into	O
the	O
CONTEXT	O
of	O
this	O
CPS	O
act	O
(	O
i.e.	O
,	O
it	O
refers	O
to	O
an	O
event	O
of	O
building	O
a	O
tower	O
)	O
.	O
Additionally	O
,	O
the	O
act	O
also	O
indicates	O
the	O
relation	O
between	O
this	O
objective	O
and	O
any	O
pre	O
-	O
existing	O
objectives	O
.	O
In	O
this	O
example	O
,	O
the	O
relation	O
was	O
identified	O
as	O
GOAL	O
,	O
indicating	O
that	O
this	O
is	O
a	O
top	O
-	O
level	O
objective	O
(	O
we	O
will	O
discuss	O
later	O
other	O
types	O
of	O
relations	O
between	O
objectives	O
available	O
in	O
our	O
model	O
)	O
.	O
Once	O
an	O
objective	O
has	O
been	O
jointly	O
committed	O
to	O
,	O
either	O
agent	B-DatasetName
can	O
propose	O
to	O
drop	O
their	O
commitment	O
to	O
it	O
,	O
via	O
a	O
CPS	O
act	O
called	O
ABANDON	O
.	O
Or	O
,	O
they	O
might	O
propose	O
to	O
shift	O
focus	O
from	O
the	O
active	O
objective	O
(	O
the	O
one	O
currently	O
driving	O
the	O
agents	O
'	O
behavior	O
)	O
,	O
by	O
an	O
act	O
called	O
DEFER	O
,	O
which	O
will	O
result	O
in	O
the	O
objective	O
becoming	O
inactive	O
.	O
A	O
proposal	O
to	O
bring	O
an	O
inactive	O
objective	O
back	O
into	O
an	O
active	O
state	O
an	O
agent	B-DatasetName
results	O
in	O
a	O
SELECT	O
act	O
.	O
Finally	O
,	O
an	O
agent	B-DatasetName
can	O
propose	O
that	O
an	O
objective	O
should	O
be	O
considered	O
completed	O
,	O
via	O
a	O
RELEASE	O
act	O
.	O
All	O
these	O
four	O
acts	O
only	O
take	O
as	O
an	O
argument	O
the	O
objective	O
's	O
unique	O
identifier	O
,	O
for	O
example	O
:	O
(	O
ABANDON	O
:	O
i	O
d	O
O1	O
)	O
.	O
Note	O
that	O
all	O
of	O
these	O
four	O
acts	O
can	O
be	O
proposed	O
,	O
indicating	O
the	O
agent	B-DatasetName
's	O
intentional	O
stance	O
towards	O
their	O
commitment	O
to	O
that	O
objective	O
.	O
The	O
user	O
performs	O
a	O
proposal	O
via	O
a	O
speech	O
act	O
.	O
The	O
same	O
intention	O
may	O
be	O
expressed	O
by	O
different	O
surface	O
speech	O
acts	O
.	O
Going	O
back	O
to	O
our	O
example	O
,	O
the	O
objective	O
of	O
building	O
a	O
tower	O
together	O
can	O
be	O
expressed	O
via	O
a	O
direct	O
proposal	O
(	O
"	O
Let	O
's	O
build	O
a	O
tower	O
"	O
)	O
;	O
a	O
question	O
(	O
"	O
Can	O
we	O
build	O
a	O
tower	O
?	O
"	O
)	O
;	O
or	O
an	O
indirect	O
speech	O
act	O
(	O
"	O
I	O
think	O
we	O
should	O
build	O
a	O
tower	O
"	O
)	O
.	O
The	O
CPSA	O
recognizes	O
the	O
user	O
intent	O
in	O
all	O
these	O
variants	O
,	O
using	O
the	O
surface	O
speech	O
act	O
and	O
other	O
linguistic	O
cues	O
present	O
in	O
the	O
communicative	O
act	O
it	O
receives	O
from	O
NLU	O
)	O
.	O
Thus	O
,	O
they	O
all	O
result	O
in	O
the	O
same	O
ADOPT	O
act	O
as	O
above	O
.	O
If	O
,	O
on	O
the	O
other	O
hand	O
,	O
the	O
BA	B-DatasetName
wants	O
to	O
propose	O
that	O
an	O
objective	O
be	O
jointly	O
pursued	O
,	O
say	O
that	O
it	O
wants	O
to	O
start	O
working	O
on	O
O1	O
by	O
a	O
subgoal	O
O2	O
of	O
placing	O
a	O
block	O
on	O
the	O
table	O
,	O
it	O
can	O
do	O
so	O
via	O
a	O
PROPOSE	O
act	O
,	O
whose	O
content	O
is	O
the	O
intention	O
to	O
commit	O
to	O
that	O
objective	O
:	O
where	O
C2	O
is	O
indexed	O
into	O
the	O
CONTEXT	O
of	O
the	O
act	O
for	O
a	O
representation	O
of	O
the	O
event	O
of	O
placing	O
a	O
block	O
on	O
the	O
table	O
.	O
Upon	O
receiving	O
this	O
act	O
,	O
the	O
CPSA	O
will	O
update	O
the	O
collaborative	O
state	O
to	O
reflect	O
the	O
BA	B-DatasetName
's	O
intention	O
to	O
commit	O
to	O
O2	O
,	O
and	O
formulate	O
a	O
communicative	O
act	O
for	O
NLG	O
to	O
realize	O
the	O
proposal	O
in	O
a	O
system	O
utterance	O
.	O
For	O
a	O
proposal	O
to	O
result	O
in	O
a	O
shared	O
objective	O
,	O
the	O
two	O
agents	O
must	O
agree	O
to	O
commit	O
to	O
it	O
.	O
The	O
CPSA	O
is	O
responsible	O
for	O
gathering	O
the	O
agreements	O
of	O
both	O
the	O
user	O
and	O
the	O
BA	B-DatasetName
.	O
When	O
the	O
CPSA	O
recognizes	O
that	O
the	O
user	O
is	O
proposing	O
an	O
objective	O
,	O
it	O
will	O
first	O
send	O
an	O
EVALUATE	O
act	O
to	O
the	O
BA	B-DatasetName
,	O
whose	O
content	O
is	O
the	O
proposed	O
objective	O
,	O
e.g.	O
,	O
:	O
(	O
EVALUATE	O
:	O
content	O
(	O
ADOPT	O
:	O
i	O
d	O
O1	O
:	O
what	O
C1	O
:	O
as	O
(	O
GOAL	O
)	O
)	O
This	O
act	O
creates	O
an	O
obligation	O
on	O
the	O
part	O
of	O
the	O
BA	B-DatasetName
to	O
evaluate	O
whether	O
it	O
is	O
able	O
to	O
commit	O
to	O
it	O
in	O
the	O
current	O
situation	O
,	O
and	O
,	O
if	O
so	O
,	O
respond	O
by	O
signaling	O
agreement	O
(	O
ACCEPTABLE	O
)	O
,	O
rejection	O
(	O
REJECTED	O
)	O
,	O
or	O
,	O
when	O
it	O
can	O
not	O
even	O
interpret	O
what	O
the	O
objective	O
is	O
,	O
a	O
failure	O
(	O
FAILURE	O
)	O
.	O
For	O
example	O
,	O
the	O
BA	B-DatasetName
's	O
agreement	O
,	O
that	O
is	O
,	O
its	O
intention	O
to	O
commit	O
to	O
the	O
objective	O
proposed	O
by	O
the	O
user	O
,	O
would	O
be	O
communicated	O
via	O
:	O
(	O
ACCEPTABLE	O
:	O
content	O
(	O
ADOPT	O
:	O
i	O
d	O
O1	O
:	O
what	O
C1	O
:	O
as	O
(	O
GOAL	O
)	O
)	O
Since	O
the	O
user	O
has	O
already	O
signaled	O
their	O
intention	O
to	O
commit	O
to	O
the	O
objective	O
by	O
proposing	O
it	O
,	O
on	O
receiving	O
from	O
the	O
BA	B-DatasetName
that	O
the	O
objective	O
is	O
ACCEPTABLE	O
,	O
the	O
CPSA	O
knows	O
that	O
there	O
is	O
mutual	O
agreement	O
,	O
decides	O
that	O
that	O
the	O
objective	O
is	O
now	O
adopted	O
,	O
and	O
sends	O
back	O
to	O
the	O
BA	B-DatasetName
the	O
following	O
CPS	O
act	O
:	O
to	O
signal	O
that	O
now	O
there	O
is	O
a	O
joint	O
commitment	O
to	O
O1	O
.	O
This	O
creates	O
an	O
obligation	O
on	O
the	O
part	O
of	O
the	O
BA	B-DatasetName
to	O
pursue	O
O1	O
in	O
whatever	O
manner	O
it	O
deems	O
appropriate	O
.	O
When	O
we	O
have	O
a	O
system	O
-	O
proposed	O
objective	O
,	O
such	O
as	O
O2	O
above	O
,	O
if	O
the	O
user	O
expresses	O
their	O
acceptance	O
(	O
"	O
Yes	O
"	O
,	O
"	O
Sure	O
"	O
,	O
"	O
I	O
can	O
handle	O
that	O
"	O
,	O
etc	O
.	O
)	O
,	O
the	O
CPSA	O
will	O
recognize	O
this	O
as	O
completing	O
the	O
agreement	O
,	O
and	O
then	O
it	O
would	O
adopt	O
the	O
objective	O
and	O
send	O
the	O
COMMIT	O
act	O
to	O
the	O
BA	B-DatasetName
.	O
Having	O
described	O
in	O
some	O
detail	O
how	O
objectives	O
are	O
created	O
,	O
and	O
how	O
the	O
CPSA	O
decides	O
that	O
there	O
is	O
joint	O
commitment	O
to	O
them	O
,	O
let	O
us	O
turn	O
briefly	O
to	O
some	O
of	O
the	O
details	O
that	O
we	O
brushed	O
over	O
.	O
Relations	O
between	O
objectives	O
:	O
We	O
mentioned	O
above	O
two	O
relations	O
between	O
the	O
objective	O
currently	O
under	O
consideration	O
and	O
the	O
prior	O
objectives	O
(	O
either	O
previously	O
adopted	O
ones	O
,	O
or	O
ones	O
that	O
have	O
been	O
discussed	O
but	O
are	O
still	O
being	O
negotiated	O
)	O
,	O
namely	O
GOAL	O
and	O
SUBGOAL	O
.	O
Currently	O
the	O
CPSA	O
can	O
infer	O
two	O
more	O
.	O
One	O
is	O
MODIFICATION	O
,	O
used	O
when	O
one	O
of	O
the	O
agents	O
is	O
expressing	O
an	O
intention	O
of	O
changing	O
in	O
some	O
manner	O
a	O
prior	O
objective	O
(	O
for	O
example	O
,	O
if	O
one	O
of	O
the	O
agents	O
had	O
suggested	O
placing	O
a	O
blue	O
block	O
on	O
the	O
table	O
,	O
the	O
other	O
agent	B-DatasetName
might	O
suggest	O
placing	O
a	O
red	O
block	O
instead	O
)	O
.	O
The	O
second	O
one	O
we	O
call	O
ELABORATION	O
,	O
and	O
is	O
used	O
by	O
the	O
CPSA	O
to	O
signal	O
that	O
it	O
has	O
insufficient	O
knowledge	O
to	O
decide	O
whether	O
the	O
objective	O
under	O
discussion	O
is	O
really	O
a	O
subgoal	O
or	O
a	O
modification	O
of	O
another	O
one	O
,	O
or	O
,	O
perhaps	O
a	O
new	O
top	O
-	O
level	O
goal	O
.	O
It	O
is	O
possible	O
,	O
however	O
,	O
that	O
the	O
BA	B-DatasetName
may	O
be	O
able	O
to	O
use	O
its	O
more	O
detailed	O
knowledge	O
of	O
the	O
situation	O
to	O
make	O
that	O
determination	O
.	O
Thus	O
,	O
upon	O
receiving	O
an	O
objective	O
marked	O
as	O
an	O
elaboration	O
of	O
another	O
one	O
,	O
if	O
the	O
BA	B-DatasetName
deems	O
it	O
acceptable	O
,	O
it	O
has	O
the	O
obligation	O
to	O
clarify	O
the	O
relation	O
as	O
well	O
.	O
Rejections	O
and	O
failures	O
:	O
If	O
a	O
user	O
proposes	O
an	O
objective	O
,	O
presumably	O
they	O
have	O
an	O
expectation	O
that	O
the	O
objective	O
is	O
achievable	O
.	O
If	O
the	O
BA	B-DatasetName
rejects	O
it	O
,	O
the	O
user	O
will	O
likely	O
not	O
be	O
satisfied	O
with	O
a	O
simple	O
"	O
No	O
"	O
.	O
Similarly	O
,	O
if	O
the	O
BA	B-DatasetName
fails	O
to	O
understand	O
the	O
objective	O
(	O
or	O
if	O
it	O
encounters	O
any	O
other	O
type	O
of	O
failure	O
,	O
e.g.	O
,	O
while	O
trying	O
to	O
perform	O
some	O
action	O
)	O
,	O
the	O
system	O
should	O
be	O
able	O
to	O
explain	O
what	O
happened	O
.	O
Thus	O
,	O
the	O
REJECTED	O
and	O
FAILURE	O
CPS	O
acts	O
have	O
features	O
for	O
optionally	O
specifying	O
a	O
reason	O
and	O
a	O
possible	O
way	O
of	O
repairing	O
the	O
situation	O
.	O
The	O
reason	O
for	O
rejection	O
/	O
failure	O
is	O
one	O
of	O
a	O
relatively	O
small	O
set	O
of	O
predefined	O
ones	O
(	O
e.g.	O
,	O
UNKNOWN	O
-	O
OBJECT	O
,	O
FAILED	O
-	O
ACTION	O
)	O
,	O
and	O
it	O
is	O
expected	O
that	O
the	O
NLG	O
component	O
will	O
make	O
use	O
of	O
it	O
to	O
generate	O
more	O
helpful	O
utterances	O
.	O
As	O
for	O
how	O
to	O
repair	O
the	O
situation	O
,	O
this	O
can	O
be	O
an	O
alternative	O
objective	O
,	O
that	O
the	O
BA	B-DatasetName
is	O
ready	O
to	O
commit	O
to	O
,	O
which	O
could	O
be	O
either	O
a	O
modification	O
of	O
the	O
reject	O
-	O
ed	O
one	O
,	O
or	O
,	O
perhaps	O
,	O
an	O
objective	O
which	O
,	O
if	O
realized	O
,	O
would	O
make	O
the	O
rejected	O
objective	O
acceptable	O
.	O
For	O
example	O
,	O
if	O
the	O
user	O
wanted	O
to	O
build	O
an	O
all	O
-	O
blue	O
5	O
-	O
block	O
tower	O
,	O
but	O
the	O
BA	B-DatasetName
has	O
only	O
4	O
blue	O
blocks	O
,	O
it	O
would	O
reject	O
the	O
goal	O
(	O
INSUFFICIENT	O
-	O
RESOURCES	O
)	O
,	O
but	O
it	O
could	O
suggest	O
as	O
an	O
alternative	O
that	O
a	O
4	O
-	O
block	O
blue	O
tower	O
would	O
be	O
an	O
achievable	O
alternative	O
.	O
This	O
might	O
be	O
realized	O
as	O
"	O
Sorry	O
,	O
I	O
do	O
n't	O
have	O
enough	O
blocks	O
for	O
that	O
,	O
but	O
we	O
can	O
build	O
a	O
4	O
-	O
block	O
blue	O
tower	O
.	O
"	O
.	O
If	O
the	O
user	O
accepts	O
(	O
"	O
OK	O
"	O
)	O
,	O
the	O
CPSA	O
will	O
immediately	O
commit	O
to	O
the	O
suggested	O
objective	O
.	O

Collaborative	O
problem	O
solving	O
requires	O
not	O
only	O
joint	O
commitments	O
to	O
certain	O
objectives	O
,	O
but	O
also	O
a	O
set	O
of	O
shared	O
beliefs	O
about	O
the	O
situation	O
.	O
These	O
shared	O
beliefs	O
occasionally	O
need	O
to	O
be	O
updated	O
.	O
One	O
agent	B-DatasetName
may	O
inform	O
the	O
other	O
of	O
a	O
fact	O
that	O
they	O
believe	O
the	O
other	O
should	O
know	O
.	O
This	O
may	O
come	O
about	O
unprompted	O
or	O
as	O
a	O
result	O
of	O
being	O
asked	O
.	O
The	O
CPS	O
Model	O
offers	O
little	O
guidance	O
on	O
how	O
such	O
acts	O
fit	O
in	O
,	O
even	O
though	O
they	O
are	O
very	O
common	O
in	O
conversation	O
.	O
The	O
examples	O
given	O
seem	O
to	O
suggest	O
an	O
interpretation	O
of	O
questions	O
and	O
simple	O
assertions	O
based	O
on	O
plan	O
recognition	O
(	O
Allen	O
,	O
1979	O
)	O
,	O
which	O
is	O
a	O
tall	O
order	O
,	O
particularly	O
for	O
a	O
domainindependent	O
dialogue	O
manager	O
.	O
When	O
agent	B-DatasetName
A	O
informs	O
agent	B-DatasetName
B	O
of	O
a	O
fact	O
P	O
,	O
this	O
indicates	O
A	O
's	O
immediate	O
intention	O
that	O
B	O
knows	O
P.	O
Similarly	O
,	O
if	O
A	O
asks	O
B	O
whether	O
P	O
is	O
true	O
(	O
an	O
ask	O
-	O
if	O
speech	O
act	O
)	O
or	O
what	O
object	O
satisfies	O
P	O
(	O
an	O
ask	O
-	O
wh	O
speech	O
act	O
)	O
,	O
A	O
's	O
immediate	O
intention	O
is	O
that	O
B	O
informs	O
A	O
of	O
those	O
particular	O
facts	O
(	O
Allen	O
and	O
Perrault	O
,	O
1980	O
)	O
.	O
Getting	O
at	O
the	O
intentions	O
behind	O
these	O
immediate	O
intentions	O
requires	O
fairly	O
sophisticated	O
,	O
often	O
domainspecific	O
reasoning	O
(	O
in	O
our	O
implementation	O
the	O
CPSA	O
can	O
do	O
that	O
to	O
some	O
extent	O
via	O
abstract	O
task	O
models	O
,	O
but	O
,	O
due	O
to	O
space	O
limitations	O
,	O
we	O
will	O
not	O
discuss	O
it	O
here	O
)	O
.	O
Therefore	O
,	O
we	O
created	O
a	O
small	O
set	O
of	O
CPS	O
acts	O
for	O
representing	O
the	O
intentions	O
to	O
impart	O
and	O
request	O
knowledge	O
about	O
situations	O
.	O
In	O
our	O
model	O
,	O
an	O
assertion	O
of	O
a	O
fact	O
results	O
in	O
the	O
following	O
CPS	O
act	O
:	O
where	O
C3	B-DatasetName
is	O
an	O
identifier	O
pointing	O
to	O
a	O
representation	O
of	O
the	O
content	O
of	O
the	O
assertion	O
in	O
the	O
CONTEXT	O
of	O
the	O
CPS	O
act	O
.	O
The	O
relation	O
between	O
an	O
ASSERTION	O
act	O
and	O
an	O
existing	O
objective	O
(	O
or	O
NIL	O
if	O
no	O
such	O
objective	O
exists	O
)	O
is	O
an	O
underspeci	O
-	O
fied	O
one	O
,	O
of	O
contributing	O
somehow	O
to	O
it	O
.	O
The	O
BA	B-DatasetName
needs	O
to	O
decide	O
,	O
if	O
it	O
accepts	O
A3	O
,	O
how	O
this	O
addition	O
will	O
change	O
its	O
understanding	O
of	O
the	O
situation	O
and	O
affect	O
O1	O
or	O
any	O
other	O
(	O
adopted	O
)	O
objective	O
.	O
For	O
ask	O
-	O
if	O
questions	O
the	O
CPSA	O
will	O
produce	O
the	O
following	O
act	O
:	O
(	O
ASK	O
-	O
IF	O
:	O
i	O
d	O
A4	O
:	O
query	O
Q4	O
:	O
as	O
(	O
QUERY	O
-	O
IN	O
-	O
CONTEXT	O
:	O
goal	O
O1	O
)	O
)	O
Here	O
Q4	O
is	O
an	O
identifier	O
pointing	O
to	O
a	O
representation	O
(	O
in	O
the	O
CONTEXT	O
of	O
the	O
CPS	O
act	O
)	O
of	O
a	O
statement	O
to	O
be	O
evaluated	O
for	O
its	O
truth	O
value	O
.	O
For	O
ask	O
-	O
wh	O
questions	O
the	O
CPSA	O
produces	O
acts	O
in	O
the	O
following	O
format	O
:	O
This	O
expresses	O
the	O
intention	O
of	O
knowing	O
the	O
value	O
of	O
an	O
entity	O
(	O
W5	O
)	O
,	O
possibly	O
restricted	O
to	O
a	O
set	O
of	O
choices	O
(	O
S5	O
)	O
,	O
that	O
makes	O
a	O
proposition	O
(	O
Q5	O
)	O
true	O
.	O
As	O
before	O
,	O
all	O
these	O
identifiers	O
should	O
be	O
given	O
appropriate	O
descriptions	O
in	O
the	O
CONTEXT	O
.	O
This	O
act	O
can	O
thus	O
represent	O
the	O
intention	O
expressed	O
by	O
a	O
question	O
such	O
as	O
"	O
What	O
color	O
should	O
we	O
use	O
for	O
the	O
first	O
block	O
,	O
blue	O
or	O
red	O
?	O
"	O
.	O
Finally	O
,	O
an	O
answer	O
to	O
a	O
question	O
takes	O
the	O
following	O
form	O
:	O
It	O
is	O
important	O
to	O
note	O
that	O
we	O
treat	O
these	O
intentions	O
as	O
special	O
types	O
of	O
objectives	O
,	O
that	O
can	O
become	O
adopted	O
,	O
active	O
,	O
etc	O
.	O
,	O
just	O
like	O
other	O
objectives	O
.	O
For	O
example	O
,	O
if	O
one	O
of	O
these	O
CPS	O
acts	O
is	O
initiated	O
by	O
the	O
user	O
,	O
the	O
act	O
must	O
be	O
evaluated	O
by	O
the	O
BA	B-DatasetName
.	O
If	O
it	O
deems	O
the	O
act	O
ACCEPTABLE	O
,	O
the	O
CPSA	O
will	O
commit	O
to	O
working	O
on	O
it	O
(	O
updating	O
the	O
system	O
's	O
beliefs	O
,	O
or	O
answering	O
the	O
question	O
)	O
.	O
If	O
originating	O
from	O
the	O
BA	B-DatasetName
,	O
the	O
act	O
must	O
be	O
proposed	O
first	O
,	O
and	O
realized	O
through	O
a	O
communicative	O
act	O
.	O
(	O
Side	O
effects	O
:	O
We	O
noted	O
above	O
that	O
updating	O
the	O
system	O
's	O
beliefs	O
about	O
the	O
situation	O
may	O
affect	O
the	O
status	O
of	O
existing	O
objectives	O
.	O
Insofar	O
as	O
the	O
BA	B-DatasetName
is	O
capable	O
of	O
foreseeing	O
these	O
effects	O
,	O
it	O
ought	O
to	O
inform	O
the	O
CPSA	O
so	O
the	O
collaborative	O
state	O
can	O
be	O
updated	O
.	O
Any	O
such	O
changes	O
would	O
result	O
in	O
an	O
obligation	O
to	O
inform	O
the	O
user	O
.	O
In	O
our	O
model	O
we	O
use	O
an	O
additional	O
feature	O
for	O
the	O
ACCEPTABLE	O
act	O
(	O
see	O
previous	O
section	O
)	O
,	O
for	O
describing	O
the	O
effect	O
.	O
Its	O
value	O
is	O
an	O
objective	O
to	O
be	O
proposed	O
.	O
For	O
example	O
,	O
if	O
,	O
in	O
the	O
context	O
of	O
the	O
shared	O
objective	O
of	O
building	O
a	O
tower	O
,	O
the	O
system	O
asks	O
"	O
Who	O
is	O
going	O
to	O
move	O
the	O
blocks	O
?	O
"	O
,	O
and	O
the	O
user	O
says	O
"	O
I	O
will	O
"	O
,	O
this	O
answer	O
has	O
the	O
side	O
effect	O
of	O
modifying	O
the	O
existing	O
objective	O
(	O
in	O
this	O
case	O
specializing	O
it	O
to	O
include	O
the	O
identity	O
of	O
the	O
builder	O
)	O
.	O
The	O
system	O
's	O
acceptance	O
of	O
the	O
answer	O
will	O
necessarily	O
imply	O
the	O
acceptance	O
of	O
the	O
modification	O
as	O
well	O
,	O
and	O
the	O
CPSA	O
will	O
update	O
the	O
collaborative	O
state	O
accordingly	O
.	O

Another	O
important	O
role	O
of	O
the	O
CPSA	O
in	O
managing	O
the	O
dialogue	O
is	O
to	O
negotiate	O
initiative	O
.	O
To	O
facilitate	O
an	O
orderly	O
conversation	O
,	O
it	O
restricts	O
both	O
the	O
timing	O
and	O
the	O
magnitude	O
of	O
the	O
BA	B-DatasetName
's	O
ability	O
to	O
affect	O
the	O
collaborative	O
state	O
.	O
It	O
does	O
so	O
via	O
a	O
special	O
CPS	O
act	O
,	O
called	O
WHAT	O
-	O
NEXT	O
,	O
which	O
takes	O
a	O
single	O
argument	O
:	O
the	O
identifier	O
of	O
an	O
adopted	O
shared	O
objective	O
(	O
usually	O
the	O
one	O
that	O
is	O
active	O
)	O
.	O
This	O
act	O
can	O
be	O
sent	O
to	O
the	O
BA	B-DatasetName
whenever	O
there	O
are	O
no	O
pending	O
updates	O
to	O
the	O
collaborative	O
state	O
,	O
and	O
no	O
outstanding	O
communicative	O
acts	O
to	O
process	O
or	O
to	O
wait	O
on	O
.	O
In	O
effect	O
,	O
by	O
sending	O
this	O
act	O
,	O
the	O
CPSA	O
transfers	O
the	O
task	O
initiative	O
to	O
the	O
BA	B-DatasetName
,	O
which	O
gives	O
it	O
the	O
chance	O
to	O
,	O
ultimately	O
,	O
influence	O
discourse	O
initiative	O
as	O
well	O
.	O
The	O
BA	B-DatasetName
has	O
the	O
obligation	O
to	O
respond	O
with	O
a	O
single	O
update	O
to	O
the	O
collaborative	O
state	O
,	O
presumably	O
the	O
one	O
with	O
the	O
highest	O
priority	O
.	O
This	O
restriction	O
is	O
critical	O
,	O
because	O
it	O
frees	O
the	O
CPSA	O
from	O
the	O
need	O
to	O
consider	O
too	O
many	O
options	O
about	O
what	O
to	O
do	O
and	O
say	O
next	O
,	O
a	O
decision	O
that	O
,	O
in	O
many	O
situations	O
,	O
would	O
require	O
domain	O
-	O
specific	O
knowledge	O
.	O
The	O
BA	B-DatasetName
's	O
reply	O
to	O
a	O
WHAT	O
-	O
NEXT	O
depends	O
on	O
its	O
own	O
private	O
problem	O
-	O
solving	O
state	O
.	O
It	O
may	O
be	O
that	O
it	O
has	O
done	O
some	O
planning	O
and	O
,	O
as	O
a	O
result	O
,	O
it	O
wants	O
to	O
propose	O
a	O
way	O
of	O
making	O
progress	O
towards	O
accomplishing	O
the	O
active	O
objective	O
.	O
It	O
may	O
be	O
that	O
it	O
does	O
not	O
have	O
sufficient	O
information	O
to	O
make	O
progress	O
,	O
in	O
which	O
case	O
it	O
may	O
formulate	O
an	O
intention	O
to	O
ask	O
the	O
user	O
to	O
provide	O
the	O
information	O
.	O
Or	O
,	O
if	O
the	O
active	O
objective	O
is	O
a	O
question	O
,	O
it	O
may	O
have	O
come	O
up	O
with	O
an	O
answer	O
;	O
that	O
update	O
would	O
prob	O
-	O
ably	O
get	O
very	O
high	O
priority	O
.	O
All	O
these	O
possibilities	O
are	O
handled	O
by	O
acts	O
we	O
have	O
already	O
discussed	O
.	O
One	O
other	O
possibility	O
is	O
that	O
the	O
BA	B-DatasetName
is	O
currently	O
not	O
doing	O
any	O
reasoning	O
,	O
but	O
simply	O
acting	O
on	O
the	O
active	O
objective	O
,	O
or	O
has	O
accomplished	O
it	O
.	O
Updates	O
to	O
the	O
status	O
of	O
an	O
objective	O
are	O
communicated	O
via	O
a	O
special	O
CPS	O
act	O
,	O
which	O
takes	O
the	O
following	O
form	O
:	O
(	O
EXECUTION	O
-	O
STATUS	O
:	O
goal	O
A1	O
:	O
status	O
GS	O
)	O
Here	O
GS	O
is	O
an	O
expression	O
that	O
indicates	O
the	O
status	O
of	O
the	O
goal	O
.	O
Currently	O
it	O
can	O
be	O
one	O
of	O
three	O
indicators	O
:	O
1	O
.	O
DONE	O
,	O
which	O
signifies	O
that	O
A1	O
was	O
accomplished	O
.	O
CPSA	O
will	O
create	O
a	O
communicative	O
act	O
to	O
inform	O
the	O
user	O
,	O
and	O
,	O
if	O
the	O
user	O
agrees	O
,	O
releases	O
the	O
objective	O
.	O
2	O
.	O
WORKING	O
-	O
ON	O
-	O
IT	O
,	O
which	O
indicates	O
that	O
the	O
BA	B-DatasetName
is	O
actively	O
pursuing	O
A1	O
,	O
but	O
it	O
will	O
take	O
more	O
time	O
.	O
The	O
CPSA	O
may	O
decide	O
to	O
inform	O
the	O
user	O
,	O
and	O
creates	O
a	O
trigger	O
for	O
itself	O
to	O
check	O
back	O
later	O
.	O
3	O
.	O
WAITING	O
-	O
FOR	O
-	O
USER	O
,	O
which	O
indicates	O
that	O
the	O
BA	B-DatasetName
can	O
not	O
make	O
progress	O
on	O
A1	O
because	O
it	O
is	O
waiting	O
for	O
the	O
user	O
to	O
act	O
on	O
it	O
(	O
or	O
another	O
objective	O
that	O
A1	O
depends	O
on	O
)	O
.	O
As	O
a	O
result	O
,	O
the	O
CPSA	O
will	O
construct	O
a	O
communicative	O
act	O
to	O
prompt	O
the	O
user	O
.	O
This	O
CPS	O
act	O
also	O
allows	O
the	O
BA	B-DatasetName
to	O
communicate	O
partial	O
execution	O
status	O
(	O
that	O
it	O
has	O
executed	O
some	O
actions	O
,	O
though	O
it	O
has	O
not	O
accomplished	O
the	O
objective	O
yet	O
)	O
,	O
but	O
we	O
leave	O
those	O
details	O
out	O
of	O
this	O
discussion	O
.	O

We	O
implemented	O
our	O
CPS	O
model	O
as	O
a	O
component	O
in	O
the	O
TRIPS	O
system	O
(	O
Allen	O
et	O
al	O
,	O
2000	O
)	O
,	O
which	O
has	O
recently	O
been	O
released	O
in	O
the	O
public	O
domain	O
under	O
a	O
GNU	O
GPL	O
License	O
.	O
The	O
TRIPS	O
system	O
comes	O
with	O
a	O
broad	O
coverage	O
parser	O
with	O
an	O
extensive	O
grammar	O
and	O
an	O
effective	O
100	O
,	O
000	O
+	O
word	O
semantic	O
vocabulary	O
defined	O
in	O
terms	O
of	O
a	O
4000	O
concept	O
domain	O
-	O
independent	O
ontology	B-MethodName
.	O
It	O
operates	O
in	O
concert	O
with	O
a	O
suite	O
of	O
statistical	O
preprocessing	O
components	O
,	O
performing	O
tasks	O
such	O
as	O
part	O
-	O
ofspeech	O
tagging	O
,	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
,	O
and	O
identification	O
of	O
likely	O
constituent	O
boundaries	O
.	O
These	O
preprocessed	O
inputs	O
are	O
provided	O
to	O
the	O
core	O
TRIPS	O
parser	O
as	O
advice	O
.	O
The	O
parser	O
con	O
-	O
structs	O
from	O
the	O
input	O
a	O
logical	O
form	O
,	O
which	O
is	O
a	O
semantic	O
representation	O
that	O
captures	O
an	O
unscoped	O
modal	O
logic	O
(	O
Manshadi	O
et	O
al	O
,	O
2008	O
)	O
.	O
The	O
logical	O
form	O
includes	O
the	O
surface	O
speech	O
act	O
,	O
semantic	O
types	O
,	O
semantic	O
roles	O
for	O
predicate	O
arguments	O
,	O
and	O
dependency	O
relations	O
.	O
TRIPS	O
also	O
includes	O
an	O
interpretation	O
manager	O
that	O
converts	O
the	O
logical	O
forms	O
into	O
communicative	O
acts	O
,	O
performing	O
language	O
-	O
based	O
intention	O
recognition	O
and	O
normalizing	O
different	O
surface	O
forms	O
.	O
We	O
packaged	O
the	O
TRIPS	O
NLU	O
components	O
(	O
including	O
the	O
lexicon	O
and	O
ontology	B-MethodName
)	O
with	O
our	O
CPS	O
agent	B-DatasetName
,	O
thereby	O
creating	O
a	O
dialogue	O
system	O
shell	O
,	O
which	O
we	O
call	O
Cogent	O
.	O
This	O
system	O
does	O
not	O
include	O
a	O
BA	B-DatasetName
or	O
an	O
NLG	O
component	O
(	O
Cogent	O
's	O
components	O
are	O
surrounded	O
with	O
a	O
dashed	O
line	O
in	O
Figure	O
1	O
)	O
.	O
Thus	O
,	O
it	O
is	O
a	O
true	O
domain	O
-	O
independent	O
shell	O
,	O
not	O
a	O
system	O
that	O
can	O
be	O
adapted	O
to	O
other	O
domains	O
.	O
It	O
can	O
carry	O
out	O
very	O
minimal	O
conversations	O
because	O
social	O
conversational	O
acts	O
such	O
as	O
greetings	O
are	O
handled	O
in	O
a	O
domain	O
-	O
independent	O
manner	O
in	O
the	O
CPSA	O
.	O
But	O
,	O
ultimately	O
,	O
the	O
purpose	O
of	O
the	O
shell	O
is	O
to	O
be	O
used	O
to	O
create	O
domain	O
applications	O
.	O
The	O
success	O
of	O
the	O
task	O
we	O
set	O
to	O
accomplish	O
is	O
whether	O
this	O
shell	O
can	O
be	O
and	O
is	O
used	O
by	O
independent	O
developers	O
to	O
develop	O
operational	O
dialogue	O
systems	O
in	O
domains	O
of	O
their	O
choice	O
.	O
As	O
discussed	O
in	O
the	O
previous	O
section	O
,	O
the	O
CPS	O
acts	O
and	O
the	O
obligations	O
they	O
engender	O
establish	O
a	O
protocol	O
that	O
developers	O
of	O
behavioral	O
agents	O
must	O
implement	O
.	O
Other	O
than	O
that	O
,	O
we	O
believe	O
the	O
CPSA	O
offers	O
functionality	O
to	O
develop	O
different	O
styles	O
of	O
conversational	O
agents	O
(	O
user	O
-	O
driven	O
,	O
system	O
-	O
driven	O
or	O
fully	O
mixed	O
-	O
initiative	O
)	O
.	O
The	O
developers	O
also	O
must	O
implement	O
their	O
own	O
NL	O
Generation	O
component	O
,	O
for	O
reasons	O
that	O
we	O
touched	O
upon	O
earlier	O
.	O
Of	O
note	O
,	O
by	O
default	O
all	O
CPS	O
acts	O
have	O
their	O
contents	O
expressed	O
in	O
the	O
TRIPS	O
ontology	B-MethodName
.	O
We	O
are	O
also	O
providing	O
a	O
tool	O
for	O
mapping	O
concepts	O
in	O
the	O
TRIPS	O
ontology	B-MethodName
to	O
domain	O
ontologies	O
.	O
We	O
have	O
adapted	O
the	O
TRIPS	O
interpretation	O
manager	O
to	O
use	O
these	O
mappings	O
to	O
produce	O
content	O
in	O
the	O
domain	O
ontology	B-MethodName
,	O
to	O
make	O
it	O
easier	O
for	O
the	O
Behavioral	O
Agents	O
to	O
interpret	O
the	O
CONTEXT	O
associated	O
with	O
each	O
CPS	O
act	O
.	O
The	O
details	O
of	O
the	O
ontology	B-MethodName
mapping	O
tool	O
and	O
the	O
mappings	O
it	O
creates	O
are	O
,	O
however	O
,	O
beyond	O
the	O
scope	O
of	O
this	O
paper	O
.	O

We	O
describe	O
briefly	O
six	O
system	O
prototypes	O
that	O
have	O
been	O
built	O
using	O
Cogent	O
as	O
the	O
base	O
frame	O
-	O
work	O
;	O
thus	O
,	O
they	O
all	O
use	O
the	O
same	O
CPS	O
agent	B-DatasetName
described	O
above	O
.	O
In	O
all	O
cases	O
,	O
the	O
developers	O
of	O
these	O
prototypes	O
used	O
the	O
protocol	O
described	O
above	O
to	O
create	O
behavioral	O
agents	O
that	O
,	O
in	O
turn	O
,	O
act	O
as	O
integrators	O
of	O
other	O
problem	O
solvers	O
.	O
The	O
descriptions	O
of	O
these	O
systems	O
are	O
going	O
to	O
be	O
necessarily	O
brief	O
;	O
the	O
interested	O
reader	O
is	O
encouraged	O
to	O
follow	O
the	O
references	O
to	O
get	O
a	O
better	O
understanding	O
of	O
their	O
capabilities	O
and	O
the	O
kinds	O
of	O
dialogues	O
they	O
support	O
(	O
unfortunately	O
,	O
not	O
all	O
systems	O
have	O
been	O
published	O
yet	O
)	O
.	O
All	O
these	O
systems	O
have	O
been	O
developed	O
as	O
part	O
of	O
DARPA	B-DatasetName
's	O
Communicating	O
with	O
Computers	O
(	O
CwC	O
)	O
program	O
3	O
.	O
Cabot	O
:	O
This	O
is	O
a	O
mixed	O
-	O
initiative	O
system	O
for	O
planning	O
and	O
execution	O
in	O
the	O
blocks	O
world	O
,	O
the	O
tasks	O
being	O
of	O
jointly	O
building	O
structures	O
(	O
Perera	O
et	O
al	O
,	O
2017	O
)	O
.	O
Both	O
the	O
user	O
and	O
the	O
system	O
can	O
come	O
up	O
with	O
their	O
own	O
goals	O
,	O
and	O
,	O
if	O
necessary	O
,	O
they	O
will	O
negotiate	O
constraints	O
on	O
those	O
structures	O
(	O
size	O
,	O
colors	O
,	O
etc	O
.	O
)	O
so	O
all	O
the	O
goals	O
can	O
be	O
completed	O
.	O
They	O
also	O
negotiate	O
their	O
roles	O
in	O
building	O
these	O
structures	O
(	O
"	O
architect	O
"	O
or	O
"	O
builder	O
"	O
)	O
.	O
This	O
system	O
uses	O
a	O
2D	O
simulated	O
version	O
of	O
the	O
blocks	O
world	O
.	O
The	O
examples	O
used	O
in	O
this	O
paper	O
are	O
from	O
interactions	O
with	O
this	O
system	O
.	O
Cabot	O
-	O
L	O
:	O
This	O
system	O
learns	O
names	O
and	O
structural	O
properties	O
of	O
complex	O
objects	O
in	O
a	O
physically	O
situated	O
blocks	O
world	O
scenario	O
(	O
Perera	O
et	O
al	O
,	O
2017	O
;	O
Perera	O
et	O
al	O
,	O
2018	O
)	O
.	O
The	O
user	O
teaches	O
the	O
system	O
by	O
providing	O
examples	O
of	O
structures	O
together	O
with	O
descriptions	O
in	O
language	O
.	O
The	O
system	O
has	O
capabilities	O
to	O
perceive	O
the	O
world	O
and	O
detect	O
changes	O
to	O
it	O
,	O
and	O
can	O
ask	O
the	O
user	O
questions	O
about	O
various	O
features	O
of	O
the	O
structures	O
,	O
to	O
learn	O
a	O
general	O
model	O
.	O
To	O
validate	O
the	O
inferred	O
model	O
,	O
the	O
user	O
can	O
then	O
show	O
additional	O
examples	O
and	O
ask	O
the	O
system	O
to	O
classify	O
them	O
and	O
explain	O
its	O
reasoning	O
.	O
The	O
user	O
and	O
the	O
system	O
can	O
interact	O
via	O
either	O
written	O
or	O
spoken	O
language	O
.	O
BoB	O
:	O
This	O
system	O
acts	O
as	O
an	O
assistant	O
biologist	O
.	O
It	O
has	O
fairly	O
extensive	O
knowledge	O
of	O
molecular	O
biology	O
and	O
can	O
assist	O
the	O
user	O
by	O
responding	O
to	O
inquiries	O
about	O
properties	O
of	O
genes	O
,	O
proteins	O
,	O
molecular	O
mechanisms	O
,	O
their	O
relationship	O
to	O
cellular	O
processes	O
and	O
disease	O
,	O
building	O
and	O
visualizing	O
complex	O
causal	O
models	O
,	O
running	O
simulations	O
on	O
these	O
models	O
to	O
detect	O
their	O
dynamic	O
properties	O
,	O
etc	O
.	O
To	O
manage	O
this	O
wide	O
range	O
of	O
problemsolving	O
behaviors	O
,	O
BoB	O
's	O
BA	B-DatasetName
integrates	O
a	O
variety	O
of	O
agents	O
with	O
specific	O
expertise	O
.	O
ing	O
domain	O
-	O
specific	O
named	O
entity	O
recognizers	O
)	O
and	O
some	O
additional	O
ontology	B-MethodName
concepts	O
and	O
mappings	O
;	O
we	O
provided	O
those	O
customizations	O
.	O
The	O
version	O
of	O
the	O
TRIPS	O
Parser	O
we	O
started	O
with	O
proved	O
to	O
be	O
fairly	O
robust	O
,	O
but	O
we	O
did	O
have	O
to	O
adapt	O
it	O
in	O
response	O
to	O
failures	O
reported	O
by	O
the	O
dialogue	O
system	O
developers	O
.	O
Nevertheless	O
,	O
these	O
enhancements	O
were	O
not	O
domain	O
-	O
specificthat	O
is	O
,	O
the	O
same	O
parser	O
,	O
with	O
the	O
same	O
grammar	O
,	O
is	O
used	O
for	O
all	O
systems	O
.	O
In	O
all	O
systems	O
,	O
developers	O
used	O
custom	O
template	O
-	O
based	O
NLG	O
.	O

In	O
this	O
paper	O
we	O
reported	O
on	O
the	O
development	O
of	O
a	O
new	O
domain	O
-	O
independent	O
dialogue	O
manager	O
based	O
on	O
the	O
collaborative	O
problem	O
solving	O
model	O
.	O
We	O
packaged	O
this	O
dialogue	O
manager	O
with	O
a	O
suite	O
of	O
broad	O
coverage	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
components	O
(	O
from	O
the	O
TRIPS	O
system	O
)	O
and	O
created	O
a	O
new	O
,	O
domain	O
-	O
independent	O
CPS	O
-	O
based	O
dialogue	O
system	O
shell	O
.	O
This	O
shell	O
has	O
been	O
used	O
by	O
several	O
independent	O
teams	O
of	O
researchers	O
to	O
develop	O
dialogue	O
systems	O
in	O
a	O
variety	O
of	O
application	O
domains	O
,	O
with	O
different	O
conversational	O
styles	O
.	O
We	O
believe	O
this	O
to	O
be	O
the	O
first	O
successful	O
implementation	O
of	O
a	O
domain	O
-	O
independent	O
dialogue	O
system	O
shell	O
based	O
on	O
the	O
CPS	O
model	O
(	O
or	O
any	O
other	O
model	O
of	O
equivalent	O
complexity	O
)	O
.	O
We	O
do	O
not	O
claim	O
the	O
CPSA	O
to	O
be	O
complete	O
,	O
however	O
.	O
For	O
example	O
,	O
it	O
can	O
sometimes	O
detect	O
an	O
ambiguity	O
in	O
the	O
user	O
's	O
intention	O
and	O
generate	O
a	O
clarification	O
question	O
,	O
but	O
its	O
abilities	O
in	O
this	O
regard	O
are	O
fairly	O
limited	O
.	O
BoB	O
has	O
demonstrated	O
some	O
limited	O
handling	O
of	O
hypotheticals	O
(	O
in	O
what	O
-	O
if	O
questions	O
)	O
at	O
the	O
problem	O
-	O
solving	O
level	O
,	O
but	O
the	O
CPSA	O
itself	O
does	O
not	O
yet	O
track	O
hypothetical	O
situations	O
.	O
We	O
expect	O
that	O
,	O
with	O
wider	O
adoption	O
,	O
we	O
will	O
inevitably	O
be	O
confronted	O
with	O
the	O
need	O
to	O
improve	O
both	O
our	O
model	O
and	O
its	O
implementation	O
.	O
As	O
noted	O
above	O
in	O
reference	O
to	O
BoB	O
and	O
Musica	O
,	O
for	O
domains	O
requiring	O
adaptation	O
of	O
the	O
NLU	O
components	O
,	O
language	O
specialists	O
are	O
still	O
needed	O
.	O
We	O
have	O
not	O
yet	O
endeavored	O
to	O
create	O
tools	O
that	O
would	O
make	O
it	O
easier	O
for	O
dialogue	O
system	O
developers	O
to	O
adapt	O
/	O
improve	O
themselves	O
the	O
NLU	O
components	O
.	O
Our	O
current	O
focus	O
is	O
on	O
evaluating	O
the	O
robustness	O
of	O
the	O
intention	O
recognition	O
functionality	O
of	O
the	O
CPSA	O
.	O

This	O
research	O
was	O
supported	O
by	O
the	O
DARPA	B-DatasetName
Communicating	O
with	O
Computers	O
program	O
,	O
under	O
ARO	O
contract	O
W911NF	O
-	O
15	O
-	O
1	O
-	O
0542	O
.	O

On	O
Negative	O
Interference	O
in	O
Multilingual	O
Models	O
:	O
Findings	O
and	O
A	O
Meta	B-TaskName
-	I-TaskName
Learning	I-TaskName
Treatment	O

Modern	O
multilingual	O
models	O
are	O
trained	O
on	O
concatenated	O
text	O
from	O
multiple	O
languages	O
in	O
hopes	O
of	O
conferring	O
benefits	O
to	O
each	O
(	O
positive	O
transfer	O
)	O
,	O
with	O
the	O
most	O
pronounced	O
benefits	O
accruing	O
to	O
low	O
-	O
resource	O
languages	O
.	O
However	O
,	O
recent	O
work	O
has	O
shown	O
that	O
this	O
approach	O
can	O
degrade	O
performance	O
on	O
high	O
-	O
resource	O
languages	O
,	O
a	O
phenomenon	O
known	O
as	O
negative	O
interference	O
.	O
In	O
this	O
paper	O
,	O
we	O
present	O
the	O
first	O
systematic	O
study	O
of	O
negative	O
interference	O
.	O
We	O
show	O
that	O
,	O
contrary	O
to	O
previous	O
belief	O
,	O
negative	O
interference	O
also	O
impacts	O
low	O
-	O
resource	O
languages	O
.	O
While	O
parameters	O
are	O
maximally	O
shared	O
to	O
learn	O
language	O
-	O
universal	O
structures	O
,	O
we	O
demonstrate	O
that	O
language	O
-	O
specific	O
parameters	O
do	O
exist	O
in	O
multilingual	O
models	O
and	O
they	O
are	O
a	O
potential	O
cause	O
of	O
negative	O
interference	O
.	O
Motivated	O
by	O
these	O
observations	O
,	O
we	O
also	O
present	O
a	O
meta	B-TaskName
-	I-TaskName
learning	I-TaskName
algorithm	O
that	O
obtains	O
better	O
cross	O
-	O
lingual	O
transferability	O
and	O
alleviates	O
negative	O
interference	O
,	O
by	O
adding	O
languagespecific	O
layers	O
as	O
meta	O
-	O
parameters	O
and	O
training	O
them	O
in	O
a	O
manner	O
that	O
explicitly	O
improves	O
shared	O
layers	O
'	O
generalization	O
on	O
all	O
languages	O
.	O
Overall	O
,	O
our	O
results	O
show	O
that	O
negative	O
interference	O
is	O
more	O
common	O
than	O
previously	O
known	O
,	O
suggesting	O
new	O
directions	O
for	O
improving	O
multilingual	O
representations	O
.	O
1	O

Advances	O
in	O
pretraining	O
language	O
models	O
(	O
Devlin	O
et	O
al	O
,	O
2018	O
;	O
Yang	O
et	O
al	O
,	O
2019	O
)	O
as	O
general	O
-	O
purpose	O
representations	O
have	O
pushed	O
the	O
state	O
of	O
the	O
art	O
on	O
a	O
variety	O
of	O
natural	O
language	O
tasks	O
.	O
However	O
,	O
not	O
all	O
languages	O
enjoy	O
large	O
public	O
datasets	O
for	O
pretraining	O
and/or	O
downstream	O
tasks	O
.	O
Multilingual	O
language	O
models	O
such	O
as	O
mBERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
and	O
XLM	B-MethodName
(	O
Lample	O
and	O
Conneau	O
,	O
2019	O
)	O
have	O
been	O
proven	O
effective	O
for	O
cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
transfer	I-TaskName
learning	O
by	O
pretraining	O
a	O
single	O
shared	O
Transformer	B-MethodName
model	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
jointly	O
on	O
multiple	O
languages	O
.	O
The	O
goals	O
of	O
multilingual	O
modeling	O
are	O
not	O
limited	O
to	O
improving	O
language	O
modeling	O
in	O
low	O
-	O
resource	O
languages	O
(	O
Lample	O
and	O
Conneau	O
,	O
2019	O
)	O
,	O
but	O
also	O
include	O
zero	O
-	O
shot	O
crosslingual	O
transfer	O
on	O
downstream	O
tasks	O
-	O
it	O
has	O
been	O
shown	O
that	O
multilingual	O
models	O
can	O
generalize	O
to	O
target	O
languages	O
even	O
when	O
labeled	O
training	O
data	O
is	O
only	O
available	O
in	O
the	O
source	O
language	O
(	O
typically	O
English	O
)	O
on	O
a	O
wide	O
range	O
of	O
tasks	O
(	O
Pires	O
et	O
al	O
,	O
2019	O
;	O
Wu	O
and	O
Dredze	O
,	O
2019	O
;	O
Hu	O
et	O
al	O
,	O
2020	O
)	O
.	O
However	O
,	O
multilingual	O
models	O
are	O
not	O
equally	O
beneficial	O
for	O
all	O
languages	O
.	O
demonstrated	O
that	O
including	O
more	O
languages	O
in	O
a	O
single	O
model	O
can	O
improve	O
performance	O
for	O
lowresource	O
languages	O
but	O
hurt	O
performance	O
for	O
highresource	O
languages	O
.	O
Similarly	O
,	O
recent	O
work	O
(	O
Johnson	O
et	O
al	O
,	O
2017	O
;	O
Tan	O
et	O
al	O
,	O
2019	O
;	O
Aharoni	O
et	O
al	O
,	O
2019	O
;	O
in	O
multilingual	O
neural	O
machine	B-TaskName
translation	I-TaskName
(	O
NMT	O
)	O
also	O
observed	O
performance	O
degradation	O
on	O
high	O
-	O
resource	O
language	O
pairs	O
.	O
In	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
(	O
Ruder	O
,	O
2017	O
)	O
,	O
this	O
phenomenon	O
is	O
known	O
as	O
negative	O
interference	O
or	O
negative	O
transfer	O
(	O
Wang	O
et	O
al	O
,	O
2019	O
)	O
,	O
where	O
training	O
multiple	O
tasks	O
jointly	O
hinders	O
the	O
performance	O
on	O
individual	O
tasks	O
.	O
Despite	O
these	O
empirical	O
observations	O
,	O
little	O
prior	O
work	O
analyzed	O
or	O
showed	O
how	O
to	O
mitigate	O
negative	O
interference	O
in	O
multilingual	O
language	O
models	O
.	O
Particularly	O
,	O
it	O
is	O
natural	O
to	O
ask	O
:	O
(	O
1	O
)	O
Can	O
negative	O
interference	O
occur	O
for	O
low	O
-	O
resource	O
languages	O
also	O
?	O
(	O
2	O
)	O
What	O
factors	O
play	O
an	O
important	O
role	O
in	O
causing	O
it	O
?	O
(	O
3	O
)	O
Can	O
we	O
mitigate	O
negative	O
interference	O
to	O
improve	O
the	O
model	O
's	O
cross	O
-	O
lingual	O
transferability	O
?	O
In	O
this	O
paper	O
,	O
we	O
take	O
a	O
step	O
towards	O
addressing	O
these	O
questions	O
.	O
We	O
pretrain	O
a	O
set	O
of	O
monolingual	O
and	O
bilingual	O
models	O
and	O
evaluate	O
them	O
on	O
a	O
range	O
of	O
downstream	O
tasks	O
to	O
analyze	O
negative	O
interference	O
.	O
We	O
seek	O
to	O
individually	O
characterize	O
the	O
un	O
-	O
derlying	O
factors	O
of	O
negative	O
interference	O
through	O
a	O
set	O
of	O
ablation	O
studies	O
and	O
glean	O
insights	O
on	O
its	O
causes	O
.	O
Specifically	O
,	O
we	O
examine	O
if	O
training	O
corpus	O
size	O
and	O
language	O
similarity	O
affect	O
negative	O
interference	O
,	O
and	O
also	O
measure	O
gradient	O
and	O
parameter	O
similarities	O
between	O
languages	O
.	O
Our	O
results	O
show	O
that	O
negative	O
interference	O
can	O
occur	O
in	O
both	O
high	O
-	O
resource	O
and	O
low	O
-	O
resource	O
languages	O
.	O
In	O
particular	O
,	O
we	O
observe	O
that	O
neither	O
subsampling	O
the	O
training	O
corpus	O
nor	O
adding	O
typologically	O
similar	O
languages	O
substantially	O
impacts	O
negative	O
interference	O
.	O
On	O
the	O
other	O
hand	O
,	O
we	O
show	O
that	O
gradient	O
conflicts	O
and	O
language	O
-	O
specific	O
parameters	O
do	O
exist	O
in	O
multilingual	O
models	O
,	O
suggesting	O
that	O
languages	O
are	O
fighting	O
for	O
model	O
capacity	O
,	O
which	O
potentially	O
causes	O
negative	O
interference	O
.	O
We	O
further	O
test	O
whether	O
explicitly	O
assigning	O
language	O
-	O
specific	O
modules	O
to	O
each	O
language	O
can	O
alleviate	O
negative	O
interference	O
,	O
and	O
find	O
that	O
the	O
resulting	O
model	O
performs	O
better	O
within	O
each	O
individual	O
language	O
but	O
worse	O
on	O
zero	O
-	O
shot	O
cross	O
-	O
lingual	O
tasks	O
.	O
Motivated	O
by	O
these	O
observations	O
,	O
we	O
further	O
propose	O
to	O
meta	O
-	O
learn	O
these	O
language	O
-	O
specific	O
parameters	O
to	O
explicitly	O
improve	O
generalization	O
of	O
shared	O
parameters	O
on	O
all	O
languages	O
.	O
Empirically	O
,	O
our	O
method	O
improves	O
not	O
only	O
within	O
-	O
language	O
performance	O
on	O
monolingual	O
tasks	O
but	O
also	O
cross	O
-	O
lingual	O
transferability	O
on	O
zero	O
-	O
shot	O
transfer	O
benchmarks	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
work	O
to	O
systematically	O
study	O
and	O
remedy	O
negative	O
interference	O
in	O
multilingual	O
language	O
models	O
.	O

Multilingual	O
transfer	B-TaskName
learning	I-TaskName
aims	O
at	O
utilizing	O
knowledge	O
transfer	O
across	O
languages	O
to	O
boost	O
performance	O
on	O
low	O
-	O
resource	O
languages	O
.	O
State	O
-	O
of	O
-	O
theart	O
multilingual	O
language	O
models	O
are	O
trained	O
on	O
multiple	O
languages	O
jointly	O
to	O
enable	O
cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
transfer	I-TaskName
through	O
parameter	O
sharing	O
.	O
However	O
,	O
languages	O
are	O
heterogeneous	O
,	O
with	O
different	O
vocabularies	O
,	O
morphosyntactic	O
rules	O
,	O
and	O
different	O
pragmatics	O
across	O
cultures	O
.	O
It	O
is	O
therefore	O
natural	O
to	O
ask	O
,	O
is	O
knowledge	O
transfer	O
beneficial	O
for	O
all	O
languages	O
in	O
a	O
multilingual	O
model	O
?	O
To	O
analyze	O
the	O
effect	O
of	O
knowledge	O
transfer	O
from	O
other	O
languages	O
on	O
a	O
specific	O
language	O
lg	O
,	O
we	O
can	O
compare	O
multilingual	O
models	O
with	O
the	O
monolingual	O
model	O
trained	O
on	O
lg	O
.	O
For	O
example	O
,	O
in	O
Figure	O
1	O
,	O
we	O
compare	O
the	O
performance	O
on	O
a	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
task	O
of	O
monolingually	O
-	O
trained	O
models	O
vs.	O
bilingual	O
models	O
(	O
trained	O
on	O
lg	O
and	O
English	O
)	O
vs.	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
XLM	B-MethodName
.	O
We	O
can	O
see	O
that	O
monolingual	O
models	O
outperform	O
multilingual	O
models	O
on	O
four	O
out	O
of	O
six	O
languages	O
(	O
See	O
3.3	O
for	O
details	O
)	O
.	O
This	O
shows	O
that	O
language	O
conflicts	O
may	O
induce	O
negative	O
impacts	O
on	O
certain	O
languages	O
,	O
which	O
we	O
refer	O
to	O
as	O
negative	O
interference	O
.	O
Here	O
,	O
we	O
investigate	O
the	O
causes	O
of	O
negative	O
interference	O
(	O
3.3	O
)	O
and	O
methods	O
to	O
overcome	O
it	O
(	O
4	O
)	O
.	O
3	O
Investigating	O
the	O
Sources	O
of	O
Negative	O
Interference	O
in	O
Multilingual	O
Models	O

Recent	O
work	O
(	O
Yu	O
et	O
al	O
,	O
2020	O
)	O
shows	O
that	O
gradient	O
conflict	O
between	O
dissimilar	O
tasks	O
,	O
defined	O
as	O
a	O
negative	O
cosine	O
similarity	O
between	O
gradients	O
,	O
is	O
predictive	O
of	O
negative	O
interference	O
in	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
.	O
Therefore	O
,	O
we	O
study	O
whether	O
gradient	O
conflicts	O
exist	O
between	O
languages	O
in	O
multilingual	O
models	O
.	O
In	O
particular	O
,	O
we	O
sample	O
one	O
batch	O
for	O
each	O
language	O
in	O
the	O
model	O
and	O
compute	O
the	O
corresponding	O
gradients	O
'	O
cosine	O
similarity	O
for	O
every	O
10	O
steps	O
during	O
pretraining	O
.	O

Unsupervised	O
multilingual	O
language	O
models	O
such	O
as	O
mBERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
and	O
XLM	B-MethodName
(	O
Lample	O
and	O
Conneau	O
,	O
2019	O
;	O
work	O
surprisingly	O
well	O
on	O
many	O
NLP	O
tasks	O
without	O
parallel	O
training	O
signals	O
(	O
Pires	O
et	O
al	O
,	O
2019	O
;	O
Wu	O
and	O
Dredze	O
,	O
2019	O
)	O
.	O
A	O
line	O
of	O
follow	O
-	O
up	O
work	O
Artetxe	O
et	O
al	O
,	O
2019	O
;	O
Karthikeyan	O
et	O
al	O
,	O
2020	O
)	O
study	O
what	O
contributes	O
to	O
the	O
cross	O
-	O
lingual	O
ability	O
of	O
these	O
models	O
.	O
They	O
show	O
that	O
vocabulary	O
overlap	O
is	O
not	O
required	O
for	O
multilingual	O
models	O
,	O
and	O
suggest	O
that	O
abstractions	O
shared	O
across	O
languages	O
emerge	O
automatically	O
during	O
pretraining	O
.	O
Another	O
line	O
of	O
research	O
investigate	O
how	O
to	O
further	O
improve	O
these	O
shared	O
knowledge	O
,	O
such	O
as	O
applying	O
post	O
-	O
hoc	O
alignment	O
(	O
Wang	O
et	O
al	O
,	O
2020b	O
;	O
Cao	O
et	O
al	O
,	O
2020	O
)	O
and	O
utilizing	O
better	O
calibrated	O
training	O
signal	O
(	O
Mulcaire	O
et	O
al	O
,	O
2019	O
;	O
Huang	O
et	O
al	O
,	O
2019	O
)	O
.	O
While	O
prior	O
work	O
emphasize	O
how	O
to	O
share	O
to	O
improve	O
transferability	O
,	O
we	O
study	O
multilingual	O
models	O
from	O
a	O
different	O
perspective	O
of	O
how	O
to	O
unshare	O
to	O
resolve	O
language	O
conflicts	O
.	O
Our	O
work	O
is	O
also	O
related	O
to	O
transfer	B-TaskName
learning	I-TaskName
(	O
Pan	O
and	O
Yang	O
,	O
2010	O
)	O
and	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
(	O
Ruder	O
,	O
2017	O
)	O
.	O
In	O
particular	O
,	O
prior	O
work	O
have	O
observed	O
(	O
Rosenstein	O
et	O
al	O
,	O
2005	O
)	O
and	O
studied	O
(	O
Wang	O
et	O
al	O
,	O
2019	O
)	O
negative	O
transfer	O
,	O
such	O
that	O
transferring	O
knowledge	O
from	O
source	O
tasks	O
can	O
degrade	O
the	O
performance	O
in	O
the	O
target	O
task	O
.	O
Others	O
show	O
it	O
is	O
important	O
to	O
remedy	O
negative	O
transfer	O
in	O
multi	O
-	O
source	O
settings	O
(	O
Ge	O
et	O
al	O
,	O
2014	O
;	O
Wang	O
and	O
Carbonell	O
,	O
2018	O
)	O
.	O
In	O
this	O
work	O
,	O
we	O
study	O
negative	O
transfer	O
in	O
multilingual	O
models	O
,	O
where	O
languages	O
contain	O
heavily	O
unbalanced	O
training	O
data	O
and	O
exhibit	O
complex	O
intertask	O
relatedness	O
.	O
In	O
addition	O
,	O
our	O
work	O
is	O
related	O
to	O
methods	O
that	O
measure	O
similarity	O
between	O
cross	O
-	O
lingual	O
representations	O
.	O
For	O
example	O
,	O
existing	O
methods	O
utilize	O
statistical	O
metrics	O
to	O
examine	O
cross	O
-	O
lingual	O
embeddings	O
such	O
as	O
singular	O
vector	O
canonical	O
correlation	O
analysis	O
(	O
Raghu	O
et	O
al	O
,	O
2017	O
;	O
Kudugunta	O
et	O
al	O
,	O
2019	O
)	O
,	O
eigenvector	O
similarity	O
(	O
Søgaard	O
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
centered	O
kernel	O
alignment	O
(	O
Kornblith	O
et	O
al	O
,	O
2019	O
;	O
.	O
While	O
these	O
methods	O
focus	O
on	O
testing	O
latent	O
representations	O
,	O
we	O
directly	O
compare	O
similarity	O
of	O
neural	O
network	O
structures	O
through	O
network	B-TaskName
pruning	I-TaskName
.	O
Finally	O
,	O
our	O
work	O
is	O
related	O
to	O
meta	O
learning	O
,	O
which	O
sets	O
a	O
meta	O
task	O
to	O
learn	O
model	O
initialization	O
for	O
fast	O
adaptation	O
(	O
Finn	O
et	O
al	O
,	O
2017	O
;	O
Gu	O
et	O
al	O
,	O
2018	O
;	O
Flennerhag	O
et	O
al	O
,	O
2019	O
)	O
,	O
data	O
selection	O
(	O
Wang	O
et	O
al	O
,	O
2020a	O
)	O
,	O
andhyperparameters	O
(	O
Baydin	O
et	O
al	O
,	O
2018	O
)	O
.	O
In	O
our	O
case	O
,	O
the	O
meta	O
task	O
is	O
to	O
mitigate	O
negative	O
interference	O
.	O

We	O
show	O
the	O
full	O
results	O
on	O
the	O
TyDiQA	B-DatasetName
-	I-DatasetName
GoldP	I-DatasetName
dataset	O
in	O
Table	O
7	O
.	O

UoB	O
-	O
UK	O
at	O
SemEval	O
-	O
2016	O
Task	O
1	O
:	O
A	O
Flexible	O
and	O
Extendable	O
System	O
for	O
Semantic	O
Text	B-TaskName
Similarity	I-TaskName
using	O
Types	O
,	O
Surprise	O
and	O
Phrase	O
Linking	O

We	O
present	O
in	O
this	O
paper	O
a	O
system	O
for	O
measuring	O
Semantic	O
Text	B-TaskName
Similarity	I-TaskName
(	O
STS	B-TaskName
)	O
in	O
English	O
.	O
We	O
introduce	O
three	O
novel	O
techniques	O
:	O
the	O
use	O
of	O
Types	O
,	O
methods	O
of	O
linking	O
phrases	O
,	O
and	O
the	O
use	O
of	O
a	O
Surprise	O
Factor	O
to	O
generate	O
8	O
,	O
370	O
similarity	O
measures	O
,	O
which	O
we	O
then	O
combine	O
using	O
Support	O
Vector	O
and	O
Kernel	O
Ridge	O
Regression	O
.	O
Our	O
system	O
out	O
performs	O
the	O
State	O
of	O
the	O
Art	O
in	O
SemEval	O
2015	O
,	O
and	O
our	O
best	O
performing	O
run	O
achieved	O
a	O
score	O
of	O
.7094	O
on	O
the	O
2016	O
test	O
set	O
as	O
a	O
whole	O
,	O
and	O
over	O
0.8	O
on	O
the	O
majority	O
of	O
the	O
datasets	O
.	O
Additionally	O
,	O
the	O
use	O
of	O
Surprise	O
,	O
Types	O
and	O
phrase	O
linking	O
is	O
not	O
limited	O
to	O
STS	B-TaskName
and	O
can	O
be	O
used	O
across	O
various	O
Natural	O
Language	O
Processing	O
tasks	O
,	O
while	O
our	O
method	O
of	O
combining	O
scores	O
provides	O
a	O
flexible	O
way	O
of	O
combining	O
variously	O
generated	O
Similarity	O
Scores	O
.	O

Word	B-TaskName
embeddings	I-TaskName
provide	O
a	O
method	O
of	O
mapping	O
words	O
or	O
phrases	O
to	O
vectors	O
,	O
whose	O
cosine	O
distance	O
represents	O
semantic	B-TaskName
similarity	I-TaskName
.	O
They	O
have	O
proved	O
to	O
be	O
powerful	O
in	O
many	O
NLP	O
tasks	O
,	O
and	O
have	O
been	O
used	O
by	O
top	O
ranking	O
systems	O
at	O
SemEval	O
STS	B-TaskName
(	O
Sultan	O
et	O
al	O
,	O
2015	O
;	O
Hänig	O
et	O
al	O
,	O
2015	O
)	O
.	O
We	O
use	O
word2vec	O
1	O
,	O
with	O
the	O
model	O
trained	O
by	O
Google	B-DatasetName
on	O
the	O
Google	B-DatasetName
News	O
dataset	O
,	O
through	O
its	O
Python	O
interface	O
Gensim	O
2	O
.	O
We	O
make	O
use	O
of	O
word2vec	O
in	O
two	O
distinct	O
ways	O
.	O
The	O
first	O
is	O
by	O
extracting	O
the	O
mean	O
of	O
the	O
vector	O
representation	O
of	O
each	O
word	O
in	O
a	O
Type	O
and	O
finding	O
its	O
cosine	O
similarity	O
between	O
the	O
two	O
sentences	O
.	O
The	O
second	O
is	O
by	O
adding	O
the	O
word2vec	O
similarity	O
scores	O
of	O
words	O
not	O
aligned	O
within	O
the	O
same	O
Type	O
.	O
We	O
also	O

Given	O
a	O
sentence	O
pair	O
,	O
we	O
calculate	O
their	O
similarity	O
based	O
only	O
on	O
how	O
similar	O
corresponding	O
Parts	O
-	O
of	O
-	O
Speech	O
(	O
POS	O
)	O
are	O
,	O
a	O
method	O
previous	O
systems	O
have	O
made	O
use	O
of	O
,	O
either	O
implicitly	O
(	O
Kashyap	O
et	O
al	O
,	O
2014	O
;	O
Sultan	O
et	O
al	O
,	O
2015	O
)	O
or	O
explicitly	O
(	O
Hänig	O
et	O
al	O
,	O
2015	O
)	O
.	O
We	O
extend	O
this	O
idea	O
by	O
defining	O
what	O
we	O
call	O
word	O
Types	O
,	O
which	O
further	O
subdivide	O
each	O
POS	O
.	O
A	O
Type	O
represents	O
an	O
abstract	O
concept	O
that	O
several	O
words	O
can	O
share	O
.	O
Consider	O
the	O
sentence	O
pair	O
"	O
A	O
man	O
is	O
sitting	O
on	O
a	O
stool	O
"	O
,	O
"	O
A	O
boy	O
is	O
sitting	O
on	O
a	O
chair	O
"	O
.	O
Although	O
the	O
words	O
"	O
man	O
"	O
,	O
"	O
boy	O
"	O
,	O
"	O
stool	O
"	O
and	O
"	O
chair	O
"	O
are	O
all	O
nouns	O
,	O
an	O
effective	O
strategy	O
for	O
comparing	O
these	O
sentences	O
would	O
be	O
to	O
compare	O
the	O
first	O
two	O
and	O
the	O
last	O
two	O
words	O
independently	O
,	O
before	O
then	O
adding	O
up	O
their	O
similarity	O
.	O
To	O
achieve	O
this	O
we	O
categorise	O
words	O
into	O
different	O
Types	O
,	O
which	O
are	O
then	O
compared	O
across	O
sentences	O
.	O
In	O
this	O
case	O
,	O
such	O
a	O
categorisation	O
might	O
place	O
the	O
first	O
two	O
into	O
the	O
Type	O
"	O
Person	O
"	O
and	O
the	O
others	O
into	O
the	O
category	O
"	O
Artifact	O
"	O
.	O
This	O
problem	O
could	O
very	O
easily	O
extend	O
to	O
the	O
problem	O
of	O
Word	B-TaskName
Sense	I-TaskName
Disambiguation	I-TaskName
,	O
which	O
we	O
avoid	O
by	O
use	O
of	O
a	O
heuristic	O
.	O
We	O
calculate	O
the	O
Type	O
of	O
a	O
noun	O
by	O
the	O
use	O
of	O
WordNet	O
(	O
Miller	O
,	O
1995	O
)	O
hypernyms	O
:	O
W	O
1	O
is	O
consid	O
-	O
ered	O
a	O
hypernym	O
of	O
W	O
2	O
if	O
∀e	O
W	O
2	O
,	O
e	O
is	O
an	O
instance	O
of	O
W	O
1	O
.	O
We	O
recursively	O
find	O
hypernyms	O
until	O
we	O
reach	O
a	O
manually	O
selected	O
set	O
of	O
concepts	O
(	O
such	O
as	O
food.n.02	O
)	O
.	O
We	O
manually	O
combine	O
sets	O
of	O
such	O
concepts	O
to	O
define	O
a	O
Type	O
.	O
As	O
a	O
concrete	O
example	O
,	O
we	O
combine	O
the	O
WordNet	O
concepts	O
"	O
communication.n.02	O
"	O
,	O
"	O
food.n.02	O
"	O
and	O
other	O
similar	O
concepts	O
into	O
the	O
Type	O
"	O
thing	O
r1	O
"	O
.	O
As	O
a	O
single	O
word	O
can	O
be	O
part	O
of	O
several	O
Types	O
,	O
based	O
on	O
the	O
particular	O
sense	O
of	O
the	O
word	O
,	O
we	O
pick	O
the	O
most	O
frequently	O
occurring	O
Type	O
for	O
each	O
word	O
.	O
3	O

Consider	O
sentences	O
with	O
the	O
the	O
phrases	O
"	O
Prime	O
Minister	O
"	O
and	O
"	O
Prime	O
Number	O
"	O
.	O
Although	O
the	O
word	O
"	O
Prime	O
"	O
is	O
present	O
in	O
both	O
sentences	O
,	O
the	O
context	O
in	O
which	O
it	O
is	O
being	O
used	O
makes	O
this	O
irrelevant	O
.	O
In	O
this	O
particular	O
case	O
,	O
the	O
semantic	B-TaskName
similarity	I-TaskName
of	O
the	O
sentences	O
is	O
dependent	O
on	O
the	O
head	O
of	O
the	O
phrase	O
that	O
the	O
word	O
"	O
Prime	O
"	O
is	O
contained	O
in	O
(	O
i.e.	O
"	O
Minister	O
"	O
and	O
"	O
Number	O
"	O
)	O
.	O
This	O
is	O
also	O
the	O
case	O
with	O
phrases	O
that	O
contain	O
adjectives	O
and	O
adverbs	O
.	O
We	O
address	O
this	O
by	O
finding	O
phrases	O
that	O
consist	O
of	O
adjectives	O
,	O
adverbs	O
and	O
nouns	O
,	O
and	O
varying	O
the	O
importance	O
of	O
the	O
semantic	B-TaskName
similarity	I-TaskName
between	O
words	O
that	O
are	O
not	O
the	O
head	O
of	O
that	O
phrase	O
.	O
The	O
similarity	O
of	O
each	O
word	O
,	O
that	O
is	O
part	O
of	O
such	O
a	O
phrase	O
,	O
but	O
not	O
the	O
head	O
of	O
the	O
phrase	O
,	O
is	O
additionally	O
weighted	O
in	O
three	O
different	O
ways	O
:	O
The	O
first	O
assigns	O
a	O
zero	O
or	O
one	O
weight	O
based	O
on	O
whether	O
or	O
not	O
the	O
head	O
of	O
the	O
phrase	O
is	O
aligned	O
,	O
the	O
second	O
provides	O
a	O
weight	O
based	O
on	O
the	O
number	O
of	O
words	O
,	O
following	O
this	O
word	O
,	O
that	O
are	O
aligned	O
in	O
the	O
phrase	O
and	O
the	O
third	O
simply	O
ignores	O
the	O
phrase	O
structure	O
.	O

In	O
this	O
paper	O
we	O
have	O
described	O
the	O
system	O
we	O
used	O
for	O
participation	O
in	O
the	O
SemEval	O
STS	B-TaskName
Monolingual	O
Task	O
which	O
made	O
use	O
of	O
Types	O
,	O
Phrase	O
Linking	O
,	O
and	O
a	O
method	O
of	O
establishing	O
common	O
noun	O
importance	O
.	O
In	O
the	O
future	O
,	O
we	O
intend	O
to	O
experiment	O
with	O
including	O
features	O
for	O
each	O
of	O
the	O
Methods	O
during	O
the	O
training	O
phase	O
,	O
other	O
kinds	O
of	O
phrases	O
,	O
and	O
different	O
Type	O
definitions	O
.	O
We	O
also	O
intend	O
to	O
use	O
the	O
STS	B-TaskName
data	O
for	O
learning	O
the	O
weights	O
of	O
different	O
Types	O
for	O
use	O
in	O
other	O
NLP	O
applications	O
.	O
We	O
believe	O
that	O
Types	O
have	O
significant	O
potential	O
and	O
intend	O
to	O
explore	O
them	O
in	O
greater	O
detail	O
.	O
Our	O
immediate	O
objectives	O
will	O
be	O
in	O
better	O
defining	O
types	O
,	O
re	O
-	O
categorising	O
common	O
noun	O
Types	O
based	O
on	O
clearer	O
instructions	O
to	O
manual	O
annotators	O
,	O
including	O
finer	O
definitions	O
of	O
Types	O
for	O
proper	O
nouns	O
using	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
,	O
and	O
exploring	O
methods	O
of	O
defining	O
Types	O
for	O
verbs	O
,	O
adverbs	O
and	O
adjectives	O
.	O
We	O
also	O
intend	O
to	O
explore	O
the	O
use	O
of	O
Types	O
in	O
Question	O
Classification	B-TaskName
and	O
Question	B-TaskName
Answering	I-TaskName
.	O

UnibucKernel	O
:	O
Geolocating	O
Swiss	O
German	O
Jodels	O
Using	O
Ensemble	B-TaskName
Learning	I-TaskName

In	O
this	O
work	O
,	O
we	O
describe	O
our	O
approach	O
addressing	O
the	O
Social	O
Media	O
Variety	O
Geolocation	O
task	O
featured	O
in	O
the	O
2021	O
VarDial	O
Evaluation	O
Campaign	O
.	O
We	O
focus	O
on	O
the	O
second	O
subtask	O
,	O
which	O
is	O
based	O
on	O
a	O
data	O
set	O
formed	O
of	O
approximately	O
30	O
thousand	O
Swiss	O
German	O
Jodels	O
.	O
The	O
dialect	B-TaskName
identification	I-TaskName
task	O
is	O
about	O
accurately	O
predicting	O
the	O
latitude	O
and	O
longitude	O
of	O
test	O
samples	O
.	O
We	O
frame	O
the	O
task	O
as	O
a	O
double	O
regression	O
problem	O
,	O
employing	O
an	O
XGBoost	O
metalearner	O
with	O
the	O
combined	O
power	O
of	O
a	O
variety	O
of	O
machine	O
learning	O
approaches	O
to	O
predict	O
both	O
latitude	O
and	O
longitude	O
.	O
The	O
models	O
included	O
in	O
our	O
ensemble	O
range	O
from	O
simple	O
regression	O
techniques	O
,	O
such	O
as	O
Support	O
Vector	O
Regression	O
,	O
to	O
deep	O
neural	O
models	O
,	O
such	O
as	O
a	O
hybrid	O
neural	O
network	O
and	O
a	O
neural	O
transformer	O
.	O
To	O
minimize	O
the	O
prediction	O
error	O
,	O
we	O
approach	O
the	O
problem	O
from	O
a	O
few	O
different	O
perspectives	O
and	O
consider	O
various	O
types	O
of	O
features	O
,	O
from	O
lowlevel	O
character	O
n	O
-	O
grams	O
to	O
high	O
-	O
level	O
BERT	B-MethodName
embeddings	O
.	O
The	O
XGBoost	O
ensemble	O
resulted	O
from	O
combining	O
the	O
power	O
of	O
the	O
aforementioned	O
methods	O
achieves	O
a	O
median	O
distance	O
of	O
23.6	O
km	O
on	O
the	O
test	O
data	O
,	O
which	O
places	O
us	O
on	O
the	O
third	O
place	O
in	O
the	O
ranking	O
,	O
at	O
a	O
difference	O
of	O
6.05	O
km	O
and	O
2.9	O
km	O
from	O
the	O
submissions	O
on	O
the	O
first	O
and	O
second	O
places	O
,	O
respectively	O
.	O

The	O
Social	O
Media	O
Variety	O
Geolocation	O
(	O
SMG	O
)	O
task	O
was	O
proposed	O
,	O
for	O
the	O
second	O
year	O
consecutively	O
,	O
in	O
the	O
2021	O
edition	O
of	O
the	O
VarDial	O
Evaluation	O
Campaign	O
(	O
Chakravarthi	O
et	O
al	O
,	O
2021	O
)	O
.	O
This	O
task	O
is	O
aimed	O
at	O
geolocation	O
prediction	O
based	O
on	O
short	O
text	O
messages	O
exchanged	O
by	O
the	O
users	O
of	O
social	O
media	O
platforms	O
such	O
as	O
Twitter	O
or	O
Jodel	O
.	O
The	O
location	O
from	O
where	O
a	O
short	O
text	O
was	O
posted	O
on	O
a	O
certain	O
social	O
media	O
platform	O
is	O
expressed	O
by	O
two	O
components	O
:	O
the	O
latitude	O
and	O
the	O
longitude	O
.	O
Naturally	O
,	O
the	O
geolocation	O
task	O
is	O
formulated	O
as	O
a	O
double	O
regression	O
problem	O
.	O
Twitter	O
and	O
Jodel	O
are	O
the	O
platforms	O
used	O
for	O
data	O
collection	O
,	O
and	O
similar	O
to	O
the	O
previous	O
spin	O
of	O
SMG	O
at	O
VarDial	O
2020	O
(	O
Gȃman	O
et	O
al	O
,	O
2020	O
)	O
,	O
the	O
task	O
is	O
divided	O
into	O
three	O
subtasks	O
,	O
by	O
language	O
area	O
,	O
namely	O
:	O
Standard	O
German	O
Jodels	O
(	O
DE	O
-	O
AT	O
)	O
-	O
which	O
targets	O
conversations	O
initiated	O
in	O
Germany	O
and	O
Austria	O
in	O
regional	O
dialectal	O
forms	O
(	O
Hovy	O
and	O
Purschke	O
,	O
2018	O
)	O
.	O
Swiss	O
German	O
Jodels	O
(	O
CH	O
)	O
-	O
containing	O
a	O
smaller	O
number	O
of	O
Jodel	O
conversations	O
from	O
the	O
German	O
speaking	O
half	O
of	O
Switzerland	O
(	O
Hovy	O
and	O
Purschke	O
,	O
2018	O
)	O
.	O
BCMS	O
Tweets	O
-	O
from	O
the	O
area	O
of	O
Bosnia	O
and	O
Herzegovina	O
,	O
Croatia	O
,	O
Montenegro	O
and	O
Serbia	O
where	O
the	O
macro	O
-	O
language	O
is	O
BCMS	O
,	O
with	O
both	O
similarities	O
and	O
a	O
fair	O
share	O
of	O
variation	O
among	O
the	O
component	O
languages	O
(	O
Ljubešić	O
et	O
al	O
,	O
2016	O
)	O
.	O
The	O
focus	O
of	O
our	O
work	O
falls	O
only	O
on	O
the	O
second	O
subtask	O
,	O
SMG	O
-	O
CH	O
,	O
tackled	O
via	O
a	O
variety	O
of	O
handcrafted	O
and	O
deep	O
learning	O
models	O
.	O
We	O
propose	O
a	O
single	O
ensemble	O
model	O
joining	O
the	O
power	O
of	O
several	O
individual	O
models	O
through	O
meta	B-TaskName
-	I-TaskName
learning	I-TaskName
based	O
on	O
Extreme	O
Gradient	O
Boosting	O
(	O
XGBoost	O
)	O
(	O
Chen	O
and	O
Guestrin	O
,	O
2016	O
)	O
.	O
We	O
trained	O
two	O
independent	O
ensemble	O
models	O
,	O
each	O
predicting	O
one	O
of	O
the	O
components	O
that	O
form	O
the	O
geographical	O
coordinates	O
(	O
latitude	O
and	O
longitude	O
)	O
.	O
The	O
first	O
model	O
plugged	O
into	O
our	O
meta	O
-	O
learner	O
is	O
a	O
Support	O
Vector	O
Regression	O
(	O
SVR	O
)	O
model	O
(	O
Chang	O
and	O
Lin	O
,	O
2002	O
)	O
based	O
on	O
string	O
kernels	O
.	O
Previous	O
usage	O
in	O
dialect	B-TaskName
identification	I-TaskName
has	O
proved	O
the	O
efficiency	O
of	O
this	O
technique	O
in	O
the	O
task	O
of	O
interest	O
(	O
Butnaru	O
and	O
Ionescu	O
,	O
2018b	O
;	O
Gȃman	O
and	O
Ionescu	O
,	O
2020	O
;	O
Ionescu	O
and	O
Butnaru	O
,	O
2017	O
;	O
.	O
The	O
second	O
model	O
included	O
in	O
the	O
ensemble	O
is	O
a	O
hybrid	O
convolutional	O
neural	O
network	O
(	O
CNN	O
)	O
(	O
Liang	O
et	O
al	O
,	O
2017	O
)	O
that	O
combines	O
,	O
in	O
the	O
same	O
architecture	O
,	O
character	O
-	O
level	O
(	O
Zhang	O
et	O
al	O
,	O
2015	O
)	O
and	O
word	O
-	O
level	O
representations	O
.	O
The	O
ability	O
of	O
capturing	O
morphological	O
relationships	O
at	O
the	O
character	O
level	O
and	O
using	O
them	O
as	O
features	O
for	O
CNNs	O
is	O
also	O
known	O
to	O
give	O
promising	O
results	O
in	O
dialect	B-TaskName
identification	I-TaskName
Tudoreanu	O
,	O
2019	O
)	O
.	O
Different	O
from	O
works	O
using	O
solely	O
character	O
-	O
level	O
CNNs	O
for	O
dialect	B-TaskName
identification	I-TaskName
Tudoreanu	O
,	O
2019	O
)	O
,	O
we	O
believe	O
that	O
the	O
addition	O
of	O
words	O
might	O
bring	O
the	O
benefit	O
of	O
learning	O
dialectspecific	O
multi	O
-	O
word	O
expressions	O
that	O
are	O
hard	O
to	O
capture	O
at	O
the	O
character	O
level	O
(	O
Dhingra	O
et	O
al	O
,	O
2016	O
;	O
Ling	O
et	O
al	O
,	O
2015	O
)	O
.	O
Bidirectional	O
Encoder	O
Representations	O
from	O
Transformers	O
(	O
BERT	B-MethodName
)	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
is	O
a	O
top	O
performing	O
technique	O
used	O
in	O
recent	O
years	O
for	O
solving	O
mainstream	O
NLP	O
problems	O
.	O
Thus	O
,	O
it	O
seems	O
fit	O
to	O
also	O
include	O
the	O
outputs	O
of	O
a	O
fine	O
-	O
tuned	O
German	O
version	O
of	O
BERT	B-MethodName
in	O
our	O
XGBoost	O
meta	O
-	O
learner	O
.	O
We	O
conducted	O
experiments	O
on	O
the	O
development	O
set	O
provided	O
by	O
the	O
shared	O
task	O
organizers	O
(	O
Hovy	O
and	O
Purschke	O
,	O
2018	O
)	O
in	O
order	O
to	O
decide	O
which	O
model	O
to	O
choose	O
as	O
our	O
submission	O
for	O
the	O
SMG	O
-	O
CH	O
subtask	O
.	O
Our	O
results	O
indicate	O
that	O
the	O
ensemble	O
model	O
attains	O
the	O
best	O
performance	O
.	O
With	O
median	O
distances	O
that	O
are	O
5	O
-	O
6	O
km	O
higher	O
,	O
all	O
the	O
other	O
models	O
,	O
tested	O
individually	O
on	O
the	O
development	O
set	O
,	O
provide	O
slightly	O
worse	O
predictions	O
.	O
The	O
remainder	O
of	O
this	O
paper	O
is	O
organized	O
as	O
follows	O
.	O
We	O
present	O
related	O
work	O
on	O
dialect	B-TaskName
identification	I-TaskName
and	O
geolocation	O
of	O
short	O
texts	O
in	O
Section	O
2	O
.	O
Our	O
approach	O
is	O
described	O
in	O
detail	O
in	O
Section	O
3	O
.	O
We	O
present	O
the	O
experiments	O
and	O
empirical	O
results	O
in	O
Section	O
4	O
.	O
Finally	O
,	O
our	O
conclusions	O
are	O
drawn	O
in	O
Section	O
5	O
.	O

Our	O
study	O
of	O
the	O
related	O
work	O
starts	O
with	O
a	O
brief	O
overview	O
of	O
geotagging	O
based	O
on	O
text	O
.	O
Then	O
,	O
we	O
look	O
at	O
more	O
specific	O
methods	O
studying	O
geotagging	O
in	O
social	O
media	O
and	O
we	O
also	O
investigate	O
the	O
amount	O
of	O
research	O
done	O
,	O
from	O
a	O
computational	O
linguistics	O
perspective	O
,	O
in	O
geotagging	O
by	O
dialect	O
,	O
with	O
focus	O
on	O
last	O
year	O
's	O
approaches	O
for	O
the	O
same	O
subtask	O
,	O
namely	O
SMG	O
-	O
CH	O
at	O
VarDial	O
2020	O
.	O
Text	O
-	O
based	O
geotagging	O
.	O
The	O
works	O
based	O
on	O
text	O
to	O
perform	O
geotagging	O
can	O
be	O
divided	O
into	O
three	O
generic	O
categories	O
by	O
the	O
approaches	O
taken	O
in	O
order	O
to	O
predict	O
location	O
.	O
The	O
first	O
type	O
of	O
approach	O
relies	O
mainly	O
on	O
gazetteers	O
as	O
the	O
source	O
of	O
the	O
location	O
mappings	O
.	O
This	O
tool	O
is	O
adopted	O
in	O
a	O
number	O
of	O
works	O
(	O
Cheng	O
et	O
al	O
,	O
2010	O
;	O
Quercini	O
et	O
al	O
,	O
2010	O
)	O
,	O
from	O
ruled	O
-	O
based	O
methods	O
(	O
Bilhaut	O
et	O
al	O
,	O
2003	O
)	O
to	O
various	O
machine	O
learning	O
techniques	O
that	O
rely	O
on	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
Ding	O
et	O
al	O
,	O
2000	O
;	O
Gelernter	O
and	O
Mushegian	O
,	O
2011	O
;	O
Qin	O
et	O
al	O
,	O
2010	O
)	O
.	O
This	O
category	O
of	O
methods	O
brings	O
the	O
disadvantage	O
of	O
relying	O
on	O
specific	O
mentions	O
of	O
locations	O
in	O
text	O
,	O
rather	O
than	O
inferring	O
them	O
in	O
a	O
less	O
straightforward	O
manner	O
.	O
These	O
direct	O
mentions	O
of	O
places	O
are	O
not	O
a	O
safe	O
assumption	O
,	O
especially	O
when	O
it	O
comes	O
to	O
social	O
media	O
platforms	O
which	O
represent	O
the	O
data	O
source	O
in	O
some	O
of	O
these	O
studies	O
(	O
Cheng	O
et	O
al	O
,	O
2010	O
)	O
.	O
The	O
other	O
two	O
main	O
categories	O
of	O
approaches	O
for	O
text	O
-	O
based	O
geolocation	O
rely	O
on	O
either	O
supervised	O
(	O
Kinsella	O
et	O
al	O
,	O
2011	O
;	O
Wing	O
and	O
Baldridge	O
,	O
2011	O
)	O
or	O
unsupervised	O
(	O
Ahmed	O
et	O
al	O
,	O
2013	O
;	O
Eisenstein	O
et	O
al	O
,	O
2010	O
;	O
Hong	O
et	O
al	O
,	O
2012	O
)	O
learning	O
.	O
The	O
latter	O
methods	O
usually	O
employ	O
clustering	O
techniques	O
based	O
on	O
topic	B-TaskName
models	I-TaskName
.	O
Geolocation	O
in	O
social	O
media	O
.	O
A	O
number	O
of	O
works	O
(	O
Rout	O
et	O
al	O
,	O
2013	O
)	O
look	O
at	O
this	O
task	O
from	O
a	O
supervised	O
learning	O
perspective	O
.	O
However	O
,	O
in	O
these	O
studies	O
,	O
other	O
details	O
(	O
e.g.	O
social	O
ties	O
)	O
in	O
the	O
users	O
profile	O
are	O
considered	O
rather	O
than	O
their	O
written	O
content	O
.	O
Other	O
works	O
in	O
this	O
area	O
are	O
related	O
to	O
our	O
current	O
interest	O
in	O
studying	O
language	O
variation	O
for	O
the	O
geolocation	O
of	O
social	O
media	O
posts	O
(	O
Doyle	O
,	O
2014	O
;	O
Eisenstein	O
et	O
al	O
,	O
2010	O
;	O
Han	O
et	O
al	O
,	O
2014	O
;	O
Rahimi	O
et	O
al	O
,	O
2017	O
;	O
Roller	O
et	O
al	O
,	O
2012	O
)	O
.	O
Among	O
these	O
,	O
various	O
machine	O
learning	O
techniques	O
are	O
employed	O
in	O
location	O
prediction	O
,	O
ranging	O
from	O
probabilistic	O
graphical	O
models	O
(	O
Eisenstein	O
et	O
al	O
,	O
2010	O
)	O
and	O
adaptive	O
grid	O
search	O
(	O
Roller	O
et	O
al	O
,	O
2012	O
)	O
to	O
Bayesian	O
methods	O
(	O
Doyle	O
,	O
2014	O
)	O
and	O
neural	O
networks	O
(	O
Rahimi	O
et	O
al	O
,	O
2017	O
)	O
.	O
Dialect	O
-	O
based	O
geolocation	O
.	O
Many	O
dialects	O
are	O
covered	O
in	O
the	O
text	O
-	O
based	O
geotagging	O
research	O
to	O
date	O
,	O
including	O
Dutch	O
(	O
Wieling	O
et	O
al	O
,	O
2011	O
)	O
,	O
British	O
(	O
Szmrecsanyi	O
,	O
2008	O
)	O
,	O
American	O
(	O
Eisenstein	O
et	O
al	O
,	O
2010	O
;	O
Huang	O
et	O
al	O
,	O
2016	O
)	O
and	O
African	O
American	O
Vernacular	O
English	O
(	O
Jones	O
,	O
2015	O
)	O
.	O
Out	O
of	O
all	O
the	O
languages	O
that	O
were	O
subject	O
to	O
location	O
detection	O
by	O
dialect	O
,	O
we	O
are	O
interested	O
in	O
German	O
.	O
In	O
this	O
direction	O
,	O
the	O
study	O
that	O
is	O
the	O
most	O
relevant	O
to	O
our	O
work	O
is	O
that	O
of	O
Hovy	O
and	O
Purschke	O
(	O
2018	O
)	O
which	O
targets	O
the	O
German	O
language	O
and	O
its	O
variations	O
.	O
Approximately	O
16.8	O
million	O
online	O
posts	O
from	O
the	O
German	O
-	O
speaking	O
area	O
of	O
Europe	O
are	O
employed	O
in	O
this	O
study	O
with	O
the	O
aim	O
of	O
learning	O
document	O
representations	O
of	O
cities	O
.	O
A	O
small	O
fraction	O
of	O
these	O
posts	O
are	O
Jodels	O
collected	O
from	O
the	O
German	O
speaking	O
side	O
of	O
Switzerland	O
,	O
and	O
these	O
are	O
also	O
used	O
in	O
the	O
SMG	O
-	O
CH	O
subtask	O
that	O
we	O
are	O
addressing	O
in	O
this	O
paper	O
.	O
The	O
assumption	O
here	O
is	O
that	O
the	O
methods	O
should	O
manage	O
to	O
capture	O
enough	O
regional	O
variations	O
in	O
the	O
written	O
language	O
,	O
which	O
can	O
serve	O
as	O
the	O
means	O
to	O
automatically	O
distinguish	O
the	O
geographical	O
region	O
of	O
social	O
media	O
posts	O
.	O
The	O
verification	O
performed	O
in	O
this	O
direction	O
in	O
the	O
original	O
paper	O
(	O
Hovy	O
and	O
Purschke	O
,	O
2018	O
)	O
used	O
clustering	O
to	O
determine	O
larger	O
regions	O
covering	O
a	O
given	O
dialect	O
.	O
However	O
,	O
given	O
the	O
shared	O
task	O
formulation	O
,	O
we	O
take	O
a	O
different	O
approach	O
and	O
use	O
the	O
provided	O
data	O
in	O
a	O
double	O
regression	O
setup	O
,	O
addressing	O
the	O
problem	O
both	O
from	O
a	O
shallow	O
and	O
a	O
deep	O
learning	O
perspective	O
.	O
As	O
previously	O
mentioned	O
,	O
this	O
is	O
the	O
second	O
consecutive	O
year	O
in	O
which	O
SMG	O
-	O
CH	O
is	O
featured	O
at	O
Var	O
-	O
Dial	O
(	O
Chakravarthi	O
et	O
al	O
,	O
2021	O
)	O
,	O
with	O
a	O
similar	O
format	O
,	O
although	O
an	O
updated	O
data	O
set	O
.	O
The	O
participants	O
of	O
the	O
2020	O
SMG	O
-	O
CH	O
shared	O
task	O
(	O
Gȃman	O
et	O
al	O
,	O
2020	O
)	O
studied	O
this	O
task	O
from	O
a	O
variety	O
of	O
angles	O
.	O
Some	O
techniques	O
are	O
based	O
on	O
deep	O
neural	O
networks	O
such	O
as	O
the	O
popular	O
BERT	B-MethodName
architecture	O
(	O
Gȃman	O
and	O
Ionescu	O
,	O
2020	O
;	O
Scherrer	O
and	O
Ljubešić	O
,	O
2020	O
)	O
,	O
bidirectional	O
Long	B-MethodName
Short	I-MethodName
-	I-MethodName
Term	I-MethodName
Memory	I-MethodName
(	O
LSTM	B-MethodName
)	O
networks	O
applied	O
on	O
FastText	B-MethodName
embeddings	O
(	O
Mishra	O
,	O
2020	O
)	O
or	O
character	O
-	O
level	O
CNNs	O
(	O
Gȃman	O
and	O
Ionescu	O
,	O
2020	O
)	O
.	O
Other	O
techniques	O
are	O
based	O
on	O
shallow	O
or	O
handcrafted	O
features	O
such	O
as	O
a	O
gridbased	O
prediction	O
using	O
an	O
n	O
-	O
gram	O
language	O
model	O
(	O
Jauhiainen	O
et	O
al	O
,	O
2020	O
)	O
or	O
a	O
clustering	O
technique	O
that	O
shifts	O
the	O
problem	O
into	O
a	O
discrete	O
space	O
,	O
then	O
uses	O
an	O
SVM	B-MethodName
for	O
the	O
classification	O
of	O
posts	O
into	O
regions	O
.	O
Our	O
best	O
submission	O
(	O
Gȃman	O
and	O
Ionescu	O
,	O
2020	O
)	O
in	O
last	O
year	O
's	O
campaign	O
was	O
an	O
ensemble	O
based	O
on	O
XGBoost	O
as	O
meta	O
-	O
learner	O
over	O
the	O
predictions	O
of	O
three	O
different	O
models	O
:	O
ν	O
-	O
SVR	O
with	O
string	O
kernels	O
,	O
a	O
character	O
-	O
level	O
CNN	O
and	O
an	O
LSTM	B-MethodName
based	O
on	O
BERT	B-MethodName
embeddings	O
.	O
This	O
year	O
,	O
we	O
acknowledge	O
that	O
our	O
deep	O
learning	O
models	O
had	O
sub	O
-	O
optimal	O
results	O
in	O
our	O
previous	O
participation	O
at	O
SMG	O
-	O
CH	O
.	O
Consequently	O
,	O
for	O
this	O
year	O
,	O
we	O
bring	O
stronger	O
neural	O
networks	O
into	O
the	O
XGBoost	O
ensemble	O
.	O
Instead	O
of	O
using	O
an	O
LSTM	B-MethodName
with	O
BERT	B-MethodName
embeddings	O
,	O
we	O
finetune	O
the	O
cased	O
version	O
of	O
German	O
BERT	B-MethodName
and	O
use	O
it	O
directly	O
for	O
double	O
regression	O
in	O
a	O
setup	O
that	O
is	O
more	O
suitable	O
to	O
the	O
data	O
set	O
size	O
,	O
compared	O
to	O
our	O
previous	O
endeavour	O
.	O
Instead	O
of	O
a	O
character	O
-	O
level	O
CNN	O
,	O
we	O
use	O
a	O
hybrid	O
CNN	O
which	O
learns	O
end	O
-	O
to	O
-	O
end	O
rep	O
-	O
resentations	O
for	O
both	O
words	O
and	O
characters	O
.	O
The	O
aforementioned	O
deep	O
learning	O
models	O
are	O
plugged	O
into	O
an	O
XGBoost	O
ensemble	O
alongside	O
a	O
retrained	O
version	O
of	O
the	O
ν	O
-	O
SVR	O
model	O
employed	O
in	O
our	O
past	O
participation	O
(	O
Gȃman	O
and	O
Ionescu	O
,	O
2020	O
)	O
.	O
Our	O
current	O
changes	O
introduce	O
a	O
significant	O
improvement	O
in	O
median	O
distance	O
compared	O
to	O
our	O
previous	O
results	O
,	O
on	O
a	O
similar	O
(	O
and	O
perhaps	O
more	O
challenging	O
)	O
data	O
set	O
.	O

Transformers	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
represent	O
an	O
important	O
advance	O
in	O
NLP	O
,	O
with	O
many	O
benefits	O
over	O
the	O
traditional	O
sequential	O
neural	O
architectures	O
.	O
Based	O
on	O
an	O
encoder	O
-	O
decoder	O
architecture	O
with	O
attention	O
,	O
transformers	O
proved	O
to	O
be	O
better	O
at	O
modeling	O
long	O
-	O
term	O
dependencies	O
in	O
sequences	O
,	O
while	O
being	O
effectively	O
trained	O
as	O
the	O
sequential	O
dependency	O
of	O
previous	O
tokens	O
is	O
removed	O
.	O
Unlike	O
other	O
contemporary	O
attempts	O
at	O
using	O
transformers	O
in	O
language	O
modeling	O
(	O
Radford	O
et	O
al	O
,	O
2018	O
)	O
,	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
builds	O
deep	O
language	O
representations	O
in	O
a	O
self	O
-	O
supervised	O
fashion	O
and	O
incorporates	O
context	O
from	O
both	O
directions	O
.	O
The	O
masked	B-TaskName
language	I-TaskName
modeling	I-TaskName
technique	O
enables	O
BERT	B-MethodName
to	O
pretrain	O
these	O
deep	O
bidirectional	O
representations	O
,	O
that	O
can	O
be	O
further	O
fine	O
-	O
tuned	O
and	O
adapted	O
for	O
a	O
variety	O
of	O
downstream	O
tasks	O
,	O
without	O
significant	O
architectural	O
updates	O
.	O
We	O
also	O
make	O
use	O
of	O
this	O
property	O
in	O
the	O
current	O
work	O
,	O
employing	O
the	O
Hugging	O
Face	O
(	O
Wolf	O
et	O
al	O
,	O
2020	O
)	O
version	O
of	O
the	O
cased	O
German	O
BERT	B-MethodName
model	O
1	O
.	O
The	O
model	O
was	O
initially	O
trained	O
on	O
the	O
latest	O
German	O
Wikipedia	O
dump	O
,	O
the	O
OpenLe	O
-	O
galData	O
dump	O
and	O
a	O
collection	O
of	O
news	O
articles	O
,	O
summing	O
up	O
to	O
a	O
total	O
of	O
12	O
GB	O
of	O
text	O
files	O
.	O
We	O
fine	O
-	O
tune	O
this	O
pre	O
-	O
trained	O
German	O
BERT	B-MethodName
model	O
for	O
the	O
geolocation	O
of	O
Swiss	O
German	O
short	O
texts	O
,	O
in	O
a	O
regression	O
setup	O
.	O
The	O
choice	O
of	O
hyperparameters	O
is	O
,	O
in	O
part	O
,	O
inspired	O
by	O
the	O
winning	O
system	O
in	O
the	O
last	O
year	O
's	O
SMG	O
-	O
CH	O
subtask	O
at	O
VarDial	O
(	O
Scherrer	O
and	O
Ljubešić	O
,	O
2020	O
)	O
.	O

We	O
train	O
three	O
different	O
models	O
which	O
rely	O
on	O
different	O
learning	O
methods	O
and	O
types	O
of	O
features	O
to	O
perform	O
the	O
required	O
double	O
regression	O
.	O
Thus	O
,	O
we	O
have	O
the	O
hybrid	O
CNN	O
relying	O
on	O
both	O
words	O
and	O
characters	O
as	O
features	O
,	O
the	O
shallow	O
ν	O
-	O
SVR	O
based	O
on	O
string	O
kernels	O
and	O
three	O
fine	O
-	O
tuned	O
German	O
BERT	B-MethodName
models	O
looking	O
at	O
higher	O
-	O
level	O
features	O
and	O
understanding	O
dependencies	O
in	O
a	O
bidirectional	O
manner	O
.	O
Table	O
1	O
shows	O
the	O
preliminary	O
results	O
obtained	O
on	O
the	O
development	O
set	O
by	O
each	O
individual	O
model	O
as	O
well	O
as	O
the	O
results	O
of	O
the	O
submitted	O
XGBoost	O
ensemble	O
.	O
The	O
individual	O
models	O
provide	O
quite	O
similar	O
results	O
in	O
terms	O
of	O
the	O
median	O
distance	O
between	O
the	O
predicted	O
and	O
ground	O
-	O
truth	O
locations	O
.	O
These	O
results	O
stay	O
around	O
a	O
value	O
of	O
30	O
km	O
for	O
the	O
median	O
distance	O
and	O
35	O
km	O
for	O
the	O
mean	O
distance	O
.	O
Among	O
the	O
independent	O
models	O
,	O
the	O
hybrid	O
CNN	O
obtains	O
slightly	O
better	O
results	O
in	O
terms	O
of	O
the	O
median	O
distance	O
(	O
30.05	O
km	O
)	O
,	O
whereas	O
the	O
second	O
attempt	O
at	O
fine	O
-	O
tuning	O
BERT	B-MethodName
gives	O
the	O
worst	O
distances	O
,	O
namely	O
33.86	O
km	O
for	O
the	O
median	O
distance	O
and	O
38.85	O
km	O
for	O
the	O
mean	O
distance	O
.	O
ν	O
-	O
SVR	O
surpasses	O
all	O
the	O
other	O
models	O
,	O
by	O
a	O
small	O
margin	O
,	O
in	O
terms	O
of	O
the	O
mean	O
distance	O
(	O
34.82	O
km	O
)	O
.	O
The	O
results	O
of	O
the	O
submitted	O
XGBoost	O
ensemble	O
model	O
stand	O
proof	O
that	O
our	O
intuition	O
was	O
correct	O
,	O
namely	O
that	O
all	O
these	O
individual	O
models	O
have	O
the	O
potential	O
to	O
complement	O
each	O
other	O
if	O
put	O
together	O
in	O
an	O
ensemble	O
.	O
Indeed	O
,	O
the	O
submitted	O
system	O
clearly	O
surpasses	O
the	O
best	O
individual	O
model	O
by	O
approximately	O
5	O
km	O
in	O
terms	O
of	O
both	O
the	O
median	O
and	O
the	O
mean	O
distance	O
metrics	O
.	O

In	O
this	O
paper	O
,	O
we	O
proposed	O
an	O
ensemble	B-TaskName
learning	I-TaskName
model	O
for	O
the	O
geolocation	O
of	O
Swiss	O
German	O
social	O
media	O
posts	O
.	O
The	O
ensemble	O
is	O
based	O
on	O
an	O
XGBoost	O
meta	O
-	O
learner	O
applied	O
on	O
top	O
of	O
three	O
individual	O
models	O
:	O
a	O
hybrid	O
CNN	O
,	O
an	O
approach	O
based	O
on	O
string	O
kernels	O
and	O
a	O
fine	O
-	O
tuned	O
German	O
BERT	B-MethodName
model	O
.	O
Given	O
the	O
final	O
results	O
obtained	O
in	O
the	O
SMG	O
-	O
CH	O
subtask	O
,	O
we	O
conclude	O
that	O
predicting	O
the	O
location	O
of	O
Swiss	O
German	O
social	O
media	O
posts	O
is	O
a	O
challenging	O
task	O
,	O
the	O
median	O
distance	O
being	O
higher	O
than	O
20	O
km	O
.	O
Using	O
external	O
data	O
sources	O
to	O
build	O
a	O
language	O
model	O
seems	O
to	O
be	O
a	O
more	O
promising	O
path	O
towards	O
success	O
,	O
as	O
shown	O
by	O
the	O
final	O
standings	O
of	O
the	O
VarDial	O
2020	O
(	O
Gȃman	O
et	O
al	O
,	O
2020	O
)	O
and	O
2021	O
(	O
Chakravarthi	O
et	O
al	O
,	O
2021	O
SMG	O
shared	O
tasks	O
.	O
In	O
future	O
work	O
,	O
we	O
aim	O
to	O
study	O
the	O
applicability	O
of	O
our	O
ensemble	O
on	O
other	O
geolocation	O
tasks	O
,	O
perhaps	O
taking	O
into	O
consideration	O
future	O
VarDial	O
challenges	O
.	O

BERT	B-MethodName
Prescriptions	O
to	O
Avoid	O
Unwanted	O
Headaches	O
:	O
A	O
Comparison	O
of	O
Transformer	B-MethodName
Architectures	O
for	O
Adverse	O
Drug	O
Event	B-TaskName
Detection	I-TaskName

Pretrained	O
transformer	O
-	O
based	O
models	O
,	O
such	O
as	O
BERT	B-MethodName
and	O
its	O
variants	O
,	O
have	O
become	O
a	O
common	O
choice	O
to	O
obtain	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performances	O
in	O
NLP	O
tasks	O
.	O
In	O
the	O
identification	O
of	O
Adverse	O
Drug	O
Events	O
(	O
ADE	O
)	O
from	O
social	O
media	O
texts	O
,	O
for	O
example	O
,	O
BERT	B-MethodName
architectures	O
rank	O
first	O
in	O
the	O
leaderboard	O
.	O
However	O
,	O
a	O
systematic	O
comparison	O
between	O
these	O
models	O
has	O
not	O
yet	O
been	O
done	O
.	O
In	O
this	O
paper	O
,	O
we	O
aim	O
at	O
shedding	O
light	O
on	O
the	O
differences	O
between	O
their	O
performance	O
analyzing	O
the	O
results	O
of	O
12	O
models	O
,	O
tested	O
on	O
two	O
standard	O
benchmarks	O
.	O
SpanBERT	O
and	O
PubMedBERT	O
emerged	O
as	O
the	O
best	O
models	O
in	O
our	O
evaluation	O
:	O
this	O
result	O
clearly	O
shows	O
that	O
span	O
-	O
based	O
pretraining	O
gives	O
a	O
decisive	O
advantage	O
in	O
the	O
precise	O
recognition	O
of	O
ADEs	O
,	O
and	O
that	O
in	O
-	O
domain	O
language	O
pretraining	O
is	O
particularly	O
useful	O
when	O
the	O
transformer	O
model	O
is	O
trained	O
just	O
on	O
biomedical	O
text	O
from	O
scratch	O
.	O

The	O
identification	O
of	O
Adverse	O
Drug	O
Events	O
(	O
ADEs	O
)	O
from	O
text	O
recently	O
attracted	O
a	O
lot	O
of	O
attention	O
in	O
the	O
NLP	O
community	O
.	O
On	O
the	O
one	O
hand	O
,	O
it	O
represents	O
a	O
challenge	O
even	O
for	O
the	O
most	O
advanced	O
NLP	O
technologies	O
,	O
since	O
mentions	O
of	O
ADEs	O
can	O
be	O
found	O
in	O
different	O
varieties	O
of	O
online	O
text	O
and	O
present	O
unconventional	O
linguistic	O
features	O
(	O
they	O
may	O
involve	O
specialized	O
language	O
,	O
or	O
consist	O
of	O
discontinuous	O
spans	O
of	O
tokens	O
etc	O
.	O
)	O
(	O
Dai	O
,	O
2018	O
)	O
.	O
On	O
the	O
other	O
hand	O
,	O
the	O
task	O
has	O
an	O
industrial	O
application	O
of	O
primary	O
importance	O
in	O
the	O
field	O
of	O
digital	O
pharmacovigilance	O
Karimi	O
et	O
al	O
,	O
2015b	O
)	O
.	O
This	O
raising	O
interest	O
is	O
attested	O
,	O
for	O
example	O
,	O
by	O
the	O
ACL	O
workshop	O
series	O
on	O
Social	O
Media	O
Health	O
Mining	O
(	O
SMM4H	B-DatasetName
)	O
,	O
in	O
which	O
shared	O
tasks	O
on	O
ADE	O
detection	O
have	O
been	O
regularly	O
organized	O
since	O
2016	O
(	O
Paul	O
et	O
al	O
,	O
2016	O
;	O
Sarker	O
and	O
Gonzalez	O
-	O
Hernandez	O
,	O
2017	O
;	O
Weissenbacher	O
et	O
al	O
,	O
2018Weissenbacher	O
et	O
al	O
,	O
,	O
2019	O
.	O
With	O
the	O
recent	O
introduction	O
of	O
Transformers	O
architectures	O
and	O
their	O
impressive	O
achievements	O
in	O
NLP	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
;	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
it	O
is	O
not	O
surprising	O
that	O
these	O
tools	O
have	O
become	O
a	O
common	O
choice	O
for	O
the	O
researchers	O
working	O
in	O
the	O
area	O
.	O
The	O
contribution	O
of	O
this	O
paper	O
is	O
a	O
comparison	O
between	O
different	O
Transformers	O
on	O
ADE	O
detection	O
,	O
in	O
order	O
to	O
understand	O
which	O
one	O
is	O
the	O
most	O
appropriate	O
for	O
tackling	O
the	O
task	O
.	O
Shared	O
tasks	O
are	O
not	O
the	O
best	O
scenario	O
for	O
addressing	O
this	O
question	O
,	O
since	O
the	O
wide	O
range	O
of	O
differences	O
in	O
the	O
architectures	O
(	O
which	O
could	O
include	O
,	O
for	O
example	O
,	O
ensembles	O
of	O
Transformers	O
and	O
other	O
types	O
of	O
networks	O
)	O
does	O
not	O
allow	O
a	O
comparison	O
on	O
the	O
same	O
grounds	O
.	O
In	O
our	O
view	O
,	O
two	O
key	O
questions	O
deserve	O
a	O
particular	O
attention	O
in	O
this	O
evaluation	O
.	O
First	O
,	O
whether	O
there	O
is	O
an	O
advantage	O
in	O
using	O
a	O
model	O
with	O
some	O
form	O
of	O
in	O
-	O
domain	O
language	O
pretraining	O
,	O
given	O
the	O
wide	O
availability	O
of	O
Transformers	O
for	O
the	O
biomedical	O
domain	O
(	O
Lee	O
et	O
al	O
,	O
2020	O
;	O
Gu	O
et	O
al	O
,	O
2020	O
)	O
.	O
Second	O
,	O
whether	O
a	O
model	O
trained	O
to	O
predict	O
coherent	O
spans	O
of	O
text	O
instead	O
of	O
single	O
words	O
can	O
achieve	O
a	O
better	O
performance	O
(	O
Joshi	O
et	O
al	O
,	O
2019	O
)	O
,	O
since	O
our	O
goal	O
is	O
to	O
identify	O
the	O
groups	O
of	O
tokens	O
corresponding	O
to	O
ADEs	O
as	O
precisely	O
as	O
possible	O
.	O
Two	O
models	O
that	O
we	O
introduce	O
for	O
the	O
first	O
time	O
in	O
this	O
task	O
,	O
SpanBERT	O
and	O
PubMedBERT	O
,	O
achieved	O
the	O
top	O
performance	O
.	O
The	O
former	O
takes	O
advantage	O
of	O
a	O
span	O
-	O
based	O
pretraining	O
objective	O
,	O
while	O
the	O
latter	O
shows	O
that	O
in	O
-	O
domain	O
language	O
data	O
are	O
better	O
used	O
for	O
training	O
the	O
model	O
from	O
scratch	O
,	O
without	O
any	O
general	O
-	O
domain	O
pretraining	O
.	O

Automatic	O
extraction	O
of	O
ADE	O
in	O
social	O
media	O
started	O
receiving	O
more	O
attention	O
in	O
the	O
last	O
few	O
years	O
,	O
given	O
the	O
increasing	O
number	O
of	O
users	O
that	O
discuss	O
their	O
drug	O
-	O
related	O
experiences	O
on	O
Twitter	O
and	O
similar	O
platforms	O
.	O
Studies	O
like	O
;	O
;	O
Daniulaityte	O
et	O
al	O
(	O
2016	O
)	O
were	O
among	O
the	O
first	O
to	O
propose	O
machine	O
learning	O
systems	O
for	O
the	O
detection	O
of	O
ADE	O
in	O
social	O
media	O
texts	O
,	O
using	O
traditional	O
feature	B-TaskName
engineering	I-TaskName
and	O
word	B-TaskName
embeddings	I-TaskName
-	O
based	O
approaches	O
.	O
With	O
the	O
introduction	O
of	O
the	O
SMM4H	B-DatasetName
shared	O
task	O
,	O
methods	O
based	O
on	O
neural	O
networks	O
became	O
a	O
more	O
and	O
more	O
common	O
choice	O
for	O
tackling	O
the	O
task	O
(	O
Wu	O
et	O
al	O
,	O
2018	O
;	O
Nikhil	O
and	O
Mundra	O
,	O
2018	O
)	O
,	O
and	O
finally	O
,	O
it	O
was	O
the	O
turn	O
of	O
Transformer	B-MethodName
-	O
based	O
models	O
such	O
as	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
and	O
BioBERT	O
(	O
Lee	O
et	O
al	O
,	O
2020	O
)	O
,	O
which	O
are	O
the	O
building	O
blocks	O
of	O
most	O
of	O
the	O
top	O
performing	O
systems	O
in	O
the	O
recent	O
competitions	O
Mahata	O
et	O
al	O
,	O
2019	O
;	O
Miftahutdinov	O
et	O
al	O
,	O
2019	O
)	O
.	O
At	O
the	O
same	O
time	O
,	O
the	O
task	O
has	O
been	O
independently	O
tackled	O
also	O
by	O
researchers	O
in	O
Named	B-TaskName
Entity	I-TaskName
Recognition	I-TaskName
,	O
since	O
ADE	O
detection	O
represents	O
a	O
classical	O
case	O
of	O
a	O
challenging	O
task	O
where	O
the	O
entities	O
can	O
be	O
composed	O
by	O
discontinuous	O
spans	O
of	O
text	O
(	O
Stanovsky	O
et	O
al	O
,	O
2017	O
;	O
Dai	O
et	O
al	O
,	O
2020	O
;	O
Wunnava	O
et	O
al	O
,	O
2020	O
)	O
.	O

There	O
is	O
little	O
doubt	O
that	O
Transformers	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
have	O
been	O
the	O
dominant	O
class	O
of	O
NLP	O
systems	O
in	O
the	O
last	O
few	O
years	O
.	O
The	O
"	O
golden	O
child	O
"	O
of	O
this	O
revolution	O
is	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
which	O
was	O
the	O
first	O
system	O
to	O
apply	O
the	O
bidirectional	O
training	O
of	O
a	O
Transformer	B-MethodName
to	O
a	O
language	O
modeling	O
task	O
.	O
More	O
specifically	O
,	O
BERT	B-MethodName
is	O
trained	O
with	O
a	O
Masked	B-TaskName
Language	I-TaskName
Modeling	I-TaskName
objective	O
:	O
random	O
words	O
in	O
the	O
input	O
sentences	O
are	O
replaced	O
by	O
a	O
[	O
MASK	O
]	O
token	O
and	O
the	O
model	O
attempts	O
to	O
predict	O
the	O
masked	O
token	O
based	O
on	O
the	O
surrounding	O
context	O
.	O
Following	O
BERT	B-MethodName
's	O
success	O
,	O
several	O
similar	O
architectures	O
have	O
been	O
introduced	O
in	O
biomedical	O
NLP	O
,	O
proposing	O
different	O
forms	O
of	O
in	O
-	O
domain	O
training	O
or	O
using	O
different	O
corpora	O
(	O
Beltagy	O
et	O
al	O
,	O
2019	O
;	O
Alsentzer	O
et	O
al	O
,	O
2019	O
;	O
Lee	O
et	O
al	O
,	O
2020	O
;	O
Gu	O
et	O
al	O
,	O
2020	O
)	O
.	O
Some	O
of	O
them	O
already	O
proved	O
to	O
be	O
efficient	O
for	O
ADE	O
detection	O
:	O
for	O
example	O
,	O
the	O
top	O
system	O
of	O
the	O
SMM4H	B-DatasetName
shared	O
task	O
2019	O
is	O
based	O
on	O
an	O
ensemble	O
of	O
BioBERTs	O
(	O
Weissenbacher	O
et	O
al	O
,	O
2019	O
)	O
.	O
Another	O
potentially	O
interesting	O
addition	O
to	O
the	O
library	O
of	O
BERTs	O
for	O
ADE	O
detection	O
is	O
SpanBERT	O
(	O
Joshi	O
et	O
al	O
,	O
2019	O
)	O
.	O
During	O
the	O
training	O
of	O
Span	O
-	O
BERT	B-MethodName
,	O
random	O
contiguous	O
spans	O
of	O
tokens	O
are	O
masked	O
,	O
rather	O
than	O
individual	O
words	O
,	O
forcing	O
the	O
model	O
to	O
predict	O
the	O
full	O
span	O
from	O
the	O
tokens	O
at	O
its	O
boundaries	O
.	O
We	O
decided	O
to	O
introduce	O
SpanBERT	O
in	O
our	O
experiments	O
because	O
longer	O
spans	O
and	O
relations	O
between	O
multiple	O
spans	O
of	O
text	O
are	O
a	O
key	O
factor	O
in	O
ADE	O
detection	O
,	O
and	O
thus	O
encoding	O
such	O
information	O
is	O
potentially	O
an	O
advantage	O
.	O
3	O
Experimental	O
Settings	O

The	O
datasets	O
chosen	O
for	O
the	O
experiments	O
are	O
two	O
widely	O
used	O
benchmarks	O
.	O
They	O
are	O
annotated	O
for	O
the	O
presence	O
of	O
ADEs	O
at	O
character	O
level	O
:	O
each	O
document	O
is	O
accompanied	O
by	O
list	O
of	O
start	O
and	O
end	O
indices	O
for	O
the	O
ADEs	O
contained	O
in	O
it	O
.	O
We	O
convert	O
these	O
annotations	O
using	O
the	O
IOB	O
annotation	O
scheme	O
for	O
the	O
tokens	O
:	O
B	O
marks	O
the	O
start	O
of	O
a	O
mention	O
,	O
I	O
and	O
O	O
the	O
tokens	O
inside	O
and	O
outside	O
a	O
mention	O
respectively	O
.	O
CADEC	O
(	O
Karimi	O
et	O
al	O
,	O
2015a	O
)	O
contains	O
1250	O
posts	O
from	O
the	O
health	O
-	O
related	O
forum	O
"	O
AskaPatient	O
"	O
,	O
annotated	O
for	O
the	O
presence	O
of	O
ADEs	O
.	O
We	O
use	O
the	O
splits	O
made	O
publicly	O
available	O
by	O
Dai	O
et	O
al	O
(	O
2020	O
)	O
.	O
SMM4H	B-DatasetName
is	O
the	O
training	O
dataset	O
for	O
Task	O
2	O
of	O
the	O
SMM4H	B-DatasetName
shared	O
task	O
2019	O
(	O
Weissenbacher	O
et	O
al	O
,	O
2019	O
)	O
.	O
It	O
contains	O
2276	O
tweets	O
which	O
mention	O
at	O
least	O
one	O
drug	O
name	O
,	O
1300	O
of	O
which	O
are	O
positive	O
for	O
the	O
presence	O
of	O
ADEs	O
while	O
the	O
other	O
976	O
are	O
negative	O
samples	O
.	O
The	O
competition	O
includes	O
a	O
blind	O
test	O
set	O
,	O
but	O
in	O
order	O
to	O
perform	O
a	O
deeper	O
analysis	O
on	O
the	O
results	O
,	O
we	O
use	O
the	O
training	O
set	O
only	O
.	O
As	O
far	O
as	O
we	O
know	O
there	O
is	O
no	O
official	O
split	O
for	O
the	O
training	O
set	O
al	O
ne	O
,	O
so	O
we	O
partitioned	O
it	O
into	O
training	O
,	O
validation	O
and	O
test	O
sets	O
(	O
60:20:20	O
)	O
,	O
maintaining	O
the	O
proportions	O
of	O
positive	O
and	O
negative	O
samples	O
.	O
This	O
split	O
and	O
the	O
code	O
for	O
all	O
the	O
experiments	O
are	O
available	O
at	O
https://github.com/AilabUdineGit/ADE	O
.	O
The	O
datasets	O
correspond	O
to	O
different	O
text	O
genres	O
:	O
the	O
tweets	O
of	O
SMM4H	B-DatasetName
are	O
mostly	O
short	O
messages	O
,	O
containing	O
informal	O
language	O
,	O
while	O
the	O
texts	O
of	O
CADEC	O
are	O
longer	O
and	O
structured	O
descriptions	O
.	O
To	O
verify	O
this	O
point	O
,	O
we	O
used	O
the	O
TEXTSTAT	O
Python	O
package	O
to	O
extract	O
some	O
statistics	O
from	O
the	O
texts	O
of	O
the	O
two	O
datasets	O
(	O
see	O
Appendix	O
A	O
)	O
.	O

For	O
all	O
of	O
the	O
BERT	B-MethodName
variants	O
,	O
we	O
take	O
into	O
account	O
two	O
versions	O
.	O
The	O
first	O
one	O
simply	O
uses	O
the	O
model	O
to	O
generate	O
a	O
sequence	O
of	O
embeddings	O
(	O
one	O
for	O
each	O
sub	O
-	O
word	O
token	O
)	O
,	O
which	O
are	O
then	O
passed	O
to	O
a	O
Linear	B-MethodName
Layer	I-MethodName
+	O
Softmax	B-MethodName
to	O
project	O
them	O
to	O
the	O
output	O
space	O
(	O
one	O
value	O
for	O
each	O
output	O
label	O
)	O
and	O
turn	O
them	O
into	O
a	O
probability	O
distribution	O
over	O
the	O
labels	O
.	O
The	O
second	O
version	O
combines	O
the	O
Transformerbased	O
model	O
with	O
a	O
Conditional	B-MethodName
Random	I-MethodName
Field	I-MethodName
(	O
CRF	B-MethodName
)	O
classifier	O
(	O
Lafferty	O
et	O
al	O
,	O
2001	O
;	O
Papay	O
et	O
al	O
,	O
2020	O
)	O
.	O
The	O
outputs	O
generated	O
by	O
the	O
first	O
version	O
become	O
the	O
input	O
of	O
a	O
CRF	B-MethodName
module	O
,	O
producing	O
another	O
sequence	O
of	O
subword	O
-	O
level	O
IOB	O
labels	O
.	O
This	O
step	O
aims	O
at	O
denoising	B-TaskName
the	O
output	O
labels	O
produced	O
by	O
the	O
previous	O
components	O
.	O
The	O
output	O
labels	O
are	O
calculated	O
for	O
sub	O
-	O
word	O
tokens	O
,	O
then	O
we	O
aggregate	O
each	O
set	O
of	O
sub	O
-	O
word	O
labels	O
{	O
i	O
}	O
into	O
a	O
word	O
label	O
L	O
using	O
the	O
first	O
rule	O
that	O
applies	O
:	O
(	O
i	O
)	O
if	O
i	O
=	O
O	O
for	O
all	O
i	O
,	O
then	O
L	O
=	O
O	O
;	O
(	O
ii	O
)	O
if	O
i	O
=	O
B	O
for	O
any	O
i	O
,	O
then	O
L	O
=	O
B	O
;	O
(	O
iii	O
)	O
if	O
i	O
=	O
I	O
for	O
any	O
i	O
,	O
then	O
L	O
=	O
I.	O
The	O
aggregated	O
output	O
is	O
a	O
sequence	O
of	O
word	O
-	O
level	O
IOB	O
labels	O
.	O

Table	O
5	O
is	O
a	O
summary	O
of	O
the	O
information	O
about	O
the	O
version	O
of	O
all	O
Transformer	B-MethodName
-	O
based	O
models	O
used	O
and	O
their	O
pretraining	O
methods	O
.	O
D	O
Detailed	O
metrics	O
of	O
all	O
the	O
models	O
(	O
Weissenbacher	O
et	O
al	O
,	O
2019	O
)	O
and	O
take	O
into	O
account	O
"	O
partial"matches	O
,	O
in	O
which	O
it	O
is	O
sufficient	O
for	O
a	O
system	O
prediction	O
to	O
partially	O
overlap	O
with	O
the	O
gold	O
annotation	O
to	O
be	O
considered	O
as	O
a	O
true	O
match	O
.	O

IUCL	O
at	O
SemEval	O
-	O
2016	O
Task	O
6	O
:	O
An	O
Ensemble	O
Model	O
for	O
Stance	B-TaskName
Detection	I-TaskName
in	O
Twitter	O

We	O
present	O
the	O
IUCL	O
system	O
,	O
based	O
on	O
supervised	O
learning	O
,	O
for	O
the	O
shared	O
task	O
on	O
stance	B-TaskName
detection	I-TaskName
.	O
Our	O
official	O
submission	O
,	O
the	O
random	O
forest	O
model	O
,	O
reaches	O
a	O
score	O
of	O
63.60	O
,	O
and	O
is	O
ranked	O
6th	O
out	O
of	O
19	O
teams	O
.	O
We	O
also	O
use	O
gradient	O
boosting	O
decision	O
trees	O
and	O
SVM	B-MethodName
and	O
merge	O
all	O
classifiers	O
into	O
an	O
ensemble	O
method	O
.	O
Our	O
analysis	O
shows	O
that	O
random	O
forest	O
is	O
good	O
at	O
retrieving	O
minority	O
classes	O
and	O
gradient	O
boosting	O
majority	O
classes	O
.	O
The	O
strengths	O
of	O
different	O
classifiers	O
wrt	O
.	O
precision	O
and	O
recall	O
complement	O
each	O
other	O
in	O
the	O
ensemble	O
.	O

Stance	B-TaskName
detection	I-TaskName
is	O
a	O
difficult	O
task	O
since	O
it	O
often	O
requires	O
reasoning	O
in	O
order	O
to	O
determine	O
whether	O
an	O
utterance	O
is	O
in	O
favor	O
of	O
or	O
against	O
a	O
specific	O
issue	O
.	O
In	O
the	O
shared	O
task	O
(	O
see	O
Mohammad	O
et	O
al	O
(	O
2016	O
)	O
for	O
details	O
about	O
the	O
shared	O
task	O
)	O
,	O
we	O
interpret	O
it	O
as	O
a	O
variant	O
of	O
sentiment	B-TaskName
analysis	I-TaskName
and	O
adopt	O
an	O
approach	O
that	O
combines	O
shallow	O
lexical	O
features	O
with	O
an	O
ensemble	O
of	O
different	O
supervised	O
machine	O
learning	O
classifiers	O
.	O
Previous	O
work	O
has	O
shown	O
that	O
using	O
"	O
arguing	O
"	O
features	O
based	O
on	O
an	O
arguing	O
lexicon	O
along	O
with	O
modal	O
verbs	O
and	O
targets	O
identified	O
via	O
syntactic	O
rules	O
(	O
Somasundaran	O
and	O
Wiebe	O
,	O
2010	O
)	O
;	O
finding	O
polarized	O
relations	O
between	O
aspects	O
and	O
topics	O
(	O
Somasundaran	O
and	O
Wiebe	O
,	O
2009	O
)	O
;	O
adding	O
semantic	O
frames	O
(	O
Hasan	O
and	O
Ng	O
,	O
2013	O
)	O
and	O
contextual	O
features	O
(	O
Anand	O
et	O
al	O
,	O
2011	O
)	O
generally	O
improve	O
results	O
.	O
Since	O
some	O
of	O
these	O
features	O
do	O
not	O
generalize	O
across	O
targets	O
(	O
Anand	O
et	O
al	O
,	O
2011	O
)	O
,	O
and	O
since	O
we	O
have	O
an	O
additional	O
challenge	O
in	O
processing	O
Twitter	O
data	O
,	O
we	O
rely	O
on	O
unigram	O
features	O
and	O
word	O
vectors	O
.	O
This	O
means	O
that	O
our	O
approach	O
is	O
incapable	O
of	O
handling	O
sarcasm	O
or	O
humor	O
.	O
Instead	O
,	O
it	O
provides	O
a	O
robust	O
basis	O
on	O
which	O
we	O
can	O
later	O
add	O
more	O
informative	O
features	O
.	O
Our	O
approach	O
consists	O
of	O
classifiers	O
with	O
a	O
bag	O
of	O
words	O
(	O
unigrams	O
)	O
or	O
with	O
word	O
vectors	O
as	O
features	O
.	O
We	O
use	O
three	O
separate	O
classifiers	O
(	O
SVMs	O
,	O
random	O
forest	O
,	O
gradient	O
boosting	O
decision	O
trees	O
)	O
and	O
an	O
ensemble	O
classifier	O
(	O
TiMBL	O
)	O
.	O
Our	O
official	O
submission	O
is	O
the	O
random	O
forest	O
classifier	O
with	O
word	O
unigrams	O
.	O

Preprocessing	O
mostly	O
consists	O
of	O
tokenization	O
.	O
During	O
tokenization	O
,	O
we	O
normalize	O
capitalization	O
,	O
and	O
all	O
punctuation	O
signs	O
are	O
separated	O
except	O
for	O
@	O
and	O
#	O
,	O
as	O
these	O
symbols	O
indicate	O
hashtags	O
and	O
handles	O
.	O
We	O
extract	O
frequency	O
counts	O
of	O
each	O
token	O
in	O
the	O
entire	O
corpus	O
and	O
in	O
each	O
stance	O
(	O
Favor	O
,	O
Against	O
,	O
None	O
)	O
per	O
target	O
for	O
use	O
in	O
the	O
feature	B-MethodName
selection	I-MethodName
process	O
.	O
We	O
experimented	O
with	O
TWEEBOPARSER	O
(	O
Kong	O
et	O
al	O
,	O
2014	O
)	O
,	O
a	O
dependency	O
parser	O
specifically	O
designed	O
for	O
Twitter	O
data	O
,	O
to	O
extract	O
dependency	O
relations	O
among	O
words	O
.	O
We	O
extract	O
POS	O
tags	O
,	O
multiword	O
expressions	O
,	O
and	O
dependency	O
triples	O
from	O
the	O
parses	O
.	O
However	O
,	O
due	O
to	O
the	O
feature	O
sparsity	O
,	O
none	O
of	O
them	O
improved	O
over	O
unigrams	O
.	O
Thus	O
,	O
they	O
are	O
not	O
used	O
in	O
the	O
final	O
systems	O
.	O

One	O
of	O
the	O
major	O
decisions	O
in	O
developing	O
a	O
machine	O
learning	O
system	O
for	O
stance	B-TaskName
detection	I-TaskName
lies	O
in	O
the	O
choice	O
of	O
features	O
and	O
of	O
feature	O
representations	O
.	O
Detecting	O
stance	O
in	O
political	O
tweets	O
can	O
be	O
regarded	O
as	O
a	O
form	O
of	O
sentiment	B-TaskName
analysis	I-TaskName
for	O
short	O
text	O
,	O
and	O
we	O
assume	O
that	O
different	O
stances	O
of	O
tweets	O
are	O
partially	O
expressed	O
by	O
the	O
choice	O
of	O
words	O
.	O
For	O
example	O
,	O
not	O
mentioning	O
any	O
words	O
that	O
express	O
a	O
polarized	O
attitude	O
indicates	O
that	O
a	O
tweet	O
is	O
most	O
likely	O
a	O
None	O
stance	O
.	O
Tweets	O
are	O
relatively	O
short	O
documents	O
,	O
we	O
use	O
bag	O
of	O
words	O
(	O
unigrams	O
)	O
since	O
in	O
this	O
case	O
bigrams	O
and	O
trigrams	O
are	O
likely	O
to	O
be	O
too	O
sparse	O
to	O
be	O
informative	O
.	O
Another	O
possibility	O
would	O
be	O
to	O
follow	O
approaches	O
in	O
sentiment	B-TaskName
analysis	I-TaskName
and	O
use	O
sentiment	O
lexicons	O
.	O
However	O
,	O
such	O
lexicons	O
are	O
normally	O
general	O
purpose	O
resources	O
,	O
and	O
domain	O
specific	O
information	O
is	O
not	O
included	O
.	O
In	O
contrast	O
,	O
we	O
need	O
such	O
domain	O
specific	O
knowledge	O
,	O
for	O
example	O
to	O
capture	O
the	O
fact	O
that	O
"	O
dear	O
lord	O
"	O
is	O
an	O
indication	O
of	O
a	O
negative	O
stance	O
towards	O
the	O
target	O
Atheism	O
while	O
it	O
may	O
have	O
a	O
different	O
meaning	O
when	O
it	O
occurs	O
for	O
the	O
target	O
Hillary	O
Clinton	O
.	O
Since	O
unigrams	O
include	O
a	O
high	O
number	O
of	O
irrelevant	O
features	O
and	O
also	O
constitute	O
a	O
rather	O
impoverished	O
representation	O
,	O
we	O
use	O
feature	B-MethodName
selection	I-MethodName
as	O
well	O
as	O
word	O
vectors	O
in	O
our	O
experiments	O
.	O
Table	O
1	O
summarizes	O
the	O
features	O
used	O
for	O
each	O
of	O
our	O
models	O
.	O
We	O
use	O
information	O
gain	O
(	O
IG	O
)	O
for	O
feature	B-MethodName
selection	I-MethodName
on	O
unigrams	O
.	O
Global	O
refers	O
to	O
global	O
features	O
(	O
see	O
section	O
2.2.3	O
)	O
.	O
The	O
three	O
classifiers	O
are	O
GBDT	O
,	O
random	O
forest	O
,	O
and	O
SVM	B-MethodName
;	O
the	O
ensemble	O
uses	O
their	O
output	O
(	O
predicted	O
label	O
and	O
its	O
probability	O
)	O
.	O

There	O
are	O
issues	O
resulting	O
from	O
the	O
large	O
number	O
of	O
bag	O
-	O
of	O
-	O
words	O
features	O
:	O
1	O
)	O
Not	O
all	O
words	O
are	O
good	O
indicators	O
for	O
stance	O
;	O
some	O
words	O
occur	O
evenly	O
across	O
the	O
data	O
set	O
.	O
2	O
)	O
Rare	O
words	O
,	O
which	O
are	O
less	O
likely	O
to	O
occur	O
in	O
the	O
test	O
data	O
,	O
do	O
not	O
contribute	O
much	O
.	O
To	O
alleviate	O
these	O
problems	O
,	O
we	O
perform	O
feature	B-MethodName
selection	I-MethodName
using	O
information	O
gain	O
(	O
IG	O
)	O
.	O
IG	O
esti	O
-	O
mates	O
the	O
amount	O
of	O
information	O
a	O
word	O
gives	O
for	O
the	O
decision	O
on	O
the	O
stance	O
.	O
We	O
choose	O
IG	O
because	O
it	O
has	O
been	O
shown	O
to	O
be	O
robust	O
across	O
different	O
sentiment	B-TaskName
analysis	I-TaskName
data	O
sets	O
and	O
across	O
different	O
skewing	O
ratios	O
,	O
compared	O
to	O
other	O
feature	B-MethodName
selection	I-MethodName
methods	O
(	O
Liu	O
et	O
al	O
,	O
2014	O
)	O
.	O
Note	O
that	O
different	O
from	O
its	O
use	O
in	O
decision	O
trees	O
,	O
we	O
use	O
IG	O
as	O
an	O
external	O
filter	O
to	O
select	O
a	O
subset	O
of	O
features	O
,	O
before	O
and	O
independent	O
of	O
any	O
classifiers	O
.	O

One	O
limitation	O
of	O
bag	O
-	O
of	O
-	O
words	O
features	O
is	O
that	O
they	O
are	O
very	O
sparse	O
,	O
and	O
they	O
can	O
not	O
handle	O
out	O
-	O
ofvocabulary	O
words	O
properly	O
.	O
Since	O
tweets	O
are	O
relatively	O
short	O
,	O
and	O
the	O
amount	O
of	O
official	O
training	O
data	O
is	O
small	O
,	O
it	O
is	O
likely	O
that	O
the	O
out	O
-	O
of	O
-	O
vocabulary	O
rate	O
is	O
high	O
.	O
Thus	O
we	O
also	O
build	O
models	O
using	O
word	O
vectors	O
,	O
which	O
represent	O
each	O
word	O
with	O
a	O
vector	O
of	O
continuous	O
values	O
.	O
Word	O
vectors	O
have	O
been	O
shown	O
to	O
capture	O
the	O
similarity	O
among	O
words	O
and	O
thus	O
alleviate	O
data	O
sparseness	O
(	O
Collobert	O
et	O
al	O
,	O
2011	O
)	O
.	O
We	O
have	O
experimented	O
with	O
two	O
different	O
word	O
vector	O
models	O
,	O
word2vec	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
and	O
GloVe	B-MethodName
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
.	O
We	O
have	O
used	O
the	O
pre	O
-	O
trained	O
word2vec	O
obtained	O
from	O
the	O
Google	B-DatasetName
News	O
dataset	O
,	O
which	O
contains	O
a	O
300	O
-	O
dimensional	O
vector	O
representation	O
for	O
3	O
million	O
words	O
and	O
phrases	O
1	O
,	O
and	O
the	O
pre	O
-	O
trained	O
GloVe	B-MethodName
,	O
which	O
is	O
obtained	O
from	O
2	O
billion	O
tweets	O
and	O
has	O
a	O
250dimensional	O
vector	O
representation	O
for	O
1.2	O
million	O
words	O
and	O
phrases	O
2	O
.	O
To	O
construct	O
a	O
representation	O
for	O
a	O
tweet	O
,	O
we	O
look	O
up	O
a	O
word	O
in	O
the	O
word	O
vectors	O
model	O
,	O
then	O
average	O
all	O
vectors	O
for	O
words	O
to	O
produce	O
a	O
vector	O
representation	O
for	O
the	O
tweet	O
.	O
For	O
example	O
,	O
to	O
represent	O
a	O
15	O
word	O
tweet	O
using	O
word2vec	O
,	O
we	O
first	O
obtain	O
a	O
300dimensional	O
vector	O
for	O
each	O
word	O
,	O
then	O
average	O
all	O
15	O
vectors	O
.	O
This	O
means	O
that	O
the	O
word	O
order	O
is	O
lost	O
and	O
the	O
representation	O
constitutes	O
a	O
"	O
bag	O
of	O
vectors	O
"	O
.	O

We	O
have	O
performed	O
a	O
comparison	O
of	O
both	O
word	O
vector	O
variants	O
in	O
a	O
5	O
-	O
fold	O
cross	O
validation	O
experiment	O
on	O
the	O
training	O
data	O
.	O
Table	O
2	O
summarizes	O
the	O
results	O
.	O
We	O
can	O
see	O
that	O
GloVe	B-MethodName
performs	O
consistently	O
better	O
than	O
word2vec	O
except	O
for	O
Feminist	O
where	O
word2vec	O
is	O
0.6	O
%	O
better	O
than	O
GloVe	B-MethodName
.	O
We	O
assume	O
that	O
this	O
performance	O
gap	O
is	O
mainly	O
caused	O
by	O
the	O
domain	O
difference	O
from	O
which	O
the	O
word	O
vectors	O
are	O
obtained	O
:	O
We	O
used	O
GloVe	B-MethodName
pretrained	O
on	O
tweets	O
and	O
word2vec	O
pre	O
-	O
trained	O
on	O
news	O
.	O
This	O
leads	O
to	O
a	O
higher	O
number	O
of	O
out	O
-	O
of	O
-	O
vocabulary	O
words	O
for	O
the	O
word2vec	O
model	O
.	O
In	O
other	O
words	O
,	O
GloVe	B-MethodName
provides	O
a	O
broader	O
coverage	O
for	O
this	O
data	O
set	O
.	O

The	O
bag	O
-	O
of	O
-	O
words	O
features	O
used	O
in	O
the	O
classifiers	O
(	O
see	O
section	O
3	O
)	O
assume	O
that	O
the	O
words	O
are	O
considered	O
independently	O
.	O
However	O
,	O
in	O
many	O
situations	O
,	O
it	O
is	O
the	O
distributions	O
of	O
positively	O
and	O
negatively	O
oriented	O
words	O
that	O
determine	O
the	O
final	O
stance	O
of	O
a	O
tweet	O
.	O
A	O
low	O
coverage	O
of	O
words	O
from	O
these	O
two	O
distributions	O
is	O
a	O
strong	O
indicator	O
for	O
None	O
stance	O
as	O
well	O
.	O
This	O
is	O
especially	O
important	O
for	O
the	O
ensemble	O
classifier	O
.	O
For	O
this	O
reason	O
,	O
we	O
have	O
developed	O
two	O
additional	O
features	O
for	O
the	O
ensemble	O
,	O
which	O
capture	O
information	O
from	O
these	O
two	O
distributions	O
:	O
one	O
feature	O
for	O
positive	O
orientation	O
and	O
one	O
for	O
negative	O
orientation	O
.	O
The	O
feature	O
is	O
a	O
numeric	O
score	O
,	O
representing	O
the	O
association	O
of	O
a	O
tweet	O
with	O
positive	O
or	O
negative	O
stance	O
respectively	O
.	O
The	O
positive	O
orientation	O
is	O
calculated	O
based	O
on	O
the	O
following	O
equation	O
:	O
score	O
pos	O
T	O
=	O
1	O
|	O
T	O
|	O
w⊂T	O
f	O
req	O
(	O
w	O
)	O
in	O
POS	O
w	O
⊂V	O
f	O
req	O
(	O
w	O
)	O
in	O
POS	O
where	O
T	O
is	O
a	O
tweet	O
,	O
|	O
T	O
|	O
is	O
the	O
tweet	O
length	O
excluding	O
stop	O
words	O
.	O
V	O
is	O
the	O
entire	O
vocabulary	O
.	O
F	O
req	O
(	O
w	O
)	O
is	O
the	O
frequency	O
count	O
of	O
w	O
in	O
the	O
following	O
set	O
.	O
P	O
OS	O
is	O
the	O
set	O
of	O
all	O
positive	O
tweets	O
.	O
This	O
score	O
measures	O
for	O
each	O
word	O
(	O
its	O
lemma	B-DatasetName
)	O
the	O
association	O
with	O
positive	O
stance	O
,	O
sums	O
up	O
all	O
words	O
in	O
the	O
tweet	O
,	O
and	O
normalizes	O
the	O
score	O
by	O
the	O
tweet	O
length	O
.	O
The	O
score	O
for	O
the	O
negative	O
orientation	O
is	O
calculated	O
accordingly	O
.	O
The	O
None	O
orientation	O
is	O
not	O
calculated	O
since	O
it	O
is	O
already	O
represented	O
by	O
the	O
absence	O
of	O
positively	O
or	O
negatively	O
oriented	O
words	O
.	O
I.e.	O
,	O
we	O
assume	O
that	O
if	O
a	O
tweet	O
has	O
low	O
positive	O
and	O
negative	O
orientations	O
,	O
it	O
indicates	O
a	O
None	O
stance	O
.	O

Table	O
4	O
shows	O
the	O
results	O
of	O
the	O
three	O
individual	O
classifiers	O
as	O
well	O
as	O
of	O
the	O
two	O
ensemble	O
model	O
variants	O
,	O
one	O
combining	O
only	O
the	O
individual	O
classifiers	O
'	O
outputs	O
(	O
EnsembleNG	O
)	O
,	O
the	O
other	O
one	O
(	O
EnsembleG	O
)	O
including	O
also	O
the	O
global	O
features	O
(	O
see	O
section	O
2.2.3	O
)	O
.	O
These	O
results	O
show	O
that	O
the	O
GBDT	O
approach	O
using	O
GloVe	B-MethodName
reaches	O
the	O
highest	O
result	O
(	O
64.64	O
)	O
among	O
the	O
individual	O
classifiers	O
.	O
The	O
random	O
forest	O
classifier	O
,	O
which	O
constitutes	O
our	O
official	O
submission	O
is	O
about	O
1	O
percentage	O
point	O
lower	O
(	O
63.60	O
)	O
,	O
and	O
the	O
SVM	B-MethodName
classifier	O
is	O
about	O
1.5	O
percentage	O
points	O
below	O
that	O
(	O
61.93	O
)	O
.	O
A	O
closer	O
look	O
at	O
the	O
ensemble	O
variants	O
shows	O
that	O
using	O
the	O
global	O
features	O
has	O
a	O
detrimental	O
effect	O
across	O
all	O
targets	O
,	O
most	O
likely	O
because	O
this	O
information	O
is	O
too	O
coarse	O
.	O
The	O
other	O
ensemble	O
classifier	O
improves	O
over	O
GBDT	O
by	O
1.5	O
percentage	O
points	O
(	O
66.14	O
)	O
.	O
This	O
shows	O
that	O
we	O
can	O
benefit	O
from	O
important	O
information	O
from	O
all	O
individual	O
classifiers	O
.	O

In	O
this	O
shared	O
task	O
,	O
we	O
regard	O
stance	B-TaskName
detection	I-TaskName
as	O
a	O
special	O
case	O
of	O
sentiment	B-TaskName
analysis	I-TaskName
,	O
using	O
supervised	O
classifiers	O
and	O
bag	O
of	O
unigrams	O
and	O
word	O
vectors	O
as	O
features	O
.	O
Our	O
submitted	O
system	O
is	O
based	O
on	O
a	O
random	O
forest	O
classifier	O
because	O
of	O
its	O
capability	O
to	O
handle	O
overfitting	O
and	O
to	O
generalize	O
over	O
the	O
test	O
data	O
.	O
Since	O
the	O
amount	O
of	O
available	O
training	O
data	O
is	O
small	O
,	O
random	O
forest	O
's	O
ability	O
to	O
sample	O
data	O
points	O
and	O
fea	O
-	O
ture	O
subspaces	O
reduces	O
data	O
sparsity	O
.	O
The	O
submitted	O
system	O
has	O
an	O
official	O
score	O
of	O
63.60	O
and	O
ranked	O
6th	O
out	O
of	O
19	O
teams	O
.	O
We	O
also	O
experimented	O
with	O
other	O
single	O
models	O
(	O
SVM	B-MethodName
and	O
GBDT	O
)	O
and	O
with	O
an	O
ensemble	O
model	O
built	O
on	O
a	O
memory	O
-	O
based	O
classifier	O
.	O
The	O
GBDT	O
model	O
using	O
GloVe	B-MethodName
word	O
vectors	O
reaches	O
a	O
higher	O
score	O
of	O
64.64	O
,	O
which	O
may	O
be	O
a	O
result	O
of	O
the	O
word	O
vectors	O
'	O
capability	O
to	O
capture	O
similarities	O
among	O
words	O
,	O
which	O
helps	O
in	O
dealing	O
with	O
out	O
-	O
of	O
-	O
vocabulary	O
words	O
.	O
The	O
ensemble	O
model	O
that	O
aggregates	O
information	O
from	O
the	O
three	O
individual	O
classifiers	O
reaches	O
the	O
highest	O
performance	O
of	O
66.14	O
.	O
Our	O
hypothesis	O
is	O
that	O
different	O
strengths	O
(	O
e.g.	O
,	O
good	O
performance	O
for	O
minority	O
/	O
majority	O
classes	O
)	O
from	O
individual	O
models	O
complement	O
each	O
other	O
in	O
the	O
ensemble	O
.	O
However	O
a	O
closer	O
look	O
at	O
the	O
performance	O
of	O
all	O
classifiers	O
and	O
ensembles	O
across	O
individual	O
targets	O
shows	O
that	O
no	O
system	O
reaches	O
consistently	O
good	O
results	O
across	O
all	O
targets	O
.	O
The	O
best	O
performing	O
ensemble	O
(	O
EnsembleNG	O
)	O
outperforms	O
individual	O
classifiers	O
only	O
for	O
Abortion	O
and	O
Feminist	O
;	O
for	O
the	O
other	O
targets	O
,	O
random	O
forest	O
or	O
GBDT	O
reach	O
higher	O
accuracies	O
.	O
Some	O
of	O
the	O
variation	O
in	O
system	O
performance	O
can	O
be	O
explained	O
by	O
the	O
class	O
imbalance	O
present	O
in	O
the	O
data	O
sets	O
for	O
the	O
different	O
targets	O
,	O
but	O
further	O
work	O
is	O
required	O
to	O
identify	O
other	O
factors	O
.	O
Finally	O
,	O
it	O
is	O
worth	O
pointing	O
out	O
that	O
our	O
approach	O
to	O
stance	B-TaskName
detection	I-TaskName
utilizes	O
very	O
surface	O
oriented	O
features	O
.	O
To	O
boost	O
performance	O
,	O
we	O
may	O
need	O
to	O
develop	O
methods	O
that	O
incorporate	O
inference	O
,	O
entailment	O
,	O
and	O
world	O
knowledge	O
,	O
for	O
example	O
,	O
to	O
handle	O
cases	O
such	O
as	O
"	O
keep	O
H.	O
out	O
of	O
the	O
white	O
house	O
"	O
.	O

AStarTwice	O
at	O
SemEval	O
-	O
2021	O
Task	O
5	O
:	O
Toxic	O
Span	O
Detection	O
using	O
RoBERTa	B-MethodName
-	O
CRF	B-MethodName
,	O
Domain	O
Specific	O
Pre	O
-	O
Training	O
and	O
Self	O
-	O
Training	O

In	O
recent	O
years	O
there	O
has	O
been	O
an	O
exponential	O
increase	O
in	O
the	O
use	O
of	O
social	O
network	O
platforms	O
.	O
With	O
rising	O
abusive	B-TaskName
language	I-TaskName
and	O
hate	O
on	O
such	O
platforms	O
,	O
it	O
is	O
more	O
important	O
than	O
ever	O
to	O
maintain	O
online	O
conversations	O
constructive	O
and	O
inclusive	O
.	O
This	O
problem	O
can	O
be	O
tackled	O
by	O
filtering	O
toxic	O
comments	O
/	O
posts	O
.	O
The	O
massive	O
volume	O
of	O
data	O
generated	O
at	O
a	O
fast	O
pace	O
makes	O
manually	O
filtering	O
each	O
comment	O
complicated	O
and	O
time	O
-	O
consuming	O
.	O
This	O
process	O
can	O
be	O
automated	O
by	O
modelling	O
it	O
as	O
a	O
supervised	O
classification	O
problem	O
.	O
A	O
similar	O
task	O
was	O
proposed	O
in	O
SemEval	O
-	O
2019	O
Task	O
6	O
:	O
Identifying	O
and	O
Categorizing	O
Offensive	O
Language	O
in	O
Social	O
Media	O
(	O
OffensEval	O
)	O
(	O
Zampieri	O
et	O
al	O
,	O
2019	O
)	O
.	O
Most	O
of	O
the	O
top	O
-	O
ranked	O
teams	O
in	O
this	O
task	O
used	O
transformer	O
language	O
models	O
(	O
Liu	O
et	O
al	O
,	O
2019a	O
;	O
Zhu	O
et	O
al	O
,	O
2019	O
;	O
Pelicon	O
et	O
al	O
,	O
2019	O
;	O
Wu	O
et	O
al	O
,	O
2019	O
)	O
or	O
an	O
ensemble	O
of	O
CNN	O
and	O
RNN	O
(	O
Mahata	O
et	O
al	O
,	O
2019	O
;	O
Mitrović	O
et	O
al	O
,	O
2019	O
)	O
to	O
classify	O
the	O
sentences	O
.	O
The	O
problem	O
with	O
the	O
above	O
approach	O
is	O
that	O
it	O
does	O
n't	O
give	O
moderators	O
much	O
knowledge	O
about	O
the	O
reason	O
for	O
a	O
sentence	O
's	O
toxicity	O
.	O
Highlighting	O
toxic	O
spans	O
can	O
help	O
human	O
moderators	O
who	O
frequently	O
deal	O
with	O
long	O
comments	O
and	O
prefer	O
attribution	O
rather	O
than	O
just	O
an	O
unexplained	O
toxicity	O
score	O
.	O
SemEval	O
2021	O
Task	O
5	O
:	O
Toxic	O
Span	O
Detection	O
(	O
Pavlopoulos	O
et	O
al	O
,	O
2021	O
)	O
gave	O
a	O
chance	O
to	O
propose	O
NLP	O
systems	O
to	O
solve	O
this	O
problem	O
.	O
The	O
task	O
is	O
concerned	O
with	O
developing	O
systems	O
that	O
can	O
recognise	O
spans	O
that	O
contribute	O
to	O
the	O
text	O
's	O
toxicity	O
.	O
This	O
task	O
had	O
a	O
few	O
challenges	O
.	O
Since	O
the	O
samples	O
were	O
from	O
an	O
online	O
commenting	O
platform	O
,	O
they	O
were	O
grammatically	O
incorrect	O
and	O
consisted	O
of	O
many	O
out	O
of	O
vocabulary	O
words	O
.	O
The	O
noisy	O
and	O
ambiguous	O
structure	O
of	O
comments	O
significantly	O
hampers	O
the	O
performance	O
of	O
general	O
NLP	O
models	O
.	O
The	O
training	O
dataset	O
had	O
a	O
little	O
less	O
than	O
8000	O
samples	O
.	O
Thus	O
,	O
there	O
was	O
a	O
need	O
to	O
select	O
systems	O
that	O
can	O
produce	O
meaningful	O
results	O
,	O
even	O
with	O
a	O
limited	O
number	O
of	O
training	O
samples	O
.	O
Undoubtedly	O
,	O
the	O
hardest	O
part	O
is	O
to	O
identify	O
spans	O
that	O
can	O
account	O
for	O
the	O
toxicity	O
of	O
the	O
sample	O
.	O
The	O
span	O
could	O
be	O
as	O
small	O
as	O
a	O
single	O
token	O
and	O
as	O
large	O
as	O
the	O
sample	O
itself	O
.	O
The	O
linguistic	O
variations	O
in	O
the	O
usage	O
of	O
words	O
and	O
phrases	O
make	O
such	O
attribution	O
even	O
more	O
difficult	O
.	O
We	O
formulated	O
the	O
task	O
as	O
a	O
sequence	O
tagging	O
problem	O
and	O
used	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2019b	O
)	O
,	O
a	O
pre	O
-	O
trained	O
Transformer	B-MethodName
-	O
based	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
language	O
model	O
as	O
our	O
base	O
model	O
.	O
We	O
further	O
pre	O
-	O
trained	O
RoBERTa	B-MethodName
on	O
the	O
Civil	O
Comments	O
Dataset	O
as	O
a	O
masked	O
language	O
model	O
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
to	O
create	O
a	O
domain	O
-	O
specific	O
model	O
.	O
We	O
employed	O
a	O
Conditional	B-MethodName
Random	I-MethodName
Field	I-MethodName
(	O
CRF	B-MethodName
)	O
layer	O
(	O
Lafferty	O
et	O
al	O
,	O
2001	O
)	O
for	O
predicting	O
the	O
most	O
probabilistic	O
sequence	O
of	O
labels	O
for	O
each	O
input	O
sequence	O
.	O
We	O
also	O
applied	O
a	O
few	O
pre	O
-	O
processing	O
steps	O
,	O
which	O
lead	O
to	O
significant	O
performance	O
improvements	O
.	O
Lastly	O
,	O
we	O
leveraged	O
the	O
semi	O
-	O
supervised	O
learning	O
technique	O
of	O
self	O
-	O
training	O
(	O
Yarowsky	O
,	O
1995	O
;	O
Liao	O
and	O
Veeramachaneni	O
,	O
2009	O
;	O
Jurkiewicz	O
et	O
al	O
,	O
2020	O
)	O
by	O
training	O
our	O
model	O
on	O
the	O
manually	O
annotated	O
dataset	O
and	O
using	O
it	O
to	O
further	O
extend	O
the	O
training	O
set	O
by	O
generating	O
toxic	O
spans	O
for	O
other	O
unannotated	O
datasets	O
.	O
We	O
have	O
made	O
our	O
system	O
's	O
implementation	O
available	O
through	O
GitHub	O
1	O
.	O
The	O
rest	O
of	O
the	O
paper	O
is	O
organised	O
as	O
follows	O
.	O
Section	O
2	O
explains	O
our	O
model	O
implementation	O
in	O
detail	O
.	O
Section	O
3	O
and	O
4	O
presents	O
our	O
experimental	O
setup	O
and	O
achieved	O
results	O
,	O
respectively	O
.	O
In	O
section	O
4	O
,	O
we	O
perform	O
error	O
analysis	O
,	O
followed	O
by	O
conclusions	O
in	O
the	O
last	O
section	O
.	O

We	O
formulated	O
the	O
task	O
as	O
a	O
token	O
level	O
sequence	O
tagging	O
problem	O
where	O
we	O
classify	O
each	O
token	O
as	O
Begin	O
,	O
Inside	O
or	O
Outside	O
(	O
BIO	O
scheme	O
)	O
.	O
Having	O
begin	O
and	O
end	O
tags	O
helps	O
formulate	O
the	O
notion	O
of	O
spans	O
better	O
and	O
creates	O
dependencies	O
between	O
various	O
tokens	O
of	O
a	O
toxic	O
span	O
(	O
Singh	O
et	O
al	O
,	O
2020	O
)	O
,	O
allowing	O
it	O
to	O
perform	O
better	O
than	O
other	O
alternatives	O
such	O
as	O
IO	O
(	O
Inside	O
Outside	O
)	O
.	O
Pre	O
-	O
Processing	O
:	O
We	O
applied	O
a	O
few	O
preprocessing	O
steps	O
before	O
fine	O
-	O
tuning	O
RoBERTa	B-MethodName
on	O
the	O
input	O
text	O
samples	O
.	O
First	O
,	O
we	O
converted	O
all	O
the	O
text	O
samples	O
to	O
lowercase	O
.	O
We	O
observed	O
that	O
punctuation	O
marks	O
did	O
not	O
add	O
any	O
significant	O
information	O
to	O
the	O
semantics	O
of	O
a	O
sentence	O
.	O
Therefore	O
,	O
as	O
a	O
part	O
of	O
the	O
data	O
cleaning	O
,	O
punctuation	O
marks	O
such	O
as	O
commas	O
and	O
dashes	O
were	O
removed	O
.	O
We	O
also	O
collapsed	O
multiple	O
space	O
characters	O
into	O
a	O
single	O
space	O
.	O
Model	O
:	O
We	O
provided	O
the	O
text	O
samples	O
as	O
input	O
to	O
our	O
pre	O
-	O
trained	O
RoBERTa	B-MethodName
(	O
p	O
)	O
model	O
to	O
get	O
768dimensional	O
contextual	O
embeddings	O
for	O
each	O
token	O
.	O
These	O
contextual	O
embeddings	O
were	O
passed	O
through	O
two	O
dense	O
layers	O
of	O
512	O
and	O
128	O
dimensions	O
,	O
followed	O
by	O
a	O
Conditional	O
Random	O
Fields	O
(	O
CRF	B-MethodName
)	O
(	O
Lafferty	O
et	O
al	O
,	O
2001	O
)	O
layer	O
with	O
three	O
labels	O
(	O
B	O
-	O
Begin	O
,	O
I	O
-	O
Inside	O
or	O
O	O
-	O
Outside	O
)	O
.	O
The	O
CRF	B-MethodName
layer	O
models	O
the	O
correlation	O
between	O
the	O
labels	O
predicted	O
for	O
the	O
individual	O
tokens	O
.	O
It	O
receives	O
the	O
logits	O
for	O
each	O
input	O
token	O
and	O
predicts	O
the	O
most	O
probabilistic	O
sequence	O
of	O
labels	O
for	O
each	O
input	O
sequence	O
.	O
Figure	O
1	O
shows	O
our	O
model	O
architecture	O
.	O

There	O
has	O
been	O
a	O
growing	O
interest	O
in	O
the	O
compression	O
of	O
pre	O
-	O
trained	O
language	O
models	O
.	O
We	O
consider	O
three	O
varieties	O
of	O
methods	O
:	O
distillation	O
,	O
pruning	O
,	O
and	O
structured	O
pruning	O
.	O
Knowledge	B-MethodName
distillation	I-MethodName
,	O
introduced	O
by	O
Hinton	O
et	O
al	O
(	O
2015	O
)	O
,	O
is	O
a	O
popular	O
compression	O
technique	O
.	O
Researchers	O
have	O
applied	O
this	O
method	O
to	O
a	O
variety	O
of	O
NLP	O
models	O
(	O
Tang	O
et	O
al	O
,	O
2019	O
;	O
Sun	O
et	O
al	O
,	O
2019	O
;	O
Turc	O
et	O
al	O
,	O
2019	O
)	O
.	O
Distillation	O
has	O
been	O
used	O
to	O
obtain	O
significantly	O
smaller	O
BERT	B-MethodName
models	O
achieving	O
competitive	O
performances	O
.	O
Sanh	O
et	O
al	O
(	O
2019	O
)	O
distills	O
BERT	B-MethodName
into	O
shallower	O
students	O
during	O
the	O
pre	O
-	O
training	O
stage	O
and	O
optionally	O
during	O
the	O
finetuning	O
stage	O
.	O
MobileBERT	B-MethodName
(	O
Sun	O
et	O
al	O
,	O
2020	O
)	O
and	O
TinyBERT	O
(	O
Jiao	O
et	O
al	O
,	O
2019	O
)	O
are	O
obtained	O
thanks	O
to	O
a	O
layer	O
-	O
wise	O
distillation	O
strategy	O
.	O
While	O
the	O
distillation	O
of	O
former	O
is	O
task	O
-	O
agnostic	O
,	O
the	O
one	O
used	O
to	O
obtain	O
the	O
latter	O
is	O
task	O
-	O
specific	O
.	O
Other	O
previous	O
work	O
has	O
focused	O
on	O
unstructured	O
pruning	O
(	O
LeCun	O
et	O
al	O
,	O
1989	O
;	O
Han	O
et	O
al	O
,	O
2015	O
;	O
Frankle	O
and	O
Carbin	O
,	O
2018	O
)	O
.	O
When	O
targeting	O
transformer	O
models	O
,	O
it	O
is	O
typical	O
to	O
select	O
the	O
weights	O
to	O
prune	O
based	O
on	O
their	O
magnitude	O
(	O
Gordon	O
et	O
al	O
,	O
2020	O
)	O
,	O
or	O
by	O
computing	O
an	O
importance	O
score	O
using	O
a	O
firstorder	O
method	O
(	O
Sanh	O
et	O
al	O
,	O
2020	O
)	O
.	O
While	O
these	O
methods	O
allow	O
for	O
a	O
significant	O
reduction	O
in	O
model	O
size	O
,	O
specialized	O
hardware	O
is	O
required	O
to	O
make	O
use	O
of	O
the	O
resulting	O
unstructured	O
sparse	O
matrices	O
in	O
order	O
to	O
speed	O
up	O
inference	O
.	O
In	O
contrast	O
,	O
structured	O
pruning	O
removes	O
coherent	O
groups	O
of	O
weights	O
(	O
Murray	O
and	O
Chiang	O
,	O
2015	O
;	O
See	O
et	O
al	O
,	O
2016	O
;	O
Joulin	O
et	O
al	O
,	O
2016	O
;	O
Fan	O
et	O
al	O
,	O
2020	O
;	O
Sajjad	O
et	O
al	O
,	O
2020	O
)	O
.	O
Recent	O
works	O
(	O
Michel	O
et	O
al	O
,	O
2019	O
;	O
Voita	O
et	O
al	O
,	O
2019	O
)	O
show	O
that	O
some	O
heads	O
can	O
be	O
removed	O
without	O
significant	O
degradation	O
in	O
performance	O
,	O
leading	O
to	O
the	O
conclusion	O
that	O
most	O
heads	O
provide	O
redundant	O
information	O
.	O
Other	O
authors	O
have	O
worked	O
on	O
combining	O
matrix	O
factorization	O
and	O
weight	O
pruning	O
.	O
While	O
Mao	O
et	O
al	O
(	O
2020	O
)	O
combine	O
SVD	B-DatasetName
-	O
based	O
matrix	O
factorization	O
with	O
unstructured	O
pruning	O
,	O
use	O
structured	O
pruning	O
in	O
order	O
to	O
reduce	O
the	O
rank	O
.	O
Related	O
to	O
our	O
approach	O
,	O
Kim	O
and	O
Awadalla	O
(	O
2020	O
)	O
and	O
McCarley	O
(	O
2019	O
)	O
both	O
apply	O
structured	O
pruning	O
on	O
the	O
heads	O
of	O
the	O
multi	B-MethodName
-	I-MethodName
head	I-MethodName
attention	I-MethodName
(	O
MHA	O
)	O
and	O
on	O
the	O
inner	O
-	O
layer	O
nodes	O
of	O
the	O
feed	O
-	O
forward	O
network	O
(	O
FFN	O
)	O
.	O
The	O
former	O
uses	O
predefined	O
pruning	O
ratios	O
,	O
shared	O
across	O
all	O
layers	O
,	O
in	O
order	O
to	O
select	O
the	O
modules	O
to	O
prune	O
after	O
sorting	O
them	O
given	O
an	O
importance	O
score	O
.	O
McCarley	O
(	O
2019	O
)	O
compares	O
dif	O
-	O
ferent	O
methods	O
to	O
compute	O
the	O
prunable	O
module	O
masks	O
and	O
find	O
L0	O
regularization	O
to	O
perform	O
the	O
best	O
.	O

In	O
this	O
work	O
,	O
we	O
extend	O
movement	B-MethodName
pruning	I-MethodName
to	O
work	O
on	O
blocks	O
of	O
local	O
parameters	O
.	O
Specifically	O
,	O
each	O
matrix	O
in	O
the	O
transformer	O
is	O
partitioned	O
into	O
fixedsized	O
blocks	O
.	O
This	O
setting	O
goes	O
beyond	O
the	O
arbitrary	O
pruning	O
of	O
unstructured	O
methods	O
,	O
with	O
the	O
goal	O
of	O
encouraging	O
the	O
data	O
locality	O
closer	O
to	O
what	O
would	O
be	O
needed	O
for	O
efficiency	O
.	O
2	O
Our	O
approach	O
is	O
extremely	O
simple	O
.	O
For	O
each	O
parameter	O
matrix	O
W	O
R	O
M	O
×N	O
,	O
we	O
assume	O
a	O
fixedsized	O
block	O
structure	O
(	O
M	O
,	O
N	O
)	O
.	O
Each	O
of	O
these	O
blocks	O
acts	O
as	O
an	O
individual	O
group	O
in	O
the	O
regularization	O
with	O
a	O
shared	O
score	O
parameter	O
derived	O
from	O
the	O
corresponding	O
score	O
matrix	O
S	O
R	O
M	O
/	O
M	O
×N	O
/	O
N	O
.	O
Computing	O
the	O
masked	O
weight	O
is	O
done	O
by	O
expanding	O
the	O
thresholded	O
values	O
,	O
i.e.	O
W	O
i	O
,	O
j	O
=	O
W	O
i	O
,	O
j	O
*	O
M	O
(	O
S	O
)	O
i	O
/	O
M	O
,	O
j	O
/	O
N	O
As	O
in	O
past	O
work	O
,	O
this	O
model	O
is	O
trained	O
with	O
distillation	O
to	O
match	O
the	O
performance	O
of	O
a	O
teacher	O
model	O
.	O
Unlike	O
other	O
distillation	O
approaches	O
that	O
require	O
fully	O
specifying	O
the	O
new	O
model	O
structure	O
,	O
our	O
method	O
only	O
requires	O
the	O
size	O
and	O
shapes	O
of	O
the	O
blocks	O
,	O
i.e.	O
the	O
set	O
of	O
(	O
M	O
,	O
N	O
)	O
for	O
each	O
parameter	O
matrix	O
in	O
the	O
model	O
.	O
If	O
blocks	O
are	O
too	O
large	O
,	O
then	O
they	O
are	O
difficult	O
to	O
prune	O
,	O
but	O
if	O
they	O
are	O
too	O
small	O
they	O
do	O
not	O
support	O
efficient	O
inference	O
.	O
To	O
reduce	O
the	O
search	O
space	O
,	O
we	O
will	O
limit	O
ourselves	O
to	O
test	O
(	O
M	O
,	O
N	O
)	O
att	O
and	O
(	O
M	O
,	O
N	O
)	O
ff	O
:	O
the	O
same	O
block	O
size	O
will	O
be	O
used	O
for	O
all	O
layers	O
for	O
attention	O
weights	O
W	O
q	O
,	O
W	O
k	O
,	O
W	O
v	O
and	O
W	O
o	O
on	O
one	O
hand	O
,	O
and	O
for	O
the	O
feed	O
-	O
forward	O
weights	O
W	O
1	O
and	O
W	O
2	O
on	O
the	O
other	O
hand	O
.	O
We	O
split	O
the	O
movement	B-MethodName
pruning	I-MethodName
regularization	O
term	O
into	O
:	O
λ	O
att	O
σ	O
(	O
S	O
att	O
)	O
+	O
λ	O
ffn	O
σ	O
(	O
S	O
ffn	O
)	O
This	O
allows	O
us	O
to	O
take	O
into	O
account	O
the	O
difference	O
in	O
terms	O
of	O
gradient	O
received	O
by	O
the	O
score	O
parameters	O
.	O
To	O
reduce	O
further	O
the	O
search	O
space	O
,	O
we	O
will	O
test	O
on	O
two	O
kinds	O
of	O
blocks	O
:	O
(	O
32	O
,	O
32	O
)	O
:	O
square	O
blocks	O
(	O
Block	O
)	O
(	O
1	O
,	O
d	O
model	O
)	O
and	O
(	O
d	O
model	O
,	O
1	O
)	O
:	O
dimension	O
pruning	O
on	O
paired	O
FFN	O
rows	O
and	O
columns	O
(	O
Dim	O
)	O
These	O
block	O
sizes	O
allow	O
for	O
efficient	O
models	O
:	O
blocks	O
of	O
size	O
at	O
least	O
(	O
16	O
,	O
16	O
)	O
are	O
efficient	O
to	O
compute	O
with	O
appropriate	O
GPU	O
kernels	O
,	O
whereas	O
full	O
rows	O
,	O
columns	O
or	O
heads	O
can	O
be	O
entirely	O
removed	O
from	O
the	O
matrix	O
:	O
the	O
remaining	O
matrix	O
is	O
then	O
dense	O
.	O
We	O
also	O
include	O
two	O
additional	O
baseline	O
block	O
types	O
used	O
to	O
verify	O
the	O
approach	O
:	O
(	O
2	O
n	O
,	O
2	O
n	O
)	O
,	O
n	O
[	O
2	O
,	O
5	O
]	O
:	O
smaller	O
power	O
of	O
two	O
square	O
block	O
sizes	O
to	O
study	O
the	O
impact	O
of	O
size	O
on	O
performance	O
(	O
Block	O
)	O
(	O
d	O
model	O
n	O
heads	O
,	O
d	O
model	O
)	O
:	O
for	O
attention	O
heads	O
(	O
Heads	O
)	O
The	O
first	O
considers	O
small	O
blocks	O
,	O
and	O
the	O
second	O
considers	O
very	O
large	O
functional	O
blocks	O
.	O

We	O
are	O
using	O
a	O
minimal	O
set	O
of	O
hyperparameters	O
.	O
The	O
ratio	O
of	O
λ	O
att	O
and	O
λ	O
ffn	O
is	O
fixed	O
by	O
the	O
relative	O
sizes	O
.	O
We	O
performed	O
a	O
few	O
experiments	O
with	O
differ	O
-	O
ent	O
values	O
fixed	O
manually	O
for	O
these	O
parameters	O
,	O
but	O
their	O
influence	O
is	O
minor	O
.	O
The	O
main	O
hyperparameter	O
is	O
the	O
number	O
of	O
training	O
epochs	O
.	O
For	O
SQuAD	B-DatasetName
v1.1	O
,	O
we	O
are	O
using	O
20	O
epochs	O
instead	O
of	O
typically	O
2	O
for	O
BERT	B-MethodName
models	O
.	O
This	O
means	O
a	O
fine	O
-	O
tuning	O
is	O
taking	O
about	O
12h	O
with	O
our	O
method	O
instead	O
of	O
45mn	O
with	O
a	O
standard	O
finetuning	O
setup	O
.	O
This	O
number	O
has	O
to	O
be	O
large	O
enough	O
to	O
let	O
pruning	O
happen	O
slowly	O
enough	O
for	O
a	O
given	O
task	O
.	O
A	O
warming	O
up	O
phase	O
and	O
a	O
post	O
-	O
pruning	O
cooldown	O
phase	O
are	O
helpful	O
,	O
but	O
their	O
exact	O
length	O
has	O
not	O
a	O
large	O
impact	O
on	O
final	O
performance	O
.	O
We	O
believe	O
the	O
training	O
time	O
is	O
less	O
important	O
than	O
the	O
inference	O
time	O
for	O
energy	O
consideration	O
,	O
as	O
inference	O
is	O
performed	O
repeatedly	O
.	O
Our	O
method	O
is	O
optimizing	O
inference	O
by	O
a	O
large	O
factor	O
:	O
the	O
training	O
energy	O
is	O
potentially	O
recouped	O
by	O
a	O
large	O
margin	O
with	O
inference	O
savings	O
.	O
Finally	O
,	O
the	O
checkpoints	O
created	O
during	O
the	O
experiments	O
are	O
available	O
on	O
an	O
AWS	O
S3	O
bucket	O
,	O
with	O
their	O
metadata	O
and	O
training	O
parameters	O
,	O
totaling	O
3	O
TB	O
of	O
data	O
,	O
to	O
facilitate	O
reproduction	O
of	O
our	O
results	O
and	O
to	O
make	O
it	O
possible	O
to	O
study	O
further	O
the	O
behavior	O
of	O
those	O
models	O
.	O
Code	O
for	O
experiments	O
,	O
analysis	O
,	O
and	O
tools	O
to	O
prepare	O
the	O
present	O
paper	O
are	O
available	O
on	O
GitHub	O
(	O
see	O
Appendix	O
A	O
)	O
.	O

We	O
report	O
experimental	O
results	O
with	O
the	O
addition	O
of	O
a	O
teacher	O
distillation	O
step	O
as	O
previous	O
work	O
showed	O
this	O
boosts	O
movement	B-MethodName
pruning	I-MethodName
at	O
little	O
cost	O
.	O
In	O
this	O
section	O
,	O
we	O
conduct	O
an	O
ablation	O
study	O
to	O
evaluate	O
the	O
impact	O
of	O
distillation	O
using	O
a	O
BERT	B-MethodName
-	O
base	O
teacher	O
.	O

We	O
have	O
shown	O
that	O
we	O
can	O
extract	O
small	O
pruned	O
models	O
that	O
are	O
at	O
an	O
equivalent	O
or	O
better	O
than	O
distilled	O
networks	O
.	O
This	O
approach	O
can	O
be	O
done	O
during	O
fine	O
-	O
tuning	O
and	O
not	O
pre	O
-	O
training	O
.	O
The	O
method	O
does	O
not	O
resort	O
to	O
techniques	O
such	O
as	O
data	B-TaskName
augmentation	I-TaskName
or	O
architecture	O
search	O
,	O
and	O
it	O
works	O
on	O
a	O
diverse	O
set	O
of	O
tasks	O
and	O
base	O
models	O
.	O
As	O
better	O
and	O
larger	O
models	O
are	O
published	O
at	O
an	O
increasing	O
pace	O
,	O
we	O
can	O
rely	O
on	O
a	O
simple	O
and	O
robust	O
method	O
to	O
accelerate	O
them	O
on	O
specific	O
tasks	O
without	O
sacrificing	O
accuracy	B-MetricName
and	O
distribute	O
these	O
models	O
easily	O
while	O
keeping	O
most	O
of	O
the	O
original	O
model	O
accuracy	B-MetricName
.	O

The	O
hyperparameters	O
of	O
the	O
experiments	O
are	O
available	O
as	O
JSON	O
files	O
(	O
one	O
file	O
per	O
task	O
)	O
in	O
the	O
same	O
repository	O
:	O
each	O
entry	O
contains	O
all	O
the	O
information	O
to	O
fine	O
-	O
tune	O
and	O
prune	O
the	O
model	O
,	O
its	O
evaluation	O
results	O
,	O
and	O
detailed	O
statistics	O
about	O
its	O
final	O
sparsity	O
.	O
For	O
example	O
,	O
the	O
SQuAD	B-DatasetName
V1	O
checkpoints	O
referenced	O
in	O
this	O
paper	O
are	O
listed	O
with	O
the	O
hyperparameters	O
and	O
related	O
information	O
.	O

Some	O
of	O
the	O
models	O
we	O
produced	O
during	O
this	O
research	O
can	O
be	O
used	O
directly	O
from	O
the	O
Hugging	O
Face	B-TaskName
model	I-TaskName
hub	O
.	O
The	O
other	O
models	O
and	O
the	O
checkpoints	O
,	O
including	O
the	O
intermediary	O
ones	O
that	O
were	O
saved	O
during	O
training	O
,	O
are	O
available	O
on	O
Amazon	O
S3	O
.	O

We	O
introduce	O
the	O
first	O
dataset	O
for	O
human	O
edits	O
of	O
machine	O
-	O
generated	O
visual	O
stories	O
and	O
explore	O
how	O
these	O
collected	O
edits	O
may	O
be	O
used	O
for	O
the	O
visual	O
story	O
post	O
-	O
editing	O
task	O
.	O
The	O
dataset	O
,	O
VIST	B-DatasetName
-	I-DatasetName
Edit	I-DatasetName
1	O
,	O
includes	O
14	O
,	O
905	O
humanedited	O
versions	O
of	O
2	O
,	O
981	O
machine	O
-	O
generated	O
visual	O
stories	O
.	O
The	O
stories	O
were	O
generated	O
by	O
two	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
visual	B-TaskName
storytelling	I-TaskName
models	O
,	O
each	O
aligned	O
to	O
5	O
human	O
-	O
edited	O
versions	O
.	O
We	O
establish	O
baselines	O
for	O
the	O
task	O
,	O
showing	O
how	O
a	O
relatively	O
small	O
set	O
of	O
human	O
edits	O
can	O
be	O
leveraged	O
to	O
boost	O
the	O
performance	O
of	O
large	O
visual	B-TaskName
storytelling	I-TaskName
models	O
.	O
We	O
also	O
discuss	O
the	O
weak	O
correlation	O
between	O
automatic	O
evaluation	O
scores	O
and	O
human	O
ratings	O
,	O
motivating	O
the	O
need	O
for	O
new	O
automatic	O
metrics	O
.	O

Professional	O
writers	O
emphasize	O
the	O
importance	O
of	O
editing	O
.	O
Stephen	O
King	O
once	O
put	O
it	O
this	O
way	O
:	O
"	O
to	O
write	O
is	O
human	O
,	O
to	O
edit	O
is	O
divine	O
.	O
"	O
(	O
King	O
,	O
2000	O
)	O
Mark	O
Twain	O
had	O
another	O
quote	O
:	O
"	O
Writing	O
is	O
easy	O
.	O
All	O
you	O
have	O
to	O
do	O
is	O
cross	O
out	O
the	O
wrong	O
words	O
.	O
"	O
(	O
Twain	O
,	O
1876	O
)	O
Given	O
that	O
professionals	O
revise	O
and	O
rewrite	O
their	O
drafts	O
intensively	O
,	O
machines	O
that	O
generate	O
stories	O
may	O
also	O
benefit	O
from	O
a	O
good	O
editor	O
.	O
Per	O
the	O
evaluation	O
of	O
the	O
first	O
Visual	B-TaskName
Storytelling	I-TaskName
Challenge	O
(	O
Mitchell	O
et	O
al	O
,	O
2018	O
)	O
,	O
the	O
ability	O
of	O
an	O
algorithm	O
to	O
tell	O
a	O
sound	O
story	O
is	O
still	O
far	O
from	O
that	O
of	O
a	O
human	O
.	O
Users	O
will	O
inevitably	O
need	O
to	O
edit	O
generated	O
stories	O
before	O
putting	O
them	O
to	O
real	O
uses	O
,	O
such	O
as	O
sharing	O
on	O
social	O
media	O
.	O
We	O
introduce	O
the	O
first	O
dataset	O
for	O
human	O
edits	O
of	O
machine	O
-	O
generated	O
visual	O
stories	O
,	O
VIST	B-DatasetName
-	I-DatasetName
Edit	I-DatasetName
,	O
and	O
explore	O
how	O
these	O
collected	O
edits	O
may	O
be	O
used	O
for	O
the	O
task	O
of	O
visual	O
story	O
post	O
-	O
editing	O
(	O
see	O
Figure	O
1	O
)	O
.	O
The	O
original	O
visual	B-TaskName
storytelling	I-TaskName
(	O
VIST	B-DatasetName
)	O
task	O
,	O
as	O
introduced	O
by	O
Huang	O
et	O
al	O
(	O
2016	O
)	O
,	O
takes	O
a	O
sequence	O
of	O
five	O
photos	O
as	O
input	O
and	O
generates	O
a	O
short	O
story	O
describing	O
the	O
photo	O
sequence	O
.	O
Huang	O
et	O
al	O
also	O
released	O
the	O
VIST	B-DatasetName
dataset	O
,	O
containing	O
20	O
,	O
211	O
photo	O
sequences	O
,	O
aligned	O
to	O
human	O
-	O
written	O
stories	O
.	O
On	O
the	O
other	O
hand	O
,	O
the	O
automatic	O
postediting	O
task	O
revises	O
the	O
story	O
generated	O
from	O
visual	B-TaskName
storytelling	I-TaskName
models	O
,	O
given	O
both	O
a	O
machinegenerated	O
story	O
and	O
a	O
photo	O
sequence	O
.	O
Automatic	B-TaskName
post	I-TaskName
-	I-TaskName
editing	I-TaskName
treats	O
the	O
VIST	B-DatasetName
system	O
as	O
a	O
black	O
box	O
that	O
is	O
fixed	O
and	O
not	O
modifiable	O
.	O
Its	O
goal	O
is	O
to	O
correct	O
systematic	O
errors	O
of	O
the	O
VIST	B-DatasetName
system	O
and	O
leverage	O
the	O
user	O
edit	O
data	O
to	O
improve	O
story	O
quality	O
.	O
In	O
this	O
paper	O
,	O
we	O
(	O
i	O
)	O
collect	O
human	O
edits	O
for	O
machine	O
-	O
generated	O
stories	O
from	O
two	O
different	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
,	O
(	O
ii	O
)	O
analyze	O
what	O
people	O
edited	O
,	O
and	O
(	O
iii	O
)	O
advance	O
the	O
task	O
of	O
visual	O
story	O
post	O
-	O
editing	O
.	O
In	O
addition	O
,	O
we	O
establish	O
baselines	O
for	O
the	O
task	O
,	O
and	O
discuss	O
the	O
weak	O
correlation	O
between	O
automatic	O
evaluation	O
scores	O
and	O
human	O
ratings	O
,	O
motivating	O
the	O
need	O
for	O
new	O
metrics	O
.	O

The	O
visual	O
story	O
post	O
-	O
editing	O
task	O
is	O
related	O
to	O
(	O
i	O
)	O
automatic	B-TaskName
post	I-TaskName
-	I-TaskName
editing	I-TaskName
and	O
(	O
ii	O
)	O
stylized	O
visual	O
captioning	O
.	O
Automatic	B-TaskName
post	I-TaskName
-	I-TaskName
editing	I-TaskName
(	O
APE	B-DatasetName
)	O
revises	O
the	O
text	O
generated	O
typically	O
from	O
a	O
machine	B-TaskName
translation	I-TaskName
(	O
MT	O
)	O
system	O
,	O
given	O
both	O
the	O
source	O
sentences	O
and	O
translated	O
sentences	O
.	O
Like	O
the	O
proposed	O
VIST	B-DatasetName
post	O
-	O
editing	O
task	O
,	O
APE	B-DatasetName
aims	O
to	O
correct	O
the	O
systematic	O
errors	O
of	O
MT	O
,	O
reducing	O
translator	O
workloads	O
and	O
increasing	O
productivity	O
(	O
Astudillo	O
et	O
al	O
,	O
2018	O
)	O
.	O
Recently	O
,	O
neural	O
models	O
have	O
been	O
applied	O
to	O
APE	B-DatasetName
in	O
a	O
sentence	O
-	O
to	O
-	O
sentence	O
manner	O
(	O
Libovickỳ	O
et	O
al	O
,	O
2016	O
;	O
Junczys	O
-	O
Dowmunt	O
and	O
Grundkiewicz	O
,	O
2016	O
)	O
,	O
differing	O
from	O
previous	O
phrase	O
-	O
based	O
models	O
that	O
translate	O
and	O
reorder	O
phrase	O
segments	O
for	O
each	O
sentence	O
,	O
such	O
as	O
(	O
Simard	O
et	O
al	O
,	O
2007	O
;	O
Béchara	O
et	O
al	O
,	O
2011	O
)	O
.	O
More	O
sophisticated	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
with	O
the	O
attention	O
mechanism	O
were	O
also	O
introduced	O
(	O
Junczys	O
-	O
Dowmunt	O
and	O
Grundkiewicz	O
,	O
2017	O
;	O
Libovickỳ	O
and	O
Helcl	O
,	O
2017	O
)	O
.	O
While	O
this	O
line	O
of	O
work	O
is	O
relevant	O
and	O
encouraging	O
,	O
it	O
has	O
not	O
explored	O
much	O
in	O
a	O
creative	O
writing	O
context	O
.	O
It	O
is	O
noteworthy	O
that	O
Roemmele	O
et	O
al	O
previously	O
developed	O
an	O
online	O
system	O
,	O
Creative	O
Help	O
,	O
for	O
collecting	O
human	O
edits	O
for	O
computer	O
-	O
generated	O
narrative	O
text	O
(	O
Roemmele	O
and	O
Gordon	O
,	O
2018b	O
)	O
.	O
The	O
collected	O
data	O
could	O
be	O
useful	O
for	O
story	O
APE	B-DatasetName
tasks	O
.	O
Visual	O
story	O
post	O
-	O
editing	O
could	O
also	O
be	O
considered	O
relevant	O
to	O
style	B-TaskName
transfer	I-TaskName
on	O
image	O
captions	O
.	O
Both	O
tasks	O
take	O
images	O
and	O
source	O
text	O
(	O
i.e.	O
,	O
machine	O
-	O
generated	O
stories	O
or	O
descriptive	O
captions	O
)	O
as	O
inputs	O
and	O
generate	O
modified	O
text	O
(	O
i.e.	O
,	O
postedited	O
stories	O
or	O
stylized	O
captions	O
)	O
.	O
End	O
-	O
to	O
-	O
end	O
neural	O
models	O
have	O
been	O
applied	O
to	O
the	O
transfer	O
styles	O
of	O
image	O
captions	O
.	O
For	O
example	O
,	O
StyleNet	O
,	O
an	O
encoder	O
-	O
decoder	O
-	O
based	O
model	O
trained	O
on	O
paired	O
images	O
and	O
factual	O
captions	O
together	O
with	O
an	O
unlabeled	O
stylized	O
text	O
corpus	O
,	O
can	O
transfer	O
descriptive	O
image	O
captions	O
to	O
creative	O
captions	O
,	O
e.g.	O
,	O
humorous	O
or	O
romantic	O
(	O
Gan	O
et	O
al	O
,	O
2017	O
)	O
.	O
Its	O
advanced	O
version	O
with	O
an	O
attention	O
mechanism	O
,	O
SemStyle	O
,	O
was	O
also	O
introduced	O
(	O
Mathews	O
et	O
al	O
,	O
2018	O
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
adopt	O
the	O
APE	B-DatasetName
approach	O
to	O
treat	O
preand	O
post	O
-	O
edited	O
stories	O
as	O
parallel	O
data	O
instead	O
of	O
the	O
style	B-TaskName
transfer	I-TaskName
approach	O
that	O
omits	O
this	O
parallel	O
relationship	O
during	O
model	O
training	O
.	O

Obtaining	O
Machine	O
-	O
Generated	O
Visual	O
Stories	O
This	O
VIST	B-DatasetName
-	I-DatasetName
Edit	I-DatasetName
dataset	O
contains	O
visual	O
stories	O
gen	O
-	O
erated	O
by	O
two	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
,	O
GLAC	O
and	O
AREL	O
.	O
GLAC	O
(	O
Global	B-MethodName
-	I-MethodName
Local	I-MethodName
Attention	I-MethodName
Cascading	O
Networks	O
)	O
(	O
Kim	O
et	O
al	O
,	O
2018	O
)	O
achieved	O
the	O
highest	O
human	O
evaluation	O
score	O
in	O
the	O
first	O
VIST	B-DatasetName
Challenge	O
(	O
Mitchell	O
et	O
al	O
,	O
2018	O
)	O
.	O
We	O
obtain	O
the	O
pre	O
-	O
trained	O
GLAC	O
model	O
provided	O
by	O
the	O
authors	O
via	O
Github	O
and	O
run	O
it	O
on	O
the	O
entire	O
VIST	B-DatasetName
test	O
set	O
and	O
obtain	O
2	O
,	O
019	O
stories	O
.	O
AREL	O
(	O
Adversarial	O
REward	O
Learning	O
)	O
was	O
the	O
earliest	O
available	O
implementation	O
online	O
,	O
and	O
achieved	O
the	O
highest	O
METEOR	B-DatasetName
score	O
on	O
public	O
test	O
set	O
in	O
the	O
VIST	B-DatasetName
Challenge	O
.	O
We	O
also	O
acquire	O
a	O
small	O
set	O
of	O
human	O
edits	O
for	O
962	O
AREL	O
's	O
stories	O
generated	O
using	O
VIST	B-DatasetName
test	O
set	O
,	O
collected	O
by	O
Hsu	O
et	O
al	O
(	O
2019	O
)	O
.	O
Crowdsourcing	O
Edits	O
For	O
each	O
machinegenerated	O
visual	O
story	O
,	O
we	O
recruit	O
five	O
crowd	O
workers	O
from	O
Amazon	O
Mechanical	O
Turk	O
(	O
MTurk	O
)	O
to	O
revise	O
it	O
(	O
at	O
$	O
0.12	O
/	O
HIT	O
,	O
)	O
respectively	O
.	O
We	O
instruct	O
workers	O
to	O
edit	O
the	O
story	O
"	O
as	O
if	O
these	O
were	O
your	O
photos	O
,	O
and	O
you	O
would	O
like	O
using	O
this	O
story	O
to	O
share	O
your	O
experience	O
with	O
your	O
friends	O
.	O
"	O
We	O
also	O
ask	O
workers	O
to	O
stick	O
with	O
the	O
photos	O
of	O
the	O
original	O
story	O
so	O
that	O
workers	O
would	O
not	O
ignore	O
the	O
machine	O
-	O
generated	O
story	O
and	O
write	O
a	O
new	O
one	O
from	O
scratch	O
.	O
Figure	O
2	O
shows	O
the	O
interface	O
.	O
For	O
GLAC	O
,	O
we	O
collect	O
2	O
,	O
019	O
×	O
5	O
=	O
10	O
,	O
095	O
edited	O
stories	O
in	O
total	O
;	O
and	O
for	O
AREL	O
,	O
962	O
×	O
5	O
=	O
4	O
,	O
810	O
edited	O
stories	O
have	O
been	O
collected	O
by	O
Hsu	O
et	O
al	O
(	O
2019	O
)	O
.	O
Data	O
Post	O
-	O
processing	O
We	O
tokenize	O
all	O
stories	O
using	O
CoreNLP	O
(	O
Manning	O
et	O
al	O
,	O
2014	O
)	O
and	O
replace	O
all	O
people	O
names	O
with	O
generic	O
[	O
male	O
/	O
female	O
]	O
tokens	O
.	O
Each	O
of	O
GLAC	O
and	O
AREL	O
set	O
is	O
released	O
as	O
training	O
,	O
validation	O
,	O
and	O
test	O
following	O
an	O
80	O
%	O
,	O
10	O
%	O
,	O
10	O
%	O
split	O
,	O
respectively	O
.	O

We	O
analyze	O
human	O
edits	O
for	O
GLAC	O
and	O
AREL	O
.	O
First	O
,	O
crowd	O
workers	O
systematically	O
increase	O
lexical	O
diversity	O
.	O
We	O
use	O
type	O
-	O
token	O
ratio	O
(	O
TTR	O
)	O
,	O
the	O
ratio	O
between	O
the	O
number	O
of	O
word	O
types	O
and	O
the	O
number	O
of	O
tokens	O
,	O
to	O
estimate	O
the	O
lexical	O
diversity	O
of	O
a	O
story	O
(	O
Hardie	O
and	O
McEnery	O
,	O
2006	O
)	O
.	O
Figure	O
3	O
shows	O
significant	O
(	O
p<.001	O
,	O
paired	O
t	O
-	O
test	O
)	O
positive	O
shifts	O
of	O
TTR	O
for	O
both	O
AREL	O
and	O
GLAC	O
,	O
which	O
confirms	O
the	O
findings	O
in	O
Hsu	O
et	O
al	O
(	O
2019	O
)	O
.	O
Figure	O
3	O
also	O
indicates	O
that	O
GLAC	O
generates	O
stories	O
with	O
higher	O
lexical	O
diversity	O
than	O
that	O
of	O
AREL	O
.	O
Second	O
,	O
people	O
shorten	O
AREL	O
's	O
stories	O
but	O
lengthen	O
GLAC	O
's	O
stories	O
.	O
We	O
calculate	O
the	O
average	O
number	O
of	O
Part	B-DatasetName
-	I-DatasetName
Of	I-DatasetName
-	O
Speech	O
(	O
POS	O
)	O
tags	O
for	O
tokens	O
in	O
each	O
story	O
using	O
the	O
python	O
NLTK	O
(	O
Bird	O
et	O
al	O
,	O
2009	O
)	O
package	O
,	O
as	O
shown	O
in	O
Table	O
1	O
.	O
We	O
also	O
find	O
that	O
the	O
average	O
number	O
of	O
tokens	O
in	O
an	O
AREL	O
story	O
(	O
43.0	O
,	O
SD=5.0	O
)	O
decreases	O
(	O
41.9	O
,	O
SD=5.6	O
)	O
after	O
human	O
editing	O
,	O
while	O
that	O
of	O
GLAC	O
(	O
35.0	O
,	O
SD=4.5	O
)	O
increases	O
(	O
36.7	O
,	O
SD=5.9	O
)	O
.	O
Hsu	O
has	O
observed	O
that	O
people	O
often	O
replace	O
"	O
determiner	O
/	O
article	O
+	O
noun	O
"	O
phrases	O
(	O
e.g.	O
,	O
"	O
a	O
boy	O
"	O
)	O
with	O
pronouns	O
(	O
e.g.	O
,	O
"	O
he	O
"	O
)	O
in	O
AREL	O
stories	O
(	O
2019	O
)	O
.	O
However	O
,	O
this	O
observation	O
can	O
not	O
explain	O
the	O
story	O
lengthening	O
in	O
GLAC	O
,	O
where	O
each	O
story	O
on	O
average	O
has	O
an	O
increased	O
0.9	O
nouns	O
after	O
editing	O
.	O
Given	O
the	O
average	O
per	O
-	O
story	O
edit	O
distances	O
(	O
Levenshtein	O
,	O
1966	O
;	O
Damerau	O
,	O
1964	O
)	O
for	O
AREL	O
(	O
16.84	O
,	O
SD=5.64	O
)	O
and	O
GLAC	O
(	O
17.99	O
,	O
SD=5.56	O
)	O
are	O
similar	O
,	O
this	O
difference	O
is	O
unlikely	O
to	O
be	O
caused	O
by	O
deviation	O
in	O
editing	O
amount	O
.	O
Deleting	O
extra	O
words	O
requires	O
much	O
less	O
time	O
than	O
other	O
editing	O
operations	O
(	O
Popovic	O
et	O
al	O
,	O
2014	O
)	O
.	O
Per	O
Figure	O
3	O
,	O
AREL	O
's	O
stories	O
are	O
much	O
more	O
repetitive	O
.	O
We	O
further	O
analyze	O
the	O
type	O
-	O
token	O
ratio	O
for	O
nouns	O
(	O
T	O
T	O
R	O
noun	O
)	O
and	O
find	O
AREL	O
generates	O
duplicate	O
nouns	O
.	O
The	O
average	O
T	O
T	O
R	O
noun	O
of	O
an	O
AREL	O
's	O
story	O
is	O
0.76	O
while	O
that	O
of	O
GLAC	O
is	O
0.90	O
.	O
For	O
reference	O
,	O
the	O
average	O
T	O
T	O
R	O
noun	O
of	O
a	O
human	O
-	O
written	O
story	O
(	O
the	O
entire	O
VIST	B-DatasetName
dataset	O
)	O
is	O
0.86	O
.	O
Thus	O
,	O
we	O
hypothesize	O
workers	O
prioritized	O
their	O
efforts	O
in	O
deleting	O
repetitive	O
words	O
for	O
AREL	O
,	O
resulting	O
in	O
the	O
reduction	O
of	O
story	O
length	O
.	O

We	O
report	O
baseline	O
experiments	O
on	O
the	O
visual	O
story	O
post	O
-	O
editing	O
task	O
in	O
Table	O
2	O
.	O
AREL	O
's	O
post	O
-	O
editing	O
models	O
are	O
trained	O
on	O
the	O
augmented	O
AREL	O
training	O
set	O
and	O
evaluated	O
on	O
the	O
AREL	O
test	O
set	O
of	O
VIST	B-DatasetName
-	I-DatasetName
Edit	I-DatasetName
,	O
and	O
GLAC	O
's	O
models	O
are	O
tested	O
using	O
GLAC	O
sets	O
,	O
too	O
.	O
Figure	O
4	O
shows	O
examples	O
of	O
the	O
output	O
.	O
Human	O
evaluations	O
(	O
Table	O
2	O
)	O
indicate	O
that	O
the	O
post	O
-	O
editing	O
model	O
improves	O
visual	O
story	O
quality	O
.	O

Two	O
neural	O
approaches	O
,	O
Long	B-MethodName
short	I-MethodName
-	I-MethodName
term	I-MethodName
memory	I-MethodName
(	O
LSTM	B-MethodName
)	O
and	O
Transformer	B-MethodName
,	O
are	O
used	O
as	O
baselines	O
,	O
where	O
we	O
experiment	O
using	O
(	O
i	O
)	O
text	O
only	O
(	O
T	O
)	O
and	O
(	O
ii	O
)	O
both	O
text	O
and	O
images	O
(	O
T+I	O
)	O
as	O
inputs	O
.	O
LSTM	B-MethodName
An	O
LSTM	B-MethodName
seq2seq	B-MethodName
model	O
is	O
used	O
(	O
Sutskever	O
et	O
al	O
,	O
2014	O
)	O
.	O
For	O
the	O
text	O
-	O
only	O
setting	O
,	O
the	O
original	O
stories	O
and	O
the	O
human	O
-	O
edited	O
stories	O
are	O
treated	O
as	O
source	O
-	O
target	O
pairs	O
.	O
For	O
the	O
text	O
-	O
image	O
setting	O
,	O
we	O
first	O
extract	O
the	O
image	O
features	O
using	O
the	O
pre	O
-	O
trained	O
ResNet	B-MethodName
-	O
152	O
model	O
and	O
represent	O
each	O
image	O
as	O
a	O
2048	B-DatasetName
-	O
dimensional	O
vector	O
.	O
We	O
then	O
apply	O
a	O
dense	O
layer	O
on	O
image	O
features	O
in	O
order	O
to	O
both	O
fit	O
its	O
dimension	O
to	O
the	O
word	O
embedding	O
and	O
learn	O
the	O
adjusting	O
transformation	O
.	O
By	O
placing	O
the	O
image	O
features	O
in	O
front	O
of	O
the	O
sequence	O
of	O
text	O
embedding	O
,	O
the	O
input	O
sequence	O
becomes	O
a	O
matrix	O
R	O
(	O
5+len	O
)	O
×dim	O
,	O
where	O
len	O
is	O
the	O
text	O
sequence	O
length	O
,	O
5	O
means	O
5	O
photos	O
,	O
and	O
dim	O
is	O
the	O
dimension	O
of	O
the	O
word	O
embedding	O
.	O
The	O
input	O
sequence	O
with	O
both	O
image	O
information	O
and	O
text	O
information	O
is	O
then	O
encoded	O
by	O
LSTM	B-MethodName
,	O
identical	O
as	O
in	O
the	O
text	O
-	O
only	O
setting	O
.	O

We	O
also	O
use	O
the	O
Transformer	B-MethodName
architecture	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
)	O
,	O
Written	O
-	O
by	O
-	O
a	O
-	O
Human	O
(	O
"	O
This	O
story	O
sounds	O
like	O
it	O
was	O
written	O
by	O
a	O
human	O
.	O
"	O
)	O
,	O
Visually	O
-	O
Grounded	O
,	O
and	O
Detailed	O
.	O
We	O
take	O
the	O
average	O
of	O
the	O
five	O
judgments	O
as	O
the	O
final	O
score	O
for	O
each	O
story	O
.	O
LSTM	B-MethodName
(	O
T	O
)	O
improves	O
all	O
aspects	O
for	O
stories	O
by	O
AREL	O
,	O
and	O
improves	O
"	O
Focus	O
"	O
and	O
"	O
Human	O
-	O
like	O
"	O
aspects	O
for	O
stories	O
by	O
GLAC	O
.	O
enriched	O
embedding	O
.	O
It	O
is	O
noteworthy	O
that	O
the	O
position	O
encoding	O
is	O
only	O
applied	O
on	O
text	O
embedding	O
.	O
The	O
input	O
matrix	O
R	O
(	O
len+5	O
)	O
×dim	O
is	O
then	O
passed	O
into	O
the	O
Transformer	B-MethodName
as	O
in	O
the	O
text	O
-	O
only	O
setting	O
.	O

Data	B-TaskName
Augmentation	I-TaskName
In	O
order	O
to	O
obtain	O
sufficient	O
training	O
samples	O
for	O
neural	O
models	O
,	O
we	O
pair	O
lessedited	O
stories	O
with	O
more	O
-	O
edited	O
stories	O
of	O
the	O
same	O
photo	O
sequence	O
to	O
augment	O
the	O
data	O
.	O
In	O
VIST	B-DatasetName
-	I-DatasetName
Edit	I-DatasetName
,	O
five	O
human	O
-	O
edited	O
stories	O
are	O
collected	O
for	O
each	O
photo	O
sequence	O
.	O
We	O
use	O
the	O
human	O
-	O
edited	O
stories	O
that	O
are	O
less	O
edited	O
-	O
measured	O
by	O
its	O
Normalized	O
Damerau	O
-	O
Levenshtein	O
distance	O
(	O
Levenshtein	O
,	O
1966	O
;	O
Damerau	O
,	O
1964	O
)	O
to	O
the	O
original	O
story	O
-	O
as	O
the	O
source	O
and	O
pair	O
them	O
with	O
the	O
stories	O
that	O
are	O
more	O
edited	O
(	O
as	O
the	O
target	O
.	O
)	O
This	O
data	B-TaskName
augmentation	I-TaskName
strategy	O
gives	O
us	O
in	O
total	O
fifteen	O
(	O
5	O
2	O
+5	O
=	O
15	O
)	O
training	O
samples	O
given	O
five	O
human	O
-	O
edited	O
stories	O
.	O
Human	O
Evaluation	O
Following	O
the	O
evaluation	O
procedure	O
of	O
the	O
first	O
VIST	B-DatasetName
Challenge	O
(	O
Mitchell	O
et	O
al	O
,	O
2018	O
)	O
,	O
for	O
each	O
visual	O
story	O
,	O
we	O
recruit	O
five	O
human	O
judges	O
on	O
MTurk	O
to	O
rate	O
it	O
on	O
six	O
aspects	O
(	O
at	O
$	O
0.1	O
/	O
HIT	O
.	O
)	O
We	O
take	O
the	O
average	O
of	O
the	O
five	O
judgments	O
as	O
the	O
final	O
scores	O
for	O
the	O
story	O
.	O
Table	O
2	O
shows	O
the	O
results	O
.	O
The	O
LSTM	B-MethodName
using	O
text	O
-	O
only	O
input	O
outperforms	O
all	O
other	O
baselines	O
.	O
It	O
improves	O
all	O
six	O
aspects	O
for	O
stories	O
by	O
AREL	O
,	O
and	O
improves	O
"	O
Focus	O
"	O
and	O
"	O
Human	O
-	O
like	O
"	O
aspects	O
for	O
stories	O
by	O
GLAC	O
.	O
These	O
results	O
demonstrate	O
that	O
a	O
relatively	O
small	O
set	O
of	O
human	O
edits	O
can	O
be	O
used	O
to	O
boost	O
the	O
story	O
quality	O
of	O
an	O
existing	O
large	O
VIST	B-DatasetName
model	O
.	O
Table	O
2	O
also	O
suggests	O
that	O
the	O
quality	O
of	O
a	O
post	O
-	O
edited	O
story	O
is	O
heavily	O
decided	O
by	O
its	O
pre	O
-	O
edited	O
version	O
.	O
Even	O
after	O
editing	O
by	O
human	O
editors	O
,	O
AREL	O
's	O
stories	O
still	O
do	O
not	O
achieve	O
the	O
quality	O
of	O
pre	O
-	O
edited	O
stories	O
by	O
GLAC	O
.	O
The	O
inefficacy	O
of	O
image	O
features	O
and	O
Transformer	B-MethodName
model	O
might	O
be	O
caused	O
by	O
the	O
small	O
size	O
of	O
VIST	B-DatasetName
-	I-DatasetName
Edit	I-DatasetName
.	O
It	O
also	O
requires	O
further	O
research	O
to	O
develop	O
a	O
post	O
-	O
editing	O
model	O
in	O
a	O
multimodal	O
context	O
.	O

VIST	B-DatasetName
-	I-DatasetName
Edit	I-DatasetName
,	O
the	O
first	O
dataset	O
for	O
human	O
edits	O
of	O
machine	O
-	O
generated	O
visual	O
stories	O
,	O
is	O
introduced	O
.	O
We	O
argue	O
that	O
human	O
editing	O
on	O
machinegenerated	O
stories	O
is	O
unavoidable	O
,	O
and	O
such	O
edited	O
data	O
can	O
be	O
leveraged	O
to	O
enable	O
automatic	O
postediting	O
.	O
We	O
have	O
established	O
baselines	O
for	O
the	O
task	O
of	O
visual	O
story	O
post	O
-	O
editing	O
,	O
and	O
have	O
motivated	O
the	O
need	O
for	O
a	O
new	O
automatic	O
evaluation	O
metric	O
.	O

Evaluation	O
of	O
acoustic	O
word	B-TaskName
embeddings	I-TaskName

Recently	O
,	O
researchers	O
in	O
speech	B-TaskName
recognition	I-TaskName
have	O
started	O
to	O
reconsider	O
using	O
whole	O
words	O
as	O
the	O
basic	O
modeling	O
unit	O
,	O
instead	O
of	O
phonetic	O
units	O
.	O
These	O
systems	O
rely	O
on	O
a	O
function	O
that	O
embeds	O
an	O
arbitrary	O
or	O
fixed	O
dimensional	O
speech	O
segments	O
to	O
a	O
vector	O
in	O
a	O
fixed	O
-	O
dimensional	O
space	O
,	O
named	O
acoustic	O
word	O
embedding	O
.	O
Thus	O
,	O
speech	O
segments	O
of	O
words	O
that	O
sound	O
similarly	O
will	O
be	O
projected	O
in	O
a	O
close	O
area	O
in	O
a	O
continuous	O
space	O
.	O
This	O
paper	O
focuses	O
on	O
the	O
evaluation	O
of	O
acoustic	O
word	B-TaskName
embeddings	I-TaskName
.	O
We	O
propose	O
two	O
approaches	O
to	O
evaluate	O
the	O
intrinsic	O
performances	O
of	O
acoustic	O
word	B-TaskName
embeddings	I-TaskName
in	O
comparison	O
to	O
orthographic	O
representations	O
in	O
order	O
to	O
evaluate	O
whether	O
they	O
capture	O
discriminative	O
phonetic	O
information	O
.	O
Since	O
French	O
language	O
is	O
targeted	O
in	O
experiments	O
,	O
a	O
particular	O
focus	O
is	O
made	O
on	O
homophone	O
words	O
.	O

Recent	O
studies	O
have	O
started	O
to	O
reconsider	O
the	O
use	O
of	O
whole	O
words	O
as	O
the	O
basic	O
modeling	O
unit	O
in	O
speech	B-TaskName
recognition	I-TaskName
and	O
query	O
applications	O
,	O
instead	O
of	O
phonetic	O
units	O
.	O
These	O
systems	O
are	O
based	O
on	O
the	O
use	O
of	O
acoustic	O
word	O
embedding	O
,	O
which	O
are	O
projection	O
of	O
arbitrary	O
or	O
fixed	O
dimensional	O
speech	O
segments	O
into	O
a	O
continuous	O
space	O
,	O
in	O
a	O
manner	O
that	O
preserve	O
acoustic	O
similarity	O
between	O
words	O
.	O
Thus	O
,	O
speech	O
segments	O
of	O
words	O
that	O
sound	O
similarly	O
will	O
have	O
similar	O
embeddings	O
.	O
Acoustic	O
word	O
embedding	O
were	O
successfully	O
used	O
in	O
a	O
queryby	O
-	O
example	O
search	O
system	O
(	O
Kamper	O
et	O
al	O
,	O
2015	O
;	O
Levin	O
et	O
al	O
,	O
2013	O
)	O
and	O
in	O
a	O
ASR	O
lattice	O
re	O
-	O
scoring	O
system	O
(	O
Bengio	O
and	O
Heigold	O
,	O
2014	O
)	O
.	O
The	O
authors	O
in	O
(	O
Bengio	O
and	O
Heigold	O
,	O
2014	O
)	O
proposed	O
an	O
approach	O
to	O
build	O
acoustic	O
word	O
em	O
-	O
beddings	O
from	O
an	O
orthographic	O
representation	O
of	O
the	O
word	O
.	O
This	O
paper	O
focuses	O
on	O
the	O
evaluation	O
of	O
these	O
acoustic	O
word	B-TaskName
embeddings	I-TaskName
.	O
We	O
propose	O
two	O
approaches	O
to	O
evaluate	O
the	O
intrinsic	O
performances	O
of	O
acoustic	O
word	B-TaskName
embeddings	I-TaskName
in	O
comparison	O
to	O
orthographic	O
representations	O
.	O
In	O
particular	O
we	O
want	O
to	O
evaluate	O
whether	O
they	O
capture	O
discriminative	O
information	O
about	O
their	O
pronunciation	O
,	O
approximated	O
by	O
their	O
phonetic	O
representation	O
.	O
In	O
our	O
experiments	O
,	O
we	O
focus	O
on	O
French	O
language	O
whose	O
particularity	O
is	O
to	O
be	O
rich	O
of	O
homophone	O
words	O
.	O
This	O
aspect	O
is	O
also	O
studied	O
in	O
this	O
work	O
.	O

Table	O
1	O
:	O
Example	O
of	O
the	O
content	O
of	O
the	O
three	O
lists	O
.	O
In	O
the	O
case	O
of	O
the	O
orthographic	O
and	O
phonetic	O
similarity	O
tasks	O
,	O
the	O
evaluation	O
of	O
the	O
acoustic	O
embeddings	O
is	O
performed	O
by	O
ranking	O
the	O
pairs	O
according	O
to	O
their	O
cosine	O
similarities	O
and	O
measuring	O
the	O
Spearman	O
's	O
rank	O
correlation	O
coefficient	O
(	O
Spearman	O
's	O
ρ	O
)	O
.	O
This	O
approach	O
is	O
used	O
in	O
(	O
Gao	O
et	O
al	O
,	O
2014	O
;	O
Ji	O
et	O
al	O
,	O
2015	O
;	O
Levy	O
et	O
al	O
,	O
2015	O
;	O
Ghannay	O
et	O
al	O
,	O
2016	O
)	O
to	O
evaluate	O
the	O
linguistic	O
word	B-TaskName
embeddings	I-TaskName
on	O
similarity	O
tasks	O
,	O
in	O
which	O
the	O
similarity	O
scores	O
are	O
attributed	O
by	O
human	O
annotators	O
.	O
For	O
the	O
homophone	O
detection	O
task	O
,	O
the	O
evaluation	O
is	O
performed	O
in	O
terms	O
of	O
precision	O
.	O
For	O
each	O
word	O
w	O
in	O
the	O
Homophones	O
list	O
,	O
let	O
L	O
H	O
(	O
w	O
)	O
be	O
the	O
list	O
of	O
k	O
homophones	O
of	O
the	O
word	O
w	O
,	O
and	O
L	O
H	O
neighbour	O
(	O
w	O
)	O
be	O
the	O
list	O
of	O
k	O
nearest	O
neighbours	O
extracted	O
based	O
on	O
the	O
cosine	O
similarity	O
and	O
L	O
H	O
f	O
ound	O
(	O
w	O
)	O
be	O
the	O
intersection	O
between	O
L	O
H	O
(	O
w	O
)	O
and	O
L	O
H	O
neighbour	O
(	O
w	O
)	O
,	O
that	O
corresponds	O
to	O
the	O
list	O
of	O
homophones	O
found	O
of	O
the	O
word	O
w.	O
The	O
precision	O
P	O
w	O
of	O
the	O
word	O
w	O
is	O
defined	O
as	O
:	O
P	O
w	O
=	O
|	O
L	O
H	O
f	O
ound	O
(	O
w	O
)	O
|	O
|	O
L	O
H	O
(	O
w	O
)	O
|	O
(	O
3	O
)	O
where	O
|	O
.	O
|	O
refers	O
to	O
the	O
size	O
of	O
a	O
list	O
.	O
We	O
define	O
the	O
overall	O
homophone	O
detection	O
precision	O
on	O
the	O
Homophones	O
list	O
as	O
the	O
average	O
of	O
the	O
P	O
w	O
:	O
P	O
=	O
N	O
i=1	O
P	O
w	O
i	O
N	O
(	O
4	O
)	O
where	O
N	O
is	O
the	O
number	O
of	O
candidate	O
words	O
which	O
have	O
a	O
none	O
-	O
empty	O
Homophones	O
list	O
.	O
3	O
Experiments	O
on	O
acoustic	O
word	B-TaskName
embeddings	I-TaskName

The	O
embeddings	O
we	O
evaluate	O
are	O
built	O
from	O
two	O
different	O
vocabularies	O
:	O
the	O
one	O
used	O
to	O
train	O
the	O
neural	O
network	O
models	O
(	O
CNN	O
and	O
DNN	O
)	O
,	O
composed	O
of	O
52k	O
words	O
present	O
in	O
the	O
manual	O
transcriptions	O
of	O
the	O
488	O
hours	O
of	O
audio	O
;	O
and	O
another	O
one	O
composed	O
of	O
160k	O
words	O
.	O
The	O
words	O
present	O
in	O
the	O
52k	O
vocabulary	O
are	O
nearly	O
all	O
present	O
in	O
the	O
160k	O
vocabulary	O
.	O
The	O
evaluation	O
sets	O
described	O
in	O
section	O
2.2	O
are	O
generated	O
from	O
these	O
two	O
vocabularies	O
:	O
in	O
the	O
52k	O
vocabulary	O
,	O
all	O
the	O
acoustic	O
word	B-TaskName
embeddings	I-TaskName
w	O
+	O
are	O
related	O
to	O
words	O
which	O
have	O
been	O
observed	O
during	O
the	O
training	O
of	O
the	O
CNN	O
.	O
This	O
means	O
that	O
at	O
least	O
two	O
acoustic	O
signal	O
embeddings	O
have	O
been	O
computed	O
from	O
the	O
audio	O
for	O
each	O
one	O
of	O
these	O
words	O
;	O
in	O
the	O
160k	O
vocabulary	O
,	O
about	O
110k	O
acoustic	O
word	B-TaskName
embeddings	I-TaskName
were	O
computed	O
for	O
words	O
never	O
observed	O
in	O
the	O
audio	O
data	O
.	O

The	O
quantitative	O
evaluation	O
of	O
the	O
acoustic	O
word	B-TaskName
embeddings	I-TaskName
w	O
+	O
is	O
performed	O
on	O
orthographic	O
similarity	O
,	O
phonetic	O
similarity	O
,	O
and	O
homophones	O
detection	O
tasks	O
.	O
Results	O
are	O
summarized	O
in	O
table	O
2	O
They	O
show	O
that	O
the	O
acoustic	O
word	B-TaskName
embeddings	I-TaskName
w	O
+	O
are	O
more	O
relevant	O
for	O
the	O
phonetic	O
similarity	O
task	O
,	O
while	O
o	O
+	O
are	O
obviously	O
the	O
best	O
ones	O
on	O
the	O
orthographic	O
similarity	O
task	O
.	O
These	O
results	O
show	O
that	O
the	O
projection	O
of	O
the	O
orthographic	O
embeddings	O
o	O
+	O
into	O
the	O
acoustic	O
embeddings	O
space	O
s	O
changes	O
their	O
properties	O
,	O
since	O
they	O
have	O
captured	O
more	O
information	O
about	O
word	O
pronunciation	O
while	O
they	O
have	O
lost	O
information	O
about	O
spelling	O
.	O
So	O
,	O
in	O
addition	O
to	O
making	O
possible	O
a	O
measure	O
of	O
similarity	O
distance	O
between	O
the	O
acoustic	O
signal	O
(	O
represented	O
by	O
s	O
)	O
and	O
a	O
word	O
(	O
represented	O
by	O
w	O
+	O
)	O
,	O
acoustic	O
word	B-TaskName
embeddings	I-TaskName
are	O
better	O
than	O
orthographic	O
ones	O
to	O
measure	O
the	O
phonetic	O
proximity	O
between	O
two	O
words	O
.	O
For	O
the	O
homophone	O
detection	O
task	O
,	O
the	O
Homophones	O
list	O
is	O
computed	O
from	O
the	O
160k	O
vocabulary	O
:	O
that	O
results	O
to	O
53869	O
homophone	O
pairs	O
in	O
total	O
.	O
The	O
52k	O
vocabulary	O
contains	O
13561	O
homophone	O
pairs	O
which	O
are	O
included	O
in	O
the	O
pairs	O
present	O
in	O
the	O
160k	O
vocabulary	O
.	O
As	O
we	O
can	O
see	O
,	O
the	O
w	O
+	O
acoustic	O
embeddings	O
outperform	O
the	O
orthographic	O
ones	O
on	O
this	O
task	O
on	O
the	O
two	O
data	O
sets	O
.	O
This	O
confirms	O
that	O
acoustic	O
word	B-TaskName
embeddings	I-TaskName
have	O
captured	O
additional	O
information	O
about	O
word	O
pronunciation	O
than	O
the	O
one	O
carried	O
by	O
orthographic	O
word	B-TaskName
embeddings	I-TaskName
.	O
For	O
this	O
task	O
we	O
can	O
not	O
compare	O
the	O
results	O
between	O
the	O
two	O
vocabularies	O
,	O
since	O
the	O
precision	O
measure	O
is	O
dependent	O
to	O
the	O
number	O
of	O
events	O
.	O
For	O
the	O
Spearman	O
's	O
correlation	O
,	O
a	O
comparison	O
is	O
roughly	O
possible	O
and	O
results	O
show	O
that	O
the	O
way	O
to	O
compute	O
w	O
+	O
is	O
effective	O
to	O
generalize	O
this	O
computation	O
to	O
word	O
not	O
observed	O
in	O
the	O
audio	O
training	O
data	O
.	O

To	O
give	O
more	O
insight	O
into	O
the	O
difference	O
of	O
the	O
quality	O
of	O
the	O
orthographic	O
word	B-TaskName
embeddings	I-TaskName
o	O
+	O
and	O
the	O
acoustic	O
ones	O
w	O
+	O
,	O
we	O
propose	O
an	O
empirical	O
comparison	O
by	O
showing	O
the	O
nearest	O
neighbours	O
of	O
a	O
given	O
set	O
of	O
words	O
.	O
Table	O
3	O
shows	O
examples	O
of	O
such	O
neighbour	O
.	O
It	O
can	O
be	O
seen	O
that	O
,	O
as	O
expected	O
,	O
neighbour	O
of	O
any	O
given	O
word	O
share	O
the	O
same	O
spelling	O
with	O
it	O
when	O
they	O
are	O
induced	O
by	O
the	O
orthographic	O
embeddings	O
and	O
arguably	O
sound	O
like	O
it	O
when	O
they	O
are	O
induced	O
by	O
the	O
acoustic	O
word	O
ones	O
.	O

In	O
this	O
paper	O
,	O
we	O
have	O
investigated	O
the	O
intrinsic	O
evaluation	O
of	O
acoustic	O
word	B-TaskName
embeddings	I-TaskName
.	O
These	O
latter	O
offer	O
the	O
opportunity	O
of	O
an	O
a	O
priori	O
acoustic	O
representation	O
of	O
words	O
that	O
can	O
be	O
compared	O
,	O
in	O
terms	O
of	O
similarity	O
,	O
to	O
an	O
embedded	O
representation	O
of	O
the	O
audio	O
signal	O
.	O
We	O
have	O
proposed	O
two	O
approaches	O
to	O
evaluate	O
the	O
performances	O
of	O
these	O
acoustic	O
word	B-TaskName
embeddings	I-TaskName
and	O
compare	O
them	O
to	O
their	O
orthographic	O
embeddings	O
:	O
orthographic	O
and	O
phonetic	O
performance	O
by	O
ranking	O
pairs	O
and	O
measuring	O
the	O
Spearman	O
's	O
rank	O
correlation	O
coefficient	O
(	O
Spearman	O
's	O
ρ	O
)	O
,	O
and	O
by	O
measuring	O
the	O
precision	O
in	O
a	O
homophone	O
detection	O
task	O
.	O
Experiments	O
show	O
that	O
the	O
acoustic	O
word	B-TaskName
embeddings	I-TaskName
are	O
better	O
than	O
orthographic	O
ones	O
to	O
measure	O
the	O
phonetic	O
proximity	O
between	O
two	O
words	O
.	O
More	O
,	O
they	O
are	O
better	O
too	O
on	O
homophone	O
detection	O
task	O
.	O
This	O
confirms	O
that	O
acoustic	O
word	B-TaskName
embeddings	I-TaskName
have	O
captured	O
additional	O
information	O
about	O
word	O
pronunciation	O
.	O

Hyperbolic	O
Capsule	O
Networks	O
for	O
Multi	B-TaskName
-	I-TaskName
Label	I-TaskName
Classification	I-TaskName

Although	O
deep	O
neural	O
networks	O
are	O
effective	O
at	O
extracting	O
high	O
-	O
level	O
features	O
,	O
classification	O
methods	O
usually	O
encode	O
an	O
input	O
into	O
a	O
vector	O
representation	O
via	O
simple	O
feature	O
aggregation	O
operations	O
(	O
e.g.	O
pooling	O
)	O
.	O
Such	O
operations	O
limit	O
the	O
performance	O
.	O
For	O
instance	O
,	O
a	O
multi	O
-	O
label	O
document	O
may	O
contain	O
several	O
concepts	O
.	O
In	O
this	O
case	O
,	O
one	O
vector	O
can	O
not	O
sufficiently	O
capture	O
its	O
salient	O
and	O
discriminative	O
content	O
.	O
Thus	O
,	O
we	O
propose	O
Hyperbolic	O
Capsule	O
Networks	O
(	O
HYPERCAPS	O
)	O
for	O
Multi	B-TaskName
-	I-TaskName
Label	I-TaskName
Classification	I-TaskName
(	O
MLC	O
)	O
,	O
which	O
have	O
two	O
merits	O
.	O
First	O
,	O
hyperbolic	O
capsules	O
are	O
designed	O
to	O
capture	O
fine	O
-	O
grained	O
document	O
information	O
for	O
each	O
label	O
,	O
which	O
has	O
the	O
ability	O
to	O
characterize	O
complicated	O
structures	O
among	O
labels	O
and	O
documents	O
.	O
Second	O
,	O
Hyperbolic	O
Dynamic	O
Routing	O
(	O
HDR	O
)	O
is	O
introduced	O
to	O
aggregate	O
hyperbolic	O
capsules	O
in	O
a	O
label	O
-	O
aware	O
manner	O
,	O
so	O
that	O
the	O
label	O
-	O
level	O
discriminative	O
information	O
can	O
be	O
preserved	O
along	O
the	O
depth	O
of	O
neural	O
networks	O
.	O
To	O
efficiently	O
handle	O
large	O
-	O
scale	O
MLC	O
datasets	O
,	O
we	O
additionally	O
present	O
a	O
new	O
routing	O
method	O
to	O
adaptively	O
adjust	O
the	O
capsule	O
number	O
during	O
routing	O
.	O
Extensive	O
experiments	O
are	O
conducted	O
on	O
four	O
benchmark	O
datasets	O
.	O
Compared	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
,	O
HY	O
-	O
PERCAPS	O
significantly	O
improves	O
the	O
performance	O
of	O
MLC	O
especially	O
on	O
tail	O
labels	O
.	O

The	O
main	O
difference	O
between	O
Multi	B-TaskName
-	I-TaskName
Class	I-TaskName
Classification	I-TaskName
(	O
MCC	O
)	O
and	O
Multi	B-TaskName
-	I-TaskName
Label	I-TaskName
Classification	I-TaskName
(	O
MLC	O
)	O
is	O
that	O
datasets	O
in	O
MCC	O
have	O
only	O
serval	O
mutually	O
exclusive	O
classes	O
,	O
while	O
datasets	O
in	O
MLC	O
contain	O
much	O
more	O
correlated	O
labels	O
.	O
MLC	O
allows	O
label	O
co	O
-	O
occurrence	O
in	O
one	O
document	O
,	O
which	O
indicates	O
that	O
the	O
labels	O
are	O
not	O
disjointed	O
.	O
In	O
addition	O
,	O
a	O
large	O
fraction	O
of	O
the	O
labels	O
are	O
the	O
infrequently	O
occurring	O
tail	O
labels	O
(	O
Bhatia	O
et	O
al	O
,	O
2015	O
)	O
,	O
which	O
is	O
also	O
referred	O
as	O
the	O
power	O
-	O
law	O
label	O
distribution	O
.	O
Figure	O
1	O
illustrates	O
the	O
label	O
distribution	O
of	O
EUR	O
-	O
LEX57	O
K	O
(	O
Chalkidis	O
et	O
al	O
,	O
2019	O
)	O
.	O
A	O
multi	O
-	O
label	O
document	O
usually	O
has	O
serval	O
head	O
and	O
tail	O
labels	O
,	O
and	O
hence	O
contain	O
several	O
concepts	O
about	O
both	O
its	O
head	O
and	O
tail	O
labels	O
simultaneously	O
.	O
Recent	O
works	O
for	O
text	B-TaskName
classification	I-TaskName
,	O
such	O
as	O
CNN	O
-	O
KIM	O
(	O
Kim	O
,	O
2014	O
)	O
and	O
FASTTEXT	B-MethodName
(	O
Joulin	O
et	O
al	O
,	O
2017	O
)	O
,	O
focus	O
on	O
encoding	O
a	O
document	O
into	O
a	O
fixed	O
-	O
length	O
vector	O
as	O
the	O
distributed	O
document	O
representation	O
(	O
Le	O
and	O
Mikolov	O
,	O
2014	O
)	O
.	O
These	O
encoding	O
based	O
deep	O
learning	O
methods	O
use	O
simple	O
operations	O
(	O
e.g.	O
pooling	O
)	O
to	O
aggregate	O
features	O
extracted	O
by	O
neural	O
networks	O
and	O
construct	O
the	O
document	O
vector	O
representation	O
.	O
A	O
Fully	O
-	O
Connected	O
(	O
FC	O
)	O
layer	O
is	O
usually	O
applied	O
upon	O
the	O
document	O
vector	O
to	O
predict	O
the	O
probability	O
of	O
each	O
label	O
.	O
And	O
each	O
row	O
in	O
its	O
weight	O
matrix	O
can	O
be	O
interpreted	O
as	O
a	O
label	O
vector	O
representation	O
(	O
Du	O
et	O
al	O
,	O
2019b	O
)	O
.	O
In	O
this	O
way	O
,	O
the	O
label	O
probability	O
can	O
be	O
predicted	O
by	O
computing	O
the	O
dot	O
product	O
between	O
label	O
and	O
document	O
vectors	O
,	O
which	O
is	O
proportional	O
to	O
the	O
scalar	O
projection	O
of	O
the	O
label	O
vector	O
onto	O
the	O
document	O
vector	O
as	O
shown	O
in	O
Figure	O
2	O
.	O
For	O
example	O
,	O
label	O
"	O
movie	O
"	O
should	O
have	O
the	O
largest	O
scalar	O
projection	O
onto	O
a	O
document	O
about	O
"	O
movie	O
"	O
.	O
However	O
,	O
even	O
the	O
learned	O
label	O
representation	O
of	O
"	O
music	O
"	O
can	O
be	O
distinguished	O
from	O
"	O
movie	O
"	O
,	O
it	O
may	O
also	O
have	O
a	O
large	O
scalar	O
projection	O
onto	O
the	O
document	O
.	O
Moreover	O
,	O
multi	O
-	O
label	O
documents	O
always	O
contain	O
several	O
concepts	O
about	O
multiple	O
labels	O
,	O
such	O
as	O
a	O
document	O
about	O
"	O
sport	O
movie	O
"	O
.	O
Whereas	O
the	O
document	O
vector	O
representation	O
is	O
identical	O
to	O
all	O
the	O
labels	O
,	O
and	O
training	O
instances	O
for	O
tail	O
labels	O
are	O
inadequate	O
compared	O
to	O
head	O
labels	O
.	O
The	O
imbalance	O
between	O
head	O
and	O
tail	O
labels	O
makes	O
it	O
hard	O
for	O
the	O
FC	O
layer	O
to	O
make	O
prediction	O
,	O
especially	O
on	O
tail	O
labels	O
.	O
In	O
this	O
case	O
,	O
one	O
vector	O
can	O
not	O
sufficiently	O
capture	O
its	O
salient	O
and	O
discriminative	O
content	O
.	O
Therefore	O
,	O
the	O
performance	O
of	O
constructing	O
the	O
document	O
vector	O
representation	O
via	O
simple	O
aggregation	O
operations	O
is	O
limited	O
for	O
MLC	O
.	O
Capsule	O
networks	O
(	O
Sabour	O
et	O
al	O
,	O
2017	O
;	O
Yang	O
et	O
al	O
,	O
2018a	O
)	O
has	O
recently	O
proposed	O
to	O
use	O
dynamic	O
routing	O
in	O
place	O
of	O
pooling	O
and	O
achieved	O
better	O
performance	O
for	O
classification	O
tasks	O
.	O
In	O
fact	O
,	O
capsules	O
are	O
fine	O
-	O
grained	O
features	O
compared	O
to	O
the	O
distributed	O
document	O
representation	O
,	O
and	O
dynamic	O
routing	O
is	O
a	O
label	O
-	O
aware	O
feature	O
aggregation	O
procedure	O
.	O
(	O
Zhao	O
et	O
al	O
,	O
2019	O
)	O
improves	O
the	O
scalability	O
of	O
capsule	O
networks	O
for	O
MLC	O
.	O
However	O
,	O
they	O
only	O
use	O
CNN	O
to	O
construct	O
capsules	O
,	O
which	O
capture	O
local	O
contextual	O
information	O
(	O
Wang	O
et	O
al	O
,	O
2016	O
)	O
.	O
Effectively	O
learning	O
the	O
document	O
information	O
about	O
multiple	O
labels	O
is	O
crucial	O
for	O
MLC	O
.	O
Thus	O
we	O
propose	O
to	O
connect	O
CNN	O
and	O
RNN	O
in	O
parallel	O
to	O
capture	O
both	O
local	O
and	O
global	O
contextual	O
information	O
,	O
which	O
would	O
be	O
complementary	O
to	O
each	O
other	O
.	O
Nevertheless	O
,	O
Euclidean	O
capsules	O
necessitate	O
designing	O
a	O
non	O
-	O
linear	O
squashing	O
function	O
.	O
Inspired	B-DatasetName
by	O
the	O
hyperbolic	O
representation	B-TaskName
learning	I-TaskName
methods	O
which	O
demonstrate	O
that	O
the	O
hyper	O
-	O
bolic	O
space	O
has	O
more	O
representation	O
capacity	O
than	O
the	O
Euclidean	O
space	O
(	O
Nickel	O
and	O
Kiela	O
,	O
2017	O
;	O
Ganea	O
et	O
al	O
,	O
2018a	O
)	O
,	O
Hyperbolic	O
Capsule	O
Networks	O
(	O
HYPERCAPS	O
)	O
is	O
proposed	O
.	O
Capsules	O
are	O
constrained	O
in	O
the	O
hyperbolic	O
space	O
which	O
does	O
not	O
require	O
the	O
squashing	O
function	O
.	O
Hyperbolic	O
Dynamic	O
Routing	O
(	O
HDR	O
)	O
is	O
introduced	O
to	O
aggregate	O
hyperbolic	O
capsules	O
in	O
a	O
label	O
-	O
aware	O
manner	O
.	O
Moreover	O
,	O
in	O
order	O
to	O
fit	O
the	O
large	O
label	O
set	O
of	O
MLC	O
and	O
improve	O
the	O
scalability	O
of	O
HYPERCAPS	O
,	O
adaptive	O
routing	O
is	O
presented	O
to	O
adjust	O
the	O
number	O
of	O
capsules	O
participated	O
in	O
the	O
routing	O
procedure	O
.	O
The	O
main	O
contributions	O
of	O
our	O
work	O
are	O
therefore	O
summarized	O
as	O
follows	O
:	O
We	O
propose	O
to	O
connect	O
CNN	O
and	O
RNN	O
in	O
parallel	O
to	O
simultaneously	O
extract	O
local	O
and	O
global	O
contextual	O
information	O
,	O
which	O
would	O
be	O
complementary	O
to	O
each	O
other	O
.	O
HYPERCAPS	O
with	O
HDR	O
are	O
formulated	O
to	O
aggregate	O
features	O
in	O
a	O
label	O
-	O
aware	O
manner	O
,	O
and	O
hyperbolic	O
capsules	O
benefits	O
from	O
the	O
representation	O
capacity	O
of	O
the	O
hyperbolic	O
space	O
.	O
Adaptive	O
routing	O
is	O
furthermore	O
presented	O
to	O
improve	O
the	O
scalability	O
of	O
HYPERCAPS	O
and	O
fit	O
the	O
large	O
label	O
set	O
of	O
MLC	O
.	O
Extensive	O
experiments	O
on	O
four	O
benchmark	O
MLC	O
datasets	O
demonstrate	O
the	O
effectiveness	O
of	O
HYPER	O
-	O
CAPS	O
,	O
especially	O
on	O
tail	O
labels	O
.	O

In	O
order	O
to	O
make	O
neural	O
networks	O
work	O
in	O
the	O
hyperbolic	O
space	O
,	O
formalism	O
of	O
the	O
Möbius	O
gyrovector	O
space	O
is	O
adopted	O
(	O
Ganea	O
et	O
al	O
,	O
2018b	O
)	O
.	O
An	O
n	O
-	O
dimensional	O
Poincaré	O
ball	O
B	O
n	O
is	O
a	O
Riemannian	O
manifold	O
defined	O
as	O
B	O
n	O
=	O
{	O
x	O
R	O
n	O
|	O
x	O
<	O
1	O
}	O
,	O
with	O
its	O
tangent	O
space	O
around	O
p	O
B	O
n	O
denoted	O
as	O
T	O
p	O
B	O
n	O
and	O
the	O
conformal	O
factor	O
as	O
λ	O
p	O
:	O
=	O
2	O
1−	O
p	O
2	O
.	O
The	O
exponential	O
map	O
exp	O
p	O
:	O
T	O
p	O
B	O
n	O
B	O
n	O
for	O
w	O
T	O
p	O
B	O
n	O
\	O
{	O
0	B-DatasetName
}	O
is	O
consequently	O
defined	O
as	O
exp	O
p	O
(	O
w	O
)	O
=	O
p	O
(	O
tanh	O
(	O
λ	O
p	O
2	O
w	O
)	O
w	O
w	O
)	O
.	O
(	O
1	O
)	O
To	O
work	O
with	O
hyperbolic	O
capsules	O
,	O
Möbius	O
operations	O
in	O
the	O
Poincaré	O
ball	O
also	O
need	O
to	O
be	O
formulated	O
.	O
Möbius	O
addition	O
for	O
u	O
,	O
v	O
B	O
n	O
is	O
defined	O
as	O
u	O
v	O
=	O
(	O
1	O
+	O
2	O
u	O
,	O
v	O
+	O
v	O
2	O
)	O
u+	O
(	O
1−	O
u	O
2	O
)	O
v	O
1	O
+	O
2	O
u	O
,	O
v	O
+	O
u	O
2	O
v	O
2	O
,	O
(	O
2	O
)	O
where	O
,	O
denotes	O
the	O
Euclidean	O
inner	O
product	O
.	O
Thus	O
Möbius	O
summation	O
can	O
be	O
formulated	O
as	O
n	O
M	O
i	O
=	O
m	O
p	O
i	O
=	O
p	O
m	O
p	O
n	O
,	O
p	O
i	O
B	O
n	O
.	O
(	O
3	O
)	O
Möbius	O
scalar	O
multiplication	O
for	O
k	O
R	O
and	O
p	O
B	O
n	O
\	O
{	O
0	B-DatasetName
}	O
is	O
defined	O
as	O
k	O
⊗	O
p	O
=	O
tanh	O
(	O
k	O
tanh	O
−1	O
(	O
p	O
)	O
)	O
p	O
p	O
.	O
(	O
4	O
)	O
And	O
k	O
⊗	O
p	O
=	O
0	B-DatasetName
when	O
p	O
=	O
0	B-DatasetName
B	O
n	O
.	O
The	O
definition	O
of	O
Möbius	O
matrix	O
-	O
vector	O
multiplication	O
for	O
M	O
R	O
m×n	O
and	O
p	O
B	O
n	O
when	O
M	O
p	O
=	O
0	B-DatasetName
is	O
as	O
follows	O
M	O
⊗	O
p	O
=	O
tanh	O
(	O
M	O
p	O
p	O
tanh	O
−1	O
(	O
p	O
)	O
)	O
M	O
p	O
M	O
p	O
.	O
(	O
5	O
)	O
And	O
M	O
⊗	O
p	O
=	O
0	B-DatasetName
when	O
M	O
p	O
=	O
0	B-DatasetName
.	O
HDR	O
is	O
developed	O
based	O
on	O
these	O
operations	O
.	O

Neural	O
networks	O
are	O
generally	O
used	O
as	O
effective	O
feature	O
extractors	O
for	O
text	B-TaskName
classification	I-TaskName
.	O
Kernels	O
of	O
CNN	O
can	O
be	O
used	O
to	O
capture	O
local	O
n	O
-	O
gram	O
contextual	O
information	O
at	O
different	O
positions	O
of	O
a	O
text	O
sequence	O
,	O
while	O
hidden	O
states	O
of	O
RNN	O
can	O
represent	O
global	O
long	O
-	O
term	O
dependencies	O
of	O
the	O
text	O
(	O
Wang	O
et	O
al	O
,	O
2016	O
)	O
.	O
Hence	O
,	O
we	O
propose	O
to	O
obtain	O
the	O
combination	O
of	O
local	O
and	O
global	O
hyperbolic	O
capsules	O
by	O
connecting	O
CNN	O
and	O
RNN	O
in	O
parallel	O
,	O
which	O
would	O
be	O
complementary	O
to	O
each	O
other	O
.	O
Given	O
a	O
text	O
sequence	O
of	O
a	O
document	O
with	O
T	O
word	O
tokens	O
x	O
=	O
[	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
T	O
]	O
,	O
pre	O
-	O
trained	O
wdimensional	O
word	B-TaskName
embeddings	I-TaskName
(	O
e.g.	O
GLOVE	O
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
)	O
are	O
used	O
to	O
compose	O
word	O
vector	O
representations	O
E	O
=	O
[	O
e	O
1	O
,	O
.	O
.	O
.	O
,	O
e	O
T	O
]	O
R	O
T	O
×w	O
,	O
upon	O
which	O
CNN	O
and	O
RNN	O
connected	O
in	O
parallel	O
are	O
used	O
to	O
construct	O
local	O
and	O
global	O
hyperbolic	O
capsules	O
in	O
the	O
Poincaré	O
ball	O
.	O
Figure	O
3	O
illustrates	O
the	O
framework	O
for	O
HYPERCAPS	O
.	O

N	O
-	O
gram	O
kernels	O
K	O
R	O
k×w	O
with	O
different	O
window	O
size	O
k	O
are	O
applied	O
on	O
the	O
local	O
region	O
of	O
the	O
word	O
representations	O
E	O
t	O
:	O
t+k−1	O
R	O
k×w	O
to	O
construct	O
the	O
local	O
features	O
as	O
l	O
t	O
=	O
ϕ	O
(	O
K	O
E	O
t	O
:	O
t+k−1	O
)	O
,	O
(	O
6	O
)	O
where	O
denotes	O
the	O
element	O
-	O
wise	O
multiplication	O
and	O
ϕ	O
is	O
a	O
non	O
-	O
linearity	O
(	O
e.g.	O
ReLU	B-MethodName
)	O
.	O
For	O
simplicity	O
,	O
the	O
bias	O
term	O
is	O
omitted	O
.	O
With	O
totally	O
d	O
channels	O
,	O
the	O
local	O
hyperbolic	O
capsules	O
at	O
position	O
t	O
can	O
be	O
constructed	O
as	O
l	O
t	O
=	O
exp	O
0	B-DatasetName
(	O
[	O
l	O
(	O
1	O
)	O
t	O
,	O
.	O
.	O
.	O
,	O
l	O
(	O
d	O
)	O
t	O
]	O
)	O
B	O
d	O
.	O
(	O
7	O
)	O
Therefore	O
,	O
a	O
k	O
-	O
gram	O
kernel	O
with	O
1	O
stride	O
can	O
construct	O
T	O
−k+1	O
local	O
hyperbolic	O
capsules	O
.	O
The	O
local	O
hyperbolic	O
capsule	O
set	O
is	O
denoted	O
as	O
{	O
u	O
1	O
,	O
.	O
.	O
.	O
,	O
u	O
L	O
}	O
.	O

Bidirectional	B-MethodName
GRU	I-MethodName
(	O
Chung	O
et	O
al	O
,	O
2014	O
)	O
is	O
adopted	O
to	O
incorporate	O
forward	O
and	O
backward	O
global	O
contextual	O
information	O
and	O
construct	O
the	O
global	O
hyperbolic	O
capsules	O
.	O
Forward	O
and	O
backward	O
hidden	O
states	O
at	O
time	O
-	O
step	O
t	O
are	O
obtained	O
by	O
−	O
h	O
t	O
=	O
GRU	B-MethodName
(	O
−−	O
h	O
t−1	O
,	O
e	O
t	O
)	O
,	O
−	O
h	O
t	O
=	O
GRU	B-MethodName
(	O
−−	O
h	O
t+1	O
,	O
e	O
t	O
)	O
.	O
(	O
8	O
)	O
Each	O
of	O
the	O
total	O
2	O
T	O
hidden	O
states	O
can	O
be	O
taken	O
as	O
a	O
global	O
hyperbolic	O
capsule	O
using	O
the	O
exponential	O
map	O
,	O
i.e.	O
−	O
g	O
t	O
=	O
exp	O
0	B-DatasetName
(	O
−	O
h	O
t	O
)	O
,	O
and	O
equally	O
for	O
the	O
backward	O
capsules	O
.	O
The	O
global	O
hyperbolic	O
capsule	O
set	O
is	O
denoted	O
as	O
{	O
u	O
1	O
,	O
.	O
.	O
.	O
,	O
u	O
G	O
}	O
.	O

Datasets	O
Experiments	O
are	O
carried	O
out	O
on	O
four	O
publicly	O
available	O
MLC	O
datasets	O
,	O
including	O
the	O
small	O
-	O
scale	O
AAPD	O
(	O
Yang	O
et	O
al	O
,	O
2018b	O
)	O
and	O
RCV1	B-DatasetName
(	O
Lewis	O
et	O
al	O
,	O
2004	O
)	O
,	O
the	O
large	O
-	O
scale	O
ZHIHU	O
1	O
and	O
EUR	O
-	O
LEX57	O
K	O
(	O
Chalkidis	O
et	O
al	O
,	O
2019	O
)	O
.	O
Labels	O
are	O
divided	O
into	O
head	O
and	O
tail	O
sets	O
according	O
to	O
their	O
number	O
of	O
training	O
instances	O
,	O
i.e.	O
labels	O
have	O
less	O
than	O
average	O
number	O
of	O
training	O
instances	O
are	O
divided	O
into	O
the	O
tail	O
label	O
set	O
.	O
Their	O
statistics	O
can	O
be	O
found	O
in	O
Table	O
1	O
.	O

We	O
use	O
the	O
rank	O
-	O
based	O
evaluation	O
metrics	O
which	O
have	O
been	O
widely	O
adopted	O
for	O
MLC	O
tasks	O
(	O
Bhatia	O
et	O
al	O
,	O
2015	O
;	O
Liu	O
et	O
al	O
,	O
2017	O
)	O
,	O
i.e.	O
Precision@k	O
(	O
P@k	O
for	O
short	O
)	O
and	O
nDCG@k	O
,	O
which	O
are	O
respectively	O
defined	O
as	O
P@k	O
=	O
1	O
k	O
j	O
rank	O
k	O
(	O
a	O
)	O
y	O
j	O
,	O
(	O
19	O
)	O
nDCG@k	O
=	O
j	O
rank	O
k	O
(	O
a	O
)	O
y	O
j	O
/log	O
(	O
j	O
+	O
1	O
)	O
min	O
(	O
k	O
,	O
y	O
0	B-DatasetName
)	O
j=1	O
1	O
/	O
log	O
(	O
j	O
+	O
1	O
)	O
,	O
(	O
20	O
)	O
where	O
y	O
j	O
{	O
0	B-DatasetName
,	O
1	O
}	O
denotes	O
the	O
the	O
ground	O
truth	O
about	O
label	O
j	O
,	O
rank	O
k	O
(	O
a	O
)	O
denotes	O
the	O
indices	O
of	O
the	O
candidate	O
label	O
-	O
aware	O
hyperbolic	O
capsules	O
with	O
k	O
largest	O
activations	O
in	O
descending	O
order	O
,	O
and	O
y	O
0	B-DatasetName
is	O
the	O
true	O
label	O
number	O
for	O
the	O
document	O
instance	O
.	O
The	O
final	O
results	O
are	O
averaged	O
over	O
all	O
the	O
test	O
instances	O
.	O
Baselines	O
To	O
demonstrate	O
the	O
effectiveness	O
of	O
HYPERCAPS	O
on	O
the	B-DatasetName
benchmark	I-DatasetName
datasets	O
,	O
six	O
comparative	O
text	B-TaskName
classification	I-TaskName
methods	O
are	O
chosen	O
as	O
the	O
baselines	O
.	O
FASTTEXT	B-MethodName
(	O
Joulin	O
et	O
al	O
,	O
2017	O
)	O
is	O
a	O
representative	O
encoding	O
-	O
based	O
method	O
which	O
use	O
average	B-MethodName
pooling	I-MethodName
to	O
construct	O
document	O
representations	O
and	O
MLP	B-DatasetName
to	O
make	O
the	O
predictions	O
.	O
SLEEC	O
(	O
Bhatia	O
et	O
al	O
,	O
2015	O
)	O
is	O
a	O
typical	O
label	O
-	O
embedding	O
method	O
for	O
MLC	O
,	O
which	O
uses	O
k	B-MethodName
-	I-MethodName
nearest	I-MethodName
neighbors	I-MethodName
search	O
to	O
predict	O
the	O
labels	O
.	O
XML	O
-	O
CNN	O
(	O
Liu	O
et	O
al	O
,	O
2017	O
)	O
employs	O
CNN	O
as	O
local	O
n	O
-	O
gram	O
feature	O
extractors	O
and	O
a	O
dynamic	O
pooling	O
technique	O
as	O
aggregation	O
method	O
.	O
SGM	O
(	O
Yang	O
et	O
al	O
,	O
2018b	O
)	O
applies	O
the	O
seq2seq	B-MethodName
model	O
with	O
attention	O
mechanism	O
,	O
which	O
takes	O
the	O
global	O
contextual	O
information	O
.	O
REGGNN	O
(	O
Xu	O
et	O
al	O
,	O
2019	O
)	O
uses	O
a	O
combination	O
of	O
CNN	O
and	O
LSTM	B-MethodName
with	O
a	O
dynamic	O
gate	O
that	O
controls	O
the	O
information	O
from	O
these	O
two	O
parts	O
.	O
NLP	O
-	O
CAP	B-DatasetName
(	O
Zhao	O
et	O
al	O
,	O
2019	O
)	O
is	O
a	O
capsule	O
-	O
based	O
approach	O
for	O
MLC	O
,	O
which	O
reformulates	O
the	O
routing	O
algorithm	O
.	O
NLP	O
-	O
CAP	B-DatasetName
use	O
only	O
CNN	O
to	O
construct	O
capsules	O
,	O
and	O
it	O
applies	O
the	O
squashing	O
function	O
onto	O
capsules	O
.	O
Implementation	O
Details	O
All	O
the	O
words	O
are	O
converted	O
to	O
lower	O
case	O
and	O
padding	O
is	O
used	O
to	O
handle	O
the	O
various	O
lengths	O
of	O
the	O
text	O
sequences	O
.	O
Maximum	O
length	O
of	O
AAPD	O
,	O
RCV1	B-DatasetName
and	O
EUR	O
-	O
LEX57	O
K	O
is	O
set	O
to	O
500	O
,	O
while	O
maximum	O
length	O
of	O
ZHIHU	O
is	O
50	O
.	O
To	O
compose	O
the	O
word	O
vector	O
representations	O
,	O
pre	O
-	O
trained	O
300	O
-	O
dimensional	O
GLOVE	O
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
word	B-TaskName
embeddings	I-TaskName
are	O
used	O
for	O
AAPD	O
,	O
RCV1	B-DatasetName
and	O
EUR	O
-	O
LEX57	O
K	O
,	O
while	O
ZHIHU	O
uses	O
its	O
specified	O
256	O
-	O
dimensional	O
word	B-TaskName
embeddings	I-TaskName
.	O
The	O
dimension	O
of	O
the	O
Poincaré	O
ball	O
is	O
set	O
to	O
32	O
with	O
a	O
radius	O
1	O
−	O
(	O
=	O
10	O
−5	O
)	O
to	O
avoid	O
numerical	O
errors	O
.	O
Multiple	O
one	O
-	O
dimensional	O
convolutional	O
kernels	O
(	O
with	O
window	O
sizes	O
of	O
2	O
,	O
4	O
,	O
8	O
)	O
are	O
applied	O
in	O
the	O
local	O
hyperbolic	O
capsule	O
layer	O
.	O
The	O
number	O
of	O
compressed	O
local	O
and	O
global	O
hyperbolic	O
capsules	O
is	O
128	O
.	O
Adaptive	O
routing	O
layer	O
is	O
not	O
applied	O
on	O
the	O
small	O
-	O
scale	O
datasets	O
AAPD	O
and	O
RCV1	B-DatasetName
.	O
The	O
maximum	O
candidate	O
label	O
number	O
is	O
set	O
to	O
200	O
for	O
the	O
large	O
-	O
scale	O
datasets	O
ZHIHU	O
and	O
EUR	O
-	O
LEX57K.	O
For	O
the	O
baselines	O
,	O
hyperparameters	O
recommended	O
by	O
their	O
authors	O
are	O
adopted	O
.	O

Recent	O
research	O
on	O
representation	B-TaskName
learning	I-TaskName
(	O
Nickel	O
and	O
Kiela	O
,	O
2017	O
)	O
indicates	O
that	O
hyperbolic	O
space	O
is	O
superior	O
to	O
Euclidean	O
space	O
in	O
terms	O
of	O
representation	O
capacity	O
,	O
especially	O
in	O
low	O
dimension	O
.	O
(	O
Ganea	O
et	O
al	O
,	O
2018b	O
)	O
generalizes	O
operations	O
for	O
neural	O
networks	O
in	O
the	O
Poincaré	O
ball	O
using	O
formalism	O
of	O
Möbius	O
gyrovector	O
space	O
.	O
Some	O
works	O
lately	O
demonstrate	O
the	O
superiority	O
of	O
the	O
hyperbolic	O
space	O
for	O
serval	O
natural	O
language	O
processing	O
tasks	O
,	O
such	O
as	O
textual	O
entailment	O
(	O
Ganea	O
et	O
al	O
,	O
2018a	O
)	O
,	O
machine	B-TaskName
translation	I-TaskName
(	O
Gulcehre	O
et	O
al	O
,	O
2019	O
)	O
and	O
word	O
embedding	O
(	O
Tifrea	O
et	O
al	O
,	O
2019	O
)	O
.	O
Our	O
work	O
presents	O
the	O
Hyperbolic	O
Capsule	O
Networks	O
(	O
HYPERCAPS	O
)	O
for	O
MLC	O
.	O

We	O
present	O
the	O
Hyperbolic	O
Capsule	O
Networks	O
(	O
HYPERCAPS	O
)	O
with	O
Hyperbolic	O
Dynamic	O
Routing	O
(	O
HDR	O
)	O
and	O
adaptive	O
routing	O
for	O
Multi	B-TaskName
-	I-TaskName
Label	I-TaskName
Classification	I-TaskName
(	O
MLC	O
)	O
.	O
The	O
proposed	O
HYPERCAPS	O
takes	O
advantage	O
of	O
the	O
parallel	O
combination	O
of	O
finegrained	O
local	O
and	O
global	O
contextual	O
information	O
and	O
label	O
-	O
aware	O
feature	O
aggregation	O
method	O
HDR	O
to	O
dynamically	O
construct	O
label	O
-	O
aware	O
hyperbolic	O
capsules	O
for	O
tail	O
and	O
head	O
labels	O
.	O
Adaptive	O
routing	O
is	O
additionally	O
applied	O
to	O
improve	O
the	O
scalability	O
of	O
HYPERCAPS	O
by	O
controlling	O
the	O
number	O
of	O
capsules	O
during	O
the	O
routing	O
procedure	O
.	O
Extensive	O
experiments	O
are	O
carried	O
out	O
on	O
four	O
benchmark	O
datasets	O
.	O
Results	O
compared	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
demonstrate	O
the	O
superiority	O
of	O
HYPERCAPS	O
,	O
especially	O
on	O
tail	O
labels	O
.	O
As	O
recent	O
works	O
explore	O
the	O
superiority	O
of	O
hyperbolic	O
space	O
to	O
Euclidean	O
space	O
for	O
serval	O
natural	O
language	O
processing	O
tasks	O
,	O
we	O
intend	O
to	O
couple	O
with	O
the	O
hyperbolic	O
neural	O
networks	O
(	O
Ganea	O
et	O
al	O
,	O
2018b	O
)	O
and	O
the	O
hyperbolic	O
word	O
embedding	O
method	O
such	O
as	O
POINCARÉGLOVE	O
(	O
Tifrea	O
et	O
al	O
,	O
2019	O
)	O
in	O
the	O
future	O
.	O

In	O
its	O
fifth	O
iteration	O
,	O
#	O
SMM4H	B-DatasetName
2020	O
continues	O
to	O
serve	O
as	O
a	O
venue	O
for	O
bringing	O
together	O
researchers	O
interested	O
in	O
addressing	O
the	O
significant	O
opportunities	O
and	O
challenges	O
of	O
utilizing	O
the	O
vast	O
amount	O
of	O
data	O
on	O
social	O
media	O
for	O
health	O
informatics	O
.	O
For	O
#	O
SMM4H	B-DatasetName
2020	O
,	O
we	O
accepted	O
5	O
workshop	O
papers	O
(	O
acceptance	O
rate	O
of	O
56	O
%	O
)	O
and	O
26	O
shared	O
task	O
system	O
description	O
papers	O
.	O
Each	O
submission	O
was	O
peer	O
-	O
reviewed	O
by	O
two	O
reviewers	O
.	O
The	O
accepted	O
workshop	O
papers	O
span	O
a	O
range	O
of	O
social	O
media	O
data	O
-	O
Twitter	O
,	O
Facebook	O
,	O
Reddit	B-DatasetName
,	O
and	O
online	O
health	O
forums	O
-	O
and	O
health	O
domains	O
,	O
including	O
diabetes	O
,	O
depression	O
,	O
COVID	O
-	O
19	O
,	O
medical	O
misinformation	O
,	O
and	O
adverse	O
drug	O
reactions	O
.	O
Cornelius	O
et	O
al	O
present	O
an	O
online	O
platform	O
that	O
aggregates	O
and	O
visualizes	O
methods	O
for	O
extracting	O
information	O
related	O
to	O
COVID	O
-	O
19	O
on	O
Twitter	O
.	O
Dirkson	O
et	O
al	O
explore	O
modeling	O
conversational	O
features	O
of	O
posts	O
,	O
in	O
addition	O
to	O
the	O
posts	O
themselves	O
,	O
for	O
detecting	O
adverse	O
drug	O
reactions	O
on	O
Facebook	O
,	O
and	O
medical	O
misinformation	O
.	O
Romberg	O
et	O
al	O
present	O
an	O
annotated	O
,	O
Germanlanguage	O
corpus	O
for	O
extracting	O
information	O
needs	O
expressed	O
online	O
by	O
patients	O
with	O
diabetes	O
.	O
Moßburger	O
et	O
al	O
use	O
various	O
text	O
mining	O
techniques	O
to	O
compare	O
features	O
of	O
depression	O
forums	O
on	O
Reddit	B-DatasetName
and	O
a	O
curated	O
,	O
moderated	O
site	O
.	O
Finally	O
,	O
Owen	O
et	O
al	O
present	O
an	O
annotated	O
,	O
English	O
-	O
language	O
corpus	O
for	O
detecting	O
depression	O
and	O
anxiety	O
on	O
Twitter	O
.	O
The	O
#	O
SMM4H	B-DatasetName
2020	O
shared	O
tasks	O
sought	O
to	O
advance	O
the	O
use	O
of	O
Twitter	O
data	O
(	O
tweets	O
)	O
for	O
pharmacovigilance	O
,	O
toxicovigilance	O
,	O
and	O
epidemiology	O
of	O
birth	O
defects	O
.	O
In	O
addition	O
to	O
re	O
-	O
reruns	O
of	O
three	O
tasks	O
,	O
#	O
SMM4H	B-DatasetName
2020	O
included	O
new	O
tasks	O
for	O
detecting	O
adverse	O
drug	O
reactions	O
in	O
French	O
and	O
Russian	O
tweets	O
,	O
characterizing	O
chatter	O
related	O
to	O
prescription	O
medication	O
abuse	O
,	O
and	O
detecting	O
self	O
reports	O
of	O
birth	O
defect	O
pregnancy	O
outcomes	O
.	O
The	O
five	O
tasks	O
required	O
methods	O
for	O
binary	O
classification	O
,	O
multiclass	O
classification	O
,	O
and	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
.	O
With	O
29	O
teams	O
and	O
a	O
total	O
of	O
130	O
system	O
submissions	O
,	O
participation	O
in	O
the	O
#	O
SMM4H	B-DatasetName
shared	O
tasks	O
continues	O
to	O
grow	O
.	O
Among	O
the	O
26	O
shared	O
task	O
system	O
description	O
papers	O
that	O
were	O
accepted	O
,	O
6	O
teams	O
were	O
invited	O
to	O
present	O
their	O
system	O
orally	O
.	O
The	O
organizing	O
committee	O
of	O
#	O
SMM4H	B-DatasetName
2020	O
would	O
like	O
to	O
thank	O
the	O
program	O
committee	O
,	O
the	O
additional	O
reviewers	O
of	O
system	O
description	O
papers	O
,	O
the	O
organizers	O
of	O
COLING	O
2020	O
(	O
especially	O
the	O
workshop	O
cochairs	O
)	O
,	O
the	O
annotators	O
of	O
the	O
shared	O
task	O
data	O
,	O
and	O
,	O
of	O
course	O
,	O
everyone	O
who	O
submitted	O
a	O
paper	O
or	O
participated	O
in	O
the	O
shared	O
tasks	O
.	O
#	O
SMM4H	B-DatasetName
2020	O
would	O
not	O
have	O
been	O
possible	O
without	O
all	O
of	O
them	O
.	O

Reinforcement	O
learning	O
(	O
RL	O
)	O
is	O
an	O
attractive	O
solution	O
for	O
task	O
-	O
oriented	O
dialog	O
systems	O
.	O
However	O
,	O
extending	O
RL	O
-	O
based	O
systems	O
to	O
handle	O
new	O
intents	O
and	O
slots	O
requires	O
a	O
system	O
redesign	O
.	O
The	O
high	O
maintenance	O
cost	O
makes	O
it	O
difficult	O
to	O
apply	O
RL	O
methods	O
to	O
practical	O
systems	O
on	O
a	O
large	O
scale	O
.	O
To	O
address	O
this	O
issue	O
,	O
we	O
propose	O
a	O
practical	O
teacherstudent	O
framework	O
to	O
extend	O
RL	O
-	O
based	O
dialog	O
systems	O
without	O
retraining	O
from	O
scratch	O
.	O
Specifically	O
,	O
the	O
"	O
student	O
"	O
is	O
an	O
extended	O
dialog	O
manager	O
based	O
on	O
a	O
new	O
ontology	B-MethodName
,	O
and	O
the	O
"	O
teacher	O
"	O
is	O
existing	O
resources	O
used	O
for	O
guiding	O
the	O
learning	O
process	O
of	O
the	O
"	O
student	O
"	O
.	O
By	O
specifying	O
constraints	O
held	O
in	O
the	O
new	O
dialog	O
manager	O
,	O
we	O
transfer	O
knowledge	O
of	O
the	O
"	O
teacher	O
"	O
to	O
the	O
"	O
student	O
"	O
without	O
additional	O
resources	O
.	O
Experiments	O
show	O
that	O
the	O
performance	O
of	O
the	O
extended	O
system	O
is	O
comparable	O
to	O
the	O
system	O
trained	O
from	O
scratch	O
.	O
More	O
importantly	O
,	O
the	O
proposed	O
framework	O
makes	O
no	O
assumption	O
about	O
the	O
unsupported	O
intents	O
and	O
slots	O
,	O
which	O
makes	O
it	O
possible	O
to	O
improve	O
RL	O
-	O
based	O
systems	O
incrementally	O
.	O

With	O
the	O
flourish	O
development	O
of	O
virtual	O
personal	O
assistants	O
(	O
e.g.	O
,	O
Amazon	O
Alexa	O
and	O
Google	B-DatasetName
Assistant	O
)	O
,	O
task	O
-	O
oriented	O
dialog	O
systems	O
,	O
which	O
can	O
help	O
users	O
accomplish	O
tasks	O
naturally	O
,	O
have	O
been	O
a	O
focal	O
point	O
in	O
both	O
academic	O
and	O
industry	O
research	O
.	O
In	O
the	O
early	O
work	O
,	O
the	O
task	O
-	O
oriented	O
dialog	O
system	O
is	O
merely	O
a	O
set	O
of	O
hand	O
-	O
crafted	O
mapping	O
rules	O
defined	O
by	O
experts	O
.	O
This	O
is	O
referred	O
to	O
as	O
a	O
rule	O
-	O
based	O
system	O
.	O
Although	O
rule	O
-	O
based	O
systems	O
often	O
have	O
acceptable	O
performance	O
,	O
they	O
are	O
inconvenient	O
and	O
difficult	O
to	O
be	O
optimized	O
.	O
Recently	O
,	O
reinforcement	O
learning	O
approaches	O
have	O
been	O
applied	O
to	O
optimize	O
dialog	O
systems	O
through	O
interaction	O
with	O
a	O
user	O
simulator	O
or	O
employed	O
real	O
users	O
online	O
(	O
Gašić	O
et	O
al	O
,	O
2011	O
;	O
Su	O
et	O
al	O
,	O
2016a	O
;	O
Li	O
et	O
al	O
,	O
2016	O
,	O
Figure	O
1	O
:	O
An	O
example	O
of	O
a	O
task	O
-	O
oriented	O
dialog	O
after	O
the	O
system	O
comes	O
online	O
.	O
The	O
user	O
is	O
confused	O
because	O
the	O
"	O
confirm	O
"	O
intent	O
has	O
not	O
been	O
considered	O
in	O
the	O
deployed	O
system	O
.	O
Dialog	O
rules	O
should	O
be	O
embedded	O
in	O
a	O
new	O
system	O
to	O
handle	O
such	O
situations	O
.	O
2017b	O
)	O
.	O
It	O
has	O
been	O
proven	O
that	O
RL	O
-	O
based	O
dialog	O
systems	O
can	O
abandon	O
hand	O
-	O
crafted	O
dialog	O
manager	O
and	O
achieve	O
more	O
robust	O
performance	O
than	O
rulebased	O
systems	O
(	O
Young	O
et	O
al	O
,	O
2013	O
)	O
.	O
Typically	O
,	O
the	O
first	O
step	O
of	O
building	O
RL	O
-	O
based	O
dialog	O
systems	O
is	O
defining	O
a	O
user	O
model	O
1	O
and	O
necessary	O
system	O
actions	O
to	O
complete	O
a	O
specific	O
task	O
(	O
e.g.	O
,	O
seek	O
restaurants	O
information	O
or	O
book	O
hotels	O
)	O
.	O
Based	O
on	O
such	O
ontology	B-MethodName
,	O
developers	O
can	O
extract	O
dialog	O
features	O
and	O
train	O
the	O
dialog	O
manager	O
model	O
in	O
an	O
interaction	O
environment	O
.	O
Such	O
systems	O
work	O
well	O
if	O
real	O
users	O
are	O
consistent	O
with	O
the	O
predefined	O
user	O
model	O
.	O
However	O
,	O
as	O
shown	O
in	O
Fig	O
.	O
1	O
,	O
the	O
unanticipated	O
actions	O
2	O
of	O
real	O
users	O
will	O
lead	O
to	O
a	O
poor	O
user	O
experience	O
.	O
In	O
this	O
situation	O
,	O
the	O
original	O
system	O
should	O
be	O
extended	O
to	O
support	O
new	O
user	O
actions	O
based	O
on	O
user	O
feedback	O
.	O
However	O
,	O
adding	O
new	O
intents	O
or	O
slots	O
will	O
change	O
the	O
predefined	O
ontology	B-MethodName
.	O
As	O
a	O
consequence	O
,	O
developers	O
need	O
to	O
extract	O
additional	O
dialog	O
features	O
based	O
on	O
new	O
ontology	B-MethodName
.	O
Besides	O
,	O
new	O
system	O
actions	O
may	O
be	O
added	O
to	O
deal	O
with	O
new	O
user	O
actions	O
.	O
The	O
network	O
architecture	O
of	O
the	O
new	O
system	O
and	O
the	O
original	O
one	O
will	O
be	O
different	O
.	O
The	O
new	O
system	O
can	O
not	O
inherit	O
the	O
parameters	O
from	O
the	O
old	O
one	O
directly	O
.	O
It	O
will	O
make	O
the	O
original	O
dialog	O
manager	O
model	O
invalid	O
.	O
Therefore	O
,	O
developers	O
have	O
to	O
retrain	O
the	O
new	O
system	O
by	O
interacting	O
with	O
users	O
from	O
scratch	O
.	O
Though	O
there	O
are	O
many	O
methods	O
to	O
train	O
a	O
RL	O
-	O
based	O
dialog	O
manager	O
efficiently	O
(	O
Su	O
et	O
al	O
,	O
2016a	O
(	O
Su	O
et	O
al	O
,	O
,	O
2017Lipton	O
et	O
al	O
,	O
2017	O
;	O
Chen	O
et	O
al	O
,	O
2017	O
)	O
,	O
the	O
unmaintainable	O
RL	O
-	O
based	O
dialog	O
systems	O
will	O
still	O
be	O
put	O
on	O
the	O
shelf	O
in	O
real	O
-	O
world	O
applications	O
(	O
Paek	O
and	O
Pieraccini	O
,	O
2008	O
;	O
Paek	O
,	O
2006	O
)	O
.	O
To	O
alleviate	O
this	O
problem	O
,	O
we	O
propose	O
a	O
teacherstudent	O
framework	O
to	O
maintain	O
the	O
RL	O
-	O
based	O
dialog	O
manager	O
without	O
training	O
from	O
scratch	O
.	O
The	O
idea	O
is	O
to	O
transfer	O
the	O
knowledge	O
of	O
existing	O
resources	O
to	O
a	O
new	O
dialog	O
manager	O
.	O
Specifically	O
,	O
after	O
the	O
system	O
is	O
deployed	O
,	O
if	O
developers	O
find	O
some	O
intents	O
and	O
slots	O
missing	O
before	O
,	O
they	O
can	O
define	O
a	O
few	O
simple	O
dialog	O
rules	O
to	O
handle	O
such	O
situations	O
.	O
For	O
example	O
,	O
under	O
the	O
condition	O
shown	O
in	O
Fig	O
.	O
1	O
,	O
a	O
reasonable	O
strategy	O
is	O
to	O
inform	O
the	O
user	O
of	O
the	O
location	O
of	O
this	O
restaurant	O
.	O
Then	O
we	O
encode	O
information	O
of	O
such	O
hand	O
-	O
crafted	O
logic	O
rules	O
into	O
the	O
new	O
dialog	O
manager	O
model	O
.	O
Meanwhile	O
,	O
user	O
logs	O
and	O
dialog	O
policy	O
of	O
the	O
original	O
system	O
can	O
guide	O
the	O
new	O
system	O
to	O
complete	O
tasks	O
like	O
the	O
original	O
one	O
.	O
Under	O
the	O
guidance	O
of	O
the	O
"	O
teacher	O
"	O
(	O
logic	O
rules	O
,	O
user	O
logs	O
,	O
and	O
original	O
policy	O
)	O
,	O
we	O
can	O
reforge	O
an	O
extended	O
dialog	O
manager	O
(	O
the	O
"	O
student	O
"	O
)	O
without	O
a	O
new	O
interaction	O
environment	O
.	O
We	O
conduct	O
a	O
series	O
of	O
experiments	O
with	O
simulated	O
and	O
real	O
users	O
on	O
restaurant	O
domain	O
.	O
The	O
extensive	O
experiments	O
demonstrate	O
that	O
our	O
method	O
can	O
overcome	O
the	O
problem	O
brought	O
by	O
the	O
unpredictable	O
user	O
behavior	O
after	O
deployment	O
.	O
Owing	O
to	O
reuse	O
of	O
existing	O
resources	O
,	O
our	O
framework	O
saves	O
time	O
in	O
designing	O
new	O
interaction	O
environments	O
and	O
retraining	O
RL	O
-	O
based	O
systems	O
from	O
scratch	O
.	O
More	O
importantly	O
,	O
our	O
method	O
does	O
not	O
make	O
any	O
assumptions	O
about	O
the	O
unsupported	O
intents	O
and	O
slots	O
.	O
So	O
the	O
system	O
can	O
be	O
incrementally	O
extended	O
once	O
developers	O
find	O
new	O
intents	O
and	O
slots	O
that	O
are	O
not	O
taken	O
into	O
account	O
before	O
.	O
As	O
far	O
as	O
we	O
know	O
,	O
we	O
are	O
the	O
first	O
to	O
discuss	O
the	O
maintainability	O
of	O
deep	O
reinforcement	O
learning	O
based	O
dialog	O
systems	O
systematically	O
.	O

Dialog	O
Manager	O
The	O
dialog	O
manager	O
of	O
taskoriented	O
dialog	O
systems	O
,	O
which	O
consists	O
of	O
a	O
state	O
tracker	O
and	O
a	O
dialog	O
policy	O
module	O
,	O
controls	O
the	O
dialog	O
flow	O
.	O
Recently	O
,	O
deep	O
reinforcement	O
learning	O
(	O
Mnih	O
et	O
al	O
,	O
2013	O
(	O
Mnih	O
et	O
al	O
,	O
,	O
2015	O
has	O
been	O
applied	O
to	O
optimize	O
the	O
dialog	O
manager	O
in	O
an	O
"	O
endto	O
-	O
end	O
"	O
way	O
,	O
including	O
deep	B-MethodName
Q	I-MethodName
-	I-MethodName
Network	I-MethodName
(	O
Lipton	O
et	O
al	O
,	O
2017	O
;	O
Li	O
et	O
al	O
,	O
2017b	O
;	O
Peng	O
et	O
al	O
,	O
2017	O
;	O
Zhao	O
and	O
Eskenazi	O
,	O
2016	O
)	O
and	O
policy	B-TaskName
gradient	I-TaskName
methods	I-TaskName
(	O
Williams	O
et	O
al	O
,	O
2017	O
;	O
Su	O
et	O
al	O
,	O
2016b	O
;	O
Dhingra	O
et	O
al	O
,	O
2017	O
)	O
.	O
RL	O
methods	O
have	O
shown	O
great	O
potential	O
in	O
building	O
a	O
robust	O
dialog	O
system	O
automatically	O
.	O
However	O
,	O
RL	O
-	O
based	O
approaches	O
are	O
rarely	O
used	O
in	O
real	O
-	O
world	O
applications	O
because	O
of	O
the	O
maintainability	O
problem	O
(	O
Paek	O
and	O
Pieraccini	O
,	O
2008	O
;	O
Paek	O
,	O
2006	O
)	O
.	O
To	O
extend	O
the	O
domain	O
of	O
dialog	O
systems	O
,	O
Gašic	O
et	O
al	O
(	O
2014	O
)	O
explicitly	O
defined	O
kernel	O
functions	O
between	O
the	O
belief	O
states	O
that	O
come	O
from	O
different	O
domains	O
.	O
However	O
,	O
defining	O
an	O
appropriate	O
kernel	O
function	O
is	O
nontrivial	O
when	O
the	O
ontology	B-MethodName
has	O
changed	O
drastically	O
.	O
Shah	O
et	O
al	O
(	O
2016	O
)	O
proposed	O
to	O
integrate	O
turnlevel	O
feedback	O
with	O
a	O
task	O
-	O
level	O
reward	O
signal	O
to	O
learn	O
how	O
to	O
handle	O
new	O
user	O
intents	O
.	O
This	O
approach	O
alleviates	O
the	O
problem	O
that	O
arises	O
from	O
the	O
difference	O
between	O
training	O
and	O
deployment	O
phases	O
.	O
But	O
it	O
still	O
fails	O
when	O
the	O
developers	O
have	O
not	O
considered	O
all	O
user	O
actions	O
in	O
advance	O
.	O
Lipton	O
et	O
al	O
(	O
2017	O
)	O
proposed	O
to	O
use	O
BBQ	B-DatasetName
-	O
Networks	O
to	O
extend	O
the	O
domain	O
.	O
However	O
,	O
similar	O
to	O
Shah	O
et	O
al	O
(	O
2016	O
)	O
,	O
the	O
BBQ	B-DatasetName
-	O
Networks	O
have	O
reserved	O
a	O
few	O
bits	O
in	O
the	O
feature	O
vector	O
for	O
new	O
intents	O
and	O
slots	O
.	O
And	O
system	O
actions	O
for	O
handling	O
new	O
user	O
actions	O
have	O
been	O
considered	O
in	O
the	O
original	O
system	O
design	O
.	O
This	O
assumption	O
is	O
not	O
practical	O
enough	O
.	O
Compared	O
to	O
the	O
existing	O
domain	O
extension	O
methods	O
,	O
our	O
work	O
addresses	O
a	O
more	O
practical	O
problem	O
:	O
new	O
intents	O
and	O
slots	O
are	O
unknown	O
to	O
the	O
original	O
system	O
.	O
If	O
we	O
need	O
to	O
extend	O
the	O
dialog	O
system	O
,	O
we	O
should	O
design	O
a	O
new	O
network	O
architecture	O
to	O
represent	O
new	O
user	O
actions	O
and	O
take	O
new	O
system	O
actions	O
into	O
account	O
.	O
Knowledge	B-MethodName
Distillation	I-MethodName
Our	O
proposed	O
framework	O
is	O
inspired	O
by	O
recent	O
work	O
in	O
knowledge	B-MethodName
distillation	I-MethodName
(	O
Bucilu	O
et	O
al	O
,	O
2006	O
;	O
Ba	O
and	O
Caruana	O
,	O
2014	O
;	O
Li	O
et	O
al	O
,	O
2014	O
)	O
.	O
Knowledge	B-MethodName
distillation	I-MethodName
means	O
training	O
a	O
compact	O
model	O
to	O
mimic	O
a	O
larger	O
teacher	O
model	O
by	O
approximating	O
the	O
function	O
learned	O
by	O
the	O
teacher	O
.	O
Hinton	O
et	O
al	O
(	O
2015	O
)	O
introduced	O
knowledge	B-MethodName
distillation	I-MethodName
to	O
transfer	O
knowledge	O
from	O

Let	O
A	O
u	O
and	O
A	O
s	O
denote	O
the	O
supported	O
user	O
and	O
system	O
action	O
sets	O
in	O
the	O
original	O
system	O
design	O
respectively	O
.	O
u	O
t	O
denotes	O
the	O
user	O
input	O
in	O
the	O
t	O
-	O
th	O
turn	O
.	O
The	O
LU	O
module	O
converts	O
u	O
t	O
into	O
a	O
domain	O
specific	O
intent	O
and	O
associated	O
slots	O
to	O
form	O
a	O
user	O
action	O
a	O
u	O
t	O
A	O
u	O
.	O
The	O
system	O
will	O
return	O
an	O
action	O
a	O
s	O
t	O
A	O
s	O
according	O
to	O
the	O
dialog	O
manager	O
π	O
(	O
θ	B-HyperparameterName
)	O
.	O
Note	O
that	O
not	O
all	O
user	O
actions	O
are	O
taken	O
into	O
account	O
at	O
the	O
beginning	O
of	O
system	O
design	O
.	O
After	O
deployment	O
,	O
the	O
developers	O
can	O
find	O
that	O
some	O
user	O
actions	O
A	O
u	O
new	O
can	O
not	O
be	O
handled	O
by	O
the	O
original	O
system	O
based	O
on	O
the	O
human	O
-	O
machine	O
interaction	O
logs	O
D.	O
Generally	O
speaking	O
,	O
A	O
u	O
new	O
consists	O
of	O
new	O
intents	O
and	O
slots	O
.	O
Our	O
goal	O
is	O
to	O
extend	O
the	O
original	O
system	O
to	O
support	O
the	O
new	O
user	O
action	O
set	O
A	O
u	O
=	O
A	O
u	O
∪A	O
u	O
new	O
.	O
The	O
extended	O
dialog	O
manager	O
and	O
new	O
system	O
action	O
set	O
are	O
denoted	O
as	O
π	O
(	O
θ	B-HyperparameterName
)	O
and	O
A	O
s	O
respectively	O
.	O
To	O
handle	O
new	O
user	O
actions	O
,	O
more	O
system	O
actions	O
may	O
be	O
added	O
to	O
the	O
new	O
system	O
.	O
It	O
means	O
that	O
A	O
s	O
is	O
a	O
subset	O
of	O
A	O
s	O
.	O

Sim2	O
LU	O
Error	B-MetricName
Rate	O
Succ	O
.	O
Turn	O
Reward	O
Satis	O
.	O
Succ	O
.	O
Turn	O
Reward	O
Satis	O
.	O
Although	O
"	O
Satis	O
.	O
"	O
is	O
obtained	O
based	O
on	O
our	O
handcrafted	O
dialog	O
rules	O
,	O
it	O
approximately	O
measures	O
the	O
subjective	O
experience	O
of	O
real	O
users	O
after	O
system	O
deployment	O
.	O

To	O
evaluate	O
our	O
approach	O
,	O
we	O
design	O
another	O
user	O
simulator	O
,	O
which	O
we	O
denote	O
as	O
Sim2	O
,	O
to	O
simulate	O
the	O
unpredictable	O
real	O
customers	O
.	O
The	O
user	O
action	O
set	O
of	O
Sim2	O
is	O
denoted	O
as	O
A	O
u	O
.	O
The	O
difference	O
between	O
A	O
u	O
and	O
A	O
u	O
is	O
reflected	O
on	O
the	O
domain	O
specific	O
intents	O
7	O
.	O
Specifically	O
,	O
in	O
addition	O
to	O
the	O
intents	O
of	O
Sim1	O
,	O
A	O
u	O
includes	O
the	O
"	O
confirm	O
"	O
intent	O
.	O
The	O
difference	O
in	O
user	O
action	O
sets	O
will	O
result	O
in	O
different	O
interaction	O
strategies	O
between	O
Sim1	O
and	O
Sim2	O
.	O
To	O
verify	O
whether	O
a	O
recommended	O
restaurant	O
meets	O
his	O
(	O
her	O
)	O
constraints	O
,	O
Sim1	O
can	O
only	O
request	O
what	O
the	O
value	O
of	O
a	O
specific	O
slot	O
is	O
,	O
but	O
Sim2	O
can	O
request	O
or	O
confirm	O
.	O
After	O
obtaining	O
the	O
original	O
system	O
S	O
1	O
,	O
we	O
deploy	O
it	O
to	O
interact	O
with	O
Sim1	O
and	O
Sim2	O
respectively	O
,	O
under	O
different	O
LU	O
error	O
rates	O
(	O
Li	O
et	O
al	O
,	O
2017a	O
)	O
.	O
In	O
each	O
condition	O
,	O
we	O
simulate	O
3200	O
episodes	O
to	O
obtain	O
the	O
performance	O
.	O
details	O
of	O
the	O
test	O
performance	O
.	O
Table	O
2	O
shows	O
the	O
statistics	O
of	O
turns	O
when	O
S	O
1	O
interacts	O
with	O
Sim2	O
.	O
As	O
shown	O
in	O
Table	O
1	O
,	O
S	O
1	O
achieves	O
higher	O
dialog	O
success	O
rate	O
and	O
rewards	O
when	O
testing	O
with	O
Sim1	O
.	O
When	O
interacting	O
with	O
Sim2	O
,	O
nearly	O
half	O
of	O
the	O
responses	O
to	O
unsupported	O
user	O
actions	O
are	O
not	O
reasonable	O
.	O
Notice	O
even	O
though	O
Sim2	O
contains	O
new	O
user	O
actions	O
,	O
some	O
of	O
the	O
new	O
actions	O
might	O
be	O
appropriately	O
handled	O
by	O
S	O
1	O
.	O
It	O
may	O
be	O
due	O
to	O
the	O
robustness	O
of	O
our	O
RL	O
-	O
based	O
system	O
.	O
But	O
it	O
's	O
far	O
from	O
being	O
desired	O
.	O
The	O
unpredictable	O
real	O
user	O
behavior	O
in	O
the	O
deployment	O
stage	O
will	O
lead	O
to	O
a	O
poor	O
user	O
experience	O
in	O
real	O
-	O
world	O
applications	O
.	O
It	O
proves	O
the	O
importance	O
of	O
a	O
maintainable	O
system	O
.	O
To	O
maintain	O
the	O
original	O
system	O
,	O
we	O
define	O
a	O
few	O
simple	O
logic	O
rules	O
to	O
handle	O
unsupported	O
user	O
actions	O
:	O
if	O
users	O
confirm	O
the	O
value	O
of	O
a	O
slot	O
in	O
current	O
turn	O
,	O
the	O
system	O
should	O
inform	O
users	O
of	O
that	O
value	O
.	O
These	O
rules	O
8	O
are	O
intuitive	O
and	O
reasonable	O
to	O
handle	O
queries	O
such	O
as	O
"	O
Is	O
this	O
restaurant	O
located	O
in	O
Zhongguancun	O
?	O
"	O
.	O
There	O
are	O
four	O
slots	O
9	O
that	O
can	O
be	O
used	O
for	O
confirmation	O
,	O
so	O
we	O
define	O
four	O
logic	O
rules	O
in	O
all	O
.	O
Due	O
to	O
the	O
change	O
in	O
ontology	B-MethodName
,	O
we	O
add	O
a	O
new	O
status	O
in	O
dialog	O
features	O
to	O
represent	O
the	O
"	O
confirm	O
"	O
intent	O
of	O
users	O
.	O
It	O
leads	O
to	O
a	O
change	O
in	O
the	O
model	O
architecture	O
of	O
extended	O
dialog	O
manager	O
.	O
Then	O
we	O
distill	O
knowledge	O
of	O
the	O
S	O
1	O
and	O
logic	O
rules	O
into	O
the	O
extended	O
system	O
.	O
No	O
additional	O
data	O
is	O
used	O
to	O
obtain	O
the	O
extended	O
system	O
.	O
For	O
comparison	O
,	O
we	O
retrain	O
another	O
new	O
system	O
(	O
contrast	O
system	O
)	O
from	O
scratch	O
by	O
interacting	O
8	O
In	O
the	O
practical	O
dialog	O
system	O
,	O
we	O
can	O
inject	O
more	O
complex	O
logic	O
rules	O
and	O
take	O
dialog	O
history	O
into	O
account	O
.	O
These	O
rules	O
are	O
not	O
limited	O
to	O
question	O
/	O
answer	O
mapping	O
.	O
9	O
They	O
are	O
"	O
name	O
"	O
,	O
"	O
area	O
"	O
,	O
"	O
price	O
range	O
"	O
and	O
"	O
cuisine	O
"	O
.	O
with	O
Sim2	O
.	O
After	O
about	O
2600	O
interactions	O
with	O
Sim2	O
,	O
the	O
performance	O
of	O
contrast	O
system	O
starts	O
to	O
converge	O
.	O
Note	O
that	O
in	O
order	O
to	O
build	O
the	O
contrast	O
system	O
,	O
the	O
developers	O
need	O
to	O
redesign	O
a	O
new	O
user	O
simulator	O
or	O
hire	O
real	O
users	O
.	O
It	O
's	O
expensive	O
and	O
impractical	O
in	O
industrial	O
applications	O
.	O
Then	O
we	O
simulate	O
3200	O
interactions	O
with	O
Sim2	O
to	O
obtain	O
its	O
performance	O
.	O
Fig	O
.	O
4	O
illustrates	O
the	O
performance	O
of	O
different	O
systems	O
.	O
As	O
can	O
be	O
seen	O
,	O
the	O
extended	O
system	O
performs	O
better	O
than	O
the	O
original	O
system	O
in	O
terms	O
of	O
dialog	O
success	O
rate	O
and	O
"	O
Satis	O
.	O
"	O
.	O
This	O
is	O
to	O
a	O
large	O
degree	O
attributed	O
to	O
the	O
consideration	O
of	O
new	O
user	O
actions	O
.	O
Fig	O
.	O
4	O
(	O
a	O
)	O
shows	O
that	O
the	O
contrast	O
system	O
achieves	O
higher	O
dialog	O
success	O
rate	O
than	O
the	O
extended	O
system	O
.	O
But	O
the	O
gap	O
is	O
negligible	O
.	O
However	O
,	O
the	O
contrast	O
system	O
is	O
trained	O
from	O
scratch	O
under	O
a	O
new	O
interaction	O
environment	O
and	O
the	O
extended	O
system	O
is	O
trained	O
by	O
transferring	O
knowledge	O
of	O
the	O
original	O
system	O
and	O
logic	O
rules	O
.	O
To	O
train	O
the	O
contrast	O
system	O
,	O
about	O
2600	O
episodes	O
are	O
sampled	O
by	O
interacting	O
with	O
a	O
new	O
interaction	O
environment	O
.	O
But	O
no	O
additional	O
data	O
is	O
used	O
to	O
train	O
the	O
extended	O
system	O
.	O
In	O
Fig	O
.	O
4	O
(	O
b	O
)	O
,	O
the	O
"	O
Satis	O
.	O
"	O
of	O
the	O
extended	O
system	O
is	O
slightly	O
higher	O
than	O
the	O
contrast	O
system	O
.	O
This	O
is	O
due	O
to	O
the	O
fact	O
that	O
the	O
extended	O
system	O
learns	O
how	O
to	O
deal	O
with	O
new	O
user	O
actions	O
from	O
logic	O
rules	O
but	O
the	O
contrast	O
system	O
obtains	O
dialog	O
policy	O
by	O
exploring	O
the	O
environment	O
.	O
As	O
a	O
result	O
,	O
the	O
contrast	O
system	O
learns	O
a	O
more	O
flexible	O
dialog	O
policy	O
than	O
the	O
extended	O
system	O
10	O
.	O
However	O
,	O
the	O
"	O
Satis	O
.	O
"	O
has	O
a	O
bias	O
to	O
the	O
suboptimal	O
rules	O
,	O
Left	O
column	O
shows	O
the	O
dialog	O
context	O
condition	O
;	O
Right	O
column	O
shows	O
the	O
corresponding	O
system	O
action	O
.	O
We	O
define	O
14	O
rules	O
in	O
all	O
to	O
handle	O
newfound	O
intents	O
and	O
slots	O
shown	O
in	O
Table	O
3	O
.	O
rather	O
than	O
the	O
optimal	O
policy	O
gained	O
from	O
the	O
environment	O
.	O
It	O
suggests	O
the	O
extended	O
system	O
can	O
be	O
further	O
refined	O
by	O
reinforcement	O
learning	O
once	O
a	O
new	O
interaction	O
environment	O
is	O
available	O
.	O

While	O
classic	O
NLG	O
systems	O
typically	O
made	O
use	O
of	O
hierarchically	O
structured	O
content	O
plans	O
that	O
included	O
discourse	O
relations	O
as	O
central	O
components	O
,	O
more	O
recent	O
neural	O
approaches	O
have	O
mostly	O
mapped	O
simple	O
,	O
flat	O
inputs	O
to	O
texts	O
without	O
representing	O
discourse	O
relations	O
explicitly	O
.	O
In	O
this	O
paper	O
,	O
we	O
investigate	O
whether	O
it	O
is	O
beneficial	O
to	O
include	O
discourse	O
relations	O
in	O
the	O
input	O
to	O
neural	O
data	O
-	O
to	O
-	O
text	O
generators	O
for	O
texts	O
where	O
discourse	O
relations	O
play	O
an	O
important	O
role	O
.	O
To	O
do	O
so	O
,	O
we	O
reimplement	O
the	O
sentence	O
planning	O
and	O
realization	O
components	O
of	O
a	O
classic	O
NLG	O
system	O
,	O
Methodius	O
,	O
using	O
LSTM	B-MethodName
sequence	O
-	O
to	O
-	O
sequence	O
(	O
seq2seq	B-MethodName
)	O
models	O
.	O
We	O
find	O
that	O
although	O
seq2seq	B-MethodName
models	O
can	O
learn	O
to	O
generate	O
fluent	O
and	O
grammatical	O
texts	O
remarkably	O
well	O
with	O
sufficiently	O
representative	O
Methodius	O
training	O
data	O
,	O
they	O
can	O
not	O
learn	O
to	O
correctly	O
express	O
Methodius	O
's	O
SIMILARITY	O
and	O
CONTRAST	O
comparisons	O
unless	O
the	O
corresponding	O
RST	O
relations	O
are	O
included	O
in	O
the	O
inputs	O
.	O
Additionally	O
,	O
we	O
experiment	O
with	O
using	O
self	O
-	O
training	O
and	O
reverse	O
model	O
reranking	O
to	O
better	O
handle	O
train	O
/	O
test	O
data	O
mismatches	O
,	O
and	O
find	O
that	O
while	O
these	O
methods	O
help	O
reduce	O
content	O
errors	O
,	O
it	O
remains	O
essential	O
to	O
include	O
discourse	O
relations	O
in	O
the	O
input	O
to	O
obtain	O
optimal	O
performance	O
.	O

Traditional	O
approaches	O
to	O
the	O
task	O
of	O
natural	O
language	O
generation	O
(	O
NLG	O
)	O
have	O
employed	O
a	O
pipeline	O
of	O
modules	O
,	O
moving	O
from	O
an	O
initial	O
abstract	O
meaning	O
representation	O
(	O
MR	B-DatasetName
)	O
to	O
human	O
-	O
readable	O
natural	O
language	O
(	O
Reiter	O
and	O
Dale	O
,	O
2000	O
)	O
.	O
In	O
the	O
last	O
decade	O
,	O
the	O
success	O
of	O
neural	O
methods	O
in	O
other	O
domains	O
of	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
has	O
led	O
to	O
the	O
development	O
of	O
neural	O
'	O
end	O
-	O
to	O
-	O
end	O
'	O
(	O
e2e	B-DatasetName
)	O
*	O
The	O
first	O
two	O
authors	O
are	O
listed	O
in	O
random	O
order	O
(	O
equal	O
contribution	O
)	O
,	O
then	O
the	O
other	O
authors	O
are	O
listed	O
in	O
alphabetical	O
order	O
by	O
last	O
name	O
.	O
E	O
-	O
mail	O
:	O
stevensguille.1@buckeyemail.osu.edu	O
architectures	O
in	O
NLG	O
(	O
Dušek	O
et	O
al	O
,	O
2020	O
)	O
,	O
where	O
a	O
direct	O
mapping	O
from	O
MRs	O
to	O
text	O
is	O
learned	O
.	O
Since	O
target	O
texts	O
for	O
training	O
neural	O
models	O
are	O
typically	O
crowd	O
-	O
sourced	O
,	O
the	O
neural	O
approach	O
promises	O
to	O
make	O
it	O
easier	O
to	O
scale	O
up	O
the	O
development	O
of	O
NLG	O
systems	O
in	O
comparison	O
to	O
classic	O
approaches	O
,	O
which	O
generally	O
require	O
domain	O
-	O
or	O
applicationspecific	O
rules	O
to	O
be	O
developed	O
,	O
even	O
if	O
the	O
modules	O
themselves	O
are	O
reusable	O
.	O
Accompanying	O
the	O
increase	O
in	O
crowd	O
-	O
sourced	O
corpora	O
has	O
been	O
a	O
comparative	O
simplification	O
of	O
both	O
MRs	O
and	O
tasks	O
.	O
In	O
particular	O
,	O
classic	O
NLG	O
systems	O
typically	O
made	O
use	O
of	O
hierarchically	O
structured	O
content	O
plans	O
that	O
included	O
discourse	O
relations	O
as	O
central	O
components	O
,	O
where	O
the	O
discourse	O
relations	O
-	O
often	O
based	O
on	O
Rhetorical	O
Structure	O
Theory	O
(	O
RST	O
)	O
(	O
Mann	O
and	O
Thompson	O
,	O
1988	O
;	O
Taboada	O
and	O
Mann	O
,	O
2006	O
)	O
-	O
group	O
together	O
and	O
connect	O
elementary	O
propositions	O
or	O
messages	O
(	O
Hovy	O
,	O
1993	O
;	O
Stede	O
and	O
Umbach	O
,	O
1998	O
;	O
Isard	O
,	O
2016	O
)	O
.	O
By	O
contrast	O
,	O
more	O
recent	O
neural	O
approaches	O
-	O
in	O
particular	O
,	O
those	O
developed	O
for	O
the	O
E2E	B-DatasetName
and	O
WebNLG	B-DatasetName
shared	O
task	O
challenges	O
-	O
have	O
mostly	O
mapped	O
simple	O
,	O
flat	O
inputs	O
to	O
texts	O
without	O
representing	O
discourse	O
relations	O
explicitly	O
.	O
The	O
absence	O
of	O
discourse	O
relations	O
in	O
work	O
on	O
neural	O
NLG	O
to	O
date	O
is	O
somewhat	O
understandable	O
given	O
that	O
neural	O
systems	O
have	O
primarily	O
tackled	O
texts	O
that	O
merely	O
describe	O
entities	O
,	O
rather	O
than	O
comparing	O
them	O
,	O
situating	O
them	O
in	O
time	O
,	O
discussing	O
causal	O
or	O
other	O
contingency	O
relations	O
among	O
them	O
,	O
or	O
constructing	O
persuasive	O
arguments	O
about	O
them	O
,	O
where	O
discourse	O
relations	O
are	O
crucial	O
for	O
coherence	O
(	O
Prasad	O
et	O
al	O
,	O
2008	O
)	O
.	O
Recently	O
,	O
Balakrishnan	O
et	O
al	O
(	O
2019a	O
)	O
have	O
argued	O
that	O
discourse	O
relations	O
should	O
be	O
reintroduced	O
into	O
neural	O
generation	O
in	O
order	O
to	O
enable	O
the	O
correct	O
expression	O
of	O
these	O
relations	O
to	O
be	O
more	O
reliably	O
controlled	O
.	O
However	O
,	O
they	O
do	O
note	O
that	O
only	O
6	O
%	O
of	O
the	O
crowd	O
-	O
sourced	O
E2E	B-DatasetName
Challenge	O
texts	O
contain	O
discourse	O
connectives	O
ex	O
-	O
pressing	O
CONTRAST	O
,	O
and	O
though	O
they	O
introduce	O
a	O
conversational	O
weather	O
dataset	O
that	O
uses	O
both	O
CONTRAST	O
and	O
JUSTIFY	O
relations	O
with	O
greater	O
frequency	O
,	O
it	O
is	O
fair	O
to	O
say	O
that	O
the	O
use	O
of	O
hierarchical	O
MRs	O
that	O
incorporate	O
discourse	O
relations	O
remains	O
far	O
from	O
common	O
practice	O
.	O
In	O
this	O
paper	O
,	O
we	O
investigate	O
whether	O
it	O
is	O
beneficial	O
to	O
include	O
discourse	O
relations	O
in	O
the	O
input	O
to	O
neural	O
data	O
-	O
to	O
-	O
text	O
generators	O
for	O
texts	O
where	O
discourse	O
relations	O
play	O
an	O
important	O
role	O
.	O
To	O
do	O
so	O
,	O
we	O
reimplement	O
the	O
sentence	O
planning	O
and	O
realization	O
components	O
of	O
a	O
classic	O
NLG	O
system	O
,	O
Methodius	O
(	O
Isard	O
,	O
2016	O
)	O
,	O
using	O
LSTM	B-MethodName
sequenceto	O
-	O
sequence	O
(	O
seq2seq	B-MethodName
)	O
models	O
,	O
since	O
Methodius	O
makes	O
similarity	O
or	O
contrast	O
comparisons	O
in	O
most	O
of	O
its	O
outputs	O
.	O
Specifically	O
,	O
rather	O
than	O
crowd	O
-	O
source	O
output	O
texts	O
for	O
Methodius	O
's	O
content	O
plans	O
,	O
we	O
run	O
the	O
existing	O
system	O
to	O
obtain	O
target	O
texts	O
for	O
training	O
seq2seq	B-MethodName
models	O
,	O
and	O
experiment	O
with	O
input	O
MRs	O
(	O
derived	O
from	O
the	O
content	O
plans	O
)	O
that	O
contain	O
discourse	O
relations	O
as	O
well	O
as	O
ones	O
that	O
leave	O
them	O
out	O
.	O
1	O
In	O
our	O
experiments	O
,	O
we	O
observe	O
that	O
the	O
seq2seq	B-MethodName
models	O
learn	O
to	O
generate	O
fluent	O
and	O
grammatical	O
texts	O
remarkably	O
well	O
.	O
As	O
such	O
,	O
we	O
focus	O
our	O
evaluation	O
on	O
the	O
correct	O
and	O
coherent	O
expression	O
of	O
discourse	O
relations	O
.	O
Since	O
the	O
Methodius	O
texts	O
are	O
somewhat	O
formulaic	O
following	O
delexicalization	O
and	O
entity	O
anonymization	O
,	O
it	O
is	O
possible	O
to	O
write	O
accurate	O
automatic	O
correctness	O
checks	O
for	O
these	O
relations	O
.	O
Using	O
these	O
automatic	O
checks	O
,	O
we	O
find	O
that	O
even	O
with	O
sufficiently	O
representative	O
Methodius	O
training	O
data	O
,	O
LSTM	B-MethodName
seq2seq	B-MethodName
models	O
can	O
not	O
learn	O
to	O
correctly	O
express	O
Methodius	O
's	O
similarity	O
and	O
contrast	O
comparisons	O
unless	O
the	O
corresponding	O
RST	O
relations	O
are	O
included	O
in	O
the	O
inputs	O
.	O
This	O
is	O
an	O
at	O
least	O
somewhat	O
surprising	O
result	O
,	O
since	O
these	O
relations	O
are	O
easily	O
inferred	O
from	O
the	O
input	O
facts	O
being	O
compared	O
.	O
The	O
major	O
conclusion	O
of	O
our	O
experiments	O
is	O
that	O
explicitly	O
encoding	O
discourse	O
information	O
using	O
RST	O
relations	O
boosts	O
coherence	O
by	O
enabling	O
rhetorical	O
structure	O
to	O
be	O
reliably	O
lexicalized	O
.	O
Several	O
techniques	O
for	O
improving	O
the	O
models	O
are	O
also	O
considered	O
,	O
especially	O
for	O
situations	O
where	O
the	O
training	O
data	O
exhibits	O
mismatches	O
with	O
the	O
test	O
data	O
(	O
as	O
can	O
happen	O
in	O
practice	O
)	O
.	O
One	O
technique	O
involves	O
outputting	O
a	O
beam	O
of	O
possible	O
text	O
outputs	O
and	O
reranking	O
them	O
by	O
checking	O
the	O
correspondence	O
between	O
the	O
input	O
meaning	O
representation	O
and	O
the	O
meaning	O
representation	O
produced	O
by	O
using	O
a	O
reversed	O
model	O
to	O
map	O
texts	O
to	O
meaning	O
representations	O
.	O
The	O
other	O
technique	O
is	O
self	O
-	O
training	O
(	O
Li	O
and	O
White	O
,	O
2020	O
)	O
,	O
i.e.	O
,	O
using	O
an	O
initial	O
model	O
to	O
generate	O
additional	O
training	O
data	O
.	O
This	O
method	O
drastically	O
increases	O
the	O
amount	O
of	O
training	O
data	O
available	O
for	O
what	O
is	O
otherwise	O
quite	O
a	O
small	O
corpus	O
.	O
The	O
upshot	O
of	O
these	O
techniques	O
is	O
moderate	O
improvement	O
in	O
the	O
performance	O
of	O
both	O
models	O
with	O
respect	O
to	O
the	O
evaluation	O
metrics	O
just	O
mentioned	O
.	O
But	O
the	O
conclusion	O
remains	O
that	O
the	O
model	O
trained	O
on	O
explicit	O
RST	O
information	O
continues	O
to	O
outperform	O
the	O
model	O
without	O
explicit	O
RST	O
structure	O
in	O
the	O
input	O
.	O

The	O
Methodius	O
system	O
(	O
Isard	O
,	O
2016	O
)	O
was	O
developed	O
for	O
multilingual	O
text	B-TaskName
generation	I-TaskName
,	O
based	O
on	O
the	O
M	O
-	O
PIRO	O
project	O
(	O
Isard	O
et	O
al	O
,	O
2003	O
;	O
Isard	O
,	O
2007	O
)	O
which	O
focused	O
on	O
museum	O
exhibit	O
descriptions	O
.	O
Methodius	O
consists	O
of	O
several	O
components	O
.	O
The	O
content	O
module	O
selects	O
content	O
from	O
a	O
database	O
and	O
creates	O
a	O
content	O
plan	O
,	O
which	O
is	O
a	O
tree	O
where	O
the	O
nodes	O
are	O
labeled	O
with	O
rhetorical	O
relations	O
or	O
facts	O
,	O
following	O
the	O
structures	O
proposed	O
in	O
RST	O
.	O
Fig	O
.	O
1	O
shows	O
a	O
content	O
plan	O
.	O
The	O
content	O
plan	O
is	O
rewritten	O
into	O
a	O
sequence	O
of	O
logical	O
forms	O
,	O
one	O
per	O
sentence	O
,	O
by	O
the	O
sentence	O
planner	O
.	O
The	O
logical	O
forms	O
are	O
then	O
realized	O
as	O
a	O
text	O
by	O
means	O
of	O
a	O
Combinatory	O
Categorial	O
Grammar	O
(	O
CCG	O
)	O
using	O
OpenCCG	O
(	O
White	O
,	O
2006	O
)	O
.	O
The	O
Methodius	O
system	O
is	O
designed	O
to	O
respond	O
to	O
the	O
behaviour	O
of	O
the	O
its	O
intended	O
users	O
.	O
Sequences	O
of	O
exhibits	O
,	O
dubbed	O
'	O
chains	O
'	O
,	O
are	O
constructed	O
while	O
the	O
user	O
moves	O
through	O
the	O
museum	O
.	O
The	O
chains	O
control	O
dependencies	O
between	O
exhibit	O
descriptions	O
,	O
limit	O
redundancy	O
,	O
and	O
provide	O
discourse	O
continuity	O
.	O
While	O
RST	O
defines	O
a	O
number	O
of	O
rhetorical	O
relations	O
,	O
Methodius	O
incorporates	O
only	O
four	O
of	O
them	O
:	O
ELABORATION	O
,	O
JOINT	O
,	O
SIMILARITY	O
and	O
CON	O
-	O
TRAST	O
.	O
ELABORATION	O
connects	O
the	O
main	O
fact	O
about	O
a	O
focal	O
entity	O
with	O
other	O
,	O
peripheral	O
facts	O
about	O
that	O
entity	O
.	O
JOINT	O
connects	O
two	O
facts	O
of	O
equal	O
status	O
.	O
SIMILARITY	O
and	O
CONTRAST	O
each	O
connect	O
two	O
facts	O
of	O
equal	O
status	O
,	O
but	O
they	O
do	O
opposite	O
jobs	O
:	O
SIMILARITY	O
is	O
used	O
to	O
express	O
the	O
similarity	O
of	O
two	O
entities	O
in	O
terms	O
of	O
a	O
commonly	O
shared	O
feature	O
,	O
while	O
CONTRAST	O
is	O
used	O
to	O
show	O
that	O
the	O
values	O
of	O
a	O
shared	O
feature	O
of	O
the	O
given	O
entities	O
differ	O
.	O
For	O
instance	O
,	O
unlike	O
the	O
previous	O
coins	O
you	O
saw	O
,	O
which	O
are	O
located	O
in	O
the	O
Athens	O
Numismatic	O
Museum	O
,	O
this	O
In	O
the	O
experiments	O
discussed	O
below	O
we	O
focus	O
on	O
SIMILARITY	O
and	O
CONTRAST	O
because	O
the	O
Methodius	O
corpus	O
lexicalizes	O
them	O
.	O
Due	O
to	O
the	O
dynamic	O
generation	O
of	O
the	O
exhibit	O
descriptions	O
,	O
SIMILAR	O
-	O
ITY	O
and	O
CONTRAST	O
link	O
information	O
in	O
the	O
current	O
exhibit	O
to	O
previously	O
mentioned	O
exhibits	O
and	O
their	O
properties	O
-	O
as	O
such	O
,	O
correctly	O
generating	O
such	O
expressions	O
is	O
vital	O
to	O
maintaining	O
the	O
coherence	O
of	O
the	O
exhibit	O
chain	O
.	O
3	O
Data	O
Preprocessing	O

We	O
ran	O
self	O
-	O
training	O
experiments	O
with	O
two	O
sets	O
of	O
unlabeled	O
data	O
.	O
One	O
of	O
them	O
consists	O
of	O
the	O
content	O
plans	O
generated	O
by	O
Methodius	O
.	O
The	O
other	O
one	O
,	O
dubbed	O
'	O
heuristic	O
,	O
'	O
is	O
developed	O
from	O
the	O
existing	O
labeled	O
data	O
.	O
The	O
heuristic	O
data	O
is	O
produced	O
by	O
the	O
following	O
method	O
:	O
for	O
every	O
content	O
plan	O
produced	O
by	O
Methodius	O
,	O
extract	O
the	O
set	O
of	O
subtrees	O
of	O
the	O
content	O
plan	O
which	O
respect	O
some	O
soft	O
constraints	O
on	O
structure	O
.	O
We	O
avoid	O
extracting	O
trees	O
that	O
start	O
with	O
an	O
optional	O
type	O
.	O
The	O
subtrees	O
are	O
randomly	O
selected	O
but	O
their	O
distribution	O
is	O
required	O
to	O
closely	O
follow	O
the	O
distribution	O
of	O
distinct	O
RST	O
types	O
in	O
the	O
training	O
data	O
.	O
Since	O
the	O
size	O
of	O
the	O
Methodius	O
data	O
set	O
is	O
limited	O
,	O
the	O
heuristic	O
data	O
set	O
provides	O
useful	O
cheap	O
supplementary	O
content	O
for	O
training	O
(	O
compared	O
to	O
the	O
cost	O
of	O
eliciting	O
text	O
corresponding	O
to	O
content	O
plans	O
through	O
e.g.	O
Turkers	O
)	O
.	O
We	O
are	O
thus	O
interested	O
whether	O
having	O
genuine	O
Methodius	O
content	O
plans	O
,	O
which	O
are	O
not	O
straightforward	O
to	O
generate	O
in	O
large	O
amounts	O
,	O
could	O
be	O
completed	O
by	O
a	O
heuristic	O
data	O
set	O
generated	O
from	O
the	O
labeled	O
training	O
data	O
set	O
.	O
The	O
FACT	O
models	O
were	O
trained	O
on	O
the	O
FACT	O
versions	O
of	O
the	O
data	O
set	O
,	O
which	O
is	O
obtained	O
by	O
simply	O
deleting	O
the	O
RST	O
structure	O
from	O
the	O
RST	O
data	O
set	O
.	O
4	O
We	O
refer	O
to	O
the	O
models	O
(	O
for	O
sake	O
of	O
clarity	O
)	O
by	O
the	O
names	O
in	O
Table	O
3	O
.	O
There	O
are	O
only	O
947	O
content	O
plans	O
for	O
selftraining	O
,	O
while	O
the	O
training	O
set	O
size	O
is	O
4304	O
.	O
The	O
limited	O
number	O
of	O
content	O
plans	O
for	O
self	O
-	O
trainining	O
is	O
due	O
to	O
the	O
homogeneity	O
of	O
the	O
Methodius	O
output	O
,	O
the	O
intention	O
to	O
sync	O
the	O
length	O
of	O
training	O
and	O
test	O
sets	O
,	O
and	O
the	O
finite	O
number	O
of	O
exhibits	O
in	O
the	O
Methodius	O
data	O
base	O
.	O
These	O
content	O
plans	O
,	O
which	O
are	O
harvested	O
from	O
Methodius	O
,	O
are	O
on	O
average	O
just	O
half	O
the	O
length	O
of	O
the	O
content	O
plans	O
in	O
the	O
training	O
set	O
.	O
Their	O
shortness	O
ensures	O
the	O
system	O
is	O
exposed	O
to	O
items	O
of	O
multiple	O
lengths	O
.	O
Because	O
of	O
their	O
reduced	O
length	O
and	O
their	O
production	O
by	O
the	O
Methodius	O
system	O
,	O
variation	O
in	O
the	O
content	O
of	O
the	O
short	O
sequences	O
is	O
limited	O
.	O
The	O
unique	O
unlabelled	O
data	O
size	O
differs	O
between	O
RST	O
and	O
FACT	O
data	O
sets	O
,	O
because	O
the	O
data	O
for	O
FACT	O
is	O
produced	O
by	O
pruning	O
the	O
RST	O
data	O
,	O
the	O
deletion	O
of	O
structure	O
reduces	O
the	O
heterogeneity	O
of	O
data	O
,	O
resulting	O
in	O
fewer	O
unique	O
sequences	O
for	O
the	O
FACT	O
-	O
LG	O
input	O
.	O
We	O
trained	O
the	O
following	O
models	O
:	O
LBL	O
:	O
A	O
standard	O
LSTM	B-MethodName
seq2seq	B-MethodName
model	O
with	O
attention	O
on	O
the	O
labeled	O
data	O
,	O
which	O
is	O
also	O
the	O
base	O
model	O
for	O
the	O
other	O
methods	O
.	O
ST	O
-	O
VAN	O
:	O
A	O
model	O
trained	O
with	O
vanilla	O
selftraining	O
.	O
ST	O
-	O
RMR	O
:	O
A	O
model	O
self	O
-	O
trained	O
with	O
reverse	O
model	O
reranking	O
for	O
pseudo	O
-	O
labeling	O
.	O
Models	O
were	O
trained	O
over	O
several	O
iterations	O
,	O
though	O
for	O
exposition	O
the	O
results	O
reported	O
below	O
concern	O
just	O
the	O
best	O
model	O
iterations	O
.	O
5	O
BLEU4	O
is	O
calculated	O
on	O
both	O
the	O
standard	O
and	O
challenge	O
test	O
sets	O
.	O
BLEU4	O
,	O
though	O
limited	O
in	O
the	O
conclusions	O
it	O
supports	O
,	O
seems	O
informative	O
enough	O
to	O
allow	O
one	O
to	O
distinguish	O
between	O
RST	O
and	O
FACT	O
models	O
;	O
we	O
report	O
it	O
in	O
Appendix	O
D.	O
BLEU4	O
is	O
on	O
average	O
5	O
or	O
more	O
points	O
higher	O
for	O
RST	O
models	O
than	O
FACT	O
models	O
across	O
the	O
test	O
sets	O
.	O

We	O
count	O
the	O
sum	O
of	O
repetitions	O
,	O
hallucinations	O
and	O
omissions	O
per	O
test	O
set	O
and	O
report	O
the	O
average	O
per	O
item	O
,	O
simply	O
dividing	O
the	O
sum	O
by	O
the	O
number	O
of	O
test	O
set	O
samples	O
.	O
Fig	O
.	O
2	O
and	O
Fig	O
.	O
3	O
show	O
the	O
results	O
,	O
chiefly	O
the	O
uniform	O
improvement	O
of	O
the	O
self	O
-	O
training	O
and	O
reranking	O
models	O
over	O
the	O
baseline	O
LSTM	B-MethodName
models	O
.	O
RST	O
-	O
SM	O
with	O
self	O
-	O
training	O
is	O
the	O
best	O
model	O
.	O
RST	O
-	O
SM	O
with	O
both	O
self	O
-	O
training	O
and	O
reverse	O
model	O
reranking	O
produced	O
some	O
of	O
the	O
best	O
results	O
too	O
.	O
RST	O
-	O
SM	O
and	O
RST	O
-	O
LG	O
show	O
similar	O
performance	O
when	O
it	O
comes	O
to	O
repetitions	O
,	O
hallucinations	O
,	O
and	O
omissions	O
on	O
the	O
standard	O
test	O
set	O
.	O
RST	O
-	O
SM	O
outperforms	O
RST	O
-	O
LG	O
on	O
the	O
challenge	O
set	O
.	O
RST	O
models	O
uniformly	O
outperform	O
FACT	O
models	O
.	O
We	O
observed	O
the	O
models	O
sometimes	O
produced	O
stuttering	O
,	O
i.e.	O
multiple	O
repetition	O
.	O
Even	O
one	O
of	O
the	O
best	O
models	O
with	O
respect	O
to	O
the	O
standard	O
test	O
set	O
-	O
RST	O
-	O
SM	O
-	O
ST	O
-	O
VAN	O
(	O
see	O
Fig	O
.	O
2	O
)	O
-	O
produced	O
two	O
examples	O
of	O
stuttering	O
(	O
out	O
of	O
799	O
)	O
with	O
57	O
and	O
59	O
repetitions	O
respectively	O
.	O
Just	O
these	O
two	O
outputs	O
nearly	O
doubled	O
the	O
average	O
error	O
rate	O
of	O
RST	O
-	O
SM	O
-	O
ST	O
-	O
VAN	O
.	O
The	O
other	O
models	O
reported	O
here	O
did	O
not	O
produce	O
such	O
extreme	O
stuttering	O
.	O
But	O
despite	O
stuttering	O
,	O
RST	O
-	O
SM	O
-	O
ST	O
-	O
VAN	O
is	O
still	O
the	O
best	O
model	O
with	O
respect	O
to	O
the	O
metrics	O
considered	O
here	O
.	O
In	O
Appendix	O
C	O
,	O
model	O
performance	O
is	O
reported	O
by	O
simply	O
counting	O
the	O
total	O
number	O
of	O
test	O
examples	O
in	O
which	O
a	O
model	O
generates	O
neither	O
repetitions	O
,	O
nor	O
omissions	O
,	O
nor	O
hallucinations	O
.	O
The	O
following	O
error	O
from	O
FACT	O
-	O
LG	O
-	O
ST	O
-	O
RMR	O
shows	O
multiple	O
hallucination	O
of	O
the	O
exhibit	O
item	O
's	O
creation	O
time	O
.	O
T	O
this	O
is	O
an	O
imperial	O
portrait	O
and	O
it	O
portrays	O
roman	O
-	O
emperor0	O
.	O
like	O
the	O
coin	O
you	O
recently	O
saw	O
,	O
this	O
imperial	O
portrait	O
was	O
created	O
during	O
historical	O
-	O
period0	O
.	O
H	O
this	O
is	O
an	O
imperial	O
portrait	O
and	O
it	O
portrays	O
roman	O
-	O
emperor0	O
.	O
like	O
the	O
coin	O
,	O
this	O
imperial	O
portrait	O
was	O
created	O
during	O
historical	O
-	O
period0	O
.	O
it	O
was	O
created	O
in	O
entity0	O
-	O
creation	O
-	O
time	O
and	O
it	O
was	O
created	O
in	O
entity0	O
-	O
creation	O
-	O
time	O
.	O
Further	O
errors	O
are	O
shown	O
in	O
Appendix	O
E.	O

The	O
best	O
performances	O
are	O
shown	O
by	O
RST	O
-	O
SM	O
and	O
RST	O
-	O
LG	O
.	O
Even	O
RST	O
-	O
LBL	O
produces	O
only	O
12	O
mistakes	O
out	O
of	O
799	O
test	O
items	O
.	O
Production	O
of	O
rhetorical	O
connectives	O
corresponding	O
to	O
CONTRAST	O
and	O
SIM	O
-	O
ILARITY	O
is	O
uniformly	O
correct	O
.	O
After	O
fine	O
tuning	O
and	O
reranking	O
,	O
the	O
errors	O
reduced	O
to	O
0	B-DatasetName
and	O
2	O
respectively	O
.	O
With	O
respect	O
to	O
the	O
FACT	O
models	O
,	O
LBL	O
makes	O
mistakes	O
,	O
but	O
improves	O
upon	O
self	O
-	O
training	O
and	O
reranking	O
.	O
Nonetheless	O
RST	O
models	O
outperform	O
the	O
FACT	O
models	O
.	O
While	O
the	O
best	O
FACT	O
model	O
performs	O
well	O
with	O
respect	O
to	O
producing	O
the	O
correct	O
discourse	O
connective	O
/	O
structure	O
,	O
this	O
model	O
produces	O
serious	O
content	O
errors	O
that	O
render	O
some	O
outputs	O
(	O
discussed	O
in	O
Section	O
8.1	O
)	O
incoherent	O
.	O

While	O
traditional	O
natural	O
language	O
generation	O
systems	O
,	O
e.g.	O
Methodius	O
,	O
often	O
employ	O
knowledge	B-TaskName
graphs	I-TaskName
,	O
the	O
use	O
of	O
such	O
structure	O
in	O
neural	O
NLG	O
is	O
underdeveloped	O
.	O
An	O
exception	O
in	O
this	O
respect	O
is	O
WebNLG	B-DatasetName
(	O
Gardent	O
et	O
al	O
,	O
2017	O
)	O
,	O
which	O
is	O
a	O
multilingual	O
corpus	O
for	O
natural	O
language	O
generation	O
.	O
An	O
For	O
future	O
work	O
,	O
there	O
are	O
number	O
of	O
direction	O
we	O
intend	O
to	O
explore	O
,	O
including	O
the	O
following	O
:	O
Study	O
whether	O
large	O
-	O
scale	O
pretrained	O
models	O
likewise	O
fail	O
to	O
generalize	O
well	O
without	O
dis	O
-	O
,	O
where	O
towards	O
errors	O
counts	O
if	O
either	O
there	O
is	O
an	O
incorrectly	O
generated	O
discourse	O
cue	O
word	O
,	O
or	O
there	O
has	O
been	O
a	O
cue	O
word	O
generated	O
while	O
the	O
target	O
has	O
none	O
,	O
or	O
no	O
cue	O
word	O
is	O
generated	O
but	O
the	O
reference	O
contains	O
one	O
.	O
The	O
dotted	O
line	O
links	O
two	O
models	O
if	O
there	O
is	O
a	O
significant	O
difference	O
between	O
their	O
performance	O
in	O
terms	O
of	O
Fisher	O
's	O
Exact	O
Test	O
statistics	O
(	O
we	O
take	O
the	O
significance	O
threshold	O
5	O
%	O
)	O
.	O
course	O
relations	O
in	O
the	O
input	O
.	O
Experiment	O
with	O
more	O
diverse	O
outputs	O
for	O
Methodius	O
,	O
e.g.	O
crowd	O
-	O
sourcing	O
further	O
outputs	O
to	O
express	O
the	O
content	O
plans	O
.	O
Study	O
whether	O
constrained	O
decoding	O
could	O
be	O
used	O
to	O
reduce	O
discourse	O
structure	O
errors	O
.	O

SunBear	O
at	O
WNUT	O
-	O
2020	O
Task	O
2	O
:	O
Improving	O
RoBERTa	B-MethodName
-	O
Based	O
Noisy	O
Text	B-TaskName
Classification	I-TaskName
with	O
Knowledge	O
of	O
the	O
Data	O
domain	O

Taking	O
advantage	O
of	O
RoBERTa	B-MethodName
as	O
a	O
backbone	O
,	O
we	O
propose	O
a	O
customized	O
network	O
with	O
appreciably	O
modifications	O
.	O
Figure	O
2	O
illustrates	O
our	O
proposed	O
architecture	O
.	O
The	O
"	O
base	O
"	O
version	O
of	O
RoBERTa	B-MethodName
is	O
used	O
.	O
It	O
has	O
12	O
Transformer	B-MethodName
blocks	O
,	O
each	O
block	O
outputs	O
a	O
768	O
-	O
D	O
vector	O
for	O
each	O
token	O
.	O
Since	O
the	O
output	O
of	O
different	O
Transformer	B-MethodName
blocks	O
represent	O
different	O
semantic	O
levels	O
for	O
the	O
inputs	O
,	O
in	O
our	O
experiments	O
we	O
combine	O
outputs	O
of	O
those	O
Transformer	B-MethodName
blocks	O
by	O
concatenation	O
.	O
This	O
combination	O
is	O
fed	O
to	O
a	O
classification	O
head	O
.	O
We	O
propose	O
two	O
types	O
of	O
the	O
head	O
:	O
MLP	B-DatasetName
Head	O
:	O
A	O
simple	O
feed	O
forward	O
network	O
with	O
one	O
hidden	O
layer	O
.	O
This	O
head	O
takes	O
the	O
last	O
token	O
embedding	O
as	O
its	O
input	O
.	O
BiLSTM	B-MethodName
Head	O
:	O
A	O
recurrent	O
neural	O
network	O
with	O
one	O
Bidirectional	B-MethodName
LSTM	I-MethodName
layer	O
.	O
This	O
network	O
takes	O
embeddings	O
of	O
all	O
tokens	O
.	O
The	O
hyperparameters	O
are	O
shown	O
in	O
Section	O
4	O
.	O
3.2	O
Fine	O
-	O
tuning	O
Masked	O
Language	O
Model	O
(	O
MLM	B-DatasetName
)	O

We	O
assume	O
fine	O
-	O
tuning	O
only	O
on	O
the	O
dataset	O
might	O
cause	O
overfitting	O
on	O
the	O
chosen	O
dataset	O
only	O
.	O
Hence	O
,	O
we	O
propose	O
a	O
hierarchical	O
fine	O
-	O
tuning	O
strategy	O
for	O
RoBERTa	B-MethodName
:	O
the	O
first	O
phase	O
we	O
train	O
with	O
custom	O
domain	O
COVID	O
Tweets	O
dataset	O
for	O
domain	B-TaskName
adaptation	I-TaskName
,	O
then	O
the	O
second	O
phase	O
is	O
a	O
fine	O
-	O
tuning	O
process	O
with	O
WNUT	O
Task	O
2	O
dataset	O
for	O
task	O
adaptation	O
.	O
Our	O
custom	O
COVID	O
Tweets	O
dataset	O
is	O
gathered	O
from	O
Twitter	O
platform	O
,	O
including	O
unlabeled	O
1	O
million	O
posts	O
in	O
general	O
COVID	O
domain	O
,	O
which	O
has	O
the	O
hashtag	O
of	O
#	O
Covid	O
,	O
#	O
Covid19	O
,	O
and	O
#	O
Coronavirus	O
.	O
We	O
expect	O
this	O
model	O
to	O
generalize	O
better	O
on	O
different	O
distributed	O
dataset	O
in	O
the	O
same	O
field	O
of	O
COVID	O
Tweets	O
.	O
Figure	O
2	O
:	O
The	O
architecture	O
of	O
the	O
proposed	O
model	O
.	O
The	O
input	O
is	O
tokenized	O
into	O
a	O
sequence	O
of	O
BPE	B-MethodName
tokens	O
.	O
RoBERTa	B-MethodName
,	O
the	O
"	O
base	O
"	O
version	O
,	O
takes	O
this	O
sequence	O
and	O
propagates	O
it	O
through	O
12	O
Transformer	B-MethodName
layers	O
.	O
By	O
concatenating	O
outputs	O
from	O
these	O
12	O
layers	O
,	O
we	O
form	O
a	O
long	O
sentence	O
representation	O
for	O
the	O
follow	O
-	O
up	O
classification	O
head	O
,	O
which	O
is	O
a	O
simple	O
Multi	O
-	O
layer	O
Perceptron	O
/	O
Long	B-MethodName
Short	I-MethodName
-	I-MethodName
Term	I-MethodName
Memory	I-MethodName
network	O
.	O

Recently	O
research	O
(	O
Xie	O
et	O
al	O
,	O
2019	O
;	O
Edunov	O
et	O
al	O
,	O
2018	O
)	O
have	O
shown	O
that	O
back	O
-	O
translating	O
monolingual	O
data	O
can	O
be	O
used	O
as	O
a	O
potential	O
form	O
of	O
data	B-TaskName
augmentation	I-TaskName
in	O
Text	B-TaskName
Classification	I-TaskName
.	O
The	O
idea	O
behind	O
back	O
translation	O
is	O
to	O
translate	O
a	O
sentence	O
from	O
the	O
original	O
language	O
(	O
English	O
)	O
to	O
another	O
selected	O
language	O
and	O
then	O
translate	O
back	O
to	O
the	O
original	O
language	O
.	O
This	O
utilizes	O
the	O
power	O
of	O
current	O
welldeveloped	O
translation	O
engines	O
.	O
In	O
our	O
experiment	O
,	O
25	O
%	O
of	O
the	O
data	O
samples	O
is	O
back	O
-	O
translated	O
into	O
Vietnamese	O
,	O
the	O
same	O
amount	O
goes	O
for	O
Italian	O
and	O
French	O
,	O
and	O
the	O
rest	O
25	O
%	O
is	O
kept	O
unchanged	O
.	O
This	O
assures	O
the	O
languages	O
contribute	O
equally	O
to	O
the	O
overall	O
dataset	O
.	O
Totally	O
,	O
the	O
dataset	O
size	O
is	O
increased	O
by	O
75	O
%	O
.	O

In	O
this	O
paper	O
,	O
we	O
have	O
explored	O
and	O
proposed	O
our	O
pipeline	O
to	O
solve	O
the	O
Identification	O
of	O
Informative	O
COVID	O
-	O
19	O
English	O
Tweet	O
task	O
by	O
using	O
a	O
pretrained	O
universal	O
language	O
model	O
.	O
By	O
conducting	O
a	O
lot	O
of	O
experiments	O
,	O
we	O
have	O
demonstrated	O
that	O
the	O
use	O
of	O
RoBERTa	B-MethodName
and	O
our	O
fine	O
-	O
tuning	O
strategy	O
is	O
highly	O
effective	O
in	O
text	B-TaskName
classification	I-TaskName
tasks	O
.	O
With	O
our	O
proposed	O
methods	O
,	O
we	O
have	O
achieved	O
prominent	O
results	O
on	O
the	O
WNUT	O
Task	O
2	O
.	O
For	O
future	O
work	O
,	O
we	O
will	O
design	O
more	O
complex	O
classification	O
head	O
architectures	O
to	O
improve	O
model	O
's	O
performance	O
as	O
well	O
as	O
solving	O
problems	O
indicated	O
in	O
Section	O
4.4	O
.	O
Furthermore	O
,	O
we	O
would	O
like	O
to	O
employ	O
our	O
model	O
and	O
pipeline	O
in	O
different	O
languages	O
such	O
as	O
Vietnamese	O
to	O
see	O
how	O
they	O
adapt	O
to	O
new	O
languages	O
.	O

Relations	O
between	O
comprehensibility	O
and	O
adequacy	O
errors	O
in	O
machine	B-TaskName
translation	I-TaskName
output	O

This	O
work	O
presents	O
a	O
detailed	O
analysis	O
of	O
translation	O
errors	O
perceived	O
by	O
readers	O
as	O
comprehensibility	O
and/or	O
adequacy	O
issues	O
.	O
The	O
main	O
finding	O
is	O
that	O
good	O
comprehensibility	O
,	O
similarly	O
to	O
good	O
fluency	O
,	O
can	O
mask	O
a	O
number	O
of	O
adequacy	O
errors	O
.	O
Of	O
all	O
major	O
adequacy	O
errors	O
,	O
30	O
%	O
were	O
fully	O
comprehensible	O
,	O
thus	O
fully	O
misleading	O
the	O
reader	O
to	O
accept	O
the	O
incorrect	O
information	O
.	O
Another	O
25	O
%	O
of	O
major	O
adequacy	O
errors	O
were	O
perceived	O
as	O
almost	O
comprehensible	O
,	O
thus	O
being	O
potentially	O
misleading	O
.	O
Also	O
,	O
a	O
vast	O
majority	O
of	O
omissions	O
(	O
about	O
70	O
%	O
)	O
is	O
hidden	O
by	O
comprehensibility	O
.	O
Further	O
analysis	O
of	O
misleading	O
translations	O
revealed	O
that	O
the	O
most	O
frequent	O
error	O
types	O
are	O
ambiguity	O
,	O
mistranslation	O
,	O
noun	O
phrase	O
error	O
,	O
word	O
-	O
by	O
-	O
word	B-TaskName
translation	I-TaskName
,	O
untranslated	O
word	O
,	O
subject	O
-	O
verb	O
agreement	O
,	O
and	O
spelling	O
error	O
in	O
the	O
source	O
text	O
.	O
However	O
,	O
none	O
of	O
these	O
error	O
types	O
appears	O
exclusively	O
in	O
misleading	O
translations	O
,	O
but	O
are	O
also	O
frequent	O
in	O
fully	O
incorrect	O
(	O
incomprehensible	O
inadequate	O
)	O
and	O
discarded	O
correct	O
(	O
incomprehensible	O
adequate	O
)	O
translations	O
.	O
Deeper	O
analysis	O
is	O
needed	O
to	O
potentially	O
detect	O
underlying	O
phenomena	O
specifically	O
related	O
to	O
misleading	O
translations	O
.	O

Our	O
analysis	O
has	O
been	O
carried	O
out	O
on	O
written	O
usergenerated	O
content	O
,	O
namely	O
user	O
reviews	O
.	O
Two	O
types	O
of	O
publicly	O
available	O
user	O
reviews	O
written	O
in	O
English	O
have	O
been	O
analysed	O
:	O
IMDb	B-DatasetName
movie	I-DatasetName
reviews	I-DatasetName
1	O
(	O
Maas	O
et	O
al	O
,	O
2011	O
)	O
and	O
Amazon	O
product	O
reviews	O
2	O
(	O
McAuley	O
et	O
al	O
,	O
2015	O
)	O
.	O
A	O
set	O
of	O
those	O
user	O
reviews	O
was	O
translated	O
into	O
Croatian	O
and	O
Serbian	O
,	O
two	O
closely	O
related	O
mid	O
-	O
size	O
less	O
-	O
resourced	O
morphologically	O
rich	O
European	O
languages	O
.	O
The	O
reviews	O
were	O
translated	O
3	O
by	O
three	O
on	O
-	O
line	O
systems	O
:	O
Google	B-DatasetName
Translate	O
4	O
,	O
Bing	O
5	O
and	O
Amazon	O
translate	O
6	O
.	O
The	O
analysed	O
text	O
consists	O
of	O
a	O
mixture	O
of	O
MT	O
outputs	O
from	O
the	O
three	O
systems	O
including	O
222	O
translated	O
reviews	O
consisting	O
of	O
about	O
1500	O
sentences	O
(	O
segments	O
)	O
and	O
19837	O
untokenised	O
words	O
in	O
total	O
.	O
This	O
text	O
was	O
then	O
given	O
to	O
the	O
annotators	O
to	O
mark	O
comprehensibility	O
and	O
adequacy	O
issues	O
,	O
and	O
the	O
process	O
is	O
described	O
in	O
details	O
in	O
the	O
next	O
section	O
.	O
The	O
annotated	O
text	O
is	O
publicly	O
available	O
under	O
the	O
Creative	O
Commons	O
CC	O
-	O
BY	O
licence	O
.	O
7	O

A	O
word	O
in	O
the	O
source	O
language	O
is	O
simply	O
copied	O
to	O
the	O
translated	O
text	O
.	O
word	O
-	O
by	O
-	O
word	B-TaskName
translation	I-TaskName
A	O
sequence	O
of	O
source	O
words	O
is	O
translated	O
as	O
single	O
words	O
-	O
the	O
translation	O
choice	O
of	O
each	O
word	O
looks	O
random	O
,	O
both	O
lexically	O
and	O
morphologically	O
,	O
without	O
taking	O
into	O
account	O
any	O
context	O
.	O
Table	O
3	O
shows	O
these	O
error	O
types	O
and	O
their	O
percentages	O
for	O
misleading	O
translations	O
.	O
These	O
error	O
types	O
are	O
the	O
certainly	O
"	O
dangerous	O
"	O
because	O
they	O
can	O
easily	O
mislead	O
the	O
reader	O
to	O
accept	O
incorrect	O
information	O
.	O
However	O
,	O
the	O
very	O
same	O
error	O
types	O
are	O
often	O
perceived	O
as	O
fully	O
incorrect	O
(	O
incomprehensible	O
inadequate	O
)	O
,	O
too	O
.	O
Furthermore	O
,	O
they	O
(	O
except	O
of	O
untranslated	O
words	O
)	O
even	O
often	O
lead	O
to	O
discarding	O
correct	O
information	O
(	O
incomprehensible	O
adequate	O
)	O
.	O
Further	O
in	O
-	O
depth	O
analysis	O
is	O
needed	O
to	O
determine	O
whether	O
there	O
are	O
some	O
underlying	O
phenomena	O
related	O
exclusively	O
to	O
the	O
misleading	O
translations	O
.	O
Five	O
examples	O
of	O
different	O
perceptions	O
of	O
ambiguity	O
errors	O
,	O
noun	O
phrase	O
errors	O
and	O
word	O
-	O
by	O
-	O
word	O
translations	O
are	O
presented	O
in	O
Table	O
4	O
.	O
All	O
sentences	O
except	O
3	O
)	O
have	O
misleading	O
parts	O
(	O
fully	O
misleading	O
marked	O
as	O
red	O
and	O
potentially	O
misleading	O
as	O
green	O
)	O
.	O
In	O
the	O
sentences	O
1	O
)	O
and	O
2	O
)	O
there	O
is	O
only	O
one	O
misleading	O
ambiguous	O
word	O
.	O
The	O
incorrectly	O
chosen	O
variants	O
of	O
these	O
words	O
are	O
fully	O
comprehensible	O
so	O
that	O
without	O
the	O
source	O
text	O
,	O
the	O
reader	O
was	O
not	O
able	O
to	O
figure	O
out	O
that	O
the	O
information	O
is	O
not	O
correct	O
.	O
On	O
the	O
other	O
hand	O
,	O
the	O
ambiguous	O
word	O
in	O
the	O
sentence	O
3	O
)	O
,	O
together	O
with	O
the	O
noun	O
phrase	O
,	O
is	O
perceived	O
as	O
both	O
incomprehensible	O
and	O
inadequate	O
(	O
marked	O
as	O
violet	O
)	O
.	O
Sentences	O
4	O
)	O
and	O
5	O
)	O
illustrate	O
how	O
different	O
parts	O
of	O
a	O
phrase	O
translated	O
word	O
-	O
by	O
-	O
word	O
are	O
perceived	O
in	O
different	O
ways	O
:	O
violet	O
denotes	O
fully	O
incorrect	O
,	O
red	O
denotes	O
misleading	O
,	O
and	O
cyan	O
denotes	O
discarding	O
almost	O
correct	O
translation	O
.	O
It	O
might	O
be	O
worth	O
noting	O
that	O
all	O
sentences	O
are	O
perfectly	O
fluent	O
except	O
the	O
sentence	O
3	O
)	O
which	O
is	O
very	O
disfluent	O
.	O
Propagation	O
effect	O
Table	O
3	O
also	O
shows	O
that	O
there	O
is	O
a	O
strong	O
effect	O
of	O
propagation	O
for	O
comprehensibility	O
-	O
many	O
correct	O
words	O
are	O
perceived	O
as	O
incomprehensible	O
because	O
of	O
errors	O
in	O
surrounding	O
words	O
.	O
In	O
many	O
cases	O
,	O
the	O
reader	O
finds	O
the	O
whole	O
sentence	O
incomprehensible	O
.	O
An	O
example	O
of	O
propagation	O
can	O
be	O
seen	O
in	O
Table	O
5	O
.	O
All	O
words	O
in	O
bold	O
are	O
correct	O
,	O
but	O
all	O
were	O
perceived	O
as	O
major	O
comprehensibility	O
issues	O
due	O
to	O
different	O
types	O
of	O
errors	O
in	O
surrounding	O
words	O
:	O
a	O
red	O
misleading	O
omission	O
,	O
a	O
fully	O
incorrect	O
violet	O
word	O
,	O
and	O
an	O
incomprehensible	O
group	O
of	O
almost	O
correct	O
cyan	O
words	O
.	O
It	O
should	O
be	O
mentioned	O
that	O
for	O
some	O
adequacy	O
errors	O
,	O
annotators	O
also	O
marked	O
one	O
or	O
two	O
neighbouring	O
words	O
which	O
were	O
not	O
really	O
incorrect	O
,	O
but	O
that	O
happened	O
very	O
rarely	O
.	O
Omissions	O
Since	O
several	O
studies	O
reported	O
that	O
the	O
omissions	O
are	O
generally	O
problematic	O
to	O
spot	O
without	O
access	O
to	O
the	O
source	O
text	O
,	O
we	O
compared	O
the	O
frequencies	O
of	O
omissions	O
percieved	O
only	O
as	O
comprehensibility	O
issue	O
,	O
only	O
as	O
adequacy	O
issue	O
,	O
and	O
as	O
both	O
(	O
regardless	O
of	O
the	O
severity	O
grade	O
)	O
.	O
Table	O
6	O
confirms	O
the	O
previous	O
findings	O
:	O
a	O
vast	O
majority	O
of	O
omissions	O
(	O
71.5	O
%	O
)	O
was	O
perceived	O
only	O
as	O
adequacy	O
error	O
.	O
Only	O
9	O
%	O
of	O
actual	O
omissions	O
were	O
also	O
perceived	O
as	O
comprehensibility	O
issues	O
.	O
Apart	O
from	O
this	O
,	O
19	O
%	O
of	O
omissions	O
were	O
perceived	O
as	O
exclusively	O
comprehensibility	O
issues	O
and	O
are	O
not	O
related	O
to	O
anything	O
actually	O
omitted	O
from	O
the	O
source	O
text	O
.	O
The	O
most	O
probable	O
reason	O
is	O
the	O
influence	O
of	O
other	O
surrounding	O
errors	O
,	O
but	O
further	O
analysis	O
is	O
needed	O
to	O
better	O
understand	O
this	O
effect	O
.	O

This	O
research	O
is	O
being	O
conducted	O
with	O
the	O
financial	O
support	O
of	O
the	O
European	O
Association	O
for	O
Machine	B-TaskName
Translation	I-TaskName
(	O
EAMT	O
)	O
under	O
its	O
programme	O
"	O
2019	O
Sponsorship	O
of	O
Activities	O
"	O
at	O
the	O
ADAPT	O
Research	O
Centre	O
at	O
Dublin	O
City	O
University	O
.	O
The	O
ADAPT	O
SFI	O
Centre	O
for	O
Digital	O
Media	O
Technology	O
is	O
funded	O
by	O
Science	O
Foundation	O
Ireland	O
through	O
the	O
SFI	O
Research	O
Centres	O
Programme	O
and	O
is	O
co	O
-	O
funded	O
under	O
the	O
European	O
Regional	O
Development	O
Fund	O
(	O
ERDF	O
)	O
through	O
Grant	O
13	O
/	O
RC/2106	O
.	O
We	O
would	O
like	O
to	O
thank	O
all	O
the	O
evaluators	O
for	O
providing	O
us	O
with	O
annotations	O
and	O
feedback	O
.	O

Bilingual	O
Character	O
Representation	O
for	O
Efficiently	O
Addressing	O
Out	O
-	O
of	O
-	O
Vocabulary	O
Words	O
in	O
Code	O
-	O
Switching	O
Named	B-TaskName
Entity	I-TaskName
Recognition	I-TaskName

Named	B-TaskName
Entity	I-TaskName
Recognition	I-TaskName
(	O
NER	B-TaskName
)	O
predicts	O
which	O
word	O
tokens	O
refer	O
to	O
location	O
,	O
people	O
,	O
organization	O
,	O
time	O
,	O
and	O
other	O
entities	O
from	O
a	O
word	O
sequence	O
.	O
Deep	O
neural	O
network	O
models	O
have	O
successfully	O
achieved	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
NER	B-TaskName
tasks	O
(	O
Cohen	O
;	O
Chiu	O
and	O
Nichols	O
,	O
2016	O
;	O
Lample	O
et	O
al	O
,	O
2016	O
;	O
Shen	O
et	O
al	O
,	O
2017	O
)	O
using	O
monolingual	O
corpus	O
.	O
However	O
,	O
learning	O
from	O
code	O
-	O
switching	O
tweets	O
data	O
is	O
very	O
challenging	O
due	O
to	O
several	O
reasons	O
:	O
(	O
1	O
)	O
words	O
may	O
have	O
different	O
semantics	O
in	O
different	O
context	O
and	O
language	O
,	O
for	O
instance	O
,	O
the	O
word	O
"	O
cola	O
"	O
can	O
be	O
associated	O
with	O
product	O
or	O
"	O
queue	O
"	O
in	O
Spanish	O
(	O
2	O
)	O
data	O
from	O
social	O
media	O
are	O
noisy	O
,	O
with	O
many	O
inconsistencies	O
such	O
as	O
spelling	O
mistakes	O
,	O
repetitions	O
,	O
and	O
informalities	O
which	O
eventually	O
points	O
to	O
Out	O
-	O
of	O
-	O
Vocabulary	O
(	O
OOV	O
)	O
words	O
issue	O
(	O
3	O
)	O
entities	O
may	O
appear	O
in	O
different	O
language	O
other	O
than	O
the	O
matrix	O
language	O
.	O
For	O
example	O
"	O
todos	O
los	O
Domingos	O
en	O
Westland	O
Mall	B-DatasetName
"	O
where	O
"	O
Westland	O
Mall	B-DatasetName
"	O
is	O
an	O
English	O
named	O
entity	O
.	O
Our	O
contributions	O
are	O
two	O
-	O
fold	O
:	O
(	O
1	O
)	O
bilingual	O
character	O
bidirectional	O
RNN	O
is	O
used	O
to	O
capture	O
character	O
-	O
level	O
information	O
and	O
tackle	O
OOV	O
words	O
issue	O
(	O
2	O
)	O
we	O
apply	O
transfer	B-TaskName
learning	I-TaskName
from	O
monolingual	O
pre	O
-	O
trained	O
word	O
vectors	O
to	O
adapt	O
the	O
model	O
with	O
different	O
domains	O
in	O
a	O
bilingual	O
setting	O
.	O
In	O
our	O
model	O
,	O
we	O
use	O
LSTM	B-MethodName
to	O
capture	O
long	O
-	O
range	O
dependencies	O
of	O
the	O
word	O
sequence	O
and	O
character	O
sequence	O
in	O
bilingual	O
character	O
RNN	O
.	O
In	O
our	O
experiments	O
,	O
we	O
show	O
the	O
efficiency	O
of	O
our	O
model	O
in	O
handling	O
OOV	O
words	O
and	O
bilingual	O
word	O
context	O
.	O

Convolutional	O
Neural	O
Network	O
(	O
CNN	O
)	O
was	O
used	O
in	O
NER	B-TaskName
task	O
as	O
word	O
decoder	O
by	O
Collobert	O
et	O
al	O
(	O
2011	O
)	O
and	O
a	O
few	O
years	O
later	O
,	O
Huang	O
et	O
al	O
(	O
2015	O
)	O
introduced	O
Bidirectional	O
Long	O
-	O
Short	O
Term	O
Memory	O
(	O
BiLSTM	B-MethodName
)	O
(	O
Sundermeyer	O
et	O
al	O
,	O
2012	O
)	O
.	O
Character	O
-	O
level	O
features	O
were	O
explored	O
by	O
using	O
neural	O
architecture	O
and	O
replaced	O
hand	O
-	O
crafted	O
features	O
Lample	O
et	O
al	O
,	O
2016	O
;	O
Chiu	O
and	O
Nichols	O
,	O
2016	O
;	O
Limsopatham	O
and	O
Collier	O
,	O
2016	O
)	O
.	O
Lample	O
et	O
al	O
(	O
2016	O
)	O
also	O
showed	O
Conditional	B-MethodName
Random	I-MethodName
Field	I-MethodName
(	O
CRF	B-MethodName
)	O
(	O
Lafferty	O
et	O
al	O
,	O
2001	O
)	O
decoders	O
to	O
improve	O
the	O
results	O
and	O
used	O
Stack	O
memory	O
-	O
based	O
LSTMs	O
for	O
their	O
work	O
in	O
sequence	O
chunking	B-TaskName
.	O
Aguilar	O
et	O
al	O
(	O
2017	O
)	O
proposed	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
by	O
combining	O
Part	B-TaskName
-	I-TaskName
of	I-TaskName
-	I-TaskName
Speech	I-TaskName
tagging	I-TaskName
task	O
with	O
NER	B-TaskName
and	O
using	O
gazetteers	O
to	O
provide	O
language	O
-	O
specific	O
knowledge	O
.	O
Characterlevel	O
embeddings	O
were	O
used	O
to	O
handle	O
the	O
OOV	O
words	O
problem	O
in	O
NLP	O
tasks	O
such	O
as	O
NER	B-TaskName
(	O
Lample	O
et	O
al	O
,	O
2016	O
)	O
,	O
POS	O
tagging	O
,	O
and	O
language	O
modeling	O
.	O

For	O
our	O
experiment	O
,	O
we	O
use	O
English	O
-	O
Spanish	O
(	O
ENG	O
-	O
SPA	O
)	O
Tweets	O
data	O
from	O
Twitter	O
provided	O
by	O
62.62	O
%	O
16.76	O
%	O
19.12	O
%	O
3.91	O
%	O
54.59	O
%	O
+	O
FastText	B-MethodName
(	O
spa	O
)	O
49.76	O
%	O
12.38	O
%	O
11.98	O
%	O
3.91	O
%	O
39.45	O
%	O
+	O
token	O
replacement	O
12.43	O
%	O
12.35	O
%	O
7.18	O
%	O
3.91	O
%	O
9.60	O
%	O
+	O
token	O
normalization	O
7.94	O
%	O
8.38	O
%	O
5.01	O
%	O
1.67	O
%	O
6.08	O
%	O
Aguilar	O
et	O
al	O
(	O
2018	O
)	O
.	O
There	O
are	O
nine	O
different	O
named	O
-	O
entity	O
labels	O
.	O
The	O
labels	O
use	O
IOB	O
format	O
(	O
Inside	O
,	O
Outside	O
,	O
Beginning	O
)	O
where	O
every	O
token	O
is	O
labeled	O
as	O
B	O
-	O
label	O
in	O
the	O
beginning	O
and	O
follows	O
with	O
I	O
-	O
label	O
if	O
it	O
is	O
inside	O
a	O
named	O
entity	O
,	O
or	O
O	O
otherwise	O
.	O
For	O
example	O
"	O
Kendrick	O
Lamar	O
"	O
is	O
represented	O
as	O
B	O
-	O
PER	O
I	O
-	O
PER	O
.	O
Table	O
2	O
and	O
Table	O
3	O
show	O
the	O
statistics	O
of	O
the	O
dataset	O
.	O
"	O
Person	O
"	O
,	O
"	O
Location	O
"	O
,	O
and	O
"	O
Product	O
"	O
are	O
the	O
most	O
frequent	O
entities	O
in	O
the	O
dataset	O
,	O
and	O
the	O
least	O
common	O
ones	O
are	O
"	O
Time	O
"	O
,	O
"	O
Event	O
"	O
,	O
and	O
"	O
Other	O
"	O
categories	O
.	O
'	O
Other	O
"	O
category	O
is	O
the	O
least	O
trivial	O
among	O
all	O
because	O
it	O
is	O
not	O
well	O
clustered	O
like	O
others	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
word	O
-	O
level	O
and	O
character	O
-	O
level	O
features	O
used	O
in	O
our	O
model	O
.	O
Word	O
Representation	O
:	O
Words	O
are	O
encoded	O
into	O
continuous	O
representation	O
.	O
The	O
vocabulary	O
is	O
built	O
from	O
training	O
data	O
.	O
The	O
Twitter	O
data	O
are	O
very	O
noisy	O
,	O
there	O
are	O
many	O
spelling	O
mistakes	O
,	O
irregular	O
ways	O
to	O
use	O
a	O
word	O
and	O
repeating	O
characters	O
.	O
We	O
apply	O
several	O
strategies	O
to	O
overcome	O
the	O
issue	O
.	O
We	O
use	O
300	O
-	O
dimensional	O
English	O
and	O
Spanish	O
FastText	B-MethodName
pre	O
-	O
trained	O
word	O
vectors	O
which	O
comprise	O
two	O
million	O
words	O
vocabulary	O
each	O
and	O
they	O
are	O
trained	O
using	O
Common	B-DatasetName
Crawl	I-DatasetName
and	O
Wikipedia	O
.	O
To	O
create	O
the	O
shared	O
vocabulary	O
,	O
we	O
concatenate	O
English	O
and	O
Spanish	O
word	O
vectors	O
.	O
For	O
preprocessing	O
,	O
we	O
propose	O
the	O
following	O
steps	O
:	O
1	O
.	O
Token	O
replacement	O
:	O
Replace	O
user	O
hashtags	O
(	O
#	O
user	O
)	O
and	O
mentions	O
(	O
@user	O
)	O
with	O
"	O
USR	O
"	O
,	O
and	O
URL	O
(	O
https://domain.com	O
)	O
with	O
"	O
URL	O
"	O
.	O
2	O
.	O
Token	O
normalization	O
:	O
Concatenate	O
Spanish	O
and	O
English	O
FastText	B-MethodName
word	O
vector	O
vocabulary	O
.	O
Normalize	O
OOV	O
words	O
by	O
using	O
one	O
out	O
of	O
these	O
heuristics	O
and	O
check	O
if	O
the	O
word	O
exists	O
in	O
the	O
vocabulary	O
sequentially	O
Then	O
,	O
the	O
effectiveness	O
of	O
the	O
preprocessing	O
and	O
transfer	B-TaskName
learning	I-TaskName
in	O
handling	O
OOV	O
words	O
are	O
analyzed	O
.	O
The	O
statistics	O
is	O
showed	O
in	O
Table	O
1	O
.	O
It	O
is	O
clear	O
that	O
using	O
FastText	B-MethodName
word	O
vectors	O
reduce	O
the	O
OOV	O
words	O
rate	O
especially	O
when	O
we	O
concatenate	O
the	O
vocabulary	O
of	O
both	O
languages	O
.	O
Furthermore	O
,	O
the	O
preprocessing	O
strategies	O
dramatically	O
decrease	O
the	O
number	O
of	O
unknown	O
words	O
.	O
Character	O
Representation	O
:	O
We	O
concatenate	O
all	O
possible	O
characters	O
for	O
English	O
and	O
Spanish	O
,	O
including	O
numbers	O
and	O
special	O
characters	O
.	O
English	O
and	O
Spanish	O
have	O
most	O
of	O
the	O
characters	O
in	O
common	O
,	O
but	O
,	O
with	O
some	O
additional	O
unique	O
Spanish	O
characters	O
.	O
All	O
cases	O
are	O
kept	O
as	O
they	O
are	O
.	O

Table	O
4	O
shows	O
the	O
results	O
for	O
ENG	O
-	O
SPA	O
tweets	O
.	O
Adding	O
pre	O
-	O
trained	O
word	O
vectors	O
and	O
characterlevel	O
features	O
improved	O
the	O
performance	O
.	O
Interestingly	O
,	O
our	O
initial	O
attempts	O
at	O
adding	O
character	O
-	O
level	O
features	O
did	O
not	O
improve	O
the	O
overall	O
performance	O
,	O
until	O
we	O
apply	O
dropout	O
to	O
the	O
Char	O
-	O
RNN	O
.	O
The	O
performance	O
of	O
the	O
model	O
improves	O
significantly	O
after	O
transfer	B-TaskName
learning	I-TaskName
with	O
FastText	B-MethodName
word	O
vectors	O
while	O
it	O
also	O
reduces	O
the	O
number	O
of	O
OOV	O
words	O
in	O
the	O
development	O
and	O
test	O
set	O
.	O
The	O
margin	O
between	O
ours	O
and	O
first	O
place	O
model	O
is	O
small	O
,	O
approximately	O
1	O
%	O
.	O
We	O
try	O
to	O
use	O
sub	O
-	O
words	O
representation	O
from	O
Spanish	O
FastText	B-MethodName
,	O
however	O
,	O
it	O
does	O
not	O
improve	O
the	O
result	O
since	O
the	O
OOV	O
words	O
consist	O
of	O
many	O
special	O
characters	O
,	O
for	O
example	O
,	O
"	O
/IAtrevido	O
/	O
Provocativo	O
"	O
,	O
"	O
Twets	O
/	O
wek	O
"	O
,	O
and	O
possibly	O
create	O
noisy	O
vectors	O
and	O
most	O
of	O
them	O
are	O
not	O
entity	O
words	O
.	O

This	O
paper	O
presents	O
a	O
bidirectional	B-MethodName
LSTM	I-MethodName
-	O
based	O
model	O
with	O
hierarchical	O
architecture	O
using	O
bilingual	O
character	O
RNN	O
to	O
address	O
the	O
OOV	O
words	O
issue	O
.	O
Moreover	O
,	O
token	O
replacement	O
,	O
token	O
normalization	O
,	O
and	O
transfer	B-TaskName
learning	I-TaskName
reduce	O
OOV	O
words	O
rate	O
even	O
further	O
and	O
significantly	O
improves	O
the	O
performance	O
.	O
The	O
model	O
achieved	O
62.76	O
%	O
F1score	O
for	O
English	O
-	O
Spanish	O
language	O
pair	O
without	O
using	O
any	O
gazetteer	O
and	O
knowledge	O
-	O
based	O
information	O
.	O

This	O
work	O
is	O
partially	O
funded	O
by	O
ITS/319/16FP	O
of	O
the	O
Innovation	O
Technology	O
Commission	O
,	O
HKUST	O
16214415	O
&	O
16248016	O
of	O
Hong	O
Kong	O
Research	O
Grants	O
Council	O
,	O
and	O
RDC	O
1718050	O
-	O
0	B-DatasetName
of	O
EMOS.AI	O
.	O

We	O
propose	O
an	O
approach	O
to	O
automatically	O
test	O
for	O
originality	O
in	O
generation	O
tasks	O
where	O
no	O
standard	O
automatic	O
measures	O
exist	O
.	O
Our	O
proposal	O
addresses	O
original	O
uses	O
of	O
language	O
,	O
not	O
necessarily	O
original	O
ideas	O
.	O
We	O
provide	O
an	O
algorithm	O
for	O
our	O
approach	O
and	O
a	O
run	O
-	O
time	O
analysis	O
.	O
The	O
algorithm	O
,	O
which	O
finds	O
all	O
of	O
the	O
original	O
fragments	O
in	O
a	O
ground	O
-	O
truth	O
corpus	O
and	O
can	O
reveal	O
whether	O
a	O
generated	O
fragment	O
copies	O
an	O
original	O
without	O
attribution	O
,	O
has	O
a	O
run	O
-	O
time	O
complexity	O
of	O
θ	B-HyperparameterName
(	O
n	O
log	O
n	O
)	O
where	O
n	O
is	O
the	O
number	O
of	O
sentences	O
in	O
the	O
ground	O
truth	O
.	O

This	O
research	O
addresses	O
an	O
ethical	O
consideration	O
for	O
Natural	O
Language	O
Generation	O
,	O
namely	O
,	O
plagiarism	O
.	O
The	O
Oxford	O
English	O
Dictionary	O
defines	O
original	O
(	O
adjective	O
)	O
as	O
"	O
present	O
or	O
existing	O
from	O
the	O
beginning	O
;	O
first	O
or	O
earliest	O
"	O
and	O
"	O
created	O
directly	O
and	O
personally	O
by	O
a	O
particular	O
artist	O
;	O
not	O
a	O
copy	O
or	O
imitation	O
"	O
.	O
But	O
,	O
if	O
we	O
apply	O
the	O
definitions	O
of	O
"	O
original	O
"	O
to	O
language	O
,	O
then	O
there	O
are	O
two	O
ways	O
in	O
which	O
a	O
piece	O
of	O
generated	O
text	O
may	O
be	O
original	O
.	O
For	O
one	O
,	O
the	O
text	O
may	O
express	O
an	O
"	O
original	O
idea	O
"	O
,	O
such	O
as	O
Einstein	O
did	O
in	O
1905	O
with	O
"	O
E	O
=	O
mc	O
2	O
"	O
.	O
On	O
the	O
other	O
hand	O
,	O
a	O
non	O
-	O
original	O
idea	O
may	O
be	O
expressed	O
in	O
an	O
original	O
way	O
,	O
via	O
,	O
for	O
example	O
,	O
figurative	O
language	O
.	O
Our	O
proposed	O
approach	O
addresses	O
original	O
uses	O
of	O
language	O
.	O
It	O
does	O
not	O
necessarily	O
address	O
original	O
ideas	O
.	O
How	O
do	O
we	O
protect	O
intellectual	O
property	O
when	O
it	O
comes	O
to	O
language	O
generators	O
that	O
are	O
trained	O
on	O
a	O
world	O
-	O
wide	O
-	O
web	O
of	O
data	O
?	O
Our	O
language	O
generators	O
have	O
to	O
be	O
held	O
accountable	O
.	O
They	O
should	O
also	O
be	O
protected	O
.	O
What	O
if	O
a	O
language	O
generator	O
generates	O
an	O
original	O
analogy	O
?	O
What	O
if	O
it	O
writes	O
a	O
poem	O
that	O
is	O
so	O
great	O
that	O
it	O
ends	O
up	O
in	O
the	O
history	O
books	O
?	O
Multiple	O
language	O
generators	O
may	O
be	O
trained	O
on	O
the	O
same	O
ground	O
truth	O
(	O
e.g.	O
,	O
Wikipedia	O
)	O
with	O
the	O
same	O
embedding	O
vectors	O
(	O
e.g.	O
,	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
and	O
GPT	B-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
;	O
Radford	O
et	O
al	O
,	O
2018	O
)	O
)	O
and	O
the	O
same	O
technologies	O
(	O
deep	O
neural	O
networks	O
,	O
LSTM	B-MethodName
cells	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
,	O
transformers	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
)	O
.	O
It	O
will	O
become	O
a	O
question	O
of	O
"	O
Whose	O
generator	O
said	O
it	O
first	O
?	O
"	O
With	O
automatic	O
language	O
generation	O
,	O
we	O
need	O
a	O
way	O
to	O
automatically	O
measure	O
,	O
store	O
,	O
and	O
reference	O
original	O
ideas	O
and	O
language	O
.	O
We	O
propose	O
one	O
possible	O
solution	O
to	O
these	O
originality	O
-	O
related	O
problems	O
.	O
For	O
the	O
purposes	O
of	O
our	O
analyses	O
,	O
we	O
define	O
ground	O
truth	O
as	O
the	O
set	O
of	O
sentences	O
that	O
are	O
compared	O
with	O
the	O
generated	O
sentences	O
.	O
The	O
ground	O
truth	O
may	O
be	O
larger	O
than	O
the	O
training	O
set	O
,	O
but	O
should	O
include	O
the	O
training	O
set	O
.	O
The	O
gound	O
truth	O
would	O
also	O
,	O
ideally	O
,	O
grow	O
.	O
For	O
example	O
,	O
the	O
ground	O
truth	O
could	O
start	O
out	O
as	O
the	O
training	O
set	O
,	O
but	O
as	O
new	O
sentences	O
are	O
generated	O
with	O
a	O
trained	O
model	O
,	O
then	O
the	O
new	O
sentences	O
may	O
be	O
added	O
to	O
the	O
ground	O
truth	O
.	O
We	O
also	O
claim	O
that	O
generated	O
sentences	O
should	O
only	O
be	O
added	O
to	O
the	O
ground	O
truth	O
if	O
they	O
are	O
original	O
or	O
include	O
citations	O
where	O
appropriate	O
.	O

Ground	O
Truth	O
Fragment	O
Ground	O
-	O
truth	O
fragments	O
that	O
appear	O
once	O
and	O
only	O
once	O
in	O
the	O
ground	O
truth	O
are	O
considered	O
original	O
.	O
1	O
Likewise	O
,	O
fragments	O
that	O
appear	O
more	O
than	O
once	O
in	O
the	O
ground	O
truth	O
are	O
considered	O
"	O
not	O
original	O
"	O
.	O
For	O
example	O
,	O
"	O
lengthened	O
shadow	O
"	O
appeared	O
twice	O
in	O
our	O
ground	O
truth	O
and	O
so	O
it	O
is	O
not	O
considered	O
an	O
original	O
phrase	O
in	O
the	O
ground	O
truth	O
.	O
Combining	O
non	O
-	O
original	O
fragments	O
to	O
generate	O
a	O
new	O
idea	O
or	O
analogy	O
,	O
however	O
,	O
could	O
be	O
considered	O
an	O
original	O
use	O
of	O
language	O
.	O
For	O
example	O
,	O
"	O
the	O
writer	O
is	O
the	O
lengthened	O
shadow	O
of	O
a	O
man	O
"	O
contains	O
the	O
fragments	O
"	O
the	O
writer	O
is	O
"	O
and	O
"	O
the	O
lengthened	O
shadow	O
"	O
and	O
"	O
of	O
a	O
man	O
"	O
which	O
are	O
not	O
original	O
fragments	O
in	O
our	O
ground	O
truth	O
.	O
However	O
,	O
the	O
way	O
in	O
which	O
they	O
are	O
combined	O
in	O
this	O
example	O
creates	O
an	O
original	O
use	O
of	O
language	O
-	O
in	O
this	O
case	O
,	O
a	O
metaphor	O
.	O
(	O
Examples	O
of	O
fragments	O
that	O
appeared	O
many	O
times	O
in	O
our	O
training	O
set	O
are	O
"	O
it	O
is	O
"	O
and	O
"	O
human	O
life	O
"	O
.	O
)	O
Original	O
C	O
=	O
1	O
Not	O
Original	O
C	O
≥	O
2	O
Generated	O
Fragment	O
Original	O
C	O
=	O
0	B-DatasetName
Not	O
Original	O
,	O
Citation	O
Needed	O
C	O
=	O
1	O
Not	O
Original	O
,	O
No	O
Citation	O
Needed	O
C	O
≥	O
2	O
Here	O
is	O
one	O
possible	O
use	O
of	O
GOT	O
.	O
If	O
a	O
generated	O
sentence	O
contains	O
a	O
fragment	O
that	O
appears	O
once	O
and	O
only	O
once	O
in	O
the	O
ground	O
truth	O
(	O
after	O
duplicate	O
sentences	O
are	O
removed	O
from	O
the	O
ground	O
truth	O
)	O
,	O
then	O
the	O
generated	O
sentence	O
may	O
be	O
discarded	O
because	O
it	O
contains	O
a	O
fragment	O
from	O
the	O
ground	O
truth	O
that	O
is	O
a	O
candidate	O
for	O
protection	O
as	O
intellectual	O
property	O
.	O
In	O
other	O
words	O
,	O
the	O
sentence	O
may	O
be	O
in	O
violation	O
of	O
a	O
copyright	O
law	O
.	O
Otherwise	O
,	O
the	O
sentence	O
could	O
include	O
a	O
citation	O
of	O
the	O
source	O
for	O
the	O
original	O
fragment	O
.	O
The	O
definition	O
of	O
ground	O
-	O
truth	O
original	O
fragments	O
actually	O
calls	O
for	O
more	O
nuance	O
,	O
which	O
we	O
will	O
elaborate	O
and	O
explain	O
how	O
to	O
compute	O
next	O
.	O
We	O
maintain	O
a	O
count	O
per	O
fragment	O
that	O
is	O
incremented	O
each	O
time	O
the	O
fragment	O
appears	O
in	O
a	O
new	O
sentence	O
in	O
a	O
new	O
document	O
or	O
by	O
a	O
different	O
author	O
(	O
if	O
the	O
author	O
can	O
be	O
determined	O
in	O
both	O
instances	O
)	O
in	O
the	O
ground	O
truth	O
.	O
In	O
other	O
words	O
,	O
if	O
a	O
fragment	O
in	O
the	O
ground	O
truth	O
is	O
repeated	O
in	O
the	O
same	O
document	O
,	O
or	O
by	O
the	O
same	O
author	O
across	O
documents	O
,	O
then	O
the	O
count	O
for	O
that	O
fragment	O
is	O
incremented	O
only	O
once	O
.	O
(	O
Therefore	O
,	O
an	O
author	O
,	O
if	O
known	O
,	O
should	O
also	O
be	O
stored	O
for	O
each	O
fragment	O
,	O
at	O
least	O
until	O
the	O
count	O
for	O
that	O
fragment	O
is	O
greater	O
than	O
1	O
.	O
When	O
the	O
count	O
for	O
a	O
fragment	O
is	O
greater	O
than	O
1	O
,	O
then	O
it	O
has	O
already	O
been	O
determined	O
that	O
the	O
fragment	O
was	O
seen	O
a	O
second	O
time	O
in	O
a	O
different	O
document	O
by	O
a	O
different	O
known	O
,	O
or	O
unknown	O
,	O
author	O
.	O
)	O
The	O
count	O
for	O
a	O
fragment	O
will	O
be	O
1	O
if	O
it	O
occurs	O
just	O
once	O
in	O
the	O
ground	O
truth	O
,	O
or	O
if	O
all	O
of	O
its	O
occurrences	O
are	O
in	O
the	O
same	O
document	O
or	O
by	O
the	O
same	O
author	O
;	O
otherwise	O
,	O
the	O
count	O
will	O
be	O
greater	O
than	O
1	O
.	O
Now	O
,	O
a	O
ground	O
-	O
truth	O
fragment	O
is	O
said	O
to	O
be	O
original	O
if	O
and	O
only	O
if	O
its	O
count	O
is	O
1	O
.	O
See	O
Algorithm	O
1	O
for	O
psuedo	O
-	O
code	O
to	O
test	O
for	O
originality	O
and	O
find	O
all	O
original	O
fragements	O
in	O
a	O
dataset	O
.	O
To	O
examine	O
fragments	O
,	O
we	O
use	O
a	O
window	O
length	O
of	O
wl	O
varying	O
between	O
2	O
and	O
the	O
sentence	O
length	O
,	O
where	O
wl	O
is	O
the	O
number	O
of	O
words	O
in	O
the	O
fragment	O
.	O
If	O
the	O
first	O
or	O
last	O
word	O
in	O
the	O
window	O
is	O
a	O
determinant	O
(	O
e.g.	O
,	O
'	O
a	O
'	O
or	O
'	O
the	O
'	O
)	O
,	O
any	O
use	O
of	O
the	O
verbs	O
to	O
be	O
and	O
to	O
have	O
(	O
'	O
is	O
'	O
,	O
'	O
are	O
'	O
,	O
'	O
am	O
'	O
,	O
'	O
was	O
'	O
,	O
'	O
were	O
'	O
,	O
'	O
has	O
'	O
,	O
'	O
had	O
'	O
,	O
'	O
have	O
'	O
)	O
,	O
punctuation	O
mark	O
,	O
or	O
preposition	O
/	O
subordinating	O
conjunction	O
(	O
e.g.	O
,	O
'	O
to	O
'	O
,	O
'	O
of	O
'	O
,	O
or	O
'	O
from	O
'	O
)	O
,	O
the	O
window	O
is	O
moved	O
one	O
step	O
to	O
the	O
right	O
.	O
(	O
Shortening	O
the	O
window	O
to	O
get	O
rid	O
of	O
the	O
determinant	O
,	O
special	O
verb	O
,	O
special	O
character	O
,	O
or	O
preposition	O
would	O
result	O
in	O
a	O
window	O
size	O
already	O
covered	O
in	O
the	O
previous	O
step	O
.	O
)	O
All	O
words	O
and	O
characters	O
are	O
allowed	O
in	O
the	O
other	O
positions	O
of	O
the	O
window	O
,	O
so	O
,	O
for	O
example	O
,	O
a	O
comma	O
or	O
preposition	O
may	O
appear	O
in	O
the	O
middle	O
of	O
a	O
window	O
of	O
size	O
3	O
or	O
more	O
.	O

To	O
see	O
how	O
GOT	O
performed	O
on	O
a	O
generation	O
task	O
,	O
we	O
applied	O
it	O
to	O
a	O
metaphor	O
generator	O
that	O
we	O
built	O
,	O
based	O
on	O
an	O
RNN	O
(	O
Elman	O
,	O
1990	O
)	O
architecture	O
with	O
LSTM	B-MethodName
cells	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
for	O
training	O
a	O
language	O
model	O
on	O
the	O
language	O
of	O
metaphors	O
,	O
using	O
only	O
metaphors	O
and	O
their	O
topics	O
as	O
input	O
.	O
(	O
A	O
topic	O
was	O
inserted	O
at	O
the	O
beginning	O
of	O
each	O
input	O
sentence	O
.	O
)	O
The	O
model	O
was	O
trained	O
to	O
predict	O
the	O
next	O
word	O
in	O
the	O
sentences	O
from	O
our	O
ground	O
truth	O
-	O
a	O
set	O
of	O
22	O
,	O
113	O
quotes	O
,	O
where	O
each	O
quote	O
contains	O
at	O
least	O
one	O
metaphor	O
and	O
is	O
labeled	O
with	O
a	O
topic	O
.	O
There	O
are	O
1	O
,	O
684	O
unique	O
topics	O
(	O
e.g.	O
,	O
"	O
animals	O
"	O
,	O
"	O
fear	O
"	O
,	O
"	O
fishing	O
"	O
,	O
"	O
grandparents	O
"	O
,	O
"	O
happiness	O
"	O
,	O
"	O
motives	O
"	O
,	O
"	O
politics	O
"	O
,	O
and	O
more	O
examples	O
listed	O
in	O
Table	O
2	O
)	O
and	O
the	O
dataset	O
is	O
currently	O
available	O
to	O
the	O
public	O
online	O
as	O
part	O
of	O
"	O
Dr.	O
Mardy	O
's	O
Dictionary	O
of	O
Metaphorical	O
Quotations	O
"	O
(	O
Grothe	O
,	O
2008	O
)	O
.	O
To	O
the	O
trained	O
language	O
model	O
,	O
we	O
apply	O
an	O
inference	O
engine	O
that	O
uses	O
weighted	O
random	O
choice	O
with	O
a	O
"	O
constraining	O
factor	O
"	O
to	O
encourage	O
language	O
coherence	O
and	O
originality	O
in	O
the	O
output	O
,	O
and	O
pat	O
-	O
terns	O
of	O
metaphors	O
to	O
encourage	O
the	O
generation	O
of	O
grammatically	O
correct	O
metaphors	O
(	O
Brooks	O
and	O
Youssef	O
,	O
2020	O
)	O
.	O
The	O
constraining	O
factor	O
,	O
c	O
(	O
for	O
c	O
≥	O
1	O
)	O
,	O
causes	O
the	O
inference	O
engine	O
to	O
select	O
-	O
with	O
a	O
probability	O
of	O
1	O
c	O
-	O
the	O
most	O
likely	O
word	O
to	O
appear	O
next	O
.	O
Otherwise	O
,	O
and	O
with	O
a	O
probability	O
of	O
1	O
−	O
1	O
c	O
,	O
the	O
inference	O
engine	O
will	O
make	O
a	O
weighted	O
random	O
selection	O
.	O
Selecting	O
the	O
most	O
likely	O
next	O
word	O
encourages	O
language	O
coherencey	O
in	O
the	O
output	O
,	O
while	O
weighted	O
random	O
selection	O
encourages	O
originality	O
.	O
(	O
We	O
found	O
that	O
a	O
constraining	O
factor	O
of	O
3	O
or	O
4	O
worked	O
best	O
with	O
our	O
model	O
.	O
)	O
A	O
generated	O
sentence	O
failed	O
the	O
GOT	O
if	O
a	O
fragment	O
of	O
at	O
least	O
2	O
words	O
appeared	O
as	O
an	O
"	O
original	O
"	O
fragment	O
in	O
the	O
training	O
set	O
;	O
that	O
is	O
,	O
if	O
the	O
fragment	O
appeared	O
just	O
once	O
in	O
the	O
ground	O
truth	O
.	O
Using	O
our	O
metaphor	O
generator	O
,	O
we	O
generated	O
500	O
metaphors	O
from	O
randomly	O
chosen	O
topics	O
.	O
Applying	O
GOT	O
on	O
each	O
of	O
the	O
500	O
generated	O
metaphors	O
,	O
we	O
found	O
that	O
only	O
32	O
repeated	O
an	O
"	O
original	O
"	O
fragment	O
from	O
the	O
training	O
set	O
.	O
From	O
this	O
experiment	O
,	O
we	O
conclude	O
that	O
out	O
of	O
the	O
500	O
generated	O
metaphors	O
,	O
468	O
of	O
them	O
,	O
or	O
just	O
over	O
93	O
%	O
,	O
can	O
be	O
considered	O
original	O
.	O
(	O
Table	O
2	O
provides	O
examples	O
from	O
our	O
metaphor	O
generator	O
on	O
randomly	O
generated	O
topics	O
.	O
)	O

Classical	O
approaches	O
to	O
question	O
calibration	O
are	O
either	O
subjective	O
or	O
require	O
newly	O
created	O
questions	O
to	O
be	O
deployed	O
before	O
being	O
calibrated	O
.	O
Recent	O
works	O
explored	O
the	O
possibility	O
of	O
estimating	O
question	O
difficulty	O
from	O
text	O
,	O
but	O
did	O
not	O
experiment	O
with	O
the	O
most	O
recent	O
NLP	O
models	O
,	O
in	O
particular	O
Transformers	O
.	O
In	O
this	O
paper	O
,	O
we	O
compare	O
the	O
performance	O
of	O
previous	O
literature	O
with	O
Transformer	B-MethodName
models	O
experimenting	O
on	O
a	O
public	O
and	O
a	O
private	O
dataset	O
.	O
Our	O
experimental	O
results	O
show	O
that	O
Transformers	O
are	O
capable	O
of	O
outperforming	O
previously	O
proposed	O
models	O
.	O
Moreover	O
,	O
if	O
an	O
additional	O
corpus	O
of	O
related	O
documents	O
is	O
available	O
,	O
Transformers	O
can	O
leverage	O
that	O
information	O
to	O
further	O
improve	O
calibration	O
accuracy	B-MetricName
.	O
We	O
characterize	O
the	O
dependence	O
of	O
the	O
model	O
performance	O
on	O
some	O
properties	O
of	O
the	O
questions	O
,	O
showing	O
that	O
it	O
performs	O
best	O
on	O
questions	O
ending	O
with	O
a	O
question	O
mark	O
and	O
Multiple	O
-	O
Choice	O
Questions	O
(	O
MCQs	O
)	O
with	O
one	O
correct	O
choice	O
.	O

This	O
section	O
describes	O
how	O
we	O
build	O
the	O
different	O
models	O
which	O
are	O
compared	O
with	O
the	O
current	O
state	O
of	O
the	O
art	O
of	O
QDE	O
from	O
text	O
.	O
These	O
models	O
are	O
built	O
upon	O
the	O
two	O
pre	O
-	O
trained	O
language	O
models	O
,	O
finetuning	O
them	O
with	O
two	O
different	O
approaches	O
.	O
The	O
first	O
approach	O
consists	O
in	O
directly	O
fine	O
-	O
tuning	O
the	O
pre	O
-	O
trained	O
model	O
for	O
the	O
task	O
of	O
QDE	O
from	O
text	O
.	O
The	O
second	O
approach	O
is	O
made	O
of	O
two	O
steps	O
:	O
we	O
i	O
)	O
further	O
pre	O
-	O
train	O
the	O
pre	O
-	O
trained	O
model	O
on	O
the	O
task	O
of	O
Masked	B-TaskName
Language	I-TaskName
Modeling	I-TaskName
(	O
MLM	B-DatasetName
)	O
to	O
improve	O
domain	O
knowledge	O
,	O
and	O
subsequently	O
ii	O
)	O
fine	O
-	O
tune	O
it	O
on	O
the	O
task	O
of	O
QDE	O
from	O
text	O
.	O
This	O
is	O
all	O
done	O
separately	O
for	O
the	O
two	O
datasets	O
:	O
we	O
do	O
not	O
perform	O
any	O
experiments	O
across	O
the	O
two	O
datasets	O
.	O

This	O
is	O
the	O
simplest	O
of	O
the	O
two	O
approaches	O
;	O
the	O
architecture	O
used	O
for	O
fine	O
-	O
tuning	O
is	O
shown	O
in	O
Figure	O
1	O
.	O
Given	O
the	O
pre	O
-	O
trained	O
language	O
model	O
,	O
we	O
stack	O
an	O
additional	O
fully	O
connected	O
layer	O
on	O
top	O
of	O
the	O
network	O
,	O
in	O
order	O
to	O
use	O
that	O
as	O
the	O
new	O
output	O
.	O
Following	O
the	O
fine	O
-	O
tuning	O
guidelines	O
in	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
we	O
use	O
only	O
the	O
first	O
output	O
of	O
the	O
pre	O
-	O
trained	O
language	O
model	O
.	O
This	O
works	O
since	O
the	O
first	O
output	O
correspond	O
to	O
the	O
special	O
token	O
[	O
CLS	O
]	O
which	O
is	O
added	O
at	O
the	O
beginning	O
of	O
the	O
input	O
text	O
and	O
is	O
the	O
only	O
one	O
used	O
for	O
regression	O
and	O
classification	O
.	O
Since	O
question	O
calibration	O
is	O
a	O
regression	O
task	O
,	O
the	O
additional	O
output	O
layer	O
has	O
one	O
neuron	O
,	O
and	O
the	O
weights	O
of	O
the	O
connections	O
with	O
the	O
previous	O
layer	O
are	O
randomly	O
initialized	O
.	O
During	O
fine	O
-	O
tuning	O
,	O
both	O
the	O
weights	O
of	O
the	O
additional	O
layer	O
and	O
the	O
internal	O
weights	O
of	O
the	O
pre	O
-	O
trained	O
language	O
model	O
are	O
updated	O
.	O
For	O
the	O
input	O
,	O
we	O
use	O
the	O
same	O
tokenization	O
and	O
the	O
same	O
encoding	O
as	O
the	O
original	O
models	O
.	O
That	O
is	O
,	O
all	O
the	O
input	O
samples	O
start	O
with	O
the	O
special	O
token	O
[	O
CLS	O
]	O
and	O
contain	O
two	O
sentences	O
,	O
separated	O
by	O
the	O
[	O
SEP	O
]	O
token	O
(	O
another	O
special	O
token	O
)	O
:	O
the	O
first	O
sentence	O
is	O
the	O
(	O
tokenized	O
)	O
question	O
while	O
the	O
second	O
one	O
contains	O
the	O
(	O
tokenized	O
)	O
choices	O
'	O
text	O
1	O
.	O
Specifically	O
,	O
we	O
experiment	O
with	O
three	O
different	O
encodings	O
for	O
the	O
second	O
sentence	O
:	O
i	O
)	O
Q	O
only	O
,	O
we	O
leave	O
it	O
empty	O
,	O
thus	O
considering	O
only	O
the	O
text	O
of	O
the	O
question	O
,	O
ii	O
)	O
Q+correct	O
,	O
we	O
use	O
the	O
text	O
of	O
the	O
correct	O
choice	O
(	O
s	O
)	O
(	O
as	O
in	O
Figure	O
1	O
)	O
,	O
iii	O
)	O
Q+all	O
,	O
we	O
use	O
the	O
text	O
of	O
all	O
the	O
possible	O
choices	O
,	O
concatenating	O
them	O
in	O
a	O
single	O
sentence	O
.	O
For	O
the	O
ASSISTments	O
dataset	O
,	O
only	O
the	O
first	O
encoding	O
is	O
possible	O
as	O
the	O
text	O
of	O
the	O
choices	O
(	O
correct	O
answer	O
and	O
distractors	O
)	O
is	O
not	O
available	O
.	O
We	O
also	O
experimented	O
a	O
fourth	O
approach	O
,	O
considering	O
all	O
the	O
possible	O
choices	O
using	O
several	O
[	O
SEP	O
]	O
tokens	O
between	O
each	O
choice	O
;	O
however	O
,	O
this	O
approach	O
performed	O
largely	O
worse	O
that	O
the	O
others	O
,	O
thus	O
we	O
do	O
not	O
report	O
it	O
here	O
.	O
We	O
believe	O
that	O
the	O
model	O
,	O
in	O
that	O
case	O
,	O
does	O
not	O
have	O
enough	O
training	O
questions	O
to	O
learn	O
the	O
meaning	O
of	O
the	O
additional	O
separators	O
(	O
BERT	B-MethodName
and	O
DistilBERT	B-MethodName
are	O
pre	O
-	O
trained	O
to	O
use	O
only	O
one	O
[	O
SEP	O
]	O
token	O
)	O
.	O

Masked	B-TaskName
Language	I-TaskName
Modeling	I-TaskName
(	O
MLM	B-DatasetName
)	O
is	O
a	O
fill	O
-	O
in	O
-	O
theblank	O
task	O
,	O
where	O
a	O
word	O
of	O
the	O
input	O
text	O
is	O
substituted	O
by	O
a	O
[	O
MASK	O
]	O
token	O
and	O
the	O
model	O
is	O
trained	O
to	O
use	O
the	O
surrounding	O
words	O
to	O
predict	O
the	O
word	O
that	O
was	O
masked	O
.	O
We	O
leverage	O
MLM	B-DatasetName
to	O
perform	O
an	O
additional	O
pre	O
-	O
training	O
of	O
the	O
pre	O
-	O
trained	O
language	O
models	O
before	O
the	O
fine	O
-	O
tuning	O
on	O
QDE	O
from	O
text	O
.	O
Our	O
goal	O
is	O
to	O
let	O
the	O
model	O
learn	O
the	O
questions	O
'	O
topics	O
more	O
accurately	O
than	O
how	O
it	O
would	O
do	O
with	O
the	O
fine	O
-	O
tuning	O
on	O
QDE	O
only	O
.	O
In	O
order	O
for	O
MLM	B-DatasetName
to	O
be	O
effective	O
,	O
though	O
,	O
we	O
need	O
an	O
additional	O
dataset	O
of	O
documents	O
about	O
the	O
same	O
topics	O
that	O
are	O
assessed	O
by	O
the	O
questions	O
:	O
this	O
is	O
available	O
only	O
for	O
the	O
CloudAcademy	O
dataset	O
,	O
which	O
contains	O
the	O
transcript	O
of	O
some	O
of	O
the	O
video	O
-	O
lectures	O
on	O
the	O
e	O
-	O
learning	O
platform	O
.	O
In	O
practice	O
,	O
we	O
perform	O
pretraining	O
with	O
MLM	B-DatasetName
as	O
follows	O
.	O
We	O
randomly	O
mask	O
15	O
%	O
of	O
the	O
words	O
of	O
the	O
available	O
lectures	O
,	O
then	O
train	O
the	O
language	O
model	O
to	O
predict	O
the	O
masked	O
words	O
sentence	O
by	O
sentence	O
.	O
The	O
actual	O
prediction	O
is	O
performed	O
by	O
stacking	O
a	O
fully	O
connected	O
layer	O
and	O
a	O
softmax	B-MethodName
layer	O
on	O
top	O
of	O
the	O
original	O
pre	O
-	O
trained	O
model	O
:	O
for	O
each	O
masked	O
sentence	O
,	O
this	O
additional	O
layer	O
consumes	O
as	O
input	O
the	O
contextual	O
embedding	O
corresponding	O
to	O
the	O
[	O
MASK	O
]	O
token	O
,	O
and	O
tries	O
to	O
predict	O
the	O
word	O
that	O
should	O
be	O
inserted	O
in	O
its	O
place	O
.	O
After	O
pre	O
-	O
training	O
the	O
model	O
on	O
the	O
task	O
of	O
MLM	B-DatasetName
,	O
the	O
additional	O
dense	O
and	O
softmax	B-MethodName
layers	O
are	O
removed	O
from	O
the	O
network	O
,	O
thus	O
leaving	O
us	O
with	O
and	O
the	O
[	O
SEP	O
]	O
token	O
is	O
still	O
used	O
only	O
to	O
indicate	O
the	O
end	O
of	O
the	O
question	O
.	O
We	O
use	O
this	O
naming	O
(	O
"	O
Sentence	O
1	O
"	O
and	O
"	O
Sentence	O
2	O
"	O
)	O
since	O
it	O
is	O
the	O
one	O
used	O
in	O
the	O
original	O
paper	O
.	O
a	O
pre	O
-	O
trained	O
model	O
which	O
has	O
the	O
same	O
architecture	O
as	O
the	O
original	O
one	O
,	O
with	O
the	O
only	O
difference	O
that	O
all	O
the	O
internal	O
weights	O
were	O
updated	O
during	O
the	O
additional	O
MLM	B-DatasetName
pre	O
-	O
training	O
.	O
The	O
architecture	O
for	O
the	O
final	O
fine	O
-	O
tuning	O
for	O
QDE	O
from	O
text	O
is	O
the	O
same	O
as	O
the	O
one	O
shown	O
in	O
Figure	O
1	O
.	O

In	O
this	O
work	O
we	O
use	O
the	O
publicly	O
available	O
data	O
collection	O
provided	O
by	O
ASSISTments	O
and	O
the	O
private	O
CloudAcademy	O
data	O
collection	O
.	O
Both	O
data	O
collections	O
are	O
made	O
of	O
two	O
datasets	O
:	O
i	O
)	O
the	O
Answers	O
dataset	O
(	O
referred	O
to	O
as	O
A	O
)	O
and	O
ii	O
)	O
the	O
Questions	O
dataset	O
(	O
Q	O
)	O
.	O
It	O
is	O
important	O
to	O
remark	O
that	O
A	O
and	O
Q	O
are	O
abstract	O
names	O
:	O
we	O
have	O
one	O
A	O
dataset	O
for	O
CloudAcademy	O
and	O
one	O
A	O
dataset	O
for	O
ASSISTments	O
(	O
similarly	O
for	O
Q	O
)	O
.	O
A	O
contains	O
the	O
students	O
'	O
answers	O
:	O
for	O
each	O
one	O
,	O
it	O
stores	O
the	O
user	O
ID	O
,	O
the	O
question	O
ID	O
,	O
the	O
correctness	O
of	O
the	O
answer	O
and	O
a	O
timestamp	O
.	O
Importantly	O
,	O
A	O
contains	O
only	O
"	O
first	O
timers	O
"	O
,	O
meaning	O
that	O
we	O
consider	O
only	O
the	O
first	O
interaction	O
between	O
a	O
student	O
and	O
a	O
question	O
.	O
Q	O
contains	O
the	O
textual	O
information	O
about	O
the	O
items	O
:	O
question	O
ID	O
,	O
question	O
text	O
and	O
,	O
in	O
the	O
case	O
of	O
CloudAcademy	O
,	O
the	O
text	O
of	O
the	O
possible	O
choices	O
.	O
For	O
the	O
experiments	O
on	O
CloudAcademy	O
data	O
,	O
we	O
also	O
have	O
access	O
to	O
an	O
additional	O
dataset	O
-	O
referred	O
to	O
as	O
Lectures	O
(	O
L	O
)	O
which	O
contains	O
the	O
transcripts	O
of	O
some	O
online	O
lectures	O
available	O
on	O
the	O
platform	O
and	O
is	O
used	O
for	O
the	O
additional	O
MLM	B-DatasetName
pre	O
-	O
training	O
.	O

As	O
displayed	O
in	O
Figure	O
2	O
,	O
training	O
is	O
performed	O
in	O
two	O
steps	O
,	O
repeated	O
for	O
the	O
two	O
datasets	O
:	O
i	O
)	O
the	O
IRT	O
model	O
is	O
trained	O
in	O
order	O
to	O
calibrate	O
the	O
questions	O
and	O
obtain	O
the	O
ground	O
truth	O
difficulties	O
,	O
then	O
ii	O
)	O
these	O
ground	O
truth	O
latent	O
traits	O
are	O
used	O
as	O
target	O
values	O
to	O
train	O
the	O
model	O
on	O
QDE	O
from	O
text	O
.	O
The	O
first	O
step	O
consists	O
in	O
using	O
A	O
to	O
estimate	O
with	O
IRT	O
the	O
target	O
difficulty	O
of	O
all	O
the	O
questions	O
.	O
Specifically	O
,	O
we	O
use	O
pyirt	O
6	O
for	O
the	O
estimation	O
and	O
consider	O
[	O
−5	O
;	O
5	O
]	O
as	O
the	O
possible	O
range	O
of	O
difficulties	O
.	O
Difficulties	O
estimated	O
at	O
this	O
stage	O
will	O
later	O
be	O
used	O
as	O
ground	O
truth	O
and	O
therefore	O
are	O
inserted	O
as	O
target	O
values	O
in	O
Q.	O
Then	O
,	O
Q	O
is	O
split	O
into	O
a	O
train	O
dataset	O
(	O
Q	O
TRAIN	O
)	O
,	O
used	O
to	O
train	O
our	O
model	O
on	O
QDE	O
from	O
text	O
,	O
and	O
a	O
test	O
dataset	O
(	O
Q	O
TEST	O
)	O
,	O
which	O
is	O
used	O
for	O
the	O
final	O
evaluation	O
of	O
the	O
model	O
;	O
we	O
keep	O
80	O
%	O
of	O
the	O
questions	O
for	O
training	O
and	O
20	O
%	O
for	O
testing	O
.	O
At	O
training	O
time	O
,	O
we	O
keep	O
a	O
portion	O
of	O
Q	O
TRAIN	O
(	O
10	O
%	O
)	O
as	O
development	O
set	O
,	O
for	O
hyperparameter	O
tuning	O
.	O
When	O
L	O
is	O
used	O
for	O
pre	O
-	O
training	O
the	O
model	O
,	O
the	O
setup	O
is	O
very	O
similar	O
.	O
The	O
only	O
difference	O
is	O
that	O
the	O
regression	O
model	O
,	O
before	O
being	O
fine	O
-	O
tuned	O
on	O
the	O
task	O
of	O
QDE	O
from	O
text	O
using	O
Q	O
TRAIN	O
,	O
is	O
pre	O
-	O
trained	O
on	O
the	O
task	O
of	O
MLM	B-DatasetName
on	O
L.	O
Being	O
an	O
unsupervised	O
task	O
,	O
we	O
use	O
the	O
whole	O
L	O
for	O
this	O
.	O
Transformers	O
are	O
implemented	O
with	O
the	O
transformers	O
7	O
library	O
from	O
HuggingFace	O
;	O
fine	O
-	O
tuning	O
and	O
pre	O
-	O
training	O
are	O
performed	O
with	O
TensorFlow	O
8	O
.	O
Hyperparameters	O
are	O
shown	O
in	O
Appendix	O
C.	O

In	O
this	O
paper	O
we	O
have	O
performed	O
a	O
study	O
of	O
how	O
Transformer	B-MethodName
models	O
perform	O
in	O
the	O
task	O
of	O
QDE	O
from	O
text	O
,	O
and	O
have	O
proposed	O
a	O
model	O
which	O
outperforms	O
previous	O
approaches	O
.	O
Specifically	O
,	O
the	O
proposed	O
model	O
is	O
built	O
upon	O
a	O
pre	O
-	O
trained	O
BERT	B-MethodName
language	O
model	O
,	O
which	O
is	O
fine	O
-	O
tuned	O
for	O
the	O
task	O
of	O
QDE	O
from	O
text	O
.	O
Previous	O
approaches	O
either	O
require	O
an	O
additional	O
dataset	O
of	O
documents	O
about	O
the	O
same	O
topics	O
assessed	O
by	O
the	O
questions	O
or	O
can	O
not	O
leverage	O
such	O
information	O
;	O
differently	O
from	O
them	O
the	O
proposed	O
model	O
is	O
capable	O
of	O
outperforming	O
state	O
of	O
the	O
art	O
approaches	O
being	O
trained	O
only	O
on	O
the	O
text	O
of	O
the	O
questions	O
,	O
and	O
can	O
be	O
further	O
improved	O
if	O
such	O
additional	O
dataset	O
is	O
available	O
.	O
As	O
an	O
outcome	O
from	O
our	O
analysis	O
,	O
we	O
can	O
say	O
that	O
:	O
i	O
)	O
if	O
an	O
additional	O
dataset	O
is	O
available	O
,	O
BERT	B-MethodName
with	O
MLM	B-DatasetName
pre	O
-	O
training	O
seems	O
to	O
be	O
the	O
best	O
performing	O
model	O
;	O
ii	O
)	O
if	O
the	O
only	O
available	O
data	O
is	O
the	O
text	O
of	O
the	O
questions	O
,	O
DistilBERT	B-MethodName
might	O
be	O
a	O
better	O
option	O
,	O
as	O
it	O
has	O
basically	O
the	O
same	O
performance	O
as	O
BERT	B-MethodName
but	O
at	O
a	O
fraction	O
of	O
the	O
computational	O
cost	O
.	O
Furthermore	O
,	O
we	O
studied	O
the	O
effect	O
of	O
some	O
questions	O
characteristics	O
on	O
BERT	B-MethodName
and	O
R2DE	O
,	O
comparing	O
the	O
two	O
models	O
.	O
We	O
have	O
observed	O
that	O
the	O
magnitude	O
of	O
the	O
error	O
naturally	O
increases	O
with	O
the	O
magnitude	O
of	O
the	O
difficulty	O
(	O
especially	O
for	O
R2DE	O
)	O
,	O
but	O
there	O
is	O
not	O
a	O
clear	O
correlation	O
between	O
the	O
input	O
length	O
and	O
the	O
accuracy	B-MetricName
of	O
the	O
estimation	O
.	O
We	O
have	O
also	O
observed	O
that	O
both	O
models	O
are	O
less	O
accurate	O
in	O
estimating	O
the	O
difficulty	O
of	O
cloze	O
questions	O
,	O
compared	O
to	O
questions	O
that	O
end	O
with	O
a	O
question	O
mark	O
,	O
and	O
that	O
the	O
decrease	O
in	O
accuracy	B-MetricName
is	O
lower	O
for	O
BERT	B-MethodName
.	O
We	O
believe	O
that	O
this	O
happens	O
because	O
underscores	O
are	O
not	O
frequent	O
in	O
natural	O
language	O
and	O
thus	O
the	O
model	O
has	O
a	O
chance	O
of	O
learning	O
them	O
only	O
during	O
the	O
fine	O
-	O
tuning	O
on	O
QDE	O
,	O
not	O
during	O
MLM	B-DatasetName
pre	O
-	O
training	O
.	O
This	O
is	O
probably	O
not	O
enough	O
data	O
for	O
learning	O
(	O
from	O
scratch	O
)	O
the	O
meaning	O
of	O
underscores	O
in	O
exam	O
questions	O
.	O
Lastly	O
,	O
BERT	B-MethodName
performs	O
better	O
on	O
questions	O
with	O
only	O
one	O
correct	O
choice	O
than	O
on	O
questions	O
with	O
multiple	O
correct	O
choices	O
(	O
for	O
the	O
latter	O
,	O
it	O
is	O
also	O
outperformed	O
by	O
R2DE	O
)	O
.	O
This	O
might	O
be	O
due	O
to	O
the	O
encoding	O
we	O
used	O
for	O
multiple	O
correct	O
choices	O
,	O
and	O
it	O
is	O
worth	O
exploring	O
in	O
future	O
research	O
.	O
Future	O
works	O
could	O
continue	O
to	O
dig	O
deeper	O
into	O
the	O
analysis	O
of	O
the	O
model	O
,	O
as	O
we	O
believe	O
that	O
the	O
accuracy	B-MetricName
of	O
the	O
estimation	O
could	O
be	O
further	O
improved	O
by	O
using	O
an	O
ensemble	O
model	O
in	O
which	O
different	O
sub	O
-	O
models	O
are	O
used	O
depending	O
on	O
some	O
characteristic	O
of	O
the	O
question	O
under	O
calibration	O
.	O
Also	O
,	O
future	O
works	O
will	O
try	O
to	O
explore	O
the	O
attention	B-HyperparameterName
layers	I-HyperparameterName
of	O
the	O
proposed	O
model	O
,	O
as	O
it	O
might	O
provide	O
useful	O
information	O
about	O
the	O
reasons	O
why	O
the	O
model	O
works	O
better	O
on	O
some	O
questions	O
.	O

Copy	O
the	O
running	O
instance	O
using	O
the	O
"	O
Instance	O
Copy	O
"	O
command	O
to	O
the	O
EU	O
region	O
.	O
skill	O
levels	O
of	O
the	O
students	O
and	O
the	O
difficulty	O
of	O
the	O
questions	O
via	O
likelihood	O
maximization	O
,	O
by	O
selecting	O
the	O
configuration	O
(	O
i.e.the	O
θs	O
and	O
bs	O
)	O
that	O
maximizes	O
the	O
probability	O
of	O
the	O
observed	O
results	O
.	O
Also	O
,	O
it	O
is	O
possible	O
to	O
assess	O
the	O
knowledge	O
levelθ	O
i	O
of	O
a	O
student	O
i	O
from	O
the	O
correctness	O
of	O
its	O
answers	O
to	O
a	O
set	O
of	O
calibrated	O
assessment	O
items	O
Q	O
=	O
q	O
1	O
,	O
q	O
2	O
,	O
...	O
,	O
q	O
Nq	B-DatasetName
.	O
This	O
is	O
done	O
by	O
maximizing	O
the	O
results	O
of	O
the	O
multiplication	O
between	O
the	O
i.r.f	O
.	O
of	O
the	O
questions	O
that	O
were	O
answered	O
correctly	O
and	O
the	O
complementary	O
(	O
i.e.	O
1	O
−	O
P	O
C	O
)	O
of	O
the	O
i.r.f	O
.	O
of	O
the	O
questions	O
that	O
were	O
answered	O
erroneously	O
.	O

Interactive	O
topic	B-TaskName
models	I-TaskName
are	O
powerful	O
tools	O
for	O
understanding	O
large	O
collections	O
of	O
text	O
.	O
However	O
,	O
existing	O
sampling	O
-	O
based	O
interactive	O
topic	O
modeling	O
approaches	O
scale	O
poorly	O
to	O
large	O
data	O
sets	O
.	O
Anchor	O
methods	O
,	O
which	O
use	O
a	O
single	O
word	O
to	O
uniquely	O
identify	O
a	O
topic	O
,	O
offer	O
the	O
speed	O
needed	O
for	O
interactive	O
work	O
but	O
lack	O
both	O
a	O
mechanism	O
to	O
inject	O
prior	O
knowledge	O
and	O
lack	O
the	O
intuitive	O
semantics	O
needed	O
for	O
userfacing	O
applications	O
.	O
We	O
propose	O
combinations	O
of	O
words	O
as	O
anchors	O
,	O
going	O
beyond	O
existing	O
single	O
word	O
anchor	O
algorithmsan	O
approach	O
we	O
call	O
"	O
Tandem	O
Anchors	O
"	O
.	O
We	O
begin	O
with	O
a	O
synthetic	O
investigation	O
of	O
this	O
approach	O
then	O
apply	O
the	O
approach	O
to	O
interactive	O
topic	O
modeling	O
in	O
a	O
user	O
study	O
and	O
compare	O
it	O
to	O
interactive	O
and	O
noninteractive	O
approaches	O
.	O
Tandem	O
anchors	O
are	O
faster	O
and	O
more	O
intuitive	O
than	O
existing	O
interactive	O
approaches	O
.	O
Topic	B-TaskName
models	I-TaskName
distill	O
large	O
collections	O
of	O
text	O
into	O
topics	O
,	O
giving	O
a	O
high	O
-	O
level	O
summary	O
of	O
the	O
thematic	O
structure	O
of	O
the	O
data	O
without	O
manual	O
annotation	O
.	O
In	O
addition	O
to	O
facilitating	O
discovery	O
of	O
topical	O
trends	O
(	O
Gardner	O
et	O
al	O
,	O
2010	O
)	O
,	O
topic	O
modeling	O
is	O
used	O
for	O
a	O
wide	O
variety	O
of	O
problems	O
including	O
document	B-TaskName
classification	I-TaskName
(	O
Rubin	O
et	O
al	O
,	O
2012	O
)	O
,	O
information	B-TaskName
retrieval	I-TaskName
(	O
Wei	O
and	O
Croft	O
,	O
2006	O
)	O
,	O
author	O
identification	O
(	O
Rosen	O
-	O
Zvi	O
et	O
al	O
,	O
2004	O
)	O
,	O
and	O
sentiment	B-TaskName
analysis	I-TaskName
(	O
Titov	O
and	O
McDonald	O
,	O
2008	O
)	O
.	O
However	O
,	O
the	O
most	O
compelling	O
use	O
of	O
topic	B-TaskName
models	I-TaskName
is	O
to	O
help	O
users	O
understand	O
large	O
datasets	O
(	O
Chuang	O
et	O
al	O
,	O
2012	O
)	O
.	O
Interactive	O
topic	O
modeling	O
allows	O
non	O
-	O
experts	O
to	O
refine	O
automatically	O
generated	O
topics	O
,	O
making	O
topic	B-TaskName
models	I-TaskName
less	O
of	O
a	O
"	O
take	O
it	O
or	O
leave	O
it	O
"	O
proposition	O
.	O
Including	O
humans	O
input	O
during	O
training	O
improves	O
the	O
quality	O
of	O
the	O
model	O
and	O
allows	O
users	O
to	O
guide	O
topics	O
in	O
a	O
specific	O
way	O
,	O
custom	O
tailoring	O
the	O
model	O
for	O
a	O
specific	O
downstream	O
task	O
or	O
analysis	O
.	O
The	O
downside	O
is	O
that	O
interactive	O
topic	O
modeling	O
is	O
slow	O
-	O
algorithms	O
typically	O
scale	O
with	O
the	O
size	O
of	O
the	O
corpus	O
-	O
and	O
requires	O
non	O
-	O
intuitive	O
information	O
from	O
the	O
user	O
in	O
the	O
form	O
of	O
must	O
-	O
link	O
and	O
can	O
not	O
-	O
link	O
constraints	O
(	O
Andrzejewski	O
et	O
al	O
,	O
2009	O
)	O
.	O
We	O
address	O
these	O
shortcomings	O
of	O
interactive	O
topic	O
modeling	O
by	O
using	O
an	O
interactive	O
version	O
of	O
the	O
anchor	O
words	O
algorithm	O
for	O
topic	B-TaskName
models	I-TaskName
.	O
The	O
anchor	O
algorithm	O
(	O
Arora	O
et	O
al	O
,	O
2013	O
)	O
is	O
an	O
alternative	O
topic	O
modeling	O
algorithm	O
which	O
scales	O
with	O
the	O
number	O
of	O
unique	O
word	O
types	O
in	O
the	O
data	O
rather	O
than	O
the	O
number	O
of	O
documents	O
or	O
tokens	O
(	O
Section	O
1	O
)	O
.	O
This	O
makes	O
the	O
anchor	O
algorithm	O
fast	O
enough	O
for	O
interactive	O
use	O
,	O
even	O
in	O
web	O
-	O
scale	O
document	O
collections	O
.	O
A	O
drawback	O
of	O
the	O
anchor	O
method	O
is	O
that	O
anchor	O
words	O
-	O
words	O
that	O
have	O
high	O
probability	O
of	O
being	O
in	O
a	O
single	O
topic	O
-	O
are	O
not	O
intuitive	O
.	O
We	O
extend	O
the	O
anchor	O
algorithm	O
to	O
use	O
multiple	O
anchor	O
words	O
in	O
tandem	O
(	O
Section	O
2	O
)	O
.	O
Tandem	O
anchors	O
not	O
only	O
improve	O
interactive	O
refinement	O
,	O
but	O
also	O
make	O
the	O
underlying	O
anchor	O
-	O
based	O
method	O
more	O
intuitive	O
.	O
For	O
interactive	O
topic	O
modeling	O
,	O
tandem	O
anchors	O
produce	O
higher	O
quality	O
topics	O
than	O
single	O
word	O
anchors	O
(	O
Section	O
3	O
)	O
.	O
Tandem	O
anchors	O
provide	O
a	O
framework	O
for	O
fast	O
interactive	O
topic	O
modeling	O
:	O
users	O
improve	O
and	O
refine	O
an	O
existing	O
model	O
through	O
multiword	O
anchors	O
(	O
Section	O
4	O
)	O
.	O
Compared	O
to	O
existing	O
methods	O
such	O
as	O
Interactive	O
Topic	B-TaskName
Models	I-TaskName
,	O
our	O
method	O
is	O
much	O
faster	O
.	O

We	O
now	O
describe	O
more	O
concretely	O
how	O
to	O
combine	O
an	O
anchor	O
facets	O
to	O
describe	O
the	O
cooccurrence	O
pattern	O
of	O
our	O
new	O
pseudoword	O
anchor	O
.	O
In	O
tandem	O
anchors	O
,	O
we	O
create	O
vector	O
representations	O
that	O
combine	O
the	O
information	O
from	O
anchor	O
facets	O
.	O
Our	O
anchor	O
facets	O
are	O
G	O
1	O
.	O
.	O
.	O
G	O
K	O
,	O
where	O
G	O
k	O
is	O
a	O
set	O
of	O
anchor	O
facets	O
which	O
will	O
form	O
the	O
kth	B-DatasetName
pseudoword	O
anchor	O
.	O
The	O
pseudowords	O
are	O
g	O
1	O
.	O
.	O
.	O
g	O
K	O
,	O
where	O
g	O
k	O
is	O
the	O
pseudoword	O
from	O
G	O
k	O
.	O
These	O
pseudowords	O
form	O
the	O
new	O
rows	O
of	O
S.	O
We	O
give	O
several	O
candidates	O
for	O
combining	O
anchors	O
facets	O
into	O
a	O
single	O
multiword	O
anchor	O
;	O
we	O
compare	O
their	O
performance	O
in	O
Section	O
3	O
.	O
Vector	O
Average	O
An	O
obvious	O
function	O
for	O
computing	O
the	O
central	O
tendency	O
is	O
the	O
vector	O
average	O
.	O
For	O
each	O
anchor	O
facet	O
,	O
S	O
g	O
k	O
,	O
j	O
=	O
i	O
G	O
k	O
S	O
i	O
,	O
j	O
|	O
G	O
k	O
|	O
,	O
(	O
2	O
)	O
where	O
|	O
G	O
k	O
|	O
is	O
the	O
cardinality	O
of	O
G	O
k	O
.	O
Vector	O
average	O
makes	O
the	O
pseudoword	O
S	O
g	O
k	O
,	O
j	O
more	O
central	O
,	O
which	O
is	O
intuitive	O
but	O
inconsistent	O
with	O
the	O
interpretation	O
from	O
Arora	O
et	O
al	O
(	O
2013	O
)	O
that	O
anchors	O
should	O
be	O
extreme	O
points	O
whose	O
linear	O
combinations	O
explain	O
more	O
central	O
words	O
.	O
Or	O
-	O
operator	O
An	O
alternative	O
approach	O
is	O
to	O
consider	O
a	O
cooccurrence	O
with	O
any	O
anchor	O
facet	O
in	O
G	O
k	O
.	O
For	O
word	O
j	O
,	O
we	O
use	O
De	O
Morgan	O
's	O
laws	O
to	O
set	O
S	O
g	O
k	O
,	O
j	O
=	O
1	O
−	O
i	O
G	O
k	O
(	O
1	O
−	O
S	O
i	O
,	O
j	O
)	O
.	O
(	O
3	O
)	O
Unlike	O
the	O
average	O
,	O
which	O
pulls	O
the	O
pseudoword	O
inward	O
,	O
this	O
or	O
-	O
operator	O
pushes	O
the	O
word	O
outward	O
,	O
increasing	O
each	O
of	O
the	O
dimensions	O
.	O
Increasing	O
the	O
volume	O
of	O
the	O
simplex	O
spanned	O
by	O
the	O
anchors	O
explains	O
more	O
words	O
.	O
Element	O
-	O
wise	O
Min	O
Vector	O
average	O
and	O
oroperator	O
are	O
both	O
sensitive	O
to	O
outliers	O
and	O
can	O
not	O
account	O
for	O
polysemous	O
anchor	O
facets	O
.	O
Returning	O
to	O
our	O
previous	O
example	O
,	O
both	O
"	O
camera	O
"	O
and	O
"	O
bag	O
"	O
are	O
bad	O
anchors	O
for	O
camera	O
bags	O
because	O
they	O
appear	O
in	O
documents	O
discussing	O
other	O
products	O
.	O
However	O
,	O
if	O
both	O
"	O
camera	O
"	O
and	O
"	O
bag	O
"	O
are	O
anchor	O
facets	O
,	O
we	O
can	O
look	O
at	O
an	O
intersection	O
of	O
their	O
contexts	O
:	O
words	O
that	O
appear	O
with	O
both	O
.	O
Using	O
the	O
intersection	O
,	O
the	O
cooccurrence	O
pattern	O
of	O
our	O
anchor	O
facet	O
will	O
only	O
include	O
terms	O
relevant	O
to	O
camera	O
bags	O
.	O
Mathematically	O
,	O
this	O
is	O
an	O
element	O
-	O
wise	O
min	O
operator	O
,	O
S	O
g	O
k	O
,	O
j	O
=	O
min	O
i	O
G	O
k	O
S	O
i	O
,	O
j	O
.	O
(	O
4	O
)	O
This	O
construction	O
,	O
while	O
perhaps	O
not	O
as	O
simple	O
as	O
the	O
previous	O
two	O
,	O
is	O
robust	O
to	O
words	O
which	O
have	O
cooccurrences	O
which	O
are	O
not	O
unique	O
to	O
a	O
single	O
topic	O
.	O
Harmonic	O
Mean	O
Leveraging	O
the	O
intuition	O
that	O
we	O
should	O
use	O
a	O
combination	O
function	O
which	O
is	O
both	O
centralizing	O
(	O
like	O
vector	O
average	O
)	O
and	O
ignores	O
large	O
outliers	O
(	O
like	O
element	O
-	O
wise	O
min	O
)	O
,	O
the	O
final	O
combination	O
function	O
is	O
the	O
element	O
-	O
wise	O
harmonic	O
mean	O
.	O
Thus	O
,	O
for	O
each	O
anchor	O
facet	O
S	O
g	O
k	O
,	O
j	O
=	O
i	O
G	O
k	O
S	O
−1	O
i	O
,	O
j	O
|	O
G	O
k	O
|	O
−1	O
.	O
(	O
5	O
)	O
Since	O
the	O
harmonic	O
mean	O
tends	O
towards	O
the	O
lowest	O
values	O
in	O
the	O
set	O
,	O
it	O
is	O
not	O
sensitive	O
to	O
large	O
outliers	O
,	O
giving	O
us	O
robustness	O
to	O
polysemous	O
words	O
.	O

We	O
use	O
the	O
well	O
-	O
known	O
20	B-DatasetName
Newsgroups	I-DatasetName
dataset	O
(	O
20NEWS	O
)	O
used	O
in	O
previous	O
interactive	O
topic	O
modeling	O
work	O
:	O
18	O
,	O
846	O
Usenet	O
postings	O
from	O
20	O
different	O
newgroups	O
in	O
the	O
early	O
1990s	O
.	O
1	O
We	O
remove	O
the	O
newsgroup	O
headers	O
from	O
each	O
message	O
,	O
which	O
contain	O
the	O
newsgroup	O
names	O
,	O
but	O
otherwise	O
left	O
messages	O
intact	O
with	O
any	O
footers	O
or	O
quotes	O
.	O
We	O
then	O
remove	O
stopwords	O
and	O
words	O
which	O
appear	O
in	O
fewer	O
than	O
100	O
documents	O
or	O
more	O
than	O
1	O
,	O
500	O
documents	O
.	O
To	O
seed	O
the	O
tandem	O
anchors	O
,	O
we	O
use	O
the	O
titles	O
of	O
newsgroups	O
.	O
To	O
build	O
each	O
multiword	O
anchor	O
facet	O
,	O
we	O
split	O
the	O
title	O
on	O
word	O
boundaries	O
and	O
expand	O
any	O
abbreviations	O
or	O
acronyms	O
.	O
For	O
example	O
,	O
the	O
newsgroup	O
title	O
'	O
comp.os.mswindows.misc	O
'	O
becomes	O
{	O
"	O
computer	O
"	O
,	O
"	O
operating	O
"	O
,	O
"	O
system	O
"	O
,	O
"	O
microsoft	O
"	O
,	O
"	O
windows	O
"	O
,	O
"	O
miscellaneous	O
"	O
}	O
.	O
We	O
do	O
not	O
fully	O
specify	O
the	O
topic	O
;	O
the	O
title	O
gives	O
some	O
intuition	O
,	O
but	O
the	O
topic	O
modeling	O
algorithm	O
must	O
still	O
recover	O
the	O
complete	O
topic	O
-	O
word	O
distributions	O
.	O
This	O
is	O
akin	O
to	O
knowing	O
the	O
names	O
of	O
the	O
categories	O
used	O
but	O
nothing	O
else	O
.	O
Critically	O
,	O
the	O
topic	O
modeling	O
algorithm	O
has	O
no	O
knowledge	O
of	O
document	O
-	O
label	O
relationships	O
.	O

Tandem	O
anchors	O
will	O
enable	O
users	O
to	O
direct	O
topic	O
inference	O
to	O
improve	O
topic	O
quality	O
.	O
However	O
,	O
for	O
the	O
algorithm	O
to	O
be	O
interactive	O
we	O
must	O
also	O
consider	O
runtime	O
.	O
Cook	O
and	O
Thomas	O
(	O
2005	O
)	O
argue	O
that	O
for	O
interactive	O
applications	O
with	O
user	O
-	O
initiated	O
actions	O
like	O
ours	O
the	O
response	O
time	O
should	O
be	O
less	O
than	O
ten	O
seconds	O
.	O
Longer	O
waits	O
can	O
increase	O
the	O
cognitive	O
load	O
on	O
the	O
user	O
and	O
harm	O
the	O
user	O
interaction	O
.	O
6	O
Significant	O
at	O
p	O
<	O
0.01/4	O
when	O
using	O
two	O
-	O
tailed	O
t	O
-	O
tests	O
with	O
a	O
Bonferroni	O
correction	O
.	O
For	O
each	O
of	O
our	O
evaluations	O
,	O
we	O
verify	O
the	O
normality	O
of	O
our	O
data	O
(	O
D'Agostino	O
and	O
Pearson	O
,	O
1973	O
)	O
and	O
use	O
two	O
-	O
tailed	O
t	O
-	O
tests	O
with	O
Bonferroni	O
correction	O
to	O
determine	O
whether	O
the	O
differences	O
between	O
the	O
different	O
methods	O
are	O
significant	O
.	O
Fortunately	O
,	O
the	O
runtime	O
of	O
tandem	O
anchors	O
is	O
amenable	O
to	O
interactive	O
topic	O
modeling	O
.	O
On	O
20NEWS	O
,	O
interactive	O
updates	O
take	O
a	O
median	O
time	O
of	O
2.13	O
seconds	O
.	O
This	O
result	O
was	O
obtained	O
using	O
a	O
single	O
core	O
of	O
an	O
AMD	O
Phemon	O
II	O
X6	O
1090	O
T	O
processor	O
.	O
Furthermore	O
,	O
larger	O
datasets	O
typically	O
have	O
a	O
sublinear	O
increase	O
in	O
distinct	O
word	O
types	O
,	O
so	O
we	O
can	O
expect	O
to	O
see	O
similar	O
run	O
times	O
,	O
even	O
on	O
much	O
larger	O
datasets	O
.	O
Compared	O
to	O
other	O
interactive	O
topic	O
modeling	O
algorithms	O
,	O
tandem	O
anchors	O
has	O
a	O
very	O
attractive	O
run	O
time	O
.	O
For	O
example	O
,	O
using	O
an	O
optimized	O
version	O
of	O
the	O
sampler	O
for	O
the	O
Interactive	O
Topic	O
Model	O
described	O
by	O
Hu	O
and	O
Boyd	O
-	O
Graber	O
(	O
2012	O
)	O
,	O
and	O
the	O
recommended	O
30	O
iterations	O
of	O
sampling	O
,	O
the	O
Interactive	O
Topic	O
Model	O
updates	O
with	O
a	O
median	O
time	O
of	O
24.8	O
seconds	O
(	O
Hu	O
and	O
Boyd	O
-	O
Graber	O
,	O
2012	O
)	O
,	O
which	O
is	O
well	O
beyond	O
our	O
desired	O
update	O
time	O
for	O
interactive	O
use	O
and	O
an	O
order	O
of	O
magnitude	O
slower	O
than	O
tandem	O
anchors	O
.	O
Another	O
promising	O
interactive	O
topic	O
modeling	O
approach	O
is	O
Utopian	O
(	O
Choo	O
et	O
al	O
,	O
2013	O
)	O
,	O
which	O
uses	O
non	O
-	O
negative	O
factorization	O
,	O
albeit	O
without	O
the	O
benefit	O
of	O
anchor	O
words	O
.	O
Utopian	O
is	O
much	O
slower	O
than	O
tandem	O
anchors	O
.	O
Even	O
on	O
the	O
small	O
InfoVis	O
-	O
VAST	B-DatasetName
dataset	O
which	O
contains	O
only	O
515	O
documents	O
,	O
Utopian	O
takes	O
48	O
seconds	O
to	O
converge	O
.	O
While	O
the	O
times	O
are	O
not	O
strictly	O
comparable	O
due	O
to	O
differing	O
datasets	O
,	O
Utopian	O
scales	O
linearly	O
with	O
the	O
size	O
of	O
the	O
data	O
,	O
we	O
can	O
intuit	O
that	O
even	O
for	O
moderately	O
sized	O
datasets	O
such	O
as	O
20NEWS	O
,	O
Utopian	O
is	O
infeasible	O
for	O
interactive	O
topic	O
modeling	O
due	O
to	O
run	O
time	O
.	O
While	O
each	O
of	O
these	O
interactive	O
topic	O
modeling	O
algorithms	O
do	O
achieve	O
reasonable	O
topics	O
,	O
only	O
our	O
algorithm	O
fits	O
the	O
run	O
time	O
requirements	O
for	O
inter	O
-	O
Figure	O
2	O
:	O
Interface	O
for	O
user	O
study	O
with	O
multiword	O
anchors	O
applied	O
to	O
interactive	O
topic	O
modeling	O
.	O
activity	O
.	O
Furthermore	O
,	O
since	O
tandem	O
anchors	O
scales	O
with	O
the	O
size	O
of	O
the	O
vocabulary	O
rather	O
than	O
the	O
size	O
of	O
the	O
data	O
,	O
this	O
trend	O
will	O
only	O
become	O
more	O
pronounced	O
as	O
we	O
increase	O
the	O
amount	O
of	O
data	O
.	O

Given	O
high	O
quality	O
anchor	O
facets	O
,	O
the	O
tandem	O
anchor	O
algorithm	O
can	O
produce	O
high	O
quality	O
topic	B-TaskName
models	I-TaskName
(	O
particularly	O
when	O
the	O
harmonic	O
mean	O
combiner	O
is	O
used	O
)	O
.	O
Moreover	O
,	O
the	O
tandem	O
anchor	O
algorithm	O
is	O
fast	O
enough	O
to	O
be	O
interactive	O
(	O
as	O
opposed	O
to	O
model	O
-	O
based	O
approaches	O
such	O
as	O
the	O
Interactive	O
Topic	O
Model	O
)	O
.	O
We	O
now	O
turn	O
our	O
attention	O
to	O
our	O
main	O
experiment	O
:	O
tandem	O
anchors	O
applied	O
to	O
the	O
problem	O
of	O
interactive	O
topic	O
modeling	O
.	O
We	O
compare	O
both	O
single	O
word	O
and	O
tandem	O
anchors	O
in	O
our	O
study	O
.	O
We	O
do	O
not	O
include	O
the	O
Interactive	O
Topic	O
Model	O
or	O
Utopian	O
,	O
as	O
their	O
run	O
times	O
are	O
too	O
slow	O
for	O
our	O
users	O
.	O

To	O
show	O
that	O
interactive	O
tandem	O
anchor	O
words	O
are	O
fast	O
,	O
effective	O
,	O
and	O
intuitive	O
,	O
we	O
ask	O
users	O
to	O
understand	O
a	O
dataset	O
using	O
the	O
anchor	O
word	O
algorithm	O
.	O
For	O
this	O
user	O
study	O
,	O
we	O
recruit	O
twenty	O
participants	O
drawn	O
from	O
a	O
university	O
student	O
body	O
.	O
The	O
student	O
median	O
age	O
is	O
twenty	O
-	O
two	O
.	O
Seven	O
are	O
female	O
,	O
and	O
thirteen	O
are	O
male	O
.	O
None	O
of	O
the	O
students	O
had	O
any	O
prior	O
familiarity	O
with	O
topic	O
modeling	O
or	O
the	O
20NEWS	O
dataset	O
.	O
Each	O
participant	O
sees	O
a	O
simple	O
user	O
interface	O
(	O
Figure	O
2	O
)	O
with	O
topic	O
given	O
as	O
a	O
row	O
with	O
two	O
columns	O
.	O
The	O
left	O
column	O
allows	O
users	O
to	O
view	O
and	O
edit	O
topics	O
'	O
anchor	O
words	O
;	O
the	O
right	O
column	O
lists	O
the	O
most	O
probable	O
words	O
in	O
each	O
topic	O
.	O
7	O
The	O
user	O
can	O
remove	O
an	O
anchor	O
word	O
or	O
drag	O
words	O
from	O
7	O
While	O
we	O
use	O
topics	O
generated	O
using	O
harmonic	O
mean	O
for	O
our	O
final	O
analysis	O
,	O
users	O
were	O
shown	O
topics	O
generated	O
using	O
the	O
min	O
combiner	O
.	O
However	O
,	O
this	O
does	O
not	O
change	O
our	O
result	O
.	O
the	O
topic	O
word	O
lists	O
(	O
right	O
column	O
)	O
to	O
become	O
an	O
anchor	O
word	O
.	O
Users	O
can	O
also	O
add	O
additional	O
topics	O
by	O
clicking	O
the	O
"	O
Add	O
Anchor	O
"	O
to	O
create	O
additional	O
anchors	O
.	O
If	O
the	O
user	O
wants	O
to	O
add	O
a	O
word	O
to	O
a	O
tandem	O
anchor	O
set	O
that	O
does	O
not	O
appear	O
in	O
the	O
interface	O
,	O
they	O
manually	O
type	O
the	O
word	O
(	O
restricted	O
to	O
the	O
model	O
's	O
vocabulary	O
)	O
.	O
When	O
the	O
user	O
wants	O
to	O
see	O
the	O
updated	O
topics	O
for	O
their	O
newly	O
refined	O
anchors	O
,	O
they	O
click	O
"	O
Update	O
Topics	O
"	O
.	O
We	O
give	O
each	O
a	O
participant	O
a	O
high	O
level	O
overview	O
of	O
topic	O
modeling	O
.	O
We	O
also	O
describe	O
common	O
problems	O
with	O
topic	B-TaskName
models	I-TaskName
including	O
intruding	O
topic	O
words	O
,	O
duplicate	O
topics	O
,	O
and	O
ambiguous	O
topics	O
.	O
Users	O
are	O
instructed	O
to	O
use	O
their	O
best	O
judgement	O
to	O
determine	O
if	O
topics	O
are	O
useful	O
.	O
The	O
task	O
is	O
to	O
edit	O
the	O
anchor	O
words	O
to	O
improve	O
the	O
topics	O
.	O
We	O
asked	O
that	O
users	O
spend	O
at	O
least	O
twenty	O
minutes	O
,	O
but	O
no	O
more	O
than	O
thirty	O
minutes	O
.	O
We	O
repeat	O
the	O
task	O
twice	O
:	O
once	O
with	O
tandem	O
anchors	O
,	O
and	O
once	O
with	O
single	O
word	O
anchors	O
.	O
8	O

Tandem	O
anchors	O
extend	O
the	O
anchor	O
words	O
algorithm	O
to	O
allow	O
multiple	O
words	O
to	O
be	O
combined	O
into	O
anchor	O
facets	O
.	O
For	O
interactive	O
topic	O
modeling	O
,	O
using	O
anchor	O
facets	O
in	O
place	O
of	O
single	O
word	O
anchors	O
produces	O
higher	O
quality	O
topic	B-TaskName
models	I-TaskName
and	O
are	O
more	O
intuitive	O
to	O
use	O
.	O
Furthermore	O
,	O
our	O
approach	O
scales	O
much	O
better	O
than	O
existing	O
interactive	O
topic	O
modeling	O
techniques	O
,	O
allowing	O
interactivity	O
on	O
large	O
datasets	O
for	O
which	O
interactivity	O
was	O
previous	O
impossible	O
.	O

Separating	O
Retention	O
from	O
Extraction	O
in	O
the	O
Evaluation	O
of	O
End	O
-	O
to	O
-	O
end	O
Relation	B-TaskName
Extraction	I-TaskName

State	O
-	O
of	O
-	O
the	O
-	O
art	O
NLP	O
models	O
can	O
adopt	O
shallow	O
heuristics	O
that	O
limit	O
their	O
generalization	O
capability	O
(	O
McCoy	O
et	O
al	O
,	O
2019	O
)	O
.	O
Such	O
heuristics	O
include	O
lexical	O
overlap	O
with	O
the	O
training	O
set	O
in	O
Named	O
-	O
Entity	O
Recognition	O
(	O
Taillé	O
et	O
al	O
,	O
2020a	O
)	O
and	O
Event	O
or	O
Type	O
heuristics	O
in	O
Relation	B-TaskName
Extraction	I-TaskName
(	O
Rosenman	O
et	O
al	O
,	O
2020	O
)	O
.	O
In	O
the	O
more	O
realistic	O
end	O
-	O
to	O
-	O
end	O
RE	O
setting	O
,	O
we	O
can	O
expect	O
yet	O
another	O
heuristic	O
:	O
the	O
mere	O
retention	O
of	O
training	O
relation	O
triples	O
.	O
In	O
this	O
paper	O
we	O
propose	O
several	O
experiments	O
confirming	O
that	O
retention	O
of	O
known	O
facts	O
is	O
a	O
key	O
factor	O
of	O
performance	O
on	O
standard	O
benchmarks	O
.	O
Furthermore	O
,	O
one	O
experiment	O
suggests	O
that	O
a	O
pipeline	O
model	O
able	O
to	O
use	O
intermediate	O
type	O
representations	O
is	O
less	O
prone	O
to	O
over	O
-	O
rely	O
on	O
retention	O
.	O

Information	O
Extraction	O
(	O
IE	O
)	O
aims	O
at	O
converting	O
the	O
information	O
expressed	O
in	O
a	O
text	O
into	O
a	O
predefined	O
structured	O
format	O
of	O
knowledge	O
.	O
This	O
global	O
goal	O
has	O
been	O
divided	O
into	O
subtasks	O
easier	O
to	O
perform	O
automatically	O
and	O
evaluate	O
.	O
Hence	O
,	O
Named	B-TaskName
Entity	I-TaskName
Recognition	I-TaskName
(	O
NER	B-TaskName
)	O
and	O
Relation	B-TaskName
Extraction	I-TaskName
(	O
RE	O
)	O
are	O
two	O
key	O
IE	O
tasks	O
among	O
others	O
such	O
as	O
Coreference	B-TaskName
Resolution	I-TaskName
(	O
CR	O
)	O
,	O
Entity	B-TaskName
Linking	I-TaskName
or	O
Event	B-TaskName
Extraction	I-TaskName
.	O
Traditionally	O
performed	O
as	O
a	O
pipeline	O
(	O
Bach	O
and	O
Badaskar	O
,	O
2007	O
)	O
,	O
these	O
two	O
tasks	O
can	O
be	O
tackled	O
jointly	O
in	O
order	O
to	O
model	O
their	O
interdependency	O
,	O
alleviate	O
error	O
propagation	O
and	O
obtain	O
a	O
more	O
realistic	O
evaluation	O
setting	O
(	O
Roth	O
and	O
Yih	O
,	O
2002	O
;	O
Li	O
and	O
Ji	O
,	O
2014	O
)	O
.	O
Following	O
the	O
general	O
trend	O
in	O
Natural	O
Language	O
Processing	O
(	O
NLP	O
)	O
,	O
the	O
recent	O
quantitative	O
improvements	O
reported	O
on	O
Entity	O
and	O
Relation	B-TaskName
Extraction	I-TaskName
benchmarks	O
are	O
at	O
least	O
partly	O
explained	O
by	O
the	O
use	O
of	O
larger	O
and	O
larger	O
pretrained	B-TaskName
Language	I-TaskName
Models	I-TaskName
(	O
LMs	O
)	O
such	O
as	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
to	O
obtain	O
contextual	O
word	O
representations	O
.	O
Concurrently	O
,	O
Code	O
for	O
reproducing	O
our	O
evaluation	O
settings	O
is	O
available	O
at	O
github.com/btaille/retex	O
there	O
is	O
a	O
realization	O
that	O
new	O
evaluation	O
protocols	O
are	O
necessary	O
to	O
better	O
understand	O
the	O
strengths	O
and	O
shortcomings	O
of	O
the	O
obtained	O
neural	O
network	O
models	O
,	O
beyond	O
a	O
single	O
holistic	O
metric	O
on	O
an	O
hold	O
-	O
out	O
test	O
set	O
(	O
Ribeiro	O
et	O
al	O
,	O
2020	O
)	O
.	O
In	O
particular	O
,	O
generalisation	O
to	O
unseen	O
data	O
is	O
a	O
key	O
factor	O
in	O
the	O
evaluation	O
of	O
deep	O
neural	O
networks	O
.	O
It	O
is	O
all	O
the	O
more	O
important	O
in	O
IE	O
tasks	O
that	O
revolve	O
around	O
the	O
extraction	O
of	O
mentions	O
:	O
small	O
spans	O
of	O
words	O
that	O
are	O
likely	O
to	O
occur	O
in	O
both	O
the	O
evaluation	O
and	O
training	O
datasets	O
.	O
This	O
lexical	O
overlap	O
has	O
been	O
shown	O
to	O
be	O
correlated	O
to	O
neural	O
networks	O
performance	O
in	O
NER	B-TaskName
(	O
Augenstein	O
et	O
al	O
,	O
2017	O
;	O
Taillé	O
et	O
al	O
,	O
2020a	O
)	O
.	O
For	O
pipeline	O
RE	O
,	O
Rosenman	O
et	O
al	O
(	O
2020	O
)	O
and	O
Peng	O
et	O
al	O
(	O
2020	O
)	O
expose	O
shallow	O
heuristics	O
in	O
neural	O
models	O
:	O
relying	O
too	O
much	O
on	O
the	O
type	O
of	O
the	O
candidate	O
arguments	O
or	O
on	O
the	O
presence	O
of	O
specific	O
triggers	O
in	O
their	O
contexts	O
.	O
In	O
end	O
-	O
to	O
-	O
end	O
Relation	B-TaskName
Extraction	I-TaskName
,	O
we	O
can	O
expect	O
that	O
these	O
NER	B-TaskName
and	O
RE	O
heuristics	O
are	O
combined	O
.	O
In	O
this	O
work	O
,	O
we	O
argue	O
that	O
current	O
evaluation	O
benchmarks	O
measure	O
both	O
the	O
desired	O
ability	O
to	O
extract	O
information	O
contained	O
in	O
a	O
text	O
but	O
also	O
the	O
capacity	O
of	O
the	O
model	O
to	O
simply	O
retain	O
labeled	O
(	O
head	O
,	O
predicate	O
,	O
tail	O
)	O
triples	O
during	O
training	O
.	O
And	O
when	O
the	O
model	O
is	O
evaluated	O
on	O
a	O
sentence	O
expressing	O
a	O
relation	O
seen	O
during	O
training	O
,	O
it	O
is	O
hard	O
to	O
disentangle	O
which	O
of	O
these	O
two	O
behaviours	O
is	O
predominant	O
.	O
However	O
,	O
we	O
can	O
hypothesize	O
that	O
the	O
model	O
can	O
simply	O
retrieve	O
previously	O
seen	O
information	O
acting	O
like	O
a	O
mere	O
compressed	O
form	O
of	O
knowledge	O
base	O
probed	O
with	O
a	O
relevant	O
query	O
.	O
Thus	O
,	O
testing	O
on	O
too	O
much	O
examples	O
with	O
seen	O
triples	O
can	O
lead	O
to	O
overestimate	O
the	O
generalizability	O
of	O
a	O
model	O
.	O
Even	O
without	O
labeled	O
data	O
,	O
LMs	O
are	O
able	O
to	O
learn	O
some	O
relations	O
between	O
words	O
that	O
can	O
be	O
probed	O
with	O
cloze	O
sentences	O
where	O
an	O
argument	O
is	O
masked	O
(	O
Petroni	O
et	O
al	O
,	O
2019	O
)	O
.	O
This	O
raises	O
the	O
additional	O
question	O
of	O
lexical	O
overlap	O
with	O
the	O
orders	O
of	O
magnitude	O
larger	O
unlabeled	O
LM	O
pretraining	O
corpora	O
that	O
will	O
remain	O
out	O
of	O
scope	O
of	O
this	O
paper	O
.	O

We	O
study	O
three	O
recent	O
end	O
-	O
to	O
-	O
end	O
RE	O
models	O
on	O
CoNLL04	O
(	O
Roth	O
and	O
Yih	O
,	O
2004	O
)	O
,	O
ACE05	O
(	O
Walker	O
et	O
al	O
,	O
2006	O
)	O
and	O
SciERC	B-DatasetName
(	O
Luan	O
et	O
al	O
,	O
2018	O
)	O
.	O
They	O
rely	O
on	O
various	O
pretrained	O
LMs	O
and	O
for	O
a	O
fairer	O
comparison	O
,	O
we	O
use	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
on	O
ACE05	O
and	O
CoNLL04	O
and	O
SciBERT	O
(	O
Beltagy	O
et	O
al	O
,	O
2019	O
)	O
on	O
SciERC	B-DatasetName
1	O
.	O
PURE	O
(	O
Zhong	O
and	O
Chen	O
,	O
2021	O
)	O
follows	O
the	O
pipeline	O
approach	O
.	O
The	O
NER	B-TaskName
model	O
is	O
a	O
classical	O
span	O
-	O
based	O
model	O
(	O
Sohrab	O
and	O
Miwa	O
,	O
2018	O
)	O
.	O
Special	O
tokens	O
corresponding	O
to	O
each	O
predicted	O
entity	O
span	O
are	O
added	O
and	O
used	O
as	O
representation	O
for	O
Relation	B-TaskName
Classification	I-TaskName
.	O
For	O
a	O
fairer	O
comparison	O
with	O
other	O
models	O
,	O
we	O
study	O
the	O
approximation	O
model	O
that	O
only	O
requires	O
one	O
pass	O
in	O
each	O
encoder	O
and	O
limits	O
to	O
sentence	O
-	O
level	O
prediction	O
.	O
However	O
,	O
it	O
still	O
requires	O
finetuning	O
and	O
storing	O
two	O
pretrained	O
LMs	O
instead	O
of	O
a	O
single	O
one	O
for	O
the	O
following	O
models	O
.	O
SpERT	O
(	O
Eberts	O
and	O
Ulges	O
,	O
2020	O
)	O
uses	O
a	O
similar	O
span	O
-	O
based	O
NER	B-TaskName
module	O
.	O
RE	O
is	O
performed	O
based	O
on	O
the	O
filtered	O
representations	O
of	O
candidate	O
arguments	O
as	O
well	O
as	O
a	O
max	O
-	O
pooled	O
representation	O
of	O
their	O
middle	O
context	O
.	O
While	O
Entity	O
Filtering	O
is	O
close	O
to	O
the	O
pipeline	O
approach	O
,	O
the	O
NER	B-TaskName
and	O
RE	O
modules	O
share	O
a	O
common	O
entity	O
representation	O
and	O
are	O
trained	O
jointly	O
.	O
We	O
also	O
study	O
the	O
ablation	O
of	O
the	O
max	O
-	O
pooled	O
context	O
representation	O
that	O
we	O
denote	O
Ent	O
-	O
SpERT	O
.	O
Two	O
are	O
better	O
than	O
one	O
(	O
TABTO	O
)	O
(	O
Wang	O
and	O
Lu	O
,	O
2020	O
)	O
intertwines	O
a	O
sequence	O
encoder	O
and	O
a	O
table	O
encoder	O
in	O
a	O
Table	O
Filling	O
approach	O
(	O
Miwa	O
and	O
Sasaki	O
,	O
2014	O
)	O
.	O
Contrary	O
to	O
previous	O
models	O
the	O
pretrained	O
LM	O
is	O
frozen	O
and	O
both	O
the	O
final	O
hidden	O
states	O
and	O
attention	O
weights	O
are	O
used	O
by	O
the	O
encoders	O
.	O
The	O
prediction	O
is	O
finally	O
performed	O
by	O
a	O
Multi	O
-	O
Dimensional	O
RNN	O
(	O
MD	O
-	O
RNN	O
)	O
.	O
Because	O
it	O
is	O
not	O
based	O
on	O
span	O
-	O
level	O
predictions	O
,	O
this	O
model	O
can	O
not	O
detect	O
nested	O
entities	O
,	O
e.g.	O
on	O
SciERC	B-DatasetName
.	O

We	O
first	O
observe	O
very	O
different	O
statistics	O
of	O
Mention	O
and	O
Relation	O
Lexical	O
Overlap	O
in	O
the	O
three	O
datasets	O
,	O
which	O
can	O
be	O
explained	O
by	O
the	O
singularities	O
of	O
their	O
entities	O
and	O
relations	O
.	O
In	O
CoNLL04	O
,	O
mentions	O
are	O
mainly	O
Named	O
Entities	O
denoted	O
with	O
proper	O
names	O
while	O
in	O
ACE05	O
the	O
surface	O
forms	O
are	O
very	O
often	O
common	O
names	O
or	O
even	O
pronouns	O
,	O
which	O
explains	O
the	O
occurrence	O
of	O
training	O
entity	O
mentions	O
such	O
as	O
"	O
it	O
"	O
,	O
"	O
which	O
"	O
,	O
"	O
people	O
"	O
in	O
test	O
examples	O
.	O
This	O
also	O
leads	O
to	O
a	O
weaker	O
entity	O
label	O
consistency	O
(	O
Fu	O
et	O
al	O
,	O
2020a	O
)	O
:	O
"	O
it	O
"	O
is	O
labeled	O
with	O
every	O
possible	O
entity	O
type	O
and	O
appears	O
mostly	O
unlabeled	O
whereas	O
a	O
mention	O
such	O
as	O
"	O
President	O
Kennedy	O
"	O
is	O
always	O
labeled	O
as	O
a	O
person	O
in	O
CoNLL04	O
.	O
Similarly	O
,	O
mentions	O
in	O
SciERC	B-DatasetName
are	O
common	O
names	O
which	O
can	O
be	O
tagged	O
with	O
different	O
labels	O
and	O
they	O
can	O
also	O
be	O
nested	O
.	O
Both	O
the	O
poor	O
label	O
consistency	O
as	O
well	O
as	O
the	O
nested	O
nature	O
of	O
entities	O
hurt	O
the	O
performance	O
of	O
the	O
retention	O
heuristic	O
.	O
For	O
RE	O
,	O
while	O
SciERC	B-DatasetName
has	O
almost	O
no	O
exact	O
overlap	O
between	O
test	O
and	O
train	O
relations	O
,	O
ACE05	O
and	O
CoNLL04	O
have	O
similar	O
levels	O
of	O
exact	B-MetricName
match	I-MetricName
.	O
The	O
larger	O
proportion	O
of	O
partial	O
match	O
in	O
ACE05	O
is	O
explained	O
by	O
the	O
pronouns	O
that	O
are	O
more	O
likely	O
to	O
co	O
-	O
occur	O
in	O
several	O
instances	O
.	O
The	O
difference	O
in	O
performance	O
of	O
the	O
heuristic	O
is	O
also	O
explained	O
by	O
a	O
poor	O
relation	O
label	O
consistency	O
.	O

While	O
we	O
can	O
not	O
evaluate	O
TABTO	O
on	O
SciERC	B-DatasetName
because	O
it	O
is	O
unfit	O
for	O
extraction	O
of	O
nested	O
entities	O
,	O
we	O
can	O
notice	O
different	O
hierarchies	O
of	O
models	O
on	O
every	O
dataset	O
suggesting	O
that	O
there	O
is	O
no	O
one	O
-	O
size	O
-	O
fits	O
-	O
all	O
best	O
model	O
,	O
at	O
least	O
in	O
current	O
evaluation	O
settings	O
.	O
The	O
most	O
obvious	O
comparison	O
is	O
between	O
SpERT	O
and	O
Ent	O
-	O
SpERT	O
where	O
the	O
explicit	O
representation	O
of	O
context	O
is	O
ablated	O
.	O
This	O
results	O
in	O
a	O
loss	B-MetricName
of	O
performance	O
on	O
the	O
RE	O
part	O
and	O
especially	O
on	O
partially	O
matching	O
or	O
new	O
relations	O
for	O
which	O
the	O
entity	O
representations	O
pairs	O
have	O
not	O
been	O
seen	O
.	O
Ent	O
-	O
SpERT	O
is	O
particularly	O
effective	O
on	O
Exact	O
Matches	O
on	O
CoNLL04	O
,	O
suggesting	O
its	O
retention	O
capability	O
.	O
Other	O
comparisons	O
are	O
more	O
difficult	O
,	O
given	O
the	O
numerous	O
variations	O
between	O
the	O
very	O
structure	O
of	O
each	O
model	O
as	O
well	O
as	O
training	O
procedures	O
.	O
However	O
,	O
the	O
PURE	O
pipeline	O
setting	O
seems	O
to	O
only	O
be	O
more	O
effective	O
on	O
ACE05	O
where	O
its	O
NER	B-TaskName
performance	O
is	O
significantly	O
better	O
,	O
probably	O
because	O
learning	O
a	O
separate	O
NER	B-TaskName
and	O
RE	O
encoder	O
enables	O
to	O
learn	O
and	O
capture	O
more	O
specific	O
information	O
for	O
each	O
distinctive	O
task	O
.	O
Even	O
then	O
,	O
TABTO	O
yields	O
better	O
Boundaries	O
performance	O
only	O
penalized	O
on	O
the	O
Strict	O
setting	O
by	O
entity	O
types	O
confusions	O
.	O
On	O
the	O
contrary	O
,	O
on	O
CoNLL04	O
,	O
TABTO	O
significantly	O
outperforms	O
its	O
counterparts	O
,	O
especially	O
on	O
unseen	O
relations	O
.	O
This	O
indicates	O
that	O
it	O
proposes	O
a	O
more	O
effective	O
incorporation	O
of	O
contextual	O
information	O
in	O
this	O
case	O
where	O
relation	O
and	O
argument	O
types	O
are	O
mapped	O
bijectively	O
.	O
On	O
SciERC	B-DatasetName
,	O
performance	O
of	O
all	O
models	O
is	O
already	O
compromised	O
at	O
the	O
NER	B-TaskName
level	O
before	O
the	O
RE	O
step	O
,	O
which	O
makes	O
further	O
distinction	O
between	O
model	O
performance	O
even	O
more	O
difficult	O
.	O

Ground	O
Truth	O
Relation	O
Original	O
John	O
Wilkes	O
Booth	O
,	O
who	O
assassinated	O
President	O
Lincoln	O
,	O
was	O
an	O
actor	O
.	O
(	O
John	O
Wilkes	O
Booth	O
,	O
Kill	O
,	O
President	O
Lincoln	O
)	O
Swapped	O
President	O
Lincoln	O
,	O
who	O
assassinated	O
John	O
Wilkes	O
Booth	O
,	O
was	O
an	O
actor	O
.	O
(	O
President	O
Lincoln	O
,	O
Kill	O
,	O
John	O
Wilkes	O
Booth	O
)	O
similarly	O
to	O
what	O
is	O
proposed	O
in	O
(	O
Ribeiro	O
et	O
al	O
,	O
2020	O
)	O
.	O
We	O
propose	O
a	O
very	O
focused	O
experiment	O
that	O
consists	O
in	O
selecting	O
asymmetric	O
relations	O
that	O
occur	O
between	O
entities	O
of	O
same	O
type	O
and	O
swap	O
the	O
head	O
with	O
the	O
tail	O
in	O
the	O
input	O
.	O
If	O
the	O
model	O
predicts	O
the	O
original	O
triple	O
,	O
then	O
it	O
over	O
relies	O
on	O
the	O
retention	O
heuristic	O
,	O
whereas	O
finding	O
the	O
swapped	O
triple	O
is	O
an	O
evidence	O
of	O
broader	O
context	O
incorporation	O
.	O
We	O
show	O
an	O
example	O
in	O
Table	O
2	O
.	O
Because	O
of	O
the	O
requirements	O
of	O
this	O
experiment	O
,	O
we	O
have	O
to	O
limit	O
to	O
two	O
relations	O
in	O
CoNLL04	O
:	O
"	O
Kill	O
"	O
between	O
people	O
and	O
"	O
Located	O
in	O
"	O
between	O
locations	O
.	O
Indeed	O
,	O
CoNLL04	O
is	O
the	O
only	O
dataset	O
with	O
a	O
bijective	O
mapping	O
between	O
the	O
type	O
of	O
a	O
relation	O
and	O
the	O
types	O
of	O
its	O
arguments	O
and	O
the	O
consistent	O
proper	O
nouns	O
mentions	O
makes	O
the	O
swaps	O
mostly	O
grammatically	O
correct	O
.	O
For	O
each	O
relation	O
type	O
,	O
we	O
only	O
consider	O
sentences	O
with	O
exactly	O
one	O
instance	O
of	O
corresponding	O
relation	O
and	O
swap	O
its	O
arguments	O
.	O
We	O
only	O
consider	O
this	O
relation	O
in	O
the	O
RE	O
scores	O
reported	O
in	O
Table	O
3	O
.	O
We	O
use	O
the	O
strict	O
RE	O
score	O
as	O
well	O
as	O
revRE	O
which	O
measures	O
the	O
extraction	O
of	O
the	O
reverse	O
relation	O
,	O
not	O
expressed	O
in	O
the	O
sentence	O
.	O
For	O
each	O
relation	O
,	O
the	O
hierarchy	O
of	O
models	O
corresponds	O
to	O
the	O
overall	O
CoNLL04	O
.	O
Swapping	O
arguments	O
has	O
a	O
limited	O
effect	O
on	O
NER	B-TaskName
,	O
mostly	O
for	O
the	O
"	O
Located	O
in	O
"	O
relation	O
.	O
However	O
,	O
it	O
leads	O
to	O
a	O
drop	O
in	O
RE	O
for	O
every	O
model	O
and	O
the	O
revRE	O
score	O
indicates	O
that	O
SpERT	O
and	O
TABTO	O
predict	O
the	O
reverse	O
relation	O
more	O
often	O
than	O
the	O
newly	O
expressed	O
one	O
.	O
This	O
is	O
another	O
proof	O
of	O
the	O
retention	O
heuristic	O
of	O
end	O
-	O
to	O
-	O
end	O
models	O
,	O
although	O
it	O
might	O
also	O
be	O
attributed	O
to	O
the	O
language	O
model	O
to	O
the	O
language	O
model	O
.	O
In	O
particular	O
for	O
the	O
"	O
Located	O
in	O
"	O
relation	O
,	O
swapped	O
heads	O
and	O
tails	O
are	O
not	O
exactly	O
equivalent	O
since	O
the	O
former	O
are	O
mainly	O
cities	O
and	O
the	O
latter	O
countries	O
.	O
On	O
the	O
contrary	O
,	O
the	O
PURE	O
model	O
is	O
less	O
prone	O
to	O
information	O
retention	O
,	O
as	O
shown	O
by	O
its	O
revRE	O
scores	O
significantly	O
smaller	O
than	O
the	O
standard	O
RE	O
scores	O
on	O
swapped	O
sentences	O
.	O
Hence	O
,	O
it	O
outperforms	O
SpERT	O
and	O
TABTO	O
on	O
swapped	O
sentences	O
despite	O
being	O
the	O
least	O
effective	O
on	O
the	O
original	O
dataset	O
.	O
The	O
important	O
discrepancy	O
in	O
results	O
can	O
be	O
explained	O
by	O
the	O
different	O
types	O
of	O
representations	O
used	O
by	O
these	O
models	O
.	O
The	O
pipeline	O
approach	O
allows	O
the	O
use	O
of	O
argument	O
type	O
representations	O
in	O
the	O
Relation	O
Classifier	O
whereas	O
most	O
end	O
-	O
to	O
-	O
end	O
models	O
use	O
lexical	O
features	O
in	O
a	O
shared	O
entity	O
representation	O
used	O
for	O
both	O
NER	B-TaskName
and	O
RE	O
.	O
These	O
conclusions	O
from	O
quantitative	O
results	O
are	O
validated	O
qualitatively	O
.	O
We	O
can	O
observe	O
that	O
the	O
four	O
predominant	O
patterns	O
are	O
intuitive	O
behaviours	O
on	O
sentences	O
with	O
swapped	O
relations	O
:	O
retention	O
of	O
the	O
incorrect	O
original	O
triple	O
,	O
prediction	O
of	O
the	O
correct	O
swapped	O
triple	O
and	O
prediction	O
of	O
none	O
or	O
both	O
triples	O
.	O
We	O
report	O
some	O
examples	O
in	O
Table	O
9	O
and	O
Table	O
10	O
in	O
the	O
Appendix	O
.	O

In	O
this	O
paper	O
,	O
we	O
study	O
three	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
endto	O
-	O
end	O
Relation	B-TaskName
Extraction	I-TaskName
models	O
in	O
order	O
to	O
highlight	O
their	O
tendency	O
to	O
retain	O
seen	O
relations	O
.	O
We	O
confirm	O
that	O
retention	O
of	O
seen	O
mentions	O
and	O
relations	O
play	O
an	O
important	O
role	O
in	O
overall	O
RE	O
performance	O
and	O
can	O
explain	O
the	O
relatively	O
higher	O
scores	O
on	O
CoNLL04	O
and	O
ACE05	O
compared	O
to	O
SciERC	B-DatasetName
.	O
Furthermore	O
,	O
our	O
experiment	O
on	O
swapping	O
relation	O
heads	O
and	O
tails	O
tends	O
to	O
show	O
that	O
the	O
intermediate	O
manipulation	O
of	O
type	O
representations	O
instead	O
of	O
lexical	O
features	O
enabled	O
in	O
the	O
pipeline	O
PURE	O
model	O
makes	O
it	O
less	O
prone	O
to	O
over	O
-	O
rely	O
on	O
retention	O
.	O
While	O
the	O
limited	O
extend	O
of	O
our	O
swapping	O
experiment	O
is	O
an	O
obvious	O
limitation	O
of	O
this	O
work	O
,	O
it	O
shows	O
limitations	O
of	O
both	O
current	O
benchmarks	O
and	O
models	O
.	O
It	O
is	O
an	O
encouragement	O
to	O
propose	O
new	O
benchmarks	O
that	O
might	O
be	O
easily	O
modified	O
by	O
design	O
to	O
probe	O
such	O
lexical	O
overlap	O
heuristics	O
.	O
Contextual	O
information	O
could	O
for	O
example	O
be	O
contained	O
in	O
templates	O
of	O
that	O
would	O
be	O
filled	O
with	O
different	O
(	O
head	O
,	O
tail	O
)	O
pairs	O
either	O
seen	O
or	O
unseen	O
during	O
training	O
.	O
Furthermore	O
,	O
pretrained	B-TaskName
Language	I-TaskName
Models	I-TaskName
can	O
already	O
capture	O
relational	O
information	O
between	O
phrases	O
(	O
Petroni	O
et	O
al	O
,	O
2019	O
)	O
and	O
further	O
experiments	O
could	O
help	O
distinguish	O
their	O
role	O
in	O
the	O
retention	O
behaviour	O
of	O
RE	O
models	O
.	O

HacRED	O
:	O
A	O
Large	O
-	O
Scale	O
Relation	B-TaskName
Extraction	I-TaskName
Dataset	O
Toward	O
Hard	O
Cases	O
in	O
Practical	O
Applications	O

A	O
series	O
of	O
datasets	O
have	O
been	O
built	O
for	O
RE	O
as	O
of	O
late	O
,	O
which	O
have	O
extraordinarily	O
advanced	O
the	O
improvement	O
of	O
RE	O
systems	O
.	O
RE	O
datasets	O
such	O
as	O
SemEval	O
-	O
2010	O
Task	O
8	O
(	O
Hendrickx	O
et	O
al	O
,	O
2009	O
)	O
and	O
ACE05	O
are	O
constructed	O
through	O
human	O
annotation	O
with	O
relatively	O
limited	O
relation	O
types	O
and	O
size	O
.	O
A	O
large	O
-	O
scale	O
dataset	O
TACRED	B-DatasetName
(	O
Zhang	O
et	O
al	O
,	O
2017	O
)	O
is	O
obtained	O
via	O
crowdsourcing	O
to	O
satisfy	O
the	O
training	O
of	O
data	O
-	O
hungry	O
models	O
.	O
As	O
RE	O
applications	O
differ	O
much	O
in	O
various	O
scenarios	O
,	O
constructing	O
datasets	O
aimed	O
at	O
specific	O
targets	O
is	O
a	O
popular	O
trend	O
in	O
RE	O
.	O
DocRED	B-DatasetName
(	O
Yao	O
et	O
al	O
,	O
2019	O
)	O
is	O
constructed	O
to	O
accelerate	O
the	O
research	O
on	O
document	O
-	O
level	O
RE	O
.	O
To	O
meet	O
the	O
challenges	O
of	O
fewshot	O
RE	O
,	O
FewRel	B-DatasetName
(	O
Han	O
et	O
al	O
,	O
2018	O
)	O
as	O
well	O
as	O
FewRel	B-DatasetName
2.0	I-DatasetName
(	O
Gao	O
et	O
al	O
,	O
2019	O
)	O
have	O
been	O
presented	O
.	O
RELX	B-DatasetName
(	O
Koksal	O
and	O
Ozgur	O
,	O
2020	O
)	O
is	O
a	O
benchmark	O
for	O
cross	O
-	O
lingual	O
RE	O
.	O
Jia	O
et	O
al	O
(	O
2020	O
)	O
propose	O
the	O
task	O
of	O
interpersonal	O
RE	O
in	O
dyadic	O
dialogues	O
and	O
further	O
construct	O
a	O
corresponding	O
dataset	O
called	O
DDRel	B-DatasetName
.	O
Compared	O
with	O
previous	O
RE	O
datasets	O
,	O
HacRED	O
is	O
derived	O
from	O
the	O
analysis	O
of	O
the	O
performance	O
gap	O
between	O
popular	O
datasets	O
and	O
practical	O
applications	O
.	O
It	O
targets	O
towards	O
promoting	O
the	O
RE	O
models	O
to	O
extract	O
information	O
from	O
the	O
complex	O
contexts	O
.	O

To	O
analyze	O
where	O
models	O
struggle	O
in	O
practical	O
instances	O
and	O
distinguish	O
the	O
hard	O
cases	O
,	O
we	O
conduct	O
a	O
manual	O
exploratory	O
analysis	O
on	O
the	O
errorprone	O
instances	O
of	O
SOTA	O
models	O
(	O
CGCN	O
,	O
CasRel	O
,	O
DGCNN	B-MethodName
-	O
BERT	B-MethodName
)	O
on	O
NYT	O
,	O
DuIE	O
and	O
industry	O
data	O
.	O
Then	O
we	O
formulate	O
the	O
potential	O
causes	O
of	O
the	O
errors	O
with	O
nine	O
indicators	O
illustrated	O
as	O
follows	O
:	O
Text	O
Length	O
.	O
We	O
notice	O
that	O
models	O
tend	O
to	O
fail	O
on	O
instances	O
with	O
longer	O
text	O
.	O
The	O
experiments	O
of	O
Alt	O
et	O
al	O
(	O
2020	O
)	O
also	O
reflect	O
that	O
RE	O
models	O
get	O
a	O
relatively	O
higher	O
error	O
rate	O
with	O
the	O
length	O
of	O
sentence	O
greater	O
than	O
30	O
in	O
TACRED	B-DatasetName
.	O
Argument	O
Distance	O
.	O
We	O
observe	O
that	O
the	O
performance	O
of	O
the	O
models	O
declines	O
when	O
the	O
arguments	O
(	O
i.e.	O
,	O
head	O
and	O
tail	O
entity	O
mentions	O
)	O
are	O
far	O
away	O
,	O
especially	O
in	O
inter	O
-	O
sentence	O
RE	O
.	O
Distractors	O
.	O
Extracting	O
triples	O
in	O
contexts	O
with	O
linguistic	O
distractors	O
is	O
tough	O
for	O
current	O
models	O
.	O
For	O
example	O
,	O
drop	O
out	O
will	O
contribute	O
to	O
wrong	O
relation	O
graduate_from	O
between	O
entity	O
mentions	O
with	O
PERSON	O
and	O
SCHOOL	O
type	O
.	O
Reasoning	O
.	O
Reasoning	O
is	O
needed	O
to	O
extract	O
the	O
relation	O
mentioned	O
implicitly	O
in	O
the	O
text	O
.	O
Recent	O
work	O
suggests	O
that	O
future	O
researchers	O
consider	O
incorporating	O
common	O
sense	O
knowledge	O
or	O
improved	O
causal	O
modules	O
in	O
RE	O
tasks	O
(	O
Han	O
et	O
al	O
,	O
2018	O
)	O
.	O
Homogeneous	O
Entities	O
.	O
The	O
context	O
contains	O
multiple	O
homogeneous	O
entity	O
mentions	O
with	O
iden	O
-	O
Similar	O
Relations	O
.	O
Models	O
struggle	O
to	O
identify	O
the	O
correct	O
relation	O
among	O
those	O
semantically	O
similar	O
ones	O
concurrently	O
mentioned	O
in	O
context	O
.	O
A	O
sharp	O
decrease	O
is	O
also	O
found	O
in	O
few	O
-	O
shot	O
RE	O
when	O
selecting	O
N	O
similar	O
relations	O
on	O
N	O
-	O
way	O
K	O
-	O
shot	O
settings	O
(	O
Han	O
et	O
al	O
,	O
2020	O
)	O
.	O
Long	O
-	O
tail	O
Relations	O
.	O
Only	O
a	O
handful	O
instances	O
are	O
available	O
for	O
long	O
-	O
tail	O
relations	O
in	O
common	O
datasets	O
.	O
Current	O
data	O
-	O
hungry	O
models	O
struggle	O
to	O
learn	O
the	O
semantic	O
patterns	O
on	O
these	O
relations	O
.	O
Multiple	O
Triples	O
.	O
Models	O
always	O
get	O
a	O
poor	O
performance	O
on	O
the	O
instances	O
with	O
numerous	O
triples	O
.	O
Overlapping	O
Triples	O
.	O
Different	O
triples	O
involve	O
the	O
identical	O
entity	O
mentions	O
.	O
Many	O
existing	O
models	O
can	O
not	O
well	O
handle	O
the	O
EntityPairOverlap	O
and	O
SingleEntityOverlap	O
(	O
Zeng	O
et	O
al	O
,	O
2018	O
)	O
instances	O
.	O
Table	O
1	O
provides	O
various	O
examples	O
from	O
NYT	O
and	O
corresponding	O
hard	O
case	O
indicators	O
.	O
In	O
Table	O
2	O
,	O
the	O
proportion	O
growing	O
on	O
the	O
error	O
instances	O
reflects	O
the	O
gap	O
between	O
existing	O
datasets	O
and	O
practical	O
data	O
,	O
which	O
also	O
proves	O
the	O
effectiveness	O
of	O
these	O
indicators	O
.	O

The	O
overall	O
architecture	O
of	O
the	O
proposed	O
caseoriented	O
construction	O
framework	O
is	O
illustrated	O
in	O
Figure	O
2	O
.	O
Different	O
from	O
previous	O
works	O
(	O
Zhang	O
et	O
al	O
,	O
2017	O
,	O
Zaporojets	O
et	O
al	O
,	O
2020	O
which	O
start	O
crowdsourcing	O
annotation	O
straight	O
after	O
the	O
data	O
collection	O
stage	O
,	O
we	O
introduce	O
additional	O
stages	O
of	O
hard	O
case	O
feature	B-TaskName
engineering	I-TaskName
and	O
target	O
instance	O
prediction	O
.	O
Moreover	O
,	O
we	O
design	O
a	O
novel	O
three	O
-	O
stage	O
annotation	O
method	O
and	O
employ	O
CrowdTruth2.0	O
.	O

To	O
avoid	O
data	O
bias	O
to	O
high	O
-	O
frequency	O
entities	O
and	O
relations	O
,	O
we	O
first	O
obtain	O
about	O
5	O
million	O
plain	O
texts	O
and	O
800	O
thousand	O
triples	O
from	O
CN	O
-	O
DBpedia	B-DatasetName
.	O
The	O
abundant	O
texts	O
and	O
triples	O
contribute	O
to	O
a	O
more	O
reasonable	O
distribution	O
.	O
We	O
use	O
fine	O
-	O
grained	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
toolkit	O
TexSmart	O
(	O
Zhang	O
et	O
al	O
,	O
2020	O
)	O
and	O
entity	B-TaskName
linking	I-TaskName
(	O
Chen	O
et	O
al	O
,	O
2018	O
)	O
to	O
align	O
mentioned	O
entities	O
in	O
texts	O
to	O
those	O
in	O
triples	O
.	O
Finally	O
,	O
we	O
construct	O
a	O
distantly	O
supervised	O
dataset	O
D	O
ds	O
with	O
1.6	O
million	O
instances	O
,	O
where	O
we	O
select	O
challenging	O
instances	O
in	O
the	O
following	O
steps	O
.	O

It	O
is	O
impossible	O
to	O
manually	O
select	O
all	O
instances	O
to	O
construct	O
a	O
large	O
-	O
scale	O
dataset	O
.	O
So	O
we	O
utilize	O
a	O
classifier	O
to	O
recall	O
more	O
hard	O
cases	O
similar	O
to	O
the	O
seed	O
samples	O
selected	O
by	O
experts	O
.	O
The	O
classifiers	O
consist	O
of	O
three	O
categories	O
:	O
(	O
1	O
)	O
Decision	O
tree	O
(	O
Quinlan	O
,	O
1986	O
)	O
;	O
(	O
2	O
)	O
Deep	O
classifiers	O
by	O
positive	O
negative	O
(	O
PN	O
)	O
learning	O
(	O
Rakhlin	O
,	O
2016	O
)	O
;	O
(	O
3	O
)	O
Deep	O
classifiers	O
by	O
positive	O
unlabeled	O
(	O
PU	O
)	O
learning	O
(	O
Kiryo	O
et	O
al	O
,	O
2017	O
;	O
du	O
Plessis	O
et	O
al	O
,	O
2015	O
)	O
.	O
First	O
of	O
all	O
,	O
we	O
adopt	O
the	O
decision	O
tree	O
to	O
make	O
the	O
classifier	O
aware	O
of	O
the	O
indicators	O
explicitly	O
.	O
Then	O
,	O
we	O
form	O
the	O
representation	O
vector	O
as	O
recommended	O
in	O
Baldini	O
Soares	O
et	O
al	O
(	O
2019	O
et	O
al	O
,	O
1998	O
)	O
and	O
BiLSTM	B-MethodName
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
,	O
to	O
capture	O
the	O
context	O
information	O
.	O
More	O
training	O
details	O
can	O
be	O
found	O
in	O
Appendix	O
B.	O
We	O
ensemble	O
multiple	O
classifiers	O
by	O
weighted	O
average	O
and	O
distinguish	O
hard	O
cases	O
with	O
high	O
confidence	O
in	O
the	O
original	O
massive	O
unlabeled	O
dataset	O
.	O
Besides	O
,	O
we	O
directly	O
select	O
instances	O
by	O
implicit	O
semantic	O
patterns	O
to	O
explore	O
more	O
hard	O
cases	O
fitting	O
the	O
indicator	O
of	O
Reasoning	O
which	O
is	O
not	O
well	O
quantified	O
by	O
the	O
auxiliary	O
features	O
.	O
Finally	O
,	O
we	O
obtain	O
the	O
dataset	O
D	O
hc	O
ready	O
for	O
annotation	O
.	O

To	O
make	O
instances	O
in	O
D	O
hc	O
fully	O
and	O
accurately	O
labeled	O
,	O
we	O
develop	O
a	O
novel	O
three	O
-	O
stage	O
RE	O
annotation	O
platform	O
taking	O
the	O
following	O
two	O
aspects	O
into	O
consideration	O
:	O
(	O
1	O
)	O
Heavy	O
workload	O
of	O
annotating	O
all	O
information	O
at	O
once	O
results	O
in	O
growing	O
negative	O
feedback	O
as	O
the	O
task	O
goes	O
on	O
;	O
(	O
2	O
)	O
Aggregated	O
method	O
,	O
such	O
as	O
majority	O
vote	O
(	O
Dumitrache	O
et	O
al	O
,	O
2018	O
)	O
,	O
is	O
insufficient	O
for	O
complicated	O
and	O
openended	O
tasks	O
.	O
To	O
relieve	O
the	O
pressure	O
of	O
workers	O
,	O
we	O
divide	O
the	O
whole	O
task	O
into	O
three	O
partitions	O
consisting	O
of	O
Relation	O
Annotation	O
,	O
Entity	O
Annotation	O
,	O
and	O
Triple	O
Annotation	O
.	O
Moreover	O
,	O
we	O
utilize	O
patterns	O
and	O
toolkits	O
to	O
provide	O
high	O
-	O
quality	O
recommendations	O
in	O
each	O
stage	O
for	O
higher	O
recall	O
.	O
To	O
capture	O
the	O
label	O
disagreement	O
more	O
thoroughly	O
among	O
workers	O
,	O
we	O
employ	O
CrowdTruth2.0	O
(	O
Dumitrache	O
et	O
al	O
,	O
2018	O
)	O
,	O
which	O
models	O
the	O
quality	O
of	O
workers	O
,	O
documents	O
,	O
and	O
annotations	O
.	O
In	O
short	O
,	O
in	O
the	O
Relation	O
Annotation	O
,	O
workers	O
select	O
the	O
missed	O
relations	O
or	O
delete	O
wrong	O
recom	O
-	O
mended	O
ones	O
.	O
When	O
all	O
relations	O
are	O
annotated	O
,	O
NER	B-TaskName
toolkit	O
recommends	O
multiple	O
entity	O
mentions	O
with	O
the	O
corresponding	O
type	O
based	O
on	O
schema	O
information	O
.	O
Workers	O
also	O
need	O
to	O
append	O
new	O
entity	O
mentions	O
or	O
delete	O
incorrect	O
ones	O
in	O
the	O
Entity	O
Annotation	O
.	O
As	O
for	O
Triple	O
Annotation	O
,	O
workers	O
verify	O
the	O
correctness	O
of	O
a	O
candidate	O
triples	O
automatically	O
generated	O
by	O
permutation	O
of	O
entity	O
arguments	O
and	O
relations	O
based	O
on	O
schema	O
.	O
Note	O
that	O
every	O
input	O
data	O
in	O
the	O
three	O
stage	O
is	O
assigned	O
to	O
three	O
different	O
annotators	O
and	O
aggregated	O
by	O
CrowdTruth2.0	O
.	O
Detailed	O
annotation	O
process	O
is	O
in	O
Appendix	O
D.	O

We	O
randomly	O
select	O
200	O
contexts	O
from	O
test	O
set	O
and	O
ask	O
three	O
volunteers	O
to	O
extract	O
relational	O
facts	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
manner	O
.	O
Schema	O
information	O
like	O
entity	O
type	O
set	O
as	O
well	O
as	O
relation	O
set	O
is	O
provided	O
but	O
no	O
entity	O
mentions	O
.	O
As	O
for	O
relation	B-TaskName
classification	I-TaskName
task	O
,	O
three	O
volunteers	O
select	O
the	O
relation	O
,	O
including	O
NA	O
regarded	O
as	O
negative	O
,	O
of	O
the	O
given	O
entity	O
pair	O
.	O
As	O
demonstrated	O
in	O
Table	O
9	O
,	O
humans	O
fulfill	O
excellent	O
results	O
which	O
indicate	O
the	O
possible	O
ceiling	O
performance	O
on	O
HacRED	O
.	O

We	O
illustrate	O
the	O
three	O
-	O
stage	O
annotation	O
method	O
.	O
Given	O
the	O
context	O
in	O
Figure	O
5	O
,	O
director	O
,	O
cast_member	O
,	O
and	O
adapted_by	O
is	O
appended	O
to	O
the	O
annotation	O
of	O
Stage	O
1	O
by	O
relational	O
pattern	O
.	O
Crowdsourcing	O
workers	O
select	O
the	O
missing	O
relation	O
such	O
as	O
author	O
.	O
When	O
all	O
relation	O
mentions	O
are	O
annotated	O
,	O
NER	B-TaskName
toolkit	O
recommend	O
multiple	O
entity	O
mentions	O
with	O
the	O
corresponding	O
type	O
.	O
Workers	O
need	O
to	O
select	O
the	O
highlighted	O
words	O
that	O
are	O
not	O
covered	O
by	O
entity	O
recommendation	O
in	O
the	O
Stage	O
2	O
.	O
After	O
stage	O
2	O
,	O
all	O
mentions	O
in	O
context	O
with	O
specific	O
type	O
are	O
obtained	O
.	O
As	O
the	O
example	O
shown	O
in	O
Figure	O
5	O
,	O
given	O
the	O
target	O
entity	O
type	O
of	O
PERSON	O
,	O
platform	O
recommends	O
the	O
candidates	O
including	O
PERSON	O
-	O
1	O
to	O
PERSON	O
-	O
4	O
.	O
Workers	O
select	O
highlighted	O
words	O
PERSON	O
-	O
5	O
which	O
is	O
missed	O
.	O
In	O
the	O
final	O
stage	O
,	O
we	O
generate	O
the	O
candidate	O
triples	O
automatically	O
by	O
permutation	O
of	O
arguments	O
and	O
relations	O
based	O
on	O
triple	O
schema	O
.	O
Due	O
to	O
the	O
relation	O
director	O
connects	O
arguments	O
with	O
entity	O
type	O
PERSON	O
and	O
FILM	O
,	O
we	O
generate	O
the	O
triple	O
(	O
PERSON	O
-	O
2	O
,	O
director	O
,	O
FILM	O
)	O
and	O
ask	O
annotator	O
to	O
verify	O
the	O
correctness	O
.	O
Note	O
that	O
we	O
employ	O
the	O
powerful	O
quality	O
control	O
method	O
crowdtruth2.0	O
in	O
every	O
stages	O
to	O
prevent	O
error	O
propagation	O
.	O
As	O
a	O
result	O
,	O
all	O
triples	O
marked	O
as	O
valid	O
are	O
saved	O
.	O
E	O
Calculation	O
of	O
the	O
UQS	O
,	O
AQS	O
,	O
and	O
WQS	O
Metrics	O
in	O
CrowdTruth2.0	O
We	O
give	O
the	O
details	O
of	O
the	O
calculation	O
in	O
data	O
quality	O
evaluation	O
.	O
We	O
calculate	O
the	O
three	O
metric	O
unit	O
quality	O
score	O
(	O
UQS	O
)	O
,	O
annotation	O
quality	O
score	O
(	O
AQS	O
)	O
,	O
and	O
worker	O
quality	O
score	O
(	O
WQS	O
)	O
by	O
CrowdTruth2.0	O
(	O
Dumitrache	O
et	O
al	O
,	O
2018	O
)	O
on	O
the	O
whole	O
9	O
,	O
231	O
instances	O
in	O
HacRED	O
proposed	O
as	O
follows	O
,	O
where	O
W	O
1	O
,	O
W	O
2	O
is	O
the	O
weight	O
of	O
the	O
iteration	O
method	O
and	O
is	O
initialized	O
as	O
one	O
,	O
u	O
is	O
the	O
unit	O
for	O
annotation	O
,	O
a	O
is	O
one	O
annotation	O
given	O
a	O
unit	O
,	O
i	O
,	O
j	O
denotes	O
the	O
different	O
workers	O
.	O
We	O
straightforward	O
report	O
the	O
average	O
of	O
these	O
metrics	O
in	O
Section	O
5.1	O
.	O
U	O
QS	O
(	O
u	O
)	O
=	O
∑	O
i	O
,	O
j	O
W1	O
(	O
i	O
,	O
j	O
,	O
u	O
)	O
W	O
QS	O
(	O
i	O
)	O
W	O
QS	O
(	O
j	O
)	O
∑	O
i	O
,	O
j	O
W	O
QS	O
(	O
i	O
)	O
W	O
QS	O
(	O
j	O
)	O
(	O
2	O
)	O
AQS	O
(	O
a	O
)	O
=	O
∑	O
i	O
,	O
j	O
W	O
QS	O
(	O
i	O
)	O
W	O
QS	O
(	O
j	O
)	O
Pa	O
(	O
i	O
|	O
j	O
)	O
∑	O
i	O
,	O
j	O
W	O
QS	O
(	O
i	O
)	O
W	O
QS	O
(	O
j	O
)	O
(	O
3	O
)	O
W1	O
(	O
i	O
,	O
j	O
,	O
u	O
)	O
W	O
QS	O
(	O
j	O
)	O
U	O
QS	O
(	O
u	O
)	O
∑	O
j	O
,	O
u	O
W	O
QS	O
(	O
j	O
)	O
U	O
QS	O
(	O
u	O
)	O
W	O
QS	O
(	O
i	O
)	O
=	O
W	O
U	O
A	O
(	O
i	O
)	O
W	O
W	O
A	O
(	O
i	O
)	O
W	O
U	O
A	O
(	O
i	O
)	O
=	O
∑	O
u	O
W2	O
(	O
u	O
,	O
i	O
)	O
U	O
QS	O
(	O
u	O
)	O
∑	O
u	O
U	O
QS	O
(	O
u	O
)	O
W	O
W	O
A	O
(	O
i	O
)	O
=	O
∑	O
j	O
,	O
u	O
(	O
4	O
)	O

Task	O
-	O
oriented	O
dialog	O
systems	O
are	O
becoming	O
pervasive	O
,	O
and	O
many	O
companies	O
heavily	O
rely	O
on	O
them	O
to	O
complement	O
human	O
agents	O
for	O
customer	O
service	O
in	O
call	O
centers	O
.	O
With	O
globalization	O
,	O
the	O
need	O
for	O
providing	O
cross	O
-	O
lingual	O
customer	O
support	O
becomes	O
more	O
urgent	O
than	O
ever	O
.	O
However	O
,	O
cross	O
-	O
lingual	O
support	O
poses	O
great	O
challenges	O
-	O
it	O
requires	O
a	O
large	O
amount	O
of	O
additional	O
annotated	O
data	O
from	O
native	O
speakers	O
.	O
In	O
order	O
to	O
bypass	O
the	O
expensive	O
human	O
annotation	O
and	O
achieve	O
the	O
first	O
step	O
towards	O
the	O
ultimate	O
goal	O
of	O
building	O
a	O
universal	O
dialog	O
system	O
,	O
we	O
set	O
out	O
to	O
build	O
a	O
cross	O
-	O
lingual	O
state	O
tracking	O
framework	O
.	O
Specifically	O
,	O
we	O
assume	O
that	O
there	O
exists	O
a	O
source	O
language	O
with	O
dialog	O
belief	O
tracking	O
annotations	O
while	O
the	O
target	O
languages	O
have	O
no	O
annotated	O
dialog	O
data	O
of	O
any	O
form	O
.	O
Then	O
,	O
we	O
pre	O
-	O
train	O
a	O
state	O
tracker	O
for	O
the	O
source	O
language	O
as	O
a	O
teacher	O
,	O
which	O
is	O
able	O
to	O
exploit	O
easy	O
-	O
to	O
-	O
access	O
parallel	O
data	O
.	O
We	O
then	O
distill	O
and	O
transfer	O
its	O
own	O
knowledge	O
to	O
the	O
student	O
state	O
tracker	O
in	O
target	O
languages	O
.	O
We	O
specifically	O
discuss	O
two	O
types	O
of	O
common	O
parallel	O
resources	O
:	O
bilingual	O
corpus	O
and	O
bilingual	O
dictionary	O
,	O
and	O
design	O
different	O
transfer	B-TaskName
learning	I-TaskName
strategies	O
accordingly	O
.	O
Experimentally	O
,	O
we	O
successfully	O
use	O
English	O
state	O
tracker	O
as	O
the	O
teacher	O
to	O
transfer	O
its	O
knowledge	O
to	O
both	O
Italian	O
and	O
German	O
trackers	O
and	O
achieve	O
promising	O
results	O
.	O

Over	O
the	O
past	O
few	O
years	O
,	O
we	O
have	O
witnessed	O
the	O
burgeoning	O
of	O
real	O
-	O
world	O
applications	O
of	O
dialog	O
systems	O
,	O
with	O
many	O
academic	O
,	O
industrial	O
,	O
and	O
startup	O
efforts	O
racing	O
to	O
lead	O
the	O
widely	O
-	O
believed	O
next	O
-	O
generation	O
human	O
-	O
machine	O
interfaces	O
.	O
As	O
a	O
result	O
,	O
numerous	O
task	O
-	O
oriented	O
dialog	O
systems	O
such	O
as	O
virtual	O
assistants	O
and	O
customer	O
conversation	O
services	O
were	O
developed	O
Rojas	O
-	O
Barahona	O
et	O
al	O
,	O
2017	O
;	O
Bordes	O
and	O
Weston	O
,	O
2017	O
;	O
Williams	O
et	O
al	O
,	O
2017	O
;	O
Li	O
et	O
al	O
,	O
2017	O
)	O
,	O
with	O
Google	B-DatasetName
Duplexbeing	O
the	O
most	O
recent	O
example	O
.	O
With	O
the	O
rapid	O
process	O
of	O
globalization	O
,	O
more	O
countries	O
have	O
observed	O
growing	O
populations	O
of	O
immigrants	O
,	O
and	O
more	O
companies	O
have	O
moved	O
forward	O
to	O
develop	O
their	O
overseas	O
business	O
sectors	O
.	O
To	O
provide	O
better	O
customer	O
service	O
and	O
bring	O
down	O
the	O
cost	O
of	O
labor	O
at	O
call	O
centers	O
,	O
the	O
development	O
of	O
universal	O
dialog	O
systems	O
has	O
become	O
a	O
practical	O
issue	O
.	O
A	O
straightforward	O
strategy	O
is	O
to	O
separately	O
collect	O
training	O
data	O
and	O
train	O
dialog	O
systems	O
for	O
each	O
language	O
.	O
However	O
,	O
it	O
is	O
not	O
only	O
tedious	O
but	O
also	O
expensive	O
.	O
Two	O
settings	O
naturally	O
arise	O
for	O
more	O
efficient	O
usage	O
of	O
the	O
training	O
data	O
:	O
(	O
1	O
)	O
Multi	O
-	O
lingual	O
setting	O
:	O
we	O
annotate	O
data	O
for	O
multiple	O
languages	O
and	O
train	O
a	O
single	O
model	O
,	O
with	O
possible	O
innovations	O
on	O
joint	O
training	O
.	O
(	O
2	O
)	O
Crosslingual	O
setting	O
:	O
we	O
annotate	O
data	O
and	O
train	O
a	O
model	O
for	O
only	O
one	O
(	O
popular	O
)	O
language	O
,	O
and	O
transfer	O
the	O
learned	O
knowledge	O
to	O
other	O
languages	O
.	O
Here	O
we	O
are	O
interested	O
in	O
the	O
second	O
case	O
,	O
and	O
the	O
important	O
research	O
question	O
we	O
ask	O
is	O
:	O
How	O
can	O
we	O
build	O
cross	O
-	O
lingual	O
dialog	O
systems	O
that	O
can	O
support	O
less	O
popular	O
,	O
low	O
-	O
or	O
even	O
zero	O
-	O
resource	O
languages	O
?	O
As	O
an	O
initial	O
step	O
towards	O
cross	O
-	O
lingual	O
dialog	O
systems	O
,	O
we	O
focus	O
on	O
the	O
cornerstone	O
of	O
dialog	O
systems	O
-	O
dialog	O
state	O
tracking	O
(	O
DST	O
)	O
,	O
or	O
belief	O
tracking	O
,	O
a	O
key	O
component	O
for	O
understanding	O
user	O
inputs	O
and	O
updating	O
belief	O
state	O
,	O
i.e.	O
,	O
a	O
system	O
's	O
internal	O
representation	O
of	O
the	O
state	O
of	O
conversation	O
(	O
Young	O
et	O
al	O
,	O
2010	O
)	O
.	O
Based	O
on	O
the	O
perceived	O
belief	O
state	O
,	O
the	O
dialog	O
manager	O
can	O
decide	O
which	O
action	O
to	O
take	O
,	O
and	O
what	O
verbal	O
response	O
to	O
generate	O
(	O
Precup	O
and	O
Teh	O
,	O
2017	O
;	O
Bordes	O
and	O
Weston	O
,	O
2017	O
)	O
.	O
DST	O
models	O
require	O
a	O
considerable	O
amount	O
of	O
annotated	O
data	O
for	O
training	O
(	O
Henderson	O
et	O
al	O
,	O
2014b	O
;	O
.	O
For	O
a	O
common	O
dialog	O
shown	O
in	O
Figure	O
1	O
,	O
a	O
typical	O
data	O
acquisition	O
process	O
(	O
Rojas	O
-	O
Barahona	O
et	O
al	O
,	O
2017	O
)	O
not	O
only	O
requires	O
two	O
human	O
users	O
to	O
converse	B-DatasetName
for	O
multiple	O
turns	O
but	O
also	O
requires	O
annotators	O
to	O
identify	O
user	O
's	O
intention	O
in	O
each	O
turn	O
.	O
Such	O
two	O
-	O
step	O
annotation	O
is	O
very	O
expensive	O
,	O
especially	O
for	O
rare	O
languages	O
.	O
We	O
study	O
the	O
novel	O
problem	O
of	O
cross	O
-	O
lingual	O
DST	O
,	O
where	O
one	O
leverages	O
the	O
annotated	O
data	O
of	O
a	O
source	O
language	O
to	O
train	O
DST	O
for	O
a	O
target	O
language	O
with	O
zero	O
annotated	O
data	O
(	O
Figure	O
1	O
)	O
;	O
no	O
conversation	O
dialog	O
or	O
dialog	O
state	O
annotation	O
is	O
available	O
for	O
the	O
target	O
language	O
.	O
In	O
order	O
to	O
deal	O
with	O
this	O
zero	O
-	O
resource	O
challenging	O
scenario	O
,	O
we	O
first	O
decouple	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
neural	O
belief	O
tracker	O
framework	O
into	O
sub	O
-	O
modules	O
,	O
namely	O
utterance	O
encoder	O
,	O
context	O
gate	O
,	O
and	O
slotvalue	O
decoder	O
.	O
By	O
introducing	O
a	O
teacher	O
-	O
student	O
framework	O
,	O
we	O
are	O
able	O
to	O
transfer	O
knowledge	O
across	O
languages	O
module	O
by	O
module	O
,	O
following	O
the	O
divide	O
-	O
and	O
-	O
conquer	O
philosophy	O
.	O
Requiring	O
no	O
target	O
-	O
side	O
dialog	O
data	O
,	O
our	O
method	O
relies	O
on	O
other	O
easy	O
-	O
to	O
-	O
access	O
parallel	O
resources	O
to	O
understand	O
the	O
connection	O
between	O
languages	O
.	O
Depending	O
on	O
the	O
popularity	O
and	O
availability	O
of	O
target	O
language	O
resources	O
,	O
we	O
study	O
two	O
kinds	O
of	O
parallel	O
data	O
:	O
bilingual	O
corpus	O
and	O
bilingual	O
dictionary	O
,	O
and	O
we	O
respectively	O
design	O
two	O
transfer	B-TaskName
learning	I-TaskName
strategies	O
.	O
We	O
use	O
the	O
popular	O
Wizard	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	I-DatasetName
Oz	I-DatasetName
(	O
Rojas	O
-	O
Barahona	O
et	O
al	O
,	O
2017	O
)	O
dataset	O
as	O
our	O
DST	O
benchmark	O
to	O
evaluate	O
the	O
effectiveness	O
of	O
our	O
crosslingual	O
transfer	B-TaskName
learning	I-TaskName
.	O
We	O
specify	O
English	O
as	O
the	O
source	O
(	O
primary	O
)	O
language	O
and	O
two	O
different	O
European	O
languages	O
(	O
German	O
and	O
Italian	O
)	O
as	O
our	O
zero	O
-	O
annotation	O
target	O
languages	O
.	O
Compared	O
with	O
an	O
array	O
of	O
alternative	O
transfer	B-TaskName
learning	I-TaskName
strategies	O
,	O
our	O
cross	O
-	O
lingual	O
DST	O
models	O
consistently	O
achieve	O
promising	O
results	O
in	O
both	O
scenarios	O
for	O
both	O
zero	O
-	O
annotation	O
languages	O
.	O
To	O
ensure	O
reproducibility	O
,	O
we	O
release	O
our	O
code	O
,	O
training	O
data	O
and	O
parallel	O
resources	O
in	O
the	O
github	O
1	O
.	O
Our	O
main	O
contributions	O
are	O
three	O
-	O
fold	O
:	O
Towards	O
building	O
cross	O
-	O
lingual	O
dialog	O
systems	O
,	O
we	O
are	O
the	O
first	O
to	O
study	O
the	O
crosslingual	O
dialog	O
state	O
tracking	O
problem	O
.	O
We	O
systematically	O
study	O
different	O
scenarios	O
for	O
this	O
problem	O
based	O
on	O
the	O
availability	O
of	O
parallel	O
data	O
and	O
propose	O
novel	O
transfer	B-TaskName
learning	I-TaskName
methods	O
to	O
tackle	O
the	O
problem	O
.	O
We	O
empirically	O
demonstrate	O
the	O
efficacy	O
of	O
the	O
proposed	O
methods	O
,	O
showing	O
that	O
our	O
methods	O
can	O
accurately	O
track	O
dialog	O
states	O
for	O
1	O
https://github.com/wenhuchen/	O
Cross	O
-	O
Lingual	O
-	O
NBT	O
languages	O
with	O
zero	O
annotated	O
data	O
.	O

Cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
transfer	I-TaskName
learning	O
has	O
been	O
a	O
very	O
popular	O
topic	O
during	O
the	O
years	O
,	O
which	O
can	O
be	O
seen	O
as	O
a	O
transductive	O
process	O
.	O
In	O
such	O
process	O
,	O
the	O
input	O
domains	O
of	O
the	O
source	O
and	O
target	O
are	O
different	O
(	O
Pan	O
and	O
Yang	O
,	O
2010	O
)	O
since	O
each	O
language	O
has	O
its	O
own	O
distinct	O
lexicon	O
.	O
By	O
discovering	O
the	O
underlying	O
connections	O
between	O
the	O
source	O
and	O
target	O
domain	O
,	O
we	O
could	O
design	O
transfer	O
algorithms	O
for	O
different	O
tasks	O
.	O
Recently	O
,	O
algorithms	O
have	O
been	O
successfully	O
designed	O
for	O
POS	O
tagging	O
(	O
Zhang	O
et	O
al	O
,	O
2016	O
;	O
,	O
NER	B-TaskName
(	O
Pan	O
et	O
al	O
,	O
2017	O
;	O
Ni	O
et	O
al	O
,	O
2017	O
)	O
as	O
well	O
as	O
image	B-TaskName
captioning	I-TaskName
(	O
Miyazaki	O
and	O
Shimizu	O
,	O
2016	O
)	O
.	O
These	O
methods	O
first	O
aim	O
at	O
discovering	O
the	O
relatedness	O
between	O
two	O
languages	O
and	O
separate	O
languagecommon	O
modules	O
from	O
language	O
-	O
specific	O
modules	O
,	O
then	O
resort	O
to	O
external	O
resources	O
to	O
transfer	O
the	O
knowledge	O
across	O
the	O
language	O
boundary	O
.	O
Our	O
method	O
addresses	O
the	O
transfer	B-TaskName
learning	I-TaskName
using	O
a	O
teacher	O
-	O
student	O
framework	O
and	O
proposes	O
to	O
use	O
the	O
teacher	O
to	O
gradually	O
guide	O
the	O
student	O
to	O
make	O
more	O
proper	O
decisions	O
.	O
The	O
dialog	O
states	O
are	O
defined	O
as	O
a	O
set	O
of	O
search	O
constraints	O
(	O
i.e.	O
informable	O
slots	O
or	O
goals	O
)	O
that	O
the	O
user	O
specified	O
through	O
the	O
dialog	O
and	O
a	O
set	O
of	O
attribute	O
questions	O
regarding	O
the	O
search	O
results	O
(	O
i.e.	O
requestable	O
slots	O
or	O
requests	O
)	O
.	O
The	O
objective	O
of	O
dialog	O
state	O
tracking	O
(	O
DST	O
)	O
is	O
to	O
predict	O
and	O
track	O
the	O
user	O
intention	O
(	O
i.e.	O
,	O
the	O
values	O
of	O
the	O
aforementioned	O
slots	O
)	O
at	O
each	O
time	O
step	O
based	O
on	O
the	O
current	O
user	O
utterance	O
and	O
the	O
entire	O
dialog	O
history	O
.	O
As	O
shown	O
in	O
Figure	O
2	O
,	O
for	O
each	O
slot	O
,	O
the	O
DST	O
computes	O
an	O
output	O
distribution	O
of	O
the	O
candidate	O
values	O
using	O
three	O
inputs	O
:	O
(	O
i	O
)	O
system	O
response	O
a	O
t	O
,	O
which	O
is	O
the	O
sentence	O
generated	O
by	O
the	O
system	O
,	O
(	O
ii	O
)	O
utterance	O
u	O
t	O
,	O
which	O
is	O
the	O
sentence	O
from	O
the	O
user	O
,	O
and	O
(	O
iii	O
)	O
previous	O
state	O
,	O
which	O
denotes	O
the	O
selected	O
slot	O
-	O
value	O
pairs	O
.	O
We	O
define	O
the	O
ontology	B-MethodName
of	O
the	O
dialog	O
system	O
to	O
be	O
the	O
set	O
of	O
all	O
the	O
possible	O
words	O
the	O
dialog	O
slot	O
and	O
value	O
can	O
take	O
.	O
In	O
this	O
paper	O
,	O
we	O
are	O
interested	O
in	O
learning	O
a	O
cross	O
-	O
lingual	O
DST	O
.	O
Specifically	O
,	O
we	O
assume	O
that	O
the	O
DST	O
for	O
the	O
source	O
language	O
has	O
access	O
to	O
a	O
human	O
-	O
annotated	O
training	O
dataset	O
D	O
while	O
the	O
DSTs	O
for	O
the	O
target	O
languages	O
do	O
not	O
have	O
access	O
to	O
annotated	O
data	O
in	O
other	O
languages	O
except	O
for	O
testing	O
data	O
.	O
We	O
here	O
mainly	O
consider	O
two	O
different	O
types	O
of	O
parallel	O
resources	O
to	O
assist	O
the	O
transfer	B-TaskName
learning	I-TaskName
:	O
(	O
1	O
)	O
Bilingual	O
Corpus	O
,	O
where	O
abundant	O
bilingual	O
corpora	O
exist	O
between	O
the	O
source	O
and	O
the	O
target	O
languages	O
.	O
This	O
is	O
often	O
the	O
case	O
for	O
common	O
language	O
pairs	O
like	O
German	O
,	O
Italian	O
,	O
and	O
French	O
,	O
etc	O
.	O
(	O
2	O
)	O
Bilingual	O
Dictionary	O
,	O
where	O
public	O
bilingual	O
dictionaries	O
exist	O
between	O
the	O
source	O
and	O
the	O
target	O
languages	O
,	O
but	O
large	O
-	O
scaled	O
parallel	O
corpus	O
are	O
harder	O
to	O
obtain	O
.	O
This	O
can	O
be	O
the	O
case	O
for	O
rarer	O
languages	O
like	O
Finnish	O
,	O
Bulgarian	O
,	O
etc	O
.	O
Furthermore	O
,	O
we	O
assume	O
that	O
all	O
the	O
languages	O
share	O
a	O
common	O
multi	O
-	O
lingual	O
database	O
,	O
whose	O
column	O
/	O
row	O
names	O
and	O
entry	O
values	O
are	O
stored	O
via	O
multiple	O
languages	O
(	O
see	O
the	O
database	O
in	O
Figure	O
1	O
)	O
.	O
That	O
is	O
,	O
the	O
ontology	B-MethodName
of	O
dialog	O
among	O
different	O
languages	O
is	O
known	O
with	O
a	O
one	O
-	O
to	O
-	O
one	O
mapping	O
between	O
them	O
(	O
e.g.	O
,	O
greek	O
=	O
griechisch	O
=	O
greco	O
,	O
food	O
=	O
essen	O
=	O
cibo	O
)	O
.	O
Based	O
on	O
that	O
,	O
we	O
could	O
construct	O
a	O
mapping	O
function	O
M	O
to	O
associate	O
the	O
ontology	B-MethodName
terms	O
from	O
different	O
languages	O
with	O
predesigned	O
language	O
-	O
agnostic	O
concepts	O
:	O
for	O
exam	O
-	O
ple	O
,	O
M	O
(	O
f	O
oods	O
)	O
=	O
M	O
(	O
Essen	O
)	O
=	O
M	O
(	O
Cibo	O
)	O
=	O
FOOD	O
.	O
We	O
illustrate	O
our	O
problem	O
definition	O
in	O
Fig	O
-	O
ure	O
2	O
.	O

The	O
second	O
part	O
is	O
the	O
context	O
gate	O
,	O
which	O
takes	O
the	O
system	O
acts	O
a	O
t	O
=	O
(	O
t	O
q	O
,	O
t	O
s	O
,	O
t	O
v	O
)	O
2	O
tq	O
represents	O
the	O
system	O
request	O
,	O
ts	B-MethodName
,	O
tv	O
represents	O
the	O
system	O
confirmation	O
.	O
If	O
the	O
system	O
wants	O
to	O
request	O
some	O
information	O
from	O
the	O
user	O
by	O
asking	O
"	O
what	O
's	O
your	O
favorite	O
area	O
?	O
"	O
,	O
then	O
NBT	O
sets	O
tq="AREA	O
"	O
.	O
If	O
the	O
system	O
wants	O
to	O
confirm	O
some	O
information	O
from	O
a	O
user	O
by	O
asking	O
"	O
should	O
I	O
try	O
Persian	O
restaurants	O
in	O
the	O
north	O
?	O
"	O
then	O
NBT	O
sets	O
ts	B-MethodName
,	O
tv="area	O
,	O
north	O
"	O
.	O
and	O
the	O
candidate	O
slot	O
-	O
value	O
pair	O
(	O
c	O
s	O
,	O
c	O
v	O
)	O
as	O
its	O
inputs	O
and	O
filter	O
out	O
the	O
desired	O
information	O
from	O
the	O
encoded	O
utterance	O
.	O
The	O
context	O
gate	O
g	O
is	O
a	O
sum	O
of	O
three	O
separate	O
gates	O
:	O
g	O
(	O
c	O
s	O
,	O
c	O
v	O
,	O
a	O
t	O
)	O
=	O
g	O
1	O
+	O
g	O
2	O
+	O
g	O
3	O
(	O
1	O
)	O
where	O
the	O
individual	O
gates	O
are	O
defined	O
as	O
:	O
g	O
1	O
=	O
σ	O
(	O
W	O
s	O
c	O
(	O
c	O
s	O
+	O
c	O
v	O
)	O
+	O
b	O
s	O
c	O
)	O
g	O
2	O
=	O
(	O
c	O
s	O
W	O
q	O
t	O
t	O
q	O
)	O
[	O
1	O
,	O
,	O
1	O
]	O
H	O
g	O
3	O
=	O
(	O
c	O
s	O
W	O
s	O
t	O
t	O
s	O
)	O
(	O
c	O
v	O
W	O
v	O
t	O
t	O
v	O
)	O
[	O
1	O
,	O
,	O
1	O
]	O
H	O
(	O
2	O
)	O
where	O
W	O
s	O
c	O
,	O
W	O
q	O
t	O
,	O
W	O
s	O
t	O
,	O
W	O
v	O
t	O
R	O
H×H	O
are	O
the	O
weight	O
matrices	O
,	O
and	O
and	O
denote	O
the	O
Hadamard	O
product	O
and	O
the	O
inner	O
product	O
,	O
respectively	O
.	O
The	O
three	O
gates	O
g	O
1	O
R	O
H	O
,	O
g	O
2	O
R	O
H	O
,	O
g	O
3	O
R	O
H	O
model	O
the	O
relevance	O
between	O
the	O
candidate	O
slot	O
and	O
value	O
,	O
the	O
system	O
request	O
and	O
the	O
system	O
confirms	O
,	O
respectively	O
.	O
The	O
transformation	O
matrices	O
W	O
q	O
t	O
,	O
W	O
s	O
t	O
,	O
W	O
v	O
t	O
are	O
added	O
to	O
the	O
original	O
NBT	O
to	O
increase	O
the	O
model	O
flexibility	O
of	O
the	O
gates	O
.	O

In	O
this	O
section	O
,	O
we	O
develop	O
a	O
cross	O
-	O
lingual	O
Neural	O
Belief	O
Tracker	O
(	O
XL	O
-	O
NBT	O
)	O
that	O
distills	O
knowledge	O
from	O
one	O
NBT	O
to	O
another	O
using	O
a	O
teacherstudent	O
framework	O
.	O
We	O
assume	O
the	O
ontology	B-MethodName
mapping	O
M	O
is	O
known	O
a	O
priori	O
(	O
see	O
Figure	O
3	O
)	O
.	O
XL	O
-	O
NBT	O
uses	O
language	O
-	O
specific	O
utterance	O
encoder	O
and	O
context	O
gate	O
for	O
each	O
input	O
language	O
while	O
sharing	O
a	O
common	O
(	O
language	O
-	O
agnostic	O
)	O
slot	O
-	O
value	O
decoder	O
across	O
different	O
languages	O
(	O
see	O
Figure	O
3	O
)	O
.	O
The	O
key	O
idea	O
is	O
to	O
optimize	O
the	O
language	O
-	O
specific	O
components	O
of	O
the	O
student	O
network	O
(	O
NBT	O
of	O
the	O
target	O
language	O
)	O
so	O
that	O
their	O
outputs	O
are	O
languageagnostic	O
.	O
This	O
is	O
achieved	O
by	O
making	O
these	O
outputs	O
close	O
to	O
that	O
of	O
the	O
teacher	O
network	O
(	O
NBT	O
of	O
the	O
source	O
language	O
)	O
,	O
as	O
we	O
detail	O
below	O
.	O

The	O
Wizard	O
of	O
Oz	O
(	O
WOZ	O
)	O
(	O
Rojas	O
-	O
Barahona	O
et	O
al	O
,	O
2017	O
)	O
dataset	O
is	O
used	O
for	O
training	O
and	O
evaluation	O
,	O
which	O
consists	O
of	O
user	O
conversations	O
with	O
taskoriented	O
dialog	O
systems	O
designed	O
to	O
help	O
users	O
find	O
suitable	O
restaurants	O
around	O
Cambridge	B-DatasetName
,	O
UK	O
.	O
The	O
corpus	O
contains	O
three	O
informable	O
(	O
i.e.	O
goaltracking	O
)	O
slots	O
:	O
FOOD	O
,	O
AREA	O
,	O
and	O
PRICE	O
.	O
The	O
users	O
can	O
specify	O
values	O
for	O
these	O
slots	O
in	O
order	O
to	O
find	O
which	O
best	O
meet	O
their	O
criteria	O
.	O
Once	O
the	O
system	O
suggests	O
a	O
restaurant	O
,	O
the	O
users	O
can	O
ask	O
about	O
the	O
values	O
of	O
up	O
to	O
eight	O
requestable	O
slots	O
(	O
PHONE	O
NUMBER	O
,	O
ADDRESS	O
,	O
etc	O
.	O
)	O
.	O
Multilingual	O
WOZ	O
2.0	O
has	O
expanded	O
this	O
dataset	O
to	O
include	O
more	O
dialogs	O
and	O
more	O
languages	O
.	O
The	O
train	O
,	O
valid	O
and	O
test	O
datasets	O
for	O
three	O
different	O
languages	O
(	O
English	O
,	O
German	O
,	O
Italian	O
)	O
are	O
available	O
online	O
3	O
.	O
We	O
use	O
the	O
English	O
as	O
source	O
language	O
where	O
600	O
dialogs	O
are	O
used	O
for	O
training	O
,	O
200	O
for	O
validation	O
and	O
400	O
for	O
testing	O
.	O
We	O
use	O
the	O
German	O
and	O
Italian	O
as	O
the	O
target	O
language	O
to	O
transfer	O
our	O
knowledge	O
from	O
English	O
DST	O
system	O
.	O
In	O
the	O
experiments	O
,	O
we	O
do	O
not	O
have	O
access	O
to	O
any	O
training	O
or	O
validation	B-DatasetName
dataset	I-DatasetName
for	O
German	O
and	O
Italian	O
,	O
and	O
we	O
only	O
have	O
access	O
to	O
their	O
testing	O
dataset	O
which	O
is	O
composed	O
of	O
400	O
dialogs	O
.	O
For	O
external	O
resource	O
,	O
we	O
use	O
the	O
IWSLT2014	O
Ted	O
Talk	O
parallel	O
corpus	O
(	O
Mauro	O
et	O
al	O
,	O
2012	O
)	O
from	O
the	O
official	O
website	O
4	O
for	O
bilingual	O
corpus	O
scenario	O
.	O
In	O
the	O
IWSLT2014	O
parallel	O
corpus	O
,	O
we	O
only	O
keep	O
the	O
sentences	O
between	O
4	O
and	O
40	O
words	O
and	O
decrease	O
the	O
sentence	O
pairs	O
to	O
around	O
150K.	O
We	O
use	O
Panlex	B-DatasetName
(	O
Kamholz	O
et	O
al	O
,	O
2014	O
)	O
as	O
our	O
data	O
source	O
and	O
crawl	O
translations	O
for	O
all	O
the	O
words	O
appearing	O
in	O
the	O
dialog	O
datasets	O
to	O
build	O
our	O
bilingual	O
dictionary	O
.	O
We	O
specifically	O
investigate	O
two	O
kinds	O
of	O
pretrained	O
embedding	O
,	O
and	O
we	O
use	O
Glove	O
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
as	O
the	O
monolingual	O
embedding	O
and	O
MUSE	B-DatasetName
(	O
Conneau	O
et	O
al	O
,	O
2017	O
)	O
as	O
the	O
bilingual	O
embedding	O
to	O
see	O
their	O
impacts	O
on	O
the	O
DST	O
performance	O
.	O
We	O
split	O
the	O
raw	O
DST	O
corpus	O
into	O
turn	O
-	O
level	O
examples	O
.	O
During	O
training	O
,	O
we	O
use	O
the	O
ground	O
truth	O
previous	O
state	O
V	O
t−1	O
as	O
inputs	O
.	O
At	O
test	O
time	O
,	O
we	O
use	O
the	O
model	O
searched	O
states	O
as	O
the	O
previous	O
state	O
to	O
continue	O
tracking	O
intention	O
until	O
the	O
end	O
of	O
the	O
dialog	O
.	O
When	O
the	O
dialog	O
terminates	O
,	O
we	O
use	O
two	O
evaluation	O
metrics	O
introduced	O
in	O
Henderson	O
et	O
al	O
(	O
2014a	O
)	O
to	O
evaluate	O
the	O
DST	O
performance	O
:	O
(	O
1	O
)	O
Goals	O
:	O
the	O
proportion	O
of	O
dialog	O
turns	O
where	O
all	O
the	O
users	O
search	O
goal	O
constraints	O
were	O
correctly	O
identified	O
.	O
(	O
2	O
)	O
Requests	O
:	O
similarly	O
,	O
the	O
proportion	O
of	O
dialog	O
turns	O
where	O
users	O
requests	O
for	O
information	O
were	O
identified	O
correctly	O
.	O
Our	O
implementation	O
is	O
based	O
on	O
the	O
NBT	O
5	O
,	O
the	O
details	O
of	O
our	O
system	O
setting	O
are	O
described	O
in	O
the	O
appendix	O
.	O

In	O
our	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
teacher	O
-	O
student	O
framework	O
to	O
perform	O
cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
transfer	I-TaskName
learning	O
for	O
DST	O
.	O
The	O
key	O
idea	O
of	O
our	O
model	O
is	O
to	O
decouple	O
the	O
current	O
DST	O
neural	O
network	O
into	O
two	O
separate	O
modules	O
and	O
transfer	O
them	O
separately	O
.	O
We	O
believe	O
our	O
method	O
can	O
be	O
further	O
extended	O
into	O
a	O
general	O
purpose	O
multi	O
-	O
lingual	O
transfer	O
framework	O
to	O
resolve	O
other	O
NLP	O
matching	O
or	O
classification	O
problems	O
.	O

We	O
are	O
gratefully	O
supported	O
by	O
a	O
Tencent	O
AI	O
Lab	O
Rhino	O
-	O
Bird	O
Gift	O
Fund	O
.	O
We	O
are	O
also	O
very	O
thankful	O
for	O
the	O
public	O
belief	O
tracking	O
code	O
and	O
multilingual	O
state	O
-	O
tracking	O
datasets	O
released	O
by	O
Nikola	O
Mrksic	O
from	O
the	O
University	O
of	O
Cambridge	B-DatasetName
.	O

A	O
Multi	O
-	O
Type	O
Multi	O
-	O
Span	O
Network	O
for	O
Reading	B-TaskName
Comprehension	I-TaskName
that	O
Requires	O
Discrete	O
Reasoning	O

In	O
the	O
reading	B-TaskName
comprehension	I-TaskName
task	O
that	O
requires	O
discrete	O
reasoning	O
,	O
a	O
passage	O
and	O
a	O
question	O
are	O
given	O
.	O
The	O
goal	O
is	O
to	O
predict	O
an	O
answer	O
to	O
the	O
question	O
by	O
reading	O
and	O
understanding	O
the	O
passage	O
.	O
Unlike	O
previous	O
dataset	O
such	O
as	O
SQuAD	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
where	O
the	O
answer	O
is	O
limited	O
to	O
be	O
a	O
single	O
span	O
of	O
text	O
,	O
DROP	B-DatasetName
loosens	O
the	O
constraint	O
so	O
that	O
the	O
answer	O
involves	O
various	O
types	O
such	O
as	O
number	O
,	O
date	O
,	O
or	O
span	O
of	O
text	O
(	O
Figure	O
1	O
)	O
.	O
Moreover	O
,	O
the	O
answer	O
can	O
be	O
multiple	O
text	O
strings	O
instead	O
of	O
single	O
continuous	O
span	O
(	O
A	O
2	O
)	O
.	O
To	O
suc	O
-	O
Passage	O
:	O
As	O
of	O
the	O
census	O
of	O
2000	O
,	O
there	O
were	O
218	O
,	O
590	O
people	O
,	O
79	O
,	O
667	O
households	O
,	O
...	O
22.5	O
%	O
were	O
of	O
German	O
people	O
,	O
13.1	O
%	O
Irish	O
people	O
,	O
9.8	O
%	O
Italian	O
people	O
,	O
...	O
Q1	O
:	O
Which	O
group	O
from	O
the	O
census	O
is	O
larger	O
:	O
German	O
or	O
Irish	O
?	O
A1	O
:	O
German	O
Q2	O
:	O
Which	O
ancestral	O
groups	O
are	O
at	O
least	O
10	O
%	O
?	O
A2	O
:	O
German	O
,	O
Irish	O
Q3	O
:	O
How	O
many	O
more	O
people	O
are	O
there	O
than	O
households	O
?	O
A3	O
:	O
138	O
,	O
923	O
Q4	O
:	O
How	O
many	O
percent	O
were	O
not	O
German	O
?	O
A4	O
:	O
77.5	O
Figure	O
1	O
:	O
Question	O
-	O
answer	O
pairs	O
along	O
with	O
a	O
passage	O
from	O
the	O
DROP	B-DatasetName
dataset	O
.	O
cessfully	O
find	O
the	O
answer	O
,	O
some	O
discrete	O
reasoning	O
abilities	O
,	O
such	O
as	O
sorting	O
(	O
A	O
1	O
)	O
,	O
subtraction	O
(	O
A	O
3	O
)	O
,	O
and	O
negation	O
(	O
A	O
4	O
)	O
,	O
are	O
required	O
.	O

Figure	O
2	O
gives	O
an	O
overview	O
of	O
our	O
model	O
that	O
aims	O
to	O
combine	O
neural	O
reading	B-TaskName
comprehension	I-TaskName
with	O
numerical	O
reasoning	O
.	O
Our	O
model	O
uses	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
as	O
encoder	O
:	O
we	O
map	O
word	B-TaskName
embeddings	I-TaskName
into	O
contextualized	O
representations	O
using	O
pre	O
-	O
trained	O
Transformer	B-MethodName
blocks	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
(	O
3.1	O
)	O
.	O
Based	O
on	O
the	O
representations	O
,	O
we	O
employ	O
a	O
multi	O
-	O
type	O
answer	O
predictor	O
that	O
is	O
able	O
to	O
produce	O
four	O
answer	O
types	O
:	O
(	O
1	O
)	O
span	O
from	O
the	O
text	O
;	O
(	O
2	O
)	O
arithmetic	O
expression	O
;	O
(	O
3	O
)	O
count	O
number	O
;	O
(	O
4	O
)	O
negation	O
on	O
numbers	O
(	O
3.2	O
)	O
.	O
Following	O
Dua	O
et	O
al	O
(	O
2019	O
)	O
,	O
we	O
first	O
predict	O
the	O
answer	O
type	O
of	O
a	O
given	O
passage	O
-	O
question	O
pair	O
,	O
and	O
then	O
adopt	O
individual	O
prediction	O
strategies	O
.	O
To	O
support	O
multispan	O
extraction	O
(	O
3.3	O
)	O
,	O
the	O
model	O
explicitly	O
predicts	O
the	O
number	O
of	O
answer	O
spans	O
.	O
It	O
then	O
outputs	O
non	O
-	O
overlapped	O
spans	O
until	O
the	O
specific	O
amount	O
is	O
reached	O
.	O
Moreover	O
,	O
we	O
do	O
not	O
directly	O
use	O
the	O
arithmetic	O
expression	O
that	O
possesses	O
the	O
maximum	O
probability	O
,	O
but	O
instead	O
re	O
-	O
rank	O
several	O
expression	O
candidates	O
that	O
are	O
decoded	O
by	O
beam	O
search	O
to	O
further	O
confirm	O
the	O
prediction	O
(	O
3.4	O
)	O
.	O
Finally	O
,	O
the	O
model	O
is	O
trained	O
under	O
weakly	O
-	O
supervised	O
signals	O
to	O
maximize	O
the	O
marginal	O
likelihood	O
over	O
all	O
possible	O
annotations	O
(	O
3.5	O
)	O
.	O

To	O
obtain	O
a	O
universal	O
representation	O
for	O
both	O
the	O
question	O
and	O
the	O
passage	O
,	O
we	O
utilize	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
a	O
pre	O
-	O
trained	O
deep	O
bidirectional	O
Transformer	B-MethodName
model	O
that	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
across	O
various	O
tasks	O
,	O
as	O
the	O
encoder	O
.	O
Specifically	O
,	O
we	O
first	O
tokenize	O
the	O
question	O
and	O
The	O
multi	O
-	O
type	O
answer	O
predictor	O
supports	O
four	O
kinds	O
of	O
answer	O
types	O
including	O
span	O
,	O
addition	O
/	O
subtraction	O
,	O
count	O
,	O
and	O
negation	O
.	O
A	O
multi	O
-	O
span	O
extraction	O
method	O
is	O
proposed	O
to	O
dynamically	O
produce	O
one	O
or	O
several	O
spans	O
.	O
The	O
arithmetic	O
expression	O
reranking	O
mechanism	O
aims	O
to	O
rank	O
expression	O
candidates	O
that	O
are	O
decoded	O
by	O
beam	O
search	O
for	O
further	O
validating	O
the	O
prediction	O
.	O
the	O
passage	O
using	O
the	O
WordPiece	B-MethodName
vocabulary	O
(	O
Wu	O
et	O
al	O
,	O
2016	O
)	O
,	O
and	O
then	O
generate	O
the	O
input	O
sequence	O
by	O
concatenating	O
a	O
[	O
CLS	O
]	O
token	O
,	O
the	O
tokenized	O
question	O
,	O
a	O
[	O
SEP	O
]	O
token	O
,	O
the	O
tokenized	O
passage	O
,	O
and	O
a	O
final	O
[	O
SEP	O
]	O
token	O
.	O
For	O
each	O
token	O
in	O
the	O
sequence	O
,	O
its	O
input	O
representation	O
is	O
the	O
elementwise	O
addition	O
of	O
WordPiece	B-MethodName
embeddings	O
,	O
positional	O
embeddings	O
,	O
and	O
segment	O
embeddings	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
.	O
As	O
a	O
result	O
,	O
a	O
list	O
of	O
input	O
embeddings	O
H	O
0	B-DatasetName
2	O
R	O
T	O
⇥	O
D	O
can	O
be	O
obtained	O
,	O
where	O
D	O
is	O
the	O
hidden	O
size	O
and	O
T	O
is	O
the	O
sequence	O
length	O
.	O
A	O
series	O
of	O
L	O
pre	O
-	O
trained	O
Transformer	B-MethodName
blocks	O
are	O
then	O
used	O
to	O
project	O
the	O
input	O
embeddings	O
into	O
contextualized	O
representations	O
H	O
i	O
as	O
:	O
H	O
i	O
=	O
TransformerBlock	O
(	O
H	O
i	O
1	O
)	O
,	O
8i	O
2	O
[	O
1	O
,	O
L	O
]	O
Here	O
,	O
we	O
omit	O
a	O
detailed	O
introduction	O
of	O
the	O
block	O
architecture	O
and	O
refer	O
readers	O
to	O
Vaswani	O
et	O
al	O
(	O
2017	O
)	O
for	O
more	O
details	O
.	O

Rather	O
than	O
restricting	O
the	O
answer	O
to	O
always	O
be	O
a	O
span	O
of	O
text	O
,	O
the	O
discrete	O
-	O
reasoning	O
reading	B-TaskName
comprehension	I-TaskName
task	O
involves	O
different	O
answer	O
types	O
(	O
e.g.	O
,	O
number	O
,	O
date	O
,	O
span	O
of	O
text	O
)	O
.	O
Following	O
Dua	O
et	O
al	O
(	O
2019	O
)	O
,	O
we	O
design	O
a	O
multi	O
-	O
type	O
answer	O
predictor	O
to	O
selectively	O
produce	O
different	O
kinds	O
of	O
answers	O
such	O
as	O
span	O
,	O
count	O
number	O
,	O
and	O
arithmetic	O
expression	O
.	O
To	O
further	O
increase	O
answer	O
coverage	O
,	O
we	O
propose	O
adding	O
a	O
new	O
answer	O
type	O
to	O
support	O
logical	O
negation	O
.	O
Moreover	O
,	O
unlike	O
prior	O
work	O
that	O
separately	O
predicts	O
passage	O
spans	O
and	O
question	O
spans	O
,	O
our	O
approach	O
directly	O
extracts	O
spans	O
from	O
the	O
input	O
sequence	O
.	O
Answer	O
type	B-TaskName
prediction	I-TaskName
Inspired	B-DatasetName
by	O
the	O
Augmented	O
QANet	O
model	O
(	O
Dua	O
et	O
al	O
,	O
2019	O
)	O
,	O
we	O
use	O
the	O
contextualized	O
token	O
representations	O
from	O
the	O
last	O
four	O
blocks	O
(	O
H	O
L	O
3	O
,	O
...	O
,	O
H	O
L	O
)	O
as	O
the	O
inputs	O
to	O
our	O
answer	O
predictor	O
,	O
which	O
are	O
denoted	O
as	O
M	O
0	B-DatasetName
,	O
M	O
1	O
,	O
M	O
2	O
,	O
M	O
3	O
,	O
respectively	O
.	O
To	O
predict	O
the	O
answer	O
type	O
,	O
we	O
first	O
split	O
the	O
representation	O
M	O
2	O
into	O
a	O
question	O
representation	O
Q	O
2	O
and	O
a	O
passage	O
representation	O
P	O
2	O
according	O
to	O
the	O
index	O
of	O
intermediate	O
[	O
SEP	O
]	O
token	O
.	O
Then	O
the	O
model	O
computes	O
two	O
vectors	O
h	O
Q	O
2	O
and	O
h	O
P	O
2	O
that	O
summarize	O
the	O
question	O
and	O
passage	O
information	O
respectively	O
:	O
↵	O
Q	O
=	O
softmax	B-MethodName
(	O
W	O
Q	O
Q	O
2	O
)	O
,	O
h	O
Q	O
2	O
=	O
↵	O
Q	O
Q	O
2	O
where	O
h	O
P	O
2	O
is	O
computed	O
in	O
a	O
similar	O
way	O
over	O
P	O
2	O
.	O
Next	O
,	O
we	O
calculate	O
a	O
probability	O
distribution	O
to	O
represent	O
the	O
choices	O
of	O
different	O
answer	O
types	O
as	O
:	O
p	O
type	O
=	O
softmax	B-MethodName
(	O
FFN	O
(	O
[	O
h	O
Q	O
2	O
;	O
h	O
P	O
2	O
;	O
h	O
CLS	O
]	O
)	O
)	O
Here	O
,	O
h	O
CLS	O
is	O
the	O
first	O
vector	O
in	O
the	O
final	O
contextualized	O
representation	O
M	O
3	O
,	O
and	O
FFN	O
denotes	O
a	O
feed	O
-	O
forward	O
network	O
consisting	O
of	O
two	O
linear	O
projections	O
with	O
a	O
GeLU	B-MethodName
activation	O
(	O
Hendrycks	O
and	O
Gimpel	O
,	O
2016	O
)	O
followed	O
by	O
a	O
layer	B-MethodName
normalization	I-MethodName
(	O
Lei	O
Ba	O
et	O
al	O
,	O
2016	O
)	O

Span	O
To	O
extract	O
the	O
answer	O
either	O
from	O
the	O
passage	O
or	O
from	O
the	O
question	O
,	O
we	O
combine	O
the	O
gating	O
mechanism	O
of	O
Wang	O
et	O
al	O
(	O
2017	O
)	O
with	O
the	O
standard	O
decoding	O
strategy	O
of	O
Seo	O
et	O
al	O
(	O
2017	O
)	O
to	O
predict	O
the	O
starting	O
and	O
ending	O
positions	O
across	O
the	O
entire	O
sequence	O
.	O
Specifically	O
,	O
we	O
first	O
compute	O
three	O
vectors	O
,	O
namely	O
g	O
Q	O
0	B-DatasetName
,	O
g	O
Q	O
1	O
,	O
g	O
Q	O
2	O
,	O
that	O
summarize	O
the	O
question	O
information	O
among	O
different	O
levels	O
of	O
question	O
representations	O
:	O
Q	O
=	O
softmax	B-MethodName
(	O
FFN	O
(	O
Q	O
2	O
)	O
,	O
g	O
Q	O
2	O
=	O
Q	O
Q	O
2	O
where	O
g	O
Q	O
0	B-DatasetName
and	O
g	O
Q	O
1	O
are	O
computed	O
over	O
Q	O
0	B-DatasetName
and	O
Q	O
1	O
respectively	O
,	O
in	O
a	O
similar	O
way	O
as	O
described	O
above	O
.	O
Then	O
we	O
compute	O
the	O
probabilities	O
of	O
the	O
starting	O
and	O
ending	O
indices	O
of	O
the	O
answer	O
span	O
from	O
the	O
input	O
sequence	O
as	O
:	O
M	O
start	O
=	O
[	O
M	O
2	O
;	O
M	O
0	B-DatasetName
;	O
g	O
Q	O
2	O
⌦	O
M	O
2	O
;	O
g	O
Q	O
0	B-DatasetName
⌦	O
M	O
0	B-DatasetName
]	O
,	O
M	O
end	O
=	O
[	O
M	O
2	O
;	O
M	O
1	O
;	O
g	O
Q	O
2	O
⌦	O
M	O
2	O
;	O
g	O
Q	O
1	O
⌦	O
M	O
1	O
]	O
,	O
p	O
start	O
=	O
softmax	B-MethodName
(	O
W	O
SMstart	O
)	O
,	O
p	O
end	O
=	O
softmax	B-MethodName
(	O
W	O
EMend	O
)	O
where	O
⌦	O
denotes	O
the	O
outer	O
product	O
between	O
the	O
vector	O
g	O
and	O
each	O
token	O
representation	O
in	O
M.	O

In	O
order	O
to	O
model	O
the	O
process	O
of	O
performing	O
addition	O
or	O
subtraction	O
among	O
multiple	O
numbers	O
mentioned	O
in	O
the	O
passage	O
,	O
we	O
assign	O
a	O
three	O
-	O
way	O
categorical	O
variable	O
(	O
plus	O
,	O
minus	O
,	O
or	O
zero	O
)	O
for	O
each	O
number	O
to	O
indicate	O
its	O
sign	O
,	O
similar	O
to	O
Dua	O
et	O
al	O
(	O
2019	O
)	O
.	O
As	O
a	O
result	O
,	O
an	O
arithmetic	O
expression	O
that	O
has	O
a	O
number	O
as	O
the	O
final	O
answer	O
can	O
be	O
obtained	O
and	O
easily	O
evaluated	O
.	O
Specifically	O
,	O
for	O
each	O
number	O
mentioned	O
in	O
the	O
passage	O
,	O
we	O
gather	O
its	O
corresponding	O
representation	O
from	O
the	O
concatenation	O
of	O
M	O
2	O
and	O
M	O
3	O
,	O
eventually	O
yielding	O
U	O
=	O
(	O
u	O
1	O
,	O
...	O
,	O
u	O
N	O
)	O
2	O
R	O
N	O
⇥	O
2	O
⇤	O
D	O
where	O
N	O
numbers	O
exist	O
.	O
Then	O
the	O
probabilities	O
of	O
the	O
i	O
-	O
th	O
number	O
being	O
assigned	O
a	O
plus	O
,	O
minus	O
or	O
zero	O
is	O
computed	O
as	O
:	O
p	O
sign	O
i	O
=	O
softmax	B-MethodName
(	O
FFN	O
(	O
[	O
u	O
i	O
;	O
h	O
Q	O
2	O
;	O
h	O
P	O
2	O
;	O
h	O
CLS	O
]	O
)	O
)	O
Count	O
We	O
consider	O
the	O
ability	O
of	O
counting	O
entities	O
and	O
model	O
it	O
as	O
a	O
multi	B-TaskName
-	I-TaskName
class	I-TaskName
classification	I-TaskName
problem	O
.	O
To	O
achieve	O
this	O
,	O
the	O
model	O
first	O
produces	O
a	O
vector	O
h	O
U	O
that	O
summarizes	O
the	O
important	O
information	O
among	O
all	O
mentioned	O
numbers	O
,	O
and	O
then	O
computes	O
a	O
counting	O
probability	O
distribution	O
as	O
:	O
↵	O
U	O
=	O
softmax	B-MethodName
(	O
W	O
U	O
U	O
)	O
,	O
h	O
U	O
=	O
↵	O
U	O
U	O
,	O
p	O
count	O
=	O
softmax	B-MethodName
(	O
FFN	O
(	O
[	O
h	O
U	O
;	O
h	O
Q	O
2	O
;	O
h	O
P	O
2	O
;	O
h	O
CLS	O
]	O
)	O
)	O
Negation	O
One	O
obvious	O
but	O
important	O
linguistic	O
phenomenon	O
that	O
prior	O
work	O
fails	O
to	O
capture	O
is	O
negation	O
.	O
We	O
find	O
there	O
are	O
many	O
cases	O
in	O
DROP	B-DatasetName
that	O
require	O
to	O
perform	O
logical	O
negation	O
on	O
numbers	O
.	O
The	O
question	O
(	O
Q	O
4	O
)	O
in	O
Figure	O
1	O
gives	O
a	O
qualitative	O
example	O
of	O
this	O
phenomenon	O
.	O
To	O
model	O
this	O
phenomenon	O
,	O
we	O
assign	O
a	O
two	O
-	O
way	O
categorical	O
variable	O
for	O
each	O
number	O
to	O
indicate	O
whether	O
a	O
negation	O
operation	O
should	O
be	O
performed	O
.	O
Then	O
we	O
compute	O
the	O
probabilities	O
of	O
logical	O
negation	O
on	O
the	O
i	O
-	O
th	O
number	O
as	O
:	O
p	O
negation	O
i	O
=	O
softmax	B-MethodName
(	O
FFN	O
(	O
[	O
u	O
i	O
;	O
h	O
Q	O
2	O
;	O
h	O
P	O
2	O
;	O
h	O
CLS	O
]	O
)	O
)	O

As	O
discussed	O
in	O
3.2	O
,	O
we	O
model	O
the	O
phenomenon	O
of	O
discrete	O
reasoning	O
on	O
numbers	O
by	O
learning	O
to	O
predict	O
a	O
plus	O
,	O
minus	O
,	O
or	O
zero	O
for	O
each	O
number	O
in	O
the	O
passage	O
.	O
In	O
this	O
way	O
,	O
an	O
arithmetic	O
expression	O
composed	O
of	O
signed	O
numbers	O
can	O
be	O
obtained	O
,	O
where	O
the	O
final	O
answer	O
can	O
be	O
deduced	O
by	O
performing	O
simple	O
arithmetic	O
computation	O
.	O
However	O
,	O
since	O
the	O
sign	O
of	O
each	O
number	O
is	O
only	O
determined	O
by	O
the	O
number	O
representation	O
and	O
some	O
coarsegrained	O
global	O
representations	O
,	O
the	O
context	O
information	O
of	O
the	O
expression	O
itself	O
has	O
not	O
been	O
considered	O
.	O
As	O
a	O
result	O
,	O
the	O
model	O
may	O
predict	O
some	O
Algorithm	O
1	O
Multi	O
-	O
span	O
extraction	O
Input	O
:	O
p	O
start	O
;	O
p	O
end	O
;	O
p	O
span	O
1	O
:	O
Generate	O
the	O
set	O
S	O
by	O
extracting	O
top	O
-	O
K	O
spans	O
2	O
:	O
Sort	O
S	O
in	O
descending	O
order	O
of	O
span	O
scores	O
3	O
:	O
t	O
=	O
arg	O
max	O
p	O
span	O
+	O
1	O
4	O
:	O
InitializeS	O
=	O
{	O
}	O
5	O
:	O
while	O
S	O
6	O
=	O
{	O
}	O
and	O
|	O
S	O
|	O
<	O
t	O
do	O
6	O
:	O
for	O
si	O
in	O
S	O
do	O
7	O
:	O
Add	O
span	O
si	O
toS	O
8	O
:	O
Remove	O
span	O
si	O
from	O
S	O
9	O
:	O
for	O
sj	O
in	O
S	O
do	O
10	O
:	O
if	O
f1	O
(	O
si	O
,	O
sj	O
)	O
>	O
0	B-DatasetName
then	O
11	O
:	O
Remove	O
span	O
sj	O
from	O
S	O
12	O
:	O
returnS	O
obviously	O
wrong	O
expressions	O
(	O
e.g.	O
,	O
the	O
signs	O
that	O
have	O
maximum	O
probabilities	O
are	O
either	O
minus	O
or	O
zero	O
,	O
resulting	O
in	O
a	O
large	O
negative	O
value	O
)	O
.	O
Therefore	O
,	O
in	O
order	O
to	O
further	O
validate	O
the	O
prediction	O
,	O
it	O
is	O
necessary	O
to	O
rank	O
several	O
highly	O
confident	O
expression	O
candidates	O
using	O
the	O
representation	O
summarized	O
from	O
the	O
expression	O
's	O
context	O
.	O
Specifically	O
,	O
we	O
use	O
beam	O
search	O
to	O
produce	O
top	O
-	O
ranked	O
arithmetic	O
expressions	O
,	O
which	O
are	O
sent	O
back	O
to	O
the	O
network	O
for	O
reranking	O
.	O
Since	O
each	O
expression	O
consists	O
of	O
several	O
signed	O
numbers	O
,	O
we	O
construct	O
an	O
expression	O
representation	O
by	O
taking	O
both	O
the	O
numbers	O
and	O
the	O
signs	O
into	O
account	O
.	O
For	O
each	O
number	O
in	O
the	O
expression	O
,	O
we	O
gather	O
its	O
corresponding	O
vector	O
from	O
the	O
representation	O
U.	O
As	O
for	O
the	O
signs	O
,	O
we	O
initialize	O
an	O
embedding	O
matrix	O
E	O
2	O
R	O
3	O
⇥	O
2	O
⇤	O
D	O
,	O
and	O
find	O
the	O
sign	O
embeddings	O
for	O
each	O
signed	O
number	O
.	O
In	O
this	O
way	O
,	O
given	O
the	O
i	O
-	O
th	O
expression	O
that	O
contains	O
M	O
signed	O
numbers	O
at	O
most	O
,	O
we	O
can	O
obtain	O
number	O
vectors	O
V	O
i	O
2	O
R	O
M	O
⇥	O
2	O
⇤	O
D	O
as	O
well	O
as	O
sign	O
embeddings	O
C	O
i	O
2	O
R	O
M	O
⇥	O
2	O
⇤	O
D	O
.	O
Then	O
the	O
expression	O
representation	O
along	O
with	O
the	O
reranking	O
probability	O
can	O
be	O
calculated	O
as	O
:	O
↵	O
V	O
i	O
=	O
softmax	B-MethodName
(	O
W	O
V	O
(	O
V	O
i	O
+	O
C	O
i	O
)	O
)	O
,	O
h	O
V	O
i	O
=	O
↵	O
V	O
i	O
(	O
V	O
i	O
+	O
C	O
i	O
)	O
,	O
p	O
arith	O
i	O
=	O
softmax	B-MethodName
(	O
FFN	O
(	O
[	O
h	O
V	O
i	O
;	O
h	O
Q	O
2	O
;	O
h	O
P	O
2	O
;	O
h	O
CLS	O
]	O
)	O
)	O
3	O
.	O

Reading	B-TaskName
comprehension	I-TaskName
benchmarks	O
Promising	O
advancements	O
have	O
been	O
made	O
for	O
reading	B-TaskName
comprehension	I-TaskName
due	O
to	O
the	O
creation	O
of	O
many	O
large	O
datasets	O
.	O
While	O
early	O
research	O
used	O
cloze	O
-	O
style	O
tests	O
(	O
Hermann	O
et	O
al	O
,	O
2015	O
;	O
Hill	O
et	O
al	O
,	O
2016	O
)	O
,	O
most	O
of	O
recent	O
works	O
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
;	O
Joshi	O
et	O
al	O
,	O
2017	O
)	O
are	O
designed	O
to	O
extract	O
answers	O
from	O
the	O
passage	O
.	O
Despite	O
their	O
success	O
,	O
these	O
datasets	O
only	O
require	O
shallow	O
pattern	O
matching	O
and	O
simple	O
logical	O
reasoning	O
,	O
thus	O
being	O
well	O
solved	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
.	O
Recently	O
,	O
Dua	O
et	O
al	O
(	O
2019	O
)	O
released	O
a	O
new	O
benchmark	O
named	O
DROP	B-DatasetName
that	O
demands	O
discrete	O
reasoning	O
as	O
well	O
as	O
deeper	O
paragraph	O
understanding	O
to	O
find	O
the	O
answers	O
.	O
Saxton	O
et	O
al	O
(	O
2019	O
)	O
introduced	O
a	O
dataset	O
consisting	O
of	O
different	O
types	O
of	O
mathematics	O
problems	O
to	O
focuses	O
on	O
mathematical	O
computation	O
.	O
We	O
choose	O
to	O
work	O
on	O
DROP	B-DatasetName
to	O
test	O
both	O
the	O
numerical	O
reasoning	O
and	O
linguistic	O
comprehension	O
abilities	O
.	O
Neural	O
reading	O
models	O
Previous	O
neural	O
reading	O
models	O
,	O
such	O
as	O
BiDAF	O
(	O
Seo	O
et	O
al	O
,	O
2017	O
)	O
,	O
R	O
-	O
Net	O
(	O
Wang	O
et	O
al	O
,	O
2017	O
)	O
,	O
QANet	O
(	O
Yu	O
et	O
al	O
,	O
2018	O
)	O
,	O
Reinforced	O
Mreader	O
(	O
Hu	O
et	O
al	O
,	O
2018	O
)	O
,	O
are	O
usually	O
designed	O
to	O
extract	O
a	O
continuous	O
span	O
of	O
text	O
as	O
the	O
answer	O
.	O
Dua	O
et	O
al	O
(	O
2019	O
)	O
enhanced	O
prior	O
single	O
-	O
type	B-TaskName
prediction	I-TaskName
to	O
support	O
various	O
answer	O
types	O
such	O
as	O
span	O
,	O
count	O
number	O
,	O
and	O
addition	O
/	O
subtraction	O
.	O
Different	O
from	O
these	O
approaches	O
,	O
our	O
model	O
additionally	O
supports	O
a	O
new	O
negation	O
type	O
to	O
increase	O
answer	O
coverage	O
,	O
and	O
learns	O
to	O
dynamically	O
extract	O
one	O
or	O
multiple	O
spans	O
.	O
Morevoer	O
,	O
answer	O
reranking	O
has	O
been	O
well	O
studied	O
in	O
several	O
prior	O
works	O
(	O
Cui	O
et	O
al	O
,	O
2016	O
;	O
Wang	O
et	O
al	O
,	O
2018a	O
,	O
b	O
,	O
c	O
;	O
Hu	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
follow	O
this	O
line	O
of	O
work	O
,	O
but	O
propose	O
ranking	O
arithmetic	O
expressions	O
instead	O
of	O
candidate	O
answers	O
.	O
End	O
-	O
to	O
-	O
end	O
symbolic	O
reasoning	O
Combining	O
neural	O
methods	O
with	O
symbolic	O
reasoning	O
was	O
considered	O
by	O
Graves	O
et	O
al	O
(	O
2014	O
)	O
;	O
Sukhbaatar	O
et	O
al	O
(	O
2015	O
)	O
,	O
where	O
neural	O
networks	O
augmented	O
with	O
external	O
memory	O
are	O
trained	O
to	O
execute	O
simple	O
programs	O
.	O
Later	O
works	O
on	O
program	B-TaskName
induction	I-TaskName
(	O
Reed	O
and	O
De	O
Freitas	O
,	O
2016	O
;	O
Neelakantan	O
et	O
al	O
,	O
2016	O
;	O
Liang	O
et	O
al	O
,	O
2017	O
)	O
extended	O
this	O
idea	O
by	O
using	O
several	O
built	O
-	O
in	O
logic	O
operations	O
along	O
with	O
a	O
key	O
-	O
value	O
memory	O
to	O
learn	O
different	O
types	O
of	O
compositional	O
programs	O
such	O
as	O
addition	O
or	O
sorting	O
.	O
In	O
contrast	O
to	O
these	O
works	O
,	O
MTMSN	O
does	O
not	O
model	O
various	O
types	O
of	O
reasoning	O
with	O
a	O
universal	O
memory	O
mechanism	O
but	O
instead	O
deals	O
each	O
type	O
with	O
individual	O
predicting	O
strategies	O
.	O
Visual	B-DatasetName
question	I-DatasetName
answering	I-DatasetName
In	O
computer	O
vision	O
community	O
,	O
the	O
most	O
similar	O
work	O
to	O
our	O
approach	O
is	O
Neural	O
Module	O
Networks	O
(	O
Andreas	O
et	O
al	O
,	O
2016b	O
)	O
,	O
where	O
a	O
dependency	O
parser	O
is	O
used	O
to	O
lay	O
out	O
a	O
neural	O
network	O
composed	O
of	O
several	O
pre	O
-	O
defined	O
modules	O
.	O
Later	O
,	O
Andreas	O
et	O
al	O
(	O
2016a	O
)	O
proposed	O
dynamically	O
choosing	O
an	O
optimal	O
layout	O
structure	O
from	O
a	O
list	O
of	O
layout	O
candidates	O
that	O
are	O
produced	O
by	O
off	O
-	O
the	O
-	O
shelf	O
parsers	O
.	O
Hu	O
et	O
al	O
(	O
2017	O
)	O
introduced	O
an	O
end	O
-	O
to	O
-	O
end	O
module	O
network	O
that	O
learns	O
to	O
predict	O
instance	O
-	O
specific	O
network	O
layouts	O
without	O
the	O
aid	O
of	O
a	O
parser	O
.	O
Compared	O
to	O
these	O
approaches	O
,	O
MTMSN	O
has	O
a	O
static	O
network	O
layout	O
that	O
can	O
not	O
be	O
changed	O
during	O
training	O
and	O
evaluation	O
,	O
where	O
pre	O
-	O
defined	O
"	O
modules	O
"	O
are	O
used	O
to	O
handle	O
different	O
types	O
of	O
answers	O
.	O

Abstract	O
Text	B-TaskName
Summarization	I-TaskName
:	O
A	O
Low	O
Resource	O
Challenge	O

Automatic	O
text	B-TaskName
summarization	I-TaskName
is	O
considered	O
as	O
a	O
challenging	O
task	O
because	O
while	O
summarizing	O
a	O
piece	O
of	O
text	O
,	O
we	O
read	O
it	O
entirely	O
to	O
develop	O
our	O
understanding	O
to	O
prepare	O
highlighting	O
its	O
main	O
points	O
.	O
Due	O
to	O
the	O
lack	O
of	O
human	O
knowledge	O
and	O
language	O
processing	O
abilities	O
in	O
computers	O
,	O
automatic	O
text	B-TaskName
summarization	I-TaskName
is	O
a	O
major	O
non	O
-	O
trivial	O
task	O
(	O
Allahyari	O
et	O
al	O
,	O
2017	O
)	O
.	O
Two	O
major	O
approaches	O
for	O
automatic	O
summarization	B-TaskName
are	O
:	O
extractive	O
and	O
abstractive	O
.	O
The	O
extractive	B-TaskName
summarization	I-TaskName
approach	O
produces	O
summaries	O
by	O
choosing	O
a	O
subset	O
of	O
sentences	O
in	O
the	O
original	O
text	O
.	O
The	O
abstract	O
text	B-TaskName
summarization	I-TaskName
approach	O
aims	O
to	O
shorten	O
the	O
long	O
text	O
into	O
a	O
humanreadable	O
form	O
that	O
contains	O
the	O
most	O
important	O
fact	O
from	O
the	O
original	O
text	O
(	O
Allahyari	O
et	O
al	O
,	O
2017	O
;	O
Kryściński	O
et	O
al	O
,	O
2018	O
)	O
.	O
The	O
deep	O
learning	O
-	O
based	O
neural	O
attention	O
model	O
when	O
applying	O
to	O
abstract	O
text	B-TaskName
summarization	I-TaskName
performs	O
well	O
compared	O
to	O
standard	O
learning	O
-	O
based	O
approaches	O
(	O
Rush	O
et	O
al	O
,	O
2015	O
)	O
.	O
Abstract	O
text	B-TaskName
summarization	I-TaskName
using	O
the	O
attentional	O
encoder	O
-	O
decoder	O
recurrent	O
neural	O
network	O
approach	O
shows	O
a	O
stateof	O
-	O
the	O
-	O
art	O
performance	O
and	O
sets	O
a	O
baseline	O
model	O
(	O
Nallapati	O
et	O
al	O
,	O
2016	O
)	O
.	O
Further	O
improvements	O
are	O
introduced	O
to	O
the	O
baseline	O
model	O
by	O
using	O
the	O
pointer	O
generator	O
network	O
and	O
coverage	O
mechanism	O
using	O
reinforcement	O
learning	O
based	O
training	O
procedure	O
(	O
See	O
et	O
al	O
,	O
2017	O
;	O
Paulus	O
et	O
al	O
,	O
2017	O
)	O
.	O
There	O
is	O
an	O
inherent	O
limitation	O
to	O
natural	O
language	O
processing	O
tasks	O
such	O
as	O
text	B-TaskName
summarization	I-TaskName
for	O
resource	O
-	O
poor	O
and	O
morphological	O
complex	O
languages	O
owing	O
to	O
a	O
shortage	O
of	O
quality	O
linguistic	O
data	O
available	O
(	O
Kurniawan	O
and	O
Louvan	O
,	O
2018	O
)	O
.	O
The	O
use	O
of	O
synthetic	O
data	O
along	O
with	O
the	O
real	O
data	O
is	O
one	O
of	O
the	O
popular	O
approaches	O
followed	O
in	O
machine	B-TaskName
translation	I-TaskName
domain	O
for	O
the	O
low	O
resource	O
conditions	O
to	O
improve	O
the	O
translation	O
quality	O
(	O
Bojar	O
and	O
Tamchyna	O
,	O
2011	O
;	O
Hoang	O
et	O
al	O
,	O
2018	O
;	O
Chinea	O
-	O
Rıos	O
et	O
al	O
,	O
2017	O
)	O
.	O
The	O
iterative	O
back	O
-	O
translation	O
(	O
e.g.	O
training	O
back	O
-	O
translation	O
systems	O
multiple	O
times	O
)	O
were	O
also	O
found	O
effective	O
in	O
machine	B-TaskName
translation	I-TaskName
(	O
Hoang	O
et	O
al	O
,	O
2018	O
)	O
.	O
We	O
explore	O
similar	O
approaches	O
in	O
our	O
experiments	O
for	O
the	O
text	B-TaskName
summarization	I-TaskName
task	O
.	O
The	O
organizations	O
of	O
this	O
paper	O
is	O
as	O
follows	O
:	O
Section	O
1	O
describes	O
related	O
work	O
on	O
abstract	O
text	B-TaskName
summarization	I-TaskName
.	O
Section	O
2	O
explains	O
the	O
techniques	O
followed	O
in	O
our	O
work	O
.	O
Section	O
3	O
describes	O
the	O
dataset	O
used	O
in	O
our	O
experiment	O
.	O
Section	O
4	O
explains	O
the	O
experimental	O
settings	O
:	O
models	O
and	O
their	O
parameters	O
.	O
Section	O
5	O
provides	O
evaluation	O
results	O
with	O
analysis	O
and	O
discussion	O
.	O
Section	O
6	O
provides	O
conclusion	O
to	O
the	O
paper	O
.	O

Across	O
all	O
experiments	O
performed	O
in	O
this	O
paper	O
,	O
we	O
have	O
used	O
the	O
Transformer	B-MethodName
model	O
as	O
implemented	O
in	O
OpenNMT	O
-	O
py	O
1	O
(	O
Vaswani	O
et	O
al	O
,	O
2018	O
;	O
See	O
et	O
al	O
,	O
2017	O
)	O
.	O
The	O
Transformer	B-MethodName
model	O
is	O
based	O
on	O
encoder	O
/	O
decoder	O
architecture	O
.	O
In	O
context	O
to	O
summarize	O
,	O
it	O
takes	O
text	O
as	O
input	O
and	O
provides	O
its	O
summary	O
.	O
We	O
use	O
synthetic	O
data	O
as	O
shown	O
in	O
Figure	O
1	O
to	O
increase	O
the	O
size	O
of	O
the	O
training	O
data	O
.	O
Figure	O
1	O
:	O
Generation	O
of	O
synthetic	O
data	O
using	O
a	O
reverse	O
system	O
.	O
To	O
generate	O
synthetic	O
data	O
,	O
first	O
,	O
a	O
system	O
in	O
the	O
reverse	O
direction	O
(	O
i.e.	O
source	O
as	O
summary	O
and	O
target	O
as	O
text	O
)	O
is	O
trained	O
and	O
then	O
used	O
to	O
generate	O
text	O
for	O
the	O
given	O
summary	O
.	O
Then	O
both	O
the	O
real	O
and	O
synthetic	O
data	O
acts	O
as	O
input	O
to	O
the	O
final	O
system	O
.	O

We	O
use	O
German	O
wiki	O
data	O
(	O
spread	O
across	O
different	O
domain	O
)	O
collected	O
from	O
the	O
SwissText	O
2019	O
2	O
(	O
real	O
data	O
)	O
and	O
Common	B-DatasetName
Crawl	I-DatasetName
3	O
data	O
(	O
synthetic	O
data	O
)	O
in	O
our	O
experiment	O
.	O
The	O
statistics	O
of	O
all	O
the	O
datasets	O
are	O
shown	O
in	O
Table	O
1	O
.	O

The	O
data	O
crawled	O
from	O
the	O
Internet	O
(	O
Common	B-DatasetName
Crawl	I-DatasetName
)	O
used	O
to	O
prepare	O
synthetic	O
data	O
to	O
boost	O
the	O
training	O
.	O
The	O
steps	O
followed	O
to	O
create	O
the	O
synthetic	O
dataset	O
as	O
follows	O
:	O
Step	O
1	O
:	O
Build	O
vocab	O
:	O
We	O
create	O
vocabulary	O
using	O
SwissText	O
based	O
on	O
the	O
occurrence	O
of	O
the	O
most	O
frequent	O
(	O
top	O
N	O
)	O
German	O
words	O
.	O
Step	O
2	O
:	O
Sentence	O
selection	O
:	O
The	O
sentences	O
from	O
the	O
Common	B-DatasetName
Crawl	I-DatasetName
data	O
are	O
selected	O
with	O
respect	O
to	O
the	O
vocabulary	O
based	O
on	O
the	O
threshold	O
we	O
provide	O
(	O
e.g.	O
a	O
sentence	O
has	O
10	O
words	O
and	O
the	O
threshold	O
is	O
10	O
%	O
(	O
0.1	O
)	O
)	O
.	O
For	O
a	O
sentence	O
to	O
be	O
selected	O
,	O
at	O
least	O
1	O
out	O
of	O
10	O
words	O
should	O
be	O
in	O
the	O
vocabulary	O
.	O
Step	O
3	O
:	O
Filtering	O
:	O
Select	O
random	O
sentences	O
(	O
e.g.	O
100	O
K	O
)	O
from	O
the	O
selected	O
Common	B-DatasetName
Crawl	I-DatasetName
data	O
in	O
the	O
previous	O
step	O
.	O
Step	O
4	O
:	O
Generate	O
summary	O
:	O
The	O
100	O
K	O
data	O
obtained	O
from	O
the	O
previous	O
step	O
are	O
used	O
as	O
a	O
summary	O
and	O
required	O
to	O
generate	O
corresponding	O
text	O
.	O
We	O
use	O
the	O
reverse	O
trained	O
model	O
where	O
we	O
provide	O
the	O
summary	O
as	O
source	O
and	O
target	O
as	O
text	O
.	O
This	O
results	O
in	O
the	O
text	O
as	O
well	O
as	O
the	O
corresponding	O
summary	O
as	O
additional	O
data	O
to	O
be	O
utilized	O
along	O
with	O
real	O
data	O
(	O
SwissText	O
)	O
.	O
Eventually	O
,	O
the	O
190	O
K	O
dataset	O
is	O
created	O
(	O
denote	O
as	O
Train	O
RealSynth	O
)	O
as	O
a	O
combination	O
of	O
90	O
K	O
SwissText	O
train	O
data	O
(	O
real	O
)	O
and	O
100	O
K	O
synthetic	O
data	O
.	O
This	O
dataset	O
is	O
used	O
in	O
the	O
experimental	O
setup	O
S2	O
(	O
described	O
in	O
details	O
in	O
Section	O
4.3	O
)	O
.	O

This	O
section	O
describes	O
our	O
experiments	O
conducted	O
for	O
the	O
text	B-TaskName
summarization	I-TaskName
task	O
.	O

We	O
use	O
3	O
settings	O
:	O
(	O
i	O
)	O
real	O
data	O
(	O
we	O
set	O
this	O
as	O
the	O
baseline	O
in	O
our	O
experiment	O
)	O
,	O
(	O
ii	O
)	O
real	O
data	O
and	O
synthetic	O
data	O
,	O
and	O
(	O
iii	O
)	O
real	O
and	O
regenerated	O
synthetic	O
data	O
for	O
the	O
summarization	B-TaskName
task	O
,	O
described	O
as	O
follows	O
:	O
1	O
.	O
S1	O
:	O
Transformer	B-MethodName
model	O
using	O
Train	O
Real	O
data	O
In	O
this	O
setup	O
,	O
we	O
use	O
the	O
"	O
Train	O
Real	O
"	O
data	O
for	O
training	O
the	O
Transformer	B-MethodName
model	O
.	O

In	O
this	O
setup	O
,	O
we	O
use	O
the	O
"	O
Train	O
RealSynth	O
"	O
data	O
for	O
training	O
the	O
Transformer	B-MethodName
model	O
.	O
As	O
the	O
balance	O
between	O
real	O
and	O
synthetic	O
data	O
is	O
an	O
important	O
factor	O
,	O
we	O
maintain	O
a	O
1:1	O
ratio	O
(	O
e.g.	O
1	O
(	O
real	O
)	O
:1	O
(	O
synthetic	O
)	O
)	O
for	O
our	O
experiment	O
(	O
Sennrich	O
et	O
al	O
,	O
2016	O
)	O
.	O

Transformer	B-MethodName
Model	O
using	O
Train	O
RealSynthRegen	O
data	O
We	O
propose	O
an	O
iterative	O
approach	O
to	O
improve	O
the	O
quality	O
of	O
synthetic	O
summaries	O
.	O
In	O
this	O
setup	O
,	O
after	O
training	O
a	O
system	O
with	O
(	O
real+synthetic	O
)	O
data	O
,	O
it	O
is	O
used	O
to	O
regenerate	O
synthetic	O
data	O
for	O
the	O
final	O
system	O
.	O
As	O
a	O
result	O
,	O
the	O
input	O
data	O
to	O
the	O
final	O
system	O
is	O
a	O
combination	O
of	O
real	O
and	O
regenerated	O
synthetic	O
data	O
as	O
shown	O
in	O
Figure	O
2	O
.	O

The	O
copying	O
mechanism	O
is	O
applied	O
during	O
training	O
.	O
It	O
allows	O
the	O
summarizer	O
to	O
fall	O
back	O
and	O
copy	O
the	O
source	O
text	O
when	O
encounters	O
<	O
unk	O
>	O
tokens	O
by	O
referencing	O
to	O
the	O
softmax	B-MethodName
of	O
the	O
multiplication	O
between	O
attention	O
scores	O
of	O
the	O
output	O
with	O
the	O
attention	O
scores	O
of	O
the	O
source	O
(	O
See	O
et	O
al	O
,	O
2017	O
)	O
.	O
The	O
systems	O
are	O
trained	O
for	O
300	O
K	O
iterations	O
.	O

In	O
this	O
paper	O
,	O
we	O
highlighted	O
the	O
implementation	O
of	O
synthetic	O
data	O
for	O
the	O
abstract	O
text	O
summariza	O
-	O
Ref	O
Summary	O
:	O
"	O
Das	O
Feuerschiff	O
Relandersgrund	O
war	O
ein	O
finnisches	O
Feuerschiff	O
,	O
das	O
von	O
1888	O
bis	O
1914	O
i	O
m	O
Schrenmeer	O
bei	O
Rauma	O
positioniert	O
war	O
.	O
Heute	O
dient	O
es	O
als	O
Restaurantschiff	O
in	O
Helsinki	O
.	O
"	O
Gloss	O
:	O
The	O
lightship	O
Relandersgrund	O
was	O
a	O
Finnish	O
lightship	O
,	O
which	O
was	O
built	O
from	O
1888	O
to	O
1914	O
Schrenmeer	O
was	O
positioned	O
at	O
Rauma	O
.	O
Today	O
serves	O
it	O
as	O
a	O
restaurant	O
ship	O
in	O
Helsin	O
S1	O
Summary	O
:	O
:	O
"	O
Die	O
"	O
Rauma	O
"	O
.	O
ist	O
ein	O
1886	O
-	O
1888	O
Feuerschiff	O
der	O
norwegischen	O
Reederei	O
"	O
Libauskij	O
"	O
,	O
Das	O
Schiff	O
wurde	O
in	O
den	O
1930er	O
Jahren	O
gebaut	O
und	O
in	O
den	O
2000er	O
Jahren	O
als	O
Museumsschiff	O
als	O
"	O
Gloss:"The	O
"	O
Rauma	O
"	O
.	O
is	O
a	O
1886	O
-	O
1888	O
Lightship	O
of	O
the	O
Norwegian	O
shipping	O
company	O
"	O
Libauskij	O
"	O
,	O
The	O
ship	O
was	O
built	O
in	O
the	O
1930s	O
and	O
in	O
the	O
2000s	O
as	O
a	O
museum	O
ship	O
as	O
S2	O
Summary	O
:	O
:	O
"	O
Das	O
Feuerschiff	O
Relandersgrund	O
war	O
ein	O
Feuerschiff	O
des	O
das	O
von	O
1888	O
bis	O
1914	O
i	O
m	O
Einsatz	O
war	O
.	O
Heute	O
dient	O
es	O
als	O
Restaurantschiff	O
in	O
Kotka	O
,	O
"	O
Gloss	O
:	O
The	O
lightship	O
Relandersgrund	O
was	O
on	O
Lightship	O
of	O
the	O
1888	O
to	O
1914	O
was	O
in	O
use	O
.	O
Today	O
it	O
serves	O
as	O
a	O
restaurant	O
ship	O
in	O
Kotka	O
S3	O
Summary	O
:	O
:	O
"	O
Das	O
Kotka	O
.	O
"	O
ist	O
ein	O
finnischer	O
Museumsschiff	O
der	O
i	O
m	O
Zweiten	O
Weltkrieg	O
von	O
der	O
russischen	O
Marine	O
als	O
Restaurantschiff	O
1	O
"	O
eingesetzt	O
wurde	O
.	O
I	O
m	O
Mittelalter	O
war	O
das	O
Schiff	O
unter	O
dem	O
Namen	O
"	O
Vuolle	O
"	O
1	O
"	O
fr	O
die	O
finnische	O
Marine	O
1	O
"	O
Gloss	O
:	O
The	O
Kotka	O
.	O
"	O
Is	O
a	O
Finnish	O
one	O
Museum	O
ship	O
of	O
the	O
World	O
War	O
II	O
Russian	O
Navy	O
used	O
as	O
a	O
restaurant	O
ship	O
1	O
"	O
has	O
been	O
.	O
In	O
the	O
Middle	O
Ages	O
,	O
the	O
ship	O
was	O
under	O
the	O
name	O
"	O
Vuolle	O
"	O
1	O
"	O
for	O
the	O
Finnish	O
Navy	O
1	O
tion	O
task	O
under	O
low	O
resource	O
condition	O
,	O
which	O
helps	O
improving	O
the	O
text	B-TaskName
summarization	I-TaskName
system	O
in	O
terms	O
of	O
automatic	O
evaluation	O
metrics	O
.	O
As	O
the	O
next	O
step	O
,	O
we	O
plan	O
to	O
investigate	O
:	O
i	O
)	O
synthetic	O
summarization	B-TaskName
data	O
,	O
and	O
ii	O
)	O
applying	O
transfer	B-TaskName
learning	I-TaskName
on	O
text	B-TaskName
summarization	I-TaskName
for	O
the	O
multilingual	O
low	O
resource	O
data	O
set	O
with	O
little	O
or	O
no	O
ground	O
truth	O
summaries	O
(	O
Keneshloo	O
et	O
al	O
,	O
2018	O
)	O
.	O

The	O
work	O
is	O
supported	O
by	O
an	O
innovation	O
project	O
(	O
under	O
an	O
InnoSuisse	O
grant	O
)	O
oriented	O
to	O
improve	O
the	O
automatic	B-TaskName
speech	I-TaskName
recognition	I-TaskName
and	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
technologies	O
for	O
German	O
.	O
Title	O
:	O
"	O
SM2	O
:	O
Extracting	O
Semantic	O
Meaning	O
from	O
Spoken	O
Material	O
"	O
funding	O
application	O
no	O
.	O
29814.1	O
IP	O
-	O
ICT	O
.	O

A	O
Negative	O
Case	O
Analysis	O
of	O
Visual	B-TaskName
Grounding	I-TaskName
Methods	O
for	O
VQA	B-TaskName

Existing	O
Visual	B-DatasetName
Question	I-DatasetName
Answering	I-DatasetName
(	O
VQA	B-TaskName
)	O
methods	O
tend	O
to	O
exploit	O
dataset	O
biases	O
and	O
spurious	O
statistical	O
correlations	O
,	O
instead	O
of	O
producing	O
right	O
answers	O
for	O
the	O
right	O
reasons	O
.	O
To	O
address	O
this	O
issue	O
,	O
recent	O
bias	O
mitigation	O
methods	O
for	O
VQA	B-TaskName
propose	O
to	O
incorporate	O
visual	O
cues	O
(	O
e.g.	O
,	O
human	O
attention	O
maps	O
)	O
to	O
better	O
ground	O
the	O
VQA	B-TaskName
models	O
,	O
showcasing	O
impressive	O
gains	O
.	O
However	O
,	O
we	O
show	O
that	O
the	O
performance	O
improvements	O
are	O
not	O
a	O
result	O
of	O
improved	O
visual	B-TaskName
grounding	I-TaskName
,	O
but	O
a	O
regularization	O
effect	O
which	O
prevents	O
over	O
-	O
fitting	O
to	O
linguistic	O
priors	O
.	O
For	O
instance	O
,	O
we	O
find	O
that	O
it	O
is	O
not	O
actually	O
necessary	O
to	O
provide	O
proper	O
,	O
humanbased	O
cues	O
;	O
random	O
,	O
insensible	O
cues	O
also	O
result	O
in	O
similar	O
improvements	O
.	O
Based	O
on	O
this	O
observation	O
,	O
we	O
propose	O
a	O
simpler	O
regularization	O
scheme	O
that	O
does	O
not	O
require	O
any	O
external	O
annotations	O
and	O
yet	O
achieves	O
near	O
state	O
-	O
of	O
-	O
theart	O
performance	O
on	O
VQA	B-TaskName
-	O
CPv2	O
1	O
.	O

As	O
expected	O
of	O
any	O
real	O
world	O
dataset	O
,	O
VQA	B-TaskName
datasets	O
also	O
contain	O
dataset	O
biases	O
(	O
Goyal	O
et	O
al	O
,	O
2017	O
)	O
.	O
The	O
VQA	B-DatasetName
-	I-DatasetName
CP	I-DatasetName
dataset	O
was	O
introduced	O
to	O
study	O
the	O
robustness	O
of	O
VQA	B-TaskName
methods	O
against	O
linguistic	O
biases	O
.	O
Since	O
it	O
contains	O
different	O
answer	O
distributions	O
in	O
the	O
train	O
and	O
test	O
sets	O
,	O
VQA	B-DatasetName
-	I-DatasetName
CP	I-DatasetName
makes	O
it	O
nearly	O
impossible	O
for	O
the	O
models	O
that	O
rely	O
upon	O
linguistic	O
correlations	O
to	O
perform	O
well	O
on	O
the	O
test	O
set	O
Shrestha	O
et	O
al	O
,	O
2019	O
)	O
.	O

VQA	B-TaskName
algorithms	O
without	O
explicit	O
bias	O
mitigation	O
mechanisms	O
fail	O
on	O
VQA	B-DatasetName
-	I-DatasetName
CP	I-DatasetName
,	O
so	O
recent	O
works	O
have	O
focused	O
on	O
the	O
following	O
solutions	O
:	O

Given	O
a	O
question	O
Q	O
and	O
an	O
image	O
I	O
,	O
e.g.	O
,	O
represented	O
by	O
bottom	O
-	O
up	O
region	O
proposals	O
:	O
v	O
(	O
Anderson	O
et	O
al	O
,	O
2018	O
)	O
,	O
a	O
VQA	B-TaskName
model	O
is	O
tasked	O
with	O
predicting	O
the	O
answer	O
a	O
:	O
P	O
(	O
a	O
|	O
Q	O
,	O
I	O
)	O
=	O
f	O
V	O
QA	O
(	O
v	O
,	O
Q	O
)	O
.	O
(	O
1	O
)	O

Without	O
additional	O
regularization	O
,	O
existing	O
VQA	B-TaskName
models	O
such	O
as	O
the	O
baseline	O
model	O
used	O
in	O
this	O
work	O
:	O
UpDn	O
(	O
Anderson	O
et	O
al	O
,	O
2018	O
)	O
,	O
tend	O
to	O
rely	O
on	O
the	O
linguistic	O
priors	O
:	O
P	O
(	O
a	O
|	O
Q	O
)	O
to	O
answer	O
questions	O
.	O
Such	O
models	O
fail	O
on	O
VQA	B-DatasetName
-	I-DatasetName
CP	I-DatasetName
,	O
because	O
the	O
priors	O
in	O
the	O
test	O
set	O
differ	O
from	O
the	O
train	O
set	O
.	O

We	O
compare	O
the	O
baseline	O
UpDn	O
model	O
with	O
HINT	O
and	O
SCR	O
-	O
variants	O
trained	O
on	O
VQAv2	O
or	O
VQA	B-TaskName
-	O
CPv2	O
to	O
study	O
the	O
causes	O
behind	O
the	O
improvements	O
.	O
We	O
report	O
mean	O
accuracies	O
across	O
5	O
runs	O
,	O
where	O
a	O
pretrained	O
UpDn	O
model	O
is	O
fine	O
-	O
tuned	O
on	O
subsets	O
with	O
human	O
attention	O
maps	O
and	O
textual	O
explanations	O
for	O
HINT	O
and	O
SCR	O
respectively	O
.	O
Further	O
training	O
details	O
are	O
provided	O
in	O
the	O
Appendix	O
.	O

In	O
our	O
next	O
experiment	O
we	O
studied	O
how	O
random	O
visual	O
cues	O
performed	O
with	O
HINT	O
and	O
SCR	O
.	O
We	O
assign	O
random	O
importance	O
scores	O
to	O
the	O
visual	O
regions	O
:	O
S	O
rand	O
∼	O
uniform	O
(	O
0	B-DatasetName
,	O
1	O
)	O
.	O
We	O
test	O
two	O
variants	O
of	O
randomness	O
:	O
Fixed	O
random	O
regions	O
,	O
where	O
1	O
,	O
both	O
of	O
these	O
variants	O
obtain	O
similar	O
results	O
as	O
the	O
model	O
trained	O
with	O
human	O
-	O
based	O
importance	O
scores	O
.	O
The	O
performance	O
improves	O
even	O
when	O
the	O
importance	O
scores	O
are	O
changed	O
every	O
epoch	O
,	O
indicating	O
that	O
it	O
is	O
not	O
even	O
necessary	O
to	O
look	O
at	O
the	O
same	O
visual	O
regions	O
.	O

As	O
observed	O
by	O
Selvaraju	O
et	O
al	O
(	O
2019	O
)	O
and	O
as	O
shown	O
in	O
Fig	O
.	O
2	O
,	O
we	O
observe	O
small	O
improvements	O
on	O
VQAv2	O
when	O
the	O
models	O
are	O
fine	O
-	O
tuned	O
on	O
the	O
entire	O
train	O
set	O
.	O
However	O
,	O
if	O
we	O
were	O
to	O
compare	O
against	O
the	O
improvements	O
in	O
VQA	B-TaskName
-	O
CPv2	O
in	O
a	O
fair	O
manner	O
,	O
i.e.	O
,	O
only	O
use	O
the	O
instances	O
with	O
visual	O
cues	O
while	O
fine	O
-	O
tuning	O
,	O
then	O
,	O
the	O
performance	O
on	O
VQAv2	O
drops	O
continuously	O
during	O
the	O
course	O
of	O
the	O
training	O
.	O
This	O
indicates	O
that	O
HINT	O
and	O
SCR	O
help	O
forget	O
linguistic	O
priors	O
,	O
which	O
is	O
beneficial	O
for	O
VQA	B-TaskName
-	O
CPv2	O
but	O
not	O
for	O
VQAv2	O
.	O

In	O
order	O
to	O
quantitatively	O
assess	O
visual	B-TaskName
grounding	I-TaskName
,	O
we	O
propose	O
a	O
new	O
metric	O
called	O
:	O
Correctly	O
Predicted	O
but	O
Improperly	O
Grounded	O
(	O
CPIG	O
)	O
:	O
%	O
CP	O
IG	O
=	O
N	O
correct	O
ans	O
,	O
improper	O
grounding	O
N	O
correct	O
ans	O
×	O
100	O
%	O
,	O
which	O
is	O
the	O
number	O
instances	O
for	O
which	O
the	O
most	O
sensitive	O
visual	O
region	O
used	O
to	O
correctly	O
predict	O
the	O
answer	O
is	O
not	O
within	O
top	O
-	O
3	O
most	O
relevant	O
ground	O
truth	O
regions	O
,	O
normalized	O
by	O
the	O
total	O
number	O
of	O
correct	O
predictions	O
.	O
HINT	O
and	O
SCR	O
trained	O
on	O
relevant	O
regions	O
obtained	O
lower	O
CPIG	O
values	O
that	O
other	O
variants	O
(	O
70.24	O
%	O
and	O
80.22	O
%	O
respectively	O
)	O
,	O
indicating	O
they	O
are	O
better	O
than	O
other	O
variants	O
at	O
finding	O
relevant	O
regions	O
.	O
However	O
,	O
these	O
numbers	O
are	O
still	O
high	O
,	O
and	O
show	O
that	O
only	O
29.76	O
%	O
and	O
19.78	O
%	O
of	O
the	O
correct	O
predictions	O
for	O
HINT	O
and	O
SCR	O
were	O
properly	O
grounded	O
.	O
Further	O
analysis	O
is	O
presented	O
in	O
the	O
Appendix	O
.	O

Here	O
,	O
we	O
showed	O
that	O
existing	O
visual	B-TaskName
grounding	I-TaskName
based	O
bias	O
mitigation	O
methods	O
for	O
VQA	B-TaskName
are	O
not	O
working	O
as	O
intended	O
.	O
We	O
found	O
that	O
the	O
accuracy	B-MetricName
improvements	O
stem	O
from	O
a	O
regularization	O
effect	O
rather	O
than	O
proper	O
visual	B-TaskName
grounding	I-TaskName
.	O
We	O
proposed	O
a	O
simple	O
regularization	O
scheme	O
which	O
,	O
despite	O
not	O
requiring	O
additional	O
annotations	O
,	O
rivals	O
state	O
-	O
of	O
-	O
theart	O
accuracy	B-MetricName
.	O
Future	O
visual	B-TaskName
grounding	I-TaskName
methods	O
should	O
be	O
tested	O
with	O
a	O
more	O
comprehensive	O
experimental	O
setup	O
and	O
datasets	O
for	O
proper	O
evaluation	O
.	O

Following	O
(	O
Selvaraju	O
et	O
al	O
,	O
2019	O
)	O
,	O
we	O
report	O
Spearman	O
's	O
rank	O
correlation	O
between	O
network	O
's	O
sensitivity	O
scores	O
and	O
human	O
-	O
based	O
scores	O
in	O
Table	O
A3	O
.	O
For	O
HINT	O
and	O
our	O
zero	O
-	O
out	O
regularizer	O
,	O
we	O
use	O
human	O
-	O
based	O
attention	O
maps	O
.	O
For	O
SCR	O
,	O
we	O
use	O
textual	O
explanation	O
-	O
based	O
scores	O
.	O
We	O
find	O
that	O
HINT	O
trained	O
on	O
human	O
attention	O
maps	O
has	O
the	O
highest	O
correlation	O
coefficients	O
for	O
both	O
datasets	O
.	O
However	O
,	O
compared	O
to	O
baseline	O
,	O
HINT	O
variants	O
trained	O
on	O
random	O
visual	O
cues	O
also	O
show	O
improved	O
correlations	O
.	O
For	O
SCR	O
,	O
we	O
obtain	O
surprising	O
results	O
,	O
with	O
the	O
model	O
trained	O
on	O
irrelevant	O
cues	O
obtaining	O
higher	O
correlation	O
than	O
that	O
trained	O
on	O
relevant	O
visual	O
cues	O
.	O
As	O
expected	O
,	O
applying	O
our	O
regularizer	O
does	O
not	O
improve	O
rank	O
correlation	O
.	O
Since	O
HINT	O
trained	O
on	O
relevant	O
cues	O
obtains	O
the	O
highest	O
correlation	O
values	O
,	O
it	O
does	O
indicate	O
improvement	O
in	O
visual	B-TaskName
grounding	I-TaskName
.	O
However	O
,	O
as	O
we	O
have	O
seen	O
,	O
the	O
improvements	O
in	O
performance	O
can	O
not	O
necessarily	O
be	O
attributed	O
to	O
better	O
overlap	O
with	O
ground	O
truth	O
localizations	O
.	O

Presentation	O
of	O
qualitative	O
examples	O
in	O
visual	B-TaskName
grounding	I-TaskName
models	O
for	O
VQA	B-TaskName
suffers	O
from	O
confirmation	O
bias	O
i.e.	O
,	O
while	O
it	O
is	O
possible	O
to	O
find	O
qualitative	O
samples	O
that	O
look	O
at	O
relevant	O
regions	O
to	O
answer	O
questions	O
properly	O
,	O
it	O
is	O
also	O
possible	O
to	O
find	O
samples	O
that	O
produce	O
correct	O
answers	O
without	O
looking	O
at	O
relevant	O
regions	O
.	O
We	O
present	O
examples	O
for	O
such	O
cases	O
in	O
Fig	O
.	O
A3	O
.	O
We	O
next	O
present	O
a	O
quantitative	O
assessment	O
of	O
visual	B-TaskName
grounding	I-TaskName
,	O
which	O
does	O
not	O
suffer	O
from	O
the	O
confirmation	O
bias	O
.	O

Table	O
A4	O
shows	O
VQA	B-TaskName
accuracy	B-MetricName
for	O
each	O
answer	O
type	O
on	O
VQACPv2	O
's	O
test	O
set	O
.	O
HINT	O
/	O
SCR	O
and	O
our	O
regularizer	O
show	O
large	O
gains	O
in	O
'	O
Yes	O
/	O
No	O
'	O
questions	O
.	O
We	O
hypothesize	O
that	O
the	O
methods	O
help	O
forget	O
linguistic	O
priors	O
,	O
which	O
improves	O
test	O
accuracy	B-MetricName
of	O
such	O
questions	O
.	O
In	O
the	O
train	O
set	O
of	O
VQACPv2	O
,	O
the	O
answer	O
'	O
no	O
'	O
is	O
more	O
frequent	O
than	O
the	O
answer	O
'	O
yes	O
'	O
,	O
tempting	O
the	O
baseline	O
model	O
to	O
answer	O
'	O
yes	O
/	O
no	O
'	O
questions	O
with	O
'	O
no	O
'	O
.	O
However	O
,	O
in	O
the	O
test	O
set	O
,	O
answer	O
'	O
yes	O
'	O
is	O
more	O
frequent	O
.	O
Regularization	O
effects	O
caused	O
by	O
HINT	O
/	O
SCR	O
and	O
our	O
method	O
cause	O
the	O
models	O
to	O
weaken	O
this	O
prior	O
i.e.	O
,	O
reduce	O
the	O
tendency	O
to	O
just	O
predict	O
'	O
no	O
'	O
,	O
which	O
would	O
increase	O
accuracy	B-MetricName
at	O
test	O
because	O
'	O
yes	O
'	O
is	O
more	O
frequent	O
in	O
the	O
test	O
set	O
.	O
Next	O
,	O
all	O
of	O
the	O
methods	O
perform	O
poorly	O
on	O
'	O
Number	O
(	O
Num	O
)	O
'	O
answer	O
type	O
,	O
showing	O
that	O
methods	O
find	O
it	O
difficult	O
to	O
answer	O
questions	O
that	O
are	O
most	O
reliant	O
on	O
correct	O
visual	B-TaskName
grounding	I-TaskName
such	O
as	O
:	O
localizing	O
and	O
counting	O
objects	O
.	O
Finally	O
,	O
we	O
do	O
not	O
observe	O
large	O
improvements	O
in	O
'	O
Other	O
'	O
question	O
type	O
,	O
most	O
likely	O
due	O
to	O
the	O
large	O
number	O
of	O
answers	O
present	O
under	O
this	O
answer	O
type	O
.	O

The	O
Hebrew	O
Universal	B-DatasetName
Dependency	I-DatasetName
Treebank	I-DatasetName
:	O
Past	O
,	O
Present	O
and	O
Future	O

The	O
Hebrew	O
treebank	O
(	O
HTB	O
)	O
,	O
consisting	O
of	O
6221	O
morpho	O
-	O
syntactically	O
annotated	O
newspaper	O
sentences	O
,	O
has	O
been	O
the	O
only	O
resource	O
for	O
training	O
and	O
validating	O
statistical	O
parsers	O
and	O
taggers	O
for	O
Hebrew	O
,	O
for	O
almost	O
two	O
decades	O
now	O
.	O
During	O
these	O
decades	O
,	O
the	O
HTB	O
has	O
gone	O
through	O
a	O
trajectory	O
of	O
automatic	O
and	O
semi	O
-	O
automatic	O
conversions	O
,	O
until	O
arriving	O
at	O
its	O
UDv2	O
form	O
.	O
In	O
this	O
work	O
we	O
manually	O
validate	O
the	O
UDv2	O
version	O
of	O
the	O
HTB	O
,	O
and	O
,	O
according	O
to	O
our	O
findings	O
,	O
we	O
apply	O
scheme	O
changes	O
that	O
bring	O
the	O
UD	B-DatasetName
HTB	O
to	O
the	O
same	O
theoretical	O
grounds	O
as	O
the	O
rest	O
of	O
UD	B-DatasetName
.	O
Our	O
experimental	O
parsing	O
results	O
with	O
UDv2New	O
confirm	O
that	O
improving	O
the	O
coherence	O
and	O
internal	O
consistency	O
of	O
the	O
UD	B-DatasetName
HTB	O
indeed	O
leads	O
to	O
improved	O
parsing	O
performance	O
.	O
At	O
the	O
same	O
time	O
,	O
our	O
analysis	O
demonstrates	O
that	O
there	O
is	O
more	O
to	O
be	O
done	O
at	O
the	O
point	O
of	O
intersection	O
of	O
UD	B-DatasetName
with	O
other	O
linguistic	O
processing	O
layers	O
,	O
in	O
particular	O
,	O
at	O
the	O
points	O
where	O
UD	B-DatasetName
interfaces	O
external	O
morphological	O
and	O
lexical	O
resources	O
.	O

The	O
Hebrew	O
Treebank	O
(	O
HTB	O
)	O
,	O
initially	O
introduced	O
by	O
Sima'an	O
et	O
al	O
(	O
2001	O
)	O
,	O
is	O
the	O
first	O
,	O
and	O
so	O
far	O
only	O
,	O
gold	O
standard	O
for	O
morphologically	O
and	O
syntactically	O
annotated	O
sentences	O
in	O
Modern	O
Hebrew	O
.	O
It	O
was	O
created	O
with	O
the	O
main	O
goal	O
in	O
mind	O
to	O
enable	O
the	O
development	O
of	O
statistical	O
models	O
for	O
morphological	O
and	O
syntactic	O
parsing	O
for	O
Hebrew	O
,	O
but	O
also	O
to	O
facilitate	O
linguistic	O
investigations	O
into	O
the	O
structure	O
and	O
distribution	O
of	O
linguistic	O
Semitic	O
phenomena	O
.	O
The	O
pilot	O
version	O
of	O
Sima'an	O
et	O
al	O
(	O
2001	O
)	O
has	O
been	O
minimal	O
-	O
it	O
consisted	O
of	O
500	O
sentences	O
,	O
morphologically	O
and	O
syntactically	O
annotated	O
by	O
hand	O
.	O
This	O
modest	O
start	O
,	O
however	O
,	O
defined	O
linguistic	O
conventions	O
and	O
annotation	O
principles	O
that	O
would	O
continue	O
to	O
affect	O
many	O
treebank	O
versions	O
derived	O
from	O
the	O
HTB	O
for	O
many	O
years	O
,	O
including	O
the	O
universal	B-DatasetName
dependencies	I-DatasetName
(	O
UD	B-DatasetName
)	O
HTB	O
version	O
.	O
During	O
these	O
two	O
decades	O
,	O
the	O
HTB	O
has	O
expanded	O
from	O
500	O
to	O
6221	O
sentences	O
and	O
changed	O
several	O
forms	O
.	O
The	O
different	O
versions	O
of	O
the	O
treebank	O
reflect	O
different	O
theories	O
and	O
formal	O
representation	O
types	O
,	O
that	O
in	O
turn	O
reflect	O
different	O
,	O
and	O
sometimes	O
contradictory	O
,	O
linguistic	O
annotation	O
principles	O
.	O
The	O
reasons	O
for	O
these	O
differences	O
were	O
sometimes	O
practical	O
,	O
e.g.	O
,	O
a	O
new	O
version	O
was	O
derived	O
to	O
answer	O
an	O
emerging	O
technological	O
need	O
,	O
and	O
sometimes	O
socio	O
-	O
academic	O
,	O
e.g.	O
,	O
because	O
different	O
teams	O
adopted	O
different	O
linguistic	O
theories	O
as	O
their	O
underlying	O
annotation	O
principles	O
.	O
The	O
HTB	O
thus	O
enabled	O
the	O
development	O
of	O
many	O
statistical	O
morphological	O
and	O
syntactic	O
processing	O
models	O
(	O
Adler	O
,	O
2007	O
;	O
Bar	O
-	O
haim	O
et	O
al	O
,	O
2008	O
;	O
Shacham	O
and	O
Wintner	O
,	O
2007	O
;	O
Tsarfaty	O
,	O
2006	O
;	O
Goldberg	O
and	O
Tsarfaty	O
,	O
2008	O
;	O
Goldberg	O
and	O
Elhadad	O
,	O
2009	O
;	O
Tsarfaty	O
,	O
2010	O
;	O
Goldberg	O
andElhadad	O
,	O
2010	O
,	O
2011	O
;	O
More	O
and	O
Tsarfaty	O
,	O
2016	O
;	O
More	O
et	O
al	O
,	O
In	O
Press	O
)	O
,	O
but	O
these	O
models	O
were	O
trained	O
on	O
vastly	O
different	O
versions	O
of	O
the	O
treebank	O
,	O
obeying	O
different	O
theories	O
and	O
annotation	O
schemes	O
,	O
which	O
then	O
rendered	O
the	O
reported	O
results	O
mostly	O
non	O
-	O
comparable	O
.	O
Hebrew	O
dependency	B-TaskName
parsing	I-TaskName
presents	O
an	O
acute	O
version	O
of	O
this	O
syndrome	O
.	O
Studies	O
such	O
as	O
Goldberg	O
and	O
Elhadad	O
(	O
2011	O
)	O
,	O
Tsarfaty	O
et	O
al	O
(	O
2012	O
)	O
,	O
More	O
et	O
al	O
(	O
In	O
Press	O
)	O
,	O
as	O
well	O
as	O
the	O
SPMRL	O
shared	O
tasks	O
(	O
Seddah	O
et	O
al	O
,	O
2013	O
(	O
Seddah	O
et	O
al	O
,	O
,	O
2014	O
,	O
all	O
present	O
attachment	O
scores	O
on	O
Hebrew	O
dependency	B-TaskName
parsing	I-TaskName
.	O
But	O
for	O
reporting	O
these	O
scores	O
they	O
use	O
HTB	O
versions	O
that	O
reflect	O
distinct	O
schemes	O
,	O
sometime	O
reporting	O
different	O
metrics	O
,	O
which	O
makes	O
the	O
numerical	O
comparison	O
between	O
the	O
respective	O
results	O
meaningless	O
(	O
Tsarfaty	O
et	O
al	O
,	O
2011	O
)	O
.	O
This	O
is	O
why	O
the	O
UD	B-DatasetName
initiative	O
comes	O
as	O
a	O
blessing	O
,	O
not	O
only	O
for	O
the	O
cross	O
-	O
linguistic	O
parsing	O
community	O
but	O
also	O
for	O
the	O
Hebrew	O
NLP	O
community	O
-	O
by	O
presenting	O
a	O
unique	O
opportunity	O
to	O
standardize	O
the	O
resources	O
and	O
metrics	O
used	O
for	O
Hebrew	O
parsing	O
.	O
Ideally	O
,	O
the	O
current	O
UDv2	O
version	O
would	O
make	O
for	O
such	O
a	O
standard	O
Hebrew	O
resource	O
.	O
Unfortunately	O
though	O
,	O
many	O
of	O
the	O
conversion	O
processes	O
since	O
Sima	O
'	O
an	O
et	O
al	O
(	O
2001	O
)	O
to	O
the	O
present	O
UDv2	O
have	O
been	O
automatic	O
or	O
semi	O
-	O
automatic	O
,	O
with	O
no	O
point	O
of	O
systematic	O
qualitative	O
validation	O
.	O
This	O
resulted	O
in	O
odd	O
,	O
and	O
sometime	O
plain	O
wrong	O
,	O
dependency	O
structures	O
,	O
with	O
respect	O
to	O
the	O
UD	B-DatasetName
scheme	O
.	O
In	O
this	O
work	O
we	O
take	O
the	O
opportunity	O
to	O
validate	O
the	O
UDv2	O
HTB	O
,	O
by	O
manually	O
going	O
through	O
the	O
published	O
trees	O
,	O
identifying	O
systematic	O
errors	O
or	O
annotation	O
inconsistencies	O
,	O
and	O
locating	O
cases	O
where	O
the	O
annotated	O
structures	O
contradict	O
the	O
UD	B-DatasetName
guidelines	O
(	O
or	O
spirit	O
)	O
.	O
We	O
identified	O
and	O
corrected	O
three	O
main	O
points	O
of	O
failure	O
in	O
the	O
UD	B-DatasetName
HTB	O
:	O
(	O
i	O
)	O
the	O
classification	O
of	O
argument	O
types	O
,	O
deriving	O
from	O
the	O
classification	O
in	O
the	O
original	O
HTB	O
(	O
ii	O
)	O
a	O
mix	O
-	O
up	O
of	O
morphological	O
and	O
syntactic	O
properties	O
,	O
where	O
morphological	O
features	O
serve	O
as	O
syntactic	O
sub	O
-	O
relations	O
and	O
vice	O
versa	O
,	O
and	O
(	O
iii	O
)	O
a	O
mix	O
up	O
of	O
language	O
-	O
specific	O
versus	O
universal	O
phenomena	O
,	O
where	O
label	O
sub	O
-	O
typing	O
is	O
exploited	O
to	O
indicate	O
a	O
supposedly	O
language	O
-	O
specific	O
phenomenon	O
,	O
which	O
in	O
fact	O
has	O
a	O
designated	O
universal	O
label	O
elsewhere	O
.	O
Based	O
on	O
these	O
corrections	O
,	O
we	O
present	O
a	O
revised	O
version	O
of	O
the	O
HTB	O
that	O
we	O
call	O
UDv2New	O
.	O
We	O
use	O
UDv2	O
and	O
UDv2New	O
to	O
train	O
a	O
morphosyntactic	O
parser	O
(	O
More	O
et	O
al	O
,	O
In	O
Press	O
)	O
and	O
provide	O
baseline	O
results	O
on	O
Hebrew	O
UD	B-DatasetName
parsing	O
,	O
in	O
both	O
ideal	O
and	O
realistic	O
scenarios	O
.	O
Comparing	O
our	O
Hebrew	O
parsing	O
results	O
on	O
UDv2	O
and	O
UDv2New	O
,	O
we	O
verify	O
that	O
the	O
improvement	O
of	O
linguistic	O
coherence	O
and	O
annotation	O
consistency	O
has	O
also	O
led	O
to	O
improved	O
parsing	O
performance	O
.	O
Lessons	O
learned	O
from	O
our	O
empirical	O
analysis	O
concern	O
the	O
systematic	O
organization	O
of	O
natural	O
language	O
grammar	O
in	O
UD	B-DatasetName
,	O
and	O
in	O
particular	O
(	O
i	O
)	O
the	O
need	O
to	O
standardize	O
the	O
interface	O
of	O
UD	B-DatasetName
treebanks	O
to	O
external	O
morphological	O
and	O
lexical	O
resources	O
,	O
and	O
(	O
ii	O
)	O
the	O
need	O
to	O
organize	O
the	O
form	O
-	O
function	O
mapping	O
in	O
a	O
language	O
-	O
specific	O
vs.	O
family	O
-	O
specific	O
vs.	O
strictly	O
-	O
universal	O
relations	O
taxonomy	O
,	O
within	O
and	O
across	O
treebanks	O
.	O
The	O
remainder	O
of	O
this	O
paper	O
is	O
organized	O
as	O
follows	O
.	O
In	O
Section	O
2	O
we	O
describe	O
the	O
trajectory	O
of	O
the	O
HTB	O
from	O
its	O
inception	O
to	O
UDv2	O
.	O
In	O
Section	O
3	O
we	O
present	O
our	O
validation	O
process	O
and	O
the	O
scheme	O
changes	O
we	O
applied	O
.	O
In	O
Section	O
4	O
we	O
present	O
raw	O
-	O
to	O
-	O
dependencies	O
Hebrew	O
parsing	O
results	O
and	O
in	O
Section	O
5	O
we	O
share	O
our	O
future	O
plans	O
and	O
lessons	O
learned	O
.	O
Finally	O
,	O
in	O
Section	O
6	O
we	O
conclude	O
.	O

The	O
RR	B-DatasetName
version	O
of	O
the	O
Unified	O
-	O
SD	O
HTB	O
provided	O
the	O
basis	O
for	O
automatically	O
converting	O
the	O
Hebrew	O
trees	O
into	O
UDv1	O
trees	O
.	O
The	O
UD	B-DatasetName
HTB	O
assumes	O
the	O
same	O
segmentation	O
principles	O
as	O
the	O
first	O
edition	O
of	O
the	O
HTB	O
,	O
segmenting	O
off	O
prefixes	O
and	O
suffixes	O
,	O
with	O
the	O
addition	O
of	O
splitting	O
off	O
genitive	O
pronominal	O
clitics	O
from	O
nouns	O
.	O
Goldberg	O
and	O
Tsarfaty	O
(	O
2014	O
)	O
devised	O
an	O
automatic	O
process	O
that	O
chooses	O
a	O
lexical	O
head	O
in	O
each	O
relational	O
network	O
of	O
each	O
constituent	O
in	O
the	O
RR	B-DatasetName
treebank	O
.	O
They	O
also	O
mapped	O
the	O
fine	O
-	O
grained	O
POS	O
categories	O
to	O
the	O
coarse	O
-	O
grained	O
UPOS	O
categories	O
in	O
UD	B-DatasetName
,	O
and	O
remaining	O
POS	O
distinctions	O
in	O
HebLex	O
(	O
HebBinyan	O
,	O
construct	O
-	O
states	O
,	O
etc	O
.	O
)	O
are	O
stored	O
in	O
FEATS	O
.	O
The	O
label	O
set	O
of	O
U	O
-	O
SD	O
was	O
automatically	O
mapped	O
to	O
UD	B-DatasetName
,	O
and	O
relations	O
from	O
U	O
-	O
SD	O
outside	O
of	O
UD	B-DatasetName
were	O
kept	O
as	O
relation	O
:	O
subtype	O
.	O
The	O
conversion	O
of	O
UDv1	O
to	O
UDv2	O
was	O
also	O
done	O
automatically	O
,	O
by	O
augmenting	O
the	O
script	O
of	O
Goldberg	O
and	O
.	O
Points	O
of	O
failure	O
of	O
the	O
UDv1	O
version	O
of	O
the	O
HTB	O
to	O
comply	O
with	O
UDv2	O
were	O
identified	O
by	O
aiming	O
to	O
locate	O
skewed	O
distributions	O
of	O
tags	O
or	O
labels	O
,	O
and	O
they	O
were	O
corrected	O
in	O
the	O
conversion	O
script	O
on	O
a	O
case	O
by	O
case	O
basis	O
.	O
This	O
process	O
has	O
stopped	O
when	O
the	O
treebank	O
complied	O
with	O
the	O
UDv2	O
validation	O
script	O
.	O
The	O
converted	O
HTB	O
is	O
documented	O
on	O
the	O
UD	B-DatasetName
webpage	O
.	O
7	O

Open	O
Clausal	O
Complements	O
.	O
In	O
the	O
UDv2	O
HTB	O
,	O
predicative	O
complements	O
were	O
labeled	O
advmod	O
when	O
adjectival	O
.	O
Following	O
the	O
UDv2	O
guidelines	O
,	O
we	O
label	O
them	O
xcomp	O
,	O
as	O
they	O
are	O
subordinated	O
predicates	O
,	O
after	O
all	O
,	O
even	O
if	O
not	O
verbal	O
.	O
Argument	O
iobj	O
vs.	O
obl	O
.	O
Some	O
UD	B-DatasetName
definitions	O
stand	O
in	O
clear	O
contrast	O
with	O
the	O
canonical	O
syntactic	O
analysis	O
of	O
Hebrew	O
.	O
Perhaps	O
the	O
most	O
salient	O
case	O
is	O
of	O
core	O
arguments	O
.	O
The	O
canonical	O
view	O
of	O
Hebrew	O
core	O
arguments	O
(	O
Coffin	O
and	O
Bolozky	O
(	O
2005	O
)	O
p.	O
290	O
)	O
is	O
of	O
a	O
direct	O
object	O
,	O
marked	O
by	O
an	O
accusative	O
case	O
when	O
definite	O
,	O
and	O
an	O
indirect	O
object	O
,	O
marked	O
by	O
an	O
oblique	O
case	O
marker	O
when	O
a	O
pronoun	O
,	O
and	O
preceded	O
by	O
a	O
preposition	O
when	O
common	O
or	O
proper	O
noun	O
.	O
UDv2	O
dedicates	O
an	O
iobj	O
(	O
indirect	O
object	O
)	O
relation	O
to	O
secondary	O
core	O
arguments	O
which	O
are	O
not	O
preceded	O
by	O
prepositions	O
,	O
and	O
arguments	O
which	O
do	O
follow	O
a	O
preposition	O
are	O
labeled	O
obl	O
,	O
whether	O
core	O
or	O
non	O
-	O
core	O
.	O
We	O
revised	O
the	O
labels	O
accordingly	O
.	O
Predicate	O
types	O
:	O
the	O
case	O
of	O
auxiliaries	O
As	O
part	O
of	O
the	O
shift	O
towards	O
a	O
lexically	O
-	O
driven	O
analysis	O
,	O
structural	O
changes	O
were	O
made	O
to	O
sentences	O
containing	O
auxiliary	O
elements	O
and	O
copulas	O
.	O
There	O
are	O
three	O
main	O
sets	O
of	O
these	O
:	O
(	O
i	O
)	O
Auxiliary	O
elements	O
marking	O
modality	O
,	O
(	O
ii	O
)	O
Auxiliary	O
verbs	O
which	O
mostly	O
mark	O
habituality	O
,	O
but	O
occasionally	O
participate	O
in	O
negation	O
or	O
tense	O
inflection	O
when	O
the	O
predicate	O
has	O
no	O
past	O
/	O
future	O
form	O
,	O
and	O
(	O
iii	O
)	O
Positive	O
or	O
negative	O
copulars	O
.	O
Modals	O
do	O
not	O
constitute	O
any	O
uniform	O
syntactic	O
class	O
in	O
Hebrew	O
,	O
and	O
there	O
is	O
an	O
ongoing	O
debate	O
as	O
to	O
the	O
POS	O
of	O
each	O
modal	O
expression	O
(	O
cf	O
.	O
Netzer	O
et	O
al	O
(	O
2007	O
)	O
)	O
.	O
In	O
line	O
with	O
Netzer	O
et	O
al	O
s	O
conclusion	O
,	O
these	O
are	O
tagged	O
as	O
AUX	O
in	O
the	O
UD	B-DatasetName
HTB	O
.	O
In	O
UDv2	O
,	O
the	O
modal	O
served	O
as	O
the	O
head	O
of	O
the	O
clause	O
,	O
while	O
the	O
following	O
predicate	O
was	O
labeled	O
xcomp	O
,	O
as	O
it	O
is	O
consistently	O
realized	O
in	O
Hebrew	O
in	O
infinitive	O
form	O
.	O
As	O
of	O
UDv2New	O
,	O
those	O
modals	O
which	O
are	O
tagged	O
as	O
AUX	O
are	O
also	O
labeled	O
aux	O
,	O
and	O
the	O
subsequent	O
predicate	O
receives	O
the	O
label	O
which	O
was	O
attributed	O
to	O
the	O
modal	O
.	O
See	O
Table	O
1	O
.	O
In	O
the	O
opposite	O
direction	O
,	O
auxiliary	O
verbs	O
,	O
such	O
as	O
the	O
ones	O
in	O
sets	O
ii	O
and	O
iii	O
were	O
tagged	O
as	O
VERB	O
.	O
As	O
the	O
UDv2	O
scheme	O
dedicates	O
an	O
AUX	O
tag	O
to	O
function	O
words	O
in	O
auxiliary	O
functions	O
even	O
when	O
they	O
are	O
verbs	O
,	O
we	O
changed	O
them	O
to	O
AUX	O
as	O
well	O
in	O
UDv2New	O
.	O
Finally	O
,	O
consistency	O
across	O
sets	O
ii	O
and	O
iii	O
was	O
achieved	O
by	O
unifying	O
the	O
labeling	O
of	O
copular	O
verbs	O
as	O
cop	O
regardless	O
of	O
their	O
inflection	O
,	O
whereas	O
previous	O
versions	O
labeled	O
past	O
and	O
future	O
inflections	O
of	O
copular	O
verbs	O
as	O
aux	O
.	O

As	O
UD	B-DatasetName
aspires	O
to	O
present	O
a	O
set	O
of	O
tags	O
which	O
are	O
relevant	O
to	O
as	O
many	O
languages	O
as	O
possible	O
,	O
natu	O
-	O
11	O
All	O
analyses	O
are	O
visualized	O
in	O
the	O
supp	O
.	O
materials	O
.	O
rally	O
many	O
language	O
-	O
specific	O
phenomena	O
are	O
left	O
unanswered	O
.	O
To	O
allow	O
representation	O
of	O
these	O
,	O
the	O
UD	B-DatasetName
scheme	O
allows	O
for	O
sub	O
-	O
relations	O
in	O
the	O
form	O
of	O
relation	O
:	O
subtype	O
,	O
as	O
exemplified	O
above	O
.	O
However	O
,	O
although	O
originally	O
aiming	O
toward	O
coverage	O
of	O
language	O
-	O
specific	O
phenomena	O
,	O
this	O
structure	O
can	O
be	O
frequently	O
seen	O
as	O
a	O
subtype	O
of	O
relation	O
which	O
is	O
present	O
in	O
many	O
languages	O
(	O
e.g.	O
nsubj	O
:	O
pass	O
,	O
which	O
is	O
in	O
use	O
for	O
subjects	O
of	O
passive	O
sentences	O
-	O
not	O
unique	O
to	O
any	O
one	O
language	O
or	O
even	O
a	O
family	O
of	O
languages	O
)	O
.	O
In	O
our	O
revision	O
to	O
adhere	O
to	O
UDv2	O
guidelines	O
,	O
we	O
tried	O
as	O
much	O
as	O
possible	O
to	O
narrow	O
the	O
use	O
of	O
relation	O
:	O
subtype	O
to	O
Hebrew	O
-	O
specific	O
phenomena	O
,	O
eliminating	O
any	O
hierarchical	O
structure	O
of	O
dependency	O
relations	O
.	O
As	O
a	O
result	O
,	O
the	O
following	O
subtypes	O
were	O
reduced	O
to	O
their	O
parent	O
relation	O
:	O
(	O
i	O
)	O
det	B-DatasetName
:	O
quant	O
,	O
originally	O
marking	O
an	O
arbitrary	O
subset	O
of	O
existential	O
quantifiers	O
,	O
was	O
reduced	O
to	O
simply	O
det	B-DatasetName
,	O
and	O
(	O
ii	O
)	O
advmod	O
:	O
phrase	O
,	O
originally	O
marking	O
multi	O
-	O
word	O
adverbials	O
,	O
were	O
re	O
-	O
structured	O
as	O
advmod+fixed	O
,	O
in	O
line	O
with	O
the	O
UD	B-DatasetName
guidelines	O
for	O
multi	O
-	O
word	O
-	O
expressions	O
.	O
From	O
conj	O
:	O
discourse	O
to	O
parataxis	O
An	O
interesting	O
case	O
is	O
with	O
labels	O
not	O
used	O
at	O
all	O
in	O
the	O
older	O
versions	O
of	O
the	O
UD	B-DatasetName
HTB	O
,	O
while	O
language	O
-	O
specific	O
labels	O
stand	O
to	O
mark	O
their	O
function	O
.	O
The	O
UD	B-DatasetName
label	O
parataxis	O
,	O
for	O
instance	O
,	O
describes	O
a	O
relation	O
between	O
two	O
(	O
or	O
more	O
)	O
sentences	O
which	O
are	O
syntactically	O
independent	O
(	O
i.e.	O
do	O
not	O
stand	O
in	O
subordination	O
or	O
conjunction	O
relation	O
to	O
one	O
another	O
)	O
,	O
but	O
are	O
thematically	O
connected	O
,	O
and	O
consequently	O
punctuated	O
as	O
the	O
same	O
sentence	O
.	O
Previously	O
,	O
this	O
relation	O
was	O
labeled	O
in	O
the	O
HTB	O
as	O
conj	O
:	O
discourse	O
,	O
simply	O
classifying	O
conjunctions	O
that	O
are	O
not	O
explicitly	O
marked	O
as	O
of	O
type	O
discourse	O
.	O
In	O
our	O
revised	O
version	O
,	O
we	O
comply	O
with	O
UD	B-DatasetName
guidelines	O
and	O
label	O
this	O
relation	O
'	O
parataxis	O
'	O
.	O
From	O
PART	O
to	O
ADP	O
The	O
accusative	O
and	O
possessive	O
case	O
markers	O
in	O
Hebrew	O
,	O
AT	O
and	O
FL	O
respectively	O
,	O
are	O
realised	O
as	O
separate	O
tokens	O
,	O
as	O
opposed	O
to	O
some	O
other	O
case	O
markers	O
,	O
which	O
prefix	O
the	O
following	O
nouns	O
.	O
Furthermore	O
,	O
a	O
possessive	O
case	O
marker	O
may	O
also	O
morphologically	O
suffix	O
the	O
noun	O
,	O
whether	O
instead	O
of	O
or	O
in	O
addition	O
to	O
the	O
above	O
-	O
mentioned	O
particle	O
.	O
In	O
older	O
versions	O
of	O
HTB	O
,	O
while	O
preposition	O
(	O
whether	O
standalone	O
or	O
not	O
)	O
were	O
tagged	O
IN	O
,	O
the	O
accusative	O
case	O
marker	O
was	O
tagged	O
AT	O
and	O
the	O
possessive	O
case	O
marker	O
was	O
tagged	O
POSS	O
.	O
As	O
a	O
result	O
,	O
automatic	O
conversions	O
led	O
to	O
converting	O
IN	O
to	O
ADP	O
across	O
the	O
board	O
,	O
while	O
AT	O
and	O
FL	O
were	O
converted	O
into	O
PART	O
.	O
As	O
there	O
is	O
no	O
real	O
difference	O
between	O
AT	O
and	O
FL	O
and	O
prepositions	O
according	O
to	O
the	O
UDv2	O
scheme	O
,	O
and	O
as	O
they	O
are	O
in	O
no	O
way	O
particles	O
,	O
we	O
converted	O
them	O
into	O
ADP	O
.	O

Goal	O
:	O
We	O
wish	O
to	O
examine	O
the	O
empirical	O
impact	O
of	O
our	O
effort	O
to	O
correct	O
the	O
treebank	O
and	O
retain	O
linguistic	O
(	O
as	O
well	O
as	O
cross	O
-	O
treebank	O
)	O
coherence	O
in	O
its	O
annotation	O
scheme	O
.	O
Indeed	O
,	O
ease	O
of	O
parsing	O
should	O
not	O
be	O
the	O
indication	O
for	O
selecting	O
one	O
scheme	O
over	O
another	O
,	O
but	O
the	O
hypothesis	O
is	O
that	O
,	O
within	O
one	O
and	O
the	O
same	O
set	O
of	O
guidelines	O
,	O
a	O
version	O
that	O
presents	O
better	O
coherence	O
and	O
consistency	O
will	O
also	O
be	O
more	O
suitable	O
for	O
statistical	O
training	O
and	O
will	O
yield	O
better	O
results	O
.	O
Settings	O
:	O
To	O
gauge	O
the	O
effect	O
of	O
our	O
revision	O
we	O
conducted	O
two	O
sets	O
of	O
experiments	O
:	O
one	O
with	O
the	O
HTB	O
UDv2	O
version	O
used	O
in	O
the	O
recent	O
shared	O
task	O
,	O
and	O
another	O
our	O
revised	O
UDv2New	O
.	O
We	O
use	O
the	O
syntactic	O
evaluation	O
script	O
provided	O
by	O
the	O
CoNLL	O
shared	O
task	O
2018	O
.	O
We	O
train	O
on	O
the	O
portion	O
defined	O
as	O
train	O
set	O
and	O
report	O
results	O
on	O
the	O
dev	O
set	O
.	O
For	O
training	O
and	O
parsing	O
we	O
used	O
yap	O
,	O
13	O
a	O
transitionbased	O
morphosyntactic	O
parser	O
written	O
in	O
go	O
,	O
which	O
includes	O
a	O
morphological	O
analyzer	O
,	O
a	O
morphological	O
disambiguator	O
,	O
and	O
syntactic	O
parser	O
.	O
In	O
previous	O
work	O
yap	O
was	O
shown	O
to	O
obtain	O
state	O
of	O
the	O
art	O
results	O
on	O
Hebrew	O
parsing	O
using	O
the	O
SPMRL	O
version	O
of	O
the	O
treebank	O
(	O
More	O
et	O
al	O
,	O
In	O
Press	O
)	O
.	O
Here	O
we	O
report	O
its	O
performance	O
on	O
the	O
UD	B-DatasetName
HTB	O
.	O
Scenarios	O
:	O
Because	O
of	O
its	O
rich	O
morphology	O
and	O
orthographic	O
convention	O
to	O
attach	O
or	O
fuse	O
adpositions	O
and	O
pronominals	O
onto	O
open	O
-	O
class	O
categories	O
,	O
there	O
is	O
severe	O
ambiguity	O
in	O
the	O
morphological	B-TaskName
analysis	I-TaskName
of	O
the	O
Hebrew	O
input	O
tokens	O
.	O
This	O
is	O
further	O
magnified	O
by	O
the	O
lack	O
of	O
diacritics	O
in	O
Hebrew	O
written	O
texts	O
.	O
Hence	O
,	O
it	O
is	O
unknown	O
upfront	O
how	O
many	O
morphemes	O
(	O
in	O
the	O
HTB	O
terminology	O
)	O
or	O
syntactic	O
words	O
(	O
in	O
the	O
UD	B-DatasetName
terminology	O
)	O
are	O
in	O
the	O
space	O
-	O
delimited	O
tokens	O
.	O
We	O
examine	O
two	O
kinds	O
of	O
scenarios	O
:	O
ideal	O
:	O
assuming	O
gold	O
morphological	B-TaskName
analysis	I-TaskName
and	O
disambiguation	O
given	O
by	O
an	O
oracle	O
.	O
realistic	O
:	O
assuming	O
automatically	O
predicted	O
morphological	B-TaskName
analysis	I-TaskName
and	O
disambiguation	O
.	O
We	O
use	O
yap	O
for	O
predicting	O
morphological	B-TaskName
analysis	I-TaskName
(	O
MA	O
)	O
and	O
morphological	B-TaskName
disambiguation	I-TaskName
(	O
More	O
,	O
2016	O
)	O
,	O
and	O
we	O
contrast	O
the	O
use	O
of	O
a	O
data	O
-	O
driven	O
lexicon	O
baselinelex	O
with	O
an	O
external	O
broad	O
-	O
coverage	O
lexicon	O
HebLex	O
.	O
To	O
gauge	O
the	O
effect	O
of	O
the	O
lexical	O
coverage	O
of	O
the	O
morphological	O
resource	O
,	O
we	O
contrast	O
each	O
variant	O
with	O
an	O
infused	O
scenario	O
,	O
where	O
the	O
correct	O
analysis	O
is	O
injected	O
into	O
the	O
lattice	O
.	O
Note	O
that	O
the	O
input	O
in	O
the	O
infused	O
cases	O
is	O
still	O
high	O
as	O
there	O
are	O
many	O
MA	O
alternatives	O
.	O
However	O
,	O
the	O
correct	O
morphological	B-TaskName
disambiguation	I-TaskName
is	O
guaranteed	O
to	O
be	O
one	O
of	O
the	O
morphological	O
MA	O
provided	O
to	O
the	O
system	O
as	O
input	O
.	O
Results	O
:	O
Table	O
2	O
shows	O
the	O
parsing	O
results	O
in	O
an	O
ideal	O
scenario	O
,	O
assuming	O
gold	O
morphology	O
.	O
Here	O
we	O
see	O
that	O
there	O
is	O
a	O
consistent	O
improvement	O
for	O
all	O
metrics	O
.	O
This	O
supports	O
our	O
conjecture	O
that	O
a	O
more	O
consistent	O
and	O
coherent	O
annotation	O
of	O
the	O
treebank	O
will	O
benefit	O
parsing	O
,	O
and	O
it	O
corroborates	O
a	O
wider	O
conjecture	O
,	O
that	O
,	O
when	O
it	O
comes	O
to	O
supervised	O
learning	O
,	O
the	O
quality	O
of	O
the	O
annotated	O
data	O
is	O
as	O
important	O
as	O
the	O
learning	O
algorithm	O
(	O
and	O
maybe	O
more	O
important	O
)	O
.	O
Table	O
3	O
shows	O
the	O
parsing	O
results	O
in	O
realistic	O
scenarios	O
,	O
where	O
we	O
assume	O
automatically	O
predicted	O
morphological	B-TaskName
analysis	I-TaskName
and	O
disambiguation	O
.	O
As	O
expected	O
,	O
the	O
results	O
substantially	O
drop	O
relative	O
to	O
the	O
ideal	O
scenario	O
.	O
Also	O
expected	O
is	O
the	O
result	O
that	O
assuming	O
an	O
external	O
broad	O
-	O
coverage	O
lexicon	O
substantially	O
improves	O
the	O
results	O
relative	O
to	O
a	O
data	O
-	O
driven	O
lexicon	O
learned	O
from	O
the	O
treebank	O
.	O
The	O
result	O
that	O
seems	O
less	O
expected	O
here	O
is	O
that	O
,	O
as	O
opposed	O
to	O
the	O
ideal	O
scenario	O
,	O
we	O
see	O
no	O
improvement	O
in	O
the	O
results	O
of	O
UDv2New	O
relative	O
to	O
UDv2	O
.	O
For	O
some	O
of	O
the	O
metrics	O
the	O
results	O
slightly	O
drop	O
.	O
This	O
drop	O
could	O
be	O
either	O
due	O
to	O
parser	O
errors	O
,	O
or	O
due	O
to	O
the	O
lack	O
of	O
lexical	O
coverage	O
of	O
the	O
lexicon	O
with	O
respect	O
to	O
our	O
revised	O
UDv2New	O
scheme	O
.	O
To	O
test	O
this	O
,	O
we	O
execute	O
an	O
infused	O
scenario	O
where	O
the	O
morphological	B-TaskName
analysis	I-TaskName
lattices	O
are	O
guaranteed	O
to	O
also	O
include	O
the	O
correct	O
analysis	O
.	O
Here	O
we	O
see	O
a	O
substantial	O
improvement	O
for	O
both	O
types	O
of	O
lexica	O
,	O
on	O
all	O
the	O
different	O
metrics	O
,	O
for	O
the	O
UDv2New	O
version	O
.	O
This	O
result	O
suggests	O
that	O
the	O
drop	O
has	O
indeed	O
been	O
due	O
to	O
the	O
insufficient	O
lexical	O
coverage	O
of	O
the	O
resources	O
,	O
or	O
due	O
to	O
mismatches	O
between	O
the	O
lexicon	O
and	O
the	O
new	O
scheme	O
.	O
As	O
far	O
as	O
the	O
statistical	O
components	O
for	O
morphological	O
and	O
syntactic	O
analysis	O
and	O
disambiguation	O
go	O
,	O
the	O
revised	O
version	O
helps	O
the	O
parser	O
obtain	O
better	O
disambiguation	O
,	O
in	O
line	O
of	O
our	O
results	O
in	O
the	O
gold	O
experiments	O
.	O

The	O
original	O
HTB	O
(	O
Sima'an	O
et	O
al	O
,	O
2001	O
;	O
Guthmann	O
et	O
al	O
,	O
2008	O
)	O
has	O
seen	O
many	O
revisions	O
all	O
of	O
which	O
executed	O
automatically	O
,	O
or	O
semi	O
-	O
automatically	O
.	O
Our	O
endeavor	O
here	O
has	O
been	O
to	O
manually	O
verify	O
the	O
current	O
version	O
of	O
the	O
UD	B-DatasetName
HTB	O
resulting	O
analyses	O
,	O
and	O
to	O
correct	O
lingering	O
errors	O
.	O
Apart	O
from	O
being	O
linguistically	O
justified	O
,	O
this	O
process	O
has	O
proven	O
to	O
be	O
also	O
empirically	O
valuable	O
,	O
as	O
indeed	O
this	O
revision	O
has	O
led	O
to	O
a	O
improvement	O
in	O
parsing	O
results	O
.	O
Much	O
work	O
is	O
still	O
needed	O
in	O
order	O
to	O
bring	O
the	O
level	O
of	O
performance	O
to	O
be	O
adequate	O
for	O
downstream	O
applications	O
,	O
in	O
particular	O
in	O
realistic	O
scenarios	O
.	O
We	O
conjecture	O
that	O
in	O
order	O
to	O
obtain	O
decent	O
performance	O
,	O
the	O
work	O
on	O
the	O
treebank	O
should	O
be	O
complemented	O
by	O
adapting	O
language	O
-	O
specific	O
lexica	O
to	O
the	O
set	O
of	O
guidelines	O
for	O
word	O
segmentation	O
and	O
for	O
representing	O
morphology	O
,	O
as	O
defined	O
by	O
UD	B-DatasetName
.	O
Even	O
when	O
external	O
lexica	O
assumes	O
the	O
same	O
labeling	O
scheme	O
as	O
UD	B-DatasetName
,	O
gaps	O
between	O
the	O
theories	O
underlying	O
the	O
development	O
of	O
these	O
resources	O
could	O
lead	O
to	O
lack	O
of	O
coverage	O
that	O
substantially	O
harms	O
parsing	O
performance	O
.	O
Additional	O
lessons	O
learned	O
from	O
our	O
manual	O
verification	O
process	O
have	O
to	O
do	O
with	O
the	O
organization	O
of	O
morphological	O
features	O
and	O
syntactic	O
subtypes	O
within	O
the	O
HTB	O
and	O
in	O
the	O
UD	B-DatasetName
treebanks	O
collection	O
in	O
general	O
.	O
In	O
the	O
HTB	O
UDv2	O
,	O
there	O
appeared	O
to	O
be	O
a	O
mix	O
between	O
the	O
linguistic	O
notions	O
expressed	O
using	O
these	O
two	O
mechanisms	O
.	O
For	O
example	O
,	O
subtypes	O
were	O
sometimes	O
used	O
to	O
indicate	O
morphological	O
features	O
(	O
see	O
the	O
case	O
for	O
acl	O
:	O
inf	O
)	O
while	O
the	O
features	O
column	O
is	O
exploited	O
to	O
express	O
syntactic	O
properties	O
.	O
We	O
argue	O
that	O
clearer	O
guidelines	O
are	O
needed	O
in	O
the	O
general	O
UD	B-DatasetName
scheme	O
,	O
instructing	O
directly	O
what	O
kind	O
of	O
linguistic	O
information	O
should	O
go	O
where	O
,	O
by	O
which	O
formal	O
means	O
.	O
Furthermore	O
,	O
it	O
seems	O
to	O
us	O
that	O
the	O
languagespecific	O
mechanisms	O
are	O
exploited	O
for	O
expressing	O
phenomena	O
that	O
could	O
potentially	O
be	O
crosslinguistic	O
,	O
or	O
at	O
least	O
shared	O
by	O
a	O
language	O
family	O
.	O
An	O
example	O
to	O
this	O
is	O
the	O
feature	O
HebBinyan	O
in	O
the	O
UD	B-DatasetName
HTB	O
,	O
which	O
stores	O
the	O
value	O
of	O
the	O
morphological	O
template	O
of	O
the	O
verb	O
.	O
The	O
phenomenon	O
of	O
Binyan	O
(	O
a	O
root	O
-	O
template	O
construction	O
)	O
is	O
clearly	O
not	O
Hebrew	O
specific	O
-	O
in	O
fact	O
all	O
Semitic	O
languages	O
have	O
Binyanim	O
(	O
morphological	O
constructions	O
)	O
in	O
their	O
grammar	O
,	O
so	O
we	O
see	O
no	O
good	O
reason	O
for	O
not	O
unifying	O
this	O
feature	O
across	O
the	O
Semitic	O
sub	O
-	O
family	O
.	O
Same	O
goes	O
with	O
marking	O
construct	O
state	O
nouns	O
,	O
a	O
phenomenon	O
that	O
extends	O
beyond	O
Semitic	O
languages	O
,	O
and	O
is	O
currently	O
marked	O
differently	O
in	O
each	O
language	O
(	O
Hebrew	O
,	O
Arabic	O
,	O
Persian	O
,	O
etc	O
.	O
)	O
.	O
We	O
propose	O
that	O
the	O
next	O
major	O
revision	O
of	O
the	O
UD	B-DatasetName
treebank	O
scheme	O
could	O
ideally	O
focus	O
on	O
the	O
universal	O
organization	O
of	O
the	O
grammar	O
,	O
and	O
will	O
center	O
around	O
these	O
themes	O
:	O
subtypes	O
:	O
A	O
universal	O
inventory	O
and	O
management	O
of	O
the	O
sub	O
-	O
label	O
system	O
which	O
will	O
define	O
what	O
linguistic	O
phenomena	O
can	O
count	O
as	O
subtype	O
of	O
a	O
label	O
,	O
and	O
will	O
maintain	O
crosslinguistic	O
consistency	O
in	O
its	O
use	O
for	O
shared	O
phenomena	O
.	O
features	O
:	O
A	O
universal	O
inventory	O
and	O
management	O
of	O
features	O
which	O
will	O
define	O
what	O
can	O
count	O
as	O
a	O
feature	O
,	O
and	O
will	O
foster	O
crosslinguistic	O
reuse	O
.	O
lexical	O
resources	O
:	O
For	O
languages	O
that	O
have	O
external	O
lexica	O
,	O
especially	O
in	O
the	O
case	O
of	O
morphologically	O
rich	O
and	O
resource	O
scarce	O
languages	O
,	O
an	O
effort	O
is	O
needed	O
to	O
verify	O
that	O
the	O
labeling	O
scheme	O
theoretical	O
guidelines	O
underlying	O
lexica	O
are	O
harmonized	O
with	O
the	O
UD	B-DatasetName
guidelines	O
.	O
Such	O
lexica	O
can	O
be	O
made	O
available	O
via	O
the	O
CoNLL	O
-	O
UL	O
format	O
(	O
More	O
et	O
al	O
,	O
2018	O
)	O
to	O
benefit	O
the	O
entire	O
UD	B-DatasetName
community	O
.	O
semantic	O
applications	O
:	O
in	O
addition	O
to	O
aligning	O
lexical	O
resources	O
,	O
it	O
is	O
important	O
to	O
advance	O
the	O
usability	O
of	O
UD	B-DatasetName
in	O
down	O
-	O
stream	O
application	O
scenarios	O
,	O
by	O
making	O
available	O
the	O
additional	O
layer	O
of	O
enhanced	O
dependencies	O
.	O

In	O
this	O
paper	O
we	O
describe	O
the	O
long	O
and	O
multiphased	O
process	O
of	O
coming	O
-	O
into	O
-	O
existence	O
of	O
the	O
Hebrew	O
version	O
of	O
the	O
HTB	O
.	O
Most	O
of	O
the	O
process	O
has	O
consisted	O
of	O
automatic	O
conversions	O
between	O
different	O
schemes	O
.	O
In	O
this	O
work	O
we	O
manually	O
verified	O
the	O
recent	O
UD	B-DatasetName
HTB	O
version	O
and	O
corrected	O
lingering	O
errors	O
.	O
The	O
revised	O
version	O
is	O
more	O
linguistically	O
and	O
cross	O
-	O
linguistically	O
consistent	O
and	O
obtains	O
better	O
parsing	O
results	O
in	O
scenarios	O
that	O
are	O
not	O
dependent	O
on	O
the	O
coverage	O
of	O
external	O
lexica	O
.	O
Our	O
future	O
plans	O
include	O
a	O
comprehensive	O
revision	O
of	O
the	O
lexical	O
and	O
morphological	O
resources	O
associated	O
with	O
the	O
UD	B-DatasetName
scheme	O
,	O
to	O
improve	O
the	O
empirical	O
parsing	O
results	O
in	O
realistic	O
scenarios	O
,	O
and	O
the	O
addition	O
of	O
enhanced	O
dependencies	O
,	O
which	O
would	O
be	O
more	O
adequate	O
for	O
downstream	O
semantic	O
tasks	O
.	O

Language	O
models	O
are	O
trained	O
only	O
on	O
text	O
despite	O
the	O
fact	O
that	O
humans	O
learn	O
their	O
first	O
language	O
in	O
a	O
highly	O
interactive	O
and	O
multimodal	O
environment	O
where	O
the	O
first	O
set	O
of	O
learned	O
words	O
are	O
largely	O
concrete	O
,	O
denoting	O
physical	O
entities	O
and	O
embodied	O
states	O
.	O
To	O
enrich	O
language	O
models	O
with	O
some	O
of	O
this	O
missing	O
experience	O
,	O
we	O
leverage	O
two	O
sources	O
of	O
information	O
:	O
(	O
1	O
)	O
the	O
Lancaster	O
Sensorimotor	O
norms	O
,	O
which	O
provide	O
ratings	O
(	O
means	O
and	O
standard	O
deviations	O
)	O
for	O
over	O
40	O
,	O
000	O
English	O
words	O
along	O
several	O
dimensions	O
of	O
embodiment	O
,	O
and	O
which	O
capture	O
the	O
extent	O
to	O
which	O
something	O
is	O
experienced	O
across	O
11	O
different	O
sensory	O
modalities	O
,	O
and	O
(	O
2	O
)	O
vectors	O
from	O
coefficients	O
of	O
binary	O
classifiers	O
trained	O
on	O
images	O
for	O
the	O
BERT	B-MethodName
vocabulary	O
.	O
We	O
pre	O
-	O
trained	O
the	O
ELECTRA	B-MethodName
model	O
and	O
fine	O
-	O
tuned	O
the	O
RoBERTa	B-MethodName
model	O
with	O
these	O
two	O
sources	O
of	O
information	O
then	O
evaluate	O
using	O
the	O
established	O
GLUE	B-DatasetName
benchmark	O
and	O
the	O
Visual	B-TaskName
Dialog	I-TaskName
benchmark	O
.	O
We	O
find	O
that	O
enriching	O
language	O
models	O
with	O
the	O
Lancaster	O
norms	O
and	O
image	O
vectors	O
improves	O
results	O
in	O
both	O
tasks	O
,	O
with	O
some	O
implications	O
for	O
robust	O
language	O
models	O
that	O
capture	O
holistic	O
linguistic	O
meaning	O
in	O
a	O
language	O
learning	O
context	O
.	O

Children	O
learn	O
their	O
first	O
spoken	O
language	O
in	O
a	O
highly	O
interactive	O
setting	O
where	O
generally	O
the	O
first	O
words	O
children	O
learn	O
are	O
concrete	O
words	O
that	O
denote	O
physical	O
objects	O
,	O
which	O
is	O
an	O
important	O
developmental	O
step	O
in	O
child	O
first	O
language	B-TaskName
acquisition	I-TaskName
(	O
Kuperman	O
et	O
al	O
,	O
2012a	O
;	O
McCune	O
,	O
2008	O
;	O
Clark	O
,	O
2013	O
)	O
.	O
This	O
is	O
partly	O
because	O
handling	O
the	O
Symbol	O
Grounding	O
Problem	O
-	O
the	O
ablity	O
to	O
connect	O
symbolic	O
knowledge	O
of	O
language	O
with	O
representations	O
of	O
the	O
physical	O
world	O
(	O
Harnad	O
,	O
1990	O
)	O
-	O
must	O
take	O
place	O
before	O
children	O
learn	O
more	O
abstract	O
concepts	O
later	O
in	O
their	O
cognitive	O
development	O
(	O
Borghi	O
et	O
al	O
,	O
2019	O
;	O
Ponari	O
et	O
al	O
,	O
2018	O
)	O
.	O
Importantly	O
,	O
the	O
physical	O
world	O
is	O
not	O
just	O
the	O
visual	O
world	O
;	O
children	O
learn	O
that	O
words	O
ground	O
into	O
proprioperceptive	O
states	O
(	O
e.g.	O
,	O
a	O
hand	O
grasp	O
around	O
an	O
object	O
has	O
specific	O
muscle	O
activations	O
tied	O
to	O
the	O
word	O
grab	O
)	O
,	O
interoceptive	O
states	O
(	O
i.e.	O
,	O
affect	O
and	O
valence	O
)	O
,	O
as	O
well	O
as	O
all	O
other	O
sensory	O
modalities	O
(	O
e.g.	O
,	O
the	O
word	O
stinky	O
grounds	O
into	O
olfactory	O
,	O
the	O
word	O
loud	O
grounds	O
into	O
auditory	O
)	O
.	O
These	O
claims	O
are	O
evidenced	O
in	O
a	O
large	O
body	O
of	O
child	O
development	O
and	O
cognitive	O
science	O
literature	O
.	O
Smith	O
and	O
Gasser	O
(	O
2005	O
)	O
,	O
for	O
example	O
,	O
identified	O
that	O
babies	O
'	O
experience	O
of	O
the	O
world	O
is	O
profoundly	O
multimodal	O
:	O
babies	O
live	O
in	O
a	O
physical	O
world	O
full	O
of	O
rich	O
regularities	O
that	O
organize	O
perception	O
,	O
action	O
and	O
thought	O
;	O
babies	O
learn	O
in	O
a	O
social	O
world	O
to	O
learn	O
a	O
shared	O
linguistic	O
communicative	O
system	O
that	O
is	O
symbolic	O
.	O
Furthermore	O
,	O
a	O
growing	O
body	O
of	O
literature	O
from	O
linguistics	O
and	O
computational	O
linguistics	O
makes	O
a	O
strong	O
case	O
that	O
the	O
process	O
of	O
language	O
learning	O
(	O
indeed	O
,	O
general	O
human	O
cognition	O
)	O
is	O
embodied	O
,	O
interactive	O
,	O
and	O
enacted	O
;	O
i.e.	O
,	O
movement	O
in	O
the	O
world	O
is	O
required	O
(	O
Pulvermüller	O
,	O
1999	O
;	O
Lakoff	O
and	O
Johnson	O
,	O
1999	O
;	O
Barsalou	O
,	O
2008	O
;	O
Johnson	O
,	O
2008	O
;	O
Smith	O
and	O
Samuelson	O
,	O
2009	O
;	O
Di	O
Paolo	O
et	O
al	O
,	O
2018	O
;	O
Bisk	O
et	O
al	O
,	O
2020	O
)	O
;	O
see	O
also	O
the	O
prior	O
work	O
in	O
developmental	O
robotics	O
research	O
;	O
e.g.	O
,	O
Cangelosi	O
and	O
Schlesinger	O
(	O
2015	O
)	O
,	O
Chapter	O
7	O
.	O
1	O
Taken	O
together	O
,	O
it	O
is	O
clear	O
that	O
aspects	O
of	O
the	O
physical	O
world	O
are	O
necessary	O
for	O
holistic	O
knowledge	O
of	O
semantic	O
meaning	O
,	O
which	O
has	O
implications	O
for	O
how	O
language	O
is	O
modeled	O
computationally	O
.	O
In	O
particular	O
,	O
what	O
does	O
this	O
mean	O
for	O
language	O
models	O
that	O
are	O
trained	O
purely	O
on	O
text	O
(	O
likely	O
largely	O
written	O
by	O
adults	O
)	O
,	O
such	O
as	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
or	O
GPT	B-MethodName
-	O
3	O
?	O
These	O
models	O
have	O
clearly	O
led	O
to	O
important	O
advances	O
for	O
natural	O
language	O
processing	O
tasks	O
and	O
applications	O
,	O
but	O
it	O
is	O
also	O
clear	O
that	O
language	O
models	O
trained	O
only	O
on	O
text	O
are	O
missing	O
critical	O
semantic	O
information	O
(	O
Bender	O
and	O
Koller	O
,	O
2020	O
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
contribute	O
to	O
a	O
growing	O
body	O
of	O
recent	O
work	O
that	O
attempts	O
to	O
addresses	O
these	O
limitations	O
by	O
(	O
1	O
)	O
leveraging	O
multimodal	O
and	O
sensorimotor	O
knowledge	O
of	O
the	O
Lancaster	O
Sensorimotor	O
Norms	O
(	O
Lynott	O
et	O
al	O
,	O
2019	O
)	O
and	O
(	O
2	O
)	O
using	O
vectorized	O
representations	O
of	O
images	O
by	O
treating	O
both	O
(	O
1	O
)	O
and	O
(	O
2	O
)	O
as	O
embeddings	O
of	O
language	O
models	O
for	O
GLUE	B-DatasetName
and	O
Visual	B-TaskName
Dialog	I-TaskName
benchmarks	O
.	O
In	O
the	O
following	O
section	O
,	O
we	O
explain	O
related	O
work	O
-	O
a	O
growing	O
body	O
of	O
literature	O
that	O
is	O
adding	O
multimodal	O
information	O
to	O
language	O
models	O
,	O
then	O
we	O
explain	O
our	O
two	O
embeddings	O
that	O
we	O
will	O
use	O
.	O
We	O
explore	O
how	O
these	O
embeddings	O
can	O
be	O
used	O
to	O
enrich	O
the	O
ELEC	O
-	O
TRA	O
language	O
model	O
's	O
pre	O
-	O
training	O
and	O
fine	O
-	O
tuning	O
,	O
and	O
evaluate	O
on	O
the	O
GLUE	B-DatasetName
benchmark	O
(	O
Experiment	O
1	O
,	O
Section	O
4	O
)	O
,	O
and	O
how	O
they	O
can	O
be	O
used	O
to	O
replace	O
input	O
embeddings	O
for	O
a	O
pre	O
-	O
trained	O
RoBERTa	B-MethodName
model	O
for	O
the	O
Visual	B-TaskName
Dialog	I-TaskName
task	O
(	O
Experiment	O
2	O
,	O
Section	O
5	O
)	O
.	O
Our	O
experiments	O
shed	O
light	O
on	O
how	O
useful	O
multimodal	O
information	O
can	O
be	O
in	O
a	O
task	O
that	O
is	O
text	O
-	O
only	O
(	O
Experiment	O
1	O
)	O
and	O
a	O
task	O
that	O
is	O
multimodal	O
(	O
Experiment	O
2	O
)	O
.	O
Our	O
results	O
show	O
that	O
our	O
parsimonious	O
method	O
to	O
unifying	O
vision	O
(	O
and	O
sensorimotor	O
knowledge	O
)	O
in	O
existing	O
language	O
models	O
shows	O
improvements	O
in	O
multimodal	O
benchmarks	O
with	O
accessible	O
hardware	O
(	O
i.e.	O
,	O
a	O
single	O
GPU	O
)	O
as	O
a	O
step	O
towards	O
models	O
that	O
can	O
be	O
trained	O
in	O
settings	O
similar	O
to	O
that	O
of	O
child	O
language	O
learners	O
.	O

Language	O
models	O
are	O
trained	O
on	O
text	O
.	O
Günther	O
et	O
al	O
(	O
2018	O
)	O
took	O
up	O
the	O
question	O
do	O
words	O
inherit	O
sensorimotor	O
activation	O
from	O
purely	O
linguistic	O
context	O
?	O
and	O
showed	O
that	O
experience	O
is	O
necessary	O
for	O
reactivating	O
experiential	O
traces	O
,	O
but	O
this	O
reactivation	O
is	O
not	O
a	O
necessary	O
condition	O
for	O
understanding	O
the	O
corresponding	O
aspects	O
of	O
word	O
meaning	O
.	O
We	O
take	O
this	O
to	O
mean	O
that	O
humans	O
are	O
very	O
adept	O
at	O
learning	O
new	O
concepts	O
from	O
language	O
exposure	O
alone	O
(	O
i.e.	O
,	O
abstract	O
concepts	O
)	O
;	O
e.g.	O
,	O
someone	O
who	O
has	O
never	O
seen	O
a	O
zebra	O
before	O
,	O
but	O
hears	O
them	O
described	O
as	O
"	O
horses	O
with	O
vertical	O
black	O
and	O
white	O
stripes	O
"	O
can	O
compose	O
a	O
connotation	O
of	O
what	O
zebra	O
denotes	O
without	O
direct	O
visual	O
exposure	O
.	O
However	O
,	O
this	O
only	O
works	O
if	O
an	O
agent	B-DatasetName
that	O
has	O
learned	O
the	O
language	O
has	O
the	O
knowledge	O
of	O
horses	O
,	O
black	O
,	O
white	O
,	O
stripes	O
,	O
and	O
vertical	O
concepts	O
-	O
i.e.	O
,	O
via	O
direct	O
experience	O
,	O
not	O
just	O
through	O
linguistic	O
exposure	O
or	O
encyclopedic	O
definitions	O
.	O
These	O
claims	O
are	O
further	O
backed	O
up	O
by	O
neuroscience	O
research	O
that	O
showed	O
that	O
neural	O
assemblies	O
encode	O
concrete	O
content	O
words	O
(	O
i.e.	O
,	O
words	O
that	O
denote	O
visual	O
objects	O
)	O
and	O
verbs	O
(	O
i.e.	O
,	O
words	O
that	O
denote	O
actions	O
)	O
are	O
learned	O
and	O
represented	O
in	O
different	O
brain	O
regions	O
(	O
Pulvermüller	O
,	O
1999	O
;	O
Borghesani	O
et	O
al	O
,	O
2019	O
)	O
.	O
Rogers	O
et	O
al	O
(	O
2020	O
)	O
provides	O
a	O
recent	O
primer	O
and	O
overview	O
of	O
research	O
that	O
has	O
attempted	O
to	O
uncover	O
strengths	O
and	O
weaknesses	O
of	O
BERT	B-MethodName
and	O
related	O
language	O
models	O
(	O
so	O
-	O
called	O
BERTology	O
)	O
.	O
While	O
our	O
work	O
does	O
fit	O
into	O
that	O
growing	O
body	O
of	O
literature	O
,	O
our	O
criticisms	O
on	O
current	O
language	O
models	O
specifically	O
lies	O
in	O
the	O
fact	O
that	O
they	O
are	O
only	O
trained	O
on	O
easy	O
-	O
to	O
-	O
obtain	O
text	O
.	O
This	O
criticism	O
is	O
born	O
out	O
in	O
Forbes	O
et	O
al	O
(	O
2019	O
)	O
which	O
showed	O
that	O
BERT	B-MethodName
can	O
guess	O
affordances	O
and	O
properties	O
of	O
objects	O
because	O
that	O
information	O
can	O
be	O
found	O
in	O
text	O
(	O
e.g.	O
,	O
a	O
typical	O
chair	O
has	O
the	O
affordance	O
of	O
being	O
sittable	O
,	O
and	O
a	O
property	O
of	O
having	O
legs	O
)	O
,	O
but	O
has	O
no	O
notion	O
of	O
how	O
objects	O
are	O
related	O
semantically	O
to	O
each	O
other	O
,	O
and	O
Da	O
and	O
Kasai	O
(	O
2019	O
)	O
further	O
showed	O
that	O
real	O
-	O
world	O
perceptual	O
properties	O
are	O
likely	O
to	O
be	O
assumed	O
instead	O
of	O
inferred	O
.	O
Furthermore	O
,	O
Bender	O
and	O
Koller	O
(	O
2020	O
)	O
make	O
a	O
strong	O
case	O
that	O
BERT	B-MethodName
learns	O
form	O
instead	O
of	O
meaning	O
,	O
and	O
while	O
the	O
fact	O
that	O
BERT	B-MethodName
performs	O
so	O
well	O
on	O
many	O
tasks	O
is	O
difficult	O
to	O
dispute	O
,	O
models	O
trained	O
on	O
text	O
are	O
missing	O
semantic	O
information	O
crucial	O
for	O
holistic	O
language	O
understanding	O
.	O
Since	O
before	O
BERT	B-MethodName
which	O
has	O
proven	O
powerful	O
in	O
many	O
language	O
processing	O
tasks	O
,	O
efforts	O
have	O
been	O
made	O
to	O
encode	O
multimodal	O
(	O
i.e.	O
,	O
more	O
than	O
just	O
text	O
as	O
a	O
learning	O
modality	O
)	O
information	O
into	O
embeddings	O
and	O
language	O
models	O
(	O
Takano	O
and	O
Utsumi	O
,	O
2016	O
;	O
Kiros	O
et	O
al	O
,	O
2014	O
;	O
Zellers	O
et	O
al	O
,	O
2021	O
)	O
and	O
recent	O
,	O
continued	O
efforts	O
towards	O
bridging	O
grounded	O
visual	O
representations	O
to	O
distributional	O
representations	O
of	O
word	O
meanings	O
give	O
credence	O
to	O
the	O
claim	O
that	O
text	O
-	O
only	O
models	O
like	O
BERT	B-MethodName
are	O
missing	O
crucial	O
semantic	O
information	O
because	O
enriching	O
BERT	B-MethodName
with	O
visual	O
information	O
improves	O
performance	O
in	O
several	O
known	O
tasks	O
(	O
Kim	O
et	O
al	O
,	O
2019	O
;	O
Lu	O
et	O
al	O
,	O
2019	O
;	O
Li	O
et	O
al	O
,	O
2019	O
)	O
.	O
These	O
models	O
usually	O
treat	O
language	O
and	O
vision	O
as	O
separate	O
pipelines	O
;	O
our	O
method	O
directly	O
endows	O
the	O
language	O
model	O
with	O
visual	O
and	O
sensorimotor	O
knowledge	O
.	O

In	O
this	O
section	O
,	O
we	O
motivate	O
and	O
introduce	O
of	O
multimodal	O
information	O
we	O
will	O
use	O
in	O
our	O
experiments	O
.	O
The	O
Lancaster	O
Sensorimotor	O
Norms	O
The	O
Lancaster	O
Sensorimotor	O
norms	O
(	O
Lynott	O
et	O
al	O
,	O
2019	O
)	O
provide	O
ratings	O
(	O
means	O
and	O
standard	O
deviations	O
)	O
for	O
40	O
,	O
000	O
English	O
words	O
along	O
dimensions	O
of	O
embodiment	O
which	O
capture	O
the	O
extent	O
to	O
which	O
a	O
concept	O
is	O
experienced	O
across	O
11	O
different	O
sensory	O
modalities	O
,	O
and	O
measures	O
derived	O
from	O
those	O
categories	O
,	O
listed	O
below	O
(	O
each	O
has	O
an	O
example	O
word	O
that	O
rates	O
highly	O
for	O
that	O
modalitiy	O
)	O
:	O
Auditory	O
-	O
sound	O
;	O
ping	O
Gustatory	O
-	O
having	O
to	O
do	O
with	O
eating	O
;	O
cream	O
Haptic	O
-	O
muscle	O
movement	O
;	O
handshake	O
Interoceptive	O
-	O
having	O
to	O
do	O
with	O
affect	O
or	O
emotion	B-DatasetName
;	O
headache	O
Olfactory	O
-	O
smell	O
;	O
incense	O
Visual	O
-	O
visual	O
;	O
barcode	O
Foot	O
-	O
leg	O
-	O
haptics	O
for	O
foot	O
/	O
leg	O
;	O
run	O
Hand	O
-	O
arm	O
-	O
haptics	O
for	O
hand	O
/	O
arm	O
;	O
pointing	O
Head	O
-	O
having	O
to	O
do	O
with	O
the	O
head	O
;	O
eye	O
Mouth	O
-	O
haptics	O
for	O
mouth	O
;	O
kiss	O
Torso	O
-	O
haptics	O
for	O
torso	O
;	O
breath	O
Max	O
-	O
strength.perceptual	O
-	O
the	O
highest	O
rating	O
across	O
the	O
11	O
sensorimotor	O
dimensions	O
Minkowski3.perceptual	O
-	O
treating	O
the	O
11	O
modalities	O
as	O
a	O
vector	O
,	O
this	O
represents	O
the	O
distance	O
of	O
the	O
vector	O
from	O
the	O
origin	O
with	O
influence	O
of	O
weaker	O
dimensions	O
attenuated	O
Exclusivity.perceptual	O
-	O
the	O
extent	O
to	O
which	O
a	O
concept	O
(	O
out	O
of	O
the	O
11	O
)	O
which	O
is	O
experienced	O
through	O
a	O
single	O
perceptual	O
modalitiy	O
The	O
last	O
three	O
can	O
be	O
seen	O
as	O
aggregates	O
from	O
the	O
11	O
modalities	O
;	O
they	O
also	O
have	O
.action	O
values	O
representing	O
the	O
extent	O
to	O
which	O
a	O
concept	O
is	O
experienced	O
as	O
an	O
action	O
(	O
as	O
opposed	O
to	O
.perceptual	O
)	O
,	O
and	O
.sensorimotor	O
values	O
representing	O
the	O
extent	O
a	O
concept	O
is	O
experience	O
as	O
sensorimotor	O
.	O
As	O
these	O
norms	O
were	O
derived	O
from	O
surveys	O
given	O
to	O
adults	O
,	O
these	O
norms	O
represent	O
the	O
degree	O
to	O
which	O
the	O
survey	O
participants	O
assigned	O
those	O
words	O
to	O
those	O
categories	O
.	O
Though	O
this	O
does	O
not	O
represent	O
a	O
neurophysiological	O
grounding	O
of	O
words	O
to	O
those	O
modalities	O
learned	O
through	O
interaction	O
and	O
embodiment	O
,	O
this	O
serves	O
as	O
a	O
useful	O
approximation	O
.	O
The	O
final	O
set	O
is	O
a	O
vocabulary	O
of	O
39	O
,	O
707	O
words	O
(	O
after	O
removing	O
rows	O
which	O
had	O
null	O
values	O
)	O
,	O
each	O
represented	O
as	O
a	O
vector	O
of	O
length	O
39	O
(	O
i.e.	O
,	O
11	O
mean	O
,	O
11	O
stdev	O
columns	O
;	O
Max	O
-	O
strength	O
,	O
Minkowski	O
,	O
and	O
Exclusivity	O
columns	O
for	O
different	O
ways	O
of	O
aggregating	O
the	O
modalities	O
)	O
.	O
We	O
normalize	O
each	O
value	O
in	O
the	O
vector	O
independently	O
to	O
a	O
value	O
between	O
0	B-DatasetName
-	O
1	O
by	O
dividing	O
each	O
value	O
over	O
its	O
max	O
value	O
.	O
We	O
call	O
this	O
the	O
Lancaster	O
vectors	O
.	O
We	O
performed	O
t	O
-	O
SNE	O
on	O
the	O
Lancaster	O
vectors	O
(	O
mapping	O
to	O
2	O
dimensions	O
)	O
to	O
determine	O
if	O
clus	O
-	O
ters	O
would	O
reveal	O
any	O
intuitions	O
about	O
the	O
kinds	O
of	O
semantic	O
relatedness	O
that	O
the	O
words	O
might	O
have	O
with	O
each	O
other	O
.	O
Some	O
clusters	O
emerged	O
such	O
as	O
foods	O
(	O
presumably	O
because	O
they	O
have	O
similar	O
gustatory	O
ratings	O
)	O
,	O
leg	O
-	O
movement	O
verbs	O
(	O
e.g.	O
,	O
walk	O
,	O
jump	O
,	O
sit	O
)	O
,	O
colors	O
with	O
eye	O
-	O
related	O
words	O
(	O
e.g.	O
,	O
purple	O
,	O
green	O
,	O
blue	O
,	O
dark	O
,	O
see	O
,	O
eyes	O
)	O
,	O
soft	O
things	O
(	O
e.g.	O
,	O
hug	O
,	O
tummy	O
,	O
pillow	O
,	O
clothes	O
)	O
,	O
audio	O
-	O
related	O
words	O
(	O
e.g.	O
,	O
talk	O
,	O
story	O
,	O
sound	O
,	O
music	O
,	O
lie	O
,	O
say	O
)	O
,	O
among	O
others	O
.	O

The	O
main	O
contribution	O
of	O
this	O
paper	O
is	O
to	O
explore	O
using	O
the	O
Lancaster	O
Sensorimotor	O
Norms	O
and	O
the	O
Words	O
-	O
as	O
-	O
Classifiers	O
model	O
as	O
vectorized	O
knowledge	O
from	O
the	O
physical	O
world	O
on	O
the	O
GLUE	B-DatasetName
and	O
Visual	B-TaskName
Dialog	I-TaskName
tasks	O
.	O
Lancaster	O
norms	O
performed	O
well	O
on	O
their	O
own	O
in	O
one	O
GLUE	B-DatasetName
task	O
compared	O
to	O
other	O
word	B-TaskName
embeddings	I-TaskName
like	O
GloVe	B-MethodName
,	O
and	O
coupled	O
with	O
the	O
WAC	O
vectors	O
as	O
the	O
embedding	O
in	O
an	O
ELECTRA	B-MethodName
model	O
,	O
they	O
performed	O
respectably	O
on	O
the	O
GLUE	B-DatasetName
task	O
.	O
The	O
WAC	O
vectors	O
,	O
when	O
used	O
as	O
embeddings	O
in	O
the	O
RoBERTa	B-MethodName
model	O
performed	O
well	O
on	O
the	O
Visual	B-TaskName
Dialog	I-TaskName
task	O
,	O
particularly	O
when	O
the	O
vocabulary	O
was	O
more	O
restricted	O
to	O
the	O
Age	O
of	O
Acquisition	O
vocabulary	O
.	O
Crucially	O
,	O
this	O
work	O
differs	O
from	O
other	O
visually	O
grounded	O
models	O
because	O
the	O
grounded	O
knowledge	O
is	O
part	O
of	O
the	O
language	O
model	O
itself	O
(	O
i.e.	O
,	O
the	O
embeddings	O
)	O
rather	O
than	O
computed	O
in	O
parallel	O
and	O
added	O
for	O
a	O
task	O
-	O
specific	O
purpose	O
.	O
Moreover	O
,	O
standard	O
language	O
models	O
can	O
not	O
actually	O
identify	O
denotations	O
when	O
they	O
are	O
present	O
;	O
i.e.	O
,	O
ELECTRA	B-MethodName
and	O
RoBERTa	B-MethodName
are	O
not	O
actually	O
capable	O
of	O
determining	O
if	O
an	O
object	O
is	O
red	O
or	O
soft	O
from	O
observing	O
that	O
object	O
-	O
a	O
basic	O
ability	O
for	O
a	O
language	O
learning	O
child	O
-	O
simply	O
because	O
those	O
models	O
can	O
not	O
observe	O
the	O
world	O
outside	O
of	O
text	O
,	O
though	O
the	O
purpose	O
of	O
the	O
WAC	O
(	O
and	O
models	O
like	O
VilBERT	B-MethodName
)	O
model	O
is	O
to	O
do	O
just	O
that	O
:	O
identify	O
denotations	O
;	O
by	O
coupling	O
WAC	O
with	O
ELECTRA	B-MethodName
and	O
RoBERTa	B-MethodName
,	O
both	O
models	O
can	O
make	O
use	O
of	O
that	O
capability	O
.	O
This	O
work	O
is	O
critical	O
in	O
our	O
ongoing	O
efforts	O
towards	O
a	O
model	O
that	O
learns	O
language	O
in	O
a	O
co	O
-	O
located	O
setting	O
in	O
an	O
embodied	O
platform	O
.	O
In	O
particular	O
,	O
our	O
knowledge	O
from	O
this	O
paper	O
informs	O
us	O
that	O
the	O
ELECTRA	B-MethodName
model	O
with	O
embeddings	O
tied	O
to	O
WAC	O
classifier	O
weights	O
is	O
a	O
good	O
candidate	O
for	O
live	O
interaction	O
of	O
a	O
robot	O
that	O
is	O
learning	O
words	O
from	O
a	O
human	O
collaborator	O
because	O
the	O
ELECTRA	B-MethodName
-	O
WAC	O
model	O
can	O
function	O
with	O
small	O
amounts	O
of	O
data	O
and	O
the	O
embedding	O
layer	O
can	O
successfully	O
be	O
tied	O
to	O
weights	O
of	O
the	O
WAC	O
classifiers	O
.	O
We	O
leave	O
implementation	O
and	O
evaluation	O
of	O
this	O
model	O
on	O
a	O
robotic	O
platform	O
for	O
future	O
work	O
.	O

Verbal	O
prediction	O
has	O
been	O
shown	O
to	O
be	O
critical	O
during	O
online	O
comprehension	O
of	O
Subject	O
-	O
Object	O
-	O
Verb	O
(	O
SOV	O
)	O
languages	O
.	O
In	O
this	O
work	O
we	O
present	O
three	O
computational	O
models	O
to	O
predict	O
clause	O
final	O
verbs	O
in	O
Hindi	O
given	O
its	O
prior	O
arguments	O
.	O
The	O
models	O
differ	O
in	O
their	O
use	O
of	O
prior	O
context	O
during	O
the	O
prediction	O
processthe	O
context	O
is	O
either	O
noisy	O
or	O
noise	O
-	O
free	O
.	O
Model	O
predictions	O
are	O
compared	O
with	O
the	O
sentence	B-TaskName
completion	I-TaskName
data	O
obtained	O
from	O
Hindi	O
native	O
speakers	O
.	O
Results	O
show	O
that	O
models	O
that	O
assume	O
noisy	O
context	O
outperform	O
the	O
noise	O
-	O
free	O
model	O
.	O
In	O
particular	O
,	O
a	O
lossy	O
context	O
model	O
that	O
assumes	O
prior	O
context	O
to	O
be	O
affected	O
by	O
predictability	O
and	O
recency	O
captures	O
the	O
distribution	O
of	O
the	O
predicted	O
verb	O
class	O
and	O
error	O
sources	O
best	O
.	O
The	O
success	O
of	O
the	O
predictabilityrecency	O
lossy	O
context	O
model	O
is	O
consistent	O
with	O
the	O
noisy	O
channel	O
hypothesis	O
for	O
sentence	O
comprehension	O
and	O
supports	O
the	O
idea	O
that	O
the	O
reconstruction	O
of	O
the	O
context	O
during	O
prediction	O
is	O
driven	O
by	O
prior	O
linguistic	O
exposure	O
.	O
These	O
results	O
also	O
shed	O
light	O
on	O
the	O
nature	O
of	O
the	O
noise	O
that	O
affects	O
the	O
reconstruction	O
process	O
.	O
Overall	O
the	O
results	O
pose	O
a	O
challenge	O
to	O
the	O
adaptability	O
hypothesis	O
that	O
assumes	O
use	O
of	O
noise	O
-	O
free	O
preverbal	O
context	O
for	O
robust	O
verbal	O
prediction	O
.	O

In	O
spite	O
of	O
the	O
proposed	O
central	O
role	O
of	O
verb	O
prediction	O
during	O
online	O
processing	O
of	O
Hindi	O
(	O
e.g.	O
,	O
Vasishth	O
and	O
Lewis	O
,	O
2006	O
;	O
Agrawal	O
et	O
al	O
,	O
2017	O
;	O
Husain	O
et	O
al	O
,	O
2014	O
)	O
,	O
there	O
is	O
a	O
surprising	O
lack	O
of	O
any	O
modeling	O
attempt	O
to	O
understand	O
the	O
processes	O
that	O
subserve	O
verbal	O
predictions	O
in	O
the	O
language	O
.	O
While	O
there	O
are	O
computational	O
metrics	O
that	O
model	O
reading	O
time	O
data	O
(	O
e.g.	O
,	O
Hale	O
,	O
2001	O
;	O
Shain	O
et	O
al	O
,	O
2016	O
;	O
Futrell	O
et	O
al	O
,	O
2020	O
)	O
,	O
a	O
computational	O
model	O
that	O
makes	O
precise	O
verbal	O
prediction	O
in	O
SOV	O
languages	O
has	O
not	O
been	O
investigated	O
thoroughly	O
(	O
but	O
see	O
,	O
Grissom	O
II	O
et	O
al	O
,	O
2016	O
,	O
for	O
an	O
initial	O
attempt	O
)	O
.	O
Understanding	O
the	O
mechanisms	O
that	O
subserve	O
verbal	O
prediction	O
in	O
SOV	O
languages	O
is	O
critical	O
to	O
understanding	O
how	O
these	O
languages	O
are	O
processed	O
(	O
cf	O
.	O
Konieczny	O
,	O
2000	O
;	O
Vasishth	O
et	O
al	O
,	O
2010	O
;	O
Husain	O
et	O
al	O
,	O
2014	O
;	O
Levy	O
and	O
Keller	O
,	O
2013	O
;	O
Kuperberg	O
and	O
Jaeger	O
,	O
2016	O
)	O
.	O
Our	O
work	O
fills	O
this	O
gap	O
in	O
the	O
literature	O
.	O
In	O
this	O
section	O
we	O
summarize	O
the	O
key	O
results	O
of	O
a	O
recent	O
study	O
by	O
Apurva	O
and	O
Husain	O
(	O
2020	O
)	O
who	O
investigated	O
the	O
nature	O
of	O
verbal	O
prediction	O
in	O
Hindi	O
using	O
a	O
series	O
sentence	B-TaskName
completion	I-TaskName
studies	O
.	O
Later	O
,	O
in	O
sections	O
4	O
,	O
5	O
we	O
present	O
three	O
computational	O
models	O
to	O
account	O
for	O
these	O
results	O
.	O

Apurva	O
and	O
Husain	O
(	O
2020	O
)	O
used	O
the	O
sentence	B-TaskName
completion	I-TaskName
paradigm	O
(	O
Taylor	O
,	O
1953	O
)	O
to	O
probe	O
the	O
nature	O
of	O
clause	O
final	O
verbal	O
prediction	O
when	O
differing	O
the	O
number	O
of	O
preverbal	O
nouns	O
that	O
precede	O
the	O
tobe	O
-	O
completed	O
target	O
verb	O
.	O
The	O
number	O
of	O
nouns	O
ranged	O
from	O
1	O
to	O
3	O
and	O
appeared	O
in	O
different	O
casemarker	O
order	O
.	O
All	O
preverbal	O
nouns	O
were	O
proper	O
nouns	O
.	O
Example	O
1	O
shows	O
some	O
of	O
the	O
conditions	O
where	O
3	O
preverbal	O
nouns	O
preceded	O
the	O
target	O
verb	O
.	O
In	O
the	O
example	O
,	O
ne	O
is	O
the	O
Ergative	O
case	O
-	O
marker	O
,	O
ko	O
is	O
the	O
Accusative	O
case	O
-	O
marker	O
and	O
se	O
is	O
the	O
Ablative	O
case	O
-	O
marker	O
.	O
In	O
all	O
,	O
there	O
were	O
6	O
conditions	O
in	O
this	O
experiment	O
(	O
ne	O
-	O
ko	O
-	O
se	O
,	O
ne	O
-	O
se	O
-	O
ko	O
,	O
ko	O
-	O
ne	O
-	O
se	O
,	O
ko	O
-	O
se	O
-	O
ne	O
,	O
se	O
-	O
ko	O
-	O
ne	O
,	O
se	O
-	O
ne	O
-	O
ko	O
)	O
.	O
36	O
native	O
speakers	O
participated	O
in	O
the	O
3	O
-	O
NP	O
condition	O
experiments	O
.	O
Similar	O
to	O
the	O
3	O
-	O
NP	O
conditions	O
,	O
the	O
1	O
-	O
NP	O
and	O
2	O
-	O
NP	O
items	O
had	O
proper	O
nouns	O
and	O
the	O
nouns	O
occurred	O
in	O
various	O
case	O
-	O
marker	O
order	O
.	O
25	O
native	O
speakers	O
participated	O
in	O
the	O
1	O
-	O
NP	O
and	O
2	O
-	O
NP	O
condition	O
experiments	O
.	O
(	O
The	O
key	O
result	O
from	O
these	O
completion	O
studies	O
was	O
that	O
the	O
number	O
of	O
ungrammatical	O
verbal	O
completions	O
increased	O
as	O
the	O
number	O
of	O
preverbal	O
nominals	O
increased	O
.	O
For	O
the	O
1	O
-	O
NP	O
conditions	O
the	O
percentage	O
ungrammatical	O
completions	O
was	O
4	O
%	O
,	O
for	O
the	O
2	O
-	O
NP	O
conditions	O
this	O
was	O
8	O
%	O
,	O
while	O
for	O
the	O
3	O
-	O
NP	O
conditions	O
the	O
ungrammatical	O
completions	O
increased	O
to	O
15	O
%	O
.	O
In	O
addition	O
,	O
the	O
completion	O
data	O
was	O
also	O
analyzed	O
for	O
the	O
nature	O
of	O
grammatical	O
and	O
ungrammatical	O
verbal	O
completions	O
.	O
Completions	O
were	O
analyzed	O
based	O
on	O
the	O
verb	O
classes	O
rather	O
than	O
lexical	O
identity	O
(	O
cf	O
.	O
Luke	O
and	O
Christianson	O
,	O
2016	O
)	O
.	O
The	O
data	O
contains	O
a	O
distribution	O
over	O
a	O
total	O
of	O
18	O
verbs	O
classes	O
for	O
the	O
2	O
-	O
NP	O
and	O
3	O
-	O
NP	O
conditions	O
.	O
In	O
majority	O
of	O
the	O
grammatical	O
completions	O
,	O
Hindi	O
native	O
speakers	O
posit	O
simple	O
syntactic	O
structures	O
(	O
in	O
terms	O
of	O
the	O
number	O
of	O
clausal	O
embeddings	O
and	O
the	O
number	O
of	O
core	O
argument	O
structure	O
)	O
.	O
For	O
the	O
2	O
-	O
NP	O
conditions	O
,	O
the	O
topmost	O
verb	O
classes	O
were	O
T	O
(	O
Transitive	O
verb	O
)	O
,	O
IN	O
(	O
Intransitive	O
verb	O
)	O
,	O
and	O
DT	O
(	O
Ditransitive	O
verb	O
)	O
.	O
For	O
the	O
3	O
-	O
NP	O
conditions	O
,	O
CAUS	O
(	O
Causative	O
verb	O
)	O
and	O
T	O
DT	O
(	O
Transitive	O
non	O
-	O
finite	O
verb	O
followed	O
by	O
a	O
ditransitive	O
matrix	O
verb	O
)	O
were	O
consistently	O
the	O
most	O
frequent	O
,	O
covering	O
at	O
least	O
50	O
%	O
of	O
completions	O
between	O
them	O
for	O
all	O
conditions	O
.	O
Some	O
of	O
the	O
other	O
classes	O
observed	O
were	O
DT	O
,	O
N	O
T	O
DT	O
,	O
and	O
DT	O
DT	O
.	O
Interestingly	O
,	O
while	O
the	O
3	O
-	O
NP	O
conditions	O
can	O
be	O
grammatically	O
completed	O
using	O
a	O
double	O
embedded	O
structure	O
(	O
e.g.	O
,	O
IN	O
DT	O
DT	O
)	O
,	O
such	O
cases	O
were	O
not	O
found	O
in	O
the	O
completion	O
data	O
.	O
Among	O
the	O
ungrammatical	O
verb	O
completions	O
across	O
various	O
conditions	O
,	O
N	O
DT	O
,	O
IN	O
DT	O
and	O
CAUS	O
were	O
consistently	O
the	O
most	O
frequent	O
verb	O
classes	O
predicted	O
.	O
Similar	O
to	O
the	O
trend	O
in	O
the	O
grammatical	O
completions	O
discussed	O
above	O
,	O
the	O
parser	O
posits	O
simple	O
structures	O
even	O
when	O
making	O
mistakes	O
.	O
Additionally	O
,	O
a	O
closer	O
analysis	O
of	O
the	O
ungrammatical	O
completions	O
showed	O
the	O
formation	O
of	O
locally	O
coherent	O
parses	O
(	O
Tabor	O
et	O
al	O
,	O
2004	O
)	O
for	O
the	O
various	O
3	O
-	O
NP	O
conditions	O
where	O
the	O
first	O
noun	O
was	O
ignored	O
and	O
only	O
the	O
2nd	O
and	O
the	O
3rd	O
nouns	O
were	O
used	O
to	O
make	O
the	O
prediction	O
(	O
we	O
call	O
these	O
N2	O
-	O
N3	O
errors	O
)	O
.	O
Other	O
errors	O
were	O
made	O
when	O
either	O
N2	O
or	O
N3	O
were	O
ignored	O
to	O
make	O
the	O
prediction	O
(	O
we	O
call	O
these	O
N1	O
-	O
N3	O
,	O
N1	O
-	O
N2	O
errors	O
respectively	O
)	O
.	O
The	O
errors	O
also	O
show	O
a	O
subject	O
primacy	O
effect	O
(	O
Häussler	O
and	O
Bader	O
,	O
2015	O
;	O
Knoedler	O
et	O
al	O
,	O
1999	O
)	O
where	O
the	O
presence	O
of	O
an	O
Ergative	O
case	O
marker	O
on	O
N1	O
is	O
not	O
forgotten	O
.	O
This	O
leads	O
to	O
lack	O
of	O
passive	O
predictions	O
in	O
such	O
cases	O
.	O
2	O
To	O
sum	O
up	O
,	O
the	O
key	O
results	O
of	O
the	O
completion	O
studies	O
were	O
,	O
(	O
a	O
)	O
verb	O
prediction	O
was	O
good	O
in	O
1	O
-	O
NP	O
and	O
2	O
-	O
NP	O
conditions	O
,	O
(	O
b	O
)	O
predictions	O
deteriorated	O
in	O
3	O
-	O
NP	O
conditions	O
,	O
(	O
c	O
)	O
grammatical	O
verbal	O
completions	O
are	O
syntactically	O
simple	O
rather	O
than	O
complex	O
(	O
e.g.	O
,	O
clausal	O
embeddings	O
are	O
avoided	O
)	O
,	O
(	O
d	O
)	O
error	O
types	O
for	O
the	O
3	O
-	O
NP	O
conditions	O
show	O
use	O
of	O
two	O
preverbal	O
NPs	O
to	O
make	O
predictions	O
,	O
as	O
well	O
as	O
being	O
sensitive	O
to	O
subject	O
primacy	O
.	O
Table	O
1	O
provides	O
the	O
details	O
on	O
the	O
number	O
of	O
grammatical	O
and	O
ungrammatical	O
completions	O
over	O
all	O
conditions	O
.	O
Also	O
see	O
Table	O
3	O
for	O
verb	O
class	O
numbers	O
for	O
the	O
2	O
-	O
NP	O
conditions	O
.	O
Table	O
2	O
shows	O
examples	O
of	O
various	O
error	O
types	O
in	O
the	O
3	O
-	O
NP	O
conditions	O
.	O

We	O
use	O
the	O
monolingual	O
Hindi	O
corpus	O
developed	O
by	O
IIT	O
Bombay	O
(	O
Kunchukuttan	O
et	O
al	O
,	O
2017	O
)	O
.	O
It	O
is	O
a	O
collection	O
of	O
raw	O
sentences	O
of	O
Hindi	O
taken	O
from	O
various	O
sources	O
(	O
HindMonoCorp	O
(	O
Bojar	O
et	O
al	O
,	O
2014	O
)	O
,	O
BBC	O
,	O
Wikipedia	O
etc	O
.	O
)	O
.	O
For	O
training	O
our	O
models	O
,	O
we	O
use	O
the	O
first	O
5	O
million	O
sentences	O
of	O
this	O
data	O
.	O
For	O
the	O
sentence	O
simplification	O
step	O
(	O
described	O
in	O
the	O
Section	O
3.2	O
)	O
,	O
we	O
use	O
the	O
ISC	O
dependency	O
parser	O
for	O
Hindi	O
.	O
3	O
Moreover	O
,	O
as	O
the	O
sentence	B-TaskName
completion	I-TaskName
experiment	O
included	O
only	O
animate	O
nouns	O
in	O
various	O
items	O
(	O
see	O
Section	O
2	O
)	O
,	O
we	O
use	O
an	O
additional	O
animacy	O
annotation	O
(	O
Jena	O
et	O
al	O
,	O
2013	O
)	O
to	O
label	O
the	O
nouns	O
accordingly	O
.	O

A	O
key	O
aim	O
of	O
the	O
behavioral	O
experiments	O
discussed	O
in	O
Section	O
2	O
was	O
to	O
investigate	O
the	O
role	O
of	O
preverbal	O
arguments	O
on	O
clause	O
final	O
verbal	O
prediction	O
.	O
Consequently	O
,	O
our	O
models	O
had	O
to	O
be	O
trained	O
on	O
sentences	O
with	O
various	O
features	O
(	O
e.g.	O
,	O
case	O
-	O
marker	O
,	O
animacy	O
)	O
of	O
the	O
preverbal	O
arguments	O
.	O
Since	O
the	O
raw	O
data	O
may	O
contain	O
other	O
intervening	O
material	O
(	O
nominal	O
modifiers	O
,	O
verbal	O
adjuncts	O
,	O
etc	O
.	O
)	O
,	O
4	O
the	O
task	O
necessitated	O
removal	O
of	O
such	O
material	O
from	O
the	O
training	O
corpus	O
to	O
render	O
it	O
more	O
tractable	O
to	O
the	O
appropriate	O
computational	O
model	O
.	O
Thus	O
,	O
we	O
simplify	O
each	O
sentence	O
in	O
the	O
training	O
data	O
by	O
removing	O
these	O
intervening	O
materials	O
while	O
ensuring	O
that	O
the	O
grammaticality	O
of	O
the	O
sentence	O
remains	O
intact	O
.	O
5	O
This	O
,	O
of	O
course	O
,	O
implies	O
that	O
the	O
model	O
only	O
uses	O
the	O
local	O
argument	O
structure	O
to	O
make	O
the	O
necessary	O
verbal	O
prediction	O
.	O
The	O
sentence	O
simplification	O
process	O
preserves	O
verbal	O
and	O
nominal	O
arguments	O
,	O
such	O
as	O
direct	O
/	O
oblique	O
objects	O
,	O
case	O
-	O
markers	O
,	O
and	O
auxiliaries	O
,	O
but	O
removes	O
adjective	O
phrases	O
,	O
relative	O
clauses	O
,	O
and	O
adjuncts	O
.	O
It	O
treats	O
conjunct	O
structures	O
as	O
separate	O
components	O
.	O
It	O
identifies	O
intra	O
-	O
sentential	O
noun	O
ellipsis	O
and	O
truncates	O
a	O
sequence	O
that	O
displays	O
such	O
a	O
structure	O
,	O
while	O
processing	O
its	O
other	O
verbs	O
.	O
For	O
example	O
:	O
We	O
also	O
flatten	O
all	O
the	O
nouns	O
in	O
the	O
data	O
to	O
"	O
noun	O
tokens	O
"	O
by	O
merging	O
the	O
noun	O
and	O
its	O
corresponding	O
case	O
-	O
marker	O
.	O
Since	O
we	O
are	O
interested	O
in	O
capturing	O
the	O
variations	O
of	O
the	O
completions	O
for	O
different	O
order	O
of	O
case	O
-	O
markers	O
in	O
the	O
prompt	O
,	O
we	O
can	O
abstract	O
away	O
from	O
the	O
lexicality	O
of	O
the	O
nouns	O
.	O
Thus	O
,	O
we	O
replace	O
the	O
nominal	O
lexical	O
item	O
with	O
its	O
corresponding	O
label	O
depending	O
on	O
whether	O
it	O
is	O
animate	O
(	O
A	O
)	O
or	O
not	O
(	O
N	O
)	O
.	O
Such	O
an	O
abstraction	O
is	O
well	O
motivated	O
considering	O
that	O
humans	O
are	O
known	O
to	O
be	O
sensitive	O
to	O
both	O
syntactic	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
tags	O
as	O
well	O
as	O
lexical	O
semantics	O
during	O
sentence	O
processing	O
(	O
e.g.	O
,	O
Demberg	O
and	O
Keller	O
,	O
2008	O
;	O
Trueswell	O
et	O
al	O
,	O
1994	O
)	O
.	O

All	O
the	O
models	O
are	O
evaluated	O
by	O
comparing	O
the	O
model	O
output	O
with	O
the	O
sentence	B-TaskName
completion	I-TaskName
data	O
obtained	O
from	O
the	O
native	O
speakers	O
;	O
specifically	O
,	O
model	O
output	O
is	O
evaluated	O
in	O
terms	O
of	O
the	O
nature	O
of	O
the	O
predicted	O
verb	O
class	O
.	O
We	O
let	O
VC	O
denote	O
the	O
set	O
of	O
all	O
verb	O
-	O
classes	O
,	O
h	O
(	O
x	O
)	O
denotes	O
the	O
probability	O
distribution	O
of	O
verb	O
-	O
class	O
predictions	O
made	O
by	O
humans	O
,	O
and	O
m	O
(	O
x	O
)	O
denotes	O
the	O
corresponding	O
distribution	O
of	O
the	O
model	O
.	O
We	O
measure	O
KL	O
-	O
divergence	O
between	O
these	O
two	O
distributions	O
,	O
replacing	O
zero	O
probabilities	O
with	O
a	O
fixed	O
value	O
9	O
(	O
=	O
10	O
−5	O
)	O
;	O
this	O
is	O
shown	O
in	O
(	O
1	O
)	O
KLp	O
(	O
h	O
|	O
|	O
m	O
)	O
=	O
KL	O
(	O
h	O
|	O
|	O
m	O
)	O
(	O
1	O
)	O
where	O
KL	O
denotes	O
the	O
KL	O
-	O
divergence	O
and	O
m	O
is	O
a	O
distribution	O
such	O
that	O
m	O
(	O
x	O
)	O
=	O
max	O
(	O
m	O
(	O
x	O
)	O
,	O
10	O
−5	O
)	O
for	O
each	O
x	O
VC	O
.	O
Apart	O
from	O
this	O
primary	O
measure	O
,	O
we	O
use	O
two	O
other	O
metrics	O
F	O
and	O
D	O
to	O
quantify	O
the	O
span	O
and	O
quality	O
of	O
model	O
predictions	O
with	O
respect	O
to	O
the	O
predicted	O
verb	O
classes	O
,	O
respectively	O
,	O
in	O
order	O
to	O
better	O
understand	O
these	O
characteristics	O
of	O
each	O
model	O
(	O
see	O
Section	O
6.1	O
)	O
.	O
Further	O
,	O
to	O
ascertain	O
a	O
qualitative	O
understanding	O
of	O
the	O
model	O
performance	O
,	O
we	O
also	O
evaluate	O
each	O
model	O
on	O
the	O
basis	O
of	O
the	O
following	O
characteristics	O
that	O
are	O
displayed	O
in	O
the	O
completion	O
data	O
discussed	O
in	O
Section	O
2	O
:	O
Deterioration	O
in	O
the	O
number	O
of	O
grammatical	O
completions	O
on	O
the	O
3	O
-	O
NP	O
conditions	O
compared	O
to	O
the	O
2	O
-	O
NP	O
conditions	O
Within	O
the	O
grammatical	O
completions	O
,	O
a	O
preference	O
for	O
simpler	O
structures	O
as	O
opposed	O
to	O
complex	O
or	O
embedded	O
constructions	O
Exhibition	O
of	O
similar	O
types	O
of	O
errors	O
as	O
humans	O
;	O
for	O
example	O
,	O
in	O
3	O
-	O
NP	O
conditions	O
,	O
N1	O
-	O
N2	O
errors	O
,	O
as	O
well	O
as	O
a	O
sensitivity	O
to	O
subject	O
primacy	O
with	O
the	O
Ergative	O
case	O
.	O
For	O
the	O
3	O
-	O
NP	O
conditions	O
,	O
we	O
classify	O
errors	O
into	O
types	O
based	O
on	O
their	O
compatibility	O
with	O
a	O
2	O
-	O
NP	O
sub	O
-	O
context	O
(	O
N1	O
-	O
N2	O
,	O
N1	O
-	O
N3	O
,	O
N2	O
-	O
N3	O
)	O
.	O
For	O
example	O
,	O
an	O
error	O
type	O
of	O
N1	O
-	O
N2	O
would	O
mean	O
that	O
the	O
corresponding	O
ungrammatical	O
prediction	O
is	O
compatible	O
only	O
with	O
first	O
two	O
NPs	O
and	O
not	O
the	O
full	O
3	O
-	O
NP	O
context	O
.	O
This	O
scheme	O
follows	O
the	O
error	O
types	O
found	O
in	O
the	O
completion	O
data	O
discussed	O
in	O
Section	O
2	O
.	O
Additionally	O
,	O
see	O
Section	O
2	O
of	O
the	O
supplementary	B-DatasetName
material	I-DatasetName
for	O
examples	O
of	O
various	O
errors	O
.	O

In	O
this	O
section	O
,	O
we	O
discuss	O
two	O
models	O
to	O
test	O
the	O
noisy	O
channel	O
hypothesis	O
.	O
As	O
stated	O
in	O
Section	O
1	O
,	O
the	O
underlying	O
assumption	O
is	O
that	O
human	O
communication	O
is	O
noisy	O
(	O
Gibson	O
et	O
al	O
,	O
2013	O
;	O
Kurumada	O
and	O
Jaeger	O
,	O
2015	O
)	O
and	O
the	O
comprehender	O
has	O
to	O
reinterpret	O
the	O
input	O
to	O
make	O
prediction	O
about	O
upcoming	O
linguistic	O
material	O
.	O
In	O
order	O
to	O
evaluate	O
this	O
hypothesis	O
,	O
we	O
implement	O
different	O
versions	O
of	O
the	O
lossy	O
-	O
context	O
surprisal	O
metric	O
(	O
Futrell	O
et	O
al	O
,	O
2020	O
)	O
.	O
Lossy	O
-	O
context	O
surprisal	O
holds	O
that	O
processing	O
difficulty	O
at	O
a	O
word	O
in	O
a	O
context	O
is	O
proportional	O
to	O
the	O
surprisal	O
of	O
a	O
word	O
given	O
a	O
lossy	O
memory	O
representation	O
of	O
the	O
context	O
.	O
The	O
two	O
models	O
discussed	O
in	O
sections	O
5.1	O
and	O
5.2	O
differ	O
in	O
their	O
noise	O
functions	O
that	O
affect	O
the	O
interpretation	O
of	O
the	O
preverbal	O
context	O
.	O
For	O
the	O
current	O
investigation	O
,	O
lossy	O
-	O
context	O
surprisal	O
is	O
extended	O
to	O
model	O
the	O
sentencecompletion	O
task	O
.	O
The	O
word	O
with	O
the	O
highest	O
probability	O
in	O
a	O
given	O
context	O
is	O
assumed	O
to	O
be	O
most	O
likely	O
to	O
complete	O
the	O
sentence	O
(	O
cf	O
.	O
Levy	O
,	O
2008	O
;	O
Smith	O
and	O
Levy	O
,	O
2013	O
)	O
.	O
As	O
noted	O
by	O
Futrell	O
et	O
al	O
(	O
2020	O
)	O
,	O
the	O
lossy	O
surprisal	O
model	O
is	O
not	O
representation	O
-	O
agnostic	O
.	O
Its	O
predictions	O
are	O
dependent	O
on	O
a	O
noise	O
distribution	O
(	O
M	O
)	O
.	O
One	O
can	O
then	O
obtain	O
:	O
p	O
(	O
w	O
|	O
r	O
)	O
∝	O
c	O
p	O
M	O
(	O
r	O
|	O
c	O
)	O
p	O
(	O
c	O
)	O
p	O
L	O
(	O
w	O
|	O
c	O
)	O
,	O
(	O
2	O
)	O
where	O
w	O
is	O
the	O
predicted	O
word	O
and	O
r	O
is	O
the	O
result	O
of	O
adding	O
noise	O
to	O
the	O
context	O
c.	O
Here	O
,	O
we	O
consider	O
L	O
to	O
be	O
a	O
4	O
-	O
gram	O
model	O
,	O
same	O
as	O
the	O
one	O
discussed	O
in	O
Section	O
4	O
.	O
Moreover	O
,	O
for	O
c	O
=	O
w	O
1	O
w	O
2	O
w	O
n	O
we	O
calculate	O
p	O
(	O
c	O
)	O
also	O
using	O
L	O
p	O
(	O
c	O
)	O
=	O
n	O
i=1	O
p	O
L	O
(	O
w	O
i	O
|	O
w	O
i−3	O
w	O
i−2	O
w	O
i−1	O
)	O
In	O
addition	O
,	O
if	O
|	O
c	O
|	O
=	O
n	O
≤	O
2	O
,	O
we	O
do	O
n't	O
add	O
any	O
noise	O
to	O
the	O
context	O
and	O
simply	O
use	O
the	O
ngram	O
model	O
L	O
for	O
prediction	O
.	O
In	O
other	O
words	O
,	O
if	O
c	O
=	O
w	O
1	O
w	O
2	O
or	O
c	O
=	O
w	O
1	O
,	O
then	O
we	O
consider	O
p	O
(	O
w	O
|	O
r	O
)	O
=	O
p	O
L	O
(	O
w	O
|	O
c	O
)	O
.	O
Since	O
we	O
only	O
consider	O
erasure	O
-	O
based	O
noise	O
distributions	O
,	O
this	O
is	O
done	O
to	O
ensure	O
that	O
the	O
whole	O
context	O
is	O
not	O
lost	O
during	O
prediction	O
.	O
In	O
order	O
to	O
get	O
an	O
average	O
behavior	O
of	O
the	O
model	O
,	O
we	O
run	O
the	O
model	O
10	O
times	O
and	O
then	O
take	O
the	O
top	O
50	O
predictions	O
based	O
on	O
the	O
total	O
probability	O
of	O
each	O
prediction	O
.	O
In	O
other	O
words	O
,	O
suppose	O
a	O
phrase	O
s	O
is	O
predicted	O
to	O
follow	O
a	O
given	O
preverbal	O
arguments	O
in	O
a	O
condition	O
.	O
Then	O
,	O
the	O
total	O
probability	O
of	O
s	O
to	O
be	O
predicted	O
in	O
the	O
given	O
condition	O
by	O
the	O
average	O
model	O
is	O
equal	O
to	O
1	O
10	O
10	O
i=1	O
p	O
i	O
(	O
s	O
)	O
,	O
where	O
p	O
i	O
(	O
s	O
)	O
denotes	O
the	O
probability	O
of	O
prediction	O
s	O
in	O
the	O
ith	O
run	O
.	O
Note	O
that	O
if	O
s	O
is	O
not	O
predicted	O
in	O
the	O
ith	O
run	O
,	O
then	O
p	O
i	O
(	O
s	O
)	O
=	O
0	B-DatasetName
.	O
In	O
the	O
next	O
subsections	O
,	O
we	O
present	O
two	O
models	O
with	O
different	O
noise	O
distribution	O
.	O

In	O
this	O
section	O
we	O
assess	O
the	O
span	O
and	O
quality	O
of	O
the	O
predictions	O
made	O
by	O
the	O
models	O
when	O
compared	O
to	O
the	O
human	O
data	O
.	O
The	O
span	O
of	O
verb	O
prediction	O
made	O
by	O
the	O
model	O
can	O
be	O
computed	O
by	O
the	O
proportion	O
of	O
human	O
distribution	O
that	O
the	O
model	O
misses	O
on	O
.	O
Formally	O
,	O
F	O
(	O
h	O
|	O
|	O
m	O
)	O
∝	O
x	O
VC	O
m	O
(	O
x	O
)	O
=	O
0	B-DatasetName
h	O
(	O
x	O
)	O
(	O
5	O
)	O
Since	O
the	O
model	O
will	O
not	O
be	O
able	O
to	O
predict	O
all	O
verb	O
classes	O
that	O
humans	O
produce	O
,	O
we	O
formulate	O
a	O
metric	O
to	O
evaluate	O
the	O
quality	O
of	O
the	O
predictions	O
that	O
the	O
model	O
makes	O
.	O
For	O
this	O
,	O
we	O
restrict	O
the	O
verb	O
classes	O
to	O
only	O
those	O
that	O
are	O
predicted	O
by	O
the	O
model	O
and	O
find	O
the	O
KL	O
-	O
divergence	O
(	O
Kullback	O
and	O
Leibler	O
,	O
1951	O
)	O
on	O
those	O
verb	O
classes	O
between	O
the	O
model	O
and	O
the	O
human	O
;	O
this	O
is	O
shown	O
in	O
(	O
6	O
)	O
D	O
(	O
h	O
|	O
|	O
m	O
)	O
=	O
x	O
VC	O
m	O
(	O
x	O
)	O
=	O
0	B-DatasetName
h	O
(	O
x	O
)	O
log	O
h	O
(	O
x	O
)	O
m	O
(	O
x	O
)	O
(	O
6	O
)	O
where	O
h	O
(	O
x	O
)	O
is	O
normalized	O
from	O
h	O
(	O
x	O
)	O
after	O
removing	O
x	O
where	O
m	O
(	O
x	O
)	O
=	O
0	B-DatasetName
.	O
Note	O
that	O
higher	O
the	O
F	O
,	O
lower	O
is	O
the	O
model	O
's	O
span	O
;	O
and	O
similarly	O
,	O
higher	O
the	O
D	O
,	O
lower	O
is	O
its	O
quality	O
of	O
predictions	O
(	O
as	O
compared	O
to	O
humans	O
)	O
.	O
Table	O
5	O
shows	O
that	O
for	O
both	O
F	O
and	O
D	O
,	O
the	O
LC	O
-	O
Surp	O
Pred	O
-	O
Rec	O
model	O
consistently	O
outperforms	O
the	O
LC	O
-	O
Surp	O
Pred	O
-	O
Bias	O
and	O
the	O
4	O
-	O
gram	O
surprisal	O
model	O
.	O
This	O
suggests	O
that	O
when	O
compared	O
to	O
the	O
human	O
data	O
,	O
the	O
LC	O
-	O
Surp	O
Pred	O
-	O
Rec	O
is	O
better	O
in	O
predicting	O
the	O
valid	O
verb	O
class	O
both	O
in	O
terms	O
of	O
span	O
and	O
the	O
quality	O
of	O
the	O
predictions	O
.	O

In	O
order	O
to	O
interpret	O
the	O
metrics	O
mentioned	O
in	O
Table	O
5	O
,	O
we	O
did	O
a	O
detailed	O
analysis	O
of	O
the	O
model	O
output	O
in	O
terms	O
of	O
the	O
nature	O
of	O
verb	O
class	O
and	O
the	O
type	O
of	O
prediction	O
errors	O
.	O
This	O
is	O
summarized	O
in	O
Table	O
6	O
.	O
One	O
can	O
note	O
that	O
Grammaticality	O
in	O
all	O
models	O
drops	O
in	O
3	O
-	O
NP	O
conditions	O
as	O
compared	O
to	O
2	O
-	O
NP	O
conditions	O
,	O
in	O
line	O
with	O
the	O
human	O
data	O
(	O
cf	O
.	O
Section	O
2	O
)	O
13	O
.	O
The	O
models	O
prefer	O
simple	O
outcomes	O
,	O
and	O
largely	O
predict	O
DT	O
,	O
CAU	O
S	O
(	O
grammatical	O
)	O
and	O
T	O
,	O
N	O
DT	O
(	O
ungrammatical	O
)	O
.	O
Investigating	O
the	O
reason	O
for	O
the	O
better	O
span	O
of	O
the	O
Pred	O
-	O
Rec	O
model	O
,	O
we	O
find	O
that	O
it	O
is	O
primarily	O
due	O
to	O
the	O
important	O
T	O
DT	O
verb	O
class	O
.	O
This	O
embedded	O
13	O
See	O
Section	O
5	O
of	O
the	O
supplement	O
for	O
actual	O
percentages	O
.	O
structure	O
is	O
often	O
used	O
by	O
humans	O
,	O
and	O
neither	O
of	O
the	O
4	O
-	O
gram	O
or	O
the	O
Pred	O
-	O
Bias	O
model	O
managed	O
to	O
predict	O
it	O
;	O
thus	O
,	O
we	O
can	O
link	O
the	O
better	O
span	O
numbers	O
of	O
the	O
Pred	O
-	O
Rec	O
model	O
to	O
an	O
observable	O
improvement	O
in	O
the	O
nature	O
of	O
verbal	O
predictions	O
.	O
We	O
also	O
study	O
the	O
error	O
types	O
made	O
by	O
the	O
models	O
and	O
compare	O
them	O
to	O
human	O
errors	O
.	O
The	O
4	O
-	O
gram	O
model	O
by	O
its	O
nature	O
is	O
only	O
capable	O
of	O
making	O
the	O
locally	O
coherent	O
N2	O
-	O
N3	O
errors	O
,	O
whereas	O
both	O
the	O
Pred	O
-	O
Bias	O
and	O
Pred	O
-	O
Rec	O
models	O
produce	O
N1	O
-	O
N3	O
and	O
N1	O
-	O
N2	O
errors	O
as	O
well	O
.	O
However	O
,	O
while	O
the	O
human	O
data	O
was	O
sensitive	O
to	O
the	O
subject	O
primacy	O
effect	O
-	O
presence	O
of	O
Ergative	O
case	O
-	O
marker	O
never	O
lead	O
to	O
passive	O
verb	O
completion	O
;	O
none	O
of	O
the	O
models	O
is	O
able	O
to	O
fully	O
replicate	O
this	O
pattern	O
.	O
However	O
,	O
the	O
4	O
-	O
gram	O
model	O
produces	O
the	O
least	O
percentage	O
of	O
passives	O
,	O
followed	O
by	O
the	O
Pred	O
-	O
Rec	O
model	O
.	O
See	O
Section	O
6	O
of	O
the	O
supplementary	B-DatasetName
material	I-DatasetName
for	O
more	O
details	O
about	O
error	O
types	O
.	O

Results	O
show	O
that	O
the	O
Lossy	O
context	O
surprisal	O
model	O
with	O
Predictability	O
Recency	O
Bias	O
noise	O
performs	O
best	O
in	O
terms	O
of	O
the	O
distribution	O
of	O
predicted	O
verbs	O
and	O
the	O
error	O
types	O
vis	O
-	O
à	O
-	O
vis	O
the	O
completion	O
data	O
.	O
This	O
provides	O
support	O
for	O
the	O
noisy	O
channel	O
hypothesis	O
and	O
poses	O
a	O
challenge	O
to	O
the	O
adaptability	O
hypothesis	O
.	O
In	O
addition	O
,	O
the	O
comparison	O
of	O
the	O
two	O
lossy	O
surprisal	O
models	O
sheds	O
light	O
on	O
the	O
nature	O
of	O
the	O
noise	O
during	O
the	O
reconstruction	O
process	O
.	O
Results	O
show	O
that	O
qualitatively	O
all	O
the	O
models	O
capture	O
the	O
completion	O
data	O
to	O
a	O
certain	O
extent	O
(	O
see	O
,	O
Section	O
6.2	O
)	O
.	O
At	O
the	O
same	O
time	O
,	O
overall	O
the	O
noisy	O
context	O
models	O
performed	O
better	O
than	O
the	O
ngram	O
model	O
in	O
two	O
clear	O
ways	O
.	O
First	O
,	O
the	O
models	O
were	O
able	O
to	O
capture	O
the	O
differential	O
nature	O
of	O
casemarker	O
combination	O
in	O
a	O
limited	O
context	O
.	O
This	O
leads	O
to	O
better	O
coverage	O
of	O
error	O
sources	O
(	O
both	O
in	O
terms	O
of	O
errors	O
made	O
and	O
not	O
made	O
)	O
.	O
Second	O
,	O
the	O
models	O
were	O
therefore	O
also	O
better	O
at	O
making	O
better	O
verb	O
predictions	O
compared	O
to	O
the	O
n	O
-	O
gram	O
model	O
.	O
In	O
particular	O
,	O
the	O
overall	O
success	O
of	O
the	O
Pred	O
-	O
Rec	O
model	O
showed	O
that	O
reconstruction	O
of	O
the	O
noisy	O
context	O
in	O
influenced	O
by	O
both	O
past	O
exposure	O
of	O
preverbal	O
subcontext	O
and	O
the	O
recency	O
of	O
the	O
context	O
(	O
cf	O
.	O
Futrell	O
et	O
al	O
,	O
2020	O
)	O
.	O
Put	O
differently	O
,	O
the	O
reconstruction	O
of	O
the	O
context	O
is	O
driven	O
by	O
sub	O
-	O
strings	O
that	O
are	O
more	O
frequent	O
(	O
e.g.	O
,	O
ne	O
-	O
ko	O
)	O
and	O
that	O
are	O
closer	O
to	O
the	O
verb	O
.	O
Critically	O
,	O
this	O
shows	O
that	O
the	O
reconstruction	O
process	O
is	O
not	O
random	O
.	O
14	O
While	O
the	O
performance	O
of	O
the	O
predictability	O
recency	O
model	O
is	O
good	O
,	O
it	O
suffers	O
from	O
three	O
issues	O
(	O
a	O
)	O
it	O
overestimates	O
the	O
number	O
of	O
errors	O
made	O
by	O
humans	O
,	O
(	O
b	O
)	O
its	O
overall	O
coverage	O
for	O
various	O
verb	O
class	O
is	O
low	O
,	O
and	O
(	O
c	O
)	O
it	O
is	O
insensitive	O
to	O
subject	O
primacy	O
.	O
The	O
model	O
is	O
able	O
to	O
successfully	O
predict	O
verb	O
phrase	O
involving	O
no	O
clausal	O
embedding	O
,	O
and	O
to	O
a	O
limited	O
extent	O
,	O
those	O
with	O
embeddings	O
.	O
While	O
certain	O
complex	O
structures	O
such	O
as	O
N	O
DT	O
DT	O
,	O
predicted	O
rarely	O
by	O
humans	O
,	O
are	O
dropped	O
entirely	O
by	O
the	O
model	O
,	O
its	O
prediction	O
for	O
the	O
T	O
DT	O
structure	O
which	O
is	O
frequent	O
in	O
the	O
completion	O
data	O
is	O
not	O
that	O
high	O
.	O
An	O
investigation	O
into	O
the	O
data	O
also	O
shows	O
a	O
scarcity	O
of	O
training	O
examples	O
that	O
exhibit	O
an	O
animate	O
3	O
-	O
NP	O
context	O
followed	O
by	O
such	O
T	O
DT	O
continuations	O
.	O
15	O
One	O
reason	O
for	O
this	O
could	O
be	O
the	O
size	O
of	O
the	O
training	O
data	O
,	O
currently	O
5	O
million	O
sentences	O
;	O
future	O
work	O
can	O
train	O
on	O
a	O
larger	O
data	O
set	O
.	O
Another	O
possibility	O
is	O
that	O
certain	O
patterns	O
in	O
the	O
human	O
data	O
are	O
not	O
captured	O
in	O
the	O
written	O
corpus	O
used	O
for	O
training	O
and	O
requires	O
a	O
dialogue	O
corpus	O
.	O
Unfortunately	O
,	O
such	O
a	O
corpus	O
currently	O
does	O
not	O
exist	O
for	O
Hindi	O
and	O
attempts	O
to	O
modeling	O
using	O
such	O
a	O
data	O
will	O
have	O
to	O
wait	O
its	O
availability	O
.	O
Relatedly	O
,	O
argue	O
that	O
prediction	O
based	O
on	O
corpus	O
frequency	O
of	O
syntactic	O
information	O
may	O
not	O
be	O
able	O
to	O
fully	O
capture	O
the	O
notion	O
of	O
preactivation	O
during	O
the	O
completion	O
task	O
.	O
Hence	O
,	O
future	O
work	O
will	O
need	O
to	O
incorporate	O
other	O
sources	O
of	O
information	O
.	O
Finally	O
,	O
the	O
results	O
show	O
that	O
the	O
4	O
-	O
gram	O
model	O
is	O
more	O
sensitive	O
to	O
subject	O
primacy	O
.	O
This	O
is	O
because	O
,	O
the	O
4	O
-	O
gram	O
model	O
(	O
unlike	O
noisy	O
context	O
models	O
)	O
has	O
access	O
to	O
the	O
N1	O
features	O
when	O
making	O
predictions	O
.	O
It	O
can	O
thus	O
correctly	O
use	O
the	O
N1	O
case	O
feature	O
to	O
avoid	O
predicting	O
passive	O
verbs	O
.	O
This	O
suggests	O
that	O
a	O
noise	O
function	O
relying	O
only	O
on	O
local	O
information	O
will	O
be	O
limited	O
in	O
accounting	O
for	O
the	O
current	O
data	O
.	O
tion	O
5	O
,	O
we	O
also	O
investigated	O
a	O
purely	O
random	O
noise	O
function	O
.	O
Due	O
to	O
space	O
constraint	O
,	O
details	O
of	O
this	O
model	O
have	O
been	O
mentioned	O
as	O
supplementary	B-DatasetName
material	I-DatasetName
(	O
Section	O
7	O
)	O
.	O
15	O
See	O
Section	O
3	O
of	O
the	O
supplementary	B-DatasetName
material	I-DatasetName
for	O
more	O
details	O
on	O
training	O
data	O
.	O
The	O
current	O
work	O
provided	O
the	O
first	O
set	O
of	O
detailed	O
results	O
towards	O
modeling	O
clause	O
final	O
verb	O
prediction	O
in	O
an	O
SOV	O
language	O
.	O
The	O
work	O
demonstrated	O
the	O
effectiveness	O
of	O
lossy	O
surprisal	O
models	O
and	O
probed	O
the	O
nature	O
of	O
the	O
noise	O
function	O
during	O
the	O
reconstruction	O
process	O
.	O
In	O
addition	O
to	O
the	O
quantitative	O
analyses	O
demonstrating	O
the	O
success	O
of	O
the	O
Predictability	O
Recency	O
lossy	O
surprisal	O
model	O
,	O
a	O
key	O
contribution	O
of	O
the	O
work	O
was	O
that	O
it	O
highlighted	O
the	O
nature	O
of	O
model	O
's	O
closeness	O
to	O
the	O
human	O
data	O
,	O
both	O
in	O
terms	O
of	O
verb	O
class	O
prediction	O
and	O
the	O
error	O
type	O
.	O
Overall	O
,	O
the	O
results	O
support	O
the	O
proposals	O
that	O
highlight	O
the	O
detrimental	O
effect	O
of	O
increased	O
complexity	O
of	O
the	O
preverbal	O
linguistic	O
material	O
in	O
SOV	O
languages	O
(	O
e.g.	O
,	O
Gibson	O
et	O
al	O
,	O
2013	O
;	O
Ueno	O
and	O
Polinsky	O
,	O
2009	O
;	O
Ros	O
et	O
al	O
,	O
2015	O
;	O
Yadav	O
et	O
al	O
,	O
2020	O
)	O
.	O
Future	O
models	O
need	O
to	O
explore	O
other	O
noise	O
functions	O
to	O
investigate	O
the	O
interaction	O
of	O
context	O
predictability	O
with	O
recency	O
as	O
well	O
as	O
primacy	O
of	O
non	O
-	O
local	O
information	O
(	O
e.g.	O
,	O
subject	O
)	O
.	O
Further	O
,	O
these	O
models	O
need	O
to	O
be	O
tested	O
to	O
investigate	O
the	O
effect	O
of	O
distance	O
(	O
e.g.	O
,	O
Vasishth	O
and	O
Lewis	O
,	O
2006	O
)	O
and	O
structural	O
complexity	O
(	O
Vasishth	O
et	O
al	O
,	O
2010	O
)	O
on	O
verbal	O
prediction	O
in	O
SOV	O
languages	O
.	O

Interpretability	O
methods	O
for	O
neural	O
networks	O
are	O
difficult	O
to	O
evaluate	O
because	O
we	O
do	O
not	O
understand	O
the	O
black	O
-	O
box	O
models	O
typically	O
used	O
to	O
test	O
them	O
.	O
This	O
paper	O
proposes	O
a	O
framework	O
in	O
which	O
interpretability	O
methods	O
are	O
evaluated	O
using	O
manually	O
constructed	O
networks	O
,	O
which	O
we	O
call	O
white	O
-	O
box	O
networks	O
,	O
whose	O
behavior	O
is	O
understood	O
a	O
priori	O
.	O
We	O
evaluate	O
five	O
methods	O
for	O
producing	O
attribution	O
heatmaps	O
by	O
applying	O
them	O
to	O
white	O
-	O
box	O
LSTM	B-MethodName
classifiers	O
for	O
tasks	O
based	O
on	O
formal	O
languages	O
.	O
Although	O
our	O
white	O
-	O
box	O
classifiers	O
solve	O
their	O
tasks	O
perfectly	O
and	O
transparently	O
,	O
we	O
find	O
that	O
all	O
five	O
attribution	O
methods	O
fail	O
to	O
produce	O
the	O
expected	O
model	O
explanations	O
.	O

Attribution	O
methods	O
are	O
a	O
family	O
of	O
interpretability	O
techniques	O
for	O
individual	O
neural	O
network	O
predictions	O
that	O
attempt	O
to	O
measure	O
the	O
importance	O
of	O
input	O
features	O
for	O
determining	O
the	O
model	O
's	O
output	O
.	O
Given	O
an	O
input	O
,	O
an	O
attribution	O
method	O
produces	O
a	O
vector	O
of	O
attribution	O
or	O
relevance	O
scores	O
,	O
which	O
is	O
typically	O
visualized	O
as	O
a	O
heatmap	B-MethodName
that	O
highlights	O
portions	O
of	O
the	O
input	O
that	O
contribute	O
to	O
model	O
behavior	O
.	O
In	O
the	O
context	O
of	O
NLP	O
,	O
attribution	O
scores	O
are	O
usually	O
computed	O
at	O
the	O
token	O
level	O
,	O
so	O
that	O
each	O
score	O
represents	O
the	O
importance	O
of	O
a	O
token	O
within	O
an	O
input	O
sequence	O
.	O
These	O
heatmaps	O
can	O
be	O
used	O
to	O
identify	O
keywords	O
upon	O
which	O
networks	O
base	O
their	O
decisions	O
(	O
Li	O
et	O
al	O
,	O
2016	O
;	O
Sundararajan	O
et	O
al	O
,	O
2017	O
;	O
Arras	O
et	O
al	O
,	O
2017a	O
,	O
b	O
;	O
Murdoch	O
et	O
al	O
,	O
2018	O
,	O
inter	O
alia	O
)	O
.	O
One	O
of	O
the	O
main	O
challenges	O
facing	O
the	O
evaluation	O
of	O
attribution	O
methods	O
is	O
that	O
it	O
is	O
difficult	O
to	O
assess	O
the	O
quality	O
of	O
a	O
heatmap	B-MethodName
when	O
the	O
network	O
in	O
question	O
is	O
not	O
understood	O
in	O
the	O
first	O
place	O
.	O
If	O
a	O
word	O
is	O
deemed	O
relevant	O
by	O
an	O
attribution	O
method	O
,	O
we	O
do	O
not	O
know	O
whether	O
the	O
model	O
actually	O
considers	O
that	O
word	O
relevant	O
,	O
or	O
whether	O
the	O
attribu	O
-	O
tion	O
method	O
has	O
erroneously	O
estimated	O
its	O
importance	O
.	O
Indeed	O
,	O
previous	O
studies	O
have	O
argued	O
that	O
attribution	O
methods	O
are	O
sensitive	O
to	O
features	O
unrelated	O
to	O
model	O
behavior	O
in	O
some	O
cases	O
(	O
e.g.	O
,	O
Kindermans	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
altogether	O
insensitive	O
to	O
model	O
behavior	O
in	O
others	O
(	O
Adebayo	O
et	O
al	O
,	O
2018	O
)	O
.	O
To	O
tease	O
the	O
evaluation	O
of	O
attribution	O
methods	O
apart	O
from	O
the	O
interpretation	O
of	O
models	O
,	O
this	O
paper	O
proposes	O
an	O
evaluation	O
framework	O
for	O
attribution	O
methods	O
in	O
NLP	O
that	O
uses	O
only	O
models	O
that	O
are	O
fully	O
understood	O
a	O
priori	O
.	O
Instead	O
of	O
testing	O
attribution	O
methods	O
on	O
black	O
-	O
box	O
models	O
obtained	O
through	O
training	O
,	O
we	O
construct	O
white	O
-	O
box	O
models	O
for	O
testing	O
by	O
directly	O
setting	O
network	O
parameters	O
by	O
hand	O
.	O
Our	O
focus	O
is	O
on	O
white	O
-	O
box	O
LSTMs	O
that	O
implement	O
intuitive	O
strategies	O
for	O
solving	O
simple	O
classification	O
tasks	O
based	O
on	O
formal	O
languages	O
with	O
deterministic	O
solutions	O
.	O
We	O
apply	O
our	O
framework	O
to	O
five	O
attribution	O
methods	O
:	O
occlusion	O
(	O
Zeiler	O
and	O
Fergus	O
,	O
2014	O
)	O
,	O
saliency	O
(	O
Simonyan	O
et	O
al	O
,	O
2014	O
;	O
Li	O
et	O
al	O
,	O
2016	O
)	O
,	O
gradient	O
×	O
input	O
,	O
(	O
G	O
×	O
I	O
,	O
Shrikumar	O
et	O
al	O
,	O
2017	O
)	O
,	O
integrated	O
gradients	O
(	O
IG	O
,	O
Sundararajan	O
et	O
al	O
,	O
2017	O
)	O
,	O
and	O
layer	O
-	O
wise	O
relevance	O
propagation	O
(	O
LRP	O
,	O
Bach	O
et	O
al	O
,	O
2015	O
)	O
.	O
In	O
doing	O
so	O
,	O
we	O
make	O
the	O
following	O
contributions	O
.	O
We	O
construct	O
four	O
white	O
-	O
box	O
LSTMs	O
that	O
can	O
be	O
used	O
to	O
test	O
attribution	O
methods	O
.	O
We	O
provide	O
a	O
complete	O
description	O
of	O
our	O
model	O
weights	O
in	O
Appendix	O
A.	O
1	O
Beyond	O
the	O
five	O
methods	O
considered	O
here	O
,	O
our	O
white	O
-	O
box	O
networks	O
can	O
be	O
used	O
to	O
test	O
any	O
attribution	O
method	O
compatible	O
with	O
LSTMs	O
.	O
Empirically	O
,	O
we	O
show	O
that	O
all	O
five	O
attribution	O
methods	O
produce	O
erroneous	O
heatmaps	O
for	O
our	O
white	O
-	O
box	O
networks	O
,	O
despite	O
the	O
models	O
'	O
transparent	O
behavior	O
.	O
As	O
a	O
preview	O
of	O
our	O
re	O
-	O
sults	O
,	O
Table	O
1	O
shows	O
sample	O
heatmaps	O
computed	O
for	O
two	O
models	O
designed	O
to	O
identify	O
the	O
non	O
-	O
contiguous	O
subsequence	O
ab	O
in	O
the	O
input	O
aacb	O
.	O
Even	O
though	O
both	O
models	O
'	O
outputs	O
are	O
determined	O
by	O
the	O
presence	O
of	O
the	O
two	O
as	O
and	O
the	O
b	O
,	O
all	O
four	O
methods	O
either	O
incorrectly	O
highlight	O
the	O
c	O
or	O
fail	O
to	O
highlight	O
at	O
least	O
one	O
of	O
the	O
as	O
in	O
at	O
least	O
one	O
case	O
.	O
We	O
identify	O
two	O
general	O
ways	O
in	O
which	O
four	O
of	O
the	O
five	O
methods	O
do	O
not	O
behave	O
as	O
intended	O
.	O
Firstly	O
,	O
while	O
saliency	O
,	O
G	O
×	O
I	O
and	O
IG	O
are	O
theoretically	O
invariant	O
to	O
differences	O
in	O
model	O
implementation	O
(	O
Sundararajan	O
et	O
al	O
,	O
2017	O
)	O
,	O
in	O
practice	O
we	O
find	O
that	O
these	O
methods	O
can	O
still	O
produce	O
qualitatively	O
different	O
heatmaps	O
for	O
nearly	O
identical	O
models	O
.	O
Secondly	O
,	O
we	O
find	O
that	O
LRP	O
is	O
susceptible	O
to	O
numerical	O
issues	O
,	O
which	O
cause	O
heatmaps	O
to	O
be	O
zeroed	O
out	O
when	O
values	O
are	O
rounded	O
to	O
zero	O
.	O

Several	O
approaches	O
have	O
been	O
taken	O
in	O
the	O
literature	O
for	O
understanding	O
how	O
to	O
evaluate	O
attribution	O
methods	O
.	O
On	O
a	O
theoretical	O
level	O
,	O
axiomatic	O
approaches	O
propose	O
formal	O
desiderata	O
that	O
attribution	O
methods	O
should	O
satisfy	O
,	O
such	O
as	O
implementation	O
invariance	O
(	O
Sundararajan	O
et	O
al	O
,	O
2017	O
)	O
,	O
input	O
translation	O
invariance	O
(	O
Kindermans	O
et	O
al	O
,	O
2019	O
)	O
,	O
continuity	O
with	O
respect	O
to	O
inputs	O
(	O
Montavon	O
et	O
al	O
,	O
2018	O
;	O
Ghorbani	O
et	O
al	O
,	O
2019	O
)	O
,	O
or	O
the	O
existence	O
of	O
relationships	O
between	O
attribution	O
scores	O
and	O
logit	O
or	O
softmax	B-MethodName
scores	O
(	O
Sundararajan	O
et	O
al	O
,	O
2017	O
;	O
Ancona	O
et	O
al	O
,	O
2018	O
;	O
Montavon	O
,	O
2019	O
)	O
.	O
The	O
degree	O
to	O
which	O
attribution	O
methods	O
fulfill	O
these	O
criteria	O
can	O
be	O
determined	O
either	O
mathematically	O
or	O
empirically	O
.	O
Other	O
approaches	O
,	O
which	O
are	O
more	O
experimental	O
in	O
nature	O
,	O
attempt	O
to	O
directly	O
assess	O
the	O
relationship	O
between	O
attribution	O
scores	O
and	O
model	O
behav	O
-	O
ior	O
.	O
A	O
common	O
test	O
,	O
due	O
to	O
Bach	O
et	O
al	O
(	O
2015	O
)	O
and	O
Samek	O
et	O
al	O
(	O
2017	O
)	O
and	O
applied	O
to	O
sequence	O
modeling	O
by	O
Arras	O
et	O
al	O
(	O
2017a	O
)	O
,	O
involves	O
ablating	O
or	O
perturbing	O
parts	O
of	O
the	O
input	O
,	O
from	O
those	O
with	O
the	O
highest	O
attribution	O
scores	O
to	O
those	O
with	O
the	O
lowest	O
,	O
and	O
counting	O
the	O
number	O
of	O
features	O
that	O
need	O
to	O
be	O
ablated	O
in	O
order	O
to	O
change	O
the	O
model	O
's	O
prediction	O
.	O
Another	O
test	O
,	O
proposed	O
by	O
Adebayo	O
et	O
al	O
(	O
2018	O
)	O
,	O
tracks	O
how	O
heatmaps	O
change	O
as	O
layers	O
of	O
a	O
network	O
are	O
incrementally	O
randomized	O
.	O
A	O
third	O
kind	O
of	O
approach	O
evaluates	O
the	O
extent	O
to	O
which	O
heatmaps	O
identify	O
salient	O
input	O
features	O
.	O
For	O
example	O
,	O
Zhang	O
et	O
al	O
(	O
2018	O
)	O
propose	O
the	O
pointing	O
game	O
task	O
,	O
in	O
which	O
the	O
highest	O
-	O
relevance	O
pixel	O
for	O
an	O
image	O
classifier	O
input	O
must	O
belong	O
to	O
the	O
object	O
described	O
by	O
the	O
target	O
output	O
class	O
.	O
Within	O
this	O
framework	O
,	O
)	O
,	O
Poerner	O
et	O
al	O
(	O
2018	O
,	O
Arras	O
et	O
al	O
(	O
2019	O
)	O
,	O
and	O
Yang	O
and	O
Kim	O
(	O
2019	O
)	O
construct	O
datasets	O
in	O
which	O
input	O
features	O
exhibit	O
experimentally	O
controlled	O
notions	O
of	O
importance	O
,	O
yielding	O
"	O
ground	O
truth	O
"	O
attributions	O
against	O
which	O
heatmaps	O
can	O
be	O
evaluated	O
.	O
Our	O
paper	O
incorporates	O
elements	O
of	O
the	O
groundtruth	O
approaches	O
,	O
since	O
it	O
is	O
straightforward	O
to	O
determine	O
which	O
input	O
features	O
are	O
important	O
for	O
our	O
formal	O
language	O
tasks	O
.	O
We	O
enhance	O
these	O
approaches	O
by	O
using	O
white	O
-	O
box	O
models	O
that	O
are	O
guaranteed	O
to	O
be	O
sensitive	O
to	O
those	O
features	O
.	O

Counter	O
languages	O
(	O
Fischer	O
,	O
1966	O
;	O
Fischer	O
et	O
al	O
,	O
1968	O
)	O
are	O
languages	O
recognized	O
by	O
automata	O
equipped	O
with	O
counters	O
.	O
Weiss	O
et	O
al	O
(	O
2018	O
)	O
demonstrate	O
using	O
an	O
acceptance	O
task	O
for	O
the	O
languages	O
a	O
n	O
b	O
n	O
and	O
a	O
n	O
b	O
n	O
c	O
n	O
that	O
LSTMs	O
naturally	O
learn	O
to	O
use	O
cell	O
state	O
units	O
as	O
counters	O
.	O
Merrill	O
's	O
(	O
2019	O
)	O
asymptotic	O
analysis	O
shows	O
that	O
LSTM	B-MethodName
acceptors	O
accept	O
only	O
counter	O
languages	O
when	O
their	O
weights	O
are	O
fully	O
saturated	O
.	O
Thus	O
,	O
counter	O
languages	O
may	O
be	O
viewed	O
as	O
a	O
characterization	O
of	O
the	O
expressive	O
power	O
of	O
LSTMs	O
.	O
We	O
define	O
the	O
counting	O
task	O
based	O
on	O
a	O
simple	O
example	O
of	O
a	O
counting	O
language	O
.	O
Task	O
1	O
(	O
Counting	O
Task	O
)	O
.	O
Given	O
a	O
string	O
in	O
x	O
{	O
a	O
,	O
b	O
}	O
*	O
,	O
determine	O
whether	O
or	O
not	O
x	O
has	O
strictly	O
more	O
as	O
than	O
bs	O
.	O
Example	O
2	O
.	O
The	O
counting	O
task	O
classifies	O
aaab	O
as	O
True	O
,	O
ab	O
as	O
False	O
,	O
and	O
bbbba	O
as	O
False	O
.	O
A	O
counter	O
automaton	O
can	O
solve	O
the	O
counting	O
task	O
by	O
incrementing	O
its	O
counter	O
whenever	O
an	O
a	O
is	O
encountered	O
and	O
decrementing	O
it	O
whenever	O
a	O
b	O
is	O
encountered	O
.	O
It	O
outputs	O
True	O
if	O
and	O
only	O
if	O
its	O
counter	O
is	O
at	O
least	O
1	O
.	O
We	O
expect	O
attribution	O
scores	O
for	O
all	O
input	O
symbols	O
to	O
have	O
roughly	O
the	O
same	O
magnitude	O
,	O
but	O
that	O
scores	O
assigned	O
to	O
a	O
will	O
have	O
the	O
opposite	O
sign	O
to	O
those	O
assigned	O
to	O
b.	O

The	O
Dyck	O
language	O
is	O
the	O
language	O
D	O
generated	O
by	O
the	O
following	O
context	O
-	O
free	O
grammar	O
,	O
where	O
ε	B-HyperparameterName
is	O
the	O
empty	O
string	O
.	O

We	O
use	O
two	O
approaches	O
to	O
construct	O
white	O
-	O
box	O
networks	O
for	O
our	O
tasks	O
.	O
In	O
the	O
counter	O
-	O
based	O
approach	O
,	O
the	O
cell	O
state	O
contains	O
a	O
set	O
of	O
counters	O
,	O
which	O
are	O
incremented	O
or	O
decremented	O
throughout	O
the	O
computation	O
.	O
The	O
network	O
's	O
final	O
output	O
is	O
based	O
on	O
the	O
values	O
of	O
the	O
counters	O
.	O
In	O
the	O
automaton	O
-	O
based	O
approach	O
,	O
we	O
use	O
the	O
LSTM	B-MethodName
to	O
simulate	O
an	O
automaton	O
,	O
with	O
the	O
cell	O
state	O
containing	O
a	O
representation	O
of	O
the	O
automaton	O
's	O
state	O
.	O
We	O
use	O
a	O
counter	O
-	O
based	O
network	O
to	O
solve	O
the	O
counter	O
task	O
and	O
an	O
automaton	O
-	O
based	O
network	O
to	O
solve	O
the	O
bracket	O
prediction	O
task	O
.	O
We	O
use	O
both	O
kinds	O
of	O
networks	O
to	O
solve	O
the	O
SP	O
task	O
.	O
All	O
networks	O
perfectly	O
solve	O
the	O
tasks	O
they	O
were	O
designed	O
for	O
.	O
This	O
section	O
describes	O
our	O
white	O
-	O
box	O
networks	O
at	O
a	O
high	O
level	O
;	O
a	O
detailed	O
description	O
is	O
given	O
in	O
Appendix	O
A.	O
In	O
the	O
rest	O
of	O
this	O
paper	O
,	O
we	O
identify	O
the	O
alphabet	O
symbols	O
a	O
,	O
b	O
,	O
c	O
,	O
and	O
d	O
with	O
the	O
one	O
-	O
hot	O
vectors	O
for	O
indices	O
1	O
,	O
2	O
,	O
3	O
,	O
and	O
4	O
,	O
respectively	O
.	O
The	O
vectors	O
f	O
(	O
t	O
)	O
,	O
i	O
(	O
t	O
)	O
,	O
and	O
o	O
(	O
t	O
)	O
represent	O
the	O
forget	O
,	O
input	O
,	O
and	O
output	O
gates	O
,	O
respectively	O
.	O
g	O
(	O
t	O
)	O
is	O
the	O
value	O
added	O
to	O
the	O
cell	O
state	O
at	O
each	O
time	O
step	O
,	O
and	O
σ	O
represents	O
the	O
sigmoid	O
function	O
.	O
We	O
assume	O
that	O
the	O
hidden	O
state	O
h	O
(	O
t	O
)	O
and	O
cell	O
state	O
c	O
(	O
t	O
)	O
are	O
updated	O
as	O
follows	O
.	O
c	O
(	O
t	O
)	O
=	O
f	O
(	O
t	O
)	O
c	O
(	O
t−1	O
)	O
+	O
i	O
(	O
t	O
)	O
g	O
(	O
t	O
)	O
h	O
(	O
t	O
)	O
=	O
o	O
(	O
t	O
)	O
tanh	O
(	O
c	O
(	O
t	O
)	O
)	O

In	O
the	O
counter	O
-	O
based	O
approach	O
,	O
each	O
position	O
of	O
the	O
cell	O
state	O
contains	O
the	O
value	O
of	O
a	O
counter	O
.	O
To	O
adjust	O
the	O
counter	O
in	O
position	O
j	O
by	O
some	O
value	O
v	O
(	O
−1	O
,	O
1	O
)	O
,	O
we	O
set	O
g	O
(	O
t	O
)	O
j	O
=	O
v	O
,	O
and	O
we	O
saturate	O
the	O
gates	O
by	O
setting	O
them	O
to	O
σ	O
(	O
m	O
)	O
≈	O
1	O
,	O
where	O
m	O
≫	O
0	B-DatasetName
is	O
a	O
large	O
constant	O
.	O
For	O
example	O
,	O
our	O
network	O
for	O
the	O
counting	O
task	O
uses	O
a	O
single	O
hidden	O
unit	O
,	O
with	O
the	O
gates	O
always	O
saturated	O
and	O
with	O
g	O
(	O
t	O
)	O
given	O
by	O
g	O
(	O
t	O
)	O
=	O
tanh	O
(	O
u	O
[	O
1	O
−1	O
]	O
x	O
(	O
t	O
)	O
)	O
,	O
where	O
u	O
>	O
0	B-DatasetName
is	O
a	O
hyperparameter	O
that	O
scales	O
the	O
counter	O
by	O
a	O
factor	O
of	O
v	O
=	O
tanh	O
(	O
u	O
)	O
.	O
2	O
When	O
x	O
(	O
t	O
)	O
=	O
a	O
,	O
we	O
have	O
g	O
(	O
t	O
)	O
=	O
v	O
,	O
so	O
the	O
counter	O
is	O
incremented	O
by	O
v.	O
When	O
x	O
(	O
t	O
)	O
=	O
b	O
,	O
we	O
compute	O
g	O
(	O
t	O
)	O
=	O
−v	O
,	O
so	O
the	O
counter	O
is	O
decremented	O
by	O
v.	O
For	O
the	O
SP	O
task	O
,	O
we	O
use	O
seven	O
counters	O
.	O
The	O
first	O
four	O
counters	O
record	O
how	O
many	O
occurrences	O
of	O
each	O
symbol	O
have	O
been	O
observed	O
at	O
time	O
step	O
t.	O
The	O
next	O
three	O
counters	O
record	O
the	O
number	O
of	O
bs	O
,	O
cs	O
,	O
and	O
ds	O
that	O
form	O
one	O
of	O
the	O
four	O
distinguished	O
subsequences	O
with	O
an	O
earlier	O
symbol	O
.	O
For	O
example	O
,	O
after	O
seeing	O
the	O
input	O
aaabbc	O
,	O
the	O
counterbased	O
network	O
for	O
the	O
SP	O
task	O
satisfies	O
c	O
(	O
6	O
)	O
=	O
v	O
[	O
3	O
2	O
1	O
0	B-DatasetName
2	O
1	O
0	B-DatasetName
]	O
⊤	O
.	O
The	O
first	O
four	O
counters	O
represent	O
the	O
fact	O
that	O
the	O
input	O
has	O
3	O
as	O
,	O
2	O
bs	O
,	O
1	O
c	O
,	O
and	O
no	O
ds	O
.	O
Counter	O
#	O
5	O
is	O
2v	O
because	O
the	O
two	O
bs	O
form	O
a	O
subsequence	O
with	O
the	O
as	O
,	O
and	O
counter	O
#	O
6	O
is	O
v	O
because	O
the	O
c	O
forms	O
a	O
subsequence	O
with	O
the	O
bs	O
.	O
The	O
logit	O
scores	O
of	O
our	O
counter	O
-	O
based	O
networks	O
are	O
computed	O
by	O
a	O
linear	O
decoder	O
using	O
the	O
tanh	O
of	O
the	O
counter	O
values	O
.	O
For	O
the	O
counting	O
task	O
,	O
the	O
score	O
of	O
the	O
True	O
class	O
is	O
h	O
(	O
t	O
)	O
,	O
while	O
the	O
score	O
of	O
the	O
False	O
class	O
is	O
fixed	O
to	O
tanh	O
(	O
v	O
)	O
/2	O
.	O
This	O
means	O
that	O
the	O
network	O
outputs	O
True	O
if	O
and	O
only	O
if	O
the	O
final	O
counter	O
value	O
is	O
at	O
least	O
v.	O
For	O
the	O
SP	O
task	O
,	O
the	O
score	O
of	O
the	O
True	O
class	O
is	O
h	O
(	O
t	O
)	O
5	O
+	O
h	O
(	O
t	O
)	O
6	O
+	O
h	O
(	O
t	O
)	O
7	O
,	O
while	O
the	O
score	O
of	O
the	O
False	O
class	O
is	O
again	O
tanh	O
(	O
v	O
)	O
/2	O
.	O

Let	O
X	O
be	O
a	O
matrix	O
of	O
input	O
vectors	O
,	O
such	O
that	O
the	O
input	O
at	O
time	O
t	O
is	O
the	O
row	O
vector	O
X	O
t	O
,	O
:	O
=	O
(	O
x	O
(	O
t	O
)	O
)	O
⊤	O
.	O
Given	O
X	O
,	O
an	O
LSTM	B-MethodName
classifier	O
produces	O
a	O
vector	O
y	O
of	O
logit	O
scores	O
.	O
Based	O
on	O
X	O
,	O
ŷ	O
,	O
and	O
possibly	O
a	O
baseline	O
input	O
X	O
,	O
an	O
attribution	O
method	O
assigns	O
an	O
attribution	O
score	O
R	O
(	O
c	O
)	O
t	O
,	O
i	O
(	O
X	O
)	O
to	O
input	O
feature	O
X	O
t	O
,	O
i	O
for	O
each	O
output	O
class	O
c.	O
These	O
feature	O
-	O
level	O
scores	O
are	O
then	O
aggregated	O
to	O
produce	O
token	O
-	O
level	O
scores	O
:	O
R	O
(	O
c	O
)	O
t	O
(	O
X	O
)	O
=	O
∑	O
i	O
R	O
(	O
c	O
)	O
t	O
,	O
i	O
(	O
X	O
)	O
.	O
Broadly	O
speaking	O
,	O
our	O
five	O
attribution	O
methods	O
are	O
grouped	O
into	O
three	O
types	O
:	O
one	O
perturbation	O
-	O
based	O
,	O
three	O
gradient	O
-	O
based	O
,	O
and	O
one	O
decompositionbased	O
.	O
The	O
following	O
subsections	O
describe	O
how	O
each	O
method	O
computes	O
R	O
(	O
c	O
)	O
t	O
,	O
i	O
(	O
X	O
)	O
.	O

Perturbation	O
-	O
based	O
methods	O
are	O
premised	O
on	O
the	O
idea	O
that	O
if	O
X	O
t	O
,	O
i	O
is	O
an	O
important	O
input	O
feature	O
,	O
then	O
changing	O
the	O
value	O
of	O
X	O
t	O
,	O
i	O
would	O
causeŷ	O
to	O
change	O
.	O
The	O
one	O
perturbation	O
method	O
we	O
consider	O
is	O
occlusion	O
.	O
In	O
this	O
method	O
,	O
R	O
(	O
c	O
)	O
t	O
,	O
i	O
(	O
X	O
)	O
is	O
the	O
change	O
inŷ	O
c	O
observed	O
when	O
X	O
t	O
,	O
:	O
is	O
replaced	O
by	O
0	B-DatasetName
.	O
Gradient	O
-	O
based	O
methods	O
rely	O
on	O
the	O
same	O
intuition	O
as	O
perturbation	O
-	O
based	O
methods	O
,	O
but	O
use	O
automatic	O
differentiation	O
to	O
simulate	O
infinitesimal	O
perturbations	O
.	O
The	O
definitions	O
of	O
our	O
three	O
gradientbased	O
methods	O
are	O
given	O
in	O
Table	O
2	O
.	O
The	O
most	O
basic	O
of	O
these	O
is	O
saliency	O
,	O
which	O
simply	O
measures	O
relevance	O
by	O
the	O
derivative	O
of	O
the	O
logit	O
score	O
with	O
respect	O
to	O
each	O
input	O
feature	O
.	O
G	O
×	O
I	O
attempts	O
to	O
improve	O
upon	O
saliency	O
by	O
using	O
the	O
first	O
-	O
order	O
terms	O
in	O
a	O
Taylor	O
-	O
series	O
approximation	O
of	O
the	O
model	O
instead	O
of	O
the	O
gradients	O
on	O
their	O
own	O
.	O
IG	O
is	O
designed	O
to	O
address	O
the	O
issue	O
of	O
small	O
gradients	O
found	O
in	O
saturated	O
units	O
by	O
integrating	O
G	O
×	O
I	O
along	O
the	O
line	O
connecting	O
X	O
to	O
a	O
baseline	O
input	O
X	O
,	O
here	O
taken	O
to	O
be	O
the	O
zero	O
matrix	O
.	O

Occlusion	O
,	O
G	O
×	O
I	O
,	O
and	O
IG	O
are	O
well	O
-	O
behaved	O
for	O
the	O
counting	O
task	O
.	O
As	O
expected	O
,	O
these	O
methods	O
assign	O
a	O
a	O
positive	O
value	O
and	O
b	O
a	O
negative	O
value	O
when	O
the	O
output	O
class	O
for	O
attribution	O
is	O
c	O
=	O
True	O
.	O
When	O
the	O
number	O
of	O
as	O
is	O
different	O
from	O
the	O
number	O
of	O
bs	O
,	O
occlusion	O
assigns	O
a	O
lower	O
-	O
magnitude	O
score	O
to	O
the	O
symbol	O
with	O
fewer	O
instances	O
.	O
When	O
c	O
=	O
False	O
,	O
all	O
relevance	O
scores	O
are	O
0	B-DatasetName
.	O
This	O
is	O
becauseŷ	O
False	O
is	O
fixed	O
to	O
a	O
constant	O
value	O
supplied	O
by	O
a	O
bias	O
term	O
,	O
so	O
input	O
features	O
can	O
not	O
affect	O
its	O
value	O
.	O
Saliency	O
and	O
LRP	O
both	O
fail	O
to	O
produce	O
nonzero	O
scores	O
,	O
at	O
least	O
in	O
some	O
cases	O
.	O
Saliency	O
scores	O
satisfy	O
R	O
(	O
True	O
)	O
t	O
,	O
1	O
(	O
X	O
)	O
=	O
−R	O
(	O
True	O
)	O
t	O
,	O
2	O
(	O
X	O
)	O
,	O
resulting	O
in	O
token	O
-	O
level	O
scores	O
of	O
0	B-DatasetName
for	O
all	O
inputs	O
.	O
Heatmaps	O
#	O
3	O
and	O
#	O
4	O
show	O
that	O
LRP	O
assigns	O
scores	O
of	O
0	B-DatasetName
to	O
prefixes	O
containing	O
equal	O
numbers	O
of	O
as	O
and	O
bs	O
.	O
We	O
will	O
see	O
in	O
Subsection	O
7.1	O
that	O
this	O
phenomenon	O
appears	O
to	O
be	O
related	O
to	O
the	O
fact	O
that	O
the	O
LSTM	B-MethodName
gates	O
are	O
saturated	O
.	O

We	O
obtain	O
radically	O
different	O
heatmaps	O
for	O
the	O
two	O
SP	O
task	O
networks	O
,	O
despite	O
the	O
fact	O
that	O
they	O
produce	O
the	O
same	O
classifications	O
for	O
all	O
inputs	O
.	O
For	O
the	O
counter	O
-	O
based	O
network	O
,	O
all	O
methods	O
except	O
for	O
saliency	O
assign	O
positive	O
scores	O
for	O
c	O
=	O
True	O
to	O
symbols	O
constituting	O
one	O
of	O
the	O
four	O
subsequences	O
,	O
and	O
scores	O
of	O
zero	O
elsewhere	O
.	O
The	O
saliency	O
heatmaps	O
do	O
not	O
adhere	O
to	O
this	O
pattern	O
,	O
and	O
instead	O
generally	O
assign	O
higher	O
scores	O
to	O
tokens	O
occurring	O
near	O
the	O
end	O
of	O
the	O
input	O
.	O
Heatmaps	O
#	O
7	O
-	O
10	O
show	O
that	O
LRP	O
fails	O
to	O
assign	O
positive	O
scores	O
to	O
the	O
first	O
symbol	O
of	O
each	O
subsequence	O
,	O
while	O
the	O
other	O
methods	O
generally	O
do	O
not	O
.	O
4	O
The	O
LRP	O
behavior	O
reflects	O
the	O
fact	O
that	O
the	O
initial	O
a	O
does	O
not	O
increment	O
the	O
subsequence	O
counters	O
,	O
which	O
determine	O
the	O
final	O
logit	O
score	O
.	O
In	O
contrast	O
,	O
the	O
behavior	O
of	O
occlusion	O
,	O
G	O
×	O
I	O
,	O
and	O
IG	O
is	O
explained	O
by	O
the	O
fact	O
that	O
removing	O
either	O
the	O
a	O
or	O
the	O
b	O
destroys	O
the	O
subsequence	O
.	O
Note	O
that	O
the	O
as	O
in	O
heatmap	B-MethodName
#	O
9	O
receive	O
scores	O
of	O
0	B-DatasetName
from	O
occlusion	O
and	O
G	O
×	O
I	O
,	O
since	O
removing	O
only	O
one	O
of	O
the	O
two	O
as	O
does	O
not	O
destroy	O
the	O
subsequence	O
.	O
For	O
the	O
FSA	O
-	O
based	O
network	O
,	O
saliency	O
,	O
G	O
×	O
I	O
,	O
and	O
LRP	O
assign	O
only	O
the	O
last	O
symbol	O
a	O
nonzero	O
score	O
when	O
the	O
relevance	O
output	O
class	O
c	O
matches	O
the	O
network	O
's	O
predicted	O
class	O
.	O
IG	O
appears	O
to	O
produce	O
erratic	O
heatmaps	O
,	O
exhibiting	O
no	O
immediately	O
obvious	O
pattern	O
.	O
Although	O
occlusion	O
appears	O
to	O
be	O
erratic	O
at	O
first	O
glance	O
,	O
its	O
behavior	O
can	O
be	O
explained	O
by	O
the	O
fact	O
that	O
changing	O
x	O
(	O
t	O
)	O
to	O
0	B-DatasetName
causes	O
h	O
(	O
t	O
)	O
to	O
be	O
0	B-DatasetName
,	O
which	O
the	O
LSTM	B-MethodName
interprets	O
as	O
the	O
initial	O
state	O
of	O
the	O
FSA	O
;	O
thus	O
,	O
R	O
(	O
c	O
)	O
t	O
(	O
X	O
)	O
̸	O
=	O
0	B-DatasetName
precisely	O
when	O
X	O
t+1	O
:	O
,	O
:	O
is	O
classified	O
differently	O
from	O
X.	O
In	O
all	O
cases	O
,	O
the	O
heatmaps	O
for	O
the	O
FSA	O
-	O
based	O
network	O
diverge	O
significantly	O
from	O
the	O
expected	O
heatmaps	O
.	O

Topic	O
information	O
as	O
a	O
crucial	O
auxiliary	O
for	O
text	O
understanding	O
has	O
drawn	O
great	O
attention	O
in	O
recent	O
decades	O
(	O
Wu	O
et	O
al	O
,	O
2019	O
;	O
Sahlgren	O
,	O
2020	O
)	O
.	O
In	O
the	O
literature	O
,	O
previous	O
studies	O
on	O
topic	O
modeling	O
usually	O
extract	O
topics	O
by	O
introducing	O
latent	O
variables	O
for	O
tokens	O
for	O
topic	O
assigning	O
(	O
Hofmann	O
,	O
1999	O
;	O
Blei	O
et	O
al	O
,	O
2003	O
;	O
Yishu	O
et	O
al	O
,	O
2017	O
)	O
.	O
Similarly	O
,	O
researches	O
on	O
text	O
-	O
tilling	O
achieve	O
topic	O
segments	O
through	O
lexical	O
cohesion	O
modeling	O
(	O
Hearst	O
,	O
1997	O
;	O
Purver	O
et	O
al	O
,	O
2006	O
)	O
.	O
Instead	O
of	O
lexical	O
cohesion	O
measuring	O
,	O
Rahimi	O
et	O
al	O
(	O
2015	O
)	O
put	O
their	O
attention	O
on	O
evaluating	O
the	O
organization	O
and	O
cohesion	O
of	O
pieces	O
of	O
evidence	O
and	O
build	O
topic	O
chains	O
on	O
related	O
text	O
units	O
.	O
Besides	O
,	O
recent	O
studies	O
on	O
argument	B-TaskName
mining	I-TaskName
explore	O
to	O
build	O
links	O
or	O
clusters	O
for	O
topic	O
-	O
dependent	O
arguments	O
(	O
Wachsmuth	O
et	O
al	O
,	O
2018	O
;	O
Shnarch	O
et	O
al	O
,	O
2018	O
;	O
Reimers	O
et	O
al	O
,	O
2019	O
)	O
.	O
Obviously	O
,	O
more	O
and	O
more	O
researches	O
show	O
that	O
there	O
are	O
certain	O
structures	O
among	O
topic	O
segments	O
that	O
deserve	O
deeper	O
exploration	O
.	O
In	O
this	O
work	O
,	O
we	O
aim	O
to	O
explore	O
the	O
cohesion	O
of	O
topic	O
-	O
related	O
text	O
segments	O
.	O
Different	O
from	O
Rahimi	O
et	O
al	O
(	O
2015	O
)	O
,	O
we	O
show	O
great	O
interest	O
in	O
uncovering	O
how	O
fine	O
-	O
grain	O
topics	O
emerge	O
,	O
evolve	O
,	O
and	O
disappear	O
in	O
an	O
article	O
,	O
which	O
is	O
referred	O
to	O
as	O
discourselevel	O
topic	O
chain	O
(	O
DTC	O
)	O
parsing	O
.	O
Since	O
the	O
DTC	O
structure	O
can	O
provide	O
relatively	O
rich	O
and	O
low	O
-	O
noise	O
information	O
about	O
certain	O
topic	O
aspects	O
of	O
articles	O
,	O
it	O
is	O
meaningful	O
for	O
various	O
NLP	O
tasks	O
like	O
summarization	B-TaskName
(	O
Perez	O
-	O
Beltrachini	O
et	O
al	O
,	O
2019	O
)	O
,	O
document	O
similarity	O
measuring	O
(	O
Gong	O
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
response	B-TaskName
generation	I-TaskName
(	O
Dziri	O
et	O
al	O
,	O
2019	O
)	O
.	O
In	O
the	O
literature	O
,	O
topic	O
detection	O
and	O
tracking	O
(	O
TDT	O
)	O
(	O
Allan	O
,	O
2002	O
)	O
is	O
a	O
research	O
area	O
most	O
similar	O
to	O
DTC	O
parsing	O
which	O
aims	O
at	O
identifying	O
new	O
events	O
and	O
tracking	O
how	O
they	O
change	O
over	O
time	O
.	O
However	O
,	O
the	O
events	O
in	O
the	O
TDT	O
task	O
refer	O
to	O
happenings	O
at	O
certain	O
places	O
and	O
times	O
which	O
only	O
compose	O
a	O
small	O
subset	O
of	O
general	O
topics	O
.	O
Recently	O
,	O
Xi	O
and	O
Zhou	O
(	O
2017	O
)	O
manually	O
annotate	O
the	O
first	O
Chinese	O
DTC	O
corpus	O
based	O
on	O
the	O
theme	O
-	O
rheme	O
theory	O
(	O
Halliday	O
and	O
Matthiessen	O
,	O
2004	O
)	O
.	O
By	O
contrast	O
,	O
due	O
to	O
the	O
lack	O
of	O
corpus	O
,	O
previous	O
study	O
on	O
English	O
DTC	O
parsing	O
usually	O
uses	O
unsupervised	O
methods	O
(	O
Kim	O
and	O
Oh	O
,	O
2011	O
)	O
to	O
explore	O
the	O
structure	O
and	O
trends	O
of	O
important	O
topics	O
hidden	O
within	O
news	O
articles	O
.	O
Obviously	O
,	O
one	O
intractable	O
problem	O
facing	O
DTC	O
parsing	O
is	O
the	O
lack	O
of	O
data	O
.	O
This	O
research	O
is	O
primarily	O
motivated	O
by	O
(	O
Polanyi	O
and	O
Scha	O
,	O
1984	O
;	O
Kim	O
and	O
Oh	O
,	O
2011	O
)	O
on	O
the	O
topic	O
chain	O
concept	O
,	O
(	O
Xi	O
and	O
Zhou	O
,	O
2017	O
)	O
on	O
DTC	O
corpus	O
construction	O
,	O
and	O
(	O
Reimers	O
et	O
al	O
,	O
2019	O
)	O
on	O
topic	O
-	O
dependent	O
argument	O
linking	O
.	O
And	O
our	O
contributions	O
mainly	O
include	O
two	O
aspects	O
:	O
(	O
i	O
)	O
building	O
an	O
English	O
corpus	O
of	O
discourse	O
-	O
level	O
topic	O
chain	O
(	O
EDTC	O
)	O
through	O
a	O
two	O
-	O
step	O
annotation	O
method	O
and	O
(	O
ii	O
)	O
lunching	O
a	O
simple	O
but	O
robust	O
Bert	O
-	O
based	O
baseline	O
system	O
for	O
automatic	O
DTC	O
parsing	O
.	O
Moreover	O
,	O
as	O
implied	O
in	O
recent	O
researches	O
on	O
discourse	O
rhetorical	O
structure	O
(	O
DRS	O
)	O
parsing	O
(	O
Zhang	O
et	O
al	O
,	O
2020	O
;	O
Kobayashi	O
et	O
al	O
,	O
2021	O
;	O
Zhang	O
et	O
al	O
,	O
2021	O
)	O
DTC	O
structures	O
for	O
the	O
385	O
Wall	O
Street	O
Journal	O
(	O
WSJ	O
)	O
articles	O
in	O
the	O
RST	B-DatasetName
-	I-DatasetName
DT	I-DatasetName
corpus	O
aiming	O
to	O
build	O
a	O
bridge	O
between	O
discourse	O
rhetorical	O
structure	O
and	O
DTC	O
structure	O
for	O
discourse	O
researchers	O
to	O
utilize	O
.	O

A	O
Chinese	O
saying	O
about	O
Shakespeare	O
is	O
that	O
"	O
There	O
are	O
a	O
thousand	O
Hamlets	O
in	O
a	O
thousand	O
people	O
's	O
eyes	O
"	O
.	O
From	O
the	O
above	O
annotation	O
process	O
we	O
find	O
that	O
one	O
intractable	O
problem	O
of	O
DTC	O
annotation	O
is	O
the	O
high	O
subjective	O
differences	O
between	O
annotators	O
.	O
More	O
precisely	O
,	O
judging	O
whether	O
the	O
temporary	O
TE	O
evolves	O
from	O
the	O
previous	O
one	O
is	O
really	O
a	O
very	O
subjective	O
problem	O
,	O
and	O
it	O
is	O
hard	O
to	O
make	O
a	O
strict	O
regulation	O
for	O
the	O
annotators	O
.	O
In	O
this	O
case	O
,	O
we	O
tackle	O
the	O
issue	O
from	O
two	O
aspects	O
:	O
(	O
i	O
)	O
using	O
a	O
well	O
pretrained	O
topic	O
model	O
to	O
assist	O
manual	O
annotation	O
in	O
a	O
two	O
-	O
step	O
fashion	O
and	O
(	O
ii	O
)	O
calculating	O
the	O
confidence	O
scores	O
of	O
the	O
annotations	O
for	O
data	O
filtrating	O
.	O
Two	O
-	O
Step	O
Annotation	O
:	O
The	O
two	O
-	O
step	O
method	O
consists	O
of	O
two	O
phases	O
:	O
first	O
automatically	O
building	O
topic	O
links	O
between	O
topic	O
-	O
related	O
DTUs	O
2	O
and	O
then	O
manually	O
refining	O
the	O
automatic	O
annotations	O
for	O
DTC	O
structures	O
.	O
As	O
depicted	O
in	O
Figure	O
2	O
,	O
each	O
DTU	B-DatasetName
is	O
preceded	O
by	O
an	O
index	O
pair	O
(	O
i	O
,	O
j	O
)	O
according	O
to	O
which	O
u	O
-	O
i	O
and	O
u	O
-	O
j	O
are	O
connected	O
through	O
a	O
topic	O
link	O
.	O
And	O
u	O
-	O
i	O
is	O
an	O
ending	O
unit	O
when	O
j	O
equals	O
-	O
1	O
.	O
The	O
solid	O
arcs	O
in	O
the	O
example	O
refer	O
to	O
the	O
topic	O
links	O
generated	O
in	O
the	O
first	O
stage	O
.	O
On	O
this	O
basis	O
,	O
we	O
bring	O
in	O
an	O
auxiliary	O
marker	O
to	O
refine	O
the	O
chain	O
structures	O
where	O
"	O
×	O
"	O
means	O
that	O
the	O
initial	O
topic	O
arcs	O
(	O
either	O
machine	O
-	O
labeled	O
or	O
manually	O
labeled	O
links	O
)	O
are	O
unreasonable	O
and	O
should	O
be	O
deleted	O
directly	O
,	O
and	O
"	O
=	O
"	O
means	O
that	O
the	O
original	O
arcs	O
should	O
be	O
replaced	O
with	O
more	O
proper	O
topic	O
links	O
predicted	O
by	O
the	O
human	O
annotators	O
,	O
e.g.	O
,	O
the	O
dashed	O
arcs	O
in	O
the	O
example	O
.	O
In	O
this	O
way	O
,	O
we	O
can	O
dynamically	O
optimize	O
the	O
DTC	O
structures	O
during	O
the	O
human	O
annotation	O
process	O
thus	O
determining	O
the	O
most	O
relevant	O
DTUs	O
for	O
annotation	O
.	O
Our	O
statistics	O
show	O
that	O
around	O
37.4	O
%	O
of	O
the	O
automatic	O
annotations	O
are	O
retained	O
in	O
the	O
corpus	O
and	O
62.6	O
%	O
of	O
them	O
are	O
invalid	O
and	O
re	O
-	O
annotated	O
by	O
our	O
annotators	O
.	O
According	O
to	O
this	O
,	O
although	O
there	O
is	O
a	O
great	O
dissimilarity	O
between	O
automatic	O
and	O
manually	O
annotated	O
structures	O
,	O
the	O
topic	O
links	O
of	O
the	O
pre	O
-	O
trained	O
model	O
do	O
provide	O
a	O
good	O
2	O
Recently	O
,	O
Reimers	O
et	O
al	O
(	O
2019	O
)	O
use	O
superior	O
contextualized	O
language	O
models	O
for	O
argument	O
linking	O
,	O
which	O
has	O
proven	O
to	O
have	O
great	O
capabilities	O
in	O
aggregating	O
arguments	O
for	O
unseen	O
topics	O
(	O
https://github.com/UKPLab	O
)	O
.	O
To	O
improve	O
the	O
reliability	O
of	O
the	O
initial	O
chains	O
,	O
we	O
only	O
keep	O
the	O
topic	O
links	O
with	O
topic	O
similarity	O
higher	O
than	O
0.9	O
in	O
the	O
first	O
stage	O
.	O
reference	O
for	O
better	O
annotation	O
consistency	O
.	O
Annotation	O
Confidence	O
:	O
As	O
stated	O
before	O
,	O
considering	O
the	O
problem	O
of	O
subjective	O
difference	O
,	O
it	O
's	O
really	O
challenging	O
to	O
build	O
a	O
topic	O
link	O
between	O
two	O
DTUs	O
because	O
we	O
're	O
not	O
sure	O
if	O
they	O
're	O
the	O
most	O
relevant	O
.	O
Although	O
it	O
is	O
hard	O
to	O
strictly	O
regulate	O
the	O
annotators	O
'	O
subjectivities	O
,	O
it	O
is	O
feasible	O
to	O
calculate	O
the	O
reliability	O
of	O
each	O
annotation	O
item	O
.	O
Therefore	O
,	O
we	O
aim	O
to	O
ensure	O
the	O
quality	O
of	O
the	O
corpus	O
by	O
filtering	O
out	O
the	O
annotations	O
with	O
low	O
confidence	O
scores	O
.	O
Specifically	O
,	O
given	O
the	O
annotation	O
results	O
of	O
the	O
pre	O
-	O
trained	O
topic	O
model	O
,	O
(	O
τ	O
,	O
ι	O
)	O
,	O
and	O
that	O
of	O
three	O
annotators	O
,	O
(	O
τ	O
,	O
ν	O
)	O
,	O
(	O
τ	O
,	O
ι	O
)	O
,	O
and	O
(	O
τ	O
,	O
ν	O
)	O
,	O
on	O
the	O
DTU	B-DatasetName
τ	O
,	O
we	O
set	O
the	O
confidence	O
of	O
the	O
pre	O
-	O
trained	O
topic	O
model	O
to	O
0.5	O
and	O
that	O
of	O
human	O
annotators	O
to	O
1	O
,	O
then	O
the	O
confidence	O
score	O
of	O
each	O
annotation	O
on	O
τ	O
can	O
be	O
calculated	O
as	O
:	O
(	O
τ	O
,	O
ι	O
)	O
(	O
0.5	O
+	O
1	O
)	O
/3.5	O
,	O
(	O
τ	O
,	O
ν	O
)	O
2/3.5	O
.	O
Based	O
on	O
the	O
results	O
,	O
the	O
annotation	O
(	O
τ	O
,	O
ν	O
)	O
with	O
the	O
highest	O
confidence	O
score	O
of	O
0.57	O
is	O
determined	O
as	O
the	O
result	O
.	O
Following	O
this	O
way	O
,	O
we	O
can	O
greatly	O
alleviate	O
the	O
"	O
subjectivity	O
"	O
problem	O
by	O
retaining	O
annotations	O
with	O
high	O
confidence	O
.	O
According	O
to	O
our	O
statistics	O
,	O
the	O
averaged	O
confidence	O
score	O
of	O
each	O
DTU	B-DatasetName
annotation	O
is	O
around	O
0.73	O
.	O
Data	O
Details	O
.	O
The	O
annotated	O
corpus	O
contains	O
385	O
news	O
articles	O
(	O
7962	O
DTUs	O
)	O
from	O
RST	B-DatasetName
-	I-DatasetName
DT	I-DatasetName
.	O
We	O
annotate	O
4122	O
topic	O
links	O
corresponding	O
to	O
1757	O
topic	O
chains	O
in	O
the	O
corpus	O
,	O
and	O
the	O
chain	O
length	O
distribution	O
is	O
presented	O
in	O
Table	O
1	O
.	O
Obviously	O
,	O
the	O
distribution	O
of	O
chain	O
langths	O
is	O
uneven	O
and	O
most	O
chains	O
have	O
less	O
than	O
5	O
topic	O
arcs	O
.	O
For	O
supervised	O
learning	O
,	O
we	O
have	O
divided	O
the	O
dataset	O
into	O
three	O
parts	O
(	O
the	O
test	O
corpus	O
is	O
consist	O
with	O
that	O
of	O
RST	B-DatasetName
-	I-DatasetName
DT	I-DatasetName
)	O
,	O
as	O
shown	O
in	O
Table	O
2	O
.	O
Based	O
on	O
the	O
test	O
corpus	O
,	O
we	O
calculate	O
the	O
annotation	O
consistency	O
with	O
an	O
averaged	O
Cohen	O
's	O
kappa	O
value	O
of	O
0.72	O
.	O
Concretely	O
,	O
we	O
compare	O
three	O
groups	O
of	O
manual	O
annotations	O
on	O
DTUs	O
with	O
each	O
other	O
for	O
kappa	O
value	O
calculation	O
and	O
report	O
the	O
average	O
score	O
.	O
The	O
data	O
and	O
codes	O
are	O
published	O
at	O
https://github	O
.	O
com	O
/	O
NLP	O
-	O
Discourse	O
-	O
SoochowU	O
/	O
DTCP	O
.	O

In	O
this	O
research	O
,	O
we	O
explored	O
how	O
fine	O
-	O
grain	O
topics	O
emerge	O
,	O
evolve	O
,	O
and	O
disappear	O
within	O
an	O
article	O
.	O
To	O
address	O
the	O
lack	O
of	O
data	O
,	O
we	O
built	O
an	O
English	O
DTC	O
corpus	O
through	O
a	O
two	O
-	O
step	O
annotation	O
method	O
,	O
and	O
filtered	O
out	O
the	O
annotations	O
with	O
low	O
confidence	O
scores	O
to	O
ensure	O
the	O
high	O
reliability	O
of	O
the	O
corpus	O
.	O
During	O
annotation	O
,	O
we	O
found	O
that	O
each	O
annotated	O
topic	O
chain	O
does	O
provide	O
relatively	O
low	O
-	O
noise	O
information	O
about	O
a	O
certain	O
aspect	O
of	O
the	O
article	O
and	O
the	O
complete	O
DTC	O
structure	O
can	O
well	O
describe	O
the	O
overall	O
vein	O
of	O
topics	O
in	O
an	O
article	O
.	O
With	O
this	O
in	O
mind	O
,	O
we	O
introduced	O
a	O
simple	O
and	O
robust	O
baseline	O
system	O
,	O
and	O
the	O
parsing	O
model	O
we	O
trained	O
can	O
be	O
straightforwardly	O
harnessed	O
in	O
downstream	O
topic	O
-	O
sensitive	O
NLP	O
tasks	O
to	O
boost	O
performance	O
.	O
It	O
is	O
worth	O
mentioning	O
that	O
we	O
annotated	O
the	O
WSJ	O
articles	O
in	O
the	O
RST	B-DatasetName
-	I-DatasetName
DT	I-DatasetName
corpus	O
also	O
aim	O
to	O
allow	O
the	O
discourse	O
researchers	O
to	O
explore	O
the	O
potential	O
correlation	O
between	O
RST	O
-	O
and	O
DTC	O
-	O
style	O
discourse	O
analysis	O
in	O
future	O
work	O
.	O
Inc.	O
said	O
it	O
downgraded	O
its	O
rating	O
to	O
B	O
-	O
2	O
from	O
Ba	O
-	O
3	O
on	O
less	O
than	O
$	O
20	O
million	O
of	O
this	O
thrift	O
's	O
senior	O
subordinated	O
notes	O
.	O
[	O
u7	O
]	O
The	O
rating	O
concern	O
said	O
Franklin	O
's	O
"	O
troubled	O
diversification	O
record	O
in	O
the	O
securities	O
business	O
"	O
was	O
one	O
reason	O
for	O
the	O
downgrade	O
,	O
citing	O
the	O
troubles	O
at	O
its	O
L.F.	O
Rothschild	O
subsidiary	O
and	O
the	O
possible	O
sale	O
of	O
other	O
subsidiaries	O
.	O
"	O
They	O
perhaps	O
had	O
concern	O
that	O
we	O
were	O
getting	O
out	O
of	O
all	O
these	O
,	O
"	O
said	O
Franklin	O
President	O
Duane	O
H.	O
Hall	O
.	O
"	O
I	O
think	O
it	O
was	O
a	O
little	O
premature	O
on	O
their	O
part	O
.	O
"	O
wsj_2375	O
u1	O
u2	O
u3	O
u4	O
u5	O
u6	O
u7	O
u1	O
u2	O
u3	O
u4	O
u5	O
u6	O
u7	O
u8	O
u9	O
u10	O
u11	O
u12	O
u13	O
[	O
u7	O
]	O
MedChem	O
said	O
the	O
court	O
's	O
ruling	O
was	O
issued	O
as	O
part	O
of	O
a	O
"	O
firstphase	O
trial	O
"	O
in	O
the	O
patent	O
-	O
infringement	O
proceedings	O
and	O
concerns	O
only	O
one	O
of	O
its	O
defenses	O
in	O
the	O
case	O
.	O
[	O
u8	O
]	O
It	O
said	O
it	O
is	O
considering	O
"	O
all	O
of	O
its	O
options	O
in	O
light	O
of	O
the	O
decision	O
,	O
including	O
a	O
possible	O
appeal	O
.	O
"	O
The	O
medical	O
-	O
products	O
company	O
added	O
that	O
it	O
plans	O
to	O
"	O
assert	O
its	O
other	O
defenses	O
"	O
against	O
Pharmacia	O
's	O
lawsuit	O
,	O
including	O
the	O
claim	O
that	O
it	O
has	O
n't	O
infringed	O
on	O
Pharmacia	O
's	O
patent	O
.	O
[	O
u9	O
]	O
MedChem	O
said	O
that	O
the	O
court	O
scheduled	O
a	O
conference	O
for	O
next	O
Mondayto	O
set	O
a	O
date	O
for	O
proceedings	O
on	O
Pharmacia	O
's	O
motion	O
for	O
a	O
preliminary	O
injunction	O
.	O
wsj_2336	O

XMU	O
Neural	O
Machine	B-TaskName
Translation	I-TaskName
Systems	O
for	O
WMT	O
17	O

This	O
paper	O
describes	O
the	O
Neural	O
Machine	B-TaskName
Translation	I-TaskName
systems	O
of	O
Xiamen	O
University	O
for	O
the	O
translation	O
tasks	O
of	O
WMT	O
17	O
.	O
Our	O
systems	O
are	O
based	O
on	O
the	O
Encoder	O
-	O
Decoder	O
framework	O
with	O
attention	O
.	O
We	O
participated	O
in	O
three	O
directions	O
of	O
shared	O
news	O
translation	O
tasks	O
:	O
English	O
German	O
and	O
Chinese↔English	O
.	O
We	O
experimented	O
with	O
deep	O
architectures	O
,	O
different	O
segmentation	O
models	O
,	O
synthetic	O
training	O
data	O
and	O
targetbidirectional	O
translation	O
models	O
.	O
Experiments	O
show	O
that	O
all	O
methods	O
can	O
give	O
substantial	O
improvements	O
.	O

Neural	O
Machine	B-TaskName
Translation	I-TaskName
(	O
NMT	O
)	O
(	O
Cho	O
et	O
al	O
,	O
2014	O
;	O
Bahdanau	O
et	O
al	O
,	O
2015	O
)	O
has	O
achieved	O
great	O
success	O
in	O
recent	O
years	O
and	O
obtained	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
various	O
language	O
pairs	O
(	O
Zhou	O
et	O
al	O
,	O
2016	O
;	O
Sennrich	O
et	O
al	O
,	O
2016a	O
;	O
Wu	O
et	O
al	O
,	O
2016	O
)	O
.	O
This	O
paper	O
describes	O
the	O
NMT	O
systems	O
of	O
Xiamen	O
University	O
(	O
XMU	O
)	O
for	O
the	O
WMT	O
17	O
.	O
We	O
participated	O
in	O
three	O
directions	O
of	O
shared	O
news	O
translation	O
tasks	O
:	O
English	O
German	O
and	O
Chinese↔English	O
.	O
We	O
use	O
two	O
different	O
NMTs	O
for	O
shared	O
news	O
translation	O
tasks	O
:	O
MININMT	O
:	O
A	O
deep	O
NMT	O
system	O
(	O
Zhou	O
et	O
al	O
,	O
2016	O
;	O
Wu	O
et	O
al	O
,	O
2016	O
;	O
Wang	O
et	O
al	O
,	O
2017	O
)	O
with	O
a	O
simple	O
architecture	O
.	O
The	O
decoder	O
is	O
a	O
stacked	O
Long	B-MethodName
Short	I-MethodName
-	I-MethodName
Term	I-MethodName
Memory	I-MethodName
(	O
LSTM	B-MethodName
)	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
with	O
8	O
layers	O
.	O
The	O
encoder	O
has	O
two	O
variants	O
.	O
For	O
English	O
-	O
German	O
translation	O
,	O
we	O
use	O
an	O
interleaved	O
bidirectional	O
encoder	O
with	O
2	O
columns	O
.	O
Each	O
column	O
consists	O
of	O
4	O
LSTMs	O
.	O
For	O
Chinese	O
-	O
English	O
translation	O
,	O
we	O
use	O
a	O
stacked	O
bidirectional	O
encoder	O
with	O
8	O
layers	O
.	O
DL4MT	O
:	O
Our	O
reimplementation	O
of	O
dl4mttutorial	O
1	O
with	O
minor	O
changes	O
.	O
We	O
also	O
use	O
a	O
modified	O
version	O
of	O
AmuNMT	O
C++	O
decoder	O
2	O
for	O
decoding	O
.	O
This	O
system	O
is	O
used	O
in	O
the	O
English	O
-	O
Chinese	O
translation	O
task	O
.	O
We	O
use	O
both	O
Byte	B-MethodName
Pair	I-MethodName
Encoding	I-MethodName
(	O
BPE	B-MethodName
)	O
(	O
Sennrich	O
et	O
al	O
,	O
2016c	O
)	O
and	O
mixed	O
word	O
/	O
character	O
segmentation	O
(	O
Wu	O
et	O
al	O
,	O
2016	O
)	O
to	O
achieve	O
open	O
-	O
vocabulary	O
translation	O
.	O
Back	O
-	O
translation	O
method	O
(	O
Sennrich	O
et	O
al	O
,	O
2016b	O
)	O
is	O
applied	O
to	O
make	O
use	O
of	O
monolingual	O
data	O
.	O
We	O
also	O
use	O
target	O
-	O
bidiretional	O
translation	O
models	O
to	O
alleviate	O
the	O
label	O
bias	O
problem	O
(	O
Lafferty	O
et	O
al	O
,	O
2001	O
)	O
.	O
The	O
remainder	O
of	O
this	O
paper	O
is	O
organized	O
as	O
follows	O
:	O
Section	O
2	O
describes	O
the	O
architecture	O
of	O
MIN	O
-	O
INMT	O
.	O
Section	O
3	O
describes	O
all	O
experimental	O
features	O
used	O
in	O
WMT	O
17	O
shared	O
translation	O
tasks	O
.	O
Section	O
4	O
shows	O
the	O
results	O
of	O
our	O
experiments	O
.	O
Section	O
5	O
shows	O
the	O
results	O
of	O
shared	O
translation	O
task	O
.	O
Finally	O
,	O
we	O
conclude	O
in	O
section	O
6	O
.	O

Deep	O
architectures	O
have	O
recently	O
shown	O
promising	O
results	O
on	O
various	O
language	O
pairs	O
(	O
Zhou	O
et	O
al	O
,	O
2016	O
;	O
Wu	O
et	O
al	O
,	O
2016	O
;	O
Wang	O
et	O
al	O
,	O
2017	O
)	O
.	O
We	O
also	O
experimented	O
with	O
a	O
deep	O
architecture	O
as	O
depicted	O
in	O
Figure	O
1	O
.	O
We	O
use	O
LSTM	B-MethodName
as	O
the	O
main	O
recurrent	O
unit	O
and	O
residual	O
connections	O
(	O
He	O
et	O
al	O
,	O
2016	O
)	O
to	O
help	O
training	O
.	O
Given	O
a	O
source	O
sentence	O
x	O
=	O
{	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
S	O
}	O
and	O
a	O
target	O
sentence	O
y	O
=	O
{	O
y	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
T	O
}	O
,	O
the	O
encoder	O
maps	O
the	O
source	O
sentence	O
x	O
into	O
a	O
sequence	O
of	O
annotation	O
vectors	O
{	O
x	O
i	O
}	O
.	O
(	O
Zhou	O
et	O
al	O
,	O
2016	O
)	O
and	O
GNMT	O
(	O
Wu	O
et	O
al	O
,	O
2016	O
)	O
.	O
Both	O
the	O
encoder	O
and	O
decoder	O
adopt	O
LSTM	B-MethodName
as	O
its	O
main	O
recurrent	O
unit	O
.	O
We	O
also	O
use	O
residual	O
connections	O
(	O
He	O
et	O
al	O
,	O
2016	O
)	O
to	O
help	O
training	O
,	O
but	O
here	O
we	O
omit	O
it	O
for	O
clarity	O
.	O
We	O
use	O
black	O
lines	O
to	O
denote	O
input	O
connections	O
while	O
use	O
blue	O
lines	O
to	O
denote	O
recurrent	O
connections	O
.	O
translation	O
y	O
t	O
given	O
the	O
source	O
annotation	O
vectors	O
{	O
x	O
i	O
}	O
and	O
target	O
history	O
y	O
<	O
t	O
.	O

The	O
interleaved	O
bidirectional	O
encoder	O
was	O
introduced	O
by	O
(	O
Zhou	O
et	O
al	O
,	O
2016	O
)	O
,	O
which	O
is	O
also	O
used	O
in	O
(	O
Wang	O
et	O
al	O
,	O
2017	O
)	O
.	O
Like	O
(	O
Zhou	O
et	O
al	O
,	O
2016	O
)	O
,	O
our	O
interleaved	O
bidirectional	O
encoder	O
consists	O
of	O
two	O
columns	O
.	O
In	O
interleaved	O
bidirectional	O
encoder	O
,	O
the	O
LSTMs	O
in	O
adjacent	O
layers	O
run	O
in	O
opposite	O
directions	O
:	O
−	O
x	O
i	O
t	O
=	O
LSTM	B-MethodName
f	O
i	O
(	O
−	O
x	O
i−1	O
t	O
,	O
−	O
s	O
i	O
t+	O
(	O
−1	O
)	O
i	O
)	O
(	O
1	O
)	O
−	O
x	O
i	O
t	O
=	O
LSTM	B-MethodName
b	O
i	O
(	O
−	O
x	O
i−1	O
t	O
,	O
−	O
s	O
i	O
t+	O
(	O
−1	O
)	O
i+1	O
)	O
(	O
2	O
)	O
Here	O
x	O
0	B-DatasetName
t	O
R	O
e	O
is	O
the	O
word	O
embedding	O
of	O
word	O
x	O
t	O
,	O
x	O
i	O
t	O
R	O
h	O
is	O
the	O
output	O
of	O
LSTM	B-MethodName
unit	O
and	O
s	O
i	O
t	O
=	O
(	O
c	O
i	O
t	O
,	O
m	O
i	O
t	O
)	O
denotes	O
the	O
memory	O
and	O
hidden	O
state	O
of	O
LSTM	B-MethodName
.	O
We	O
set	O
both	O
e	O
and	O
h	O
to	O
512	O
in	O
all	O
our	O
experiments	O
.	O
The	O
annotation	O
vectors	O
x	O
i	O
R	O
2h	O
are	O
obtained	O
by	O
concatenating	O
the	O
final	O
output	O
−	O
x	O
Lenc	O
and	O
−	O
x	O
Lenc	O
of	O
two	O
encoder	O
columns	O
.	O
In	O
our	O
experiments	O
,	O
we	O
set	O
L	O
enc	O
=	O
4	O
.	O

To	O
better	O
exploit	O
source	O
representation	O
,	O
we	O
adopt	O
a	O
stacked	O
bidirectional	O
encoder	O
.	O
As	O
shown	O
in	O
Figure	O
1	O
,	O
all	O
layers	O
in	O
the	O
encoder	O
are	O
bidirectional	O
.	O
The	O
calculation	O
is	O
described	O
as	O
follows	O
:	O
−	O
x	O
i	O
=	O
LSTM	B-MethodName
f	O
i	O
(	O
x	O
i−1	O
t	O
,	O
−	O
s	O
i	O
t−1	O
)	O
(	O
3	O
)	O
−	O
x	O
i	O
=	O
LSTM	B-MethodName
b	O
i	O
(	O
x	O
i−1	O
t	O
,	O
−	O
s	O
i	O
t+1	O
)	O
(	O
4	O
)	O
x	O
i	O
=	O
[	O
−	O
x	O
i	O
T	O
;	O
−	O
x	O
i	O
T	O
]	O
T	O
(	O
5	O
)	O
To	O
reduce	O
parameters	O
,	O
we	O
reduce	O
the	O
dimension	O
of	O
hidden	O
units	O
from	O
h	O
to	O
h/2	O
so	O
that	O
x	O
i	O
R	O
h	O
.	O
The	O
annotation	O
vectors	O
are	O
taken	O
from	O
the	O
output	O
x	O
Lenc	O
of	O
top	O
LSTM	B-MethodName
layer	O
.	O
In	O
our	O
experiments	O
,	O
L	O
enc	O
is	O
set	O
to	O
8	O
.	O

To	O
enable	O
open	O
-	O
vocabulary	O
,	O
we	O
use	O
two	O
approaches	O
:	O
BPE	B-MethodName
and	O
mixed	O
word	O
/	O
character	O
segmentation	O
.	O
In	O
most	O
of	O
our	O
experiments	O
,	O
we	O
use	O
BPE	B-MethodName
3	O
(	O
Sennrich	O
et	O
al	O
,	O
2016c	O
)	O
with	O
50	O
K	O
operations	O
.	O
In	O
our	O
preliminary	O
experiments	O
,	O
we	O
found	O
that	O
BPE	B-MethodName
works	O
better	O
than	O
UNK	O
replacement	O
techniques	O
.	O
For	O
English	O
-	O
Chinese	O
translation	O
task	O
,	O
we	O
apply	O
mixed	O
word	O
/	O
character	O
model	O
(	O
Wu	O
et	O
al	O
,	O
2016	O
)	O
to	O
Chinese	O
sentences	O
.	O
We	O
keep	O
the	O
most	O
frequent	O
50	O
K	O
words	O
and	O
split	O
other	O
words	O
into	O
characters	O
.	O
Unlike	O
(	O
Wu	O
et	O
al	O
,	O
2016	O
)	O
,	O
we	O
do	O
not	O
add	O
any	O
prefixes	O
or	O
suffixes	O
to	O
the	O
segmented	O
Chinese	O
characters	O
.	O
In	O
post	O
-	O
processing	O
step	O
,	O
we	O
simply	O
remove	O
all	O
the	O
spaces	O
.	O

We	O
describe	O
XMU	O
's	O
neural	O
machine	B-TaskName
translation	I-TaskName
systems	O
for	O
the	O
WMT	O
17	O
shared	O
news	O
translation	O
tasks	O
.	O
All	O
our	O
models	O
perform	O
quite	O
well	O
on	O
all	O
tasks	O
we	O
participated	O
.	O
Experiments	O
also	O
show	O
the	O
effectiveness	O
of	O
all	O
features	O
we	O
used	O
.	O

Rigid	O
Formats	O
Controlled	O
Text	B-TaskName
Generation	I-TaskName

Neural	O
text	B-TaskName
generation	I-TaskName
has	O
made	O
tremendous	O
progress	O
in	O
various	O
tasks	O
.	O
One	O
common	O
characteristic	O
of	O
most	O
of	O
the	O
tasks	O
is	O
that	O
the	O
texts	O
are	O
not	O
restricted	O
to	O
some	O
rigid	O
formats	O
when	O
generating	O
.	O
However	O
,	O
we	O
may	O
confront	O
some	O
special	O
text	O
paradigms	O
such	O
as	O
Lyrics	O
(	O
assume	O
the	O
music	O
score	O
is	O
given	O
)	O
,	O
Sonnet	O
,	O
SongCi	O
(	O
classical	O
Chinese	O
poetry	O
of	O
the	O
Song	O
dynasty	O
)	O
,	O
etc	O
.	O
The	O
typical	O
characteristics	O
of	O
these	O
texts	O
are	O
in	O
three	O
folds	O
:	O
(	O
1	O
)	O
They	O
must	O
comply	O
fully	O
with	O
the	O
rigid	O
predefined	O
formats	O
.	O
(	O
2	O
)	O
They	O
must	O
obey	O
some	O
rhyming	O
schemes	O
.	O
(	O
3	O
)	O
Although	O
they	O
are	O
restricted	O
to	O
some	O
formats	O
,	O
the	O
sentence	O
integrity	O
must	O
be	O
guaranteed	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
text	B-TaskName
generation	I-TaskName
based	O
on	O
the	O
predefined	O
rigid	O
formats	O
has	O
not	O
been	O
well	O
investigated	O
.	O
Therefore	O
,	O
we	O
propose	O
a	O
simple	O
and	O
elegant	O
framework	O
named	O
SongNet	B-MethodName
to	O
tackle	O
this	O
problem	O
.	O
The	O
backbone	O
of	O
the	O
framework	O
is	O
a	O
Transformer	B-MethodName
-	O
based	O
auto	O
-	O
regressive	O
language	O
model	O
.	O
Sets	O
of	O
symbols	O
are	O
tailor	O
-	O
designed	O
to	O
improve	O
the	O
modeling	O
performance	O
especially	O
on	O
format	O
,	O
rhyme	O
,	O
and	O
sentence	O
integrity	O
.	O
We	O
improve	O
the	O
attention	O
mechanism	O
to	O
impel	O
the	O
model	O
to	O
capture	O
some	O
future	O
information	O
on	O
the	O
format	O
.	O
A	O
pre	O
-	O
training	O
and	O
fine	O
-	O
tuning	O
framework	O
is	O
designed	O
to	O
further	O
improve	O
the	O
generation	O
quality	O
.	O
Extensive	O
experiments	O
conducted	O
on	O
two	O
collected	O
corpora	O
demonstrate	O
that	O
our	O
proposed	O
framework	O
generates	O
significantly	O
better	O
results	O
in	O
terms	O
of	O
both	O
automatic	O
metrics	O
and	O
the	O
human	O
evaluation	O
.	O
1	O

Recent	O
years	O
have	O
seen	O
the	O
tremendous	O
progress	O
in	O
the	O
area	O
of	O
natural	O
language	O
generation	O
especially	O
benefiting	O
by	O
the	O
neural	O
network	O
models	O
such	O
as	O
Recurrent	O
Neural	O
Networks	O
(	O
RNN	O
)	O
or	O
Convolutional	O
Neural	O
Networks	O
(	O
CNN	O
)	O
based	O
sequence	O
-	O
tosequence	O
(	O
seq2seq	B-MethodName
)	O
frameworks	O
(	O
Bahdanau	O
et	O
al	O
,	O
1	O
Code	O
:	O
http://github.com/lipiji/SongNet	O
Let	O
me	O
not	O
to	O
the	O
marriage	O
of	O
true	O
minds	O
Admit	O
impediments	O
,	O
love	O
is	O
not	O
love	O

We	O
conduct	O
all	O
the	O
experiments	O
on	O
two	O
collected	O
corpus	O
with	O
different	O
literary	O
genres	O
:	O
SongCi	O
and	O
Sonnet	O
,	O
in	O
Chinese	O
and	O
English	O
respectively	O
.	O
The	O
statistic	O
number	O
are	O
shown	O
in	O
Table	O
3	O
.	O
We	O
can	O
see	O
that	O
Sonnet	O
is	O
in	O
small	O
size	O
since	O
we	O
only	O
utilize	O
the	O
samples	O
from	O
the	O
Shakespeare	O
's	O
Sonnets	O
(	O
Shakespeare	O
,	O
2000	O
)	O
.	O
Since	O
SongCi	O
and	O
Sonnet	O
are	O
in	O
different	O
languages	O
,	O
thus	O
we	O
conduct	O
the	O
pre	O
-	O
training	O
procedure	O
on	O
two	O
large	O
scale	O
corpus	O
in	O
the	O
corresponding	O
languages	O
respectively	O
.	O
For	O
Chinese	O
,	O
we	O
collect	O
Chinese	O
Wikipedia	O
(	O
1700	O
M	O
Characters	O
)	O
and	O
a	O
merged	O
Chinese	O
News	O
(	O
9200	O
M	O
Characters	O
)	O
corpus	O
from	O
the	O
Internet	O
.	O
We	O
did	O
not	O
conduct	O
the	O
word	O
segmenting	O
operations	O
on	O
the	O
Chinese	O
datasets	O
,	O
which	O
means	O
that	O
we	O
just	O
use	O
the	O
characters	O
to	O
build	O
the	O
vocabulary	O
,	O
and	O
the	O
size	O
is	O
27681	O
.	O
For	O
English	O
,	O
same	O
as	O
BERT	B-MethodName
,	O
we	O
employ	O
English	O
Wikipedia	O
(	O
2400	O
M	O
words	O
)	O
and	O
BooksCorpus	O
(	O
980	O
M	O
words	O
)	O
(	O
Zhu	O
et	O
al	O
,	O
2015	O
)	O
to	O
conduct	O
the	O
pre	O
-	O
training	O
.	O
We	O
did	O
not	O
use	O
BPE	B-MethodName
operation	O
(	O
Sennrich	O
et	O
al	O
,	O
2015	O
)	O
on	O
this	O
corpus	O
considering	O
the	O
format	O
controlling	O
purpose	O
.	O
We	O
keep	O
the	O
most	O
frequent	O
50	O
,	O
000	O
words	O
to	O
build	O
the	O
vocabulary	O
.	O

Sequence	O
-	O
to	O
-	O
sequence	O
framework	O
with	O
attention	O
mechanism	O
.	O
We	O
regard	O
the	O
format	O
and	O
rhyme	O
symbols	O
C	O
as	O
the	O
input	O
sequence	O
,	O
and	O
the	O
target	O
as	O
the	O
output	O
sequence	O
.	O
GPT2	O
We	O
fine	O
-	O
tune	O
the	O
GPT2	O
models	O
(	O
the	O
pretraining	O
versions	O
are	O
used	O
for	O
sentence	O
integrity	O
evaluation	O
)	O
on	O
SongCi	O
and	O
Sonnet	O
respectively	O
.	O
SongNet	B-MethodName
Out	O
proposed	O
framework	O
with	O
both	O
the	O
per	O
-	O
training	O
and	O
fine	O
-	O
tuning	O
stages	O
.	O
We	O
also	O
conduct	O
ablation	O
analysis	O
to	O
verify	O
the	O
performance	O
of	O
the	O
defined	O
symbols	O
as	O
well	O
as	O
the	O
variants	O
of	O
model	O
structures	O
.	O
SongNet	B-MethodName
(	O
only	O
pre	O
-	O
tuning	O
)	O
Without	O
the	O
finetuning	O
stage	O
.	O
SongNet	B-MethodName
(	O
only	O
fine	O
-	O
tuning	O
)	O
Without	O
the	O
pretraining	O
stage	O
.	O
SongNet	B-MethodName
-	O
GRU	B-MethodName
Employ	O
GRU	B-MethodName
to	O
replace	O
Transformer	B-MethodName
as	O
the	O
core	O
structure	O
.	O

SongNet	B-MethodName
-	O
SongCi	O
CiPai	O
:	O
Zhe	O
Gu	O
Tian	O
,	O
Format	O
:	O
7	O
.	O
7	O
.	O
7	O
,	O
7	O
.	O
3	O
,	O
3	O
.	O
7	O
.	O
what	O
lies	O
,	O
for	O
when	O
you	O
are	O
not	O
that	O
,	O
\	O
no	O
one	O
in	O
this	O
and	O
that	O
can	O
see	O
me	O
lies	O
!	O
Table	O
5	O
:	O
Cases	O
of	O
the	O
generated	O
results	O
for	O
SongCi	O
and	O
Sonnet	O
respectively	O
.	O
For	O
SongCi	O
,	O
the	O
number	O
in	O
Format	O
(	O
e.g.	O
,	O
3	O
,	O
5	O
,	O
7	O
)	O
denotes	O
the	O
number	O
of	O
tokens	O
in	O
one	O
sentence	O
.	O
The	O
rhyming	O
words	O
are	O
labeled	O
in	O
red	O
color	O
and	O
italic	O
font	O
following	O
is	O
the	O
Pinyin	O
.	O
(	O
Since	O
cases	O
are	O
provided	O
to	O
confirm	O
the	O
format	O
consistency	O
,	O
thus	O
we	O
did	O
not	O
conduct	O
translation	O
for	O
the	O
Chinese	O
samples	O
.	O
Translation	B-TaskName
for	O
Chinese	O
poetry	O
is	O
also	O
a	O
challenging	O
task	O
.	O
)	O

SongNet	B-MethodName
-	O
SongCi	O
CiPai	O
:	O
Bu	O
Suan	O
Zi	O
,	O
Format	O
:	O
5	O
,	O
5	O
.	O
7	O
,	O
5	O
.	O
5	O
,	O
5	O
.	O
7	O
,	O
5	O
.	O
though	O
all	O
thy	O
love	O
with	O
thy	O
hearts	O
,	O
thou	O
still	O
are	O
lacking	O
of	O
my	O
dead	O
;	O
if	O
thy	O
love	O
love	O
is	O
lost	O
to	O
your	O
love	O
and	O
parts	O
,	O
and	O
yet	O
mine	O
own	O
heart	O
can	O
be	O
buried	O
.	O
so	O
many	O
are	O
ill	O
or	O
in	O
tear	O
,	O
hath	O
not	O
this	O
time	O
that	O
we	O
will	O
make	O
their	O
eye	O
,	O
for	O
that	O
which	O
lies	O
not	O
well	O
hath	O
now	O
appear	O
,	O
no	O
longer	O
nor	O
the	O
world	O
that	O
holds	O
thee	O
lie	O
!	O
for	O
if	O
it	O
would	O
be	O
buried	O
in	O
my	O
live	O
,	O
or	O
by	O
the	O
earth	O
of	O
mine	O
was	O
gone	O
,	O
then	O
my	O
own	O
parts	O
as	O
my	O
body	O
and	O
mine	O
give	O
,	O
may	O
not	O
be	O
so	O
far	O
beyond	O
thine	O
alone	O
:	O
so	O
far	O
as	O
thee	O
and	O
this	O
world	O
view	O
find	O
thee	O
,	O
then	O
mine	O
life	O
be	O
far	O
enough	O
from	O
all	O
thee	O
and	O
no	O
me	O
.	O
Format	O
C	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
(	O
1	O
)	O
(	O
2	O
)	O
Format	O
C	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
_	O
(	O
1	O
)	O
(	O
2	O
)	O
SongNet	B-MethodName
-	O
Sonnet	O
_	O
_	O
_	O
_	O
Table	O
6	O
:	O
Cases	O
of	O
the	O
generated	O
results	O
given	O
the	O
formats	O
with	O
partial	O
pre	O
-	O
defined	O
content	O
.	O
Format	O
token	O
"	O
"	O
needs	O
to	O
be	O
translated	O
to	O
real	O
word	O
token	O
.	O

For	O
human	O
evaluation	O
,	O
we	O
just	O
conduct	O
the	O
judging	O
on	O
the	O
results	O
generated	O
by	O
our	O
final	O
model	O
SongNet	B-MethodName
.	O
From	O
the	O
result	O
we	O
can	O
observe	O
that	O
the	O
results	O
on	O
corpus	O
SongCi	O
is	O
much	O
better	O
than	O
the	O
ones	O
on	O
corpus	O
Sonnet	O
,	O
which	O
is	O
because	O
the	O
corpus	O
scale	O
is	O
different	O
.	O
And	O
the	O
the	O
small	O
scale	O
also	O
lead	O
to	O
dramatically	O
dropping	O
on	O
all	O
the	O
metrics	O
.	O

Table	O
5	O
depicts	O
several	O
generated	O
cases	O
for	O
SongCi	O
and	O
Sonnet	O
respectively	O
.	O
For	O
SongCi	O
,	O
the	O
formats	O
(	O
CiPai	O
)	O
are	O
all	O
cold	O
-	O
start	O
samples	O
which	O
are	O
not	O
in	O
the	O
training	O
set	O
or	O
even	O
newly	O
defined	O
.	O
Our	O
model	O
can	O
still	O
generate	O
high	O
quality	O
results	O
on	O
the	O
aspects	O
of	O
format	O
,	O
rhyme	O
as	O
well	O
as	O
integrity	O
.	O
However	O
,	O
for	O
corpus	O
Sonnet	O
,	O
even	O
though	O
the	O
model	O
can	O
generate	O
14	O
lines	O
text	O
,	O
the	O
quality	O
is	O
not	O
as	O
good	O
as	O
SongCi	O
due	O
to	O
the	O
insufficient	O
training	O
-	O
set	O
(	O
only	O
100	O
samples	O
)	O
.	O
We	O
will	O
address	O
this	O
interesting	O
and	O
challenging	O
few	O
-	O
shot	O
issue	O
in	O
the	O
future	O
.	O
In	O
addition	O
,	O
we	O
mentioned	O
that	O
our	O
model	O
has	O
the	O
ability	O
of	O
refining	O
and	O
polishing	O
given	O
the	O
format	O
C	O
which	O
contains	O
some	O
fixed	O
text	O
information	O
.	O
The	O
examples	O
of	O
the	O
generated	O
results	O
under	O
this	O
setting	O
are	O
shown	O
in	O
Table	O
6	O
,	O
which	O
show	O
that	O
our	O
model	O
SongNet	B-MethodName
can	O
generate	O
satisfying	O
results	O
especially	O
on	O
SongCi	O
.	O

We	O
propose	O
to	O
tackle	O
a	O
challenging	O
task	O
called	O
rigid	O
formats	O
controlled	O
text	B-TaskName
generation	I-TaskName
.	O
A	O
pre	O
-	O
training	O
and	O
fine	O
-	O
tuning	O
framework	O
SongNet	B-MethodName
is	O
designed	O
to	O
address	O
the	O
problem	O
.	O
Sets	O
of	O
symbols	O
are	O
tailordesigned	O
to	O
improve	O
the	O
modeling	O
performance	O
for	O
format	O
,	O
rhyme	O
,	O
and	O
sentence	O
integrity	O
.	O
Extensive	O
experiments	O
conducted	O
on	O
two	O
collected	O
corpora	O
demonstrate	O
that	O
our	O
framework	O
generates	O
significantly	O
better	O
results	O
in	O
terms	O
of	O
both	O
automatic	O
metrics	O
and	O
human	O
evaluations	O
given	O
arbitrary	O
cold	O
start	O
formats	O
.	O

In	O
this	O
paper	O
,	O
we	O
report	O
on	O
the	O
shared	O
task	O
on	O
metaphor	O
identification	O
on	O
VU	O
Amsterdam	O
Metaphor	O
Corpus	O
and	O
on	O
a	O
subset	O
of	O
the	O
TOEFL	O
Native	B-TaskName
Language	I-TaskName
Identification	I-TaskName
Corpus	O
.	O
The	O
shared	O
task	O
was	O
conducted	O
as	O
apart	O
of	O
the	O
ACL	O
2020	O
Workshop	O
on	O
Processing	O
Figurative	O
Language	O
.	O

Over	O
the	O
last	O
decade	O
,	O
automated	O
detection	O
of	O
metaphor	O
has	O
become	O
an	O
popular	O
topic	O
,	O
which	O
manifests	O
itself	O
in	O
both	O
a	O
variety	O
of	O
approaches	O
and	O
in	O
an	O
increasing	O
variety	O
of	O
data	O
to	O
which	O
the	O
methods	O
are	O
applied	O
.	O
In	O
terms	O
of	O
methods	O
,	O
approaches	O
based	O
on	O
feature	O
-	O
engineering	O
in	O
a	O
supervised	O
machine	O
learning	O
paradigm	O
explored	O
features	O
based	O
on	O
concreteness	O
and	O
imageability	O
,	O
semantic	O
classification	O
using	O
WordNet	O
,	O
FrameNet	B-DatasetName
,	O
VerbNet	O
,	O
SUMO	O
ontology	B-MethodName
,	O
property	O
norms	O
,	O
and	O
distributional	O
semantic	O
models	O
,	O
syntactic	O
dependency	O
patterns	O
,	O
sensorial	O
and	O
vision	O
-	O
based	O
features	O
Köper	O
and	O
i	O
m	O
Walde	O
,	O
2017	O
;	O
Tekiroglu	O
et	O
al	O
,	O
2015	O
;	O
Tsvetkov	O
et	O
al	O
,	O
2014	O
;	O
Beigman	O
Klebanov	O
et	O
al	O
,	O
2014	O
;	O
Dunn	O
,	O
2013	O
;	O
Neuman	O
et	O
al	O
,	O
2013	O
;	O
Mohler	O
et	O
al	O
,	O
2013	O
;	O
Hovy	O
et	O
al	O
,	O
2013	O
;	O
Tsvetkov	O
et	O
al	O
,	O
2013	O
;	O
Turney	O
et	O
al	O
,	O
2011	O
;	O
Shutova	O
et	O
al	O
,	O
2010	O
;	O
Gedigian	O
et	O
al	O
,	O
2006	O
)	O
;	O
see	O
and	O
Veale	O
et	O
al	O
(	O
2016	O
)	O
for	O
reviews	O
of	O
supervised	O
as	O
well	O
as	O
semi	O
-	O
supervised	O
and	O
unsupervised	O
approaches	O
.	O
Recently	O
,	O
deep	O
learning	O
methods	O
have	O
been	O
explored	O
for	O
token	O
-	O
level	O
metaphor	O
detection	O
(	O
Mao	O
et	O
al	O
,	O
2019	O
;	O
Dankers	O
et	O
al	O
,	O
2019	O
;	O
Gao	O
et	O
al	O
,	O
2018	O
;	O
Wu	O
et	O
al	O
,	O
2018	O
;	O
Rei	O
et	O
al	O
,	O
2017	O
;	O
Gutierrez	O
et	O
al	O
,	O
2017	O
;	O
Do	O
Dinh	O
and	O
Gurevych	O
,	O
2016	O
)	O
.	O
In	O
terms	O
of	O
data	O
,	O
researchers	O
used	O
specially	O
constructed	O
or	O
selected	O
sets	O
,	O
such	O
as	O
adjective	O
noun	O
pairs	O
Tsvetkov	O
et	O
al	O
,	O
2014	O
)	O
,	O
WordNet	O
synsets	O
and	O
glosses	O
(	O
Mohammad	O
et	O
al	O
,	O
2016	O
)	O
,	O
annotated	O
lexical	O
items	O
(	O
from	O
a	O
range	O
of	O
word	O
classes	O
)	O
in	O
sentences	O
sampled	O
from	O
corpora	O
(	O
Özbal	O
et	O
al	O
,	O
2016	O
;	O
Jang	O
et	O
al	O
,	O
2015	O
;	O
Hovy	O
et	O
al	O
,	O
2013	O
;	O
Birke	O
and	O
Sarkar	O
,	O
2006	O
)	O
,	O
all	O
the	O
way	O
to	O
annotation	O
of	O
all	O
words	O
in	O
running	O
text	O
for	O
metaphoricity	O
Steen	O
et	O
al	O
,	O
2010	O
)	O
;	O
Veale	O
et	O
al	O
(	O
2016	O
)	O
review	O
various	O
annotated	O
datasets	O
.	O
The	O
goal	O
of	O
this	O
shared	O
task	O
is	O
to	O
detect	O
,	O
at	O
the	O
word	O
level	O
,	O
all	O
content	O
word	O
metaphors	O
in	O
a	O
given	O
text	O
.	O
We	O
are	O
using	O
two	O
datasets	O
-	O
VUA	O
and	O
TOEFL	O
,	O
to	O
be	O
described	O
shortly	O
.	O
There	O
are	O
two	O
tracks	O
for	O
each	O
dataset	O
,	O
for	O
a	O
total	O
of	O
four	O
tracks	O
:	O
VUA	O
All	O
POS	O
,	O
VUA	O
Verbs	O
,	O
TOEFL	O
All	O
POS	O
,	O
and	O
TOEFL	O
Verbs	O
.	O
The	O
AllPOS	O
track	O
is	O
concerned	O
with	O
the	O
detection	O
of	O
all	O
content	O
words	O
,	O
i.e.	O
,	O
nouns	O
,	O
verbs	O
,	O
adverbs	O
and	O
adjectives	O
that	O
are	O
labeled	O
as	O
metaphorical	O
while	O
the	O
Verbs	O
track	O
is	O
concerned	O
only	O
with	O
verbs	O
that	O
are	O
metaphorical	O
.	O
We	O
excluded	O
all	O
forms	O
of	O
be	O
,	O
do	O
,	O
and	O
have	O
for	O
both	O
tracks	O
.	O
For	O
each	O
dataset	O
,	O
each	O
participating	O
individual	O
or	O
team	O
can	O
elect	O
to	O
compete	O
in	O
the	O
All	O
POS	O
track	O
,	O
Verbs	O
track	O
,	O
or	O
both	O
.	O
The	O
competition	O
is	O
organized	O
into	O
two	O
phases	O
:	O
training	O
and	O
testing	O
.	O

We	O
make	O
available	O
to	O
shared	O
task	O
participants	O
a	O
number	O
of	O
features	O
from	O
prior	O
published	O
work	O
on	O
metaphor	O
detection	O
,	O
including	O
unigram	O
features	O
,	O
features	O
based	O
on	O
WordNet	O
,	O
VerbNet	O
,	O
and	O
those	O
derived	O
from	O
a	O
distributional	O
semantic	O
model	O
,	O
POS	O
-	O
based	O
,	O
concreteness	O
and	O
difference	O
in	O
concreteness	O
,	O
as	O
well	O
as	O
topic	B-TaskName
models	I-TaskName
.	O
We	O
adopted	O
three	O
informed	O
baselines	O
from	O
prior	O
work	O
.	O
As	O
Baseline	O
1	O
:	O
UL	O
+	O
WordNet	O
+	O
CCDB	O
,	O
we	O
use	O
the	O
best	O
system	O
from	O
Beigman	O
.	O
The	O
features	O
are	O
:	O
lemmatized	O
unigrams	O
,	O
generalized	O
WordNet	O
semantic	O
classes	O
,	O
and	O
difference	O
in	O
concreteness	O
ratings	O
between	O
verbs	O
/	O
adjectives	O
and	O
nouns	O
(	O
UL	O
+	O
WN	O
+	O
CCDB	O
)	O
.	O
6	O
Baseline	O
2	O
:	O
bot.zen	O
is	O
one	O
of	O
the	O
top	O
-	O
ranked	O
systems	O
in	O
the	O
first	O
metaphor	O
shared	O
task	O
in	O
2018	O
by	O
Stemle	O
and	O
Onysko	O
(	O
2018	O
)	O
that	O
uses	O
a	O
bi	O
-	O
directional	O
recursive	O
neural	O
network	O
architecture	O
with	O
long	O
-	O
term	O
short	O
-	O
term	O
memory	O
(	O
LSTM	B-MethodName
BiRNN	O
)	O
and	O
implements	O
a	O
flat	O
sequenceto	O
-	O
sequence	O
neural	O
network	O
with	O
one	O
hidden	O
layer	O
using	O
TensorFlow	O
and	O
Keras	O
in	O
Python	O
.	O
The	O
system	O
uses	O
fastText	B-MethodName
word	B-TaskName
embeddings	I-TaskName
from	O
different	O
corpora	O
,	O
including	O
learner	O
corpus	O
and	O
BNC	O
data	O
.	O
Finally	O
,	O
Baseline	O
3	O
:	O
BERT	B-MethodName
is	O
constructed	O
by	O
finetuning	O
the	O
BERT	B-MethodName
model	O
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
in	O
a	O
standard	O
token	B-TaskName
classification	I-TaskName
task	O
:	O
After	O
obtaining	O
the	O
contextualized	O
embeddings	O
of	O
a	O
sentence	O
,	O
we	O
apply	O
a	O
linear	B-MethodName
layer	I-MethodName
followed	O
by	O
softmax	B-MethodName
on	O
each	O
token	O
to	O
predict	O
whether	O
it	O
is	O
metaphorical	O
or	O
not	O
.	O
gives	O
more	O
details	O
about	O
the	O
architecture	O
of	O
this	O
baseline	O
.	O
For	O
Verbs	O
tracks	O
,	O
we	O
tune	O
the	O
system	O
on	O
All	O
POS	O
data	O
and	O
test	O
on	O
Verbs	O
,	O
as	O
this	O
produced	O
better	O
results	O
during	O
preliminary	O
experimentation	O
than	O
training	O
on	O
Verbs	O
only	O
.	O
Zenith	O
:	O
Character	O
embeddings	O
+	O
Similarity	O
Networks	O
+	O
Bi	O
-	O
LSTM	B-MethodName
+	O
Transformer	B-MethodName
Kumar	B-DatasetName
and	O
Sharma	O
(	O
2020	O
)	O
added	O
lexical	O
and	O
orthographic	O
information	O
via	O
character	O
embeddings	O
in	O
addition	O
to	O
GloVe	B-MethodName
and	O
ELMo	B-MethodName
embeddings	O
for	O
an	O
enriched	O
input	O
representation	O
.	O
The	O
authors	O
also	O
constructed	O
a	O
similarity	O
metric	O
between	O
the	O
literal	O
and	O
contextual	O
representations	O
of	O
a	O
word	O
as	O
another	O
input	O
component	O
.	O
A	O
Bi	O
-	O
LSTM	B-MethodName
network	O
and	O
Transformer	B-MethodName
network	O
are	O
trained	O
independently	O
and	O
combined	O
in	O
an	O
ensemble	O
.	O
Eventually	O
,	O
adding	O
both	O
character	O
-	O
based	O
information	O
and	O
similarity	O
network	O
are	O
the	O
most	O
helpful	O
,	O
as	O
evidenced	O
by	O
results	O
obtained	O
using	O
cross	O
-	O
validation	O
on	O
the	O
training	O
datasets	O
.	O
rowanhm	O
:	O
Static	O
and	O
contextual	O
embeddings	O
+	O
concreteness	O
+	O
Multi	O
-	O
layer	O
Perceptron	O
Maudslay	O
et	O
al	O
(	O
2020	O
)	O
created	O
a	O
system	O
that	O
combines	O
the	O
concreteness	O
of	O
a	O
word	O
,	O
its	O
static	O
embedding	O
and	O
its	O
contextual	O
embedding	O
before	O
providing	O
them	O
as	O
inputs	O
into	O
a	O
deep	O
Multi	O
-	O
layer	O
Perceptron	O
network	O
which	O
predicts	O
word	O
metaphoricity	O
.	O
Specifically	O
,	O
the	O
concreteness	O
value	O
of	O
a	O
word	O
is	O
formulated	O
as	O
a	O
linear	O
interpolation	O
between	O
two	O
reference	O
vectors	O
(	O
concrete	O
and	O
abstract	O
)	O
which	O
were	O
randomly	O
initialized	O
and	O
learned	O
from	O
data	O
.	O
iiegn	O
:	O
LSTM	B-MethodName
BiRNN	O
+	O
metadata	O
;	O
combine	O
TOEFL	O
and	O
VUA	O
data	O
Stemle	O
and	O
Onysko	O
(	O
2020	O
)	O
used	O
an	O
LSTM	B-MethodName
BiRNN	O
classifier	O
to	O
study	O
the	O
relationship	O
between	O
the	O
metadata	O
in	O
the	O
TOEFL	O
corpus	O
(	O
proficiency	O
,	O
L1	O
of	O
the	O
author	O
,	O
and	O
the	O
prompt	O
to	O
which	O
the	O
essay	O
is	O
responding	O
)	O
and	O
classifier	O
performance	O
.	O
The	O
system	O
is	O
an	O
extension	O
of	O
the	O
authors	O
'	O
system	O
for	O
the	O
2018	O
shared	O
task	O
(	O
Stemle	O
and	O
Onysko	O
,	O
2018	O
)	O
that	O
served	O
as	O
one	O
of	O
the	O
baseline	O
in	O
the	O
current	O
shared	O
task	O
(	O
see	O
section	O
4.1	O
)	O
.	O
Analyzing	O
the	O
training	O
data	O
,	O
the	O
authors	O
observed	O
that	O
essays	O
written	O
by	O
more	O
proficient	O
users	O
had	O
significantly	O
more	O
metaphors	O
,	O
and	O
that	O
essays	O
responding	O
to	O
some	O
of	O
the	O
prompts	O
had	O
significantly	O
more	O
metaphors	O
than	O
other	O
prompts	O
;	O
however	O
,	O
using	O
proficiency	O
and	O
prompt	O
metadata	O
explicitly	O
in	O
the	O
classifier	O
did	O
not	O
improve	O
performance	O
.	O
The	O
authors	O
also	O
experimented	O
with	O
combining	O
VUA	O
and	O
TOEFL	O
data	O
.	O
Duke	O
Data	O
Science	O
:	O
BERT	B-MethodName
,	O
XNET	O
language	O
models	O
+	O
POS	O
tags	O
as	O
features	O
for	O
a	O
Bi	O
-	O
LSTM	B-MethodName
classifier	O
Liu	O
et	O
al	O
(	O
2020	O
)	O
use	O
pre	O
-	O
trained	O
BERT	B-MethodName
and	O
XLNet	B-MethodName
language	O
models	O
to	O
create	O
contextualized	O
embeddings	O
,	O
which	O
are	O
combined	O
with	O
POS	O
tags	O
to	O
generate	O
features	O
for	O
a	O
Bi	O
-	O
LSTM	B-MethodName
for	O
token	O
-	O
level	O
metaphor	O
classification	O
.	O
For	O
the	O
testing	O
phase	O
,	O
the	O
authros	O
used	O
an	O
ensemble	O
strategy	O
,	O
training	O
four	O
copies	O
of	O
the	O
Bi	O
-	O
LSTM	B-MethodName
with	O
different	O
initializations	O
and	O
averaging	O
their	O
predictions	O
.	O
To	O
increase	O
the	O
likelihood	O
of	O
prediction	O
of	O
a	O
metaphor	O
label	O
,	O
a	O
token	O
is	O
declared	O
a	O
metaphor	O
if	O
:	O
(	O
1	O
)	O
its	O
predicted	O
probability	O
is	O
higher	O
than	O
the	O
threshold	O
,	O
or	O
(	O
2	O
)	O
if	O
its	O
probability	O
is	O
three	O
orders	O
of	O
magnitude	O
higher	O
than	O
the	O
median	O
predicted	O
probability	O
for	O
that	O
word	O
in	O
the	O
evaluation	O
set	O
.	O
chasingkangaroos	O
:	O
RNN	O
+	O
BiLSTM	B-MethodName
+	O
Attention	O
+	O
Ensemble	O
Brooks	O
and	O
Youssef	O
(	O
2020	O
)	O
use	O
an	O
ensemble	O
of	O
RNN	O
models	O
with	O
Bi	O
-	O
LSTMs	O
and	O
bidirectional	O
attention	O
mechanisms	O
.	O
Each	O
word	O
was	O
represented	O
by	O
an	O
11	O
-	O
gram	O
and	O
appeared	O
at	O
the	O
center	O
of	O
the	O
11	O
-	O
gram	O
;	O
each	O
word	O
in	O
the	O
11	O
-	O
gram	O
was	O
represented	O
by	O
a	O
1	O
,	O
324	O
dimensional	O
word	O
embedding	O
(	O
concatenation	O
of	O
ELMo	B-MethodName
and	O
GloVe	B-MethodName
embeddings	I-MethodName
)	O
.	O
The	O
authors	O
experimented	O
with	O
ensembles	O
of	O
models	O
that	O
implement	O
somewhat	O
different	O
architecture	O
(	O
in	O
terms	O
of	O
attention	O
)	O
and	O
models	O
trained	O
on	O
all	O
POS	O
and	O
on	O
a	O
specific	O
POS	O
.	O
baseline	O
system	O
(	O
also	O
one	O
of	O
the	O
shared	O
task	O
baselines	O
,	O
see	O
section	O
4.1	O
)	O
uses	O
BERT	B-MethodName
-	O
after	O
obtaining	O
the	O
contextualized	O
embeddings	O
of	O
a	O
sentence	O
,	O
a	O
linear	B-MethodName
layer	I-MethodName
is	O
applied	O
followed	O
by	O
softmax	B-MethodName
on	O
each	O
token	O
to	O
predict	O
whether	O
it	O
is	O
metaphorical	O
or	O
not	O
.	O
The	O
authors	O
spell	O
-	O
correct	O
the	O
TOEFL	O
data	O
,	O
which	O
improves	O
performance	O
.	O
present	O
two	O
multi	O
-	O
task	O
settings	O
:	O
In	O
the	O
first	O
,	O
metaphor	O
detection	O
on	O
out	O
-	O
of	O
-	O
domain	O
data	O
is	O
treated	O
as	O
an	O
auxiliary	O
task	O
;	O
in	O
the	O
second	O
,	O
idiom	O
detection	O
on	O
in	O
-	O
domain	O
data	O
is	O
the	O
auxiliary	O
task	O
.	O
Performance	O
on	O
TOEFL	O
is	O
helped	O
by	O
the	O
first	O
multi	O
-	O
task	O
setting	O
;	O
performance	O
on	O
VUA	O
is	O
helped	O
by	O
the	O
second	O
.	O

UoB	O
team	O
:	O
Bi	O
-	O
LSTM	B-MethodName
+	O
GloVe	B-MethodName
embeddings	I-MethodName
+	O
concreteness	O
Alnafesah	O
et	O
al	O
(	O
2020	O
)	O
explore	O
ways	O
of	O
using	O
concreteness	O
information	O
in	O
a	O
neural	O
metaphor	O
detection	O
context	O
.	O
GloVe	B-MethodName
embeddings	I-MethodName
are	O
used	O
as	O
features	O
to	O
an	O
SVM	B-MethodName
classifier	O
to	O
learn	O
concreteness	O
values	O
,	O
training	O
it	O
using	O
human	O
labels	O
of	O
concreteness	O
.	O
Then	O
,	O
for	O
metaphor	O
detection	O
,	O
every	O
input	O
word	O
is	O
represented	O
as	O
a	O
304	O
-	O
dimensional	O
vector	O
-	O
300	O
dimensions	O
are	O
GloVe	B-MethodName
pre	O
-	O
trained	O
embeddings	O
,	O
plus	O
probabilities	O
for	O
the	O
four	O
concreteness	O
classes	O
.	O
These	O
representations	O
of	O
words	O
are	O
given	O
as	O
input	O
to	O
a	O
Bi	O
-	O
LSTM	B-MethodName
which	O
outputs	O
a	O
sequence	O
of	O
labels	O
.	O
Results	O
suggest	O
that	O
explicit	O
concreteness	O
information	O
helps	O
improve	O
metaphor	O
detection	O
,	O
relative	O
to	O
a	O
baseline	O
that	O
uses	O
GloVe	B-MethodName
embeddings	I-MethodName
only	O
.	O
zhengchang	O
:	O
ALBERT	B-MethodName
+	O
BiLSTM	B-MethodName
Li	O
et	O
al	O
(	O
2020	O
)	O
use	O
a	O
sequence	O
labeling	O
model	O
based	O
on	O
ALBERT	B-MethodName
-	O
LSTM	B-MethodName
-	O
Softmax	B-MethodName
.	O
Embeddings	O
produced	O
by	O
BERT	B-MethodName
serve	O
as	O
input	O
to	O
BiLSTM	B-MethodName
,	O
as	O
well	O
as	O
to	O
the	O
final	O
softmax	B-MethodName
layer	O
.	O
The	O
authors	O
report	O
on	O
experiments	O
with	O
inputs	O
to	O
BERT	B-MethodName
(	O
single	O
-	O
sentence	O
vs	O
pairs	O
;	O
variants	O
using	O
BERT	B-MethodName
tokenization	O
)	O
,	O
spellcorrection	O
of	O
the	O
TOEFL	O
data	O
,	O
and	O
CRF	B-MethodName
vs	O
softmax	B-MethodName
at	O
the	O
classification	O
layer	O
.	O
PolyU	O
-	O
LLT	O
:	O
Sensorimotor	O
and	O
embodiment	O
features	O
+	O
embeddings	O
+	O
n	O
-	O
grams	O
+	O
logistic	B-MethodName
regression	I-MethodName
classifier	O
Wan	O
et	O
al	O
(	O
2020	O
)	O
use	O
sensorimotor	O
and	O
embodiment	O
features	O
.	O
They	O
use	O
the	O
Lancaster	O
Sensorimotor	O
norms	O
(	O
Lynott	O
et	O
al	O
,	O
2019	O
)	O
that	O
include	O
measures	O
of	O
sensorimotor	O
strength	O
for	O
about	O
40	O
K	O
English	O
words	O
across	O
six	O
perceptual	O
modalities	O
(	O
e.g.	O
,	O
touch	O
,	O
hearing	O
,	O
smell	O
)	O
,	O
and	O
five	O
action	O
effectors	O
(	O
mouth	O
/	O
throat	O
,	O
hand	O
/	O
arm	O
,	O
etc	O
)	O
,	O
and	O
embodiment	O
norms	O
from	O
Sidhu	O
et	O
al	O
(	O
2014	O
)	O
.	O
The	O
authors	O
also	O
use	O
word	O
,	O
lemma	B-DatasetName
,	O
and	O
POS	O
n	O
-	O
grams	O
;	O
word2vec	O
and	O
GloVe	B-MethodName
word	B-TaskName
embeddings	I-TaskName
,	O
as	O
well	O
as	O
cosine	O
distance	O
measurements	O
using	O
the	O
embeddings	O
.	O
The	O
different	O
features	O
are	O
combined	O
using	O
logistic	B-MethodName
regression	I-MethodName
and	O
other	O
classifiers	O
.	O

Interactive	O
Query	O
-	O
Assisted	O
Summarization	B-TaskName
via	O
Deep	O
Reinforcement	O
Learning	O

Interactive	O
summarization	B-TaskName
is	O
a	O
task	O
that	O
facilitates	O
user	O
-	O
guided	O
exploration	O
of	O
information	O
within	O
a	O
document	O
set	O
.	O
While	O
one	O
would	O
like	O
to	O
employ	O
state	O
of	O
the	O
art	O
neural	O
models	O
to	O
improve	O
the	O
quality	O
of	O
interactive	O
summarization	B-TaskName
,	O
many	O
such	O
technologies	O
can	O
not	O
ingest	O
the	O
full	O
document	O
set	O
or	O
can	O
not	O
operate	O
at	O
sufficient	O
speed	O
for	O
interactivity	O
.	O
To	O
that	O
end	O
,	O
we	O
propose	O
two	O
novel	O
deep	O
reinforcement	O
learning	O
models	O
for	O
the	O
task	O
that	O
address	O
,	O
respectively	O
,	O
the	O
subtask	O
of	O
summarizing	O
salient	O
information	O
that	O
adheres	O
to	O
user	O
queries	O
,	O
and	O
the	O
subtask	O
of	O
listing	O
suggested	O
queries	O
to	O
assist	O
users	O
throughout	O
their	O
exploration	O
.	O
1	O
In	O
particular	O
,	O
our	O
models	O
allow	O
encoding	O
the	O
interactive	O
session	O
state	O
and	O
history	O
to	O
refrain	O
from	O
redundancy	O
.	O
Together	O
,	O
these	O
models	O
compose	O
a	O
state	O
of	O
the	O
art	O
solution	O
that	O
addresses	O
all	O
of	O
the	O
task	O
requirements	O
.	O
We	O
compare	O
our	O
solution	O
to	O
a	O
recent	O
interactive	O
summarization	B-TaskName
system	O
,	O
and	O
show	O
through	O
an	O
experimental	O
study	O
involving	O
real	O
users	O
that	O
our	O
models	O
are	O
able	O
to	O
improve	O
informativeness	O
while	O
preserving	O
positive	O
user	O
experience	O
.	O

Integrating	O
human	O
interaction	O
into	O
NLP	O
tasks	O
has	O
been	O
gaining	O
the	O
interest	O
of	O
the	O
NLP	O
community	O
.	O
Human	O
-	O
machine	O
cooperation	O
can	O
improve	O
the	O
general	O
quality	O
of	O
results	O
,	O
as	O
well	O
as	O
provide	O
a	O
higher	O
sense	O
of	O
control	O
for	O
the	O
targeted	O
consumer	O
.	O
We	O
focus	O
on	O
the	O
task	O
of	O
interactive	O
summarization	B-TaskName
(	O
INTSUMM	O
:	O
Shapira	O
et	O
al	O
,	O
2021b	O
)	O
which	O
enables	O
information	O
exploration	O
within	O
a	O
document	O
set	O
on	O
a	O
topic	O
,	O
by	O
means	O
of	O
user	O
-	O
guided	O
summarization	B-TaskName
.	O
As	O
illustrated	O
in	O
Figure	O
1	O
,	O
a	O
user	O
can	O
incrementally	O
expand	O
on	O
a	O
summary	O
by	O
submitting	O
requests	O
to	O
the	O
system	O
,	O
in	O
order	O
to	O
expose	O
the	O
information	O
of	O
interest	O
within	O
the	O
topic	O
.	O
A	O
proper	O
exploration	O
session	O
demands	O
access	O
to	O
all	O
information	O
within	O
the	O
document	O
set	O
,	O
and	O
fast	O
reaction	O
time	O
for	O
smooth	O
human	O
Figure	O
1	O
:	O
An	O
INTSUMM	O
system	O
,	O
ingesting	O
a	O
large	O
document	O
set	O
.	O
A	O
user	O
interactively	O
submits	O
queries	O
in	O
order	O
to	O
expand	O
on	O
the	O
information	O
.	O
The	O
system	O
is	O
required	O
to	O
process	O
the	O
full	O
document	O
set	O
for	O
comprehensive	O
exploration	O
,	O
respond	O
quickly	O
,	O
and	O
expose	O
nonredundant	O
salient	O
information	O
that	O
also	O
complies	O
to	O
the	O
input	O
queries	O
.	O
See	O
real	O
example	O
in	O
Figure	O
5	O
.	O
engagement	O
(	O
Anderson	O
,	O
2020	O
;	O
Attig	O
et	O
al	O
,	O
2017	O
)	O
.	O
In	O
addition	O
,	O
presented	O
information	O
must	O
consider	O
the	O
session	O
history	O
to	O
refrain	O
from	O
repetitiveness	O
.	O
While	O
it	O
is	O
worthwhile	O
to	O
apply	O
recent	O
NLP	O
advances	O
that	O
excel	O
at	O
extracting	O
salient	O
and	O
querybiased	O
information	O
,	O
those	O
advances	O
usually	O
come	O
at	O
a	O
cost	O
of	O
rather	O
small	O
input	O
size	O
limits	O
or	O
heavy	O
computation	O
time	O
.	O
Indeed	O
,	O
all	O
previous	O
interactive	O
summarization	B-TaskName
systems	O
we	O
know	O
of	O
either	O
apply	O
traditional	O
methods	O
or	O
are	O
inadequate	O
for	O
real	O
-	O
time	O
processing	O
due	O
to	O
high	O
latency	O
(	O
2	O
)	O
.	O
Our	O
goal	O
is	O
to	O
overcome	O
these	O
obstacles	O
,	O
and	O
leverage	O
advanced	O
methods	O
to	O
improve	O
information	O
exposure	O
while	O
keeping	O
latency	O
acceptable	O
for	O
interaction	O
.	O
As	O
depicted	O
in	O
Figure	O
1	O
,	O
an	O
INTSUMM	O
system	O
provides	O
an	O
initial	O
generic	O
summary	O
as	O
an	O
overview	O
of	O
the	O
topic	O
,	O
after	O
which	O
a	O
user	O
can	O
iteratively	O
issue	O
queries	O
to	O
the	O
system	O
for	O
summary	O
expansions	O
on	O
subtopics	O
of	O
interest	O
.	O
To	O
support	O
querying	O
,	O
the	O
system	O
offers	O
a	O
list	O
of	O
suggested	O
queries	O
,	O
hinting	O
at	O
information	O
concealed	O
within	O
the	O
document	O
set	O
.	O
We	O
address	O
the	O
INTSUMM	O
task	O
components	O
through	O
two	O
subtasks	O
:	O
(	O
1	O
)	O
generating	O
the	O
initial	O
summary	O
and	O
query	O
responses	O
,	O
and	O
(	O
2	O
)	O
generating	O
lists	O
of	O
suggested	O
queries	O
.	O
For	O
each	O
of	O
the	O
subtasks	O
we	O
propose	O
a	O
deep	O
reinforcement	O
learning	O
(	O
RL	O
)	O
algorithm	O
that	O
addresses	O
the	O
respective	O
sub	O
-	O
task	O
requirements	O
.	O
To	O
enable	O
comprehensive	O
topic	O
exploration	O
,	O
our	O
models	O
speedily	O
process	O
the	O
full	O
document	O
set	O
,	O
as	O
inspired	O
by	O
.	O
Additionally	O
,	O
they	O
are	O
able	O
to	O
peek	O
at	O
session	O
history	O
to	O
comply	O
to	O
the	O
current	O
state	O
of	O
the	O
interaction	O
.	O
The	O
model	O
for	O
the	O
query	O
-	O
assisted	O
summarization	B-TaskName
subtask	O
,	O
M	O
Summ	O
,	O
incorporates	O
the	O
query	O
sequence	O
by	O
(	O
1	O
)	O
encoding	O
a	O
query	O
into	O
the	O
contextual	O
sentence	O
representations	O
,	O
(	O
2	O
)	O
attending	O
the	O
representations	O
using	O
a	O
new	O
query	O
-	O
biased	O
variant	O
of	O
the	O
maximal	O
marginal	O
relevance	O
(	O
MMR	O
:	O
Carbonell	O
and	O
Goldstein	O
,	O
1998	O
)	O
function	O
,	O
and	O
(	O
3	O
)	O
a	O
dual	O
reward	O
mechanism	O
for	O
policy	O
optimization	O
(	O
Pasunuru	O
and	O
Bansal	O
,	O
2018	O
)	O
which	O
we	O
adapt	O
to	O
consider	O
both	O
reference	O
summaries	O
and	O
the	O
query	O
(	O
3	O
)	O
.	O
The	O
model	O
for	O
the	O
suggested	O
queries	O
list	O
generation	O
subtask	O
,	O
M	O
Sugg	O
,	O
works	O
at	O
the	O
phrase	O
level	O
,	O
as	O
opposed	O
to	O
the	O
sentence	O
level	O
,	O
to	O
enable	O
extraction	O
of	O
important	O
phrases	O
that	O
serve	O
as	O
suggested	O
queries	O
.	O
Similarly	O
to	O
M	O
Summ	O
,	O
the	O
model	O
learns	O
importance	O
with	O
consideration	O
to	O
session	O
history	O
,	O
but	O
without	O
an	O
input	O
query	O
-	O
as	O
its	O
role	O
is	O
to	O
suggest	O
such	O
a	O
query	O
(	O
4	O
)	O
.	O
The	O
models	O
are	O
trained	O
on	O
the	O
DUC	O
2	O
2007	O
multidocument	O
summarization	B-TaskName
(	O
MDS	O
)	O
news	O
-	O
domain	O
dataset	O
,	O
with	O
adaptions	O
for	O
our	O
task	O
setting	O
.	O
For	O
testing	O
,	O
we	O
follow	O
the	O
INTSUMM	O
evaluation	O
framework	O
of	O
Shapira	O
et	O
al	O
(	O
2021b	O
)	O
to	O
run	O
simulations	O
,	O
collect	O
real	O
user	O
sessions	O
,	O
and	O
assess	O
the	O
results	O
,	O
using	O
DUC	B-DatasetName
2006	I-DatasetName
.	O
In	O
principle	O
,	O
summary	O
informativeness	O
,	O
i.e.	O
general	O
salience	O
,	O
could	O
potentially	O
come	O
at	O
the	O
expense	O
of	O
query	O
responsiveness	O
,	O
but	O
importantly	O
,	O
our	O
results	O
show	O
that	O
our	O
RL	O
-	O
based	O
solution	O
is	O
able	O
to	O
significantly	O
improve	O
information	O
exposure	O
over	O
the	O
baseline	O
of	O
Shapira	O
et	O
al	O
(	O
2021b	O
)	O
,	O
without	O
compromising	O
user	O
experience	O
(	O
5	O
)	O
.	O

Interactive	O
summarization	B-TaskName
facilitates	O
user	O
-	O
guided	O
information	O
navigation	O
within	O
document	O
sets	O
.	O
The	O
task	O
suffered	O
from	O
a	O
lack	O
of	O
a	O
methodological	O
evaluation	O
,	O
until	O
Shapira	O
et	O
al	O
(	O
2021b	O
)	O
formalized	O
the	O
INTSUMM	O
task	O
with	O
a	O
framework	O
consisting	O
of	O
a	O
benchmark	O
,	O
evaluation	O
metrics	O
,	O
a	O
session	O
collection	O
process	O
and	O
baseline	O
systems	O
.	O
This	O
framework	O
,	O
that	O
we	O
leverage	O
,	O
enables	O
comparison	O
and	O
analysis	O
of	O
systems	O
,	O
allowing	O
principled	O
research	O
on	O
the	O
task	O
and	O
accelerated	O
development	O
of	O
algorithms	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
all	O
previous	O
works	O
on	O
INTSUMM	O
have	O
either	O
applied	O
more	O
traditional	O
text	O
-	O
processing	O
methods	O
or	O
require	O
costly	O
prepro	O
-	O
cessing	O
of	O
inputs	O
to	O
facilitate	O
seamless	O
interaction	O
.	O
Leuski	O
et	O
al	O
(	O
2003	O
)	O
used	O
surface	O
-	O
form	O
features	O
for	O
processing	O
content	O
,	O
and	O
Baumel	O
et	O
al	O
(	O
2014	O
)	O
adapted	O
classic	O
MDS	O
algorithms	O
like	O
LexRank	O
(	O
Erkan	O
and	O
Radev	O
,	O
2004	O
)	O
and	O
KLSum	O
(	O
Haghighi	O
and	O
Vanderwende	O
,	O
2009	O
)	O
.	O
Christensen	O
et	O
al	O
(	O
2014	O
)	O
optimized	O
discourse	O
graphs	O
and	O
Shapira	O
et	O
al	O
(	O
2017	O
)	O
relied	O
on	O
a	O
knowledge	O
representation	O
,	O
both	O
expensively	O
pre	O
-	O
generating	O
hierarchical	O
summaries	O
that	O
limit	O
expansions	O
to	O
pre	O
-	O
prepared	O
information	O
selections	O
.	O
Hirsch	O
et	O
al	O
(	O
2021	O
)	O
applied	O
advanced	O
coreference	B-TaskName
resolution	I-TaskName
algorithms	O
that	O
take	O
several	O
hours	O
for	O
preprocessing	O
a	O
document	O
set	O
.	O
The	O
two	O
INTSUMM	O
baseline	O
systems	O
of	O
Shapira	O
et	O
al	O
(	O
2021b	O
)	O
use	O
sentence	O
clustering	O
or	O
TextRank	O
(	O
Mihalcea	O
and	O
Tarau	O
,	O
2004	O
)	O
for	O
summarization	B-TaskName
,	O
sentence	O
similarity	O
heuristics	O
for	O
query	O
-	O
responses	O
,	O
and	O
n	O
-	O
gram	O
frequency	O
or	O
TextRank	O
for	O
suggested	O
query	O
extraction	O
.	O
Moreover	O
,	O
their	O
query	O
-	O
response	O
generators	O
strictly	O
consider	O
a	O
given	O
query	O
,	O
ignoring	O
history	O
or	O
global	O
informativeness	O
.	O
Our	O
proposed	O
algorithms	O
significantly	O
improve	O
information	O
exposure	O
over	O
the	O
latter	O
baselines	O
,	O
using	O
advanced	O
deep	O
RL	O
methods	O
,	O
working	O
in	O
real	O
time	O
.	O
We	O
next	O
review	O
some	O
recent	O
techniques	O
in	O
MDS	O
,	O
query	O
-	O
focused	O
summarization	B-TaskName
and	O
multi	O
-	O
document	O
keyphrase	B-TaskName
extraction	I-TaskName
,	O
all	O
of	O
which	O
relate	O
to	O
the	O
INTSUMM	O
task	O
and	O
our	O
choice	O
of	O
algorithms	O
.	O
The	O
subtask	O
of	O
query	O
-	O
assisted	O
summarization	B-TaskName
.	O
Non	O
-	O
interactive	O
MDS	O
has	O
been	O
researched	O
extensively	O
,	O
with	O
few	O
recent	O
neural	O
-	O
based	O
methods	O
that	O
can	O
handle	O
relatively	O
large	O
inputs	O
.	O
For	O
example	O
,	O
Wang	O
et	O
al	O
(	O
2020	O
)	O
use	O
graph	O
neural	O
networks	O
to	O
globally	O
score	O
sentence	O
salience	O
,	O
Xiao	O
et	O
al	O
(	O
2021	O
)	O
summarize	O
using	O
Longformers	O
(	O
Beltagy	O
et	O
al	O
,	O
2020	O
)	O
,	O
and	O
combine	O
a	O
Longformer	B-MethodName
with	O
BART	B-MethodName
(	O
Lewis	O
et	O
al	O
,	O
2020	O
)	O
and	O
incorporate	O
graphical	O
representation	O
of	O
information	O
.	O
apply	O
deep	O
RL	O
for	O
autoregressive	O
sentence	O
selection	O
,	O
and	O
,	O
in	O
contrast	O
to	O
most	O
other	O
neural	O
methods	O
,	O
can	O
ingest	O
the	O
full	O
document	O
set	O
.	O
In	O
the	O
query	O
-	O
focused	O
summarization	B-TaskName
(	O
QFS	O
)	O
task	O
summaries	O
are	O
biased	O
on	O
a	O
query	O
.	O
To	O
accommodate	O
a	O
query	O
,	O
use	O
conditional	O
selfattention	O
to	O
enforce	O
dependency	O
of	O
the	O
query	O
on	O
source	O
words	O
.	O
Pasunuru	O
et	O
al	O
(	O
2021a	O
)	O
and	O
Kulkarni	O
et	O
al	O
(	O
2021	O
)	O
hierarchically	O
encode	O
a	O
query	O
with	O
the	O
documents	O
.	O
These	O
and	O
other	O
QFS	O
methods	O
require	O
large	O
training	O
sets	O
,	O
and	O
limit	O
the	O
allowed	O
input	O
size	O
(	O
Baumel	O
et	O
al	O
,	O
2018	O
;	O
Laskar	O
et	O
al	O
,	O
2020	O
)	O
.	O
Relatedly	O
,	O
incremental	O
update	O
summarization	B-TaskName
(	O
Mc	O
-	O
Creadie	O
et	O
al	O
,	O
2014	O
;	O
Lin	O
et	O
al	O
,	O
2017	O
)	O
marks	O
queryrelevant	O
information	O
as	O
reported	O
texts	O
stream	O
in	O
,	O
avoiding	O
repeating	O
information	O
marked	O
earlier	O
.	O
Interactivity	O
is	O
not	O
a	O
constraining	O
factor	O
here	O
,	O
yielding	O
solutions	O
with	O
relatively	O
high	O
computation	O
time	O
.	O
With	O
respect	O
to	O
the	O
above	O
related	O
work	O
,	O
we	O
develop	O
a	O
model	O
inspired	O
by	O
,	O
which	O
is	O
closest	O
to	O
our	O
requirements	O
.	O
To	O
facilitate	O
an	O
interactive	O
setting	O
,	O
our	O
model	O
(	O
1	O
)	O
enables	O
query+history	O
injection	O
,	O
(	O
2	O
)	O
supports	O
full	O
input	O
processing	O
,	O
necessary	O
for	O
complete	O
information	O
availability	O
during	O
exploration	O
,	O
(	O
3	O
)	O
has	O
low	O
latency	O
at	O
inference	O
time	O
,	O
and	O
(	O
4	O
)	O
requires	O
a	O
relatively	O
small	O
training	O
set	O
.	O
The	O
subtask	O
of	O
suggested	O
-	O
queries	O
list	O
generation	O
.	O
Extracting	O
suggested	O
queries	O
on	O
a	O
document	O
set	O
most	O
resembles	O
the	O
multi	O
-	O
document	O
keyphrase	B-TaskName
extraction	I-TaskName
(	O
MDKE	O
)	O
task	O
since	O
it	O
aims	O
to	O
identify	O
salient	O
keyphrases	O
(	O
Shapira	O
et	O
al	O
,	O
2021a	O
)	O
.	O
MDKE	O
was	O
mostly	O
addressed	O
using	O
traditional	O
heuristics	O
or	O
graph	O
-	O
centrality	O
algorithms	O
applied	O
over	O
the	O
documents	O
(	O
e.g.	O
Mihalcea	O
and	O
Tarau	O
,	O
2004	O
;	O
Florescu	O
and	O
Caragea	O
,	O
2017	O
)	O
.	O
In	O
contrast	O
to	O
MDKE	O
,	O
the	O
suggested	O
queries	O
extraction	O
subtask	O
is	O
a	O
new	O
paradigm	O
that	O
updates	O
"	O
keyphrases	O
"	O
with	O
respect	O
to	O
session	O
history	O
.	O
While	O
previous	O
methods	O
for	O
keyphrase	B-TaskName
extraction	I-TaskName
could	O
potentially	O
be	O
adapted	O
for	O
our	O
dynamic	O
setting	O
,	O
we	O
choose	O
to	O
focus	O
in	O
this	O
work	O
on	O
a	O
deep	O
RL	O
architecture	O
for	O
suggested	O
queries	O
that	O
resonates	O
our	O
model	O
for	O
query	O
-	O
assisted	O
summarization	B-TaskName
and	O
allows	O
sharing	O
insights	O
between	O
the	O
models	O
.	O

The	O
subtask	O
of	O
query	O
-	O
assisted	O
summarization	B-TaskName
covers	O
two	O
main	O
components	O
of	O
the	O
INTSUMM	O
task	O
:	O
the	O
generators	O
of	O
an	O
initial	O
summary	O
and	O
of	O
queryresponses	O
.	O
The	O
initial	O
summary	O
concisely	O
specifies	O
some	O
central	O
issues	O
from	O
the	O
input	O
topic	O
(	O
not	O
biased	O
on	O
a	O
query	O
)	O
to	O
initiate	O
the	O
user	O
's	O
understanding	O
of	O
the	O
topic	O
and	O
to	O
motivate	O
further	O
exploration	O
.	O
Then	O
,	O
for	O
each	O
user	O
submitted	O
query	O
,	O
the	O
query	O
-	O
response	O
generator	O
non	O
-	O
redundantly	O
expands	O
on	O
the	O
previously	O
presented	O
information	O
with	O
topically	O
salient	O
responses	O
that	O
are	O
also	O
biased	O
around	O
the	O
query	O
.	O
We	O
next	O
formally	O
define	O
the	O
subtask	O
and	O
then	O
describe	O
our	O
RL	O
model	O
for	O
it	O
.	O

The	O
input	O
to	O
the	O
query	O
-	O
assisted	O
summarization	B-TaskName
subtask	O
is	O
tuple	O
(	O
D	O
,	O
q	O
,	O
E	O
in	O
,	O
m	O
)	O
,	O
such	O
that	O
:	O
D	O
is	O
a	O
document	O
set	O
on	O
a	O
topic	O
where	O
the	O
j	O
-	O
th	O
sentence	O
in	O
the	O
concatenation	O
of	O
D	O
's	O
documents	O
is	O
denoted	O
s	O
j	O
;	O
q	O
is	O
a	O
query	O
,	O
and	O
can	O
be	O
empty	O
(	O
denoted	O
_	O
)	O
for	O
an	O
unbiased	O
generic	O
summary	O
;	O
E	O
in	O
=	O
{	O
e	O
in	O
1	O
,	O
...	O
,	O
e	O
in	O
k	O
}	O
is	O
a	O
sequence	O
of	O
sentences	O
from	O
D	O
termed	O
the	O
history	O
,	O
containing	O
texts	O
previously	O
output	O
in	O
the	O
session	O
;	O
and	O
m	O
is	O
the	O
number	O
of	O
sentences	O
to	O
output	O
.	O
The	O
output	O
is	O
sentence	O
sequence	O
E	O
out	O
=	O
{	O
e	O
out	O
1	O
,	O
...	O
,	O
e	O
out	O
m	O
}	O
from	O
D	O
(	O
extractive	B-TaskName
summarization	I-TaskName
)	O
.	O
When	O
inputting	O
(	O
D	O
,	O
_	O
,	O
{	O
}	O
,	O
m	O
)	O
,	O
the	O
output	O
is	O
a	O
generic	O
summary	O
of	O
m	O
sentences	O
,	O
that	O
can	O
serve	O
as	O
the	O
initial	O
summary	O
;	O
and	O
when	O
q	O
and	O
E	O
in	O
are	O
not	O
empty	O
,	O
the	O
output	O
is	O
an	O
expansion	O
on	O
E	O
in	O
in	O
response	O
to	O
q	O
,	O
containing	O
new	O
salient	O
information	O
biased	O
on	O
q.	O
D	O
is	O
paired	O
with	O
a	O
set	O
of	O
generic	O
reference	O
summaries	O
R	O
,	O
which	O
is	O
used	O
for	O
training	O
or	O
as	O
a	O
part	O
of	O
the	O
evaluation	O
effort	O
.	O

Pre	O
-	O
training	O
.	O
To	O
provide	O
a	O
warm	O
start	O
for	O
training	O
M	O
Summ	O
,	O
a	O
reduced	O
version	O
of	O
M	O
Summ	O
is	O
first	O
pre	O
-	O
trained	O
for	O
generic	O
extractive	O
single	O
-	O
document	B-TaskName
summarization	I-TaskName
using	O
the	O
large	O
-	O
scale	O
CNN	B-DatasetName
/	I-DatasetName
Daily	I-DatasetName
Mail	I-DatasetName
corpus	O
(	O
Hermann	O
et	O
al	O
,	O
2015	O
)	O
,	O
as	O
proposed	O
by	O
Chen	O
and	O
Bansal	O
(	O
2018	O
)	O
For	O
each	O
topic	O
,	O
we	O
generate	O
an	O
"	O
oracle	O
"	O
extractive	O
summary	O
by	O
greedily	O
aggregating	O
10	O
sentences	O
from	O
D	O
,	O
that	O
maximizes	O
the	O
ROUGE	O
∆	O
-	O
1	O
recall	O
against	O
R.	O
Then	O
for	O
each	O
sentence	O
,	O
we	O
extract	O
a	O
bi	O
-	O
or	O
trigram	O
that	O
is	O
most	O
lexically	O
-	O
unique	O
to	O
the	O
sentence	O
,	O
in	O
comparison	O
to	O
all	O
other	O
sentences	O
in	O
D.	O
This	O
yields	O
a	O
sequence	O
of	O
10	O
"	O
queries	O
"	O
that	O
could	O
easily	O
render	O
the	O
corresponding	O
oracle	O
summary	O
.	O
The	O
intuition	O
for	O
this	O
approach	O
is	O
that	O
it	O
would	O
teach	O
M	O
Summ	O
that	O
it	O
is	O
worthwhile	O
to	O
consider	O
a	O
given	O
query	O
when	O
selecting	O
a	O
sentence	O
that	O
is	O
informative	O
with	O
respect	O
to	O
the	O
reference	O
summaries	O
.	O
This	O
further	O
assists	O
in	O
fulfilling	O
the	O
dual	O
requirements	O
of	O
selecting	O
a	O
globally	O
informative	O
sentence	O
that	O
also	O
adheres	O
to	O
the	O
query	O
.	O
4	O
Appendix	O
B.3	O
discusses	O
usage	O
of	O
different	O
query	O
types	O
for	O
training	O
.	O
Validation	O
metric	O
.	O
As	O
the	O
interactive	O
session	O
progresses	O
,	O
a	O
recall	O
curve	O
emerges	O
,	O
that	O
maps	O
the	O
ROUGE	O
recall	O
score	O
(	O
here	O
ROUGE	O
-	O
1	O
)	O
versus	O
the	O
expanding	O
summary	O
token	O
-	O
length	O
.	O
Once	O
the	O
session	O
halts	O
,	O
the	O
area	O
under	O
the	O
curve	O
indicates	O
the	O
efficacy	O
of	O
the	O
session	O
for	O
information	O
exposure	O
.	O
A	O
higher	O
value	O
implies	O
faster	O
unveiling	O
of	O
salient	O
information	O
.	O
Normalizing	O
by	O
the	O
final	O
summary	O
length	O
allows	O
approximate	O
comparability	O
between	O
different	O
length	O
sessions	O
.	O
We	O
hence	O
use	O
the	O
average	O
(	O
over	O
topics	O
)	O
length	O
-	O
normalized	O
area	O
under	O
the	O
recall	O
curve	O
for	O
validating	O
the	O
training	O
progress	O
.	O

The	O
INTSUMM	O
task	O
involves	O
human	O
users	O
by	O
definition	O
.	O
Nevertheless	O
,	O
running	O
on	O
simulated	O
query	O
lists	O
and	O
session	O
histories	O
is	O
pertinent	O
for	O
efficient	O
system	O
evaluation	O
and	O
comparison	O
of	O
methods	O
.	O
To	O
simulate	O
the	O
query	O
-	O
assisted	O
summarization	B-TaskName
algorithms	O
,	O
we	O
utilize	O
the	O
real	O
sessions	O
recorded	O
by	O
Shapira	O
et	O
al	O
(	O
2021b	O
)	O
:	O
3	O
-	O
4	O
user	O
sessions	O
on	O
20	O
topics	O
from	O
DUC	B-DatasetName
2006	I-DatasetName
collected	O
with	O
S	O
2	O
.	O
In	O
our	O
simulation	O
,	O
each	O
summary	O
-	O
so	O
-	O
far	O
from	O
a	O
recorded	O
session	O
is	O
fed	O
as	O
input	O
to	O
the	O
system	O
together	O
with	O
the	O
following	O
recorded	O
user	O
query	O
.	O
We	O
then	O
measure	O
R	O
recall	O
1∆	O
(	O
difference	O
of	O
ROUGE	O
-	O
1	O
recall	O
incurred	O
by	O
the	O
query	O
response	O
compared	O
with	O
the	O
input	O
summary	O
-	O
so	O
-	O
far	O
)	O
.	O
Additionally	O
,	O
we	O
use	O
R	O
F	O
1	O
1	O
(	O
ROUGE	O
-	O
1	O
F	O
1	O
)	O
for	O
initial	O
summary	O
informativeness	O
.	O
Both	O
are	O
measured	O
w.r.t	O
.	O
the	O
reference	O
summaries	O
,	O
normalized	O
by	O
the	O
output	O
length	O
,	O
and	O
averaged	O
per	O
session	O
recording	O
,	O
and	O
then	O
over	O
all	O
sessions	O
and	O
topics	O
,	O
to	O
get	O
an	O
overall	O
system	O
infor	O
-	O
mativeness	O
score	O
.	O
We	O
also	O
measure	O
system	O
queryresponsiveness	O
using	O
the	O
QSIM	O
metric	O
.	O
Table	O
1	O
presents	O
a	O
representative	O
partial	O
ablation	O
of	O
the	O
M	O
Summ	O
model	O
.	O
All	O
variants	O
were	O
configured	O
to	O
output	O
sentences	O
of	O
up	O
to	O
30	O
tokens	O
,	O
initial	O
summaries	O
are	O
75	O
tokens	O
,	O
and	O
query	O
responses	O
are	O
2	O
sentences	O
.	O
Configurations	O
i	O
-	O
iv	O
use	O
the	O
query	O
in	O
training	O
,	O
while	O
v	O
and	O
vi	O
do	O
not	O
.	O
Each	O
configuration	O
is	O
measured	O
for	O
informativeness	O
(	O
columns	O
marked	O
with	O
†	O
)	O
,	O
and	O
for	O
query	O
-	O
responsiveness	O
(	O
QSIM	O
column	O
)	O
.	O
Out	O
of	O
configurations	O
i	O
-	O
iv	O
,	O
config	O
.	O
i	O
,	O
where	O
we	O
employ	O
all	O
mechanisms	O
for	O
query	O
inclusion	O
,	O
yields	O
the	O
best	O
overall	O
scores	O
in	O
both	O
informativeness	O
and	O
query	O
-	O
responsiveness	O
,	O
despite	O
the	O
inherent	O
tradeoff	O
between	O
the	O
two	O
.	O
In	O
the	O
second	O
set	O
of	O
configurations	O
(	O
v	O
-	O
vi	O
)	O
,	O
we	O
observe	O
that	O
ignoring	O
the	O
query	O
at	O
train	O
time	O
substantially	O
degrades	O
queryresponsiveness	O
,	O
and	O
this	O
is	O
expectedly	O
further	O
exacerbated	O
when	O
also	O
ignoring	O
the	O
query	O
at	O
inference	O
time	O
.	O
However	O
,	O
disregarding	O
the	O
query	O
gives	O
more	O
informative	O
expansions	O
with	O
respect	O
to	O
reference	O
summaries	O
,	O
since	O
the	O
model	O
was	O
trained	O
only	O
to	O
optimize	O
content	O
informativeness	O
,	O
and	O
is	O
less	O
likely	O
to	O
sidetrack	O
to	O
the	O
query	O
-	O
related	O
information	O
.	O
Compared	O
to	O
S	O
2	O
(	O
last	O
row	O
)	O
,	O
our	O
model	O
significantly	O
improves	O
informativeness	O
.	O
Queryresponsiveness	O
is	O
better	O
in	O
the	O
S	O
2	O
baseline	O
since	O
its	O
query	O
-	O
response	O
generator	O
simply	O
invokes	O
a	O
function	O
similar	O
to	O
QSIM	O
,	O
but	O
for	O
the	O
price	O
of	O
lower	O
informativeness	O
.	O
Still	O
,	O
this	O
does	O
not	O
lead	O
to	O
inferior	O
overall	O
user	O
experience	O
,	O
see	O
5.3	O
.	O

Interactive	O
summarization	B-TaskName
for	O
information	O
exploration	O
is	O
a	O
task	O
that	O
requires	O
compliance	O
to	O
user	O
requests	O
and	O
session	O
history	O
,	O
while	O
comprehensively	O
handling	O
a	O
large	O
input	O
document	O
set	O
.	O
These	O
requirements	O
pose	O
a	O
challenge	O
for	O
advanced	O
text	O
processing	O
methods	O
due	O
to	O
the	O
need	O
for	O
fast	O
reaction	O
time	O
.	O
We	O
present	O
novel	O
deep	O
reinforcement	O
learning	O
based	O
algorithms	O
that	O
answer	O
to	O
the	O
task	O
requirements	O
,	O
improving	O
salient	O
information	O
exposure	O
while	O
satisfying	O
user	O
queries	O
and	O
keeping	O
user	O
experience	O
positive	O
.	O
We	O
note	O
that	O
while	O
M	O
Summ	O
is	O
designed	O
for	O
the	O
INTSUMM	O
task	O
,	O
it	O
may	O
potentially	O
be	O
serviceable	O
for	O
standard	O
MDS	O
,	O
QFS	O
,	O
update	O
summarization	B-TaskName
and	O
combinations	O
thereof	O
.	O
This	O
can	O
be	O
accommodated	O
by	O
a	O
proper	O
choice	O
of	O
input	O
,	O
e.g.	O
,	O
QFS	O
can	O
be	O
addressed	O
by	O
giving	O
M	O
Summ	O
as	O
input	O
a	O
query	O
,	O
an	O
empty	O
history	O
and	O
target	O
summary	O
length	O
.	O
In	O
future	O
work	O
,	O
we	O
may	O
study	O
the	O
performance	O
of	O
our	O
solutions	O
for	O
such	O
tasks	O
,	O
as	O
well	O
as	O
strive	O
to	O
further	O
improve	O
their	O
performance	O
on	O
both	O
ends	O
of	O
the	O
INTSUMM	O
task	O
-	O
selecting	O
topically	O
salient	O
information	O
and	O
responding	O
to	O
user	O
queries	O
.	O

We	O
extracted	O
all	O
noun	O
-	O
phrases	O
from	O
the	O
document	O
set	O
by	O
first	O
mapping	O
all	O
tokens	O
to	O
their	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
tags	O
,	O
and	O
then	O
applying	O
a	O
regularexpression	O
chunker	O
with	O
regex	O
:	O
{	O
(	O
<	O
JJ	O
>	O
*	O
<	O
NN	O
.	O
*	O
>	O
+	O
<	O
IN	O
>	O
)	O
?	O
<	O
JJ	O
>	O
*	O
<	O
NN	O
.	O
*	O
>	O
+	O
}	O
.	O
These	O
steps	O
were	O
accomplished	O
with	O
NLTK	O
.	O
Phrase	O
length	O
.	O
There	O
is	O
no	O
limit	O
set	O
on	O
the	O
phrase	O
length	O
.	O
We	O
tried	O
training	O
and	O
inferring	O
with	O
a	O
phrase	O
length	O
constraint	O
of	O
4	O
words	O
,	O
but	O
found	O
that	O
this	O
gave	O
worse	O
results	O
overall	O
.	O
History	O
sentences	O
to	O
phrases	O
.	O
M	O
Sugg	O
works	O
on	O
the	O
phrase	O
level	O
.	O
Meanwhile	O
,	O
in	O
our	O
extractive	O
interactive	O
setting	O
,	O
the	O
history	O
is	O
a	O
set	O
of	O
sentences	O
already	O
presented	O
to	O
the	O
reader	O
.	O
Therefore	O
,	O
when	O
extracting	O
phrases	O
from	O
D	O
,	O
we	O
also	O
link	O
each	O
phrase	O
to	O
its	O
source	O
sentence	O
,	O
and	O
obtain	O
E	O
in	O
by	O
compiling	O
the	O
phrases	O
linked	O
from	O
the	O
history	O
sentences	O
.	O

While	O
DUC	B-DatasetName
2006	I-DatasetName
(	O
our	O
test	O
set	O
)	O
and	O
2007	O
(	O
our	O
train	O
/	O
validation	O
set	O
)	O
were	O
originally	O
designed	O
for	O
the	O
query	O
-	O
focused	O
summarization	B-TaskName
task	O
,	O
they	O
contain	O
excessive	O
topic	O
concentration	O
due	O
to	O
their	O
long	O
and	O
descriptive	O
topic	O
queries	O
(	O
Baumel	O
et	O
al	O
,	O
2016	O
)	O
.	O
Hence	O
,	O
their	O
reference	O
summaries	O
can	O
practically	O
be	O
considered	O
generic	O
.	O
9	O
https://spacy.io/	O

Modern	O
Natural	O
Language	O
Processing	O
(	O
NLP	O
)	O
models	O
are	O
known	O
to	O
be	O
sensitive	O
to	O
input	O
perturbations	O
and	O
their	O
performance	O
can	O
decrease	O
when	O
applied	O
to	O
real	O
-	O
world	O
,	O
noisy	O
data	O
.	O
However	O
,	O
it	O
is	O
still	O
unclear	O
why	O
models	O
are	O
less	O
robust	O
to	O
some	O
perturbations	O
than	O
others	O
.	O
In	O
this	O
work	O
,	O
we	O
test	O
the	O
hypothesis	O
that	O
the	O
extent	O
to	O
which	O
a	O
model	O
is	O
affected	O
by	O
an	O
unseen	O
textual	O
perturbation	O
(	O
robustness	O
)	O
can	O
be	O
explained	O
by	O
the	O
learnability	O
of	O
the	O
perturbation	O
(	O
defined	O
as	O
how	O
well	O
the	O
model	O
learns	O
to	O
identify	O
the	O
perturbation	O
with	O
a	O
small	O
amount	O
of	O
evidence	O
)	O
.	O
We	O
further	O
give	O
a	O
causal	O
justification	O
for	O
the	O
learnability	O
metric	O
.	O
We	O
conduct	O
extensive	O
experiments	O
with	O
four	O
prominent	O
NLP	O
models	O
-	O
TextRNN	O
,	O
BERT	B-MethodName
,	O
RoBERTa	B-MethodName
and	O
XLNetover	O
eight	O
types	O
of	O
textual	O
perturbations	O
on	O
three	O
datasets	O
.	O
We	O
show	O
that	O
a	O
model	O
which	O
is	O
better	O
at	O
identifying	O
a	O
perturbation	O
(	O
higher	O
learnability	O
)	O
becomes	O
worse	O
at	O
ignoring	O
such	O
a	O
perturbation	O
at	O
test	O
time	O
(	O
lower	O
robustness	O
)	O
,	O
providing	O
empirical	O
support	O
for	O
our	O
hypothesis	O
.	O

Despite	O
the	O
success	O
of	O
deep	O
neural	O
models	O
on	O
many	O
Natural	O
Language	O
Processing	O
(	O
NLP	O
)	O
tasks	O
(	O
Liu	O
et	O
al	O
,	O
2016	O
;	O
Devlin	O
et	O
al	O
,	O
2019	O
;	O
Liu	O
et	O
al	O
,	O
2019b	O
)	O
,	O
recent	O
work	O
has	O
discovered	O
that	O
these	O
models	O
are	O
not	O
robust	O
to	O
noisy	O
input	O
from	O
the	O
real	O
world	O
and	O
thus	O
their	O
performance	O
will	O
decrease	O
(	O
Prabhakaran	O
et	O
al	O
,	O
2019	O
;	O
Niu	O
et	O
al	O
,	O
2020	O
;	O
Ribeiro	O
et	O
al	O
,	O
2020	O
;	O
Moradi	O
and	O
Samwald	O
,	O
2021	O
)	O
.	O
A	O
reliable	O
NLP	O
system	O
should	O
not	O
be	O
easily	O
fooled	O
by	O
slight	O
noise	O
in	O
the	O
text	O
.	O
Although	O
a	O
wide	O
range	O
of	O
evaluation	O
approaches	O
for	O
robust	O
NLP	O
models	O
have	O
been	O
proposed	O
(	O
Ribeiro	O
et	O
al	O
,	O
2020	O
;	O
Morris	O
et	O
al	O
,	O
2020	O
;	O
Goel	O
et	O
al	O
,	O
2021	O
;	O
,	O
few	O
attempts	O
have	O
been	O
made	O
to	O
understand	O
these	O
benchmark	O
results	O
.	O
Given	O
the	O
difference	O
of	O
robustness	O
between	O
models	O
and	O
perturbations	O
,	O
it	O
is	O
a	O
natural	O
question	O
why	O
models	O
are	O
more	O
sensitive	O
to	O
some	O
perturbations	O
than	O
others	O
.	O
It	O
is	O
crucial	O
to	O
avoid	O
over	O
-	O
sensitivity	O
to	O
input	O
perturbations	O
,	O
and	O
understanding	O
why	O
it	O
happens	O
is	O
useful	O
for	O
revealing	O
the	O
weaknesses	O
of	O
current	O
models	O
and	O
designing	O
more	O
robust	O
training	O
methods	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
a	O
quantitative	O
measure	O
to	O
interpret	O
the	O
robustness	O
of	O
NLP	O
models	O
to	O
textual	O
perturbations	O
has	O
yet	O
to	O
be	O
proposed	O
.	O
To	O
improve	O
the	O
robustness	O
under	O
perturbation	O
,	O
it	O
is	O
common	O
practice	O
to	O
leverage	O
data	B-TaskName
augmentation	I-TaskName
(	O
Li	O
and	O
Specia	O
,	O
2019	O
;	O
Min	O
et	O
al	O
,	O
2020	O
;	O
Tan	O
and	O
Joty	O
,	O
2021	O
)	O
.	O
Similarly	O
,	O
how	O
much	O
data	B-TaskName
augmentation	I-TaskName
through	O
the	O
perturbation	O
improves	O
model	O
robustness	O
varies	O
between	O
models	O
and	O
perturbations	O
.	O
In	O
this	O
work	O
,	O
we	O
aim	O
to	O
investigate	O
two	O
Research	O
Questions	O
(	O
RQ	O
)	O
:	O
RQ1	O
:	O
Why	O
are	O
NLP	O
models	O
less	O
robust	O
to	O
some	O
perturbations	O
than	O
others	O
?	O
RQ2	O
:	O
Why	O
does	O
data	B-TaskName
augmentation	I-TaskName
work	O
better	O
at	O
improving	O
the	O
model	O
robustness	O
to	O
some	O
perturbations	O
than	O
others	O
?	O
We	O
test	O
a	O
hypothesis	O
for	O
RQ1	O
that	O
the	O
extent	O
to	O
which	O
a	O
model	O
is	O
affected	O
by	O
an	O
unseen	O
textual	O
perturbation	O
(	O
robustness	O
)	O
can	O
be	O
explained	O
by	O
the	O
learnability	O
of	O
the	O
perturbation	O
(	O
defined	O
as	O
how	O
well	O
the	O
model	O
learns	O
to	O
identify	O
the	O
perturbation	O
with	O
a	O
small	O
amount	O
of	O
evidence	O
)	O
.	O
We	O
also	O
validate	O
another	O
hypothesis	O
for	O
RQ2	O
that	O
the	O
learnability	O
metric	O
is	O
predictive	O
of	O
the	O
improvement	O
on	O
robust	O
performance	O
brought	O
by	O
data	B-TaskName
augmentation	I-TaskName
along	O
a	O
perturbation	O
.	O
Our	O
proposed	O
learnability	O
is	O
inspired	O
by	O
the	O
concepts	O
of	O
Randomized	O
Controlled	O
Trial	O
(	O
RCT	O
)	O
and	O
Average	O
Treatment	O
Effect	O
(	O
ATE	O
)	O
from	O
Causal	B-MethodName
Inference	I-MethodName
(	O
Rubin	O
,	O
1974	O
;	O
Holland	O
,	O
1986	O
)	O
.	O
Estimation	O
of	O
perturbation	O
learnability	O
for	O
a	O
model	O
consists	O
of	O
three	O
steps	O
:	O
①	O
randomly	O
labelling	O
a	O
dataset	O
,	O
②	O
perturbing	O
examples	O
of	O
a	O
particular	O
pseudo	O
class	O
with	O
probabilities	O
,	O
and	O
③	O
using	O
ATE	O
to	O
measure	O
the	O
ease	O
with	O
which	O
the	O
model	O
learns	O
the	O
perturbation	O
.	O
The	O
core	O
intuition	O
for	O
our	O
method	O
is	O
to	O
frame	O
an	O
RCT	O
as	O
a	O
perturbation	O
identification	O
task	O
and	O
formalize	O
the	O
notion	O
of	O
learnability	O
Exp	O
No	O
.	O
Measurement	O

Perturbation	O
Training	O
Examples	O
Test	O
Examples	O
0	B-DatasetName
Standard	O
original	O
l	O
(	O
x	O
i	O
,	O
0	B-DatasetName
)	O
,	O
(	O
x	O
j	O
,	O
1	O
)	O
(	O
x	O
i	O
,	O
0	B-DatasetName
)	O
,	O
(	O
x	O
j	O
,	O
1	O
)	O
1	O
Robustness	O
original	O
l	O
{	O
0	B-DatasetName
,	O
1	O
}	O
(	O
x	O
i	O
,	O
0	B-DatasetName
)	O
,	O
(	O
x	O
j	O
,	O
1	O
)	O
(	O
x	O
*	O
i	O
,	O
0	B-DatasetName
)	O
,	O
(	O
x	O
*	O
j	O
,	O
1	O
)	O
2	O
Data	B-TaskName
Augmentation	I-TaskName
original	O
l	O
{	O
0	B-DatasetName
,	O
1	O
}	O
(	O
x	O
i	O
,	O
0	B-DatasetName
)	O
,	O
(	O
x	O
j	O
,	O
1	O
)	O
(	O
x	O
*	O
i	O
,	O
0	B-DatasetName
)	O
,	O
(	O
x	O
*	O
j	O
,	O
1	O
)	O
(	O
x	O
*	O
i	O
,	O
0	B-DatasetName
)	O
,	O
(	O
x	O
*	O
j	O
,	O
1	O
)	O
3	O
Learnability	O
random	O
l	O
′	O
{	O
1	O
′	O
}	O
(	O
x	O
j	O
,	O
0	B-DatasetName
′	O
)	O
,	O
(	O
x	O
*	O
i	O
,	O
1	O
′	O
)	O
(	O
x	O
*	O
i	O
,	O
1	O
′	O
)	O
4	O
random	O
l	O
′	O
{	O
1	O
′	O
}	O
(	O
x	O
j	O
,	O
0	B-DatasetName
′	O
)	O
,	O
(	O
x	O
*	O
i	O
,	O
1	O
′	O
)	O
(	O
x	O
i	O
,	O
1	O
′	O
)	O
Table	O
1	O
:	O
Example	O
experiment	O
settings	O
for	O
measuring	O
learnability	O
,	O
robustness	O
and	O
improvement	O
by	O
data	B-TaskName
augmentation	I-TaskName
.	O
We	O
perturb	O
an	O
example	O
if	O
its	O
label	O
falls	O
in	O
the	O
set	O
of	O
label	O
(	O
s	O
)	O
in	O
"	O
Perturbation	O
"	O
column	O
.	O
means	O
no	O
perturbation	O
at	O
all	O
.	O
Training	O
/	O
test	O
examples	O
are	O
the	O
expected	O
input	O
data	O
,	O
assuming	O
we	O
have	O
only	O
one	O
negative	O
(	O
x	O
i	O
,	O
0	B-DatasetName
)	O
and	O
positive	O
(	O
x	O
j	O
,	O
1	O
)	O
example	O
in	O
our	O
original	O
training	O
/	O
test	O
set	O
.	O
l	O
′	O
is	O
a	O
random	O
label	O
and	O
x	O
*	O
is	O
a	O
perturbed	O
example	O
.	O
as	O
a	O
causal	O
estimand	O
based	O
on	O
ATE	O
.	O
We	O
conduct	O
extensive	O
experiments	O
on	O
four	O
neural	O
NLP	O
models	O
with	O
eight	O
different	O
perturbations	O
across	O
three	O
datasets	O
and	O
find	O
strong	O
evidence	O
for	O
our	O
two	O
hypotheses	O
.	O
Combining	O
these	O
two	O
findings	O
,	O
we	O
further	O
show	O
that	O
data	B-TaskName
augmentation	I-TaskName
is	O
only	O
more	O
effective	O
at	O
improving	O
robustness	O
against	O
perturbations	O
that	O
a	O
model	O
is	O
more	O
sensitive	O
to	O
,	O
contributing	O
to	O
the	O
interpretation	O
of	O
robustness	O
and	O
data	B-TaskName
augmentation	I-TaskName
.	O
Learnability	O
provides	O
a	O
clean	O
setup	O
for	O
analysis	O
of	O
the	O
model	O
behaviour	O
under	O
perturbation	O
,	O
which	O
contributes	O
better	O
model	O
interpretation	O
as	O
well	O
.	O
Contribution	O
.	O
This	O
work	O
provides	O
an	O
empirical	O
explanation	O
for	O
why	O
NLP	O
models	O
are	O
less	O
robust	O
to	O
some	O
perturbations	O
than	O
others	O
.	O
The	O
key	O
to	O
this	O
question	O
is	O
perturbation	O
learnability	O
,	O
which	O
is	O
grounded	O
in	O
the	O
causality	O
framework	O
.	O
We	O
show	O
a	O
statistically	O
significant	O
inverse	O
correlation	O
between	O
learnability	O
and	O
robustness	O
.	O

With	O
the	O
above	O
-	O
defined	O
terminologies	O
,	O
we	O
propose	O
hypotheses	O
for	O
RQ1	O
and	O
RQ2	O
in	O
Section	O
1	O
,	O
respectively	O
.	O
Hypothesis	O
1	O
(	O
H1	O
)	O
:	O
A	O
model	O
for	O
which	O
a	O
perturbation	O
is	O
more	O
learnable	O
is	O
less	O
robust	O
against	O
the	O
same	O
perturbation	O
at	O
the	O
test	O
time	O
.	O
This	O
is	O
not	O
obvious	O
because	O
the	O
model	O
encounters	O
this	O
perturbation	O
during	O
training	O
in	O
learnability	O
estimation	O
while	O
they	O
do	O
not	O
in	O
robustness	O
measurement	O
.	O
Hypothesis	O
2	O
(	O
H2	O
)	O
:	O
A	O
model	O
for	O
which	O
a	O
perturbation	O
is	O
more	O
learnable	O
experiences	O
bigger	O
robustness	O
gains	O
with	O
data	B-TaskName
augmentation	I-TaskName
along	O
such	O
a	O
perturbation	O
.	O
We	O
validate	O
both	O
Hypotheses	O
1	O
and	O
2	O
with	O
experiments	O
on	O
several	O
perturbations	O
and	O
models	O
described	O
in	O
Section	O
4.1	O
and	O
4.2	O
.	O

In	O
Section	O
2.1	O
,	O
we	O
introduce	O
the	O
term	O
"	O
learnability	O
"	O
in	O
an	O
intuitive	O
way	O
.	O
Now	O
we	O
map	O
it	O
to	O
a	O
formal	O
,	O
quantitative	O
measure	O
in	O
standard	O
statistical	O
frameworks	O
.	O
Learnability	O
is	O
actually	O
motivated	O
by	O
concepts	O
from	O
the	O
causality	O
literature	O
.	O
We	O
provide	O
a	O
brief	O
introduction	O
to	O
basic	O
concepts	O
of	O
causal	B-MethodName
inference	I-MethodName
in	O
Appendix	O
B.	O
In	O
fact	O
,	O
learnability	O
is	O
the	O
causal	O
effect	O
of	O
perturbation	O
on	O
models	O
,	O
which	O
is	O
often	O
difficult	O
to	O
measure	O
due	O
to	O
the	O
confounding	O
latent	O
features	O
.	O
In	O
the	O
language	O
of	O
causality	O
,	O
this	O
is	O
"	O
correlation	O
is	O
not	O
causation	O
"	O
.	O
Causality	O
provides	O
insight	O
on	O
how	O
to	O
fully	O
decouple	O
the	O
effect	O
of	O
perturbation	O
and	O
other	O
latent	O
features	O
.	O
We	O
introduce	O
the	O
causal	O
motivations	O
for	O
step	O
2.1	O
and	O
2.1	O
of	O
learnability	O
estimation	O
in	O
the	O
following	O
Section	O
3.1	O
and	O
3.2	O
,	O
respectively	O
.	O

To	O
test	O
the	O
learnability	O
,	O
robustness	O
and	O
improvement	O
by	O
data	B-TaskName
augmentation	I-TaskName
with	O
different	O
NLP	O
models	O
and	O
perturbations	O
,	O
we	O
experiment	O
with	O
four	O
modern	O
and	O
representative	O
neural	O
NLP	O
models	O
:	O
TextRNN	O
(	O
Liu	O
et	O
al	O
,	O
2016	O
)	O
,	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2019b	O
)	O
and	O
XLNet	B-MethodName
(	O
Yang	O
et	O
al	O
,	O
2019	O
)	O
.	O
For	O
TextRNN	O
,	O
we	O
use	O
the	O
implementation	O
by	O
an	O
open	O
-	O
source	O
text	B-TaskName
classification	I-TaskName
toolkit	O
NeuralClassifier	O
(	O
Liu	O
et	O
al	O
,	O
2019a	O
)	O
.	O
For	O
the	O
other	O
three	O
pretrained	O
models	O
,	O
we	O
use	O
the	O
bert	O
-	O
base	O
-	O
cased	O
,	O
roberta	O
-	O
base	O
,	O
xlnet	B-MethodName
-	O
base	O
-	O
cased	O
versions	O
from	O
Hugging	O
Face	O
(	O
Wolf	O
et	O
al	O
,	O
2020	O
)	O
,	O
respectively	O
.	O
These	O
two	O
platforms	O
support	O
most	O
of	O
the	O
common	O
NLP	O
models	O
,	O
thus	O
facilitating	O
extension	O
studies	O
of	O
more	O
models	O
in	O
future	O
.	O
We	O
use	O
three	O
common	O
binary	O
text	B-TaskName
classification	I-TaskName
datasets	O
-	O
IMDB	B-DatasetName
movie	I-DatasetName
reviews	I-DatasetName
(	O
IMDB	B-DatasetName
)	O
(	O
Pang	O
and	O
Lee	O
,	O
2005	O
)	O
,	O
Yelp	O
polarity	O
reviews	O
(	O
YELP	O
)	O
(	O
Zhang	O
et	O
al	O
,	O
2015	O
)	O
,	O
Quora	O
Question	O
Pair	O
(	O
QQP	B-DatasetName
)	O
(	O
Iyer	O
et	O
al	O
,	O
2017	O
)	O
-	O
as	O
our	O
testbeds	O
.	O
IMDB	B-DatasetName
and	O
YELP	O
datasets	O
present	O
the	O
task	O
of	O
sentiment	B-TaskName
analysis	I-TaskName
,	O
where	O
each	O
sentence	O
is	O
labelled	O
as	O
positive	O
or	O
negative	O
sentiment	O
.	O
QQP	B-DatasetName
is	O
a	O
paraphrase	O
detection	O
task	O
,	O
where	O
each	O
pair	O
of	O
sentences	O
is	O
marked	O
as	O
semantically	O
equivalent	O
or	O
not	O
.	O
To	O
control	O
the	O
effect	O
of	O
dataset	O
size	O
and	O
imbalanced	O
classes	O
,	O
all	O
datasets	O
are	O
randomly	O
subsampled	O
to	O
the	O
same	O
size	O
as	O
IMDB	B-DatasetName
(	O
50k	O
)	O
with	O
balanced	O
classes	O
.	O
The	O
training	O
steps	O
for	O
all	O
experiments	O
are	O
the	O
same	O
as	O
well	O
.	O
We	O
implement	O
perturbations	O
g	O
(	O
⋅	O
)	O
with	O
two	O
self	O
-	O
designed	O
ones	O
and	O
six	O
selected	O
ones	O
from	O
the	O
NL	O
-	O
Augmenter	O
library	O
(	O
Dhole	O
et	O
al	O
,	O
2021	O
)	O
.	O
For	O
perturbation	O
probabilities	O
,	O
we	O
choose	O
0.001	O
,	O
0.005	O
,	O
0.01	O
,	O
0.02	O
,	O
0.05	O
,	O
0.10	O
,	O
0.50	O
,	O
1.00	O
.	O
We	O
run	O
all	O
experiments	O
across	O
three	O
random	O
seeds	B-DatasetName
and	O
report	O
the	O
average	O
results	O
.	O

Figure	O
3	O
shows	O
learnability	O
as	O
a	O
function	O
of	O
perturbation	O
probability	O
.	O
Learnability	O
@	O
p	O
generally	O
increases	O
as	O
we	O
increase	O
the	O
perturbation	O
probability	O
,	O
and	O
when	O
we	O
perturb	O
all	O
the	O
examples	O
(	O
i.e.	O
,	O
p	O
=	O
1.0	O
)	O
,	O
every	O
model	O
can	O
easily	O
identify	O
it	O
well	O
,	O
resulting	O
in	O
the	O
maximum	O
learnability	O
of	O
1.0	O
.	O
This	O
shows	O
that	O
neural	O
NLP	O
models	O
master	O
these	O
perturbations	O
eventually	O
.	O
At	O
lower	O
perturbation	O
probabilities	O
,	O
some	O
models	O
still	O
learn	O
that	O
perturbation	O
alone	O
predicts	O
the	O
label	O
.	O
In	O
fact	O
,	O
the	O
major	O
difference	O
between	O
different	O
p	O
−	O
learnability	O
curves	O
is	O
the	O
area	O
of	O
lower	O
perturbation	O
probabilities	O
and	O
this	O
provides	O
motivation	O
for	O
using	O
log	O
AU	O
C	O
instead	O
of	O
AU	O
C	O
as	O
the	O
summarization	B-TaskName
of	O
learnability	O
at	O
different	O
p	O
(	O
Section	O
2.1	O
)	O
.	O
Table	O
2	O
shows	O
the	O
average	O
learnability	O
over	O
all	O
perturbation	O
probabilities	O
of	O
each	O
modelperturbation	O
pair	O
on	O
IMDB	B-DatasetName
dataset	O
in	O
Figure	O
3	O
.	O
4	O
It	O
reveals	O
the	O
most	O
learnable	O
perturbation	O
for	O
each	O
model	O
.	O
For	O
example	O
,	O
the	O
learnability	O
of	O
"	O
vi	O
-	O
sual_attack_letters	O
"	O
and	O
"	O
leet_letters	O
"	O
are	O
very	O
high	O
for	O
all	O
four	O
models	O
,	O
likely	O
due	O
to	O
their	O
strong	O
effects	O
on	O
the	O
tokenization	O
process	O
(	O
Salesky	O
et	O
al	O
,	O
2021	O
)	O
.	O
Perturbations	O
like	O
"	O
white_space_perturbation	O
"	O
and	O
"	O
duplicate_punctuations	O
"	O
are	O
less	O
learnable	O
for	O
pretrained	O
models	O
,	O
probably	O
because	O
they	O
have	O
weaker	O
effects	O
on	O
the	O
subword	O
level	O
tokenization	O
,	O
or	O
they	O
may	O
have	O
encountered	O
similar	O
noise	O
in	O
the	O
pretraining	O
corpora	O
.	O
We	O
observe	O
that	O
"	O
dupli	O
-	O
cate_punctuations	O
"	O
already	O
exists	O
in	O
the	O
original	O
text	O
of	O
YELP	O
dataset	O
(	O
e.g.	O
,	O
"	O
The	O
burgers	O
are	O
awesome	O
!	O
!	O
"	O
)	O
,	O
thus	O
violating	O
our	O
assumptions	O
for	O
perturbations	O
in	O
Section	O
4.1	O
.	O
As	O
a	O
result	O
,	O
the	O
curve	O
for	O
4	O
Please	O
refer	O
to	O
Appendix	O
E	O
for	O
benchmark	O
results	O
on	O
YELP	O
(	O
Table	O
5	O
)	O
and	O
QQP	B-DatasetName
(	O
this	O
perturbation	O
substantially	O
deviates	O
from	O
others	O
in	O
Figure	O
3	O
.	O
We	O
do	O
not	O
count	O
this	O
perturbation	O
on	O
YELP	O
dataset	O
in	O
the	O
following	O
analysis	O
.	O
The	O
perturbation	O
learnability	O
experiments	O
provide	O
a	O
clean	O
setup	O
for	O
NLP	O
practitioners	O
to	O
analyze	O
the	O
effect	O
of	O
textual	O
perturbations	O
on	O
models	O
.	O

We	O
observe	O
a	O
negative	O
correlation	O
between	O
learnability	O
(	O
Equation	O
4	O
)	O
and	O
robustness	O
(	O
Equation	O
1	O
)	O
across	O
all	O
three	O
datasets	O
in	O
Table	O
2	O
,	O
validating	O
Hypothesis	O
1	O
.	O
Table	O
2	O
also	O
quantifies	O
the	O
trend	O
that	O
data	B-TaskName
augmentation	I-TaskName
with	O
a	O
perturbation	O
the	O
model	O
is	O
less	O
robust	O
to	O
has	O
more	O
improvement	O
on	O
robustness	O
(	O
Hypothesis	O
2	O
)	O
.	O
We	O
plot	O
the	O
correlations	O
on	O
IMDB	B-DatasetName
dataset	O
in	O
Figure	O
4a	O
and	O
4b	O
.	O
5	O
Both	O
the	O
correlations	O
between	O
1	O
)	O
learnability	O
vs.	O
robustness	O
and	O
2	O
)	O
learnability	O
vs.	O
improvement	O
by	O
data	B-TaskName
augmentation	I-TaskName
are	O
strong	O
(	O
Spearman	O
|	O
ρ	O
|	O
>	O
0.6	O
)	O
and	O
highly	O
significant	O
(	O
p	O
-	O
value	O
<	O
0.001	O
)	O
,	O
which	O
firmly	O
supports	O
our	O
hypotheses	O
.	O
Our	O
findings	O
provide	O
insight	O
about	O
when	O
the	O
model	O
is	O
less	O
robust	O
and	O
when	O
data	B-TaskName
augmentation	I-TaskName
works	O
better	O
for	O
improving	O
robustness	O
.	O
Figure	O
4c	O
shows	O
that	O
the	O
more	O
learnable	O
a	O
perturbation	O
is	O
for	O
a	O
model	O
,	O
the	O
greater	O
the	O
likelihood	O
that	O
its	O
robustness	O
can	O
be	O
improved	O
through	O
data	B-TaskName
augmentation	I-TaskName
along	O
this	O
perturbation	O
.	O
We	O
argue	O
that	O
this	O
is	O
not	O
simply	O
because	O
there	O
is	O
more	O
room	O
for	O
improvement	O
by	O
data	B-TaskName
augmentation	I-TaskName
.	O
From	O
a	O
causal	O
perspective	O
,	O
learnability	O
acts	O
as	O
a	O
common	O
cause	O
(	O
confounder	O
)	O
for	O
both	O
robustness	O
and	O
improvement	O
by	O
data	B-TaskName
augmentation	I-TaskName
.	O
This	O
indicates	O
a	O
potential	O
limitation	O
of	O
using	O
data	B-TaskName
augmentation	I-TaskName
for	O
improving	O
robustness	O
to	O
perturbations	O
:	O
data	B-TaskName
augmentation	I-TaskName
is	O
only	O
more	O
effective	O
at	O
improving	O
robustness	O
against	O
perturbations	O
more	O
learnable	O
for	O
a	O
model	O
.	O

Robustness	O
of	O
NLP	O
Models	O
to	O
Perturbations	O
.	O
The	O
performance	O
of	O
NLP	O
models	O
can	O
decrease	O
when	O
encountering	O
noisy	O
data	O
in	O
the	O
real	O
world	O
.	O
Recent	O
works	O
(	O
Prabhakaran	O
et	O
al	O
,	O
2019	O
;	O
Ribeiro	O
et	O
al	O
,	O
2020	O
;	O
Niu	O
et	O
al	O
,	O
2020	O
;	O
Moradi	O
and	O
Samwald	O
,	O
2021	O
)	O
present	O
comprehensive	O
evaluations	O
of	O
the	O
robustness	O
of	O
NLP	O
models	O
to	O
different	O
types	O
of	O
perturbations	O
,	O
including	O
typos	O
,	O
changed	O
entities	O
,	O
negation	O
,	O
etc	O
.	O
Their	O
results	O
reveal	O
the	O
phenomenon	O
that	O
NLP	O
models	O
can	O
handle	O
some	O
specific	O
types	O
of	O
perturbation	O
more	O
effectively	O
than	O
others	O
.	O
However	O
,	O
they	O
do	O
not	O
go	O
into	O
a	O
deeper	O
analysis	O
of	O
the	O
reason	O
behind	O
the	O
difference	O
of	O
robustness	O
between	O
models	O
and	O
perturbations	O
.	O
Interpretation	O
of	O
Data	B-TaskName
Augmentation	I-TaskName
.	O
Although	O
data	B-TaskName
augmentation	I-TaskName
has	O
been	O
widely	O
used	O
in	O
CV	O
(	O
Sato	O
et	O
al	O
,	O
2015	O
;	O
DeVries	O
and	O
Taylor	O
,	O
2017	O
;	O
Dwibedi	O
et	O
al	O
,	O
2017	O
)	O
and	O
NLP	O
(	O
Wang	O
and	O
Yang	O
,	O
2015	O
;	O
Kobayashi	O
,	O
2018	O
;	O
Wei	O
and	O
Zou	O
,	O
2019	O
)	O
,	O
the	O
underlying	O
mechanism	O
of	O
its	O
effectiveness	O
remains	O
under	O
-	O
researched	O
.	O
Recent	O
studies	O
aim	O
to	O
quantify	O
intuitions	O
of	O
how	O
data	B-TaskName
augmentation	I-TaskName
improves	O
model	O
generalization	O
.	O
Gontijo	O
-	O
Lopes	O
et	O
al	O
(	O
2020	O
)	O
introduce	O
affinity	O
and	O
diversity	O
,	O
and	O
find	O
a	O
correlation	O
between	O
the	O
two	O
metrics	O
and	O
augmentation	O
performance	O
in	O
image	B-TaskName
classification	I-TaskName
.	O
In	O
NLP	O
,	O
Kashefi	O
and	O
Hwa	O
(	O
2020	O
)	O
propose	O
a	O
KL	O
-	O
divergence	O
-	O
based	O
metric	O
to	O
predict	O
augmentation	O
performance	O
.	O
Our	O
proposed	O
learnability	O
metric	O
implies	O
when	O
data	B-TaskName
augmentation	I-TaskName
works	O
better	O
and	O
thus	O
acts	O
as	O
a	O
complement	O
to	O
this	O
line	O
of	O
research	O
.	O

The	O
aim	O
of	O
causal	B-MethodName
inference	I-MethodName
is	O
to	O
investigate	O
how	O
a	O
treatment	O
T	O
affects	O
the	O
outcome	O
Y	O
.	O
Confounder	O
X	O
refers	O
to	O
a	O
variable	O
that	O
influences	O
both	O
treatment	O
T	O
and	O
outcome	O
Y	O
.	O
For	O
example	O
,	O
sleeping	O
with	O
shoes	O
on	O
(	O
T	O
)	O
is	O
strongly	O
associated	O
with	O
waking	O
up	O
with	O
a	O
headache	O
(	O
Y	O
)	O
,	O
but	O
they	O
both	O
have	O
a	O
common	O
cause	O
:	O
drinking	O
the	O
night	O
before	O
(	O
X	O
)	O
(	O
Neal	O
,	O
2020	O
)	O
.	O
In	O
our	O
work	O
,	O
we	O
aim	O
to	O
study	O
how	O
a	O
perturbation	O
(	O
treatment	O
)	O
affects	O
the	O
model	O
's	O
prediction	O
(	O
outcome	O
)	O
.	O
However	O
,	O
the	O
latent	O
features	O
and	O
other	O
noise	O
usually	O
act	O
as	O
confounders	O
.	O
Causality	O
offers	O
solutions	O
for	O
two	O
questions	O
:	O
1	O
)	O
how	O
to	O
eliminate	O
the	O
spurious	O
association	O
and	O
isolate	O
the	O
treatment	O
's	O
causal	O
effect	O
;	O
and	O
2	O
)	O
how	O
varying	O
T	O
affects	O
Y	O
,	O
given	O
both	O
variables	O
are	O
causallyrelated	O
.	O
We	O
leverage	O
both	O
of	O
these	O
properties	O
in	O
our	O
proposed	O
method	O
.	O
Let	O
us	O
now	O
introduce	O
Randomized	O
Controlled	O
Trial	O
and	O
Average	O
Treatment	O
Effect	O
as	O
key	O
concepts	O
in	O
answering	O
the	O
above	O
two	O
questions	O
,	O
respectively	O
.	O
Randomized	O
Controlled	O
Trial	O
(	O
RCT	O
)	O
.	O
In	O
an	O
RCT	O
,	O
each	O
participant	O
is	O
randomly	O
assigned	O
to	O
either	O
the	O
treatment	O
group	O
or	O
the	O
non	O
-	O
treatment	O
group	O
.	O
In	O
this	O
way	O
,	O
the	O
only	O
difference	O
between	O
the	O
two	O
groups	O
is	O
the	O
treatment	O
they	O
receive	O
.	O
Randomized	O
experiments	O
ideally	O
guarantee	O
that	O
there	O
is	O
no	O
confounding	O
factor	O
,	O
and	O
thus	O
any	O
observed	O
association	O
is	O
actually	O
causal	O
.	O
We	O
operationalize	O
RCT	O
as	O
a	O
perturbation	O
classification	O
task	O
in	O
Section	O
3.1	O
.	O
Average	O
Treatment	O
Effect	O
(	O
ATE	O
)	O
.	O
In	O
Section	O
3.2	O
,	O
we	O
apply	O
ATE	O
(	O
Holland	O
,	O
1986	O
)	O
as	O
a	O
measure	O
of	O
learnability	O
.	O
ATE	O
is	O
based	O
on	O
Individual	O
Treatment	O
Effect	O
(	O
ITE	O
,	O
Equation	O
9	O
)	O
,	O
which	O
is	O
the	O
difference	O
of	O
the	O
outcome	O
with	O
and	O
without	O
treatment	O
.	O
IT	O
E	O
i	O
=	O
Y	O
i	O
(	O
1	O
)	O
−	O
Y	O
i	O
(	O
0	B-DatasetName
)	O
.	O
(	O
9	O
)	O
Here	O
,	O
Y	O
i	O
(	O
1	O
)	O
is	O
the	O
outcome	O
Y	O
of	O
individual	O
i	O
that	O
receives	O
treatment	O
(	O
T	O
=	O
1	O
)	O
,	O
while	O
Y	O
i	O
(	O
0	B-DatasetName
)	O
is	O
the	O
opposite	O
.	O
In	O
the	O
above	O
example	O
,	O
waking	O
up	O
with	O
a	O
headache	O
(	O
Y	O
=	O
1	O
)	O
with	O
shoes	O
on	O
(	O
T	O
=	O
1	O
)	O
means	O
Y	O
i	O
(	O
1	O
)	O
=	O
1	O
.	O
We	O
calculate	O
the	O
Average	O
Treatment	O
Effect	O
(	O
ATE	O
)	O
by	O
taking	O
an	O
average	O
over	O
ITEs	O
:	O
AT	O
E	O
=	O
E	O
[	O
Y	O
(	O
1	O
)	O
]	O
−	O
E	O
[	O
Y	O
(	O
0	B-DatasetName
)	O
]	O
.	O
(	O
10	O
)	O
ATE	O
quantifies	O
how	O
the	O
outcome	O
Y	O
is	O
expected	O
to	O
change	O
if	O
we	O
modify	O
the	O
treatment	O
T	O
from	O
0	B-DatasetName
to	O
1	O
.	O
We	O
provide	O
specific	O
definitions	O
of	O
ITE	O
and	O
ATE	O
in	O
Section	O
3.2	O
.	O

Explanation	O
Graph	B-TaskName
Generation	I-TaskName
via	O
Pre	O
-	O
trained	O
Language	O
Models	O
:	O
An	O
Empirical	O
Study	O
with	O
Contrastive	B-MethodName
Learning	I-MethodName

Graph	B-TaskName
Generation	I-TaskName
from	O
Language	O
Models	O
.	O
Representative	O
works	O
on	O
graph	B-TaskName
generation	I-TaskName
from	O
language	O
models	O
include	O
knowledge	B-TaskName
graph	I-TaskName
completion	I-TaskName
models	O
like	O
Comet	O
Hwang	O
et	O
al	O
,	O
2021	O
)	O
that	O
fine	O
-	O
tune	O
GPT	B-MethodName
(	O
Radford	O
et	O
al	O
,	O
2019	O
;	O
Brown	O
et	O
al	O
,	O
2020	O
)	O
and	O
BART	B-MethodName
(	O
Lewis	O
et	O
al	O
,	O
2020	O
)	O
,	O
generation	O
of	O
event	O
influence	O
graphs	O
(	O
Tandon	O
et	O
al	O
,	O
2019	O
;	O
Madaan	O
et	O
al	O
,	O
2020	O
)	O
,	O
partially	O
ordered	O
scripts	O
(	O
Sakaguchi	O
et	O
al	O
,	O
2021	O
)	O
,	O
temporal	O
graphs	O
(	O
Madaan	O
and	O
Yang	O
,	O
2021	O
)	O
,	O
entailment	O
trees	O
,	O
proof	O
graphs	O
(	O
Saha	O
et	O
al	O
,	O
2020	O
;	O
Saha	O
et	O
al	O
,	O
2021a	O
)	O
and	O
commonsense	O
explanation	O
graphs	O
(	O
Saha	O
et	O
al	O
,	O
2021b	O
)	O
.	O
Linguistic	O
tasks	O
like	O
syntactic	O
parsing	O
Mohammadshahi	O
and	O
Henderson	O
,	O
2021	O
;	O
Kondratyuk	O
and	O
Straka	O
,	O
2019	O
)	O
and	O
semantic	B-TaskName
parsing	I-TaskName
(	O
Chen	O
et	O
al	O
,	O
2020b	O
;	O
Shin	O
et	O
al	O
,	O
2021	O
)	O
have	O
also	O
made	O
use	O
of	O
language	O
models	O
.	O
There	O
is	O
also	O
a	O
large	O
body	O
of	O
work	O
on	O
building	O
generative	O
models	O
for	O
learning	O
unconditional	O
graph	O
distributions	O
(	O
You	O
et	O
al	O
,	O
2018	O
;	O
Simonovsky	O
and	O
Komodakis	O
,	O
2018	O
;	O
Grover	O
et	O
al	O
,	O
2019	O
;	O
Liao	O
et	O
al	O
,	O
2019	O
;	O
Shi	O
*	O
et	O
al	O
,	O
2020	O
)	O
without	O
any	O
semantics	O
attached	O
to	O
the	O
graphs	O
.	O
Our	O
novelty	O
lies	O
in	O
presenting	O
the	O
first	O
systematic	O
analysis	O
of	O
structure	O
and	O
semantics	O
of	O
graph	B-TaskName
generation	I-TaskName
for	O
two	O
downstream	O
NLP	O
tasks	O
using	O
pre	O
-	O
trained	O
language	O
models	O
and	O
improving	O
them	O
via	O
constrastive	O
learning	O
.	O
Data	B-TaskName
Augmentation	I-TaskName
and	O
Contrastive	B-MethodName
Learning	I-MethodName
.	O
Data	B-TaskName
Augmentation	I-TaskName
for	O
NLP	O
(	O
Hedderich	O
et	O
al	O
,	O
2020	O
;	O
has	O
been	O
a	O
powerful	O
tool	O
in	O
low	O
-	O
data	O
settings	O
,	O
ranging	O
from	O
its	O
early	O
usages	O
with	O
synonym	O
replacement	O
(	O
Kolomiyets	O
et	O
al	O
,	O
2011	O
;	O
Wang	O
and	O
Yang	O
,	O
2015	O
)	O
to	O
more	O
recent	O
methods	O
of	O
perturbing	O
hidden	O
representations	O
(	O
Miyato	O
et	O
al	O
,	O
2016	O
;	O
.	O
Contrastive	B-MethodName
learning	I-MethodName
,	O
beyond	O
its	O
historical	O
use	O
in	O
learning	O
robust	O
image	O
representations	O
(	O
Chopra	O
et	O
al	O
,	O
2005	O
;	O
Hadsell	O
et	O
al	O
,	O
2006	O
;	O
Gutmann	O
and	O
Hyvärinen	O
,	O
2010	O
;	O
Hoffer	O
and	O
Ailon	O
,	O
2015	O
;	O
Hjelm	O
et	O
al	O
,	O
2018	O
;	O
Chen	O
et	O
al	O
,	O
2020a	O
;	O
He	O
et	O
al	O
,	O
2020	O
)	O
has	O
been	O
explored	O
in	O
supervised	O
scenarios	O
(	O
Khosla	O
et	O
al	O
,	O
2020	O
;	O
Gunel	O
et	O
al	O
,	O
2020	O
)	O
and	O
for	O
NLP	O
,	O
in	O
training	O
self	O
-	O
supervised	O
language	O
models	O
(	O
Fang	O
et	O
al	O
,	O
2020	O
)	O
,	O
learning	O
sentence	O
representations	O
(	O
Gao	O
et	O
al	O
,	O
2021	O
)	O
,	O
document	O
clustering	O
,	O
summarization	B-TaskName
Cao	O
and	O
Wang	O
,	O
2021	O
)	O
and	O
generic	O
text	B-TaskName
generation	I-TaskName
.	O
It	O
has	O
also	O
been	O
used	O
in	O
unconditional	O
graph	B-TaskName
representation	I-TaskName
learning	I-TaskName
(	O
You	O
et	O
al	O
,	O
2020	O
;	O
Hassani	O
and	O
Khasahmadi	O
,	O
2020	O
;	O
.	O
We	O
follow	O
this	O
rich	O
line	O
of	O
work	O
to	O
explore	O
their	O
applicability	O
in	O
supervised	O
graph	B-TaskName
generation	I-TaskName
tasks	O
from	O
pretrained	B-TaskName
language	I-TaskName
models	I-TaskName
in	O
low	O
-	O
resource	O
settings	O
.	O
Generative	O
Commonsense	O
Reasoning	O
.	O
While	O
traditional	O
commonsense	O
reasoning	O
tasks	O
are	O
discriminative	O
in	O
nature	O
(	O
Zellers	O
et	O
al	O
,	O
2018	O
;	O
Talmor	O
et	O
al	O
,	O
2019	O
;	O
Bisk	O
et	O
al	O
,	O
2020	O
;	O
Sakaguchi	O
et	O
al	O
,	O
2020	O
;	O
Talmor	O
et	O
al	O
,	O
2021	O
)	O
,	O
recent	O
focus	O
on	O
generative	O
evaluation	O
have	O
led	O
to	O
the	O
development	O
of	O
tasks	O
and	O
benchmarks	O
that	O
explore	O
unstructured	O
commonsense	O
sentence	O
generation	O
(	O
Lin	O
et	O
al	O
,	O
2020	O
)	O
,	O
event	O
influence	O
graph	B-TaskName
generation	I-TaskName
(	O
Madaan	O
et	O
al	O
,	O
2020	O
)	O
,	O
commonsense	O
explanation	O
graph	B-TaskName
generation	I-TaskName
(	O
Saha	O
et	O
al	O
,	O
2021b	O
)	O
,	O
etc	O
.	O
We	O
experiment	O
with	O
two	O
graph	B-TaskName
generation	I-TaskName
tasks	O
,	O
primarily	O
focusing	O
on	O
ExplaGraphs	O
(	O
Saha	O
et	O
al	O
,	O
2021b	O
)	O
because	O
of	O
the	O
clear	O
distinction	O
in	O
the	O
underlying	O
structural	O
constraints	O
and	O
the	O
semantic	O
aspect	O
dealing	O
with	O
commonsense	O
.	O

Most	O
prior	O
works	O
that	O
collect	O
human	O
-	O
annotated	O
graphs	O
for	O
a	O
downstream	O
NLP	O
task	O
have	O
found	O
such	O
collection	O
processes	O
to	O
be	O
quite	O
expensive	O
and	O
tedious	O
(	O
Tandon	O
et	O
al	O
,	O
2019	O
;	O
Saha	O
et	O
al	O
,	O
2021b	O
)	O
.	O
For	O
instance	O
,	O
Saha	O
et	O
al	O
(	O
2021b	O
)	O
obtained	O
high	O
-	O
quality	O
data	O
only	O
after	O
multiple	O
rounds	O
of	O
refinement	O
and	O
employ	O
trained	O
expert	O
annotators	O
for	O
entailment	O
tree	O
construction	O
.	O
The	O
corresponding	O
datasets	O
are	O
also	O
relatively	O
small	O
in	O
size	O
(	O
2	O
-	O
3k	O
)	O
,	O
thus	O
limiting	O
the	O
prospect	O
of	O
large	O
-	O
scale	O
training	O
.	O
Hence	O
,	O
our	O
approach	O
towards	O
improving	O
explanation	O
graph	B-TaskName
generation	I-TaskName
is	O
through	O
data	B-TaskName
augmentation	I-TaskName
techniques	O
that	O
perturb	O
human	O
-	O
curated	O
graphs	O
to	O
construct	O
positive	O
and	O
negative	O
graphs	O
.	O
As	O
noted	O
earlier	O
,	O
we	O
wish	O
to	O
construct	O
graphs	O
that	O
enable	O
better	O
learning	O
of	O
structural	O
graph	O
constraints	O
and	O
their	O
semantics	O
.	O

Next	O
we	O
propose	O
different	O
methods	O
of	O
leveraging	O
these	O
positive	O
and	O
negative	O
graphs	O
for	O
explanation	O
graph	B-TaskName
generation	I-TaskName
.	O
Our	O
models	O
either	O
use	O
only	O
positive	O
graphs	O
as	O
simple	O
data	B-TaskName
augmentation	I-TaskName
,	O
only	O
negative	O
graphs	O
in	O
a	O
max	O
-	O
margin	O
model	O
,	O
or	O
both	O
in	O
a	O
Generate	O
&	O
Refine	O
model	O
and	O
a	O
Contrastive	O
model	O
.	O

In	O
this	O
first	O
simple	O
approach	O
,	O
we	O
augment	O
the	O
training	O
data	O
with	O
the	O
synthetically	O
created	O
positive	O
graphs	O
and	O
retrain	O
the	O
baseline	O
T5	B-MethodName
model	O
.	O

ExplaGraphs	O
was	O
constructed	O
using	O
a	O
"	O
Refinement	O
"	O
phase	O
wherein	O
the	O
initially	O
constructed	O
graphs	O
that	O
are	O
marked	O
incorrect	O
by	O
human	O
verifiers	O
are	O
further	O
refined	O
by	O
another	O
set	O
of	O
annotators	O
.	O
Here	O
we	O
emulate	O
the	O
graph	O
refinement	O
phase	O
with	O
the	O
help	O
of	O
a	O
model	O
.	O
Specifically	O
,	O
our	O
approach	O
is	O
a	O
2	O
-	O
stage	O
pipeline	O
-	O
first	O
,	O
an	O
initial	O
graph	O
is	O
generated	O
by	O
the	O
baseline	O
T5	B-MethodName
model	O
and	O
second	O
,	O
an	O
Explanation	O
Graph	O
Refinement	O
model	O
conditions	O
on	O
the	O
initial	O
graph	O
,	O
along	O
with	O
the	O
belief	O
,	O
argument	O
and	O
the	O
stance	O
to	O
refine	O
the	O
graph	O
.	O
The	O
refiner	O
is	O
also	O
a	O
T5	B-MethodName
model	O
fine	O
-	O
tuned	O
with	O
the	O
prefix	O
"	O
Refine	O
the	O
Explanation	O
Graph	O
for	O
"	O
on	O
all	O
positive	O
and	O
negative	O
graphs	O
described	O
in	O
Sec	O
.	O
4	O
.	O
Note	O
that	O
our	O
approach	O
differs	O
from	O
the	O
actual	O
data	O
collection	O
process	O
in	O
two	O
aspects	O
.	O
Unlike	O
the	O
human	O
-	O
annotated	O
graphs	O
,	O
which	O
are	O
refined	O
only	O
for	O
semantic	O
correctness	O
,	O
the	O
model	O
-	O
generated	O
graphs	O
can	O
be	O
both	O
structurally	O
and	O
semantically	O
incorrect	O
.	O
Second	O
,	O
our	O
approach	O
does	O
not	O
involve	O
a	O
graph	O
verification	O
stage	O
and	O
thus	O
,	O
the	O
refiner	O
model	O
acts	O
on	O
all	O
(	O
correct	O
and	O
incorrect	O
)	O
graphs	O
generated	O
in	O
stage	O
1	O
and	O
is	O
thus	O
trained	O
with	O
both	O
correct	O
and	O
incorrect	O
graphs	O
.	O

Automatically	O
evaluating	O
graphs	O
for	O
semantic	O
correctness	O
is	O
challenging	O
.	O
We	O
conduct	O
human	O
evaluation	O
to	O
further	O
validate	O
our	O
findings	O
.	O
We	O
compare	O
the	O
graphs	O
generated	O
by	O
T5	B-MethodName
and	O
our	O
Max	O
-	O
Margin	O
model	O
on	O
Amazon	O
Mechanical	O
Turk	O
where	O
three	O
annotators	O
choose	O
which	O
graph	O
is	O
better	O
or	O
if	O
they	O
are	O
mostly	O
similar	O
(	O
instructions	O
in	O
Appendix	O
F	O
)	O
.	O
For	O
fair	O
comparison	O
,	O
we	O
evaluate	O
only	O
those	O
samples	O
where	O
both	O
models	O
predict	O
the	O
correct	O
stance	O
and	O
the	O
graphs	O
are	O
also	O
structurally	O
correct	O
.	O
In	O
fact	O
,	O
this	O
lets	O
us	O
evaluate	O
the	O
semantic	O
aspect	O
in	O
isolation	O
when	O
both	O
graphs	O
are	O
structurally	O
correct	O
.	O
With	O
majority	O
voting	O
on	O
150	O
samples	O
,	O
we	O
observe	O
that	O
our	O
Max	O
-	O
Margin	O
model	O
's	O
graphs	O
are	O
preferred	O
13	O
%	O
more	O
times	O
compared	O
to	O
those	O
of	O
the	O
T5	B-MethodName
model	O
(	O
43	O
%	O
vs	O
30	O
%	O
and	O
statistically	O
significant	O
with	O
p	O
<	O
0.05	O
)	O
while	O
in	O
22	O
%	O
cases	O
,	O
the	O
graphs	O
are	O
marked	O
similar	O
(	O
remaining	O
have	O
no	O
majority	O
)	O
.	O

In	O
ness	O
and	O
diversity	O
in	O
these	O
graphs	O
and	O
hence	O
are	O
the	O
best	O
candidates	O
for	O
contrastive	B-MethodName
learning	I-MethodName
.	O

We	O
presented	O
an	O
empirical	O
study	O
of	O
graph	O
structure	O
and	O
semantics	O
for	O
end	O
-	O
to	O
-	O
end	O
explanation	O
graph	B-TaskName
generation	I-TaskName
from	O
pre	O
-	O
trained	O
language	O
models	O
and	O
showed	O
that	O
the	O
generated	O
graphs	O
often	O
violate	O
structural	O
constraints	O
or	O
are	O
semantically	O
incorrect	O
.	O
We	O
significantly	O
improve	O
both	O
the	O
structural	O
and	O
semantic	O
accuracy	B-MetricName
of	O
graph	B-TaskName
generation	I-TaskName
by	O
proposing	O
contrastive	B-MethodName
learning	I-MethodName
models	O
that	O
leverage	O
simple	O
yet	O
efficient	O
methods	O
of	O
graph	O
perturbations	O
and	O
also	O
generalize	O
to	O
similar	O
graph	B-TaskName
generation	I-TaskName
tasks	O
.	O

The	O
task	O
of	O
temporal	O
graph	B-TaskName
generation	I-TaskName
requires	O
constructing	O
a	O
temporal	O
graph	O
from	O
a	O
document	O
(	O
see	O
Fig	O
.	O
4	O
)	O
.	O
The	O
nodes	O
in	O
the	O
graph	O
are	O
events	O
from	O
the	O
7	O
:	O
Train	O
,	O
validation	O
and	O
test	O
split	O
sizes	O
of	O
the	O
two	O
datasets	O
.	O
For	O
Temporal	O
Graph	B-TaskName
Generation	I-TaskName
,	O
we	O
randomly	O
sample	O
1.3	O
%	O
of	O
the	O
overall	O
corpus	O
(	O
Madaan	O
and	O
Yang	O
,	O
2021	O
)	O
.	O
document	O
(	O
e.g.	O
,	O
"	O
Markovic	O
jailed	O
"	O
or	O
"	O
Covering	O
up	O
attempted	O
murder	O
"	O
)	O
and	O
the	O
edges	O
are	O
temporal	O
relations	O
between	O
the	O
events	O
(	O
e.g.	O
,	O
"	O
Markovic	O
jailed	O
;	O
before	O
;	O
Covering	O
up	O
attempted	O
murder	O
"	O
)	O
.	O
The	O
authors	O
consider	O
five	O
temporal	O
relations	O
(	O
"	O
before	O
"	O
,	O
"	O
after	O
"	O
,	O
"	O
simultaneous	O
"	O
,	O
"	O
is	O
included	O
"	O
and	O
"	O
includes	O
"	O
)	O
and	O
build	O
an	O
automatically	O
constructed	O
large	O
-	O
scale	O
dataset	O
for	O
the	O
task	O
.	O
Following	O
our	O
overall	O
goal	O
of	O
improving	O
graph	B-TaskName
generation	I-TaskName
in	O
limited	O
data	O
settings	O
,	O
we	O
randomly	O
sample	O
1.3	O
%	O
of	O
the	O
overall	O
corpus	O
(	O
∼	O
9.5k	O
samples	O
)	O
as	O
the	O
training	O
corpus	O
such	O
that	O
all	O
graphs	O
are	O
connected	O
DAGs	O
.	O
5	O
Following	O
Madaan	O
and	O
Yang	O
(	O
2021	O
)	O
,	O
we	O
represent	O
graphs	O
in	O
DOT	O
format	O
(	O
Koutsofios	O
and	O
North	O
,	O
1996	O
)	O
as	O
shown	O
in	O
Fig	O
.	O
4	O
.	O
We	O
find	O
that	O
the	O
specifics	O
of	O
the	O
graph	O
representations	O
do	O
not	O
matter	O
much	O
,	O
as	O
long	O
as	O
all	O
the	O
edges	O
are	O
concatenated	O
in	O
one	O
particular	O
ordering	O
(	O
either	O
DFS	O
,	O
BFS	O
or	O
Topological	O
order	O
)	O
.	O
We	O
construct	O
semantic	O
negative	O
graphs	O
by	O
randomly	O
sampling	O
a	O
fraction	O
of	O
the	O
edges	O
and	O
performing	O
the	O
following	O
operations	O
.	O
If	O
an	O
edge	O
relation	O
is	O
one	O
of	O
"	O
before	O
"	O
,	O
"	O
after	O
"	O
or	O
"	O
simulatenous	O
"	O
,	O
we	O
replace	O
it	O
with	O
any	O
other	O
relation	O
from	O
this	O
set	O
and	O
if	O
the	O
relation	O
is	O
one	O
of	O
"	O
is	O
included	O
"	O
or	O
"	O
includes	O
"	O
we	O
replace	O
it	O
with	O
the	O
other	O
relation	O
.	O
Note	O
that	O
these	O
perturbations	O
will	O
always	O
lead	O
to	O
incorrect	O
graphs	O
because	O
"	O
A	O
before	O
B	O
"	O
implies	O
that	O
"	O
A	O
after	O
B	O
"	O
or	O
"	O
A	O
simultaneous	O
B	O
"	O
do	O
not	O
hold	O
.	O
Finally	O
,	O
we	O
construct	O
positive	O
graphs	O
by	O
randomly	O
sampling	O
a	O
fraction	O
of	O
edges	O
and	O
replacing	O
them	O
using	O
the	O
following	O
rules	O
:	O
(	O
1	O
)	O
"	O
A	O
before	O
B	O
"	O
with	O
"	O
B	O
after	O
A	O
"	O
and	O
viseversa	O
,	O
(	O
2	O
)	O
"	O
A	O
simultaneous	O
B	O
"	O
with	O
"	O
B	O
simultaneous	O
A	O
"	O
,	O
(	O
3	O
)	O
"	O
A	O
includes	O
B	O
"	O
with	O
"	O
B	O
is	O
included	O
A	O
"	O
.	O
Note	O
that	O
all	O
these	O
operations	O
preserve	O
the	O
temporal	O
meaning	O
of	O
the	O
graph	O
and	O
are	O
done	O
in	O
a	O
way	O
such	O
that	O
the	O
perturbed	O
graph	O
continues	O
to	O
be	O
a	O
connected	O
DAG	O
.	O

Table	O
8	O
shows	O
the	O
results	O
of	O
all	O
models	O
on	O
the	O
Ex	O
-	O
plaGraphs	O
(	O
Saha	O
et	O
al	O
,	O
2021b	O
)	O
(	O
Madaan	O
et	O
al	O
,	O
2020	O
)	O
showing	O
the	O
source	O
document	O
,	O
the	O
target	O
temporal	O
graph	O
and	O
the	O
corresponding	O
DOT	O
representation	O
.	O
Figure	O
5	O
:	O
Interface	O
for	O
human	O
evaluation	O
of	O
commonsense	O
explanation	O
graphs	O
.	O
T5	B-MethodName
model	O
on	O
the	O
facts	O
based	O
on	O
ConceptNet	B-DatasetName
relations	O
from	O
ATOMIC	B-DatasetName
-	O
2020	O
(	O
Hwang	O
et	O
al	O
,	O
2021	O
,	O
a	O
large	O
-	O
scale	O
commonsense	O
knowledge	O
base	O
.	O
The	O
fine	O
-	O
tuning	O
objective	O
is	O
to	O
predict	O
the	O
target	O
concept	O
given	O
the	O
source	O
concept	O
and	O
the	O
relation	O
.	O
Next	O
,	O
we	O
fine	O
-	O
tune	O
this	O
model	O
further	O
on	O
the	O
end	O
-	O
task	O
of	O
graph	B-TaskName
generation	I-TaskName
which	O
leads	O
to	O
small	O
improvements	O
in	O
both	O
StCA	O
and	O
SeCA	O
.	O
This	O
suggests	O
that	O
better	O
methods	O
of	O
inducing	O
commonsense	O
knowledge	O
in	O
these	O
models	O
can	O
potentially	O
lead	O
to	O
bigger	O
gains	O
with	O
more	O
semantically	O
coherent	O
graphs	O
.	O

In	O
Fig	O
.	O
6	O
,	O
7	O
,	O
8	O
and	O
9	O
,	O
we	O
show	O
various	O
examples	O
of	O
explanation	O
graphs	O
generated	O
by	O
our	O
models	O
.	O
In	O
Fig	O
.	O
6	O
and	O
7	O
,	O
our	O
proposed	O
models	O
improve	O
upon	O
Contrastive	O
Graph	O
T5	B-MethodName
-	O
generated	O
Graph	O
Semantically	O
Incorrect	O
Figure	O
7	O
:	O
Example	O
of	O
explanation	O
graphs	O
generated	O
by	O
different	O
models	O
.	O
The	O
baseline	O
T5	B-MethodName
-	O
generated	O
graph	O
is	O
semantically	O
incorrect	O
(	O
incoherent	O
relations	O
marked	O
in	O
dashed	O
red	O
)	O
while	O
our	O
proposed	O
models	O
generate	O
both	O
structurally	O
and	O
semantically	O
correct	O
graphs	O
.	O

We	O
thank	O
the	O
reviewers	O
for	O
their	O
helpful	O
feedback	O
and	O
the	O
annotators	O
for	O
their	O
time	O
and	O
effort	O
.	O
This	O
work	O
was	O
supported	O
by	O
DARPA	B-DatasetName
MCS	O
Grant	O
N66001	O
-	O
19	O
-	O
2	O
-	O
4031	O
,	O
NSF	O
-	O
CAREER	O
Award	O
1846185	O
,	O
DARPA	B-DatasetName
YFA17	O
-	O
D17AP00022	O
,	O
ONR	O
Grant	O
N00014	O
-	O
18	O
-	O
1	O
-	O
2871	O
,	O
Microsoft	O
Investigator	O
Fellowship	O
,	O
and	O
Munroe	O
&	O
Rebecca	O
Cobey	O
Fellowship	O
.	O
The	O
views	O
in	O
this	O
article	O
are	O
those	O
of	O
the	O
authors	O
and	O
not	O
the	O
funding	O
agency	O
.	O

the	O
incorrect	O
semantic	O
relations	O
from	O
the	O
T5	B-MethodName
baseline	O
graphs	O
.	O
Fig	O
.	O
8	O
shows	O
an	O
example	O
where	O
all	O
generated	O
graphs	O
,	O
while	O
different	O
,	O
are	O
correct	O
.	O
Finally	O
,	O
Fig	O
9	O
shows	O
an	O
example	O
where	O
although	O
our	O
proposed	O
models	O
improve	O
the	O
semantic	O
aspect	O
compared	O
to	O
the	O
baseline	O
graph	O
,	O
the	O
generated	O
graphs	O
are	O
disconnected	O
and	O
hence	O
structurally	O
incorrect	O
.	O
Overall	O
,	O
our	O
quantitative	O
results	O
and	O
human	O
evaluation	O
suggest	O
that	O
there	O
is	O
significant	O
room	O
for	O
improvement	O
on	O
the	O
task	O
of	O
commonsense	O
explanation	O
graph	B-TaskName
generation	I-TaskName
.	O

Structurally	O
Incorrect	O
T5	B-MethodName
-	O
generated	O
Graph	O
Semantically	O
Incorrect	O
Figure	O
9	O
:	O
Example	O
of	O
explanation	O
graphs	O
generated	O
by	O
different	O
models	O
.	O
T5	B-MethodName
generates	O
a	O
semantically	O
incorrect	O
graph	O
.	O
Our	O
models	O
generate	O
graphs	O
,	O
which	O
while	O
contain	O
meaningful	O
edges	O
,	O
are	O
disconnected	O
and	O
hence	O
are	O
structurally	O
incorrect	O
.	O

Syntactic	O
analyses	O
and	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
for	O
PubMed	O
and	O
PubMed	O
Central	O
-	O
up	O
-	O
to	O
-	O
the	O
-	O
minute	O

Although	O
advanced	O
text	O
mining	O
methods	O
specifically	O
adapted	O
to	O
the	O
biomedical	O
domain	O
are	O
continuously	O
being	O
developed	O
,	O
their	O
applications	O
on	O
large	O
scale	O
have	O
been	O
scarce	O
.	O
One	O
of	O
the	O
main	O
reasons	O
for	O
this	O
is	O
the	O
lack	O
of	O
computational	O
resources	O
and	O
workforce	O
required	O
for	O
processing	O
large	O
text	O
corpora	O
.	O
In	O
this	O
paper	O
we	O
present	O
a	O
publicly	O
available	O
resource	O
distributing	O
preprocessed	O
biomedical	O
literature	O
including	O
sentence	O
splitting	O
,	O
tokenization	O
,	O
part	B-TaskName
-	I-TaskName
of	I-TaskName
-	I-TaskName
speech	I-TaskName
tagging	I-TaskName
,	O
syntactic	O
parses	O
and	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
.	O
The	O
aim	O
of	O
this	O
work	O
is	O
to	O
support	O
the	O
future	O
development	O
of	O
largescale	O
text	O
mining	O
resources	O
by	O
eliminating	O
the	O
time	O
consuming	O
but	O
necessary	O
preprocessing	O
steps	O
.	O
This	O
resource	O
covers	O
the	O
whole	O
of	O
PubMed	O
and	O
PubMed	O
Central	O
Open	O
Access	O
section	O
,	O
currently	O
containing	O
26	O
M	O
abstracts	O
and	O
1.4	O
M	O
full	O
articles	O
,	O
constituting	O
over	O
388	O
M	O
analyzed	O
sentences	O
.	O
The	O
resource	O
is	O
based	O
on	O
a	O
fully	O
automated	O
pipeline	O
,	O
guaranteeing	O
that	O
the	O
distributed	O
data	O
is	O
always	O
up	O
-	O
to	O
-	O
date	O
.	O
The	O
resource	O
is	O
available	O
at	O
https://turkunlp	O
.	O
github.io/pubmed_parses/.	O

Due	O
to	O
the	O
rapid	O
growth	O
of	O
biomedical	O
literature	O
,	O
the	O
maintenance	O
of	O
manually	O
curated	O
databases	O
,	O
usually	O
updated	O
following	O
new	O
discoveries	O
published	O
in	O
articles	O
,	O
has	O
become	O
unfeasible	O
.	O
This	O
has	O
led	O
to	O
a	O
significant	O
interest	O
in	O
developing	O
automated	O
text	O
mining	O
methods	O
specifically	O
for	O
the	O
biomedical	O
domain	O
.	O
Various	O
community	O
efforts	O
,	O
mainly	O
in	O
the	O
form	O
of	O
shared	O
tasks	O
,	O
have	O
resulted	O
in	O
steady	O
improvement	O
in	O
biomedical	O
text	O
mining	O
methods	O
(	O
Kim	O
et	O
al	O
,	O
2009	O
;	O
Segura	O
Bedmar	O
et	O
al	O
,	O
2013	O
)	O
.	O
For	O
instance	O
the	O
GENIA	B-DatasetName
shared	O
tasks	O
focusing	O
on	O
extracting	O
biological	O
events	O
,	O
such	O
as	O
gene	O
regulations	O
,	O
have	O
consistently	O
gathered	O
wide	O
interest	O
and	O
have	O
led	O
to	O
the	O
development	O
of	O
several	O
text	O
mining	O
tools	O
(	O
Miwa	O
et	O
al	O
,	O
2012	O
;	O
Björne	O
and	O
Salakoski	O
,	O
2013	O
)	O
.	O
These	O
methods	O
have	O
been	O
also	O
succesfully	O
applied	O
on	O
a	O
large	O
scale	O
and	O
several	O
biomedical	O
text	O
mining	O
databases	O
are	O
publicly	O
available	O
(	O
Van	O
Landeghem	O
et	O
al	O
,	O
2013a	O
;	O
Franceschini	O
et	O
al	O
,	O
2013	O
;	O
Müller	O
et	O
al	O
,	O
2004	O
)	O
.	O
Although	O
these	O
resources	O
exist	O
,	O
their	O
number	O
does	O
not	O
reflect	O
the	O
vast	O
amount	O
of	O
fundamental	O
research	O
invested	O
in	O
the	O
underlying	O
methods	O
,	O
mainly	O
due	O
to	O
the	O
nontrivial	O
amount	O
of	O
manual	O
labor	O
and	O
computational	O
resources	O
required	O
to	O
process	O
large	O
quantities	O
of	O
textual	O
data	O
.	O
Another	O
issue	O
arising	O
from	O
the	O
challenging	O
text	O
preprocessing	O
is	O
the	O
lack	O
of	O
maintenance	O
of	O
the	O
existing	O
databases	O
which	O
in	O
effect	O
nullifies	O
the	O
purpose	O
of	O
text	O
mining	O
as	O
these	O
resources	O
tend	O
to	O
be	O
almost	O
as	O
much	O
out	O
-	O
of	O
-	O
date	O
as	O
their	O
manually	O
curated	O
counterparts	O
.	O
According	O
to	O
MEDLINE	O
statistics	O
1	O
806	O
,	O
326	O
new	O
articles	O
were	O
indexed	O
during	O
2015	O
and	O
thus	O
a	O
text	O
mining	O
resource	O
will	O
miss	O
on	O
average	O
67	O
thousand	O
articles	O
each	O
month	O
it	O
has	O
n't	O
been	O
updated	O
.	O
In	O
this	O
paper	O
we	O
present	O
a	O
resource	O
aiming	O
to	O
support	O
the	O
development	O
and	O
maintenance	O
of	O
large	O
-	O
scale	O
biomedical	O
text	O
mining	O
.	O
The	O
resource	O
includes	O
all	O
PubMed	O
abstracts	O
as	O
well	O
as	O
full	O
articles	O
from	O
the	O
open	O
access	O
section	O
of	O
PubMed	O
Central	O
(	O
PMCOA	O
)	O
,	O
with	O
the	O
fundamental	O
language	O
technology	O
building	O
blocks	O
,	O
such	O
as	O
part	O
-	O
ofspeech	O
(	O
POS	O
)	O
tagging	O
and	O
syntactic	O
parses	O
,	O
readily	O
available	O
.	O
In	O
addition	O
,	O
recognition	O
of	O
several	O
bio	O
-	O
logically	O
relevant	O
named	O
entities	O
,	O
such	O
as	O
proteins	O
and	O
chemicals	O
is	O
included	O
.	O
Hence	O
we	O
hope	O
that	O
this	O
resource	O
eliminates	O
the	O
need	O
of	O
the	O
tedious	O
preprocessing	O
involved	O
in	O
utilizing	O
the	O
PubMed	O
data	O
and	O
allows	O
swifter	O
development	O
of	O
new	O
information	O
extraction	O
databases	O
.	O
The	O
resource	O
is	O
constructed	O
with	O
an	O
automated	O
pipeline	O
which	O
provides	O
weekly	O
updates	O
with	O
the	O
latest	O
articles	O
indexed	O
in	O
PubMed	O
and	O
PubMed	O
Central	O
,	O
ensuring	O
the	O
timeliness	O
of	O
the	O
distributed	O
data	O
.	O
All	O
the	O
data	O
is	O
downloadable	O
in	O
an	O
easily	O
handleable	O
XML	O
format	O
,	O
also	O
used	O
by	O
the	O
widely	O
adapted	O
event	B-TaskName
extraction	I-TaskName
system	O
TEES	O
(	O
Björne	O
and	O
Salakoski	O
,	O
2015	O
)	O
.	O
A	O
detailed	O
description	O
of	O
this	O
format	O
is	O
available	O
on	O
the	O
website	O
.	O

In	O
this	O
section	O
,	O
we	O
discuss	O
our	O
processing	O
pipeline	O
as	O
shown	O
in	O
Figure	O
1	O
.	O
Firstly	O
,	O
both	O
PubMed	O
and	O
PMCOA	O
documents	O
are	O
downloaded	O
from	O
NCBI	O
FTP	O
services	O
.	O
For	O
the	O
periodical	O
updates	O
of	O
our	O
resource	O
this	O
is	O
done	O
weekly	O
-	O
the	O
same	O
interval	O
the	O
official	O
PMCOA	O
dataset	O
is	O
updated	O
.	O
From	O
the	O
PubMed	O
incremental	O
updates	O
we	O
only	O
include	O
newly	O
added	O
documents	O
and	O
ignore	O
other	O
updates	O
.	O
As	O
the	O
PMCOA	O
does	O
not	O
provide	O
incremental	O
updates	O
,	O
we	O
use	O
the	O
index	O
file	O
and	O
compare	O
it	O
to	O
the	O
previous	O
file	O
list	O
to	O
select	O
new	O
articles	O
for	O
processing	O
.	O
Even	O
though	O
the	O
PubMed	O
and	O
PMCOA	O
documents	O
are	O
provided	O
in	O
slightly	O
different	O
XML	O
formats	O
,	O
they	O
can	O
be	O
processed	O
in	O
similar	O
fashion	O
.	O
As	O
a	O
result	O
,	O
the	O
rest	O
of	O
the	O
pipeline	O
discussed	O
in	O
this	O
section	O
is	O
applied	O
to	O
both	O
document	O
types	O
.	O
Both	O
PubMed	O
XML	O
articles	O
and	O
PMCOA	O
NXML	O
full	O
texts	O
are	O
preprocessed	O
using	O
publicly	O
available	O
tools	O
2	O
(	O
Pyysalo	O
et	O
al	O
,	O
2013	O
)	O
.	O
These	O
tools	O
convert	O
XML	O
documents	O
to	O
plain	O
text	O
and	O
change	O
character	O
encoding	O
from	O
UTF	O
-	O
8	O
to	O
ASCII	O
as	O
many	O
of	O
the	O
legacy	O
language	O
processing	O
tools	O
are	O
incapable	O
of	O
handling	O
non	O
-	O
ASCII	O
characters	O
.	O
Additionally	O
,	O
all	O
excess	O
meta	O
data	O
is	O
removed	O
,	O
leaving	O
titles	O
,	O
abstracts	O
and	O
full	O
-	O
text	O
contents	O
for	O
further	O
processing	O
.	O
These	O
documents	O
are	O
subsequently	O
split	O
into	O
sentences	O
using	O
GENIA	B-DatasetName
sentence	O
splitter	O
(	O
Saetre	O
et	O
al	O
,	O
2007	O
)	O
as	O
most	O
linguistic	O
analyses	O
are	O
done	O
on	O
the	O
sentence	O
level	O
.	O
GENIA	B-DatasetName
sentence	O
splitter	O
is	O
trained	O
on	O
biomedical	O
text	O
(	O
GENIA	B-DatasetName
corpus	O
)	O
and	O
has	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
this	O
domain	O
.	O
The	O
whole	O
data	O
is	O
parsed	O
with	O
the	O
BLLIP	O
constituent	O
parser	O
(	O
Charniak	O
and	O
Johnson	O
,	O
2005	O
)	O
,	O
using	O
a	O
model	O
adapted	O
for	O
the	O
biomedical	O
domain	O
(	O
McClosky	O
,	O
2010	O
)	O
,	O
as	O
provided	O
in	O
the	O
TEES	O
processing	O
pipeline	O
.	O
The	O
distributed	O
tokenization	O
and	O
POS	O
tagging	O
are	O
also	O
produced	O
with	O
the	O
parser	O
pipeline	O
.	O
We	O
chose	O
to	O
use	O
this	O
tool	O
as	O
the	O
performance	O
of	O
the	O
TEES	O
software	O
has	O
been	O
previously	O
evaluated	O
on	O
a	O
large	O
-	O
scale	O
together	O
with	O
this	O
parsing	O
pipeline	O
(	O
Van	O
Landeghem	O
et	O
al	O
,	O
2013b	O
)	O
and	O
it	O
should	O
be	O
a	O
reliable	O
choice	O
for	O
biomedical	O
relation	B-TaskName
extraction	I-TaskName
.	O
Since	O
dependency	B-TaskName
parsing	I-TaskName
has	O
become	O
the	O
prevalent	O
approach	O
in	O
modeling	O
syntactic	O
relations	O
,	O
we	O
also	O
provide	O
conversions	O
to	O
the	O
collapsed	O
Stanford	O
dependency	O
scheme	O
(	O
De	O
Marneffe	O
et	O
al	O
,	O
2006	O
)	O
.	O
The	O
pipeline	O
is	O
run	O
in	O
parallel	O
on	O
a	O
cluster	O
computer	O
with	O
the	O
input	O
data	O
divided	O
into	O
smaller	O
batches	O
.	O
The	O
size	O
of	O
these	O
batches	O
is	O
altered	O
along	O
the	O
pipeline	O
to	O
adapt	O
to	O
the	O
varying	O
computational	O
requirements	O
of	O
the	O
different	O
tools	O
.	O

Named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
is	O
one	O
of	O
the	O
fundamental	O
tasks	O
in	O
BioNLP	O
as	O
most	O
of	O
the	O
cru	O
-	O
(	O
Okazaki	O
,	O
2007	O
)	O
.	O
Having	O
a	O
single	O
tool	O
for	O
this	O
processing	O
step	O
instead	O
of	O
using	O
the	O
various	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
tools	O
is	O
critical	O
for	O
the	O
maintainability	O
of	O
the	O
processing	O
pipeline	O
.	O
NERsuite	O
was	O
selected	O
as	O
several	O
biological	O
models	O
are	O
readily	O
available	O
for	O
this	O
software	O
(	O
Kaewphan	O
et	O
al	O
,	O
2016	O
;	O
Pyysalo	O
and	O
Ananiadou	O
,	O
2014	O
)	O
and	O
as	O
it	O
supports	O
label	O
weighting	O
(	O
Minkov	O
et	O
al	O
,	O
2006	O
)	O
unlike	O
many	O
other	O
NER	B-TaskName
tools	O
.	O
For	O
cell	O
line	O
names	O
we	O
use	O
a	O
publicly	O
available	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
model	O
(	O
Kaewphan	O
et	O
al	O
,	O
2016	O
)	O
,	O
whereas	O
for	O
the	O
other	O
entity	O
types	O
we	O
train	O
our	O
own	O
models	O
with	O
manually	O
annotated	O
data	O
from	O
GENETAG	O
(	O
Tanabe	O
et	O
al	O
,	O
2005	O
)	O
,	O
CHEMDNER	O
(	O
Krallinger	O
et	O
al	O
,	O
2015	O
)	O
,	O
SPECIES	O
(	O
Pafilis	O
et	O
al	O
,	O
2013	O
)	O
and	O
NCBI	B-DatasetName
disease	I-DatasetName
(	O
Doǧan	O
et	O
al	O
,	O
2014	O
)	O
corpora	O
for	O
GGPs	O
,	O
chemicals	O
,	O
organisms	O
and	O
diseases	O
,	O
respectively	O
.	O
All	O
these	O
corpora	O
are	O
comprised	O
of	O
biomedical	O
articles	O
and	O
should	O
thus	O
reflect	O
well	O
the	O
text	O
types	O
seen	O
in	O
PubMed	O
.	O
All	O
used	O
corpora	O
provide	O
the	O
data	O
divided	O
to	O
training	O
,	O
development	O
and	O
test	O
sets	O
in	O
advance	O
,	O
the	O
3	O
http://nersuite.nlplab.org/	O
SPECIES	O
corpus	O
being	O
an	O
exception	O
.	O
For	O
this	O
corpus	O
we	O
do	O
our	O
own	O
data	O
division	O
with	O
random	O
sampling	O
on	O
document	O
level	O
,	O
for	O
each	O
taxonomy	O
category	O
separately	O
.	O
For	O
each	O
entity	O
type	O
,	O
the	O
C2	O
value	O
,	O
as	O
well	O
as	O
the	O
label	O
weights	O
are	O
selected	O
to	O
optimize	O
the	O
F	O
-	O
score	O
on	O
the	O
development	O
set	O
.	O
For	O
the	O
training	O
of	O
the	O
final	O
models	O
used	O
in	O
the	O
resource	O
,	O
we	O
use	O
the	O
whole	O
corpora	O
,	O
i.e.	O
the	O
combination	O
of	O
training	O
,	O
development	O
and	O
test	O
sets	O
.	O
Detailed	O
performance	O
evaluations	O
for	O
all	O
entity	O
types	O
are	O
shown	O
in	O
Table	O
1	O
.	O
We	O
evaluate	O
NERsuite	O
in	O
terms	O
of	O
precision	O
,	O
recall	O
and	O
F	O
-	O
score	O
against	O
the	O
test	O
data	O
using	O
"	O
strict	O
matching	O
"	O
criteria	O
,	O
i.e.	O
only	O
consider	O
the	O
tagged	O
entities	O
correct	O
if	O
they	O
are	O
perfectly	O
matched	O
with	O
the	O
gold	O
standard	O
data	O
.	O
These	O
results	O
may	O
not	O
be	O
directly	O
comparable	O
to	O
the	O
results	O
reported	O
in	O
other	O
studies	O
as	O
relaxed	O
evaluation	O
methods	O
are	O
sometimes	O
used	O
.	O
However	O
,	O
we	O
can	O
conclude	O
that	O
our	O
system	O
is	O
on	O
par	O
with	O
the	O
methods	O
published	O
elsewhere	O
and	O
the	O
limitation	O
of	O
using	O
a	O
single	O
tool	O
does	O
not	O
have	O
a	O
significant	O
negative	O
impact	O
on	O
the	O
overall	O
performance	O
.	O

Our	O
future	O
efforts	O
will	O
focus	O
on	O
expanding	O
the	O
coverage	O
of	O
supported	O
entity	O
types	O
to	O
mutations	O
and	O
anatomical	O
entities	O
(	O
Wei	O
et	O
al	O
,	O
2013	O
;	O
Pyysalo	O
and	O
Ananiadou	O
,	O
2014	O
)	O
,	O
deepening	O
the	O
captured	O
information	O
of	O
biological	O
processes	O
and	O
bringing	O
text	O
mining	O
one	O
step	O
closer	O
to	O
extracting	O
a	O
realistic	O
view	O
of	O
biological	O
knowledge	O
.	O
As	O
many	O
of	O
the	O
NER	B-TaskName
training	O
corpora	O
include	O
only	O
abstracts	O
and	O
are	O
limited	O
to	O
specific	O
domains	O
,	O
the	O
generalizability	O
of	O
the	O
trained	O
NER	B-TaskName
models	O
to	O
full	O
articles	O
and	O
to	O
the	O
wide	O
spectrum	O
of	O
topics	O
covered	O
in	O
PubMed	O
is	O
not	O
clear	O
.	O
Thus	O
we	O
wish	O
to	O
assess	O
how	O
well	O
these	O
models	O
perform	O
on	O
largescale	O
datasets	O
and	O
analyze	O
how	O
their	O
performance	O
could	O
be	O
improved	O
on	O
out	O
-	O
of	O
-	O
domain	O
documents	O
.	O
We	O
plan	O
to	O
also	O
include	O
entity	O
normalization	O
for	O
all	O
supported	O
types	O
,	O
but	O
as	O
we	O
wish	O
to	O
minimize	O
the	O
number	O
of	O
individual	O
tools	O
in	O
the	O
processing	O
pipeline	O
,	O
we	O
are	O
developing	O
a	O
generic	O
approach	O
suitable	O
for	O
most	O
entity	O
types	O
.	O

BOUN	O
-	O
ISIK	O
Participation	O
:	O
An	O
Unsupervised	O
Approach	O
for	O
the	O
Named	O
Entity	O
Normalization	O
and	O
Relation	B-TaskName
Extraction	I-TaskName
of	O
Bacteria	O
Biotopeṡ	O

This	O
paper	O
presents	O
our	O
participation	O
at	O
the	O
Bacteria	O
Biotope	O
Task	O
of	O
the	O
BioNLP	O
Shared	O
Task	O
2019	O
.	O
Our	O
participation	O
includes	O
two	O
systems	O
for	O
the	O
two	O
subtasks	O
of	O
the	O
Bacteria	O
Biotope	O
Task	O
:	O
the	O
normalization	O
of	O
entities	O
(	O
BB	O
-	O
norm	O
)	O
and	O
the	O
identification	O
of	O
the	O
relations	O
between	O
the	O
entities	O
given	O
a	O
biomedical	O
text	O
(	O
BB	O
-	O
rel	O
)	O
.	O
For	O
the	O
normalization	O
of	O
entities	O
,	O
we	O
utilized	O
word	B-TaskName
embeddings	I-TaskName
and	O
syntactic	O
re	O
-	O
ranking	O
.	O
For	O
the	O
relation	B-TaskName
extraction	I-TaskName
task	O
,	O
pre	O
-	O
defined	O
rules	O
are	O
used	O
.	O
Although	O
both	O
approaches	O
are	O
unsupervised	O
,	O
in	O
the	O
sense	O
that	O
they	O
do	O
not	O
need	O
any	O
labeled	O
data	O
,	O
they	O
achieved	O
promising	O
results	O
.	O
Especially	O
,	O
for	O
the	O
BB	O
-	O
norm	O
task	O
,	O
the	O
results	O
have	O
shown	O
that	O
the	O
proposed	O
method	O
performs	O
as	O
good	O
as	O
deep	O
learning	O
based	O
methods	O
,	O
which	O
require	O
labeled	O
data	O
.	O

The	O
amount	O
of	O
electronic	O
resources	O
in	O
the	O
biomedical	O
domain	O
and	O
its	O
rapid	O
growth	O
are	O
major	O
challenges	O
for	O
the	O
scientists	O
who	O
make	O
research	O
in	O
this	O
domain	O
.	O
Text	O
mining	O
methods	O
which	O
aim	O
to	O
automatically	O
extract	O
useful	O
information	O
from	O
the	O
text	O
of	O
these	O
electronic	O
resources	O
provide	O
convenience	O
to	O
the	O
researchers	O
.	O
A	O
number	O
of	O
shared	O
tasks	O
,	O
including	O
the	O
BioNLP	O
Shared	O
Tasks	O
,	O
have	O
been	O
conducted	O
with	O
the	O
goal	O
of	O
developing	O
biomedical	O
text	O
mining	O
methods	O
.	O
In	O
2011	O
,	O
the	O
Bacteria	O
Biotope	O
Task	O
has	O
been	O
conducted	O
for	O
the	O
first	O
time	O
as	O
a	O
part	O
of	O
the	O
BioNLP	O
Shared	O
Task	O
targeting	O
the	O
extraction	O
of	O
useful	O
information	O
regarding	O
bacteria	O
and	O
their	O
habitats	O
(	O
Bossy	O
et	O
al	O
,	O
2011	O
)	O
.	O
Since	O
then	O
,	O
the	O
participant	O
teams	O
of	O
the	O
following	O
shared	O
task	O
series	O
developed	O
various	O
solutions	O
for	O
the	O
problem	O
of	O
bacteria	O
biotopes	O
(	O
Bossy	O
et	O
al	O
,	O
2015	O
;	O
Deleger	O
et	O
al	O
,	O
2016	O
)	O
.	O
The	O
Bacteria	O
Biotope	O
Task	O
of	O
the	O
BioNLP	O
Shared	O
Task	O
2019	O
(	O
Bossy	O
et	O
al	O
,	O
2019	O
)	O
is	O
the	O
final	O
version	O
of	O
the	O
tasks	O
that	O
have	O
been	O
conducted	O
until	O
now	O
readdressing	O
the	O
problem	O
of	O
extraction	O
of	O
the	O
information	O
regarding	O
the	O
bacteria	O
biotopes	O
.	O
This	O
year	O
's	O
task	O
has	O
presented	O
the	O
opportunity	O
to	O
the	O
participants	O
to	O
develop	O
solutions	O
for	O
three	O
subproblems	O
:	O
normalization	O
(	O
BB	O
-	O
norm	O
)	O
,	O
relation	B-TaskName
extraction	I-TaskName
(	O
BB	O
-	O
rel	O
)	O
,	O
and	O
knowledge	O
base	O
extraction	O
(	O
BB	O
-	O
kb	O
)	O
.	O
For	O
the	O
BB	O
-	O
norm	O
task	O
of	O
the	O
Bacteria	O
Biotope	O
Task	O
of	O
the	O
BioNLP	O
Shared	O
Task	O
2019	O
,	O
the	O
participants	O
are	O
expected	O
to	O
develop	O
systems	O
to	O
link	O
the	O
named	O
entities	O
(	O
Microorganism	O
,	O
Habitat	O
,	O
and	O
Phenotype	O
)	O
in	O
a	O
given	O
text	O
through	O
a	O
given	O
ontology	B-MethodName
,	O
when	O
the	O
entities	O
are	O
given	O
with	O
their	O
boundaries	O
.	O
For	O
instance	O
,	O
the	O
sample	O
sentence	O
"	O
Atypical	O
mycobacteria	O
causing	O
non	O
-	O
pulmonary	O
disease	O
in	O
Queensland	O
.	O
"	O
consists	O
of	O
the	O
following	O
mentions	O
:	O
"	O
mycobacteria	O
"	O
microorganism	O
mention	O
,	O
"	O
causing	O
non	O
-	O
pulmonary	O
disease	O
"	O
phenotype	O
mention	O
,	O
and	O
"	O
pulmonary	O
"	O
habitat	O
mention	O
,	O
which	O
should	O
be	O
normalized	O
to	O
the	O
"	O
Mycobacteria	O
"	O
term	O
in	O
the	O
NCBI	O
taxonomy	O
,	O
and	O
"	O
human	O
pathogen	O
"	O
and	O
"	O
lung	O
"	O
terms	O
in	O
the	O
Onto	O
-	O
Biotope	O
ontology	B-MethodName
,	O
respectively	O
.	O
For	O
the	O
BB	O
-	O
rel	O
task	O
of	O
the	O
Bacteria	O
Biotopes	O
Task	O
of	O
the	O
BioNLP	O
Shared	O
Task	O
2019	O
,	O
the	O
participants	O
are	O
required	O
to	O
extract	O
the	O
relations	O
between	O
the	O
entities	O
when	O
the	O
entities	O
are	O
given	O
.	O
There	O
are	O
two	O
types	O
of	O
relations	O
:	O
Lives	O
in	O
relation	O
,	O
which	O
indicates	O
a	O
localization	O
relation	O
between	O
a	O
Microorganism	O
entity	O
and	O
a	O
Habitat	O
/	O
Geographical	O
entity	O
,	O
and	O
Exhibits	O
relation	O
,	O
which	O
indicates	O
a	O
property	O
relation	O
between	O
a	O
Phenotype	O
entity	O
and	O
a	O
Microorganism	O
entity	O
.	O
For	O
instance	O
,	O
the	O
sample	O
sentence	O
above	O
indicates	O
two	O
relations	O
:	O
a	O
Lives	O
in	O
relation	O
between	O
the	O
"	O
Mycobacteria	O
"	O
Microorganism	O
entity	O
and	O
the	O
"	O
Queensland	O
"	O
Geographical	O
entity	O
,	O
and	O
an	O
Exhibits	O
relation	O
between	O
the	O
"	O
Mycobacteria	O
"	O
Microorganism	O
entity	O
and	O
the	O
"	O
causing	O
nonpulmonary	O
disease	O
"	O
Phenotype	O
entity	O
.	O
We	O
participated	O
at	O
the	O
Bacteria	O
Biotope	O
Task	O
in	O
the	O
BioNLP	O
Shared	O
Task	O
2019	O
with	O
our	O
system	O
(	O
named	O
as	O
the	O
BOUN	O
-	O
ISIK	O
system	O
)	O
and	O
ob	O
-	O
tained	O
promising	O
results	O
in	O
the	O
official	O
evaluation	O
.	O
This	O
paper	O
presents	O
our	O
participating	O
system	O
for	O
two	O
sub	O
-	O
tasks	O
:	O
one	O
for	O
the	O
BB	O
-	O
norm	O
(	O
Entity	O
Normalization	O
)	O
sub	O
-	O
task	O
and	O
one	O
for	O
the	O
BB	O
-	O
rel	O
(	O
Relation	B-TaskName
Extraction	I-TaskName
)	O
sub	O
-	O
task	O
.	O
For	O
the	O
entity	O
normalization	O
sub	O
-	O
task	O
,	O
we	O
utilized	O
word	B-TaskName
embeddings	I-TaskName
and	O
syntactic	O
re	O
-	O
ranking	O
to	O
normalize	O
the	O
entities	O
.	O
On	O
the	O
other	O
hand	O
,	O
for	O
the	O
relation	B-TaskName
extraction	I-TaskName
sub	O
-	O
task	O
,	O
we	O
proposed	O
a	O
rule	O
-	O
based	O
method	O
.	O
Although	O
both	O
systems	O
are	O
unsupervised	O
,	O
they	O
achieved	O
promising	O
results	O
.	O
For	O
the	O
BB	O
-	O
norm	O
sub	O
-	O
task	O
,	O
the	O
official	O
results	O
of	O
our	O
system	O
achieved	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
the	O
BioNLP	O
Shared	O
Task	O
2019	O
Bacteria	O
Biotope	O
task	O
test	O
data	O
set	O
.	O
The	O
results	O
have	O
shown	O
that	O
our	O
unsupervised	O
approach	O
,	O
which	O
does	O
not	O
require	O
labeled	O
data	O
,	O
performs	O
as	O
good	O
as	O
the	O
deep	O
learning	O
based	O
methods	O
,	O
which	O
require	O
labeled	O
data	O
.	O

Several	O
approaches	O
,	O
which	O
consider	O
the	O
extraction	O
of	O
relations	O
between	O
various	O
biomedical	O
entities	O
such	O
as	O
protein	O
/	O
protein	O
(	O
Giuliano	O
et	O
al	O
,	O
2006	O
;	O
Airola	O
et	O
al	O
,	O
2008	O
;	O
Choi	O
,	O
2018	O
)	O
,	O
drug	O
/	O
drug	O
(	O
Segura	O
-	O
Bedmar	O
et	O
al	O
,	O
2011	O
;	O
Kim	O
et	O
al	O
,	O
2015	O
)	O
,	O
and	O
gene	O
/	O
disease	O
(	O
Bravo	O
et	O
al	O
,	O
2015	O
)	O
from	O
biomedical	O
text	O
,	O
have	O
been	O
presented	O
in	O
the	O
literature	O
.	O
Relation	B-TaskName
extraction	I-TaskName
in	O
the	O
bacteria	O
biotopes	O
domain	O
has	O
also	O
attracted	O
considerable	O
attention	O
owing	O
to	O
the	O
BioNLP	O
Bacteria	O
Biotope	O
Shared	O
Tasks	O
.	O
Previous	O
work	O
in	O
the	O
bacteria	O
biotopes	O
domain	O
consists	O
of	O
the	O
extraction	O
of	O
relations	O
between	O
bacteria	O
entities	O
and	O
habitat	O
entities	O
(	O
Localization	O
Relation	B-TaskName
Extraction	I-TaskName
)	O
and	O
of	O
relations	O
between	O
two	O
habitat	O
entities	O
(	O
Part	O
Of	O
Relation	B-TaskName
Extraction	I-TaskName
)	O
.	O
The	O
participants	O
of	O
the	O
BioNLP	O
Shared	O
Task	O
2011	O
,	O
which	O
is	O
the	O
first	O
shared	O
task	O
that	O
addressed	O
the	O
relation	B-TaskName
extraction	I-TaskName
task	O
of	O
bacteria	O
biotopes	O
,	O
utilized	O
both	O
machine	O
learning	O
and	O
rule	O
-	O
based	O
approaches	O
for	O
detecting	O
the	O
Localization	O
and	O
Part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
relations	O
among	O
bacteria	O
and	O
habitats	O
(	O
Bossy	O
et	O
al	O
,	O
2011	O
)	O
.	O
Sub	O
-	O
task	O
2	O
of	O
the	O
Bacteria	O
Biotope	O
(	O
BB	O
)	O
Task	O
in	O
the	O
BioNLP	O
Shared	O
Task	O
2013	O
also	O
gave	O
another	O
opportunity	O
to	O
scientists	O
to	O
address	O
the	O
task	O
of	O
extracting	O
the	O
Localization	O
and	O
Part	O
Of	O
relations	O
in	O
the	O
bacteria	O
biotopes	O
domain	O
.	O
For	O
this	O
subtask	O
,	O
the	O
best	O
F	O
-	O
score	O
(	O
42	O
%	O
)	O
was	O
obtained	O
by	O
the	O
TEES	O
2.1	O
system	O
(	O
Björne	O
and	O
Salakoski	O
,	O
2013	O
)	O
,	O
which	O
used	O
support	B-MethodName
vector	I-MethodName
machine	I-MethodName
classification	O
.	O
After	O
the	O
shared	O
task	O
,	O
a	O
new	O
sentence	O
-	O
level	O
cooccurrence	O
approach	O
with	O
an	O
anaphora	O
resolution	O
component	O
in	O
order	O
to	O
handle	O
relations	O
that	O
span	O
multiple	O
sentences	O
has	O
been	O
developed	O
in	O
(	O
Karadeniz	O
andÖzgür	O
,	O
2015	O
)	O
,	O
which	O
resulted	O
in	O
an	O
improved	O
F	O
-	O
score	O
performance	O
of	O
53	O
%	O
on	O
Sub	O
-	O
task	O
2	O
.	O
In	O
the	O
BioNLP	O
Shared	O
Task	O
2016	O
,	O
the	O
VERSE	B-MethodName
team	O
(	O
Lever	O
and	O
Jones	O
,	O
2016	O
)	O
achieved	O
the	O
best	O
F	O
-	O
score	O
,	O
which	O
is	O
56	O
%	O
,	O
on	O
the	O
relation	B-TaskName
extraction	I-TaskName
sub	O
-	O
task	O
of	O
Bacteria	O
Biotopes	O
by	O
utilizing	O
support	O
vector	O
machines	O
.	O
(	O
NCBI	O
,	O
2018	O
)	O
were	O
provided	O
,	O
while	O
in	O
the	O
test	O
phase	O
,	O
only	O
the	O
entity	O
boundaries	O
and	O
the	O
entity	O
types	O
were	O
given	O
by	O
the	O
task	O
organizers	O
.	O
For	O
the	O
training	O
and	O
development	O
phases	O
of	O
BBrel	O
,	O
document	O
texts	O
with	O
manually	O
annotated	O
Microorganism	O
,	O
Habitat	O
,	O
Phenotype	O
and	O
Geographical	O
entities	O
,	O
as	O
well	O
as	O
the	O
Lives	O
in	O
and	O
Exhibits	O
relations	O
were	O
provided	O
,	O
while	O
in	O
the	O
test	O
phase	O
,	O
document	O
texts	O
annotated	O
only	O
for	O
Microorganism	O
,	O
Habitat	O
,	O
Phenotype	O
and	O
Geographical	O
entities	O
were	O
given	O
.	O
Since	O
our	O
system	O
for	O
the	O
named	O
entity	O
normalization	O
and	O
relation	B-TaskName
extraction	I-TaskName
of	O
bacteria	O
biotopes	O
is	O
based	O
on	O
unsupervised	O
approaches	O
and	O
does	O
not	O
require	O
any	O
labeled	O
training	O
data	O
,	O
the	O
errors	O
of	O
the	O
developed	O
system	O
are	O
analyzed	O
on	O
the	O
provided	O
training	O
and	O
the	O
development	O
sets	O
.	O
The	O
test	O
set	O
is	O
used	O
for	O
the	O
evaluation	O
of	O
the	O
performance	O
of	O
the	O
system	O
.	O

In	O
this	O
section	O
of	O
the	O
paper	O
,	O
the	O
utilized	O
methods	O
for	O
the	O
BB	O
-	O
norm	O
task	O
are	O
explained	O
in	O
detail	O
.	O
The	O
BB	O
-	O
norm	O
task	O
includes	O
the	O
normalization	O
of	O
Habitat	O
entities	O
and	O
Phenotype	O
entities	O
in	O
a	O
given	O
set	O
of	O
documents	O
through	O
the	O
Onto	O
-	O
Biotope	O
ontology	B-MethodName
and	O
the	O
normalization	O
of	O
Microorganism	O
entities	O
through	O
the	O
NCBI	O
Taxonomy	O
.	O
The	O
methods	O
developed	O
for	O
the	O
normalization	O
of	O
the	O
named	O
entities	O
can	O
be	O
categorized	O
into	O
two	O
according	O
to	O
the	O
type	O
of	O
the	O
entities	O
:	O
Habitat	O
and	O
Phenotype	O
Normalization	O
and	O
Microorganism	O
Normalization	O
.	O

For	O
the	O
normalization	O
of	O
semantically	O
meaningful	O
entities	O
such	O
as	O
Habitat	O
and	O
Phenotype	O
entities	O
,	O
a	O
two	O
-	O
step	O
approach	O
that	O
we	O
have	O
previously	O
proposed	O
in	O
(	O
Karadeniz	O
andÖzgür	O
,	O
2019	O
)	O
is	O
adapted	O
to	O
this	O
new	O
data	O
set	O
.	O
According	O
to	O
this	O
approach	O
,	O
for	O
the	O
normalization	O
of	O
an	O
entity	O
mention	O
,	O
the	O
top	O
k	O
semantically	O
most	O
similar	O
ontology	B-MethodName
concepts	O
are	O
found	O
at	O
the	O
first	O
step	O
using	O
the	O
word	O
embedding	O
representations	O
of	O
the	O
entity	O
mention	O
and	O
the	O
ontology	B-MethodName
concepts	O
.	O
At	O
the	O
second	O
step	O
,	O
these	O
top	O
k	O
semantically	O
most	O
similar	O
concepts	O
are	O
re	O
-	O
ranked	O
according	O
to	O
a	O
similarity	O
metric	O
that	O
utilizes	O
the	O
constituency	O
parses	O
of	O
the	O
entity	O
mention	O
and	O
ontology	B-MethodName
concept	O
phrases	O
.	O
The	O
resulting	O
most	O
similar	O
ontology	B-MethodName
concept	O
is	O
assigned	O
as	O
the	O
normalized	O
concept	O
for	O
the	O
corresponding	O
mention	O
.	O
The	O
details	O
of	O
this	O
approach	O
are	O
explained	O
in	O
the	O
following	O
subsections	O
.	O

In	O
the	O
pre	O
-	O
processing	O
step	O
,	O
the	O
named	O
entity	O
mentions	O
and	O
the	O
ontology	B-MethodName
concept	O
names	O
are	O
tokenized	O
,	O
and	O
the	O
stop	O
-	O
words	O
are	O
removed	O
from	O
the	O
mentions	O
and	O
the	O
ontology	B-MethodName
concept	O
names	O
.	O
The	O
intuition	O
behind	O
the	O
adapted	O
method	O
is	O
that	O
semantically	O
similar	O
words	O
have	O
similar	O
word	O
vectors	O
.	O
Following	O
this	O
intuition	O
,	O
the	O
semantic	B-TaskName
similarity	I-TaskName
between	O
named	O
entity	O
mentions	O
and	O
ontology	B-MethodName
concept	O
terms	O
would	O
be	O
higher	O
for	O
the	O
similar	O
pairs	O
,	O
and	O
lower	O
for	O
the	O
dissimilar	O
pairs	O
,	O
if	O
the	O
words	O
can	O
be	O
converted	O
into	O
a	O
machine	O
processable	O
format	O
such	O
as	O
real	O
-	O
valued	O
vectors	O
.	O
After	O
pre	O
-	O
processing	O
,	O
to	O
convert	O
each	O
word	O
into	O
a	O
real	O
-	O
valued	O
vector	O
,	O
we	O
utilized	O
a	O
pre	O
-	O
trained	O
word	O
embedding	O
model	O
(	O
Chiu	O
et	O
al	O
,	O
2016	O
)	O
,	O
which	O
has	O
been	O
trained	O
on	O
PubMed	O
by	O
using	O
the	O
Word2Vec	O
tool	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
.	O
The	O
corresponding	O
word	O
vectors	O
are	O
obtained	O
for	O
each	O
word	O
by	O
using	O
this	O
previously	O
trained	O
model	O
.	O
For	O
the	O
multiword	O
named	O
entity	O
mentions	O
and	O
ontology	B-MethodName
concept	O
terms	O
,	O
the	O
vector	O
representations	O
are	O
obtained	O
by	O
averaging	O
the	O
real	O
-	O
valued	O
vectors	O
of	O
their	O
composing	O
words	O
.	O

After	O
the	O
vector	O
representations	O
are	O
obtained	O
for	O
each	O
entity	O
mention	O
and	O
for	O
each	O
ontology	B-MethodName
concept	O
term	O
,	O
the	O
semantic	B-TaskName
similarity	I-TaskName
between	O
each	O
pair	O
is	O
computed	O
by	O
using	O
the	O
cosine	O
similarity	O
.	O
For	O
each	O
entity	O
mention	O
,	O
the	O
top	O
k	O
most	O
similar	O
ontology	B-MethodName
concepts	O
are	O
retained	O
as	O
candidates	O
for	O
further	O
processing	O
,	O
i.e.	O
,	O
for	O
syntactic	O
weighting	O
based	O
re	O
-	O
ranking	O
.	O
k	O
is	O
chosen	O
as	O
5	O
based	O
on	O
the	O
results	O
obtained	O
in	O
our	O
previous	O
study	O
(	O
Karadeniz	O
andÖzgür	O
,	O
2019	O
)	O
.	O

For	O
our	O
re	O
-	O
ranking	O
approach	O
,	O
the	O
assumption	O
is	O
that	O
the	O
entity	O
mentions	O
are	O
noun	O
phrases	O
and	O
the	O
most	O
informative	O
words	O
in	O
the	O
mentions	O
are	O
the	O
heads	O
of	O
the	O
noun	O
phrases	O
.	O
We	O
used	O
the	O
Stanford	O
Parser	O
(	O
version	O
3.8.0	O
)	O
(	O
Klein	O
and	O
Manning	O
,	O
2003	O
)	O
to	O
obtain	O
the	O
corresponding	O
head	O
words	O
of	O
the	O
entity	O
mentions	O
by	O
providing	O
the	O
entity	O
mentions	O
as	O
input	O
and	O
extracting	O
the	O
syntactic	O
parses	O
of	O
the	O
mentions	O
as	O
output	O
.	O
Next	O
,	O
the	O
top	O
level	O
rightmost	O
"	O
noun	O
"	O
is	O
searched	O
in	O
the	O
tree	O
structured	O
syntactic	O
parse	O
and	O
assigned	O
as	O
the	O
head	O
of	O
the	O
mention	O
phrase	O
.	O
The	O
semantic	O
similarities	O
are	O
recomputed	O
using	O
the	O
mathematical	O
formulation	O
shown	O
in	O
Equation	O
(	O
1	O
)	O
,	O
which	O
considers	O
also	O
the	O
similarity	O
between	O
the	O
head	O
words	O
of	O
the	O
entity	O
mention	O
and	O
ontology	B-MethodName
concept	O
pair	O
.	O
In	O
Equation	O
(	O
1	O
)	O
,	O
S	O
RR	B-DatasetName
(	O
m	O
,	O
c	O
)	O
is	O
the	O
final	O
computed	O
similarity	O
between	O
mention	O
m	O
and	O
the	O
candidate	O
concept	O
c	O
,	O
and	O
S	O
S	O
is	O
the	O
semantic	B-TaskName
similarity	I-TaskName
,	O
in	O
which	O
m	O
head	O
is	O
the	O
head	O
word	O
of	O
the	O
mention	O
m	O
and	O
c	O
head	O
is	O
the	O
head	O
word	O
of	O
the	O
concept	O
c	O
,	O
S	O
S	O
(	O
m	O
,	O
c	O
)	O
is	O
the	O
similarity	O
between	O
mention	O
m	O
and	O
concept	O
c	O
computed	O
as	O
described	O
in	O
Section	O
3.1.1	O
,	O
and	O
w	O
is	O
a	O
weighting	O
parameter	O
which	O
can	O
take	O
values	O
between	O
0	B-DatasetName
and	O
1	O
.	O
w	O
is	O
chosen	O
as	O
0.25	O
based	O
on	O
the	O
results	O
reported	O
in	O
our	O
previous	O
study	O
(	O
Karadeniz	O
andÖzgür	O
,	O
2019	O
)	O
.	O

Our	O
system	O
for	O
the	O
relation	B-TaskName
extraction	I-TaskName
sub	O
-	O
task	O
is	O
based	O
on	O
the	O
naive	O
assumption	O
that	O
the	O
related	O
entities	O
for	O
most	O
of	O
the	O
relations	O
appear	O
within	O
the	O
same	O
sentence	O
.	O
Therefore	O
,	O
firstly	O
,	O
the	O
input	O
texts	O
are	O
split	O
into	O
sentences	O
using	O
the	O
NLTK	O
library	O
.	O
For	O
the	O
extraction	O
of	O
Lives	O
in	O
relations	O
,	O
all	O
the	O
sentences	O
in	O
the	O
related	O
document	O
are	O
searched	O
to	O
determine	O
whether	O
there	O
exists	O
a	O
Microorganism	O
entity	O
and	O
a	O
Habitat	O
entity	O
or	O
a	O
Microorganism	O
entity	O
and	O
a	O
Geographical	O
entity	O
in	O
the	O
corresponding	O
sentence	O
.	O
If	O
there	O
exists	O
such	O
a	O
pair	O
,	O
this	O
will	O
be	O
a	O
sign	O
of	O
a	O
Lives	O
in	O
relation	O
.	O
For	O
any	O
given	O
sentence	O
,	O
there	O
can	O
be	O
more	O
than	O
one	O
Habitat	O
entity	O
and	O
Microorganism	O
entity	O
.	O
For	O
this	O
kind	O
of	O
sentences	O
,	O
two	O
different	O
approaches	O
,	O
which	O
are	O
called	O
smart	O
matching	O
and	O
distributed	O
matching	O
,	O
are	O
applied	O
.	O
In	O
smart	O
matching	O
,	O
each	O
Habitat	O
entity	O
is	O
paired	O
with	O
the	O
closest	O
Microorganism	O
entity	O
.	O
In	O
other	O
words	O
,	O
the	O
locations	O
of	O
each	O
type	O
of	O
entities	O
in	O
the	O
sentences	O
are	O
checked	O
,	O
and	O
then	O
the	O
pairing	O
process	O
of	O
the	O
Microorganism	O
and	O
the	O
Habitat	O
entities	O
are	O
done	O
based	O
on	O
the	O
proximity	O
criteria	O
.	O
In	O
distributed	O
matching	O
,	O
on	O
the	O
other	O
hand	O
,	O
each	O
Habitat	O
entity	O
is	O
paired	O
with	O
every	O
Microorganism	O
entity	O
in	O
the	O
sentence	O
.	O
Distributed	O
matching	O
can	O
be	O
seen	O
as	O
a	O
type	O
of	O
N	O
x	O
N	O
matching	O
,	O
while	O
smart	O
matching	O
1	O
x	O
1	O
matching	O
.	O
The	O
performance	O
of	O
each	O
approach	O
is	O
tested	O
on	O
the	O
development	O
data	O
set	O
.	O
While	O
there	O
is	O
slight	O
increase	O
in	O
the	O
precision	O
,	O
the	O
recall	O
is	O
observed	O
to	O
decrease	O
considerably	O
for	O
the	O
smart	O
matching	O
method	O
(	O
see	O
Table	O
1	O
)	O
.	O
As	O
a	O
result	O
,	O
the	O
distributed	O
matching	O
approach	O
is	O
used	O
in	O
the	O
final	O
submission	O
.	O
For	O
the	O
overlapping	O
entities	O
in	O
which	O
one	O
entity	O
contains	O
another	O
,	O
some	O
relations	O
can	O
be	O
ignored	O
.	O
For	O
instance	O
,	O
for	O
the	O
sample	O
sentence	O
"	O
An	O
example	O
of	O
this	O
fact	O
is	O
the	O
presence	O
of	O
Psychrobacter	O
DNA	O
on	O
the	O
surface	O
of	O
Formaggio	O
di	O
Fossa	O
cheeses	O
"	O
,	O
the	O
Habitat	O
entity	O
"	O
surface	O
of	O
Formaggio	O
di	O
Fossa	O
cheeses	O
"	O
,	O
Habitat	O
entity	O
"	O
Formaggio	O
di	O
Fossa	O
cheeses	O
"	O
,	O
and	O
Habitat	O
entity	O
"	O
cheeses	O
"	O
are	O
overlapping	O
entities	O
.	O
In	O
this	O
case	O
,	O
it	O
would	O
not	O
be	O
appropriate	O
to	O
build	O
three	O
relations	O
such	O
as	O
"	O
Psychrobacter	O
"	O
-	O
"	O
surface	O
of	O
Formaggio	O
di	O
Fossa	O
cheeses	O
"	O
,	O
"	O
Psychrobacter	O
"	O
-	O
"	O
Formaggio	O
di	O
Fossa	O
cheeses	O
"	O
,	O
and	O
"	O
Psychrobacter	O
"	O
-	O
"	O
cheeses	O
"	O
.	O
Instead	O
of	O
extracting	O
multiple	O
relations	O
,	O
"	O
cheeses	O
"	O
can	O
be	O
ignored	O
and	O
two	O
relations	O
between	O
"	O
Psychrobacter	O
"	O
-	O
"	O
surface	O
of	O
Formaggio	O
di	O
Fossa	O
cheeses	O
"	O
and	O
"	O
Psychrobacter	O
"	O
-	O
"	O
Formaggio	O
di	O
Fossa	O
cheeses	O
"	O
are	O
extracted	O
.	O
This	O
strategy	O
,	O
where	O
the	O
shortest	O
overlapping	O
entity	O
is	O
ignored	O
,	O
is	O
called	O
as	O
the	O
soft	O
filter	O
operation	O
.	O
On	O
the	O
other	O
hand	O
,	O
the	O
strategy	O
when	O
only	O
the	O
longest	O
overlapping	O
entity	O
is	O
retained	O
and	O
the	O
remaining	O
ones	O
are	O
ignored	O
,	O
is	O
named	O
as	O
the	O
hard	O
filter	O
operation	O
.	O
In	O
hard	O
filtering	O
,	O
"	O
Psychrobacter	O
"	O
-	O
"	O
Formaggio	O
di	O
Fossa	O
cheeses	O
"	O
and	O
"	O
Psychrobacter	O
"	O
-	O
"	O
cheeses	O
"	O
are	O
ignored	O
and	O
only	O
one	O
relation	O
between	O
"	O
Psychrobacter	O
"	O
-	O
"	O
surface	O
of	O
Formaggio	O
di	O
Fossa	O
cheeses	O
"	O
is	O
extracted	O
.	O
The	O
performance	O
of	O
each	O
approach	O
is	O
tested	O
on	O
the	O
development	O
data	O
set	O
(	O
see	O
Table	O
2	O
)	O
.	O
Since	O
our	O
rule	O
-	O
based	O
system	O
for	O
relation	B-TaskName
extraction	I-TaskName
is	O
based	O
on	O
the	O
assumption	O
that	O
most	O
of	O
the	O
relations	O
appear	O
within	O
the	O
same	O
sentences	O
,	O
our	O
system	O
is	O
not	O
able	O
to	O
catch	O
the	O
relations	O
that	O
cross	O
sentence	O
boundaries	O
.	O
To	O
overcome	O
this	O
problem	O
,	O
a	O
new	O
rule	O
,	O
which	O
is	O
called	O
remote	O
matching	O
,	O
is	O
integrated	O
into	O
the	O
system	O
.	O
According	O
to	O
this	O
rule	O
,	O
if	O
there	O
exists	O
only	O
one	O
entity	O
type	O
(	O
Microorganism	O
)	O
in	O
a	O
sentence	O
,	O
and	O
within	O
a	O
context	O
window	O
of	O
three	O
sentences	O
there	O
exists	O
only	O
one	O
entity	O
(	O
Habitat	O
or	O
Geographical	O
)	O
,	O
then	O
there	O
is	O
a	O
relation	O
between	O
these	O
two	O
entities	O
.	O
The	O
performance	O
of	O
the	O
remote	O
matching	O
rule	O
is	O
tested	O
on	O
the	O
development	O
data	O
set	O
.	O
The	O
results	O
show	O
that	O
the	O
number	O
of	O
the	O
predicted	O
relations	O
increased	O
,	O
which	O
also	O
led	O
to	O
an	O
increase	O
in	O
recall	O
.	O
The	O
obtained	O
precision	O
and	O
recall	O
values	O
are	O
51.4	O
%	O
and	O
78.5	O
%	O
,	O
respectively	O
.	O

In	O
this	O
study	O
,	O
we	O
presented	O
two	O
systems	O
that	O
are	O
implemented	O
in	O
the	O
scope	O
of	O
the	O
BioNLP	O
Shared	O
Task	O
2019	O
-	O
Bacteria	O
Biotope	O
Task	O
.	O
The	O
aim	O
of	O
the	O
first	O
system	O
is	O
the	O
normalization	O
of	O
the	O
entity	O
mentions	O
in	O
a	O
biomedical	O
text	O
through	O
the	O
corresponding	O
ontology	B-MethodName
,	O
whereas	O
the	O
goal	O
of	O
the	O
second	O
system	O
is	O
the	O
extraction	O
of	O
localization	O
and	O
property	O
relations	O
between	O
the	O
related	O
entities	O
when	O
the	O
entities	O
are	O
given	O
.	O
Both	O
systems	O
are	O
unsupervised	O
in	O
the	O
sense	O
that	O
they	O
do	O
not	O
require	O
domainspecific	O
labeled	O
data	O
,	O
while	O
the	O
normalization	O
system	O
makes	O
use	O
of	O
word	B-TaskName
embeddings	I-TaskName
and	O
syntactic	O
re	O
-	O
ranking	O
.	O
According	O
to	O
the	O
official	O
evaluation	O
,	O
both	O
of	O
our	O
systems	O
achieved	O
promising	O
results	O
,	O
which	O
have	O
shown	O
that	O
the	O
proposed	O
methods	O
are	O
comparable	O
to	O
or	O
better	O
than	O
the	O
labeled	O
data	O
driven	O
deep	O
learning	O
based	O
approaches	O
used	O
in	O
the	O
shared	O
task	O
.	O

Second	O
Language	B-TaskName
Acquisition	I-TaskName
Modeling	O

We	O
present	O
the	O
task	O
of	O
second	O
language	B-TaskName
acquisition	I-TaskName
(	O
SLA	O
)	O
modeling	O
.	O
Given	O
a	O
history	O
of	O
errors	O
made	O
by	O
learners	O
of	O
a	O
second	O
language	O
,	O
the	O
task	O
is	O
to	O
predict	O
errors	O
that	O
they	O
are	O
likely	O
to	O
make	O
at	O
arbitrary	O
points	O
in	O
the	O
future	O
.	O
We	O
describe	O
a	O
large	O
corpus	O
of	O
more	O
than	O
7	O
M	O
words	O
produced	O
by	O
more	O
than	O
6k	O
learners	O
of	O
English	O
,	O
Spanish	O
,	O
and	O
French	O
using	O
Duolingo	O
,	O
a	O
popular	O
online	O
language	O
-	O
learning	O
app	O
.	O
Then	O
we	O
report	O
on	O
the	O
results	O
of	O
a	O
shared	O
task	O
challenge	O
aimed	O
studying	O
the	O
SLA	O
task	O
via	O
this	O
corpus	O
,	O
which	O
attracted	O
15	O
teams	O
and	O
synthesized	O
work	O
from	O
various	O
fields	O
including	O
cognitive	O
science	O
,	O
linguistics	O
,	O
and	O
machine	O
learning	O
.	O

As	O
computer	O
-	O
based	O
educational	O
apps	O
increase	O
in	O
popularity	O
,	O
they	O
generate	O
vast	O
amounts	O
of	O
student	O
learning	O
data	O
which	O
can	O
be	O
harnessed	O
to	O
drive	O
personalized	O
instruction	O
.	O
While	O
there	O
have	O
been	O
some	O
recent	O
advances	O
for	O
educational	O
software	O
in	O
domains	O
like	O
mathematics	O
,	O
learning	O
a	O
language	O
is	O
more	O
nuanced	O
,	O
involving	O
the	O
interaction	O
of	O
lexical	O
knowledge	O
,	O
morpho	O
-	O
syntactic	O
processing	O
,	O
and	O
several	O
other	O
skills	O
.	O
Furthermore	O
,	O
most	O
work	O
that	O
has	O
applied	O
natural	O
language	O
processing	O
to	O
language	O
learner	O
data	O
has	O
focused	O
on	O
intermediate	O
-	O
toadvanced	O
students	O
of	O
English	O
,	O
particularly	O
in	O
assessment	O
settings	O
.	O
Much	O
less	O
work	O
has	O
been	O
devoted	O
to	O
beginners	O
,	O
learners	O
of	O
languages	O
other	O
than	O
English	O
,	O
or	O
ongoing	O
study	O
over	O
time	O
.	O
We	O
propose	O
second	O
language	B-TaskName
acquisition	I-TaskName
(	O
SLA	O
)	O
modeling	O
as	O
a	O
new	O
computational	O
task	O
to	O
help	O
broaden	O
our	O
understanding	O
in	O
this	O
area	O
.	O
First	O
,	O
we	O
describe	O
a	O
new	O
corpus	O
of	O
language	O
learner	O
data	O
,	O
containing	O
more	O
than	O
7.1	O
M	O
words	O
,	O
annotated	O
for	O
production	O
errors	O
that	O
were	O
made	O
by	O
more	O
than	O
6.4k	O
learners	O
of	O
English	O
,	O
Spanish	O
,	O
and	O
French	O
,	O
during	O
their	O
first	O
30	O
days	O
of	O
learning	O
with	O
Duolingo	O
(	O
a	O
popular	O
online	O
language	O
-	O
learning	O
app	O
)	O
.	O
Then	O
we	O
report	O
on	O
the	O
results	O
of	O
a	O
"	O
shared	O
task	O
"	O
challenge	O
organized	O
by	O
the	O
authors	O
using	O
this	O
SLA	O
modeling	O
corpus	O
,	O
which	O
brought	O
together	O
15	O
research	O
teams	O
.	O
Our	O
goal	O
for	O
this	O
work	O
is	O
threefold	O
:	O
(	O
1	O
)	O
to	O
synthesize	O
years	O
of	O
research	O
in	O
cognitive	O
science	O
,	O
linguistics	O
,	O
and	O
machine	O
learning	O
,	O
(	O
2	O
)	O
to	O
facilitate	O
cross	O
-	O
dialog	O
among	O
these	O
disciplines	O
through	O
a	O
common	O
large	O
-	O
scale	O
empirical	O
task	O
,	O
and	O
in	O
so	O
doing	O
(	O
3	O
)	O
to	O
shed	O
light	O
on	O
the	O
most	O
effective	O
approaches	O
to	O
SLA	O
modeling	O
.	O

Sample	O
data	O
from	O
the	O
resulting	O
corpus	O
can	O
be	O
found	O
in	O
Figure	O
3	O
.	O
Each	O
token	O
from	O
the	O
reference	O
answer	O
is	O
labeled	O
according	O
to	O
the	O
alignment	O
with	O
the	O
learner	O
's	O
response	O
(	O
the	O
final	O
column	O
:	O
0	B-DatasetName
for	O
correct	O
and	O
1	O
for	O
incorrect	O
)	O
.	O
Tokens	O
are	O
grouped	O
together	O
by	O
exercise	O
,	O
including	O
user	O
-	O
,	O
exercise	O
-	O
,	O
and	O
session	O
-	O
level	O
meta	O
-	O
data	O
in	O
the	O
previous	O
line	O
(	O
marked	O
by	O
the	O
#	O
character	O
)	O
.	O
We	O
included	O
all	O
exercises	O
done	O
by	O
the	O
users	O
sampled	O
from	O
the	O
30	O
-	O
day	O
data	O
collection	O
window	O
.	O
The	O
overall	O
format	O
is	O
inspired	O
by	O
the	O
Universal	B-DatasetName
Dependencies	I-DatasetName
(	O
UD	B-DatasetName
)	O
format	O
2	O
.	O
Column	O
1	O
is	O
a	O
unique	O
B64	O
-	O
encoded	O
token	O
ID	O
,	O
column	O
2	O
is	O
a	O
token	O
(	O
word	O
)	O
,	O
and	O
columns	O
3	O
-	O
6	O
are	O
morpho	O
-	O
syntactic	O
features	O
from	O
the	O
UD	B-DatasetName
tag	O
set	O
(	O
part	O
of	O
speech	O
,	O
morphology	O
features	O
,	O
and	O
dependency	O
parse	O
labels	O
and	O
edges	O
)	O
.	O
These	O
were	O
generated	O
by	O
processing	O
the	O
aligned	O
reference	O
answers	O
with	O
Google	B-DatasetName
SyntaxNet	O
(	O
Andor	O
et	O
al	O
,	O
2016	O
)	O
.	O
Because	O
UD	B-DatasetName
tags	O
are	O
meant	O
to	O
be	O
language	O
-	O
agnostic	O
,	O
it	O
was	O
our	O
goal	O
to	O
help	O
make	O
cross	O
-	O
lingual	O
SLA	O
modeling	O
more	O
straightforward	O
by	O
providing	O
these	O
features	O
.	O
Exercise	O
meta	O
-	O
data	O
includes	O
the	O
following	O
:	O
user	O
:	O
8	O
-	O
character	O
unique	O
anonymous	O
user	O
ID	O
for	O
each	O
learner	O
(	O
B64	O
-	O
encoded	O
)	O
countries	O
:	O
2	O
-	O
character	O
ISO	O
country	O
codes	O
from	O
which	O
this	O
learner	O
has	O
done	O
exercises	O
days	O
:	O
number	O
of	O
days	O
since	O
the	O
learner	O
started	O
learning	O
this	O
language	O
on	O
Duolingo	O
client	O
:	O
session	O
device	O
platform	O
session	O
:	O
session	O
type	O
(	O
e.g.	O
,	O
lesson	O
or	O
practice	O
)	O
format	O
:	O
exercise	O
format	O
(	O
see	O
Figure	O
1	O
)	O
time	O
:	O
the	O
time	O
(	O
in	O
seconds	O
)	O
it	O
took	O
the	O
learner	O
to	O
submit	O
a	O
response	O
for	O
this	O
exercise	O
.	O
Lesson	O
sessions	O
(	O
about	O
77	O
%	O
of	O
the	O
data	O
set	O
)	O
are	O
where	O
new	O
words	O
or	O
concepts	O
are	O
introduced	O
,	O
although	O
lessons	O
also	O
include	O
previously	O
-	O
learned	O
material	O
(	O
e.g.	O
,	O
each	O
exercise	O
attempts	O
to	O
introduce	O
only	O
one	O
new	O
word	O
or	O
inflection	O
,	O
so	O
all	O
other	O
tokens	O
should	O
have	O
been	O
seen	O
by	O
the	O
student	O
be	O
-	O
.	O
fore	O
)	O
.	O
Practice	O
sessions	O
(	O
22	O
%	O
)	O
should	O
contain	O
only	O
previously	O
-	O
seen	O
words	O
and	O
concepts	O
.	O
Test	O
sessions	O
(	O
1	O
%	O
)	O
are	O
mini	O
-	O
quizzes	O
that	O
allow	O
a	O
student	O
to	O
skip	O
out	O
of	O
a	O
single	O
skill	O
in	O
the	O
curriculum	O
(	O
i.e.	O
,	O
the	O
student	O
may	O
have	O
never	O
seen	O
this	O
content	O
before	O
in	O
the	O
Duolingo	O
app	O
,	O
but	O
may	O
well	O
have	O
had	O
prior	O
knowledge	O
before	O
starting	O
the	O
course	O
)	O
.	O
It	O
is	O
worth	O
mentioning	O
that	O
for	O
the	O
shared	O
task	O
,	O
we	O
did	O
not	O
provide	O
actual	O
learner	O
responses	O
,	O
only	O
the	O
closest	O
reference	O
answers	O
.	O
Releasing	O
such	O
data	O
(	O
at	O
least	O
in	O
the	O
TEST	O
set	O
)	O
would	O
by	O
definition	O
give	O
away	O
the	O
labels	O
and	O
might	O
undermine	O
the	O
task	O
.	O
However	O
,	O
we	O
plan	O
to	O
release	O
a	O
future	O
version	O
of	O
the	O
corpus	O
that	O
is	O
enhanced	O
with	O
additional	O
meta	O
-	O
data	O
,	O
including	O
the	O
actual	O
learner	O
responses	O
.	O

SLA	O
modeling	O
is	O
a	O
rich	O
problem	O
,	O
and	O
presents	O
a	O
opportunity	O
to	O
synthesize	O
work	O
from	O
various	O
subfields	O
in	O
cognitive	O
science	O
,	O
linguistics	O
,	O
and	O
machine	O
learning	O
.	O
This	O
section	O
highlights	O
a	O
few	O
key	O
concepts	O
from	O
these	O
fields	O
,	O
and	O
how	O
they	O
relate	O
to	O
the	O
approaches	O
taken	O
by	O
shared	O
task	O
participants	O
.	O
Item	O
response	O
theory	O
(	O
IRT	O
)	O
is	O
a	O
common	O
psychometric	O
modeling	O
approach	O
used	O
in	O
educational	O
software	O
(	O
e.g.	O
,	O
Chen	O
et	O
al	O
,	O
2005	O
)	O
.	O
In	O
its	O
simplest	O
form	O
(	O
Rasch	O
,	O
1980	O
)	O
,	O
an	O
IRT	O
model	O
is	O
a	O
logistic	B-MethodName
regression	I-MethodName
with	O
two	O
weights	O
:	O
one	O
representing	O
the	O
learner	O
's	O
ability	O
(	O
i.e.	O
,	O
user	O
ID	O
)	O
,	O
and	O
the	O
other	O
representing	O
the	O
difficulty	O
of	O
the	O
exercise	O
or	O
test	O
item	O
(	O
i.e.	O
,	O
token	O
ID	O
)	O
.	O
An	O
extension	O
of	O
this	O
idea	O
is	O
the	O
additive	O
factor	O
model	O
(	O
Cen	O
et	O
al	O
,	O
2008	O
)	O
which	O
adds	O
additional	O
"	O
knowledge	O
components	O
"	O
(	O
e.g.	O
,	O
lexical	O
,	O
morphological	O
,	O
or	O
syntactic	O
features	O
)	O
.	O
Teams	O
that	O
employed	O
linear	O
models	O
(	O
including	O
our	O
baseline	O
)	O
are	O
essentially	O
all	O
additive	O
factor	O
IRT	O
models	O
.	O
For	O
decades	O
,	O
tutoring	O
systems	O
have	O
also	O
employed	O
sequence	O
models	O
like	O
HMMs	O
to	O
perform	O
knowledge	B-TaskName
tracing	I-TaskName
(	O
Corbett	O
and	O
Anderson	O
,	O
1995	O
)	O
,	O
a	O
way	O
of	O
estimating	O
a	O
learner	O
's	O
mastery	O
of	O
knowledge	O
over	O
time	O
.	O
RNN	O
-	O
based	O
approaches	O
that	O
encode	O
user	O
performance	O
over	O
time	O
(	O
i.e.	O
,	O
that	O
span	O
across	O
exercises	O
)	O
are	O
therefore	O
variants	O
of	O
deep	O
knowledge	B-TaskName
tracing	I-TaskName
(	O
Piech	O
et	O
al	O
,	O
2015	O
)	O
.	O
Relatedly	O
,	O
the	O
spacing	O
effect	O
(	O
Dempster	O
,	O
1989	O
)	O
is	O
the	O
observation	O
that	O
people	O
will	O
not	O
only	O
learn	O
but	O
also	O
forget	O
over	O
time	O
,	O
and	O
they	O
remember	O
more	O
effectively	O
through	O
scheduled	O
practices	O
that	O
are	O
spaced	O
out	O
.	O
Settles	O
and	O
Meeder	O
(	O
2016	O
)	O
and	O
Ridgeway	O
et	O
al	O
(	O
2017	O
)	O
recently	O
proposed	O
non	O
-	O
linear	O
regressions	O
that	O
explicitly	O
encode	O
the	O
rate	O
of	O
forgetting	O
as	O
part	O
of	O
a	O
decision	O
surface	O
,	O
however	O
none	O
of	O
the	O
current	O
teams	O
chose	O
to	O
do	O
this	O
.	O
Instead	O
,	O
forgetting	O
was	O
either	O
modeled	O
through	O
engineered	O
features	O
(	O
e.g.	O
,	O
user	O
/	O
token	O
histories	O
)	O
,	O
or	O
opaquely	O
handled	O
by	O
sequential	O
RNN	O
architectures	O
.	O
SLA	O
modeling	O
also	O
bears	O
some	O
similarity	O
to	O
research	O
in	O
grammatical	B-TaskName
error	I-TaskName
detection	I-TaskName
(	O
Leacock	O
et	O
al	O
,	O
2010	O
)	O
and	O
correction	O
(	O
Ng	O
et	O
al	O
,	O
2013	O
)	O
.	O
For	O
these	O
tasks	O
,	O
a	O
model	O
is	O
given	O
a	O
(	O
possibly	O
ill	O
-	O
formed	O
)	O
sequence	O
of	O
words	O
produced	O
by	O
a	O
learner	O
,	O
and	O
the	O
task	O
is	O
to	O
identify	O
which	O
are	O
mistakes	O
.	O
SLA	O
modeling	O
is	O
in	O
some	O
sense	O
the	O
opposite	O
:	O
given	O
a	O
well	O
-	O
formed	O
sequence	O
of	O
words	O
that	O
a	O
learner	O
should	O
be	O
able	O
to	O
produce	O
,	O
identify	O
where	O
they	O
are	O
likely	O
to	O
make	O
mistakes	O
.	O
Given	O
these	O
similarities	O
,	O
a	O
few	O
teams	O
adapted	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
GEC	O
/	O
GED	B-DatasetName
approaches	O
to	O
create	O
their	O
SLA	O
modeling	O
systems	O
.	O
Finally	O
,	O
multitask	O
learning	O
(	O
e.g.	O
,	O
Caruana	O
,	O
1997	O
)	O
is	O
the	O
idea	O
that	O
machine	O
learning	O
systems	O
can	O
do	O
better	O
at	O
multiple	O
related	O
tasks	O
by	O
trying	O
to	O
solve	O
them	O
simultaneously	O
.	O
For	O
example	O
,	O
recent	O
work	O
in	O
machine	B-TaskName
translation	I-TaskName
has	O
demonstrated	O
gains	O
through	O
learning	O
to	O
translate	O
multiple	O
languages	O
with	O
a	O
unified	O
model	O
(	O
Dong	O
et	O
al	O
,	O
2015	O
)	O
.	O
Similarly	O
,	O
the	O
three	O
language	O
tracks	O
in	O
this	O
work	O
presented	O
an	O
opportunity	O
to	O
explore	O
a	O
unified	O
multitask	O
framework	O
,	O
which	O
a	O
few	O
teams	O
did	O
with	O
positive	O
results	O
.	O

In	O
this	O
section	O
,	O
we	O
analyze	O
the	O
various	O
modeling	O
choices	O
explored	O
by	O
the	O
different	O
teams	O
in	O
order	O
to	O
shed	O
light	O
on	O
what	O
kinds	O
of	O
algorithmic	O
and	O
feature	B-TaskName
engineering	I-TaskName
decisions	O
appear	O
to	O
be	O
useful	O
for	O
the	O
SLA	O
modeling	O
task	O
.	O

Popularity	O
Effect	O
.	O
ture	O
the	O
same	O
phenomena	O
)	O
.	O
We	O
also	O
include	O
each	O
feature	O
's	O
popularity	O
and	O
an	O
effect	O
estimate	O
5	O
.	O
Broadly	O
speaking	O
,	O
results	O
suggest	O
that	O
feature	B-TaskName
engineering	I-TaskName
had	O
a	O
much	O
smaller	O
impact	O
on	O
system	O
performance	O
than	O
the	O
choice	O
of	O
learning	O
algorithm	O
.	O
Only	O
"	O
response	O
time	O
"	O
and	O
"	O
days	O
in	O
course	O
"	O
showed	O
even	O
marginally	O
significant	O
trends	O
.	O
Of	O
particular	O
interest	O
is	O
the	O
observation	O
that	O
morpho	O
-	O
syntactic	O
features	O
(	O
described	O
in	O
2.4	O
)	O
actually	O
seem	O
to	O
have	O
weakly	O
negative	O
effects	O
.	O
This	O
echoes	O
singsound	O
's	O
finding	O
that	O
their	O
linguistic	O
encoder	O
contributed	O
the	O
least	O
to	O
system	O
performance	O
,	O
and	O
Cambridge	B-DatasetName
determined	O
through	O
ablation	O
studies	O
that	O
these	O
features	O
in	O
fact	O
hurt	O
their	O
system	O
.	O
One	O
reasonable	O
explanation	O
is	O
that	O
these	O
automaticallygenerated	O
features	O
contain	O
too	O
many	O
systematic	O
parsing	O
errors	O
to	O
provide	O
value	O
.	O
(	O
Note	O
that	O
NYU	O
artificially	O
introduced	O
punctuation	O
to	O
the	O
exercises	O
and	O
re	O
-	O
parsed	O
the	O
data	O
in	O
their	O
work	O
.	O
)	O
As	O
for	O
newly	O
-	O
engineered	O
features	O
,	O
word	O
information	O
such	O
as	O
frequency	O
,	O
semantic	O
embeddings	O
,	O
and	O
stemming	O
were	O
popular	O
.	O
It	O
may	O
be	O
that	O
these	O
features	O
showed	O
such	O
little	O
return	O
because	O
our	O
corpus	O
was	O
too	O
biased	O
toward	O
beginners	O
-	O
thus	O
representing	O
a	O
very	O
narrow	O
sample	O
of	O
language	O
-	O
for	O
these	O
features	O
to	O
be	O
meaningful	O
.	O
Cognate	O
features	O
were	O
an	O
interesting	O
idea	O
used	O
by	O
a	O
few	O
teams	O
,	O
and	O
may	O
have	O
been	O
more	O
useful	O
if	O
the	O
data	O
included	O
users	O
from	O
a	O
wider	O
variety	O
of	O
different	O
L1	O
language	O
backgrounds	O
.	O
Spaced	O
repetition	O
features	O
also	O
exhibited	O
marginal	O
(	O
but	O
statistically	O
insignificant	O
)	O
gains	O
.	O
We	O
posit	O
that	O
the	O
30	O
-	O
day	O
window	O
we	O
used	O
for	O
data	O
collection	O
was	O
simply	O
not	O
long	O
enough	O
for	O
these	O
features	O
to	O
capture	O
more	O
longterm	O
learning	O
(	O
and	O
forgetting	O
)	O
trends	O
.	O

In	O
this	O
work	O
,	O
we	O
presented	O
the	O
task	O
of	O
second	O
language	B-TaskName
acquisition	I-TaskName
(	O
SLA	O
)	O
modeling	O
,	O
described	O
a	O
large	O
data	O
set	O
for	O
studying	O
this	O
task	O
,	O
and	O
reported	O
on	O
the	O
results	O
of	O
a	O
shared	O
task	O
challenge	O
that	O
explored	O
this	O
new	O
domain	O
.	O
The	O
task	O
attracted	O
strong	O
participation	O
from	O
15	O
teams	O
,	O
who	O
represented	O
a	O
wide	O
variety	O
of	O
fields	O
including	O
cognitive	O
science	O
,	O
linguistics	O
,	O
and	O
machine	O
learning	O
.	O
Among	O
our	O
key	O
findings	O
is	O
the	O
observation	O
that	O
,	O
for	O
this	O
particular	O
formulation	O
of	O
the	O
task	O
,	O
the	O
choice	O
of	O
learning	O
algorithm	O
appears	O
to	O
be	O
more	O
important	O
than	O
clever	O
feature	B-TaskName
engineering	I-TaskName
.	O
In	O
particular	O
,	O
the	O
most	O
effective	O
teams	O
employed	O
sequence	O
models	O
(	O
e.g.	O
,	O
RNNs	O
)	O
that	O
can	O
capture	O
user	O
performance	O
over	O
time	O
,	O
and	O
tree	O
ensembles	O
(	O
e.g.	O
,	O
GBDTs	O
)	O
that	O
can	O
capture	O
non	O
-	O
linear	O
relationships	O
among	O
features	O
.	O
Furthermore	O
,	O
using	O
a	O
multitask	O
framework	O
-	O
in	O
this	O
case	O
,	O
a	O
unified	O
model	O
that	O
leverages	O
data	O
from	O
all	O
three	O
language	O
tracks	O
-	O
can	O
provide	O
further	O
improvements	O
.	O
Still	O
,	O
many	O
teams	O
opted	O
for	O
a	O
simpler	O
algorithm	O
(	O
e.g.	O
,	O
logistic	B-MethodName
regression	I-MethodName
)	O
and	O
concentrated	O
instead	O
on	O
more	O
psychologically	O
-	O
motivated	O
features	O
.	O
While	O
these	O
teams	O
did	O
not	O
always	O
perform	O
as	O
well	O
,	O
several	O
demonstrated	O
through	O
ablation	O
studies	O
that	O
these	O
features	O
can	O
be	O
useful	O
within	O
the	O
limitations	O
of	O
the	O
algorithm	O
.	O
It	O
is	O
possible	O
that	O
the	O
constraints	O
of	O
the	O
SLA	O
modeling	O
data	O
set	O
(	O
beginner	O
language	O
,	O
homogeneous	O
L1	O
language	O
background	O
,	O
short	O
30	O
-	O
day	O
time	O
frame	O
,	O
etc	O
.	O
)	O
prevented	O
these	O
features	O
from	O
being	O
more	O
useful	O
across	O
different	O

Gaze	O
fixation	O
patterns	O
have	O
been	O
shown	O
to	O
strongly	O
reflect	O
the	O
online	O
cognitive	O
processing	O
demands	O
of	O
1	O
More	O
precisely	O
,	O
our	O
measure	O
of	O
dissimilarity	O
between	O
experimental	O
conditions	O
is	O
analogous	O
to	O
ground	O
distance	O
and	O
dissimilarity	O
between	O
RDMs	O
to	O
earth	O
mover	O
's	O
distance	O
.	O
human	O
readers	O
(	O
Raney	O
et	O
al	O
,	O
2014	O
;	O
Ashby	O
et	O
al	O
,	O
2005	O
)	O
and	O
to	O
be	O
dependent	O
upon	O
a	O
number	O
of	O
linguistic	O
factors	O
(	O
Van	O
Gompel	O
,	O
2007	O
)	O
.	O
Specifically	O
,	O
it	O
has	O
been	O
demonstrated	O
that	O
word	O
frequency	O
,	O
syntactic	O
complexity	O
,	O
and	O
lexical	O
ambiguity	O
play	O
a	O
strong	O
part	O
in	O
determining	O
which	O
sentences	O
are	O
difficult	O
for	O
humans	O
to	O
process	O
(	O
Rayner	O
and	O
Duffy	O
,	O
1986	O
;	O
Duffy	O
et	O
al	O
,	O
1988	O
;	O
Levy	O
,	O
2008	O
)	O
.	O
Using	O
the	O
RSA	O
framework	O
,	O
we	O
aim	O
to	O
explore	O
how	O
gaze	O
fixation	O
patterns	O
and	O
the	O
linguistic	O
factors	O
associated	O
with	O
sentence	O
processing	O
difficulty	O
relate	O
to	O
the	O
representational	O
spaces	O
of	O
popular	O
language	O
encoders	O
.	O
Namely	O
,	O
we	O
hypothesize	O
that	O
,	O
for	O
a	O
given	O
sentence	O
,	O
disagreement	O
between	O
hidden	O
layers	O
corresponds	O
to	O
processing	O
difficulty	O
.	O
Because	O
layer	O
disagreement	O
for	O
a	O
sentence	O
measures	O
the	O
extent	O
to	O
which	O
two	O
layers	O
(	O
e.g.	O
within	O
BERT	B-MethodName
)	O
disagree	O
with	O
each	O
other	O
about	O
the	O
pairwise	O
similarity	O
of	O
the	O
sentence	O
(	O
with	O
other	O
sentences	O
in	O
the	O
corpus	O
)	O
,	O
a	O
sentence	O
with	O
high	O
layer	O
disagreement	O
will	O
have	O
unstable	O
similarity	O
relationships	O
to	O
other	O
sentences	O
in	O
the	O
corpus	O
.	O
This	O
indicates	O
that	O
it	O
has	O
a	O
degraded	O
encoder	O
representation	O
.	O
Going	O
further	O
,	O
we	O
also	O
hypothesize	O
that	O
models	O
'	O
representations	O
of	O
said	O
sentences	O
may	O
be	O
confounded	O
,	O
in	O
part	O
,	O
by	O
factors	O
that	O
are	O
known	O
to	O
influence	O
humans	O
.	O
Eye	O
-	O
tracking	O
data	O
For	O
our	O
experiments	O
,	O
we	O
make	O
use	O
of	O
the	O
Dundee	O
eye	O
-	O
tracking	O
corpus	O
(	O
Kennedy	O
et	O
al	O
,	O
2003	O
)	O
,	O
the	O
English	O
part	O
of	O
which	O
consists	O
of	O
eye	O
-	O
movement	O
data	O
recorded	O
as	O
10	O
native	O
participants	O
read	O
2	O
,	O
368	O
sentences	O
from	O
20	O
newspaper	O
articles	O
.	O
We	O
consider	O
the	O
following	O
fixation	O
features	O
:	O
TOTAL	O
FIXATION	O
DURATION	O
and	O
FIRST	O
PASS	B-DatasetName
DURATION	O
.	O
For	O
each	O
of	O
the	O
features	O
,	O
we	O
first	O
take	O
the	O
average	O
of	O
the	O
measurements	O
recorded	O
for	O
all	O
10	O
participants	O
per	O
word	O
,	O
then	O
ob	O
-	O
tain	O
sentence	O
-	O
level	O
annotations	O
by	O
summing	O
the	O
measurements	O
of	O
all	O
words	O
in	O
a	O
sentence	O
and	O
dividing	O
by	O
its	O
length	O
.	O
The	O
result	O
of	O
this	O
is	O
two	O
vectors	O
V	O
totf	O
ix	O
and	O
V	O
f	O
irstpass	O
of	O
length	O
2	O
,	O
368	O
,	O
where	O
each	O
cell	O
in	O
the	O
vector	O
corresponds	O
to	O
a	O
sentence	O
's	O
average	O
total	O
fixation	O
and	O
average	O
first	O
pass	O
duration	O
,	O
respectively	O
.	O
Syntactic	O
complexity	O
,	O
word	O
frequency	O
,	O
and	O
lexical	O
ambiguity	O
We	O
also	O
consider	O
the	O
three	O
following	O
linguistic	O
features	O
which	O
affect	O
processing	O
difficulty	O
.	O
For	O
each	O
of	O
the	O
following	O
the	O
result	O
is	O
also	O
a	O
vector	O
of	O
length	O
2	O
,	O
368	O
where	O
each	O
cell	O
corresponds	O
to	O
a	O
sentence	O
:	O
a.	O
the	O
average	O
word	O
log	O
frequency	O
per	O
sentence	O
extracted	O
from	O
the	O
British	O
National	O
Corpus	O
(	O
Leech	O
,	O
1992	O
)	O
,	O
V	O
logF	O
req	O
.	O
.	O
b.	O
the	O
average	O
number	O
of	O
senses	O
per	O
word	O
per	O
sentence	O
extracted	O
from	O
WordNet	O
(	O
Miller	O
,	O
1995	O
)	O
,	O
V	O
wordSense	O
.	O
c.	O
Yngve	O
scores	O
,	O
a	O
standard	O
measure	O
of	O
syntactic	O
complexity	O
based	O
on	O
cognitive	O
load	O
(	O
Yngve	O
,	O
1960	O
)	O
,	O
V	O
Y	O
ngve	O
.	O
Pretrained	O
encoders	O
We	O
conduct	O
our	O
analysis	O
on	O
pretrained	O
BERT	B-MethodName
-	O
large	O
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
and	O
ELMo	B-MethodName
(	O
Peters	O
et	O
al	O
,	O
2018	O
)	O
,	O
two	O
widely	O
employed	O
contextual	O
sentence	O
encoders	O
.	O
To	O
obtain	O
a	O
representation	O
of	O
a	O
sentence	O
from	O
a	O
given	O
layer	O
L	O
,	O
we	O
perform	O
mean	O
-	O
pooling	O
over	O
the	O
time	O
-	O
steps	O
which	O
correspond	O
to	O
the	O
words	O
of	O
a	O
sentence	O
,	O
obtaining	O
a	O
vector	O
representation	O
of	O
the	O
sentence	O
.	O
Meanpooling	O
is	O
a	O
common	O
approach	O
for	O
obtaining	O
vector	O
representations	O
of	O
sentences	O
for	O
downstream	O
tasks	O
(	O
Peters	O
et	O
al	O
,	O
2018	O
;	O
Conneau	O
et	O
al	O
,	O
2017b	O
)	O
.	O
We	O
refer	O
to	O
ELMo	B-MethodName
's	O
lowest	O
layer	O
as	O
E1	O
,	O
BERT	B-MethodName
's	O
11th	O
layer	O
as	O
B11	O
,	O
etc	O
.	O
RDMs	O
We	O
construct	O
an	O
RDM	O
(	O
see	O
2	O
)	O
for	O
each	O
contextual	O
encoder	O
's	O
layers	O
.	O
Each	O
RDM	O
is	O
a	O
2	O
,	O
368	O
×	O
2	O
,	O
368	O
matrix	O
which	O
represents	O
the	O
dissimilarity	O
structure	O
of	O
the	O
layer	O
,	O
(	O
i.e.	O
,	O
each	O
row	O
vector	O
in	O
the	O
matrix	O
contains	O
the	O
dissimilarity	O
of	O
a	O
given	O
sentence	O
to	O
every	O
other	O
sentence	O
)	O
.	O
We	O
then	O
compute	O
the	O
correlations	O
between	O
the	O
two	O
different	O
RDMs	O
.	O
For	O
our	O
evaluation	O
of	O
how	O
well	O
the	O
representational	O
geometry	O
of	O
a	O
layer	O
correlates	O
to	O
another	O
,	O
we	O
employ	O
Kendall	O
's	O
τ	O
A	O
as	O
suggested	O
in	O
Nili	O
et	O
al	O
(	O
2014	O
)	O
,	O
computing	O
the	O
pairwise	O
correlation	O
for	O
each	O
two	O
corresponding	O
rows	O
in	O
two	O
RDMs	O
.	O
This	O
second	O
-	O
order	O
analysis	O
gives	O
us	O
a	O
pairwise	O
relational	O
similarity	O
vector	O
V	O
Corr	O
L	O
i	O
−L	O
j	O
of	O
length	O
2	O
,	O
368	O
,	O
which	O
has	O
the	O
correlations	O
between	O
two	O
layers	O
L	O
i	O
and	O
L	O
j	O
's	O
RDMs	O
for	O
each	O
of	O
the	O
sentences	O
.	O
Third	O
-	O
order	O
analysis	O
The	O
final	O
part	O
of	O
our	O
analysis	O
involves	O
computing	O
correlations	O
(	O
Spearman	O
's	O
ρ	O
)	O
of	O
{	O
V	O
Corr	O
L	O
i	O
−L	O
j	O
,	O
V	O
logF	O
req	O
,	O
V	O
Y	O
ngve	O
,	O
V	O
wordSense	O
}	O
with	O
each	O
of	O
V	O
totf	O
ix	O
and	O
V	O
f	O
irstpass	O
.	O
The	O
results	O
from	O
this	O
are	O
shown	O
in	O
Table	O
1	O
.	O
The	O
top	O
section	O
of	O
the	O
table	O
shows	O
correlations	O
when	O
L	O
i	O
and	O
L	O
j	O
are	O
the	O
three	O
final	O
adjacent	O
layers	O
in	O
BERT	B-MethodName
and	O
ELMo	B-MethodName
.	O
The	O
middle	O
section	O
shows	O
the	O
results	O
for	O
top	O
three	O
BERT	B-MethodName
layer	O
pairs	O
L	O
i	O
and	O
L	O
j	O
which	O
maximize	O
the	O
correlation	O
scores	O
.	O
The	O
final	O
section	O
shows	O
correlation	O
with	O
the	O
linguistic	O
features	O
.	O
Finally	O
,	O
Figure	O
2	O
shows	O
Spearman	O
's	O
ρ	O
correlations	O
between	O
V	O
Corr	O
L	O
i	O
−L	O
j	O
and	O
each	O
of	O
V	O
totf	O
ix	O
,	O
and	O
V	O
Y	O
ngve	O
for	O
all	O
combinations	O
of	O
the	O
24	O
BERT	B-MethodName
layers	O
.	O

Our	O
results	O
show	O
highly	O
significant	O
negative	O
correlations	O
between	O
V	O
Corr	O
L	O
i	O
−L	O
j	O
and	O
sentence	O
gaze	O
fixation	O
times	O
.	O
These	O
findings	O
confirm	O
the	O
hypothesis	O
that	O
the	O
sentences	O
that	O
are	O
most	O
challenging	O
for	O
humans	O
to	O
process	O
,	O
are	O
the	O
sentences	O
(	O
a	O
)	O
the	O
layers	O
of	O
BERT	B-MethodName
disagree	O
most	O
on	O
among	O
themselves	O
;	O
and	O
(	O
b	O
)	O
that	O
ELMo	B-MethodName
and	O
BERT	B-MethodName
disagree	O
most	O
on	O
,	O
indicating	O
that	O
there	O
may	O
be	O
common	O
factors	O
which	O
affect	O
human	O
processing	O
difficulty	O
and	O
result	O
in	O
disagreement	O
between	O
layers	O
.	O
By	O
Layer	O
disagreement	O
we	O
refer	O
to	O
the	O
expression	O
1	O
−	O
V	O
Corr	O
L	O
i	O
−L	O
j	O
.	O
It	O
is	O
important	O
to	O
note	O
that	O
these	O
encoders	O
are	O
trained	O
with	O
a	O
language	B-TaskName
modelling	I-TaskName
objective	O
,	O
unlike	O
models	O
where	O
reading	O
behaviour	O
is	O
explicitly	O
modelled	O
(	O
Hahn	O
and	O
Keller	O
,	O
2016	O
)	O
or	O
predicted	O
(	O
Matthies	O
and	O
Søgaard	O
,	O
2013	O
)	O
.	O
Indeed	O
,	O
the	O
similarities	O
here	O
emerge	O
naturally	O
as	O
a	O
function	O
of	O
the	O
task	O
being	O
performed	O
.	O
This	O
can	O
be	O
seen	O
as	O
analogous	O
to	O
the	O
case	O
of	O
similarities	O
observed	O
between	O
neural	O
networks	O
trained	O
to	O
perform	O
object	B-TaskName
recognition	I-TaskName
and	O
spatio	O
-	O
temporal	O
cortical	O
dynamics	O
(	O
Cichy	O
et	O
al	O
,	O
2016	O
)	O
.	O
Syntactic	O
complexity	O
Figure	O
2	O
shows	O
that	O
,	O
for	O
all	O
combinations	O
of	O
BERT	B-MethodName
layers	O
,	O
total	O
fixation	O
time	O
and	O
Yngve	O
scores	O
have	O
strong	O
negative	O
and	O
positive	O
correlations	O
(	O
respectively	O
)	O
with	O
layer	O
disagreement	O
.	O
Furthermore	O
,	O
we	O
observe	O
that	O
disagreement	O
between	O
middle	O
layers	O
seems	O
to	O
show	O
the	O
strongest	O
correlation	O
with	O
Yngve	O
scores	O
.	O
To	O
confirm	O
this	O
,	O
we	O
split	O
the	O
correlations	O
into	O
four	O
groups	O
:	O
"	O
low	O
"	O
(	O
i	O
,	O
j	O
[	O
1	O
,	O
8	O
]	O
)	O
,	O
"	O
middle	O
"	O
(	O
i	O
,	O
j	O
1	O
:	O
Spearman	O
's	O
ρ	O
between	O
V	O
Corr	O
L	O
i	O
−L	O
j	O
,	O
V	O
logF	O
req	O
.	O
,	O
VwordSense	O
,	O
V	O
Y	O
ngve	O
and	O
each	O
of	O
V	O
totf	O
ix	O
and	O
V	O
f	O
irstpass	O
.	O
All	O
correlations	O
significant	O
with	O
p	O
<	O
0.0001	O
after	O
Bonferroni	O
correction	O
unless	O
marked	O
with	O
*	O
.	O
[	O
9	O
,	O
16	O
]	O
)	O
,	O
"	O
high	O
"	O
(	O
i	O
,	O
j	O
[	O
17	O
,	O
24	O
]	O
)	O
,	O
and	O
"	O
out	O
"	O
(	O
|	O
i	O
−	O
j	O
|	O
>	O
7	O
)	O
,	O
with	O
the	O
latter	O
representing	O
out	O
-	O
ofgroup	O
correlations	O
(	O
e.g.	O
Corr	O
L	O
1	O
−L	O
24	O
)	O
.	O
To	O
account	O
for	O
correlations	O
between	O
disagreeing	O
adjacent	O
layers	O
(	O
e.g.	O
|	O
i	O
−	O
j	O
|	O
=	O
1	O
)	O
and	O
Yngve	O
scores	O
being	O
higher	O
(	O
as	O
a	O
possible	O
confounding	O
factor	O
)	O
,	O
we	O
also	O
distinguish	O
layers	O
as	O
either	O
"	O
adjacent	O
"	O
or	O
"	O
non	O
-	O
adjacent	O
"	O
.	O
Considering	O
these	O
two	O
factors	O
as	O
three	O
-	O
and	O
two	O
-	O
leveled	O
independent	O
variables	O
respectively	O
,	O
we	O
conduct	O
a	O
two	O
-	O
way	O
analysis	O
of	O
variance	O
.	O
The	O
analysis	O
reveals	O
that	O
the	O
effect	O
of	O
group	O
is	O
significant	O
at	O
F	O
(	O
3	O
,	O
275	O
)	O
=	O
78.47	O
,	O
p	O
<	O
0.0001	O
,	O
with	O
"	O
low	O
"	O
(	O
µ	O
=	O
0.65	O
,	O
σ	O
=	O
0.08	O
)	O
,	O
"	O
middle	O
"	O
(	O
µ	O
=	O
0.84	O
,	O
σ	O
=	O
0.03	O
)	O
,	O
"	O
high	O
"	O
(	O
µ	O
=	O
0.80	O
,	O
σ	O
=	O
0.05	O
)	O
,	O
and	O
"	O
out	O
"	O
(	O
µ	O
=	O
0.80	O
,	O
σ	O
=	O
0.05	O
)	O
.	O
Neither	O
the	O
effect	O
of	O
adjacency	O
nor	O
its	O
interaction	O
with	O
group	O
proved	O
to	O
be	O
significant	O
.	O
This	O
can	O
be	O
seen	O
as	O
(	O
modest	O
)	O
support	O
for	O
the	O
findings	O
of	O
previous	O
work	O
(	O
Blevins	O
et	O
al	O
,	O
2018	O
;	O
Tenney	O
et	O
al	O
,	O
2019	O
)	O
:	O
namely	O
,	O
that	O
the	O
intermediate	O
layers	O
of	O
neural	O
language	O
models	O
encode	O
the	O
most	O
syntax	O
,	O
and	O
are	O
therefore	O
possibly	O
more	O
sensitive	O
towards	O
syntactic	O
complexity	O
.	O
A	O
very	O
similar	O
pattern	O
is	O
observed	O
for	O
total	O
fixation	O
time	O
.	O
When	O
considered	O
together	O
with	O
the	O
correlation	O
between	O
V	O
Y	O
ngve	O
and	O
fixation	O
times	O
,	O
this	O
indicates	O
a	O
tripartite	O
affinity	O
between	O
layer	O
disagreement	O
,	O
syntactic	O
complexity	O
,	O
and	O
fixation	O
.	O
Lexical	O
Ambiguity	O
and	O
Word	O
Frequency	O
Finally	O
,	O
we	O
observe	O
that	O
V	O
logF	O
req	O
.	O
has	O
a	O
moderate	O
correlation	O
with	O
both	O
fixation	O
time	O
and	O
layer	O
disagreement	O
and	O
that	O
V	O
wordSense	O
is	O
nearly	O
uncorrelated	O
to	O
both	O
.	O
Detailed	O
plots	O
of	O
the	O
latter	O
can	O
be	O
found	O
in	O
Appendix	O
A.	O

We	O
would	O
like	O
to	O
thank	O
Vinit	O
Ravishankar	O
,	O
Matt	O
Lamm	O
,	O
and	O
the	O
anonymous	O
reviewers	O
for	O
their	O
helpful	O
comments	O
.	O
Mostafa	O
Abdou	O
and	O
Anders	O
Søgaard	O
are	O
supported	O
by	O
a	O
Google	B-DatasetName
Focused	O
Research	O
Award	O
and	O
a	O
Facebook	O
Research	O
Award	O
.	O

Investigating	O
the	O
Generative	O
Approach	O
for	O
Question	B-TaskName
Answering	I-TaskName
in	O
E	O
-	O
Commerce	O

Many	O
e	O
-	O
commerce	O
websites	O
provide	O
Productrelated	O
Question	B-TaskName
Answering	I-TaskName
(	O
PQA	O
)	O
platform	O
where	O
potential	O
customers	O
can	O
ask	O
questions	O
related	O
to	O
a	O
product	O
,	O
and	O
other	O
consumers	O
can	O
post	O
an	O
answer	O
to	O
that	O
question	O
based	O
on	O
their	O
experience	O
.	O
Recently	O
,	O
there	O
has	O
been	O
a	O
growing	O
interest	O
in	O
providing	O
automated	O
responses	O
to	O
product	O
questions	O
.	O
In	O
this	O
paper	O
,	O
we	O
investigate	O
the	O
suitability	O
of	O
the	O
generative	O
approach	O
for	O
PQA	O
.	O
We	O
use	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
generative	O
models	O
proposed	O
by	O
Deng	B-DatasetName
et	I-DatasetName
al	I-DatasetName
(	O
2020	O
)	O
and	O
Lu	O
et	O
al	O
(	O
2020	O
)	O
for	O
this	O
purpose	O
.	O
On	O
closer	O
examination	O
,	O
we	O
find	O
several	O
drawbacks	O
in	O
this	O
approach	O
:	O
(	O
1	O
)	O
input	O
reviews	O
are	O
not	O
always	O
utilized	O
significantly	O
for	O
answer	B-TaskName
generation	I-TaskName
,	O
(	O
2	O
)	O
the	O
performance	O
of	O
the	O
models	O
is	O
abysmal	O
while	O
answering	O
the	O
numerical	O
questions	O
,	O
(	O
3	O
)	O
many	O
of	O
the	O
generated	O
answers	O
contain	O
phrases	O
like	O
"	O
I	O
do	O
not	O
know	O
"	O
which	O
are	O
taken	O
from	O
the	O
reference	O
answer	O
in	O
training	O
data	O
,	O
and	O
these	O
answers	O
do	O
not	O
convey	O
any	O
information	O
to	O
the	O
customer	O
.	O
Although	O
these	O
approaches	O
achieve	O
a	O
high	O
ROUGE	O
score	O
,	O
it	O
does	O
not	O
reflect	O
upon	O
these	O
shortcomings	O
of	O
the	O
generated	O
answers	O
.	O
We	O
hope	O
that	O
our	O
analysis	O
will	O
lead	O
to	O
more	O
rigorous	O
PQA	O
approaches	O
,	O
and	O
future	O
research	O
will	O
focus	O
on	O
addressing	O
these	O
shortcomings	O
in	O
PQA	O
.	O

With	O
the	O
increase	O
in	O
e	O
-	O
commerce	O
shopping	O
,	O
customer	O
-	O
generated	O
product	O
queries	O
are	O
also	O
growing	O
.	O
Manually	O
answering	O
the	O
questions	O
in	O
real	O
-	O
time	O
is	O
infeasible	O
,	O
and	O
also	O
some	O
questions	O
go	O
unanswered	O
for	O
an	O
extended	O
period	O
.	O
It	O
is	O
necessary	O
to	O
answer	O
the	O
user	O
queries	O
in	O
the	O
e	O
-	O
commerce	O
business	O
automatically	O
.	O
The	O
user	O
reviews	O
are	O
a	O
vast	O
source	O
of	O
information	O
with	O
diverse	O
opinions	O
,	O
and	O
they	O
can	O
be	O
used	O
to	O
answer	O
user	O
queries	O
.	O
Earlier	O
works	O
on	O
product	O
question	B-TaskName
answering	I-TaskName
(	O
PQA	O
)	O
focus	O
on	O
retrieval	O
-	O
based	O
approaches	O
and	O
binary	O
answer	O
prediction	O
tasks	O
.	O
McAuley	O
and	O
Yang	O
(	O
2016	O
)	O
;	O
Fan	O
et	O
al	O
(	O
2019	O
)	O
;	O
aim	O
to	O
predict	O
the	O
answer	O
as	O
"	O
yes	O
/	O
no	O
"	O
based	O
on	O
the	O
relevant	O
reviews	O
,	O
customer	O
ratings	O
,	O
aspects	O
in	O
the	O
reviews	O
,	O
etc	O
.	O
Retrieval	O
-	O
based	O
approaches	O
try	O
to	O
find	O
the	O
most	O
relevant	O
review	O
snippet	O
as	O
the	O
answer	O
(	O
Chen	O
et	O
al	O
,	O
2019a	O
)	O
and	O
use	O
a	O
ranked	O
list	O
of	O
review	O
snippets	O
as	O
the	O
response	O
for	O
a	O
given	O
question	O
.	O
With	O
the	O
success	O
of	O
machine	B-TaskName
translation	I-TaskName
(	O
Sutskever	O
et	O
al	O
,	O
2014	O
)	O
and	O
summarization	B-TaskName
(	O
See	O
et	O
al	O
,	O
2017	O
)	O
,	O
the	O
PQA	O
approaches	O
are	O
shifting	O
towards	O
natural	O
answer	B-TaskName
generation	I-TaskName
from	O
relevant	O
product	O
reviews	O
(	O
Gao	O
et	O
al	O
,	O
2019	O
;	O
Chen	O
et	O
al	O
,	O
2019b	O
;	O
Deng	B-DatasetName
et	I-DatasetName
al	I-DatasetName
,	O
2020	O
;	O
Lu	O
et	O
al	O
,	O
2020	O
;	O
Gao	O
et	O
al	O
,	O
2021	O
)	O
.	O
In	O
this	O
work	O
,	O
we	O
analyse	O
the	O
answer	O
generated	O
from	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
generative	O
models	O
OAAG	O
(	O
Deng	B-DatasetName
et	I-DatasetName
al	I-DatasetName
,	O
2020	O
)	O
and	O
CHIME	O
(	O
Lu	O
et	O
al	O
,	O
2020	O
)	O
in	O
detail	O
beyond	O
their	O
traditional	O
scores	O
on	O
popular	O
metrics	O
such	O
as	O
ROUGE	O
(	O
Lin	O
,	O
2004	O
)	O
.	O
We	O
find	O
that	O
despite	O
achieving	O
a	O
good	O
score	O
on	O
these	O
metrics	O
,	O
generated	O
answers	O
have	O
several	O
drawbacks	O
that	O
can	O
lead	O
to	O
user	O
dissatisfaction	O
.	O
(	O
Ni	O
et	O
al	O
,	O
2019	O
;	O
He	O
and	O
McAuley	O
,	O
2016	O
)	O
includes	O
users	O
'	O
reviews	O
along	O
with	O
a	O
rating	O
of	O
the	O
product	O
given	O
by	O
the	O
same	O
user	O
.	O
The	O
Product	O
ID	O
is	O
used	O
to	O
align	O
the	O
question	O
with	O
its	O
reviews	O
.	O

We	O
use	O
Opinion	O
-	O
aware	O
Answer	B-TaskName
Generation	I-TaskName
(	O
OAAG	O
)	O
model	O
(	O
Deng	B-DatasetName
et	I-DatasetName
al	I-DatasetName
,	O
2020	O
)	O
and	O
Crosspassage	O
Hierarchical	O
Memory	B-MethodName
Network	I-MethodName
(	O
CHIME	O
)	O
model	O
(	O
Lu	O
et	O
al	O
,	O
2020	O
)	O
for	O
our	O
analysis	O
.	O
Following	O
the	O
generative	O
approach	O
,	O
these	O
two	O
models	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
the	O
Amazon	O
Question	B-TaskName
Answering	I-TaskName
dataset	O
.	O
There	O
are	O
thousands	O
of	O
products	O
in	O
each	O
category	O
in	O
the	O
Amazon	O
Product	O
Review	O
dataset	O
,	O
and	O
each	O
product	O
has	O
thousands	O
of	O
reviews	O
.	O
All	O
the	O
reviews	O
may	O
not	O
be	O
relevant	O
for	O
a	O
particular	O
query	O
,	O
and	O
therefore	O
,	O
to	O
answer	O
a	O
product	O
-	O
related	O
question	O
,	O
models	O
need	O
to	O
filter	O
out	O
the	O
irrelevant	O
reviews	O
first	O
.	O
OAAG	O
and	O
CHIME	O
use	O
the	O
BM25	O
algorithm	O
to	O
retrieve	O
and	O
rank	O
all	O
the	O
review	O
snippets	O
of	O
a	O
product	O
,	O
and	O
the	O
top	O
k	O
relevant	O
snippets	O
(	O
we	O
use	O
top	O
10	O
reviews	O
snippets	O
)	O
for	O
that	O
question	O
are	O
taken	O
as	O
the	O
premise	O
of	O
the	O
answer	O
.	O

Upon	O
retrieving	O
the	O
relevant	O
reviews	O
,	O
OAAG	O
uses	O
an	O
encoder	O
-	O
decoder	O
model	O
for	O
answer	B-TaskName
generation	I-TaskName
.	O
OAAG	O
encodes	O
the	O
question	O
and	O
each	O
review	O
corresponding	O
to	O
that	O
question	O
using	O
a	O
Bi	O
-	O
LSTM	B-MethodName
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
network	O
.	O
They	O
apply	O
a	O
co	O
-	O
attention	O
mechanism	O
over	O
these	O
encodings	O
to	O
get	O
the	O
question	O
and	O
review	O
representations	O
.	O
They	O
utilize	O
the	O
ratings	O
of	O
the	O
retrieved	O
reviews	O
to	O
mine	O
the	O
general	O
opinion	O
about	O
the	O
question	O
using	O
the	O
attention	O
mechanism	O
.	O
Finally	O
,	O
they	O
employ	O
a	O
multi	O
-	O
view	O
pointer	O
-	O
generator	O
network	O
that	O
copies	O
words	O
from	O
the	O
question	O
as	O
well	O
as	O
from	O
the	O
reviews	O
and	O
fuses	O
the	O
opinion	O
by	O
re	O
-	O
weighting	O
the	O
attention	O
scores	O
of	O
the	O
words	O
in	O
reviews	O
to	O
generate	O
an	O
opinionated	O
answer	O
.	O
They	O
report	O
ROUGE	O
-	O
based	O
scores	O
to	O
compare	O
the	O
model	O
performance	O
against	O
the	O
previous	O
approaches	O
(	O
Chen	O
et	O
al	O
,	O
2019b	O
;	O
Gao	O
et	O
al	O
,	O
2019	O
)	O
.	O

CHIME	O
uses	O
a	O
transformer	O
-	O
based	O
encoder	O
-	O
decoder	O
model	O
to	O
generate	O
the	O
response	O
.	O
It	O
extends	O
pretrained	O
XLNet	B-MethodName
(	O
Yang	O
et	O
al	O
,	O
2019	O
)	O
with	O
an	O
auxiliary	O
memory	O
module	O
that	O
consists	O
of	O
two	O
components	O
:	O
the	O
context	O
memory	O
,	O
and	O
the	O
answer	O
memory	O
.	O
Given	O
a	O
question	O
with	O
K	O
review	O
passages	O
,	O
it	O
creates	O
K	O
training	O
instances	O
,	O
each	O
consisting	O
of	O
the	O
question	O
,	O
a	O
review	O
passage	O
,	O
and	O
the	O
reference	O
answer	O
.	O
Each	O
training	O
instance	O
is	O
fed	O
into	O
an	O
XLNet	B-MethodName
encoder	O
to	O
get	O
the	O
hidden	O
representations	O
that	O
are	O
used	O
to	O
update	O
the	O
two	O
memories	O
.	O
The	O
context	O
memory	O
mechanism	O
sequentially	O
reads	O
the	O
review	O
passages	O
and	O
gathers	O
the	O
cross	O
-	O
passage	O
evidences	O
to	O
identify	O
the	O
most	O
prominent	O
opinion	O
in	O
reviews	O
.	O
The	O
answer	O
memory	O
works	O
as	O
a	O
buffer	O
to	O
gradually	O
refine	O
the	O
generated	O
answers	O
after	O
reading	O
each	O
(	O
question	O
,	O
review	O
passage	O
)	O
pair	O
.	O
After	O
reading	O
the	O
last	O
review	O
,	O
the	O
answer	O
memory	O
is	O
fed	O
to	O
the	O
decoder	O
to	O
get	O
a	O
final	O
response	O
.	O

Different	O
types	O
of	O
questions	O
are	O
asked	O
on	O
the	O
Amazon	O
product	B-DatasetName
page	I-DatasetName
like	O
numerical	O
,	O
"	O
yes	O
/	O
no	O
"	O
,	O
descriptive	O
.	O
The	O
generative	O
model	O
may	O
not	O
be	O
suitable	O
for	O
answering	O
all	O
kinds	O
of	O
questions	O
.	O
So	O
,	O
we	O
categorize	O
the	O
questions	O
as	O
template	O
-	O
based	O
and	O
descriptive	O
.	O
For	O
template	O
-	O
based	O
questions	O
,	O
the	O
answer	O
can	O
be	O
yes	O
or	O
no	O
without	O
any	O
explanation	O
.	O
We	O
filter	O
the	O
questions	O
where	O
the	O
answer	O
starts	O
with	O
'	O
yes	O
'	O
,	O
'	O
yeah	O
'	O
,	O
'	O
no	O
'	O
,	O
'	O
nope	O
'	O
and	O
mark	O
these	O
as	O
templatebased	O
questions	O
.	O
Both	O
categories	O
contain	O
∼75	O
%	O
descriptive	O
questions	O
.	O
Table	O
2	O
summarizes	O
the	O
result	O
of	O
the	O
template	O
-	O
based	O
and	O
generative	O
questions	O
.	O
Both	O
models	O
'	O
performance	O
in	O
descriptive	O
questions	O
is	O
better	O
than	O
the	O
template	O
-	O
based	O
questions	O
.	O
Furthermore	O
,	O
we	O
categorized	O
the	O
questions	O
into	O
numerical	O
and	O
non	O
-	O
numerical	O
questions	O
.	O
We	O
consider	O
a	O
question	O
to	O
be	O
numerical	O
if	O
there	O
are	O
numbers	O
in	O
the	O
question	O
or	O
in	O
the	O
reference	O
answer	O
.	O
The	O
test	O
datasets	O
of	O
both	O
the	O
categories	O
have	O
∼19	O
%	O
numerical	O
questions	O
.	O
The	O
OAAG	O
model	O
performs	O
better	O
in	O
answering	O
non	O
-	O
numerical	O
questions	O
,	O
while	O
CHIME	O
performs	O
better	O
in	O
answering	O
numerical	O
questions	O
.	O
Although	O
the	O
ROUGE	O
scores	O
are	O
close	O
in	O
numerical	O
and	O
non	O
-	O
numerical	O
questions	O
for	O
both	O
the	O
models	O
,	O
on	O
analyzing	O
the	O
numerical	O
answers	O
,	O
we	O
find	O
that	O
the	O
words	O
in	O
generated	O
and	O
reference	O
answers	O
might	O
match	O
,	O
but	O
the	O
numbers	O
generally	O
do	O
not	O
match	O
.	O
3	O
We	O
present	O
some	O
examples	O
of	O
numerical	O
questions	O
with	O
their	O
answers	O
in	O
Table	O
A.4	O
of	O
Appendix	O
.	O

This	O
paper	O
describes	O
the	O
Air	O
Force	O
Research	O
Laboratory	O
(	O
AFRL	O
)	O
machine	B-TaskName
translation	I-TaskName
systems	O
and	O
the	O
improvements	O
that	O
were	O
developed	O
during	O
the	O
WMT19	O
evaluation	O
campaign	O
.	O
This	O
year	O
,	O
we	O
refine	O
our	O
approach	O
to	O
training	O
popular	O
neural	O
machine	B-TaskName
translation	I-TaskName
toolkits	O
,	O
experiment	O
with	O
a	O
new	O
domain	B-TaskName
adaptation	I-TaskName
technique	O
and	O
again	O
measure	O
improvements	O
in	O
performance	O
on	O
the	O
Russian	O
-	O
English	O
language	O
pair	O
.	O

As	O
part	O
of	O
the	O
2019	O
Conference	O
on	O
Machine	B-TaskName
Translation	I-TaskName
(	O
Bojar	O
et	O
al	O
,	O
2019	O
)	O
news	O
-	O
translation	O
shared	O
task	O
,	O
the	O
AFRL	O
Human	O
Language	O
Technology	O
team	O
participated	O
in	O
the	O
Russian	O
-	O
English	O
portion	O
of	O
the	O
competition	O
.	O
We	O
build	O
on	O
our	O
strategies	O
from	O
last	O
year	O
(	O
Gwinnup	O
et	O
al	O
,	O
2018	O
)	O
,	O
adding	O
additional	O
language	O
ID	O
based	O
data	O
processing	O
and	O
optimizing	O
subword	O
segmentation	O
strategies	O
.	O
For	O
Russian	O
-	O
English	O
we	O
again	O
submitted	O
an	O
entry	O
comprising	O
our	O
best	O
systems	O
trained	O
with	O
Marian	O
(	O
Junczys	O
-	O
Dowmunt	O
et	O
al	O
,	O
2018	O
)	O
,	O
Sockeye	O
(	O
Hieber	O
et	O
al	O
,	O
2017	O
)	O
with	O
Elastic	B-MethodName
Weight	I-MethodName
Consolidation	I-MethodName
(	O
EWC	B-MethodName
)	O
(	O
Thompson	O
et	O
al	O
,	O
2019	O
)	O
,	O
OpenNMT	O
(	O
Klein	O
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
Moses	O
(	O
Koehn	O
et	O
al	O
,	O
2007	O
)	O
combined	O
using	O
the	O
Jane	O
system	O
combination	O
method	O
(	O
Freitag	O
et	O
al	O
,	O
2014	O
)	O
.	O

We	O
used	O
and	O
preprocess	O
data	O
as	O
outlined	O
in	O
Gwinnup	O
et	O
al	O
(	O
2018	O
)	O
.	O
For	O
all	O
systems	O
trained	O
,	O
we	O
applied	O
either	O
byte	O
-	O
pair	O
encoding	O
(	O
BPE	B-MethodName
)	O
(	O
Sennrich	O
et	O
al	O
,	O
2016	O
)	O
or	O
SentencePiece	B-MethodName
(	O
Kudo	O
and	O
Richardson	O
,	O
2018	O
)	O
subword	O
strategies	O
to	O
address	O
the	O
vocabulary	O
-	O
size	O
problem	O
.	O
For	O
this	O
year	O
,	O
we	O
also	O
employed	O
a	O
language	O
ID	O
filtering	O
step	O
for	O
the	O
BPE	B-MethodName
-	O
based	O
systems	O
.	O
Using	O
the	O
pre	O
-	O
built	O
language	O
ID	O
model	O
developed	O
by	O
the	O
authors	O
of	O
fastText	B-MethodName
(	O
Joulin	O
et	O
al	O
,	O
2016a	O
,	O
b	O
)	O
,	O
we	O
developed	O
a	O
utility	O
that	O
examined	O
the	O
source	O
and	O
target	O
sentence	O
pairs	O
and	O
discarded	O
that	O
pair	O
if	O
either	O
side	O
fell	O
below	O
0.8	O
1	O
probability	O
of	O
the	O
desired	O
language	O
.	O
We	O
applied	O
this	O
filtering	O
to	O
all	O
provided	O
parallel	O
corpora	O
,	O
removing	O
33.7	O
%	O
of	O
lines	O
.	O
This	O
process	O
was	O
particularly	O
effective	O
when	O
used	O
to	O
filter	O
the	O
Paracrawl	B-DatasetName
corpus	O
where	O
57.1	O
%	O
of	O
lines	O
were	O
removed	O
.	O
Pre	O
and	O
post	O
-	O
filtering	O
line	O
counts	O
for	O
various	O
corpora	O
are	O
shown	O
in	O
Table	O
1	O
.	O

One	O
of	O
the	O
problems	O
faced	O
when	O
addressing	O
the	O
closed	O
-	O
vocabulary	O
problem	O
is	O
the	O
granularity	O
of	O
the	O
subword	O
units	O
either	O
produced	O
by	O
SentencePiece	B-MethodName
or	O
BPE	B-MethodName
.	O
To	O
that	O
end	O
,	O
we	O
examined	O
varying	O
the	O
number	O
of	O
BPE	B-MethodName
merge	O
operations	O
in	O
order	O
to	O
determine	O
an	O
optimal	O
setting	O
to	O
maximize	O
performance	O
for	O
the	O
Russian	O
-	O
English	O
language	O
pair	O
.	O
For	O
the	O
OpenNMT	O
-	O
based	O
systems	O
,	O
a	O
vocabulary	O
size	O
of	O
32k	O
entries	O
was	O
employed	O
during	O
training	O
of	O
a	O
SentencePiece	B-MethodName
segmentation	O
model	O
2	O
.	O
This	O
vocabulary	O
size	O
was	O
determined	O
empirically	O
from	O
the	O
training	O
data	O
.	O
Alternatively	O
,	O
for	O
the	O
BPE	B-MethodName
-	O
based	O
systems	O
,	O
we	O
systematically	O
examined	O
varying	O
sizes	O
of	O
BPE	B-MethodName
merge	O
operations	O
and	O
vocabulary	O
sizes	O
in	O
10k	O
increments	O
from	O
30k	O
to	O
80k	O
.	O
Results	O
in	O
Table	O
3	O
show	O
that	O
40k	O
BPE	B-MethodName
merge	O
operations	O
perform	O
best	O
across	O
all	O
test	O
sets	O
decoded	O
for	O
this	O
language	O
pair	O
.	O
All	O
subsequent	O
Marian	O
experiments	O
in	O
this	O
work	O
utilize	O
this	O
40k	O
BPE	B-MethodName
training	O
corpus	O
.	O

This	O
year	O
,	O
we	O
focused	O
system	O
-	O
building	O
efforts	O
on	O
the	O
Marian	O
,	O
Sockeye	O
,	O
OpenNMT	O
,	O
and	O
Moses	O
toolkits	O
,	O
having	O
explored	O
a	O
variety	O
of	O
parameters	O
,	O
data	O
,	O
and	O
conditions	O
.	O
While	O
most	O
of	O
our	O
experimentation	O
builds	O
off	O
of	O
previous	O
years	O
'	O
efforts	O
,	O
we	O
did	O
examine	O
domain	B-TaskName
adaptation	I-TaskName
via	O
continued	O
training	O
,	O
including	O
Elastic	B-MethodName
Weight	I-MethodName
Consolidation	I-MethodName
(	O
EWC	B-MethodName
)	O
(	O
Thompson	O
et	O
al	O
,	O
2019	O
)	O
.	O

For	O
our	O
Sockeye	O
(	O
Hieber	O
et	O
al	O
,	O
2017	O
)	O
systems	O
,	O
we	O
experimented	O
with	O
continued	O
training	O
(	O
Luong	O
and	O
Manning	O
,	O
2015	O
;	O
Sennrich	O
et	O
al	O
,	O
2015	O
)	O
-	O
a	O
means	O
to	O
specialize	O
a	O
model	O
in	O
a	O
new	O
domain	O
after	O
a	O
period	O
of	O
training	O
on	O
a	O
general	O
domain	O
.	O
One	O
downside	O
of	O
utilizing	O
continued	O
training	O
is	O
the	O
model	O
adapts	O
"	O
too	O
-	O
well	O
"	O
to	O
the	O
new	O
domain	O
at	O
the	O
expense	O
of	O
performance	O
in	O
the	O
original	O
domain	O
(	O
Freitag	O
and	O
Al	O
-	O
Onaizan	O
,	O
2016	O
)	O
.	O
One	O
method	O
to	O
mitigate	O
this	O
performance	O
drop	O
is	O
to	O
prevent	O
certain	O
parameters	O
of	O
the	O
network	O
from	O
changing	O
with	O
Elastic	B-MethodName
Weight	I-MethodName
Consolidation	I-MethodName
(	O
EWC	B-MethodName
)	O
(	O
Kirkpatrick	O
et	O
al	O
,	O
2017	O
)	O
.	O
Thompson	O
et	O
al	O
(	O
2019	O
)	O
ference	O
in	O
style	O
and	O
content	O
.	O
Here	O
,	O
we	O
created	O
a	O
news	O
subdomain	O
corpus	O
from	O
the	O
newstest2014	O
through	O
newstest2017	O
test	O
sets	O
.	O
The	O
intuition	O
is	O
that	O
more	O
current	O
events	O
will	O
be	O
discussed	O
in	O
these	O
test	O
sets	O
than	O
the	O
remainder	O
of	O
the	O
provided	O
training	O
corpora	O
,	O
allowing	O
better	O
adaptation	O
of	O
new	O
events	O
in	O
the	O
newest	O
test	O
sets	O
(	O
newstest2018	O
and	O
newstest2019	O
.	O
)	O
We	O
first	O
trained	O
a	O
baseline	O
transformer	O
system	O
using	O
the	O
best	O
-	O
performing	O
BPE	B-MethodName
parameters	O
from	O
Section	O
2.2	O
,	O
512	O
-	O
dimension	O
word	B-TaskName
embeddings	I-TaskName
,	O
6	O
layer	O
encoder	O
and	O
decoder	O
,	O
8	O
attention	O
heads	O
,	O
label	B-MethodName
smoothing	I-MethodName
and	O
transformer	O
attention	B-MethodName
dropout	I-MethodName
of	O
0.1	O
.	O
We	O
then	O
continue	O
-	O
train	O
a	O
model	O
on	O
the	O
adaptation	O
set	O
described	O
above	O
.	O
We	O
also	O
followed	O
the	O
Sockeye	O
EWC	B-MethodName
training	O
procedure	O
,	O
producing	O
a	O
model	O
more	O
resilient	O
to	O
overfitting	O
due	O
to	O
continued	O
training	O
.	O
Results	O
for	O
these	O
systems	O
are	O
shown	O
in	O
Table	O
5	O
.	O
We	O
see	O
that	O
the	O
baseline	O
Sockeye	O
transformer	O
model	O
performs	O
similarly	O
to	O
the	O
baseline	O
singlemodel	O
Marian	O
transformer	O
system	O
shown	O
in	O
Table	O
4	O
.	O
The	O
continued	O
-	O
training	O
system	O
(	O
con't	O
train	O
)	O
system	O
predictably	O
overfit	O
on	O
newstest2014	O
as	O
expected	O
,	O
since	O
that	O
test	O
set	O
is	O
a	O
part	O
of	O
the	O
adaptation	O
set	O
.	O
Likewise	O
,	O
performance	O
on	O
the	O
out	O
-	O
ofdomain	O
newstest2018	O
also	O
dropped	O
as	O
a	O
result	O
of	O
overfitting	O
.	O
The	O
best	O
-	O
performing	O
EWC	B-MethodName
system	O
5	O
5	O
EWC	B-MethodName
applied	O
with	O
weight	O
-	O
decay	O
of	O
0.001	O
and	O
learning	O
-	O
actually	O
improved	O
performance	O
on	O
2018	O
with	O
lesspronounced	O
overfitting	O
on	O
2014	O
.	O
For	O
system	O
combination	O
outlined	O
later	O
in	O
Section	O
4	O
,	O
we	O
decoded	O
test	O
sets	O
with	O
an	O
ensemble	O
of	O
the	O
four	O
highest	O
-	O
scoring	O
model	O
checkpoints	O
from	O
the	O
best	O
EWC	B-MethodName
training	O
run	O
.	O

Our	O
first	O
Open	O
-	O
NMT	O
system	O
was	O
trained	O
using	O
the	O
Transformer	B-MethodName
architecture	O
with	O
the	O
default	O
"	O
Trans	O
-	O
formerBig	O
"	O
settings	O
as	O
described	O
in	O
Vaswani	O
et	O
al	O
(	O
2017	O
)	O
:	O
6	O
layers	O
of	O
1024	O
units	O
,	O
16	O
attention	O
heads	O
.	O
Dropout	B-MethodName
rates	O
of	O
0.3	O
for	O
layers	O
and	O
0.1	O
for	O
attention	O
heads	O
and	O
relu	B-MethodName
's	O
.	O
Training	O
data	O
for	O
this	O
system	O
utilized	O
the	O
training	O
corpus	O
from	O
our	O
WMT17	O
Russian	O
-	O
English	O
system	O
(	O
Gwinnup	O
et	O
al	O
,	O
2017	O
)	O
consisting	O
of	O
provided	O
parallel	O
and	O
backtranslated	O
rate	O
of	O
0.00001	O
data	O
.	O
This	O
data	O
was	O
then	O
processed	O
with	O
a	O
joint	O
32k	O
word	O
vocabulary	O
SentencePiece	B-MethodName
model	O
.	O

For	O
our	O
second	O
OpenNMT	O
system	O
,	O
we	O
first	O
trained	O
language	O
-	O
specific	O
,	O
32k	O
word	O
vocabularies	O
using	O
SentencePiece	B-MethodName
.	O
WMT	O
news	O
test	O
data	O
from	O
all	O
years	O
except	O
2014	O
and	O
2017	O
were	O
used	O
to	O
train	O
Senten	O
-	O
cePiece	O
.	O
These	O
data	O
,	O
with	O
the	O
addition	O
of	O
the	O
language	O
ID	O
filtered	O
ParaCrawl	B-DatasetName
corpus	O
outlined	O
in	O
Section	O
2.1	O
,	O
were	O
used	O
for	O
training	O
the	O
system	O
.	O
WMT	O
news	O
test	O
data	O
from	O
2014	O
was	O
used	O
for	O
validation	O
.	O
OpenNMT	O
-	O
tf	O
was	O
used	O
to	O
create	O
the	O
system	O
,	O
using	O
the	O
stock	O
"	O
Transformer	B-MethodName
"	O
model	O
.	O

We	O
submitted	O
the	O
final	O
5	O
-	O
system	O
combination	O
outlined	O
in	O
Section	O
4	O
and	O
the	O
four	O
-	O
checkpoint	O
EWC	B-MethodName
ensemble	O
detailed	O
in	O
Section	O
3.2	O
to	O
the	O
Russian	O
-	O
English	O
portion	O
of	O
the	O
WMT19	O
news	O
task	O
evaluation	O
.	O
Selected	O
newstest2019	O
automatic	O
scores	O
from	O
the	O
WMT	O
Evaluation	O
Matrix	O
6	O
are	O
shown	O
in	O
Table	O
7	O
.	O

We	O
presented	O
a	O
series	O
of	O
improvements	O
to	O
our	O
Russian	O
-	O
English	O
systems	O
,	O
including	O
improved	O
preprocessing	O
and	O
domain	B-TaskName
adaptation	I-TaskName
.	O
Clever	O
remixing	O
of	O
older	O
techniques	O
from	O
the	O
phrasebased	O
MT	O
era	O
enabled	O
improvements	O
in	O
ensembled	O
neural	O
decoding	O
.	O
Lastly	O
,	O
we	O
performed	O
system	O
combination	O
to	O
leverage	O
benefits	O
from	O
these	O
new	O
techniques	O
and	O
favorite	O
approaches	O
from	O
previous	O
years	O
.	O

Analysis	O
of	O
online	O
user	O
discussion	O
continues	O
to	O
be	O
a	O
critical	O
area	O
of	O
interdisciplinary	O
research	O
.	O
Increasing	O
rates	O
of	O
internet	O
access	O
and	O
the	O
development	O
of	O
a	O
diverse	O
range	O
of	O
online	O
forums	O
has	O
allowed	O
for	O
conversation	O
between	O
individuals	O
across	O
the	O
globe	O
on	O
an	O
extraordinary	O
range	O
of	O
topics	O
.	O
However	O
,	O
this	O
has	O
been	O
accompanied	O
by	O
a	O
surge	O
in	O
abuse	O
and	O
other	O
negative	O
behaviours	O
online	O
,	O
the	O
impacts	O
of	O
which	O
have	O
been	O
well	O
-	O
documented	O
in	O
academic	O
research	O
.	O
It	O
has	O
been	O
found	O
that	O
targeted	O
negative	O
comments	O
and	O
harassment	O
online	O
can	O
seriously	O
impact	O
individual	O
well	O
-	O
being	O
(	O
Weingartner	O
and	O
Stahel	O
,	O
2019	O
;	O
Bauman	O
,	O
2013	O
)	O
,	O
force	O
users	O
to	O
leave	O
a	O
community	O
or	O
reduce	O
online	O
participation	O
(	O
Wulczyn	O
et	O
al	O
,	O
2017	O
;	O
Blackburn	O
and	O
Kwak	O
,	O
2014	O
)	O
,	O
and	O
potentially	O
lead	O
to	O
offline	O
hate	O
-	O
crimes	O
(	O
Mulki	O
et	O
al	O
,	O
2019	O
;	O
Hassan	O
et	O
al	O
,	O
2018	O
)	O
.	O
While	O
these	O
forms	O
of	O
comments	O
may	O
be	O
explicit	O
or	O
overtly	O
harmful	O
,	O
they	O
are	O
also	O
often	O
difficult	O
to	O
detect	O
or	O
ambiguous	O
.	O
Where	O
there	O
are	O
insufficient	O
moderation	O
resources	O
to	O
scale	O
with	O
a	O
forum	O
's	O
user	O
-	O
base	O
,	O
this	O
can	O
lead	O
to	O
unchecked	O
negative	O
discourse	O
,	O
or	O
cause	O
website	O
administrators	O
to	O
restrict	O
user	O
comment	O
functions	O
.	O
This	O
means	O
that	O
research	O
which	O
aims	O
to	O
enable	O
automated	O
moderation	O
,	O
provide	O
a	O
review	O
triage	O
service	O
for	O
human	O
moderation	O
teams	O
,	O
or	O
design	O
systems	O
to	O
nudge	O
users	O
towards	O
healthier	O
conversation	O
,	O
has	O
significant	O
potential	O
for	O
contributing	O
to	O
both	O
the	O
availability	O
and	O
quality	O
of	O
online	O
discourse	O
.	O
A	O
persistent	O
challenge	O
for	O
researchers	O
and	O
site	O
administrators	O
in	O
this	O
area	O
is	O
the	O
need	O
to	O
:	O
(	O
a	O
)	O
establish	O
a	O
typology	O
of	O
comments	O
which	O
are	O
undesirable	O
in	O
online	O
discussions	O
;	O
(	O
b	O
)	O
apply	O
this	O
typology	O
in	O
a	O
consistent	O
and	O
reliable	O
manner	O
;	O
and	O
(	O
c	O
)	O
account	O
for	O
adversarial	O
user	O
behaviour	O
in	O
response	O
to	O
moderation	O
.	O
This	O
is	O
complicated	O
by	O
the	O
fact	O
that	O
there	O
is	O
no	O
single	O
objective	O
set	O
of	O
categories	O
for	O
speech	O
which	O
ought	O
to	O
be	O
excluded	O
in	O
all	O
contexts	O
,	O
with	O
perceptions	O
of	O
undesirable	O
speech	O
differing	O
across	O
individuals	O
,	O
cultures	O
,	O
geographies	O
,	O
and	O
online	O
communities	O
(	O
Vidgen	O
et	O
al	O
,	O
2019	O
)	O
.	O
Prior	O
research	O
on	O
toxic	O
comments	O
online	O
has	O
found	O
that	O
classifiers	O
trained	O
on	O
crowdsourced	O
data	O
can	O
be	O
effective	O
at	O
detecting	O
the	O
most	O
overt	O
forms	O
of	O
toxic	O
comments	O
.	O
However	O
,	O
there	O
remain	O
difficulties	O
in	O
detecting	O
subtler	O
forms	O
of	O
toxicity	O
which	O
may	O
be	O
implicit	O
,	O
require	O
idiosyncratic	O
knowledge	O
,	O
familiarity	O
with	O
the	O
conversation	O
context	O
,	O
or	O
familiarity	O
with	O
particular	O
cultural	O
tropes	O
(	O
Kohli	O
et	O
al	O
,	O
2018	O
;	O
van	O
Aken	O
et	O
al	O
,	O
2018	O
;	O
Parekh	O
and	O
Patel	O
,	O
2017	O
)	O
.	O
One	O
of	O
the	O
key	O
ingredients	O
to	O
progress	O
on	O
this	O
front	O
will	O
be	O
high	O
quality	O
,	O
large	O
,	O
annotated	O
datasets	O
addressing	O
these	O
more	O
subtle	O
harmful	O
attributes	O
,	O
from	O
which	O
machine	O
learning	O
models	O
will	O
be	O
able	O
to	O
learn	O
.	O
Unfortunately	O
,	O
for	O
most	O
subtler	O
toxic	O
attributes	O
there	O
are	O
few	O
available	O
datasets	O
(	O
or	O
none	O
,	O
particularly	O
in	O
many	O
languages	O
other	O
than	O
English	O
)	O
,	O
which	O
is	O
a	O
bottleneck	O
preventing	O
further	O
research	O
(	O
Fortuna	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
aim	O
to	O
contribute	O
to	O
research	O
in	O
this	O
area	O
through	O
the	O
release	O
of	O
the	O
Unhealthy	O
Comment	O
Corpus	O
(	O
UCC	B-DatasetName
)	O
of	O
approximately	O
44	O
,	O
000	O
comments	O
and	O
corresponding	O
crowdsourced	O
labels	O
and	O
confidence	O
scores	O
.	O
The	O
labelling	O
typology	O
for	O
the	O
dataset	O
identifies	O
for	O
each	O
comment	O
a	O
higher	O
-	O
level	O
classification	O
of	O
whether	O
that	O
comment	O
'	O
has	O
a	O
place	O
in	O
a	O
healthy	O
online	O
conversation	O
'	O
,	O
accompanied	O
for	O
each	O
comment	O
by	O
binary	O
labels	O
for	O
whether	O
it	O
is	O
:	O
(	O
1	O
)	O
hostile	O
,	O
(	O
2	O
)	O
antagonistic	O
,	O
insulting	O
,	O
provocative	O
or	O
trolling	O
(	O
together	O
,	O
'	O
antagonistic	O
'	O
)	O
,	O
(	O
3	O
)	O
dismissive	O
,	O
(	O
4	O
)	O
condescending	O
or	O
patronising	O
(	O
together	O
,	O
'	O
condescending	O
'	O
)	O
,	O
(	O
5	O
)	O
sarcastic	O
,	O
and/or	O
(	O
6	O
)	O
an	O
unfair	O
generalisation	O
.	O
For	O
each	O
label	O
there	O
is	O
also	O
an	O
associated	O
confidence	O
score	O
(	O
between	O
0.5	O
and	O
1	O
)	O
.	O
The	O
UCC	B-DatasetName
is	O
open	O
source	O
and	O
available	O
on	O
Github	O
.	O
1	O
The	O
UCC	B-DatasetName
contributes	O
further	O
high	O
quality	O
data	O
on	O
attributes	O
like	O
sarcasm	O
,	O
hostility	O
,	O
and	O
condescension	O
,	O
adding	O
to	O
existing	O
datasets	O
on	O
these	O
and	O
related	O
attributes	O
(	O
Wang	O
and	O
Potts	O
,	O
2019	O
;	O
Davidson	O
et	O
al	O
,	O
2017	O
;	O
Wulczyn	O
et	O
al	O
,	O
2017	O
;	O
Chen	O
et	O
al	O
,	O
2017	O
)	O
,	O
and	O
provides	O
(	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
)	O
the	O
first	O
dataset	O
of	O
this	O
scale	O
with	O
labels	O
for	O
dismissiveness	O
,	O
unfair	O
generalisations	O
,	O
antagonistic	O
behavior	O
,	O
and	O
overall	O
assessments	O
of	O
whether	O
those	O
comments	O
fall	O
within	O
'	O
healthy	O
'	O
conversation	O
.	O
We	O
also	O
make	O
use	O
of	O
and	O
illustrate	O
the	O
benefits	O
of	O
annotator	O
trustworthiness	O
scores	O
when	O
crowdsourcing	O
labels	O
on	O
subjective	O
data	O
of	O
this	O
sort	O
.	O
1	O
github.com/conversationai/unhealthy	O
-	O
conversations	O
This	O
paper	O
is	O
structured	O
as	O
follows	O
.	O
Section	O
2	O
outlines	O
the	O
motivation	O
and	O
background	O
to	O
the	O
UCC	B-DatasetName
attribute	O
typology	O
.	O
Section	O
3	O
details	O
the	O
data	O
collection	O
and	O
quality	O
control	O
processes	O
.	O
In	O
Section	O
4	O
we	O
present	O
some	O
summary	O
statistics	O
,	O
benefits	O
,	O
and	O
limitations	O
of	O
the	O
data	O
,	O
and	O
in	O
Section	O
5	O
we	O
present	O
a	O
baseline	O
classification	O
model	O
for	O
this	O
dataset	O
,	O
and	O
evaluate	O
its	O
performance	O
.	O
Section	O
6	O
highlights	O
potential	O
sources	O
of	O
bias	O
in	O
this	O
dataset	O
,	O
and	O
the	O
need	O
to	O
be	O
cognisant	O
of	O
these	O
when	O
conducting	O
further	O
research	O
in	O
this	O
area	O
.	O

Also	O
included	O
in	O
the	O
UCC	B-DatasetName
dataset	O
are	O
the	O
individual	O
annotations	O
for	O
each	O
comment	O
by	O
all	O
'	O
trusted	O
'	O
annotators	O
.	O
Users	O
of	O
the	O
data	O
may	O
therefore	O
apply	O
any	O
alternative	O
trustworthiness	O
threshold	O
,	O
or	O
use	O
a	O
preferred	O
aggregation	O
method	O
to	O
derive	O
labels	O
.	O

Human	O
sarcasm	O
is	O
often	O
communicated	O
by	O
stating	O
something	O
which	O
the	O
author	O
presumes	O
to	O
be	O
so	O
obviously	O
untrue	O
that	O
it	O
will	O
be	O
read	O
as	O
sarcastic	O
.	O
These	O
presumptions	O
reflect	O
the	O
author	O
's	O
biases	O
-	O
or	O
in	O
the	O
cases	O
of	O
comment	O
annotation	O
,	O
labelling	O
comments	O
as	O
sarcastic	O
reflects	O
the	O
annotators	O
beliefs	O
of	O
what	O
is	O
obviously	O
untrue	O
.	O
With	O
the	O
comment	O
corpus	O
being	O
in	O
English	O
,	O
and	O
given	O
the	O
subtlety	O
of	O
the	O
attributes	O
,	O
higher	O
quality	O
annotations	O
were	O
likely	O
to	O
be	O
achieved	O
by	O
annotators	O
with	O
first	O
-	O
language	O
proficiency	O
in	O
English	O
.	O
The	O
best	O
proxy	O
for	O
this	O
available	O
on	O
the	O
Figure	O
Eight	O
platform	O
was	O
to	O
restrict	O
the	O
country	O
of	O
origin	O
of	O
our	O
annotators	O
to	O
a	O
limited	O
subset	O
of	O
countries	O
with	O
a	O
large	O
English	O
-	O
speaking	O
population	O
(	O
as	O
either	O
an	O
official	O
language	O
or	O
primary	O
second	O
language	O
)	O
,	O
in	O
particular	O
:	O
the	O
United	O
States	O
,	O
the	O
United	O
Kingdom	O
,	O
South	O
Africa	O
,	O
Sweden	O
,	O
New	O
Zealand	O
,	O
Norway	O
,	O
Netherlands	O
,	O
Denmark	O
,	O
Canada	O
,	O
and	O
Australia	O
.	O
Although	O
our	O
early	O
iterations	O
of	O
this	O
annotation	O
job	O
indicated	O
a	O
significant	O
reduction	O
in	O
annotators	O
failing	O
test	O
comments	O
once	O
this	O
was	O
enforced	O
,	O
this	O
introduces	O
a	O
clear	O
cultural	O
and	O
geographic	O
bias	O
.	O
For	O
example	O
,	O
the	O
comment	O
'	O
Iran	O
and	O
Turkey	O
are	O
the	O
BEST	O
places	O
to	O
be	O
a	O
woman	O
!	O
'	O
,	O
was	O
scored	O
as	O
sarcastic	O
with	O
72	O
%	O
confidence	O
by	O
the	O
annotators	O
.	O
Finding	O
this	O
comment	O
sarcastic	O
relies	O
on	O
an	O
assumption	O
by	O
the	O
annotators	O
(	O
of	O
which	O
the	O
pool	O
excludes	O
residents	O
of	O
Iran	O
and	O
Turkey	O
)	O
that	O
Iran	O
and	O
Turkey	O
are	O
clearly	O
not	O
the	O
best	O
places	O
to	O
be	O
women	O
.	O
Our	O
annotators	O
were	O
not	O
selected	O
as	O
broadly	O
representative	O
across	O
language	O
,	O
geography	O
,	O
culture	O
,	O
or	O
other	O
attributes	O
and	O
this	O
assumption	O
is	O
not	O
universal	O
.	O
While	O
important	O
research	O
has	O
begun	O
to	O
explore	O
the	O
composition	O
of	O
the	O
global	O
crowd	O
workforce	O
,	O
it	O
remains	O
difficult	O
to	O
select	O
for	O
annotators	O
representative	O
of	O
specific	O
characteristics	O
on	O
crowd	O
work	O
platforms	O
(	O
Posch	O
et	O
al	O
,	O
2018	O
)	O
.	O
In	O
the	O
current	O
version	O
of	O
the	O
Appen	O
platform	O
,	O
unless	O
annotators	O
are	O
asked	O
standalone	O
questions	O
on	O
demographics	O
,	O
the	O
only	O
available	O
de	O
-	O
tails	O
are	O
the	O
annotators	O
'	O
country	O
and/or	O
city	O
(	O
and	O
even	O
then	O
,	O
only	O
for	O
some	O
annotators	O
)	O
.	O
Research	O
and	O
modelling	O
based	O
on	O
this	O
dataset	O
,	O
and	O
similar	O
datasets	O
,	O
requires	O
the	O
exercise	O
of	O
great	O
care	O
in	O
mitigating	O
biases	O
produced	O
by	O
the	O
underlying	O
data	O
collection	O
.	O
This	O
potential	O
selection	B-TaskName
bias	I-TaskName
is	O
likely	O
to	O
be	O
evident	O
across	O
the	O
broader	O
healthy	O
/	O
unhealthy	O
categorisation	O
along	O
with	O
each	O
of	O
the	O
attributes	O
.	O
Prior	O
research	O
has	O
found	O
substantial	O
disagreement	O
on	O
subtle	O
attributes	O
of	O
speech	O
both	O
among	O
individuals	O
and	O
across	O
geographies	O
(	O
Salminen	O
et	O
al	O
,	O
2018a	O
)	O
.	O
Finally	O
,	O
the	O
source	O
of	O
the	O
comments	O
and	O
their	O
manner	O
of	O
presentation	O
could	O
introduce	O
bias	O
into	O
the	O
dataset	O
.	O
The	O
source	O
data	O
is	O
solely	O
from	O
a	O
Canadian	O
online	O
newspaper	O
comment	O
section	O
and	O
comments	O
were	O
presented	O
in	O
isolation	O
to	O
annotators	O
,	O
without	O
the	O
surrounding	O
context	O
of	O
the	O
news	O
article	O
and	O
other	O
comments	O
.	O
Annotators	O
were	O
also	O
provided	O
with	O
the	O
standard	O
questionnaire	O
(	O
Appendix	O
A	O
)	O
,	O
which	O
includes	O
high	O
level	O
descriptions	O
of	O
the	O
attributes	O
that	O
may	O
not	O
generalise	O
across	O
cultures	O
.	O
There	O
is	O
a	O
substantial	O
body	O
of	O
research	O
demonstrating	O
the	O
potential	O
impact	O
of	O
introducing	O
biased	O
datasets	O
,	O
and	O
Vidgen	O
et	O
al	O
(	O
Vidgen	O
et	O
al	O
,	O
2019	O
)	O
note	O
that	O
public	O
datasets	O
in	O
this	O
area	O
are	O
prone	O
to	O
systematic	O
bias	O
and	O
mislabelling	O
,	O
with	O
interannotator	O
agreement	O
typically	O
low	O
for	O
complex	O
multi	O
-	O
class	O
tasks	O
of	O
this	O
kind	O
.	O
These	O
challenges	O
are	O
to	O
be	O
expected	O
in	O
a	O
relatively	O
new	O
field	O
which	O
aims	O
to	O
improve	O
on	O
human	O
baseline	O
moderation	O
for	O
highly	O
subjective	O
characteristics	O
of	O
online	O
discussion	O
.	O
At	O
this	O
early	O
stage	O
of	O
research	O
,	O
we	O
must	O
be	O
mindful	O
of	O
addressing	O
these	O
biases	O
and	O
cognisant	O
that	O
the	O
manner	O
in	O
which	O
this	O
data	O
is	O
collected	O
can	O
have	O
critical	O
impacts	O
on	O
users	O
in	O
a	O
production	O
environment	O
.	O
It	O
is	O
important	O
to	O
note	O
at	O
this	O
stage	O
of	O
the	O
field	O
in	O
general	O
,	O
and	O
with	O
our	O
understanding	O
of	O
this	O
dataset	O
in	O
particular	O
,	O
that	O
the	O
UCC	B-DatasetName
dataset	O
is	O
not	O
designed	O
to	O
train	O
models	O
which	O
are	O
immediately	O
available	O
for	O
automated	O
moderation	O
without	O
human	O
intervention	O
in	O
a	O
live	O
online	O
setting	O
.	O
As	O
the	O
field	O
develops	O
further	O
,	O
initial	O
use	O
-	O
cases	O
may	O
include	O
less	O
interventionist	O
'	O
nudges	O
'	O
or	O
reminders	O
of	O
how	O
a	O
comment	O
could	O
be	O
perceived	O
by	O
a	O
reader	O
to	O
assist	O
participants	O
in	O
discussions	O
online	O
.	O

We	O
introduced	O
a	O
new	O
corpus	O
of	O
labelled	O
comments	O
and	O
a	O
typology	O
for	O
some	O
of	O
the	O
more	O
subtle	O
aspects	O
of	O
unhealthy	O
online	O
conversation	O
.	O
Our	O
typology	O
provides	O
6	O
sub	O
-	O
attributes	O
of	O
typically	O
unhealthy	O
con	O
-	O
tributions	O
,	O
and	O
confidence	O
scores	O
for	O
the	O
labels	O
.	O
We	O
described	O
the	O
process	O
and	O
challenges	O
in	O
creating	O
such	O
a	O
dataset	O
,	O
and	O
provided	O
statistics	O
to	O
convey	O
the	O
scale	O
of	O
data	O
.	O
In	O
particular	O
,	O
we	O
note	O
that	O
although	O
there	O
is	O
a	O
substantial	O
body	O
of	O
research	O
on	O
more	O
extreme	O
forms	O
of	O
negative	O
contributions	O
,	O
such	O
as	O
toxicity	O
,	O
the	O
subtler	O
forms	O
of	O
unhealthy	O
comments	O
in	O
our	O
typology	O
are	O
often	O
similarly	O
prevalent	O
online	O
.	O
Our	O
analysis	O
also	O
shows	O
that	O
the	O
sub	O
-	O
attributes	O
are	O
largely	O
independent	O
from	O
overt	O
toxicity	O
,	O
and	O
mostly	O
correlated	O
with	O
unhealthy	O
contributions	O
.	O
We	O
also	O
provide	O
results	O
from	O
a	O
modern	O
baseline	O
ML	O
model	O
(	O
fine	O
tuning	O
BERT	B-MethodName
)	O
and	O
note	O
that	O
performance	O
exceeds	O
that	O
of	O
a	O
crowd	O
-	O
worker	O
.	O
This	O
suggests	O
that	O
further	O
work	O
could	O
also	O
be	O
done	O
to	O
collect	O
a	O
larger	O
corpus	O
of	O
annotations	O
to	O
improve	O
the	O
capacity	O
to	O
measure	O
models	O
in	O
this	O
domain	O
.	O
While	O
this	O
dataset	O
provides	O
a	O
new	O
contribution	O
in	O
gathering	O
the	O
6	O
attributes	O
under	O
the	O
umbrella	O
of	O
an	O
'	O
unhealthy	O
'	O
conversation	O
,	O
there	O
also	O
remains	O
an	O
open	O
question	O
as	O
to	O
how	O
exhaustive	O
this	O
typology	O
of	O
unhealthy	O
contributions	O
is	O
.	O
Future	O
research	O
and	O
annotation	O
work	O
could	O
further	O
refine	O
the	O
typology	O
,	O
amend	O
the	O
standard	O
questionnaire	O
,	O
or	O
apply	O
it	O
to	O
forums	O
which	O
differ	O
in	O
cultural	O
and	O
geographic	O
context	O
.	O
Further	O
work	O
also	O
includes	O
exploring	O
the	O
unintended	O
biases	O
in	O
the	O
model	O
and	O
data	O
.	O
This	O
dataset	O
is	O
well	O
-	O
placed	O
to	O
further	O
explore	O
early	O
signs	O
of	O
conversations	O
going	O
awry	O
(	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
,	O
while	O
models	O
based	O
on	O
the	O
data	O
could	O
be	O
explored	O
to	O
provide	O
assistance	O
to	O
moderating	O
online	O
conversations	O
.	O
A	O
Annotator	O
Questionnaire	O

In	O
this	O
paper	O
,	O
we	O
describe	O
two	O
systems	O
we	O
developed	O
for	O
the	O
three	O
tracks	O
we	O
have	O
participated	O
in	O
the	O
BEA	O
-	O
2019	O
GEC	O
Shared	O
Task	O
.	O
We	O
investigate	O
competitive	O
classification	O
models	O
with	O
bi	O
-	O
directional	O
recurrent	O
neural	O
networks	O
(	O
Bi	O
-	O
RNN	O
)	O
and	O
neural	O
machine	B-TaskName
translation	I-TaskName
(	O
NMT	O
)	O
models	O
.	O
For	O
different	O
tracks	O
,	O
we	O
use	O
ensemble	O
systems	O
to	O
selectively	O
combine	O
the	O
NMT	O
models	O
,	O
the	O
classification	O
models	O
,	O
and	O
some	O
rules	O
,	O
and	O
demonstrate	O
that	O
an	O
ensemble	O
solution	O
can	O
effectively	O
improve	O
GEC	O
performance	O
over	O
single	O
systems	O
.	O
Our	O
GEC	O
systems	O
ranked	O
the	O
first	O
in	O
the	O
Unrestricted	B-DatasetName
Track	O
,	O
and	O
the	O
third	O
in	O
both	O
the	O
Restricted	B-DatasetName
Track	O
and	O
the	O
Low	O
Resource	O
Track	O
.	O

Grammatical	B-TaskName
error	I-TaskName
correction	I-TaskName
(	O
GEC	O
)	O
is	O
the	O
task	O
of	O
automatically	O
correcting	O
grammatical	O
errors	O
in	O
text	O
.	O
With	O
the	O
increasing	O
number	O
of	O
language	O
learners	O
,	O
GEC	O
has	O
gained	O
more	O
and	O
more	O
attention	O
from	O
educationists	O
and	O
researchers	O
in	O
the	O
past	O
decade	O
.	O
The	O
following	O
is	O
a	O
GEC	O
example	O
:	O
I	O
[	O
fall	O
fell	O
]	O
asleep	O
at	O
11	O
p.m.	O
last	O
[	O
nigh	O
night	O
]	O
.	O
Here	O
fall	O
needs	O
to	O
be	O
corrected	O
to	O
its	O
past	O
tense	O
form	O
and	O
nigh	O
is	O
a	O
spelling	O
mistake	O
.	O
GEC	O
is	O
considered	O
as	O
a	O
mapping	O
task	O
from	O
incorrect	O
sentences	O
to	O
correct	O
sentences	O
.	O
Incorrect	O
sentences	O
can	O
be	O
seen	O
as	O
being	O
produced	O
by	O
adding	O
noises	O
to	O
correct	O
sentences	O
.	O
The	O
added	O
noise	O
does	O
not	O
happen	O
randomly	O
,	O
but	O
occurs	O
when	O
people	O
learn	O
or	O
use	O
the	O
language	O
according	O
to	O
a	O
certain	O
error	O
distribution	O
and	O
language	O
usage	O
bias	O
.	O
Initially	O
,	O
people	O
used	O
rule	O
-	O
based	O
approaches	O
to	O
solve	O
GEC	O
problems	O
(	O
Naber	O
and	O
Miłkowski	O
,	O
2005	O
)	O
.	O
Rules	O
are	O
relatively	O
easy	O
to	O
make	O
but	O
with	O
poor	O
generalization	O
.	O
Later	O
researchers	O
began	O
to	O
treat	O
GEC	O
as	O
a	O
classification	O
task	O
.	O
According	O
to	O
the	O
grammatical	O
information	O
around	O
the	O
target	O
word	O
,	O
classifiers	O
can	O
be	O
constructed	O
to	O
predict	O
the	O
true	O
grammatical	O
role	O
of	O
the	O
target	O
word	O
.	O
One	O
drawback	O
of	O
the	O
classification	O
methods	O
for	O
GEC	O
is	O
that	O
training	O
different	O
classifiers	O
for	O
different	O
error	O
types	O
may	O
be	O
resource	O
-	O
intensive	O
and	O
inefficient	O
since	O
there	O
are	O
many	O
grammatical	O
error	O
types	O
.	O
Recently	O
,	O
translation	O
methods	O
have	O
become	O
the	O
focus	O
of	O
research	O
,	O
and	O
there	O
is	O
a	O
clear	O
trend	O
that	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
GEC	O
systems	O
are	O
being	O
shifted	O
from	O
traditional	O
NLP	O
methods	O
to	O
NMT	O
based	O
methods	O
.	O
In	O
recent	O
years	O
,	O
GEC	O
performance	O
has	O
seen	O
significant	O
improvement	O
in	O
some	O
public	O
GEC	O
test	O
sets	O
(	O
Ge	O
et	O
al	O
,	O
2018	O
)	O
.	O
In	O
CoNLL	O
-	O
2013	O
(	O
Ng	O
et	O
al	O
,	O
2013	O
and	O
CoNLL	O
-	O
2014	O
(	O
Ng	O
et	O
al	O
,	O
2014	O
)	O
GEC	O
Shared	O
Task	O
,	O
machine	O
learning	O
based	O
GEC	O
methods	O
emerged	O
with	O
relatively	O
good	O
performance	O
.	O
Classification	B-TaskName
methods	O
achieved	O
the	O
best	O
result	O
in	O
CoNLL	O
-	O
2013	O
(	O
Rozovskaya	O
et	O
al	O
,	O
2013	O
)	O
.	O
After	O
that	O
,	O
statistical	O
machine	B-TaskName
translation	I-TaskName
(	O
SMT	O
)	O
methods	O
began	O
to	O
show	O
better	O
performance	O
in	O
CoNLL	O
-	O
2014	O
(	O
Felice	O
et	O
al	O
,	O
2014	O
)	O
.	O
(	O
Chollampatt	O
et	O
al	O
,	O
2016	O
)	O
was	O
the	O
first	O
study	O
to	O
obtain	O
the	O
state	O
-	O
ofthe	O
-	O
art	O
result	O
with	O
neural	O
networks	O
.	O
Then	O
after	O
(	O
Junczys	O
-	O
Dowmunt	O
and	O
Grundkiewicz	O
,	O
2016	O
)	O
,	O
machine	B-TaskName
translation	I-TaskName
methods	O
became	O
the	O
mainstream	O
in	O
GEC	O
solutions	O
.	O
In	O
addition	O
,	O
an	O
RNNbased	O
context	O
model	O
achieved	O
better	O
results	O
than	O
previous	O
traditional	O
classification	O
models	O
(	O
Wang	O
et	O
al	O
,	O
2017	O
)	O
.	O
Using	O
a	O
CNN	O
-	O
based	O
sequenceto	O
-	O
sequence	O
architecture	O
(	O
Gehring	O
et	O
al	O
,	O
2017	O
)	O
,	O
(	O
Chollampatt	O
and	O
Ng	O
,	O
2018	O
)	O
proposed	O
the	O
first	O
end	O
-	O
to	O
-	O
end	O
NMT	O
model	O
and	O
reported	O
the	O
state	O
-	O
ofthe	O
-	O
art	O
result	O
.	O
As	O
Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
plays	O
an	O
increasingly	O
important	O
role	O
in	O
sequence	O
modeling	O
,	O
Transformer	B-MethodName
-	O
based	O
end	O
-	O
to	O
-	O
end	O
NMT	O
models	O
began	O
to	O
lead	O
the	O
current	O
GEC	O
research	O
Ge	O
et	O
al	O
,	O
2018	O
;	O
Zhao	O
et	O
al	O
,	O
2019	O
)	O
.	O
It	O
is	O
worth	O
mentioning	O
that	O
(	O
Lichtarge	O
et	O
al	O
,	O
2019	O
)	O
used	O
Wikipedia	O
ed	O
-	O
its	O
history	O
corpus	O
,	O
which	O
is	O
huge	O
but	O
noisy	O
,	O
and	O
gained	O
a	O
result	O
very	O
close	O
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
result	O
.	O
Learning	O
a	O
GEC	O
translation	O
model	O
from	O
noisy	O
data	O
is	O
a	O
worthy	O
future	O
direction	O
as	O
the	O
GEC	O
parallel	O
corpus	O
is	O
expensive	O
to	O
obtain	O
.	O
This	O
paper	O
describes	O
our	O
two	O
systems	O
for	O
the	O
three	O
tracks	O
in	O
the	O
BEA	O
-	O
2019	O
GEC	O
Shared	O
Task	O
(	O
Bryant	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
use	O
two	O
popular	O
NMT	O
models	O
and	O
two	O
improved	O
versions	O
of	O
neural	O
classification	O
models	O
to	O
train	O
the	O
basic	O
models	O
.	O
Ensemble	O
strategies	O
are	O
then	O
used	O
to	O
combine	O
outcomes	O
from	O
different	O
models	O
.	O
Our	O
two	O
systems	O
for	O
the	O
three	O
tracks	O
are	O
described	O
in	O
next	O
section	O
.	O
In	O
Section	O
3	O
,	O
we	O
evaluate	O
the	O
systems	O
on	O
the	O
development	O
data	O
and	O
show	O
the	O
final	O
results	O
on	O
the	O
test	O
data	O
.	O
Section	O
4	O
concludes	O
the	O
paper	O
and	O
summarizes	O
the	O
future	O
work	O
.	O

We	O
submitted	O
the	O
same	O
system	O
output	O
for	O
the	O
Restricted	B-DatasetName
and	O
Unrestricted	B-DatasetName
tasks	O
.	O
The	O
system	O
uses	O
several	O
ensemble	O
methods	O
to	O
combine	O
the	O
CNNbased	O
and	O
Transformer	B-MethodName
-	O
based	O
translation	O
models	O
,	O
described	O
in	O
details	O
below	O
.	O

Transformer	B-MethodName
is	O
currently	O
considered	O
to	O
be	O
one	O
of	O
the	O
most	O
powerful	O
models	O
for	O
sequence	O
modeling	O
.	O
For	O
GEC	O
,	O
some	O
of	O
the	O
best	O
recent	O
results	O
reported	O
on	O
CoNLL	O
-	O
2014	O
test	O
set	O
are	O
obtained	O
by	O
Transformer	B-MethodName
-	O
based	O
translation	O
models	O
.	O
We	O
trained	O
eight	O
Transformer	B-MethodName
-	O
based	O
translation	O
models	O
in	O
a	O
low	O
resource	O
translation	O
paradigm	O
.	O
We	O
tuned	O
parameters	O
for	O
domain	O
and	O
error	O
adaptation	O
.	O
We	O
also	O
compared	O
the	O
results	O
using	O
2	O
GPUs	O
and	O
4	O
GPUs	O
as	O
the	O
authors	O
reported	O
the	O
difference	O
in	O
their	O
Github	O
repository	O
1	O
.	O

For	O
the	O
Low	O
Resource	O
Track	O
we	O
developed	O
different	O
individual	O
systems	O
and	O
used	O
an	O
ensemble	O
method	O
to	O
combine	O
them	O
.	O
For	O
the	O
translation	O
model	O
,	O
we	O
did	O
not	O
obtain	O
very	O
strong	O
performance	O
because	O
the	O
training	O
data	O
is	O
limited	O
.	O
We	O
also	O
explored	O
the	O
noisy	O
Wikipedia	O
edit	O
history	O
corpus	O
for	O
the	O
Transformer	B-MethodName
-	O
based	O
translation	O
model	O
.	O
However	O
,	O
we	O
noticed	O
that	O
,	O
for	O
some	O
error	O
types	O
with	O
clear	O
definitions	O
,	O
the	O
classifiers	O
trained	O
on	O
a	O
large	O
amount	O
of	O
native	O
corpus	O
have	O
good	O
performance	O
.	O
In	O
addition	O
,	O
we	O
made	O
some	O
grammatical	O
rules	O
to	O
correct	O
errors	O
and	O
adopted	O
an	O
off	O
-	O
the	O
-	O
shelf	O
spelling	O
checker	O
(	O
Kelly	O
,	O
2006	O
)	O
.	O
Finally	O
,	O
we	O
leverage	O
a	O
sim	O
-	O
ple	O
ensemble	O
method	O
to	O
combine	O
all	O
of	O
the	O
classifiers	O
,	O
rules	O
,	O
spelling	O
checker	O
and	O
translation	O
models	O
.	O
Note	O
that	O
for	O
the	O
Restricted	B-DatasetName
and	O
Unrestricted	B-DatasetName
tracks	O
,	O
we	O
did	O
not	O
observe	O
any	O
gain	O
from	O
the	O
classification	O
models	O
or	O
the	O
rule	O
-	O
based	O
methods	O
,	O
therefore	O
only	O
the	O
translation	O
systems	O
were	O
used	O
for	O
those	O
tracks	O
.	O

After	O
an	O
analysis	O
of	O
the	O
development	O
sets	O
,	O
we	O
decided	O
to	O
build	O
classifiers	O
for	O
eight	O
common	O
error	O
types	O
.	O
Based	O
on	O
(	O
Wang	O
et	O
al	O
,	O
2017	O
)	O
,	O
we	O
developed	O
two	O
classification	O
model	O
structures	O
for	O
the	O
eight	O
error	O
types	O
.	O
(	O
A	O
)	O
Bi	O
-	O
GRU	B-MethodName
context	O
model	O
Figure	O
2	O
shows	O
the	O
bi	O
-	O
directional	O
GRU	B-MethodName
context	O
model	O
we	O
use	O
to	O
determine	O
the	O
right	O
grammatical	O
category	O
for	O
a	O
target	O
word	O
.	O
The	O
concatenated	O
left	O
and	O
right	O
source	O
states	O
of	O
the	O
target	O
word	O
form	O
the	O
contextual	O
semantic	O
vector	O
representation	O
.	O
This	O
is	O
used	O
as	O
a	O
query	O
to	O
calculate	O
the	O
attention	O
weight	O
a	O
t	O
.	O
An	O
attention	O
vector	O
C	O
t	O
is	O
then	O
computed	O
as	O
the	O
weighted	O
average	O
,	O
according	O
to	O
a	O
t	O
,	O
over	O
all	O
the	O
source	O
states	O
.	O
C	O
t	O
is	O
then	O
fed	O
through	O
a	O
fully	O
connected	O
layer	O
and	O
softmax	B-MethodName
layer	O
to	O
produce	O
the	O
predictive	O
distribution	O
.	O
We	O
use	O
this	O
to	O
train	O
models	O
for	O
the	O
following	O
error	O
types	O
:	O
Subject	O
-	O
verb	O
agreement	O
,	O
Article	O
,	O
Plural	O
or	O
singular	O
noun	O
,	O
Verb	O
form	O
,	O
Preposition	O
substitution	O
,	O
Missing	O
comma	O
and	O
Period	O
comma	O
substitution	O
.	O
Labels	O
for	O
each	O
task	O
were	O
extracted	O
automatically	O
from	O
the	O
native	O
corpus	O
through	O
part	O
-	O
ofspeech	O
tagging	O
tools	O
.	O
(	O
B	O
)	O
Pointer	O
context	O
model	O
The	O
classifiers	O
above	O
use	O
the	O
same	O
classification	O
labels	O
for	O
different	O
target	O
words	O
.	O
We	O
also	O
need	O
a	O
classification	O
model	O
to	O
deal	O
with	O
the	O
problem	O
as	O
in	O
the	O
Word	O
form	O
task	O
,	O
where	O
each	O
word	O
has	O
a	O
different	O
set	O
of	O
predictive	O
labels	O
(	O
as	O
shown	O
for	O
word	O
'	O
gone	O
'	O
in	O
Figure	O
3	O
)	O
.	O
Inspired	B-DatasetName
by	O
the	O
Pointer	B-MethodName
network	I-MethodName
model	O
(	O
Vinyals	O
et	O
al	O
,	O
2015	O
)	O
,	O
we	O
proposed	O
the	O
pointer	O
context	O
model	O
.	O
Figure	O
3	O
shows	O
the	O
pointer	O
context	O
model	O
that	O
takes	O
the	O
target	O
word	O
's	O
confusion	O
set	O
as	O
the	O
label	O
candidates	O
.	O
The	O
computation	O
path	O
is	O
the	O
same	O
as	O
the	O
Bi	O
-	O
GRU	B-MethodName
model	O
structure	O
.	O
We	O
concatenate	O
the	O
target	O
word	O
's	O
char	O
-	O
based	O
embedding	O
and	O
C	O
t	O
to	O
obtain	O
C	O
1	O
t	O
,	O
and	O
then	O
use	O
it	O
as	O
the	O
query	O
to	O
compute	O
dot	O
product	O
a	O
1	O
t	O
with	O
each	O
of	O
the	O
word	B-TaskName
embeddings	I-TaskName
in	O
the	O
confusion	O
set	O
.	O
a	O
1	O
t	O
is	O
then	O
fed	O
through	O
a	O
softmax	B-MethodName
layer	O
to	O
produce	O
the	O
predictive	O
distribution	O
.	O
This	O
model	O
is	O
very	O
effective	O
at	O
dealing	O
with	O
varying	O
number	O
of	O
candidates	O
as	O
seen	O
in	O
the	O
Word	O
form	O
task	O
.	O

We	O
use	O
the	O
same	O
Transformer	B-MethodName
-	O
based	O
translation	O
model	O
mentioned	O
in	O
Subsection	O
3.2.2	O
.	O
Due	O
to	O
the	O
limitation	O
of	O
the	O
corpus	O
,	O
we	O
leverage	O
the	O
Wiked	O
(	O
Grundkiewicz	O
and	O
Junczys	O
-	O
Dowmunt	O
,	O
2014	O
)	O
as	O
our	O
training	O
corpus	O
for	O
the	O
NMT	O
model	O
.	O

We	O
use	O
the	O
conflict	O
solver	O
described	O
above	O
to	O
do	O
the	O
ensemble	O
for	O
all	O
of	O
the	O
outputs	O
of	O
the	O
classifiers	O
,	O
rules	O
,	O
spell	O
checker	O
and	O
NMT	O
model	O
.	O
(	O
Mizumoto	O
et	O
al	O
,	O
2012	O
)	O
,	O
NUCLE	O
(	O
Ng	O
et	O
al	O
,	O
2014	O
,	O
W&I+LOCNESS	O
(	O
Bryant	O
et	O
al	O
,	O
2019	O
)	O
and	O
Common	B-DatasetName
Crawl	I-DatasetName
.	O
We	O
use	O
Common	B-DatasetName
Crawl	I-DatasetName
to	O
pretrain	O
the	O
decoder	O
parameters	O
for	O
the	O
Transformer	B-MethodName
-	O
based	O
translation	O
model	O
.	O
FCE	B-DatasetName
,	O
Lang	O
-	O
8	O
,	O
NUCLE	O
and	O
W&I	O
are	O
used	O
to	O
train	O
all	O
of	O
the	O
translation	O
models	O
.	O
It	O
is	O
worth	O
noting	O
that	O
we	O
did	O
data	B-TaskName
augmentation	I-TaskName
for	O
W&I	O
to	O
train	O
all	O
of	O
the	O
translation	O
models	O
.	O
The	O
data	O
sets	O
used	O
in	O
Low	O
Resource	O
Track	O
include	O
Wiked	O
,	O
Wikipedia	O
Dumps	O
and	O
Common	B-DatasetName
Crawl	I-DatasetName
.	O
All	O
of	O
the	O
classifiers	O
are	O
trained	O
on	O
Wikipedia	O
Dumps	O
and	O
the	O
translation	O
model	O
is	O
trained	O
on	O
Wiked	O
corpus	O
.	O
For	O
Wiked	O
corpus	O
,	O
we	O
did	O
some	O
data	O
cleaning	O
work	O
.	O
We	O
discarded	O
some	O
noisy	O
sentences	O
that	O
include	O
error	O
types	O
such	O
as	O
U	O
:	O
OTHER	O
,	O
R	O
:	O
OTHER	O
,	O
R	O
:	O
NOUN	O
,	O
etc	O
.	O
The	O
development	O
set	O
from	O
W&I+LOCNESS	O
are	O
used	O
in	O
all	O
the	O
tracks	O
.	O
Following	O
the	O
data	O
pre	O
-	O
processing	O
pipeline	O
used	O
to	O
generate	O
the	O
data	O
provided	O
by	O
the	O
shared	O
task	O
,	O
we	O
tokenize	O
all	O
of	O
the	O
data	O
using	O
spaCy	O
3	O
.	O

We	O
added	O
the	O
W&I	O
corpus	O
eight	O
times	O
to	O
the	O
training	O
corpus	O
for	O
domain	B-TaskName
adaptation	I-TaskName
.	O
Table	O
2	O
shows	O
the	O
performance	O
of	O
the	O
single	O
CNN	O
-	O
based	O
translation	O
models	O
.	O
All	O
the	O
parameters	O
in	O
Table	O
2	O
are	O
tuned	O
over	O
the	O
W&I+LOCNESS	O
development	O
set	O
.	O
Table	O
3	O
shows	O
the	O
results	O
of	O
the	O
four	O
CNN	O
-	O
based	O
ensemble	O
systems	O
.	O
We	O
use	O
ensembles	O
in	O
the	O
same	O
way	O
as	O
(	O
Chollampatt	O
and	O
Ng	O
,	O
2018	O
)	O
.	O
The	O
above	O
results	O
prove	O
that	O
the	O
ensemble	O
method	O
has	O
yielded	O
a	O
very	O
large	O
improvement	O
in	O
this	O
task	O
.	O

Table	O
6	O
summarizes	O
some	O
results	O
on	O
the	O
development	O
set	O
and	O
gives	O
the	O
official	O
test	O
result	O
.	O
We	O
can	O
see	O
that	O
the	O
individual	O
CNN	O
or	O
Transformer	B-MethodName
-	O
based	O
translation	O
models	O
perform	O
reasonably	O
well	O
,	O
and	O
the	O
ensemble	O
methods	O
consistently	O
outperform	O
the	O
individual	O
systems	O
.	O
The	O
second	O
pass	O
correction	O
further	O
improves	O
the	O
performance	O
,	O
and	O
the	O
last	O
post	O
-	O
processing	O
step	O
boosts	O
both	O
recall	O
and	O
F	O
0.5	O
.	O
Step	O

A	O
Transformer	B-MethodName
-	O
based	O
translation	O
model	O
is	O
trained	O
on	O
the	O
filtered	O
Wiked	O
corpus	O
.	O
The	O
model	O
architecture	O
follows	O
that	O
in	O
.	O
Although	O
the	O
performance	O
of	O
the	O
NMT	O
model	O
is	O
not	O
strong	O
,	O
it	O
provides	O
good	O
performance	O
equivalent	O
to	O
the	O
classifiers	O
for	O
some	O
error	O
types	O
.	O

We	O
have	O
presented	O
two	O
different	O
systems	O
for	O
the	O
three	O
GEC	O
tracks	O
.	O
When	O
there	O
is	O
a	O
sufficient	O
parallel	O
learner	O
corpus	O
,	O
such	O
as	O
in	O
Restricted	B-DatasetName
Track	O
and	O
Unrestricted	B-DatasetName
Track	O
,	O
the	O
NMT	O
ensemble	O
model	O
is	O
the	O
best	O
choice	O
to	O
implement	O
a	O
GEC	O
system	O
.	O
We	O
have	O
evaluated	O
two	O
kinds	O
of	O
NMT	O
models	O
:	O
CNN	O
-	O
based	O
and	O
Transformer	B-MethodName
-	O
based	O
translation	O
models	O
.	O
We	O
have	O
also	O
explored	O
different	O
ensemble	O
strategies	O
from	O
multiple	O
base	O
mod	O
-	O
els	O
to	O
maximize	O
the	O
overall	O
system	O
performance	O
.	O
Finally	O
we	O
reached	O
the	O
result	O
of	O
F	O
0.5	O
=	O
0.6678	O
on	O
the	O
official	O
test	O
set	O
in	O
Restricted	B-DatasetName
Track	O
and	O
Unrestricted	B-DatasetName
Track	O
,	O
ranking	O
the	O
third	O
in	O
the	O
Restricted	B-DatasetName
track	O
4	O
.	O
It	O
is	O
worth	O
noting	O
that	O
there	O
is	O
a	O
huge	O
gap	O
between	O
the	O
results	O
on	O
the	O
development	O
set	O
and	O
the	O
test	O
set	O
,	O
which	O
suggests	O
that	O
there	O
might	O
be	O
an	O
unneglectable	O
mismatch	O
between	O
the	O
development	O
set	O
and	O
the	O
test	O
set	O
.	O
Indeed	O
,	O
the	O
development	O
set	O
is	O
annotated	O
by	O
one	O
annotator	O
,	O
while	O
the	O
test	O
set	O
is	O
annotated	O
by	O
five	O
,	O
as	O
announced	O
officially	O
.	O
For	O
Low	O
Resource	O
Track	O
,	O
there	O
is	O
a	O
lack	O
of	O
parallel	O
learner	O
corpus	O
,	O
and	O
thus	O
we	O
rely	O
less	O
on	O
the	O
translation	O
models	O
.	O
We	O
have	O
built	O
eight	O
classifiers	O
trained	O
on	O
Wikipedia	O
dumps	O
according	O
to	O
different	O
error	O
types	O
and	O
an	O
NMT	O
model	O
trained	O
on	O
the	O
Wikipedia	O
edits	O
history	O
corpus	O
.	O
By	O
a	O
simple	O
ensemble	O
method	O
,	O
we	O
reached	O
F	O
0.5	O
=	O
0.5181	O
,	O
placing	O
our	O
system	O
in	O
the	O
third	O
place	O
in	O
Low	O
Resource	O
Track	O
.	O
Although	O
GEC	O
has	O
reached	O
the	O
human	O
level	O
performance	O
on	O
some	O
GEC	O
test	O
sets	O
,	O
there	O
is	O
still	O
room	O
for	O
improvement	O
.	O
In	O
a	O
low	O
resource	O
setup	O
,	O
how	O
to	O
deal	O
with	O
the	O
huge	O
but	O
noisy	O
data	O
is	O
worth	O
exploring	O
.	O
(	O
Lichtarge	O
et	O
al	O
,	O
2019	O
)	O
gave	O
a	O
good	O
solution	O
on	O
this	O
topic	O
,	O
but	O
more	O
work	O
needs	O
to	O
be	O
done	O
.	O
Second	O
,	O
we	O
will	O
investigate	O
methods	O
such	O
as	O
the	O
reinforcement	O
learning	O
based	O
method	O
(	O
Wu	O
et	O
al	O
,	O
2018	O
)	O
to	O
address	O
the	O
mismatch	O
between	O
the	O
training	O
objectives	O
and	O
evaluation	O
methods	O
in	O
GEC	O
.	O

In	O
this	O
paper	O
we	O
present	O
a	O
new	O
unsupervised	O
approach	O
,	O
"	O
Attraction	O
to	O
Topics	O
"	O
-	O
A2	O
T	O
,	O
for	O
the	O
detection	O
of	O
argumentative	O
units	O
,	O
a	O
sub	O
-	O
task	O
of	O
argument	B-TaskName
mining	I-TaskName
.	O
Motivated	O
by	O
the	O
importance	O
of	O
topic	O
identification	O
in	O
manual	O
annotation	O
,	O
we	O
examine	O
whether	O
topic	O
modeling	O
can	O
be	O
used	O
for	O
performing	O
unsupervised	O
detection	O
of	O
argumentative	O
sentences	O
,	O
and	O
to	O
what	O
extend	O
topic	O
modeling	O
can	O
be	O
used	O
to	O
classify	O
sentences	O
as	O
claims	O
and	O
premises	O
.	O
Preliminary	O
evaluation	O
results	O
suggest	O
that	O
topic	O
information	O
can	O
be	O
successfully	O
used	O
for	O
the	O
detection	O
of	O
argumentative	O
sentences	O
,	O
at	O
least	O
for	O
corpora	O
used	O
in	O
the	O
evaluation	O
.	O
Our	O
approach	O
has	O
been	O
evaluated	O
on	O
two	O
English	O
corpora	O
,	O
the	O
first	O
of	O
which	O
contains	O
90	O
persuasive	O
essays	O
,	O
while	O
the	O
second	O
is	O
a	O
collection	O
of	O
340	O
documents	O
from	O
user	O
generated	O
content	O
.	O

Argument	B-TaskName
mining	I-TaskName
involves	O
the	O
automatic	O
discovery	O
of	O
argument	O
components	O
(	O
i.e.	O
claims	O
,	O
premises	O
)	O
and	O
the	O
argumentative	O
relations	O
(	O
i.e.	O
supports	O
,	O
attacks	O
)	O
among	O
these	O
components	O
in	O
texts	O
.	O
Primarily	O
aiming	O
to	O
extract	O
arguments	O
from	O
texts	O
in	O
order	O
to	O
provide	O
structured	O
data	O
for	O
computational	O
models	O
of	O
argument	O
and	O
reasoning	O
engines	O
(	O
Lippi	O
and	O
Torroni	O
,	O
2015a	O
)	O
,	O
argument	B-TaskName
mining	I-TaskName
has	O
additionally	O
the	O
potential	O
to	O
support	O
applications	O
in	O
various	O
research	O
fields	O
,	O
such	O
as	O
opinion	B-TaskName
mining	I-TaskName
(	O
Goudas	O
et	O
al	O
,	O
2015	O
)	O
,	O
stance	B-TaskName
detection	I-TaskName
(	O
Hasan	O
and	O
Ng	O
,	O
2014	O
)	O
,	O
policy	O
modelling	O
(	O
Florou	O
et	O
al	O
,	O
2013	O
;	O
Goudas	O
et	O
al	O
,	O
2014	O
)	O
,	O
legal	O
information	O
systems	O
(	O
Palau	O
and	O
Moens	O
,	O
2009	O
)	O
,	O
etc	O
.	O
Argument	B-TaskName
mining	I-TaskName
is	O
usually	O
addressed	O
as	O
a	O
pipeline	O
of	O
several	O
sub	O
-	O
tasks	O
.	O
Typically	O
the	O
first	O
sub	O
-	O
task	O
is	O
the	O
separation	O
between	O
argumentative	O
and	O
non	O
-	O
argumentative	O
text	O
units	O
,	O
which	O
can	O
be	O
performed	O
at	O
various	O
granularity	O
levels	O
,	O
from	O
clauses	O
to	O
several	O
sentences	O
,	O
usually	O
depending	O
on	O
corpora	O
characteristics	O
.	O
Detection	O
of	O
argumentative	O
units	O
(	O
AU	O
)	O
1	O
,	O
as	O
discussed	O
in	O
Section	O
2	O
,	O
is	O
typically	O
modeled	O
as	O
a	O
fully	O
-	O
supervised	O
classification	O
task	O
,	O
either	O
a	O
binary	O
one	O
,	O
where	O
units	O
are	O
separated	O
in	O
argumentative	O
and	O
non	O
-	O
argumentative	O
ones	O
with	O
argumentative	O
ones	O
to	O
be	O
subsequently	O
classified	O
in	O
claims	O
and	O
premises	O
as	O
a	O
second	O
step	O
,	O
or	O
as	O
a	O
multi	O
-	O
class	O
one	O
,	O
where	O
identification	O
of	O
argumentative	O
units	O
and	O
classification	O
into	O
claims	O
and	O
premises	O
are	O
performed	O
as	O
a	O
single	O
step	O
.	O
According	O
to	O
a	O
recent	O
survey	O
(	O
Lippi	O
and	O
Torroni	O
,	O
2015a	O
)	O
,	O
the	O
performance	O
of	O
proposed	O
approaches	O
depends	O
on	O
highly	O
engineered	O
and	O
sophisticated	O
,	O
manually	O
constructed	O
,	O
features	O
.	O
However	O
,	O
fully	O
-	O
supervised	O
approaches	O
rely	O
on	O
manually	O
annotated	O
datasets	O
,	O
the	O
construction	O
of	O
which	O
is	O
a	O
laborious	O
,	O
costly	O
,	O
and	O
error	O
-	O
prone	O
process	O
,	O
requiring	O
significant	O
effort	O
from	O
human	O
experts	O
.	O
At	O
the	O
same	O
time	O
,	O
reliance	O
on	O
sophisticated	O
features	O
may	O
hinder	O
the	O
generalisation	O
of	O
an	O
approach	O
to	O
new	O
corpora	O
types	O
and	O
domains	O
(	O
Lippi	O
and	O
Torroni	O
,	O
2015a	O
)	O
.	O
The	O
removal	O
of	O
manual	O
supervision	O
through	O
exploitation	O
of	O
unsupervised	O
approaches	O
is	O
a	O
possible	O
solution	O
to	O
both	O
of	O
the	O
aforementioned	O
problems	O
.	O

Topics	O
seem	O
to	O
be	O
related	O
to	O
the	O
task	O
of	O
argument	B-TaskName
mining	I-TaskName
,	O
at	O
least	O
for	O
some	O
types	O
of	O
corpora	O
,	O
as	O
topic	O
identification	O
frequently	O
appears	O
as	O
a	O
step	O
in	O
the	O
process	O
of	O
manual	O
annotation	O
of	O
arguments	O
in	O
texts	O
(	O
Stab	O
and	O
Gurevych	O
,	O
2014a	O
)	O
.	O
However	O
,	O
despite	O
its	O
apparent	O
importance	O
in	O
manual	O
annotation	O
,	O
only	O
a	O
small	O
number	O
of	O
studies	O
have	O
examined	O
the	O
inclusion	O
of	O
topic	O
information	O
in	O
sub	O
-	O
tasks	O
of	O
argument	B-TaskName
mining	I-TaskName
.	O
Habernal	O
and	O
Gurevych	O
(	O
2015	O
)	O
have	O
included	O
sentiment	O
and	O
topic	O
information	O
as	O
features	O
for	O
classifying	O
sentences	O
as	O
claims	O
,	O
premises	O
,	O
backing	O
and	O
non	O
-	O
argumentative	O
units	O
.	O
A	O
less	O
direct	O
exploitation	O
of	O
topic	O
information	O
has	O
been	O
presented	O
in	O
(	O
Nguyen	O
and	O
Litman	O
,	O
2015	O
)	O
,	O
where	O
topics	O
have	O
been	O
used	O
to	O
extract	O
lexicons	O
of	O
argument	O
and	O
domain	O
words	O
,	O
which	O
can	O
provide	O
evidence	O
regarding	O
the	O
existence	O
of	O
argument	O
components	O
.	O
In	O
this	O
paper	O
we	O
propose	O
"	O
Attraction	O
to	O
Topics	O
"	O
-	O
A2	O
T	O
,	O
an	O
unsupervised	O
approach	O
based	O
on	O
topic	O
modeling	O
techniques	O
for	O
detecting	O
argumentative	O
discourse	O
units	O
at	O
sentence	O
-	O
level	O
granularity	O
(	O
a	O
sub	O
-	O
task	O
known	O
as	O
"	O
argumentative	O
sentence	O
detection	O
"	O
)	O
.	O
The	O
goals	O
of	O
A2	O
T	O
are	O
twofold	O
.	O
On	O
the	O
one	O
side	O
,	O
A2	O
T	O
enforces	O
identification	O
of	O
sentences	O
that	O
contain	O
argument	O
components	O
,	O
by	O
also	O
distinguishing	O
them	O
from	O
the	O
non	O
-	O
argumentative	O
sentences	O
that	O
do	O
not	O
contain	O
argument	O
components	O
.	O
On	O
the	O
other	O
side	O
,	O
A2	O
T	O
classifies	O
the	O
discovered	O
argumentative	O
sentences	O
according	O
to	O
their	O
role	O
,	O
as	O
major	O
claims	O
,	O
claims	O
,	O
and	O
premises	O
.	O
The	O
rest	O
of	O
the	O
paper	O
is	O
organized	O
as	O
follows	O
:	O
Section	O
2	O
presents	O
an	O
overview	O
of	O
approaches	O
related	O
to	O
argument	B-TaskName
mining	I-TaskName
focusing	O
on	O
the	O
detection	O
of	O
argumentative	O
units	O
,	O
while	O
Section	O
3	O
presents	O
our	O
approach	O
on	O
applying	O
topic	O
modeling	O
for	O
identifying	O
sentences	O
that	O
contain	O
argument	O
components	O
.	O
Section	O
4	O
presents	O
our	O
experimental	O
setting	O
and	O
evaluation	O
results	O
,	O
with	O
Section	O
5	O
concluding	O
this	O
paper	O
and	O
proposing	O
some	O
directions	O
for	O
further	O
research	O
.	O

UDapter	O
:	O
Language	O
Adaptation	O
for	O
Truly	O
Universal	O
Dependency	B-TaskName
Parsing	I-TaskName

Recent	O
advances	O
in	O
multilingual	O
dependency	B-TaskName
parsing	I-TaskName
have	O
brought	O
the	O
idea	O
of	O
a	O
truly	O
universal	O
parser	O
closer	O
to	O
reality	O
.	O
However	O
,	O
crosslanguage	O
interference	O
and	O
restrained	O
model	O
capacity	O
remain	O
major	O
obstacles	O
.	O
To	O
address	O
this	O
,	O
we	O
propose	O
a	O
novel	O
multilingual	O
task	O
adaptation	O
approach	O
based	O
on	O
contextual	O
parameter	O
generation	O
and	O
adapter	O
modules	O
.	O
This	O
approach	O
enables	O
to	O
learn	O
adapters	O
via	O
language	O
embeddings	O
while	O
sharing	O
model	O
parameters	O
across	O
languages	O
.	O
It	O
also	O
allows	O
for	O
an	O
easy	O
but	O
effective	O
integration	O
of	O
existing	O
linguistic	O
typology	O
features	O
into	O
the	O
parsing	O
network	O
.	O
The	O
resulting	O
parser	O
,	O
UDapter	O
,	O
outperforms	O
strong	O
monolingual	O
and	O
multilingual	O
baselines	O
on	O
the	O
majority	O
of	O
both	O
high	O
-	O
resource	O
and	O
lowresource	O
(	O
zero	O
-	O
shot	O
)	O
languages	O
,	O
showing	O
the	O
success	O
of	O
the	O
proposed	O
adaptation	O
approach	O
.	O
Our	O
in	O
-	O
depth	O
analyses	O
show	O
that	O
soft	O
parameter	O
sharing	O
via	O
typological	O
features	O
is	O
key	O
to	O
this	O
success	O
.	O
1	O

In	O
this	O
section	O
,	O
we	O
present	O
our	O
truly	O
universal	O
dependency	O
parser	O
,	O
UDapter	O
.	O
UDapter	O
consists	O
of	O
a	O
biaffine	O
attention	O
layer	O
stacked	O
on	O
top	O
of	O
the	O
pretrained	O
Transformer	B-MethodName
encoder	O
(	O
mBERT	B-MethodName
)	O
.	O
This	O
is	O
similar	O
to	O
(	O
Wu	O
and	O
Dredze	O
,	O
2019	O
;	O
Kondratyuk	O
and	O
Straka	O
,	O
2019	O
)	O
,	O
except	O
that	O
our	O
mBERT	B-MethodName
layers	O
are	O
interleaved	O
with	O
special	O
adapter	O
layers	O
inspired	O
by	O
Houlsby	O
et	O
al	O
(	O
2019	O
)	O
.	O
While	O
mBERT	B-MethodName
weights	O
are	O
frozen	O
,	O
biaffine	O
attention	O
and	O
adapter	O
layer	O
weights	O
are	O
generated	O
by	O
a	O
contextual	O
parameter	O
generator	O
(	O
Platanios	O
et	O
al	O
,	O
2018	O
)	O
that	O
takes	O
a	O
language	O
embedding	O
as	O
input	O
and	O
is	O
updated	O
while	O
training	O
on	O
the	O
treebanks	O
.	O
Note	O
that	O
the	O
proposed	O
adaptation	O
approach	O
is	O
not	O
restricted	O
to	O
dependency	B-TaskName
parsing	I-TaskName
and	O
is	O
in	O
principle	O
applicable	O
to	O
a	O
range	O
of	O
multilingual	B-TaskName
NLP	I-TaskName
tasks	O
.	O
We	O
will	O
now	O
describe	O
the	O
components	O
of	O
our	O
model	O
.	O

The	O
top	O
layer	O
of	O
UDapter	O
is	O
a	O
graph	O
-	O
based	O
biaffine	O
attention	O
parser	O
proposed	O
by	O
Dozat	O
and	O
Manning	O
(	O
2017	O
)	O
.	O
In	O
this	O
model	O
,	O
an	O
encoder	O
generates	O
an	O
internal	O
representation	O
r	O
i	O
for	O
each	O
word	O
;	O
the	O
decoder	O
takes	O
r	O
i	O
and	O
passes	O
it	O
through	O
separate	O
feedforward	O
layers	O
(	O
MLP	B-DatasetName
)	O
,	O
and	O
finally	O
uses	O
deep	O
biaffine	O
attention	O
to	O
score	O
arcs	O
connecting	O
a	O
head	O
and	O
a	O
tail	O
:	O
h	O
(	O
head	O
)	O
i	O
=	O
MLP	B-DatasetName
(	O
head	O
)	O
(	O
r	O
i	O
)	O
(	O
1	O
)	O
h	O
(	O
tail	O
)	O
i	O
=	O
MLP	B-DatasetName
(	O
tail	O
)	O
(	O
r	O
i	O
)	O
(	O
2	O
)	O
s	O
(	O
arc	O
)	O
=	O
Biaffine	O
(	O
H	O
(	O
head	O
)	O
,	O
H	O
(	O
tail	O
)	O
)	O
(	O
3	O
)	O
Similarly	O
,	O
label	O
scores	O
are	O
calculated	O
by	O
using	O
a	O
biaffine	O
classifier	O
over	O
two	O
separate	O
feedforward	O
layers	O
.	O
Finally	O
,	O
the	O
Chu	O
-	O
Liu	O
/	O
Edmonds	O
algorithm	O
(	O
Chu	O
,	O
1965	O
;	O
Edmonds	O
,	O
1967	O
)	O
is	O
used	O
to	O
find	O
the	O
highest	O
scoring	O
valid	O
dependency	O
tree	O
.	O

Overall	O
,	O
UDapter	O
outperforms	O
the	O
monolingual	O
and	O
multilingual	O
baselines	O
on	O
both	O
high	O
-	O
resource	O
and	O
zero	O
-	O
shot	O
languages	O
.	O
Below	O
,	O
we	O
elaborate	O
on	O
the	O
detailed	O
results	O
.	O
High	O
-	O
resource	O
Languages	O
Labelled	O
Attachement	O
Scores	O
(	O
LAS	O
)	O
on	O
the	O
high	O
-	O
resource	O
set	O
are	O
given	O
in	O
Table	O
1	O
.	O
UDapter	O
consistently	O
outperforms	O
both	O
our	O
monolingual	O
and	O
multilingual	O
baselines	O
in	O
all	O
languages	O
,	O
and	O
beats	O
the	O
previous	O
work	O
,	O
setting	O
a	O
new	O
state	O
of	O
the	O
art	O
,	O
in	O
9	O
out	O
of	O
13	O
languages	O
.	O
Statistical	O
significance	O
testing	O
8	O
applied	O
between	O
UDapter	O
and	O
multi	O
/	O
mono	O
-	O
udify	O
confirms	O
that	O
UDapter	O
's	O
performance	O
is	O
significantly	O
better	O
than	O
the	O
baselines	O
in	O
11	O
out	O
of	O
13	O
languages	O
(	O
all	O
except	O
en	O
and	O
it	O
)	O
.	O
Among	O
directly	O
comparable	O
baselines	O
,	O
multiudify	O
gives	O
the	O
worst	O
performance	O
in	O
the	O
typologically	O
diverse	O
high	O
-	O
resource	O
setting	O
.	O
This	O
multilingual	O
model	O
is	O
clearly	O
worse	O
than	O
its	O
monolingually	O
trained	O
counterparts	O
mono	O
-	O
udify	O
:	O
83.0	O
vs	O
86.0	O
.	O
This	O
result	O
resounds	O
with	O
previous	O
findings	O
in	O
multilingual	O
NMT	O
(	O
Arivazhagan	O
et	O
al	O
,	O
2019	O
)	O
and	O
highlights	O
the	O
importance	O
of	O
language	O
adaptation	O
even	O
when	O
using	O
high	O
-	O
quality	O
sentence	O
representations	O
like	O
those	O
produced	O
by	O
mBERT	B-MethodName
.	O
To	O
understand	O
the	O
relevance	O
of	O
adapters	O
,	O
we	O
also	O
evaluate	O
a	O
model	O
which	O
has	O
almost	O
the	O
same	O
architecture	O
as	O
multi	O
-	O
udify	O
except	O
for	O
the	O
adapter	O
modules	O
and	O
the	O
tuning	O
choice	O
(	O
frozen	O
mBERT	B-MethodName
weights	O
)	O
.	O
Interestingly	O
,	O
this	O
adapter	O
-	O
only	O
model	O
considerably	O
outperforms	O
multi	O
-	O
udify	O
(	O
85.0	O
vs	O
83.0	O
)	O
,	O
indicating	O
that	O
adapter	O
modules	O
are	O
also	O
effective	O
in	O
multilingual	O
scenarios	O
.	O
Finally	O
,	O
UDapter	O
achieves	O
the	O
overall	O
best	O
results	O
,	O
with	O
consistent	O
gains	O
over	O
both	O
multi	O
-	O
udify	O
and	O
adapter	O
-	O
only	O
,	O
showing	O
the	O
importance	O
of	O
linguistically	O
informed	O
adaptation	O
even	O
for	O
in	O
-	O
training	O
languages	O
.	O
Low	O
-	O
Resource	O
Languages	O
Average	O
LAS	O
on	O
the	O
30	O
low	O
-	O
resource	O
languages	O
are	O
shown	O
in	O
column	O
lr	O
-	O
avg	O
of	O
Table	O
1	O
.	O
Overall	O
,	O
UDapter	O
slightly	O
outperforms	O
the	O
multi	O
-	O
udify	O
baseline	O
(	O
36.5	O
vs	O
36.3	O
)	O
,	O
which	O
shows	O
the	O
benefits	O
of	O
our	O
approach	O
on	O
both	O
in	O
-	O
training	O
and	O
zero	O
-	O
shot	O
languages	O
.	O
For	O
a	O
closer	O
look	O
,	O
Table	O
2	O
provides	O
individual	O
results	O
for	O
the	O
18	O
representative	O
languages	O
in	O
our	O
low	O
-	O
resource	O
set	O
.	O
Here	O
we	O
find	O
a	O
mixed	O
picture	O
:	O
UDapter	O
outperforms	O
multi	O
-	O
udify	O
on	O
13	O
out	O
of	O
18	O
languages	O
9	O
.	O
Achieving	O
improvements	O
in	O
the	O
zero	O
-	O
shot	O
parsing	O
9	O
LAS	O
scores	O
for	O
all	O
30	O
languages	O
are	O
given	O
in	O
Appendix	O
A.2	O
.	O
By	O
significance	O
testing	O
,	O
UDapter	O
is	O
significantly	O
better	O
than	O
multi	O
-	O
udify	O
on	O
16/30	O
low	O
-	O
resource	O
languages	O
,	O
which	O
is	O
shown	O
in	O
setup	O
is	O
very	O
difficult	O
,	O
thus	O
we	O
believe	O
this	O
result	O
is	O
an	O
important	O
step	O
towards	O
overcoming	O
the	O
problem	O
of	O
positive	O
/	O
negative	O
transfer	O
trade	O
-	O
off	O
.	O
Indeed	O
,	O
UDapter	O
-	O
proxy	O
results	O
show	O
that	O
choosing	O
a	O
proxy	O
language	O
embedding	O
from	O
the	O
same	O
language	O
family	O
underperforms	O
UDapter	O
,	O
apart	O
from	O
not	O
being	O
available	O
for	O
many	O
languages	O
.	O
This	O
indicates	O
the	O
importance	O
of	O
typological	O
features	O
in	O
our	O
approach	O
(	O
see	O
5.2	O
for	O
further	O
analysis	O
)	O
.	O

Figure	O
2	O
presents	O
the	O
LAS	O
gain	O
of	O
UDapter	O
over	O
the	O
multi	O
-	O
udify	O
baseline	O
for	O
each	O
high	O
-	O
resource	O
language	O
along	O
with	O
the	O
respective	O
treebank	O
training	O
size	O
.	O
To	O
summarize	O
,	O
the	O
gains	O
are	O
higher	O
for	O
languages	O
with	O
less	O
training	O
data	O
.	O
This	O
suggests	O
that	O
in	O
UDapter	O
,	O
useful	O
knowledge	O
is	O
shared	O
among	O
intraining	O
languages	O
,	O
which	O
benefits	O
low	O
resource	O
languages	O
without	O
hurting	O
high	O
resource	O
ones	O
.	O
For	O
zero	O
-	O
shot	O
languages	O
,	O
the	O
difference	O
between	O
the	O
two	O
models	O
is	O
small	O
compared	O
to	O
high	O
-	O
resource	O
languages	O
(	O
+1.2	O
LAS	O
)	O
.	O
While	O
it	O
is	O
harder	O
to	O
find	O
a	O
trend	O
here	O
,	O
we	O
notice	O
that	O
UDapter	O
is	O
typically	O
beneficial	O
for	O
the	O
languages	O
not	O
present	O
in	O
the	O
mBERT	B-MethodName
training	O
corpus	O
:	O
it	O
outperforms	O
multi	O
-	O
udify	O
in	O
13	O
out	O
of	O
22	O
(	O
non	O
-	O
mBERT	B-MethodName
)	O
languages	O
.	O
This	O
suggests	O
that	O
typological	O
feature	O
-	O
based	O
adaptation	O
leads	O
to	O
improved	O
sentence	O
representations	O
when	O
the	O
pretrained	O
encoder	O
has	O
not	O
been	O
exposed	O
to	O
a	O
language	O
.	O

Table	O
4	O
shows	O
LAS	O
scores	O
on	O
all	O
30	O
low	O
-	O
resouce	O
languages	O
for	O
UDapter	O
,	O
original	O
UDify	O
(	O
Kondratyuk	O
and	O
Straka	O
,	O
2019	O
)	O
,	O
and	O
re	O
-	O
trained	O
'	O
multiudify	O
'	O
.	O
Languages	O
with	O
'	O
*	O
'	O
are	O
not	O
included	O
in	O
mBERT	B-MethodName
training	O
data	O
.	O
Note	O
that	O
original	O
UDify	O
is	O
trained	O
on	O
all	O
available	O
UD	B-DatasetName
treebanks	O
from	O
75	O
languages	O
.	O
For	O
the	O
zero	O
-	O
shot	O
languages	O
,	O
we	O
obtained	O
original	O
UDify	O
scores	O
by	O
running	O
the	O
pre	O
-	O
trained	O
model	O
.	O

CompLx@SMM4H'22	O
:	O
In	O
-	O
domain	O
pretrained	B-TaskName
language	I-TaskName
models	I-TaskName
for	O
detection	O
of	O
adverse	O
drug	O
reaction	O
mentions	O
in	O
English	O
tweets	O

The	O
paper	O
describes	O
the	O
system	O
that	O
team	O
CompLx	O
developed	O
for	O
sub	O
-	O
task	O
1a	O
of	O
the	O
Social	O
Media	O
Mining	O
for	O
Health	O
2022	O
(	O
#	O
SMM4H	B-DatasetName
)	O
Shared	O
Task	O
.	O
We	O
finetune	O
a	O
RoBERTa	B-MethodName
model	O
,	O
a	O
pretrained	O
,	O
transformer	O
-	O
based	O
language	O
model	O
,	O
on	O
a	O
provided	O
dataset	O
to	O
classify	O
English	O
tweets	O
for	O
mentions	O
of	O
Adverse	O
Drug	O
Reactions	O
(	O
ADRs	O
)	O
,	O
i.e.	O
negative	O
side	O
effects	O
related	O
to	O
medication	O
intake	O
.	O
With	O
only	O
a	O
simple	O
finetuning	O
,	O
our	O
approach	O
achieves	O
competitive	O
results	O
,	O
significantly	O
outperforming	O
the	O
average	O
score	O
across	O
submitted	O
systems	O
.	O
We	O
make	O
the	O
model	O
checkpoints	O
1	O
and	O
code	O
2	O
publicly	O
available	O
.	O
We	O
also	O
create	O
a	O
web	O
application	O
3	O
to	O
provide	O
a	O
userfriendly	O
,	O
readily	O
accessible	O
interface	O
for	O
anyone	O
interested	O
in	O
exploring	O
the	O
model	O
's	O
capabilities	O
.	O

The	O
Shared	O
Task	O
(	O
Weissenbacher	O
et	O
al	O
,	O
2022	O
)	O
of	O
the	O
2022	O
Social	O
Media	O
Mining	O
for	O
Health	O
Applications	O
(	O
#	O
SMM4H	B-DatasetName
)	O
workshop	O
proposed	O
ten	O
sub	O
-	O
tasks	O
in	O
the	O
domain	O
of	O
social	O
media	O
mining	O
for	O
health	O
monitoring	O
and	O
surveillance	O
.	O
From	O
the	O
perspective	O
of	O
Natural	O
Language	O
Processing	O
(	O
NLP	O
)	O
,	O
these	O
tasks	O
present	O
a	O
considerable	O
challenge	O
since	O
the	O
nature	O
of	O
social	O
media	O
posts	O
requires	O
dealing	O
with	O
both	O
a	O
significant	O
level	O
of	O
language	O
variation	O
(	O
informal	O
and	O
colloquial	O
expressions	O
,	O
ambiguity	O
,	O
multilingual	O
posts	O
)	O
and	O
data	O
sparsity	O
,	O
as	O
well	O
as	O
a	O
widespread	O
presence	O
of	O
noise	O
such	O
as	O
misspellings	O
of	O
clinical	O
concepts	O
and	O
syntactic	O
errors	O
.	O
In	O
the	O
2022	O
instantiation	O
of	O
the	O
#	O
SMM4H	B-DatasetName
Shared	O
Task	O
,	O
our	O
team	O
participated	O
in	O
:	O
(	O
i	O
)	O
sub	O
-	O
task	O
1a	O
,	O
the	O
classification	O
of	O
English	O
tweets	O
containing	O
mentions	O
of	O
Adverse	O
Drug	O
Reactions	O
(	O
ADRs	O
)	O
(	O
Magge	O
et	O
al	O
,	O
2021	O
)	O
,	O
(	O
ii	O
)	O
sub	O
-	O
task	O
3	O
,	O
the	O
classification	O
of	O
English	O
tweets	O
(	O
3a	O
)	O
and	O
WebMD	O
reviews	O
(	O
3b	O
)	O
contain	O
-	O
1	O
https://huggingface.co/orestxherija/roberta	O
-	O
base	O
-	O
adr	O
-	O
smm4h2022	O
2	O
https://github.com/orestxherija/CompLx	O
-	O
SMM4H2022	O
3	O
https://huggingface.co/spaces/orestxherija/adr	O
-	O
mentionclassifier	O
ing	O
mentions	O
of	O
changes	O
in	O
medication	O
treatments	O
,	O
and	O
(	O
iii	O
)	O
sub	O
-	O
task	O
8	O
,	O
the	O
classification	O
of	O
English	O
tweets	O
self	O
-	O
reporting	O
chronic	O
stress	O
.	O
In	O
this	O
paper	O
we	O
primarily	O
describe	O
our	O
approach	O
for	O
task	O
1a	O
,	O
as	O
that	O
constituted	O
the	O
major	O
focus	O
of	O
our	O
efforts	O
.	O
To	O
address	O
these	O
challenges	O
,	O
we	O
finetune	O
a	O
variant	O
of	O
a	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
model	O
,	O
a	O
transformer	O
-	O
based	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
language	O
model	O
pretrained	O
on	O
approximately	O
128	O
million	O
tweets	O
(	O
Loureiro	O
et	O
al	O
,	O
2022	O
)	O
on	O
each	O
sub	O
-	O
task	O
's	O
provided	O
dataset	O
.	O
Without	O
any	O
domain	B-TaskName
adaptation	I-TaskName
efforts	O
(	O
apart	O
from	O
standard	O
finetuning	O
on	O
the	O
downstream	O
task	O
)	O
or	O
hyperparameter	O
optimizations	O
,	O
the	O
model	O
outperforms	O
the	O
average	O
of	O
all	O
submissions	O
for	O
sub	O
-	O
task	O
1a	O
by	O
a	O
9	O
%	O
absolute	O
difference	O
in	O
F1score	O
.	O
In	O
the	O
following	O
sections	O
,	O
we	O
introduce	O
the	O
subtasks	O
'	O
datasets	O
,	O
describe	O
the	O
model	O
architecture	O
and	O
training	O
setup	O
,	O
report	O
our	O
results	O
,	O
and	O
conclude	O
with	O
a	O
discussion	O
of	O
related	O
research	O
and	O
potential	O
avenues	O
for	O
future	O
work	O
.	O

Label	O
vyvanse	O
make	O
me	O
so	O
hyper	O
and	O
creative	O
and	O
i	O
think	O
of	O
so	O
many	O
tweets	O
ADR	O
feed	O
an	O
ocd	B-DatasetName
vyvanse	O
and	O
cover	O
him	O
in	O
crayons	O
No	O
ADR	O
trazodone	O
has	O
screwed	O
up	O
my	O
sleep	O
schedule	O
.	O
its	O
helping	O
tho	O
.	O
ADR	O
on	O
medication	O
-	O
related	O
keywords	O
for	O
label	O
assignment	O
is	O
going	O
to	O
be	O
problematic	O
:	O
both	O
the	O
first	O
and	O
the	O
second	O
example	O
contain	O
the	O
medication	O
term	O
"	O
vyvanse	O
"	O
but	O
they	O
have	O
been	O
assigned	O
different	O
labels	O
,	O
"	O
ADR	O
"	O
and	O
"	O
No	O
ADR	O
"	O
respectively	O
.	O
This	O
motivates	O
the	O
use	O
of	O
a	O
modeling	O
approach	O
that	O
leverages	O
the	O
overall	O
semantic	O
content	O
of	O
the	O
sentence	O
,	O
rather	O
than	O
keyword	O
matching	O
with	O
individual	O
constituents	O
.	O
3	O
Modeling	O
Approach	O

The	O
establishment	O
of	O
language	O
modeling	O
as	O
the	O
pretraining	O
step	O
in	O
the	O
transfer	B-TaskName
learning	I-TaskName
pipeline	O
revolutionized	O
modern	O
NLP	O
with	O
models	O
such	O
as	O
ULMFiT	B-MethodName
(	O
Howard	O
and	O
Ruder	O
,	O
2018	O
)	O
,	O
ELMo	B-MethodName
(	O
Peters	O
et	O
al	O
,	O
2018	O
)	O
and	O
,	O
most	O
notably	O
,	O
transformerbased	O
language	O
models	O
such	O
as	O
GPT	B-MethodName
(	O
Radford	O
et	O
al	O
,	O
2018	O
)	O
and	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
.	O
In	O
recent	O
years	O
,	O
there	O
have	O
been	O
intensive	O
efforts	O
in	O
the	O
research	O
community	O
to	O
produce	O
ever	O
-	O
larger	O
transformer	O
-	O
based	O
pretrained	B-TaskName
language	I-TaskName
models	I-TaskName
that	O
are	O
trained	O
using	O
a	O
variety	O
of	O
datasets	O
,	O
transformermodel	O
architectures	O
,	O
training	O
objectives	O
and	O
optimization	O
techniques	O
.	O
This	O
should	O
come	O
as	O
no	O
surprise	O
,	O
since	O
such	O
language	O
models	O
have	O
dominated	O
virtually	O
all	O
NLP	O
leaderboards	O
,	O
most	O
notably	O
GLUE	B-DatasetName
(	O
Wang	O
et	O
al	O
,	O
2018	O
)	O
and	O
SuperGLUE	B-DatasetName
(	O
Wang	O
et	O
al	O
,	O
2019	O
)	O
.	O
Considering	O
this	O
overwhelming	O
success	O
,	O
we	O
opt	O
for	O
a	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
model	O
4	O
that	O
has	O
been	O
trained	O
on	O
approximately	O
128	O
million	O
tweets	O
(	O
Loureiro	O
et	O
al	O
,	O
2022	O
)	O
.	O
Our	O
exact	O
modeling	O
approach	O
is	O
depicted	O
in	O
Figure	O
[	O
1	O
]	O
.	O
We	O
opt	O
for	O
a	O
model	O
that	O
has	O
been	O
trained	O
on	O
an	O
in	O
-	O
domain	O
corpus	O
,	O
namely	O
tweets	O
,	O
as	O
transfer	B-TaskName
learning	I-TaskName
has	O
been	O
shown	O
to	O
yield	O
improved	O
results	O
when	O
there	O
is	O
indomain	O
pretraining	O
(	O
Gururangan	O
et	O
al	O
,	O
2020	O
)	O
.	O
We	O
do	O
not	O
use	O
any	O
text	O
normalization	O
steps	O
.	O
4	O
https://huggingface.co/cardiffnlp/twitter	O
-	O
roberta	O
-	O
base	O
-	O
mar2022	O

Clause	O
-	O
Wise	O
and	O
Recursive	O
Decoding	O
for	O
Complex	O
and	O
Cross	O
-	O
Domain	O
Text	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
SQL	I-TaskName
Generation	O

Our	O
work	O
is	O
related	O
to	O
the	O
grammar	O
-	O
based	O
constrained	O
decoding	O
approaches	O
for	O
semantic	B-TaskName
parsing	I-TaskName
(	O
Yin	O
and	O
Neubig	O
,	O
2017	O
;	O
Rabinovich	O
et	O
al	O
,	O
2017	O
;	O
Iyer	O
et	O
al	O
,	O
2018	O
)	O
.	O
While	O
their	O
approaches	O
are	O
focused	O
on	O
general	O
purpose	O
code	B-TaskName
generation	I-TaskName
,	O
we	O
instead	O
focus	O
on	O
SQL	O
-	O
specific	O
grammar	O
to	O
address	O
the	O
text	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
SQL	I-TaskName
task	O
.	O
Our	O
task	O
differs	O
from	O
code	B-TaskName
generation	I-TaskName
in	O
two	O
aspects	O
.	O
First	O
,	O
it	O
takes	O
a	O
database	O
schema	O
as	O
an	O
input	O
in	O
addition	O
to	O
natural	O
language	O
.	O
To	O
predict	O
SQL	O
correctly	O
,	O
a	O
model	O
should	O
fully	O
understand	O
the	O
relationship	O
between	O
the	O
question	O
and	O
the	O
schema	O
.	O
Second	O
,	O
as	O
SQL	O
is	O
a	O
non	O
-	O
procedural	O
language	O
,	O
predictions	O
of	O
SQL	O
clauses	O
do	O
not	O
need	O
to	O
be	O
done	O
sequentially	O
.	O
For	O
text	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
SQL	I-TaskName
generation	O
,	O
several	O
SQLspecific	O
approaches	O
have	O
been	O
proposed	O
(	O
Zhong	O
et	O
al	O
,	O
2017	O
;	O
Xu	O
et	O
al	O
,	O
2017	O
;	O
Huang	O
et	O
al	O
,	O
2018	O
;	O
Yu	O
et	O
al	O
,	O
2018a	O
;	O
Dong	O
and	O
Lapata	O
,	O
2018	O
;	O
Yavuz	O
et	O
al	O
,	O
2018	O
)	O
based	O
on	O
WikiSQL	B-DatasetName
dataset	O
(	O
Zhong	O
et	O
al	O
,	O
2017	O
)	O
.	O
However	O
,	O
all	O
of	O
them	O
are	O
limited	O
to	O
the	O
specific	O
WikiSQL	B-DatasetName
SQL	O
sketch	O
,	O
which	O
only	O
supports	O
very	O
simple	O
queries	O
.	O
It	O
includes	O
only	O
the	O
SELECT	O
and	O
WHERE	O
clauses	O
,	O
only	O
a	O
single	O
expression	O
in	O
the	O
SELECT	O
clause	O
,	O
and	O
works	O
only	O
for	O
a	O
single	O
table	O
.	O
To	O
predict	O
more	O
complex	O
SQL	O
queries	O
,	O
sequence	O
-	O
to	O
-	O
sequence	O
(	O
Iyer	O
et	O
al	O
,	O
2017	O
;	O
Finegan	O
-	O
Dollak	O
et	O
al	O
,	O
2018	O
)	O
and	O
template	O
-	O
based	O
(	O
Finegan	O
-	O
Dollak	O
et	O
al	O
,	O
2018	O
;	O
Lee	O
et	O
al	O
,	O
2019	O
)	O
approaches	O
have	O
been	O
proposed	O
.	O
However	O
,	O
they	O
focused	O
only	O
on	O
specific	O
databases	O
such	O
as	O
ATIS	B-DatasetName
(	O
Price	O
,	O
1990	O
)	O
and	O
GeoQuery	O
(	O
Zelle	O
and	O
Mooney	O
,	O
1996	O
)	O
.	O
Because	O
they	O
only	O
considered	O
question	O
and	O
SQL	O
pairs	O
without	O
requiring	O
an	O
understanding	O
of	O
database	O
schema	O
,	O
their	O
approaches	O
can	O
not	O
generalize	O
to	O
unseen	O
databases	O
.	O
SyntaxSQLNet	O
(	O
Yu	O
et	O
al	O
,	O
2018b	O
)	O
is	O
the	O
first	O
and	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
model	O
for	O
the	O
Spider	O
(	O
Yu	O
et	O
al	O
,	O
2018c	O
)	O
,	O
a	O
complex	O
and	O
cross	O
-	O
domain	O
text	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
SQL	I-TaskName
task	O
.	O
They	O
proposed	O
an	O
SQL	O
specific	O
syntax	O
tree	O
-	O
based	O
decoder	O
with	O
SQL	O
generation	O
history	O
.	O
Our	O
approach	O
differs	O
from	O
their	O
model	O
in	O
the	O
following	O
aspects	O
.	O
First	O
,	O
taking	O
into	O
account	O
that	O
SQL	O
corresponds	O
to	O
non	O
-	O
procedural	O
language	O
,	O
we	O
develop	O
a	O
clause	O
-	O
specific	O
decoder	O
for	O
each	O
SQL	O
clause	O
,	O
where	O
SyntaxSQLNet	O
predicts	O
SQL	O
tokens	O
sequentially	O
.	O
For	O
example	O
,	O
in	O
SyntaxSQL	O
-	O
Net	O
,	O
a	O
single	O
column	O
prediction	O
module	O
works	O
both	O
in	O
the	O
SELECT	O
and	O
WHERE	O
clauses	O
,	O
depending	O
on	O
the	O
SQL	O
decoding	O
history	O
.	O
In	O
contrast	O
,	O
we	O
define	O
and	O
train	O
decoding	O
modules	O
separately	O
for	O
each	O
SQL	O
clause	O
to	O
fully	O
utilize	O
clausedependent	O
context	O
.	O
Second	O
,	O
we	O
apply	O
sequenceto	O
-	O
sequence	O
architecture	O
to	O
predict	O
columns	O
instead	O
of	O
using	O
the	O
sequence	O
-	O
to	O
-	O
set	O
framework	O
from	O
SyntaxSQLNet	O
,	O
because	O
correct	O
ordering	O
is	O
essential	O
for	O
the	O
GROUP	O
BY	O
and	O
ORDER	O
BY	O
clauses	O
.	O
Finally	O
,	O
we	O
introduce	O
a	O
self	O
-	O
attention	O
mechanism	O
(	O
Lin	O
et	O
al	O
,	O
2017	O
)	O
to	O
efficiently	O
encode	O
database	O
schema	O
,	O
which	O
includes	O
multiple	O
tables	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
recursive	O
and	O
SQL	O
clause	O
-	O
wise	O
decoding	O
neural	O
architecture	O
to	O
address	O
the	O
complex	O
and	O
cross	O
-	O
domain	O
text	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
SQL	I-TaskName
task	O
.	O
We	O
evaluate	O
our	O
model	O
with	O
the	O
Spider	O
dataset	O
,	O
and	O
the	O
experimental	O
result	O
shows	O
that	O
our	O
model	O
significantly	O
outperforms	O
previous	O
work	O
for	O
generating	O
not	O
only	O
simple	O
queries	O
,	O
but	O
also	O
complex	O
and	O
nested	O
queries	O
.	O
(	O
Yu	O
et	O
al	O
,	O
2018b	O
)	O
,	O
and	O
modified	O
SQLNet	O
(	O
Xu	O
et	O
al	O
,	O
2017	O
)	O
by	O
Yu	O
et	O
al	O
(	O
2018c	O
)	O
,	O
respectively	O
.	O

Previous	O
work	O
on	O
readability	O
has	O
classified	O
or	O
ranked	O
texts	O
based	O
on	O
document	O
-	O
level	O
measures	O
such	O
as	O
word	O
length	O
,	O
sentence	O
length	O
,	O
number	O
of	O
different	O
phrasal	O
categories	O
&	O
parse	O
tree	O
depth	O
(	O
Petersen	O
,	O
2007	O
)	O
,	O
and	O
discourse	O
coherence	O
(	O
Graesser	O
et	O
al	O
,	O
2004	O
)	O
,	O
inter	O
alia	O
.	O
However	O
,	O
not	O
all	O
applications	O
that	O
need	O
readability	O
ratings	O
deal	O
with	O
long	O
documents	O
.	O
For	O
many	O
applications	O
in	O
text	B-TaskName
simplification	I-TaskName
,	O
computer	O
-	O
aided	O
language	O
learning	O
(	O
CALL	O
)	O
systems	O
,	O
authorship	O
tools	O
,	O
translation	O
,	O
and	O
information	B-TaskName
retrieval	I-TaskName
,	O
sentence	O
-	O
level	O
readability	O
metrics	O
are	O
direly	O
needed	O
.	O
For	O
instance	O
,	O
an	O
automatic	O
text	B-TaskName
simplification	I-TaskName
system	O
must	O
begin	O
by	O
asking	O
which	O
portions	O
of	O
a	O
text	O
need	O
to	O
be	O
simplified	O
.	O
To	O
this	O
end	O
,	O
a	O
measure	O
that	O
can	O
assign	O
ratings	O
on	O
a	O
sentence	O
-	O
by	O
-	O
sentence	O
level	O
can	O
help	O
target	O
simplification	O
only	O
to	O
those	O
sentences	O
which	O
need	O
it	O
most	O
,	O
and	O
such	O
measures	O
also	O
serve	O
to	O
confirm	O
that	O
the	O
resulting	O
'	O
simplified	O
'	O
sentence	O
is	O
in	O
fact	O
simpler	O
than	O
the	O
original	O
sentence	O
.	O
Similarly	O
,	O
CALL	O
and	O
other	O
pedagogical	O
systems	O
will	O
benefit	O
if	O
it	O
is	O
possible	O
to	O
predict	O
which	O
portions	O
of	O
a	O
text	O
will	O
be	O
harder	O
for	O
students	O
.	O
Authorship	O
tools	O
can	O
offer	O
more	O
specific	O
editorial	O
advice	O
when	O
they	O
know	O
why	O
individual	O
sentences	O
can	O
cause	O
difficulties	O
for	O
readers	O
.	O
Translation	B-TaskName
tools	O
can	O
aim	O
to	O
preserve	O
not	O
just	O
meaning	O
but	O
also	O
the	O
approximate	O
difficulty	O
of	O
the	O
sentences	O
they	O
are	O
translating	O
or	O
use	O
a	O
sentence	O
-	O
level	O
difficulty	O
metric	O
to	O
target	O
output	O
that	O
is	O
easier	O
to	O
understand	O
.	O
Furthermore	O
,	O
information	B-TaskName
retrieval	I-TaskName
systems	O
also	O
benefit	O
when	O
they	O
can	O
return	O
not	O
merely	O
relevant	O
texts	O
,	O
but	O
also	O
texts	O
appropriate	O
to	O
the	O
reading	O
level	O
of	O
the	O
user	O
.	O
Recently	O
there	O
has	O
been	O
an	O
increased	O
interest	O
in	O
sentential	O
models	O
of	O
text	O
difficulty	O
in	O
the	O
automatic	O
text	B-TaskName
simplification	I-TaskName
and	O
summarization	B-TaskName
communities	O
in	O
particular	O
(	O
Vajjala	O
and	O
Meurers	O
,	O
2014	O
;	O
Macdonald	O
and	O
Siddharthan	O
,	O
2016	O
)	O
.	O
One	O
area	O
that	O
has	O
produced	O
a	O
lot	O
of	O
research	O
on	O
sentence	O
level	O
processing	O
difficulty	O
is	O
psycholinguistics	O
.	O
Over	O
the	O
past	O
three	O
decades	O
,	O
a	O
number	O
of	O
theories	O
of	O
human	O
sentence	O
processing	O
(	O
i.e.	O
reading	O
)	O
have	O
been	O
proposed	O
and	O
validated	O
in	O
a	O
large	O
variety	O
of	O
experimental	O
studies	O
.	O
The	O
most	O
important	O
sentence	O
processing	O
theories	O
have	O
furthermore	O
been	O
implemented	O
based	O
on	O
broad	O
-	O
coverage	O
tools	O
,	O
so	O
that	O
estimates	O
for	O
arbitrary	O
sentences	O
can	O
be	O
generated	O
automatically	O
.	O
For	O
example	O
,	O
eyetracking	O
studies	O
of	O
reading	O
times	O
on	O
a	O
large	O
corpus	O
of	O
newspaper	O
text	O
have	O
found	O
that	O
measures	O
such	O
as	O
integration	O
cost	O
and	O
surprisal	O
provide	O
partial	O
explanations	O
for	O
subjects	O
'	O
reading	O
behavior	O
(	O
Demberg	O
and	O
Keller	O
,	O
2008	O
)	O
.	O
This	O
paper	O
leverages	O
these	O
implemented	O
measures	O
based	O
on	O
psycholinguistic	O
theories	O
of	O
sen	O
-	O
tence	O
processing	O
in	O
order	O
to	O
test	O
whether	O
they	O
can	O
help	O
to	O
more	O
accurately	O
score	O
individual	O
sentences	O
with	O
respect	O
to	O
their	O
difficulty	O
.	O
In	O
the	O
process	O
,	O
we	O
evaluate	O
the	O
contributions	O
of	O
the	O
individual	O
features	O
to	O
our	O
models	O
,	O
testing	O
their	O
utility	O
in	O
examining	O
fine	O
-	O
grained	O
distinctions	O
in	O
sentence	O
difficulty	O
.	O
Section	O
2	O
reviews	O
the	O
literature	O
on	O
readability	O
in	O
general	O
before	O
we	O
shift	O
to	O
psycholinguistic	O
theories	O
of	O
sentence	O
processing	O
in	O
Section	O
4	O
.	O
In	O
Section	O
5	O
we	O
discuss	O
our	O
methods	O
,	O
including	O
the	O
corpora	O
used	O
,	O
how	O
features	O
were	O
extracted	O
,	O
and	O
the	O
set	O
up	O
for	O
our	O
averaged	O
perceptron	O
models	O
.	O
Section	O
6	O
presents	O
our	O
findings	O
which	O
we	O
connect	O
to	O
related	O
work	O
on	O
sentence	O
-	O
level	O
readability	O
models	O
in	O
3	O
.	O
Finally	O
we	O
offer	O
our	O
conclusions	O
and	O
suggestions	O
for	O
future	O
work	O
in	O
Section	O
7	O
.	O
2	O
Readability	O
Chall	O
's	O
(	O
1958	O
)	O
comprehensive	O
review	O
of	O
readability	O
research	O
in	O
the	O
first	O
half	O
of	O
the	O
20	O
th	O
century	O
divides	O
the	O
early	O
work	O
in	O
readability	O
into	O
"	O
survey	O
and	O
experimental	O
studies	O
"	O
and	O
"	O
quantitative	O
associational	O
studies	O
"	O
.	O
Studies	O
of	O
the	O
former	O
category	O
took	O
place	O
during	O
the	O
1930s	O
and	O
1940s	O
and	O
included	O
surveys	O
of	O
expert	O
and	O
reader	O
opinion	O
as	O
well	O
as	O
experimental	O
studies	O
which	O
manipulated	O
texts	O
according	O
to	O
one	O
variable	O
at	O
a	O
time	O
in	O
order	O
to	O
determine	O
the	O
effects	O
of	O
those	O
variables	O
on	O
readers	O
.	O
The	O
results	O
of	O
these	O
studies	O
suggest	O
that	O
,	O
once	O
you	O
have	O
managed	O
to	O
control	O
for	O
reader	O
interest	O
in	O
the	O
content	O
of	O
a	O
text	O
,	O
the	O
most	O
important	O
factor	O
with	O
respect	O
to	O
its	O
readability	O
is	O
its	O
'	O
style	O
'	O
,	O
e.g.	O
its	O
"	O
scope	O
of	O
vocabulary	O
and	O
...	O
kinds	O
of	O
sentences	O
"	O
(	O
Gray	O
andLeary	O
,	O
1935	O
,	O
as	O
quoted	O
in	O
(	O
Chall	O
,	O
1958	O
)	O
)	O
.	O
Our	O
study	O
belongs	O
to	O
the	O
second	O
class	O
,	O
relating	O
the	O
features	O
of	O
a	O
text	O
to	O
its	O
ordering	O
relative	O
to	O
some	O
other	O
texts	O
.	O
The	O
earliest	O
work	O
in	O
this	O
direction	O
was	O
by	O
L.	O
A.	O
Sherman	O
,	O
who	O
proposed	O
a	O
quantitative	O
analysis	O
of	O
text	O
difficulty	O
based	O
on	O
the	O
number	O
of	O
clauses	O
per	O
sentence	O
,	O
among	O
other	O
features	O
(	O
Sherman	O
,	O
1893	O
)	O
.	O
Where	O
Sherman	O
's	O
pedagogical	O
focus	O
was	O
on	O
literature	O
,	O
Lively	O
&	O
Pressey	O
(	O
1923	O
)	O
focused	O
on	O
vocabulary	O
as	O
a	O
bottleneck	O
in	O
science	O
education	O
.	O
Work	O
in	O
this	O
vein	O
led	O
to	O
the	O
development	O
of	O
a	O
number	O
of	O
readability	O
formulae	O
in	O
the	O
mid	O
-	O
20	O
th	O
century	O
1	O
,	O
including	O
the	O
familiar	O
Flesch	O
-	O
Kincaid	O
Grade	O
-	O
Level	O
score	O
(	O
Kincaid	O
et	O
al	O
,	O
1975	O
)	O
.	O
These	O
formulae	O
typically	O
use	O
a	O
linear	O
combination	O
of	O
average	O
word	O
length	O
and	O
average	O
sentence	O
length	O
,	O
though	O
some	O
also	O
incorporate	O
a	O
vocabulary	O
-	O
diversity	O
term	O
.	O
The	O
simple	O
,	O
twofeature	O
versions	O
of	O
these	O
models	O
are	O
still	O
widely	O
used	O
,	O
and	O
inspired	O
our	O
BASELINE	O
model	O
.	O
More	O
recently	O
,	O
Petersen	O
(	O
2007	O
)	O
sought	O
to	O
apply	O
familiar	O
natural	O
language	O
processing	O
techniques	O
to	O
the	O
problem	O
of	O
identifying	O
text	O
difficulty	O
for	O
nonnative	O
readers	O
.	O
In	O
particular	O
,	O
she	O
used	O
a	O
number	O
of	O
parse	O
-	O
based	O
features	O
which	O
captured	O
,	O
for	O
example	O
,	O
the	O
average	O
number	O
of	O
noun	O
and	O
verb	O
phrases	O
per	O
sentence	O
and	O
the	O
height	O
of	O
the	O
parse	O
tree	O
.	O
Petersen	O
trained	O
SVM	B-MethodName
classifiers	O
to	O
classify	O
texts	O
as	O
belonging	O
to	O
one	O
of	O
four	O
primary	O
school	O
grade	O
levels	O
based	O
on	O
the	O
Weekly	O
Reader	O
educational	O
newspaper	O
2	O
.	O
These	O
document	O
-	O
level	O
models	O
achieved	O
Fscores	O
in	O
the	O
range	O
of	O
0.5	O
to	O
0.7	O
,	O
compared	O
to	O
the	O
F	O
-	O
scores	O
between	O
0.25	O
and	O
0.45	O
achieved	O
by	O
the	O
Flesch	O
-	O
Kincaid	O
Reading	O
Ease	O
score	O
for	O
the	O
same	O
texts	O
.	O
Recent	O
work	O
has	O
also	O
looked	O
at	O
features	O
related	O
to	O
discourse	O
and	O
working	O
memory	O
constraints	O
.	O
Feng	O
et	O
al	O
(	O
2009	O
)	O
worked	O
on	O
a	O
model	O
of	O
readability	O
for	O
adults	O
with	O
intellectual	O
disabilities	O
.	O
Considering	O
working	O
memory	O
constraints	O
,	O
they	O
extracted	O
features	O
related	O
to	O
the	O
number	O
of	O
entities	O
mentioned	O
in	O
a	O
document	O
and	O
the	O
'	O
lexical	O
chains	O
'	O
(	O
Galley	O
and	O
McKeown	O
,	O
2003	O
)	O
that	O
connected	O
them	O
.	O
They	O
found	O
that	O
their	O
features	O
resulted	O
in	O
a	O
better	O
correlation	O
(	O
Pearson	O
's	O
r	O
=	O
−0.352	O
)	O
compared	O
to	O
both	O
Flesch	O
-	O
Kincaid	O
score	O
(	O
r	O
=	O
−0.270	O
)	O
and	O
a	O
number	O
of	O
'	O
basic	O
'	O
linguistic	O
features	O
based	O
on	O
those	O
used	O
by	O
Petersen	O
&	O
Ostendorf	O
(	O
2009	O
)	O
(	O
r	O
=	O
−0.283	O
)	O
.	O
3	O
Coh	O
-	O
Metrix	B-MethodName
(	O
Graesser	O
et	O
al	O
,	O
2004	O
)	O
also	O
includes	O
a	O
number	O
of	O
measures	O
related	O
to	O
discourse	O
coherence	O
,	O
for	O
example	O
.	O
Such	O
features	O
are	O
not	O
suited	O
to	O
the	O
problem	O
of	O
determining	O
the	O
difficulty	O
of	O
sentences	O
in	O
isolation	O
,	O
but	O
they	O
have	O
also	O
been	O
shown	O
to	O
better	O
predict	O
readability	O
for	O
second	O
-	O
language	O
learners	O
compared	O
to	O
'	O
traditional	O
'	O
readability	O
measures	O
like	O
those	O
described	O
above	O
(	O
Crossley	O
et	O
al	O
,	O
2011	O
)	O
.	O

For	O
our	O
purposes	O
,	O
we	O
focus	O
on	O
readability	O
as	O
reading	O
ease	O
and	O
on	O
linguistic	O
constraints	O
in	O
particular	O
,	O
rather	O
than	O
constraints	O
of	O
medium	O
(	O
relating	O
to	O
e.g.	O
legibility	O
)	O
,	O
reader	O
interest	O
,	O
or	O
comprehensibility	O
.	O
Without	O
directly	O
modeling	O
comprehensibility	O
,	O
we	O
assume	O
that	O
making	O
material	O
easier	O
to	O
read	O
will	O
also	O
make	O
it	O
easier	O
to	O
comprehend	O
.	O
Here	O
we	O
focus	O
on	O
four	O
psycholinguistic	O
theories	O
of	O
human	O
sentence	O
processing	O
:	O
idea	O
density	O
,	O
surprisal	O
,	O
integration	O
cost	O
,	O
and	O
embedding	O
depth	O
.	O
Kintsch	O
(	O
1972	O
)	O
defined	O
propositional	O
idea	O
density	O
as	O
the	O
ratio	O
of	O
propositions	O
or	O
ideas	O
to	O
words	O
in	O
the	O
sentences	O
.	O
4	O
Keenan	O
&	O
Kintsch	O
conducted	O
two	O
different	O
experiments	O
in	O
order	O
to	O
examine	O
free	O
reading	O
behavior	O
as	O
well	O
as	O
subjects	O
'	O
performance	O
in	O
speeded	O
reading	O
conditions	O
.	O
They	O
found	O
that	O
"	O
the	O
number	O
of	O
propositions	O
[	O
in	O
a	O
text	O
]	O
had	O
a	O
large	O
effect	O
upon	O
reading	O
times	O
,	O
[	O
but	O
]	O
it	O
could	O
only	O
account	O
for	O
21	O
%	O
of	O
their	O
variance	O
"	O
when	O
subjects	O
were	O
allowed	O
to	O
read	O
freely	O
.	O
Subjects	O
'	O
overall	O
recall	O
was	O
worse	O
for	O
more	O
dense	O
texts	O
in	O
the	O
speeded	O
reading	O
condition	O
.	O
In	O
addition	O
to	O
effects	O
of	O
idea	O
density	O
,	O
they	O
found	O
that	O
propositions	O
which	O
were	O
presented	O
as	O
surface	O
-	O
form	O
modifiers	O
(	O
as	O
opposed	O
to	O
,	O
e.g.	O
,	O
main	O
verbs	O
)	O
were	O
"	O
very	O
poorly	O
recalled	O
"	O
and	O
that	O
propositions	O
playing	O
a	O
subordinate	O
role	O
relative	O
to	O
another	O
proposition	O
were	O
also	O
less	O
-	O
well	O
recalled	O
.	O
Finally	O
,	O
propositions	O
involving	O
a	O
proper	O
name	O
were	O
generally	O
recalled	O
better	O
than	O
similar	O
propositions	O
involving	O
,	O
e.g.	O
,	O
a	O
common	O
noun	O
.	O
While	O
Kintsch	O
&	O
Keenan	O
(	O
1973	O
)	O
looked	O
at	O
the	O
influence	O
of	O
propositional	O
idea	O
density	O
on	O
reading	O
times	O
and	O
recall	O
for	O
both	O
individual	O
sentences	O
as	O
well	O
as	O
short	O
paragraphs	O
,	O
work	O
since	O
the	O
1970s	O
has	O
been	O
limited	O
to	O
the	O
level	O
of	O
multiple	O
sentences	O
and	O
used	O
primarily	O
as	O
an	O
indicator	O
of	O
cognitive	O
deficits	O
(	O
Ferguson	O
et	O
al	O
,	O
2014	O
;	O
Bryant	O
et	O
al	O
,	O
2013	O
;	O
Farias	O
et	O
al	O
,	O
2012	O
;	O
Riley	O
et	O
al	O
,	O
2005	O
)	O
.	O
This	O
paper	O
returns	O
to	O
the	O
examination	O
of	O
idea	O
density	O
's	O
applicability	O
for	O
individual	O
sentences	O
.	O
Surprisal	O
,	O
on	O
the	O
other	O
hand	O
,	O
has	O
been	O
widely	O
examined	O
in	O
theories	O
of	O
language	O
comprehension	O
at	O
a	O
variety	O
of	O
levels	O
,	O
including	O
the	O
word	O
-	O
and	O
sentence	O
-	O
levels	O
.	O
Surprisal	O
is	O
another	O
word	O
for	O
Shannon	O
(	O
1948	O
)	O
information	O
,	O
operationalized	O
in	O
linguistics	O
as	O
the	O
probability	O
of	O
the	O
current	O
word	O
conditioned	O
on	O
the	O
preceding	O
sequence	O
of	O
words	O
:	O
surprisal	O
(	O
w	O
n	O
)	O
=	O
−log	O
(	O
P	O
(	O
w	O
n	O
|	O
w	O
1	O
.	O
.	O
.	O
w	O
n−1	O
)	O
)	O
(	O
1	O
)	O
where	O
w	O
i	O
is	O
the	O
i	O
th	O
word	O
in	O
the	O
sentence	O
and	O
P	O
(	O
w	O
1	O
.	O
.	O
.	O
w	O
i	O
)	O
denotes	O
the	O
probability	O
of	O
the	O
sequence	O
of	O
i	O
words	O
w	O
1	O
.	O
.	O
.	O
w	O
i	O
.	O
One	O
reason	O
psycholinguists	O
consider	O
surprisal	O
as	O
a	O
factor	O
in	O
sentence	O
processing	O
difficulty	O
is	O
that	O
it	O
makes	O
sense	O
in	O
a	O
model	O
of	O
language	O
users	O
as	O
rational	O
learners	O
.	O
Levy	O
(	O
2008	O
)	O
argues	O
the	O
rational	O
reader	O
's	O
attention	O
must	O
be	O
spread	O
across	O
all	O
possible	O
analyses	O
for	O
the	O
sentence	O
being	O
observed	O
.	O
Based	O
on	O
prior	O
experience	O
,	O
the	O
reader	O
expects	O
some	O
analyses	O
to	O
be	O
more	O
probable	O
than	O
others	O
and	O
therefore	O
allocates	O
more	O
resources	O
to	O
those	O
analyses	O
.	O
In	O
this	O
analysis	O
,	O
surprisal	O
is	O
derived	O
as	O
a	O
measure	O
of	O
the	O
cost	O
paid	O
when	O
the	O
reader	O
misallocates	O
resources	O
:	O
when	O
a	O
new	O
word	O
invalidates	O
a	O
highly	O
probable	O
analysis	O
,	O
the	O
reader	O
has	O
effectively	O
'	O
wasted	O
'	O
whatever	O
resources	O
were	O
allocated	O
to	O
that	O
analysis	O
.	O
The	O
notion	O
of	O
surprisal	O
is	O
also	O
used	O
in	O
theories	O
of	O
language	O
production	O
,	O
see	O
the	O
Uniform	O
Information	O
Density	O
hypothesis	O
(	O
Jaeger	O
,	O
2006	O
;	O
Levy	O
and	O
Jaeger	O
,	O
2007	O
;	O
Jaeger	O
,	O
2010	O
,	O
UID	O
)	O
.	O
While	O
surprisal	O
focuses	O
on	O
predictability	O
effects	O
in	O
sentence	O
processing	O
,	O
Gibson	O
's	O
(	O
1998	O
;	O
2000	O
)	O
Dependency	O
Locality	O
Theory	O
(	O
DLT	O
)	O
focuses	O
on	O
the	O
memory	O
cost	O
of	O
recalling	O
referents	O
and	O
integrating	O
new	O
ones	O
into	O
a	O
mental	O
representation	O
.	O
DLT	O
proposes	O
that	O
the	O
the	O
distance	O
between	O
syntactic	O
heads	O
and	O
dependents	O
,	O
measured	O
by	O
the	O
number	O
of	O
intervening	O
discourse	O
referents	O
,	O
approximates	O
the	O
difficulty	O
that	O
the	O
listener	O
or	O
reader	O
will	O
have	O
integrating	O
the	O
two	O
units	O
.	O
This	O
model	O
maintains	O
that	O
the	O
act	O
of	O
creating	O
a	O
new	O
discourse	O
referent	O
and	O
holding	O
it	O
in	O
memory	O
makes	O
it	O
more	O
difficult	O
to	O
recall	O
a	O
previous	O
discourse	O
referent	O
and	O
connect	O
that	O
discourse	O
referent	O
to	O
the	O
current	O
one	O
.	O
5	O
In	O
addition	O
to	O
integration	O
cost	O
,	O
DLT	O
proposes	O
a	O
storage	O
cost	O
associated	O
with	O
the	O
number	O
of	O
open	O
dependencies	O
that	O
must	O
be	O
maintained	O
in	O
memory	O
.	O
The	O
notion	O
of	O
connected	O
components	O
in	O
van	O
Schijndel	O
et	O
al	O
's	O
(	O
2012	O
;	O
incremental	O
parsing	O
model	O
picks	O
up	O
this	O
idea	O
.	O
Related	O
models	O
were	O
also	O
suggested	O
earlier	O
by	O
Yngve	O
(	O
1960	O
)	O
and	O
Miller	O
's	O
(	O
1956a	O
;	O
1956b	O
)	O
whose	O
work	O
was	O
based	O
on	O
results	O
showing	O
that	O
human	O
working	O
memory	O
is	O
limited	O
to	O
7	O
±	O
2	O
items	O
.	O
Yngve	O
's	O
mechanistic	O
,	O
incremental	O
model	O
of	O
language	O
production	O
considered	O
the	O
evaluation	O
of	O
phrase	O
structure	O
grammars	O
(	O
PSGs	O
)	O
in	O
a	O
system	O
with	O
finite	O
memory	O
,	O
exploring	O
the	O
structure	O
speakers	O
must	O
keep	O
track	O
of	O
during	O
production	O
and	O
how	O
grammars	O
might	O
be	O
structured	O
to	O
avoid	O
overtaxing	O
working	O
memory	O
.	O
Van	O
Schijndel	O
et	O
al	O
develop	O
this	O
idea	O
further	O
in	O
the	O
context	O
of	O
a	O
hierarchical	O
sequence	O
model	O
of	O
parsing	O
.	O
In	O
this	O
incremental	O
model	O
of	O
parsing	O
,	O
at	O
each	O
stage	O
the	O
reader	O
has	O
an	O
active	O
state	O
(	O
e.g.	O
S	O
for	O
sentence	O
)	O
and	O
an	O
awaited	O
state	O
(	O
e.g.	O
VP	O
for	O
verb	O
phrase	O
)	O
.	O
6	O
At	O
each	O
new	O
word	O
,	O
the	O
parser	O
must	O
decide	O
between	O
continuing	O
to	O
analyze	O
the	O
current	O
connected	O
component	O
or	O
hypothesizing	O
the	O
start	O
of	O
a	O
new	O
one	O
.	O
7	O
These	O
measures	O
provide	O
an	O
idealized	O
representation	O
of	O
the	O
number	O
of	O
different	O
states	O
a	O
human	O
parser	O
must	O
keep	O
track	O
of	O
at	O
any	O
point	O
in	O
time	O
.	O
We	O
refer	O
to	O
this	O
number	O
of	O
states	O
as	O
the	O
embedding	O
depth	O
of	O
a	O
sentence	O
at	O
a	O
particular	O
word	O
,	O
and	O
the	O
ModelBlocks	O
parser	O
of	O
van	O
Schijndel	O
et	O
al	O
(	O
2012	O
)	O
calculates	O
this	O
number	O
of	O
states	O
averaged	O
over	O
the	O
beam	O
of	O
currently	O
plausible	O
parses	O
.	O
Also	O
of	O
interest	O
is	O
the	O
embedding	O
difference	O
,	O
which	O
is	O
the	O
embedding	O
depth	O
at	O
the	O
present	O
word	O
relative	O
to	O
the	O
previous	O
word	O
,	O
elaborated	O
upon	O
in	O
the	O
following	O
example	O
.	O
Consider	O
the	O
state	O
described	O
above	O
(	O
i.e.	O
that	O
of	O
being	O
in	O
the	O
active	O
state	O
S	O
and	O
awaiting	O
state	O
VP	O
)	O
might	O
be	O
reached	O
after	O
a	O
reader	O
has	O
observed	O
a	O
noun	O
phrase	O
,	O
resulting	O
in	O
the	O
state	O
S	O
/	O
VP	O
.	O
This	O
means	O
that	O
the	O
word	O
sequence	O
observed	O
so	O
far	O
will	O
be	O
consistent	O
with	O
a	O
sentence	O
if	O
the	O
reader	O
now	O
observes	O
a	O
verb	O
phrase	O
.	O
If	O
,	O
however	O
,	O
the	O
next	O
word	O
in	O
the	O
input	O
is	O
inconsistent	O
with	O
the	O
start	O
of	O
a	O
verb	O
phrase	O
(	O
e.g.	O
the	O
relative	O
clause	O
marker	O
that	O
)	O
,	O
then	O
this	O
parse	O
will	O
be	O
ruled	O
out	O
and	O
another	O
must	O
be	O
considered	O
.	O
At	O
this	O
point	O
the	O
parser	O
must	O
hypothesize	O
the	O
beginning	O
of	O
a	O
new	O
connected	O
component	O
,	O
i.e.	O
a	O
new	O
syntactic	O
substructure	O
that	O
must	O
be	O
completed	O
before	O
continuing	O
to	O
parse	O
the	O
top	O
-	O
level	O
of	O
the	O
sentence	O
.	O
Therefore	O
,	O
the	O
parser	O
must	O
now	O
keep	O
track	O
of	O
two	O
states	O
:	O
(	O
1	O
)	O
the	O
fact	O
that	O
we	O
are	O
still	O
looking	O
for	O
a	O
VP	O
to	O
complete	O
the	O
overall	O
sentence	O
;	O
and	O
(	O
2	O
)	O
the	O
fact	O
that	O
we	O
now	O
have	O
a	O
relative	O
clause	O
to	O
parse	O
before	O
we	O
can	O
complete	O
the	O
current	O
NP	O
.	O
In	O
this	O
example	O
,	O
we	O
are	O
at	O
embedding	O
depth	O
1	O
or	O
0	B-DatasetName
up	O
until	O
we	O
encounter	O
the	O
word	O
that	O
,	O
which	O
increases	O
the	O
embedding	O
depth	O
by	O
1	O
,	O
resulting	O
in	O
a	O
nonzero	O
embedding	O
difference	O
score	O
.	O

We	O
used	O
two	O
corpora	O
in	O
this	O
work	O
.	O
The	O
English	O
and	O
Simple	O
English	O
Wikipedia	O
corpus	O
of	O
Hwang	O
et	O
al	O
(	O
2015	O
,	O
ESEW	O
)	O
is	O
a	O
new	O
corpus	O
of	O
more	O
than	O
150k	O
sentence	O
pairs	O
designed	O
to	O
address	O
the	O
flaws	O
of	O
the	O
Parallel	O
Wikipedia	O
Corpus	O
of	O
Zhu	O
et	O
al	O
(	O
2010	O
,	O
PWKP	O
)	O
,	O
which	O
was	O
previously	O
dominant	O
in	O
work	O
on	O
text	B-TaskName
simplification	I-TaskName
,	O
by	O
using	O
a	O
more	O
sophisticated	O
method	O
of	O
aligning	O
pairs	O
of	O
English	O
and	O
Simple	O
English	O
sentences	O
.	O
We	O
used	O
the	O
section	O
labeled	O
as	O
having	O
'	O
good	O
'	O
alignments	O
for	O
our	O
work	O
and	O
assumed	O
that	O
,	O
in	O
every	O
sentence	O
pair	O
,	O
the	O
Simple	O
English	O
sentence	O
should	O
be	O
ranked	O
as	O
easier	O
than	O
the	O
English	O
sentence	O
(	O
rank=1	O
<	O
rank=2	O
in	O
Table	O
1	O
)	O
.	O
This	O
provides	O
a	O
large	O
corpus	O
with	O
noisy	O
labels	O
,	O
as	O
there	O
are	O
likely	O
to	O
be	O
instances	O
where	O
the	O
English	O
and	O
Simple	O
English	O
sentences	O
are	O
not	O
substantially	O
different	O
or	O
the	O
English	O
sentence	O
is	O
the	O
easier	O
one	O
.	O
8	O
For	O
a	O
more	O
controlled	O
corpus	O
,	O
we	O
use	O
Vajjala	O
's	O
(	O
2015	O
)	O
One	O
Stop	O
English	O
(	O
OSE	O
)	O
corpus	O
.	O
This	O
corpus	O
consists	O
of	O
1577	O
sentence	O
triples	O
,	O
drawn	O
from	O
news	O
stories	O
edited	O
to	O
three	O
difficulty	O
levels	O
:	O
elementary	O
,	O
intermediate	O
,	O
and	O
advanced	O
.	O
Vajjala	O
used	O
T	O
F	O
*	O
IDF	O
and	O
cosine	O
similarity	O
scores	O
to	O
align	O
sentences	O
from	O
stories	O
drawn	O
from	O
onestopenglish.com	O
.	O
While	O
One	O
Stop	O
English	O
does	O
not	O
publish	O
an	O
explanation	O
of	O
their	O
methods	O
for	O
creating	O
these	O
texts	O
,	O
they	O
are	O
at	O
least	O
created	O
by	O
human	O
editors	O
for	O
pedagogical	O
purposes	O
,	O
so	O
the	O
labels	O
should	O
be	O
more	O
consistent	O
and	O
reliable	O
than	O
those	O
associated	O
with	O
the	O
ESEW	O
corpus	O
.	O
The	O
three	O
levels	O
of	O
OSE	O
make	O
it	O
possible	O
to	O
compare	O
system	O
performance	O
on	O
sentence	O
pairs	O
which	O
are	O
close	O
to	O
one	O
another	O
in	O
difficulty	O
(	O
e.g.	O
'	O
advanced	O
'	O
versus	O
'	O
intermediate	O
'	O
sentences	O
)	O
with	O
performance	O
on	O
pairs	O
which	O
are	O
further	O
apart	O
,	O
as	O
with	O
'	O
advanced	O
'	O
sentences	O
paired	O
with	O
their	O
'	O
elementary	O
'	O
counterparts	O
.	O
In	O
this	O
paper	O
we	O
will	O
refer	O
to	O
the	O
pairs	O
of	O
advanced	O
and	O
elementary	O
sentences	O
as	O
OSE	O
f	O
ar	O
,	O
the	O
remaining	O
pairs	O
as	O
OSE	O
near	O
,	O
and	O
the	O
full	O
OSE	O
dataset	O
as	O
OSE	O
all	O
.	O
An	O
example	O
triple	O
of	O
sentences	O
from	O
the	O
corpus	O
is	O
given	O
in	O
Table	O
2	O
.	O

We	O
used	O
two	O
parsers	O
to	O
extract	O
22	O
features	O
from	O
the	O
corpora	O
.	O
The	O
ModelBlocks	O
parser	O
provided	O
features	O
based	O
on	O
surprisal	O
and	O
embedding	O
depth	O
while	O
the	O
Stanford	O
parser	O
9	O
provided	O
the	O
dependency	O
parses	O
used	O
to	O
calculate	O
integration	O
cost	O
and	O
idea	O
density	O
features	O
.	O
Both	O
parsers	O
are	O
trained	O
and	O
perform	O
near	O
the	O
state	O
of	O
the	O
art	O
on	O
the	O
standard	O
sections	O
of	O
the	O
Wall	O
Street	O
Journal	O
section	O
of	O
the	O
Penn	B-DatasetName
Treebank	I-DatasetName
.	O
From	O
ModelBlocks	O
'	O
complexity	O
feature	O
extraction	O
mode	O
,	O
we	O
took	O
the	O
lexical	O
and	O
syntactic	O
surprisal	O
features	O
.	O
We	O
used	O
the	O
average	O
lexical	O
surprisal	O
and	O
average	O
syntactic	O
surprisal	O
as	O
idealized	O
measures	O
of	O
the	O
channel	O
capacity	O
required	O
to	O
read	O
a	O
sentence	O
.	O
While	O
this	O
underestimates	O
the	O
channel	O
capacity	O
required	O
to	O
process	O
a	O
sentence	O
,	O
it	O
is	O
at	O
least	O
internally	O
consistent	O
,	O
insofar	O
as	O
a	O
sentence	O
with	O
higher	O
average	O
surprisal	O
overall	O
is	O
likely	O
to	O
require	O
a	O
higher	O
channel	O
capacity	O
as	O
well	O
.	O
We	O
also	O
used	O
the	O
maximum	O
of	O
each	O
form	O
of	O
surprisal	O
as	O
a	O
measure	O
of	O
the	O
maximum	O
demand	O
on	O
cognitive	O
resources	O
.	O
These	O
features	O
comprise	O
the	O
SURPRISAL	O
model	O
.	O
We	O
also	O
calculated	O
average	O
and	O
maximum	O
values	O
for	O
the	O
embedding	O
depth	O
and	O
embedding	O
difference	O
output	O
from	O
ModelBlocks	O
.	O
The	O
average	O
provides	O
an	O
estimate	O
of	O
the	O
typical	O
memory	O
load	O
throughout	O
a	O
sentence	O
,	O
while	O
the	O
(	O
absolute	O
)	O
embedding	O
difference	O
is	O
a	O
measure	O
of	O
how	O
many	O
times	O
a	O
reader	O
needs	O
to	O
push	O
or	O
pop	O
a	O
connected	O
component	O
to	O
or	O
from	O
their	O
memory	O
store	O
.	O
These	O
features	O
comprise	O
the	O
EMBEDDING	O
model	O
.	O
To	O
extract	O
the	O
remaining	O
features	O
,	O
we	O
first	O
ran	O
the	O
Stanford	O
dependency	O
parser	O
on	O
both	O
corpora	O
.	O
The	O
program	O
icy	O
-	O
parses	O
uses	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
tags	O
and	O
head	O
-	O
dependent	O
relations	O
to	O
determine	O
the	O
total	O
,	O
average	O
,	O
and	O
maximum	O
integration	O
cost	O
across	O
a	O
sentence	O
.	O
Here	O
average	O
integration	O
cost	O
functions	O
as	O
another	O
kind	O
of	O
memory	O
load	O
estimate	O
while	O
the	O
maximum	O
value	O
models	O
the	O
most	O
-	O
Rank	O
Sentence	O
2	O
Gingerbread	O
was	O
brought	O
to	O
Europe	O
in	O
992	O
by	O
the	O
Armenian	O
monk	O
Gregory	O
of	O
Nicopolis	O
-	O
LRB	O
-	O
Gregory	O
Makar	O
-	O
RRB	O
-	O
-	O
LRB	O
-	O
Grégoire	O
de	O
Nicopolis	O
-	O
RRB	O
-	O
.	O
1	O
Armenian	O
monk	O
Gregory	O
of	O
Nicopolis	O
-	O
LRB	O
-	O
Gregory	O
Makar	O
-	O
RRB	O
-	O
-	O
LRB	O
-	O
Grgoire	O
de	O
Nicopolis	O
-	O
RRB	O
-	O
brought	O
ginger	O
bread	O
to	O
Europe	O
in	O
992	O
.	O
Table	O
1	O
:	O
Example	O
sentences	O
from	O
English	O
(	O
2	O
)	O
and	O
Simple	O
(	O
1	O
)	O
English	O
Wikipedia	O
.	O

In	O
order	O
to	O
rank	O
sentences	O
,	O
we	O
need	O
some	O
way	O
of	O
generating	O
a	O
complexity	O
score	O
for	O
each	O
sentence	O
.	O
Using	O
a	O
perceptron	O
model	O
allows	O
us	O
to	O
train	O
a	O
simple	O
linear	O
scoring	O
model	O
by	O
converting	O
the	O
ranking	O
task	O
into	O
a	O
classification	O
task	O
.	O
Suppose	O
we	O
have	O
two	O
sentences	O
s	O
1	O
and	O
s	O
2	O
with	O
feature	O
vectors	O
s	O
1	O
and	O
s	O
2	O
such	O
that	O
s	O
1	O
is	O
more	O
complex	O
than	O
s	O
2	O
.	O
Then	O
we	O
want	O
to	O
train	O
a	O
perceptron	O
model	O
such	O
that	O
score	O
(	O
s	O
1	O
)	O
>	O
score	O
(	O
s	O
2	O
)	O
(	O
2	O
)	O
W	O
s	O
1	O
>	O
W	O
s	O
2	O
(	O
3	O
)	O
W	O
(	O
s	O
1	O
−	O
s	O
2	O
)	O
>	O
0	B-DatasetName
(	O
4	O
)	O
We	O
refer	O
to	O
the	O
vector	O
s	O
1	O
−	O
s	O
2	O
as	O
a	O
vector	O
of	O
difference	O
features	O
.	O
In	O
order	O
to	O
train	O
the	O
model	O
,	O
we	O
take	O
all	O
pairs	O
of	O
sentences	O
present	O
in	O
a	O
given	O
corpus	O
and	O
create	O
a	O
difference	O
vector	O
as	O
above	O
.	O
In	O
half	O
of	O
the	O
cases	O
,	O
we	O
flip	O
the	O
sign	O
of	O
the	O
difference	O
vector	O
,	O
creating	O
a	O
binary	O
classification	O
task	O
with	O
balanced	O
classes	O
.	O
The	O
learning	O
problem	O
is	O
now	O
to	O
classify	O
each	O
difference	O
vector	O
based	O
on	O
whether	O
the	O
first	O
term	O
in	O
the	O
difference	O
was	O
the	O
'	O
easier	O
'	O
or	O
the	O
'	O
harder	O
'	O
sentence	O
Note	O
that	O
the	O
benefit	O
to	O
this	O
approach	O
is	O
that	O
the	O
resulting	O
weight	O
vector	O
W	O
learned	O
via	O
the	O
classification	O
task	O
can	O
be	O
used	O
directly	O
to	O
score	O
individual	O
sentences	O
as	O
well	O
,	O
with	O
the	O
expectation	O
that	O
higher	O
scores	O
will	O
correspond	O
to	O
more	O
difficult	O
sentences	O
.	O
We	O
use	O
an	O
averaged	O
perceptron	O
model	O
(	O
Collins	O
,	O
2002	O
)	O
implemented	O
in	O
Python	O
as	O
our	O
classifier	O
.	O

Narrated	O
instructional	O
videos	O
provide	O
rich	O
visual	O
,	O
acoustic	O
and	O
language	O
information	O
for	O
people	O
to	O
easily	O
understand	O
how	O
to	O
complete	O
a	O
task	O
by	O
procedures	O
.	O
An	O
increasing	O
amount	O
of	O
people	O
resort	O
to	O
narrated	O
instructional	O
videos	O
to	O
learn	O
skills	O
and	O
solve	O
problems	O
.	O
For	O
example	O
,	O
people	O
would	O
like	O
to	O
watch	O
videos	O
to	O
repair	O
a	O
water	O
damaged	O
plasterboard	O
/	O
drywall	O
ceiling	O
1	O
or	O
cook	O
Cottage	O
Pie	O
2	O
.	O
This	O
motivates	O
us	O
to	O
investigate	O
whether	O
machines	O
can	O
understand	O
narrated	O
instructional	O
videos	O
like	O
In	O
this	O
task	O
,	O
the	O
video	O
frames	O
and	O
the	O
transcript	O
are	O
given	O
to	O
(	O
1	O
)	O
extract	O
procedures	O
in	O
the	O
video	O
,	O
(	O
2	O
)	O
generate	O
a	O
descriptive	O
and	O
informative	O
sentence	O
as	O
the	O
caption	O
of	O
each	O
procedure	O
.	O
humans	O
.	O
Besides	O
,	O
watching	O
a	O
long	O
video	O
is	O
timeconsuming	O
,	O
captions	O
of	O
videos	O
provide	O
a	O
quick	O
overview	O
of	O
video	O
content	O
for	O
people	O
to	O
learn	O
the	O
main	O
steps	O
rapidly	O
.	O
Inspired	B-DatasetName
by	O
this	O
,	O
our	O
task	O
is	O
to	O
generate	O
procedure	O
captions	O
from	O
narrated	O
instructional	O
videos	O
which	O
are	O
a	O
sequence	O
of	O
step	O
-	O
wise	O
clips	O
with	O
a	O
description	O
as	O
shown	O
in	O
Figure	O
1	O
.	O
Previous	O
works	O
on	O
video	B-TaskName
understanding	I-TaskName
tend	O
to	O
recognize	O
actions	O
in	O
video	O
clips	O
by	O
detecting	O
pose	O
(	O
Wang	O
et	O
al	O
,	O
2013a	O
;	O
Packer	O
et	O
al	O
,	O
2012	O
)	O
and	O
motion	O
(	O
Wang	O
et	O
al	O
,	O
2013b	O
;	O
Yang	O
et	O
al	O
,	O
2013	O
)	O
or	O
both	O
(	O
Wang	O
et	O
al	O
,	O
2014	O
)	O
and	O
fine	O
-	O
grained	O
features	O
.	O
These	O
works	O
take	O
low	O
-	O
level	O
vision	O
features	O
into	O
account	O
and	O
can	O
only	O
detect	O
human	O
actions	O
,	O
instead	O
of	O
complicated	O
events	O
that	O
occur	O
in	O
the	O
scene	O
.	O
To	O
deeply	O
understand	O
the	O
video	O
content	O
,	O
Video	O
Dense	O
Captioning	O
(	O
Krishna	O
et	O
al	O
,	O
2017	O
)	O
is	O
proposed	O
to	O
generate	O
semantic	O
captions	O
for	O
a	O
video	O
.	O
The	O
goal	O
of	O
this	O
task	O
is	O
to	O
identify	O
all	O
events	O
inside	O
a	O
video	O
and	O
our	O
target	O
is	O
the	O
video	O
dense	O
captioning	O
on	O
narrated	O
instructional	O
videos	O
which	O
we	O
call	O
dense	O
procedure	O
captioning	O
.	O
Different	O
from	O
videos	O
in	O
the	O
open	O
domain	O
,	O
instructional	O
videos	O
contain	O
an	O
explicit	O
sequential	O
structure	O
of	O
procedures	O
accompanied	O
by	O
a	O
series	O
of	O
shots	O
and	O
descriptive	O
transcripts	O
.	O
Moreover	O
,	O
they	O
contain	O
fine	O
-	O
grained	O
information	O
including	O
actions	O
,	O
entities	O
,	O
and	O
their	O
interactions	O
.	O
According	O
to	O
our	O
analysis	O
,	O
many	O
fine	O
-	O
grained	O
entities	O
and	O
actions	O
also	O
present	O
in	O
captions	O
which	O
are	O
ignored	O
by	O
previous	O
works	O
like	O
(	O
Krishna	O
et	O
al	O
,	O
2017	O
;	O
Zhou	O
et	O
al	O
,	O
2018b	O
)	O
.	O
The	O
procedure	O
caption	O
should	O
be	O
detailed	O
and	O
informative	O
.	O
Previous	O
works	O
(	O
Krishna	O
et	O
al	O
,	O
2017	O
;	O
Xu	O
et	O
al	O
,	O
2016	O
)	O
for	O
video	B-TaskName
captioning	I-TaskName
usually	O
consist	O
of	O
two	O
stages	O
:	O
(	O
1	O
)	O
temporal	O
event	O
proposition	O
;	O
and	O
(	O
2	O
)	O
event	O
captioning	O
.	O
However	O
,	O
there	O
are	O
two	O
challenges	O
for	O
narrated	O
instructional	O
videos	O
:	O
one	O
of	O
the	O
challenges	O
is	O
that	O
video	O
content	O
fails	O
to	O
provide	O
semantic	O
information	O
so	O
as	O
to	O
extract	O
procedures	O
semantically	O
;	O
the	O
other	O
challenge	O
is	O
that	O
it	O
is	O
hard	O
to	O
recognize	O
fine	O
-	O
grained	O
entities	O
from	O
the	O
video	O
content	O
only	O
,	O
and	O
thus	O
tends	O
to	O
generate	O
coarse	O
captions	O
.	O
Previous	O
models	O
for	O
dense	B-TaskName
video	I-TaskName
captioning	I-TaskName
only	O
use	O
video	O
signals	O
without	O
considering	O
transcripts	O
.	O
We	O
argue	O
that	O
transcripts	O
in	O
narrated	O
instructional	O
videos	O
can	O
enhance	O
video	O
representation	O
by	O
providing	O
fine	O
-	O
grained	O
complimentary	O
and	O
semantic	O
textual	O
information	O
.	O
As	O
shown	O
in	O
Figure	O
1	O
,	O
the	O
task	O
takes	O
a	O
video	O
with	O
a	O
transcript	O
as	O
input	O
and	O
extracts	O
the	O
main	O
procedures	O
as	O
well	O
as	O
these	O
captions	O
.	O
The	O
whole	O
video	O
is	O
divided	O
into	O
four	O
proposal	O
procedure	O
spans	O
in	O
sequential	O
order	O
including	O
:	O
(	O
1	O
)	O
grate	O
some	O
pecorino	O
cheese	O
and	O
beat	O
the	O
eggs	O
during	O
time	O
span	O
[	O
0:00:12	O
-	O
0:00:46	O
]	O
,	O
(	O
2	O
)	O
then	O
stir	O
cheese	O
into	O
the	O
eggs	O
during	O
[	O
0:00:52	O
-	O
0:01:10	O
]	O
,	O
and	O
so	O
on	O
.	O
Besides	O
video	O
content	O
,	O
transcripts	O
can	O
provide	O
semantic	O
information	O
.	O
Our	O
model	O
embeds	O
transcript	O
using	O
a	O
pre	O
-	O
trained	O
context	O
-	O
aware	O
model	O
to	O
provide	O
rich	O
semantic	O
information	O
.	O
Furthermore	O
,	O
with	O
the	O
transcript	O
,	O
our	O
model	O
can	O
directly	O
"	O
copy	O
"	O
many	O
fine	O
-	O
grained	O
entities	O
,	O
e.g.	O
pecorino	O
cheese	O
for	O
procedure	O
captioning	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
utilizing	O
multi	O
-	O
modal	O
content	O
of	O
videos	O
including	O
frame	O
features	O
and	O
transcripts	O
to	O
conduct	O
procedure	O
extraction	O
and	O
captioning	O
.	O
First	O
,	O
we	O
use	O
the	O
transcript	O
of	O
instructional	O
videos	O
as	O
a	O
global	O
text	O
feature	O
and	O
fuse	O
it	O
with	O
video	O
signals	O
to	O
construct	O
context	O
-	O
aware	O
features	O
.	O
Then	O
we	O
use	O
temporal	O
convolution	B-MethodName
to	O
encode	O
these	O
features	O
and	O
generate	O
procedure	O
proposals	O
.	O
Next	O
,	O
the	O
fused	O
features	O
of	O
video	O
and	O
transcript	O
tokens	O
within	O
the	O
proposed	O
time	O
span	O
are	O
used	O
to	O
generate	O
the	O
final	O
caption	O
via	O
a	O
recurrent	O
model	O
.	O
Experiments	O
on	O
the	O
YouCookII	O
dataset	O
(	O
Zhou	O
et	O
al	O
,	O
2018a	O
)	O
(	O
a	O
cooking	O
-	O
domain	O
instructional	O
video	O
corpus	O
)	O
are	O
conducted	O
to	O
show	O
that	O
our	O
model	O
can	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
and	O
the	O
ablation	O
studies	O
demonstrate	O
that	O
the	O
transcript	O
can	O
not	O
only	O
improve	O
procedure	O
proposition	O
performance	O
but	O
also	O
be	O
very	O
effective	O
for	O
procedure	O
captioning	O
.	O
The	O
contributions	O
of	O
this	O
paper	O
are	O
as	O
follows	O
:	O
1	O
.	O
We	O
propose	O
a	O
model	O
fusing	O
transcript	O
of	O
narrated	O
instructional	O
video	O
during	O
procedure	O
extraction	O
and	O
captioning	O
.	O
2	O
.	O
We	O
employ	O
the	O
pre	O
-	O
trained	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
and	O
self	O
-	O
attention	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
layer	O
to	O
embed	O
transcript	O
,	O
and	O
then	O
integrate	O
them	O
to	O
visual	O
encoding	O
during	O
procedure	O
extraction	O
.	O
3	O
.	O
We	O
adopt	O
the	O
sequence	O
-	O
to	O
-	O
sequence	O
model	O
to	O
generate	O
captions	O
by	O
merging	O
tokens	O
of	O
the	O
transcript	O
with	O
the	O
aligned	O
video	O
frames	O
.	O

Narrated	O
Instructional	O
Video	B-TaskName
Understanding	I-TaskName
Previous	O
works	O
aim	O
to	O
ground	O
the	O
description	O
to	O
the	O
video	O
.	O
(	O
Malmaud	O
et	O
al	O
,	O
2015	O
)	O
adopted	O
an	O
HMM	O
model	O
to	O
align	O
the	O
recipe	O
steps	O
to	O
the	O
narration	O
.	O
(	O
Naim	O
et	O
al	O
,	O
2015	O
)	O
utilize	O
latent	O
-	O
variable	O
based	O
discriminative	O
models	O
(	O
CRF	B-MethodName
,	O
Structured	O
Perceptron	O
)	O
for	O
unsupervised	O
alignment	O
.	O
Besides	O
the	O
alignment	O
of	O
transcripts	O
with	O
video	O
,	O
(	O
Alayrac	O
et	O
al	O
,	O
2016	O
(	O
Alayrac	O
et	O
al	O
,	O
,	O
2018	O
propose	O
to	O
learn	O
the	O
main	O
steps	O
from	O
a	O
set	O
of	O
narrated	O
instructional	O
videos	O
for	O
five	O
different	O
tasks	O
and	O
formulate	O
the	O
problem	O
into	O
two	O
clustering	O
problems	O
.	O
Graph	O
-	O
based	O
clustering	O
is	O
also	O
adopted	O
to	O
learn	O
the	O
semantic	O
storyline	O
of	O
instructional	O
videos	O
in	O
(	O
Sener	O
et	O
al	O
,	O
2015	O
)	O
.	O
These	O
works	O
assume	O
that	O
"	O
one	O
task	O
"	O
has	O
the	O
same	O
procedures	O
.	O
Different	O
from	O
previous	O
works	O
,	O
we	O
focus	O
on	O
learning	O
more	O
complicated	O
procedures	O
for	O
Temporal	O
action	O
proposal	O
is	O
designed	O
to	O
divide	O
a	O
long	O
video	O
into	O
contiguous	O
segments	O
as	O
a	O
sequence	O
of	O
actions	O
,	O
which	O
is	O
similar	O
to	O
the	O
first	O
stage	O
of	O
our	O
model	O
.	O
(	O
Shou	O
et	O
al	O
,	O
2016	O
)	O
adopt	O
3D	O
convolutional	O
neural	O
networks	O
to	O
generate	O
multi	O
-	O
scale	O
proposals	O
.	O
DAPs	O
in	O
(	O
Escorcia	O
et	O
al	O
,	O
2016	O
)	O
apply	O
a	O
sliding	O
window	O
and	O
a	O
Long	B-MethodName
Short	I-MethodName
-	I-MethodName
Term	I-MethodName
Memory	I-MethodName
(	O
LSTM	B-MethodName
)	O
network	O
for	O
video	O
content	O
encoding	O
and	O
predicting	O
proposals	O
covered	O
by	O
the	O
window	O
.	O
SST	B-DatasetName
in	O
(	O
Buch	O
et	O
al	O
,	O
2017	O
)	O
effectively	O
generates	O
proposals	O
in	O
a	O
single	O
pass	O
.	O
However	O
,	O
previous	O
methods	O
do	O
not	O
consider	O
context	O
information	O
to	O
produce	O
nonoverlapped	O
procedures	O
.	O
(	O
Zhou	O
et	O
al	O
,	O
2018a	O
)	O
is	O
the	O
most	O
similar	O
work	O
to	O
ours	O
,	O
which	O
is	O
designed	O
to	O
detect	O
long	O
complicated	O
event	O
proposals	O
rather	O
than	O
actions	O
.	O
We	O
adopt	O
this	O
framework	O
and	O
inject	O
the	O
textual	O
transcript	O
of	O
narrated	O
instructional	O
videos	O
as	O
our	O
first	O
step	O
.	O
Dense	O
video	O
caption	O
aims	O
to	O
generate	O
descriptive	O
sentences	O
for	O
all	O
events	O
in	O
the	O
video	O
.	O
Different	O
from	O
video	B-TaskName
captioning	I-TaskName
and	O
paragraph	O
generation	O
,	O
dense	O
video	O
caption	O
requires	O
segmenting	O
of	O
each	O
video	O
into	O
a	O
sequence	O
of	O
temporal	O
propos	O
-	O
als	O
with	O
corresponding	O
captions	O
.	O
(	O
Krishna	O
et	O
al	O
,	O
2017	O
)	O
resorts	O
to	O
the	O
DAP	O
method	O
(	O
Escorcia	O
et	O
al	O
,	O
2016	O
)	O
for	O
event	B-TaskName
detection	I-TaskName
and	O
apply	O
the	O
contextaware	O
S2VT	O
model	O
(	O
Venugopalan	O
et	O
al	O
,	O
2015	O
)	O
.	O
(	O
Yu	O
et	O
al	O
,	O
2018	O
)	O
propose	O
to	O
generate	O
long	O
and	O
detailed	O
description	O
for	O
sport	O
videos	O
.	O
(	O
Li	O
et	O
al	O
,	O
2018	O
)	O
train	O
jointly	O
on	O
unifying	O
the	O
temporal	O
proposal	O
localization	O
and	O
sentence	O
generation	O
for	O
dense	B-TaskName
video	I-TaskName
captioning	I-TaskName
.	O
(	O
Xiong	O
et	O
al	O
,	O
2018	O
)	O
assembles	O
temporally	O
localized	O
description	O
to	O
produce	O
a	O
descriptive	O
paragraph	O
.	O
(	O
Duan	O
et	O
al	O
,	O
2018	O
)	O
propose	O
weakly	O
supervised	O
dense	O
event	O
captioning	O
,	O
which	O
does	O
not	O
require	O
temporal	O
segment	O
annotations	O
,	O
and	O
decomposes	O
the	O
problem	O
into	O
a	O
pair	O
of	O
dual	O
tasks	O
.	O
(	O
Wang	O
et	O
al	O
,	O
2018a	O
)	O
exploit	O
both	O
past	O
and	O
future	O
context	O
for	O
predicting	O
accurate	O
event	O
proposals	O
.	O
(	O
Zhou	O
et	O
al	O
,	O
2018b	O
)	O
adopt	O
a	O
transformer	O
for	O
action	O
proposing	O
and	O
captioning	O
simultaneously	O
.	O
Besides	O
,	O
there	O
are	O
also	O
some	O
works	O
try	O
to	O
incorporate	O
multi	O
-	O
modal	O
information	O
(	O
e.g.	O
audio	O
stream	O
)	O
for	O
dense	B-TaskName
video	I-TaskName
captioning	I-TaskName
task	O
(	O
Ramanishka	O
et	O
al	O
,	O
2016	O
;	O
Xu	O
et	O
al	O
,	O
2017	O
;	O
Wang	O
et	O
al	O
,	O
2018b	O
)	O
.	O
The	O
major	O
difference	O
is	O
that	O
our	O
work	O
adopts	O
a	O
different	O
model	O
structure	O
and	O
fuses	O
transcripts	O
to	O
further	O
enhance	O
semantic	O
representation	O
.	O
Experiments	O
show	O
that	O
transcripts	O
can	O
improve	O
both	O
procedure	O
ex	O
-	O
traction	O
and	O
captioning	O
.	O

We	O
first	O
encode	O
transcripts	O
and	O
video	O
frames	O
separately	O
and	O
then	O
extract	O
cross	O
-	O
modal	O
features	O
by	O
feeding	O
both	O
embeddings	O
into	O
a	O
context	O
-	O
aware	O
model	O
.	O
To	O
embed	O
transcripts	O
,	O
we	O
first	O
split	O
all	O
tokens	O
in	O
the	O
transcript	O
by	O
a	O
sliding	O
window	O
and	O
input	O
them	O
into	O
a	O
uncased	O
BERT	B-MethodName
-	O
large	O
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
model	O
.	O
Next	O
,	O
we	O
encode	O
these	O
sentences	O
by	O
a	O
Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
and	O
take	O
the	O
first	O
output	O
as	O
the	O
context	O
-	O
aware	O
transcript	O
embedding	O
e	O
R	O
e	O
.	O
To	O
embed	O
the	O
videos	O
,	O
we	O
uniformly	O
sample	O
T	O
frames	O
and	O
encode	O
each	O
frame	O
v	O
t	O
in	O
V	O
=	O
{	O
v	O
1	O
,	O
,	O
v	O
T	O
}	O
to	O
an	O
embedding	O
representation	O
by	O
an	O
ImageNet	B-DatasetName
-	O
pre	O
-	O
trained	O
ResNet	B-MethodName
-	O
32	O
(	O
He	O
et	O
al	O
,	O
2016	O
)	O
network	O
.	O
Then	O
we	O
adopt	O
another	O
Transformer	B-MethodName
model	O
to	O
further	O
encode	O
the	O
context	O
information	O
,	O
and	O
output	O
X	O
=	O
{	O
x	O
1	O
,	O
,	O
x	O
T	O
}	O
R	O
T	O
×d	O
.	O
Finally	O
,	O
we	O
combine	O
each	O
of	O
the	O
frame	O
features	O
in	O
X	O
with	O
transcript	O
feature	O
e	O
to	O
get	O
the	O
fused	O
feature	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
in	O
order	O
to	O
encode	O
past	O
and	O
future	O
contextual	O
information	O
of	O
video	O
frames	O
:	O
C	O
=	O
{	O
c	O
1	O
,	O
,	O
c	O
t	O
,	O
,	O
c	O
T	O
|	O
c	O
t	O
=	O
{	O
x	O
t	O
e	O
}	O
}	O
and	O
feed	O
it	O
into	O
a	O
Bi	O
-	O
directional	O
LSTM	B-MethodName
F	O
=	O
Bi	O
-	O
LSTM	B-MethodName
(	O
C	O
)	O
where	O
F	O
=	O
{	O
f	O
1	O
f	O
T	O
}	O
R	O
T	O
×f	O
,	O
and	O
f	O
is	O
the	O
hidden	O
size	O
of	O
the	O
LSTM	B-MethodName
layers	O
.	O

We	O
design	O
an	O
LSTM	B-MethodName
based	O
sequence	O
-	O
to	O
-	O
sequence	O
model	O
(	O
Sutskever	O
et	O
al	O
,	O
2014	O
)	O
to	O
generate	O
captions	O
for	O
each	O
extracted	O
procedure	O
.	O
For	O
the	O
(	O
k	O
,	O
t	O
)	O
-	O
th	O
extracted	O
procedure	O
,	O
we	O
calculate	O
the	O
starting	O
time	O
t	O
s	O
and	O
ending	O
time	O
t	O
e	O
separately	O
and	O
retrieve	O
all	O
tokens	O
within	O
the	O
time	O
span	O
[	O
t	O
s	O
,	O
t	O
e	O
]	O
:	O
E	O
(	O
t	O
s	O
,	O
t	O
e	O
)	O
=	O
{	O
e	O
ts	B-MethodName
,	O
,	O
e	O
te	O
}	O
⊂	O
{	O
e	O
1	O
,	O
,	O
e	O
Q	O
}	O
where	O
Q	O
is	O
the	O
total	O
word	O
count	O
of	O
a	O
video	O
's	O
transcript	O
.	O
On	O
each	O
step	O
,	O
we	O
concatenate	O
the	O
embedding	O
representation	O
of	O
each	O
token	O
q	O
E	O
(	O
t	O
s	O
,	O
t	O
e	O
)	O
,	O
i.e.	O
q	O
,	O
with	O
the	O
nearest	O
video	O
frame	O
feature	O
fq	O
into	O
the	O
input	O
vector	O
e	O
q	O
=	O
{	O
q	O
fq	O
}	O
of	O
the	O
encoder	O
.	O
We	O
employ	O
the	O
hidden	O
state	O
of	O
the	O
last	O
step	O
after	O
encoding	O
all	O
tokens	O
in	O
E	O
(	O
t	O
s	O
,	O
t	O
e	O
)	O
and	O
decode	O
the	O
caption	O
of	O
this	O
extracted	O
procedure	O
as	O
W	O
=	O
{	O
w	O
1	O
,	O
,	O
w	O
Z	O
}	O
where	O
Z	O
is	O
the	O
word	O
count	O
of	O
the	O
decoded	O
procedure	O
caption	O
.	O

In	O
this	O
paper	O
,	O
we	O
use	O
the	O
YouCookII	O
3	O
(	O
Zhou	O
et	O
al	O
,	O
2018a	O
)	O
dataset	O
to	O
conduct	O
experiments	O
.	O
It	O
contains	O
2000	O
videos	O
dumped	O
from	O
YouTube	O
which	O
are	O
all	O
instructional	O
cooking	O
recipe	O
videos	O
.	O
For	O
each	O
video	O
,	O
human	O
annotators	O
were	O
asked	O
to	O
first	O
label	O
the	O
starting	O
and	O
ending	O
time	O
of	O
procedure	O
segments	O
,	O
and	O
then	O
write	O
captions	O
for	O
each	O
procedure	O
.	O
This	O
dataset	O
contains	O
pre	O
-	O
processed	O
frame	O
features	O
(	O
T	O
=	O
500	O
frames	O
for	O
each	O
video	O
,	O
each	O
frame	O
feature	O
is	O
a	O
512	O
-	O
d	O
vector	O
,	O
extracted	O
by	O
ResNet	B-MethodName
-	O
32	O
)	O
which	O
were	O
used	O
in	O
(	O
Zhou	O
et	O
al	O
,	O
2018a	O
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
also	O
use	O
these	O
pre	O
-	O
computed	O
video	O
features	O
for	O
our	O
task	O
.	O
Besides	O
the	O
video	O
content	O
,	O
our	O
proposed	O
model	O
also	O
relies	O
on	O
transcripts	O
to	O
provide	O
multi	O
-	O
modality	O
information	O
.	O
Since	O
the	O
YouCookII	O
dataset	O
does	O
not	O
have	O
transcripts	O
,	O
we	O
crawl	O
all	O
transcripts	O
automatically	O
generated	O
by	O
YouTube	O
's	O
ASR	O
engine	O
.	O
YouCookII	O
provides	O
a	O
partition	O
on	O
these	O
2000	O
videos	O
:	O
1333	O
for	O
training	O
,	O
457	O
for	O
validation	O
and	O
210	O
for	O
testing	O
.	O
However	O
,	O
the	O
labels	O
of	O
210	O
testing	O
videos	O
are	O
unpublished	O
,	O
we	O
can	O
only	O
adopt	O
the	O
training	O
and	O
validation	B-DatasetName
dataset	I-DatasetName
for	O
our	O
experiment	O
.	O
We	O
also	O
remove	O
several	O
videos	O
which	O
are	O
unavailable	O
on	O
YouTube	O
.	O
In	O
all	O
,	O
we	O
use	O
1387	O
videos	O
from	O
the	O
YouCookII	O
dataset	O
.	O
We	O
split	O
these	O
videos	O
into	O
967	O
for	O
training	O
,	O
210	O
for	O
validation	O
and	O
210	O
for	O
testing	O
.	O
As	O
shown	O
in	O

As	O
shown	O
in	O
Table	O
1	O
,	O
we	O
first	O
show	O
the	O
results	O
reported	O
in	O
(	O
Zhou	O
et	O
al	O
,	O
2018a	O
)	O
which	O
use	O
the	O
full	O
dataset	O
with	O
2000	O
videos	O
.	O
In	O
order	O
to	O
ensure	O
a	O
fair	O
comparison	O
,	O
we	O
first	O
run	O
the	O
ProcNets	O
on	O
the	O
validation	B-DatasetName
dataset	I-DatasetName
of	O
YouCookII	O
and	O
get	O
a	O
comparable	O
result	O
.	O
In	O
further	O
experiments	O
,	O
we	O
directly	O
use	O
the	O
subset	O
(	O
the	O
our	O
partition	O
in	O
the	O
table	O
)	O
described	O
in	O
the	O
previous	O
section	O
.	O
Moreover	O
,	O
we	O
conduct	O
two	O
experiments	O
to	O
demonstrate	O
the	O
effectiveness	O
of	O
incorporating	O
transcripts	O
in	O
this	O
task	O
.	O
The	O
Ours	O
(	O
Full	O
Model	O
)	O
is	O
the	O
final	O
model	O
we	O
propose	O
,	O
which	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
.	O
The	O
Ours	O
(	O
Video	O
Only	O
)	O
model	O
considers	O
video	O
content	O
without	O
transcripts	O
in	O
the	O
procedure	O
extraction	O
module	O
.	O
Compared	O
with	O
ProcNets	O
,	O
our	O
video	O
only	O
model	O
adds	O
a	O
captioning	O
module	O
,	O
which	O
helps	O
the	O
procedure	O
extraction	O
module	O
to	O
get	O
a	O
better	O
result	O
.	O

Domain	B-TaskName
Adaptation	I-TaskName
for	O
Sentiment	B-TaskName
Analysis	I-TaskName
using	O
Keywords	O
in	O
the	O
Target	O
Domain	O
as	O
the	O
Learning	O
Weight	O

This	O
paper	O
proposes	O
a	O
new	O
method	O
of	O
instance	O
-	O
based	O
domain	B-TaskName
adaptation	I-TaskName
for	O
sentiment	B-TaskName
analysis	I-TaskName
.	O
First	O
,	O
our	O
method	O
defines	O
the	O
likelihood	O
of	O
keywords	O
,	O
through	O
the	O
value	O
of	O
inverse	O
document	O
frequency	O
(	O
IDF	O
)	O
,	O
for	O
each	O
word	O
in	O
documents	O
in	O
the	O
target	O
domain	O
.	O
Next	O
,	O
the	O
keyword	O
content	O
rate	O
of	O
a	O
document	O
is	O
calculated	O
using	O
the	O
likelihood	O
of	O
keywords	O
and	O
the	O
domain	B-TaskName
adaptation	I-TaskName
is	O
performed	O
by	O
giving	O
the	O
keyword	O
content	O
rate	O
to	O
each	O
document	O
in	O
the	O
source	O
domain	O
as	O
the	O
weight	O
.	O
The	O
experiment	O
used	O
an	O
Amazon	O
dataset	O
to	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
proposed	O
method	O
.	O
Although	O
the	O
instance	O
-	O
based	O
method	O
has	O
not	O
shown	O
great	O
efficiency	O
,	O
the	O
advantages	O
combining	O
instance	O
-	O
based	O
method	O
and	O
feature	O
-	O
based	O
method	O
are	O
shown	O
in	O
this	O
paper	O
.	O

Domain	B-TaskName
adaptation	I-TaskName
is	O
roughly	O
divided	O
into	O
two	O
types	O
:	O
the	O
supervised	O
approach	O
using	O
labeled	O
data	O
in	O
the	O
target	O
domain	O
and	O
the	O
unsupervised	O
approach	O
that	O
does	O
not	O
use	O
them	O
.	O
For	O
supervised	O
approach	O
,	O
Daumé	O
's	O
method	O
(	O
Daumé	O
III	O
,	O
2007	O
)	O
has	O
become	O
a	O
standard	O
method	O
because	O
of	O
its	O
simplicity	O
and	O
high	O
ability	O
.	O
The	O
method	O
in	O
the	O
current	O
research	O
is	O
an	O
unsupervised	O
approach	O
.	O
Unsupervised	O
approaches	O
can	O
further	O
be	O
divided	O
into	O
two	O
types	O
:	O
feature	O
-	O
based	O
and	O
instance	O
-	O
based	O
(	O
Pan	O
and	O
Yang	O
,	O
2010	O
)	O
.	O
They	O
are	O
both	O
weighted	O
learning	O
methods	O
;	O
feature	O
-	O
based	O
methods	O
give	O
weights	O
to	O
features	O
and	O
instance	O
-	O
based	O
methods	O
give	O
weights	O
to	O
instances	O
.	O
Among	O
featurebased	O
methods	O
,	O
the	O
most	O
representative	O
method	O
is	O
structural	O
correspondence	O
learning	O
(	O
SCL	O
)	O
(	O
Blitzer	O
et	O
al	O
,	O
2006	O
)	O
.	O
In	O
addition	O
,	O
CORAL	O
has	O
attracted	O
much	O
attention	O
for	O
its	O
simplicity	O
and	O
high	O
ability	O
in	O
recent	O
years	O
.	O
Moreover	O
,	O
the	O
feature	O
-	O
based	O
methods	O
with	O
deep	O
learning	O
(	O
Glorot	O
et	O
al	O
,	O
2011	O
)	O
,	O
the	O
expanded	O
CORAL	O
and	O
adversarial	O
networks	O
(	O
Ganin	O
and	O
Lempitsky	O
,	O
2015	O
)	O
(	O
Tzeng	O
et	O
al	O
,	O
2017	O
)	O
are	O
also	O
considered	O
as	O
the	O
state	O
of	O
the	O
art	O
.	O
On	O
the	O
other	O
hand	O
,	O
instance	O
-	O
based	O
methods	O
have	O
not	O
been	O
studied	O
as	O
much	O
as	O
feature	O
-	O
based	O
methods	O
.	O
The	O
instance	O
-	O
based	O
method	O
assumes	O
a	O
covariate	O
shift	O
.	O
A	O
covariate	O
shift	O
assumes	O
P	O
S	O
(	O
c	O
|	O
x	O
)	O
=	O
P	O
T	O
(	O
c	O
|	O
x	O
)	O
and	O
P	O
S	O
(	O
x	O
)	O
=	O
P	O
T	O
(	O
x	O
)	O
.	O
Under	O
a	O
covariate	O
shift	O
,	O
P	O
T	O
(	O
c	O
|	O
x	O
)	O
can	O
be	O
obtained	O
by	O
the	O
weighted	O
learning	O
that	O
uses	O
the	O
probability	O
density	O
ratio	O
r	O
=	O
P	O
T	O
(	O
x	O
)	O
/P	O
S	O
(	O
x	O
)	O
as	O
the	O
weight	O
of	O
the	O
document	O
of	O
the	O
source	O
data	O
x.	O
There	O
are	O
a	O
variety	O
of	O
methods	O
for	O
calculating	O
the	O
probability	O
density	O
ratio	O
.	O
The	O
simplest	O
way	O
to	O
calculate	O
the	O
ratio	O
is	O
directly	O
estimate	O
P	O
S	O
(	O
x	O
)	O
and	O
P	O
T	O
(	O
x	O
)	O
,	O
but	O
in	O
the	O
case	O
of	O
complex	O
models	O
,	O
the	O
problem	O
will	O
be	O
more	O
complicated	O
.	O
Thus	O
,	O
the	O
method	O
that	O
directly	O
models	O
the	O
probability	O
density	O
ratio	O
was	O
studied	O
.	O
Among	O
these	O
methods	O
,	O
uLSIF	O
(	O
Yamada	O
et	O
al	O
,	O
2011	O
)	O
is	O
widely	O
used	O
because	O
the	O
time	O
complexity	O
of	O
the	O
method	O
is	O
relatively	O
small	O
.	O
However	O
,	O
P	O
(	O
x	O
)	O
of	O
bag	O
-	O
of	O
-	O
words	O
can	O
be	O
modeled	O
by	O
Naive	O
Bayes	O
model	O
if	O
the	O
problem	O
is	O
limited	O
to	O
natural	O
language	O
processing	O
.	O
Therefore	O
,	O
(	O
Shinnou	O
and	O
Sasaki	O
,	O
2014	O
)	O
defined	O
P	O
r	O
(	O
x	O
)	O
,	O
the	O
prior	O
of	O
x	O
,	O
as	O
follows	O
:	O
P	O
R	O
(	O
x	O
)	O
=	O
∏	O
n	O
i=1	O
P	O
R	O
(	O
f	O
i	O
)	O
,	O
where	O
x	O
denotes	O
a	O
data	O
in	O
the	O
domain	O
R	O
and	O
x	O
has	O
a	O
set	O
of	O
features	O
,	O
that	O
is	O
,	O
x	O
=	O
{	O
f	O
1	O
,	O
f	O
2	O
,	O
,	O
f	O
n	O
}	O
.	O
They	O
also	O
obtain	O
P	O
R	O
(	O
f	O
i	O
)	O
using	O
the	O
following	O
equation	O
:	O
P	O
R	O
(	O
f	O
)	O
=	O
n	O
(	O
R	O
,	O
f	O
)	O
+1	O
N	O
(	O
R	O
)	O
+2	O
.	O
Here	O
,	O
n	O
(	O
R	O
;	O
f	O
)	O
is	O
the	O
frequency	O
of	O
feature	O
f	O
in	O
the	O
domain	O
R	O
,	O
and	O
n	O
(	O
R	O
)	O
is	O
the	O
number	O
of	O
data	O
in	O
the	O
domain	O
R.	O
Therefore	O
,	O
the	O
probability	O
density	O
ratio	O
is	O
obtained	O
as	O
follows	O
:	O
r	O
=	O
P	O
T	O
(	O
x	O
)	O
P	O
S	O
(	O
x	O
)	O
=	O
n	O
(	O
T	O
,	O
f	O
)	O
+	O
1	O
N	O
(	O
T	O
)	O
+	O
2	O
N	O
(	O
S	O
)	O
+	O
2	O
n	O
(	O
S	O
,	O
f	O
)	O
+	O
1	O
(	O
1	O
)	O
3	O
Proposed	O
Method	O

Set	O
the	O
weight	O
w	O
x	O
of	O
the	O
instance	O
x	O
in	O
the	O
source	O
domain	O
.	O
The	O
words	O
(	O
file	O
)	O
x	O
is	O
{	O
w	O
i	O
}	O
K	O
i=1	O
,	O
and	O
the	O
frequency	O
within	O
x	O
for	O
word	O
w	O
i	O
is	O
f	O
i	O
.	O
Using	O
these	O
,	O
w	O
x	O
is	O
given	O
by	O
the	O
following	O
equation	O
:	O
w	O
x	O
=	O
1	O
∑	O
k	O
i=1	O
f	O
i	O
K	O
∑	O
i=1	O
f	O
i	O
l	O
w	O
i	O
4	O
Experiment	O
The	O
Amazon	O
dataset	O
(	O
Blitzer	O
et	O
al	O
,	O
2007	O
)	O
used	O
in	O
the	O
experiment	O
is	O
specifically	O
developed	O
using	O
the	O
processed_acl.tar.gz	O
file	O
on	O
the	O
following	O
website	O
.	O
https://www.cs.jhu.edu/˜mdredze/	O
datasets	O
/	O
sentiment/.	O
The	O
data	O
include	O
books	O
(	O
B	O
)	O
,	O
dvd	O
(	O
D	O
)	O
,	O
electronics	O
(	O
E	O
)	O
,	O
and	O
kitchen	O
(	O
K	O
)	O
.	O
The	O
number	O
of	O
files	O
contained	O
in	O
each	O
domain	O
is	O
shown	O
in	O
The	O
learning	O
algorithm	O
is	O
an	O
SVM	B-MethodName
with	O
scikitlearn	O
.	O
The	O
core	O
is	O
linear	O
,	O
the	O
value	O
of	O
the	O
c	O
parameter	O
is	O
fixed	O
at	O
0.1	O
,	O
and	O
the	O
scikit	O
-	O
learn	O
SVM	B-MethodName
supports	O
Weighted	O
-	O
Learning	O
1	O
,	O
so	O
the	O
scikit	O
-	O
learn	O
SVM	B-MethodName
is	O
used	O
here	O
.	O
domain	O
adaptations	O
are	O
:	O
B	O
D	O
,	O
B	O
E	O
,	O
B	O
K	O
,	O
D	O
B	O
,	O
D	O
E	O
,	O
D	O
K	O
,	O
E	O
B	O
,	O
E	O
D	O
,	O
E	O
K	O
,	O
K	O
B	O
,	O
K	O
D	O
,	O
K	O
E.	O
See	O
Table	O
1	O
for	O
the	O
results	O
of	O
the	O
two	O
methods	O
uLSIF	O
(	O
Yamada	O
et	O
al	O
,	O
2011	O
)	O
and	O
using	O
equation	O
(	O
1	O
)	O
of	O
Naive	O
Bayes	O
for	O
determining	O
the	O
rate	O
density	O
ratio	O
of	O
each	O
domain	O
and	O
the	O
proposed	O
method	O
.	O
NONE	O
in	O
Table	O
1	O
means	O
that	O
the	O
domain	B-TaskName
adaptation	I-TaskName
method	O
was	O
not	O
used	O
but	O
simply	O
applies	O
the	O
classifier	O
formed	O
from	O
the	O
training	O
data	O
of	O
the	O
source	O
domain	O
to	O
the	O
result	O
of	O
the	O
test	O
data	O
in	O
the	O
target	O
domain	O
was	O
applied	O
.	O
In	O
addition	O
,	O
IDEAL	O
is	O
a	O
result	O
using	O
the	O
training	O
data	O
in	O
the	O
target	O
domain	O
to	O
learn	O
through	O
the	O
classifier	O
and	O
apply	O
it	O
to	O
the	O
test	O
data	O
in	O
the	O
target	O
domain	O
.	O
Using	O
the	O
case	O
as	O
a	O
weighted	O
method	O
,	O
a	O
compari	O
-	O
son	O
of	O
uLSIF	O
,	O
NB	O
,	O
the	O
our	O
method	O
shows	O
that	O
the	O
six	O
highest	O
correct	O
answer	O
rates	O
in	O
the	O
12	O
domain	O
adaptations	O
are	O
obtained	O
by	O
our	O
method	O
,	O
and	O
the	O
remaining	O
six	O
highest	O
positive	O
answer	O
rates	O
are	O
obtained	O
by	O
NB	O
.	O
When	O
we	O
take	O
12	O
averages	O
,	O
the	O
solution	O
rate	O
of	O
our	O
method	O
is	O
more	O
than	O
that	O
of	O
NB	O
,	O
and	O
our	O
method	O
is	O
weighted	O
with	O
example	O
and	O
,	O
which	O
is	O
excellent	O
.	O

This	O
paper	O
proposed	O
a	O
method	O
for	O
instance	O
-	O
based	O
domain	B-TaskName
adaptation	I-TaskName
of	O
sentiment	B-TaskName
analysis	I-TaskName
.	O
For	O
outline	O
,	O
from	O
the	O
target	O
domain	O
,	O
using	O
IDF	O
to	O
set	O
the	O
likelihood	O
of	O
keywords	O
,	O
and	O
the	O
data	O
in	O
the	O
source	O
domain	O
,	O
the	O
content	O
rate	O
in	O
the	O
target	O
domain	O
keyword	O
,	O
and	O
the	O
keyword	O
content	O
rate	O
as	O
the	O
weight	O
.	O
In	O
the	O
experiment	O
,	O
we	O
compared	O
our	O
proposed	O
method	O
with	O
two	O
typical	O
instance	O
-	O
based	O
methods	O
:	O
uLSI	O
using	O
the	O
probability	O
density	O
ratio	O
and	O
the	O
method	O
defining	O
weight	O
through	O
Naive	O
Bayes	O
model	O
.	O
However	O
,	O
using	O
an	O
instance	O
-	O
based	O
alone	O
to	O
perform	O
domain	B-TaskName
adaptation	I-TaskName
has	O
a	O
very	O
small	O
effect	O
,	O
the	O
combining	O
instance	O
-	O
based	O
method	O
and	O
feature	O
-	O
based	O
method	O
is	O
assured	O
as	O
shown	O
in	O
this	O
paper	O
.	O
Further	O
,	O
the	O
combination	O
is	O
easy	O
to	O
implement	O
in	O
the	O
neural	O
network	O
model	O
.	O
Thus	O
,	O
we	O
will	O
investigate	O
this	O
approach	O
in	O
future	O
.	O

Joint	O
Learning	O
of	O
POS	O
and	O
Dependencies	O
for	O
Multilingual	O
Universal	O
Dependency	B-TaskName
Parsing	I-TaskName

In	O
this	O
section	O
,	O
we	O
describe	O
our	O
joint	O
model	O
2	O
for	O
POS	O
tagging	O
and	O
dependency	B-TaskName
parsing	I-TaskName
in	O
the	O
CoNLL	O
2018	O
Shared	O
Task	O
,	O
which	O
is	O
built	O
on	O
the	O
STACKPTR	O
parser	O
introduced	O
by	O
(	O
Ma	O
et	O
al	O
,	O
2018	O
)	O
.	O
Our	O
model	O
is	O
mainly	O
composed	O
of	O
three	O
components	O
,	O
the	O
representation	O
(	O
Section	O
2.1	O
)	O
,	O
POS	O
tagger	O
(	O
Section	O
2.2	O
)	O
and	O
dependency	O
parser	O
(	O
Section	O
2.3	O
)	O
.	O
Figure	O
1	O
illustrates	O
the	O
overall	O
model	O
.	O

Representation	O
is	O
a	O
key	O
component	O
in	O
various	O
NLP	O
models	O
,	O
and	O
good	O
representations	O
should	O
ideally	O
model	O
both	O
complex	O
characteristics	O
and	O
linguistic	O
contexts	O
.	O
In	O
our	O
system	O
,	O
we	O
follow	O
the	O
bidirectional	B-MethodName
LSTM	I-MethodName
-	O
CNN	O
architecture	O
(	O
BiLSTM	B-MethodName
-	O
CNNs	O
)	O
(	O
Chiu	O
and	O
Nichols	O
,	O
2016	O
;	O
Ma	O
and	O
Hovy	O
,	O
2016	O
)	O
,	O
where	O
CNNs	O
encode	O
word	O
information	O
into	O
character	O
-	O
level	O
representation	O
and	O
BiLSTM	B-MethodName
models	O
context	O
information	O
of	O
each	O
word	O
.	O
Character	O
Level	O
Representation	O
Though	O
word	O
embedding	O
is	O
popular	O
in	O
many	O
existing	O
parsers	O
,	O
they	O
are	O
not	O
ideal	O
for	O
languages	O
with	O
high	O
out	O
-	O
ofvocabulary	O
(	O
OOV	O
)	O
ratios	O
.	O
Hence	O
,	O
our	O
system	O
introduces	O
the	O
character	O
-	O
level	O
(	O
Li	O
et	O
al	O
,	O
2018a	O
)	O
representation	O
to	O
address	O
the	O
challenge	O
.	O
Formally	O
,	O
given	O
a	O
word	O
w	O
=	O
{	O
BOW	O
,	O
c	O
1	O
,	O
c	O
2	O
,	O
...	O
,	O
c	O
n	O
,	O
EOW	O
}	O
,	O
where	O
two	O
special	O
BOW	O
(	O
begin	O
-	O
of	O
-	O
word	O
)	O
and	O
EOW	O
(	O
end	O
-	O
of	O
-	O
word	O
)	O
tags	O
indicate	O
the	O
begin	O
and	O
end	O
positions	O
respectively	O
,	O
we	O
use	O
the	O
CNN	O
to	O
extract	O
character	O
-	O
level	O
representation	O
as	O
follows	O
:	O
e	O
c	O
=	O
M	O
axP	O
ool	O
(	O
Conv	O
(	O
w	O
)	O
)	O
where	O
the	O
CNN	O
is	O
similar	O
to	O
the	O
one	O
in	O
(	O
Chiu	O
and	O
Nichols	O
,	O
2016	O
)	O
,	O
but	O
we	O
use	O
only	O
characters	O
as	O
the	O
inputs	O
to	O
CNN	O
,	O
without	O
character	O
type	O
features	O
.	O
Word	O
Level	O
Representation	O
Word	O
embedding	O
is	O
a	O
standard	O
component	O
of	O
most	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
NLP	O
architectures	O
.	O
Due	O
to	O
their	O
ability	O
to	O
capture	O
syntactic	O
and	O
semantic	O
information	O
of	O
words	O
from	O
large	O
scale	O
unlabeled	O
texts	O
,	O
we	O
pre	O
-	O
train	O
the	O
word	B-TaskName
embeddings	I-TaskName
from	O
the	O
given	O
training	O
dataset	O
by	O
word2vec	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
toolkit	O
.	O
For	O
low	O
-	O
resource	O
languages	O
without	O
available	O
training	O
data	O
,	O
we	O
sample	O
the	O
training	O
dataset	O
from	O
similar	O
languages	O
to	O
generate	O
a	O
mixed	O
dataset	O
.	O

To	O
enrich	O
morphological	O
information	O
,	O
we	O
also	O
incorporate	O
UPOS	O
tag	O
embeddings	O
into	O
the	O
representation	O
.	O
Therefore	O
,	O
we	O
jointly	O
predict	O
the	O
UPOS	O
tag	O
in	O
our	O
system	O
.	O
The	O
architecture	O
for	O
the	O
POS	O
tagger	O
in	O
our	O
model	O
is	O
almost	O
identical	O
to	O
that	O
of	O
the	O
parser	O
.	O
The	O
tagger	O
uses	O
a	O
BiLSTM	B-MethodName
over	O
the	O
concatenation	O
of	O
word	B-TaskName
embeddings	I-TaskName
and	O
character	O
embeddings	O
:	O
s	O
pos	O
i	O
=	O
BiLST	O
M	O
pos	O
(	O
e	O
w	O
i	O
e	O
c	O
i	O
)	O
Then	O
we	O
calculate	O
the	O
probability	O
of	O
tag	O
for	O
each	O
type	O
using	O
affine	O
classifiers	O
as	O
follows	O
:	O
h	O
pos	O
i	O
=	O
M	O
LP	O
pos	O
(	O
s	O
pos	O
i	O
)	O
r	O
pos	O
i	O
=	O
W	O
pos	O
h	O
pos	O
i	O
+	O
b	O
pos	O
y	O
pos	O
i	O
=	O
arg	O
max	O
(	O
r	O
i	O
)	O
The	O
tag	O
classifier	O
is	O
trained	O
jointly	O
using	O
crossentropy	O
losses	O
that	O
are	O
summed	O
together	O
with	O
the	O
dependency	O
parser	O
loss	B-MetricName
during	O
optimization	O
.	O

In	O
order	O
to	O
integrate	O
contextual	O
information	O
,	O
we	O
concatenate	O
the	O
character	O
embedding	O
e	O
c	O
,	O
pre	O
-	O
trained	O
word	O
embedding	O
e	O
w	O
and	O
UPOS	O
tag	O
embedding	O
e	O
pos	O
,	O
then	O
feed	O
them	O
into	O
the	O
BiLSTM	B-MethodName
.	O
We	O
take	O
the	O
bidirectional	O
vectors	O
at	O
the	O
final	O
layer	O
as	O
the	O
contextsensitive	O
representation	O
:	O
−	O
s	O
i	O
=	O
LST	O
M	O
f	O
orward	O
(	O
e	O
w	O
i	O
e	O
c	O
i	O
e	O
pos	O
i	O
)	O
−	O
s	O
i	O
=	O
LST	O
M	O
backward	O
(	O
e	O
w	O
i	O
e	O
c	O
i	O
e	O
pos	O
i	O
)	O
s	O
i	O
=	O
−	O
s	O
i	O
−	O
s	O
i	O
Notably	O
,	O
we	O
use	O
the	O
UPOS	O
tag	O
from	O
the	O
output	O
of	O
our	O
POS	O
tagging	O
model	O
.	O

The	O
training	O
objective	O
of	O
pur	O
system	O
is	O
to	O
learn	O
the	O
probability	O
of	O
UPOS	O
tags	O
P	O
θ	B-HyperparameterName
pos	O
(	O
y	O
pos	O
|	O
x	O
)	O
and	O
the	O
dependency	O
trees	O
P	O
θ	B-HyperparameterName
dep	O
(	O
y	O
dep	O
|	O
x	O
,	O
y	O
pos	O
)	O
.	O
Given	O
a	O
sentence	O
x	O
,	O
the	O
probabilities	O
are	O
factorized	O
as	O
:	O
P	O
θ	B-HyperparameterName
pos	O
(	O
y	O
pos	O
|	O
x	O
)	O
=	O
k	O
i=1	O
P	O
θ	B-HyperparameterName
pos	O
(	O
p	O
i	O
|	O
x	O
)	O
y	O
pos	O
=	O
arg	O
max	O
ypos	O
Ypos	O
(	O
P	O
θ	B-HyperparameterName
pos	O
(	O
y	O
pos	O
|	O
x	O
)	O
)	O
P	O
θ	B-HyperparameterName
dep	O
(	O
y	O
dep	O
|	O
x	O
,	O
y	O
pos	O
)	O
=	O
k	O
i=1	O
P	O
θ	B-HyperparameterName
dep	O
(	O
p	O
i	O
|	O
p	O
<	O
i	O
,	O
x	O
,	O
y	O
pos	O
)	O
=	O
k	O
i=1	O
l	O
i	O
j=1	O
P	O
θ	B-HyperparameterName
dep	O
(	O
c	O
i	O
,	O
j	O
|	O
c	O
i	O
,	O
<	O
j	O
,	O
p	O
<	O
i	O
,	O
x	O
,	O
y	O
pos	O
)	O
where	O
θ	B-HyperparameterName
pos	O
and	O
θ	B-HyperparameterName
dep	O
represent	O
the	O
model	O
parameters	O
respectively	O
.	O
p	O
<	O
i	O
denotes	O
the	O
preceding	O
dependency	O
paths	O
that	O
have	O
already	O
been	O
generated	O
.	O
c	O
i	O
,	O
j	O
represents	O
the	O
j	O
th	O
word	O
in	O
p	O
i	O
and	O
c	O
i	O
,	O
j	O
denotes	O
all	O
the	O
proceeding	O
words	O
on	O
the	O
path	O
p	O
i	O
.	O
Therefore	O
,	O
the	O
whole	O
loss	B-MetricName
is	O
the	O
sum	O
of	O
three	O
objectives	O
:	O
Loss	O
=	O
Loss	O
pos	O
+	O
Loss	O
arc	O
+	O
Loss	O
label	O
where	O
the	O
Loss	O
pos	O
,	O
Loss	O
arc	O
and	O
Loss	O
label	O
are	O
the	O
conditional	O
likehood	O
of	O
their	O
corresponding	O
target	O
,	O
using	O
the	O
cross	O
-	O
entropy	O
loss	B-MetricName
.	O
Specifically	O
,	O
we	O
train	O
a	O
dependency	O
label	O
classifier	O
following	O
,	O
which	O
takes	O
the	O
dependency	O
head	O
-	O
child	O
pair	O
as	O
input	O
features	O
.	O

In	O
this	O
paper	O
,	O
we	O
describe	O
our	O
system	O
in	O
the	O
CoNLL	O
2018	O
shared	O
task	O
on	O
UD	B-DatasetName
parsing	O
.	O
Our	O
system	O
uses	O
a	O
transition	O
-	O
based	O
neural	O
network	O
architecture	O
for	O
dependency	B-TaskName
parsing	I-TaskName
,	O
which	O
predicts	O
the	O
UPOS	O
tag	O
and	O
dependencies	O
jointly	O
.	O
Combining	O
pointer	O
networks	O
with	O
an	O
internal	O
stack	O
to	O
track	O
the	O
status	O
of	O
the	O
top	O
-	O
down	O
,	O
depth	O
-	O
first	O
search	O
in	O
the	O
parsing	O
decoding	O
procedure	O
,	O
the	O
STACKPTR	O
parser	O
is	O
able	O
to	O
capture	O
information	O
from	O
the	O
whole	O
sentence	O
and	O
all	O
the	O
previously	O
derived	O
subtrees	O
,	O
removing	O
the	O
left	O
-	O
to	O
-	O
right	O
restriction	O
in	O
classical	O
transition	O
-	O
based	O
parsers	O
,	O
while	O
maintaining	O

Get	O
To	O
The	O
Point	O
:	O
Summarization	B-TaskName
with	O
Pointer	O
-	O
Generator	O
Networks	O

Neural	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
have	O
provided	O
a	O
viable	O
new	O
approach	O
for	O
abstractive	B-TaskName
text	I-TaskName
summarization	I-TaskName
(	O
meaning	O
they	O
are	O
not	O
restricted	O
to	O
simply	O
selecting	O
and	O
rearranging	O
passages	O
from	O
the	O
original	O
text	O
)	O
.	O
However	O
,	O
these	O
models	O
have	O
two	O
shortcomings	O
:	O
they	O
are	O
liable	O
to	O
reproduce	O
factual	O
details	O
inaccurately	O
,	O
and	O
they	O
tend	O
to	O
repeat	O
themselves	O
.	O
In	O
this	O
work	O
we	O
propose	O
a	O
novel	O
architecture	O
that	O
augments	O
the	O
standard	O
sequence	O
-	O
to	O
-	O
sequence	O
attentional	O
model	O
in	O
two	O
orthogonal	O
ways	O
.	O
First	O
,	O
we	O
use	O
a	O
hybrid	O
pointer	O
-	O
generator	O
network	O
that	O
can	O
copy	O
words	O
from	O
the	O
source	O
text	O
via	O
pointing	O
,	O
which	O
aids	O
accurate	O
reproduction	O
of	O
information	O
,	O
while	O
retaining	O
the	O
ability	O
to	O
produce	O
novel	O
words	O
through	O
the	O
generator	O
.	O
Second	O
,	O
we	O
use	O
coverage	O
to	O
keep	O
track	O
of	O
what	O
has	O
been	O
summarized	O
,	O
which	O
discourages	O
repetition	O
.	O
We	O
apply	O
our	O
model	O
to	O
the	O
CNN	B-DatasetName
/	I-DatasetName
Daily	I-DatasetName
Mail	I-DatasetName
summarization	B-TaskName
task	O
,	O
outperforming	O
the	O
current	O
abstractive	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
by	O
at	O
least	O
2	O
ROUGE	O
points	O
.	O

We	O
use	O
the	O
CNN	B-DatasetName
/	I-DatasetName
Daily	I-DatasetName
Mail	I-DatasetName
dataset	O
(	O
Hermann	O
et	O
al	O
,	O
2015	O
;	O
,	O
which	O
contains	O
online	O
news	O
articles	O
(	O
781	O
tokens	O
on	O
average	O
)	O
paired	O
with	O
multi	O
-	O
sentence	O
summaries	O
(	O
3.75	O
sentences	O
or	O
56	O
tokens	O
on	O
average	O
)	O
.	O
We	O
used	O
scripts	O
supplied	O
by	O
to	O
obtain	O
the	O
same	O
version	O
of	O
the	O
the	O
data	O
,	O
which	O
has	O
287	O
,	O
226	O
training	O
pairs	O
,	O
13	O
,	O
368	O
validation	O
pairs	O
and	O
11	O
,	O
490	O
test	O
pairs	O
.	O
Both	O
the	O
dataset	O
's	O
published	O
results	O
(	O
Nallapati	O
et	O
al	O
,	O
,	O
2017	O
use	O
the	O
anonymized	O
version	O
of	O
the	O
data	O
,	O
which	O
has	O
been	O
pre	O
-	O
processed	O
to	O
replace	O
each	O
named	O
entity	O
,	O
e.g.	O
,	O
The	O
United	O
Nations	B-DatasetName
,	O
with	O
its	O
own	O
unique	O
identifier	O
for	O
the	O
example	O
pair	O
,	O
e.g.	O
,	O
@entity5	O
.	O
By	O
contrast	O
,	O
we	O
operate	O
directly	O
on	O
the	O
original	O
text	O
(	O
or	O
non	O
-	O
anonymized	O
version	O
of	O
the	O
data	O
)	O
,	O
2	O
which	O
we	O
believe	O
is	O
the	O
favorable	O
problem	O
to	O
solve	O
because	O
it	O
requires	O
no	O
pre	O
-	O
processing	O
.	O

We	O
find	O
that	O
both	O
our	O
baseline	O
models	O
perform	O
poorly	O
with	O
respect	O
to	O
ROUGE	O
and	O
METEOR	B-DatasetName
,	O
and	O
in	O
fact	O
the	O
larger	O
vocabulary	O
size	O
(	O
150k	O
)	O
does	O
not	O
seem	O
to	O
help	O
.	O
Even	O
the	O
better	O
-	O
performing	O
baseline	O
(	O
with	O
50k	O
vocabulary	O
)	O
produces	O
summaries	O
with	O
several	O
common	O
problems	O
.	O
Factual	O
details	O
are	O
frequently	O
reproduced	O
incorrectly	O
,	O
often	O
replacing	O
an	O
uncommon	O
(	O
but	O
in	O
-	O
vocabulary	O
)	O
word	O
with	O
a	O
morecommon	O
alternative	O
.	O
For	O
example	O
in	O
Figure	O
1	O
,	O
the	O
baseline	O
model	O
appears	O
to	O
struggle	O
with	O
the	O
rare	O
word	O
thwart	O
,	O
producing	O
destabilize	O
instead	O
,	O
which	O
leads	O
to	O
the	O
fabricated	O
phrase	O
destabilize	O
nigeria	O
's	O
economy	O
.	O
Even	O
more	O
catastrophically	O
,	O
the	O
summaries	O
sometimes	O
devolve	O
into	O
repetitive	O
nonsense	O
,	O
such	O
as	O
the	O
third	O
sentence	O
produced	O
by	O
the	O
baseline	O
model	O
in	O
Figure	O
1	O
.	O
In	O
addition	O
,	O
the	O
baseline	O
model	O
ca	O
n't	O
reproduce	O
out	O
-	O
of	O
-	O
vocabulary	O
words	O
(	O
such	O
as	O
muhammadu	O
buhari	O
in	O
Figure	O
1	O
)	O
.	O
Further	O
examples	O
of	O
all	O
these	O
problems	O
are	O
provided	O
in	O
the	O
supplementary	B-DatasetName
material	I-DatasetName
.	O
Our	O
pointer	O
-	O
generator	O
model	O
achieves	O
much	O
better	O
ROUGE	O
and	O
METEOR	B-DatasetName
scores	O
than	O
the	O
baseline	O
,	O
despite	O
many	O
fewer	O
training	O
epochs	O
.	O
The	O
difference	O
in	O
the	O
summaries	O
is	O
also	O
marked	O
:	O
outof	O
-	O
vocabulary	O
words	O
are	O
handled	O
easily	O
,	O
factual	O
details	O
are	O
almost	O
always	O
copied	O
correctly	O
,	O
and	O
there	O
are	O
no	O
fabrications	O
(	O
see	O
Figure	O
1	O
)	O
.	O
However	O
,	O
repetition	O
is	O
still	O
very	O
common	O
.	O
Our	O
pointer	O
-	O
generator	O
model	O
with	O
coverage	O
improves	O
the	O
ROUGE	O
and	O
METEOR	B-DatasetName
scores	O
further	O
,	O
convincingly	O
surpassing	O
the	O
best	O
abstractive	O
model	O
Article	O
:	O
smugglers	O
lure	O
arab	O
and	O
african	O
migrants	O
by	O
offering	O
discounts	O
to	O
get	O
onto	O
overcrowded	O
ships	O
if	O
people	O
bring	O
more	O
potential	O
passengers	O
,	O
a	O
cnn	O
investigation	O
has	O
revealed	O
.	O
(	O
...	O
)	O
Summary	O
:	O
cnn	O
investigation	O
uncovers	O
the	O
business	O
inside	O
a	O
human	O
smuggling	O
ring	O
.	O
Article	O
:	O
eyewitness	O
video	O
showing	O
white	O
north	O
charleston	O
police	O
officer	O
michael	O
slager	O
shooting	O
to	O
death	O
an	O
unarmed	O
black	O
man	O
has	O
exposed	O
discrepancies	O
in	O
the	O
reports	O
of	O
the	O
first	O
officers	O
on	O
the	O
scene	O
.	O
(	O
...	O
)	O
Summary	O
:	O
more	O
questions	O
than	O
answers	O
emerge	O
in	O
controversial	O
s.c	O
.	O
police	O
shooting	O
.	O
of	O
by	O
several	O
ROUGE	O
points	O
.	O
Despite	O
the	O
brevity	O
of	O
the	O
coverage	O
training	O
phase	O
(	O
about	O
1	O
%	O
of	O
the	O
total	O
training	O
time	O
)	O
,	O
the	O
repetition	O
problem	O
is	O
almost	O
completely	O
eliminated	O
,	O
which	O
can	O
be	O
seen	O
both	O
qualitatively	O
(	O
Figure	O
1	O
)	O
and	O
quantitatively	O
(	O
Figure	O
4	O
)	O
.	O
However	O
,	O
our	O
best	O
model	O
does	O
not	O
quite	O
surpass	O
the	O
ROUGE	O
scores	O
of	O
the	O
lead	O
-	O
3	O
baseline	O
,	O
nor	O
the	O
current	O
best	O
extractive	O
model	O
(	O
Nallapati	O
et	O
al	O
,	O
2017	O
)	O
.	O
We	O
discuss	O
this	O
issue	O
in	O
section	O
7.1	O
.	O

It	O
is	O
clear	O
from	O
Table	O
1	O
that	O
extractive	O
systems	O
tend	O
to	O
achieve	O
higher	O
ROUGE	O
scores	O
than	O
abstractive	O
,	O
and	O
that	O
the	O
extractive	O
lead	O
-	O
3	O
baseline	O
is	O
extremely	O
strong	O
(	O
even	O
the	O
best	O
extractive	O
system	O
beats	O
it	O
by	O
only	O
a	O
small	O
margin	O
)	O
.	O
We	O
offer	O
two	O
possible	O
explanations	O
for	O
these	O
observations	O
.	O
Firstly	O
,	O
news	O
articles	O
tend	O
to	O
be	O
structured	O
with	O
the	O
most	O
important	O
information	O
at	O
the	O
start	O
;	O
this	O
partially	O
explains	O
the	O
strength	O
of	O
the	O
lead	O
-	O
3	O
baseline	O
.	O
Indeed	O
,	O
we	O
found	O
that	O
using	O
only	O
the	O
first	O
400	O
tokens	O
(	O
about	O
20	O
sentences	O
)	O
of	O
the	O
article	O
yielded	O
significantly	O
higher	O
ROUGE	O
scores	O
than	O
using	O
the	O
first	O
800	O
tokens	O
.	O
Secondly	O
,	O
the	O
nature	O
of	O
the	O
task	O
and	O
the	O
ROUGE	O
metric	O
make	O
extractive	O
approaches	O
and	O
the	O
lead	O
-	O
3	O
baseline	O
difficult	O
to	O
beat	O
.	O
The	O
choice	O
of	O
content	O
for	O
the	O
reference	O
summaries	O
is	O
quite	O
subjective	O
-	O
sometimes	O
the	O
sentences	O
form	O
a	O
self	O
-	O
contained	O
summary	O
;	O
other	O
times	O
they	O
simply	O
showcase	O
a	O
few	O
interesting	O
details	O
from	O
the	O
article	O
.	O
Given	O
that	O
the	O
articles	O
contain	O
39	O
sentences	O
on	O
average	O
,	O
there	O
are	O
many	O
equally	O
valid	O
ways	O
to	O
choose	O
3	O
or	O
4	O
highlights	O
in	O
this	O
style	O
.	O
Abstraction	O
introduces	O
even	O
more	O
options	O
(	O
choice	O
of	O
phrasing	O
)	O
,	O
further	O
decreas	O
-	O
ing	O
the	O
likelihood	O
of	O
matching	O
the	O
reference	O
summary	O
.	O
For	O
example	O
,	O
smugglers	O
profit	O
from	O
desperate	O
migrants	O
is	O
a	O
valid	O
alternative	O
abstractive	O
summary	O
for	O
the	O
first	O
example	O
in	O
Figure	O
5	O
,	O
but	O
it	O
scores	O
0	B-DatasetName
ROUGE	O
with	O
respect	O
to	O
the	O
reference	O
summary	O
.	O
This	O
inflexibility	O
of	O
ROUGE	O
is	O
exacerbated	O
by	O
only	O
having	O
one	O
reference	O
summary	O
,	O
which	O
has	O
been	O
shown	O
to	O
lower	O
ROUGE	O
's	O
reliability	O
compared	O
to	O
multiple	O
reference	O
summaries	O
(	O
Lin	O
,	O
2004a	O
)	O
.	O
Due	O
to	O
the	O
subjectivity	O
of	O
the	O
task	O
and	O
thus	O
the	O
diversity	O
of	O
valid	O
summaries	O
,	O
it	O
seems	O
that	O
ROUGE	O
rewards	O
safe	O
strategies	O
such	O
as	O
selecting	O
the	O
first	O
-	O
appearing	O
content	O
,	O
or	O
preserving	O
original	O
phrasing	O
.	O
While	O
the	O
reference	O
summaries	O
do	O
sometimes	O
deviate	O
from	O
these	O
techniques	O
,	O
those	O
deviations	O
are	O
unpredictable	O
enough	O
that	O
the	O
safer	O
strategy	O
obtains	O
higher	O
ROUGE	O
scores	O
on	O
average	O
.	O
This	O
may	O
explain	O
why	O
extractive	O
systems	O
tend	O
to	O
obtain	O
higher	O
ROUGE	O
scores	O
than	O
abstractive	O
,	O
and	O
even	O
extractive	O
systems	O
do	O
not	O
significantly	O
exceed	O
the	O
lead	O
-	O
3	O
baseline	O
.	O
To	O
explore	O
this	O
issue	O
further	O
,	O
we	O
evaluated	O
our	O
systems	O
with	O
the	O
METEOR	B-DatasetName
metric	O
,	O
which	O
rewards	O
not	O
only	O
exact	O
word	O
matches	O
,	O
but	O
also	O
matching	O
stems	O
,	O
synonyms	O
and	O
paraphrases	O
(	O
from	O
a	O
predefined	O
list	O
)	O
.	O
We	O
observe	O
that	O
all	O
our	O
models	O
receive	O
over	O
1	O
METEOR	B-DatasetName
point	O
boost	O
by	O
the	O
inclusion	O
of	O
stem	O
,	O
synonym	O
and	O
paraphrase	O
matching	O
,	O
indicating	O
that	O
they	O
may	O
be	O
performing	O
some	O
abstraction	O
.	O
However	O
,	O
we	O
again	O
observe	O
that	O
the	O
lead	O
-	O
3	O
baseline	O
is	O
not	O
surpassed	O
by	O
our	O
models	O
.	O
It	O
may	O
be	O
that	O
news	O
article	O
style	O
makes	O
the	O
lead	O
-	O
3	O
baseline	O
very	O
strong	O
with	O
respect	O
to	O
any	O
metric	O
.	O
We	O
believe	O
that	O
investigating	O
this	O
issue	O
further	O
is	O
an	O
important	O
direction	O
for	O
future	O
work	O
.	O

We	O
thank	O
the	O
ACL	O
reviewers	O
for	O
their	O
helpful	O
comments	O
.	O
This	O
work	O
was	O
begun	O
while	O
the	O
first	O
author	O
was	O
an	O
intern	O
at	O
Google	B-DatasetName
Brain	O
and	O
continued	O
at	O
Stanford	O
.	O
Stanford	O
University	O
gratefully	O
acknowledges	O
the	O
support	O
of	O
the	O
DARPA	B-DatasetName
DEFT	O
Program	O
AFRL	O
contract	O
no	O
.	O
FA8750	O
-	O
13	O
-	O
2	O
-	O
0040	O
.	O
Any	O
opinions	O
in	O
this	O
material	O
are	O
those	O
of	O
the	O
authors	O
alone	O
.	O

Re	O
-	O
evaluating	O
Evaluation	O
in	O
Text	B-TaskName
Summarization	I-TaskName

Automated	O
evaluation	O
metrics	O
as	O
a	O
stand	O
-	O
in	O
for	O
manual	O
evaluation	O
are	O
an	O
essential	O
part	O
of	O
the	O
development	O
of	O
text	O
-	O
generation	O
tasks	O
such	O
as	O
text	B-TaskName
summarization	I-TaskName
.	O
However	O
,	O
while	O
the	O
field	O
has	O
progressed	O
,	O
our	O
standard	O
metrics	O
have	O
not	O
-	O
for	O
nearly	O
20	O
years	O
ROUGE	O
has	O
been	O
the	O
standard	O
evaluation	O
in	O
most	O
summarization	B-TaskName
papers	O
.	O
In	O
this	O
paper	O
,	O
we	O
make	O
an	O
attempt	O
to	O
re	O
-	O
evaluate	O
the	O
evaluation	O
method	O
for	O
text	B-TaskName
summarization	I-TaskName
:	O
assessing	O
the	O
reliability	O
of	O
automatic	O
metrics	O
using	O
top	O
-	O
scoring	O
system	O
outputs	O
,	O
both	O
abstractive	O
and	O
extractive	O
,	O
on	O
recently	O
popular	O
datasets	O
for	O
both	O
systemlevel	O
and	O
summary	O
-	O
level	O
evaluation	O
settings	O
.	O
We	O
find	O
that	O
conclusions	O
about	O
evaluation	O
metrics	O
on	O
older	O
datasets	O
do	O
not	O
necessarily	O
hold	O
on	O
modern	O
datasets	O
and	O
systems	O
.	O
We	O
release	O
a	O
dataset	O
of	O
human	O
judgments	O
that	O
are	O
collected	O
from	O
25	O
top	O
-	O
scoring	O
neural	O
summarization	B-TaskName
systems	O
(	O
14	O
abstractive	O
and	O
11	O
extractive	O
)	O
:	O

In	O
this	O
section	O
we	O
describe	O
the	O
datasets	O
,	O
systems	O
,	O
metrics	O
,	O
and	O
meta	O
evaluation	O
methods	O
used	O
below	O
.	O
-	O
2008	O
,	O
2009	O
(	O
Dang	O
and	O
Owczarzak	O
,	O
2008	O
,	O
2009	O
are	O
multi	O
-	O
document	O
,	O
multi	O
-	O
reference	O
summarization	B-TaskName
datasets	O
.	O
Human	O
judgments	O
are	O
available	O
on	O
for	O
the	O
system	O
summaries	O
submitted	O
during	O
the	O
TAC	O
-	O
2008	O
,	O
TAC	O
-	O
2009	O
shared	O
tasks	O
.	O
CNN	O
/	O
DailyMail	O
(	O
CNNDM	O
)	O
(	O
Hermann	O
et	O
al	O
,	O
2015	O
;	O
Nallapati	O
et	O
al	O
,	O
2016	O
)	O
is	O
a	O
commonly	O
used	O
summarization	B-TaskName
dataset	O
that	O
contains	O
news	O
articles	O
and	O
associated	O
highlights	O
as	O
summaries	O
.	O
We	O
use	O
the	O
version	O
without	O
entities	O
anonymized	O
.	O

We	O
use	O
the	O
following	O
representative	O
top	O
-	O
scoring	O
systems	O
that	O
either	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
(	O
SOTA	O
)	O
results	O
or	O
competitive	O
performance	O
,	O
for	O
which	O
we	O
could	O
gather	O
the	O
outputs	O
on	O
the	O
CNNDM	O
dataset	O
.	O
Extractive	B-TaskName
summarization	I-TaskName
systems	O
.	O
We	O
use	O
CNN	O
-	O
LSTM	B-MethodName
-	O
BiClassifier	O
(	O
CLSTM	O
-	O
SL	O
;	O
Kedzie	O
et	O
al	O
(	O
2018	O
)	O
)	O
,	O
Latent	O
(	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
,	O
Ban	O
-	O
ditSum	O
(	O
Dong	O
et	O
al	O
,	O
2018	O
)	O
,	O
REFRESH	O
(	O
Narayan	O
et	O
al	O
,	O
2018	O
)	O
,	O
NeuSum	O
,	O
HIBERT	O
(	O
Zhang	O
et	O
al	O
,	O
2019b	O
)	O
,	O
Bert	O
-	O
Sum	O
-	O
Ext	O
(	O
Liu	O
and	O
Lapata	O
,	O
2019a	O
)	O
,	O
CNN	O
-	O
Transformer	B-MethodName
-	O
BiClassifier	O
(	O
CTrans	O
-	O
SL	O
;	O
Zhong	O
et	O
al	O
(	O
2019	O
)	O
)	O
,	O
CNN	O
-	O
Transformer	B-MethodName
-	O
Pointer	O
(	O
CTrans	O
-	O
PN	O
;	O
Zhong	O
et	O
al	O
(	O
2019	O
)	O
)	O
,	O
HeterGraph	O
(	O
Wang	O
et	O
al	O
,	O
2020	O
)	O
and	O
MatchSum	O
(	O
Zhong	O
et	O
al	O
,	O
2020	O
)	O
as	O
representatives	O
of	O
extractive	O
systems	O
,	O
totaling	O
11	O
extractive	O
system	O
outputs	O
for	O
each	O
document	O
in	O
the	O
CNNDM	O
test	O
set	O
.	O
Abstractive	O
summarization	B-TaskName
systems	O
.	O
We	O
use	O
pointer	O
-	O
generator+coverage	O
(	O
See	O
et	O
al	O
,	O
2017	O
)	O
,	O
fastAbsRL	O
(	O
Chen	O
and	O
Bansal	O
,	O
2018	O
)	O
,	O
fastAbsRLrank	O
(	O
Chen	O
and	O
Bansal	O
,	O
2018	O
)	O
,	O
Bottom	O
-	O
up	O
(	O
Gehrmann	O
et	O
al	O
,	O
2018	O
)	O
,	O
T5	B-MethodName
(	O
Raffel	O
et	O
al	O
,	O
2019	O
)	O
,	O
Unilm	O
-	O
v1	O
(	O
Dong	O
et	O
al	O
,	O
2019	O
)	O
,	O
Unilm	O
-	O
v2	O
(	O
Dong	O
et	O
al	O
,	O
2019	O
)	O
,	O
twoStageRL	O
(	O
Zhang	O
et	O
al	O
,	O
2019a	O
)	O
,	O
pre	O
-	O
SummAbs	O
(	O
Liu	O
and	O
Lapata	O
,	O
2019b	O
)	O
,	O
preSummAbsext	O
(	O
Liu	O
and	O
Lapata	O
,	O
2019b	O
)	O
BART	B-MethodName
(	O
Lewis	O
et	O
al	O
,	O
2019	O
)	O
and	O
Semsim	O
(	O
Yoon	O
et	O
al	O
,	O
2020	O
)	O
as	O
abstractive	O
systems	O
.	O
In	O
total	O
,	O
we	O
use	O
14	O
abstractive	O
system	O
outputs	O
for	O
each	O
document	O
in	O
the	O
CNNDM	O
test	O
set	O
.	O

We	O
follow	O
a	O
3	O
-	O
step	O
process	O
to	O
collect	O
human	O
judgments	O
:	O
(	O
1	O
)	O
we	O
collect	O
system	O
-	O
generated	O
summaries	O
on	O
the	O
most	O
-	O
commonly	O
used	O
summarization	B-TaskName
dataset	O
,	O
CNNDM	O
;	O
(	O
2	O
)	O
we	O
select	O
representative	O
test	O
samples	O
from	O
CNNDM	O
and	O
(	O
3	O
)	O
we	O
manually	O
evaluate	O
system	O
-	O
generated	O
summaries	O
of	O
the	O
aboveselected	O
test	O
samples	O
.	O

Motivated	O
by	O
the	O
central	O
research	O
question	O
:	O
"	O
does	O
the	O
rapid	O
progress	O
of	O
model	O
development	O
in	O
summarization	B-TaskName
models	O
require	O
us	O
to	O
re	O
-	O
evaluate	O
the	O
evaluation	O
process	O
used	O
for	O
text	B-TaskName
summarization	I-TaskName
?	O
"	O
We	O
use	O
the	O
collected	O
human	O
judgments	O
to	O
meta	O
-	O
evaluate	O
current	O
metrics	O
from	O
four	O
diverse	O
viewpoints	O
,	O
measuring	O
the	O
ability	O
of	O
metrics	O
to	O
:	O
(	O
1	O
)	O
evaluate	O
all	O
systems	O
;	O
(	O
2	O
)	O
evaluate	O
top	O
-	O
k	O
strongest	O
systems	O
;	O
(	O
3	O
)	O
compare	O
two	O
systems	O
;	O
(	O
4	O
)	O
evaluate	O
individual	O
summaries	O
.	O
We	O
find	O
that	O
many	O
previously	O
attested	O
properties	O
of	O
metrics	O
observed	O
on	O
TAC	O
exhibit	O
different	O
trends	O
on	O
the	O
new	O
CNNDM	O
dataset	O
.	O

This	O
work	O
is	O
connected	O
to	O
the	O
following	O
threads	O
of	O
topics	O
in	O
text	B-TaskName
summarization	I-TaskName
.	O
Human	O
Judgment	O
Collection	O
Despite	O
many	O
approaches	O
to	O
the	O
acquisition	O
of	O
human	O
judgment	O
(	O
Chaganty	O
et	O
al	O
,	O
2018	O
;	O
Nenkova	O
and	O
Passonneau	O
,	O
2004	O
;	O
Shapira	O
et	O
al	O
,	O
2019	O
;	O
Fan	O
et	O
al	O
,	O
2018	O
)	O
,	O
Pyramid	O
(	O
Nenkova	O
and	O
Passonneau	O
,	O
2004	O
)	O
has	O
been	O
a	O
mainstream	O
method	O
to	O
meta	O
-	O
evaluate	O
various	O
automatic	O
metrics	O
.	O
Specifically	O
,	O
Pyramid	O
provides	O
a	O
robust	O
technique	O
for	O
evaluating	O
content	O
selection	O
by	O
exhaustively	O
obtaining	O
a	O
set	O
of	O
Semantic	O
Content	O
Units	O
(	O
SCUs	O
)	O
from	O
a	O
set	O
of	O
references	O
,	O
and	O
then	O
scoring	O
system	O
summaries	O
on	O
how	O
many	O
SCUs	O
can	O
be	O
inferred	O
from	O
them	O
.	O
Recently	O
,	O
Shapira	O
et	O
al	O
(	O
2019	O
)	O
proposed	O
a	O
lightweight	O
and	O
crowdsourceable	O
version	O
of	O
the	O
original	O
Pyramid	O
,	O
and	O
demonstrated	O
it	O
on	O
the	O
DUC	B-DatasetName
2005	I-DatasetName
(	O
Dang	O
,	O
2005	O
)	O
and	O
2006	O
(	O
Dang	O
,	O
2006	O
)	O
multi	B-TaskName
-	I-TaskName
document	I-TaskName
summarization	I-TaskName
datasets	O
.	O
In	O
this	O
paper	O
,	O
our	O
human	O
evaluation	O
methodology	O
is	O
based	O
on	O
the	O
Pyramid	O
(	O
Nenkova	O
and	O
Passonneau	O
,	O
2004	O
)	O
and	O
LitePyramids	O
(	O
Shapira	O
et	O
al	O
,	O
2019	O
)	O
techniques	O
.	O
Chaganty	O
et	O
al	O
(	O
2018	O
)	O
also	O
obtain	O
human	O
evaluations	O
on	O
system	O
summaries	O
on	O
the	O
CNNDM	O
dataset	O
,	O
but	O
with	O
a	O
focus	O
on	O
language	O
quality	O
of	O
summaries	O
.	O
In	O
comparison	O
,	O
our	O
work	O
is	O
focused	O
on	O
evaluating	O
content	O
selection	O
.	O
Our	O
work	O
also	O
covers	O
more	O
systems	O
than	O
their	O
study	O
(	O
11	O
extractive	O
+	O
14	O
abstractive	O
vs.	O
4	O
abstractive	O
)	O
.	O

Our	O
work	O
not	O
only	O
diagnoses	O
the	O
limitations	O
of	O
current	O
metrics	O
but	O
also	O
highlights	O
the	O
importance	O
of	O
upgrading	O
the	O
existing	O
meta	O
-	O
evaluation	O
testbed	O
,	O
keeping	O
it	O
up	O
-	O
to	O
-	O
date	O
with	O
the	O
rapid	O
development	O
of	O
systems	O
and	O
datasets	O
.	O
In	O
closing	O
,	O
we	O
highlight	O
some	O
potential	O
future	O
directions	O
:	O
(	O
1	O
)	O
The	O
choice	O
of	O
metrics	O
depends	O
not	O
only	O
on	O
different	O
tasks	O
(	O
e.g	O
,	O
summarization	B-TaskName
,	O
translation	O
)	O
but	O
also	O
on	O
different	O
datasets	O
(	O
e.g.	O
,	O
TAC	O
,	O
CNNDM	O
)	O
and	O
application	O
scenarios	O
(	O
e.g	O
,	O
system	O
-	O
level	O
,	O
summary	O
-	O
level	O
)	O
.	O
Future	O
works	O
on	O
meta	O
-	O
evaluation	O
should	O
investigate	O
the	O
effect	O
of	O
these	O
settings	O
on	O
the	O
performance	O
of	O
metrics	O
.	O
(	O
2	O
)	O
Metrics	O
easily	O
overfit	O
on	O
limited	O
datasets	O
.	O
Multidataset	O
meta	O
-	O
evaluation	O
can	O
help	O
us	O
better	O
understand	O
each	O
metric	O
's	O
peculiarity	O
,	O
therefore	O
achieving	O
a	O
better	O
choice	O
of	O
metrics	O
under	O
diverse	O
scenarios	O
.	O
(	O
3	O
)	O
Our	O
collected	O
human	O
judgments	O
can	O
be	O
used	O
as	O
supervision	O
to	O
instantiate	O
the	O
most	O
recentlyproposed	O
pretrain	O
-	O
then	O
-	O
finetune	O
framework	O
(	O
originally	O
for	O
machine	B-TaskName
translation	I-TaskName
)	O
(	O
Sellam	O
et	O
al	O
,	O
2020	O
)	O
,	O
learning	O
a	O
robust	O
metric	O
for	O
text	B-TaskName
summarization	I-TaskName
.	O

Building	O
a	O
De	B-TaskName
-	I-TaskName
identification	I-TaskName
System	O
for	O
Real	O
Swedish	O
Clinical	O
Text	O
Using	O
Pseudonymised	O
Clinical	O
Text	O

This	O
article	O
presents	O
experiments	O
with	O
pseudonymised	O
Swedish	O
clinical	O
text	O
used	O
as	O
training	O
data	O
to	O
de	O
-	O
identify	O
real	O
clinical	O
text	O
with	O
the	O
future	O
aim	O
to	O
transfer	O
non	O
-	O
sensitive	O
training	O
data	O
to	O
other	O
hospitals	O
.	O
Conditional	O
Random	O
Fields	O
(	O
CFR	O
)	O
and	O
Long	B-MethodName
Short	I-MethodName
-	I-MethodName
Term	I-MethodName
Memory	I-MethodName
(	O
LSTM	B-MethodName
)	O
machine	O
learning	O
algorithms	O
were	O
used	O
to	O
train	O
deidentification	O
models	O
.	O
The	O
two	O
models	O
were	O
trained	O
on	O
pseudonymised	O
data	O
and	O
evaluated	O
on	O
real	O
data	O
.	O
For	O
benchmarking	O
,	O
models	O
were	O
also	O
trained	O
on	O
real	O
data	O
,	O
and	O
evaluated	O
on	O
real	O
data	O
as	O
well	O
as	O
trained	O
on	O
pseudonymised	O
data	O
and	O
evaluated	O
on	O
pseudonymised	O
data	O
.	O
CRF	B-MethodName
showed	O
better	O
performance	O
for	O
some	O
PHI	O
information	O
like	O
Date	O
Part	O
,	O
First	O
Name	O
and	O
Last	O
Name	O
;	O
consistent	O
with	O
some	O
reports	O
in	O
the	O
literature	O
.	O
In	O
contrast	O
,	O
poor	O
performances	O
on	O
Location	O
and	O
Health	O
Care	O
Unit	O
information	O
were	O
noted	O
,	O
partially	O
due	O
to	O
the	O
constrained	O
vocabulary	O
in	O
the	O
pseudonymised	O
training	O
data	O
.	O
It	O
is	O
concluded	O
that	O
it	O
is	O
possible	O
to	O
train	O
transferable	O
models	O
based	O
on	O
pseudonymised	O
Swedish	O
clinical	O
data	O
,	O
but	O
even	O
small	O
narrative	O
and	O
distributional	O
variation	O
could	O
negatively	O
impact	O
performance	O
.	O

Electronic	O
health	O
records	O
(	O
EHR	O
)	O
are	O
produced	O
in	O
a	O
steady	O
stream	O
,	O
with	O
the	O
potential	O
of	O
advancing	O
future	O
medical	O
care	O
.	O
Research	O
on	O
EHR	O
data	O
holds	O
the	O
potential	O
to	O
improve	O
our	O
understanding	O
of	O
patient	O
care	O
,	O
care	O
processes	O
,	O
and	O
disease	O
characteristics	O
and	O
progression	O
.	O
However	O
,	O
much	O
of	O
the	O
data	O
⇤	O
Hercules	O
Dalianis	O
is	O
also	O
guest	O
professor	O
at	O
the	O
Norwegian	O
Centre	O
for	O
E	O
-	O
health	O
Research	O
is	O
sensitive	O
,	O
containing	O
Protected	O
Health	O
Information	O
(	O
PHI	O
)	O
such	O
as	O
personal	O
names	O
,	O
addresses	O
,	O
phone	O
numbers	O
,	O
that	O
can	O
identify	O
particular	O
individuals	O
and	O
thus	O
can	O
not	O
be	O
available	O
to	O
the	O
public	O
for	O
general	O
scientific	O
inquiry	O
.	O
Although	O
good	O
progress	O
has	O
been	O
made	O
in	O
the	O
general	O
sub	O
-	O
field	O
of	O
de	O
-	O
identifying	O
clinical	O
text	O
,	O
the	O
problem	O
is	O
still	O
not	O
fully	O
resolved	O
(	O
Meystre	O
et	O
al	O
,	O
2010	O
;	O
Yogarajan	O
et	O
al	O
,	O
2018	O
)	O
.	O
This	O
study	O
examines	O
the	O
use	O
of	O
pseudonymised	O
health	O
records	O
as	O
training	O
data	O
for	O
de	B-TaskName
-	I-TaskName
identification	I-TaskName
tasks	O
.	O
Several	O
ethical	O
and	O
scientific	O
issues	O
arise	O
regarding	O
the	O
balance	O
between	O
maintaining	O
patient	O
confidentiality	O
and	O
the	O
need	O
for	O
wider	O
application	O
of	O
trained	O
models	O
.	O
How	O
will	O
a	O
de	B-TaskName
-	I-TaskName
identification	I-TaskName
system	O
be	O
constructed	O
and	O
used	O
in	O
a	O
cross	O
hospital	O
setting	O
without	O
risking	O
the	O
privacy	O
of	O
patients	O
?	O
Is	O
it	O
possible	O
to	O
obscuring	O
the	O
training	O
data	O
by	O
pseudonymising	O
it	O
and	O
then	O
use	O
it	O
for	O
the	O
training	O
of	O
a	O
machine	O
learning	O
system	O
?	O
De	B-TaskName
-	I-TaskName
identification	I-TaskName
and	O
pseudonymisation	O
are	O
two	O
related	O
concepts	O
.	O
In	O
this	O
paper	O
de	B-TaskName
-	I-TaskName
identification	I-TaskName
is	O
used	O
as	O
a	O
more	O
general	O
term	O
to	O
describe	O
the	O
process	O
of	O
finding	O
personal	O
health	O
information	O
to	O
be	O
able	O
to	O
conceal	O
identifying	O
information	O
.	O
A	O
pseudonymised	O
text	O
is	O
a	O
text	O
where	O
the	O
personal	O
health	O
information	O
has	O
been	O
identified	O
either	O
manually	O
or	O
automatically	O
and	O
then	O
replaced	O
with	O
realistic	O
surrogates	O
.	O
The	O
research	O
question	O
in	O
this	O
study	O
is	O
whether	O
it	O
is	O
possible	O
to	O
use	O
de	O
-	O
identified	O
and	O
pseudonymised	O
clinical	O
text	O
in	O
Swedish	O
as	O
training	O
data	O
for	O
deidentifying	O
real	O
clinical	O
text	O
,	O
and	O
hence	O
make	O
it	O
possible	O
to	O
transfer	O
the	O
system	O
cross	O
hospital	O
.	O
We	O
highlight	O
whether	O
learning	O
from	O
the	O
exist	O
-	O
ing	O
,	O
non	O
-	O
sensitive	O
,	O
pseudonymised	O
Swedish	O
clinical	O
text	O
can	O
be	O
useful	O
in	O
a	O
new	O
and	O
different	O
context	O
;	O
considering	O
the	O
normal	O
variations	O
in	O
the	O
distribution	O
and	O
nature	O
of	O
PHI	O
information	O
,	O
and	O
potential	O
effects	O
of	O
scrubbing	O
(	O
Berman	O
,	O
2003	O
)	O
,	O
that	O
is	O
,	O
removing	O
and	O
modifying	O
PHIs	O
that	O
was	O
carried	O
out	O
to	O
patient	O
records	O
during	O
the	O
de	B-TaskName
-	I-TaskName
identification	I-TaskName
process	O
.	O

In	O
this	O
study	O
,	O
machine	O
learning	O
approaches	O
are	O
used	O
since	O
the	O
best	O
de	B-TaskName
-	I-TaskName
identification	I-TaskName
systems	O
appear	O
to	O
be	O
machine	O
learning	O
-	O
based	O
(	O
Kushida	O
et	O
al	O
,	O
2012	O
)	O
.	O
While	O
rule	O
-	O
based	O
methods	O
such	O
as	O
using	O
dictionaries	O
and	O
pattern	O
-	O
matching	O
were	O
previously	O
more	O
prevalent	O
than	O
machine	O
learning	O
methods	O
for	O
solving	O
text	O
-	O
based	O
de	B-TaskName
-	I-TaskName
identification	I-TaskName
problems	O
(	O
Meystre	O
et	O
al	O
,	O
2010	O
)	O
,	O
today	O
it	O
is	O
more	O
typical	O
to	O
have	O
both	O
approaches	O
used	O
,	O
since	O
rule	O
-	O
based	O
methods	O
still	O
yield	O
better	O
results	O
for	O
some	O
PHI	O
information	O
(	O
Neamatullah	O
et	O
al	O
,	O
2008b	O
)	O
.	O
Dictionaries	O
and	O
patterns	O
were	O
therefore	O
used	O
as	O
features	O
within	O
one	O
of	O
the	O
models	O
.	O

Two	O
different	O
data	O
sets	O
for	O
de	B-TaskName
-	I-TaskName
identification	I-TaskName
were	O
used	O
:	O
Stockholm	O
EPR	O
PHI	O
Psuedo	O
Corpus	O
(	O
Pseudo	O
)	O
as	O
well	O
as	O
the	O
Stockholm	O
EPR	O
PHI	O
Cor	O
-	O
The	O
Stockholm	O
EPR	O
PHI	O
Pseudo	O
Corpus	O
was	O
produced	O
from	O
the	O
Stockholm	O
EPR	O
PHI	O
Corpus	O
by	O
automatically	O
pseudonymising	O
all	O
PHIs	O
.	O
This	O
process	O
is	O
described	O
by	O
Dalianis	O
(	O
2019	O
)	O
.	O
The	O
Stockholm	O
EPR	O
PHI	O
Corpus	O
is	O
described	O
by	O
Dalianis	O
and	O
Velupillai	O
(	O
2010	O
)	O
.	O
An	O
example	O
is	O
shown	O
in	O
Figure	O
1	O
(	O
Dalianis	O
et	O
al	O
,	O
2015	O
)	O
.	O
The	O
number	O
of	O
entities	O
and	O
types	O
of	O
entities	O
in	O
both	O
the	O
Stockholm	O
EPR	O
PHI	O
Psuedo	O
Corpus	O
and	O
the	O
Stockholm	O
EPR	O
PHI	O
Corpus	O
is	O
shown	O
in	O
Table	O
2	O
.	O
From	O
Table	O
2	O
,	O
it	O
can	O
be	O
observed	O
that	O
the	O
distribution	O
of	O
PHI	O
instances	O
between	O
the	O
two	O
data	O
sets	O
is	O
somewhat	O
similar	O
,	O
but	O
there	O
is	O
a	O
significant	O
difference	O
when	O
it	O
comes	O
to	O
unique	O
instances	O
between	O
the	O
two	O
data	O
sets	O
.	O
In	O
total	O
,	O
the	O
Real	O
data	O
set	O
contains	O
proportionally	O
more	O
unique	O
instances	O
than	O
the	O
Pseudo	O
data	O
set	O
.	O
The	O
entities	O
in	O
the	O
Real	O
data	O
set	O
al	O
o	O
tend	O
to	O
have	O
more	O
tokens	O
.	O

Using	O
the	O
de	O
-	O
identified	O
and	O
pseudonymised	O
data	O
set	O
,	O
two	O
models	O
were	O
trained	O
based	O
on	O
two	O
machine	O
learning	O
algorithms	O
;	O
CRF	B-MethodName
and	O
the	O
deep	O
learning	O
algorithm	O
LSTM	B-MethodName
.	O
The	O
two	O
algorithms	O
were	O
chosen	O
since	O
both	O
have	O
been	O
shown	O
to	O
produce	O
state	O
of	O
the	O
art	O
performance	O
,	O
and	O
applying	O
the	O
two	O
on	O
Swedish	O
clinical	O
data	O
sets	O
makes	O
for	O
an	O
informative	O
comparison	O
.	O
The	O
two	O
models	O
were	O
evaluated	O
on	O
both	O
the	O
real	O
data	O
set	O
that	O
is	O
annotated	O
for	O
PHI	O
,	O
but	O
not	O
pseudonymised	O
,	O
'	O
Pseudo	O
-	O
Real	O
'	O
,	O
as	O
well	O
as	O
on	O
the	O
pseudonymised	O
data	O
set	O
,	O
'	O
Pseudo	O
-	O
Pseudo	O
'	O
.	O
For	O
additional	O
comparison	O
basis	O
models	O
trained	O
on	O
the	O
real	O
data	O
set	O
were	O
evaluated	O
on	O
test	O
sets	O
from	O
the	O
same	O
data	O
set	O
,	O
'	O
Real	O
-	O
Real	O
'	O
.	O

In	O
this	O
study	O
,	O
the	O
CRF	B-MethodName
algorithm	O
implemented	O
in	O
CRFSuite	O
(	O
Okazaki	O
,	O
2007	O
)	O
is	O
used	O
with	O
the	O
sklearn	O
-	O
crfsuite	O
wrapper	O
2	O
and	O
the	O
LSTM	B-MethodName
architecture	O
described	O
by	O
Lample	O
et	O
al	O
(	O
2016	O
)	O
,	O
based	O
on	O
an	O
open	O
-	O
source	O
implementation	O
with	O
Tensorflow	O
3	O
is	O
used	O
.	O
The	O
linear	O
-	O
chain	O
Conditional	O
Random	O
Fields	O
model	O
,	O
implemented	O
with	O
sklearn	O
-	O
CRFSuite	O
4	O
,	O
Figure	O
1	O
:	O
Example	O
of	O
a	O
pseudonymised	O
record	O
.	O
The	O
original	O
Swedish	O
pseudonymised	O
record	O
is	O
to	O
the	O
right	O
and	O
the	O
translated	O
version	O
is	O
to	O
the	O
left	O
.	O
The	O
underlined	O
words	O
are	O
the	O
surrogates	O
,	O
where	O
real	O
data	O
has	O
been	O
replaced	O
with	O
pseudonyms	O
.	O
uses	O
lexical	O
,	O
orthographic	O
,	O
syntactic	O
and	O
dictionary	O
features	O
.	O
The	O
CRF	B-MethodName
is	O
based	O
on	O
trial	O
-	O
and	O
-	O
error	O
experiments	O
with	O
feature	O
sets	O
described	O
by	O
Berg	O
and	O
Dalianis	O
(	O
2019	O
)	O
,	O
and	O
uses	O
the	O
same	O
features	O
except	O
for	O
section	O
features	O
.	O

The	O
long	B-MethodName
short	I-MethodName
-	I-MethodName
term	I-MethodName
memory	I-MethodName
(	O
LSTM	B-MethodName
)	O
needs	O
word	B-TaskName
embeddings	I-TaskName
as	O
features	O
for	O
the	O
training	O
.	O
Word2vec	O
5	O
was	O
used	O
to	O
produce	O
word	B-TaskName
embeddings	I-TaskName
using	O
shallow	O
neural	O
networks	O
,	O
based	O
on	O
two	O
corpora	O
;	O
a	O
clinical	O
corpus	O
and	O
medical	O
journals	O
.	O
For	O
the	O
training	O
using	O
real	O
clinical	O
data	O
,	O
word	B-TaskName
embeddings	I-TaskName
were	O
produced	O
using	O
a	O
clinical	O
corpus	O
of	O
200	O
million	O
tokens	O
that	O
produced	O
300	O
,	O
824	O
vectors	O
with	O
a	O
dimension	O
of	O
300	O
.	O
For	O
the	O
training	O
with	O
pseudo	O
clinical	O
data	O
,	O
word	B-TaskName
embeddings	I-TaskName
were	O
produced	O
using	O
Läkartidningen	O
corpus	O
(	O
The	O
Swedish	O
scientific	O
medical	O
journals	O
from	O
1996	O
to	O
2005	O
)	O
containing	O
21	O
million	O
tokens	O
that	O
produced	O
118	O
,	O
662	O
vectors	O
with	O
a	O
dimension	O
of	O
300	O
.	O
The	O
reason	O
for	O
using	O
Läkartidningen	O
is	O
that	O
the	O
corpus	O
does	O
not	O
contain	O
sensitive	O
data	O
and	O
hence	O
is	O
also	O
more	O
easily	O
usable	O
for	O
transferable	O
cross	O
hospital	O
training	O
.	O

The	O
training	O
set	O
used	O
in	O
this	O
study	O
has	O
a	O
substantially	O
constrained	O
vocabulary	O
compared	O
to	O
the	O
evaluation	O
set	O
,	O
which	O
may	O
partially	O
explain	O
the	O
overall	O
performance	O
achieved	O
when	O
evaluating	O
on	O
real	O
data	O
(	O
Pseudo	O
-	O
Real	O
version	O
of	O
the	O
data	O
has	O
less	O
PHI	O
tokens	O
and	O
the	O
entities	O
are	O
more	O
often	O
single	O
tokens	O
.	O
The	O
Full	O
Date	O
structure	O
yyyyddmm	O
-	O
yyyyddmm	O
is	O
commonly	O
occurring	O
in	O
the	O
pseudo	O
data	O
,	O
and	O
the	O
dash	O
between	O
the	O
dates	O
,	O
"	O
-	O
"	O
,	O
is	O
often	O
incorrectly	O
identified	O
.	O
For	O
example	O
,	O
using	O
the	O
CRF	B-MethodName
algorithm	O
on	O
real	O
-	O
data	O
training	O
and	O
pseudo	O
-	O
data	O
testing	O
(	O
Real	O
-	O
Pseudo	O
)	O
,	O
of	O
the	O
159	O
instances	O
not	O
identified	O
as	O
full	O
dates	O
tokens	O
,	O
sixty	O
contain	O
'	O
-	O
'	O
.	O
The	O
pseudo	O
data	O
uses	O
the	O
structure	O
yyyyddmm	O
while	O
the	O
real	O
data	O
uses	O
yyddmm	O
,	O
which	O
leads	O
to	O
errors	O
.	O
For	O
these	O
kinds	O
of	O
errors	O
on	O
standard	O
data	O
formats	O
such	O
as	O
dates	O
,	O
it	O
is	O
easy	O
to	O
see	O
how	O
rule	O
-	O
based	O
approaches	O
using	O
regular	O
expressions	O
could	O
significantly	O
improve	O
the	O
overall	O
performance	O
of	O
the	O
system	O
.	O
The	O
weakest	O
performance	O
area	O
was	O
for	O
location	O
information	O
.	O
There	O
is	O
a	O
large	O
variety	O
of	O
locations	O
in	O
the	O
pseudo	O
-	O
data	O
.	O
These	O
are	O
also	O
fairly	O
specific	O
and	O
unlikely	O
to	O
occur	O
in	O
the	O
real	O
data	O
,	O
for	O
example	O
,	O
locations	O
with	O
very	O
few	O
inhabitants	O
.	O
These	O
uncommon	O
rural	O
places	O
have	O
names	O
similar	O
to	O
residential	O
homes	O
(	O
äldreboenden	O
)	O
.	O
There	O
are	O
multiple	O
instances	O
of	O
the	O
suffix	O
'	O
gården	O
'	O
(	O
yard	O
)	O
in	O
the	O
location	O
pseudo	O
-	O
PHI	O
,	O
whereas	O
,	O
in	O
the	O
real	O
data	O
,	O
the	O
same	O
suffix	O
is	O
common	O
for	O
care	O
units	O
.	O
In	O
the	O
pseudo	O
-	O
data	O
,	O
the	O
care	O
units	O
are	O
more	O
general	O
than	O
in	O
the	O
real	O
data	O
,	O
often	O
too	O
general	O
to	O
be	O
annotated	O
in	O
the	O
real	O
data	O
set	O
.	O
Infirmaries	O
are	O
fairly	O
common	O
in	O
the	O
real	O
data	O
but	O
non	O
-	O
existent	O
in	O
the	O
pseudo	O
data	O
.	O
This	O
lack	O
of	O
variation	O
in	O
the	O
pseudo	O
is	O
partially	O
responsible	O
for	O
the	O
drop	O
in	O
performance	O
.	O
There	O
are	O
at	O
least	O
two	O
ways	O
to	O
think	O
about	O
mitigating	O
this	O
poor	O
performance	O
.	O
First	O
,	O
location	O
and	O
care	O
unit	O
could	O
be	O
combined	O
as	O
one	O
entity	O
type	O
since	O
they	O
are	O
conceptually	O
very	O
similar	O
,	O
and	O
sometimes	O
have	O
interchangeable	O
entity	O
names	O
.	O
Secondly	O
,	O
using	O
more	O
detailed	O
municipality	O
street	O
and	O
location	O
mapping	O
databases	O
as	O
dictionaries	O
could	O
be	O
considered	O
.	O

There	O
is	O
one	O
similar	O
study	O
to	O
ours	O
but	O
for	O
English	O
by	O
Yeniterzi	O
et	O
al	O
(	O
2010	O
)	O
,	O
where	O
the	O
authors	O
train	O
their	O
de	B-TaskName
-	I-TaskName
identification	I-TaskName
system	O
with	O
all	O
combinations	O
of	O
pseudonymised	O
textual	O
data	O
(	O
or	O
what	O
they	O
call	O
resynthesized	O
records	O
)	O
and	O
real	O
data	O
and	O
their	O
results	O
are	O
in	O
line	O
with	O
ours	O
.	O
However	O
,	O
there	O
are	O
some	O
studies	O
on	O
cross	O
-	O
domain	B-TaskName
adaptation	I-TaskName
.	O
In	O
cross	O
-	O
domain	O
adaption	O
there	O
is	O
,	O
however	O
,	O
a	O
substantial	O
domain	O
change	O
between	O
the	O
training	O
and	O
testing	O
data	O
,	O
unlike	O
in	O
this	O
study	O
.	O
Martinez	O
et	O
al	O
(	O
2014	O
)	O
used	O
models	O
trained	O
in	O
one	O
hospital	O
on	O
pathology	O
reports	O
in	O
another	O
hospital	O
.	O
Their	O
system	O
only	O
required	O
minor	O
feature	O
normalisation	O
,	O
and	O
the	O
reported	O
results	O
were	O
comparable	O
across	O
the	O
hospitals	O
.	O
Although	O
this	O
demonstrates	O
feasibility	O
,	O
it	O
is	O
important	O
to	O
note	O
that	O
the	O
pathology	O
reports	O
were	O
from	O
the	O
same	O
medical	O
sub	O
-	O
speciality	O
with	O
only	O
some	O
narrative	O
differences	O
.	O
In	O
this	O
study	O
,	O
in	O
addition	O
to	O
narrative	O
differences	O
between	O
the	O
training	O
data	O
and	O
the	O
target	O
evaluation	O
data	O
,	O
the	O
number	O
of	O
care	O
units	O
and	O
locations	O
involved	O
,	O
as	O
well	O
as	O
personal	O
names	O
,	O
are	O
widely	O
varied	O
.	O
With	O
large	O
amounts	O
of	O
out	O
of	O
vocabulary	O
variation	O
,	O
training	O
on	O
limited	O
data	O
will	O
likely	O
yield	O
poor	O
results	O
.	O
In	O
practice	O
,	O
these	O
data	O
types	O
exist	O
in	O
other	O
non	O
-	O
sensitive	O
sources	O
such	O
as	O
city	O
and	O
rural	O
location	O
and	O
street	O
mapping	O
data	O
.	O
Except	O
for	O
location	O
and	O
care	O
unit	O
,	O
evaluation	O
on	O
pseudo	O
-	O
data	O
(	O
Pseudo	O
-	O
Pseudo	O
)	O
produced	O
better	O
outcomes	O
compared	O
to	O
performance	O
on	O
real	O
-	O
data	O
(	O
Pseudo	O
-	O
Real	O
)	O
,	O
which	O
can	O
be	O
expected	O
.	O
What	O
was	O
a	O
bit	O
unexpected	O
was	O
the	O
lower	O
performance	O
of	O
the	O
LSTM	B-MethodName
algorithm	O
.	O
The	O
algorithm	O
's	O
results	O
would	O
potentially	O
have	O
been	O
improved	O
by	O
larger	O
vector	O
data	O
or	O
more	O
labelled	O
data	O
(	O
Dernoncourt	O
et	O
al	O
,	O
2017	O
)	O
.	O
While	O
clinical	O
notes	O
have	O
unique	O
linguistic	O
structures	O
and	O
grammatical	O
peculiarities	O
,	O
nonclinical	O
data	O
sources	O
could	O
still	O
provide	O
important	O
contextual	O
information	O
for	O
constructing	O
a	O
useful	O
vector	O
space	O
.	O
Additional	O
sources	O
using	O
nonsensitive	O
data	O
,	O
such	O
as	O
public	O
corpora	O
in	O
the	O
general	O
domain	O
,	O
hold	O
a	O
potential	O
to	O
improve	O
performance	O
on	O
the	O
de	B-TaskName
-	I-TaskName
identification	I-TaskName
task	O
,	O
therefore	O
this	O
line	O
of	O
inquiry	O
will	O
be	O
followed	O
up	O
on	O
in	O
future	O
work	O
.	O
In	O
the	O
same	O
vein	O
,	O
factoring	O
in	O
part	O
of	O
speech	O
tags	O
from	O
other	O
sources	O
of	O
clinical	O
data	O
could	O
be	O
useful	O
in	O
this	O
case	O
.	O
For	O
instance	O
,	O
there	O
are	O
deidentification	O
databases	O
of	O
clinical	O
text	O
,	O
such	O
as	O
MIMIC	O
(	O
Neamatullah	O
et	O
al	O
,	O
2008a	O
;	O
Goldberger	O
et	O
al	O
,	O
2000	O
)	O
,	O
which	O
could	O
be	O
used	O
as	O
additional	O
information	O
for	O
training	O
purposes	O
,	O
and	O
using	O
only	O
the	O
part	O
of	O
speech	O
tags	O
reduces	O
security	O
risks	O
.	O
Current	O
results	O
are	O
calculated	O
as	O
exact	O
matches	O
,	O
and	O
the	O
partial	O
match	O
is	O
not	O
factored	O
in	O
,	O
which	O
may	O
affect	O
the	O
result	O
.	O
As	O
mentioned	O
in	O
the	O
analysis	O
the	O
CRF	B-MethodName
algorithm	O
rarely	O
classifies	O
the	O
'	O
-	O
'	O
in	O
between	O
dates	O
as	O
a	O
part	O
of	O
the	O
dates	O
,	O
and	O
these	O
are	O
therefore	O
not	O
counted	O
as	O
matches	O
despite	O
the	O
most	O
identifying	O
parts	O
of	O
the	O
entity	O
being	O
identified	O
.	O
To	O
improve	O
the	O
general	O
performance	O
,	O
a	O
combination	O
of	O
both	O
the	O
LSTM	B-MethodName
and	O
CRF	B-MethodName
algorithms	O
could	O
be	O
performed	O
instead	O
of	O
testing	O
them	O
independently	O
.	O
Combining	O
high	O
-	O
performance	O
algorithms	O
and	O
the	O
use	O
of	O
ensemble	O
methods	O
seem	O
to	O
produce	O
the	O
best	O
results	O
as	O
reported	O
in	O
the	O
literature	O
(	O
Dernoncourt	O
et	O
al	O
,	O
2017	O
;	O
Liu	O
et	O
al	O
,	O
2017	O
)	O
,	O
and	O
these	O
techniques	O
will	O
be	O
investigated	O
in	O
future	O
work	O
on	O
the	O
data	O
sets	O
.	O

The	O
results	O
of	O
this	O
study	O
suggest	O
that	O
although	O
it	O
is	O
possible	O
to	O
train	O
models	O
on	O
pseudonymised	O
data	O
for	O
use	O
in	O
different	O
contexts	O
,	O
there	O
is	O
severe	O
deterioration	O
in	O
performance	O
for	O
some	O
PHI	O
information	O
.	O
Even	O
small	O
narrative	O
and	O
distributional	O
variation	O
could	O
negatively	O
impact	O
performance	O
.	O
Transferring	O
a	O
system	O
from	O
one	O
set	O
of	O
clinical	O
text	O
to	O
a	O
different	O
set	O
could	O
result	O
in	O
the	O
performance	O
of	O
the	O
system	O
deteriorating	O
;	O
in	O
this	O
study	O
the	O
Pseudo	O
-	O
Real	O
case	O
.	O
This	O
problem	O
,	O
what	O
we	O
call	O
The	O
cross	O
pseudo	O
-	O
real	O
text	O
adaptation	O
problem	O
,	O
is	O
an	O
issue	O
that	O
could	O
happen	O
due	O
to	O
the	O
pseudonymisation	O
/	O
de	B-TaskName
-	I-TaskName
identification	I-TaskName
processes	O
on	O
the	O
training	O
data	O
due	O
to	O
the	O
narrative	O
and	O
distributional	O
variation	O
as	O
well	O
as	O
other	O
differences	O
in	O
the	O
nature	O
of	O
the	O
PHI	O
between	O
the	O
training	O
data	O
and	O
the	O
target	O
.	O
In	O
the	O
future	O
,	O
we	O
will	O
try	O
to	O
improve	O
the	O
pseudonymisation	O
module	O
described	O
in	O
Dalianis	O
(	O
2019	O
)	O
to	O
produce	O
a	O
larger	O
variation	O
in	O
the	O
vocabulary	O
as	O
the	O
lack	O
of	O
variation	O
may	O
affect	O
the	O
current	O
result	O
negatively	O
.	O
We	O
will	O
also	O
apply	O
the	O
learned	O
models	O
to	O
other	O
Nordic	O
languages	O
such	O
as	O
Norwegian	O
clinical	O
text	O
and	O
use	O
the	O
system	O
as	O
a	O
pre	O
-	O
annotation	O
system	O
to	O
assist	O
the	O
manual	O
annotators	O
in	O
their	O
work	O
to	O
create	O
a	O
Norwegian	O
gold	O
standard	O
.	O

IIITT	O
at	O
CASE	O
2021	O
Task	O
1	O
:	O
Leveraging	O
Pretrained	B-TaskName
Language	I-TaskName
Models	I-TaskName
for	O
Multilingual	O
Protest	O
Detection	O

The	O
recent	O
surge	O
in	O
social	O
media	O
users	O
has	O
led	O
many	O
people	O
to	O
express	O
their	O
opinions	O
on	O
various	O
global	O
issues	O
.	O
These	O
opinions	O
travel	O
far	O
and	O
wide	O
within	O
a	O
matter	O
of	O
seconds	O
(	O
Hossny	O
et	O
al	O
,	O
2018	O
)	O
.	O
This	O
can	O
influence	O
many	O
people	O
and	O
may	O
engage	O
public	O
movements	O
(	O
Won	O
et	O
al	O
,	O
2017a	O
)	O
.	O
Therefore	O
,	O
there	O
is	O
a	O
definite	O
need	O
to	O
detect	O
these	O
protests	O
and	O
analyse	O
them	O
to	O
know	O
the	O
significant	O
areas	O
of	O
disinterest	O
.	O
Being	O
a	O
free	O
and	O
easy	O
to	O
use	O
platform	O
,	O
social	O
media	O
has	O
become	O
a	O
part	O
of	O
our	O
day	O
to	O
day	O
life	O
.	O
It	O
incorporates	O
people	O
of	O
different	O
ages	O
,	O
gender	O
,	O
location	O
,	O
religions	O
,	O
background	O
,	O
and	O
so	O
on	O
.	O
The	O
enormous	O
number	O
of	O
rich	O
and	O
diversified	O
users	O
results	O
in	O
an	O
enormous	O
amount	O
of	O
information	O
being	O
generated	O
,	O
which	O
is	O
helpful	O
in	O
many	O
ways	O
(	O
Kapoor	O
et	O
al	O
,	O
2018	O
)	O
.	O
Some	O
of	O
this	O
even	O
contains	O
private	O
information	O
about	O
the	O
users	O
,	O
which	O
others	O
could	O
misuse	O
.	O
Cases	O
were	O
also	O
found	O
where	O
certain	O
users	O
were	O
being	O
targeted	O
and	O
harassed	O
by	O
people	O
using	O
this	O
platform	O
,	O
a	O
common	O
scenario	O
in	O
cyberbullying	O
(	O
Abaido	O
,	O
2020	O
)	O
.	O
Social	O
media	O
plays	O
a	O
crucial	O
role	O
in	O
amplifying	O
these	O
protests	O
and	O
movements	O
(	O
Won	O
et	O
al	O
,	O
2017b	O
)	O
.	O
It	O
enables	O
political	O
groups	O
and	O
protesters	O
to	O
organise	O
protest	O
movements	O
and	O
share	O
information	O
.	O
It	O
acts	O
as	O
a	O
platform	O
for	O
the	O
people	O
who	O
are	O
underrepresented	O
by	O
giving	O
a	O
voice	O
to	O
them	O
.	O
It	O
also	O
offers	O
new	O
opportunities	O
for	O
people	O
to	O
engage	O
in	O
activism	O
,	O
political	O
resistance	O
,	O
and	O
protest	O
outside	O
the	O
political	O
groups	O
and	O
civic	O
institutions	O
.	O
Thus	O
,	O
it	O
has	O
a	O
social	O
impact	O
on	O
everyone	O
(	O
Pulido	O
et	O
al	O
,	O
2018	O
)	O
.	O
It	O
is	O
to	O
be	O
noted	O
that	O
social	O
media	O
,	O
similar	O
to	O
news	O
media	O
,	O
plays	O
a	O
vital	O
role	O
in	O
its	O
social	O
and	O
political	O
events	O
worldwide	O
(	O
Holt	O
et	O
al	O
,	O
2013	O
)	O
.	O
For	O
the	O
above	O
reasons	O
,	O
we	O
can	O
state	O
that	O
social	O
media	O
plays	O
a	O
crucial	O
role	O
in	O
most	O
worldwide	O
events	O
.	O
The	O
English	O
language	O
is	O
widely	O
regarded	O
as	O
the	O
first	O
Lingua	O
Franca	O
.	O
Statistically	O
,	O
it	O
is	O
one	O
of	O
the	O
most	O
widely	O
spoken	O
languages	O
globally	O
,	O
having	O
official	O
status	O
in	O
over	O
53	O
countries	O
(	O
Crystal	O
,	O
2008	O
)	O
.	O
Over	O
400	O
million	O
people	O
speak	O
English	O
as	O
their	O
primary	O
language	O
and	O
widely	O
spoken	O
in	O
the	O
United	O
States	O
and	O
the	O
United	O
Kingdom	O
.	O
BlackLivesMatter	O
(	O
Dave	O
et	O
al	O
,	O
2020	O
)	O
,	O
EarthDay	O
(	O
Rome	O
,	O
2010	O
)	O
are	O
some	O
of	O
the	O
major	O
protests	O
that	O
have	O
occurred	O
in	O
these	O
countries	O
.	O
Español	O
commonly	O
referred	O
to	O
as	O
Spanish	O
,	O
is	O
spoken	O
by	O
over	O
360	O
million	O
people	O
worldwide	O
,	O
with	O
most	O
of	O
its	O
speakers	O
residing	O
in	O
Mexico	O
,	O
Argentina	O
,	O
Spain	O
.	O
15	O
-	O
M	O
Movement	O
(	O
Casero	O
-	O
Ripollés	O
andFeenstra	O
,	O
2012	O
)	O
andYoSoy132	O
(	O
García	O
andTreré	O
,	O
2014	O
)	O
are	O
some	O
of	O
the	O
recent	O
protests	O
where	O
people	O
have	O
been	O
vocal	O
about	O
in	O
the	O
Spanish	O
language	O
.	O
Portuguese	O
has	O
over	O
220	O
million	O
native	O
speakers	O
.	O
Brazil	O
,	O
Portugal	O
,	O
Angola	O
are	O
some	O
of	O
the	O
major	O
countries	O
where	O
this	O
language	O
is	O
spoken	O
.	O
Protests	O
like	O
Racism	O
Kills	O
,	O
May	O
68	O
(	O
Ross	O
,	O
2008	O
)	O
are	O
the	O
recent	O
ones	O
that	O
occurred	O
in	O
the	O
Portuguese	O
language	O
.	O
The	O
recent	O
upheavals	O
of	O
protests	O
are	O
due	O
to	O
so	O
-	O
Sentence	O
Language	O
Label	O
Fabius	O
ran	O
against	O
Royal	O
for	O
the	O
presidential	O
nomination	O
in	O
2007	O
.	O
English	O
Event	O
He	O
planned	O
to	O
start	O
a	O
race	O
war	O
.	O
English	O
Event	O
Metro	O
police	O
intervened	O
and	O
the	O
fire	O
was	O
put	O
out	O
.	O
English	O
Not	O
-	O
event	O
Pero	O
no	O
esése	O
el	O
mayor	O
problema	O
.	O
Spanish	O
Event	O
La	O
Argentina	O
retrocedería	O
un	O
paso	O
todos	O
los	O
días	O
.	O
Spanish	O
Event	O
Carrió	O
no	O
objetó	O
que	O
se	O
trató	O
de	O
un	O
secuestro	O
.	O
Spanish	O
Not	O
-	O
event	O
Os	O
servidores	O
do	O
Piauí	O
estão	O
em	O
greve	O
há	O
17	O
dias	O
.	O
Portuguese	O
Not	O
-	O
event	O
E	O
uma	O
nova	O
experiência	O
mobilizatória	O
.	O
Portuguese	O
Event	O
E	O
decidiram	O
iràs	O
aulas	O
e	O
passar	O
o	O
dia	O
de	O
saia	O
.	O
Portuguese	O
Not	O
-	O
event	O
cial	O
media	O
,	O
youth	O
,	O
exaggeration	O
of	O
certain	O
events	O
.	O
(	O
Basile	O
and	O
Caselli	O
,	O
2020	O
)	O
.	O
Any	O
early	O
detection	O
of	O
mass	O
protest	O
detection	O
through	O
social	O
media	O
platforms	O
such	O
as	O
Facebook	O
,	O
Twitter	O
,	O
and	O
Instagram	O
to	O
help	O
minimizing	O
the	O
aftermath	O
of	O
the	O
protests	O
(	O
Wilson	O
,	O
2017	O
)	O
.	O
This	O
has	O
motivated	O
Natural	O
Language	O
Processing	O
(	O
NLP	O
)	O
researchers	O
to	O
develop	O
NLP	O
systems	O
to	O
generalize	O
on	O
data	O
coming	O
from	O
diverse	O
sources	O
to	O
leverage	O
the	O
NLP	O
systems	O
to	O
more	O
realistic	O
environments	O
(	O
Büyüköz	O
et	O
al	O
,	O
2020	O
)	O
.	O
Hence	O
,	O
there	O
is	O
a	O
need	O
to	O
develop	O
NLP	O
systems	O
that	O
could	O
be	O
generalized	O
to	O
any	O
protest	O
/	O
events	O
(	O
Peng	O
et	O
al	O
,	O
2013	O
)	O
,	O
which	O
has	O
motivated	O
us	O
to	O
participate	O
in	O
the	O
shared	O
task	O
for	O
multilingual	O
protest	O
detection	O
(	O
Hürriyetoglu	O
et	O
al	O
,	O
2019a	O
The	O
objective	O
of	O
the	O
task	O
is	O
to	O
identify	O
if	O
any	O
sentence	O
talks	O
about	O
any	O
mentions	O
of	O
protests	O
or	O
events	O
in	O
three	O
languages	O
,	O
namely	O
,	O
English	O
,	O
Spanish	O
,	O
and	O
Portuguese	O
.	O
Hence	O
,	O
we	O
treat	O
this	O
as	O
a	O
sequence	O
classification	O
task	O
.	O
The	O
rest	O
of	O
the	O
paper	O
is	O
organized	O
as	O
follows	O
,	O
Section	O
2	O
presents	O
previous	O
work	O
on	O
protest	O
detection	O
and	O
analysis	O
.	O
Section	O
3	O
entails	O
a	O
comprehensive	O
analysis	O
of	O
the	O
dataset	O
used	O
for	O
our	O
cause	O
.	O
Next	O
,	O
section	O
4	O
gives	O
a	O
detailed	O
description	O
of	O
the	O
models	O
used	O
for	O
the	O
multilingual	O
event	B-TaskName
detection	I-TaskName
.	O
Finally	O
,	O
section	O
5	O
analyses	O
the	O
results	O
obtained	O
,	O
and	O
Section	O
6	O
concludes	O
our	O
work	O
while	O
discussing	O
the	O
potential	O
directions	O
for	O
future	O
work	O
.	O

We	O
used	O
pretrained	O
transformer	O
-	O
based	O
models	O
for	O
identifying	O
if	O
a	O
sentence	O
talks	O
about	O
an	O
event	O
or	O
not	O
.	O
The	O
models	O
that	O
were	O
used	O
are	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
and	O
DistilBERT	B-MethodName
(	O
Sanh	O
et	O
al	O
,	O
2019	O
)	O
.	O
Even	O
though	O
there	O
are	O
3	O
different	O
languages	O
,	O
we	O
used	O
a	O
single	O
model	O
for	O
all	O
three	O
due	O
to	O
memory	O
constraints	O
and	O
reduced	O
training	O
time	O
.	O
We	O
fine	O
-	O
tuned	O
these	O
models	O
for	O
sequence	O
classification	O
.	O
Soft	O
Voting	O
is	O
done	O
on	O
all	O
these	O
models	O
to	O
produce	O
the	O
respective	O
final	O
outputs	O
for	O
the	O
languages	O
.	O
In	O
soft	O
voting	O
,	O
each	O
classifier	O
predicts	O
that	O
a	O
specific	O
data	O
point	O
belongs	O
to	O
the	O
particular	O
target	O
class	O
.	O
A	O
weighted	O
sum	O
of	O
the	O
predictions	O
is	O
done	O
based	O
on	O
the	O
importance	O
of	O
the	O
classifier	O
(	O
all	O
models	O
have	O
equal	O
weights	O
)	O
.	O
The	O
overall	O
prediction	O
is	O
chosen	O
as	O
the	O
target	O
with	O
the	O
greatest	O
sum	O
of	O
the	O
weighted	O
probability	O
,	O
thus	O
winning	O
the	O
vote	O
(	O
Beyeler	O
,	O
2017	O
;	O
Hande	O
et	O
al	O
,	O
2021	O
)	O
.	O

Robustly	O
Optimized	O
BERT	B-MethodName
(	O
RoBERTa	B-MethodName
)	O
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
follows	O
the	O
same	O
architecture	O
of	O
BERT	B-MethodName
while	O
differing	O
in	O
the	O
pretraining	O
strategy	O
.	O
It	O
is	O
pretrained	O
with	O
MLM	B-DatasetName
as	O
its	O
objective	O
where	O
the	O
model	O
tries	O
to	O
predict	O
the	O
masked	O
words	O
.	O
RoBERTa	B-MethodName
model	O
is	O
trained	O
on	O
the	O
vast	O
English	O
Wikipedia	O
and	O
CC	B-DatasetName
-	I-DatasetName
News	I-DatasetName
datasets	O
.	O
The	O
NSP	O
is	O
not	O
employed	O
as	O
a	O
pretraining	O
strategy	O
,	O
and	O
the	O
tokens	O
are	O
dynamically	O
masked	O
,	O
making	O
the	O
model	O
slightly	O
different	O
to	O
BERT	B-MethodName
.	O
During	O
tokenization	O
,	O
RoBERTa	B-MethodName
follows	O
byte	O
-	O
pair	O
encoding	O
(	O
BPE	B-MethodName
)	O
(	O
Gallé	O
,	O
2019	O
)	O
as	O
opposed	O
to	O
WordPiece	B-MethodName
employed	O
in	O
BERT	B-MethodName
.	O
We	O
use	O
robertabase	O
,	O
a	O
pretrained	O
language	O
model	O
consisting	O
of	O
12	O
layers	O
,	O
768	O
hidden	O
,	O
12	O
attention	O
heads	O
,	O
and	O
125	O
million	O
parameters	O
.	O

According	O
to	O
the	O
Oxford	O
English	O
Dictionary	O
,	O
emotion	B-DatasetName
is	O
defined	O
as	O
"	O
[	O
a	O
]	O
strong	O
feeling	O
deriving	O
from	O
one	O
's	O
circumstances	O
,	O
mood	O
,	O
or	O
relationships	O
with	O
others	O
.	O
"	O
1	O
This	O
"	O
standard	O
"	O
definition	O
identifies	O
emotions	O
as	O
constructs	O
involving	O
something	O
innate	O
that	O
is	O
often	O
invoked	O
in	O
social	O
interactions	O
and	O
that	O
aids	O
in	O
communicating	O
with	O
others	O
(	O
Hwang	O
and	O
Matsumoto	O
,	O
2016	O
)	O
.	O
It	O
is	O
no	O
exaggeration	O
that	O
humans	O
are	O
emotional	O
beings	O
:	O
Emotions	O
are	O
an	O
integral	O
part	O
of	O
human	O
life	O
,	O
and	O
affect	O
our	O
decision	B-TaskName
making	I-TaskName
as	O
well	O
as	O
our	O
mental	O
and	O
physical	O
health	O
.	O
As	O
such	O
,	O
developing	O
emotion	B-DatasetName
detection	O
models	O
is	O
important	O
;	O
they	O
have	O
a	O
wide	O
array	O
of	O
applications	O
,	O
ranging	O
from	O
building	O
nuanced	O
virtual	O
assistants	O
that	O
cater	O
for	O
the	O
emotions	O
of	O
their	O
users	O
to	O
detecting	O
the	O
emotions	O
of	O
social	O
media	O
users	O
in	O
order	O
to	O
understand	O
their	O
mental	O
and/or	O
physical	O
health	O
.	O
However	O
,	O
emotion	B-DatasetName
detection	O
has	O
remained	O
a	O
challenging	O
task	O
,	O
partly	O
due	O
to	O
the	O
limited	O
availability	O
of	O
labeled	O
data	O
and	O
partly	O
due	O
the	O
controversial	O
nature	O
of	O
what	O
emotions	O
themselves	O
are	O
(	O
Aaron	O
C.	O
Weidman	O
and	O
Tracy	O
,	O
2017	O
)	O
.	O
Recent	O
advances	O
in	O
machine	O
learning	O
for	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
suggest	O
that	O
,	O
given	O
enough	O
labeled	O
data	O
,	O
there	O
should	O
be	O
an	O
opportunity	O
to	O
build	O
better	O
emotion	B-DatasetName
detection	O
models	O
.	O
Manual	O
labeling	O
of	O
data	O
,	O
however	O
,	O
is	O
costly	O
and	O
so	O
it	O
is	O
desirable	O
to	O
develop	O
labeled	O
emotion	B-DatasetName
data	O
without	O
annotators	O
.	O
While	O
the	O
proliferation	O
of	O
social	O
media	O
has	O
made	O
it	O
possible	O
for	O
us	O
to	O
acquire	O
large	O
datasets	O
with	O
implicit	O
labels	O
in	O
the	O
form	O
of	O
hashtags	O
(	O
Mohammad	O
and	O
Kiritchenko	O
,	O
2015	O
)	O
,	O
such	O
labels	O
are	O
noisy	O
and	O
reliable	O
.	O
In	O
this	O
work	O
,	O
we	O
seek	O
to	O
enable	O
deep	O
learning	O
by	O
creating	O
a	O
large	O
dataset	O
of	O
fine	O
-	O
grained	O
emotions	O
using	O
Twitter	O
data	O
.	O
More	O
specifically	O
,	O
we	O
harness	O
cues	O
in	O
Twitter	O
data	O
in	O
the	O
form	O
of	O
emotion	B-DatasetName
hashtags	O
as	O
a	O
way	O
to	O
build	O
a	O
labeled	O
emotion	B-DatasetName
dataset	O
that	O
we	O
then	O
exploit	O
using	O
distant	O
supervision	O
(	O
Mintz	O
et	O
al	O
,	O
2009	O
)	O
(	O
the	O
use	O
of	O
hashtags	O
as	O
a	O
surrogate	O
for	O
annotator	O
-	O
generated	O
emotion	B-DatasetName
labels	O
)	O
to	O
build	O
emotion	B-DatasetName
models	O
grounded	O
in	O
psychology	O
.	O
We	O
construct	O
such	O
a	O
dataset	O
and	O
exploit	O
it	O
using	O
powerful	O
deep	O
learning	O
methods	O
to	O
build	O
accurate	O
,	O
high	O
coverage	O
models	O
for	O
emotion	B-DatasetName
prediction	O
.	O
Overall	O
,	O
we	O
make	O
the	O
following	O
contributions	O
:	O
1	O
)	O
Grounded	O
in	O
psychological	O
theory	O
of	O
emotions	O
,	O
we	O
build	O
a	O
large	O
-	O
scale	O
,	O
high	O
quality	O
dataset	O
of	O
tweets	O
labeled	O
with	O
emotions	O
.	O
Key	O
to	O
this	O
are	O
methods	O
to	O
ensure	O
data	O
quality	O
,	O
2	O
)	O
we	O
validate	O
the	O
data	O
collection	O
method	O
using	O
human	O
annotations	O
,	O
3	O
)	O
we	O
develop	O
powerful	O
deep	O
learning	O
models	O
using	O
a	O
gated	O
recurrent	O
network	O
to	O
exploit	O
the	O
data	O
,	O
yielding	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
24	O
fine	O
-	O
grained	O
types	O
of	O
emotions	O
,	O
and	O
4	O
)	O
we	O
extend	O
the	O
task	O
beyond	O
these	O
emotion	B-DatasetName
types	O
to	O
model	O
Plutick	O
's	O
8	O
primary	O
emotion	B-DatasetName
dimensions	O
.	O
Our	O
emotion	B-DatasetName
modeling	O
relies	O
on	O
distant	O
supervision	O
(	O
Read	O
,	O
2005	O
;	O
Mintz	O
et	O
al	O
,	O
2009	O
)	O
,	O
the	O
approach	O
of	O
using	O
cues	O
in	O
data	O
(	O
e.g.	O
,	O
hashtags	O
or	O
emoticons	O
)	O
as	O
a	O
proxy	O
for	O
"	O
ground	O
truth	O
"	O
labels	O
as	O
we	O
explained	O
above	O
.	O
Distant	O
supervision	O
has	O
been	O
investigated	O
by	O
a	O
number	O
of	O
researchers	O
for	O
emotion	B-DatasetName
detection	O
(	O
Tanaka	O
et	O
al	O
,	O
2005	O
;	O
Mohammad	O
,	O
2012	O
;	O
Purver	O
and	O
Battersby	O
,	O
2012	O
;	O
Wang	O
et	O
al	O
,	O
2012	O
;	O
Pak	O
and	O
Paroubek	O
,	O
2010	O
;	O
Yang	O
et	O
al	O
,	O
2007	O
)	O
and	O
for	O
other	O
semantic	O
tasks	O
such	O
as	O
sentiment	B-TaskName
analysis	I-TaskName
(	O
Read	O
,	O
2005	O
;	O
Go	O
et	O
al	O
,	O
2009	O
)	O
and	O
sarcasm	B-TaskName
detection	I-TaskName
(	O
González	O
-	O
Ibánez	O
et	O
al	O
,	O
2011	O
)	O
.	O
In	O
these	O
works	O
,	O
authors	O
successfully	O
use	O
emoticons	O
and/or	O
hashtags	O
as	O
marks	O
to	O
label	O
data	O
after	O
performing	O
varying	O
degrees	O
of	O
data	O
quality	O
assurance	O
.	O
We	O
take	O
a	O
similar	O
approach	O
,	O
using	O
a	O
larger	O
collection	O
of	O
tweets	O
,	O
richer	O
emotion	B-DatasetName
definitions	O
,	O
and	O
stronger	O
filtering	O
for	O
tweet	O
quality	O
.	O
The	O
remainder	O
of	O
the	O
paper	O
is	O
organized	O
as	O
follows	O
:	O
We	O
first	O
overview	O
related	O
literature	O
in	O
Section	O
2	O
,	O
describe	O
our	O
data	O
collection	O
in	O
Section	O
3.1	O
,	O
and	O
the	O
annotation	O
study	O
we	O
performed	O
to	O
validate	O
our	O
distant	O
supervision	O
method	O
in	O
Section	O
4	O
.	O
We	O
then	O
describe	O
our	O
methods	O
in	O
Section	O
5	O
,	O
provide	O
results	O
in	O
Section	O
6	O
,	O
and	O
conclude	O
in	O
Section	O
8	O
.	O

A	O
number	O
of	O
studies	O
have	O
also	O
been	O
performed	O
to	O
analyze	O
and/or	O
model	O
mood	O
in	O
social	O
media	O
data	O
.	O
(	O
De	O
Choudhury	O
et	O
al	O
,	O
2012	O
)	O
identify	O
more	O
than	O
200	O
moods	O
frequent	O
on	O
Twitter	O
as	O
extracted	O
from	O
psychological	O
literature	O
and	O
filtered	O
by	O
AMT	O
workers	O
.	O
They	O
then	O
collect	O
tweets	O
which	O
have	O
one	O
of	O
the	O
moods	O
in	O
their	O
mood	O
lexicon	O
in	O
the	O
form	O
of	O
a	O
hashtag	O
.	O
To	O
verify	O
the	O
quality	O
of	O
the	O
mood	O
data	O
,	O
the	O
authors	O
run	O
AMT	O
studies	O
where	O
they	O
ask	O
workers	O
whether	O
a	O
tweet	O
displayed	O
the	O
respective	O
mood	O
hashtag	O
or	O
not	O
and	O
find	O
that	O
in	O
83	O
%	O
of	O
the	O
cases	O
hashtagged	O
moods	O
at	O
the	O
end	O
of	O
posts	O
did	O
capture	O
users	O
'	O
moods	O
,	O
whereas	O
for	O
posts	O
with	O
mood	O
hashtags	O
anywhere	O
in	O
the	O
tweet	O
,	O
only	O
58	O
%	O
of	O
the	O
cases	O
capture	O
the	O
mood	O
of	O
users	O
.	O
Although	O
they	O
did	O
not	O
build	O
models	O
for	O
mood	O
detection	O
,	O
the	O
annotation	O
studies	O
(	O
De	O
Choudhury	O
et	O
al	O
,	O
2012	O
)	O
perform	O
further	O
support	O
our	O
specific	O
use	O
of	O
hashtags	O
to	O
label	O
emotions	O
.	O
(	O
Mishne	O
and	O
De	O
Rijke	O
,	O
2006	O
)	O
collect	O
user	O
-	O
labeled	O
mood	O
from	O
blog	O
post	O
text	O
on	O
LiveJournal	O
and	O
exploit	O
them	O
for	O
predicting	O
the	O
intensity	O
of	O
moods	O
over	O
a	O
time	O
span	O
rather	O
than	O
at	O
the	O
post	O
level	O
.	O
Similarly	O
,	O
(	O
Nguyen	O
,	O
2010	O
)	O
builds	O
models	O
to	O
infer	O
patterns	O
of	O
moods	O
in	O
a	O
large	O
collection	O
of	O
LiveJournal	O
posts	O
.	O
Some	O
of	O
the	O
moods	O
in	O
these	O
LiveJournal	O
studies	O
(	O
e.g.	O
,	O
hungry	O
,	O
cold	O
)	O
,	O
as	O
(	O
De	O
Choudhury	O
et	O
al	O
,	O
2012	O
)	O
explain	O
,	O
would	O
not	O
fit	O
any	O
psychological	O
theory	O
.	O
Our	O
work	O
is	O
different	O
in	O
that	O
it	O
is	O
situated	O
in	O
psychological	O
theory	O
of	O
emotion	B-DatasetName
.	O

In	O
spite	O
of	O
the	O
effectiveness	O
of	O
feature	B-TaskName
engineering	I-TaskName
for	O
NLP	O
,	O
it	O
is	O
a	O
labor	O
intensive	O
task	O
that	O
also	O
needs	O
domain	O
expertise	O
.	O
More	O
importantly	O
,	O
feature	B-TaskName
engineering	I-TaskName
falls	O
short	O
of	O
extracting	O
and	O
organizing	O
all	O
the	O
discriminative	O
information	O
from	O
data	O
Goodfellow	O
et	O
al	O
,	O
2016	O
)	O
.	O
Neural	O
networks	O
(	O
Goodfellow	O
et	O
al	O
,	O
2016	O
)	O
have	O
emerged	O
as	O
a	O
successful	O
class	O
of	O
methods	O
that	O
has	O
the	O
power	O
of	O
automatically	O
discovering	O
the	O
representations	O
needed	O
for	O
detection	O
or	O
classification	O
and	O
has	O
been	O
successfully	O
applied	O
to	O
multiple	O
NLP	O
tasks	O
.	O
A	O
line	O
of	O
studies	O
in	O
the	O
literature	O
(	O
e.g.	O
,	O
(	O
Labutov	O
and	O
Lip	O
-	O
son	O
,	O
2013	O
;	O
Maas	O
et	O
al	O
,	O
2011	O
;	O
Tang	O
et	O
al	O
,	O
2014b	O
,	O
a	O
)	O
aim	O
to	O
learn	O
sentiment	O
-	O
specific	O
word	B-TaskName
embeddings	I-TaskName
(	O
Bengio	O
et	O
al	O
,	O
2003	O
;	O
from	O
neighboring	O
text	O
.	O
Another	O
thread	O
of	O
research	O
focuses	O
on	O
learning	O
semantic	B-TaskName
composition	I-TaskName
(	O
Mitchell	O
and	O
Lapata	O
,	O
2010	O
)	O
,	O
including	O
extensions	O
to	O
phrases	O
and	O
sentences	O
with	O
recursive	O
neural	O
networks	O
(	O
a	O
class	O
of	O
syntax	O
-	O
tree	O
models	O
)	O
(	O
Socher	O
et	O
al	O
,	O
2013	O
;	O
Irsoy	O
and	O
Cardie	O
,	O
2014	O
;	O
Li	O
et	O
al	O
,	O
2015	O
)	O
and	O
to	O
documents	O
with	O
distributed	O
representations	O
of	O
sentences	O
and	O
paragraphs	O
(	O
Le	O
and	O
Mikolov	O
,	O
2014	O
;	O
Tang	O
et	O
al	O
,	O
2015	O
)	O
for	O
modeling	O
sentiment	O
.	O
Long	O
-	O
short	O
term	O
memory	O
(	O
LSTM	B-MethodName
)	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
and	O
Gated	O
Recurrent	O
Neural	O
Nets	O
(	O
GRNNs	O
)	O
(	O
Cho	O
et	O
al	O
,	O
2014	O
;	O
Chung	O
et	O
al	O
,	O
2015	O
)	O
,	O
variations	O
of	O
recurrent	O
neural	O
networks	O
(	O
RNNs	O
)	O
,	O
a	O
type	O
of	O
networks	O
suitable	O
for	O
handling	O
time	O
-	O
series	O
data	O
like	O
speech	O
(	O
Graves	O
et	O
al	O
,	O
2013	O
)	O
or	O
handwriting	B-TaskName
recognition	I-TaskName
(	O
Graves	O
,	O
2012	O
;	O
Graves	O
and	O
Schmidhuber	O
,	O
2009	O
)	O
,	O
have	O
also	O
been	O
used	O
successfully	O
for	O
sentiment	B-TaskName
analysis	I-TaskName
(	O
Ren	O
et	O
al	O
,	O
2016	O
;	O
Tai	O
et	O
al	O
,	O
2015	O
;	O
Tang	O
et	O
al	O
,	O
2015	O
;	O
.	O
Convolutional	O
neural	O
networks	O
(	O
CNNs	O
)	O
have	O
also	O
been	O
quite	O
successful	O
in	O
NLP	O
,	O
and	O
have	O
been	O
applied	O
to	O
a	O
range	O
of	O
sentence	B-TaskName
classification	I-TaskName
tasks	O
,	O
including	O
sentiment	B-TaskName
analysis	I-TaskName
(	O
Blunsom	O
et	O
al	O
,	O
2014	O
;	O
Kim	O
,	O
2014	O
;	O
Zhang	O
et	O
al	O
,	O
2015	O
)	O
.	O
Other	O
architectures	O
have	O
also	O
been	O
recently	O
proposed	O
(	O
e.g.	O
,	O
(	O
Bradbury	O
et	O
al	O
,	O
2016	O
)	O
)	O
.	O
A	O
review	O
of	O
neural	O
network	O
methods	O
for	O
NLP	O
can	O
be	O
found	O
in	O
(	O
Goldberg	O
,	O
2016	O
)	O
.	O

To	O
be	O
able	O
to	O
use	O
deep	O
learning	O
for	O
modeling	O
emotion	B-DatasetName
,	O
we	O
needed	O
a	O
large	O
dataset	O
of	O
labeled	O
tweets	O
.	O
Since	O
there	O
is	O
no	O
such	O
human	O
-	O
labeled	O
dataset	O
publicly	O
available	O
,	O
we	O
follow	O
(	O
Mohammad	O
,	O
2012	O
;	O
Mintz	O
et	O
al	O
,	O
2009	O
;	O
Purver	O
and	O
Battersby	O
,	O
2012	O
;	O
González	O
-	O
Ibánez	O
et	O
al	O
,	O
2011	O
;	O
Wang	O
et	O
al	O
,	O
2012	O
)	O
in	O
adopting	O
distant	O
supervision	O
:	O
We	O
collect	O
tweets	O
with	O
emotion	B-DatasetName
-	O
carrying	O
hashtags	O
as	O
a	O
surrogate	O
for	O
emotion	B-DatasetName
labels	O
.	O
To	O
be	O
able	O
to	O
collect	O
enough	O
tweets	O
to	O
serve	O
our	O
need	O
,	O
we	O
developed	O
a	O
list	O
of	O
hashtags	O
representing	O
each	O
of	O
the	O
24	O
emotions	O
proposed	O
by	O
Robert	O
Plutchick	O
(	O
Plutchik	O
,	O
1980	O
(	O
Plutchik	O
,	O
,	O
1985	O
(	O
Plutchik	O
,	O
,	O
1994	O
.	O
Plutchik	O
(	O
Plutchik	O
,	O
2001	O
)	O
organizes	O
emotions	O
in	O
a	O
three	O
-	O
dimensional	O
circumplex	O
model	O
analogous	O
to	O
the	O
colors	O
on	O
a	O
color	O
wheel	O
.	O
The	O
cone	O
's	O
vertical	O
dimension	O
represents	O
intensity	O
,	O
and	O
the	O
3	O
circle	O
represent	O
degrees	O
of	O
similarity	O
among	O
the	O
various	O
emotion	B-DatasetName
types	O
.	O
The	O
eight	O
sectors	O
are	O
meant	O
to	O
capture	O
that	O
there	O
are	O
eight	O
primary	O
emotion	B-DatasetName
dimensions	O
arranged	O
as	O
four	O
pairs	O
of	O
opposites	O
.	O
Emotions	O
in	O
the	O
blank	O
spaces	O
are	O
the	O
primary	O
emotion	B-DatasetName
dyads	O
(	O
i.e.	O
,	O
emotions	O
that	O
are	O
mixtures	O
of	O
two	O
of	O
the	O
primary	O
emotions	O
)	O
.	O
For	O
this	O
work	O
,	O
we	O
exclude	O
the	O
dyads	O
in	O
the	O
exploded	O
model	O
from	O
our	O
treatment	O
.	O
For	O
simplicity	O
,	O
we	O
refer	O
to	O
the	O
circles	O
as	O
plutchik	O
-	O
1	O
:	O
with	O
the	O
emotions	O
{	O
admiration	O
,	O
amazement	O
,	O
ecstasy	O
,	O
grief	O
,	O
loathing	O
,	O
rage	O
,	O
terror	O
,	O
vigilance	O
}	O
,	O
plutchik	O
-	O
2	O
:	O
with	O
the	O
emotions	O
{	O
joy	O
,	O
trust	O
,	O
fear	O
,	O
surprise	O
,	O
sadness	O
,	O
disgust	O
,	O
anger	O
,	O
anticipation	O
}	O
,	O
and	O
plutchik	O
-	O
3	O
:	O
with	O
the	O
emotions	O
{	O
acceptance	O
,	O
annoyance	O
,	O
apprehension	O
,	O
boredom	O
,	O
distraction	O
,	O
interest	O
,	O
pensiveness	O
,	O
serenity	O
}	O
.	O
The	O
wheel	O
is	O
shown	O
in	O
Figure	O
1	O
.	O
For	O
each	O
emotion	B-DatasetName
type	O
,	O
we	O
prepared	O
a	O
seed	O
set	O
of	O
hashtags	O
representing	O
the	O
emotion	B-DatasetName
.	O
We	O
used	O
Google	B-DatasetName
synonyms	O
and	O
other	O
online	O
dictionaries	O
and	O
thesauri	O
(	O
e.g.	O
,	O
www.thesaurus	O
.	O
com	O
)	O
to	O
expand	O
the	O
initial	O
seed	O
set	O
of	O
each	O
emotion	B-DatasetName
.	O
We	O
acquire	O
a	O
total	O
of	O
665	O
emotion	B-DatasetName
hashtags	O
across	O
the	O
24	O
emotion	B-DatasetName
types	O
.	O
For	O
example	O
,	O
for	O
the	O
joy	O
emotion	B-DatasetName
,	O
a	O
subset	O
of	O
the	O
seeds	B-DatasetName
in	O
our	O
expanded	O
set	O
is	O
{	O
"	O
happy	O
"	O
,	O
"	O
happiness	O
"	O
,	O
"	O
joy	O
"	O
,	O
"	O
joyful	O
"	O
,	O
"	O
joyfully	O
"	O
,	O
"	O
delighted	O
"	O
,	O
"	O
feelingsunny	O
"	O
,	O
"	O
blithe	O
"	O
,	O
"	O
beatific	O
"	O
,	O
"	O
exhilarated	O
"	O
,	O
"	O
blissful	O
"	O
,	O
"	O
walkingonair	O
"	O
,	O
"	O
jubilant	O
"	O
}	O
.	O
We	O
then	O
used	O
the	O
expanded	O
set	O
to	O
extract	O
tweets	O
with	O
hashtags	O
from	O
the	O
set	O
from	O
a	O
number	O
of	O
massive	O
-	O
scale	O
in	O
-	O
house	O
Twitter	O
datasets	O
.	O
We	O
also	O
used	O
Twitter	O
API	O
to	O
crawl	O
Twitter	O
with	O
hashtags	O
from	O
the	O
expanded	O
set	O
.	O
Using	O
this	O
method	O
,	O
we	O
were	O
able	O
to	O
acquire	O
a	O
dataset	O
of	O
about	O
1/4	O
billion	O
tweets	O
covering	O
an	O
extended	O
time	O
span	O
from	O
July	O
2009	O
till	O
January	O
2017	O
.	O

Twitter	O
data	O
are	O
very	O
noisy	O
,	O
not	O
only	O
because	O
of	O
use	O
of	O
non	O
-	O
standard	O
typography	O
(	O
which	O
is	O
less	O
of	O
a	O
problem	O
here	O
)	O
but	O
due	O
to	O
the	O
many	O
duplicate	O
tweets	O
and	O
the	O
fact	O
that	O
tweets	O
often	O
have	O
multiple	O
emotion	B-DatasetName
hashtags	O
.	O
Since	O
these	O
reduce	O
our	O
ability	O
to	O
build	O
accurate	O
models	O
,	O
we	O
need	O
to	O
clean	O
the	O
data	O
and	O
remove	O
duplicates	O
.	O
Starting	O
with	O
>	O
1/4	O
billion	O
tweets	O
,	O
we	O
employ	O
a	O
rigorous	O
and	O
strict	O
pipeline	O
.	O
This	O
results	O
in	O
a	O
vastly	O
smaller	O
set	O
of	O
about	O
1.6	O
million	O
dependable	O
labeled	O
tweets	O
.	O
Since	O
our	O
goal	O
is	O
to	O
create	O
non	O
-	O
overlapping	O
categories	O
at	O
the	O
level	O
of	O
a	O
tweet	O
,	O
we	O
first	O
removed	O
all	O
tweets	O
with	O
hashtags	O
belonging	O
to	O
more	O
than	O
one	O
emotion	B-DatasetName
of	O
the	O
24	O
emotion	B-DatasetName
categories	O
.	O
Since	O
it	O
was	O
observed	O
(	O
e.g.	O
,	O
(	O
Mohammad	O
,	O
2012	O
;	O
Wang	O
et	O
al	O
,	O
2012	O
)	O
)	O
and	O
also	O
confirmed	O
by	O
our	O
annotation	O
study	O
as	O
described	O
in	O
Section	O
4	O
,	O
that	O
hashtags	O
in	O
tweets	O
with	O
URLs	O
are	O
less	O
likely	O
to	O
correlate	O
with	O
a	O
true	O
emotion	B-DatasetName
label	O
,	O
we	O
remove	O
all	O
tweets	O
with	O
URLs	O
from	O
our	O
data	O
.	O
We	O
filter	O
out	O
duplicates	O
using	O
a	O
two	O
-	O
step	O
procedure	O
:	O
1	O
)	O
we	O
remove	O
all	O
retweets	O
(	O
based	O
on	O
existence	O
of	O
the	O
token	O
"	O
RT	O
"	O
regardless	O
of	O
case	O
)	O
and	O
2	O
)	O
we	O
use	O
the	O
Python	O
library	O
pandas	O
http://pandas	O
.	O
pydata.org/	O
"	O
drop	O
duplicates	O
"	O
method	O
to	O
compare	O
the	O
tweet	O
texts	O
of	O
all	O
the	O
tweets	O
after	O
normalizing	O
character	O
repetitions	O
[	O
all	O
consecutive	O
characters	O
of	O
>	O
2	O
to	O
2	O
]	O
and	O
user	O
mentions	O
(	O
as	O
detected	O
by	O
a	O
string	O
starting	O
with	O
an	O
"	O
@	O
"	O
sign	O
)	O
.	O
We	O
then	O
performed	O
a	O
manual	O
inspection	O
of	O
a	O
random	O
sample	O
of	O
1	O
,	O
000	O
tweets	O
from	O
the	O
data	O
and	O
found	O
no	O
evidence	O
of	O
any	O
remaining	O
tweet	O
duplicates	O
.	O
Next	O
,	O
even	O
though	O
the	O
emotion	B-DatasetName
hashtags	O
themselves	O
are	O
exclusively	O
in	O
English	O
,	O
we	O
observe	O
the	O
data	O
do	O
have	O
tweets	O
in	O
languages	O
other	O
than	O
English	O
.	O
This	O
is	O
due	O
to	O
code	O
-	O
switching	O
,	O
but	O
also	O
to	O
the	O
fact	O
that	O
our	O
data	O
dates	O
back	O
to	O
2009	O
and	O
Twitter	O
did	O
not	O
allow	O
use	O
of	O
hashtags	O
for	O
several	O
non	O
-	O
English	O
languages	O
until	O
2012	O
.	O
To	O
filter	O
out	O
non	O
-	O
English	O
,	O
we	O
use	O
the	O
langid	O
(	O
Lui	O
and	O
Baldwin	O
,	O
2012	O
)	O
(	O
https://github.com/	O
saffsd	O
/	O
langid.py	O
)	O
library	O
to	O
assign	O
language	O
tags	O
to	O
the	O
tweets	O
.	O
Since	O
the	O
common	O
wisdom	O
in	O
the	O
literature	O
(	O
e.g.	O
,	O
(	O
Mohammad	O
,	O
2012	O
;	O
Wang	O
et	O
al	O
,	O
2012	O
)	O
)	O
is	O
to	O
restrict	O
data	O
to	O
hashtags	O
occurring	O
in	O
final	O
position	O
of	O
a	O
tweet	O
,	O
we	O
investigate	O
correlations	O
between	O
a	O
tweet	O
's	O
relevance	O
and	O
emotion	B-DatasetName
hashtag	O
location	O
in	O
Section	O
4	O
and	O
test	O
models	O
exclusively	O
on	O
data	O
with	O
hashtags	O
occurring	O
in	O
final	O
position	O
.	O
We	O
also	O
only	O
use	O
tweets	O
con	O
-	O
taining	O
at	O
least	O
5	O
words	O
.	O
Table	O
2	O
shows	O
statistics	O
of	O
the	O
data	O
after	O
applying	O
our	O
cleaning	O
,	O
filtering	O
,	O
language	B-TaskName
identification	I-TaskName
,	O
and	O
deduplication	O
pipeline	O
.	O
Since	O
our	O
focus	O
is	O
on	O
English	O
,	O
we	O
only	O
show	O
statistics	O
for	O
tweets	O
tagged	O
with	O
an	O
"	O
en	O
"	O
(	O
for	O
"	O
English	O
"	O
)	O
label	O
by	O
langid	O
.	O
Table	O
2	O
provides	O
three	O
types	O
of	O
relevant	O
statistics	O
:	O
1	O
)	O
counts	O
of	O
all	O
tweets	O
,	O
2	O
)	O
counts	O
of	O
tweets	O
with	O
at	O
least	O
5	O
words	O
and	O
the	O
emotion	B-DatasetName
hashtags	O
occurring	O
in	O
the	O
last	O
quarter	O
of	O
the	O
tweet	O
text	O
(	O
based	O
on	O
character	O
count	O
)	O
,	O
and	O
3	O
)	O
counts	O
of	O
tweets	O
with	O
at	O
least	O
5	O
words	O
and	O
the	O
emotion	B-DatasetName
hashtags	O
occurring	O
as	O
the	O
final	O
word	O
in	O
the	O
tweet	O
text	O
.	O
As	O
the	O
last	O
column	O
in	O
Table	O
2	O
shows	O
,	O
employing	O
our	O
most	O
strict	O
criterion	O
where	O
an	O
emotion	B-DatasetName
hashtag	O
must	O
occur	O
finally	O
in	O
a	O
tweet	O
of	O
a	O
minimal	O
length	O
5	O
words	O
,	O
we	O
acquire	O
a	O
total	O
of	O
1	O
,	O
608	O
,	O
233	O
tweets	O
:	O
205	O
,	O
125	O
tweets	O
for	O
plutchik	O
-	O
1	O
,	O
790	O
,	O
059	O
for	O
plutchik	O
-	O
2	O
,	O
and	O
613	O
,	O
049	O
for	O
plutchik	O
-	O
3	O
.	O

For	O
our	O
core	O
modeling	O
,	O
we	O
use	O
Gated	O
Recurrent	O
Neural	O
Networks	O
(	O
GRNNs	O
)	O
,	O
a	O
modern	O
variation	O
of	O
recurrent	O
neural	O
networks	O
(	O
RNNs	O
)	O
,	O
which	O
we	O
now	O
turn	O
to	O
introduce	O
.	O
For	O
notation	O
,	O
we	O
denote	O
scalars	O
with	O
italic	O
lowercase	O
(	O
e.g.	O
,	O
x	O
)	O
,	O
vectors	O
with	O
bold	O
lowercase	O
(	O
e.g.	O
,	O
x	O
)	O
,	O
and	O
matrices	O
with	O
bold	O
uppercase	O
(	O
e.g.	O
,	O
W	O
)	O
.	O
Recurrent	O
Neural	O
Network	O
A	O
recurrent	O
neural	O
network	O
(	O
RNN	O
)	O
is	O
one	O
type	O
of	O
neural	O
network	O
architecture	O
that	O
is	O
particularly	O
suited	O
for	O
modeling	O
sequential	O
information	O
.	O
At	O
each	O
time	O
step	O
t	O
,	O
an	O
RNN	O
takes	O
an	O
input	O
vector	O
x	O
t	O
IR	O
n	O
and	O
a	O
hidden	O
state	O
vector	O
h	O
t−1	O
IR	O
m	O
and	O
produces	O
the	O
next	O
hidden	O
state	O
h	O
t	O
by	O
applying	O
the	O
recursive	O
operation	O
:	O
h	O
t	O
=	O
f	O
(	O
Wx	O
t	O
+	O
Uh	O
t−1	O
+	O
b	O
)	O
(	O
1	O
)	O
Where	O
the	O
input	O
to	O
hidden	O
matrix	O
W	O
IR	O
mxn	O
,	O
the	O
hidden	O
to	O
hidden	O
matrix	O
U	O
IR	O
mxm	O
,	O
and	O
the	O
bias	O
vector	O
b	O
IR	O
m	O
are	O
parameters	O
of	O
an	O
affine	O
transformation	O
and	O
f	O
is	O
an	O
element	O
-	O
wise	O
nonlinearity	O
.	O
While	O
an	O
RNN	O
can	O
in	O
theory	O
summarize	O
all	O
historical	O
information	O
up	O
to	O
time	O
step	O
h	O
t	O
,	O
in	O
practice	O
it	O
runs	O
into	O
the	O
problem	O
of	O
vanishing	O
/	O
exploding	O
gradients	O
(	O
Bengio	O
et	O
al	O
,	O
1994	O
;	O
Pascanu	O
et	O
al	O
,	O
2013	O
)	O
while	O
attempting	O
to	O
learn	O
longrange	O
dependencies	O
.	O
LSTM	B-MethodName
Long	B-MethodName
short	I-MethodName
-	I-MethodName
term	I-MethodName
memory	I-MethodName
(	O
LSTM	B-MethodName
)	O
networks	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
addresses	O
this	O
exact	O
problem	O
of	O
learning	O
long	O
-	O
term	O
dependencies	O
by	O
augmenting	O
an	O
RNN	O
with	O
a	O
memory	O
cell	O
c	O
t	O
IR	O
n	O
at	O
each	O
time	O
step	O
.	O
As	O
such	O
,	O
in	O
addition	O
to	O
the	O
input	O
vector	O
x	O
t	O
,	O
the	O
hiddent	O
vector	O
h	O
t−1	O
,	O
an	O
LSTM	B-MethodName
takes	O
a	O
cell	O
state	O
vector	O
c	O
t−1	O
and	O
produces	O
h	O
t	O
and	O
c	O
t	O
via	O
the	O
following	O
calculations	O
:	O
i	O
t	O
=	O
σ	O
W	O
i	O
x	O
t	O
+	O
U	O
i	O
h	O
t−1	O
+	O
b	O
i	O
f	O
t	O
=	O
σ	O
W	O
f	O
x	O
t	O
+	O
U	O
f	O
h	O
t−1	O
+	O
b	O
f	O
o	O
t	O
=	O
σ	O
(	O
W	O
o	O
x	O
t	O
+	O
U	O
o	O
h	O
t−1	O
+	O
b	O
o	O
)	O
g	O
t	O
=	O
tanh	O
(	O
W	O
g	O
x	O
t	O
+	O
U	O
g	O
h	O
t−1	O
+	O
b	O
g	O
)	O
c	O
t	O
=	O
f	O
t	O
c	O
t−1	O
+	O
i	O
t	O
g	O
t	O
h	O
t	O
=	O
o	O
t	O
tanh	O
(	O
c	O
t	O
)	O
(	O
2	O
)	O
Where	O
σ	O
(	O
)	O
and	O
tanh	O
(	O
)	O
are	O
the	O
element	O
-	O
wise	O
sigmoid	O
and	O
hyperbolic	O
tangent	O
functions	O
,	O
the	O
element	O
-	O
wise	O
multiplication	O
operator	O
,	O
and	O
i	O
t	O
,	O
f	O
t	O
,	O
o	O
t	O
are	O
the	O
input	O
,	O
forget	O
,	O
and	O
output	O
gates	O
.	O
The	O
g	O
t	O
is	O
a	O
new	O
memory	O
cell	O
vector	O
with	O
candidates	O
that	O
could	O
be	O
added	O
to	O
the	O
state	O
.	O
The	O
LSTM	B-MethodName
parameters	O
W	O
j	O
,	O
U	O
j	O
,	O
and	O
b	O
j	O
are	O
for	O
j	O
{	O
i	O
,	O
f	O
,	O
o	O
,	O
g	O
}	O
.	O
GRNNs	O
(	O
Cho	O
et	O
al	O
,	O
2014	O
;	O
Chung	O
et	O
al	O
,	O
2015	O
)	O
propose	O
a	O
variation	O
of	O
LSTM	B-MethodName
with	O
a	O
reset	O
gate	O
r	O
t	O
,	O
an	O
update	O
state	O
z	O
t	O
,	O
and	O
a	O
new	O
simpler	O
hidden	O
unit	O
h	O
t	O
,	O
as	O
follows	O
:	O
r	O
t	O
=	O
σ	O
(	O
W	O
r	O
x	O
t	O
+	O
U	O
r	O
h	O
t−1	O
+	O
b	O
r	O
)	O
z	O
t	O
=	O
σ	O
(	O
W	O
z	O
x	O
t	O
+	O
U	O
z	O
h	O
t−1	O
+	O
b	O
z	O
)	O
h	O
t	O
=	O
tanh	O
Wx	O
t	O
+	O
r	O
t	O
*	O
Uhh	O
t−1	O
+	O
bh	O
h	O
t	O
=	O
z	O
t	O
*	O
h	O
t−1	O
+	O
(	O
1	O
−	O
z	O
t	O
)	O
*	O
h	O
t	O
(	O
3	O
)	O
The	O
GRNN	O
parameters	O
W	O
j	O
,	O
U	O
j	O
,	O
and	O
b	O
j	O
are	O
for	O
j	O
{	O
r	O
,	O
z	O
,	O
h	O
}	O
.	O
In	O
this	O
set	O
up	O
,	O
the	O
hidden	O
state	O
is	O
forced	O
to	O
ignore	O
a	O
previous	O
hidden	O
state	O
when	O
the	O
reset	O
gate	O
is	O
close	O
to	O
0	B-DatasetName
,	O
thus	O
enabling	O
the	O
network	O
to	O
forget	O
or	O
drop	O
irrelevant	O
information	O
.	O
Additionally	O
,	O
the	O
update	O
gate	O
controls	O
how	O
much	O
information	O
carries	O
over	O
from	O
a	O
previous	O
hidden	O
state	O
to	O
the	O
current	O
hidden	O
state	O
(	O
similar	O
to	O
an	O
LSTM	B-MethodName
memory	O
cell	O
)	O
.	O
We	O
use	O
GRNNs	O
as	O
they	O
are	O
simpler	O
and	O
faster	O
than	O
LSTM	B-MethodName
.	O
For	O
GRNNs	O
,	O
we	O
use	O
Theano	O
(	O
Theano	O
Development	O
Team	O
,	O
2016	O
)	O
.	O
Online	O
Classifiers	O
We	O
compare	O
the	O
performance	O
of	O
the	O
GRNNs	O
to	O
four	O
online	O
classifiers	O
that	O
are	O
capable	O
of	O
handling	O
the	O
data	O
size	O
:	O
Stochastic	B-MethodName
Gradient	I-MethodName
Descent	I-MethodName
(	O
SGD	B-MethodName
)	O
,	O
Multinomial	O
Naive	O
Bayes	O
(	O
MNB	O
)	O
,	O
Perceptron	O
,	O
and	O
the	O
Passive	O
Agressive	O
Classifier	O
(	O
PAC	O
)	O
.	O
These	O
classifiers	O
learn	O
online	O
from	O
mini	O
-	O
batches	O
of	O
data	O
.	O
We	O
use	O
minibatches	O
of	O
10	O
,	O
000	O
instances	O
with	O
all	O
the	O
four	O
classifiers	O
.	O
We	O
use	O
the	O
scikit	O
-	O
learn	O
implementation	O
of	O
these	O
classifiers	O
(	O
http://scikit	O
-	O
learn	O
.	O
org	O
)	O
.	O
Settings	O
We	O
aim	O
to	O
model	O
Plutchik	O
's	O
24	O
finegrained	O
emotions	O
as	O
well	O
as	O
his	O
8	O
primary	O
emotion	B-DatasetName
dimensions	O
where	O
each	O
3	O
related	O
types	O
of	O
emotion	B-DatasetName
(	O
perceived	O
as	O
varying	O
in	O
intensity	O
)	O
are	O
combined	O
in	O
one	O
dimension	O
.	O
We	O
now	O
turn	O
to	O
describing	O
our	O
experiments	O
experiments	O
.	O

In	O
this	O
paper	O
,	O
we	O
built	O
a	O
large	O
,	O
automatically	O
curated	O
dataset	O
for	O
emotion	B-DatasetName
detection	O
using	O
distant	O
supervision	O
and	O
then	O
used	O
GRNNs	O
to	O
model	O
finegrained	O
emotion	B-DatasetName
,	O
achieving	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
.	O
We	O
also	O
extended	O
the	O
classification	O
to	O
8	O
primary	O
emotion	B-DatasetName
dimensions	O
situated	O
in	O
psychological	O
theory	O
of	O
emotion	B-DatasetName
.	O

Emotion	O
lexicons	O
describe	O
the	O
affective	O
meaning	O
of	O
words	O
and	O
thus	O
constitute	O
a	O
centerpiece	O
for	O
advanced	O
sentiment	O
and	O
emotion	B-DatasetName
analysis	O
.	O
Yet	O
,	O
manually	O
curated	O
lexicons	O
are	O
only	O
available	O
for	O
a	O
handful	O
of	O
languages	O
,	O
leaving	O
most	O
languages	O
of	O
the	O
world	O
without	O
such	O
a	O
precious	O
resource	O
for	O
downstream	O
applications	O
.	O
Even	O
worse	O
,	O
their	O
coverage	O
is	O
often	O
limited	O
both	O
in	O
terms	O
of	O
the	O
lexical	O
units	O
they	O
contain	O
and	O
the	O
emotional	O
variables	O
they	O
feature	O
.	O
In	O
order	O
to	O
break	O
this	O
bottleneck	O
,	O
we	O
here	O
introduce	O
a	O
methodology	O
for	O
creating	O
almost	O
arbitrarily	O
large	O
emotion	B-DatasetName
lexicons	O
for	O
any	O
target	O
language	O
.	O
Our	O
approach	O
requires	O
nothing	O
but	O
a	O
source	O
language	O
emotion	B-DatasetName
lexicon	O
,	O
a	O
bilingual	O
word	B-TaskName
translation	I-TaskName
model	O
,	O
and	O
a	O
target	O
language	O
embedding	O
model	O
.	O
Fulfilling	O
these	O
requirements	O
for	O
91	O
languages	O
,	O
we	O
are	O
able	O
to	O
generate	O
representationally	O
rich	O
high	O
-	O
coverage	O
lexicons	O
comprising	O
eight	O
emotional	O
variables	O
with	O
more	O
than	O
100k	O
lexical	O
entries	O
each	O
.	O
We	O
evaluated	O
the	O
automatically	O
generated	O
lexicons	O
against	O
human	O
judgment	O
from	O
26	O
datasets	O
,	O
spanning	O
12	O
typologically	O
diverse	O
languages	O
,	O
and	O
found	O
that	O
our	O
approach	O
produces	O
results	O
in	O
line	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
monolingual	O
approaches	O
to	O
lexicon	O
creation	O
and	O
even	O
surpasses	O
human	O
reliability	O
for	O
some	O
languages	O
and	O
variables	O
.	O
Code	O
and	O
data	O
are	O
available	O
at	O
github.com/JULIELab/MEmoLon	O
archived	O
under	O

An	O
emotion	B-DatasetName
lexicon	O
is	O
a	O
lexical	O
repository	O
which	O
encodes	O
the	O
affective	O
meaning	O
of	O
individual	O
words	O
(	O
lexical	O
entries	O
)	O
.	O
Most	O
simply	O
,	O
affective	O
meaning	O
can	O
be	O
encoded	O
in	O
terms	O
of	O
polarity	O
,	O
i.e.	O
,	O
the	O
distinction	O
whether	O
an	O
item	O
is	O
considered	O
as	O
positive	O
,	O
negative	O
,	O
or	O
neutral	O
.	O
This	O
is	O
the	O
case	O
for	O
many	O
well	O
-	O
known	O
resources	O
such	O
as	O
WORDNET	O
-	O
AFFECT	O
(	O
Strapparava	O
and	O
Valitutti	O
,	O
2004	O
)	O
,	O
SENTIWORD	O
-	O
NET	O
(	O
Baccianella	O
et	O
al	O
,	O
2010	O
)	O
,	O
or	O
VADER	O
(	O
Hutto	O
and	O
Gilbert	O
,	O
2014	O
)	O
.	O
Yet	O
,	O
an	O
increasing	O
number	O
of	O
researchers	O
focus	O
on	O
more	O
expressive	O
encodings	O
for	O
affective	O
states	O
inspired	O
by	O
distinct	O
lines	O
of	O
work	O
in	O
psychology	O
Buechel	O
and	O
Hahn	O
,	O
2017	O
;	O
Sedoc	O
et	O
al	O
,	O
2017	O
;	O
Abdul	O
-	O
Mageed	O
and	O
Ungar	O
,	O
2017	O
;	O
Bostan	O
and	O
Klinger	O
,	O
2018	O
;	O
Mohammad	O
,	O
2018	O
;	O
Troiano	O
et	O
al	O
,	O
2019	O
)	O
.	O
Psychologists	O
,	O
on	O
the	O
one	O
hand	O
,	O
value	O
such	O
lexicons	O
as	O
a	O
controlled	O
set	O
of	O
stimuli	O
for	O
designing	O
experiments	O
,	O
e.g.	O
,	O
to	O
investigate	O
patterns	O
of	O
lexical	O
access	O
or	O
the	O
structure	O
of	O
memory	O
Monnier	O
and	O
Syssau	O
,	O
2008	O
)	O
.	O
NLP	O
researchers	O
,	O
on	O
the	O
other	O
hand	O
,	O
use	O
them	O
to	O
augment	O
the	O
emotional	O
loading	O
of	O
word	B-TaskName
embeddings	I-TaskName
(	O
Yu	O
et	O
al	O
,	O
2017	O
;	O
Khosla	O
et	O
al	O
,	O
2018	O
)	O
,	O
as	O
additional	O
input	O
to	O
sentence	O
-	O
level	O
emotion	B-DatasetName
models	O
so	O
that	O
the	O
performance	O
of	O
even	O
the	O
most	O
sophisticated	O
neural	O
network	O
gets	O
boosted	O
(	O
Mohammad	O
and	O
Bravo	O
-	O
Marquez	O
,	O
2017	O
;	O
De	O
Bruyne	O
et	O
al	O
,	O
2019	O
)	O
,	O
or	O
rely	O
on	O
them	O
in	O
a	O
keyword	O
-	O
spotting	O
approach	O
when	O
no	O
training	O
data	O
is	O
available	O
,	O
e.g.	O
,	O
for	O
studies	O
dealing	O
with	O
historical	O
language	O
stages	O
.	O
As	O
with	O
any	O
kind	O
of	O
manually	O
curated	O
resource	O
,	O
the	O
availability	O
of	O
emotion	B-DatasetName
lexicons	O
is	O
heavily	O
restricted	O
to	O
only	O
a	O
few	O
languages	O
whose	O
exact	O
number	O
varies	O
depending	O
on	O
the	O
variables	O
under	O
scrutiny	O
.	O
For	O
example	O
,	O
we	O
are	O
aware	O
of	O
lexicons	O
for	O
15	O
languages	O
that	O
encode	O
the	O
emotional	O
variables	O
of	O
Valence	O
,	O
Arousal	O
,	O
and	O
Dominance	O
(	O
see	O
Section	O
2	O
)	O
.	O
This	O
number	O
leaves	O
the	O
majority	O
of	O
the	O
world	O
's	O
(	O
less	O
-	O
resourced	O
)	O
languages	O
without	O
such	O
a	O
dataset	O
.	O
In	O
case	O
such	O
a	O
lexicon	O
exists	O
for	O
a	O
particular	O
language	O
,	O
it	O
is	O
often	O
severely	O
limited	O
in	O
size	O
,	O
sometimes	O
only	O
comprising	O
some	O
hundreds	O
of	O
entries	O
(	O
Davidson	O
and	O
Innes	O
-	O
Ker	O
,	O
2014	O
)	O
.	O
Yet	O
,	O
even	O
the	O
largest	O
lexicons	O
typically	O
cover	O
only	O
some	O
ten	O
thousands	O
of	O
words	O
,	O
still	O
leaving	O
out	O
major	O
portions	O
of	O
the	O
emotion	B-DatasetName
-	O
carrying	O
vocabulary	O
.	O
This	O
is	O
especially	O
true	O
for	O
languages	O
with	O
complex	O
morphology	O
or	O
productive	O
compounding	O
,	O
such	O
as	O
Finnish	O
,	O
Turkish	O
,	O
Czech	O
,	O
or	O
German	O
.	O
Finally	O
,	O
the	O
diversity	O
of	O
emotion	B-DatasetName
representation	O
schemes	O
adds	O
another	O
layer	O
of	O
complexity	O
.	O
While	O
psychologists	O
and	O
NLP	O
researchers	O
alike	O
find	O
that	O
different	O
sets	O
of	O
emotional	O
variables	O
are	O
complementary	O
to	O
each	O
other	O
(	O
Stevenson	O
et	O
al	O
,	O
2007	O
;	O
Pinheiro	O
et	O
al	O
,	O
2017	O
;	O
Barnes	O
et	O
al	O
,	O
2019	O
;	O
De	O
Bruyne	O
et	O
al	O
,	O
2019	O
)	O
,	O
manually	O
creating	O
emotion	B-DatasetName
lexicons	O
for	O
every	O
language	O
and	O
every	O
emotion	B-DatasetName
representation	O
scheme	O
is	O
virtually	O
impossible	O
.	O
We	O
here	O
propose	O
an	O
approach	O
based	O
on	O
crosslingual	O
distant	O
supervision	O
to	O
generate	O
almost	O
arbitrarily	O
large	O
emotion	B-DatasetName
lexicons	O
for	O
any	O
target	O
language	O
and	O
emotional	O
variable	O
,	O
provided	O
the	O
following	O
requirements	O
are	O
met	O
:	O
a	O
source	O
language	O
emotion	B-DatasetName
lexicon	O
covering	O
the	O
desired	O
variables	O
,	O
a	O
bilingual	O
word	B-TaskName
translation	I-TaskName
model	O
,	O
and	O
a	O
target	O
language	O
embedding	O
model	O
.	O
By	O
fulfilling	O
these	O
preconditions	O
,	O
we	O
can	O
automatically	O
generate	O
emotion	B-DatasetName
lexicons	O
for	O
91	O
languages	O
covering	O
ratings	O
for	O
eight	O
emotional	O
variables	O
and	O
hundreds	O
of	O
thousands	O
of	O
lexical	O
entries	O
each	O
.	O
Our	O
experiments	O
reveal	O
that	O
our	O
method	O
is	O
on	O
a	O
par	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
monolingual	O
approaches	O
and	O
compares	O
favorably	O
with	O
(	O
sometimes	O
even	O
outperforms	O
)	O
human	O
reliability	O
.	O

Representing	O
Emotion	O
.	O
Whereas	O
research	O
in	O
NLP	O
has	O
focused	O
for	O
a	O
very	O
long	O
time	O
almost	O
exclusively	O
on	O
polarity	O
,	O
more	O
recently	O
,	O
there	O
has	O
been	O
a	O
growing	O
interest	O
in	O
more	O
informative	O
representation	O
structures	O
for	O
affective	O
states	O
by	O
including	O
different	O
groups	O
of	O
emotional	O
variables	O
(	O
Bostan	O
and	O
Klinger	O
,	O
2018	O
)	O
.	O
Borrowing	O
from	O
distinct	O
schools	O
of	O
thought	O
in	O
psychology	O
,	O
these	O
variables	O
can	O
typically	O
be	O
subdivided	O
into	O
dimensional	O
vs.	O
discrete	O
approaches	O
to	O
emotion	B-DatasetName
representation	O
(	O
Calvo	O
and	O
Mac	O
Kim	O
,	O
2013	O
)	O
.	O
The	O
dimensional	O
approach	O
assumes	O
that	O
emotional	O
states	O
can	O
be	O
composed	O
out	O
of	O
several	O
foundational	O
factors	O
,	O
most	O
noticeably	O
Valence	O
(	O
corresponding	O
to	O
polarity	O
)	O
,	O
Arousal	O
(	O
measuring	O
calmness	O
vs.	O
excitement	O
)	O
,	O
and	O
Dominance	O
(	O
the	O
perceived	O
degree	O
of	O
control	O
in	O
a	O
social	O
situation	O
)	O
;	O
VAD	O
,	O
for	O
short	O
(	O
Bradley	O
and	O
Lang	O
,	O
1994	O
)	O
.	O
Conversely	O
,	O
the	O
discrete	O
approach	O
assumes	O
that	O
emotional	O
states	O
can	O
be	O
reduced	O
to	O
a	O
small	O
,	O
evolutionary	O
motivated	O
set	O
of	O
basic	O
emotions	O
(	O
Ekman	O
,	O
1992	O
)	O
.	O
Although	O
the	O
exact	O
division	O
of	O
the	O
set	O
has	O
been	O
subject	O
of	O
hot	O
debates	O
,	O
recently	O
constructed	O
datasets	O
(	O
see	O
Section	O
4	O
)	O
most	O
often	O
cover	O
the	O
categories	O
of	O
Joy	O
,	O
Anger	O
,	O
Sadness	O
,	O
Fear	O
,	O
and	O
Disgust	O
;	O
BE5	O
,	O
for	O
short	O
.	O
Plutchik	O
's	O
Wheel	O
of	O
Emotion	O
takes	O
a	O
middle	O
ground	O
between	O
those	O
two	O
positions	O
by	O
postulating	O
emotional	O
categories	O
which	O
are	O
yet	O
grouped	O
into	O
opposite	O
pairs	O
along	O
different	O
levels	O
of	O
intensity	O
(	O
Plutchik	O
,	O
1980	O
)	O
.	O
Another	O
dividing	O
line	O
between	O
representational	O
approaches	O
is	O
whether	O
target	O
variables	O
are	O
encoded	O
in	O
terms	O
of	O
(	O
strict	O
)	O
class	O
-	O
membership	O
or	O
scores	O
for	O
numerical	O
strength	O
.	O
In	O
the	O
first	O
case	O
,	O
emotion	B-DatasetName
analysis	O
translates	O
into	O
a	O
(	O
multi	O
-	O
class	O
)	O
classification	O
problem	O
,	O
whereas	O
the	O
latter	O
turns	O
it	O
into	O
a	O
regression	O
problem	O
.	O
While	O
our	O
proposed	O
methodology	O
is	O
agnostic	O
towards	O
the	O
chosen	O
emotion	B-DatasetName
format	O
,	O
we	O
will	O
focus	O
on	O
the	O
VAD	O
and	O
BE5	O
formats	O
here	O
,	O
using	O
numerical	O
ratings	O
(	O
see	O
the	O
examples	O
in	O
Table	O
1	O
)	O
due	O
to	O
the	O
widespread	O
availability	O
of	O
such	O
data	O
.	O
Accordingly	O
,	O
this	O
paper	O
treats	O
word	O
emotion	B-DatasetName
prediction	O
as	O
a	O
regression	O
problem	O
.	O
1.6	O
7.4	O
2.7	O
1.2	O
2.9	O
3.3	O
3.9	O
2.5	O
nuclear	O
4.3	O
7.3	O
4.1	O
1.4	O
2.2	O
1.9	O
3.2	O
1.6	O
ownership	O
5.9	O
4.4	O
7.5	O
2.1	O
1.4	O
1.2	O
1.4	O
1.3	O
Fear	O
,	O
and	O
Disgust	O
[	O
BE5	O
]	O
.	O
VAD	O
uses	O
1	O
-	O
to	O
-	O
9	O
scales	O
(	O
"	O
5	O
"	O
encodes	O
the	O
neutral	O
value	O
)	O
and	O
BE5	O
1	O
-	O
to	O
-	O
5	O
scales	O
(	O
"	O
1	O
"	O
encodes	O
the	O
neutral	O
value	O
)	O
.	O
Building	O
Emotion	O
Lexicons	O
.	O
Usually	O
,	O
the	O
ground	O
truth	O
for	O
affective	O
word	O
ratings	O
(	O
i.e.	O
,	O
the	O
assignment	O
of	O
emotional	O
values	O
to	O
a	O
lexical	O
item	O
)	O
is	O
acquired	O
in	O
a	O
questionnaire	O
study	O
design	O
where	O
subjects	O
(	O
annotators	O
)	O
receive	O
lists	O
of	O
words	O
which	O
they	O
rate	O
according	O
to	O
different	O
emotion	B-DatasetName
variables	O
or	O
categories	O
.	O
Aggregating	O
individual	O
ratings	O
of	O
multiple	O
annotators	O
then	O
results	O
in	O
the	O
final	O
emotion	B-DatasetName
lexicon	O
(	O
Bradley	O
and	O
Lang	O
,	O
1999	O
)	O
.	O
Recently	O
,	O
this	O
workflow	O
has	O
often	O
been	O
enhanced	O
by	O
crowdsourcing	O
(	O
Mohammad	O
and	O
Turney	O
,	O
2013	O
)	O
and	O
best	O
-	O
worst	O
scaling	O
(	O
Kiritchenko	O
and	O
Mohammad	O
,	O
2016	O
)	O
.	O
As	O
a	O
viable	O
alternative	O
to	O
manual	O
acquisition	O
,	O
such	O
lexicons	O
can	O
also	O
be	O
created	O
by	O
automatic	O
means	O
(	O
Bestgen	O
,	O
2008	O
;	O
Köper	O
and	O
Schulte	O
i	O
m	O
Walde	O
,	O
2016	O
;	O
Shaikh	O
et	O
al	O
,	O
2016	O
)	O
,	O
i.e.	O
,	O
by	O
learning	O
to	O
predict	O
emotion	B-DatasetName
labels	O
for	O
unseen	O
words	O
.	O
Researchers	O
have	O
worked	O
on	O
this	O
prediction	O
problem	O
for	O
quite	O
a	O
long	O
time	O
.	O
Early	O
work	O
tended	O
to	O
focus	O
on	O
word	O
statistics	O
,	O
often	O
in	O
combination	O
with	O
linguistic	O
rules	O
(	O
Hatzivassiloglou	O
and	O
McKeown	O
,	O
1997	O
;	O
Turney	O
and	O
Littman	O
,	O
2003	O
)	O
.	O
More	O
recent	O
approaches	O
focus	O
heavily	O
on	O
word	B-TaskName
embeddings	I-TaskName
,	O
either	O
using	O
semi	O
-	O
supervised	O
graph	O
-	O
based	O
approaches	O
Hamilton	O
et	O
al	O
,	O
2016	O
;	O
Sedoc	O
et	O
al	O
,	O
2017	O
)	O
or	O
fully	O
supervised	O
methods	O
(	O
Rosenthal	O
et	O
al	O
,	O
2015	O
;	O
Li	O
et	O
al	O
,	O
2017	O
;	O
Rothe	O
et	O
al	O
,	O
2016	O
;	O
Du	O
and	O
Zhang	O
,	O
2016	O
)	O
.	O
Most	O
important	O
for	O
this	O
work	O
,	O
Buechel	O
and	O
Hahn	O
(	O
2018b	O
)	O
report	O
on	O
near	O
-	O
human	O
performance	O
using	O
a	O
combination	O
of	O
FASTTEXT	B-MethodName
vectors	O
and	O
a	O
multi	O
-	O
task	O
feed	O
-	O
forward	O
network	O
(	O
see	O
Section	O
4	O
)	O
.	O
While	O
this	O
line	O
of	O
work	O
can	O
add	O
new	O
words	O
,	O
it	O
does	O
not	O
extend	O
lexicons	O
to	O
other	O
emotional	O
variables	O
or	O
languages	O
.	O
A	O
relatively	O
new	O
way	O
of	O
generating	O
novel	O
labels	O
is	O
emotion	B-DatasetName
representation	O
mapping	O
(	O
ERM	O
)	O
,	O
an	O
annotation	O
projection	O
that	O
translates	O
ratings	O
from	O
one	O
emotion	B-DatasetName
format	O
into	O
another	O
,	O
e.g.	O
,	O
mapping	O
VAD	O
labels	O
into	O
BE5	O
,	O
or	O
vice	O
versa	O
(	O
Hoffmann	O
et	O
al	O
,	O
2012	O
;	O
Hahn	O
,	O
2016	O
,	O
2018a	O
;	O
Alarcão	O
and	O
Fonseca	O
,	O
2017	O
;	O
Landowska	O
,	O
2018	O
;	O
Zhou	O
et	O
al	O
,	O
2020	O
;	O
Park	O
et	O
al	O
,	O
2019	O
)	O
.	O
While	O
our	O
work	O
uses	O
ERM	O
to	O
add	O
additional	O
emotion	B-DatasetName
variables	O
to	O
the	O
source	O
lexicon	O
,	O
ERM	O
alone	O
can	O
neither	O
increase	O
the	O
coverage	O
of	O
a	O
lexicon	O
,	O
nor	O
adapt	O
it	O
to	O
another	O
language	O
.	O
Translating	O
Emotions	O
.	O
The	O
approach	O
we	O
propose	O
is	O
strongly	O
tied	O
to	O
the	O
observation	O
by	O
Leveau	O
et	O
al	O
(	O
2012	O
)	O
and	O
Warriner	O
et	O
al	O
(	O
2013	O
)	O
who	O
found	O
-	O
comparing	O
a	O
large	O
number	O
of	O
existing	O
emotion	B-DatasetName
lexicons	O
of	O
different	O
languages	O
-	O
that	O
translational	O
equivalents	O
of	O
words	O
show	O
strong	O
stability	O
and	O
adherence	O
to	O
their	O
emotional	O
value	O
.	O
Yet	O
,	O
their	O
work	O
is	O
purely	O
descriptive	O
.	O
They	O
do	O
not	O
exploit	O
their	O
observation	O
to	O
create	O
new	O
ratings	O
,	O
and	O
only	O
consider	O
manual	O
rather	O
than	O
automatic	O
translation	O
.	O
Making	O
indirect	O
use	O
of	O
this	O
observation	O
,	O
Mohammad	O
and	O
Turney	O
(	O
2013	O
)	O
offer	O
machine	O
-	O
translated	O
versions	O
of	O
their	O
NRC	O
Emotion	O
Lexicon	O
.	O
Also	O
,	O
many	O
approaches	O
in	O
cross	O
-	O
lingual	O
sentiment	B-TaskName
analysis	I-TaskName
(	O
on	O
the	O
sentence	O
-	O
level	O
)	O
rely	O
on	O
translating	O
polarity	O
lexicons	O
(	O
Abdalla	O
and	O
Hirst	O
,	O
2017	O
;	O
Barnes	O
et	O
al	O
,	O
2018	O
)	O
.	O
Perhaps	O
most	O
similar	O
to	O
our	O
work	O
,	O
Chen	O
and	O
Skiena	O
(	O
2014	O
)	O
create	O
(	O
polarity	O
-	O
only	O
)	O
lexicons	O
for	O
136	O
languages	O
by	O
building	O
a	O
multilingual	O
word	O
graph	O
and	O
propagating	O
sentiment	O
labels	O
through	O
that	O
graph	O
.	O
Yet	O
,	O
their	O
method	O
is	O
restricted	O
to	O
high	O
frequency	O
words	O
-	O
their	O
lexicons	O
cover	O
between	O
12	O
and	O
4	O
,	O
653	O
entries	O
,	O
whereas	O
our	O
approach	O
exceeds	O
this	O
limit	O
by	O
more	O
than	O
two	O
orders	O
of	O
magnitude	O
.	O
Our	O
methodology	O
also	O
resembles	O
previous	O
work	O
which	O
models	O
word	O
emotion	B-DatasetName
for	O
historical	O
language	O
stages	O
(	O
Cook	O
and	O
Stevenson	O
,	O
2010	O
;	O
Hamilton	O
et	O
al	O
,	O
2016	O
;	O
Hellrich	O
et	O
al	O
,	O
2018	O
;	O
Li	O
et	O
al	O
,	O
2019	O
)	O
.	O
Work	O
in	O
this	O
direction	O
typically	O
comes	O
up	O
with	O
a	O
set	O
of	O
seed	O
words	O
with	O
assumingly	O
temporally	O
stable	O
affective	O
meaning	O
(	O
our	O
work	O
assumes	O
stability	O
against	O
translation	O
)	O
and	O
then	O
uses	O
distributional	O
methods	O
to	O
derive	O
emotion	B-DatasetName
ratings	O
in	O
the	O
target	O
language	O
stage	O
.	O
However	O
,	O
gold	O
data	O
for	O
the	O
target	O
language	O
(	O
stage	O
)	O
is	O
usually	O
inaccessible	O
,	O
often	O
preventing	O
evaluation	O
against	O
human	O
judgment	O
.	O
In	O
contrast	O
,	O
we	O
here	O
propose	O
several	O
alternative	O
evaluation	O
set	O
-	O
ups	O
as	O
an	O
integral	O
part	O
of	O
our	O
methodology	O
.	O

Our	O
methodology	O
integrates	O
(	O
1	O
)	O
cross	O
-	O
lingual	O
generation	O
and	O
expansion	O
of	O
emotion	B-DatasetName
lexicons	O
and	O
(	O
2	O
)	O
their	O
evaluation	O
against	O
gold	O
and	O
silver	O
standard	O
data	O
.	O
Consequently	O
,	O
a	O
key	O
aspect	O
of	O
our	O
workflow	O
design	O
is	O
how	O
data	O
is	O
split	O
into	O
train	O
,	O
dev	O
,	O
and	O
test	O
sets	O
at	O
different	O
points	O
of	O
the	O
generation	O
process	O
.	O
Figure	O
1	O
gives	O
an	O
overview	O
of	O
our	O
framework	O
including	O
a	O
toy	O
example	O
for	O
illustration	O
.	O
Lexicon	O
Generation	O
.	O
We	O
start	O
with	O
a	O
lexicon	O
(	O
Source	O
)	O
of	O
arbitrary	O
size	O
,	O
emotion	B-DatasetName
format	O
1	O
and	O
source	O
language	O
which	O
is	O
partitioned	O
into	O
train	O
,	O
dev	O
,	O
and	O
test	O
splits	O
denoted	O
by	O
Source	O
-	O
train	O
,	O
Source	O
-	O
dev	O
,	O
and	O
Source	O
-	O
test	O
,	O
respectively	O
.	O
Next	O
,	O
we	O
leverage	O
a	O
bilingual	O
word	B-TaskName
translation	I-TaskName
model	O
between	O
source	O
and	O
desired	O
target	O
language	O
to	O
build	O
the	O
first	O
target	O
-	O
side	O
emotion	B-DatasetName
lexicon	O
denoted	O
as	O
TargetMT	O
.	O
Source	O
words	O
are	O
translated	O
according	O
to	O
the	O
model	O
,	O
whereas	O
target	O
-	O
side	O
emotion	B-DatasetName
labels	O
are	O
simply	O
copied	O
from	O
the	O
source	O
to	O
the	O
target	O
(	O
see	O
Section	O
2	O
)	O
.	O
Entries	O
are	O
assigned	O
to	O
train	O
,	O
dev	O
,	O
or	O
test	O
set	O
according	O
to	O
their	O
source	O
-	O
side	O
assignment	O
(	O
cf	O
.	O
Figure	O
1	O
)	O
.	O
The	O
choice	O
of	O
our	O
translation	O
service	O
(	O
see	O
below	O
)	O
ensures	O
that	O
each	O
source	O
word	O
receives	O
exactly	O
one	O
translation	O
.	O
TargetMT	O
is	O
then	O
used	O
as	O
the	O
distant	O
supervisor	O
to	O
train	O
a	O
model	O
that	O
predicts	O
word	O
emotions	O
based	O
on	O
target	O
-	O
side	O
word	B-TaskName
embeddings	I-TaskName
.	O
TargetMT	O
-	O
train	O
and	O
TargetMT	O
-	O
dev	O
are	O
used	O
to	O
fit	O
model	O
parameters	O
and	O
optimize	O
hyperparameters	O
,	O
respectively	O
,	O
whereas	O
TargetMT	O
-	O
test	O
is	O
held	O
out	O
for	O
later	O
evaluation	O
.	O
Once	O
finalized	O
,	O
the	O
model	O
is	O
used	O
to	O
predict	O
new	O
labels	O
for	O
the	O
words	O
in	O
TargetMT	O
,	O
resulting	O
in	O
a	O
second	O
target	O
-	O
side	O
emotion	B-DatasetName
lexicon	O
denoted	O
TargetPred	O
.	O
Our	O
rationale	O
for	O
doing	O
so	O
is	O
that	O
a	O
reasonably	O
trained	O
model	O
should	O
generalize	O
well	O
(	O
4.3	O
,	O
7.3	O
)	O
)	O
test	O
(	O
Terrorismus	O
,	O
(	O
1.6	O
,	O
7.4	O
)	O
:	O
Schematic	O
view	O
on	O
the	O
methodology	O
for	O
generating	O
and	O
evaluating	O
an	O
emotion	B-DatasetName
lexicon	O
for	O
a	O
given	O
target	O
language	O
based	O
on	O
source	O
language	O
supervision	O
.	O
Included	O
is	O
a	O
toy	O
example	O
starting	O
with	O
an	O
English	O
VA	O
lexicon	O
(	O
sunshine	O
,	O
nuclear	O
,	O
terrorism	O
and	O
the	O
associated	O
numerical	O
scores	O
for	O
Valence	O
and	O
Arousal	O
)	O
and	O
resulting	O
in	O
an	O
extended	O
German	O
lexicon	O
which	O
incorporates	O
translated	O
entries	O
with	O
altered	O
VA	O
scores	O
and	O
additional	O
entries	O
originating	O
from	O
the	O
embedding	O
model	O
with	O
newly	O
learned	O
scores	O
.	O
over	O
the	O
entire	O
TargetMT	O
lexicon	O
because	O
it	O
has	O
access	O
to	O
the	O
target	O
-	O
side	O
embedding	O
vectors	O
.	O
Hence	O
,	O
it	O
may	O
mitigate	O
some	O
of	O
the	O
errors	O
which	O
were	O
introduced	O
in	O
previous	O
steps	O
,	O
either	O
by	O
machine	B-TaskName
translation	I-TaskName
or	O
by	O
assuming	O
that	O
sourceand	O
target	O
-	O
side	O
emotion	B-DatasetName
are	O
always	O
identical	O
.	O
We	O
validate	O
this	O
assumption	O
in	O
Section	O
6	O
.	O
We	O
also	O
predict	O
ratings	O
for	O
all	O
the	O
words	O
in	O
the	O
embedding	O
model	O
,	O
leading	O
to	O
a	O
large	O
number	O
of	O
new	O
entries	O
.	O
The	O
splits	O
are	O
defined	O
as	O
follows	O
:	O
let	O
M	O
T	O
train	O
,	O
M	O
T	O
dev	O
,	O
and	O
M	O
T	O
test	O
denote	O
the	O
set	O
of	O
words	O
in	O
train	O
,	O
dev	O
,	O
and	O
test	O
split	O
of	O
TargetMT	O
,	O
respectively	O
.	O
Likewise	O
,	O
let	O
P	O
train	O
,	O
P	O
dev	O
,	O
and	O
P	O
test	O
denote	O
the	O
splits	O
of	O
TargetPred	O
and	O
let	O
E	O
denote	O
the	O
set	O
of	O
words	O
in	O
the	O
embedding	O
model	O
.	O
Then	O
P	O
train	O
:	O
=	O
M	O
T	O
train	O
P	O
dev	O
:	O
=	O
M	O
T	O
dev	O
\	O
M	O
T	O
train	O
P	O
test	O
:	O
=	O
(	O
M	O
T	O
test	O
∪	O
E	O
)	O
\	O
(	O
M	O
T	O
dev	O
∪	O
M	O
T	O
train	O
)	O
The	O
above	O
definitions	O
help	O
clarify	O
the	O
way	O
we	O
address	O
polysemy	O
.	O
2	O
Ambiguity	O
on	O
the	O
target	O
-	O
side	O
2	O
In	O
short	O
,	O
our	O
work	O
evades	O
this	O
problem	O
by	O
dealing	O
with	O
lexical	O
entries	O
exclusively	O
on	O
the	O
type	O
-	O
rather	O
than	O
the	O
senselevel	O
.	O
From	O
a	O
lexicological	O
perspective	O
,	O
this	O
may	O
seem	O
like	O
a	O
strong	O
assumption	O
.	O
From	O
a	O
modeling	O
perspective	O
,	O
however	O
,	O
it	O
appears	O
almost	O
obvious	O
as	O
it	O
aligns	O
well	O
with	O
the	O
major	O
components	O
of	O
our	O
methodology	O
,	O
i.e.	O
,	O
lexicons	O
,	O
embeddings	O
,	O
and	O
translation	O
.	O
The	O
lexicons	O
we	O
work	O
with	O
follow	O
the	O
design	O
of	O
behavioral	O
experiments	O
:	O
a	O
stimulus	O
(	O
word	O
type	O
)	O
is	O
given	O
to	O
may	O
result	O
in	O
multiple	O
source	O
entries	O
translating	O
to	O
the	O
same	O
target	O
-	O
side	O
word	O
.	O
3	O
This	O
circumstance	O
leads	O
to	O
"	O
partial	O
duplicates	O
"	O
in	O
TargetMT	O
,	O
i.e.	O
,	O
groups	O
of	O
entries	O
with	O
the	O
same	O
word	O
type	O
but	O
different	O
emotion	B-DatasetName
values	O
(	O
because	O
they	O
were	O
derived	O
from	O
distinct	O
Source	O
entries	O
)	O
.	O
Such	O
overlap	O
could	O
do	O
harm	O
to	O
the	O
integrity	O
of	O
our	O
evaluation	O
since	O
knowledge	O
may	O
"	O
leak	O
"	O
from	O
training	O
to	O
validation	O
phase	O
,	O
i.e.	O
,	O
by	O
testing	O
the	O
model	O
on	O
words	O
it	O
has	O
already	O
seen	O
during	O
training	O
,	O
although	O
with	O
distinct	O
emotion	B-DatasetName
labels	O
.	O
The	O
proposed	O
data	O
partitioning	O
eliminates	O
such	O
distortion	O
effects	O
.	O
Since	O
partial	O
duplicates	O
receive	O
the	O
same	O
embedding	O
vector	O
,	O
the	O
prediction	O
model	O
assigns	O
the	O
same	O
emotion	B-DatasetName
value	O
to	O
both	O
,	O
thus	O
merging	O
them	O
in	O
TargetPred	O
.	O
Evaluation	O
Methodology	O
.	O
The	O
main	O
advantage	O
of	O
the	O
above	O
generation	O
method	O
is	O
that	O
it	O
allows	O
us	O
to	O
create	O
large	O
-	O
scale	O
emotion	B-DatasetName
lexicons	O
for	O
languages	O
a	O
subject	O
and	O
the	O
response	O
(	O
rating	O
)	O
is	O
recorded	O
.	O
The	O
absence	O
of	O
sense	O
-	O
level	O
annotation	O
simplifies	O
the	O
mapping	O
between	O
lexicon	O
and	O
embedding	O
entries	O
.	O
While	O
sense	O
embeddings	O
form	O
an	O
active	O
area	O
of	O
research	O
(	O
Camacho	O
-	O
Collados	O
and	O
Pilehvar	O
,	O
2018	O
;	O
Chi	O
and	O
Chen	O
,	O
2018	O
)	O
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
type	O
-	O
level	O
embeddings	O
yield	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
downstream	O
applications	O
.	O
3	O
Source	O
-	O
side	O
polysemy	O
,	O
in	O
contrast	O
to	O
its	O
target	O
-	O
side	O
counterpart	O
,	O
is	O
less	O
of	O
a	O
problem	O
,	O
because	O
we	O
receive	O
only	O
a	O
single	O
candidate	O
during	O
translation	O
.	O
This	O
may	O
result	O
in	O
cases	O
where	O
the	O
translation	O
misaligns	O
with	O
the	O
copied	O
emotion	B-DatasetName
value	O
in	O
TargetMT	O
.	O
Yet	O
,	O
the	O
prediction	O
step	O
partly	O
mitigates	O
such	O
inconsistencies	O
(	O
see	O
Section	O
6	O
)	O
.	O
for	O
which	O
gold	O
data	O
is	O
lacking	O
.	O
But	O
if	O
that	O
is	O
the	O
case	O
,	O
how	O
can	O
we	O
assess	O
the	O
quality	O
of	O
the	O
generated	O
lexicons	O
?	O
Our	O
solution	O
is	O
to	O
propose	O
two	O
different	O
evaluation	O
scenarios	O
-	O
a	O
gold	O
evaluation	O
which	O
is	O
a	O
strict	O
comparison	O
against	O
human	O
judgment	O
,	O
meaning	O
that	O
it	O
is	O
limited	O
to	O
languages	O
where	O
such	O
data	O
(	O
denoted	O
TargetGold	O
)	O
is	O
available	O
,	O
and	O
a	O
silver	O
evaluation	O
which	O
substitutes	O
human	O
judgments	O
by	O
automatically	O
derived	O
ones	O
(	O
silver	O
standard	O
)	O
which	O
is	O
feasible	O
for	O
any	O
language	O
in	O
our	O
study	O
.	O
The	O
rationale	O
is	O
that	O
if	O
both	O
,	O
gold	O
and	O
silver	O
evaluation	O
,	O
strongly	O
agree	O
with	O
each	O
other	O
,	O
we	O
can	O
use	O
one	O
as	O
proxy	O
for	O
the	O
other	O
when	O
no	O
target	O
-	O
side	O
gold	O
data	O
exists	O
(	O
examined	O
in	O
Section	O
6	O
)	O
.	O
Note	O
that	O
our	O
lexicon	O
generation	O
approach	O
consists	O
of	O
two	O
major	O
steps	O
,	O
translation	O
and	O
prediction	O
.	O
However	O
,	O
these	O
two	O
steps	O
are	O
not	O
equally	O
important	O
for	O
each	O
generated	O
entry	O
in	O
TargetPred	O
.	O
Words	O
,	O
such	O
as	O
German	O
Sonnenschein	O
for	O
which	O
a	O
translational	O
equivalent	O
already	O
exists	O
in	O
the	O
Source	O
(	O
"	O
sunshine	O
"	O
;	O
see	O
Figure	O
1	O
)	O
,	O
mainly	O
rely	O
on	O
translation	O
,	O
while	O
the	O
prediction	O
step	O
acts	O
as	O
an	O
optional	O
refinement	O
procedure	O
.	O
In	O
contrast	O
,	O
the	O
prediction	O
step	O
is	O
crucial	O
for	O
words	O
,	O
such	O
as	O
Erdbeben	O
,	O
whose	O
translational	O
equivalents	O
(	O
"	O
earthquake	O
"	O
)	O
are	O
missing	O
in	O
the	O
Source	O
.	O
Yet	O
,	O
these	O
words	O
also	O
depend	O
on	O
the	O
translation	O
step	O
for	O
producing	O
training	O
data	O
.	O
These	O
considerations	O
are	O
important	O
for	O
deciding	O
which	O
words	O
to	O
evaluate	O
on	O
.	O
We	O
may	O
choose	O
to	O
base	O
our	O
evaluation	O
on	O
the	O
full	O
TargetPred	O
lexicon	O
,	O
including	O
words	O
from	O
the	O
training	O
set	O
-	O
after	O
all	O
,	O
the	O
word	O
emotion	B-DatasetName
model	O
does	O
not	O
have	O
access	O
to	O
any	O
target	O
-	O
side	O
gold	O
data	O
.	O
The	O
problem	O
with	O
this	O
approach	O
is	O
that	O
it	O
merges	O
words	O
that	O
mainly	O
rely	O
on	O
translation	O
,	O
because	O
their	O
equivalents	O
are	O
in	O
the	O
Source	O
,	O
and	O
those	O
which	O
largely	O
depend	O
on	O
prediction	O
,	O
because	O
they	O
are	O
taken	O
from	O
the	O
embedding	O
model	O
.	O
In	O
this	O
case	O
,	O
generalizability	O
of	O
evaluation	O
results	O
becomes	O
questionable	O
.	O
Thus	O
,	O
our	O
evaluation	O
methodology	O
needs	O
to	O
fulfill	O
the	O
following	O
two	O
requirements	O
:	O
(	O
1	O
)	O
evaluation	O
must	O
not	O
be	O
performed	O
on	O
translational	O
equivalents	O
of	O
the	O
Source	O
entries	O
to	O
which	O
the	O
model	O
already	O
had	O
access	O
during	O
training	O
(	O
e.g.	O
,	O
Sonnenschein	O
and	O
nuklear	O
in	O
our	O
example	O
from	O
Figure	O
1	O
)	O
;	O
but	O
,	O
on	O
the	O
other	O
hand	O
,	O
(	O
2	O
)	O
a	O
reasonable	O
number	O
of	O
instances	O
must	O
be	O
available	O
for	O
evaluation	O
(	O
ideally	O
,	O
as	O
many	O
as	O
possible	O
to	O
increase	O
reliability	O
)	O
.	O
The	O
intricate	O
cross	O
-	O
lingual	O
train	O
-	O
dev	O
-	O
test	O
set	O
assignment	O
of	O
our	O
generation	O
methodology	O
is	O
in	O
place	O
so	O
that	O
we	O
meet	O
these	O
two	O
requirements	O
.	O
In	O
particular	O
,	O
for	O
our	O
silver	O
evaluation	O
,	O
we	O
intersect	O
TargetMT	O
-	O
test	O
with	O
TargetPred	O
-	O
test	O
and	O
compute	O
the	O
correlation	O
of	O
these	O
two	O
sets	O
individually	O
for	O
each	O
emotion	B-DatasetName
variable	O
.	O
Pearson	O
's	O
r	O
will	O
be	O
used	O
as	O
correlation	O
measure	O
throughout	O
this	O
paper	O
.	O
Establishing	O
a	O
test	O
set	O
at	O
the	O
very	O
start	O
of	O
our	O
workflow	O
,	O
Source	O
-	O
test	O
,	O
assures	O
that	O
there	O
is	O
a	O
relatively	O
large	O
overlap	O
between	O
the	O
two	O
sets	O
and	O
,	O
by	O
extension	O
,	O
that	O
our	O
requirements	O
for	O
the	O
evaluation	O
are	O
met	O
.	O
The	O
gold	O
evaluation	O
is	O
a	O
somewhat	O
more	O
challenging	O
case	O
,	O
because	O
we	O
can	O
,	O
in	O
general	O
,	O
not	O
guarantee	O
that	O
the	O
overlap	O
of	O
a	O
TargetGold	O
lexicon	O
with	O
TargetPred	O
-	O
test	O
will	O
be	O
of	O
any	O
particular	O
size	O
.	O
For	O
this	O
reason	O
,	O
the	O
words	O
of	O
the	O
embedding	O
model	O
are	O
added	O
to	O
TargetPred	O
-	O
test	O
(	O
see	O
above	O
)	O
,	O
maximizing	O
the	O
expected	O
overlap	O
with	O
TargetGold	O
.	O
In	O
practical	O
terms	O
,	O
we	O
intersect	O
TargetGold	O
with	O
TargetPred	O
-	O
test	O
and	O
compute	O
the	O
variable	O
-	O
wise	O
correlation	O
between	O
these	O
sets	O
,	O
in	O
parallel	O
to	O
the	O
silver	O
evaluation	O
.	O
A	O
complementary	O
strategy	O
for	O
maximizing	O
overlap	O
,	O
by	O
exploiting	O
dependencies	O
between	O
published	O
lexicons	O
,	O
is	O
described	O
below	O
.	O

Gold	O
Lexicons	O
and	O
Data	O
Splits	O
.	O
We	O
use	O
the	O
English	O
emotion	B-DatasetName
lexicon	O
from	O
Warriner	O
et	O
al	O
(	O
2013	O
)	O
as	O
first	O
part	O
of	O
our	O
Source	O
dataset	O
.	O
This	O
popular	O
resource	O
comprises	O
about	O
14k	O
entries	O
in	O
VAD	O
format	O
collected	O
via	O
crowdsourcing	O
.	O
Since	O
manually	O
gathered	O
BE5	O
ratings	O
are	O
available	O
only	O
for	O
a	O
subset	O
of	O
this	O
lexicon	O
(	O
Stevenson	O
et	O
al	O
,	O
2007	O
)	O
,	O
we	O
add	O
BE5	O
ratings	O
from	O
Buechel	O
and	O
Hahn	O
(	O
2018a	O
)	O
who	O
used	O
emotion	B-DatasetName
representation	O
mapping	O
(	O
see	O
Section	O
2	O
)	O
to	O
convert	O
the	O
existing	O
VAD	O
ratings	O
,	O
showing	O
that	O
this	O
is	O
about	O
as	O
reliable	O
as	O
human	O
annotation	O
.	O
As	O
apparent	O
from	O
the	O
previous	O
section	O
,	O
a	O
crucial	O
aspect	O
for	O
applying	O
our	O
methodology	O
is	O
the	O
design	O
of	O
the	O
train	O
-	O
dev	O
-	O
test	O
split	O
of	O
the	O
Source	O
because	O
it	O
directly	O
impacts	O
the	O
amount	O
of	O
words	O
we	O
can	O
test	O
our	O
lexicons	O
on	O
during	O
gold	O
evaluation	O
.	O
In	O
line	O
with	O
these	O
considerations	O
,	O
we	O
choose	O
the	O
lexical	O
items	O
which	O
are	O
already	O
present	O
in	O
ANEW	O
(	O
Bradley	O
and	O
Lang	O
,	O
1999	O
)	O
as	O
Source	O
-	O
test	O
set	O
.	O
ANEW	O
is	O
the	O
precursor	O
to	O
the	O
version	O
later	O
distributed	O
by	O
Warriner	O
et	O
al	O
(	O
2013	O
)	O
;	O
it	O
is	O
widely	O
used	O
and	O
has	O
been	O
adapted	O
to	O
a	O
wide	O
range	O
of	O
languages	O
.	O
With	O
this	O
choice	O
,	O
it	O
is	O
likely	O
that	O
a	O
resulting	O
TargetPred	O
-	O
test	O
set	O
has	O
a	O
large	O
overlap	O
with	O
the	O
respective	O
TargetGold	O
lexicon	O
.	O
As	O
for	O
the	O
TargetGold	O
lexicons	O
,	O
we	O
included	O
every	O
VA	O
(	O
D	O
)	O
and	O
BE5	O
lexicon	O
we	O
could	O
get	O
hold	O
of	O
with	O
more	O
than	O
500	O
entries	O
.	O
This	O
resulted	O
in	O
26	O
datasets	O
covering	O
12	O
quite	O
diverse	O
languages	O
(	O
see	O
Table	O
2	O
)	O
.	O
Note	O
that	O
we	O
also	O
include	O
English	O
lexicons	O
in	O
the	O
gold	O
evaluation	O
.	O
In	O
these	O
cases	O
,	O
no	O
translation	O
will	O
be	O
carried	O
out	O
(	O
Source	O
is	O
identical	O
to	O
TargetMT	O
)	O
so	O
that	O
only	O
the	O
expansion	O
step	O
is	O
validated	O
.	O
Appendix	O
A.1	O
gives	O
further	O
details	O
on	O
data	O
preparation	O
.	O
Translation	B-TaskName
.	O
We	O
used	O
the	O
GOOGLE	O
CLOUD	B-DatasetName
TRANSLATION	O
API	O
4	O
to	O
produce	O
word	O
-	O
to	O
-	O
word	B-TaskName
translation	I-TaskName
tables	O
.	O
This	O
is	O
a	O
commercial	O
service	O
,	O
total	O
translation	O
costs	O
amount	O
to	O
160	O
EUR	O
.	O
API	O
calls	O
were	O
performed	O
in	O
November	O
2019	O
.	O
Embeddings	O
.	O
We	O
use	O
the	O
fastText	B-MethodName
embedding	O
models	O
from	O
Grave	O
et	O
al	O
(	O
2018	O
)	O
trained	O
for	O
157	O
languages	O
on	O
the	O
respective	O
WIKIPEDIA	O
and	O
the	O
respective	O
part	O
of	O
COMMONCRAWL	O
.	O
These	O
resources	O
not	O
only	O
greatly	O
facilitate	O
our	O
work	O
but	O
also	O
increase	O
comparability	O
across	O
languages	O
.	O
The	O
restriction	O
to	O
"	O
only	O
"	O
91	O
languages	O
comes	O
from	O
intersecting	O
the	O
ones	O
covered	O
by	O
the	O
vectors	O
with	O
the	O
languages	O
covered	O
by	O
the	O
translation	O
service	O
.	O
Models	O
.	O
Since	O
our	O
proposed	O
methodology	O
is	O
agnostic	O
towards	O
the	O
chosen	O
word	O
emotion	B-DatasetName
model	O
,	O
we	O
will	O
re	O
-	O
use	O
models	O
from	O
the	O
literature	O
.	O
In	O
particular	O
,	O
we	O
will	O
rely	O
on	O
the	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
feed	O
-	O
forward	O
network	O
(	O
MTLFFN	O
)	O
worked	O
out	O
by	O
Buechel	O
and	O
Hahn	O
(	O
2018b	O
)	O
.	O
This	O
network	O
constitutes	O
the	O
current	O
state	O
of	O
the	O
art	O
for	O
monolingual	O
emotion	B-DatasetName
lexicon	O
creation	O
(	O
expanding	O
an	O
existing	O
lexicon	O
for	O
a	O
given	O
language	O
)	O
for	O
many	O
of	O
the	O
datasets	O
in	O
Table	O
2	O
.	O
The	O
MTLFFN	O
has	O
two	O
hidden	O
layers	O
of	O
256	O
and	O
128	O
units	O
,	O
respectively	O
,	O
and	O
takes	O
pre	O
-	O
trained	O
embedding	O
vectors	O
as	O
input	O
.	O
Its	O
distinguishing	O
feature	O
is	O
that	O
hidden	O
layer	O
parameters	O
are	O
shared	O
between	O
the	O
different	O
emotion	B-DatasetName
target	O
variables	O
,	O
thus	O
constituting	O
a	O
mild	O
form	O
of	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
(	O
MTL	O
)	O
.	O
We	O
apply	O
MTL	O
to	O
VAD	O
and	O
BE5	O
variables	O
individually	O
(	O
but	O
not	O
between	O
both	O
groups	O
)	O
,	O
thus	O
training	O
two	O
distinct	O
emotion	B-DatasetName
models	O
per	O
language	O
,	O
following	O
the	O
outcome	O
of	O
a	O
development	O
experiment	O
.	O
Details	O
are	O
given	O
in	O
Appendix	O
A.2	O
together	O
with	O
the	O
remainder	O
of	O
the	O
model	O
specifications	O
.	O
Being	O
aware	O
of	O
the	O
infamous	O
instability	O
of	O
neural	O
approaches	O
(	O
Reimers	O
and	O
Gurevych	O
,	O
2017	O
)	O
,	O
we	O
also	O
employ	O
a	O
ridge	O
regression	O
model	O
,	O
an	O
L	O
2	O
regularized	O
version	O
of	O
linear	B-MethodName
regression	I-MethodName
,	O
as	O
a	O
more	O
robust	O
,	O
yet	O
al	O
o	O
powerful	O
baseline	O
(	O
Li	O
et	O
al	O
,	O
2017	O
)	O
.	O

Emotion	O
lexicons	O
are	O
at	O
the	O
core	O
of	O
sentiment	B-TaskName
analysis	I-TaskName
,	O
a	O
rapidly	O
flourishing	O
field	O
of	O
NLP	O
.	O
Yet	O
,	O
despite	O
large	O
community	O
efforts	O
,	O
the	O
coverage	O
of	O
existing	O
lexicons	O
is	O
still	O
limited	O
in	O
terms	O
of	O
languages	O
,	O
size	O
,	O
and	O
types	O
of	O
emotion	B-DatasetName
variables	O
.	O
While	O
there	O
are	O
techniques	O
to	O
tackle	O
these	O
three	O
forms	O
of	O
sparsity	O
in	O
isolation	O
,	O
we	O
introduced	O
a	O
methodology	O
which	O
allows	O
us	O
to	O
cope	O
with	O
them	O
simultaneously	O
by	O
jointly	O
combining	O
emotion	B-DatasetName
representation	O
mapping	O
,	O
machine	B-TaskName
translation	I-TaskName
,	O
and	O
embedding	O
-	O
based	O
lexicon	O
expansion	O
.	O
Our	O
study	O
is	O
"	O
large	O
-	O
scale	O
"	O
in	O
many	O
respects	O
.	O
We	O
created	O
representationally	O
complex	O
lexiconscomprising	O
8	O
distinct	O
emotion	B-DatasetName
variables	O
-	O
for	O
91	O
languages	O
with	O
up	O
to	O
2	O
million	O
entries	O
each	O
.	O
The	O
evaluation	O
of	O
the	O
generated	O
lexicons	O
featured	O
26	O
manually	O
annotated	O
datasets	O
spanning	O
12	O
diverse	O
languages	O
.	O
The	O
predicted	O
ratings	O
showed	O
consistently	O
high	O
correlation	O
with	O
human	O
judgment	O
,	O
compared	O
favorably	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
monolingual	O
approaches	O
to	O
lexicon	O
expansion	O
and	O
even	O
surpassed	O
human	O
inter	O
-	O
study	O
reliability	O
in	O
some	O
cases	O
.	O
The	O
sheer	O
number	O
of	O
test	O
sets	O
we	O
used	O
allowed	O
us	O
to	O
validate	O
fundamental	O
methodological	O
assumptions	O
underlying	O
our	O
approach	O
.	O
Firstly	O
,	O
the	O
evaluation	O
procedure	O
,	O
which	O
is	O
integrated	O
into	O
the	O
generation	O
methodology	O
,	O
allows	O
us	O
to	O
reliably	O
estimate	O
the	O
quality	O
of	O
resulting	O
lexicons	O
,	O
even	O
without	O
target	O
language	O
gold	O
standard	O
.	O
Secondly	O
,	O
our	O
data	O
suggests	O
that	O
embedding	O
-	O
based	O
word	O
emotion	B-DatasetName
models	O
can	O
be	O
used	O
as	O
a	O
repair	O
mechanism	O
,	O
mitigating	O
poor	O
target	O
-	O
language	O
emotion	B-DatasetName
estimates	O
acquired	O
by	O
simple	O
word	O
-	O
to	O
-	O
word	B-TaskName
translation	I-TaskName
.	O
Future	O
work	O
will	O
have	O
to	O
deepen	O
the	O
way	O
we	O
deal	O
with	O
word	O
sense	O
ambiguity	O
by	O
way	O
of	O
exchanging	O
the	O
simplifying	O
type	O
-	O
level	O
approach	O
our	O
current	O
work	O
is	O
based	O
on	O
with	O
a	O
semantically	O
more	O
informed	O
sense	O
-	O
level	O
approach	O
.	O
A	O
promising	O
direction	O
would	O
be	O
to	O
combine	O
a	O
multilingual	O
sense	O
inventory	O
such	O
as	O
BABELNET	O
(	O
Navigli	O
and	O
Ponzetto	O
,	O
2012	O
)	O
with	O
sense	O
embeddings	O
(	O
Camacho	O
-	O
Collados	O
and	O
Pilehvar	O
,	O
2018	O
)	O
.	O

Paraphrase	B-TaskName
Generation	I-TaskName
:	O
A	O
Survey	O
of	O
the	O
State	O
of	O
the	O
Art	O

This	O
paper	O
focuses	O
on	O
paraphrase	B-TaskName
generation	I-TaskName
,	O
which	O
is	O
a	O
widely	O
studied	O
natural	O
language	O
generation	O
task	O
in	O
NLP	O
.	O
With	O
the	O
development	O
of	O
neural	O
models	O
,	O
paraphrase	B-TaskName
generation	I-TaskName
research	O
has	O
exhibited	O
a	O
gradual	O
shift	O
to	O
neural	O
methods	O
in	O
the	O
recent	O
years	O
.	O
This	O
has	O
provided	O
architectures	O
for	O
contextualized	O
representation	O
of	O
an	O
input	O
text	O
and	O
generating	O
fluent	O
,	O
diverse	O
and	O
human	O
-	O
like	O
paraphrases	O
.	O
This	O
paper	O
surveys	O
various	O
approaches	O
to	O
paraphrase	B-TaskName
generation	I-TaskName
with	O
a	O
main	O
focus	O
on	O
neural	O
methods	O
.	O

Paraphrases	O
are	O
texts	O
that	O
convey	O
the	O
same	O
meaning	O
while	O
using	O
different	O
words	O
or	O
sentence	O
structures	O
.	O
The	O
generation	O
of	O
paraphrases	O
is	O
a	O
longstanding	O
problem	O
for	O
natural	O
language	O
learning	O
.	O
For	O
example	O
,	O
the	O
question	O
How	O
do	O
I	O
improve	O
my	O
English	O
could	O
be	O
equivalently	O
phrased	O
as	O
What	O
is	O
the	O
best	O
way	O
to	O
learn	O
English	O
.	O
Paraphrasing	O
can	O
be	O
play	O
an	O
important	O
role	O
in	O
language	O
understanding	O
tasks	O
,	O
such	O
as	O
question	B-TaskName
answering	I-TaskName
(	O
Dong	O
et	O
al	O
,	O
2017	O
;	O
Zhu	O
et	O
al	O
,	O
2017	O
)	O
,	O
machine	B-TaskName
translation	I-TaskName
(	O
Seraj	O
et	O
al	O
,	O
2015	O
;	O
Thompson	O
and	O
Post	O
,	O
2020a	O
)	O
,	O
and	O
semantic	B-TaskName
parsing	I-TaskName
(	O
Berant	O
and	O
Liang	O
,	O
2014	O
;	O
.	O
And	O
it	O
is	O
also	O
a	O
good	O
way	O
for	O
data	B-TaskName
augmentation	I-TaskName
(	O
Kumar	B-DatasetName
et	O
al	O
,	O
2019	O
;	O
Gao	O
et	O
al	O
,	O
2020	O
)	O
.	O
Given	O
a	O
sentence	O
,	O
paraphrase	B-TaskName
generation	I-TaskName
aims	O
to	O
create	O
its	O
paraphrases	O
that	O
can	O
have	O
a	O
different	O
wording	O
or	O
different	O
structure	O
from	O
the	O
original	O
sentence	O
,	O
while	O
preserving	O
the	O
original	O
meaning	O
.	O
The	O
focus	O
of	O
paraphrase	B-TaskName
generation	I-TaskName
has	O
exhibited	O
a	O
gradual	O
shift	O
from	O
classical	O
approaches	O
to	O
more	O
advanced	O
neural	O
approaches	O
in	O
the	O
recent	O
years	O
with	O
the	O
rapid	O
development	O
of	O
various	O
neural	O
models	O
.	O
Neural	O
models	O
have	O
changed	O
the	O
traditional	O
way	O
paraphrase	B-TaskName
generation	I-TaskName
is	O
performed	O
and	O
also	O
provided	O
new	O
directions	O
and	O
architectures	O
for	O
the	O
NLP	O
community	O
.	O
While	O
several	O
surveys	O
on	O
the	O
traditional	O
methods	O
and	O
limited	O
neural	O
methods	O
for	O
paraphrase	O
gener	O
-	O

What	O
is	O
the	O
distance	O
between	O
Sun	O
and	O
Earth	O
if	O
at	O
any	O
time	O
in	O
the	O
preparation	O
of	O
this	O
product	O
the	O
integrity	O
of	O
this	O
container	O
is	O
compromised	O
it	O
should	O
not	O
be	O
used	O
.	O
this	O
container	O
should	O
not	O
be	O
used	O
if	O
the	O
product	O
is	O
compromised	O
at	O
any	O
time	O
in	O
preparation	O
.	O
ation	O
have	O
been	O
published	O
(	O
Metzler	O
et	O
al	O
,	O
2011	O
;	O
Gupta	O
and	O
Krzyżak	O
,	O
2020	O
)	O
,	O
there	O
is	O
no	O
thorough	O
and	O
comprehensive	O
survey	O
on	O
neural	O
methods	O
for	O
paraphrase	B-TaskName
generation	I-TaskName
.	O
To	O
our	O
best	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
survey	O
on	O
neural	O
methods	O
for	O
paraphrase	B-TaskName
generation	I-TaskName
.	O
Therefore	O
,	O
our	O
goal	O
in	O
this	O
paper	O
is	O
to	O
provide	O
a	O
timely	O
survey	O
on	O
paraphrase	B-TaskName
generation	I-TaskName
,	O
with	O
a	O
main	O
focus	O
on	O
neural	O
methods	O
.	O
In	O
the	O
following	O
section	O
,	O
we	O
will	O
first	O
introduce	O
the	O
most	O
frequently	O
used	O
datasets	O
for	O
paraphrase	B-TaskName
generation	I-TaskName
(	O
Section	O
2	O
)	O
.	O
Then	O
we	O
list	O
the	O
traditional	O
evaluation	O
metrics	O
in	O
Section	O
3	O
.	O
In	O
Section	O
4	O
,	O
we	O
present	O
some	O
of	O
the	O
traditional	O
approaches	O
that	O
were	O
used	O
before	O
the	O
neural	O
methods	O
.	O
Neural	O
models	O
,	O
the	O
main	O
focus	O
of	O
this	O
paper	O
,	O
will	O
be	O
discussed	O
in	O
Section	O
5	O
.	O
After	O
introducing	O
all	O
the	O
methods	O
,	O
we	O
compare	O
the	O
performance	O
of	O
the	O
different	O
models	O
for	O
paraphrase	B-TaskName
generation	I-TaskName
in	O
Section	O
6	O
.	O
Finally	O
,	O
we	O
identify	O
some	O
research	O
gaps	O
in	O
paraphrase	B-TaskName
generation	I-TaskName
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
several	O
datasets	O
that	O
have	O
been	O
extensively	O
used	O
for	O
paraphrase	B-TaskName
generation	I-TaskName
.	O
PPDB	O
The	O
paraphrase	O
database	O
(	O
Ganitkevitch	O
et	O
al	O
,	O
2013	O
)	O
WikiAnswer	O
This	O
dataset	O
(	O
Fader	O
et	O
al	O
,	O
2013	O
)	O
contains	O
approximately	O
18	O
million	O
word	O
-	O
aligned	O
question	O
pairs	O
that	O
are	O
paraphrases	O
.	O
The	O
word	O
alignments	O
provided	O
by	O
this	O
dataset	O
al	O
o	O
relate	O
the	O
synonyms	O
in	O
the	O
paraphrase	O
sentences	O
.	O
However	O
,	O
all	O
the	O
sentences	O
provided	O
in	O
this	O
dataset	O
are	O
questions	O
,	O
which	O
restricts	O
the	O
paraphrases	O
to	O
only	O
questions	O
.	O
MSCOCO	B-DatasetName
MSCOCO	B-DatasetName
(	O
Lin	O
et	O
al	O
,	O
2014	O
)	O
ParaNMT	O
ParaNMT	O
)	O
is	O
a	O
dataset	O
of	O
more	O
than	O
50	O
million	O
English	O
-	O
English	O
sentential	O
paraphrase	O
pairs	O
.	O
The	O
pairs	O
were	O
generated	O
automatically	O
by	O
using	O
back	O
-	O
translation	O
to	O
translate	O
the	O
non	O
-	O
English	O
side	O
of	O
a	O
large	O
Czech	O
-	O
English	O
parallel	O
corpus	O
.	O
Owing	O
to	O
its	O
recency	O
,	O
it	O
has	O
not	O
been	O
used	O
widely	O
.	O

Rule	O
-	O
based	O
paraphrase	B-TaskName
generation	I-TaskName
approaches	O
build	O
on	O
hand	O
-	O
crafted	O
or	O
automatically	O
collected	O
paraphrase	O
rules	O
.	O
In	O
the	O
early	O
works	O
,	O
these	O
rules	O
were	O
mainly	O
hand	O
-	O
crafted	O
(	O
McKeown	O
,	O
1983	O
)	O
.	O
Due	O
to	O
the	O
significant	O
manual	O
efforts	O
,	O
some	O
researchers	O
have	O
sought	O
to	O
collect	O
paraphrase	O
rules	O
automatically	O
(	O
Lin	O
and	O
Pantel	O
,	O
2001	O
;	O
Barzilay	O
and	O
Lee	O
,	O
2003	O
)	O
.	O
However	O
,	O
the	O
limitation	O
of	O
the	O
extracting	O
methods	O
has	O
led	O
to	O
the	O
generation	O
of	O
long	O
and	O
complex	O
paraphrase	O
patterns	O
,	O
in	O
turn	O
impacting	O
performance	O
.	O

This	O
approach	O
is	O
based	O
on	O
statistical	O
machine	B-TaskName
translation	I-TaskName
(	O
SMT	O
)	O
and	O
is	O
motivated	O
by	O
the	O
fact	O
that	O
paraphrase	B-TaskName
generation	I-TaskName
can	O
be	O
seen	O
as	O
a	O
special	O
case	O
of	O
machine	B-TaskName
translation	I-TaskName
(	O
i.e.	O
,	O
monolingual	O
machine	B-TaskName
translation	I-TaskName
)	O
.	O
A	O
machine	B-TaskName
translation	I-TaskName
model	O
normally	O
finds	O
a	O
best	O
translationê	O
of	O
a	O
text	O
in	O
language	O
f	O
to	O
a	O
text	O
in	O
language	O
e	O
by	O
utilizing	O
a	O
statistical	O
translation	O
model	O
p	O
(	O
f	O
|	O
e	O
)	O
and	O
a	O
language	O
model	O
p	O
(	O
e	O
)	O
:	O
e	O
=	O
arg	O
max	O
e	O
e	O
*	O
p	O
(	O
f	O
|	O
e	O
)	O
p	O
(	O
e	O
)	O
Applying	O
this	O
idea	O
to	O
paraphrase	B-TaskName
generation	I-TaskName
,	O
such	O
a	O
model	O
will	O
find	O
a	O
best	O
paraphraset	O
of	O
a	O
text	O
in	O
the	O
source	O
side	O
s	O
to	O
a	O
text	O
in	O
the	O
target	O
side	O
t	O
obtained	O
as	O
,	O
t	O
=	O
arg	O
max	O
t	O
t	O
*	O
p	O
(	O
s	O
|	O
t	O
)	O
p	O
(	O
t	O
)	O
For	O
instance	O
,	O
(	O
Wubben	O
et	O
al	O
,	O
2010	O
)	O
constructed	O
a	O
large	O
-	O
scale	O
parallel	O
corpus	O
containing	O
paraphrases	O
collected	O
from	O
the	O
headlines	O
that	O
appeared	O
in	O
Google	B-DatasetName
News	O
.	O
Then	O
they	O
trained	O
a	O
Phrase	O
-	O
Based	O
Machine	B-TaskName
Translation	I-TaskName
model	O
(	O
PBMT	O
)	O
(	O
Koehn	O
et	O
al	O
,	O
2007	O
)	O
on	O
their	O
parallel	O
corpus	O
using	O
the	O
MOSES	B-DatasetName
package	O
.	O
The	O
trained	O
PBMT	O
is	O
finally	O
used	O
to	O
generate	O
paraphrases	O
.	O

Early	O
works	O
on	O
paraphrasing	O
mainly	O
focused	O
on	O
template	O
-	O
based	O
or	O
statistical	O
machine	B-TaskName
translation	I-TaskName
approaches	O
.	O
However	O
,	O
the	O
matching	O
of	O
templates	O
and	O
modeling	O
of	O
a	O
statistical	O
translation	O
model	O
are	O
both	O
challenging	O
tasks	O
.	O
With	O
the	O
recent	O
advances	O
of	O
neural	O
networks	O
,	O
especially	O
the	O
sequence	O
-	O
to	O
-	O
sequence	O
framework	O
,	O
Seq2Seq	B-MethodName
models	O
were	O
first	O
use	O
for	O
paraphrase	B-TaskName
generation	I-TaskName
by	O
(	O
Prakash	O
et	O
al	O
,	O
2016	O
)	O
.	O
Their	O
work	O
inspired	O
the	O
wide	O
use	O
of	O
neural	O
models	O
for	O
paraphrase	B-TaskName
generation	I-TaskName
.	O
Below	O
we	O
introduce	O
the	O
main	O
approaches	O
based	O
on	O
neural	O
models	O
that	O
are	O
used	O
for	O
paraphrase	B-TaskName
generation	I-TaskName
.	O

Currently	O
,	O
most	O
of	O
the	O
existing	O
paraphrase	B-TaskName
generation	I-TaskName
models	O
are	O
based	O
on	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
consisting	O
of	O
an	O
encoder	O
and	O
a	O
decoder	O
.	O
The	O
encoder	O
will	O
encode	O
the	O
source	O
texts	O
into	O
a	O
contextualized	O
vector	O
representation	O
along	O
with	O
a	O
list	O
of	O
vector	O
representations	O
capturing	O
the	O
semantics	O
of	O
each	O
word	O
and	O
context	O
.	O
Then	O
,	O
the	O
decoder	O
will	O
generate	O
paraphrases	O
based	O
on	O
the	O
vectors	O
given	O
by	O
the	O
encoder	O
.	O

The	O
main	O
purpose	O
of	O
encoding	O
is	O
to	O
extract	O
the	O
semantic	O
information	O
for	O
the	O
decoder	O
to	O
generate	O
paraphrases	O
.	O
With	O
the	O
development	O
of	O
various	O
neural	O
models	O
,	O
researchers	O
also	O
have	O
multiple	O
choices	O
for	O
the	O
encoder	O
.	O
Encoder	O
With	O
a	O
consistent	O
goal	O
of	O
learning	O
better	O
abstract	O
contextualized	O
representation	O
of	O
the	O
input	O
text	O
,	O
several	O
architectures	O
have	O
been	O
explored	O
by	O
researchers	O
.	O
(	O
Prakash	O
et	O
al	O
,	O
2016	O
)	O
first	O
utilized	O
a	O
seq2seq	B-MethodName
model	O
implemented	O
as	O
recurrent	O
neural	O
networks	O
-	O
long	O
short	O
term	O
memory	O
networks	O
(	O
LSTMs	O
)	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
-	O
to	O
process	O
long	O
sequences	O
.	O
A	O
nonvolutional	O
neural	O
network	O
(	O
CNN	O
)	O
has	O
also	O
been	O
used	O
to	O
construct	O
seq2seq	B-MethodName
models	O
as	O
a	O
CNN	O
has	O
fewer	O
parameters	O
and	O
thus	O
is	O
faster	O
to	O
train	O
(	O
Vizcarra	O
and	O
Ochoa	O
-	O
Luna	O
,	O
2020	O
)	O
.	O
The	O
Transformer	B-MethodName
model	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
has	O
shown	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
multiple	O
text	B-TaskName
generation	I-TaskName
tasks	O
.	O
Due	O
to	O
the	O
Transformer	B-MethodName
's	O
improved	O
ability	O
to	O
capture	O
long	O
-	O
range	O
dependencies	O
in	O
sentences	O
,	O
utilized	O
a	O
Transformer	B-MethodName
to	O
construct	O
their	O
seq2seq	B-MethodName
model	O
.	O
More	O
recently	O
,	O
large	O
language	O
models	O
using	O
transformer	O
architectures	O
have	O
achieved	O
state	O
-	O
ofthe	O
-	O
art	O
results	O
for	O
many	O
NLP	O
tasks	O
while	O
using	O
less	O
supervised	O
data	O
than	O
before	O
.	O
Therefore	O
,	O
some	O
researchers	O
also	O
utilized	O
large	O
pretrained	B-TaskName
language	I-TaskName
models	I-TaskName
such	O
as	O
GPT	B-MethodName
-	O
2	O
(	O
Radford	O
et	O
al	O
,	O
2019	O
)	O
and	O
BART	B-MethodName
(	O
Lewis	O
et	O
al	O
,	O
2020	O
)	O
as	O
their	O
encoder	O
-	O
decoder	O
framework	O
(	O
Witteveen	O
and	O
Andrews	O
,	O
2019	O
;	O
Hegde	O
and	O
Patil	O
,	O
2020	O
;	O
Garg	O
et	O
al	O
,	O
2021	O
)	O
.	O

At	O
the	O
decoding	O
side	O
,	O
the	O
contextualized	O
representation	O
is	O
used	O
at	O
each	O
decoding	O
step	O
with	O
the	O
vector	O
representation	O
of	O
previously	O
generated	O
words	O
.	O
Finally	O
,	O
a	O
distribution	O
over	O
the	O
vocabulary	O
is	O
obtained	O
and	O
the	O
word	O
with	O
highest	O
probability	O
will	O
be	O
generated	O
.	O
This	O
method	O
is	O
greedy	O
decoding	O
.	O
Besides	O
,	O
a	O
more	O
commonly	O
used	O
method	O
called	O
beam	O
search	O
(	O
Wiseman	O
and	O
Rush	O
,	O
2016	O
)	O
is	O
used	O
,	O
which	O
identifies	O
the	O
k	O
-	O
best	O
paths	O
up	O
to	O
current	O
timestep	O
during	O
decoding	O
.	O
However	O
,	O
greedy	O
decoding	O
and	O
beam	O
search	O
methods	O
are	O
both	O
generic	O
approaches	O
for	O
all	O
text	B-TaskName
generation	I-TaskName
tasks	O
without	O
a	O
specific	O
focus	O
on	O
paraphrase	B-TaskName
generation	I-TaskName
.	O
Therefore	O
,	O
with	O
the	O
goal	O
of	O
generating	O
paraphrases	O
and	O
avoiding	O
the	O
words	O
existing	O
in	O
the	O
source	O
sentences	O
,	O
a	O
few	O
blocking	O
mechanisms	O
have	O
been	O
proposed	O
to	O
prevent	O
the	O
decoder	O
from	O
generating	O
the	O
same	O
words	O
in	O
the	O
source	O
sentences	O
.	O
This	O
is	O
also	O
a	O
way	O
to	O
guarantee	O
the	O
diversity	O
of	O
the	O
generated	O
paraphrases	O
and	O
prevent	O
the	O
models	O
from	O
directly	O
copying	O
the	O
input	O
into	O
the	O
output	O
paraphrases	O
(	O
Niu	O
et	O
al	O
,	O
2020	O
;	O
Thompson	O
and	O
Post	O
,	O
2020b	O
)	O
.	O

Encoder	O
-	O
Decoder	O
Architecture	O
The	O
numerous	O
attempts	O
that	O
have	O
been	O
made	O
to	O
improve	O
the	O
Encoder	O
-	O
Decoder	O
architecture	O
for	O
paraphrase	B-TaskName
generation	I-TaskName
can	O
be	O
broadly	O
categorized	O
into	O
two	O
types	O
based	O
on	O
their	O
focus	O
:	O
A.	O
Model	O
-	O
focused	O
;	O
and	O
B.	O
Attribute	O
-	O
focused	O
.	O
Next	O
we	O
introduce	O
them	O
respectively	O
with	O
more	O
fine	O
-	O
grained	O
divisions	O
.	O

Although	O
recent	O
neural	O
models	O
have	O
shown	O
great	O
advances	O
,	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
are	O
still	O
not	O
satisfactory	O
enough	O
.	O
Therefore	O
,	O
more	O
advanced	O
paraphrasing	O
models	O
still	O
need	O
to	O
be	O
explored	O
.	O
Below	O
we	O
discuss	O
several	O
potential	O
directions	O
of	O
research	O
that	O
we	O
believe	O
are	O
worth	O
studying	O
.	O
Pretrained	B-TaskName
language	I-TaskName
models	I-TaskName
Virtually	O
all	O
recent	O
work	O
related	O
to	O
the	O
application	O
of	O
pretrained	B-TaskName
language	I-TaskName
models	I-TaskName
on	O
paraphrase	B-TaskName
generation	I-TaskName
is	O
quite	O
naive	O
.	O
Therefore	O
,	O
we	O
could	O
combine	O
the	O
large	O
pretrained	B-TaskName
language	I-TaskName
models	I-TaskName
with	O
other	O
mechanisms	O
,	O
for	O
example	O
reinforcement	O
learning	O
,	O
VAE	B-MethodName
and	O
GAN	B-MethodName
.	O

We	O
demonstrate	O
that	O
it	O
is	O
feasible	O
to	O
accurately	O
diacritize	O
Hebrew	O
script	O
without	O
any	O
human	O
-	O
curated	O
resources	O
other	O
than	O
plain	O
diacritized	O
text	O
.	O
We	O
present	O
NAKDIMON	O
,	O
a	O
two	O
-	O
layer	O
character	O
-	O
level	O
LSTM	B-MethodName
,	O
that	O
performs	O
on	O
par	O
with	O
much	O
more	O
complicated	O
curationdependent	O
systems	O
,	O
across	O
a	O
diverse	O
array	O
of	O
modern	O
Hebrew	O
sources	O
.	O
The	O
model	O
is	O
accompanied	O
by	O
a	O
training	O
set	O
and	O
a	O
test	O
set	O
,	O
collected	O
from	O
diverse	O
sources	O
.	O

Learning	O
directly	O
from	O
plain	O
diacritized	O
text	O
can	O
go	O
a	O
long	O
way	O
,	O
even	O
with	O
relatively	O
limited	O
resources	O
.	O
NAKDIMON	O
demonstrates	O
that	O
a	O
simple	O
architecture	O
for	O
diacritizing	O
Hebrew	O
text	O
as	O
a	O
sequence	O
tagging	O
problem	O
can	O
achieve	O
performance	O
on	O
par	O
with	O
much	O
more	O
complex	O
systems	O
.	O
We	O
also	O
introduce	O
and	O
release	O
a	O
corpus	O
of	O
dotted	O
Hebrew	O
text	O
,	O
as	O
well	O
as	O
a	O
source	O
-	O
balanced	O
test	O
set	O
.	O
In	O
the	O
future	O
,	O
we	O
wish	O
to	O
evaluate	O
the	O
utility	O
of	O
dotting	O
as	O
a	O
feature	O
for	O
downstream	O
tasks	O
such	O
as	O
question	B-TaskName
answering	I-TaskName
,	O
machine	B-TaskName
translation	I-TaskName
,	O
and	O
speech	O
generation	O
,	O
taking	O
advantage	O
of	O
the	O
fact	O
that	O
our	O
simplified	O
model	O
can	O
be	O
easily	O
integrated	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
Hebrew	O
processing	O
system	O
.	O

We	O
present	O
results	O
for	O
the	O
Dicta	O
test	O
set	O
in	O
Table	O
5	O
.	O
In	O
order	O
to	O
provide	O
fair	O
comparison	O
and	O
to	O
preempt	O
overfitting	O
on	O
this	O
test	O
data	O
,	O
we	O
ran	O
this	O
test	O
in	O
a	O
preliminary	O
setup	O
on	O
a	O
variant	O
of	O
NAKDIMON	O
which	O
was	O
not	O
tuned	O
or	O
otherwise	O
unfairly	O
trained	O
.	O
This	O
system	O
,	O
NAKDIMON	O
0	B-DatasetName
,	O
differs	O
from	O
our	O
final	O
variant	O
in	O
three	O
main	O
aspects	O
:	O
it	O
is	O
not	O
trained	O
on	O
the	O
Dicta	O
portion	O
of	O
our	O
training	O
corpus	O
(	O
2.2	O
)	O
,	O
it	O
is	O
not	O
trained	O
on	O
the	O
AUTOMATIC	O
corpus	O
,	O
and	O
it	O
employs	O
a	O
residual	B-MethodName
connection	I-MethodName
between	O
the	O
two	O
character	O
Bi	O
-	O
LSTM	B-MethodName
layers	O
.	O
Testing	O
on	O
the	O
Dicta	O
test	O
set	O
required	O
some	O
minimal	O
evaluation	O
adaptations	O
resulting	O
from	O
encoding	O
constraints	O
(	O
for	O
example	O
,	O
we	O
do	O
not	O
distinguish	O
between	O
kamatz	O
katan	O
and	O
kamatz	O
gadol	O
)	O
.	O
Thus	O
,	O
we	O
copy	O
the	O
results	O
reported	O
in	O

Transfer	B-TaskName
learning	I-TaskName
has	O
proven	O
to	O
be	O
crucial	O
in	O
advancing	O
the	O
state	O
of	O
speech	O
and	O
natural	O
language	O
processing	O
research	O
in	O
recent	O
years	O
.	O
In	O
speech	O
,	O
a	O
model	O
pre	O
-	O
trained	O
by	O
self	B-TaskName
-	I-TaskName
supervised	I-TaskName
learning	I-TaskName
transfers	O
remarkably	O
well	O
on	O
multiple	O
tasks	O
.	O
However	O
,	O
the	O
lack	O
of	O
a	O
consistent	O
evaluation	O
methodology	O
is	O
limiting	O
towards	O
a	O
holistic	O
understanding	O
of	O
the	O
efficacy	O
of	O
such	O
models	O
.	O
SUPERB	O
was	O
a	O
step	O
towards	O
introducing	O
a	O
common	O
benchmark	O
to	O
evaluate	O
pretrained	O
models	O
across	O
various	O
speech	O
tasks	O
.	O
In	O
this	O
paper	O
,	O
we	O
introduce	O
SUPERB	O
-	O
SG	O
,	O
a	O
new	O
benchmark	O
focused	O
on	O
evaluating	O
the	O
semantic	O
and	O
generative	O
capabilities	O
of	O
pre	O
-	O
trained	O
models	O
by	O
increasing	O
task	O
diversity	O
and	O
difficulty	O
over	O
SUPERB	O
.	O
We	O
use	O
a	O
lightweight	O
methodology	O
to	O
test	O
the	O
robustness	O
of	O
representations	O
learned	O
by	O
pre	O
-	O
trained	O
models	O
under	O
shifts	O
in	O
data	O
domain	O
and	O
quality	O
across	O
different	O
types	O
of	O
tasks	O
.	O
It	O
entails	O
freezing	O
pretrained	O
model	O
parameters	O
,	O
only	O
using	O
simple	O
task	O
-	O
specific	O
trainable	O
heads	O
.	O
The	O
goal	O
is	O
to	O
be	O
inclusive	O
of	O
all	O
researchers	O
,	O
and	O
encourage	O
efficient	O
use	O
of	O
computational	O
resources	O
.	O
We	O
also	O
show	O
that	O
the	O
task	O
diversity	O
of	O
SUPERB	O
-	O
SG	O
coupled	O
with	O
limited	O
task	O
supervision	O
is	O
an	O
effective	O
recipe	O
for	O
evaluating	O
the	O
generalizability	O
of	O
model	O
representation	O
.	O
Equal	O
contribution	O
.	O

Transfer	B-TaskName
learning	I-TaskName
is	O
a	O
paradigm	O
in	O
machine	O
learning	O
that	O
has	O
been	O
very	O
effective	O
for	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
(	O
Peters	O
et	O
al	O
,	O
2018	O
;	O
Devlin	O
et	O
al	O
,	O
2019	O
;	O
Lan	O
et	O
al	O
,	O
2019	O
;	O
Dong	O
et	O
al	O
,	O
2019	O
;	O
Raffel	O
et	O
al	O
,	O
2020	O
;	O
Conneau	O
et	O
al	O
,	O
2020	O
)	O
,	O
and	O
speech	O
processing	O
(	O
van	O
den	O
Oord	O
et	O
al	O
,	O
2018	O
;	O
Rivière	O
et	O
al	O
,	O
2020	O
;	O
Chung	O
et	O
al	O
,	O
2019	O
;	O
Schneider	O
et	O
al	O
,	O
2019	O
;	O
Baevski	O
et	O
al	O
,	O
2020b	O
;	O
Hsu	O
et	O
al	O
,	O
2021	O
;	O
Liu	O
et	O
al	O
,	O
2020c	O
,	O
b	O
;	O
Ravanelli	O
et	O
al	O
,	O
2020	O
;	O
.	O
Self	B-TaskName
-	I-TaskName
supervised	I-TaskName
learning	I-TaskName
(	O
SSL	B-DatasetName
)	O
is	O
the	O
main	O
driver	O
of	O
this	O
paradigm	O
,	O
an	O
effective	O
and	O
scalable	O
way	O
to	O
learn	O
high	O
-	O
level	O
representation	O
of	O
language	O
that	O
transfers	O
to	O
a	O
variety	O
of	O
tasks	O
.	O
SSL	B-DatasetName
entails	O
learning	O
from	O
the	O
input	O
or	O
some	O
perturbation	O
of	O
it	O
without	O
the	O
need	O
for	O
labelled	O
data	O
.	O
This	O
has	O
unlocked	O
the	O
usage	O
of	O
large	O
amounts	O
of	O
cheaply	O
available	O
unlabelled	O
data	O
.	O
It	O
lends	O
naturally	O
to	O
neural	O
network	O
models	O
that	O
have	O
been	O
shown	O
to	O
possess	O
impressive	O
scaling	O
characteristics	O
such	O
that	O
it	O
is	O
often	O
enough	O
to	O
increase	O
the	O
model	O
and	O
data	O
sizes	O
to	O
improve	O
downstream	O
performance	O
(	O
Hestness	O
et	O
al	O
,	O
2017	O
;	O
Jozefowicz	O
et	O
al	O
,	O
2016	O
;	O
Mahajan	O
et	O
al	O
,	O
2018	O
;	O
Radford	O
et	O
al	O
,	O
2019	O
)	O
.	O
Speech	O
signal	O
consists	O
of	O
acoustic	O
,	O
linguistic	O
,	O
prosodic	O
,	O
and	O
speaker	O
characteristics	O
.	O
SSL	B-DatasetName
algo	O
-	O
rithms	O
in	O
speech	O
must	O
be	O
evaluated	O
in	O
their	O
ability	O
to	O
produce	O
representations	O
that	O
are	O
useful	O
for	O
tasks	O
that	O
demand	O
understanding	O
of	O
linguistic	O
,	O
speaker	O
,	O
and	O
prosodic	O
elements	O
of	O
spoken	O
language	O
as	O
well	O
as	O
high	O
-	O
level	O
semantics	O
.	O
Researchers	O
have	O
used	O
auto	O
-	O
regressive	O
,	O
contrastive	O
,	O
discriminative	O
and	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
objectives	O
to	O
pre	O
-	O
train	O
models	O
,	O
and	O
have	O
investigated	O
their	O
capabilities	O
across	O
tasks	O
like	O
phoneme	O
recognition	O
(	O
van	O
den	O
Oord	O
et	O
al	O
,	O
2018	O
;	O
Chung	O
et	O
al	O
,	O
2019	O
)	O
,	O
automatic	B-TaskName
speech	I-TaskName
recognition	I-TaskName
(	O
ASR	O
)	O
(	O
Liu	O
et	O
al	O
,	O
2020b	O
;	O
Schneider	O
et	O
al	O
,	O
2019	O
;	O
Ravanelli	O
et	O
al	O
,	O
2020	O
;	O
Hsu	O
et	O
al	O
,	O
2021	O
;	O
Chang	O
et	O
al	O
,	O
2021	O
)	O
,	O
speaker	B-TaskName
verification	I-TaskName
(	O
Fan	O
et	O
al	O
,	O
2020	O
)	O
,	O
speaker	B-TaskName
identification	I-TaskName
(	O
Chung	O
et	O
al	O
,	O
2019	O
;	O
Liu	O
et	O
al	O
,	O
2020c	O
)	O
,	O
emotion	B-TaskName
recognition	I-TaskName
(	O
Macary	O
et	O
al	O
,	O
2021	O
)	O
,	O
speech	O
translation	O
(	O
Chung	O
et	O
al	O
,	O
2019	O
)	O
,	O
voice	B-TaskName
conversion	I-TaskName
(	O
Lin	O
et	O
al	O
,	O
2020	O
;	O
Huang	O
et	O
al	O
,	O
2021a	O
)	O
,	O
spoken	B-TaskName
language	I-TaskName
understanding	I-TaskName
(	O
Lai	O
et	O
al	O
,	O
2021	O
)	O
,	O
and	O
text	O
-	O
tospeech	O
(	O
Álvarez	O
et	O
al	O
,	O
2019	O
)	O
.	O
However	O
,	O
the	O
methodologies	O
in	O
such	O
studies	O
vary	O
in	O
the	O
use	O
of	O
datasets	O
,	O
fine	O
-	O
tuning	O
strategies	O
and	O
task	O
-	O
specific	O
model	O
architectures	O
.	O
To	O
bridge	O
this	O
gap	O
,	O
SUPERB	O
(	O
Yang	O
et	O
al	O
,	O
2021	O
)	O
introduced	O
a	O
standardized	O
benchmark	O
of	O
10	O
speech	O
tasks	O
to	O
compare	O
13	O
pre	O
-	O
trained	O
models	O
and	O
a	O
Log	O
Mel	O
-	O
Filterbank	O
baseline	O
.	O
It	O
studied	O
the	O
models	O
'	O
performance	O
in	O
tasks	O
focusing	O
on	O
linguistic	O
(	O
phoneme	O
recognition	O
and	O
automatic	B-TaskName
speech	I-TaskName
recognition	I-TaskName
,	O
keyword	B-TaskName
spotting	I-TaskName
and	O
query	O
by	O
example	O
)	O
,	O
shallow	O
semantic	O
(	O
intent	B-TaskName
classification	I-TaskName
and	O
slot	B-TaskName
filling	I-TaskName
)	O
,	O
speaker	O
(	O
speaker	B-TaskName
identification	I-TaskName
,	O
speaker	B-TaskName
verification	I-TaskName
and	O
speaker	B-TaskName
diarization	I-TaskName
)	O
,	O
and	O
prosodic	O
(	O
emotion	B-TaskName
recognition	I-TaskName
)	O
characteristics	O
.	O
In	O
this	O
paper	O
,	O
we	O
introduce	O
SUPERB	O
-	O
SG	O
,	O
a	O
benchmark	O
with	O
5	O
new	O
tasks	O
,	O
which	O
are	O
speech	O
translation	O
,	O
out	O
-	O
of	O
-	O
domain	O
ASR	O
,	O
voice	B-TaskName
conversion	I-TaskName
,	O
speech	B-TaskName
separation	I-TaskName
,	O
and	O
speech	B-TaskName
enhancement	I-TaskName
,	O
with	O
an	O
emphasis	O
on	O
evaluating	O
the	O
semantic	O
and	O
generative	O
capabilities	O
of	O
pre	O
-	O
trained	O
models	O
that	O
require	O
high	O
-	O
level	O
representations	O
to	O
capture	O
linguistic	O
,	O
semantic	O
,	O
and	O
speaker	O
characteristics	O
.	O
These	O
tasks	O
go	O
beyond	O
speech	B-TaskName
recognition	I-TaskName
by	O
focusing	O
on	O
various	O
other	O
aspects	O
that	O
are	O
essential	O
to	O
building	O
intelligent	O
speech	O
interfaces	O
.	O
Further	O
,	O
we	O
show	O
that	O
while	O
SSL	B-DatasetName
models	O
achieve	O
close	O
to	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
many	O
tasks	O
,	O
there	O
is	O
n't	O
one	O
model	O
that	O
outperforms	O
all	O
others	O
,	O
and	O
that	O
a	O
simple	O
Log	O
Mel	O
-	O
Filterbank	O
can	O
perform	O
competitively	O
on	O
some	O
tasks	O
.	O
We	O
also	O
demonstrate	O
the	O
robustness	O
of	O
our	O
methodology	O
with	O
an	O
ablation	O
study	O
over	O
different	O
task	O
-	O
specific	O
model	O
architectures	O
and	O
data	O
sizes	O
.	O

As	O
more	O
powerful	O
SSL	B-DatasetName
models	O
are	O
proposed	O
with	O
promising	O
performance	O
on	O
various	O
tasks	O
,	O
researchers	O
continually	O
try	O
to	O
find	O
extensive	O
evaluation	O
methods	O
to	O
assess	O
model	O
performance	O
,	O
and	O
wish	O
to	O
holistically	O
understand	O
the	O
capability	O
of	O
the	O
learned	O
representations	O
in	O
these	O
models	O
.	O
SUPERB	O
(	O
Yang	O
et	O
al	O
,	O
2021	O
)	O
is	O
a	O
framework	O
to	O
benchmark	O
the	O
SSL	B-DatasetName
models	O
on	O
10	O
speech	O
tasks	O
by	O
learning	O
task	O
-	O
specific	O
prediction	O
heads	O
on	O
top	O
of	O
the	O
frozen	O
shared	O
SSL	B-DatasetName
models	O
.	O
Although	O
the	O
tasks	O
in	O
SUPERB	O
span	O
across	O
different	O
domains	O
,	O
most	O
of	O
them	O
are	O
simple	O
classification	O
problems	O
,	O
or	O
only	O
require	O
utilization	O
of	O
shallow	O
semantics	O
.	O
In	O
contrast	O
,	O
we	O
focus	O
on	O
harder	O
semantic	O
and	O
generative	O
tasks	O
.	O
Another	O
recently	O
proposed	O
benchmark	O
is	O
the	O
LeBenchmark	O
(	O
Evain	O
et	O
al	O
,	O
2021	O
)	O
,	O
investigating	O
the	O
performance	O
of	O
SSL	B-DatasetName
models	O
trained	O
on	O
French	O
data	O
with	O
three	O
semantic	O
tasks	O
.	O
However	O
,	O
they	O
only	O
consider	O
wav2vec	O
2.0	O
(	O
Baevski	O
et	O
al	O
,	O
2020b	O
)	O
with	O
different	O
architectures	O
as	O
their	O
upstream	O
models	O
(	O
i.e.	O
,	O
networks	O
pre	O
-	O
trained	O
with	O
SSL	B-DatasetName
)	O
.	O
Here	O
,	O
we	O
evaluate	O
a	O
diverse	O
set	O
of	O
SSL	B-DatasetName
models	O
,	O
and	O
offer	O
a	O
more	O
comprehensive	O
analysis	O
.	O
The	O
Zero	O
Resource	O
Speech	O
Benchmark	O
2021	O
(	O
Nguyen	O
et	O
al	O
,	O
2020	O
)	O
introduces	O
unsupervised	O
speech	O
processing	O
tasks	O
,	O
particularly	O
the	O
spoken	O
language	O
modeling	O
problem	O
.	O
They	O
evaluate	O
the	O
SSL	B-DatasetName
models	O
via	O
zero	O
-	O
shot	O
probings	O
at	O
four	O
linguistic	O
levels	O
.	O
While	O
their	O
benchmark	O
task	O
is	O
specific	O
for	O
certain	O
domain	O
,	O
we	O
use	O
various	O
tasks	O
to	O
evaluate	O
different	O
aspects	O
of	O
SSL	B-DatasetName
models	O
.	O
The	O
HEAR	O
2021	O
Challenge	O
2	O
aims	O
to	O
develop	O
general	O
-	O
purpose	O
audio	O
representation	O
by	O
focusing	O
on	O
audio	O
tasks	O
beyond	O
speech	O
that	O
include	O
sound	B-TaskName
event	I-TaskName
detection	I-TaskName
,	O
speech	B-DatasetName
commands	I-DatasetName
and	O
pitch	O
&	O
chroma	O
classification	O
.	O
We	O
specifically	O
focus	O
on	O
various	O
aspects	O
of	O
speech	O
processing	O
,	O
thus	O
providing	O
a	O
wide	O
variety	O
of	O
spoken	O
language	O
tasks	O
.	O

Speech	B-TaskName
separation	I-TaskName
(	O
SS	O
)	O
is	O
the	O
task	O
of	O
separating	O
target	O
speech	O
from	O
background	O
interference	O
.	O
We	O
choose	O
frequency	O
domain	O
method	O
instead	O
of	O
a	O
time	O
domain	O
based	O
method	O
because	O
of	O
the	O
stride	O
size	O
constraint	O
and	O
computational	O
cost	O
.	O

Speech	B-TaskName
enhancement	I-TaskName
(	O
SE	O
)	O
is	O
the	O
task	O
of	O
removing	O
background	O
noise	O
from	O
a	O
degraded	O
speech	O
signal	O
,	O
and	O
it	O
aims	O
to	O
improve	O
the	O
perceived	O
quality	O
and	O
intelligibility	O
of	O
the	O
signal	O
.	O
We	O
include	O
this	O
.	O
A	O
3	O
-	O
layer	O
BLSTM	O
model	O
similar	O
to	O
the	O
speech	B-TaskName
separation	I-TaskName
task	O
is	O
trained	O
to	O
predict	O
the	O
spectral	O
mask	O
for	O
the	O
clean	O
signal	O
.	O
The	O
mean	O
square	O
error	O
between	O
the	O
predicted	O
mask	O
and	O
INPSM	O
is	O
used	O
as	O
the	O
objective	O
.	O

We	O
evaluate	O
the	O
tasks	O
on	O
15	O
upstream	O
models	O
,	O
which	O
are	O
PASE+	B-MethodName
(	O
Ravanelli	O
et	O
al	O
,	O
2020	O
)	O
,	O
APC	O
(	O
Chung	O
et	O
al	O
,	O
2019	O
)	O
,	O
VQ	O
-	O
APC	O
,	O
NPC	O
(	O
Liu	O
et	O
al	O
,	O
2020a	O
)	O
,	O
Mockingjay	O
(	O
Liu	O
et	O
al	O
,	O
2020c	O
)	O
,	O
TERA	O
(	O
Liu	O
et	O
al	O
,	O
2020b	O
)	O
,	O
DeCoAR	O
2.0	O
(	O
Ling	O
and	O
Liu	O
,	O
2020	O
)	O
,	O
Modifile	O
CPC	O
(	O
Rivière	O
et	O
al	O
,	O
2020	O
)	O
,	O
wav2vec	O
family	O
(	O
Schneider	O
et	O
al	O
,	O
2019	O
)	O
(	O
Baevski	O
et	O
al	O
,	O
2020a	O
)	O
(	O
Baevski	O
et	O
al	O
,	O
2020b	O
)	O
and	O
HuBERT	O
(	O
Hsu	O
et	O
al	O
,	O
2021	O
)	O
.	O
They	O
span	O
across	O
different	O
architectures	O
,	O
sizes	O
and	O
learning	O
objectives	O
.	O
Some	O
models	O
also	O
use	O
vector	O
quantization	B-TaskName
which	O
has	O
an	O
added	O
benefit	O
of	O
signal	O
compression	O
.	O
For	O
grounding	O
,	O
we	O
use	O
Log	O
Mel	O
Filterbank	O
as	O
our	O
baseline	O
.	O
The	O
detailed	O
properties	O
of	O
upstream	O
mod	O
-	O
els	O
are	O
shown	O
in	O
Table	O
1	O
.	O

The	O
results	O
of	O
the	O
upstream	O
models	O
evaluated	O
on	O
SUPERB	O
-	O
SG	O
are	O
shown	O
in	O
Table	O
2	O
.	O
We	O
only	O
report	O
the	O
averaged	O
WER	O
for	O
OOD	O
-	O
ASR	O
.	O
Full	O
results	O
can	O
be	O
found	O
in	O
Appendix	O
A.	O
For	O
speech	O
-	O
to	O
-	O
text	O
tasks	O
(	O
ST	O
and	O
OOD	O
-	O
ASR	O
)	O
,	O
wav2vec	O
2.0	O
and	O
HuBERT	O
offer	O
competitive	O
results	O
,	O
while	O
DeCoAR	O
2.0	O
shows	O
some	O
improvements	O
.	O
In	O
speech	O
generation	O
tasks	O
(	O
VC	O
,	O
SS	O
,	O
and	O
SE	O
)	O
,	O
FBANK	O
yields	O
comparable	O
or	O
superior	O
performance	O
than	O
some	O
SSL	B-DatasetName
models	O
,	O
especially	O
for	O
those	O
metrics	O
that	O
take	O
the	O
quality	O
of	O
the	O
output	O
signal	O
into	O
account	O
.	O
For	O
VC	O
,	O
the	O
3	O
reported	O
metrics	O
have	O
the	O
same	O
trend	O
for	O
respective	O
models	O
.	O
Here	O
,	O
vq	O
-	O
wav2vec	O
achieves	O
the	O
best	O
performance	O
on	O
MCD	O
and	O
ASV	O
,	O
while	O
HuBERT	O
performs	O
the	O
best	O
on	O
WER	O
.	O
For	O
SS	O
,	O
Hubert	O
-	O
Large	O
achieves	O
the	O
best	O
performance	O
,	O
followed	O
by	O
Modified	O
CPC	O
.	O
PASE+	B-MethodName
,	O
which	O
is	O
pre	O
-	O
trained	O
with	O
denoising	B-TaskName
tasks	O
,	O
performs	O
better	O
than	O
half	O
the	O
SSL	B-DatasetName
models	O
,	O
but	O
this	O
observation	O
does	O
n't	O
transfer	O
to	O
the	O
other	O
tasks	O
.	O
For	O
SE	O
,	O
all	O
upstream	O
models	O
perform	O
comparably	O
.	O
The	O
largest	O
gap	O
is	O
only	O
0.17	O
in	O
PESQ	O
and	O
1.1	O
in	O
STOI	O
.	O
Overall	O
,	O
no	O
model	O
outperforms	O
all	O
others	O
on	O
all	O
tasks	O
.	O
However	O
,	O
HuBERT	O
-	O
Large	O
performs	O
most	O
competitively	O
on	O
all	O
downstream	O
tasks	O
,	O
especially	O
those	O
requiring	O
linguistic	O
and	O
semantic	O
signals	O
.	O

We	O
analyze	O
the	O
correlations	O
between	O
tasks	O
in	O
SUPERB	O
-	O
SG	O
to	O
understand	O
the	O
similarity	O
between	O
tasks	O
,	O
and	O
verify	O
if	O
the	O
experimental	O
results	O
agree	O
with	O
the	O
common	O
understanding	O
of	O
related	O
tasks	O
based	O
on	O
shared	O
representation	O
they	O
require	O
.	O
To	O
compute	O
the	O
correlation	O
,	O
we	O
first	O
change	O
all	O
metrics	O
into	O
a	O
higher	O
-	O
better	O
manner	O
.	O
Then	O
,	O
we	O
compute	O
the	O
Spearman	O
's	O
rank	O
correlation	O
coefficients	O
(	O
Spearman	O
's	O
ρ	O
)	O
between	O
all	O
pairs	O
of	O
tasks	O
.	O
For	O
multiple	O
metrics	O
contained	O
in	O
a	O
single	O
task	O
,	O
such	O
as	O
MCD	O
/	O
WER	O
/	O
ASV	O
in	O
VC	O
as	O
well	O
as	O
PESQ	O
/	O
STOI	O
in	O
SE	O
,	O
we	O
compute	O
each	O
of	O
them	O
separately	O
.	O
To	O
make	O
our	O
analysis	O
more	O
representative	O
and	O
generalized	O
to	O
all	O
speech	O
domains	O
,	O
we	O
bring	O
back	O
the	O
six	O
tasks	O
from	O
SUPERB	O
(	O
Yang	O
et	O
al	O
,	O
2021	O
)	O
that	O
are	O
considered	O
representative	O
of	O
the	O
following	O
four	O
domains	O
:	O
(	O
i	O
)	O
Content	O
recognition	O
tasks	O
contain	O
-	O
ing	O
Phoneme	O
Recognition	O
(	O
PR	O
)	O
,	O
Automatic	B-TaskName
Speech	I-TaskName
Recognition	I-TaskName
(	O
ASR	O
)	O
(	O
ii	O
)	O
Speaker	O
identity	O
tasks	O
including	O
Identification	O
(	O
SID	B-DatasetName
)	O
,	O
Automatic	O
Speaker	B-TaskName
Verification	I-TaskName
(	O
ASV	O
)	O
(	O
iii	O
)	O
Semantics	O
task	O
which	O
is	O
Intent	B-TaskName
Classification	I-TaskName
(	O
IC	O
)	O
and	O
(	O
iv	O
)	O
Prosodic	O
task	O
which	O
is	O
Emotion	B-TaskName
Recognition	I-TaskName
(	O
ER	O
)	O
.	O
Together	O
with	O
the	O
5	O
tasks	O
introduced	O
in	O
this	O
paper	O
,	O
we	O
show	O
the	O
results	O
of	O
total	O
11	O
downstream	O
tasks	O
with	O
the	O
14	O
corresponding	O
metrics	O
in	O
Figure	O
2	O
.	O
Overall	O
,	O
results	O
show	O
that	O
all	O
tasks	O
except	O
SS	O
and	O
SE	O
have	O
strong	O
positive	O
correlation	O
among	O
them	O
.	O
One	O
possible	O
explanation	O
for	O
SS	O
and	O
SE	O
not	O
showing	O
strong	O
correlation	O
is	O
that	O
the	O
low	O
-	O
level	O
information	O
closely	O
related	O
to	O
audio	O
signals	O
is	O
more	O
critical	O
as	O
they	O
need	O
to	O
reconstruct	O
clean	O
speech	O
from	O
interfering	O
speakers	O
and	O
background	O
noise	O
by	O
estimating	O
the	O
STFT	O
masks	O
.	O
As	O
a	O
result	O
,	O
high	O
-	O
level	O
information	O
extracted	O
from	O
SSL	B-DatasetName
models	O
has	O
little	O
benefit	O
for	O
these	O
tasks	O
but	O
is	O
helpful	O
for	O
other	O
tasks	O
.	O
As	O
noted	O
earlier	O
,	O
there	O
is	O
only	O
a	O
small	O
gap	O
in	O
performance	O
between	O
FBANK	O
and	O
SSL	B-DatasetName
models	O
.	O
If	O
we	O
leave	O
SS	O
and	O
SE	O
out	O
,	O
all	O
correlation	O
coefficients	O
are	O
greater	O
than	O
0.58	O
,	O
showing	O
that	O
the	O
SSL	B-DatasetName
model	O
representations	O
are	O
useful	O
for	O
multiple	O
domains	O
.	O
Although	O
the	O
Spearman	O
's	O
ρ	O
are	O
large	O
in	O
general	O
in	O
Figure	O
2	O
,	O
differences	O
between	O
tasks	O
are	O
observable	O
.	O
Here	O
,	O
we	O
focus	O
on	O
the	O
relation	O
between	O
correlation	O
and	O
similarity	O
of	O
tasks	O
.	O
We	O
list	O
the	O
most	O
and	O
the	O
least	O
two	O
correlated	O
tasks	O
comparing	O
with	O
ST	O
,	O
OOD	O
-	O
ASR	O
,	O
VC	O
,	O
SS	O
,	O
and	O
SE	O
.	O
SS	O
and	O
SE	O
are	O
skipped	O
as	O
candidates	O
for	O
for	O
the	O
least	O
correlated	O
tasks	O
since	O
they	O
dominate	O
the	O
results	O
.	O
For	O
VC	O
,	O
we	O
average	O
the	O
correlation	O
coefficients	O
across	O
the	O
three	O
metrics	O
.	O
The	O
results	O
are	O
shown	O
in	O
Table	O
3	O
.	O
ST	O
and	O
OOD	O
-	O
ASR	O
are	O
highly	O
correlated	O
with	O
ASR	O
since	O
they	O
both	O
transform	O
speech	O
signals	O
into	O
discrete	O
text	O
tokens	O
.	O
IC	O
is	O
also	O
correlated	O
with	O
ST	O
since	O
semantic	O
information	O
is	O
required	O
to	O
perform	O
both	O
tasks	O
.	O
Moreover	O
,	O
ASV	O
and	O
VC	O
are	O
the	O
least	O
correlated	O
tasks	O
since	O
they	O
primarily	O
focus	O
on	O
the	O
speaker	O
information	O
with	O
lesser	O
regard	O
to	O
the	O
semantic	O
content	O
.	O
However	O
,	O
the	O
absolute	O
correlation	O
values	O
are	O
still	O
larger	O
than	O
0.7	O
.	O
For	O
VC	O
,	O
the	O
speaker	O
information	O
needs	O
to	O
be	O
removed	O
while	O
the	O
content	O
has	O
to	O
be	O
kept	O
,	O
similar	O
to	O
PR	O
and	O
ASR	O
but	O
different	O
from	O
SID	B-DatasetName
.	O
SS	O
and	O
SE	O
are	O
correlated	O
with	O
each	O
other	O
and	O
have	O
a	O
much	O
lower	O
correlation	O
with	O
speaker	O
identity	O
and	O
semantics	O
tasks	O
,	O
supporting	O
our	O
assumption	O
.	O
Overall	O
,	O
we	O
find	O
that	O
empirically	O
highly	O
-	O
correlated	O
tasks	O
require	O
similar	O
knowledge	O
or	O
understanding	O
ability	O
.	O
To	O
give	O
a	O
broader	O
view	O
of	O
our	O
correlation	O
results	O
,	O
we	O
further	O
cluster	O
the	O
downstream	O
tasks	O
by	O
their	O
correlation	O
with	O
each	O
other	O
using	O
K	O
-	O
means	O
.	O
In	O
this	O
way	O
,	O
all	O
the	O
tasks	O
are	O
considered	O
simultaneously	O
,	O
and	O
the	O
grouping	O
is	O
driven	O
automatically	O
by	O
the	O
empirical	O
correlation	O
results	O
.	O
If	O
more	O
than	O
one	O
metric	O
are	O
used	O
in	O
a	O
downstream	O
task	O
,	O
we	O
cluster	O
them	O
independently	O
.	O
The	O
clustering	O
results	O
are	O
shown	O
in	O
Table	O
4	O
and	O
a	O
rearranged	O
correlation	O
map	O
is	O
shown	O
in	O
Figure	O
3	O
.	O
The	O
result	O
shows	O
that	O
the	O
clusters	O
of	O
the	O
tasks	O
align	O
with	O
our	O
empirical	O
knowledge	O
.	O
Cluster	O
A	O
includes	O
tasks	O
that	O
require	O
content	O
information	O
,	O
while	O
tasks	O
in	O
cluster	O
B	O
are	O
more	O
sensitive	O
to	O
speaker	O
and	O
prosodic	O
features	O
.	O
Cluster	O
C	O
contains	O
metrics	O
MCD	O
and	O
ASV	O
of	O
VC	O
,	O
which	O
are	O
used	O
to	O
evaluate	O
the	O
signal	O
quality	O
and	O
the	O
rates	O
of	O
speaker	O
transfer	O
.	O
It	O
is	O
worth	O
noting	O
that	O
WER	O
in	O
VC	O
belongs	O
to	O
cluster	O
A	O
,	O
showing	O
that	O
it	O
is	O
more	O
similar	O
to	O
content	O
-	O
related	O
tasks	O
.	O
Furthermore	O
,	O
clusters	O
D	O
,	O
E	O
,	O
and	O
F	O
each	O
contain	O
one	O
of	O
the	O
metrics	O
in	O
SS	O
and	O
SE	O
,	O
aligning	O
with	O
our	O
assumption	O
that	O
these	O
tasks	O
utilize	O
different	O
types	O
of	O
information	O
compared	O
to	O
other	O
tasks	O
.	O
With	O
the	O
analysis	O
of	O
the	O
correlation	O
between	O
tasks	O
,	O
we	O
empirically	O
confirm	O
the	O
reliability	O
of	O
the	O
results	O
,	O
and	O
show	O
that	O
we	O
increase	O
the	O
heterogeneity	O
among	O
speech	O
tasks	O
over	O
SUPERB	O
.	O
We	O
further	O
discover	O
shared	O
properties	O
between	O
tasks	O
with	O
clustering	O
,	O
and	O
the	O
result	O
is	O
aligned	O
with	O
our	O
common	O
understanding	O
of	O
related	O
tasks	O
.	O

To	O
study	O
the	O
impact	O
of	O
downstream	O
model	O
architecture	O
and	O
the	O
data	O
sizes	O
used	O
in	O
SUPERB	O
-	O
SG	O
we	O
evaluate	O
the	O
robustness	O
of	O
SUPERB	O
-	O
SG	O
with	O
variations	O
in	O
downstream	O
model	O
as	O
well	O
as	O
training	O
data	O
size	O
,	O
and	O
show	O
that	O
our	O
conclusions	O
still	O
hold	O
true	O
.	O
We	O
choose	O
ST	O
,	O
OOD	O
-	O
ASR	O
and	O
SS	O
as	O
the	O
downstream	O
tasks	O
for	O
evaluation	O
with	O
an	O
aim	O
to	O
cover	O
semantic	O
,	O
content	O
recognition	O
,	O
and	O
generative	O
task	O
types	O
.	O
For	O
the	O
upstream	O
models	O
,	O
FBANK	O
,	O
TERA	O
,	O
CPC	O
,	O
wav2vec	O
2.0	O
Base	O
and	O
HuBERT	O
Base	O
are	O
used	O
to	O
cover	O
different	O
SSL	B-DatasetName
algorithms	O
.	O

To	O
study	O
the	O
effect	O
of	O
data	O
size	O
,	O
we	O
create	O
3	O
pseudo	O
datasets	O
per	O
task	O
by	O
sub	O
-	O
sampling	O
10	O
%	O
,	O
5	O
%	O
and	O
1	O
%	O
from	O
the	O
original	O
training	O
set	O
while	O
fixing	O
the	O
validation	O
and	O
test	O
sets	O
.	O
The	O
statistics	O
of	O
the	O
datasets	O
are	O
shown	O
in	O
Table	O
7	O
,	O
and	O
the	O
results	O
are	O
in	O
Table	O
8	O
.	O
The	O
ranking	O
of	O
the	O
upstream	O
models	O
remains	O
almost	O
the	O
same	O
for	O
10	O
%	O
of	O
training	O
data	O
.	O
When	O
that	O
is	O
further	O
reduced	O
to	O
5	O
%	O
,	O
there	O
is	O
a	O
change	O
in	O
ranking	O
in	O
SS	O
due	O
to	O
a	O
performance	O
drop	O
in	O
Modified	O
CPC	O
.	O
Excluding	O
Modified	O
CPC	O
,	O
the	O
ranking	O
is	O
still	O
fixed	O
showing	O
that	O
the	O
relative	O
performance	O
of	O
the	O
upstream	O
models	O
is	O
agnostic	O
to	O
data	O
size	O
.	O
Furthermore	O
,	O
when	O
using	O
only	O
1	O
%	O
of	O
training	O
data	O
,	O
most	O
of	O
the	O
SSL	B-DatasetName
models	O
fail	O
on	O
the	O
3	O
downstream	O
tasks	O
.	O
This	O
phenomenon	O
is	O
caused	O
by	O
insufficient	O
task	O
-	O
specific	O
knowledge	O
due	O
to	O
limited	O
training	O
data	O
size	O
.	O
Although	O
SSL	B-DatasetName
models	O
learn	O
highlevel	O
representations	O
from	O
the	O
unlabeled	O
speech	O
signal	O
,	O
acquisition	O
of	O
task	O
-	O
specific	O
knowledge	O
such	O
as	O
translingual	O
ability	O
in	O
ST	O
,	O
text	O
-	O
level	O
token	O
mapping	O
in	O
OOD	O
-	O
ASR	O
,	O
and	O
mask	O
prediction	O
in	O
SS	O
,	O
requires	O
non	O
-	O
trivial	O
supervision	O
.	O
We	O
We	O
have	O
open	O
-	O
sourced	O
all	O
the	O
codes	O
1	O
and	O
released	O
a	O
challenge	O
3	O
to	O
encourage	O
further	O
research	O
of	O
SSL	B-DatasetName
in	O
speech	O
.	O
We	O
welcome	O
the	O
community	O
to	O
participate	O
and	O
advance	O
the	O
research	O
frontier	O
together	O
.	O

Here	O
,	O
we	O
provide	O
complete	O
results	O
of	O
OOD	O
-	O
ASR	O
tasks	O
,	O
as	O
shown	O
in	O
Tables	O
9	O
,	O
10	O
,	O
11	O
.	O
All	O
upstream	O
models	O
used	O
in	O
this	O
paper	O
are	O
trained	O
with	O
English	O
speech	O
data	O
,	O
but	O
we	O
are	O
also	O
interested	O
in	O
multilingual	O
pre	O
-	O
trained	O
models	O
in	O
OOD	O
-	O
ASR	O
.	O
Therefore	O
,	O
we	O
evaluate	O
the	O
wav2vec	O
2.0	O
XLSR	B-MethodName
model	O
on	O
the	O
OOD	O
-	O
ASR	O
tasks	O
,	O
as	O
shown	O
in	O
the	O
last	O
row	O
of	O
Table	O
9	O
.	O
XLSR	B-MethodName
has	O
identical	O
architecture	O
as	O
wav2vec	O
2.0	O
Large	O
,	O
but	O
is	O
trained	O
with	O
56k	O
hours	O
of	O
speech	O
including	O
53	O
different	O
languages	O
.	O
The	O
pre	O
-	O
training	O
data	O
of	O
XLSR	B-MethodName
cover	O
our	O
cross	O
-	O
lingual	O
tasks	O
'	O
training	O
data	O
.	O
As	O
expected	O
,	O
using	O
multilingual	O
data	O
improves	O
OOD	O
-	O
ASR	O
tasks	O
and	O
achieves	O
the	O
best	O
performance	O
among	O
all	O
upstream	O
models	O
.	O

Building	O
an	O
end	O
-	O
to	O
-	O
end	O
conversational	O
agent	B-DatasetName
for	O
multi	O
-	O
domain	O
task	O
-	O
oriented	O
dialogues	O
has	O
been	O
an	O
open	O
challenge	O
for	O
two	O
main	O
reasons	O
.	O
First	O
,	O
tracking	O
dialogue	O
states	O
of	O
multiple	O
domains	O
is	O
non	O
-	O
trivial	O
as	O
the	O
dialogue	O
agent	B-DatasetName
must	O
obtain	O
complete	O
states	O
from	O
all	O
relevant	O
domains	O
,	O
some	O
of	O
which	O
might	O
have	O
shared	O
slots	O
among	O
domains	O
as	O
well	O
as	O
unique	O
slots	O
specifically	O
for	O
one	O
domain	O
only	O
.	O
Second	O
,	O
the	O
dialogue	O
agent	B-DatasetName
must	O
also	O
process	O
various	O
types	O
of	O
information	O
across	O
domains	O
,	O
including	O
dialogue	O
context	O
,	O
dialogue	O
states	O
,	O
and	O
database	O
,	O
to	O
generate	O
natural	O
responses	O
to	O
users	O
.	O
Unlike	O
the	O
existing	O
approaches	O
that	O
are	O
often	O
designed	O
to	O
train	O
each	O
module	O
separately	O
,	O
we	O
propose	O
"	O
UniConv	O
"	O
-	O
a	O
novel	O
unified	O
neural	O
architecture	O
for	O
end	O
-	O
to	O
-	O
end	O
conversational	O
systems	O
in	O
multi	O
-	O
domain	O
task	O
-	O
oriented	O
dialogues	O
,	O
which	O
is	O
designed	O
to	O
jointly	O
train	O
(	O
i	O
)	O
a	O
Bi	O
-	O
level	O
State	O
Tracker	O
which	O
tracks	O
dialogue	O
states	O
by	O
learning	O
signals	O
at	O
both	O
slot	O
and	O
domain	O
level	O
independently	O
,	O
and	O
(	O
ii	O
)	O
a	O
Joint	O
Dialogue	O
Act	O
and	O
Response	O
Generator	O
which	O
incorporates	O
information	O
from	O
various	O
input	O
components	O
and	O
models	O
dialogue	O
acts	O
and	O
target	O
responses	O
simultaneously	O
.	O
We	O
conduct	O
comprehensive	O
experiments	O
in	O
dialogue	B-TaskName
state	I-TaskName
tracking	I-TaskName
,	O
contextto	O
-	O
text	O
,	O
and	O
end	O
-	O
to	O
-	O
end	O
settings	O
on	O
the	O
Multi	O
-	O
WOZ2.1	O
benchmark	O
,	O
achieving	O
superior	O
performance	O
over	O
competitive	O
baselines	O
.	O

A	O
conventional	O
approach	O
to	O
task	O
-	O
oriented	O
dialogues	O
is	O
to	O
solve	O
four	O
distinct	O
tasks	O
:	O
(	O
1	O
)	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
(	O
NLU	O
)	O
which	O
parses	O
user	O
utterance	O
into	O
a	O
semantic	O
frame	O
,	O
(	O
2	O
)	O
dialogue	B-TaskName
state	I-TaskName
tracking	I-TaskName
(	O
DST	O
)	O
which	O
updates	O
the	O
slots	O
and	O
values	O
from	O
semantic	O
frames	O
to	O
the	O
latest	O
values	O
for	O
knowledge	O
base	O
retrieval	O
,	O
(	O
3	O
)	O
dialogue	O
policy	O
which	O
determines	O
an	O
appropriate	O
dialogue	O
act	O
for	O
the	O
next	O
system	O
response	O
,	O
and	O
(	O
4	O
)	O
response	B-TaskName
generation	I-TaskName
which	O
generates	O
a	O
natural	O
language	O
sequence	O
conditioned	O
on	O
the	O
dialogue	O
act	O
.	O
This	O
traditional	O
pipeline	O
modular	O
framework	O
has	O
achieved	O
remarkable	O
successes	O
in	O
task	O
-	O
oriented	O
dialogues	O
(	O
Wen	O
et	O
al	O
,	O
2017	O
;	O
Liu	O
and	O
Lane	O
,	O
2017	O
;	O
Williams	O
et	O
al	O
,	O
2017	O
;	O
Zhao	O
et	O
al	O
,	O
2017	O
)	O
.	O
However	O
,	O
such	O
kind	O
of	O
dialogue	O
system	O
is	O
not	O
fully	O
optimized	O
as	O
the	O
modules	O
are	O
loosely	O
integrated	O
and	O
often	O
not	O
trained	O
jointly	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
manner	O
,	O
and	O
thus	O
may	O
suffer	O
from	O
increasing	O
error	O
propagation	O
between	O
the	O
modules	O
as	O
the	O
complexity	O
of	O
the	O
dialogues	O
evolves	O
.	O
A	O
typical	O
case	O
of	O
a	O
complex	O
dialogue	O
setting	O
is	O
when	O
the	O
dialogue	O
extends	O
over	O
multiple	O
domains	O
.	O
A	O
dialogue	O
state	O
in	O
a	O
multi	O
-	O
domain	O
dialogue	O
should	O
include	O
slots	O
of	O
all	O
applicable	O
domains	O
up	O
to	O
the	O
current	O
turn	O
(	O
See	O
Table	O
1	O
)	O
.	O
Each	O
domain	O
can	O
have	O
shared	O
slots	O
that	O
are	O
common	O
among	O
domains	O
or	O
unique	O
slots	O
that	O
are	O
not	O
shared	O
with	O
any	O
.	O
Directly	O
applying	O
single	O
-	O
domain	O
DST	O
to	O
multi	O
-	O
domain	O
dialogues	O
is	O
not	O
straightforward	O
because	O
the	O
dialogue	O
states	O
extend	O
to	O
multiple	O
domains	O
.	O
A	O
possible	O
approach	O
is	O
to	O
process	O
a	O
dialogue	O
of	O
N	O
D	O
domains	O
multiple	O
times	O
,	O
each	O
time	O
obtaining	O
a	O
dialogue	O
state	O
of	O
one	O
domain	O
.	O
However	O
,	O
this	O
approach	O
does	O
not	O
allow	O
learning	O
co	O
-	O
reference	O
in	O
dialogues	O
in	O
which	O
users	O
can	O
switch	O
from	O
one	O
domain	O
to	O
another	O
.	O
As	O
the	O
number	O
of	O
dialogue	O
domains	O
increases	O
,	O
traditional	O
pipeline	O
approaches	O
propagate	O
errors	O
from	O
dialogue	O
states	O
to	O
dialogue	O
policy	O
and	O
subsequently	O
,	O
to	O
natural	O
language	O
generator	O
.	O
Recent	O
efforts	O
Madotto	O
et	O
al	O
,	O
2018	O
;	O
Wu	O
et	O
al	O
,	O
2019b	O
)	O
address	O
this	O
problem	O
with	O
an	O
integrated	O
sequence	O
-	O
to	O
-	O
sequence	O
structure	O
.	O
These	O
approaches	O
often	O
consider	O
knowledge	O
bases	O
as	O
memory	O
tuples	O
rather	O
than	O
relational	O
entity	O
tables	O
.	O
While	O
achieving	O
impressive	O
performance	O
,	O
these	O
approaches	O
are	O
not	O
scalable	O
to	O
large	O
-	O
scale	O
knowledgebases	O
,	O
e.g.	O
thousands	O
of	O
entities	O
,	O
as	O
the	O
memory	O
cost	O
to	O
query	O
entity	O
attributes	O
increases	O
substantially	O
.	O
Another	O
limitation	O
of	O
these	O
approaches	O
is	O
the	O
absence	O
of	O
dialogue	O
act	O
modelling	O
.	O
Dialogue	O
act	O
Human	O
:	O
could	O
you	O
make	O
a	O
suggestion	O
?	O
one	O
in	O
the	O
centre	O
?	O
Dialogue	O
agent	B-DatasetName
:	O
fitzbillies	O
restaurant	O
is	O
an	O
expensive	O
british	O
restaurant	O
in	O
the	O
centre	O
.	O
can	O
i	O
book	O
that	O
for	O
you	O
?	O
Dialogue	O
state	O
:	O
{	O
restaurant	O
:	O
{	O
pricerange	O
:	O
expensive	O
,	O
area	O
:	O
centre	O
}	O
}	O
Dialogue	O
acts	O
:	O
[	O
inform	O
-	O
restaurant	O
,	O
request	O
-	O
booking	O
]	O
...	O
...	O
Human	O
:	O
also	O
,	O
i	O
need	O
the	O
number	O
for	O
kings	O
hedges	O
learner	O
pool	O
.	O
Dialogue	O
agent	B-DatasetName
:	O
the	O
phone	O
number	O
for	O
the	O
pool	O
is	O
01234567	O
,	O
is	O
there	O
something	O
else	O
i	O
can	O
help	O
you	O
?	O
Dialogue	O
state	O
:	O
{	O
restaurant	O
:	O
{	O
pricerange	O
:	O
expensive	O
,	O
area	O
:	O
centre	O
,	O
name	O
=	O
fizbillies	O
restaurant	O
,	O
request=	O
[	O
address	O
]	O
}	O
,	O
attraction	O
:	O
{	O
name	O
:	O
kings	O
hedges	O
learner	O
pool	O
,	O
request=	O
[	O
phone	O
]	O
}	O
}	O
Dialogue	O
acts	O
:	O
[	O
inform	O
-	O
phone	O
]	O
Table	O
1	O
:	O
Example	O
of	O
a	O
multi	O
-	O
domain	O
dialogue	O
with	O
two	O
domains	O
:	O
restaurant	O
and	O
attraction	O
.	O
is	O
particularly	O
important	O
in	O
task	O
-	O
oriented	O
dialogues	O
as	O
it	O
determines	O
the	O
general	O
decision	O
towards	O
task	O
completion	O
before	O
a	O
dialogue	O
agent	B-DatasetName
can	O
materialize	O
it	O
into	O
natural	O
language	O
response	O
(	O
See	O
Table	O
1	O
)	O
.	O
To	O
tackle	O
the	O
challenges	O
in	O
multi	O
-	O
domain	O
taskoriented	O
dialogues	O
while	O
reducing	O
error	O
propagation	O
among	O
dialogue	O
system	O
modules	O
and	O
keeping	O
the	O
models	O
scalable	O
,	O
we	O
propose	O
UniConv	O
,	O
a	O
unified	O
neural	O
network	O
architecture	O
for	O
end	O
-	O
to	O
-	O
end	O
dialogue	O
systems	O
.	O
UniConv	O
consists	O
of	O
a	O
Bi	O
-	O
level	O
State	O
Tracking	O
(	O
BDST	O
)	O
module	O
which	O
embeds	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
as	O
it	O
can	O
directly	O
parse	O
dialogue	O
context	O
into	O
a	O
structured	O
dialogue	O
state	O
rather	O
than	O
relying	O
on	O
the	O
semantic	O
frame	O
output	O
from	O
an	O
NLU	O
module	O
in	O
each	O
dialogue	O
turn	O
.	O
BDST	O
implicitly	O
models	O
and	O
integrates	O
slot	O
representations	O
from	O
dialogue	O
contextual	O
cues	O
to	O
directly	O
generate	O
slot	O
values	O
in	O
each	O
turn	O
and	O
thus	O
,	O
remove	O
the	O
need	O
for	O
explicit	O
slot	O
tagging	O
features	O
from	O
an	O
NLU	O
.	O
This	O
approach	O
is	O
more	O
practical	O
than	O
the	O
traditional	O
pipeline	O
models	O
as	O
we	O
do	O
not	O
need	O
slot	O
tagging	O
annotation	O
.	O
Furthermore	O
,	O
BDST	O
tracks	O
dialogue	O
states	O
in	O
dialogue	O
context	O
in	O
both	O
slot	O
and	O
domain	O
levels	O
.	O
The	O
output	O
representations	O
from	O
two	O
levels	O
are	O
combined	O
in	O
a	O
late	O
fusion	O
approach	O
to	O
learn	O
multi	O
-	O
domain	O
dialogue	O
states	O
.	O
Our	O
dialogue	O
state	O
tracker	O
disentangles	O
slot	O
and	O
domain	O
representation	B-TaskName
learning	I-TaskName
while	O
enabling	O
deep	O
learning	O
of	O
shared	O
representations	O
of	O
slots	O
common	O
among	O
domains	O
.	O
UniConv	O
integrates	O
BDST	O
with	O
a	O
Joint	O
Dialogue	O
Act	O
and	O
Response	O
Generator	O
(	O
DARG	O
)	O
that	O
simultaneously	O
models	O
dialogue	O
acts	O
and	O
generates	O
system	O
responses	O
by	O
learning	O
a	O
latent	O
variable	O
representing	O
dialogue	O
acts	O
and	O
semantically	O
conditioning	O
output	O
response	O
tokens	O
on	O
this	O
latent	O
variable	O
.	O
The	O
multitask	O
setting	O
of	O
DARG	O
allows	O
our	O
models	O
to	O
model	O
dialogue	O
acts	O
and	O
utilize	O
the	O
distributed	O
representations	O
of	O
dialogue	O
acts	O
,	O
rather	O
than	O
hard	O
discrete	O
output	O
values	O
from	O
a	O
dialogue	O
policy	O
module	O
,	O
on	O
output	O
response	O
tokens	O
.	O
Our	O
response	O
generator	O
incorporates	O
information	O
from	O
dialogue	O
input	O
components	O
and	O
intermediate	O
representations	O
progressively	O
over	O
multiple	O
attention	O
steps	O
.	O
The	O
output	O
representations	O
are	O
refined	O
after	O
each	O
step	O
to	O
obtain	O
high	O
-	O
resolution	O
signals	O
needed	O
to	O
generate	O
appropriate	O
dialogue	O
acts	O
and	O
responses	O
.	O
We	O
combine	O
both	O
BDST	O
and	O
DARG	O
for	O
end	O
-	O
to	O
-	O
end	O
neural	O
dialogue	O
systems	O
,	O
from	O
input	O
dialogues	O
to	O
output	O
system	O
responses	O
.	O
We	O
evaluate	O
our	O
models	O
on	O
the	O
large	O
-	O
scale	O
Mul	O
-	O
tiWOZ	O
benchmark	O
,	O
and	O
compare	O
with	O
the	O
existing	O
methods	O
in	O
DST	O
,	O
context	O
-	O
to	O
-	O
text	B-TaskName
generation	I-TaskName
,	O
and	O
end	O
-	O
to	O
-	O
end	O
settings	O
.	O
The	O
promising	O
performance	O
in	O
all	O
tasks	O
validates	O
the	O
efficacy	O
of	O
our	O
method	O
.	O

Dialogue	B-TaskName
State	I-TaskName
Tracking	I-TaskName
.	O
Traditionally	O
,	O
DST	O
models	O
are	O
designed	O
to	O
track	O
states	O
of	O
singledomain	O
dialogues	O
such	O
as	O
WOZ	O
(	O
Wen	O
et	O
al	O
,	O
2017	O
)	O
and	O
DSTC2	O
(	O
Henderson	O
et	O
al	O
,	O
2014a	O
)	O
benchmarks	O
.	O
There	O
have	O
been	O
recent	O
efforts	O
that	O
aim	O
to	O
tackle	O
multi	O
-	O
domain	O
DST	O
such	O
as	O
(	O
Ramadan	O
et	O
al	O
,	O
2018	O
;	O
Lee	O
et	O
al	O
,	O
2019	O
;	O
Wu	O
et	O
al	O
,	O
2019a	O
;	O
.	O
These	O
models	O
can	O
be	O
categorized	O
into	O
two	O
main	O
categories	O
:	O
Fixed	O
vocabulary	O
models	O
(	O
Zhong	O
et	O
al	O
,	O
2018	O
;	O
Ramadan	O
et	O
al	O
,	O
2018	O
;	O
Lee	O
et	O
al	O
,	O
2019	O
)	O
,	O
which	O
assume	O
known	O
slot	O
ontology	B-MethodName
with	O
a	O
fixed	O
candidate	O
set	O
for	O
each	O
slot	O
.	O
On	O
the	O
other	O
hand	O
,	O
open	O
-	O
vocabulary	O
models	O
(	O
Lei	O
et	O
al	O
,	O
2018	O
;	O
Wu	O
et	O
al	O
,	O
2019a	O
;	O
Le	O
et	O
al	O
,	O
2020	O
)	O
derive	O
the	O
candidate	O
set	O
based	O
on	O
the	O
source	O
sequence	O
i.e.	O
dialogue	O
history	O
,	O
itself	O
.	O
Our	O
approach	O
is	O
more	O
related	O
to	O
the	O
open	O
-	O
vocabulary	O
approach	O
as	O
we	O
aim	O
to	O
generate	O
unique	O
dialogue	O
states	O
depending	O
on	O
the	O
input	O
dialogue	O
.	O
Different	O
from	O
previous	O
Context	O
-	O
to	O
-	O
Text	B-TaskName
Generation	I-TaskName
.	O
This	O
task	O
was	O
traditionally	O
solved	O
by	O
two	O
separate	O
dialogue	O
modules	O
:	O
Dialogue	O
Policy	O
(	O
Peng	O
et	O
al	O
,	O
2017	O
(	O
Peng	O
et	O
al	O
,	O
,	O
2018	O
and	O
NLG	O
(	O
Wen	O
et	O
al	O
,	O
2016	O
;	O
Su	O
et	O
al	O
,	O
2018	O
)	O
.	O
Recent	O
work	O
attempts	O
to	O
combine	O
these	O
two	O
modules	O
to	O
directly	O
generate	O
system	O
responses	O
with	O
or	O
without	O
modeling	O
dialogue	O
acts	O
.	O
Zhao	O
et	O
al	O
(	O
2019	O
)	O
models	O
action	O
space	O
of	O
dialogue	O
agent	B-DatasetName
as	O
latent	O
variables	O
.	O
predicts	O
dialogue	O
acts	O
using	O
a	O
hierarchical	O
graph	O
structure	O
with	O
each	O
path	O
representing	O
a	O
unique	O
act	O
.	O
Pei	O
et	O
al	O
(	O
2019	O
)	O
;	O
Peng	O
et	O
al	O
(	O
2019	O
)	O
use	O
multiple	O
dialogue	O
agents	O
,	O
each	O
trained	O
for	O
a	O
specific	O
dialogue	O
domain	O
,	O
and	O
combine	O
them	O
through	O
a	O
common	O
dialogue	O
agent	B-DatasetName
.	O
Mehri	O
et	O
al	O
(	O
2019	O
)	O
models	O
dialogue	O
policy	O
and	O
NLG	O
separately	O
and	O
fuses	O
feature	O
representations	O
at	O
different	O
levels	O
to	O
generate	O
responses	O
.	O
Our	O
models	O
simultaneously	O
learn	O
dialogue	O
acts	O
as	O
a	O
latent	O
variable	O
while	O
allowing	O
semantic	O
conditioning	O
on	O
distributed	O
representations	O
of	O
dialogue	O
acts	O
rather	O
than	O
hard	O
discrete	O
features	O
.	O
End	O
-	O
to	O
-	O
End	O
Dialogue	O
Systems	O
.	O
In	O
this	O
task	O
,	O
conventional	O
approaches	O
combine	O
Natural	B-TaskName
Language	I-TaskName
Understanding	I-TaskName
(	O
NLU	O
)	O
,	O
DST	O
,	O
Dialogue	O
Policy	O
,	O
and	O
NLG	O
,	O
into	O
a	O
pipeline	O
architecture	O
(	O
Wen	O
et	O
al	O
,	O
2017	O
;	O
Bordes	O
et	O
al	O
,	O
2016	O
;	O
Liu	O
and	O
Lane	O
,	O
2017	O
;	O
Liu	O
and	O
Perez	O
,	O
2017	O
;	O
Williams	O
et	O
al	O
,	O
2017	O
;	O
Zhao	O
et	O
al	O
,	O
2017	O
;	O
Jhunjhunwala	O
et	O
al	O
,	O
2020	O
)	O
.	O
Another	O
framework	O
does	O
not	O
explicitly	O
modularize	O
these	O
components	O
but	O
incorporate	O
them	O
through	O
a	O
sequence	O
-	O
to	O
-	O
sequence	O
framework	O
Lei	O
et	O
al	O
,	O
2018	O
;	O
Yavuz	O
et	O
al	O
,	O
2019	O
)	O
and	O
a	O
memory	O
-	O
based	O
entity	O
dataset	O
of	O
triplets	O
Madotto	O
et	O
al	O
,	O
2018	O
;	O
Gangi	O
Reddy	O
et	O
al	O
,	O
2019	O
;	O
Wu	O
et	O
al	O
,	O
2019b	O
)	O
.	O
These	O
approaches	O
bypass	O
dialogue	O
state	O
and/or	O
act	O
modeling	O
and	O
aim	O
to	O
generate	O
output	O
responses	O
directly	O
.	O
They	O
achieve	O
impressive	O
success	O
in	O
generating	O
dialogue	O
responses	O
in	O
open	O
-	O
domain	O
dialogues	O
with	O
unstructured	O
knowledge	O
bases	O
.	O
However	O
,	O
in	O
a	O
task	O
-	O
oriented	O
setting	O
with	O
an	O
entity	O
dataset	O
,	O
they	O
might	O
suffer	O
from	O
an	O
explosion	O
of	O
memory	O
size	O
when	O
the	O
number	O
of	O
entities	O
from	O
multiple	O
dialogue	O
domains	O
increases	O
.	O
Our	O
work	O
is	O
more	O
related	O
to	O
the	O
traditional	O
pipeline	O
strategy	O
but	O
we	O
integrate	O
our	O
dialogue	O
models	O
by	O
unifying	O
two	O
major	O
components	O
rather	O
than	O
using	O
the	O
traditional	O
four	O
-	O
module	O
architecture	O
,	O
to	O
alleviate	O
error	O
propagation	O
from	O
upstream	O
to	O
downstream	O
components	O
.	O
Different	O
from	O
prior	O
work	O
such	O
as	O
(	O
Shu	O
et	O
al	O
,	O
2019	O
)	O
,	O
our	O
model	O
facilitates	O
multi	O
-	O
domain	O
state	O
tracking	O
and	O
allows	O
learning	O
dialogue	O
acts	O
during	O
response	B-TaskName
generation	I-TaskName
.	O

Slot	O
-	O
level	O
DST	O
.	O
We	O
adopt	O
the	O
Transformer	B-MethodName
attention	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
,	O
which	O
consists	O
of	O
a	O
dot	B-MethodName
-	I-MethodName
product	I-MethodName
attention	I-MethodName
with	O
skip	O
connection	O
,	O
to	O
integrate	O
dialogue	O
contextual	O
information	O
into	O
each	O
slot	O
representation	O
.	O
We	O
denote	O
Att	O
(	O
Z	O
1	O
,	O
Z	O
2	O
)	O
as	O
the	O
attention	O
operation	O
from	O
Z	O
2	O
on	O
Z	O
1	O
.	O
We	O
first	O
enable	O
models	O
to	O
process	O
all	O
slot	O
representations	O
together	O
rather	O
than	O
separately	O
as	O
in	O
previous	O
DST	O
models	O
(	O
Ramadan	O
et	O
al	O
,	O
2018	O
;	O
Wu	O
et	O
al	O
,	O
2019a	O
)	O
.	O
This	O
strategy	O
allows	O
our	O
models	O
to	O
explicitly	O
learn	O
dependencies	O
between	O
all	O
pairs	O
of	O
slots	O
.	O
Many	O
pairs	O
of	O
slots	O
could	O
exhibit	O
correlation	O
such	O
as	O
time	O
-	O
wise	O
relation	O
(	O
"	O
departure_time	O
"	O
and	O
"	O
arrival_time	O
"	O
)	O
.	O
We	O
obtain	O
Z	O
dst	O
SS	O
=	O
Att	O
(	O
Z	O
S	O
,	O
Z	O
S	O
)	O
R	O
S	O
×d	O
.	O
We	O
incorporate	O
the	O
dialogue	O
information	O
by	O
learning	O
dependencies	O
between	O
each	O
slot	O
representation	O
and	O
each	O
token	O
in	O
the	O
dialogue	O
history	O
.	O
Previous	O
approaches	O
such	O
as	O
(	O
Budzianowski	O
and	O
Vulić	O
,	O
2019	O
)	O
consider	O
all	O
dialogue	O
history	O
as	O
a	O
single	O
sequence	O
but	O
we	O
separate	O
them	O
into	O
two	O
inputs	O
because	O
the	O
information	O
in	O
X	O
utt	O
is	O
usually	O
more	O
important	O
to	O
generate	O
responses	O
while	O
X	O
ctx	O
includes	O
more	O
background	O
information	O
.	O
We	O
then	O
obtain	O
Z	O
dst	O
S	O
,	O
ctx	O
=	O
Att	O
(	O
Z	O
ctx	O
,	O
Z	O
dst	O
SS	O
)	O
R	O
S	O
×d	O
and	O
Z	O
dst	O
S	O
,	O
utt	O
=	O
Att	O
(	O
Z	O
utt	O
,	O
Z	O
dst	O
S	O
,	O
ctx	O
)	O
R	O
S	O
×d	O
.	O
Following	O
(	O
Lei	O
et	O
al	O
,	O
2018	O
)	O
,	O
we	O
incorporate	O
dialogue	O
state	O
of	O
the	O
previous	O
turn	O
B	O
t−1	O
which	O
is	O
a	O
more	O
compact	O
representation	O
of	O
dialogue	O
context	O
.	O
Hence	O
,	O
we	O
can	O
replace	O
the	O
full	O
dialogue	O
context	O
to	O
only	O
R	O
t−1	O
as	O
the	O
remaining	O
part	O
is	O
represented	O
in	O
B	O
t−1	O
.	O
This	O
approach	O
avoids	O
taking	O
in	O
all	O
dialogue	O
history	O
and	O
is	O
scalable	O
as	O
the	O
conversation	O
grows	O
longer	O
.	O
We	O
add	O
the	O
attention	O
layer	O
to	O
obtain	O
Z	O
dst	O
S	O
,	O
st	O
=	O
Att	O
(	O
Z	O
prev	O
st	O
,	O
Z	O
dst	O
S	O
,	O
ctx	O
)	O
R	O
S	O
×d	O
(	O
See	O
Figure	O
1	O
)	O
.	O
We	O
further	O
improve	O
the	O
feature	O
representations	O
by	O
repeating	O
the	O
attention	O
sequence	O
over	O
N	O
dst	O
S	O
times	O
.	O
We	O
denote	O
the	O
final	O
output	O
Z	O
dst	O
S	O
.	O
Domain	O
-	O
level	O
DST	O
.	O
We	O
adopt	O
a	O
similar	O
architecture	O
to	O
learn	O
domain	O
-	O
level	O
representations	O
.	O
The	O
representations	O
learned	O
in	O
this	O
module	O
exhibit	O
global	O
information	O
while	O
slot	O
-	O
level	O
representations	O
contain	O
local	O
dependencies	O
to	O
decode	O
multi	O
-	O
domain	O
dialogue	O
states	O
.	O
First	O
,	O
we	O
enable	O
the	O
domain	O
-	O
level	O
DST	O
to	O
capture	O
dependencies	O
between	O
all	O
pairs	O
of	O
domains	O
.	O
For	O
example	O
,	O
some	O
domains	O
such	O
as	O
"	O
taxi	O
"	O
are	O
typically	O
paired	O
with	O
other	O
domains	O
such	O
as	O
"	O
attraction	O
"	O
,	O
but	O
usually	O
not	O
with	O
the	O
"	O
train	O
"	O
domain	O
.	O
We	O
then	O
obtain	O
Z	O
dst	O
DD	O
=	O
Att	O
(	O
Z	O
D	O
,	O
Z	O
D	O
)	O
R	O
D	O
×d	O
.	O
We	O
then	O
allow	O
models	O
to	O
capture	O
dependencies	O
between	O
each	O
domain	O
representation	O
and	O
each	O
token	O
in	O
dialogue	O
context	O
and	O
current	O
user	O
utterance	O
.	O
By	O
segregating	O
dialogue	O
context	O
and	O
current	O
utterance	O
,	O
our	O
models	O
can	O
potentially	O
detect	O
changes	O
of	O
dialogue	O
domains	O
from	O
past	O
turns	O
to	O
the	O
current	O
turn	O
.	O
Especially	O
in	O
multi	O
-	O
domain	O
dialogues	O
,	O
users	O
can	O
switch	O
from	O
one	O
domain	O
to	O
another	O
and	O
the	O
next	O
system	O
response	O
should	O
address	O
the	O
latest	O
domain	O
.	O
We	O
then	O
obtain	O
Z	O
dst	O
D	O
,	O
ctx	O
=	O
Att	O
(	O
Z	O
ctx	O
,	O
Z	O
dst	O
DD	O
)	O
R	O
D	O
×d	O
and	O
Z	O
dst	O
D	O
,	O
utt	O
=	O
Att	O
(	O
Z	O
utt	O
,	O
Z	O
dst	O
D	O
,	O
ctx	O
)	O
R	O
D	O
×d	O
sequentially	O
.	O
Similar	O
to	O
the	O
slot	O
-	O
level	O
module	O
,	O
we	O
refine	O
feature	O
representations	O
over	O
N	O
dst	O
D	O
times	O
and	O
denote	O
the	O
final	O
output	O
as	O
Z	O
dst	O
D	O
.	O
Domain	O
-	O
Slot	O
DST	O
.	O
We	O
combined	O
domain	O
and	O
slot	O
representations	O
by	O
expanding	O
the	O
tensors	O
to	O
identical	O
dimensions	O
i.e.	O
D	O
×	O
S	O
×	O
d.	O
We	O
then	O
apply	O
Hadamard	O
product	O
,	O
resulting	O
in	O
domain	O
-	O
slot	O
joint	O
features	O
Z	O
dst	O
DS	O
R	O
D	O
×	O
S	O
×d	O
.	O
We	O
then	O
apply	O
a	O
self	O
-	O
attention	O
layer	O
to	O
allow	O
learning	O
of	O
dependencies	O
between	O
joint	O
domain	O
-	O
slot	O
features	O
:	O
Z	O
dst	O
=	O
Att	O
(	O
Z	O
dst	O
DS	O
,	O
Z	O
dst	O
DS	O
)	O
R	O
D	O
×	O
S	O
×d	O
.	O
In	O
this	O
attention	O
,	O
we	O
mask	O
the	O
intermediate	O
representations	O
in	O
positions	O
of	O
invalid	O
domain	O
-	O
slot	O
pairs	O
.	O
Compared	O
to	O
previous	O
work	O
such	O
as	O
(	O
Wu	O
et	O
al	O
,	O
2019a	O
)	O
,	O
we	O
adopt	O
a	O
late	O
fusion	O
method	O
whereby	O
domain	O
and	O
slot	O
representations	O
are	O
integrated	O
in	O
deeper	O
layers	O
.	O

We	O
evaluate	O
our	O
models	O
with	O
the	O
multi	O
-	O
domain	O
dialogue	O
corpus	O
MultiWOZ	B-DatasetName
2.0	I-DatasetName
and	O
2.1	O
(	O
Eric	O
et	O
al	O
,	O
2019	O
)	O
(	O
The	O
latter	O
includes	O
corrected	O
state	O
labels	O
for	O
the	O
DST	O
task	O
)	O
.	O
From	O
the	O
dialogue	O
state	O
annotation	O
of	O
the	O
training	O
data	O
,	O
we	O
identified	O
all	O
possible	O
domains	O
and	O
slots	O
.	O
We	O
identified	O
D	O
=	O
7	O
domains	O
and	O
S	O
=	O
30	O
slots	O
,	O
including	O
19	O
inform	O
slots	O
and	O
11	O
request	O
slots	O
.	O
We	O
also	O
identified	O
A	O
=	O
32	O
acts	O
.	O
The	O
corpus	O
includes	O
8	O
,	O
438	O
dialogues	O
in	O
the	O
training	O
set	O
and	O
1	O
,	O
000	O
in	O
each	O
validation	O
and	O
test	O
set	O
.	O
We	O
present	O
a	O
summary	O
of	O
the	O
dataset	O
in	O
Table	O
2	O
.	O
For	O
additional	O
information	O
of	O
data	O
pre	O
-	O
processing	O
procedures	O
,	O
domains	O
,	O
slots	O
,	O
and	O
entity	O
DBs	O
,	O
please	O
refer	O
to	O
Appendix	O
A.	O

We	O
select	O
d	O
=	O
256	O
,	O
h	O
att	O
=	O
8	O
,	O
N	O
dst	O
S	O
=	O
N	O
dst	O
D	O
=	O
N	O
gen	O
=	O
3	O
.	O
We	O
employed	O
dropout	O
(	O
Srivastava	O
et	O
al	O
,	O
2014	O
)	O
of	O
0.3	O
and	O
label	B-MethodName
smoothing	I-MethodName
(	O
Szegedy	O
et	O
al	O
,	O
2016	O
)	O
on	O
target	O
system	O
responses	O
during	O
training	O
.	O

We	O
A	O
Data	O
Pre	O
-	O
processing	O
First	O
,	O
we	O
delexicalize	O
each	O
target	O
system	O
response	O
sequence	O
by	O
replacing	O
the	O
matched	O
entity	O
attribute	O
that	O
appears	O
in	O
the	O
sequence	O
to	O
the	O
canonical	O
tag	O
domain_slot	O
.	O
For	O
example	O
,	O
the	O
original	O
target	O
response	O
'	O
the	O
train	O
i	O
d	O
is	O
tr8259	O
departing	O
from	O
cambridge	O
'	O
is	O
delexicalized	O
into	O
'	O
the	O
train	O
i	O
d	O
is	O
train_id	O
departing	O
from	O
train_departure	O
'	O
.	O
We	O
use	O
the	O
provided	O
entity	O
databases	O
(	O
DBs	O
)	O
to	O
match	O
potential	O
attributes	O
in	O
all	O
target	O
system	O
responses	O
.	O
To	O
construct	O
dialogue	O
history	O
,	O
we	O
keep	O
the	O
original	O
version	O
of	O
all	O
text	O
,	O
including	O
system	O
responses	O
of	O
previous	O
turns	O
,	O
rather	O
than	O
the	O
delexicalized	O
form	O
.	O
We	O
split	O
all	O
sequences	O
of	O
dialogue	O
history	O
,	O
user	O
utterances	O
of	O
the	O
current	O
turn	O
,	O
dialogue	O
states	O
,	O
and	O
delexicalized	O
target	O
responses	O
,	O
into	O
case	O
-	O
insensitive	O
tokens	O
.	O
We	O
share	O
the	O
embedding	O
weights	O
of	O
all	O
source	O
sequences	O
,	O
including	O
dialogue	O
history	O
,	O
user	O
utterance	O
,	O
and	O
dialogue	O
states	O
,	O
but	O
use	O
a	O
separate	O
embedding	O
matrix	O
to	O
encode	O
the	O
target	O
system	O
responses	O
.	O
We	O
summarize	O
the	O
number	O
of	O
dialogues	O
in	O
each	O
domain	O
in	O
Table	O
2	O
.	O
For	O
each	O
domain	O
,	O
a	O
dialogue	O
is	O
selected	O
as	O
long	O
as	O
the	O
whole	O
dialogue	O
(	O
i.e.	O
singledomain	O
dialogue	O
)	O
or	O
parts	O
of	O
the	O
dialogue	O
(	O
i.e.	O
in	O
multi	O
-	O
domain	O
dialogue	O
)	O
is	O
involved	O
with	O
the	O
domain	O
.	O
For	O
each	O
domain	O
,	O
we	O
also	O
build	O
a	O
set	O
of	O
possible	O
inform	O
and	O
request	O
slots	O
using	O
the	O
dialogue	O
state	O
annotation	O
in	O
the	O
training	O
data	O
.	O
The	O
details	O
of	O
slots	O
and	O
database	O
in	O
each	O
domain	O
can	O
be	O
seen	O
in	O
Table	O
9	O
.	O
The	O
DBs	O
of	O
3	O
domains	O
taxi	O
,	O
police	O
,	O
and	O
hospital	O
are	O
not	O
available	O
as	O
part	O
of	O
the	B-DatasetName
benchmark	I-DatasetName
.	O
On	O
average	O
,	O
each	O
dialogue	O
has	O
1.8	O
domains	O
and	O
extends	O
over	O
13	O
turns	O
.	O

We	O
describe	O
our	O
baseline	O
models	O
in	O
DST	O
,	O
contextto	O
-	O
text	B-TaskName
generation	I-TaskName
,	O
and	O
end	O
-	O
to	O
-	O
end	O
dialogue	O
tasks	O
.	O

FJST	O
and	O
HJST	O
(	O
Eric	O
et	O
al	O
,	O
2019	O
)	O
.	O
These	O
models	O
adopt	O
a	O
fixed	O
-	O
vocabulary	O
DST	O
approach	O
.	O
Both	O
models	O
include	O
encoder	O
modules	O
(	O
either	O
bidirectional	B-MethodName
LSTM	I-MethodName
or	O
hierarchical	O
LSTM	B-MethodName
)	O
to	O
encode	O
the	O
dialogue	O
history	O
.	O
The	O
models	O
pass	O
the	O
context	O
hidden	O
states	O
to	O
separate	O
linear	O
transformation	O
to	O
obtain	O
final	O
vectors	O
to	O
predict	O
individual	O
slots	O
separately	O
.	O
The	O
output	O
vector	O
is	O
used	O
to	O
measure	O
a	O
score	O
of	O
each	O
candidate	O
from	O
a	O
predefined	O
candidate	O
set	O
.	O
DST	O
Reader	O
.	O
This	O
model	O
considers	O
the	O
DST	O
task	O
as	O
a	O
reading	B-TaskName
comprehension	I-TaskName
task	O
and	O
predicts	O
each	O
slot	O
as	O
a	O
span	O
over	O
tokens	O
within	O
dialogue	O
history	O
.	O
DST	O
Reader	O
utilizes	O
attentionbased	O
neural	O
networks	O
with	O
additional	O
modules	O
to	O
predict	O
slot	O
type	O
and	O
carryover	O
probability	O
.	O
TSCP	O
(	O
Lei	O
et	O
al	O
,	O
2018	O
)	O
.	O
The	O
model	O
adopts	O
a	O
sequence	O
-	O
to	O
-	O
sequence	O
framework	O
with	O
a	O
pointer	B-MethodName
network	I-MethodName
to	O
generate	O
dialogue	O
states	O
.	O
The	O
source	O
sequence	O
is	O
a	O
combination	O
of	O
the	O
last	O
user	O
utterance	O
,	O
dialogue	O
state	O
of	O
the	O
previous	O
turn	O
,	O
and	O
user	O
utterance	O
.	O
To	O
compare	O
with	O
TSCP	O
in	O
a	O
multi	O
-	O
domain	O
task	O
-	O
oriented	O
dialogue	O
setting	O
,	O
we	O
adapt	O
the	O
model	O
to	O
multi	O
-	O
domain	O
dialogues	O
by	O
formulating	O
the	O
dialogue	O
state	O
of	O
the	O
previous	O
turn	O
similarly	O
as	O
our	O
models	O
.	O
We	O
reported	O
the	O
performance	O
when	O
the	O
maximum	O
length	O
of	O
the	O
output	O
dialogue	O
state	O
sequence	O
L	O
is	O
set	O
to	O
20	O
tokens	O
(	O
original	O
default	O
parameter	O
is	O
8	O
tokens	O
but	O
we	O
expect	O
longer	O
dialogue	O
state	O
in	O
MultiWOZ	B-DatasetName
benchmark	O
and	O
selected	O
20	O
tokens	O
)	O
.	O
HyST	O
.	O
This	O
model	O
combines	O
the	O
advantage	O
of	O
fixed	O
-	O
vocabulary	O
and	O
openvocabulary	O
approaches	O
.	O
The	O
model	O
uses	O
an	O
openvocabulary	O
approach	O
in	O
which	O
the	O
set	O
of	O
candidates	O
of	O
each	O
slot	O
is	O
constructed	O
based	O
on	O
all	O
word	O
ngrams	O
in	O
the	O
dialogue	O
history	O
.	O
Both	O
approaches	O
are	O
applied	O
in	O
all	O
slots	O
and	O
depending	O
on	O
their	O
performance	O
in	O
the	O
validation	O
set	O
,	O
the	O
better	O
approach	O
is	O
used	O
to	O
predict	O
individual	O
slots	O
during	O
test	O
time	O
.	O
TRADE	O
(	O
Wu	O
et	O
al	O
,	O
2019a	O
)	O
.	O
The	O
model	O
adopts	O
a	O
sequence	O
-	O
to	O
-	O
sequence	O
framework	O
with	O
a	O
pointer	B-MethodName
network	I-MethodName
to	O
generate	O
individual	O
slot	O
token	O
-	O
by	O
-	O
token	O
.	O
The	O
prediction	O
is	O
additionally	O
supported	O
by	O
a	O
slot	O
gating	O
component	O
that	O
decides	O
whether	O
the	O
slot	O
is	O
"	O
none	O
"	O
,	O
"	O
dontcare	O
"	O
,	O
or	O
"	O
generate	O
"	O
.	O
When	O
the	O
gate	O
of	O
a	O
slot	O
is	O
predicted	O
as	O
"	O
generate	O
"	O
,	O
the	O
model	O
will	O
generate	O
value	O
as	O
a	O
natural	O
output	O
sequence	O
for	O
that	O
slot	O
.	O
NADST	O
(	O
Le	O
et	O
al	O
,	O
2020	O
)	O
.	O
The	O
model	O
proposes	O
a	O
non	O
-	O
autoregressive	O
approach	O
for	O
dialogue	B-TaskName
state	I-TaskName
tracking	I-TaskName
which	O
enables	O
learning	O
dependencies	O
between	O
domain	O
-	O
level	O
and	O
slot	O
-	O
level	O
representations	O
as	O
well	O
as	O
token	O
-	O
level	O
representations	O
of	O
slot	O
values	O
.	O
DSTQA	O
(	O
Zhou	O
and	O
Small	O
,	O
2019	O
)	O
.	O
The	O
model	O
treats	O
dialogue	B-TaskName
state	I-TaskName
tracking	I-TaskName
as	O
a	O
question	B-TaskName
answering	I-TaskName
problem	O
in	O
which	O
state	O
values	O
can	O
be	O
predicted	O
through	O
lexical	O
spans	O
or	O
unique	O
generated	O
values	O
.	O
It	O
is	O
enhanced	O
with	O
a	O
knowledge	O
graph	O
where	O
each	O
node	O
represent	O
a	O
slot	O
and	O
edges	O
are	O
based	O
on	O
overlaps	O
of	O
their	O
value	O
sets	O
.	O
SOM	B-MethodName
-	O
DST	O
(	O
Kim	O
et	O
al	O
,	O
2020	O
)	O
.	O
This	O
is	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
model	O
on	O
the	O
MultiWOZ2.1	O
dataset	O
.	O
The	O
model	O
exploits	O
a	O
selectively	O
overwriting	O
mechanism	O
on	O
a	O
fixed	O
-	O
sized	O
memory	O
of	O
dialogue	O
states	O
.	O
i	O
d	O
,	O
address	O
,	O
area	O
,	O
internet	O
,	O
parking	O
,	O
single	O
,	O
double	O
,	O
family	O
,	O
name	O
,	O
phone	O
,	O
postcode	O
,	O
pricerange	O
'	O
,	O
takesbookings	O
,	O
stars	O
,	O
type	O
At	O
each	O
dialogue	O
turn	O
,	O
the	O
mechanism	O
involve	O
decision	B-TaskName
making	I-TaskName
on	O
whether	O
to	O
update	O
or	O
carryover	O
the	O
state	O
values	O
from	O
previous	O
turns	O
.	O

Figure	O
1	O
:	O
Confounding	O
bias	O
from	O
the	O
data	O
generation	O
mechanism	O
.	O
u	O
refers	O
to	O
the	O
unobserved	O
mechanism	O
(	O
i.e.	O
,	O
plaintiffs	O
sue	O
when	O
they	O
have	O
a	O
high	O
probability	O
to	O
be	O
supported	O
)	O
that	O
causes	O
the	O
judgment	O
in	O
dataset	O
D	O
(	O
J	O
)	O
to	O
be	O
imbalanced	O
.	O
D	O
(	O
J	O
)	O
I	O
denotes	O
that	O
the	O
imbalanced	O
data	O
D	O
(	O
J	O
)	O
has	O
a	O
causal	O
effect	O
on	O
the	O
representation	O
of	O
input	O
I	O
(	O
i.e.	O
,	O
plaintiff	O
's	O
claim	O
and	O
fact	O
description	O
)	O
,	O
and	O
D	O
(	O
J	O
)	O
V	O
denotes	O
that	O
D	O
(	O
J	O
)	O
has	O
a	O
causal	O
effect	O
on	O
the	O
representation	O
of	O
court	O
's	O
view	O
V	O
.	O
Such	O
imbalance	O
in	O
D	O
(	O
J	O
)	O
leads	O
to	O
the	O
confounding	O
bias	O
that	O
the	O
representations	O
of	O
I	O
and	O
V	O
tend	O
to	O
be	O
supportive	O
,	O
and	O
blind	O
the	O
conventional	O
training	O
on	O
P	O
(	O
V	O
|	O
I	O
)	O
.	O
2019	O
)	O
.	O
Court	O
's	O
view	O
can	O
be	O
regarded	O
as	O
the	O
interpretation	O
for	O
the	O
sentence	O
of	O
a	O
case	O
.	O
Being	O
an	O
important	O
portion	O
of	O
verdict	O
,	O
court	O
's	O
view	O
is	O
difficult	O
to	O
generate	O
due	O
to	O
its	O
logic	O
reasoning	O
in	O
the	O
content	O
.	O
Therefore	O
court	O
's	O
view	O
generation	O
is	O
regarded	O
as	O
one	O
of	O
the	O
most	O
critical	O
functions	O
in	O
a	O
legal	O
assistant	O
system	O
.	O
Court	O
's	O
view	O
consists	O
of	O
two	O
main	O
parts	O
,	O
including	O
the	O
judgment	O
and	O
the	O
rationales	O
,	O
where	O
the	O
judgment	O
is	O
a	O
response	O
to	O
the	O
plaintiff	O
's	O
claims	O
in	O
civil	O
cases	O
or	O
charges	O
in	O
criminal	O
cases	O
,	O
and	O
the	O
rationales	O
are	O
summarized	O
from	O
the	O
fact	O
description	O
to	O
derive	O
and	O
explain	O
the	O
judgment	O
.	O
Recently	O
,	O
Ye	O
et	O
al	O
(	O
2018	O
)	O
investigated	O
the	O
problem	O
of	O
court	O
's	O
view	O
generation	O
for	O
the	O
criminal	O
cases	O
,	O
but	O
it	O
focused	O
on	O
the	O
generation	O
of	O
rationales	O
in	O
the	O
court	O
's	O
view	O
based	O
on	O
the	O
given	O
criminal	O
charge	O
and	O
fact	O
description	O
of	O
a	O
case	O
.	O
Such	O
an	O
experimental	O
scenario	O
is	O
not	O
applicable	O
to	O
the	O
practical	O
situation	O
since	O
the	O
rationales	O
should	O
be	O
concluded	O
before	O
reaching	O
the	O
final	O
judgment	O
.	O
Moreover	O
,	O
dif	O
-	O
Figure	O
2	O
:	O
An	O
example	O
of	O
plaintiff	O
's	O
claim	O
,	O
fact	O
description	O
,	O
and	O
court	O
's	O
view	O
from	O
a	O
legal	O
document	O
in	O
a	O
civil	O
case	O
.	O
The	O
judgment	O
is	O
non	O
-	O
support	O
since	O
there	O
exists	O
a	O
rejection	O
on	O
one	O
of	O
the	O
plaintiff	O
's	O
claims	O
in	O
the	O
court	O
's	O
view	O
.	O
ferent	O
from	O
the	O
criminal	O
cases	O
,	O
in	O
civil	O
cases	O
,	O
the	O
judgment	O
depends	O
not	O
only	O
on	O
the	O
facts	O
recognized	O
but	O
also	O
on	O
the	O
claims	O
that	O
the	O
plaintiff	O
declared	O
.	O
In	O
this	O
paper	O
,	O
we	O
focus	O
on	O
the	O
problem	O
of	O
automatically	O
generating	O
the	O
court	O
's	O
view	O
in	O
civil	O
cases	O
by	O
injecting	O
the	O
plaintiff	O
's	O
claim	O
and	O
fact	O
description	O
,	O
as	O
shown	O
in	O
Fig	O
.	O
2	O
.	O
In	O
such	O
a	O
context	O
,	O
the	O
problem	O
of	O
the	O
court	O
's	O
view	O
generation	O
can	O
be	O
formulated	O
as	O
a	O
text	O
-	O
to	O
-	O
text	O
natural	O
language	O
generation	O
(	O
NLG	O
)	O
problem	O
,	O
where	O
the	O
input	O
is	O
the	O
plaintiff	O
's	O
claim	O
and	O
the	O
fact	O
description	O
,	O
and	O
the	O
output	O
is	O
the	O
corresponding	O
court	O
's	O
view	O
that	O
contains	O
the	O
judgment	O
and	O
the	O
rationales	O
1	O
.	O
Although	O
classical	O
text	O
generative	O
models	O
(	O
e.g.	O
,	O
sequence	O
-	O
tosequence	O
model	O
Sutskever	O
et	O
al	O
,	O
2014	O
,	O
attentionbased	O
model	O
,	O
andpointer	O
-	O
generator	O
networks	O
See	O
et	O
al	O
,	O
2017	O
)	O
have	O
been	O
applied	O
to	O
many	O
text	B-TaskName
generation	I-TaskName
tasks	O
,	O
yet	O
,	O
in	O
the	O
task	O
of	O
the	O
court	O
's	O
view	O
generation	O
,	O
such	O
techniques	O
can	O
not	O
be	O
simply	O
applied	O
for	O
the	O
following	O
reasons	O
:	O
(	O
1	O
)	O
There	O
exists	O
"	O
no	O
claim	O
,	O
no	O
trial	O
"	O
principle	O
in	O
civil	O
legal	O
systems	O
:	O
The	O
judgment	O
in	O
the	O
real	O
court	O
's	O
view	O
is	O
the	O
response	O
to	O
the	O
claims	O
declared	O
by	O
the	O
plaintiff	O
,	O
where	O
its	O
rationales	O
summarize	O
the	O
corresponding	O
facts	O
.	O
In	O
other	O
words	O
,	O
there	O
exists	O
a	O
correspondence	O
relationship	O
between	O
the	O
input	O
(	O
claims	O
and	O
facts	O
)	O
and	O
the	O
generated	O
text	O
(	O
court	O
's	O
view	O
)	O
.	O
For	O
example	O
,	O
the	O
plaintiff	O
's	O
claims	O
shown	O
in	O
Fig	O
.	O
2	O
mentioned	O
the	O
principal	O
and	O
the	O
interest	O
,	O
respectively	O
.	O
Hence	O
,	O
the	O
court	O
's	O
view	O
of	O
this	O
case	O
would	O
and	O
might	O
only	O
focus	O
on	O
the	O
facts	O
about	O
the	O
principal	O
and	O
the	O
interest	O
.	O
(	O
2	O
)	O
The	O
imbalance	O
of	O
judgment	O
in	O
civil	O
cases	O
:	O
The	O
distribution	O
of	O
judgment	O
results	O
of	O
civil	O
cases	O
is	O
very	O
imbalanced	O
.	O
For	O
example	O
,	O
over	O
76	O
%	O
of	O
cases	O
were	O
supported	O
in	O
private	O
lending	O
,	O
which	O
is	O
the	O
most	O
frequent	O
category	O
in	O
civil	O
cases	O
.	O
Such	O
an	O
imbalance	O
of	O
judgment	O
would	O
blind	O
the	O
training	O
of	O
the	O
model	O
by	O
focusing	O
on	O
the	O
supported	O
cases	O
while	O
ignoring	O
the	O
non	O
-	O
supported	O
cases	O
,	O
leading	O
to	O
incorrect	O
judgment	O
generation	O
of	O
court	O
's	O
view	O
.	O
From	O
the	O
perspective	O
of	O
causality	O
(	O
Pearl	O
,	O
2009	O
;	O
Kuang	O
et	O
al	O
,	O
2020	O
)	O
,	O
the	O
imbalance	O
of	O
judgment	O
reveals	O
the	O
confounding	O
bias	O
induced	O
by	O
the	O
data	O
generation	O
mechanism	O
that	O
plaintiffs	O
sue	O
when	O
they	O
have	O
a	O
high	O
probability	O
to	O
be	O
supported	O
.	O
Such	O
imbalanced	O
data	O
would	O
cause	O
the	O
learned	O
representation	O
of	O
both	O
inputs	O
(	O
claims	O
and	O
recognized	O
facts	O
)	O
and	O
output	O
(	O
court	O
's	O
view	O
)	O
to	O
be	O
supported	O
,	O
leading	O
to	O
confounding	O
bias	O
between	O
inputs	O
and	O
output	O
,	O
and	O
blinding	O
the	O
training	O
process	O
of	O
conventional	O
NLG	O
models	O
as	O
we	O
demonstrated	O
in	O
Fig	O
.	O
1	O
.	O
To	O
address	O
these	O
challenges	O
,	O
we	O
propose	O
an	O
Attentional	O
and	O
Counterfactual	O
based	O
Natural	O
Language	O
Generation	O
(	O
AC	O
-	O
NLG	O
)	O
method	O
by	O
jointly	O
optimizing	O
a	O
claim	O
-	O
aware	O
encoder	O
,	O
a	O
pair	O
of	O
counterfactual	O
decoders	O
to	O
generate	O
judgmentdiscriminative	O
court	O
's	O
views	O
(	O
both	O
supportive	O
and	O
non	O
-	O
supportive	O
views	O
)	O
and	O
a	O
synergistic	O
judgment	O
predictive	O
model	O
.	O
Specifically	O
,	O
the	O
claim	O
-	O
aware	O
encoder	O
is	O
designed	O
to	O
represent	O
the	O
fact	O
description	O
which	O
emphasizes	O
on	O
the	O
declared	O
claims	O
.	O
The	O
counterfactual	O
decoder	O
is	O
inspired	O
by	O
the	O
backdoor	O
adjustment	O
in	O
causal	B-MethodName
inference	I-MethodName
(	O
Pearl	O
et	O
al	O
,	O
2016	O
;	O
Kuang	O
et	O
al	O
,	O
2020	O
)	O
to	O
address	O
the	O
confounding	O
bias	O
and	O
the	O
imbalance	O
problem	O
in	O
judgment	O
.	O
To	O
determine	O
the	O
judgment	O
result	O
of	O
each	O
case	O
,	O
a	O
judgment	O
predictive	O
model	O
is	O
jointly	O
learned	O
with	O
the	O
two	O
decoders	O
and	O
decides	O
which	O
output	O
to	O
be	O
selected	O
as	O
the	O
final	O
generated	O
court	O
's	O
view	O
.	O
We	O
validate	O
the	O
effectiveness	O
of	O
our	O
AC	O
-	O
NLG	O
method	O
with	O
extensive	O
experiments	O
on	O
real	O
legal	O
documents	O
.	O
Comprehensive	O
experiments	O
show	O
the	O
effectiveness	O
of	O
our	O
method	O
under	O
both	O
quantitative	O
and	O
qualitative	O
evaluation	O
metrics	O
.	O
Since	O
legal	O
AI	O
is	O
a	O
sensitive	O
field	O
,	O
we	O
make	O
ethical	O
discussion	O
in	O
the	O
penultimate	O
section	O
(	O
Sec	O
.	O
6	O
)	O
.	O
The	O
main	O
contributions	O
of	O
this	O
paper	O
can	O
be	O
sum	O
-	O
marized	O
as	O
follows	O
:	O
We	O
investigate	O
the	O
problem	O
of	O
de	O
-	O
biased	O
court	O
's	O
view	O
generation	O
in	O
civil	O
cases	O
from	O
a	O
causal	O
perspective	O
,	O
considering	O
the	O
issue	O
of	O
confounding	O
bias	O
from	O
judgment	O
imbalance	O
.	O
We	O
propose	O
a	O
novel	O
AC	O
-	O
NLG	O
model	O
to	O
jointly	O
optimize	O
a	O
claim	O
-	O
aware	O
encoder	O
and	O
a	O
pair	O
of	O
counterfactual	O
decoders	O
for	O
generating	O
a	O
judgment	O
-	O
discriminative	O
court	O
's	O
view	O
by	O
incorporating	O
with	O
a	O
judgment	O
predictive	O
model	O
.	O
We	O
construct	O
a	O
dataset	O
based	O
on	O
raw	O
civil	O
legal	O
documents	O
,	O
where	O
each	O
case	O
is	O
objectively	O
split	O
into	O
three	O
parts	O
:	O
plaintiff	O
's	O
claim	O
,	O
fact	O
description	O
,	O
and	O
court	O
's	O
view	O
with	O
human	O
annotation	O
on	O
the	O
judgment	O
.	O
To	O
motivate	O
other	O
scholars	O
to	O
investigate	O
this	O
novel	O
but	O
important	O
problem	O
,	O
we	O
make	O
the	O
experiment	O
dataset	O
publicly	O
available	O
2	O
.	O
We	O
validate	O
the	O
superior	O
performance	O
of	O
the	O
proposed	O
method	O
with	O
extensive	O
experiments	O
.	O
Our	O
method	O
can	O
be	O
applied	O
to	O
other	O
natural	O
language	O
generation	O
tasks	O
with	O
confounding	O
bias	O
or	O
data	O
imbalance	O
.	O
2	O
Related	O
Work	O

In	O
recent	O
years	O
,	O
many	O
researchers	O
from	O
both	O
law	O
and	O
computer	O
science	O
fields	O
have	O
been	O
exploring	O
the	O
potential	O
and	O
methods	O
to	O
perform	O
judicial	O
decisions	O
and	O
auxiliary	O
tasks	O
,	O
aiming	O
at	O
helping	O
lawyers	O
and	O
lower	O
court	O
judges	O
.	O
In	O
recent	O
work	O
,	O
judicial	O
intelligence	O
is	O
also	O
applied	O
to	O
various	O
tasks	O
of	O
natural	O
language	O
processing	O
.	O
Since	O
most	O
of	O
the	O
legal	O
documents	O
appear	O
in	O
textual	O
form	O
,	O
many	O
NLP	O
technologies	O
have	O
been	O
brought	O
into	O
the	O
legal	O
field	O
to	O
improve	O
the	O
efficiency	O
of	O
legal	O
work	O
.	O
Charge	O
prediction	O
is	O
a	O
common	O
task	O
of	O
judgment	O
prediction	O
,	O
considered	O
as	O
a	O
text	B-TaskName
classification	I-TaskName
problem	O
(	O
Lin	O
et	O
al	O
,	O
2012	O
;	O
Zhong	O
et	O
al	O
,	O
2018	O
;	O
Hu	O
et	O
al	O
,	O
2018	O
;	O
Jiang	O
et	O
al	O
,	O
2018	O
;	O
Chalkidis	O
et	O
al	O
,	O
2019	O
)	O
.	O
Besides	O
,	O
there	O
are	O
also	O
works	O
on	O
legal	O
questions	O
classification	O
(	O
Xiao	O
et	O
al	O
,	O
2017	O
)	O
,	O
law	O
articles	O
recommendation	O
(	O
Chen	O
et	O
al	O
,	O
2019	O
)	O
,	O
controversy	O
focus	O
mining	O
(	O
Duan	O
et	O
al	O
,	O
2019	O
)	O
and	O
relevant	O
case	O
retrieval	O
(	O
Chen	O
et	O
al	O
,	O
2013	O
)	O
.	O
Ye	O
et	O
al	O
(	O
2018	O
)	O
explored	O
the	O
court	O
's	O
view	O
generation	O
in	O
criminal	O
cases	O
,	O
where	O
the	O
input	O
is	O
only	O
fact	O
description	O
,	O
and	O
the	O
court	O
's	O
view	O
generation	O
is	O
conditioned	O
on	O
the	O
known	O
judgment	O
results	O
,	O
which	O
is	O
not	O
applicable	O
in	O
real	O
cases	O
.	O
2	O
https://github.com/wuyiquan/AC	O
-	O
NLG	O

Our	O
task	O
aims	O
at	O
generating	O
the	O
court	O
's	O
view	O
based	O
on	O
the	O
plaintiff	O
's	O
claim	O
and	O
the	O
fact	O
description	O
,	O
which	O
can	O
be	O
regarded	O
as	O
a	O
NLG	O
task	O
.	O
NLG	O
has	O
been	O
widely	O
studied	O
and	O
applied	O
to	O
many	O
tasks	O
,	O
such	O
as	O
machine	B-TaskName
translation	I-TaskName
(	O
Wu	O
et	O
al	O
,	O
2016	O
)	O
,	O
question	B-TaskName
answering	I-TaskName
(	O
McCann	O
et	O
al	O
,	O
2018	O
;	O
Bagchi	O
and	O
Wynter	O
,	O
2013	O
)	O
and	O
text	B-TaskName
summarization	I-TaskName
(	O
Rush	O
et	O
al	O
,	O
2015	O
)	O
.	O
The	O
recent	O
success	O
of	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
(	O
Sutskever	O
et	O
al	O
,	O
2014	O
)	O
,	O
in	O
which	O
recurrent	O
neural	O
networks	O
(	O
RNNs	O
)	O
reading	O
and	O
generating	O
text	O
simultaneously	O
,	O
has	O
made	O
the	O
generation	O
task	O
feasible	O
.	O
Bahdanau	O
et	O
al	O
(	O
2014	O
)	O

Causal	B-MethodName
Inference	I-MethodName
(	O
Pearl	O
,	O
2009	O
;	O
Kuang	O
et	O
al	O
,	O
2020	O
)	O
is	O
a	O
powerful	O
statistical	O
modeling	O
tool	O
for	O
explanatory	O
analysis	O
by	O
removing	O
confounding	O
bias	O
in	O
data	O
.	O
That	O
bias	O
might	O
bring	O
a	O
spurious	O
correlation	O
or	O
confounding	O
effect	O
among	O
variables	O
.	O
Recently	O
,	O
many	O
methods	O
have	O
been	O
proposed	O
to	O
remove	O
confounding	O
bias	O
in	O
the	O
literature	O
of	O
causal	B-MethodName
inference	I-MethodName
,	O
including	O
do	O
-	O
operation	O
based	O
on	O
structure	O
causal	O
model	O
(	O
Pearl	O
,	O
2009	O
)	O
and	O
counterfactual	O
outcome	O
prediction	O
based	O
on	O
potential	O
outcome	O
framework	O
(	O
Imbens	O
and	O
Rubin	O
,	O
2015	O
)	O
.	O
With	O
dooperation	O
,	O
the	O
backdoor	O
adjustment	O
(	O
Pearl	O
et	O
al	O
,	O
2016	O
)	O
have	O
been	O
proposed	O
for	O
data	O
de	O
-	O
bias	O
.	O
In	O
this	O
paper	O
,	O
we	O
sketch	O
the	O
causal	O
structure	O
model	O
of	O
our	O
problem	O
,	O
as	O
shown	O
in	O
Fig	O
.	O
1	O
,	O
and	O
adopt	O
backdoor	O
for	O
confounding	O
bias	O
reduction	O
.	O

In	O
this	O
work	O
,	O
we	O
focus	O
on	O
the	O
problem	O
of	O
the	O
court	O
's	O
view	O
generation	O
in	O
civil	O
cases	O
,	O
where	O
the	O
input	O
is	O
the	O
plaintiff	O
's	O
claim	O
and	O
the	O
fact	O
description	O
,	O
and	O
the	O
output	O
is	O
the	O
corresponding	O
court	O
's	O
view	O
.	O
We	O
formulate	O
our	O
problem	O
with	O
the	O
definition	O
of	O
the	O
plaintiff	O
's	O
claim	O
,	O
the	O
fact	O
description	O
,	O
and	O
the	O
court	O
's	O
view	O
,	O
as	O
shown	O
in	O
Fig	O
.	O
2	O
.	O
Plaintiff	O
's	O
claim	O
(	O
C	O
)	O
is	O
a	O
descriptive	O
sentence	O
that	O
depicts	O
the	O
claims	O
from	O
the	O
plaintiff	O
.	O
In	O
a	O
civil	O
case	O
,	O
it	O
often	O
appears	O
multiple	O
claims	O
from	O
the	O
plaintiff	O
.	O
For	O
example	O
,	O
the	O
plaintiff	O
's	O
claim	O
demonstrated	O
in	O
Fig	O
.	O
2	O
contains	O
the	O
principal	O
claim	O
and	O
the	O
interest	O
claim	O
.	O
Here	O
,	O
we	O
denote	O
the	O
plaintiff	O
's	O
claim	O
in	O
a	O
case	O
as	O
a	O
sentence	O
form	O
c	O
=	O
{	O
w	O
c	O
t	O
}	O
m	O
t=1	O
,	O
where	O
w	O
c	O
t	O
represents	O
one	O
word	O
,	O
and	O
m	O
is	O
the	O
number	O
of	O
words	O
in	O
plaintiff	O
's	O
claim	O
.	O
Fact	O
description	O
(	O
F	O
)	O
is	O
also	O
a	O
descriptive	O
sentence	O
,	O
which	O
describes	O
the	O
identified	O
facts	O
(	O
relevant	O
events	O
that	O
have	O
happened	O
)	O
in	O
a	O
case	O
,	O
as	O
Fig	O
.	O
2	O
shows	O
.	O
Here	O
,	O
we	O
denote	O
the	O
fact	O
description	O
in	O
a	O
case	O
as	O
f	O
=	O
{	O
w	O
f	O
t	O
}	O
n	O
t=1	O
,	O
where	O
n	O
is	O
the	O
length	O
.	O
Court	O
's	O
view	O
(	O
V	O
)	O
contains	O
two	O
main	O
components	O
,	O
judgment	O
and	O
rationales	O
,	O
where	O
the	O
judgment	O
is	O
to	O
respond	O
the	O
plaintiff	O
's	O
claims	O
,	O
and	O
the	O
rationales	O
are	O
the	O
claim	O
-	O
related	O
summarization	B-TaskName
on	O
the	O
fact	O
description	O
to	O
determine	O
and	O
interpret	O
the	O
judgment	O
.	O
Here	O
,	O
we	O
denote	O
the	O
court	O
's	O
view	O
as	O
v	O
=	O
{	O
w	O
v	O
t	O
}	O
l	O
t=1	O
,	O
where	O
l	O
is	O
the	O
length	O
.	O
Moreover	O
,	O
we	O
use	O
a	O
variable	O
j	O
to	O
denote	O
the	O
judgment	O
in	O
the	O
court	O
's	O
view	O
.	O
For	O
simplicity	O
,	O
we	O
set	O
j	O
=	O
1	O
to	O
denote	O
supported	O
judgment	O
(	O
all	O
the	O
claims	O
are	O
judged	O
to	O
be	O
accepted	O
)	O
,	O
and	O
j	O
=	O
0	B-DatasetName
to	O
denote	O
non	O
-	O
supported	O
judgment	O
.	O
Then	O
,	O
the	O
problem	O
of	O
court	O
's	O
view	O
generation	O
can	O
be	O
denoted	O
as	O
follow	O
:	O
Problem	O
1	O
(	O
Court	O
's	O
View	O
Generation	O
)	O
.	O
Given	O
the	O
plaintiff	O
's	O
claim	O
c	O
=	O
{	O
w	O
c	O
t	O
}	O
m	O
t=1	O
and	O
the	O
fact	O
de	O
-	O
scription	O
f	O
=	O
{	O
w	O
f	O
t	O
}	O
n	O
t=1	O
,	O
our	O
task	O
is	O
to	O
generate	O
the	O
court	O
's	O
view	O
v	O
=	O
{	O
w	O
v	O
t	O
}	O
l	O
t=1	O
.	O

As	O
shown	O
in	O
Fig	O
.	O
1	O
,	O
the	O
confounding	O
bias	O
from	O
the	O
data	O
generation	O
mechanism	O
would	O
blind	O
the	O
conventional	O
training	O
on	O
P	O
(	O
V	O
|	O
I	O
)	O
,	O
and	O
current	O
sequenceto	O
-	O
sequence	O
models	O
struggle	O
to	O
solve	O
this	O
problem	O
.	O
Here	O
,	O
we	O
see	O
through	O
why	O
these	O
models	O
fail	O
mathematically	O
.	O
For	O
a	O
certain	O
case	O
,	O
given	O
the	O
input	O
I	O
=	O
(	O
c	O
,	O
f	O
)	O
,	O
using	O
Bayes	O
rule	O
,	O
we	O
would	O
train	O
the	O
model	O
to	O
generate	O
the	O
court	O
's	O
view	O
V	O
as	O
follow	O
:	O
P	O
(	O
V	O
|	O
I	O
)	O
=	O
j	O
P	O
(	O
V	O
|	O
I	O
,	O
j	O
)	O
P	O
(	O
j	O
|	O
I	O
)	O
(	O
1	O
)	O
If	O
the	O
supported	O
cases	O
dominate	O
our	O
training	O
data	O
,	O
e.g.	O
,	O
P	O
(	O
j	O
=	O
1	O
|	O
I	O
)	O
≈	O
1	O
.	O
Thus	O
,	O
P	O
(	O
V	O
|	O
I	O
)	O
degrades	O
to	O
P	O
(	O
V	O
|	O
I	O
,	O
j	O
=	O
1	O
)	O
,	O
which	O
would	O
ignore	O
the	O
representation	O
of	O
non	O
-	O
supported	O
cases	O
,	O
leading	O
to	O
the	O
learned	O
representations	O
of	O
inputs	O
I	O
and	O
output	O
V	O
tend	O
to	O
be	O
supported	O
.	O
Thus	O
,	O
the	O
model	O
tends	O
to	O
build	O
a	O
strong	O
connection	O
between	O
inputs	O
and	O
the	O
supported	O
court	O
's	O
view	O
,	O
even	O
for	O
the	O
cases	O
that	O
are	O
non	O
-	O
supported	O
.	O
In	O
this	O
way	O
,	O
the	O
representation	O
of	O
input	O
I	O
is	O
contaminated	O
by	O
the	O
confounding	O
bias	O
from	O
I	O
D	O
(	O
J	O
)	O
V	O
.	O
Backdoor	O
adjustment	O
is	O
a	O
main	O
de	O
-	O
confounding	O
technique	O
in	O
causal	B-MethodName
inference	I-MethodName
(	O
Pearl	O
et	O
al	O
,	O
2016	O
;	O
Pearl	O
,	O
2009	O
)	O
.	O
De	O
-	O
confounding	O
seeks	O
the	O
exact	O
causal	O
effect	O
of	O
one	O
variable	O
on	O
another	O
,	O
which	O
appeals	O
for	O
our	O
court	O
's	O
view	O
generation	O
task	O
since	O
the	O
court	O
's	O
view	O
should	O
be	O
faithful	O
only	O
to	O
the	O
content	O
of	O
the	O
plaintiff	O
's	O
claims	O
and	O
fact	O
descriptions	O
.	O
The	O
backdoor	O
adjustment	O
makes	O
a	O
do	O
-	O
operation	O
on	O
I	O
,	O
which	O
promotes	O
the	O
posterior	O
probability	O
from	O
passive	O
observation	O
to	O
active	O
intervention	O
.	O
The	O
backdoor	O
adjustment	O
addresses	O
the	O
confounding	O
bias	O
by	O
computing	O
the	O
interventional	O
posterior	O
P	O
(	O
V	O
|	O
do	O
(	O
I	O
)	O
)	O
and	O
controlling	O
the	O
confounder	O
as	O
:	O
P	O
(	O
V	O
|	O
do	O
(	O
I	O
)	O
)	O
=	O
j	O
P	O
(	O
V	O
|	O
I	O
,	O
j	O
)	O
P	O
(	O
j	O
)	O
(	O
2	O
)	O
In	O
our	O
problem	O
,	O
the	O
variable	O
j	O
is	O
a	O
binary	O
variable	O
(	O
support	O
or	O
non	O
-	O
support	O
)	O
,	O
hence	O
,	O
P	O
(	O
V	O
|	O
do	O
(	O
I	O
)	O
)	O
=	O
P	O
(	O
V	O
|	O
I	O
,	O
j	O
=	O
0	B-DatasetName
)	O
P	O
(	O
j	O
=	O
0	B-DatasetName
)	O
+	O
P	O
(	O
V	O
|	O
I	O
,	O
j	O
=	O
1	O
)	O
P	O
(	O
j	O
=	O
1	O
)	O
The	O
main	O
difference	O
between	O
traditional	O
posterior	O
in	O
Eq	O
.	O
1	O
and	O
interventional	O
posterior	O
in	O
Eq	O
.	O
2	O
is	O
that	O
P	O
(	O
j	O
|	O
I	O
)	O
is	O
changed	O
to	O
P	O
(	O
j	O
)	O
.	O
Since	O
the	O
backdooor	O
adjustment	O
help	O
to	O
cut	O
the	O
dependence	O
between	O
D	O
(	O
J	O
)	O
and	O
I	O
,	O
we	O
can	O
eliminate	O
the	O
confounding	O
bias	O
from	O
data	O
generation	O
mechanism	O
and	O
learn	O
a	O
interventional	O
model	O
for	O
de	O
-	O
biased	O
court	O
's	O
view	O
generation	O
.	O

We	O
implement	O
the	O
following	O
baselines	O
for	O
comparison	O
:	O
S2S	O
Sequence	O
-	O
to	O
-	O
sequence	O
model	O
(	O
Sutskever	O
et	O
al	O
,	O
2014	O
)	O
is	O
a	O
classic	O
model	O
for	O
NLG	O
task	O
.	O
We	O
concatenate	O
the	O
plaintiff	O
claims	O
and	O
facts	O
descriptions	O
as	O
input	O
.	O
PGN	O
Pointer	O
Generator	O
Networks	O
(	O
See	O
et	O
al	O
,	O
2017	O
)	O
utilizes	O
a	O
pointer	B-MethodName
network	I-MethodName
to	O
solve	O
the	O
outof	O
-	O
vocabulary	O
(	O
OOV	O
)	O
problem	O
,	O
which	O
is	O
essential	O
for	O
the	O
court	O
's	O
view	O
generation	O
since	O
many	O
nouns	O
occur	O
there	O
.	O
Oversampling	O
is	O
a	O
common	O
method	O
to	O
alleviate	O
data	O
imbalance	O
.	O
We	O
oversample	O
the	O
non	O
-	O
supported	O
cases	O
so	O
that	O
the	O
ratio	O
between	O
supported	O
cases	O
and	O
non	O
-	O
supported	O
cases	O
become	O
1	O
:	O
1	O
.	O
S2SwS	O
Apply	O
oversampling	O
to	O
S2S.	O
PGNwS	O
Apply	O
oversampling	O
to	O
PGN	O
.	O
AC	O
-	O
NLGwS	O
Apply	O
oversampling	O
to	O
AC	O
-	O
NLG	O
.	O
We	O
do	O
ablation	O
experiments	O
as	O
follows	O
:	O
AC	O
-	O
NLGw	O
/	O
oD	O
We	O
remove	O
the	O
decoder	O
and	O
train	O
the	O
remaining	O
model	O
(	O
encoder	O
and	O
predictor	O
)	O
as	O
a	O
classification	O
task	O
for	O
judgment	O
prediction	O
.	O
AC	O
-	O
NLGw	O
/	O
oBA	O
We	O
remove	O
the	O
backdoor	O
adjustment	O
by	O
replacing	O
the	O
pair	O
of	O
counterfactual	O
decoders	O
and	O
predictor	O
with	O
a	O
single	O
decoder	O
,	O
but	O
keep	O
the	O
claim	O
-	O
aware	O
attention	O
mechanism	O
.	O
AC	O
-	O
NLGw	O
/	O
oCA	O
We	O
remove	O
the	O
claim	O
-	O
aware	O
attention	O
,	O
and	O
concatenate	O
the	O
claims	O
and	O
the	O
facts	O
instead	O
.	O

We	O
use	O
Gensim	O
(	O
Řehůřek	O
and	O
Sojka	O
,	O
2010	O
)	O
with	O
a	O
large	O
-	O
scale	O
generic	O
corpus	O
to	O
train	O
a	O
language	O
model	O
as	O
the	O
pre	O
-	O
trained	O
model	O
,	O
then	O
use	O
it	O
to	O
initialize	O
the	O
word	B-TaskName
embeddings	I-TaskName
,	O
which	O
is	O
in	O
the	O
dimension	O
of	O
300	O
.	O
8	O

Alexander	O
M	O
Rush	O
,	O
Sumit	O
Chopra	O
,	O
and	O
Jason	O
Weston	O
.	O
2015	O
.	O
A	O
neural	O
attention	O
model	O
for	O
abstractive	O
sentence	B-TaskName
summarization	I-TaskName
.	O
arXiv	B-DatasetName
preprint	O
arXiv:1509.00685	O
.	O
Abigail	O
See	O
,	O
Peter	O
J	O
Liu	O
,	O
and	O
Christopher	O
D	O
Manning	O
.	O
2017	O
.	O
Get	O
to	O
the	O
point	O
:	O
Summarization	B-TaskName
with	O
pointer	O
-	O
generator	O
networks	O
.	O
arXiv	B-DatasetName
preprint	O
arXiv:1704.04368	O
.	O

The	O
court	O
concluded	O
that	O
:	O
The	O
loan	O
relationship	O
between	O
the	O
plaintiff	O
A	O
and	O
the	O
defendant	O
B	O
is	O
legal	O
and	O
effective	O
.	O
After	O
the	O
defendant	O
borrowed	O
money	O
,	O
he	O
should	O
take	O
the	O
responsibility	O
to	O
return	O
the	O
loan	O
and	O
pay	O
legal	O
interest	O
.	O
If	O
the	O
borrower	O
and	O
the	O
lender	O
have	O
not	O
agreed	O
on	O
the	O
interest	O
on	O
the	O
loan	O
,	O
it	O
shall	O
be	O
deemed	O
as	O
non	O
-	O
payment	O
of	O
interest	O
Acceptance	O
.	O
However	O
,	O
the	O
plaintiff	O
has	O
the	O
right	O
to	O
claim	O
the	O
interest	O
to	O
be	O
calculated	O
from	O
the	B-DatasetName
benchmark	I-DatasetName
interest	O
rate	O
of	O
the	O
same	O
grade	O
of	O
loans	O
issued	O
by	O
the	O
People	O
's	O
Bank	O
of	O
China	O
at	O
the	O
same	O
period	O
when	O
the	O
loan	O
occurred	O
since	O
the	O
date	O
of	O
the	O
prosecution	O
Rejection	O
.	O

Opinion	O
target	O
extraction	O
and	O
opinion	O
term	B-TaskName
extraction	I-TaskName
are	O
two	O
fundamental	O
tasks	O
in	O
Aspect	O
Based	O
Sentiment	B-TaskName
Analysis	I-TaskName
(	O
ABSA	O
)	O
.	O
Many	O
recent	O
works	O
on	O
ABSA	O
focus	O
on	O
Targetoriented	O
Opinion	O
Words	O
(	O
or	O
Terms	O
)	O
Extraction	O
(	O
TOWE	O
)	O
,	O
which	O
aims	O
at	O
extracting	O
the	O
corresponding	O
opinion	O
words	O
for	O
a	O
given	O
opinion	O
target	O
.	O
TOWE	O
can	O
be	O
further	O
applied	O
to	O
Aspect	O
-	O
Opinion	O
Pair	O
Extraction	O
(	O
AOPE	O
)	O
which	O
aims	O
at	O
extracting	O
aspects	O
(	O
i.e.	O
,	O
opinion	O
targets	O
)	O
and	O
opinion	O
terms	O
in	O
pairs	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
Target	O
-	O
Specified	O
sequence	O
labeling	O
with	O
Multi	O
-	O
head	O
Self	O
-	O
Attention	O
(	O
TSMSA	O
)	O
for	O
TOWE	O
,	O
in	O
which	O
any	O
pre	O
-	O
trained	O
language	O
model	O
with	O
multi	O
-	O
head	O
self	O
-	O
attention	O
can	O
be	O
integrated	O
conveniently	O
.	O
As	O
a	O
case	O
study	O
,	O
we	O
also	O
develop	O
a	O
Multi	O
-	O
Task	O
structure	O
named	O
MT	O
-	O
TSMSA	O
for	O
AOPE	O
by	O
combining	O
our	O
TSMSA	O
with	O
an	O
aspect	O
and	O
opinion	O
term	B-TaskName
extraction	I-TaskName
module	O
.	O
Experimental	O
results	O
indicate	O
that	O
TSMSA	O
outperforms	O
the	B-DatasetName
benchmark	I-DatasetName
methods	O
on	O
TOWE	O
significantly	O
;	O
meanwhile	O
,	O
the	O
performance	O
of	O
MT	O
-	O
TSMSA	O
is	O
similar	O
or	O
even	O
better	O
than	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
AOPE	O
baseline	O
models	O
.	O

Aspect	B-TaskName
-	I-TaskName
Based	I-TaskName
Sentiment	I-TaskName
Analysis	I-TaskName
(	O
ABSA	O
)	O
(	O
Pontiki	O
et	O
al	O
,	O
2014	O
)	O
has	O
attracted	O
much	O
attention	O
of	O
researchers	O
in	O
recent	O
years	O
.	O
In	O
ABSA	O
,	O
aspect	O
(	O
or	O
called	O
opinion	O
target	O
)	O
extraction	O
and	O
opinion	O
term	B-TaskName
extraction	I-TaskName
are	O
two	O
fundamental	O
tasks	O
.	O
Aspect	O
is	O
the	O
word	O
or	O
phrase	O
in	O
the	O
reviews	O
referring	O
to	O
the	O
object	O
towards	O
which	O
users	O
show	O
attitudes	O
,	O
while	O
opinion	O
terms	O
are	O
those	O
words	O
or	O
phrases	O
representing	O
users	O
'	O
attitudes	O
(	O
Wu	O
et	O
al	O
,	O
2020	O
)	O
.	O
For	O
example	O
,	O
in	O
the	O
sentence	O
"	O
The	O
dim	O
sum	O
is	O
delicious	O
.	O
"	O
,	O
the	O
phrase	O
"	O
dim	O
sum	O
"	O
is	O
an	O
aspect	O
and	O
the	O
word	O
"	O
delicious	O
"	O
is	O
an	O
opinion	O
term	O
.	O
See	O
the	O
upper	O
part	O
of	O
Table	O
1	O
for	O
more	O
examples	O
.	O
Plenty	O
of	O
works	O
based	O
on	O
neural	O
networks	O
have	O
been	O
done	O
in	O
both	O
aspect	O
*	O
The	O
corresponding	O
author	O
.	O

"	O
Soooo	O
great	O
!	O
The	O
food	O
is	O
delicious	O
and	O
inexpensive	O
,	O
and	O
the	O
environment	O
is	O
in	O
a	O
nice	O
.	O
The	O
only	O
problem	O
is	O
that	O
the	O
soup	O
and	O
dessert	O
are	O
ordinary	O
.	O
"	O
Aspect	O
-	O
Opinion	O
Pairs	O
:	O
food	O
:	O
[	O
delicious	O
,	O
inexpensive	O
]	O
(	O
one	O
-	O
to	O
-	O
many	O
)	O
environment	O
:	O
[	O
nice	O
]	O
(	O
one	O
-	O
to	O
-	O
one	O
)	O
soup	O
,	O
dessert	O
:	O
[	O
ordinary	O
]	O
(	O
many	O
-	O
to	O
-	O
one	O
)	O
Table	O
1	O
:	O
The	O
upper	O
part	O
is	O
a	O
restaurant	O
review	O
and	O
the	O
lower	O
part	O
shows	O
the	O
corresponding	O
aspect	O
-	O
opinion	O
pairs	O
.	O
Extracted	O
aspects	O
and	O
opinion	O
terms	O
are	O
marked	O
in	O
red	O
and	O
blue	O
,	O
respectively	O
.	O
and	O
opinion	O
term	B-TaskName
extraction	I-TaskName
(	O
Liu	O
et	O
al	O
,	O
2015	O
;	O
Poria	O
et	O
al	O
,	O
2016	O
;	O
Xu	O
et	O
al	O
,	O
2018	O
)	O
;	O
moreover	O
,	O
some	O
studies	O
combine	O
these	O
two	O
tasks	O
into	O
a	O
multi	O
-	O
task	O
structure	O
to	O
extract	O
aspects	O
and	O
opinion	O
terms	O
simultaneously	O
(	O
Wang	O
et	O
al	O
,	O
2016	O
(	O
Wang	O
et	O
al	O
,	O
,	O
2017Li	O
and	O
Lam	O
,	O
2017	O
;	O
Dai	O
and	O
Song	O
,	O
2019	O
)	O
.	O
However	O
,	O
one	O
critical	O
deficiency	O
in	O
the	O
researches	O
mentioned	O
above	O
is	O
that	O
they	O
ignore	O
the	O
relation	O
of	O
aspects	O
and	O
opinion	O
terms	O
,	O
which	O
leads	O
to	O
the	O
birth	O
of	O
Target	O
-	O
oriented	O
Opinion	O
Words	O
(	O
or	O
Terms	O
)	O
Extraction	O
(	O
TOWE	O
)	O
(	O
Fan	O
et	O
al	O
,	O
2019	O
)	O
for	O
extracting	O
the	O
corresponding	O
opinion	O
terms	O
of	O
a	O
given	O
opinion	O
target	O
.	O
Subsequently	O
,	O
Aspect	O
-	O
Opinion	O
Pair	O
Extraction	O
(	O
AOPE	O
)	O
(	O
Chen	O
et	O
al	O
,	O
2020	O
)	O
and	O
Pair	O
-	O
wise	O
Aspect	O
and	O
Opinion	O
Terms	O
Extraction	O
(	O
PAOTE	O
)	O
have	O
emerged	O
,	O
which	O
both	O
aim	O
at	O
extracting	O
aspects	O
and	O
opinion	O
terms	O
in	O
pairs	O
.	O
AOPE	O
and	O
PAOTE	O
are	O
exactly	O
the	O
same	O
task	O
,	O
only	O
named	O
differently	O
.	O
In	O
the	O
following	O
,	O
we	O
use	O
AOPE	O
to	O
denote	O
this	O
task	O
for	O
simplicity	O
.	O
It	O
can	O
be	O
considered	O
that	O
AOPE	O
contains	O
aspect	O
and	O
opinion	O
word	O
extraction	O
and	O
TOWE	O
.	O
Since	O
aspect	B-TaskName
extraction	I-TaskName
has	O
been	O
fully	O
studied	O
and	O
satisfactory	O
results	O
have	O
been	O
obtained	O
,	O
TOWE	O
,	O
which	O
aims	O
at	O
mining	O
the	O
relation	O
between	O
aspects	O
and	O
opinion	O
terms	O
,	O
is	O
the	O
key	O
to	O
the	O
AOPE	O
task	O
.	O
As	O
shown	O
in	O
the	O
lower	O
part	O
of	O
Table	O
1	O
,	O
the	O
relational	O
structure	O
of	O
the	O
aspect	O
-	O
opinion	O
pairs	O
within	O
a	O
sentence	O
can	O
be	O
complicated	O
,	O
including	O
one	O
-	O
to	O
-	O
one	O
,	O
one	O
-	O
to	O
-	O
many	O
,	O
and	O
many	O
-	O
to	O
-	O
one	O
.	O
The	O
challenge	O
of	O
TOWE	O
is	O
the	O
learning	O
of	O
representations	O
of	O
the	O
given	O
opinion	O
target	O
accurately	O
and	O
a	O
few	O
works	O
focus	O
on	O
this	O
task	O
.	O
For	O
instance	O
,	O
Fan	O
et	O
al	O
(	O
2019	O
)	O
propose	O
an	O
Inward	O
-	O
Outward	O
LSTM	B-MethodName
to	O
pass	O
target	O
information	O
to	O
the	O
left	O
context	O
and	O
the	O
right	O
context	O
of	O
the	O
target	O
respectively	O
,	O
and	O
then	O
they	O
combine	O
the	O
left	O
,	O
right	O
,	O
and	O
global	O
context	O
to	O
encode	O
the	O
sentence	O
.	O
Recently	O
,	O
SDRN	O
(	O
Chen	O
et	O
al	O
,	O
2020	O
)	O
and	O
SpanMlt	O
both	O
adopt	O
a	O
pre	O
-	O
trained	O
language	O
model	O
to	O
learn	O
contextual	O
representations	O
for	O
AOPE	O
.	O
In	O
SDRN	O
,	O
a	O
double	O
-	O
channel	O
recurrent	O
network	O
and	O
a	O
synchronization	O
unit	O
are	O
applied	O
to	O
extract	O
aspects	O
,	O
opinion	O
terms	O
and	O
their	O
relevancy	O
.	O
In	O
SpanMlt	O
,	O
the	O
terms	O
are	O
extracted	O
under	O
annotated	O
span	O
boundaries	O
with	O
contextual	O
representations	O
,	O
and	O
then	O
the	O
relations	O
between	O
every	O
two	O
span	O
combinations	O
are	O
identified	O
.	O
However	O
,	O
apart	O
from	O
hyper	O
-	O
parameters	O
in	O
the	O
pre	O
-	O
trained	O
language	O
model	O
,	O
these	O
two	O
methods	O
introduce	O
many	O
other	O
hyper	O
-	O
parameters	O
(	O
e.g.	O
,	O
the	O
hidden	O
size	O
,	O
thresholds	O
and	O
recurrent	O
steps	O
in	O
SDRN	O
,	O
and	O
the	O
span	O
length	O
,	O
top	O
k	O
spans	O
and	O
the	O
balanced	O
factor	O
of	O
different	O
tasks	O
in	O
SpanMlt	O
)	O
.	O
Some	O
of	O
these	O
hyper	O
-	O
parameters	O
have	O
a	O
significant	O
impact	O
on	O
the	O
model	O
performance	O
.	O
Motivated	O
by	O
the	O
previous	O
work	O
and	O
to	O
address	O
the	O
challenges	O
mentioned	O
above	O
,	O
we	O
propose	O
a	O
Target	O
-	O
Specified	O
sequence	O
labeling	O
method	O
based	O
on	O
Multi	O
-	O
head	O
Self	O
-	O
Attention	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
(	O
TSMSA	O
)	O
.	O
The	O
sentence	O
is	O
first	O
processed	O
in	O
the	O
format	O
"	O
[	O
SEP	O
]	O
Aspect	O
[	O
SEP	O
]	O
"	O
(	O
e.g.	O
,	O
"	O
The	O
[	O
SEP	O
]	O
food	O
[	O
SEP	O
]	O
is	O
delicious	O
.	O
"	O
)	O
,	O
which	O
is	O
inspired	O
by	O
Soares	O
et	O
al	O
(	O
2019	O
)	O
who	O
utilized	O
a	O
special	O
symbol	O
"	O
[	O
SEP	O
]	O
"	O
to	O
label	O
all	O
entities	O
and	O
output	O
their	O
corresponding	O
representations	O
.	O
Then	O
we	O
develop	O
a	O
sequence	O
labeling	O
model	O
based	O
on	O
multi	O
-	O
head	O
self	O
-	O
attention	O
to	O
identify	O
the	O
corresponding	O
opinion	O
terms	O
.	O
By	O
using	O
the	O
special	O
symbol	O
and	O
self	O
-	O
attention	O
mechanism	O
,	O
TSMSA	O
is	O
capable	O
of	O
capturing	O
the	O
information	O
of	O
the	O
specific	O
aspect	O
.	O
To	O
improve	O
the	O
performance	O
of	O
our	O
model	O
,	O
we	O
apply	O
pre	O
-	O
trained	O
language	O
models	O
like	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
which	O
contain	O
a	O
multi	O
-	O
head	O
self	O
-	O
attention	O
module	O
as	O
the	O
encoder	O
.	O
As	O
a	O
case	O
study	O
,	O
we	O
integrate	O
aspect	O
and	O
opinion	O
term	B-TaskName
extraction	I-TaskName
,	O
and	O
TOWE	O
into	O
a	O
Multi	O
-	O
Task	O
architecture	O
named	O
MT	O
-	O
TSMSA	O
to	O
validate	O
the	O
effectiveness	O
of	O
our	O
method	O
on	O
the	O
AOPE	O
task	O
.	O
In	O
addition	O
,	O
apart	O
from	O
hyper	O
-	O
parameters	O
in	O
the	O
pre	O
-	O
trained	O
language	O
model	O
,	O
we	O
only	O
need	O
to	O
adjust	O
the	O
balanced	O
factor	O
of	O
different	O
tasks	O
in	O
MT	O
-	O
TSMSA	O
.	O
In	O
summary	O
,	O
our	O
main	O
contributions	O
are	O
as	O
follows	O
:	O
We	O
propose	O
a	O
target	O
-	O
specified	O
sequence	O
labeling	O
method	O
with	O
multi	O
-	O
head	O
self	O
-	O
attention	O
mechanism	O
to	O
perform	O
TOWE	O
,	O
which	O
generates	O
target	O
-	O
specific	O
context	O
representations	O
for	O
different	O
targets	O
in	O
the	O
same	O
review	O
with	O
the	O
special	O
symbol	O
and	O
multi	O
-	O
head	O
self	O
-	O
attention	O
.	O
Pre	O
-	O
trained	O
language	O
models	O
can	O
be	O
conveniently	O
applied	O
to	O
improve	O
the	O
performance	O
.	O
For	O
our	O
TSMSA	O
and	O
MT	O
-	O
TSMSA	O
,	O
only	O
a	O
small	O
amount	O
of	O
hyper	O
-	O
parameters	O
need	O
to	O
be	O
adjusted	O
when	O
using	O
pre	O
-	O
trained	O
language	O
models	O
.	O
Compared	O
to	O
the	O
existing	O
models	O
for	O
TOWE	O
and	O
AOPE	O
,	O
we	O
alleviate	O
the	O
tradeoff	O
issue	O
between	O
a	O
model	O
's	O
complexity	O
and	O
performance	O
.	O
Extensive	O
experiments	O
validate	O
that	O
our	O
TSMSA	O
can	O
achieve	O
the	O
best	O
performance	O
on	O
TOWE	O
,	O
and	O
MT	O
-	O
TSMSA	O
performs	O
quite	O
competitive	O
on	O
AOPE	O
.	O
The	O
rest	O
of	O
this	O
paper	O
is	O
organized	O
as	O
follows	O
.	O
Section	O
2	O
introduces	O
the	O
existing	O
studies	O
on	O
TOWE	O
and	O
AOPE	O
,	O
respectively	O
.	O
Section	O
3	O
details	O
the	O
proposed	O
TSMSA	O
and	O
MT	O
-	O
TSMSA	O
.	O
Section	O
4	O
presents	O
our	O
experimental	O
results	O
and	O
discussions	O
.	O
Finally	O
,	O
we	O
draw	O
conclusions	O
in	O
Section	O
5	O
.	O

Plenty	O
of	O
works	O
have	O
been	O
carried	O
out	O
for	O
aspect	B-TaskName
extraction	I-TaskName
and	O
opinion	O
term	B-TaskName
extraction	I-TaskName
.	O
Early	O
researches	O
can	O
be	O
divided	O
into	O
unsupervised	O
/	O
semisupervised	O
methods	O
(	O
Hu	O
and	O
Liu	O
,	O
2004	O
;	O
Zhuang	O
et	O
al	O
,	O
2006	O
;	O
Qiu	O
et	O
al	O
,	O
2011	O
)	O
and	O
supervised	O
methods	O
(	O
Jakob	O
and	O
Gurevych	O
,	O
2010	O
;	O
Shu	O
et	O
al	O
,	O
2017	O
)	O
.	O
With	O
the	O
development	O
of	O
neural	O
networks	O
,	O
deep	O
learning	O
methods	O
(	O
Liu	O
et	O
al	O
,	O
2015	O
;	O
Yin	O
et	O
al	O
,	O
2016	O
;	O
Poria	O
et	O
al	O
,	O
2016	O
;	O
Xu	O
et	O
al	O
,	O
2018	O
)	O
have	O
made	O
impressive	O
progress	O
in	O
recent	O
years	O
.	O
Several	O
works	O
integrate	O
aspect	B-TaskName
extraction	I-TaskName
and	O
opinion	O
term	B-TaskName
extraction	I-TaskName
into	O
a	O
co	O
-	O
extraction	O
process	O
.	O
Qiu	O
et	O
al	O
(	O
2011	O
)	O
expand	O
the	O
list	O
of	O
aspects	O
and	O
opinion	O
terms	O
in	O
a	O
bootstrapping	O
method	O
by	O
double	O
propagation	O
.	O
Some	O
other	O
works	O
adopt	O
the	O
co	O
-	O
extraction	O
structure	O
in	O
neural	O
networks	O
with	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
(	O
Wang	O
et	O
al	O
,	O
2016	O
(	O
Wang	O
et	O
al	O
,	O
,	O
2017Li	O
and	O
Lam	O
,	O
2017	O
)	O
.	O
However	O
,	O
the	O
above	O
methods	O
ignore	O
the	O
relation	O
between	O
aspects	O
and	O
opinion	O
terms	O
and	O
only	O
a	O
few	O
works	O
focus	O
on	O
this	O
field	O
.	O
Rule	O
-	O
based	O
methods	O
(	O
Hu	O
and	O
Liu	O
,	O
2004	O
;	O
Zhuang	O
et	O
al	O
,	O
2006	O
)	O
are	O
proposed	O
to	O
select	O
corresponding	O
opinion	O
terms	O
with	O
distance	O
rule	O
and	O
syntactic	O
rule	O
templates	O
based	O
on	O
dependency	B-TaskName
parsing	I-TaskName
trees	O
.	O
However	O
,	O
the	O
performance	O
of	O
these	O
methods	O
heavily	O
relies	O
on	O
expert	O
knowledge	O
and	O
these	O
rules	O
usually	O
cover	O
only	O
a	O
small	O
amount	O
of	O
cases	O
.	O
Fan	O
et	O
al	O
(	O
2019	O
)	O
carry	O
out	O
TOWE	O
by	O
extracting	O
the	O
corresponding	O
opinion	O
terms	O
for	O
a	O
given	O
aspect	O
,	O
and	O
then	O
utilize	O
Inward	O
-	O
Outward	O
LSTM	B-MethodName
to	O
generate	O
implicit	O
representations	O
of	O
aspects	O
.	O
Nevertheless	O
,	O
this	O
approach	O
is	O
not	O
capable	O
of	O
applying	O
powerful	O
pre	O
-	O
trained	O
language	O
models	O
like	O
BERT	B-MethodName
as	O
the	O
encoder	O
to	O
perform	O
better	O
.	O
Our	O
model	O
aims	O
to	O
extract	O
corresponding	O
opinion	O
terms	O
of	O
the	O
given	O
aspect	O
with	O
explicit	O
representations	O
,	O
in	O
addition	O
to	O
boost	O
performance	O
by	O
employing	O
BERT	B-MethodName
as	O
the	O
encoder	O
.	O

Aspect	O
-	O
Opinion	O
Pair	O
Extraction	O
(	O
AOPE	O
)	O
(	O
Chen	O
et	O
al	O
,	O
2020	O
)	O
and	O
Pair	O
-	O
wise	O
Aspect	O
and	O
Opinion	O
Terms	O
Extraction	O
(	O
PAOTE	O
)	O
)	O
both	O
aim	O
at	O
extracting	O
aspects	O
and	O
opinion	O
terms	O
in	O
pairs	O
.	O
AOPE	O
and	O
PAOTE	O
are	O
essentially	O
the	O
same	O
task	O
with	O
different	O
names	O
,	O
and	O
they	O
can	O
be	O
split	O
into	O
aspect	B-TaskName
extraction	I-TaskName
and	O
TOWE	O
.	O
Chen	O
et	O
al	O
(	O
2020	O
)	O
propose	O
a	O
Synchronous	O
Double	O
-	O
channel	O
Recurrent	O
Network	O
(	O
SDRN	O
)	O
which	O
consists	O
of	O
an	O
opinion	O
entity	O
extraction	O
unit	O
,	O
a	O
relation	O
detection	O
unit	O
,	O
and	O
a	O
synchronization	O
unit	O
for	O
pair	O
extraction	O
.	O
develop	O
a	O
span	O
-	O
based	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
framework	O
(	O
SpanMlt	O
)	O
where	O
the	O
terms	O
are	O
extracted	O
under	O
annotated	O
span	O
boundaries	O
,	O
so	O
as	O
to	O
identify	O
the	O
relations	O
between	O
every	O
two	O
span	O
combinations	O
.	O
However	O
,	O
SDRN	O
contains	O
a	O
lot	O
of	O
hyperparameters	O
and	O
SpanMlt	O
generates	O
a	O
great	O
many	O
of	O
candidate	O
spans	O
if	O
the	O
value	O
of	O
maximal	O
length	O
of	O
a	O
span	O
is	O
large	O
or	O
the	O
sentence	O
is	O
too	O
long	O
.	O
The	O
advantage	O
of	O
our	O
methods	O
is	O
that	O
only	O
a	O
small	O
amount	O
of	O
hyper	O
-	O
parameters	O
adjustment	O
is	O
required	O
and	O
similar	O
or	O
even	O
better	O
performance	O
can	O
be	O
achieved	O
.	O

The	O
structures	O
of	O
our	O
Target	O
-	O
Specified	O
sequence	O
labeling	O
method	O
based	O
on	O
Multi	O
-	O
head	O
Self	O
-	O
Attention	O
(	O
TSMSA	O
)	O
and	O
the	O
Multi	O
-	O
Task	O
version	O
(	O
MT	O
-	O
TSMSA	O
)	O
are	O
shown	O
in	O
Figure	O
1	O
(	O
c	O
)	O
and	O
(	O
d	O
)	O
.	O
As	O
aforementioned	O
,	O
we	O
first	O
use	O
a	O
special	O
symbol	O
"	O
[	O
SEP	O
]	O
"	O
to	O
label	O
each	O
aspect	O
.	O
Next	O
,	O
the	O
multi	O
-	O
head	O
self	O
-	O
attention	O
method	O
is	O
applied	O
to	O
capture	O
the	O
context	O
representations	O
of	O
the	O
specific	O
aspect	O
explicitly	O
,	O
then	O
they	O
are	O
passed	O
to	O
a	O
projection	O
layer	O
and	O
a	O
Conditional	B-MethodName
Random	I-MethodName
Field	I-MethodName
(	O
CRF	B-MethodName
)	O
(	O
Lafferty	O
et	O
al	O
,	O
2001	O
)	O
layer	O
for	O
sequence	O
labeling	O
.	O
Furthermore	O
,	O
the	O
aspect	O
and	O
opinion	O
words	O
extraction	O
(	O
task	O
0	B-DatasetName
)	O
as	O
well	O
as	O
the	O
target	O
-	O
oriented	O
opinion	O
words	O
extraction	O
(	O
task	O
1	O
)	O
are	O
combined	O
for	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
.	O
These	O
two	O
tasks	O
share	O
the	O
parameters	O
of	O
encoder	O
but	O
differ	O
in	O
projection	O
and	O
CRF	B-MethodName
layers	O
.	O

We	O
describe	O
the	O
multi	O
-	O
head	O
self	O
-	O
attention	O
approach	O
according	O
to	O
Vaswani	O
et	O
al	O
(	O
2017	O
)	O
with	O
the	O
details	O
shown	O
in	O
Figure	O
1	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
.	O
For	O
each	O
attention	O
head	O
in	O
the	O
above	O
approach	O
,	O
we	O
first	O
compute	O
the	O
scaled	B-MethodName
dot	I-MethodName
-	I-MethodName
product	I-MethodName
attention	I-MethodName
.	O
Particularly	O
,	O
the	O
input	O
consists	O
of	O
a	O
set	O
of	O
queries	O
,	O
keys	O
,	O
and	O
values	O
,	O
where	O
d	O
k	O
stands	O
for	O
the	O
dimension	O
of	O
queries	O
and	O
keys	O
,	O
and	O
d	O
v	O
represents	O
the	O
dimension	O
of	O
values	O
.	O
Then	O
they	O
are	O
packed	O
together	O
into	O
matrices	O
Q	O
,	O
K	O
,	O
and	O
V	O
,	O
respectively	O
.	O
The	O
scaled	B-MethodName
dot	I-MethodName
-	I-MethodName
product	I-MethodName
attention	I-MethodName
is	O
calculated	O
as	O
follows	O
:	O
Attention	O
(	O
Q	O
,	O
K	O
,	O
V	O
)	O
=	O
sof	B-DatasetName
tmax	O
(	O
QK	O
T	O
√	O
d	O
k	O
)	O
V.	O
(	O
1	O
)	O
Next	O
,	O
given	O
the	O
number	O
of	O
attention	O
heads	O
h	O
,	O
we	O
can	O
get	O
the	O
dimension	O
of	O
output	O
d	O
model	O
=	O
h	O
×	O
d	O
v	O
.	O
Finally	O
,	O
the	O
multi	B-MethodName
-	I-MethodName
head	I-MethodName
attention	I-MethodName
is	O
described	O
as	O
follows	O
:	O
length	O
.	O
The	O
parameter	O
matrices	O
of	O
projections	O
are	O
M	O
H	O
(	O
I	O
,	O
h	O
)	O
=	O
Concat	O
(	O
head	O
1	O
,	O
...	O
,	O
head	O
h	O
)	O
W	O
O	O
,	O
(	O
2	O
)	O
head	O
i	O
=	O
Attention	O
(	O
IW	O
Q	O
i	O
,	O
IW	O
K	O
i	O
,	O
IW	O
V	O
i	O
)	O
,	O
(	O
3	O
)	O
where	O
I	O
=	O
{	O
i	O
1	O
,	O
i	O
2	O
,	O
...	O
,	O
i	O
n	O
}	O
(	O
the	O
dimension	O
of	O
i	O
is	O
d	O
model	O
)	O
W	O
Q	O
i	O
R	O
d	O
model	O
×d	O
k	O
,	O
W	O
K	O
i	O
R	O
d	O
model	O
×d	O
k	O
,	O
W	O
V	O
i	O
R	O
d	O
model	O
×dv	O
,	O
and	O
W	O
O	O
i	O
R	O
d	O
model	O
×d	O
model	O
.	O

Given	O
a	O
sequential	O
representation	O
H	O
l	O
and	O
a	O
sequential	O
label	O
Y	O
=	O
{	O
y	O
1	O
,	O
...	O
,	O
y	O
n	O
}	O
(	O
y	O
i	O
{	O
B	O
,	O
I	O
,	O
O	O
,	O
[	O
SEP	O
]	O
}	O
or	O
y	O
i	O
{	O
B	O
-	O
ASP	O
,	O
I	O
-	O
ASP	O
,	O
B	O
-	O
OP	O
,	O
I	O
-	O
OP	O
,	O
O	O
}	O
1	O
)	O
,	O
we	O
can	O
use	O
H	O
l	O
to	O
compute	O
p	O
(	O
Y	O
|	O
H	O
l	O
)	O
.	O
Greedy	O
decoding	O
or	O
CRF	B-MethodName
can	O
be	O
adopted	O
in	O
the	O
decoding	O
process	O
.	O
CRF	B-MethodName
is	O
chosen	O
as	O
our	O
decoding	O
strategy	O
because	O
CRF	B-MethodName
has	O
the	O
ability	O
to	O
capture	O
the	O
correlations	O
between	O
tokens	O
and	O
labels	O
and	O
the	O
correlations	O
between	O
adjacent	O
labels	O
simultaneously	O
.	O
Given	O
a	O
new	O
sentence	O
,	O
we	O
use	O
Viterbi	O
algorithm	O
(	O
Viterbi	O
,	O
1967	O
)	O
to	O
predict	O
the	O
label	O
sequence	O
by	O
maximizing	O
the	O
conditional	O
probability	O
p	O
(	O
Y	O
|	O
H	O
l	O
)	O
in	O
the	O
decoding	O
process	O
.	O

For	O
TOWE	O
,	O
a	O
sentence	O
with	O
a	O
given	O
aspect	O
(	O
i.e.	O
,	O
target	O
)	O
is	O
first	O
processed	O
into	O
target	O
-	O
specified	O
mode	O
(	O
"	O
[	O
SEP	O
]	O
Aspect	O
[	O
SEP	O
]	O
"	O
)	O
with	O
the	O
special	O
symbol	O
"	O
[	O
SEP	O
]	O
"	O
and	O
then	O
passed	O
into	O
TSMSA	O
,	O
the	O
outputs	O
of	O
which	O
are	O
the	O
target	O
-	O
oriented	O
opinion	O
terms	O
.	O
For	O
AOPE	O
,	O
MT	O
-	O
TSMSA	O
generates	O
aspect	O
-	O
opinion	O
pairs	O
by	O
a	O
two	O
-	O
stage	O
inference	O
process	O
.	O
Firstly	O
,	O
a	O
sentence	O
is	O
passed	O
into	O
MT	O
-	O
TSMSA	O
,	O
where	O
aspects	O
are	O
extracted	O
in	O
task	O
0	B-DatasetName
.	O
Secondly	O
,	O
given	O
extracted	O
aspects	O
,	O
repeating	O
the	O
inference	O
process	O
of	O
TOWE	O
,	O
MT	O
-	O
TSMSA	O
outputs	O
the	O
target	O
-	O
oriented	O
opinion	O
terms	O
from	O
task	O
1	O
.	O
Accordingly	O
,	O
the	O
combinations	O
of	O
aspects	O
from	O
task	O
0	B-DatasetName
and	O
target	O
-	O
orient	O
opinion	O
terms	O
from	O
task	O
1	O
are	O
aspect	O
-	O
opinion	O
pairs	O
.	O

Fan	O
et	O
al	O
(	O
2019	O
)	O
have	O
employed	O
various	O
baselines	O
in	O
TOWE	O
,	O
including	O
Distance	O
-	O
rule	O
(	O
Hu	O
and	O
Liu	O
,	O
2004	O
)	O
,	O
Dependency	O
-	O
rule	O
(	O
Zhuang	O
et	O
al	O
,	O
2006	O
)	O
,	O
BiLSTM	B-MethodName
+	O
Distance	O
-	O
rule	O
,	O
and	O
TC	O
-	O
BiLSTM	B-MethodName
,	O
except	O
for	O
BERT	B-MethodName
-	O
based	O
methods	O
.	O
To	O
achieve	O
comprehensive	O
comparative	O
analysis	O
,	O
we	O
develop	O
baselines	O
of	O
BERT	B-MethodName
+	O
Distance	O
-	O
rule	O
and	O
Target	O
-	O
fused	O
BERT	B-MethodName
(	O
TF	O
-	O
BERT	B-MethodName
)	O
for	O
this	O
task	O
.	O
The	O
former	O
trains	O
a	O
sentence	O
-	O
level	O
opinion	O
term	B-TaskName
extraction	I-TaskName
model	O
by	O
BERT	B-MethodName
,	O
and	O
the	O
target	O
-	O
oriented	O
opinion	O
term	O
is	O
the	O
one	O
nearest	O
to	O
each	O
aspect	O
.	O
The	O
latter	O
utilizes	O
the	O
average	B-MethodName
pooling	I-MethodName
of	O
target	O
word	B-TaskName
embeddings	I-TaskName
to	O
represent	O
the	O
target	O
information	O
.	O
The	O
word	O
representation	O
at	O
each	O
position	O
is	O
the	O
addition	O
of	O
word	O
embedding	O
and	O
target	O
information	O
,	O
which	O
is	O
fed	O
into	O
BERT	B-MethodName
to	O
extract	O
target	O
-	O
oriented	O
opinion	O
terms	O
.	O
have	O
applied	O
some	O
baselines	O
in	O
AOPE	O
,	O
including	O
HAST	O
(	O
Li	O
et	O
al	O
,	O
2018	O
)	O
+	O
IOG	O
and	O
JERE	O
-	O
MHS	O
(	O
Bekoulis	O
et	O
al	O
,	O
2018	O
)	O
.	O
Besides	O
the	O
above	O
methods	O
,	O
we	O
also	O
employ	O
the	O
following	O
baselines	O
:	O
IOG	O
(	O
Fan	O
et	O
al	O
,	O
2019	O
)	O
utilizes	O
an	O
Inward	O
-	O
Outward	O
LSTM	B-MethodName
and	O
a	O
Global	O
LSTM	B-MethodName
to	O
capture	O
the	O
information	O
of	O
aspects	O
and	O
global	O
information	O
respectively	O
,	O
then	O
it	O
combines	O
these	O
information	O
for	O
sequence	O
labeling	O
.	O
SpanMlt	O
)	O
is	O
a	O
span	O
-	O
based	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
framework	O
where	O
the	O
terms	O
are	O
extracted	O
with	O
annotated	O
span	O
boundaries	O
and	O
then	O
the	O
relations	O
between	O
combinations	O
of	O
every	O
two	O
spans	O
are	O
identified	O
.	O
SDRN	O
(	O
Chen	O
et	O
al	O
,	O
2020	O
)	O
utilizes	O
BERT	B-MethodName
as	O
the	O
encoder	O
which	O
consists	O
of	O
an	O
opinion	O
entity	O
extraction	O
unit	O
,	O
a	O
relation	O
detection	O
unit	O
,	O
and	O
a	O
synchronization	O
unit	O
for	O
the	O
AOPE	O
task	O
.	O
In	O
the	O
case	O
of	O
TOWE	O
,	O
this	O
model	O
extracts	O
the	O
target	O
-	O
oriented	O
opinion	O
terms	O
with	O
given	O
correct	O
aspects	O
.	O

As	O
mentioned	O
above	O
,	O
our	O
method	O
can	O
be	O
applied	O
to	O
AOPE	O
by	O
combining	O
TOWE	O
with	O
aspect	O
and	O
opinion	O
term	B-TaskName
extraction	I-TaskName
.	O
We	O
here	O
compare	O
the	O
performance	O
of	O
our	O
multi	O
-	O
task	O
model	O
(	O
i.e.	O
,	O
MT	O
-	O
TSMSA	O
)	O
with	O
the	O
following	O
competitive	O
models	O
:	O
HAST	O
+	O
IOG	O
,	O
JERE	O
-	O
MHS	O
,	O
SpanMlt	O
,	O
and	O
SDRN	O
.	O
The	O
results	O
are	O
shown	O
in	O
Table	O
4	O
.	O
Note	O
that	O
the	O
overlapping	O
ratios	O
of	O
pairs	O
in	O
14lap	O
,	O
14res	O
,	O
and	O
15res	O
are	O
78.8	O
%	O
,	O
92	O
%	O
,	O
and	O
99.8	O
%	O
for	O
(	O
Fan	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
87.1	O
%	O
,	O
86.2	O
%	O
,	O
and	O
86.4	O
%	O
for	O
(	O
Chen	O
et	O
al	O
,	O
2020	O
)	O
,	O
respectively	O
.	O
Thus	O
,	O
there	O
is	O
a	O
difference	O
(	O
within	O
2	O
%	O
mostly	O
)	O
between	O
the	O
results	O
on	O
these	O
two	O
datasets	O
.	O
and	O
Chen	O
et	O
al	O
(	O
2020	O
)	O
.	O
Best	O
results	O
are	O
marked	O
in	O
bold	O
.	O
The	O
performance	O
of	O
JERE	O
-	O
MHS	O
is	O
better	O
than	O
HAST	O
+	O
IOG	O
,	O
which	O
indicates	O
that	O
the	O
degree	O
of	O
error	O
propagation	O
in	O
the	O
separate	O
training	O
model	O
might	O
be	O
smaller	O
than	O
it	O
in	O
the	O
model	O
of	O
joint	O
training	O
.	O
Moreover	O
,	O
SpanMlt	O
,	O
SDRN	O
,	O
and	O
MT	O
-	O
TSMSA	O
(	O
BERT	B-MethodName
)	O
use	O
powerful	O
pre	O
-	O
trained	O
language	O
models	O
,	O
which	O
have	O
a	O
significant	O
improvement	O
in	O
the	O
performance	O
on	O
AOPE	O
.	O
We	O
observe	O
that	O
SDRN	O
and	O
MT	O
-	O
TSMSA	O
(	O
BERT	B-MethodName
)	O
perform	O
better	O
than	O
Span	O
-	O
Mlt	O
,	O
showing	O
that	O
selecting	O
top	O
k	O
spans	O
from	O
candidate	O
spans	O
as	O
pairs	O
might	O
miss	O
some	O
correct	O
pairs	O
.	O
Compared	O
to	O
SDRN	O
,	O
MT	O
-	O
TSMSA	O
(	O
BERT	B-MethodName
)	O
performs	O
better	O
on	O
three	O
datasets	O
and	O
nearly	O
the	O
same	O
on	O
four	O
datasets	O
.	O
Overall	O
,	O
MT	O
-	O
TSMSA	O
achieves	O
quite	O
competitive	O
performance	O
on	O
AOPE	O
by	O
simply	O
incorporating	O
our	O
TSMSA	O
into	O
a	O
multitask	O
structure	O
.	O

To	O
evaluate	O
the	O
impacts	O
of	O
different	O
word	B-TaskName
embeddings	I-TaskName
and	O
training	O
strategies	O
on	O
our	O
models	O
,	O
we	O
conduct	O
ablation	O
experiments	O
by	O
varying	O
the	O
above	O
factors	O
.	O
The	O
results	O
shown	O
in	O
Table	O
5	O
indicate	O
that	O
a	O
suitable	O
word	O
embedding	O
is	O
capable	O
of	O
improving	O
the	O
performance	O
of	O
our	O
models	O
.	O
Firstly	O
,	O
BERT	B-MethodName
embedding	O
shows	O
poor	O
performance	O
when	O
compared	O
to	O
Glove	O
.	O
We	O
conjecture	O
that	O
BERT	B-MethodName
embedding	O
needs	O
to	O
cooperate	O
with	O
the	O
pre	O
-	O
trained	O
encoder	O
of	O
BERT	B-MethodName
to	O
perform	O
better	O
on	O
TOWE	O
.	O
Secondly	O
,	O
applying	O
the	O
word	O
embedding	O
and	O
the	O
encoder	O
of	O
BERT	B-MethodName
without	O
fine	O
-	O
tuning	O
also	O
fails	O
to	O
work	O
on	O
TOWE	O
.	O
The	O
reason	O
may	O
be	O
that	O
the	O
encoder	O
of	O
BERT	B-MethodName
without	O
fine	O
-	O
tuning	O
can	O
not	O
capture	O
the	O
information	O
of	O
the	O
specific	O
aspect	O
with	O
the	O
symbol	O
"	O
[	O
SEP	O
]	O
"	O
.	O
Furthermore	O
,	O
opinion	O
terms	O
extracted	O
from	O
task	O
0	B-DatasetName
help	O
to	O
identify	O
the	O
corresponding	O
opinion	O
terms	O
in	O
task	O
1	O
,	O
which	O
means	O
that	O
the	O
multi	O
-	O
task	O
structure	O
is	O
able	O
to	O
achieve	O
better	O
results	O
than	O
the	O
single	O
-	O
task	O
structure	O
on	O
TOWE	O
.	O
Although	O
the	O
improvement	O
is	O
not	O
significant	O
in	O
average	O
,	O
we	O
observe	O
that	O
the	O
former	O
structure	O
can	O
achieve	O
more	O
stable	O
performance	O
than	O
the	O
latter	O
one	O
.	O

In	O
this	O
part	O
,	O
we	O
apply	O
an	O
open	O
source	O
tool	O
4	O
to	O
visualize	O
the	O
attention	O
scores	O
of	O
TSMSA	O
(	O
BERT	B-MethodName
)	O
and	O
describe	O
two	O
attention	O
heads	O
on	O
the	O
tenth	O
layer	O
in	O
Figure	O
3	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
,	O
where	O
attention	O
scores	O
less	O
than	O
0.1	O
and	O
unimportant	O
words	O
are	O
not	O
displayed	O
.	O
As	O
we	O
can	O
see	O
,	O
the	O
words	O
"	O
nice	O
"	O
and	O
"	O
great	O
"	O
are	O
both	O
close	O
to	O
the	O
aspect	O
"	O
food	O
"	O
,	O
but	O
"	O
nice	O
"	O
will	O
not	O
pay	O
attention	O
to	O
this	O
aspect	O
.	O
In	O
addition	O
,	O
"	O
great	O
"	O
and	O
"	O
reasonable	O
"	O
focus	O
on	O
the	O
special	O
symbol	O
"	O
[	O
SEP	O
]	O
"	O
and	O
the	O
specific	O
aspect	O
"	O
food	O
"	O
,	O
as	O
shown	O
in	O
Figure	O
3	O
(	O
a	O
)	O
.	O
At	O
the	O
same	O
time	O
,	O
"	O
food	O
"	O
gives	O
attention	O
to	O
"	O
great	O
"	O
and	O
"	O
reasonable	O
"	O
on	O
different	O
attention	O
heads	O
,	O
as	O
described	O
in	O
Figure	O
3	O
(	O
b	O
)	O
.	O
All	O
these	O
instances	O
reveal	O
that	O
multi	O
-	O
head	O
self	O
-	O
attention	O
mechanism	O
is	O
capable	O
of	O
capturing	O
the	O
representation	O
of	O
a	O
[	O
SEP	O
]	O
[	O
SEP	O
]	O
[	O
SEP	O
]	O
[	O
SEP	O
]	O
[	O
SEP	O
]	O
[	O
SEP	O
]	O
[	O
SEP	O
]	O
specific	O
aspect	O
.	O

The	O
framing	O
of	O
political	O
issues	O
can	O
influence	O
policy	O
and	O
public	O
opinion	O
.	O
Even	O
though	O
the	O
public	O
plays	O
a	O
key	O
role	O
in	O
creating	O
and	O
spreading	O
frames	O
,	O
little	O
is	O
known	O
about	O
how	O
ordinary	O
people	O
on	O
social	O
media	O
frame	O
political	O
issues	O
.	O
By	O
creating	O
a	O
new	O
dataset	O
of	O
immigrationrelated	O
tweets	O
labeled	O
for	O
multiple	O
framing	O
typologies	O
from	O
political	O
communication	O
theory	O
,	O
we	O
develop	O
supervised	O
models	O
to	O
detect	O
frames	O
.	O
We	O
demonstrate	O
how	O
users	O
'	O
ideology	O
and	O
region	O
impact	O
framing	O
choices	O
,	O
and	O
how	O
a	O
message	O
's	O
framing	O
influences	O
audience	O
responses	O
.	O
We	O
find	O
that	O
the	O
more	O
commonlyused	O
issue	O
-	O
generic	O
frames	O
obscure	O
important	O
ideological	O
and	O
regional	O
patterns	O
that	O
are	O
only	O
revealed	O
by	O
immigration	O
-	O
specific	O
frames	O
.	O
Furthermore	O
,	O
frames	O
oriented	O
towards	O
human	O
interests	O
,	O
culture	O
,	O
and	O
politics	O
are	O
associated	O
with	O
higher	O
user	O
engagement	O
.	O
This	O
large	O
-	O
scale	O
analysis	O
of	O
a	O
complex	O
social	O
and	O
linguistic	O
phenomenon	O
contributes	O
to	O
both	O
NLP	O
and	O
social	O
science	O
research	O
.	O
Frame	O
Type	O
Frame	O
Description	O
Issue	O
-	O
Generic	O
Economic	O
Financial	O
implications	O
of	O
an	O
issue	O
Policy	O
Capacity	O
&	O
Resources	O
The	O
availability	O
or	O
lack	O
of	O
time	O
,	O
physical	O
,	O
human	O
,	O
or	O
financial	O
resources	O
Morality	O
&	O
Ethics	B-DatasetName
Perspectives	O
compelled	O
by	O
religion	O
or	O
secular	O
sense	O
of	O
ethics	O
or	O
social	O
responsibility	O
Fairness	B-TaskName
&	O
Equality	O
The	O
(	O
in	O
)	O
equality	O
with	O
which	O
laws	O
,	O
punishments	O
,	O
rewards	O
,	O
resources	O
are	O
distributed	O
Legality	O
,	O
Constitutionality	O
&	O
Jurisdiction	O
Court	O
cases	O
and	O
existing	O
laws	O
that	O
regulate	O
policies	O
;	O
constitutional	O
interpretation	O
;	O
legal	O
processes	O
such	O
as	O
seeking	O
asylum	O
or	O
obtaining	O
citizenship	O
;	O
jurisdiction	O

Because	O
many	O
people	O
now	O
generate	O
and	O
consume	O
political	O
content	O
on	O
social	O
media	O
,	O
scholars	O
have	O
increasingly	O
used	O
automated	O
techniques	O
to	O
study	O
framing	O
on	O
social	O
media	O
.	O
Large	O
-	O
scale	O
research	O
of	O
framing	O
on	O
Twitter	O
has	O
commonly	O
focused	O
on	O
unsupervised	O
approaches	O
.	O
(	O
e.g.	O
,	O
Russell	O
Neuman	O
et	O
al	O
,	O
2014	O
;	O
Meraz	O
and	O
Papacharissi	O
,	O
2013	O
;	O
de	O
Saint	O
Laurent	O
et	O
al	O
,	O
2020	O
)	O
.	O
Such	O
approaches	O
,	O
including	O
those	O
focused	O
on	O
hashtag	O
analysis	O
,	O
can	O
reveal	O
interesting	O
framing	O
patterns	O
.	O
For	O
instance	O
,	O
Siapera	O
et	O
al	O
(	O
2018	O
)	O
shows	O
that	O
frame	O
usage	O
varies	O
across	O
events	O
.	O
Similarly	O
,	O
topic	B-TaskName
models	I-TaskName
have	O
been	O
used	O
to	O
compare	O
"	O
refugee	O
crisis	O
"	O
media	O
discourses	O
across	O
the	O
European	O
countries	O
(	O
Heidenreich	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
to	O
uncover	O
differences	O
in	O
attitudes	O
towards	O
migrants	O
(	O
Hartnett	O
,	O
2019	O
)	O
.	O
Although	O
lexicon	O
analysis	O
and	O
topic	B-TaskName
models	I-TaskName
can	O
provide	O
insights	O
about	O
immigration	O
discourse	O
,	O
here	O
,	O
we	O
adopt	O
a	O
supervised	O
approach	O
to	O
ground	O
our	O
work	O
in	O
framing	O
research	O
and	O
to	O
enable	O
robust	O
evaluation	O
.	O
We	O
draw	O
inspiration	O
from	O
a	O
growing	O
body	O
of	O
NLP	O
research	O
that	O
uses	O
supervised	O
approaches	O
to	O
detect	O
issue	O
-	O
generic	O
policy	O
frames	O
in	O
news	O
articles	O
,	O
a	O
task	O
popularized	O
by	O
the	O
Media	O
Frames	O
Corpus	O
(	O
Card	O
et	O
al	O
,	O
2015	O
)	O
,	O
which	O
contains	O
issuegeneric	O
policy	O
frame	O
labels	O
for	O
articles	O
across	O
several	O
issues	O
(	O
Boydstun	O
et	O
al	O
,	O
2013	O
)	O
.	O
Using	O
this	O
corpus	O
,	O
prior	O
work	O
has	O
detected	O
frames	O
with	O
techniques	O
including	O
logistic	B-MethodName
regression	I-MethodName
(	O
Card	O
et	O
al	O
,	O
Table	O
1	O
:	O
List	O
of	O
all	O
issue	O
-	O
generic	O
policy	O
(	O
Boydstun	O
et	O
al	O
,	O
2013	O
)	O
,	O
immigration	O
-	O
specific	O
(	O
Benson	O
,	O
2013Hovden	O
and	O
Mjelde	O
,	O
2019	O
)	O
,	O
and	O
narrative	O
(	O
Iyengar	O
,	O
1991	O
)	O
frames	O
with	O
brief	O
descriptions	O
.	O
2016	O
)	O
,	O
recurrent	O
neural	O
networks	O
(	O
Naderi	O
and	O
Hirst	O
,	O
2017	O
)	O
,	O
lexicon	O
induction	O
(	O
Field	O
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
fine	O
-	O
tuning	O
pretrained	B-TaskName
language	I-TaskName
models	I-TaskName
(	O
Khanehzar	O
et	O
al	O
,	O
2019	O
;	O
Kwak	O
et	O
al	O
,	O
2020	O
)	O
.	O
Roy	O
and	O
Goldwasser	O
(	O
2020	O
)	O
further	O
extracted	O
subcategories	O
of	O
issue	O
-	O
generic	O
policy	O
frames	O
in	O
newspaper	O
coverage	O
using	O
a	O
weakly	O
-	O
supervised	O
approach	O
.	O
Finally	O
,	O
issue	O
-	O
generic	O
frames	O
have	O
also	O
been	O
computationally	O
studied	O
in	O
other	O
media	O
,	O
including	O
online	O
fora	O
and	O
politicians	O
'	O
tweets	O
(	O
Johnson	O
et	O
al	O
,	O
2017	O
;	O
Hartmann	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
build	O
upon	O
this	O
literature	O
by	O
incorporating	O
additional	O
frame	O
typologies	O
that	O
reflect	O
important	O
dimensions	O
of	O
media	O
discourse	O
with	O
real	O
-	O
world	O
consequences	O
(	O
Iyengar	O
,	O
1991	O
;	O
Gross	O
,	O
2008	O
;	O
Eberl	O
et	O
al	O
,	O
2018	O
)	O
.	O
Beyond	O
detecting	O
frames	O
,	O
we	O
computationally	O
analyze	O
frame	O
-	O
building	O
and	O
frame	O
-	O
setting	O
among	O
social	O
media	O
users	O
;	O
though	O
well	O
-	O
studied	O
in	O
traditional	O
news	O
media	O
,	O
little	O
is	O
known	O
about	O
how	O
social	O
media	O
users	O
frame	O
immigration	O
or	O
its	O
effects	O
(	O
Eberl	O
et	O
al	O
,	O
2018	O
)	O
.	O
Noting	O
that	O
issue	O
-	O
generic	O
policy	O
frames	O
obscure	O
important	O
linguistic	O
differences	O
,	O
several	O
works	O
stud	O
-	O
ied	O
issue	O
-	O
specific	O
frames	O
in	O
news	O
media	O
for	O
issues	O
such	O
as	O
missile	O
defense	O
and	O
gun	O
violence	O
(	O
Morstatter	O
et	O
al	O
,	O
2018	O
;	O
Liu	O
et	O
al	O
,	O
2019a	O
)	O
.	O
We	O
extend	O
issuespecific	O
frame	O
analyses	O
to	O
immigration	O
by	O
adopting	O
an	O
immigration	O
-	O
specific	O
typology	O
developed	O
by	O
political	O
communication	O
scholars	O
(	O
Benson	O
,	O
2013	O
)	O
.	O
In	O
contrast	O
to	O
prior	O
NLP	O
work	O
focused	O
on	O
traditional	O
media	O
or	O
political	O
elites	O
(	O
Johnson	O
et	O
al	O
,	O
2017	O
;	O
Field	O
et	O
al	O
,	O
2018	O
)	O
,	O
we	O
highlight	O
the	O
role	O
that	O
social	O
media	O
publics	O
play	O
in	O
generating	O
and	O
propagating	O
frames	O
.	O
Furthermore	O
,	O
we	O
provide	O
a	O
new	O
computational	O
model	O
of	O
narrative	O
framing	O
(	O
Iyengar	O
,	O
1991	O
)	O
,	O
that	O
together	O
with	O
models	O
for	O
issue	O
-	O
generic	O
policy	O
and	O
issue	O
-	O
specific	O
frames	O
,	O
provides	O
complementary	O
views	O
on	O
the	O
framing	O
of	O
immigration	O
.	O
Finally	O
,	O
our	O
large	O
-	O
scale	O
analysis	O
of	O
frame	O
-	O
setting	O
illustrates	O
the	O
potential	O
for	O
using	O
NLP	O
to	O
understand	O
how	O
a	O
message	O
's	O
framing	O
shapes	O
its	O
audience	O
behavior	O
.	O

We	O
first	O
collect	O
a	O
large	O
dataset	O
of	O
immigrationrelated	O
tweets	O
,	O
and	O
then	O
annotate	O
a	O
subset	O
of	O
this	O
full	O
dataset	O
for	O
multiple	O
types	O
of	O
frames	O
.	O
Data	O
Collection	O
We	O
extract	O
all	O
English	O
-	O
language	O
tweets	O
in	O
2018	O
and	O
2019	O
from	O
the	O
Twitter	O
Decahose	O
containing	O
at	O
least	O
one	O
of	O
the	O
following	O
terms	O
:	O
immigration	O
,	O
immigrant	O
(	O
s	O
)	O
,	O
emigration	O
,	O
emigrant	O
(	O
s	O
)	O
,	O
migration	O
,	O
migrant	O
(	O
s	O
)	O
,	O
illegal	O
alien	O
(	O
s	O
)	O
,	O
illegals	O
,	O
and	O
undocumented	O
1	O
.	O
We	O
focus	O
on	O
content	O
creation	O
and	O
thus	O
exclude	O
retweets	O
from	O
our	O
dataset	O
,	O
though	O
we	O
consider	O
retweeting	O
rates	O
when	O
analyzing	O
the	O
social	O
influence	O
of	O
different	O
frames	O
.	O
We	O
further	O
restrict	O
our	O
dataset	O
to	O
tweets	O
whose	O
authors	O
are	O
identified	O
as	O
being	O
located	O
in	O
the	O
United	O
States	O
(	O
US	O
)	O
,	O
United	O
Kingdom	O
(	O
GB	O
)	O
,	O
and	O
European	O
Union	O
(	O
EU	O
)	O
by	O
an	O
existing	O
location	O
inference	O
tool	O
(	O
Compton	O
et	O
al	O
,	O
2014	O
)	O
.	O
To	O
compare	O
framing	O
across	O
political	O
ideologies	O
,	O
we	O
obtain	O
ideal	O
point	O
estimates	O
for	O
nearly	O
two	O
-	O
thirds	O
of	O
US	O
-	O
based	O
users	O
with	O
Barberá	O
(	O
2015	O
)	O
's	O
Bayesian	O
Spatial	O
Following	O
model	O
.	O
Our	O
full	O
dataset	O
contains	O
over	O
2.66	O
million	O
tweets	O
,	O
86.2	O
%	O
of	O
which	O
are	O
from	O
the	O
United	O
States	O
,	O
10.4	O
%	O
from	O
the	O
United	O
Kingdom	O
,	O
and	O
3.4	O
%	O
from	O
the	O
European	O
Union	O
.	O
Data	O
Annotation	O
Tweets	O
are	O
annotated	O
using	O
three	O
frame	O
typologies	O
:	O
(	O
i	O
)	O
issue	O
-	O
generic	O
policy	O
,	O
(	O
ii	O
)	O
immigration	O
-	O
specific	O
,	O
and	O
(	O
iii	O
)	O
narrative	O
frames	O
,	O
where	O
a	O
tweet	O
may	O
use	O
multiple	O
frames	O
simultaneously	O
.	O
We	O
use	O
Boydstun	O
et	O
al	O
(	O
2013	O
)	O
's	O
Policy	O
Frames	O
Codebook	O
to	O
formulate	O
our	O
initial	O
guidelines	O
to	O
code	O
for	O
policy	O
frames	O
.	O
We	O
use	O
Benson	O
(	O
2013	O
)	O
's	O
immigration	O
-	O
specific	O
frames	O
,	O
but	O
follow	O
Hovden	O
and	O
Mjelde	O
(	O
2019	O
)	O
in	O
including	O
an	O
additional	O
category	O
for	O
framing	O
immigrants	O
as	O
victims	O
of	O
war	O
.	O
Finally	O
,	O
we	O
code	O
for	O
narrative	O
frames	O
using	O
definitions	O
from	O
Iyengar	O
(	O
1991	O
)	O
.	O
All	O
frames	O
and	O
descriptions	O
can	O
be	O
found	O
in	O
Table	O
1	O
,	O
with	O
a	O
complete	O
codebook	O
in	O
Supplementary	O
Materials	O
.	O
Because	O
annotation	O
guidelines	O
from	O
prior	O
work	O
focus	O
on	O
elite	O
communications	O
,	O
we	O
first	O
adjusted	O
our	O
codebook	O
to	O
address	O
challenges	O
posed	O
by	O
Twitter	O
content	O
.	O
Changes	O
were	O
made	O
based	O
on	O
feedback	O
from	O
four	O
trained	O
annotators	O
who	O
labeled	O
360	O
tweets	O
from	O
2018	O
,	O
split	O
between	O
the	O
EU	O
,	O
GB	O
,	O
and	O
US	O
.	O
Even	O
for	O
humans	O
,	O
identifying	O
frames	O
in	O
tweets	O
is	O
a	O
difficult	O
task	O
.	O
Defining	O
the	O
boundaries	O
of	O
what	O
constitutes	O
a	O
message	O
is	O
not	O
trivial	O
.	O
Beyond	O
the	O
text	O
,	O
frames	O
could	O
be	O
identified	O
in	O
hashtags	O
,	O
images	O
,	O
videos	O
,	O
and	O
content	O
from	O
linked	O
pages	O
.	O
Furthermore	O
,	O
tweets	O
are	O
often	O
replies	O
to	O
other	O
users	O
or	O
part	O
of	O
a	O
larger	O
thread	O
.	O
This	O
additional	O
context	O
may	O
influence	O
an	O
issue	O
's	O
framing	O
.	O
For	O
simplicity	O
,	O
we	O
treat	O
each	O
tweet	O
as	O
a	O
standalone	O
message	O
and	O
label	O
frames	O
based	O
only	O
on	O
the	O
text	O
(	O
including	O
hashtags	O
)	O
.	O
Unlike	O
news	O
stories	O
,	O
where	O
frames	O
are	O
clearly	O
cued	O
,	O
tweets	O
often	O
implicitly	O
allude	O
to	O
frames	O
due	O
to	O
character	O
limitations	O
.	O
For	O
example	O
,	O
a	O
tweet	O
expressing	O
desire	O
to	O
"	O
drive	O
immigrants	O
out	O
"	O
with	O
no	O
additional	O
context	O
may	O
suggest	O
a	O
criminal	O
frame	O
,	O
but	O
criminality	O
is	O
not	O
explicit	O
.	O
To	O
minimize	O
errors	O
,	O
we	O
avoid	O
making	O
assumptions	O
about	O
intended	O
meaning	O
and	O
interpret	O
al	O
messages	O
literally	O
.	O
Training	O
,	O
development	O
,	O
and	O
test	O
data	O
were	O
annotated	O
using	O
two	O
procedures	O
after	O
four	O
annotators	O
completed	O
four	O
rounds	O
of	O
training	O
.	O
The	O
dataset	O
contains	O
equal	O
numbers	O
of	O
tweets	O
from	O
the	O
EU	O
,	O
UK	O
,	O
and	O
US	O
.	O
Training	O
data	O
was	O
singly	O
annotated	O
and	O
includes	O
3	O
,	O
600	O
tweets	O
,	O
while	O
the	O
development	O
and	O
test	O
sets	O
each	O
contain	O
450	O
tweets	O
(	O
10	O
%	O
of	O
the	O
full	O
dataset	O
)	O
and	O
were	O
consensus	O
-	O
coded	O
by	O
pairs	O
of	O
trained	O
annotators	O
.	O
We	O
opt	O
for	O
this	O
two	O
-	O
tier	O
approach	O
due	O
to	O
(	O
i	O
)	O
the	O
inherent	O
difficulty	O
of	O
the	O
task	O
2	O
and	O
(	O
ii	O
)	O
the	O
need	O
to	O
maximize	O
diversity	O
seen	O
in	O
training	O
.	O
During	O
annotator	O
training	O
,	O
pilot	O
studies	O
attained	O
moderate	O
agreement	O
,	O
suggesting	O
that	O
to	O
attain	O
high	O
-	O
reliability	O
,	O
consensus	O
coding	O
with	O
adjudication	O
would	O
be	O
needed	O
(	O
Krippendorff	O
,	O
2013	O
)	O
,	O
which	O
comes	O
at	O
a	O
cost	O
of	O
substantially	O
increased	O
time	O
.	O
Because	O
a	O
large	O
dataset	O
of	O
unique	O
,	O
singlycoded	O
documents	O
is	O
preferable	O
to	O
a	O
small	O
dataset	O
of	O
documents	O
coded	O
by	O
multiple	O
annotators	O
for	O
text	B-TaskName
classification	I-TaskName
(	O
Barbera	O
et	O
al	O
,	O
2021	O
)	O
,	O
we	O
decided	O
to	O
increase	O
corpus	O
diversity	O
in	O
the	O
training	O
data	O
by	O
singly	O
-	O
annotating	O
,	O
at	O
the	O
expense	O
of	O
potentially	O
noisier	O
annotation	O
,	O
and	O
to	O
consensus	O
code	O
all	O
evaluation	O
data	O
.	O
On	O
the	O
double	O
annotated	O
data	O
,	O
annotators	O
attained	O
Krippendorff	O
's	O
α=0.45	O
.	O
Additional	O
details	O
are	O
provided	O
in	O
Supplementary	B-DatasetName
Material	I-DatasetName
(	O
B	O
,	O
Figures	O
6	O
and	O
7	O
)	O
.	O
Results	O
We	O
observe	O
differences	O
across	O
frame	O
typologies	O
in	O
coverage	O
rates	O
within	O
the	O
annotated	O
data	O
set	O
.	O
While	O
84	O
%	O
of	O
tweets	O
are	O
labeled	O
with	O
at	O
least	O
one	O
issue	O
-	O
generic	O
policy	O
frame	O
and	O
85	O
%	O
with	O
at	O
least	O
one	O
narrative	O
frame	O
,	O
only	O
51	O
%	O
are	O
labeled	O
with	O
at	O
least	O
one	O
issue	O
-	O
specific	O
frame	O
.	O
This	O
difference	O
is	O
due	O
to	O
immigration	O
-	O
specific	O
frames	O
being	O
more	O
narrowly	O
-	O
defined	O
,	O
as	O
they	O
require	O
explicit	O
judgment	O
of	O
immigrants	O
as	O
heroes	O
,	O
victims	O
,	O
or	O
threats	O
.	O
Further	O
details	O
about	O
frame	O
distributions	O
Random	O
LogReg	O
RoBERTa	B-MethodName
FT	O
RoBERTa	B-MethodName
0.193	O
0.296	O
0.611	O
0.657	O
in	O
our	O
annotations	O
can	O
be	O
found	O
in	O
Supplementary	B-DatasetName
Material	I-DatasetName
(	O
A	O
,	O
Figure	O
5	O
)	O
.	O
While	O
the	O
precision	O
of	O
issue	O
-	O
specific	O
frames	O
can	O
reveal	O
patterns	O
otherwise	O
obscured	O
by	O
the	O
broader	O
issue	O
-	O
generic	O
frames	O
,	O
this	O
lack	O
of	O
coverage	O
presents	O
two	O
challenges	O
:	O
1	O
)	O
automated	O
detection	O
is	O
more	O
challenging	O
given	O
this	O
sparsity	O
and	O
2	O
)	O
analyses	O
of	O
issue	O
-	O
specific	O
frames	O
do	O
not	O
capture	O
a	O
large	O
portion	O
of	O
immigration	O
-	O
related	O
discourse	O
.	O
By	O
incorporating	O
multiple	O
framing	O
strategies	O
,	O
we	O
leverage	O
both	O
the	O
coverage	O
of	O
issue	O
-	O
generic	O
frames	O
and	O
the	O
precision	O
and	O
interpretability	O
of	O
issue	O
-	O
specific	O
frames	O
.	O

We	O
detect	O
frames	O
for	O
all	O
2.6	O
M	O
immigration	O
-	O
related	O
tweets	O
using	O
the	O
finetuned	O
RoBERTa	B-MethodName
model	O
with	O
the	O
best	O
-	O
performing	O
seed	O
on	O
development	O
data	O
.	O
Using	O
this	O
labeled	O
data	O
,	O
we	O
estimate	O
the	O
effects	O
of	O
region	O
and	O
ideology	O
by	O
fitting	O
separate	O
mixed	O
-	O
effects	O
logistic	B-MethodName
regression	I-MethodName
models	O
to	O
predict	O
the	O
presence	O
or	O
absence	O
of	O
each	O
frame	O
.	O
We	O
treat	O
region	O
(	O
US	O
,	O
UK	O
,	O
and	O
EU	O
)	O
as	O
a	O
categorical	O
variable	O
,	O
with	O
US	O
as	O
the	O
reference	O
level	O
.	O
Ideology	O
is	O
estimated	O
using	O
the	O
method	O
of	O
Barberá	O
(	O
2015	O
)	O
,	O
which	O
is	O
based	O
on	O
users	O
'	O
connections	O
to	O
US	O
political	O
elites	O
;	O
as	O
such	O
,	O
we	O
restrict	O
our	O
analysis	O
of	O
ideology	O
to	O
only	O
tweets	O
from	O
the	O
United	O
States	O
.	O
To	O
account	O
for	O
exogenous	O
events	O
that	O
may	O
impact	O
framing	O
,	O
we	O
include	O
nested	O
random	O
effects	O
for	O
year	O
,	O
month	O
,	O
and	O
date	O
.	O
We	O
further	O
control	O
for	O
user	O
characteristics	O
(	O
e.g.	O
the	O
author	O
's	O
follower	O
count	O
,	O
friends	O
count	O
,	O
verified	O
status	O
and	O
number	O
of	O
prior	O
tweets	O
)	O
as	O
well	O
as	O
other	O
tweet	O
characteristics	O
(	O
e.g.	O
tweet	O
length	O
,	O
if	O
a	O
tweet	O
is	O
a	O
reply	O
,	O
and	O
whether	O
the	O
tweet	O
contains	O
hashtags	O
,	O
URLs	O
,	O
or	O
mentions	O
of	O
other	O
users	O
)	O
.	O
We	O
apply	O
Holm	O
-	O
Bonferroni	O
corrections	O
on	O
p	O
-	O
values	O
before	O
significance	O
testing	O
to	O
account	O
for	O
multiple	O
hypothesis	O
testing	O
.	O
Ideology	O
Ideology	O
is	O
strongly	O
predictive	O
of	O
framing	O
strategies	O
in	O
all	O
three	O
categories	O
,	O
as	O
shown	O
in	O
Figure	O
2	O
.	O
Our	O
results	O
reveal	O
three	O
broad	O
themes	O
.	O
First	O
,	O
prior	O
work	O
has	O
argued	O
that	O
liberals	O
and	O
conservatives	O
adhere	O
to	O
different	O
moral	O
foundations	O
,	O
with	O
conservatives	O
being	O
more	O
sensitive	O
to	O
in	O
-	O
group	O
/	O
loyalty	O
and	O
authority	O
than	O
liberals	O
,	O
who	O
are	O
more	O
sensitive	O
to	O
care	O
and	O
fairness	O
(	O
Graham	O
et	O
al	O
,	O
2009	O
)	O
.	O
Our	O
results	O
agree	O
with	O
this	O
argument	O
.	O
Liberals	O
are	O
more	O
likely	O
to	O
frame	O
immigration	O
as	O
a	O
fairness	O
and	O
morality	O
issue	O
,	O
and	O
immigrants	O
as	O
victims	O
of	O
discrimination	O
and	O
inhumane	O
policies	O
.	O
More	O
conservative	O
authors	O
,	O
on	O
the	O
other	O
hand	O
,	O
fo	O
-	O
cus	O
on	O
frames	O
with	O
implications	O
for	O
the	O
in	O
-	O
group	O
.	O
They	O
express	O
concerns	O
about	O
1	O
)	O
immigrants	O
imposing	O
a	O
burden	O
on	O
taxpayers	O
and	O
governmental	O
programs	O
and	O
2	O
)	O
immigrants	O
being	O
criminals	O
and	O
threats	O
to	O
public	O
safety	O
.	O
We	O
qualitatively	O
observe	O
three	O
distinct	O
,	O
though	O
unsubstantiated	O
,	O
conservative	O
claims	O
contributing	O
to	O
the	O
latter	O
:	O
(	O
i.	O
)	O
Immigrants	O
commit	O
violent	O
crimes	O
(	O
Light	O
and	O
Miller	O
,	O
2018	O
)	O
,	O
(	O
ii	O
.	O
)	O
Undocumented	O
immigrants	O
illegally	O
vote	O
in	O
US	O
elections	O
(	O
Smith	O
,	O
2017	O
;	O
Udani	O
and	O
Kimball	O
,	O
2018	O
)	O
,	O
and	O
(	O
iii	O
.	O
)	O
Immigrants	O
are	O
criminals	O
simply	O
by	O
virtue	O
of	O
being	O
immigrants	O
(	O
Ewing	O
et	O
al	O
,	O
2015	O
)	O
.	O
Figure	O
2	O
shows	O
a	O
clear	O
ideological	O
stratification	O
for	O
issue	O
-	O
specific	O
frames	O
:	O
liberals	O
favor	O
hero	O
and	O
victim	O
frames	O
,	O
while	O
conservatives	O
favor	O
threat	O
frames	O
.	O
This	O
finding	O
is	O
consistent	O
with	O
prior	O
work	O
on	O
the	O
role	O
perceived	O
threats	O
play	O
in	O
shaping	O
white	O
American	O
attitudes	O
towards	O
immigration	O
(	O
Brader	O
et	O
al	O
,	O
2008	O
)	O
,	O
and	O
the	O
disposition	O
of	O
political	O
conservatism	O
to	O
avoid	O
potential	O
threats	O
(	O
Jost	O
et	O
al	O
,	O
2003	O
)	O
.	O
Second	O
,	O
while	O
all	O
frame	O
categories	O
show	O
ideological	O
bias	O
,	O
issue	O
-	O
specific	O
frames	O
are	O
the	O
most	O
extreme	O
.	O
Most	O
notably	O
,	O
our	O
analysis	O
shows	O
that	O
focusing	O
solely	O
on	O
issue	O
-	O
generic	O
policy	O
frames	O
would	O
obscure	O
important	O
patterns	O
.	O
For	O
example	O
,	O
the	O
issuegeneric	O
cultural	O
identity	O
frame	O
shows	O
a	O
slight	O
liberal	O
bias	O
;	O
yet	O
,	O
related	O
issue	O
-	O
specific	O
frames	O
diverge	O
:	O
hero	O
:	O
cultural	O
diversity	O
is	O
very	O
liberal	O
while	O
threat	O
:	O
national	O
cohesion	O
is	O
very	O
conservative	O
.	O
Similarly	O
,	O
the	O
issue	O
-	O
generic	O
economic	O
policy	O
frame	O
is	O
slightly	O
favored	O
by	O
more	O
conservative	O
authors	O
,	O
but	O
the	O
related	O
issue	O
-	O
specific	O
frames	O
threat	O
:	O
jobs	O
and	O
hero	O
:	O
worker	O
reveal	O
ideological	O
divides	O
.	O
This	O
finding	O
highlights	O
the	O
importance	O
of	O
using	O
multiple	O
framing	O
typologies	O
to	O
provide	O
a	O
more	O
nuanced	O
analysis	O
of	O
immigration	O
discourse	O
.	O
Third	O
,	O
more	O
liberal	O
authors	O
tend	O
to	O
use	O
episodic	O
frames	O
,	O
while	O
conservative	O
authors	O
tend	O
to	O
use	O
thematic	O
frames	O
.	O
This	O
difference	O
is	O
consistent	O
with	O
Somaini	O
(	O
2019	O
)	O
's	O
finding	O
that	O
a	O
local	O
liberal	O
newspaper	O
featured	O
more	O
episodic	O
framing	O
in	O
immigration	O
coverage	O
,	O
but	O
a	O
comparable	O
conservative	O
newspaper	O
featured	O
more	O
thematic	O
framing	O
.	O
Other	O
efforts	O
that	O
examine	O
the	O
relationship	O
between	O
narrative	O
frames	O
and	O
cognitive	O
and	O
emotional	O
responses	O
provide	O
some	O
clues	O
for	O
the	O
observed	O
pattern	O
.	O
For	O
instance	O
,	O
Aarøe	O
(	O
2011	O
)	O
shows	O
that	O
thematic	O
frames	O
are	O
stronger	O
when	O
there	O
are	O
no	O
or	O
weak	O
emotional	O
responses	O
;	O
and	O
that	O
the	O
opposite	O
is	O
true	O
for	O
episodic	O
frames	O
.	O
The	O
divergence	O
of	O
findings	O
could	O
be	O
driven	O
by	O
partisans	O
'	O
differing	O
emotional	O
responses	O
.	O
Our	O
findings	O
also	O
highlight	O
important	O
consequences	O
for	O
opinion	O
formation	O
.	O
Iyengar	O
(	O
1990	O
)	O
shows	O
that	O
episodic	O
framing	O
diverts	O
attention	O
from	O
societal	O
and	O
political	O
party	O
responsibility	O
;	O
our	O
results	O
suggest	O
that	O
liberal	O
Twitter	O
users	O
are	O
likely	O
to	O
produce	O
(	O
and	O
,	O
due	O
to	O
partisan	O
self	O
-	O
segregation	O
,	O
consume	O
)	O
social	O
media	O
content	O
with	O
such	O
effects	O
.	O
Region	O
Immigration	O
framing	O
depends	O
heavily	O
on	O
one	O
's	O
geopolitical	O
entity	O
(	O
US	O
,	O
UK	O
,	O
and	O
EU	O
)	O
,	O
as	O
shown	O
in	O
Figure	O
3	O
.	O
Several	O
notable	O
themes	O
emerge	O
.	O
First	O
,	O
many	O
ideologically	O
-	O
extreme	O
frames	O
in	O
the	O
US	O
,	O
including	O
crime	O
&	O
punishment	O
,	O
security	O
&	O
defense	O
,	O
threat	O
:	O
public	O
order	O
,	O
and	O
threat	O
:	O
fiscal	O
are	O
all	O
significantly	O
more	O
likely	O
to	O
be	O
found	O
in	O
US	O
-	O
based	O
tweets	O
relative	O
to	O
the	O
UK	O
and	O
EU	O
.	O
This	O
pattern	O
suggests	O
that	O
region	O
and	O
ideology	O
,	O
and	O
likely	O
many	O
other	O
factors	O
,	O
interact	O
in	O
intricate	O
ways	O
to	O
shape	O
how	O
ordinary	O
people	O
frame	O
political	O
issues	O
.	O
Second	O
,	O
cultural	O
identity	O
is	O
more	O
strongly	O
associated	O
with	O
both	O
the	O
UK	O
and	O
EU	O
than	O
the	O
US	O
.	O
Perhaps	O
immigrants	O
'	O
backgrounds	O
are	O
more	O
marked	O
in	O
European	O
discourse	O
than	O
in	O
US	O
discourse	O
because	O
the	O
UK	O
and	O
EU	O
have	O
longer	O
histories	O
of	O
cultural	O
and	O
ethnic	O
homogeneity	O
(	O
Thorbjørnsrud	O
,	O
2015	O
)	O
.	O
This	O
finding	O
also	O
reflects	O
that	O
Europeans	O
'	O
attitudes	O
towards	O
immigration	O
depend	O
on	O
where	O
immigrants	O
are	O
from	O
and	O
parallels	O
how	O
European	O
newspapers	O
frame	O
immigration	O
differently	O
depending	O
on	O
migrants	O
'	O
countries	O
of	O
origin	O
(	O
Eberl	O
et	O
al	O
,	O
2018	O
)	O
.	O
Finally	O
,	O
the	O
bottom	O
of	O
Figure	O
3	O
shows	O
that	O
users	O
from	O
the	O
UK	O
are	O
more	O
likely	O
to	O
invoke	O
labor	O
-	O
related	O
frames	O
.	O
This	O
prevalence	O
of	O
labor	O
and	O
economic	O
frames	O
has	O
also	O
been	O
found	O
in	O
British	O
traditional	O
media	O
(	O
Caviedes	O
,	O
2015	O
;	O
Lawlor	O
,	O
2015	O
)	O
,	O
and	O
has	O
been	O
attributed	O
to	O
differences	O
in	O
the	O
labor	O
market	O
.	O
Unlike	O
migrants	O
in	O
the	O
US	O
,	O
Italy	O
,	O
and	O
France	O
,	O
who	O
often	O
work	O
clandestinely	O
in	O
different	O
economic	O
sectors	O
than	O
domestic	O
workers	O
,	O
UK	O
migrants	O
have	O
proper	O
authorization	O
and	O
are	O
thus	O
viewed	O
as	O
competition	O
for	O
British	O
workers	O
because	O
they	O
can	O
work	O
in	O
the	O
same	O
industries	O
(	O
Caviedes	O
,	O
2015	O
)	O
.	O

Our	O
analysis	O
of	O
frame	O
-	O
building	O
involves	O
inferring	O
political	O
ideology	O
and	O
regional	O
from	O
users	O
with	O
existing	O
tools	O
,	O
so	O
we	O
aggregated	O
this	O
information	O
in	O
our	O
analysis	O
in	O
order	O
to	O
minimize	O
the	O
risk	O
of	O
exposing	O
potentially	O
sensitive	O
personal	O
data	O
about	O
individuals	O
.	O
Our	O
dataset	O
includes	O
tweet	O
IDs	O
along	O
with	O
frame	O
labels	O
,	O
but	O
no	O
additional	O
social	O
information	O
.	O
However	O
,	O
there	O
are	O
also	O
ethical	O
consequences	O
of	O
categorizing	O
people	O
along	O
these	O
social	O
dimensions	O
.	O
We	O
acknowledge	O
that	O
reducing	O
people	O
's	O
social	O
identities	O
to	O
region	O
and	O
ideology	O
obscures	O
the	O
wide	O
range	O
of	O
unobservable	O
and	O
non	O
-	O
quantifiable	O
predispositions	O
and	O
experiences	O
that	O
may	O
impact	O
framing	O
and	O
attitudes	O
towards	O
immigration	O
.	O
We	O
emphasize	O
that	O
our	O
dataset	O
is	O
not	O
fully	O
representative	O
of	O
all	O
immigration	O
discourse	O
and	O
should	O
not	O
be	O
treated	O
as	O
such	O
.	O
Twitter	O
's	O
demographics	O
are	O
not	O
representative	O
of	O
the	O
global	O
population	O
(	O
Mislove	O
et	O
al	O
,	O
2011	O
)	O
.	O
Furthermore	O
,	O
our	O
dataset	O
only	O
includes	O
tweets	O
with	O
authors	O
from	O
particular	O
Western	O
countries	O
.	O
All	O
tweets	O
were	O
automatically	O
identified	O
by	O
Twitter	O
as	O
being	O
written	O
in	O
English	O
,	O
thus	O
additionally	O
imposing	O
standard	O
language	O
ideologies	O
on	O
the	O
data	O
that	O
we	O
include	O
(	O
Milroy	O
,	O
2001	O
)	O
.	O
Furthermore	O
,	O
language	O
choice	O
itself	O
can	O
be	O
a	O
socially	O
and	O
politically	O
meaningful	O
linguistic	O
cue	O
that	O
may	O
have	O
unique	O
interactions	O
with	O
framing	O
(	O
e.g.	O
,	O
Gal	O
,	O
1978	O
;	O
Shoemark	O
et	O
al	O
,	O
2017	O
;	O
Stewart	O
et	O
al	O
,	O
2018	O
;	O
Ndubuisi	O
-	O
Obi	O
et	O
al	O
,	O
2019	O
)	O
.	O
Although	O
we	O
do	O
not	O
focus	O
on	O
abusive	B-TaskName
language	I-TaskName
,	O
our	O
topical	O
content	O
contains	O
frequent	O
instances	O
of	O
racism	O
,	O
Islamophobia	O
,	O
antisemitism	O
,	O
and	O
personal	O
insults	O
.	O
We	O
caution	O
future	O
researchers	O
about	O
potentially	O
traumatic	O
psychological	O
effects	O
of	O
working	O
with	O
this	O
dataset	O
.	O
We	O
aim	O
to	O
support	O
immigrants	O
,	O
an	O
often	O
marginalized	O
group	O
,	O
by	O
shedding	O
light	O
on	O
their	O
representation	O
on	O
social	O
media	O
.	O
However	O
,	O
there	O
is	O
a	O
risk	O
that	O
malicious	O
agents	O
could	O
exploit	O
our	O
framesetting	O
findings	O
by	O
disseminating	O
harmful	O
content	O
packaged	O
in	O
more	O
popular	O
frames	O
.	O

Tables	O
5	O
-	O
8	O
and	O
Figures	O
8	O
-	O
9	O
provide	O
details	O
about	O
the	O
fine	O
-	O
tuned	O
RoBERTa	B-MethodName
models	O
'	O
performance	O
.	O

We	O
probe	O
the	O
heterogeneity	O
in	O
levels	O
of	O
abusive	B-TaskName
language	I-TaskName
in	O
different	O
sections	O
of	O
the	O
Internet	O
,	O
using	O
an	O
annotated	O
corpus	O
of	O
Wikipedia	O
page	O
edit	O
comments	O
to	O
train	O
a	O
binary	O
classifier	O
for	O
abuse	B-TaskName
detection	I-TaskName
.	O
Our	O
test	O
data	O
come	O
from	O
the	O
CrimeBB	O
Corpus	O
of	O
hacking	O
-	O
related	O
forum	O
posts	O
and	O
we	O
find	O
that	O
(	O
a	O
)	O
forum	O
interactions	O
are	O
rarely	O
abusive	O
,	O
(	O
b	O
)	O
the	O
abusive	B-TaskName
language	I-TaskName
which	O
does	O
exist	O
tends	O
to	O
be	O
relatively	O
mild	O
compared	O
to	O
that	O
found	O
in	O
the	O
Wikipedia	O
comments	O
domain	O
,	O
and	O
tends	O
to	O
involve	O
aggressive	O
posturing	O
rather	O
than	O
hate	B-DatasetName
speech	I-DatasetName
or	O
threats	O
of	O
violence	O
.	O
We	O
observe	O
that	O
the	O
purpose	O
of	O
conversations	O
in	O
online	O
forums	O
tend	O
to	O
be	O
more	O
constructive	O
and	O
informative	O
than	O
those	O
in	O
Wikipedia	O
page	O
edit	O
comments	O
which	O
are	O
geared	O
more	O
towards	O
adversarial	O
interactions	O
,	O
and	O
that	O
this	O
may	O
explain	O
the	O
lower	O
levels	O
of	O
abuse	O
found	O
in	O
our	O
forum	O
data	O
than	O
in	O
Wikipedia	O
comments	O
.	O
Further	O
work	O
remains	O
to	O
be	O
done	O
to	O
compare	O
these	O
results	O
with	O
other	O
inter	O
-	O
domain	O
classification	O
experiments	O
,	O
and	O
to	O
understand	O
the	O
impact	O
of	O
aggressive	O
language	O
in	O
forum	O
conversations	O
.	O

The	O
automatic	O
identification	O
of	O
abusive	B-TaskName
language	I-TaskName
online	O
1	O
is	O
of	O
growing	O
interest	O
and	O
concerns	O
have	O
proliferated	O
about	O
aggressive	O
Internet	O
behaviours	O
commonly	O
known	O
as	O
'	O
trolling	O
'	O
.	O
From	O
an	O
applications	O
perspective	O
,	O
the	O
accurate	O
detection	O
of	O
vitriolic	O
language	O
is	O
one	O
of	O
the	O
clearest	O
examples	O
of	O
natural	O
language	O
processing	O
for	O
social	O
good	O
,	O
assuming	O
data	O
has	O
been	O
collected	O
ethically	O
and	O
stored	O
legally	O
,	O
and	O
that	O
any	O
intervention	O
is	O
left	O
to	O
the	O
appropriate	O
authorities	O
(	O
Kennedy	O
et	O
al	O
,	O
2017	O
;	O
Kumar	B-DatasetName
et	O
al	O
,	O
2018	O
)	O
.	O
Meanwhile	O
from	O
a	O
theoretical	O
point	O
of	O
view	O
,	O
there	O
are	O
many	O
outstanding	O
linguistic	O
and	O
sociological	O
research	O
questions	O
surrounding	O
Internet	O
aggression	O
and	O
how	O
it	O
manifests	O
itself	O
in	O
writing	O
(	O
Pieschl	O
et	O
al	O
,	O
2015	O
;	O
Waseem	O
et	O
al	O
,	O
2017	O
)	O
.	O
The	O
question	O
we	O
address	O
here	O
is	O
whether	O
online	O
abusive	B-TaskName
language	I-TaskName
is	O
of	O
one	O
type	O
or	O
whether	O
there	O
is	O
discernible	O
variation	O
in	O
the	O
level	O
of	O
abuse	O
found	O
in	O
different	O
subsections	O
of	O
the	O
Internet	O
.	O
We	O
do	O
not	O
claim	O
to	O
have	O
the	O
final	O
answer	O
to	O
this	O
nebulous	O
question	O
,	O
but	O
instead	O
we	O
have	O
addressed	O
one	O
small	O
part	O
of	O
the	O
whole	O
:	O
is	O
the	O
level	O
of	O
abuse	O
found	O
in	O
one	O
Internet	O
domain	O
-	O
namely	O
discussions	O
about	O
English	O
Wikipedia	O
page	O
edits	O
-	O
similar	O
to	O
that	O
found	O
in	O
another	O
domain	O
,	O
that	O
of	O
an	O
online	O
hacking	O
forum	O
?	O
We	O
show	O
that	O
the	O
type	O
of	O
abusive	B-TaskName
language	I-TaskName
occurring	O
in	O
the	O
latter	O
is	O
more	O
closely	O
aligned	O
with	O
the	O
milder	O
levels	O
of	O
abuse	O
of	O
those	O
found	O
in	O
Wikipedia	O
discussions	O
,	O
and	O
consider	O
why	O
this	O
might	O
be	O
.	O
We	O
observe	O
that	O
the	O
online	O
hacking	O
forum	O
tends	O
to	O
contain	O
texts	O
aimed	O
at	O
helping	O
or	O
informing	O
other	O
users	O
,	O
whereas	O
the	O
Wikipedia	O
conversations	O
are	O
inherently	O
more	O
adversarial	O
since	O
they	O
relate	O
to	O
recent	O
page	O
edits	O
and	O
disputes	O
arising	O
.	O
Where	O
abusive	B-TaskName
language	I-TaskName
is	O
found	O
in	O
the	O
online	O
hacking	O
forum	O
,	O
it	O
tends	O
to	O
involve	O
profane	O
namecalling	O
,	O
insults	O
and	O
heated	O
disputes	O
,	O
rather	O
than	O
hate	B-DatasetName
speech	I-DatasetName
or	O
threats	O
of	O
violence	O
-	O
those	O
which	O
have	O
tended	O
to	O
be	O
the	O
more	O
prominent	O
causes	O
for	O
public	O
concern	O
.	O
Note	O
here	O
that	O
we	O
make	O
a	O
distinction	O
between	O
aggressive	O
and	O
offensive	O
language	O
:	O
the	O
former	O
often	O
involves	O
the	O
latter	O
,	O
but	O
not	O
always	O
so	O
.	O
Offensive	O
language	O
-	O
identifiable	O
word	O
tokens	O
such	O
as	O
swearwords	O
and	O
the	O
like	O
-	O
may	O
offend	O
but	O
is	O
not	O
always	O
used	O
aggressively	O
;	O
sometimes	O
it	O
is	O
used	O
in	O
a	O
jocular	O
fashion	O
,	O
for	O
example	O
.	O
Aggressive	O
language	O
,	O
which	O
more	O
often	O
than	O
not	O
is	O
built	O
on	O
the	O
composition	O
of	O
many	O
words	O
,	O
involves	O
a	O
hostile	O
stance	O
from	O
one	O
speaker	O
or	O
writer	O
to	O
another	O
.	O
It	O
is	O
this	O
which	O
might	O
seem	O
to	O
be	O
abusive	O
and	O
which	O
we	O
seek	O
to	O
automatically	O
detect	O
and	O
better	O
understand	O
.	O
We	O
also	O
distinguish	O
aggressive	O
language	O
from	O
hate	B-DatasetName
speech	I-DatasetName
-	O
that	O
which	O
might	O
be	O
characterised	O
as	O
prejudicial	O
diatribes	O
to	O
provoke	O
action	O
,	O
perhaps	O
violent	O
,	O
against	O
a	O
group	O
or	O
groups	O
-	O
and	O
from	O
cyberbullying	O
-	O
that	O
which	O
involves	O
a	O
sustained	O
period	O
of	O
persecution	O
against	O
an	O
individual	O
or	O
individuals	O
.	O
Certainly	O
the	O
distinctions	O
are	O
fuzzy	O
at	O
the	O
edges	O
,	O
but	O
these	O
might	O
be	O
thought	O
of	O
as	O
the	O
canonical	O
definitions	O
of	O
these	O
abuse	O
types	O
.	O
We	O
are	O
dealing	O
with	O
what	O
we	O
deem	O
to	O
be	O
one	O
-	O
off	O
instances	O
of	O
aggression	O
in	O
online	O
communities	O
,	O
though	O
if	O
these	O
were	O
shown	O
to	O
be	O
prejudicial	O
against	O
a	O
group	O
,	O
or	O
sustained	O
against	O
an	O
individual	O
,	O
then	O
the	O
instances	O
start	O
to	O
move	O
into	O
hate	B-DatasetName
speech	I-DatasetName
or	O
cyberbullying	O
behaviours	O
.	O
In	O
both	O
Wikipedia	O
edits	O
and	O
the	O
online	O
hacking	O
forum	O
,	O
abusive	O
comments	O
are	O
infrequent	O
in	O
the	O
community	O
as	O
a	O
whole	O
and	O
the	O
general	O
objective	O
of	O
gaining	O
reputation	O
in	O
the	O
domain	O
dis	O
-	O
incentivises	O
aggressive	O
behaviour	O
.	O
Nevertheless	O
we	O
show	O
that	O
aggressive	O
language	O
which	O
does	O
occur	O
may	O
be	O
detected	O
fairly	O
well	O
by	O
training	O
on	O
the	O
Wikipedia	O
edits	O
corpus	O
-	O
the	O
advantage	O
being	O
that	O
it	O
has	O
been	O
multiply	O
and	O
widely	O
annotated	O
-	O
and	O
setting	O
the	O
threshold	O
for	O
a	O
binary	O
aggression	O
classifier	O
at	O
a	O
fairly	O
moderate	O
level	O
relative	O
to	O
the	O
worst	O
types	O
of	O
abuse	O
found	O
in	O
Wikipedia	O
comments	O
.	O
Future	O
work	O
remains	O
to	O
be	O
done	O
to	O
more	O
broadly	O
characterise	O
intra	O
-	O
community	O
behaviour	O
in	O
different	O
subsections	O
of	O
the	O
Internet	O
.	O

Offensive	O
language	O
serves	O
many	O
purposes	O
in	O
everyday	O
discourse	O
:	O
from	O
deliberate	O
effect	O
in	O
humour	O
to	O
self	O
-	O
directed	O
profanity	O
to	O
toxic	O
or	O
abusive	O
intent	O
.	O
We	O
are	O
not	O
concerned	O
here	O
with	O
humorous	O
uses	O
of	O
offensive	O
language	O
or	O
with	O
general	O
profanity	O
.	O
Instead	O
we	O
are	O
interested	O
in	O
toxic	O
and	O
abusive	O
behaviour	O
,	O
specifically	O
online	O
harassment	O
involving	O
abusive	B-TaskName
language	I-TaskName
,	O
aggression	O
and	O
personal	O
attacks	O
.	O
There	O
has	O
been	O
work	O
on	O
other	O
forms	O
of	O
abusive	O
behaviour	O
,	O
such	O
as	O
hate	B-DatasetName
speech	I-DatasetName
(	O
Warner	O
and	O
Hirschberg	O
,	O
2012	O
;	O
Kwok	O
and	O
Wang	O
,	O
2013	O
;	O
Ribeiro	O
et	O
al	O
,	O
2018	O
)	O
and	O
cyberbullying	O
(	O
Xu	O
et	O
al	O
,	O
2013	O
;	O
Pieschl	O
et	O
al	O
,	O
2015	O
)	O
,	O
and	O
we	O
put	O
these	O
aside	O
for	O
now	O
as	O
challenging	O
,	O
distinct	O
topics	O
(	O
though	O
with	O
the	O
fuzzy	O
edges	O
described	O
above	O
)	O
.	O
In	O
terms	O
of	O
online	O
harassment	O
,	O
previous	O
work	O
has	O
centred	O
around	O
definitions	O
,	O
automatic	O
detection	O
,	O
and	B-DatasetName
dataset	I-DatasetName
creation	O
-	O
for	O
example	O
the	O
Hate	B-DatasetName
Speech	I-DatasetName
Twitter	O
Annotations	O
and	O
Wikipedia	O
Comments	O
Corpus	O
(	O
Waseem	O
and	O
Hovy	O
,	O
2016	O
;	O
Wulczyn	O
et	O
al	O
,	O
2017	O
)	O
.	O
Most	O
work	O
has	O
been	O
conducted	O
on	O
English	O
data	O
,	O
with	O
some	O
extensions	O
to	O
other	O
languages	O
(	O
e.g.	O
Arabic	O
(	O
Mubarak	O
et	O
al	O
,	O
2017	O
)	O
,	O
Slovene	O
(	O
Fišer	O
et	O
al	O
,	O
2017	O
)	O
)	O
.	O
Automated	O
detection	O
approaches	O
have	O
drawn	O
on	O
classic	O
document	B-TaskName
classification	I-TaskName
methods	O
for	O
spam	B-TaskName
detection	I-TaskName
and	O
sentiment	B-TaskName
analysis	I-TaskName
,	O
and	O
tend	O
to	O
use	O
lexical	O
and	O
syntactic	O
features	O
(	O
Nobata	O
et	O
al	O
,	O
2016	O
;	O
Li	O
et	O
al	O
,	O
2017	O
;	O
Bourgonje	O
et	O
al	O
,	O
2018	O
)	O
.	O
Machine	O
learning	O
techniques	O
range	O
from	O
logistic	B-MethodName
regression	I-MethodName
(	O
Cheng	O
et	O
al	O
,	O
2015	O
)	O
to	O
support	O
vector	O
machines	O
(	O
Yin	O
et	O
al	O
,	O
2009	O
)	O
to	O
neural	O
networks	O
(	O
Gambäck	O
and	O
Sikdar	O
,	O
2017	O
)	O
.	O
Our	O
aim	O
here	O
is	O
not	O
especially	O
to	O
push	O
the	O
boundaries	O
on	O
detection	O
techniques	O
-	O
though	O
naturally	O
we	O
wish	O
our	O
classifier	O
to	O
perform	O
fairly	O
well	O
-	O
but	O
rather	O
we	O
are	O
interested	O
in	O
how	O
to	O
make	O
use	O
of	O
existing	O
labelled	O
training	O
data	O
when	O
predicting	O
personal	O
attacks	O
in	O
other	O
corpora	O
.	O
In	O
case	O
any	O
persuasion	O
is	O
needed	O
that	O
improved	O
understanding	O
,	O
detection	O
and	O
action	O
on	O
abusive	B-TaskName
language	I-TaskName
are	O
desirable	O
,	O
there	O
is	O
evidence	O
that	O
experience	O
of	O
online	O
harassment	O
leads	O
to	O
decreased	O
online	O
participation	O
and	O
is	O
connected	O
with	O
oppression	O
,	O
violence	O
and	O
suicide	O
(	O
Dinakar	O
et	O
al	O
,	O
2011	O
;	O
Sood	O
et	O
al	O
,	O
2012	O
;	O
Wulczyn	O
et	O
al	O
,	O
2017	O
)	O
.	O
Of	O
course	O
there	O
may	O
be	O
reasons	O
to	O
be	O
concerned	O
about	O
the	O
perpetrator	O
's	O
wellbeing	O
along	O
with	O
that	O
of	O
the	O
victims	O
(	O
Cheng	O
et	O
al	O
,	O
2017	O
)	O
.	O

It	O
is	O
evident	O
from	O
our	O
classification	O
experiments	O
that	O
levels	O
of	O
linguistic	O
aggression	O
in	O
HackForums	O
tend	O
to	O
be	O
milder	O
than	O
those	O
in	O
WikiComments	O
,	O
if	O
we	O
take	O
the	O
optimal	O
value	O
of	O
t	O
to	O
lie	O
between	O
2	O
and	O
3	O
(	O
Table	O
3	O
)	O
whereas	O
for	O
WikiComments	O
it	O
was	O
found	O
to	O
be	O
4.25	O
(	O
Wulczyn	O
et	O
al	O
,	O
2017	O
)	O
.	O
A	O
possible	O
explanation	O
for	O
this	O
finding	O
may	O
be	O
the	O
difference	O
in	O
purposes	O
of	O
the	O
two	O
sources	O
for	O
our	O
test	O
and	O
training	O
data	O
:	O
discussion	O
of	O
Wikipedia	O
page	O
edits	O
often	O
end	O
up	O
as	O
arguments	O
between	O
contributors	O
.	O
The	O
fact	O
these	O
arguments	O
may	O
become	O
aggressive	O
or	O
personally	O
offensive	O
at	O
times	O
is	O
unsurprising	O
.	O
In	O
HackForums	O
,	O
where	O
our	O
test	O
data	O
came	O
from	O
,	O
users	O
often	O
have	O
the	O
intention	O
of	O
educating	O
others	O
,	O
learning	O
from	O
others	O
,	O
buying	O
and	O
selling	O
products	O
,	O
and	O
in	O
many	O
cases	O
discouraging	O
others	O
from	O
acting	O
illegally	O
online	O
(	O
those	O
with	O
a	O
so	O
-	O
called	O
'	O
white	O
hat	O
'	O
hacking	O
ethos	O
-	O
hackers	O
who	O
identify	O
security	O
vulnerabilities	O
and	O
report	O
them	O
rather	O
than	O
exploit	O
them	O
)	O
.	O
HackForums	O
is	O
not	O
an	O
oasis	O
of	O
calm	O
,	O
positive	O
behaviour	O
,	O
however	O
-	O
on	O
the	O
con	O
-	O
trary	O
,	O
users	O
can	O
often	O
be	O
off	O
-	O
hand	O
in	O
their	O
comments	O
,	O
dismissive	O
of	O
'	O
noobs	O
'	O
and	O
'	O
skids	O
'	O
(	O
script	O
kiddies	O
-	O
a	O
novice	O
or	O
tinkerer	O
)	O
,	O
sarcastic	O
and	O
rude	O
.	O
These	O
attitudes	O
,	O
where	O
they	O
do	O
not	O
cross	O
the	O
line	O
into	O
aggressive	O
behaviour	O
,	O
map	O
to	O
our	O
negative	O
label	O
for	O
author	O
intent	O
.	O
Debates	O
about	O
hacking	O
techniques	O
,	O
authorship	O
of	O
code	O
,	O
and	O
user	O
behaviour	O
(	O
e.g.	O
spam	O
,	O
posting	O
out	O
-	O
of	O
-	O
date	O
tutorials	O
,	O
offering	O
hacking	O
tools	O
which	O
do	O
n't	O
work	O
as	O
advertised	O
)	O
are	O
frequent	O
.	O
But	O
on	O
the	O
whole	O
,	O
the	O
forum	O
exists	O
for	O
information	O
and	O
technology	O
exchange	O
and	O
the	O
white	O
hat	O
hackers	O
,	O
along	O
with	O
active	O
administrators	O
and	O
a	O
reputation	O
scoring	O
system	O
,	O
help	O
to	O
constrain	O
user	O
behaviour	O
.	O
Indeed	O
this	O
highly	O
active	O
reputation	O
scoring	O
system	O
may	O
deter	O
aggressive	O
online	O
harassment	O
and	O
allow	O
for	O
users	O
to	O
engender	O
trust	O
in	O
what	O
could	O
otherwise	O
be	O
quite	O
untrustworthy	O
environments	O
(	O
Holt	O
et	O
al	O
,	O
2016	O
;	O
Décary	O
-	O
Hétu	O
and	O
Leppänen	O
,	O
2016	O
)	O
.	O
Furthermore	O
,	O
online	O
deviant	O
communities	O
such	O
as	O
these	O
tend	O
to	O
be	O
rather	O
homogeneous	O
,	O
particularly	O
involving	O
young	O
males	O
(	O
Hutchings	O
and	O
Chua	O
,	O
2017	O
)	O
.	O
Therefore	O
the	O
targets	O
for	O
any	O
harassment	O
may	O
be	O
off	O
,	O
rather	O
than	O
on	O
,	O
the	O
forum	O
.	O
Aside	O
from	O
aggression	O
,	O
we	O
also	O
labelled	O
positive	O
texts	O
(	O
which	O
answer	O
others	O
'	O
questions	O
,	O
contain	O
laughter	O
-	O
related	O
word	O
tokens	O
or	O
emoticons	O
,	O
or	O
praise	O
the	O
work	O
of	O
others	O
)	O
,	O
neutral	O
texts	O
,	O
and	O
negative	O
texts	O
(	O
including	O
users	O
stating	O
that	O
others	O
can	O
not	O
or	O
should	O
not	O
do	O
something	O
,	O
sarcasm	O
and	O
arguments	O
)	O
.	O
These	O
intent	O
types	O
are	O
the	O
majority	O
labels	O
in	O
our	O
4123	O
post	O
subset	O
,	O
with	O
1562	O
positive	O
,	O
2566	O
neutral	O
and	O
788	O
negative	O
occurrences	O
(	O
the	O
posts	O
could	O
be	O
multiply	O
labelled	O
,	O
hence	O
these	O
counts	O
sum	O
to	O
more	O
than	O
4123	O
)	O
.	O
Minority	O
labels	O
are	O
aggression	O
(	O
n=100	O
)	O
,	O
users	O
posting	O
to	O
moderate	O
discussion	O
(	O
n=119	O
)	O
,	O
and	O
requests	O
to	O
continue	O
discussion	O
in	O
private	O
messaging	O
(	O
n=238	O
)	O
.	O
We	O
further	O
subdivide	O
our	O
set	O
of	O
100	O
aggressive	O
forum	O
posts	O
into	O
seven	O
classes	O
:	O
simply	O
aggressive	O
,	O
personal	O
denigration	O
,	O
alludes	O
to	O
violence	O
,	O
refers	O
to	O
disability	O
,	O
features	O
misogyny	O
,	O
homophobia	O
,	O
racism	O
.	O
Personal	O
denigration	O
typically	O
involves	O
name	O
-	O
calling	O
-	O
dismissing	O
someone	O
as	O
an	O
idiot	O
or	O
moron	O
,	O
doubting	O
their	O
technical	O
skills	O
,	O
and	O
so	O
on	O
.	O
The	O
other	O
classes	O
indicate	O
that	O
the	O
author	O
of	O
the	O
post	O
alludes	O
to	O
violence	O
(	O
"	O
I	O
'll	O
cut	O
your	O
neck	O
"	O
)	O
,	O
disability	O
(	O
"	O
you	O
're	O
a	O
retard	O
"	O
)	O
,	O
misogyny	O
(	O
"	O
stop	O
bitching	O
"	O
)	O
,	O
homophobia	O
(	O
"	O
that	O
's	O
gay	O
"	O
)	O
,	O
and	O
racism	O
(	O
"	O
fucking	O
jew	O
"	O
)	O
.	O
Note	O
that	O
,	O
with	O
the	O
exception	O
of	O
'	O
simply	O
aggressive	O
'	O
which	O
tends	O
to	O
be	O
a	O
fallback	O
if	O
the	O
post	O
falls	O
into	O
no	O
other	O
class	O
,	O
the	O
posts	O
may	O
be	O
assigned	O
multiple	O
labels	O
and	O
that	O
a	O
single	O
annotator	O
undertook	O
labelling	O
.	O
Label	O
counts	O
are	O
shown	O
in	O
Table	O
4	O
.	O
We	O
find	O
that	O
most	O
aggressive	O
posts	O
are	O
just	O
that	O
-	O
simply	O
aggressive	O
manners	O
of	O
writing	O
which	O
would	O
be	O
out	O
of	O
place	O
in	O
polite	O
discourse	O
.	O
For	O
example	O
,	O
authors	O
add	O
emphasis	O
with	O
the	O
f	O
-	O
word	O
,	O
including	O
formulaic	O
phrases	O
in	O
acronym	O
form	O
(	O
'	O
gtfo	O
'	O
,	O
'	O
wtf	O
'	O
,	O
'	O
stfu	O
'	O
)	O
.	O
The	O
next	O
most	O
common	O
aggression	O
type	O
is	O
personal	O
denigration	O
:	O
most	O
often	O
calling	O
the	O
addressee	O
's	O
intelligence	O
into	O
question	O
,	O
or	O
doubting	O
their	O
motives	O
.	O
After	O
that	O
,	O
the	O
minority	O
labels	O
are	O
those	O
which	O
might	O
feature	O
in	O
hate	B-DatasetName
speech	I-DatasetName
:	O
discriminating	O
against	O
women	O
,	O
homosexuals	O
and	O
ethnicities	O
.	O
In	O
addition	O
,	O
the	O
'	O
refers	O
to	O
dis	O
-	O

The	O
ContrastMedium	O
Algorithm	O
:	O
Taxonomy	O
Induction	O
From	O
Noisy	O
Knowledge	B-TaskName
Graphs	I-TaskName
With	O
Just	O
a	O
Few	O
Links	O

In	O
this	O
paper	O
,	O
we	O
present	O
ContrastMedium	O
,	O
an	O
algorithm	O
that	O
transforms	O
noisy	O
semantic	O
networks	O
into	O
full	O
-	O
fledged	O
,	O
clean	O
taxonomies	O
.	O
ContrastMedium	O
is	O
able	O
to	O
identify	O
the	O
embedded	O
taxonomy	O
structure	O
from	O
a	O
noisy	O
knowledge	O
graph	O
without	O
explicit	O
human	O
supervision	O
such	O
as	O
,	O
for	O
instance	O
,	O
a	O
set	O
of	O
manually	O
selected	O
input	O
root	O
and	O
leaf	O
concepts	O
.	O
This	O
is	O
achieved	O
by	O
leveraging	O
structural	O
information	O
from	O
a	O
companion	O
reference	O
taxonomy	O
,	O
to	O
which	O
the	O
input	O
knowledge	O
graph	O
is	O
linked	O
(	O
either	O
automatically	O
or	O
manually	O
)	O
.	O
When	O
used	O
in	O
conjunction	O
with	O
methods	O
for	O
hypernym	O
acquisition	O
and	O
knowledge	O
base	O
linking	O
,	O
our	O
methodology	O
provides	O
a	O
complete	O
solution	O
for	O
end	O
-	O
to	O
-	O
end	O
taxonomy	O
induction	O
.	O
We	O
conduct	O
experiments	O
using	O
automatically	O
acquired	O
knowledge	B-TaskName
graphs	I-TaskName
,	O
as	O
well	O
as	O
a	O
SemEval	O
benchmark	O
,	O
and	O
show	O
that	O
our	O
method	O
is	O
able	O
to	O
achieve	O
high	O
performance	O
on	O
the	O
task	O
of	O
taxonomy	O
induction	O
.	O

Recent	O
years	O
have	O
witnessed	O
an	O
impressive	O
amount	O
of	O
work	O
on	O
automatic	O
construction	O
of	O
wide	O
-	O
coverage	O
knowledge	O
resources	O
.	O
Web	O
-	O
scale	O
open	B-TaskName
information	I-TaskName
extraction	I-TaskName
systems	O
like	O
NELL	B-DatasetName
(	O
Carlson	O
et	O
al	O
,	O
2010	O
)	O
or	O
ReVerb	B-DatasetName
have	O
been	O
successful	O
in	O
acquiring	O
massive	O
amounts	O
of	O
machine	O
-	O
readable	O
knowledge	O
by	O
effectively	O
tapping	O
large	O
amounts	O
of	O
text	O
from	O
Web	O
pages	O
.	O
However	O
,	O
the	O
output	O
of	O
these	O
systems	O
does	O
not	O
consist	O
of	O
a	O
clean	O
,	O
fully	O
-	O
semantified	O
output	O
.	O
Such	O
output	O
,	O
on	O
the	O
other	O
hand	O
,	O
could	O
be	O
provided	O
by	O
the	O
vocabulary	O
of	O
large	O
-	O
scale	O
ontologies	O
like	O
DBpedia	B-DatasetName
(	O
Bizer	O
et	O
al	O
,	O
2009	O
)	O
or	O
YAGO	B-DatasetName
(	O
Hoffart	O
et	O
al	O
,	O
2013	O
)	O
and	O
the	O
integration	O
of	O
open	O
and	O
closed	O
information	O
extraction	O
approaches	O
(	O
Dutta	O
et	O
al	O
,	O
2014	O
)	O
.	O
The	O
use	O
of	O
an	O
encyclopedia	O
-	O
centric	O
(	O
e.g.	O
,	O
Wikipedia	O
-	O
based	O
)	O
dictionary	O
of	O
entities	O
leads	O
to	O
poor	O
coverage	O
of	O
domain	O
-	O
specific	O
terminologies	O
.	O
This	O
can	O
be	O
alleviated	O
by	O
constructing	O
knowledge	O
bases	O
of	O
ever	O
increasing	O
coverage	O
and	O
complexity	O
from	O
the	O
Web	O
(	O
Wu	O
et	O
al	O
,	O
2012	O
;	O
Gupta	O
et	O
al	O
,	O
2014	O
;	O
Dong	O
et	O
al	O
,	O
2014	O
)	O
or	O
by	O
community	O
efforts	O
(	O
Bollacker	O
et	O
al	O
,	O
2008	O
)	O
.	O
However	O
,	O
the	O
focus	O
on	O
large	O
size	O
and	O
wide	O
coverage	O
at	O
entity	O
level	O
has	O
led	O
all	O
these	O
resources	O
to	O
avoid	O
the	O
complementary	O
problem	O
of	O
curating	O
and	O
maintaining	O
a	O
clean	O
taxonomic	O
backbone	O
with	O
as	O
minimal	O
supervision	O
as	O
possible	O
.	O
That	O
is	O
,	O
no	O
resource	O
,	O
to	O
date	O
,	O
integrates	O
structured	O
information	O
from	O
existing	O
wide	O
-	O
coverage	O
knowledge	B-TaskName
graphs	I-TaskName
with	O
empirical	O
evidence	O
from	O
text	O
for	O
the	O
explicit	O
goal	O
of	O
building	O
full	O
-	O
fledged	O
taxonomies	O
consisting	O
of	O
a	O
clean	O
and	O
fully	O
-	O
connected	O
directed	O
acyclic	O
graph	O
(	O
DAG	O
)	O
.	O
This	O
is	O
despite	O
the	O
fact	O
that	O
taxonomies	O
have	O
been	O
known	O
for	O
a	O
long	O
time	O
to	O
provide	O
valid	O
tools	O
to	O
represent	O
domain	O
-	O
specific	O
knowledge	O
with	O
dozens	O
of	O
scientific	O
,	O
industrial	O
and	O
social	O
applications	O
(	O
Glass	O
and	O
Vessey	O
,	O
1995	O
)	O
.	O
In	O
taxonomy	O
induction	O
,	O
the	O
required	O
domain	O
knowledge	O
can	O
be	O
acquired	O
with	O
many	O
different	O
methods	O
for	O
hypernym	O
extraction	O
,	O
ranging	O
from	O
simple	O
lexical	O
patterns	O
(	O
Hearst	O
,	O
1992	O
;	O
Oakes	O
,	O
2005	O
;	O
Kozareva	O
and	O
Hovy	O
,	O
2010	O
)	O
to	O
statistical	O
and	O
machine	O
learning	O
techniques	O
(	O
Caraballo	O
,	O
1999	O
;	O
Agirre	O
et	O
al	O
,	O
2000	O
;	O
Ritter	O
et	O
al	O
,	O
2009	O
;	O
Velardi	O
et	O
al	O
,	O
2013	O
)	O
.	O
Recent	O
efforts	O
,	O
such	O
as	O
Microsoft	O
's	O
Probase	O
(	O
Wu	O
et	O
al	O
,	O
2012	O
)	O
or	O
the	O
WebIsaDB	O
(	O
Seitner	O
et	O
al	O
,	O
2016	O
)	O
similarly	O
focus	O
on	O
'	O
local	O
'	O
extraction	O
of	O
single	O
hypernym	O
relations	O
,	O
and	O
do	O
not	O
address	O
the	O
problem	O
of	O
how	O
to	O
combine	O
these	O
single	O
relations	O
into	O
a	O
coherent	O
taxonomy	O
.	O
When	O
taxonomies	O
are	O
automatically	O
acquired	O
,	O
their	O
cleaning	O
(	O
also	O
called	O
"	O
pruning	O
"	O
)	O
becomes	O
a	O
mandatory	O
step	O
(	O
Velardi	O
et	O
al	O
,	O
2013	O
)	O
.	O
The	O
contributions	O
of	O
this	O
paper	O
are	O
two	O
-	O
fold	O
:	O
1	O
.	O
We	O
introduce	O
a	O
new	O
algorithm	O
,	O
named	O
Con	O
-	O
trastMedium	O
,	O
which	O
,	O
given	O
a	O
noisy	O
knowledge	O
graph	O
and	O
its	O
(	O
possibly	O
automatically	O
generated	O
)	O
links	O
to	O
a	O
companion	O
taxonomy	O
,	O
is	O
able	O
to	O
output	O
a	O
full	O
-	O
fledged	O
taxonomy	O
.	O
Information	O
from	O
the	O
reference	O
taxonomy	O
is	O
projected	O
onto	O
the	O
input	O
noisy	O
graph	O
to	O
automatically	O
acquire	O
topological	O
clues	O
,	O
which	O
are	O
then	O
used	O
to	O
drive	O
the	O
cleaning	O
process	O
.	O
The	O
reference	O
taxonomy	O
provides	O
us	O
with	O
ground	O
-	O
truth	O
taxonomic	O
relations	O
that	O
make	O
our	O
knowledge	O
-	O
based	O
method	O
not	O
truly	O
unsupervised	O
sensu	O
stricto	O
.	O
However	O
,	O
the	O
availability	O
of	O
resources	O
like	O
,	O
for	O
instance	O
,	O
WordNet	O
(	O
Fellbaum	O
,	O
1998	O
)	O
or	O
BabelNet	O
(	O
Navigli	O
and	O
Ponzetto	O
,	O
2012	O
)	O
implies	O
that	O
these	O
requirements	O
are	O
trivially	O
satisfied	O
;	O
2	O
.	O
We	O
combine	O
our	O
approach	O
with	O
an	O
unsupervised	O
framework	O
for	O
knowledge	O
acquisition	O
from	O
text	O
to	O
provide	O
a	O
full	O
end	O
-	O
to	O
-	O
end	O
pipeline	O
for	O
taxonomy	O
induction	O
from	O
scratch	O
.	O

At	O
its	O
core	O
,	O
our	O
algorithm	O
relies	O
on	O
the	O
notion	O
of	O
a	O
linked	O
noisy	O
knowledge	O
graph	O
(	O
LNKG	O
)	O
.	O
This	O
consists	O
of	O
a	O
quintuple	O
(	O
G	O
,	O
KB	O
,	O
KB	O
root	O
,	O
λ	O
,	O
M	O
)	O
where	O
:	O
i	O
)	O
G	O
=	O
(	O
V	O
G	O
,	O
E	O
G	O
)	O
is	O
a	O
noisy	O
knowledge	O
graph	O
;	O
ii	O
)	O
KB	O
=	O
(	O
V	O
KB	O
,	O
E	O
KB	O
)	O
is	O
a	O
companion	O
knowledge	O
base	O
providing	O
a	O
ground	O
-	O
truth	O
taxonomy	O
;	O
iii	O
)	O
KB	O
root	O
is	O
the	O
root	O
node	O
of	O
the	O
reference	O
knowledge	O
base	O
KB	O
(	O
if	O
several	O
top	O
-	O
level	O
nodes	O
exist	O
,	O
an	O
artificial	O
root	O
can	O
be	O
created	O
by	O
connecting	O
them	O
all	O
)	O
;	O
iv	O
)	O
λ	O
is	O
a	O
conventional	O
symbol	O
to	O
represent	O
the	O
"	O
undefined	O
concept	O
"	O
,	O
i.e.	O
,	O
a	O
place	O
-	O
holder	O
for	O
empty	O
mappings	O
;	O
v	O
)	O
M	O
:	O
V	O
G	O
V	O
KB	O
∪	O
{	O
λ	O
}	O
is	O
the	O
function	O
,	O
which	O
maps	O
nodes	O
of	O
V	O
G	O
into	O
nodes	O
of	O
V	O
KB	O
or	O
into	O
the	O
undefined	O
concept	O
λ	O
.	O
The	O
key	O
ideas	O
behind	O
ContrastMedium	O
are	O
:	O
Identification	O
of	O
important	O
topological	O
clues	O
from	O
the	O
companion	O
knowledge	O
base	O
KB	O
in	O
order	O
to	O
hierarchically	O
sort	O
the	O
concepts	O
in	O
G.	O
For	O
our	O
purposes	O
,	O
KB	O
is	O
expected	O
to	O
be	O
able	O
to	O
provide	O
ground	O
-	O
truth	O
taxonomic	O
relations	O
that	O
can	O
be	O
safely	O
projected	O
onto	O
G	O
to	O
guide	O
the	O
cleaning	O
process	O
:	O
that	O
is	O
,	O
we	O
assume	O
it	O
to	O
be	O
reasonably	O
clean	O
.	O
In	O
contrast	O
,	O
we	O
do	O
not	O
make	O
any	O
assumption	O
on	O
how	O
KB	O
has	O
been	O
created	O
:	O
our	O
approach	O
can	O
be	O
used	O
with	O
either	O
manually	O
created	O
taxonomies	O
like	O
WordNet	O
or	O
(	O
semi	O
-	O
)	O
automatically	O
induced	O
ones	O
,	O
provided	O
they	O
are	O
of	O
sufficient	O
quality	O
.	O
Hence	O
,	O
our	O
method	O
is	O
knowledge	O
-	O
based	O
without	O
the	O
need	O
of	O
further	O
supervision	O
other	O
than	O
that	O
contained	O
in	O
KB	O
;	O
Projection	O
of	O
topological	O
clues	O
from	O
KB	O
back	O
onto	O
the	O
LNKG	O
G	O
on	O
the	O
basis	O
of	O
the	O
links	O
found	O
in	O
the	O
mapping	O
M	O
.	O
Similarly	O
to	O
the	O
case	O
of	O
the	O
reference	O
knowledge	O
base	O
,	O
we	O
do	O
not	O
make	O
any	O
assumption	O
on	O
how	O
the	O
links	O
between	O
G	O
and	O
KB	O
have	O
been	O
created	O
:	O
while	O
there	O
exists	O
different	O
methods	O
to	O
automatically	O
link	O
(	O
lexical	O
)	O
knowledge	O
bases	O
(	O
Navigli	O
and	O
Ponzetto	O
,	O
2012	O
;	O
,	O
we	O
later	O
show	O
that	O
it	O
is	O
also	O
possible	O
to	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
with	O
a	O
few	O
manually	O
given	O
links	O
;	O
Propagation	O
of	O
the	O
topological	O
clues	O
across	O
the	O
entire	O
NKG	O
G.	O
That	O
is	O
,	O
to	O
cope	O
with	O
the	O
partial	O
coverage	O
of	O
automatic	O
mappings	O
,	O
as	O
well	O
as	O
the	O
need	O
to	O
reduce	O
the	O
number	O
of	O
manually	O
created	O
KB	O
links	O
,	O
we	O
apply	O
a	O
signal	O
propagation	O
technique	O
that	O
solely	O
relies	O
on	O
the	O
structure	O
of	O
G	O
;	O
To	O
make	O
use	O
of	O
the	O
resulting	O
topological	O
clues	O
to	O
drive	O
the	O
taxonomy	O
pruning	O
process	O
.	O
That	O
is	O
,	O
propagated	O
topological	O
clues	O
from	O
KB	O
are	O
additionally	O
leveraged	O
to	O
ensure	O
that	O
the	O
output	O
results	O
in	O
a	O
proper	O
taxonomic	O
structure	O
.	O
We	O
rely	O
on	O
the	O
metaphor	O
of	O
a	O
contrast	O
medium	O
(	O
CM	O
)	O
to	O
describe	O
our	O
approach	O
,	O
which	O
is	O
summarized	O
in	O
Figure	O
1	O
.	O
In	O
the	O
context	O
of	O
clinical	O
analysis	O
,	O
a	O
CM	O
is	O
injected	O
into	O
the	O
human	O
body	O
to	O
highlight	O
specific	O
complex	O
internal	O
body	O
structures	O
(	O
in	O
general	O
,	O
the	O
venuous	O
system	O
)	O
.	O
In	O
a	O
similar	O
fashion	O
,	O
we	O
detect	O
the	O
topological	O
structure	O
of	O
a	O
graph	O
by	O
propagating	O
a	O
certain	O
amount	O
of	O
CM	O
that	O
we	O
initially	O
inject	O
through	O
the	O
node	O
KB	O
root	O
of	O
the	O
companion	O
knowledge	O
base	O
KB	O
.	O
The	O
highlighted	O
structure	O
indicates	O
the	O
distance	O
of	O
a	O
node	O
with	O
respect	O
to	O
the	O
node	O
KB	O
root	O
.	O
Then	O
the	O
lowest	O
values	O
of	O
contrast	O
medium	O
indicate	O
the	O
leaf	O
terminological	O
nodes	O
.	O
The	O
observed	O
quantities	O
are	O
then	O
transferred	O
to	O
corresponding	O
nodes	O
of	O
the	O
noisy	O
graph	O
by	O
the	O
mapping	O
M	O
.	O
Next	O
,	O
the	O
medium	O
is	O
propagated	O
by	O
'	O
shaking	O
'	O
the	O
noisy	O
graph	O
.	O
We	O
let	O
the	O
fluid	O
reach	O
all	O
the	O
components	O
G	O
by	O
alternating	O
two	O
phases	O
of	O
propagation	O
:	O
letting	O
the	O
CM	O
to	O
flow	O
through	O
both	O
incoming	O
(	O
'	O
shake	O
up	O
'	O
)	O
;	O
and	O
outgoing	O
(	O
'	O
shake	O
down	O
'	O
)	O
edges	O
.	O
At	O
the	O
end	O
,	O
we	O
use	O
the	O
partial	O
order	O
induced	O
by	O
the	O
observed	O
node	O
level	O
of	O
CM	O
to	O
drive	O
the	O
pruning	O
phase	O
,	O
and	O
'	O
stretch	O
'	O
the	O
original	O
NKG	O
G	O
into	O
a	O
proper	O
DAG	O
.	O
Our	O
approach	O
is	O
presented	O
in	O
Algorithms	O
1	O
and	O
2	O
.	O
3	O
It	O
consists	O
of	O
the	O
following	O
main	O
steps	O
:	O
1	O
)	O
CM	O
injection	O
Cf	O
.	O
Figure	O
1	O
,	O
block	O
1	O
and	O
Algorithm	O
1	O
,	O
lines	O
1	O
-	O
2	O
.	O
We	O
initially	O
define	O
the	O
function	O
C	O
KB	O
:	O
V	O
KB	O
[	O
0.0	O
−	O
1.0	O
]	O
and	O
assign	O
a	O
zero	O
contrast	O
medium	O
level	O
to	O
all	O
the	O
nodes	O
of	O
the	O
KB	O
graph	O
C	O
KB	O
(	O
x	O
)	O
=	O
0	B-DatasetName
,	O
x	O
V	O
KB	O
(	O
line	O
1	O
)	O
.	O
Next	O
,	O
we	O
call	O
the	O
routine	O
'	O
injectContrastMedium	O
'	O
which	O
:	O
1	O
)	O
assigns	O
an	O
initial	O
contrast	O
level	O
equals	O
to	O
1.0	O
to	O
the	O
node	O
KB	O
root	O
of	O
the	O
KB	O
graph	O
;	O
ii	O
)	O
uses	O
the	O
routine	O
"	O
Shake	O
"	O
with	O
the	O
direction	O
parameter	O
equals	O
to	O
"	O
DOWN	O
"	O
(	O
see	O
Algorithm	O
2	O
and	O
Step	O
3	O
"	O
Graph	O
shaking	O
"	O
for	O
more	O
details	O
)	O
to	O
let	O
the	O
CM	O
drop	O
through	O
KB	O
.	O
In	O
practice	O
,	O
the	O
shaking	O
routine	O
implements	O
a	O
node	O
contrast	O
medium	O
level	O
propagation	O
algorithm	O
following	O
the	O
outgoing	O
(	O
'	O
down	O
'	O
)	O
or	O
the	O
incoming	O
(	O
'	O
up	O
'	O
)	O
edges	O
of	O
the	O
graph	O
.	O
2	O
)	O
CM	O
transfer	O
Cf	O
.	O
Figure	O
1	O
,	O
block	O
2	O
and	O
Algorithm	O
1	O
,	O
lines	O
3	O
-	O
5	O
.	O
In	O
the	O
next	O
phase	O
,	O
we	O
first	O
extract	O
the	O
hypernymy	O
subgraph	O
T	O
=	O
(	O
V	O
T	O
,	O
E	O
T	O
)	O
of	O
G	O
(	O
see	O
Section	O
3.1	O
)	O
and	O
then	O
follow	O
the	O
links	O
in	O
the	O
mapping	O
M	O
to	O
transfer	O
the	O
contrast	O
medium	O
levels	O
,	O
i.e.	O
,	O
C	O
T	O
(	O
y	O
)	O
=	O
C	O
KB	O
(	O
x	O
)	O
(	O
s.t	O
.	O
x	O
V	O
KB	O
,	O
y	O
V	O
T	O
,	O
(	O
y	O
x	O
)	O
M	O
)	O
.	O
3	O
)	O
Graph	O
shaking	O
Cf	O
.	O
Figure	O
1	O
,	O
block	O
3	O
and	O
Algorithm	O
1	O
,	O
lines	O
6	O
-	O
8	O
.	O
After	O
having	O
transferred	O
the	O
CM	O
to	O
the	O
target	O
hypernym	O
graph	O
T	O
of	O
G	O
,	O
we	O
shake	O
T	O
to	O
let	O
the	O
CM	O
flow	O
by	O
traversing	O
the	O
incoming	O
,	O
the	O
outgoing	O
,	O
and	O
finally	O
the	O
incoming	O
edges	O
again	O
-	O
see	O
Algorithm	O
2	O
for	O
details	O
on	O
the	O
'	O
Shake	O
'	O
routine	O
.	O
Note	O
that	O
these	O
two	O
kinds	O
of	O
propagation	O
are	O
needed	O
since	O
the	O
CM	O
needs	O
to	O
be	O
propagated	O
through	O
all	O
the	O
nodes	O
of	O
the	O
graph	O
to	O
highlight	O
the	O
topological	O
clues	O
we	O
are	O
searching	O
for	O
.	O
In	O
particular	O
,	O
in	O
Algorithm	O
2	O
at	O
each	O
iteration	O
t	O
for	O
each	O
node	O
x	O
V	O
graph	O
,	O
depending	O
on	O
the	O
value	O
of	O
the	O
parameter	O
direction	O
(	O
line	O
8	O
and	O
line	O
12	O
)	O
:	O
i	O
)	O
we	O
observe	O
a	O
CM	O
level	O
for	O
the	O
node	O
x	O
(	O
line	O
7	O
)	O
;	O
ii	O
)	O
if	O
direction	O
=	O
=	O
DOWN	O
(	O
lines	O
9	O
-	O
11	O
)	O
we	O
traverse	O
all	O
the	O
outgoing	O
edges	O
(	O
x	O
,	O
y	O
)	O
of	O
x	O
and	O
propagate	O
the	O
observed	O
CM	O
level	O
of	O
x	O
,	O
otherwise	O
(	O
direction	O
=	O
=	O
UP	O
,	O
lines	O
13	O
-	O
15	O
)	O
we	O
traverse	O
the	O
incoming	O
edges	O
(	O
y	O
,	O
x	O
)	O
and	O
propagate	O
the	O
CM	O
level	O
to	O
the	O
nodes	O
y	O
;	O
iii	O
)	O
the	O
value	O
of	O
F	O
lown	O
graph	O
(	O
x	O
)	O
is	O
incremented	O
by	O
the	O
observed	O
CM	O
level	O
(	O
line	O
16	O
)	O
;	O
iv	O
)	O
for	O
each	O
node	O
x	O
we	O
reset	O
the	O
current	O
observed	O
value	O
of	O
the	O
CM	O
level	O
with	O
the	O
portion	O
of	O
the	O
liquid	O
which	O
has	O
flown	O
from	O
the	O
incoming	O
or	O
the	O
outgoing	O
edges	O
during	O
the	O
propagation	O
(	O
lines	O
17	O
-	O
18	O
)	O
.	O
Depending	O
on	O
the	O
propagation	O
direction	O
,	O
we	O
have	O
two	O
different	O
behaviours	O
for	O
the	O
CM	O
.	O
When	O
exiting	O
a	O
node	O
x	O
through	O
out	O
the	O
outgoing	O
edges	O
(	O
direction	O
=	O
=	O
DOWN	O
)	O
we	O
increment	O
the	O
level	O
of	O
contrast	O
medium	O
of	O
the	O
reached	O
nodes	O
by	O
the	O
observed	O
value	O
of	O
x	O
divided	O
by	O
number	O
of	O
outgoing	O
edges	O
of	O
x.	O
By	O
converse	B-DatasetName
,	O
when	O
we	O
climb	O
(	O
direction	O
=	O
=	O
UP	O
)	O
across	O
the	O
incoming	O
edges	O
of	O
a	O
node	O
x	O
we	O
increment	O
the	O
CM	O
level	O
of	O
the	O
reached	O
node	O
by	O
the	O
observed	O
CM	O
quantity	O
of	O
x	O
divided	O
by	O
the	O
number	O
of	O
incoming	O
edges	O
of	O
x.	O
Note	O
that	O
the	O
sequence	O
UP	O
/	O
DOWN	O
/	O
UP	O
and	O
the	O
specular	O
DOWN	O
/	O
UP	O
/	O
DOWN	O
are	O
the	O
only	O
ones	O
from	O
the	O
8	O
possible	O
combinations	O
which	O
can	O
guarantee	O
the	O
contrast	O
medium	O
to	O
flow	O
on	O
the	O
entire	O
graph	O
.	O
We	O
simply	O
selected	O
the	O
first	O
sequence	O
since	O
the	O
final	O
rank	O
places	O
candidate	O
root	O
nodes	O
on	O
the	O
top	O
(	O
and	O
candidate	O
leaf	O
nodes	O
on	O
the	O
bottom	O
)	O
.	O
4	O
)	O
Pruning	O
Cf	O
.	O
Figure	O
1	O
,	O
block	O
4	O
and	O
Algorithm	O
1	O
,	O
lines	O
9	O
.	O
Finally	O
,	O
we	O
create	O
a	O
clean	O
taxonomy	O
T	O
by	O
pruning	O
the	O
graph	O
T	O
on	O
the	O
basis	O
of	O
the	O
contrast	O
levels	O
found	O
in	O
C	O
T	O
.	O
CM	O
levels	O
in	O
C	O
T	O
can	O
be	O
used	O
to	O
induce	O
a	O
order	O
of	O
the	O
nodes	O
that	O
,	O
intuitively	O
,	O
captures	O
the	O
level	O
of	O
conceptual	O
abstraction	O
for	O
the	O
nodes	O
in	O
T	O
.	O
We	O
use	O
them	O
to	O
produce	O
a	O
clean	O
taxonomy	O
as	O
follows	O
.	O
We	O
first	O
sort	O
the	O
nodes	O
v	O
V	O
T	O
in	O
a	O
list	O
S	O
=	O
s	O
0	B-DatasetName
,	O
s	O
1	O
,	O
.	O
.	O
.	O
,	O
s	O
|	O
V	O
T	O
|	O
−1	O
by	O
the	O
decreasing	O
resulting	O
CM	O
level	O
value	O
in	O
C	O
T	O
.	O
The	O
nodes	O
with	O
a	O
higher	O
level	O
of	O
contrast	O
medium	O
are	O
candidates	O
to	O
be	O
at	O
the	O
top	O
level	O
while	O
the	O
ones	O
at	O
the	O
end	O
of	O
the	O
list	O
are	O
candidates	O
to	O
be	O
leaf	O
nodes	O
of	O
the	O
output	O
taxonomy	O
.	O
Next	O
,	O
the	O
pruning	O
routine	O
starts	O
from	O
a	O
graph	O
T	O
=	O
(	O
V	O
T	O
=	O
V	O
T	O
,	O
E	O
T	O
=	O
)	O
and	O
for	O
each	O
node	O
s	O
S	O
(	O
from	O
the	O
last	O
node	O
to	O
the	O
first	O
)	O
add	O
to	O
E	O
T	O
all	O
the	O
edges	O
of	O
the	O
kind	O
e	O
=	O
(	O
y	O
,	O
s	O
)	O
where	O
a	O
path	O
from	O
y	O
to	O
s	O
does	O
not	O
exists	O
in	O
T	O
and	O
with	O
y	O
belonging	O
to	O
one	O
of	O
the	O
following	O
:	O
i	O
)	O
the	O
set	O
of	O
peers	O
{	O
x	O
S	O
s.	O

We	O
perform	O
two	O
sets	O
of	O
experiments	O
.	O
We	O
first	O
evaluate	O
our	O
approach	O
when	O
applied	O
to	O
large	O
,	O
automatically	O
induced	O
noisy	O
knowledge	B-TaskName
graphs	I-TaskName
(	O
Section	O
4.1	O
)	O
and	O
then	O
quantify	O
the	O
impact	O
it	O
can	O
have	O
to	O
further	O
improve	O
the	O
quality	O
of	O
the	O
output	O
of	O
state	O
-	O
ofthe	O
-	O
art	O
taxonomy	O
induction	O
systems	O
(	O
Section	O
4.2	O
)	O
.	O

We	O
first	O
apply	O
ContrastMedium	O
to	O
a	O
variety	O
of	O
knowledge	B-TaskName
graphs	I-TaskName
that	O
have	O
been	O
automatically	O
acquired	O
and	O
linked	O
to	O
reference	O
KBs	O
like	O
Word	O
-	O
Net	O
and	O
BabelNet	O
using	O
unsupervised	O
methods	O
(	O
Section	O
3.2	O
)	O
.	O
Our	O
research	O
questions	O
(	O
RQs	O
)	O
are	O
:	O
RQ2	O
What	O
is	O
the	O
quality	O
of	O
the	O
resulting	O
taxonomies	O
?	O

We	O
apply	O
our	O
pruning	O
algorithm	O
to	O
the	O
automatically	O
acquired	O
KBs	O
presented	O
by	O
.	O
These	O
noisy	O
knowledge	B-TaskName
graphs	I-TaskName
have	O
been	O
induced	O
from	O
large	O
text	O
corpora	O
and	O
include	O
both	O
taxonomic	O
and	O
other	O
(	O
i.e.	O
,	O
related	O
,	O
topically	O
associative	O
)	O
semantic	O
relations	O
(	O
cf	O
.	O
Table	O
1	O
)	O
,	O
as	O
well	O
as	O
automatically	O
induced	O
mappings	O
to	O
lexical	O
knowledge	O
bases	O
like	O
WordNet	O
and	O
BabelNet	O
.	O
These	O
NKGs	O
have	O
been	O
induced	O
from	O
a	O
100	O
million	O
sentence	O
news	O
corpus	O
(	O
news	O
)	O
and	O
from	O
a	O
35	O
million	O
sentence	O
Wikipedia	O
corpus	O
(	O
wiki	O
)	O
,	O
using	O
different	O
parameter	O
values	O
to	O
generate	O
sense	O
inventories	O
of	O
different	O
granularities	O
(	O
e.g.	O
,	O
1.8	O
vs.	O
6.0	O
average	O
senses	O
per	O
term	O
for	O
the	O
wiki	O
-	O
p1.8	O
and	O
wiki	O
-	O
p6.0	O
datasets	O
,	O
respectively	O
)	O
.	O
Table	O
2	O
:	O
Dimensions	O
of	O
the	O
four	O
datasets	O
adopted	O
as	O
linked	O
noisy	O
knowledge	B-TaskName
graphs	I-TaskName
.	O
the	O
dimensions	O
for	O
each	O
of	O
the	O
four	O
NKGs	O
-	O
number	O
of	O
senses	O
,	O
average	O
and	O
maximum	O
sense	O
polysemy	O
,	O
number	O
and	O
average	O
hypernyms	O
per	O
sense	O
,	O
the	O
number	O
of	O
linked	O
senses	O
to	O
WordNet	O
concepts	O
(	O
i.e.	O
,	O
"	O
links	O
"	O
)	O
,	O
and	O
the	O
number	O
of	O
nodes	O
and	O
edges	O
for	O
the	O
corresponding	O
hypernymy	O
graph	O
.	O
Since	O
our	O
algorithm	O
primarily	O
focuses	O
on	O
conceptual	O
hierarchical	O
(	O
taxonomic	O
)	O
structures	O
-	O
referred	O
to	O
as	O
the	O
TBox	O
in	O
Knowledge	O
Representationwe	O
use	O
the	O
WordNet	O
mappings	O
only	O
,	O
since	O
the	O
manual	O
inspection	O
of	O
the	O
BabelNet	O
mappings	O
revealed	O
that	O
they	O
are	O
focused	O
primarily	O
on	O
instances	O
(	O
that	O
is	O
,	O
ABox	O
statements	O
)	O
.	O
In	O
order	O
to	O
have	O
a	O
complete	O
quintuple	O
for	O
each	O
NKG	O
,	O
we	O
selected	O
,	O
for	O
the	O
companion	O
KB	O
,	O
the	O
top	O
KB	O
root	O
concept	O
entity	O
of	O
the	O
WordNet	O
taxonomy	O
(	O
SynsetID	O
SID	B-DatasetName
-	O
00001740	O
-	O
N	O
)	O
.	O

We	O
use	O
the	B-DatasetName
benchmark	I-DatasetName
data	O
from	O
the	O
SemEval	O
-	O
15	O
task	O
17	O
"	O
Taxonomy	O
Extraction	O
Evaluation	O
:	O
TExEval	O
"	O
(	O
Bordea	O
et	O
al	O
,	O
2015	O
)	O
,	O
since	O
it	O
provides	O
us	O
with	O
gold	O
-	O
standard	O
datasets	O
and	O
system	O
outputs	O
within	O
a	O
standard	O
,	O
easy	O
-	O
to	O
-	O
reproduce	O
setting	O
.	O
Initially	O
,	O
we	O
select	O
from	O
the	O
participating	O
systems	O
4	O
the	O
two	O
best	O
performing	O
taxonomies	O
based	O
on	O
the	O
Cumula	O
-	O
tive	O
Fowlkes&Mallows	O
(	O
CF&M	O
)	O
measure	O
(	O
Velardi	O
et	O
al	O
,	O
2012	O
)	O
,	O
the	O
Equipments	O
and	O
Sciences	O
taxonomies	O
from	O
the	O
INRIASAC	O
and	O
the	O
LT3	O
teams	O
respectively	O
.	O
We	O
next	O
apply	O
our	O
approach	O
to	O
these	O
taxonomies	O
,	O
in	O
order	O
to	O
clean	O
them	O
in	O
a	O
postprocessing	O
fashion	O
.	O
By	O
selecting	O
the	O
top	O
-	O
systems	O
we	O
can	O
see	O
how	O
far	O
we	O
can	O
advance	O
the	O
state	O
-	O
ofthe	O
-	O
art	O
overall	O
.	O
Besides	O
,	O
these	O
two	O
taxonomies	O
are	O
also	O
the	O
ones	O
containing	O
the	O
highest	O
number	O
of	O
cycles	O
,	O
giving	O
the	O
application	O
of	O
our	O
cleaning	O
algorithm	O
a	O
more	O
challenging	O
(	O
and	O
meaningful	O
)	O
setting	O
.	O
To	O
remove	O
the	O
effects	O
of	O
automatic	O
linking	O
and	O
quantify	O
the	O
amount	O
of	O
manual	O
efforts	O
needed	O
by	O
our	O
approach	O
,	O
10	O
random	O
concepts	O
from	O
each	O
of	O
these	O
resources	O
are	O
manually	O
linked	O
to	O
Word	O
-	O
Net	O
,	O
and	O
the	O
taxonomies	O
subsequently	O
pruned	O
using	O
ContrastMedium	O
and	O
the	O
baseline	O
.	O
We	O
then	O
evaluate	O
performance	O
following	O
the	O
task	O
's	O
experimental	O
setting	O
and	O
compute	O
the	O
CF&M	O
measure	O
for	O
different	O
levels	O
of	O
manually	O
-	O
created	O
KB	O
links	O
.	O

In	O
this	O
paper	O
,	O
we	O
presented	O
ContrastMedium	O
,	O
a	O
novel	O
algorithm	O
that	O
can	O
be	O
applied	O
to	O
automatically	O
linked	O
noisy	O
knowledge	B-TaskName
graphs	I-TaskName
to	O
provide	O
an	O
end	O
-	O
to	O
-	O
end	O
solution	O
for	O
fully	O
unsupervised	O
taxonomy	O
induction	O
from	O
scratch	O
,	O
i.e.	O
,	O
without	O
any	O
human	O
effort	O
.	O
Our	O
results	O
indicate	O
that	O
Con	O
-	O
trastMedium	O
can	O
be	O
successfully	O
applied	O
to	O
a	O
wide	O
range	O
of	O
automatically	O
acquired	O
KBs	O
,	O
ranging	O
from	O
large	O
linked	O
noisy	O
knowledge	B-TaskName
graphs	I-TaskName
all	O
the	O
way	O
to	O
small	O
-	O
scale	O
induced	O
taxonomies	O
to	O
produce	O
high	O
-	O
quality	O
isa	O
hierarchies	O
that	O
achieve	O
state	O
-	O
ofthe	O
-	O
art	O
results	O
on	O
SemEval	O
benchmarks	O
.	O
As	O
future	O
work	O
,	O
we	O
plan	O
to	O
improve	O
the	O
scalability	O
of	O
the	O
algorithm	O
,	O
in	O
particular	O
its	O
time	O
complexity	O
order	O
,	O
and	O
apply	O
it	O
to	O
Web	O
-	O
scale	O
resources	O
like	O
the	O
WebIsaDB	O
(	O
Seitner	O
et	O
al	O
,	O
2016	O
)	O
or	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
approaches	O
like	O
TAXI	O
,	O
as	O
well	O
as	O
to	O
publicly	O
release	O
the	O
created	O
resources	O
.	O

Recently	O
,	O
some	O
researchers	O
have	O
tackled	O
argumentation	O
synthesis	O
statistically	O
with	O
neural	O
networks	O
.	O
For	O
instance	O
,	O
Wang	O
and	O
Ling	O
(	O
2016	O
)	O
employed	O
a	O
sequence	O
-	O
to	O
-	O
sequence	O
model	O
to	O
generate	O
summaries	O
of	O
argumentative	O
texts	O
,	O
and	O
Hua	O
and	O
Wang	O
(	O
2018	O
)	O
did	O
similar	O
to	O
generate	O
counterarguments	O
.	O
Using	O
neural	O
methods	O
in	O
text	B-TaskName
generation	I-TaskName
,	O
it	O
is	O
possible	O
to	O
achieve	O
output	O
that	O
is	O
on	O
topic	O
and	O
grammatically	O
(	O
more	O
or	O
less	O
)	O
correct	O
.	O
However	O
,	O
when	O
the	O
desired	O
text	O
is	O
to	O
span	O
multiple	O
sentences	O
,	O
the	O
generated	O
text	O
regularly	O
suffers	O
from	O
incoherence	O
and	O
repetitiveness	O
,	O
as	O
for	O
instance	O
discussed	O
by	O
Holtzman	O
et	O
al	O
(	O
2018	O
)	O
who	O
examine	O
texts	O
that	O
were	O
produced	O
by	O
RNNs	O
in	O
various	O
domains	O
.	O
While	O
these	O
problems	O
may	O
be	O
tolerable	O
to	O
some	O
extent	O
in	O
some	O
applications	O
,	O
such	O
as	O
chatbots	O
,	O
bad	O
text	O
can	O
not	O
be	O
accepted	O
in	O
an	O
argumentative	O
or	O
debating	O
scenario	O
,	O
where	O
the	O
goal	O
is	O
to	O
convince	O
or	O
persuade	O
a	O
reader	O
(	O
rather	O
than	O
to	O
merely	O
inform	O
or	O
entertain	O
)	O
.	O
Holtzman	O
et	O
al	O
(	O
2018	O
)	O
propose	O
to	O
alleviate	O
incoherence	O
and	O
repetitiveness	O
by	O
training	O
a	O
set	O
of	O
discriminators	O
,	O
which	O
aim	O
to	O
ensure	O
that	O
a	O
text	O
respects	O
the	O
Gricean	O
maxims	O
of	O
quantity	O
,	O
quality	O
,	O
relation	O
,	O
and	O
manner	O
(	O
Grice	O
,	O
1975	O
)	O
.	O
To	O
this	O
end	O
,	O
they	O
employ	O
specific	O
datasets	O
,	O
such	O
as	O
one	O
that	O
opposes	O
authentic	O
text	O
continuation	O
to	O
randomly	O
-	O
sampled	O
text	O
.	O
The	O
discriminators	O
learn	O
optimal	O
weightings	O
for	O
the	O
various	O
models	O
and	O
their	O
combination	O
,	O
such	O
that	O
overall	O
text	O
quality	O
is	O
maximized	O
.	O
For	O
argumentation	O
,	O
we	O
hypothesize	O
that	O
one	O
needs	O
to	O
go	O
even	O
further	O
and	O
eventually	O
account	O
for	O
the	O
author	O
,	O
implementing	O
her	O
underlying	O
intention	O
in	O
the	O
different	O
parts	O
of	O
an	O
argumentative	O
text	O
as	O
well	O
as	O
in	O
the	O
relations	O
between	O
the	O
parts	O
.	O
In	O
the	O
past	O
times	O
of	O
rule	O
-	O
based	O
text	B-TaskName
generation	I-TaskName
,	O
argumentation	O
synthesis	O
was	O
a	O
popular	O
task	O
(	O
Zukerman	O
et	O
al	O
,	O
2000	O
)	O
.	O
Approaches	O
involved	O
much	O
handcrafted	O
(	O
linguistic	O
and	O
domain	O
)	O
knowledge	O
and	O
user	O
modeling	O
.	O
For	O
example	O
,	O
the	O
system	O
of	O
Carenini	O
and	O
Moore	O
(	O
2006	O
)	O
compares	O
attributes	O
of	O
houses	O
(	O
from	O
a	O
database	O
)	O
to	O
desired	O
target	O
attributes	O
(	O
from	O
a	O
user	O
model	O
)	O
,	O
to	O
then	O
recommend	O
a	O
house	O
to	O
the	O
reader	O
in	O
a	O
convincing	O
text	O
following	O
the	O
Gricean	O
maxims	O
.	O
To	O
this	O
end	O
,	O
it	O
selected	O
house	O
attributes	O
potentially	O
interesting	O
to	O
the	O
user	O
,	O
arranged	O
,	O
and	O
finally	O
phrased	O
them	O
.	O
The	O
resulting	O
texts	O
resembled	O
the	O
arguments	O
we	O
work	O
with	O
here	O
,	O
which	O
have	O
been	O
manually	O
composed	O
by	O
experts	O
(	O
Wachsmuth	O
et	O
al	O
,	O
2018	O
)	O
from	O
the	O
claims	O
,	O
evidence	O
,	O
and	O
objections	O
in	O
the	O
arg	O
-	O
microtext	O
corpus	O
(	O
Peldszus	O
and	O
Stede	O
,	O
2016	O
)	O
.	O
To	O
achieve	O
a	O
similar	O
level	O
of	O
output	O
control	O
,	O
today	O
's	O
text	O
-	O
to	O
-	O
text	B-TaskName
generation	I-TaskName
models	O
need	O
to	O
account	O
for	O
the	O
various	O
interdependencies	O
between	O
the	O
text	O
units	O
to	O
be	O
combined	O
.	O
Most	O
related	O
to	O
our	O
approach	O
is	O
the	O
system	O
of	O
,	O
where	O
a	O
user	O
can	O
enter	O
a	O
claimlike	O
topic	O
along	O
with	O
a	O
stance	O
.	O
The	O
system	O
then	O
generates	O
argumentative	O
paragraphs	O
on	O
specific	O
aspects	O
of	O
the	O
topic	O
by	O
selecting	O
sentences	O
from	O
10	O
million	O
news	O
texts	O
of	O
the	O
Gigaword	O
corpus	O
.	O
Potentially	O
relevant	O
aspects	O
are	O
those	O
that	O
trigger	O
evaluative	O
judgment	O
in	O
the	O
reader	O
.	O
The	O
sentences	O
are	O
arranged	O
so	O
that	O
the	O
text	O
starts	O
with	O
a	O
claim	O
sentence	O
and	O
is	O
followed	O
by	O
support	O
sentences	O
,	O
employing	O
the	O
approach	O
of	O
.	O
The	O
support	O
sentences	O
are	O
ordered	O
by	O
maximizing	O
the	O
semantic	O
connectivity	O
between	O
sentences	O
.	O
Finally	O
,	O
some	O
rephrasing	O
is	O
done	O
in	O
terms	O
of	O
certain	O
aspects	O
of	O
surface	O
realization	O
.	O
In	O
a	O
manual	O
evaluation	O
,	O
however	O
,	O
no	O
text	O
was	O
seen	O
as	O
sounding	O
natural	O
,	O
underlining	O
the	O
difficulty	O
of	O
the	O
task	O
.	O
In	O
contrast	O
to	O
,	O
we	O
learn	O
directly	O
from	O
input	O
data	O
what	O
argumentative	O
discourse	O
units	O
to	O
combine	O
and	O
how	O
to	O
arrange	O
them	O
.	O
We	O
leave	O
surface	O
realization	O
aside	O
to	O
keep	O
the	O
focus	O
on	O
the	O
argument	O
composition	O
.	O

Thesis	O
t1	O
German	O
universities	O
should	O
on	O
no	O
account	O
charge	O
tuition	O
fees	O
t2	O
the	O
universities	O
in	O
Germany	O
should	O
not	O
under	O
any	O
circumstances	O
charge	O
tuition	O
fees	O
t3	O
tuition	O
fees	O
should	O
not	O
generally	O
be	O
charged	O
by	O
universities	O
t4	O
universities	O
should	O
not	O
charge	O
tuition	O
fees	O
in	O
Germany	O
Con	O
c1	O
one	O
could	O
argue	O
that	O
an	O
increase	O
in	O
tuition	O
fees	O
would	O
allow	O
institutions	O
to	O
be	O
better	O
equipped	O
c2	O
those	O
who	O
study	O
later	O
decide	O
this	O
early	O
on	O
,	O
anyway	O
c3	B-DatasetName
to	O
oblige	O
non	O
-	O
academics	O
to	O
finance	O
others	O
'	O
degrees	O
through	O
taxes	O
is	O
not	O
just	O
c4	B-DatasetName
unfortunately	O
sponsoring	O
can	O
lead	O
to	O
disagreeable	O
dependencies	O
in	O
some	O
cases	O
Pro	O
p1	O
education	O
and	O
training	O
are	O
fundamental	O
rights	O
which	O
the	O
state	O
,	O
the	O
society	O
must	O
provide	O
p2	O
education	O
must	O
not	O
be	O
a	O
question	O
of	O
money	O
in	O
a	O
wealthy	O
society	O
such	O
as	O
Germany	O
p3	B-DatasetName
fees	O
result	O
in	O
longer	O
durations	O
of	O
studies	O
p4	O
funding	O
-	O
wise	O
it	O
ought	O
to	O
be	O
considered	O
how	O
costs	O
incurred	O
by	O
students	O
from	O
other	O
(	O
federal	O
)	O
states	O
can	O
be	O
reimbursed	O
p5	O
if	O
a	O
university	O
lacks	O
the	O
funds	O
,	O
sponsors	O
must	O
be	O
found	O
p6	O
longer	O
durations	O
of	O
studies	O
are	O
costly	O
p7	O
studying	O
and	O
taking	O
higher	O
degrees	O
must	O
remain	O
a	O
basic	O
right	O
for	O
everyone	O
p8	O
there	O
are	O
other	O
instruments	O
to	O
motivate	O
tighter	O
discipline	O
while	O
studying	O
p9	O
this	O
would	O
impede	O
or	O
prevent	O
access	O
to	O
those	O
who	O
are	O
financially	O
weaker	O
p10	O
this	O
would	O
mean	O
that	O
only	O
those	O
people	O
with	O
wealthy	O
parents	O
or	O
a	O
previous	O
education	O
and	O
a	O
part	O
-	O
time	O
job	O
while	O
studying	O
would	O
be	O
able	O
to	O
apply	O
for	O
a	O
degree	O
programme	O
in	O
the	O
first	O
place	O
p11	O
universities	O
are	O
for	O
all	O
citizens	O
,	O
independent	O
of	O
their	O
finances	O
p12	O
what	O
is	O
the	O
good	O
of	O
a	O
wonderfully	O
outfitted	O
university	O
if	O
it	O
does	O
n't	O
actually	O
allow	O
the	O
majority	O
of	O
clever	O
people	O
to	O
broaden	O
their	O
horizons	O
with	O
all	O
that	O
great	O
equipment	O
Topic	O
Should	O
all	O
universities	O
in	O
Germany	O
charge	O
tuition	O
fees	O
?	O
Stance	O
Con	O
Table	O
1	O
:	O
The	O
candidate	O
thesis	O
,	O
con	O
,	O
and	O
pro	O
units	O
for	O
one	O
topic	O
-	O
stance	O
pair	O
in	O
the	O
dataset	O
of	O
Wachsmuth	O
et	O
al	O
(	O
2018	O
)	O
.	O
Some	O
other	O
approaches	O
have	O
been	O
proposed	O
that	O
recompose	O
existing	O
text	O
segments	O
in	O
new	O
arguments	O
.	O
In	O
particular	O
,	O
Bilu	O
and	O
Slonim	O
(	O
2016	O
)	O
generated	O
new	O
claims	O
by	O
"	O
recycling	O
"	O
topics	O
and	O
predicates	O
that	O
were	O
found	O
in	O
a	O
database	O
of	O
claims	O
.	O
Claim	O
selection	O
involves	O
preferring	O
predicates	O
that	O
are	O
generally	O
amenable	O
to	O
claim	O
units	O
and	O
that	O
are	O
relevant	O
for	O
the	O
target	O
topic	O
.	O
Egan	O
et	O
al	O
(	O
2016	O
)	O
created	O
summaries	O
of	O
the	O
main	O
points	O
in	O
a	O
debate	O
,	O
and	O
synthesized	O
complete	O
arguments	O
from	O
a	O
set	O
of	O
manually	O
curated	O
topic	O
-	O
stance	O
relations	O
based	O
on	O
the	O
fine	O
-	O
grained	O
argument	O
model	O
of	O
Toulmin	O
(	O
1958	O
)	O
.	O
However	O
,	O
we	O
are	O
not	O
aware	O
of	O
any	O
approach	O
that	O
synthesizes	O
arguments	O
fully	O
automatically	O
,	O
let	O
al	O
ne	O
that	O
follows	O
rhetorical	O
considerations	O
in	O
the	O
synthesis	O
process	O
.	O

Logos	O
-	O
oriented	O
c1	O
one	O
could	O
argue	O
that	O
an	O
increase	O
in	O
tuition	O
fees	O
would	O
allow	O
institutions	O
to	O
be	O
better	O
equipped	O
,	O
t1	O
however	O
German	O
universities	O
should	O
on	O
no	O
account	O
charge	O
tuition	O
fees	O
.	O
p1	O
education	O
and	O
training	O
are	O
fundamental	O
rights	O
which	O
the	O
state	O
,	O
the	O
society	O
must	O
provide	O
,	O
p12	O
because	O
what	O
is	O
the	O
good	O
of	O
a	O
wonderfully	O
outfitted	O
university	O
if	O
it	O
does	O
n't	O
actually	O
allow	O
the	O
majority	O
of	O
clever	O
people	O
to	O
broaden	O
their	O
horizons	O
with	O
all	O
that	O
great	O
equipment	O
.	O
p4	O
Besides	O
,	O
funding	O
-	O
wise	O
it	O
ought	O
to	O
be	O
considered	O
how	O
costs	O
incurred	O
by	O
students	O
from	O
other	O
(	O
federal	O
)	O
states	O
can	O
be	O
reimbursed	O
.	O
Pathos	O
-	O
oriented	O
p1	O
education	O
and	O
training	O
are	O
fundamental	O
rights	O
which	O
the	O
state	O
,	O
the	O
society	O
must	O
provide	O
.	O
t2	O
This	O
is	O
why	O
the	O
universities	O
in	O
Germany	O
should	O
not	O
under	O
any	O
circumstances	O
charge	O
tuition	O
fees	O
.	O
c1	O
one	O
could	O
argue	O
that	O
an	O
increase	O
in	O
tuition	O
fees	O
would	O
allow	O
institutions	O
to	O
be	O
better	O
equipped	O
,	O
p3	B-DatasetName
however	O
fees	O
result	O
in	O
longer	O
durations	O
of	O
studies	O
p6	O
and	O
longer	O
durations	O
of	O
studies	O
are	O
costly	O
.	O
Wachsmuth	O
et	O
al	O
(	O
2018	O
)	O
.	O
The	O
italiced	O
connectives	O
were	O
added	O
by	O
the	O
participants	O
;	O
they	O
are	O
not	O
part	O
of	O
the	O
ADUs	O
.	O
pathos	O
-	O
oriented	O
argumentative	O
texts	O
.	O
We	O
use	O
these	O
260	O
texts	O
to	O
develop	O
and	O
evaluate	O
our	O
computational	O
model	O
for	O
argumentation	O
synthesis	O
.	O

We	O
start	O
from	O
a	O
training	O
set	O
of	O
ADUs	O
for	O
a	O
set	O
of	O
m	O
topic	O
-	O
stance	O
pairs	O
.	O
To	O
generalize	O
the	O
language	O
model	O
beyond	O
the	O
covered	O
topics	O
,	O
each	O
ADU	O
is	O
represented	O
using	O
features	O
that	O
aim	O
to	O
capture	O
general	O
emotion	B-DatasetName
-	O
related	O
and	O
logic	O
-	O
related	O
characteristics	O
,	O
accounting	O
for	O
the	O
two	O
given	O
strategies	O
.	O
In	O
particular	O
,	O
we	O
first	O
cluster	O
the	O
pool	O
of	O
all	O
training	O
ADUs	O
based	O
on	O
their	O
feature	O
representation	O
.	O
As	O
a	O
result	O
,	O
each	O
ADU	O
is	O
represented	O
by	O
a	O
cluster	O
label	O
(	O
A	O
-	O
F	O
in	O
Figure	O
2	O
)	O
,	O
where	O
each	O
label	O
represents	O
one	O
ADU	O
type	O
.	O
Now	O
,	O
for	O
each	O
of	O
the	O
strategies	O
,	O
we	O
map	O
each	O
manually	O
-	O
generated	O
sequence	O
of	O
ADUs	O
to	O
a	O
sequence	O
of	O
cluster	O
labels	O
.	O
Using	O
these	O
sequences	O
of	O
labels	O
,	O
we	O
train	O
one	O
separated	O
selection	O
language	O
model	O
for	O
each	O
strategy	O
.	O
For	O
clustering	O
,	O
we	O
rely	O
on	O
topic	O
-	O
independent	O
features	O
that	O
we	O
expect	O
to	O
implicitly	O
encode	O
logical	O
and	O
emotional	O
strategies	O
:	O
(	O
1	O
)	O
psychological	O
meaningfulness	O
(	O
Pennebaker	O
et	O
al	O
,	O
2015	O
)	O
,	O
(	O
2	O
)	O
eight	O
basic	O
emotions	O
(	O
Plutchik	O
,	O
1980	O
;	O
Mohammad	O
and	O
Turney	O
,	O
2013	O
)	O
,	O
and	O
(	O
3	O
)	O
argumentativeness	O
(	O
Somasundaran	O
et	O
al	O
,	O
2007	O
)	O
.	O
In	O
the	O
following	O
,	O
we	O
elaborate	O
on	O
the	O
concrete	O
features	O
that	O
we	O
extract	O
:	O
The	O
input	O
is	O
a	O
corpus	O
of	O
argumentative	O
texts	O
for	O
m	O
topic	O
-	O
stance	O
pairs	O
,	O
each	O
decomposed	O
into	O
a	O
sequence	O
of	O
theses	O
,	O
con	O
units	O
,	O
and	O
pro	O
units	O
.	O
Initially	O
,	O
the	O
set	O
of	O
all	O
these	O
ADUs	O
is	O
clustered	O
to	O
obtain	O
a	O
set	O
topic	O
-	O
independent	O
ADU	O
types	O
,	O
called	O
A	O
-	O
F	O
here	O
.	O
(	O
1	O
)	O
Selection	O
language	O
model	O
:	O
Each	O
argument	O
is	O
converted	O
from	O
a	O
sequence	O
of	O
ADUs	O
to	O
a	O
sequence	O
of	O
ADU	O
types	O
,	O
where	O
a	O
language	O
model	O
is	O
trained	O
on	O
these	O
type	O
sequences	O
.	O
(	O
2	O
)	O
Arrangement	O
language	O
model	O
:	O
Each	O
argument	O
is	O
converted	O
from	O
a	O
sequence	O
of	O
ADUs	O
to	O
a	O
sequence	O
of	O
ADU	O
roles	O
(	O
thesis	O
,	O
pro	O
,	O
and	O
con	O
)	O
where	O
a	O
language	O
model	O
is	O
trained	O
on	O
these	O
ADU	O
role	O
sequences	O
.	O
(	O
3	O
)	O
Phrasing	O
regression	O
model	O
:	O
A	O
linear	B-MethodName
regression	I-MethodName
model	O
is	O
trained	O
which	O
scores	O
each	O
ADU	O
sequence	O
with	O
respect	O
to	O
its	O
semantic	O
coherence	O
.	O

LIWC	O
is	O
a	O
lexicon	O
-	O
based	O
text	O
analysis	O
that	O
counts	O
words	O
in	O
psychologically	O
meaningful	O
categories	O
(	O
Tausczik	O
and	O
Pennebaker	O
,	O
2010	O
)	O
.	O
We	O
use	O
the	O
version	O
by	O
Pennebaker	O
et	O
al	O
(	O
2015	O
)	O
,	O
which	O
contains	O
the	O
following	O
15	O
dimensions	O
:	O
1	O
.	O
Language	O
metrics	O
,	O
e.g.	O
,	O
words	O
per	O
sentence	O
.	O
2	O
.	O
Function	O
words	O
,	O
e.g.	O
,	O
pronouns	O
and	O
auxiliary	O
verbs	O
.	O
3	O
.	O
Other	O
grammar	O
,	O
e.g.	O
,	O
common	O
verbs	O
and	O
comparisons	O
.	O
4	O
.	O
Affect	O
words	O
,	O
e.g.	O
,	O
positive	O
emotion	B-DatasetName
words	O
.	O
5	O
.	O
Social	O
words	O
,	O
e.g.	O
,	O
"	O
family	O
"	O
and	O
"	O
friends	O
"	O
.	O
6	O
.	O
Cognitive	O
processes	O
,	O
e.g.	O
,	O
"	O
discrepancies	O
"	O
and	O
"	O
certainty	O
"	O
.	O
7	O
.	O
Perceptual	O
processes	O
,	O
e.g.	O
,	O
"	O
feeling	O
"	O
.	O
8	O
.	O
Biological	O
processes	O
,	O
e.g.	O
,	O
"	O
health	O
"	O
.	O
9	O
.	O
Core	O
drives	O
and	O
needs	O
,	O
e.g.	O
,	O
"	O
power	O
"	O
and	O
"	O
reward	O
focused	O
"	O
.	O
10	O
.	O
Time	O
orientation	O
,	O
e.g.	O
,	O
past	O
-	O
focused	O
.	O
11	O
.	O
Relativity	O
,	O
e.g.	O
,	O
"	O
time	O
"	O
and	O
"	O
space	O
"	O
.	O
12	O
.	O
Personal	O
concerns	O
,	O
e.g.	O
,	O
"	O
work	O
"	O
and	O
"	O
leisure	O
"	O
.	O
13	O
.	O
Informal	O
speech	O
,	O
e.g.	O
,	O
fillers	O
and	O
nonfluencies	O
.	O
14	O
.	O
Punctuation	O
,	O
e.g.	O
,	O
periods	O
and	O
commas	O
.	O
15	O
.	O
Summary	O
variables	O
,	O
as	O
detailed	O
below	O
.	O
There	O
are	O
four	O
summary	O
variables	O
,	O
each	O
of	O
which	O
is	O
derived	O
from	O
various	O
LIWC	O
dimensions	O
:	O
(	O
1	O
)	O
analytical	O
thinking	O
(	O
Pennebaker	O
et	O
al	O
,	O
2014	O
)	O
,	O
i.e.	O
,	O
the	O
degree	O
to	O
which	O
people	O
use	O
narrative	O
language	O
(	O
low	O
value	O
)	O
,	O
or	O
more	O
logical	O
and	O
formal	O
language	O
(	O
high	O
)	O
;	O
(	O
2	O
)	O
clout	O
(	O
Kacewicz	O
et	O
al	O
,	O
2014	O
)	O
,	O
i.e.	O
,	O
the	O
relative	O
social	O
status	O
,	O
confidence	O
,	O
and	O
leadership	O
displayed	O
in	O
a	O
text	O
;	O
(	O
3	O
)	O
authenticity	O
(	O
Newman	O
et	O
al	O
,	O
2003	O
)	O
,	O
i.e.	O
,	O
the	O
degree	O
to	O
which	O
people	O
reveal	O
themselves	O
in	O
an	O
authentic	O
way	O
;	O
and	O
(	O
4	O
)	O
emotional	O
tone	O
(	O
Cohn	O
et	O
al	O
,	O
2004	O
)	O
,	O
i.e.	O
,	O
negative	O
for	O
values	O
lower	O
than	O
50	O
and	O
positive	O
otherwise	O
.	O

For	O
each	O
argument	O
,	O
we	O
opt	O
for	O
a	O
feature	O
representation	O
that	O
embeds	O
the	O
content	O
properties	O
of	O
ADUs	O
in	O
order	O
to	O
capture	O
their	O
content	O
relationship	O
.	O
Concretely	O
,	O
we	O
represent	O
each	O
argument	O
by	O
calculating	O
the	O
semantic	O
similarities	O
of	O
each	O
adjacent	O
bigram	O
in	O
a	O
human	O
-	O
generated	O
argument	O
.	O
We	O
train	O
a	O
linear	B-MethodName
regression	I-MethodName
model	O
where	O
each	O
instance	O
represents	O
the	O
features	O
of	O
one	O
argument	O
.	O
To	O
this	O
end	O
,	O
we	O
set	O
a	O
score	O
to	O
be	O
the	O
sum	O
of	O
the	O
probabilities	O
of	O
ADU	O
bigrams	O
occurring	O
in	O
one	O
argument	O
.	O
The	O
phrasing	O
model	O
scores	O
each	O
of	O
the	O
filtered	O
arguments	O
given	O
as	O
output	O
by	O
the	O
arrangement	O
model	O
.	O
The	O
argument	O
with	O
the	O
highest	O
score	O
is	O
the	O
final	O
generated	O
argument	O
.	O

universities	O
should	O
not	O
charge	O
tuition	O
fees	O
in	O
Germany	O
.	O
c3	B-DatasetName
to	O
oblige	O
non	O
-	O
academics	O
to	O
finance	O
others	O
'	O
degrees	O
through	O
taxes	O
is	O
not	O
just	O
.	O
p9	O
this	O
would	O
impede	O
or	O
prevent	O
access	O
to	O
those	O
who	O
are	O
financially	O
weaker	O
.	O
p5	O
if	O
a	O
university	O
lacks	O
the	O
funds	O
,	O
sponsors	O
must	O
be	O
found	O
.	O
p8	O
there	O
are	O
other	O
instruments	O
to	O
motivate	O
tighter	O
discipline	O
while	O
studying	O
.	O

p2	O
education	O
must	O
not	O
be	O
a	O
question	O
of	O
money	O
in	O
a	O
wealthy	O
society	O
such	O
as	O
Germany	O
.	O
c1	O
one	O
could	O
argue	O
that	O
an	O
increase	O
in	O
tuition	O
fees	O
would	O
allow	O
institutions	O
to	O
be	O
better	O
equipped	O
.	O
p7	O
studying	O
and	O
taking	O
higher	O
degrees	O
must	O
remain	O
a	O
basic	O
right	O
for	O
everyone	O
.	O
p6	O
longer	O
durations	O
of	O
studies	O
are	O
costly	O
.	O
t2	O
the	O
universities	O
in	O
Germany	O
should	O
not	O
under	O
any	O
circumstances	O
charge	O
tuition	O
fees	O
.	O
Table	O
6	O
:	O
Comparison	O
of	O
two	O
con	O
arguments	O
computationally	O
synthesized	O
with	O
our	O
model	O
for	O
the	O
topic	O
Should	O
all	O
universities	O
in	O
Germany	O
charge	O
tuition	O
fees	O
?	O
,	O
each	O
being	O
a	O
sequence	O
of	O
five	O
ADUs	O
.	O
A	O
logos	O
-	O
oriented	O
argument	O
(	O
t	O
4	O
,	O
c	O
3	O
,	O
p	O
9	O
,	O
p	O
5	O
,	O
p	O
8	O
)	O
and	O
a	O
pathos	O
-	O
oriented	O
argument	O
(	O
p	O
2	O
,	O
c	O
1	O
,	O
p	O
7	O
,	O
p	O
6	O
,	O
t	O
2	O
)	O
.	O
The	O
thesis	O
of	O
each	O
argument	O
is	O
marked	O
bold	O
.	O
and	O
by	O
the	O
baseline	O
,	O
with	O
and	O
without	O
considering	O
the	O
ordering	O
of	O
ADUs	O
.	O
Our	O
models	O
outperform	O
the	O
baseline	O
for	O
1	O
-	O
grams	O
and	O
2	O
-	O
grams	O
in	O
all	O
cases	O
.	O
For	O
sequential	O
3	O
-	O
grams	O
,	O
however	O
,	O
it	O
did	O
not	O
achieve	O
any	O
overlap	O
with	O
the	O
human	O
-	O
generated	O
arguments	O
for	O
either	O
strategy	O
.	O
This	O
may	O
be	O
explained	O
by	O
the	O
fact	O
that	O
the	O
employed	O
selection	O
and	O
phrasing	O
models	O
are	O
based	O
on	O
2	O
-	O
grams	O
only	O
.	O
For	O
n	O
≥	O
2	O
,	O
the	O
synthesis	O
generally	O
does	O
not	O
work	O
well	O
anymore	O
.	O
We	O
believe	O
that	O
the	O
small	O
data	O
size	O
is	O
a	O
main	O
cause	O
behind	O
this	O
,	O
although	O
it	O
may	O
also	O
point	O
to	O
the	O
limitation	O
of	O
composing	O
ADUs	O
based	O
on	O
surface	O
features	O
.	O
In	O
the	O
non	O
-	O
sequential	O
case	O
,	O
though	O
,	O
our	O
model	O
performs	O
comparably	O
well	O
for	O
3	O
-	O
grams	O
,	O
and	O
it	O
even	O
manages	O
to	O
correctly	O
synthesize	O
some	O
ADU	O
4	O
-	O
grams	O
.	O
In	O
Table	O
6	O
,	O
we	O
exemplify	O
the	O
top	O
-	O
scored	O
arguments	O
for	O
one	O
topic	O
-	O
stance	O
pair	O
,	O
synthesized	O
by	O
our	O
approach	O
for	O
logos	O
and	O
for	O
pathos	O
respectively	O
.	O
They	O
indicate	O
that	O
our	O
model	O
was	O
able	O
to	O
learn	O
strategy	O
-	O
specific	O
differences	O
.	O
6	O
In	O
particular	O
,	O
the	O
logos	O
argument	O
starts	O
with	O
the	O
thesis	O
(	O
t	O
2	O
)	O
,	O
as	O
argumentation	O
guidelines	O
suggest	O
.	O
It	O
then	O
reasons	O
based	O
on	O
consequences	O
and	O
alternatives	O
.	O
Matching	O
intu	O
-	O
ition	O
,	O
the	O
pathos	O
argument	O
appeals	O
more	O
to	O
emotion	B-DatasetName
,	O
reflected	O
in	O
phrases	O
such	O
as	O
"	O
wealthy	O
society	O
"	O
and	O
"	O
under	O
any	O
circumstances	O
"	O
.	O
Particularly	O
the	O
thesis	O
(	O
t	O
4	O
)	O
has	O
a	O
more	O
intense	O
tonality	O
than	O
t	O
2	O
,	O
and	O
putting	O
it	O
at	O
the	O
end	O
creates	O
additional	O
emphasis	O
.	O

Natural	B-TaskName
language	I-TaskName
inference	I-TaskName
(	O
NLI	O
)	O
has	O
been	O
widely	O
used	O
as	O
a	O
task	O
to	O
train	O
and	O
evaluate	O
models	O
for	O
language	O
understanding	O
.	O
However	O
,	O
the	O
ability	O
of	O
NLI	O
models	O
to	O
perform	O
inferences	O
requiring	O
understanding	O
of	O
figurative	O
language	O
such	O
as	O
idioms	O
and	O
metaphors	O
remains	O
understudied	O
.	O
We	O
introduce	O
the	O
IMPLI	O
(	O
Idiomatic	O
and	O
Metaphoric	O
Paired	O
Language	O
Inference	O
)	O
dataset	O
,	O
an	O
English	O
dataset	O
consisting	O
of	O
paired	O
sentences	O
spanning	O
idioms	O
and	O
metaphors	O
.	O
We	O
develop	O
novel	O
methods	O
to	O
generate	O
24k	O
semiautomatic	O
pairs	O
as	O
well	O
as	O
manually	O
creating	O
1.8k	O
gold	O
pairs	O
.	O
We	O
use	O
IMPLI	O
to	O
evaluate	O
NLI	O
models	O
based	O
on	O
RoBERTa	B-MethodName
fine	O
-	O
tuned	O
on	O
the	O
widely	O
used	O
MNLI	B-DatasetName
dataset	O
.	O
We	O
then	O
show	O
that	O
while	O
they	O
can	O
reliably	O
detect	O
entailment	O
relationship	O
between	O
figurative	O
phrases	O
with	O
their	O
literal	O
counterparts	O
,	O
they	O
perform	O
poorly	O
on	O
similarly	O
structured	O
examples	O
where	O
pairs	O
are	O
designed	O
to	O
be	O
non	O
-	O
entailing	O
.	O
This	O
suggests	O
the	O
limits	O
of	O
current	O
NLI	O
models	O
with	O
regard	O
to	O
understanding	O
figurative	O
language	O
and	O
this	O
dataset	O
serves	O
as	O
a	O
benchmark	O
for	O
future	O
improvements	O
in	O
this	O
direction	O
.	O
1	O
*	O
The	O
work	O
was	O
done	O
while	O
the	O
second	O
author	O
was	O
still	O
affiliated	O
with	O
the	O
UKP	B-DatasetName
Lab	O
at	O
TU	O
Darmstadt	O
.	O

Understanding	O
figurative	O
language	O
(	O
i.e.	O
,	O
that	O
in	O
which	O
the	O
intended	O
meaning	O
of	O
the	O
utterance	O
differs	O
from	O
the	O
literal	O
compositional	O
meaning	O
)	O
is	O
a	O
particularly	O
difficult	O
area	O
in	O
NLP	O
(	O
Shutova	O
,	O
2011	O
;	O
Veale	O
et	O
al	O
,	O
2016	O
)	O
,	O
but	O
is	O
essential	O
for	O
proper	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
.	O
We	O
consider	O
here	O
two	O
types	O
of	O
figurative	O
language	O
:	O
idioms	O
and	O
metaphors	O
.	O
Idioms	O
can	O
be	O
viewed	O
as	O
non	O
-	O
compositional	O
multiword	O
expressions	O
(	O
Jochim	O
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
have	O
been	O
historically	O
difficult	O
for	O
NLP	O
systems	O
.	O
For	O
instance	O
,	O
sentiment	O
systems	O
struggle	O
with	O
multiword	O
expressions	O
in	O
which	O
individual	O
words	O
do	O
not	O
directly	O
contribute	O
to	O
the	O
sentiment	O
(	O
Sag	B-MethodName
et	O
al	O
,	O
2002	O
)	O
.	O

Figurative	O
language	O
includes	O
idioms	O
,	O
metaphors	O
,	O
metonymy	O
,	O
hyperbole	O
,	O
and	O
more	O
.	O
Critically	O
,	O
figurative	O
language	O
is	O
that	O
in	O
which	O
speaker	O
meaning	O
(	O
what	O
the	O
speaker	O
intends	O
to	O
accomplish	O
through	O
an	O
utterance	O
)	O
differs	O
from	O
the	O
literal	O
meaning	O
of	O
that	O
utterance	O
.	O
This	O
leads	O
to	O
problems	O
in	O
NLP	O
systems	O
if	O
they	O
are	O
trained	O
mostly	O
on	O
literal	O
data	O
,	O
as	O
their	O
representations	O
for	O
particular	O
words	O
and/or	O
phrases	O
will	O
not	O
reflect	O
their	O
figurative	O
intended	O
meanings	O
.	O
Figurative	O
language	O
has	O
a	O
significant	O
impact	O
on	O
many	O
NLP	O
tasks	O
.	O
Metaphoric	O
understanding	O
has	O
been	O
shown	O
to	O
be	O
necessary	O
for	O
proper	O
machine	B-TaskName
translation	I-TaskName
(	O
Mao	O
et	O
al	O
,	O
2018	O
;	O
Mohammad	O
et	O
al	O
,	O
2016	O
)	O
.	O
Sentiment	B-TaskName
analysis	I-TaskName
also	O
relies	O
critically	O
on	O
figurative	O
language	O
:	O
irony	O
and	O
sarcasm	O
can	O
reverse	O
the	O
polarity	O
of	O
a	O
sentence	O
,	O
while	O
metaphors	O
and	O
idioms	O
may	O
make	O
more	O
subtle	O
changes	O
in	O
the	O
speaker	O
meaning	O
(	O
Ghosh	O
et	O
al	O
,	O
2015	O
)	O
.	O
Political	O
discourse	O
tasks	O
including	O
bias	O
,	O
misinformation	O
,	O
and	O
political	O
framing	O
detection	O
benefit	O
from	O
joint	O
learning	O
with	O
metaphoricity	O
(	O
Huguet	O
Cabot	O
et	O
al	O
,	O
2020	O
)	O
.	O
Figurative	O
language	O
engendered	O
by	O
creativity	O
on	O
social	O
media	O
also	O
poses	O
difficulty	O
for	O
many	O
NLP	O
tasks	O
including	O
identifying	O
depression	O
symptoms	O
(	O
Yadav	O
et	O
al	O
,	O
2020	O
;	O
Iyer	O
et	O
al	O
,	O
2019	O
)	O
and	O
hate	B-TaskName
speech	I-TaskName
detection	I-TaskName
(	O
Lemmens	O
et	O
al	O
,	O
2021	O
)	O
.	O
We	O
are	O
here	O
focused	O
on	O
idioms	O
and	O
metaphors	O
.	O
There	O
is	O
currently	O
a	O
gap	O
in	O
diagnostic	O
datasets	O
for	O
idioms	O
,	O
and	O
our	O
work	O
fills	O
this	O
gap	O
.	O
There	O
exist	O
some	O
relevant	O
metaphoric	O
resources	O
(	O
see	O
2.2	O
)	O
;	O
metaphors	O
are	O
known	O
to	O
be	O
extremely	O
common	O
and	O
important	O
to	O
understanding	O
figurative	O
language	O
,	O
our	O
resource	O
serves	O
to	O
build	O
upon	O
this	O
work	O
.	O

To	O
build	O
idiomatic	O
pairs	O
,	O
we	O
use	O
three	O
corpora	O
that	O
contain	O
sentences	O
with	O
idiomatic	O
expressions	O
(	O
IEs	O
)	O
labelled	O
as	O
either	O
figurative	O
or	O
literal	O
.	O
2	O
These	O
are	O
the	O
MAGPIE	O
Corpus	O
(	O
Haagsma	O
et	O
al	O
,	O
2020	O
)	O
,	O
the	O
PIE	B-DatasetName
Corpus	O
(	O
Adewumi	O
et	O
al	O
,	O
2021	O
)	O
,	O
and	O
the	O
SemEval	B-DatasetName
2013	I-DatasetName
Task	O
5	O
(	O
Korkontzelos	O
et	O
al	O
,	O
2013	O
)	O
.	O
We	O
collect	O
the	O
total	O
set	O
of	O
IEs	O
that	O
are	O
present	O
in	O
these	O
corpora	O
.	O
We	O
then	O
extract	O
definitions	O
for	O
these	O
using	O
freely	O
available	O
online	O
idiom	O
dictionaries	O
.	O
3	O
These	O
definitions	O
are	O
often	O
faulty	O
,	O
incomplete	O
,	O
or	O
improperly	O
formatted	O
.	O
We	O
employed	O
annotators	O
to	O
make	O
manual	O
corrections	O
.	O
The	O
annotators	O
were	O
given	O
the	O
original	O
IE	O
as	O
well	O
as	O
the	O
definition	O
extracted	O
from	O
the	O
dictionary	O
.	O
The	O
annotators	O
were	O
asked	O
to	O
ensure	O
that	O
the	O
dictionary	O
definition	O
given	O
was	O
(	O
1	O
)	O
a	O
correct	O
literal	O
interpretation	O
and	O
(	O
2	O
)	O
fit	O
syntactically	O
in	O
the	O
same	O
environments	O
as	O
the	O
original	O
IE	O
.	O
If	O
the	O
definition	O
met	O
both	O
of	O
these	O
criteria	O
,	O
the	O
IE	O
can	O
be	O
replaced	O
by	O
its	O
definition	O
to	O
yield	O
an	O
entailment	O
pair	O
.	O
If	O
either	O
criterion	O
was	O
not	O
met	O
,	O
annotators	O
were	O
asked	O
to	O
minimally	O
update	O
the	O
definition	O
so	O
that	O
it	O
satisfied	O
the	O
requirements	O
.	O
In	O
total	O
this	O
process	O
yielded	O
697	O
IE	O
definitions	O
.	O
We	O
then	O
used	O
the	O
above	O
corpora	O
,	O
replacing	O
these	O
definitions	O
into	O
the	O
original	O
sentences	O
(	O
see	O
Figure	O
1	O
)	O
.	O
We	O
use	O
the	O
figurative	O
/	O
literal	O
labels	O
from	O
the	O
as	O
right	O
as	O
rain	O
original	O
corpora	O
:	O
replacing	O
them	O
into	O
figurative	O
contexts	O
yields	O
entailment	O
relations	O
,	O
while	O
replacing	O
them	O
into	O
contexts	O
where	O
the	O
phrase	O
is	O
meant	O
literally	O
then	O
yields	O
non	O
-	O
entailments	O
.	O

Figure	O
3	O
:	O
Metaphor	O
entailment	O
generation	O
.	O
Pairs	O
are	O
generated	O
using	O
annotator	O
-	O
defined	O
literal	O
translations	O
substituted	O
into	O
metaphoric	O
contexts	O
.	O
2	O
.	O
hard	O
truth	O
unpleasant	O
truth	O
3	O
.	O
hairy	O
problem	O
difficult	O
problem	O
These	O
can	O
then	O
be	O
replaced	O
in	O
a	O
similar	O
fashion	O
:	O
we	O
start	O
with	O
the	O
original	O
figurative	O
sentence	O
,	O
replace	O
the	O
ME	O
with	O
the	O
literal	O
replacements	O
,	O
and	O
the	O
result	O
is	O
an	O
entailing	O
pair	O
with	O
the	O
metaphoric	O
sentence	O
entailing	O
the	O
literal	O
.	O
We	O
apply	O
this	O
procedure	O
to	O
the	O
dataset	O
of	O
Tsvetkov	O
et	O
al	O
(	O
2014	O
)	O
,	O
yielding	O
100	O
metaphoric	O
/	O
literal	O
NLI	O
entailment	O
pairs	O
.	O
We	O
then	O
take	O
a	O
portion	O
of	O
the	O
Common	B-DatasetName
Crawl	I-DatasetName
dataset	O
4	O
,	O
and	O
identify	O
sentences	O
that	O
contain	O
these	O
original	O
MEs	O
.	O
We	O
identify	O
sentences	O
that	O
contain	O
the	O
words	O
from	O
the	O
metaphoric	O
phrase	O
,	O
and	O
replace	O
the	O
metaphoric	O
word	O
itself	O
with	O
its	O
literal	O
counterpart	O
.	O
This	O
yields	O
645	O
additional	O
silver	O
pairs	O
.	O

For	O
all	O
silver	O
methods	O
,	O
we	O
also	O
employ	O
syntactic	O
postprocessing	O
to	O
overcome	O
a	O
number	O
of	O
hurdles	O
.	O
First	O
,	O
phrases	O
used	O
idiomatically	O
often	O
follow	O
different	O
syntactic	O
patterns	O
than	O
when	O
used	O
literally	O
.	O
Original	O
:	O
These	O
point	O
out	O
of	O
this	O
world	O
,	O
but	O
where	O
to	O
is	O
not	O
made	O
clear	O
.	O
Replaced	O
:	O
*	O
These	O
point	O
wonderful	O
,	O
but	O
where	O
to	O
is	O
not	O
made	O
clear	O
.	O
This	O
phrase	O
in	O
literal	O
contexts	O
functions	O
syntactically	O
as	O
a	O
prepositional	O
phrase	O
,	O
while	O
idiomatically	O
it	O
is	O
used	O
as	O
an	O
adjective	O
.	O
When	O
replaced	O
with	O
the	O
definition	O
"	O
wonderful	O
"	O
in	O
a	O
literal	O
context	O
,	O
we	O
get	O
a	O
grammatically	O
incoherent	O
sentence	O
.	O
Second	O
,	O
phrases	O
in	O
their	O
literal	O
usage	O
often	O
do	O
not	O
form	O
full	O
constituents	O
,	O
due	O
to	O
the	O
string	O
-	O
matching	O
approach	O
of	O
the	O
original	O
datasets	O
.	O
Many	O
literal	O
usages	O
of	O
these	O
phrases	O
are	O
thus	O
incompatible	O
with	O
the	O
defined	O
replacement	O
.	O
I	O
think	O
[	O
this	O
one	O
has	O
to	O
die	O
]	O
for	O
the	O
other	O
one	O
to	O
live	O
.	O
Turn	O
in	O
[	O
the	O
raw	O
edges	O
]	O
of	O
both	O
seam	O
allowances	O
towards	O
each	O
other	O
and	O
match	O
the	O
folded	O
edges	O
.	O
To	O
avoid	O
these	O
issues	O
,	O
we	O
ran	O
syntactic	O
parsing	O
on	O
the	O
definition	O
and	O
the	O
expression	O
within	O
each	O
context	O
,	O
requiring	O
that	O
the	O
expression	O
in	O
context	O
begins	O
with	O
the	O
same	O
part	O
of	O
speech	O
as	O
the	O
definition	O
and	O
that	O
it	O
does	O
not	O
end	O
inside	O
of	O
another	O
phrase	O
.	O
Additionally	O
,	O
for	O
each	O
replacement	O
,	O
we	O
ensured	O
that	O
the	O
verb	O
conjugation	O
matched	O
the	O
context	O
.	O
For	O
this	O
,	O
we	O
identified	O
the	O
conjugation	O
in	O
the	O
context	O
,	O
and	O
used	O
a	O
de	O
-	O
lemmatization	B-TaskName
script	O
to	O
conjugate	O
the	O
replacement	O
verb	O
to	O
match	O
the	O
original	O
.	O

In	O
order	O
for	O
these	O
automatically	O
created	O
pairs	O
to	O
be	O
useful	O
for	O
NLI	O
-	O
based	O
evaluation	O
,	O
they	O
need	O
to	O
be	O
of	O
sufficiently	O
high	O
quality	O
.	O
As	O
the	O
annotators	O
were	O
generating	O
novel	O
definitions	O
and	O
pairs	O
,	O
rather	O
than	O
inter	O
-	O
annotator	O
agreement	O
,	O
we	O
instead	O
evaluate	O
the	O
quality	O
of	O
the	O
resulting	O
pairs	O
by	O
testing	O
whether	O
the	O
automatically	O
generated	O
pairs	O
contained	O
the	O
appropriate	O
entailment	O
relation	O
.	O
For	O
this	O
task	O
,	O
each	O
annotator	O
was	O
given	O
100	O
samples	O
for	O
each	O
general	O
category	O
of	O
silver	O
generations	O
(	O
idiomatic	O
entailments	O
,	O
idiomatic	O
non	O
-	O
entailments	O
,	O
and	O
metaphoric	O
entailments	O
)	O
.	O
They	O
were	O
asked	O
if	O
the	O
entailment	O
relation	O
between	O
the	O
two	O
sentences	O
was	O
as	O
expected	O
.	O
An	O
expert	O
than	O
adjudicated	O
disagreements	O
to	O
determine	O
the	O
final	O
percentage	O
of	O
valid	O
pairs	O
.	O
To	O
evaluate	O
the	O
syntactic	O
validity	O
of	O
the	O
generated	O
pairs	O
,	O
we	O
additionally	O
ran	O
the	O
Stanford	O
PCFG	O
dependency	O
parser	O
(	O
Klein	O
and	O
Manning	O
,	O
2003	O
)	O
the	O
pairs	O
.	O
Per	O
previous	O
work	O
in	O
NLI	O
(	O
Williams	O
et	O
al	O
,	O
2018	O
)	O
,	O
we	O
evaluate	O
the	O
proportion	O
of	O
sentences	O
for	O
which	O
the	O
root	O
node	O
is	O
S.	O
Table	O
4	O
shows	O
the	O
results	O
.	O
The	O
semi	O
-	O
supervised	O
examples	O
evoked	O
the	O
correct	O
entailment	O
relation	O
between	O
%	O
88	O
and	O
%	O
97	O
of	O
the	O
time	O
:	O
while	O
there	O
is	O
still	O
noise	O
present	O
,	O
this	O
indicates	O
the	O
effectiveness	O
of	O
the	O
proposed	O
methods	O
.	O
With	O
regard	O
to	O
syntax	O
,	O
we	O
see	O
S	O
node	O
roots	O
for	O
between	O
82	O
%	O
and	O
%	O
90	O
of	O
the	O
sentences	O
:	O
within	O
the	O
range	O
of	O
the	O
SNLI	B-DatasetName
performance	O
(	O
74	O
%	O
-	O
88	O
%	O
)	O
,	O
and	O
slightly	O
behind	O
the	O
MNLI	B-DatasetName
(	O
91	O
%	O
-	O
98	O
%	O
)	O
.	O
We	O
find	O
that	O
the	O
generated	O
hypotheses	O
are	O
not	O
significantly	O
different	O
in	O
quality	O
than	O
the	O
premises	O
.	O
This	O
indicates	O
that	O
the	O
method	O
for	O
generation	O
preserves	O
the	O
original	O
syntax	O
.	O
These	O
methods	O
allow	O
us	O
to	O
quickly	O
generate	O
a	O
substantial	O
number	O
of	O
high	O
-	O
quality	O
pairs	O
to	O
evaluate	O
NLI	O
systems	O
on	O
figurative	O
language	O
.	O
However	O
,	O
they	O
may	O
introduce	O
additional	O
bias	O
as	O
we	O
employ	O
a	O
number	O
of	O
restrictions	O
in	O
order	O
to	O
ensure	O
syntactic	O
and	O
semantic	O
compatibility	O
,	O
and	O
we	O
lack	O
full	O
nonentailment	O
pairs	O
for	O
metaphoric	O
data	O
.	O
We	O
therefore	O
expand	O
our	O
dataset	O
with	O
manually	O
generated	O
pairs	O
.	O

Using	O
the	O
IMPLI	O
dataset	O
,	O
we	O
aim	O
to	O
answer	O
a	O
series	O
of	O
questions	O
via	O
NLI	O
pertaining	O
to	O
language	O
models	O
'	O
ability	O
to	O
understand	O
and	O
represent	O
figurative	O
language	O
accurately	O
.	O
These	O
questions	O
are	O
:	O
Our	O
dataset	O
provides	O
unique	O
advantages	O
in	O
addressing	O
these	O
research	O
questions	O
that	O
cover	O
gaps	O
in	O
previous	O
work	O
:	O
it	O
contains	O
a	O
large	O
number	O
of	O
both	O
entailments	O
and	O
non	O
-	O
entailments	O
and	O
is	O
large	O
enough	O
to	O
be	O
used	O
for	O
training	O
the	O
models	O
.	O
Idioms	O
Metaphors	O
Model	O
MNLI	B-DatasetName
MNLI	B-DatasetName
-	I-DatasetName
MM	I-DatasetName
S	O
S	O
l	O
S	O
d	O
G	O
G	O
a	O
G	O
S	O
G	O
G	O
roberta	O
-	O

To	O
evaluate	O
incorporating	O
idioms	O
into	O
training	O
,	O
we	O
then	O
split	O
the	O
idiom	O
data	O
by	O
idiomatic	O
phrase	O
types	O
,	O
keeping	O
a	O
set	O
of	O
IEs	O
separate	O
as	O
test	O
data	O
to	O
assess	O
whether	O
the	O
model	O
can	O
learn	O
to	O
correctly	O
handle	O
novel	O
,	O
unseen	O
phrases	O
.	O
Our	O
goal	O
is	O
to	O
assess	O
whether	O
poor	O
performance	O
is	O
due	O
to	O
models	O
'	O
not	O
containing	O
these	O
expressions	O
in	O
training	O
,	O
or	O
because	O
their	O
ability	O
to	O
represent	O
figurative	O
language	O
inherently	O
limited	O
.	O
We	O
hypothesize	O
that	O
the	O
noncompositional	O
nature	O
of	O
these	O
types	O
of	O
figuration	O
should	O
lead	O
to	O
poor	O
performance	O
on	O
unseen	O
phrases	O
,	O
even	O
if	O
the	O
model	O
is	O
trained	O
on	O
other	O
idiomatic	O
data	O
.	O
For	O
each	O
task	O
,	O
we	O
split	O
the	O
data	O
into	O
10	O
folds	O
by	O
IE	O
and	O
incrementally	O
incorporate	O
these	O
folds	O
into	O
the	O
original	O
MNLI	B-DatasetName
for	O
training	O
,	O
leaving	O
one	O
fold	O
out	O
for	O
testing	O
.	O
We	O
experiment	O
with	O
incorporating	O
all	O
training	O
data	O
for	O
both	O
labels	O
,	O
as	O
well	O
as	O
using	O
only	O
entailment	O
or	O
non	O
-	O
entailment	O
samples	O
.	O
We	O
then	O
evaluate	O
our	O
results	O
on	O
the	O
entire	O
test	O
set	O
,	O
as	O
well	O
as	O
the	O
entailment	O
and	O
non	O
-	O
entailment	O
partitions	O
.	O
Figure	O
4	O
shows	O
the	O
results	O
,	O
highlighting	O
that	O
additional	O
training	O
data	O
yields	O
only	O
small	O
improvements	O
.	O
Pairs	O
with	O
non	O
-	O
entailment	O
relations	O
remain	O
exceedingly	O
difficult	O
,	O
with	O
performance	O
capping	O
out	O
at	O
only	O
slightly	O
better	O
than	O
chance	O
.	O
As	O
hypothesized	O
,	O
additional	O
training	O
data	O
is	O
only	O
somewhat	O
effective	O
in	O
improving	O
language	O
models	O
'	O
idiomatic	O
capabilities	O
;	O
this	O
is	O
not	O
sufficient	O
to	O
overcome	O
difficulties	O
from	O
literal	O
usages	O
of	O
idiomatic	O
phrases	O
and	O
adversarial	O
definitions	O
,	O
indicating	O
that	O
idiomatic	O
language	O
remains	O
difficult	O
for	O
pre	O
-	O
trained	O
language	O
models	O
to	O
learn	O
to	O
represent	O
.	O
R3	O
:	O
Syntactic	O
Flexibility	O
Finally	O
,	O
we	O
assess	O
models	O
'	O
representation	O
of	O
idiomatic	O
compositionality	O
.	O
Nunberg	O
et	O
al	O
(	O
1994	O
)	O
indicate	O
that	O
there	O
are	O
two	O
general	O
types	O
of	O
idioms	O
:	O
"	O
idiomatic	O
phrases	O
"	O
,	O
which	O
exhibit	O
limited	O
flexibility	O
and	O
generally	O
occur	O
only	O
in	O
a	O
single	O
surface	O
form	O
,	O
and	O
"	O
idiomatically	O
combining	O
expressions	O
"	O
or	O
ICEs	O
,	O
in	O
which	O
the	O
constituent	O
elements	O
of	O
the	O
idiom	O
carry	O
semantic	O
meaning	O
which	O
can	O
influence	O
their	O
syntactic	O
properties	O
,	O
allowing	O
them	O
to	O
be	O
more	O
syntactically	O
flexible	O
.	O
For	O
example	O
,	O
in	O
the	O
idiom	O
spill	O
the	O
beans	O
,	O
we	O
can	O
map	O
the	O
spilling	O
activity	O
to	O
divulging	O
of	O
information	O
,	O
and	O
the	O
beans	O
to	O
the	O
information	O
.	O
Because	O
this	O
expression	O
has	O
semantic	O
mappings	O
to	O
figurative	O
meaning	O
for	O
its	O
syntactic	O
constituents	O
,	O
Nunberg	O
et	O
al	O
(	O
1994	O
)	O
argue	O
that	O
it	O
can	O
be	O
more	O
syntactically	O
flexible	O
,	O
allowing	O
for	O
expressions	O
like	O
the	O
beans	O
that	O
were	O
spilled	O
by	O
Martha	O
to	O
maintain	O
idiomatic	O
meaning	O
.	O
For	O
fixed	O
expressions	O
such	O
as	O
kick	O
the	O
bucket	O
,	O
no	O
syntactic	O
constituents	O
map	O
directly	O
to	O
the	O
figurative	O
meaning	O
(	O
"	O
die	O
"	O
)	O
.	O
We	O
then	O
expect	O
less	O
syntactic	O
flexibility	O
,	O
and	O
thus	O
the	O
bucket	O
that	O
was	O
kicked	O
by	O
John	O
loses	O
its	O
idiomatic	O
meaning	O
.	O
We	O
hypothesize	O
that	O
model	O
performance	O
will	O
be	O
correlated	O
with	O
the	O
degree	O
to	O
which	O
a	O
given	O
idiom	O
type	O
is	O
flexible	O
:	O
more	O
fixed	O
expressions	O
may	O
be	O
easier	O
,	O
as	O
they	O
are	O
seen	O
in	O
regular	O
,	O
fixed	O
patterns	O
that	O
the	O
models	O
can	O
memorize	O
,	O
while	O
more	O
flexible	O
ICEs	O
will	O
be	O
more	O
difficult	O
,	O
as	O
they	O
can	O
appear	O
in	O
different	O
patterns	O
,	O
cases	O
,	O
and	O
word	O
order	O
,	O
often	O
even	O
mixing	O
in	O
with	O
other	O
constituents	O
.	O
To	O
test	O
this	O
,	O
we	O
define	O
an	O
ICE	O
score	O
as	O
the	O
percentage	O
of	O
times	O
a	O
phrase	O
occurs	O
in	O
our	O
test	O
data	O
in	O
a	O
form	O
that	O
does	O
not	O
match	O
its	O
original	O
base	O
form	O
.	O
Higher	O
percentages	O
mean	O
the	O
phrase	O
occurs	O
more	O
frequently	O
in	O
a	O
non	O
-	O
standard	O
form	O
,	O
acting	O
as	O
a	O
measure	O
for	O
the	O
syntactic	O
flexibility	O
of	O
the	O
expression	O
.	O
We	O
assessed	O
the	O
performance	O
of	O
the	O
roberta	O
-	O
base	O
model	O
for	O
each	O
idiom	O
type	O
,	O
evaluating	O
Spearman	O
correlations	O
between	O
performance	O
and	O
idioms	O
'	O
ICE	O
scores	O
.	O
We	O
found	O
no	O
correlation	O
between	O
ICE	O
scores	O
and	O
performance	O
for	O
entailments	O
,	O
nor	O
for	O
adversarial	O
definition	O
non	O
-	O
entailments	O
(	O
r	O
=	O
.004/.45	O
,	O
p	O
=	O
.921/.399	O
,	O
see	O
Appendix	O
C	O
)	O
.	O
However	O
,	O
we	O
do	O
see	O
a	O
weak	O
but	O
significant	O
correlation	O
(	O
r	O
=	O
.188	O
,	O
p	O
=	O
0.016	O
)	O
with	O
non	O
-	O
entailments	O
from	O
literal	O
contexts	O
:	O
the	O
model	O
performs	O
better	O
when	O
the	O
phrases	O
are	O
more	O
flexible	O
,	O
contrary	O
to	O
our	O
initial	O
hypothesis	O
.	O
One	O
possible	O
explanation	O
is	O
that	O
the	O
model	O
memorizes	O
a	O
specific	O
figurative	O
meanings	O
for	O
each	O
fixed	O
expression	O
,	O
disregarding	O
the	O
possibility	O
of	O
these	O
words	O
being	O
used	O
literally	O
.	O
When	O
the	O
expression	O
is	O
used	O
in	O
a	O
literal	O
context	O
,	O
the	O
model	O
then	O
still	O
assumes	O
the	O
figurative	O
meaning	O
,	O
resulting	O
in	O
errors	O
on	O
non	O
-	O
entailment	O
samples	O
.	O
The	O
ICEs	O
are	O
more	O
fluid	O
,	O
and	O
thus	O
the	O
model	O
is	O
less	O
likely	O
to	O
have	O
a	O
concrete	O
representation	O
for	O
the	O
given	O
phrase	O
:	O
it	O
is	O
better	O
able	O
to	O
reason	O
about	O
the	O
context	O
and	O
interacting	O
words	O
within	O
the	O
expression	O
,	O
making	O
it	O
easier	O
to	O
distinguish	O
the	O
entailing	O
and	O
non	O
-	O
entailing	O
samples	O
.	O

In	O
this	O
work	O
,	O
we	O
introduce	O
the	O
IMPLI	O
dataset	O
,	O
which	O
we	O
then	O
use	O
to	O
evaluate	O
NLI	O
models	O
'	O
capabilities	O
on	O
figurative	O
language	O
.	O
We	O
show	O
that	O
while	O
widely	O
used	O
MNLI	B-DatasetName
models	O
handle	O
entailment	O
admirably	O
and	O
metaphoric	O
expressions	O
are	O
relatively	O
easy	O
,	O
nonentailment	O
idiomatic	O
relationships	O
are	O
more	O
difficult	O
.	O
Additionally	O
,	O
adding	O
idiom	O
-	O
specific	O
training	O
data	O
fails	O
to	O
alleviate	O
poor	O
performance	O
for	O
nonentailing	O
pairs	O
.	O
This	O
highlights	O
how	O
currently	O
language	O
models	O
are	O
inherently	O
limited	O
in	O
representing	O
some	O
figurative	O
phenomena	O
and	O
can	O
provide	O
a	O
target	O
for	O
future	O
model	O
improvements	O
.	O
For	O
future	O
work	O
,	O
we	O
aim	O
to	O
expand	O
our	O
data	O
collection	O
processes	O
to	O
new	O
data	O
sources	O
.	O
Our	O
dataset	O
creation	O
procedure	O
relies	O
on	O
annotated	O
samples	O
and	O
definitions	O
:	O
as	O
more	O
idiomatic	O
and	O
metaphoric	O
resources	O
become	O
available	O
,	O
this	O
process	O
is	O
broadly	O
extendable	O
to	O
create	O
new	O
figurative	O
/	O
literal	O
pairs	O
.	O
Additionally	O
,	O
we	O
only	O
explore	O
this	O
data	O
for	O
evaluating	O
NLI	O
systems	O
:	O
this	O
data	O
could	O
also	O
be	O
used	O
for	O
other	O
parallel	O
data	O
tasks	O
such	O
as	O
figurative	O
language	O
interpretation	O
(	O
Shutova	O
,	O
2013	O
;	O
Su	O
et	O
al	O
,	O
2017	O
)	O
and	O
figurative	O
paraphrase	B-TaskName
generation	I-TaskName
.	O
As	O
natural	O
language	O
generation	O
often	O
relies	O
on	O
training	O
or	O
fine	O
-	O
tuning	O
models	O
with	O
paired	O
sentences	O
,	O
this	O
data	O
could	O
be	O
a	O
valuable	O
resource	O
for	O
figurative	O
language	O
generation	O
systems	O
.	O

Previous	O
research	O
shows	O
that	O
NLI	O
systems	O
exploit	O
cues	O
based	O
on	O
lexical	O
overlap	O
,	O
predicting	O
entailment	O
for	O
overlapping	O
sentences	O
(	O
McCoy	O
et	O
al	O
,	O
2019	O
;	O
Nie	O
et	O
al	O
,	O
2019	O
)	O
.	O
Our	O
dataset	O
consists	O
mostly	O
of	O
pairs	O
with	O
high	O
overlap	O
:	O
this	O
could	O
explain	O
why	O
the	O
nonentailment	O
sections	O
are	O
more	O
difficult	O
.	O
We	O
thus	O
evaluate	O
system	O
predictions	O
for	O
our	O
datasets	O
as	O
a	O
function	O
of	O
lexical	O
overlap	O
.	O
Figure	O
5	O
shows	O
densitybased	O
histograms	O
of	O
the	O
results	O
,	O
comparing	O
overlap	O
via	O
Levenshtein	O
distance	O
(	O
Levenshtein	O
,	O
1965	O
)	O
for	O
correctly	O
and	O
incorrectly	O
classified	O
pairs	O
.	O
Our	O
data	O
contains	O
higher	O
overlap	O
than	O
the	O
MNLI	B-DatasetName
data	O
,	O
with	O
the	O
bulk	O
of	O
the	O
density	O
falling	O
on	O
minimally	O
distant	O
pairs	O
.	O
We	O
also	O
note	O
a	O
distinct	O
difference	O
between	O
our	O
entailment	O
and	O
non	O
-	O
entailment	O
pairs	O
:	O
non	O
-	O
entailments	O
contain	O
extremely	O
high	O
overlap	O
and	O
are	O
frequently	O
misclassified	O
in	O
these	O
cases	O
where	O
the	O
distance	O
is	O
small	O
,	O
matching	O
previous	O
reports	O
for	O
NLI	O
tasks	O
:	O
lexical	O
overlap	O
is	O
a	O
key	O
artifact	O
for	O
entailment	O
,	O
and	O
this	O
reliance	O
persists	O
when	O
classifying	O
idiomatic	O
pairs	O
.	O

BioNLP	O
-	O
OST	B-DatasetName
2019	O
RDoC	O
Tasks	O
:	O
Multi	O
-	O
grain	O
Neural	O
Relevance	O
Ranking	O
Using	O
Topics	O
and	O
Attention	O
Based	O
Query	O
-	O
Document	O
-	O
Sentence	O
Interactions	O

The	O
scientific	O
research	O
output	O
of	O
the	O
biomedical	O
community	O
is	O
becoming	O
more	O
sub	O
-	O
domain	O
specialized	O
and	O
increasing	O
at	O
a	O
faster	O
pace	O
.	O
Most	O
of	O
the	O
biomedical	O
domain	O
knowledge	O
is	O
in	O
the	O
form	O
of	O
unstructured	O
text	O
data	O
.	O
Natural	O
Language	O
Processing	O
(	O
NLP	O
)	O
techniques	O
such	O
as	O
relation	B-TaskName
extraction	I-TaskName
and	O
information	B-TaskName
retrieval	I-TaskName
have	O
enabled	O
us	O
to	O
effectively	O
mine	O
relevant	O
information	O
from	O
a	O
large	O
corpus	O
.	O
These	O
techniques	O
have	O
significantly	O
reduced	O
the	O
time	O
and	O
effort	O
required	O
for	O
knowledge	O
min	O
-	O
*	O
:	O
Equal	O
Contribution	O
ing	O
and	O
information	O
extraction	O
from	O
past	O
scientific	O
studies	O
and	O
electronic	O
health	O
reports	O
(	O
EHR	O
)	O
.	O
Information	B-TaskName
Retrieval	I-TaskName
(	O
IR	O
)	O
is	O
the	O
process	O
of	O
retrieving	O
relevant	O
information	O
from	O
an	O
unstructured	O
text	O
corpus	O
,	O
which	O
satisfies	O
a	O
given	O
query	O
/	O
requirement	O
,	O
for	O
example	O
Google	B-DatasetName
search	O
,	O
email	O
search	O
,	O
database	O
search	O
etc	O
.	O
This	O
is	O
generally	O
achieved	O
by	O
converting	O
the	O
query	O
and	O
the	O
document	O
collection	O
into	O
an	O
external	O
representation	O
which	O
by	O
preserving	O
the	O
important	O
semantical	O
information	O
can	O
reduce	O
the	O
IR	O
processing	O
time	O
.	O
This	O
external	O
representation	O
can	O
be	O
generated	O
using	O
either	O
statistical	O
approach	O
i.e.	O
,	O
word	O
counts	O
or	O
distributed	O
semantical	O
approach	O
i.e.	O
,	O
word	B-TaskName
embeddings	I-TaskName
.	O
Therefore	O
,	O
there	O
is	O
a	O
motivation	O
to	O
develop	O
such	O
IR	O
system	O
which	O
can	O
understand	O
the	O
specialized	O
sub	O
-	O
domain	O
language	O
and	O
domainspecific	O
jargon	O
of	O
biomedical	O
domain	O
and	O
assist	O
researchers	O
and	O
medical	O
professionals	O
by	O
effectively	O
and	O
efficiently	O
retrieving	O
most	O
relevant	O
information	O
given	O
a	O
query	O
.	O
RDoC	O
Tasks	O
aims	O
at	O
exploring	O
information	B-TaskName
retrieval	I-TaskName
(	O
IR	O
)	O
and	O
information	O
extraction	O
(	O
IE	O
)	O
tasks	O
on	O
selected	O
abstracts	O
from	O
PubMed	O
dataset	O
.	O
While	O
Task	O
-	O
1	O
aims	O
to	O
rank	O
abstracts	O
i.e.	O
,	O
coarse	O
granularity	O
,	O
Task	O
-	O
2	O
aims	O
to	O
rank	O
sentences	O
i.e.	O
,	O
fine	O
granularity	O
and	O
hence	O
the	O
term	O
multi	O
-	O
grain	O
.	O
An	O
RDoC	O
construct	O
combines	O
information	O
from	O
multiple	O
sources	O
like	O
genomics	O
,	O
symptoms	O
,	O
behaviour	O
etc	O
.	O
and	O
therefore	O
,	O
is	O
a	O
much	O
broader	O
way	O
of	O
describing	O
mental	O
health	O
disorders	O
than	O
symptoms	O
based	O
approach	O
.	O
Table	O
1	O
shows	O
the	O
association	O
between	O
PubMed	O
abstracts	O
and	O
RDoC	O
constructs	O
depending	O
on	O
the	O
semantic	O
knowledge	O
of	O
the	O
highlighted	O
content	O
words	O
.	O
Both	O
of	O
these	O
tasks	O
aim	O
in	O
the	O
direction	O
of	O
ease	O
of	O
accessibility	O
of	O
PubMed	O
abstracts	O
labelled	O
with	O
diverse	O
RDoC	O
constructs	O
so	O
that	O
this	O
information	O
can	O
reach	O
its	O
full	O
potential	O
and	O
can	O
be	O
of	O
help	O
to	O
biomedical	O
researchers	O
and	O
healthcare	O
professionals	O
.	O
(	O
PMID	O
)	O
.	O
Highlighted	O
words	O
(	O
blue	O
and	O
red	O
)	O
in	O
each	O
abstract	O
shows	O
content	O
words	O
which	O
together	O
provide	O
the	O
semantic	O
understanding	O
of	O
the	O
corresponding	O
RDoC	O
constructs	O
.	O

The	O
Linguistic	O
Annotation	O
Workshop	O
(	O
LAW	B-DatasetName
)	O
is	O
organized	O
annually	O
by	O
the	O
Association	O
for	O
Computational	O
Linguistics	O
'	O
Special	O
Interest	O
Group	O
for	O
Annotation	O
(	O
ACL	O
SIGANN	O
)	O
.	O
It	O
provides	O
a	O
forum	O
to	O
facilitate	O
the	O
exchange	O
and	O
propagation	O
of	O
research	O
results	O
concerned	O
with	O
the	O
annotation	O
,	O
manipulation	O
,	O
and	O
exploitation	O
of	O
corpora	O
;	O
work	O
towards	O
harmonisation	O
and	O
interoperability	O
from	O
the	O
perspective	O
of	O
the	O
increasingly	O
large	O
number	O
of	O
tools	O
and	O
frameworks	O
for	O
annotated	O
language	O
resources	O
;	O
and	O
work	O
towards	O
a	O
consensus	O
on	O
all	O
issues	O
crucial	O
to	O
the	O
advancement	O
of	O
the	O
field	O
of	O
corpus	O
annotation	O
.	O
These	O
proceedings	O
include	O
papers	O
that	O
were	O
presented	O
at	O
LAW	B-DatasetName
XIII	O
,	O
held	O
in	O
conjunction	O
with	O
the	O
annual	O
meeting	O
of	O
the	O
Association	O
for	O
Computational	O
Linguistics	O
(	O
ACL	O
)	O
in	O
Florence	B-MethodName
,	O
Italy	O
,	O
on	O
August	O
1	O
,	O
2019	O
.	O
The	O
series	O
is	O
now	O
in	O
its	O
thirteenth	O
year	O
.	O
The	O
first	O
workshop	O
took	O
place	O
in	O
2007	O
at	O
the	O
ACL	O
in	O
Prague	O
.	O
Since	O
then	O
,	O
the	O
LAW	B-DatasetName
has	O
been	O
held	O
every	O
year	O
,	O
consistently	O
drawing	O
substantial	O
participation	O
(	O
both	O
in	O
terms	O
of	O
paper	O
/	O
poster	O
submissions	O
and	O
participation	O
in	O
the	O
actual	O
workshop	O
)	O
providing	O
evidence	O
that	O
the	O
LAW	B-DatasetName
's	O
overall	O
focus	O
continues	O
to	O
be	O
an	O
important	O
area	O
of	O
interest	O
in	O
the	O
field	O
.	O
This	O
year	O
's	O
LAW	B-DatasetName
has	O
received	O
52	O
submissions	O
,	O
out	O
of	O
which	O
28	O
papers	O
have	O
been	O
accepted	O
to	O
be	O
presented	O
at	O
the	O
workshop	O
,	O
10	O
as	O
talks	O
and	O
18	O
as	O
posters	O
.	O
In	O
addition	O
to	O
oral	O
and	O
poster	O
paper	O
presentations	O
,	O
LAW	B-DatasetName
XIII	O
also	O
features	O
an	O
invited	O
talk	O
by	O
Rebecca	O
Passonneau	O
and	O
a	O
discussion	O
session	O
.	O
Our	O
thanks	O
go	O
to	O
SIGANN	O
,	O
our	O
organizing	O
committee	O
,	O
for	O
its	O
continuing	O
organization	O
of	O
the	O
LAW	B-DatasetName
workshops	O
,	O
and	O
to	O
the	O
ACL	O
2019	O
workshop	O
chairs	O
for	O
their	O
support	O
.	O
Also	O
,	O
we	O
thank	O
Jet	O
Hoek	O
,	O
the	O
LAW	B-DatasetName
XIII	O
publications	O
chair	O
,	O
for	O
her	O
invaluable	O
help	O
with	O
these	O
proceedings	O
.	O
Most	O
of	O
all	O
,	O
we	O
would	O
like	O
to	O
thank	O
all	O
the	O
authors	O
for	O
submitting	O
their	O
papers	O
to	O
the	O
workshop	O
,	O
and	O
our	O
program	O
committee	O
members	O
for	O
their	O
dedication	O
and	O
their	O
thoughtful	O
reviews	O
.	O

At	O
a	O
high	O
level	O
,	O
the	O
four	O
components	O
of	O
a	O
Hybrid	O
Code	O
Network	O
are	O
a	O
recurrent	O
neural	O
network	O
;	O
domain	O
-	O
specific	O
software	O
;	O
domain	O
-	O
specific	O
action	O
templates	O
;	O
and	O
a	O
conventional	O
entity	O
extraction	O
module	O
for	O
identifying	O
entity	O
mentions	O
in	O
text	O
.	O
Both	O
the	O
RNN	O
and	O
the	O
developer	O
code	O
maintain	O
state	O
.	O
Each	O
action	O
template	O
can	O
be	O
a	O
textual	O
communicative	O
action	O
or	O
an	O
API	O
call	O
.	O
The	O
HCN	O
model	O
is	O
summarized	O
in	O
Figure	O
1	O
.	O
The	O
cycle	O
begins	O
when	O
the	O
user	O
provides	O
an	O
utterance	O
,	O
as	O
text	O
(	O
step	O
1	O
)	O
.	O
The	O
utterance	O
is	O
featurized	O
in	O
several	O
ways	O
.	O
First	O
,	O
a	O
bag	O
of	O
words	O
vector	O
is	O
formed	O
(	O
step	O
2	O
)	O
.	O
Second	O
,	O
an	O
utterance	O
embedding	O
is	O
formed	O
,	O
using	O
a	O
pre	O
-	O
built	O
utterance	O
embedding	O
model	O
(	O
step	O
3	O
)	O
.	O
Third	O
,	O
an	O
entity	O
extraction	O
module	O
identifies	O
entity	O
mentions	O
(	O
step	O
4	O
)	O
-	O
for	O
example	O
,	O
identifying	O
"	O
Jennifer	O
Jones	O
"	O
as	O
a	O
<	O
name	O
>	O
entity	O
.	O
The	O
text	O
and	O
entity	O
mentions	O
are	O
then	O
passed	O
to	O
"	O
Entity	O
tracking	O
"	O
code	O
provided	O
by	O
the	O
developer	O
(	O
step	O
5	O
)	O
,	O
which	O
grounds	O
and	O
maintains	O
entitiesfor	O
example	O
,	O
mapping	O
the	O
text	O
"	O
Jennifer	O
Jones	O
"	O
to	O
a	O
specific	O
row	O
in	O
a	O
database	O
.	O
This	O
code	O
can	O
optionally	O
return	O
an	O
"	O
action	O
mask	O
"	O
,	O
indicating	O
actions	O
which	O
are	O
permitted	O
at	O
the	O
current	O
timestep	O
,	O
as	O
a	O
bit	O
vector	O
.	O
For	O
example	O
,	O
if	O
a	O
target	O
phone	O
number	O
has	O
not	O
yet	O
been	O
identified	O
,	O
the	O
API	O
action	O
to	O
place	O
a	O
phone	O
call	O
may	O
be	O
masked	O
.	O
It	O
can	O
also	O
optionally	O
return	O
"	O
context	O
features	O
"	O
which	O
are	O
features	O
the	O
developer	O
thinks	O
will	O
be	O
useful	O
for	O
distinguish	O
-	O
ing	O
among	O
actions	O
,	O
such	O
as	O
which	O
entities	O
are	O
currently	O
present	O
and	O
which	O
are	O
absent	O
.	O
The	O
feature	O
components	O
from	O
steps	O
1	O
-	O
5	O
are	O
concatenated	O
to	O
form	O
a	O
feature	O
vector	O
(	O
step	O
6	O
)	O
.	O
This	O
vector	O
is	O
passed	O
to	O
an	O
RNN	O
,	O
such	O
as	O
a	O
long	O
shortterm	O
memory	O
(	O
LSTM	B-MethodName
)	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
or	O
gated	B-MethodName
recurrent	I-MethodName
unit	I-MethodName
(	O
GRU	B-MethodName
)	O
(	O
Chung	O
et	O
al	O
,	O
2014	O
)	O
.	O
The	O
RNN	O
computes	O
a	O
hidden	O
state	O
(	O
vector	O
)	O
,	O
which	O
is	O
retained	O
for	O
the	O
next	O
timestep	O
(	O
step	O
8	O
)	O
,	O
and	O
passed	O
to	O
a	O
dense	O
layer	O
with	O
a	O
softmax	B-MethodName
activation	O
,	O
with	O
output	O
dimension	O
equal	O
to	O
the	O
number	O
of	O
distinct	O
system	O
action	O
templates	O
(	O
step	O
9	O
)	O
.	O
1	O
Thus	O
the	O
output	O
of	O
step	O
9	O
is	O
a	O
distribution	O
over	O
action	O
templates	O
.	O
Next	O
,	O
the	O
action	O
mask	O
is	O
applied	O
as	O
an	O
element	O
-	O
wise	O
multiplication	O
,	O
and	O
the	O
result	O
is	O
normalized	O
back	O
to	O
a	O
probability	O
distribution	O
(	O
step	O
10	O
)	O
-	O
this	O
forces	O
non	O
-	O
permitted	O
actions	O
to	O
take	O
on	O
probability	O
zero	O
.	O
From	O
the	O
resulting	O
distribution	O
(	O
step	O
11	O
)	O
,	O
an	O
action	O
is	O
selected	O
(	O
step	O
12	O
)	O
.	O
When	O
RL	O
is	O
active	O
,	O
exploration	O
is	O
required	O
,	O
so	O
in	O
this	O
case	O
an	O
action	O
is	O
sampled	O
from	O
the	O
distribution	O
;	O
when	O
RL	O
is	O
not	O
active	O
,	O
the	O
best	O
action	O
should	O
be	O
chosen	O
,	O
and	O
so	O
the	O
action	O
with	O
the	O
highest	O
probability	O
is	O
always	O
selected	O
.	O
The	O
selected	O
action	O
is	O
next	O
passed	O
to	O
"	O
Entity	O
output	O
"	O
developer	O
code	O
that	O
can	O
substitute	O
in	O
entities	O
(	O
step	O
13	O
)	O
and	O
produce	O
a	O
fully	O
-	O
formed	O
action	O
-	O
for	O
example	O
,	O
mapping	O
the	O
template	O
"	O
<	O
city	O
>	O
,	O
right	O
?	O
"	O
to	O
"	O
Seattle	O
,	O
right	O
?	O
"	O
.	O
In	O
step	O
14	O
,	O
control	O
branches	O
depending	O
on	O
the	O
type	O
of	O
the	O
action	O
:	O
if	O
it	O
is	O
an	O
API	O
action	O
,	O
the	O
corresponding	O
API	O
call	O
in	O
the	O
developer	O
code	O
is	O
invoked	O
(	O
step	O
15	O
)	O
-	O
for	O
example	O
,	O
to	O
render	O
rich	O
content	O
to	O
the	O
user	O
.	O
APIs	O
can	O
act	O
as	O
sensors	O
and	O
return	O
features	O
relevant	O
to	O
the	O
dialog	O
,	O
so	O
these	O
can	O
be	O
added	O
to	O
the	O
feature	O
vector	O
in	O
the	O
next	O
timestep	O
(	O
step	O
16	O
)	O
.	O
If	O
the	O
action	O
is	O
text	O
,	O
it	O
is	O
rendered	O
to	O
the	O
user	O
(	O
step	O
17	O
)	O
,	O
and	O
cycle	O
then	O
repeats	O
.	O
The	O
action	O
taken	O
is	O
provided	O
as	O
a	O
feature	O
to	O
the	O
RNN	O
in	O
the	O
next	O
timestep	O
(	O
step	O
18	O
)	O
.	O

This	O
paper	O
has	O
introduced	O
Hybrid	O
Code	O
Networks	O
for	O
end	O
-	O
to	O
-	O
end	O
learning	O
of	O
task	O
-	O
oriented	O
dialog	O
systems	O
.	O
HCNs	O
support	O
a	O
separation	O
of	O
concerns	O
where	O
procedural	O
knowledge	O
and	O
constraints	O
can	O
be	O
expressed	O
in	O
software	O
,	O
and	O
the	O
control	O
flow	O
is	O
learned	O
.	O
Compared	O
to	O
existing	O
end	O
-	O
to	O
-	O
end	O
approaches	O
,	O
HCNs	O
afford	O
more	O
developer	O
control	O
and	O
require	O
less	O
training	O
data	O
,	O
at	O
the	O
expense	O
of	O
a	O
small	O
amount	O
of	O
developer	O
effort	O
.	O
Results	O
in	O
this	O
paper	O
have	O
explored	O
three	O
different	O
dialog	O
domains	O
.	O
On	O
a	O
public	O
benchmark	O
in	O
the	O
restaurants	O
domain	O
,	O
HCNs	O
exceeded	O
performance	O
of	O
purely	O
learned	O
models	O
.	O
Results	O
in	O
two	O
troubleshooting	O
domains	O
exceeded	O
performance	O
of	O
a	O
commercially	O
deployed	O
rule	O
-	O
based	O
system	O
.	O
Finally	O
,	O
in	O
a	O
name	O
-	O
dialing	O
domain	O
,	O
results	O
from	O
dialog	O
simulation	O
show	O
that	O
HCNs	O
can	O
also	O
be	O
optimized	O
with	O
a	O
mixture	O
of	O
reinforcement	O
and	O
supervised	O
learning	O
.	O
In	O
future	O
work	O
,	O
we	O
plan	O
to	O
extend	O
HCNs	O
by	O
incorporating	O
lines	O
of	O
existing	O
work	O
,	O
such	O
as	O
integrating	O
the	O
entity	O
extraction	O
step	O
into	O
the	O
neural	O
network	O
(	O
Dhingra	O
et	O
al	O
,	O
2017	O
)	O
,	O
adding	O
richer	O
utterance	O
embeddings	O
(	O
Socher	O
et	O
al	O
,	O
2013	O
)	O
,	O
and	O
supporting	O
text	B-TaskName
generation	I-TaskName
(	O
Sordoni	O
et	O
al	O
,	O
2015	O
)	O
.	O
We	O
will	O
also	O
explore	O
using	O
HCNs	O
with	O
automatic	B-TaskName
speech	I-TaskName
recognition	I-TaskName
(	O
ASR	O
)	O
input	O
,	O
for	O
example	O
by	O
forming	O
features	O
from	O
n	O
-	O
grams	O
of	O
the	O
ASR	O
n	O
-	O
best	O
results	O
(	O
Henderson	O
et	O
al	O
,	O
2014b	O
)	O
.	O
Of	O
course	O
,	O
we	O
also	O
plan	O
to	O
deploy	O
the	O
model	O
in	O
a	O
live	O
dialog	O
system	O
.	O
More	O
broadly	O
,	O
HCNs	O
are	O
a	O
general	O
model	O
for	O
stateful	O
control	O
,	O
and	O
we	O
would	O
be	O
interested	O
to	O
explore	O
applications	O
beyond	O
dialog	O
systemsfor	O
example	O
,	O
in	O
NLP	O
medical	O
settings	O
or	O
humanrobot	O
NL	O
interaction	O
tasks	O
,	O
providing	O
domain	O
constraints	O
are	O
important	O
for	O
safety	O
;	O
and	O
in	O
resourcepoor	O
settings	O
,	O
providing	O
domain	O
knowledge	O
can	O
amplify	O
limited	O
data	O
.	O

Discourse	O
Self	O
-	O
Attention	O
for	O
Discourse	O
Element	O
Identification	O
in	O
Argumentative	O
Student	O
Essays	B-DatasetName

Discourse	O
describes	O
how	O
a	O
document	O
is	O
organized	O
.	O
This	O
paper	O
focuses	O
on	O
the	O
task	O
of	O
discourse	O
element	O
identification	O
(	O
DEI	O
)	O
in	O
argumentative	O
student	O
essays	O
.	O
Discourse	O
elements	O
represent	O
the	O
function	O
and	O
contribution	O
of	O
every	O
discourse	O
unit	O
to	O
the	O
discourse	O
.	O
Burstein	O
et	O
al	O
(	O
2003	O
)	O
formulate	O
discourse	O
elements	O
as	O
5	O
categories	O
:	O
introduction	O
,	O
thesis	O
,	O
main	O
idea	O
,	O
supporting	O
and	O
conclusion	O
,	O
while	O
argument	O
components	O
such	O
as	O
major	O
claim	O
,	O
claim	O
and	O
premise	O
are	O
used	O
as	O
discourse	O
elements	O
in	O
argumentation	O
structure	O
parsing	O
in	O
persuasive	O
essays	O
(	O
Stab	O
and	O
Gurevych	O
,	O
2014	O
)	O
.	O
DEI	O
can	O
benefit	O
automated	B-TaskName
essay	I-TaskName
scoring	I-TaskName
in	O
many	O
aspects	O
:	O
modeling	O
organization	O
,	O
inferring	O
topics	O
and	O
opinions	O
or	O
used	O
as	O
features	O
for	O
scoring	O
systems	O
(	O
Attali	O
and	O
Burstein	O
,	O
2006	O
;	O
Burstein	O
et	O
al	O
,	O
2001	O
;	O
Persing	O
et	O
al	O
,	O
2010	O
;	O
Song	O
et	O
al	O
,	O
2020	O
)	O
.	O
Despite	O
its	O
importance	O
,	O
DEI	O
is	O
challenging	O
.	O
First	O
,	O
the	O
ambiguity	O
of	O
sentences	O
makes	O
learning	O
models	O
difficult	O
to	O
distinguish	O
some	O
discourse	O
elements	O
.	O
For	O
example	O
,	O
the	O
thesis	O
is	O
defined	O
as	O
expressing	O
the	O
central	O
claim	O
of	O
the	O
author	O
and	O
the	O
main	O
ideas	O
support	O
the	O
thesis	O
from	O
specific	O
aspects	O
.	O
However	O
,	O
it	O
is	O
hard	O
to	O
distinguish	O
them	O
from	O
their	O
content	O
and	O
style	O
.	O
Second	O
,	O
the	O
discourse	O
element	O
of	O
a	O
specific	O
sentence	O
depends	O
on	O
context	O
.	O
As	O
a	O
result	O
,	O
considering	O
individual	O
sentences	O
only	O
would	O
have	O
difficulties	O
in	O
identifying	O
discourse	O
elements	O
.	O
The	O
relations	O
and	O
relatedness	O
among	O
multiple	O
sentences	O
should	O
be	O
explored	O
.	O
Third	O
,	O
the	O
data	O
imbalance	O
problem	O
is	O
serious	O
,	O
e.g.	O
,	O
the	O
number	O
of	O
elaboration	O
sentences	O
could	O
be	O
10	O
times	O
more	O
than	O
the	O
number	O
of	O
thesis	O
sentences	O
.	O
The	O
minority	O
discourse	O
elements	O
(	O
such	O
as	O
thesis	O
,	O
main	O
ideas	O
or	O
major	O
claim	O
)	O
are	O
harder	O
to	O
be	O
recalled	O
although	O
they	O
have	O
important	O
roles	O
in	O
many	O
scenarios	O
,	O
e.g.	O
,	O
evaluating	O
the	O
organization	O
of	O
essays	O
(	O
Attali	O
and	O
Burstein	O
,	O
2006	O
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
method	O
to	O
explicitly	O
model	O
sentence	O
positions	O
and	O
relations	O
to	O
improve	O
discourse	O
element	O
identification	O
in	O
argumentative	O
student	O
essays	O
.	O
Our	O
idea	O
is	O
partially	O
motivated	O
by	O
the	O
self	O
-	O
attention	O
mechanism	O
such	O
as	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
Self	O
-	O
attention	O
is	O
usually	O
applied	O
to	O
capture	O
dependencies	O
between	O
words	O
.	O
We	O
aim	O
to	O
apply	O
self	O
-	O
attention	O
mechanism	O
to	O
describe	O
relations	O
between	O
sentences	O
.	O
On	O
one	O
hand	O
,	O
position	O
information	O
is	O
important	O
for	O
DEI	O
to	O
give	O
clues	O
on	O
discourse	O
elements	O
beyond	O
content	O
and	O
style	O
,	O
because	O
authors	O
usually	O
hold	O
some	O
conventions	O
to	O
organize	O
content	O
.	O
Position	O
is	O
one	O
of	O
the	O
most	O
useful	O
feature	O
classes	O
in	O
feature	O
-	O
based	O
DEI	O
(	O
Burstein	O
et	O
al	O
,	O
2003	O
;	O
Stab	O
and	O
Gurevych	O
,	O
2014	O
)	O
.	O
Previous	O
neural	O
network	O
models	O
usually	O
cast	O
DEI	O
as	O
a	O
classification	O
or	O
sequence	O
labeling	O
task	O
and	O
do	O
not	O
explicitly	O
model	O
position	O
information	O
.	O
Motivated	O
by	O
the	O
positional	O
encoding	O
of	O
words	O
,	O
we	O
propose	O
a	O
simple	O
structural	O
positional	O
encoding	O
strategy	O
for	O
a	O
sentence	O
by	O
considering	O
its	O
relative	O
position	O
in	O
its	O
essay	O
,	O
relative	O
position	O
of	O
its	O
paragraph	O
in	O
its	O
essay	O
,	O
and	O
its	O
relative	O
position	O
within	O
its	O
paragraph	O
.	O
On	O
the	O
other	O
hand	O
,	O
relatedness	O
among	O
sentences	O
may	O
also	O
indicate	O
properties	O
of	O
discourse	O
elements	O
.	O
For	O
example	O
,	O
thesis	O
sentences	O
should	O
have	O
close	O
relations	O
to	O
the	O
whole	O
essay	O
;	O
main	O
ideas	O
usually	O
locate	O
in	O
similar	O
positions	O
and	O
have	O
high	O
relatedness	O
.	O
Relatedness	O
between	O
discourse	O
elements	O
has	O
shown	O
to	O
be	O
an	O
important	O
indicator	O
of	O
essay	O
coherence	O
(	O
Higgins	O
et	O
al	O
,	O
2004	O
)	O
.	O
We	O
compute	O
inter	O
-	O
sentence	O
attention	O
vectors	O
to	O
represent	O
either	O
element	O
-	O
wise	O
or	O
content	O
-	O
wise	O
relations	O
to	O
other	O
sentences	O
,	O
which	O
bring	O
in	O
additional	O
information	O
beyond	O
individual	O
sentences	O
and	O
enhance	O
sentence	O
representation	O
without	O
extra	O
information	O
.	O
Experiments	O
show	O
that	O
the	O
proposed	O
approach	O
can	O
get	O
considerable	O
improvements	O
compared	O
with	O
feature	O
-	O
based	O
and	O
neural	O
network	O
based	O
baselines	O
on	O
a	O
Chinese	O
dataset	O
and	O
obtain	O
competitive	O
results	O
compared	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
method	O
on	O
an	O
English	O
dataset	O
.	O
The	O
structural	O
positional	O
encodings	O
of	O
sentences	O
show	O
effectiveness	O
to	O
achieve	O
obvious	O
overall	O
improvements	O
.	O
The	O
inter	O
-	O
sentence	O
attention	O
vectors	O
enhance	O
sentence	O
representation	O
helping	O
identify	O
discourse	O
elements	O
as	O
well	O
.	O

DEI	O
could	O
be	O
seen	O
as	O
a	O
subtask	O
in	O
discourse	O
structure	O
analysis	O
.	O
It	O
aims	O
to	O
identify	O
discourse	O
elements	O
,	O
determine	O
their	O
functions	O
and	O
establish	O
relationships	O
among	O
them	O
in	O
an	O
argumentative	O
text	O
.	O
Typical	O
tasks	O
in	O
this	O
line	O
include	O
argumentative	O
zoning	O
(	O
Teufel	O
et	O
al	O
,	O
1999	O
)	O
,	O
argumentation	O
mining	O
(	O
Mochales	O
and	O
Moens	O
,	O
2011	O
;	O
Lippi	O
and	O
Torroni	O
,	O
2016	O
)	O
and	O
analyzing	O
argumentative	O
student	O
essays	O
(	O
Burstein	O
et	O
al	O
,	O
2003	O
;	O
Stab	O
and	O
Gurevych	O
,	O
2014	O
)	O
.	O
Argumentative	O
zoning	O
identifies	O
arguments	O
in	O
scientific	O
articles	O
(	O
Teufel	O
et	O
al	O
,	O
1999	O
;	O
Guo	O
et	O
al	O
,	O
2010	O
)	O
.	O
Argumentation	O
mining	O
aims	O
to	O
identify	O
argument	O
components	O
and	O
relations	O
from	O
legal	O
texts	O
(	O
Palau	O
and	O
Moens	O
,	O
2009	O
;	O
Mochales	O
and	O
Moens	O
,	O
2011	O
)	O
or	O
argumentative	O
texts	O
(	O
Stab	O
and	O
Gurevych	O
,	O
2014	O
;	O
.	O
The	O
solutions	O
to	O
these	O
tasks	O
usually	O
adopt	O
similar	O
machine	O
learning	O
methods	O
but	O
use	O
domain	O
related	O
features	O
.	O
The	O
methods	O
could	O
be	O
roughly	O
classified	O
into	O
the	O
following	O
categories	O
.	O
Classification	B-TaskName
based	O
methods	O
cast	O
DEI	O
as	O
a	O
classification	O
problem	O
.	O
Various	O
classifiers	O
have	O
been	O
tested	O
,	O
such	O
as	O
SVM	B-MethodName
(	O
Stab	O
and	O
Gurevych	O
,	O
2014	O
)	O
,	O
decision	O
trees	O
(	O
Burstein	O
et	O
al	O
,	O
2003	O
(	O
Burstein	O
et	O
al	O
,	O
,	O
2001	O
and	O
naive	O
Bayes	O
,	O
maximum	O
entropy	O
model	O
(	O
Moens	O
et	O
al	O
,	O
2007	O
;	O
Palau	O
and	O
Moens	O
,	O
2009	O
)	O
.	O
Sequence	O
labeling	O
based	O
methods	O
exploit	O
contextual	O
information	O
for	O
DEI	O
with	O
conditional	O
random	O
fields	O
(	O
Hirohata	O
et	O
al	O
,	O
2008	O
;	O
Song	O
et	O
al	O
,	O
2015	O
)	O
or	O
recurrent	O
neural	O
networks	O
.	O
Establishing	O
relations	O
between	O
sentences	O
is	O
often	O
viewed	O
as	O
a	O
classification	O
tasks	O
as	O
well	O
(	O
Stab	O
and	O
Gurevych	O
,	O
2014	O
)	O
.	O
Parsing	O
based	O
methods	O
are	O
also	O
adopted	O
to	O
build	O
more	O
complex	O
structures	O
with	O
techniques	O
like	O
ILP	O
or	O
RST	O
style	O
parsing	O
(	O
Peldszus	O
and	O
Stede	O
,	O
2015	O
)	O
.	O
Feature	B-TaskName
engineering	I-TaskName
.	O
Some	O
common	O
features	O
are	O
shared	O
across	O
these	O
tasks	O
,	O
including	O
syntactic	O
,	O
lexical	O
,	O
semantic	O
and	O
discourse	O
relations	O
.	O
There	O
are	O
also	O
domain	O
related	O
features	O
to	O
further	O
boost	O
the	O
performance	O
.	O
Mochales	O
and	O
Moens	O
(	O
2011	O
)	O
designed	O
special	O
features	O
for	O
argumentation	O
mining	O
in	O
legal	O
texts	O
.	O
Nguyen	O
and	O
Litman	O
(	O
2015	O
)	O
identified	O
claims	O
based	O
on	O
domain	O
words	O
.	O
Lippi	O
and	O
Torroni	O
(	O
2015	O
)	O
modeled	O
syntactic	O
structures	O
for	O
content	O
independent	O
claim	O
detection	O
based	O
on	O
tree	O
kernels	O
.	O
Our	O
work	O
is	O
mostly	O
related	O
to	O
DEI	O
in	O
argumentative	O
student	O
essays	O
(	O
Burstein	O
et	O
al	O
,	O
2003	O
;	O
Stab	O
and	O
Gurevych	O
,	O
2014	O
)	O
,	O
which	O
is	O
useful	O
for	O
qualifying	O
essay	O
organization	O
(	O
Persing	O
et	O
al	O
,	O
2010	O
)	O
,	O
argumentation	O
(	O
Persing	O
and	O
Ng	O
,	O
2016	O
;	O
Wachsmuth	O
et	O
al	O
,	O
2016	O
)	O
and	O
general	O
writing	O
(	O
Burstein	O
et	O
al	O
,	O
2003	O
;	O
Ong	O
et	O
al	O
,	O
2014	O
;	O
Song	O
et	O
al	O
,	O
2014	O
)	O
.	O
The	O
major	O
feature	O
classes	O
proposed	O
by	O
Burstein	O
et	O
al	O
(	O
2003	O
)	O
and	O
Stab	O
and	O
Gurevych	O
(	O
2014	O
)	O
are	O
used	O
to	O
build	O
a	O
baseline	O
.	O
The	O
features	O
include	O
:	O
position	O
,	O
cue	O
words	O
,	O
lexical	O
features	O
(	O
main	O
verbs	O
,	O
adverbs	O
and	O
connectives	O
)	O
and	O
structural	O
features	O
(	O
such	O
as	O
number	O
of	O
clauses	O
)	O
.	O
Some	O
of	O
these	O
features	O
are	O
based	O
on	O
manually	O
collected	O
lexicons	O
.	O
Deep	O
Learning	O
Methods	O
have	O
achieved	O
great	O
success	O
in	O
many	O
NLP	O
tasks	O
.	O
proposed	O
neural	O
argumentation	O
mining	O
models	O
based	O
on	O
sequence	O
tagging	O
or	O
dependency	B-TaskName
parsing	I-TaskName
.	O
It	O
exploits	O
inter	O
-	O
sentence	O
relations	O
but	O
needs	O
sophisticated	O
language	O
processing	O
.	O
exploited	O
CNN	O
and	O
LSTM	B-MethodName
for	O
classifying	O
sentences	O
to	O
identify	O
claims	O
from	O
different	O
domains	O
.	O
It	O
mainly	O
depends	O
on	O
the	O
content	O
of	O
components	O
but	O
does	O
not	O
sufficiently	O
model	O
positions	O
and	O
exploit	O
inter	O
-	O
sentence	O
relatedness	O
.	O

Vaswani	O
et	O
al	O
(	O
2017	O
)	O
proposed	O
the	O
self	O
-	O
attention	O
mechanism	O
and	O
achieved	O
state	O
of	O
the	O
art	O
results	O
in	O
many	O
NLP	O
tasks	O
.	O
Since	O
then	O
,	O
self	O
-	O
attention	O
has	O
drawn	O
increasing	O
interests	O
due	O
to	O
flexibility	O
in	O
modeling	O
long	O
range	O
interactions	O
.	O
Self	O
-	O
attention	O
ignores	O
word	O
order	O
in	O
a	O
sentence	O
.	O
As	O
a	O
result	O
,	O
position	O
representations	O
are	O
developed	O
to	O
cooperate	O
with	O
self	O
-	O
attention	O
.	O
In	O
addition	O
to	O
the	O
sinusoidal	O
position	O
representation	O
proposed	O
by	O
Vaswani	O
et	O
al	O
(	O
2017	O
)	O
,	O
there	O
are	O
also	O
other	O
variations	O
to	O
bias	O
the	O
selection	O
of	O
attentive	O
regions	O
(	O
Shen	O
et	O
al	O
,	O
2018	O
;	O
Shaw	O
et	O
al	O
,	O
2018	O
;	O
Yang	O
et	O
al	O
,	O
2019	O
)	O
.	O
In	O
NLP	O
,	O
self	O
-	O
attention	O
is	O
mostly	O
applied	O
to	O
sequential	O
structures	O
such	O
as	O
a	O
sequence	O
of	O
words	O
.	O
Mihaylov	O
and	O
Frank	O
(	O
2019	O
)	O
proposed	O
a	O
discourse	O
-	O
aware	O
selfattention	O
encoder	O
for	O
reading	B-TaskName
comprehension	I-TaskName
on	O
narrative	O
texts	O
,	O
where	O
event	O
chains	O
,	O
discourse	O
relations	O
and	O
coreference	O
relations	O
are	O
used	O
for	O
connecting	O
sentences	O
.	O
Self	O
-	O
attention	O
can	O
be	O
also	O
extended	O
to	O
2d	O
-	O
dimensions	O
for	O
image	O
processing	O
(	O
Parmar	O
et	O
al	O
,	O
2018	O
)	O
and	O
lattice	O
inputs	O
(	O
Sperber	O
et	O
al	O
,	O
2019	O
)	O
.	O

We	O
use	O
Hierarchical	O
BiLSTM	B-MethodName
(	O
HBiLSTM	O
)	O
,	O
which	O
is	O
similar	O
to	O
(	O
Yang	O
et	O
al	O
,	O
2016	O
)	O
,	O
as	O
the	O
base	O
model	O
to	O
model	O
sentence	O
and	O
discourse	O
level	O
representations	O
.	O
The	O
task	O
is	O
to	O
assign	O
discourse	O
element	O
labels	O
y	O
=	O
(	O
y	O
1	O
,	O
...	O
,	O
y	O
n	O
)	O
to	O
sentences	O
(	O
x	O
1	O
,	O
...	O
,	O
x	O
n	O
)	O
in	O
a	O
text	O
,	O
where	O
x	O
i	O
,	O
1	O
≤	O
i	O
≤	O
n	O
,	O
is	O
a	O
sentence	O
of	O
a	O
sequence	O
of	O
words	O
and	O
y	O
i	O
Y	O
,	O
Y	O
is	O
a	O
set	O
of	O
pre	O
-	O
defined	O
discourse	O
elements	O
.	O

A	O
sequence	O
of	O
words	O
x	O
=	O
{	O
w	O
1	O
,	O
...	O
,	O
w	O
N	O
}	O
is	O
modeled	O
with	O
a	O
RNN	O
encoder	O
and	O
is	O
converted	O
into	O
a	O
sequence	O
of	O
hidden	O
states	O
H	O
=	O
{	O
h	O
1	O
,	O
...	O
,	O
h	O
N	O
}	O
.	O
The	O
hidden	O
state	O
at	O
the	O
i	O
-	O
th	O
step	O
is	O
h	O
i	O
=	O
f	O
(	O
e	O
(	O
w	O
i	O
)	O
,	O
h	O
i−1	O
)	O
,	O
(	O
1	O
)	O
where	O
f	O
is	O
a	O
RNN	O
unit	O
,	O
e	O
(	O
w	O
i	O
)	O
R	O
d	O
is	O
the	O
embedding	O
of	O
a	O
word	O
,	O
and	O
h	O
i−1	O
is	O
the	O
hidden	O
state	O
of	O
the	O
previous	O
step	O
.	O
The	O
whole	O
sequence	O
could	O
be	O
represented	O
as	O
a	O
fixed	O
length	O
vector	O
c	O
=	O
φ	O
(	O
{	O
h	O
1	O
,	O
,	O
h	O
N	O
}	O
)	O
to	O
represent	O
the	O
semantic	O
of	O
a	O
sentence	O
,	O
where	O
φ	O
(	O
)	O
is	O
a	O
function	O
to	O
summarize	O
hidden	O
states	O
.	O
In	O
this	O
work	O
,	O
Long	B-MethodName
Short	I-MethodName
-	I-MethodName
Term	I-MethodName
Memory	I-MethodName
(	O
LSTM	B-MethodName
)	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
is	O
used	O
as	O
the	O
RNN	O
unit	O
and	O
the	O
sequence	O
is	O
encoded	O
in	O
a	O
Bidirectional	O
way	O
that	O
a	O
hidden	O
state	O
h	O
i	O
=	O
[	O
−	O
h	O
i	O
;	O
−	O
h	O
i	O
]	O
is	O
the	O
concatenation	O
of	O
the	O
corresponding	O
hidden	O
states	O
from	O
both	O
directions	O
.	O
The	O
summarization	B-TaskName
function	O
φ	O
(	O
)	O
could	O
be	O
based	O
on	O
the	O
attention	O
mechanism	O
.	O

In	O
the	O
discourse	O
element	O
layer	O
,	O
we	O
feed	O
sentence	O
representations	O
C	O
=	O
(	O
c	O
1	O
,	O
...	O
,	O
c	O
n	O
)	O
R	O
d×n	O
to	O
a	O
BiL	O
-	O
STM	O
and	O
use	O
a	O
nonlinear	O
layer	O
to	O
map	O
semantic	O
representations	O
to	O
discourse	O
element	O
representations	O
,	O
D	O
=	O
tanh	O
(	O
BiLSTM	B-MethodName
(	O
C	O
)	O
)	O
.	O
(	O
2	O
)	O

Finally	O
,	O
we	O
use	O
a	O
linear	O
and	O
a	O
softmax	B-MethodName
layer	O
to	O
predict	O
the	O
discourse	O
element	O
of	O
every	O
sentence	O
,	O
Y	O
=	O
softmax	B-MethodName
(	O
linear	O
(	O
D	O
)	O
)	O
,	O
(	O
3	O
)	O
where	O
Y	O
R	O
|	O
Y	O
|	O
×n	O
refers	O
to	O
the	O
probabilities	O
of	O
every	O
sentence	O
over	O
discourse	O
element	O
categories	O
.	O
The	O
baseline	O
mainly	O
exploits	O
interactions	O
between	O
adjacent	O
sentences	O
,	O
but	O
long	O
distance	O
interactions	O
and	O
sentence	O
positions	O
are	O
not	O
explicitly	O
considered	O
,	O
which	O
may	O
be	O
also	O
important	O
to	O
determine	O
the	O
function	O
of	O
sentences	O
in	O
argumentative	O
discourse	O
.	O

We	O
propose	O
the	O
Discourse	O
Self	O
-	O
Attention	O
(	O
DiSA	O
)	O
layer	O
to	O
improve	O
the	O
baseline	O
by	O
explicitly	O
modeling	O
sentence	O
positions	O
and	O
inter	O
-	O
sentence	O
interactions	O
.	O
The	O
architecture	O
is	O
illustrated	O
in	O
Figure	O
1	O
.	O
The	O
sentences	O
in	O
an	O
essay	O
are	O
converted	O
to	O
sentence	B-TaskName
embeddings	I-TaskName
C	O
through	O
the	O
BiLSTM	B-MethodName
encoder	O
introduced	O
in	O
Section	O
3.1	O
,	O
which	O
are	O
used	O
as	O
the	O
input	O
of	O
DiSA	O
.	O
DiSA	O
explicitly	O
represents	O
sentence	O
positions	O
,	O
which	O
are	O
integrated	O
with	O
the	O
content	O
representations	O
of	O
sentences	O
to	O
get	O
element	O
representations	O
.	O
DiSA	O
also	O
has	O
an	O
inter	O
-	O
sentence	O
attention	O
module	O
to	O
get	O
both	O
element	O
-	O
wise	O
and	O
content	O
-	O
wise	O
attention	O
vectors	O
of	O
sentences	O
to	O
capture	O
sentence	O
interactions	O
.	O
The	O
attention	O
vectors	O
and	O
element	O
representations	O
are	O
concatenated	O
and	O
fed	O
to	O
a	O
linear	B-MethodName
layer	I-MethodName
and	O
a	O
softmax	B-MethodName
layer	O
for	O
prediction	O
.	O

We	O
also	O
use	O
the	O
English	O
student	B-DatasetName
essay	I-DatasetName
dataset	O
released	O
by	O
.	O
This	O
dataset	O
marks	O
argument	O
components	O
,	O
i.e.	O
,	O
major	O
claim	O
,	O
claim	O
,	O
and	O
premise	O
,	O
at	O
clause	O
level	O
.	O
Table	O
2	O
shows	O
an	O
example	O
sentence	O
.	O
The	O
consecutive	O
words	O
in	O
bold	O
form	O
three	O
components	O
,	O
corresponding	O
to	O
claim	O
,	O
major	O
claim	O
and	O
premise	O
,	O
respectively	O
.	O
Because	O
our	O
model	O
is	O
at	O
sentence	O
level	O
,	O
we	O
convert	O
the	O
original	O
annotations	O
to	O
sentence	O
level	O
.	O
First	O
,	O
an	O
essay	O
is	O
split	O
into	O
sentences	O
by	O
NLTK	O
.	O
Then	O
if	O
a	O
sentence	O
contains	O
only	O
one	O
argument	O
component	O
,	O
we	O
annotate	O
this	O
sentence	O
as	O
the	O
type	O
of	O
this	O
component	O
;	O
if	O
a	O
sentence	O
contains	O
more	O
than	O
one	O
argument	O
component	O
,	O
we	O
further	O
separate	O
it	O
into	O
multiple	O
sentences	O
to	O
ensure	O
that	O
each	O
sentence	O
has	O
only	O
one	O
argument	O
.	O
The	O
beginning	O
of	O
a	O
new	O
sentence	O
is	O
from	O
the	O
end	O
of	O
the	O
last	O
component	O
.	O
The	O
end	O
of	O
a	O
new	O
sentence	O
is	O
the	O
end	O
of	O
the	O
component	O
it	O
contains	O
.	O
As	O
shown	O
in	O
Table	O
2	O
,	O
three	O
sentences	O
s	O
1	O
,	O
s	O
2	O
and	O
s	O
3	O
are	O
generated	O
from	O
the	O
original	O
example	O
sentence	O
.	O
If	O
a	O
sentence	O
does	O
not	O
have	O
any	O
argument	O
component	O
,	O
its	O
label	O
is	O
other	O
.	O
Table	O
3	O
shows	O
the	O
basic	O
statistics	O
of	O
the	O
converted	O
dataset	O
.	O

We	O
compare	O
with	O
the	O
following	O
systems	O
.	O
Feature	O
-	O
based	O
.	O
We	O
adapt	O
features	O
from	O
previous	O
feature	O
-	O
based	O
methods	O
(	O
Burstein	O
et	O
al	O
,	O
2003	O
;	O
Stab	O
and	O
Gurevych	O
,	O
2014	O
;	O
Song	O
et	O
al	O
,	O
2015	O
)	O
to	O
build	O
a	O
feature	O
-	O
based	O
CRF	B-MethodName
model	O
.	O
HBiLSTM	O
.	O
The	O
baseline	O
described	O
in	O
Section	O
3	O
uses	O
two	O
BiLSTM	B-MethodName
layers	O
to	O
encode	O
word	O
sequences	O
and	O
sentences	O
.	O
BERT	B-MethodName
.	O
We	O
fine	O
-	O
tune	O
BERT	B-MethodName
on	O
training	O
data	O
to	O
train	O
a	O
sentence	O
classifier	O
,	O
because	O
the	O
lengths	O
of	O
many	O
Chinese	O
essays	O
exceed	O
the	O
length	O
constraint	O
of	O
BERT	B-MethodName
and	O
it	O
is	O
expensive	O
to	O
train	O
BERT	B-MethodName
-	O
like	O
models	O
at	O
discourse	O
level	O
.	O

Less	O
is	O
More	O
:	O
Attention	O
Supervision	O
with	O
Counterfactuals	O
for	O
Text	B-TaskName
Classification	I-TaskName

We	O
aim	O
to	O
leverage	O
human	O
and	O
machine	O
intelligence	O
together	O
for	O
attention	O
supervision	O
.	O
Specifically	O
,	O
we	O
show	O
that	O
human	O
annotation	O
cost	O
can	O
be	O
kept	O
reasonably	O
low	O
,	O
while	O
its	O
quality	O
can	O
be	O
enhanced	O
by	O
machine	O
selfsupervision	O
.	O
Specifically	O
,	O
for	O
this	O
goal	O
,	O
we	O
explore	O
the	O
advantage	O
of	O
counterfactual	O
reasoning	O
,	O
over	O
associative	O
reasoning	O
typically	O
used	O
in	O
attention	O
supervision	O
.	O
Our	O
empirical	O
results	O
show	O
that	O
this	O
machine	O
-	O
augmented	O
human	O
attention	O
supervision	O
is	O
more	O
effective	O
than	O
existing	O
methods	O
requiring	O
a	O
higher	O
annotation	O
cost	O
,	O
in	O
text	B-TaskName
classification	I-TaskName
tasks	O
,	O
including	O
sentiment	B-TaskName
analysis	I-TaskName
and	O
news	O
categorization	O
.	O

The	O
practical	O
importance	O
of	O
attention	O
mechanism	O
has	O
been	O
well	O
-	O
established	O
,	O
for	O
both	O
(	O
a	O
)	O
improving	O
NLP	O
models	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
,	O
and	O
also	O
(	O
b	O
)	O
enhancing	O
human	O
understanding	O
of	O
models	O
(	O
Serrano	O
and	O
Smith	O
,	O
2019	O
;	O
Wiegreffe	O
and	O
Pinter	O
,	O
2019	O
)	O
.	O
This	O
paper	O
pursues	O
the	O
former	O
direction	O
,	O
but	O
unlike	O
existing	O
models	O
,	O
typically	O
using	O
attention	O
in	O
"	O
unsupervised	O
"	O
nature	O
.	O
Adding	O
human	O
supervision	O
to	O
attention	O
has	O
been	O
shown	O
to	O
improve	O
model	O
predictions	O
and	O
explanations	O
(	O
Jain	O
and	O
Wallace	O
,	O
2019	O
)	O
.	O
For	O
example	O
,	O
consider	O
a	O
review	O
in	O
(	O
Tang	O
et	O
al	O
,	O
2019	O
)	O
"	O
this	O
place	O
is	O
small	O
and	O
crowded	O
but	O
the	O
service	O
is	O
quick	O
"	O
.	O
Models	O
with	O
unsupervised	O
attention	O
may	O
attend	O
highly	O
on	O
"	O
quick	O
"	O
,	O
a	O
generic	O
strong	O
signal	O
for	O
restaurant	O
reviews	O
,	O
but	O
one	O
may	O
supervise	O
to	O
focus	O
on	O
"	O
crowded	O
"	O
to	O
guide	O
models	O
to	O
predict	O
a	O
negative	O
sentiment	O
correctly	O
.	O
For	O
this	O
goal	O
,	O
attention	O
supervision	O
task	O
(	O
Yu	O
et	O
al	O
,	O
2017	O
;	O
Liu	O
et	O
al	O
,	O
2017	O
)	O
treats	O
attention	O
as	O
output	O
variables	O
so	O
that	O
models	O
can	O
be	O
trained	O
to	O
generate	O
similar	O
attention	O
to	O
human	O
supervision	O
.	O
We	O
categorize	O
such	O
human	O
supervision	O
into	O
the	O
following	O
two	O
levels	O
:	O
Sample	O
level	O
rationale	O
:	O
In	O
the	O
above	O
example	O
,	O
whether	O
to	O
attend	O
on	O
quick	O
or	O
crowded	O
depends	O
on	O
the	O
ground	O
-	O
truth	O
sentiment	O
class	O
.	O
Human	O
annotator	O
is	O
required	O
to	O
examine	O
each	O
training	O
sample	O
,	O
and	O
highlight	O
important	O
words	O
specific	O
to	O
a	O
sample	O
and	O
its	O
class	O
label	O
.	O
Task	O
level	O
:	O
An	O
alternative	O
with	O
lower	O
annotation	O
overhead	O
would	O
be	O
annotating	O
vocabulary	O
,	O
separately	O
from	O
training	O
samples	O
.	O
That	O
is	O
,	O
both	O
quick	O
and	O
crowded	O
are	O
annotated	O
to	O
attend	O
,	O
since	O
both	O
have	O
high	O
importance	O
for	O
the	O
target	O
task	O
of	O
sentiment	O
classification	O
.	O
A	O
naive	O
belief	O
would	O
be	O
assuming	O
the	O
former	O
with	O
a	O
higher	O
annotation	O
cost	O
is	O
more	O
effective	O
at	O
supervising	O
the	O
model	O
's	O
attention	O
.	O
Our	O
key	O
claim	O
,	O
in	O
contrast	O
,	O
is	O
that	O
requiring	O
more	O
annotation	O
,	O
or	O
,	O
sample	O
-	O
specific	O
supervision	O
,	O
can	O
be	O
less	O
effective	O
than	O
requiring	O
less	O
from	O
human	O
then	O
augmenting	O
it	O
by	O
machine	O
(	O
less	O
-	O
is	O
-	O
more	O
-	O
hypothesis	O
)	O
.	O
Similar	O
skepticism	O
on	O
asking	O
more	O
,	O
or	O
sample	O
-	O
level	O
rationales	O
from	O
humans	O
,	O
was	O
explored	O
in	O
(	O
Bao	O
et	O
al	O
,	O
2018	O
)	O
,	O
where	O
machine	O
attention	O
from	O
large	O
additional	O
annotations	O
was	O
more	O
effective	O
supervisions	O
than	O
rationales	O
.	O
In	O
this	O
paper	O
,	O
we	O
validate	O
less	O
-	O
is	O
-	O
more	O
without	O
additional	O
annotation	O
overhead	O
,	O
by	O
proposing	O
a	O
holistic	O
approach	O
of	O
combining	O
both	O
human	O
annotation	O
and	O
machine	O
attention	O
.	O
Key	O
distinctions	O
from	O
(	O
Bao	O
et	O
al	O
,	O
2018	O
)	O
are	O
(	O
a	O
)	O
humans	O
annotate	O
even	O
less	O
,	O
and	O
(	O
b	O
)	O
without	O
additional	O
training	O
resources	O
.	O
Specifically	O
,	O
we	O
start	O
by	O
loosening	O
the	O
definition	O
of	O
human	O
annotation	O
(	O
Camburu	O
et	O
al	O
,	O
2018	O
;	O
Zhong	O
et	O
al	O
,	O
2019	O
)	O
into	O
the	O
task	O
-	O
level	O
annotation	O
:	O
it	O
reduces	O
annotation	O
cost	O
to	O
the	O
size	O
of	O
vocabulary	O
,	O
or	O
often	O
to	O
zero	O
,	O
when	O
public	O
resources	O
such	O
as	O
sentiment	O
lexicon	O
replace	O
such	O
annotation	O
.	O
We	O
show	O
the	O
effectiveness	O
of	O
this	O
zero	O
-	O
cost	O
supervision	O
,	O
for	O
both	O
sentiment	O
classification	O
and	O
news	O
categorization	O
scenarios	O
,	O
after	O
our	O
proposed	O
adaptation	O
.	O
Our	O
adaptation	O
goal	O
is	O
an	O
unsupervised	O
adaptation	O
of	O
task	O
-	O
level	O
human	O
annotation	O
to	O
samplelevel	O
supervision	O
signals	O
for	O
attention	O
/	O
classification	O
models	O
.	O
Specifically	O
,	O
we	O
propose	O
Sample	O
-	O
level	O
AttentioN	O
Adaptation	O
(	O
SANA	O
)	O
.	O
Specifically	O
,	O
for	O
self	O
-	O
supervising	O
such	O
adaptation	O
,	O
SANA	O
conducts	O
what	O
-	O
if	O
tests	O
per	O
each	O
sample	O
,	O
of	O
whether	O
the	O
permutation	O
on	O
human	O
annotation	O
changes	O
the	O
machine	O
prediction	O
.	O
That	O
is	O
,	O
we	O
collect	O
the	O
counterfactual	O
(	O
machine	O
)	O
supervisions	O
for	O
free	O
,	O
by	O
observing	O
whether	O
highly	O
attended	O
word	O
by	O
human	O
leads	O
to	O
the	O
same	O
machine	O
prediction	O
,	O
compared	O
to	O
when	O
such	O
attention	O
is	O
counterfactually	O
lowered	O
.	O
In	O
such	O
a	O
case	O
,	O
SANA	O
supervises	O
to	O
reduce	O
the	O
importance	O
of	O
the	O
word	O
.	O
We	O
validate	O
such	O
counterfactual	O
signals	O
are	O
missing	O
pieces	O
for	O
adapting	O
word	O
importance	O
to	O
sample	O
-	O
specific	O
prediction	O
.	O
We	O
evaluate	O
SANA	O
on	O
three	O
popular	O
datasets	O
,	O
SST2	B-DatasetName
,	O
IMDB	B-DatasetName
,	O
and	O
20NG	O
.	O
In	O
all	O
of	O
the	O
text	B-TaskName
classification	I-TaskName
datasets	O
,	O
SANA	O
achieves	O
significant	O
improvements	O
over	O
baselines	O
,	O
using	O
unsupervised	O
attention	O
or	O
supervised	O
with	O
task	O
-	O
or	O
sample	O
-	O
level	O
human	O
annotations	O
,	O
in	O
the	O
following	O
four	O
dimensions	O
:	O
Models	O
supervised	O
by	O
SANA	O
predict	O
more	O
accurately	O
,	O
explain	O
causality	O
of	O
attention	O
better	O
,	O
and	O
are	O
more	O
robust	O
over	O
adversarial	O
attacks	O
,	O
and	O
more	O
tolerant	O
of	O
the	O
scarcity	O
of	O
training	O
samples	O
.	O

Sample	O
-	O
level	O
annotation	O
is	O
reportedly	O
too	O
expensive	O
in	O
many	O
practical	O
settings	O
(	O
Zhong	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
is	O
far	O
difficult	O
for	O
humans	O
to	O
capture	O
the	O
dependency	O
with	O
corresponding	O
class	O
labels	O
.	O
In	O
contrast	O
,	O
annotators	O
may	O
select	O
important	O
words	O
for	O
a	O
target	O
task	O
,	O
namely	O
task	O
-	O
level	O
attention	O
annotation	O
(	O
Def	O
.	O
3.1	O
)	O
,	O
without	O
looking	O
up	O
individual	O
samples	O
and	O
their	O
labels	O
.	O
Definition	O
3.1	O
(	O
Task	O
-	O
level	O
Attention	O
Annotation	O
)	O
Assuming	O
the	O
existence	O
of	O
the	O
vocabulary	O
V	O
,	O
the	O
vocab	O
-	O
level	O
annotation	O
A	O
task	O
{	O
0	B-DatasetName
,	O
1	O
}	O
|	O
V	O
|	O
is	O
a	O
binary	O
vector	O
of	O
the	O
hard	O
selection	O
for	O
words	O
in	O
V	O
,	O
i.e.	O
,	O
∀w	O
t	O
V	O
:	O
A	O
task	O
(	O
w	O
t	O
)	O
{	O
0	B-DatasetName
,	O
1	O
}	O
.	O
Based	O
on	O
A	O
task	O
,	O
when	O
given	O
an	O
input	O
sample	O
x	O
,	O
we	O
can	O
use	O
a	O
proxy	O
of	O
the	O
sample	O
-	O
level	O
annotation	O
A	O
,	O
i.e.	O
,	O
∀w	O
t	O
x	O
:	O
A	O
(	O
w	O
t	O
)	O
=	O
A	O
task	O
(	O
w	O
t	O
)	O
.	O
As	O
shown	O
in	O
Tab	O
.	O
1	O
,	O
the	O
annotation	O
space	O
,	O
which	O
is	O
referred	O
to	O
as	O
a	O
word	O
set	O
size	O
for	O
annotation	O
,	O
is	O
10∼36	O
times	O
smaller	O
at	O
task	O
-	O
level	O
than	O
at	O
samplelevel	O
.	O
Generally	O
,	O
the	O
vocabulary	O
size	O
is	O
far	O
smaller	O
than	O
the	O
total	O
number	O
of	O
word	O
occurrences	O
in	O
training	O
samples	O
.	O
Our	O
goal	O
is	O
thus	O
to	O
keep	O
annotation	O
cost	O
cognitively	O
reasonable	O
(	O
Zou	O
et	O
al	O
,	O
2018	O
;	O
Zhao	O
et	O
al	O
,	O
2018	O
)	O
,	O
leaving	O
machine	O
self	O
-	O
supervision	O
to	O
close	O
the	O
annotation	O
quality	O
gap	O
(	O
Sec	O
.	O
3.1	O
and	O
3.2	O
)	O
.	O
Meanwhile	O
,	O
we	O
present	O
a	O
setup	O
of	O
zero	O
-	O
cost	O
supervision	O
,	O
which	O
allows	O
us	O
attention	O
supervision	O
without	O
any	O
human	O
efforts	O
in	O
all	O
scenarios	O
using	O
public	O
resources	O
and	O
tools	O
(	O
Sec	O
.	O
3.3	O
)	O
.	O

From	O
this	O
point	O
on	O
,	O
for	O
task	O
-	O
level	O
supervision	O
,	O
we	O
assume	O
zero	O
-	O
cost	O
human	O
annotation	O
efforts	O
,	O
either	O
by	O
using	O
public	O
resources	O
or	O
self	O
-	O
supervision	O
.	O
Supervision	O
by	O
public	O
resources	O
Task	O
-	O
level	O
annotation	O
are	O
often	O
publicly	O
available	O
as	O
resources	O
or	O
tools	O
.	O
For	O
example	O
,	O
sentiment	O
lexicon	O
(	O
Esuli	O
and	O
Sebastiani	O
,	O
2006	O
)	O
consists	O
of	O
sentiment	O
words	O
,	O
which	O
are	O
important	O
to	O
the	O
sentiment	O
classification	O
task	O
,	O
and	O
named	O
-	O
entity	O
recognizer	O
(	O
NER	B-TaskName
)	O
(	O
Peters	O
et	O
al	O
,	O
2017	O
)	O
can	O
collect	O
entity	O
words	O
commonly	O
attended	O
in	O
news	O
categorization	O
task	O
.	O
We	O
empirically	O
show	O
that	O
both	O
lexicon	O
and	O
NER	B-TaskName
can	O
be	O
adequate	O
substitutes	O
for	O
the	O
manual	O
task	O
-	O
level	O
annotation	O
.	O

To	O
validate	O
the	O
effectiveness	O
of	O
SANA	O
,	O
we	O
use	O
the	O
following	O
three	O
text	B-TaskName
classification	I-TaskName
datasets	O
,	O
which	O
are	O
widely	O
used	O
Jain	O
and	O
Wallace	O
,	O
2019	O
)	O
and	O
statistically	O
diverse	O
as	O
well	O
.	O
We	O
split	O
the	O
official	O
training	O
split	O
into	O
90	O
%	O
and	O
10	O
%	O
as	O
training	O
and	O
validation	O
sets	O
respectively	O
.	O
We	O
expect	O
SANA	O
in	O
two	O
-	O
sentence	O
tasks	O
,	O
such	O
as	O
SNLI	B-DatasetName
and	O
MPQA	B-DatasetName
,	O
would	O
be	O
promising	O
,	O
which	O
we	O
leave	O
as	O
future	O
work	O
.	O
SST2	B-DatasetName
(	O
Socher	O
et	O
al	O
,	O
2013	O
)	O
:	O
Stanford	O
Sen	O
-	O
timent	O
Treebank	O
provides	O
around	O
11	O
K	O
sentences	O
tagged	O
with	O
sentiment	O
on	O
a	O
scale	O
from	O
1	O
(	O
most	O
negative	O
)	O
to	O
5	O
(	O
most	O
positive	O
)	O
.	O
We	O
filter	O
out	O
neutral	O
samples	O
and	O
dichotomize	O
the	O
remaining	O
sentences	O
into	O
positive	O
(	O
4	O
,	O
5	O
)	O
and	O
negative	O
(	O
1	O
,	O
2	O
)	O
.	O
We	O
set	O
the	O
maximum	O
sequence	O
length	O
as	O
30	O
.	O
IMDB	B-DatasetName
(	O
Maas	O
et	O
al	O
,	O
2011	O
)	O
:	O
IMDB	B-DatasetName
Large	O
Movie	O
Review	O
Corpus	O
is	O
a	O
binary	O
sentiment	O
classification	O
dataset	O
containing	O
50	O
K	O
polarized	O
(	O
positive	O
or	O
negative	O
)	O
movie	O
reviews	O
,	O
split	O
into	O
half	O
for	O
training	O
and	O
testing	O
.	O
We	O
set	O
the	O
maximum	O
sequence	O
length	O
as	O
180	O
.	O
20NG	O
:	O
20	B-DatasetName
Newsgroups	I-DatasetName
2	O
contains	O
around	O
19	O
K	O
documents	O
evenly	O
categorized	O
into	O
20	O
different	O
categories	O
.	O
Following	O
(	O
Jain	O
and	O
Wallace	O
,	O
2019	O
)	O
,	O
we	O
extract	O
samples	O
belonging	O
to	O
baseball	O
and	O
hockey	O
classes	O
,	O
which	O
we	O
designate	O
as	O
0	B-DatasetName
and	O
1	O
,	O
deriving	O
a	O
binary	O
classification	O
task	O
(	O
Hockey	O
vs	O
Baseball	O
)	O
.	O
We	O
set	O
the	O
maximum	O
sequence	O
length	O
as	O
300	O
.	O

This	O
section	O
studies	O
whether	O
attention	O
,	O
after	O
supervision	O
,	O
is	O
more	O
effective	O
for	O
human	O
consumption	O
as	O
model	O
explanation	O
.	O
Existing	O
metrics	O
for	O
explainability	O
measure	O
whether	O
attention	O
correlates	O
with	O
(	O
a	O
)	O
class	O
prediction	O
or	O
(	O
b	O
)	O
feature	B-TaskName
importance	I-TaskName
,	O
discussed	O
in	O
the	O
next	O
sections	O
respectively	O
.	O

One	O
measure	O
for	O
the	O
explainability	O
of	O
attention	O
is	O
whether	O
each	O
attention	O
weight	O
captures	O
the	O
causality	O
of	O
word	O
and	O
class	O
prediction	O
,	O
by	O
permuting	O
words	O
and	O
observing	O
prediction	O
changes	O
.	O
If	O
the	O
learning	O
is	O
successful	O
,	O
such	O
causal	O
signals	O
should	O
be	O
consistently	O
observed	O
in	O
the	O
test	O
predictions	O
.	O
To	O
validate	O
this	O
,	O
we	O
employ	O
the	O
attention	O
-	O
permutation	O
experiments	O
designed	O
in	O
(	O
Jain	O
and	O
Wallace	O
,	O
2019	O
)	O
,	O
i.e.	O
,	O
what	O
-	O
if	O
simulation	O
.	O
Specifically	O
,	O
when	O
given	O
an	O
input	O
sample	O
in	O
the	O
test	O
phase	O
,	O
we	O
look	O
into	O
whether	O
the	O
randomly	O
mutated	O
attention	O
(	O
i.e.	O
,	O
cause	O
)	O
from	O
the	O
original	O
attention	O
yields	O
any	O
changes	O
in	O
the	O
corresponding	O
prediction	O
result	O
(	O
i.e.	O
,	O
effect	O
)	O
.	O
Here	O
,	O
T	O
V	O
D	O
for	O
the	O
permutation	O
can	O
be	O
regarded	O
as	O
a	O
desirable	O
evaluation	O
measure	O
:	O
as	O
T	O
V	O
D	O
is	O
lower	O
,	O
the	O
(	O
original	O
)	O
learned	O
attention	O
has	O
a	O
weak	O
mapping	O
with	O
the	O
model	O
prediction	O
,	O
and	O
vice	O
versa	O
.	O
The	O
results	O
are	O
presented	O
in	O
Fig	O
.	O
2	O
,	O
where	O
x	O
-	O
axis	O
refers	O
to	O
T	O
V	O
D	O
values	O
,	O
i.e.	O
,	O
the	O
difference	O
of	O
model	O
predictions	O
,	O
and	O
y	O
-	O
axis	O
refers	O
to	O
the	O
frequency	O
of	O
what	O
-	O
if	O
simulations	O
on	O
their	O
returning	O
T	O
V	O
D	O
value	O
.	O
To	O
carefully	O
analyze	O
this	O
,	O
we	O
divide	O
the	O
simulation	O
results	O
by	O
four	O
different	O
intervals	O
of	O
input	O
sequence	O
length	O
,	O
which	O
can	O
be	O
an	O
influencing	O
factor	O
:	O
as	O
the	O
perturbations	O
on	O
longer	O
texts	O
are	O
unlikely	O
to	O
make	O
prediction	O
changes	O
(	O
Sen	O
et	O
al	O
,	O
2020	O
)	O
.	O
In	O
this	O
figure	O
,	O
we	O
can	O
observe	O
that	O
SANA	O
has	O
the	O
lowest	O
frequency	O
on	O
T	O
V	O
D	O
=	O
0	B-DatasetName
in	O
all	O
cases	O
,	O
showing	O
the	O
distribution	O
skewed	O
to	O
larger	O
T	O
V	O
D	O
(	O
i.e.	O
,	O
right	O
on	O
x	O
-	O
axis	O
)	O
compared	O
to	O
baselines	O
.	O
Such	O
distribution	O
suggests	O
that	O
attention	O
in	O
SANA	O
strongly	O
affects	O
model	O
prediction	O
by	O
the	O
causal	O
signals	O
.	O
In	O
unsupervised	O
and	O
vocab	O
(	O
i.e.	O
,	O
task	O
-	O
level	O
)	O
,	O
the	O
distributions	O
are	O
skewed	O
to	O
lower	O
T	O
V	O
D	O
(	O
i.e.	O
,	O
left	O
on	O
x	O
-	O
axis	O
)	O
,	O
having	O
larger	O
frequency	O
on	O
zero	O
T	O
V	O
D	O
than	O
SANA	O
.	O
These	O
patterns	O
indicate	O
the	O
baselines	O
have	O
weak	O
attentions	O
loosely	O
aligned	O
to	O
model	O
predictions	O
,	O
motivating	O
SANA	O
even	O
working	O
well	O
in	O
long	O
texts	O
.	O

As	O
an	O
alternative	O
metric	O
of	O
attention	O
explainablity	O
,	O
(	O
Jain	O
and	O
Wallace	O
,	O
2019	O
)	O
considers	O
the	O
relationship	O
between	O
attention	O
weights	O
and	O
gradient	O
-	O
based	O
feature	B-TaskName
importance	I-TaskName
score	O
of	O
each	O
word	O
.	O
However	O
,	O
prior	O
research	O
suggests	O
using	O
word	O
as	O
a	O
unit	O
of	O
importance	O
feature	O
is	O
rather	O
artificial	O
,	O
as	O
word	O
is	O
contextualized	O
by	O
,	O
and	O
interacts	O
with	O
other	O
words	O
:	O
(	O
Wiegreffe	O
and	O
Pinter	O
,	O
2019	O
)	O
observes	O
such	O
limitation	O
,	O
and	O
Shapley	O
(	O
Chen	O
et	O
al	O
,	O
2018	O
)	O
measures	O
interaction	O
between	O
features	O
for	O
capturing	O
dependency	O
of	O
arbitrary	O
subsets	O
.	O
For	O
this	O
purpose	O
,	O
we	O
report	O
the	O
KL	O
divergence	O
between	O
C	O
-	O
Shapley	O
4	O
and	O
attention	O
weights	O
,	O
D	O
KL	O
(	O
Shapley	O
(	O
x	O
)	O
|	O
|	O
attention	O
(	O
x	O
)	O
)	O
.	O
We	O
present	O
the	O
results	O
in	O
Tab	O
.	O
4	O
,	O
showing	O
SANA	O
approach	O
is	O
the	O
most	O
well	O
correlated	O
method	O
with	O
Shapley	O
scores	O
,	O
well	O
capturing	O
word	O
dependency	O
.	O
Intuitively	O
,	O
C	O
-	O
Shapley	O
observes	O
the	O
interaction	O
in	O
n	O
-	O
gram	O
,	O
and	O
our	O
work	O
,	O
attending	O
upon	O
hidden	O
representations	O
of	O
RNN	O
,	O
which	O
are	O
soft	O
n	O
-	O
grams	O
,	O
captures	O
similar	O
interactions	O
.	O
This	O
result	O
manifests	O
that	O
,	O
standing	O
on	O
self	O
-	O
supervision	O
signals	O
,	O
our	O
counterfactual	O
process	O
can	O
improve	O
the	O
explanation	O
on	O
the	O
contextualization	O
ability	O
of	O
RNN	O
architectures	O
.	O

This	O
work	O
is	O
supported	O
by	O
AI	O
Graduate	O
School	O
Program	O
(	O
2020	O
-	O
0	B-DatasetName
-	O
01361	O
)	O
and	O
IITP	O
grant	O
(	O
No.2017	O
-	O
0	B-DatasetName
-	O
01779	O
,	O
XAI	O
)	O
supervised	O
by	O
IITP	O
.	O
Hwang	O
is	O
a	O
corresponding	O
author	O
.	O

The	O
FST	O
described	O
here	O
is	O
primarily	O
built	O
by	O
Jack	O
Rueter	O
,	O
beginning	O
with	O
work	O
in	O
the	O
1990s	O
.	O
The	O
Komi	O
-	O
Zyrian	O
finite	O
-	O
state	O
description	O
began	O
with	O
⁶http://videocorpora.ru	O
a	O
trilingual	O
glossary	O
Ӧшкамӧшка	O
ичӧт	O
кыввор	O
,	O
комиа	O
-	O
англискӧя	O
-	O
финскӧя	O
(	O
Rueter	O
,	O
1995	O
)	O
,	O
designed	O
for	O
use	O
by	O
Finnish	O
and	O
English	O
speaking	O
students	O
of	O
Komi	O
,	O
without	O
previous	O
knowledge	O
of	O
Russian	O
,	O
to	O
accompany	O
the	O
коми	O
кыв	O
'	O
Komi	O
language	O
'	O
reader	O
(	O
Цыпанов	O
,	O
1992	O
)	O
,	O
used	O
for	O
instruction	O
in	O
the	O
Universities	O
of	O
Helsinki	O
and	O
Turku	O
.	O
Later	O
,	O
with	O
a	O
scholarship	O
from	O
the	O
Kordelin	O
Foundation	O
,	O
this	O
vocabulary	O
was	O
augmented	O
.	O
First	O
,	O
the	O
extension	O
was	O
intended	O
to	O
complement	O
a	O
second	O
Komi	O
reader	O
by	O
Манова	O
(	O
1994	O
)	O
,	O
and	O
then	O
to	O
outline	O
the	O
Komi	O
stem	O
vocabulary	O
of	O
the	O
Komi	O
-	O
Russian	O
dictionary	O
by	O
Лыткин	O
and	O
Тимушев	O
(	O
1961	O
)	O
.	O
A	O
large	O
portion	O
of	O
the	O
work	O
done	O
with	O
this	O
dictionary	O
was	O
only	O
possible	O
with	O
the	O
painstaking	O
hours	O
spent	O
by	O
Vera	O
Chernykh	O
.	O
Thus	O
,	O
the	O
approximately	O
3000	O
-	O
word	O
glossary	O
providing	O
the	O
lexical	O
base	O
for	O
a	O
finite	O
-	O
state	O
description	O
of	O
Komi	O
-	O
Zyrian	O
,	O
presented	O
at	O
Permistika	O
6	O
at	O
the	O
Udmurt	O
State	O
University	O
in	O
Izhevsk	O
,	O
1996	O
(	O
published	O
in	O
Rueter	O
,	O
2000	O
,	O
was	O
extended	O
to	O
over	O
6000	O
lexical	O
entries	O
.	O
In	O
2004	O
Trond	O
Trosterud	O
invited	O
Rueter	O
to	O
Tromsø	O
to	O
learn	O
more	O
about	O
the	O
Xerox	O
Finite	O
-	O
state	O
technology	O
(	O
XFST	O
)	O
being	O
implemented	O
at	O
Giellatekno	O
as	O
described	O
in	O
(	O
Trosterud	O
,	O
2004	O
)	O
and	O
for	O
Komi	O
in	O
(	O
Trosterud	O
,	O
2004b	O
)	O
.	O
Here	O
the	O
Komi	O
transducer	O
and	O
lexicon	O
were	O
to	O
be	O
developed	O
further	O
than	O
before	O
,	O
and	O
to	O
be	O
connected	O
to	O
an	O
infrastructure	O
that	O
was	O
compatible	O
with	O
a	O
larger	O
array	O
of	O
languages	O
.	O
To	O
summarise	O
some	O
of	O
the	O
new	O
improvements	O
,	O
there	O
were	O
no	O
longer	O
problems	O
with	O
Cyrillic	O
letters	O
requiring	O
representation	O
as	O
conversions	O
from	O
Latin	O
letters	O
.	O
It	O
was	O
now	O
possible	O
to	O
write	O
rules	O
directly	O
addressing	O
elements	O
of	O
the	O
Komi	O
orthography	O
.	O
This	O
direct	O
use	O
of	O
the	O
vernacular	O
in	O
the	O
code	O
may	O
have	O
,	O
in	O
fact	O
,	O
contributed	O
to	O
the	O
belief	O
of	O
the	O
developer	O
that	O
only	O
the	O
normative	O
language	O
needed	O
description	O
.	O
(	O
It	O
was	O
not	O
until	O
many	O
years	O
later	O
that	O
work	O
with	O
other	O
under	O
-	O
resourced	O
languages	O
,	O
such	O
as	O
Mansi	O
(	O
2015present	O
)	O
,	O
Olonets	O
-	O
Karelian	O
(	O
2013	O
-	O
present	O
)	O
,	O
Skolt	O
Saami	O
(	O
2015	O
and	O
Võro	O
(	O
2014	O
-	O
present	O
)	O
,	O
made	O
it	O
obvious	O
that	O
non	O
-	O
standard	O
words	O
also	O
require	O
description	O
.	O
)	O
One	O
of	O
the	O
most	O
important	O
items	O
at	O
this	O
point	O
was	O
that	O
the	O
lexicon	O
and	O
morphology	O
were	O
open	O
-	O
source	O
.	O
This	O
meant	O
,	O
in	O
turn	O
,	O
that	O
Komi	O
could	O
be	O
worked	O
on	O
by	O
others	O
and	O
tested	O
in	O
projects	O
.	O
Here	O
,	O
Komi	O
was	O
ideal	O
.	O
The	O
morphology	O
is	O
very	O
concatenative	O
,	O
and	O
the	O
orthography	O
contains	O
only	O
two	O
more	O
letters	O
than	O
the	O
Russian	O
,	O
i.e.	O
problems	O
with	O
some	O
rarer	O
Cyrillic	O
letters	O
could	O
be	O
evaluated	O
and	O
solved	O
.	O
In	O
2012	O
-	O
2016	O
Paula	O
Kokkonen	O
worked	O
in	O
conjunction	O
with	O
one	O
of	O
Rueter	O
's	O
projects	O
,	O
where	O
she	O
improved	O
the	O
Finnish	O
translations	O
and	O
inspected	O
the	O
English	O
translations	O
for	O
Komi	O
lexemes	O
.	O
This	O
work	O
significantly	O
increased	O
the	O
coverage	O
of	O
Finnish	O
translations	O
in	O
the	O
multilingual	O
dictionary	O
that	O
was	O
created	O
in	O
this	O
point	O
.	O
During	O
the	O
period	O
2012	O
-	O
2021	O
,	O
FU	O
-	O
Lab	O
and	O
Giellatekno	O
collaboration	O
has	O
featured	O
active	O
FST	O
development	O
,	O
including	O
multiple	O
use	O
,	O
and	O
especially	O
improvement	O
in	O
the	O
disambiguation	O
rules	O
and	O
lexical	O
coverage	O
.	O
Morphological	B-TaskName
analysis	I-TaskName
is	O
a	O
central	O
component	O
in	O
a	O
modern	O
corpus	O
,	O
and	O
issues	O
such	O
as	O
ambiguity	O
are	O
also	O
always	O
present	O
when	O
FST	O
is	O
used	O
in	O
this	O
context	O
(	O
Ӧньӧ	O
Лав	O
,	O
2015	O
,	O
140	O
)	O
.	O
Collaboration	O
may	O
also	O
lead	O
to	O
unforeseeable	O
development	O
.	O
When	O
two	O
infrastructures	O
are	O
aligned	O
,	O
there	O
are	O
often	O
competing	O
priorities	O
.	O
This	O
has	O
also	O
been	O
the	O
case	O
here	O
,	O
i.e.	O
whereas	O
FU	O
-	O
Lab	O
has	O
demonstrated	O
immediate	O
interest	O
in	O
the	O
facilitation	O
of	O
writing	O
,	O
spell	O
checking	O
,	O
dictionaries	O
and	O
corpora	O
for	O
the	O
language	O
community	O
,	O
Giellatekno	O
has	O
pushed	O
for	O
research	O
-	O
related	O
morphological	O
description	O
,	O
analysis	O
and	O
lexica	O
for	O
the	O
research	O
community	O
,	O
but	O
which	O
can	O
,	O
in	O
fact	O
,	O
later	O
be	O
applied	O
to	O
the	O
production	O
of	O
spell	O
checking	O
and	O
other	O
derivative	O
tools	O
.	O
This	O
divergence	O
in	O
priority	O
lead	O
to	O
some	O
duplicate	O
work	O
in	O
morphology	O
.	O
Helsinki	O
Finite	O
-	O
State	O
Technology	O
(	O
HFST	O
)	O
(	O
Lindén	O
et	O
al	O
,	O
2013	O
)	O
at	O
Giellatekno	O
with	O
multi	O
-	O
use	O
priorities	O
was	O
pitted	O
against	O
the	O
quick	O
but	O
single	O
-	O
use	O
Hunspell	O
strategies	O
practiced	O
at	O
FU	O
-	O
Lab	O
.	O
Thus	O
,	O
some	O
of	O
the	O
technical	O
complexities	O
on	O
the	O
Giellatekno	O
side	O
had	O
to	O
be	O
simplified	O
so	O
that	O
one	O
set	O
of	O
lexica	O
might	O
be	O
shared	O
.	O
Giellatekno	O
had	O
plenty	O
to	O
gain	O
from	O
the	O
lexical	O
work	O
done	O
at	O
FU	O
-	O
Lab	O
,	O
on	O
the	O
one	O
hand	O
,	O
but	O
it	O
was	O
not	O
able	O
to	O
capitalize	O
on	O
its	O
own	O
sophisticated	O
two	O
-	O
level	O
description	O
as	O
a	O
result	O
of	O
it	O
,	O
on	O
the	O
other	O
.	O
As	O
regards	O
morphophonological	O
descriptions	O
,	O
stem	O
-	O
final	O
variation	O
had	O
to	O
be	O
moved	O
one	O
step	O
away	O
from	O
the	O
initial	O
LEMMA	B-DatasetName
+	O
COLON	O
+	O
STEM	O
+	O
CONTINUATIONLEXICON	O
declaration	O
in	O
the	O
code	O
.	O
While	O
Jack	O
Rueter	O
has	O
often	O
quickly	O
followed	O
the	O
suggestion	O
of	O
XML	O
maintenance	O
of	O
lexical	O
materials	O
,	O
it	O
has	O
turned	O
out	O
that	O
collaboration	O
pulls	O
away	O
from	O
this	O
write	O
-	O
only	O
-	O
once	O
policy	O
.	O
The	O
more	O
people	O
there	O
are	O
working	O
with	O
one	O
data	O
set	O
,	O
the	O
more	O
documentation	O
required	O
for	O
maintaining	O
mutual	O
working	O
principles	O
.	O
Simple	O
and	O
complex	O
XML	O
systems	O
alike	O
require	O
a	O
working	O
front	O
-	O
end	O
,	O
otherwise	O
,	O
as	O
has	O
been	O
the	O
case	O
here	O
,	O
the	O
workers	O
opt	O
out	O
of	O
the	O
XML	O
database	O
and	O
end	O
up	O
working	O
more	O
on	O
materials	O
that	O
can	O
not	O
be	O
readily	O
integrated	O
back	O
into	O
the	O
system	O
.	O
At	O
the	O
moment	O
the	O
XML	O
transformation	O
is	O
not	O
being	O
used	O
in	O
FST	O
development	O
.	O
Instead	O
,	O
other	O
solutions	O
for	O
database	O
implementation	O
are	O
being	O
worked	O
on	O
,	O
see	O
Alnajjar	O
et	O
al	O
(	O
2020a	O
)	O
;	O
.	O
Only	O
time	O
will	O
reveal	O
which	O
directions	O
of	O
development	O
have	O
contributed	O
the	O
most	O
to	O
the	O
infrastructure	O
.	O
In	O
2018	O
-	O
2021	O
,	O
Niko	O
Partanen	O
has	O
been	O
improving	O
the	O
dialectal	O
lexicon	O
coverage	O
of	O
the	O
transducer	O
while	O
conducting	O
his	O
doctoral	O
studies	O
in	O
Komi	O
dialectology	O
.	O
In	O
connection	O
to	O
this	O
work	O
,	O
in	O
2020	O
-	O
2021	O
,	O
Jack	O
Rueter	O
has	O
improved	O
the	O
coverage	O
of	O
dialectal	O
morphology	O
,	O
specifically	O
taking	O
into	O
account	O
the	O
phenomena	O
found	O
in	O
the	O
Izhma	O
dialect	O
.	O
This	O
work	O
by	O
both	O
of	O
them	O
was	O
done	O
within	O
a	O
Kone	O
Foundation	O
funded	O
research	O
project	O
Language	O
Documentation	O
Meets	O
Language	O
Technology	O
:	O
The	O
Next	O
Step	O
in	O
the	O
Description	O
of	O
Komi	O
.	O
The	O
work	O
shows	O
that	O
it	O
is	O
a	O
feasible	O
strategy	O
to	O
improve	O
the	O
analyser	O
so	O
that	O
the	O
work	O
aligns	O
with	O
specific	O
goals	O
and	O
needs	O
of	O
an	O
individual	O
project	O
or	O
dataset	O
.	O
It	O
does	O
create	O
an	O
imbalance	O
in	O
to	O
which	O
degree	O
different	O
dialects	O
are	O
represented	O
,	O
but	O
for	O
a	O
language	O
as	O
large	O
as	O
Komi	O
doing	O
everything	O
at	O
the	O
same	O
time	O
is	O
not	O
possible	O
either	O
.	O
Mika	O
Hämäläinen	O
's	O
role	O
has	O
been	O
central	O
in	O
building	O
more	O
widely	O
accessible	O
computational	O
infrastructure	O
to	O
access	O
these	O
transducers	O
(	O
Hämäläinen	O
,	O
2019	O
)	O
.	O
In	O
the	O
recent	O
work	O
to	O
create	O
an	O
online	O
editing	O
platform	O
that	O
would	O
allow	O
improved	O
access	O
to	O
the	O
lexical	O
materials	O
,	O
Khalid	O
Alnajjar	O
has	O
been	O
in	O
an	O
irreplaceable	O
position	O
(	O
Alnajjar	O
et	O
al	O
,	O
2020a	O
)	O
.	O
This	O
all	O
shows	O
that	O
managing	O
a	O
transducer	O
for	O
a	O
language	O
like	O
Komi	O
is	O
a	O
multi	O
-	O
partnered	O
operation	O
that	O
calls	O
for	O
wide	O
collaboration	O
between	O
different	O
groups	O
and	O
even	O
infrastructures	O
.	O
Since	O
2017	O
,	O
work	O
has	O
been	O
conducted	O
within	O
the	O
Universal	B-DatasetName
Dependencies	I-DatasetName
project	O
to	O
better	O
cover	O
Komi	O
varieties	O
,	O
most	O
recently	O
(	O
Zeman	O
et	O
al	O
,	O
2021	O
)	O
,	O
see	O
also	O
Nivre	O
et	O
al	O
(	O
2020	O
)	O
.	O
There	O
are	O
two	O
Zyrian	O
treebanks	O
(	O
Partanen	O
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
work	O
with	O
Permyak	O
progresses	O
at	O
many	O
levels	O
(	O
Rueter	O
et	O
al	O
,	O
2020c	O
)	O
.	O
Especially	O
in	O
the	O
initial	O
phase	O
of	O
the	O
treebank	O
,	O
building	O
the	O
finite	O
-	O
state	O
descriptions	O
is	O
in	O
a	O
pivotal	O
role	O
,	O
and	O
maintaining	O
interoperability	O
between	O
the	O
FST	O
and	O
treebank	O
development	O
allows	O
very	O
efficient	O
use	O
of	O
both	O
systems	O
.	O
A	O
similar	O
approach	O
has	O
also	O
been	O
systematically	O
used	O
for	O
other	O
languages	O
,	O
such	O
as	O
Karelian	O
(	O
Pirinen	O
,	O
2019a	O
)	O
and	O
both	O
Mordvinic	O
languages	O
.	O
Indeed	O
,	O
managing	O
systematic	O
and	O
comparable	O
use	O
of	O
tags	O
and	O
conventions	O
across	O
languages	O
is	O
one	O
of	O
the	O
primary	O
concerns	O
in	O
our	O
work	O
as	O
well	O
,	O
and	O
there	O
have	O
been	O
specific	O
surveys	O
that	O
try	O
to	O
track	O
the	O
progress	O
of	O
different	O
Uralic	O
treebanks	O
(	O
Rueter	O
and	O
Partanen	O
,	O
2019	O
)	O
.	O
We	O
can	O
also	O
mention	O
that	O
the	O
practices	O
described	O
here	O
have	O
also	O
be	O
adopted	O
for	O
the	O
development	O
of	O
Amazon	O
minority	O
language	O
description	O
for	O
Apurinã	O
in	O
Helsinki	O
-	O
Belém	O
.	O
In	O
the	O
approach	O
discussed	O
here	O
,	O
this	O
harmonization	O
starts	O
at	O
the	O
transducer	O
level	O
and	O
the	O
documentation	O
therein	O
.	O
In	O
the	O
context	O
of	O
concrete	O
applications	O
of	O
the	O
Komi	O
FST	O
,	O
we	O
can	O
highlight	O
work	O
by	O
Gerstenberger	O
et	O
al	O
(	O
2017	O
)	O
,	O
where	O
the	O
analyser	O
was	O
integrated	O
into	O
the	O
popular	O
multimedia	O
annotation	O
software	O
ELAN	O
.	O
In	O
addition	O
,	O
the	O
most	O
significant	O
Komi	O
online	O
resource	O
,	O
the	O
National	O
Komi	O
Corpus	O
,	O
contains	O
annotations	O
done	O
with	O
the	O
transducer⁷.	O
Next	O
we	O
describe	O
some	O
of	O
the	O
challenges	O
and	O
important	O
phenomena	O
that	O
have	O
been	O
addressed	O
in	O
various	O
ways	O
when	O
creating	O
the	O
Komi	O
analyser	O
.	O

The	O
Komi	O
-	O
Zyrian	O
language	O
is	O
known	O
to	O
display	O
a	O
typologically	O
common	O
l	O
-	O
vocalization	O
,	O
which	O
is	O
a	O
process	O
where	O
a	O
lateral	O
approximant	O
is	O
replaced	O
by	O
a	O
labiodental	O
fricative	O
/v/	O
or	O
labiodental	O
approximant	O
/ʋ/.	O
In	O
the	O
Komi	O
grammaticography	O
this	O
is	O
known	O
as	O
l	O
/	O
v	O
variation	O
.	O
Another	O
comparable	O
stem	O
-	O
alterating	O
⁷http://komicorpora.ru	O
phenomena	O
are	O
the	O
paragogic	O
consonants	O
in	O
some	O
word	O
stems	O
.	O
These	O
phenomena	O
can	O
be	O
dealt	O
with	O
in	O
much	O
the	O
same	O
way	O
,	O
as	O
they	O
share	O
a	O
common	O
trigger	O
.	O
Words	O
with	O
l	O
/	O
v	O
or	O
paragogic	O
consonant	O
variation	O
in	O
their	O
stems	O
can	O
be	O
identified	O
on	O
the	O
basis	O
of	O
whether	O
the	O
stem	O
is	O
followed	O
by	O
an	O
vowel	O
-	O
initial	O
suffix	O
,	O
on	O
the	O
one	O
hand	O
,	O
or	O
a	O
consonant	O
-	O
initial	O
suffix	O
(	O
alternatively	O
word	O
boundary	O
)	O
,	O
on	O
the	O
other	O
.	O
In	O
the	O
description	O
of	O
these	O
words	O
it	O
has	O
been	O
suggested	O
that	O
erroneous	O
forms	O
be	O
specifically	O
identified	O
.	O
Special	O
tags	O
indicating	O
the	O
absence	O
of	O
paragogic	O
consonants	O
or	O
substandard	O
realization	O
of	O
the	O
stem	O
-	O
final	O
l	O
/	O
v	O
have	O
been	O
implemented	O
for	O
Komi	O
-	O
Zyrian	O
and	O
reflect	O
parallel	O
tags	O
previously	O
implemented	O
in	O
the	O
FST	O
descriptions	O
of	O
other	O
languages	O
in	O
the	O
GiellaLT	O
infrastructure	O
,	O
Northern	O
and	O
Skolt	O
Saami	O
,	O
Erzya	O
,	O
Moksha	O
,	O
Võro	O
to	O
mention	O
a	O
few	O
.	O
When	O
we	O
include	O
more	O
dialectal	O
materials	O
in	O
the	O
description	O
,	O
we	O
also	O
have	O
to	O
account	O
for	O
processes	O
where	O
l	O
-	O
vocalization	O
triggers	O
vowel	O
lengthening	O
.	O
There	O
are	O
also	O
secondary	O
types	O
of	O
l	O
-	O
vocalization	O
,	O
influencing	O
stems	O
ending	O
in	O
the	O
sequence	O
/	O
-	O
el/	O
,	O
and	O
triggering	O
change	O
/	O
-	O
ej/.	O
Currently	O
this	O
is	O
treated	O
at	O
the	O
lemma	B-DatasetName
level	O
,	O
so	O
that	O
the	O
non	O
-	O
standard	O
forms	O
are	O
connected	O
to	O
the	O
standard	O
lemmas	O
,	O
with	O
an	O
additional	O
tag	O
indicating	O
dialectal	O
form	O
or	O
error	O
.	O
Even	O
the	O
dialectal	O
variants	O
where	O
neither	O
types	O
of	O
the	O
variation	O
are	O
met	O
are	O
exceptions	O
from	O
the	O
point	O
of	O
view	O
of	O
the	O
standard	O
language	O
.	O
We	O
have	O
devised	O
a	O
tagging	O
system	O
for	O
various	O
subtypes	O
,	O
but	O
the	O
exact	O
implementation	O
is	O
still	O
being	O
designed	O
and	O
planned	O
further	O
.	O
We	O
discuss	O
in	O
Section	O
4.2	O
related	O
challenges	O
in	O
more	O
detail	O
.	O

In	O
recent	O
years	O
many	O
neural	O
network	O
based	O
approaches	O
have	O
been	O
becoming	O
popular	O
and	O
also	O
shown	O
good	O
results	O
.	O
In	O
a	O
recent	O
study	O
by	O
Pirinen	O
(	O
2019b	O
)	O
the	O
neural	O
models	O
were	O
better	O
than	O
the	O
traditional	O
rule	O
-	O
based	O
approaches	O
for	O
Finnish	O
.	O
Our	O
team	O
is	O
always	O
following	O
new	O
developments	O
of	O
the	O
field	O
,	O
but	O
we	O
also	O
believe	O
that	O
different	O
approaches	O
can	O
be	O
successfully	O
combined	O
.	O
We	O
already	O
see	O
studies	O
emerging	O
where	O
a	O
neural	O
network	O
has	O
been	O
used	O
to	O
learn	O
to	O
generate	O
predictions	O
from	O
an	O
FST	O
.	O
Their	O
research	O
is	O
also	O
used	O
the	O
Komi	O
-	O
Zyrian	O
FST	O
presented	O
in	O
this	O
paper	O
.	O
The	O
results	O
were	O
promising	O
and	O
we	O
are	O
eager	O
to	O
see	O
how	O
this	O
ideology	O
of	O
using	O
neural	O
networks	O
and	O
rule	O
-	O
based	O
systems	O
side	O
by	O
side	O
rather	O
than	O
as	O
competing	O
systems	O
plays	O
out	O
in	O
the	O
future	O
.	O
For	O
the	O
NLP	O
pipeline	O
of	O
Komi	O
the	O
most	O
important	O
new	O
developments	O
will	O
be	O
connected	O
to	O
improvements	O
in	O
the	O
dependency	B-TaskName
parsing	I-TaskName
side	O
of	O
the	O
analysis	O
,	O
ideally	O
in	O
connection	O
to	O
automatic	O
and	O
rule	O
-	O
based	O
methods	O
of	O
disambiguation	O
.	O
Komi	O
Constraint	O
Grammar	O
has	O
currently	O
focused	O
to	O
disambiguation	O
,	O
and	O
the	O
tagging	O
and	O
parsing	O
sections	O
are	O
largely	O
missing	O
.	O
It	O
remains	O
to	O
be	O
seen	O
what	O
kind	O
of	O
an	O
approach	O
will	O
be	O
the	O
most	O
successful	O
here	O
.	O
At	O
the	O
same	O
time	O
Komi	O
Universal	B-DatasetName
Dependencies	I-DatasetName
treebanks	O
have	O
started	O
to	O
be	O
large	O
enough	O
that	O
their	O
further	O
modeling	O
with	O
deep	O
learning	O
starts	O
to	O
be	O
an	O
attractive	O
and	O
possibly	O
fruitful	O
task	O
.	O
Komi	O
texts	O
are	O
also	O
present	O
in	O
many	O
different	O
orthographies	O
,	O
and	O
taking	O
all	O
of	O
them	O
into	O
account	O
is	O
a	O
large	O
and	O
important	O
task	O
(	O
Rueter	O
and	O
Ponomareva	O
,	O
2019	O
)	O
.	O
Since	O
the	O
corpora	O
of	O
Latin	O
Komi	O
texts	O
are	O
also	O
now	O
available⁹	O
,	O
the	O
future	O
for	O
these	O
lines	O
of	O
research	O
is	O
exciting	O
and	O
promising	O
.	O
This	O
also	O
connects	O
to	O
various	O
transcription	O
systems	O
used	O
in	O
linguistic	O
publications	O
and	O
text	O
collections	O
:	O
these	O
materials	O
should	O
be	O
republished	O
in	O
the	O
contemporary	O
orthography	O
in	O
order	O
to	O
make	O
them	O
maximally	O
useful	O
for	O
the	O
language	O
communities	O
themselves	O
.	O
Yet	O
another	O
future	O
task	O
is	O
to	O
provide	O
access	O
to	O
the	O
multilingual	O
Komi	O
lexicon	O
the	O
FST	O
is	O
based	O
on	O
in	O
a	O
form	O
that	O
is	O
truly	O
accessible	O
and	O
openly	O
available	O
.	O
One	O
solution	O
could	O
be	O
to	O
use	O
online	O
dictionary	O
editing	O
platforms	O
,	O
which	O
are	O
strongly	O
linked	O
to	O
the	O
FST	O
development	O
work	O
,	O
and	O
thereby	O
benefit	O
it	O
directly	O
(	O
Alnajjar	O
et	O
al	O
,	O
2020b	O
)	O
.	O
These	O
lexicons	O
have	O
already	O
been	O
published	O
in	O
Zenodo	O
(	O
Rueter	O
et	O
al	O
,	O
2020b	O
)	O
,	O
and	O
already	O
their	O
earliest	O
version	O
has	O
been	O
published	O
in	O
print	O
(	O
Rueter	O
,	O
1995	O
)	O
.	O
Thereby	O
the	O
work	O
described	O
here	O
in	O
various	O
ways	O
continues	O
an	O
already	O
nological	O
and	O
practical	O
changes	O
that	O
these	O
decades	O
have	O
shown	O
.	O
We	O
believe	O
this	O
line	O
of	O
investigation	O
of	O
the	O
Komi	O
language	O
will	O
boldly	O
continue	O
the	O
next	O
25	O
years	O
,	O
but	O
also	O
hope	O
the	O
reports	O
of	O
how	O
the	O
work	O
progresses	O
will	O
become	O
even	O
more	O
regularly	O
.	O
We	O
also	O
foresee	O
that	O
further	O
development	O
of	O
the	O
Komi	O
FST	O
will	O
bring	O
new	O
tools	O
to	O
benefit	O
both	O
the	O
public	O
and	O
research	O
communities	O
.	O
Such	O
might	O
be	O
machine	B-TaskName
translation	I-TaskName
,	O
on	O
the	O
one	O
hand	O
(	O
Tiedemann	O
,	O
2021	O
)	O
,	O
and	O
translation	O
studies	O
,	O
on	O
the	O
other	O
(	O
cf	O
.	O
Цыпанов	O
,	O
2021	O
)	O
.	O
This	O
,	O
of	O
course	O
,	O
does	O
not	O
close	O
the	O
circle	O
,	O
but	O
merely	O
the	O
ever	O
continuous	O
spiral	O
of	O
development	O
.	O

We	O
consider	O
entity	O
-	O
level	O
sentiment	B-TaskName
analysis	I-TaskName
in	O
Arabic	O
,	O
a	O
morphologically	O
rich	O
language	O
with	O
increasing	O
resources	O
.	O
We	O
present	O
a	O
system	O
that	O
is	O
applied	O
to	O
complex	O
posts	O
written	O
in	O
response	O
to	O
Arabic	O
newspaper	O
articles	O
.	O
Our	O
goal	O
is	O
to	O
identify	O
important	O
entity	O
"	O
targets	O
"	O
within	O
the	O
post	O
along	O
with	O
the	O
polarity	O
expressed	O
about	O
each	O
target	O
.	O
We	O
achieve	O
significant	O
improvements	O
over	O
multiple	O
baselines	O
,	O
demonstrating	O
that	O
the	O
use	O
of	O
specific	O
morphological	O
representations	O
improves	O
the	O
performance	O
of	O
identifying	O
both	O
important	O
targets	O
and	O
their	O
sentiment	O
,	O
and	O
that	O
the	O
use	O
of	O
distributional	O
semantic	O
clusters	O
further	O
boosts	O
performances	O
for	O
these	O
representations	O
,	O
especially	O
when	O
richer	O
linguistic	O
resources	O
are	O
not	O
available	O
.	O

Target	O
-	O
specific	O
sentiment	B-TaskName
analysis	I-TaskName
has	O
recently	O
become	O
a	O
popular	O
problem	O
in	O
natural	O
language	O
processing	O
.	O
In	O
interpreting	O
social	O
media	O
posts	O
,	O
analysis	O
needs	O
to	O
include	O
more	O
than	O
just	O
whether	O
people	O
feel	O
positively	O
or	O
negatively	O
;	O
it	O
also	O
needs	O
to	O
include	O
what	O
they	O
like	O
or	O
dislike	O
.	O
The	O
task	O
of	O
finding	O
all	O
targets	O
within	O
the	O
data	O
has	O
been	O
called	O
"	O
open	O
-	O
domain	O
targeted	O
sentiment	O
"	O
(	O
Mitchell	O
et	O
al	O
,	O
2013	O
;	O
Zhang	O
et	O
al	O
,	O
2015	O
)	O
.	O
If	O
we	O
could	O
successfully	O
identify	O
the	O
targets	O
of	O
sentiment	O
,	O
it	O
would	O
be	O
valuable	O
for	O
a	O
number	O
of	O
applications	O
including	O
sentiment	O
summarization	B-TaskName
,	O
question	B-TaskName
answering	I-TaskName
,	O
understanding	O
public	O
opinion	O
during	O
political	O
conflict	O
,	O
or	O
assessing	O
needs	O
of	O
populations	O
during	O
natural	O
disasters	O
.	O
In	O
this	O
paper	O
,	O
we	O
address	O
the	O
open	O
-	O
domain	O
targeted	O
sentiment	O
task	O
.	O
Input	O
to	O
our	O
system	O
consists	O
of	O
online	O
posts	O
,	O
which	O
can	O
be	O
comprised	O
of	O
one	O
or	O
multiple	O
sentences	O
,	O
contain	O
multiple	O
entities	O
with	O
different	O
sentiment	O
,	O
and	O
have	O
different	O
domains	O
.	O
Our	O
goal	O
is	O
to	O
identify	O
the	O
important	O
entities	O
towards	O
which	O
opinions	O
are	O
expressed	O
in	O
the	O
post	O
;	O
these	O
can	O
include	O
any	O
nominal	O
or	O
noun	O
phrase	O
,	O
including	O
events	O
,	O
or	O
concepts	O
,	O
and	O
they	O
are	O
not	O
restricted	O
to	O
named	O
entities	O
as	O
has	O
been	O
the	O
case	O
in	O
some	O
previous	O
work	O
.	O
The	O
only	O
constraint	O
is	O
that	O
the	O
entities	O
need	O
to	O
be	O
explicitly	O
mentioned	O
in	O
the	O
text	O
.	O
Our	O
work	O
also	O
differs	O
from	O
much	O
work	O
on	O
targeted	O
sentiment	B-TaskName
analysis	I-TaskName
in	O
that	O
posts	O
are	O
long	O
,	O
complex	O
,	O
with	O
many	O
annotated	O
targets	O
and	O
a	O
lack	O
of	O
punctuation	O
that	O
is	O
characteristic	O
of	O
Arabic	O
online	O
language	O
.	O
Figure	O
1	O
shows	O
an	O
example	O
post	O
,	O
where	O
targets	O
are	O
either	O
labeled	O
positive	O
(	O
green	O
)	O
if	O
a	O
positive	O
opinion	O
is	O
expressed	O
about	O
them	O
and	O
negative	O
(	O
yellow	O
)	O
if	O
a	O
negative	O
opinion	O
is	O
expressed	O
.	O
To	O
identify	O
targets	O
and	O
sentiment	O
,	O
we	O
develop	O
two	O
sequence	O
labeling	O
models	O
,	O
a	O
target	O
-	O
specific	O
model	O
and	O
a	O
sentiment	O
-	O
specific	O
model	O
.	O
Our	O
models	O
try	O
to	O
learn	O
syntactic	O
relations	O
between	O
entities	O
and	O
opinion	O
words	O
,	O
but	O
they	O
also	O
make	O
use	O
of	O
(	O
1	O
)	O
Arabic	O
morphology	O
and	O
(	O
2	O
)	O
entity	O
semantics	O
.	O
Our	O
use	O
of	O
morphology	O
allows	O
us	O
to	O
capture	O
all	O
"	O
words	O
"	O
that	O
play	O
a	O
role	O
in	O
identification	O
of	O
the	O
target	O
,	O
while	O
our	O
use	O
of	O
entity	O
semantics	O
allows	O
us	O
to	O
group	O
together	O
similar	O
entities	O
which	O
may	O
all	O
be	O
targets	O
of	O
the	O
same	O
sentiment	O
;	O
for	O
example	O
,	O
if	O
a	O
commenter	O
expresses	O
negative	O
sentiment	O
towards	O
the	O
United	O
States	O
,	O
they	O
may	O
also	O
express	O
negative	O
sentiment	O
towards	O
America	O
or	O
Obama	O
.	O
Our	O
results	O
show	O
that	O
morphology	O
matters	O
when	O
identifying	O
entity	O
targets	O
and	O
the	O
sentiment	O
expressed	O
towards	O
them	O
.	O
We	O
find	O
for	O
instance	O
that	O
the	O
attaching	O
Arabic	O
definite	O
article	O
Al+	O
is	O
an	O
important	O
indicator	O
of	O
the	O
presence	O
of	O
a	O
target	O
entity	O
and	O
splitting	O
it	O
off	O
boosts	O
recall	O
of	O
targets	O
,	O
while	O
sentiment	O
models	O
perform	O
better	O
when	O
less	O
tokens	O
are	O
split	O
.	O
We	O
also	O
conduct	O
a	O
detailed	O
analysis	O
of	O
errors	O
revealing	O
that	O
the	O
task	O
generally	O
entails	O
hard	O
problems	O
such	O
as	O
a	O
considerable	O
amount	O
of	O
implicit	O
sentiment	O
and	O
the	O
presence	O
of	O
multiple	O
targets	O
with	O
varying	O
importance	O
.	O
In	O
what	O
follows	O
,	O
we	O
describe	O
related	O
work	O
(	O
2	O
)	O
,	O
data	O
and	O
models	O
(	O
3	O
and	O
4	O
)	O
,	O
and	O
linguistic	O
decisions	O
made	O
for	O
Arabic	O
(	O
5	O
)	O
.	O
In	O
6	O
,	O
we	O
describe	O
our	O
use	O
of	O
word	O
vector	O
clusters	O
learned	O
on	O
a	O
large	O
Arabic	O
corpus	O
.	O
Finally	O
,	O
7	O
presents	O
experiments	O
and	O
detailed	O
error	O
analysis	O
.	O

Aspect	O
-	O
based	O
and	O
Entity	O
-	O
specific	O
Analysis	O
Early	O
work	O
in	O
target	O
-	O
based	O
sentiment	O
looked	O
at	O
identifying	O
aspects	O
in	O
a	O
restricted	O
domain	O
:	O
product	O
or	O
customer	O
reviews	O
.	O
Many	O
of	O
these	O
systems	O
used	O
unsupervised	O
and	O
topical	O
methods	O
for	O
determining	O
aspects	O
of	O
products	O
;	O
Hu	O
and	O
Liu	O
(	O
2004	O
)	O
used	O
frequent	O
feature	O
mining	O
to	O
find	O
noun	O
phrase	O
aspects	O
,	O
Brody	O
and	O
Elhadad	O
(	O
2010	O
)	O
used	O
topic	O
modeling	O
to	O
find	O
important	O
keywords	O
in	O
restaurant	O
reviews	O
,	O
and	O
Somasundaran	O
and	O
Wiebe	O
(	O
2009	O
)	O
mined	O
the	O
web	O
to	O
find	O
important	O
aspects	O
associated	O
with	O
debate	O
topics	O
and	O
their	O
corresponding	O
polarities	O
.	O
SemEval	O
2014	O
Task	O
4	O
(	O
Pontiki	O
et	O
al	O
,	O
2014	O
)	O
ran	O
several	O
subtasks	O
for	O
identifying	O
aspect	O
terms	O
and	O
sentiment	O
towards	O
aspects	O
and	O
terms	O
in	O
restaurant	O
and	O
laptop	O
reviews	O
.	O
Entity	O
-	O
specific	O
sentiment	B-TaskName
analysis	I-TaskName
has	O
been	O
frequently	O
studied	O
in	O
social	O
media	O
and	O
online	O
posts	O
.	O
Jiang	O
et	O
al	O
(	O
2011	O
)	O
proposed	O
identifying	O
sentiment	O
of	O
a	O
tweet	O
towards	O
a	O
specific	O
named	O
entity	O
,	O
taking	O
into	O
account	O
multiple	O
mentions	O
of	O
the	O
given	O
entity	O
.	O
Biyani	O
et	O
al	O
(	O
2015	O
)	O
studied	O
sentiment	O
towards	O
entities	O
in	O
online	O
posts	O
,	O
where	O
the	O
local	O
part	O
of	O
the	O
post	O
that	O
contained	O
the	O
entity	O
or	O
mentions	O
of	O
it	O
was	O
identified	O
and	O
the	O
sentiment	O
was	O
classified	O
using	O
a	O
number	O
of	O
linguistic	O
features	O
.	O
The	O
entities	O
were	O
selected	O
beforehand	O
and	O
consisted	O
of	O
known	O
,	O
named	O
entities	O
.	O
More	O
recent	O
work	O
uses	O
LSTM	B-MethodName
and	O
RNN	O
networks	O
to	O
determine	O
sentiment	O
toward	O
aspects	O
in	O
product	O
reviews	O
(	O
Wang	O
et	O
al	O
,	O
2016	O
)	O
and	O
towards	O
entities	O
in	O
Twitter	O
(	O
Dong	O
et	O
al	O
,	O
2014	O
;	O
Tang	O
et	O
al	O
,	O
2015	O
)	O
.	O
SemEval	O
2016	O
ran	O
two	O
tasks	O
on	O
sentiment	B-TaskName
analysis	I-TaskName
(	O
Nakov	O
et	O
al	O
,	O
2016	O
)	O
and	O
stance	O
(	O
Mohammad	O
et	O
al	O
,	O
2016	O
)	O
towards	O
pre	O
-	O
defined	O
topics	O
in	O
Twitter	O
,	O
both	O
on	O
English	O
data	O
.	O
Open	O
domain	O
targeted	O
analysis	O
In	O
early	O
work	O
.	O
Kim	O
and	O
Hovy	O
(	O
2006	O
)	O
proposed	O
finding	O
opinion	O
target	O
and	O
sources	O
in	O
news	O
text	O
by	O
automatic	O
labeling	O
of	O
semantic	O
roles	O
.	O
Here	O
,	O
opinion	O
-	O
target	O
relationships	O
were	O
restricted	O
to	O
relations	O
that	O
can	O
be	O
captured	O
using	O
semantic	O
roles	O
.	O
Ruppenhofer	O
et	O
al	O
(	O
2008	O
)	O
discussed	O
the	O
challenges	O
of	O
identifying	O
targets	O
in	O
open	O
-	O
domain	O
text	O
which	O
can	O
not	O
be	O
addressed	O
by	O
semantic	B-TaskName
role	I-TaskName
labeling	I-TaskName
,	O
such	O
as	O
implicitly	O
conveyed	O
sentiment	O
,	O
global	O
and	O
local	O
targets	O
related	O
to	O
the	O
same	O
entity	O
,	O
and	O
the	O
need	O
for	O
distinguishing	O
between	O
entity	O
and	O
proposition	O
targets	O
.	O
Sequence	O
labeling	O
models	O
became	O
more	O
popular	O
for	O
this	O
problem	O
:	O
Mitchell	O
et	O
al	O
(	O
2013	O
)	O
used	O
CRF	B-MethodName
model	O
combinations	O
to	O
identify	O
named	O
entity	O
targets	O
in	O
English	O
and	O
Spanish	O
,	O
and	O
Yang	O
and	O
Cardie	O
(	O
2013	O
)	O
used	O
joint	O
modeling	O
to	O
predict	O
opinion	O
expressions	O
and	O
their	O
source	O
and	O
target	O
spans	O
in	O
news	O
articles	O
,	O
improving	O
over	O
several	O
single	O
CRF	B-MethodName
models	O
.	O
Their	O
focus	O
was	O
on	O
identifying	O
directly	O
subjective	O
opinion	O
expressions	O
(	O
e.g	O
"	O
I	O
hate	O
[	O
this	O
dictator	O
]	O
"	O
vs.	O
"	O
[	O
This	O
dictator	O
]	O
is	O
destroying	O
his	O
country	O
.	O
"	O
)	O
Recent	O
work	O
(	O
Deng	O
and	O
Wiebe	O
,	O
2015	O
)	O
identifies	O
entity	O
sources	O
and	O
targets	O
,	O
as	O
well	O
as	O
the	O
sentiment	O
expressed	O
by	O
and	O
towards	O
these	O
entities	O
.	O
This	O
work	O
was	O
based	O
on	O
probablistic	O
soft	O
logic	O
models	O
,	O
also	O
with	O
a	O
focus	O
on	O
direct	O
subjective	O
expressions	O
.	O
There	O
is	O
also	O
complementary	O
work	O
on	O
using	O
neural	O
networks	O
for	O
tagging	O
open	O
-	O
domain	O
targets	O
(	O
Zhang	O
et	O
al	O
,	O
2015	O
;	O
in	O
shorter	O
posts	O
.	O
Previous	O
work	O
listed	O
did	O
not	O
consider	O
word	O
morphology	O
,	O
or	O
explicitly	O
model	O
distributional	O
entity	O
semantics	O
as	O
indicative	O
of	O
the	O
presence	O
of	O
sentiment	O
targets	O
.	O

Past	O
work	O
in	O
Arabic	O
machine	B-TaskName
translation	I-TaskName
(	O
Habash	O
and	O
Sadat	O
,	O
2006	O
)	O
and	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
Benajiba	O
et	O
al	O
,	O
2008	O
)	O
considered	O
the	O
tokenization	O
of	O
complex	O
Arabic	O
words	O
as	O
we	O
do	O
in	O
our	O
sequence	O
labeling	O
task	O
.	O
Analysis	O
of	O
such	O
segmentation	O
schemes	O
has	O
not	O
been	O
reported	O
for	O
Arabic	O
sentiment	O
tasks	O
,	O
which	O
cover	O
mostly	O
sentence	O
-	O
level	O
sentiment	B-TaskName
analysis	I-TaskName
and	O
where	O
the	O
lemma	B-DatasetName
or	O
surface	O
bag	O
-	O
of	O
-	O
word	O
representations	O
have	O
typically	O
been	O
sufficient	O
.	O
There	O
are	O
now	O
many	O
studies	O
on	O
sentence	O
-	O
level	O
sentiment	B-TaskName
analysis	I-TaskName
in	O
Arabic	O
news	O
and	O
social	O
media	O
(	O
Abdul	O
-	O
Mageed	O
and	O
Diab	O
,	O
2011	O
;	O
Mourad	O
and	O
Darwish	O
,	O
2013	O
;	O
Refaee	O
and	O
Rieser	O
,	O
2014	O
;	O
Salameh	O
et	O
al	O
,	O
2015	O
)	O
.	O
Elarnaoty	O
et	O
al	O
(	O
2012	O
)	O
proposed	O
identifying	O
sources	O
of	O
opinions	O
in	O
Arabic	O
using	O
a	O
CRF	B-MethodName
with	O
a	O
number	O
of	O
patterns	O
,	O
lexical	O
and	O
subjectivity	O
clues	O
;	O
they	O
did	O
not	O
discuss	O
morphology	O
or	O
syntactic	O
relations	O
.	O
developed	O
a	O
dataset	O
and	O
built	O
a	O
majority	O
baseline	O
for	O
finding	O
targets	O
in	O
Arabic	O
book	O
reviews	O
of	O
known	O
aspects	O
;	O
Obaidat	O
et	O
al	O
(	O
2015	O
)	O
also	O
developed	O
a	O
lexicon	O
-	O
based	O
approach	O
to	O
improve	O
on	O
this	O
baseline	O
.	O
Abu	O
-	O
Jbara	O
et	O
al	O
(	O
2013	O
)	O
created	O
a	O
simple	O
opinion	O
-	O
target	O
system	O
for	O
Arabic	O
by	O
identifying	O
noun	O
phrases	O
in	O
polarized	O
text	O
;	O
this	O
was	O
done	O
intrinsically	O
as	O
part	O
of	O
an	O
effort	O
to	O
identify	O
opinion	O
subgroups	O
in	O
online	O
discussions	O
.	O
There	O
are	O
no	O
other	O
sentiment	O
target	O
studies	O
in	O
Arabic	O
that	O
we	O
know	O
of	O
.	O
In	O
our	O
experiments	O
,	O
we	O
compare	O
to	O
methods	O
similar	O
to	O
these	O
baseline	O
systems	O
,	O
as	O
well	O
as	O
to	O
results	O
of	O
English	O
work	O
that	O
is	O
comparable	O
to	O
ours	O
.	O
Entity	O
Clusters	O
It	O
has	O
been	O
shown	O
consistently	O
that	O
semantic	O
word	O
clusters	O
improve	O
the	O
performance	O
of	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
Täckström	O
et	O
al	O
,	O
2012	O
;	O
Zirikly	O
and	O
Hagiwara	O
,	O
2015	O
;	O
Turian	O
et	O
al	O
,	O
2010	O
)	O
and	O
semantic	B-TaskName
parsing	I-TaskName
(	O
Saleh	O
et	O
al	O
,	O
2014	O
)	O
;	O
we	O
are	O
not	O
aware	O
of	O
such	O
work	O
for	O
identifying	O
entity	O
targets	O
of	O
sentiment	O
.	O

For	O
modeling	O
the	O
data	O
,	O
we	O
choose	O
Conditional	O
Random	O
Fields	O
(	O
CRF	B-MethodName
)	O
(	O
Lafferty	O
et	O
al	O
,	O
2001	O
)	O
for	O
the	O
ability	O
to	O
engineer	O
Arabic	O
linguistic	O
features	O
and	O
because	O
of	O
the	O
success	O
of	O
CRF	B-MethodName
models	O
in	O
the	O
past	O
for	O
entity	O
identification	O
and	O
classification	O
related	O
tasks	O
.	O
We	O
build	O
two	O
linear	O
chain	O
CRF	B-MethodName
models	O
:	O
1	O
.	O
Target	O
Model	O
This	O
model	O
predicts	O
a	O
sequence	O
of	O
labels	O
E	O
for	O
a	O
sequence	O
of	O
input	O
tokens	O
x	O
,	O
where	O
E	O
i	O
{	O
T	O
(	O
target	O
)	O
,	O
O	O
(	O
not_target	O
)	O
}	O
and	O
each	O
token	O
x	O
i	O
is	O
represented	O
by	O
a	O
feature	O
vector	O
f	O
it	O
.	O
A	O
token	O
is	O
labeled	O
T	O
if	O
it	O
is	O
part	O
of	O
a	O
target	O
;	O
a	O
target	O
can	O
contain	O
one	O
or	O
more	O
consecutive	O
tokens	O
.	O

In	O
Arabic	O
,	O
clitics	O
and	O
affixes	O
can	O
attach	O
to	O
the	O
beginning	O
and	O
end	O
of	O
the	O
word	O
stem	O
,	O
making	O
words	O
complex	O
.	O
For	O
example	O
,	O
in	O
the	O
sentence	O
'	O
So	O
they	O
welcomed	O
her	O
'	O
,	O
the	O
discourse	O
conjuction	O
(	O
so	O
+	O
)	O
,	O
the	O
opinion	O
target	O
(	O
her	O
+	O
)	O
,	O
opinion	O
holder	O
(	O
they	O
)	O
,	O
and	O
the	O
opinion	O
expression	O
itself	O
(	O
welcomed	O
)	O
are	O
all	O
collapsed	O
in	O
the	O
same	O
word	O
.	O
Clitics	O
,	O
such	O
as	O
conjunctions	O
+	O
w+	O
,	O
prepositions	O
+	O
b+	O
,	O
the	O
definite	O
article	O
Al+	O
'	O
the	O
'	O
(	O
all	O
of	O
which	O
attach	O
at	O
the	O
beginning	O
)	O
,	O
and	O
possessive	O
pronouns	O
and	O
object	O
pronouns	O
+	O
+	O
h	O
+	O
+	O
hA	O
'	O
his	O
/	O
her	O
'	O
or	O
'	O
him	O
/	O
her	O
'	O
(	O
which	O
attach	O
at	O
the	O
end	O
)	O
can	O
all	O
function	O
as	O
individual	O
words	O
.	O
Thus	O
,	O
they	O
can	O
be	O
represented	O
as	O
separate	O
tokens	O
in	O
the	O
CRF	B-MethodName
.	O
The	O
morphological	O
analyzer	O
MADAMIRA	O
(	O
Pasha	O
et	O
al	O
,	O
2014	O
)	O
enables	O
the	O
tokenization	O
of	O
a	O
word	O
using	O
multiple	O
schemes	O
.	O
We	O
consider	O
the	O
following	O
two	O
schemes	O
:	O
D3	B-DatasetName
:	O
the	O
Declitization	O
scheme	O
which	O
splits	O
off	O
conjunction	O
clitics	O
,	O
particles	O
and	O
prepositions	O
,	O
Al+	O
,	O
and	O
all	O
the	O
enclitics	O
at	O
the	O
end	O
.	O
ATB	O
:	O
the	O
Penn	O
Arabic	O
Treebank	O
tokenization	O
,	O
which	O
separates	O
all	O
clitics	O
above	O
except	O
the	O
definite	O
article	O
Al+	O
,	O
which	O
it	O
keeps	O
attached	O
.	O
For	O
a	O
detailed	O
description	O
of	O
Arabic	O
concatenative	O
morphology	O
and	O
tokenization	O
schemes	O
,	O
the	O
reader	O
is	O
referred	O
to	O
Habash	O
(	O
2010	O
)	O
.	O
For	O
each	O
token	O
,	O
we	O
add	O
a	O
part	O
of	O
speech	O
feature	O
.	O
For	O
word	O
form	O
(	O
non	O
-	O
clitic	O
)	O
tokens	O
,	O
we	O
use	O
the	O
part	O
of	O
speech	O
(	O
POS	O
)	O
feature	O
produced	O
by	O
the	O
morphological	O
analyzer	O
.	O
We	O
consider	O
the	O
surface	O
word	O
and	O
the	O
lemma	B-DatasetName
for	O
representing	O
the	O
word	O
form	O
.	O
For	O
the	O
clitics	O
that	O
were	O
split	O
off	O
,	O
we	O
use	O
a	O
detailed	O
POS	O
feature	O
that	O
is	O
also	O
extracted	O
from	O
the	O
output	O
of	O
the	O
analyzer	O
and	O
can	O
take	O
such	O
forms	O
as	O
DET	B-DatasetName
for	O
Al+	O
or	O
poss_pron_3MP	O
for	O
third	O
person	O
masculine	O
possessive	O
pronouns	O
.	O
Table	O
2	O
shows	O
the	O
words	O
and	O
part	O
of	O
speech	O
for	O
the	O
input	O
sentence	O
'	O
so	O
they	O
welcomed	O
her	O
'	O
fa	O
-	O
istaqbalu	O
-	O
ha	O
,	O
using	O
the	O
lemma	B-DatasetName
representation	O
for	O
the	O
word	O
form	O
and	O
the	O
D3	B-DatasetName
tokenization	O
scheme	O
.	O
These	O
lexical	O
and	O
POS	O
features	O
are	O
added	O
to	O
both	O
our	O
target	O
model	O
and	O
sentiment	O
model	O
.	O

The	O
choice	O
of	O
sentiment	O
lexicon	O
is	O
an	O
important	O
consideration	O
when	O
developing	O
systems	O
for	O
new	O
and/or	O
low	O
-	O
resource	O
languages	O
.	O
We	O
consider	O
three	O
lexicons	O
:	O
(	O
1	O
)	O
SIFAAT	O
,	O
a	O
manually	O
constructed	O
Arabic	O
lexicon	O
of	O
3982	O
adjectives	O
(	O
Abdul	O
-	O
Mageed	O
and	O
Diab	O
,	O
2011	O
)	O
,	O
(	O
2	O
)	O
ArSenL	O
,	O
an	O
Arabic	O
lexicon	O
developed	O
by	O
linking	O
English	O
SentiWord	O
-	O
Net	O
with	O
Arabic	O
WordNet	O
and	O
an	O
Arabic	O
lexical	O
database	O
(	O
Badaro	O
et	O
al	O
,	O
2014	O
)	O
,	O
and	O
(	O
3	O
)	O
the	O
English	O
MPQA	B-DatasetName
lexicon	O
(	O
Wilson	O
et	O
al	O
,	O
2005	O
)	O
,	O
where	O
we	O
look	O
up	O
words	O
by	O
matching	O
on	O
the	O
English	O
glosses	O
produced	O
by	O
the	O
morphological	O
analyzer	O
MADAMIRA	O
.	O
For	O
the	O
target	O
model	O
,	O
we	O
add	O
token	O
-	O
level	O
binary	O
features	O
representing	O
subjectivity	O
,	O
and	O
for	O
the	O
sentiment	O
model	O
,	O
we	O
add	O
both	O
subjectivity	O
and	O
polarity	O
features	O
.	O
We	O
also	O
add	O
a	O
feature	O
specifying	O
respectively	O
the	O
subjectivity	O
or	O
polarity	O
of	O
the	O
parent	O
word	O
of	O
the	O
token	O
in	O
the	O
dependency	O
tree	O
in	O
the	O
target	O
or	O
sentiment	O
model	O
.	O

We	O
ran	O
the	O
CATiB	O
(	O
Columbia	O
Arabic	O
Treebank	O
)	O
dependency	O
parser	O
(	O
Shahrour	O
et	O
al	O
,	O
2015	O
)	O
on	O
our	O
data	O
.	O
CATiB	O
uses	O
a	O
number	O
of	O
intuitive	O
labels	O
specifying	O
the	O
token	O
's	O
syntactic	O
role	O
:	O
e.g	O
SBJ	O
,	O
OBJ	O
,	O
MOD	B-DatasetName
,	O
and	O
IDF	O
for	O
the	O
Arabic	O
idafa	O
construct	O
(	O
e.g	O
president	O
of	O
government	O
)	O
,	O
as	O
well	O
as	O
its	O
part	O
of	O
speech	O
role	O
.	O
In	O
addition	O
to	O
the	O
sentiment	O
dependency	O
features	O
specifying	O
the	O
sentiment	O
of	O
parent	O
words	O
,	O
we	O
added	O
dependency	O
features	O
specifying	O
the	O
syntactic	O
role	O
of	O
the	O
token	O
in	O
relation	O
to	O
its	O
parent	O
,	O
and	O
the	O
path	O
from	O
the	O
token	O
to	O
the	O
parent	O
,	O
e.g	O
nom_obj_vrb	O
or	O
nom_idf_nom	O
,	O
as	O
well	O
as	O
the	O
sentiment	O
path	O
from	O
the	O
token	O
to	O
the	O
parent	O
,	O
e.g	O
nom	O
(	O
neutral	O
)	O
_	O
obj_vrb	O
(	O
negative	O
)	O
.	O

The	O
morphological	O
analyzer	O
MADAMIRA	O
also	O
produces	O
base	O
phrase	O
chunks	O
(	O
BPC	O
)	O
and	O
named	O
entity	O
tags	O
(	O
NER	B-TaskName
)	O
for	O
each	O
token	O
.	O
We	O
add	O
features	O
for	O
these	O
as	O
well	O
,	O
based	O
on	O
the	O
hypothesis	O
that	O
they	O
will	O
help	O
define	O
the	O
spans	O
for	O
entity	O
targets	O
,	O
whether	O
they	O
are	O
named	O
entities	O
or	O
any	O
noun	O
phrases	O
.	O
We	O
refer	O
to	O
the	O
sentiment	O
and	O
target	O
models	O
that	O
utilize	O
Arabic	O
morphology	O
,	O
sentiment	O
,	O
syntactic	O
relations	O
and	O
entity	O
chunks	O
as	O
best	O
-	O
linguistic	O
.	O

Similar	O
entities	O
which	O
occur	O
in	O
the	O
context	O
of	O
the	O
same	O
topic	O
or	O
the	O
same	O
larger	O
entity	O
are	O
likely	O
to	O
occur	O
as	O
targets	O
alongside	O
each	O
other	O
and	O
to	O
have	O
similar	O
sentiment	O
expressed	O
towards	O
them	O
.	O
They	O
may	O
repeat	O
frequently	O
in	O
a	O
post	O
even	O
if	O
they	O
do	O
not	O
explicitly	O
or	O
lexically	O
refer	O
to	O
the	O
same	O
person	O
or	O
object	O
.	O
For	O
example	O
,	O
someone	O
writing	O
about	O
American	O
foreign	O
policy	O
may	O
frequently	O
refer	O
to	O
entities	O
such	O
as	O
{	O
the	O
United	O
States	O
,	O
America	O
,	O
Obama	O
,	O
the	O
Americans	O
,	O
Westerners	O
}	O
.	O
Such	O
entities	O
can	O
cluster	O
together	O
semantically	O
and	O
it	O
is	O
likely	O
that	O
a	O
person	O
expressing	O
positive	O
or	O
negative	O
sentiment	O
towards	O
one	O
of	O
these	O
entities	O
may	O
also	O
express	O
the	O
same	O
sentiment	O
towards	O
the	O
other	O
entities	O
in	O
this	O
set	O
.	O
Moreover	O
,	O
cluster	O
features	O
serve	O
as	O
a	O
denser	O
feature	O
representation	O
with	O
a	O
reduced	O
feature	O
space	O
compared	O
to	O
Arabic	O
lexical	O
features	O
.	O
Such	O
features	O
can	O
benefit	O
the	O
CRF	B-MethodName
where	O
a	O
limited	O
amount	O
of	O
training	O
data	O
is	O
available	O
for	O
target	O
entities	O
.	O
To	O
utilize	O
the	O
semantics	O
of	O
word	O
clusters	O
,	O
we	O
build	O
word	O
embedding	O
vectors	O
using	O
the	O
skip	O
-	O
gram	O
method	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
and	O
cluster	O
them	O
using	O
the	O
K	O
-	O
Means	O
algorithm	O
(	O
MacQueen	O
,	O
1967	O
)	O
,	O
with	O
Euclidean	O
distance	O
as	O
a	O
metric	O
.	O
Euclidean	O
distance	O
serves	O
as	O
a	O
semantic	B-TaskName
similarity	I-TaskName
metric	O
and	O
has	O
been	O
commonly	O
used	O
as	O
a	O
distance	O
-	O
based	O
measure	O
for	O
clustering	O
word	O
vectors	O
.	O
The	O
vectors	O
are	O
built	O
on	O
Arabic	O
Wikipedia	O
2	O
on	O
a	O
corpus	O
of	O
137	O
M	O
words	O
resulting	O
in	O
a	O
vocabulary	O
of	O
254	O
K	O
words	O
.	O
We	O
preprocess	O
the	O
corpus	O
by	O
tokenizing	O
(	O
using	O
the	O
schemes	O
described	O
in	O
section	O
5	O
)	O
and	O
lemmatizing	O
before	O
building	O
the	O
word	O
vectors	O
.	O
We	O
vary	O
the	O
number	O
of	O
clusters	O
and	O
use	O
the	O
clusters	O
as	O
binary	O
features	O
in	O
our	O
target	O
and	O
sentiment	O
models	O
.	O

Setup	O
To	O
build	O
our	O
sentiment	O
and	O
target	O
models	O
,	O
we	O
use	O
CRF++	O
(	O
Kudo	O
,	O
2005	O
)	O
to	O
build	O
linear	O
-	O
chain	O
sequences	O
.	O
We	O
use	O
a	O
context	O
window	O
of	O
+	O
/	O
-	O
2	O
for	O
all	O
features	O
except	O
the	O
syntactic	O
dependencies	O
,	O
where	O
we	O
use	O
a	O
window	O
of	O
+	O
/	O
-	O
4	O
to	O
better	O
capture	O
syntactic	O
relations	O
in	O
the	O
posts	O
.	O
For	O
the	O
sentiment	O
model	O
,	O
we	O
include	O
the	O
context	O
of	O
the	O
previous	O
predicted	O
label	O
,	O
to	O
avoid	O
predicting	O
consecutive	O
tokens	O
with	O
opposite	O
polarity	O
.	O
We	O
evaluate	O
all	O
our	O
experiments	O
on	O
the	O
development	O
set	O
which	O
contains	O
116	O
posts	O
and	O
442	O
targets	O
,	O
and	O
present	O
a	O
final	O
result	O
with	O
the	O
best	O
models	O
on	O
the	O
unseen	O
test	O
.	O
For	O
the	O
SentiWordNetbased	O
lexicon	O
ArSenL	O
,	O
we	O
tune	O
for	O
the	O
sentiment	O
score	O
threshold	O
and	O
use	O
t=0.2	O
.	O
We	O
use	O
Google	B-DatasetName
's	O
word2vec	O
tool	O
3	O
for	O
building	O
and	O
clustering	O
word	O
vectors	O
with	O
dimension	O
200	O
.	O
We	O
vary	O
the	O
number	O
of	O
clusters	O
k	O
between	O
10	O
(	O
25	O
K	O
words	O
/	O
cluster	O
)	O
and	O
20	O
K	O
(	O
12	O
words	O
/	O
cluster	O
)	O
.	O
Baselines	O
For	O
evaluating	O
the	O
predicted	O
targets	O
,	O
we	O
follow	O
work	O
in	O
English	O
(	O
Deng	O
and	O
Wiebe	O
,	O
2015	O
)	O
and	O
use	O
the	O
all	O
-	O
NP	O
baseline	O
,	O
where	O
all	O
nouns	O
and	O
noun	O
phrases	O
in	O
the	O
post	O
are	O
predicted	O
as	O
important	O
targets	O
.	O
For	O
evaluating	O
sentiment	O
towards	O
targets	O
,	O
we	O
consider	O
four	O
baselines	O
:	O
the	O
majority	O
baseline	O
which	O
always	O
predicts	O
negative	O
,	O
and	O
the	O
lexicon	O
baseline	O
evaluated	O
in	O
the	O
case	O
of	O
each	O
of	O
our	O
three	O
lexicons	O
:	O
manually	O
created	O
,	O
WordNet	O
-	O
based	O
,	O
and	O
English	O
-	O
translated	O
.	O
The	O
strong	O
lexicon	O
baseline	O
splits	O
the	O
post	O
into	O
sentences	O
or	O
phrases	O
by	O
punctuation	O
,	O
finds	O
the	O
phrase	O
that	O
contains	O
the	O
predicted	O
target	O
,	O
and	O
returns	O
positive	O
if	O
there	O
are	O
more	O
positive	O
words	O
than	O
negative	O
words	O
,	O
and	O
negative	O
otherwise	O
.	O
These	O
baselines	O
are	O
similar	O
to	O
the	O
methods	O
of	O
previously	O
published	O
work	O
for	O
Arabic	O
targeted	O
sentiment	O
Obaidat	O
et	O
al	O
,	O
2015	O
;	O
Abu	O
-	O
Jbara	O
et	O
al	O
,	O
2013	O
)	O
.	O
We	O
run	O
our	O
pipelined	O
models	O
for	O
all	O
morphological	O
representation	O
schemes	O
:	O
surface	O
word	O
(	O
no	O
token	O
splits	O
)	O
,	O
lemma	B-DatasetName
(	O
no	O
clitics	O
)	O
,	O
lemma	B-DatasetName
with	O
ATB	O
clitics	O
(	O
contain	O
all	O
token	O
splits	O
except	O
Al+	O
)	O
,	O
and	O
lemma	B-DatasetName
with	O
D3	B-DatasetName
clitics	O
(	O
contains	O
all	O
token	O
splits	O
)	O
.	O
We	O
explore	O
the	O
effect	O
of	O
semantic	O
word	O
clusters	O
in	O
these	O
scenarios	O
.	O
Finally	O
we	O
show	O
our	O
bestlinguistic	O
(	O
high	O
-	O
resource	O
)	O
model	O
,	O
and	O
the	O
resulting	O
integration	O
with	O
word	O
clusters	O
.	O

We	O
analyzed	O
the	O
output	O
of	O
our	O
best	O
linguistic	O
models	O
on	O
the	O
development	O
set	O
,	O
and	O
observed	O
the	O
following	O
kind	O
of	O
errors	O
:	O
Implicit	O
Sentiment	O
This	O
was	O
the	O
most	O
common	O
kind	O
of	O
error	O
observed	O
.	O
Commenters	O
frequently	O
expressed	O
complex	O
subjective	O
language	O
without	O
using	O
sentiment	O
words	O
,	O
often	O
resorting	O
to	O
sarcasm	O
,	O
metaphor	O
,	O
and	O
argumentative	O
language	O
.	O
We	O
also	O
observed	O
persistent	O
errors	O
where	O
positive	O
sentiment	O
was	O
identified	O
towards	O
an	O
entity	O
because	O
of	O
misleading	O
polar	O
words	O
;	O
e.g	O
minds	O
was	O
consistently	O
predicted	O
to	O
be	O
positive	O
even	O
though	O
the	O
post	O
in	O
question	O
was	O
using	O
implicit	O
language	O
to	O
express	O
negative	O
sentiment	O
;	O
the	O
English	O
gloss	O
Example	O
1	O
Till	O
when	O
will	O
[	O
the	O
world	O
]	O
-	O
wait	O
before	O
it	O
intervenes	O
against	O
these	O
[	O
crimes	O
against	O
humanity	O
]	O
-	O
committed	O
by	O
this	O
[	O
criminal	O
bloody	O
regime	O
]	O
-	O
which	O
will	O
not	O
stop	O
doing	O
that	O
...	O
because	O
its	O
presence	O
has	O
always	O
been	O
associated	O
with	O
oppression	O
and	O
murder	O
and	O
crime	O
...	O
But	O
now	O
it	O
's	O
time	O
for	O
it	O
to	O
disappear	O
and	O
descend	O
into	O
[	O
the	O
trash	O
of	O
history	O
]	O
-	O
.	O
Output	O
the	O
world	O
:	O
neg	O
crimes	O
:	O
neg	O
criminal	O
bloody	O
regime	O
:	O
neg	O
the	O
trash	O
of	O
history	O
:	O
neg	O
Example	O
2	O
[	O
Malaysia	O
]	O
+	O
is	O
considered	O
the	O
most	O
successful	O
country	O
in	O
Eastern	O
Asia	O
,	O
and	O
its	O
economic	O
success	O
has	O
spread	O
to	O
other	O
[	O
aspects	O
of	O
life	O
in	O
Malaysia	O
]	O
+	O
,	O
for	O
its	O
[	O
services	O
to	O
its	O
citizens	O
]	O
+	O
have	O
improved	O
,	O
and	O
there	O
has	O
been	O
an	O
increase	O
in	O
[	O
the	O
quality	O
of	O
its	O
health	O
and	O
educational	O
and	O
social	O
and	O
financial	O
and	O
touristic	O
services	O
]	O
+	O
,	O
which	O
has	O
made	O
it	O
excellent	O
for	O
foreign	O
investments	O
.	O
Output	O
Malaysia	O
:	O
pos	O
health	O
:	O
pos	O
educational	O
and	O
social	O
:	O
neg	O
financial	O
:	O
neg	O
is	O
brains	O
,	O
which	O
appears	O
as	O
a	O
positive	O
subjective	O
word	O
in	O
the	O
MPQA	B-DatasetName
lexicon	O
.	O
The	O
posts	O
also	O
contained	O
cases	O
of	O
complex	O
coreference	O
where	O
subjective	O
statements	O
were	O
at	O
long	O
distances	O
from	O
the	O
targets	O
they	O
discussed	O
.	O
Annotation	O
Errors	O
Our	O
models	O
often	O
correctly	O
predicted	O
targets	O
with	O
reasonable	O
sentiment	O
which	O
were	O
not	O
marked	O
as	O
important	O
targets	O
by	O
annotators	O
;	O
this	O
points	O
to	O
the	O
subjective	O
nature	O
of	O
the	O
task	O
.	O
Sentiment	O
lexicon	O
misses	O
These	O
errors	O
resulted	O
from	O
mis	O
-	O
match	O
between	O
the	O
sentiment	O
of	O
the	O
English	O
gloss	O
and	O
the	O
intended	O
Arabic	O
meaning	O
,	O
leading	O
to	O
polar	O
sentiment	O
being	O
missed	O
.	O
Primary	O
Targets	O
The	O
data	O
contains	O
multiple	O
entity	O
targets	O
and	O
not	O
all	O
are	O
of	O
equal	O
importance	O
.	O
Out	O
of	O
the	O
first	O
50	O
posts	O
manually	O
analyzed	O
on	O
the	O
dev	O
set	O
,	O
we	O
found	O
that	O
in	O
38	O
out	O
of	O
50	O
cases	O
(	O
76	O
%	O
)	O
the	O
correct	O
primary	O
targets	O
were	O
identified	O
(	O
the	O
most	O
important	O
topical	O
sentiment	O
target	O
(	O
s	O
)	O
addressed	O
by	O
the	O
post	O
)	O
;	O
in	O
4	O
cases	O
,	O
a	O
target	O
was	O
predicted	O
where	O
the	O
annotations	O
contained	O
no	O
polar	O
targets	O
at	O
all	O
,	O
and	O
in	O
the	O
remaining	O
cases	O
the	O
primary	O
target	O
was	O
missed	O
.	O
Correct	O
sentiment	O
polarity	O
was	O
predicted	O
for	O
31	O
out	O
of	O
the	O
38	O
correct	O
targets	O
(	O
81.6	O
%	O
)	O
.	O
In	O
general	O
,	O
our	O
analysis	O
showed	O
that	O
our	O
system	O
does	O
well	O
on	O
posts	O
where	O
targets	O
and	O
subjective	O
language	O
are	O
well	O
formed	O
,	O
but	O
that	O
the	O
important	O
target	O
identification	O
task	O
is	O
difficult	O
and	O
made	O
more	O
complex	O
by	O
the	O
long	O
and	O
repetitive	O
nature	O
of	O
the	O
posts	O
.	O
Table	O
7	O
shows	O
two	O
examples	O
of	O
the	O
translated	O
output	O
of	O
SMARTies	O
,	O
the	O
first	O
on	O
more	O
wellformed	O
text	O
and	O
the	O
second	O
on	O
text	O
that	O
is	O
more	O
difficult	O
to	O
parse	O
.	O

We	O
presented	O
a	O
linguistically	O
inspired	O
system	O
that	O
can	O
recognize	O
important	O
entity	O
targets	O
along	O
with	O
sentiment	O
in	O
opinionated	O
posts	O
in	O
Arabic	O
.	O
The	O
targets	O
can	O
be	O
any	O
type	O
of	O
entity	O
or	O
event	O
,	O
and	O
they	O
are	O
not	O
known	O
beforehand	O
.	O
Both	O
target	O
and	O
sentiment	O
results	O
significantly	O
improve	O
multiple	O
lexical	O
baselines	O
and	O
are	O
comparable	O
to	O
previously	O
published	O
results	O
in	O
similar	O
tasks	O
for	O
English	O
,	O
a	O
similarly	O
hard	O
task	O
.	O
Our	O
task	O
is	O
further	O
complicated	O
by	O
the	O
informal	O
and	O
very	O
long	O
sentences	O
that	O
are	O
used	O
in	O
Arabic	O
online	O
posts	O
.	O
We	O
showed	O
that	O
the	O
choice	O
of	O
morphological	O
representation	O
significantly	O
affects	O
the	O
performance	O
of	O
the	O
target	O
and	O
sentiment	O
models	O
.	O
This	O
could	O
shed	O
light	O
on	O
further	O
research	O
in	O
target	O
-	O
specific	O
sentiment	B-TaskName
analysis	I-TaskName
for	O
morphologically	O
complex	O
languages	O
,	O
an	O
area	O
little	O
investigated	O
previously	O
.	O
We	O
also	O
showed	O
that	O
the	O
use	O
of	O
semantic	O
clusters	O
boosts	O
performance	O
for	O
both	O
target	O
and	O
sentiment	O
identification	O
.	O
Furthermore	O
,	O
semantic	O
clusters	O
alone	O
can	O
achieve	O
performance	O
close	O
to	O
a	O
more	O
resource	O
-	O
rich	O
linguistic	O
model	O
relying	O
on	O
syntax	O
and	O
sentiment	O
lexicons	O
,	O
and	O
would	O
thus	O
be	O
a	O
good	O
approach	O
for	O
low	O
-	O
resource	O
languages	O
.	O
Integrating	O
different	O
morphological	O
preprocessing	O
schemes	O
along	O
with	O
clusters	O
gives	O
our	O
best	O
result	O
.	O
Our	O
code	O
and	O
data	O
is	O
publicly	O
available	O
6	O
.	O
Future	O
work	O
will	O
consider	O
cross	O
-	O
lingual	O
clusters	O
and	O
morphologically	O
different	O
languages	O
.	O

This	O
work	O
was	O
supported	O
in	O
part	O
by	O
grant	O
NPRP	O
6	O
-	O
716	O
-	O
1	O
-	O
138	O
from	O
the	O
Qatar	O
National	O
Research	O
Fund	O
,	O
by	O
DARPA	B-DatasetName
DEFT	O
grant	O
FA8750	O
-	O
12	O
-	O
2	O
-	O
0347	O
and	O
by	O
DARPA	B-DatasetName
LORELEI	O
grant	O
HR0011	O
-	O
15	O
-	O
2	O
-	O
0041	O
.	O
The	O
views	O
expressed	O
are	O
those	O
of	O
the	O
authors	O
and	O
do	O
not	O
reflect	O
the	O
official	O
policy	O
or	O
position	O
of	O
the	O
Department	O
of	O
Defense	O
or	O
the	O
U.S	O
government	O
.	O
We	O
thank	O
anonymous	O
reviewers	O
for	O
their	O
helpful	O
comments	O
.	O
We	O
thank	O
Yves	O
Petinot	O
for	O
providing	O
feedback	O
on	O
the	O
paper	O
.	O
We	O
thank	O
Nizar	O
Habash	O
and	O
Mona	O
Diab	O
for	O
helpful	O
discussions	O
.	O

UWB	O
at	O
SemEval	O
-	O
2016	O
Task	O
2	O
:	O
Interpretable	O
Semantic	B-TaskName
Textual	I-TaskName
Similarity	I-TaskName
with	O
Distributional	O
Semantics	O
for	O
Chunks	O

We	O
introduce	O
a	O
system	O
focused	O
on	O
solving	O
SemEval	O
2016	O
Task	O
2	O
-	O
Interpretable	O
Semantic	B-TaskName
Textual	I-TaskName
Similarity	I-TaskName
.	O
The	O
system	O
explores	O
machine	O
learning	O
and	O
rule	O
-	O
based	O
approaches	O
to	O
the	O
task	O
.	O
We	O
focus	O
on	O
machine	O
learning	O
and	O
experiment	O
with	O
a	O
wide	O
variety	O
of	O
machine	O
learning	O
algorithms	O
as	O
well	O
as	O
with	O
several	O
types	O
of	O
features	O
.	O
The	O
core	O
of	O
our	O
system	O
consists	O
in	O
exploiting	O
distributional	O
semantics	O
to	O
compare	O
similarity	O
of	O
sentence	O
chunks	O
.	O
The	O
system	O
won	O
the	O
competition	O
in	O
2016	O
in	O
the	O
"	O
Gold	O
standard	O
chunk	O
scenario	O
"	O
.	O
We	O
have	O
not	O
participated	O
in	O
the	O
"	O
System	O
chunk	O
scenario	O
"	O
.	O

The	O
goal	O
of	O
the	O
Interpretable	O
Semantic	B-TaskName
Textual	I-TaskName
Similarity	I-TaskName
task	O
is	O
to	O
go	O
deeper	O
with	O
the	O
assessment	O
of	O
semantic	B-TaskName
textual	I-TaskName
similarity	I-TaskName
of	O
sentence	O
pairs	O
.	O
It	O
is	O
requested	O
to	O
add	O
an	O
explanatory	O
layer	O
that	O
offers	O
a	O
deeper	O
insight	O
into	O
the	O
sentence	O
similarities	O
.	O
The	O
sentences	O
are	O
split	O
into	O
chunks	O
and	O
the	O
first	O
goal	O
is	O
to	O
find	O
corresponding	O
chunks	O
(	O
with	O
respect	O
to	O
their	O
meanings	O
)	O
among	O
the	O
compared	O
sentences	O
.	O
When	O
the	O
corresponding	O
chunks	O
are	O
known	O
,	O
the	O
chunks	O
are	O
annotated	O
with	O
their	O
similarity	O
scores	O
and	O
their	O
relation	O
types	O
(	O
e.g.	O
equivalent	O
,	O
more	O
specific	O
,	O
etc	O
)	O
.	O
The	O
task	O
follows	O
a	O
pilot	O
task	O
from	O
the	O
preceding	O
SemEval	O
2015	O
competition	O
(	O
Agirre	O
et	O
al	O
,	O
2015	O
)	O
.	O
The	O
best	O
performing	O
systems	O
adopted	O
various	O
approaches	O
,	O
(	O
Banjade	O
et	O
al	O
,	O
2015	O
)	O
relied	O
on	O
handcrafter	O
rules	O
,	O
(	O
Karumuri	O
et	O
al	O
,	O
2015	O
)	O
employed	O
a	O
classifier	O
for	O
relation	O
types	O
and	O
they	O
associated	O
each	O
relation	O
with	O
a	O
precomputed	O
similarity	O
score	O
and	O
(	O
Hänig	O
et	O
al	O
,	O
2015	O
)	O
extended	O
their	O
word	B-TaskName
alignment	I-TaskName
algorithm	O
for	O
the	O
task	O
.	O

The	O
data	O
consist	O
of	O
sentence	O
pairs	O
S	O
a	O
i	O
and	O
S	O
b	O
i	O
,	O
where	O
a	O
denotes	O
the	O
first	O
item	O
of	O
the	O
pair	O
,	O
b	O
denotes	O
the	O
second	O
item	O
of	O
the	O
pair	O
and	O
i	O
indexes	O
the	O
sentences	O
(	O
for	O
simiplicity	O
we	O
further	O
omit	O
i	O
for	O
sentences	O
)	O
.	O
We	O
perceive	O
a	O
sentence	O
S	O
a	O
to	O
be	O
an	O
ordered	O
set	O
of	O
chunks	O
CH	O
a	O
j	O
S	O
a	O
and	O
the	O
chunks	O
to	O
be	O
ordered	O
sets	O
of	O
words	O
w	O
k	O
CH	O
a	O
j	O
(	O
and	O
analogically	O
for	O
sentence	O
S	O
b	O
)	O
.	O
Next	O
we	O
define	O
two	O
functions	O
:	O
sim	O
(	O
CH	O
a	O
i	O
,	O
CH	O
b	O
j	O
)	O
{	O
0	B-DatasetName
,	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
,	O
5	O
}	O
for	O
chunk	O
similarity	O
and	O
rel	O
(	O
CH	O
a	O
i	O
,	O
CH	O
b	O
j	O
)	O
TYPE	O
for	O
chunk	O
relation	O
type	O
.	O
The	O
possible	O
types	O
are	O
:	O
TYPE	O
=	O
{	O
EQUI	O
,	O
OPPO	O
,	O
SPE1	O
,	O
SPE2	O
,	O
SIMI	O
,	O
REL	O
}	O
.	O
These	O
are	O
the	O
main	O
types	O
.	O
All	O
these	O
types	O
can	O
have	O
two	O
modifiers	O
(	O
FACT	O
,	O
POL	O
)	O
.	O
The	O
modifiers	O
are	O
optionally	O
attached	O
to	O
the	O
main	O
types	O
.	O
For	O
example	O
,	O
you	O
can	O
generate	O
SPE1	O
FACT	O
.	O
For	O
more	O
information	O
,	O
please	O
see	O
the	O
annotation	O
guidelines	O
1	O
.	O

As	O
a	O
first	O
step	O
of	O
our	O
approach	O
we	O
perform	O
the	O
following	O
text	O
preprocessing	O
:	O
Stopwords	O
removal	O
-	O
we	O
mark	O
the	O
words	O
found	O
in	O
a	O
predefined	O
list	O
of	O
32	O
stopwords	O
.	O
Special	O
character	O
removal	O
we	O
remove	O
special	O
characters	O
that	O
violate	O
the	O
tokenization	O
.	O
E.g.	O
in	O
one	O
of	O
the	O
datasets	O
,	O
dots	O
,	O
commas	O
,	O
quotation	O
marks	O
and	O
other	O
punctuation	O
characters	O
were	O
present	O
in	O
tokens	O
.	O
Lowercasing	O
-	O
we	O
remove	O
casing	O
from	O
the	O
words	O
.	O
Lemmatization	B-TaskName
-	O
we	O
find	O
lemmas	O
with	O
the	O
Stanford	O
CoreNLP	O
tool	O
.	O
Our	O
preprocessing	O
rather	O
adds	O
new	O
information	O
and	O
does	O
not	O
modify	O
the	O
original	O
information	O
.	O
Thus	O
,	O
the	O
original	O
word	O
and	O
all	O
the	O
generated	O
variants	O
are	O
always	O
available	O
.	O
In	O
this	O
way	O
,	O
we	O
can	O
generate	O
the	O
output	O
file	O
with	O
identical	O
words	O
(	O
including	O
the	O
special	O
characters	O
)	O
from	O
the	O
input	O
.	O
The	O
dataset	O
are	O
already	O
tokenized	O
.	O

We	O
divide	O
the	O
employed	O
features	O
into	O
four	O
categories	O
:	O
lexical	O
,	O
syntactic	O
,	O
semantic	O
,	O
external	O
.	O
Lexical	O
features	O
consist	O
of	O
the	O
following	O
features	O
:	O
word	O
base	O
form	O
overlap	O
,	O
word	O
lemma	B-DatasetName
overlap	O
,	O
chunk	O
length	O
difference	O
,	O
word	O
sentence	O
positions	O
difference	O
.	O
Syntactic	O
features	O
contains	O
closest	O
common	O
parent	O
comparison	O
(	O
we	O
compute	O
the	O
closest	O
common	O
parent	O
of	O
all	O
words	O
for	O
each	O
chunk	O
in	O
the	O
parse	O
tree	O
and	O
retrieve	O
the	O
name	O
of	O
the	O
parent	O
node	O
)	O
,	O
parse	O
tree	O
path	O
comparison	O
(	O
we	O
compute	O
the	O
path	O
from	O
the	O
root	O
of	O
the	O
sentence	O
to	O
the	O
chunk	O
)	O
.	O
POS	O
(	O
Part	O
Of	O
Speech	O
)	O
count	O
difference	O
(	O
e.g.	O
differences	O
in	O
counts	O
of	O
nouns	O
,	O
adjectives	O
,	O
verbs	O
,	O
etc	O
)	O
.	O
POS	O
tagging	O
and	O
syntactic	O
parsing	O
are	O
performed	O
with	O
Stanford	O
CoreNLP	O
.	O
Semantic	O
features	O
are	O
described	O
in	O
Section	O
2.2	O
.	O
Additionally	O
,	O
some	O
members	O
of	O
our	O
team	O
participated	O
in	O
the	O
STS	B-TaskName
task	O
(	O
task	O
1	O
)	O
of	O
the	O
SemEval	O
2016	O
(	O
Brychcín	O
and	O
Svoboda	O
,	O
2016	O
)	O
and	O
they	O
annotated	O
the	O
semantic	B-TaskName
similarity	I-TaskName
of	O
the	O
whole	O
sentences	O
with	O
their	O
system	O
for	O
us	O
.	O
This	O
score	O
is	O
used	O
as	O
one	O
feature	O
.	O
External	O
features	O
consist	O
of	O
the	O
WordNet	O
-	O
Lin	O
similarity	O
metric	O
(	O
Lin	O
,	O
1998	O
)	O
and	O
the	O
paraphrase	O
database	O
(	O
Ganitkevitch	O
et	O
al	O
,	O
2013	O
)	O
feature	O
.	O

We	O
attempt	O
to	O
solve	O
the	O
task	O
with	O
a	O
rule	O
-	O
based	O
approach	O
as	O
well	O
.	O
First	O
,	O
we	O
define	O
the	O
similarity	O
of	O
chunks	O
as	O
described	O
in	O
Section	O
2.2	O
.	O
The	O
similarity	O
is	O
then	O
used	O
for	O
the	O
chunk	O
alignment	O
.	O
We	O
employ	O
an	O
algorithm	O
inspired	O
by	O
the	O
IBM	O
word	O
model	O
II	O
for	O
machine	B-TaskName
translation	I-TaskName
(	O
Brown	O
et	O
al	O
,	O
1993	O
)	O
.	O
We	O
iterate	O
over	O
all	O
chunks	O
from	O
sentence	O
S	O
a	O
and	O
find	O
the	O
chunk	O
with	O
maximal	O
similarity	O
from	O
sentence	O
S	O
b	O
.	O
More	O
chunks	O
from	O
sentence	O
S	O
a	O
can	O
be	O
aligned	O
to	O
one	O
chunk	O
in	O
the	O
sentence	O
S	O
b	O
.	O
In	O
this	O
way	O
,	O
we	O
obtain	O
N:1	O
mapping	O
.	O
Then	O
,	O
we	O
do	O
the	O
same	O
with	O
the	O
reversed	O
order	O
of	O
sentences	O
and	O
get	O
the	O
1	O
:	O
M	O
mapping	O
.	O
Then	O
,	O
we	O
compare	O
the	O
mappings	O
and	O
take	O
the	O
one	O
with	O
the	O
highest	O
overall	O
similarity	O
.	O
In	O
this	O
way	O
,	O
it	O
is	O
ensured	O
that	O
we	O
generate	O
only	O
valid	O
mappings	O
(	O
unlike	O
in	O
the	O
previous	O
case	O
of	O
machine	O
learning	O
-	O
see	O
Section	O
3.3	O
)	O
.	O
The	O
relation	O
types	O
are	O
then	O
determined	O
by	O
an	O
extremely	O
simple	O
algorithm	O
:	O
If	O
the	O
similarity	O
is	O
5	O
,	O
then	O
the	O
relation	O
type	O
is	O
EQUI	O
.	O
If	O
the	O
similarity	O
is	O
4	O
or	O
3	O
and	O
chunks	O
contain	O
the	O
same	O
amount	O
of	O
words	O
,	O
then	O
the	O
relation	O
is	O
SIMI	O
.	O
If	O
the	O
similarity	O
is	O
4	O
or	O
3	O
,	O
then	O
chunk	O
with	O
more	O
words	O
is	O
more	O
specific	O
.	O
If	O
the	O
similarity	O
is	O
2	O
or	O
1	O
,	O
then	O
the	O
relation	O
is	O
SIMI	O
.	O
If	O
the	O
similarity	O
is	O
0	B-DatasetName
,	O
then	O
the	O
relation	O
type	O
is	O
NOALI	O
.	O

The	O
machine	O
learning	O
approach	O
with	O
combination	O
of	O
methods	O
for	O
the	O
distributional	O
semantics	O
(	O
Word2Vec	O
and	O
GloVe	B-MethodName
)	O
proved	O
to	O
be	O
very	O
capable	O
of	O
solving	O
the	O
advanced	O
task	O
of	O
Interpretable	O
Semantic	B-TaskName
Textual	I-TaskName
Similarity	I-TaskName
.	O
We	O
have	O
chosen	O
not	O
to	O
tune	O
the	O
system	O
for	O
individual	O
datasets	O
but	O
to	O
tune	O
it	O
for	O
the	O
task	O
as	O
a	O
whole	O
.	O
The	O
modified	O
lexical	O
semantic	O
vectors	O
approach	O
seems	O
to	O
be	O
an	O
attractive	O
alternative	O
to	O
the	O
more	O
traditional	O
vector	O
composition	O
.	O

Event2Mind	B-DatasetName
:	O
Commonsense	O
Inference	O
on	O
Events	O
,	O
Intents	O
,	O
and	O
Reactions	O

The	O
first	O
type	O
of	O
pragmatic	O
inference	O
is	O
about	O
intent	O
.	O
We	O
define	O
intent	O
as	O
an	O
explanation	O
of	O
why	O
the	O
agent	B-DatasetName
causes	O
a	O
volitional	O
event	O
to	O
occur	O
(	O
or	O
"	O
none	O
"	O
if	O
the	O
event	O
phrase	O
was	O
unintentional	O
)	O
.	O
The	O
intent	O
can	O
be	O
considered	O
a	O
mental	O
pre	O
-	O
condition	O
of	O
an	O
action	O
or	O
an	O
event	O
.	O
For	O
example	O
,	O
if	O
the	O
event	O
phrase	O
is	O
PersonX	O
takes	O
a	O
stab	O
at	O
,	O
the	O
annotated	O
intent	O
might	O
be	O
that	O
"	O
PersonX	O
wants	O
to	O
solve	O
a	O
problem	O
"	O
.	O
The	O
second	O
type	O
of	O
pragmatic	O
inference	O
is	O
about	O
emotional	O
reaction	O
.	O
We	O
define	O
reaction	O
as	O
an	O
explanation	O
of	O
how	O
the	O
mental	O
states	O
of	O
the	O
agent	B-DatasetName
and	O
other	O
people	O
involved	O
in	O
the	O
event	O
would	O
change	O
as	O
a	O
result	O
.	O
The	O
reaction	O
can	O
be	O
considered	O
a	O
mental	O
post	O
-	O
condition	O
of	O
an	O
action	O
or	O
an	O
event	O
.	O
For	O
example	O
,	O
if	O
the	O
event	O
phrase	O
is	O
that	O
PersonX	O
gives	O
PersonY	O
as	O
a	O
gift	O
,	O
PersonX	O
might	O
"	O
feel	O
good	O
about	O
themselves	O
"	O
as	O
a	O
result	O
,	O
and	O
PersonY	O
might	O
"	O
feel	O
grateful	O
"	O
or	O
"	O
feel	O
thankful	O
"	O
.	O

We	O
extract	O
phrasal	O
events	O
from	O
three	O
different	O
corpora	O
for	O
broad	O
coverage	O
:	O
the	O
ROC	O
Story	O
training	O
set	O
(	O
Mostafazadeh	O
et	O
al	O
,	O
2016	O
)	O
,	O
the	O
Google	B-DatasetName
Syntactic	O
N	O
-	O
grams	O
(	O
Goldberg	O
and	O
Orwant	O
,	O
2013	O
)	O
,	O
and	O
the	O
Spinn3r	O
corpus	O
(	O
Gordon	O
and	O
Swanson	O
,	O
2008	O
)	O
.	O
We	O
derive	O
events	O
from	O
the	O
set	O
of	O
verb	O
phrases	O
in	O
our	O
corpora	O
,	O
based	O
on	O
syntactic	O
parses	O
(	O
Klein	O
and	O
Manning	O
,	O
2003	O
)	O
.	O
We	O
then	O
replace	O
the	O
predicate	O
subject	O
and	O
other	O
entities	O
with	O
the	O
typed	O
variables	O
(	O
e.g.	O
,	O
PersonX	O
,	O
PersonY	O
)	O
,	O
and	O
selectively	O
substitute	O
verb	O
arguments	O
with	O
blanks	O
(	O
)	O
.	O
We	O
use	O
frequency	O
thresholds	O
to	O
select	O
events	O
to	O
annotate	O
(	O
for	O
details	O
,	O
see	O
Appendix	O
A.1	O
)	O
.	O
Additionally	O
,	O
we	O
supplement	O
the	O
list	O
of	O
events	O
with	O
all	O
2	O
,	O
000	O
verb	O
idioms	O
found	O
in	O
Wiktionary	O
,	O
in	O
order	O
to	O
cover	O
events	O
that	O
are	O
less	O
compositional	O
.	O
2	O
Our	O
final	O
annotation	O
corpus	O
contains	O
nearly	O
25	O
,	O
000	O
event	O
phrases	O
,	O
spanning	O
over	O
1	O
,	O
300	O
unique	O
verb	O
predicates	O
(	O
Table	O
2	O
)	O
.	O

We	O
design	O
an	O
Amazon	O
Mechanical	O
Turk	O
task	O
to	O
annotate	O
the	O
mental	O
pre	O
-	O
and	O
post	O
-	O
conditions	O
of	O
event	O
phrases	O
.	O
A	O
snippet	O
of	O
our	O
MTurk	O
HIT	O
design	O
is	O
shown	O
in	O
Figure	O
2	O
.	O
For	O
each	O
phrase	O
,	O
we	O
ask	O
three	O
annotators	O
whether	O
the	O
agent	B-DatasetName
of	O
the	O
event	O
,	O
PersonX	O
,	O
intentionally	O
causes	O
the	O
event	O
,	O
and	O
if	O
so	O
,	O
to	O
provide	O
up	O
to	O
three	O
possible	O
textual	O
descriptions	O
of	O
their	O
intents	O
.	O
We	O
then	O
ask	O
annotators	O
to	O
provide	O
up	O
to	O
three	O
possible	O
reactions	O
that	O
PersonX	O
might	O
experience	O
as	O
a	O
result	O
.	O
We	O
also	O
ask	O
annotators	O
to	O
provide	O
up	O
to	O
three	O
possible	O
reactions	O
of	O
other	O
people	O
,	O
when	O
applicable	O
.	O
These	O
other	O
people	O
can	O
be	O
either	O
explicitly	O
mentioned	O
(	O
e.g.	O
,	O
"	O
PersonY	O
"	O
in	O
PersonX	O
punches	O
PersonY	O
's	O
lights	O
out	O
)	O
,	O
or	O
only	O
implied	O
(	O
e.g.	O
,	O
given	O
the	O
event	O
description	O
PersonX	O
yells	O
at	O
the	O
classroom	O
,	O
we	O
can	O
infer	O
that	O
other	O
people	O
such	O
as	O
"	O
students	O
"	O
in	O
the	O
classroom	O
may	O
be	O
affected	O
by	O
the	O
act	O
of	O
PersonX	O
)	O
.	O
For	O
quality	O
control	O
,	O
we	O
periodically	O
removed	O
workers	O
with	O
high	O
disagreement	O
rates	O
,	O
at	O
our	O
discretion	O
.	O

Our	O
dataset	O
contains	O
nearly	O
25	O
,	O
000	O
event	O
phrases	O
,	O
with	O
annotators	O
rating	O
91	O
%	O
of	O
our	O
extracted	O
events	O
as	O
"	O
valid	O
"	O
(	O
i.e.	O
,	O
the	O
event	O
makes	O
sense	O
)	O
.	O
Of	O
those	O
events	O
,	O
annotations	O
for	O
the	O
multiple	O
choice	O
portions	O
of	O
the	O
task	O
(	O
whether	O
or	O
not	O
there	O
exists	O
intent	O
/	O
reaction	O
)	O
agree	O
moderately	O
,	O
with	O
an	O
average	O
Cohen	O
's	O
	O
=	O
0.45	O
(	O
Table	O
2	O
)	O
.	O
The	O
individual	O
	O
scores	O
generally	O
indicate	O
that	O
turkers	O
disagree	O
half	O
as	O
often	O
as	O
if	O
they	O
were	O
randomly	O
selecting	O
answers	O
.	O
Importantly	O
,	O
this	O
level	O
of	O
agreement	O
is	O
acceptable	O
in	O
our	O
task	O
formulation	O
for	O
two	O
reasons	O
.	O
First	O
,	O
unlike	O
linguistic	O
annotations	O
on	O
syntax	O
or	O
semantics	O
where	O
experts	O
in	O
the	O
corresponding	O
theory	O
would	O
generally	O
agree	O
on	O
a	O
single	O
correct	O
label	O
,	O
pragmatic	O
interpretations	O
may	O
better	O
be	O
defined	O
as	O
distributions	O
over	O
multiple	O
correct	O
labels	O
(	O
e.g.	O
,	O
after	O
PersonX	O
takes	O
a	O
test	O
,	O
PersonX	O
might	O
feel	O
relieved	O
and/or	O
stressed	O
;	O
de	O
Marneffe	O
et	O
al	O
,	O
2012	O
)	O
.	O
Second	O
,	O
because	O
we	O
formulate	O
our	O
task	O
as	O
a	O
conditional	O
language	O
modeling	O
problem	O
,	O
where	O
a	O
distribution	O
over	O
the	O
textual	O
descriptions	O
of	O
intents	O
and	O
reactions	O
is	O
conditioned	O
on	O
the	O
event	O
description	O
,	O
this	O
variation	O
in	O
the	O
labels	O
is	O
only	O
as	O
expected	O
.	O
A	O
majority	O
of	O
our	O
events	O
are	O
annotated	O
as	O
willingly	O
caused	O
by	O
the	O
agent	B-DatasetName
(	O
86	O
%	O
,	O
Cohen	O
's	O
	O
=	O
0.48	O
)	O
,	O
and	O
26	O
%	O
involve	O
other	O
people	O
(	O
	O
=	O
0.41	O
)	O
.	O
Most	O
event	O
patterns	O
in	O
our	O
data	O
are	O
fully	O
instantiated	O
,	O
with	O
only	O
22	O
%	O
containing	O
blanks	O
(	O
)	O
.	O
In	O
our	O
corpus	O
,	O
the	O
intent	O
annotations	O
are	O
slightly	O
longer	O
(	O
3.4	O
words	O
on	O
average	O
)	O
than	O
the	O
reaction	O
annotations	O
(	O
1.5	O
words	O
)	O
.	O

Through	O
Event2Mind	B-DatasetName
inference	O
,	O
we	O
can	O
attempt	O
to	O
bring	O
to	O
the	O
surface	O
what	O
is	O
implied	O
about	O
people	O
's	O
behavior	O
and	O
mental	O
states	O
.	O
We	O
employ	O
this	O
inference	O
to	O
analyze	O
implicit	O
bias	O
in	O
modern	O
films	O
.	O
As	O
shown	O
in	O
Figure	O
7	O
,	O
our	O
model	O
is	O
able	O
to	O
analyze	O
character	O
portrayal	O
beyond	O
what	O
is	O
explicit	O
in	O
text	O
,	O
by	O
performing	O
pragmatic	O
inference	O
on	O
character	O
actions	O
to	O
explain	O
aspects	O
of	O
a	O
character	O
's	O
mental	O
state	O
.	O
In	O
this	O
section	O
,	O
we	O
use	O
our	O
model	O
's	O
inference	O
to	O
shed	O
light	O
on	O
gender	O
differences	O
in	O
intents	O
behind	O
and	O
reactions	O
to	O
characters	O
'	O
actions	O
.	O

For	O
our	O
portrayal	O
analyses	O
,	O
we	O
use	O
scene	O
descriptions	O
from	O
772	O
movie	O
scripts	O
released	O
by	O
Gorinski	O
and	O
Lapata	O
(	O
2015	O
)	O
,	O
assigned	O
to	O
over	O
21	O
,	O
000	O
characters	O
as	O
done	O
by	O
Sap	O
et	O
al	O
(	O
2017	O
)	O
.	O
We	O
extract	O
events	O
from	O
the	O
scene	O
descriptions	O
,	O
and	O
generate	O
their	O
10	O
most	O
probable	O
intent	O
and	O
reaction	O
sequences	O
using	O
our	O
BiRNN	O
sequence	O
model	O
(	O
as	O
in	O
Figure	O
7	O
)	O
.	O
We	O
then	O
categorize	O
generated	O
intents	O
and	O
reactions	O
into	O
groups	O
based	O
on	O
LIWC	O
category	O
scores	O
of	O
the	O
generated	O
output	O
(	O
Tausczik	O
and	O
Pennebaker	O
,	O
2016	O
)	O
.	O
3	O
The	O
intent	O
and	O
reaction	O
categories	O
are	O
then	O
(	O
1990	O
,	O
bottom	O
)	O
,	O
augmented	O
with	O
Event2mind	B-DatasetName
inferences	O
on	O
the	O
characters	O
'	O
intents	O
and	O
reactions	O
.	O
E.g.	O
,	O
our	O
model	O
infers	O
that	O
the	O
event	O
PersonX	O
sits	O
on	O
PersonX	O
's	O
bed	O
,	O
lost	O
in	O
thought	O
implies	O
that	O
the	O
agent	B-DatasetName
,	O
Vivian	O
,	O
is	O
sad	O
or	O
worried	O
.	O
aggregated	O
for	O
each	O
character	O
,	O
and	O
standardized	O
(	O
zero	O
-	O
mean	O
and	O
unit	O
variance	O
)	O
.	O
We	O
compute	O
correlations	O
with	O
gender	O
for	O
each	O
category	O
of	O
intent	O
or	O
reaction	O
using	O
a	O
logistic	B-MethodName
regression	I-MethodName
model	O
,	O
testing	O
significance	O
while	O
using	O
Holm	O
's	O
correction	O
for	O
multiple	O
comparisons	O
(	O
Holm	O
,	O
1979	O
)	O
.	O
4	O
To	O
account	O
for	O
the	O
gender	O
skew	O
in	O
scene	O
presence	O
(	O
29.4	O
%	O
of	O
scenes	O
have	O
women	O
)	O
,	O
we	O
statistically	O
control	O
for	O
the	O
total	O
number	O
of	O
words	O
in	O
a	O
character	O
's	O
scene	O
descriptions	O
.	O
Note	O
that	O
the	O
original	O
event	O
phrases	O
are	O
all	O
gender	O
agnostic	O
,	O
as	O
their	O
participants	O
have	O
been	O
replaced	O
by	O
variables	O
(	O
e.g.	O
,	O
PersonX	O
)	O
.	O
We	O
also	O
find	O
that	O
the	O
types	O
of	O
gender	O
biases	O
uncovered	O
remain	O
similar	O
when	O
we	O
run	O
these	O
analyses	O
on	O
the	O
human	O
annotations	O
or	O
the	O
generated	O
words	O
and	O
phrases	O
from	O
the	O
BiRNN	O
with	O
n	O
-	O
gram	O
re	O
-	O
ranking	O
decoding	O
setup	O
.	O
and	O
Needs	O
'	O
,	O
'	O
Personal	O
Concerns	O
'	O
,	O
'	O
Biological	O
Processes	O
'	O
,	O
'	O
Cognitive	O
Processes	O
'	O
,	O
'	O
Social	O
Words	O
'	O
,	O
'	O
Affect	O
Words	O
'	O
,	O
'	O
Perceptual	O
Processes	O
'	O
.	O
We	O
refer	O
the	O
reader	O
to	O
Tausczik	O
and	O
Pennebaker	O
(	O
2016	O
)	O
or	O
http://liwc.wpengine.com/	O
compare	O
-	O
dictionaries/	O
for	O
a	O
complete	O
list	O
of	O
category	O
descriptions	O
.	O
4	O
Given	O
the	O
data	O
limitation	O
,	O
we	O
represent	O
gender	O
as	O
a	O
binary	O
,	O
but	O
acknowledge	O
that	O
gender	O
is	O
a	O
more	O
complex	O
social	O
construct	O
.	O

Intents	O
and	O
Reactions	O
Our	O
Event2Mind	B-DatasetName
inferences	O
automate	O
portrayal	O
analyses	O
that	O
previously	O
required	O
manual	O
annotations	O
(	O
Behm	O
-	O
Morawitz	O
and	O
Mastro	O
,	O
2008	O
;	O
Prentice	O
and	O
Carranza	O
,	O
2002	O
;	O
England	O
et	O
al	O
,	O
2011	O
)	O
.	O
Shown	O
in	O
Table	O
4	O
,	O
our	O
results	O
indicate	O
a	O
gender	O
bias	O
in	O
the	O
behavior	O
ascribed	O
to	O
characters	O
,	O
consistent	O
with	O
psychology	O
and	O
gender	O
studies	O
literature	O
(	O
Collins	O
,	O
2011	O
)	O
.	O
Specifically	O
,	O
events	O
with	O
female	O
semantic	O
agents	O
are	O
intended	O
to	O
be	O
helpful	O
to	O
other	O
people	O
(	O
intents	O
involving	O
FRIEND	O
,	O
FAMILY	O
,	O
and	O
AFFILIATION	O
)	O
,	O
particularly	O
relating	O
to	O
eating	O
and	O
making	O
food	O
for	O
themselves	O
and	O
others	O
(	O
INGEST	O
,	O
BODY	O
)	O
.	O
Events	O
with	O
male	O
agents	O
on	O
the	O
other	O
hand	O
are	O
motivated	O
by	O
and	O
resulting	O
in	O
achievements	O
(	O
ACHIEVE	O
,	O
MONEY	O
,	O
REWARDS	O
,	O
POWER	O
)	O
.	O
Women	O
's	O
looks	O
and	O
sexuality	O
are	O
also	O
emphasized	O
,	O
as	O
their	O
actions	O
'	O
intents	O
and	O
reactions	O
are	O
sexual	O
,	O
seen	O
,	O
or	O
felt	O
(	O
SEXUAL	O
,	O
SEE	O
,	O
PERCEPT	O
)	O
.	O
Men	O
's	O
actions	O
,	O
on	O
the	O
other	O
hand	O
,	O
are	O
motivated	O
by	O
violence	O
or	O
fighting	O
(	O
DEATH	O
,	O
ANGER	O
,	O
RISK	O
)	O
,	O
with	O
strong	O
negative	O
reactions	O
(	O
SAD	O
,	O
ANGER	O
,	O
NEGA	O
-	O

Prior	O
work	O
has	O
sought	O
formal	O
frameworks	O
for	O
inferring	O
roles	O
and	O
other	O
attributes	O
in	O
relation	O
to	O
events	O
(	O
Baker	O
et	O
al	O
,	O
1998	O
;	O
Das	O
et	O
al	O
,	O
2014	O
;	O
Schuler	O
et	O
al	O
,	O
2009	O
;	O
Hartshorne	O
et	O
al	O
,	O
2013	O
,	O
inter	O
alia	O
)	O
,	O
implicitly	O
connoted	O
by	O
events	O
(	O
Reisinger	O
et	O
al	O
,	O
2015	O
;	O
White	O
et	O
al	O
,	O
2016	O
;	O
Greene	O
,	O
2007	O
;	O
Rashkin	O
et	O
al	O
,	O
2016	O
)	O
,	O
or	O
sentiment	O
polarities	O
of	O
events	O
(	O
Ding	O
and	O
Riloff	O
,	O
2016	O
;	O
Choi	O
and	O
Wiebe	O
,	O
2014	O
;	O
Russo	O
et	O
al	O
,	O
2015	O
;	O
Ding	O
and	O
Riloff	O
,	O
2018	O
)	O
.	O
In	O
addition	O
,	O
recent	O
work	O
has	O
studied	O
the	O
patterns	O
which	O
evoke	O
certain	O
polarities	O
(	O
Reed	O
et	O
al	O
,	O
2017	O
)	O
,	O
the	O
desires	O
which	O
make	O
events	O
affective	O
(	O
Ding	O
et	O
al	O
,	O
2017	O
)	O
,	O
the	O
emotions	O
caused	O
by	O
events	O
(	O
Vu	O
et	O
al	O
,	O
2014	O
)	O
,	O
or	O
,	O
conversely	O
,	O
identifying	O
events	O
or	O
reasoning	O
behind	O
particular	O
emotions	O
(	O
Gui	O
et	O
al	O
,	O
2017	O
)	O
.	O
Compared	O
to	O
this	O
prior	O
literature	O
,	O
our	O
work	O
uniquely	O
learns	O
to	O
model	O
intents	O
and	O
reactions	O
over	O
a	O
diverse	O
set	O
of	O
events	O
,	O
includes	O
inference	O
over	O
event	O
participants	O
not	O
explicitly	O
mentioned	O
in	O
text	O
,	O
and	O
formulates	O
the	O
task	O
as	O
predicting	O
the	O
textual	O
descriptions	O
of	O
the	O
implied	O
commonsense	O
instead	O
of	O
classifying	O
various	O
event	O
attributes	O
.	O
Previous	O
work	O
in	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
has	O
focused	O
on	O
linguistic	O
entailment	O
(	O
Bowman	O
et	O
al	O
,	O
2015	O
;	O
Bos	O
and	O
Markert	O
,	O
2005	O
)	O
while	O
ours	O
focuses	O
on	O
commonsense	O
-	O
based	O
inference	O
.	O
There	O
also	O
has	O
been	O
inference	O
or	O
entailment	O
work	O
that	O
is	O
more	O
generation	O
focused	O
:	O
generating	O
,	O
e.g.	O
,	O
entailed	O
statements	O
(	O
Zhang	O
et	O
al	O
,	O
2017	O
;	O
Blouw	O
and	O
Eliasmith	O
,	O
2018	O
)	O
,	O
explanations	O
of	O
causality	O
(	O
Kang	O
et	O
al	O
,	O
2017	O
)	O
,	O
or	O
paraphrases	O
(	O
Dong	O
et	O
al	O
,	O
2017	O
)	O
.	O
Our	O
work	O
also	O
aims	O
at	O
generating	O
inferences	O
from	O
sentences	O
;	O
however	O
,	O
our	O
models	O
infer	O
implicit	O
information	O
about	O
mental	O
states	O
and	O
causality	O
,	O
which	O
has	O
not	O
been	O
studied	O
by	O
most	O
previous	O
systems	O
.	O
Also	O
related	O
are	O
commonsense	O
knowledge	O
bases	O
(	O
Espinosa	O
and	O
Lieberman	O
,	O
2005	O
;	O
Speer	O
and	O
Havasi	O
,	O
2012	O
)	O
.	O
Our	O
work	O
complements	O
these	O
ex	O
-	O
isting	O
resources	O
by	O
providing	O
commonsense	O
relations	O
that	O
are	O
relatively	O
less	O
populated	O
in	O
previous	O
work	O
.	O
For	O
instance	O
,	O
ConceptNet	B-DatasetName
contains	O
only	O
25	O
%	O
of	O
our	O
events	O
,	O
and	O
only	O
12	O
%	O
have	O
relations	O
that	O
resemble	O
intent	O
and	O
reaction	O
.	O
We	O
present	O
a	O
more	O
detailed	O
comparison	O
with	O
ConceptNet	B-DatasetName
in	O
Appendix	O
C.	O

We	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
insightful	O
comments	O
.	O
We	O
also	O
thank	O
xlab	O
members	O
at	O
the	O
University	O
of	O
Washington	O
,	O
Martha	O
Palmer	O
,	O
Tim	O
O'Gorman	O
,	O
Susan	O
Windisch	O
Brown	O
,	O
Ghazaleh	O
Kazeminejad	O
as	O
well	O
as	O
other	O
members	O
at	O
the	O
University	O
of	O
Colorado	O
at	O
Boulder	O
for	O
many	O
helpful	O
comments	O
for	O
our	O
development	O
of	O
the	O
annotation	O
pipeline	O
.	O
This	O
work	O
was	O
supported	O
in	O
part	O
by	O
National	O
Science	O
Foundation	O
Graduate	O
Research	O
Fellowship	O
Program	O
under	O
grant	O
DGE	O
-	O
1256082	O
,	O
NSF	O
grant	O
IIS	O
-	O
1714566	O
,	O
and	O
the	O
DARPA	B-DatasetName
CwC	O
program	O
through	O
ARO	O
(	O
W911NF	O
-	O
15	O
-	O
1	O
-	O
0543	O
)	O
.	O

Learning	O
to	O
Generalize	O
to	O
More	O
:	O
Continuous	O
Semantic	O
Augmentation	O
for	O
Neural	O
Machine	B-TaskName
Translation	I-TaskName

The	O
principal	O
task	O
in	O
supervised	O
neural	O
machine	B-TaskName
translation	I-TaskName
(	O
NMT	O
)	O
is	O
to	O
learn	O
to	O
generate	O
target	O
sentences	O
conditioned	O
on	O
the	O
source	O
inputs	O
from	O
a	O
set	O
of	O
parallel	O
sentence	O
pairs	O
,	O
and	O
thus	O
produce	O
a	O
model	O
capable	O
of	O
generalizing	O
to	O
unseen	O
instances	O
.	O
However	O
,	O
it	O
is	O
commonly	O
observed	O
that	O
the	O
generalization	O
performance	O
of	O
the	O
model	O
is	O
highly	O
influenced	O
by	O
the	O
amount	O
of	O
parallel	O
data	O
used	O
in	O
training	O
.	O
Although	O
data	B-TaskName
augmentation	I-TaskName
is	O
widely	O
used	O
to	O
enrich	O
the	O
training	O
data	O
,	O
conventional	O
methods	O
with	O
discrete	O
manipulations	O
fail	O
to	O
generate	O
diverse	O
and	O
faithful	O
training	O
samples	O
.	O
In	O
this	O
paper	O
,	O
we	O
present	O
a	O
novel	O
data	B-TaskName
augmentation	I-TaskName
paradigm	O
termed	O
Continuous	O
Semantic	O
Augmentation	O
(	O
CSANMT	O
)	O
,	O
which	O
augments	O
each	O
training	O
instance	O
with	O
an	O
adjacency	O
semantic	O
region	O
that	O
could	O
cover	O
adequate	O
variants	O
of	O
literal	O
expression	O
under	O
the	O
same	O
meaning	O
.	O
We	O
conduct	O
extensive	O
experiments	O
on	O
both	O
rich	O
-	O
resource	O
and	O
low	O
-	O
resource	O
settings	O
involving	O
various	O
language	O
pairs	O
,	O
including	O
WMT14	B-DatasetName
English	O
{	O
German	O
,	O
French	O
}	O
,	O
NIST	O
Chinese	O
English	O
and	O
multiple	O
low	O
-	O
resource	O
IWSLT	O
translation	O
tasks	O
.	O
The	O
provided	O
empirical	O
evidences	O
show	O
that	O
CSANMT	O
sets	O
a	O
new	O
level	O
of	O
performance	O
among	O
existing	O
augmentation	O
techniques	O
,	O
improving	O
on	O
the	O
state	O
-	O
of	O
-	O
theart	O
by	O
a	O
large	O
margin	O
.	O
1	O

Data	B-TaskName
Augmentation	I-TaskName
(	O
DA	O
)	O
Kobayashi	O
,	O
2018	O
;	O
Gao	O
et	O
al	O
,	O
2019	O
;	O
Khayrallah	O
et	O
al	O
,	O
2020	O
;	O
Pham	O
et	O
al	O
,	O
2021	O
)	O
has	O
been	O
widely	O
used	O
in	O
neural	O
machine	B-TaskName
translation	I-TaskName
.	O
The	O
most	O
popular	O
one	O
is	O
the	O
family	O
of	O
back	O
-	O
translation	O
(	O
Sennrich	O
et	O
al	O
,	O
2016a	O
;	O
Nguyen	O
et	O
al	O
,	O
2020	O
)	O
,	O
which	O
utilizes	O
a	O
target	O
-	O
to	O
-	O
source	O
model	O
to	O
translate	O
monolingual	O
target	O
sentences	O
back	O
into	O
the	O
source	O
language	O
.	O
Besides	O
,	O
constructing	O
adversarial	O
training	O
instances	O
with	O
diverse	O
literal	O
forms	O
via	O
word	O
replacing	O
or	O
embedding	O
interpolating	O
(	O
Wang	O
et	O
al	O
,	O
2018	O
;	O
Cheng	O
et	O
al	O
,	O
2020	O
)	O
is	O
beneficial	O
to	O
improve	O
the	O
generalization	O
performance	O
of	O
NMT	O
models	O
.	O
Vicinal	O
Risk	O
Minimization	O
(	O
VRM	O
)	O
(	O
Chapelle	O
et	O
al	O
,	O
2000	O
)	O
is	O
another	O
principle	O
of	O
data	B-TaskName
augmentation	I-TaskName
,	O
in	O
which	O
DA	O
is	O
formalized	O
as	O
extracting	O
additional	O
pseudo	O
samples	O
from	O
the	O
vicinal	O
distribution	O
of	O
observed	O
instances	O
.	O
Typically	O
the	O
vicinity	O
of	O
a	O
training	O
example	O
is	O
defined	O
using	O
datasetdependent	O
heuristics	O
,	O
such	O
as	O
color	O
(	O
scale	O
,	O
mixup	B-MethodName
)	O
augmentation	O
(	O
Simonyan	O
and	O
Zisserman	O
,	O
2014	O
;	O
Krizhevsky	O
et	O
al	O
,	O
2012	O
;	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
in	O
computer	O
vision	O
and	O
adversarial	O
augmentation	O
with	O
manifold	O
neighborhoods	O
(	O
Ng	O
et	O
al	O
,	O
2020	O
;	O
Cheng	O
et	O
al	O
,	O
2021	O
)	O
in	O
NLP	O
.	O
Our	O
approach	O
relates	O
to	O
VRM	O
that	O
involves	O
with	O
an	O
adjacency	O
semantic	O
region	O
as	O
the	O
vicinity	O
manifold	O
for	O
each	O
training	O
instance	O
.	O
Sentence	O
Representation	B-TaskName
Learning	I-TaskName
is	O
a	O
well	O
investigated	O
area	O
with	O
dozens	O
of	O
methods	O
(	O
Kiros	O
et	O
al	O
,	O
2015	O
;	O
.	O
In	O
recent	O
years	O
,	O
the	O
methods	O
built	O
on	O
large	O
pre	O
-	O
trained	O
models	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
;	O
Conneau	O
et	O
al	O
,	O
2020	O
)	O
have	O
been	O
widely	O
used	O
for	O
learning	O
sentence	O
level	O
representations	O
(	O
Reimers	O
and	O
Gurevych	O
,	O
2019	O
;	O
Huang	O
et	O
al	O
,	O
2019	O
;	O
Yang	O
et	O
al	O
,	O
2019	O
)	O
.	O
Our	O
work	O
is	O
also	O
related	O
to	O
the	O
methods	O
that	O
aims	O
at	O
learning	O
the	O
uni	O
-	O
versal	O
representation	O
(	O
Zhang	O
et	O
al	O
,	O
2016	O
;	O
Schwenk	O
and	O
Douze	O
,	O
2017	O
;	O
for	O
multiple	O
semantically	O
-	O
equivalent	O
sentences	O
in	O
NMT	O
.	O
In	O
this	O
context	O
,	O
contrastive	B-MethodName
learning	I-MethodName
has	O
become	O
a	O
popular	O
paradigm	O
in	O
NLP	O
(	O
Kong	O
et	O
al	O
,	O
2020	O
;	O
Clark	O
et	O
al	O
,	O
2020	O
;	O
Gao	O
et	O
al	O
,	O
2021	O
)	O
.	O
The	O
most	O
related	O
work	O
are	O
and	O
Chi	O
et	O
al	O
(	O
2021	O
)	O
,	O
which	O
suggested	O
transforming	O
cross	O
-	O
lingual	O
sentences	O
into	O
a	O
shared	O
vector	O
by	O
contrastive	O
objectives	O
.	O

Sampling	O
Strategies	O
(	O
i	O
)	O
,	O
r	O
x	O
′	O
(	O
j	O
)	O
ω	O
(	O
k	O
)	O
∼	O
ηN	O
0	B-DatasetName
,	O
diag	O
(	O
W	O
2	O
r	O
)	O
+	O
(	O
1.0	O
−	O
η	O
)	O
N	O
0	B-DatasetName
,	O
1	O
2	O
ditto	O
ω	O
(	O
k	O
)	O
∼	O
ηU	O
−	O
W	O
r	O
,	O
W	O
r	O
+	O
(	O
1.0	O
−	O
η	O
)	O
U	O
ā	O
−	O
1	O
,	O
1	O
−ā	O
whereā	O
=	O
1	O
k−1	O
k−1	O
i=1	O
ω	O
(	O
i	O
)	O
3	O
E	O
(	O
x	O
(	O
i	O
)	O
,	O
y	O
(	O
i	O
)	O
)	O
∼B	O
−	O
KL	O
p	O
(	O
r	O
x	O
(	O
i	O
)	O
)	O
∥	O
q	O
(	O
r	O
x	O
(	O
i	O
)	O
,	O
r	O
y	O
(	O
i	O
)	O
)	O
r	O
x	O
=	O
µ	O
+	O
ϵ	O
σ	O
where	O
p	O
(	O
r	O
x	O
(	O
i	O
)	O
)	O
∼	O
N	O
(	O
µ	O
,	O
σ	O
2	O
)	O
and	O
q	O
(	O
r	O
x	O
(	O
i	O
)	O
,	O
r	O
y	O
(	O
i	O
)	O
)	O
∼	O
N	O
(	O
µ	O
′	O
,	O
σ	O
′2	O
)	O
where	O
ϵ	O
is	O
a	O
standard	O
Gaussian	O
noise	O
4	O
E	O
(	O
x	O
(	O
i	O
)	O
,	O
y	O
(	O
i	O
)	O
)	O
∼B	O
r	O
T	O
x	O
(	O
i	O
)	O
r	O
y	O
(	O
i	O
)	O
∥r	O
x	O
(	O
i	O
)	O
∥	O
∥r	O
y	O
(	O
i	O
)	O
∥	O
ω	O
(	O
k	O
)	O
∼	O
ηN	O
0	B-DatasetName
,	O
diag	O
(	O
W	O
2	O
r	O
)	O
+	O
(	O
1.0	O
−	O
η	O
)	O
N	O
1	O
k−1	O
k−1	O
i=1	O
ω	O
(	O
i	O
)	O
,	O
1	O

In	O
this	O
section	O
,	O
we	O
study	O
the	O
robustness	O
of	O
our	O
CSANMT	O
towards	O
both	O
noisy	O
inputs	O
and	O
the	O
translationese	O
effect	O
(	O
Volansky	O
et	O
al	O
,	O
2013	O
)	O
on	O
new	O
-	O
stest2014	O
for	O
the	O
WMT14	B-DatasetName
English	O
-	O
German	O
task	O
.	O
Noisy	O
Inputs	O
.	O
Inspired	B-DatasetName
by	O
(	O
Gao	O
et	O
al	O
,	O
2019	O
)	O
,	O
we	O
construct	O
noisy	O
test	O
sets	O
via	O
several	O
strategies	O
described	O
as	O
follows	O
:	O
Original	O
:	O
the	O
original	O
testset	O
without	O
any	O
manipulations	O
;	O
WS	O
:	O
word	O
swap	O
,	O
randomly	O
swap	O
words	O
in	O
nearby	O
positions	O
within	O
a	O
window	O
size	O
3	O
(	O
Artetxe	O
et	O
al	O
,	O
2018	O
;	O
Lample	O
et	O
al	O
,	O
2018b	O
)	O
;	O
WD	O
:	O
word	O
dropout	O
,	O
randomly	O
drop	O
words	O
with	O
a	O
ratio	O
of	O
15	O
%	O
(	O
Iyyer	O
et	O
al	O
,	O
2015	O
;	O
Lample	O
et	O
al	O
,	O
2018b	O
)	O
;	O
WR	O
:	O
word	O
replace	O
,	O
randomly	O
replace	O
word	O
tokens	O
with	O
a	O
placeholder	O
token	O
(	O
e.g.	O
,	O
[	O
UNK	O
]	O
)	O
(	O
Xie	O
et	O
al	O
,	O
2017	O
)	O
or	O
with	O
a	O
relevant	O
(	O
measured	O
by	O
the	O
similarity	O
of	O
word	B-TaskName
embeddings	I-TaskName
)	O
alternative	O
.	O
The	O
replacement	O
ratio	O
also	O
is	O
15	O
%	O
.	O
natural	O
source	O
translationese	O
target	O
(	O
X	O
Y	O
*	O
)	O
;	O
translationese	O
source	O
natural	O
target	O
(	O
X	O
*	O
Y	O
)	O
;	O
round	O
-	O
trip	O
translationese	O
source	O
translationese	O
target	O
(	O
X	O
*	O
*	O
Y	O
*	O
)	O
,	O
where	O
X	O
Y	O
*	O
X	O
*	O
*	O
.	O
Results	O
.	O
As	O
shown	O
in	O
Table	O
9	O
,	O
our	O
approach	O
shows	O
better	O
robustness	O
over	O
two	O
baseline	O
methods	O
across	O
various	O
artificial	O
noises	O
.	O
Moreover	O
,	O
CSANMT	O
consistently	O
outperforms	O
the	O
baseline	O
in	O
all	O
three	O
translationese	O
scenarios	O
,	O
the	O
same	O
is	O
true	O
for	O
back	O
-	O
translation	O
.	O
However	O
,	O
Edunov	O
et	O
al	O
(	O
2020	O
)	O
shows	O
that	O
BT	O
improves	O
only	O
in	O
the	O
X	O
*	O
Y	O
scenario	O
.	O
Our	O
explanation	O
for	O
the	O
inconsistency	O
is	O
that	O
BT	O
without	O
monolingual	O
data	O
in	O
our	O
setting	O
benefits	O
from	O
the	O
natural	O
parallel	O
data	O
to	O
deal	O
with	O
the	O
translationese	O
sources	O
.	O

)	O
)	O
codes	O
with	O
60	O
K	O
merge	O
operations	O
to	O
build	O
two	O
vocabularies	O
comprising	O
47	O
K	O
Chinese	O
sub	O
-	O
words	O
and	O
30	O
K	O
English	O
sub	O
-	O
words	O
.	O
For	O
the	O
En	O
De	O
task	O
,	O
we	O
employ	O
the	O
popular	O
WMT14	B-DatasetName
dataset	O
,	O
which	O
consists	O
of	O
approximately	O
4.5	O
M	O
sentence	O
pairs	O
for	O
training	O
.	O
We	O
select	O
newstest2013	O
as	O
the	O
validation	O
set	O
and	O
newstest2014	O
as	O
the	O
test	O
set	O
.	O
All	O
sentences	O
had	O
been	O
jointly	O
byte	O
-	O
pair	O
-	O
encoded	O
with	O
32	O
K	O
merge	O
operations	O
,	O
which	O
results	O
in	O
a	O
shared	O
source	O
-	O
target	O
vocabulary	O
of	O
about	O
37	O
K	O
tokens	O
.	O
For	O
the	O
En	O
Fr	O
task	O
,	O
we	O
use	O
the	O
significantly	O
larger	O
WMT14	B-DatasetName
dataset	O
consisting	O
of	O
36	O
M	O
sentence	O
pairs	O
.	O
The	O
combination	O
of	O
{	O
newstest2012	O
,	O
2013	O
}	O
was	O
used	O
for	O
model	B-TaskName
selection	I-TaskName
and	O
the	O
experimental	O
results	O
were	O
reported	O
on	O
newstest2014	O
.	O

Traditionally	O
,	O
dialogue	O
systems	O
have	O
been	O
characterized	O
in	O
terms	O
of	O
whether	O
they	O
are	O
task	O
-	O
or	O
non	O
-	O
task	O
-	O
oriented	O
.	O
In	O
task	B-TaskName
-	I-TaskName
oriented	I-TaskName
dialogue	I-TaskName
systems	I-TaskName
,	O
such	O
as	O
an	O
airline	O
ticket	O
reservation	O
system	O
(	O
Hemphill	O
et	O
al	O
,	O
1990	O
)	O
,	O
eliciting	O
specific	O
information	O
from	O
the	O
user	O
,	O
such	O
as	O
the	O
date	O
,	O
time	O
,	O
and	O
destination	O
of	O
the	O
flight	O
,	O
is	O
an	O
important	O
functionality	O
for	O
completing	O
the	O
task	O
.	O
However	O
,	O
in	O
non	O
-	O
task	B-TaskName
-	I-TaskName
oriented	I-TaskName
dialogue	I-TaskName
systems	I-TaskName
,	O
the	O
system	O
does	O
not	O
have	O
a	O
clear	O
goal	O
of	O
eliciting	O
information	O
from	O
the	O
user	O
,	O
and	O
the	O
content	O
of	O
the	O
dialogue	O
is	O
free	O
.	O
In	O
this	O
study	O
,	O
as	O
another	O
type	O
of	O
dialogue	O
system	O
,	O
we	O
focus	O
on	O
interviewing	O
systems	O
,	O
in	O
which	O
the	O
goal	O
is	O
to	O
acquire	O
a	O
user	O
model	O
through	O
a	O
flexible	O
flow	O
of	O
dialogue	O
.	O
Specifically	O
,	O
we	O
propose	O
Figure	O
1	O
:	O
Overview	O
of	O
the	O
proposed	O
method	O
:	O
taking	O
dialogue	O
history	O
as	O
input	O
,	O
a	O
model	O
predicts	O
the	O
interviewer	O
's	O
intent	O
(	O
communicative	O
function	O
)	O
,	O
another	O
model	O
decides	O
the	O
content	O
of	O
the	O
utterance	O
(	O
semantic	O
content	O
)	O
,	O
and	O
the	O
outputs	O
of	O
these	O
models	O
are	O
combined	O
to	O
generate	O
a	O
response	O
.	O
(	O
For	O
details	O
,	O
refer	O
to	O
Section	O
4	O
)	O
a	O
method	O
for	O
interviewing	O
a	O
user	O
's	O
preference	O
for	O
food	O
.	O
To	O
generate	O
such	O
dialogues	O
,	O
the	O
system	O
must	O
be	O
able	O
to	O
generate	O
appropriate	O
questions	O
to	O
elicit	O
the	O
user	O
's	O
preferences	O
for	O
food	O
while	O
touching	O
on	O
various	O
topics	O
in	O
the	O
food	O
domain	O
,	O
such	O
as	O
how	O
to	O
eat	O
,	O
how	O
to	O
cook	O
,	O
etc	O
.	O
,	O
without	O
limiting	O
the	O
content	O
of	O
the	O
dialogue	O
as	O
a	O
task	O
-	O
oriented	O
dialogue	O
does	O
.	O
One	O
possible	O
approach	O
for	O
achieving	O
the	O
requirements	O
discussed	O
above	O
is	O
end	O
-	O
to	O
-	O
end	O
neural	O
network	O
,	O
where	O
dialogue	B-TaskName
generation	I-TaskName
is	O
the	O
task	O
of	O
predicting	O
the	O
next	O
utterance	O
using	O
dialogue	O
history	O
as	O
input	O
(	O
Vinyals	O
and	O
Le	O
,	O
2015	O
;	O
.	O
This	O
method	O
is	O
widely	O
used	O
to	O
generate	O
open	O
-	O
domain	O
dialogues	O
,	O
such	O
as	O
chitchats	O
.	O
However	O
,	O
it	O
requires	O
a	O
large	O
amount	O
of	O
dialogue	O
data	O
to	O
learn	O
the	O
model	O
.	O
Otherwise	O
,	O
less	O
informative	O
and	O
contextually	O
inappropriate	O
utterances	O
are	O
frequently	O
generated	O
.	O
To	O
overcome	O
this	O
drawback	O
,	O
we	O
propose	O
a	O
method	O
that	O
first	O
determines	O
the	O
intention	O
and	O
semantic	O
content	O
of	O
the	O
interviewer	O
's	O
next	O
utterance	O
and	O
then	O
combines	O
these	O
to	O
generate	O
questions	O
from	O
the	O
interviewer	O
.	O
Figure	O
1	O
shows	O
the	O
proposed	O
approach	O
.	O
First	O
,	O
we	O
trained	O
two	O
models	O
.	O
The	O
first	O
is	O
a	O
classification	O
model	O
that	O
takes	O
the	O
dialogue	O
history	O
as	O
input	O
and	O
determines	O
the	O
interviewer	O
's	O
intention	O
for	O
the	O
next	O
utterance	O
.	O
The	O
second	O
is	O
a	O
generator	O
model	O
,	O
which	O
also	O
takes	O
the	O
dialogue	O
history	O
as	O
input	O
and	O
outputs	O
the	O
semantic	O
content	O
of	O
the	O
utterance	O
,	O
including	O
the	O
target	O
(	O
e.g.	O
,	O
dish	O
or	O
ingredient	O
)	O
mentioned	O
in	O
the	O
utterance	O
and	O
its	O
related	O
information	O
(	O
e.g.	O
,	O
taste	O
or	O
how	O
to	O
eat	O
)	O
.	O
Next	O
,	O
a	O
template	O
for	O
sentence	O
generation	O
is	O
selected	O
based	O
on	O
these	O
two	O
outputs	O
,	O
and	O
they	O
are	O
applied	O
to	O
the	O
selected	O
template	O
to	O
generate	O
sentences	O
.	O
Compared	O
to	O
learning	O
a	O
model	O
that	O
directly	O
generates	O
a	O
surface	O
expression	O
,	O
the	O
models	O
for	O
predicting	O
the	O
intent	O
and	O
semantic	O
content	O
of	O
an	O
utterance	O
can	O
be	O
learned	O
using	O
a	O
smaller	O
amount	O
of	O
data	O
.	O
Additionally	O
,	O
because	O
the	O
content	O
of	O
an	O
utterance	O
is	O
determined	O
based	O
on	O
the	O
context	O
obtained	O
from	O
the	O
dialogue	O
history	O
,	O
appropriate	O
utterances	O
that	O
are	O
related	O
to	O
the	O
preceding	O
utterances	O
can	O
be	O
generated	O
.	O
The	O
contributions	O
of	O
this	O
study	O
are	O
as	O
follows	O
:	O
Collection	O
of	O
118	O
text	O
-	O
based	O
dialogues	O
for	O
interviewing	O
food	O
preferences	O
.	O
Proposal	O
of	O
an	O
annotation	O
schema	O
for	O
utterance	O
intention	O
and	O
semantic	O
content	O
of	O
utterances	O
,	O
and	O
creation	O
of	O
a	O
dataset	O
with	O
these	O
annotations	O
.	O
Creation	O
of	O
a	O
classification	O
model	O
for	O
utterance	O
intention	O
and	O
a	O
generative	O
model	O
of	O
semantic	O
content	O
of	O
utterances	O
.	O
Demonstration	O
of	O
the	O
effectiveness	O
of	O
the	O
proposed	O
method	O
using	O
an	O
automated	O
evaluation	O
method	O
.	O
Presentation	O
of	O
examples	O
of	O
dialogues	O
generated	O
by	O
the	O
proposed	O
method	O
,	O
and	O
discussion	O
of	O
the	O
quality	O
of	O
the	O
dialogues	O
.	O

Task	O
-	O
oriented	O
dialog	O
systems	O
are	O
typically	O
designed	O
to	O
collect	O
information	O
from	O
users	O
.	O
For	O
example	O
,	O
previous	O
studies	O
have	O
proposed	O
an	O
airline	O
ticket	O
reservation	O
system	O
(	O
TIS	O
)	O
(	O
Hemphill	O
et	O
al	O
,	O
1990	O
)	O
,	O
a	O
restaurant	O
reservation	O
system	O
(	O
Henderson	O
et	O
al	O
,	O
2014	O
)	O
,	O
and	O
interview	O
systems	O
to	O
collect	O
information	O
,	O
such	O
as	O
public	O
opinion	O
polls	O
and	O
class	O
evaluation	O
interview	O
systems	O
(	O
Johnston	O
et	O
al	O
,	O
2013	O
;	O
Stent	O
et	O
al	O
,	O
2006	O
)	O
.	O
In	O
these	O
systems	O
,	O
the	O
purpose	O
of	O
the	O
dialogue	O
is	O
to	O
obtain	O
information	O
to	O
accomplish	O
a	O
predefined	O
task	O
.	O
Meanwhile	O
,	O
chitchat	O
does	O
not	O
have	O
a	O
clear	O
goal	O
as	O
a	O
task	O
-	O
oriented	O
dialogue	O
does	O
,	O
but	O
this	O
type	O
of	O
dialogue	O
has	O
the	O
potential	O
to	O
elicit	O
a	O
variety	O
of	O
information	O
from	O
the	O
user	O
.	O
For	O
example	O
,	O
the	O
system	O
asks	O
follow	O
-	O
up	O
questions	O
such	O
as	O
"	O
Please	O
tell	O
me	O
more	O
about	O
the	O
keyword	O
"	O
by	O
using	O
a	O
keyword	O
from	O
the	O
user	O
's	O
preceding	O
utterance	O
.	O
To	O
improve	O
such	O
interviewing	O
functionality	O
,	O
relevant	O
topics	O
and	O
questions	O
should	O
be	O
selected	O
and	O
the	O
dialogue	O
strategies	O
should	O
be	O
modified	O
.	O
To	O
address	O
these	O
issues	O
,	O
we	O
propose	O
a	O
method	O
to	O
determine	O
the	O
target	O
object	O
and	O
semantic	O
content	O
of	O
the	O
system	O
response	O
based	O
on	O
the	O
dialogue	O
context	O
.	O
Previous	O
studies	O
on	O
dialogue	B-TaskName
generation	I-TaskName
have	O
proposed	O
different	O
techniques	O
to	O
generate	O
task	O
-	O
and	O
non	O
-	O
task	O
-	O
oriented	O
dialogue	O
.	O
Early	O
studies	O
on	O
generating	O
open	O
-	O
domain	O
chitchat	O
proposed	O
DNN	O
-	O
based	O
techniques	O
to	O
generate	O
system	O
responses	O
by	O
exploiting	O
the	O
data	O
-	O
driven	O
approach	O
(	O
Sordoni	O
et	O
al	O
,	O
2015a	O
;	O
Vinyals	O
and	O
Le	O
,	O
2015	O
;	O
.	O
Recent	O
studies	O
have	O
proposed	O
incorporating	O
useful	O
information	O
(	O
that	O
is	O
relevant	O
to	O
the	O
domain	O
)	O
and	O
responses	O
into	O
the	O
model	O
,	O
thus	O
improving	O
the	O
quality	O
of	O
generated	O
responses	O
(	O
Li	O
et	O
al	O
,	O
2018	O
)	O
.	O
Some	O
studies	O
have	O
exploited	O
word	O
-	O
based	O
information	O
,	O
such	O
as	O
nouns	O
extracted	O
from	O
the	O
user	O
's	O
preceding	O
utterances	O
and	O
a	O
set	O
of	O
keywords	O
predicted	O
to	O
be	O
used	O
in	O
the	O
response	O
(	O
Serban	O
et	O
al	O
,	O
2017	O
;	O
Xu	O
et	O
al	O
,	O
2021	O
)	O
.	O
Other	O
studies	O
have	O
used	O
knowledge	O
ontologies	O
,	O
including	O
commonsense	O
(	O
Wu	O
et	O
al	O
,	O
2020	O
;	O
Moon	O
et	O
al	O
,	O
2019	O
;	O
Galetzka	O
et	O
al	O
,	O
2021	O
)	O
.	O
However	O
,	O
these	O
end	O
-	O
to	O
-	O
end	O
methods	O
,	O
in	O
which	O
training	O
models	O
directly	O
generate	O
system	O
responses	O
,	O
require	O
a	O
large	O
amount	O
of	O
training	O
data	O
,	O
and	O
our	O
corpus	O
was	O
not	O
sufficiently	O
large	O
for	O
this	O
approach	O
.	O
In	O
traditional	O
task	B-TaskName
-	I-TaskName
oriented	I-TaskName
dialogue	I-TaskName
systems	I-TaskName
,	O
the	O
information	O
required	O
to	O
achieve	O
the	O
dialogue	O
goals	O
is	O
limited	O
to	O
the	O
task	O
domain	O
.	O
Therefore	O
,	O
the	O
internal	O
state	O
of	O
the	O
system	O
is	O
defined	O
as	O
a	O
slot	O
-	O
value	O
pair	O
,	O
and	O
the	O
system	O
generates	O
responses	O
through	O
the	O
following	O
modules	O
:	O
a	O
)	O
understanding	O
the	O
user	O
's	O
utterance	O
,	O
b	O
)	O
determining	O
the	O
system	O
action	O
(	O
e.g.	O
,	O
the	O
intention	O
and	O
the	O
slot	O
-	O
value	O
as	O
the	O
utterance	O
content	O
)	O
based	O
on	O
the	O
internal	O
state	O
,	O
and	O
c	O
)	O
generating	O
a	O
response	O
sentence	O
from	O
the	O
system	O
action	O
.	O
The	O
action	O
of	O
the	O
system	O
is	O
determined	O
by	O
rule	O
-	O
based	O
,	O
statistical	O
-	O
based	O
(	O
Young	O
et	O
al	O
,	O
2010	O
)	O
,	O
deep	O
learning	O
(	O
Chen	O
et	O
al	O
,	O
2019	O
)	O
and	O
reinforcement	O
learning	O
approaches	O
(	O
Sankar	O
and	O
Ravi	O
,	O
2019	O
)	O
.	O
In	O
this	O
study	O
,	O
we	O
exploited	O
the	O
approach	O
described	O
above	O
,	O
which	O
represents	O
the	O
interviewer	O
's	O
utterance	O
as	O
structured	O
semantic	O
content	O
composed	O
of	O
the	O
intent	O
of	O
the	O
utterance	O
,	O
the	O
objects	O
mentioned	O
in	O
the	O
utterance	O
,	O
and	O
their	O
attributes	O
and	O
values	O
.	O
We	O
created	O
a	O
machine	O
learning	O
model	O
to	O
predict	O
these	O
types	O
of	O
information	O
and	O
generate	O
responses	O
based	O
on	O
the	O
determined	O
actions	O
.	O

This	O
section	O
proposes	O
a	O
Communicative	O
Function	O
Prediction	O
(	O
CFP	O
)	O
model	O
that	O
predicts	O
the	O
communicative	O
function	O
label	O
to	O
specify	O
the	O
intention	O
of	O
the	O
next	O
interviewer	O
's	O
message	O
,	O
such	O
as	O
selfdisclosure	O
and	O
questions	O
.	O
A	O
fine	O
-	O
tuning	O
approach	O
was	O
employed	O
to	O
train	O
the	O
CFP	O
model	O
.	O
We	O
used	O
the	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
Japanese	O
pre	O
-	O
trained	O
model	O
3	O
.	O
As	O
demonstrated	O
in	O
Figure	O
4	O
,	O
the	O
input	O
is	O
a	O
dialogue	O
context	O
consisting	O
of	O
multiple	O
previous	O
messages	O
concatenated	O
using	O
[	O
SEP	O
]	O
.	O
This	O
sequence	O
is	O
the	O
same	O
as	O
that	O
used	O
to	O
train	O
the	O
SCG	O
model	O
in	O
Section	O
4.1	O
.	O
Using	O
this	O
sequence	O
as	O
the	O
input	O
,	O
we	O
trained	O
a	O
model	O
that	O
predicted	O
the	O
communicative	O
function	O
label	O
of	O
the	O
interviewer	O
's	O
next	O
message	O
.	O
We	O
use	O
the	O
representation	O
of	O
the	O
final	O
layer	O
of	O
the	O
special	O
classification	O
token	O
(	O
[	O
CLS	O
]	O
)	O
,	O
which	O
is	O
placed	O
at	O
the	O
beginning	O
of	O
the	O
input	O
,	O
as	O
the	O
input	O
for	O
a	O
downstream	O
classification	O
task	O
.	O
As	O
described	O
in	O
Section	O
5.1	O
,	O
the	O
communicative	O
function	O
classifier	O
predicts	O
7	O
labels	O
,	O
reduced	O
from	O
the	O
32	O
labels	O
presented	O
in	O
Section	O
3.2	O
.	O

Table	O
4	O
presents	O
the	O
sequence	O
of	O
five	O
context	O
utterances	O
and	O
the	O
interviewer	O
's	O
utterance	O
which	O
follows	O
the	O
context	O
.	O
"	O
Human	O
"	O
is	O
the	O
real	O
interviewer	O
utterance	O
(	O
ground	O
truth	O
)	O
.	O
"	O
Retrieval	O
,	O
"	O
"	O
Text	B-TaskName
Generation	I-TaskName
,	O
"	O
and	O
"	O
Proposed	O
"	O
are	O
the	O
outputs	O
by	O
the	O
methods	O
examined	O
in	O
our	O
experiment	O
.	O
In	O
Dialogue	O
-	O
1	O
in	O
Table	O
4	O
,	O
the	O
interviewer	O
utterance	O
generated	O
by	O
the	O
retrieval	O
model	O
asks	O
whether	O
the	O
user	O
eats	O
vegetables	O
.	O
This	O
utterance	O
is	O
not	O
appropriate	O
because	O
in	O
previous	O
-	O
3	O
,	O
the	O
customer	O
had	O
already	O
said	O
that	O
he	O
/	O
she	O
eats	O
vegetables	O
.	O
By	O
contrast	O
,	O
the	O
proposed	O
model	O
generated	O
a	O
question	O
to	O
elicit	O
more	O
information	O
according	O
to	O
the	O
current	O
context	O
of	O
the	O
hot	O
-	O
pot	O
dish	O
by	O
asking	O
the	O
favorite	O
ingredients	O
for	O
the	O
dish	O
.	O
In	O
Dialogue	O
-	O
2	O
in	O
Table	O
4	O
,	O
all	O
three	O
models	O
failed	O
to	O
generate	O
an	O
utterance	O
about	O
the	O
current	O
topic	O
focus	O
(	O
cheese	O
)	O
,	O
but	O
the	O
retrieval	O
and	O
text	B-TaskName
generation	I-TaskName
models	O
still	O
successfully	O
generated	O
a	O
natural	O
response	O
.	O
However	O
,	O
the	O
utterance	O
generated	O
by	O
the	O
proposed	O
model	O
appears	O
to	O
be	O
abrupt	O
.	O
This	O
is	O
because	O
the	O
selected	O
template	O
was	O
not	O
appropriate	O
or	O
expressive	O
.	O
Providing	O
more	O
templates	O
and	O
improving	O
the	O
template	O
selection	O
mechanism	O
are	O
necessary	O
to	O
generate	O
more	O
expressive	O
responses	O
.	O

The	O
#	O
MeToo	O
movement	O
on	O
social	O
media	O
platforms	O
initiated	O
discussions	O
over	O
several	O
facets	O
of	O
sexual	O
harassment	O
in	O
our	O
society	O
.	O
Prior	O
work	O
by	O
the	O
NLP	O
community	O
for	O
automated	O
identification	O
of	O
the	O
narratives	O
related	O
to	O
sexual	O
abuse	O
disclosures	O
barely	O
explored	O
this	O
social	O
phenomenon	O
as	O
an	O
independent	O
task	O
.	O
However	O
,	O
emotional	O
attributes	O
associated	O
with	O
textual	O
conversations	O
related	O
to	O
the	O
#	O
MeToo	O
social	O
movement	O
are	O
complexly	O
intertwined	O
with	O
such	O
narratives	O
.	O
We	O
formulate	O
the	O
task	O
of	O
identifying	O
narratives	O
related	O
to	O
the	O
sexual	O
abuse	O
disclosures	O
in	O
online	O
posts	O
as	O
a	O
joint	O
modeling	O
task	O
that	O
leverages	O
their	O
emotional	O
attributes	O
through	O
multitask	O
learning	O
.	O
Our	O
results	O
demonstrate	O
that	O
positive	O
knowledge	O
transfer	O
via	O
context	O
-	O
specific	O
shared	O
representations	O
of	O
a	O
flexible	O
cross	O
-	O
stitched	O
parameter	O
sharing	O
model	O
helps	O
establish	O
the	O
inherent	O
benefit	O
of	O
jointly	O
modeling	O
tasks	O
related	O
to	O
sexual	O
abuse	O
disclosures	O
with	O
emotion	B-TaskName
classification	I-TaskName
from	O
the	O
text	O
in	O
homogeneous	O
and	O
heterogeneous	O
settings	O
.	O
We	O
show	O
how	O
for	O
more	O
domain	O
-	O
specific	O
tasks	O
related	O
to	O
sexual	O
abuse	O
disclosures	O
such	O
as	O
sarcasm	O
identification	O
and	O
dialogue	O
act	O
(	O
refutation	O
,	O
justification	O
,	O
allegation	O
)	O
classification	O
,	O
homogeneous	O
multitask	O
learning	O
is	O
helpful	O
,	O
whereas	O
for	O
more	O
general	O
tasks	O
such	O
as	O
stance	O
and	O
hate	B-TaskName
speech	I-TaskName
detection	I-TaskName
,	O
heterogeneous	O
multitask	O
learning	O
with	O
emotion	B-TaskName
classification	I-TaskName
works	O
better	O
.	O
1	O

The	O
#	O
MeToo	O
movement	O
2	O
was	O
started	O
as	O
an	O
initiative	O
to	O
empower	O
women	O
against	O
long	O
-	O
standing	O
issues	O
related	O
to	O
sexual	O
abuse	O
at	O
workplaces	O
,	O
public	O
spaces	O
,	O
and	O
private	O
organizations	O
(	O
McKenna	O
and	O
Chughtai	O
,	O
2020	O
)	O
.	O
The	O
usage	O
of	O
a	O
dedicated	O
hashtag	O
#	O
MeToo	O
on	O
media	O
platforms	O
signified	O
a	O
social	O
support	O
system	O
for	O
women	O
from	O
different	O
sections	O
1	O
Code	O
&	O
Implementation	O
:	O
https://github.com/	O
midas	O
-	O
research	O
/	O
metoo	O
-	O
mtl	O
-	O
naacl	O
2	O
https://metoomvmt.org/	O
of	O
society	O
.	O
The	O
movement	O
initiated	O
discussions	O
on	O
many	O
socially	O
stigmatized	O
issues	O
that	O
were	O
missing	O
from	O
the	O
virtual	O
space	O
(	O
Clark	O
-	O
Parsons	O
,	O
2019	O
)	O
.	O
Such	O
conversations	O
invited	O
various	O
reactions	O
on	O
the	O
web	O
,	O
involving	O
support	O
to	O
the	O
cause	O
of	O
the	O
movement	O
and	O
even	O
outright	O
bullying	O
.	O
While	O
many	O
users	O
took	O
part	O
in	O
the	O
vilification	O
of	O
the	O
survivors	O
,	O
the	O
movement	O
also	O
saw	O
opposition	O
by	O
factions	O
of	O
the	O
society	O
that	O
felt	O
threatened	O
by	O
the	O
impact	O
of	O
social	O
media	O
in	O
raising	O
awareness	O
about	O
the	O
scale	O
of	O
everyday	O
sexual	O
harassment	O
faced	O
by	O
women	O
in	O
workplaces	O
and	O
institutions	O
(	O
Tambe	O
,	O
2018	O
)	O
.	O
In	O
many	O
instances	O
,	O
the	O
public	O
disclosures	O
of	O
survivor	O
-	O
narrated	O
incidents	O
involved	O
widespread	O
use	O
of	O
hate	O
-	O
language	O
and	O
online	O
trolling	O
,	O
both	O
against	O
the	O
victims	O
and	O
alleged	O
oppressors	O
(	O
Franks	O
,	O
2019	O
)	O
.	O
The	O
#	O
MeToo	O
movement	O
also	O
led	O
to	O
people	O
coming	O
out	O
with	O
allegations	O
,	O
refutations	O
,	O
and	O
justifications	O
about	O
traumatic	O
experiences	O
as	O
they	O
transitioned	O
to	O
active	O
participants	O
in	O
the	O
mainstream	O
conversation	O
.	O
A	O
closer	O
look	O
at	O
the	O
online	O
posts	O
about	O
the	O
#	O
MeToo	O
movement	O
revealed	O
that	O
sarcasm	O
was	O
often	O
used	O
as	O
a	O
thin	O
veil	O
in	O
such	O
discussions	O
to	O
humorously	O
mask	O
disapproval	O
,	O
wit	O
,	O
and	O
personal	O
attacks	O
(	O
Sandhu	O
et	O
al	O
,	O
2019	O
)	O
.	O
The	O
complex	O
narratives	O
present	O
in	O
the	O
conversations	O
on	O
stigmatized	O
issues	O
like	O
sexual	O
abuse	O
create	O
an	O
opportunity	O
for	O
researchers	O
to	O
study	O
how	O
people	O
express	O
their	O
opinions	O
on	O
a	O
sensitive	O
topic	O
in	O
an	O
informal	O
social	O
setting	O
.	O
It	O
also	O
offers	O
a	O
chance	O
to	O
social	O
media	O
regulators	O
for	O
fostering	O
social	O
inclusion	O
,	O
community	O
integration	O
,	O
and	O
improving	O
the	O
individual	O
perception	O
of	O
being	O
supported	O
by	O
others	O
.	O
This	O
paper	O
aims	O
at	O
categorizing	O
the	O
posts	O
related	O
to	O
the	O
#	O
MeToo	O
movement	O
on	O
the	O
basis	O
of	O
stance	O
(	O
support	O
or	O
opposition	O
)	O
,	O
hate	O
-	O
speech	O
,	O
sarcasm	O
,	O
and	O
dialogue	O
acts	O
(	O
allegation	O
,	O
refutation	O
,	O
or	O
justification	O
of	O
sexual	O
misconduct	O
)	O
.	O
We	O
focus	O
our	O
analysis	O
on	O
a	O
publicly	O
available	O
dataset	O
that	O
is	O
created	O
in	O
the	O
backdrop	O
of	O
mass	O
instances	O
of	O
sexual	O
harassment	O
disclosures	O
and	O
includes	O
nuanced	O
labels	O
to	O
identify	O
accompanying	O
linguistic	O
behaviors	O
.	O
Existing	O
literature	O
has	O
emphasized	O
that	O
the	O
text	O
's	O
emotional	O
attributes	O
have	O
a	O
high	O
correlation	O
with	O
dialogue	O
narratives	O
describing	O
instances	O
of	O
sexual	O
harassment	O
(	O
Lane	O
and	O
Hedin	O
,	O
2020	O
)	O
.	O
Prior	O
works	O
(	O
Anzovino	O
et	O
al	O
,	O
2018	O
;	O
Sharifirad	O
et	O
al	O
,	O
2018	O
)	O
have	O
mostly	O
focused	O
on	O
label	O
specific	O
detection	O
of	O
linguistic	O
narratives	O
related	O
to	O
sexual	O
harassment	O
disclosures	O
in	O
isolation	O
by	O
exploiting	O
lexical	O
features	O
(	O
Chowdhury	O
et	O
al	O
,	O
2019	O
;	O
Karlekar	O
and	O
Bansal	O
,	O
2018	O
)	O
.	O
However	O
,	O
subtle	O
intricacies	O
present	O
in	O
the	O
discussion	O
of	O
sexual	O
abuse	O
disclosures	O
often	O
reflect	O
the	O
speaker	O
's	O
affective	O
and	O
psychological	O
state	O
,	O
which	O
are	O
overlooked	O
by	O
feature	O
-	O
engineered	O
models	O
.	O
For	O
instance	O
,	O
part	O
(	O
a	O
)	O
of	O
Figure	O
1	O
shows	O
a	O
tweet	O
expressing	O
support	O
towards	O
the	O
#	O
MeToo	O
movement	O
but	O
in	O
a	O
tone	O
that	O
might	O
be	O
difficult	O
for	O
naive	O
neural	O
learning	O
models	O
to	O
capture	O
without	O
context	O
.	O
Part	O
(	O
b	O
)	O
of	O
Figure	O
1	O
presents	O
a	O
tweet	O
in	O
which	O
the	O
author	O
has	O
an	O
initial	O
positive	O
outlook	O
,	O
which	O
later	O
reverses	O
to	O
disgust	O
for	O
the	O
subject	O
.	O
The	O
lack	O
of	O
context	O
about	O
the	O
event	O
and	O
contrasting	O
qualifications	O
describing	O
the	O
oppressor	O
makes	O
the	O
correct	O
classification	O
of	O
the	O
sexual	O
harassment	O
disclosure	O
label	O
extremely	O
challenging	O
for	O
traditional	O
classifiers	O
without	O
emotional	O
labels	O
'	O
additional	O
supervision	O
.	O
Moreover	O
,	O
apart	O
from	O
their	O
inherent	O
complexity	O
,	O
conversations	O
related	O
to	O
the	O
#	O
MeToo	O
movement	O
also	O
pose	O
a	O
challenge	O
of	O
emotional	O
ambiguity	O
.	O
This	O
work	O
is	O
the	O
first	O
attempt	O
at	O
joint	O
modeling	O
of	O
narratives	O
related	O
to	O
sexual	O
abuse	O
disclosures	O
and	O
emotion	B-TaskName
classification	I-TaskName
to	O
learn	O
the	O
patterns	O
of	O
their	O
interaction	O
via	O
parameter	O
sharing	O
techniques	O
offered	O
by	O
Multitask	O
Learning	O
(	O
MTL	O
)	O
.	O
The	O
affective	O
features	O
,	O
which	O
result	O
from	O
a	O
joint	O
learning	O
setup	O
through	O
shared	O
parameters	O
,	O
will	O
encompass	O
the	O
text	O
's	O
emotional	O
content	O
that	O
is	O
likely	O
to	O
be	O
predictive	O
of	O
narratives	O
corresponding	O
to	O
sexual	O
abuse	O
disclo	O
-	O
sures	O
.	O
More	O
specifically	O
,	O
we	O
formulate	O
an	O
MTL	O
framework	O
for	O
multi	B-TaskName
-	I-TaskName
label	I-TaskName
classification	I-TaskName
of	O
narratives	O
related	O
to	O
sexual	O
abuse	O
disclosures	O
(	O
stance	O
,	O
hate	O
-	O
speech	O
,	O
sarcasm	O
,	O
dialogue	O
acts	O
)	O
and	O
emotional	O
classification	O
in	O
the	O
context	O
of	O
the	O
#	O
MeToo	O
movement	O
.	O
MTL	O
(	O
Caruana	O
,	O
1997	O
)	O
allows	O
two	O
or	O
more	O
related	O
tasks	O
to	O
be	O
learned	O
jointly	O
.	O
This	O
facilitates	O
the	O
transfer	O
of	O
inductive	O
bias	O
and	O
better	O
generalization	O
across	O
related	O
tasks	O
on	O
account	O
of	O
shared	O
representations	O
of	O
linguistic	O
features	O
.	O

We	O
experiment	O
with	O
MTL	O
architectures	O
employing	O
a	O
flexible	O
cross	O
-	O
stitched	O
parameter	O
sharing	O
method	O
that	O
benefits	O
from	O
both	O
hard	O
-	O
parameter	O
sharing	O
and	O
soft	O
parameter	O
sharing	O
through	O
a	O
gated	O
mechanism	O
using	O
a	O
weighted	O
summation	O
(	O
Section	O
4	O
)	O
.	O
Hard	O
parameter	O
sharing	O
allows	O
for	O
sharing	O
lower	O
-	O
level	O
word	O
representations	O
,	O
and	O
soft	O
parameter	O
sharing	O
permits	O
the	O
sharing	O
of	O
task	O
-	O
specific	O
networks	O
.	O
We	O
explore	O
two	O
flavors	O
of	O
multitask	O
learning	O
:	O
(	O
i	O
)	O
Homogeneous	O
MTL	O
-	O
Intradomain	O
MTL	O
between	O
related	O
tasks	O
of	O
sexual	O
abuse	O
disclosure	O
narratives	O
,	O
and	O
(	O
ii	O
)	O
Heterogeneous	O
MTL	O
-	O
cross	O
-	O
domain	O
MTL	O
between	O
pairs	O
of	O
tasks	O
in	O
emotion	B-TaskName
classification	I-TaskName
and	O
narratives	O
of	O
sexual	O
abuse	O
disclosure	O
(	O
Section	O
5.2	O
)	O
.	O
Our	O
results	O
demonstrate	O
that	O
both	O
Homogeneous	O
and	O
Heterogeneous	O
MTL	O
setups	O
outperform	O
the	O
Single	O
Task	O
Learning	O
(	O
STL	O
)	O
technique	O
across	O
various	O
tasks	O
(	O
Section	O
6	O
)	O
.	O
Further	O
,	O
we	O
conduct	O
a	O
qualitative	O
analysis	O
of	O
several	O
samples	O
to	O
analyze	O
the	O
benefit	O
of	O
joint	O
training	O
of	O
related	O
tasks	O
(	O
Section	O
6.4	O
)	O
,	O
keeping	O
in	O
mind	O
the	O
ethical	O
concerns	O
of	O
communities	O
affected	O
by	O
this	O
research	O
(	O
Section	O
7	O
)	O
.	O

Sexual	O
Harassment	O
Disclosures	O
on	O
Social	O
Media	O
Several	O
works	O
have	O
focused	O
on	O
identifying	O
sexual	O
violence	O
(	O
Leatherman	O
,	O
2011	O
)	O
,	O
harassment	O
and	O
sexism	O
(	O
Wekerle	O
et	O
al	O
,	O
2018	O
;	O
Manikonda	O
et	O
al	O
,	O
2018b	O
)	O
in	O
social	O
media	O
posts	O
by	O
analyzing	O
factors	O
such	O
as	O
linguistic	O
themes	O
,	O
social	O
engagement	O
,	O
and	O
lexical	O
attributes	O
.	O
Jha	O
and	O
Mamidi	O
(	O
2017	O
)	O
experimented	O
with	O
algorithms	O
such	O
as	O
SVM	B-MethodName
and	O
BiLSTM	B-MethodName
along	O
with	O
fastText	B-MethodName
to	O
categorize	O
hostility	O
of	O
sexist	O
posts	O
.	O
(	O
Parikh	O
et	O
al	O
,	O
2019	O
)	O
proposed	O
a	O
multi	O
-	O
label	O
CNN	O
-	O
based	O
neural	O
architecture	O
along	O
with	O
word	O
and	O
sentence	O
level	O
embeddings	O
for	O
identifying	O
variants	O
of	O
sexism	O
present	O
in	O
online	O
social	O
platforms	O
.	O
Chowdhury	O
et	O
al	O
(	O
2019	O
)	O
emphasized	O
the	O
use	O
of	O
linguistic	O
themes	O
,	O
contextual	O
meta	O
-	O
data	O
,	O
and	O
semantic	O
cues	O
for	O
evaluating	O
human	O
behaviors	O
related	O
to	O
sex	O
-	O
ual	O
abuse	O
disclosures	O
.	O
All	O
of	O
these	O
works	O
have	O
dealt	O
with	O
modeling	O
sexual	O
disclosure	O
narratives	O
as	O
single	O
-	O
task	O
learning	O
problems	O
and	O
were	O
restricted	O
to	O
label	O
specific	O
detection	O
(	O
Marwa	O
et	O
al	O
,	O
2018	O
;	O
.	O
Multitask	O
Learning	O
Frameworks	O
for	O
learning	O
representations	O
across	O
two	O
different	O
sources	O
within	O
the	O
same	O
domain	O
follow	O
multitask	O
learning	O
(	O
Caruana	O
,	O
1997	O
)	O
.	O
The	O
ability	O
to	O
utilize	O
knowledge	O
from	O
various	O
sources	O
compensates	O
for	O
missing	O
data	O
and	O
complements	O
existing	O
meta	O
-	O
data	O
(	O
Tan	O
et	O
al	O
,	O
2013	O
;	O
Ding	O
et	O
al	O
,	O
2014	O
)	O
,	O
thus	O
allowing	O
for	O
effective	O
sharing	O
of	O
task	O
-	O
invariant	O
features	O
(	O
Caruana	O
,	O
1997	O
;	O
Zhang	O
and	O
Wang	O
,	O
2016	O
;	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
.	O
MTL	O
has	O
been	O
utilized	O
for	O
name	O
error	O
recognition	O
(	O
Cheng	O
et	O
al	O
,	O
2015	O
)	O
,	O
tagging	O
-	O
chunking	B-TaskName
(	O
Collobert	O
et	O
al	O
,	O
2011	O
)	O
,	O
machine	B-TaskName
translation	I-TaskName
(	O
Luong	O
et	O
al	O
,	O
2015	O
)	O
and	O
relation	B-TaskName
extraction	I-TaskName
(	O
Gupta	O
et	O
al	O
,	O
2016	O
)	O
.	O
Liu	O
et	O
al	O
(	O
2017	O
)	O
used	O
shared	O
and	O
private	O
latent	O
features	O
leveraging	O
multitask	O
learning	O
for	O
different	O
text	B-TaskName
classification	I-TaskName
tasks	O
.	O
Rajamanickam	O
et	O
al	O
(	O
2020	O
)	O
;	O
Duong	O
et	O
al	O
(	O
2016	O
)	O
;	O
Liu	O
et	O
al	O
(	O
2016	O
)	O
proposed	O
a	O
joint	O
framework	O
for	O
modeling	O
abuse	O
and	O
emotion	B-DatasetName
detection	O
and	O
showed	O
improvements	O
over	O
STL	O
and	O
transfer	B-TaskName
learning	I-TaskName
.	O
Akhtar	O
et	O
al	O
(	O
2018	O
)	O
proposed	O
a	O
multitask	O
ensemble	O
architecture	O
for	O
jointly	O
modeling	O
emotion	B-DatasetName
,	O
sentiment	O
,	O
and	O
intensity	O
,	O
which	O
gave	O
improvements	O
over	O
single	O
-	O
label	O
classification	O
.	O

We	O
aim	O
to	O
analyze	O
different	O
perspectives	O
of	O
the	O
complex	O
narratives	O
pertaining	O
to	O
the	O
#	O
MeToo	O
movement	O
on	O
social	O
media	O
platforms	O
.	O
Specifically	O
,	O
given	O
a	O
tweet	O
text	O
,	O
we	O
formulate	O
for	O
it	O
a	O
multi	O
-	O
label	O
multiclass	O
classification	O
problem	O
with	O
definitions	O
taken	O
from	O
previous	O
works	O
(	O
ElSherief	O
et	O
al	O
,	O
2018	O
)	O
Stance	B-TaskName
Detection	I-TaskName
:	O
Determining	O
the	O
opinion	O
of	O
the	O
author	O
of	O
a	O
tweet	O
,	O
regarding	O
a	O
particular	O
target	O
of	O
interest	O
(	O
Augenstein	O
et	O
al	O
,	O
2016	O
)	O
.	O
Stance	B-TaskName
detection	I-TaskName
is	O
categorized	O
into	O
three	O
classes	O
:	O
Support	O
for	O
when	O
the	O
author	O
favors	O
the	O
#	O
MeToo	O
movement	O
or	O
it	O
's	O
cause	O
;	O
Opposition	O
,	O
representing	O
opposing	O
stance	O
or	O
indifference	O
towards	O
the	O
movement	O
;	O
or	O
Neither	O
,	O
when	O
the	O
text	O
does	O
not	O
have	O
a	O
clear	O
viewpoint	O
(	O
Mohammad	O
and	O
Turney	O
,	O
2013	O
)	O
.	O
Hate	B-DatasetName
Speech	I-DatasetName
Identification	O
:	O
Detection	O
of	O
hate	B-DatasetName
speech	I-DatasetName
involves	O
labeling	O
the	O
tweets	O
as	O
Directed	O
Hate	O
if	O
the	O
comment	O
is	O
targeted	O
towards	O
an	O
individual	O
or	O
an	O
entity	O
,	O
Generalized	O
Hate	O
if	O
it	O
is	O
targeted	O
towards	O
a	O
community	O
or	O
a	O
section	O
of	O
people	O
or	O
Neither	O
otherwise	O
(	O
Basile	O
et	O
al	O
,	O
2019	O
)	O
.	O
Sarcasm	B-TaskName
Detection	I-TaskName
:	O
Given	O
a	O
tweet	O
t	O
i	O
,	O
we	O
aim	O
to	O
map	O
it	O
to	O
either	O
be	O
Sarcastic	O
or	O
Not	O
Sarcastic	O
based	O
on	O
the	O
presence	O
of	O
implicit	O
sarcastic	O
tone	O
of	O
the	O
post	O
(	O
Bamman	O
and	O
Smith	O
,	O
2015	O
)	O
.	O
Dialogue	B-TaskName
Act	I-TaskName
Classification	I-TaskName
:	O
These	O
are	O
a	O
function	O
of	O
a	O
speaker	O
's	O
utterance	O
during	O
a	O
conversation	O
,	O
for	O
example	O
,	O
question	O
,	O
answer	O
,	O
suggestion	O
,	O
etc	O
.	O
,	O
and	O
are	O
classified	O
into	O
three	O
classes	O
,	O
namely	O
Allegation	O
(	O
when	O
the	O
author	O
intends	O
to	O
allege	O
an	O
individual	O
or	O
group	O
of	O
sexual	O
misconduct	O
)	O
(	O
Hutchings	O
,	O
2012	O
)	O
,	O
Justification	O
(	O
tweets	O
where	O
the	O
author	O
is	O
justifying	O
their	O
actions	O
)	O
,	O
and	O
Refutation	O
(	O
for	O
when	O
the	O
author	O
refutes	O
any	O
accusation	O
with	O
or	O
without	O
evidence	O
)	O
.	O
Modeling	O
Settings	O
To	O
validate	O
MTL	O
's	O
performance	O
across	O
different	O
domains	O
,	O
we	O
also	O
experiment	O
with	O
emotion	B-DatasetName
detection	O
as	O
the	O
auxiliary	O
task	O
.	O
We	O
aim	O
to	O
predict	O
one	O
or	O
more	O
of	O
the	O
several	O
emotions	O
representing	O
the	O
affective	O
state	O
of	O
the	O
authors	O
-	O
(	O
anger	O
,	O
disgust	O
,	O
anticipation	O
,	O
fear	O
,	O
joy	O
,	O
love	O
,	O
optimism	O
,	O
pessimism	O
,	O
sadness	O
,	O
surprise	O
and	O
trust	O
)	O
.	O
We	O
conceptualize	O
three	O
diverse	O
problem	O
settings	O
and	O
compare	O
them	O
to	O
analyze	O
MTL	O
within	O
and	O
across	O
domains	O
.	O
These	O
are	O
(	O
i	O
)	O
Single	O
Task	O
Learning	O
:	O
Independent	O
optimization	O
of	O
the	O
four	O
mentioned	O
tasks	O
associated	O
with	O
sexual	O
abuse	O
disclosure	O
narrative	O
classification	O
,	O
(	O
ii	O
)	O
Homogeneous	O
Multitask	O
Learning	O
:	O
Simultaneous	O
optimization	O
of	O
a	O
pair	O
selected	O
from	O
the	O
four	O
tasks	O
associated	O
with	O
the	O
sexual	O
abuse	O
disclosure	O
posts	O
,	O
and	O
(	O
iii	O
)	O
Heterogeneous	O
Multitask	O
Learning	O
:	O
Classification	B-TaskName
of	O
narratives	O
associated	O
with	O
sexual	O
abuse	O
disclosure	O
as	O
the	O
primary	O
task	O
and	O
emotion	B-DatasetName
detection	O
as	O
the	O
auxiliary	O
task	O
.	O

Building	O
on	O
the	O
success	O
of	O
transformer	O
-	O
based	O
models	O
in	O
NLP	O
,	O
we	O
chose	O
BERTweet	O
(	O
Dat	O
Quoc	O
Nguyen	O
and	O
Nguyen	O
,	O
2020	O
)	O
,	O
a	O
pre	O
-	O
trained	O
language	O
model	O
trained	O
on	O
850	O
million	O
English	O
tweets	O
.	O
BERTweet	O
has	O
been	O
trained	O
with	O
the	O
same	O
training	O
procedure	O
as	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
and	O
has	O
the	O
same	O
model	O
configuration	O
as	O
the	O
BERT	B-MethodName
base	O
architecture	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
.	O
The	O
key	O
component	O
in	O
transformer	O
-	O
based	O
models	O
is	O
the	O
token	O
level	O
selfattention	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
that	O
enables	O
them	O
to	O
generate	O
dynamic	O
contextualized	O
embeddings	O
as	O
opposed	O
to	O
static	O
embeddings	O
of	O
GloVe	B-MethodName
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
.	O
Let	O
(	O
w	O
1	O
,	O
w	O
2	O
,	O
...	O
,	O
w	O
n	O
)	O
represent	O
the	O
sequence	O
of	O
tokens	O
from	O
a	O
given	O
tweet	O
t.	O
These	O
tokens	O
are	O
pre	O
-	O
processed	O
and	O
passed	O
through	O
BERTweet	O
3	O
.	O
We	O
consider	O
embeddings	O
from	O
the	O
last	O
layer	O
of	O
BERTweet	O
and	O
obtain	O
an	O
embedding	O
e	O
i	O
for	O
a	O
given	O
tweet	O
t	O
i	O
.	O
Embedding	O
for	O
each	O
tweet	O
is	O
of	O
dimension	O
m	O
×	O
k	O
,	O
where	O
k	O
represents	O
the	O
dimension	O
size	O
of	O
BERT	B-MethodName
based	O
model	O
and	O
m	O
represents	O
the	O
maximum	O
length	O
for	O
the	O
tweets	O
.	O
e	O
i	O
=	O
BERT	B-MethodName
weet	O
(	O
t	O
i	O
)	O
(	O
1	O
)	O
These	O
representations	O
from	O
Equation	O
1	O
are	O
passed	O
through	O
a	O
stacked	O
BiLSTM	B-MethodName
encoder	O
.	O
Dropout	B-MethodName
is	O
then	O
applied	O
to	O
these	O
encoded	O
representations	O
h	O
(	O
t	O
)	O
(	O
Equation	O
4represents	O
general	O
formulation	O
for	O
both	O
the	O
tasks	O
)	O
.	O
These	O
are	O
then	O
passed	O
to	O
a	O
BiLSTM	B-MethodName
decoder	O
,	O
followed	O
by	O
a	O
dropout	O
layer	O
and	O
then	O
a	O
linear	O
output	O
layer	O
to	O
get	O
output	O
o	O
(	O
p	O
)	O
(	O
p	O
representing	O
primary	O
task	O
)	O
or	O
o	O
(	O
a	O
)	O
(	O
a	O
representing	O
auxiliary	O
task	O
)	O
.	O
−	O
−	O
h	O
(	O
f	O
)	O
t	O
=	O
BiLST	O
M	O
(	O
f	O
)	O
(	O
e	O
t	O
,	O
h	O
(	O
f	O
)	O
t−1	O
)	O
(	O
2	O
)	O
−	O
−	O
h	O
(	O
b	O
)	O
t	O
=	O
BiLST	O
M	O
(	O
b	O
)	O
(	O
e	O
t	O
,	O
h	O
(	O
b	O
)	O
t+1	O
)	O
(	O
3	O
)	O
h	O
t	O
=	O
[	O
−	O
−	O
h	O
(	O
f	O
)	O
t	O
,	O
−−	O
h	O
(	O
b	O
)	O
T	O
−t	O
]	O
(	O
4	O
)	O

Single	O
Task	O
Learning	O
STL	O
experiments	O
optimize	O
each	O
of	O
the	O
tasks	O
associated	O
with	O
identifying	O
narratives	O
related	O
to	O
sexual	O
abuse	O
disclosures	O
within	O
#	O
MeToo	O
movement	O
(	O
Section	O
3	O
)	O
and	O
emotion	B-DatasetName
4	O
The	O
publicly	O
available	O
dataset	O
can	O
be	O
found	O
at	O
https	O
:	O
//doi.org/10.7910	O
/	O
DVN	O
/	O
JN4EYU	O
.	O
5	O
https://competitions.codalab.org/	O
competitions/17751	O
detection	O
,	O
independently	O
.	O
We	O
experiment	O
with	O
two	O
distinct	O
embedding	O
spaces	O
-	O
GloVe	B-MethodName
-	O
Twitter	O
and	O
BERTweet	O
.	O
Based	O
on	O
the	O
superior	O
performance	O
of	O
BERTweet	O
with	O
respect	O
to	O
GloVe	B-MethodName
-	O
Twitter	O
,	O
we	O
preferred	O
it	O
for	O
further	O
experimentation	O
and	O
studies	O
.	O
Homogeneous	O
Multitask	O
Learning	O
For	O
this	O
setup	O
,	O
we	O
test	O
the	O
simultaneous	O
optimization	O
of	O
two	O
different	O
tasks	O
-	O
both	O
related	O
to	O
sexual	O
harassment	O
disclosure	O
narratives	O
,	O
with	O
one	O
of	O
them	O
being	O
primary	O
and	O
another	O
coupled	O
as	O
the	O
auxiliary	O
.	O
The	O
results	O
were	O
obtained	O
for	O
a	O
total	O
of	O
12	O
pairs	O
.	O
Heterogeneous	O
Multitask	O
Learning	O
In	O
these	O
sets	O
of	O
experiments	O
,	O
we	O
evaluate	O
the	O
positive	O
transfer	O
of	O
representations	O
across	O
datasets	O
by	O
considering	O
the	O
identification	O
of	O
narratives	O
associated	O
with	O
sexual	O
abuse	O
disclosure	O
as	O
the	O
primary	O
task	O
and	O
emotion	B-DatasetName
detection	O
as	O
the	O
auxiliary	O
task	O
.	O

The	O
aim	O
of	O
this	O
paper	O
is	O
not	O
limited	O
to	O
achieving	O
the	O
state	O
of	O
the	O
art	O
performance	O
in	O
terms	O
of	O
evaluation	O
metrics	O
but	O
rather	O
to	O
conduct	O
a	O
thorough	O
study	O
to	O
compare	O
and	O
contrast	O
different	O
methodologies	O
for	O
the	O
benefit	O
of	O
the	O
research	O
community	O
.	O
As	O
per	O
our	O
hypothesis	O
and	O
preliminary	O
results	O
on	O
STL	O
experiments	O
on	O
the	O
#	B-DatasetName
MeTooMA	I-DatasetName
dataset	O
,	O
models	O
trained	O
using	O
BERTweet	O
embeddings	O
perform	O
far	O
better	O
than	O
GloVe	B-MethodName
-	O
Twitter	O
.	O
This	O
is	O
largely	O
true	O
because	O
BERTweet	O
is	O
specifically	O
pre	O
-	O
trained	O
on	O
English	O
tweets	O
and	O
is	O
better	O
suited	O
to	O
handle	O
Twitter	O
-	O
specific	O
data	O
,	O
typically	O
having	O
a	O
short	O
length	O
,	O
informal	O
grammar	O
,	O
and	O
irregular	O
vocabulary	O
(	O
e.g.	O
,	O
abbreviations	O
and	O
typographical	O
errors	O
)	O
(	O
Kireyev	O
et	O
al	O
,	O
2009	O
)	O
.	O

Results	O
in	O
highly	O
correlated	O
to	O
emotion	B-TaskName
recognition	I-TaskName
.	O
This	O
is	O
indicative	O
of	O
positive	O
knowledge	O
transfer	O
between	O
the	O
two	O
domains	O
.	O
Such	O
joint	O
optimization	O
boosts	O
the	O
overall	O
performance	O
of	O
both	O
primary	O
and	O
auxiliary	O
tasks	O
through	O
parameter	O
sharing	O
to	O
learn	O
common	O
representations	O
that	O
may	O
be	O
mutually	O
beneficial	O
to	O
both	O
related	O
tasks	O
.	O

To	O
emphasize	O
our	O
proposed	O
approach	O
,	O
we	O
perform	O
a	O
qualitative	O
study	O
by	O
handpicking	O
examples	O
from	O
the	O
dataset	O
.	O
We	O
analyze	O
token	O
-	O
level	O
attention	O
assigned	O
to	O
individual	O
terms	O
by	O
BERTweet	O
,	O
where	O
color	O
intensity	O
corresponds	O
to	O
the	O
attention	O
score	O
.	O
These	O
results	O
are	O
shown	O
in	O
Table	O
4	O
.	O
We	O
infer	O
that	O
Homogeneous	O
and	O
Heterogeneous	O
multitask	O
learning	O
shows	O
superior	O
performance	O
in	O
every	O
instance	O
compared	O
to	O
STL	O
.	O
Learning	O
effective	O
features	O
across	O
the	O
joint	O
formulation	O
of	O
pair	O
-	O
wise	O
tasks	O
in	O
Homogeneous	O
MTL	O
is	O
evident	O
from	O
T	O
4	O
,	O
where	O
BERT	B-MethodName
's	O
self	O
-	O
attention	O
allots	O
a	O
higher	O
weight	O
to	O
words	O
such	O
as	O
ideology	O
,	O
stigma	O
,	O
and	O
forward	O
in	O
line	O
with	O
the	O
actual	O
label	O
as	O
Support	O
.	O
Similarly	O
for	O
T	O
5	O
,	O
highlighted	O
terms	O
such	O
as	O
trap	O
and	O
bait	O
are	O
indicative	O
of	O
the	O
opposing	O
nature	O
of	O
the	O
tweets	O
,	O
hence	O
identified	O
as	O
belonging	O
to	O
Refutation	O
.	O
On	O
the	O
other	O
hand	O
,	O
due	O
to	O
positive	O
knowledge	O
transfer	O
from	O
the	O
emotion	B-TaskName
recognition	I-TaskName
task	O
,	O
Heterogeneous	O
MTL	O
obtains	O
better	O
performance	O
in	O
several	O
cases	O
.	O
Words	O
such	O
as	O
grave	O
,	O
mistake	O
and	O
swindling	O
in	O
T	O
2	O
connoted	O
a	O
negative	O
emotion	B-DatasetName
,	O
hence	O
accordingly	O
being	O
identified	O
as	O
belonging	O
to	O
the	O
Oppose	O
category	O
.	O
Similarly	O
,	O
terms	O
such	O
as	O
hope	O
and	O
pain	O
were	O
given	O
higher	O
token	O
-	O
level	O
attention	O
in	O
T	O
1	O
emphasizing	O
a	O
positive	O
emotion	B-DatasetName
and	O
thus	O
can	O
be	O
correlated	O
with	O
belonging	O
to	O
the	O
Support	O
category	O
.	O
An	O
interesting	O
observation	O
is	O
the	O
presence	O
of	O
named	O
entities	O
in	O
T	O
5	O
and	O
T	O
6	O
,	O
resulting	O
in	O
the	O
incorrect	O
prediction	O
via	O
Heterogeneous	O
MTL	O
.	O
Therefore	O
,	O
a	O
limitation	O
of	O
the	O
single	O
task	O
learning	O
and	O
Heterogeneous	O
MTL	O
is	O
the	O
inability	O
to	O
mitigate	O
the	O
effect	O
of	O
named	O
entities	O
or	O
specific	O
events	O
in	O
the	O
text	O
to	O
influence	O
the	O
knowledge	O
transfer	O
and	O
create	O
negative	O
shared	O
representations	O
.	O

In	O
this	O
work	O
,	O
we	O
have	O
proposed	O
a	O
flexible	O
crossstitched	O
multitask	O
learning	O
framework	O
for	O
the	O
de	O
-	O
tection	O
of	O
narratives	O
linked	O
with	O
sexual	O
abuse	O
disclosure	O
on	O
social	O
media	O
.	O
Our	O
methodology	O
takes	O
advantage	O
of	O
the	O
affective	O
features	O
from	O
emotions	O
and	O
related	O
tasks	O
to	O
encourage	O
knowledge	O
transfer	O
and	O
attain	O
auxiliary	O
knowledge	O
.	O
Qualitative	O
and	O
quantitative	O
results	O
demonstrate	O
how	O
joint	O
optimization	O
of	O
Stance	B-TaskName
detection	I-TaskName
and	O
Sarcasm	O
identification	O
benefit	O
each	O
other	O
,	O
indicating	O
their	O
relatedness	O
and	O
dependence	O
on	O
each	O
other	O
.	O
Similarly	O
,	O
we	O
observe	O
that	O
tasks	O
like	O
Hate	O
-	O
Speech	O
classification	O
and	O
Stance	O
labeling	O
benefit	O
from	O
each	O
other	O
and	O
from	O
emotion	B-DatasetName
detection	O
,	O
thus	O
reinforcing	O
the	O
benefit	O
of	O
joint	O
linguistic	O
learning	O
between	O
the	O
related	O
tasks	O
.	O
In	O
the	O
future	O
,	O
we	O
aim	O
to	O
explore	O
how	O
this	O
joint	O
learning	O
paradigm	O
can	O
be	O
effectively	O
leveraged	O
for	O
improving	O
performance	O
on	O
downstream	O
tasks	O
like	O
emotion	B-DatasetName
analysis	O
,	O
identifying	O
suicidal	O
tendencies	O
among	O
abuse	O
survivors	O
.	O
Application	O
from	O
this	O
work	O
also	O
has	O
utility	O
for	O
problems	O
such	O
as	O
identification	O
of	O
patterns	O
of	O
reported	O
sexual	O
harassment	O
narratives	O
,	O
hate	B-TaskName
speech	I-TaskName
detection	I-TaskName
,	O
the	O
spread	O
of	O
rumors	O
and	O
fake	O
news	O
,	O
and	O
entity	O
extraction	O
for	O
digital	O
vigilantism	O
(	O
Yuce	O
et	O
al	O
,	O
2014	O
;	O
Hosterman	O
et	O
al	O
,	O
2018	O
)	O
.	O

The	O
Power	O
of	O
Prompt	O
Tuning	O
for	O
Low	O
-	O
Resource	O
Semantic	B-TaskName
Parsing	I-TaskName

Prompt	O
tuning	O
has	O
recently	O
emerged	O
as	O
an	O
effective	O
method	O
for	O
adapting	O
pre	O
-	O
trained	O
language	O
models	O
to	O
a	O
number	O
of	O
language	O
understanding	O
and	O
generation	O
tasks	O
.	O
In	O
this	O
paper	O
,	O
we	O
investigate	O
prompt	O
tuning	O
for	O
semantic	B-TaskName
parsing	I-TaskName
-	O
the	O
task	O
of	O
mapping	O
natural	O
language	O
utterances	O
onto	O
formal	O
meaning	O
representations	O
.	O
On	O
the	O
low	O
-	O
resource	O
splits	O
of	O
Overnight	O
and	O
TOPv2	B-DatasetName
,	O
we	O
find	O
that	O
a	O
prompt	O
tuned	O
T5	B-MethodName
-	O
xl	O
significantly	O
outperforms	O
its	O
fine	O
-	O
tuned	O
counterpart	O
,	O
as	O
well	O
as	O
strong	O
GPT	B-MethodName
-	O
3	O
and	O
BART	B-MethodName
baselines	O
.	O
We	O
also	O
conduct	O
ablation	O
studies	O
across	O
different	O
model	O
scales	O
and	O
target	O
representations	O
,	O
finding	O
that	O
,	O
with	O
increasing	O
model	O
scale	O
,	O
prompt	O
tuned	O
T5	B-MethodName
models	O
improve	O
at	O
generating	O
target	O
representations	O
that	O
are	O
far	O
from	O
the	O
pre	O
-	O
training	O
distribution	O
.	O

With	O
the	O
widespread	O
success	O
of	O
pre	O
-	O
trained	O
language	O
models	O
(	O
LMs	O
;	O
Devlin	O
et	O
al	O
2019	O
;	O
Raffel	O
et	O
al	O
2020	O
;	O
Bommasani	O
et	O
al	O
2021	O
)	O
,	O
it	O
becomes	O
increasingly	O
important	O
to	O
explore	O
how	O
such	O
models	O
can	O
be	O
adapted	O
to	O
downstream	O
tasks	O
.	O
One	O
adaptation	O
method	O
which	O
has	O
recently	O
attracted	O
much	O
attention	O
is	O
prompt	O
design	O
(	O
Brown	O
et	O
al	O
,	O
2020	O
;	O
Shin	O
et	O
al	O
,	O
2020	O
)	O
,	O
which	O
modulates	O
the	O
behaviour	O
of	O
a	O
LM	O
through	O
a	O
task	O
description	O
and	O
a	O
few	O
inputoutput	O
examples	O
.	O
Brown	O
et	O
al	O
(	O
2020	O
)	O
show	O
that	O
this	O
adaptation	O
strategy	O
is	O
increasingly	O
effective	O
for	O
larger	O
LMs	O
.	O
However	O
,	O
prompt	O
design	O
is	O
sensitive	O
to	O
the	O
exact	O
phrasing	O
of	O
the	O
prompt	O
,	O
and	O
,	O
more	O
importantly	O
,	O
performs	O
worse	O
than	O
fine	O
-	O
tuning	O
models	O
on	O
task	O
-	O
specific	O
examples	O
(	O
Lester	O
et	O
al	O
,	O
2021	O
)	O
.	O
Prompt	O
tuning	O
has	O
recently	O
arisen	O
as	O
a	O
strong	O
performing	O
alternative	O
adaption	O
method	O
(	O
Lester	O
et	O
al	O
,	O
2021	O
)	O
.	O
Rather	O
than	O
hand	O
-	O
designing	O
discrete	O
prompts	O
,	O
prompt	O
tuning	O
optimizes	O
the	O
embeddings	O
of	O
a	O
number	O
of	O
task	O
-	O
specific	O
prompt	O
tokens	O
.	O
In	O
contrast	O
to	O
fine	O
-	O
tuning	O
,	O
this	O
method	O
keeps	O
almost	O
all	O
LM	O
parameters	O
frozen	O
.	O
On	O
a	O
set	O
of	O
language	O
understanding	O
tasks	O
,	O
Lester	O
et	O
al	O
(	O
2021	O
)	O
show	O
that	O
prompt	O
tuning	O
becomes	O
competitive	O
with	O
finetuning	O
for	O
the	O
largest	O
pre	O
-	O
trained	O
T5	B-MethodName
models	O
(	O
Raffel	O
et	O
al	O
,	O
2020	O
)	O
.	O
Li	O
and	O
Liang	O
(	O
2021	O
)	O
also	O
explore	O
a	O
related	O
parameter	O
-	O
efficient	O
adaptation	O
method	O
called	O
prefix	O
-	O
tuning	O
,	O
finding	O
that	O
it	O
outperforms	O
fine	O
-	O
tuning	O
on	O
low	O
-	O
resource	O
natural	O
language	O
generation	O
tasks	O
.	O
In	O
this	O
paper	O
,	O
we	O
investigate	O
prompt	O
tuning	O
for	O
semantic	B-TaskName
parsing	I-TaskName
.	O
This	O
task	O
is	O
fundamentally	O
different	O
from	O
the	O
aforementioned	O
language	O
understanding	O
and	O
generation	O
tasks	O
,	O
as	O
it	O
requires	O
that	O
models	O
output	O
formal	O
meaning	O
representations	O
which	O
do	O
not	O
resemble	O
the	O
natural	O
language	O
distribution	O
seen	O
during	O
pre	O
-	O
training	O
.	O
In	O
particular	O
,	O
we	O
focus	O
on	O
the	O
low	O
-	O
resource	O
setup	O
because	O
examples	O
for	O
semantic	B-TaskName
parsing	I-TaskName
are	O
difficult	O
and	O
expensive	O
to	O
collect	O
(	O
Wang	O
et	O
al	O
,	O
2015	O
;	O
Marzoev	O
et	O
al	O
,	O
2020	O
)	O
.	O
We	O
therefore	O
evaluate	O
prompt	O
tuning	O
on	O
two	O
datasets	O
:	O
the	O
200	O
-	O
shot	O
version	O
of	O
Overnight	O
(	O
Wang	O
et	O
al	O
,	O
2015	O
;	O
Shin	O
et	O
al	O
,	O
2021	O
)	O
and	O
the	O
low	O
-	O
resource	O
splits	O
TOPv2	B-DatasetName
(	O
Chen	O
et	O
al	O
,	O
2020	O
)	O
.	O
On	O
both	O
datasets	O
,	O
we	O
compare	O
prompt	O
tuning	O
T5	B-MethodName
against	O
fine	O
-	O
tuning	O
and	O
investigate	O
the	O
effect	O
of	O
canonicalizing	O
the	O
meaning	O
representation	O
,	O
i.e.	O
to	O
what	O
extent	O
naturalizing	O
the	O
logical	O
forms	O
influences	O
performance	O
.	O
In	O
addition	O
,	O
we	O
study	O
the	O
effect	O
of	O
T5	B-MethodName
model	O
scale	O
on	O
Overnight	O
as	O
well	O
as	O
varying	O
data	O
regimes	O
on	O
TOPv2	B-DatasetName
.	O
Our	O
main	O
findings	O
can	O
be	O
summarized	O
as	O
follows	O
:	O
For	O
large	O
T5	B-MethodName
models	O
,	O
prompt	O
tuning	O
significantly	O
outperforms	O
fine	O
-	O
tuning	O
in	O
the	O
low	O
-	O
data	O
regime	O
,	O
resulting	O
in	O
an	O
absolute	O
improvement	O
of	O
6	O
%	O
and	O
15	O
%	O
on	O
Overnight	O
and	O
TOPv2	B-DatasetName
,	O
respectively	O
.	O
This	O
performance	O
gap	O
decreases	O
when	O
more	O
training	O
data	O
becomes	O
available	O
.	O
With	O
growing	O
model	O
size	O
,	O
prompt	O
tuned	O
T5	B-MethodName
models	O
are	O
increasingly	O
capable	O
of	O
outputting	O
diverse	O
target	O
representations	O
(	O
see	O
Figure	O
1	O
)	O
.	O
On	O
Overnight	O
,	O
we	O
find	O
that	O
the	O
disparity	O
between	O
canonical	O
and	O
meaning	O
representations	O
shrinks	O
from	O
17	O
%	O
to	O
4	O
%	O
for	O
T5	B-MethodName
-	O
small	O
and	O
T5	B-MethodName
-	O
xl	O
,	O
respectively	O
.	O
On	O
TOPv2	B-DatasetName
,	O
prompt	O
tuned	O
T5	B-MethodName
-	O
large	O
models	O
are	O
much	O
better	O
at	O
generating	O
out	O
-	O
of	O
-	O
vocabulary	O
tokens	O
than	O
T5	B-MethodName
-	O
small	O
.	O

Our	O
work	O
is	O
related	O
to	O
recent	O
work	O
on	O
semantic	B-TaskName
parsing	I-TaskName
and	O
prompt	O
tuning	O
,	O
which	O
we	O
briefly	O
describe	O
below	O
.	O

Semantic	B-TaskName
parsing	I-TaskName
is	O
the	O
task	O
of	O
converting	O
a	O
natural	O
language	O
utterance	O
u	O
=	O
(	O
u	O
1	O
,	O
.	O
.	O
.	O
,	O
u	O
N	O
)	O
to	O
a	O
formal	O
meaning	O
representation	O
z	O
=	O
(	O
z	O
1	O
,	O
.	O
.	O
.	O
,	O
z	O
M	O
)	O
.	O
These	O
meaning	O
representations	O
,	O
also	O
referred	O
to	O
as	O
logical	O
forms	O
,	O
can	O
be	O
interpreted	O
by	O
machines	O
and	O
executed	O
in	O
a	O
real	O
environment	O
.	O
For	O
example	O
,	O
ThingTalk	O
(	O
Campagna	O
et	O
al	O
,	O
2019	O
)	O
and	O
TOP	O
(	O
Gupta	O
et	O
al	O
,	O
2018	O
)	O
are	O
meaning	O
representations	O
for	O
executing	O
commands	O
of	O
virtual	O
assistants	O
,	O
while	O
SQL	O
is	O
a	O
representation	O
for	O
interacting	O
with	O
relational	O
databases	O
.	O
In	O
recent	O
years	O
,	O
neural	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
have	O
become	O
the	O
dominant	O
approach	O
for	O
semantic	B-TaskName
parsing	I-TaskName
tasks	O
(	O
Dong	O
and	O
Lapata	O
,	O
2016	O
)	O
.	O
Canonicalization	O
A	O
common	O
simplification	O
step	O
in	O
semantic	B-TaskName
parsing	I-TaskName
is	O
to	O
canonicalize	O
the	O
meaning	O
representations	O
.	O
That	O
is	O
,	O
the	O
meaning	O
representation	O
z	O
is	O
naturalized	O
to	O
a	O
canonical	O
form	O
c	O
through	O
a	O
grammar	O
or	O
set	O
of	O
rules	O
.	O
Examples	O
of	O
the	O
meaning	O
and	O
canonical	O
representation	O
for	O
Overnight	O
and	O
TOPv2	B-DatasetName
(	O
Wang	O
et	O
al	O
,	O
2015	O
;	O
Chen	O
et	O
al	O
,	O
2020	O
)	O
can	O
be	O
found	O
in	O
Fig	O
.	O
2	O
.	O
When	O
canonical	O
representations	O
are	O
available	O
,	O
Berant	O
and	O
Liang	O
(	O
2014	O
)	O
argue	O
that	O
semantic	B-TaskName
parsing	I-TaskName
can	O
be	O
seen	O
as	O
a	O
paraphrase	O
task	O
.	O
They	O
propose	O
to	O
use	O
a	O
paraphrase	O
model	O
-	O
using	O
e.g.	O
word	O
vectors	O
trained	O
on	O
Wikipedia	O
-	O
to	O
find	O
the	O
best	O
paraphrase	O
of	O
utterance	O
u	O
among	O
a	O
set	O
of	O
canonical	O
utterances	O
.	O
They	O
show	O
this	O
paraphrase	O
model	O
improves	O
results	O
over	O
directly	O
generating	O
logical	O
forms	O
on	O
two	O
question	O
-	O
answering	O
datasets	O
.	O
Marzoev	O
et	O
al	O
(	O
2020	O
)	O
extends	O
this	O
work	O
by	O
showing	O
that	O
pre	O
-	O
trained	O
language	O
models	O
like	O
BERT	B-MethodName
can	O
be	O
effective	O
paraphrasers	O
.	O
While	O
Berant	O
and	O
Liang	O
(	O
2014	O
)	O
;	O
Marzoev	O
et	O
al	O
(	O
2020	O
)	O
use	O
models	O
to	O
score	O
canonical	O
utterances	O
,	O
Shin	O
et	O
al	O
(	O
2021	O
)	O
propose	O
to	O
constrain	O
the	O
generation	O
process	O
of	O
autoregressive	O
models	O
like	O
BART	B-MethodName
and	O
GPT	B-MethodName
-	O
3	O
.	O
On	O
a	O
number	O
of	O
few	O
-	O
shot	O
semantic	B-TaskName
parsing	I-TaskName
tasks	O
,	O
they	O
demonstrate	O
the	O
benefit	O
of	O
generating	O
canonical	O
representations	O
over	O
meaning	O
representations	O
.	O

Lester	O
et	O
al	O
(	O
2021	O
)	O
evaluates	O
prompt	O
tuning	O
on	O
SuperGLUE	B-DatasetName
,	O
a	O
benchmark	O
consisting	O
of	O
eight	O
language	O
understanding	O
tasks	O
.	O
They	O
find	O
that	O
prompt	O
tuning	O
becomes	O
competitive	O
with	O
fine	O
-	O
tuning	O
for	O
the	O
largest	O
T5	B-MethodName
model	O
.	O
Li	O
and	O
Liang	O
(	O
2021	O
)	O
propose	O
prefix	O
-	O
tuning	O
to	O
adapt	O
BART	B-MethodName
and	O
GPT	B-MethodName
-	O
2	O
for	O
natural	O
language	O
generation	O
tasks	O
.	O
This	O
method	O
differs	O
from	O
Lester	O
et	O
al	O
(	O
2021	O
)	O
in	O
that	O
it	O
prepends	O
trainable	O
embeddings	O
for	O
each	O
layer	O
of	O
the	O
language	O
model	O
rather	O
than	O
introducing	O
token	O
embeddings	O
at	O
the	O
input	O
layer	O
.	O
They	O
demonstrate	O
that	O
pre	O
-	O
fix	O
outperforms	O
fine	O
-	O
tuning	O
baselines	O
.	O
Similarly	O
,	O
Liu	O
et	O
al	O
(	O
2021	O
)	O
also	O
show	O
encouraging	O
results	O
for	O
prompt	O
tuning	O
on	O
natural	O
language	O
understand	O
and	O
generation	O
tasks	O
.	O
Qin	O
and	O
Eisner	O
(	O
2021	O
)	O
also	O
explores	O
prompt	O
tuning	O
but	O
for	O
a	O
knowledge	O
extraction	O
task	O
.	O
Inserting	O
general	O
adapter	O
layers	O
into	O
pre	O
-	O
trained	O
language	O
models	O
is	O
also	O
proposed	O
in	O
Houlsby	O
et	O
al	O
(	O
2019	O
)	O
;	O
Mahabadi	O
et	O
al	O
(	O
2021	O
)	O
.	O
Related	O
to	O
our	O
work	O
are	O
also	O
other	O
few	O
-	O
shot	O
adaptation	O
techniques	O
like	O
PET	B-DatasetName
(	O
Schick	O
and	O
Schütze	O
,	O
2021	O
)	O
.	O
Moreover	O
,	O
adapter	O
layers	O
have	O
also	O
been	O
explored	O
in	O
the	O
computer	O
vision	O
domain	O
(	O
Rebuffi	O
et	O
al	O
,	O
2017	O
;	O
de	O
Vries	O
et	O
al	O
,	O
2017	O
)	O
.	O

To	O
evaluate	O
low	O
-	O
resource	O
prompt	O
tuning	O
,	O
we	O
compare	O
against	O
fine	O
-	O
tuned	O
variants	O
of	O
the	O
same	O
model	O
on	O
two	O
semantic	B-TaskName
parsing	I-TaskName
datasets	O
with	O
canonical	O
representations	O
available	O
.	O
We	O
compare	O
both	O
large	O
and	O
small	O
variants	O
of	O
the	O
T5	B-MethodName
architecture	O
on	O
these	O
datasets	O
and	O
experiment	O
with	O
various	O
canonicalized	O
representations	O
.	O

Chen	O
et	O
al	O
apply	O
a	O
set	O
of	O
simple	O
modifications	O
to	O
the	O
TOPv2	B-DatasetName
meaning	O
representations	O
to	O
arrive	O
at	O
a	O
canonical	O
form	O
used	O
in	O
all	O
their	O
experiments	O
.	O
Unlike	O
Overnight	O
,	O
these	O
pre	O
-	O
processing	O
steps	O
are	O
largely	O
small	O
encoding	O
differences	O
and	O
do	O
not	O
change	O
the	O
syntactic	O
structure	O
of	O
the	O
logical	O
forms	O
.	O
We	O
adopt	O
all	O
of	O
these	O
canonicalization	O
steps	O
(	O
except	O
for	O
lexicographic	O
sorting	O
of	O
the	O
semantic	O
parse	O
tree	O
)	O
and	O
add	O
an	O
ontology	B-MethodName
label	O
shortening	O
step	O
.	O
Examples	O
of	O
these	O
transformations	O
can	O
be	O
seen	O
in	O
Fig	O
.	O
2	O
and	O
are	O
briefly	O
described	O
below	O
.	O
Simplify	O
removes	O
redundant	O
utterance	O
tokens	O
unnecessary	O
for	O
interpreting	O
the	O
meaning	O
representation	O
.	O
Out	O
-	O
of	O
-	O
Vocab	O
adds	O
the	O
entire	O
intent	O
or	O
slot	O
label	O
to	O
the	O
tokenizer	O
as	O
a	O
new	O
single	O
tokens	O
with	O
a	O
corresponding	O
randomly	O
initialized	O
embedding	O
.	O
In	O
-	O
Vocab	O
replaces	O
the	O
intent	O
and	O
slot	O
labels	O
with	O
a	O
short	O
unique	O
identifier	O
representable	O
by	O
the	O
pre	O
-	O
trained	O
tokenizer	O
.	O
We	O
perform	O
an	O
ablation	O
over	O
these	O
canonicalization	O
choices	O
,	O
repeating	O
each	O
experiment	O
three	O
times	O
with	O
varying	O
random	O
seed	O
.	O
For	O
each	O
domain	O
,	O
we	O
report	O
the	O
average	O
over	O
5	O
runs	O
trained	O
on	O
randomly	O
sampled	O
splits	O
of	O
200	O
examples	O
for	O
fine	O
-	O
tuned	O
(	O
FT	O
)	O
and	O
prompt	O
tuned	O
(	O
PT	O
)	O
models	O
.	O

In	O
(	O
2020	O
)	O
.	O
In	O
Table	O
4	O
,	O
we	O
summarize	O
the	O
results	O
of	O
the	O
canonicalization	O
ablation	O
study	O
for	O
TOPv2	B-DatasetName
.	O

Our	O
main	O
finding	O
is	O
that	O
prompt	O
tuned	O
T5	B-MethodName
models	O
become	O
better	O
at	O
generating	O
meaning	O
representations	O
with	O
increased	O
model	O
size	O
.	O
On	O
Overnight	O
,	O
we	O
see	O
the	O
absolute	O
difference	O
between	O
canonical	O
and	O
meaning	O
representations	O
shrink	O
from	O
17.5	O
points	O
for	O
T5	B-MethodName
-	O
small	O
to	O
3.4	O
points	O
for	O
T5	B-MethodName
-	O
xl	O
(	O
Table	O
1	O
)	O
.	O
This	O
gap	O
shrinks	O
another	O
18	O
%	O
to	O
2.8	O
points	O
when	O
we	O
apply	O
constrained	O
decoding	O
to	O
T5	B-MethodName
-	O
xl	O
(	O
Table	O
2	O
)	O
.	O
By	O
contrast	O
,	O
Shin	O
et	O
al	O
(	O
2021	O
)	O
reports	O
an	O
11.7	O
point	O
difference	O
when	O
prompting	O
GPT	B-MethodName
-	O
3	O
.	O
For	O
our	O
finetuning	O
baselines	O
,	O
we	O
observe	O
a	O
small	O
performance	O
gap	O
of	O
4	O
points	O
across	O
target	O
representations	O
for	O
BART	B-MethodName
and	O
T5	B-MethodName
-	O
xl	O
,	O
while	O
we	O
observe	O
no	O
gap	O
for	O
T5	B-MethodName
-	O
small	O
,	O
T5	B-MethodName
-	O
base	O
,	O
and	O
T5	B-MethodName
-	O
large	O
models	O
.	O
In	O
our	O
TOPv2	B-DatasetName
experiments	O
we	O
find	O
similar	O
evidence	O
of	O
large	O
T5	B-MethodName
model	O
flexibility	O
for	O
generating	O
sequences	O
far	O
from	O
the	O
training	O
distribution	O
.	O
In	O
particular	O
,	O
for	O
our	O
most	O
intrusive	O
canonicalization	O
scheme	O
Out	O
-	O
of	O
-	O
Vocab	O
,	O
which	O
adds	O
novel	O
tokens	O
to	O
the	O
vocabulary	O
and	O
leaves	O
these	O
embeddings	O
un	O
-	O
trained	O
,	O
we	O
find	O
no	O
significant	O
reduction	O
in	O
performance	O
for	O
T5	B-MethodName
-	O
large	O
across	O
all	O
data	O
resource	O
levels	O
.	O
T5	B-MethodName
-	O
small	O
,	O
in	O
comparison	O
,	O
sees	O
almost	O
a	O
50	O
%	O
drop	O
in	O
performance	O
relative	O
to	O
no	O
canonicalization	O
(	O
None	O
)	O
at	O
the	O
10	O
SPIS	O
level	O
and	O
continues	O
to	O
underperform	O
by	O
33	O
%	O
at	O
the	O
500	O
SPIS	O
level	O
.	O
Interestingly	O
,	O
we	O
find	O
that	O
In	O
-	O
Vocab	O
drastically	O
reduces	O
performance	O
for	O
T5	B-MethodName
-	O
small	O
at	O
the	O
10	O
SPIS	O
level	O
-	O
30.9	O
%	O
vs.	O
43.4	O
%	O
for	O
None	O
-	O
but	O
slightly	O
outperforms	O
it	O
at	O
500	O
SPIS	O
.	O
We	O
speculate	O
that	O
In	O
-	O
Vocab	O
effectively	O
anonymizes	O
the	O
ontol	O
-	O
ogy	O
tokens	O
,	O
obscuring	O
information	O
that	O
is	O
useful	O
for	O
prediction	O
.	O
In	O
low	O
-	O
data	O
regimes	O
there	O
is	O
not	O
enough	O
training	O
data	O
to	O
learn	O
the	O
semantics	O
of	O
these	O
anonymized	O
tokens	O
,	O
whereas	O
with	O
enough	O
data	O
this	O
problem	O
vanishes	O
.	O

We	O
find	O
that	O
prompt	O
tuning	O
is	O
an	O
effective	O
method	O
for	O
adapting	O
language	O
models	O
to	O
the	O
semantic	B-TaskName
parsing	I-TaskName
task	O
.	O
Prompt	O
tuning	O
significantly	O
outperforms	O
fine	O
-	O
tuning	O
in	O
low	O
-	O
data	O
regimes	O
,	O
and	O
remains	O
competitive	O
in	O
the	O
fully	O
supervised	O
setting	O
.	O
We	O
furthermore	O
find	O
that	O
while	O
canonicalizing	O
meaning	O
representations	O
can	O
slightly	O
improve	O
performance	O
,	O
the	O
disparity	O
between	O
target	O
representations	O
decreases	O
when	O
prompt	O
tuning	O
larger	O
T5	B-MethodName
models	O
.	O
This	O
result	O
differs	O
from	O
previous	O
work	O
(	O
Shin	O
et	O
al	O
,	O
2021	O
)	O
which	O
suggested	O
that	O
pre	O
-	O
trained	O
LMs	O
are	O
much	O
better	O
equipped	O
to	O
output	O
canonical	O
than	O
meaning	O
representations	O
.	O
However	O
,	O
a	O
significant	O
limitation	O
of	O
prompt	O
tuning	O
is	O
that	O
it	O
takes	O
more	O
time	O
to	O
converge	O
than	O
fine	O
-	O
tuning	O
.	O
We	O
believe	O
one	O
fruitful	O
direction	O
for	O
future	O
research	O
is	O
to	O
find	O
ways	O
to	O
reduce	O
the	O
compute	O
required	O
to	O
prompt	O
tune	O
.	O

Prompt	O
tuned	O
parameter	O
efficiency	O
comes	O
at	O
a	O
cost	O
:	O
we	O
find	O
that	O
prompt	O
tuning	O
takes	O
significantly	O
longer	O
to	O
train	O
with	O
early	B-MethodName
stopping	I-MethodName
than	O
does	O
finetuning	O
.	O
On	O
the	O
Overnight	O
dataset	O
,	O
fine	O
-	O
tuned	O
models	O
typically	O
took	O
250	O
epochs	O
before	O
validation	O
performance	O
plateaued	O
.	O
Our	O
prompt	O
tuned	O
models	O
frequently	O
took	O
more	O
than	O
1000	O
epochs	O
when	O
predicting	O
canonical	O
representations	O
,	O
and	O
up	O
to	O
5	O
,	O
000	O
when	O
predicting	O
meaning	O
representations	O
.	O
In	O
Figure	O
3	O
,	O
we	O
show	O
example	O
training	O
curves	O
for	O
prompt	O
tuning	O
and	O
fine	O
-	O
tuning	O
.	O

5	O
:	O
Results	O
across	O
all	O
model	O
size	O
,	O
target	O
representation	O
,	O
tuning	O
method	O
,	O
and	O
decoding	O
method	O
for	O
Overnight	O
dataset	O
.	O
BART	B-MethodName
,	O
GPT	B-MethodName
-	O
2	O
,	O
and	O
GPT	B-MethodName
-	O
3	O
results	O
results	O
are	O
included	O
from	O
Shin	O
et	O
al	O
(	O
2021	O
)	O

Many	O
NLP	O
learning	O
tasks	O
can	O
be	O
decomposed	O
into	O
several	O
distinct	O
sub	O
-	O
tasks	O
,	O
each	O
associated	O
with	O
a	O
partial	O
label	O
.	O
In	O
this	O
paper	O
we	O
focus	O
on	O
a	O
popular	O
class	O
of	O
learning	O
problems	O
,	O
sequence	O
prediction	O
applied	O
to	O
several	O
sentiment	B-TaskName
analysis	I-TaskName
tasks	O
,	O
and	O
suggest	O
a	O
modular	O
learning	O
approach	O
in	O
which	O
different	O
sub	O
-	O
tasks	O
are	O
learned	O
using	O
separate	O
functional	O
modules	O
,	O
combined	O
to	O
perform	O
the	O
final	O
task	O
while	O
sharing	O
information	O
.	O
Our	O
experiments	O
show	O
this	O
approach	O
helps	O
constrain	O
the	O
learning	O
process	O
and	O
can	O
alleviate	O
some	O
of	O
the	O
supervision	O
efforts	O
.	O

Many	O
natural	O
language	O
processing	O
tasks	O
attempt	O
to	O
replicate	O
complex	O
human	O
-	O
level	O
judgments	O
,	O
which	O
often	O
rely	O
on	O
a	O
composition	O
of	O
several	O
sub	O
-	O
tasks	O
into	O
a	O
unified	O
judgment	O
.	O
For	O
example	O
,	O
consider	O
the	O
Targeted	O
-	O
Sentiment	O
task	O
(	O
Mitchell	O
et	O
al	O
,	O
2013	O
)	O
,	O
assigning	O
a	O
sentiment	O
polarity	O
score	O
to	O
entities	O
depending	O
on	O
the	O
context	O
that	O
they	O
appear	O
in	O
.	O
Given	O
the	O
sentence	O
"	O
according	O
to	O
a	O
CNN	O
poll	O
,	O
Green	O
Book	O
will	O
win	O
the	O
best	O
movie	O
award	O
"	O
,	O
the	O
system	O
has	O
to	O
identify	O
both	O
entities	O
,	O
and	O
associate	O
the	O
relevant	O
sentiment	O
value	O
with	O
each	O
one	O
(	O
neutral	O
with	O
CNN	O
,	O
and	O
positive	O
with	O
Green	O
Book	O
)	O
.	O
This	O
task	O
can	O
be	O
viewed	O
as	O
a	O
combination	O
of	O
two	O
tasks	O
,	O
entity	O
identification	O
,	O
locating	O
contiguous	O
spans	O
of	O
words	O
corresponding	O
to	O
relevant	O
entities	O
,	O
and	O
sentiment	O
prediction	O
,	O
specific	O
to	O
each	O
entity	O
based	O
on	O
the	O
context	O
it	O
appears	O
in	O
.	O
Despite	O
the	O
fact	O
that	O
this	O
form	O
of	O
functional	O
task	O
decomposition	O
is	O
natural	O
for	O
many	O
learning	O
tasks	O
,	O
it	O
is	O
typically	O
ignored	O
and	O
learning	O
is	O
defined	O
as	O
a	O
monolithic	O
process	O
,	O
combining	O
the	O
tasks	O
into	O
a	O
single	O
learning	O
problem	O
.	O
Our	O
goal	O
in	O
this	O
paper	O
is	O
to	O
take	O
a	O
step	O
towards	O
modular	O
learning	O
architectures	O
that	O
exploit	O
the	O
learning	O
tasks	O
'	O
inner	O
structure	O
,	O
and	O
as	O
a	O
result	O
simplify	O
the	O
learning	O
process	O
and	O
reduce	O
the	O
annotation	O
effort	O
.	O
We	O
introduce	O
a	O
novel	O
task	O
decomposition	O
approach	O
,	O
learning	O
with	O
partial	O
labels	O
,	O
in	O
which	O
the	O
task	O
output	O
labels	O
decompose	O
hierarchically	O
,	O
into	O
partial	O
labels	O
capturing	O
different	O
aspects	O
,	O
or	O
sub	O
-	O
tasks	O
,	O
of	O
the	O
final	O
task	O
.	O
We	O
show	O
that	O
learning	O
with	O
partial	O
labels	O
can	O
help	O
support	O
weakly	O
-	O
supervised	O
learning	O
when	O
only	O
some	O
of	O
the	O
partial	O
labels	O
are	O
available	O
.	O
Given	O
the	O
popularity	O
of	O
sequence	O
labeling	O
tasks	O
in	O
NLP	O
,	O
we	O
demonstrate	O
the	O
strength	O
of	O
this	O
approach	O
over	O
several	O
sentiment	B-TaskName
analysis	I-TaskName
tasks	O
,	O
adapted	O
for	O
sequence	O
prediction	O
.	O
These	O
include	O
target	O
-	O
sentiment	O
prediction	O
(	O
Mitchell	O
et	O
al	O
,	O
2013	O
)	O
,	O
aspect	O
-	O
sentiment	O
prediction	O
(	O
Pontiki	O
et	O
al	O
,	O
2016	O
)	O
and	O
subjective	O
text	O
span	O
identification	O
and	O
polarity	O
prediction	O
.	O
To	O
ensure	O
the	O
broad	O
applicability	O
of	O
our	O
approach	O
to	O
other	O
problems	O
,	O
we	O
extend	O
the	O
popular	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
(	O
Lample	O
et	O
al	O
,	O
2016	O
)	O
model	O
that	O
was	O
applied	O
to	O
many	O
sequence	O
labeling	O
tasks	O
1	O
.	O
The	O
modular	O
learning	O
process	O
corresponds	O
to	O
a	O
task	O
decomposition	O
,	O
in	O
which	O
the	O
prediction	O
label	O
,	O
y	O
,	O
is	O
deconstructed	O
into	O
a	O
set	O
of	O
partial	O
labels	O
{	O
y	O
0	B-DatasetName
,	O
..	O
,	O
y	O
k	O
}	O
,	O
each	O
defining	O
a	O
sub	O
-	O
task	O
,	O
capturing	O
a	O
different	O
aspect	O
of	O
the	O
original	O
task	O
.	O
Intuitively	O
,	O
the	O
individual	O
sub	O
-	O
tasks	O
are	O
significantly	O
easier	O
to	O
learn	O
,	O
suggesting	O
that	O
if	O
their	O
dependencies	O
are	O
modeled	O
correctly	O
when	O
learning	O
the	O
final	O
task	O
,	O
they	O
can	O
constrain	O
the	O
learning	O
problem	O
,	O
leading	O
to	O
faster	O
convergence	O
and	O
a	O
better	O
overall	O
learning	O
outcome	O
.	O
In	O
addition	O
,	O
the	O
modular	O
approach	O
helps	O
alleviate	O
the	O
supervision	O
problem	O
,	O
as	O
often	O
providing	O
full	O
supervision	O
for	O
the	O
overall	O
task	O
is	O
costly	O
,	O
while	O
providing	O
additional	O
partial	O
labels	O
is	O
significantly	O
easier	O
.	O
For	O
example	O
,	O
annotating	O
entity	O
segments	O
syntactically	O
is	O
considerably	O
easier	O
than	O
determining	O
their	O
associated	O
sentiment	O
,	O
which	O
requires	O
understanding	O
the	O
nuances	O
of	O
the	O
context	O
they	O
appear	O
in	O
semantically	O
.	O
By	O
exploiting	O
modularity	O
,	O
the	O
entity	O
segmentation	O
partial	O
labels	O
can	O
be	O
used	O
to	O
help	O
improve	O
that	O
specific	O
aspect	O
of	O
the	O
overall	O
task	O
.	O
Our	O
modular	O
task	O
decomposition	O
approach	O
is	O
partially	O
inspired	O
by	O
findings	O
in	O
cognitive	O
neuroscience	O
,	O
namely	O
the	O
two	O
-	O
streams	O
hypothesis	O
,	O
a	O
widely	O
accepted	O
model	O
for	O
neural	O
processing	O
of	O
cognitive	O
information	O
in	O
vision	O
and	O
hearing	O
(	O
Eysenck	O
and	O
Keane	O
,	O
2005	O
)	O
,	O
suggesting	O
the	O
brain	O
processes	O
information	O
in	O
a	O
modular	O
way	O
,	O
split	O
between	O
a	O
"	O
where	O
"	O
(	O
dorsal	O
)	O
pathway	O
,	O
specialized	O
for	O
locating	O
objects	O
and	O
a	O
"	O
what	O
"	O
(	O
ventral	O
)	O
pathway	O
,	O
associated	O
with	O
object	O
representation	O
and	O
recognition	O
(	O
Mishkin	O
et	O
al	O
,	O
1983	O
;	O
Geschwind	O
and	O
Galaburda	O
,	O
1987	O
;	O
Kosslyn	O
,	O
1987	O
;	O
Rueckl	O
et	O
al	O
,	O
1989	O
)	O
.	O
Jacobs	O
et	O
al	O
(	O
1991	O
)	O
provided	O
a	O
computational	O
perspective	O
,	O
investigating	O
the	O
"	O
what	O
"	O
and	O
"	O
where	O
"	O
decomposition	O
on	O
a	O
computer	O
vision	O
task	O
.	O
We	O
observe	O
that	O
this	O
task	O
decomposition	O
naturally	O
fits	O
many	O
NLP	O
tasks	O
and	O
borrow	O
the	O
notation	O
.	O
In	O
the	O
target	O
-	O
sentiment	O
tasks	O
we	O
address	O
in	O
this	O
paper	O
,	O
the	O
segmentation	O
tagging	O
task	O
can	O
be	O
considered	O
as	O
a	O
"	O
where	O
"	O
-	O
task	O
(	O
i.e.	O
,	O
the	O
location	O
of	O
entities	O
)	O
,	O
and	O
the	O
sentiment	O
recognition	O
as	O
the	O
"	O
what	O
"	O
-	O
task	O
.	O
Our	O
approach	O
is	O
related	O
to	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
(	O
Caruana	O
,	O
1997	O
)	O
,	O
which	O
has	O
been	O
extensively	O
applied	O
in	O
NLP	O
(	O
Toshniwal	O
et	O
al	O
,	O
2017	O
;	O
Eriguchi	O
et	O
al	O
,	O
2017	O
;	O
Collobert	O
et	O
al	O
,	O
2011	O
;	O
Luong	O
,	O
2016	O
;	O
Liu	O
et	O
al	O
,	O
2018	O
)	O
.	O
However	O
,	O
instead	O
of	O
simply	O
aggregating	O
the	O
objective	O
functions	O
of	O
several	O
different	O
tasks	O
,	O
we	O
suggest	O
to	O
decompose	O
a	O
single	O
task	O
into	O
multiple	O
inter	O
-	O
connected	O
sub	O
-	O
tasks	O
and	O
then	O
integrate	O
the	O
representation	O
learned	O
into	O
a	O
single	O
module	O
for	O
the	O
final	O
decision	O
.	O
We	O
study	O
several	O
modular	O
neural	O
architectures	O
,	O
which	O
differ	O
in	O
the	O
way	O
information	O
is	O
shared	O
between	O
tasks	O
,	O
the	O
learning	O
representation	O
associated	O
with	O
each	O
task	O
and	O
the	O
way	O
the	O
dependency	O
between	O
decisions	O
is	O
modeled	O
.	O
Our	O
experiments	O
were	O
designed	O
to	O
answer	O
two	O
questions	O
.	O
First	O
,	O
can	O
the	O
task	O
structure	O
be	O
exploited	O
to	O
simplify	O
a	O
complex	O
learning	O
task	O
by	O
using	O
a	O
modular	O
approach	O
?	O
Second	O
,	O
can	O
partial	O
labels	O
be	O
used	O
effectively	O
to	O
reduce	O
the	O
annotation	O
effort	O
?	O
To	O
answer	O
the	O
first	O
question	O
,	O
we	O
conduct	O
experiments	O
over	O
several	O
sequence	O
prediction	O
tasks	O
,	O
and	O
compare	O
our	O
approach	O
to	O
several	O
recent	O
models	O
for	O
deep	O
structured	B-TaskName
prediction	I-TaskName
(	O
Lample	O
et	O
al	O
,	O
2016	O
;	O
Ma	O
and	O
Hovy	O
,	O
2016	O
;	O
Liu	O
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
when	O
available	O
,	O
previously	O
published	O
results	O
(	O
Mitchell	O
et	O
al	O
,	O
2013	O
;	O
Zhang	O
et	O
al	O
,	O
2015	O
;	O
Li	O
and	O
Lu	O
,	O
2017	O
;	O
Ma	O
et	O
al	O
,	O
2018	O
)	O
We	O
show	O
that	O
modular	O
learning	O
indeed	O
helps	O
simplify	O
the	O
learning	O
task	O
compared	O
to	O
traditional	O
monolithic	O
approaches	O
.	O
To	O
answer	O
the	O
second	O
question	O
,	O
we	O
evaluate	O
our	O
model	O
's	O
ability	O
to	O
leverage	O
partial	O
labels	O
in	O
two	O
ways	O
.	O
First	O
,	O
by	O
restricting	O
the	O
amount	O
of	O
full	O
labels	O
,	O
and	O
observing	O
the	O
improvement	O
when	O
providing	O
increasing	O
amounts	O
of	O
partial	O
labels	O
for	O
only	O
one	O
of	O
the	O
sub	O
-	O
tasks	O
.	O
Second	O
,	O
we	O
learn	O
the	O
sub	O
-	O
tasks	O
using	O
completely	O
disjoint	O
datasets	O
of	O
partial	O
labels	O
,	O
and	O
show	O
that	O
the	O
knowledge	O
learned	O
by	O
the	O
sub	O
-	O
task	O
modules	O
can	O
be	O
integrated	O
into	O
the	O
final	O
decision	O
module	O
using	O
a	O
small	O
amount	O
of	O
full	O
labels	O
.	O
Our	O
contributions	O
:	O
(	O
1	O
)	O
We	O
provide	O
a	O
general	O
modular	O
framework	O
for	O
sequence	O
learning	O
tasks	O
.	O
While	O
we	O
focus	O
on	O
sentiment	B-TaskName
analysis	I-TaskName
task	O
,	O
the	O
framework	O
is	O
broadly	O
applicable	O
to	O
many	O
other	O
tagging	O
tasks	O
,	O
for	O
example	O
,	O
NER	B-TaskName
(	O
Carreras	O
et	O
al	O
,	O
2002	O
;	O
Lample	O
et	O
al	O
,	O
2016	O
)	O
and	O
SRL	O
(	O
Zhou	O
and	O
Xu	O
,	O
2015	O
)	O
,	O
to	O
name	O
a	O
few	O
.	O
(	O
2	O
)	O
We	O
introduce	O
a	O
novel	O
weakly	O
supervised	O
learning	O
approach	O
,	O
learning	O
with	O
partial	O
labels	O
,	O
that	O
exploits	O
the	O
modular	O
structure	O
to	O
reduce	O
the	O
supervision	O
effort	O
.	O
(	O
3	O
)	O
We	O
evaluated	O
our	O
proposed	O
model	O
,	O
in	O
both	O
the	O
fullysupervised	O
and	O
weakly	O
supervised	O
scenarios	O
,	O
over	O
several	O
sentiment	B-TaskName
analysis	I-TaskName
tasks	O
.	O

From	O
a	O
technical	O
perspective	O
,	O
our	O
task	O
decomposition	O
approach	O
is	O
related	O
to	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
(	O
Caruana	O
,	O
1997	O
)	O
,	O
specifically	O
,	O
when	O
the	O
tasks	O
share	O
information	O
using	O
a	O
shared	O
deep	O
representation	O
(	O
Collobert	O
et	O
al	O
,	O
2011	O
;	O
Luong	O
,	O
2016	O
)	O
.	O
However	O
,	O
most	O
prior	O
works	O
aggregate	O
multiple	O
losses	O
on	O
either	O
different	O
pre	O
-	O
defined	O
tasks	O
at	O
the	O
final	O
layer	O
(	O
Collobert	O
et	O
al	O
,	O
2011	O
;	O
Luong	O
,	O
2016	O
)	O
,	O
or	O
on	O
a	O
language	O
model	O
at	O
the	O
bottom	O
level	O
(	O
Liu	O
et	O
al	O
,	O
2018	O
)	O
.	O
This	O
work	O
suggests	O
to	O
decompose	O
a	O
given	O
task	O
into	O
sub	O
-	O
tasks	O
whose	O
integration	O
comprise	O
the	O
original	O
task	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
Ma	O
et	O
al	O
(	O
2018	O
)	O
,	O
focusing	O
on	O
targeted	O
sentiment	O
is	O
most	O
similar	O
to	O
our	O
approach	O
.	O
They	O
suggest	O
a	O
joint	O
learning	O
approach	O
,	O
modeling	O
a	O
sequential	O
relationship	O
between	O
two	O
tasks	O
,	O
entity	O
identification	O
and	O
target	O
sentiment	O
.	O
We	O
take	O
a	O
different	O
approach	O
viewing	O
each	O
of	O
the	O
model	O
components	O
as	O
a	O
separate	O
module	O
,	O
predicted	O
independently	O
and	O
then	O
integrated	O
into	O
the	O
final	O
decision	O
module	O
.	O
As	O
we	O
demonstrate	O
in	O
our	O
experiments	O
,	O
this	O
approach	O
leads	O
to	O
better	O
performance	O
and	O
increased	O
flexibil	O
-	O
ity	O
,	O
as	O
it	O
allows	O
us	O
to	O
decouple	O
the	O
learning	O
process	O
and	O
learn	O
the	O
tasks	O
independently	O
.	O
Other	O
modular	O
neural	O
architectures	O
were	O
recently	O
studied	O
for	O
tasks	O
combining	O
vision	O
and	O
language	O
analysis	O
(	O
Andreas	O
et	O
al	O
,	O
2016	O
;	O
Hu	O
et	O
al	O
,	O
2017	O
;	O
Yu	O
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
were	O
tailored	O
for	O
the	O
grounded	O
language	O
setting	O
.	O
To	O
help	O
ensure	O
the	O
broad	O
applicability	O
of	O
our	O
framework	O
,	O
we	O
provide	O
a	O
general	O
modular	O
network	O
formulation	O
for	O
sequence	O
labeling	O
tasks	O
by	O
adapting	O
a	O
neural	O
-	O
CRF	B-MethodName
to	O
capture	O
the	O
task	O
structure	O
.	O
This	O
family	O
of	O
models	O
,	O
combining	O
structured	B-TaskName
prediction	I-TaskName
with	O
deep	O
learning	O
showed	O
promising	O
results	O
(	O
Gillick	O
et	O
al	O
,	O
2015	O
;	O
Lample	O
et	O
al	O
,	O
2016	O
;	O
Ma	O
and	O
Hovy	O
,	O
2016	O
;	O
Zhang	O
et	O
al	O
,	O
2015	O
;	O
Li	O
and	O
Lu	O
,	O
2017	O
)	O
,	O
by	O
using	O
rich	O
representations	O
through	O
neural	O
models	O
to	O
generate	O
decision	O
candidates	O
,	O
while	O
utilizing	O
an	O
inference	O
procedure	O
to	O
ensure	O
coherent	O
decisions	O
.	O
Our	O
main	O
observation	O
is	O
that	O
modular	O
learning	O
can	O
help	O
alleviate	O
some	O
of	O
the	O
difficulty	O
involved	O
in	O
training	O
these	O
powerful	O
models	O
.	O

Using	O
neural	O
networks	O
to	O
generate	O
emission	O
potentials	O
in	O
CRFs	O
was	O
applied	O
successfully	O
in	O
several	O
sequence	O
prediction	O
tasks	O
,	O
such	O
as	O
word	O
segmentation	O
(	O
Chen	O
et	O
al	O
,	O
2017	O
)	O
,	O
NER	B-TaskName
(	O
Ma	O
and	O
Hovy	O
,	O
2016	O
;	O
Lample	O
et	O
al	O
,	O
2016	O
)	O
,	O
chunking	B-TaskName
and	O
PoS	O
tagging	O
(	O
Liu	O
et	O
al	O
,	O
2018	O
;	O
Zhang	O
et	O
al	O
,	O
2017	O
)	O
.	O
A	O
sequence	O
is	O
represented	O
as	O
a	O
sequence	O
of	O
L	O
tokens	O
:	O
x	O
=	O
[	O
x	O
1	O
,	O
x	O
2	O
,	O
.	O
.	O
.	O
,	O
x	O
L	O
]	O
,	O
each	O
token	O
corresponds	O
to	O
a	O
label	O
y	O
Y	O
,	O
where	O
Y	O
is	O
the	O
set	O
of	O
all	O
possible	O
tags	O
.	O
An	O
inference	O
procedure	O
is	O
designed	O
to	O
find	O
the	O
most	O
probable	O
sequence	O
y	O
*	O
=	O
[	O
y	O
1	O
,	O
y	O
2	O
,	O
.	O
.	O
.	O
,	O
y	O
L	O
]	O
by	O
solving	O
,	O
either	O
exactly	O
or	O
approximately	O
,	O
the	O
following	O
optimization	O
problem	O
:	O
y	O
*	O
=	O
arg	O
max	O
y	O
P	O
(	O
y	O
|	O
x	O
)	O
.	O
Despite	O
the	O
difference	O
in	O
tasks	O
,	O
these	O
models	O
follow	O
a	O
similar	O
general	O
architecture	O
:	O
(	O
1	O
)	O
Characterlevel	O
information	O
,	O
such	O
as	O
prefix	O
,	O
suffix	O
and	O
capitalization	O
,	O
is	O
represented	O
through	O
a	O
character	O
embedding	O
layer	O
learned	O
using	O
a	O
bi	O
-	O
directional	O
LSTM	B-MethodName
(	O
BiLSTM	B-MethodName
)	O
.	O
(	O
2	O
)	O
Word	O
-	O
level	O
information	O
is	O
obtained	O
through	O
a	O
word	O
embedding	O
layer	O
.	O
(	O
3	O
)	O
The	O
two	O
representations	O
are	O
concatenated	O
to	O
represent	O
an	O
input	O
token	O
,	O
used	O
as	O
input	O
to	O
a	O
word	O
-	O
level	O
BiLSTM	B-MethodName
which	O
generates	O
the	O
emission	O
potentials	O
for	O
a	O
succeeding	O
CRF	B-MethodName
.	O
(	O
4	O
)	O
The	O
CRF	B-MethodName
is	O
used	O
as	O
an	O
inference	O
layer	O
to	O
generate	O
the	O
globally	O
-	O
normalized	O
probability	O
of	O
possible	O
tag	O
sequences	O
.	O

A	O
CRF	B-MethodName
model	O
describes	O
the	O
probability	O
of	O
predicted	O
labels	O
y	O
,	O
given	O
a	O
sequence	O
x	O
as	O
input	O
,	O
as	O
,	O
ỹ	O
)	O
is	O
the	O
partition	O
function	O
that	O
marginalize	O
over	O
all	O
possible	O
assignments	O
to	O
the	O
predicted	O
labels	O
of	O
the	O
sequence	O
,	O
and	O
Φ	O
(	O
x	O
,	O
y	O
)	O
is	O
the	O
scoring	O
function	O
,	O
which	O
is	O
defined	O
as	O
:	O
P	O
Λ	O
(	O
y	O
|	O
x	O
)	O
=	O
e	O
Φ	O
(	O
x	O
,	O
y	O
)	O
Z	O
,	O
where	O
Z	O
=	O
ỹ	O
e	O
Φ	O
(	O
x	O
Φ	O
(	O
x	O
,	O
y	O
)	O
=	O
t	O
φ	O
(	O
x	O
,	O
y	O
t	O
)	O
+	O
ψ	O
(	O
y	O
t−1	O
,	O
y	O
t	O
)	O
.	O
The	O
partition	O
function	O
Z	O
can	O
be	O
computed	O
efficiently	O
via	O
the	O
forward	O
-	O
backward	O
algorithm	O
.	O
The	O
term	O
φ	O
(	O
x	O
,	O
y	O
t	O
)	O
corresponds	O
to	O
the	O
score	O
of	O
a	O
particular	O
tag	O
y	O
t	O
at	O
position	O
t	O
in	O
the	O
sequence	O
,	O
and	O
ψ	O
(	O
y	O
t−1	O
,	O
y	O
t	O
)	O
represents	O
the	O
score	O
of	O
transition	O
from	O
the	O
tag	O
at	O
position	O
t	O
−	O
1	O
to	O
the	O
tag	O
at	O
position	O
t.	O
In	O
the	O
Neural	O
CRF	B-MethodName
model	O
,	O
φ	O
(	O
x	O
,	O
y	O
t	O
)	O
is	O
generated	O
by	O
the	O
aforementioned	O
Bi	O
-	O
LSTM	B-MethodName
while	O
ψ	O
(	O
y	O
t−1	O
,	O
y	O
t	O
)	O
by	O
a	O
transition	O
matrix	O
.	O

To	O
accommodate	O
our	O
task	O
decomposition	O
approach	O
,	O
we	O
first	O
define	O
the	O
notion	O
of	O
partial	O
labels	O
,	O
and	O
then	O
discuss	O
different	O
neural	O
architectures	O
capturing	O
the	O
dependencies	O
between	O
the	O
modules	O
trained	O
over	O
the	O
different	O
partial	O
labels	O
.	O
Partial	O
Labels	O
and	O
Task	O
Decomposition	O
:	O
Given	O
a	O
learning	O
task	O
,	O
defined	O
over	O
an	O
output	O
space	O
y	O
Y	O
,	O
where	O
Y	O
is	O
the	O
set	O
of	O
all	O
possible	O
tags	O
,	O
each	O
specific	O
label	O
y	O
is	O
decomposed	O
into	O
a	O
set	O
of	O
partial	O
labels	O
,	O
{	O
y	O
0	B-DatasetName
,	O
..	O
,	O
y	O
k	O
}	O
.	O
We	O
refer	O
to	O
y	O
as	O
the	O
full	O
label	O
.	O
According	O
to	O
this	O
definition	O
,	O
a	O
specific	O
assignment	O
to	O
all	O
k	O
partial	O
labels	O
defines	O
a	O
single	O
full	O
label	O
.	O
Note	O
the	O
difference	O
between	O
partially	O
labeled	O
data	O
(	O
Cour	O
et	O
al	O
,	O
2011	O
)	O
,	O
in	O
which	O
instances	O
can	O
have	O
more	O
than	O
a	O
single	O
full	O
label	O
,	O
and	O
our	O
setup	O
in	O
which	O
the	O
labels	O
are	O
partial	O
.	O
In	O
all	O
our	O
experiments	O
,	O
the	O
partial	O
labels	O
refer	O
to	O
two	O
sub	O
-	O
tasks	O
,	O
(	O
1	O
)	O
a	O
segmentation	O
task	O
,	O
identifying	O
Beginning	O
,	O
Inside	O
and	O
Outside	O
of	O
an	O
entity	O
or	O
aspect	O
.	O
(	O
2	O
)	O
one	O
or	O
more	O
type	O
recognition	O
tasks	O
,	O
recognizing	O
the	O
aspect	O
type	O
and/or	O
the	O
sentiment	O
polarity	O
associated	O
with	O
it	O
.	O
Hence	O
,	O
a	O
tag	O
y	O
t	O
at	O
location	O
t	O
is	O
divided	O
into	O
y	O
seg	O
t	O
and	O
y	O
typ	O
t	O
,	O
corresponding	O
to	O
segmentation	O
and	O
type	O
(	O
sentiment	O
type	O
here	O
)	O
respectively	O
.	O
Fig	O
.	O
1	O
provides	O
an	O
example	O
of	O
the	O
target	O
-	O
sentiment	O
task	O
.	O
Note	O
that	O
the	O
sentiment	O
labels	O
do	O
not	O
capture	O
segmentation	O
information	O
.	O
Text	O
ABC	B-MethodName
News	O
'	O
President	O
Tag	O
B	O
-	O
neu	O
O	O
O	O
Christiane	O
Amanpour	O
Exclusive	O
Interview	B-DatasetName
with	O
Seg	O
Senti	O
Mubarak	O
E	O
-	O
neu	O
B	O
-	O
neu	O
E	O
-	O
neu	O
B	O
-	O
neu	O
E	O
-	O
neu	O
O	O
B	O
O	O
O	O
E	O
B	O
E	O
B	O
E	O
O	O
neu	O
O	O
O	O
neu	O
neu	O
neu	O
neu	O
neu	O
O	O
Figure	O
1	O
:	O
Target	O
-	O
sentiment	O
decomposition	O
example	O
.	O
Modular	O
Learning	O
architectures	O
:	O
We	O
propose	O
three	O
different	O
models	O
,	O
in	O
which	O
information	O
from	O
the	O
partial	O
labels	O
can	O
be	O
used	O
.	O
All	O
the	O
models	O
have	O
similar	O
modules	O
types	O
,	O
corresponding	O
to	O
the	O
segmentation	O
and	O
type	O
sub	O
-	O
tasks	O
,	O
and	O
the	O
decision	O
module	O
for	O
predicting	O
the	O
final	O
task	O
.	O
The	O
modules	O
are	O
trained	O
over	O
the	O
partial	O
segmentation	O
(	O
y	O
seg	O
)	O
and	O
type	O
(	O
y	O
typ	O
)	O
labels	O
,	O
and	O
the	O
full	O
label	O
y	O
information	O
,	O
respectively	O
.	O
These	O
three	O
models	O
differ	O
in	O
the	O
way	O
they	O
share	O
information	O
.	O
Model	O
1	O
,	O
denoted	O
Twofold	O
Modular	O
,	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
-	O
T	O
,	O
is	O
similar	O
in	O
spirit	O
to	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
(	O
Collobert	O
et	O
al	O
,	O
2011	O
)	O
with	O
three	O
separate	O
modules	O
.	O
Model	O
2	O
,	O
denoted	O
Twofold	O
modular	O
Infusion	O
,	O
(	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
-	O
TI	O
)	O
and	O
Model	O
3	O
,	O
denoted	O
Twofold	O
modular	O
Infusion	O
with	O
guided	O
gating	O
,	O
(	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
-	O
TI	O
(	O
g	O
)	O
)	O
both	O
infuse	O
information	O
flow	O
from	O
two	O
sub	O
-	O
task	O
modules	O
into	O
the	O
decision	O
module	O
.	O
The	O
difference	O
is	O
whether	O
the	O
infusion	O
is	O
direct	O
or	O
goes	O
through	O
a	O
guided	O
gating	O
mechanism	O
.	O
The	O
three	O
models	O
are	O
depicted	O
in	O
Fig	O
.	O
2	O
and	O
described	O
in	O
details	O
in	O
the	O
following	O
paragraphs	O
.	O
In	O
all	O
of	O
these	O
models	O
,	O
underlying	O
neural	O
architecture	O
are	O
used	O
for	O
the	O
emission	O
potentials	O
when	O
CRF	B-MethodName
inference	O
layers	O
are	O
applied	O
on	O
top	O
.	O

The	O
twofold	O
modular	O
model	O
enhances	O
the	O
original	O
monolithic	O
model	O
by	O
using	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
with	O
shared	O
underlying	O
representations	O
.	O
The	O
segmentation	O
module	O
and	O
the	O
type	O
module	O
are	O
trained	O
jointly	O
with	O
the	O
decision	O
module	O
,	O
and	O
all	O
the	O
modules	O
share	O
information	O
by	O
using	O
the	O
same	O
embedding	O
level	O
representation	O
,	O
as	O
shown	O
in	O
Figure	O
2a	O
.	O
Since	O
the	O
information	O
above	O
the	O
embedding	O
level	O
is	O
independent	O
,	O
the	O
LSTM	B-MethodName
layers	O
in	O
the	O
different	O
modules	O
do	O
not	O
share	O
information	O
,	O
so	O
we	O
refer	O
to	O
these	O
layers	O
of	O
each	O
module	O
as	O
private	O
.	O
The	O
segmentation	O
module	O
predicts	O
the	O
segmentation	O
BIO	O
labels	O
at	O
position	O
t	O
of	O
the	O
sequence	O
by	O
using	O
the	O
representations	O
extracted	O
from	O
its	O
private	O
word	O
level	O
bi	O
-	O
directional	O
LSTM	B-MethodName
(	O
denoted	O
as	O
H	O
seg	O
)	O
as	O
emission	O
for	O
a	O
individual	O
CRF	B-MethodName
:	O
h	O
seg	O
t	O
=	O
H	O
seg	O
(	O
e	O
t	O
,	O
−	O
h	O
seg	O
t−1	O
,	O
−	O
h	O
seg	O
t+1	O
)	O
,	O
φ	O
(	O
x	O
,	O
y	O
seg	O
t	O
)	O
=	O
W	O
seg	O
h	O
seg	O
t	O
+	O
b	O
seg	O
,	O
where	O
W	O
seg	O
and	O
b	O
seg	O
denote	O
the	O
parameters	O
of	O
the	O
segmentation	O
module	O
emission	O
layer	O
,	O
and	O
H	O
seg	O
denotes	O
its	O
private	O
LSTM	B-MethodName
layer	O
.	O
This	O
formulation	O
allows	O
the	O
model	O
to	O
forge	O
the	O
segmentation	O
path	O
privately	O
through	O
backpropagation	O
by	O
providing	O
the	O
segmentation	O
information	O
y	O
seg	O
individually	O
,	O
in	O
addition	O
to	O
the	O
complete	O
tag	O
information	O
y.	O
The	O
type	O
module	O
,	O
using	O
y	O
typ	O
,	O
is	O
constructed	O
in	O
a	O
similar	O
way	O
.	O
By	O
using	O
representations	O
from	O
the	O
its	O
own	O
private	O
LSTM	B-MethodName
layers	O
,	O
the	O
type	O
module	O
predicts	O
the	O
sentiment	O
(	O
entity	O
)	O
type	O
at	O
position	O
t	O
of	O
the	O
sequence	O
:	O
h	O
typ	O
t	O
=	O
H	O
typ	O
(	O
e	O
t	O
,	O
−	O
h	O
typ	O
t−1	O
,	O
−	O
h	O
typ	O
t+1	O
)	O
,	O
φ	O
(	O
x	O
,	O
y	O
typ	O
t	O
)	O
=	O
W	O
typ	O
h	O
typ	O
t	O
+	O
b	O
typ	O
.	O
Both	O
the	O
segmentation	O
information	O
y	O
seg	O
and	O
the	O
type	O
information	O
y	O
typ	O
are	O
provided	O
together	O
with	O
the	O
complete	O
tag	O
sequence	O
y	O
,	O
enabling	O
the	O
model	O
to	O
learn	O
segmentation	O
and	O
type	O
recognition	O
simultaneously	O
using	O
two	O
different	O
paths	O
.	O
Also	O
,	O
the	O
decomposed	O
tags	O
naturally	O
augment	O
more	O
training	O
data	O
to	O
the	O
model	O
,	O
avoiding	O
over	O
-	O
fitting	O
due	O
to	O
more	O
complicated	O
structure	O
.	O
The	O
shared	O
representation	O
beneath	O
the	O
private	O
LSTMs	O
layers	O
are	O
updated	O
via	O
the	O
back	O
-	O
propagated	O
errors	O
from	O
all	O
the	O
three	O
modules	O
.	O

The	O
twofold	O
modular	O
infusion	O
model	O
provides	O
a	O
stronger	O
connection	O
between	O
the	O
functionalities	O
of	O
the	O
two	O
sub	O
-	O
tasks	O
modules	O
and	O
the	O
final	O
decision	O
module	O
,	O
differing	O
from	O
multi	O
-	O
task	O
leaning	O
.	O
In	O
this	O
model	O
,	O
instead	O
of	O
separating	O
the	O
pathways	O
from	O
the	O
decision	O
module	O
as	O
in	O
the	O
previous	O
twofold	O
modular	O
model	O
,	O
the	O
segmentation	O
and	O
the	O
type	O
representation	O
are	O
used	O
as	O
input	O
to	O
the	O
final	O
decision	O
module	O
.	O
The	O
model	O
structure	O
is	O
shown	O
in	O
Figure	O
2b	O
,	O
and	O
can	O
be	O
described	O
formally	O
as	O
:	O
I	O
seg	O
t	O
=	O
W	O
seg	O
h	O
seg	O
t	O
+	O
b	O
seg	O
,	O
I	O
typ	O
t	O
=	O
W	O
typ	O
h	O
typ	O
t	O
+	O
b	O
typ	O
,	O
S	O
t	O
=	O
W	O
[	O
h	O
t	O
;	O
I	O
seg	O
t	O
;	O
I	O
typ	O
t	O
]	O
+	O
b	O
,	O
where	O
S	O
t	O
is	O
the	O
shared	O
final	O
emission	O
potential	O
to	O
the	O
CRF	B-MethodName
layer	O
in	O
the	O
decision	O
module	O
,	O
and	O
;	O
is	O
the	O
Figure	O
2	O
:	O
Three	O
modular	O
models	O
for	O
task	O
decomposition	O
.	O
In	O
them	O
,	O
blue	O
blocks	O
are	O
segmentation	O
modules	O
,	O
detecting	O
entity	O
location	O
and	O
segmentation	O
,	O
and	O
yellow	O
blocks	O
are	O
the	O
type	O
modules	O
,	O
recognizing	O
the	O
entity	O
type	O
or	O
sentiment	O
polarity	O
.	O
Green	O
blocks	O
are	O
the	O
final	O
decision	O
modules	O
,	O
integrating	O
all	O
the	O
decisions	O
.	O
(	O
G	O
)	O
refers	O
to	O
"	O
Guided	O
Gating	O
"	O
concatenation	O
operator	O
,	O
combining	O
the	O
representation	O
from	O
the	O
decision	O
module	O
and	O
that	O
from	O
the	O
type	O
module	O
and	O
the	O
segmentation	O
module	O
.	O
The	O
term	O
"	O
Infusion	O
"	O
used	O
for	O
naming	O
this	O
module	O
is	O
intended	O
to	O
indicate	O
that	O
both	O
modules	O
actively	O
participate	O
in	O
the	O
final	O
decision	O
process	O
,	O
rather	O
than	O
merely	O
form	O
two	O
independent	O
paths	O
as	O
in	O
the	O
twofold	O
modular	O
model	O
.	O
This	O
formulation	O
provides	O
an	O
alternative	O
way	O
of	O
integrating	O
the	O
auxiliary	O
sub	O
-	O
tasks	O
back	O
into	O
the	O
major	O
task	O
in	O
the	O
neural	O
structure	O
to	O
help	O
improve	O
learning	O
.	O

Our	O
experimental	O
evaluation	O
is	O
designed	O
to	O
evaluate	O
the	O
two	O
key	O
aspects	O
of	O
our	O
model	O
:	O
(	O
Q1	O
)	O
Can	O
the	O
modular	O
architecture	O
alleviate	O
the	O
difficulty	O
of	O
learning	O
the	O
final	O
task	O
?	O
To	O
answer	O
this	O
question	O
,	O
we	O
compare	O
our	O
modular	O
architecture	O
to	O
the	O
traditional	O
neural	O
-	O
CRF	B-MethodName
model	O
and	O
several	O
recent	O
competitive	O
models	O
for	O
sequence	O
labeling	O
combining	O
inference	O
and	O
deep	O
learning	O
.	O
The	O
results	O
are	O
summarized	O
in	O
Tables	O
1	O
-	O
3	O
.	O
(	O
Q2	O
)	O
Can	O
partial	O
labels	O
be	O
used	O
effectively	O
as	O
a	O
new	O
form	O
of	O
weak	O
-	O
supervision	O
?	O
To	O
answer	O
this	O
question	O
we	O
compared	O
the	O
performance	O
of	O
the	O
model	O
when	O
trained	O
using	O
disjoint	O
sets	O
of	O
partial	O
and	O
full	O
labels	O
,	O
and	O
show	O
that	O
adding	O
examples	O
only	O
associated	O
with	O
partial	O
labels	O
,	O
can	O
help	O
boost	O
performance	O
on	O
the	O
final	O
task	O
.	O
The	O
results	O
are	O
summarized	O
in	O
Figures	O
3	O
-	O
5	O
.	O

We	O
adapted	O
the	O
SemEval	B-DatasetName
2013	I-DatasetName
Task	O
2	O
subtask	O
A	O
as	O
another	O
task	O
to	O
evaluate	O
our	O
model	O
.	O
In	O
this	O
task	O
,	O
the	O
system	O
is	O
given	O
a	O
marked	O
phrase	O
inside	O
a	O
longer	O
text	O
,	O
and	O
is	O
asked	O
to	O
label	O
its	O
polarity	O
.	O
Unlike	O
the	O
original	O
task	O
,	O
we	O
did	O
not	O
assume	O
the	O
sequence	O
is	O
known	O
,	O
resulting	O
in	O
two	O
decisions	O
,	O
identifying	O
subjective	O
expressions	O
(	O
i.e.	O
,	O
a	O
segmentation	O
task	O
)	O
and	O
labeling	O
their	O
polarity	O
,	O
which	O
can	O
be	O
modeled	O
jointly	O
as	O
a	O
sequence	O
labeling	O
task	O
.	O

Our	O
first	O
set	O
of	O
results	O
are	O
designed	O
to	O
compare	O
our	O
modular	O
learning	O
models	O
,	O
utilize	O
partial	O
labels	O
decomposition	O
,	O
with	O
traditional	O
monolithic	O
models	O
,	O
that	O
learn	O
directly	O
over	O
the	O
full	O
labels	O
.	O
In	O
all	O
three	O
tasks	O
,	O
we	O
compare	O
with	O
strong	O
sequence	O
prediction	O
models	O
,	O
including	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
(	O
Lample	O
et	O
al	O
,	O
2016	O
)	O
,	O
which	O
is	O
directly	O
equivalent	O
to	O
our	O
baseline	O
model	O
(	O
i.e.	O
,	O
final	O
task	O
decision	O
without	O
the	O
modules	O
)	O
,	O
and	O
LSTM	B-MethodName
-	O
CNN	O
-	O
CRF	B-MethodName
(	O
Ma	O
and	O
Hovy	O
,	O
2016	O
)	O
and	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
-	O
LM	O
(	O
Liu	O
et	O
al	O
,	O
2018	O
)	O
which	O
use	O
a	O
richer	O
latent	O
representation	O
for	O
scoring	O
the	O
emission	O
potentials	O
.	O
(	O
denoted	O
E+A	O
)	O
.	O
The	O
second	O
adds	O
a	O
third	O
module	O
that	O
predicts	O
the	O
sentiment	O
polarity	O
associated	O
with	O
the	O
aspect	O
(	O
denoted	O
E+A+S	O
)	O
.	O
I.e.	O
,	O
for	O
a	O
given	O
sentence	O
,	O
label	O
its	O
entity	O
span	O
,	O
the	O
aspect	O
category	O
of	O
the	O
entity	O
and	O
the	O
sentiment	O
polarity	O
of	O
the	O
entity	O
at	O
the	O
same	O
time	O
.	O
The	O
results	O
over	O
four	O
languages	O
are	O
summarized	O
in	O
Tab	O
.	O
2	O
.	O
In	O
all	O
cases	O
,	O
our	O
modular	O
approach	O
outperforms	O
all	O
monolithic	O
approaches	O
.	O
Subjective	O
Phrase	O
Identification	O
and	O
Classification	B-TaskName
This	O
dataset	O
contains	O
tweets	O
annotated	O
with	O
sentiment	O
phrases	O
,	O
used	O
for	O
training	O
the	O
models	O
.	O
As	O
in	O
the	O
original	O
SemEval	O
task	O
,	O
it	O
is	O
tested	O
in	O
two	O
settings	O
,	O
in	O
-	O
domain	O
,	O
where	O
the	O
test	O
data	O
also	O
consists	O
of	O
tweets	O
,	O
and	O
out	O
-	O
of	O
-	O
domain	O
,	O
where	O
the	O
test	O
set	O
consists	O
of	O
SMS	O
text	O
messages	O
.	O
We	O
present	O
the	O
results	O
of	O
experiments	O
on	O
these	O
data	O
set	O
in	O
Table	O
3	O
.	O

We	O
present	O
and	O
study	O
several	O
modular	O
neural	O
architectures	O
designed	O
for	O
a	O
novel	O
learning	O
scenario	O
:	O
learning	O
from	O
partial	O
labels	O
.	O
We	O
experiment	O
with	O
several	O
sentiment	B-TaskName
analysis	I-TaskName
tasks	O
.	O
Our	O
models	O
,	O
inspired	O
by	O
cognitive	O
neuroscience	O
findings	O
(	O
Jacobs	O
et	O
al	O
,	O
1991	O
;	O
Eysenck	O
and	O
Keane	O
,	O
2005	O
)	O
and	O
multitask	O
learning	O
,	O
suggest	O
a	O
functional	O
decomposition	O
of	O
the	O
original	O
task	O
into	O
two	O
simpler	O
sub	O
-	O
tasks	O
.	O
We	O
evaluate	O
different	O
methods	O
for	O
sharing	O
information	O
and	O
integrating	O
the	O
modules	O
into	O
the	O
final	O
decision	O
,	O
such	O
that	O
a	O
better	O
model	O
can	O
be	O
learned	O
,	O
while	O
converging	O
faster	O
5	O
.	O
As	O
our	O
experiments	O
show	O
,	O
modular	O
learning	O
can	O
be	O
used	O
with	O
weak	O
supervision	O
,	O
using	O
examples	O
annotated	O
with	O
partial	O
labels	O
only	O
.	O
The	O
modular	O
approach	O
also	O
provides	O
interesting	O
directions	O
for	O
future	O
research	O
,	O
focusing	O
on	O
alleviating	O
the	O
supervision	O
bottleneck	O
by	O
using	O
large	O
amount	O
of	O
partially	O
labeled	O
data	O
that	O
are	O
cheaper	O
and	O
easy	O
to	O
obtain	O
,	O
together	O
with	O
only	O
a	O
handful	O
amount	O
of	O
annotated	O
data	O
,	O
a	O
scenario	O
especially	O
suitable	O
for	O
low	O
-	O
resource	O
languages	O
.	O

In	O
Figure	O
6	O
,	O
we	O
show	O
an	O
example	O
of	O
task	O
decomposition	O
for	O
standard	O
NER	B-TaskName
.	O
In	O
Figure	O
7	O
,	O
we	O
show	O
another	O
example	O
of	O
task	O
decomposition	O
for	O
target	O
sentiment	O
,	O
in	O
addition	O
to	O
the	O
one	O
in	O
the	O
main	O
text	O
.	O

NER	B-TaskName
datasets	O
We	O
evaluated	O
our	O
models	O
on	O
three	O
NER	B-TaskName
datasets	O
,	O
the	O
English	O
,	O
Dutch	O
and	O
Spanish	O
parts	O
of	O
the	O
2002	O
and	O
2003	O
CoNLL	O
shared	O
tasks	O
(	O
Sang	O
and	O
F.	O
,	O
2002	O
;	O
Sang	O
et	O
al	O
,	O
2003	O
)	O
.	O
We	O
used	O
the	O
original	O
division	O
of	O
training	O
,	O
validation	O
and	O
test	O
sets	O
.	O
The	O
task	O
is	O
defined	O
over	O
four	O
different	O
entity	O
types	O
:	O
PERSON	O
,	O
LOCATION	O
,	O
ORGANIZATION	O
,	O
MISC	O
.	O
We	O
used	O
the	O
BIOES	O
tagging	O
scheme	O
during	O
the	O
training	O
,	O
and	O
convert	O
them	O
back	O
to	O
original	O
tagging	O
scheme	O
in	O
testing	O
as	O
previous	O
studies	O
show	O
that	O
using	O
this	O
tagging	O
scheme	O
instead	O
of	O
BIO2	O
can	O
help	O
improve	O
performance	O
(	O
Ratinov	O
and	O
Roth	O
,	O
2009	O
;	O
Lample	O
et	O
al	O
,	O
2016	O
;	O
Ma	O
and	O
Hovy	O
,	O
2016	O
;	O
Liu	O
et	O
al	O
,	O
2018	O
)	O
.	O
As	O
a	O
result	O
,	O
the	O
segmentation	O
module	O
had	O
5	O
output	O
labels	O
,	O
and	O
the	O
entity	O
module	O
had	O
4	O
.	O
The	O
final	O
decision	O
task	O
,	O
consisted	O
of	O
the	O
Cartesian	O
product	O
of	O
the	O
segmentation	O
set	O
(	O
BIES	O
)	O
and	O
the	O
entity	O
set	O
,	O
plus	O
the	O
"	O
O	O
"	O
tag	O
,	O
resulting	O
in	O
17	O
labels	O
.	O
Results	O
on	O
NER	B-TaskName
We	O
compared	O
our	O
models	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
systems	O
on	O
English	O
6	O
,	O
Dutch	O
and	O
Spanish	O
.	O
For	O
Dutch	O
and	O
Spanish	O
,	O
we	O
used	O
cross	O
-	O
lingual	O
embedding	O
as	O
a	O
way	O
to	O
exploit	O
lexical	O
information	O
.	O
The	O
results	O
are	O
shown	O
in	O
Tab	O
.	O
5	O
and	O
Tab	O
.	O
6	O
7	O
.	O
Our	O
best	O
-	O
performing	O
model	O
outperform	O
all	O
the	O
competing	O
systems	O
.	O

We	O
conducted	O
additional	O
experiments	O
on	O
knowledge	O
integration	O
in	O
the	O
same	O
setting	O
as	O
in	O
the	O
main	O
text	O
to	O
investigate	O
the	O
properties	O
of	O
the	O
modules	O
.	O
Figure	O
8	O
shows	O
the	O
results	O
for	O
Dutch	O
and	O
Spanish	O
NER	B-TaskName
datasets	O
,	O
while	O
Figure	O
9	O
shows	O
the	O
results	O
for	O
the	O
Subjective	O
Polarity	O
Disambiguation	O
Datasets	O
using	O
the	O
in	O
-	O
domain	O
data	O
.	O

We	O
thank	O
the	O
reviewers	O
for	O
their	O
insightful	O
comments	O
.	O
We	O
thank	O
the	O
NVIDIA	O
Corporation	O
for	O
their	O
GPU	O
donation	O
,	O
used	O
in	O
this	O
work	O
.	O
This	O
work	O
was	O
partially	O
funded	O
by	O
a	O
Google	B-DatasetName
Gift	O
.	O

Identifying	O
Aggression	O
and	O
Toxicity	O
in	O
Comments	O
using	O
Capsule	B-MethodName
Network	I-MethodName

Early	O
works	O
in	O
automated	O
detection	O
of	O
abusive	B-TaskName
language	I-TaskName
made	O
use	O
of	O
basic	O
machine	O
learning	O
like	O
Tf	O
-	O
Idf	O
(	O
Yin	O
et	O
al	O
,	O
2009	O
)	O
,	O
SVM	B-MethodName
(	O
Warner	O
and	O
Hirschberg	O
,	O
2012	O
)	O
,	O
Naive	O
Bayes	O
,	O
random	O
forests	O
,	O
or	O
logistic	B-MethodName
regression	I-MethodName
over	O
a	O
bag	O
-	O
of	O
-	O
ngrams	O
and	O
achieved	O
limited	O
success	O
.	O
Newer	O
approaches	O
include	O
solving	O
problems	O
using	O
deep	O
learning	O
architectures	O
like	O
CNNs	O
(	O
Kim	O
,	O
2014	O
;	O
Zhang	O
et	O
al	O
,	O
2015	O
;	O
Conneau	O
et	O
al	O
,	O
2017b	O
;	O
Park	O
and	O
Fung	O
,	O
2017	O
)	O
which	O
just	O
focus	O
on	O
spatial	O
patterns	O
or	O
LSTMs	O
which	O
treat	O
text	O
as	O
sequences	O
(	O
Tai	O
et	O
al	O
,	O
2015	O
;	O
Mousa	O
and	O
Schuller	O
,	O
2017	O
)	O
.	O
Another	O
popular	O
approach	O
completely	O
ignores	O
the	O
order	O
of	O
words	O
but	O
focuses	O
on	O
their	O
compositions	O
as	O
a	O
collection	O
,	O
like	O
probabilistic	O
topic	O
modeling	O
(	O
Blei	O
et	O
al	O
,	O
2003	O
;	O
Mcauliffe	O
and	O
Blei	O
,	O
2008	O
)	O
and	O
Earth	O
Movers	O
Distance	O
based	O
modeling	O
(	O
Kusner	O
et	O
al	O
,	O
2015	O
;	O
Ye	O
et	O
al	O
,	O
2017	O
)	O
.	O
Recently	O
Capsule	B-MethodName
Network	I-MethodName
(	O
Sabour	O
et	O
al	O
,	O
2017	O
)	O
has	O
been	O
used	O
in	O
text	B-TaskName
classification	I-TaskName
(	O
Zhao	O
et	O
al	O
,	O
2018	O
)	O
.It	O
makes	O
use	O
of	O
the	O
dynamic	O
routing	O
process	O
to	O
alleviate	O
the	O
disturbance	O
of	O
some	O
noise	O
capsules	O
which	O
may	O
contain	O
background	O
information	O
such	O
as	O
stop	O
words	O
and	O
words	O
that	O
are	O
unrelated	O
to	O
specific	O
categories	O
and	O
show	O
that	O
capsule	O
networks	O
achieves	O
significant	O
improvement	O
over	O
strong	O
baseline	O
methods	O
.	O
As	O
we	O
focus	O
to	O
solve	O
the	O
problem	O
of	O
toxic	O
comments	O
and	O
cyberbullying	O
,	O
we	O
are	O
confronted	O
with	O
the	O
issue	O
of	O
large	O
class	O
imbalance	O
.	O
We	O
use	O
focal	B-MethodName
loss	I-MethodName
(	O
Lin	O
et	O
al	O
,	O
2017	O
)	O
to	O
tackle	O
it	O
as	O
it	O
prevents	O
the	O
vast	O
number	O
of	O
easy	O
negatives	O
from	O
overwhelming	O
the	O
detector	O
during	O
training	O
.	O
Also	O
,	O
in	O
the	O
online	O
space	O
people	O
tend	O
to	O
talk	O
using	O
different	O
languages	O
in	O
the	O
same	O
comment	O
and	O
often	O
use	O
transliteration	O
.	O
We	O
show	O
that	O
our	O
model	O
is	O
suitable	O
for	O
such	O
data	O
as	O
well	O
.	O

In	O
this	O
section	O
we	O
attempt	O
to	O
describe	O
different	O
models	O
that	O
we	O
have	O
used	O
for	O
the	O
classification	O
process	O
.	O
We	O
seek	O
to	O
answer	O
the	O
following	O
questions	O
:	O
(	O
1	O
)	O
Is	O
combination	O
of	O
Capsules	O
and	O
focal	B-MethodName
loss	I-MethodName
the	O
new	O
apotheosis	O
for	O
toxic	B-TaskName
comment	I-TaskName
classification	I-TaskName
problems	O
?	O
(	O
2	O
)	O
Can	O
capsules	O
solve	O
the	O
problem	O
of	O
OOV	O
and	O
transliteration	O
implicitly	O
?	O

Recently	O
,	O
Kaggle	O
hosted	O
a	O
competition	O
named	O
Toxic	B-TaskName
Comment	I-TaskName
Classification	I-TaskName
.	O
This	O
dataset	O
has	O
been	O
contributed	O
by	O
Conversation	O
AI	O
,	O
which	O
is	O
a	O
research	O
initiative	O
founded	O
by	O
Jigsaw	B-MethodName
and	O
Google	B-DatasetName
.	O
The	O
task	O
was	O
comprised	O
of	O
calculating	O
the	O
log	B-MetricName
-	I-MetricName
likelihood	I-MetricName
of	O
a	O
sentence	O
for	O
the	O
six	O
classes	O
,	O
i.e.	O
,	O
given	O
a	O
sentence	O
calculate	O
the	O
probability	O
of	O
it	O
belonging	O
to	O
six	O
classes	O
.	O
The	O
six	O
different	O
classes	O
were	O
toxic	O
,	O
severe	O
toxic	O
,	O
obscene	O
,	O
threat	O
,	O
insult	O
and	O
identity	O
hate	O
.	O

"	O
First	O
Shared	O
Task	O
on	O
Aggression	B-TaskName
Identification	I-TaskName
"	O
released	O
a	O
dataset	O
for	O
Aggression	B-TaskName
Identification	I-TaskName
.	O
The	O
task	O
was	O
to	O
classify	O
the	O
comments	O
into	O
one	O
of	O
the	O
three	O
different	O
classes	O
Overtly	O
Aggressive	O
,	O
Covertly	O
Aggressive	O
,	O
and	O
Non	O
-	O
aggressive	O
.	O
The	O
train	O
data	O
was	O
given	O
in	O
English	O
and	O
Hindi	O
,	O
where	O
some	O
of	O
the	O
comments	O
in	O
Hindi	O
dataset	O
were	O
transliterated	O
to	O
English	O
.	O

We	O
evaluate	O
and	O
compare	O
our	O
model	O
with	O
several	O
strong	O
baseline	O
methods	O
including	O
:	O
LSTM	B-MethodName
with	O
Maxpool	O
(	O
Lai	O
et	O
al	O
,	O
2015	O
)	O
,	O
Attention	O
networks	O
(	O
Raffel	O
and	O
Ellis	O
,	O
2015	O
)	O
,	O
Pre	O
-	O
trained	O
LSTMs	O
(	O
Dai	O
and	O
Le	O
,	O
2015	O
)	O
,	O
Hierarchical	O
ConvNet	O
(	O
Conneau	O
et	O
al	O
,	O
2017a	O
)	O
,	O
Bi	O
-	O
LSTM	B-MethodName
with	O
Skip	O
-	O
connections	O
,	O
variation	O
of	O
CNN	O
-	O
LSTM	B-MethodName
(	O
Wang	O
et	O
al	O
,	O
2016	O
)	O
,	O
CNN	O
-	O
multifilter	O
(	O
Kim	O
,	O
2014	O
)	O
,	O
Bi	O
-	O
LSTM	B-MethodName
with	O
xgboost	O
and	O
logistic	B-MethodName
regression	I-MethodName
.	O
We	O
experiment	O
with	O
these	O
models	O
on	O
three	O
datasets	O
.	O
The	O
models	O
were	O
first	O
evaluated	O
on	O
Kaggle	O
competition	O
for	O
Toxic	B-TaskName
Comment	I-TaskName
Classification	I-TaskName
.	O
All	O
the	O
model	O
parameters	O
and	O
attributes	O
were	O
decided	O
on	O
the	O
basis	O
of	O
our	O
best	O
performing	O
model	O
,	O
and	O
were	O
kept	O
same	O
for	O
the	O
rest	O
of	O
experimentations	O
and	O
datasets	O
.	O

In	O
this	O
work	O
,	O
we	O
have	O
proposed	O
to	O
automatically	O
detect	O
toxicity	O
and	O
aggression	O
in	O
comments	O
,	O
we	O
show	O
that	O
with	O
minimal	O
preprocessing	O
techniques	O
we	O
are	O
able	O
to	O
achieve	O
a	O
good	O
model	O
performance	O
and	O
demonstrated	O
how	O
OOV	O
words	O
and	O
semantic	O
sense	O
are	O
learnt	O
implicitly	O
with	O
random	O
initialisation	O
.	O
We	O
show	O
the	O
effectiveness	O
of	O
our	O
proposed	O
model	O
against	O
strong	O
benchmark	O
algorithms	O
and	O
that	O
it	O
outperforms	O
others	O
.	O
In	O
this	O
work	O
,	O
we	O
did	O
basic	O
preprocessing	O
of	O
the	O
data	O
,	O
however	O
in	O
future	O
we	O
intend	O
to	O
explore	O
more	O
preprocessing	O
techniques	O
for	O
the	O
dataset	O
,	O
like	O
data	B-TaskName
augmentation	I-TaskName
using	O
translation	O
approaches	O
and	O
methods	O
to	O
deal	O
with	O
mispelled	O
words	O
.	O
We	O
further	O
would	O
examine	O
the	O
results	O
of	O
capsule	O
net	O
by	O
visualising	O
which	O
words	O
or	O
phrases	O
does	O
the	O
model	O
correctly	O
recognises	O
for	O
classification	O
as	O
opposed	O
to	O
benchmark	O
algorithms	O
.	O
Also	O
,	O
we	O
would	O
like	O
to	O
examine	O
the	O
usage	O
of	O
focal	B-MethodName
loss	I-MethodName
with	O
the	O
rest	O
of	O
the	O
baseline	O
models	O
.	O

In	O
one	O
of	O
our	O
experiments	O
,	O
the	O
summary	O
vector	O
obtained	O
from	O
LSTMs	O
was	O
concatenated	O
with	O
the	O
vector	O
obtained	O
after	O
appying	O
Max	O
Over	O
Time	O
on	O
the	O
hidden	O
state	O
representation	O
of	O
the	O
input	O
.	O
The	O
intuition	O
behind	O
this	O
was	O
that	O
,	O
by	O
passing	O
most	O
relevant	O
features	O
along	O
with	O
summary	O
of	O
the	O
input	O
to	O
the	O
softmax	B-MethodName
layer	O
may	O
enhance	O
the	O
clasification	O
process	O
.	O
From	O
the	O
experiments	O
we	O
obtained	O
competetive	O
results	O
using	O
this	O
model	O
.	O

In	O
(	O
Dai	O
and	O
Le	O
,	O
2015	O
)	O
,	O
authors	O
claimed	O
that	O
by	O
pretraining	O
LSTMs	O
on	O
some	O
related	O
task	O
as	O
Auto	O
-	O
Encoder	O
or	O
as	O
a	O
Language	O
Model	O
,	O
could	O
optimize	O
the	O
stability	O
of	O
the	O
LSTMs	O
training	O
process	O
.	O
The	O
authors	O
reported	O
improvenemt	O
in	O
error	O
rates	O
by	O
good	O
margin	O
in	O
many	O
tasks	O
like	O
,	O
text	B-TaskName
classification	I-TaskName
on	O
20	O
Newsgroup	O
,	O
IMDB	B-DatasetName
etc	O
.	O
For	O
our	O
experiments	O
we	O
gathered	O
many	O
related	O
datasets	O
like	O
all	O
of	O
Wikimedia	O
datasets	O
(	O
Wulczyn	O
et	O
al	O
,	O
2017	O
)	O
,	O
TRAC	O
shared	O
dataset	O
,	O
IMDB	B-DatasetName
movie	I-DatasetName
reviews	I-DatasetName
dataset	O
.	O
An	O
autoencoder	B-MethodName
was	O
trained	O
on	O
these	O
datasets	O
and	O
the	O
LSTMs	O
from	O
the	O
encoder	O
part	O
were	O
extracted	O
and	O
used	O
in	O
the	O
classifcation	O
task	O
.	O

The	O
idea	O
of	O
applying	O
CNNs	O
for	O
text	B-TaskName
classification	I-TaskName
was	O
proposed	O
in	O
(	O
Kim	O
,	O
2014	O
)	O
,	O
where	O
authors	O
applied	O
filters	O
of	O
different	O
length	O
to	O
extract	O
N	O
-	O
gram	O
features	O
from	O
text	O
.	O
The	O
authors	O
tried	O
static	O
and	O
dynamic	O
embedding	O
channels	O
and	O
concluded	O
that	O
the	O
model	O
with	O
combination	O
of	O
both	O
outperformed	O
others	O
.	O
For	O
our	O
setting	O
we	O
found	O
that	O
filters	O
of	O
length	O
[	O
2	O
,	O
3	O
,	O
4	O
]	O
have	O
outperformed	O
other	O
filter	O
sizes	O
,	O
we	O
tried	O
various	O
combinations	O
from	O
range	O
[	O
2	O
,	O
5	O
]	O
.	O
For	O
activations	O
we	O
used	O
Leaky	B-MethodName
ReLU	I-MethodName
,	O
and	O
performed	O
Batch	B-MethodName
Normalization	I-MethodName
to	O
stablize	O
the	O
data	O
.	O

A	O
joint	O
architecture	O
of	O
CNNs	O
and	O
RNNs	O
were	O
proposed	O
in	O
(	O
Wang	O
et	O
al	O
,	O
2016	O
)	O
,	O
where	O
the	O
authors	O
tried	O
combination	O
of	O
CNNs	O
with	O
different	O
RNNs	O
like	O
GRUs	O
and	O
LSTMs	O
.	O
In	O
our	O
experiment	O
,	O
we	O
again	O
used	O
Leaky	B-MethodName
ReLU	I-MethodName
for	O
CNNs	O
activations	O
,	O
filter	O
size	O
of	O
3	O
was	O
fixed	O
for	O
the	O
experiments	O
to	O
decide	O
the	O
dropout	O
values	O
and	O
other	O
hyperparameters	O
tuning	O
.	O

In	O
(	O
Lai	O
et	O
al	O
,	O
2015	O
)	O
,	O
authors	O
took	O
Max	O
Over	O
Time	O
on	O
the	O
RNN	O
representation	O
of	O
the	O
input	O
.	O
Their	O
model	O
RNN	O
outperformed	O
other	O
models	O
in	O
3	O
out	O
of	O
4	O
datasets	O
.	O
In	O
our	O
experiments	O
,	O
we	O
fixed	O
LSTM	B-MethodName
units	O
to	O
be	O
51	O
,	O
and	O
rest	O
of	O
the	O
parameters	O
were	O
decided	O
on	O
the	O
basis	O
of	O
validation	O
-	O
data	O
experiments	O
.	O

BERT	B-MethodName
Post	O
-	O
Training	O
for	O
Review	O
Reading	B-TaskName
Comprehension	I-TaskName
and	O
Aspect	B-TaskName
-	I-TaskName
based	I-TaskName
Sentiment	I-TaskName
Analysis	I-TaskName

Question	O
-	O
answering	O
plays	O
an	O
important	O
role	O
in	O
e	O
-	O
commerce	O
as	O
it	O
allows	O
potential	O
customers	O
to	O
actively	O
seek	O
crucial	O
information	O
about	O
products	O
or	O
services	O
to	O
help	O
their	O
purchase	O
decision	B-TaskName
making	I-TaskName
.	O
Inspired	B-DatasetName
by	O
the	O
recent	O
success	O
of	O
machine	B-TaskName
reading	I-TaskName
comprehension	I-TaskName
(	O
MRC	O
)	O
on	O
formal	O
documents	O
,	O
this	O
paper	O
explores	O
the	O
potential	O
of	O
turning	O
customer	O
reviews	O
into	O
a	O
large	O
source	O
of	O
knowledge	O
that	O
can	O
be	O
exploited	O
to	O
answer	O
user	O
questions	O
.	O
We	O
call	O
this	O
problem	O
Review	O
Reading	B-TaskName
Comprehension	I-TaskName
(	O
RRC	O
)	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
no	O
existing	O
work	O
has	O
been	O
done	O
on	O
RRC	O
.	O
In	O
this	O
work	O
,	O
we	O
first	O
build	O
an	O
RRC	O
dataset	O
called	O
ReviewRC	O
based	O
on	O
a	O
popular	O
benchmark	O
for	O
aspectbased	O
sentiment	B-TaskName
analysis	I-TaskName
.	O
Since	O
ReviewRC	O
has	O
limited	O
training	O
examples	O
for	O
RRC	O
(	O
and	O
also	O
for	O
aspect	B-TaskName
-	I-TaskName
based	I-TaskName
sentiment	I-TaskName
analysis	I-TaskName
)	O
,	O
we	O
then	O
explore	O
a	O
novel	O
post	O
-	O
training	O
approach	O
on	O
the	O
popular	O
language	O
model	O
BERT	B-MethodName
to	O
enhance	O
the	O
performance	O
of	O
fine	O
-	O
tuning	O
of	O
BERT	B-MethodName
for	O
RRC	O
.	O
To	O
show	O
the	O
generality	O
of	O
the	O
approach	O
,	O
the	O
proposed	O
post	O
-	O
training	O
is	O
also	O
applied	O
to	O
some	O
other	O
review	O
-	O
based	O
tasks	O
such	O
as	O
aspect	B-TaskName
extraction	I-TaskName
and	O
aspect	O
sentiment	O
classification	O
in	O
aspect	B-TaskName
-	I-TaskName
based	I-TaskName
sentiment	I-TaskName
analysis	I-TaskName
.	O
Experimental	O
results	O
demonstrate	O
that	O
the	O
proposed	O
posttraining	O
is	O
highly	O
effective	O
1	O
.	O

For	O
online	O
commerce	O
,	O
question	O
-	O
answering	O
(	O
QA	O
)	O
serves	O
either	O
as	O
a	O
standalone	O
application	O
of	O
customer	O
service	O
or	O
as	O
a	O
crucial	O
component	O
of	O
a	O
dialogue	O
system	O
that	O
answers	O
user	O
questions	O
.	O
Many	O
intelligent	O
personal	O
assistants	O
(	O
such	O
as	O
Amazon	O
Alexa	O
and	O
Google	B-DatasetName
Assistant	O
)	O
support	O
online	O
shopping	O
by	O
allowing	O
the	O
user	O
to	O
speak	O
directly	O
to	O
the	O
assistants	O
.	O
One	O
major	O
hindrance	O
for	O
this	O
mode	O
of	O
shopping	O
is	O
that	O
such	O
systems	O
have	O
limited	O
capability	O
to	O
answer	O
user	O
questions	O
about	O
products	O
(	O
or	O
services	O
)	O
,	O
which	O
are	O
vital	O
for	O
customer	O
decision	B-TaskName
making	I-TaskName
.	O
As	O
such	O
,	O
an	O
intelligent	O
agent	B-DatasetName
that	O
can	O
automatically	O
answer	O
customers	O
'	O
questions	O
is	O
very	O
important	O
for	O
the	O
success	O
of	O
online	O
businesses	O
.	O
Given	O
the	O
ever	O
-	O
changing	O
environment	O
of	O
products	O
and	O
services	O
,	O
it	O
is	O
very	O
hard	O
,	O
if	O
not	O
impossible	O
,	O
to	O
pre	O
-	O
compile	O
an	O
up	O
-	O
to	O
-	O
date	O
and	O
reliable	O
knowledge	O
base	O
to	O
cover	O
a	O
wide	O
assortment	O
of	O
questions	O
that	O
customers	O
may	O
ask	O
,	O
such	O
as	O
in	O
factoidbased	O
KB	O
-	O
QA	O
(	O
Xu	O
et	O
al	O
,	O
2016	O
;	O
Fader	O
et	O
al	O
,	O
2014	O
;	O
Kwok	O
et	O
al	O
,	O
2001	O
;	O
Yin	O
et	O
al	O
,	O
2015	O
)	O
.	O
As	O
a	O
compromise	O
,	O
many	O
online	O
businesses	O
leverage	O
community	O
question	O
-	O
answering	O
(	O
CQA	O
)	O
(	O
McAuley	O
and	O
Yang	O
,	O
2016	O
)	O
to	O
crowdsource	O
answers	O
from	O
existing	O
customers	O
.	O
However	O
,	O
the	O
problem	O
with	O
this	O
approach	O
is	O
that	O
many	O
questions	O
are	O
not	O
answered	O
,	O
and	O
if	O
they	O
are	O
answered	O
,	O
the	O
answers	O
are	O
delayed	O
,	O
which	O
is	O
not	O
suitable	O
for	O
interactive	O
QA	O
.	O
In	O
this	O
paper	O
,	O
we	O
explore	O
the	O
potential	O
of	O
using	O
product	O
reviews	O
as	O
a	O
large	O
source	O
of	O
user	O
experiences	O
that	O
can	O
be	O
exploited	O
to	O
obtain	O
answers	O
to	O
user	O
questions	O
.	O
Although	O
there	O
are	O
existing	O
studies	O
that	O
have	O
used	O
information	B-TaskName
retrieval	I-TaskName
(	O
IR	O
)	O
techniques	O
(	O
McAuley	O
and	O
Yang	O
,	O
2016	O
;	O
Yu	O
and	O
Lam	O
,	O
2018	O
)	O
to	O
find	O
a	O
whole	O
review	O
as	O
the	O
response	O
to	O
a	O
user	O
question	O
,	O
giving	O
the	O
whole	O
review	O
to	O
the	O
user	O
is	O
undesirable	O
as	O
it	O
is	O
quite	O
time	O
-	O
consuming	O
for	O
the	O
user	O
to	O
read	O
it	O
.	O
Inspired	B-DatasetName
by	O
the	O
success	O
of	O
Machine	O
Reading	O
Comphrenesions	O
(	O
MRC	O
)	O
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
(	O
Rajpurkar	O
et	O
al	O
,	O
,	O
2018	O
,	O
we	O
propose	O
a	O
novel	O
task	O
called	O
Review	O
Reading	B-TaskName
Comprehension	I-TaskName
(	O
RRC	O
)	O
as	O
following	O
.	O
Problem	O
Definition	O
:	O
Given	O
a	O
question	O
q	O
=	O
(	O
q	O
1	O
,	O
.	O
.	O
.	O
,	O
q	O
m	O
)	O
from	O
a	O
customer	O
(	O
or	O
user	O
)	O
about	O
a	O
product	O
and	O
a	O
review	O
d	O
=	O
(	O
d	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
n	O
)	O
for	O
that	O
product	O
containing	O
the	O
information	O
to	O
answer	O
q	O
,	O
find	O
a	O
sequence	O
of	O
tokens	O
(	O
a	O
text	O
span	O
)	O
a	O
=	O
(	O
d	O
s	O
,	O
.	O
.	O
.	O
,	O
d	O
e	O
)	O
in	O
d	O
that	O
answers	O
q	O
correctly	O
,	O
where	O
1	O
≤	O
s	O
≤	O
n	O
,	O
1	O
≤	O
e	O
≤	O
n	O
,	O
and	O
s	O
≤	O
e.	O
A	O
sample	O
laptop	O
review	O
is	O
shown	O
in	O
Table	O
1	O
.	O
We	O
can	O
see	O
that	O
customers	O
may	O
not	O
only	O
ask	O
factoid	O
Questions	O
Q1	O
:	O
Does	O
it	O
have	O
an	O
internal	O
hard	O
drive	O
?	O
Q2	O
:	O
How	O
large	O
is	O
the	O
internal	O
hard	O
drive	O
?	O
Q3	O
:	O
is	O
the	O
capacity	O
of	O
the	O
internal	O
hard	O
drive	O
OK	O
?	O
Review	O
Excellent	O
value	O
and	O
a	O
must	O
buy	O
for	O
someone	O
looking	O
for	O
a	O
Macbook	O
.	O
You	O
ca	O
n't	O
get	O
any	O
better	O
than	O
this	O
price	O
and	O
it	O
come	O
with	O
A1	O
an	O
internal	O
disk	O
drive	O
.	O
All	O
the	O
newer	O
MacBooks	O
do	O
not	O
.	O
Plus	O
you	O
get	O
500	O
GB	O
A2	O
which	O
is	O
also	O
a	O
great	O
A3	O
feature	O
.	O
Also	O
,	O
the	O
resale	O
value	O
on	O
this	O
will	O
keep	O
.	O
I	O
highly	O
recommend	O
you	O
get	O
one	O
before	O
they	O
are	O
gone	O
.	O
Table	O
1	O
:	O
An	O
example	O
of	O
review	O
reading	B-TaskName
comprehension	I-TaskName
:	O
we	O
show	O
3	O
questions	O
and	O
their	O
corresponding	O
answer	O
spans	O
from	O
a	O
review	O
.	O
questions	O
such	O
as	O
the	O
specs	O
about	O
some	O
aspects	O
of	O
the	O
laptop	O
as	O
in	O
the	O
first	O
and	O
second	O
questions	O
but	O
also	O
subjective	O
or	O
opinion	O
questions	O
about	O
some	O
aspects	O
(	O
capacity	O
of	O
the	O
hard	O
drive	O
)	O
,	O
as	O
in	O
the	O
third	O
question	O
.	O
RRC	O
poses	O
some	O
domain	O
challenges	O
compared	O
to	O
the	O
traditional	O
MRC	O
on	O
Wikipedia	O
,	O
such	O
as	O
the	O
need	O
for	O
rich	O
product	O
knowledge	O
,	O
informal	O
text	O
,	O
and	O
fine	O
-	O
grained	O
opinions	O
(	O
there	O
is	O
almost	O
no	O
subjective	O
content	O
in	O
Wikipedia	O
articles	O
)	O
.	O
Research	O
also	O
shows	O
that	O
yes	O
/	O
no	O
questions	O
are	O
very	O
frequent	O
for	O
products	O
with	O
complicated	O
specifications	O
(	O
McAuley	O
and	O
Yang	O
,	O
2016	O
;	O
Xu	O
et	O
al	O
,	O
2018b	O
)	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
no	O
existing	O
work	O
has	O
been	O
done	O
in	O
RRC	O
.	O
This	O
work	O
first	O
builds	O
an	O
RRC	O
dataset	O
called	O
ReviewRC	O
,	O
using	O
reviews	O
from	O
SemEval	O
2016	O
Task	O
5	O
2	O
,	O
which	O
is	O
a	O
popular	O
dataset	O
for	O
aspect	B-TaskName
-	I-TaskName
based	I-TaskName
sentiment	I-TaskName
analysis	I-TaskName
(	O
ABSA	O
)	O
(	O
Hu	O
and	O
Liu	O
,	O
2004	O
)	O
in	O
the	O
domains	O
of	O
laptop	O
and	O
restaurant	O
.	O
We	O
detail	O
ReviewRC	O
in	O
Sec	O
.	O
5	O
.	O
Given	O
the	O
wide	O
spectrum	O
of	O
domains	O
(	O
types	O
of	O
products	O
or	O
services	O
)	O
in	O
online	O
businesses	O
and	O
the	O
prohibitive	O
cost	O
of	O
annotation	O
,	O
ReviewRC	O
can	O
only	O
be	O
considered	O
to	O
have	O
a	O
limited	O
number	O
of	O
annotated	O
examples	O
for	O
supervised	O
training	O
,	O
which	O
still	O
leaves	O
the	O
domain	O
challenges	O
partially	O
unresolved	O
.	O
This	O
work	O
adopts	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
as	O
the	O
base	O
model	O
as	O
it	O
achieves	O
the	O
state	O
-	O
ofthe	O
-	O
art	O
performance	O
on	O
MRC	O
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
(	O
Rajpurkar	O
et	O
al	O
,	O
,	O
2018	O
.	O
Although	O
BERT	B-MethodName
aims	O
to	O
learn	O
contextualized	O
representations	O
across	O
a	O
wide	O
range	O
of	O
NLP	O
tasks	O
(	O
to	O
be	O
task	O
-	O
agnostic	O
)	O
,	O
leveraging	O
BERT	B-MethodName
alone	O
still	O
leaves	O
the	O
domain	O
challenges	O
un	O
-	O
2	O
http://alt.qcri.org/semeval2016/	O
task5/.	O
We	O
choose	O
these	O
review	O
datasets	O
to	O
align	O
RRC	O
with	O
existing	O
research	O
on	O
sentiment	B-TaskName
analysis	I-TaskName
.	O
resolved	O
(	O
as	O
BERT	B-MethodName
is	O
trained	O
on	O
Wikipedia	O
articles	O
and	O
has	O
almost	O
no	O
understanding	O
of	O
opinion	O
text	O
)	O
,	O
and	O
it	O
also	O
introduces	O
another	O
challenge	O
of	O
task	O
-	O
awareness	O
(	O
the	O
RRC	O
task	O
)	O
,	O
called	O
the	O
task	O
challenge	O
.	O
This	O
challenge	O
arises	O
when	O
the	O
taskagnostic	O
BERT	B-MethodName
meets	O
the	O
limited	O
number	O
of	O
finetuning	O
examples	O
in	O
ReviewRC	O
(	O
see	O
Sec	O
.	O
5	O
)	O
for	O
RRC	O
,	O
which	O
is	O
insufficient	O
to	O
fine	O
-	O
tune	O
BERT	B-MethodName
to	O
ensure	O
full	O
task	O
-	O
awareness	O
of	O
the	O
system	O
3	O
.	O
To	O
address	O
all	O
the	O
above	O
challenges	O
,	O
we	O
propose	O
a	O
novel	O
joint	O
post	O
-	O
training	O
technique	O
that	O
takes	O
BERT	B-MethodName
's	O
pre	O
-	O
trained	O
weights	O
as	O
the	O
initialization	O
4	O
for	O
basic	O
language	O
understanding	O
and	O
adapt	O
BERT	B-MethodName
with	O
both	O
domain	O
knowledge	O
and	O
task	O
(	O
MRC	O
)	O
knowledge	O
before	O
fine	O
-	O
tuning	O
using	O
the	O
domain	O
end	O
task	O
annotated	O
data	O
for	O
the	O
domain	O
RRC	O
.	O
This	O
technique	O
leverages	O
knowledge	O
from	O
two	O
sources	O
:	O
unsupervised	O
domain	O
reviews	O
and	O
supervised	O
(	O
yet	O
out	O
-	O
of	O
-	O
domain	O
)	O
MRC	O
data	O
5	O
,	O
where	O
the	O
former	O
enhances	O
domain	O
-	O
awareness	O
and	O
the	O
latter	O
strengthens	O
MRC	O
task	O
-	O
awareness	O
.	O
As	O
a	O
general	O
-	O
purpose	O
approach	O
,	O
we	O
show	O
that	O
the	O
proposed	O
method	O
can	O
also	O
benefit	O
ABSA	O
tasks	O
such	O
as	O
aspect	B-TaskName
extraction	I-TaskName
(	O
AE	B-MethodName
)	O
and	O
aspect	O
sentiment	O
classification	O
(	O
ASC	O
)	O
.	O
The	O
main	O
contributions	O
of	O
this	O
paper	O
are	O
as	O
follows	O
.	O
(	O
1	O
)	O
It	O
proposes	O
the	O
new	O
problem	O
of	O
review	O
reading	B-TaskName
comprehension	I-TaskName
(	O
RRC	O
)	O
.	O
(	O
2	O
)	O
To	O
solve	O
this	O
new	O
problem	O
,	O
an	O
annotated	O
dataset	O
for	O
RRC	O
is	O
created	O
.	O
(	O
3	O
)	O
It	O
proposes	O
a	O
general	O
-	O
purpose	O
posttraining	O
approach	O
to	O
improve	O
RRC	O
,	O
AE	B-MethodName
,	O
and	O
ASC	O
.	O
Experimental	O
results	O
demonstrate	O
that	O
the	O
proposed	O
approach	O
is	O
effective	O
.	O

Many	O
datasets	O
have	O
been	O
created	O
for	O
MRC	O
from	O
formally	O
written	O
and	O
objective	O
texts	O
,	O
e.g.	O
,	O
Wikipedia	O
(	O
WikiReading	B-DatasetName
(	O
Hewlett	O
et	O
al	O
,	O
2016	O
)	O
,	O
SQuAD	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
(	O
Rajpurkar	O
et	O
al	O
,	O
,	O
2018	O
,	O
Wiki	O
-	O
Hop	O
(	O
Welbl	O
et	O
al	O
,	O
2018	O
)	O
,	O
DRCD	B-DatasetName
(	O
Shao	O
et	O
al	O
,	O
2018	O
)	O
,	O
QuAC	B-DatasetName
(	O
Choi	O
et	O
al	O
,	O
2018	O
)	O
,	O
HotpotQA	B-DatasetName
)	O
news	O
and	O
other	O
articles	O
(	O
CNN	B-DatasetName
/	I-DatasetName
Daily	I-DatasetName
Mail	I-DatasetName
(	O
Hermann	O
et	O
al	O
,	O
2015	O
)	O
,	O
NewsQA	B-DatasetName
(	O
Trischler	O
et	O
al	O
,	O
2016	O
)	O
,	O
RACE	B-DatasetName
(	O
Lai	O
et	O
al	O
,	O
2017	O
)	O
)	O
,	O
fictional	O
stories	O
(	O
MCTest	B-DatasetName
(	O
Richardson	O
et	O
al	O
,	O
2013	O
)	O
,	O
CBT	B-DatasetName
(	O
Hill	O
et	O
al	O
,	O
2015	O
)	O
,	O
NarrativeQA	B-DatasetName
(	O
Kočiskỳ	O
et	O
al	O
,	O
2018	O
)	O
)	O
,	O
and	O
general	O
Web	O
documents	O
(	O
MS	B-DatasetName
MARCO	I-DatasetName
(	O
Nguyen	O
et	O
al	O
,	O
2016	O
)	O
,	O
TriviaQA	B-DatasetName
(	O
Joshi	O
et	O
al	O
,	O
2017	O
)	O
,	O
SearchQA	B-DatasetName
(	O
Dunn	O
et	O
al	O
,	O
2017	O
)	O
)	O
.	O
Also	O
,	O
CoQA	B-DatasetName
(	O
Reddy	O
et	O
al	O
,	O
2018	O
)	O
is	O
built	O
from	O
multiple	O
sources	O
,	O
such	O
as	O
Wikipedia	O
,	O
Reddit	B-DatasetName
,	O
News	O
,	O
Mid	O
/	O
High	O
School	O
Exams	O
,	O
Literature	O
,	O
etc	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
MRC	O
has	O
not	O
been	O
used	O
on	O
reviews	O
,	O
which	O
are	O
primarily	O
subjective	O
.	O
As	O
such	O
,	O
we	O
created	O
a	O
review	O
-	O
based	O
MRC	O
dataset	O
called	O
Re	O
-	O
viewRC	O
.	O
Answers	O
from	O
ReviewRC	O
are	O
extractive	O
(	O
similar	O
to	O
SQuAD	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
(	O
Rajpurkar	O
et	O
al	O
,	O
,	O
2018	O
)	O
rather	O
than	O
abstractive	O
(	O
or	O
generative	O
)	O
(	O
such	O
as	O
in	O
MS	B-DatasetName
MARCO	I-DatasetName
(	O
Nguyen	O
et	O
al	O
,	O
2016	O
)	O
and	O
CoQA	B-DatasetName
(	O
Reddy	O
et	O
al	O
,	O
2018	O
)	O
)	O
.	O
This	O
is	O
crucial	O
because	O
online	O
businesses	O
are	O
typically	O
cost	O
-	O
sensitive	O
and	O
extractive	O
answers	O
written	O
by	O
humans	O
can	O
avoid	O
generating	O
incorrect	O
answers	O
beyond	O
the	O
contents	O
in	O
reviews	O
by	O
an	O
AI	O
agent	B-DatasetName
.	O
Community	O
QA	O
(	O
CQA	O
)	O
is	O
widely	O
adopted	O
by	O
online	O
businesses	O
(	O
McAuley	O
and	O
Yang	O
,	O
2016	O
)	O
to	O
help	O
users	O
.	O
However	O
,	O
since	O
it	O
solely	O
relies	O
on	O
humans	O
to	O
give	O
answers	O
,	O
it	O
often	O
takes	O
a	O
long	O
time	O
to	O
get	O
a	O
question	O
answered	O
or	O
even	O
not	O
answered	O
at	O
all	O
as	O
we	O
discussed	O
in	O
the	O
introduction	O
.	O
Although	O
there	O
exist	O
researches	O
that	O
align	O
reviews	O
to	O
questions	O
as	O
an	O
information	B-TaskName
retrieval	I-TaskName
task	O
(	O
McAuley	O
and	O
Yang	O
,	O
2016	O
;	O
Yu	O
and	O
Lam	O
,	O
2018	O
)	O
,	O
giving	O
a	O
whole	O
review	O
to	O
the	O
user	O
to	O
read	O
is	O
time	O
-	O
consuming	O
and	O
not	O
suitable	O
for	O
customer	O
service	O
settings	O
that	O
require	O
interactive	O
responses	O
.	O
Knowledge	O
bases	O
(	O
KBs	O
)	O
(	O
such	O
as	O
Freebase	O
(	O
Dong	O
et	O
al	O
,	O
2015	O
;	O
Xu	O
et	O
al	O
,	O
2016	O
;	O
Yao	O
and	O
Van	O
Durme	O
,	O
2014	O
)	O
or	O
DBpedia	B-DatasetName
(	O
Lopez	O
et	O
al	O
,	O
2010	O
;	O
Unger	O
et	O
al	O
,	O
2012	O
)	O
)	O
have	O
been	O
used	O
for	O
question	B-TaskName
answering	I-TaskName
(	O
Yu	O
and	O
Lam	O
,	O
2018	O
)	O
.	O
However	O
,	O
the	O
ever	O
-	O
changing	O
nature	O
of	O
online	O
businesses	O
,	O
where	O
new	O
products	O
and	O
services	O
appear	O
constantly	O
,	O
makes	O
it	O
prohibitive	O
to	O
build	O
a	O
highquality	O
KB	O
to	O
cover	O
all	O
new	O
products	O
and	O
services	O
.	O
Reviews	O
also	O
serve	O
as	O
a	O
rich	O
resource	O
for	O
sentiment	B-TaskName
analysis	I-TaskName
(	O
Pang	O
et	O
al	O
,	O
2002	O
;	O
Hu	O
and	O
Liu	O
,	O
2004	O
;	O
Liu	O
,	O
2012Liu	O
,	O
,	O
2015	O
.	O
Although	O
documentlevel	O
(	O
review	O
)	O
sentiment	O
classification	O
may	O
be	O
considered	O
as	O
a	O
solved	O
problem	O
(	O
given	O
ratings	O
are	O
largely	O
available	O
)	O
,	O
aspect	B-TaskName
-	I-TaskName
based	I-TaskName
sentiment	I-TaskName
analysis	I-TaskName
(	O
ABSA	O
)	O
is	O
still	O
an	O
open	O
challenge	O
,	O
where	O
alleviating	O
the	O
cost	O
of	O
human	O
annotation	O
is	O
also	O
a	O
major	O
issue	O
.	O
ABSA	O
aims	O
to	O
turn	O
unstructured	O
reviews	O
into	O
structured	O
fine	O
-	O
grained	O
aspects	O
(	O
such	O
as	O
the	O
"	O
battery	O
"	O
of	O
a	O
laptop	O
)	O
and	O
their	O
associated	O
opinions	O
(	O
e.g.	O
,	O
"	O
good	O
battery	O
"	O
is	O
positive	O
about	O
the	O
aspect	O
battery	O
)	O
.	O
Two	O
important	O
tasks	O
in	O
ABSA	O
are	O
aspect	B-TaskName
extraction	I-TaskName
(	O
AE	B-MethodName
)	O
and	O
aspect	O
sentiment	O
classification	O
(	O
ASC	O
)	O
(	O
Hu	O
and	O
Liu	O
,	O
2004	O
)	O
,	O
where	O
the	O
former	O
aims	O
to	O
extract	O
aspects	O
(	O
e.g.	O
,	O
"	O
battery	O
"	O
)	O
and	O
the	O
latter	O
targets	O
to	O
identify	O
the	O
polarity	O
for	O
a	O
given	O
aspect	O
(	O
e.g.	O
,	O
positive	O
for	O
battery	O
)	O
.	O
Recently	O
,	O
supervised	O
deep	O
learning	O
models	O
dominate	O
both	O
tasks	O
(	O
Wang	O
et	O
al	O
,	O
,	O
2017Xu	O
et	O
al	O
,	O
2018a	O
;	O
Tang	O
et	O
al	O
,	O
2016	O
;	O
and	O
many	O
of	O
these	O
models	O
use	O
handcrafted	O
features	O
,	O
lexicons	O
,	O
and	O
complicated	O
neural	O
network	O
architectures	O
to	O
remedy	O
the	O
insufficient	O
training	O
examples	O
from	O
both	O
tasks	O
.	O
Although	O
these	O
approaches	O
may	O
achieve	O
better	O
performances	O
by	O
manually	O
injecting	O
human	O
knowledge	O
into	O
the	O
model	O
,	O
human	O
baby	O
-	O
sat	O
models	O
may	O
not	O
be	O
intelligent	O
enough	O
6	O
and	O
automated	O
representation	B-TaskName
learning	I-TaskName
from	O
review	O
corpora	O
is	O
always	O
preferred	O
(	O
Xu	O
et	O
al	O
,	O
2018a	O
;	O
.	O
We	O
push	O
forward	O
this	O
trend	O
with	O
the	O
recent	O
advance	O
in	O
pre	O
-	O
trained	O
language	O
models	O
from	O
deep	O
learning	O
(	O
Peters	O
et	O
al	O
,	O
2018	O
;	O
Howard	O
and	O
Ruder	O
,	O
2018	O
;	O
Devlin	O
et	O
al	O
,	O
2018	O
;	O
Radford	O
et	O
al	O
,	O
2018a	O
,	O
b	O
)	O
.	O
Although	O
it	O
is	O
practical	O
to	O
train	O
domain	O
word	B-TaskName
embeddings	I-TaskName
from	O
scratch	O
on	O
large	O
-	O
scale	O
review	O
corpora	O
(	O
Xu	O
et	O
al	O
,	O
2018a	O
)	O
,	O
it	O
is	O
impractical	O
to	O
train	O
language	O
models	O
from	O
scratch	O
with	O
limited	O
computational	O
resources	O
.	O
As	O
such	O
,	O
we	O
show	O
that	O
it	O
is	O
practical	O
to	O
adapt	O
language	O
models	O
pre	O
-	O
trained	O
from	O
formal	O
texts	O
to	O
domain	O
reviews	O
.	O

In	O
this	O
section	O
,	O
we	O
briefly	O
review	O
BERT	B-MethodName
and	O
derive	O
its	O
fine	O
-	O
tuning	O
formulation	O
on	O
three	O
(	O
3	O
)	O
reviewbased	O
end	O
tasks	O
.	O

As	O
a	O
subsequent	O
task	O
of	O
AE	B-MethodName
,	O
aspect	O
sentiment	O
classification	O
(	O
ASC	O
)	O
aims	O
to	O
classify	O
the	O
sentiment	O
polarity	O
(	O
positive	O
,	O
negative	O
,	O
or	O
neutral	O
)	O
expressed	O
on	O
an	O
aspect	O
extracted	O
from	O
a	O
review	O
sentence	O
.	O
There	O
are	O
two	O
inputs	O
to	O
ASC	O
:	O
an	O
aspect	O
and	O
a	O
review	O
sentence	O
mentioning	O
that	O
aspect	O
.	O
Consequently	O
,	O
ASC	O
is	O
close	O
to	O
RRC	O
as	O
the	O
question	O
is	O
just	O
about	O
an	O
aspect	O
and	O
the	O
review	O
is	O
just	O
a	O
review	O
sentence	O
but	O
ASC	O
only	O
needs	O
to	O
output	O
a	O
class	O
of	O
polarity	O
instead	O
of	O
a	O
textual	O
span	O
.	O
Let	O
As	O
a	O
summary	O
of	O
these	O
tasks	O
,	O
insufficient	O
supervised	O
training	O
data	O
significantly	O
limits	O
the	O
performance	O
gain	O
across	O
these	O
3	O
review	O
-	O
based	O
tasks	O
.	O
Al	O
-	O
though	O
BERT	B-MethodName
's	O
pre	O
-	O
trained	O
weights	O
strongly	O
boost	O
the	O
performance	O
of	O
many	O
other	O
NLP	O
tasks	O
on	O
formal	O
texts	O
,	O
we	O
observe	O
in	O
Sec	O
.	O
5	O
that	O
BERT	B-MethodName
's	O
weights	O
only	O
result	O
in	O
limited	O
gain	O
or	O
worse	O
performance	O
compared	O
with	O
existing	O
baselines	O
.	O
In	O
the	O
next	O
section	O
,	O
we	O
introduce	O
the	O
post	O
-	O
training	O
step	O
to	O
boost	O
the	O
performance	O
of	O
all	O
these	O
3	O
tasks	O
.	O
x	O
=	O
(	O
[	O
CLS	O
]	O
,	O
q	O
1	O
,	O
.	O
.	O
.	O
,	O
q	O
m	O
,	O
[	O
SEP	O
]	O
,	O
d	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
n	O
,	O
[	O
SEP	O
]	O
)	O
,	O

As	O
there	O
are	O
no	O
existing	O
datasets	O
for	O
RRC	O
and	O
to	O
be	O
consistent	O
with	O
existing	O
research	O
on	O
sentiment	B-TaskName
analysis	I-TaskName
,	O
we	O
adopt	O
the	O
laptop	O
and	O
restaurant	O
reviews	O
of	O
SemEval	O
2016	O
Task	O
5	O
as	O
the	O
source	O
to	O
create	O
datasets	O
for	O
RRC	O
.	O
We	O
do	O
not	O
use	O
SemEval	O
2014	O
Task	O
4	O
or	O
SemEval	O
2015	O
Task	O
12	O
because	O
these	O
datasets	O
do	O
not	O
come	O
with	O
the	O
review	O
(	O
document	O
)	O
level	O
XML	O
tags	O
to	O
recover	O
whole	O
reviews	O
from	O
review	O
sentences	O
.	O
We	O
keep	O
the	O
split	O
of	O
training	O
and	O
testing	O
of	O
the	O
SemEval	O
2016	O
Task	O
5	O
datasets	O
and	O
annotate	O
multiple	O
QAs	O
for	O
each	O
review	O
following	O
the	O
way	O
of	O
constructing	O
QAs	O
for	O
the	O
SQuAD	B-DatasetName
1.1	O
datasets	O
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
.	O
To	O
make	O
sure	O
our	O
questions	O
are	O
close	O
to	O
realworld	O
questions	O
,	O
2	O
annotators	O
are	O
first	O
exposed	O
to	O
400	O
QAs	O
from	O
CQA	O
(	O
under	O
the	O
laptop	O
category	O
in	O
Amazon.com	O
or	O
popular	O
restaurants	O
in	O
Yelp.com	O
)	O
to	O
get	O
familiar	O
with	O
real	O
questions	O
.	O
Then	O
they	O
are	O
asked	O
to	O
read	O
reviews	O
and	O
independently	O
label	O
textual	O
spans	O
and	O
ask	O
corresponding	O
questions	O
when	O
they	O
feel	O
the	O
textual	O
spans	O
contain	O
valuable	O
information	O
that	O
customers	O
may	O
care	O
about	O
.	O
The	O
textual	O
spans	O
are	O
labeled	O
to	O
be	O
as	O
concise	O
as	O
possible	O
but	O
still	O
human	O
-	O
readable	O
.	O
Note	O
that	O
the	O
annotations	O
for	O
sentiment	B-TaskName
analysis	I-TaskName
tasks	O
are	O
not	O
exposed	O
to	O
annotators	O
to	O
avoid	O
biased	O
annotation	O
on	O
RRC	O
.	O
Since	O
it	O
is	O
unlikely	O
that	O
the	O
two	O
annotators	O
can	O
label	O
the	O
same	O
QAs	O
(	O
the	O
same	O
questions	O
with	O
the	O
same	O
answer	O
spans	O
)	O
,	O
they	O
further	O
mutually	O
check	O
each	O
other	O
's	O
annotations	O
and	O
disagreements	O
are	O
discussed	O
until	O
agreements	O
are	O
reached	O
.	O
Annotators	O
are	O
encouraged	O
to	O
label	O
as	O
many	O
questions	O
as	O
possible	O
from	O
testing	O
reviews	O
to	O
get	O
more	O
test	O
examples	O
.	O
A	O
training	O
review	O
is	O
encouraged	O
to	O
have	O
2	O
questions	O
(	O
training	O
examples	O
)	O
on	O
average	O
to	O
have	O
good	O
coverage	O
of	O
reviews	O
.	O
The	O
annotated	O
data	O
is	O
in	O
the	O
format	O
of	O
SQuAD	B-DatasetName
1.1	O
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
to	O
ensure	O
compatibility	O
with	O
existing	O
implementations	O
of	O
MRC	O
models	O
.	O
The	O
statistics	O
of	O
the	O
RRC	O
dataset	O
(	O
ReviewRC	O
)	O
are	O
shown	O
in	O
Table	O
2	O
.	O
Since	O
SemEval	O
datasets	O
do	O
not	O
come	O
with	O
a	O
validation	O
set	O
,	O
we	O
further	O
split	O
20	O
%	O
of	O
reviews	O
from	O
the	O
training	O
set	O
for	O
validation	O
.	O
Statistics	O
of	O
datasets	O
for	O
AE	B-MethodName
and	O
ASC	O
are	O
given	O
in	O
Table	O
3	O
.	O
For	O
AE	B-MethodName
,	O
we	O
choose	O
SemEval	O
2014	O
Task	O
4	O
for	O
laptop	O
and	O
SemEval	O
-	O
2016	O
Task	O
5	O
for	O
restaurant	O
to	O
be	O
consistent	O
with	O
(	O
Xu	O
et	O
al	O
,	O
2018a	O
)	O
and	O
other	O
previous	O
works	O
.	O
For	O
ASC	O
,	O
we	O
use	O
SemEval	O
2014	O
Task	O
4	O
for	O
both	O
laptop	O
and	O
restaurant	O
as	O
existing	O
research	O
frequently	O
uses	O
this	O
version	O
.	O
We	O
use	O
150	O
examples	O
from	O
the	O
training	O
set	O
of	O
all	O
these	O
datasets	O
for	O
validation	O
.	O

For	O
domain	O
knowledge	O
post	O
-	O
training	O
,	O
we	O
use	O
Amazon	O
laptop	O
reviews	O
(	O
He	O
and	O
McAuley	O
,	O
2016	O
)	O
and	O
Yelp	O
Dataset	O
Challenge	O
reviews	O
8	O
.	O
For	O
laptop	O
,	O
we	O
filtered	O
out	O
reviewed	O
products	O
that	O
have	O
appeared	O
in	O
the	O
validation	O
/	O
test	O
reviews	O
to	O
avoid	O
training	O
bias	O
for	O
test	O
data	O
(	O
Yelp	O
reviews	O
do	O
not	O
have	O
this	O
issue	O
as	O
the	O
source	O
reviews	O
of	O
SemEval	O
are	O
not	O
from	O
Yelp	O
)	O
.	O
Since	O
the	O
number	O
of	O
reviews	O
is	O
small	O
,	O
we	O
choose	O
a	O
duplicate	O
factor	O
of	O
5	O
(	O
each	O
review	O
For	O
the	O
restaurant	O
domain	O
,	O
we	O
use	O
Yelp	O
reviews	O
from	O
restaurant	O
categories	O
that	O
the	O
SemEval	O
reviews	O
also	O
belong	O
to	O
(	O
Xu	O
et	O
al	O
,	O
2018a	O
)	O
.	O
We	O
choose	O
700	O
K	O
reviews	O
to	O
ensure	O
it	O
is	O
large	O
enough	O
to	O
generate	O
training	O
examples	O
(	O
with	O
a	O
duplicate	O
factor	O
of	O
1	O
)	O
to	O
cover	O
all	O
post	O
-	O
training	O
steps	O
that	O
we	O
can	O
afford	O
(	O
discussed	O
in	O
Section	O
5.3	O
)	O
9	O
.	O
This	O
gives	O
us	O
2	O
,	O
677	O
,	O
025	O
post	O
-	O
training	O
examples	O
for	O
restaurant	O
domain	O
knowledge	O
learning	O
.	O
For	O
MRC	O
task	O
-	O
awareness	O
post	O
-	O
training	O
,	O
we	O
leverage	O
SQuAD	B-DatasetName
1.1	O
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
that	O
come	O
with	O
87	O
,	O
599	O
training	O
examples	O
from	O
442	O
Wikipedia	O
articles	O
.	O

The	O
results	O
of	O
RRC	O
,	O
AE	B-MethodName
and	O
ASC	O
are	O
shown	O
in	O
Tables	O
4	O
,	O
5	O
and	O
6	O
,	O
respectively	O
.	O
To	O
answer	O
RQ1	O
,	O
we	O
observed	O
that	O
the	O
proposed	O
joint	O
post	O
-	O
training	O
(	O
BERT	B-MethodName
-	O
PT	O
)	O
has	O
the	O
best	O
performance	O
over	O
all	O
tasks	O
in	O
all	O
domains	O
,	O
which	O
show	O
the	O
benefits	O
of	O
having	O
two	O
types	O
of	O
knowledge	O
.	O
To	O
answer	O
RQ2	O
,	O
to	O
our	O
surprise	O
we	O
found	O
that	O
the	O
vanilla	O
pre	O
-	O
trained	O
weights	O
of	O
BERT	B-MethodName
do	O
not	O
work	O
well	O
for	O
review	O
-	O
based	O
tasks	O
,	O
although	O
it	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
many	O
other	O
NLP	O
tasks	O
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
.	O
This	O
justifies	O
the	O
need	O
to	O
adapt	O
BERT	B-MethodName
to	O
review	O
-	O
based	O
tasks	O
.	O
To	O
answer	O
RQ3	O
,	O
we	O
noticed	O
that	O
the	O
roles	O
of	O
domain	O
knowledge	O
and	O
task	O
knowledge	O
vary	O
for	O
different	O
tasks	O
and	O
domains	O
.	O
For	O
RRC	O
,	O
we	O
found	O
that	O
the	O
performance	O
gain	O
of	O
BERT	B-MethodName
-	O
PT	O
mostly	O
comes	O
from	O
task	O
-	O
awareness	O
(	O
MRC	O
)	O
post	O
-	O
training	O
(	O
as	O
indicated	O
by	O
BERT	B-MethodName
-	O
MRC	O
)	O
.	O
The	O
domain	O
knowledge	O
helps	O
more	O
for	O
restaurant	O
than	O
for	O
laptop	O
.	O
We	O
suspect	O
the	O
reason	O
is	O
that	O
certain	O
types	O
of	O
knowledge	O
(	O
such	O
as	O
specifications	O
)	O
of	O
laptop	O
are	O
already	O
present	O
in	O
Wikipedia	O
,	O
whereas	O
Wikipedia	O
has	O
little	O
knowledge	O
about	O
restaurant	O
.	O
We	O
further	O
investigated	O
the	O
examples	O
improved	O
by	O
BERT	B-MethodName
-	O
MRC	O
and	O
found	O
that	O
the	O
boundaries	O
of	O
spans	O
(	O
especially	O
short	O
spans	O
)	O
were	O
greatly	O
improved	O
.	O
For	O
AE	B-MethodName
,	O
we	O
found	O
that	O
great	O
performance	O
boost	O
comes	O
mostly	O
from	O
domain	O
knowledge	O
posttraining	O
,	O
which	O
indicates	O
that	O
contextualized	O
representations	O
of	O
domain	O
knowledge	O
are	O
very	O
important	O
for	O
AE	B-MethodName
.	O
BERT	B-MethodName
-	O
MRC	O
has	O
almost	O
no	O
improvement	O
on	O
restaurant	O
,	O
which	O
indicates	O
Wikipedia	O
may	O
have	O
no	O
knowledge	O
about	O
aspects	O
of	O
restaurant	O
.	O
We	O
suspect	O
that	O
the	O
improvements	O
on	O
laptop	O
come	O
from	O
the	O
fact	O
that	O
many	O
answer	O
spans	O
in	O
SQuAD	B-DatasetName
are	O
noun	O
terms	O
,	O
which	O
bear	O
a	O
closer	O
relationship	O
with	O
laptop	O
aspects	O
.	O
For	O
ASC	O
,	O
we	O
observed	O
that	O
large	O
-	O
scale	O
annotated	O
MRC	O
data	O
is	O
very	O
useful	O
.	O
We	O
suspect	O
the	O
reason	O
is	O
that	O
ASC	O
can	O
be	O
interpreted	O
as	O
a	O
special	O
MRC	O
problem	O
,	O
where	O
all	O
questions	O
are	O
about	O
the	O
polarity	O
of	O
a	O
given	O
aspect	O
.	O
MRC	O
training	O
data	O
may	O
help	O
BERT	B-MethodName
to	O
understand	O
the	O
input	O
format	O
of	O
ASC	O
given	O
their	O
closer	O
input	O
formulation	O
.	O
Again	O
,	O
domain	O
knowledge	O
post	O
-	O
training	O
also	O
helps	O
ASC	O
.	O
We	O
further	O
investigated	O
the	O
errors	O
from	O
BERT	B-MethodName
-	O
PT	O
over	O
the	O
3	O
tasks	O
.	O
The	O
errors	O
on	O
RRC	O
mainly	O
come	O
from	O
boundaries	O
of	O
spans	O
that	O
are	O
not	O
concise	O
enough	O
and	O
incorrect	O
location	O
of	O
spans	O
that	O
may	O
have	O
certain	O
nearby	O
words	O
related	O
to	O
the	O
question	O
.	O
We	O
believe	O
precisely	O
understanding	O
user	O
's	O
experience	O
is	O
challenging	O
from	O
only	O
domain	O
posttraining	O
given	O
limited	O
help	O
from	O
the	O
RRC	O
data	O
and	O
no	O
help	O
from	O
the	O
Wikipedia	O
data	O
.	O
For	O
AE	B-MethodName
,	O
errors	O
mostly	O
come	O
from	O
annotation	O
inconsistency	O
and	O
boundaries	O
of	O
aspects	O
(	O
e.g.	O
,	O
apple	O
OS	O
is	O
predicted	O
as	O
OS	O
)	O
.	O
Restaurant	O
suffers	O
from	O
rare	O
aspects	O
like	O
the	O
names	O
of	O
dishes	O
.	O
ASC	O
tends	O
to	O
have	O
more	O
errors	O
as	O
the	O
decision	O
boundary	O
between	O
the	O
negative	O
and	O
neutral	O
examples	O
is	O
unclear	O
(	O
e.g.	O
,	O
even	O
annotators	O
may	O
not	O
sure	O
whether	O
the	O
reviewer	O
shows	O
no	O
opinion	O
or	O
slight	O
negative	O
opinion	O
when	O
mentioning	O
an	O
aspect	O
)	O
.	O
Also	O
,	O
BERT	B-MethodName
-	O
PT	O
has	O
the	O
problem	O
of	O
dealing	O
with	O
one	O
sentence	O
with	O
two	O
opposite	O
opinions	O
(	O
"	O
The	O
screen	O
is	O
good	O
but	O
not	O
for	O
windows	O
.	O
"	O
)	O
.	O
We	O
believe	O
that	O
such	O
training	O
examples	O
are	O
rare	O
.	O

We	O
proposed	O
a	O
new	O
task	O
called	O
review	O
reading	B-TaskName
comprehension	I-TaskName
(	O
RRC	O
)	O
and	O
investigated	O
the	O
possibility	O
of	O
turning	O
reviews	O
as	O
a	O
valuable	O
resource	O
for	O
answering	O
user	O
questions	O
.	O
We	O
adopted	O
BERT	B-MethodName
as	O
our	O
base	O
model	O
and	O
proposed	O
a	O
joint	O
post	O
-	O
training	O
approach	O
to	O
enhancing	O
both	O
the	O
domain	O
and	O
task	O
knowledge	O
.	O
We	O
further	O
explored	O
the	O
use	O
of	O
this	O
approach	O
in	O
two	O
other	O
review	O
-	O
based	O
tasks	O
:	O
aspect	B-TaskName
extraction	I-TaskName
and	O
aspect	O
sentiment	O
classification	O
.	O
Experimental	O
results	O
show	O
that	O
the	O
post	O
-	O
training	O
approach	O
before	O
fine	O
-	O
tuning	O
is	O
effective	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
framework	O
for	O
sarcasm	O
generation	O
;	O
the	O
system	O
takes	O
a	O
literal	O
negative	O
opinion	O
as	O
input	O
and	O
translates	O
it	O
into	O
a	O
sarcastic	O
version	O
.	O
Our	O
framework	O
does	O
not	O
require	O
any	O
paired	O
data	O
for	O
training	O
.	O
Sarcasm	O
emanates	O
from	O
context	O
-	O
incongruity	O
which	O
becomes	O
apparent	O
as	O
the	O
sentence	O
unfolds	O
.	O
Our	O
framework	O
introduces	O
incongruity	O
into	O
the	O
literal	O
input	O
version	O
through	O
modules	O
that	O
:	O
(	O
a	O
)	O
filter	O
factual	O
content	O
from	O
the	O
input	O
opinion	O
,	O
(	O
b	O
)	O
retrieve	O
incongruous	O
phrases	O
related	O
to	O
the	O
filtered	O
facts	O
and	O
(	O
c	O
)	O
synthesize	O
sarcastic	O
text	O
from	O
the	O
filtered	O
and	O
incongruous	O
phrases	O
.	O
The	O
framework	O
employs	O
reinforced	O
neural	O
sequence	B-MethodName
to	I-MethodName
sequence	I-MethodName
learning	O
and	O
information	B-TaskName
retrieval	I-TaskName
and	O
is	O
trained	O
only	O
using	O
unlabeled	O
non	O
-	O
sarcastic	O
and	O
sarcastic	O
opinions	O
.	O
Since	O
no	O
labeled	O
dataset	O
exists	O
for	O
such	O
a	O
task	O
,	O
for	O
evaluation	O
,	O
we	O
manually	O
prepare	O
a	O
benchmark	O
dataset	O
containing	O
literal	O
opinions	O
and	O
their	O
sarcastic	O
paraphrases	O
.	O
Qualitative	O
and	O
quantitative	O
performance	O
analyses	O
on	O
the	O
data	O
reveal	O
our	O
system	O
's	O
superiority	O
over	O
baselines	O
,	O
built	O
using	O
known	O
unsupervised	O
statistical	O
and	O
neural	O
machine	B-TaskName
translation	I-TaskName
and	O
style	B-TaskName
transfer	I-TaskName
techniques	O
.	O

Sarcasm	O
1	O
is	O
an	O
intensive	O
,	O
ironic	O
construct	O
that	O
is	O
intended	O
to	O
express	O
contempt	O
or	O
ridicule	O
.	O
It	O
is	O
often	O
linked	O
with	O
intelligence	O
,	O
creativity	O
,	O
and	O
wit	O
,	O
and	O
therefore	O
empowering	O
machines	O
to	O
generate	O
sarcasm	O
is	O
in	O
line	O
with	O
the	O
key	O
goals	O
of	O
Strong	O
AI	O
2	O
.	O
From	O
the	O
perspective	O
of	O
Natural	O
Language	O
Generation	O
(	O
NLG	O
)	O
,	O
sarcasm	O
generation	O
remains	O
an	O
important	O
problem	O
and	O
can	O
prove	O
useful	O
in	O
downstream	O
applications	O
such	O
as	O
conversation	O
systems	O
,	O
recommenders	O
,	O
and	O
online	O
content	O
generators	O
.	O
For	O
instance	O
,	O
in	O
a	O
conversational	O
setting	O
,	O
a	O
more	O
natural	O
and	O
intriguing	O
form	O
of	O
conversation	O
between	O
humans	O
and	O
machines	O
could	O
happen	O
if	O
machines	O
can	O
intermittently	O
generate	O
sarcastic	O
responses	O
,	O
like	O
their	O
human	O
counterparts	O
.	O
Over	O
the	O
years	O
,	O
a	O
lot	O
of	O
research	O
and	O
development	O
efforts	O
have	O
gone	O
into	O
the	O
problem	O
of	O
detecting	O
sarcasm	O
in	O
text	O
,	O
which	O
aims	O
to	O
classify	O
whether	O
a	O
given	O
text	O
contains	O
sarcasm	O
or	O
not	O
(	O
Joshi	O
et	O
al	O
(	O
2017b	O
)	O
provide	O
an	O
overview	O
)	O
.	O
However	O
,	O
systems	O
for	O
generation	O
of	O
sarcasm	O
have	O
been	O
elusive	O
.	O
This	O
is	O
probably	O
due	O
to	O
the	O
fact	O
that	O
in	O
sarcasm	O
generation	O
both	O
selection	O
of	O
contents	O
for	O
sarcastic	O
opinion	O
generation	O
and	O
surface	O
realization	O
of	O
contents	O
in	O
natural	O
language	O
form	O
are	O
highly	O
nuanced	O
.	O
In	O
the	O
broader	O
area	O
of	O
style	O
transformation	O
of	O
texts	O
,	O
most	O
of	O
the	O
existing	O
works	O
have	O
focused	O
narrowly	O
on	O
transformations	O
at	O
lexical	O
and	O
syntax	O
levels	O
,	O
i.e.	O
,	O
text	B-TaskName
simplification	I-TaskName
(	O
Siddharthan	O
,	O
2014	O
)	O
,	O
text	O
formalization	O
(	O
Jain	O
et	O
al	O
,	O
2018	O
)	O
,	O
sentiment	O
style	B-TaskName
transfer	I-TaskName
(	O
Shen	O
et	O
al	O
,	O
2017	O
;	O
Xu	O
et	O
al	O
,	O
2018	O
)	O
,	O
sentiment	O
flipping	O
and	O
understanding	O
humor	O
(	O
West	O
and	O
Horvitz	O
,	O
2019	O
)	O
.	O
However	O
,	O
very	O
little	O
work	O
has	O
been	O
done	O
(	O
(	O
Piwek	O
,	O
2003	O
)	O
,	O
(	O
Hovy	O
,	O
1987	O
)	O
)	O
on	O
incorporating	O
pragmatics	O
into	O
generation	O
tasks	O
such	O
as	O
sarcasm	O
.	O
Sarcasm	O
generation	O
offers	O
a	O
rich	O
playground	O
to	O
study	O
this	O
challenge	O
and	O
push	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
text	O
transformation	O
.	O
Moreover	O
,	O
being	O
a	O
pragmatic	O
task	O
,	O
sarcasm	O
construction	O
offers	O
diverse	O
ways	O
to	O
convey	O
the	O
same	O
intent	O
,	O
based	O
on	O
cultural	O
,	O
social	O
and	O
demographic	O
backgrounds	O
.	O
Hence	O
,	O
a	O
supervised	O
treatment	O
of	O
sarcasm	O
generation	O
using	O
paired	O
labeled	O
data	O
(	O
such	O
as	O
parallel	O
sentences	O
)	O
will	O
be	O
highly	O
restrictive	O
.	O
This	O
further	O
motivates	O
the	O
need	O
for	O
exploring	O
unsupervised	O
approaches	O
as	O
the	O
one	O
we	O
propose	O
in	O
this	O
paper	O
.	O
We	O
make	O
the	O
first	O
attempt	O
towards	O
automatic	O
sarcasm	O
generation	O
where	O
the	O
generation	O
is	O
conditioned	O
on	O
a	O
literal	O
input	O
sentence	O
.	O
For	O
example	O
,	O
the	O
literal	O
opinion	O
"	O
I	O
hate	O
it	O
when	O
my	O
bus	O
is	O
late	O
.	O
"	O
should	O
be	O
transformed	O
into	O
"	O
Absolutely	O
love	O
waiting	O
for	O
the	O
bus	O
"	O
.	O
As	O
sarcasm	O
conveys	O
a	O
negative	O
sentiment	O
,	O
our	O
system	O
expects	O
a	O
negative	O
sentiment	O
opinion	O
as	O
input	O
.	O
Out	O
of	O
various	O
possible	O
theories	O
proposed	O
to	O
explain	O
the	O
phenomenon	O
of	O
sarcasm	O
construction	O
(	O
Joshi	O
et	O
al	O
,	O
2017b	O
)	O
,	O
our	O
framework	O
relies	O
on	O
the	O
theory	O
of	O
context	O
incongruity	O
(	O
Campbell	O
and	O
Katz	O
,	O
2012	O
)	O
.	O
Context	O
incongruity	O
is	O
prevalent	O
in	O
textual	O
sarcasm	O
(	O
Riloff	O
et	O
al	O
,	O
2013	O
;	O
Joshi	O
et	O
al	O
,	O
2015b	O
)	O
.	O
The	O
theory	O
presents	O
sarcasm	O
as	O
a	O
contrast	O
between	O
positive	O
sentiment	O
context	O
(	O
e.g.	O
,	O
absolutely	O
loved	O
it	O
)	O
and	O
negative	O
situational	O
context	O
(	O
e.g.	O
,	O
my	O
bus	O
is	O
late	O
)	O
.	O
In	O
our	O
framework	O
,	O
translation	O
of	O
literal	O
sentences	O
to	O
sarcastic	O
ones	O
happens	O
in	O
four	O
stages	O
viz	O
.	O
,	O
(	O
1	O
)	O
Sentiment	O
Neutralization	O
,	O
during	O
which	O
sentiment	O
-	O
bearing	O
words	O
and	O
phrases	O
are	O
filtered	O
from	O
the	O
input	O
,	O
(	O
2	O
)	O
Positive	O
Sentiment	O
Induction	O
,	O
where	O
the	O
neutralized	O
input	O
is	O
translated	O
into	O
phrases	O
conveying	O
a	O
strong	O
positive	O
sentiment	O
,	O
(	O
3	O
)	O
Negative	O
Situation	O
Retrieval	O
,	O
during	O
which	O
a	O
negative	O
situation	O
related	O
to	O
the	O
input	O
is	O
retrieved	O
,	O
and	O
(	O
4	O
)	O
Sarcasm	O
Synthesis	O
,	O
where	O
appropriate	O
sarcastic	O
constructs	O
are	O
formed	O
from	O
the	O
positive	O
sentiment	O
and	O
negative	O
situation	O
phrases	O
gathered	O
in	O
the	O
first	O
three	O
stages	O
.	O
Training	O
and	O
development	O
of	O
these	O
modules	O
require	O
only	O
three	O
unpaired	O
corpora	O
of	O
positive	O
,	O
negative	O
,	O
and	O
sarcastic	O
opinions	O
.	O
For	O
evaluating	O
the	O
system	O
,	O
we	O
manually	O
prepare	O
a	O
small	O
benchmark	O
dataset	O
which	O
contains	O
a	O
set	O
of	O
literal	O
opinions	O
and	O
their	O
corresponding	O
sarcastic	O
paraphrases	O
.	O
Quantitative	O
evaluation	O
of	O
our	O
system	O
is	O
done	O
using	O
popular	O
translation	O
-	O
evaluation	O
metrics	O
,	O
and	O
document	O
similarity	O
measurement	O
metrics	O
.	O
For	O
qualitative	O
evaluation	O
,	O
we	O
consider	O
the	O
human	O
judgment	O
of	O
sarcastic	O
intensity	O
,	O
fluency	O
,	O
and	O
adequacy	O
of	O
the	O
generated	O
sentences	O
.	O
As	O
baselines	O
,	O
we	O
consider	O
some	O
of	O
our	O
simplistic	O
model	O
variants	O
and	O
existing	O
systems	O
for	O
unsupervised	B-TaskName
machine	I-TaskName
translation	I-TaskName
and	O
style	B-TaskName
transfer	I-TaskName
.	O
Our	O
overall	O
observation	O
is	O
that	O
our	O
system	O
often	O
generates	O
sarcasm	O
of	O
better	O
quality	O
than	O
the	O
baselines	O
.	O
The	O
code	O
,	O
data	O
,	O
and	O
resources	O
are	O
available	O
at	O
https://github.com/	O
TarunTater	O
/	O
sarcasm	O
generation	O
.	O

Once	O
the	O
neutralized	O
output	O
is	O
extracted	O
,	O
it	O
is	O
passed	O
to	O
the	O
positive	O
sentiment	O
induction	O
module	O
which	O
transforms	O
it	O
into	O
a	O
positive	O
sentiment	O
sentence	O
.	O
For	O
this	O
,	O
we	O
use	O
a	O
traditional	O
sequence	B-MethodName
to	I-MethodName
sequence	I-MethodName
pipeline	O
(	O
Bahdanau	O
et	O
al	O
,	O
2014	O
)	O
with	O
attention	O
and	O
copy	O
mechanisms	O
(	O
Gulcehre	O
et	O
al	O
,	O
2016	O
)	O
.	O
The	O
input	O
is	O
a	O
set	O
of	O
words	O
coming	O
from	O
the	O
neutralization	O
module	O
.	O
These	O
are	O
transformed	O
into	O
embeddings	O
and	O
are	O
then	O
encoded	O
with	O
the	O
help	O
of	O
LSTM	B-MethodName
layers	O
.	O
The	O
decoder	O
attends	O
over	O
the	O
encoded	O
output	O
and	O
produces	O
the	O
output	O
tokens	O
based	O
on	O
the	O
attended	O
vector	O
and	O
the	O
previously	O
generated	O
tokens	O
.	O
This	O
is	O
a	O
standard	O
technique	O
,	O
typically	O
used	O
in	O
neural	O
machine	B-TaskName
translation	I-TaskName
.	O
As	O
the	O
output	O
from	O
the	O
system	O
is	O
expected	O
to	O
be	O
positive	O
in	O
sentiment	O
,	O
for	O
training	O
the	O
framework	O
,	O
we	O
use	O
only	O
a	O
set	O
of	O
positive	O
sentences	O
from	O
P	O
.	O
Each	O
sentence	O
in	O
the	O
data	O
is	O
filtered	O
using	O
the	O
neutralization	O
module	O
.	O
The	O
filtered	O
version	O
,	O
and	O
the	O
original	O
positive	O
sentence	O
are	O
used	O
as	O
source	O
,	O
target	O
pairs	O
.	O

Negative	O
situations	O
present	O
in	O
sarcastic	O
opinions	O
are	O
typically	O
extrinsic	O
and	O
are	O
loosely	O
related	O
to	O
the	O
semantics	O
of	O
its	O
literal	O
version	O
.	O
Hence	O
,	O
a	O
sequence	B-MethodName
to	I-MethodName
sequence	I-MethodName
module	O
analogous	O
to	O
Section	O
5	O
-	O
grams	O
:	O
getting	O
up	O
for	O
school	O
facts	O
,	O
getting	O
yelled	O
at	O
by	O
people	O
,	O
trying	O
to	O
schedule	O
my	O
classes	O
,	O
feeling	O
like	O
every	O
single	O
person	O
,	O
walking	O
to	O
class	O
in	O
pouring	O
,	O
making	O
people	O
who	O
already	O
hate	O
,	O
working	O
on	O
my	O
last	O
day	O
,	O
spending	O
countless	O
hours	O
at	O
doctors	O
,	O
getting	O
overdraft	O
statements	O
in	O
mail	O
4	O
-	O
grams	O
:	O
talking	O
about	O
world	O
politics	O
,	O
stuck	O
in	O
a	O
generation	O
,	O
sitting	O
in	O
class	O
wondering	O
,	O
canceled	O
at	O
short	O
notice	O
,	O
distancing	O
myself	O
from	O
certain	O
,	O
wipe	O
my	O
own	O
tears	O
3	O
-	O
grams	O
:	O
born	O
not	O
breathing	O
,	O
paid	O
to	O
sleep	O
,	O
scared	O
those	O
faces	O
,	O
taking	O
a	O
shower	O
,	O
starting	O
your	O
monday	O
,	O
accused	O
of	O
everything	O
,	O
worrying	O
about	O
someone	O
,	O
fight	O
jealousy	O
arguments	O
,	O
license	O
to	O
trill	O
,	O
awarded	O
literature	O
prize	O
2	O
-	O
grams	O
:	O
scratching	O
itchy	O
,	O
looking	O
chair	O
,	O
getting	O
hiv	O
,	O
shot	O
first	O
,	O
collecting	O
death	O
,	O
lost	O
respect	O
1	O
-	O
gram	O
:	O
canceled	O
,	O
sleeping	O
,	O
trying	O
,	O
buying	O
,	O
stapling	O
(	O
Riloff	O
et	O
al	O
,	O
2013	O
)	O
3.2	O
may	O
not	O
be	O
very	O
useful	O
.	O
Moreover	O
,	O
for	O
sarcasm	O
generation	O
,	O
for	O
a	O
certain	O
topic	O
,	O
it	O
is	O
safe	O
to	O
assume	O
that	O
there	O
can	O
be	O
a	O
finite	O
set	O
of	O
negative	O
situations	O
.	O
From	O
this	O
set	O
appropriate	O
situation	O
phrases	O
can	O
be	O
"	O
retrieved	O
"	O
depending	O
on	O
the	O
given	O
input	O
.	O
Thus	O
,	O
finding	O
out	O
appropriate	O
negative	O
situations	O
boils	O
down	O
to	O
two	O
sub	O
-	O
problems	O
of	O
(	O
a	O
)	O
preparing	O
a	O
finite	O
set	O
of	O
negative	O
situations	O
,	O
and	O
(	O
b	O
)	O
setting	O
up	O
the	O
negative	O
situation	O
retrieval	O
process	O
.	O
We	O
discuss	O
each	O
of	O
these	O
two	O
steps	O
below	O
.	O

The	O
idea	O
is	O
to	O
find	O
negative	O
situations	O
relevant	O
to	O
the	O
input	O
sentence	O
.	O
We	O
implement	O
an	O
information	B-TaskName
retrieval	I-TaskName
system	O
based	O
on	O
PyLucene	O
.	O
All	O
the	O
negative	O
situations	O
from	O
the	O
gazetteer	O
(	O
Sec	O
.	O
3.3.1	O
)	O
are	O
first	O
indexed	O
.	O
The	O
input	O
sentence	O
is	O
considered	O
as	O
the	O
query	O
for	O
which	O
the	O
most	O
relevant	O
negative	O
situation	O
is	O
retrieved	O
from	O
the	O
indexed	O
list	O
.	O
The	O
factors	O
involved	O
in	O
PyLucene	O
's	O
retrieval	O
algorithm	O
include	O
tf	O
-	O
idf	O
,	O
number	O
of	O
matching	O
terms	O
in	O
the	O
query	O
sentence	O
and	O
the	O
retrieved	O
sentence	O
,	O
and	O
importance	O
measure	O
of	O
a	O
term	O
according	O
to	O
the	O
total	O
number	O
of	O
terms	O
in	O
the	O
search	O
.	O
Once	O
the	O
positive	O
sentiment	O
and	O
negative	O
situations	O
are	O
generated	O
for	O
the	O
input	O
sentence	O
,	O
they	O
undergo	O
a	O
post	O
-	O
processing	O
step	O
where	O
stopwords	O
and	O
redundant	O
words	O
are	O
removed	O
and	O
given	O
as	O
input	O
to	O
the	O
sarcasm	O
synthesis	O
module	O
.	O

As	O
stated	O
earlier	O
,	O
our	O
system	O
does	O
not	O
rely	O
on	O
any	O
paired	O
data	O
for	O
training	O
.	O
It	O
requires	O
three	O
corpora	O
of	O
positive	O
sentences	O
,	O
negative	O
sentences	O
,	O
and	O
sarcastic	O
sentences	O
collected	O
independently	O
.	O
For	O
positive	O
and	O
negative	O
sentiment	O
corpora	O
P	O
and	O
N	O
,	O
we	O
considered	O
short	O
sentences	O
/	O
snippets	O
from	O
the	O
following	O
well	O
-	O
known	O
sources	O
such	O
as	O
(	O
a	O
)	O
Stanford	O
Sentiment	O
Treebank	O
Dataset	O
,	O
(	O
b	O
)	O
Amazon	O
Product	O
Reviews	O
,	O
(	O
c	O
)	O
Yelp	O
Reviews	O
(	O
d	O
)	O
Sentiment	O
140	O
dataset	O
(	O
See	O
Kotzias	O
et	O
al	O
(	O
2015	O
)	O
for	O
sources	O
)	O
.	O
The	O
above	O
datasets	O
primarily	O
contain	O
tweets	O
and	O
short	O
snippets	O
.	O
Tweets	O
are	O
normalized	O
by	O
removing	O
hashtags	O
,	O
usernames	O
,	O
and	O
performing	O
spell	O
checking	O
and	O
lexical	B-TaskName
normalization	I-TaskName
using	O
NLTK	O
(	O
Loper	O
and	O
Bird	O
,	O
2002	O
)	O
.	O
We	O
then	O
filtered	O
out	O
sentences	O
with	O
more	O
than	O
30	O
words	O
.	O
Approximately	O
50	O
,	O
000	O
sentences	O
from	O
each	O
category	O
are	O
retained	O
.	O
Then	O
,	O
based	O
on	O
the	O
vocabulary	O
overlap	O
with	O
our	O
sarcasm	O
corpus	O
S	O
,	O
47	O
,	O
827	O
sentences	O
are	O
finally	O
retained	O
from	O
each	O
category	O
(	O
total	O
number	O
of	O
instances	O
is	O
95654	O
)	O
.	O
For	O
the	O
unlabelled	O
sarcasm	O
corpus	O
S	O
,	O
we	O
relied	O
on	O
popular	O
datasets	O
used	O
for	O
sarcasm	B-TaskName
detection	I-TaskName
tasks	O
such	O
as	O
the	O
ones	O
by	O
Ghosh	O
and	O
Veale	O
(	O
2016	O
)	O
,	O
Riloff	O
et	O
al	O
(	O
2013	O
)	O
,	O
and	O
the	O
Reddit	B-DatasetName
Sarcasm	O
Corpus	O
4	O
.	O
Sentences	O
are	O
denoised	O
,	O
spell	O
corrected	O
and	O
normalized	O
.	O
Average	O
sentence	O
length	O
is	O
kept	O
as	O
30	O
words	O
.	O
A	O
total	O
number	O
of	O
306	O
,	O
141	O
sentences	O
are	O
thus	O
collected	O
.	O
A	O
common	O
vocabulary	O
of	O
size	O
20	O
,	O
000	O
is	O
extracted	O
(	O
based	O
on	O
frequency	O
)	O
for	O
all	O
the	O
modules	O
from	O
the	O
three	O
corpora	O
.	O
Each	O
corpus	O
is	O
divided	O
into	O
a	O
train	O
-	O
valid	O
-	O
test	O
split	O
of	O
80	O
%	O
-	O
10	O
%	O
-	O
10	O
%	O
.	O

We	O
also	O
consider	O
human	O
judgment	O
scores	O
indicating	O
whether	O
the	O
generated	O
output	O
is	O
nonsarcastic	O
/	O
sarcastic	O
(	O
0/1	O
labels	O
)	O
,	O
how	O
fluent	O
it	O
is	O
(	O
in	O
a	O
scale	O
of	O
1	O
-	O
5	O
,	O
1	O
being	O
lower	O
)	O
,	O
and	O
to	O
what	O
extent	O
it	O
is	O
related	O
to	O
the	O
input	O
(	O
in	O
a	O
scale	O
of	O
1	O
-	O
5	O
)	O
.	O
The	O
relatedness	O
measure	O
is	O
important	O
as	O
the	O
objective	O
of	O
the	O
task	O
is	O
to	O
produce	O
a	O
sarcastic	O
version	O
of	O
the	O
input	O
text	O
without	O
altering	O
the	O
semantics	O
much	O
.	O
For	O
human	O
evaluation	O
,	O
we	O
consider	O
only	O
the	O
30	O
sentences	O
randomly	O
picked	O
from	O
the	B-DatasetName
benchmark	I-DatasetName
(	O
test	O
)	O
dataset	O
.	O
Sarcasm	O
is	O
a	O
difficult	O
topic	O
,	O
so	O
we	O
stuck	O
to	O
only	O
two	O
annotators	O
who	O
had	O
a	O
better	O
understanding	O
of	O
the	O
language	O
and	O
socio	O
-	O
cultural	O
diversities	O
.	O

For	O
comparison	O
,	O
we	O
consider	O
the	O
following	O
four	O
systems	O
:	O
1	O
.	O
SarcasmBot	O
:	O
This	O
is	O
an	O
open	O
-	O
sourced	O
sarcasm	O
generation	O
chatbot	B-TaskName
released	O
by	O
Joshi	O
et	O
al	O
(	O
2015a	O
)	O
.	O
The	O
system	O
generates	O
a	O
sarcastic	O
response	O
to	O
an	O
input	O
utterance	O
.	O

Monoses	O
:	O
This	O
is	O
similar	O
to	O
UNMT	O
but	O
based	O
on	O
unsupervised	O
Statistical	O
Machine	B-TaskName
Translation	I-TaskName
(	O
Artetxe	O
et	O
al	O
,	O
2018	O
)	O
.	O

This	O
is	O
based	O
on	O
the	O
cross	O
alignment	O
technique	O
proposed	O
by	O
Shen	O
et	O
al	O
(	O
2017	O
)	O
,	O
used	O
for	O
the	O
task	O
of	O
sentiment	O
translation	O
.	O
5	O
.	O
FLIP	O
:	O
This	O
is	O
based	O
on	O
heuristics	O
for	O
sentiment	O
reversal	O
.	O
For	O
this	O
,	O
the	O
input	O
sentence	O
is	O
first	O
dependency	O
-	O
parsed	O
.	O
The	O
root	O
verb	O
is	O
determined	O
along	O
with	O
its	O
tense	O
and	O
aspects	O
with	O
the	O
help	O
of	O
its	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
tags	O
7	O
.	O
The	O
sentiment	O
of	O
root	O
verb	O
is	O
determined	O
using	O
sentiment	O
lexicon	O
8	O
.	O
If	O
the	O
verb	O
has	O
non	O
-	O
zero	O
positive	O
or	O
negative	O
sentiment	O
score	O
,	O
its	O
antonym	O
is	O
found	O
using	O
WordNet	O
.	O
Appropriate	O
tense	O
and	O
aspect	O
form	O
of	O
the	O
antonym	O
is	O
then	O
obtained	O
9	O
.	O
The	O
modified	O
antonym	O
replaces	O
the	O
original	O
root	O
verb	O
.	O
Similarly	O
,	O
we	O
replace	O
adjective	O
and	O
adverbs	O
with	O
words	O
carrying	O
opposite	O
sentiment	O
.	O
For	O
training	O
the	O
above	O
systems	O
(	O
except	O
FLIP	O
)	O
,	O
we	O
used	O
S	O
at	O
one	O
side	O
and	O
a	O
larger	O
version	O
of	O
combined	O
P	O
and	O
N	O
containing	O
558	O
,	O
235	O
sentences	O
on	O
the	O
other	O
side	O
,	O
curated	O
from	O
the	O
same	O
sources	O
as	O
mentioned	O
earlier	O
.	O
Apart	O
from	O
this	O
system	O
,	O
we	O
also	O
tested	O
some	O
of	O
our	O
model	O
variants	O
,	O
which	O
are	O
presumably	O
inferior	O
and	O
can	O
be	O
considered	O
as	O
baselines	O
.	O
These	O
are	O
termed	O
as	O
:	O
1	O
.	O
SG	O
NORMAL	O
:	O
a	O
system	O
with	O
only	O
the	O
sarcasm	O
synthesizer	O
module	O
which	O
takes	O
the	O
input	O
directly	O
(	O
after	O
removing	O
stopwords	O
from	O
the	O
input	O
)	O
,	O
2	O
.	O
SG	O
RL	O
:	O
,	O
same	O
as	O
SG	O
NORMAL	O
but	O
also	O
applies	O
reinforcement	O
learning	O
,	O
3	O
.	O
ALL	O
NORMAL	O
:	O
,	O
the	O
complete	O
system	O
,	O
with	O
sarcasm	O
synthesizer	O
trained	O
without	O
reinforcement	O
learning	O
strategy	O
.	O
4	O
.	O
ALL	O
RL	O
:	O
,	O
the	O
complete	O
system	O
with	O
reinforcement	O
learning	O
.	O

As	O
stated	O
earlier	O
,	O
not	O
many	O
systems	O
for	O
sarcasm	O
generation	O
exist	O
today	O
.	O
The	O
closest	O
work	O
to	O
ours	O
is	O
the	O
one	O
by	O
Joshi	O
et	O
al	O
(	O
2015a	O
)	O
which	O
employs	O
a	O
heuristic	O
driven	O
approach	O
for	O
generating	O
a	O
sarcastic	O
response	O
to	O
an	O
input	O
utterance	O
.	O
Since	O
,	O
the	O
output	O
of	O
the	O
system	O
is	O
a	O
response	O
,	O
the	O
system	O
is	O
not	O
suitable	O
for	O
translating	O
a	O
literal	O
input	O
text	O
into	O
a	O
sarcastic	O
version	O
.	O
Unlike	O
sarcasm	O
generation	O
,	O
sar	O
-	O
casm	O
detection	O
has	O
been	O
a	O
well	O
-	O
known	O
problem	O
with	O
several	O
available	O
solutions	O
.	O
For	O
this	O
problem	O
,	O
traditional	O
supervised	O
and	O
deep	O
neural	O
network	O
based	O
solutions	O
have	O
been	O
proposed	O
.	O
The	O
supervised	O
approaches	O
rely	O
on	O
:	O
(	O
a	O
)	O
Unigrams	O
and	O
Pragmatic	O
features	O
(	O
González	O
-	O
Ibánez	O
et	O
al	O
,	O
2011	O
;	O
Barbieri	O
et	O
al	O
,	O
2014	O
;	O
Joshi	O
et	O
al	O
,	O
2015b	O
)	O
(	O
b	O
)	O
Stylistic	O
patterns	O
(	O
Davidov	O
et	O
al	O
,	O
2010	O
)	O
and	O
patterns	O
related	O
to	O
situational	O
disparity	O
(	O
Riloff	O
et	O
al	O
,	O
2013	O
)	O
and	O
(	O
c	O
)	O
Cognitive	O
features	O
extracted	O
from	O
gaze	O
patterns	O
(	O
Mishra	O
et	O
al	O
,	O
2016	O
(	O
Mishra	O
et	O
al	O
,	O
,	O
2017	O
.	O
Recent	O
systems	O
are	O
based	O
on	O
variants	O
of	O
deep	O
neural	O
networks	O
built	O
on	O
the	O
top	O
of	O
embeddings	O
.	O
Deep	O
neural	O
networks	O
based	O
solutions	O
for	O
sarcasm	B-TaskName
detection	I-TaskName
include	O
(	O
Ghosh	O
and	O
Veale	O
,	O
2016	O
)	O
who	O
uses	O
a	O
combination	O
of	O
RNNs	O
and	O
CNNs	O
for	O
sarcasm	B-TaskName
detection	I-TaskName
,	O
and	O
(	O
Tay	O
et	O
al	O
,	O
2018	O
)	O
,	O
who	O
propose	O
a	O
variant	O
of	O
CNN	O
for	O
extracting	O
features	O
related	O
to	O
context	O
incongruity	O
.	O
A	O
few	O
works	O
exist	O
in	O
the	O
domains	O
of	O
irony	O
,	O
pun	O
and	O
humour	O
generation	O
and	O
are	O
summarized	O
by	O
Wallace	O
(	O
2015	O
)	O
,	O
Ritchie	O
(	O
2005	O
)	O
and	O
Strapparava	O
et	O
al	O
(	O
2011	O
)	O
respectively	O
.	O
However	O
,	O
most	O
of	O
these	O
are	O
heuristic	O
driven	O
and	O
,	O
hence	O
,	O
may	O
not	O
be	O
easily	O
scaled	O
to	O
new	O
domains	O
and	O
languages	O
.	O
From	O
the	O
perspective	O
of	O
language	O
style	B-TaskName
transfer	I-TaskName
.	O
Shen	O
et	O
al	O
(	O
2017	O
)	O
propose	O
an	O
unsupervised	O
scheme	O
to	O
learn	O
latent	O
content	O
distribution	O
across	O
different	O
text	O
corpora	O
and	O
use	O
it	O
for	O
sentiment	O
style	B-TaskName
transfer	I-TaskName
.	O
Xu	O
et	O
al	O
(	O
2018	O
)	O
introduce	O
an	O
unsupervised	O
sentiment	O
translation	O
technique	O
through	O
sentiment	O
neutralization	O
and	O
reinforced	O
sequence	O
generation	O
.	O
propose	O
a	O
style	B-TaskName
transfer	I-TaskName
technique	O
based	O
on	O
unsupervised	O
MT	O
inspired	O
by	O
Artetxe	O
et	O
al	O
(	O
2017	O
)	O
.	O
Artetxe	O
et	O
al	O
(	O
2018	O
)	O
have	O
recently	O
proposed	O
an	O
unsupervised	O
statistical	O
machine	B-TaskName
translation	I-TaskName
scheme	O
.	O
We	O
adopt	O
some	O
of	O
these	O
modules	O
for	O
the	O
task	O
of	O
sarcasm	O
generation	O
.	O
As	O
far	O
as	O
we	O
know	O
,	O
our	O
proposed	O
model	O
is	O
the	O
first	O
of	O
its	O
kind	O
for	O
end	O
-	O
to	O
-	O
end	O
neural	O
sarcasm	O
generation	O
.	O

We	O
proposed	O
a	O
first	O
of	O
its	O
kind	O
approach	O
for	O
textual	O
sarcasm	O
generation	O
from	O
literal	O
opinionated	O
texts	O
.	O
We	O
designed	O
a	O
modular	O
framework	O
for	O
extracting	O
facts	O
from	O
the	O
input	O
,	O
generating	O
incongruous	O
positive	O
and	O
negative	O
situational	O
phrases	O
related	O
to	O
the	O
facts	O
,	O
and	O
finally	O
generating	O
sarcastic	O
variations	O
.	O
For	O
evaluation	O
,	O
we	O
prepared	O
a	O
benchmark	O
dataset	O
containing	O
literal	O
opinions	O
and	O
their	O
sarcastic	O
versions	O
.	O
Through	O
qualitative	O
and	O
quantitative	O
anal	O
-	O
ysis	O
of	O
the	O
system	O
's	O
performance	O
on	O
the	B-DatasetName
benchmark	I-DatasetName
dataset	O
,	O
we	O
observed	O
that	O
our	O
system	O
often	O
generates	O
better	O
sarcastic	O
sentences	O
compared	O
to	O
some	O
of	O
our	O
trivial	O
model	O
variants	O
,	O
and	O
unsupervised	O
systems	O
used	O
for	O
machine	B-TaskName
translation	I-TaskName
and	O
sentiment	O
style	B-TaskName
transfer	I-TaskName
.	O
In	O
the	O
future	O
,	O
we	O
would	O
like	O
to	O
extend	O
this	O
framework	O
for	O
cross	O
-	O
lingual	O
and	O
cross	O
-	O
cultural	O
sarcasm	O
and	O
irony	O
generation	O
.	O

Detection	O
of	O
Adverse	O
Drug	O
Reaction	O
mentions	O
in	O
tweets	O
using	O
ELMo	B-MethodName

This	O
paper	O
describes	O
the	O
models	O
used	O
by	O
our	O
team	O
in	O
SMM4H	B-DatasetName
2019	O
shared	O
task	O
(	O
Weissenbacher	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
submitted	O
results	O
for	O
subtasks	O
1	O
and	O
2	O
.	O
For	O
task	O
1	O
which	O
aims	O
to	O
detect	O
tweets	O
with	O
Adverse	O
Drug	O
Reaction	O
(	O
ADR	O
)	O
mentions	O
we	O
used	O
ELMo	B-MethodName
embeddings	O
which	O
is	O
a	O
deep	O
contextualized	O
word	O
representation	O
able	O
to	O
capture	O
both	O
syntactic	O
and	O
semantic	O
characteristics	O
.	O
For	O
task	O
2	O
,	O
which	O
focuses	O
on	O
extraction	O
of	O
ADR	O
mentions	O
,	O
first	O
the	O
same	O
architecture	O
as	O
task	O
1	O
was	O
used	O
to	O
identify	O
whether	O
or	O
not	O
a	O
tweet	O
contains	O
ADR	O
.	O
Then	O
,	O
for	O
tweets	O
positively	O
classified	O
as	O
mentioning	O
ADR	O
,	O
the	O
relevant	O
text	O
span	O
was	O
identified	O
by	O
similarity	O
matching	O
with	O
3	O
different	O
lexicon	O
sets	O
.	O

Solving	O
Aspect	O
Category	O
Sentiment	B-TaskName
Analysis	I-TaskName
as	O
a	O
Text	B-TaskName
Generation	I-TaskName
Task	O

Aspect	O
category	O
sentiment	B-TaskName
analysis	I-TaskName
has	O
attracted	O
increasing	O
research	O
attention	O
.	O
The	O
dominant	O
methods	O
make	O
use	O
of	O
pre	O
-	O
trained	O
language	O
models	O
by	O
learning	O
effective	O
aspect	O
category	O
-	O
specific	O
representations	O
,	O
and	O
adding	O
specific	O
output	O
layers	O
to	O
its	O
pre	O
-	O
trained	O
representation	O
.	O
We	O
consider	O
a	O
more	O
direct	O
way	O
of	O
making	O
use	O
of	O
pre	O
-	O
trained	O
language	O
models	O
,	O
by	O
casting	O
the	O
ACSA	O
tasks	O
into	O
natural	O
language	O
generation	O
tasks	O
,	O
using	O
natural	O
language	O
sentences	O
to	O
represent	O
the	O
output	O
.	O
Our	O
method	O
allows	O
more	O
direct	O
use	O
of	O
pre	O
-	O
trained	O
knowledge	O
in	O
seq2seq	B-MethodName
language	O
models	O
by	O
directly	O
following	O
the	O
task	O
setting	O
during	O
pre	O
-	O
training	O
.	O
Experiments	O
on	O
several	O
benchmarks	O
show	O
that	O
our	O
method	O
gives	O
the	O
best	O
reported	O
results	O
,	O
having	O
large	O
advantages	O
in	O
few	O
-	O
shot	O
and	O
zero	O
-	O
shot	O
settings	O
.	O

Aspect	B-TaskName
-	I-TaskName
based	I-TaskName
sentiment	I-TaskName
analysis	I-TaskName
(	O
ABSA	O
)	O
is	O
a	O
finegrained	O
sentiment	B-TaskName
analysis	I-TaskName
task	O
that	O
includes	O
a	O
number	O
of	O
subtasks	O
,	O
two	O
of	O
which	O
are	O
aspect	O
category	O
sentiment	B-TaskName
analysis	I-TaskName
(	O
ACSA	O
)	O
and	O
aspect	B-TaskName
category	I-TaskName
detection	I-TaskName
(	O
ACD	O
)	O
.	O
Figure	O
1	O
shows	O
an	O
example	O
,	O
where	O
the	O
input	O
is	O
"	O
The	O
restaurant	O
was	O
expensive	O
,	O
but	O
the	O
menu	O
was	O
great	O
"	O
.	O
ACD	O
detects	O
the	O
aspect	O
categories	O
,	O
such	O
as	O
price	O
and	O
food	O
,	O
and	O
ACSA	O
predicts	O
the	O
sentiment	O
polarities	O
toward	O
each	O
aspect	O
category	O
.	O
In	O
this	O
work	O
,	O
we	O
focus	O
on	O
these	O
two	O
tasks	O
as	O
well	O
as	O
the	O
joint	O
task	O
that	O
combines	O
both	O
.	O
Previous	O
studies	O
have	O
investigated	O
various	O
methods	O
that	O
treat	O
ACSA	O
and	O
ACD	O
as	O
classification	O
tasks	O
,	O
learning	O
aspect	O
-	O
specific	O
sentence	O
representations	O
(	O
Wang	O
et	O
al	O
,	O
2016	O
;	O
Ruder	O
et	O
al	O
,	O
2016	O
)	O
.	O
Recently	O
,	O
pre	O
-	O
trained	O
language	O
models	O
(	O
PLM	O
)	O
have	O
shown	O
their	O
effectiveness	O
to	O
this	O
end	O
(	O
Jiang	O
et	O
al	O
,	O
2019	O
)	O
.	O
The	O
main	O
idea	O
is	O
to	O
make	O
use	O
of	O
pre	O
-	O
trained	O
models	O
such	O
as	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019a	O
)	O
for	O
representing	O
an	O
aspect	O
-	O
specific	O
form	O
of	O
the	O
input	O
(	O
e.g.	O
,	O
by	O
concatenating	O
the	O
aspect	O
category	O
to	O
the	O
end	O
of	O
the	O
input	O
sentence	O
(	O
Figure	O
3	O
(	O
a	O
)	O
)	O
)	O
,	O
which	O
provides	O
useful	O
semantic	O
features	O
for	O
ACSA	O
and	O
ACD	O
classifiers	O
.	O
Such	O
methods	O
have	O
given	O
highly	O
competitive	O
results	O
Li	O
et	O
al	O
,	O
2020b	O
)	O
.	O
The	O
above	O
classification	O
models	O
benefit	O
from	O
contextualized	O
representations	O
,	O
which	O
contain	O
knowledge	O
learned	O
by	O
pre	O
-	O
training	O
over	O
large	O
data	O
(	O
Lin	O
et	O
al	O
,	O
2019	O
)	O
.	O
However	O
,	O
their	O
use	O
of	O
pre	O
-	O
trained	O
knowledge	O
can	O
be	O
viewed	O
as	O
indirect	O
due	O
to	O
at	O
least	O
two	O
reasons	O
.	O
First	O
,	O
the	O
classification	O
task	O
is	O
performed	O
by	O
using	O
a	O
neural	O
network	O
on	O
top	O
of	O
pretrained	O
representation	O
,	O
with	O
separate	O
network	O
parameters	O
.	O
Second	O
,	O
the	O
integration	O
of	O
aspect	O
category	O
makes	O
the	O
aspect	O
-	O
specific	O
input	O
representation	O
not	O
exactly	O
a	O
natural	O
language	O
sentence	O
,	O
which	O
differs	O
from	O
the	O
pre	O
-	O
training	O
setting	O
.	O
Intuitively	O
,	O
more	O
pre	O
-	O
trained	O
knowledge	O
could	O
be	O
leveraged	O
by	O
connecting	O
pre	O
-	O
training	O
and	O
ACSA	O
at	O
the	O
task	O
level	O
,	O
rather	O
than	O
only	O
at	O
the	O
representation	O
level	O
.	O
We	O
investigate	O
the	O
above	O
potentials	O
by	O
casting	O
the	O
sentiment	O
classification	O
tasks	O
into	O
language	B-TaskName
modelling	I-TaskName
tasks	O
.	O
In	O
particular	O
,	O
as	O
shown	O
in	O
Figure	O
2	O
,	O
both	O
ACSA	O
and	O
ACD	O
are	O
transformed	O
into	O
sequence	O
-	O
to	O
-	O
sequence	O
(	O
seq2seq	B-MethodName
)	O
tasks	O
,	O
where	O
the	O
encoder	O
takes	O
the	O
input	O
sentence	O
and	O
the	O
decoder	O
generates	O
a	O
natural	O
language	O
sentence	O
.	O
For	O
ACD	O
,	O
the	O
output	O
follows	O
a	O
template	O
stating	O
whether	O
the	O
specific	O
aspect	O
is	O
discussed	O
(	O
e.g.	O
,	O
"	O
The	O
category_type	O
category	O
is	O
discussed	O
"	O
)	O
;	O
for	O
ACSA	O
,	O
the	O
sentiment	O
polarity	O
of	O
a	O
specific	O
aspect	O
is	O
stated	O
(	O
e.g.	O
,	O
"	O
The	O
sentiment	O
polarity	O
of	O
given_category	O
is	O
polarity_type	O
"	O
)	O
.	O
The	O
setting	O
corresponds	O
closely	O
to	O
the	O
denoising	B-TaskName
auto	O
-	O

The	O
price	O
category	O
is	O
discussed	O
(	O
scoring	O
:	O
0.9	O
)	O
The	O
price	O
category	O
is	O
not	O
discussed	O
(	O
scoring	O
:	O
0.1	O
)	O
encoder	O
training	O
scheme	O
of	O
BART	B-MethodName
(	O
Lewis	O
et	O
al	O
,	O
2020	O
)	O
,	O
which	O
we	O
use	O
as	O
the	O
pre	O
-	O
trained	O
model	O
.	O
Compared	O
with	O
classification	O
-	O
based	O
methods	O
,	O
our	O
method	O
does	O
not	O
include	O
more	O
network	O
parameters	O
,	O
and	O
thus	O
can	O
potentially	O
generalize	O
better	O
to	O
new	O
domains	O
(	O
Brown	O
et	O
al	O
,	O
2020	O
;	O
Gao	O
et	O
al	O
,	O
2020	O
)	O
.	O
Given	O
a	O
new	O
domain	O
with	O
completely	O
unseen	O
aspect	O
categories	O
and	O
sentiment	O
labels	O
,	O
our	O
method	O
can	O
be	O
applied	O
without	O
changing	O
output	O
layer	O
structure	O
.	O

In	O
addition	O
to	O
classification	O
-	O
based	O
methods	O
,	O
we	O
take	O
masked	O
language	O
models	O
(	O
MLM	B-DatasetName
)	O
as	O
a	O
baseline	O
also	O
,	O
for	O
which	O
a	O
natural	O
counterpart	O
of	O
our	O
method	O
is	O
a	O
mask	O
-	O
refilling	O
task	O
.	O
As	O
shown	O
in	O
Figure	O
3	O
(	O
b	O
)	O
,	O
different	O
from	O
our	O
method	O
,	O
the	O
output	O
template	O
is	O
concatenated	O
to	O
the	O
input	O
,	O
with	O
the	O
keyword	O
being	O
masked	O
for	O
prediction	O
.	O
This	O
MLM	B-DatasetName
task	O
corresponds	O
closely	O
to	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019a	O
)	O
pre	O
-	O
training	O
.	O
In	O
comparison	O
to	O
this	O
MLM	B-DatasetName
method	O
,	O
a	O
generation	O
method	O
can	O
better	O
learn	O
the	O
correlation	O
between	O
the	O
input	O
and	O
output	O
template	O
as	O
two	O
related	O
sequences	O
,	O
which	O
has	O
been	O
demonstrated	O
by	O
the	O
strong	O
performance	O
of	O
BART	B-MethodName
for	O
abstractive	B-TaskName
text	I-TaskName
summarization	I-TaskName
(	O
Lewis	O
et	O
al	O
,	O
2020	O
)	O
.	O
Experimental	O
results	O
on	O
three	O
standard	O
benchmarks	O
datasets	O
show	O
that	O
both	O
generation	O
and	O
MLM	B-DatasetName
methods	O
outperform	O
classification	O
methods	O
using	O
the	O
same	O
pre	O
-	O
trained	O
language	O
models	O
.	O
Finally	O
,	O
generation	O
methods	O
give	O
stronger	O
performances	O
than	O
MLM	B-DatasetName
methods	O
,	O
outperforming	O
the	O
previous	O
stateof	O
-	O
the	O
-	O
art	O
methods	O
by	O
a	O
large	O
margin	O
.	O
In	O
addition	O
,	O
using	O
the	O
generation	O
method	O
,	O
we	O
show	O
that	O
jointly	O
performing	O
ACSA	O
and	O
ACD	O
leads	O
to	O
better	O
results	O
than	O
the	O
traditional	O
pipeline	O
.	O
To	O
our	O
knowledge	O
,	O
we	O
are	O
the	O
first	O
to	O
employ	O
a	O
generative	O
pre	O
-	O
trained	O
language	O
model	O
to	O
address	O
an	O
ACSA	O
/	O
ACD	O
problem	O
.	O
We	O
release	O
our	O
code	O
at	O
https://github	O
.	O
com	O
/	O
lgw863	O
/	O
ACSA	O
-	O
generation	O
.	O

Aspect	O
Category	O
Sentiment	B-TaskName
Analysis	I-TaskName
Wang	O
et	O
al	O
(	O
2016	O
)	O
propose	O
an	O
attention	O
-	O
based	O
LSTM	B-MethodName
network	O
,	O
which	O
can	O
concentrate	O
on	O
different	O
parts	O
of	O
a	O
sentence	O
when	O
different	O
aspect	O
categories	O
are	O
taken	O
as	O
input	O
.	O
Ruder	O
et	O
al	O
(	O
2016	O
)	O
model	O
the	O
interdependencies	O
of	O
sentences	O
in	O
a	O
text	O
with	O
a	O
hierarchical	O
bidirectional	B-MethodName
LSTM	I-MethodName
.	O
Yin	O
et	O
al	O
(	O
2017	O
)	O
model	O
the	O
task	O
as	O
a	O
machine	O
comprehension	O
problem	O
by	O
constructing	O
pseudo	O
question	O
-	O
answer	O
pairs	O
.	O
Xue	O
and	O
Li	O
(	O
2018	O
)	O
Liang	O
et	O
al	O
(	O
2019	O
)	O
and	O
incorporate	O
aspect	O
category	O
information	O
into	O
sentence	O
encoders	O
in	O
the	O
context	O
modeling	O
stage	O
.	O
construct	O
auxiliary	O
sentences	O
from	O
the	O
aspect	O
categories	O
and	O
convert	O
ACSA	O
to	O
a	O
sentence	O
-	O
pair	O
classification	O
task	O
.	O
Li	O
et	O
al	O
(	O
2020b	O
)	O
predict	O
the	O
sentiment	O
of	O
an	O
aspect	O
category	O
mentioned	O
in	O
a	O
sentence	O
by	O
aggregating	O
the	O
sentiments	O
of	O
the	O
words	O
indicating	O
the	O
aspect	O
category	O
in	O
the	O
sentence	O
.	O
Several	O
joint	O
models	O
were	O
proposed	O
to	O
avoid	O
error	O
propagation	O
,	O
which	O
perform	O
ACD	O
and	O
ACSA	O
jointly	O
.	O
Schmitt	O
et	O
al	O
(	O
2018	O
)	O
propose	O
two	O
joint	O
models	O
:	O
end	O
-	O
to	O
-	O
end	O
LSTM	B-MethodName
and	O
end	O
-	O
to	O
-	O
end	O
CNN	O
,	O
which	O
produce	O
all	O
the	O
aspect	O
categories	O
and	O
their	O
corresponding	O
sentiment	O
polarities	O
at	O
once	O
.	O
Hu	O
et	O
al	O
(	O
2019	O
)	O
propose	O
constrained	O
attention	O
networks	O
(	O
CAN	O
)	O
to	O
constrain	O
the	O
attention	O
weight	O
allocation	O
.	O
propose	O
the	O
aspect	O
-	O
level	O
sentiment	O
capsules	O
model	O
(	O
AS	O
-	O
Capsules	O
)	O
,	O
which	O
utilizes	O
the	O
correlation	O
between	O
aspect	O
category	O
and	O
sentiment	O
through	O
shared	O
components	O
.	O
Li	O
et	O
al	O
(	O
2020a	O
)	O
propose	O
a	O
novel	O
joint	O
model	O
which	O
contains	O
a	O
shared	O
sentiment	O
prediction	O
layer	O
.	O
All	O
the	O
models	O
above	O
are	O
classification	O
methods	O
,	O
which	O
use	O
a	O
separate	O
output	O
network	O
to	O
give	O
the	O
output	O
label	O
.	O
In	O
contrast	O
,	O
we	O
investigate	O
natural	O
language	O
generation	O
methods	O
by	O
directly	O
following	O
the	O
pre	O
-	O
training	O
process	O
of	O
language	O
models	O
.	O
Masked	O
Language	O
Model	O
Methods	O
There	O
is	O
a	O
line	O
of	O
work	O
using	O
the	O
masked	O
language	O
model	O
(	O
MLM	B-DatasetName
)	O
for	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
tasks	O
.	O
The	O
basic	O
idea	O
is	O
to	O
leverage	O
information	O
from	O
pre	O
-	O
trained	O
models	O
by	O
defining	O
specific	O
sentence	O
prompt	O
in	O
a	O
language	B-TaskName
modelling	I-TaskName
task	O
.	O
Brown	O
et	O
al	O
(	O
2020	O
)	O
use	O
prompt	O
for	O
few	B-TaskName
-	I-TaskName
shot	I-TaskName
learning	I-TaskName
in	O
text	B-TaskName
classification	I-TaskName
tasks	O
.	O
rephrase	O
inputs	O
as	O
cloze	O
questions	O
for	O
text	B-TaskName
classification	I-TaskName
.	O
and	O
Gao	O
et	O
al	O
(	O
2020	O
)	O

The	O
menu	O
was	O
great	O
<	O
/s	O
>	O
extend	O
by	O
automatically	O
generating	O
label	O
words	O
and	O
templates	O
,	O
respectively	O
.	O
Petroni	O
et	O
al	O
(	O
2019	O
)	O
extract	O
relation	O
between	O
entities	O
from	O
BERT	B-MethodName
by	O
constructing	O
cloze	O
-	O
style	O
templates	O
.	O
We	O
are	O
the	O
first	O
to	O
apply	O
such	O
methods	O
to	O
ACSA	O
,	O
taking	O
it	O
as	O
a	O
baseline	O
.	O
Different	O
from	O
these	O
template	O
-	O
based	O
models	O
,	O
our	O
final	O
model	O
uses	O
BART	B-MethodName
for	O
text	B-TaskName
generation	I-TaskName
,	O
which	O
better	O
models	O
the	O
correlations	O
between	O
the	O
input	O
sentence	O
and	O
the	O
output	O
sentence	O
compared	O
with	O
BERT	B-MethodName
.	O
Generation	O
Methods	O
There	O
has	O
been	O
work	O
casting	O
NLP	O
problems	O
as	O
sequence	O
generation	O
tasks	O
(	O
Vinyals	O
et	O
al	O
,	O
2015	O
;	O
Ma	O
et	O
al	O
,	O
2017	O
;	O
Stanovsky	O
and	O
Dagan	O
,	O
2018	O
;	O
Raffel	O
et	O
al	O
,	O
2020	O
)	O
,	O
where	O
the	O
output	O
is	O
a	O
sequence	O
of	O
tokens	O
rather	O
than	O
a	O
natural	O
language	O
sentence	O
.	O
Daza	O
and	O
Frank	O
(	O
2018	O
)	O
treat	O
semantic	O
role	O
labelling	O
as	O
a	O
sequence	O
-	O
to	O
-	O
sequence	O
process	O
.	O
solve	O
the	O
entity	O
-	O
relation	B-TaskName
extraction	I-TaskName
task	O
as	O
a	O
multi	O
-	O
turn	O
question	B-TaskName
answering	I-TaskName
generation	O
method	O
.	O
Our	O
work	O
is	O
similar	O
in	O
casting	O
an	O
NLP	O
task	O
as	O
a	O
generation	O
task	O
.	O
Different	O
from	O
the	O
above	O
methods	O
,	O
our	O
goal	O
is	O
to	O
make	O
the	O
most	O
of	O
pre	O
-	O
trained	O
knowledge	O
in	O
BART	B-MethodName
for	O
ACSA	O
.	O

Formally	O
for	O
ACD	O
,	O
the	O
input	O
is	O
a	O
sentence	O
X	O
=	O
{	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
n	O
}	O
=	O
x	O
1	O
:	O
n	O
,	O
where	O
x	O
i	O
denotes	O
the	O
i	O
-	O
th	O
word	O
.	O
For	O
ACSA	O
,	O
a	O
set	O
of	O
pre	O
-	O
identified	O
aspect	O
categories	O
are	O
also	O
given	O
.	O
We	O
introduce	O
relevant	O
pre	O
-	O
trained	O
language	O
models	O
in	O
3.1	O
,	O
classification	O
methods	O
in	O
Section	O
3.2	O
,	O
MLM	B-DatasetName
methods	O
in	O
Section	O
3.3	O
,	O
and	O
our	O
generation	O
method	O
in	O
Section	O
3.4	O
.	O

We	O
take	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019a	O
)	O
and	O
BART	B-MethodName
(	O
Lewis	O
et	O
al	O
,	O
2020	O
)	O
as	O
the	O
pre	O
-	O
trained	O
language	O
models	O
.	O
Both	O
are	O
built	O
on	O
the	O
Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
architecture	O
.	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019a	O
)	O
is	O
an	O
encoder	O
stack	O
of	O
Transformer	B-MethodName
for	O
masked	O
text	O
filling	O
,	O
where	O
a	O
model	O
uses	O
the	O
context	O
words	O
to	O
predict	O
masked	O
words	O
.	O
BART	B-MethodName
(	O
Lewis	O
et	O
al	O
,	O
2020	O
)	O
is	O
a	O
denoising	B-TaskName
auto	O
-	O
encoder	O
seq2seq	B-MethodName
model	O
pre	O
-	O
training	O
for	O
natural	O
language	O
generation	O
.	O
Its	O
training	O
applies	O
document	O
corruption	O
such	O
as	O
randomly	O
deleting	O
tokens	O
from	O
the	O
input	O
and	O
corrupting	O
text	O
with	O
an	O
arbitrary	O
noising	O
function	O
.	O
BART	B-MethodName
is	O
trained	O
to	O
reconstruct	O
the	O
original	O
text	O
.	O

We	O
use	O
a	O
multi	O
-	O
layer	O
perceptrons	O
network	O
as	O
the	O
classifier	O
model	O
,	O
which	O
takes	O
a	O
representation	O
vector	O
as	O
input	O
.	O
Both	O
BERT	B-MethodName
and	O
BART	B-MethodName
are	O
considered	O
as	O
the	O
encoders	O
.	O
BERT	B-MethodName
Classification	B-TaskName
BERT	B-MethodName
adopts	O
"	O
[	O
CLS	O
]	O
in	O
-	O
put	O
sentence	O
[	O
SEP	O
]	O
given_category	O
[	O
SEP	O
]	O
"	O
as	O
input	O
.	O
The	O
final	O
hidden	O
state	O
corresponding	O
to	O
"	O
[	O
CLS	O
]	O
"	O
is	O
used	O
as	O
the	O
representation	O
for	O
classification	O
.	O
BART	B-MethodName
Classification	B-TaskName
BART	B-MethodName
adopts	O
"	O
S	O
input	O
sentence	O
/S	O
given_category	O
/S	O
"	O
as	O
input	O
and	O
predicts	O
the	O
sentiment	O
polarity	O
of	O
the	O
sentence	O
towards	O
the	O
given	O
category	O
.	O
The	O
same	O
input	O
is	O
fed	O
into	O
the	O
encoder	O
and	O
decoder	O
(	O
see	O
Figure	O
3	O
(	O
a	O
)	O
)	O
.	O
Formally	O
,	O
suppose	O
that	O
the	O
query	O
category	O
is	O
a	O
,	O
x	O
0	B-DatasetName
=	O
S	O
,	O
x	O
n+1	O
=	O
/S	O
,	O
x	O
n+2	O
=	O
a	O
,	O
x	O
n+3	O
=	O
/S	O
,	O
then	O
the	O
input	O
to	O
BART	B-MethodName
is	O
x	O
0	B-DatasetName
:	O
n+3	O
=	O
S	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
n	O
/S	O
a	O
/S	O
.	O
The	O
output	O
hidden	O
vec	O
-	O
tors	O
obtained	O
by	O
the	O
BART	B-MethodName
encoder	O
(	O
ENCODER	O
)	O
and	O
BART	B-MethodName
decoder	O
(	O
DECODER	O
)	O
are	O
:	O
h	O
enc	O
=	O
ENCODER	O
(	O
x	O
0	B-DatasetName
:	O
n+3	O
)	O
h	O
0	B-DatasetName
.	O
.	O
.	O
h	O
n+3	O
=	O
DECODER	O
(	O
h	O
enc	O
;	O
x	O
0	B-DatasetName
:	O
n+3	O
)	O
The	O
output	O
vector	O
h	O
n+3	O
is	O
then	O
taken	O
as	O
the	O
representation	O
vector	O
for	O
classification	O
.	O

Masked	O
language	O
models	O
(	O
MLM	B-DatasetName
)	O
(	O
Devlin	O
et	O
al	O
,	O
2019a	O
)	O
complete	O
a	O
given	O
prompt	O
by	O
filling	O
missing	O
tokens	O
.	O
We	O
refer	O
to	O
the	O
template	O
including	O
a	O
given	O
category	O
and	O
MASK	O
token	O
together	O
as	O
a	O
prompt	O
.	O
For	O
sentiment	B-TaskName
analysis	I-TaskName
tasks	O
,	O
BERT	B-MethodName
MLM	B-DatasetName
adopts	O
the	O
input	O
sentence	O
and	O
the	O
prompt	O
as	O
the	O
model	O
input	O
and	O
predicts	O
the	O
sentiment	O
polarity	O
label	O
word	O
towards	O
the	O
given	O
category	O
.	O
For	O
BART	B-MethodName
MLM	B-DatasetName
,	O
the	O
same	O
input	O
is	O
fed	O
into	O
the	O
encoder	O
and	O
decoder	O
,	O
and	O
the	O
highest	O
decoder	O
prediction	O
from	O
label	O
words	O
of	O
the	O
MASK	O
token	O
is	O
the	O
predicted	O
polarity	O
label	O
(	O
see	O
Figure	O
3	O
(	O
b	O
)	O
)	O
.	O
We	O
use	O
the	O
same	O
template	O
in	O
the	O
MLM	B-DatasetName
method	O
and	O
generation	O
method	O
,	O
following	O
the	O
template	O
creation	O
method	O
in	O
section	O
3.4.1	O
.	O

We	O
take	O
both	O
ACSA	O
and	O
ACD	O
as	O
language	O
model	O
ranking	O
problems	O
under	O
a	O
seq2seq	B-MethodName
framework	O
(	O
see	O
Figure	O
3	O
(	O
c	O
)	O
)	O
.	O
The	O
target	O
sequence	O
T	O
a	O
i	O
,	O
p	O
k	O
(	O
T	O
a	O
i	O
)	O
=	O
{	O
t	O
1	O
,	O
.	O
.	O
.	O
,	O
t	O
m	O
}	O
is	O
a	O
template	O
filled	O
by	O
the	O
given	O
category	O
a	O
i	O
and	O
the	O
polarity	O
type	O
p	O
k	O
.	O
We	O
first	O
introduce	O
how	O
to	O
create	O
templates	O
in	O
Section	O
3.4.1	O
,	O
and	O
then	O
show	O
the	O
inference	O
and	O
training	O
details	O
in	O
Section	O
3.4.2	O
and	O
Section	O
3.4.3	O
,	O
respectively	O
.	O

We	O
compare	O
our	O
generation	O
method	O
with	O
classification	O
and	O
MLM	B-DatasetName
baselines	O
(	O
Figure	O
3	O
)	O
using	O
the	O
same	O
encoder	O
.	O
In	O
particular	O
,	O
BART	B-MethodName
generation	O
(	O
i.e.	O
,	O
Figure	O
3	O
(	O
c	O
)	O
)	O
is	O
compared	O
with	O
BART	B-MethodName
classification	O
(	O
Figure	O
3	O
(	O
a	O
)	O
)	O
and	O
BART	B-MethodName
MLM	B-DatasetName
(	O
Figure	O
3	O
(	O
b	O
)	O
)	O
,	O
as	O
well	O
as	O
BERT	B-MethodName
classification	O
and	O
BERT	B-MethodName
MLM	B-DatasetName
.	O
In	O
addition	O
,	O
our	O
method	O
is	O
also	O
compared	O
with	O
other	O
models	O
in	O
the	O
literature	O
as	O
follows	O
.	O
For	O
sentence	O
-	O
level	O
ACSA	O
,	O
we	O
also	O
compare	O
our	O
method	O
with	O
the	O
following	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
in	O
the	O
literature	O
.	O
(	O
1	O
)	O
non	O
-	O
BERT	B-MethodName
models	O
:	O
GCAE	O
(	O
Xue	O
and	O
Li	O
,	O
2018	O
)	O
,	O
As	O
-	O
capsule	O
and	O
CapsNet	B-MethodName
(	O
Jiang	O
et	O
al	O
,	O
2019	O
)	O
;	O
(	O
2	O
)	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019b	O
)	O
based	O
models	O
:	O
BERT	B-MethodName
-	O
pair	O
-	O
QA	O
-	O
B	O
,	O
CapsNet	B-MethodName
-	O
BERT	B-MethodName
(	O
Jiang	O
et	O
al	O
,	O
2019	O
)	O
and	O
AC	O
-	O
MIMLLN	O
-	O
BERT	B-MethodName
(	O
Li	O
et	O
al	O
,	O
2020b	O
)	O
.	O
For	O
document	O
-	O
level	O
ACSA	O
,	O
we	O
compare	O
our	O
method	O
with	O
the	O
following	O
methods	O
.	O
(	O
1	O
)	O
non	O
-	O
BERT	B-MethodName
models	O
:	O
LSTM	B-MethodName
(	O
Tang	O
et	O
al	O
,	O
2015	O
)	O
,	O
HAN	O
(	O
Yang	O
et	O
al	O
,	O
2016	O
)	O

Figure	O
6	O
shows	O
typical	O
examples	O
from	O
the	O
test	O
set	O
which	O
can	O
not	O
be	O
inferred	O
by	O
the	O
BART	B-MethodName
classification	O
model	O
.	O
In	O
sentence	O
(	O
a	O
)	O
,	O
the	O
given	O
category	O
miscellaneous	O
does	O
not	O
occur	O
as	O
a	O
term	O
in	O
the	O
given	O
sentence	O
.	O
Our	O
method	O
can	O
synthesize	O
different	O
sentiment	O
polarities	O
with	O
different	O
aspects	O
to	O
obtain	O
correct	O
polarity	O
.	O
In	O
sentence	O
(	O
b	O
)	O
,	O
"	O
the	O
value	O
on	O
the	O
kids	O
menu	O
is	O
good	O
"	O
,	O
good	O
modifies	O
the	O
value	O
,	O
rather	O
than	O
the	O
given	O
category	O
menu	O
.	O
Our	O
method	O
gives	O
the	O
correct	O
polarity	O
,	O
not	O
being	O
affected	O
by	O
the	O
surrounding	O
other	O
aspect	O
sentiments	O
.	O
The	O
last	O
instance	O
(	O
c	O
)	O
has	O
conditional	O
reasoning	O
which	O
is	O
difficult	O
for	O
BART	B-MethodName
classification	O
.	O
In	O
contrast	O
,	O
BART	B-MethodName
generation	O
gives	O
the	O
correct	O
label	O
by	O
correctly	O
recognizing	O
the	O
negativity	O
in	O
"	O
if	O
there	O
was	O
...	O
would	O
be	O
a	O
bit	O
more	O
inviting	O
"	O
.	O
This	O
is	O
likely	O
because	O
our	O
method	O
makes	O
use	O
of	O
pre	O
-	O
trained	O
knowledge	O
to	O
infer	O
the	O
inter	O
-	O
sentential	O
correlations	O
between	O
the	O
input	O
and	O
the	O
output	O
sequences	O
,	O
which	O
the	O
BART	B-MethodName
classification	O
model	O
failed	O
to	O
achieve	O
due	O
to	O
the	O
indirect	O
use	O
of	O
BART	B-MethodName
in	O
the	O
additional	O
classification	O
network	O
.	O

We	O
investigated	O
a	O
generation	O
method	O
for	O
aspect	B-TaskName
category	I-TaskName
detection	I-TaskName
(	O
ACD	O
)	O
and	O
aspect	O
category	O
sentiment	B-TaskName
analysis	I-TaskName
(	O
ACSA	O
)	O
,	O
which	O
can	O
make	O
better	O
use	O
of	O
BART	B-MethodName
's	O
advantages	O
in	O
making	O
semantic	O
level	O
summaries	O
to	O
the	O
input	O
by	O
not	O
introducing	O
additional	O
model	O
parameters	O
.	O
Experiments	O
show	O
that	O
our	O
proposed	O
method	O
obtains	O
superior	O
performance	O
over	O
the	O
baseline	O
models	O
for	O
both	O
sentence	O
-	O
level	O
and	O
document	O
-	O
level	O
aspect	O
sentiment	B-TaskName
analysis	I-TaskName
.	O
In	O
contrast	O
to	O
the	O
traditional	O
sentiment	O
classification	O
methods	O
,	O
our	O
method	O
is	O
also	O
more	O
powerful	O
on	O
zero	O
-	O
shot	O
and	O
few	O
-	O
shot	O
tasks	O
.	O

TripAdvisor	O
(	O
Wang	O
et	O
al	O
,	O
2010	O
)	O
and	O
BeerAdvocate	B-DatasetName
(	O
McAuley	O
et	O
al	O
,	O
2012	O
;	O
Lei	O
et	O
al	O
,	O
2016	O
)	O
contain	O
seven	O
aspects	O
(	O
value	O
,	O
room	O
,	O
location	O
,	O
cleanliness	O
,	O
check	O
in	O
/	O
front	O
desk	O
,	O
service	O
,	O
and	O
business	O
service	O
)	O
and	O
four	O
aspects	O
(	O
feel	O
,	O
look	O
,	O
smell	O
,	O
and	O
taste	O
)	O
respectively	O
.	O
We	O
randomly	O
split	O
them	O
into	O
training	O
,	O
development	O
,	O
and	O
testing	O
sets	O
with	O
80/10/10	O
%	O
.	O
Statistics	O
of	O
these	O
three	O
sentence	O
-	O
level	O
datasets	O
are	O
given	O
in	O
Table	O
9	O
and	O
two	O
document	O
-	O
level	O
datasets	O
are	O
described	O
in	O
Table	O
10	O
.	O

Each	O
method	O
is	O
trained	O
for	O
30	O
epochs	O
,	O
during	O
which	O
the	O
model	O
with	O
the	O
best	O
performance	O
on	O
the	O
validation	O
set	O
is	O
saved	O
.	O
We	O
also	O
apply	O
early	B-MethodName
stopping	I-MethodName
in	O
training	O
,	O
which	O
means	O
that	O
the	O
training	O
will	O
stop	O
if	O
the	O
performance	O
on	O
validation	O
set	O
does	O
not	O
improve	O
in	O
5	O
epochs	O
.	O

While	O
describing	O
the	O
architecture	O
of	O
our	O
system	O
,	O
we	O
use	O
the	O
notions	O
of	O
a	O
VMWE	O
token	O
(	O
its	O
occurrence	O
in	O
running	O
text	O
)	O
and	O
a	O
VMWE	O
type	O
(	O
abstraction	O
over	O
all	O
occurrences	O
of	O
a	O
given	O
VMWE	O
)	O
,	O
as	O
introduced	O
by	O
Savary	O
et	O
al	O
(	O
2019b	O
)	O
.	O
We	O
represent	O
VMWE	O
types	O
as	O
multisets	O
of	O
lemmas	O
and	O
POS	O
.	O
2	O
Our	O
system	O
uses	O
a	O
mixture	O
of	O
discovery	O
and	O
identification	O
methods	O
,	O
as	O
defined	O
by	O
Constant	O
et	O
al	O
(	O
2017	O
)	O
.	O
Namely	O
,	O
VMWE	O
discovery	O
consists	O
in	O
generating	O
lists	O
of	O
MWE	O
types	O
out	O
of	O
context	O
,	O
while	O
VMWE	O
identification	O
marks	O
VMWE	O
tokens	O
in	O
running	O
text	O
.	O
The	O
system	O
is	O
freely	O
available	O
online	O
(	O
https://gitlab.com/	O
cpasquer	O
/	O
st_2020	O
)	O
.	O
This	O
work	O
is	O
licensed	O
under	O
a	O
Creative	O
Commons	O
Attribution	O
4.0	O
International	O
License	O
.	O
License	O
details	O
:	O
http://	O
creativecommons.org/licenses/by/4.0/.	O
1	O
http://hdl.handle.net/11234/1	O
-	O
3367	O
2	O
VMWEs	O
are	O
represented	O
as	O
multisets	O
(	O
i.e.	O
bags	O
of	O
elements	O
with	O
repetition	O
allowed	O
)	O
,	O
since	O
the	O
same	O
lemma	B-DatasetName
and/or	O
POS	O
can	O
occur	O
twice	O
,	O
as	O
in	O
appeler	O
un	O
chat	O
un	O
chat	O
'	O
to	O
call	O
a	O
cat	O
a	O
cat'⇒'to	O
call	O
a	O
spade	B-MethodName
a	O
spade	B-MethodName
'	O
.	O
Seen2Seen	O
in	O
a	O
nutshell	O
Seen2Seen	O
is	O
a	O
VMWE	O
identification	O
system	O
dedicated	O
to	O
only	O
those	O
VMWEs	O
which	O
have	O
been	O
previously	O
seen	O
in	O
the	O
training	O
data	O
.	O
Its	O
detailed	O
description	O
is	O
provided	O
in	O
Pasquer	O
et	O
al	O
(	O
2020	O
)	O
,	O
but	O
a	O
brief	O
overview	O
is	O
included	O
here	O
to	O
make	O
the	O
current	O
paper	O
self	O
-	O
contained	O
.	O
Seen2Seen	O
extracts	O
lemma	B-DatasetName
combinations	O
of	O
VMWEs	O
seen	O
in	O
Train	O
,	O
looking	O
for	O
the	O
same	O
combinations	O
(	O
within	O
one	O
sentence	O
)	O
in	O
Test	O
,	O
with	O
an	O
expected	O
high	O
recall	O
.	O
To	O
improve	O
precision	O
,	O
up	O
to	O
eight	O
independent	O
criteria	O
can	O
be	O
used	O
:	O
(	O
1	O
)	O
component	O
lemmas	O
should	O
be	O
disambiguated	O
by	O
their	O
POS	O
,	O
(	O
2	O
)	O
components	O
should	O
appear	O
in	O
specific	O
orders	O
(	O
e.g.	O
the	O
determiner	O
before	O
the	O
noun	O
)	O
,	O
(	O
3	O
)	O
the	O
order	O
of	O
"	O
gap	O
"	O
words	O
possibly	O
occurring	O
between	O
components	O
is	O
also	O
considered	O
,	O
(	O
4	O
)	O
components	O
should	O
not	O
be	O
too	O
far	O
from	O
each	O
other	O
in	O
a	O
sentence	O
,	O
(	O
5	O
)	O
closer	O
components	O
are	O
preferred	O
over	O
distant	O
ones	O
,	O
(	O
6	O
)	O
components	O
should	O
be	O
syntactically	O
connected	O
,	O
(	O
7	O
)	O
nominal	O
components	O
should	O
appear	O
with	O
a	O
previously	O
seen	O
inflection	O
,	O
and	O
(	O
8	O
)	O
nested	O
VMWEs	O
should	O
be	O
annotated	O
as	O
in	O
Train	O
.	O
We	O
select	O
the	O
combination	O
of	O
criteria	O
with	O
maximal	O
performance	O
on	O
Dev	O
among	O
all	O
2	O
8	O
=	O
256	O
possibilities	O
.	O
The	O
candidates	O
remaining	O
after	O
applying	O
the	O
criteria	O
are	O
annotated	O
as	O
VMWEs	O
.	O
This	O
relatively	O
simple	O
system	O
relying	O
on	O
morphosyntactic	O
filters	O
and	O
tuned	O
for	O
8	O
parameters	O
was	O
evaluated	O
on	O
11	O
languages	O
of	O
the	O
PARSEME	O
shared	O
task	O
1.1	O
(	O
Ramisch	O
et	O
al	O
,	O
2018	O
)	O
.	O
Seen2Seen	O
outperformed	O
the	O
best	O
systems	O
not	O
only	O
on	O
seen	O
(	O
F=0.8276	O
)	O
,	O
but	O
even	O
on	O
all	O
seen	O
and	O
unseen	O
VMWEs	O
(	O
F=0.6653	O
)	O
.	O
3	O
In	O
edition	O
1.2	O
of	O
the	O
PARSEME	O
shared	O
task	O
,	O
Seen2Seen	O
scored	O
best	O
(	O
out	O
of	O
2	O
)	O
in	O
the	O
global	O
ranking	O
of	O
the	O
closed	O
track	O
and	O
second	O
(	O
out	O
of	O
9	O
)	O
across	O
both	O
tracks	O
.	O
It	O
outperformed	O
6	O
other	O
open	O
track	O
systems	O
,	O
notably	O
those	O
using	O
complex	O
neural	O
architectures	O
and	O
contextual	O
word	B-TaskName
embeddings	I-TaskName
.	O
We	O
believe	O
that	O
these	O
competitive	O
results	O
are	O
due	O
to	O
carefully	O
taking	O
the	O
nature	O
of	O
VMWEs	O
into	O
account	O
(	O
Savary	O
et	O
al	O
,	O
2019a	O
)	O
.	O
Since	O
Seen2Seen	O
,	O
by	O
design	O
,	O
does	O
not	O
account	O
for	O
unseen	O
VMWEs	O
,	O
its	O
score	O
in	O
this	O
category	O
is	O
very	O
low	O
(	O
F=1.12	O
)	O
.	O
4	O
Therefore	O
,	O
it	O
was	O
later	O
extended	O
with	O
a	O
VMWE	O
discovery	O
module	O
.	O
Seen2Unseen	O
is	O
precisely	O
this	O
extended	O
system	O
.	O
It	O
relies	O
on	O
Seen2Seen	O
for	O
seen	O
VMWEs	O
and	O
on	O
discovery	O
methods	O
described	O
below	O
for	O
unseen	O
VMWEs	O
.	O

We	O
assume	O
that	O
seen	O
VMWEs	O
could	O
help	O
identify	O
unseen	O
ones	O
by	O
using	O
(	O
i	O
)	O
lexical	O
variation	O
,	O
tolerated	O
by	O
some	O
VMWEs	O
(	O
e.g.	O
take	O
a	O
bath	O
/	O
shower	O
)	O
,	O
and	O
(	O
ii	O
)	O
translation	O
,	O
e.g.	O
(	O
FR	O
)	O
prendre	O
décision	O
'	O
take	O
decision	O
'	O
=	O
(	O
PL	O
)	O
podejmować	O
decyzję	O
=	O
(	O
PT	O
)	O
tomar	O
decisão	O
=	O
(	O
SV	O
)	O
fatta	O
beslut	O
.	O
5	O
We	O
also	O
expect	O
seen	O
and	O
unseen	O
VMWEs	O
to	O
share	O
characteristics	O
,	O
such	O
as	O
the	O
distance	O
between	O
components	O
or	O
their	O
syntactic	O
dependency	O
relations	O
,	O
e.g.	O
nouns	O
often	O
being	O
objects	O
of	O
verbs	O
.	O
The	O
categories	O
that	O
should	O
benefit	O
from	O
our	O
strategy	O
are	O
,	O
mainly	O
,	O
light	O
-	O
verb	O
constructions	O
(	O
LVCs	O
)	O
containing	O
nouns	O
and	O
,	O
in	O
some	O
cases	O
,	O
verbal	O
idioms	O
(	O
VIDs	O
)	O
.	O
These	O
categories	O
are	O
universal	O
,	O
so	O
our	O
method	O
can	O
be	O
applied	O
to	O
the	O
14	O
languages	O
of	O
the	O
PST	O
.	O
Since	O
LVCs	O
are	O
often	O
verb	O
-	O
noun	O
pairs	O
,	O
Seen2Unseen	O
quasiexclusively	O
focuses	O
on	O
them	O
.	O
6	O
Consequently	O
,	O
we	O
do	O
not	O
aim	O
at	O
exhaustively	O
identifying	O
unseen	O
VMWEs	O
,	O
but	O
at	O
determining	O
to	O
what	O
extent	O
seen	O
verb	O
-	O
noun	O
VMWEs	O
can	O
help	O
us	O
discover	O
new	O
unseen	O
ones	O
.	O
Resources	O
In	O
addition	O
to	O
the	O
PST	O
Train	O
,	O
Dev	O
and	O
Test	O
corpora	O
,	O
we	O
used	O
the	O
CoNLL	O
2017	O
shared	O
task	O
parsed	O
corpora	O
,	O
hereafter	O
CoNLL	O
-	O
ST	O
(	O
Ginter	O
et	O
al	O
,	O
2017	O
)	O
.	O
7	O
The	O
CoNLL	O
-	O
ST	O
corpora	O
were	O
preferred	O
over	O
the	O
PST	O
-	O
provided	O
parsed	O
corpora	O
because	O
they	O
are	O
conveniently	O
released	O
with	O
pre	O
-	O
trained	O
100	O
-	O
dimensional	O
word2vec	O
embeddings	O
for	O
the	O
14	O
languages	O
of	O
the	O
PST	O
,	O
which	O
we	O
used	O
to	O
generate	O
lexical	O
variants	O
.	O
Additionally	O
,	O
we	O
used	O
a	O
free	O
library	O
to	O
implement	O
translation	O
towards	O
French	O
and	O
Italian	O
.	O
8	O
We	O
automatically	O
translated	O
all	O
VMWEs	O
in	O
the	O
other	O
13	O
languages	O
into	O
French	O
(	O
resp	O
.	O
Italian	O
)	O
,	O
privileged	O
due	O
to	O
the	O
availability	O
of	O
two	O
Wikitionary	O
-	O
based	O
lexicons	O
in	O
the	O
same	O
format	O
for	O
both	O
languages	O
.	O
3	O
In	O
this	O
paragraph	O
we	O
refer	O
to	O
macro	O
-	O
averaged	O
MWE	O
-	O
based	O
F	O
-	O
scores	O
.	O
4	O
The	O
score	O
is	O
not	O
null	O
due	O
to	O
different	O
implementations	O
of	O
unseen	O
VMWEs	O
in	O
the	O
evaluation	O
script	O
and	O
in	O
Seen2Seen	O
.	O
5	O
Languages	O
are	O
referred	O
to	O
with	O
their	O
PST	O
identifier	O
:	O
e.g.	O
FR	O
for	O
French	O
.	O
6	O
We	O
also	O
model	O
inherently	O
reflexive	O
verbs	O
with	O
cranberry	O
words	O
,	O
i.e.	O
verbs	O
which	O
never	O
occur	O
without	O
a	O
reflexive	O
pronoun	O
,	O
e.g.	O
(	O
FR	O
)	O
s'évanouir	O
vs.	O
*	O
évanouir	O
.	O
With	O
1	O
VMWE	O
discovered	O
in	O
Portuguese	O
and	O
3	O
in	O
French	O
,	O
this	O
module	O
is	O
omitted	O
here	O
.	O
7	O
http://hdl.handle.net/11234/1	O
-	O
1989	O
8	O
Googletrans	O
:	O
https://pypi.org/project/googletrans	O
,	O
implementing	O
the	O
Google	B-DatasetName
Translate	O
API	O
.	O
9	O
For	O
French	O
:	O
http://redac.univ	O
-	O
tlse2.fr/lexicons/glaff_en.html	O
,	O
for	O
Italian	O
:	O
http://redac	O
.	O
univ	O
-	O
tlse2.fr/lexiques/glaffit.html	O
10	O
In	O
case	O
of	O
multiple	O
POS	O
or	O
lemmas	O
,	O
the	O
most	O
frequent	O
verb	O
-	O
noun	O
combination	O
in	O
CoNLL	O
-	O
ST	O
was	O
selected	O
.	O
Unseen	O
VMWE	O
identification	O
To	O
support	O
identification	O
of	O
unseen	O
VMWEs	O
we	O
use	O
a	O
combination	O
of	O
semi	O
-	O
supervised	O
discovery	O
and	O
identification	O
methods	O
:	O
lexical	O
replacement	O
,	O
translation	O
and	O
statistical	O
ranking	O
.	O
For	O
a	O
language	O
L	O
,	O
let	O
SeenV	O
N	O
L	O
be	O
the	O
set	O
of	O
all	O
seen	O
LVC	O
and	O
VID	O
types	O
having	O
exactly	O
one	O
verb	O
and	O
one	O
noun	O
(	O
and	O
any	O
number	O
of	O
components	O
with	O
other	O
POS	O
tags	O
)	O
.	O
Let	O
each	O
type	O
in	O
SeenV	O
N	O
L	O
be	O
linked	O
with	O
its	O
manually	O
annotated	O
occurrences	O
in	O
Train	O
.	O
This	O
set	O
is	O
used	O
in	O
the	O
following	O
steps	O
:	O
2	O
Translation	B-TaskName
:	O
By	O
translating	O
seen	O
VMWE	O
types	O
in	O
one	O
language	O
we	O
obtain	O
a	O
list	O
of	O
VMWE	O
type	O
candidates	O
in	O
another	O
language	O
:	O
T	O
RAN	B-MethodName
S	O
L	O
is	O
built	O
only	O
for	O
French	O
and	O
Italian	O
,	O
and	O
is	O
empty	O
for	O
other	O
languages	O
.	O
T	O
RAN	B-MethodName
S	O
F	O
R	O
(	O
resp	O
.	O
T	O
RAN	B-MethodName
S	O
IT	O
)	O
contains	O
automatic	O
translations	O
of	O
each	O
VMWE	O
in	O
SeenV	O
N	O
L	O
,	O
with	O
L	O
=	O
FR	O
(	O
resp	O
.	O
L	O
=	O
IT	O
)	O
,	O
into	O
French	O
(	O
resp	O
.	O
Italian	O
)	O
.	O
We	O
eliminate	O
translations	O
which	O
do	O
not	O
contain	O
exactly	O
one	O
verb	O
and	O
one	O
noun	O
(	O
and	O
possible	O
components	O
of	O
other	O
POS	O
)	O
,	O
e.g.	O
due	O
to	O
a	O
wrong	O
translation	O
.	O
For	O
the	O
remaining	O
translations	O
,	O
we	O
keep	O
only	O
the	O
verb	O
and	O
the	O
noun	O
lemmas	O
.	O
3	O
Statistical	O
ranking	O
:	O
This	O
approach	O
is	O
based	O
on	O
statistical	O
characteristics	O
of	O
both	O
seen	O
VMWEs	O
and	O
unseen	O
VMWE	O
candidates	O
.	O
We	O
first	O
calculate	O
3	O
sets	O
of	O
features	O
for	O
the	O
whole	O
SeenV	O
N	O
L	O
list	O
:	O
Dist	O
L	O
is	O
the	O
maximal	O
verb	O
-	O
noun	O
distance	O
for	O
all	O
VMWE	O
tokens	O
occurring	O
at	O
least	O
twice	O
in	O
SeenV	O
N	O
L	O
.	O
This	O
should	O
help	O
eliminate	O
candidates	O
whose	O
components	O
are	O
too	O
distant	O
in	O
a	O
sentence	O
.	O
P	O
L	O
Dep	O
(	O
Dep	O
V	O
,	O
Dep	O
N	O
)	O
is	O
the	O
ratio	O
of	O
VMWE	O
tokens	O
in	O
SeenV	O
N	O
L	O
in	O
which	O
the	O
incoming	O
dependencies	O
of	O
the	O
verb	O
and	O
of	O
the	O
noun	O
are	O
Dep	O
V	O
and	O
Dep	O
N	O
.	O
For	O
instance	O
,	O
P	O
F	O
R	O
Dep	O
(	O
root	O
,	O
obj	O
)	O
is	O
higher	O
than	O
P	O
F	O
R	O
Dep	O
(	O
root	O
,	O
nsubj	O
)	O
because	O
,	O
in	O
French	O
,	O
active	O
voice	O
(	O
e.g.	O
rendre	O
une	O
visite	O
'	O
pay	O
a	O
visit	O
'	O
)	O
is	O
more	O
frequent	O
than	O
passive	O
voice	O
(	O
e.g.	O
malediction	O
fut	O
lancée	O
'	O
curse	O
was	O
cast	O
'	O
)	O
.	O
We	O
thus	O
favour	O
the	O
most	O
commonly	O
observed	O
VMWE	O
dependencies	O
.	O
P	O
L	O
Dist	O
(	O
i	O
)	O
is	O
the	O
ratio	O
of	O
VMWE	O
tokens	O
in	O
SeenV	O
N	O
L	O
in	O
which	O
the	O
number	O
of	O
words	O
inserted	O
between	O
the	O
verb	O
and	O
the	O
noun	O
is	O
i.	O
For	O
instance	O
,	O
P	O
F	O
R	O
Dist	O
(	O
0	B-DatasetName
)	O
=	O
0.46	O
,	O
i.e.	O
occurrences	O
in	O
which	O
the	O
verb	O
and	O
the	O
noun	O
are	O
contiguous	O
represent	O
46	O
%	O
of	O
SeenV	O
N	O
F	O
R	O
.	O
This	O
ratio	O
tends	O
to	O
decrease	O
as	O
i	O
increases	O
:	O
P	O
F	O
R	O
Dist	O
(	O
2	O
)	O
=	O
0.11	O
,	O
P	O
F	O
R	O
Dist	O
(	O
5	O
)	O
=	O
0.006	O
,	O
etc	O
.	O
Candidates	O
whose	O
number	O
of	O
intervening	O
words	O
i	O
has	O
higher	O
P	O
L	O
Dist	O
(	O
i	O
)	O
likely	O
are	O
true	O
VMWEs	O
.	O
Given	O
these	O
characteristics	O
of	O
seen	O
VMWEs	O
,	O
we	O
proceed	O
to	O
extracting	O
and	O
ranking	O
unseen	O
VMWE	O
candidates	O
.	O
Namely	O
,	O
Cand	O
L	O
is	O
the	O
list	O
of	O
all	O
occurrences	O
of	O
verb	O
-	O
noun	O
pairs	O
in	O
Test	O
such	O
that	O
:	O
(	O
i	O
)	O
the	O
verb	O
and	O
the	O
noun	O
are	O
directly	O
connected	O
by	O
a	O
syntactic	O
dependency	O
,	O
(	O
ii	O
)	O
the	O
distance	O
between	O
the	O
verb	O
and	O
the	O
noun	O
does	O
not	O
exceed	O
Dist	O
L	O
,	O
and	O
(	O
iii	O
)	O
the	O
verb	O
and	O
the	O
noun	O
never	O
co	O
-	O
occur	O
with	O
a	O
direct	O
dependency	O
link	O
in	O
Train	O
or	O
in	O
Dev	O
.	O
The	O
latter	O
condition	O
excludes	O
both	O
seen	O
VMWEs	O
(	O
already	O
covered	O
by	O
Seen2Seen	O
)	O
and	O
verb	O
-	O
noun	O
constructions	O
not	O
annotated	O
as	O
VMWEs	O
in	O
Train	O
or	O
Dev	O
,	O
i.e.	O
being	O
no	O
VMWEs	O
,	O
e.g.	O
(	O
FR	O
)	O
avoir	O
an	O
'	O
have	O
year	O
'	O
in	O
elle	O
a	O
quinze	O
ans	O
'	O
she	O
is	O
15	O
years	O
old	O
'	O
.	O
Cand	O
L	O
is	O
then	O
ranked	O
by	O
considering	O
statistical	O
properties	O
.	O
For	O
each	O
candidate	O
c	O
in	O
Cand	O
L	O
,	O
we	O
calculate	O
three	O
measures	O
:	O
P	O
(	O
c	O
)	O
is	O
the	O
estimated	O
joint	O
dependency	O
-	O
and	O
distance	O
-	O
based	O
probability	O
.	O
Suppose	O
that	O
i	O
is	O
the	O
number	O
of	O
words	O
inserted	O
between	O
c	O
's	O
verb	O
and	O
noun	O
,	O
and	O
their	O
incoming	O
dependencies	O
are	O
Dep	O
V	O
and	O
Dep	O
N	O
,	O
respectively	O
.	O
Then	O
,	O
P	O
(	O
c	O
)	O
=	O
21	O
)	O
0	B-DatasetName
(	O
0	B-DatasetName
)	O
0	B-DatasetName
(	O
0	B-DatasetName
)	O
0	B-DatasetName
(	O
0	B-DatasetName
)	O
SIM	O
L	O
0	B-DatasetName
(	O
0	B-DatasetName
)	O
0	B-DatasetName
(	O
0	B-DatasetName
)	O
0.45	O
(	O
11	O
)	O
0.17	O
(	O
6	O
)	O
0	B-DatasetName
(	O
0	B-DatasetName
)	O
0	B-DatasetName
(	O
0	B-DatasetName
)	O
0	B-DatasetName
(	O
0	B-DatasetName
)	O
RAN	B-MethodName
K	O
L	O
0.19	O
(	O
101	O
AM	O
I	O
(	O
c	O
)	O
is	O
the	O
augmented	O
mutual	O
information	O
of	O
c	O
's	O
type	O
in	O
the	O
CoNLL	O
-	O
ST	O
corpus	O
.	O
MWEs	O
are	O
known	O
to	O
have	O
a	O
Zipfian	O
distribution	O
and	O
to	O
often	O
mix	O
very	O
frequent	O
words	O
with	O
very	O
rare	O
ones	O
.	O
AMI	O
is	O
designed	O
specifically	O
to	O
address	O
this	O
phenomenon	O
,	O
so	O
as	O
to	O
leverage	O
the	O
rarely	O
occurring	O
expressions	O
or	O
components	O
(	O
Zhang	O
et	O
al	O
,	O
2009	O
)	O
:	O
AM	O
I	O
(	O
x	O
,	O
y	O
)	O
=	O
log	O
2	O
c	O
)	O
.	O
Cand	O
L	O
is	O
then	O
ranked	O
by	O
RR	B-DatasetName
(	O
c	O
)	O
.	O
We	O
keep	O
n	O
top	O
-	O
ranked	O
candidates	O
,	O
where	O
n	O
is	O
estimated	O
by	O
scaling	O
the	O
number	O
(	O
provided	O
the	O
organizers	O
)	O
of	O
VIDs	O
and	O
LVCs	O
in	O
Test	O
-	O
when	O
all	O
the	O
expressions	O
annotated	O
as	O
seen	O
during	O
the	O
Seen2Seen	O
phase	O
have	O
been	O
eliminated	O
-	O
by	O
the	O
recall	O
of	O
our	O
method	O
on	O
Dev	O
on	O
the	O
target	O
constructions	O
(	O
unseen	O
verb	O
-	O
noun	O
LVCs	O
and	O
VIDs	O
)	O
.	O
12	O
This	O
n	O
-	O
best	O
list	O
is	O
called	O
RAN	B-MethodName
K	O
L	O
n	O
.	O
4	O
Identification	O
proper	O
:	O
In	O
step	O
3	O
we	O
obtain	O
a	O
list	O
of	O
unseen	O
VMWE	O
candidate	O
tokens	O
Cand	O
L	O
extracted	O
from	O
Test	O
.	O
The	O
aim	O
of	O
identification	O
is	O
to	O
discriminate	O
among	O
true	O
and	O
false	O
VMWEs	O
on	O
this	O
list	O
.	O
Statistical	O
ranking	O
and	O
retaining	O
top	O
-	O
n	O
candidates	O
is	O
one	O
possible	O
statistically	O
-	O
based	O
criterion	O
.	O
But	O
we	O
hypothesise	O
that	O
some	O
candidates	O
whose	O
rank	O
is	O
worse	O
than	O
n	O
,	O
notably	O
due	O
to	O
data	O
sparseness	O
,	O
can	O
still	O
be	O
correct	O
if	O
they	O
result	O
from	O
lexical	O
replacement	O
or	O
translation	O
of	O
seen	O
VMWEs	O
.	O
Therefore	O
,	O
every	O
c	O
in	O
Cand	O
L	O
is	O
annotated	O
as	O
an	O
LVC	O
if	O
c	O
belongs	O
to	O
RAN	B-MethodName
K	O
L	O
n	O
or	O
if	O
c	O
's	O
type	O
belongs	O
to	O
M	O
IX	O
L	O
∪	O
SIM	O
L	O
∪	O
T	O
RAN	B-MethodName
S	O
L	O
.	O
P	O
L	O
Dep	O
(	O
Dep	O
V	O
,	O
Dep	O
N	O
)	O
×	O
P	O
L	O
Dist	O
(	O
i	O
)	O
.	O
P	O
(	O
x	O
,	O
y	O
)	O
P	O
(	O
x	O
)	O
P	O
(	O
y	O
)	O
(	O
1−	O
P	O
(	O
x	O
,	O
y	O
)	O
P	O
(	O
x	O
)	O
)	O
(	O
1−	O
P	O
(	O
x	O
,	O
y	O
)	O
P	O
(	O
y	O
)	O
)	O
RR	B-DatasetName
(	O
c	O
)	O

Although	O
Seen2Unseen	O
uses	O
4	O
lists	O
of	O
candidates	O
,	O
here	O
we	O
analyse	O
their	O
contribution	O
separately	O
,	O
that	O
is	O
,	O
we	O
use	O
one	O
list	O
at	O
a	O
time	O
in	O
step	O
4	O
above	O
.	O
We	O
report	O
unseen	O
MWE	O
/	O
token	O
-	O
based	O
precision	O
.	O
13	O
Sec	O
.	O
3.1	O
analyses	O
the	O
impact	O
of	O
M	O
IX	O
L	O
,	O
SIM	O
L	O
and	O
RAN	B-MethodName
K	O
L	O
n	O
,	O
while	O
Sec	O
.	O
3.2	O
discusses	O
T	O
RAN	B-MethodName
S	O
L	O
for	O
French	O
.	O
3.1	O
Impact	O
of	O
M	O
IX	O
L	O
,	O
SIM	O
L	O
and	O
RAN	B-MethodName
K	O
L	O
n	O
As	O
shown	O
in	O
Table	O
1	O
,	O
using	O
M	O
IX	O
L	O
alone	O
leads	O
to	O
precision	O
values	O
above	O
0.29	O
for	O
7	O
languages	O
out	O
of	O
14	O
.	O
Conversely	O
,	O
RAN	B-MethodName
K	O
L	O
alone	O
mostly	O
leads	O
to	O
values	O
below	O
0.22	O
(	O
except	O
for	O
Hindi	O
with	O
P	O
=	O
0.46	O
)	O
.	O
The	O
precision	O
using	O
SIM	O
L	O
alone	O
reaches	O
a	O
maximum	O
of	O
0.45	O
for	O
Basque	O
.	O
The	O
error	O
analysis	O
below	O
suggests	O
ways	O
to	O
improve	O
precision	O
.	O
In	O
French	O
,	O
using	O
M	O
IX	O
F	O
R	O
alone	O
yields	O
21	O
candidates	O
in	O
Test	O
.	O
Among	O
the	O
5	O
false	O
positives	O
,	O
there	O
is	O
one	O
literal	O
reading	O
(	O
faire	O
dessin	O
'	O
make	O
drawing	O
'	O
)	O
,	O
one	O
omitted	O
VMWE	O
(	O
recevoir	O
aide	O
'	O
receive	O
help	O
'	O
)	O
and	O
three	O
other	O
verb	O
-	O
noun	O
pairs	O
that	O
could	O
have	O
been	O
disregarded	O
(	O
being	O
coincidental	O
occurrences	O
)	O
if	O
we	O
had	O
taken	O
into	O
account	O
not	O
only	O
the	O
existence	O
of	O
the	O
syntactic	O
dependency	O
but	O
also	O
its	O
nature	O
(	O
e.g.	O
nous	O
avons	O
VERB	O
cinq	O
points	O
à	O
l'ordre	O
NOUN.xcomp	O
du	O
jour	O
'	O
we	O
have	O
five	O
items	O
on	O
the	O
agenda	O
'	O
)	O
.	O
This	O
major	O
problem	O
for	O
M	O
IX	O
L	O
is	O
shared	O
by	O
SIM	O
L	O
,	O
but	O
a	O
specific	O
drawback	O
with	O
SIM	O
L	O
is	O
that	O
not	O
all	O
words	O
that	O
occur	O
in	O
similar	O
contexts	O
are	O
actually	O
similar	O
.	O
Indeed	O
,	O
we	O
obtain	O
relevant	O
generated	O
unseen	O
verb	O
-	O
noun	O
pairs	O
,	O
including	O
synonyms	O
,	O
antonyms	O
and	O
hyponyms	O
,	O
but	O
also	O
irrelevant	O
ones	O
.	O
We	O
should	O
therefore	O
either	O
use	O
more	O
reliable	O
resources	O
,	O
such	O
as	O
synonym	O
/	O
antonym	O
dictionaries	O
,	O
and/or	O
disregard	O
frequent	O
verbs	O
(	O
to	O
have	O
,	O
to	O
do	O
,	O
etc	O
.	O
)	O
.	O
For	O
these	O
frequent	O
verbs	O
,	O
the	O
more	O
reliable	O
equivalences	O
obtained	O
by	O
M	O
IX	O
L	O
compared	O
to	O
SIM	O
L	O
should	O
be	O
preferred	O
(	O
faire	O
'	O
do	O
'	O
M	O
IX	O
F	O
R	O
=	O
subir	O
'	O
suffer	O
'	O
vs.	O
faire	O
'	O
do	O
'	O
SIM	O
F	O
R	O
=	O
passer	O
'	O
pass	O
'	O
)	O
.	O
Indeed	O
,	O
as	O
shown	O
in	O
Table	O
1	O
,	O
over	O
5	O
languages	O
with	O
M	O
IX	O
L	O
and	O
SIM	O
L	O
candidates	O
,	O
4	O
exhibit	O
a	O
better	O
precision	O
and	O
higher	O
number	O
of	O
candidates	O
for	O
M	O
IX	O
L	O
.	O
In	O
French	O
,	O
by	O
dividing	O
n	O
by	O
4	O
in	O
RAN	B-MethodName
K	O
F	O
R	O
n	O
,	O
the	O
precision	O
would	O
have	O
increased	O
from	O
0.19	O
to	O
0.45	O
(	O
18	O
VMWEs	O
over	O
40	O
candidates	O
)	O
.	O
In	O
other	O
words	O
,	O
using	O
RAN	B-MethodName
K	O
L	O
n	O
in	O
step	O
4	O
can	O
slightly	O
increase	O
recall	O
but	O
causes	O
a	O
drop	O
in	O
precision	O
,	O
unless	O
n	O
is	O
low	O
.	O
Hindi	O
appears	O
as	O
an	O
exception	O
:	O
no	O
negative	O
impact	O
is	O
observed	O
with	O
RAN	B-MethodName
K	O
HI	O
n	O
due	O
to	O
a	O
bias	O
in	O
the	O
corpora	O
(	O
compound	O
mentioned	O
in	O
the	O
dependency	O
label	O
)	O
.	O
3.2	O
Impact	O
of	O
T	O
RAN	B-MethodName
S	O
L	O
:	O
(	O
IT	O
)	O
Traduttore	O
,	O
traditore	O
'	O
translator	O
,	O
traitor	O
'	O
?	O
With	O
translational	O
equivalences	O
,	O
we	O
hypothesized	O
that	O
T	O
RAN	B-MethodName
S	O
L	O
would	O
lead	O
to	O
situations	O
such	O
as	O
:	O
exact	O
matches	O
:	O
(	O
PT	O
)	O
cometer	O
crime	O
'	O
commit	O
a	O
crime	O
'	O
(	O
FR	O
)	O
commettre	O
crime	O
,	O
partial	O
matches	O
leading	O
to	O
VMWEs	O
nonetheless	O
:	O
(	O
PT	O
)	O
causar	O
problema	O
'cause	O
problem	O
'	O
(	O
FR	O
)	O
causer	O
ennui	O
,	O
instead	O
of	O
causer	O
problème	O
,	O
no	O
match	O
,	O
but	O
another	O
VMWE	O
:	O
(	O
PT	O
)	O
ter	O
destaque	O
'	O
highlight	O
'	O
(	O
FR	O
)	O
mettre	O
en	O
évidence	O
.	O
literal	O
,	O
non	O
-	O
fluent	O
or	O
ambiguous	O
translations	O
(	O
Constant	O
et	O
al	O
,	O
2017	O
)	O
:	O
(	O
PT	O
)	O
jogar	O
o	O
toalha	O
'	O
throw	O
the	O
towel	O
'	O
⇒'give	O
up	O
'	O
(	O
FR	O
)	O
jeter	O
la	O
serviette	O
instead	O
of	O
jeter	O
l'éponge	O
'	O
throw	O
the	O
sponge	O
'	O
,	O
non	O
-	O
existing	O
VMWEs	O
in	O
the	O
target	O
language	O
:	O
(	O
TR	O
)	O
el	O
atma	O
(	O
FR	O
)	O
lancer	O
main	O
'	O
throw	O
hand	O
'	O
We	O
focus	O
on	O
French	O
due	O
to	O
the	O
high	O
number	O
of	O
candidates	O
in	O
T	O
RAN	B-MethodName
S	O
F	O
R	O
.	O
In	O
Test	O
-	O
FR	O
,	O
among	O
the	O
44	O
annotated	O
verb	O
-	O
noun	O
candidates	O
using	O
T	O
RAN	B-MethodName
S	O
F	O
R	O
alone	O
,	O
18	O
are	O
actually	O
VMWEs	O
and	O
3	O
partially	O
correspond	O
to	O
VMWEs	O
due	O
to	O
omitted	O
determiners	O
,	O
yielding	O
an	O
unseen	O
MWE	O
-	O
based	O
precision	O
of	O
0.41	O
and	O
an	O
unseen	O
token	O
-	O
based	O
precision	O
value	O
of	O
0.48	O
.	O
These	O
21	O
candidates	O
are	O
mainly	O
provided	O
by	O
Greek	O
(	O
10	O
vs.	O
6	O
from	O
PT	O
and	O
0	B-DatasetName
from	O
IT	O
or	O
RO	O
)	O
.	O
Thus	O
,	O
the	O
size	O
of	O
the	O
training	O
corpora	O
may	O
have	O
more	O
influence	O
on	O
the	O
probability	O
to	O
obtain	O
good	O
translations	O
than	O
the	O
source	O
language	O
family	O
.	O
The	O
23	O
false	O
positives	O
include	O
(	O
i	O
)	O
13	O
candidates	O
that	O
can	O
be	O
VMWEs	O
or	O
not	O
depending	O
on	O
the	O
context	O
,	O
including	O
coincidental	O
co	O
-	O
occurrences	O
,	O
literal	O
readings	O
and	O
errors	O
in	O
the	O
manually	O
annotated	O
reference	O
Test	O
corpus	O
,	O
and	O
(	O
ii	O
)	O
10	O
candidates	O
that	O
are	O
not	O
VMWEs	O
,	O
whatever	O
the	O
context	O
,	O
e.g.	O
the	O
inchoative	O
commencer	O
recherche	O
'	O
start	O
research	O
'	O
(	O
from	O
Hebrew	O
)	O
or	O
payer	O
taxe	O
'	O
pay	O
tax	O
'	O
(	O
from	O
(	O
PL	O
)	O
uiszczać	O
opłatę	O
)	O
.	O
Consequently	O
,	O
translation	O
may	O
be	O
a	O
clue	O
to	O
discover	O
unseen	O
VMWEs	O
,	O
since	O
78	O
%	O
of	O
Cand	O
F	O
R	O
∩	O
T	O
RAN	B-MethodName
S	O
F	O
R	O
are	O
VMWEs	O
out	O
of	O
context	O
,	O
but	O
barely	O
half	O
of	O
them	O
were	O
manually	O
annotated	O
in	O
context	O
.	O
As	O
highlighted	O
above	O
,	O
a	O
restriction	O
to	O
the	O
most	O
frequent	O
VMWE	O
syntactic	O
relations	O
could	O
help	O
filter	O
out	O
coincidental	O
occurrences	O
corresponding	O
to	O
39	O
%	O
of	O
false	O
positives	O
(	O
e.g.	O
lancer	O
la	O
balle	O
à	O
la	O
main	O
OBL	O
:	O
MOD	B-DatasetName
'	O
throw	O
the	O
ball	O
with	O
the	O
hand	O
'	O
)	O
.	O

We	O
proposed	O
an	O
error	O
analysis	O
for	O
our	O
system	O
Seen2Unseen	O
dedicated	O
to	O
unseen	O
verb	O
-	O
noun	O
VMWE	O
identification	O
.	O
It	O
reveals	O
that	O
lexical	O
variation	O
and	O
translation	O
can	O
produce	O
valid	O
unseen	O
VMWEs	O
but	O
their	O
ambiguity	O
in	O
context	O
must	O
be	O
solved	O
:	O
we	O
should	O
take	O
into	O
account	O
both	O
the	O
dependency	O
labels	O
(	O
to	O
avoid	O
coincidental	O
occurrences	O
)	O
and	O
the	O
probability	O
of	O
the	O
verb	O
to	O
be	O
light	O
in	O
Train	O
(	O
to	O
avoid	O
frequent	O
co	O
-	O
ocurrences	O
like	O
fumer	O
cigarette	O
'	O
smoke	O
cigarette	O
'	O
)	O
.	O
Using	O
contextual	O
rather	O
than	O
non	O
-	O
contextual	O
word	B-TaskName
embeddings	I-TaskName
might	O
also	O
be	O
helpful	O
,	O
even	O
if	O
computationally	O
more	O
intensive	O
.	O
We	O
could	O
also	O
combine	O
T	O
RAN	B-MethodName
S	O
L	O
and	O
M	O
IX	O
L	O
∪	O
SIM	O
L	O
by	O
applying	O
lexical	O
substitution	O
to	O
the	O
translated	O
VMWEs	O
.	O

We	O
examine	O
three	O
standard	O
datasets	O
in	O
our	O
experiments	O
.	O
Two	O
have	O
binary	O
class	O
labels	O
(	O
Yelp	O
,	O
IMDB	B-DatasetName
)	O
and	O
the	O
third	O
has	O
multi	O
class	O
labels	O
(	O
AG	B-DatasetName
News	I-DatasetName
)	O
.	O
These	O
have	O
been	O
used	O
in	O
adversarial	O
generation	O
and	O
defense	O
research	O
(	O
Zeng	O
et	O
al	O
,	O
2021	O
;	O
Li	O
et	O
al	O
,	O
2020	O
)	O
.	O
All	O
datasets	O
can	O
be	O
found	O
via	O
huggingface	O
2	O
.	O
1	O
.	O
IMDB	B-DatasetName
-	O
Movie	O
review	O
dataset	O
for	O
binary	O
sentiment	O
classification	O
.	O
25k	O
examples	O
are	O
provided	O
for	O
training	O
and	O
testing	O
respectively	O
.	O
2	O
.	O
Yelp	O
-	O
Yelp	O
dataset	O
for	O
binary	O
sentiment	O
classification	O
on	O
reviews	O
of	O
businesses	O
extracted	O
from	O
the	O
Yelp	O
Dataset	O
Challenge	O
3	O
.	O
560k	O
examples	O
are	O
provided	O
for	O
training	O
and	O
38k	O
for	O
testing	O
.	O
3	O
.	O
AG	B-DatasetName
News	I-DatasetName
-	O
News	O
articles	O
from	O
over	O
2000	O
news	O
sources	O
annotated	O
by	O
type	O
of	O
news	O
:	O
Sports	O
,	O
World	O
,	O
Business	O
,	O
and	O
Science	O
/	O
Tech	O
.	O
120k	O
training	O
and	O
7k	O
test	O
sets	O
are	O
provided	O
.	O
Following	O
previous	O
research	O
,	O
(	O
Li	O
et	O
al	O
,	O
2020	O
;	O
we	O
use	O
all	O
training	O
data	O
,	O
and	O
evaluate	O
our	O
method	O
on	O
random	O
1k	O
samples	O
of	O
each	O
dataset	O
for	O
the	O
case	O
where	O
the	O
local	O
classifier	O
does	O
not	O
employ	O
Sample	O
Shielding	O
.	O
Due	O
to	O
the	O
high	O
amount	O
of	O
queries	O
used	O
by	O
the	O
adversaries	O
,	O
we	O
test	O
on	O
a	O
subset	O
of	O
100	O
samples	O
for	O
the	O
case	O
where	O
the	O
attacker	O
's	O
local	O
classifier	O
employs	O
Sample	O
Shielding	O
.	O
4	O

We	O
test	O
our	O
text	O
classifier	O
shielding	O
strategy	O
against	O
4	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
(	O
SOTA	O
)	O
text	O
classifier	O
attack	O
algorithms	O
.	O
These	O
algorithms	O
have	O
shown	O
excellent	O
performance	O
in	O
causing	O
misclassifications	O
while	O
still	O
producing	O
readable	O
texts	O
.	O
We	O
defend	O
against	O
3	O
word	O
based	O
attacks	O
:	O
TextFooler	O
,	O
Bert	O
-	O
Attack	O
(	O
Li	O
et	O
al	O
,	O
2020	O
)	O
,	O
PWWS	O
(	O
Ren	O
et	O
al	O
,	O
2019	O
)	O
.	O
TextFooler	O
leverages	O
word	B-TaskName
embeddings	I-TaskName
for	O
word	O
replacements	O
,	O
Bert	O
-	O
Attack	O
leverages	O
BERT	B-MethodName
itself	O
by	O
masking	O
words	O
and	O
using	O
BERT	B-MethodName
suggestions	O
,	O
PWWS	O
selects	O
and	O
weights	O
word	O
replacements	O
from	O
WordNet	O
.	O
All	O
three	O
use	O
some	O
form	O
of	O
greedy	O
selection	O
for	O
determining	O
which	O
words	O
to	O
replace	O
.	O
We	O
also	O
defend	O
against	O
a	O
character	O
based	O
attack	O
algorithm	O
,	O
TextBugger	O
(	O
Li	O
et	O
al	O
,	O
2019	O
)	O
.	O

We	O
test	O
our	O
shielding	O
approach	O
against	O
3	O
standard	O
classifiers	O
5	O
used	O
in	O
previous	O
research	O
,	O
e.g.	O
(	O
Li	O
et	O
al	O
,	O
2021a	O
;	O
Li	O
et	O
al	O
,	O
2020	O
)	O
:	O
1	O
.	O
CNN	O
-	O
A	O
word	O
based	O
CNN	O
(	O
Kim	O
,	O
2014	O
)	O
,	O
with	O
three	O
window	O
sizes	O
(	O
3	O
,	O
4	O
,	O
5	O
)	O
,	O
100	O
filters	O
per	O
window	O
with	O
dropout	O
of	O
0.3	O
and	O
Glove	B-MethodName
embeddings	I-MethodName
.	O
2	O
.	O
LSTM	B-MethodName
-	O
A	O
word	O
based	O
bidirectional	B-MethodName
LSTM	I-MethodName
with	O
150	O
hidden	O
units	O
.	O
As	O
with	O
the	O
CNN	O
a	O
dropout	O
of	O
0.3	O
is	O
used	O
and	O
Glove	B-MethodName
embeddings	I-MethodName
are	O
leveraged	O
.	O
3	O
.	O
BERT	B-MethodName
-	O
The	O
12	O
layer	O
BERT	B-MethodName
base	O
model	O
which	O
has	O
been	O
fine	O
-	O
tuned	O
on	O
the	O
corresponding	O
dataset	O
.	O
These	O
are	O
provided	O
by	O
textattack	O
via	O
huggingface	O
6	O
.	O

Results	O
are	O
in	O
Table	O
3	O
.	O
As	O
in	O
the	O
previous	O
condition	O
,	O
classifiers	O
perform	O
well	O
on	O
original	O
texts	O
(	O
Table	O
1	O
)	O
with	O
BERT	B-MethodName
often	O
achieving	O
the	O
highest	O
accuracies	O
.	O
In	O
this	O
setting	O
,	O
every	O
query	O
by	O
an	O
attacker	O
requires	O
k	O
samples	O
to	O
be	O
processed	O
,	O
which	O
greatly	O
increases	O
attack	O
time	O
.	O
Thus	O
,	O
we	O
reduce	O
k	O
to	O
30	O
for	O
these	O
experiments	O
.	O
Sample	O
Shielding	O
repels	O
attacks	O
even	O
when	O
attacker	O
uses	O
Sample	O
Shielding	O
.	O
We	O
see	O
that	O
shielding	O
is	O
extremely	O
successful	O
in	O
almost	O
completely	O
removing	O
the	O
negative	O
effects	O
of	O
the	O
attacks	O
.	O
For	O
example	O
,	O
on	O
the	O
IMDB	B-DatasetName
-	O
TextFooler	O
combination	O
,	O
attack	O
success	O
rate	O
drops	O
from	O
100	O
to	O
5	O
for	O
LSTM	B-MethodName
,	O
100	O
to	O
1	O
for	O
CNN	O
,	O
and	O
99	O
to	O
6	O
against	O
BERT	B-MethodName
.	O
The	O
largest	O
protection	O
provided	O
by	O
Sample	O
Shielding	O
(	O
100	O
%	O
)	O
is	O
for	O
TextBugger	O
vs	O
CNN	O
in	O
IMDB	B-DatasetName
.	O
The	O
smallest	O
is	O
for	O
85	O
%	O
(	O
PWWS	O
vs	O
LSTM	B-MethodName
)	O
.	O
On	O
average	O
the	O
protection	O
is	O
88.8	O
%	O
.	O
The	O
recovered	O
accuracies	O
are	O
only	O
13	O
to	O
0	B-DatasetName
percent	O
away	O
from	O
the	O
originals	O
.	O
These	O
results	O
show	O
the	O
power	O
of	O
Sample	O
Shielding	O
as	O
even	O
with	O
knowledge	O
of	O
both	O
the	O
classifier	O
and	O
Sample	O
Shielding	O
,	O
attacks	O
struggle	O
to	O
perturb	O
the	O
text	O
in	O
a	O
manner	O
that	O
causes	O
W	O
to	O
fail	O
.	O
Furthermore	O
,	O
the	O
attacks	O
do	O
worse	O
with	O
feedback	O
from	O
Sample	O
Shielding	O
.	O
This	O
shows	O
the	O
misleading	O
nature	O
of	O
feedback	O
from	O
Sample	O
Shielding	O
,	O
and	O
unreliability	O
when	O
guiding	O
attacks	O
.	O
5	O
Additional	O
Analysis	O

Due	O
to	O
the	O
randomness	O
of	O
samples	O
,	O
there	O
may	O
be	O
concern	O
over	O
the	O
consistency	O
of	O
Sample	O
Shielding	O
.	O
To	O
address	O
this	O
,	O
we	O
ran	O
Sample	O
Shielding	O
100	O
times	O
on	O
the	O
IMDB	B-DatasetName
attacked	O
texts	O
from	O
Table	O
3	O
against	O
BERT	B-MethodName
classifier	O
.	O
Each	O
time	O
30	O
random	O
samples	O
were	O
used	O
to	O
vote	O
.	O
As	O
can	O
be	O
observed	O
from	O
Figure	O
8	O
,	O
Sample	O
Shielding	O
consistently	O
protects	O
against	O
attacks	O
.	O
Median	O
accuracies	O
are	O
above	O
80	O
%	O
dropping	O
only	O
to	O
75	O
%	O
in	O
the	O
worst	O
case	O
.	O
This	O
points	O
to	O
Sample	O
Shielding	O
as	O
a	O
consistent	O
,	O
reliable	O
defense	O
.	O

Comparisons	O
are	O
limited	O
as	O
threat	O
models	O
differ	O
.	O
As	O
noted	O
earlier	O
,	O
other	O
defenses	O
assume	O
a	O
weaker	O
threat	O
model	O
where	O
the	O
attacker	O
queries	O
the	O
web	O
-	O
site	O
's	O
shielded	O
W	O
directly	O
.	O
To	O
make	O
ours	O
equivalent	O
we	O
compare	O
SOTA	O
results	O
with	O
our	O
accuracies	O
obtained	O
by	O
the	O
attacker	O
using	O
W	O
′	O
alone	O
(	O
with	O
W	O
=	O
W	O
′	O
)	O
.	O
We	O
calculate	O
accuracies	O
right	O
after	O
the	O
final	O
perturbed	O
text	O
is	O
generated	O
using	O
W	O
′	O
eliminating	O
a	O
followup	B-DatasetName
round	O
of	O
W	O
with	O
Sample	O
Shielding	O
.	O
3	O
)	O
.	O
While	O
we	O
do	O
not	O
know	O
how	O
FreeLB++	O
,	O
RanMask	O
,	O
and	O
similar	O
defenses	O
would	O
perform	O
with	O
our	O
threat	O
model	O
any	O
deterministic	O
shield	O
would	O
give	O
the	O
exact	O
same	O
results	O
when	O
the	O
classifier	O
is	O
applied	O
once	O
again	O
by	O
the	O
website	O
.	O

First	O
,	O
in	O
future	O
work	O
we	O
will	O
add	O
in	O
direct	O
comparisons	O
to	O
the	O
two	O
closest	O
methods	O
to	O
Sample	O
Shielding	O
(	O
Zeng	O
et	O
al	O
,	O
2021	O
;	O
Wang	O
et	O
al	O
,	O
2021a	O
)	O
.	O
They	O
are	O
similar	O
in	O
spirit	O
as	O
they	O
also	O
work	O
off	O
samples	O
though	O
these	O
are	O
generated	O
differently	O
.	O
We	O
have	O
not	O
compared	O
with	O
them	O
because	O
these	O
two	O
papers	O
appeared	O
very	O
recently	O
,	O
one	O
last	O
revised	O
in	O
July	O
(	O
Zeng	O
et	O
al	O
,	O
2021	O
)	O
and	O
the	O
other	O
appeared	O
in	O
arXiv	B-DatasetName
in	O
September	O
2021	O
(	O
Wang	O
et	O
al	O
,	O
2021a	O
)	O
.	O
Second	O
,	O
the	O
neural	O
net	O
summarizer	O
leverages	O
a	O
simple	O
linear	B-MethodName
layer	I-MethodName
.	O
Other	O
networks	O
,	O
e.g.	O
,	O
LSTM	B-MethodName
,	O
maybe	O
better	O
at	O
finding	O
patterns	O
in	O
sequential	O
data	O
.	O
In	O
future	O
work	O
we	O
will	O
also	O
explore	O
layering	O
Sample	O
Shielding	O
onto	O
other	O
defense	O
strategies	O
.	O
Another	O
limitation	O
of	O
our	O
current	O
method	O
is	O
that	O
we	O
do	O
not	O
measure	O
Sample	O
Shielding	O
's	O
effectiveness	O
on	O
other	O
common	O
text	O
tasks	O
including	O
Natural	B-TaskName
Language	I-TaskName
Understanding	I-TaskName
.	O
Additionally	O
,	O
datasets	O
which	O
contain	O
the	O
shortest	O
texts	O
(	O
e.g.	O
not	O
currently	O
tested	O
in	O
our	O
experiments	O
.	O
Since	O
sample	O
shielding	O
removes	O
texts	O
,	O
it	O
's	O
performance	O
could	O
drop	O
for	O
these	O
tasks	O
and	O
short	O
texts	O
.	O
Thus	O
,	O
future	O
work	O
will	O
include	O
these	O
comparisons	O
.	O

Defenses	O
using	O
voting	O
.	O
The	O
most	O
similar	O
methods	O
to	O
our	O
own	O
are	O
RanMask	O
and	O
RS&V	O
both	O
appearing	O
within	O
the	O
last	O
five	O
months	O
.	O
RanMask	O
(	O
Zeng	O
et	O
al	O
,	O
2021	O
)	O
randomly	O
masks	O
tokens	O
in	O
input	O
texts	O
.	O
This	O
random	O
masking	O
occurs	O
n	O
times	O
generating	O
n	O
inputs	O
to	O
be	O
fed	O
to	O
a	O
classifier	O
.	O
RS&V	O
(	O
Wang	O
et	O
al	O
,	O
2021a	O
)	O
randomly	O
replaces	O
words	O
in	O
the	O
input	O
with	O
synonyms	O
.	O
This	O
it	O
does	O
k	O
times	O
to	O
produce	O
k	O
samples	O
which	O
are	O
then	O
voted	O
on	O
.	O
If	O
the	O
samples	O
vote	O
for	O
a	O
different	O
label	O
than	O
the	O
label	O
produced	O
by	O
the	O
unsampled	O
input	O
,	O
then	O
the	O
text	O
is	O
labeled	O
as	O
an	O
adversarial	B-TaskName
text	I-TaskName
.	O
Our	O
method	O
is	O
advantageous	O
since	O
it	O
does	O
not	O
rely	O
on	O
specific	O
models	O
(	O
i.e.	O
Masked	O
Language	O
Model	O
)	O
or	O
synonym	O
sources	O
.	O
Adversarial	O
training	O
.	O
Classifiers	O
train	O
on	O
perturbed	O
data	O
,	O
learning	O
to	O
identify	O
modified	O
versions	O
of	O
the	O
original	O
input	O
(	O
Wang	O
and	O
Wang	O
,	O
2020	O
;	O
Wang	O
et	O
al	O
,	O
2021b	O
;	O
Zhu	O
et	O
al	O
,	O
2020	O
;	O
Li	O
et	O
al	O
,	O
2021b	O
)	O
.	O
As	O
an	O
example	O
,	O
Gil	O
et	O
al	O
(	O
2019	O
)	O
propose	O
HotFlip	O
which	O
uses	O
white	O
-	O
box	O
knowledge	O
to	O
generate	O
adversarial	O
attacks	O
to	O
train	O
on	O
.	O
Specifically	O
,	O
they	O
flip	O
tokens	O
based	O
on	O
the	O
gradients	O
of	O
the	O
one	O
-	O
hot	O
input	O
vectors	O
.	O
However	O
,	O
adversarial	O
defenses	O
are	O
limited	O
to	O
known	O
attackers	O
.	O
In	O
contrast	O
,	O
Sample	O
Shielding	O
is	O
'	O
plug	O
-	O
and	O
-	O
play	O
'	O
as	O
it	O
is	O
a	O
pre	O
-	O
processing	O
step	O
.	O
Other	O
defenses	O
.	O
Several	O
other	O
shielding	O
methods	O
exist	O
(	O
Keller	O
et	O
al	O
,	O
2021	O
;	O
Eger	O
et	O
al	O
,	O
2019	O
;	O
Zhu	O
et	O
al	O
,	O
2021	O
)	O
.	O
For	O
example	O
,	O
Rodriguez	O
and	O
Galeano	O
(	O
2018	O
)	O
defend	O
Perspective	O
(	O
Google	B-DatasetName
's	O
toxicity	O
classification	O
model	O
)	O
by	O
neutralizing	O
adversarial	O
inputs	O
via	O
a	O
negated	O
predicates	O
list	O
.	O
Again	O
,	O
these	O
defenses	O
are	O
restricted	O
to	O
contexts	O
where	O
specific	O
lists	O
may	O
be	O
identified	O
,	O
this	O
is	O
not	O
so	O
with	O
Sample	O
Shielding	O
.	O

IIE	O
's	O
Neural	O
Machine	B-TaskName
Translation	I-TaskName
Systems	O
for	O
WMT20	O

We	O
participate	O
in	O
the	O
WMT20	O
shared	O
news	O
translation	O
task	O
in	O
one	O
language	O
pair	O
and	O
two	O
language	O
directions	O
,	O
German	O
French	O
and	O
French	O
German	O
.	O
Our	O
methods	O
are	O
based	O
on	O
techniques	O
and	O
approaches	O
used	O
in	O
submissions	O
from	O
past	O
years	O
(	O
Deng	B-DatasetName
et	I-DatasetName
al	I-DatasetName
,	O
2018	O
;	O
Ng	O
et	O
al	O
,	O
2019	O
;	O
Sun	O
et	O
al	O
,	O
2019	O
;	O
Li	O
et	O
al	O
,	O
2019	O
;	O
Xia	O
et	O
al	O
,	O
2019	O
)	O
,	O
including	O
the	O
use	O
of	O
subword	O
models	O
(	O
Sennrich	O
et	O
al	O
,	O
2016	O
)	O
,	O
iterative	O
back	O
-	O
translation	O
,	O
knowledge	B-MethodName
distillation	I-MethodName
,	O
model	O
ensembling	O
and	O
several	O
techniques	O
we	O
proposed	O
recently	O
(	O
Wei	O
et	O
al	O
,	O
2020b	O
,	O
a	O
)	O
.	O
For	O
our	O
submissions	O
of	O
two	O
language	O
directions	O
,	O
we	O
adopt	O
the	O
deep	O
transformer	O
architectures	O
(	O
48layer	O
)	O
based	O
on	O
multiscale	O
collaboration	O
mechanism	O
(	O
Wei	O
et	O
al	O
,	O
2020b	O
)	O
as	O
our	O
baseline	O
,	O
which	O
outperformed	O
the	O
standard	O
Transformer	B-MethodName
-	O
Big	O
as	O
well	O
as	O
shallower	O
models	O
significantly	O
in	O
terms	O
of	O
translation	O
quality	O
.	O
We	O
also	O
use	O
an	O
iterative	O
back	O
-	O
translation	O
approach	O
with	O
the	O
controllable	O
sampling	O
to	O
extend	O
the	O
back	O
translation	O
method	O
by	O
jointly	O
training	O
source	O
-	O
to	O
-	O
target	O
and	O
target	O
-	O
to	O
-	O
source	O
NMT	O
models	O
.	O
Moreover	O
,	O
the	O
knowledge	B-MethodName
distillation	I-MethodName
(	O
Freitag	O
et	O
al	O
,	O
2017	O
)	O
is	O
employed	O
to	O
leverage	O
the	O
source	O
-	O
side	O
monolingual	O
data	O
.	O
For	O
our	O
final	O
models	O
,	O
we	O
apply	O
a	O
domainspecific	O
fine	O
-	O
tuning	O
process	O
and	O
model	O
ensembling	O
,	O
and	O
decode	O
using	O
noisy	O
channel	O
model	O
re	O
-	O
ranking	O
.	O
The	O
paper	O
is	O
structured	O
as	O
follows	O
:	O
Section	O
2	O
describes	O
the	O
techniques	O
we	O
used	O
,	O
then	O
section	O
3	O
shows	O
the	O
experimental	O
settings	O
and	O
results	O
.	O
Finally	O
,	O
we	O
conclude	O
our	O
work	O
in	O
Section	O
4	O
.	O

Back	O
-	O
translation	O
(	O
BT	O
)	O
is	O
an	O
effective	O
and	O
commonly	O
used	O
data	B-TaskName
augmentation	I-TaskName
technique	O
to	O
incorporate	O
monolingual	O
data	O
into	O
a	O
translation	O
system	O
.	O
Back	O
-	O
translation	O
first	O
trains	O
an	O
intermediate	O
targetto	O
-	O
source	O
system	O
that	O
is	O
used	O
to	O
translate	O
monolingual	O
target	O
data	O
into	O
additional	O
synthetic	O
parallel	O
data	O
.	O
This	O
data	O
is	O
used	O
in	O
conjunction	O
with	O
human	O
translated	O
bitext	O
data	O
to	O
train	O
the	O
desired	O
source	O
-	O
totarget	O
system	O
.	O
In	O
our	O
work	O
,	O
we	O
use	O
an	O
iterative	O
back	O
-	O
translation	O
approach	O
to	O
jointly	O
train	O
source	O
-	O
to	O
-	O
target	O
and	O
targetto	O
-	O
source	O
NMT	O
models	O
.	O
The	O
process	O
can	O
be	O
summarized	O
as	O
below	O
:	O
step	O
1	O
:	O
we	O
train	O
both	O
a	O
source	O
-	O
to	O
-	O
target	O
model	O
(	O
M	O
0	B-DatasetName
x	O
y	O
)	O
and	O
a	O
target	O
-	O
to	O
-	O
source	O
model	O
(	O
M	O
0	B-DatasetName
y	O
x	O
)	O
using	O
the	O
human	O
translated	O
data	O
.	O
step	O
2	O
:	O
we	O
use	O
M	O
t	O
x	O
y	O
to	O
translate	O
source	O
-	O
side	O
monolingual	O
data	O
to	O
target	O
language	O
,	O
and	O
use	O
M	O
t	O
y	O
x	O
to	O
translate	O
target	O
-	O
side	O
monolingual	O
data	O
to	O
source	O
language	O
,	O
where	O
t	O
starts	O
from	O
0	B-DatasetName
.	O
step	O
3	O
:	O
we	O
combine	O
both	O
the	O
human	O
translated	O
data	O
and	O
pseudo	O
data	O
synthesized	O
in	O
step	O
2	O
to	O
further	O
optimize	O
the	O
two	O
NMT	O
models	O
respectively	O
.	O
Repeat	O
steps	O
2	O
-	O
3	O
until	O
the	O
models	O
converge	O
.	O
In	O
practice	O
,	O
we	O
repeat	O
3	O
times	O
for	O
steps	O
2	O
-	O
3	O
.	O
We	O
apply	O
the	O
controllable	O
sampling	O
strategy	O
(	O
Wei	O
et	O
al	O
,	O
2020a	O
)	O
to	O
synthesize	O
reasonable	O
sentences	O
which	O
are	O
at	O
both	O
high	O
quality	O
and	O
diversity	O
.	O

The	O
early	O
adoption	O
of	O
knowledge	B-MethodName
distillation	I-MethodName
(	O
KD	O
)	O
(	O
Kim	O
and	O
Rush	O
,	O
2016	O
)	O
is	O
for	O
model	B-TaskName
compression	I-TaskName
.	O
We	O
use	O
the	O
same	O
method	O
as	O
in	O
Sun	O
et	O
al	O
(	O
2019	O
)	O
that	O
adopts	O
hybrid	O
heterogeneous	O
teacher	O
:	O
base	O
transformer	O
,	O
deep	O
transformer	O
,	O
big	O
transformer	O
and	O
RNMT+	O
.	O
For	O
each	O
individual	O
model	O
,	O
we	O
use	O
the	O
other	O
two	O
models	O
as	O
the	O
teacher	O
model	O
to	O
further	O
improve	O
the	O
performance	O
.	O
In	O
addition	O
,	O
model	O
ensemble	O
is	O
also	O
used	O
to	O
boost	O
the	O
performance	O
by	O
combining	O
the	O
predictions	O
of	O
above	O
four	O
models	O
at	O
each	O
decoding	O
step	O
.	O

Fine	O
-	O
tuning	O
with	O
domain	O
-	O
specific	O
data	O
is	O
a	O
common	O
and	O
effective	O
method	O
to	O
improve	O
translation	O
quality	O
for	O
a	O
downstream	O
task	O
.	O
After	O
completing	O
training	O
on	O
the	O
bitext	O
and	O
back	O
-	O
translated	O
data	O
,	O
we	O
train	O
for	O
an	O
additional	O
epoch	O
on	O
a	O
smaller	O
in	O
-	O
domain	O
corpus	O
.	O
We	O
first	O
select	O
100	O
K	O
sentence	O
-	O
pairs	O
from	O
the	O
bilingual	O
as	O
well	O
as	O
pseudo	O
-	O
generated	O
data	O
according	O
to	O
the	O
filter	O
method	O
in	O
Deng	B-DatasetName
et	I-DatasetName
al	I-DatasetName
(	O
2018	O
)	O
and	O
continue	O
to	O
train	O
the	O
model	O
on	O
the	O
filtered	O
data	O
.	O

N	O
-	O
best	O
reranking	O
is	O
a	O
method	O
of	O
improving	O
translation	O
quality	O
by	O
scoring	O
and	O
selecting	O
a	O
candidate	O
hypothesis	O
from	O
a	O
list	O
of	O
n	O
-	O
best	O
hypotheses	O
generated	O
by	O
a	O
source	O
-	O
to	O
-	O
target	O
model	O
.	O
For	O
our	O
submissions	O
,	O
we	O
rerank	O
the	O
n	O
-	O
best	O
hypotheses	O
using	O
two	O
aspects	O
as	O
follows	O
:	O
log	O
p	O
(	O
y	O
|	O
x	O
)	O
+	O
λ	O
1	O
log	O
p	O
(	O
x	O
|	O
y	O
)	O
+	O
λ	O
2	O
log	O
p	O
(	O
y	O
)	O
(	O
7	O
)	O
The	O
weights	O
λ	O
1	O
and	O
λ	O
2	O
are	O
determined	O
by	O
tuning	O
them	O
with	O
a	O
random	B-MethodName
search	I-MethodName
on	O
a	O
validation	O
set	O
and	O
selecting	O
the	O
weights	O
that	O
give	O
the	O
best	O
performance	O
.	O

We	O
use	O
all	O
available	O
bilingual	O
datasets	O
and	O
select	O
10	O
M	O
bilingual	O
data	O
from	O
WMT'20	O
corpora	O
using	O
the	O
script	O
filter	O
interactive.py	O
1	O
.	O
We	O
share	O
a	O
vocabulary	O
for	O
the	O
two	O
languages	O
and	O
apply	O
BPE	B-MethodName
for	O
word	O
segmentation	O
with	O
32	O
K	O
merge	O
operations	O
.	O
For	O
monolingual	O
data	O
,	O
we	O
use	O
18	O
M	O
German	O
sentences	O
and	O
18	O
M	O
French	O
sentences	O
from	O
Newscrawl	O
,	O
and	O
pre	O
-	O
process	O
them	O
in	O
the	O
same	O
way	O
as	O
bilingual	O
data	O
.	O
We	O
split	O
9k	O
sentences	O
from	O
the	O
"	O
dev08	O
-	O
14	O
"	O
as	O
the	O
validation	O
set	O
and	O
use	O
newstest	O
2019	O
as	O
the	O
test	O
set	O
.	O

Through	O
subsumption	O
and	O
instantiation	O
,	O
individual	O
instances	O
(	O
"	O
artificial	O
intelligence	O
"	O
,	O
"	O
the	O
spotted	O
pig	O
"	O
)	O
otherwise	O
spanning	O
a	O
wide	O
range	O
of	O
domains	O
can	O
be	O
brought	O
together	O
and	O
organized	O
under	O
conceptual	O
hierarchies	O
.	O
The	O
hierarchies	O
connect	O
more	O
specific	O
concepts	O
(	O
"	O
computer	O
science	O
subfields	O
"	O
,	O
"	O
gastropubs	O
"	O
)	O
to	O
more	O
general	O
concepts	O
(	O
"	O
academic	O
disciplines	O
"	O
,	O
"	O
restaurants	O
"	O
)	O
through	O
IsA	O
relations	O
.	O
Explicit	O
or	O
implicit	O
properties	O
applicable	O
to	O
,	O
and	O
defining	O
,	O
more	O
general	O
concepts	O
are	O
inherited	O
by	O
their	O
more	O
specific	O
concepts	O
,	O
down	O
to	O
the	O
instances	O
connected	O
to	O
the	O
lower	O
parts	O
of	O
the	O
hierarchies	O
.	O
Subsumption	O
represents	O
a	O
crisp	O
,	O
universally	O
-	O
applicable	O
principle	O
towards	O
consistently	O
representing	O
IsA	O
relations	O
in	O
any	O
knowledge	O
resource	O
.	O
Yet	O
knowledge	O
resources	O
often	O
exhibit	O
significant	O
differences	O
in	O
their	O
scope	O
,	O
representation	O
choices	O
and	O
intended	O
usage	O
,	O
to	O
cause	O
significant	O
differences	O
in	O
their	O
expected	O
usage	O
and	O
impact	O
on	O
various	O
tasks	O
.	O
This	O
tutorial	O
examines	O
the	O
theoretical	O
foundations	O
of	O
subsumption	O
,	O
and	O
its	O
practical	O
embodiment	O
through	O
IsA	O
relations	O
compiled	O
manually	O
or	O
extracted	O
automatically	O
.	O
It	O
addresses	O
IsA	O
relations	O
from	O
their	O
formal	O
definition	O
;	O
through	O
practical	O
choices	O
made	O
in	O
their	O
representation	O
within	O
the	O
larger	O
and	O
more	O
widely	O
-	O
used	O
of	O
the	O
available	O
knowledge	O
resources	O
;	O
to	O
their	O
automatic	O
acquisition	O
from	O
document	O
repositories	O
,	O
as	O
opposed	O
to	O
their	O
manual	O
compilation	O
by	O
human	O
contributors	O
;	O
to	O
their	O
impact	O
in	O
text	O
analysis	O
and	O
information	B-TaskName
retrieval	I-TaskName
.	O
As	O
search	O
engines	O
move	O
away	O
from	O
returning	O
a	O
set	O
of	O
links	O
and	O
closer	O
to	O
returning	O
results	O
that	O
more	O
directly	O
answer	O
queries	O
,	O
IsA	O
relations	O
play	O
an	O
increasingly	O
important	O
role	O
towards	O
a	O
better	O
understanding	O
of	O
documents	O
and	O
queries	O
.	O
The	O
tutorial	O
teaches	O
the	O
audience	O
about	O
definitions	O
,	O
assumptions	O
and	O
practical	O
choices	O
related	O
to	O
modeling	O
and	O
representing	O
IsA	O
relations	O
in	O
existing	O
,	O
human	O
-	O
compiled	O
resources	O
of	O
instances	O
,	O
concepts	O
and	O
resulting	O
conceptual	O
hierarchies	O
;	O
methods	O
for	O
automatically	O
extracting	O
sets	O
of	O
instances	O
within	O
unlabeled	O
or	O
labeled	O
concepts	O
,	O
where	O
the	O
concepts	O
may	O
be	O
considered	O
as	O
a	O
flat	O
set	O
or	O
organized	O
hierarchically	O
;	O
and	O
applications	O
of	O
IsA	O
relations	O
in	O
information	B-TaskName
retrieval	I-TaskName
.	O

Marius	O
Paşca	O
is	O
a	O
research	O
scientist	O
at	O
Google	B-DatasetName
.	O
Current	O
research	O
interests	O
include	O
factual	O
information	O
extraction	O
from	O
unstructured	O
text	O
within	O
documents	O
and	O
queries	O
and	O
its	O
applications	O
to	O
Web	O
search	O
.	O

The	O
use	O
of	O
word	B-TaskName
embeddings	I-TaskName
is	O
an	O
important	O
NLP	O
technique	O
for	O
extracting	O
meaningful	O
conclusions	O
from	O
corpora	O
of	O
human	O
text	O
.	O
One	O
important	O
question	O
that	O
has	O
been	O
raised	O
about	O
word	B-TaskName
embeddings	I-TaskName
is	O
the	O
degree	O
of	O
gender	O
bias	O
learned	O
from	O
corpora	O
.	O
Bolukbasi	O
et	O
al	O
(	O
2016	O
)	O
proposed	O
an	O
important	O
technique	O
for	O
quantifying	O
gender	O
bias	O
in	O
word	B-TaskName
embeddings	I-TaskName
that	O
,	O
at	O
its	O
heart	O
,	O
is	O
lexically	O
based	O
and	O
relies	O
on	O
sets	O
of	O
highly	O
gendered	O
word	O
pairs	O
(	O
e.g.	O
,	O
mother	O
/	O
father	O
and	O
madam	O
/	O
sir	O
)	O
and	O
a	O
list	O
of	O
professions	O
words	O
(	O
e.g.	O
,	O
doctor	O
and	O
nurse	O
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
document	O
problems	O
that	O
arise	O
with	O
this	O
method	O
to	O
quantify	O
gender	O
bias	O
in	O
diachronic	O
corpora	O
.	O
Focusing	O
on	O
Arabic	O
and	O
Chinese	O
corpora	O
,	O
in	O
particular	O
,	O
we	O
document	O
clear	O
changes	O
in	O
profession	O
words	O
used	O
over	O
time	O
and	O
,	O
somewhat	O
surprisingly	O
,	O
even	O
changes	O
in	O
the	O
simpler	O
gendered	O
defining	O
set	O
word	O
pairs	O
.	O
We	O
further	O
document	O
complications	O
in	O
languages	O
such	O
as	O
Arabic	O
,	O
where	O
many	O
words	O
are	O
highly	O
polysemous	O
/	O
homonymous	O
,	O
especially	O
female	O
professions	O
words	O
.	O

Natural	O
Language	O
Processing	O
(	O
NLP	O
)	O
plays	O
a	O
significant	O
role	O
in	O
many	O
powerful	O
applications	O
such	O
as	O
speech	B-TaskName
recognition	I-TaskName
,	O
text	O
translation	O
,	O
and	O
autocomplete	O
and	O
is	O
at	O
the	O
heart	O
of	O
many	O
critical	O
automated	O
decision	O
systems	O
making	O
crucial	O
recommendations	O
about	O
our	O
future	O
world	O
.	O
Word	O
embedding	O
systems	O
are	O
widely	O
used	O
to	O
represent	O
text	O
data	O
as	O
vectors	O
and	O
enable	O
NLP	O
computation	O
.	O
Systems	O
such	O
as	O
Word2Vec	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
,	O
GloVe	B-MethodName
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
,	O
and	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
ingest	O
large	O
corpora	O
of	O
human	O
text	O
and	O
can	O
be	O
used	O
to	O
learn	O
semantic	O
and	O
syntactic	O
relationships	O
between	O
words	O
.	O
At	O
the	O
same	O
time	O
,	O
it	O
has	O
been	O
demonstrated	O
that	O
these	O
systems	O
learn	O
a	O
wide	O
variety	O
of	O
societal	O
biases	O
embedded	O
in	O
human	O
text	O
including	O
racial	O
bias	O
,	O
gender	O
bias	O
,	O
and	O
religious	O
bias	O
(	O
Caliskan	O
et	O
al	O
,	O
2017	O
;	O
Abid	O
et	O
al	O
,	O
2021	O
)	O
.	O
In	O
a	O
widely	O
cited	O
paper	O
,	O
Bolukbasi	O
et	O
al	O
(	O
2016	O
)	O
demonstrated	O
that	O
a	O
system	O
trained	O
with	O
a	O
corpora	O
of	O
Google	B-DatasetName
News	O
would	O
complete	O
the	O
word	O
comparison	O
"	O
man	O
is	O
to	O
computer	O
programmer	O
as	O
woman	O
is	O
to	O
what	O
?	O
"	O
with	O
the	O
response	O
"	O
homemaker	O
"	O
suggesting	O
an	O
alarming	O
level	O
of	O
gender	O
bias	O
when	O
used	O
in	O
tasks	O
such	O
as	O
sorting	O
resumes	O
for	O
computer	O
programming	O
jobs	O
.	O
Chen	O
et	O
al	O
(	O
2021	O
)	O
extended	O
these	O
techniques	O
beyond	O
English	O
to	O
eight	O
other	O
languages	O
(	O
Chinese	O
,	O
Spanish	O
,	O
Arabic	O
,	O
German	O
,	O
French	O
,	O
Farsi	O
,	O
Urdu	B-DatasetName
,	O
and	O
Wolof	O
)	O
and	O
applied	O
them	O
to	O
Wikipedia	O
corpora	O
in	O
each	O
of	O
these	O
languages	O
.	O
They	O
documented	O
persistent	O
gender	O
bias	O
and	O
lack	O
of	O
representation	O
in	O
the	O
modern	O
NLP	O
pipeline	O
.	O
NLP	O
research	O
often	O
uses	O
large	O
,	O
modern	O
datasets	O
like	O
Google	B-DatasetName
News	O
and	O
Wikipedia	O
.	O
Developers	O
of	O
a	O
wide	O
variety	O
of	O
NLP	O
-	O
based	O
applications	O
begin	O
with	O
large	O
pre	O
-	O
trained	O
models	O
that	O
are	O
also	O
based	O
on	O
large	O
corpora	O
of	O
human	O
text	O
(	O
Bender	O
et	O
al	O
,	O
2021	O
)	O
.	O
These	O
pre	O
-	O
trained	O
models	O
also	O
largely	O
reflect	O
the	O
speech	O
/	O
writing	O
of	O
modern	O
English	O
speakers	O
producing	O
digital	O
text	O
.	O
The	O
speech	O
/	O
writing	O
of	O
speakers	O
of	O
the	O
more	O
than	O
7	O
,	O
000	O
languages	O
spoken	O
worldwide	O
is	O
often	O
under	O
-	O
represented	O
(	O
Wali	O
et	O
al	O
,	O
2020	O
)	O
.	O
Similarly	O
,	O
historical	O
speech	O
/	O
writing	O
is	O
often	O
under	O
-	O
represented	O
despite	O
the	O
fact	O
that	O
historical	O
speech	O
/	O
writing	O
is	O
often	O
considered	O
foundational	O
to	O
cultural	O
identity	O
.	O
Investments	O
in	O
multilingual	B-TaskName
NLP	I-TaskName
and	O
processing	O
of	O
diachronic	O
corpora	O
are	O
essential	O
if	O
we	O
want	O
our	O
NLP	O
-	O
based	O
automated	O
decision	B-TaskName
making	I-TaskName
systems	O
to	O
more	O
widely	O
reflect	O
foundational	O
cultural	O
norms	O
and	O
identity	O
from	O
around	O
the	O
world	O
.	O
The	O
inspiration	O
for	O
this	O
paper	O
was	O
to	O
re	O
-	O
examine	O
Bolukbasi	O
et	O
al	O
's	O
popular	O
NLP	O
-	O
technique	O
for	O
quantifying	O
gender	O
bias	O
from	O
the	O
perspective	O
of	O
applying	O
it	O
to	O
diachronic	O
corpora	O
in	O
Arabic	O
and	O
Chinese	O
.	O
Specifically	O
,	O
Bolukbasi	O
et	O
al	O
's	O
method	O
begins	O
with	O
identifying	O
a	O
set	O
of	O
profession	O
words	O
and	O
a	O
set	O
of	O
highly	O
gendered	O
word	O
pairs	O
(	O
defining	O
set	O
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
explore	O
the	O
degree	O
to	O
which	O
these	O
words	O
might	O
change	O
over	O
time	O
.	O
We	O
document	O
ways	O
in	O
which	O
this	O
method	O
is	O
fundamentally	O
fragile	O
for	O
diachronic	O
corpora	O
because	O
of	O
the	O
way	O
these	O
sets	O
of	O
words	O
would	O
change	O
over	O
time	O
.	O
In	O
Section	O
2	O
,	O
for	O
background	O
,	O
we	O
elaborate	O
on	O
Bolukbasi	O
et	O
al	O
and	O
Chen	O
et	O
al	O
's	O
multilingual	O
extensions	O
and	O
some	O
other	O
relevant	O
related	O
work	O
.	O
Section	O
3	O
describes	O
our	O
experience	O
with	O
two	O
different	O
diachronic	O
Arabic	O
corpora	O
,	O
especially	O
the	O
impact	O
on	O
changes	O
in	O
profession	O
set	O
words	O
over	O
time	O
.	O
In	O
Section	O
4	O
,	O
we	O
discuss	O
changes	O
in	O
some	O
defining	O
set	O
words	O
in	O
Chinese	O
using	O
the	O
Google	B-DatasetName
Ngram	O
Viewer	O
.	O
We	O
conclude	O
and	O
discuss	O
future	O
work	O
in	O
Section	O
5	O
.	O
2	O
Background	O
and	O
Related	O
Work	O
Bolukbasi	O
et	O
al	O
(	O
2016	O
)	O
pioneered	O
a	O
method	O
for	O
quantifying	O
the	O
amount	O
of	O
gender	O
bias	O
learned	O
in	O
by	O
word	O
embedding	O
systems	O
and	O
many	O
researchers	O
have	O
built	O
on	O
their	O
techniques	O
including	O
Chen	O
et	O
al	O
(	O
2021	O
)	O
who	O
observed	O
substantial	O
hurdles	O
in	O
extending	O
the	O
techniques	O
beyond	O
English	O
.	O
In	O
this	O
paper	O
,	O
we	O
build	O
on	O
both	O
Bolukbasi	O
et	O
al	O
and	O
Chen	O
et	O
al	O
's	O
work	O
to	O
examine	O
additional	O
hurdles	O
that	O
would	O
arise	O
when	O
attempting	O
to	O
apply	O
these	O
techniques	O
to	O
diachronic	O
corpora	O
.	O
Bolukbasi	O
et	O
al	O
's	O
original	O
method	O
is	O
based	O
on	O
two	O
sets	O
of	O
words	O
.	O
The	O
first	O
set	O
(	O
the	O
defining	O
set	O
)	O
consists	O
of	O
10	O
highly	O
gendered	O
word	O
pairs	O
(	O
she	O
-	O
he	O
,	O
daughter	O
-	O
son	O
,	O
her	O
-	O
his	O
,	O
mother	O
-	O
father	O
,	O
woman	O
-	O
man	O
,	O
gal	O
-	O
guy	O
,	O
Mary	O
-	O
John	O
,	O
girl	O
-	O
boy	O
,	O
herself	O
-	O
himself	O
,	O
and	O
female	O
-	O
male	O
)	O
and	O
the	O
second	O
(	O
profession	O
set	O
)	O
consists	O
of	O
327	O
profession	O
words	O
such	O
as	O
nurse	O
,	O
teacher	O
,	O
writer	O
,	O
engineer	O
,	O
scientist	O
,	O
manager	O
,	O
driver	O
,	O
banker	O
,	O
musician	O
,	O
artist	O
,	O
and	O
chef	O
.	O
They	O
used	O
the	O
difference	O
between	O
the	O
defining	O
set	O
word	O
pairs	O
to	O
define	O
a	O
gendered	O
vector	O
space	O
and	O
then	O
evaluated	O
the	O
relationship	O
of	O
the	O
profession	O
words	O
relative	O
to	O
this	O
gendered	O
vector	O
space	O
.	O
Ideally	O
,	O
profession	O
words	O
would	O
not	O
reflect	O
a	O
strong	O
gender	O
bias	O
.	O
However	O
,	O
in	O
practice	O
,	O
they	O
often	O
do	O
.	O
According	O
to	O
such	O
a	O
metric	O
,	O
the	O
word	O
doctor	O
might	O
be	O
male	O
biased	O
or	O
the	O
word	O
nurse	O
female	O
biased	O
based	O
on	O
how	O
these	O
words	O
are	O
used	O
in	O
the	O
corpora	O
from	O
which	O
the	O
word	O
embedding	O
model	O
was	O
produced	O
.	O
Bolukbasi	O
et	O
al	O
(	O
2016	O
)	O
uses	O
these	O
two	O
sets	O
of	O
words	O
to	O
compute	O
a	O
gender	O
bias	O
metric	O
for	O
each	O
word	O
and	O
from	O
there	O
to	O
express	O
the	O
gender	O
bias	O
of	O
a	O
corpora	O
.	O
Specifically	O
,	O
each	O
word	O
is	O
expressed	O
as	O
a	O
vector	O
by	O
Word2Vec	O
and	O
then	O
the	O
center	O
of	O
the	O
vectors	O
for	O
each	O
defining	O
set	O
pair	O
is	O
calculated	O
.	O
For	O
example	O
,	O
to	O
calculate	O
the	O
center	O
of	O
the	O
definitional	O
pair	O
woman	O
/	O
man	O
,	O
they	O
average	O
the	O
vector	O
for	O
"	O
woman	O
"	O
with	O
the	O
vector	O
for	O
"	O
man	O
"	O
.	O
Then	O
,	O
they	O
calculate	O
the	O
distance	O
of	O
each	O
word	O
in	O
the	O
definitional	O
pair	O
from	O
the	O
center	O
by	O
subtracting	O
the	O
center	O
from	O
each	O
word	O
in	O
the	O
pair	O
(	O
e.g.	O
,	O
"	O
woman	O
"	O
-	O
center	O
)	O
.	O
They	O
then	O
apply	O
Principal	O
Component	O
Analysis	O
(	O
PCA	B-MethodName
)	O
to	O
the	O
matrix	O
of	O
these	O
distances	O
.	O
PCA	B-MethodName
is	O
an	O
approach	O
that	O
compresses	O
multiple	O
dimensions	O
into	O
fewer	O
dimensions	O
,	O
ideally	O
in	O
a	O
way	O
that	O
the	O
information	O
within	O
the	O
original	O
data	O
is	O
not	O
lost	O
.	O
Usually	O
,	O
the	O
number	O
of	O
reduced	O
dimensions	O
is	O
1	O
-	O
3	O
as	O
it	O
allows	O
for	O
easier	O
visualization	O
of	O
a	O
dataset	O
.	O
Bolukbasi	O
et	O
al	O
(	O
2016	O
)	O
used	O
the	O
first	O
eigenvalue	O
from	O
the	O
PCA	B-MethodName
matrix	O
(	O
i.e.	O
the	O
one	O
that	O
is	O
larger	O
than	O
the	O
rest	O
)	O
.	O
Because	O
the	O
defining	O
set	O
pairs	O
were	O
chosen	O
to	O
be	O
highly	O
gendered	O
,	O
they	O
expected	O
this	O
dimension	O
to	O
be	O
related	O
primarily	O
to	O
gender	O
and	O
therefore	O
called	O
it	O
the	O
gender	O
direction	O
or	O
the	O
g	O
direction	O
.	O
Finally	O
,	O
the	O
g	O
direction	O
is	O
a	O
vector	O
,	O
and	O
there	O
is	O
a	O
vector	O
representing	O
each	O
word	O
.	O
Therefore	O
,	O
they	O
used	O
cosine	O
similarity	O
between	O
the	O
vector	O
for	O
each	O
word	O
,	O
w	O
,	O
and	O
the	O
g	O
direction	O
vector	O
as	O
the	O
measure	O
of	O
gender	O
bias	O
for	O
that	O
word	O
.	O
For	O
a	O
corpora	O
or	O
other	O
collection	O
of	O
words	O
,	O
one	O
can	O
average	O
the	O
gender	O
bias	O
of	O
words	O
contained	O
in	O
the	O
corpora	O
as	O
a	O
measure	O
of	O
gender	O
bias	O
in	O
the	O
corpora	O
using	O
the	O
equation	O
of	O
Bolukbasi	O
et	O
al	O
(	O
2016	O
)	O
for	O
the	O
direct	O
gender	O
bias	O
of	O
an	O
embedding	O
:	O
DirectBias	O
c	O
=	O
1	O
|	O
N	O
|	O
w	O
N	O
|	O
cos	O
(	O
⃗	O
w	O
,	O
g	O
)	O
|	O
c	O
where	O
N	O
is	O
the	O
given	O
gender	O
neutral	O
words	O
,	O
and	O
c	O
is	O
a	O
parameter	O
that	O
determines	O
the	O
strictness	O
in	O
measuring	O
gender	O
bias	O
.	O
Chen	O
et	O
al	O
(	O
2021	O
)	O
extended	O
the	O
Bolukbasi	O
et	O
al	O
'	O
method	O
to	O
eight	O
languages	O
besides	O
English	O
-	O
Chinese	O
,	O
Spanish	O
,	O
Arabic	O
,	O
German	O
,	O
French	O
,	O
Farsi	O
,	O
Urdu	B-DatasetName
,	O
and	O
Wolof	O
.	O
In	O
order	O
to	O
do	O
so	O
,	O
they	O
first	O
made	O
modifications	O
to	O
the	O
defining	O
set	O
to	O
make	O
it	O
more	O
translatable	O
across	O
the	O
9	O
languages	O
.	O
For	O
example	O
,	O
they	O
dropped	O
pairs	O
like	O
she	O
-	O
he	O
,	O
her	O
-	O
his	O
,	O
gal	O
-	O
guy	O
,	O
Mary	O
-	O
John	O
,	O
herself	O
-	O
himself	O
,	O
and	O
femalemale	O
because	O
of	O
problems	O
in	O
translation	O
for	O
some	O
languages	O
and	O
adding	O
pairs	O
like	O
queen	O
-	O
king	O
,	O
wifehusband	O
,	O
and	O
madam	O
-	O
sir	O
.	O
Second	O
,	O
they	O
observed	O
that	O
the	O
Bolukbasi	O
et	O
al	O
's	O
method	O
can	O
not	O
be	O
applied	O
directly	O
to	O
languages	O
such	O
as	O
Spanish	O
,	O
Arabic	O
,	O
German	O
,	O
French	O
,	O
and	O
Urdu	B-DatasetName
that	O
primarily	O
use	O
grammatically	O
gendered	O
nouns	O
(	O
e.g.	O
,	O
escritor	O
/	O
escritora	O
in	O
Spanish	O
vs.	O
writer	O
in	O
English	O
)	O
.	O
They	O
solved	O
this	O
problem	O
using	O
a	O
weighted	O
average	O
of	O
the	O
number	O
of	O
occurrences	O
of	O
each	O
variant	O
of	O
the	O
professional	O
word	O
(	O
male	O
,	O
female	O
,	O
or	O
neutral	O
)	O
multiplied	O
by	O
the	O
gender	O
bias	O
score	O
for	O
that	O
variant	O
.	O
In	O
this	O
work	O
,	O
we	O
build	O
on	O
both	O
(	O
Bolukbasi	O
et	O
al	O
,	O
2016	O
;	O
Chen	O
et	O
al	O
,	O
2021	O
)	O
and	O
focus	O
on	O
the	O
unique	O
challenges	O
that	O
arise	O
when	O
applying	O
these	O
techniques	O
to	O
diachronic	O
corpora	O
.	O
Specifically	O
,	O
we	O
examined	O
changes	O
in	O
both	O
the	O
profession	O
set	O
and	O
defining	O
set	O
over	O
time	O
in	O
Arabic	O
and	O
Chinese	O
.	O
Certainly	O
,	O
professions	O
have	O
changed	O
drastically	O
over	O
that	O
amount	O
of	O
time	O
and	O
so	O
a	O
method	O
based	O
on	O
profession	O
set	O
words	O
like	O
Bolukbasi	O
et	O
al	O
's	O
method	O
will	O
have	O
substantial	O
challenges	O
.	O
We	O
explored	O
this	O
using	O
corpora	O
including	O
a	O
database	O
of	O
Arabic	O
poems	O
spanning	O
11	O
eras	O
from	O
the	O
Pre	O
-	O
Islamic	O
period	O
(	O
before	O
610	O
)	O
to	O
modern	O
day	O
.	O
While	O
we	O
saw	O
less	O
change	O
over	O
time	O
in	O
the	O
usage	O
of	O
the	O
simpler	O
defining	O
set	O
words	O
than	O
in	O
the	O
profession	O
set	O
words	O
,	O
we	O
did	O
observe	O
some	O
interesting	O
changes	O
in	O
even	O
the	O
defining	O
set	O
words	O
over	O
time	O
,	O
especially	O
in	O
Chinese	O
.	O
In	O
the	O
process	O
of	O
this	O
work	O
,	O
we	O
also	O
documented	O
further	O
complications	O
in	O
languages	O
such	O
as	O
Arabic	O
,	O
where	O
many	O
words	O
are	O
highly	O
polysemous	O
/	O
homonymous	O
,	O
especially	O
female	O
professions	O
words	O
.	O
Wevers	O
(	O
2019	O
)	O
also	O
used	O
word	B-TaskName
embeddings	I-TaskName
to	O
examine	O
gender	O
bias	O
over	O
time	O
.	O
They	O
used	O
a	O
collection	O
of	O
Dutch	O
Newspaper	O
articles	O
spanning	O
over	O
four	O
eras	O
,	O
training	O
four	O
embedding	O
models	O
per	O
newspaper	O
,	O
one	O
per	O
era	O
,	O
using	O
the	O
Gensim	O
implementation	O
of	O
Word2Vec	O
to	O
demonstrate	O
how	O
word	B-TaskName
embeddings	I-TaskName
can	O
be	O
used	O
to	O
examine	O
historical	O
language	O
change	O
.	O
They	O
observed	O
clear	O
differences	O
in	O
gender	O
bias	O
and	O
changes	O
within	O
and	O
between	O
newspapers	O
over	O
time	O
.	O
Slight	O
shifting	O
of	O
bias	O
was	O
observed	O
in	O
some	O
themes	O
like	O
shifting	O
towards	O
female	O
bias	O
in	O
themes	O
related	O
to	O
sexuality	O
and	O
leisure	O
(	O
mostly	O
seen	O
in	O
newspapers	O
with	O
religious	O
background	O
)	O
.	O
Shifting	O
towards	O
male	O
bias	O
in	O
themes	O
related	O
'	O
money	O
'	O
,	O
'	O
grooming	O
'	O
,	O
and	O
negative	O
emotions	O
,	O
especially	O
in	O
newspapers	O
with	O
a	O
liberal	O
background	O
,	O
was	O
also	O
observed	O
.	O
Rudolph	O
and	O
Blei	O
(	O
2018	O
)	O
developed	O
dynamic	O
embeddings	O
building	O
on	O
exponential	O
family	O
embeddings	O
to	O
capture	O
the	O
language	O
evolution	O
or	O
how	O
the	O
meanings	O
of	O
words	O
change	O
over	O
time	O
.	O
They	O
used	O
three	O
datasets	O
of	O
the	O
U.S.	O
Senate	O
speeches	O
from	O
1858	O
to	O
2009	O
,	O
the	O
history	O
of	O
computer	O
science	O
ACM	B-DatasetName
abstracts	O
from	O
1951	O
to	O
2014	O
,	O
and	O
machine	O
learning	O
papers	O
on	O
the	O
ArXiv	B-DatasetName
from	O
2007	O
to	O
2015	O
.	O
They	O
demonstrated	O
how	O
words	O
like	O
Intelligence	O
,	O
Iraq	O
,	O
computer	O
,	O
Bush	O
,	O
data	O
change	O
their	O
meaning	O
over	O
time	O
.	O
They	O
observed	O
that	O
the	O
dynamic	O
embeddings	O
provided	O
a	O
better	O
fit	O
than	O
classical	O
embeddings	O
and	O
captured	O
interesting	O
patterns	O
about	O
how	O
language	O
changes	O
.	O
For	O
example	O
,	O
a	O
word	O
's	O
meaning	O
can	O
change	O
(	O
e.g.	O
,	O
computer	O
)	O
;	O
its	O
dominant	O
meaning	O
can	O
change	O
(	O
e.g.	O
,	O
values	O
)	O
;	O
or	O
its	O
related	O
subject	O
matter	O
can	O
change	O
(	O
e.g.	O
,	O
Iraq	O
)	O
.	O
Xu	O
et	O
al	O
(	O
2019	O
)	O
demonstrated	O
the	O
characterization	O
of	O
the	O
semantic	O
weights	O
of	O
subword	O
units	O
in	O
the	O
composition	O
of	O
word	O
meanings	O
.	O
They	O
used	O
a	O
subword	O
-	O
incorporated	O
or	O
a	O
word	O
embedding	O
model	O
variant	O
for	O
the	O
evaluation	O
and	O
revealed	O
interesting	O
patterns	O
change	O
in	O
multiple	O
languages	O
.	O
Their	O
training	O
datasets	O
consist	O
of	O
Wikimedia	O
dumps	O
for	O
6	O
Languages	O
(	O
up	O
until	O
July	O
2017	O
)	O
consisting	O
of	O
Chinese	O
and	O
other	O
Indo	O
-	O
European	O
languages	O
like	O
English	O
,	O
French	O
,	O
German	O
,	O
and	O
Italian	O
.	O
The	O
results	O
revealed	O
major	O
differences	O
in	O
the	O
long	O
-	O
term	O
temporal	O
patterns	O
of	O
semantic	O
weights	O
between	O
Chinese	O
and	O
five	O
Indo	O
-	O
European	O
languages	O
.	O
For	O
example	O
,	O
in	O
Chinese	O
,	O
the	O
weights	O
on	O
subword	O
units	O
(	O
characters	O
)	O
show	O
a	O
decreasing	O
trend	O
,	O
i.e.	O
,	O
individual	O
characters	O
play	O
less	O
semantic	O
roles	O
in	O
newer	O
words	O
than	O
older	O
ones	O
whereas	O
the	O
opposite	O
trend	O
was	O
observed	O
in	O
other	O
languages	O
.	O
Therefore	O
,	O
Chinese	O
words	O
are	O
treated	O
more	O
as	O
a	O
whole	O
semantic	O
unit	O
"	O
synthetically	O
"	O
,	O
while	O
words	O
in	O
Indo	O
-	O
European	O
languages	O
require	O
more	O
attention	O
into	O
the	O
subword	O
units	O
"	O
analytically	O
"	O
.	O
These	O
results	O
provide	O
evidence	O
towards	O
word	O
formations	O
to	O
the	O
linguistic	O
theories	O
.	O
For	O
example	O
,	O
the	O
notion	O
of	O
"	O
word	O
"	O
in	O
Chinese	O
is	O
always	O
changing	O
:	O
Modern	O
Chinese	O
has	O
multiple	O
characters	O
as	O
a	O
whole	O
semantic	O
unit	O
opposite	O
to	O
its	O
older	O
counterpart	O
.	O
The	O
semantic	O
weight	O
carried	O
by	O
a	O
single	O
character	O
is	O
decreasing	O
over	O
time	O
.	O
This	O
is	O
strong	O
evidence	O
in	O
support	O
of	O
the	O
claim	O
that	O
Chinese	O
has	O
been	O
evolving	O
towards	O
more	O
detailed	O
multisyllabic	O
words	O
from	O
concise	O
and	O
monosyllabic	O
words	O
.	O
,	O
and	O
token	O
size	O
(	O
all	O
words	O
)	O
for	O
each	O
time	O
period	O
.	O
We	O
did	O
not	O
train	O
a	O
GloVe	B-MethodName
model	O
on	O
the	O
unknown	O
books	O
alone	O
or	O
the	O
duplicate	O
books	O
and	O
therefore	O
are	O
not	O
reporting	O
vocab	O
size	O
and	O
token	O
size	O
.	O

In	O
this	O
paper	O
,	O
we	O
use	O
two	O
Arabic	O
datasets	O
:	O
Shamela	O
Library	O
(	O
)	O
that	O
is	O
released	O
by	O
Shamela	O
Library	O
Foundation	O
(	O
2012	O
)	O
,	O
and	O
Arabic	O
Poem	O
Comprehensive	O
Dataset	O
(	O
APCD	O
)	O
by	O
(	O
Yousef	O
et	O
al	O
,	O
2018	O
)	O
.	O
Shamela	O
Library	O
is	O
a	O
free	O
project	O
that	O
collects	O
thousands	O
of	O
Islamic	O
religious	O
and	O
other	O
related	O
sciences	O
books	O
.	O
APCD	O
is	O
a	O
collection	O
of	O
Arabic	O
poems	O
spanning	O
11	O
eras	O
,	O
from	O
the	O
Pre	O
-	O
Islamic	O
(	O
before	O
610	O
)	O
to	O
the	O
Modern	O
age	O
(	O
1924	O
-	O
Now	O
)	O
.	O
Arabic	O
NLP	O
researchers	O
commonly	O
use	O
these	O
two	O
datasets	O
to	O
study	O
Arabic	O
classics	O
.	O
We	O
processed	O
the	O
Shamela	O
Library	O
dataset	O
version	O
of	O
6	O
,	O
538	O
Arabic	O
books	O
(	O
6	O
,	O
527	O
unique	O
books	O
after	O
removing	O
duplicates	O
)	O
in	O
Microsoft	O
Word	O
format	O
(	O
1997	O
)	O
(	O
1998	O
)	O
(	O
1999	O
)	O
(	O
2000	O
)	O
(	O
2001	O
)	O
(	O
2002	O
)	O
(	O
2003	O
)	O
(	O
2004	O
)	O
.	O
1	O
The	O
books	O
in	O
this	O
corpora	O
were	O
not	O
labeled	O
according	O
to	O
the	O
publication	O
dates	O
.	O
Thus	O
,	O
to	O
study	O
the	O
language	O
change	O
over	O
time	O
in	O
the	O
Arabic	O
language	O
,	O
we	O
further	O
classified	O
Shamela	O
's	O
Arabic	O
books	O
into	O
three	O
different	O
time	O
periods	O
based	O
ei	O
-	O
1	O
We	O
contribute	O
the	O
scripts	O
we	O
wrote	O
to	O
process	O
these	O
corpora	O
and	O
overcome	O
several	O
challenges	O
with	O
the	O
data	O
.	O
For	O
example	O
,	O
one	O
challenge	O
we	O
faced	O
was	O
correctly	O
converting	O
back	O
and	O
forth	O
between	O
the	O
Arabic	O
Windows	O
-	O
1256	O
to	O
the	O
Unicode	O
(	O
UTF	O
-	O
8	O
)	O
encoding	O
schemes	O
.	O
The	O
Arabic	O
books	O
were	O
written	O
in	O
an	O
old	O
version	O
of	O
Microsoft	O
Word	O
(	O
1997Word	O
(	O
-	O
2004	O
,	O
which	O
caused	O
encoding	O
scheme	O
conversion	O
errors	O
,	O
resulting	O
in	O
unreadable	O
characters	O
by	O
native	O
Arabic	O
speakers	O
or	O
even	O
NLP	O
tools	O
.	O
Scripts	O
can	O
be	O
found	O
here	O
:	O
https://github.com/	O
Clarkson	O
-	O
Accountability	O
-	O
Transparency/	O
gBiasRoadblocks	O
ther	O
on	O
their	O
publication	O
date	O
or	O
the	O
authors	O
'	O
date	O
of	O
death	O
when	O
publication	O
date	O
was	O
not	O
available	O
.	O
We	O
identified	O
books	O
written	O
before	O
Islam	O
or	O
before	O
610	O
(	O
only	O
three	O
books	O
)	O
,	O
books	O
written	O
before	O
1900	O
(	O
2	O
,	O
820	O
books	O
)	O
,	O
and	O
books	O
written	O
on	O
or	O
after	O
1900	O
(	O
773	O
books	O
)	O
.	O
We	O
were	O
not	O
able	O
to	O
identify	O
publication	O
dates	O
or	O
the	O
authors	O
'	O
dates	O
of	O
death	O
of	O
the	O
remaining	O
2	O
,	O
931	O
books	O
due	O
to	O
not	O
having	O
any	O
;	O
Table	O
1	O
summarizes	O
some	O
key	O
attributes	O
of	O
this	O
dataset	O
.	O
We	O
also	O
processed	O
the	O
APCD	O
,	O
an	O
Arabic	O
poetry	O
dataset	O
that	O
is	O
collected	O
mainly	O
from	O
the	O
Poetry	O
Encyclopedia	O
(	O
)	O
that	O
is	O
released	O
by	O
Abu	O
Dhabi	O
Department	O
of	O
Culture	O
and	O
Tourism	O
(	O
2016	O
)	O
and	O
Diwan	O
(	O
)	O
(	O
Diwan	O
,	O
2013	O
)	O
.	O
Unlike	O
Shamela	O
,	O
this	O
dataset	O
was	O
already	O
labeled	O
by	O
era	O
,	O
making	O
it	O
a	O
good	O
choice	O
for	O
studying	O
language	O
change	O
over	O
time	O
.	O
It	O
has	O
,	O
before	O
preprocessing	O
,	O
approximately	O
1	O
,	O
831	O
,	O
770	O
poetic	O
verses	O
labeled	O
by	O
their	O
meter	O
,	O
the	O
poet	O
's	O
name	O
,	O
and	O
the	O
era	O
they	O
were	O
written	O
in	O
.	O
One	O
drawback	O
of	O
this	O
corpora	O
is	O
that	O
it	O
is	O
relatively	O
small	O
.	O
Table	O
2	O
summarizes	O
some	O
key	O
attributes	O
of	O
this	O
dataset	O
.	O
We	O
then	O
produced	O
a	O
total	O
of	O
16	O
GloVe	B-MethodName
models	O
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
from	O
the	O
three	O
time	O
periods	O
of	O
Shamela	O
,	O
the	O
11	O
eras	O
of	O
APCD	O
,	O
all	O
Shamela	O
,	O
and	O
all	O
APCD	O
.	O
2	O
Each	O
GloVe	B-MethodName
model	O
is	O
a	O
contextindependent	O
model	O
that	O
produces	O
a	O
one	O
-	O
word	O
vector	O
(	O
word	O
embedding	O
)	O
for	O
each	O
word	O
even	O
if	O
that	O
word	O
appears	O
in	O
the	O
context	O
a	O
few	O
times	O
unlike	O
BERT	B-MethodName
and	O
ELMo	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2018	O
;	O
Peters	O
et	O
al	O
,	O
2018	O
)	O
.	O
Each	O
GloVe	B-MethodName
model	O
provides	O
vocabulary	O
size	O
,	O
token	O
size	O
,	O
and	O
word	O
vectors	O
.	O
It	O
is	O
important	O
to	O
note	O
that	O
before	O
training	O
GloVe	B-MethodName
models	O
,	O
it	O
was	O
necessary	O
to	O
preprocess	O
the	O
two	O
datasets	O
using	O
Linux	B-DatasetName
/	O
Unix	O
command	O
-	O
line	O
utilities	O
like	O
tr	O
(	O
for	O
translating	O
or	O
deleting	O
characters	O
)	O
,	O
sed	O
(	O
for	O
filtering	O
and	O
transforming	O
text	O
)	O
,	O
iconv	O
(	O
for	O
converting	O
between	O
encoding	O
schemes	O
)	O
,	O
and	O
awk	O
(	O
for	O
pattern	O
scanning	O
and	O
language	O
processing	O
)	O
,	O
along	O
with	O
CAMeL	O
tools	O
(	O
Obeid	O
et	O
al	O
,	O
2020	O
)	O
,	O
an	O
open	O
-	O
source	O
python	O
toolkit	O
for	O
Arabic	O
NLP	O
,	O
to	O
dediacritize	O
the	O
Arabic	O
diacritical	O
marks	O
and	O
remove	O
unnecessary	O
characters	O
.	O

We	O
began	O
with	O
a	O
consideration	O
of	O
how	O
the	O
profession	O
sets	O
used	O
in	O
Bolukbasi	O
et	O
al	O
(	O
2016	O
)	O
and	O
Chen	O
et	O
al	O
(	O
2021	O
)	O
would	O
need	O
to	O
change	O
over	O
time	O
.	O
First	O
,	O
we	O
identified	O
50	O
modern	O
profession	O
words	O
that	O
we	O
expect	O
would	O
simply	O
not	O
exist	O
in	O
the	O
older	O
time	O
periods	O
/	O
eras	O
in	O
Shamela	O
and	O
APCD	O
datasets	O
.	O
3	O
For	O
example	O
,	O
the	O
profession	O
of	O
electrician	O
would	O
not	O
have	O
existed	O
before	O
the	O
advent	O
of	O
electricity	O
.	O
Second	O
,	O
we	O
identified	O
50	O
historical	O
profession	O
words	O
that	O
we	O
think	O
exist	O
in	O
older	O
time	O
periods	O
/	O
eras	O
in	O
Shamela	O
and	O
APCD	O
datasets	O
but	O
which	O
are	O
much	O
less	O
common	O
in	O
modern	O
times	O
.	O
As	O
in	O
Chen	O
et	O
al	O
(	O
2021	O
)	O
,	O
we	O
further	O
categorized	O
each	O
word	O
based	O
on	O
gender	O
.	O
In	O
Arabic	O
,	O
most	O
profession	O
words	O
have	O
a	O
male	O
variant	O
and	O
a	O
female	O
variant	O
in	O
which	O
the	O
spelling	O
is	O
changed	O
slightly	O
based	O
on	O
gender	O
,	O
for	O
example	O
female	O
pilot	O
(	O
)	O
and	O
male	O
pilot	O
(	O
)	O
.	O
Linguistically	O
,	O
many	O
professions	O
that	O
would	O
be	O
extremely	O
uncommon	O
for	O
men	O
or	O
women	O
do	O
have	O
a	O
male	O
or	O
female	O
version	O
of	O
the	O
word	O
(	O
e.g.	O
,	O
it	O
is	O
rare	O
for	O
a	O
woman	O
to	O
have	O
the	O
profession	O
cham	O
-	O
3	O
We	O
point	O
to	O
an	O
expanded	O
technical	O
report	O
with	O
the	O
full	O
list	O
of	O
used	O
modern	O
and	O
historical	O
profession	O
words	O
.	O
The	O
report	O
can	O
be	O
accessed	O
here	O
:	O
https://lin	O
-	O
web.clarkson	O
.	O
edu/~jmatthew	O
/	O
LChange2022/	O
berlain	O
/	O
head	O
of	O
staff	O
(	O
)	O
,	O
but	O
there	O
is	O
a	O
female	O
word	O
for	O
that	O
profession	O
)	O
.	O
However	O
,	O
in	O
some	O
cases	O
,	O
either	O
the	O
male	O
or	O
female	O
version	O
does	O
not	O
even	O
exist	O
linguistically	O
(	O
e.g.	O
,	O
there	O
is	O
no	O
male	O
word	O
of	O
midwife	O
(	O
)	O
profession	O
)	O
.	O
There	O
are	O
also	O
more	O
rare	O
neutral	O
words	O
,	O
like	O
musician	O
(	O
)	O
,	O
that	O
is	O
used	O
for	O
both	O
genders	O
with	O
no	O
spelling	O
changes	O
.	O
In	O
the	O
APCD	O
dataset	O
,	O
we	O
found	O
,	O
as	O
expected	O
,	O
that	O
there	O
are	O
some	O
modern	O
professions	O
that	O
occur	O
noticeably	O
only	O
in	O
the	O
modern	O
era	O
of	O
the	O
Arabic	O
poems	O
,	O
but	O
do	O
not	O
appear	O
at	O
all	O
in	O
the	O
previous	O
historical	O
eras	O
,	O
such	O
as	O
the	O
male	O
engineer	O
(	O
)	O
that	O
occurs	O
17	O
times	O
,	O
and	O
the	O
neutral	O
profession	O
of	O
an	O
electrician	O
(	O
)	O
that	O
occurs	O
only	O
four	O
times	O
in	O
the	O
modern	O
age	O
,	O
indicating	O
that	O
those	O
modern	O
professions	O
are	O
increasingly	O
appearing	O
in	O
the	O
modern	O
age	O
of	O
the	O
Arabic	O
poems	O
and	O
confirming	O
that	O
Arabic	O
native	O
speakers	O
(	O
i.e.	O
,	O
Arabs	O
)	O
still	O
use	O
the	O
poems	O
as	O
an	O
effective	O
way	O
to	O
document	O
the	O
Arabic	O
language	O
changes	O
over	O
time	O
.	O
On	O
the	O
other	O
side	O
of	O
history	O
,	O
in	O
the	O
Shamela	O
dataset	O
,	O
we	O
found	O
that	O
a	O
few	O
historical	O
professions	O
frequently	O
occur	O
in	O
the	O
time	O
periods	O
before	O
1900	O
but	O
not	O
significantly	O
after	O
1900	O
.	O
Some	O
professions	O
reflect	O
essential	O
shifts	O
in	O
legality	O
.	O
For	O
example	O
,	O
one	O
profession	O
that	O
is	O
fortunately	O
no	O
longer	O
legal	O
or	O
acceptable	O
is	O
male	O
slaver	O
(	O
)	O
.	O
Fortunately	O
,	O
the	O
male	O
slaver	O
profession	O
appears	O
much	O
less	O
often	O
(	O
only	O
12	O
times	O
)	O
in	O
the	O
time	O
period	O
after	O
1900	O
,	O
while	O
it	O
appears	O
unpleasantly	O
118	O
times	O
before	O
the	O
1900	O
time	O
periods	O
.	O
As	O
another	O
example	O
,	O
male	O
chamberlain	O
/	O
head	O
of	O
staff	O
(	O
)	O
appears	O
9	O
,	O
518	O
before	O
the	O
1900	O
time	O
periods	O
,	O
but	O
only	O
appears	O
914	O
times	O
in	O
)	O
in	O
Shamela	O
Library	O
dataset	O
in	O
the	O
time	O
period	O
before	O
1900	O
,	O
demonstrating	O
that	O
its	O
word	O
cluster	O
is	O
including	O
different	O
words	O
with	O
different	O
meanings	O
due	O
to	O
its	O
homonymy	O
.	O
b.	O
A	O
word	O
cluster	O
of	O
chosen	O
GloVe	B-MethodName
's	O
most	O
similar	O
words	O
of	O
the	O
female	O
profession	O
trader	O
(	O
)	O
in	O
Shamela	O
Library	O
dataset	O
in	O
the	O
time	O
period	O
after	O
1900	O
,	O
illustrating	O
that	O
a	O
new	O
related	O
-	O
trading	O
activity	O
word	O
joining	O
the	O
profession	O
word	O
cluster	O
,	O
(	O
trade/	O
)	O
4	O
the	O
time	O
period	O
after	O
1900	O
,	O
showing	O
that	O
this	O
male	O
profession	O
/	O
position	O
is	O
on	O
its	O
way	O
to	O
extinction	O
.	O

The	O
Arabic	O
language	O
is	O
one	O
of	O
the	O
most	O
morphologically	O
rich	O
languages	O
,	O
with	O
a	O
high	O
level	O
of	O
orthographic	O
ambiguity	O
,	O
causing	O
native	O
speakers	O
to	O
use	O
the	O
optional	O
diacritical	O
marks	O
to	O
differentiate	O
between	O
two	O
words	O
(	O
Grosvald	O
et	O
al	O
,	O
2019	O
)	O
.	O
5	O
We	O
noticed	O
in	O
the	O
Shamela	O
Library	O
dataset	O
that	O
a	O
few	O
modern	O
profession	O
words	O
change	O
their	O
connotations	O
over	O
time	O
,	O
and	O
many	O
profession	O
words	O
have	O
alternate	O
meanings	O
due	O
to	O
the	O
Arabic	O
's	O
orthographical	O
ambiguity	O
.	O
We	O
also	O
found	O
that	O
this	O
was	O
especially	O
true	O
of	O
female	O
profession	O
words	O
.	O
For	O
example	O
,	O
the	O
word	O
(	O
)	O
for	O
female	O
teacher	O
also	O
means	O
a	O
school	O
building	O
(	O
)	O
,	O
another	O
word	O
(	O
)	O
for	O
a	O
female	O
pilot	O
also	O
means	O
an	O
airplane	O
4	O
English	O
translations	O
of	O
the	O
word	O
clusters	O
are	O
automatically	O
generated	O
using	O
Google	B-DatasetName
Translator	O
API	O
that	O
is	O
included	O
in	O
the	O
deep	O
-	O
translator	O
Python	O
model	O
(	O
https://	O
deep	O
-	O
translator.readthedocs.io	O
)	O
.	O
5	O
In	O
our	O
preprocessing	O
,	O
we	O
removed	O
the	O
optional	O
diacritical	O
marks	O
as	O
is	O
generally	O
recommended	O
for	O
Arabic	O
NLP	O
as	O
a	O
first	O
step	O
to	O
reducing	O
some	O
data	O
sparsity	O
(	O
Obeid	O
et	O
al	O
,	O
2020	O
)	O
.	O
Unfortunately	O
,	O
removing	O
diacritical	O
marks	O
increases	O
the	O
orthographic	O
ambiguity	O
,	O
but	O
retaining	O
them	O
would	O
lead	O
to	O
a	O
high	O
degree	O
of	O
variance	O
for	O
the	O
same	O
word	O
because	O
the	O
placement	O
of	O
diacritical	O
marks	O
varies	O
with	O
the	O
grammatical	O
placement	O
of	O
the	O
word	O
in	O
a	O
sentence	O
.	O
It	O
is	O
a	O
difficult	O
tradeoff	O
for	O
Arabic	O
NLP	O
that	O
other	O
researchers	O
are	O
attempting	O
to	O
tackle	O
with	O
advanced	O
techniques	O
,	O
such	O
as	O
stemming	O
and	O
lemmatization	B-TaskName
(	O
Kadri	O
and	O
Nie	O
,	O
2006	O
;	O
Mubarak	O
,	O
2017	O
)	O
.	O

)	O
.	O
In	O
all	O
these	O
cases	O
,	O
this	O
complicates	O
the	O
use	O
of	O
both	O
word	O
counts	O
and	O
word	B-TaskName
embeddings	I-TaskName
in	O
tracking	O
the	O
relative	O
uses	O
of	O
profession	O
words	O
over	O
time	O
.	O
One	O
homonymous	O
example	O
is	O
the	O
female	O
trader	O
(	O
)	O
profession	O
.	O
The	O
same	O
word	O
(	O
)	O
also	O
means	O
common	O
,	O
famous	O
,	O
familiar	O
,	O
or	O
circulating	O
to	O
describe	O
a	O
current	O
news	O
event	O
.	O
We	O
see	O
this	O
alternate	O
meaning	O
dominate	O
the	O
usage	O
of	O
the	O
word	O
,	O
complicating	O
any	O
attempt	O
to	O
study	O
the	O
prevalence	O
of	O
females	O
engaged	O
in	O
this	O
profession	O
.	O
Interestingly	O
,	O
we	O
see	O
evidence	O
of	O
change	O
over	O
time	O
in	O
the	O
usage	O
of	O
this	O
word	O
.	O
To	O
investigate	O
the	O
semantic	O
meaning	O
of	O
related	O
words	O
to	O
the	O
trading	O
activity	O
,	O
we	O
studied	O
GloVe	B-MethodName
's	O
most	O
similar	O
words	O
(	O
calculated	O
based	O
on	O
the	O
cosine	O
similarity	O
between	O
two	O
word	O
vectors	O
)	O
for	O
this	O
profession	O
word	O
in	O
two	O
time	O
periods	O
of	O
the	O
Shamela	O
Library	O
dataset	O
:	O
before	O
1900	O
and	O
after	O
1900	O
.	O
As	O
shown	O
in	O
Figure	O
1a	O
,	O
before	O
1900	O
,	O
none	O
of	O
most	O
similar	O
words	O
reflect	O
the	O
trading	O
profession	O
word	O
(	O
)	O
.	O
However	O
,	O
in	O
Figure	O
1b	O
,	O
after	O
1900	O
,	O
we	O
see	O
a	O
word	O
related	O
to	O
trading	O
activity	O
(	O
trade/	O
)	O
appear	O
in	O
the	O
most	O
similar	O
words	O
of	O
GloVe	B-MethodName
model	O
.	O
Thus	O
,	O
the	O
connotation	O
of	O
the	O
female	O
trader	O
(	O
)	O
profession	O
is	O
changing	O
over	O
time	O
to	O
more	O
often	O
reflect	O
the	O
actual	O
profession	O
of	O
female	O
trader	O
(	O
)	O
and	O
not	O
just	O
the	O
alternate	O
meaning	O
of	O
current	O
news	O
events	O
.	O

Although	O
our	O
primary	O
focus	O
in	O
this	O
study	O
has	O
been	O
on	O
Arabic	O
,	O
we	O
found	O
interesting	O
evidence	O
of	O
change	O
over	O
time	O
in	O
Chinese	O
as	O
well	O
.	O
Classical	O
Chinese	O
(	O
before	O
1900	O
)	O
uses	O
a	O
vocabulary	O
and	O
grammar	O
that	O
differs	O
significantly	O
from	O
modern	O
Chinese	O
.	O
We	O
were	O
surprised	O
to	O
find	O
evidence	O
not	O
just	O
of	O
changes	O
in	O
professions	O
over	O
time	O
,	O
but	O
also	O
changes	O
in	O
defining	O
set	O
words	O
.	O
As	O
we	O
found	O
in	O
the	O
diachronic	O
corpora	O
in	O
Arabic	O
,	O
we	O
expected	O
changes	O
in	O
profession	O
words	O
over	O
hundreds	O
of	O
years	O
,	O
but	O
thought	O
that	O
the	O
more	O
fundamental	O
defining	O
set	O
words	O
like	O
woman	O
/	O
man	O
,	O
girl	O
/	O
boy	O
and	O
madam	O
/	O
sir	O
would	O
not	O
change	O
substantially	O
.	O
In	O
Chinese	O
,	O
the	O
word	O
'	O
woman	O
'	O
can	O
be	O
translated	O
in	O
many	O
ways	O
,	O
including	O
"	O
女子	O
"	O
,	O
"	O
女人	O
"	O
,	O
and	O
"	O
妇	O
女	O
"	O
.	O
The	O
word	O
"	O
女子	O
"	O
was	O
popularly	O
used	O
in	O
an	O
-	O
Figure	O
3	O
:	O
A	O
timeline	O
of	O
word	O
frequencies	O
of	O
different	O
translations	O
of	O
word	O
'	O
woman	O
'	O
:	O
"	O
女子	O
"	O
,	O
"	O
女人	O
"	O
,	O
and	O
"	O
妇	O
女	O
"	O
that	O
were	O
found	O
in	O
multi	O
-	O
sources	O
printed	O
between	O
1500	O
and	O
2019	O
using	O
Google	B-DatasetName
Books	O
Ngram	O
Viewer	O
.	O
cient	O
times	O
,	O
but	O
its	O
usage	O
has	O
decreased	O
in	O
modern	O
writing	O
.	O
In	O
Figure	O
3	O
,	O
we	O
used	O
Google	B-DatasetName
Books	O
Ngram	O
Viewer	O
to	O
chart	O
the	O
word	O
frequencies	O
of	O
the	O
different	O
translations	O
of	O
the	O
word	O
'	O
woman	O
'	O
:	O
"	O
女子	O
"	O
,	O
"	O
女	O
人	O
"	O
,	O
and	O
"	O
妇女	O
"	O
found	O
in	O
sources	O
printed	O
between	O
1500	O
and	O
2019	O
in	O
Google	B-DatasetName
's	O
Books	O
corpora	O
in	O
English	O
,	O
Chinese	O
,	O
French	O
,	O
German	O
,	O
Hebrew	O
,	O
Italian	O
,	O
Russian	O
,	O
or	O
Spanish	O
(	O
Karch	O
,	O
2021	O
)	O
.	O
This	O
shows	O
us	O
that	O
as	O
languages	O
evolve	O
over	O
time	O
,	O
defining	O
sets	O
,	O
like	O
profession	O
sets	O
,	O
may	O
also	O
have	O
to	O
evolve	O
to	O
measure	O
gender	O
bias	O
using	O
methods	O
like	O
the	O
Bolukbasi	O
et	O
al	O
(	O
2016	O
)	O
's	O
method	O
.	O
Besides	O
using	O
Google	B-DatasetName
Books	O
Ngram	O
Viewer	O
,	O
we	O
also	O
assembled	O
a	O
small	O
collection	O
of	O
works	O
that	O
might	O
be	O
considered	O
"	O
classics	O
"	O
in	O
Chinese	O
spanning	O
the	O
period	O
475	O
BC	O
-	O
1992	O
,	O
for	O
example	O
司	O
马迁	O
(	O
Records	O
of	O
the	O
Grand	O
Historian	O
)	O
by	O
Qian	O
Sima	O
,	O
萧红	O
(	O
Tales	O
of	O
Hulan	O
River	O
)	O
by	O
Hong	O
Xiao	O
,	O
and	O
论语	O
(	O
The	O
Analects	O
)	O
.	O
We	O
found	O
that	O
roughly	O
half	O
of	O
the	O
profession	O
words	O
used	O
by	O
Chen	O
et	O
al	O
(	O
2021	O
)	O
did	O
not	O
appear	O
,	O
and	O
that	O
also	O
two	O
of	O
the	O
defining	O
set	O
words	O
"	O
boy	O
"	O
and	O
"	O
madam	O
"	O
used	O
did	O
not	O
appear	O
.	O
Interestingly	O
,	O
Google	B-DatasetName
Books	O
Ngram	O
Viewer	O
showed	O
that	O
the	O
word	O
'	O
madam	O
'	O
was	O
used	O
very	O
frequently	O
between	O
1905	O
and	O
1910	O
,	O
but	O
our	O
small	O
classics	O
corpora	O
did	O
not	O
include	O
texts	O
written	O
in	O
that	O
time	O
period	O
.	O
Again	O
,	O
these	O
results	O
indicate	O
that	O
as	O
languages	O
evolve	O
over	O
time	O
,	O
profession	O
sets	O
and	O
even	O
defining	O
set	O
words	O
would	O
have	O
to	O
evolve	O
to	O
measure	O
gender	O
bias	O
.	O

In	O
order	O
for	O
NLP	O
to	O
reflect	O
the	O
rich	O
multilingual	O
,	O
multicultural	O
,	O
and	O
historical	O
heritage	O
of	O
human	O
text	O
,	O
it	O
is	O
essential	O
that	O
NLP	O
techniques	O
be	O
extended	O
beyond	O
modern	O
digital	O
English	O
text	O
to	O
multilingual	O
and	O
diachronic	O
corpora	O
.	O
In	O
this	O
paper	O
,	O
we	O
have	O
explored	O
the	O
challenges	O
of	O
applying	O
an	O
important	O
technique	O
for	O
measuring	O
the	O
gender	O
bias	O
learned	O
by	O
word	O
embedding	O
systems	O
to	O
diachronic	O
corpora	O
.	O
We	O
also	O
have	O
shown	O
how	O
techniques	O
like	O
those	O
pioneered	O
by	O
Bolukbasi	O
et	O
al	O
(	O
2016	O
)	O
and	O
extended	O
by	O
Chen	O
et	O
al	O
(	O
2021	O
)	O
have	O
fundamental	O
limitations	O
when	O
analyzing	O
corpora	O
spanning	O
large	O
periods	O
of	O
time	O
.	O
We	O
showed	O
that	O
their	O
technique	O
based	O
on	O
analyzing	O
the	O
gender	O
bias	O
of	O
profession	O
words	O
would	O
have	O
difficulty	O
because	O
professions	O
change	O
drastically	O
over	O
hundreds	O
of	O
years	O
.	O
Interestingly	O
,	O
we	O
also	O
documented	O
changes	O
in	O
defining	O
and	O
profession	O
set	O
words	O
over	O
time	O
and	O
also	O
challenges	O
with	O
polysemous	O
/	O
homonymous	O
profession	O
words	O
especially	O
female	O
profession	O
words	O
in	O
Arabic	O
.	O
In	O
this	O
paper	O
,	O
we	O
have	O
focused	O
mostly	O
on	O
identifying	O
the	O
problems	O
with	O
techniques	O
applied	O
successfully	O
to	O
measure	O
gender	O
bias	O
in	O
modern	O
corpora	O
like	O
Google	B-DatasetName
News	O
or	O
Wikipedia	O
.	O
In	O
the	O
future	O
work	O
,	O
we	O
plan	O
to	O
focus	O
more	O
on	O
modifying	O
profession	O
sets	O
and	O
defining	O
sets	O
over	O
time	O
to	O
overcome	O
these	O
problems	O
.	O
Our	O
results	O
indicate	O
that	O
as	O
languages	O
evolve	O
over	O
time	O
,	O
defining	O
sets	O
and	O
profession	O
sets	O
would	O
have	O
to	O
evolve	O
to	O
measure	O
gender	O
bias	O
.	O
In	O
this	O
study	O
,	O
we	O
focused	O
on	O
Arabic	O
and	O
Chinese	O
,	O
but	O
we	O
would	O
like	O
to	O
extend	O
our	O
work	O
to	O
more	O
languages	O
.	O
Adding	O
an	O
English	O
corpora	O
may	O
be	O
our	O
next	O
step	O
.	O
Although	O
we	O
like	O
to	O
actively	O
focus	O
on	O
languages	O
besides	O
English	O
,	O
English	O
can	O
serve	O
as	O
an	O
important	O
comparison	O
point	O
because	O
so	O
much	O
of	O
the	O
modern	O
NLP	O
tool	O
chain	O
has	O
been	O
optimized	O
for	O
English	O
.	O
We	O
may	O
be	O
able	O
to	O
study	O
the	O
impact	O
of	O
changes	O
in	O
profession	O
sets	O
and	O
defining	O
sets	O
over	O
time	O
with	O
fewer	O
complicating	O
factors	O
.	O
We	O
would	O
also	O
like	O
to	O
experiment	O
with	O
different	O
advanced	O
Arabic	O
NLP	O
techniques	O
like	O
stemming	O
and	O
lemmatization	B-TaskName
(	O
Kadri	O
and	O
Nie	O
,	O
2006	O
;	O
Mubarak	O
,	O
2017	O
)	O
and	O
see	O
how	O
applying	O
such	O
techniques	O
could	O
improve	O
the	O
results	O
and	O
reduce	O
Arabic	O
's	O
orthographical	O
ambiguity	O
or	O
even	O
other	O
Arabic	O
NLP	O
-	O
related	O
current	O
issues	O
like	O
correcting	O
spelling	O
errors	O
,	O
especially	O
in	O
Arabic	O
dialects	O
,	O
where	O
there	O
are	O
no	O
official	O
orthography	O
rules	O
(	O
Habash	O
et	O
al	O
,	O
2018	O
)	O
.	O

Pretrained	O
multilingual	O
language	O
models	O
have	O
become	O
a	O
common	O
tool	O
in	O
transferring	O
NLP	O
capabilities	O
to	O
low	O
-	O
resource	O
languages	O
,	O
often	O
with	O
adaptations	O
.	O
In	O
this	O
work	O
,	O
we	O
study	O
the	O
performance	O
,	O
extensibility	O
,	O
and	O
interaction	O
of	O
two	O
such	O
adaptations	O
:	O
vocabulary	O
augmentation	O
and	O
script	O
transliteration	O
.	O
Our	O
evaluations	O
on	O
part	B-TaskName
-	I-TaskName
of	I-TaskName
-	I-TaskName
speech	I-TaskName
tagging	I-TaskName
,	O
universal	O
dependency	B-TaskName
parsing	I-TaskName
,	O
and	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
in	O
nine	O
diverse	O
low	O
-	O
resource	O
languages	O
uphold	O
the	O
viability	O
of	O
these	O
approaches	O
while	O
raising	O
new	O
questions	O
around	O
how	O
to	O
optimally	O
adapt	O
multilingual	O
models	O
to	O
low	O
-	O
resource	O
settings	O
.	O

We	O
expand	O
on	O
the	O
dependency	B-TaskName
parsing	I-TaskName
evaluations	O
of	O
Chau	O
et	O
al	O
(	O
2020	O
)	O
by	O
additionally	O
considering	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
and	O
part	B-TaskName
-	I-TaskName
of	I-TaskName
-	I-TaskName
speech	I-TaskName
tagging	I-TaskName
.	O
We	O
follow	O
Kondratyuk	O
and	O
Straka	O
(	O
2019	O
)	O
and	O
compute	O
the	O
CWR	O
for	O
each	O
token	O
as	O
a	O
weighted	O
sum	O
of	O
the	O
activations	O
at	O
each	O
MBERT	B-MethodName
layer	O
.	O
For	O
dependency	B-TaskName
parsing	I-TaskName
,	O
we	O
follow	O
the	O
setup	O
of	O
Chau	O
et	O
al	O
(	O
2020	O
)	O
and	O
Muller	O
et	O
al	O
(	O
2021	O
)	O
and	O
use	O
the	O
CWRs	O
as	O
input	O
to	O
the	O
graph	O
-	O
based	O
dependency	O
parser	O
of	O
Dozat	O
and	O
Manning	O
(	O
2017	O
)	O
.	O
For	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
,	O
the	O
CWRs	O
are	O
used	O
as	O
input	O
to	O
a	O
CRF	B-MethodName
layer	O
,	O
while	O
part	B-TaskName
-	I-TaskName
of	I-TaskName
-	I-TaskName
speech	I-TaskName
tagging	I-TaskName
uses	O
a	O
linear	O
projection	O
atop	O
the	O
representations	O
.	O
In	O
all	O
cases	O
,	O
the	O
underlying	O
CWRs	O
are	O
finetuned	O
during	O
downstream	O
task	O
training	O
,	O
and	O
we	O
do	O
not	O
add	O
an	O
additional	O
encoder	O
layer	O
above	O
the	O
transformer	O
outputs	O
.	O
We	O
train	O
models	O
on	O
five	O
different	O
random	O
seeds	B-DatasetName
and	O
report	O
average	O
scores	O
and	O
standard	O
errors	O
.	O

We	O
select	O
a	O
set	O
of	O
nine	O
typologically	O
diverse	O
lowresource	O
languages	O
for	O
evaluation	O
,	O
including	O
three	O
of	O
the	O
original	O
four	O
used	O
by	O
Chau	O
et	O
al	O
(	O
2020	O
)	O
.	O
These	O
languages	O
use	O
three	O
different	O
scripts	O
and	O
are	O
chosen	O
based	O
on	O
the	O
availability	O
of	O
labeled	O
datasets	O
and	O
their	O
exemplification	O
of	O
the	O
three	O
language	O
types	O
identified	O
by	O
Chau	O
et	O
al	O
(	O
2020	O
)	O
.	O
Of	O
the	O
lan	O
-	O
guages	O
seen	O
by	O
MBERT	B-MethodName
,	O
all	O
selected	O
Type	O
0	B-DatasetName
languages	O
are	O
within	O
the	O
45	O
largest	O
Wikipedias	O
,	O
while	O
the	O
remaining	O
Type	O
1	O
languages	O
are	O
within	O
the	O
top	O
100	O
.	O
The	O
Type	O
2	O
languages	O
,	O
which	O
are	O
excluded	O
from	O
MBERT	B-MethodName
,	O
are	O
all	O
outside	O
of	O
the	O
top	O
150	O
.	O
6	O
Additional	O
information	O
about	O
the	O
evaluation	O
languages	O
is	O
given	O
in	O
Tab	O
.	O
1	O
.	O
Unlabeled	O
Datasets	O
Following	O
Chau	O
et	O
al	O
(	O
2020	O
)	O
,	O
we	O
use	O
articles	O
from	O
Wikipedia	O
as	O
unlabeled	O
data	O
for	O
additional	O
pretraining	O
in	O
order	O
to	O
reflect	O
the	O
original	O
pretraining	O
data	O
.	O
We	O
downsample	O
full	O
articles	O
from	O
the	O
largest	O
Wikipedias	O
to	O
be	O
on	O
the	O
order	O
of	O
millions	O
of	O
tokens	O
in	O
order	O
to	O
simulate	O
a	O
low	O
-	O
resource	O
unlabeled	O
setting	O
,	O
and	O
we	O
remove	O
sentences	O
that	O
appear	O
in	O
the	O
labeled	O
validation	O
or	O
test	O
sets	O
.	O
Labeled	O
Datasets	O
For	O
dependency	B-TaskName
parsing	I-TaskName
and	O
part	B-TaskName
-	I-TaskName
of	I-TaskName
-	I-TaskName
speech	I-TaskName
tagging	I-TaskName
,	O
we	O
use	O
datasets	O
and	O
train	O
/	O
test	O
splits	O
from	O
Universal	B-DatasetName
Dependencies	I-DatasetName
(	O
Nivre	O
et	O
al	O
,	O
2020	O
)	O
,	O
version	O
2.5	O
(	O
Zeman	O
et	O
al	O
,	O
2019	O
)	O
.	O
POS	O
tagging	O
uses	O
language	O
-	O
specific	O
partof	O
-	O
speech	O
tags	O
(	O
XPOS	O
)	O
to	O
evaluate	O
understanding	O
of	O
language	O
-	O
specific	O
syntactic	O
phenomena	O
.	O
The	O
Belarusian	O
treebank	O
lacks	O
XPOS	O
tags	O
for	O
certain	O
examples	O
,	O
so	O
we	O
use	O
universal	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
tags	O
instead	O
.	O
Dependency	O
parsers	O
are	O
trained	O
with	O
gold	O
word	O
segmentation	O
and	O
no	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
features	O
.	O
Experiments	O
with	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
use	O
the	O
WikiAnn	B-DatasetName
dataset	O
(	O
Pan	O
et	O
al	O
,	O
2017	O
)	O
,	O
following	O
past	O
work	O
(	O
Muller	O
et	O
al	O
,	O
2021	O
;	O
Pfeiffer	O
et	O
al	O
,	O
2020	O
;	O
Wu	O
and	O
Dredze	O
,	O
2020	O
)	O
.	O
Specifically	O
,	O
we	O
use	O
the	O
balanced	O
train	O
/	O
test	O
splits	O
of	O
(	O
Rahimi	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
note	O
that	O
UD	B-DatasetName
datasets	O
were	O
unavailable	O
for	O
Meadow	O
Mari	O
,	O
and	O
partitioned	O
WikiAnn	B-DatasetName
datasets	O
were	O
missing	O
for	O
Wolof	O
.	O

To	O
measure	O
the	O
effectiveness	O
of	O
VA	O
,	O
we	O
benchmark	O
it	O
against	O
unadapted	O
MBERT	B-MethodName
,	O
as	O
well	O
as	O
directly	O
pretraining	O
MBERT	B-MethodName
on	O
the	O
unlabeled	O
data	O
without	O
modifying	O
the	O
vocabulary	O
(	O
Chau	O
et	O
al	O
,	O
2020	O
;	O
Muller	O
et	O
al	O
,	O
2021	O
;	O
Pfeiffer	O
et	O
al	O
,	O
2020	O
)	O
.	O
Following	O
Chau	O
et	O
al	O
(	O
2020	O
)	O
,	O
we	O
refer	O
to	O
the	O
latter	O
approach	O
as	O
language	O
-	O
adaptive	O
pretraining	O
(	O
LAPT	O
)	O
.	O
We	O
also	O
evaluate	O
two	O
monolingual	O
baselines	O
that	O
are	O
trained	O
on	O
our	O
unlabeled	O
data	O
:	O
fastText	B-MethodName
embeddings	O
(	O
FASTT	O
;	O
Bojanowski	O
et	O
al	O
,	O
2017	O
)	O
,	O
which	O
represent	O
a	O
static	O
word	O
vector	O
approach	O
;	O
and	O
a	O
BERT	B-MethodName
model	O
trained	O
from	O
scratch	O
(	O
BERT	B-MethodName
)	O
.	O
For	O
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
with	O
a	O
language	O
-	O
specific	O
SentencePiece	B-MethodName
tokenizer	O
(	O
Kudo	O
and	O
Richardson	O
,	O
2018	O
)	O
.	O
For	O
a	O
fair	O
comparison	O
to	O
VA	O
,	O
we	O
use	O
the	O
same	O
task	O
-	O
specific	O
architectures	O
and	O
modify	O
only	O
the	O
input	O
representations	O
.	O

Tab	O
.	O
2	O
presents	O
performance	O
of	O
the	O
different	O
input	O
representations	O
on	O
POS	O
tagging	O
,	O
dependency	B-TaskName
parsing	I-TaskName
,	O
and	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
.	O
VA	O
achieves	O
strong	O
results	O
across	O
all	O
languages	O
and	O
tasks	O
and	O
is	O
the	O
top	O
performer	O
in	O
the	O
majority	O
of	O
them	O
,	O
suggesting	O
that	O
augmenting	O
the	O
vocabulary	O
addresses	O
MBERT	B-MethodName
's	O
limited	O
vocabulary	O
coverage	O
of	O
the	O
target	O
language	O
and	O
is	O
beneficial	O
during	O
continued	O
pretraining	O
.	O
The	O
relative	O
gains	O
that	O
VA	O
provides	O
appear	O
to	O
correlate	O
not	O
only	O
with	O
language	O
type	O
,	O
as	O
in	O
the	O
findings	O
of	O
Chau	O
et	O
al	O
(	O
2020	O
)	O
,	O
but	O
also	O
with	O
each	O
language	O
's	O
script	O
.	O
For	O
instance	O
,	O
in	O
Vietnamese	O
,	O
which	O
is	O
a	O
Type	O
0	B-DatasetName
Latin	O
script	O
language	O
,	O
the	O
improvements	O
from	O
VA	O
are	O
marginal	O
at	O
best	O
,	O
reflecting	O
the	O
Latindominated	O
pretraining	O
data	O
of	O
MBERT	B-MethodName
.	O
Irish	O
,	O
the	O
Type	O
1	O
Latin	O
script	O
language	O
,	O
is	O
only	O
slightly	O
more	O
receptive	O
.	O
However	O
,	O
Type	O
0	B-DatasetName
languages	O
in	O
Cyrillic	O
and	O
Arabic	O
scripts	O
,	O
which	O
are	O
less	O
represented	O
in	O
MBERT	B-MethodName
's	O
pretraining	O
data	O
,	O
are	O
more	O
receptive	O
to	O
VA	O
,	O
with	O
VA	O
even	O
outperforming	O
all	O
other	O
methods	O
for	O
Urdu	B-DatasetName
.	O
This	O
trend	O
is	O
amplified	O
in	O
the	O
Type	O
2	O
languages	O
,	O
as	O
the	O
improvements	O
for	O
Maltese	O
and	O
Wolof	O
are	O
small	O
but	O
significant	O
.	O
However	O
,	O
they	O
are	O
dwarfed	O
in	O
magnitude	O
by	O
those	O
of	O
Uyghur	O
,	O
where	O
VA	O
achieves	O
up	O
to	O
a	O
57	O
%	O
relative	O
error	O
reduction	O
over	O
LAPT	O
.	O
This	O
result	O
corroborates	O
the	O
findings	O
of	O
both	O
Chau	O
et	O
al	O
(	O
2020	O
)	O
and	O
Muller	O
et	O
al	O
(	O
2021	O
)	O
and	O
answers	O
RQ1	O
.	O
Prior	O
to	O
specialization	O
,	O
MBERT	B-MethodName
is	O
especially	O
poorly	O
equipped	O
to	O
handle	O
unseen	O
lowresource	O
languages	O
and	O
languages	O
in	O
non	O
-	O
Latin	O
scripts	O
due	O
to	O
its	O
inability	O
to	O
model	O
the	O
script	O
itself	O
.	O
In	O
such	O
cases	O
,	O
specialization	O
via	O
VA	O
is	O
beneficial	O
,	O
providing	O
MBERT	B-MethodName
with	O
explicit	O
signal	O
about	O
the	O
target	O
language	O
and	O
script	O
while	O
maintaining	O
its	O
language	O
-	O
agnostic	O
insights	O
.	O
On	O
the	O
other	O
hand	O
,	O
this	O
also	O
motivates	O
additional	O
investigation	O
into	O
reme	O
-	O
dies	O
for	O
the	O
script	O
imbalance	O
at	O
a	O
larger	O
scale	O
,	O
e.g.	O
,	O
more	O
diverse	O
pretraining	O
data	O
.	O

Tab	O
.	O
4	O
gives	O
the	O
results	O
of	O
our	O
transliteration	O
mix	O
-	O
in	O
experiments	O
.	O
For	O
the	O
MBERT	B-MethodName
-	O
based	O
models	O
,	O
both	O
VA	O
and	O
transliteration	O
provide	O
strong	O
improvements	O
over	O
their	O
respective	O
baselines	O
.	O
Specifically	O
,	O
the	O
improvements	O
from	O
LAPT	O
to	O
VA	O
and	O
LAPT	O
to	O
LAPT	O
with	O
transliteration	O
are	O
most	O
pronounced	O
.	O
This	O
verifies	O
the	O
independent	O
results	O
of	O
Chau	O
et	O
al	O
(	O
2020	O
)	O
and	O
Muller	O
et	O
al	O
(	O
2021	O
)	O
and	O
suggests	O
that	O
in	O
the	O
non	O
-	O
Latin	O
low	O
-	O
resource	O
setting	O
,	O
unadapted	O
additional	O
pretraining	O
is	O
insufficient	O
,	O
but	O
that	O
the	O
mix	O
-	O
in	O
stage	O
between	O
initial	O
and	O
additional	O
pretraining	O
is	O
amenable	O
to	O
performance	O
-	O
improving	O
modifications	O
.	O
Unsurprisingly	O
,	O
transliteration	O
provides	O
no	O
consistent	O
improvement	O
to	O
the	O
monolingual	O
baselines	O
,	O
since	O
the	O
noisy	O
transliteration	O
process	O
removes	O
information	O
without	O
improving	O
crosslingual	O
alignment	O
.	O
However	O
,	O
VA	O
and	O
transliteration	O
appear	O
to	O
interact	O
negatively	O
.	O
Although	O
VA	O
with	O
transliteration	O
i	O
m	O
-	O
proves	O
over	O
plain	O
VA	O
for	O
Uyghur	O
POS	O
tagging	O
and	O
dependency	B-TaskName
parsing	I-TaskName
,	O
it	O
still	O
slightly	O
underperforms	O
LAPT	O
with	O
transliteration	O
for	O
the	O
latter	O
.	O
For	O
the	O
two	O
NER	B-TaskName
experiments	O
,	O
VA	O
with	O
transliteration	O
lags	O
both	O
methods	O
independently	O
.	O
One	O
possible	O
explanation	O
is	O
that	O
transliteration	O
into	O
Latin	O
script	O
serves	O
as	O
implicit	O
vocabulary	O
augmentation	O
,	O
with	O
embeddings	O
that	O
have	O
already	O
been	O
updated	O
during	O
the	O
initial	O
pretraining	O
stage	O
;	O
as	O
a	O
result	O
,	O
the	O
two	O
sources	O
of	O
augmentation	O
conflict	O
.	O
Alternatively	O
,	O
since	O
the	O
transliteration	O
process	O
merges	O
certain	O
characters	O
that	O
are	O
distinct	O
in	O
the	O
original	O
script	O
,	O
VA	O
may	O
augment	O
the	O
vocabulary	O
with	O
misleading	O
character	O
clusters	O
.	O
Either	O
way	O
,	O
additional	O
vocabulary	O
augmentation	O
is	O
generally	O
not	O
as	O
useful	O
when	O
combined	O
with	O
transliteration	O
,	O
answering	O
RQ2	O
.	O
Nonetheless	O
,	O
additional	O
investigation	O
into	O
the	O
optimal	O
amount	O
of	O
vocabulary	O
augmentation	O
might	O
yield	O
a	O
configuration	O
that	O
is	O
consistently	O
complementary	O
to	O
transliteration	O
and	O
is	O
an	O
interesting	O
direction	O
for	O
future	O
work	O
.	O
Furthermore	O
,	O
designing	O
linguistically	O
-	O
informed	O
transliteration	O
schemes	O
like	O
those	O
devised	O
by	O
Muller	O
et	O
al	O
(	O
2021	O
)	O
for	O
Uyghur	O
requires	O
large	O
amounts	O
of	O
time	O
and	O
domain	O
knowledge	O
.	O
VA	O
's	O
fully	O
data	O
-	O
driven	O
nature	O
and	O
relatively	O
comparable	O
performance	O
suggest	O
that	O
it	O
achieves	O
an	O
appealing	O
balance	O
between	O
performance	O
gain	O
and	O
implementation	O
difficulty	O
.	O

Our	O
work	O
follows	O
a	O
long	O
line	O
of	O
studies	O
investigating	O
the	O
performance	O
of	O
multilingual	O
language	O
models	O
like	O
MBERT	B-MethodName
in	O
various	O
settings	O
.	O
The	O
exact	O
source	O
of	O
such	O
models	O
'	O
crosslingual	O
ability	O
is	O
contested	O
:	O
early	O
studies	O
attributed	O
MBERT	B-MethodName
's	O
success	O
to	O
vocabulary	O
overlap	O
between	O
languages	O
(	O
Cao	O
et	O
al	O
,	O
2020	O
;	O
Pires	O
et	O
al	O
,	O
2019	O
;	O
Wu	O
and	O
Dredze	O
,	O
2019	O
)	O
but	O
subsequent	O
studies	O
find	O
typological	O
similarity	O
and	O
parameter	O
sharing	O
to	O
be	O
better	O
explanations	O
(	O
Conneau	O
et	O
al	O
,	O
2020b	O
;	O
K	O
et	O
al	O
,	O
2020	O
)	O
.	O
Nonetheless	O
,	O
past	O
work	O
has	O
consistently	O
highlighted	O
the	O
limitations	O
of	O
multilingual	O
models	O
in	O
the	O
context	O
of	O
low	O
-	O
resource	O
languages	O
.	O
Conneau	O
et	O
al	O
(	O
2020a	O
)	O
highlight	O
the	O
tension	O
between	O
crosslingual	O
transfer	O
and	O
per	O
-	O
language	O
model	O
capacity	O
,	O
which	O
poses	O
a	O
challenge	O
for	O
low	O
-	O
resource	O
languages	O
that	O
require	O
both	O
.	O
Indeed	O
,	O
Wu	O
and	O
Dredze	O
(	O
2020	O
)	O
find	O
that	O
MBERT	B-MethodName
is	O
unable	O
to	O
outperform	O
baselines	O
in	O
the	O
lowest	O
-	O
resource	O
seen	O
languages	O
.	O
Our	O
experiments	O
build	O
off	O
these	O
insights	O
,	O
which	O
motivate	O
the	O
development	O
of	O
methods	O
for	O
adapting	O
MBERT	B-MethodName
to	O
target	O
low	O
-	O
resource	O
languages	O
.	O
Adapting	O
Language	O
Models	O
Several	O
prior	O
studies	O
have	O
proposed	O
methods	O
for	O
adapting	O
pretrained	O
models	O
to	O
a	O
downstream	O
task	O
.	O
The	O
simplest	O
of	O
these	O
is	O
to	O
perform	O
additional	O
pretraining	O
on	O
unlabeled	O
data	O
in	O
the	O
target	O
language	O
(	O
Chau	O
et	O
al	O
,	O
2020	O
;	O
Muller	O
et	O
al	O
,	O
2021	O
;	O
Pfeiffer	O
et	O
al	O
,	O
2020	O
)	O
,	O
which	O
in	O
turn	O
builds	O
off	O
similar	O
approaches	O
for	O
domain	B-TaskName
adaptation	I-TaskName
(	O
Gururangan	O
et	O
al	O
,	O
2020	O
;	O
Han	O
and	O
Eisenstein	O
,	O
2019	O
)	O
.	O
Recent	O
work	O
uses	O
one	O
or	O
more	O
of	O
these	O
additional	O
pretraining	O
stages	O
to	O
specifically	O
train	O
modular	O
adapter	O
layers	O
for	O
specific	O
tasks	O
or	O
languages	O
,	O
with	O
the	O
goal	O
of	O
maintaining	O
a	O
language	O
-	O
agnostic	O
model	O
while	O
improving	O
performance	O
on	O
individual	O
languages	O
(	O
Pfeiffer	O
et	O
al	O
,	O
2020	O
(	O
Pfeiffer	O
et	O
al	O
,	O
,	O
2021aVidoni	O
et	O
al	O
,	O
2020	O
)	O
.	O
However	O
,	O
as	O
Muller	O
et	O
al	O
(	O
2021	O
)	O
note	O
,	O
the	O
typological	O
diversity	O
of	O
the	O
world	O
's	O
languages	O
ultimately	O
limits	O
the	O
viability	O
of	O
this	O
approach	O
.	O
On	O
the	O
other	O
hand	O
,	O
many	O
adaptation	O
techniques	O
have	O
focused	O
on	O
improving	O
representation	O
of	O
the	O
target	O
language	O
by	O
modifying	O
the	O
model	O
's	O
vocabulary	O
or	O
tokenization	O
schemes	O
(	O
Chung	O
et	O
al	O
,	O
2020	O
;	O
Clark	O
et	O
al	O
,	O
2021	O
;	O
Wang	O
et	O
al	O
,	O
2021	O
)	O
.	O
This	O
is	O
wellmotivated	O
:	O
Artetxe	O
et	O
al	O
(	O
2020	O
)	O
emphasize	O
representation	O
in	O
the	O
vocabulary	O
as	O
a	O
key	O
factor	O
for	O
effective	O
crosslingual	O
transfer	O
,	O
while	O
Rust	O
et	O
al	O
(	O
2021	O
)	O
find	O
that	O
MBERT	B-MethodName
's	O
tokenization	O
scheme	O
for	O
many	O
languages	O
is	O
subpar	O
.	O
Pfeiffer	O
et	O
al	O
(	O
2021b	O
)	O
further	O
observe	O
that	O
for	O
languages	O
with	O
unseen	O
scripts	O
,	O
a	O
large	O
proportion	O
of	O
the	O
language	O
is	O
mapped	O
to	O
the	O
generic	O
"	O
unknown	O
"	O
wordpiece	B-MethodName
,	O
and	O
they	O
propose	O
a	O
matrix	O
factorization	O
-	O
based	O
approach	O
to	O
improve	O
script	O
representation	O
.	O
Wang	O
et	O
al	O
(	O
2020	O
)	O
extend	O
MBERT	B-MethodName
's	O
vocabulary	O
with	O
an	O
entire	O
new	O
vocabulary	O
in	O
the	O
target	O
language	O
to	O
facilitate	O
zero	O
-	O
shot	O
transfer	O
to	O
low	O
-	O
resource	O
languages	O
from	O
English	O
.	O
The	O
present	O
study	O
most	O
closely	O
derives	O
from	O
Chau	O
et	O
al	O
(	O
2020	O
)	O
,	O
who	O
select	O
99	O
wordpieces	O
with	O
the	O
greatest	O
amount	O
of	O
coverage	O
to	O
augment	O
MBERT	B-MethodName
's	O
vocabulary	O
while	O
preserving	O
the	O
remainder	O
;	O
and	O
Muller	O
et	O
al	O
(	O
2021	O
)	O
,	O
who	O
transliterate	O
target	O
language	O
data	O
into	O
Latin	O
script	O
to	O
improve	O
vocabulary	O
coverage	O
.	O
We	O
deliver	O
new	O
insights	O
on	O
the	O
effectiveness	O
and	O
applicability	O
of	O
these	O
methods	O
.	O

We	O
explore	O
the	O
interactions	O
between	O
vocabulary	O
augmentation	O
and	O
script	O
transliteration	O
for	O
specializing	O
multilingual	O
contextual	O
word	O
representations	O
in	O
low	O
-	O
resource	O
settings	O
.	O
We	O
confirm	O
vocabulary	O
augmentation	O
's	O
effectiveness	O
on	O
multiple	O
languages	O
,	O
scripts	O
,	O
and	O
tasks	O
;	O
identify	O
the	O
mix	O
-	O
in	O
stage	O
as	O
amenable	O
to	O
specialization	O
;	O
and	O
observe	O
a	O
negative	O
interaction	O
between	O
vocabulary	O
augmentation	O
and	O
script	O
transliteration	O
.	O
Our	O
findings	O
highlight	O
several	O
open	O
questions	O
in	O
model	O
specialization	O
and	O
low	O
-	O
resource	O
natural	O
language	O
processing	O
at	O
large	O
,	O
motivating	O
further	O
study	O
in	O
this	O
area	O
.	O
Future	O
directions	O
for	O
investigation	O
are	O
manifold	O
.	O
In	O
particular	O
,	O
our	O
results	O
in	O
this	O
work	O
unify	O
the	O
separate	O
findings	O
of	O
past	O
works	O
,	O
which	O
use	O
MBERT	B-MethodName
as	O
a	O
case	O
study	O
;	O
a	O
natural	O
continuation	O
would	O
extend	O
these	O
methods	O
to	O
a	O
broader	O
set	O
of	O
multilingual	O
models	O
,	O
such	O
as	O
mT5	B-MethodName
(	O
Xue	O
et	O
al	O
,	O
2021	O
)	O
and	O
XLM	B-MethodName
-	O
R	O
(	O
Conneau	O
et	O
al	O
,	O
2020a	O
)	O
,	O
in	O
order	O
to	O
obtain	O
a	O
clearer	O
understanding	O
of	O
the	O
factors	O
behind	O
specialization	O
methods	O
'	O
patterns	O
of	O
success	O
.	O
While	O
we	O
intentionally	O
choose	O
a	O
set	O
of	O
small	O
unlabeled	O
datasets	O
to	O
evaluate	O
on	O
a	O
setting	O
applicable	O
to	O
the	O
vast	O
majority	O
of	O
the	O
world	O
's	O
low	O
-	O
resource	O
languages	O
,	O
we	O
acknowl	O
-	O
edge	O
great	O
variation	O
in	O
the	O
amount	O
of	O
unlabeled	O
data	O
available	O
in	O
different	O
languages	O
.	O
Continued	O
study	O
on	O
the	O
applicability	O
of	O
these	O
methods	O
to	O
datasets	O
of	O
different	O
sizes	O
is	O
an	O
important	O
future	O
step	O
.	O
An	O
interesting	O
direction	O
of	O
work	O
is	O
to	O
train	O
multilingual	O
models	O
on	O
data	O
where	O
script	O
respresentation	O
is	O
more	O
balanced	O
,	O
which	O
might	O
also	O
allow	O
for	O
different	O
output	O
scripts	O
for	O
transliteration	O
.	O
Given	O
that	O
the	O
mix	O
-	O
in	O
stage	O
is	O
an	O
effective	O
opportunity	O
to	O
specialize	O
models	O
to	O
target	O
languages	O
,	O
constructing	O
mix	O
-	O
ins	O
at	O
both	O
the	O
data	O
and	O
model	O
level	O
that	O
are	O
complementary	O
by	O
design	O
has	O
potential	O
to	O
be	O
beneficial	O
.	O
Finally	O
,	O
future	O
work	O
might	O
shed	O
light	O
on	O
the	O
interaction	O
between	O
different	O
configurations	O
of	O
the	O
adaptations	O
studied	O
here	O
(	O
e.g.	O
,	O
the	O
number	O
of	O
wordpiece	B-MethodName
types	O
used	O
in	O
vocabulary	O
augmentation	O
)	O
.	O

We	O
propose	O
a	O
framework	O
that	O
captures	O
the	O
denotational	O
probabilities	O
of	O
words	O
and	O
phrases	O
by	O
embedding	O
them	O
in	O
a	O
vector	O
space	O
,	O
and	O
present	O
a	O
method	O
to	O
induce	O
such	O
an	O
embedding	O
from	O
a	O
dataset	O
of	O
denotational	O
probabilities	O
.	O
We	O
show	O
that	O
our	O
model	O
successfully	O
predicts	O
denotational	O
probabilities	O
for	O
unseen	O
phrases	O
,	O
and	O
that	O
its	O
predictions	O
are	O
useful	O
for	O
textual	O
entailment	O
datasets	O
such	O
as	O
SICK	B-DatasetName
and	O
SNLI	B-DatasetName
.	O

The	O
goal	O
of	O
textual	O
entailment	O
is	O
to	O
predict	O
whether	O
a	O
hypothesis	O
sentence	O
is	O
true	O
,	O
false	O
,	O
or	O
neither	O
based	O
on	O
the	O
premise	O
text	O
(	O
Dagan	O
et	O
al	O
,	O
2013	O
)	O
.	O
Due	O
in	O
part	O
to	O
the	O
Recognizing	O
Textual	O
Entailment	O
(	O
RTE	B-DatasetName
)	O
challenges	O
(	O
Dagan	O
et	O
al	O
,	O
2006	O
)	O
,	O
the	O
task	O
of	O
textual	O
entailment	O
recognition	O
has	O
received	O
a	O
lot	O
of	O
attention	O
in	O
recent	O
years	O
.	O
Although	O
full	O
entailment	O
recognition	O
systems	O
typically	O
require	O
a	O
complete	O
NLP	O
pipeline	O
,	O
including	O
coreference	B-TaskName
resolution	I-TaskName
,	O
etc	O
.	O
,	O
this	O
paper	O
considers	O
a	O
simplified	O
variant	O
of	O
this	O
task	O
in	O
which	O
the	O
premise	O
and	O
hypothesis	O
are	O
each	O
a	O
single	O
sentence	O
.	O
This	O
simplified	O
task	O
allows	O
us	O
to	O
ignore	O
the	O
complexities	O
that	O
arise	O
in	O
longer	O
texts	O
,	O
and	O
instead	O
focus	O
on	O
the	O
purely	O
semantic	O
problem	O
of	O
how	O
to	O
represent	O
the	O
meaning	O
of	O
sentences	O
.	O
This	O
version	O
of	O
the	O
textual	O
entailment	O
task	O
has	O
been	O
popularized	O
by	O
two	O
datasets	O
,	O
the	O
Sentences	O
Involving	O
Compositional	O
Knowl	O
-	O
edge	O
(	O
SICK	B-DatasetName
)	O
dataset	O
(	O
Marelli	O
et	O
al	O
,	O
2014	O
)	O
and	O
the	O
Stanford	O
Natural	B-TaskName
Language	I-TaskName
Inference	I-TaskName
(	O
SNLI	B-DatasetName
)	O
corpus	O
(	O
Bowman	O
et	O
al	O
,	O
2015	O
)	O
,	O
both	O
of	O
which	O
involve	O
a	O
3	O
-	O
way	O
classification	O
for	O
textual	O
entailment	O
.	O
SICK	B-DatasetName
was	O
created	O
for	O
SemEval	O
2014	O
based	O
on	O
image	O
caption	O
data	O
and	O
video	O
descriptions	O
.	O
The	O
premises	O
and	O
hypotheses	O
are	O
automatically	O
generated	O
from	O
the	O
original	O
captions	O
and	O
so	O
contain	O
some	O
unintentional	O
systematic	O
patterns	O
.	O
Most	O
approaches	O
to	O
SICK	B-DatasetName
involve	O
hand	O
-	O
engineered	O
features	O
or	O
large	O
collections	O
of	O
entailment	O
rules	O
(	O
Beltagy	O
et	O
al	O
,	O
2015	O
)	O
.	O
SNLI	B-DatasetName
is	O
the	O
largest	O
textual	O
entailment	O
dataset	O
by	O
several	O
orders	O
of	O
magnitude	O
.	O
It	O
was	O
created	O
with	O
the	O
goal	O
of	O
training	O
neural	O
network	O
models	O
for	O
textual	O
entailment	O
.	O
The	O
premises	O
in	O
SNLI	B-DatasetName
are	O
captions	O
from	O
the	O
FLICKR30	O
K	O
corpus	O
(	O
Young	O
et	O
al	O
,	O
2014	O
)	O
.	O
The	O
hypotheses	O
(	O
entailed	O
,	O
contradictory	O
,	O
or	O
neutral	O
in	O
relation	O
to	O
the	O
premise	O
)	O
were	O
solicited	O
from	O
workers	O
on	O
Mechanical	O
Turk	O
.	O
Bowman	O
et	O
al	O
(	O
2015	O
)	O
initially	O
illustrated	O
the	O
effectiveness	O
of	O
LSTMs	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
on	O
SNLI	B-DatasetName
,	O
and	O
recent	O
approaches	O
have	O
focused	O
on	O
improvements	O
in	O
neural	O
network	O
architectures	O
.	O
These	O
include	O
sentence	B-TaskName
embedding	I-TaskName
models	O
(	O
Liu	O
et	O
al	O
,	O
2016	O
;	O
Munkhdalai	O
and	O
Yu	O
,	O
2017a	O
)	O
,	O
neural	O
attention	O
models	O
(	O
Rocktäschel	O
et	O
al	O
,	O
2016	O
;	O
Parikh	O
et	O
al	O
,	O
2016	O
)	O
,	O
and	O
neural	O
tree	O
-	O
based	O
models	O
(	O
Munkhdalai	O
and	O
Yu	O
,	O
2017b	O
;	O
Chen	O
et	O
al	O
,	O
2016	O
)	O
.	O
In	O
contrast	O
,	O
in	O
this	O
paper	O
we	O
focus	O
on	O
using	O
a	O
different	O
input	O
representation	O
,	O
and	O
demonstrate	O
its	O
effectiveness	O
when	O
added	O
to	O
a	O
standard	O
neural	O
network	O
model	O
for	O
textual	O
entailment	O
.	O
We	O
demonstrate	O
that	O
the	O
results	O
of	O
the	O
LSTM	B-MethodName
model	O
of	O
Bowman	O
et	O
al	O
(	O
2015	O
)	O
can	O
be	O
improved	O
by	O
adding	O
a	O
single	O
feature	O
based	O
on	O
our	O
predicted	O
denotational	O
probabilities	O
.	O
We	O
expect	O
to	O
see	O
similar	O
improvements	O
when	O
our	O
predicted	O
probabilities	O
are	O
added	O
to	O
more	O
complex	O
neural	O
network	O
entailment	O
models	O
,	O
but	O
we	O
leave	O
those	O
experiments	O
for	O
future	O
work	O
.	O

Several	O
related	O
works	O
have	O
explored	O
different	O
approaches	O
to	O
learning	O
vector	O
space	O
representations	O
that	O
express	O
entailment	O
more	O
directly	O
.	O
Kruszewski	O
et	O
al	O
(	O
2015	O
)	O
learn	O
a	O
mapping	O
from	O
an	O
existing	O
distributional	O
vector	O
representation	O
to	O
a	O
structured	O
Boolean	O
vector	O
representation	O
that	O
expresses	O
entailment	O
as	O
feature	O
inclusion	O
.	O
They	O
evaluate	O
the	O
resulting	O
representation	O
on	O
lexical	B-TaskName
entailment	I-TaskName
tasks	O
and	O
on	O
sentence	O
entailment	O
in	O
SICK	B-DatasetName
,	O
but	O
they	O
re	O
-	O
strict	O
SICK	B-DatasetName
to	O
a	O
binary	O
task	O
and	O
their	O
sentence	O
vectors	O
result	O
from	O
simple	O
composition	O
functions	O
(	O
e.g.	O
addition	O
)	O
over	O
their	O
word	O
representations	O
.	O
Henderson	O
and	O
Popa	O
(	O
2016	O
)	O
learn	O
a	O
mapping	O
from	O
an	O
existing	O
distributional	O
vector	O
representation	O
to	O
an	O
entailment	O
-	O
based	O
vector	O
representation	O
that	O
expresses	O
whether	O
information	O
is	O
known	O
or	O
unknown	O
.	O
However	O
,	O
they	O
only	O
evaluate	O
on	O
lexical	O
semantic	O
tasks	O
such	O
as	O
hyponymy	O
detection	O
.	O
Other	O
approaches	O
explore	O
the	O
idea	O
that	O
it	O
may	O
be	O
more	O
appropriate	O
to	O
represent	O
a	O
word	O
as	O
a	O
region	O
in	O
space	O
instead	O
of	O
a	O
single	O
point	O
.	O
Erk	O
(	O
2009	O
)	O
presents	O
a	O
word	O
vector	O
representation	O
in	O
which	O
the	O
hyponyms	O
of	O
a	O
word	O
are	O
mapped	O
to	O
vectors	O
that	O
exist	O
within	O
the	O
boundaries	O
of	O
that	O
word	O
vector	O
's	O
region	O
.	O
Vilnis	O
and	O
McCallum	O
(	O
2015	O
)	O
use	O
Gaussian	O
functions	O
to	O
map	O
a	O
word	O
to	O
a	O
density	O
over	O
a	O
latent	O
space	O
.	O
Both	O
papers	O
evaluate	O
their	O
models	O
only	O
on	O
lexical	O
relationships	O
.	O

In	O
contrast	O
to	O
traditional	O
distributional	O
similarities	O
,	O
Young	O
et	O
al	O
(	O
2014	O
)	O
introduced	O
the	O
concept	O
of	O
"	O
denotational	O
similarities	O
"	O
to	O
capture	O
which	O
expressions	O
can	O
be	O
used	O
to	O
describe	O
similar	O
situations	O
.	O
Young	O
et	O
al	O
first	O
define	O
the	O
visual	O
denotation	O
of	O
a	O
sentence	O
(	O
or	O
phrase	O
)	O
s	O
,	O
s	O
,	O
as	O
the	O
(	O
sub	O
)	O
set	O
of	O
images	O
that	O
s	O
can	O
describe	O
.	O
They	O
estimate	O
the	O
denotation	O
of	O
a	O
phrase	O
and	O
the	O
resulting	O
similarities	O
from	O
FLICKR30	O
K	O
,	O
a	O
corpus	O
of	O
30	O
,	O
000	O
images	O
,	O
each	O
paired	O
with	O
five	O
descriptive	O
captions	O
.	O
In	O
order	O
to	O
compute	O
visual	O
denotations	O
from	O
the	O
corpus	O
,	O
they	O
define	O
a	O
set	O
of	O
normalization	O
and	O
reduction	O
rules	O
(	O
e.g.	O
lemmatization	B-TaskName
,	O
dropping	O
modifiers	O
,	O
replacing	O
nouns	O
with	O
their	O
hypernyms	O
,	O
dropping	O
PPs	O
,	O
extracting	O
NPs	O
)	O
that	O
augment	O
the	O
original	O
FLICKR30	O
K	O
captions	O
with	O
a	O
large	O
number	O
of	O
shorter	O
,	O
more	O
generic	O
phrases	O
that	O
are	O
each	O
associated	O
with	O
a	O
subset	O
of	O
the	O
FLICKR30	O
K	O
images	O
.	O
The	O
result	O
is	O
a	O
large	O
subsumption	O
hierarchy	O
over	O
phrases	O
,	O
which	O
Young	O
et	O
al	O
call	O
a	O
denotation	O
graph	O
(	O
see	O
Figure	O
1	O
)	O
.	O
The	O
structure	O
of	O
the	O
denotation	O
graph	O
is	O
similar	O
to	O
the	O
idea	O
of	O
an	O
entailment	O
graph	O
(	O
Berant	O
et	O
al	O
,	O
2012	O
)	O
.	O
Each	O
node	O
in	O
the	O
denotation	O
graph	O
corresponds	O
to	O
a	O
phrase	O
s	O
,	O
associated	O
with	O
its	O
denotation	O
s	O
,	O
i.e.	O
the	O
set	O
of	O
images	O
that	O
correspond	O
to	O
the	O
original	O
captions	O
from	O
which	O
this	O
phrase	O
could	O
be	O
derived	O
.	O
For	O
example	O
,	O
the	O
denotation	O
of	O
a	O
phrase	O
"	O
woman	O
jog	O
on	O
beach	O
"	O
is	O
the	O
set	O
of	O
images	O
in	O
the	O
corpus	O
that	O
depict	O
a	O
woman	O
jogging	O
on	O
a	O
beach	O
.	O
Note	O
that	O
the	O
deno	O
-	O
tation	O
of	O
a	O
node	O
(	O
e.g.	O
"	O
woman	O
jog	O
on	O
beach	O
"	O
)	O
is	O
always	O
a	O
subset	O
of	O
the	O
denotations	O
of	O
any	O
of	O
its	O
ancestors	O
(	O
e.g.	O
"	O
woman	O
jog	O
"	O
,	O
"	O
person	O
jog	O
"	O
,	O
"	O
jog	O
on	O
beach	O
"	O
,	O
or	O
"	O
beach	O
"	O
)	O
.	O
The	O
denotational	O
probability	O
of	O
a	O
phrase	O
s	O
,	O
P	O
(	O
s	O
)	O
,	O
is	O
a	O
Bernoulli	O
random	O
variable	O
that	O
corresponds	O
to	O
the	O
probability	O
that	O
a	O
randomly	O
drawn	O
image	O
can	O
be	O
described	O
by	O
s.	O
Given	O
a	O
denotation	O
graph	O
over	O
N	O
images	O
,	O
P	O
(	O
s	O
)	O
=	O
|	O
s	O
|	O
N	O
.	O
The	O
joint	O
denotational	O
probability	O
of	O
two	O
phrases	O
x	O
and	O
y	O
,	O
P	O
(	O
x	O
,	O
y	O
)	O
=	O
|	O
x	O
∩	O
y	O
|	O
N	O
,	O
indicates	O
how	O
likely	O
it	O
is	O
that	O
a	O
situation	O
can	O
be	O
described	O
by	O
both	O
x	O
and	O
y.	O
Young	O
et	O
al	O
propose	O
to	O
use	O
pointwise	O
mutual	O
information	O
scores	O
(	O
akin	O
to	O
traditional	O
distributional	O
similarities	O
)	O
and	O
conditional	O
probabilities	O
P	O
(	O
x	O
|	O
y	O
)	O
=	O
|	O
x	O
∩	O
y	O
|	O
|	O
y	O
|	O
as	O
so	O
-	O
called	O
denotational	O
similarities	O
.	O
In	O
this	O
paper	O
,	O
we	O
will	O
work	O
with	O
denotational	O
conditional	O
probabilities	O
,	O
as	O
they	O
are	O
intended	O
to	O
capture	O
entailment	O
-	O
like	O
relations	O
that	O
hold	O
due	O
to	O
commonsense	O
knowledge	O
,	O
hyponymy	O
,	O
etc	O
.	O
(	O
what	O
is	O
the	O
probability	O
that	O
x	O
is	O
true	O
,	O
given	O
that	O
y	O
can	O
be	O
said	O
about	O
this	O
situation	O
?	O
)	O
.	O
In	O
an	O
ideal	O
scenario	O
,	O
if	O
the	O
premise	O
p	O
entails	O
the	O
hypothesis	O
h	O
,	O
then	O
the	O
conditional	O
probability	O
P	O
(	O
h	O
|	O
p	O
)	O
is	O
1	O
(	O
or	O
close	O
to	O
1	O
)	O
.	O
Conversely	O
,	O
if	O
h	O
contradicts	O
p	O
,	O
then	O
the	O
conditional	O
probability	O
P	O
(	O
h	O
|	O
p	O
)	O
is	O
close	O
to	O
0	B-DatasetName
.	O
We	O
therefore	O
stipulate	O
that	O
learning	O
to	O
predict	O
the	O
conditional	O
probability	O
of	O
one	O
phrase	O
h	O
given	O
another	O
phrase	O
p	O
would	O
be	O
helpful	O
in	O
predicting	O
textual	O
entailment	O
.	O
We	O
also	O
note	O
that	O
by	O
the	O
definition	O
of	O
the	O
denotation	O
graph	O
,	O
if	O
x	O
is	O
an	O
ancestor	O
of	O
y	O
in	O
the	O
graph	O
,	O
then	O
y	O
entails	O
x	O
and	O
P	O
(	O
x	O
|	O
y	O
)	O
=	O
1	O
.	O
Young	O
et	O
al	O
(	O
2014	O
)	O
and	O
show	O
that	O
denotational	O
probabilities	O
can	O
be	O
at	O
least	O
as	O
useful	O
as	O
traditional	O
distributional	O
similarities	O
for	O
tasks	O
that	O
require	O
semantic	O
inference	O
such	O
as	O
entailment	O
or	O
textual	O
similarity	O
recognition	O
.	O
However	O
,	O
their	O
systems	O
can	O
only	O
use	O
deno	O
-	O
o	O
x1	O
x2	O
y	O
x	O
P	O
(	O
X	O
,	O
Y	O
)	O
P	O
(	O
X	O
)	O
o	O
x1	O
x2	O
y	O
z	O
x	O
P	O
(	O
X	O
,	O
Y	O
)	O
P	O
(	O
Y	O
)	O
P	O
(	O
X	O
)	O
Figure	O
2	O
:	O
An	O
embedding	O
space	O
that	O
expresses	O
the	O
individual	O
probability	O
of	O
events	O
X	O
and	O
Y	O
and	O
the	O
joint	O
probability	O
P	O
(	O
X	O
,	O
Y	O
)	O
.	O
tational	O
probabilities	O
between	O
phrases	O
that	O
already	O
exist	O
in	O
the	O
denotation	O
graph	O
(	O
i.e.	O
phrases	O
that	O
can	O
be	O
derived	O
from	O
the	O
original	O
FLICKR30	O
K	O
captions	O
)	O
.	O
Here	O
,	O
we	O
present	O
a	O
model	O
that	O
learns	O
to	O
predict	O
denotational	O
probabilities	O
P	O
(	O
x	O
)	O
and	O
P	O
(	O
x	O
|	O
y	O
)	O
even	O
for	O
phrases	O
it	O
has	O
not	O
seen	O
during	O
training	O
.	O
Our	O
model	O
is	O
inspired	O
by	O
Vendrov	O
et	O
al	O
(	O
2016	O
)	O
,	O
who	O
observed	O
that	O
a	O
partial	O
ordering	O
over	O
the	O
vector	O
representations	O
of	O
phrases	O
can	O
be	O
used	O
to	O
express	O
an	O
entailment	O
relationship	O
.	O
They	O
induce	O
a	O
so	O
-	O
called	O
order	O
embedding	O
for	O
words	O
and	O
phrases	O
such	O
that	O
the	O
vector	O
x	O
corresponding	O
to	O
phrase	O
x	O
is	O
smaller	O
than	O
the	O
vector	O
y	O
,	O
i.e.	O
x	O
y	O
,	O
for	O
phrases	O
y	O
that	O
are	O
entailed	O
by	O
x	O
,	O
where	O
corresponds	O
to	O
the	O
reversed	O
product	O
order	O
on	O
R	O
N	O
+	O
(	O
x	O
y	O
⇔	O
x	O
i	O
≥	O
y	O
i	O
∀i	O
)	O
.	O
They	O
use	O
their	O
model	O
to	O
predict	O
entailment	O
labels	O
between	O
pairs	O
of	O
sentences	O
,	O
but	O
it	O
is	O
only	O
capable	O
of	O
making	O
a	O
binary	O
entailment	O
decision	O
.	O

We	O
generalize	O
this	O
idea	O
to	O
learn	O
an	O
embedding	O
space	O
that	O
expresses	O
not	O
only	O
the	O
binary	O
relation	O
that	O
phrase	O
x	O
is	O
entailed	O
by	O
phrase	O
y	O
,	O
but	O
also	O
the	O
probability	O
that	O
phrase	O
x	O
is	O
true	O
given	O
phrase	O
y.	O
Specifically	O
,	O
we	O
learn	O
a	O
mapping	O
from	O
a	O
phrase	O
x	O
to	O
an	O
N	O
-	O
dimensional	O
vector	O
x	O
R	O
N	O
+	O
such	O
that	O
the	O
vector	O
x	O
=	O
(	O
x	O
1	O
,	O
...	O
,	O
x	O
N	O
)	O
defines	O
the	O
denotational	O
probability	O
of	O
x	O
as	O
P	O
(	O
x	O
)	O
=	O
exp	O
(	O
−	O
i	O
x	O
i	O
)	O
.	O
The	O
origin	O
(	O
the	O
zero	O
vector	O
)	O
therefore	O
has	O
probability	O
exp	O
(	O
0	B-DatasetName
)	O
=	O
1	O
.	O
Any	O
other	O
vector	O
x	O
that	O
does	O
not	O
lie	O
on	O
the	O
origin	O
(	O
i.e.	O
i	O
x	O
i	O
>	O
0	B-DatasetName
)	O
has	O
probability	O
less	O
than	O
1	O
,	O
and	O
a	O
vector	O
x	O
that	O
is	O
farther	O
from	O
the	O
origin	O
than	O
a	O
vector	O
y	O
represents	O
a	O
phrase	O
x	O
that	O
has	O
a	O
smaller	O
denotational	O
probability	O
than	O
phrase	O
y.	O
We	O
can	O
visualize	O
this	O
as	O
each	O
phrase	O
vector	O
occupying	O
a	O
region	O
in	O
the	O
embedding	O
space	O
that	O
is	O
proportional	O
to	O
the	O
denotational	O
probability	O
of	O
the	O
phrase	O
.	O
Figure	O
2	O
illustrates	O
this	O
in	O
two	O
dimensions	O
.	O
The	O
zero	O
vector	O
at	O
the	O
origin	O
has	O
a	O
probability	O
pro	O
-	O
portional	O
to	O
the	O
entire	O
region	O
of	O
the	O
positive	O
orthant	O
,	O
while	O
other	O
points	O
in	O
the	O
space	O
correspond	O
to	O
smaller	O
regions	O
and	O
thus	O
probabilities	O
less	O
than	O
1	O
.	O
The	O
joint	O
probability	O
P	O
(	O
x	O
,	O
y	O
)	O
in	O
this	O
embedding	O
space	O
should	O
be	O
proportional	O
to	O
the	O
size	O
of	O
the	O
intersection	O
of	O
the	O
regions	O
of	O
x	O
and	O
y.	O
Therefore	O
,	O
we	O
define	O
the	O
joint	O
probability	O
of	O
two	O
phrases	O
x	O
and	O
y	O
to	O
correspond	O
to	O
the	O
vector	O
z	O
that	O
is	O
the	O
element	O
-	O
wise	O
maximum	O
of	O
x	O
and	O
y	O
:	O
z	O
i	O
=	O
max	O
(	O
x	O
i	O
,	O
y	O
i	O
)	O
.	O
This	O
allows	O
us	O
to	O
compute	O
the	O
conditional	O
probability	O
P	O
(	O
x	O
|	O
y	O
)	O
as	O
follows	O
:	O
P	O
(	O
x	O
|	O
y	O
)	O
=	O
P	O
(	O
x	O
,	O
y	O
)	O
P	O
(	O
y	O
)	O
=	O
exp	O
(	O
−	O
i	O
z	O
i	O
)	O
exp	O
(	O
−	O
i	O
y	O
i	O
)	O
=	O
exp	O
(	O
i	O
y	O
i	O
−	O
i	O
z	O
i	O
)	O
Shortcomings	O
We	O
note	O
that	O
this	O
embedding	O
does	O
not	O
allow	O
us	O
to	O
represent	O
the	O
negation	O
of	O
x	O
as	O
a	O
vector	O
.	O
We	O
also	O
can	O
not	O
represent	O
two	O
phrases	O
that	O
have	O
completely	O
disjoint	O
denotations	O
:	O
in	O
Figure	O
2	O
,	O
the	O
P	O
(	O
X	O
)	O
and	O
P	O
(	O
Y	O
)	O
regions	O
will	O
always	O
intersect	O
and	O
therefore	O
the	O
P	O
(	O
X	O
,	O
Y	O
)	O
region	O
will	O
always	O
have	O
an	O
area	O
greater	O
than	O
0	B-DatasetName
.	O
In	O
fact	O
,	O
in	O
our	O
embedding	O
space	O
,	O
the	O
joint	O
probability	O
represented	O
by	O
the	O
vector	O
z	O
will	O
always	O
be	O
greater	O
than	O
or	O
equal	O
to	O
the	O
product	O
of	O
the	O
probabilities	O
represented	O
by	O
the	O
vectors	O
x	O
and	O
y.	O
For	O
any	O
pair	O
x	O
=	O
(	O
x	O
1	O
,	O
...	O
,	O
x	O
N	O
)	O
and	O
y	O
=	O
(	O
y	O
1	O
,	O
...	O
,	O
y	O
N	O
)	O
,	O
P	O
(	O
X	O
,	O
Y	O
)	O
≥	O
P	O
(	O
X	O
)	O
P	O
(	O
Y	O
)	O
:	O
P	O
(	O
X	O
,	O
Y	O
)	O
=	O
exp	O
−	O
i	O
max	O
(	O
x	O
i	O
,	O
y	O
i	O
)	O
≥	O
exp	O
−	O
i	O
x	O
i	O
−	O
i	O
y	O
i	O
=	O
P	O
(	O
X	O
)	O
P	O
(	O
Y	O
)	O
(	O
Equality	O
holds	O
when	O
x	O
and	O
y	O
are	O
orthogonal	O
,	O
and	O
thus	O
i	O
x	O
i	O
+	O
i	O
y	O
i	O
=	O
i	O
max	O
(	O
x	O
i	O
,	O
y	O
i	O
)	O
)	O
.	O
Therefore	O
,	O
the	O
best	O
we	O
can	O
do	O
for	O
disjoint	O
phrases	O
is	O
learn	O
an	O
embedding	O
that	O
assumes	O
the	O
phrases	O
are	O
independent	O
.	O
In	O
other	O
words	O
,	O
we	O
can	O
map	O
the	O
disjoint	O
phrases	O
to	O
two	O
vectors	O
whose	O
computed	O
joint	O
probability	O
is	O
the	O
product	O
of	O
the	O
individual	O
phrase	O
probabilities	O
.	O
Although	O
our	O
model	O
can	O
not	O
represent	O
two	O
events	O
with	O
completely	O
disjoint	O
denotations	O
,	O
we	O
will	O
see	O
below	O
that	O
it	O
is	O
able	O
to	O
learn	O
that	O
some	O
phrase	O
pairs	O
have	O
very	O
low	O
denotational	O
conditional	O
probabilities	O
.	O
We	O
note	O
also	O
that	O
our	O
model	O
…	O
…	O
w	O
0	B-DatasetName
w	O
i	O
w	O
i+1	O
w	O
N	O
Glove	B-MethodName
embeddings	I-MethodName
LSTM	B-MethodName
RNN	O
512D	O
FF	O
p	O
(	O
x	O
)	O
LSTM	B-MethodName
is	O
run	O
once	O
per	O
phrase	O
x	O
,	O
y	O
p	O
(	O
y	O
)	O

Our	O
model	O
up	O
to	O
this	O
point	O
has	O
only	O
been	O
trained	O
on	O
short	O
phrases	O
,	O
since	O
conditional	O
probabilities	O
in	O
the	O
denotation	O
graph	O
are	O
only	O
reliable	O
for	O
phrases	O
that	O
occur	O
with	O
multiple	O
images	O
(	O
see	O
Figure	O
5	O
for	O
the	O
distribution	O
of	O
phrase	O
lengths	O
in	O
the	O
training	O
data	O
)	O
.	O
To	O
improve	O
our	O
model	O
's	O
performance	O
on	O
longer	O
sentences	O
,	O
we	O
add	O
the	O
SNLI	B-DatasetName
training	O
data	O
(	O
which	O
has	O
a	O
mean	O
sentence	O
length	O
of	O
11	O
words	O
)	O
to	O
our	O
training	O
data	O
.	O
We	O
train	O
a	O
new	O
model	O
from	O
scratch	O
on	O
a	O
corpus	O
consisting	O
of	O
the	O
previously	O
described	O
42	O
million	O
phrase	O
pairs	O
and	O
the	O
550	O
,	O
000	O
SNLI	B-DatasetName
training	O
sentence	O
pairs	O
(	O
lemmatized	O
to	O
match	O
our	O
phrase	O
pairs	O
)	O
.	O
We	O
do	O
not	O
train	O
on	O
SICK	B-DatasetName
because	O
the	O
corpus	O
is	O
much	O
smaller	O
and	O
has	O
a	O
different	O
distribution	O
of	O
phenomena	O
,	O
including	O
explicit	O
negation	O
.	O
We	O
augment	O
the	O
SNLI	B-DatasetName
data	O
with	O
approximate	O
gold	O
denotational	O
probabilities	O
by	O
assigning	O
a	O
probability	O
P	O
(	O
S	O
)	O
=	O
s	O
/	O
N	O
to	O
a	O
sentence	O
S	O
that	O
occurs	O
s	O
times	O
in	O
the	O
N	O
training	O
sentences	O
.	O
We	O
assign	O
approximate	O
gold	O
conditional	O
probabilities	O
for	O
each	O
sentence	O
pair	O
p	O
,	O
h	O
according	O
to	O
the	O
entailment	O
label	O
:	O
if	O
p	O
entails	O
h	O
,	O
then	O
P	O
(	O
h	O
|	O
p	O
)	O
=	O
0.9	O
.	O
If	O
p	O
contradicts	O
h	O
,	O
then	O
P	O
(	O
h	O
|	O
p	O
)	O
=	O
0.001	O
.	O
Otherwise	O
,	O
P	O
(	O
h	O
|	O
p	O
)	O
=	O
0.5	O
.	O
Figure	O
6	O
shows	O
the	O
predicted	O
probabilities	O
on	O
the	O
SNLI	B-DatasetName
test	O
data	O
when	O
our	O
model	O
is	O
trained	O
on	O
different	O
distributions	O
of	O
data	O
.	O
The	O
top	O
row	O
shows	O
the	O
predictions	O
of	O
our	O
model	O
when	O
trained	O
only	O
on	O
short	O
phrases	O
from	O
the	O
denotation	O
graph	O
.	O
We	O
observe	O
that	O
the	O
median	O
probabilities	O
increase	O
from	O
contradiction	O
to	O
neutral	O
to	O
entailment	O
,	O
even	O
though	O
this	O
model	O
was	O
only	O
trained	O
on	O
short	O
phrases	O
with	O
a	O
limited	O
vocabulary	O
.	O
Given	O
the	O
training	O
data	O
,	O
we	O
did	O
not	O
expect	O
these	O
probabilities	O
to	O
align	O
cleanly	O
with	O
the	O
entailment	O
labels	O
,	O
but	O
even	O
so	O
,	O
there	O
is	O
already	O
some	O
information	O
here	O
to	O
distinguish	O
between	O
entailment	O
classes	O
.	O
The	O
bottom	O
row	O
shows	O
that	O
when	O
our	O
model	O
is	O
trained	O
on	O
both	O
denotational	O
phrases	O
and	O
SNLI	B-DatasetName
sentence	O
pairs	O
with	O
approximate	O
conditional	O
probabilities	O
,	O
its	O
probability	O
predictions	O
for	O
longer	O
sentences	O
improve	O
.	O
This	O
model	O
's	O
predicted	O
conditional	O
probabilities	O
align	O
much	O
more	O
closely	O
with	O
the	O
entailment	O
class	O
labels	O
.	O
Entailing	O
sentence	O
pairs	O
have	O
high	O
conditional	O
probabilities	O
(	O
median	O
0.72	O
)	O
,	O
neutral	O
sentence	O
pairs	O
have	O
mid	O
-	O
range	O
conditional	O
probabilities	O
(	O
median	O
0.46	O
)	O
,	O
and	O
contradictory	O
sentence	O
pairs	O
have	O
conditional	O
probabilities	O
approaching	O
0	B-DatasetName
(	O
median	O
0.19	O
)	O
.	O

Neutral	O
6	O
A	O
group	O
of	O
young	O
people	O
with	O
instruments	O
are	O
on	O
stage	O
.	O
People	O
are	O
playing	O
music	O
.	O
0.86	O
7	O
Two	O
doctors	O
perform	O
surgery	O
on	O
patient	O
.	O
Two	O
doctors	O
are	O
performing	O
surgery	O
on	O
a	O
man	O
.	O
0.56	O
8	O
Two	O
young	O
boys	O
of	O
opposing	O
teams	O
play	O
football	O
,	O
while	O
wearing	O
full	O
protection	O
uniforms	O
and	O
helmets	O
.	O
Boys	O
scoring	O
a	O
touchdown	O
.	O
0.30	O
9	O
Two	O
men	O
on	O
bicycles	O
competing	O
in	O
a	O
race	O
.	O
Men	O
are	O
riding	O
bicycles	O
on	O
the	O
street	O
.	O
0.24	O
Contradiction	O
10	O
Two	O
women	O
having	O
drinks	O
and	O
smoking	O
cigarettes	O
at	O
the	O
bar	O
.	O
Three	O
women	O
are	O
at	O
a	O
bar	O
.	O
0.79	O
11	O
A	O
man	O
in	O
a	O
black	O
shirt	O
is	O
playing	O
a	O
guitar	O
.	O
The	O
man	O
is	O
wearing	O
a	O
blue	O
shirt	O
.	O
0.47	O
12	O
An	O
Asian	O
woman	O
sitting	O
outside	O
an	O
outdoor	O
market	O
stall	O
.	O
A	O
woman	O
sitting	O
in	O
an	O
indoor	O
market	O
.	O
0.22	O
13	O
A	O
white	O
dog	O
with	O
long	O
hair	O
jumps	O
to	O
catch	O
a	O
red	O
and	O
green	O
toy	O
.	O
A	O
white	O
dog	O
with	O
long	O
hair	O
is	O
swimming	O
underwater	O
.	O
0.09	O
14	O
Two	O
women	O
are	O
embracing	O
while	O
holding	O
to	O
go	O
packages	O
.	O
The	O
men	O
are	O
fighting	O
outside	O
a	O
deli	O
.	O
0.06	O
In	O
examples	O
10	O
and	O
11	O
,	O
our	O
model	O
predicts	O
low	O
probabilities	O
for	O
occasionally	O
co	O
-	O
occurring	O
events	O
,	O
which	O
are	O
still	O
more	O
likely	O
than	O
the	O
improbable	O
cooccurrence	O
in	O
example	O
13	O
.	O
Table	O
5	O
demonstrates	O
similar	O
patterns	O
for	O
pairs	O
where	O
both	O
phrases	O
were	O
unseen	O
.	O
Table	O
6	O
has	O
examples	O
of	O
predicted	O
conditional	O
probabilities	O
for	O
sentence	O
pairs	O
from	O
the	O
SNLI	B-DatasetName
development	O
data	O
.	O
Some	O
cases	O
of	O
entailment	O
are	O
straightforward	O
,	O
so	O
predicting	O
high	O
conditional	O
probability	O
is	O
relatively	O
easy	O
.	O
This	O
is	O
the	O
case	O
with	O
example	O
2	O
,	O
which	O
simply	O
involves	O
dropping	O
words	O
from	O
the	O
premise	O
to	O
reach	O
the	O
hypothesis	O
.	O
In	O
other	O
cases	O
,	O
our	O
model	O
correctly	O
predicts	O
high	O
conditional	O
probability	O
for	O
an	O
entailed	O
hypothesis	O
that	O
does	O
not	O
have	O
such	O
obvious	O
word	O
-	O
to	O
-	O
word	O
correspondence	O
with	O
the	O
premise	O
,	O
such	O
as	O
example	O
1	O
.	O
Our	O
model	O
's	O
predictions	O
are	O
less	O
accurate	O
when	O
the	O
sentence	O
structure	O
differs	O
substantially	O
between	O
premise	O
and	O
hypothesis	O
,	O
or	O
when	O
there	O
are	O
many	O
unknown	O
words	O
,	O
as	O
in	O
example	O
5	O
.	O
For	O
neutral	O
pairs	O
,	O
our	O
model	O
usually	O
predicts	O
midrange	O
probabilities	O
,	O
but	O
there	O
are	O
some	O
exceptions	O
.	O
In	O
example	O
6	O
,	O
it	O
is	O
not	O
certain	O
that	O
the	O
people	O
are	O
playing	O
music	O
,	O
but	O
it	O
is	O
a	O
reasonable	O
assumption	O
from	O
the	O
premise	O
.	O
It	O
makes	O
sense	O
that	O
in	O
this	O
case	O
,	O
our	O
model	O
assigns	O
this	O
hypothesis	O
a	O
higher	O
conditional	O
probability	O
given	O
the	O
premise	O
than	O
for	O
most	O
neutral	O
sentence	O
pairs	O
.	O
In	O
example	O
7	O
,	O
we	O
might	O
guess	O
that	O
the	O
patient	O
is	O
a	O
man	O
with	O
50	O
%	O
probability	O
,	O
so	O
the	O
predicted	O
conditional	O
probability	O
of	O
our	O
model	O
seems	O
reasonable	O
.	O
Our	O
model	O
can	O
not	O
reason	O
about	O
numbers	O
and	O
quantities	O
,	O
as	O
example	O
10	O
shows	O
.	O
It	O
also	O
fails	O
to	O
predict	O
in	O
example	O
11	O
that	O
a	O
man	O
wearing	O
a	O
black	O
shirt	O
is	O
probably	O
not	O
wearing	O
a	O
blue	O
shirt	O
as	O
well	O
.	O
However	O
,	O
our	O
model	O
does	O
correctly	O
predict	O
low	O
probabilities	O
for	O
some	O
contradictory	O
examples	O
that	O
have	O
reasonably	O
high	O
word	O
overlap	O
,	O
as	O
in	O
example	O
13	O
.	O
Finally	O
,	O
example	O
14	O
shows	O
that	O
our	O
model	O
can	O
correctly	O
predict	O
very	O
low	O
conditional	O
probability	O
for	O
sentences	O
that	O
share	O
no	O
common	O
subject	O
matter	O
.	O

This	O
work	O
was	O
supported	O
by	O
NSF	O
Grants	O
1563727	O
,	O
1405883	O
,	O
and	O
1053856	O
,	O
and	O
by	O
a	O
Google	B-DatasetName
Research	O
Award	O
.	O
Additional	O
thanks	O
to	O
Yonatan	O
Bisk	O
and	O
Pooya	O
Khorrami	O
.	O

Research	O
areas	O
related	O
to	O
this	O
work	O
include	O
integrated	O
logical	O
KRR	O
and	O
RL	O
,	O
relational	O
RL	O
,	O
and	O
integrated	O
KRR	O
and	O
probabilistic	O
planning	O
.	O
Logical	O
KRR	O
has	O
previously	O
been	O
integrated	O
with	O
RL	O
.	O
Action	O
knowledge	O
(	O
McDermott	O
et	O
al	O
,	O
1998	O
;	O
Jiang	O
et	O
al	O
,	O
2019	O
)	O
has	O
been	O
used	O
to	O
reason	O
about	O
action	O
sequences	O
and	O
help	O
an	O
RL	O
agent	B-DatasetName
explore	O
only	O
the	O
states	O
that	O
can	O
potentially	O
contribute	O
to	O
achieving	O
the	O
ultimate	O
goal	O
(	O
Leonetti	O
et	O
al	O
,	O
2016	O
)	O
.	O
As	O
a	O
result	O
,	O
their	O
agents	O
learn	O
faster	O
by	O
avoiding	O
choosing	O
"	O
unreasonable	O
"	O
actions	O
.	O
A	O
similar	O
idea	O
has	O
been	O
applied	O
to	O
domains	O
with	O
nonstationary	O
dynamics	O
(	O
Ferreira	O
et	O
al	O
,	O
2017	O
)	O
.	O
More	O
recently	O
,	O
task	O
planning	O
was	O
used	O
to	O
interact	O
with	O
the	O
high	O
level	O
of	O
a	O
hierarchical	O
RL	O
framework	O
(	O
Yang	O
et	O
al	O
,	O
2018	O
)	O
.	O
The	O
goal	O
shared	O
by	O
these	O
works	O
is	O
to	O
enable	O
RL	O
agents	O
to	O
use	O
knowledge	O
to	O
improve	O
the	O
performance	O
in	O
learning	O
(	O
e.g.	O
,	O
to	O
learn	O
faster	O
and/or	O
avoid	O
risky	O
exploration	O
)	O
.	O
However	O
,	O
the	O
KRR	O
capabilities	O
of	O
these	O
methods	O
are	O
limited	O
to	O
logical	O
action	O
knowledge	O
.	O
By	O
contrast	O
,	O
we	O
use	O
a	O
logicalprobabilistic	O
KRR	O
paradigm	O
that	O
can	O
directly	O
reason	O
with	O
probabilities	O
learned	O
from	O
RL	O
.	O
Relational	O
RL	O
(	O
RRL	O
)	O
combines	O
RL	O
with	O
relational	B-TaskName
reasoning	I-TaskName
(	O
Džeroski	O
et	O
al	O
,	O
2001	O
)	O
.	O
Action	O
models	O
have	O
been	O
incorporated	O
into	O
RRL	O
,	O
resulting	O
in	O
a	O
relational	O
temporal	O
difference	O
learning	O
method	O
(	O
Asgharbeygi	O
et	O
al	O
,	O
2006	O
)	O
.	O
Recently	O
,	O
RRL	O
has	O
been	O
deployed	O
for	O
learning	O
affordance	O
relations	O
that	O
forbid	O
the	O
execution	O
of	O
specific	O
actions	O
(	O
Sridharan	O
et	O
al	O
,	O
2017	O
)	O
.	O
These	O
RRL	O
methods	O
,	O
including	O
deep	O
RRL	O
(	O
Zambaldi	O
et	O
al	O
,	O
2018	O
)	O
,	O
exploit	O
structural	O
representations	O
over	O
states	O
and	O
actions	O
in	O
(	O
only	O
)	O
current	O
tasks	O
.	O
In	O
this	O
research	O
,	O
KRR	O
-	O
RL	O
supports	O
the	O
KRR	O
of	O
world	O
factors	O
beyond	O
those	O
in	O
state	O
and	O
action	O
representations	O
,	O
e.g.	O
,	O
time	O
in	O
navigation	O
tasks	O
,	O
as	O
detailed	O
in	O
Section	O
4.2	O
.	O
The	O
research	O
area	O
of	O
integrated	O
KRR	O
and	O
probabilistic	O
planning	O
is	O
related	O
to	O
this	O
research	O
.	O
Logicalprobabilistic	O
reasoning	O
has	O
been	O
used	O
to	O
compute	O
informative	O
priors	O
and	O
world	O
dynamics	O
Amiri	O
et	O
al	O
,	O
2020	O
)	O
for	O
probabilistic	O
planning	O
.	O
An	O
action	O
language	O
was	O
used	O
to	O
compute	O
a	O
deterministic	O
sequence	O
of	O
actions	O
for	O
robots	O
,	O
where	O
individual	O
actions	O
are	O
then	O
implemented	O
using	O
probabilistic	O
controllers	O
(	O
Sridharan	O
et	O
al	O
,	O
2019	O
)	O
.	O
Recently	O
,	O
human	O
-	O
provided	O
information	O
has	O
been	O
incorporated	O
into	O
belief	O
state	O
representations	O
to	O
guide	O
robot	O
action	O
selection	O
(	O
Chitnis	O
et	O
al	O
,	O
2018	O
)	O
.	O
In	O
comparison	O
to	O
our	O
approach	O
,	O
learning	O
(	O
from	O
reinforcement	O
or	O
not	O
)	O
was	O
not	O
discussed	O
in	O
the	O
abovementioned	O
algorithms	O
.	O
Finally	O
,	O
there	O
are	O
a	O
number	O
of	O
robot	O
reasoning	O
and	O
learning	O
architectures	O
(	O
Tenorth	O
and	O
Beetz	O
,	O
2013	O
;	O
Oh	O
et	O
al	O
,	O
2015	O
;	O
Hanheide	O
et	O
al	O
,	O
2017	O
;	O
,	O
which	O
are	O
relatively	O
complex	O
,	O
and	O
support	O
a	O
variety	O
of	O
functionalities	O
.	O
In	O
comparison	O
,	O
we	O
aim	O
at	O
a	O
concise	O
representation	O
for	O
robot	O
KRR	O
and	O
RL	O
capabilities	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
work	O
on	O
a	O
tightly	O
coupled	O
integration	O
of	O
logical	O
-	O
probabilistic	O
KRR	O
with	O
model	O
-	O
based	O
RL	O
.	O

Following	O
the	O
Markov	O
assumption	O
,	O
a	O
Markov	O
decision	O
process	O
(	O
MDP	O
)	O
can	O
be	O
described	O
as	O
a	O
fourtuple	O
S	O
,	O
A	O
,	O
T	O
,	O
R	O
(	O
Puterman	O
,	O
1994	O
)	O
.	O
S	O
defines	O
the	O
state	O
set	O
,	O
where	O
we	O
assume	O
a	O
factored	O
space	O
in	O
this	O
work	O
.	O
A	O
is	O
the	O
action	O
set	O
.	O
T	O
:	O
S	O
×	O
A	O
×	O
S	O
[	O
0	B-DatasetName
,	O
1	O
]	O
specifies	O
the	O
state	O
transition	O
probabilities	O
.	O
R	O
:	O
S	O
×	O
A	O
R	O
specifies	O
the	O
rewards	O
.	O
Solving	O
an	O
MDP	O
produces	O
an	O
action	O
policy	O
π	O
:	O
s	O
a	O
that	O
maps	O
a	O
state	O
to	O
an	O
action	O
to	O
maximize	O
long	O
-	O
term	O
rewards	O
.	O
RL	O
methods	O
fall	O
into	O
classes	O
including	O
modelbased	O
and	O
model	O
-	O
free	O
.	O
Model	O
-	O
based	O
RL	O
methods	O
learn	O
a	O
model	O
of	O
the	O
domain	O
by	O
approximating	O
R	O
(	O
s	O
,	O
a	O
)	O
and	O
P	O
(	O
s	O
|	O
s	O
,	O
a	O
)	O
for	O
state	O
-	O
action	O
pairs	O
,	O
where	O
P	O
represents	O
the	O
probabilistic	O
transition	O
system	O
.	O
An	O
agent	B-DatasetName
can	O
then	O
use	O
planning	O
methods	O
to	O
calculate	O
an	O
action	O
policy	O
(	O
Sutton	O
,	O
1990	O
;	O
Kocsis	O
and	O
Szepesvári	O
,	O
2006	O
)	O
.	O
Model	O
-	O
based	O
methods	O
are	O
particularly	O
attractive	O
in	O
this	O
work	O
,	O
because	O
they	O
output	O
partial	O
world	O
models	O
that	O
can	O
better	O
accommodate	O
the	O
diversity	O
of	O
tasks	O
we	O
are	O
concerned	O
with	O
,	O
c.f	O
.	O
,	O
modelfree	O
RL	O
that	O
is	O
typically	O
goal	O
-	O
directed	O
.	O
One	O
of	O
the	O
best	O
known	O
examples	O
of	O
model	O
-	O
based	O
RL	O
is	O
R	O
-	O
Max	O
(	O
Brafman	O
and	O
Tennenholtz	O
,	O
2002	O
)	O
,	O
which	O
is	O
guaranteed	O
to	O
learn	O
a	O
near	O
-	O
optimal	O
policy	O
with	O
a	O
polynomial	O
number	O
of	O
suboptimal	O
(	O
exploratory	O
)	O
actions	O
.	O
The	O
algorithm	O
classifies	O
each	O
state	O
-	O
action	O
pair	O
as	O
known	O
or	O
unknown	O
,	O
according	O
to	O
the	O
number	O
of	O
times	O
it	O
was	O
visited	O
.	O
When	O
planning	O
on	O
the	O
model	O
,	O
known	O
state	O
-	O
actions	O
are	O
modeled	O
with	O
the	O
learned	O
reward	O
,	O
while	O
unknown	O
stateactions	O
are	O
given	O
the	O
maximum	O
one	O
-	O
step	O
reward	O
,	O
R	O
max	O
.	O
This	O
"	O
maximum	O
-	O
reward	O
"	O
strategy	O
automatically	O
enables	O
the	O
agent	B-DatasetName
to	O
balance	O
the	O
exploration	O
of	O
unknown	O
states	O
and	O
exploitation	O
.	O
We	O
use	O
R	O
-	O
Max	O
in	O
this	O
work	O
,	O
though	O
KRR	O
-	O
RL	O
practitioners	O
can	O
use	O
supervised	O
machine	O
learning	O
methods	O
,	O
e.g.	O
,	O
imitation	B-TaskName
learning	I-TaskName
(	O
Osa	O
et	O
al	O
,	O
2018	O
)	O
,	O
to	O
build	O
the	O
model	O
learning	O
component	O
.	O

KRR	O
paradigms	O
are	O
concerned	O
with	O
concisely	O
representing	O
and	O
robustly	O
reasoning	O
with	O
declarative	O
knowledge	O
.	O
Answer	O
set	O
programming	O
(	O
ASP	O
)	O
is	O
a	O
non	O
-	O
monotonic	O
logical	O
KRR	O
paradigm	O
(	O
Baral	O
,	O
2010	O
;	O
Gelfond	O
and	O
Kahl	O
,	O
2014	O
)	O
building	O
on	O
the	O
stable	O
model	O
semantics	O
(	O
Gelfond	O
and	O
Lifschitz	O
,	O
1988	O
)	O
.	O
An	O
ASP	O
program	O
consists	O
of	O
a	O
set	O
of	O
logical	O
rules	O
,	O
in	O
the	O
form	O
of	O
"	O
head	O
:	O
-	O
body	O
"	O
,	O
that	O
read	O
"	O
head	O
is	O
true	O
if	O
body	O
is	O
true	O
"	O
.	O
Each	O
ASP	O
rule	O
is	O
of	O
the	O
form	O
:	O
where	O
a	O
...	O
f	O
are	O
literals	O
that	O
correspond	O
to	O
true	O
or	O
false	O
statements	O
.	O
Symbol	O
not	O
is	O
a	O
logical	O
connective	O
called	O
default	O
negation	O
;	O
not	O
l	O
is	O
read	O
as	O
"	O
it	O
is	O
not	O
believed	O
that	O
l	O
is	O
true	O
"	O
,	O
which	O
does	O
not	O
imply	O
that	O
l	O
is	O
false	O
.	O
ASP	O
has	O
a	O
variety	O
of	O
applications	O
(	O
Erdem	O
et	O
al	O
,	O
2016	O
)	O
.	O
Traditionally	O
,	O
ASP	O
does	O
not	O
explicitly	O
quantify	O
degrees	O
of	O
uncertainty	O
:	O
a	O
literal	O
is	O
either	O
true	O
,	O
false	O
or	O
unknown	O
.	O
P	O
-	O
log	O
extends	O
ASP	O
to	O
allow	O
probability	O
atoms	O
(	O
or	O
pr	O
-	O
atoms	O
)	O
(	O
Baral	O
et	O
al	O
,	O
2009	O
;	O
Balai	O
and	O
Gelfond	O
,	O
2017	O
)	O
.	O
The	O
following	O
pr	O
-	O
atom	O
states	O
that	O
,	O
if	O
B	O
holds	O
,	O
the	O
probability	O
of	O
a	O
(	O
t	O
)	O
=	O
y	O
is	O
v	O
:	O
pr	O
(	O
a	O
(	O
t	O
)	O
=	O
y	O
|	O
B	O
)	O
=	O
v.	O
where	O
B	O
is	O
a	O
collection	O
of	O
literals	O
or	O
their	O
default	O
negations	O
;	O
a	O
is	O
a	O
random	O
variable	O
;	O
t	O
is	O
a	O
vector	O
of	O
terms	O
(	O
a	O
term	O
is	O
a	O
constant	O
or	O
a	O
variable	O
)	O
;	O
y	O
is	O
a	O
term	O
;	O
and	O
v	O
[	O
0	B-DatasetName
,	O
1	O
]	O
.	O
Reasoning	O
with	O
an	O
ASP	O
program	O
generates	O
a	O
set	O
of	O
possible	O
worlds	O
:	O
{	O
W	O
0	B-DatasetName
,	O
W	O
1	O
,	O
}	O
.	O
The	O
pr	O
-	O
atoms	O
in	O
P	O
-	O
log	O
enable	O
calculating	O
a	O
probability	O
for	O
each	O
possible	O
world	O
.	O
Therefore	O
,	O
P	O
-	O
log	O
is	O
a	O
KRR	O
paradigm	O
that	O
supports	O
both	O
logical	O
and	O
probabilistic	O
inferences	O
.	O
We	O
use	O
P	O
-	O
log	O
in	O
this	O
work	O
for	O
KRR	O
purposes	O
.	O

KRR	O
-	O
RL	O
integrates	O
logical	O
-	O
probabilistic	O
KRR	O
and	O
model	O
-	O
based	O
RL	O
,	O
and	O
is	O
illustrated	O
in	O
Figure	O
1	O
.	O
The	O
KRR	O
component	O
includes	O
both	O
declarative	O
qualitative	O
knowledge	O
from	O
humans	O
and	O
the	O
probabilistic	O
knowledge	O
from	O
model	O
-	O
based	O
RL	O
.	O
When	O
the	O
robot	O
is	O
free	O
,	O
the	O
robot	O
arbitrarily	O
selects	O
goals	O
(	O
different	O
navigation	O
goals	O
in	O
our	O
case	O
)	O
to	O
work	O
on	O
,	O
and	O
learns	O
the	O
world	O
dynamics	O
,	O
e.g.	O
,	O
success	O
rates	O
and	O
costs	O
of	O
navigation	O
actions	O
.	O
When	O
a	O
task	O
becomes	O
available	O
,	O
the	O
KRR	O
component	O
dynamically	O
constructs	O
a	O
partial	O
world	O
model	O
(	O
excluding	O
unrelated	O
factors	O
)	O
,	O
on	O
which	O
a	O
task	O
-	O
oriented	O
controller	O
is	O
computed	O
using	O
planning	O
algorithms	O
.	O
Human	O
knowledge	O
concerns	O
environment	O
variables	O
and	O
their	O
dependencies	O
,	O
i.e.	O
,	O
what	O
variables	O
are	O
related	O
to	O
each	O
action	O
.	O
For	O
instance	O
,	O
the	O
human	O
provides	O
knowledge	O
that	O
navigation	O
actions	O
'	O
success	O
rates	O
depend	O
on	O
current	O
time	O
and	O
area	O
(	O
say	O
elevator	O
areas	O
are	O
busy	O
in	O
the	O
mornings	O
)	O
,	O
while	O
the	O
robot	O
must	O
learn	O
specific	O
probabilities	O
by	O
interacting	O
with	O
the	O
environment	O
.	O
Why	O
is	O
KRR	O
-	O
RL	O
needed	O
?	O
Consider	O
an	O
indoor	O
robot	B-TaskName
navigation	I-TaskName
domain	O
,	O
where	O
a	O
robot	O
wants	O
to	O
maximize	O
the	O
success	O
rate	O
of	O
moving	O
to	O
goal	O
positions	O
through	O
navigation	O
actions	O
.	O
Shall	O
we	O
include	O
factors	O
,	O
such	O
as	O
time	O
,	O
weather	O
,	O
positions	O
of	O
human	O
walkers	O
,	O
etc	O
,	O
into	O
the	O
state	O
space	O
?	O
On	O
the	O
one	O
hand	O
,	O
to	O
ensure	O
model	O
completeness	O
,	O
the	O
answer	O
should	O
be	O
"	O
yes	O
"	O
.	O
Human	O
walkers	O
and	O
sunlight	O
(	O
that	O
blinds	O
robot	O
's	O
LiDAR	O
sensors	O
)	O
reduce	O
the	O
success	O
rates	O
of	O
the	O
robot	O
's	O
navigation	O
actions	O
,	O
and	O
both	O
can	O
cause	O
the	O
robot	O
irrecoverably	O
lost	O
.	O
On	O
the	O
other	O
hand	O
,	O
to	O
ensure	O
computational	O
feasibility	O
,	O
the	O
answer	O
is	O
"	O
no	O
"	O
.	O
Modeling	O
whether	O
one	O
specific	O
grid	O
cell	O
being	O
occupied	O
by	O
humans	O
or	O
not	O
introduces	O
one	O
extra	O
dimension	O
in	O
the	O
state	O
space	O
,	O
and	O
doubles	O
the	O
state	O
space	O
size	O
.	O
If	O
we	O
consider	O
(	O
only	O
)	O
ten	O
such	O
grid	O
cells	O
,	O
the	O
state	O
space	O
becomes	O
2	O
10	O
≈	O
1000	O
times	O
bigger	O
.	O
As	O
a	O
result	O
,	O
RL	O
practitioners	O
frequently	O
have	O
to	O
make	O
a	O
trade	O
-	O
off	O
between	O
model	O
completeness	O
and	O
computational	O
feasibility	O
.	O
In	O
this	O
work	O
,	O
we	O
aim	O
at	O
a	O
framework	O
that	O
retains	O
both	O
model	O
scalability	O
and	O
computational	O
feasibility	O
,	O
i.e.	O
,	O
the	O
agent	B-DatasetName
is	O
able	O
to	O
learn	O
within	O
relatively	O
little	O
memory	O
while	O
computing	O
action	O
policies	O
accounting	O
for	O
a	O
large	O
number	O
of	O
domain	O
variables	O
.	O

In	O
factored	O
spaces	O
,	O
state	O
variables	O
V	O
=	O
{	O
V	O
0	B-DatasetName
,	O
V	O
1	O
,	O
...	O
,	O
V	O
n−1	O
}	O
can	O
be	O
split	O
into	O
two	O
categories	O
,	O
namely	O
endogenous	O
variables	O
V	O
en	O
and	O
exogenous	O
variables	O
V	O
ex	O
(	O
Chermack	O
,	O
2004	O
)	O
,	O
where	O
V	O
en	O
=	O
{	O
V	O
en	O
0	B-DatasetName
,	O
V	O
en	O
1	O
,	O
...	O
,	O
V	O
en	O
p−1	O
}	O
and	O
V	O
ex	O
=	O
{	O
V	O
ex	O
0	B-DatasetName
,	O
V	O
ex	O
1	O
,	O
...	O
,	O
V	O
ex	O
q−1	O
}	O
.	O
In	O
our	O
integrated	O
KRR	O
-	O
RL	O
context	O
,	O
V	O
en	O
is	O
goal	O
-	O
oriented	O
and	O
includes	O
the	O
variables	O
whose	O
values	O
the	O
robot	O
wants	O
to	O
actively	O
change	O
so	O
as	O
to	O
achieve	O
the	O
goal	O
;	O
and	O
V	O
ex	O
corresponds	O
to	O
the	O
variables	O
whose	O
values	O
affect	O
the	O
robot	O
's	O
action	O
outcomes	O
,	O
but	O
the	O
robot	O
can	O
not	O
(	O
or	O
does	O
not	O
want	O
to	O
)	O
change	O
their	O
values	O
.	O
Therefore	O
,	O
V	O
en	O
and	O
V	O
ex	O
both	O
depend	O
on	O
task	O
τ	O
.	O
Continuing	O
the	O
navigation	O
example	O
,	O
robot	O
position	O
is	O
an	O
endogenous	O
variable	O
,	O
and	O
current	O
time	O
is	O
an	O
exogenous	O
variable	O
.	O
For	O
each	O
task	O
,	O
V	O
=	O
V	O
en	O
∪	O
V	O
ex	O
and	O
n	O
=	O
p	O
+	O
q	O
,	O
and	O
RL	O
agents	O
learn	O
in	O
spaces	O
specified	O
by	O
V	O
en	O
.	O
The	O
KRR	O
component	O
models	O
V	O
,	O
their	O
dependencies	O
from	O
human	O
knowledge	O
,	O
and	O
conditional	O
probabilities	O
on	O
how	O
actions	O
change	O
their	O
values	O
,	O
as	O
learned	O
through	O
model	O
-	O
based	O
RL	O
.	O
When	O
a	O
task	O
arrives	O
,	O
the	O
KRR	O
component	O
uses	O
probabilistic	O
rules	O
to	O
generate	O
a	O
task	O
-	O
oriented	O
Markov	O
decision	O
process	O
(	O
MDP	O
)	O
(	O
Puterman	O
,	O
1994	O
)	O
,	O
which	O
only	O
contains	O
a	O
subset	O
of	O
V	O
that	O
are	O
relevant	O
to	O
the	O
current	O
task	O
,	O
Procedure	O
1	O
Learning	O
in	O
KRR	O
-	O
RL	O
Framework	O
Require	O
:	O
Logical	O
rules	O
Π	O
L	O
;	O
probabilistic	O
rules	O
Π	O
P	O
;	O
random	O
variables	O
V	O
=	O
{	O
V	O
0	B-DatasetName
,	O
V	O
1	O
,	O
...	O
,	O
V	O
n−1	O
}	O
;	O
task	O
selector	O
∆	O
;	O
and	O
guidance	O
functions	O
(	O
from	O
human	O
knowledge	O
)	O
of	O
f	O
V	O
(	O
V	O
,	O
τ	O
)	O
and	O
f	O
A	O
(	O
τ	O
)	O
1	O
:	O
while	O
Robot	O
has	O
no	O
task	O
do	O
2	O
:	O
τ	O
∆	O
(	O
)	O
:	O
a	O
task	O
is	O
heuristically	O
selected	O
3	O
:	O
V	O
en	O
f	O
V	O
(	O
V	O
,	O
τ	O
)	O
,	O
and	O
V	O
ex	O
V	O
\	O
V	O
en	O
4	O
:	O
A	O
f	O
A	O
(	O
τ	O
)	O
5	O
:	O
M	O
Procedure	O
-	O
2	O
(	O
Π	O
L	O
,	O
Π	O
P	O
,	O
V	O
en	O
,	O
V	O
ex	O
,	O
A	O
)	O
6	O
:	O
Initialize	O
agent	B-DatasetName
:	O
agent	B-DatasetName
R	O
-	O
Max	O
(	O
M	O
)	O
7	O
:	O
RL	O
agent	B-DatasetName
repeatedly	O
works	O
on	O
task	O
τ	O
,	O
and	O
keeps	O
maintaining	O
task	O
model	O
M	O
,	O
until	O
policy	O
convergence	O
8	O
:	O
end	O
while	O
9	O
:	O
Use	O
M	O
to	O
update	O
Π	O
P	O
i.e.	O
,	O
V	O
en	O
,	O
and	O
their	O
transition	O
probabilities	O
.	O
Given	O
this	O
task	O
-	O
oriented	O
MDP	O
,	O
a	O
corresponding	O
action	O
policy	O
is	O
computed	O
using	O
value	O
iteration	O
or	O
policy	O
iteration	O
.	O
Procedures	O
1	O
and	O
2	O
focus	O
on	O
how	O
our	O
KRR	O
-	O
RL	O
agent	B-DatasetName
learns	O
by	O
interacting	O
with	O
an	O
environment	O
when	O
there	O
is	O
no	O
task	O
assigned	O
.	O
1	O
Next	O
,	O
we	O
present	O
the	O
details	O
of	O
these	O
two	O
interleaved	O
processes	O
.	O
Procedure	O
1	O
includes	O
the	O
steps	O
of	O
the	O
learning	O
process	O
.	O
When	O
the	O
robot	O
is	O
free	O
,	O
it	O
interacts	O
with	O
the	O
environment	O
by	O
heuristically	O
selecting	O
a	O
task	O
2	O
,	O
and	O
repeatedly	O
using	O
a	O
model	O
-	O
based	O
RL	O
approach	O
,	O
R	O
-	O
Max	O
(	O
Brafman	O
and	O
Tennenholtz	O
,	O
2002	O
)	O
in	O
our	O
case	O
,	O
to	O
complete	O
the	O
task	O
.	O
The	O
two	O
guidance	O
functions	O
come	O
from	O
human	O
knowledge	O
.	O
For	O
instance	O
,	O
given	O
a	O
navigation	O
task	O
,	O
it	O
comes	O
from	O
human	O
knowledge	O
that	O
the	O
robot	O
should	O
model	O
its	O
own	O
position	O
(	O
specified	O
by	O
f	O
V	O
)	O
and	O
actions	O
that	O
help	O
the	O
robot	O
move	O
between	O
positions	O
(	O
specified	O
by	O
f	O
A	O
)	O
.	O
After	O
the	O
policy	O
converges	O
or	O
this	O
learning	O
process	O
is	O
interrupted	O
(	O
e.g.	O
,	O
by	O
task	O
arrivals	O
)	O
,	O
the	O
robot	O
uses	O
the	O
learned	O
probabilities	O
to	O
update	O
the	O
corresponding	O
world	O
dynamics	O
in	O
KRR	O
.	O
For	O
instance	O
,	O
the	O
robot	O
may	O
have	O
learned	O
the	O
probability	O
and	O
cost	O
of	O
navigating	O
through	O
a	O
particular	O
area	O
in	O
early	O
morning	O
.	O
In	O
case	O
this	O
learning	O
process	O
is	O
interrupted	O
,	O
the	O
sofar	O
-	O
"	O
known	O
"	O
probabilities	O
are	O
used	O
for	O
knowledge	O
base	O
update	O
.	O
Procedure	O
2	O
includes	O
the	O
steps	O
for	O
building	O
the	O
probabilistic	O
transition	O
system	O
of	O
MDPs	O
.	O
The	O
key	O
point	O
is	O
that	O
we	O
consider	O
only	O
endogenous	O
variables	O
in	O
the	O
task	O
-	O
specific	O
state	O
space	O
.	O
However	O
,	O
when	O
1	O
As	O
soon	O
as	O
the	O
robot	O
's	O
learning	O
process	O
is	O
interrupted	O
by	O
the	O
arrival	O
of	O
a	O
real	O
service	O
task	O
(	O
identified	O
via	O
dialog	O
)	O
,	O
it	O
will	O
call	O
Procedure	O
2	O
to	O
generate	O
a	O
controller	O
to	O
complete	O
the	O
task	O
.	O
This	O
process	O
is	O
not	O
included	O
in	O
the	O
procedures	O
.	O
2	O
Here	O
curriculum	O
learning	O
in	O
RL	O
(	O
Narvekar	O
et	O
al	O
,	O
2017	O
)	O
can	O
play	O
a	O
role	O
to	O
task	O
selection	O
and	O
we	O
leave	O
this	O
aspect	O
of	O
the	O
problem	O
for	O
future	O
work	O
.	O
Figure	O
2	O
:	O
Transition	O
system	O
specified	O
for	O
delivery	O
tasks	O
,	O
where	O
question	O
-	O
asking	O
actions	O
are	O
used	O
for	O
estimating	O
the	O
service	O
request	O
in	O
dialog	O
.	O
Once	O
the	O
robot	O
becomes	O
confident	O
about	O
the	O
service	O
request	O
,	O
it	O
starts	O
to	O
work	O
on	O
the	O
navigation	O
subtask	O
.	O
After	O
the	O
robot	O
arrives	O
,	O
the	O
robot	O
might	O
have	O
to	O
come	O
back	O
to	O
the	O
dialog	O
subtask	O
and	O
redeliver	O
,	O
depending	O
on	O
whether	O
the	O
service	O
request	O
was	O
correctly	O
identified	O
.	O
reasoning	O
to	O
compute	O
the	O
transition	O
probabilities	O
(	O
Line	O
5	O
)	O
,	O
the	O
KRR	O
component	O
uses	O
both	O
Π	O
P	O
and	O
V	O
ex	O
.	O
The	O
computed	O
probabilistic	O
transition	O
systems	O
are	O
used	O
for	O
building	O
task	O
-	O
oriented	O
controllers	O
,	O
i.e.	O
,	O
π	O
,	O
for	O
task	O
completions	O
.	O
In	O
this	O
way	O
,	O
the	O
dynamically	O
constructed	O
controllers	O
do	O
not	O
directly	O
include	O
exogenous	O
variables	O
,	O
but	O
their	O
parameters	O
already	O
account	O
for	O
the	O
values	O
of	O
all	O
variables	O
.	O
Next	O
,	O
we	O
demonstrate	O
how	O
our	O
KRR	O
-	O
RL	O
framework	O
is	O
instantiated	O
on	O
a	O
real	O
robot	O
.	O

We	O
consider	O
a	O
mobile	O
service	O
robot	O
domain	O
where	O
a	O
robot	O
can	O
do	O
navigation	O
,	O
dialog	O
,	O
and	O
delivery	O
tasks	O
.	O
A	O
navigation	O
task	O
requires	O
the	O
robot	O
to	O
use	O
a	O
sequence	O
of	O
(	O
unreliable	O
)	O
navigation	O
actions	O
to	O
move	O
from	O
one	O
point	O
to	O
another	O
.	O
In	O
a	O
dialog	O
task	O
,	O
the	O
robot	O
uses	O
spoken	O
dialog	O
actions	O
to	O
specify	O
service	O
requests	O
from	O
people	O
under	O
imperfect	O
language	O
understanding	O
.	O
There	O
is	O
the	O
trend	O
of	O
integrating	O
language	O
and	O
navigation	O
in	O
the	O
NLP	O
and	O
CV	O
communities	O
(	O
Chen	O
et	O
al	O
,	O
2019	O
;	O
Shridhar	O
et	O
al	O
,	O
2020	O
)	O
.	O
In	O
this	O
paper	O
,	O
they	O
are	O
integrated	O
into	O
delivery	O
tasks	O
that	O
require	O
the	O
robot	O
to	O
use	O
dialog	O
to	O
figure	O
out	O
the	O
delivery	O
request	O
and	O
conduct	O
navigation	O
tasks	O
to	O
physically	O
fulfill	O
the	O
request	O
.	O
Specifically	O
,	O
a	O
delivery	O
task	O
requires	O
the	O
robot	O
to	O
deliver	O
item	O
I	O
to	O
room	O
R	O
for	O
person	O
P	O
,	O
resulting	O
in	O
services	O
in	O
the	O
form	O
of	O
<	O
I	O
,	O
R	O
,	O
P	O
>	O
.	O
The	O
challenges	O
come	O
from	O
unreliable	O
human	O
language	O
understanding	O
(	O
e.g.	O
,	O
speech	B-TaskName
recognition	I-TaskName
)	O
and	O
unforeseen	O
obstacles	O
that	O
probabilistically	O
block	O
the	O
robot	O
in	O
navigation	O
.	O

The	O
robot	O
needs	O
spoken	O
dialog	O
to	O
identify	O
the	O
request	O
under	O
unreliable	O
language	O
understanding	O
,	O
and	O
navigation	O
controllers	O
for	O
physically	O
making	O
the	O
delivery	O
.	O
The	O
service	O
request	O
is	O
not	O
directly	O
observable	O
to	O
the	O
robot	O
,	O
and	O
has	O
to	O
be	O
estimated	O
by	O
asking	O
questions	O
,	O
such	O
as	O
"	O
What	O
item	O
do	O
you	O
want	O
?	O
"	O
and	O
"	O
Is	O
Procedure	O
2	O
Model	O
Construction	O
for	O
Task	O
Completion	O
Require	O
:	O
Π	O
L	O
;	O
Π	O
P	O
;	O
V	O
en	O
;	O
V	O
ex	O
;	O
Action	O
set	O
A	O
1	O
:	O
for	O
V	O
i	O
V	O
en	O
,	O
i	O
in	O
[	O
0	B-DatasetName
,	O
,	O
|	O
V	O
en	O
|	O
−1	O
]	O
do	O
2	O
:	O
for	O
each	O
possible	O
value	O
v	O
in	O
range	O
(	O
V	O
i	O
)	O
do	O
3	O
:	O
for	O
each	O
a	O
A	O
do	O
4	O
:	O
for	O
each	O
possible	O
value	O
v	O
in	O
range	O
(	O
V	O
i	O
)	O
do	O
5	O
:	O
M	O
(	O
v	O
|	O
a	O
,	O
v	O
)	O
Reason	O
with	O
Π	O
L	O
and	O
Π	O
P	O
w.r.t	O
V	O
ex	O
6	O
:	O
end	O
for	O
7	O
:	O
end	O
for	O
8	O
:	O
end	O
for	O
9	O
:	O
end	O
for	O
10	O
:	O
return	O
M	O
this	O
delivery	O
for	O
Alice	O
?	O
"	O
Once	O
the	O
robot	O
is	O
confident	O
about	O
the	O
request	O
,	O
it	O
takes	O
a	O
delivery	O
action	O
(	O
i.e.	O
,	O
serve	O
(	O
I	O
,	O
R	O
,	O
P	O
)	O
)	O
.	O
We	O
follow	O
a	O
standard	O
way	O
to	O
use	O
partially	O
observable	O
MDPs	O
(	O
POMDPs	O
)	O
(	O
Kaelbling	O
et	O
al	O
,	O
1998	O
)	O
to	O
build	O
our	O
dialog	O
manager	O
,	O
as	O
reviewed	O
in	O
(	O
Young	O
et	O
al	O
,	O
2013	O
)	O
.	O
The	O
state	O
set	O
S	O
is	O
specified	O
using	O
curr	O
s.	O
The	O
action	O
set	O
A	O
is	O
specified	O
using	O
serve	O
and	O
question	O
-	O
asking	O
actions	O
.	O
Question	O
-	O
asking	O
actions	O
do	O
not	O
change	O
the	O
current	O
state	O
,	O
and	O
delivery	O
actions	O
lead	O
to	O
one	O
of	O
the	O
terminal	O
states	O
(	O
success	O
or	O
failure	O
)	O
.	O
3	O
After	O
the	O
robot	O
becomes	O
confident	O
about	O
the	O
request	O
via	O
dialog	O
,	O
it	O
will	O
take	O
a	O
delivery	O
action	O
serve	O
{	O
I	O
,	O
R	O
,	O
P	O
}	O
.	O
This	O
delivery	O
action	O
is	O
then	O
implemented	O
with	O
a	O
sequence	O
of	O
act	O
move	O
actions	O
.	O
When	O
the	O
request	O
identification	O
is	O
incorrect	O
,	O
the	O
robot	O
needs	O
to	O
come	O
back	O
to	O
the	O
shop	O
,	O
figure	O
out	O
the	O
correct	O
request	O
,	O
and	O
redeliver	O
,	O
where	O
we	O
assume	O
the	O
robot	O
will	O
correctly	O
identify	O
the	O
request	O
in	O
the	O
second	O
dialog	O
.	O
We	O
use	O
an	O
MDP	O
to	O
model	O
this	O
robot	B-TaskName
navigation	I-TaskName
task	O
,	O
where	O
the	O
states	O
and	O
actions	O
are	O
specified	O
using	O
sorts	O
cell	O
and	O
move	O
.	O
We	O
use	O
pr	O
-	O
atoms	O
to	O
represent	O
the	O
success	O
rates	O
of	O
the	O
unreliable	O
movements	O
,	O
which	O
are	O
learned	O
through	O
model	O
-	O
based	O
RL	O
.	O
The	O
dialog	O
system	O
builds	O
on	O
our	O
previous	O
work	O
(	O
Lu	O
et	O
al	O
,	O
2017	O
)	O
.	O
Figure	O
2	O
shows	O
the	O
probabilistic	O
transitions	O
in	O
delivery	O
tasks	O
.	O

We	O
use	O
R	O
-	O
Max	O
(	O
Brafman	O
and	O
Tennenholtz	O
,	O
2002	O
)	O
,	O
a	O
model	O
-	O
based	O
RL	O
algorithm	O
,	O
to	O
help	O
our	O
robot	O
learn	O
the	O
success	O
rate	O
of	O
navigation	O
actions	O
in	O
different	O
positions	O
.	O
The	O
agent	B-DatasetName
first	O
initializes	O
an	O
MDP	O
,	O
from	O
which	O
it	O
uses	O
R	O
-	O
Max	O
to	O
learn	O
the	O
partial	O
world	O
model	O
(	O
of	O
navigation	O
tasks	O
)	O
.	O
Specifically	O
,	O
it	O
initializes	O
the	O
transition	O
function	O
with	O
T	O
N	O
(	O
s	O
,	O
a	O
,	O
s	O
v	O
)	O
=	O
1.0	O
,	O
where	O
s	O
S	O
and	O
a	O
A	O
,	O
meaning	O
that	O
starting	O
from	O
any	O
state	O
,	O
after	O
any	O
action	O
,	O
the	O
next	O
state	O
is	O
always	O
s	O
v	O
.	O
The	O
reward	O
function	O
is	O
initialized	O
with	O
R	O
(	O
s	O
,	O
a	O
)	O
=	O
R	O
max	O
,	O
where	O
R	O
max	O
is	O
an	O
upper	O
bound	O
of	O
reward	O
.	O
The	O
initialization	O
of	O
T	O
N	O
and	O
R	O
enables	O
the	O
learner	O
to	O
automatically	O
balance	O
exploration	O
and	O
exploitation	O
.	O
There	O
is	O
a	O
fixed	O
small	O
cost	O
for	O
each	O
navigation	O
action	O
.	O
The	O
robot	O
receives	O
a	O
big	O
bonus	O
if	O
it	O
successfully	O
achieves	O
the	O
goal	O
(	O
R	O
max	O
)	O
,	O
whereas	O
it	O
receives	O
a	O
big	O
penalty	O
otherwise	O
(	O
−R	O
max	O
)	O
.	O
A	O
transition	O
probability	O
in	O
navigation	O
,	O
T	O
N	O
(	O
s	O
,	O
a	O
,	O
s	O
)	O
,	O
is	O
not	O
computed	O
until	O
there	O
are	O
a	O
minimum	O
number	O
(	O
M	O
)	O
of	O
transition	O
samples	O
visiting	O
s	O
.	O
We	O
recompute	O
the	O
action	O
policy	O
after	O
E	O
action	O
steps	O
.	O

The	O
update	O
of	O
knowledge	O
base	O
is	O
achieved	O
through	O
updating	O
the	O
success	O
rate	O
of	O
delivery	O
actions	O
serve	O
(	O
I	O
,	O
R	O
,	O
P	O
)	O
(	O
in	O
dialog	O
task	O
)	O
using	O
the	O
success	O
rate	O
of	O
navigation	O
actions	O
act	O
move	O
=	O
M	O
in	O
different	O
positions	O
.	O
T	O
D	O
(	O
s	O
r	O
,	O
a	O
d	O
,	O
s	O
t	O
)	O
=	O
P	O
N	O
(	O
s	O
sp	O
,	O
s	O
gl	O
)	O
,	O
if	O
s	O
r	O
a	O
d	O
P	O
N	O
(	O
s	O
sp	O
,	O
s	O
mi	O
)	O
×P	O
N	O
(	O
s	O
mi	O
,	O
s	O
sp	O
)	O
×P	O
N	O
(	O
s	O
sp	O
,	O
s	O
gl	O
)	O
,	O
if	O
s	O
r	O
⊗	O
a	O
d	O
where	O
T	O
D	O
(	O
s	O
r	O
,	O
a	O
d	O
,	O
s	O
t	O
)	O
is	O
the	O
probability	O
of	O
fulfilling	O
request	O
s	O
r	O
using	O
delivery	O
action	O
a	O
d	O
;	O
s	O
t	O
is	O
the	O
"	O
success	O
"	O
terminal	O
state	O
;	O
s	O
sp	O
,	O
s	O
mi	O
and	O
s	O
gl	O
are	O
states	O
of	O
the	O
robot	O
being	O
in	O
the	O
shop	O
,	O
a	O
misidentified	O
goal	O
position	O
,	O
and	O
real	O
goal	O
position	O
respectively	O
;	O
and	O
P	O
N	O
(	O
s	O
,	O
s	O
)	O
is	O
the	O
probability	O
of	O
the	O
robot	O
successfully	O
navigating	O
from	O
s	O
to	O
s	O
positions	O
.	O
When	O
s	O
r	O
and	O
a	O
d	O
are	O
aligned	O
in	O
all	O
three	O
dimensions	O
(	O
i.e.	O
,	O
s	O
r	O
a	O
d	O
)	O
,	O
the	O
robot	O
needs	O
to	O
navigate	O
once	O
from	O
the	O
shop	O
(	O
s	O
sp	O
)	O
to	O
the	O
requested	O
navigation	O
goal	O
(	O
s	O
gl	O
)	O
.	O
P	O
N	O
(	O
s	O
sp	O
,	O
s	O
gl	O
)	O
is	O
the	O
probability	O
of	O
the	O
corresponding	O
navigation	O
task	O
.	O
When	O
the	O
request	O
and	O
delivery	O
action	O
are	O
not	O
aligned	O
in	O
at	O
least	O
one	O
dimension	O
(	O
i.e.	O
,	O
s	O
r	O
⊗	O
a	O
d	O
)	O
,	O
the	O
robot	O
has	O
to	O
navigate	O
back	O
to	O
the	O
shop	O
to	O
figure	O
out	O
the	O
correct	O
request	O
,	O
and	O
then	O
redeliver	O
,	O
resulting	O
in	O
three	O
navigation	O
tasks	O
.	O
Intuitively	O
,	O
the	O
penalty	O
of	O
failures	O
in	O
a	O
dialog	O
subtask	O
depends	O
on	O
the	O
difficulty	O
of	O
the	O
wrongly	O
identified	O
navigation	O
subtask	O
.	O
For	O
instance	O
,	O
a	O
robot	O
supposed	O
to	O
deliver	O
to	O
a	O
near	O
(	O
distant	O
)	O
location	O
being	O
wrongly	O
directed	O
to	O
a	O
distant	O
(	O
near	O
)	O
location	O
,	O
due	O
to	O
a	O
failure	O
in	O
the	O
dialog	O
subtask	O
,	O
will	O
produce	O
a	O
higher	O
(	O
lower	O
)	O
penalty	O
to	O
the	O
dialog	O
agent	B-DatasetName
.	O

In	O
this	O
section	O
,	O
the	O
goal	O
is	O
to	O
evaluate	O
our	O
hypothesis	O
that	O
our	O
KRR	O
-	O
RL	O
framework	O
enables	O
a	O
robot	O
to	O
learn	O
from	O
model	O
-	O
based	O
RL	O
,	O
reason	O
with	O
both	O
the	O
learned	O
knowledge	O
and	O
human	O
knowledge	O
,	O
and	O
dynamically	O
construct	O
task	O
-	O
oriented	O
controllers	O
.	O
Specifically	O
,	O
our	O
robot	O
learns	O
from	O
navigation	O
tasks	O
,	O
and	O
applied	O
the	O
learned	O
knowledge	O
(	O
through	O
KRR	O
)	O
to	O
navigation	O
,	O
dialog	O
,	O
and	O
delivery	O
tasks	O
.	O
We	O
also	O
evaluated	O
whether	O
the	O
learned	O
knowledge	O
can	O
be	O
represented	O
and	O
applied	O
to	O
tasks	O
under	O
different	O
world	O
settings	O
.	O
In	O
addition	O
to	O
simulation	O
experiments	O
,	O
we	O
have	O
used	O
a	O
real	O
robot	O
to	O
demonstrate	O
how	O
our	O
robot	O
learns	O
from	O
navigation	O
to	O
perform	O
better	O
in	O
dialog	O
.	O
Figure	O
3	O
shows	O
the	O
map	O
of	O
the	O
working	O
environment	O
(	O
generated	O
using	O
a	O
real	O
robot	O
)	O
used	O
in	O
both	O
simulation	O
and	O
real	O
-	O
robot	O
experiments	O
.	O
Human	O
walkers	O
in	O
the	O
blocking	O
areas	O
(	O
"	O
BA	B-DatasetName
"	O
)	O
can	O
probabilistically	O
impede	O
the	O
robot	O
,	O
resulting	O
in	O
different	O
success	O
rates	O
in	O
navigation	O
tasks	O
.	O
We	O
have	O
implemented	O
our	O
KRR	O
-	O
RL	O
framework	O
on	O
a	O
mobile	O
robot	O
in	O
an	O
office	O
environment	O
.	O
As	O
shown	O
in	O
Figure	O
3	O
,	O
the	O
robot	O
is	O
equipped	O
with	O
two	O
Lidar	O
sensors	O
for	O
localization	O
and	O
obstacle	O
avoidance	O
in	O
navigation	O
,	O
and	O
a	O
Kinect	O
RGB	O
-	O
D	O
camera	O
for	O
human	O
-	O
robot	O
interaction	O
.	O
We	O
use	O
the	O
Speech	O
Application	O
Programming	O
Interface	O
(	O
SAPI	O
)	O
package	O
(	O
http://www.iflytek.com/en	O
)	O
for	O
speech	B-TaskName
recognition	I-TaskName
.	O
The	O
robot	O
software	O
runs	O
in	O
the	O
Robot	O
Operating	O
System	O
(	O
ROS	O
)	O
(	O
Quigley	O
et	O
al	O
,	O
2009	O
)	O
.	O
(	O
e	O
)	O
Robot	O
was	O
on	O
the	O
way	O
to	O
the	O
kitchen	O
to	O
pick	O
up	O
the	O
object	O
;	O
and	O
(	O
f	O
)	O
Robot	O
arrived	O
at	O
the	O
kitchen	O
,	O
and	O
was	O
going	O
to	O
pick	O
up	O
the	O
object	O
for	O
delivery	O
.	O
An	O
Illustrative	O
Trial	O
on	O
a	O
Robot	O
:	O
Figure	O
4	O
shows	O
the	O
screenshots	O
of	O
milestones	O
of	O
a	O
demo	O
video	O
,	O
which	O
will	O
be	O
made	O
available	O
given	O
its	O
acceptance	O
.	O
After	O
hearing	O
"	O
a	O
coke	O
for	O
Bob	O
to	O
office2	O
"	O
,	O
the	O
three	O
sub	O
-	O
beliefs	O
are	O
updated	O
(	O
turn1	O
)	O
.	O
Since	O
the	O
robot	O
is	O
aware	O
of	O
its	O
unreliable	O
speech	B-TaskName
recognition	I-TaskName
,	O
it	O
asked	O
about	O
the	O
item	O
,	O
"	O
Which	O
item	O
is	O
it	O
?	O
"	O
After	O
hearing	O
"	O
a	O
coke	O
"	O
,	O
the	O
belief	O
is	O
updated	O
(	O
turn2	O
)	O
,	O
and	O
the	O
robot	O
further	O
confirmed	O
on	O
the	O
item	O
by	O
asking	O
"	O
Should	O
I	O
deliver	O
a	O
coke	O
?	O
"	O
It	O
received	O
a	O
positive	O
response	O
(	O
turn3	O
)	O
,	O
and	O
decided	O
to	O
move	O
on	O
to	O
ask	O
about	O
the	O
delivery	O
room	O
:	O
"	O
Should	O
I	O
deliver	O
to	O
office	O
2	O
?	O
"	O
After	O
this	O
question	O
,	O
the	O
robot	O
did	O
not	O
further	O
confirm	O
the	O
delivery	O
room	O
,	O
because	O
it	O
learned	O
through	O
model	O
-	O
based	O
RL	O
that	O
navigating	O
to	O
office2	O
is	O
relatively	O
easy	O
and	O
it	O
decided	O
that	O
it	O
is	O
more	O
worth	O
risking	O
an	O
error	O
and	O
having	O
to	O
replan	O
than	O
it	O
is	O
to	O
ask	O
the	O
person	O
another	O
question	O
.	O
The	O
robot	O
became	O
confident	O
in	O
three	O
dimensions	O
of	O
the	O
service	O
request	O
(	O
<	O
coke	O
,	O
Bob	O
,	O
office2	O
>	O
in	O
turn4	O
)	O
without	O
asking	O
about	O
person	O
,	O
because	O
of	O
the	O
prior	O
knowledge	O
(	O
encoded	O
in	O
P	O
-	O
log	O
)	O
about	O
Bob	O
's	O
office	O
.	O
Figure	O
5	O
shows	O
the	O
belief	O
changes	O
(	O
in	O
the	O
di	O
-	O
mensions	O
of	O
item	O
,	O
person	O
,	O
and	O
room	O
)	O
as	O
the	O
robot	O
interacts	O
with	O
a	O
human	O
user	O
.	O
The	O
robot	O
started	O
with	O
a	O
uniform	O
distribution	O
in	O
all	O
three	O
categories	O
.	O
It	O
should	O
be	O
noted	O
that	O
,	O
although	O
the	O
marginal	O
distributions	O
are	O
uniform	O
,	O
the	O
joint	O
belief	O
distribution	O
is	O
not	O
,	O
as	O
the	O
robot	O
has	O
prior	O
knowledge	O
such	O
as	O
Bob	O
's	O
office	O
is	O
office2	O
and	O
people	O
prefer	O
deliveries	O
to	O
their	O
own	O
offices	O
.	O
Demo	O
video	O
is	O
not	O
included	O
to	O
respect	O
the	O
anonymous	O
review	O
process	O
.	O

In	O
this	O
experiment	O
,	O
the	O
robot	O
learns	O
in	O
the	O
shop	O
-	O
room1	O
navigation	O
task	O
,	O
and	O
extracts	O
the	O
learned	O
partial	O
world	O
model	O
to	O
the	O
shop	O
-	O
room2	O
task	O
.	O
It	O
should	O
be	O
noted	O
that	O
navigation	O
from	O
shop	O
to	O
room2	O
requires	O
traveling	O
in	O
areas	O
that	O
are	O
unnecessary	O
in	O
the	O
shop	O
-	O
room1	O
task	O
.	O
Figure	O
6	O
presents	O
the	O
results	O
,	O
where	O
each	O
data	O
points	O
corresponds	O
to	O
an	O
average	O
of	O
1000	O
trials	O
.	O
Each	O
episode	O
allows	O
at	O
most	O
200	O
(	O
300	O
)	O
steps	O
in	O
small	O
(	O
large	O
)	O
domain	O
.	O
The	O
curves	O
are	O
smoothed	O
using	O
a	O
window	O
of	O
10	O
episodes	O
.	O
The	O
results	O
suggest	O
that	O
with	O
knowledge	O
extraction	O
(	O
the	O
dashed	O
line	O
)	O
the	O
robot	O
learns	O
faster	O
than	O
without	O
extraction	O
,	O
and	O
this	O
performance	O
improvement	O
is	O
more	O
significant	O
in	O
a	O
larger	O
domain	O
(	O
the	O
Right	O
subfigure	O
)	O
.	O
Learning	O
to	O
Dialog	O
and	O
Navigate	B-TaskName
from	O
Navigation	O
Tasks	O
Robot	O
delivering	O
objects	O
requires	O
both	O
tasks	O
:	O
dialog	O
management	O
for	O
specifying	O
service	O
request	O
(	O
under	O
unreliable	O
speech	B-TaskName
recognition	I-TaskName
)	O
and	O
navigation	O
for	O
physically	O
delivering	O
objects	O
(	O
under	O
unforeseen	O
obstacles	O
)	O
.	O
Our	O
office	O
domain	O
includes	O
five	O
rooms	O
,	O
two	O
persons	O
,	O
and	O
three	O
items	O
,	O
resulting	O
in	O
30	O
possible	O
service	O
requests	O
.	O
In	O
the	O
dialog	O
manager	O
,	O
the	O
reward	O
function	O
gives	O
delivery	O
actions	O
a	O
big	O
bonus	O
(	O
80	O
)	O
if	O
a	O
request	O
is	O
fulfilled	O
,	O
and	O
a	O
big	O
penalty	O
(	O
-	O
80	O
)	O
otherwise	O
.	O
General	B-DatasetName
questions	O
and	O
confirming	O
questions	O
cost	O
2.0	O
and	O
1.5	O
respectively	O
.	O
In	O
case	O
a	O
dialog	O
does	O
not	O
end	O
after	O
20	O
turns	O
,	O
the	O
robot	O
is	O
forced	O
to	O
work	O
on	O
the	O
most	O
likely	O
delivery	O
.	O
The	O
cost	O
/	O
bonus	O
/	O
penalty	O
values	O
are	O
heuristically	O
set	O
in	O
this	O
work	O
,	O
following	O
guidelines	O
based	O
on	O
studies	O
from	O
the	O
literature	O
on	O
dialog	O
agent	B-DatasetName
behaviors	O
(	O
Zhang	O
and	O
Stone	O
,	O
2015	O
)	O
.	O
Table	O
1	O
reports	O
the	O
robot	O
's	O
overall	O
performance	O
in	O
delivery	O
tasks	O
,	O
which	O
requires	O
accurate	O
dialog	O
for	O
identifying	O
delivery	O
tasks	O
and	O
safe	O
navigation	O
for	O
object	O
delivery	O
.	O
We	O
conduct	O
10	O
,	O
000	O
simulation	O
trials	O
under	O
each	O
blocking	O
rate	O
.	O
Without	O
learning	O
from	O
RL	O
,	O
the	O
robot	O
uses	O
a	O
world	O
model	O
(	O
outdated	O
)	O
that	O
was	O
learned	O
under	O
br	O
=	O
0.3	O
.	O
With	O
learning	O
,	O
the	O
robot	O
updates	O
its	O
world	O
model	O
in	O
domains	O
with	O
different	O
blocking	O
rates	O
.	O
We	O
can	O
see	O
,	O
when	O
learning	O
is	O
enabled	O
,	O
our	O
KRR	O
-	O
RL	O
framework	O
produces	O
higher	O
overall	O
reward	O
,	O
higher	O
request	O
fulfillment	O
rate	O
,	O
and	O
lower	O
question	O
-	O
asking	O
cost	O
.	O
The	O
improvement	O
is	O
statistically	O
significant	O
,	O
i.e.	O
,	O
the	O
p−values	O
are	O
0.028	O
,	O
0.035	O
,	O
and	O
0.049	O
for	O
overall	O
reward	O
,	O
when	O
br	O
is	O
0.1	O
,	O
0.5	O
,	O
and	O
0.7	O
respectively	O
(	O
100	O
randomly	O
selected	O
trials	O
with	O
/	O
without	O
extraction	O
)	O
.	O
Learning	O
to	O
Adjust	O
Dialog	O
Strategies	O
from	O
Navigation	O
In	O
the	O
last	O
experiment	O
,	O
we	O
quantify	O
the	O
information	O
collected	O
in	O
dialog	O
in	O
terms	O
of	O
entropy	O
reduction	O
.	O
The	O
hypothesis	O
is	O
that	O
,	O
using	O
our	O
KRR	O
-	O
RL	O
framework	O
,	O
the	O
dialog	O
manager	O
wants	O
to	O
collect	O
more	O
information	O
before	O
physically	O
working	O
on	O
more	O
challenging	O
tasks	O
.	O
In	O
each	O
trial	O
,	O
we	O
randomly	O
generate	O
a	O
belief	O
distribution	O
over	O
all	O
possible	O
service	O
requests	O
,	O
evaluate	O
the	O
entropy	O
of	O
this	O
belief	O
,	O
and	O
record	O
the	O
suggested	O
action	O
given	O
this	O
belief	O
.	O
We	O
then	O
statistically	O
analyze	O
the	O
entropy	O
values	O
of	O
beliefs	O
,	O
under	O
which	O
delivery	O
actions	O
are	O
suggested	O
.	O
Table	O
2	O
shows	O
that	O
,	O
when	O
br	O
grows	O
from	O
0.1	O
to	O
0.7	O
,	O
the	O
means	O
of	O
belief	O
entropy	O
decreases	O
(	O
i.e.	O
,	O
belief	O
is	O
more	O
converged	O
)	O
.	O
This	O
suggests	O
that	O
the	O
robot	O
collected	O
more	O
information	O
in	O
dialog	O
in	O
environments	O
that	O
are	O
more	O
challenging	O
for	O
navigation	O
,	O
which	O
is	O
consistent	O
with	O
Table	O
1	O
in	O
the	O
main	O
paper	O
.	O
Comparing	O
the	O
three	O
columns	O
of	O
results	O
,	O
we	O
find	O
the	O
robot	O
collects	O
the	O
most	O
information	O
before	O
it	O
delivers	O
to	O
room5	O
.	O
This	O
is	O
because	O
such	O
delivery	O
tasks	O
are	O
the	O
most	O
difficult	O
due	O
to	O
the	O
location	O
of	O
room5	O
.	O
The	O
results	O
support	O
our	O
hypothesis	O
that	O
learning	O
from	O
navigation	O
tasks	O
enables	O
the	O
robot	O
to	O
adjust	O
its	O
information	O
gathering	O
strategy	O
in	O
dialog	O
given	O
tasks	O
of	O
different	O
difficulties	O
.	O

We	O
develop	O
a	O
KRR	O
-	O
RL	O
framework	O
that	O
integrates	O
computational	O
paradigms	O
of	O
logical	O
-	O
probabilistic	O
knowledge	O
representation	O
and	O
reasoning	O
(	O
KRR	O
)	O
,	O
and	O
model	O
-	O
based	O
reinforcement	O
learning	O
(	O
RL	O
)	O
.	O
Our	O
KRR	O
-	O
RL	O
agent	B-DatasetName
learns	O
world	O
dynamics	O
via	O
modelbased	O
RL	O
,	O
and	O
then	O
incorporates	O
the	O
learned	O
dynamics	O
into	O
the	O
logical	O
-	O
probabilistic	O
reasoning	O
module	O
,	O
which	O
is	O
used	O
for	O
dynamic	O
construction	O
of	O
efficient	O
run	O
-	O
time	O
task	O
-	O
specific	O
planning	O
models	O
.	O
Experiments	O
were	O
conducted	O
using	O
a	O
mobile	O
robot	O
(	O
simulated	O
and	O
physical	O
)	O
working	O
on	O
delivery	O
tasks	O
that	O
involve	O
both	O
navigation	O
and	O
dialog	O
.	O
Results	O
suggested	O
that	O
the	O
learned	O
knowledge	O
from	O
RL	O
can	O
be	O
represented	O
and	O
used	O
for	O
reasoning	O
by	O
the	O
KRR	O
component	O
,	O
enabling	O
the	O
robot	O
to	O
dynamically	O
generate	O
task	O
-	O
oriented	O
action	O
policies	O
.	O
The	O
integration	O
of	O
a	O
KRR	O
paradigm	O
and	O
modelbased	O
RL	O
paves	O
the	O
way	O
for	O
at	O
least	O
the	O
following	O
research	O
directions	O
.	O
We	O
plan	O
to	O
study	O
how	O
to	O
sequence	O
source	O
tasks	O
to	O
help	O
the	O
robot	O
perform	O
the	O
best	O
in	O
the	O
target	O
task	O
(	O
i.e.	O
,	O
a	O
curriculum	O
learning	O
problem	O
within	O
the	O
RL	O
context	O
(	O
Narvekar	O
et	O
al	O
,	O
2017	O
)	O
)	O
.	O
Balancing	O
the	O
efficiencies	O
between	O
service	O
task	O
completion	O
and	O
RL	O
is	O
another	O
topic	O
for	O
further	O
study	O
-	O
currently	O
the	O
robot	O
optimizes	O
for	O
task	O
completions	O
(	O
without	O
considering	O
the	O
potential	O
knowledge	O
learned	O
in	O
this	O
process	O
)	O
once	O
a	O
task	O
becomes	O
available	O
.	O
Fundamentally	O
,	O
all	O
domain	O
variables	O
are	O
endogenous	O
,	O
because	O
one	O
can	O
hardly	O
find	O
variables	O
whose	O
values	O
are	O
completely	O
independent	O
from	O
robot	O
actions	O
.	O
However	O
,	O
for	O
practical	O
reasons	O
(	O
such	O
as	O
limited	O
computational	O
resources	O
)	O
,	O
people	O
have	O
to	O
limit	O
the	O
number	O
of	O
endogenous	O
.	O
It	O
remains	O
an	O
open	O
question	O
of	O
how	O
to	O
decide	O
what	O
variables	O
should	O
be	O
considered	O
as	O
being	O
endogenous	O
.	O

Partner	O
Personas	O
Generation	O
for	O
Dialogue	O
Response	B-TaskName
Generation	I-TaskName

Incorporating	O
personas	O
information	O
allows	O
diverse	O
and	O
engaging	O
responses	O
in	O
dialogue	O
response	B-TaskName
generation	I-TaskName
.	O
Unfortunately	O
,	O
prior	O
works	O
have	O
primarily	O
focused	O
on	O
self	O
personas	O
and	O
have	O
overlooked	O
the	O
value	O
of	O
partner	O
personas	O
.	O
Moreover	O
,	O
in	O
practical	O
applications	O
,	O
the	O
availability	O
of	O
the	O
gold	O
partner	O
personas	O
is	O
often	O
not	O
the	O
case	O
.	O
This	O
paper	O
attempts	O
to	O
tackle	O
these	O
issues	O
by	O
offering	O
a	O
novel	O
framework	O
that	O
leverages	O
automatic	O
partner	O
personas	O
generation	O
to	O
enhance	O
the	O
succeeding	O
dialogue	O
response	B-TaskName
generation	I-TaskName
.	O
Our	O
framework	O
employs	O
reinforcement	O
learning	O
with	O
a	O
dedicatedly	O
designed	O
critic	O
network	O
for	O
reward	O
judgement	O
.	O
Experimental	O
results	O
from	O
automatic	O
and	O
human	O
evaluations	O
indicate	O
that	O
our	O
framework	O
is	O
capable	O
of	O
generating	O
relevant	O
,	O
interesting	O
,	O
coherent	O
and	O
informative	O
partner	O
personas	O
,	O
even	O
compared	O
to	O
the	O
ground	O
truth	O
partner	O
personas	O
.	O
This	O
enhances	O
the	O
succeeding	O
dialogue	O
response	B-TaskName
generation	I-TaskName
,	O
which	O
surpasses	O
our	O
competitive	O
baselines	O
that	O
condition	O
on	O
the	O
ground	O
truth	O
partner	O
personas	O
.	O

Building	O
informative	O
and	O
engaging	O
dialogue	O
agents	O
Roller	O
et	O
al	O
,	O
2021	O
)	O
is	O
a	O
popular	O
research	O
direction	O
within	O
the	O
area	O
of	O
natural	O
language	O
processing	O
.	O
For	O
the	O
sake	O
of	O
engagement	O
,	O
diverse	O
and	O
consistent	O
responses	O
(	O
Song	O
et	O
al	O
,	O
2020	O
(	O
Song	O
et	O
al	O
,	O
,	O
2021	O
are	O
important	O
factors	O
,	O
and	O
personas	O
information	O
gives	O
rise	O
to	O
both	O
.	O
There	O
are	O
two	O
types	O
of	O
personas	O
,	O
namely	O
self	O
persona	O
and	O
partner	O
persona	O
.	O
The	O
former	O
refers	O
to	O
a	O
self	O
profile	O
consisting	O
of	O
several	O
sentences	O
representing	O
the	O
dialogue	O
agents	O
.	O
Such	O
a	O
persona	O
allows	O
producing	O
consistent	O
responses	O
rather	O
than	O
solely	O
relying	O
on	O
the	O
personas	O
that	O
are	O
randomly	O
learned	O
and	O
embedded	O
in	O
the	O
model	O
parameters	O
(	O
Kim	O
et	O
al	O
,	O
2020	O
)	O
.	O
The	O
latter	O
refers	O
to	O
a	O
profile	O
that	O
represents	O
the	O
users	O
.	O
Leveraging	O
such	O
partner	O
personas	O
has	O
been	O
empirically	O
shown	O
to	O
be	O
helpful	O
for	O
dialogue	O
response	O
selection	O
(	O
Gu	O
et	O
al	O
,	O
2021	O
)	O
.	O
Unfortunately	O
,	O
the	O
existence	O
of	O
partner	O
personas	O
suffers	O
from	O
the	O
cold	O
start	O
(	O
Schein	O
et	O
al	O
,	O
2002	O
;	O
Zhang	O
et	O
al	O
,	O
2014	O
;	O
at	O
the	O
beginning	O
of	O
the	O
conversation	O
.	O
Most	O
of	O
the	O
works	O
,	O
if	O
not	O
all	O
,	O
(	O
Li	O
et	O
al	O
,	O
2016b	O
;	O
Mazaré	O
et	O
al	O
,	O
2018	O
;	O
Gu	O
et	O
al	O
,	O
2019	O
;	O
Zhao	O
et	O
al	O
,	O
2019	O
;	O
Madotto	O
et	O
al	O
,	O
2019	O
;	O
Majumder	O
et	O
al	O
,	O
2020	O
;	O
Wu	O
et	O
al	O
,	O
2020a	O
;	O
Song	O
et	O
al	O
,	O
2020	O
)	O
have	O
been	O
either	O
overlooking	O
partner	O
personas	O
or	O
simply	O
focusing	O
on	O
the	O
impractical	O
situation	O
where	O
partner	O
personas	O
guarantee	O
to	O
exist	O
.	O
In	O
contrast	O
,	O
our	O
work	O
does	O
not	O
suffer	O
from	O
the	O
practical	O
issue	O
when	O
partner	O
personas	O
are	O
missing	O
during	O
inference	O
,	O
and	O
our	O
proposed	O
framework	O
surpasses	O
the	O
baseline	O
that	O
conditions	O
on	O
the	O
ground	O
truth	O
partner	O
personas	O
.	O
To	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
attempt	O
to	O
formulate	O
partner	O
personas	O
generation	O
for	O
improved	O
performance	O
on	O
the	O
downstream	O
dialogue	O
response	B-TaskName
generation	I-TaskName
.	O
Our	O
work	O
is	O
motivated	O
by	O
the	O
underlying	O
hypothesis	O
that	O
partner	O
personas	O
generation	O
is	O
plausible	O
given	O
the	O
self	O
personas	O
and	O
dialogue	O
context	O
.	O
Automatic	O
and	O
human	O
evaluation	O
results	O
support	O
the	O
hypothesis	O
and	O
indicate	O
that	O
generated	O
personas	O
are	O
even	O
more	O
interesting	O
than	O
the	O
ground	O
truth	O
,	O
which	O
improves	O
the	O
downstream	O
dialogue	O
response	B-TaskName
generation	I-TaskName
.	O
This	O
paper	O
thus	O
paves	O
the	O
way	O
to	O
exploit	O
partner	O
personas	O
generation	O
(	O
PPG	O
)	O
for	O
dialogue	O
response	B-TaskName
generation	I-TaskName
(	O
DRG	O
)	O
.	O
We	O
propose	O
a	O
novel	O
framework	O
composed	O
of	O
three	O
major	O
components	O
,	O
namely	O
a	O
personas	O
generator	O
,	O
a	O
dialogue	O
response	O
generator	O
and	O
a	O
critic	O
network	O
.	O
The	O
personas	O
generator	O
generates	O
partner	O
personas	O
,	O
which	O
the	O
dialogue	O
response	O
generator	O
conditions	O
on	O
.	O
We	O
employ	O
reinforcement	O
learning	O
with	O
a	O
critic	O
network	O
that	O
propagates	O
the	O
reward	O
back	O
to	O
the	O
generators	O
for	O
joint	O
training	O
.	O
Prior	O
works	O
have	O
investigated	O
partner	O
persona	O
retrieval	O
.	O
The	O
human	O
-	O
constructed	O
ground	O
truth	O
personas	O
serve	O
as	O
the	O
upper	O
bound	O
for	O
such	O
retrieval	O
-	O
based	O
systems	O
,	O
and	O
we	O
argue	O
that	O
the	O
ground	O
truth	O
is	O
not	O
coherent	O
and	O
diverse	O
enough	O
.	O
Interestingly	O
,	O
we	O
observe	O
that	O
the	O
generative	O
counterpart	O
proposed	O
in	O
our	O
framework	O
generates	O
relevant	O
,	O
informative	O
and	O
coherent	O
partner	O
personas	O
,	O
which	O
further	O
improves	O
the	O
succeeding	O
dialogue	O
response	B-TaskName
generation	I-TaskName
.	O
It	O
follows	O
another	O
advantage	O
that	O
our	O
framework	O
does	O
not	O
need	O
an	O
external	O
database	O
to	O
retrieve	O
from	O
(	O
Madotto	O
et	O
al	O
,	O
2020	O
;	O
.	O
One	O
close	O
work	O
to	O
ours	O
is	O
a	O
multi	O
-	O
task	O
framework	O
for	O
meta	B-TaskName
-	I-TaskName
learning	I-TaskName
(	O
Lee	O
et	O
al	O
,	O
2021	O
)	O
that	O
uses	O
personas	O
reconstruction	O
as	O
an	O
auxiliary	O
task	O
to	O
improve	O
response	O
consistency	O
.	O
The	O
differences	O
are	O
that	O
theirs	O
does	O
not	O
differentiate	O
between	O
self	O
personas	O
and	O
partner	O
personas	O
,	O
while	O
ours	O
does	O
.	O
Theirs	O
indicates	O
an	O
improvement	O
over	O
personality	O
consistency	O
,	O
while	O
ours	O
report	O
improvements	O
for	O
the	O
overall	O
quality	O
.	O
We	O
conduct	O
an	O
empirical	O
comparison	O
with	O
their	O
model	O
by	O
reconstructing	O
the	O
partner	O
personas	O
.	O
Experimental	O
results	O
indicate	O
that	O
such	O
a	O
multi	O
-	O
task	O
model	O
does	O
not	O
work	O
well	O
in	O
our	O
problem	O
setting	O
.	O
Very	O
recently	O
,	O
formulates	O
personas	O
generation	O
as	O
a	O
Seq2Seq	B-MethodName
task	O
for	O
improved	O
downstream	O
response	B-TaskName
generation	I-TaskName
via	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
.	O
In	O
contrast	O
,	O
our	O
work	O
leverages	O
reinforcement	O
learning	O
to	O
jointly	O
train	O
the	O
partner	O
personas	O
generator	O
and	O
the	O
response	O
generator	O
.	O
Automatic	O
and	O
human	O
evaluation	O
results	O
indicate	O
that	O
our	O
framework	O
can	O
generate	O
partner	O
personas	O
that	O
are	O
more	O
diverse	O
and	O
interesting	O
than	O
the	O
ground	O
truth	O
partner	O
personas	O
and	O
generate	O
more	O
diverse	O
and	O
engaging	O
responses	O
than	O
the	O
baseline	O
conditioned	O
on	O
ground	O
truth	O
partner	O
personas	O
.	O
1	O
2	O
Related	O
Work	O

Conditioning	O
on	O
personas	O
helps	O
to	O
produce	O
informative	O
and	O
engaging	O
responses	O
.	O
The	O
most	O
wellknown	O
multi	O
-	O
turn	O
dialogue	O
dataset	O
conditioned	O
on	O
personal	O
profiles	O
is	O
PERSONACHAT	O
,	O
in	O
which	O
two	O
crowdsourcers	O
converse	B-DatasetName
and	O
find	O
more	O
about	O
each	O
other	O
.	O
The	O
community	O
has	O
proposed	O
many	O
methods	O
to	O
better	O
utilize	O
self	O
personas	O
.	O
Mazaré	O
et	O
al	O
(	O
2018	O
)	O
employs	O
a	O
pre	O
-	O
training	O
stage	O
based	O
on	O
dedicatedly	O
extracted	O
large	O
-	O
scale	O
persona	O
-	O
based	O
dialogues	O
.	O
Zhao	O
et	O
al	O
(	O
2019	O
)	O
fuses	O
information	O
in	O
personas	O
and	O
dialogue	O
context	O
into	O
individual	O
contextualized	O
representations	O
by	O
attending	O
to	O
different	O
parts	O
of	O
both	O
.	O
Gu	O
et	O
al	O
(	O
2019	O
)	O
exploits	O
the	O
interaction	O
between	O
personas	O
,	O
dialogue	O
context	O
and	O
response	O
to	O
improve	O
retrieval	O
-	O
based	O
dialogue	O
agents	O
.	O
Madotto	O
et	O
al	O
(	O
2019	O
)	O
leverages	O
meta	B-TaskName
-	I-TaskName
learning	I-TaskName
with	O
several	O
dialogues	O
of	O
the	O
current	O
speakers	O
to	O
enhance	O
response	O
personality	O
.	O
Welleck	O
et	O
al	O
(	O
2019	O
)	O
releases	O
a	O
dataset	O
for	O
measuring	O
dialogue	O
consistency	O
.	O
Song	O
et	O
al	O
(	O
2020	O
)	O
employs	O
a	O
multi	O
-	O
stage	O
pipeline	O
to	O
improve	O
response	O
personality	O
by	O
response	O
rewriting	O
.	O
Lee	O
et	O
al	O
(	O
2021	O
)	O
uses	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
for	O
improved	O
personality	O
consistency	O
in	O
the	O
meta	B-TaskName
-	I-TaskName
learning	I-TaskName
scenario	O
.	O
Gu	O
et	O
al	O
(	O
2021	O
)	O
employs	O
four	O
different	O
strategies	O
for	O
personas	O
fusing	O
to	O
leverage	O
both	O
self	O
persona	O
.	O
However	O
,	O
most	O
of	O
these	O
works	O
focus	O
on	O
exploiting	O
self	O
personas	O
rather	O
than	O
partner	O
personas	O
,	O
and	O
they	O
assume	O
the	O
existance	O
of	O
the	O
gold	O
partner	O
personas	O
.	O
Li	O
et	O
al	O
(	O
2014	O
)	O
leverages	O
distant	O
supervision	O
to	O
classify	O
the	O
spouse	O
,	O
education	O
and	O
job	O
information	O
from	O
user	O
twitters	O
.	O
Wu	O
et	O
al	O
(	O
2020b	O
)	O
proposes	O
a	O
twostaged	O
profile	O
extractor	O
that	O
extracts	O
attributes	O
before	O
extracting	O
the	O
underlying	O
relationship	O
.	O
proposes	O
to	O
categorize	O
the	O
profile	O
extraction	O
task	O
into	O
two	O
different	O
difficulties	O
,	O
namely	O
'	O
extraction	O
'	O
and	O
'	O
inference	O
'	O
,	O
and	O
they	O
leverage	O
a	O
GPT	B-MethodName
-	O
based	O
generator	O
to	O
extract	O
user	O
profiles	O
.	O
These	O
works	O
have	O
formulated	O
user	O
profile	O
extraction	O
as	O
a	O
classification	O
task	O
that	O
conditions	O
on	O
an	O
input	O
sentence	O
,	O
and	O
they	O
aim	O
at	O
better	O
profile	O
extraction	O
.	O
In	O
contrast	O
,	O
we	O
propose	O
to	O
formulate	O
personas	O
generation	O
to	O
be	O
conditioned	O
dialogue	O
input	O
to	O
be	O
jointly	O
trained	O
with	O
response	B-TaskName
generation	I-TaskName
.	O
While	O
ground	O
truth	O
personas	O
serve	O
as	O
the	O
upper	O
bound	O
for	O
these	O
user	O
profile	O
extractors	O
,	O
we	O
empirically	O
demonstrate	O
that	O
our	O
reinforcement	O
learning	O
algorithm	O
surpasses	O
the	O
response	O
model	O
conditioned	O
on	O
the	O
ground	O
truth	O
partner	O
personas	O
.	O
As	O
supported	O
by	O
our	O
human	O
evaluation	O
,	O
we	O
believe	O
the	O
underlying	O
reason	O
is	O
that	O
our	O
model	O
can	O
leverage	O
pre	O
-	O
trained	O
generators	O
to	O
generate	O
coherent	O
and	O
relevant	O
partner	O
personas	O
.	O

We	O
propose	O
a	O
novel	O
framework	O
composed	O
of	O
three	O
major	O
components	O
,	O
namely	O
a	O
partner	O
personas	O
generator	O
,	O
a	O
dialogue	O
response	O
generator	O
and	O
a	O
reinforcement	O
learning	O
component	O
with	O
a	O
critic	O
network	O
.	O
Figure	O
1	O
depicts	O
the	O
inference	O
flow	O
of	O
our	O
setting	O
.	O
The	O
input	O
dialogue	O
context	O
with	O
self	O
persona	O
is	O
first	O
fed	O
into	O
the	O
partner	O
personas	O
generator	O
.	O
The	O
generated	O
partner	O
personas	O
output	O
is	O
then	O
concatenated	O
with	O
the	O
dialogue	O
context	O
and	O
the	O
self	O
personas	O
as	O
the	O
input	O
into	O
the	O
dialogue	O
response	O
generator	O
.	O
In	O
the	O
beginning	O
,	O
we	O
train	O
our	O
partner	O
personas	O
generator	O
and	O
dialogue	O
response	O
generator	O
under	O
supervised	O
learning	O
.	O
In	O
the	O
training	O
stage	O
,	O
we	O
use	O
the	O
ground	O
truth	O
partner	O
personas	O
to	O
train	O
the	O
dialogue	O
response	O
generator	O
,	O
and	O
we	O
replace	O
it	O
with	O
generated	O
partner	O
personas	O
in	O
the	O
inference	O
stage	O
.	O
After	O
the	O
supervised	O
learning	O
stage	O
,	O
the	O
second	O
stage	O
is	O
a	O
reinforcement	O
learning	O
stage	O
which	O
jointly	O
optimizes	O
both	O
partner	O
personas	O
generator	O
and	O
dialogue	O
response	O
generator	O
as	O
depicted	O
in	O
Figure	O
2	O
to	O
train	O
the	O
partner	O
personas	O
generator	O
under	O
the	O
reward	O
signal	O
that	O
is	O
relevant	O
to	O
dialogue	O
response	B-TaskName
generation	I-TaskName
as	O
well	O
as	O
fine	O
-	O
tuning	O
dialogue	O
response	O
generator	O
trained	O
on	O
the	O
generated	O
partner	O
personas	O
.	O
2	O
Particularly	O
,	O
we	O
employ	O
a	O
dedicatedly	O
designed	O
critic	O
network	O
that	O
receives	O
generated	O
partner	O
personas	O
and	O
generated	O
dialogue	O
responses	O
as	O
the	O
input	O
and	O
output	O
a	O
reward	O
that	O
measures	O
the	O
relevance	O
between	O
the	O
generated	O
personas	O
and	O
responses	O
and	O
propagates	O
back	O
to	O
the	O
generators	O
.	O

A	O
Seq2Seq	B-MethodName
neural	O
network	O
(	O
Sutskever	O
et	O
al	O
,	O
2014	O
)	O
is	O
adopted	O
as	O
our	O
partner	O
personas	O
generator	O
for	O
the	O
task	O
of	O
partner	O
personas	O
generation	O
(	O
PPG	O
)	O
.	O
The	O
concatenation	O
of	O
dialogue	O
context	O
c	O
and	O
self	O
personas	O
s	O
is	O
fed	O
as	O
an	O
input	O
into	O
the	O
partner	O
personas	O
generator	O
.	O
The	O
personas	O
generator	O
then	O
outputs	O
an	O
approximated	O
partner	O
personasp	O
conditioned	O
on	O
the	O
input	O
that	O
maximises	O
the	O
following	O
likelihood	O
:	O
P	O
(	O
p	O
|	O
s	O
,	O
c	O
)	O
=	O
T	O
t=1	O
P	O
(	O
p	O
t	O
|	O
p	O
1	O
,	O
...	O
,	O
p	O
t−1	O
,	O
s	O
,	O
c	O
)	O
,	O
where	O
T	O
represents	O
the	O
length	O
of	O
the	O
generated	O
partner	O
personas	O
andp	O
t	O
represents	O
the	O
word	O
at	O
the	O
position	O
t	O
that	O
has	O
been	O
inferenced	O
.	O
For	O
training	O
,	O
the	O
ground	O
truth	O
partner	O
personas	O
p	O
is	O
used	O
and	O
we	O
train	O
our	O
generator	O
to	O
maximise	O
the	O
likelihood	O
P	O
(	O
p	O
|	O
s	O
,	O
c	O
)	O
.	O
We	O
generate	O
the	O
complete	O
partner	O
personas	O
profiles	O
in	O
an	O
one	O
-	O
off	O
shot	O
for	O
all	O
the	O
dialogue	O
samples	O
.	O

We	O
also	O
adopt	O
a	O
Seq2Seq	B-MethodName
neural	O
network	O
for	O
the	O
task	O
of	O
dialogue	O
response	B-TaskName
generation	I-TaskName
(	O
DRG	O
)	O
.	O
During	O
inference	O
,	O
the	O
concatenation	O
of	O
dialogue	O
context	O
c	O
,	O
self	O
personas	O
s	O
,	O
and	O
generated	O
partner	O
personasp	O
is	O
fed	O
as	O
an	O
input	O
into	O
the	O
dialogue	O
response	O
generator	O
.	O
The	O
response	O
generator	O
then	O
outputs	O
a	O
dialogue	O
responser	O
conditioned	O
on	O
the	O
input	O
,	O
which	O
maximises	O
the	O
conditional	O
likelihood	O
:	O
P	O
(	O
r	O
|	O
s	O
,	O
p	O
,	O
c	O
)	O
.	O
For	O
training	O
,	O
the	O
ground	O
truth	O
partner	O
personas	O
p	O
and	O
the	O
ground	O
truth	O
dialogue	O
responses	O
r	O
are	O
used	O
.	O

We	O
build	O
our	O
baselines	O
,	O
the	O
partner	O
personas	O
generator	O
and	O
the	O
dialogue	O
response	O
generator	O
based	O
on	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
pre	O
-	O
trained	O
dialogue	O
model	O
DIALOGPT	O
for	O
parameters	O
Table	O
2	O
:	O
Case	O
studies	O
that	O
compare	O
our	O
framework	O
against	O
the	O
baseline	O
with	O
the	O
complete	O
partner	O
personas	O
as	O
well	O
as	O
the	O
human	O
response	O
.	O
We	O
present	O
the	O
preceding	O
partner	O
utterance	O
as	O
dialogue	O
context	O
,	O
and	O
we	O
give	O
the	O
most	O
salient	O
ground	O
truth	O
partner	O
personas	O
(	O
Gold	O
Partner	O
)	O
and	O
generated	O
partner	O
personas	O
(	O
Generated	O
Partner	O
)	O
for	O
clarity	O
.	O
initialization	O
.	O
More	O
implementation	O
details	O
can	O
be	O
found	O
in	O
Appendx	O
B.	O
The	O
dialogue	O
response	B-TaskName
generation	I-TaskName
results	O
are	O
presented	O
in	O
Table	O
1	O
.	O
Our	O
framework	O
with	O
reinforcement	O
learning	O
attains	O
the	O
best	O
over	O
all	O
the	O
metrics	O
on	O
both	O
PERSONACHAT	O
-	O
ORI	O
and	O
PERSONACHAT	O
-	O
REV	O
.	O
This	O
supports	O
the	O
usefulness	O
of	O
our	O
framework	O
,	O
which	O
generates	O
reasonable	O
personas	O
and	O
effectively	O
enhances	O
the	O
succeeding	O
dialogue	O
response	B-TaskName
generation	I-TaskName
,	O
through	O
the	O
use	O
of	O
RL	O
.	O
Although	O
TRANSFERTRANSFO	O
attains	O
a	O
better	O
score	O
on	O
the	O
PPL	O
than	O
the	O
fine	O
-	O
tuned	O
GPT	B-MethodName
-	O
2	O
,	O
GPT	B-MethodName
-	O
2	O
have	O
better	O
extrinsic	O
scores	O
than	O
TRANSFER	O
-	O
TRANSFO	O
.	O
GPT	B-MethodName
-	O
2	O
also	O
has	O
better	O
overall	O
scores	O
than	O
the	O
E2E	B-DatasetName
baselines	O
without	O
the	O
complete	O
partner	O
personas	O
.	O
However	O
,	O
it	O
is	O
surpassed	O
by	O
the	O
E2E	B-DatasetName
baseline	O
with	O
the	O
complete	O
partner	O
personas	O
during	O
training	O
and	O
inference	O
.	O
The	O
E2E	B-DatasetName
baseline	O
with	O
the	O
complete	O
ground	O
truth	O
partner	O
personas	O
attains	O
better	O
scores	O
on	O
all	O
of	O
the	O
metrics	O
than	O
our	O
remaining	O
baselines	O
.	O
Our	O
framework	O
with	O
RL	O
succeeds	O
the	O
performance	O
of	O
such	O
a	O
competitive	O
baseline	O
for	O
both	O
PERSONACHAT	O
-	O
ORI	O
and	O
PERSONACHAT	O
-	O
REV	O
,	O
indicating	O
our	O
proposed	O
framework	O
's	O
robustness	O
against	O
paraphrasal	O
.	O
The	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
comparison	O
model	O
(	O
Lee	O
et	O
al	O
,	O
2021	O
)	O
produces	O
less	O
promising	O
results	O
.	O
Concretely	O
,	O
we	O
postulate	O
that	O
the	O
nature	O
of	O
PPG	O
and	O
DRG	O
largely	O
differs	O
.	O
The	O
textual	O
format	O
of	O
partner	O
personas	O
always	O
initiates	O
with	O
first	O
-	O
person	O
sentence	O
starters	O
,	O
while	O
dialogue	O
responses	O
are	O
more	O
general	O
,	O
ranging	O
from	O
greetings	O
to	O
goodbyes	O
.	O
Therefore	O
,	O
it	O
could	O
be	O
hard	O
to	O
capture	O
both	O
in	O
a	O
single	O
model	O
.	O

Table	O
4	O
presents	O
the	O
quality	O
measurements	O
of	O
the	O
generated	O
partner	O
personas	O
from	O
our	O
PPG	O
with	O
no	O
RL	O
.	O
We	O
observe	O
that	O
our	O
models	O
have	O
much	O
higher	O
Distinct	O
-	O
N	O
scores	O
as	O
the	O
number	O
of	O
unique	O
words	O
in	O
the	O
generated	O
output	O
is	O
much	O
higher	O
than	O
the	O
ground	O
truth	O
test	O
personas	O
.	O
Compared	O
to	O
the	O
ground	O
truth	O
personas	O
that	O
are	O
limited	O
sets	O
of	O
traits	O
,	O
our	O
generator	O
can	O
leverage	O
the	O
power	O
of	O
pre	O
-	O
trained	O
models	O
for	O
better	O
diversity	O
.	O
The	O
remaining	O
metrics	O
also	O
report	O
reasonable	O
scores	O
,	O
suggesting	O
the	O
plausbility	O
to	O
formulate	O
personas	O
generation	O
as	O
a	O
Seq2Seq	B-MethodName
task	O
.	O

We	O
hired	O
experienced	O
annotators	O
who	O
have	O
degrees	O
relevant	O
to	O
English	O
Linguistics	O
to	O
conduct	O
evaluation	O
on	O
PERSONACHAT	O
-	O
ORI	O
.	O
For	O
both	O
DRG	O
and	O
PPG	O
,	O
we	O
present	O
a	O
questionnaire	O
composed	O
of	O
800	O
questions	O
with	O
randomly	O
sampled	O
200	O
test	O
instances	O
to	O
three	O
annotators	O
who	O
compare	O
model	O
outputs	O
under	O
A	O
/	O
B	O
testing	O
.	O
As	O
in	O
Zou	O
et	O
al	O
(	O
2021	O
)	O
and	O
ACUTE	O
-	O
Evals	O
(	O
Li	O
et	O
al	O
,	O
2020	O
)	O
,	O
annotators	O
follow	O
the	O
criteria	O
which	O
we	O
present	O
in	O
Appendix	O
D.	O
trained	O
under	O
RL	O
surpasses	O
the	O
E2E	B-DatasetName
model	O
that	O
leverages	O
both	O
training	O
and	O
inference	O
ground	O
truth	O
partner	O
personas	O
from	O
all	O
the	O
aspects	O
.	O
Table	O
6	O
presents	O
the	O
human	O
evaluation	O
results	O
on	O
PPG	O
.	O
We	O
observe	O
that	O
our	O
PPG	O
generates	O
personas	O
that	O
are	O
more	O
coherent	O
and	O
interesting	O
than	O
the	O
ground	O
truth	O
,	O
which	O
align	O
with	O
the	O
facts	O
observed	O
in	O
Section	O
5.4	O
and	O
Section	O
5.5	O
indicating	O
that	O
our	O
generated	O
partner	O
personas	O
are	O
more	O
coherent	O
and	O
diverse	O
.	O

We	O
conduct	O
an	O
ablation	O
study	O
on	O
PERSONACHAT	O
-	O
ORI	O
as	O
reported	O
in	O
Table	O
7	O
to	O
present	O
the	O
performance	O
of	O
our	O
framework	O
when	O
one	O
of	O
the	O
components	O
is	O
frozen	O
during	O
RL	O
.	O
The	O
result	O
indicates	O
that	O
our	O
proposed	O
framework	O
yields	O
the	O
best	O
result	O
when	O
both	O
of	O
the	O
components	O
are	O
actively	O
trained	O
under	O
RL	O
.	O
We	O
also	O
notice	O
that	O
scaling	O
the	O
RL	O
reward	O
for	O
either	O
PPG	O
or	O
DRG	O
by	O
10	O
leads	O
to	O
minor	O
decrease	O
in	O
the	O
performance	O
.	O
Further	O
scaling	O
deteriorates	O
the	O
quality	O
of	O
response	B-TaskName
generation	I-TaskName
.	O

Our	O
novel	O
framework	O
incorporates	O
partner	O
personas	O
generation	O
into	O
dialogue	O
response	B-TaskName
generation	I-TaskName
.	O
It	O
effectively	O
mitigates	O
the	O
problem	O
that	O
partner	O
personas	O
are	O
not	O
available	O
in	O
practical	O
applications	O
as	O
well	O
as	O
the	O
cold	O
start	O
problem	O
during	O
early	O
conversation	O
.	O
The	O
experimental	O
results	O
with	O
both	O
automatic	O
and	O
human	O
evaluation	O
demonstrate	O
that	O
our	O
framework	O
generates	O
coherent	O
,	O
diverse	O
,	O
interesting	O
and	O
engaging	O
partner	O
personas	O
,	O
even	O
compared	O
to	O
the	O
ground	O
truth	O
partner	O
personas	O
.	O
We	O
employ	O
reinforcement	O
learning	O
with	O
a	O
dedicatedly	O
designed	O
critic	O
network	O
that	O
boosts	O
the	O
response	B-TaskName
generation	I-TaskName
by	O
conditioning	O
on	O
the	O
generated	O
personas	O
.	O
Automatic	O
and	O
human	O
evaluation	O
results	O
indicate	O
that	O
our	O
response	O
generator	O
surpasses	O
our	O
competitive	O
baselines	O
that	O
condition	O
on	O
the	O
ground	O
truth	O
partner	O
personas	O
.	O
Extensive	O
case	O
studies	O
demonstrate	O
that	O
our	O
framework	O
can	O
generate	O
satisfying	O
dialogue	O
responses	O
and	O
partner	O
personas	O
.	O

Our	O
work	O
uses	O
an	O
off	O
-	O
the	O
-	O
shelf	O
persona	O
-	O
based	O
conversational	O
dataset	O
PERSONACHAT	O
,	O
which	O
is	O
collected	O
and	O
built	O
by	O
crowdsourcing	O
to	O
converse	B-DatasetName
based	O
on	O
a	O
fake	O
set	O
of	O
discrete	O
traits	O
.	O
There	O
is	O
no	O
personal	O
information	O
and	O
hence	O
no	O
ethics	O
concern	O
,	O
but	O
this	O
might	O
result	O
in	O
limited	O
usefulness	O
as	O
there	O
could	O
be	O
discrepancies	O
between	O
the	O
collected	O
samples	O
and	O
real	O
-	O
life	O
conversation	O
.	O
It	O
is	O
also	O
more	O
expensive	O
to	O
collect	O
real	O
data	O
.	O
However	O
,	O
PERSONACHAT	O
has	O
been	O
widely	O
used	O
by	O
the	O
community	O
as	O
a	O
standard	O
dataset	O
.	O
Many	O
well	O
-	O
known	O
persona	O
-	O
based	O
datasets	O
suffer	O
from	O
the	O
same	O
problem	O
(	O
Urbanek	O
et	O
al	O
,	O
2019	O
)	O
as	O
widely	O
known	O
.	O
Although	O
Mazaré	O
et	O
al	O
(	O
2018	O
)	O
proposed	O
a	O
useful	O
method	O
to	O
collect	O
large	O
-	O
scale	O
persona	O
-	O
based	O
dialogue	O
datasets	O
by	O
extracting	O
persona	O
from	O
user	O
comments	O
with	O
classifiers	O
trained	O
on	O
revised	O
personas	O
from	O
PERSONACHAT	O
which	O
can	O
improve	O
the	O
model	O
performance	O
on	O
PERSONACHAT	O
.	O
For	O
legal	O
reasons	O
,	O
they	O
did	O
not	O
release	O
this	O
dataset	O
at	O
the	O
time	O
of	O
writing	O
.	O
Similarly	O
,	O
Zheng	O
et	O
al	O
(	O
2019	O
)	O
proposed	O
a	O
persona	O
-	O
based	O
dialogue	O
dataset	O
with	O
diversified	O
traits	O
,	O
but	O
it	O
is	O
not	O
currently	O
online	O
readily	O
available	O
.	O
Zhong	O
et	O
al	O
(	O
2020	O
)	O
has	O
followed	O
the	O
approach	O
suggested	O
by	O
Mazaré	O
et	O
al	O
(	O
2018	O
)	O
to	O
build	O
an	O
empathetic	O
conversation	O
dataset	O
based	O
on	O
personas	O
.	O
8	O
:	O
More	O
generated	O
personas	O
.	O
We	O
highlight	O
in	O
pink	O
for	O
informativeness	O
and	O
in	O
yellow	O
for	O
coherence	O
.	O
However	O
,	O
their	O
main	O
focus	O
is	O
to	O
investigate	O
the	O
impact	O
of	O
personas	O
on	O
empathetic	O
dialogue	B-TaskName
generation	I-TaskName
.	O
Therefore	O
,	O
we	O
choose	O
to	O
follow	O
the	O
community	O
to	O
investigate	O
our	O
method	O
on	O
the	O
most	O
well	O
-	O
known	O
dataset	O
,	O
PERSONACHAT	O
.	O

Active	B-TaskName
Learning	I-TaskName
via	O
Membership	O
Query	O
Synthesis	O
for	O
Semi	O
-	O
supervised	O
Sentence	B-TaskName
Classification	I-TaskName

Active	B-TaskName
learning	I-TaskName
(	O
AL	O
)	O
is	O
a	O
technique	O
for	O
reducing	O
manual	O
annotation	O
effort	O
during	O
the	O
annotation	O
of	O
training	O
data	O
for	O
machine	O
learning	O
classifiers	O
.	O
For	O
NLP	O
tasks	O
,	O
pool	O
-	O
based	O
and	O
stream	O
-	O
based	O
sampling	O
techniques	O
have	O
been	O
used	O
to	O
select	O
new	O
instances	O
for	O
AL	O
while	O
generating	O
new	O
,	O
artificial	O
instances	O
via	O
Membership	O
Query	O
Synthesis	O
was	O
,	O
up	O
to	O
know	O
,	O
considered	O
to	O
be	O
infeasible	O
for	O
NLP	O
problems	O
.	O
We	O
present	O
the	O
first	O
successful	O
attempt	O
to	O
use	O
Membership	O
Query	O
Synthesis	O
for	O
generating	O
AL	O
queries	O
for	O
natural	O
language	O
processing	O
,	O
using	O
Variational	O
Autoencoders	B-MethodName
for	O
query	O
generation	O
.	O
We	O
evaluate	O
our	O
approach	O
in	O
a	O
text	B-TaskName
classification	I-TaskName
task	O
and	O
demonstrate	O
that	O
query	O
synthesis	O
shows	O
competitive	O
performance	O
to	O
pool	O
-	O
based	O
AL	O
strategies	O
while	O
substantially	O
reducing	O
annotation	O
time	O
.	O

Active	B-TaskName
learning	I-TaskName
(	O
AL	O
)	O
has	O
the	O
potential	O
to	O
substantially	O
reduce	O
the	O
amount	O
of	O
labeled	O
instances	O
needed	O
to	O
reach	O
a	O
certain	O
classifier	O
performance	O
in	O
supervised	O
machine	O
learning	O
.	O
It	O
works	O
by	O
selecting	O
new	O
instances	O
that	O
are	O
highly	O
informative	O
for	O
the	O
classifier	O
,	O
so	O
that	O
comparable	O
classification	O
accuracies	O
can	O
be	O
obtained	O
on	O
a	O
much	O
smaller	O
training	O
set	O
.	O
AL	O
strategies	O
can	O
be	O
categorized	O
into	O
pool	O
-	O
based	O
sampling	O
,	O
stream	O
-	O
based	O
sampling	O
and	O
Membership	O
Query	O
Synthesis	O
(	O
MQS	O
)	O
.	O
The	O
first	O
two	O
strategies	O
sample	O
new	O
instances	O
either	O
from	O
a	O
data	O
pool	O
or	O
from	O
a	O
stream	O
of	O
data	O
.	O
The	O
third	O
,	O
MQS	O
,	O
generates	O
artificial	O
AL	O
instances	O
from	O
the	O
region	O
of	O
uncertainty	O
of	O
the	O
classifier	O
.	O
While	O
it	O
is	O
known	O
that	O
MQS	O
can	O
reduce	O
the	O
predictive	O
error	O
rate	O
more	O
quickly	O
than	O
pool	O
-	O
based	O
sampling	O
(	O
Ling	O
and	O
Du	O
,	O
2008	O
)	O
,	O
so	O
far	O
it	O
has	O
not	O
been	O
used	O
for	O
NLP	O
tasks	O
because	O
artificially	O
created	O
textual	O
instances	O
are	O
uninterpretable	O
for	O
human	O
annotators	O
.	O
We	O
provide	O
proof	O
of	O
concept	O
that	O
generating	O
highly	O
informative	O
artificial	O
training	O
instances	O
for	O
text	B-TaskName
classification	I-TaskName
is	O
feasible	O
.	O
We	O
use	O
Variational	O
Autoencoders	B-MethodName
(	O
VAE	B-MethodName
)	O
(	O
Kingma	O
and	O
Welling	O
,	O
2013	O
)	O
to	O
learn	O
representations	O
from	O
unlabeled	O
text	O
in	O
an	O
unsupervised	O
fashion	O
by	O
encoding	O
individual	O
sentences	O
as	O
low	O
-	O
dimensional	O
vectors	O
in	O
latent	O
space	O
.	O
In	O
addition	O
to	O
mapping	O
input	O
sequences	O
into	O
latent	O
space	O
,	O
the	O
VAE	B-MethodName
can	O
also	O
learn	O
to	O
generate	O
new	O
instances	O
from	O
this	O
space	O
.	O
We	O
utilize	O
these	O
abilities	O
to	O
generate	O
new	O
examples	O
for	O
active	B-TaskName
learning	I-TaskName
from	O
a	O
region	O
in	O
latent	O
space	O
where	O
the	O
classifier	O
is	O
most	O
uncertain	O
,	O
and	O
hand	O
them	O
over	O
to	O
the	O
annotator	O
who	O
then	O
provides	O
labels	O
for	O
the	O
newly	O
created	O
instances	O
.	O
We	O
test	O
our	O
approach	O
in	O
a	O
text	B-TaskName
classification	I-TaskName
setup	O
with	O
a	O
real	O
human	O
annotator	O
in	O
the	O
loop	O
.	O
Our	O
experiments	O
show	O
that	O
query	O
synthesis	O
for	O
NLP	O
is	O
not	O
only	O
feasible	O
but	O
can	O
outperform	O
other	O
AL	O
strategies	O
in	O
a	O
sentiment	O
classification	O
task	O
with	O
respect	O
to	O
annotation	O
time	O
.	O
The	O
paper	O
is	O
structured	O
as	O
follows	O
.	O
We	O
first	O
review	O
related	O
work	O
(	O
2	O
)	O
and	O
introduce	O
a	O
formal	O
description	O
of	O
the	O
problem	O
(	O
3	O
)	O
.	O
Then	O
we	O
describe	O
our	O
approach	O
(	O
4	O
)	O
,	O
present	O
the	O
experiments	O
(	O
5	O
)	O
and	O
analyze	O
the	O
results	O
(	O
6	O
)	O
.	O
We	O
discuss	O
limitations	O
and	O
possible	O
further	O
experiments	O
(	O
7	O
)	O
and	O
finally	O
conclude	O
our	O
findings	O
(	O
8	O
)	O
.	O

Membership	O
query	O
synthesis	O
was	O
introduced	O
by	O
Angluin	O
(	O
1988	O
)	O
and	O
describes	O
a	O
setting	O
where	O
the	O
model	O
generates	O
new	O
queries	O
instead	O
of	O
selecting	O
existing	O
ones	O
.	O
Early	O
experiments	O
in	O
image	O
processing	O
(	O
Lang	O
and	O
Baum	O
,	O
1992	O
)	O
,	O
however	O
,	O
showed	O
that	O
the	O
generated	O
queries	O
are	O
hard	O
to	O
interpret	O
by	O
human	O
annotators	O
.	O
This	O
holds	O
true	O
even	O
for	O
recent	O
approaches	O
using	O
Generative	O
Adversarial	O
Networks	O
(	O
GANs	O
)	O
(	O
Goodfellow	O
et	O
al	O
,	O
2014	O
)	O
to	O
create	O
uncertain	O
instances	O
(	O
Zhu	O
and	O
Bento	O
,	O
2017	O
;	O
Huijser	O
and	O
van	O
Gemert	O
,	O
2017	O
)	O
.	O
In	O
contrast	O
to	O
i	O
m	O
-	O
age	O
processing	O
,	O
discrete	O
domains	O
like	O
natural	O
language	O
do	O
not	O
exhibit	O
a	O
direct	O
mapping	O
from	O
feature	O
to	O
instance	O
space	O
.	O
Strategies	O
that	O
circumvent	O
this	O
problem	O
include	O
the	O
search	O
for	O
nearest	O
(	O
observed	O
)	O
neighbors	O
in	O
feature	O
space	O
(	O
Wang	O
et	O
al	O
,	O
2015	O
)	O
or	O
crafting	O
queries	O
by	O
switching	O
words	O
(	O
Awasthi	O
and	O
Kanade	O
,	O
2012	O
)	O
.	O
Sentence	O
representation	B-TaskName
learning	I-TaskName
(	O
Kiros	O
et	O
al	O
,	O
2015	O
;	O
Conneau	O
et	O
al	O
,	O
2017	O
;	O
Subramanian	O
et	O
al	O
,	O
2018	O
;	O
in	O
combination	O
with	O
new	O
methods	O
for	O
semi	O
-	O
supervised	O
learning	O
Hu	O
et	O
al	O
,	O
2017	O
;	O
Xu	O
et	O
al	O
,	O
2017	O
;	O
Odena	O
,	O
2016	O
;	O
Radford	O
et	O
al	O
,	O
2017	O
)	O
have	O
shown	O
to	O
improve	O
classification	O
tasks	O
by	O
leveraging	O
unlabeled	O
text	O
.	O
Methods	O
based	O
on	O
deep	O
generative	O
models	O
like	O
GANs	O
or	O
VAEs	O
are	O
able	O
to	O
generate	O
sentences	O
from	O
any	O
point	O
in	O
representation	O
space	O
.	O
Mehrjou	O
et	O
al	O
(	O
2018	O
)	O
use	O
VAEs	O
to	O
learn	O
structural	O
information	O
from	O
unlabeled	O
data	O
and	O
use	O
it	O
as	O
an	O
additional	O
criterion	O
in	O
conventional	O
active	B-TaskName
learning	I-TaskName
to	O
make	O
it	O
more	O
robust	O
against	O
outliers	O
and	O
noise	O
.	O
We	O
use	O
VAEs	O
to	O
generate	O
AL	O
queries	O
from	O
specific	O
regions	O
in	O
latent	O
space	O
.	O
To	O
ensure	O
that	O
the	O
generated	O
instances	O
are	O
not	O
only	O
informative	O
for	O
the	O
ML	O
classifier	O
but	O
also	O
meaningful	O
for	O
the	O
human	O
annotator	O
,	O
we	O
adapt	O
the	O
approach	O
of	O
Wang	O
et	O
al	O
(	O
2015	O
)	O
(	O
see	O
3.1	O
)	O
.	O
In	O
contrast	O
to	O
their	O
work	O
,	O
however	O
,	O
we	O
do	O
not	O
sample	O
existing	O
instances	O
from	O
the	O
pool	O
that	O
are	O
similar	O
to	O
the	O
synthetic	O
ones	O
but	O
directly	O
generate	O
the	O
new	O
queries	O
.	O
To	O
our	O
best	O
knowledge	O
,	O
our	O
work	O
is	O
the	O
first	O
to	O
present	O
positive	O
results	O
for	O
Membership	O
Query	O
Synthesis	O
for	O
text	B-TaskName
classification	I-TaskName
.	O

Arbitrary	O
points	O
in	O
feature	O
space	O
are	O
hard	O
to	O
interpret	O
for	O
humans	O
.	O
To	O
evade	O
this	O
problem	O
,	O
Wang	O
et	O
al	O
(	O
2015	O
)	O
use	O
the	O
nearest	O
neighbor	O
in	O
a	O
pool	O
of	O
unlabeled	O
data	O
as	O
a	O
representative	O
which	O
is	O
then	O
presented	O
to	O
the	O
human	O
annotator	O
.	O
To	O
identify	O
uncertain	O
points	O
along	O
the	O
separating	O
hyperplane	O
of	O
an	O
SVM	B-MethodName
the	O
following	O
approach	O
is	O
proposed	O
.	O
First	O
the	O
location	O
of	O
the	O
decision	O
boundary	O
is	O
approximated	O
by	O
a	O
binary	O
-	O
search	O
like	O
procedure	O
.	O
An	O
initial	O
Opposite	O
Pair	O
(	O
z	O
+	O
,	O
z	O
−	O
)	O
is	O
formed	O
by	O
centroid	O
c	O
+	O
and	O
centroid	O
c	O
−	O
of	O
positive	O
and	O
negative	O
labeled	O
instances	O
respectively	O
.	O
The	O
mid	O
point	O
z	O
s	O
is	O
queried	O
and	O
,	O
depending	O
on	O
the	O
annotated	O
label	O
l	O
,	O
replaces	O
the	O
corresponding	O
z	O
l	O
.	O
This	O
step	O
is	O
repeated	O
b	O
times	O
,	O
reducing	O
the	O
distance	O
between	O
the	O
initial	O
centroids	O
by	O
a	O
factor	O
of	O
2	O
b	O
.	O
Figure	O
1a	O
depicts	O
this	O
process	O
.	O
Then	O
the	O
mid	O
-	O
perpendicular	O
vector	O
of	O
the	O
Opposite	O
Pair	O
is	O
calculated	O
by	O
using	O
the	O
Gram	O
-	O
Schmidt	O
process	O
to	O
orthogonalize	O
a	O
random	O
vector	O
z	O
r	O
and	O
normalize	O
its	O
magnitude	O
to	O
λ	O
.	O
The	O
new	O
point	O
z	O
s	O
=	O
z	O
r	O
+	O
(	O
z	O
+	O
+	O
z	O
−	O
)	O
/2	O
is	O
close	O
to	O
the	O
decision	O
boundary	O
and	O
queried	O
for	O
its	O
class	O
.	O
Depending	O
on	O
the	O
receive	O
label	O
the	O
point	O
z	O
s	O
replaces	O
z	O
+	O
or	O
z	O
−	O
in	O
the	O
Opposite	O
Pair	O
.	O
This	O
process	O
(	O
Figure	O
1b	O
)	O
is	O
repeated	O
until	O
n	O
−	O
b	O
points	O
along	O
the	O
separating	O
hyperplane	O
are	O
queried	O
.	O

We	O
train	O
a	O
Variational	B-MethodName
Autoencoder	I-MethodName
on	O
an	O
unlabeled	O
corpus	O
of	O
sentences	O
.	O
The	O
text	B-TaskName
classification	I-TaskName
task	O
is	O
performed	O
on	O
a	O
binary	O
sentiment	O
dataset	O
split	O
into	O
training	O
,	O
development	O
and	O
test	O
set	O
.	O
As	O
depicted	O
in	O
Figure	O
2	O
,	O
the	O
sentences	O
in	O
the	O
classification	O
dataset	O
are	O
vectorized	O
using	O
the	O
VAE	B-MethodName
encoder	O
which	O
generates	O
the	O
latent	O
variable	O
z	O
for	O
each	O
sentence	O
x.	O
This	O
is	O
done	O
deterministically	O
by	O
dropping	O
the	O
σ	O
term	O
in	O
Equation	O
1	O
,	O
further	O
referred	O
to	O
as	O
z	O
=	O
enc	O
(	O
x	O
)	O
.	O
Next	O
,	O
a	O
Learner	O
is	O
trained	O
to	O
fit	O
a	O
linear	O
hyperplane	O
to	O
separate	O
the	O
positive	O
from	O
the	O
negative	O
instances	O
.	O
We	O
use	O
the	O
procedure	O
described	O
in	O
3.1	O
to	O
select	O
new	O
query	O
points	O
for	O
AL	O
.	O
But	O
instead	O
of	O
searching	O
for	O
the	O
nearest	O
neighbor	O
in	O
the	O
pool	O
,	O
we	O
decode	O
the	O
point	O
x	O
=	O
dec	O
(	O
z	O
)	O
into	O
a	O
human	O
readable	O
sentence	O
which	O
is	O
then	O
handed	O
over	O
to	O
the	O
human	O
annotator	O
.	O
The	O
annotator	O
assigns	O
a	O
binary	O
label	O
to	O
the	O
instance	O
and	O
the	O
next	O
query	O
point	O
is	O
calculated	O
.	O
One	O
important	O
parameter	O
for	O
active	B-TaskName
learning	I-TaskName
determines	O
how	O
many	O
new	O
instances	O
are	O
to	O
be	O
selected	O
in	O
each	O
AL	O
iteration	O
.	O
Wang	O
et	O
al	O
(	O
2015	O
)	O
use	O
a	O
predefined	O
number	O
of	O
instances	O
to	O
be	O
selected	O
along	O
the	O
hyperplane	O
.	O
Because	O
we	O
know	O
that	O
a	O
Gaussian	O
prior	O
is	O
imposed	O
on	O
the	O
feature	O
space	O
,	O
we	O
instead	O
stop	O
the	O
selection	O
process	O
when	O
the	O
magnitude	O
of	O
z	O
s	O
exceeds	O
the	O
expectation	O
.	O
The	O
expected	O
distance	O
of	O
a	O
point	O
sampled	O
from	O
the	O
k	O
-	O
dimensional	O
Gaussian	O
prior	O
to	O
the	O
origin	O
is	O
E	O
[	O
χ	O
2	O
k	O
]	O
=	O
√	O
k.	O
Then	O
the	O
schedule	O
restarts	O
,	O
learning	O
a	O
new	O
decision	O
boundary	O
,	O
and	O
ultimately	O
ter	O
-	O
minates	O
when	O
the	O
annotation	O
budget	O
is	O
exhausted	O
.	O
We	O
refer	O
to	O
this	O
method	O
as	O
gen	O
wang	O
.	O
When	O
nearest	O
neighbor	O
search	O
is	O
used	O
instead	O
,	O
we	O
refer	O
to	O
the	O
selection	O
method	O
as	O
nn	O
wang	O
.	O
In	O
addition	O
,	O
we	O
explore	O
a	O
method	O
,	O
gen	O
uniform	O
,	O
where	O
step	O
b	O
)	O
in	O
Figure	O
1	O
is	O
reduced	O
to	O
generating	O
only	O
one	O
midperpendicular	O
vector	O
with	O
a	O
magnitude	O
drawn	O
from	O
a	O
uniform	O
distribution	O
.	O
In	O
each	O
iteration	O
this	O
vector	O
will	O
point	O
to	O
a	O
random	O
direction	O
with	O
a	O
different	O
magnitude	O
,	O
selecting	O
diverse	O
points	O
close	O
to	O
the	O
hyperplane	O
.	O
The	O
maximum	O
magnitude	O
is	O
set	O
in	O
a	O
way	O
that	O
the	O
resulting	O
point	O
is	O
not	O
further	O
away	O
than	O
√	O
k	O
from	O
the	O
origin	O
.	O
Similar	O
to	O
above	O
we	O
refer	O
to	O
this	O
method	O
as	O
nn	O
uniform	O
when	O
using	O
nearest	O
neighbor	O
search	O
.	O
The	O
number	O
of	O
possible	O
directions	O
along	O
the	O
hyperplane	O
grows	O
with	O
the	O
size	O
of	O
the	O
latent	O
variable	O
.	O
With	O
this	O
modification	O
we	O
expect	O
to	O
explore	O
more	O
diverse	O
points	O
than	O
following	O
the	O
same	O
direction	O
for	O
several	O
steps	O
.	O

In	O
this	O
section	O
we	O
want	O
to	O
explore	O
how	O
the	O
ability	O
to	O
generate	O
human	O
readable	O
sentences	O
from	O
arbitrary	O
points	O
in	O
the	O
feature	O
space	O
affects	O
active	B-TaskName
learning	I-TaskName
performance	O
.	O
We	O
compare	O
our	O
approach	O
to	O
a	O
number	O
of	O
baselines	O
(	O
5.3	O
)	O
,	O
where	O
in	O
each	O
experiment	O
we	O
select	O
/	O
generate	O
500	O
instances	O
,	O
present	O
them	O
to	O
a	O
human	O
annotator	O
to	O
get	O
a	O
label	O
and	O
evaluate	O
the	O
performance	O
of	O
each	O
setting	O
in	O
a	O
sentiment	O
classification	O
task	O
.	O
We	O
start	O
the	O
active	B-TaskName
learning	I-TaskName
process	O
with	O
two	O
utterances	O
in	O
the	O
seed	O
set	O
,	O
namely	O
'	O
good	O
movie	O
'	O
and	O
'	O
bad	O
movie	O
'	O
.	O
tator	O
can	O
skip	O
neutral	O
or	O
uninterpretable	O
instances	O
.	O
These	O
skip	O
actions	O
also	O
count	O
towards	O
the	O
annotation	O
budget	O
.	O

We	O
compare	O
our	O
approach	O
to	O
Membership	O
Query	O
Synthesis	O
for	O
text	B-TaskName
classification	I-TaskName
to	O
four	O
baselines	O
.	O
The	O
first	O
baseline	O
selects	O
instances	O
from	O
the	O
pool	O
by	O
random	O
choice	O
.	O
The	O
least	O
confidence	O
baseline	O
computes	O
the	O
distance	O
of	O
the	O
instances	O
in	O
the	O
pool	O
to	O
the	O
separating	O
hyperplane	O
and	O
chooses	O
the	O
one	O
closest	O
to	O
the	O
hyperplane	O
.	O
The	O
third	O
and	O
fourth	O
baseline	O
follow	O
the	O
procedure	O
described	O
in	O
4	O
but	O
search	O
for	O
the	O
nearest	O
neighbor	O
(	O
nn	O
uniform	O
,	O
nn	O
wang	O
)	O
instead	O
of	O
synthesising	O
the	O
exact	O
query	O
point	O
.	O
Nearest	O
neighbor	O
is	O
defined	O
by	O
the	O
minimal	O
euclidean	O
distance	O
between	O
the	O
query	O
point	O
and	O
the	O
latent	O
representation	O
of	O
the	O
pool	O
instance	O
.	O

To	O
explore	O
the	O
ability	O
of	O
the	O
model	O
to	O
generate	O
unseen	O
instances	O
we	O
calculate	O
the	O
percentage	O
of	O
instances	O
not	O
seen	O
in	O
the	O
pool	O
.	O
We	O
only	O
look	O
at	O
instances	O
with	O
an	O
annotated	O
sentiment	O
label	O
,	O
because	O
skipped	O
examples	O
often	O
include	O
noise	O
and	O
thus	O
are	O
unlikely	O
to	O
be	O
present	O
in	O
the	O
pool	O
.	O
41	O
and	O
51	O
percent	O
of	O
labeled	O
instances	O
are	O
newly	O
generated	O
by	O
gen	O
uniform	O
and	O
gen	O
wang	O
respectively	O
.	O
This	O
provides	O
more	O
evidence	O
that	O
the	O
model	O
is	O
capable	O
of	O
generating	O
new	O
and	O
informative	O
instances	O
.	O
No	O
.	O
Instance	O
Label	O
1	O
.	O
the	O
acting	O
is	O
excellent	O
1	O
2	O
.	O
powerful	O
and	O
moving	O
1	O
3	O
.	O
this	O
movie	O
is	O
very	O
enjoyable	O
1	O
4	O
.	O
a	O
complete	O
mess	O
0	B-DatasetName
5	O
.	O
nothing	O
spectacular	O
0	B-DatasetName
6	O
.	O
absolutely	O
terrible	O
!	O
0	B-DatasetName
7	O
.	O
the	O
plot	O
is	O
UNK	O
skip	O
8	O
.	O
well	O
done	O
by	O
UNK	O
1	O
9	O
.	O
the	O
UNK	O
is	O
a	O
disappointment	O
0	B-DatasetName
Label	O
1	O
for	O
positive	O
and	O
0	B-DatasetName
for	O
negative	O
class	O
.	O
Figure	O
6	O
:	O
Plot	O
of	O
the	O
2	O
most	O
important	O
dimensions	O
of	O
selected	O
/	O
generated	O
instances	O
in	O
latent	O
space	O
.	O
Gray	O
points	O
indicate	O
negative	O
,	O
black	O
points	O
positive	O
labels	O
.	O
The	O
blue	O
square	O
denotes	O
'	O
bad	O
movie	O
'	O
and	O
the	O
red	O
cross	O
'	O
good	O
movie	O
'	O
.	O

To	O
further	O
analyze	O
the	O
behavior	O
of	O
the	O
different	O
AL	O
strategies	O
,	O
we	O
apply	O
dimensionality	B-TaskName
reduction	I-TaskName
and	O
visualize	O
the	O
instances	O
in	O
latent	O
space	O
(	O
Figure	O
6	O
)	O
.	O
The	O
two	O
largest	O
absolute	O
coefficients	O
of	O
the	O
trained	O
SVM	B-MethodName
's	O
linear	O
kernel	O
identify	O
the	O
most	O
important	O
dimensions	O
.	O
Figure	O
6	O
plots	O
the	O
points	O
,	O
represented	O
by	O
theses	O
two	O
dimensions	O
,	O
selected	O
by	O
different	O
active	B-TaskName
learning	I-TaskName
schedules	O
.	O
The	O
generated	O
instances	O
lie	O
densely	O
around	O
the	O
seed	O
points	O
,	O
while	O
pool	O
instances	O
are	O
more	O
distributed	O
.	O
In	O
gen	O
wang	O
one	O
can	O
see	O
how	O
the	O
instances	O
are	O
loosely	O
following	O
one	O
direction	O
similar	O
to	O
Figure1	O
.	O
As	O
indicated	O
in	O
Figure	O
2	O
a	O
pool	O
instance	O
is	O
represented	O
as	O
z	O
=	O
enc	O
(	O
x	O
)	O
.	O
The	O
same	O
is	O
true	O
for	O
the	O
instances	O
in	O
the	O
development	O
,	O
test	O
and	O
seed	O
set	O
.	O
For	O
the	O
generated	O
instances	O
there	O
are	O
two	O
options	O
.	O
If	O
z	O
is	O
a	O
point	O
selected	O
in	O
feature	O
space	O
and	O
x	O
=	O
dec	O
(	O
z	O
)	O
is	O
the	O
decoded	O
query	O
sequence	O
,	O
the	O
annotated	O
instance	O
can	O
either	O
be	O
represented	O
as	O
z	O
or	O
asẑ	O
=	O
enc	O
(	O
x	O
)	O
.	O
In	O
a	O
perfect	O
VAE	B-MethodName
z	O
andẑ	O
should	O
be	O
nearly	O
identical	O
.	O
In	O
practice	O
howeverẑ	O
ends	O
up	O
at	O
a	O
different	O
location	O
in	O
feature	O
space	O
.	O
Figure	O
7	O
depicts	O
the	O
distribution	O
of	O
distances	O
between	O
z	O
andẑ	O
generated	O
with	O
the	O
gen	O
uniform	O
method	O
.	O
We	O
observe	O
that	O
models	O
trained	O
onẑ	O
perform	O
better	O
than	O
those	O
trained	O
on	O
z	O
,	O
presumably	O
because	O
the	O
test	O
instances	O
are	O
represented	O
the	O
same	O
way	O
.	O
To	O
evaluate	O
ifẑ	O
is	O
still	O
an	O
informative	O
point	O
and	O
not	O
just	O
positioned	O
randomly	O
in	O
feature	O
space	O
,	O
we	O
train	O
a	O
model	O
on	O
actual	O
randomly	O
sampled	O
points	O
.	O
The	O
sampled	O
point	O
z	O
∼	O
N	O
(	O
0	B-DatasetName
,	O
I	O
)	O
is	O
decoded	O
to	O
query	O
sequence	O
x	O
,	O
labeled	O
and	O
subsequently	O
re	O
-	O
encoded	O
toẑ	O
=	O
enc	O
(	O
x	O
)	O
.	O
With	O
the	O
same	O
amount	O
of	O
instances	O
,	O
this	O
model	O
performs	O
much	O
worse	O
than	O
gen	O
uniform	O
,	O
indicating	O
that	O
pointẑ	O
still	O
preserves	O
some	O
of	O
the	O
informativeness	O
of	O
z.	O
We	O
thus	O
assume	O
that	O
the	O
closerẑ	O
is	O
to	O
selected	O
point	O
z	O
,	O
the	O
better	O
the	O
generation	O
based	O
active	B-TaskName
learning	I-TaskName
schedules	O
will	O
work	O
.	O

Related	O
work	O
in	O
the	O
context	O
of	O
semi	O
-	O
supervised	O
learning	O
has	O
focused	O
on	O
developing	O
methods	O
to	O
generate	O
synthetic	O
training	O
instances	O
for	O
different	O
tasks	O
(	O
Sennrich	O
et	O
al	O
,	O
2016	O
;	O
Hayashi	O
et	O
al	O
,	O
2018	O
;	O
Alberti	O
et	O
al	O
,	O
2019	O
;	O
Winata	O
et	O
al	O
,	O
2019	O
)	O
,	O
in	O
order	O
to	O
accelerate	O
the	O
learning	O
process	O
.	O
Sennrich	O
et	O
al	O
(	O
2016	O
)	O
create	O
artificial	O
training	O
instances	O
for	O
machine	B-TaskName
translation	I-TaskName
,	O
using	O
monolingual	O
data	O
paired	O
with	O
automatic	O
back	O
-	O
translations	O
.	O
Their	O
work	O
obtains	O
substantial	O
improvements	O
for	O
several	O
languages	O
and	O
has	O
triggered	O
many	O
follow	O
-	O
up	O
studies	O
that	O
apply	O
the	O
idea	O
of	O
back	O
-	O
translation	O
to	O
different	O
tasks	O
.	O
For	O
example	O
,	O
Hayashi	O
et	O
al	O
(	O
2018	O
)	O
Alberti	O
et	O
al	O
(	O
2019	O
)	O
use	O
a	O
large	O
number	O
of	O
synthetic	O
instances	O
to	O
pre	O
-	O
train	O
a	O
Question	B-TaskName
Answering	I-TaskName
(	O
QA	O
)	O
model	O
that	O
is	O
then	O
fine	O
-	O
tuned	O
on	O
the	O
target	O
QA	O
dataset	O
.	O
Their	O
approach	O
results	O
in	O
significant	O
improvements	O
over	O
models	O
that	O
are	O
trained	O
without	O
the	O
synthetic	O
datapoints	O
.	O
While	O
these	O
studies	O
show	O
that	O
huge	O
amounts	O
of	O
synthetic	O
training	O
data	O
can	O
crucially	O
improve	O
the	O
learning	O
process	O
,	O
our	O
approach	O
uses	O
a	O
different	O
paradigm	O
.	O
Instead	O
of	O
generating	O
millions	O
of	O
synthetic	O
data	O
points	O
,	O
our	O
method	O
is	O
data	O
-	O
lean	O
and	O
only	O
needs	O
a	O
few	O
hundred	O
instances	O
to	O
improve	O
the	O
classifier	O
.	O
Another	O
difference	O
is	O
that	O
we	O
do	O
not	O
rely	O
on	O
automatically	O
generated	O
labels	O
but	O
use	O
human	O
annotations	O
instead	O
.	O
Due	O
to	O
the	O
practical	O
constraints	O
of	O
the	O
active	B-TaskName
learning	I-TaskName
process	O
,	O
we	O
need	O
to	O
keep	O
the	O
training	O
time	O
short	O
enough	O
so	O
that	O
the	O
human	O
annotator	O
does	O
not	O
have	O
to	O
wait	O
for	O
the	O
next	O
set	O
of	O
instances	O
to	O
annotate	O
.	O
This	O
rules	O
out	O
the	O
use	O
of	O
computation	O
-	O
intensive	O
models	O
and	O
large	O
training	O
sets	O
.	O
Given	O
that	O
we	O
use	O
an	O
SVM	B-MethodName
for	O
classification	O
,	O
we	O
do	O
not	O
expect	O
a	O
strong	O
effect	O
for	O
adding	O
large	O
numbers	O
of	O
additional	O
training	O
instances	O
,	O
given	O
that	O
the	O
majority	O
of	O
those	O
data	O
points	O
will	O
not	O
be	O
positioned	O
close	O
to	O
the	O
decision	O
boundary	O
.	O
One	O
of	O
the	O
main	O
drawbacks	O
of	O
our	O
work	O
is	O
its	O
limitation	O
to	O
binary	O
sentence	B-TaskName
classification	I-TaskName
.	O
However	O
,	O
multi	B-TaskName
-	I-TaskName
class	I-TaskName
classification	I-TaskName
in	O
an	O
one	O
-	O
vs	O
-	O
rest	O
schema	O
is	O
compatible	O
with	O
our	O
method	O
and	O
worth	O
further	O
exploration	O
.	O
Another	O
interesting	O
direction	O
for	O
future	O
work	O
is	O
the	O
synthesis	O
of	O
data	O
for	O
more	O
complex	O
tasks	O
like	O
Natural	B-TaskName
Language	I-TaskName
Inference	I-TaskName
(	O
NLI	O
)	O
or	O
QA	O
.	O
This	O
,	O
however	O
,	O
requires	O
modifications	O
to	O
the	O
structure	O
of	O
the	O
autoencoder	B-MethodName
and	O
exceeds	O
the	O
scope	O
of	O
this	O
work	O
.	O
Membership	O
Query	O
Synthesis	O
might	O
also	O
be	O
an	O
interesting	O
approach	O
for	O
tasks	O
where	O
the	O
automatic	O
extraction	O
of	O
large	O
amounts	O
of	O
unlabelled	O
data	O
is	O
not	O
straight	O
-	O
forward	O
.	O
One	O
example	O
that	O
comes	O
to	O
mind	O
is	O
the	O
detection	O
of	O
offensive	O
language	O
or	O
'	O
hate	B-DatasetName
speech	I-DatasetName
'	O
,	O
where	O
we	O
have	O
to	O
deal	O
with	O
highly	O
unbalanced	O
training	O
sets	O
with	O
only	O
a	O
small	O
number	O
of	O
positive	O
instances	O
,	O
and	O
attempts	O
to	O
increase	O
this	O
number	O
have	O
been	O
shown	O
to	O
result	O
in	O
systematically	O
biased	O
datasets	O
(	O
Davidson	O
et	O
al	O
,	O
2019	O
;	O
Wiegand	O
et	O
al	O
,	O
2019	O
)	O
.	O
Table	O
2	O
suggests	O
that	O
the	O
generator	O
produces	O
instances	O
with	O
a	O
more	O
balanced	O
class	O
ratio	O
(	O
1.7	O
and	O
1.2	O
)	O
than	O
the	O
pool	O
data	O
(	O
2.6	O
)	O
it	O
was	O
trained	O
on	O
.	O
It	O
might	O
be	O
worthwhile	O
to	O
explore	O
whether	O
the	O
generation	O
of	O
synthetic	O
training	O
instances	O
can	O
help	O
to	O
mitigate	O
the	O
problem	O
to	O
select	O
instances	O
from	O
both	O
classes	O
in	O
an	O
highly	O
imbalanced	O
data	O
pool	O
.	O

This	O
work	O
is	O
the	O
first	O
to	O
show	O
that	O
Membership	O
Query	O
Synthesis	O
in	O
an	O
NLP	O
setting	O
is	O
feasible	O
.	O
Our	O
approach	O
uses	O
a	O
Variational	B-MethodName
Autoencoder	I-MethodName
as	O
a	O
representation	O
learner	O
and	O
generates	O
informative	O
ac	O
-	O
tive	O
learning	O
queries	O
from	O
latent	O
space	O
.	O
The	O
classification	O
performance	O
for	O
the	O
generated	O
instances	O
is	O
competitive	O
with	O
pool	O
-	O
based	O
active	B-TaskName
learning	I-TaskName
strategies	O
and	O
outperforms	O
other	O
AL	O
strategies	O
with	O
regard	O
to	O
annotation	O
cost	O
(	O
time	O
)	O
and	O
computational	O
complexity	O
.	O
The	O
main	O
advantage	O
of	O
Membership	O
Query	O
Synthesis	O
for	O
active	B-TaskName
learning	I-TaskName
is	O
that	O
it	O
allows	O
us	O
to	O
target	O
specific	O
points	O
along	O
the	O
separating	O
hyperplane	O
and	O
thus	O
to	O
provide	O
the	O
classifier	O
with	O
information	O
on	O
specific	O
areas	O
of	O
uncertainty	O
in	O
the	O
data	O
space	O
.	O
While	O
pool	O
-	O
based	O
active	B-TaskName
learning	I-TaskName
has	O
the	O
same	O
objective	O
,	O
Membership	O
Query	O
Synthesis	O
gives	O
us	O
a	O
more	O
precise	O
tool	O
to	O
explore	O
the	O
data	O
space	O
and	O
to	O
generate	O
exactly	O
those	O
instances	O
that	O
we	O
need	O
,	O
making	O
MQS	O
a	O
promising	O
approach	O
for	O
future	O
work	O
in	O
active	B-TaskName
learning	I-TaskName
.	O

DAMO	O
-	O
NLP	O
at	O
SemEval	O
-	O
2022	O
Task	O
11	O
:	O
A	O
Knowledge	O
-	O
based	O
System	O
for	O
Multilingual	B-TaskName
Named	I-TaskName
Entity	I-TaskName
Recognition	I-TaskName

The	O
MultiCoNER	B-DatasetName
shared	O
task	O
aims	O
at	O
detecting	O
semantically	O
ambiguous	O
and	O
complex	O
named	O
entities	O
in	O
short	O
and	O
low	O
-	O
context	O
settings	O
for	O
multiple	O
languages	O
.	O
The	O
lack	O
of	O
contexts	O
makes	O
the	O
recognition	O
of	O
ambiguous	O
named	O
entities	O
challenging	O
.	O
To	O
alleviate	O
this	O
issue	O
,	O
our	O
team	O
DAMO	O
-	O
NLP	O
proposes	O
a	O
knowledge	O
-	O
based	O
system	O
,	O
where	O
we	O
build	O
a	O
multilingual	O
knowledge	O
base	O
based	O
on	O
Wikipedia	O
to	O
provide	O
related	O
context	O
information	O
to	O
the	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
model	O
.	O
Given	O
an	O
input	O
sentence	O
,	O
our	O
system	O
effectively	O
retrieves	O
related	O
contexts	O
from	O
the	O
knowledge	O
base	O
.	O
The	O
original	O
input	O
sentences	O
are	O
then	O
augmented	O
with	O
such	O
context	O
information	O
,	O
allowing	O
significantly	O
better	O
contextualized	O
token	O
representations	O
to	O
be	O
captured	O
.	O
Our	O
system	O
wins	O
10	O
out	O
of	O
13	O
tracks	O
in	O
the	O
MultiCoNER	B-DatasetName
shared	O
task	O
.	O
1	O
*	O
:	O
project	O
lead	O
.	O
⋆	O
:	O
equal	O
contributions	O
.	O

The	O
MultiCoNER	B-DatasetName
shared	O
task	O
(	O
Malmasi	O
et	O
al	O
,	O
2022b	O
)	O
aims	O
at	O
building	O
Named	B-TaskName
Entity	I-TaskName
Recognition	I-TaskName
(	O
NER	B-TaskName
)	O
systems	O
for	O
11	O
languages	O
,	O
including	O
English	O
,	O
Spanish	O
,	O
Dutch	O
,	O
Russian	O
,	O
Turkish	O
,	O
Korean	O
,	O
Farsi	O
,	O
German	O
,	O
Chinese	O
,	O
Hindi	O
,	O
and	O
Bangla	O
.	O
The	O
task	O
has	O
three	O
kinds	O
of	O
tracks	O
including	O
one	O
multilingual	O
track	O
,	O
11	O
monolingual	O
tracks	O
and	O
one	O
code	O
-	O
mixed	O
track	O
.	O
The	O
multilingual	O
track	O
requires	O
training	O
multilingual	O
NER	B-TaskName
models	O
that	O
are	O
able	O
to	O
handle	O
all	O
languages	O
.	O
The	O
monolingual	O
tracks	O
require	O
training	O
individual	O
monolingual	O
models	O
where	O
each	O
model	O
works	O
for	O
only	O
one	O
language	O
.	O
The	O
code	O
-	O
mixed	O
track	O
requires	O
handling	O
code	O
-	O
mixed	O
samples	O
(	O
sentences	O
that	O
may	O
involve	O
multiple	O
languages	O
)	O
.	O
The	O
datasets	O
mainly	O
contain	O
sentences	O
from	O
three	O
domains	O
:	O
Wikipedia	O
,	O
web	O
questions	O
and	O
user	O
queries	O
,	O
köpings	O
is	O
rate	O
which	O
are	O
usually	O
short	O
and	O
low	O
-	O
context	O
sentences	O
.	O
Moreover	O
,	O
these	O
short	O
sentences	O
usually	O
contain	O
semantically	O
ambiguous	O
and	O
complex	O
entities	O
,	O
which	O
makes	O
the	O
problem	O
more	O
difficult	O
.	O
In	O
practice	O
,	O
professional	O
annotators	O
usually	O
use	O
their	O
domain	O
knowledge	O
to	O
disambiguate	O
such	O
kinds	O
of	O
entities	O
.	O
They	O
may	O
retrieve	O
the	O
related	O
documents	O
from	O
a	O
knowledge	O
base	O
(	O
KB	O
)	O
or	O
from	O
a	O
search	O
engine	O
to	O
better	O
guide	O
them	O
the	O
annotation	O
of	O
ambiguous	O
named	O
entities	O
(	O
Wang	O
et	O
al	O
,	O
2019	O
)	O
.	O
Therefore	O
,	O
we	O
believe	O
retrieving	O
related	O
knowledge	O
can	O
help	O
the	O
NER	B-TaskName
model	O
to	O
disambiguate	O
hard	O
samples	O
in	O
the	O
shared	O
task	O
as	O
well	O
.	O
A	O
motivating	O
example	O
is	O
shown	O
in	O
Figure	O
1	O
,	O
which	O
shows	O
how	O
the	O
retrieval	O
results	O
could	O
help	O
to	O
improve	O
the	O
prediction	O
in	O
practice	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
general	O
knowledgebased	O
system	O
for	O
the	O
MultiCoNER	B-DatasetName
shared	O
task	O
.	O
We	O
propose	O
to	O
retrieve	O
the	O
related	O
documents	O
of	O
the	O
input	O
sentence	O
so	O
that	O
the	O
recognition	O
of	O
difficult	O
entities	O
can	O
be	O
significantly	O
eased	O
.	O
Based	O
on	O
Wikipedia	O
of	O
the	O
11	O
languages	O
,	O
we	O
build	O
a	O
multilingual	O
KB	O
to	O
search	O
for	O
the	O
related	O
documents	O
of	O
the	O
input	O
sentence	O
.	O
We	O
then	O
feed	O
the	O
input	O
sentence	O
and	O
the	O
related	O
documents	O
into	O
the	O
NER	B-TaskName
model	O
.	O
Moreover	O
,	O
we	O
propose	O
an	O
iterative	O
retrieval	O
approach	O
to	O
i	O
m	O
-	O
prove	O
the	O
retrieval	O
quality	O
.	O
During	O
training	O
,	O
we	O
propose	O
multi	O
-	O
stage	O
fine	O
-	O
tuning	O
.	O
We	O
first	O
train	O
a	O
multilingual	O
model	O
so	O
that	O
the	O
NER	B-TaskName
model	O
can	O
learn	O
from	O
all	O
annotations	O
.	O
Next	O
,	O
we	O
train	O
the	O
monolingual	O
models	O
(	O
one	O
for	O
each	O
language	O
)	O
and	O
a	O
code	O
-	O
mixed	O
model	O
by	O
using	O
the	O
fine	O
-	O
tuned	O
XLM	B-MethodName
-	O
RoBERTa	B-MethodName
(	O
XLM	B-MethodName
-	O
R	O
)	O
(	O
Conneau	O
et	O
al	O
,	O
2020	O
)	O
embeddings	O
in	O
the	O
multilingual	O
model	O
as	O
initialization	O
to	O
further	O
boost	O
model	O
performance	O
on	O
monolingual	O
and	O
code	O
-	O
mixed	O
tracks	O
.	O
For	O
each	O
track	O
,	O
we	O
train	O
multiple	O
models	O
with	O
different	O
random	O
seeds	B-DatasetName
and	O
use	O
majority	O
voting	O
to	O
form	O
the	O
final	O
predictions	O
.	O
Besides	O
the	O
system	O
description	O
,	O
we	O
make	O
the	O
following	O
observations	O
based	O
on	O
our	O
experiments	O
:	O
1	O
.	O
Knowledge	O
-	O
based	O
systems	O
can	O
significantly	O
improve	O
both	O
in	O
-	O
and	O
out	O
-	O
of	O
-	O
domain	O
performance	O
compared	O
with	O
system	O
without	O
knowledge	O
inputs	O
.	O
2	O
.	O
Our	O
multi	O
-	O
stage	O
fine	O
-	O
tuning	O
approach	O
can	O
help	O
improve	O
model	O
performance	O
in	O
all	O
the	O
monolingual	O
and	O
code	O
-	O
mixed	O
tracks	O
.	O
The	O
approach	O
can	O
also	O
reduce	O
the	O
training	O
time	O
to	O
speed	O
up	O
our	O
system	O
building	O
at	O
different	O
stages	O
.	O
3	O
.	O
Our	O
iterative	O
retrieval	O
strategy	O
can	O
further	O
improve	O
the	O
retrieval	O
quality	O
and	O
result	O
in	O
significant	O
improvement	O
on	O
the	O
performance	O
of	O
codemixed	O
track	O
.	O
4	O
.	O
Searching	O
over	O
Wikipedia	O
KB	O
performs	O
better	O
than	O
using	O
online	O
search	O
engines	O
on	O
the	O
Multi	O
-	O
CoNER	O
datasets	O
.	O
5	O
.	O
Comparing	O
with	O
other	O
model	O
variants	O
we	O
have	O
tried	O
,	O
our	O
NER	B-TaskName
model	O
enjoys	O
a	O
good	O
balance	O
between	O
model	O
performance	O
and	O
speed	O
.	O

We	O
introduce	O
how	O
our	O
knowledge	O
-	O
based	O
NER	B-TaskName
system	O
works	O
in	O
this	O
section	O
.	O
Given	O
a	O
sentence	O
of	O
n	O
tokens	O
x	O
=	O
{	O
x	O
1	O
,	O
,	O
x	O
n	O
}	O
,	O
the	O
sentence	O
is	O
fed	O
into	O
our	O
knowledge	O
retrieval	O
module	O
.	O
The	O
knowledge	O
retrieval	O
module	O
takes	O
the	O
sentence	O
as	O
the	O
query	O
and	O
retrieves	O
top	O
-	O
k	O
related	O
paragraphs	O
in	O
KB	O
.	O
The	O
system	O
then	O
concatenates	O
the	O
input	O
sentence	O
and	O
the	O
related	O
paragraphs	O
together	O
and	O
feeds	O
the	O
concatenated	O
sequence	O
into	O
the	O
embeddings	O
.	O
The	O
output	O
token	O
representations	O
of	O
the	O
input	O
sentence	O
are	O
fed	O
into	O
a	O
linear	O
-	O
chain	O
conditional	B-MethodName
random	I-MethodName
field	I-MethodName
(	O
CRF	B-MethodName
)	O
(	O
Lafferty	O
et	O
al	O
,	O
2001	O
)	O
layer	O
and	O
the	O
CRF	B-MethodName
layer	O
produces	O
the	O
label	O
predictions	O
.	O
Given	O
the	O
label	O
predictions	O
of	O
multiple	O
NER	B-TaskName
models	O
with	O
different	O
random	O
seeds	B-DatasetName
,	O
the	O
ensemble	O
module	O
uses	O
a	O
voting	O
strategy	O
to	O
decide	O
the	O
final	O
predictionŝ	O
y	O
=	O
{	O
ŷ	O
1	O
,	O
,	O
ŷ	O
n	O
}	O
of	O
the	O
sentence	O
.	O
The	O
architecture	O
of	O
our	O
framework	O
is	O
shown	O
in	O
Figure	O
2	O
.	O

Retrieval	O
-	O
augmented	O
context	O
is	O
effective	O
for	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
tasks	O
(	O
Wang	O
et	O
al	O
,	O
2021b	O
)	O
,	O
as	O
external	O
relevant	O
contexts	O
can	O
provide	O
auxiliary	O
information	O
for	O
disambiguating	O
complex	O
named	O
entities	O
.	O
We	O
construct	O
multilingual	O
KBs	O
based	O
on	O
Wikipedia	O
pages	O
of	O
the	O
11	O
languages	O
,	O
and	O
then	O
retrieve	O
relevant	O
documents	O
by	O
using	O
the	O
input	O
sentence	O
as	O
a	O
query	O
.	O
These	O
retrieved	O
documents	O
act	O
as	O
contexts	O
and	O
are	O
fed	O
into	O
the	O
NER	B-TaskName
module	O
.	O
To	O
enhance	O
the	O
retrieval	O
quality	O
,	O
we	O
further	O
designed	O
an	O
iterative	O
retrieval	O
approach	O
,	O
which	O
incorporates	O
predicted	O
entities	O
of	O
NER	B-TaskName
models	O
into	O
the	O
search	O
query	O
.	O
Knowledge	O
Base	O
Building	O
Wikipedia	O
is	O
an	O
evolving	O
source	O
of	O
knowledge	O
that	O
can	O
facilitate	O
many	O
NLP	O
tasks	O
(	O
Chen	O
et	O
al	O
,	O
2017	O
;	O
Verlinden	O
et	O
al	O
,	O
2021	O
)	O
.	O
Wikipedia	O
provides	O
a	O
rich	O
collection	O
of	O
mention	O
hyperlinks	O
(	O
referred	O
to	O
as	O
wiki	O
anchors	O
)	O
.	O
For	O
example	O
,	O
in	O
the	O
sentence	O
"	O
Steve	O
Jobs	O
founded	O
Apple	O
"	O
,	O
entities	O
"	O
Steve	O
Jobs	O
"	O
and	O
"	O
Apple	O
"	O
are	O
linked	O
to	O
the	O
wiki	O
entries	O
Steve_Jobs	O
and	O
Apple_Inc	O
respectively	O
.	O
For	O
the	O
NER	B-TaskName
task	O
,	O
these	O
anchors	O
provide	O
useful	O
clues	O
on	O
where	O
the	O
entities	O
are	O
to	O
the	O
model	O
.	O
Based	O
on	O
Wikipedia	O
we	O
can	O
build	O
local	O
Wikipedia	O
search	O
engines	O
to	O
retrieve	O
the	O
relevant	O
context	O
of	O
the	O
input	O
sentences	O
for	O
each	O
language	O
.	O
We	O
download	O
the	O
latest	O
(	O
2021	O
-	O
12	O
-	O
20	O
)	O
version	O
of	O
the	O
Wikipedia	O
dump	O
from	O
Wikimedia	O
3	O
and	O
convert	O
it	O
to	O
plain	O
texts	O
.	O
Then	O
we	O
use	O
ElasticSearch	O
(	O
ES	O
)	O
4	O
to	O
index	O
them	O
.	O
ElasticSearch	O
is	O
document	O
-	O
oriented	O
,	O
and	O
the	O
document	O
is	O
the	O
least	O
searchable	O
unit	O
.	O
We	O
define	O
the	O
document	O
in	O
our	O
local	O
Wikipedia	O
search	O
engines	O
with	O
three	O
fields	O
:	O
sentence	O
,	O
paragraph	O
and	O
title	O
.	O
We	O
create	O
inverted	O
indexes	O
on	O
both	O
the	O
sentence	O
field	O
and	O
the	O
title	O
field	O
.	O
The	O
former	O
is	O
used	O
as	O
a	O
sentence	O
-	O
level	O
full	O
-	O
text	O
retrieval	O
field	O
,	O
while	O
the	O
latter	O
indicates	O
the	O
core	O
entity	O
described	O
by	O
the	O
wiki	O
page	O
and	O
can	O
be	O
used	O
as	O
an	O
entity	O
-	O
level	O
retrieval	O
field	O
.	O
The	O
paragraph	O
field	O
stores	O
the	O
contexts	O
of	O
the	O
sentence	O
.	O
To	O
take	O
advantage	O
of	O
the	O
rich	O
wiki	O
anchors	O
in	O
Wikipedia	O
paragraphs	O
,	O
we	O
marked	O
them	O
with	O
special	O
markers	O
.	O
For	O
example	O
,	O
to	O
incorporate	O
the	O
hyperlinks	O
[	O
Apple	O
Apple	O
Inc	O
]	O
and	O
[	O
Steve	O
Jobs	O
Steve	O
Jobs	O
]	O
to	O
the	O
paragraph	O
,	O
we	O
transformed	O
"	O
Steve	O
Jobs	O
founded	O
Apple	O
"	O
into	O
"	O
<	O
e	O
:	O
Steve	O
Jobs	O
>	O
Steve	O
Jobs</e	O
>	O
founded	O
<	O
e	O
:	O
Apple_inc	O
>	O
Apple</e	O
>	O
"	O
5	O
.	O
Sentence	O
Retrieval	O
Retrieval	O
at	O
the	O
sentence	O
level	O
takes	O
the	O
input	O
sentence	O
as	O
a	O
query	O
and	O
retrieves	O
the	O
top	O
-	O
k	O
documents	O
on	O
the	O
sentence	O
field	O
.	O
Given	O
an	O
input	O
sentence	O
,	O
we	O
select	O
the	O
corresponding	O
search	O
engine	O
according	O
to	O
the	O
language	O
of	O
the	O
sentence	O
.	O
Iterative	O
Entity	B-TaskName
Retrieval	I-TaskName
The	O
core	O
of	O
the	O
NER	B-TaskName
task	O
lies	O
in	O
the	O
entities	O
,	O
while	O
retrieval	O
at	O
the	O
sentence	O
level	O
overlooks	O
the	O
key	O
entities	O
in	O
the	O
sentences	O
.	O
For	O
this	O
reason	O
,	O
we	O
consider	O
the	O
relevance	O
of	O
the	O
entities	O
in	O
the	O
sentence	O
to	O
the	O
title	O
field	O
in	O
the	O
documents	O
during	O
retrieval	O
.	O
We	O
concatenate	O
the	O
entities	O
in	O
the	O
sentences	O
with	O
"	O
|	O
"	O
and	O
then	O
retrieve	O
them	O
on	O
the	O
title	O
field	O
.	O
On	O
the	O
training	O
and	O
development	O
sets	O
,	O
we	O
utilize	O
the	O
ground	O
-	O
truth	O
entities	O
directly	O
.	O
On	O
the	O
test	O
set	O
,	O
we	O
first	O
perform	O
the	O
sentence	O
retrieval	O
and	O
then	O
use	O
the	O
entity	O
mentions	O
6	O
predicted	O
by	O
the	O
model	O
for	O
entity	B-TaskName
retrieval	I-TaskName
.	O
This	O
bootstrapping	O
manner	O
can	O
be	O
applied	O
for	O
T	O
turns	O
.	O
Context	O
Processing	O
After	O
top	O
-	O
k	O
results	O
from	O
the	O
KB	O
are	O
retrieved	O
,	O
the	O
system	O
post	O
-	O
processes	O
the	O
retrieved	O
documents	O
into	O
the	O
contexts	O
of	O
the	O
input	O
sentence	O
.	O
There	O
are	O
three	O
options	O
of	O
utilizing	O
the	O
texts	O
in	O
the	O
documents	O
,	O
which	O
are	O
:	O
1	O
)	O
use	O
the	O
matched	O
paragraph	O
;	O
2	O
)	O
use	O
the	O
matched	O
sentence	O
;	O
3	O
)	O
use	O
the	O
matched	O
sentence	O
but	O
remove	O
the	O
wiki	O
anchors	O
.	O
We	O
compare	O
the	O
performance	O
of	O
each	O
option	O
in	O
section	O
5.4	O
.	O
In	O
each	O
retrieved	O
document	O
,	O
we	O
concatenate	O
the	O
title	O
and	O
texts	O
together	O
to	O
form	O
the	O
contextx	O
i	O
.	O
The	O
results	O
are	O
then	O
concatenated	O
into	O
{	O
x	O
1	O
,	O
,	O
x	O
k	O
}	O
based	O
on	O
the	O
retrieval	O
ranking	O
.	O

We	O
compare	O
several	O
types	O
of	O
KBs	O
and	O
contexts	O
during	O
our	O
system	O
building	O
.	O
Online	O
Search	O
Engine	O
In	O
the	O
early	O
stage	O
,	O
we	O
tried	O
to	O
use	O
the	O
knowledge	O
retrieved	O
from	O
Google	B-DatasetName
Search	O
,	O
which	O
can	O
retrieve	O
related	O
knowledge	O
from	O
a	O
large	O
scale	O
of	O
webs	O
and	O
is	O
believed	O
to	O
be	O
a	O
strong	O
multilingual	O
search	O
engine	O
.	O

Table	O
6	O
shows	O
the	O
speed	O
of	O
each	O
module	O
in	O
our	O
system	O
.	O
In	O
the	O
table	O
,	O
we	O
also	O
show	O
that	O
the	O
retrieval	O
speed	O
of	O
our	O
local	O
KB	O
is	O
significantly	O
faster	O
than	O
that	O
of	O
Google	B-DatasetName
Search	O
.	O
The	O
bottleneck	O
of	O
the	O
system	O
speed	O
is	O
the	O
NER	B-TaskName
module	O
rather	O
than	O
the	O
knowledge	O
retrieval	O
module	O
.	O
The	O
main	O
reason	O
for	O
the	O
slow	O
speed	O
of	O
the	O
NER	B-TaskName
module	O
is	O
that	O
the	O
input	O
length	O
of	O
the	O
knowledge	O
-	O
based	O
system	O
is	O
significantly	O
longer	O
than	O
the	O
original	O
input	O
.	O
Taking	O
the	O
EN	O
test	O
set	O
as	O
an	O
example	O
,	O
there	O
are	O
on	O
average	O
10	O
tokens	O
for	O
each	O
input	O
sentence	O
in	O
the	O
original	O
test	O
set	O
while	O
there	O
are	O
218	O
tokens	O
for	O
the	O
input	O
of	O
our	O
knowledge	O
-	O
based	O
system	O
.	O
The	O
longer	O
inputs	O
slow	O
down	O
the	O
encoding	O
at	O
XLM	B-MethodName
-	O
R	O
embeddings	O
.	O

)	O
CE	O
is	O
one	O
of	O
the	O
usual	O
approaches	O
to	O
NER	B-TaskName
,	O
which	O
concatenates	O
different	O
kinds	O
of	O
embeddings	O
to	O
improve	O
the	O
token	O
representations	O
.	O
In	O
the	O
early	O
stage	O
of	O
our	O
system	O
building	O
,	O
we	O
compare	O
CE	O
with	O
only	O
using	O
the	O
XLM	B-MethodName
-	O
R	O
embeddings	O
based	O
on	O
the	O
knowledge	O
retrieved	O
from	O
the	O
Google	B-DatasetName
Search	O
.	O
Results	O
in	O
Table	O
7	O
show	O
that	O
CE	O
models	O
are	O
stronger	O
than	O
the	O
models	O
using	O
XLM	B-MethodName
-	O
R	O
embeddings	O
only	O
in	O
all	O
the	O
cases	O
,	O
which	O
show	O
the	O
effectiveness	O
of	O
CE	O
.	O
ACE	O
(	O
Automated	O
Concatenation	O
of	O
Embeddings	O
)	O
ACE	O
(	O
Wang	O
et	O
al	O
,	O
2021a	O
)	O
is	O
an	O
improved	O
version	O
of	O
CE	O
which	O
automatically	O
selects	O
a	O
better	O
concatenation	O
of	O
the	O
embeddings	O
.	O
We	O
use	O
the	O
same	O
embedding	O
types	O
as	O
CE	O
and	O
the	O
knowledge	O
are	O
from	O
our	O
Wikipedia	O
KB	O
.	O
We	O
experiment	O
on	O
EN	O
,	O
ES	O
,	O
NL	O
,	O
RU	O
,	O
TR	O
,	O
KO	O
and	O
FA	B-MethodName
,	O
which	O
are	O
strong	O
with	O
PARA	O
contexts	O
.	O
In	O
Table	O
9	O
,	O
we	O
further	O
compare	O
ACE	O
with	O
ensemble	O
XLM	B-MethodName
-	O
R	O
models	O
.	O
Results	O
show	O
ACE	O
can	O
improve	O
the	O
model	O
performance	O
and	O
even	O
outperform	O
the	O
ensemble	O
models	O
13	O
.	O
The	O
results	O
in	O
Table	O
7	O
and	O
9	O
show	O
the	O
advantage	O
of	O
the	O
embedding	O
concatenation	O
.	O
However	O
,	O
as	O
we	O
have	O
shown	O
in	O
Section	O
5.5	O
,	O
the	O
prediction	O
speed	O
is	O
quite	O
slow	O
with	O
the	O
single	O
XLM	B-MethodName
-	O
R	O
embeddings	O
.	O
The	O
CE	O
models	O
further	O
slow	O
down	O
the	O
prediction	O
speed	O
since	O
the	O
models	O
contain	O
more	O
embeddings	O
.	O
The	O
ACE	O
models	O
usually	O
have	O
faster	O
prediction	O
speed	O
than	O
the	O
CE	O
models	O
.	O
However	O
,	O
training	O
the	O
ACE	O
models	O
is	O
quite	O
slow	O
.	O
It	O
takes	O
about	O
four	O
days	O
to	O
train	O
a	O
single	O
ACE	O
model	O
.	O
Moreover	O
,	O
the	O
ACE	O
models	O
can	O
not	O
use	O
the	O
development	O
set	O
to	O
train	O
the	O
model	O
since	O
they	O
use	O
development	O
score	O
as	O
the	O
reward	O
to	O
select	O
the	O
embedding	O
concatenations	O
.	O
Therefore	O
,	O
due	O
to	O
the	O
time	O
constraints	O
,	O
we	O
did	O
not	O
use	O
these	O
two	O
variants	O
in	O
our	O
submission	O
during	O
the	O
shared	O
task	O
period	O
.	O

In	O
this	O
paper	O
,	O
we	O
describe	O
our	O
knowledge	O
-	O
based	O
system	O
for	O
the	O
MultiCoNER	B-DatasetName
shared	O
task	O
,	O
which	O
wins	O
10	O
out	O
of	O
13	O
tracks	O
in	O
the	O
shared	O
task	O
.	O
We	O
construct	O
multilingual	O
KBs	O
and	O
retrieve	O
the	O
related	O
documents	O
from	O
KBs	O
to	O
enhance	O
the	O
token	O
representations	O
of	O
input	O
text	O
.	O
We	O
show	O
that	O
the	O
NER	B-TaskName
models	O
can	O
use	O
the	O
retrieved	O
knowledge	O
to	O
facilitate	O
complex	O
entity	O
prediction	O
,	O
significantly	O
improving	O
both	O
the	O
in	O
-	O
domain	O
and	O
out	O
-	O
of	O
-	O
domain	O
performance	O
.	O
Multi	O
-	O
stage	O
fine	O
-	O
tuning	O
can	O
help	O
the	O
monolingual	O
models	O
learn	O
from	O
the	O
training	O
data	O
of	O
all	O
the	O
languages	O
and	O
improve	O
the	O
model	O
performance	O
and	O
training	O
efficiency	O
.	O
We	O
also	O
show	O
that	O
the	O
system	O
presents	O
a	O
good	O
balance	O
between	O
the	O
model	O
performance	O
and	O
prediction	O
efficiency	O
to	O
meet	O
the	O
time	O
requirement	O
in	O
the	O
test	O
phase	O
.	O
We	O
believe	O
this	O
system	O
can	O
be	O
widely	O
applied	O
to	O
other	O
domains	O
for	O
the	O
task	O
of	O
NER	B-TaskName
.	O
For	O
future	O
work	O
,	O
we	O
plan	O
to	O
improve	O
the	O
retrieval	O
quality	O
and	O
adopt	O
the	O
system	O
to	O
support	O
other	O
kinds	O
of	O
entity	O
-	O
related	O
tasks	O
.	O

As	O
we	O
state	O
in	O
Section	O
3.3	O
,	O
we	O
use	O
majority	O
voting	O
as	O
the	O
ensemble	O
algorithm	O
in	O
our	O
system	O
.	O
We	O
show	O
an	O
experiment	O
about	O
how	O
the	O
voting	O
threshold	O
affect	O
the	O
ensemble	O
model	O
performance	O
during	O
our	O
system	O
building	O
on	O
the	O
development	O
set	O
.	O
We	O
ensemble	O
the	O
models	O
on	O
DE	O
,	O
ZH	O
,	O
HI	O
,	O
BN	O
,	O
MIX	O
with	O
PARA	O
since	O
these	O
five	O
tracks	O
have	O
relatively	O
lower	O
performance	O
than	O
the	O
other	O
7	O
tracks	O
.	O
In	O
Figure	O
4	O
,	O
we	O
show	O
how	O
the	O
threshold	O
of	O
the	O
majority	O
voting	O
affects	O
the	O
model	O
performance	O
.	O
From	O
the	O
figure	O
,	O
we	O
can	O
see	O
that	O
the	O
best	O
threshold	O
varies	O
over	O
the	O
language	O
.	O
Therefore	O
,	O
we	O
simply	O
choose	O
0.5	O
as	O
there	O
is	O
no	O
best	O
threshold	O
value	O
.	O
Moreover	O
,	O
we	O
compare	O
the	O
majority	O
voting	O
ensemble	O
and	O
CRF	B-MethodName
level	O
ensemble	O
in	O
Table	O
12	O
.	O
The	O
CRF	B-MethodName
level	O
ensemble	O
averages	O
the	O
emission	O
and	O
transition	O
scores	O
in	O
the	O
Eq	O
.	O
1	O
predicted	O
by	O
the	O
candidate	O
models	O
and	O
uses	O
the	O
Viterbi	O
algorithm	O
to	O
get	O
the	O
prediction	O
.	O
The	O
results	O
show	O
that	O
CRF	B-MethodName
level	O
ensemble	O
performs	O
inferior	O
to	O
the	O
majority	O
voting	O
ensemble	O
.	O
The	O
possible	O
reason	O
is	O
that	O
training	O
with	O
different	O
random	O
seeds	B-DatasetName
may	O
lead	O
to	O
different	O
emission	O
transition	O
scores	O
at	O
different	O
Avg	O
.	O
scales	O
.	O
As	O
a	O
result	O
,	O
the	O
models	O
with	O
larger	O
scales	O
have	O
higher	O
weights	O
in	O
the	O
ensemble	O
.	O

The	O
detailed	O
statistics	O
of	O
the	O
MultiCoNER	B-DatasetName
dataset	O
are	O
listed	O
in	O
Table	O
10	O
and	O
the	O
statistics	O
of	O
our	O
KBs	O
ares	O
shown	O
in	O
Table	O
11	O
.	O

English	O
-	O
Portuguese	O
Biomedical	O
Translation	B-TaskName
Task	O
Using	O
a	O
Genuine	O
Phrase	O
-	O
Based	O
Statistical	O
Machine	B-TaskName
Translation	I-TaskName
Approach	O

Our	O
approach	O
to	O
produce	O
translations	O
for	O
the	O
ACL	O
-	O
2016	O
Biomedical	O
Translation	B-TaskName
Task	O
on	O
the	O
English	O
-	O
Portuguese	O
language	O
pair	O
,	O
in	O
both	O
directions	O
,	O
is	O
described	O
.	O
Own	O
preliminary	O
tests	O
results	O
and	O
final	O
results	O
,	O
measured	O
by	O
the	O
shared	O
task	O
organizers	O
,	O
are	O
also	O
presented	O
.	O

This	O
paper	O
shows	O
how	O
we	O
obtained	O
our	O
results	O
using	O
our	O
patented	O
Machine	B-TaskName
Translation	I-TaskName
system	O
to	O
produce	O
translations	O
for	O
the	O
English	O
-	O
Portuguese	O
language	O
pair	O
from	O
the	O
Biomedical	O
Translation	B-TaskName
Task	O
.	O
Our	O
approach	O
differs	O
from	O
common	O
Statistical	O
Machine	B-TaskName
Translation	I-TaskName
approaches	O
like	O
Moses	O
(	O
Koehn	O
et	O
al	O
,	O
2007	O
)	O
in	O
several	O
aspects	O
:	O
phrases	O
are	O
not	O
analyzed	O
at	O
their	O
word	O
level	O
in	O
any	O
model	O
;	O
the	O
language	O
model	O
depends	O
on	O
the	O
target	O
al	O
ernatives	O
of	O
given	O
adjacent	O
sources	O
and	O
does	O
not	O
try	O
to	O
avoid	O
null	O
scores	O
to	O
phrases	O
that	O
do	O
not	O
occur	O
;	O
the	O
translation	O
score	O
is	O
not	O
log	O
-	O
linear	O
,	O
but	O
instead	O
a	O
tuned	O
weighted	O
average	O
between	O
the	O
translation	O
model	O
and	O
the	O
language	O
model	O
,	O
and	O
so	O
no	O
smoothing	O
techniques	O
are	O
required	O
;	O
several	O
models	O
can	O
be	O
used	O
with	O
different	O
relevances	O
or	O
weights	O
;	O
and	O
instead	O
of	O
simply	O
relying	O
on	O
statistics	O
,	O
we	O
include	O
human	O
validation	O
and	O
correction	O
on	O
several	O
stages	O
of	O
the	O
system	O
,	O
namely	O
for	O
validating	O
extracted	O
term	O
translations	O
,	O
to	O
improve	O
the	O
quality	O
of	O
the	O
source	O
data	O
used	O
in	O
the	O
automatically	O
produced	O
translations	O
.	O
As	O
requested	O
,	O
the	O
translation	O
results	O
were	O
produced	O
using	O
the	O
sentence	O
-	O
aligned	O
training	O
data	O
described	O
below	O
(	O
for	O
the	O
English	O
-	O
Portuguese	O
language	O
pair	O
,	O
in	O
our	O
case	O
)	O
,	O
provided	O
by	O
the	O
shared	O
task	O
organizers	O
:	O
medline	O
-	O
pubmed	O
:	O
parallel	O
corpora	O
from	O
medline	O
;	O
scielo	O
-	O
gma	O
-	O
biological	O
:	O
parallel	O
biological	O
documents	O
from	O
the	O
Scielo	O
database	O
(	O
Neves	O
et	O
al	O
,	O
2016	O
)	O
;	O
and	O
scielo	O
-	O
gma	O
-	O
health	O
:	O
parallel	O
health	O
documents	O
from	O
the	O
Scielo	O
database	O
(	O
Neves	O
et	O
al	O
,	O
2016	O
)	O
.	O
Table	O
1	O
shows	O
the	O
features	O
of	O
the	O
English	O
(	O
en	O
)	O
and	O
Portuguese	O
(	O
pt	O
)	O
languages	O
of	O
each	O
provided	O
corpora	O
,	O
namely	O
their	O
number	O
of	O
lines	O
and	O
words	O
.	O
The	O
training	O
corpora	O
had	O
to	O
undergo	O
several	O
processing	O
stages	O
in	O
order	O
to	O
support	O
the	O
production	O
of	O
the	O
intended	O
translations	O
,	O
as	O
described	O
in	O
the	O
following	O
section	O
.	O

In	O
order	O
to	O
produce	O
translations	O
,	O
our	O
system	O
(	O
like	O
any	O
other	O
Statistical	O
Machine	B-TaskName
Translation	I-TaskName
system	O
)	O
requires	O
a	O
translation	O
model	O
and	O
a	O
language	O
model	O
to	O
support	O
the	O
translation	O
decoding	O
stage	O
.	O
To	O
calculate	O
such	O
models	O
the	O
available	O
data	O
had	O
to	O
go	O
through	O
several	O
processing	O
steps	O
described	O
in	O
the	O
following	O
subsections	O
.	O
Since	O
each	O
of	O
the	O
training	O
corpus	O
has	O
been	O
made	O
available	O
separately	O
,	O
we	O
also	O
opted	O
to	O
process	O
each	O
of	O
them	O
separately	O
so	O
that	O
we	O
were	O
then	O
able	O
to	O
use	O
them	O
with	O
different	O
weights	O
,	O
assigning	O
more	O
or	O
less	O
weight	O
to	O
models	O
with	O
higher	O
or	O
lower	O
relevance	O
,	O
respectively	O
.	O
See	O
extended	O
explanation	O
in	O
4	O
.	O

The	O
scores	O
of	O
our	O
submitted	O
translations	O
are	O
shown	O
in	O
The	O
results	O
obtained	O
were	O
clearly	O
below	O
what	O
we	O
had	O
expected	O
.	O
And	O
what	O
is	O
most	O
disturbing	O
is	O
the	O
negative	O
impact	O
of	O
features	O
we	O
expected	O
to	O
improve	O
results	O
,	O
an	O
expectation	O
backed	O
by	O
our	O
own	O
tests	O
.	O
However	O
,	O
there	O
are	O
a	O
few	O
reasons	O
we	O
can	O
think	O
of	O
for	O
these	O
values	O
,	O
namely	O
the	O
way	O
the	O
BLEU	B-MetricName
measure	O
has	O
been	O
calculated	O
(	O
case	O
sensitivity	O
and	O
synonyms	O
penalty	O
-	O
translating	O
"	O
home	O
"	O
instead	O
of	O
"	O
house	O
"	O
might	O
be	O
perfectly	O
fine	O
)	O
,	O
the	O
differences	O
between	O
European	O
Portuguese	O
and	O
Brazilian	O
Portuguese	O
,	O
and	O
the	O
presence	O
of	O
several	O
spelling	O
and	O
alignment	O
errors	O
in	O
the	O
training	O
data	O
.	O
Nonetheless	O
,	O
we	O
can	O
still	O
take	O
several	O
actions	O
to	O
improve	O
our	O
system	O
:	O
namely	O
testing	O
both	O
parallel	O
corpora	O
,	O
health	O
and	O
biology	O
,	O
with	O
identical	O
weights	O
:	O
using	O
Europarl	O
and	O
eventually	O
EMEA	B-MethodName
corpus	O
;	O
the	O
refinement	O
of	O
our	O
phrase	O
translation	O
extraction	O
;	O
the	O
extraction	O
of	O
specific	O
bilingual	O
terminology	O
,	O
additionally	O
to	O
the	O
use	O
of	O
cognaticity	O
;	O
subsentence	O
realignment	O
after	O
the	O
bilingual	O
terminology	O
extraction	O
,	O
and	O
a	O
more	O
efficient	O
implementation	O
of	O
the	O
patterns	O
(	O
comparable	O
to	O
a	O
hierarchical	O
translation	O
)	O
application	O
.	O

Improving	O
Distantly	O
-	O
supervised	O
Entity	B-TaskName
Typing	I-TaskName
with	O
Compact	O
Latent	O
Space	O
Clustering	O

Recently	O
,	O
distant	O
supervision	O
has	O
gained	O
great	O
success	O
on	O
Fine	O
-	O
grained	O
Entity	B-TaskName
Typing	I-TaskName
(	O
FET	O
)	O
.	O
Despite	O
its	O
efficiency	O
in	O
reducing	O
manual	O
labeling	O
efforts	O
,	O
it	O
also	O
brings	O
the	O
challenge	O
of	O
dealing	O
with	O
false	O
entity	O
type	O
labels	O
,	O
as	O
distant	O
supervision	O
assigns	O
labels	O
in	O
a	O
contextagnostic	O
manner	O
.	O
Existing	O
works	O
alleviated	O
this	O
issue	O
with	O
partial	O
-	O
label	O
loss	B-MetricName
,	O
but	O
usually	O
suffer	O
from	O
confirmation	O
bias	O
,	O
which	O
means	O
the	O
classifier	O
fit	O
a	O
pseudo	O
data	O
distribution	O
given	O
by	O
itself	O
.	O
In	O
this	O
work	O
,	O
we	O
propose	O
to	O
regularize	O
distantly	O
supervised	O
models	O
with	O
Compact	O
Latent	O
Space	O
Clustering	O
(	O
CLSC	O
)	O
to	O
bypass	O
this	O
problem	O
and	O
effectively	O
utilize	O
noisy	O
data	O
yet	O
.	O
Our	O
proposed	O
method	O
first	O
dynamically	O
constructs	O
a	O
similarity	O
graph	O
of	O
different	O
entity	O
mentions	O
;	O
infer	O
the	O
labels	O
of	O
noisy	O
instances	O
via	O
label	O
propagation	O
.	O
Based	O
on	O
the	O
inferred	O
labels	O
,	O
mention	O
embeddings	O
are	O
updated	O
accordingly	O
to	O
encourage	O
entity	O
mentions	O
with	O
close	O
semantics	O
to	O
form	O
a	O
compact	O
cluster	O
in	O
the	O
embedding	O
space	O
,	O
thus	O
leading	O
to	O
better	O
classification	O
performance	O
.	O
Extensive	O
experiments	O
on	O
standard	O
benchmarks	O
show	O
that	O
our	O
CLSC	O
model	O
consistently	O
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
distantly	O
supervised	O
entity	B-TaskName
typing	I-TaskName
systems	O
by	O
a	O
significant	O
margin	O
.	O

Fine	O
-	O
grained	O
entity	B-TaskName
typing	I-TaskName
takes	O
a	O
corpus	O
and	O
an	O
external	O
knowledge	O
base	O
(	O
KB	O
)	O
with	O
a	O
type	O
hierarchy	O
Y	O
as	O
input	O
.	O
Given	O
an	O
entity	O
mention	O
(	O
i.e.	O
,	O
a	O
sequence	O
of	O
token	O
spans	O
representing	O
an	O
entity	O
)	O
in	O
the	O
corpus	O
,	O
our	O
task	O
is	O
to	O
uncover	O
its	O
corresponding	O
type	O
-	O
path	O
in	O
Y	O
based	O
on	O
the	O
context	O
.	O
By	O
applying	O
distant	O
supervision	O
,	O
each	O
mention	O
is	O
first	O
linked	O
to	O
an	O
existing	O
entity	O
in	O
KB	O
,	O
and	O
then	O
labeled	O
with	O
all	O
its	O
possible	O
types	O
.	O
Formally	O
,	O
a	O
labeled	O
corpus	O
can	O
be	O
represented	O
as	O
triples	O
D	O
=	O
{	O
(	O
m	O
i	O
,	O
c	O
i	O
,	O
Y	O
i	O
)	O
}	O
n	O
i=1	O
,	O
where	O
m	O
i	O
is	O
the	O
i	O
-	O
th	O
mention	O
,	O
c	O
i	O
is	O
the	O
context	O
of	O
m	O
i	O
,	O
Y	O
i	O
is	O
the	O
set	O
of	O
candidate	O
types	O
of	O
m	O
i	O
.	O
Note	O
that	O
types	O
in	O
Y	O
i	O
can	O
form	O
one	O
or	O
more	O
type	O
paths	O
.	O
In	O
addition	O
,	O
we	O
denote	O
all	O
terminal	O
(	O
leaf	O
)	O
types	O
of	O
each	O
type	O
path	O
in	O
Y	O
i	O
as	O
the	O
target	O
type	O
set	O
Y	O
t	O
i	O
(	O
e.g.	O
,	O
for	O
Y	O
i	O
=	O
{	O
artist	O
,	O
teacher	O
,	O
person	O
}	O
,	O
Y	O
t	O
i	O
=	O
{	O
artist	O
,	O
teacher	O
}	O
)	O
.	O
This	O
setting	O
is	O
also	O
adopted	O
by	O
(	O
Xu	O
and	O
Barbosa	O
,	O
2018	O
)	O
.	O
As	O
each	O
entity	O
in	O
KB	O
can	O
have	O
several	O
type	O
paths	O
,	O
out	O
-	O
of	O
-	O
context	O
noise	O
may	O
exist	O
when	O
Y	O
i	O
contains	O
type	O
paths	O
that	O
are	O
irrelevant	O
to	O
m	O
i	O
in	O
context	O
c	O
i	O
.	O
In	O
this	O
work	O
,	O
we	O
argue	O
triples	O
where	O
Y	O
i	O
contains	O
only	O
one	O
type	O
path	O
(	O
i.e.	O
,	O
|	O
Y	O
t	O
i	O
|	O
=	O
1	O
)	O
as	O
clean	O
data	O
.	O
Other	O
triples	O
are	O
treated	O
as	O
noisy	O
data	O
,	O
where	O
Y	O
i	O
contains	O
both	O
the	O
true	O
type	O
path	O
and	O
irrel	O
-	O
evant	O
type	O
paths	O
.	O
Noisy	O
data	O
usually	O
takes	O
a	O
considerable	O
portion	O
of	O
the	O
entire	O
dataset	O
.	O
The	O
major	O
challenge	O
for	O
distantly	O
supervised	O
typing	O
systems	O
is	O
to	O
incorporate	O
both	O
clean	O
and	O
noisy	O
data	O
to	O
train	O
high	O
-	O
quality	O
type	O
classifiers	O
.	O

We	O
evaluate	O
our	O
method	O
on	O
two	O
standard	O
benchmarks	O
:	O
OntoNotes	B-DatasetName
and	O
BBN	O
:	O
OntoNotes	B-DatasetName
:	O
The	O
OntoNotes	B-DatasetName
dataset	O
is	O
composed	O
of	O
sentences	O
from	O
the	O
Newswire	O
part	O
of	O
OntoNotes	B-DatasetName
corpus	O
(	O
Weischedel	O
et	O
al	O
,	O
2013	O
)	O
.	O
(	O
Gillick	O
et	O
al	O
,	O
2014	O
)	O
annotated	O
the	O
training	O
part	O
with	O
the	O
aid	O
of	O
DBpedia	B-DatasetName
spotlight	O
(	O
Daiber	O
et	O
al	O
,	O
2013	O
)	O
,	O
while	O
the	O
test	O
data	O
is	O
manually	O
annotated	O
.	O
BBN	O
:	O
The	O
BBN	O
dataset	O
is	O
composed	O
of	O
sentences	O
from	O
Wall	O
Street	O
Journal	O
articles	O
and	O
is	O
manually	O
annotated	O
by	O
(	O
Weischedel	O
and	O
Brunstein	O
,	O
2005	O
)	O
.	O
(	O
Ren	O
et	O
al	O
,	O
2016a	O
)	O
regenerated	O
the	O
training	O
corpus	O
via	O
distant	O
supervision	O
.	O
In	O
this	O
work	O
we	O
use	O
the	O
preprocessed	O
datasets	O
provided	O
by	O
(	O
Abhishek	O
et	O
al	O
,	O
2017	O
;	O
Xu	O
and	O
Barbosa	O
,	O
2018	O
)	O
.	O
Table	O
2	O
shows	O
detailed	O
statistics	O
of	O
the	O
datasets	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
new	O
method	O
for	O
distantly	O
supervised	O
fine	O
-	O
grained	O
entity	B-TaskName
typing	I-TaskName
,	O
which	O
leverages	O
imperfect	O
annotations	O
as	O
model	O
regularization	O
via	O
Compact	O
Latent	O
Space	O
Clustering	O
(	O
CLSC	O
)	O
.	O
Experiments	O
on	O
two	O
standard	O
benchmarks	O
demonstrate	O
that	O
our	O
method	O
consistently	O
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
.	O
Further	O
study	O
reveals	O
our	O
method	O
is	O
more	O
robust	O
than	O
the	O
former	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
approach	O
as	O
the	O
portion	O
of	O
noisy	O
data	O
rises	O
.	O
The	O
proposed	O
method	O
is	O
general	O
for	O
other	O
tasks	O
with	O
imperfect	O
annotation	O
.	O
As	O
a	O
part	O
of	O
future	O
investigation	O
,	O
we	O
plan	O
to	O
apply	O
the	O
approach	O
to	O
other	O
distantly	O
supervised	O
tasks	O
,	O
such	O
as	O
relation	B-TaskName
extraction	I-TaskName
.	O

Complementary	O
Evidence	O
Identification	O
in	O
Open	B-TaskName
-	I-TaskName
Domain	I-TaskName
Question	I-TaskName
Answering	I-TaskName

This	O
paper	O
proposes	O
a	O
new	O
problem	O
of	O
complementary	O
evidence	O
identification	O
for	O
opendomain	O
question	B-TaskName
answering	I-TaskName
(	O
QA	O
)	O
.	O
The	O
problem	O
aims	O
to	O
efficiently	O
find	O
a	O
small	O
set	O
of	O
passages	O
that	O
covers	O
full	O
evidence	O
from	O
multiple	O
aspects	O
as	O
to	O
answer	O
a	O
complex	O
question	O
.	O
To	O
this	O
end	O
,	O
we	O
proposes	O
a	O
method	O
that	O
learns	O
vector	O
representations	O
of	O
passages	O
and	O
models	O
the	O
sufficiency	O
and	O
diversity	O
within	O
the	O
selected	O
set	O
,	O
in	O
addition	O
to	O
the	O
relevance	O
between	O
the	O
question	O
and	O
passages	O
.	O
Our	O
experiments	O
demonstrate	O
that	O
our	O
method	O
considers	O
the	O
dependence	O
within	O
the	O
supporting	O
evidence	O
and	O
significantly	O
improves	O
the	O
accuracy	B-MetricName
of	O
complementary	O
evidence	O
selection	O
in	O
QA	O
domain	O
.	O

Vector	O
Space	O
Modeling	O
We	O
apply	O
BERT	B-MethodName
model	O
to	O
estimate	O
the	O
likelihood	O
of	O
a	O
paragraph	O
p	O
being	O
the	O
supporting	O
evidence	O
to	O
the	O
question	O
q	O
,	O
denoted	O
as	O
P	O
(	O
p	O
|	O
q	O
)	O
.	O
Let	O
q	O
and	O
p	O
i	O
denote	O
the	O
input	O
texts	O
of	O
a	O
question	O
and	O
a	O
passage	O
.	O
We	O
feed	O
q	O
and	O
the	O
concatenation	O
of	O
q	O
and	O
p	O
i	O
into	O
the	O
BERT	B-MethodName
model	O
,	O
and	O
use	O
the	O
hidden	O
states	O
of	O
the	O
last	O
layer	O
to	O
represent	O
q	O
and	O
p	O
i	O
in	O
vector	O
space	O
,	O
denoted	O
as	O
q	O
and	O
p	O
i	O
respectively	O
.	O
A	O
fully	O
connected	O
layer	O
f	O
(	O
)	O
followed	O
by	O
sigmoid	B-MethodName
activation	I-MethodName
is	O
added	O
to	O
the	O
end	O
of	O
the	O
BERT	B-MethodName
model	O
,	O
and	O
outputs	O
a	O
scalar	O
P	O
(	O
p	O
i	O
|	O
q	O
)	O
to	O
estimate	O
how	O
relevant	O
the	O
paragraph	O
p	O
i	O
is	O
to	O
the	O
question	O
.	O
Note	O
that	O
in	O
our	O
implementation	O
p	O
i	O
is	O
based	O
on	O
both	O
q	O
and	O
p	O
i	O
,	O
but	O
we	O
omit	O
the	O
condition	O
on	O
q	O
for	O
simplicity	O
.	O
Complementary	O
Conditions	O
Previous	O
works	O
extract	O
evidence	O
paragraphs	O
according	O
to	O
P	O
(	O
p	O
|	O
q	O
)	O
,	O
which	O
is	O
estimated	O
on	O
each	O
passage	O
separately	O
without	O
considering	O
the	O
dependency	O
among	O
selected	O
paragraphs	O
.	O
To	O
extract	O
complementary	O
evidence	O
,	O
we	O
propose	O
that	O
the	O
selected	O
passages	O
P	O
sel	O
should	O
satisfy	O
the	O
following	O
conditions	O
that	O
intuitively	O
encourage	O
each	O
selected	O
passage	O
to	O
be	O
a	O
basis	O
to	O
support	O
the	O
question	O
:	O
Relevancy	O
:	O
P	O
sel	O
should	O
have	O
a	O
high	O
probability	O
of	O
p	O
i	O
P	O
sel	O
P	O
(	O
p	O
i	O
|	O
q	O
)	O
;	O
Diversity	O
:	O
P	O
sel	O
should	O
cover	O
passages	O
as	O
diverse	O
as	O
possible	O
,	O
which	O
can	O
be	O
measured	O
by	O
the	O
average	O
distance	O
between	O
any	O
pairs	O
in	O
P	O
sel	O
,	O
e.g.	O
,	O
maximizing	O
i	O
,	O
j	O
{	O
i	O
,	O
j	O
|	O
p	O
i	O
,	O
p	O
j	O
P	O
sel	O
,	O
i	O
=	O
j	O
}	O
1	O
(	O
p	O
i	O
,	O
p	O
j	O
)	O
.	O
Here	O
1	O
(	O
,	O
)	O
denotes	O
L	O
1	O
distance	O
;	O
Compactness	O
:	O
P	O
sel	O
should	O
optimize	O
the	O
aforementioned	O
conditions	O
while	O
the	O
size	O
being	O
minimal	O
.	O
In	O
this	O
work	O
we	O
constrain	O
the	O
compactness	O
by	O
fixing	O
|	O
P	O
sel	O
|	O
and	O
meanwhile	O
maximizing	O
cos	O
(	O
i	O
{	O
i	O
|	O
p	O
i	O
P	O
sel	O
}	O
p	O
i	O
,	O
q	O
)	O
.	O
We	O
use	O
cos	O
(	O
,	O
)	O
to	O
encourage	O
the	O
collection	O
of	O
evidence	O
covers	O
what	O
needed	O
by	O
the	O
question	O
.	O

Datasets	O
Considering	O
the	O
prerequisite	O
of	O
sentence	O
-	O
level	O
evidence	O
annotations	O
,	O
we	O
evaluate	O
our	O
approach	O
on	O
two	O
datasets	O
,	O
a	O
synthetic	O
dataset	O
MNLI	B-DatasetName
-	O
12	O
and	O
a	O
real	O
application	O
HotpotQA	B-DatasetName
-	O
50	O
.	O
Data	O
sampling	O
is	O
detailed	O
in	O
Appendix	O
B.	O
MNLI	B-DatasetName
-	O
12	O
is	O
constructed	O
based	O
on	O
the	O
textual	O
entailment	O
dataset	O
MNLI	B-DatasetName
(	O
Williams	O
et	O
al	O
,	O
2018	O
)	O
,	O
in	O
order	O
to	O
verify	O
the	O
ability	O
of	O
our	O
method	O
in	O
finding	O
complementary	O
evidence	O
.	O
In	O
original	O
MNLI	B-DatasetName
,	O
each	O
premise	O
sentence	O
corresponds	O
to	O
three	O
hypotheses	O
sentences	O
:	O
entailment	O
,	O
neutral	O
and	O
contradiction	O
.	O
To	O
generate	O
complementary	O
pairs	O
for	O
each	O
premise	O
sentence	O
,	O
we	O
split	O
each	O
hypothesis	O
sentence	O
into	O
two	O
segments	O
.	O
The	O
goal	O
is	O
to	O
find	O
the	O
segment	O
combination	O
that	O
entails	O
premise	O
sentence	O
,	O
and	O
our	O
dataset	O
,	O
by	O
definition	O
,	O
ensures	O
that	O
only	O
the	O
combination	O
of	O
two	O
segments	O
from	O
the	O
entailment	O
hypothesis	O
can	O
entail	O
the	O
premise	O
,	O
not	O
any	O
of	O
its	O
subset	O
or	O
other	O
combinations	O
.	O
The	O
original	O
train	O
/	O
dev	O
/	O
test	O
splits	O
from	O
MNLI	B-DatasetName
are	O
used	O
.	O
HotpotQA	B-DatasetName
-	O
50	O
is	O
based	O
on	O
the	O
open	O
-	O
domain	O
setting	O
of	O
the	O
multi	O
-	O
hop	O
QA	O
benchmark	O
HotpotQA	B-DatasetName
(	O
Yang	O
et	O
al	O
,	O
2018	O
)	O
.	O
The	O
original	O
task	O
requires	O
to	O
find	O
evidence	O
passages	O
from	O
abstract	O
paragraphs	O
of	O
all	O
Wikipedia	O
pages	O
to	O
support	O
a	O
multi	O
-	O
hop	O
question	O
.	O
For	O
each	O
q	O
,	O
we	O
collect	O
50	O
relevant	O
passages	O
based	O
on	O
bigram	O
BM25	O
(	O
Godbole	O
et	O
al	O
,	O
2019	O
)	O
.	O
Two	O
positive	O
evidence	O
passages	O
to	O
each	O
question	O
are	O
provided	O
by	O
human	O
annotators	O
as	O
the	O
ground	O
truth	O
.	O
Note	O
that	O
there	O
is	O
no	O
guarantee	O
that	O
P	O
50	O
covers	O
both	O
evidence	O
passages	O
here	O
.	O
We	O
use	O
the	O
original	O
development	O
set	O
from	O
HotpotQA	B-DatasetName
as	O
our	O
test	O
set	O
and	O
randomly	O
split	O
a	O
subset	O
from	O
the	O
original	O
training	O
set	O
as	O
our	O
development	O
set	O
.	O

Data	O
:	O
Vector	O
representation	O
of	O
question	O
(	O
q	O
)	O
,	O
vector	O
representation	O
of	O
all	O
the	O
N	O
passages	O
{	O
pn	O
}	O
(	O
{	O
pn	O
}	O
)	O
;	O
the	O
maximum	O
number	O
of	O
passage	O
to	O
select	O
(	O
L	O
)	O
;	O
the	O
beam	O
size	O
(	O
M	O
)	O
;	O
a	O
vector	O
of	O
weights	O
for	O
all	O
regularization	O
terms	O
λ	O
.	O
Result	O
:	O
The	O
top	O
ranked	O
complementary	O
passages	O
.	O
/	O
*	O
Predict	O
the	O
probability	O
P	O
(	O
p	O
i	O
)	O
of	O
being	O
a	O
supporting	O
passage	O
for	O
each	O
passage	O
B	O
Data	O
Sampling	O
In	O
original	O
MNLI	B-DatasetName
,	O
each	O
premise	O
sentence	O
P	O
corresponds	O
to	O
one	O
entailment	O
EP	O
,	O
one	O
neutral	O
NP	O
and	O
one	O
contradiction	O
CP	O
.	O
We	O
take	O
the	O
premise	O
P	O
as	O
q	O
,	O
and	O
split	O
each	O
of	O
its	O
corresponding	O
hypotheses	O
into	O
two	O
segments	O
with	O
a	O
random	O
cutting	O
point	O
near	O
the	O
middle	O
of	O
the	O
sentence	O
,	O
resulting	O
in	O
a	O
total	O
of	O
6	O
segments	O
{	O
E	O
1	O
P	O
,	O
E	O
2	O
P	O
,	O
N	O
1	O
P	O
,	O
N	O
2	O
P	O
,	O
C	O
1	O
P	O
,	O
C	O
2	O
P	O
}	O
.	O
Mixing	O
them	O
with	O
the	O
6	O
segments	O
corresponding	O
to	O
another	O
premise	O
X	O
,	O
we	O
can	O
finally	O
have	O
P	O
+	O
=	O
{	O
E	O
1	O
P	O
,	O
E	O
2	O
P	O
}	O
and	O
P	O
−	O
=	O
{	O
N	O
1	O
P	O
,	O
N	O
2	O
P	O
,	O
C	O
1	O
P	O
,	O
C	O
2	O
P	O
,	O
E	O
1	O
X	O
,	O
E	O
2	O
X	O
,	O
N	O
1	O
X	O
,	O
N	O
2	O
X	O
,	O
C	O
1	O
X	O
,	O
C	O
2	O
X	O
}	O
.	O
Consequently	O
,	O
we	O
sample	O
one	O
positive	O
and	O
eight	O
negative	O
pairs	O
respectively	O
from	O
P	O
+	O
and	O
P	O
−	O
.	O
A	O
pair	O
like	O
{	O
E	O
1	O
P	O
,	O
C	O
2	O
X	O
}	O
is	O
considered	O
as	O
negative	O
.	O
To	O
ensure	O
the	O
segments	O
are	O
literally	O
meaningful	O
,	O
each	O
segment	O
is	O
guaranteed	O
to	O
be	O
longer	O
than	O
5	O
words	O
.	O
p	O
i	O
given	O
q	O
*	O
/	O
1	O
for	O
i	O
[	O
1	O
,	O
N	O
]	O
do	O
2	O
P	O
(	O
p	O
i	O
)	O
f	O
(	O
q	O
,	O
p	O
i	O
)	O
;	O
HotpotQA	B-DatasetName
In	O
HotpotQA	B-DatasetName
,	O
the	O
true	O
supporting	O
paragraphs	O
of	O
each	O
question	O
q	O
are	O
given	O
.	O
Therefore	O
,	O
we	O
can	O
easily	O
form	O
P	O
+	O
and	O
P	O
−	O
and	O
sample	O
positive	O
and	O
negative	O
pairs	O
of	O
paragraphs	O
respectively	O
from	O
P	O
+	O
and	O
P	O
−	O
.	O
A	O
special	O
pair	O
that	O
contains	O
one	O
true	O
supporting	O
paragraph	O
and	O
one	O
non	O
-	O
supporting	O
paragraph	O
is	O
considered	O
as	O
a	O
negative	O
pair	O
.	O

Domain	B-TaskName
adaptation	I-TaskName
in	O
practice	O
:	O
Lessons	O
from	O
a	O
real	O
-	O
world	O
information	O
extraction	O
pipeline	O

Advances	O
in	O
transfer	B-TaskName
learning	I-TaskName
and	O
domain	B-TaskName
adaptation	I-TaskName
have	O
raised	O
hopes	O
that	O
oncechallenging	O
NLP	O
tasks	O
are	O
ready	O
to	O
be	O
put	O
to	O
use	O
for	O
sophisticated	O
information	O
extraction	O
needs	O
.	O
In	O
this	O
work	O
,	O
we	O
describe	O
an	O
effort	O
to	O
do	O
just	O
that	O
-	O
combining	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
neural	O
methods	O
for	O
negation	B-TaskName
detection	I-TaskName
,	O
document	O
time	O
relation	B-TaskName
extraction	I-TaskName
,	O
and	O
aspectual	O
link	B-TaskName
prediction	I-TaskName
,	O
with	O
the	O
eventual	O
goal	O
of	O
extracting	O
drug	O
timelines	O
from	O
electronic	O
health	O
record	O
text	O
.	O
We	O
train	O
on	O
the	O
THYME	O
colon	O
cancer	O
corpus	O
and	O
test	O
on	O
both	O
the	O
THYME	O
brain	O
cancer	O
corpus	O
and	O
an	O
internal	O
corpus	O
,	O
and	O
show	O
that	O
performance	O
of	O
the	O
combined	O
systems	O
is	O
unacceptable	O
despite	O
good	O
performance	O
of	O
individual	O
systems	O
.	O
Although	O
domain	B-TaskName
adaptation	I-TaskName
shows	O
improvements	O
on	O
each	O
individual	O
system	O
,	O
the	O
model	B-TaskName
selection	I-TaskName
problem	O
is	O
a	O
barrier	O
to	O
improving	O
overall	O
pipeline	O
performance	O
.	O

We	O
began	O
this	O
work	O
by	O
developing	O
several	O
NLP	O
components	O
necessary	O
to	O
extract	O
drug	O
temporality	O
signatures	O
,	O
including	O
negation	B-TaskName
detection	I-TaskName
,	O
relation	O
to	O
document	O
creation	O
time	O
(	O
DocTimeRel	O
)	O
,	O
and	O
aspectual	O
link	O
extraction	O
(	O
ALINK	O
)	O
,	O
all	O
detailed	O
below	O
.	O
Detecting	O
negation	O
helps	O
us	O
avoid	O
false	O
positives	O
from	O
mentions	O
corresponding	O
to	O
,	O
for	O
example	O
,	O
decisions	O
to	O
not	O
use	O
a	O
drug	O
.	O
DocTimeRel	O
helps	O
us	O
distinguish	O
mentions	O
of	O
drugs	O
that	O
are	O
current	O
from	O
those	O
that	O
predate	O
the	O
current	O
time	O
period	O
,	O
or	O
are	O
being	O
speculated	O
about	O
for	O
future	O
use	O
.	O
ALINK	O
can	O
model	O
drug	O
start	O
,	O
stop	O
,	O
and	O
continuation	O
events	O
,	O
which	O
can	O
help	O
to	O
distinguish	O
whether	O
a	O
missing	O
mention	O
in	O
the	O
middle	O
of	O
a	O
record	O
corresponds	O
to	O
a	O
stop	O
and	O
restart	O
,	O
or	O
an	O
incidentally	O
omitted	O
mention	O
.	O
Figure	O
1	O
shows	O
an	O
example	O
instance	O
of	O
a	O
drug	O
mention	O
to	O
be	O
classified	O
for	O
all	O
three	O
tasks	O
.	O
The	O
THYME	O
dataset	O
(	O
Styler	O
IV	O
et	O
al	O
,	O
2014	O
)	O
,	O
released	O
as	O
part	O
of	O
Clinical	O
TempEval	O
,	O
contains	O
all	O
three	O
of	O
these	O
annotation	O
types	O
,	O
on	O
1200	O
notes	O
of	O
patients	O
with	O
colon	O
and	O
brain	O
cancer	O
.	O
We	O
train	O
all	O
models	O
on	O
the	O
colon	O
cancer	O
section	O
(	O
details	O
on	O
data	O
are	O
in	O
Section	O
4	O
)	O
.	O
While	O
our	O
bigger	O
project	O
is	O
specific	O
to	O
drug	O
mentions	O
,	O
the	O
problem	O
is	O
not	O
limited	O
to	O
drug	O
mentions	O
,	O
so	O
we	O
train	O
and	O
evaluate	O
on	O
all	O
annotated	O
events	O
in	O
the	O
THYME	O
corpus	O
.	O
We	O
also	O
assume	O
that	O
events	O
are	O
given	O
,	O
to	O
allow	O
a	O
straightforward	O
metric	O
of	O
how	O
many	O
events	O
we	O
"	O
get	O
right	O
"	O
when	O
combining	O
all	O
property	O
predictions	O
.	O
In	O
the	O
real	O
world	O
,	O
events	O
will	O
have	O
to	O
be	O
automatically	O
detected	O
,	O
so	O
our	O
metric	O
will	O
be	O
an	O
upper	O
bound	O
on	O
how	O
often	O
the	O
combined	O
models	O
get	O
everything	O
correct	O
.	O

DocTimeRel	O
classification	O
is	O
the	O
task	O
of	O
relating	O
an	O
event	O
to	O
the	O
document	O
creation	O
time	O
.	O
The	O
categories	O
are	O
BEFORE	O
,	O
OVERLAP	O
,	O
AFTER	O
,	O
and	O
BE	O
-	O
FORE	O
/	O
OVERLAP	O
.	O
As	O
above	O
,	O
we	O
model	O
this	O
as	O
a	O
span	O
-	O
in	O
-	O
context	O
classification	O
,	O
and	O
we	O
again	O
compare	O
a	O
feature	O
-	O
based	O
approach	O
with	O
a	O
RoBERTabased	O
approach	O
.	O
The	O
feature	O
-	O
based	O
approach	O
again	O
uses	O
the	O
default	O
cTAKES	O
SVM	B-MethodName
-	O
based	O
implementation	O
(	O
Lin	O
et	O
al	O
,	O
2016	O
)	O
,	O
with	O
features	O
based	O
on	O
bags	O
of	O
words	O
in	O
and	O
around	O
the	O
event	O
,	O
and	O
verb	O
tense	O
information	O
for	O
verbs	O
on	O
either	O
side	O
of	O
the	O
event	O
.	O
We	O
train	O
a	O
separate	O
RoBERTa	B-MethodName
-	O
based	O
model	O
with	O
the	O
same	O
architecture	O
as	O
the	O
negation	O
model	O
,	O
with	O
the	O
only	O
difference	O
being	O
that	O
the	O
output	O
layer	O
is	O
a	O
softmax	B-MethodName
over	O
the	O
four	O
categories	O
rather	O
than	O
a	O
sigmoid	O
.	O

Aspectual	O
link	O
extraction	O
(	O
ALINK	O
)	O
is	O
the	O
task	O
of	O
classifying	O
whether	O
an	O
event	O
mention	O
is	O
related	O
to	O
an	O
aspectual	O
temporal	O
modifier	O
,	O
for	O
example	O
,	O
discontinued	O
.	O
This	O
is	O
annotated	O
as	O
a	O
relation	O
between	O
an	O
event	O
and	O
a	O
modifier	O
,	O
but	O
we	O
model	O
it	O
as	O
an	O
event	O
property	O
classification	O
task	O
since	O
each	O
event	O
can	O
only	O
participate	O
in	O
one	O
type	O
of	O
relation	O
.	O
The	O
set	O
of	O
possible	O
labels	O
is	O
INITIATES	O
,	O
CONTINUES	O
,	O
TERMINATES	O
,	O
and	O
REINITIATES	O
.	O
We	O
are	O
not	O
aware	O
of	O
any	O
existing	O
open	O
-	O
source	O
models	O
for	O
this	O
task	O
,	O
so	O
for	O
our	O
feature	O
-	O
based	O
baseline	O
we	O
train	O
a	O
model	O
with	O
the	O
same	O
SVM	B-MethodName
classification	O
approach	O
and	O
feature	O
set	O
as	O
the	O
DocTimeRel	O
model	O
in	O
cTAKES	O
.	O
We	O
did	O
not	O
perform	O
extensive	O
feature	B-TaskName
engineering	I-TaskName
for	O
this	O
task	O
,	O
so	O
further	O
gains	O
in	O
the	O
SVM	B-MethodName
system	O
are	O
probably	O
possible	O
.	O
For	O
the	O
RoBERTa	B-MethodName
-	O
based	O
model	O
,	O
we	O
used	O
the	O
same	O
architecture	O
as	O
both	O
systems	O
above	O
,	O
with	O
a	O
softmax	B-MethodName
over	O
the	O
5	O
categories	O
-	O
the	O
4	O
ALINK	O
categories	O
above	O
as	O
well	O
as	O
NONE	O
,	O
indicating	O
a	O
drug	O
mention	O
does	O
not	O
participate	O
in	O
any	O
ALINK	O
relation	O
.	O
NONE	O
is	O
by	O
far	O
the	O
most	O
common	O
category	O
.	O

The	O
results	O
also	O
show	O
that	O
combining	O
NLP	O
systems	O
for	O
new	O
,	O
complex	O
,	O
information	O
needs	O
is	O
likely	O
to	O
run	O
into	O
issues	O
even	O
when	O
individual	O
systems	O
perform	O
well	O
.	O
In	O
particular	O
,	O
our	O
experiments	O
raise	O
questions	O
about	O
real	O
-	O
world	O
use	O
of	O
domain	B-TaskName
adaptation	I-TaskName
.	O
If	O
we	O
treated	O
THYME	O
colon	O
and	O
brain	O
sets	O
as	O
representative	O
in	O
-	O
domain	O
and	O
out	O
-	O
of	O
-	O
domain	O
datasets	O
we	O
would	O
select	O
RoBERTa	B-MethodName
or	O
RoBERTa+LM	O
for	O
everything	O
.	O
But	O
an	O
oracle	O
optimizing	O
PH	O
performance	O
would	O
tell	O
us	O
to	O
use	O
the	O
SVM	B-MethodName
for	O
negation	O
and	O
DocTimeRel	O
and	O
RoBERTa+LM	O
for	O
ALINK	O
.	O
One	O
of	O
the	O
difficulties	O
in	O
even	O
studying	O
domain	B-TaskName
adaptation	I-TaskName
is	O
model	B-TaskName
selection	I-TaskName
-	O
if	O
labeled	O
target	O
data	O
is	O
not	O
available	O
,	O
standard	O
practices	O
like	O
tuning	O
on	O
held	O
out	O
data	O
are	O
impossible	O
.	O
But	O
the	O
reality	O
our	O
results	O
suggest	O
is	O
that	O
different	O
algorithms	O
work	O
well	O
on	O
different	O
tasks	O
and	O
datasets	O
,	O
and	O
selecting	O
the	O
best	O
model	O
for	O
each	O
task	O
is	O
an	O
unsolved	O
and	O
under	O
-	O
studied	O
problem	O
.	O
One	O
direction	O
of	O
research	O
that	O
may	O
address	O
these	O
concerns	O
is	O
on	O
better	O
modeling	O
of	O
domains	O
themselves	O
.	O
The	O
problem	O
has	O
been	O
exacerbated	O
with	O
the	O
move	O
from	O
feature	O
-	O
based	O
classifiers	O
to	O
pre	O
-	O
trained	O
black	O
box	O
models	O
,	O
as	O
it	O
is	O
now	O
even	O
more	O
difficult	O
to	O
understand	O
the	O
cause	O
of	O
errors	O
in	O
new	O
domains	O
without	O
interpretable	O
features	O
.	O
Domain	B-TaskName
adaptation	I-TaskName
should	O
leverage	O
"	O
BERTology	O
"	O
and	O
interpretability	O
research	O
to	O
help	O
understand	O
how	O
different	O
aspects	O
of	O
domains	O
contribute	O
to	O
performance	O
differences	O
.	O
For	O
example	O
,	O
in	O
clinical	O
notes	O
,	O
variation	O
in	O
institutions	O
,	O
specialties	O
,	O
note	O
types	O
,	O
authors	O
,	O
etc	O
.	O
,	O
all	O
probably	O
contribute	O
differently	O
to	O
domain	O
shift	O
,	O
and	O
these	O
sources	O
of	O
variation	O
should	O
be	O
empirically	O
explored	O
.	O
Future	O
work	O
will	O
explore	O
this	O
direction	O
to	O
develop	O
unsupervised	O
model	B-TaskName
selection	I-TaskName
algorithms	O
that	O
better	O
predict	O
target	O
domain	O
performance	O
.	O

XGLUE	B-DatasetName
:	O
A	O
New	O
Benchmark	O
Dataset	O
for	O
Cross	O
-	O
lingual	O
Pre	O
-	O
training	O
,	O
Understanding	O
and	O
Generation	O

In	O
this	O
paper	O
,	O
we	O
introduce	O
XGLUE	B-DatasetName
,	O
a	O
new	O
benchmark	O
dataset	O
that	O
can	O
be	O
used	O
to	O
train	O
large	O
-	O
scale	O
cross	O
-	O
lingual	O
pre	O
-	O
trained	O
models	O
using	O
multilingual	O
and	O
bilingual	O
corpora	O
and	O
evaluate	O
their	O
performance	O
across	O
a	O
diverse	O
set	O
of	O
cross	O
-	O
lingual	O
tasks	O
.	O
Comparing	O
to	O
GLUE	B-DatasetName
(	O
Wang	O
et	O
al	O
,	O
2019	O
)	O
,	O
which	O
is	O
labeled	O
in	O
English	O
for	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
tasks	O
only	O
,	O
XGLUE	B-DatasetName
has	O
two	O
main	O
advantages	O
:	O
(	O
1	O
)	O
it	O
provides	O
11	O
diversified	O
tasks	O
that	O
cover	O
both	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
and	O
generation	O
scenarios	O
;	O
(	O
2	O
)	O
for	O
each	O
task	O
,	O
it	O
provides	O
labeled	O
data	O
in	O
multiple	O
languages	O
.	O
We	O
extend	O
a	O
recent	O
cross	O
-	O
lingual	O
pre	O
-	O
trained	O
model	O
Unicoder	O
to	O
cover	O
both	O
understanding	O
and	O
generation	O
tasks	O
,	O
which	O
is	O
evaluated	O
on	O
XGLUE	B-DatasetName
as	O
a	O
strong	O
baseline	O
.	O
We	O
also	O
evaluate	O
the	O
base	O
versions	O
(	O
12	O
-	O
layer	O
)	O
of	O
Multilingual	O
BERT	B-MethodName
,	O
XLM	B-MethodName
and	O
XLM	B-MethodName
-	O
R	O
for	O
comparison	O
.	O
1	O

Pre	O
-	O
training	O
+	O
Fine	O
-	O
tuning	O
has	O
become	O
a	O
new	O
NLP	O
paradigm	O
,	O
where	O
the	O
general	B-TaskName
knowledge	I-TaskName
are	O
firstly	O
learnt	O
from	O
large	O
-	O
scale	O
corpus	O
by	O
self	B-TaskName
-	I-TaskName
supervised	I-TaskName
learning	I-TaskName
and	O
then	O
transferred	O
to	O
downstream	O
tasks	O
by	O
task	O
-	O
specific	O
fine	O
-	O
tuning	O
.	O
Three	O
different	O
types	O
of	O
pre	O
-	O
trained	O
models	O
are	O
explored	O
recently	O
,	O
including	O
monolingual	O
pre	O
-	O
trained	O
models	O
(	O
Radford	O
et	O
al	O
,	O
2018	O
;	O
Devlin	O
et	O
al	O
,	O
2019	O
;	O
Yang	O
et	O
al	O
,	O
2019b	O
;	O
Lewis	O
et	O
al	O
,	O
2019a	O
)	O
,	O
multilingual	O
and	O
cross	O
-	O
lingual	O
pre	O
-	O
trained	O
models	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
;	O
Conneau	O
and	O
Lample	O
,	O
2019	O
;	O
and	O
multimodal	O
pre	O
-	O
trained	O
models	O
(	O
Lu	O
et	O
al	O
,	O
2019	O
;	O
Li	O
et	O
al	O
,	O
2020	O
;	O
.	O
In	O
this	O
paper	O
,	O
we	O
focus	O
on	O
the	O
cross	O
-	O
lingual	O
pretrained	O
models	O
,	O
due	O
to	O
their	O
importance	O
to	O
alleviating	O
the	O
low	O
-	O
resource	O
issue	O
among	O
languages	O
,	O
where	O
an	O
NLP	O
task	O
often	O
has	O
rich	O
training	O
data	O
in	O
one	O
language	O
(	O
such	O
as	O
English	O
)	O
but	O
has	O
few	O
or	O
no	O
training	O
data	O
in	O
other	O
languages	O
(	O
such	O
as	O
French	O
and	O
German	O
)	O
.	O
In	O
order	O
to	O
further	O
advance	O
the	O
development	O
of	O
cross	O
-	O
lingual	O
pre	O
-	O
trained	O
models	O
for	O
various	O
downstream	O
tasks	O
in	O
different	O
languages	O
,	O
this	O
paper	O
introduces	O
XGLUE	B-DatasetName
,	O
a	O
new	O
benchmark	O
dataset	O
that	O
can	O
be	O
used	O
to	O
:	O
(	O
i	O
)	O
train	O
large	O
-	O
scale	O
cross	O
-	O
lingual	O
pre	O
-	O
trained	O
models	O
using	O
multilingual	O
and	O
bilingual	O
corpora	O
,	O
(	O
ii	O
)	O
evaluate	O
generalization	O
capabilities	O
of	O
the	O
cross	O
-	O
lingual	O
pre	O
-	O
trained	O
models	O
across	O
a	O
diverse	O
set	O
of	O
cross	O
-	O
lingual	O
tasks	O
.	O
The	O
contribution	O
of	O
XGLUE	B-DatasetName
is	O
two	O
-	O
fold	O
.	O
First	O
,	O
it	O
provides	O
11	O
diversified	O
cross	O
-	O
lingual	O
tasks	O
covering	O
both	O
understanding	O
and	O
generation	O
scenarios	O
.	O
XTREME	B-DatasetName
)	O
is	O
a	O
concurrent	O
work	O
of	O
XGLUE	B-DatasetName
.	O
But	O
it	O
includes	O
cross	O
-	O
lingual	O
understanding	O
tasks	O
only	O
.	O
Besides	O
,	O
XGLUE	B-DatasetName
introduces	O
6	O
new	O
tasks	O
selected	O
from	O
Search	O
,	O
Ads	O
and	O
News	O
scenarios	O
,	O
which	O
makes	O
XGLUE	B-DatasetName
have	O
more	O
practical	O
values	O
.	O
Second	O
,	O
an	O
extended	O
version	O
of	O
Unicoder	O
is	O
described	O
and	O
evaluated	O
as	O
a	O
strong	O
cross	O
-	O
lingual	O
pre	O
-	O
trained	O
model	O
baseline	O
on	O
XGLUE	B-DatasetName
for	O
both	O
understanding	O
and	O
generation	O
tasks	O
.	O
We	O
also	O
evaluate	O
the	O
base	O
versions	O
(	O
12	O
-	O
layer	O
)	O
of	O
Multilingual	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
XLM	B-MethodName
(	O
Conneau	O
and	O
Lample	O
,	O
2019	O
)	O
and	O
XLM	B-MethodName
-	O
R	O
for	O
comparison	O
.	O

Multilingual	O
Corpus	O
We	O
extract	O
raw	O
sentences	O
from	O
Wikipedia	O
using	O
WikiExtractor	O
.	O
It	O
leads	O
to	O
a	O
101	O
G	O
multilingual	O
corpus	O
covering	O
100	O
languages	O
.	O
Bilingual	O
Corpus	O
We	O
use	O
an	O
in	O
-	O
house	O
pipeline	O
to	O
extract	O
bilingual	O
sentence	O
pairs	O
from	O
the	O
Web	O
,	O
which	O
leads	O
to	O
a	O
146	O
G	O
bilingual	O
corpus	O
covering	O
27	O
languages	O
,	O
including	O
Arabic	O
,	O
Bulgarian	O
,	O
Danish	O
,	O
German	O
,	O
Greek	O
,	O
English	O
,	O
Spanish	O
,	O
Finnish	O
,	O
French	O
,	O
Hebrew	O
,	O
Hindi	O
,	O
Hungarian	O
,	O
Indonesian	O
,	O
Italian	O
,	O
Japanese	O
,	O
Korean	O
,	O
Dutch	O
,	O
Polish	O
,	O
Portuguese	O
,	O
Russian	O
,	O
Swedish	O
,	O
Swahili	O
,	O
Thai	O
,	O
Turkish	O
,	O
Urdu	B-DatasetName
,	O
Vietnamese	O
and	O
Chinese	O
.	O
All	O
the	O
bilingual	O
pairs	O
are	O
English	O
to	O
another	O
language	O
.	O

Multilingual	O
Corpus	O
Following	O
,	O
we	O
construct	O
a	O
clean	O
version	O
of	O
Common	B-DatasetName
Crawl	I-DatasetName
(	O
CC	O
)	O
2	O
as	O
the	O
multilingual	O
corpus	O
.	O
First	O
,	O
we	O
use	O
a	O
language	B-TaskName
identification	I-TaskName
model	O
trained	O
based	O
on	O
Wikipedia	O
to	O
classify	O
the	O
language	O
of	O
each	O
page	O
in	O
CC	O
.	O
Then	O
,	O
we	O
train	O
a	O
language	O
model	O
for	O
each	O
language	O
using	O
the	O
corresponding	O
part	O
of	O
the	O
Wikipedia	O
corpus	O
,	O
and	O
use	O
it	O
to	O
filter	O
documents	O
as	O
did	O
.	O
We	O
use	O
one	O
CC	O
dump	O
for	O
English	O
and	O
twelve	O
CC	O
dumps	O
for	O
other	O
languages	O
.	O
It	O
leads	O
to	O
a	O
2	O
,	O
500	O
G	O
multilingual	O
corpus	O
covering	O
89	O
languages	O
.	O
We	O
also	O
include	O
the	O
101	O
G	O
multilingual	O
corpus	O
described	O
in	O
Section	O
2.1.1	O
.	O

We	O
reuse	O
the	O
bilingual	O
corpus	O
described	O
in	O
Section	O
2.1.1	O
.	O
We	O
will	O
add	O
CCMatrix	B-DatasetName
(	O
Schwenk	O
et	O
al	O
,	O
2019	O
)	O
in	O
the	O
future	O
.	O

We	O
select	O
11	O
cross	O
-	O
lingual	O
tasks	O
in	O
XGLUE	B-DatasetName
,	O
which	O
are	O
categorized	O
into	O
3	O
groups	O
:	O
single	O
-	O
input	O
understanding	O
tasks	O
,	O
pair	O
-	O
input	O
understanding	O
tasks	O
,	O
and	O
generation	O
tasks	O
.	O
For	O
each	O
task	O
,	O
training	O
set	O
is	O
only	O
available	O
in	O
English	O
.	O
In	O
order	O
to	O
obtain	O
a	O
good	O
performance	O
on	O
XGLUE	B-DatasetName
,	O
a	O
model	O
should	O
be	O
able	O
to	O
learn	O
how	O
to	O
do	O
a	O
task	O
well	O
using	O
its	O
English	O
training	O
set	O
,	O
and	O
then	O
transfer	O
this	O
ability	O
to	O
test	O
sets	O
in	O
other	O
languages	O
.	O
Table	O
2	O
gives	O
the	O
dataset	O
statistics	O
and	O
Table	O
3	O
lists	O
languages	O
covered	O
by	O
all	O
tasks	O
.	O
2	O
https://commoncrawl.org/.	O

QG	O
M	O
-	O
BERT	B-MethodName
-	O
-	O
0.1	O
-	O
7.8	O
0.1	O
0.1	O
-	O
0.2	O
-	O
-	O
0.1	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
1.4	O
XLM	B-MethodName
-	O
Rbase	O
-	O
-	O
0.1	O
-	O
6.0	O
0.0	O
0.0	O
-	O
0.1	O
-	O
-	O
0.0	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
1	O
.	O
We	O
find	O
(	O
1	O
)	O
Unicoder	O
LC	O
performs	O
slightly	O
better	O
than	O
M	O
-	O
BERT	B-MethodName
and	O
XLM	B-MethodName
-	O
R	O
base	O
on	O
the	O
9	O
understanding	O
tasks	O
,	O
as	O
it	O
is	O
pre	O
-	O
trained	O
based	O
on	O
multilingual	O
and	O
bilingual	O
corpora	O
at	O
the	O
same	O
time	O
and	O
uses	O
TLM	O
;	O
.	O
But	O
it	O
is	O
not	O
a	O
fair	O
comparison	O
,	O
because	O
they	O
use	O
different	O
text	O
denoising	B-TaskName
tasks	O
(	O
sentence	O
prediction	O
vs.	O
span	O
prediction	O
)	O
and	O
different	O
generation	O
mechanisms	O
(	O
single	O
-	O
token	O
prediction	O
vs.	O
multi	O
-	O
token	O
prediction	O
)	O
.	O
We	O
leave	O
combining	O
these	O
two	O
tasks	O
for	O
future	O
work	O
.	O

We	O
define	O
pivot	O
-	O
language	O
(	O
pl	O
)	O
fine	O
-	O
tuning	O
as	O
finetune	O
a	O
pre	O
-	O
trained	O
model	O
for	O
a	O
downstream	O
task	O
using	O
its	O
labeled	O
data	O
in	O
a	O
pivot	O
language	O
(	O
e.g.	O
English	O
)	O
and	O
then	O
apply	O
the	O
fine	O
-	O
tuned	O
model	O
to	O
all	O
languages	O
.	O
Table	O
4	O
chooses	O
English	O
as	O
the	O
pivot	O
language	O
,	O
as	O
all	O
tasks	O
in	O
XGLUE	B-DatasetName
have	O
labeled	O
data	O
in	O
English	O
.	O
But	O
is	O
English	O
always	O
the	O
optimal	O
choice	O
?	O
Will	O
the	O
results	O
become	O
better	O
,	O
if	O
we	O
do	O
fine	O
-	O
tuning	O
using	O
other	O
pivot	O
languages	O
?	O
To	O
answer	O
these	O
questions	O
,	O
we	O
evaluate	O
Unicoder	O
on	O
XNLI	B-DatasetName
and	O
NTG	O
using	O
different	O
pivot	O
languages	O
in	O
fine	O
-	O
tuning	O
and	O
list	O
comparison	O
results	O
in	O
Table	O
5	O
and	O
Table	O
6	O
,	O
respectively	O
.	O
(	O
1	O
)	O
For	O
each	O
test	O
set	O
in	O
language	O
l	O
i	O
in	O
Table	O
5	O
and	O
Table	O
6	O
,	O
its	O
best	O
result	O
is	O
often	O
achieved	O
when	O
the	O
model	O
is	O
fine	O
-	O
tuned	O
using	O
l	O
i	O
as	O
the	O
pivot	O
language	O
;	O
(	O
2	O
)	O
For	O
XNLI	B-DatasetName
in	O
Table	O
5	O
,	O
the	O
best	O
pivot	O
languages	O
are	O
Spanish	O
(	O
es	O
)	O
,	O
Greek	O
(	O
el	O
)	O
and	O
Turkish	O
(	O
tr	O
)	O
,	O
rather	O
than	O
English	O
(	O
en	O
)	O
.	O
For	O
NTG	O
in	O
Table	O
6	O
,	O
the	O
best	O
pivot	O
language	O
is	O
French	O
(	O
fr	O
)	O
for	O
both	O
Unicoder	O
xDAE	O
SC	O
and	O
Unicoder	O
xF	O
N	O
P	O
SC	O
.	O
It	O
means	O
the	O
average	O
quality	O
of	O
a	O
cross	O
-	O
lingual	O
pre	O
-	O
trained	O
model	O
could	O
be	O
further	O
improved	O
on	O
a	O
downstream	O
task	O
,	O
by	O
selecting	O
a	O
specific	O
pivot	O
language	O
in	O
finetuning	O
.	O

We	O
define	O
multi	O
-	O
language	O
(	O
ml	O
)	O
fine	O
-	O
tuning	O
as	O
finetune	O
a	O
pre	O
-	O
trained	O
model	O
for	O
a	O
downstream	O
task	O
using	O
all	O
its	O
available	O
labeled	O
data	O
in	O
different	O
languages	O
.	O
We	O
evaluate	O
Unicoder	O
on	O
XNLI	B-DatasetName
and	O
NTG	O
using	O
this	O
fine	O
-	O
tuning	O
method	O
and	O
list	O
evaluation	O
results	O
in	O
Table	O
7	O
and	O
Table	O
8	O
,	O
respectively	O
.	O
We	O
find	O
multi	O
-	O
language	O
fine	O
-	O
tuning	O
can	O
achieve	O
better	O
results	O
than	O
pivot	O
-	O
language	O
fine	O
-	O
tuning	O
on	O
both	O
XNLI	B-DatasetName
and	O
NTG	O
.	O
It	O
means	O
the	O
average	O
quality	O
of	O
a	O
cross	O
-	O
lingual	O
pre	O
-	O
trained	O
model	O
could	O
be	O
significantly	O
improved	O
on	O
a	O
downstream	O
task	O
,	O
by	O
using	O
combined	O
labeled	O
data	O
in	O
multiple	O
languages	O
.	O

We	O
define	O
multi	O
-	O
task	O
(	O
mt	O
)	O
fine	O
-	O
tuning	O
as	O
fine	O
-	O
tune	O
a	O
pre	O
-	O
trained	O
model	O
for	O
multiple	O
downstream	O
tasks	O
using	O
their	O
combined	O
labeled	O
data	O
.	O
To	O
reduce	O
the	O
experimental	O
cost	O
,	O
we	O
evaluate	O
Unicoder	O
on	O
5	O
understanding	O
tasks	O
:	O
XNLI	B-DatasetName
,	O
PAWS	B-DatasetName
-	I-DatasetName
X	I-DatasetName
,	O
NC	O
,	O
QAM	O
and	O
QADSM	O
,	O
using	O
their	O
merged	O
English	O
labeled	O
data	O
in	O
fine	O
-	O
tuning	O
.	O
Results	O
are	O
listed	O
in	O
Table	O
9	O
.	O
We	O
find	O
PAWS	B-DatasetName
-	I-DatasetName
X	I-DatasetName
and	O
QADSM	O
can	O
benefit	O
from	O
the	O
joint	O
fine	O
-	O
tuning	O
strategy	O
,	O
but	O
XNLI	B-DatasetName
,	O
NC	O
and	O
QAM	O
can	O
not	O
.	O
We	O
leave	O
discovering	O
relationships	O
between	O
different	O
tasks	O
for	O
better	O
downstream	O
task	O
fine	O
-	O
tuning	O
for	O
future	O
work	O
.	O

Dataset	O
GLUE	B-DatasetName
includes	O
9	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
tasks	O
that	O
are	O
labeled	O
in	O
English	O
only	O
.	O
Comparing	O
to	O
GLUE	B-DatasetName
,	O
XGLUE	B-DatasetName
not	O
only	O
expands	O
task	O
annotations	O
to	O
multiple	O
languages	O
,	O
but	O
also	O
includes	O
natural	O
language	O
generation	O
tasks	O
.	O
XNLI	B-DatasetName
(	O
Conneau	O
et	O
al	O
,	O
2018	O
)	O
,	O
NER	B-TaskName
(	O
Sang	O
,	O
2002	O
;	O
Sang	O
and	O
De	O
Meulder	O
,	O
2003	O
)	O
,	O
POS	O
Tagging	O
(	O
Kim	O
et	O
al	O
,	O
2017	O
)	O
,	O
MLQA	B-DatasetName
(	O
Lewis	O
et	O
al	O
,	O
2019b	O
)	O
and	O
PAWS	B-DatasetName
-	I-DatasetName
X	I-DatasetName
(	O
Yang	O
et	O
al	O
,	O
2019a	O
)	O
are	O
5	O
multilingual	O
datasets	O
built	O
for	O
specific	O
tasks	O
.	O
XGLUE	B-DatasetName
not	O
only	O
includes	O
these	O
5	O
existing	O
tasks	O
,	O
but	O
also	O
introduces	O
6	O
new	O
tasks	O
selected	O
from	O
real	O
-	O
world	O
scenarios	O
(	O
i.e.	O
,	O
Search	O
,	O
Ads	O
and	O
News	O
)	O
.	O
This	O
makes	O
XGLUE	B-DatasetName
have	O
more	O
practical	O
values	O
.	O
XTREME	B-DatasetName
)	O
is	O
a	O
concurrent	O
work	O
of	O
XGLUE	B-DatasetName
.	O
Comparing	O
to	O
it	O
,	O
XGLUE	B-DatasetName
includes	O
both	O
understanding	O
and	O
generation	O
tasks	O
,	O
which	O
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
is	O
the	O
first	O
attempt	O
in	O
the	O
cross	O
-	O
lingual	O
dataset	O
construction	O
efforts	O
.	O
)	O
is	O
a	O
RoBERTa	B-MethodName
-	O
version	O
XLM	B-MethodName
without	O
using	O
translation	O
language	O
model	O
in	O
pre	O
-	O
training	O
.	O
It	O
is	O
trained	O
based	O
on	O
a	O
much	O
larger	O
multilingual	O
corpus	O
(	O
i.e.	O
Com	O
-	O
mon	O
Crawl	O
)	O
and	O
become	O
the	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
XNLI	B-DatasetName
.	O
In	O
this	O
paper	O
,	O
we	O
use	O
both	O
the	O
Common	B-DatasetName
Crawl	I-DatasetName
corpus	O
and	O
the	O
bilingual	O
corpus	O
,	O
aiming	O
to	O
build	O
a	O
stronger	O
baseline	O
model	O
on	O
XGLUE	B-DatasetName
.	O
BART	B-MethodName
(	O
Lewis	O
et	O
al	O
,	O
2019a	O
)	O
and	O
ProphetNet	B-MethodName
(	O
Yan	B-DatasetName
et	I-DatasetName
al	I-DatasetName
,	O
2020	O
)	O
are	O
two	O
latest	O
generative	O
pre	O
-	O
trained	O
models	O
.	O
We	O
borrow	O
ideas	O
from	O
these	O
two	O
works	O
and	O
extend	O
Unicoder	O
to	O
cross	O
-	O
lingual	O
generation	O
tasks	O
,	O
which	O
goes	O
a	O
step	O
further	O
to	O
verify	O
and	O
explore	O
different	O
text	B-TaskName
generation	I-TaskName
approaches	O
in	O
the	O
cross	O
-	O
lingual	O
scenario	O
.	O

We	O
present	O
XGLUE	B-DatasetName
as	O
a	O
new	O
cross	O
-	O
lingual	O
benchmark	O
and	O
conduct	O
comprehensive	O
evaluations	O
with	O
interesting	O
findings	O
observed	O
.	O
We	O
thank	O
STC	O
-	O
A	O
NLP	O
,	O
Bing	O
Answers	O
,	O
Bing	O
Ads	O
,	O
Bing	O
Relevance	O
and	O
Microsoft	O
News	O
for	O
providing	O
the	O
datasets	O
.	O
A	O
The	O
fine	O
-	O
tune	O
parameters	O
of	O
Unicoder	O
on	O
XGLUE	B-DatasetName
.	O

WikiGraphs	B-DatasetName
:	O
A	O
Wikipedia	O
Text	O
-	O
Knowledge	O
Graph	O
Paired	O
Dataset	O

We	O
present	O
a	O
new	O
dataset	O
of	O
Wikipedia	O
articles	O
each	O
paired	O
with	O
a	O
knowledge	O
graph	O
,	O
to	O
facilitate	O
the	O
research	O
in	O
conditional	B-TaskName
text	I-TaskName
generation	I-TaskName
,	O
graph	B-TaskName
generation	I-TaskName
and	O
graph	B-TaskName
representation	I-TaskName
learning	I-TaskName
.	O
Existing	O
graph	O
-	O
text	O
paired	O
datasets	O
typically	O
contain	O
small	O
graphs	O
and	O
short	O
text	O
(	O
1	O
or	O
few	O
sentences	O
)	O
,	O
thus	O
limiting	O
the	O
capabilities	O
of	O
the	O
models	O
that	O
can	O
be	O
learned	O
on	O
the	O
data	O
.	O
Our	O
new	O
dataset	O
WikiGraphs	B-DatasetName
is	O
collected	O
by	O
pairing	O
each	O
Wikipedia	O
article	O
from	O
the	O
established	O
WikiText	O
-	O
103	O
benchmark	O
(	O
Merity	O
et	O
al	O
,	O
2016	O
)	O
with	O
a	O
subgraph	O
from	O
the	O
Freebase	O
knowledge	O
graph	O
(	O
Bollacker	O
et	O
al	O
,	O
2008	O
)	O
.	O
This	O
makes	O
it	O
easy	O
to	O
benchmark	O
against	O
other	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
text	O
generative	O
models	O
that	O
are	O
capable	O
of	O
generating	O
long	O
paragraphs	O
of	O
coherent	O
text	O
.	O
Both	O
the	O
graphs	O
and	O
the	O
text	O
data	O
are	O
of	O
significantly	O
larger	O
scale	O
compared	O
to	O
prior	O
graph	O
-	O
text	O
paired	O
datasets	O
.	O
We	O
present	O
baseline	O
graph	O
neural	O
network	O
and	O
transformer	O
model	O
results	O
on	O
our	O
dataset	O
for	O
3	O
tasks	O
:	O
graph	O
text	B-TaskName
generation	I-TaskName
,	O
graph	O
text	O
retrieval	O
and	O
text	O
graph	O
retrieval	O
.	O
We	O
show	O
that	O
better	O
conditioning	O
on	O
the	O
graph	O
provides	O
gains	O
in	O
generation	O
and	O
retrieval	O
quality	O
but	O
there	O
is	O
still	O
large	O
room	O
for	O
improvement	O
.	O
1	O

Parallel	O
datasets	O
that	O
pair	O
data	O
from	O
different	O
sources	O
and	O
modalities	O
have	O
enabled	O
large	O
amounts	O
of	O
research	O
on	O
cross	O
modality	O
learning	O
.	O
Paired	O
image	O
-	O
caption	O
datasets	O
enable	O
models	O
to	O
describe	O
visual	O
scenes	O
in	O
natural	O
language	O
(	O
Lin	O
et	O
al	O
,	O
2014	O
;	O
,	O
paired	O
streams	O
of	O
speech	O
and	O
transcription	O
data	O
makes	O
it	O
possible	O
to	O
train	O
speech	B-TaskName
recognition	I-TaskName
systems	O
(	O
Garofolo	O
et	O
al	O
,	O
1993	O
;	O
Panayotov	O
et	O
al	O
,	O
2015	O
)	O
or	O
text	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
speech	I-TaskName
synthesis	I-TaskName
models	O
(	O
Oord	O
et	O
al	O
,	O
2016	O
)	O
,	O
and	O
parallel	O
corpus	O
of	O
text	O
in	O
different	O
languages	O
enable	O
learned	O
machine	B-TaskName
translation	I-TaskName
models	O
(	O
Barrault	O
et	O
al	O
,	O
2020	O
)	O
.	O
We	O
present	O
a	O
new	O
dataset	O
of	O
Wikipedia	O
text	O
articles	O
each	O
paired	O
with	O
a	O
relevant	O
knowledge	O
graph	O
(	O
KG	O
)	O
,	O
which	O
enables	O
building	O
models	O
that	O
can	O
generate	O
long	O
text	O
conditioned	O
on	O
a	O
graph	O
structured	O
overview	O
of	O
relevant	O
topics	O
,	O
and	O
also	O
models	O
that	O
extract	O
or	O
generate	O
graphs	O
from	O
a	O
text	O
description	O
.	O
There	O
has	O
been	O
many	O
prior	O
efforts	O
trying	O
to	O
build	O
datasets	O
for	O
learning	O
graph	O
text	B-TaskName
generation	I-TaskName
models	O
(	O
Jin	O
et	O
al	O
,	O
2020	O
;	O
Gardent	O
et	O
al	O
,	O
2017	O
;	O
Lebret	O
et	O
al	O
,	O
2016	O
)	O
.	O
However	O
,	O
existing	O
graph	O
-	O
text	O
paired	O
datasets	O
are	O
mostly	O
small	O
scale	O
,	O
where	O
the	O
graphs	O
tend	O
to	O
have	O
10	O
-	O
20	O
or	O
even	O
less	O
nodes	O
,	O
and	O
the	O
text	O
typically	O
only	O
contains	O
one	O
or	O
a	O
few	O
sentences	O
.	O
This	O
represents	O
a	O
significant	O
contrast	O
with	O
the	O
state	O
-	O
ofthe	O
-	O
art	O
text	B-TaskName
generation	I-TaskName
models	O
(	O
Dai	O
et	O
al	O
,	O
2019	O
;	O
Brown	O
et	O
al	O
,	O
2020	O
)	O
,	O
which	O
can	O
already	O
generate	O
very	O
fluent	O
and	O
long	O
text	O
that	O
spans	O
thousands	O
of	O
tokens	O
over	O
multiple	O
paragraphs	O
.	O
We	O
attempt	O
to	O
bridge	O
this	O
gap	O
,	O
with	O
the	O
goal	O
of	O
advancing	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
graph	O
text	B-TaskName
generation	I-TaskName
models	O
,	O
graph	B-TaskName
representation	I-TaskName
learning	I-TaskName
models	O
and	O
also	O
text	O
-	O
conditioned	O
graph	O
generative	O
models	O
.	O
Each	O
text	O
document	O
in	O
our	O
dataset	O
is	O
a	O
full	O
-	O
length	O
Wikipedia	O
article	O
,	O
and	O
we	O
pair	O
each	O
of	O
them	O
with	O
a	O
KG	O
that	O
are	O
significantly	O
bigger	O
than	O
prior	O
datasets	O
of	O
similar	O
nature	O
and	O
includes	O
much	O
richer	O
information	O
.	O
Hand	O
labelling	O
text	O
articles	O
with	O
KGs	O
is	O
expensive	O
and	O
not	O
scalable	O
(	O
Lebret	O
et	O
al	O
,	O
2016	O
)	O
,	O
therefore	O
we	O
utilize	O
an	O
existing	O
and	O
established	O
knowledge	O
base	O
,	O
Freebase	O
(	O
Bollacker	O
et	O
al	O
,	O
2008	O
)	O
,	O
and	O
designed	O
an	O
automated	O
process	O
to	O
extract	O
a	O
relevant	O
subgraph	O
from	O
it	O
for	O
each	O
Wikipedia	O
article	O
.	O
To	O
make	O
the	O
text	B-TaskName
generation	I-TaskName
results	O
on	O
our	O
dataset	O
directly	O
comparable	O
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
,	O
we	O
chose	O
the	O
set	O
of	O
Wikipedia	O
articles	O
from	O
the	O
established	O
language	O
modeling	O
benchmark	O
WikiText	O
-	O
103	O
(	O
Merity	O
et	O
al	O
,	O
2016	O
)	O
,	O
which	O
contains	O
a	O
subset	O
of	O
high	O
-	O
quality	O
Wikipedia	O
articles	O
.	O
This	O
gives	O
us	O
a	O
dataset	O
of	O
23	O
,	O
522	O
graph	O
-	O
text	O
pairs	O
in	O
total	O
,	O
covering	O
82.3	O
%	O
of	O
Wikitext	O
-	O
103	O
articles	O
.	O
On	O
average	O
each	O
graph	O
has	O
38.7	O
nodes	O
and	O
48.3	O
edges	O
,	O
and	O
each	O
text	O
article	O
contains	O
3	O
,	O
533.8	O
tokens	O
.	O
In	O
addition	O
to	O
structural	O
information	O
,	O
our	O
graphs	O
also	O
contain	O
rich	O
text	O
information	O
with	O
an	O
average	O
of	O
895.1	O
tokens	O
in	O
each	O
graph	O
.	O
Furthermore	O
,	O
the	O
automatic	O
process	O
we	O
used	O
to	O
create	O
this	O
dataset	O
can	O
be	O
extended	O
to	O
pair	O
any	O
Wikipedia	O
document	O
with	O
Freebase	O
,	O
and	O
can	O
be	O
scaled	O
up	O
to	O
create	O
over	O
3	O
M	O
graph	O
-	O
text	O
pairs	O
.	O
Out	O
of	O
many	O
exciting	O
new	O
tasks	O
that	O
this	O
dataset	O
enables	O
,	O
we	O
present	O
3	O
possibilities	O
:	O
graph	O
text	B-TaskName
generation	I-TaskName
,	O
graph	O
text	O
retrieval	O
,	O
and	O
text	O
graph	O
retrieval	O
.	O
We	O
benchmarked	O
a	O
few	O
baseline	O
models	O
on	O
these	O
tasks	O
.	O
The	O
models	O
we	O
considered	O
were	O
based	O
on	O
the	O
recent	O
Transformer	B-MethodName
-	I-MethodName
XL	I-MethodName
(	O
Dai	O
et	O
al	O
,	O
2019	O
)	O
model	O
,	O
and	O
we	O
adapted	O
it	O
to	O
condition	O
the	O
text	B-TaskName
generation	I-TaskName
on	O
the	O
KG	O
in	O
different	O
ways	O
.	O
Our	O
results	O
show	O
that	O
better	O
conditioning	O
on	O
the	O
graph	O
indeed	O
improves	O
the	O
relevance	O
of	O
the	O
generated	O
text	O
and	O
the	O
retrieval	O
quality	O
.	O
However	O
,	O
there	O
is	O
still	O
significant	O
room	O
for	O
improvement	O
on	O
these	O
tasks	O
,	O
which	O
makes	O
this	O
an	O
exciting	O
dataset	O
for	O
research	O
.	O
Our	O
data	O
and	O
code	O
for	O
baseline	O
models	O
will	O
be	O
made	O
publicly	O
available	O
.	O

Graph	O
-	O
text	O
paired	O
data	O
There	O
has	O
been	O
a	O
lot	O
of	O
prior	O
work	O
on	O
creating	O
graph	O
-	O
text	O
paired	O
datasets	O
.	O
Example	O
applications	O
include	O
generating	O
text	O
summaries	O
conditioned	O
on	O
Abstract	O
Meaning	O
Representation	O
graphs	O
(	O
Liu	O
et	O
al	O
,	O
2018	O
)	O
,	O
generating	O
the	O
abstract	O
of	O
a	O
scientific	O
article	O
given	O
a	O
KG	O
and	O
title	O
(	O
Koncel	O
-	O
Kedziorski	O
et	O
al	O
,	O
2019	O
)	O
and	O
generating	O
text	O
from	O
RDF	O
triples	O
(	O
Gardent	O
et	O
al	O
,	O
2017	O
;	O
Jin	O
et	O
al	O
,	O
2020	O
)	O
.	O
In	O
the	O
following	O
we	O
will	O
mostly	O
review	O
related	O
work	O
on	O
KG	O
-	O
text	O
paired	O
datasets	O
.	O
Annotating	O
KG	O
or	O
text	O
to	O
create	O
paired	O
datasets	O
is	O
expensive	O
,	O
as	O
a	O
good	O
quality	O
annotation	O
requires	O
annotators	O
that	O
understand	O
the	O
content	O
and	O
structure	O
of	O
the	O
text	O
and	O
the	O
corresponding	O
KG	O
(	O
Jin	O
et	O
al	O
,	O
Dataset	O
2020	O
)	O
.	O
Therefore	O
previous	O
KG	O
-	O
text	O
paired	O
datasets	O
that	O
rely	O
on	O
human	O
annotation	O
have	O
limited	O
scale	O
.	O
Among	O
these	O
,	O
Gardent	O
et	O
al	O
(	O
2017	O
)	O
crowdsourced	O
human	O
annotators	O
to	O
verbalize	O
RDF	O
triplets	O
taken	O
from	O
DBpedia	B-DatasetName
(	O
Auer	O
et	O
al	O
,	O
2007	O
)	O
to	O
a	O
few	O
sentences	O
(	O
WebNLG	B-DatasetName
)	O
and	O
this	O
caused	O
errors	O
in	O
annotation	O
that	O
were	O
fixed	O
with	O
a	O
few	O
updates	O
through	O
years	O
.	O
Parikh	O
et	O
al	O
(	O
2020	O
)	O
paired	O
Wikipedia	O
Table	O
with	O
one	O
sentence	O
text	O
that	O
is	O
created	O
by	O
annotators	O
that	O
revise	O
Wikipedia	O
text	O
.	O
Another	O
line	O
of	O
research	O
focuses	O
on	O
eliminating	O
the	O
need	O
of	O
human	O
annotations	O
by	O
automatically	O
matching	O
KG	O
-	O
text	O
pairs	O
or	O
generating	O
KGs	O
from	O
text	O
using	O
existing	O
tools	O
.	O
Lebret	O
et	O
al	O
(	O
2016	O
)	O
automatically	O
matched	O
Wikipedia	O
infobox	O
of	O
biographies	O
with	O
their	O
first	O
sentence	O
.	O
Koncel	O
-	O
Kedziorski	O
et	O
al	O
(	O
2019	O
)	O
utilized	O
an	O
earlier	O
information	O
extraction	O
system	O
that	O
extracts	O
entities	O
,	O
co	O
-	O
reference	O
and	O
relations	O
from	O
given	O
text	O
to	O
build	O
KG	O
's	O
.	O
The	O
Gen	O
-	O
Wiki	O
dataset	O
(	O
Jin	O
et	O
al	O
,	O
2020	O
)	O
is	O
automatically	O
constructed	O
by	O
querying	O
KGs	O
in	O
DBpedia	B-DatasetName
with	O
the	O
title	O
of	O
articles	O
in	O
Wikipedia	O
followed	O
by	O
filtering	O
and	O
entity	O
annotation	O
.	O
We	O
construct	O
our	O
WikiGraphs	B-DatasetName
dataset	O
by	O
extracting	O
a	O
subgraph	O
from	O
Freebase	O
(	O
Bollacker	O
et	O
al	O
,	O
2008	O
)	O
for	O
each	O
Wikipedia	O
article	O
following	O
a	O
scalable	O
automatic	O
process	O
.	O
Compared	O
to	O
previous	O
work	O
,	O
our	O
WikiGraphs	B-DatasetName
dataset	O
contains	O
significantly	O
larger	O
graphs	O
and	O
longer	O
text	O
(	O
Table	O
1	O
)	O
.	O
Models	O
for	O
graph	O
-	O
text	O
paired	O
data	O
Recent	O
state	O
of	O
art	O
language	O
models	O
are	O
based	O
on	O
the	O
Transformer	B-MethodName
architecture	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
that	O
uses	O
the	O
self	O
attention	O
mechanism	O
.	O
The	O
Transformer	B-MethodName
-	I-MethodName
XL	I-MethodName
(	O
Dai	O
et	O
al	O
,	O
2019	O
)	O
model	O
further	O
introduces	O
a	O
segment	O
level	O
recurrence	O
with	O
a	O
novel	O
positional	O
encoding	O
resulting	O
in	O
impressive	O
performance	O
in	O
long	O
sequences	O
by	O
capturing	O
dependencies	O
beyond	O
a	O
fixed	O
length	O
window	O
.	O
Graph	O
neural	O
networks	O
(	O
GNNs	O
)	O
(	O
Battaglia	O
et	O
al	O
,	O
2018	O
;	O
Gilmer	O
et	O
al	O
,	O
2017	O
)	O
learn	O
representations	O
for	O
graph	O
structured	O
data	O
through	O
a	O
message	O
passing	O
process	O
.	O
This	O
class	O
of	O
models	O
naturally	O
exploit	O
the	O
graph	O
structures	O
,	O
making	O
them	O
a	O
good	O
fit	O
for	O
graph	O
data	O
.	O
GNNs	O
have	O
been	O
used	O
in	O
many	O
applications	O
on	O
KG	O
's	O
(	O
Kipf	O
and	O
Welling	O
,	O
2016	O
;	O
Xu	O
et	O
al	O
,	O
2019	O
)	O
.	O
Fundamentally	O
,	O
transformers	O
can	O
also	O
be	O
understood	O
as	O
a	O
special	O
type	O
of	O
GNNs	O
with	O
a	O
fully	O
-	O
connected	O
graph	O
structure	O
.	O
The	O
most	O
recent	O
prior	O
work	O
on	O
graph	O
-	O
to	O
-	O
text	B-TaskName
generation	I-TaskName
follows	O
an	O
encoder	O
-	O
decoder	O
architecture	O
(	O
Koncel	O
-	O
Kedziorski	O
et	O
al	O
,	O
2019	O
;	O
Jin	O
et	O
al	O
,	O
2020	O
)	O
,	O
where	O
the	O
graph	O
part	O
is	O
encoded	O
with	O
a	O
GNN	O
model	O
,	O
e.g.	O
Graph	B-MethodName
Attention	I-MethodName
Network	I-MethodName
(	O
GAT	B-MethodName
)	O
(	O
Veličković	O
et	O
al	O
,	O
2018	O
)	O
.	O
The	O
text	O
part	O
is	O
typically	O
modeled	O
using	O
an	O
attention	O
based	O
decoder	O
with	O
a	O
copy	O
mechanism	O
(	O
e.g.	O
BiLSTMs	O
as	O
in	O
(	O
Jin	O
et	O
al	O
,	O
2020	O
)	O
)	O
to	O
process	O
input	O
from	O
both	O
the	O
KG	O
and	O
text	O
.	O
The	O
models	O
we	O
benchmarked	O
for	O
graph	O
-	O
to	O
-	O
text	B-TaskName
generation	I-TaskName
were	O
based	O
on	O
the	O
Transformer	B-MethodName
-	I-MethodName
XL	I-MethodName
architecture	O
and	O
conditioned	O
on	O
the	O
graph	O
through	O
a	O
GNN	O
,	O
making	O
full	O
use	O
of	O
the	O
graph	O
structure	O
and	O
capable	O
of	O
generating	O
very	O
long	O
text	O
comparable	O
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O

Basic	O
statistics	O
about	O
our	O
WikiGraphs	B-DatasetName
dataset	O
are	O
listed	O
in	O
Table	O
2	O
.	O
An	O
illustration	O
of	O
a	O
graph	O
-	O
text	O
pair	O
is	O
shown	O
in	O
Figure	O
1	O
.	O
A	O
few	O
actual	O
examples	O
from	O
our	O
dataset	O
are	O
included	O
in	O
the	O
Appendix	O
(	O
Figure	O
7	O
,	O
8	O
)	O
.	O
All	O
of	O
the	O
articles	O
come	O
from	O
the	O
WikiText	O
-	O
103	O
dataset	O
(	O
Merity	O
et	O
al	O
,	O
2016	O
)	O
,	O
which	O
contains	O
highquality	O
articles	O
that	O
fit	O
the	O
Good	O
or	O
Featured	O
criteria	O
specified	O
by	O
the	O
Wikipedia	O
editors	O
when	O
the	O
data	O
was	O
collected	O
.	O
Merity	O
et	O
al	O
(	O
2016	O
)	O
have	O
already	O
cleaned	O
up	O
and	O
tokenized	O
the	O
articles	O
,	O
therefore	O
they	O
appear	O
as	O
plain	O
text	O
without	O
any	O
markup	O
tags	O
.	O
As	O
will	O
be	O
described	O
in	O
Section	O
3.2	O
,	O
we	O
try	O
to	O
pair	O
each	O
article	O
with	O
a	O
subgraph	O
from	O
Freebase	O
,	O
centered	O
at	O
the	O
entity	O
node	O
that	O
has	O
a	O
Wikipedia	O
link	O
to	O
the	O
title	O
of	O
the	O
article	O
.	O
We	O
are	O
not	O
able	O
to	O
match	O
every	O
article	O
to	O
an	O
entity	O
in	O
Freebase	O
,	O
but	O
through	O
this	O
process	O
we	O
retained	O
a	O
significant	O
portion	O
of	O
82.3	O
%	O
of	O
the	O
WikiText	O
-	O
103	O
articles	O
.	O
We	O
kept	O
the	O
original	O
train	O
/	O
valid	O
/	O
test	O
split	O
.	O
As	O
we	O
will	O
see	O
in	O
Section	O
4.2	O
,	O
training	O
models	O
on	O
this	O
set	O
gives	O
us	O
results	O
that	O
are	O
very	O
close	O
to	O
training	O
on	O
the	O
full	O
WikiText	O
-	O
103	O
dataset	O
when	O
evaluated	O
on	O
our	O
test	O
set	O
.	O
Therefore	O
the	O
text	O
part	O
of	O
WikiGraphs	B-DatasetName
appears	O
to	O
be	O
sufficient	O
to	O
reproduce	O
and	O
benchmark	O
against	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
text	O
generative	O
models	O
.	O
Figure	O
2	O
shows	O
the	O
distribution	O
of	O
graph	O
sizes	O
and	O
article	O
lengths	O
across	O
our	O
dataset	O
.	O
All	O
the	O
distributions	O
are	O
skewed	O
with	O
a	O
long	O
tail	O
.	O
Notably	O
,	O
average	O
graph	O
size	O
in	O
our	O
dataset	O
is	O
38.7	O
nodes	O
and	O
48.3	O
edges	O
,	O
considerably	O
larger	O
than	O
the	O
graphs	O
in	O
previous	O
datasets	O
(	O
Jin	O
et	O
al	O
,	O
2020	O
;	O
Gardent	O
et	O
al	O
,	O
2017	O
)	O
.	O
Also	O
the	O
length	O
of	O
the	O
text	O
articles	O
averages	O
to	O
3	O
,	O
533.8	O
tokens	O
and	O
can	O
go	O
up	O
to	O
26	O
,	O
994	O
tokens	O
,	O
which	O
is	O
orders	O
of	O
magnitudes	O
longer	O
than	O
the	O
text	O
data	O
in	O
previous	O
graph	O
-	O
text	O
paired	O
datasets	O
that	O
typically	O
only	O
contains	O
a	O
single	O
or	O
few	O
sentences	O
(	O
Jin	O
et	O
al	O
,	O
2020	O
;	O
Gardent	O
et	O
al	O
,	O
2017	O
;	O
Lebret	O
et	O
al	O
,	O
2016	O
)	O
.	O

The	O
graphs	O
in	O
our	O
dataset	O
contains	O
two	O
types	O
of	O
nodes	O
:	O
entities	O
and	O
string	O
literals	O
.	O
Each	O
entity	O
is	O
labeled	O
by	O
a	O
unique	O
Freebase	O
entity	O
ID	O
,	O
e.g.	O
ns	O
/	O
m.0f9q9z	O
,	O
and	O
each	O
string	O
literal	O
contains	O
some	O
natural	O
language	O
text	O
,	O
that	O
could	O
be	O
for	O
example	O
a	O
name	O
,	O
date	O
,	O
or	O
description	O
of	O
an	O
entity	O
.	O
Each	O
edge	O
in	O
the	O
graphs	O
also	O
has	O
an	O
associated	O
edge	O
label	O
,	O
e.g.	O
ns	O
/	O
common.topic.description	O
,	O
indicating	O
which	O
type	O
of	O
edge	O
it	O
is	O
.	O
There	O
are	O
a	O
total	O
of	O
522	O
different	O
edge	O
types	O
in	O
our	O
dataset	O
.	O
Figure	O
3	O
shows	O
the	O
frequency	O
of	O
all	O
the	O
different	O
edge	O
types	O
in	O
our	O
dataset	O
.	O
Every	O
graph	O
always	O
has	O
one	O
entity	O
node	O
(	O
we	O
call	O
it	O
"	O
center	O
node	O
"	O
)	O
that	O
has	O
a	O
link	O
to	O
the	O
paired	O
Wikipedia	O
article	O
,	O
through	O
a	O
special	O
edge	O
key	O
/	O
wikipedia.en	O
,	O
and	O
the	O
whole	O
graph	O
is	O
a	O
1	O
-	O
hop	O
neighborhood	O
of	O
entities	O
around	O
the	O
center	O
node	O
within	O
the	O
bigger	O
Freebase	O
KG	O
,	O
plus	O
the	O
string	O
literals	O
associated	O
with	O
all	O
the	O
entities	O
included	O
.	O
Note	O
that	O
it	O
is	O
possible	O
to	O
have	O
edges	O
between	O
the	O
1	O
-	O
hop	O
neighbors	O
of	O
the	O
center	O
node	O
,	O
therefore	O
the	O
graphs	O
typically	O
are	O
not	O
star	O
structured	O
.	O
Section	O
3.2	O
provides	O
more	O
details	O
about	O
how	O
these	O
graphs	O
are	O
constructed	O
and	O
any	O
additional	O
filtering	O
we	O
did	O
.	O
One	O
special	O
characteristic	O
about	O
our	O
graph	O
data	O
is	O
that	O
the	O
natural	O
language	O
text	O
contained	O
in	O
the	O
string	O
literal	O
nodes	O
can	O
sometimes	O
be	O
quite	O
long	O
(	O
see	O
e.g.	O
Figure	O
7	O
,	O
8	O
)	O
,	O
and	O
therefore	O
provide	O
much	O
richer	O
information	O
not	O
included	O
in	O
the	O
graph	O
structure	O
itself	O
.	O
On	O
average	O
,	O
each	O
graph	O
contains	O
895.1	O
tokens	O
across	O
all	O
the	O
string	O
literal	O
nodes	O
in	O
one	O
graph	O
(	O
Table	O
2	O
,	O
Figure	O
2	O
,	O
"	O
Tokens	O
per	O
graph	O
"	O
)	O
.	O
Figure	O
4	O
shows	O
the	O
distribution	O
of	O
per	O
-	O
graph	O
number	O
of	O
entity	O
nodes	O
and	O
string	O
literal	O
nodes	O
in	O
our	O
dataset	O
.	O
We	O
can	O
see	O
that	O
our	O
graphs	O
tend	O
to	O
have	O
more	O
string	O
literal	O
nodes	O
than	O
entity	O
nodes	O
,	O
indicating	O
that	O
the	O
entities	O
are	O
supplemented	O
with	O
the	O
rich	O
information	O
in	O
the	O
string	O
literals	O
.	O
The	O
distribution	O
of	O
information	O
is	O
not	O
uniform	O
across	O
the	O
nodes	O
in	O
a	O
graph	O
.	O
Figure	O
5	O
shows	O
that	O
most	O
entity	O
nodes	O
in	O
our	O
graph	O
has	O
a	O
small	O
degree	O
,	O
while	O
few	O
nodes	O
have	O
much	O
larger	O
degrees	O
.	O
Also	O
most	O
string	O
literal	O
nodes	O
contain	O
short	O
text	O
,	O
while	O
fewer	O
nodes	O
contain	O
longer	O
text	O
.	O
The	O
skewed	O
distribution	O
of	O
nodes	O
and	O
edges	O
in	O
our	O
dataset	O
reflect	O
the	O
nature	O
of	O
KG	O
's	O
like	O
Freebase	O
,	O
and	O
presents	O
new	O
challenges	O
to	O
graph	B-TaskName
representation	I-TaskName
learning	I-TaskName
models	O
.	O

We	O
perform	O
a	O
set	O
of	O
experiments	O
to	O
showcase	O
how	O
the	O
text	O
and	O
graph	O
information	O
can	O
be	O
combined	O
in	O
a	O
language	O
model	O
.	O
Specifically	O
,	O
we	O
consider	O
three	O
tasks	O
:	O
text	B-TaskName
generation	I-TaskName
conditioned	O
on	O
the	O
graph	O
,	O
graph	O
retrieval	O
given	O
the	O
text	O
,	O
and	O
text	O
retrieval	O
given	O
the	O
graph	O
.	O

In	O
order	O
to	O
incorporate	O
graph	O
information	O
into	O
an	O
advanced	O
language	O
model	O
,	O
we	O
adapt	O
the	O
recent	O
Transformer	B-MethodName
-	I-MethodName
XL	I-MethodName
model	O
(	O
Dai	O
et	O
al	O
,	O
2019	O
)	O
to	O
also	O
attend	O
to	O
the	O
graph	O
features	O
.	O
At	O
a	O
high	O
-	O
level	O
our	O
model	O
embeds	O
the	O
graph	O
into	O
a	O
set	O
of	O
embedding	O
vectors	O
,	O
and	O
then	O
exposes	O
these	O
embeddings	O
to	O
the	O
Transformer	B-MethodName
-	I-MethodName
XL	I-MethodName
model	O
as	O
extra	O
"	O
token	O
"	O
embeddings	O
to	O
condition	O
on	O
.	O
The	O
size	O
of	O
this	O
set	O
depends	O
on	O
the	O
graph	O
model	O
we	O
choose	O
.	O
Given	O
the	O
features	O
for	O
T	O
text	O
tokens	O
H	O
t	O
R	O
T	O
×d	O
and	O
features	O
for	O
T	O
graph	O
"	O
tokens	O
"	O
H	O
g	O
R	O
T	O
×d	O
,	O
we	O
illustrate	O
the	O
graph	O
-	O
conditioned	O
attention	O
procedure	O
with	O
a	O
single	O
head	O
as	O
follows	O
:	O
Q	O
t	O
,	O
K	O
t	O
,	O
V	O
t	O
=	O
H	O
t	O
W	O
t	O
q	O
,	O
H	O
t	O
W	O
t	O
k	O
,	O
H	O
t	O
W	O
t	O
v	O
K	O
g	O
,	O
V	O
g	O
=	O
H	O
g	O
W	O
g	O
k	O
,	O
H	O
g	O
W	O
g	O
v	O
A	O
t	O
,	O
A	O
g	O
=	O
Q	O
t	O
K	O
t	O
,	O
Q	O
t	O
K	O
g	O
A	O
,	O
V	O
=	O
[	O
A	O
t	O
A	O
g	O
]	O
,	O
[	O
V	O
t	O
V	O
g	O
]	O
O	O
=	O
Masked	O
-	O
Softmax	B-MethodName
(	O
A	O
)	O
V	O
where	O
[	O
a	O
b	O
]	O
stands	O
for	O
concatenation	O
on	O
the	O
sequence	O
dimension	O
and	O
thus	O
A	O
R	O
T	O
×	O
(	O
T	O
+	O
T	O
)	O
and	O
V	O
R	O
(	O
T	O
+	O
T	O
)	O
×d	O
h	O
,	O
where	O
d	O
h	O
is	O
the	O
head	O
dimension	O
.	O
In	O
other	O
words	O
,	O
comparing	O
to	O
the	O
original	O
Transformer	B-MethodName
-	I-MethodName
XL	I-MethodName
,	O
our	O
model	O
also	O
computes	O
the	O
attention	O
scores	O
between	O
the	O
text	O
queries	O
Q	O
t	O
and	O
both	O
the	O
text	O
keys	O
K	O
t	O
and	O
the	O
graph	O
keys	O
K	O
g	O
.	O
As	O
a	O
result	O
,	O
the	O
attention	O
outputs	O
contain	O
information	O
from	O
both	O
the	O
graph	O
and	O
the	O
text	O
context	O
.	O
Note	O
that	O
this	O
formulation	O
is	O
compatible	O
with	O
an	O
additional	O
memory	O
(	O
Dai	O
et	O
al	O
,	O
2019	O
)	O
with	O
minimal	O
changes	O
,	O
as	O
it	O
simply	O
adds	O
in	O
an	O
extra	O
set	O
of	O
"	O
tokens	O
"	O
for	O
the	O
model	O
to	O
attend	O
to	O
.	O
We	O
do	O
n't	O
use	O
position	O
encodings	O
for	O
the	O
graph	O
"	O
tokens	O
"	O
as	O
there	O
is	O
no	O
sequential	O
ordering	O
for	O
them	O
.	O
In	O
this	O
work	O
we	O
consider	O
three	O
different	O
approaches	O
for	O
encoding	O
the	O
graph	O
structure	O
:	O
Bag	O
-	O
of	O
-	O
words	O
(	O
BoW	O
)	O
:	O
we	O
construct	O
a	O
single	O
bag	O
-	O
of	O
-	O
words	O
representation	O
of	O
all	O
the	O
tokens	O
from	O
both	O
the	O
nodes	O
and	O
edges	O
in	O
the	O
graph	O
.	O
Entity	O
IDs	O
and	O
numeric	O
values	O
in	O
the	O
graph	O
are	O
replaced	O
with	O
special	O
tokens	O
<	O
entity	O
>	O
and	O
<	O
number	O
>	O
.	O
The	O
BoW	O
vector	O
is	O
further	O
projected	O
using	O
a	O
linear	B-MethodName
layer	I-MethodName
to	O
a	O
latent	O
space	O
.	O
In	O
this	O
case	O
T	O
=	O
1	O
.	O
Nodes	O
only	O
(	O
Nodes	O
)	O
:	O
we	O
construct	O
separate	O
BoW	O
representations	O
for	O
each	O
node	O
and	O
project	O
each	O
to	O
an	O
embedding	O
and	O
ignore	O
the	O
edges	O
.	O
In	O
this	O
case	O
T	O
is	O
equal	O
to	O
the	O
number	O
of	O
nodes	O
in	O
the	O
graph	O
.	O
Graph	O
neural	O
network	O
(	O
GNN	O
)	O
:	O
we	O
embed	O
BoW	O
representations	O
for	O
both	O
nodes	O
and	O
edges	O
and	O
then	O
use	O
a	O
graph	O
neural	O
network	O
(	O
Battaglia	O
et	O
al	O
,	O
2018	O
)	O
on	O
top	O
of	O
those	O
embeddings	O
to	O
compute	O
a	O
new	O
set	O
of	O
node	O
embeddings	O
.	O
T	O
is	O
equal	O
to	O
the	O
number	O
of	O
nodes	O
.	O
The	O
T	O
graph	O
embeddings	O
from	O
this	O
process	O
are	O
shared	O
across	O
all	O
the	O
time	O
steps	O
for	O
text	O
tokens	O
.	O
This	O
model	O
can	O
be	O
further	O
improved	O
,	O
e.g.	O
by	O
using	O
word	B-TaskName
embeddings	I-TaskName
and	O
text	B-TaskName
summarization	I-TaskName
techniques	O
,	O
but	O
we	O
leave	O
these	O
for	O
future	O
work	O
.	O

We	O
show	O
a	O
few	O
ablations	O
on	O
the	O
graph	O
model	O
and	O
sampling	O
parameters	O
,	O
to	O
provide	O
some	O
insights	O
into	O
the	O
models	O
.	O
Table	O
4	O
shows	O
the	O
effect	O
of	O
varying	O
the	O
number	O
of	O
message	O
passing	O
layers	O
in	O
the	O
GNN	O
.	O
We	O
can	O
observe	O
that	O
there	O
is	O
a	O
big	O
difference	O
between	O
using	O
message	O
passing	O
(	O
≥	O
1	O
layers	O
)	O
or	O
not	O
(	O
0	B-DatasetName
layers	O
)	O
in	O
terms	O
of	O
rBLEU	O
score	O
,	O
but	O
increasing	O
the	O
number	O
of	O
message	O
passing	O
layers	O
does	O
not	O
change	O
the	O
results	O
significantly	O
.	O
We	O
believe	O
however	O
,	O
that	O
these	O
results	O
can	O
be	O
improved	O
by	O
employing	O
bigger	O
and	O
more	O
powerful	O
graph	B-TaskName
representation	I-TaskName
learning	I-TaskName
models	O
,	O
and	O
potentially	O
use	O
initial	O
node	O
and	O
edge	O
representations	O
better	O
than	O
bag	O
-	O
of	O
-	O
words	O
.	O
In	O
Figure	O
6	O
we	O
show	O
the	O
effect	O
of	O
the	O
graph	O
size	O
on	O
model	O
performance	O
.	O
In	O
this	O
experiment	O
we	O
subsample	O
the	O
nodes	O
in	O
each	O
graph	O
to	O
control	O
for	O
the	O
amount	O
of	O
context	O
the	O
model	O
has	O
access	O
to	O
.	O
It	O
is	O
clear	O
from	O
the	O
results	O
that	O
when	O
we	O
heavily	O
subsample	O
and	O
keep	O
only	O
a	O
small	O
portion	O
of	O
the	O
graphs	O
,	O
the	O
GNN	O
model	O
performs	O
similarly	O
as	O
the	O
simpler	O
BoW	O
model	O
,	O
but	O
GNNs	O
benefit	O
more	O
as	O
we	O
keep	O
more	O
of	O
the	O
graph	O
structure	O
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
WikiGraphs	B-DatasetName
,	O
a	O
new	O
graphtext	O
paired	O
dataset	O
with	O
significantly	O
larger	O
graphs	O
and	O
longer	O
text	O
compared	O
to	O
previous	O
datasets	O
of	O
similar	O
nature	O
.	O
We	O
show	O
that	O
the	O
text	O
part	O
of	O
this	O
data	O
is	O
a	O
good	O
benchmark	O
for	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
text	B-TaskName
generation	I-TaskName
models	O
,	O
and	O
the	O
paired	O
dataset	O
can	O
help	O
us	O
benchmark	O
models	O
that	O
are	O
capable	O
of	O
generating	O
long	O
and	O
coherent	O
text	O
conditioned	O
on	O
a	O
graph	O
structure	O
.	O
In	O
the	O
first	O
set	O
of	O
experiments	O
on	O
this	O
dataset	O
we	O
showcase	O
3	O
different	O
tasks	O
using	O
our	O
dataset	O
,	O
and	O
demonstrate	O
the	O
benefit	O
of	O
better	O
models	O
that	O
make	O
more	O
use	O
of	O
the	O
graph	O
structure	O
.	O
There	O
is	O
still	O
significant	O
room	O
for	O
improvement	O
for	O
these	O
tasks	O
on	O
our	O
dataset	O
,	O
and	O
we	O
hope	O
the	O
release	O
of	O
the	O
data	O
and	O
baseline	O
code	O
can	O
help	O
spur	O
more	O
interest	O
in	O
developing	O
models	O
that	O
can	O
generate	O
long	O
text	O
conditioned	O
on	O
graphs	O
,	O
and	O
generate	O
graphs	O
given	O
text	O
,	O
which	O
is	O
another	O
exciting	O
direction	O
our	O
dataset	O
enables	O
but	O
we	O
did	O
not	O
explore	O
,	O
and	O
eventually	O
bridging	O
the	O
graph	O
and	O
text	O
modalities	O
.	O

We	O
show	O
additional	O
ablation	O
results	O
on	O
the	O
sample	O
length	O
(	O
Table	O
10	O
)	O
and	O
the	O
temperature	O
(	O
Table	O
11	O
)	O
for	O
greedy	O
sampling	O
.	O
Note	O
that	O
for	O
each	O
case	O
we	O
show	O
the	O
rBLEU	O
score	O
based	O
on	O
the	O
validation	O
set	O
computed	O
with	O
a	O
single	O
sampling	O
run	O
(	O
20	O
samples	O
per	O
graph	O
)	O
.	O
Note	O
that	O
the	O
GNN	O
model	O
has	O
overall	O
the	O
best	O
performance	O
.	O
However	O
as	O
the	O
sample	O
length	O
increases	O
the	O
advantage	O
of	O
the	O
GNN	O
model	O
also	O
decreases	O
.	O
This	O
indicates	O
that	O
it	O
is	O
still	O
very	O
challenging	O
to	O
generate	O
long	O
text	O
that	O
stays	O
on	O
-	O
topic	O
,	O
and	O
potentially	O
the	O
noise	O
overwhelms	O
the	O
signal	O
when	O
number	O
of	O
tokens	O
increases	O
to	O
4096	O
.	O
ns	O
/	O
type.object.id	O
ns	O
/	O
freebase.object	O
hints.best	O
hrid	O
"	O
A	O
\"protected	O
site\	O
"	O
is	O
any	O
location	O
that	O
is	O
protected	O
under	O
law	O
,	O
usually	O
by	O
being	O
designated	O
as	O
a	O
park	O
,	O
preserve	O
,	O
monument	O
,	O
etc	O
.	O
,	O
and	O
which	O
are	O
usually	O
under	O
the	O
control	O
(	O
at	O
least	O
in	O
part	O
)	O
by	O
some	O
form	O
of	O
government	O
agency	O
.	O
This	O
most	O
often	O
will	O
apply	O
to	O
areas	O
of	O
land	O
or	O
water	O
,	O
but	O
may	O
also	O
apply	O
to	O
human	O
-	O
made	O
structures	O
.	O
This	O
type	O
is	O
distinct	O
from	O
\"listed	O
sites\	O
"	O
,	O
which	O
have	O
been	O
designated	O
as	O
significant	O
,	O
but	O
may	O
not	O
have	O
any	O
legal	O
protection	O
thereby	O
.	O
However	O
,	O
many	O
such	O
places	O
will	O
be	O
both	O
,	O
...	O
Visualization	O
Ground	O
Truth	O
Text	O
Figure	O
7	O
=	O
Where	O
the	O
Streets	O
Have	O
No	O
Name	O
=	O
"	O
Where	O
the	O
Streets	O
Have	O
No	O
Name	O
"	O
is	O
a	O
song	O
by	O
Irish	O
rock	O
band	O
U2	O
.	O
It	O
is	O
the	O
opening	O
track	O
from	O
their	O
1987	O
album	O
The	O
Joshua	O
Tree	O
and	O
was	O
released	O
as	O
the	O
album	O
's	O
third	O
single	O
in	O
August	O
1987	O
.	O
The	O
song	O
's	O
hook	O
is	O
a	O
repeating	O
guitar	O
arpeggio	O
using	O
a	O
delay	O
effect	O
,	O
played	O
during	O
the	O
song	O
's	O
introduction	O
and	O
again	O
at	O
the	O
end	O
.	O
Lead	O
vocalist	O
Bono	O
wrote	O
the	O
lyrics	O
in	O
response	O
to	O
the	O
notion	O
that	O
it	O
is	O
possible	O
to	O
identify	O
a	O
person	O
's	O
religion	O
and	O
income	O
based	O
on	O
the	O
street	O
on	O
which	O
they	O
lived	O
,	O
particularly	O
in	O
Belfast	O
.	O
During	O
the	O
band	O
's	O
difficulties	O
recording	O
the	O
song	O
,	O
producer	O
Brian	O
Eno	O
considered	O
erasing	O
the	O
song	O
's	O
tapes	O
to	O
have	O
them	O
start	O
from	O
scratch	O
.	O
"	O
Where	O
the	O
Streets	O
Have	O
No	O
Name	O
"	O
was	O
praised	O
by	O
critics	O
and	O
became	O
a	O
commercial	O
success	O
,	O
peaking	O
at	O
number	O
thirteen	O
in	O
the	O
US	O
,	O
number	O
fourteen	O
in	O
Canada	O
,	O
number	O
ten	O
in	O
the	O
Netherlands	O
,	O
and	O
number	O
four	O
in	O
the	O
United	O
Kingdom	O
.	O
The	O
song	O
has	O
remained	O
a	O
staple	O
of	O
their	O
live	O
act	O
since	O
the	O
song	O
debuted	O
in	O
1987	O
on	O
The	O
Joshua	O
Tree	O
Tour	O
.	O
The	O
song	O
was	O
performed	O
on	O
a	O
Los	O
Angeles	O
rooftop	O
for	O
the	O
filming	O
of	O
its	O
music	O
video	O
,	O
which	O
won	O
a	O
Grammy	O
Award	O
for	O
Best	O
Performance	O
Music	O
Video	O
.	O
=	O
=	O
Writing	O
and	O
recording	O
=	O
=	O
The	O
music	O
for	O
"	O
Where	O
the	O
Streets	O
Have	O
No	O
Name	O
"	O
originated	O
from	O
a	O
demo	O
that	O
guitarist	O
The	O
Edge	O
composed	O
the	O
night	O
before	O
the	O
group	O
resumed	O
The	O
Joshua	O
Tree	O
sessions	O
.	O
In	O
an	O
upstairs	O
room	O
at	O
Melbeach	O
House	O
-	O
his	O
newly	O
purchased	O
home	O
-	O
The	O
Edge	O
used	O
a	O
four	O
@	O
-	O
@	O
track	O
tape	O
machine	O
to	O
record	O
an	O
arrangement	O
of	O
keyboards	O
,	O
bass	O
,	O
guitar	O
,	O
and	O
a	O
drum	O
machine	O
.	O
Realising	O
that	O
the	O
album	O
sessions	O
were	O
approaching	O
the	O
end	O
and	O
that	O
the	O
band	O
were	O
short	O
on	O
exceptional	O
live	O
songs	O
,	O
The	O
Edge	O
wanted	O
to	O
"	O
conjure	O
up	O
the	O
ultimate	O
U2	O
live	O
@	O
-	O
@	O
song	O
"	O
,	O
so	O
he	O
imagined	O
what	O
he	O
would	O
like	O
to	O
hear	O
at	O
a	O
future	O
U2	O
show	O
if	O
he	O
were	O
a	O
fan	O
.	O
After	O
finishing	O
the	O
rough	O
mix	O
,	O
he	O
felt	O
he	O
had	O
come	O
up	O
with	O
"	O
the	O
most	O
amazing	O
guitar	O
part	O
and	O
song	O
of	O
[	O
his	O
]	O
life	O
"	O
.	O
With	O
no	O
one	O
in	O
the	O
house	O
to	O
share	O
the	O
demo	O
with	O
,	O
The	O
Edge	O
recalls	O
dancing	O
around	O
and	O
punching	O
the	O
air	O
in	O
celebration	O
.	O
Although	O
the	O
band	O
liked	O
the	O
demo	O
,	O
it	O
was	O
difficult	O
for	O
them	O
to	O
record	O
the	O
song	O
.	O
Bassist	O
Adam	B-MethodName
Clayton	O
said	O
,	O
"	O
At	O
the	O
time	O
it	O
sounded	O
like	O
a	O
foreign	O
language	O
,	O
whereas	O
now	O
we	O
understand	O
how	O
it	O
works	O
"	O
.	O
The	O
arrangement	O
,	O
with	O
two	O
time	O
signature	O
shifts	O
and	O
frequent	O
chord	O
changes	O
,	O
was	O
rehearsed	O
many	O
times	O
,	O
but	O
the	O
group	O
struggled	O
to	O
get	O
a	O
performance	O
they	O
liked	O
.	O
According	O
to	O
co	O
@	O
-	O
@	O
producer	O
Daniel	O
Lanois	O
,	O
"	O
that	O
was	O
the	O
science	O
project	O
song	O
.	O
Figure	O
8	O
=	O
Fort	O
Scott	O
National	O
Historic	O
Site	O
=	O
Fort	O
Scott	O
National	O
Historic	O
Site	O
is	O
a	O
historical	O
area	O
under	O
the	O
control	O
of	O
the	O
United	O
States	O
National	O
Park	O
Service	O
in	O
Bourbon	O
County	O
,	O
Kansas	O
,	O
United	O
States	O
.	O
Named	O
after	O
General	B-DatasetName
Winfield	O
Scott	O
,	O
who	O
achieved	O
renown	O
during	O
the	O
Mexican	O
@	O
-	O
@	O
American	O
War	O
,	O
during	O
the	O
middle	O
of	O
the	O
19th	O
century	O
the	O
fort	O
served	O
as	O
a	O
military	O
base	O
for	O
US	O
Army	O
action	O
in	O
what	O
was	O
the	O
edge	O
of	O
settlement	O
in	O
1850	O
.	O
For	O
the	O
next	O
quarter	O
century	O
,	O
it	O
was	O
used	O
as	O
a	O
supply	O
base	O
and	O
to	O
provide	O
security	O
in	O
turbulent	O
areas	O
during	O
the	O
opening	O
of	O
the	O
West	O
to	O
settlement	O
,	O
a	O
period	O
which	O
included	O
Bleeding	O
Kansas	O
and	O
the	O
American	O
Civil	O
War	O
.	O
The	O
current	O
national	O
historic	O
site	O
protects	O
20	O
historic	O
structures	O
,	O
a	O
parade	O
ground	O
,	O
and	O
five	O
acres	O
(	O
20	O
@	O
,	O
@	O
000	O
m	O
2	O
)	O
of	O
restored	O
<	O
unk	O
>	O
prairie	O
,	O
inside	O
the	O
city	O
of	O
Fort	O
Scott	O
.	O
It	O
is	O
open	O
to	O
visitors	O
most	O
days	O
of	O
the	O
year	O
.	O
=	O
=	O
History	O
=	O
=	O
In	O
1842	O
,	O
Fort	O
Scott	O
was	O
named	O
after	O
Winfield	O
Scott	O
,	O
was	O
established	O
on	O
the	O
American	O
frontier	O
on	O
the	O
military	O
road	O
in	O
eastern	O
Kansas	O
between	O
Fort	O
Leavenworth	O
and	O
Fort	O
Gibson	O
.	O
It	O
was	O
established	O
to	O
provide	O
protection	O
to	O
the	O
rapidly	O
increasing	O
number	O
of	O
settlers	O
,	O
who	O
were	O
migrating	O
from	O
the	O
Eastern	O
United	O
States	O
.	O
Fort	O
Scott	O
became	O
one	O
of	O
a	O
chain	O
of	O
forts	O
intended	O
to	O
protect	O
the	O
new	O
settlers	O
from	O
the	O
Plains	O
Indians	O
,	O
as	O
well	O
as	O
to	O
protect	O
the	O
Indians	O
from	O
the	O
settlers	O
'	O
encroachment	O
.	O
The	O
United	O
States	O
government	O
intention	O
to	O
reserve	O
permanent	O
Indian	O
lands	O
west	O
of	O
the	O
Missouri	O
River	O
gave	O
way	O
to	O
the	O
competition	O
of	O
settlers	O
continuing	O
to	O
encroach	O
on	O
the	O
Indian	O
settlements	O
.	O
Fort	O
Scott	O
's	O
most	O
active	O
days	O
were	O
between	O
1842	O
and	O
1853	O
,	O
although	O
it	O
was	O
also	O
used	O
during	O
the	O
Civil	O
War	O
.	O
=	O
=	O
=	O
Army	O
days	O
=	O
=	O
=	O
The	O
Cherokee	O
of	O
Indian	O
Territory	O
(	O
now	O
Oklahoma	O
)	O
were	O
upset	O
to	O
have	O
Fort	O
Wayne	O
in	O
their	O
proximity	O
.	O
After	O
some	O
delay	O
,	O
the	O
US	O
Army	O
decided	O
to	O
abandon	O
Fort	O
Wayne	O
and	O
move	O
its	O
soldiers	O
to	O
a	O
new	O
fort	O
to	O
be	O
built	O
between	O
Fort	O
Leavenworth	O
and	O
the	O
site	O
.	O
The	O
Army	O
both	O
wanted	O
to	O
placate	O
the	O
Cherokee	O
(	O
who	O
were	O
supervised	O
by	O
the	O
Department	O
of	O
Defense	O
)	O
and	O
provide	O
more	O
defense	O
for	O
white	O
settlers	O
and	O
other	O
Indians	O
against	O
the	O
Osage	O
,	O
who	O
had	O
been	O
conducting	O
frequent	O
raids	O
in	O
the	O
area	O
.	O
On	O
April	O
1	O
,	O
1842	O
,	O
some	O
soldiers	O
of	O
Fort	O
Wayne	O
left	O
their	O
fort	O
and	O
on	O
April	O
22	O
arrived	O
where	O
Fort	O
Scott	O
would	O
be	O
built	O
,	O
in	O
the	O
Osage	O
<	O
unk	O
>	O
section	O
of	O
modern	O
@	O
-	O
@	O
day	O
Kansas	O
.	O
After	O
haggling	O
with	O
the	O
Cherokees	O
to	O
acquire	O
the	O
land	O
,	O
the	O
rest	O
of	O
Fort	O
Wayne	O
's	O
garrison	O
left	O
the	O
fort	O
on	O
May	O
26	O
and	O
arrived	O
at	O
the	O
Fort	O
Scott	O
site	O
on	O
May	O
30	O
.	O
Unlike	O
most	O
forts	O
for	O
military	O
use	O
,	O
the	O
fort	O
did	O
not	O
have	O
defensive	O
walls	O
or	O
structures	O

Generated	O
Text	O
-	O
=	O
Final	O
Fantasy	O
Adventure	O
=	O
Final	O
Fantasy	O
Adventure	O
is	O
a	O
platform	O
video	O
game	O
developed	O
by	O
Square	O
(	O
now	O
Square	O
Enix	O
)	O
and	O
published	O
by	O
Square	O
(	O
then	O
Square	O
Enix	O
)	O
for	O
the	O
Super	O
Nintendo	O
Entertainment	O
System	O
,	O
and	O
published	O
by	O
Nintendo	O
for	O
the	O
Nintendo	O
64	O
and	O
the	O
Nintendo	O
64	O
in	O
North	O
America	O
and	O
Europe	O
.	O
It	O
was	O
released	O
in	O
North	O
America	O
on	O
August	O
10	O
,	O
1995	O
for	O
the	O
Nintendo	O
64	O
and	O
in	O
Europe	O
in	O
March	O
1995	O
for	O
the	O
Wii	O
.	O
It	O
was	O
also	O
re	O
@	O
-	O
@	O
released	O
for	O
the	O
Wii	O
in	O
Japan	O
on	O
March	O
25	O
,	O
2015	O
for	O
the	O
Wii	O
U	O
's	O
Virtual	O
Console	O
service	O
.	O
Final	O
Fantasy	O
Adventure	O
is	O
set	O
in	O
the	O
fantasy	O
world	O
of	O
the	O
fictional	O
land	O
of	O
Ooo	O
,	O
between	O
1000	O
and	O
1000	O
years	O
before	O
the	O
events	O
of	O
the	O
story	O
of	O
the	O
kingdom	O
of	O
Ooo	O
.	O
The	O
story	O
follows	O
the	O
protagonists	O
,	O
Cid	O
,	O
a	O
boy	O
who	O
is	O
sent	O
to	O
Ooo	O
by	O
the	O
legendary	O
Jörmungandr	O
to	O
find	O
the	O
Wizard	O
of	O
Ooo	O
.	O
Throughout	O
the	O
game	O
,	O
players	O
train	O
for	O
a	O
series	O
of	O
random	O
battles	O
,	O
which	O
are	O
played	O
out	O
over	O
a	O
single	O
screen	O
.	O
In	O
the	O
end	O
,	O
players	O
control	O
a	O
"	O
Scarlet	O
Witch	O
"	O
who	O
is	O
summoned	O
to	O
the	O
realm	O
of	O
Ooo	O
.	O
The	O
game	O
was	O
well	O
@	O
-	O
@	O
received	O
,	O
with	O
praise	O
particularly	O
directed	O
at	O
its	O
graphics	O
and	O
its	O
gameplay	O
,	O
despite	O
criticism	O
for	O
its	O
repetitive	O
gameplay	O
.	O
Critics	O
generally	O
praised	O
its	O
graphics	O
,	O
plot	O
,	O
and	O
simplistic	O
gameplay	O
.	O
The	O
game	O
was	O
commercially	O
successful	O
,	O
selling	O
over	O
20	O
@	O
,	O
@	O
000	O
copies	O
,	O
and	O
in	O
2004	O
it	O
was	O
re	O
@	O
-	O
@	O
released	O
in	O
Japan	O
as	O
part	O
of	O
the	O
Final	O
Fantasy	O
Collections	O
series	O
.	O
=	O
=	O
Gameplay	O
=	O
=	O
The	O
game	O
is	O
a	O
third	O
@	O
-	O
@	O
person	O
shooter	O
,	O
with	O
gameplay	O
elements	O
related	O
to	O
worlds	O
,	O
time	O
travel	O
,	O
and	O
exploration	O
.	O
Characters	O
are	O
made	O
up	O
of	O
two	O
polygons	O
;	O
the	O
main	O
character	O
,	O
Aya	O
,	O
is	O
a	O
sprite	O
@	O
-	O
@	O
based	O
character	O
and	O
inhabits	O
a	O
world	O
of	O
trees	O
and	O
caves	O
,	O
where	O
she	O
is	O
able	O
to	O
perform	O
offensive	O
actions	O
along	O
the	O
way	O
.	O
The	O
protagonist	O
,	O
Aya	O
,	O
is	O
a	O
heavily	O
armored	O
,	O
humanoid	O
creature	O
with	O
red	O
eyes	O
and	O
black	O
eyes	O
.	O
In	O
contrast	O
to	O
other	O
Final	O
Fantasy	O
games	O
,	O
her	O
prominent	O
appearances	O
in	O
this	O
game	O
are	O
not	O
as	O
straightforward	O
as	O
in	O
previous	O
Final	O
Fantasy	O
installments	O
.	O
The	O
game	O
's	O
world	O
is	O
divided	O
into	O
two	O
sections	O
:	O
the	O
world	O
of	O
Ooo	O
and	O
the	O
world	O
of	O
the	O
dinosaurs	O
.	O
The	O
player	O
's	O
adventure	O
begins	O
with	O
Aya	O
,	O
a	O
girl	O
who	O
initially	O
leads	O
the	O
plot	O
of	O
Twilight	O
Town	O
,	O
but	O
later	O
returns	O
to	O
Ooo	O
to	O
save	O
her	O
sister	O
.	O
Aya	O
explores	O
the	O
area	O
and	O
is	O
able	O
to	O
save	O
her	O
family	O
.	O
After	O
interacting	O
with	O
the	O
player	O
,	O
Aya	O
is	O
able	O
to	O
resume	O
the	O
story	O
in	O
the	O
future	O
.	O
The	O
game	O
's	O
main	O
family	O
member	O
,	O
Aya	O
,	O
is	O
a	O
woman	O
named	O
Alice	O
.	O
She	O
is	O
a	O
member	O
of	O
the	O
religious	O
order	O
,	O
the	O
Jehovah	O
's	O
Witnesses	O
,	O
who	O
is	O
a	O
member	O
of	O
BoW	O
=	O
Zoo	O
Station	O
(	O
song	O
)	O
=	O
"	O
Zoo	O
Station	O
"	O
is	O
a	O
song	O
by	O
Australian	O
rock	O
band	O
U2	O
from	O
their	O
second	O
studio	O
album	O
,	O
PopMart	O
,	O
released	O
in	O
1992	O
as	O
the	O
album	O
's	O
lead	O
single	O
.	O
Rise	O
to	O
popularity	O
after	O
its	O
release	O
prompted	O
U2	O
to	O
record	O
two	O
songs	O
with	O
bassist	O
Adam	B-MethodName
Clayton	O
and	O
guitarist	O
Joe	O
Bono	O
,	O
as	O
they	O
felt	O
that	O
it	O
was	O
their	O
breakthrough	O
album	O
.	O
The	O
song	O
comprises	O
lyrics	O
set	O
in	O
an	O
acoustic	O
,	O
with	O
lyrics	O
that	O
have	O
always	O
focused	O
on	O
a	O
woman	O
being	O
stuck	O
in	O
an	O
abusive	O
relationship	O
.	O
It	O
was	O
developed	O
due	O
to	O
the	O
band	O
's	O
involvement	O
in	O
the	O
construction	O
of	O
Zoo	O
Station	O
and	O
its	O
accompanying	O
soundtrack	O
,	O
and	O
it	O
was	O
inspired	O
by	O
U2	O
's	O
admiration	O
of	O
the	O
groups	O
they	O
both	O
represented	O
.	O
The	O
song	O
was	O
inspired	O
by	O
a	O
series	O
of	O
incidents	O
that	O
occurred	O
in	O
El	O
Salvador	O
when	O
a	O
man	O
repeatedly	O
attacked	O
an	O
apartment	O
building	O
.	O
The	O
song	O
received	O
positive	O
reviews	O
from	O
music	O
critics	O
,	O
and	O
it	O
was	O
labelled	O
a	O
"	O
masterpiece	O
"	O
by	O
several	O
publications	O
.	O
Thirty	O
@	O
-	O
@	O
five	O
tracks	O
were	O
released	O
as	O
singles	O
,	O
and	O
the	O
song	O
has	O
been	O
covered	O
by	O
numerous	O
artists	O
,	O
including	O
such	O
figures	O
as	O
Sam	O
&	O
Max	O
,	O
The	O
<	O
unk	O
>	O
,	O
and	O
Mickey	O
Rourke	O
.	O
The	O
band	O
released	O
their	O
version	O
of	O
"	O
Zoo	O
Station	O
"	O
on	O
non	O
@	O
-	O
@	O
consecutive	O
studio	O
albums	O
,	O
with	O
"	O
Where	O
the	O
Streets	O
Have	O
No	O
Name	O
"	O
and	O
"	O
Numb	O
"	O
released	O
in	O
1999	O
and	O
2000	O
respectively	O
.	O
"	O
Zoo	O
Station	O
"	O
is	O
on	O
the	O
band	O
's	O
greatest	O
hits	O
album	O
,	O
10	O
Mile	O
Wild	O
,	O
and	O
on	O
their	O
greatest	O
hits	O
album	O
Across	O
the	O
Universe	O
.	O
It	O
was	O
performed	O
live	O
at	O
the	O
Vertigo	O
Tour	O
in	O
2007	O
.	O
The	O
song	O
was	O
included	O
on	O
the	O
band	O
's	O
fifth	O
studio	O
album	O
,	O
New	O
Order	O
(	O
2008	O
)	O
,	O
and	O
was	O
included	O
on	O
the	O
film	O
The	O
Last	O
Years	O
(	O
2012	O
)	O
.	O
=	O
=	O
Background	O
=	O
=	O
"	O
Zoo	O
Station	O
"	O
was	O
written	O
by	O
U2	O
frontman	O
Bono	O
and	O
produced	O
by	O
U2	O
and	O
was	O
the	O
first	O
song	O
on	O
the	O
album	O
.	O
Clayton	O
still	O
had	O
a	O
strong	O
fan	O
base	O
and	O
was	O
a	O
regular	O
singer	O
.	O
The	O
two	O
met	O
while	O
performing	O
in	O
the	O
amphitheaters	O
of	O
Los	O
Angeles	O
,	O
and	O
in	O
a	O
live	O
performance	O
he	O
was	O
the	O
lead	O
singer	O
on	O
the	O
last	O
leg	O
of	O
the	O
Vertigo	O
Tour	O
.	O
The	O
Edge	O
and	O
Clayton	O
were	O
both	O
fans	O
of	O
the	O
band	O
,	O
and	O
the	O
pair	O
decided	O
to	O
collaborate	O
on	O
the	O
album	O
.	O
Both	O
performed	O
on	O
their	O
2004	O
tour	O
,	O
and	O
made	O
a	O
solo	O
appearance	O
on	O
the	O
2004	O
The	O
Zoo	O
TV	O
Tour	O
.	O
Clayton	O
and	O
Clayton	O
had	O
been	O
close	O
friends	O
,	O
and	O
the	O
pair	O
became	O
friends	O
again	O
in	O
2008	O
.	O
In	O
late	O
April	O
2004	O
,	O
U2	O
announced	O
that	O
the	O
song	O
had	O
been	O
released	O
as	O
the	O
first	O
single	O
for	O
the	O
album	O
,	O
and	O
would	O
be	O
released	O
on	O
31	O
May	O
,	O
five	O
weeks	O
after	O
the	O
album	O
's	O
release	O
.	O
"	O
Zoo	O
Station	O
"	O
was	O
released	O
as	O
the	O
fourth	O
single	O
from	O
PopMart	O
and	O
premiered	O
on	O
13	O
June	O
2005	O
.	O
The	O
song	O
is	O
a	O
Nodes	O
=	O
You	O
Can	O
'	O
t	O
Take	O
This	O
Away	O
(	O
U2	O
song	O
)	O
=	O
"	O
You	O
Can	O
'	O
t	O
Take	O
This	O
Away	O
(	O
U2	O
song	O
)	O
"	O
is	O
a	O
song	O
by	O
the	O
Irish	O
rock	O
band	O
U2	O
.	O
It	O
was	O
written	O
by	O
vocalist	O
Bono	O
and	O
produced	O
by	O
The	O
Smiths	O
for	O
their	O
third	O
solo	O
album	O
,	O
The	O
Joshua	O
Tree	O
.	O
Inspired	B-DatasetName
by	O
Romania	O
roots	O
rock	O
and	O
roll	O
,	O
"	O
You	O
Can	O
'	O
t	O
Take	O
This	O
Away	O
"	O
is	O
a	O
song	O
about	O
a	O
man	O
who	O
suffers	O
from	O
severe	O
nightmares	O
.	O
The	O
song	O
was	O
first	O
released	O
on	O
the	O
album	O
and	O
premiered	O
on	O
radio	O
on	O
19	O
August	O
1996	O
,	O
where	O
it	O
reached	O
number	O
40	O
on	O
the	O
Billboard	O
Hot	O
100	O
.	O
"	O
You	O
Can	O
'	O
t	O
Take	O
This	O
Away	O
(	O
U2	O
song	O
)	O
"	O
received	O
mixed	O
reviews	O
from	O
critics	O
;	O
some	O
tracks	O
,	O
such	O
as	O
the	O
opening	O
chorus	O
and	O
bridge	O
,	O
were	O
praised	O
as	O
highlights	O
by	O
some	O
reviewers	O
.	O
The	O
song	O
was	O
well	O
received	O
by	O
critics	O
,	O
as	O
the	O
record	O
company	O
's	O
highest	O
@	O
-	O
@	O
selling	O
single	O
at	O
that	O
time	O
,	O
where	O
"	O
The	O
Edge	O
of	O
Forever	O
"	O
peaked	O
at	O
number	O
64	O
.	O
It	O
was	O
later	O
re	O
@	O
-	O
@	O
released	O
on	O
the	O
band	O
's	O
2006	O
compilation	O
album	O
No	O
Line	O
on	O
the	O
Horizon	O
,	O
but	O
has	O
since	O
been	O
re	O
@	O
-	O
@	O
released	O
on	O
live	O
performances	O
in	O
2006	O
and	O
2009	O
.	O
"	O
You	O
Can	O
'	O
t	O
Take	O
This	O
Away	O
(	O
U2	O
song	O
)	O
"	O
was	O
performed	O
on	O
the	O
Late	O
Show	O
with	O
David	O
Letterman	O
on	O
31	O
December	O
2005	O
.	O
"	O
You	O
Can	O
'	O
t	O
Take	O
This	O
Away	O
(	O
U2	O
song	O
)	O
"	O
has	O
since	O
been	O
covered	O
by	O
many	O
groups	O
and	O
has	O
achieved	O
enormous	O
commercial	O
success	O
.	O
A	O
video	O
for	O
the	O
song	O
was	O
filmed	O
by	O
then	O
@	O
-	O
@	O
frontman	O
Bono	O
,	O
for	O
which	O
it	O
was	O
nominated	O
for	O
a	O
Grammy	O
Award	O
.	O
=	O
=	O
Background	O
and	O
writing	O
=	O
=	O
"	O
You	O
Can	O
'	O
t	O
Take	O
This	O
Away	O
(	O
Kingdom	O
of	O
Ireland	O
song	O
)	O
"	O
is	O
a	O
track	O
that	O
features	O
Bono	O
and	O
The	O
Smiths	O
discussing	O
their	O
relationship	O
and	O
how	O
they	O
changed	O
their	O
lives	O
.	O
His	O
father	O
,	O
Jack	O
Clayton	O
,	O
was	O
assassinated	O
in	O
1981	O
at	O
the	O
age	O
of	O
23	O
.	O
Bono	O
was	O
born	O
in	O
Philadelphia	O
and	O
worked	O
for	O
a	O
business	O
,	O
first	O
working	O
as	O
a	O
secretary	O
in	O
Los	O
Angeles	O
,	O
then	O
as	O
a	O
photographer	O
for	O
a	O
commercial	O
for	O
Primus	O
.	O
He	O
later	O
worked	O
for	O
the	O
Coca	O
@	O
-	O
@	O
Cola	O
Company	O
as	O
a	O
drummer	O
in	O
the	O
music	O
industry	O
.	O
The	O
musician	O
picked	O
up	O
the	O
song	O
again	O
after	O
Nikolai	O
<	O
unk	O
>	O
,	O
an	O
engineer	O
who	O
worked	O
with	O
The	O
Smiths	O
,	O
heard	O
it	O
and	O
decided	O
to	O
play	O
it	O
for	O
him	O
after	O
U2	O
agreed	O
to	O
record	O
it	O
for	O
a	O
solo	O
album	O
.	O
The	O
band	O
originally	O
intended	O
to	O
release	O
"	O
You	O
Can	O
'	O
t	O
Take	O
This	O
Away	O
"	O
as	O
a	O
single	O
;	O
however	O
,	O
with	O
a	O
critical	O
failure	O
,	O
the	O
song	O
eventually	O
became	O
a	O
single	O
.	O
In	O
2004	O
,	O
"	O
You	O
Can	O
'	O
t	O
Take	O
This	O
Away	O
(	O
U2	O
song	O
)	O
"	O
is	O
one	O
of	O
two	O
songs	O
on	O
the	O
album	O
that	O
U2	O
released	O
as	O
a	O
single	O
with	O
approval	O
of	O
the	O
record	O
label	O
.	O
The	O
[	O
]	O
GNN	O
=	O
The	O
Edge	O
of	O
Glory	O
=	O
"	O
The	O
Edge	O
of	O
Glory	O
"	O
is	O
a	O
song	O
by	O
Irish	O
rock	O
band	O
U2	O
,	O
released	O
as	O
a	O
single	O
.	O
It	O
was	O
written	O
by	O
guitarist	O
Larry	O
Mullen	O
,	O
Jr	O
.	O
,	O
who	O
plays	O
lead	O
guitar	O
on	O
the	O
song	O
,	O
and	O
was	O
produced	O
by	O
Alex	O
<	O
unk	O
>	O
,	O
who	O
described	O
the	O
song	O
as	O
"	O
a	O
track	O
with	O
a	O
lot	O
of	O
meaning	O
,	O
but	O
no	O
connection	O
.	O
"	O
The	O
song	O
contains	O
several	O
pop	O
rock	O
elements	O
and	O
is	O
set	O
in	O
that	O
time	O
period	O
,	O
and	O
is	O
among	O
the	O
most	O
prominent	O
in	O
the	O
album	O
.	O
In	O
addition	O
to	O
its	O
lyrics	O
,	O
the	O
song	O
's	O
lyrics	O
detail	O
hypocrisy	O
,	O
and	O
also	O
deals	O
with	O
the	O
effects	O
of	O
adultery	O
.	O
The	O
song	O
's	O
lyrics	O
have	O
been	O
described	O
by	O
music	O
critics	O
as	O
being	O
autobiographical	O
.	O
The	O
lyrics	O
have	O
been	O
described	O
as	O
"	O
a	O
bold	O
exploration	O
of	O
the	O
figure	O
of	O
a	O
New	O
York	O
City	O
man	O
"	O
,	O
and	O
"	O
an	O
expression	O
of	O
the	O
inability	O
of	O
freedom	O
to	O
live	O
in	O
a	O
world	O
that	O
is	O
also	O
a	O
place	O
in	O
the	O
world	O
of	O
space	O
.	O
"	O
The	O
song	O
's	O
lyrics	O
describe	O
a	O
"	O
Manhattan	O
@	O
-	O
@	O
like	O
place	O
"	O
,	O
with	O
Bono	O
calling	O
the	O
arrival	O
a	O
"	O
pleasant	O
little	O
optimism	O
from	O
before	O
it	O
came	O
to	O
life	O
.	O
"	O
"	O
The	O
Edge	O
of	O
Glory	O
"	O
was	O
a	O
success	O
in	O
the	O
United	O
Kingdom	O
,	O
reaching	O
number	O
two	O
in	O
the	O
charts	O
in	O
the	O
United	O
States	O
,	O
and	O
topping	O
the	O
charts	O
in	O
Australia	O
and	O
New	O
Zealand	O
.	O
The	O
song	O
has	O
been	O
certified	O
platinum	O
by	O
the	O
Recording	O
Industry	O
Association	O
of	O
America	O
,	O
and	O
has	O
sold	O
over	O
four	O
million	O
copies	O
worldwide	O
.	O
The	O
song	O
has	O
been	O
covered	O
by	O
several	O
artists	O
,	O
including	O
German	O
band	O
U2	O
.	O
The	O
music	O
video	O
for	O
"	O
The	O
Edge	O
of	O
Glory	O
"	O
won	O
Best	O
Video	O
at	O
the	O
2004	O
MTV	O
Video	O
Music	O
Awards	O
.	O
The	O
video	O
also	O
served	O
as	O
an	O
inspiration	O
for	O
the	O
film	O
U2	O
360	O
(	O
1998	O
)	O
.	O
=	O
=	O
Background	O
=	O
=	O
The	O
song	O
has	O
been	O
described	O
as	O
a	O
"	O
relaxed	O
representation	O
"	O
of	O
globalization	O
,	O
with	O
Bono	O
proclaiming	O
himself	O
the	O
"	O
lost	O
king	O
of	O
rock	O
'	O
n	O
'	O
roll	O
"	O
,	O
and	O
Chris	O
McGuinness	O
as	O
"	O
the	O
only	O
one	O
who	O
has	O
ever	O
achieved	O
the	O
sound	O
of	O
a	O
rock	O
'	O
n	O
'	O
roll	O
.	O
"	O
Bono	O
's	O
lyrics	O
have	O
been	O
described	O
as	O
a	O
parody	O
of	O
Lord	O
Byron	O
's	O
"	O
My	O
Own	O
Time	O
"	O
,	O
and	O
as	O
an	O
"	O
attack	O
on	O
social	O
and	O
political	O
issues	O
"	O
.	O
=	O
=	O
Recording	O
and	O
production	O
=	O
=	O
Bono	O
and	O
U2	O
made	O
the	O
final	O
stages	O
of	O
recording	O
the	O
song	O
at	O
the	O
Abbey	O
Road	O
Studios	O
in	O
Dublin	O
,	O
Dublin	O
.	O
The	O
sessions	O
were	O
divided	O
into	O
two	O
sessions	O
:	O
Sessions	O
at	O
Damien	O
the	O
flautist	O
and	O
Context	O
at	O
the	O
Cave	O
of	O
Christ	O
.	O
The	O
results	O
of	O
the	O
sessions	O
were	O
mixed	O
by	O
Brian	O
Eno	O
.	O
U2	O
was	O
very	O
excited	O
to	O
record	O
the	O
result	O
,	O
with	O
Eno	O
referring	O
to	O
the	O
recording	O
as	O
a	O
"	O
special	O
event	O
"	O
,	O
and	O
they	O
decided	O
to	O
change	O
the	O
track	O

Generated	O
Text	O
-	O
=	O
Carbuncle	O
=	O
Bloodlines	O
is	O
a	O
2011	O
action	O
role	O
@	O
-	O
@	O
playing	O
video	O
game	O
developed	O
by	O
<	O
unk	O
>	O
and	O
published	O
by	O
Activision	O
for	O
the	O
Wii	O
.	O
The	O
game	O
is	O
part	O
of	O
a	O
sequel	O
of	O
the	O
2013	O
game	O
Eclipse	O
:	O
The	O
Series	O
.	O
It	O
was	O
published	O
by	O
Activision	O
on	O
November	O
19	O
,	O
2011	O
,	O
for	O
Microsoft	O
Windows	O
,	O
Apple	O
II	O
,	O
Xbox	O
360	O
,	O
iOS	O
,	O
and	O
Xbox	O
360	O
.	O
The	O
game	O
,	O
set	O
in	O
the	O
fictional	O
Middle	O
Ages	O
,	O
revolves	O
around	O
the	O
player	O
@	O
-	O
@	O
character	O
centaurs	O
battling	O
for	O
control	O
of	O
their	O
universe	O
.	O
The	O
protagonist	O
of	O
the	O
game	O
,	O
Percy	O
,	O
the	O
leader	O
of	O
the	O
Knights	O
of	O
the	O
Old	O
Republic	O
,	O
is	O
a	O
member	O
of	O
the	O
Knights	O
of	O
the	O
Old	O
Republic	O
,	O
and	O
is	O
appointed	O
to	O
lead	O
a	O
military	O
coup	O
to	O
overthrow	O
the	O
Irish	O
Republic	O
and	O
destroy	O
the	O
Home	O
Nations	B-DatasetName
'	O
military	O
forces	O
.	O
Though	O
the	O
game	O
mainly	O
took	O
place	O
in	O
a	O
new	O
version	O
of	O
the	O
New	O
York	O
City	O
,	O
the	O
original	O
plan	O
was	O
to	O
make	O
it	O
more	O
easily	O
accessible	O
to	O
players	O
unfamiliar	O
with	O
the	O
New	O
Republic	O
.	O
It	O
was	O
also	O
a	O
commercial	O
success	O
,	O
selling	O
more	O
than	O
900	O
@	O
,	O
@	O
000	O
copies	O
.	O
The	O
game	O
received	O
mostly	O
positive	O
reviews	O
from	O
most	O
video	O
game	O
publications	O
,	O
with	O
many	O
praising	O
the	O
visual	O
style	O
and	O
the	O
gameplay	O
,	O
but	O
many	O
said	O
that	O
it	O
was	O
not	O
as	O
good	O
as	O
that	O
of	O
the	O
previous	O
game	O
.	O
Reviewers	O
noted	O
the	O
game	O
's	O
title	O
forward	O
addressing	O
issues	O
such	O
as	O
the	O
difficulty	O
level	O
,	O
a	O
general	O
danger	O
of	O
being	O
too	O
difficult	O
to	O
fight	O
,	O
and	O
the	O
difficulty	O
of	O
playing	O
the	O
game	O
as	O
the	O
player	O
@	O
-	O
@	O
character	O
's	O
pattern	O
of	O
character	O
.	O
=	O
=	O
Gameplay	O
=	O
=	O
Bloodlines	O
is	O
a	O
crossover	O
action	O
role	O
@	O
-	O
@	O
playing	O
game	O
that	O
takes	O
place	O
in	O
the	O
fictional	O
Middle	O
Ages	O
,	O
which	O
is	O
composed	O
of	O
medieval	O
countries	O
and	O
locales	O
.	O
Valhalla	O
,	O
a	O
medieval	O
stronghold	O
,	O
is	O
the	O
game	O
's	O
main	O
setting	O
.	O
The	O
player	O
@	O
-	O
@	O
character	O
is	O
a	O
3	O
@	O
-	O
@	O
D	O
miniature	O
character	O
with	O
a	O
sword	O
and	O
shield	O
,	O
which	O
have	O
multiple	O
colored	O
attacks	O
,	O
and	O
has	O
two	O
of	O
the	O
four	O
abilities	O
,	O
which	O
are	O
progressively	O
reduced	O
from	O
the	O
first	O
one	O
and	O
allow	O
for	O
greater	O
size	O
and	O
movement	O
.	O
The	O
available	O
weapons	O
are	O
bolt	O
@	O
-	O
@	O
fired	O
weapons	O
,	O
advanced	O
weapons	O
,	O
and	O
weapons	O
that	O
can	O
be	O
used	O
in	O
battle	O
.	O
The	O
player	O
is	O
able	O
to	O
summon	O
magical	O
powers	O
to	O
attack	O
targets	O
,	O
and	O
can	O
use	O
magical	O
powers	O
to	O
enhance	O
the	O
character	O
's	O
abilities	O
.	O
<	O
unk	O
>	O
are	O
also	O
available	O
via	O
a	O
<	O
unk	O
>	O
system	O
,	O
which	O
enables	O
players	O
to	O
throw	O
stones	O
at	O
enemies	O
and	O
attack	O
enemy	O
characters	O
who	O
have	O
not	O
encountered	O
them	O
.	O
The	O
player	O
character	O
also	O
has	O
an	O
ability	O
to	O
revive	O
foes	O
by	O
performing	O
a	O
touch	O
@	O
-	O
@	O
screen	O
action	O
.	O
The	O
game	O
can	O
be	O
played	O
as	O
a	O
side	O
@	O
-	O
@	O
scrolling	O
through	O
a	O
View	O
Mode	O
,	O
which	O
can	O
be	O
used	O
in	O
the	O
single	O
@	O
-	O
@	O
player	O
mode	O
.	O
The	O
first	O
act	O
features	O
a	O
"	O
<	O
unk	O
>	O
"	O
displayed	O
from	O
a	O
first	O
@	O
-	O
@	O
person	O
perspective	O
.	O
The	O
player	O
character	O
can	O
move	O
around	O
BoW	O
=	O
Civil	O
War	O
Pass	O
=	O
Civil	O
War	O
Pass	O
,	O
also	O
known	O
as	O
the	O
Battle	O
of	O
the	O
Crater	O
or	O
the	O
Battle	O
of	O
Fort	O
Sumner	O
,	O
was	O
an	O
important	O
battle	O
fought	O
on	O
September	O
7	O
,	O
1864	O
,	O
at	O
Fort	O
Coldwater	O
,	O
in	O
the	O
state	O
of	O
Montana	O
.	O
After	O
seeing	O
repeated	O
attacks	O
on	O
the	O
fort	O
,	O
Gen.	O
James	O
A.	O
Douglas	O
,	O
the	O
commander	O
of	O
the	O
Confederate	O
forces	O
in	O
the	O
South	O
,	O
decided	O
to	O
abandon	O
the	O
fort	O
and	O
flee	O
to	O
the	O
north	O
.	O
After	O
Union	O
forces	O
struck	O
the	O
fort	O
,	O
they	O
decided	O
to	O
flee	O
south	O
to	O
the	O
Ohio	O
River	O
.	O
There	O
they	O
quickly	O
encountered	O
a	O
group	O
of	O
horses	O
,	O
who	O
were	O
used	O
to	O
build	O
a	O
pontoon	O
bridge	O
.	O
The	O
ditches	O
and	O
wooden	O
planks	O
were	O
removed	O
and	O
replaced	O
with	O
stone	O
blocks	O
to	O
make	O
them	O
float	O
(	O
plow	O
)	O
.	O
The	O
obstacles	O
that	O
were	O
created	O
in	O
the	O
river	O
valley	O
,	O
however	O
,	O
proved	O
treacherous	O
and	O
were	O
not	O
bridged	O
by	O
mountain	O
passes	O
.	O
The	O
young	O
general	O
and	O
his	O
troops	O
eventually	O
reached	O
the	O
Ohio	O
and	O
the	O
Mississippi	O
rivers	O
,	O
but	O
the	O
new	O
Presidential	O
candidate	O
,	O
Abraham	O
Lincoln	O
,	O
resigned	O
after	O
the	O
war	O
.	O
After	O
the	O
defeat	O
at	O
Fort	O
Sumner	O
,	O
General	B-DatasetName
Douglas	O
,	O
the	O
commander	O
of	O
the	O
Union	O
forces	O
,	O
planned	O
and	O
executed	O
a	O
number	O
of	O
attacks	O
on	O
Fort	O
Sumner	O
.	O
When	O
soldiers	O
arrived	O
,	O
they	O
found	O
two	O
now	O
@	O
-	O
@	O
deserted	O
locations	O
.	O
The	O
attacks	O
had	O
been	O
made	O
more	O
than	O
a	O
year	O
before	O
.	O
When	O
the	O
line	O
of	O
retreat	O
of	O
the	O
Union	O
forces	O
,	O
which	O
stretched	O
from	O
Fort	O
Sumner	O
to	O
Fort	O
Sumner	O
,	O
reached	O
Fort	O
Sumner	O
on	O
August	O
19	O
,	O
1864	O
,	O
the	O
cavalrymen	O
captured	O
it	O
on	O
September	O
30	O
.	O
In	O
November	O
1864	O
,	O
General	B-DatasetName
Douglas	O
was	O
defeated	O
at	O
the	O
Battle	O
of	O
Lake	O
Logan	O
.	O
=	O
=	O
Background	O
=	O
=	O
In	O
1861	O
,	O
with	O
the	O
Mexican	O
@	O
-	O
@	O
American	O
War	O
nearing	O
its	O
conclusion	O
,	O
the	O
American	O
public	O
began	O
to	O
think	O
of	O
an	O
armistice	O
treaty	O
,	O
or	O
peace	O
treaty	O
between	O
Mexico	O
and	O
the	O
United	O
States	O
.	O
On	O
July	O
1	O
,	O
1861	O
,	O
General	B-DatasetName
Douglas	O
sent	O
three	O
large	O
armies	O
from	O
the	O
Mexican	O
@	O
-	O
@	O
American	O
War	O
,	O
a	O
series	O
of	O
forts	O
west	O
of	O
the	O
Rockies	O
,	O
to	O
attack	O
Fort	O
Vicksburg	O
.	O
The	O
forts	O
were	O
destroyed	O
in	O
a	O
siege	O
in	O
June	O
.	O
These	O
were	O
built	O
during	O
the	O
years	O
it	O
was	O
fought	O
by	O
the	O
Confederate	O
States	O
of	O
America	O
.	O
The	O
British	O
and	O
Americans	O
were	O
unprepared	O
for	O
the	O
chance	O
of	O
victory	O
,	O
and	O
the	O
Americans	O
were	O
now	O
planning	O
to	O
take	O
control	O
of	O
the	O
Gulf	O
Coast	O
.	O
Like	O
the	O
Americans	O
,	O
the	O
British	O
were	O
planning	O
an	O
attack	O
into	O
central	O
Canada	O
.	O
The	O
British	O
were	O
aware	O
that	O
the	O
main	O
invasion	O
of	O
Canada	O
would	O
occur	O
on	O
July	O
8	O
.	O
The	O
British	O
were	O
near	O
the	O
Niagara	O
River	O
and	O
the	O
Union	O
were	O
hopefully	O
midway	O
along	O
the	O
river	O
,	O
approaching	O
Fort	O
Sumner	O
from	O
the	O
west	O
.	O
The	O
British	O
were	O
reluctant	O
to	O
move	O
toward	O
the	O
Carolinas	O
,	O
and	O
so	O
,	O
in	O
the	O
event	O
the	O
Port	O
of	O
Boston	O
was	O
abandoned	O
,	O
the	O
British	O
would	O
be	O
forced	O
to	O
travel	O
to	O
the	O
lower	O
Mississippi	O
.	O
The	O
Nodes	O
=	O
Fort	O
Scott	O
=	O
Fort	O
Scott	O
is	O
an	O
American	O
military	O
post	O
located	O
in	O
Fort	O
Lee	O
,	O
Kansas	O
.	O
It	O
is	O
named	O
in	O
honor	O
of	O
General	B-DatasetName
William	O
Scott	O
,	O
a	O
U.S.	O
Army	O
general	O
and	O
the	O
first	O
commander	O
of	O
the	O
Army	O
of	O
the	O
Potomac	O
.	O
The	O
site	O
was	O
designated	O
as	O
a	O
National	O
Historic	O
Landmark	O
in	O
1991	O
,	O
and	O
has	O
been	O
designated	O
a	O
National	O
Historic	O
Landmark	O
under	O
the	O
title	O
of	O
Fort	O
Scott	O
Historical	O
Site	O
since	O
1929	O
.	O
It	O
is	O
located	O
in	O
the	O
Rocky	O
Mountains	O
in	O
Kansas	O
and	O
is	O
known	O
as	O
the	O
"	O
James	O
Scott	O
National	O
Historic	O
Site	O
"	O
.	O
=	O
=	O
History	O
=	O
=	O
The	O
original	O
having	O
been	O
settled	O
by	O
the	O
Caddo	O
on	O
the	O
Black	O
River	O
,	O
and	O
later	O
moved	O
to	O
Fort	O
Lee	O
in	O
present	O
@	O
-	O
@	O
day	O
Decatur	O
County	O
,	O
Virginia	O
.	O
On	O
July	O
10	O
,	O
1810	O
,	O
the	O
Hennepin	O
reported	O
that	O
the	O
Caddo	O
had	O
acquired	O
the	O
territory	O
of	O
Fort	O
Lee	O
,	O
but	O
it	O
is	O
unclear	O
whether	O
he	O
was	O
present	O
there	O
.	O
He	O
may	O
have	O
taken	O
a	O
position	O
that	O
had	O
previously	O
been	O
occupied	O
by	O
other	O
people	O
.	O
Around	O
1800	O
,	O
the	O
first	O
Governor	O
of	O
Kansas	O
,	O
Colonel	O
Andrew	O
H.	O
Sharpe	O
,	O
established	O
Fort	O
Scott	O
in	O
what	O
is	O
now	O
a	O
part	O
of	O
Fort	O
Lee	O
.	O
The	O
fort	O
was	O
constructed	O
on	O
a	O
site	O
that	O
he	O
had	O
named	O
Fort	O
Scott	O
,	O
and	O
was	O
known	O
as	O
Fort	O
Douglas	O
.	O
The	O
fort	O
was	O
used	O
for	O
administrative	O
purposes	O
and	O
for	O
administration	O
of	O
the	O
Missouri	O
Territory	O
.	O
In	O
1808	O
,	O
William	O
Bolivar	O
Buckner	O
led	O
a	O
large	O
movement	O
to	O
remove	O
the	O
western	O
boundary	O
of	O
Texas	B-DatasetName
,	O
including	O
Fort	O
Scott	O
.	O
Congress	O
authorized	O
a	O
survey	O
of	O
the	O
territory	O
in	O
1817	O
,	O
and	O
a	O
survey	O
of	O
the	O
Old	O
South	O
boundary	O
was	O
completed	O
in	O
1818	O
,	O
making	O
Fort	O
Scott	O
the	O
first	O
governor	O
to	O
apply	O
federal	O
law	O
.	O
Although	O
the	O
West	O
Texas	B-DatasetName
Aftermath	O
quickly	O
became	O
a	O
national	O
concern	O
,	O
the	O
new	O
governor	O
was	O
unable	O
to	O
raise	O
sufficient	O
funds	O
to	O
maintain	O
Fort	O
Scott	O
.	O
The	O
fort	O
's	O
construction	O
and	O
construction	O
were	O
completed	O
in	O
1821	O
,	O
but	O
the	O
state	O
legislature	O
refused	O
to	O
grant	O
the	O
commissioners	O
the	O
land	O
they	O
were	O
granted	O
.	O
The	O
new	O
land	O
,	O
called	O
Fort	O
Dix	O
,	O
was	O
consequently	O
purchased	O
by	O
the	O
U.S.	O
Army	O
.	O
The	O
fort	O
's	O
name	O
was	O
later	O
changed	O
to	O
Fort	O
Lee	O
.	O
While	O
the	O
two	O
states	O
were	O
in	O
dispute	O
by	O
the	O
1832	O
treaty	O
,	O
Fort	O
Dix	O
was	O
located	O
in	O
the	O
Horn	O
of	O
the	O
Midget	O
Valley	O
,	O
part	O
of	O
the	O
Pan	O
@	O
-	O
@	O
American	O
Native	O
Reservation	O
.	O
Confederate	O
forces	O
launched	O
a	O
cavalry	O
attack	O
on	O
Fort	O
Dix	O
in	O
early	O
1835	O
,	O
but	O
both	O
sides	O
suffered	O
defeats	O
to	O
the	O
Union	O
.	O
Fort	O
Dix	O
was	O
declared	O
a	O
U.S.	O
Army	O
national	O
monument	O
by	O
President	O
Andrew	O
H.	O
Sharpe	O
in	O
September	O
1836	O
.	O
Fort	O
Dix	O
was	O
named	O
after	O
General	B-DatasetName
John	O
Scott	O
,	O
a	O
U.S.	O
Army	O
general	O
and	O
the	O
first	O
governor	O
of	O
Texas	B-DatasetName
,	O
who	O
was	O
killed	O
in	O
an	O
assassination	O
attempt	O
on	O
June	O
20	O
,	O
1855	O
.	O
Military	O
historian	O
John	O
P.	O
Pickett	O
wrote	O
that	O
it	O
was	O
the	O
first	O
military	O
governor	O
in	O
the	O
United	O
States	O
to	O
serve	O
in	O
the	O
Confederate	O
States	O
GNN	O
=	O
Fort	O
Scott	O
National	O
Historical	O
Park	O
=	O
Fort	O
Scott	O
National	O
Historical	O
Park	O
is	O
an	O
Illinois	O
state	O
historic	O
park	O
in	O
the	O
U.S.	O
state	O
of	O
Kentucky	O
.	O
It	O
is	O
located	O
at	O
53	O
25	O
4	O
N	O
65	O
41	O
16	O
W	O
,	O
at	O
the	O
edge	O
of	O
the	O
Clay	O
Creek	O
valley	O
,	O
southwest	O
of	O
New	O
Orleans	O
.	O
It	O
is	O
located	O
at	O
the	O
intersection	O
of	O
Washington	O
Boulevard	O
and	O
State	O
Route	O
63	O
,	O
and	O
is	O
the	O
largest	O
National	O
Historic	O
Landmark	O
in	O
the	O
state	O
.	O
The	O
site	O
was	O
purchased	O
by	O
Native	O
Americans	O
in	O
1803	O
and	O
the	O
site	O
was	O
added	O
to	O
the	O
National	O
Register	O
of	O
Historic	O
Places	B-DatasetName
in	O
1962	O
.	O
Since	O
1998	O
,	O
the	O
site	O
has	O
been	O
subject	O
to	O
an	O
extensive	O
series	O
of	O
historic	O
markers	O
and	O
features	O
that	O
are	O
important	O
in	O
preservation	O
of	O
American	O
historic	O
sites	O
in	O
Texas	B-DatasetName
.	O
The	O
National	O
Park	O
Service	O
includes	O
the	O
nation	O
's	O
oldest	O
extant	O
log	O
cabins	O
,	O
historic	O
buildings	O
,	O
historic	O
facilities	O
,	O
and	O
historic	O
structures	O
.	O
The	O
park	O
is	O
home	O
to	O
the	O
Mississippi	O
River	O
National	O
Historical	O
Park	O
,	O
a	O
U.S.	O
National	O
Monument	O
that	O
supplies	O
historic	O
sites	O
and	O
historic	O
sites	O
.	O
The	O
original	O
fort	O
was	O
built	O
in	O
1818	O
to	O
protect	O
U.S.	O
statehood	O
.	O
In	O
1899	O
,	O
the	O
state	O
legislature	O
constructed	O
a	O
small	O
blockhouse	O
at	O
the	O
site	O
of	O
the	O
original	O
fort	O
to	O
defend	O
it	O
from	O
Native	O
Americans	O
.	O
The	O
blockhouse	O
first	O
appeared	O
in	O
1868	O
,	O
when	O
land	O
in	O
the	O
city	O
of	O
Lisbon	O
was	O
granted	O
to	O
the	O
state	O
.	O
The	O
fort	O
has	O
remained	O
in	O
use	O
since	O
then	O
.	O
=	O
=	O
History	O
=	O
=	O
=	O
=	O
=	O
Early	O
history	O
=	O
=	O
=	O
Fort	O
Scott	O
was	O
established	O
as	O
a	O
civil	O
and	O
military	O
fortification	O
in	O
1803	O
and	O
named	O
after	O
an	O
American	O
Indian	O
.	O
The	O
land	O
that	O
would	O
become	O
Fort	O
Scott	O
was	O
originally	O
part	O
of	O
the	O
Louisiana	O
Purchase	O
,	O
which	O
was	O
granted	O
to	O
the	O
United	O
States	O
by	O
the	O
Louisiana	O
Purchase	O
Act	O
of	O
1825	O
.	O
The	O
original	O
fort	O
was	O
established	O
in	O
1828	O
by	O
an	O
act	O
of	O
Congress	O
.	O
The	O
American	O
Revolutionary	O
War	O
came	O
to	O
an	O
end	O
in	O
1830	O
,	O
but	O
Independence	O
was	O
declared	O
in	O
1831	O
and	O
Independence	O
was	O
declared	O
on	O
June	O
3	O
,	O
1830	O
.	O
The	O
post	O
@	O
-	O
@	O
war	O
Treaty	O
of	O
Paris	O
signed	O
at	O
Fort	O
Scott	O
ended	O
military	O
activity	O
in	O
the	O
region	O
.	O
War	O
by	O
the	O
United	O
States	O
reached	O
an	O
end	O
in	O
1830	O
,	O
and	O
most	O
of	O
the	O
land	O
was	O
put	O
aside	O
for	O
use	O
as	O
a	O
military	O
park	O
.	O
Fort	O
Scott	O
was	O
garrisoned	O
by	O
90	O
soldiers	O
from	O
the	O
55th	O
Louisiana	O
Regiment	O
during	O
the	O
War	O
of	O
1812	O
.	O
In	O
1837	O
,	O
the	O
Illinois	O
General	B-DatasetName
Assembly	O
passed	O
legislation	O
creating	O
Fort	O
Scott	O
as	O
a	O
federal	O
park	O
,	O
and	O
in	O
the	O
same	O
year	O
the	O
state	O
agreed	O
to	O
purchase	O
the	O
site	O
in	O
honor	O
of	O
the	O
site	O
's	O
new	O
state	O
of	O
Louisiana	O
.	O
Originally	O
,	O
only	O
about	O
half	O
of	O
Fort	O
Scott	O
was	O
owned	O
,	O
but	O
the	O
size	O
of	O
the	O
park	O
changed	O
in	O
the	O
1880s	O
from	O
a	O
forest	O
reserve	O
to	O
a	O
dirt	O
road	O
.	O
The	O
park	O
was	O
significantly	O
expanded	O
during	O
the	O
1910s	O
,	O
but	O
the	O
exact	O
date	O
is	O
disputed	O
.	O
The	O

UMDuluth	O
-	O
CS8761	O
at	O
SemEval	O
-	O
2018	O
Task	O
9	O
:	O
Hypernym	B-TaskName
Discovery	I-TaskName
using	O
Hearst	O
Patterns	O
,	O
Co	O
-	O
occurrence	O
frequencies	O
and	O
Word	B-TaskName
Embeddings	I-TaskName

Hypernym	B-TaskName
Discovery	I-TaskName
is	O
the	O
task	O
of	O
identifying	O
potential	O
hypernyms	O
for	O
a	O
given	O
term	O
.	O
A	O
hypernym	O
is	O
a	O
more	O
generalized	O
word	O
that	O
is	O
super	O
-	O
ordinate	O
to	O
more	O
specific	O
words	O
.	O
This	O
paper	O
explores	O
several	O
approaches	O
that	O
rely	O
on	O
co	O
-	O
occurrence	O
frequencies	O
of	O
word	O
pairs	O
,	O
Hearst	O
Patterns	O
based	O
on	O
regular	O
expressions	O
,	O
and	O
word	B-TaskName
embeddings	I-TaskName
created	O
from	O
the	O
UMBC	O
corpus	O
.	O
Our	O
system	O
Babbage	O
participated	O
in	O
Subtask	O
1A	O
for	O
English	O
and	O
placed	O
6th	O
of	O
19	O
systems	O
when	O
identifying	O
concept	O
hypernyms	O
,	O
and	O
12th	O
of	O
18	O
systems	O
for	O
entity	O
hypernyms	O
.	O

Hypernym	O
-	O
hyponym	O
pairs	O
exhibit	O
an	O
is	B-DatasetName
-	I-DatasetName
a	I-DatasetName
relationship	O
where	O
a	O
hypernym	O
is	O
a	O
generalization	O
of	O
a	O
hyponym	O
.	O
The	O
objective	O
of	O
SemEval	O
-	O
2018	O
Task	O
9	O
(	O
Camacho	O
-	O
Collados	O
et	O
al	O
,	O
2018	O
)	O
is	O
to	O
generate	O
a	O
ranked	O
list	O
of	O
hypernyms	O
when	O
given	O
an	O
input	O
hyponym	O
and	O
a	O
vocabulary	O
of	O
candidate	O
hypernyms	O
.	O
For	O
example	O
,	O
the	O
input	O
hyponym	O
lemongrass	O
could	O
yield	O
the	O
hypernyms	O
[	O
grass	O
,	O
oil	O
plant	O
,	O
herb	O
]	O
,	O
where	O
herb	O
would	O
be	O
the	O
best	O
candidate	O
.	O
This	O
scenario	O
is	O
illustrated	O
in	O
Figure	O
1	O
,	O
where	O
the	O
three	O
leaf	O
nodes	O
are	O
hyponyms	O
and	O
the	O
root	O
is	O
a	O
hypernym	O
.	O
Note	O
that	O
hypernym	B-TaskName
discovery	I-TaskName
is	O
distinct	O
from	O
hypernym	O
detection	O
,	O
where	O
the	O
problem	O
is	O
to	O
detect	O
if	O
a	O
hyponym	O
-	O
hypernym	O
relationship	O
exists	O
between	O
a	O
given	O
pair	O
,	O
such	O
as	O
lemongrass	O
-	O
grass	O
.	O
In	O
our	O
first	O
module	O
,	O
we	O
retrieve	O
candidate	O
hypernyms	O
for	O
an	O
input	O
term	O
using	O
a	O
paragraphlength	O
context	O
-	O
window	O
and	O
calculate	O
their	O
cooccurrence	O
frequencies	O
,	O
which	O
is	O
later	O
used	O
for	O
ranking	O
the	O
candidates	O
.	O
Our	O
second	O
module	O
uses	O
Hearst	O
Patterns	O
(	O
Hearst	O
,	O
1992	O
)	O
to	O
extract	O
hyponym	O
-	O
hypernym	O
pairs	O
and	O
ranks	O
candidate	O
hypernyms	O
based	O
on	O
co	O
-	O
occurrence	O
frequency	O
of	O
the	O
pairs	O
.	O
Our	O
final	O
module	O
employs	O
wordembeddings	O
created	O
using	O
word2vec	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
.	O
This	O
paper	O
continues	O
with	O
a	O
more	O
detailed	O
discussion	O
of	O
each	O
module	O
,	O
and	O
then	O
a	O
review	O
of	O
our	O
results	O
.	O

Babbage	O
begins	O
by	O
pre	O
-	O
processing	O
(	O
2.2.1	O
)	O
the	O
UMBC	O
Corpus	O
(	O
2.1	O
)	O
and	O
extracting	O
candidate	O
hypernyms	O
using	O
four	O
different	O
strategies	O
(	O
2.2.2	O
)	O
.	O
The	O
first	O
and	O
second	O
module	O
calculates	O
the	O
cooccurrence	O
frequencies	O
between	O
the	O
input	O
term	O
and	O
words	B-DatasetName
in	I-DatasetName
context	I-DatasetName
using	O
the	O
pre	O
-	O
processed	O
UMBC	O
Corpus	O
and	O
the	O
Hearst	O
Pattern	O
set	O
extracted	O
from	O
the	O
UMBC	O
Corpus	O
.	O
The	O
third	O
module	O
uses	O
the	O
IS	B-DatasetName
-	I-DatasetName
A	I-DatasetName
Hearst	O
Pattern	O
set	O
extracted	O
from	O
UMBC	O
Corpus	O
to	O
obtain	O
hypernyms	O
.	O
The	O
final	O
module	O
constructs	O
a	O
word	O
embedding	O
over	O
the	O
UMBC	O
corpus	O
and	O
uses	O
a	O
distance	O
measure	O
to	O
fetch	O
candidate	O
hypernyms	O
for	O
a	O
given	O
input	O
term	O
.	O

Our	O
training	O
corpus	O
is	O
the	O
University	O
of	O
Maryland	O
,	O
Baltimore	O
County	O
(	O
UMBC	O
)	O
WebBase	O
Corpus	O
(	O
Han	O
et	O
al	O
,	O
2013	O
)	O
.	O
It	O
contains	O
3	O
billion	O
words	O
from	O
paragraphs	O
obtained	O
from	O
more	O
than	O
100	O
million	O
web	O
pages	O
over	O
various	O
domains	O
.	O
We	O
use	O
the	O
28	O
GB	O
tokenized	O
version	O
of	O
UMBC	O
corpus	O
which	O
is	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
tagged	O
and	O
divided	O
among	O
408	O
files	O
.	O
There	O
is	O
also	O
a	O
vocabulary	O
file	O
with	O
218	O
,	O
755	O
unigram	O
,	O
bigram	O
and	O
trigram	O
hypernym	O
terms	O
provided	O
by	O
task	O
organizers	O
.	O
This	O
file	O
defines	O
the	O
set	O
of	O
possible	O
candidate	O
hypernyms	O
.	O

The	O
following	O
are	O
the	O
steps	O
involved	O
in	O
constructing	O
our	O
system	O
:	O
(	O
a	O
)	O
Co	O
-	O
occurrence	O
frequencies	O
from	O
Normalized	O
Corpus	O
:	O
A	O
co	O
-	O
occurrence	O
map	O
is	O
built	O
for	O
the	O
input	O
terms	O
with	O
the	O
words	O
in	O
the	O
context	O
of	O
the	O
input	O
term	O
and	O
the	O
frequency	O
of	O
their	O
co	O
-	O
occurrence	O
using	O
the	O
Normalized	O
Corpus	O
.	O
Words	O
with	O
co	O
-	O
occurrence	O
frequency	O
higher	O
than	O
5	O
are	O
listed	O
as	O
candidate	O
hypernyms	O
for	O
an	O
input	O
term	O
.	O
This	O
is	O
considered	O
the	O
first	O
module	O
result	O
.	O
(	O
b	O
)	O
Co	O
-	O
occurrence	O
frequencies	O
from	O
Hearst	O
Corpus	O
:	O
A	O
co	O
-	O
occurrence	O
map	O
similar	O
to	O
the	O
previous	O
step	O
is	O
built	O
by	O
using	O
the	O
Hearst	O
Corpus	O
.	O
All	O
the	O
words	O
which	O
occur	O
at	O
least	O
once	O
in	O
context	O
of	O
the	O
input	O
term	O
in	O
the	O
Hearst	O
Patterns	O
are	O
listed	O
as	O
candidate	O
hypernyms	O
for	O
this	O
term	O
.	O
This	O
is	O
considered	O
the	O
second	O
module	O
result	O
.	O
(	O
c	O
)	O
Co	O
-	O
occurrence	O
frequencies	O
from	O
IS	B-DatasetName
-	I-DatasetName
A	I-DatasetName
Corpus	O
:	O
All	O
the	O
words	O
which	O
occur	O
at	O
least	O
once	O
in	O
the	O
context	O
of	O
the	O
input	O
term	O
in	O
the	O
IS	B-DatasetName
-	I-DatasetName
A	I-DatasetName
Corpus	O
are	O
listed	O
as	O
candidate	O
hypernyms	O
for	O
this	O
term	O
.	O
If	O
the	O
input	O
term	O
is	O
a	O
concept	O
and	O
is	O
a	O
bigram	O
or	O
trigram	O
term	O
,	O
then	O
part	O
of	O
it	O
is	O
considered	O
as	O
a	O
hypernym	O
for	O
that	O
term	O
.	O
This	O
is	O
considered	O
the	O
third	O
module	O
result	O
.	O
(	O
d	O
)	O
Applying	O
word	B-TaskName
similarity	I-TaskName
to	O
word	B-TaskName
embeddings	I-TaskName
:	O
A	O
fixed	O
distance	O
value	O
called	O
Phi	O
is	O
used	O
to	O
extract	O
words	O
at	O
this	O
distance	O
to	O
the	O
input	O
term	O
in	O
the	O
UMBC	O
Embedding	O
.	O
These	O
words	O
are	O
listed	O
as	O
the	O
candidate	O
hypernyms	O
for	O
an	O
input	O
term	O
.	O
This	O
is	O
considered	O
our	O
final	O
module	O
result	O
.	O

The	O
task	O
description	O
states	O
that	O
our	O
system	O
should	O
predict	O
candidate	O
hypernyms	O
for	O
an	O
input	O
word	O
which	O
is	O
either	O
a	O
concept	O
or	O
an	O
entity	O
.	O
Hence	O
,	O
the	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
tag	O
for	O
all	O
candidate	O
hypernyms	O
is	O
noun	O
.	O
This	O
restricts	O
our	O
search	O
space	O
to	O
words	O
with	O
noun	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
tag	O
and	O
bigram	O
or	O
trigram	O
phrases	O
with	O
a	O
noun	O
head	O
word	O
.	O
Our	O
system	O
focuses	O
on	O
concepts	O
,	O
so	O
we	O
do	O
not	O
have	O
any	O
module	O
specific	O
for	O
entities	O
.	O
To	O
refine	O
the	O
input	O
corpus	O
as	O
per	O
these	O
specifications	O
,	O
the	O
input	O
UMBC	O
Corpus	O
is	O
processed	O
through	O
the	O
following	O
modules	O
:	O
Normalized	O
Corpus	O
:	O
The	O
POS	O
tagged	O
input	O
corpus	O
is	O
processed	O
per	O
paragraph	O
.	O
Each	O
paragraph	O
is	O
converted	O
to	O
lower	O
-	O
case	O
text	O
.	O
Then	O
,	O
bigram	O
and	O
trigram	O
noun	O
phrases	O
from	O
each	O
paragraph	O
are	O
obtained	O
using	O
the	O
POS	O
tags	O
given	O
for	O
each	O
word	O
.	O
It	O
is	O
further	O
filtered	O
by	O
removing	O
punctuation	O
marks	O
and	O
words	O
with	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
tags	O
other	O
than	O
noun	O
,	O
verb	O
,	O
adverb	O
or	O
adjective	O
.	O
This	O
filtered	O
line	O
is	O
modified	O
by	O
appending	O
it	O
with	O
bigram	O
and	O
trigram	O
noun	O
phrases	O
obtained	O
earlier	O
.	O
Hearst	O
Corpus	O
:	O
The	O
original	O
input	O
paragraph	O
is	O
searched	O
for	O
the	O
Hearst	O
Patterns	O
(	O
shown	O
in	O
Figure	O
2	O
)	O
and	O
all	O
the	O
possible	O
matches	O
are	O
returned	O
in	O
the	O
form	O
of	O
hypernym	O
:	O
one	O
or	O
more	O
hyponyms	O
.	O
Figure	O
2	O
shows	O
the	O
extraction	O
of	O
Hearst	O
Patterns	O
,	O
where	O
NP	O
represents	O
a	O
noun	O
-	O
phrase	O
where	O
the	O
head	O
word	O
is	O
tagged	O
as	O
a	O
noun	O
,	O
the	O
loved	O
-	O
ones	O
such	O
as	O
family	O
and	O
friends	O
is	O
a	O
match	O
for	O
Hearst	O
Patterns	O
(	O
from	O
Figure	O
2	O
)	O
with	O
noun	O
phrases	O
the	O
loved	O
-	O
ones	O
,	O
family	O
and	O
friends	O
.	O

This	O
module	O
uses	O
hypernym	O
-	O
hyponym	O
pairs	O
from	O
the	O
IS	B-DatasetName
-	I-DatasetName
A	I-DatasetName
Corpus	O
2.2.1	O
which	O
are	O
in	O
the	O
form	O
hyponym	O
:	O
hypernym	O
.	O
We	O
use	O
the	O
same	O
strategy	O
as	O
Co	O
-	O
occurrence	O
frequency	O
from	O
Hearst	O
Corpus	O
to	O
obtain	O
the	O
result	O
.	O

For	O
this	O
task	O
,	O
our	O
system	O
is	O
required	O
to	O
report	O
the	O
15	O
most	O
probable	O
hypernyms	O
for	O
each	O
input	O
term	O
.	O
We	O
have	O
four	O
modules	O
each	O
reporting	O
their	O
top	O
15	O
candidate	O
hypernyms	O
.	O
By	O
looking	O
at	O
the	O
training	O
scores	O
of	O
these	O
modules	O
,	O
we	O
merge	O
the	O
co	O
-	O
occurrence	O
frequencies	O
from	O
IS	B-DatasetName
-	I-DatasetName
A	I-DatasetName
corpus	O
that	O
have	O
higher	O
ranks	O
followed	O
by	O
the	O
co	O
-	O
occurrence	O
frequencies	O
from	O
Normalized	O
corpus	O
and	O
Hearst	O
Pattern	O
corpus	O
.	O
Results	O
from	O
word	O
embedding	O
module	O
are	O
given	O
the	O
lowest	O
ranks	O
.	O

This	O
project	O
was	O
carried	O
out	O
as	O
a	O
part	O
of	O
CS	B-DatasetName
8761	O
,	O
Natural	O
Language	O
Processing	O
,	O
a	O
graduate	O
level	O
class	O
offered	O
in	O
Fall	O
2017	O
at	O
the	O
University	O
of	O
Minnesota	O
,	O
Duluth	O
by	O
Dr.	O
Ted	O
Pedersen	O
.	O
All	O
authors	O
of	O
this	O
paper	O
have	O
contributed	O
equally	O
and	O
are	O
listed	O
in	O
alphabetical	O
order	O
by	O
first	O
name	O
.	O

Large	O
Scale	O
Substitution	O
-	O
based	O
Word	B-TaskName
Sense	I-TaskName
Induction	I-TaskName

We	O
present	O
a	O
word	O
-	O
sense	O
induction	O
method	O
based	O
on	O
pre	O
-	O
trained	O
masked	O
language	O
models	O
(	O
MLMs	O
)	O
,	O
which	O
can	O
cheaply	O
scale	O
to	O
large	O
vocabularies	O
and	O
large	O
corpora	O
.	O
The	O
result	O
is	O
a	O
corpus	O
which	O
is	O
sense	O
-	O
tagged	O
according	O
to	O
a	O
corpus	O
-	O
derived	O
sense	O
inventory	O
and	O
where	O
each	O
sense	O
is	O
associated	O
with	O
indicative	O
words	O
.	O
Evaluation	O
on	O
English	O
Wikipedia	O
that	O
was	O
sense	O
-	O
tagged	O
using	O
our	O
method	O
shows	O
that	O
both	O
the	O
induced	O
senses	O
,	O
and	O
the	O
per	O
-	O
instance	O
sense	O
assignment	O
,	O
are	O
of	O
high	O
quality	O
even	O
compared	O
to	O
WSD	O
methods	O
,	O
such	O
as	O
Babelfy	O
.	O
Furthermore	O
,	O
by	O
training	O
a	O
static	O
word	B-TaskName
embeddings	I-TaskName
algorithm	O
on	O
the	O
sense	O
-	O
tagged	O
corpus	O
,	O
we	O
obtain	O
high	O
-	O
quality	O
static	O
senseful	O
embeddings	O
.	O
These	O
outperform	O
existing	O
senseful	O
embeddings	O
methods	O
on	O
the	O
WiC	B-DatasetName
dataset	O
and	O
on	O
a	O
new	O
outlier	B-TaskName
detection	I-TaskName
dataset	O
we	O
developed	O
.	O
The	O
data	O
driven	O
nature	O
of	O
the	O
algorithm	O
allows	O
to	O
induce	O
corpora	O
-	O
specific	O
senses	O
,	O
which	O
may	O
not	O
appear	O
in	O
standard	O
sense	O
inventories	O
,	O
as	O
we	O
demonstrate	O
using	O
a	O
case	O
study	O
on	O
the	O
scientific	O
domain	O
.	O

Word	B-TaskName
Sense	I-TaskName
Induction	I-TaskName
and	O
Disambiguation	O
Previous	O
challenges	O
like	O
Jurgens	O
and	O
Klapaftis	O
(	O
2013	O
)	O
focused	O
on	O
word	B-TaskName
sense	I-TaskName
induction	I-TaskName
for	O
small	O
sized	O
datasets	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
we	O
are	O
the	O
first	O
to	O
perform	O
large	O
-	O
scale	O
all	O
-	O
words	O
WSI	O
.	O
The	O
closest	O
work	O
to	O
our	O
method	O
is	O
the	O
substitution	O
-	O
based	O
method	O
proposed	O
in	O
Goldberg	O
(	O
2018	O
,	O
2019	O
)	O
which	O
is	O
the	O
starting	O
point	O
to	O
our	O
paper	O
.	O
In	O
that	O
paper	O
,	O
the	O
authors	O
suggested	O
a	O
WSI	O
algorithm	O
designed	O
for	O
a	O
small	O
dataset	O
(	O
SemEval	O
2010	O
with	O
a	O
predefined	O
set	O
of	O
ambiguous	O
target	O
words	O
(	O
See	O
(	O
3	O
)	O
for	O
more	O
details	O
on	O
the	O
algorithm	O
)	O
.	O
In	O
our	O
work	O
,	O
we	O
change	O
Amrami	O
and	O
Goldberg	O
(	O
2019	O
)	O
such	O
that	O
we	O
can	O
efficiently	O
run	O
sense	O
induction	O
on	O
all	O
the	O
words	O
in	O
very	O
large	O
corpora	O
.	O
An	O
alternative	O
approach	O
for	O
sense	O
tagging	O
is	O
based	O
on	O
Word	B-TaskName
Sense	I-TaskName
Disambiguation	I-TaskName
(	O
WSD	O
)	O
.	O
The	O
two	O
main	O
WSD	O
methods	O
are	O
Supervised	O
WSD	O
and	O
Knowledge	O
-	O
based	O
WSD	O
.	O
Supervised	O
WSD	O
suffers	O
from	O
the	O
difficulty	O
of	O
obtaining	O
an	O
adequate	O
amount	O
of	O
annotated	O
data	O
.	O
Indeed	O
,	O
even	O
SemCor	O
,	O
the	O
largest	O
manually	O
annotated	O
tagged	O
corpus	O
,	O
consists	O
of	O
only	O
226	O
,	O
036	O
annotated	O
tokens	O
.	O
Among	O
different	O
supervisied	O
WSD	O
methods	O
,	O
Zhong	O
and	O
Ng	O
(	O
2010	O
)	O
suggested	O
a	O
SVM	B-MethodName
based	O
approach	O
and	O
Melamud	O
et	O
al	O
(	O
2016	O
)	O
;	O
Yuan	O
et	O
al	O
(	O
2016	O
)	O
suggested	O
LSTMs	O
paired	O
with	O
nearest	O
neighbours	O
classification	O
.	O
Knowledgebase	O
WSD	O
(	O
Moro	O
et	O
al	O
,	O
2014	O
;	O
Pasini	O
and	O
Navigli	O
,	O
2017	O
)	O
,	O
on	O
the	O
other	O
hand	O
,	O
avoids	O
the	O
reliance	O
on	O
large	O
annotated	O
word	O
-	O
to	O
-	O
sense	O
corpus	O
and	O
instead	O
maps	O
words	O
to	O
senses	O
from	O
a	O
closed	O
sense	O
inventory	O
(	O
e.g.	O
WordNet	O
(	O
Miller	O
,	O
1992	O
)	O
,	O
BabelNet	O
(	O
Navigli	O
andPonzetto	O
,	O
2010	O
)	O
)	O
.	O
As	O
such	O
,	O
the	O
quality	O
of	O
knowledge	O
-	O
based	O
WSD	O
heavily	O
depends	O
on	O
the	O
availability	O
,	O
quality	O
and	O
coverage	O
of	O
the	O
associated	O
annotated	O
resources	O
.	O
Sense	O
Embeddings	O
In	O
8	O
we	O
exploit	O
the	O
sense	O
-	O
induced	O
corpus	O
to	O
train	O
sense	O
embeddings	O
.	O
Reisinger	O
and	O
Mooney	O
(	O
2010	O
)	O
were	O
the	O
first	O
to	O
suggest	O
creating	O
multiple	O
representations	O
for	O
ambiguous	O
words	O
.	O
Numerous	O
recent	O
papers	O
(	O
Chen	O
et	O
al	O
,	O
2014	O
;	O
Rothe	O
and	O
Schütze	O
,	O
2015	O
;	O
Iacobacci	O
et	O
al	O
,	O
2015	O
;	O
Pilehvar	O
and	O
Collier	O
,	O
2016	O
;	O
Mancini	O
et	O
al	O
,	O
2017	O
;	O
Iacobacci	O
and	O
Navigli	O
,	O
2019	O
)	O
aim	O
to	O
produce	O
similar	O
embeddings	O
,	O
all	O
of	O
which	O
use	O
either	O
WordNet	O
or	O
BabelNet	O
as	O
semantic	O
network	O
.	O
Our	O
method	O
is	O
similar	O
to	O
Iacobacci	O
et	O
al	O
(	O
2015	O
)	O
,	O
with	O
the	O
difference	O
being	O
that	O
they	O
rely	O
on	O
semantic	O
networks	O
(	O
via	O
Babelfy	O
(	O
Moro	O
et	O
al	O
,	O
2014	O
)	O
)	O
.	O
In	O
contrast	O
and	O
similarly	O
to	O
us	O
,	O
Pelevina	O
et	O
al	O
(	O
2016	O
)	O
does	O
not	O
rely	O
on	O
lexical	O
resources	O
such	O
as	O
WordNet	O
.	O
The	O
authors	O
proposed	O
splitting	O
pretrained	O
embeddings	O
(	O
such	O
as	O
word2vec	O
)	O
to	O
a	O
number	O
of	O
prototype	O
senseembeddings	O
.	O
Yet	O
in	O
our	O
work	O
,	O
we	O
directly	O
learn	O
the	O
multi	O
-	O
prototype	O
sense	O
-	O
embeddings	O
which	O
is	O
only	O
possible	O
due	O
to	O
the	O
large	O
-	O
scale	O
corpus	O
annotation	O
.	O
When	O
comparing	O
both	O
methods	O
in	O
9.1	O
we	O
infer	O
it	O
is	O
better	O
to	O
directly	O
learn	O
multi	O
-	O
prototype	O
senseembeddings	O
.	O
3	O
Large	O
Scale	O
Sense	O
Induction	O

Contextualized	O
BERT	B-MethodName
vectors	O
contain	O
sense	O
information	O
,	O
and	O
clustering	O
the	O
contextualized	O
vectors	O
results	O
in	O
sense	O
clusters	O
.	O
However	O
,	O
storing	O
a	O
1024	O
dimensional	O
vector	O
of	O
32bit	O
floats	O
for	O
each	O
relevant	O
token	O
in	O
the	O
English	O
Wikipedia	O
corpus	O
requires	O
over	O
8	O
TB	O
of	O
disk	O
-	O
space	O
,	O
making	O
the	O
approach	O
cumbersome	O
and	O
not	O
-	O
scalable	O
.	O
However	O
,	O
as	O
shown	O
by	O
Amrami	O
and	O
Goldberg	O
(	O
2019	O
)	O
,	O
MLM	B-DatasetName
based	O
wordsubstitutes	O
also	O
contain	O
the	O
relevant	O
semantic	O
information	O
,	O
and	O
are	O
much	O
cheaper	O
to	O
store	O
:	O
each	O
word	O
-	O
i	O
d	O
in	O
BERTLARGE	O
's	O
vocabulary	O
can	O
be	O
represented	O
by	O
2	O
bytes	O
,	O
and	O
storing	O
the	O
top	O
-	O
5	O
substitutes	O
for	O
each	O
corpus	O
position	O
requires	O
less	O
than	O
20	O
GB	O
of	O
storage	O
space	O
.	O
3	O
Figure	O
2	O
:	O
Scalable	O
WSI	O
flow	O
.	O
Given	O
raw	O
text	O
,	O
we	O
annotate	O
each	O
word	O
with	O
its	O
top	O
-	O
k	O
substitutes	O
,	O
create	O
inverted	O
word	O
index	O
,	O
find	O
best	O
clusters	O
for	O
each	O
distinct	O
lemma	B-DatasetName
and	O
associate	O
all	O
corpus	O
words	O
with	O
a	O
matching	O
cluster	O
.	O
In	O
order	O
to	O
perform	O
WSI	O
at	O
scale	O
,	O
we	O
keep	O
the	O
main	O
intuition	O
from	O
Amrami	O
and	O
Goldberg	O
(	O
2019	O
)	O
,	O
namely	O
to	O
cluster	O
sparse	O
vectors	O
of	O
lemmas	O
of	O
the	O
top	O
-	O
k	O
MLM	B-DatasetName
-	O
derived	O
word	O
substitutions	O
.	O
This	O
results	O
in	O
vast	O
storage	O
saving	O
,	O
and	O
also	O
in	O
a	O
more	O
interpretable	O
representations	O
.	O
However	O
,	O
for	O
scalability	O
,	O
we	O
iterate	O
over	O
the	O
corpus	O
sentences	O
and	O
collect	O
the	O
top	O
-	O
k	O
substitutes	O
for	O
all	O
words	O
in	O
the	O
sentence	O
at	O
once	O
based	O
on	O
a	O
single	O
BERT	B-MethodName
call	O
for	O
that	O
sentence	O
.	O
This	O
precludes	O
us	O
from	O
using	O
the	O
dynamic	O
-	O
patterns	O
component	O
of	O
their	O
method	O
,	O
which	O
requires	O
separately	O
running	O
BERT	B-MethodName
for	O
each	O
word	O
in	O
each	O
sentence	O
.	O
However	O
,	O
as	O
we	O
show	O
in	O
Section	O
5.1	O
we	O
still	O
obtain	O
sufficiently	O
high	O
WSI	O
results	O
.	O
The	O
steps	O
for	O
performing	O
Scalable	O
WSI	O
are	O
summarized	O
in	O
Fig	O
.	O
2	O
.	O
We	O
elaborate	O
on	O
each	O
step	O
below	O
,	O
using	O
English	O
Wikipedia	O
as	O
a	O
running	O
example	O
.	O
4	O
Annotation	O
:	O
We	O
run	O
BERT	B-MethodName
-	O
large	O
-	O
cased	O
-	O
wholeword	O
-	O
masking	O
on	O
English	O
Wikipedia	O
,	O
inferring	O
substitutes	O
for	O
all	O
corpus	O
positions	O
.	O
For	O
positions	O
that	O
correspond	O
to	O
single	O
-	O
token	O
words	O
,	O
5	O
we	O
consider	O
the	O
predicted	O
words	O
,	O
filter	O
stop	O
-	O
words	O
,	O
lemmatize	O
the	O
remaining	O
words	O
(	O
Honnibal	O
et	O
al	O
,	O
2020	O
)	O
,	O
and	O
store	O
the	O
top	O
-	O
5	O
most	O
probable	O
lemmas	O
to	O
disk	O
.	O
This	O
step	O
takes	O
5	O
hours	O
on	O
20	O
cloud	O
-	O
based	O
GPU	O
machines	O
(	O
total	O
of	O
100	O
GPU	O
hours	O
)	O
,	O
resulting	O
in	O
1.63B	O
tokens	O
with	O
their	O
corresponding	O
top	O
-	O
5	O
lemmas	O
.	O
Inverted	O
Word	O
Index	O
:	O
We	O
create	O
an	O
inverted	O
index	O
mapping	O
from	O
each	O
single	O
-	O
token	O
word	O
to	O
its	O
corpus	O
occurrences	O
(	O
and	O
their	O
corresponding	O
top	O
-	O
5	O
lemmas	O
)	O
.	O
This	O
takes	O
5	O
minutes	O
on	O
a	O
96	O
cores	O
CPU	O
machine	O
,	O
and	O
10	O
GB	O
of	O
disk	O
.	O
Sense	O
Induction	O
:	O
For	O
each	O
of	O
16	O
,	O
081	O
lemmas	O
corresponding	O
to	O
single	O
-	O
token	O
words	O
,	O
we	O
retrieve	O
random	O
1000	O
instances	O
,	O
6	O
and	O
induce	O
senses	O
using	O
4	O
The	O
Wikipedia	O
corpus	O
is	O
based	O
on	O
a	O
dump	O
from	O
August	O
2020	O
,	O
with	O
text	O
extracted	O
using	O
WikiExtractor	O
(	O
Attardi	O
,	O
2015	O
)	O
.	O
5	O
We	O
exclude	O
single	O
-	O
character	O
tokens	O
,	O
stopwords	O
and	O
punctuation	O
.	O
6	O
The	O
clustering	O
algorithm	O
scales	O
super	O
-	O
linearly	O
with	O
the	O
number	O
of	O
instances	O
.	O
To	O
reduce	O
computation	O
cost	O
for	O
tokens	O
that	O
appear	O
more	O
than	O
1000	O
times	O
in	O
the	O
dataset	O
,	O
we	O
sample	O
min	O
(	O
numOccur	O
,	O
1000	O
)	O
instances	O
for	O
each	O
token	O
word	O
,	O
and	O
cluster	O
given	O
the	O
subset	O
of	O
instances	O
.	O
We	O
then	O
associate	O
each	O
of	O
the	O
remaining	O
instances	O
to	O
one	O
of	O
the	O
clusters	O
as	O
explained	O
This	O
process	O
requires	O
30	O
minutes	O
on	O
the	O
96	O
-	O
core	O
CPU	O
machine	O
,	O
and	O
uses	O
100	O
MB	O
of	O
disk	O
space	O
.	O
The	O
average	O
number	O
of	O
senses	O
per	O
lemma	B-DatasetName
is	O
3.13	O
.	O
Each	O
sense	O
is	O
associated	O
with	O
up	O
to	O
100	O
representative	O
words	O
,	O
which	O
represent	O
the	O
highest	O
-	O
degree	O
words	O
in	O
the	O
sense	O
's	O
community	O
.	O
Table	O
1	O
shows	O
the	O
5	O
senses	O
found	O
for	O
the	O
word	O
bass	O
with	O
their	O
top	O
-	O
5	O
representative	O
words	O
.	O
See	O
additional	O
examples	O
in	O
Fig	O
.	O
1	O
and	O
Appendix	O
A.	O
Tagging	O
:	O
Each	O
of	O
the	O
remaining	O
wordoccurrences	O
is	O
associated	O
with	O
a	O
sense	O
cluster	O
by	O
computing	O
the	O
Jaccard	O
similarity	O
between	O
the	O
occurrences	O
'	O
top	O
-	O
5	O
lemmas	O
and	O
the	O
cluster	O
representatives	O
,	O
and	O
choosing	O
the	O
cluster	O
that	O
maximizes	O
this	O
score	O
.	O
For	O
example	O
,	O
an	O
occurrence	O
of	O
the	O
word	O
bass	O
with	O
lemmas	O
tenor	O
,	O
baritone	O
,	O
lead	O
,	O
opera	O
,	O
soprano	O
will	O
be	O
associated	O
with	O
bass	O
3	O
.	O
This	O
takes	O
100	O
minutes	O
on	O
96	O
-	O
core	O
machine	O
,	O
and	O
25	O
GB	O
of	O
storage	O
.	O

We	O
replace	O
the	O
hierarchical	O
clustering	O
algorithm	O
used	O
by	O
Goldberg	O
(	O
2018	O
,	O
2019	O
)	O
with	O
a	O
community	O
-	O
detection	O
,	O
graph	O
-	O
based	O
clustering	O
algorithm	O
.	O
One	O
major	O
benefit	O
of	O
the	O
community	B-TaskName
detection	I-TaskName
algorithms	O
is	O
that	O
they	O
naturally	O
produces	O
a	O
dynamic	O
number	O
of	O
clusters	O
,	O
and	O
provide	O
a	O
list	O
of	O
interpretable	O
discrete	O
representative	O
lemmas	O
for	O
each	O
cluster	O
.	O
We	O
additionally	O
found	O
this	O
method	O
to	O
be	O
more	O
stable	O
.	O
Graph	O
-	O
based	O
clustering	O
for	O
word	O
-	O
sense	O
induction	O
typically	O
constructs	O
a	O
graph	O
from	O
word	O
occurrences	O
in	O
the	O
final	O
step	O
of	O
the	O
algorithm	O
.	O
or	O
collocations	O
,	O
where	O
the	O
goal	O
is	O
to	O
identify	O
sensespecific	O
sub	O
-	O
graphs	O
within	O
the	O
graph	O
that	O
best	O
induce	O
different	O
senses	O
Manandhar	O
,	O
2008	O
,	O
2010	O
)	O
.	O
We	O
instead	O
construct	O
the	O
graph	O
based	O
on	O
word	O
substitutes	O
.	O
Following	O
Jurgens	O
(	O
2011	O
)	O
,	O
we	O
pose	O
identifying	O
sense	O
-	O
specific	O
clusters	O
as	O
a	O
community	B-TaskName
detection	I-TaskName
problem	O
,	O
where	O
a	O
community	O
is	O
defined	O
as	O
a	O
group	O
of	O
connected	O
nodes	O
that	O
are	O
more	O
connected	O
to	O
each	O
other	O
than	O
to	O
the	O
rest	O
of	O
the	O
graph	O
.	O

A	O
benefit	O
of	O
a	O
WSI	O
approach	O
compared	O
to	O
WSD	O
methods	O
is	O
that	O
it	O
does	O
not	O
rely	O
on	O
a	O
pre	O
-	O
specified	O
sense	O
inventory	O
,	O
and	O
can	O
be	O
applied	O
to	O
any	O
corpus	O
for	O
which	O
a	O
BERT	B-MethodName
-	O
like	O
model	O
is	O
available	O
.	O
Thus	O
,	O
in	O
addition	O
to	O
the	O
Wikipedia	O
dataset	O
that	O
has	O
been	O
presented	O
throughout	O
the	O
paper	O
,	O
we	O
also	O
automatically	O
induce	O
senses	O
over	O
a	O
corpus	O
of	O
31	O
million	O
PubMed	O
Abstracts	O
,	O
8	O
using	O
SciBERT	O
(	O
Beltagy	O
et	O
al	O
,	O
2019	O
)	O
.	O
As	O
this	O
dataset	O
is	O
larger	O
than	O
the	O
Wikipedia	O
dump	O
,	O
the	O
process	O
required	O
roughly	O
145	O
GPU	O
hours	O
and	O
resulting	O
in	O
14	O
,	O
225	O
sense	O
-	O
annotated	O
lemmas	O
,	O
with	O
an	O
average	O
number	O
of	O
2.89	O
senses	O
per	O
lemma	B-DatasetName
.	O
This	O
dataset	O
highlights	O
the	O
data	O
-	O
driven	O
advantages	O
of	O
sense	O
-	O
induction	O
:	O
the	O
algorithm	O
recovers	O
many	O
senses	O
that	O
are	O
science	O
specific	O
and	O
are	O
not	O
represented	O
in	O
the	O
Wikipedia	O
corpora	O
.	O
While	O
performing	O
a	O
wide	O
-	O
scale	O
evaluation	O
of	O
the	O
scientific	O
WSI	O
is	O
beyond	O
our	O
scope	O
in	O
this	O
work	O
,	O
we	O
do	O
show	O
a	O
few	O
examples	O
to	O
qualitatively	O
demonstrate	O
the	O
kinds	O
of	O
induced	O
senses	O
we	O
get	O
for	O
scientific	O
texts	O
.	O
For	O
each	O
of	O
the	O
words	O
mosaic	O
,	O
race	O
and	O
swine	O
we	O
show	O
the	O
induced	O
clusters	O
and	O
the	O
top	O
-	O
5	O
cluster	O
representatives	O
for	O
each	O
cluster	O
.	O
While	O
senses	O
mosaic	O
0	B-DatasetName
(	O
the	O
common	O
mosaic	O
virus	O
of	O
plants	O
)	O
and	O
mosaic	O
2	O
(	O
"	O
something	O
resembling	O
a	O
mosaic	O
"	O
,	O
"	O
mosaic	O
of	O
..	O
"	O
)	O
are	O
represented	O
in	O
Wikipedia	O
,	O
senses	O
mosaic	O
1	O
(	O
the	O
mosaic	O
genetic	O
disorder	O
)	O
and	O
mosaic	O
3	O
(	O
mosaic	O
is	O
a	O
quality	O
,	O
e.g.	O
,	O
"	O
mosaic	O
border	O
"	O
,	O
"	O
mosaic	O
pattern	O
"	O
)	O
are	O
specific	O
to	O
the	O
scientific	O
corpora	O
(	O
The	O
Wikipedia	O
corpora	O
,	O
on	O
the	O
other	O
hand	O
,	O
includes	O
a	O
sense	O
of	O
mosaic	O
as	O
a	O
decorative	O
art	O
-	O
form	O
,	O
which	O
is	O
not	O
represented	O
in	O
Pubmed	B-DatasetName
)	O
.	O
7	O
Sense	O
-	O
aware	O
Information	B-TaskName
Retrieval	I-TaskName
An	O
immediate	O
application	O
of	O
a	O
high	O
quality	O
sensetagged	O
corpus	O
is	O
sense	O
-	O
aware	O
retrieval	O
.	O
We	O
incorporate	O
the	O
sense	O
information	O
in	O
the	O
SPIKE	O
extractive	O
search	O
system	O
(	O
Shlain	O
et	O
al	O
,	O
2020	O
)	O
9	O
for	O
Wikipedia	O
and	O
Pubmed	B-DatasetName
datasets	O
.	O
When	O
entering	O
a	O
search	O
term	O
,	O
suffixing	O
it	O
with	O
@	O
triggers	O
sense	O
selection	O
allowing	O
9	O
spike.apps.allenai.org	O
to	O
narrow	O
the	O
search	O
for	O
the	O
specific	O
sense	O
.	O
Consider	O
a	O
scientist	O
looking	O
for	O
PubMed	O
occurrences	O
of	O
the	O
word	O
"	O
swine	O
"	O
in	O
its	O
influenza	O
meaning	O
.	O
As	O
shown	O
in	O
Figure	O
3	O
,	O
this	O
can	O
be	O
easily	O
done	O
by	O
writing	O
"	O
swine@	O
"	O
and	O
choosing	O
the	O
second	O
item	O
in	O
the	O
resulting	O
popup	O
window	O
.	O
The	O
outputs	O
are	O
sentences	O
with	O
the	O
word	O
"	O
swine	O
"	O
in	O
the	O
matching	O
sense	O
.	O
As	O
far	O
as	O
we	O
know	O
,	O
SPIKE	O
is	O
the	O
first	O
system	O
with	O
such	O
WSI	O
capabilities	O
for	O
IR	O
.	O
Similarly	O
,	O
Blloshmi	O
et	O
al	O
(	O
2021	O
)	O
suggested	O
to	O
enhance	O
IR	O
with	O
sense	O
information	O
,	O
but	O
differently	O
from	O
us	O
,	O
this	O
is	O
done	O
by	O
automatically	O
tagging	O
words	O
with	O
senses	O
from	O
a	O
predefined	O
inventory	O
.	O

Learning	O
static	O
word	B-TaskName
embeddings	I-TaskName
of	O
senseambiguous	O
words	O
is	O
a	O
long	O
standing	O
research	O
goal	O
(	O
Reisinger	O
and	O
Mooney	O
,	O
2010	O
;	O
Huang	O
et	O
al	O
,	O
2012	O
)	O
.	O
There	O
are	O
numerous	O
real	O
-	O
world	O
tasks	O
where	O
context	O
is	O
not	O
available	O
,	O
precluding	O
the	O
use	O
of	O
contextualized	O
-	O
embeddings	O
.	O
These	O
include	O
Outlier	B-TaskName
Detection	I-TaskName
(	O
Camacho	O
-	O
Collados	O
and	O
Navigli	O
,	O
2016	O
;	O
Blair	O
et	O
al	O
,	O
2016	O
)	O
,	O
Term	O
Set	O
Expansion	O
(	O
Roark	O
and	O
Charniak	O
,	O
2000	O
)	O
the	O
Hypernymy	O
task	O
(	O
Breit	O
et	O
al	O
,	O
2021	O
)	O
,	O
etc	O
.	O
Additionally	O
,	O
static	O
embeddings	O
are	O
substantially	O
more	O
efficient	O
to	O
use	O
,	O
can	O
accommodate	O
larger	O
vocabulary	O
sizes	O
,	O
and	O
can	O
accommodate	O
efficient	O
indexing	O
and	O
retrieval	O
.	O
Yet	O
,	O
despite	O
their	O
flexibility	O
and	O
success	O
,	O
common	O
word	O
embedding	O
methods	O
still	O
represent	O
ambiguous	O
words	O
as	O
a	O
single	O
vector	O
,	O
and	O
suffer	O
from	O
the	O
inability	O
to	O
distinguish	O
between	O
different	O
meanings	O
of	O
a	O
word	O
(	O
Camacho	O
-	O
Collados	O
and	O
Pilehvar	O
,	O
2018	O
)	O
.	O
Using	O
our	O
sense	O
-	O
tagged	O
corpus	O
we	O
suggest	O
a	O
simple	O
and	O
effective	O
method	O
for	O
deriving	O
sense	O
-	O
aware	O
static	O
embeddings	O
:	O
We	O
run	O
an	O
off	O
-	O
the	O
-	O
shelf	O
embedding	O
algorithm	O
,	O
10	O
on	O
the	O
corpus	O
where	O
single	O
-	O
token	O
words	O
are	O
replaced	O
with	O
a	O
concatenation	O
of	O
the	O
word	O
and	O
its	O
induced	O
sense	O
(	O
e.g.	O
"	O
I	O
caught	O
a	O
bass	O
.	O
"	O
becomes	O
"	O
I	O
caught@0	O
a	O
bass@2	O
.	O
"	O
)	O
.	O
This	O
makes	O
the	O
embedding	O
algorithm	O
learn	O
embeddings	O
for	O
all	O
senses	O
of	O
each	O
word	O
out	O
-	O
of	O
-	O
the	O
-	O
box	O
.	O
11	O
An	O
integral	O
property	O
of	O
the	O
embedding	O
algorithm	O
is	O
that	O
it	O
represents	O
both	O
the	O
sense	O
-	O
annotated	O
tokens	O
and	O
the	O
other	O
vocabulary	O
items	O
in	O
the	O
same	O
embedding	O
space	O
-	O
10	O
We	O
use	O
the	O
CBOW	O
variant	O
of	O
the	O
word2vec	O
algorithm	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
as	O
implemented	O
in	O
Gensim	O
(	O
Řehůřek	O
and	O
Sojka	O
,	O
2010	O
)	O
.	O
We	O
derive	O
100	O
-	O
dimensional	O
embeddings	O
using	O
the	O
negative	O
-	O
sampling	O
algorithm	O
and	O
a	O
window	O
size	O
of	O
5	O
.	O
11	O
A	O
similar	O
approach	O
was	O
used	O
by	O
Iacobacci	O
et	O
al	O
(	O
2015	O
)	O
over	O
a	O
corpus	O
which	O
was	O
labeled	O
with	O
BabelNet	O
and	O
WordNet	O
senses	O
.	O
Figure	O
3	O
:	O
User	O
interaction	O
in	O
SPIKE	O
when	O
looking	O
for	O
the	O
word	O
"	O
swine	O
"	O
in	O
its	O
"	O
swine	O
flu	O
"	O
sense	O
.	O
(	O
Unlike	O
the	O
animal	O
/	O
experimental	O
pig	O
senses	O
)	O
this	O
helps	O
sense	O
inferring	O
about	O
words	O
that	O
are	O
represented	O
in	O
the	O
MLM	B-DatasetName
as	O
multi	O
-	O
tokens	O
words	O
(	O
Even	O
though	O
these	O
correspond	O
to	O
less	O
-	O
frequent	O
and	O
often	O
less	O
ambiguous	O
words	O
(	O
Hernández	O
-	O
Fernández	O
et	O
al	O
,	O
2016	O
;	O
Fenk	O
-	O
Oczlon	O
et	O
al	O
,	O
2010	O
;	O
Zipf	O
,	O
1945	O
)	O
)	O
.	O
For	O
example	O
,	O
in	O
the	O
top	O
-	O
5	O
nearest	O
neighbours	O
for	O
the	O
different	O
bass	O
senses	O
as	O
shown	O
below	O
,	O
smallmouth	O
and	O
pumpkinseed	O
,	O
multi	O
-	O
token	O
words	O
in	O
BERTLARGE	O
's	O
vocabulary	O
,	O
are	O
close	O
neighbours	O
the	O
bass	O
instances	O
that	O
correspond	O
to	O
the	O
fish	O
sense	O
.	O
Note	O
that	O
some	O
neighbours	O
are	O
sense	O
annotated	O
(	O
single	O
-	O
token	O
words	O
that	O
were	O
tagged	O
by	O
our	O
system	O
)	O
,	O
while	O
others	O
are	O
not	O
(	O
multi	O
-	O
token	O
words	O
)	O
.	O
For	O
English	O
Wikipedia	O
,	O
we	O
obtain	O
a	O
total	O
vocabulary	O
of	O
1.4	O
M	O
forms	O
,	O
90	O
,	O
023	O
of	O
which	O
are	O
senseannotated	O
.	O
Compared	O
to	O
the	O
community	O
-	O
based	O
representative	O
words	O
,	O
the	O
top	O
neighbours	O
in	O
the	O
embedding	O
space	O
tend	O
to	O
capture	O
members	O
of	O
the	O
same	O
semantic	O
class	O
rather	O
than	O
direct	O
potential	O
replacements	O
.	O
9	O
Sense	O
-	O
aware	O
Embeddings	B-TaskName
Evaluation	I-TaskName

Pilehvar	O
and	O
Camacho	O
-	O
Collados	O
(	O
2019	O
)	O
introduced	O
the	O
WiC	B-DatasetName
dataset	O
for	O
the	O
task	O
of	O
classifying	O
word	O
meaning	O
in	O
context	O
.	O
Each	O
instance	O
in	O
WiC	B-DatasetName
has	O
a	O
target	O
word	O
and	O
two	O
contexts	O
in	O
which	O
it	O
appears	O
.	O
The	O
goal	O
is	O
to	O
classify	O
whether	O
the	O
word	O
in	O
the	O
different	O
contexts	O
share	O
the	O
same	O
meaning	O
.	O
e.g.	O
given	O
two	O
contexts	O
:	O
There	O
's	O
a	O
lot	O
of	O
trash	O
on	O
the	O
bed	O
of	O
the	O
river	O
and	O
I	O
keep	O
a	O
glass	O
of	O
water	O
next	O
to	O
my	O
bed	O
when	O
I	O
sleep	O
,	O
our	O
method	O
should	O
return	O
False	O
as	O
the	O
sense	O
of	O
the	O
target	O
word	O
bed	O
is	O
different	O
.	O

We	O
show	O
that	O
substitution	O
-	O
based	O
word	O
-	O
sense	O
induction	O
algorithms	O
based	O
on	O
word	O
-	O
substitutions	O
derived	O
from	O
MLMs	O
are	O
easily	O
scalable	O
to	O
large	O
corpora	O
and	O
vocabulary	O
sizes	O
,	O
allowing	O
to	O
efficiently	O
obtain	O
high	O
-	O
quality	O
sense	O
annotated	O
corpora	O
.	O
We	O
demonstrate	O
the	O
utility	O
of	O
such	O
large	O
-	O
scale	O
sense	O
annotation	O
,	O
both	O
in	O
the	O
context	O
of	O
a	O
scientific	O
search	O
application	O
,	O
and	O
for	O
deriving	O
high	O
-	O
quality	O
senseaware	O
static	O
word	B-TaskName
embeddings	I-TaskName
.	O
As	O
a	O
secondary	O
contribution	O
,	O
we	O
also	O
develop	O
a	O
new	O
variant	O
of	O
the	O
Outlier	B-TaskName
Detection	I-TaskName
evaluation	O
task	O
,	O
which	O
explicitly	O
targets	O
ambiguous	O
words	O
.	O

This	O
project	O
has	O
received	O
funding	O
from	O
the	O
European	O
Research	O
Council	O
(	O
ERC	O
)	O
under	O
the	O
European	O
Union	O
's	O
Horizon	O
2020	O
research	O
and	O
innovation	O
programme	O
,	O
grant	O
agreement	O
No	O
.	O
802774	O
(	O
iEX	O
-	O
TRACT	B-DatasetName
)	O
.	O

When	O
using	O
a	O
single	O
-	O
prototype	O
vector	O
-	O
space	O
models	O
,	O
Camacho	O
-	O
Collados	O
and	O
Navigli	O
(	O
2016	O
)	O
proposed	O
a	O
procedure	O
for	O
detecting	O
outliers	O
based	O
on	O
semantic	B-TaskName
similarity	I-TaskName
using	O
compactness	O
score	O
:	O
c	O
(	O
w	O
)	O
=	O
1	O
n	O
2	O
−	O
n	O
w	O
i	O
W	O
\	O
{	O
w	O
}	O
w	O
j	O
W	O
\	O
{	O
w	O
}	O
w	O
i	O
=	O
w	O
j	O
sim	O
(	O
w	O
i	O
,	O
w	O
j	O
)	O
Where	O
D	O
is	O
the	O
entire	O
dataset	O
and	O
W	O
is	O
defined	O
as	O
{	O
w	O
1	O
,	O
w	O
2	O
,	O
,	O
w	O
n	O
,	O
w	O
n+1	O
}	O
where	O
w.l.o.g	O
.	O
{	O
w	O
1	O
,	O
w	O
2	O
,	O
,	O
w	O
n	O
}	O
are	O
the	O
group	O
elements	O
(	O
including	O
the	O
distractor	O
)	O
and	O
w	O
n+1	O
is	O
the	O
outlier	O
.	O
We	O
use	O
the	O
same	O
procedure	O
with	O
an	O
additional	O
nuance	O
,	O
we	O
expanded	O
the	O
procedure	O
to	O
receive	O
more	O
than	O
a	O
single	O
vector	O
representation	O
per	O
word	O
such	O
that	O
it	O
will	O
fit	O
multi	O
-	O
prototype	O
embeddings	O
(	O
e.g.	O
(	O
e.g.	O
word2vec	O
)	O
.	O
When	O
given	O
as	O
set	O
of	O
words	O
(	O
like	O
W	O
\	O
{	O
w	O
}	O
when	O
calculating	O
c	O
(	O
w	O
)	O
)	O
we	O
first	O
find	O
the	O
relevant	O
sense	O
for	O
each	O
element	O
before	O
inferring	O
the	O
outlier	O
.	O
Camacho	O
-	O
Collados	O
and	O
Navigli	O
(	O
2016	O
)	O
suggested	O
calculating	O
c	O
(	O
w	O
)	O
using	O
the	O
pseudo	O
inverted	O
compactness	O
score	O
.	O

In	O
-	O
Batch	O
Negatives	O
for	O
Knowledge	B-MethodName
Distillation	I-MethodName
with	O
Tightly	O
-	O
Coupled	O
Teachers	O
for	O
Dense	O
Retrieval	O

We	O
present	O
an	O
efficient	O
training	O
approach	O
to	O
text	O
retrieval	O
with	O
dense	O
representations	O
that	O
applies	O
knowledge	B-MethodName
distillation	I-MethodName
using	O
the	O
Col	O
-	O
BERT	B-MethodName
late	O
-	O
interaction	O
ranking	O
model	O
.	O
Specifically	O
,	O
we	O
propose	O
to	O
transfer	O
the	O
knowledge	O
from	O
a	O
bi	O
-	O
encoder	O
teacher	O
to	O
a	O
student	O
by	O
distilling	O
knowledge	O
from	O
ColBERT	O
's	O
expressive	O
MaxSim	O
operator	O
into	O
a	O
simple	O
dot	O
product	O
.	O
The	O
advantage	O
of	O
the	O
bi	O
-	O
encoder	O
teacherstudent	O
setup	O
is	O
that	O
we	O
can	O
efficiently	O
add	O
inbatch	O
negatives	O
during	O
knowledge	B-MethodName
distillation	I-MethodName
,	O
enabling	O
richer	O
interactions	O
between	O
teacher	O
and	O
student	O
models	O
.	O
In	O
addition	O
,	O
using	O
Col	O
-	O
BERT	B-MethodName
as	O
the	O
teacher	O
reduces	O
training	O
cost	O
compared	O
to	O
a	O
full	O
cross	O
-	O
encoder	O
.	O
Experiments	O
on	O
the	O
MS	B-DatasetName
MARCO	I-DatasetName
passage	O
and	O
document	B-TaskName
ranking	I-TaskName
tasks	O
and	O
data	O
from	O
the	O
TREC	B-DatasetName
2019	O
Deep	O
Learning	O
Track	O
demonstrate	O
that	O
our	O
approach	O
helps	O
models	O
learn	O
robust	O
representations	O
for	O
dense	O
retrieval	O
effectively	O
and	O
efficiently	O
.	O
*	O
Contributed	O
equally	O
.	O
The	O
standard	O
reranker	O
architecture	O
,	O
while	O
effective	O
,	O
exhibits	O
high	O
query	O
latency	O
,	O
on	O
the	O
order	O
of	O
seconds	O
per	O
query	O
(	O
Hofstätter	O
and	O
Hanbury	O
,	O
2019	O
;	O
Khattab	O
and	O
Zaharia	O
,	O
2020	O
)	O
because	O
expensive	O
neural	O
inference	O
must	O
be	O
applied	O
at	O
query	O
time	O
on	O
query	O
-	O
passage	O
pairs	O
.	O
This	O
design	O
is	O
known	O
as	O
a	O
cross	O
-	O
encoder	O
(	O
Humeau	O
et	O
al	O
,	O
2020	O
)	O
,	O
which	O
exploits	O
query	O
-	O
passage	O
attention	O
interactions	O
across	O
all	O
transformer	O
layers	O
.	O
As	O
an	O
alternative	O
,	O
a	O
biencoder	O
design	O
provides	O
an	O
approach	O
to	O
ranking	O
with	O
dense	O
representations	O
that	O
is	O
far	O
more	O
efficient	O
than	O
cross	O
-	O
encoders	O
(	O

d	O
−	O
q0	O
q	O
0	B-DatasetName
d	O
+	O
q0	O
q	O
0	B-DatasetName
q	O
q	O
2	O
q	O
d	O
−	O
q	O
1	O
d	O
+	O
q	O
q	O
2	O
d	O
−	O
q2	O
d	O
+	O
q2	O
q	O
0	B-DatasetName
q	O
1	O
q	O
2	O
d	O
+	O
q0	O
d	O
+	O
q	O
1	O
d	O
+	O
q2	O
d	O
−	O
q0	O
d	O
−	O
q	O
1	O
d	O
−	O
q2	O

Target	O
:	O
Pairwise	O
KD	O
Target	O
:	O
In	O
-	O
batch	O
KD	O
Teacher	O
Student	O
Embeddings	O
q	O
0	B-DatasetName
q	O
q	O
2	O
d	O
−	O
q0	O
d	O
−	O
q	O
1	O
d	O
−	O
q2	O
d	O
+	O
q0	O
d	O
+	O
q	O
1	O
d	O
+	O
q2	O
d	O
−	O
q2	O
d	O
+	O
q2	O
d	O
−	O
q	O
1	O
d	O
+	O
q	O
1	O
d	O
−	O
q0	O
d	O
+	O
q0	O
d	O
−	O
q2	O
d	O
+	O
q2	O
d	O
−	O
q	O
1	O
d	O
+	O
q	O
d	O
−	O
q0	O
d	O
+	O
q0	O
d	O
−	O
q2	O
d	O
+	O
q2	O
d	O
−	O
q	O
1	O
d	O
+	O
q	O
1	O
d	O
−	O
q0	O
d	O
+	O
q0	O
q0	O
q1	O
q2	O
d	O
−	O
q0	O
d	O
−	O
q1	O
d	O
−	O
q2	O
d	O
+	O
q0	O
d	O
+	O
q1	O
d	O
+	O
q2	O

Figure	O
1	O
:	O
Illustration	O
of	O
the	O
differences	O
between	O
pairwise	O
knowledge	B-MethodName
distillation	I-MethodName
and	O
our	O
proposed	O
in	O
-	O
batch	O
knowledge	B-MethodName
distillation	I-MethodName
.	O
computed	O
due	O
to	O
the	O
computational	O
costs	O
of	O
crossencoders	O
(	O
Hofstätter	O
et	O
al	O
,	O
2020	O
;	O
Gao	O
et	O
al	O
,	O
2020a	O
;	O
Barkan	O
et	O
al	O
,	O
2020	O
)	O
.	O
The	O
contribution	O
of	O
this	O
work	O
is	O
a	O
simple	O
technique	O
for	O
efficiently	O
adding	O
in	O
-	O
batch	O
negative	O
samples	O
during	O
knowledge	B-MethodName
distillation	I-MethodName
when	O
training	O
a	O
single	O
-	O
vector	O
bi	O
-	O
encoder	O
.	O
For	O
the	O
remainder	O
of	O
this	O
paper	O
,	O
we	O
refer	O
to	O
this	O
technique	O
as	O
"	O
in	O
-	O
batch	O
KD	O
"	O
for	O
convenience	O
.	O
We	O
empirically	O
show	O
that	O
our	O
model	O
,	O
even	O
trained	O
with	O
BM25	O
negatives	O
,	O
can	O
be	O
more	O
effective	O
than	O
cross	O
-	O
encoder	O
teachers	O
.	O
With	O
hard	O
negatives	O
,	O
our	O
method	O
approaches	O
the	O
state	O
of	O
the	O
art	O
in	O
dense	O
retrieval	O
.	O
Our	O
in	O
-	O
batch	O
KD	O
technique	O
is	O
able	O
to	O
incorporate	O
hard	O
negatives	O
in	O
a	O
computationally	O
efficient	O
manner	O
,	O
without	O
requiring	O
large	O
amounts	O
of	O
GPU	O
memory	O
for	O
large	O
batch	O
sizes	O
or	O
expensive	O
periodic	O
index	O
refreshes	O
.	O

We	O
focus	O
on	O
improving	O
the	O
training	O
efficiency	O
and	O
retrieval	O
effectiveness	O
of	O
dense	O
retrieval	O
and	O
begin	O
by	O
formalizing	O
it	O
as	O
a	O
dense	O
representation	B-TaskName
learning	I-TaskName
problem	O
.	O
To	O
be	O
more	O
specific	O
,	O
we	O
propose	O
to	O
use	O
knowledge	B-MethodName
distillation	I-MethodName
to	O
enrich	O
training	O
signals	O
and	O
stabilize	O
the	O
representation	B-TaskName
learning	I-TaskName
procedure	O
of	O
bi	O
-	O
encoder	O
models	O
in	O
the	O
context	O
of	O
the	O
well	O
-	O
known	O
Noise	O
-	O
Contrastive	O
Estimation	O
(	O
NCE	O
)	O
framework	O
.	O

A	O
cross	O
-	O
encoder	O
has	O
been	O
shown	O
to	O
be	O
an	O
effective	O
teacher	O
(	O
Hofstätter	O
et	O
al	O
,	O
2020	O
;	O
Gao	O
et	O
al	O
,	O
2020a	O
)	O
since	O
it	O
allows	O
rich	O
interactions	O
between	O
the	O
intermediate	O
transformer	O
representations	O
of	O
a	O
query	O
q	O
and	O
a	O
passage	O
p.	O
For	O
example	O
,	O
a	O
"	O
vanilla	O
"	O
crossencoder	O
design	O
using	O
BERT	B-MethodName
can	O
be	O
denoted	O
as	O
:	O
φθ	O
;	O
Cat	O
W	O
f	O
(	O
h	O
q	O
p	O
)	O
,	O
(	O
5	O
)	O
where	O
the	O
ranking	O
score	O
is	O
first	O
computed	O
by	O
the	O
hidden	O
representation	O
of	O
the	O
concatenation	O
q	O
p	O
from	O
BERT	B-MethodName
(	O
along	O
with	O
the	O
standard	O
special	O
tokens	O
)	O
and	O
then	O
mapped	O
to	O
a	O
scalar	O
by	O
a	O
pooling	O
operation	O
f	O
and	O
a	O
mapping	O
matrix	O
W	O
.	O
Although	O
effective	O
,	O
due	O
to	O
BERT	B-MethodName
's	O
quadratic	O
complexity	O
with	O
respect	O
to	O
input	O
sequence	O
length	O
,	O
this	O
design	O
makes	O
exhaustive	O
combinations	O
between	O
a	O
query	O
and	O
possible	O
candidates	O
impractical	O
,	O
since	O
this	O
requires	O
evaluating	O
cross	O
-	O
encoders	O
|	O
B	O
|	O
2	O
times	O
to	O
compute	O
Eq	O
.	O
(	O
3	O
)	O
using	O
Eq	O
.	O
(	O
5	O
)	O
.	O
Thus	O
,	O
an	O
alternative	O
is	O
to	O
conduct	O
pairwise	O
KD	O
by	O
computing	O
the	O
KL	O
divergence	O
of	O
only	O
two	O
probabilities	O
of	O
a	O
positive	O
pair	O
(	O
q	O
,	O
p	O
)	O
and	O
a	O
negative	O
pair	O
(	O
q	O
,	O
p	O
)	O
for	O
each	O
query	O
q.	O
However	O
,	O
this	O
might	O
not	O
yield	O
a	O
good	O
approximation	O
of	O
Eq	O
.	O
(	O
2	O
)	O
.	O
A	O
bi	O
-	O
encoder	O
can	O
also	O
be	O
leveraged	O
as	O
a	O
teacher	O
model	O
,	O
which	O
has	O
the	O
advantage	O
that	O
it	O
is	O
more	O
feasible	O
to	O
perform	O
exhaustive	O
comparisons	O
between	O
queries	O
and	O
passages	O
since	O
they	O
are	O
passed	O
through	O
the	O
encoder	O
independently	O
.	O
Among	O
biencoder	O
designs	O
,	O
ColBERT	O
is	O
a	O
representative	O
model	O
that	O
uses	O
late	O
interactions	O
of	O
multiple	O
vectors	O
(	O
{	O
h	O
1	O
q	O
,	O
.	O
.	O
.	O
,	O
h	O
i	O
q	O
}	O
,	O
{	O
h	O
1	O
p	O
,	O
.	O
.	O
.	O
,	O
h	O
j	O
p	O
}	O
)	O
to	O
improve	O
the	O
robustness	O
of	O
dense	O
retrieval	O
,	O
as	O
compared	O
to	O
inner	O
products	O
of	O
pairs	O
of	O
single	O
vectors	O
(	O
h	O
q	O
,	O
h	O
p	O
)	O
.	O
Specifically	O
,	O
Khattab	O
and	O
Zaharia	O
(	O
2020	O
)	O
propose	O
the	O
following	O
fine	O
-	O
grained	O
scoring	O
function	O
:	O
φθ	O
;	O
MaxSim	O
i	O
|	O
hq	O
|	O
max	O
j	O
|	O
hp	O
|	O
h	O
i	O
q	O
h	O
j	O
p	O
,	O
(	O
6	O
)	O
where	O
i	O
and	O
j	O
are	O
the	O
indices	O
of	O
token	O
representations	O
of	O
a	O
query	O
q	O
and	O
a	O
passage	O
p	O
of	O
Col	O
-	O
BERT	B-MethodName
(	O
Khattab	O
and	O
Zaharia	O
,	O
2020	O
)	O
.	O
The	O
contribution	O
of	O
our	O
work	O
is	O
in	O
-	O
batch	O
knowledge	B-MethodName
distillation	I-MethodName
with	O
a	O
tightly	O
-	O
coupled	O
teacher	O
.	O
The	O
computation	O
of	O
φθ	O
;	O
MaxSim	O
enables	O
exhaustive	O
inference	O
over	O
all	O
query	O
-	O
passage	O
combinations	O
in	O
the	O
minibatch	O
B	O
with	O
only	O
2	O
|	O
B	O
|	O
computation	O
cost	O
,	O
enabling	O
enriched	O
interactions	O
between	O
teacher	O
and	O
student	O
.	O
We	O
call	O
this	O
design	O
Tightly	O
-	O
Coupled	O
Teacher	O
ColBERT	O
(	O
TCT	O
-	O
ColBERT	O
)	O
.	O
Table	O
1	O
pro	O
TCT	O
-	O
ColBERT	O
provides	O
a	O
flexible	O
design	O
for	O
biencoders	O
,	O
as	O
long	O
as	O
the	O
encoders	O
produce	O
query	O
and	O
passage	O
representations	O
independently	O
.	O
For	O
simplicity	O
,	O
our	O
student	O
model	O
adopts	O
shared	O
encoder	O
weights	O
for	O
both	O
the	O
query	O
and	O
the	O
passage	O
,	O
just	O
like	O
the	O
teacher	O
model	O
ColBERT	O
.	O
Following	O
Khattab	O
and	O
Zaharia	O
(	O
2020	O
)	O
,	O
for	O
each	O
query	O
(	O
passage	O
)	O
,	O
we	O
prepend	O
the	O
[	O
CLS	O
]	O
token	O
and	O
another	O
special	O
[	O
Q	O
]	O
(	O
[	O
D	O
]	O
)	O
token	O
in	O
the	O
input	O
sequence	O
for	O
both	O
our	O
teacher	O
and	O
student	O
models	O
.	O
The	O
student	O
encoder	O
outputs	O
single	O
-	O
vector	O
dense	O
representations	O
(	O
h	O
q	O
,	O
h	O
p	O
)	O
by	O
performing	O
average	B-MethodName
pooling	I-MethodName
over	O
the	O
token	O
embeddings	O
from	O
the	O
final	O
layer	O
.	O

Given	O
that	O
in	O
-	O
batch	O
negative	O
sampling	O
is	O
an	O
efficient	O
way	O
to	O
add	O
more	O
information	O
into	O
knowledge	B-MethodName
distillation	I-MethodName
,	O
we	O
wonder	O
whether	O
our	O
tightly	O
-	O
coupled	O
teacher	O
design	O
works	O
well	O
when	O
applied	O
to	O
more	O
sophisticated	O
sampling	O
methods	O
.	O
Following	O
the	O
work	O
of	O
,	O
we	O
use	O
our	O
pretrained	O
bi	O
-	O
encoder	O
model	O
,	O
namely	O
TCT	O
-	O
ColBERT	O
,	O
to	O
encode	O
the	O
corpus	O
and	O
sample	O
"	O
hard	O
"	O
negatives	O
for	O
each	O
query	O
to	O
create	O
new	O
training	O
triplets	O
by	O
using	O
the	O
negatives	O
D	O
−	O
of	O
the	O
bi	O
-	O
encoder	O
instead	O
of	O
BM25	O
.	O
Specifically	O
,	O
we	O
explore	O
three	O
different	O
training	O
strategies	O
:	O
1	O
.	O
HN	O
:	O
we	O
train	O
the	O
bi	O
-	O
encoder	O
using	O
in	O
-	O
batch	O
hard	O
negatives	O
without	O
the	O
guide	O
of	O
ColBERT	O
.	O
2	O
.	O
TCT	O
HN	O
:	O
we	O
train	O
the	O
bi	O
-	O
encoder	O
with	O
TCT	O
-	O
ColBERT	O
;	O
3	O
.	O
TCT	O
HN+	O
:	O
we	O
first	O
fine	O
-	O
tune	O
our	O
ColBERT	O
teacher	O
with	O
augmented	O
training	O
data	O
containing	O
hard	O
negatives	O
and	O
then	O
distill	O
its	O
knowledge	O
into	O
the	O
bi	O
-	O
encoder	O
student	O
through	O
TCT	O
-	O
ColBERT	O
.	O
We	O
empirically	O
explore	O
the	O
effectiveness	O
of	O
these	O
strategies	O
for	O
both	O
passage	O
and	O
document	O
retrieval	O
.	O

Improving	O
the	O
effectiveness	O
of	O
single	O
-	O
vector	O
biencoders	O
is	O
an	O
important	O
research	O
direction	O
in	O
dense	O
retrieval	O
because	O
of	O
lower	O
latency	O
and	O
storage	O
requirements	O
compared	O
to	O
multi	O
-	O
vector	O
approaches	O
.	O
We	O
propose	O
a	O
teacher	O
-	O
student	O
knowledge	B-MethodName
distillation	I-MethodName
approach	O
using	O
tightly	O
coupled	O
bi	O
-	O
encoders	O
that	O
enables	O
exhaustive	O
use	O
of	O
query	O
-	O
passage	O
combinations	O
in	O
each	O
minibatch	O
.	O
More	O
importantly	O
,	O
a	O
bi	O
-	O
encoder	O
teacher	O
requires	O
less	O
computation	O
than	O
a	O
cross	O
-	O
encoder	O
teacher	O
.	O
Finally	O
,	O
our	O
approach	O
leads	O
to	O
robust	O
learned	O
representations	O
.	O
Overall	O
,	O
our	O
hard	O
negative	O
sampling	O
strategy	O
leads	O
to	O
an	O
effective	O
and	O
efficient	O
dense	O
retrieval	O
technique	O
,	O
which	O
can	O
be	O
further	O
combined	O
with	O
sparse	O
retrieval	O
techniques	O
in	O
dense	O
-	O
sparse	O
hybrids	O
.	O
Together	O
,	O
these	O
designs	O
provide	O
a	O
promising	O
solution	O
for	O
end	O
-	O
to	O
-	O
end	O
text	O
retrieval	O
that	O
balances	O
quality	O
,	O
query	O
latency	O
,	O
and	O
storage	O
requirements	O
.	O

BERT	B-MethodName
:	O
Pre	O
-	O
training	O
of	O
Deep	O
Bidirectional	O
Transformers	O
for	O
Language	O
Understanding	O

Learning	O
widely	O
applicable	O
representations	O
of	O
words	O
has	O
been	O
an	O
active	O
area	O
of	O
research	O
for	O
decades	O
,	O
including	O
non	O
-	O
neural	O
(	O
Brown	O
et	O
al	O
,	O
1992	O
;	O
Ando	O
and	O
Zhang	O
,	O
2005	O
;	O
Blitzer	O
et	O
al	O
,	O
2006	O
)	O
and	O
neural	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
methods	O
.	O
Pre	O
-	O
trained	O
word	B-TaskName
embeddings	I-TaskName
are	O
an	O
integral	O
part	O
of	O
modern	O
NLP	O
systems	O
,	O
offering	O
significant	O
improvements	O
over	O
embeddings	O
learned	O
from	O
scratch	O
(	O
Turian	O
et	O
al	O
,	O
2010	O
)	O
.	O
To	O
pretrain	O
word	O
embedding	O
vectors	O
,	O
left	O
-	O
to	O
-	O
right	O
language	O
modeling	O
objectives	O
have	O
been	O
used	O
(	O
Mnih	O
and	O
Hinton	O
,	O
2009	O
)	O
,	O
as	O
well	O
as	O
objectives	O
to	O
discriminate	O
correct	O
from	O
incorrect	O
words	O
in	O
left	O
and	O
right	O
context	O
.	O
These	O
approaches	O
have	O
been	O
generalized	O
to	O
coarser	O
granularities	O
,	O
such	O
as	O
sentence	B-TaskName
embeddings	I-TaskName
Logeswaran	O
and	O
Lee	O
,	O
2018	O
)	O
or	O
paragraph	O
embeddings	O
(	O
Le	O
and	O
Mikolov	O
,	O
2014	O
)	O
.	O
To	O
train	O
sentence	O
representations	O
,	O
prior	O
work	O
has	O
used	O
objectives	O
to	O
rank	O
candidate	O
next	O
sentences	O
(	O
Jernite	O
et	O
al	O
,	O
2017	O
;	O
Logeswaran	O
and	O
Lee	O
,	O
2018	O
)	O
,	O
left	O
-	O
to	O
-	O
right	O
generation	O
of	O
next	O
sentence	O
words	O
given	O
a	O
representation	O
of	O
the	O
previous	O
sentence	O
,	O
or	O
denoising	B-MethodName
autoencoder	I-MethodName
derived	O
objectives	O
(	O
Hill	O
et	O
al	O
,	O
2016	O
)	O
.	O
ELMo	B-MethodName
and	O
its	O
predecessor	O
(	O
Peters	O
et	O
al	O
,	O
2017	O
(	O
Peters	O
et	O
al	O
,	O
,	O
2018a	O
generalize	O
traditional	O
word	O
embedding	O
research	O
along	O
a	O
different	O
dimension	O
.	O
They	O
extract	O
context	O
-	O
sensitive	O
features	O
from	O
a	O
left	O
-	O
to	O
-	O
right	O
and	O
a	O
right	O
-	O
to	O
-	O
left	O
language	O
model	O
.	O
The	O
contextual	O
representation	O
of	O
each	O
token	O
is	O
the	O
concatenation	O
of	O
the	O
left	O
-	O
to	O
-	O
right	O
and	O
right	O
-	O
to	O
-	O
left	O
representations	O
.	O
When	O
integrating	O
contextual	O
word	B-TaskName
embeddings	I-TaskName
with	O
existing	O
task	O
-	O
specific	O
architectures	O
,	O
ELMo	B-MethodName
advances	O
the	O
state	O
of	O
the	O
art	O
for	O
several	O
major	O
NLP	O
benchmarks	O
(	O
Peters	O
et	O
al	O
,	O
2018a	O
)	O
including	O
question	B-TaskName
answering	I-TaskName
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
,	O
sentiment	B-TaskName
analysis	I-TaskName
(	O
Socher	O
et	O
al	O
,	O
2013	O
)	O
,	O
and	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
Tjong	O
Kim	O
Sang	O
and	O
De	O
Meulder	O
,	O
2003	O
)	O
.	O
Melamud	O
et	O
al	O
(	O
2016	O
)	O
proposed	O
learning	O
contextual	O
representations	O
through	O
a	O
task	O
to	O
predict	O
a	O
single	O
word	O
from	O
both	O
left	O
and	O
right	O
context	O
using	O
LSTMs	O
.	O
Similar	O
to	O
ELMo	B-MethodName
,	O
their	O
model	O
is	O
feature	O
-	O
based	O
and	O
not	O
deeply	O
bidirectional	O
.	O
Fedus	O
et	O
al	O
(	O
2018	O
)	O
shows	O
that	O
the	O
cloze	O
task	O
can	O
be	O
used	O
to	O
improve	O
the	O
robustness	O
of	O
text	B-TaskName
generation	I-TaskName
models	O
.	O

As	O
with	O
the	O
feature	O
-	O
based	O
approaches	O
,	O
the	O
first	O
works	O
in	O
this	O
direction	O
only	O
pre	O
-	O
trained	O
word	O
embedding	O
parameters	O
from	O
unlabeled	O
text	O
(	O
Collobert	O
and	O
Weston	O
,	O
2008	O
)	O
.	O
More	O
recently	O
,	O
sentence	O
or	O
document	O
encoders	O
which	O
produce	O
contextual	O
token	O
representations	O
have	O
been	O
pre	O
-	O
trained	O
from	O
unlabeled	O
text	O
and	O
fine	O
-	O
tuned	O
for	O
a	O
supervised	O
downstream	O
task	O
(	O
Dai	O
and	O
Le	O
,	O
2015	O
;	O
Howard	O
and	O
Ruder	O
,	O
2018	O
;	O
Radford	O
et	O
al	O
,	O
2018	O
)	O
.	O
The	O
advantage	O
of	O
these	O
approaches	O
is	O
that	O
few	O
parameters	O
need	O
to	O
be	O
learned	O
from	O
scratch	O
.	O
At	O
least	O
partly	O
due	O
to	O
this	O
advantage	O
,	O
OpenAI	O
GPT	B-MethodName
(	O
Radford	O
et	O
al	O
,	O
2018	O
)	O
achieved	O
previously	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
many	O
sentencelevel	O
tasks	O
from	O
the	O
GLUE	B-DatasetName
benchmark	O
(	O
Wang	O
et	O
al	O
,	O
2018a	O
)	O
.	O
Left	O
-	O
to	O
-	O
right	O
language	O
model	O
-	O
BERT	B-MethodName
BERT	B-MethodName
E	O
[	O
CLS	O
]	O
E	O
1	O
E	O
[	O
SEP	O
]	O
...	O
E	O
N	O
E	O
1	O
'	O
...	O
E	O
M	O
'	O
C	O
T	O
1	O
T	O
[	O
SEP	O
]	O
...	O
...	O
ing	O
and	O
auto	O
-	O
encoder	O
objectives	O
have	O
been	O
used	O
for	O
pre	O
-	O
training	O
such	O
models	O
(	O
Howard	O
and	O
Ruder	O
,	O
2018	O
;	O
Radford	O
et	O
al	O
,	O
2018	O
;	O
Dai	O
and	O
Le	O
,	O
2015	O
)	O
.	O

There	O
has	O
also	O
been	O
work	O
showing	O
effective	O
transfer	O
from	O
supervised	O
tasks	O
with	O
large	O
datasets	O
,	O
such	O
as	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
(	O
Conneau	O
et	O
al	O
,	O
2017	O
)	O
and	O
machine	B-TaskName
translation	I-TaskName
(	O
McCann	O
et	O
al	O
,	O
2017	O
)	O
.	O
Computer	O
vision	O
research	O
has	O
also	O
demonstrated	O
the	O
importance	O
of	O
transfer	B-TaskName
learning	I-TaskName
from	O
large	O
pre	O
-	O
trained	O
models	O
,	O
where	O
an	O
effective	O
recipe	O
is	O
to	O
fine	O
-	O
tune	O
models	O
pre	O
-	O
trained	O
with	O
I	O
m	O
a	O
-	O
geNet	B-MethodName
(	O
Deng	B-DatasetName
et	I-DatasetName
al	I-DatasetName
,	O
2009	O
;	O
Yosinski	O
et	O
al	O
,	O
2014	O
)	O
.	O

The	O
pre	O
-	O
training	O
procedure	O
largely	O
follows	O
the	O
existing	O
literature	O
on	O
language	O
model	O
pre	O
-	O
training	O
.	O
For	O
the	O
pre	O
-	O
training	O
corpus	O
we	O
use	O
the	O
BooksCorpus	O
(	O
800	O
M	O
words	O
)	O
and	O
English	O
Wikipedia	O
(	O
2	O
,	O
500	O
M	O
words	O
)	O
.	O
For	O
Wikipedia	O
we	O
extract	O
only	O
the	O
text	O
passages	O
and	O
ignore	O
lists	O
,	O
tables	O
,	O
and	O
headers	O
.	O
It	O
is	O
critical	O
to	O
use	O
a	O
document	O
-	O
level	O
corpus	O
rather	O
than	O
a	O
shuffled	O
sentence	O
-	O
level	O
corpus	O
such	O
as	O
the	O
Billion	B-DatasetName
Word	I-DatasetName
Benchmark	I-DatasetName
(	O
Chelba	O
et	O
al	O
,	O
2013	O
)	O
in	O
order	O
to	O
extract	O
long	O
contiguous	O
sequences	O
.	O

Fine	O
-	O
tuning	O
is	O
straightforward	O
since	O
the	O
selfattention	O
mechanism	O
in	O
the	O
Transformer	B-MethodName
allows	O
BERT	B-MethodName
to	O
model	O
many	O
downstream	O
taskswhether	O
they	O
involve	O
single	O
text	O
or	O
text	O
pairs	O
-	O
by	O
swapping	O
out	O
the	O
appropriate	O
inputs	O
and	O
outputs	O
.	O
For	O
applications	O
involving	O
text	O
pairs	O
,	O
a	O
common	O
pattern	O
is	O
to	O
independently	O
encode	O
text	O
pairs	O
before	O
applying	O
bidirectional	O
cross	O
attention	O
,	O
such	O
as	O
Parikh	O
et	O
al	O
(	O
2016	O
)	O
;	O
Seo	O
et	O
al	O
(	O
2017	O
)	O
.	O
BERT	B-MethodName
instead	O
uses	O
the	O
self	O
-	O
attention	O
mechanism	O
to	O
unify	O
these	O
two	O
stages	O
,	O
as	O
encoding	O
a	O
concatenated	O
text	O
pair	O
with	O
self	O
-	O
attention	O
effectively	O
includes	O
bidirectional	O
cross	O
attention	O
between	O
two	O
sentences	O
.	O
For	O
each	O
task	O
,	O
we	O
simply	O
plug	O
in	O
the	O
taskspecific	O
inputs	O
and	O
outputs	O
into	O
BERT	B-MethodName
and	O
finetune	O
all	O
the	O
parameters	O
end	O
-	O
to	O
-	O
end	O
.	O
At	O
the	O
input	O
,	O
sentence	O
A	O
and	O
sentence	O
B	O
from	O
pre	O
-	O
training	O
are	O
analogous	O
to	O
(	O
1	O
)	O
sentence	O
pairs	O
in	O
paraphrasing	O
,	O
(	O
2	O
)	O
hypothesis	O
-	O
premise	O
pairs	O
in	O
entailment	O
,	O
(	O
3	O
)	O
question	O
-	O
passage	O
pairs	O
in	O
question	B-TaskName
answering	I-TaskName
,	O
and	O
(	O
4	O
)	O
a	O
degenerate	O
text	O
-	O
pair	O
in	O
text	B-TaskName
classification	I-TaskName
or	O
sequence	O
tagging	O
.	O
At	O
the	O
output	O
,	O
the	O
token	O
representations	O
are	O
fed	O
into	O
an	O
output	O
layer	O
for	O
tokenlevel	O
tasks	O
,	O
such	O
as	O
sequence	O
tagging	O
or	O
question	B-TaskName
answering	I-TaskName
,	O
and	O
the	O
[	O
CLS	O
]	O
representation	O
is	O
fed	O
into	O
an	O
output	O
layer	O
for	O
classification	O
,	O
such	O
as	O
entailment	O
or	O
sentiment	B-TaskName
analysis	I-TaskName
.	O
Compared	O
to	O
pre	O
-	O
training	O
,	O
fine	O
-	O
tuning	O
is	O
relatively	O
inexpensive	O
.	O
All	O
of	O
the	O
results	O
in	O
the	O
paper	O
can	O
be	O
replicated	O
in	O
at	O
most	O
1	O
hour	O
on	O
a	O
single	O
Cloud	O
TPU	O
,	O
or	O
a	O
few	O
hours	O
on	O
a	O
GPU	O
,	O
starting	O
from	O
the	O
exact	O
same	O
pre	O
-	O
trained	O
model	O
.	O
7	O
We	O
describe	O
the	O
task	O
-	O
specific	O
details	O
in	O
the	O
corresponding	O
subsections	O
of	O
Section	O
4	O
.	O
More	O
details	O
can	O
be	O
found	O
in	O
Appendix	O
A.5	O
.	O

In	O
this	O
section	O
,	O
we	O
present	O
BERT	B-MethodName
fine	O
-	O
tuning	O
results	O
on	O
11	O
NLP	O
tasks	O
.	O

In	O
this	O
section	O
,	O
we	O
perform	O
ablation	O
experiments	O
over	O
a	O
number	O
of	O
facets	O
of	O
BERT	B-MethodName
in	O
order	O
to	O
better	O
understand	O
their	O
relative	O
importance	O
.	O
Additional	O
ablation	O
studies	O
can	O
be	O
found	O
in	O
Appendix	O
C.	O

We	O
demonstrate	O
the	O
importance	O
of	O
the	O
deep	O
bidirectionality	O
of	O
BERT	B-MethodName
by	O
evaluating	O
two	O
pretraining	O
objectives	O
using	O
exactly	O
the	O
same	O
pretraining	O
data	O
,	O
fine	O
-	O
tuning	O
scheme	O
,	O
and	O
hyperparameters	O
as	O
BERT	B-MethodName
BASE	B-MethodName
:	O
No	O
NSP	O
:	O
A	O
bidirectional	O
model	O
which	O
is	O
trained	O
using	O
the	O
"	O
masked	O
LM	O
"	O
(	O
MLM	B-DatasetName
)	O
but	O
without	O
the	O
"	O
next	O
sentence	O
prediction	O
"	O
(	O
NSP	O
)	O
task	O
.	O

A	O
left	O
-	O
context	O
-	O
only	O
model	O
which	O
is	O
trained	O
using	O
a	O
standard	O
Left	O
-	O
to	O
-	O
Right	O
(	O
LTR	O
)	O
LM	O
,	O
rather	O
than	O
an	O
MLM	B-DatasetName
.	O
The	O
left	O
-	O
only	O
constraint	O
was	O
also	O
applied	O
at	O
fine	O
-	O
tuning	O
,	O
because	O
removing	O
it	O
introduced	O
a	O
pre	O
-	O
train	O
/	O
fine	O
-	O
tune	O
mismatch	O
that	O
degraded	O
downstream	O
performance	O
.	O
Additionally	O
,	O
this	O
model	O
was	O
pre	O
-	O
trained	O
without	O
the	O
NSP	O
task	O
.	O
This	O
is	O
directly	O
comparable	O
to	O
OpenAI	O
GPT	B-MethodName
,	O
but	O
using	O
our	O
larger	O
training	O
dataset	O
,	O
our	O
input	O
representation	O
,	O
and	O
our	O
fine	O
-	O
tuning	O
scheme	O
.	O
We	O
first	O
examine	O
the	O
impact	O
brought	O
by	O
the	O
NSP	O
task	O
.	O
In	O
Table	O
5	O
,	O
we	O
show	O
that	O
removing	O
NSP	O
hurts	O
performance	O
significantly	O
on	O
QNLI	B-DatasetName
,	O
MNLI	B-DatasetName
,	O
and	O
SQuAD	B-DatasetName
1.1	O
.	O
Next	O
,	O
we	O
evaluate	O
the	O
impact	O
of	O
training	O
bidirectional	O
representations	O
by	O
comparing	O
"	O
No	O
NSP	O
"	O
to	O
"	O
LTR	O
&	O
No	O
NSP	O
"	O
.	O
The	O
LTR	O
model	O
performs	O
worse	O
than	O
the	O
MLM	B-DatasetName
model	O
on	O
all	O
tasks	O
,	O
with	O
large	O
drops	O
on	O
MRPC	B-DatasetName
and	O
SQuAD	B-DatasetName
.	O
For	O
SQuAD	B-DatasetName
it	O
is	O
intuitively	O
clear	O
that	O
a	O
LTR	O
model	O
will	O
perform	O
poorly	O
at	O
token	O
predictions	O
,	O
since	O
the	O
token	O
-	O
level	O
hidden	O
states	O
have	O
no	O
rightside	O
context	O
.	O
In	O
order	O
to	O
make	O
a	O
good	O
faith	O
attempt	O
at	O
strengthening	O
the	O
LTR	O
system	O
,	O
we	O
added	O
a	O
randomly	O
initialized	O
BiLSTM	B-MethodName
on	O
top	O
.	O
This	O
does	O
significantly	O
improve	O
results	O
on	O
SQuAD	B-DatasetName
,	O
but	O
the	O
results	O
are	O
still	O
far	O
worse	O
than	O
those	O
of	O
the	O
pretrained	O
bidirectional	O
models	O
.	O
The	O
BiLSTM	B-MethodName
hurts	O
performance	O
on	O
the	O
GLUE	B-DatasetName
tasks	O
.	O
We	O
recognize	O
that	O
it	O
would	O
also	O
be	O
possible	O
to	O
train	O
separate	O
LTR	O
and	O
RTL	O
models	O
and	O
represent	O
each	O
token	O
as	O
the	O
concatenation	O
of	O
the	O
two	O
models	O
,	O
as	O
ELMo	B-MethodName
does	O
.	O
However	O
:	O
(	O
a	O
)	O
this	O
is	O
twice	O
as	O
expensive	O
as	O
a	O
single	O
bidirectional	O
model	O
;	O
(	O
b	O
)	O
this	O
is	O
non	O
-	O
intuitive	O
for	O
tasks	O
like	O
QA	O
,	O
since	O
the	O
RTL	O
model	O
would	O
not	O
be	O
able	O
to	O
condition	O
the	O
answer	O
on	O
the	O
question	O
;	O
(	O
c	O
)	O
this	O
it	O
is	O
strictly	O
less	O
powerful	O
than	O
a	O
deep	O
bidirectional	O
model	O
,	O
since	O
it	O
can	O
use	O
both	O
left	O
and	O
right	O
context	O
at	O
every	O
layer	O
.	O

In	O
Section	O
3.1	O
,	O
we	O
mention	O
that	O
BERT	B-MethodName
uses	O
a	O
mixed	O
strategy	O
for	O
masking	O
the	O
target	O
tokens	O
when	O
pre	O
-	O
training	O
with	O
the	O
masked	O
language	O
model	O
(	O
MLM	B-DatasetName
)	O
objective	O
.	O
The	O
following	O
is	O
an	O
ablation	O
study	O
to	O
evaluate	O
the	O
effect	O
of	O
different	O
masking	O
strategies	O
.	O
Note	O
that	O
the	O
purpose	O
of	O
the	O
masking	O
strategies	O
is	O
to	O
reduce	O
the	O
mismatch	O
between	O
pre	O
-	O
training	O
and	O
fine	O
-	O
tuning	O
,	O
as	O
the	O
[	O
MASK	O
]	O
symbol	O
never	O
appears	O
during	O
the	O
fine	O
-	O
tuning	O
stage	O
.	O
We	O
report	O
the	O
Dev	O
results	O
for	O
both	O
MNLI	B-DatasetName
and	O
NER	B-TaskName
.	O
For	O
NER	B-TaskName
,	O
we	O
report	O
both	O
fine	O
-	O
tuning	O
and	O
feature	O
-	O
based	O
approaches	O
,	O
as	O
we	O
expect	O
the	O
mismatch	O
will	O
be	O
amplified	O
for	O
the	O
feature	O
-	O
based	O
approach	O
as	O
the	O
model	O
will	O
not	O
have	O
the	O
chance	O
to	O
adjust	O
the	O
representations	O
.	O
The	O
results	O
are	O
presented	O
in	O
Table	O
8	O
.	O
In	O
the	O
table	O
,	O
MASK	O
means	O
that	O
we	O
replace	O
the	O
target	O
token	O
with	O
the	O
[	O
MASK	O
]	O
symbol	O
for	O
MLM	B-DatasetName
;	O
SAME	O
means	O
that	O
we	O
keep	O
the	O
target	O
token	O
as	O
is	O
;	O
RND	O
means	O
that	O
we	O
replace	O
the	O
target	O
token	O
with	O
another	O
random	O
token	O
.	O
The	O
numbers	O
in	O
the	O
left	O
part	O
of	O
the	O
table	O
represent	O
the	O
probabilities	O
of	O
the	O
specific	O
strategies	O
used	O
during	O
MLM	B-DatasetName
pre	O
-	O
training	O
(	O
BERT	B-MethodName
uses	O
80	O
%	O
,	O
10	O
%	O
,	O
10	O
%	O
)	O
.	O
The	O
right	O
part	O
of	O
the	O
paper	O
represents	O
the	O
Dev	O
set	O
results	O
.	O
For	O
the	O
feature	O
-	O
based	O
approach	O
,	O
we	O
concatenate	O
the	O
last	O
4	O
layers	O
of	O
BERT	B-MethodName
as	O
the	O
features	O
,	O
which	O
was	O
shown	O
to	O
be	O
the	O
best	O
approach	O
in	O
Section	O
5.3	O
.	O
From	O
the	O
table	O
it	O
can	O
be	O
seen	O
that	O
fine	O
-	O
tuning	O
is	O
surprisingly	O
robust	O
to	O
different	O
masking	O
strategies	O
.	O
However	O
,	O
as	O
expected	O
,	O
using	O
only	O
the	O
MASK	O
strategy	O
was	O
problematic	O
when	O
applying	O
the	O
featurebased	O
approach	O
to	O
NER	B-TaskName
.	O
Interestingly	O
,	O
using	O
only	O
the	O
RND	O
strategy	O
performs	O
much	O
worse	O
than	O
our	O
strategy	O
as	O
well	O
.	O

Language	O
Understanding	O
"	O
We	O
organize	O
the	O
appendix	O
into	O
three	O
sections	O
:	O
Additional	O
implementation	O
details	O
for	O
BERT	B-MethodName
are	O
presented	O
in	O
Appendix	O
A	O
;	O
Additional	O
details	O
for	O
our	O
experiments	O
are	O
presented	O
in	O
Appendix	O
B	O
;	O
and	O
Additional	O
ablation	O
studies	O
are	O
presented	O
in	O
Appendix	O
C.	O
We	O
present	O
additional	O
ablation	O
studies	O
for	O
BERT	B-MethodName
including	O
:	O
-	O
10	O
%	O
of	O
the	O
time	O
:	O
Replace	O
the	O
word	O
with	O
a	O
random	O
word	O
,	O
e.g.	O
,	O
my	O
dog	O
is	O
hairy	O
my	O
dog	O
is	O
apple	O
10	O
%	O
of	O
the	O
time	O
:	O
Keep	O
the	O
word	O
unchanged	O
,	O
e.g.	O
,	O
my	O
dog	O
is	O
hairy	O
my	O
dog	O
is	O
hairy	O
.	O
The	O
purpose	O
of	O
this	O
is	O
to	O
bias	O
the	O
representation	O
towards	O
the	O
actual	O
observed	O
word	O
.	O
The	O
advantage	O
of	O
this	O
procedure	O
is	O
that	O
the	O
Transformer	B-MethodName
encoder	O
does	O
not	O
know	O
which	O
words	O
it	O
will	O
be	O
asked	O
to	O
predict	O
or	O
which	O
have	O
been	O
replaced	O
by	O
random	O
words	O
,	O
so	O
it	O
is	O
forced	O
to	O
keep	O
a	O
distributional	O
contextual	O
representation	O
of	O
every	O
input	O
token	O
.	O
Additionally	O
,	O
because	O
random	O
replacement	O
only	O
occurs	O
for	O
1.5	O
%	O
of	O
all	O
tokens	O
(	O
i.e.	O
,	O
10	O
%	O
of	O
15	O
%	O
)	O
,	O
this	O
does	O
not	O
seem	O
to	O
harm	O
the	O
model	O
's	O
language	O
understanding	O
capability	O
.	O
In	O
Section	O
C.2	O
,	O
we	O
evaluate	O
the	O
impact	O
this	O
procedure	O
.	O
Compared	O
to	O
standard	O
langauge	O
model	O
training	O
,	O
the	O
masked	O
LM	O
only	O
make	O
predictions	O
on	O
15	O
%	O
of	O
tokens	O
in	O
each	O
batch	O
,	O
which	O
suggests	O
that	O
more	O
pre	O
-	O
training	O
steps	O
may	O
be	O
required	O
for	O
the	O
model	O

Often	O
each	O
section	O
of	O
a	O
novel	O
is	O
written	O
from	O
the	O
perspective	O
of	O
a	O
different	O
main	O
character	O
.	O
The	O
characters	O
each	O
take	O
turns	O
in	O
the	O
spot	O
-	O
light	O
,	O
with	O
their	O
own	O
parallel	O
storylines	O
being	O
unfolded	O
by	O
the	O
author	O
.	O
As	O
readers	O
,	O
we	O
have	O
often	O
desired	O
to	O
read	O
just	O
one	O
storyline	O
at	O
a	O
time	O
,	O
particularly	O
when	O
reading	O
the	O
book	O
a	O
second	O
-	O
time	O
.	O
In	O
this	O
paper	O
,	O
we	O
present	O
a	O
tool	O
,	O
NovelPerspective	O
,	O
to	O
give	O
the	O
consumer	O
this	O
choice	O
.	O
Our	O
tool	O
allows	O
the	O
consumer	O
to	O
select	O
which	O
characters	O
of	O
the	O
book	O
they	O
are	O
interested	O
in	O
,	O
and	O
to	O
generate	O
a	O
new	O
ebook	O
file	O
containing	O
just	O
the	O
sections	O
from	O
that	O
character	O
's	O
point	O
of	O
view	O
(	O
POV	O
)	O
.	O
The	O
critical	O
part	O
of	O
this	O
system	O
is	O
the	O
detection	O
of	O
the	O
POV	O
character	O
.	O
This	O
is	O
not	O
an	O
insurmountable	O
task	O
,	O
building	O
upon	O
the	O
well	O
established	O
field	O
of	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
.	O
However	O
to	O
our	O
knowl	O
-	O
edge	O
there	O
is	O
no	O
software	O
to	O
do	O
this	O
.	O
Such	O
a	O
tool	O
would	O
have	O
been	O
useless	O
,	O
in	O
decades	O
past	O
when	O
booked	O
were	O
distributed	O
only	O
on	O
paper	O
.	O
But	O
today	O
,	O
the	O
surge	O
in	O
popularity	O
of	O
ebooks	O
has	O
opened	O
a	O
new	O
niche	O
for	O
consumer	O
narrative	O
processing	O
.	O
Methods	O
are	O
being	O
created	O
to	O
extract	O
social	O
relationships	O
between	O
characters	O
(	O
Elson	O
et	O
al	O
,	O
2010	O
;	O
Wohlgenannt	O
et	O
al	O
,	O
2016	O
)	O
;	O
to	O
align	O
scenes	O
in	O
movies	O
with	O
those	O
from	O
books	O
(	O
Zhu	O
et	O
al	O
,	O
2015	O
)	O
;	O
and	O
to	O
otherwise	O
augment	O
the	O
literature	O
consumption	O
experience	O
.	O
Tools	O
such	O
as	O
the	O
one	O
presented	O
here	O
,	O
give	O
the	O
reader	O
new	O
freedoms	O
in	O
controlling	O
how	O
they	O
consume	O
their	O
media	O
.	O
Having	O
a	O
large	O
cast	O
of	O
characters	O
,	O
in	O
particular	O
POV	O
characters	O
,	O
is	O
a	O
hallmark	O
of	O
the	O
epic	O
fantasy	O
genre	O
.	O
Well	O
known	O
examples	O
include	O
:	O
George	O
R.R.	O
Martin	O
's	O
"	O
A	O
Song	O
of	O
Ice	O
and	O
Fire	O
"	O
,	O
Robert	O
Jordan	O
's	O
"	O
Wheel	O
of	O
Time	O
"	O
,	O
Brandon	O
Sanderson	O
's	O
"	O
Cosmere	O
"	O
universe	O
,	O
and	O
Steven	O
Erikson	O
's	O
"	O
Malazan	O
Book	O
of	O
the	O
Fallen	O
"	O
,	O
amongst	O
thousands	O
of	O
others	O
.	O
Generally	O
,	O
these	O
books	O
are	O
written	O
in	O
limited	O
third	O
-	O
person	O
POV	O
;	O
that	O
is	O
to	O
say	O
the	O
reader	O
has	O
little	O
or	O
no	O
more	O
knowledge	O
of	O
the	O
situation	O
described	O
than	O
the	O
main	O
character	O
does	O
.	O
We	O
focus	O
here	O
on	O
novels	O
written	O
in	O
the	O
limited	O
third	O
-	O
person	O
POV	O
.	O
In	O
these	O
stories	O
,	O
the	O
main	O
character	O
is	O
,	O
for	O
our	O
purposes	O
,	O
the	O
POV	O
character	O
.	O
Limited	O
third	O
-	O
person	O
POV	O
is	O
written	O
in	O
the	O
thirdperson	O
,	O
that	O
is	O
to	O
say	O
the	O
character	O
is	O
referred	O
to	O
by	O
name	O
,	O
but	O
with	O
the	O
observations	O
limited	O
to	O
being	O
from	O
the	O
perspective	O
of	O
that	O
character	O
.	O
This	O
is	O
in	O
-	O
contrast	O
to	O
the	O
omniscient	O
third	O
-	O
person	O
POV	O
,	O
where	O
events	O
are	O
described	O
by	O
an	O
external	O
narrator	O
.	O
Limited	O
third	O
-	O
person	O
POV	O
is	O
extremely	O
popular	O
in	O
modern	O
fiction	O
.	O
It	O
preserves	O
the	O
advantages	O
of	O
first	O
-	O
person	O
,	O
in	O
allowing	O
the	O
reader	O
to	O
observe	O
inside	O
the	O
head	O
of	O
the	O
character	O
,	O
while	O
also	O
allowing	O
the	O
flexibility	O
to	O
the	O
perspective	O
of	O
another	O
character	O
(	O
Booth	O
,	O
1961	O
)	O
.	O
This	O
allows	O
for	O
multiple	O
concurrent	O
storylines	O
around	O
different	O
characters	O
.	O
Our	O
tool	O
helps	O
users	O
un	O
-	O
entwine	O
such	O
storylines	O
,	O
giving	O
the	O
option	O
to	O
process	O
them	O
sequentially	O
.	O
The	O
utility	O
of	O
dividing	O
a	O
book	O
in	O
this	O
way	O
varies	O
with	O
the	O
book	O
in	O
question	O
.	O
Some	O
books	O
will	O
cease	O
to	O
make	O
sense	O
when	O
the	O
core	O
storyline	O
crosses	O
over	O
different	O
characters	O
.	O
Other	O
novels	O
,	O
particularly	O
in	O
epic	O
fantasy	O
genre	O
,	O
have	O
parallel	O
storylines	O
which	O
only	O
rarely	O
intersect	O
.	O
While	O
we	O
are	O
unable	O
to	O
find	O
a	O
formal	O
study	O
on	O
this	O
,	O
anecdotally	O
many	O
readers	O
speak	O
of	O
:	O
"	O
Skipping	O
the	O
chapters	O
about	O
the	O
boring	O
characters	O
.	O
"	O
"	O
Only	O
reading	O
the	O
real	O
main	O
character	O
's	O
sections	O
.	O
"	O
"	O
Reading	O
ahead	O
,	O
past	O
the	O
side	O
-	O
stories	O
,	O
to	O
get	O
on	O
with	O
the	O
main	O
plot	O
.	O
"	O
Particularly	O
if	O
they	O
have	O
read	O
the	O
story	O
before	O
,	O
and	O
thus	O
do	O
not	O
risk	O
confusion	O
.	O
Such	O
opinions	O
are	O
a	O
matter	O
of	O
the	O
consumer	O
's	O
personal	O
taste	O
.	O
The	O
Nov	O
-	O
elPerspective	O
tool	O
gives	O
the	O
reader	O
the	O
option	O
to	O
customise	O
the	O
book	O
in	O
this	O
way	O
,	O
according	O
to	O
their	O
personal	O
preference	O
.	O
We	O
note	O
that	O
sub	O
-	O
setting	O
the	O
novel	O
once	O
does	O
not	O
prevent	O
the	O
reader	O
from	O
going	O
back	O
and	O
reading	O
the	O
intervening	O
chapters	O
if	O
it	O
ceases	O
to	O
make	O
sense	O
,	O
or	O
from	O
sub	O
-	O
setting	O
again	O
to	O
get	O
the	O
chapters	O
for	O
another	O
character	O
whose	O
path	O
intersects	O
with	O
the	O
storyline	O
they	O
are	O
currently	O
reading	O
.	O
We	O
can	O
personally	O
attest	O
for	O
some	O
books	O
reading	O
the	O
chapters	O
one	O
character	O
at	O
a	O
time	O
is	O
indeed	O
possible	O
,	O
and	O
pleasant	O
:	O
the	O
first	O
author	O
of	O
this	O
paper	O
read	O
George	O
R.R.	O
Martin	O
's	O
"	O
A	O
Song	O
of	O
Ice	O
and	O
Fire	O
"	O
series	O
in	O
exactly	O
this	O
fashion	O
.	O
The	O
primary	O
difficulty	O
in	O
segmenting	O
ebooks	O
this	O
way	O
is	O
attributing	O
each	O
section	O
to	O
its	O
POV	O
character	O
.	O
That	O
is	O
to	O
say	O
detecting	O
who	O
is	O
the	O
point	O
of	O
view	O
character	O
.	O
Very	O
few	O
books	O
indicate	O
this	O
clearly	O
,	O
and	O
the	O
reader	O
is	O
expected	O
to	O
infer	O
it	O
during	O
reading	O
.	O
This	O
is	O
easy	O
for	O
most	O
humans	O
,	O
but	O
automating	O
it	O
is	O
a	O
challenge	O
.	O
To	O
solve	O
this	O
,	O
the	O
core	O
of	O
our	O
tool	O
is	O
its	O
character	O
classification	O
system	O
.	O
We	O
investigated	O
several	O
options	O
which	O
the	O
main	O
text	O
of	O
this	O
paper	O
will	O
discuss	O
.	O

The	O
full	O
NovelPerspective	O
pipeline	O
is	O
shown	O
in	O
Figure	O
1	O
.	O
The	O
core	O
character	O
classification	O
step	O
(	O
step	O
3	O
)	O
,	O
is	O
detailed	O
in	O
Figure	O
2	O
.	O
In	O
this	O
step	O
the	O
raw	O
text	O
is	O
first	O
enriched	O
with	O
parts	O
of	O
speech	O
,	O
and	O
named	O
entity	O
tags	O
.	O
We	O
do	O
not	O
perform	O
coreference	B-TaskName
resolution	I-TaskName
,	O
working	O
only	O
with	O
direct	O
entity	O
mentions	O
.	O
From	O
this	O
,	O
features	O
are	O
extracted	O
for	O
each	O
named	O
entity	O
.	O
These	O
feature	O
vectors	O
are	O
used	O
to	O
score	O
the	O
entities	O
for	O
the	O
most	O
-	O
likely	O
POV	O
character	O
.	O
The	O
highest	O
scoring	O
character	O
is	O
returned	O
by	O
the	O
system	O
.	O
The	O
different	O
systems	O
presented	O
modify	O
the	O
Feature	O
Extraction	O
and	O
Character	O
Scoring	O
steps	O
.	O
A	O
broadly	O
similar	O
idea	O
,	O
for	O
detecting	O
the	O
focus	O
location	O
of	O
news	O
articles	O
,	O
was	O
presented	O
by	O
(	O
Imani	O
et	O
al	O
,	O
2017	O
)	O
.	O

One	O
can	O
see	O
the	O
determination	O
of	O
the	O
main	O
character	O
as	O
a	O
multi	B-TaskName
-	I-TaskName
class	I-TaskName
classification	I-TaskName
problem	O
.	O
From	O
the	O
set	O
of	O
all	O
named	O
entities	O
in	O
the	O
section	O
,	O
classify	O
that	O
section	O
as	O
to	O
which	O
one	O
is	O
the	O
main	O
character	O
.	O
Unlike	O
typical	O
multi	B-TaskName
-	I-TaskName
class	I-TaskName
classification	I-TaskName
problems	O
the	O
set	O
of	O
possible	O
classes	O
varies	O
per	O
section	O
being	O
classified	O
.	O
Further	O
,	O
even	O
the	O
total	O
set	O
of	O
possible	O
named	O
characters	O
,	O
i.e.	O
classes	O
,	O
varies	O
from	O
book	O
to	O
book	O
.	O
An	O
information	O
extraction	O
approach	O
is	O
required	O
which	O
can	O
handle	O
these	O
varying	O
classes	O
.	O
As	O
such	O
,	O
a	O
machine	O
learning	O
model	O
for	O
this	O
task	O
can	O
not	O
incorporate	O
direct	O
knowledge	O
of	O
the	O
classes	O
(	O
i.e.	O
character	O
names	O
)	O
.	O
We	O
reconsider	O
the	O
problem	O
as	O
a	O
series	O
of	O
binary	O
predictions	O
.	O
The	O
task	O
is	O
to	O
predict	O
if	O
a	O
given	O
named	O
entity	O
is	O
the	O
point	O
of	O
view	O
character	O
.	O
For	O
each	O
possible	O
character	O
(	O
i.e.	O
each	O
named	O
-	O
entity	O
that	O
occurs	O
)	O
,	O
a	O
feature	O
vector	O
is	O
extracted	O
(	O
see	O
Section	O
2.2.1	O
)	O
.	O
This	O
feature	O
vector	O
is	O
the	O
input	O
to	O
a	O
binary	O
classifier	O
,	O
which	O
determines	O
the	O
probability	O
that	O
it	O
represents	O
the	O
main	O
character	O
.	O
The	O
Character	O
Scoring	O
step	O
is	O
thus	O
the	O
running	O
of	O
the	O
binary	O
classifier	O
:	O
the	O
score	O
is	O
the	O
output	O
probability	O
normalised	O
over	O
all	O
the	O
named	O
entities	O
.	O

We	O
investigated	O
two	O
feature	O
sets	O
as	O
inputs	O
for	O
our	O
machine	O
learning	O
-	O
based	O
solution	O
.	O
They	O
correspond	O
to	O
different	O
Feature	O
Extraction	O
steps	O
in	O
Figure	O
2	O
.	O
A	O
hand	O
-	O
engineered	O
feature	O
set	O
,	O
that	O
we	O
call	O
the	O
"	O
Classical	O
"	O
feature	O
set	O
;	O
and	O
a	O
more	O
modern	O
"	O
Word	O
Embedding	O
"	O
feature	O
set	O
.	O
Both	O
feature	O
sets	O
give	O
information	O
about	O
how	O
the	O
each	O
named	O
entity	O
token	O
was	O
used	O
in	O
the	O
text	O
.	O
The	O
"	O
Classical	O
"	O
feature	O
set	O
uses	O
features	O
that	O
are	O
well	O
established	O
in	O
NLP	O
related	O
tasks	O
.	O
The	O
features	O
can	O
be	O
described	O
as	O
positional	O
features	O
,	O
like	O
in	O
the	O
First	O
Mentioned	O
baseline	O
;	O
occurrence	O
count	O
features	O
,	O
like	O
in	O
the	O
Most	O
Mentioned	O
baseline	O
and	O
adjacent	O
POS	O
counts	O
,	O
to	O
give	O
usage	O
context	O
.	O
The	O
positional	O
features	O
are	O
the	O
index	O
(	O
in	O
the	O
token	O
counts	O
)	O
of	O
the	O
first	O
and	O
last	O
occurrence	O
of	O
the	O
named	O
entity	O
.	O
The	O
occurrence	O
count	O
features	O
are	O
simply	O
the	O
number	O
of	O
occurrences	O
of	O
the	O
named	O
entity	O
,	O
supplemented	O
with	O
its	O
rank	O
on	O
that	O
count	O
compared	O
to	O
the	O
others	O
.	O
The	O
adjacent	O
POS	O
counts	O
are	O
the	O
occurrence	O
counts	O
of	O
each	O
of	O
the	O
46	O
POS	O
tags	O
on	O
the	O
word	O
prior	O
to	O
the	O
named	O
entity	O
,	O
and	O
on	O
the	O
word	O
after	O
.	O
We	O
theorised	O
that	O
this	O
POS	O
information	O
would	O
be	O
informative	O
,	O
as	O
it	O
seemed	O
reasonable	O
that	O
the	O
POV	O
character	O
would	O
be	O
described	O
as	O
doing	O
more	O
things	O
,	O
so	O
co	O
-	O
occurring	O
with	O
more	O
verbs	O
.	O
This	O
gives	O
100	O
base	O
features	O
.	O
To	O
allow	O
for	O
text	O
length	O
invariance	O
we	O
also	O
provide	O
each	O
of	O
the	O
base	O
features	O
expressed	O
as	O
a	O
portion	O
of	O
its	O
maximum	O
possible	O
value	O
(	O
e.g.	O
for	O
a	O
given	O
POS	O
tag	O
occurring	O
before	O
a	O
named	O
entity	O
,	O
the	O
potion	O
of	O
times	O
this	O
tag	O
occurred	O
)	O
.	O
This	O
gives	O
a	O
total	O
of	O
200	O
features	O
.	O
The	O
"	O
Word	O
Embedding	O
"	O
feature	O
set	O
uses	O
Fast	O
-	O
Text	O
word	O
vectors	O
(	O
Bojanowski	O
et	O
al	O
,	O
2017	O
)	O
.	O
We	O
use	O
the	O
pretrained	O
300	O
dimensional	O
embeddings	O
trained	O
on	O
English	O
Wikipedia	O
1	O
.	O
We	O
concatenate	O
the	O
300	O
dimensional	O
word	O
embedding	O
for	O
the	O
word	O
immediately	O
prior	O
to	O
,	O
and	O
immediately	O
after	O
each	O
occurrence	O
of	O
a	O
named	O
entity	O
;	O
and	O
take	O
the	O
element	O
-	O
wise	O
mean	O
of	O
this	O
concatenated	O
vector	O
over	O
all	O
occurrences	O
of	O
the	O
entity	O
.	O
Such	O
averages	O
of	O
word	B-TaskName
embeddings	I-TaskName
have	O
been	O
shown	O
to	O
be	O
a	O
useful	O
feature	O
in	O
many	O
tasks	O
(	O
White	O
et	O
al	O
,	O
2015	O
;	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
.	O
This	O
has	O
a	O
total	O
of	O
600	O
features	O
.	O

The	O
binary	O
classifier	O
,	O
that	O
predicts	O
if	O
a	O
named	O
entity	O
is	O
the	O
main	O
character	O
,	O
is	O
the	O
key	O
part	O
of	O
the	O
Character	O
Scoring	O
step	O
for	O
the	O
machine	O
learning	O
systems	O
.	O
From	O
each	O
text	O
in	O
the	O
training	O
dataset	O
we	O
generated	O
a	O
training	O
example	O
for	O
every	O
named	O
entity	O
that	O
occurred	O
.	O
All	O
but	O
one	O
of	O
these	O
was	O
a	O
negative	O
example	O
.	O
We	O
then	O
trained	O
it	O
as	O
per	O
normal	O
for	O
a	O
binary	O
classifier	O
.	O
The	O
score	O
for	O
a	O
character	O
is	O
the	O
classifier	O
's	O
predicted	O
probability	O
of	O
its	O
feature	O
vector	O
being	O
for	O
the	O
main	O
character	O
.	O
Our	O
approach	O
of	O
using	O
a	O
binary	O
classifier	O
to	O
rate	O
each	O
possible	O
class	O
,	O
may	O
seem	O
similar	O
to	O
the	O
one	O
-	O
vs	O
-	O
rest	O
approach	O
for	O
multi	B-TaskName
-	I-TaskName
class	I-TaskName
classification	I-TaskName
.	O
However	O
,	O
there	O
is	O
an	O
important	O
difference	O
.	O
Our	O
system	O
only	O
uses	O
a	O
single	O
binary	O
classifier	O
;	O
not	O
one	O
classifier	O
per	O
class	O
,	O
as	O
the	O
classes	O
in	O
our	O
case	O
vary	O
with	O
every	O
item	O
to	O
be	O
classified	O
.	O
The	O
fundamental	O
problem	O
is	O
information	O
extraction	O
,	O
and	O
the	O
classifier	O
is	O
a	O
tool	O
for	O
the	O
scoring	O
which	O
is	O
the	O
correct	O
information	O
to	O
report	O
.	O
With	O
the	O
classical	O
feature	O
set	O
we	O
use	O
logistic	B-MethodName
regression	I-MethodName
,	O
with	O
the	O
features	O
being	O
preprocessed	O
with	O
0	B-DatasetName
-	O
1	O
scaling	O
.	O
During	O
preliminary	O
testing	O
we	O
found	O
that	O
many	O
classifiers	O
had	O
similar	O
high	O
degree	O
of	O
success	O
,	O
and	O
so	O
chose	O
the	O
simplest	O
.	O
With	O
the	O
word	O
embedding	O
feature	O
set	O
we	O
used	O
a	O
radial	O
bias	O
support	B-MethodName
vector	I-MethodName
machine	I-MethodName
,	O
with	O
standardisation	O
during	O
preprocessing	O
,	O
as	O
has	O
been	O
commonly	O
used	O
with	O
word	B-TaskName
embeddings	I-TaskName
on	O
other	O
tasks	O
.	O

We	O
make	O
use	O
of	O
three	O
series	O
of	O
books	O
selected	O
from	O
our	O
own	O
personal	O
collections	O
.	O
The	O
first	O
four	O
books	O
of	O
George	O
R.	O
R.	O
Martin	O
's	O
"	O
A	O
Song	O
of	O
Ice	O
and	O
Fire	O
"	O
series	O
(	O
hereafter	O
referred	O
to	O
as	O
ASOIAF	O
)	O
;	O
The	O
two	O
books	O
of	O
Leigh	O
Bardugo	O
's	O
"	O
Six	O
of	O
Crows	O
"	O
duology	O
(	O
hereafter	O
referred	O
to	O
as	O
SOC	B-DatasetName
)	O
;	O
and	O
the	O
first	O
9	O
volumes	O
of	O
Robert	O
Jordan	O
's	O
"	O
Wheel	O
of	O
Time	O
"	O
series	O
(	O
hereafter	O
referred	O
to	O
as	O
WOT	O
)	O
.	O
In	O
Section	O
4	O
we	O
consider	O
the	O
use	O
of	O
each	O
as	O
a	O
training	O
and	O
testing	O
dataset	O
.	O
In	O
the	O
online	O
demonstration	O
(	O
Section	O
5	O
)	O
,	O
we	O
deploy	O
models	O
trained	O
on	O
the	O
combined	O
total	O
of	O
all	O
the	O
datasets	O
.	O
To	O
use	O
a	O
book	O
for	O
the	O
training	O
and	O
evaluation	O
of	O
our	O
system	O
,	O
we	O
require	O
a	O
ground	O
truth	O
for	O
each	O
section	O
's	O
POV	O
character	O
.	O
ASOIAF	O
and	O
SOC	B-DatasetName
provide	O
ground	O
truth	O
for	O
the	O
main	O
character	O
in	O
the	O
chapter	O
names	O
.	O
Every	O
chapter	O
only	O
uses	O
the	O
POV	O
of	O
that	O
named	O
character	O
.	O
WOT	O
's	O
ground	O
truth	O
comes	O
from	O
an	O
index	O
created	O
by	O
readers	O
.	O
2	O
We	O
do	O
not	O
have	O
any	O
datasets	O
with	O
labelled	O
sub	O
-	O
chapter	O
sections	O
,	O
though	O
the	O
tool	O
does	O
support	O
such	O
works	O
.	O
The	O
total	O
counts	O
of	O
chapters	O
and	O
characters	O
in	O
the	O
datasets	O
,	O
after	O
preprocessing	O
,	O
is	O
shown	O
in	O
Table	O
1	O
.	O
Preprocessing	O
consisted	O
of	O
discarding	O
chapters	O
for	O
which	O
the	O
POV	O
character	O
was	O
not	O
identified	O
(	O
e.g.	O
prologues	O
)	O
;	O
and	O
of	O
removing	O
the	O
character	O
names	O
from	O
the	O
chapter	O
titles	O
as	O
required	O
.	O

We	O
have	O
presented	O
a	O
tool	O
to	O
allow	O
consumers	O
to	O
restructure	O
their	O
ebooks	O
around	O
the	O
characters	O
they	O
find	O
most	O
interesting	O
.	O
The	O
system	O
must	O
discover	O
the	O
named	O
entities	O
that	O
are	O
present	O
in	O
each	O
section	O
of	O
the	O
book	O
,	O
and	O
then	O
classify	O
each	O
section	O
as	O
to	O
which	O
character	O
's	O
point	O
of	O
view	O
the	O
section	O
is	O
narrated	O
from	O
.	O
For	O
named	O
entity	O
detection	O
we	O
make	O
use	O
of	O
standard	O
tools	O
.	O
However	O
,	O
the	O
classification	O
is	O
non	O
-	O
trivial	O
.	O
In	O
this	O
design	O
we	O
implemented	O
several	O
systems	O
.	O
Simply	O
selecting	O
the	O
most	O
commonly	O
named	O
character	O
proved	O
successful	O
as	O
a	O
baseline	O
approach	O
.	O
To	O
improve	O
upon	O
this	O
,	O
we	O
developed	O
several	O
machine	O
learning	O
based	O
approaches	O
which	O
perform	O
very	O
well	O
.	O
While	O
none	O
of	O
the	O
classifiers	O
are	O
perfect	O
,	O
they	O
achieve	O
high	O
enough	O
accuracy	B-MetricName
to	O
be	O
useful	O
.	O
A	O
future	O
version	O
of	O
our	O
application	O
will	O
allow	O
the	O
users	O
to	O
submit	O
corrections	O
,	O
giving	O
us	O
more	O
training	O
data	O
.	O
However	O
,	O
storing	O
this	O
information	O
poses	O
copyright	O
issues	O
that	O
are	O
yet	O
to	O
be	O
resolved	O
.	O

Evaluation	O
of	O
Scientific	O
Elements	O
for	O
Text	B-TaskName
Similarity	I-TaskName
in	O
Biomedical	O
Publications	O

Rhetorical	O
elements	O
from	O
scientific	O
publications	O
provide	O
a	O
more	O
structured	O
view	O
of	O
the	O
document	O
and	O
allow	O
algorithms	O
to	O
focus	O
on	O
particular	O
parts	O
of	O
the	O
text	O
.	O
We	O
surveyed	O
the	O
literature	O
for	O
previously	O
proposed	O
schemes	O
for	O
rhetorical	O
elements	O
and	O
present	O
an	O
overview	O
of	O
its	O
current	O
state	O
of	O
the	O
art	O
.	O
We	O
also	O
searched	O
for	O
available	O
tools	O
using	O
these	O
schemes	O
and	O
applied	O
four	O
tools	O
for	O
our	O
particular	O
task	O
of	O
ranking	O
biomedical	O
abstracts	O
based	O
on	O
text	B-TaskName
similarity	I-TaskName
.	O
Comparison	O
of	O
the	O
tools	O
with	O
two	O
strong	O
baselines	O
shows	O
that	O
the	O
predictions	O
provided	O
by	O
the	O
ArguminSci	O
tool	O
can	O
support	O
our	O
use	O
case	O
of	O
mining	O
alternative	O
methods	O
for	O
animal	O
experiments	O
.	O

We	O
aim	O
to	O
mine	O
alternative	O
methods	O
to	O
animal	O
experiments	O
from	O
the	O
biomedical	O
literature	O
.	O
These	O
are	O
methods	O
that	O
address	O
any	O
of	O
the	O
so	O
-	O
called	O
3R	O
principles	O
of	O
replacement	O
(	O
no	O
animals	O
at	O
all	O
or	O
use	O
of	O
invertebrates	O
over	O
vertebrates	O
)	O
,	O
reduce	O
(	O
use	O
of	O
less	O
animals	O
)	O
,	O
or	O
refinement	O
(	O
cause	O
less	O
harm	O
to	O
animals	O
)	O
(	O
Gruber	O
and	O
Hartung	O
,	O
2004	O
;	O
Doke	O
and	O
Dhawale	O
,	O
2015	O
)	O
.	O
For	O
such	O
complex	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
applications	O
,	O
it	O
is	O
necessary	O
to	O
rely	O
on	O
appropriate	O
tools	O
to	O
precisely	O
understand	O
the	O
text	O
and	O
better	O
find	O
the	O
potential	O
relevant	O
documents	O
.	O
The	O
rhetorical	O
elements	O
,	O
such	O
as	O
zones	O
or	O
particular	O
entities	O
,	O
can	O
support	O
NLP	O
algorithms	O
by	O
focusing	O
on	O
the	O
relevant	O
elements	O
of	O
the	O
text	O
(	O
Mann	O
and	O
Thompson	O
,	O
1987	O
)	O
.	O
Given	O
a	O
certain	O
document	O
that	O
describes	O
an	O
animal	O
experiment	O
for	O
a	O
certain	O
research	O
goal	O
,	O
hereafter	O
called	O
input	O
document	O
,	O
we	O
would	O
like	O
to	O
find	O
potential	O
publications	O
,	O
hereafter	O
called	O
candidate	O
documents	O
,	O
that	O
describe	O
an	O
alternative	O
method	O
for	O
the	O
same	O
research	O
goal	O
.	O
Thus	O
,	O
some	O
of	O
the	O
scientific	O
elements	O
should	O
be	O
similar	O
between	O
input	O
and	O
candidate	O
documents	O
,	O
e.g.	O
research	O
goals	O
and	O
outcomes	O
,	O
while	O
some	O
others	O
should	O
be	O
different	O
,	O
e.g.	O
methods	O
.	O
Finding	O
an	O
alternative	O
method	O
to	O
animal	O
experiment	O
requires	O
two	O
tasks	O
:	O
(	O
a	O
)	O
performing	O
a	O
text	B-TaskName
similarity	I-TaskName
task	O
with	O
respect	O
to	O
some	O
aspects	O
of	O
the	O
publication	O
,	O
and	O
(	O
b	O
)	O
precisely	O
understanding	O
the	O
proposed	O
method	O
with	O
respect	O
to	O
the	O
3R	O
principles	O
.	O
Therefore	O
,	O
the	O
extraction	O
of	O
rhetorical	O
elements	O
has	O
the	O
potential	O
to	O
boost	O
performance	O
for	O
these	O
tasks	O
.	O
Previous	O
works	O
have	O
proposed	O
many	O
schemes	O
for	O
rhetorical	O
elements	O
in	O
scientific	O
publication	O
,	O
as	O
reviewed	O
in	O
Webber	O
et	O
al	O
(	O
2012	O
)	O
.	O
In	O
a	O
more	O
recent	O
survey	O
,	O
Nasar	O
et	O
al	O
(	O
2018	O
)	O
present	O
a	O
good	O
overview	O
on	O
both	O
metadata	O
and	O
schemes	O
for	O
scientific	O
articles	O
.	O
On	O
the	O
one	O
hand	O
,	O
many	O
of	O
these	O
schemes	O
are	O
not	O
supported	O
by	O
an	O
annotated	O
corpus	O
for	O
training	O
suitable	O
information	O
extraction	O
tools	O
.	O
On	O
the	O
other	O
hand	O
,	O
some	O
tools	O
based	O
on	O
these	O
schemes	O
are	O
readily	O
available	O
for	O
use	O
.	O
We	O
surveyed	O
published	O
schemes	O
for	O
rhetorical	O
elements	O
,	O
whether	O
focused	O
on	O
the	O
biomedical	O
domain	O
or	O
not	O
,	O
and	O
we	O
present	O
a	O
short	O
overview	O
on	O
these	O
.	O
For	O
those	O
schemes	O
for	O
which	O
we	O
could	O
find	O
available	O
tools	O
,	O
the	O
latter	O
was	O
used	O
to	O
process	O
a	O
collection	O
of	O
562	O
biomedical	O
abstracts	O
.	O
We	O
performed	O
a	O
comparison	O
of	O
the	O
output	O
(	O
rhetorical	O
elements	O
)	O
from	O
the	O
tools	O
in	O
the	O
scope	O
of	O
a	O
text	B-TaskName
similarity	I-TaskName
task	O
on	O
a	O
manually	O
annotated	O
dataset	O
.	O
In	O
this	O
work	O
,	O
we	O
limited	O
our	O
evaluation	O
for	O
text	B-TaskName
similarity	I-TaskName
but	O
did	O
not	O
address	O
whether	O
the	O
proposed	O
methods	O
comply	O
with	O
the	O
3R	O
principles	O
.	O
In	O
summary	O
,	O
the	O
contributions	O
of	O
this	O
work	O
are	O
the	O
following	O
:	O
(	O
a	O
)	O
a	O
short	O
survey	O
on	O
existing	O
schemes	O
and	O
corpora	O
for	O
rhetorical	O
elements	O
in	O
scientific	O
publications	O
;	O
(	O
b	O
)	O
the	O
identification	O
of	O
the	O
schemes	O
for	O
which	O
available	O
tools	O
are	O
readily	O
available	O
for	O
use	O
;	O
and	O
(	O
c	O
)	O
the	O
evaluation	O
of	O
the	O
available	O
tools	O
on	O
a	O
biomedical	O
use	O
case	O
for	O
text	B-TaskName
similarity	I-TaskName
.	O
The	O
next	O
section	O
presents	O
a	O
survey	O
on	O
the	O
available	O
schemes	O
,	O
followed	O
by	O
the	O
methodology	O
that	O
we	O
propose	O
to	O
compare	O
the	O
tools	O
in	O
the	O
scope	O
of	O
text	B-TaskName
similarity	I-TaskName
.	O
We	O
present	O
the	O
results	O
in	O
Section	O
4	O
and	O
our	O
discussion	O
in	O
Section	O
5	O
.	O

Many	O
schemes	O
model	O
scientific	O
elements	O
on	O
the	O
level	O
of	O
sentences	O
or	O
phrases	O
,	O
i.e.	O
,	O
for	O
document	O
zoning	O
.	O
It	O
consists	O
of	O
splitting	O
the	O
publications	O
(	O
whether	O
abstracts	O
or	O
full	O
texts	O
)	O
on	O
zones	O
according	O
to	O
its	O
scientific	O
content	O
,	O
e.g.	O
introduction	O
,	O
methods	O
,	O
results	O
.	O
Shimbo	O
et	O
al	O
(	O
2003	O
)	O
proposed	O
five	O
categories	O
and	O
used	O
structured	O
abstracts	O
from	O
Medline	O
while	O
Hirohata	O
et	O
al	O
(	O
2008	O
)	O
suggested	O
four	O
zoning	O
categories	O
.	O
Further	O
,	O
Mullen	O
et	O
al	O
(	O
2005	O
)	O
proposed	O
a	O
schema	O
in	O
which	O
labels	O
are	O
grouped	O
in	O
three	O
groups	O
.	O
Agarwal	O
and	O
Yu	O
(	O
2009	O
)	O
defined	O
four	O
categories	O
(	O
IMRAD	O
schema	O
)	O
and	O
manually	O
annotated	O
148	O
articles	O
,	O
which	O
was	O
also	O
used	O
by	O
Varga	O
et	O
al	O
(	O
2012	O
)	O
for	O
the	O
annotation	O
of	O
more	O
than	O
1	O
,	O
000	O
biomedical	O
articles	O
.	O
Ruch	O
et	O
al	O
(	O
2007	O
)	O
also	O
annotated	O
and	O
tried	O
machine	O
learning	O
in	O
biomedical	O
abstracts	O
.	O
However	O
,	O
none	O
of	O
the	O
above	O
data	O
seems	O
to	O
be	O
available	O
for	O
use	O
,	O
but	O
we	O
found	O
many	O
schemes	O
with	O
available	O
corpora	O
:	O
AZ	O
(	O
Teufel	O
and	O
Moens	O
,	O
2002	O
)	O
.	O
The	O
Argumentative	O
Zoning	O
(	O
AZ	O
)	O
schema	O
was	O
first	O
proposed	O
by	O
Teufel	O
and	O
Moens	O
(	O
2002	O
)	O
and	O
an	O
annotated	O
corpus	O
is	O
freely	O
available	O
for	O
download	O
1	O
.	O
The	O
schema	O
is	O
composed	O
of	O
seven	O
rhetorical	O
categories	O
and	O
the	O
corresponding	O
corpus	O
contains	O
80	O
articles	O
on	O
computational	O
linguistics	O
.	O
Teufel	O
et	O
al	O
(	O
2009	O
)	O
extended	O
the	O
schema	O
to	O
11	O
categories	O
(	O
the	O
AZ	O
-	O
II	O
schema	O
)	O
,	O
applied	O
it	O
to	O
chemistry	O
papers	O
,	O
and	O
later	O
compared	O
it	O
to	O
the	O
CoreSC	O
schema	O
)	O
.	O
2	O
Later	O
,	O
Kovačević	O
et	O
al	O
(	O
2012	O
)	O
annotated	O
110	O
articles	O
in	O
computational	O
linguistics	O
with	O
a	O
modified	O
version	O
of	O
the	O
AZ	O
labels	O
.	O
Mizuta	O
et	O
al	O
(	O
2006	O
)	O
also	O
adapted	O
the	O
AZ	O
schema	O
to	O
biomedicine	O
by	O
annotating	O
20	O
full	O
-	O
text	O
articles	O
.	O
CoreSC	O
.	O
This	O
schema	O
consists	O
of	O
three	O
layers	O
of	O
labels	O
and	O
the	O
corresponding	O
ART	O
corpus	O
4	O
is	O
composed	O
of	O
225	O
full	O
texts	O
.	O
The	O
corpus	O
and	O
schema	O
were	O
used	O
in	O
Guo	O
et	O
al	O
(	O
2010	O
)	O
(	O
just	O
the	O
first	O
layer	O
)	O
and	O
in	O
Liakata	O
et	O
al	O
(	O
2012a	O
)	O
for	O
two	O
life	O
sciences	O
applications	O
,	O
while	O
Liakata	O
et	O
al	O
(	O
2012b	O
)	O
compared	O
it	O
to	O
a	O
schema	O
for	O
biomedical	O
events	O
and	O
developed	O
the	O
the	O
SAPIENTA	O
software	O
5	O
.	O
Dr.	O
Inventor	O
Fisas	O
et	O
al	O
,	O
2015	O
)	O
.	O
The	O
Dr.	O
Inventor	O
Framework	O
proposes	O
five	O
categories	O
and	O
annotated	O
40	O
Computer	O
Graphics	O
papers	O
,	O
the	O
so	O
-	O
called	O
Dr.	O
Inventor	O
Rhetorically	O
Annotated	O
Corpus	O
.	O
Later	O
,	O
they	O
also	O
annotated	O
another	O
layer	O
for	O
citation	O
purposes	O
(	O
Fisas	O
et	O
al	O
,	O
2016	O
)	O
.	O
An	O
extension	O
of	O
this	O
schema	O
with	O
argumentative	O
components	O
and	O
relations	O
was	O
recently	O
published	O
(	O
Lauscher	O
et	O
al	O
,	O
2018b	O
)	O
,	O
along	O
with	O
a	O
tool	O
for	O
the	O
prediction	O
of	O
the	O
scientific	O
elements	O
(	O
Lauscher	O
et	O
al	O
,	O
2018a	O
)	O
.	O
MAZEA	O
(	O
Dayrell	O
et	O
al	O
,	O
2012	O
)	O
.	O
This	O
schema	O
considers	O
six	O
categories	O
and	O
the	O
corpus	O
was	O
annotated	O
for	O
645	O
abstracts	O
from	O
Physical	O
Sciences	O
and	O
Engineering	O
and	O
Life	O
and	O
Health	O
Sciences	O
.	O
6	O
A	O
Web	O
application	O
is	O
available	O
for	O
tagging	O
abstracts	O
.	O
PIBOSO	O
(	O
Kim	O
et	O
al	O
,	O
2011	O
)	O
.	O
It	O
was	O
designed	O
for	O
the	O
clinical	O
domain	O
and	O
proposes	O
six	O
categories	O
of	O
a	O
modified	O
version	O
of	O
the	O
PICO	O
criteria	O
.	O
It	O
was	O
used	O
for	O
the	O
ALTA	O
-	O
NICTA	O
shared	O
task	O
7	O
and	O
recent	O
works	O
using	O
this	O
corpus	O
include	O
Hassanzadeh	O
et	O
al	O
(	O
2014	O
)	O
and	O
Jin	O
and	O
Szolovits	O
(	O
2018	O
)	O
.	O
The	O
latter	O
relies	O
on	O
deep	O
learning	O
methods	O
and	O
the	O
implementation	O
is	O
readily	O
available	O
.	O
PubMed	B-DatasetName
RCT	I-DatasetName
(	O
Dernoncourt	O
and	O
Lee	O
,	O
2017	O
)	O
.	O
It	O
is	O
a	O
collection	O
that	O
includes	O
two	O
corpora	O
of	O
20	O
,	O
000	O
and	O
200	O
,	O
000	O
medical	O
abstracts	O
annotated	O
(	O
Green	O
,	O
2018	O
)	O
bio	O
CL	O
Hybrid	O
Green	O
[	O
Levels	O
1	O
-	O
3	O
]	O
1	O
.	O
Causation	O
,	O
1.1	O
One	O
Group	O
,	O
1.1.1	O
Agreement	O
Argu	O
-	O
ments	O
,	O
1.1.2	O
Eliminate	O
Candidates	O
,	O
1.1.3	O
Explanation	O
-	O
Based	O
,	O
1.2	O
Two	O
Group	O
,	O
1.2.1	O
Difference	O
,	O
1.2.2	O
Analogy	O
(	O
Causal	O
)	O
,	O
1.2.3	O
Explanation	O
-	O
Based	O
,	O
2	O
.	O
Other	O
,	O
2.1	O
Classification	B-TaskName
,	O
2.2	O
Confirma	O
-	O
tion	O
one	O
Table	O
1	O
:	O
Summary	O
of	O
the	O
selected	O
schemes	O
and	O
corresponding	O
categories	O
,	O
size	O
of	O
the	O
annotated	O
corpora	O
,	O
and	O
topic	O
of	O
the	O
latter	O
.	O
Only	O
the	O
categories	O
from	O
the	O
certain	O
levels	O
were	O
shown	O
for	O
some	O
schemes	O
with	O
various	O
layers	O
.	O
Numbers	O
or	O
the	O
corpora	O
refer	O
to	O
full	O
-	O
text	O
documents	O
,	O
unless	O
otherwise	O
stated	O
.	O
Regarding	O
the	O
topics	O
,	O
"	O
CL	O
"	O
stands	O
for	O
computational	O
linguistics	O
,	O
"	O
bio	O
"	O
for	O
biomedicine	O
,	O
"	O
chem	O
"	O
for	O
chemistry	O
,	O
"	O
CG	O
"	O
for	O
Computer	O
Graphics	O
,	O
"	O
phy	O
"	O
for	O
Physics	O
,	O
"	O
eng	O
"	O
for	O
Engineering	O
,	O
"	O
LS	O
"	O
for	O
Life	O
Sciences	O
,	O
and	O
"	O
CS	B-DatasetName
"	O
for	O
Computer	O
Science	O
.	O
with	O
five	O
categories	O
.	O
The	O
corpus	O
is	O
freely	O
available	O
8	O
as	O
well	O
as	O
at	O
least	O
two	O
tools	O
for	O
its	O
detection	O
,	O
namely	O
the	O
one	O
from	O
Jin	O
and	O
Szolovits	O
(	O
2018	O
)	O
(	O
cf	O
.	O
PIBOSO	O
above	O
)	O
and	O
one	O
based	O
on	O
AllenNLP	O
(	O
Achakulvisut	O
et	O
al	O
,	O
2018	O
)	O
.	O
Wilbur	O
(	O
Wilbur	O
et	O
al	O
,	O
2006	O
)	O
.	O
It	O
consists	O
of	O
a	O
schema	O
developed	O
for	O
biomedical	O
articles	O
on	O
five	O
dimensions	O
.	O
Later	O
,	O
the	O
authors	O
annotated	O
10	O
,	O
000	O
sentences	O
from	O
full	O
-	O
text	O
publications	O
(	O
Shatkay	O
et	O
al	O
,	O
2008	O
)	O
,	O
which	O
was	O
made	O
available	O
after	O
a	O
detailed	O
analysis	O
(	O
Rzhetsky	O
et	O
al	O
,	O
2009	O
)	O
.	O
9	O
The	O
annotation	O
are	O
on	O
the	O
level	O
of	O
fragments	O
,	O
which	O
usually	O
correspond	O
to	O
either	O
the	O
sentences	O
or	O
phrases	O
.	O

Entity	O
-	O
level	O
schemes	O
aim	O
at	O
annotating	O
the	O
elements	O
on	O
the	O
level	O
of	O
entities	O
.	O
Gupta	O
and	O
Manning	O
(	O
2011	O
)	O
proposed	O
a	O
simple	O
schema	O
based	O
on	O
three	O
concepts	O
and	O
labeled	O
474	O
abstracts	O
of	O
computational	O
linguistics	O
.	O
More	O
recently	O
,	O
Jung	O
(	O
2017	O
)	O
defined	O
five	O
entity	O
types	O
and	O
annotated	O
1	O
,	O
000	O
articles	O
about	O
information	O
and	O
communication	O
technology	O
(	O
ICT	O
)	O
and	O
chemical	O
engineering	O
.	O
Blake	O
(	O
2010	O
)	O
also	O
proposed	O
a	O
schema	O
based	O
on	O
various	O
levels	O
of	O
evidence	O
(	O
implicit	O
and	O
explicit	O
claims	O
)	O
and	O
annotated	O
29	O
full	O
-	O
text	O
biomedical	O
articles	O
.	O
However	O
,	O
none	O
of	O
the	O
above	O
data	O
seems	O
to	O
be	O
available	O
but	O
we	O
found	O
one	O
schema	O
with	O
annotated	O
corpus	O
:	O
ScienceIE	B-DatasetName
(	O
Augenstein	O
et	O
al	O
,	O
2017	O
)	O
.	O
This	O
schema	O
proposes	O
three	O
elements	O
on	O
the	O
entity	O
level	O
as	O
well	O
as	O
the	O
annotation	O
of	O
keyphrases	O
.	O
The	O
corpus	O
contains	O
500	O
articles	O
about	O
Computer	O
Science	O
,	O
Material	O
Sciences	O
and	O
Physics	O
,	O
which	O
were	O
split	O
into	O
training	O
,	O
development	O
and	O
test	O
datasets	O
and	O
used	O
for	O
the	O
a	O
SemEval	O
task	O
in	O
2017	O
.	O
We	O
found	O
the	O
implementation	O
from	O
two	O
of	O
the	O
participants	O
on	O
the	O
shared	O
task	O
,	O
namely	O
(	O
Prasad	O
and	O
Kan	O
,	O
2017	O
)	O
and	O
(	O
Eger	O
et	O
al	O
,	O
2017	O
)	O
.	O

Previous	O
work	O
also	O
considered	O
schemes	O
that	O
consider	O
relations	O
between	O
scientific	O
elements	O
.	O
Prasad	O
et	O
al	O
(	O
2011	O
)	O
defined	O
eight	O
discourse	O
relations	O
in	O
the	O
Biomedical	O
Discourse	O
Relation	O
Bank	O
(	O
Bio	B-DatasetName
-	O
DRB	O
)	O
and	O
annotated	O
24	O
articles	O
from	O
the	O
GENIA	B-DatasetName
corpus	O
,	O
which	O
was	O
later	O
used	O
in	O
a	O
couple	O
of	O
works	O
(	O
Ramesh	O
and	O
Yu	O
,	O
2010	O
;	O
Polepalli	O
Ramesh	O
et	O
al	O
,	O
2012	O
)	O
.	O
Tateisi	O
et	O
al	O
(	O
2013	O
)	O
defined	O
16	O
relations	O
and	O
annotated	O
30	O
articles	O
,	O
while	O
Meyers	O
et	O
al	O
(	O
2014	O
)	O
proposed	O
five	O
relations	O
and	O
sub	O
-	O
relations	O
with	O
which	O
they	O
annotated	O
200	O
biomedical	O
articles	O
.	O
However	O
,	O
none	O
of	O
the	O
data	O
above	O
seems	O
to	O
be	O
available	O
,	O
but	O
we	O
found	O
corpora	O
for	O
the	O
following	O
two	O
schemes	O
:	O
Gábor	O
(	O
Gábor	O
et	O
al	O
,	O
2016	O
)	O
It	O
is	O
a	O
schema	O
in	O
the	O
form	O
of	O
an	O
ontology	B-MethodName
of	O
18	O
relations	O
for	O
the	O
scientific	O
literature	O
,	O
besides	O
three	O
more	O
general	O
relations	O
.	O
Six	O
of	O
these	O
relations	O
were	O
recently	O
addressed	O
in	O
the	O
SemEval'18	O
Task	O
7	O
,	O
for	O
which	O
annotated	O
data	O
is	O
available	O
(	O
Gábor	O
et	O
al	O
,	O
2018	O
)	O
.	O
For	O
sub	O
-	O
task	O
2	O
in	O
SemEval'18	O
Task	O
7	O
,	O
the	O
code	O
from	O
the	O
team	O
that	O
obtained	O
the	O
best	O
scores	O
in	O
this	O
task	O
is	O
available	O
(	O
Luan	O
et	O
al	O
,	O
2018	O
)	O
.	O
SciDTB	O
(	O
Yang	O
and	O
Li	O
,	O
2018	O
)	O
.	O
It	O
is	O
a	O
discourse	O
treebank	O
for	O
scientific	O
articles	O
that	O
includes	O
17	O
coarse	O
-	O
grained	O
and	O
26	O
fine	O
-	O
grained	O
relation	O
types	O
.	O
They	O
annotated	O
798	O
abstracts	O
from	O
the	O
ACL	O
Anthology	O
that	O
are	O
available	O
for	O
download	O
.	O
10	O

Hybrid	O
schemes	O
contain	O
labels	O
which	O
cover	O
more	O
than	O
one	O
of	O
the	O
levels	O
above	O
.	O
Tateisi	O
et	O
al	O
(	O
2016	O
)	O
created	O
an	O
ontology	B-MethodName
of	O
entities	O
and	O
relations	O
and	O
annotated	O
400	O
abstracts	O
about	O
computational	O
linguistic	O
.	O
However	O
,	O
we	O
found	O
only	O
one	O
hybrid	O
schema	O
for	O
which	O
annotated	O
data	O
is	O
available	O
:	O
Green	O
(	O
Green	O
,	O
2018	O
)	O
.	O
It	O
is	O
schema	O
of	O
15	O
arguments	O
annotated	O
for	O
one	O
single	O
article	O
from	O
the	O
biomedical	O
domain	O
.	O
The	O
schema	O
includes	O
both	O
entities	O
and	O
relations	O
that	O
are	O
organized	O
in	O
a	O
short	O
taxonomy	O
.	O
Both	O
schema	O
and	O
the	O
annotated	O
article	O
are	O
available	O
.	O
11	O

We	O
evaluated	O
tools	O
that	O
consider	O
some	O
of	O
the	O
schemes	O
that	O
we	O
found	O
(	O
cf	O
.	O
Section	O
2	O
)	O
for	O
the	O
task	O
of	O
text	B-TaskName
similarity	I-TaskName
in	O
the	O
scope	O
of	O
our	O
use	O
case	O
of	O
mining	O
alternative	O
methods	O
for	O
animal	O
experiments	O
.	O
In	O
this	O
section	O
we	O
described	O
the	O
data	O
and	O
the	O
tools	O
that	O
we	O
used	O
as	O
well	O
as	O
the	O
evaluation	O
methodology	O
.	O

We	O
evaluated	O
the	O
selected	O
schemes	O
and	O
tools	O
for	O
the	O
task	O
of	O
text	B-TaskName
similarity	I-TaskName
.	O
For	O
this	O
purpose	O
,	O
we	O
model	O
our	O
problem	O
as	O
the	O
following	O
:	O
given	O
an	O
input	O
document	O
that	O
describes	O
an	O
animal	O
experiment	O
,	O
we	O
would	O
like	O
to	O
mine	O
similar	O
candidate	O
documents	O
that	O
are	O
potential	O
alternatives	O
to	O
animal	O
testing	O
.	O
Our	O
definition	O
of	O
similarity	O
requires	O
that	O
both	O
input	O
and	O
candidate	O
documents	O
should	O
have	O
similar	O
research	O
goal	O
and	O
comparable	O
outcomes	O
.	O
However	O
,	O
the	O
methods	O
in	O
the	O
input	O
document	O
should	O
be	O
substantial	O
different	O
from	O
those	O
in	O
the	O
candidate	O
documents	O
.	O
Therefore	O
,	O
we	O
aim	O
to	O
compare	O
input	O
and	O
candidate	O
documents	O
based	O
on	O
certain	O
rhetorical	O
elements	O
as	O
opposed	O
to	O
using	O
the	O
whole	O
text	O
.	O
Our	O
evaluation	O
datasets	O
consist	O
of	O
seven	O
input	O
documents	O
from	O
Medline	O
whose	O
identifiers	O
(	O
PMIDs	O
)	O
are	O
11489449	O
,	O
11932745	O
,	O
16192371	O
,	O
16850029	O
,	O
19735549	O
,	O
21494637	O
and	O
24204323	O
.	O
For	O
each	O
input	O
document	O
,	O
we	O
collected	O
the	O
top	O
200	O
documents	O
(	O
titles	O
and	O
abstracts	O
)	O
retrieved	O
from	O
PubMed	O
's	O
"	O
similar	O
articles	O
"	O
functionality	O
.	O
On	O
one	O
hand	O
,	O
the	O
candidate	O
documents	O
are	O
already	O
very	O
similar	O
to	O
the	O
input	O
document	O
.	O
On	O
the	O
other	O
hand	O
,	O
the	O
list	O
of	O
candidates	O
returned	O
by	O
PubMed	O
does	O
not	O
consider	O
our	O
definition	O
of	O
similarity	O
.	O
In	O
order	O
to	O
build	O
a	O
suitable	O
test	O
set	O
for	O
our	O
use	O
case	O
,	O
a	O
biomedical	O
researcher	O
manually	O
validated	O
at	O
least	O
the	O
top	O
100	O
documents	O
with	O
regards	O
to	O
three	O
degrees	O
of	O
similarity	O
:	O
very	O
similar	O
,	O
similar	O
and	O
not	O
similar	O
.	O
These	O
three	O
labels	O
only	O
consider	O
the	O
similarity	O
of	O
the	O
research	O
goals	O
of	O
each	O
pair	O
of	O
abstracts	O
(	O
input	O
vs.	O
candidate	O
documents	O
)	O
but	O
do	O
not	O
address	O
the	O
3R	O
principles	O
.	O
Some	O
documents	O
were	O
ignored	O
because	O
either	O
they	O
were	O
only	O
partially	O
similar	O
or	O
because	O
no	O
decision	O
could	O
be	O
made	O
only	O
based	O
on	O
the	O
title	O
and	O
the	O
abstract	O
.	O
After	O
manual	O
validation	O
by	O
the	O
expert	O
,	O
our	O
seven	O
datasets	O
encompass	O
a	O
total	O
of	O
562	O
publications	O
(	O
titles	O
and	O
abstracts	O
)	O
.	O
Figure	O
1	O
illustrates	O
the	O
distribution	O
of	O
the	O
labels	O
for	O
each	O
input	O
document	O
.	O
Only	O
four	O
from	O
the	O
seven	O
input	O
documents	O
had	O
very	O
similar	O
publications	O
(	O
from	O
only	O
2	O
to	O
8	O
of	O
them	O
)	O
,	O
while	O
similar	O
ones	O
(	O
from	O
only	O
4	O
to	O
19	O
)	O
could	O
be	O
found	O
for	O
all	O
of	O
them	O
.	O
However	O
,	O
the	O
non	O
similar	O
publications	O
are	O
still	O
the	O
largest	O
part	O
(	O
from	O
56	O
to	O
76	O
)	O
of	O
the	O
list	O
.	O
The	O
annotated	O
data	O
is	O
available	O
for	O
download	O
12	O
.	O
Some	O
of	O
the	O
tools	O
that	O
we	O
compared	O
require	O
some	O
linguistic	O
information	O
not	O
originally	O
included	O
in	O
our	O
documents	O
,	O
such	O
as	O
sentences	O
and	O
tokens	O
.	O
We	O
utilized	O
syntok	O
13	O
for	O
both	O
sentence	O
splitting	O
and	O
tokenization	O
to	O
build	O
input	O
data	O
for	O
one	O
of	O
12	O
https://github.com/mariananeves/	O
scientific	O
-	O
elements	O
-	O
text	O
-	O
similarity	O
13	O
https://github.com/fnl/syntok	O
the	O
tools	O
,	O
namely	O
,	O
Prasad	O
and	O
Kan	O
(	O
2017	O
)	O
.	O

We	O
found	O
a	O
few	O
available	O
tools	O
that	O
address	O
some	O
of	O
schemes	O
discussed	O
in	O
Section	O
2	O
.	O
However	O
,	O
we	O
had	O
dismiss	O
some	O
of	O
them	O
due	O
to	O
various	O
problems	O
.	O
We	O
experienced	O
many	O
problems	O
with	O
the	O
Ten	O
-	O
sorFlow	O
library	O
while	O
trying	O
the	O
tool	O
14	O
developed	O
by	O
(	O
Eger	O
et	O
al	O
,	O
2017	O
)	O
for	O
the	O
ScienceIE	B-DatasetName
schema	O
.	O
The	O
tool	O
seems	O
to	O
require	O
a	O
version	O
of	O
the	O
library	O
that	O
it	O
is	O
no	O
longer	O
available	O
and	O
we	O
could	O
not	O
resolve	O
this	O
issue	O
not	O
even	O
after	O
contacting	O
the	O
tool	O
's	O
developers	O
.	O
We	O
also	O
dismissed	O
the	O
tool	O
15	O
from	O
Jin	O
and	O
Szolovits	O
(	O
2018	O
)	O
for	O
the	O
PIBOSO	O
and	O
Pub	O
-	O
MedRCT	O
schemes	O
.	O
The	O
installation	O
worked	O
but	O
we	O
were	O
not	O
able	O
to	O
train	O
it	O
due	O
to	O
memory	O
problems	O
.	O
Finally	O
,	O
we	O
did	O
not	O
try	O
the	O
tool	O
16	O
from	O
Luan	O
et	O
al	O
(	O
2018	O
)	O
since	O
it	O
addresses	O
a	O
relationbased	O
schema	O
(	O
Gábor	O
)	O
that	O
requires	O
pre	O
-	O
tagged	O
entities	O
.	O
Using	O
named	O
entities	O
provided	O
by	O
other	O
tools	O
would	O
probably	O
add	O
too	O
much	O
noise	O
to	O
the	O
experiment	O
.	O
Finally	O
,	O
we	O
had	O
to	O
dismiss	O
the	O
SAPI	O
-	O
ENTA	O
tool	O
(	O
Liakata	O
et	O
al	O
,	O
2012b	O
)	O
because	O
it	O
only	O
allows	O
uploading	O
documents	O
one	O
by	O
one	O
to	O
the	O
Web	O
application	O
and	O
we	O
could	O
not	O
overcome	O
this	O
problem	O
.	O
We	O
describe	O
below	O
the	O
four	O
tools	O
that	O
we	O
tried	O
for	O
the	O
extraction	O
of	O
rhetorical	O
elements	O
.	O
Examples	O
for	O
the	O
sentence	O
-	O
based	O
(	O
zones	O
)	O
and	O
entitybased	O
annotations	O
are	O
shown	O
in	O
Figure	O
2	O
.	O
We	O
released	O
in	O
the	O
GitHub	O
repository	O
the	O
annotations	O
extracted	O
by	O
the	O
tools	O
in	O
the	O
JSON	O
format	O
supported	O
by	O
the	O
TextAE	O
tool	O
17	O
.	O
Achakulvisut	O
et	O
al	O
18	O
(	O
Achakulvisut	O
et	O
al	O
,	O
2018	O
)	O
(	O
PubMedRCT	O
schema	O
)	O
.	O
It	O
addresses	O
the	O
PubMed	B-DatasetName
RCT	I-DatasetName
schema	O
,	O
thus	O
provides	O
predictions	O
for	O
five	O
zoning	O
labels	O
,	O
namely	O
,	O
"	O
Background	O
"	O
,	O
"	O
Objective	O
"	O
,	O
"	O
Method	O
"	O
,	O
"	O
Results	O
"	O
and	O
"	O
Conclusions	O
"	O
.	O
We	O
utilized	O
the	O
pre	O
-	O
trained	O
models	O
for	O
Conditional	O
Random	O
Fields	O
(	O
CRF	B-MethodName
)	O
as	O
provided	O
by	O
the	O
tool	O
.	O
Given	O
that	O
there	O
is	O
no	O
publication	O
,	O
it	O
is	O
not	O
clear	O
what	O
methods	O
are	O
behind	O
the	O
available	O
models	O
,	O
but	O
probably	O
CRF	B-MethodName
.	O
ArguminSci	O
19	O
(	O
Lauscher	O
et	O
al	O
,	O
2018a	O
)	O
(	O
Dr.	O
Inventor	O
schema	O
extended	O
)	O
.	O
ArguminSci	O
is	O
available	O
both	O
for	O
download	O
as	O
well	O
as	O
on	O
-	O
line	O
(	O
Web	O
application	O
)	O
.	O
It	O
provides	O
predictions	O
for	O
five	O
schemes	O
but	O
we	O
considered	O
only	O
the	O
"	O
Discourse	O
Role	O
Classification	B-TaskName
(	O
DRC	O
)	O
"	O
whose	O
labels	O
are	O
"	O
Background	O
"	O
,	O
"	O
Challenge	O
"	O
,	O
"	O
Approach	O
"	O
,	O
"	O
Outcome	O
"	O
and	O
"	O
Future	O
Work	O
"	O
.	O
ArguminSci	O
's	O
models	O
are	O
based	O
on	O
bidirectional	O
recurrent	O
networks	O
with	O
long	O
shortterm	O
memory	O
cells	O
(	O
Bi	O
-	O
LSTMs	O
)	O
and	O
we	O
utilized	O
the	O
command	O
line	O
version	O
of	O
the	O
tool	O
.	O
MAZEA	O
tool	O
20	O
and	O
schema	O
(	O
Dayrell	O
et	O
al	O
,	O
2012	O
)	O
.	O
The	O
tool	O
addresses	O
six	O
categories	O
,	O
namely	O
,	O
"	O
Background	O
"	O
,	O
"	O
Gap	O
"	O
,	O
"	O
Purpose	O
"	O
,	O
"	O
Method	O
"	O
,	O
"	O
Result	O
"	O
and	O
"	O
Conclusion	O
"	O
.	O
It	O
is	O
currently	O
not	O
available	O
for	O
download	O
but	O
only	O
as	O
a	O
Web	O
tool	O
that	O
requires	O
to	O
manually	O
upload	O
each	O
document	O
individually	O
.	O
However	O
,	O
the	O
developers	O
kindly	O
processed	O
our	O
documents	O
locally	O
and	O
sent	O
the	O
predictions	O
back	O
to	O
us	O
.	O
The	O
tool	O
utilizes	O
machine	O
learning	O
algorithms	O
,	O
such	O
as	O
Support	O
Vector	O
Machines	O
(	O
SVM	B-MethodName
)	O
and	O
Decision	O
Trees	O
.	O
repository	O
,	O
we	O
utilized	O
the	O
scripts	O
for	O
feature	O
processing	O
and	O
the	O
template	O
to	O
train	O
the	O
model	O
with	O
CRF++	O
22	O
.	O
We	O
had	O
to	O
correct	O
the	O
provided	O
template	O
in	O
order	O
to	O
successfully	O
train	O
the	O
system	O
.	O
The	O
entity	O
recognition	O
approach	O
is	O
based	O
on	O
various	O
features	O
and	O
uses	O
the	O
CRF	B-MethodName
algorithm	O
.	O

We	O
carried	O
out	O
a	O
total	O
of	O
38	O
experiments	O
that	O
involved	O
diverse	O
tools	O
,	O
single	O
labels	O
and	O
combination	O
of	O
various	O
labels	O
.	O
We	O
ran	O
an	O
error	O
analysis	O
to	O
learn	O
more	O
about	O
the	O
false	O
negatives	O
and	O
false	O
positives	O
that	O
we	O
obtained	O
.	O
At	O
least	O
one	O
positive	O
document	O
was	O
missed	O
by	O
any	O
of	O
the	O
tools	O
,	O
i.e.	O
was	O
not	O
placed	O
among	O
the	O
top	O
10	O
positions	O
.	O
Many	O
of	O
the	O
documents	O
that	O
we	O
missed	O
are	O
certainly	O
due	O
to	O
the	O
limitation	O
of	O
considering	O
only	O
the	O
top	O
10	O
highest	O
ranked	O
positions	O
.	O
However	O
,	O
none	O
of	O
the	O
experiments	O
obtained	O
a	O
recall	O
of	O
1.0	O
.	O
The	O
highest	O
recall	O
that	O
we	O
obtained	O
was	O
0.9	O
for	O
the	O
dataset	O
3	O
(	O
16192371	O
)	O
using	O
the	O
Argu	O
-	O
minSci	O
tool	O
and	O
either	O
the	O
single	O
label	O
"	O
Outcome	O
"	O
or	O
the	O
combination	O
of	O
labels	O
"	O
Challenge	O
-	O
Outcome	O
-	O
FutureWork	O
"	O
.	O
On	O
one	O
hand	O
,	O
five	O
documents	O
were	O
missed	O
by	O
all	O
experiments	O
(	O
38	O
times	O
)	O
,	O
namely	O
,	O
candidate	O
documents	O
"	O
19155551	O
"	O
,	O
"	O
29133591	O
"	O
,	O
"	O
21362567	O
"	O
,	O
"	O
19667187	O
"	O
and	O
"	O
26047474	O
"	O
from	O
datasets	O
3	O
,	O
5	O
,	O
6	O
,	O
6	O
,	O
and	O
7	O
,	O
respectively	O
.	O
On	O
the	O
other	O
hand	O
,	O
the	O
candidate	O
document	O
"	O
25174890	O
"	O
from	O
dataset	O
6	O
was	O
the	O
least	O
missed	O
one	O
:	O
only	O
by	O
three	O
experiments	O
.	O
A	O
total	O
of	O
333	O
documents	O
were	O
wrongly	O
classified	O
as	O
positive	O
,	O
i.e.	O
were	O
placed	O
among	O
the	O
top	O
10	O
ones	O
,	O
by	O
any	O
of	O
the	O
38	O
experiments	O
.	O
No	O
candidate	O
document	O
was	O
mistakenly	O
classified	O
by	O
all	O
approaches	O
,	O
but	O
the	O
more	O
frequent	O
ones	O
were	O
:	O
"	O
21501651	O
"	O
(	O
27	O
times	O
)	O
and	O
"	O
23571276	O
"	O
(	O
25	O
times	O
)	O
,	O
both	O
from	O
dataset	O
4	O
,	O
and	O
"	O
11494364	O
"	O
(	O
25	O
times	O
)	O
from	O
dataset	O
7	O
.	O
Our	O
expert	O
checked	O
again	O
the	O
labels	O
assigned	O
to	O
the	O
top	O
FPs	O
and	O
FNs	O
above	O
described	O
and	O
confirmed	O
that	O
their	O
labels	O
are	O
correct	O
and	O
that	O
the	O
documents	O
have	O
been	O
wrongly	O
classified	O
by	O
the	O
corresponding	O
approaches	O
.	O
Our	O
experiments	O
have	O
shown	O
that	O
many	O
of	O
the	O
tools	O
can	O
indeed	O
support	O
our	O
use	O
case	O
,	O
specially	O
when	O
compared	O
to	O
the	O
original	O
list	O
provided	O
by	O
PubMed	O
.	O
Regarding	O
the	O
integration	O
of	O
these	O
tools	O
into	O
a	O
workflow	O
,	O
one	O
of	O
the	O
tools	O
is	O
currently	O
not	O
available	O
(	O
MAZEA	O
)	O
,	O
while	O
all	O
the	O
others	O
need	O
some	O
adaptations	O
to	O
be	O
used	O
in	O
real	O
-	O
life	O
applications	O
.	O
With	O
respect	O
to	O
the	O
methods	O
behind	O
the	O
tools	O
,	O
ArguminSci	O
,	O
which	O
is	O
based	O
on	O
LSTM	B-MethodName
,	O
performed	O
slightly	O
better	O
than	O
the	O
ones	O
based	O
on	O
CRF	B-MethodName
(	O
Achakulvisut	O
et	O
al	O
Prasad	O
and	O
Kan	O
)	O
and	O
superior	O
than	O
the	O
machine	O
learning	O
algorithms	O
in	O
MAZEA	O
.	O
However	O
,	O
we	O
did	O
not	O
evaluate	O
the	O
predictions	O
made	O
by	O
the	O
tools	O
,	O
but	O
only	O
their	O
impact	O
in	O
a	O
specific	O
text	B-TaskName
similarity	I-TaskName
task	O
.	O
We	O
expected	O
that	O
the	O
best	O
performing	O
tools	O
would	O
be	O
the	O
ones	O
that	O
utilized	O
corpora	O
specifically	O
built	O
for	O
the	O
biomedical	O
domain	O
.	O
From	O
the	O
tools	O
that	O
we	O
evaluated	O
,	O
only	O
Achakulvisut	O
et	O
al	O
and	O
MAZEA	O
were	O
specifically	O
trained	O
on	O
documents	O
from	O
the	O
biomedical	O
or	O
health	O
domains	O
.	O
Nevertheless	O
,	O
ArguminSci	O
,	O
the	O
best	O
performing	O
one	O
,	O
was	O
trained	O
on	O
documents	O
from	O
computer	O
graphics	O
while	O
and	O
Prasad	O
and	O
Kan	O
utilizes	O
documents	O
about	O
computational	O
linguistics	O
.	O
We	O
also	O
investigated	O
whether	O
there	O
was	O
any	O
impact	O
of	O
the	O
document	O
type	O
in	O
the	O
corpora	O
,	O
i.e.	O
either	O
full	O
texts	O
or	O
only	O
abstracts	O
,	O
on	O
the	O
performance	O
of	O
the	O
corresponding	O
tools	O
.	O
However	O
,	O
we	O
did	O
not	O
observe	O
any	O
clear	O
association	O
between	O
these	O
two	O
aspects	O
.	O
While	O
the	O
best	O
performing	O
tool	O
(	O
Argu	O
-	O
minSci	O
)	O
was	O
trained	O
on	O
full	O
texts	O
,	O
Achakulvisut	O
et	O
al	O
utilizes	O
only	O
Medline	O
abstracts	O
.	O
Similar	O
to	O
Ar	O
-	O
guminSci	O
,	O
the	O
tool	O
from	O
Prasad	O
and	O
Kan	O
is	O
also	O
based	O
on	O
full	O
text	O
documents	O
.	O
We	O
carried	O
out	O
experiments	O
with	O
various	O
tools	O
but	O
limited	O
to	O
a	O
very	O
specific	O
use	O
case	O
.	O
Even	O
though	O
our	O
datasets	O
contains	O
a	O
reasonable	O
number	O
of	O
documents	O
(	O
562	O
)	O
,	O
the	O
similarity	O
of	O
the	O
candidate	O
documents	O
was	O
computed	O
with	O
respect	O
to	O
only	O
seven	O
input	O
documents	O
,	O
and	O
datasets	O
were	O
annotated	O
by	O
only	O
one	O
annotator	O
.	O
Further	O
,	O
we	O
only	O
considered	O
titles	O
and	O
abstracts	O
in	O
our	O
evaluation	O
,	O
while	O
some	O
tools	O
were	O
trained	O
on	O
full	O
-	O
text	O
documents	O
.	O
Previous	O
work	O
has	O
already	O
shown	O
the	O
differences	O
of	O
information	O
and	O
performance	O
of	O
NLP	O
tools	O
in	O
biomedical	O
abstracts	O
and	O
full	O
texts	O
(	O
Verspoor	O
et	O
al	O
,	O
2012	O
;	O
Mons	O
et	O
al	O
,	O
2004	O
)	O
.	O
Our	O
future	O
work	O
will	O
ad	O
-	O
dress	O
many	O
aspects	O
:	O
(	O
i	O
)	O
use	O
of	O
full	O
texts	O
;	O
(	O
ii	O
)	O
improvement	O
of	O
the	O
datasets	O
with	O
additional	O
annotators	O
;	O
(	O
iii	O
)	O
estimation	O
of	O
the	O
compliance	O
with	O
the	O
3R	O
principles	O
by	O
a	O
candidate	O
document	O
,	O
in	O
addition	O
to	O
the	O
calculation	O
of	O
similarity	O
;	O
(	O
iv	O
)	O
evaluation	O
of	O
the	O
relation	O
-	O
based	O
tool	O
(	O
Luan	O
et	O
al	O
,	O
2018	O
)	O
and	O
the	O
one	O
for	O
which	O
we	O
experienced	O
memory	O
problems	O
(	O
Jin	O
and	O
Szolovits	O
,	O
2018	O
)	O
;	O
and	O
(	O
v	O
)	O
evaluation	O
of	O
other	O
schemes	O
(	O
e.g.	O
Wilbur	O
et	O
al	O
(	O
2006	O
)	O
)	O
for	O
which	O
an	O
implementation	O
is	O
currently	O
not	O
available	O
.	O

We	O
surveyed	O
schemes	O
that	O
model	O
scientific	O
elements	O
in	O
publications	O
and	O
selected	O
four	O
schemes	O
for	O
which	O
we	O
could	O
find	O
an	O
available	O
tool	O
.	O
We	O
utilized	O
the	O
predictions	O
from	O
these	O
tools	O
for	O
assessing	O
the	O
text	B-TaskName
similarity	I-TaskName
between	O
documents	O
and	O
further	O
ranking	O
them	O
in	O
the	O
scope	O
of	O
mining	O
alternative	O
methods	O
to	O
animal	O
testing	O
.	O
Our	O
experiments	O
show	O
that	O
a	O
considerable	O
improvement	O
can	O
be	O
obtained	O
when	O
using	O
ArguminSci	O
,	O
with	O
respect	O
to	O
the	O
original	O
ranking	O
returned	O
by	O
PubMed	O
and	O
to	O
the	O
strong	O
baseline	O
that	O
we	O
considered	O
.	O
However	O
,	O
there	O
is	O
still	O
much	O
room	O
for	O
improvement	O
given	O
that	O
the	O
obtained	O
scores	O
are	O
still	O
far	O
below	O
the	O
possible	O
maximum	O
values	O
.	O

For	O
Task	O
1	O
our	O
final	O
submission	O
consisted	O
of	O
an	O
ensemble	O
of	O
two	O
different	O
multilingual	O
models	O
,	O
that	O
differ	O
in	O
the	O
way	O
they	O
process	O
the	O
input	O
source	O
(	O
original	O
sentence	O
)	O
and	O
hypothesis	O
(	O
machine	B-TaskName
translation	I-TaskName
)	O
.	O
Both	O
models	O
are	O
based	O
on	O
the	O
predictor	O
-	O
estimator	O
architecture	O
,	O
using	O
different	O
pre	O
-	O
trained	O
models	O
to	O
extract	O
features	O
and	O
different	O
training	O
approaches	O
to	O
optimise	O
for	O
the	O
QE	O
task	O
.	O
The	O
key	O
idea	O
explored	O
with	O
our	O
first	O
model	O
(	O
denoted	O
by	O
M1	O
variations	O
in	O
the	O
experiments	O
)	O
,	O
revolved	O
around	O
pursuing	O
highly	O
generalisable	O
multilingual	O
models	O
,	O
robust	O
to	O
overfitting	O
.	O
To	O
this	O
end	O
,	O
we	O
train	O
a	O
cross	O
-	O
lingual	O
transformer	O
(	O
XLM	B-MethodName
-	O
RoBERTa	B-MethodName
(	O
Conneau	O
et	O
al	O
,	O
2020	O
)	O
)	O
on	O
large	O
,	O
multilingual	O
data	O
with	O
direct	O
assessments	O
and	O
then	O
use	O
adapters	O
(	O
Houlsby	O
et	O
al	O
,	O
2019	O
;	O
Pfeiffer	O
et	O
al	O
,	O
2020	O
)	O
to	O
adapt	O
to	O
the	O
domain	O
specific	O
data	O
of	O
the	O
QE	O
task	O
with	O
minimal	O
training	O
effort	O
.	O
In	O
line	O
with	O
our	O
efforts	O
for	O
good	O
generalisation	O
,	O
we	O
use	O
only	O
task	O
-	O
specific	O
adapters	O
and	O
refrain	O
from	O
using	O
specific	O
adapters	O
for	O
each	O
language	O
pair	O
.	O
For	O
these	O
experiments	O
we	O
build	O
on	O
the	O
OpenKiwi	O
architecture	O
(	O
Kepler	O
et	O
al	O
,	O
2019	O
)	O
,	O
using	O
a	O
pre	O
-	O
trained	O
xlm	B-MethodName
-	O
roberta	O
-	O
large	O
encoder	O
as	O
a	O
feature	O
predictor	O
.	O
The	O
source	O
and	O
hypothesis	O
sentences	O
are	O
jointly	O
encoded	O
with	O
hypothesis	O
first	O
.	O
Then	O
,	O
source	O
and	O
hypothesis	O
features	O
are	O
generated	O
using	O
average	B-MethodName
pooling	I-MethodName
over	O
the	O
hypothesis	O
embeddings	O
and	O
forwarded	O
to	O
the	O
estimator	O
module	O
which	O
corresponds	O
to	O
a	O
feed	O
-	O
forward	O
layer	O
.	O
Figure	O
1	O
provides	O
the	O
general	O
architecture	O
1	O
The	O
model	O
was	O
first	O
trained	O
on	O
the	O
direct	O
assessment	O
data	O
provided	O
in	O
the	O
Metrics	O
shared	O
tasks	O
(	O
Mathur	O
et	O
al	O
,	O
2020	O
)	O
,	O
as	O
described	O
in	O
3.1.2	O
.	O
Upon	O
training	O
,	O
the	O
XML	O
-	O
R	O
encoder	O
is	O
frozen	O
and	O
the	O
the	O
model	O
is	O
fine	O
-	O
tuned	O
on	O
sentence	O
regression	O
with	O
the	O
task	O
-	O
specific	O
data	O
,	O
using	O
stacked	O
adapters	O
.	O
We	O
hence	O
manage	O
to	O
maintain	O
a	O
low	O
number	O
of	O
trainable	O
parameters	O
during	O
fine	O
-	O
tuning	O
and	O
minimize	O
training	O
time	O
while	O
learning	O
to	O
predict	O
task	O
-	O
specific	O
sentence	O
scores	O
.	O
For	O
the	O
second	O
model	O
(	O
denoted	O
by	O
M2	O
-	O
KL	O
-	O
G	O
-	O
MCD	O
)	O
we	O
aimed	O
to	O
explore	O
the	O
potential	O
of	O
a	O
large	O
pre	O
-	O
trained	O
multilingual	O
model	O
(	O
trained	O
with	O
MT	O
objectives	O
)	O
.	O
We	O
use	O
the	O
mBART	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2020	O
)	O
encoder	O
-	O
decoder	O
architecture	O
to	O
encode	O
the	O
source	O
and	O
force	O
-	O
decode	O
the	O
hypothesis	O
.	O
We	O
specifically	O
use	O
the	O
mBART50	O
model	O
(	O
Tang	O
et	O
al	O
,	O
2020	O
)	O
which	O
is	O
trained	O
with	O
multilingual	O
finetuning	O
on	O
50	O
languages	O
,	O
including	O
all	O
languages	O
of	O
interest	O
for	O
the	O
QE	O
2021	O
task	O
.	O
We	O
obtain	O
the	O
features	O
by	O
averaging	O
the	O
decoder	O
embeddings	O
and	O
concatenating	O
with	O
the	O
<	O
eos	O
>	O
token	O
of	O
the	O
sequence	O
.	O
The	O
estimator	O
part	O
of	O
the	O
model	O
consists	O
of	O
a	O
bottleneck	O
feed	O
-	O
forward	O
layer	O
that	O
reduces	O
the	O
dimensionality	O
of	O
the	O
decoder	O
output	O
,	O
and	O
is	O
concatenated	O
with	O
a	O
vector	O
with	O
additional	O
glass	O
-	O
box	O
features	O
from	O
the	O
NMT	O
models	O
(	O
see	O
3.1.1	O
)	O
.	O
The	O
combined	O
vector	O
is	O
then	O
forwarded	O
to	O
a	O
feed	O
-	O
forward	O
estimator	O
and	O
the	O
full	O
model	O
is	O
fine	O
-	O
tuned	O
on	O
the	O
task	O
specific	O
QE	O
data	O
.	O
Apart	O
from	O
the	O
glass	O
-	O
box	O
features	O
we	O
experimented	O
further	O
with	O
methods	O
that	O
allow	O
the	O
model	O
to	O
be	O
more	O
robust	O
towards	O
the	O
underlying	O
uncertainty	O
of	O
its	O
predictions	O
.	O
We	O
elaborate	O
that	O
in	O
the	O
next	O
section	O
.	O
Figure	O
2	O
provides	O
a	O
general	O
architecture	O
of	O
the	O
M2	O
model	O
variations	O
.	O

Multiple	O
neural	O
models	O
are	O
involved	O
in	O
the	O
process	O
of	O
obtaining	O
and	O
scoring	O
machine	O
translations	O
,	O
which	O
naturally	O
leads	O
to	O
several	O
sources	O
of	O
uncertainty	O
.	O
These	O
sources	O
can	O
be	O
very	O
informative	O
and	O
useful	O
for	O
MT	O
evaluation	O
.	O
In	O
this	O
work	O
we	O
try	O
to	O
consider	O
three	O
types	O
of	O
uncertainty	O
:	O
(	O
1	O
)	O
uncertainty	O
of	O
the	O
NMT	O
models	O
used	O
to	O
obtain	O
the	O
hypotheses	O
,	O
(	O
2	O
)	O
data	O
(	O
aleatoric	O
)	O
uncertainty	O
for	O
which	O
we	O
use	O
the	O
inter	O
-	O
annotator	O
disagreement	O
as	O
a	O
proxy	O
,	O
and	O
(	O
3	O
)	O
uncertainty	O
of	O
the	O
MT	O
evaluation	O
model	O
itself	O
.	O
NMT	O
model	O
uncertainty	O
The	O
idea	O
of	O
extracting	O
uncertainty	O
-	O
related	O
features	O
from	O
the	O
MT	O
systems	O
in	O
order	O
to	O
estimate	O
the	O
quality	O
of	O
their	O
predictions	O
,	O
was	O
originally	O
introduced	O
by	O
Fomicheva	O
et	O
al	O
(	O
2020	O
)	O
.	O
This	O
glass	O
-	O
box	O
approach	O
to	O
QE	O
is	O
mostly	O
focusing	O
on	O
capturing	O
epistemic	O
uncertainty	O
,	O
and	O
the	O
proposed	O
features	O
are	O
extracted	O
either	O
using	O
Monte	O
Carlo	O
(	O
MC	O
)	O
dropout	O
on	O
the	O
NMT	O
or	O
using	O
the	O
output	O
probability	O
distributions	O
obtained	O
from	O
a	O
standard	O
deterministic	O
MT	O
system	O
.	O
In	O
our	O
last	O
year	O
's	O
submission	O
(	O
Moura	O
et	O
al	O
,	O
2020	O
)	O
the	O
integration	O
of	O
such	O
features	O
proved	O
to	O
be	O
effective	O
,	O
thus	O
we	O
decided	O
to	O
incorporate	O
it	O
into	O
our	O
new	O
model	O
as	O
well	O
.	O
We	O
list	O
the	O
extracted	O
features	O
below	O
:	O
TP	O
sentence	O
average	O
of	O
word	B-TaskName
translation	I-TaskName
probability	O
-	O
of	O
MT	O
output	O
generated	O
in	O
different	O
stochastic	O
passes	O
.	O

For	O
Task	O
2	O
we	O
submitted	O
an	O
ensemble	O
of	O
two	O
variations	O
of	O
the	O
first	O
model	O
(	O
M1	O
-	O
ADAPT	O
and	O
M1	O
M	O
-	O
ADAPT	O
)	O
presented	O
for	O
Task	O
1	O
(	O
see	O
3.1	O
)	O
.	O
In	O
both	O
cases	O
,	O
we	O
use	O
multi	O
-	O
task	O
training	O
and	O
a	O
feedforward	O
for	O
each	O
output	O
types	O
:	O
hypothesis	O
word	O
tags	O
,	O
hypothesis	O
gap	O
tags	O
,	O
source	O
word	O
tags	O
,	O
and	O
sentence	O
regression	O
(	O
on	O
HTER	O
scores	O
)	O
.	O
Both	O
variations	O
use	O
a	O
pre	O
-	O
trained	O
XLM	B-MethodName
-	O
RoBERTa	B-MethodName
(	O
large	O
)	O
encoder	O
to	O
extract	O
features	O
as	O
described	O
for	O
Task	O
1	O
,	O
but	O
differ	O
in	O
the	O
training	O
of	O
the	O
encoder	O
.	O
In	O
the	O
first	O
case	O
we	O
use	O
the	O
pre	O
-	O
trained	O
model	O
3	O
and	O
finetune	O
on	O
the	O
QE	O
data	O
using	O
stacked	O
adapters	O
.	O
In	O
the	O
second	O
variation	O
we	O
swap	O
the	O
original	O
pre	O
-	O
trained	O
model	O
with	O
the	O
XLM	B-MethodName
-	O
RoBERTa	B-MethodName
model	O
that	O
has	O
been	O
trained	O
on	O
the	O
Metrics	O
data	O
as	O
described	O
in	O
3.1.2	O
.	O
We	O
note	O
that	O
the	O
two	O
variations	O
favor	O
different	O
language	O
pairs	O
,	O
hence	O
we	O
combine	O
multiple	O
checkpoints	O
from	O
each	O
variation	O
(	O
ranging	O
training	O
steps	O
)	O
.	O
We	O
use	O
the	O
test	O
-	O
20	O
split	O
of	O
the	O
data	O
to	O
optimise	O
the	O
hyper	O
-	O
parameters	O
and	O
following	O
this	O
approach	O
we	O
use	O
the	O
estimated	O
top	O
-	O
3	O
checkpoints	O
from	O
each	O
variation	O
using	O
the	O
combined	O
dataset	O
4	O
and	O
the	O
top	O
checkpoint	O
for	O
the	O
non	O
-	O
augmented	O
model	O
trained	O
exclusively	O
on	O
the	O
train	O
set	O
,	O
resulting	O
in	O
total	O
7	O
checkpoints	O
in	O
our	O
final	O
ensemble	O
.	O

In	O
Table	O
6	O
is	O
an	O
excerpt	O
of	O
the	O
training	O
configuration	O
used	O
for	O
training	O
the	O
M2	O
models	O
using	O
the	O
mBART	B-MethodName
encoder	O
-	O
decoder	O
:	O

We	O
are	O
grateful	O
to	O
Alon	O
Lavie	O
and	O
Craig	O
Stewart	O
for	O
their	O
valuable	O
feedback	O
and	O
discussions	O
.	O
This	O
work	O
was	O
supported	O
by	O
the	O
P2020	O
programs	O
MAIA	O
(	O
contract	O
045909	O
)	O
and	O
Unbabel4EU	O
(	O
contract	O
042671	O
)	O
,	O
by	O
the	O
European	O
Research	O
Council	O
(	O
ERC	O
StG	O
Deep	O
-	O
SPIN	B-MethodName
758969	O
)	O
,	O
and	O
by	O
the	O
Fundação	O
para	O
a	O
Ciência	O
e	O
Tecnologia	O
through	O
contract	O
UIDB/50008/2020	O
.	O

Structure	O
-	O
Aware	O
Abstractive	O
Conversation	O
Summarization	B-TaskName
via	O
Discourse	O
and	O
Action	O
Graphs	O

Abstractive	O
conversation	O
summarization	B-TaskName
has	O
received	O
much	O
attention	O
recently	O
.	O
However	O
,	O
these	O
generated	O
summaries	O
often	O
suffer	O
from	O
insufficient	O
,	O
redundant	O
,	O
or	O
incorrect	O
content	O
,	O
largely	O
due	O
to	O
the	O
unstructured	O
and	O
complex	O
characteristics	O
of	O
human	O
-	O
human	O
interactions	O
.	O
To	O
this	O
end	O
,	O
we	O
propose	O
to	O
explicitly	O
model	O
the	O
rich	O
structures	O
in	O
conversations	O
for	O
more	O
precise	O
and	O
accurate	O
conversation	O
summarization	B-TaskName
,	O
by	O
first	O
incorporating	O
discourse	O
relations	O
between	O
utterances	O
and	O
action	O
triples	O
(	O
"	O
WHO	O
-	O
DOING	O
-	O
WHAT	O
"	O
)	O
in	O
utterances	O
through	O
structured	O
graphs	O
to	O
better	O
encode	O
conversations	O
,	O
and	O
then	O
designing	O
a	O
multi	O
-	O
granularity	O
decoder	O
to	O
generate	O
summaries	O
by	O
combining	O
all	O
levels	O
of	O
information	O
.	O
Experiments	O
show	O
that	O
our	O
proposed	O
models	O
outperform	O
state	O
-	O
of	O
-	O
theart	O
methods	O
and	O
generalize	O
well	O
in	O
other	O
domains	O
in	O
terms	O
of	O
both	O
automatic	O
evaluations	O
and	O
human	O
judgments	O
.	O
We	O
have	O
publicly	O
released	O
our	O
code	O
at	O
https://github.com/	O
GT	O
-	O
SALT	O
/	O
Structure	O
-	O
Aware	O
-	O
BART	B-MethodName
.	O

Online	O
interaction	O
has	O
become	O
an	O
indispensable	O
component	O
of	O
everyday	O
life	O
and	O
people	O
are	O
increasingly	O
using	O
textual	O
conversations	O
to	O
exchange	O
ideas	O
,	O
make	O
plans	O
,	O
and	O
share	O
information	O
.	O
However	O
,	O
it	O
is	O
time	O
-	O
consuming	O
to	O
recap	O
and	O
grasp	O
all	O
the	O
core	O
content	O
within	O
every	O
complex	O
conversation	O
(	O
Gao	O
et	O
al	O
,	O
2020	O
;	O
Feng	O
et	O
al	O
,	O
2020	O
)	O
.	O
As	O
a	O
result	O
,	O
how	O
to	O
organize	O
massive	O
everyday	O
interactions	O
into	O
natural	O
,	O
concise	O
,	O
and	O
informative	O
text	O
,	O
i.e.	O
,	O
abstractive	O
conversation	O
summarization	B-TaskName
,	O
starts	O
to	O
gain	O
importance	O
.	O
Significant	O
progress	O
has	O
been	O
made	O
on	O
abstractive	O
summarization	B-TaskName
for	O
structured	O
document	O
via	O
pointer	O
generator	O
(	O
See	O
et	O
al	O
,	O
2017	O
)	O
,	O
reinforcement	O
methods	O
(	O
Paulus	O
et	O
al	O
,	O
2018	O
;	O
and	O
pre	O
-	O
trained	O
models	O
(	O
Liu	O
and	O
Lapata	O
,	O
2019	O
;	O
Lewis	O
et	O
al	O
,	O
2020	O
;	O
.	O
Despite	O
the	O
huge	O
success	O
,	O
it	O
is	O
challenging	O
to	O
directly	O
apply	O
document	O
models	O
to	O
summarize	O
conversations	O
,	O
due	O
to	O
(	O
Gliwa	O
et	O
al	O
,	O
2019	O
)	O
.	O
The	O
annotated	O
summary	O
is	O
Simon	O
was	O
on	O
the	O
phone	O
before	O
,	O
so	O
he	O
did	O
n't	O
here	O
Helen	B-DatasetName
calling	O
.	O
Simon	O
will	O
fetch	O
Helen	B-DatasetName
some	O
tissues	O
.	O
a	O
set	O
of	O
inherent	O
differences	O
between	O
conversations	O
and	O
documents	O
(	O
Gliwa	O
et	O
al	O
,	O
2019	O
)	O
.	O
First	O
,	O
speaker	O
interruptions	O
like	O
repetitions	O
,	O
false	O
-	O
starts	O
,	O
and	O
hesitations	O
are	O
frequent	O
in	O
conversations	O
(	O
Sacks	O
et	O
al	O
,	O
1978	O
)	O
,	O
and	O
key	O
information	O
resides	O
in	O
different	O
portions	O
of	O
a	O
conversation	O
.	O
These	O
unstructured	O
properties	O
pose	O
challenges	O
for	O
models	O
to	O
focus	O
on	O
salient	O
contents	O
that	O
are	O
necessary	O
for	O
generating	O
both	O
abstractive	O
and	O
informative	O
summaries	O
.	O
Second	O
,	O
there	O
is	O
more	O
than	O
one	O
speaker	O
in	O
conversations	O
and	O
people	O
interact	O
with	O
each	O
other	O
in	O
different	O
language	O
styles	O
.	O
The	O
complex	O
interactions	O
among	O
multiple	O
speakers	O
make	O
it	O
harder	O
for	O
mod	O
-	O
els	O
to	O
identify	O
and	O
associate	O
speakers	O
with	O
correct	O
actions	O
so	O
as	O
to	O
generate	O
factual	O
summaries	O
.	O
In	O
order	O
to	O
summarize	O
the	O
unstructured	O
and	O
complex	O
conversations	O
,	O
a	O
growing	O
body	O
of	O
research	O
has	O
been	O
conducted	O
,	O
such	O
as	O
transferring	O
document	B-TaskName
summarization	I-TaskName
methods	O
to	O
conversation	O
settings	O
(	O
Shang	O
et	O
al	O
,	O
2018	O
;	O
Gliwa	O
et	O
al	O
,	O
2019	O
)	O
,	O
adopting	O
hierarchical	O
models	O
,	O
or	O
incorporating	O
conversation	O
structures	O
like	O
topic	O
segmentation	O
(	O
Liu	O
et	O
al	O
,	O
2019b	O
;	O
Chen	O
and	O
Yang	O
,	O
2020	O
)	O
,	O
dialogue	O
acts	O
(	O
Goo	O
and	O
Chen	O
,	O
2018	O
)	O
,	O
and	O
conversation	O
stages	O
(	O
Chen	O
and	O
Yang	O
,	O
2020	O
)	O
.	O
However	O
,	O
current	O
approaches	O
still	O
face	O
challenges	O
in	O
terms	O
of	O
succinctness	O
and	O
faithfulness	O
,	O
as	O
most	O
prior	O
studies	O
(	O
i	O
)	O
fail	O
to	O
explicitly	O
model	O
dependencies	O
between	O
utterances	O
which	O
can	O
help	O
identify	O
salient	O
portions	O
of	O
conversations	O
(	O
Bui	O
et	O
al	O
,	O
2009	O
)	O
,	O
and	O
(	O
ii	O
)	O
lack	O
structured	O
representations	O
to	O
learn	O
the	O
associations	O
between	O
speakers	O
,	O
actions	O
and	O
events	O
.	O
We	O
argue	O
that	O
these	O
rich	O
linguistic	O
structures	O
associated	O
with	O
conversations	O
are	O
key	O
components	O
towards	O
generating	O
abstractive	O
and	O
factual	O
conversation	O
summaries	O
.	O
To	O
this	O
end	O
,	O
we	O
present	O
a	O
structure	O
-	O
aware	O
sequence	O
-	O
to	O
-	O
sequence	O
model	O
,	O
in	O
which	O
we	O
equip	O
abstractive	O
conversation	O
summarization	B-TaskName
models	O
with	O
rich	O
conversation	O
structures	O
through	O
two	O
types	O
of	O
graphs	O
:	O
discourse	O
relation	O
graph	O
and	O
action	O
graph	O
.	O
Discourse	O
relation	O
graphs	O
are	O
constructed	O
based	O
on	O
dependency	O
-	O
based	O
discourse	O
relations	O
(	O
Kirschner	O
et	O
al	O
,	O
2012	O
;	O
Stone	O
et	O
al	O
,	O
2013	O
;	O
Asher	O
et	O
al	O
,	O
2016	O
;	O
Qin	O
et	O
al	O
,	O
2017	O
)	O
between	O
intertwined	O
utterances	O
,	O
where	O
each	O
Elementary	O
Discourse	O
Unit	O
(	O
EDU	O
)	O
is	O
one	O
single	O
utterance	O
and	O
they	O
are	O
linked	O
through	O
16	O
different	O
types	O
of	O
relations	O
(	O
Asher	O
et	O
al	O
,	O
2016	O
)	O
.	O
As	O
shown	O
in	O
Figure	O
1	O
(	O
a	O
)	O
,	O
highly	O
related	O
utterances	O
are	O
linked	O
based	O
on	O
discourse	O
relations	O
like	O
Question	O
Answer	O
Pairs	O
,	O
Comment	O
and	O
Explanation	O
.	O
Explicitly	O
modeling	O
these	O
utterances	O
relations	O
in	O
conversations	O
can	O
aid	O
models	O
in	O
recognizing	O
key	O
content	O
for	O
succinct	O
and	O
informative	O
summarization	B-TaskName
.	O
Action	O
graphs	O
are	O
constructed	O
as	O
the	O
"	O
WHO	O
-	O
DOING	O
-	O
WHAT	O
"	O
triplets	O
in	O
conversations	O
which	O
express	O
socially	O
situated	O
identities	O
and	O
activities	O
(	O
Gee	O
,	O
2014	O
)	O
.	O
For	O
instance	O
,	O
in	O
Figure	O
1	O
(	O
b	O
)	O
,	O
the	O
action	O
graph	O
provides	O
explicit	O
information	O
between	O
Simon	O
,	O
fetch	O
,	O
and	O
tissues	O
for	O
the	O
utterance	O
it	O
is	O
Simon	O
who	O
will	O
fetch	O
the	O
tissues	O
,	O
making	O
models	O
less	O
likely	O
to	O
generate	O
summaries	O
with	O
wrong	O
references	O
(	O
e.g.	O
,	O
Helen	B-DatasetName
will	O
fetch	O
the	O
tissues	O
)	O
.	O
To	O
sum	O
up	O
,	O
our	O
contributions	O
are	O
:	O
(	O
1	O
)	O
We	O
pro	O
-	O
pose	O
to	O
utilize	O
discourse	O
relation	O
graphs	O
and	O
action	O
graphs	O
to	O
better	O
encode	O
conversations	O
for	O
conversation	O
summarization	B-TaskName
.	O
(	O
2	O
)	O
We	O
design	O
structureaware	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
to	O
combine	O
these	O
structured	O
graphs	O
and	O
generate	O
summaries	O
with	O
the	O
help	O
of	O
a	O
novel	O
multi	O
-	O
granularity	O
decoder	O
.	O
(	O
3	O
)	O
We	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
proposed	O
methods	O
through	O
experiments	O
on	O
a	O
largescale	O
conversation	O
summarization	B-TaskName
dataset	O
,	O
SAM	O
-	O
Sum	O
(	O
Gliwa	O
et	O
al	O
,	O
2019	O
)	O
.	O
(	O
4	O
)	O
We	O
further	O
show	O
that	O
our	O
structure	O
-	O
aware	O
models	O
can	O
generalize	O
well	O
in	O
new	O
domains	O
such	O
as	O
debate	O
summarization	B-TaskName
.	O

Document	B-TaskName
Summarization	I-TaskName
Compared	O
to	O
extractive	B-TaskName
document	I-TaskName
summarization	I-TaskName
(	O
Gupta	O
and	O
Lehal	O
,	O
2010	O
;	O
Narayan	O
et	O
al	O
,	O
2018	O
;	O
Liu	O
and	O
Lapata	O
,	O
2019	O
)	O
,	O
abstractive	O
document	B-TaskName
summarization	I-TaskName
is	O
generally	O
considered	O
more	O
challenging	O
and	O
has	O
received	O
more	O
attention	O
.	O
Various	O
methods	O
have	O
been	O
designed	O
to	O
tackle	O
abstractive	O
document	B-TaskName
summarization	I-TaskName
like	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
(	O
Rush	O
et	O
al	O
,	O
2015	O
)	O
,	O
pointer	O
generators	O
(	O
See	O
et	O
al	O
,	O
2017	O
)	O
,	O
reinforcement	O
learning	O
methods	O
(	O
Paulus	O
et	O
al	O
,	O
2018	O
;	O
and	O
pre	O
-	O
trained	O
models	O
(	O
Lewis	O
et	O
al	O
,	O
2020	O
;	O
.	O
To	O
generate	O
faithful	O
abstractive	O
document	O
summaries	O
(	O
Maynez	O
et	O
al	O
,	O
2020	O
)	O
,	O
graphbased	O
models	O
were	O
introduced	O
recently	O
such	O
as	O
extracting	O
entity	O
types	O
(	O
Fernandes	O
et	O
al	O
,	O
2018	O
;	O
,	O
leveraging	O
knowledge	B-TaskName
graphs	I-TaskName
Zhu	O
et	O
al	O
,	O
2020a	O
)	O
or	O
designing	O
extra	O
fact	O
correction	O
modules	O
.	O
Inspired	B-DatasetName
by	O
these	O
graph	O
-	O
based	O
methods	O
,	O
we	O
also	O
construct	O
action	O
graphs	O
for	O
generating	O
more	O
factual	O
conversation	O
summaries	O
.	O
Conversation	O
Summarization	B-TaskName
Extractive	O
dialogue	O
summarization	B-TaskName
(	O
Murray	O
et	O
al	O
,	O
2005	O
)	O
has	O
been	O
studied	O
extensively	O
via	O
statistical	O
machine	O
learning	O
methods	O
such	O
as	O
skip	O
-	O
chain	O
CRFs	O
(	O
Galley	O
,	O
2006	O
)	O
,	O
SVM	B-MethodName
with	O
LDA	B-MethodName
models	O
(	O
Wang	O
and	O
Cardie	O
,	O
2013	O
)	O
,	O
and	O
multi	O
-	O
sentence	B-DatasetName
compression	I-DatasetName
algorithms	O
(	O
Shang	O
et	O
al	O
,	O
2018	O
)	O
.	O
Such	O
methods	O
struggled	O
with	O
generating	O
succinct	O
,	O
fluent	O
,	O
and	O
natural	O
summaries	O
,	O
especially	O
when	O
the	O
key	O
information	O
needs	O
to	O
be	O
aggregated	O
from	O
multiple	O
first	O
-	O
person	O
point	O
-	O
of	O
-	O
view	O
utterances	O
(	O
Song	O
et	O
al	O
,	O
2020	O
)	O
.	O
Abstractive	O
conversation	O
summarization	B-TaskName
overcomes	O
these	O
issues	O
by	O
designing	O
hierarchical	O
models	O
,	O
incorporating	O
commonsense	O
knowledge	O
(	O
Feng	O
et	O
al	O
,	O
2020	O
)	O
,	O
or	O
leveraging	O
conversational	O
structures	O
like	O
dialogue	O
acts	O
(	O
Goo	O
and	O
Chen	O
,	O
2018	O
)	O
,	O
key	O
point	O
sequences	O
(	O
Liu	O
et	O
al	O
,	O
2019a	O
)	O
,	O
topic	O
segments	O
(	O
Liu	O
et	O
al	O
,	O
2019b	O
;	O
and	O
stage	O
developments	O
(	O
Chen	O
and	O
Yang	O
,	O
2020	O
)	O
.	O
Some	O
recent	O
research	O
has	O
also	O
utilized	O
discourse	O
relations	O
as	O
input	O
features	O
in	O
classifiers	O
to	O
detect	O
important	O
content	O
in	O
conversations	O
(	O
Murray	O
et	O
al	O
,	O
2006	O
;	O
Bui	O
et	O
al	O
,	O
2009	O
;	O
Qin	O
et	O
al	O
,	O
2017	O
)	O
.	O
However	O
,	O
current	O
models	O
still	O
have	O
not	O
explicitly	O
utilized	O
the	O
dependencies	O
between	O
different	O
utterances	O
,	O
making	O
models	O
hard	O
to	O
leverage	O
long	O
-	O
range	O
dependencies	O
and	O
utilize	O
these	O
salient	O
utterances	O
.	O
Moreover	O
,	O
less	O
attention	O
has	O
been	O
paid	O
to	O
identify	O
the	O
actions	O
of	O
different	O
speakers	O
and	O
how	O
they	O
interact	O
with	O
or	O
refer	O
to	O
each	O
other	O
,	O
leading	O
to	O
unfaithful	O
summarization	B-TaskName
with	O
incorrect	O
references	O
or	O
wrong	O
reasoning	O
(	O
Gliwa	O
et	O
al	O
,	O
2019	O
)	O
.	O
To	O
fill	O
these	O
gaps	O
,	O
we	O
propose	O
to	O
explicitly	O
model	O
actions	O
within	O
utterances	O
,	O
and	O
relations	O
between	O
utterances	O
in	O
conversations	O
in	O
a	O
structured	O
way	O
,	O
by	O
using	O
discourse	O
relation	O
graphs	O
and	O
action	O
graphs	O
and	O
further	O
combining	O
these	O
through	O
relational	O
graph	O
encoders	O
and	O
multigranularity	O
decoders	O
for	O
abstractive	O
conversation	O
summarization	B-TaskName
.	O

The	O
"	O
who	O
-	O
doing	O
-	O
what	O
"	O
triples	O
from	O
utterances	O
can	O
provide	O
explicit	O
visualizations	O
of	O
speakers	O
and	O
their	O
actions	O
,	O
the	O
key	O
to	O
understanding	O
concrete	O
details	O
happened	O
in	O
conversations	O
(	O
Moser	O
,	O
2001	O
;	O
Gee	O
,	O
2014	O
;	O
Sacks	O
et	O
al	O
,	O
1978	O
)	O
.	O
Simply	O
relying	O
on	O
neural	O
models	O
to	O
identify	O
this	O
information	O
from	O
conversations	O
often	O
fail	O
to	O
produce	O
factual	O
characterizations	O
of	O
concrete	O
details	O
happened	O
(	O
Cao	O
et	O
al	O
,	O
2018	O
;	O
.	O
To	O
this	O
end	O
,	O
we	O
extract	O
"	O
WHO	O
-	O
DOING	O
-	O
WHAT	O
"	O
triples	O
from	O
utterances	O
and	O
construct	O
action	O
graphs	O
for	O
conversation	O
summarization	B-TaskName
Huang	O
et	O
al	O
,	O
2020b	O
,	O
a	O
)	O
.	O
Specifically	O
,	O
we	O
first	O
transform	O
the	O
first	O
-	O
person	O
point	O
-	O
of	O
-	O
view	O
utterances	O
to	O
its	O
thirdperson	O
point	O
-	O
of	O
-	O
view	O
forms	O
based	O
on	O
simple	O
rules	O
:	O
(	O
i	O
)	O
substituting	O
first	O
/	O
second	O
-	O
person	O
pronouns	O
with	O
the	O
names	O
of	O
current	O
speaker	O
or	O
surrounding	O
speakers	O
and	O
(	O
ii	O
)	O
replacing	O
third	O
-	O
person	O
pronouns	O
based	O
on	O
coreference	O
clusters	O
in	O
conversations	O
detected	O
by	O
the	O
Stanford	O
CoreNLP	O
(	O
Manning	O
et	O
al	O
,	O
2014	O
)	O
.	O
For	O
example	O
,	O
an	O
utterance	O
"	O
I	O
'll	O
bring	O
it	O
to	O
you	O
to	O
-	O
morrow	O
"	O
from	O
Amanda	O
to	O
Jerry	O
will	O
be	O
transformed	O
into	O
"	O
Amanda'll	O
bring	O
cakes	O
to	O
Jerry	O
tomorrow	O
"	O
.	O
Then	O
we	O
extract	O
"	O
WHO	O
-	O
DOING	O
-	O
WHAT	O
"	O
(	O
subjectpredicate	O
-	O
object	O
)	O
triples	O
from	O
transformed	O
conversations	O
using	O
the	O
open	B-TaskName
information	I-TaskName
extraction	I-TaskName
(	O
Ope	O
-	O
nIE	O
)	O
systems	O
1	O
(	O
Angeli	O
et	O
al	O
,	O
2015	O
)	O
.	O
We	O
then	O
construct	O
the	O
Action	O
Graph	O
G	O
A	O
=	O
(	O
V	O
A	O
,	O
E	O
A	O
)	O
from	O
the	O
extracted	O
triples	O
by	O
taking	O
arguments	O
(	O
"	O
WHO	O
"	O
,	O
"	O
DOING	O
"	O
,	O
or	O
"	O
WHAT	O
"	O
)	O
as	O
nodes	O
in	O
V	O
A	O
,	O
and	O
connect	O
them	O
with	O
edge	O
E	O
A	O
[	O
i	O
]	O
[	O
j	O
]	O
=	O
1	O
if	O
they	O
are	O
adjacent	O
in	O
one	O
"	O
WHO	O
-	O
DOING	O
-	O
WHAT	O
"	O
triple	O
.	O

We	O
initialize	O
our	O
utterance	O
encoder	O
F	O
U	O
(	O
.	O
)	O
with	O
a	O
pre	O
-	O
trained	O
encoder	O
,	O
i.e.	O
,	O
BART	B-MethodName
-	O
base	O
(	O
Lewis	O
et	O
al	O
,	O
2020	O
)	O
,	O
and	O
encode	O
tokens	O
{	O
x	O
i	O
,	O
0	B-DatasetName
,	O
...	O
,	O
x	O
i	O
,	O
l	O
}	O
in	O
an	O
utterance	O
u	O
i	O
into	O
its	O
hidden	O
representation	O
:	O
{	O
h	O
U	O
i	O
,	O
0	B-DatasetName
,	O
...	O
,	O
h	O
U	O
i	O
,	O
l	O
}	O
=	O
F	O
U	O
(	O
{	O
x	O
i	O
,	O
0	B-DatasetName
,	O
...	O
,	O
x	O
i	O
,	O
l	O
}	O
)	O
(	O
1	O
)	O
Here	O
we	O
add	O
a	O
special	O
token	O
x	O
i	O
,	O
0	B-DatasetName
=	O
<	O
S	O
>	O
at	O
the	O
beginning	O
of	O
each	O
utterance	O
to	O
represent	O
it	O
.	O

We	O
trained	O
and	O
evaluated	O
our	O
models	O
on	O
a	O
conversation	O
summarization	B-TaskName
dataset	O
SAMSum	B-DatasetName
(	O
Gliwa	O
et	O
al	O
,	O
2019	O
)	O
covering	O
messenger	O
-	O
like	O
conversations	O
about	O
daily	O
topics	O
,	O
such	O
as	O
arranging	O
meetings	O
and	O
discussing	O
events	O
.	O
We	O
also	O
showed	O
the	O
generalizability	O
of	O
our	O
models	O
on	O
the	O
Argumentative	O
Dialogue	O
Summary	O
Corpus	O
(	O
ADSC	O
)	O
(	O
Misra	O
et	O
al	O
,	O
2015	O
)	O
,	O
a	O
debate	O
summarization	B-TaskName
corpus	O
.	O
The	O
data	O
statistics	O
of	O
two	O
datasets	O
were	O
shown	O
in	O
Table	O
1	O
,	O
with	O
the	O
discourse	O
relation	O
types	O
distributions	O
in	O
the	O
Appendix	O
.	O

We	O
compare	O
our	O
methods	O
with	O
several	O
baselines	O
:	O
Pointer	O
Generator	O
(	O
See	O
et	O
al	O
,	O
2017	O
)	O
:	O
We	O
followed	O
the	O
settings	O
in	O
Gliwa	O
et	O
al	O
(	O
2019	O
)	O
and	O
used	O
special	O
tokens	O
to	O
separate	O
each	O
utterance	O
.	O
Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
:	O
We	O
trained	O
transformer	O
seq2seq	B-MethodName
models	O
following	O
the	O
OpenNMT	O
(	O
Klein	O
et	O
al	O
,	O
2017	O
)	O
.	O
D	O
-	O
HGN	O
(	O
Feng	O
et	O
al	O
,	O
2020	O
)	O
incorporated	O
commonsense	O
knowledge	O
from	O
ConceptNet	B-DatasetName
(	O
Liu	O
and	O
Singh	O
,	O
2004	O
)	O
for	O
dialogue	O
summarization	B-TaskName
.	O
(	O
Dror	O
et	O
al	O
,	O
2018	O
)	O
and	O
found	O
that	O
S	O
-	O
BART	B-MethodName
w.	O
Discourse	O
&	O
Action	O
significantly	O
outperformed	O
the	O
base	O
BART	B-MethodName
(	O
p	O
<	O
0.05	O
)	O
.	O
BART	B-MethodName
(	O
Lewis	O
et	O
al	O
,	O
2020	O
)	O
:	O
We	O
utilized	O
BART	B-MethodName
2	O
,	O
and	O
separated	O
utterances	O
by	O
a	O
special	O
token	O
.	O
Multi	O
-	O
View	O
Seq2Seq	B-MethodName
(	O
Chen	O
and	O
Yang	O
,	O
2020	O
)	O
utilized	O
topic	O
and	O
stage	O
views	O
on	O
top	O
of	O
BART	B-MethodName
for	O
summarizing	O
conversations	O
.	O
Here	O
we	O
implemented	O
it	O
based	O
on	O
BART	B-MethodName
-	O
base	O
models	O
.	O

We	O
conducted	O
human	O
evaluation	O
to	O
qualitatively	O
evaluate	O
the	O
generated	O
summaries	O
.	O
Specifically	O
,	O
we	O
asked	O
annotators	O
from	O
Amazon	O
Mechanical	O
Turk	O
to	O
score	O
a	O
set	O
of	O
randomly	O
sampled	O
100	O
generated	O
summaries	O
from	O
ground	O
-	O
truth	O
,	O
BART	B-MethodName
and	O
our	O
structured	O
models	O
,	O
using	O
a	O
Likert	O
scale	O
from	O
1	O
(	O
worst	O
)	O
to	O
5	O
(	O
best	O
)	O
in	O
terms	O
of	O
factualness	O
(	O
e.g.	O
,	O
associates	O
actions	O
with	O
the	O
right	O
actors	O
)	O
,	O
succinctness	O
(	O
e.g.	O
,	O
does	O
not	O
contain	O
redundant	O
information	O
)	O
,	O
and	O
informativeness	O
(	O
e.g.	O
,	O
covers	O
the	O
most	O
important	O
content	O
)	O
(	O
Feng	O
et	O
al	O
,	O
2020	O
;	O
.	O
To	O
increase	O
annotation	O
quality	O
,	O
we	O
required	O
turkers	O
to	O
have	O
a	O
98	O
%	O
approval	O
rate	O
and	O
at	O
least	O
10	O
,	O
000	O
approved	O
tasks	O
for	O
their	O
previous	O
work	O
.	O
Each	O
message	O
was	O
rated	O
by	O
three	O
workers	O
.	O
The	O
scores	O
for	O
each	O
summary	O
were	O
averaged	O
.	O
The	O
Intra	O
-	O
Class	O
Correlation	O
was	O
0.543	O
,	O
showing	O
moderate	O
agreement	O
(	O
Koo	O
and	O
Li	O
,	O
2016	O
)	O
.	O
As	O
shown	O
in	O
Table	O
4	O
,	O
S	O
-	O
BART	B-MethodName
that	O
utilized	O
structured	O
information	O
from	O
discourse	O
relation	O
graphs	O
and	O
action	O
graphs	O
generated	O
significantly	O
better	O
summaries	O
with	O
respect	O
to	O
factualness	O
,	O
succinctness	O
,	O
and	O
informativeness	O
.	O
This	O
might	O
because	O
that	O
the	O
incorporation	O
of	O
structured	O
information	O
such	O
as	O
discourse	O
relations	O
helped	O
S	O
-	O
BART	B-MethodName
to	O
recognize	O
the	O
salient	O
parts	O
in	O
conversations	O
,	O
and	O
thus	O
improve	O
the	O
succinctness	O
and	O
informativeness	O
over	O
BART	B-MethodName
.	O
Modeling	O
the	O
connections	O
between	O
speakers	O
and	O
actions	O
greatly	O
helped	O
generate	O
more	O
factual	O
summaries	O
than	O
the	O
baselines	O
,	O
e.g.	O
,	O
with	O
an	O
increase	O
of	O
0.27	O
from	O
BART	B-MethodName
to	O
S	O
-	O
BART	B-MethodName
w.	O
Action	O
.	O

In	O
this	O
work	O
,	O
we	O
introduced	O
a	O
structure	O
-	O
aware	O
sequence	O
-	O
to	O
-	O
sequence	O
model	O
for	O
abstractive	O
conversation	O
summarization	B-TaskName
by	O
incorporating	O
discourse	O
relations	O
between	O
utterances	O
,	O
and	O
the	O
connections	O
between	O
speakers	O
and	O
actions	O
within	O
utterances	O
.	O
Experiments	O
and	O
ablation	O
studies	O
on	O
SAMSum	B-DatasetName
corpus	I-DatasetName
showed	O
the	O
effectiveness	O
of	O
these	O
structured	O
graphs	O
in	O
aiding	O
the	O
task	O
of	O
conversation	O
summarization	B-TaskName
via	O
both	O
quantitative	O
and	O
qualitative	O
eval	O
-	O
uation	O
metrics	O
.	O
Results	O
in	O
zero	O
-	O
shot	O
settings	O
on	O
ADCS	O
Corpus	O
further	O
demonstrated	O
the	O
generalizability	O
of	O
our	O
structure	O
-	O
aware	O
models	O
.	O
In	O
the	O
future	O
,	O
we	O
plan	O
to	O
extend	O
our	O
current	O
conversation	O
summarization	B-TaskName
models	O
for	O
various	O
application	O
domains	O
such	O
as	O
emails	O
,	O
debates	O
,	O
and	O
podcasts	O
,	O
and	O
in	O
conversations	O
that	O
might	O
involve	O
longer	O
utterances	O
and	O
more	O
participants	O
in	O
an	O
unsynchronized	O
way	O
.	O
(	O
Asher	O
et	O
al	O
,	O
2016	O
)	O
with	O
default	O
settings	O
5	O
to	O
get	O
the	O
link	B-TaskName
prediction	I-TaskName
and	O
relation	B-TaskName
classification	I-TaskName
models	O
to	O
label	O
discourse	O
relations	O
in	O
SAMSum	B-DatasetName
and	O
ADSC	O
corpus	O
.	O
The	O
distribution	O
of	O
the	O
relation	O
types	O
in	O
two	O
datasets	O
were	O
shown	O
in	O
Table	O
9	O
.	O
The	O
major	O
discourse	O
relations	O
in	O
daily	O
conversations	O
are	O
Comment	O
,	O
Clarification	O
and	O
QA	O
pairs	O
,	O
while	O
the	O
main	O
discourse	O
relations	O
in	O
debate	O
are	O
Comment	O
,	O
Contrast	O
,	O
Clarification	O
and	O
QA	O
pairs	O
.	O

We	O
would	O
like	O
to	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
helpful	O
comments	O
,	O
and	O
the	O
members	O
of	O
Georgia	O
Tech	O
SALT	O
group	O
for	O
their	O
feedback	O
.	O
This	O
work	O
is	O
supported	O
in	O
part	O
by	O
grants	O
from	O
Google	B-DatasetName
,	O
Amazon	O
and	O
Salesforce	O
.	O

SeqMix	O
:	O
Augmenting	O
Active	O
Sequence	O
Labeling	O
via	O
Sequence	O
Mixup	B-MethodName

Active	B-TaskName
learning	I-TaskName
is	O
an	O
important	O
technique	O
for	O
low	O
-	O
resource	O
sequence	O
labeling	O
tasks	O
.	O
However	O
,	O
current	O
active	O
sequence	O
labeling	O
methods	O
use	O
the	O
queried	O
samples	O
alone	O
in	O
each	O
iteration	O
,	O
which	O
is	O
an	O
inefficient	O
way	O
of	O
leveraging	O
human	O
annotations	O
.	O
We	O
propose	O
a	O
simple	O
but	O
effective	O
data	B-TaskName
augmentation	I-TaskName
method	O
to	O
improve	O
label	O
efficiency	O
of	O
active	O
sequence	O
labeling	O
.	O
Our	O
method	O
,	O
SeqMix	O
,	O
simply	O
augments	O
the	O
queried	O
samples	O
by	O
generating	O
extra	O
labeled	O
sequences	O
in	O
each	O
iteration	O
.	O
The	O
key	O
difficulty	O
is	O
to	O
generate	O
plausible	O
sequences	O
along	O
with	O
token	O
-	O
level	O
labels	O
.	O
In	O
SeqMix	O
,	O
we	O
address	O
this	O
challenge	O
by	O
performing	O
mixup	B-MethodName
for	O
both	O
sequences	O
and	O
token	O
-	O
level	O
labels	O
of	O
the	O
queried	O
samples	O
.	O
Furthermore	O
,	O
we	O
design	O
a	O
discriminator	O
during	O
sequence	O
mixup	B-MethodName
,	O
which	O
judges	O
whether	O
the	O
generated	O
sequences	O
are	O
plausible	O
or	O
not	O
.	O
Our	O
experiments	O
on	O
Named	B-TaskName
Entity	I-TaskName
Recognition	I-TaskName
and	O
Event	B-TaskName
Detection	I-TaskName
tasks	O
show	O
that	O
SeqMix	O
can	O
improve	O
the	O
standard	O
active	O
sequence	O
labeling	O
method	O
by	O
2.27	O
%	O
-	O
3.75	O
%	O
in	O
terms	O
of	O
F	O
1	O
scores	O
.	O
The	O
code	O
and	O
data	O
for	O
SeqMix	O
can	O
be	O
found	O
at	O
https://github	O
.	O
com	O
/	O
rz	O
-	O
zhang	O
/	O
SeqMix	O
.	O

Mixup	B-MethodName
in	O
the	O
Embedding	O
Space	O
Mixup	B-MethodName
in	O
the	O
Label	O
Space	O
(	O
c	O
)	O
Label	O
-	O
constrained	O
sub	O
-	O
sequence	O
mixup	B-MethodName
Figure	O
1	O
:	O
Illustration	O
of	O
the	O
three	O
variants	O
of	O
SeqMix	O
.	O
We	O
use	O
s	O
=	O
5	O
,	O
η	O
0	B-DatasetName
=	O
3	O
5	O
for	O
whole	O
-	O
sequence	O
mixup	B-MethodName
and	O
s	O
=	O
3	O
,	O
η	O
0	B-DatasetName
=	O
2	O
3	O
for	O
sub	O
-	O
sequence	O
mixup	B-MethodName
and	O
label	O
-	O
constrained	O
sub	O
-	O
sequence	O
mixup	B-MethodName
.	O
The	O
solid	O
red	O
frames	O
indicate	O
paired	O
sequences	O
or	O
sub	O
-	O
sequences	O
,	O
and	O
the	O
red	O
dotted	O
frames	O
indicate	O
generated	O
sequence	O
or	O
sub	O
-	O
sequence	O
.	O
In	O
the	O
original	O
sequences	O
,	O
the	O
parts	O
not	O
included	O
in	O
the	O
solid	O
red	O
frames	O
will	O
be	O
unchanged	O
in	O
the	O
generated	O
sequences	O
.	O
For	O
the	O
mixup	B-MethodName
in	O
the	O
embedding	O
space	O
,	O
we	O
take	O
the	O
embedding	O
in	O
E	O
which	O
is	O
closest	O
to	O
the	O
raw	O
mixed	O
embedding	O
as	O
the	O
generated	O
embedding	O
.	O
For	O
the	O
mixup	B-MethodName
in	O
the	O
label	O
space	O
,	O
the	O
mixed	O
label	O
can	O
be	O
used	O
as	O
the	O
pseudo	O
label	O
.	O
version	O
is	O
called	O
label	O
-	O
constrained	O
sub	O
-	O
sequence	O
mixup	B-MethodName
.	O
Comparing	O
the	O
three	O
variants	O
,	O
label	O
-	O
constrained	O
sub	O
-	O
sequence	O
mixup	B-MethodName
gives	O
the	O
most	O
restrictions	O
to	O
pairing	O
parent	O
samples	O
,	O
sub	O
-	O
sequence	O
mixup	B-MethodName
sets	O
the	O
sub	O
-	O
sequence	O
-	O
level	O
pattern	O
,	O
while	O
wholesequence	O
mixup	B-MethodName
just	O
requires	O
η	O
≥	O
η	O
0	B-DatasetName
for	O
the	O
sequences	O
with	O
the	O
same	O
length	O
.	O

Active	O
Sequence	O
Labeling	O
Sequence	O
labeling	O
has	O
been	O
studied	O
extensively	O
for	O
different	O
NLP	O
problems	O
.	O
Different	O
neural	O
architectures	O
has	O
been	O
proposed	O
(	O
Huang	O
et	O
al	O
,	O
2015	O
;	O
Lample	O
et	O
al	O
,	O
2016	O
;	O
Peters	O
et	O
al	O
,	O
2018	O
;	O
Akbik	O
et	O
al	O
,	O
2018	O
)	O
in	O
recent	O
years	O
,	O
which	O
have	O
achieved	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
a	O
number	O
of	O
sequence	O
labeling	O
tasks	O
.	O
However	O
,	O
these	O
neural	O
models	O
usually	O
require	O
exhaustive	O
human	O
efforts	O
for	O
generating	O
labels	O
for	O
each	O
token	O
,	O
and	O
may	O
not	O
perform	O
well	O
in	O
lowresource	O
settings	O
.	O
To	O
improve	O
the	O
performance	O
of	O
low	O
-	O
resource	O
sequence	O
labeling	O
,	O
several	O
approaches	O
have	O
been	O
applied	O
including	O
using	O
semi	O
-	O
supervised	O
methods	O
Chen	O
et	O
al	O
,	O
2020b	O
)	O
,	O
external	O
weak	O
supervision	O
(	O
Lison	O
et	O
al	O
,	O
2020	O
;	O
Liang	O
et	O
al	O
,	O
2020	O
;	O
Ren	O
et	O
al	O
,	O
2020	O
;	O
Zhang	O
et	O
al	O
,	O
2019	O
;	O
Yu	O
et	O
al	O
,	O
2020	O
)	O
and	O
active	B-TaskName
learning	I-TaskName
(	O
Shen	O
et	O
al	O
,	O
2017	O
;	O
Hazra	O
et	O
al	O
,	O
2019	O
;	O
Liu	O
et	O
al	O
,	O
2018	O
;	O
Fang	O
et	O
al	O
,	O
2017	O
;	O
Gao	O
et	O
al	O
,	O
2019	O
)	O
.	O
In	O
this	O
study	O
,	O
we	O
mainly	O
focus	O
on	O
active	B-TaskName
learning	I-TaskName
approaches	O
which	O
select	O
samples	O
based	O
on	O
the	O
query	O
policy	O
design	O
.	O
So	O
far	O
,	O
various	O
uncertainty	O
-	O
based	O
(	O
Scheffer	O
et	O
al	O
,	O
2001	O
;	O
Culotta	O
and	O
McCallum	O
,	O
2005	O
;	O
Kim	O
et	O
al	O
,	O
2006	O
)	O
and	O
committee	O
-	O
based	O
approaches	O
(	O
Dagan	O
and	O
Engelson	O
,	O
1995	O
)	O
have	O
been	O
proposed	O
for	O
improving	O
the	O
sample	O
efficiency	O
.	O
More	O
recently	O
,	O
Shen	O
et	O
al	O
(	O
2017	O
)	O
;	O
Hazra	O
et	O
al	O
(	O
2019	O
)	O
;	O
Liu	O
et	O
al	O
(	O
2018	O
)	O
;	O
Fang	O
et	O
al	O
(	O
2017	O
)	O
further	O
improve	O
the	O
aforementioned	O
active	B-TaskName
learning	I-TaskName
approaches	O
to	O
improve	O
the	O
sampling	O
diversity	O
as	O
well	O
as	O
the	O
generalization	O
ability	O
of	O
models	O
on	O
low	O
-	O
resource	O
scenarios	O
.	O
These	O
works	O
mainly	O
claim	O
the	O
sample	O
efficiency	O
provided	O
by	O
the	O
active	B-TaskName
learning	I-TaskName
approach	O
but	O
do	O
not	O
study	B-DatasetName
data	I-DatasetName
augmentation	O
for	O
active	O
sequence	O
labeling	O
.	O
Interpolation	O
-	O
based	O
Regularizations	O
Mixup	B-MethodName
implements	O
interpolation	O
in	O
the	O
input	O
space	O
to	O
regularize	O
models	O
(	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
.	O
Recently	O
,	O
the	O
Mixup	B-MethodName
variants	O
(	O
Verma	O
et	O
al	O
,	O
2019	O
;	O
Summers	O
and	O
Dinneen	O
,	O
2019	O
;	O
Guo	O
et	O
al	O
,	O
2019b	O
)	O
turn	O
to	O
perform	O
interpolation	O
in	O
the	O
hidden	O
space	O
to	O
capture	O
higher	O
-	O
level	O
information	O
.	O
Guo	O
et	O
al	O
(	O
2019a	O
)	O
;	O
Chen	O
et	O
al	O
(	O
2020a	O
)	O
apply	O
hidden	O
-	O
space	O
Mixup	B-MethodName
for	O
text	B-TaskName
classification	I-TaskName
.	O
These	O
works	O
,	O
however	O
,	O
have	O
not	O
explored	O
how	O
to	O
perform	O
mixup	B-MethodName
for	O
sequences	O
with	O
token	O
-	O
level	O
labels	O
,	O
nor	O
do	O
they	O
consider	O
the	O
quality	O
of	O
the	O
mixed	O
-	O
up	O
samples	O
.	O
Text	O
Augmentation	O
Our	O
work	O
is	O
also	O
related	O
to	O
text	O
data	B-TaskName
augmentation	I-TaskName
.	O
Zhang	O
et	O
al	O
(	O
2015	O
)	O
;	O
Wei	O
and	O
Zou	O
(	O
2019	O
)	O
utilize	O
heuristic	O
approaches	O
including	O
synonym	O
replancement	O
,	O
random	O
insertion	O
,	O
swap	O
and	O
deletion	O
for	O
text	O
augmentation	O
,	O
Kafle	O
et	O
al	O
(	O
2017	O
)	O
;	O
Silfverberg	O
et	O
al	O
(	O
2017	O
)	O
employ	O
heuristic	O
rules	O
based	O
on	O
specific	O
task	O
,	O
Hu	O
et	O
al	O
(	O
2017	O
)	O
propose	O
to	O
augment	O
text	O
data	O
in	O
an	O
encoder	O
-	O
decoder	O
manner	O
.	O
Very	O
recently	O
,	O
(	O
Anaby	O
-	O
Tavor	O
et	O
al	O
,	O
2020	O
;	O
Kobayashi	O
,	O
2018	O
)	O
harness	O
the	O
power	O
of	O
pre	O
-	O
trained	O
language	O
models	O
and	O
augmenting	O
the	O
text	O
data	O
based	O
on	O
contextual	O
patterns	O
.	O
Although	O
these	O
methods	O
can	O
augment	O
the	O
training	O
set	O
and	O
improve	O
the	O
performance	O
of	O
text	B-TaskName
classification	I-TaskName
model	O
,	O
they	O
fail	O
to	O
generate	O
sequences	O
and	O
labels	O
simultaneously	O
,	O
thus	O
can	O
not	O
be	O
adapted	O
to	O
our	O
problem	O
where	O
tokenlevel	O
labels	O
are	O
required	O
during	O
training	O
.	O
Instead	O
,	O
in	O
our	O
study	O
,	O
we	O
propose	O
a	O
new	O
framework	O
SeqMix	O
for	O
data	B-TaskName
augmentation	I-TaskName
to	O
facilitate	O
sequence	O
labeling	O
task	O
.	O
Our	O
method	O
can	O
generate	O
token	O
-	O
level	O
labels	O
and	O
preserve	O
the	O
semantic	O
information	O
in	O
the	O
augmented	O
sentences	O
.	O
Moreover	O
,	O
it	O
can	O
be	O
naturally	O
combined	O
with	O
existing	O
active	B-TaskName
learning	I-TaskName
approaches	O
and	O
further	O
promote	O
the	O
performance	O
.	O

We	O
propose	O
a	O
simple	O
data	B-TaskName
augmentation	I-TaskName
method	O
SeqMix	O
to	O
enhance	O
active	O
sequence	O
labeling	O
.	O
By	O
performing	O
sequence	O
mixup	B-MethodName
in	O
the	O
latent	O
space	O
,	O
Se	O
-	O
qMix	O
improves	O
data	O
diversity	O
during	O
active	B-TaskName
learning	I-TaskName
,	O
while	O
being	O
able	O
to	O
generate	O
plausible	O
augmented	O
sequences	O
.	O
This	O
method	O
is	O
generic	O
to	O
different	O
active	B-TaskName
learning	I-TaskName
policies	O
and	O
various	O
sequence	O
labeling	O
tasks	O
.	O
Our	O
experiments	O
demonstrate	O
that	O
SeqMix	O
can	O
improve	O
active	B-TaskName
learning	I-TaskName
baselines	O
consistently	O
for	O
NER	B-TaskName
and	O
event	B-TaskName
detection	I-TaskName
tasks	O
;	O
and	O
its	O
benefits	O
are	O
especially	O
prominent	O
in	O
low	O
-	O
data	O
regimes	O
.	O
For	O
future	O
research	O
,	O
it	O
is	O
interesting	O
to	O
enhance	O
SeqMix	O
with	O
language	O
models	O
during	O
the	O
mixup	B-MethodName
process	O
,	O
and	O
harness	O
external	O
knowledge	O
for	O
further	O
improving	O
diversity	O
and	O
plausibility	O
.	O

Here	O
we	O
list	O
the	O
link	O
to	O
datasets	O
used	O
in	O
our	O
experiments	O
.	O
CoNLL	O
-	O
03	O
:	O
https://github.com/	O
synalp	O
/	O
NER	B-TaskName
/	O
tree	O
/	O
master	O
/	O
corpus/	O
CoNLL	O
-	O
2003	O
.	O
ACE05	O
:	O
We	O
are	O
unable	O
to	O
provide	O
the	O
downloadable	O
version	O
due	O
to	O
it	O
is	O
not	O
public	O
.	O
This	O
corpus	O
can	O
be	O
applied	O
through	O
the	O
website	O
of	O
LDC	O
:	O
https://www.ldc.upenn.edu/	O
collaborations	O
/	O
past	O
-	O
projects/	O
ace	O
.	O
Webpage	O
:	O
Please	O
refer	O
the	O
link	O
in	O
the	O
paper	O
(	O
Ratinov	O
and	O
Roth	O
,	O
2009	O
)	O
.	O

All	O
the	O
mentioned	O
dataset	O
has	O
been	O
split	O
into	O
train	O
/	O
validate	O
/	O
test	O
set	O
in	O
the	O
released	O
version	O
.	O
We	O
keep	O
consistent	O
with	O
the	O
validation	O
set	O
and	O
the	O
test	O
set	O
in	O
our	O
experiment	O
.	O
For	O
the	O
active	B-TaskName
learning	I-TaskName
paradigm	O
,	O
we	O
split	O
the	O
training	O
set	O
as	O
Table	O
3	O
.	O
The	O
active	O
learners	O
are	O
initialized	O
on	O
the	O
seed	O
set	O
,	O
then	O
they	O
implement	O
5	O
active	B-TaskName
learning	I-TaskName
rounds	O
.	O

For	O
the	O
baselines	O
,	O
we	O
take	O
random	O
sampling	O
and	O
3	O
active	B-TaskName
learning	I-TaskName
approaches	O
-	O
LC	O
sampling	O
,	O
NTE	O
sampling	O
,	O
and	O
QBC	O
sampling	O
as	O
Section	O
2.2	O
.	O

We	O
implement	O
bert	O
-	O
base	O
-	O
cased	O
as	O
the	O
underlying	O
model	O
for	O
the	O
NER	B-TaskName
task	O
and	O
bert	O
-	O
base	O
-	O
multilingualcased	O
as	O
the	O
underlying	O
model	O
for	O
the	O
event	B-TaskName
detection	I-TaskName
task	O
.	O
We	O
use	O
the	O
model	O
from	O
Huggingface	O
Transformer	B-MethodName
codebase	O
3	O
,	O
and	O
the	O
repository	O
4	O
to	O
finetune	O
our	O
model	O
for	O
sequence	O
labeling	O
task	O
.	O

In	O
our	O
model	O
,	O
we	O
use	O
bert	O
-	O
base	O
-	O
cased	O
and	O
bertbase	O
-	O
multilingual	O
-	O
cased	O
both	O
of	O
them	O
occupy	O
12layer	O
,	O
768	O
-	O
hidden	O
,	O
12	O
-	O
heads	O
with	O
110	O
M	O
parameters	O
.	O
3	O
https://github.com/huggingface/	O
transformers	O
4	O
https://github.com/kamalkraj/BERT	O
-	O
NER	B-TaskName

In	O
Section	O
3.2	O
,	O
we	O
construct	O
a	O
table	O
of	O
tokens	O
W	O
and	O
their	O
corresponding	O
contextual	O
embedding	O
E.	O
For	O
our	O
underlying	O
BERT	B-MethodName
model	O
,	O
we	O
use	O
the	O
vocabulary	O
provided	O
by	O
the	O
tokenizer	O
to	O
build	O
up	O
W	O
,	O
and	O
the	O
embedding	O
initialized	O
on	O
the	O
training	O
set	O
as	O
E.	O
We	O
also	O
need	O
to	O
construct	O
a	O
special	O
token	O
collection	O
to	O
exclude	O
some	O
generation	O
in	O
the	O
process	O
of	O
sequence	O
mixing	O
.	O
For	O
example	O
,	O
BERT	B-MethodName
places	O
token	O
[	O
CLS	O
]	O
and	O
[	O
SEP	O
]	O
at	O
the	O
starting	O
position	O
and	O
the	O
ending	O
position	O
for	O
sentence	O
,	O
and	O
pad	O
the	O
inputs	O
with	O
[	O
PAD	B-DatasetName
]	O
.	O
We	O
exclude	O
these	O
disturbing	O
tokens	O
and	O
the	O
parent	O
tokens	O
.	O

For	O
the	O
5	O
-	O
round	O
active	B-TaskName
learning	I-TaskName
with	O
SeqMix	O
augmentation	O
,	O
our	O
program	O
runs	O
about	O
500	O
seconds	O
for	O
WebPage	O
dataset	O
,	O
1700	O
seconds	O
for	O
the	O
CoNLL	O
slicing	O
dataset	O
,	O
and	O
3.5	O
hours	O
for	O
ACE	B-DatasetName
2005	I-DatasetName
.	O
If	O
the	O
QBC	O
query	O
policy	O
used	O
,	O
all	O
the	O
runtime	O
will	O
be	O
multiplied	O
about	O
3	O
times	O
.	O

Social	O
media	O
has	O
lately	O
become	O
one	O
of	O
the	O
primary	O
venues	O
where	O
users	O
express	O
their	O
opinions	O
about	O
various	O
products	O
and	O
services	O
.	O
These	O
opinions	O
are	O
extremely	O
useful	O
in	O
understanding	O
the	O
user	O
's	O
perceptions	O
and	O
sentiment	O
about	O
these	O
services	O
.	O
They	O
are	O
also	O
useful	O
in	O
identifying	O
potential	O
defects	O
(	O
Abrahams	O
et	O
al	O
,	O
2012	O
)	O
and	O
thus	O
critical	O
to	O
the	O
execution	O
of	O
downstream	O
customer	O
service	O
responses	O
.	O
Therefore	O
,	O
automatic	O
detection	O
of	O
user	O
complaints	O
on	O
social	O
media	O
could	O
prove	O
beneficial	O
to	O
both	O
the	O
clients	O
and	O
the	O
service	O
providers	O
.	O
To	O
build	O
such	O
detection	O
systems	O
,	O
we	O
could	O
employ	O
supervised	O
approaches	O
that	O
would	O
typically	O
require	O
a	O
large	O
corpus	O
of	O
labeled	O
training	O
samples	O
.	O
However	O
,	O
labeling	O
social	O
media	O
posts	O
that	O
capture	O
complaints	O
about	O
a	O
particular	O
service	O
is	O
challenging	O
because	O
of	O
their	O
low	O
prevalence	O
and	O
also	O
the	O
vast	O
amounts	O
of	O
inevitable	O
noise	O
(	O
Kietzmann	O
et	O
al	O
,	O
2011	O
;	O
Lee	O
,	O
2018	O
)	O
.	O
Additionally	O
,	O
social	O
media	O
platforms	O
are	O
also	O
likely	O
to	O
be	O
plagued	O
with	O
redundancy	O
,	O
where	O
the	O
posts	O
are	O
rephrased	O
or	O
structurally	O
morphed	O
before	O
being	O
re	O
-	O
posted	O
(	O
Ellison	O
et	O
al	O
,	O
2011	O
;	O
Harrigan	O
et	O
al	O
,	O
2012	O
)	O
.	O
Prior	O
work	O
in	O
event	B-TaskName
detection	I-TaskName
(	O
Ritter	O
et	O
al	O
,	O
2012	O
)	O
has	O
demonstrated	O
that	O
simple	O
linguistic	O
indicators	O
(	O
phrases	O
or	O
n	O
-	O
grams	O
)	O
can	O
be	O
useful	O
in	O
the	O
accurate	O
discovery	O
of	O
events	O
in	O
social	O
media	O
.	O
Though	O
user	O
complaints	O
are	O
not	O
the	O
same	O
as	O
events	O
,	O
more	O
of	O
a	O
speech	O
act	O
(	O
Preotiuc	O
-	O
Pietro	O
et	O
al	O
,	O
2019	O
)	O
,	O
we	O
posit	O
that	O
similar	O
indicators	O
can	O
be	O
used	O
in	O
complaint	O
detection	O
.	O
To	O
pursue	O
this	O
hypothesis	O
,	O
we	O
propose	O
a	O
semi	O
-	O
supervised	O
iterative	O
approach	O
to	O
identify	O
social	O
media	O
posts	O
that	O
complain	O
about	O
a	O
specific	O
service	O
.	O
In	O
our	O
approach	O
,	O
we	O
first	O
begin	O
with	O
a	O
small	O
,	O
manually	O
curated	O
dataset	O
containing	O
samples	O
of	O
social	O
media	O
posts	O
complaining	O
about	O
a	O
service	O
.	O
We	O
then	O
identify	O
linguistic	O
indicators	O
(	O
phrases	O
or	O
n	O
-	O
grams	O
)	O
that	O
serve	O
as	O
strong	O
evidence	O
of	O
this	O
phenomenon	O
.	O
These	O
indicators	O
are	O
then	O
used	O
to	O
extract	O
more	O
posts	O
from	O
the	O
unannotated	O
corpus	O
.	O
This	O
newly	O
obtained	O
data	O
is	O
then	O
used	O
to	O
create	O
a	O
new	O
set	O
of	O
indicators	O
.	O
This	O
process	O
is	O
repeated	O
until	O
it	O
reaches	O
a	O
certain	O
convergence	O
point	O
.	O
Since	O
the	O
set	O
of	O
indicators	O
is	O
growing	O
after	O
each	O
iteration	O
,	O
they	O
are	O
re	O
-	O
evaluated	O
continuously	O
in	O
terms	O
of	O
their	O
relevance	O
.	O
This	O
process	O
is	O
similar	O
to	O
the	O
mutual	O
bootstrapping	O
approach	O
for	O
information	O
extraction	O
proposed	O
in	O
(	O
Riloff	O
et	O
al	O
,	O
2003	O
)	O
.	O
We	O
employ	O
this	O
approach	O
to	O
the	O
problem	O
of	O
complaint	O
detection	O
for	O
transportation	O
services	O
on	O
Twitter	O
.	O
Transportation	O
and	O
its	O
related	O
logistic	O
services	O
are	O
critical	O
aspects	O
of	O
every	O
economy	O
as	O
they	O
account	O
for	O
nearly	O
40	O
%	O
of	O
the	O
value	O
of	O
international	O
trade	O
(	O
Rodrigue	O
,	O
2007	O
)	O
.	O
As	O
with	O
most	O
businesses	O
(	O
Gallaugher	O
and	O
Ransbotham	O
,	O
2010	O
;	O
Gottipati	O
et	O
al	O
,	O
2018	O
)	O
,	O
transportation	O
also	O
often	O
relies	O
on	O
social	O
media	O
to	O
ascertain	O
feedback	O
and	O
initiate	O
appropriate	O
responses	O
(	O
Stelzer	O
et	O
al	O
,	O
2016	O
(	O
Stelzer	O
et	O
al	O
,	O
,	O
2014	O
.	O
In	O
our	O
experimental	O
work	O
,	O
we	O
started	O
with	O
an	O
annotated	O
set	O
of	O
326	O
samples	O
of	O
transportation	O
complaints	O
,	O
and	O
after	O
four	O
iterations	O
of	O
the	O
approach	O
,	O
we	O
collected	O
2	O
,	O
840	O
indicators	O
and	O
over	O
3	O
,	O
700	O
tweets	O
.	O
We	O
annotated	O
a	O
random	O
sample	O
of	O
700	O
tweets	O
from	O
the	O
final	O
dataset	O
and	O
observed	O
that	O
over	O
47	O
%	O
of	O
the	O
samples	O
were	O
actual	O
transportation	O
complaints	O
.	O
We	O
also	O
characterize	O
the	O
performance	O
of	O
basic	O
classification	O
algorithms	O
on	O
this	O
dataset	O
.	O
In	O
doing	O
so	O
,	O
we	O
also	O
study	O
how	O
different	O
linguistic	O
features	O
contribute	O
to	O
the	O
performance	O
of	O
a	O
supervised	O
model	O
in	O
this	O
domain	O
.	O
The	O
main	O
contributions	O
of	O
this	O
paper	O
are	O
as	O
follows	O
:	O
We	O
propose	O
a	O
semi	O
-	O
supervised	O
iterative	O
approach	O
to	O
collect	O
user	O
complaints	O
about	O
a	O
service	O
from	O
social	O
media	O
platforms	O
.	O
We	O
evaluate	O
the	O
proposed	O
approach	O
for	O
the	O
problem	O
of	O
complaint	O
detection	O
for	O
transportation	O
services	O
on	O
Twitter	O
.	O
We	O
annotate	O
a	O
random	O
sample	O
of	O
the	O
resulting	O
dataset	O
to	O
establish	O
that	O
nearly	O
half	O
the	O
tweets	O
were	O
actual	O
complaints	O
.	O
We	O
release	O
a	O
curated	O
dataset	O
for	O
the	O
task	O
of	O
traffic	O
-	O
related	O
complaint	O
detection	O
in	O
social	O
media	O
1	O
.	O
Lastly	O
,	O
we	O
characterize	O
the	O
performance	O
of	O
basic	O
classification	O
algorithms	O
on	O
the	O
dataset	O
.	O

Complaints	O
are	O
often	O
considered	O
dialogue	O
acts	O
used	O
to	O
express	O
a	O
mismatch	O
between	O
the	O
expectation	O
and	O
reality	O
(	O
Olshtain	O
and	O
Weinbach	O
,	O
1985	O
)	O
.	O
The	O
problem	O
of	O
complaint	O
detection	O
is	O
of	O
great	O
interest	O
to	O
the	O
marketing	O
and	O
research	O
teams	O
of	O
various	O
service	O
providers	O
.	O
Previous	O
works	O
on	O
complaint	O
identification	O
have	O
applied	O
text	O
mining	O
with	O
LDA	B-MethodName
and	O
sentiment	B-TaskName
analysis	I-TaskName
on	O
user	O
-	O
generated	O
content	O
Duan	O
et	O
al	O
,	O
2013	O
)	O
.	O
Prior	O
works	O
have	O
also	O
focused	O
on	O
leveraging	O
data	O
streamed	O
from	O
social	O
media	O
platforms	O
for	O
outage	O
and	O
complaint	O
detection	O
as	O
they	O
are	O
publicly	O
available	O
(	O
Augustine	O
et	O
al	O
,	O
2012	O
;	O
Kursar	O
and	O
Gopinath	O
,	O
2013	O
)	O
.	O
(	O
Yang	O
et	O
al	O
,	O
2019	O
)	O
inspected	O
customer	O
support	O
dialogue	O
for	O
support	O
.	O
Different	O
complaint	O
expressions	O
have	O
been	O
explored	O
by	O
analyzing	O
variations	O
across	O
cultures	O
(	O
Cohen	O
and	O
Olshtain	O
,	O
1993	O
)	O
,	O
sociodemographic	O
traits	O
(	O
Boxer	O
,	O
1993	O
)	O
and	O
temporal	O
representations	O
(	O
Raghavan	O
,	O
2014	O
)	O
.	O
However	O
,	O
mentioned	O
works	O
on	O
user	O
-	O
generated	O
content	O
have	O
focused	O
on	O
static	O
data	O
repositories	O
only	O
.	O
These	O
have	O
not	O
been	O
robust	O
to	O
linguistic	O
variations	O
(	O
Shah	O
and	O
Zimmermann	O
,	O
2017	O
)	O
and	O
morphological	O
changes	O
(	O
Abdul	O
-	O
Mageed	O
and	O
Korayem	O
,	O
2010	O
)	O
.	O
Our	O
pipeline	O
builds	O
on	O
linguistic	O
identifiers	O
to	O
expand	O
on	O
lexical	O
cues	O
in	O
order	O
to	O
identify	O
complaint	O
relevant	O
posts	O
.	O
Researches	O
have	O
proposed	O
many	O
semisupervised	O
architectures	O
for	O
identification	O
of	O
events	O
pertaining	O
to	O
societal	O
and	O
civil	O
unrest	O
(	O
Hua	O
et	O
al	O
,	O
2013	O
)	O
,	O
using	O
speech	O
modality	O
(	O
Serizel	O
et	O
al	O
,	O
2018	O
;	O
Wu	O
et	O
al	O
,	O
2014	O
;	O
Zhang	O
et	O
al	O
,	O
2017	O
)	O
and	O
Hidden	O
Markov	O
Models	O
(	O
Zhang	O
,	O
2005	O
)	O
.	O
These	O
have	O
been	O
documented	O
to	O
give	O
better	O
performance	O
as	O
compared	O
against	O
their	O
counterparts	O
(	O
Lee	O
et	O
al	O
,	O
2017	O
;	O
Zheng	O
et	O
al	O
,	O
2017	O
)	O
with	O
minimal	O
intervention	O
(	O
Rahimi	O
et	O
al	O
,	O
2018	O
)	O
.	O
For	O
our	O
analysis	O
,	O
the	O
semi	O
-	O
supervised	O
approach	O
has	O
been	O
preferred	O
as	O
opposed	O
to	O
supervised	O
ones	O
because	O
:	O
(	O
a	O
)	O
usage	O
of	O
supervised	O
approach	O
relies	O
on	O
carefully	O
choosing	O
the	O
training	O
set	O
making	O
it	O
cumbersome	O
and	O
less	O
attractive	O
for	O
practical	O
use	O
(	O
Watanabe	O
,	O
2018	O
)	O
and	O
(	O
b	O
)	O
imbalance	O
between	O
the	O
subjective	O
and	O
objective	O
classes	O
lead	O
to	O
poor	O
performance	O
(	O
Yu	O
et	O
al	O
,	O
2015	O
)	O
.	O

We	O
also	O
wanted	O
to	O
understand	O
the	O
predictive	O
power	O
of	O
different	O
types	O
of	O
linguistic	O
features	O
towards	O
the	O
detection	O
of	O
complaints	O
.	O
These	O
features	O
can	O
be	O
broadly	O
broken	O
down	O
into	O
four	O
groups	O
.	O
(	O
i	O
)	O
The	O
first	O
group	O
of	O
features	O
are	O
based	O
on	O
simple	O
semantic	O
properties	O
such	O
as	O
n	O
-	O
grams	O
,	O
word	B-TaskName
embeddings	I-TaskName
,	O
and	O
part	O
of	O
speech	O
tags	O
.	O
(	O
ii	O
)	O
The	O
second	O
group	O
of	O
features	O
are	O
based	O
on	O
pre	O
-	O
trained	O
sentiment	O
models	O
or	O
lexicons	O
.	O
(	O
iii	O
)	O
The	O
third	O
group	O
of	O
features	O
use	O
orthographic	O
information	O
such	O
as	O
hashtags	O
,	O
user	O
mentions	O
,	O
and	O
intensifiers	O
.	O
(	O
iv	O
)	O
The	O
last	O
group	O
of	O
features	O
again	O
use	O
pre	O
-	O
trained	O
models	O
or	O
lexicons	O
associated	O
with	O
request	O
,	O
which	O
is	O
a	O
closely	O
related	O
speech	O
act	O
(	O
Švárová	O
,	O
2008	O
)	O
.	O

We	O
expect	O
sentiment	O
to	O
contribute	O
strongly	O
towards	O
the	O
prediction	O
of	O
complaints	O
.	O
We	O
experiment	O
with	O
two	O
pre	O
-	O
trained	O
models	O
:	O
Stanford	O
Sentiment	O
(	O
Socher	O
et	O
al	O
,	O
2013	O
)	O
and	O
VADER	O
(	O
Hutto	O
and	O
Gilbert	O
,	O
2014	O
)	O
.	O
Namely	O
,	O
we	O
use	O
the	O
scores	O
predicted	O
by	O
these	O
models	O
as	O
representations	O
of	O
tweets	O
.	O
Likewise	O
,	O
we	O
also	O
experiment	O
with	O
two	O
sentiment	O
lexicons	O
:	O
MPQA	B-DatasetName
(	O
Wilson	O
et	O
al	O
,	O
2005	O
)	O
,	O
NRC	O
(	O
Mohammad	O
et	O
al	O
,	O
2013	O
)	O
for	O
assigning	O
sentiment	O
scores	O
to	O
tweets	O
.	O

The	O
ever	O
-	O
increasing	O
amount	O
of	O
user	O
-	O
generated	O
data	O
introduces	O
new	O
challenges	O
in	O
terms	O
of	O
automatic	O
content	O
moderation	O
,	O
especially	O
regarding	O
hate	B-DatasetName
speech	I-DatasetName
and	I-DatasetName
offensive	I-DatasetName
language	I-DatasetName
detection	O
.	O
User	O
content	O
mostly	O
consists	O
of	O
microposts	O
,	O
where	O
the	O
context	O
of	O
a	O
post	O
can	O
be	O
missing	O
or	O
inferred	O
only	O
from	O
current	O
events	O
.	O
The	O
challenge	O
of	O
automatic	O
identification	O
and	O
detection	O
of	O
online	O
aggressiveness	O
has	O
therefore	O
gained	O
increasing	O
popularity	O
in	O
the	O
scientific	O
community	O
over	O
the	O
last	O
years	O
.	O
Several	O
recent	O
workshops	O
and	O
conferences	O
such	O
as	O
TRAC	O
(	O
Kumar	B-DatasetName
et	O
al	O
,	O
2018	O
)	O
,	O
ALW2	O
(	O
Fišer	O
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
GermEval	O
(	O
Wiegand	O
et	O
al	O
,	O
2018	O
)	O
show	O
the	O
growing	O
importance	O
of	O
this	O
subject	O
.	O
The	O
SemEval	O
2019	O
shared	O
task	O
6	O
(	O
Zampieri	O
et	O
al	O
,	O
2019b	O
)	O
further	O
addresses	O
this	O
topic	O
by	O
introducing	O
the	O
Offensive	O
Language	B-TaskName
Identification	I-TaskName
Dataset	O
(	O
OLID	B-DatasetName
)	O
,	O
which	O
consists	O
of	O
tweets	O
,	O
labeled	O
with	O
a	O
three	O
-	O
level	O
annotation	O
model	O
(	O
Zampieri	O
et	O
al	O
,	O
2019a	O
)	O
.	O
Sub	O
-	O
task	O
A	O
is	O
composed	O
of	O
a	O
binary	O
classification	O
problem	O
of	O
whether	O
a	O
tweet	O
in	O
the	O
dataset	O
is	O
offensive	O
or	O
not	O
.	O
Sub	O
-	O
task	O
B	O
focuses	O
on	O
different	O
categories	O
of	O
offensive	O
language	O
and	O
the	O
goal	O
of	O
sub	O
-	O
task	O
C	O
is	O
to	O
identify	O
the	O
targeted	O
individual	O
of	O
an	O
offensive	O
tweet	O
.	O
In	O
the	O
following	O
paper	O
,	O
we	O
present	O
our	O
contribution	O
to	O
sub	O
-	O
task	O
A.	O
After	O
the	O
related	O
work	O
section	O
,	O
we	O
outline	O
our	O
conducted	O
experiments	O
in	O
section	O
3	O
and	O
further	O
describe	O
the	O
used	O
baseline	O
model	O
,	O
as	O
well	O
as	O
the	O
submitted	O
model	O
.	O
In	O
section	O
4	O
we	O
report	O
the	O
results	O
of	O
our	O
experiments	O
on	O
the	O
OLID	B-DatasetName
dataset	O
and	O
the	O
additionally	O
used	O
GermEval	O
dataset	O
.	O
Section	O
5	O
discusses	O
our	O
results	O
and	O
section	O
6	O
concludes	O
our	O
work	O
and	O
describes	O
possible	O
future	O
work	O
.	O

Several	O
methods	O
and	O
models	O
have	O
been	O
presented	O
in	O
literature	O
over	O
the	O
last	O
decade	O
to	O
address	O
the	O
predicament	O
of	O
identifying	O
hate	B-DatasetName
speech	I-DatasetName
,	O
offensive	O
language	O
,	O
and	O
online	O
aggressiveness	O
.	O
In	O
the	O
following	O
section	O
,	O
we	O
present	O
the	O
most	O
notable	O
contributions	O
related	O
to	O
our	O
work	O
.	O
The	O
tweets	O
collected	O
by	O
Davidson	O
et	O
al	O
(	O
2017	O
)	O
were	O
divided	O
into	O
Hate	O
,	O
Offensive	O
,	O
and	O
Neither	O
.	O
Their	O
proposed	O
algorithm	O
uses	O
unigram	O
,	O
bigram	O
,	O
and	O
trigram	O
tokens	O
as	O
features	O
,	O
weighted	O
by	O
the	O
respective	O
TF	O
-	O
IDF	O
,	O
as	O
well	O
as	O
Part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
Speech	O
(	O
POS	O
)	O
tagging	O
and	O
different	O
metrics	O
to	O
determine	O
the	O
readability	O
and	O
sentiment	O
of	O
a	O
tweet	O
.	O
Logisticregression	O
and	O
linear	O
SVM	B-MethodName
result	O
in	O
the	O
best	O
performance	O
for	O
a	O
wide	O
range	O
of	O
assessed	O
classifiers	O
.	O
Nobata	O
et	O
al	O
(	O
2016	O
)	O
collected	O
comments	O
from	O
Yahoo	O
!	O
Finance	O
and	O
News	O
articles	O
over	O
a	O
time	O
period	O
of	O
one	O
year	O
and	O
labeled	O
them	O
as	O
either	O
'	O
Abusive	O
'	O
or	O
'	O
Clean	O
'	O
.	O
They	O
experimented	O
with	O
various	O
different	O
features	O
,	O
including	O
n	O
-	O
gram	O
,	O
linguistic	O
,	O
syntactic	O
,	O
and	O
distributional	O
semantics	O
features	O
.	O
Various	O
approaches	O
utilized	O
deep	O
learning	O
models	O
for	O
text	B-TaskName
categorization	I-TaskName
.	O
proposed	O
a	O
character	O
-	O
level	O
convolutional	O
network	O
for	O
text	B-TaskName
classification	I-TaskName
on	O
large	O
-	O
scale	O
datasets	O
.	O
Their	O
network	O
uses	O
1	O
-	O
dimensional	O
convolutional	O
filters	O
to	O
extract	O
features	O
from	O
different	O
character	O
embed	O
-	O
dings	O
.	O
Gambäck	O
and	O
Sikdar	O
(	O
2017	O
)	O
further	O
experimented	O
with	O
convolutional	O
networks	O
in	O
the	O
context	O
of	O
online	O
hate	B-DatasetName
speech	I-DatasetName
classification	O
.	O
Their	O
research	O
work	O
compares	O
different	O
types	O
of	O
convolutional	O
models	O
,	O
namely	O
character	O
-	O
level	O
,	O
word	O
vectors	O
with	O
a	O
pretrained	O
word2vec	O
(	O
w2v	O
)	O
model	O
,	O
randomly	O
generated	O
word	O
vectors	O
,	O
and	O
w2v	O
in	O
combination	O
with	O
character	O
n	O
-	O
grams	O
.	O
The	O
results	O
of	O
their	O
experiments	O
suggest	O
that	O
w2v	O
embeddings	O
are	O
the	O
most	O
suitable	O
for	O
this	O
task	O
.	O
Zhang	O
et	O
al	O
(	O
2018	O
)	O
suggest	O
an	O
architecture	O
similar	O
to	O
our	O
network	O
,	O
where	O
a	O
convolutional	O
filter	O
extracts	O
features	O
from	O
pretrained	O
word	B-TaskName
embeddings	I-TaskName
.	O
After	O
max	B-MethodName
pooling	I-MethodName
,	O
the	O
feature	O
maps	O
are	O
processed	O
using	O
a	O
unidirectional	O
GRU	B-MethodName
.	O
Their	O
model	O
is	O
compared	O
to	O
a	O
bag	O
-	O
of	O
-	O
n	O
-	O
gram	O
model	O
on	O
various	O
multi	O
-	O
class	O
hate	B-DatasetName
speech	I-DatasetName
datasets	O
and	O
shows	O
promising	O
results	O
.	O
A	O
detailed	O
survey	O
on	O
different	O
architectures	O
,	O
methods	O
and	O
features	O
for	O
offensive	O
language	O
detection	O
is	O
provided	O
by	O
Schmidt	O
and	O
Wiegand	O
(	O
2017	O
)	O
.	O

A	O
TF	O
-	O
IDF	O
bag	O
-	O
of	O
-	O
words	O
model	O
as	O
baseline	O
approach	O
is	O
chosen	O
to	O
evaluate	O
the	O
performance	O
of	O
our	O
model	O
.	O
We	O
limit	O
our	O
feature	O
space	O
to	O
the	O
10	O
,	O
000	O
most	O
frequently	O
used	O
unigrams	O
,	O
bigrams	O
,	O
and	O
trigrams	O
in	O
a	O
corpus	O
.	O
Furthermore	O
,	O
we	O
stem	O
each	O
token	O
in	O
the	O
preprocessing	O
phase	O
and	O
remove	O
stopwords	O
.	O
We	O
compare	O
the	O
performance	O
of	O
several	O
classifiers	O
,	O
namely	O
multinomial	O
Naive	O
Bayes	O
(	O
NB	O
)	O
,	O
SVM	B-MethodName
,	O
Decision	O
Tree	O
(	O
DT	O
)	O
,	O
and	O
Logistic	B-MethodName
Regression	I-MethodName
(	O
LogR	O
)	O
and	O
conduct	O
a	O
grid	O
search	O
to	O
optimize	O
our	O
hyper	O
-	O
parameters	O
.	O

The	O
COVID	O
-	O
19	O
pandemic	O
,	O
like	O
many	O
of	O
the	O
disease	O
outbreaks	O
that	O
have	O
preceded	O
it	O
,	O
is	O
likely	O
to	O
have	O
a	O
profound	O
effect	O
on	O
mental	O
health	O
.	O
Understanding	O
its	O
impact	O
can	O
inform	O
strategies	O
for	O
mitigating	O
negative	O
consequences	O
.	O
In	O
this	O
work	O
,	O
we	O
seek	O
to	O
better	O
understand	O
the	O
effects	O
of	O
COVID	O
-	O
19	O
on	O
mental	O
health	O
by	O
examining	O
discussions	O
within	O
mental	O
health	O
support	O
communities	O
on	O
Reddit	B-DatasetName
.	O
First	O
,	O
we	O
quantify	O
the	O
rate	O
at	O
which	O
COVID	O
-	O
19	O
is	O
discussed	O
in	O
each	O
community	O
,	O
or	O
subreddit	O
,	O
in	O
order	O
to	O
understand	O
levels	O
of	O
pandemic	O
-	O
related	O
discussion	O
.	O
Next	O
,	O
we	O
examine	O
the	O
volume	O
of	O
activity	O
in	O
order	O
to	O
determine	O
whether	O
the	O
number	O
of	O
people	O
discussing	O
mental	O
health	O
has	O
risen	O
.	O
Finally	O
,	O
we	O
analyze	O
how	O
COVID	O
-	O
19	O
has	O
influenced	O
language	O
use	O
and	O
topics	O
of	O
discussion	O
within	O
each	O
subreddit	O
.	O

The	O
implications	O
of	O
COVID	O
-	O
19	O
extend	O
far	O
beyond	O
its	O
immediate	O
physical	O
health	O
effects	O
.	O
Uncertainty	O
and	O
fear	O
surrounding	O
the	O
disease	O
and	O
its	O
effects	O
,	O
in	O
addition	O
to	O
a	O
lack	O
of	O
consistent	O
and	O
reliable	O
information	O
,	O
contribute	O
to	O
rising	O
levels	O
of	O
anxiety	O
and	O
stress	O
(	O
Torales	O
et	O
al	O
,	O
2020	O
)	O
.	O
Policies	O
designed	O
to	O
help	O
contain	O
the	O
disease	O
also	O
have	O
significant	O
consequences	O
.	O
Social	O
distancing	O
policies	O
and	O
lockdowns	O
lead	O
to	O
increased	O
feelings	O
of	O
isolation	O
and	O
uncertainty	O
(	O
Huremović	O
,	O
2019	O
)	O
.	O
They	O
have	O
also	O
triggered	O
an	O
economic	O
downturn	O
(	O
Ş	O
ahin	O
et	O
al	O
,	O
2020	O
)	O
,	O
resulting	O
in	O
soaring	O
unemployment	O
rates	O
and	O
causing	O
many	O
to	O
experience	O
financial	O
stress	O
.	O
Therefore	O
,	O
in	O
addition	O
to	O
the	O
profound	O
effects	O
on	O
physical	O
health	O
around	O
the	O
world	O
,	O
psychiatrists	O
have	O
warned	O
that	O
we	O
should	O
also	O
brace	O
for	O
a	O
mental	O
health	O
crisis	O
as	O
a	O
result	O
of	O
the	O
pandemic	O
(	O
Qiu	O
et	O
al	O
,	O
2020	O
;	O
Greenberg	O
et	O
al	O
,	O
2020	O
;	O
Yao	O
et	O
al	O
,	O
2020	O
;	O
Torales	O
et	O
al	O
,	O
2020	O
)	O
.	O
*	O
Denotes	O
equal	O
contribution	O
.	O
Indeed	O
,	O
the	O
literature	O
on	O
the	O
impact	O
of	O
past	O
epidemics	O
indicates	O
that	O
they	O
are	O
associated	O
with	O
a	O
myriad	O
of	O
adverse	O
mental	O
health	O
effects	O
.	O
In	O
a	O
review	O
of	O
studies	O
on	O
the	O
2002	O
-	O
2003	O
SARS	O
outbreak	O
,	O
the	O
2009	O
H1N1	O
influenza	O
outbreak	O
,	O
and	O
the	O
2018	O
Ebola	O
outbreak	O
,	O
Chew	O
et	O
al	O
(	O
2020	O
)	O
found	O
that	O
anxiety	O
,	O
fear	O
,	O
depression	O
,	O
anger	O
,	O
guilt	O
,	O
grief	O
,	O
and	O
post	O
-	O
traumatic	O
stress	O
were	O
all	O
commonly	O
observed	O
psychological	O
responses	O
.	O
Furthermore	O
,	O
many	O
of	O
the	O
factors	O
commonly	O
cited	O
for	O
inducing	O
these	O
responses	O
are	O
applicable	O
to	O
the	O
COVID	O
-	O
19	O
setting	O
.	O
These	O
include	O
:	O
fear	O
of	O
contracting	O
the	O
disease	O
,	O
a	O
disruption	O
in	O
daily	O
routines	O
,	O
isolation	O
related	O
to	O
being	O
quarantined	O
,	O
and	O
uncertainty	O
regarding	O
the	O
disease	O
treatment	O
process	O
and	O
outcomes	O
,	O
the	O
well	O
-	O
being	O
of	O
loved	O
ones	O
,	O
and	O
one	O
's	O
economic	O
situation	O
.	O
While	O
disease	O
outbreaks	O
pose	O
a	O
risk	O
to	O
the	O
mental	O
health	O
of	O
the	O
general	O
population	O
,	O
research	O
suggests	O
that	O
this	O
risk	O
is	O
heightened	O
for	O
those	O
with	O
preexisting	O
mental	O
health	O
concerns	O
.	O
People	O
with	O
mental	O
health	O
disorders	O
are	O
particularly	O
susceptible	O
to	O
experiencing	O
negative	O
mental	O
health	O
consequences	O
during	O
times	O
of	O
social	O
isolation	O
(	O
Usher	O
et	O
al	O
,	O
2020	O
)	O
.	O
Further	O
,	O
as	O
Yao	O
et	O
al	O
(	O
2020	O
)	O
warn	O
,	O
they	O
are	O
likely	O
to	O
have	O
a	O
stronger	O
emotional	O
response	O
to	O
the	O
feelings	O
of	O
fear	O
,	O
anxiety	O
,	O
and	O
depression	O
that	O
come	O
along	O
with	O
COVID	O
-	O
19	O
than	O
the	O
general	O
population	O
.	O
Given	O
the	O
potential	O
for	O
the	O
COVID	O
-	O
19	O
outbreak	O
to	O
have	O
devastating	O
consequences	O
for	O
mental	O
health	O
,	O
it	O
is	O
critical	O
that	O
we	O
work	O
to	O
understand	O
its	O
psychological	O
effects	O
.	O
In	O
this	O
work	O
,	O
we	O
use	O
Reddit	B-DatasetName
,	O
a	O
popular	O
social	O
media	O
platform	O
,	O
to	O
study	O
how	O
COVID	O
-	O
19	O
has	O
impacted	O
the	O
behavior	O
of	O
groups	O
of	O
users	O
who	O
express	O
mental	O
health	O
concerns	O
.	O
We	O
analyze	O
the	O
content	O
of	O
discussions	O
(	O
COVID	O
-	O
related	O
discussions	O
,	O
psycholinguistic	O
categories	O
,	O
and	O
topics	O
)	O
as	O
well	O
as	O
the	O
volume	O
of	O
communication	O
(	O
daily	O
user	O
count	O
)	O
and	O
find	O
notable	O
changes	O
in	O
each	O
category	O
.	O
Some	O
of	O
these	O
changes	O
appear	O
in	O
multiple	O
mental	O
health	O
subreddits	O
,	O
but	O
some	O
are	O
more	O
specific	O
to	O
individual	O
communities	O
that	O
relate	O
to	O
specific	O
diagnoses	O
.	O
We	O
believe	O
that	O
our	O
findings	O
can	O
help	O
us	O
better	O
understand	O
and	O
potentially	O
alleviate	O
the	O
negative	O
mental	O
health	O
effects	O
of	O
the	O
pandemic	O
;	O
for	O
instance	O
,	O
this	O
type	O
of	O
analysis	O
could	O
help	O
moderators	O
to	O
more	O
effectively	O
support	O
users	O
through	O
future	O
crises	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
the	O
method	O
that	O
we	O
propose	O
has	O
not	O
been	O
used	O
previously	O
to	O
study	O
changes	O
in	O
mental	O
health	O
subreddits	O
,	O
and	O
could	O
be	O
applied	O
to	O
understand	O
the	O
effects	O
of	O
other	O
major	O
events	O
like	O
political	O
elections	O
and	O
natural	O
disasters	O
.	O

In	O
the	O
past	O
decade	O
,	O
social	O
media	O
has	O
emerged	O
as	O
a	O
powerful	O
tool	O
for	O
understanding	O
human	O
behav	O
-	O
ior	O
,	O
and	O
correspondingly	O
mental	O
health	O
.	O
A	O
growing	O
number	O
of	O
studies	O
have	O
applied	O
computational	O
methods	O
to	O
data	O
collected	O
from	O
social	O
media	O
platforms	O
in	O
order	O
to	O
characterize	O
behavior	O
associated	O
with	O
mental	O
health	O
illnesses	O
and	O
to	O
detect	O
and	O
forecast	O
mental	O
health	O
outcomes	O
(	O
see	O
Chancellor	O
and	O
De	O
Choudhury	O
(	O
2020	O
)	O
for	O
a	O
comprehensive	O
review	O
)	O
.	O
Reddit	B-DatasetName
is	O
a	O
particularly	O
well	O
-	O
suited	O
platform	O
for	O
studying	O
mental	O
health	O
due	O
to	O
its	O
semi	O
-	O
anonymous	O
nature	O
,	O
which	O
encourages	O
user	O
honesty	O
and	O
reduces	O
inhibitions	O
associated	O
with	O
self	O
-	O
disclosure	O
(	O
De	O
Choudhury	O
and	O
De	O
,	O
2014	O
)	O
.	O
Additionally	O
,	O
Reddit	B-DatasetName
contains	O
subreddits	O
that	O
act	O
as	O
mental	O
health	O
support	O
forums	O
(	O
e.g.	O
,	O
r	O
/	O
Anxiety	O
,	O
r	O
/	O
depression	O
,	O
r	O
/	O
SuicideWatch	O
)	O
,	O
which	O
enable	O
a	O
more	O
targeted	O
analysis	O
of	O
users	O
experiencing	O
different	O
mental	O
health	O
conditions	O
.	O
A	O
number	O
of	O
existing	O
works	O
have	O
focused	O
on	O
characterizing	O
patterns	O
of	O
discourse	O
within	O
these	O
mental	O
health	O
communities	O
on	O
Reddit	B-DatasetName
.	O
These	O
include	O
studies	O
that	O
have	O
analyzed	O
longitudinal	O
trends	O
in	O
topic	O
usage	O
and	O
word	O
choice	O
(	O
Chakravorti	O
et	O
al	O
,	O
2018	O
)	O
,	O
the	O
relationship	O
between	O
user	O
participation	O
styles	O
and	O
topic	O
usage	O
(	O
Feldhege	O
et	O
al	O
,	O
2020	O
)	O
,	O
and	O
the	O
discourse	O
patterns	O
specific	O
to	O
self	O
-	O
disclosure	O
,	O
social	O
support	O
,	O
and	O
anonymous	O
posting	O
(	O
Pavalanathan	O
and	O
De	O
Choudhury	O
,	O
2015	O
;	O
De	O
Choudhury	O
and	O
De	O
,	O
2014	O
)	O
.	O
Other	O
studies	O
of	O
Reddit	B-DatasetName
mental	O
health	O
communities	O
have	O
aimed	O
to	O
quantify	O
and	O
forecast	O
changes	O
in	O
user	O
behavior	O
.	O
De	O
Choudhury	O
et	O
al	O
(	O
2016	O
)	O
presented	O
a	O
model	O
for	O
predicting	O
the	O
likelihood	O
that	O
users	O
transition	O
from	O
discussing	O
mental	O
health	O
generally	O
to	O
engaging	O
in	O
suicidal	O
ideation	O
.	O
Li	O
et	O
al	O
(	O
2018	O
)	O
analyzed	O
linguistic	O
style	O
measures	O
associated	O
with	O
increasing	O
vs	O
decreasing	O
participation	O
in	O
mental	O
health	O
subreddits	O
over	O
the	O
course	O
of	O
a	O
year	O
.	O
Kumar	B-DatasetName
et	O
al	O
(	O
2015	O
)	O
examined	O
how	O
posting	O
activity	O
in	O
r	O
/	O
SuicideWatch	O
changes	O
following	O
a	O
celebrity	O
suicide	O
.	O
Our	O
work	O
similarly	O
focuses	O
on	O
analyzing	O
temporal	O
patterns	O
in	O
user	O
activity	O
,	O
but	O
we	O
aim	O
to	O
characterize	O
changes	O
associated	O
with	O
COVID	O
-	O
19	O
.	O

Since	O
the	O
first	O
cases	O
of	O
COVID	O
-	O
19	O
were	O
reported	O
in	O
December	O
2019	O
,	O
there	O
have	O
been	O
a	O
number	O
of	O
preliminary	O
studies	O
of	O
its	O
impact	O
on	O
mental	O
health	O
.	O
In	O
a	O
survey	O
of	O
the	O
general	O
public	O
of	O
China	O
,	O
a	O
majority	O
of	O
respondents	O
perceived	O
the	O
psychological	O
impact	O
of	O
the	O
outbreak	O
to	O
be	O
moderate	O
-	O
to	O
-	O
severe	O
and	O
about	O
one	O
-	O
third	O
reported	O
experiencing	O
moderate	O
-	O
tosevere	O
anxiety	O
.	O
Studies	O
of	O
the	O
impact	O
of	O
COVID	O
-	O
19	O
among	O
residents	O
of	O
Liaoning	O
Province	O
,	O
China	O
(	O
Zhang	O
and	O
Ma	O
,	O
2020	O
)	O
and	O
the	O
adult	O
Indian	O
population	O
(	O
Roy	O
et	O
al	O
,	O
2020	O
)	O
also	O
found	O
notable	O
rates	O
of	O
mental	O
distress	O
.	O
There	O
is	O
a	O
set	O
of	O
studies	O
that	O
have	O
examined	O
the	O
mental	O
health	O
consequences	O
of	O
COVID	O
-	O
19	O
by	O
analyzing	O
online	O
behaviors	O
.	O
Jacobson	O
et	O
al	O
(	O
2020	O
)	O
explored	O
the	O
short	O
-	O
term	O
impact	O
of	O
stay	O
-	O
at	O
-	O
home	O
orders	O
in	O
the	O
United	O
States	O
by	O
analyzing	O
changes	O
in	O
the	O
rates	O
of	O
mental	O
health	O
-	O
related	O
Google	B-DatasetName
search	O
queries	O
immediately	O
after	O
orders	O
were	O
issued	O
.	O
Their	O
results	O
showed	O
that	O
rates	O
of	O
mental	O
health	O
queries	O
increased	O
leading	O
up	O
to	O
the	O
issuance	O
of	O
stay	O
-	O
at	O
-	O
homeorders	O
,	O
but	O
then	O
plateaued	O
after	O
they	O
went	O
into	O
effect	O
;	O
however	O
they	O
did	O
not	O
consider	O
the	O
longer	O
-	O
term	O
implications	O
of	O
the	O
stay	O
-	O
at	O
-	O
home	O
orders	O
on	O
mental	O
health	O
.	O
Li	O
et	O
al	O
(	O
2020	O
)	O
measured	O
psycholinguistic	O
attributes	O
of	O
posts	O
on	O
Weibo	B-DatasetName
,	O
a	O
Chinese	O
social	O
media	O
platform	O
,	O
before	O
and	O
after	O
the	O
Chinese	O
National	O
Health	O
Commission	O
declared	O
COVID	O
-	O
19	O
to	O
be	O
an	O
epidemic	O
.	O
Their	O
findings	O
showed	O
that	O
expressions	O
of	O
negative	O
emotions	O
and	O
sensitivity	O
to	O
social	O
risks	O
increased	O
following	O
the	O
declaration	O
.	O
Wolohan	O
(	O
2020	O
)	O
used	O
a	O
Long	B-MethodName
Short	I-MethodName
-	I-MethodName
Term	I-MethodName
Memory	I-MethodName
model	O
to	O
classify	O
depression	O
among	O
Reddit	B-DatasetName
users	O
in	O
April	O
2020	O
,	O
finding	O
a	O
higher	O
than	O
normal	O
depression	O
rate	O
.	O
Our	O
work	O
similarly	O
aims	O
to	O
measure	O
changes	O
in	O
online	O
behavior	O
as	O
a	O
means	O
of	O
understanding	O
the	O
relationship	O
between	O
COVID	O
-	O
19	O
and	O
mental	O
health	O
.	O
However	O
,	O
two	O
notable	O
differences	O
are	O
:	O
(	O
1	O
)	O
instead	O
of	O
analyzing	O
the	O
short	O
-	O
term	O
impact	O
of	O
a	O
specific	O
COVID	O
-	O
related	O
event	O
,	O
we	O
examine	O
more	O
general	O
changes	O
that	O
have	O
occurred	O
during	O
a	O
threemonth	O
period	O
of	O
the	O
outbreak	O
;	O
and	O
(	O
2	O
)	O
we	O
focus	O
our	O
analysis	O
on	O
activity	O
within	O
mental	O
health	O
forums	O
,	O
which	O
allows	O
us	O
to	O
examine	O
the	O
impact	O
of	O
COVID	O
-	O
19	O
specifically	O
on	O
individuals	O
who	O
have	O
expressed	O
mental	O
health	O
concerns	O
.	O

We	O
collect	O
Reddit	B-DatasetName
posts	O
from	O
three	O
mental	O
health	O
subreddits	O
using	O
the	O
Pushshift	O
API	O
1	O
(	O
Baumgartner	O
et	O
al	O
,	O
2020	O
)	O
:	O
r	O
/	O
Anxiety	O
,	O
r	O
/	O
depression	O
,	O
and	O
r	O
/	O
SuicideWatch	O
,	O
from	O
January	O
2017	O
to	O
May	O
2020	O
.	O
The	O
reasons	O
for	O
analyzing	O
these	O
three	O
subreddits	O
are	O
twofold	O
:	O
first	O
,	O
over	O
the	O
three	O
and	O
a	O
half	O
years	O
represented	O
in	O
our	O
data	O
,	O
these	O
subreddits	O
have	O
a	O
significant	O
amount	O
of	O
activity	O
(	O
≥	O
40	O
posts	O
every	O
1	O
As	O
with	O
other	O
social	O
media	O
datasets	O
,	O
there	O
may	O
be	O
noise	O
in	O
the	O
form	O
of	O
API	O
changes	O
and	O
data	O
removed	O
after	O
collection	O
.	O
For	O
the	O
dates	O
involved	O
in	O
our	O
study	O
,	O
static	O
Pushshift	O
dump	O
files	O
were	O
not	O
yet	O
available	O
.	O
day	O
)	O
,	O
making	O
it	O
feasible	O
to	O
treat	O
daily	O
values	O
as	O
a	O
time	B-TaskName
series	I-TaskName
.	O
Second	O
,	O
because	O
the	O
subreddits	O
provide	O
support	O
for	O
different	O
mental	O
health	O
disorders	O
,	O
their	O
users	O
may	O
have	O
been	O
affected	O
differently	O
by	O
COVID	O
-	O
19	O
.	O
We	O
separate	O
the	O
data	O
into	O
two	O
time	O
periods	O
:	O
pre	O
-	O
COVID	O
(	O
January	O
1	O
,	O
2017	O
-	O
February	O
29	O
,	O
2020	O
)	O
and	O
post	O
-	O
COVID	O
(	O
March	O
1	O
,	O
2020	O
-	O
May	O
31	O
,	O
2020	O
)	O
,	O
roughly	O
delineating	O
when	O
COVID	O
-	O
19	O
began	O
to	O
have	O
a	O
serious	O
impact	O
on	O
those	O
in	O
the	O
United	O
States	O
,	O
where	O
the	O
majority	O
of	O
Reddit	B-DatasetName
users	O
are	O
concentrated	O
.	O
2	O
This	O
choice	O
of	O
dates	O
was	O
informed	O
by	O
our	O
analysis	O
of	O
the	O
rates	O
at	O
which	O
COVID	O
-	O
19	O
related	O
words	O
were	O
discussed	O
in	O
each	O
subreddit	O
(	O
see	O
Section	O
5.1	O
)	O
,	O
which	O
we	O
found	O
hovered	O
around	O
0	B-DatasetName
-	O
5	O
%	O
before	O
rising	O
sharply	O
near	O
the	O
beginning	O
of	O
March	O
.	O
We	O
exclude	O
posts	O
where	O
the	O
author	O
or	O
text	O
is	O
marked	O
as	O
'	O
[	O
removed	O
]	O
'	O
or	O
'	O
[	O
deleted	O
]	O
'	O
,	O
because	O
posts	O
with	O
deleted	O
authors	O
offer	O
no	O
value	O
for	O
user	O
count	O
metrics	O
,	O
and	O
deleted	O
content	O
means	O
that	O
we	O
are	O
unable	O
to	O
capture	O
linguistic	O
signals	O
(	O
see	O
Section	O
4.1	O
for	O
more	O
details	O
on	O
these	O
metrics	O
)	O
.	O
Figure	O
1	O
shows	O
the	O
average	O
number	O
of	O
daily	O
posts	O
for	O
r	O
/	O
Anxiety	O
,	O
r	O
/	O
depression	O
,	O
and	O
r	O
/	O
SuicideWatch	O
.	O

Our	O
goal	O
is	O
to	O
identify	O
how	O
mental	O
health	O
subreddit	O
activity	O
has	O
changed	O
during	O
the	O
pandemic	O
.	O
We	O
first	O
create	O
time	B-TaskName
series	I-TaskName
for	O
a	O
number	O
of	O
metrics	O
that	O
could	O
be	O
affected	O
by	O
the	O
pandemic	O
,	O
encompassing	O
activity	O
levels	O
and	O
text	O
content	O
(	O
Section	O
4.1	O
)	O
.	O
We	O
then	O
use	O
a	O
time	B-TaskName
series	I-TaskName
intervention	O
analysis	O
technique	O
to	O
determine	O
whether	O
there	O
are	O
significant	O
changes	O
in	O
our	O
metrics	O
during	O
the	O
pandemic	O
(	O
Section	O
4.2	O
)	O
.	O

We	O
treat	O
the	O
task	O
of	O
identifying	O
changes	O
in	O
subreddit	O
activity	O
patterns	O
as	O
a	O
time	B-TaskName
series	I-TaskName
intervention	O
analysis	O
problem	O
.	O
Our	O
basic	O
approach	O
involves	O
:	O
(	O
1	O
)	O
fitting	O
a	O
time	B-TaskName
series	I-TaskName
model	O
to	O
the	O
pre	O
-	O
COVID	O
observations	O
for	O
each	O
of	O
the	O
metrics	O
described	O
above	O
and	O
then	O
(	O
2	O
)	O
examining	O
how	O
the	O
values	O
forecasted	O
by	O
the	O
model	O
compare	O
to	O
the	O
observed	O
values	O
during	O
the	O
post	O
-	O
COVID	O
time	O
period	O
.	O
It	O
is	O
worth	O
noting	O
that	O
the	O
one	O
study	O
we	O
found	O
examining	O
the	O
impact	O
of	O
an	O
event	O
on	O
activity	O
within	O
mental	O
health	O
subreddits	O
employs	O
a	O
different	O
approach	O
:	O
they	O
use	O
a	O
t	O
-	O
test	O
to	O
compare	O
the	O
observations	O
from	O
"	O
before	O
"	O
vs	O
"	O
after	O
"	O
the	O
event	O
(	O
Kumar	B-DatasetName
et	O
al	O
,	O
2015	O
)	O
.	O
However	O
,	O
their	O
problem	O
setup	O
differs	O
from	O
ours	O
in	O
that	O
they	O
consider	O
a	O
much	O
shorter	O
period	O
of	O
time	O
(	O
four	O
weeks	O
total	O
)	O
,	O
so	O
the	O
effects	O
of	O
seasonality	O
(	O
regular	O
changes	O
that	O
recur	O
each	O
year	O
)	O
and	O
longer	O
-	O
term	O
trends	O
are	O
likely	O
reduced	O
.	O
In	O
contrast	O
,	O
we	O
find	O
that	O
there	O
is	O
often	O
a	O
strong	O
trend	O
over	O
time	O
and	O
seasonal	O
component	O
in	O
our	O
data	O
,	O
making	O
a	O
direct	O
comparison	O
of	O
two	O
time	O
periods	O
with	O
a	O
t	O
-	O
test	O
unreliable	O
.	O
We	O
smooth	O
each	O
time	B-TaskName
series	I-TaskName
and	O
remove	O
day	O
-	O
ofweek	O
related	O
fluctuations	O
by	O
computing	O
a	O
sevenday	O
rolling	O
mean	O
over	O
the	O
time	B-TaskName
series	I-TaskName
.	O
We	O
use	O
the	O
Prophet	O
model	O
(	O
Taylor	O
and	O
Letham	O
,	O
2018	O
)	O
to	O
create	O
a	O
model	O
of	O
the	O
period	O
before	O
COVID	O
-	O
19	O
.	O
This	O
model	O
was	O
initially	O
created	O
by	O
Facebook	O
to	O
forecast	O
time	B-TaskName
series	I-TaskName
on	O
their	O
platform	O
,	O
such	O
as	O
the	O
number	O
of	O
events	O
created	O
per	O
day	O
or	O
the	O
number	O
of	O
active	O
users	O
;	O
we	O
find	O
that	O
our	O
time	B-TaskName
series	I-TaskName
,	O
also	O
compiled	O
from	O
social	O
media	O
,	O
have	O
many	O
similar	O
properties	O
.	O
The	O
Prophet	O
model	O
is	O
an	O
additive	O
regression	O
model	O
with	O
three	O
components	O
:	O
y	O
(	O
t	O
)	O
=	O
g	O
(	O
t	O
)	O
+	O
s	O
(	O
t	O
)	O
+	O
h	O
(	O
t	O
)	O
+	O
t	O
(	O
1	O
)	O
The	O
trend	O
is	O
encapsulated	O
by	O
g	O
(	O
t	O
)	O
,	O
a	O
piecewise	O
linear	O
model	O
.	O
The	O
seasonality	O
of	O
the	O
data	O
is	O
cap	O
-	O
tured	O
by	O
s	O
(	O
t	O
)	O
,	O
which	O
is	O
approximated	O
using	O
a	O
Fourier	O
series	O
.	O
As	O
we	O
smooth	O
our	O
data	O
on	O
a	O
weekly	O
basis	O
,	O
we	O
utilize	O
only	O
yearly	O
seasonality	O
,	O
excluding	O
the	O
optional	O
weekly	O
and	O
daily	O
seasonality	O
components	O
.	O
The	O
third	O
term	O
,	O
h	O
(	O
t	O
)	O
,	O
represents	O
holidays	O
;	O
we	O
find	O
that	O
adding	O
the	O
default	O
list	O
of	O
US	O
holidays	O
provided	O
by	O
Prophet	O
reduces	O
error	O
for	O
most	O
our	O
our	O
time	B-TaskName
series	I-TaskName
in	O
the	O
pre	O
-	O
COVID	O
period	O
,	O
likely	O
because	O
the	O
Reddit	B-DatasetName
population	O
is	O
centered	O
in	O
the	O
United	O
States	O
.	O
Finally	O
,	O
t	O
represents	O
the	O
error	O
,	O
in	O
this	O
case	O
fluctuations	O
in	O
the	O
time	B-TaskName
series	I-TaskName
that	O
are	O
not	O
captured	O
by	O
the	O
model	O
.	O
After	O
training	O
the	O
model	O
on	O
the	O
pre	O
-	O
COVID	O
data	O
,	O
we	O
predict	O
values	O
for	O
the	O
post	O
-	O
COVID	O
period	O
.	O
If	O
we	O
assume	O
that	O
there	O
is	O
no	O
change	O
during	O
this	O
time	O
period	O
,	O
we	O
would	O
expect	O
the	O
predicted	O
values	O
to	O
be	O
near	O
the	O
true	O
values	O
,	O
given	O
that	O
the	O
model	O
does	O
a	O
good	O
job	O
fitting	O
the	O
trend	O
and	O
seasonal	O
components	O
.	O
The	O
model	O
computes	O
uncertainty	O
intervals	O
over	O
the	O
predicted	O
values	O
by	O
simulating	O
ways	O
in	O
which	O
the	O
trend	O
may	O
change	O
during	O
the	O
period	O
of	O
the	O
forecast	O
.	O
We	O
use	O
this	O
method	O
to	O
compute	O
the	O
95	O
%	O
prediction	O
interval	O
.	O
Our	O
null	O
hypothesis	O
is	O
that	O
there	O
has	O
been	O
no	O
change	O
in	O
trend	O
.	O
In	O
this	O
case	O
,	O
we	O
would	O
expect	O
5	O
%	O
of	O
the	O
data	O
in	O
the	O
post	O
-	O
COVID	O
period	O
to	O
fall	O
outside	O
of	O
the	O
prediction	O
interval	O
.	O
Our	O
alternate	O
hypothesis	O
is	O
that	O
there	O
was	O
a	O
change	O
in	O
the	O
trend	O
of	O
the	O
time	B-TaskName
series	I-TaskName
(	O
which	O
may	O
be	O
attributable	O
to	O
.	O
In	O
this	O
case	O
,	O
more	O
than	O
5	O
%	O
of	O
the	O
data	O
in	O
the	O
post	O
-	O
COVID	O
period	O
will	O
fall	O
outside	O
of	O
the	O
prediction	O
interval	O
.	O
We	O
apply	O
a	O
one	O
-	O
sample	O
proportion	O
test	O
to	O
assess	O
whether	O
the	O
proportion	O
of	O
observations	O
outside	O
of	O
the	O
prediction	O
interval	O
in	O
the	O
post	O
-	O
COVID	O
period	O
is	O
significantly	O
greater	O
than	O
5	O
%	O
.	O
The	O
details	O
of	O
this	O
test	O
are	O
in	O
Appendix	O
C.	O

5.1	O
How	O
often	O
do	O
people	O
in	O
different	O
mental	O
health	O
subreddits	O
discuss	O
COVID	O
-	O
19	O
?	O
Using	O
our	O
COVID	O
-	O
19	O
lexicon	O
(	O
Section	O
4.1	O
)	O
,	O
we	O
compute	O
the	O
percentage	O
of	O
posts	O
per	O
day	O
that	O
mention	O
any	O
words	O
related	O
to	O
COVID	O
-	O
19	O
,	O
as	O
shown	O
in	O
Figure	O
1	O
.	O
We	O
see	O
that	O
COVID	O
-	O
19	O
began	O
to	O
have	O
a	O
serious	O
impact	O
on	O
discussions	O
in	O
all	O
three	O
subreddits	O
around	O
the	O
beginning	O
of	O
March	O
2020	O
,	O
as	O
is	O
clear	O
from	O
the	O
spikes	O
in	O
Figure	O
1	O
.	O
Although	O
COVID	O
-	O
19	O
is	O
discussed	O
on	O
all	O
subreddits	O
,	O
we	O
see	O
a	O
stark	O
difference	O
in	O
the	O
volume	O
of	O
discussion	O
across	O
each	O
of	O
them	O
;	O
in	O
r	O
/	O
Anxiety	O
,	O
discussion	O
of	O
COVID	O
-	O
19	O
is	O
more	O
frequent	O
than	O
it	O
is	O
in	O
r	O
/	O
depression	O
or	O
r	O
/	O
SuicideWatch	O
,	O
and	O
begins	O
earlier	O
.	O
Discussion	O
When	O
choosing	O
the	O
date	O
to	O
consider	O
as	O
the	O
beginning	O
of	O
the	O
post	O
-	O
COVID	O
period	O
in	O
our	O
time	B-TaskName
series	I-TaskName
analysis	I-TaskName
,	O
we	O
considered	O
March	O
1st	O
,	O
2020	O
as	O
a	O
sensible	O
date	O
,	O
as	O
it	O
aligns	O
with	O
the	O
time	O
at	O
which	O
the	O
United	O
States	O
(	O
where	O
the	O
majority	O
of	O
Reddit	B-DatasetName
users	O
reside	O
)	O
began	O
to	O
take	O
COVID	O
-	O
19	O
seriously	O
.	O
March	O
1st	O
closely	O
followed	O
the	O
first	O
announced	O
COVID	O
-	O
19	O
death	O
in	O
the	O
United	O
States	O
on	O
February	O
28th	O
,	O
2020	O
,	O
and	O
preceded	O
state	O
lockdowns	O
and	O
school	O
closures	O
.	O
The	O
spikes	O
at	O
the	O
beginning	O
of	O
March	O
suggest	O
that	O
this	O
date	O
also	O
reflects	O
the	O
time	O
at	O
which	O
COVID	O
-	O
19	O
began	O
to	O
have	O
a	O
notable	O
impact	O
on	O
mental	O
health	O
subreddit	O
discussions	O
.	O
Although	O
most	O
COVID	O
-	O
19	O
related	O
discussion	O
started	O
in	O
March	O
,	O
we	O
also	O
see	O
that	O
a	O
small	O
spike	O
in	O
discussion	O
rates	O
occurred	O
earlier	O
in	O
r	O
/	O
Anxiety	O
.	O
This	O
suggests	O
that	O
users	O
in	O
this	O
subreddit	O
began	O
to	O
notice	O
some	O
impact	O
from	O
COVID	O
-	O
19	O
in	O
late	O
January	O
,	O
when	O
reports	O
of	O
lockdowns	O
in	O
China	O
first	O
appeared	O
in	O
the	O
news	O
.	O
Based	O
on	O
the	O
early	O
start	O
and	O
elevated	O
rate	O
of	O
COVID	O
-	O
19	O
discussion	O
within	O
r	O
/	O
Anxiety	O
,	O
we	O
conclude	O
that	O
all	O
of	O
our	O
metrics	O
are	O
likely	O
to	O
be	O
more	O
strongly	O
affected	O
by	O
COVID	O
-	O
19	O
in	O
r	O
/	O
Anxiety	O
.	O

To	O
give	O
us	O
a	O
better	O
idea	O
of	O
how	O
common	O
language	O
dimensions	O
have	O
changed	O
,	O
while	O
the	O
LDA	B-MethodName
-	O
derived	O
topics	O
allow	O
us	O
to	O
explore	O
areas	O
of	O
discussion	O
that	O
are	O
typically	O
of	O
concern	O
in	O
these	O
subreddits	O
.	O
LIWC	O
has	O
been	O
used	O
extensively	O
in	O
mental	O
health	O
analysis	O
,	O
and	O
there	O
are	O
some	O
LIWC	O
categories	O
and	O
LDAderived	O
topics	O
that	O
overlap	O
,	O
such	O
as	O
ANXIETY	O
,	O
DEATH	O
,	O
and	O
FAMILY	O
,	O
but	O
there	O
are	O
also	O
unique	O
categories	O
covered	O
by	O
each	O
method	O
,	O
such	O
as	O
WE	O
and	O
MOTIVATION	O
.	O
For	O
each	O
of	O
the	O
metrics	O
,	O
we	O
examine	O
changes	O
that	O
have	O
occurred	O
since	O
COVID	O
-	O
19	O
by	O
computing	O
the	O
proportion	O
of	O
outliers	O
produced	O
by	O
our	O
forecasting	O
model	O
(	O
see	O
Section	O
4.2	O
)	O
in	O
the	O
post	O
-	O
COVID	O
period	O
.	O
We	O
acknowledge	O
that	O
this	O
analysis	O
may	O
occasionally	O
capture	O
misleading	O
changes	O
.	O
For	O
example	O
,	O
the	O
death	O
keyword	O
may	O
yield	O
changes	O
in	O
the	O
suicide	O
topic	O
(	O
see	O
Appendix	O
B	O
)	O
that	O
are	O
actually	O
related	O
to	O
infectious	O
disease	O
,	O
and	O
observing	O
increased	O
mentions	O
of	O
family	O
does	O
not	O
indicate	O
the	O
polarity	O
of	O
their	O
sentiment	O
.	O
We	O
leave	O
it	O
to	O
future	O
work	O
to	O
do	O
a	O
more	O
in	O
-	O
depth	O
analysis	O
of	O
the	O
context	O
surrounding	O
specific	O
outliers	O
that	O
are	O
detected	O
.	O

We	O
see	O
changes	O
in	O
some	O
categories	O
that	O
appear	O
to	O
be	O
directly	O
related	O
to	O
the	O
new	O
experience	O
of	O
living	O
during	O
a	O
global	O
pandemic	O
under	O
social	O
distancing	O
rules	O
;	O
this	O
includes	O
the	O
decrease	O
in	O
MOTION	O
,	O
which	O
makes	O
sense	O
as	O
people	O
are	O
traveling	O
and	O
moving	O
around	O
far	O
less	O
.	O
The	O
increase	O
in	O
categories	O
such	O
as	O
BIO	O
and	O
BODY	O
within	O
r	O
/	O
Anxiety	O
may	O
reflect	O
concerns	O
regarding	O
the	O
physical	O
health	O
implications	O
of	O
COVID	O
-	O
19	O
.	O
Moreover	O
,	O
it	O
appears	O
that	O
physical	O
health	O
concerns	O
are	O
especially	O
salient	O
for	O
people	O
who	O
experience	O
anxiety	O
,	O
as	O
the	O
rise	O
in	O
these	O
categories	O
is	O
not	O
present	O
in	O
the	O
other	O
subreddits	O
.	O
The	O
statistically	O
significant	O
drop	O
in	O
FOCUSFU	O
-	O
TURE	O
within	O
r	O
/	O
Anxiety	O
and	O
r	O
/	O
depression	O
indicates	O
that	O
users	O
are	O
less	O
inclined	O
to	O
speak	O
about	O
their	O
concerns	O
for	O
the	O
future	O
in	O
light	O
of	O
the	O
more	O
pressing	O
current	O
concerns	O
related	O
to	O
the	O
pandemic	O
.	O
The	O
sharp	O
increase	O
in	O
WE	O
(	O
Figure	O
3a	O
)	O
indicates	O
a	O
general	O
feeling	O
of	O
community	O
and	O
togetherness	O
,	O
which	O
speaks	O
positively	O
to	O
the	O
support	O
that	O
those	O
in	O
these	O
mental	O
health	O
communities	O
are	O
getting	O
during	O
the	O
pandemic	O
.	O
This	O
finding	O
aligns	O
with	O
a	O
study	O
by	O
Zhang	O
and	O
Ma	O
(	O
2020	O
)	O
on	O
the	O
effects	O
of	O
COVID	O
-	O
19	O
on	O
mental	O
well	O
-	O
being	O
in	O
China	O
,	O
which	O
found	O
that	O
participants	O
received	O
increased	O
support	O
from	O
friends	O
and	O
family	O
during	O
the	O
pandemic	O
.	O
In	O
addition	O
,	O
seeking	O
social	O
support	O
was	O
listed	O
as	O
a	O
common	O
coping	O
strategy	O
during	O
infections	O
disease	O
outbreaks	O
by	O
Chew	O
et	O
al	O
(	O
2020	O
)	O
.	O
An	O
increase	O
in	O
"	O
we	O
"	O
is	O
not	O
specific	O
to	O
mental	O
health	O
communities	O
;	O
researchers	O
have	O
found	O
increases	O
in	O
usage	O
of	O
the	O
pronoun	O
during	O
the	O
early	O
stages	O
of	O
COVID	O
-	O
19	O
on	O
other	O
subreddits	O
(	O
Ashokkumar	O
and	O
Pennebaker	O
,	O
2020	O
)	O
.	O
The	O
decrease	O
in	O
I	O
words	O
in	O
r	O
/	O
Anxiety	O
is	O
accompanied	O
by	O
an	O
increase	O
in	O
r	O
/	O
depression	O
(	O
Figure	O
3b	O
)	O
.	O
The	O
increase	O
of	O
usage	O
of	O
the	O
I	O
pronoun	O
is	O
concerning	O
because	O
it	O
has	O
been	O
shown	O
to	O
correlate	O
with	O
depression	O
,	O
indicating	O
that	O
an	O
increase	O
in	O
its	O
use	O
could	O
be	O
related	O
to	O
worsening	O
symptoms	O
(	O
Rude	O
et	O
al	O
,	O
2004	O
)	O
.	O
The	O
drop	O
in	O
discussion	O
of	O
WORK	O
(	O
Figure	O
3c	O
)	O
is	O
unexpected	O
,	O
as	O
the	O
economic	O
downturn	O
could	O
be	O
a	O
significant	O
motivator	O
of	O
posts	O
.	O
The	O
drop	O
indicates	O
that	O
up	O
to	O
this	O
point	O
,	O
the	O
stress	O
and	O
change	O
associated	O
with	O
adapting	O
to	O
working	O
from	O
home	O
,	O
or	O
worse	O
,	O
losing	O
one	O
's	O
job	O
has	O
not	O
been	O
a	O
frequent	O
topic	O
of	O
discussion	O
in	O
these	O
forums	O
.	O
This	O
drop	O
could	O
be	O
due	O
to	O
a	O
decrease	O
in	O
work	O
-	O
related	O
stressors	O
,	O
which	O
have	O
been	O
shown	O
to	O
cause	O
anxiety	O
and	O
depression	O
(	O
Melchior	O
et	O
al	O
,	O
2007	O
;	O
Cherry	O
,	O
1978	O
)	O
,	O
or	O
it	O
could	O
simply	O
indicate	O
that	O
the	O
stressors	O
became	O
secondary	O
to	O
other	O
concerns	O
.	O
It	O
is	O
also	O
possible	O
that	O
compared	O
to	O
the	O
general	O
population	O
,	O
Reddit	B-DatasetName
users	O
are	O
more	O
likely	O
to	O
have	O
jobs	O
that	O
can	O
be	O
done	O
remotely	O
during	O
the	O
pandemic	O
,	O
as	O
they	O
are	O
more	O
likely	O
to	O
have	O
college	O
degrees	O
than	O
the	O
general	O
population	O
.	O
3	O

In	O
this	O
study	O
,	O
we	O
examined	O
how	O
COVID	O
-	O
19	O
has	O
influenced	O
the	O
online	O
behavior	O
of	O
individuals	O
who	O
discuss	O
mental	O
health	O
concerns	O
by	O
analyzing	O
activity	O
within	O
the	O
r	O
/	O
Anxiety	O
,	O
r	O
/	O
depression	O
,	O
and	O
r	O
/	O
SuicideWatch	O
communities	O
on	O
Reddit	B-DatasetName
.	O
We	O
found	O
substantial	O
evidence	O
of	O
increases	O
in	O
anxiety	O
;	O
we	O
observed	O
an	O
increase	O
in	O
user	O
activity	O
in	O
r	O
/	O
Anxiety	O
,	O
as	O
well	O
as	O
significant	O
increases	O
in	O
discussions	O
of	O
anxiety	O
and	O
the	O
symptoms	O
associated	O
with	O
it	O
.	O
Interestingly	O
,	O
we	O
observed	O
a	O
decrease	O
in	O
activity	O
within	O
the	O
r	O
/	O
depression	O
and	O
r	O
/	O
SuicideWatch	O
subreddits	O
.	O
The	O
literature	O
on	O
the	O
impact	O
of	O
disease	O
outbreaks	O
on	O
depression	O
rates	O
contains	O
somewhat	O
contradictory	O
findings	O
;	O
we	O
therefore	O
believe	O
that	O
this	O
is	O
an	O
interesting	O
area	O
for	O
future	O
work	O
.	O
We	O
also	O
observed	O
interesting	O
changes	O
in	O
the	O
content	O
of	O
discussions	O
within	O
each	O
subreddit	O
.	O
Our	O
results	O
suggest	O
that	O
concerns	O
related	O
to	O
COVID	O
-	O
19	O
,	O
such	O
as	O
health	O
and	O
family	O
,	O
have	O
become	O
more	O
prominent	O
discussion	O
topics	O
compared	O
to	O
other	O
common	O
concerns	O
,	O
such	O
as	O
work	O
and	O
school	O
,	O
which	O
have	O
generated	O
relatively	O
less	O
discussion	O
since	O
the	O
outbreak	O
.	O
While	O
our	O
findings	O
largely	O
confirm	O
the	O
warnings	O
offered	O
by	O
psychiatrists	O
regarding	O
the	O
potential	O
for	O
COVID	O
-	O
19	O
to	O
have	O
an	O
adverse	O
effect	O
on	O
mental	O
health	O
,	O
we	O
also	O
found	O
some	O
reason	O
for	O
optimism	O
;	O
increases	O
in	O
the	O
usage	O
of	O
WE	O
as	O
well	O
as	O
the	O
INFORMATION	O
SHARING	O
topic	O
(	O
associated	O
with	O
words	O
such	O
as	O
"	O
story	O
"	O
and	O
"	O
hope	O
"	O
)	O
,	O
suggest	O
a	O
heightened	O
sense	O
of	O
community	O
and	O
shared	O
experience	O
,	O
which	O
may	O
help	O
individuals	O
cope	O
with	O
these	O
stressful	O
times	O
.	O

Figure	O
4	O
shows	O
the	O
topics	O
identified	O
by	O
the	O
LDA	B-MethodName
model	O
.	O
4	O
:	O
Topics	O
identified	O
by	O
the	O
LDA	B-MethodName
topic	O
model	O
.	O
For	O
each	O
topic	O
,	O
we	O
provide	O
a	O
summary	O
label	O
and	O
the	O
ten	O
most	O
probable	O
words	O
.	O
We	O
omit	O
labels	O
for	O
topics	O
whose	O
keywords	O
did	O
not	O
have	O
a	O
clear	O
interpretation	O
.	O

Recent	O
work	O
has	O
focused	O
on	O
the	O
analysis	O
of	O
usergenerated	O
text	O
in	O
various	O
online	O
venues	O
,	O
including	O
labeling	O
certain	O
qualities	O
of	O
individual	O
comments	O
,	O
comment	O
pairs	O
,	O
or	O
the	O
roles	O
of	O
individual	O
commenters	O
.	O
The	O
largest	O
and	O
most	O
extensively	O
annotated	O
corpus	O
predating	O
this	O
work	O
is	O
the	O
Internet	O
Argument	O
Corpus	O
(	O
IAC	O
)	O
,	O
which	O
contains	O
approximately	O
480k	O
comments	O
in	O
16.5k	O
threads	O
from	O
on	O
-	O
line	O
forums	O
in	O
which	O
users	O
debate	O
contentious	O
issues	O
.	O
The	O
IAC	O
has	O
been	O
coded	O
for	O
for	O
topic	O
(	O
3k	O
threads	O
)	O
,	O
stance	O
(	O
2k	O
authors	O
)	O
,	O
and	O
agreement	O
,	O
sarcasm	O
,	O
and	O
hostility	O
(	O
10k	O
comment	O
pairs	O
)	O
(	O
Abbott	O
et	O
al	O
,	O
2016	O
;	O
Walker	O
et	O
al	O
,	O
2012	O
)	O
.	O
Comments	O
from	O
online	O
news	O
articles	O
are	O
annotated	O
in	O
the	O
SEN	B-DatasetName
-	O
SEI	O
corpus	O
,	O
which	O
contains	O
human	O
-	O
authored	O
summaries	O
of	O
1.8k	O
comments	O
posted	O
on	O
Guardian	O
articles	O
(	O
Barker	O
et	O
al	O
,	O
2016	O
)	O
.	O
Participants	O
described	O
each	O
comment	O
with	O
short	O
,	O
free	O
-	O
form	O
text	O
labels	O
and	O
then	O
wrote	O
a	O
150	O
-	O
250	O
-	O
word	O
comment	O
summary	O
with	O
these	O
labels	O
.	O
Barker	O
et	O
al	O
(	O
2016	O
)	O
recognized	O
that	O
comments	O
have	O
diverse	O
qualities	O
,	O
many	O
of	O
which	O
are	O
coded	O
in	O
this	O
work	O
(	O
3	O
)	O
,	O
but	O
did	O
not	O
explicitly	O
collect	O
labels	O
of	O
them	O
.	O
Previous	O
works	O
present	O
a	O
survey	O
of	O
how	O
editors	O
and	O
readers	O
perceive	O
the	O
quality	O
of	O
comments	O
posted	O
in	O
online	O
news	O
publications	O
(	O
Diakopoulos	O
and	O
Naaman	O
,	O
2011	O
)	O
and	O
review	O
the	O
criteria	O
professional	O
editors	O
use	O
to	O
curate	O
comments	O
(	O
Diakopoulos	O
,	O
2015	O
)	O
.	O
The	O
latter	O
identifies	O
15	O
criteria	O
for	O
curating	O
user	O
-	O
generated	O
responses	O
,	O
from	O
online	O
and	O
radio	O
comments	O
to	O
letters	O
to	O
the	O
editor	O
.	O
Our	O
annotation	O
scheme	O
overlaps	O
with	O
those	O
criteria	O
but	O
also	O
diverges	O
as	O
we	O
wish	O
for	O
the	O
labels	O
to	O
reflect	O
the	O
nature	O
of	O
all	O
comments	O
posted	O
on	O
online	O
articles	O
instead	O
of	O
just	O
the	O
qualities	O
sought	O
in	O
editorially	O
curated	O
comments	O
.	O
ERICs	O
can	O
take	O
many	O
forms	O
and	O
may	O
not	O
reflect	O
the	O
formal	O
tone	O
or	O
intent	O
that	O
editors	O
in	O
traditional	O
news	O
outlets	O
seek	O
.	O
Our	O
coding	O
scheme	O
intersects	O
with	O
attributes	O
examined	O
in	O
several	O
different	O
areas	O
of	O
research	O
.	O
Some	O
of	O
the	O
most	O
recent	O
and	O
relevant	O
discourse	O
corpora	O
from	O
online	O
sources	O
related	O
to	O
this	O
work	O
include	O
the	O
following	O
:	O
Concepts	O
related	O
to	O
persuasiveness	O
have	O
been	O
studied	O
,	O
including	O
annotations	O
for	O
"	O
convincing	O
-	O
ness	O
"	O
in	O
debate	O
forums	O
(	O
Habernal	O
and	O
Gurevych	O
,	O
2016	O
)	O
,	O
influencers	O
in	O
discussions	O
from	O
blogs	O
and	O
Wikipedia	O
(	O
Biran	O
et	O
al	O
,	O
2012	O
)	O
,	O
and	O
user	O
relations	O
as	O
a	O
proxy	O
of	O
persuasion	O
in	O
reddit	B-DatasetName
(	O
Tan	O
et	O
al	O
,	O
2016	O
;	O
Wei	O
et	O
al	O
,	O
2016	O
)	O
.	O
Politeness	O
was	O
labeled	O
and	O
identified	O
in	O
Stack	O
Exchange	O
and	O
Wikipedia	O
discussions	O
(	O
Danescu	O
-	O
Niculescu	O
-	O
Mizil	O
et	O
al	O
,	O
2013	O
)	O
.	O
Some	O
previous	O
work	O
focused	O
on	O
detecting	O
agreement	O
has	O
considered	O
blog	O
and	O
Wikipedia	O
discussions	O
and	O
debate	O
forums	O
(	O
Skeppstedt	O
et	O
al	O
,	O
2016	O
)	O
.	O
Sarcasm	O
has	O
been	O
identified	O
in	O
a	O
corpus	O
of	O
microblogs	O
identified	O
with	O
the	O
hashtag	O
#	O
sarcasm	O
on	O
Twitter	O
(	O
González	O
-	O
Ibánez	O
et	O
al	O
,	O
2011	O
;	O
Davidov	O
et	O
al	O
,	O
2010	O
)	O
and	O
in	O
online	O
forums	O
(	O
Oraby	O
et	O
al	O
,	O
2016	O
)	O
.	O
Sentiment	O
has	O
been	O
studied	O
widely	O
,	O
often	O
in	O
the	O
context	O
of	O
reviews	O
(	O
Pang	O
and	O
Lee	O
,	O
2005	O
)	O
,	O
and	O
in	O
the	O
context	O
of	O
user	O
-	O
generated	O
exchanges	O
,	O
positive	O
and	O
negative	O
attitudes	O
have	O
been	O
identified	O
in	O
Usenet	O
discussions	O
(	O
Hassan	O
et	O
al	O
,	O
2010	O
)	O
.	O
Other	O
qualities	O
of	O
user	O
-	O
generated	O
text	O
that	O
are	O
not	O
covered	O
in	O
this	O
work	O
but	O
have	O
been	O
investigated	O
before	O
include	O
metaphor	O
(	O
Jang	O
et	O
al	O
,	O
2014	O
)	O
and	O
tolerance	O
(	O
Mukherjee	O
et	O
al	O
,	O
2013	O
)	O
in	O
online	O
discussion	O
threads	O
,	O
"	O
dogmatism	O
"	O
of	O
reddit	B-DatasetName
users	O
(	O
Fast	O
and	O
Horvitz	O
,	O
2016	O
)	O
,	O
and	O
argumentation	O
units	O
in	O
discussions	O
related	O
to	O
technology	O
.	O

Agreement	O
Agreement	O
expressed	O
with	O
explicit	O
phrasing	O
(	O
e.g.	O
,	O
I	O
disagree	O
...	O
)	O
or	O
implicitly	O
,	O
such	O
as	O
in	O
Figure	O
2	O
.	O
Annotating	O
the	O
target	O
of	O
(	O
dis	O
)	O
agreement	O
is	O
left	O
to	O
future	O
work	O
due	O
to	O
the	O
number	O
of	O
other	O
codes	O
the	O
annotators	O
need	O
to	O
attend	O
to	O
.	O
Multiple	O
labels	O
can	O
be	O
chosen	O
per	O
comment	O
,	O
since	O
a	O
comment	O
can	O
express	O
agreement	O
with	O
one	O
statement	O
and	O
disagreement	O
with	O
another	O
.	O
Agreement	O
with	O
another	O
commenter	O
Disagreement	O
with	O
another	O
commenter	O
Adjunct	O
opinion	O
:	O
Contains	O
a	O
perspective	O
that	O
has	O
not	O
yet	O
been	O
articulated	O
in	O
the	O
thread	O
.	O
Audience	O
The	O
target	O
audience	O
of	O
a	O
comment	O
.	O
Reply	O
to	O
specific	O
commenter	O
:	O
Can	O
be	O
explicit	O
(	O
i.e.	O
,	O
@HANDLE	O
)	O
or	O
implicit	O
(	O
not	O
directly	O
naming	O
the	O
commenter	O
)	O
.	O
The	O
target	O
of	O
a	O
reply	O
is	O
not	O
coded	O
.	O
Broadcast	O
message	O
:	O
Is	O
not	O
directed	O
to	O
a	O
specific	O
person	O
(	O
s	O
)	O
.	O
Persuasiveness	O
A	O
binary	O
label	O
indicating	O
whether	O
a	O
comment	O
contains	O
persuasive	O
language	O
or	O
an	O
intent	O
to	O
persuade	O
.	O
Persuasive	O
Not	O
persuasive	O
Sentiment	O
The	O
overall	O
sentiment	O
of	O
a	O
comment	O
,	O
considering	O
how	O
the	O
user	O
feels	O
with	O
respect	O
to	O
what	O
information	O
they	O
are	O
trying	O
to	O
convey	O
.	O
Negative	O
Neutral	O
Positive	O
Mixed	O
:	O
Contains	O
both	O
positive	O
and	O
negative	O
sentiments	O
.	O
Tone	O
These	O
qualities	O
describe	O
the	O
overall	O
tone	O
of	O
a	O
comment	O
,	O
and	O
more	O
than	O
one	O
can	O
apply	O
.	O
Controversial	O
:	O
Puts	O
forward	O
a	O
strong	O
opinion	O
that	O
will	O
most	O
likely	O
cause	O
disagreement	O
.	O
Funny	O
:	O
Expresses	O
or	O
intends	O
to	O
express	O
humor	O
.	O
Informative	O
:	O
Contributes	O
new	O
information	O
to	O
the	O
discussion	O
.	O
Mean	O
:	O
The	O
purpose	O
of	O
the	O
comment	O
is	O
to	O
be	O
rude	O
,	O
mean	O
,	O
or	O
hateful	O
.	O
Sarcastic	O
:	O
Uses	O
sarcasm	O
with	O
either	O
intent	O
to	O
humor	O
(	O
overlaps	O
with	O
Funny	O
)	O
or	O
offend	O
.	O
Sympathetic	O
:	O
A	O
warm	O
,	O
friendly	O
comment	O
that	O
expresses	O
positive	O
emotion	B-DatasetName
or	O
sympathy	O
.	O
Topic	O
The	O
topic	O
addressed	O
in	O
a	O
comment	O
,	O
and	O
more	O
than	O
one	O
label	O
can	O
be	O
chosen	O
.	O
Comments	O
are	O
on	O
-	O
topic	O
unless	O
either	O
Off	O
-	O
topic	O
label	O
is	O
selected	O
.	O
Off	O
-	O
topic	O
with	O
the	O
article	O
Off	O
-	O
topic	O
with	O
the	O
conversation	O
:	O
A	O
digression	O
from	O
the	O
conversation	O
.	O
Personal	O
story	O
:	O
Describes	O
the	O
user	O
's	O
personal	O
experience	O
with	O
the	O
topic	O
.	O

With	O
the	O
taxonomy	O
described	O
above	O
,	O
we	O
coded	O
comments	O
from	O
two	O
separate	O
domains	O
:	O
online	O
news	O
articles	O
and	O
debate	O
forums	O
.	O
Threads	O
from	O
online	O
news	O
articles	O
YNACC	O
contains	O
threads	O
from	O
the	O
"	O
comments	O
section	O
"	O
of	O
Yahoo	O
News	O
articles	O
from	O
April	O
2016	O
.	O
2	O
Yahoo	O
filters	O
comments	O
containing	O
hate	B-DatasetName
speech	I-DatasetName
(	O
Nobata	O
et	O
al	O
,	O
2016	O
)	O
and	O
abusive	B-TaskName
language	I-TaskName
using	O
a	O
combination	O
of	O
manual	O
review	O
and	O
automatic	O
algorithms	O
,	O
and	O
these	O
comments	O
are	O
not	O
included	O
in	O
our	O
corpus	O
.	O
From	O
the	O
remaining	O
comments	O
,	O
we	O
identified	O
threads	O
,	O
which	O
contain	O
an	O
initial	O
comment	O
and	O
at	O
least	O
one	O
comment	O
posted	O
in	O
reply	O
.	O
Yahoo	O
threads	O
have	O
a	O
single	O
-	O
level	O
of	O
embedding	O
,	O
meaning	O
that	O
users	O
can	O
only	O
post	O
replies	O
under	O
a	O
top	O
-	O
level	O
comment	O
.	O
In	O
total	O
,	O
we	O
collected	O
521	O
,	O
608	O
comments	O
in	O
137	O
,	O
620	O
threads	O
on	O
4	O
,	O
714	O
articles	O
on	O
topics	O
including	O
finance	O
,	O
sports	O
,	O
entertainment	O
,	O
and	O
lifestyle	O
.	O
We	O
also	O
collected	O
the	O
following	O
metadata	O
for	O
each	O
comment	O
:	O
unique	O
user	O
ID	O
,	O
time	O
posted	O
,	O
headline	O
,	O
URL	O
,	O
category	O
,	O
and	O
the	O
number	O
of	O
thumbs	O
up	O
and	O
thumbs	O
down	O
received	O
.	O
We	O
included	O
comments	O
posted	O
on	O
a	O
thread	O
regardless	O
of	O
how	O
much	O
time	O
had	O
elapsed	O
since	O
the	O
initial	O
comment	O
because	O
the	O
vast	O
majority	O
of	O
comments	O
were	O
posted	O
in	O
close	O
sequence	O
:	O
48	O
%	O
in	O
the	O
first	O
hour	O
after	O
an	O
initial	O
comment	O
,	O
67	O
%	O
within	O
the	O
first	O
three	O
hours	O
,	O
and	O
92	O
%	O
within	O
the	O
first	O
24	O
hours	O
.	O
We	O
randomly	O
selected	O
2	O
,	O
300	O
threads	O
to	O
annotate	O
,	O
oversampling	O
longer	O
threads	O
since	O
the	O
aver	O
-	O

In	O
this	O
paper	O
,	O
we	O
have	O
advanced	O
a	O
synthetic	O
categorization	O
of	O
the	O
sources	O
for	O
well	O
-	O
being	O
and	O
happiness	O
.	O
We	O
have	O
used	O
a	O
corpus	O
of	O
private	O
microblogs	O
from	O
the	O
ECHO	O
application	O
to	O
explore	O
how	O
well	O
we	O
can	O
map	O
linguistic	O
expressions	O
of	O
wellbeing	O
to	O
this	O
classification	O
.	O
We	O
have	O
shown	O
that	O
FrameNet	B-DatasetName
provides	O
useful	O
generalizations	O
,	O
while	O
the	O
linguistic	O
pattern	O
learner	O
AutoSlog	O
illustrates	O
the	O
details	O
and	O
challenges	O
of	O
the	O
compositional	O
nature	O
of	O
user	O
's	O
descriptions	O
of	O
their	O
daily	O
experiences	O
.	O
Moreover	O
,	O
we	O
have	O
demonstrated	O
that	O
,	O
independently	O
,	O
each	O
of	O
these	O
methods	O
can	O
produce	O
performance	O
similar	O
to	O
that	O
of	O
conventional	O
lexical	O
methods	O
with	O
a	O
feature	O
space	O
that	O
is	O
smaller	O
,	O
and	O
,	O
in	O
the	O
case	O
of	O
FrameNet	B-DatasetName
features	O
,	O
psychologically	O
grounded	O
.	O
Our	O
Autoslog	O
exploration	O
moreover	O
reveals	O
a	O
way	O
of	O
exploring	O
the	O
space	O
of	O
patterns	O
that	O
our	O
FrameNet	B-DatasetName
mapping	O
has	O
missed	O
.	O
In	O
future	O
work	O
,	O
we	O
aim	O
to	O
automatically	O
combine	O
these	O
two	O
methods	O
and	O
bring	O
the	O
Autoslog	O
patterns	O
under	O
the	O
well	O
-	O
being	O
categorization	O
we	O
have	O
advocated	O
here	O
.	O
We	O
also	O
plan	O
to	O
investigate	O
new	O
models	O
with	O
the	O
untouched	O
6224	O
Echo	O
posts	O
,	O
as	O
well	O
as	O
larger	O
public	O
corpus	O
like	O
LiveJournal	O
.	O
In	O
addition	O
,	O
we	O
plan	O
to	O
explore	O
the	O
source	O
of	O
the	O
fact	O
that	O
there	O
are	O
more	O
positive	O
patterns	O
(	O
both	O
as	O
types	O
and	O
the	O
tokens	O
they	O
capture	O
)	O
than	O
the	O
negative	O
ones	O
,	O
which	O
directly	O
relates	O
to	O
the	O
lower	O
Neg	O
recall	O
for	O
all	O
classifiers	O
we	O
tested	O
.	O
While	O
we	O
could	O
not	O
find	O
any	O
clear	O
reason	O
in	O
our	O
examination	O
of	O
the	O
data	O
,	O
this	O
asymmetry	O
may	O
indicate	O
that	O
markers	O
of	O
negativity	O
are	O
more	O
syntactically	O
distributed	O
than	O
our	O
current	O
list	O
of	O
patterns	O
looks	O
for	O
,	O
or	O
perhaps	O
less	O
linguistically	O
reliable	O
.	O

Tired	O
of	O
Topic	B-TaskName
Models	I-TaskName
?	O
Clusters	O
of	O
Pretrained	O
Word	B-TaskName
Embeddings	I-TaskName
Make	O
for	O
Fast	O
and	O
Good	O
Topics	O
too	O
!	O

Topic	B-TaskName
models	I-TaskName
are	O
a	O
useful	O
analysis	O
tool	O
to	O
uncover	O
the	O
underlying	O
themes	O
within	O
document	O
collections	O
.	O
The	O
dominant	O
approach	O
is	O
to	O
use	O
probabilistic	O
topic	B-TaskName
models	I-TaskName
that	O
posit	O
a	O
generative	O
story	O
,	O
but	O
in	O
this	O
paper	O
we	O
propose	O
an	O
alternative	O
way	O
to	O
obtain	O
topics	O
:	O
clustering	O
pretrained	O
word	B-TaskName
embeddings	I-TaskName
while	O
incorporating	O
document	O
information	O
for	O
weighted	O
clustering	O
and	O
reranking	O
top	O
words	O
.	O
We	O
provide	O
benchmarks	O
for	O
the	O
combination	O
of	O
different	O
word	B-TaskName
embeddings	I-TaskName
and	O
clustering	O
algorithms	O
,	O
and	O
analyse	O
their	O
performance	O
under	O
dimensionality	B-TaskName
reduction	I-TaskName
with	O
PCA	B-MethodName
.	O
The	O
best	O
performing	O
combination	O
for	O
our	O
approach	O
performs	O
as	O
well	O
as	O
classical	O
topic	B-TaskName
models	I-TaskName
,	O
but	O
with	O
lower	O
runtime	O
and	O
computational	O
complexity	O
.	O

Topic	B-TaskName
models	I-TaskName
are	O
the	O
standard	O
approach	O
for	O
exploratory	O
document	O
analysis	O
(	O
Boyd	O
-	O
Graber	O
et	O
al	O
,	O
2017	O
)	O
,	O
which	O
aims	O
to	O
uncover	O
main	O
themes	O
and	O
underlying	O
narratives	O
within	O
a	O
corpus	O
.	O
But	O
in	O
times	O
of	O
distributed	O
and	O
even	O
contextualized	O
embeddings	O
,	O
are	O
they	O
the	O
only	O
option	O
?	O
This	O
work	O
explores	O
an	O
alternative	O
to	O
topic	O
modeling	O
by	O
casting	O
'	O
key	O
themes	O
'	O
or	O
'	O
topics	O
'	O
as	O
clusters	O
of	O
word	O
types	O
under	O
the	O
modern	O
distributed	O
representation	B-TaskName
learning	I-TaskName
paradigm	O
:	O
unsupervised	O
pre	O
-	O
trained	O
word	B-TaskName
embeddings	I-TaskName
provide	O
a	O
representation	O
for	O
each	O
word	O
type	O
as	O
a	O
vector	O
,	O
allowing	O
us	O
to	O
cluster	O
them	O
based	O
on	O
their	O
distance	O
in	O
high	O
-	O
dimensional	O
space	O
.	O
The	O
goal	O
of	O
this	O
work	O
is	O
not	O
to	O
strictly	O
outperform	O
,	O
but	O
rather	O
to	O
benchmark	O
standard	O
clustering	O
of	O
modern	O
embedding	O
methods	O
against	O
the	O
classical	O
approach	O
of	O
Latent	O
Dirichlet	O
Allocation	O
(	O
LDA	B-MethodName
;	O
Blei	O
et	O
al	O
,	O
2003	O
)	O
.	O
We	O
restrict	O
our	O
study	O
to	O
influential	O
embedding	O
methods	O
and	O
focus	O
on	O
centroid	O
-	O
based	O
clustering	O
algorithms	O
as	O
they	O
provide	O
a	O
natural	O
way	O
to	O
obtain	O
the	O
top	O
words	O
in	O
each	O
cluster	O
based	O
on	O
distance	O
from	O
the	O
cluster	O
center	O
.	O
1	O
Aside	O
from	O
reporting	O
the	O
best	O
performing	O
combination	O
of	O
word	B-TaskName
embeddings	I-TaskName
and	O
clustering	O
algorithm	O
,	O
we	O
are	O
also	O
interested	O
in	O
whether	O
there	O
are	O
consistent	O
patterns	O
:	O
embeddings	O
which	O
perform	O
consistently	O
well	O
across	O
clustering	O
algorithms	O
might	O
be	O
good	O
representations	O
for	O
unsupervised	O
document	O
analysis	O
,	O
clustering	O
algorithms	O
that	O
perform	O
consistently	O
well	O
are	O
more	O
likely	O
to	O
generalize	O
to	O
future	O
word	O
embedding	O
methods	O
.	O
To	O
make	O
our	O
approach	O
reliably	O
work	O
as	O
well	O
as	O
LDA	B-MethodName
,	O
we	O
incorporate	O
corpus	O
frequency	O
statistics	O
directly	O
into	O
the	O
clustering	O
algorithm	O
,	O
and	O
quantify	O
the	O
effects	O
of	O
two	O
key	O
methods	O
,	O
1	O
)	O
weighting	O
terms	O
during	O
clustering	O
and	O
2	O
)	O
reranking	O
terms	O
for	O
obtaining	O
the	O
top	O
J	O
representative	O
words	O
.	O
Our	O
contributions	O
are	O
as	O
follows	O
:	O
We	O
systematically	O
apply	O
centroid	O
-	O
based	O
clustering	O
algorithms	O
on	O
top	O
of	O
a	O
variety	O
of	O
pretrained	O
word	B-TaskName
embeddings	I-TaskName
and	O
embedding	O
methods	O
for	O
document	O
analysis	O
.	O
Through	O
weighted	O
clustering	O
and	O
reranking	O
of	O
top	O
words	O
we	O
obtain	O
sensible	O
topics	O
;	O
the	O
best	O
performing	O
combination	O
is	O
comparable	O
with	O
LDA	B-MethodName
,	O
but	O
with	O
smaller	O
time	O
complexity	O
and	O
empirical	O
runtime	O
.	O
We	O
show	O
that	O
further	O
speedups	O
are	O
possible	O
by	O
reducing	O
the	O
embedding	O
dimensions	O
by	O
up	O
to	O
80	O
%	O
using	O
PCA	B-MethodName
.	O

Analyzing	O
documents	O
by	O
clustering	O
word	B-TaskName
embeddings	I-TaskName
is	O
a	O
natural	O
idea	O
-	O
clustering	O
has	O
been	O
used	O
for	O
readability	O
assessment	O
(	O
Cha	O
et	O
al	O
,	O
2017	O
)	O
,	O
argument	B-TaskName
mining	I-TaskName
(	O
Reimers	O
et	O
al	O
,	O
2019	O
)	O
,	O
document	B-TaskName
classification	I-TaskName
and	O
document	O
clustering	O
(	O
Sano	O
et	O
al	O
,	O
2017	O
)	O
,	O
inter	O
alia	O
.	O
So	O
far	O
,	O
however	O
,	O
clustering	O
word	B-TaskName
embeddings	I-TaskName
has	O
not	O
seen	O
much	O
success	O
for	O
the	O
purposes	O
of	O
topic	O
modeling	O
.	O
While	O
many	O
modern	O
efforts	O
have	O
attempted	O
to	O
incorporate	O
word	B-TaskName
embeddings	I-TaskName
into	O
the	O
probabilistic	O
LDA	B-MethodName
framework	O
(	O
Liu	O
et	O
al	O
,	O
2015	O
;	O
Nguyen	O
et	O
al	O
,	O
2015	O
;	O
Das	O
et	O
al	O
,	O
2015	O
;	O
Batmanghelich	O
et	O
al	O
,	O
2016	O
;	O
Xun	O
et	O
al	O
,	O
2017	O
;	O
Dieng	O
et	O
al	O
,	O
2019	O
)	O
,	O
relatively	O
little	O
work	O
has	O
examined	O
the	O
feasibility	O
of	O
clustering	O
embeddings	O
directly	O
.	O
Xie	O
and	O
Xing	O
(	O
2013	O
)	O
and	O
Viegas	O
et	O
al	O
(	O
2019	O
)	O
first	O
cluster	O
documents	O
and	O
subsequently	O
find	O
words	O
within	O
each	O
cluster	O
for	O
document	O
analysis	O
.	O
Sridhar	O
(	O
2015	O
)	O
targets	O
short	O
texts	O
where	O
LDA	B-MethodName
performs	O
poorly	O
in	O
particular	O
,	O
fitting	O
GMMs	O
to	O
learned	O
word2vec	O
representations	O
.	O
De	O
Miranda	O
et	O
al	O
(	O
2019	O
)	O
cluster	O
using	O
self	O
-	O
organising	O
maps	O
,	O
but	O
provide	O
only	O
qualitative	O
results	O
.	O
In	O
contrast	O
,	O
our	O
proposed	O
approach	O
is	O
straightforward	O
to	O
implement	O
,	O
feasible	O
for	O
regular	O
length	O
documents	O
,	O
requires	O
no	O
retraining	O
of	O
embeddings	O
,	O
and	O
yields	O
qualitatively	O
and	O
quantitatively	O
convincing	O
results	O
.	O
We	O
focus	O
on	O
centroid	O
based	O
k	O
-	O
means	O
(	O
KM	O
)	O
,	O
Spherical	O
k	O
-	O
means	O
(	O
SK	O
)	O
,	O
and	O
k	O
-	O
medoids	O
(	O
KD	O
)	O
for	O
hard	O
clustering	O
,	O
and	O
von	O
Mises	O
-	O
Fisher	O
Models	O
(	O
VMFM	O
)	O
and	O
Gaussian	O
Mixture	O
Models	O
(	O
GMM	O
)	O
for	O
soft	O
clustering	O
;	O
as	O
pre	O
-	O
trained	O
embeddings	O
we	O
consider	O
word2vec	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
,	O
GloVe	B-MethodName
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
,	O
FastText	B-MethodName
(	O
Bojanowski	O
et	O
al	O
,	O
2017	O
,	O
Spherical	O
(	O
Meng	O
et	O
al	O
,	O
2019	O
)	O
,	O
ELMo	B-MethodName
(	O
Peters	O
et	O
al	O
,	O
2018	O
)	O
,	O
andBERT	O
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
.	O

In	O
traditional	O
topic	O
modeling	O
(	O
LDA	B-MethodName
)	O
,	O
the	O
top	O
J	O
words	O
are	O
those	O
with	O
highest	O
probability	O
under	O
each	O
topic	O
-	O
word	O
distribution	O
.	O
For	O
centroid	O
based	O
clustering	O
algorithms	O
,	O
the	O
top	O
words	O
of	O
some	O
cluster	O
i	O
are	O
naturally	O
those	O
closest	O
to	O
the	O
cluster	O
center	O
c	O
(	O
i	O
)	O
,	O
or	O
with	O
highest	O
probability	O
under	O
the	O
cluster	O
parameters	O
.	O
Formally	O
,	O
this	O
means	O
choosing	O
the	O
set	O
of	O
types	O
J	O
as	O
argmin	O
J	O
:	O
|	O
J	O
|	O
=	O
10	O
j	O
J	O
	O
	O
	O
	O
c	O
(	O
i	O
)	O
−	O
x	O
j	O
2	O
2	O
for	O
KM	O
/	O
KD	O
,	O
cos	O
(	O
c	O
(	O
i	O
)	O
,	O
x	O
j	O
)	O
for	O
SK	O
,	O
f	O
(	O
x	O
j	O
|	O
c	O
(	O
i	O
)	O
,	O
Σ	O
i	O
)	O
for	O
GMM	O
/	O
VMFM	O
.	O
Our	O
results	O
in	O
6	O
focus	O
on	O
KM	O
and	O
GMM	O
,	O
as	O
we	O
observe	O
that	O
k	O
-	O
medoids	O
,	O
spherical	O
KM	O
and	O
von	O
Mises	O
-	O
Fisher	O
tend	O
to	O
perform	O
worse	O
than	O
KM	O
and	O
GMM	O
(	O
see	O
App	O
.	O
A	O
,	O
App	O
.	O
B	O
)	O
.	O
Note	O
that	O
it	O
is	O
possible	O
to	O
extend	O
this	O
approach	O
to	O
obtain	O
the	O
top	O
topics	O
given	O
a	O
document	O
:	O
compute	O
similarity	O
scores	O
between	O
learned	O
topic	O
cluster	O
centers	O
and	O
all	O
word	B-TaskName
embeddings	I-TaskName
from	O
that	O
particular	O
document	O
,	O
and	O
normalize	O
them	O
using	O
softmax	B-MethodName
to	O
obtain	O
a	O
(	O
non	O
-	O
calibrated	O
)	O
probability	O
distribution	O
.	O
Crucial	O
to	O
our	O
method	O
is	O
the	O
incorporation	O
of	O
corpus	O
statistics	O
on	O
top	O
of	O
vanilla	O
clustering	O
algorithms	O
,	O
which	O
we	O
will	O
describe	O
in	O
the	O
remainder	O
of	O
this	O
section	O
.	O

The	O
complexity	O
of	O
KM	O
is	O
O	O
(	O
tknm	O
)	O
,	O
and	O
of	O
GMM	O
is	O
O	O
(	O
tknm	O
3	O
)	O
,	O
for	O
t	O
iterations	O
,	O
3	O
k	O
clusters	O
(	O
topics	O
)	O
,	O
n	O
word	O
types	O
(	O
unique	O
vocabulary	O
)	O
,	O
and	O
m	O
embedding	O
dimensions	O
.	O
Weighted	O
variants	O
have	O
a	O
oneoff	O
cost	O
of	O
weight	O
initialization	O
,	O
and	O
contribute	O
a	O
constant	O
factor	O
when	O
recalulculating	O
the	O
centroid	O
during	O
clustering	O
.	O
Reranking	O
has	O
an	O
additional	O
O	O
(	O
n	O
log	O
(	O
n	O
k	O
)	O
)	O
factor	O
,	O
where	O
n	O
k	O
is	O
the	O
average	O
number	O
of	O
elements	O
in	O
a	O
cluster	O
.	O
In	O
contrast	O
,	O
LDA	B-MethodName
via	O
collapsed	O
Gibbs	O
sampling	O
has	O
a	O
complexity	O
of	O
O	O
(	O
tkN	O
)	O
,	O
where	O
N	O
is	O
the	O
number	O
of	O
all	O
tokens	O
,	O
so	O
when	O
N	O
n	O
,	O
clustering	O
methods	O
can	O
potentially	O
achieve	O
better	O
performance	O
-	O
complexity	O
tradeoffs	O
.	O
Note	O
that	O
running	O
ELMo	B-MethodName
and	O
BERT	B-MethodName
over	O
documents	O
also	O
requires	O
iterating	O
over	O
all	O
tokens	O
,	O
but	O
only	O
once	O
,	O
and	O
not	O
for	O
every	O
topic	O
and	O
iteration	O
.	O

For	O
readily	O
available	O
pretrained	O
word	B-TaskName
embeddings	I-TaskName
such	O
as	O
word2vec	O
,	O
FastText	B-MethodName
,	O
GloVe	B-MethodName
and	O
Spherical	O
,	O
the	O
embeddings	O
can	O
be	O
considered	O
as	O
'	O
given	O
'	O
as	O
the	O
practioner	O
does	O
not	O
need	O
to	O
generate	O
these	O
embeddings	O
from	O
scratch	O
.	O
However	O
for	O
contextual	O
embeddings	O
such	O
as	O
ELMo	B-MethodName
and	O
BERT	B-MethodName
,	O
there	O
is	O
additional	O
computational	O
cost	O
in	O
obtaining	O
these	O
embeddings	O
before	O
clustering	O
,	O
which	O
requires	O
passing	O
through	O
RNN	O
and	O
transformer	O
layers	O
respectively	O
.	O
This	O
can	O
be	O
trivially	O
parallelised	O
by	O
batching	O
the	O
context	O
window	O
(	O
usually	O
a	O
sentence	O
)	O
.	O
We	O
use	O
standard	O
pretrained	O
ELMo	B-MethodName
and	O
BERT	B-MethodName
models	O
in	O
our	O
experiments	O
and	O
therefore	O
do	O
not	O
consider	O
the	O
runtime	O
of	O
training	O
these	O
models	O
from	O
scratch	O
.	O

We	O
adopt	O
a	O
standard	O
60	O
-	O
40	O
train	O
-	O
test	O
split	O
for	O
20NG	O
and	O
70	O
-	O
30	O
for	O
Reuters	O
.	O
The	O
top	O
10	O
words	O
(	O
3.1	O
)	O
were	O
evaluated	O
using	O
normalized	O
pointwise	O
mutual	O
information	O
(	O
NPMI	O
;	O
Bouma	O
,	O
2009	O
)	O
which	O
has	O
been	O
shown	O
to	O
correlate	O
with	O
human	O
judgements	O
(	O
Lau	O
et	O
al	O
,	O
2014	O
)	O
.	O
NPMI	O
ranges	O
from	O
[	O
−1	O
,	O
1	O
]	O
with	O
1	O
indicating	O
perfect	O
association	O
.	O
The	O
train	O
split	O
is	O
used	O
to	O
obtain	O
the	O
top	O
topic	O
words	O
in	O
an	O
unsupervised	O
fashion	O
(	O
we	O
do	O
not	O
use	O
any	O
document	O
labels	O
)	O
,	O
and	O
the	O
test	O
split	O
is	O
used	O
to	O
evaluate	O
the	O
"	O
topic	O
coherence	O
"	O
of	O
these	O
top	O
words	O
.	O
NPMI	O
scores	O
are	O
averaged	O
across	O
all	O
topics	O
.	O
For	O
both	O
datasets	O
we	O
use	O
20	O
topics	O
;	O
which	O
gives	O
best	O
NPMI	O
out	O
of	O
20	O
,	O
50	O
,	O
100	O
topics	O
for	O
Reuters	O
,	O
and	O
is	O
the	O
ground	O
truth	O
number	O
for	O
20NG	O
.	O
The	O
NPMI	O
scores	O
presented	O
in	O
Table	O
1	O
are	O
averaged	O
across	O
cluster	O
centers	O
initialized	O
using	O
5	O
random	O
seeds	B-DatasetName
.	O

We	O
lowercase	O
tokens	O
,	O
remove	O
stopwords	O
,	O
punctuation	O
and	O
digits	O
,	O
and	O
exclude	O
words	O
that	O
appear	O
in	O
less	O
than	O
5	O
documents	O
and	O
appear	O
in	O
long	O
sentences	O
of	O
more	O
than	O
50	O
words	O
,	O
removing	O
email	O
artifacts	O
and	O
noisy	O
token	O
sequences	O
which	O
are	O
not	O
valid	O
sentences	O
.	O
An	O
analysis	O
on	O
the	O
effect	O
of	O
rare	O
word	O
removal	O
can	O
be	O
found	O
in	O
6.2	O
.	O
For	O
contextualized	O
word	B-TaskName
embeddings	I-TaskName
(	O
BERT	B-MethodName
and	O
ELMo	B-MethodName
)	O
,	O
sentences	O
served	O
as	O
the	O
context	O
window	O
to	O
obtain	O
the	O
token	O
representations	O
.	O
Subword	O
representations	O
were	O
averaged	O
for	O
BERT	B-MethodName
,	O
which	O
performs	O
better	O
than	O
just	O
using	O
the	O
first	O
subword	O
.	O

Running	O
LDA	B-MethodName
with	O
MALLET	O
(	O
McCallum	O
,	O
2002	O
)	O
takes	O
a	O
minute	O
,	O
but	O
performs	O
no	O
better	O
than	O
KM	O
w	O
r	O
,	O
which	O
takes	O
little	O
more	O
than	O
10	O
seconds	O
on	O
CPU	O
using	O
sklearn	O
(	O
Pedregosa	O
et	O
al	O
,	O
2011	O
)	O
,	O
and	O
3	O
-	O
4	O
seconds	O
using	O
a	O
simple	O
implementation	O
using	O
JAX	O
(	O
Bradbury	O
et	O
al	O
,	O
2018	O
)	O
on	O
GPU	O
.	O

From	O
Table	O
1	O
,	O
we	O
see	O
that	O
reranking	O
and	O
weighting	O
greatly	O
improves	O
clustering	O
performance	O
across	O
different	O
embeddings	O
.	O
As	O
a	O
first	O
step	O
to	O
uncover	O
why	O
,	O
we	O
investigate	O
how	O
sensitive	O
our	O
methods	O
are	O
to	O
restricting	O
the	O
clustering	O
to	O
only	O
frequently	O
appearing	O
word	O
types	O
.	O
Visualized	O
in	O
Fig	O
.	O
3	O
,	O
we	O
find	O
that	O
as	O
we	O
vary	O
the	O
cutoff	O
term	O
frequency	O
,	O
thus	O
changing	O
the	O
vocabulary	O
size	O
and	O
allowing	O
more	O
rare	O
words	O
on	O
the	O
x	O
-	O
axis	O
,	O
NPMI	O
is	O
more	O
affected	O
for	O
the	O
models	O
without	O
reweighting	O
.	O
This	O
suggests	O
that	O
reweighting	O
using	O
term	O
frequency	O
is	O
effective	O
for	O
clustering	O
without	O
the	O
need	O
for	O
ad	O
-	O
hoc	O
restriction	O
of	O
infrequent	O
terms	O
-	O
without	O
it	O
,	O
all	O
combinations	O
perform	O
poorly	O
compared	O
to	O
LDA	B-MethodName
.	O
In	O
general	O
,	O
GMM	O
outperforms	O
KM	O
for	O
both	O
weighted	O
and	O
unweighted	O
variants	O
averaged	O
across	O
all	O
embedding	O
methods	O
(	O
p	O
<	O
0.05	O
)	O
.	O
7	O

For	O
KM	O
,	O
extracted	O
topics	O
before	O
reranking	O
results	O
in	O
reasonable	O
looking	O
themes	O
,	O
but	O
scores	O
poorly	O
on	O
NPMI	O
.	O
Reranking	O
strongly	O
improves	O
KM	O
on	O
average	O
(	O
p	O
<	O
0.02	O
)	O
for	O
both	O
Reuters	O
and	O
20NG	O
.	O
Examples	O
before	O
and	O
after	O
reranking	O
are	O
provided	O
in	O
Table	O
2	O
.	O
This	O
indicates	O
that	O
while	O
cluster	O
centers	O
are	O
centered	O
around	O
valid	O
themes	O
,	O
they	O
are	O
surrounded	O
by	O
low	O
frequency	O
word	O
types	O
.	O
We	O
observe	O
that	O
when	O
applying	O
reranking	O
to	O
GMM	O
w	O
the	O
gains	O
are	O
much	O
less	O
pronounced	O
than	O
KM	O
w	O
.	O
The	O
top	O
topic	O
words	O
before	O
and	O
after	O
reranking	O
for	O
BERT	B-MethodName
-	O
GMM	O
w	O
have	O
an	O
average	O
Jaccard	O
similarity	O
score	O
of	O
0.910	O
,	O
indicating	O
that	O
the	O
cluster	O
centers	O
learned	O
by	O
weighted	O
GMMs	O
are	O
already	O
centered	O
at	O
word	O
types	O
of	O
high	O
frequency	O
in	O
the	O
training	O
corpus	O
.	O

Spherical	O
embeddings	O
and	O
BERT	B-MethodName
perform	O
consistently	O
well	O
across	O
both	O
datasets	O
.	O
For	O
20NG	O
,	O
KM	O
w	O
r	O
Spherical	O
and	O
LDA	B-MethodName
both	O
achieve	O
0.26	O
NPMI	O
.	O
For	O
Reuters	O
,	O
GMM	O
w	O
r	O
BERT	B-MethodName
achieves	O
the	O
top	O
NPMI	O
score	O
of	O
0.15	O
compared	O
to	O
0.12	O
of	O
LDA	B-MethodName
.	O
Word2vec	O
and	O
ELMo	B-MethodName
(	O
using	O
only	O
the	O
last	O
layer	O
8	O
)	O
perform	O
poorly	O
compared	O
to	O
the	O
other	O
embeddings	O
.	O
Fast	O
-	O
Text	O
and	O
GloVe	B-MethodName
can	O
achieve	O
similar	O
performance	O
to	O
BERT	B-MethodName
on	O
20NG	O
but	O
are	O
slightly	O
inferior	O
on	O
Reuters	O
.	O
Training	O
or	O
fine	O
-	O
tuning	O
embeddings	O
on	O
the	O
given	O
data	O
prior	O
to	O
clustering	O
could	O
potentially	O
achieve	O
better	O
performance	O
,	O
but	O
we	O
leave	O
this	O
to	O
future	O
work	O
.	O

We	O
find	O
that	O
our	O
approach	O
yields	O
a	O
greater	O
diversity	O
within	O
topics	O
as	O
compared	O
to	O
LDA	B-MethodName
while	O
achieving	O
comparable	O
coherence	O
scores	O
(	O
App	O
.	O
D	O
)	O
.	O
Such	O
topics	O
are	O
arguably	O
more	O
valuable	O
for	O
exploratory	O
analysis	O
.	O

We	O
outlined	O
a	O
methodology	O
for	O
clustering	O
word	B-TaskName
embeddings	I-TaskName
for	O
unsupervised	O
document	O
analysis	O
,	O
and	O
presented	O
a	O
systematic	O
comparison	O
of	O
various	O
influential	O
embedding	O
methods	O
and	O
clustering	O
algorithms	O
.	O
Our	O
experiments	O
suggest	O
that	O
pretrained	O
word	B-TaskName
embeddings	I-TaskName
(	O
both	O
contextualized	O
and	O
non	O
-	O
contextualized	O
)	O
,	O
combined	O
with	O
tf	O
-	O
weighted	O
k	O
-	O
means	O
and	O
tf	O
-	O
based	O
reranking	O
,	O
provide	O
a	O
viable	O
alternative	O
to	O
traditional	O
topic	O
modeling	O
at	O
lower	O
complexity	O
and	O
runtime	O
.	O

We	O
thank	O
Aaron	O
Mueller	O
,	O
Pamela	O
Shapiro	O
,	O
Li	O
Ke	O
,	O
Adam	B-MethodName
Poliak	O
,	O
Kevin	O
Duh	O
and	O
the	O
anonymous	O
reviewers	O
for	O
their	O
feedback	O
.	O

We	O
present	O
the	O
different	O
topics	O
generated	O
using	O
LDA	B-MethodName
(	O
Table	O
7	O
)	O
and	O
topics	O
generated	O
using	O
BERT	B-MethodName
KM	O
w	O
r	O
for	O
the	O
Reuters	O
dataset	O
(	O

Despite	O
the	O
widespread	O
success	O
of	O
selfsupervised	O
learning	O
via	O
masked	O
language	O
models	O
(	O
MLM	B-DatasetName
)	O
,	O
accurately	O
capturing	O
fine	O
-	O
grained	O
semantic	O
relationships	O
in	O
the	O
biomedical	O
domain	O
remains	O
a	O
challenge	O
.	O
This	O
is	O
of	O
paramount	O
importance	O
for	O
entity	O
-	O
level	O
tasks	O
such	O
as	O
entity	B-TaskName
linking	I-TaskName
where	O
the	O
ability	O
to	O
model	O
entity	O
relations	O
(	O
especially	O
synonymy	O
)	O
is	O
pivotal	O
.	O
To	O
address	O
this	O
challenge	O
,	O
we	O
propose	O
SAPBERT	O
,	O
a	O
pretraining	O
scheme	O
that	O
selfaligns	O
the	O
representation	O
space	O
of	O
biomedical	O
entities	O
.	O
We	O
design	O
a	O
scalable	O
metric	B-TaskName
learning	I-TaskName
framework	O
that	O
can	O
leverage	O
UMLS	B-DatasetName
,	O
a	O
massive	O
collection	O
of	O
biomedical	O
ontologies	O
with	O
4M+	O
concepts	O
.	O
In	O
contrast	O
with	O
previous	O
pipelinebased	O
hybrid	O
systems	O
,	O
SAPBERT	O
offers	O
an	O
elegant	O
one	O
-	O
model	O
-	O
for	O
-	O
all	O
solution	O
to	O
the	O
problem	O
of	O
medical	O
entity	B-TaskName
linking	I-TaskName
(	O
MEL	O
)	O
,	O
achieving	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
(	O
SOTA	O
)	O
on	O
six	O
MEL	O
benchmarking	O
datasets	O
.	O
In	O
the	O
scientific	O
domain	O
,	O
we	O
achieve	O
SOTA	O
even	O
without	O
taskspecific	O
supervision	O
.	O
With	O
substantial	O
improvement	O
over	O
various	O
domain	O
-	O
specific	O
pretrained	O
MLMs	O
such	O
as	O
BIOBERT	O
,	O
SCIBERT	O
and	O
PUB	O
-	O
MEDBERT	O
,	O
our	O
pretraining	O
scheme	O
proves	O
to	O
be	O
both	O
effective	O
and	O
robust	O
.	O
1	O

Biomedical	O
entity	O
2	O
representation	O
is	O
the	O
foundation	O
for	O
a	O
plethora	O
of	O
text	O
mining	O
systems	O
in	O
the	O
medical	B-DatasetName
domain	I-DatasetName
,	O
facilitating	O
applications	O
such	O
as	O
literature	O
search	O
(	O
Lee	O
et	O
al	O
,	O
2016	O
)	O
,	O
clinical	O
decision	B-TaskName
making	I-TaskName
(	O
Roberts	O
et	O
al	O
,	O
2015	O
)	O
and	O
relational	O
knowledge	O
discovery	O
(	O
e.g.	O
chemical	O
-	O
disease	O
,	O
drug	O
-	O
drug	O
and	O
protein	O
-	O
protein	O
relations	O
,	O
Wang	O
et	O
al	O
2018	O
)	O
.	O
The	O
heterogeneous	O
naming	O
of	O
biomedical	O
concepts	O
*	O
Work	O
conducted	O
prior	O
to	O
joining	O
Amazon	O
.	O
1	O
For	O
code	O
and	O
pretrained	O
models	O
,	O
please	O
visit	O
:	O
https	O
:	O
//github.com	O
/	O
cambridgeltl	O
/	O
sapbert	O
.	O
2	O
In	O
this	O
work	O
,	O
biomedical	O
entity	O
refers	O
to	O
the	O
surface	O
forms	O
of	O
biomedical	O
concepts	O
,	O
which	O
can	O
be	O
a	O
single	O
word	O
(	O
e.g.	O
fever	O
)	O
,	O
a	O
compound	O
(	O
e.g.	O
sars	O
-	O
cov	O
-	O
2	O
)	O
or	O
a	O
short	O
phrase	O
(	O
e.g.	O
abnormal	O
retinal	O
vascular	O
development	O
)	O
.	O
poses	O
a	O
major	O
challenge	O
to	O
representation	B-TaskName
learning	I-TaskName
.	O
For	O
instance	O
,	O
the	O
medication	O
Hydroxychloroquine	O
is	O
often	O
referred	O
to	O
as	O
Oxichlorochine	O
(	O
alternative	O
name	O
)	O
,	O
HCQ	O
(	O
in	O
social	O
media	O
)	O
and	O
Plaquenil	O
(	O
brand	O
name	O
)	O
.	O

Data	O
Preparation	O
Details	O
for	O
UMLS	B-DatasetName
Pretraining	O
.	O
We	O
download	O
the	O
full	O
release	O
of	O
UMLS	B-DatasetName
2020AA	O
version	O
.	O
9	O
We	O
then	O
extract	O
all	O
English	O
entries	O
from	O
the	O
MRCONSO.RFF	O
raw	O
file	O
and	O
convert	O
all	O
entity	O
names	O
into	O
lowercase	O
(	O
duplicates	O
are	O
removed	O
)	O
.	O
Besides	O
synonyms	O
defined	O
in	O
MRCONSO.RFF	O
,	O
we	O
also	O
include	O
tradenames	O
of	O
drugs	O
as	O
synonyms	O
(	O
extracted	O
from	O
MRREL.RRF	O
)	O
.	O
After	O
pre	O
-	O
processing	O
,	O
a	O
list	O
of	O
9	O
,	O
712	O
,	O
959	O
(	O
name	O
,	O
CUI	O
)	O
entries	O
is	O
obtained	O
.	O
However	O
,	O
random	O
batching	O
on	O
this	O
list	O
can	O
lead	O
to	O
very	O
few	O
(	O
if	O
not	O
none	O
)	O
positive	O
pairs	O
within	O
a	O
mini	O
-	O
batch	O
.	O
To	O
ensure	O
sufficient	O
positives	O
present	O
in	O
each	O
mini	O
-	O
batch	O
,	O
we	O
generate	O
offline	O
positive	O
pairs	O
in	O
the	O
format	O
of	O
(	O
name	O
1	O
,	O
name	O
2	O
,	O
CUI	O
)	O
where	O
name	O
1	O
and	O
name	O
2	O
have	O
the	O
same	O
CUI	O
label	O
.	O
This	O
can	O
be	O
achieved	O
by	O
enumerating	O
all	O
possible	O
combinations	O
of	O
synonym	O
pairs	O
with	O
common	O
CUIs	O
.	O
For	O
balanced	O
training	O
,	O
any	O
concepts	O
with	O
more	O
than	O
50	O
positive	O
pairs	O
are	O
randomly	O
trimmed	O
to	O
50	O
pairs	O
.	O
In	O
the	O
end	O
we	O
obtain	O
a	O
training	O
list	O
with	O
11	O
,	O
792	O
,	O
953	O
pairwise	O
entries	O
.	O

We	O
present	O
SAPBERT	O
,	O
a	O
self	O
-	O
alignment	O
pretraining	O
scheme	O
for	O
learning	O
biomedical	O
entity	O
representations	O
.	O
We	O
highlight	O
the	O
consistent	O
performance	O
boost	O
achieved	O
by	O
SAPBERT	O
,	O
obtaining	O
new	O
SOTA	O
in	O
all	O
six	O
widely	O
used	O
MEL	O
benchmarking	O
datasets	O
.	O
Strikingly	O
,	O
without	O
any	O
fine	O
-	O
tuning	O
on	O
task	O
-	O
specific	O
labelled	O
data	O
,	O
SAPBERT	O
already	O
outperforms	O
the	O
previous	O
supervised	O
SOTA	O
(	O
sophisticated	O
hybrid	O
entity	B-TaskName
linking	I-TaskName
systems	O
)	O
on	O
multiple	O
datasets	O
in	O
the	O
scientific	O
language	O
domain	O
.	O
Our	O
work	O
opens	O
new	O
avenues	O
to	O
explore	O
for	O
general	O
domain	O
self	O
-	O
alignment	O
(	O
e.g.	O
by	O
leveraging	O
knowledge	B-TaskName
graphs	I-TaskName
such	O
as	O
DBpedia	B-DatasetName
)	O
.	O
We	O
plan	O
to	O
incorporate	O
other	O
types	O
of	O
relations	O
(	O
i.e.	O
,	O
hypernymy	O
and	O
hyponymy	O
)	O
and	O
extend	O
our	O
model	O
to	O
sentence	O
-	O
level	O
representation	B-TaskName
learning	I-TaskName
.	O
In	O
particular	O
,	O
our	O
ongoing	O
work	O
using	O
a	O
combination	O
of	O
SAPBERT	O
and	O
ADAPTER	O
is	O
a	O
promising	O
direction	O
for	O
tackling	O
sentence	O
-	O
level	O
tasks	O
.	O

NCBI	B-DatasetName
disease	I-DatasetName
(	O
Dogan	O
et	O
al	O
,	O
2014	O
)	O
is	O
a	O
corpus	O
containing	O
793	O
fully	O
annotated	O
PubMed	O
abstracts	O
and	O
6	O
,	O
881	O
mentions	O
.	O
The	O
mentions	O
are	O
mapped	O
into	O
the	O
MEDIC	B-DatasetName
dictionary	O
(	O
Davis	O
et	O
al	O
,	O
2012	O
)	O
.	O
We	O
denote	O
this	O
dataset	O
as	O
"	O
NCBI	O
"	O
in	O
our	O
experiments	O
.	O
BC5CDR	B-DatasetName
(	O
Li	O
et	O
al	O
,	O
2016	O
)	O
consists	O
of	O
1	O
,	O
500	O
PubMed	O
articles	O
with	O
4	O
,	O
409	O
annotated	O
chemicals	O
,	O
5	O
,	O
818	O
diseases	O
and	O
3	O
,	O
116	O
chemical	O
-	O
disease	O
interactions	O
.	O
The	O
disease	O
mentions	O
are	O
mapped	O
into	O
the	O
MEDIC	B-DatasetName
dictionary	O
like	O
the	O
NCBI	B-DatasetName
disease	I-DatasetName
corpus	I-DatasetName
.	O
The	O
chemical	O
mentions	O
are	O
mapped	O
into	O
the	O
Comparative	O
Toxicogenomics	O
Database	O
(	O
CTD	O
)	O
(	O
Davis	O
et	O
al	O
,	O
2019	O
)	O
chemical	O
dictionary	O
.	O
We	O
denote	O
the	O
disease	O
and	O
chemical	O
mention	O
sets	O
as	O
"	O
BC5CDRd	O
"	O
and	O
"	O
BC5CDR	B-DatasetName
-	O
c	O
"	O
respectively	O
.	O
For	O
NCBI	O
and	O
BC5CDR	B-DatasetName
we	O
use	O
the	O
same	O
data	O
and	O
evaluation	O
protocol	O
by	O
Sung	O
et	O
al	O
(	O
2020	O
)	O
.	O
11	O
MedMentions	B-DatasetName
(	O
Mohan	O
and	O
Li	O
,	O
2018	O
)	O
is	O
a	O
verylarge	O
-	O
scale	O
entity	B-TaskName
linking	I-TaskName
dataset	O
containing	O
over	O
4	O
,	O
000	O
abstracts	O
and	O
over	O
350	O
,	O
000	O
mentions	O
linked	O
to	O
UMLS	B-DatasetName
2017AA	O
.	O
According	O
to	O
Mohan	O
and	O
Li	O
(	O
2018	O
)	O
,	O
training	O
TAGGERONE	O
,	O
a	O
very	O
popular	O
MEL	O
system	O
,	O
on	O
a	O
subset	O
of	O
MedMentions	B-DatasetName
require	O
>	O
900	O
GB	O
of	O
RAM	B-MethodName
.	O
Its	O
massive	O
number	O
of	O
mentions	O
and	O
more	O
importantly	O
the	O
used	O
reference	O
ontology	B-MethodName
(	O
UMLS	B-DatasetName
2017AA	O
has	O
3M+	O
concepts	O
)	O
make	O
the	O
application	O
of	O
most	O
MEL	O
systems	O
infeasible	O
.	O
However	O
,	O
through	O
our	O
metric	B-TaskName
learning	I-TaskName
formulation	O
,	O
SAPBERT	O
can	O
be	O
applied	O
on	O
MedMentions	B-DatasetName
with	O
minimal	O
effort	O
.	O

AskAPatient	O
(	O
Limsopatham	O
and	O
Collier	O
,	O
2016	O
)	O
includes	O
17	O
,	O
324	O
adverse	O
drug	O
reaction	O
(	O
ADR	O
)	O
annotations	O
collected	O
from	O
askapatient.com	O
blog	O
posts	O
.	O
The	O
mentions	O
are	O
mapped	O
to	O
1	O
,	O
036	O
medical	O
concepts	O
grounded	O
onto	O
SNOMED	O
-	O
CT	O
(	O
Donnelly	O
,	O
2006	O
)	O
and	O
AMT	O
(	O
the	O
Australian	O
Medicines	O
Terminology	O
)	O
.	O
For	O
this	O
dataset	O
,	O
we	O
follow	O
the	O
10	O
-	O
fold	O
evaluation	O
protocol	O
stated	O
in	O
the	O
original	O
paper	O
.	O
12	O
COMETA	B-DatasetName
(	O
Basaldella	O
et	O
al	O
,	O
2020	O
)	O
is	O
a	O
recently	O
released	O
large	O
-	O
scale	O
MEL	O
dataset	O
that	O
specifically	O
focuses	O
on	O
MEL	O
in	O
the	O
social	O
media	O
domain	O
,	O
containing	O
around	O
20k	O
medical	O
mentions	O
extracted	O
from	O
health	O
-	O
related	O
discussions	O
on	O
reddit.com	O
.	O
Mentions	O
are	O
mapped	O
to	O
SNOMED	O
-	O
CT	O
.	O
We	O
use	O
the	O
"	O
stratified	O
(	O
general	O
)	O
"	O
split	O
and	O
follow	O
the	O
evaluation	O
protocol	O
of	O
the	O
original	O
paper	O
.	O
13	O

We	O
list	O
all	O
the	O
versions	O
of	O
BERT	B-MethodName
models	O
used	O
in	O
this	O
study	O
,	O
linking	O
to	O
the	O
specific	O
versions	O
in	O
Tab	O
.	O
5	O
.	O
Note	O
that	O
we	O
exhaustively	O
tried	O
all	O
official	O
variants	O
of	O
the	O
selected	O
models	O
and	O
the	O
best	O
performing	O
ones	O
are	O
chosen	O
.	O
All	O
BERT	B-MethodName
models	O
refer	O
to	O
the	O
BERT	B-MethodName
Base	O
architecture	O
in	O
this	O
paper	O
.	O
S	O
denotes	O
the	O
set	O
of	O
all	O
surface	O
forms	O
/	O
synonyms	O
of	O
all	O
concepts	O
in	O
C	O
;	O
M	O
denotes	O
the	O
set	O
of	O
mentions	O
/	O
queries	O
.	O
COMETA	B-DatasetName
(	O
s.g	O
.	O
)	O
and	O
(	O
z.g	O
.	O
)	O
are	O
the	O
stratified	O
(	O
general	O
)	O
and	O
zeroshot	O
(	O
general	O
)	O
split	O
respectively	O
.	O
model	O
NCBI	O
BC5CDR	B-DatasetName
-	O
d	O
BC5CDR	B-DatasetName
-	O
c	O
MedMentions	B-DatasetName
AskAPatient	O
COMETA	B-DatasetName
@1	O
@5	O
@1	O
@5	O
@1	O
@5	O
@1	O
@5	O
@1	O
@5	O
@1	O
@5	O
SIEVE	O
-	O
BASED	O
(	O
D'Souza	O
and	O
Ng	O
,	O
2015	O
)	O
84.7	O
-	O
84.1	O
-	O
90.7	O
-	O
-	O
-	O
WORDCNN	O
(	O
Limsopatham	O
and	O
Collier	O
,	O
2016	O
)	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
81.4	O
-	O
-	O
-	O
WORDGRU+TF	O
-	O
IDF	O
(	O
Tutubalina	O
et	O
al	O
,	O
2018	O
)	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
85.7	O
-	O
-	O
-	O
TAGGERONE	O
87.7	O
-	O
88.9	O
-	O
94.1	O
-	O
OOM	O
OOM	O
-	O
-	O
-	O
-	O
NORMCO	O
(	O
Wright	O
et	O
al	O
,	O
2019	O
)	O
87.8	O
-	O
88.0	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
BNE	O
(	O
Phan	O
et	O
al	O
,	O
2019	O
)	O
87.7	O
-	O
90.6	O
-	O
95.8	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
BERTRANK	O
(	O
Ji	O
et	O
al	O
,	O
2020	O
)	O
89	O
.	O

We	O
thank	O
the	O
three	O
reviewers	O
and	O
the	O
Area	O
Chair	O
for	O
their	O
insightful	O
comments	O
and	O
suggestions	O
.	O
FL	O
is	O
supported	O
by	O
Grace	O
&	O
Thomas	O
C.H.	O
Chan	O
Cambridge	B-DatasetName
Scholarship	O
.	O
NC	O
and	O
MB	O
would	O
like	O
to	O

Practical	O
summarization	B-TaskName
systems	O
are	O
expected	O
to	O
produce	O
summaries	O
of	O
varying	O
lengths	O
,	O
per	O
user	O
needs	O
.	O
While	O
a	O
couple	O
of	O
early	O
summarization	B-TaskName
benchmarks	O
tested	O
systems	O
across	O
multiple	O
summary	O
lengths	O
,	O
this	O
practice	O
was	O
mostly	O
abandoned	O
due	O
to	O
the	O
assumed	O
cost	O
of	O
producing	O
reference	O
summaries	O
of	O
multiple	O
lengths	O
.	O
In	O
this	O
paper	O
,	O
we	O
raise	O
the	O
research	O
question	O
of	O
whether	O
reference	O
summaries	O
of	O
a	O
single	O
length	O
can	O
be	O
used	O
to	O
reliably	O
evaluate	O
system	O
summaries	O
of	O
multiple	O
lengths	O
.	O
For	O
that	O
,	O
we	O
have	O
analyzed	O
a	O
couple	O
of	O
datasets	O
as	O
a	O
case	O
study	O
,	O
using	O
several	O
variants	O
of	O
the	O
ROUGE	O
metric	O
that	O
are	O
standard	O
in	O
summarization	B-TaskName
evaluation	O
.	O
Our	O
findings	O
indicate	O
that	O
the	O
evaluation	O
protocol	O
in	O
question	O
is	O
indeed	O
competitive	O
.	O
This	O
result	O
paves	O
the	O
way	O
to	O
practically	O
evaluating	O
varying	O
-	O
length	O
summaries	O
with	O
simple	O
,	O
possibly	O
existing	O
,	O
summarization	B-TaskName
benchmarks	O
.	O

Automated	O
summarization	B-TaskName
systems	O
typically	O
produce	O
a	O
text	O
that	O
mimics	O
a	O
manual	O
summary	O
.	O
In	O
these	O
systems	O
,	O
an	O
important	O
aspect	O
is	O
the	O
output	O
summary	O
length	O
,	O
which	O
may	O
vary	O
according	O
to	O
user	O
needs	O
.	O
Consequently	O
,	O
output	O
length	O
has	O
been	O
a	O
common	O
tunable	O
parameter	O
in	O
pre	O
-	O
neural	O
summarization	B-TaskName
systems	O
and	O
has	O
been	O
incorporated	O
recently	O
in	O
few	O
neural	O
models	O
as	O
well	O
(	O
Kikuchi	O
et	O
al	O
,	O
2016	O
;	O
Fan	O
et	O
al	O
,	O
2017	O
;	O
Ficler	O
and	O
Goldberg	O
,	O
2017	O
)	O
.	O
It	O
was	O
originally	O
assumed	O
that	O
summarization	B-TaskName
systems	O
should	O
be	O
assessed	O
across	O
multiple	O
summary	O
lengths	O
.	O
For	O
that	O
,	O
the	O
earliest	O
Document	O
Understand	O
Conference	O
(	O
DUC	O
)	O
(	O
NIST	O
,	O
2011	O
(	O
NIST	O
,	O
)	O
benchmarks	O
,	O
in	O
2001	O
(	O
NIST	O
,	O
and	O
2002	O
,	O
defined	O
several	O
target	O
summary	O
lengths	O
and	O
evaluated	O
each	O
summary	O
against	O
(	O
manually	O
written	O
)	O
reference	O
summaries	O
of	O
the	O
same	O
length	O
.	O
However	O
,	O
due	O
to	O
the	O
high	O
cost	O
incurred	O
,	O
subsequent	O
DUC	O
and	O
TAC	O
(	O
NIST	O
,	O
2018	O
)	O
benchmarks	O
(	O
2003	O
)	O
(	O
2004	O
)	O
(	O
2005	O
)	O
(	O
2006	O
)	O
(	O
2007	O
)	O
(	O
2008	O
)	O
(	O
2009	O
)	O
(	O
2010	O
)	O
(	O
2011	O
)	O
(	O
2012	O
)	O
(	O
2013	O
)	O
(	O
2014	O
)	O
,	O
as	O
well	O
as	O
the	O
more	O
recently	O
popular	O
datasets	O
CNN	B-DatasetName
/	I-DatasetName
Daily	I-DatasetName
Mail	I-DatasetName
(	O
Nallapati	O
et	O
al	O
,	O
2016	O
)	O
and	O
Gigaword	O
(	O
Graff	O
et	O
al	O
,	O
2003	O
)	O
,	O
included	O
references	O
and	O
evaluation	O
for	O
just	O
one	O
summary	O
length	O
per	O
input	O
text	O
.	O
Accordingly	O
,	O
systems	O
were	O
asked	O
to	O
produce	O
a	O
single	O
summary	O
,	O
of	O
corresponding	O
length	O
.	O
This	O
decision	O
was	O
partly	O
supported	O
by	O
an	O
observation	O
that	O
system	O
rankings	O
tended	O
to	O
correlate	O
across	O
different	O
summary	O
lengths	O
(	O
Over	O
et	O
al	O
,	O
2007	O
)	O
,	O
even	O
though	O
,	O
as	O
we	O
show	O
in	O
Section	O
2	O
,	O
this	O
correlation	O
is	O
limited	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
that	O
the	O
summarization	B-TaskName
community	O
should	O
consider	O
resuming	O
evaluating	O
summarization	B-TaskName
systems	O
over	O
multiple	O
length	O
outputs	O
,	O
as	O
it	O
would	O
allow	O
better	O
assessment	O
of	O
length	O
-	O
related	O
performance	O
within	O
and	O
across	O
systems	O
(	O
illustrated	O
in	O
Section	O
3	O
)	O
.	O
To	O
avoid	O
the	O
need	O
in	O
multiple	O
-	O
length	O
reference	O
summaries	O
we	O
raise	O
the	O
following	O
research	O
question	O
:	O
can	O
reference	O
summaries	O
of	O
a	O
single	O
length	O
be	O
used	O
to	O
evaluate	O
system	O
summaries	O
of	O
multiple	O
lengths	O
,	O
as	O
reliably	O
as	O
when	O
using	O
references	O
of	O
multiple	O
lengths	O
,	O
with	O
respect	O
to	O
different	O
standard	O
evaluation	O
metrics	O
?	O
Recently	O
,	O
Kikuchi	O
et	O
al	O
(	O
2016	O
)	O
evaluated	O
system	O
summaries	O
of	O
three	O
different	O
lengths	O
against	O
reference	O
summaries	O
of	O
a	O
single	O
length	O
.	O
Yet	O
,	O
their	O
evaluation	O
methodology	O
was	O
not	O
assessed	O
through	O
correlation	O
to	O
human	O
judgment	O
,	O
as	O
has	O
been	O
commonly	O
done	O
for	O
other	O
automatic	O
evaluation	O
protocols	O
.	O
Here	O
,	O
we	O
provide	O
a	O
closer	O
look	O
into	O
this	O
methodology	O
,	O
given	O
its	O
potential	O
value	O
.	O
As	O
a	O
first	O
accessible	O
case	O
study	O
,	O
we	O
test	O
our	O
research	O
question	O
over	O
the	O
DUC	O
2001	O
and	O
2002	O
data	O
(	O
Section	O
2	O
)	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
these	O
are	O
the	O
only	O
two	O
datasets	O
that	O
include	O
multiple	O
length	O
reference	O
and	O
submitted	O
system	O
summaries	O
,	O
as	O
well	O
as	O
manual	O
assessment	O
of	O
the	O
latter	O
.	O
Our	O
analysis	O
reveals	O
that	O
,	O
for	O
this	O
data	O
and	O
with	O
respect	O
to	O
various	O
highly	O
utilized	O
automatic	O
ROUGE	O
metrics	O
,	O
the	O
answer	O
to	O
our	O
question	O
is	O
affirmative	O
,	O
in	O
terms	O
of	O
correlation	O
with	O
human	O
judgment	O
.	O
Our	O
promising	O
results	O
suggest	O
repeating	O
the	O
assessment	O
methodology	O
presented	O
here	O
in	O
future	O
work	O
,	O
to	O
test	O
our	O
question	O
over	O
more	O
recent	O
and	O
broader	O
summarization	B-TaskName
datasets	O
and	O
human	O
evaluation	O
schemes	O
.	O
This	O
,	O
in	O
turn	O
,	O
would	O
allow	O
the	O
community	O
to	O
feasibly	O
resume	O
proper	O
evaluation	O
and	O
deliberate	O
development	O
of	O
systems	O
that	O
target	O
effective	O
summaries	O
across	O
a	O
range	O
of	O
lengths	O
.	O

We	O
proposed	O
the	O
potential	O
value	O
of	O
evaluating	O
summarization	B-TaskName
systems	O
at	O
different	O
summary	O
lengths	O
.	O
Such	O
evaluations	O
would	O
allow	O
proper	O
evaluation	O
of	O
systems	O
'	O
"	O
length	O
knob	O
"	O
,	O
tracking	O
how	O
their	O
ranking	O
changes	O
across	O
summary	O
lengths	O
as	O
well	O
as	O
tracking	O
the	O
cross	O
-	O
length	O
behavior	O
of	O
individual	O
systems	O
.	O
Given	O
that	O
reference	O
summaries	O
of	O
a	O
single	O
length	O
are	O
usually	O
available	O
in	O
practice	O
,	O
we	O
analyzed	O
the	O
potential	O
use	O
of	O
reference	O
summaries	O
of	O
a	O
single	O
length	O
for	O
evaluating	O
system	O
summaries	O
of	O
multiple	O
lengths	O
.	O
We	O
found	O
,	O
on	O
the	O
only	O
two	O
datasets	O
readily	O
available	O
for	O
such	O
analysis	O
,	O
that	O
this	O
configuration	O
is	O
as	O
reliable	O
as	O
the	O
standard	O
configuration	O
,	O
which	O
evaluates	O
each	O
system	O
summary	O
against	O
a	O
reference	O
of	O
a	O
matching	O
length	O
.	O
To	O
broadly	O
substantiate	O
our	O
findings	O
,	O
we	O
propose	O
future	O
work	O
that	O
would	O
follow	O
our	O
assessment	O
methodology	O
over	O
test	O
samples	O
from	O
current	O
datasets	O
(	O
e.g.	O
CNN	O
/	O
DailyMail	O
)	O
,	O
judging	O
performance	O
of	O
current	O
systems	O
and	O
utilizing	O
current	O
manual	O
evaluation	O
protocols	O
.	O
This	O
would	O
require	O
preparing	O
,	O
for	O
limited	O
samples	O
,	O
additional	O
manually	O
crafted	O
summaries	O
of	O
several	O
lengths	O
and	O
manually	O
evaluating	O
system	O
summaries	O
of	O
corresponding	O
lengths	O
.	O
Using	O
such	O
data	O
,	O
it	O
will	O
be	O
possible	O
to	O
repeat	O
our	O
analysis	O
and	O
test	O
the	O
broader	O
validity	O
of	O
the	O
single	O
-	O
reference	O
-	O
length	O
configuration	O
.	O
If	O
broadly	O
assessed	O
,	O
it	O
will	O
be	O
possible	O
to	O
start	O
evaluating	O
system	O
summaries	O
of	O
multiple	O
lengths	O
over	O
most	O
currently	O
available	O
datasets	O
,	O
leveraging	O
the	O
available	O
single	O
-	O
length	O
reference	O
summaries	O
.	O
Fu	O
-	O
ture	O
benchmarks	O
could	O
require	O
systems	O
to	O
produce	O
different	O
length	O
outputs	O
,	O
while	O
feasibly	O
evaluating	O
them	O
using	O
the	O
existing	O
,	O
single	O
length	O
,	O
reference	O
summaries	O
.	O
This	O
,	O
in	O
turn	O
,	O
is	O
likely	O
to	O
drive	O
research	O
to	O
better	O
address	O
the	O
need	O
for	O
producing	O
high	O
quality	O
summaries	O
flexibly	O
across	O
a	O
range	O
of	O
summary	O
lengths	O
,	O
a	O
dimension	O
that	O
has	O
been	O
disregarded	O
for	O
long	O
.	O

Being	O
able	O
to	O
perform	O
in	O
-	O
depth	O
chat	O
with	O
humans	O
in	O
a	O
closed	O
domain	O
is	O
a	O
precondition	O
before	O
an	O
open	O
-	O
domain	O
chatbot	B-TaskName
can	O
ever	O
be	O
claimed	O
.	O
In	O
this	O
work	O
,	O
we	O
take	O
a	O
close	O
look	O
at	O
the	O
movie	O
domain	O
and	O
present	O
a	O
large	O
-	O
scale	O
high	O
-	O
quality	O
corpus	O
with	O
fine	O
-	O
grained	O
annotations	O
in	O
hope	O
of	O
pushing	O
the	O
limit	O
of	O
moviedomain	O
chatbots	O
.	O
We	O
propose	O
a	O
unified	O
,	O
readily	O
scalable	O
neural	O
approach	O
which	O
reconciles	O
all	O
subtasks	O
like	O
intent	O
prediction	O
and	O
knowledge	O
retrieval	O
.	O
The	O
model	O
is	O
first	O
pretrained	O
on	O
the	O
huge	O
general	O
-	O
domain	O
data	O
,	O
then	O
finetuned	O
on	O
our	O
corpus	O
.	O
We	O
show	O
this	O
simple	O
neural	O
approach	O
trained	O
on	O
high	O
-	O
quality	O
data	O
is	O
able	O
to	O
outperform	O
commercial	O
systems	O
replying	O
on	O
complex	O
rules	O
.	O
On	O
both	O
the	O
static	O
and	O
interactive	O
tests	O
,	O
we	O
find	O
responses	O
generated	O
by	O
our	O
system	O
exhibits	O
remarkably	O
good	O
engagement	O
and	O
sensibleness	O
close	O
to	O
human	O
-	O
written	O
ones	O
.	O
We	O
further	O
analyze	O
the	O
limits	O
of	O
our	O
work	O
and	O
point	O
out	O
potential	O
directions	O
for	O
future	O
work	O
1	O
.	O

Being	O
able	O
to	O
converse	B-DatasetName
like	O
humans	O
in	O
a	O
closed	O
domain	O
is	O
a	O
precondition	O
before	O
an	O
intelligent	O
opendomain	O
chatbot	B-TaskName
,	O
which	O
further	O
requires	O
transiting	O
among	O
various	O
domains	O
,	O
can	O
be	O
designed	O
Su	O
et	O
al	O
,	O
2020	O
)	O
.	O
Nonetheless	O
,	O
even	O
if	O
constrained	O
in	O
a	O
specific	O
domain	O
,	O
current	O
chatbots	O
are	O
still	O
far	O
from	O
satisfactory	O
.	O
Unlike	O
task	O
-	O
oriented	O
systems	O
that	O
can	O
be	O
relatively	O
well	O
-	O
resolved	O
with	O
handcrafted	O
templates	O
,	O
human	O
conversations	O
feature	O
a	O
complex	O
mixture	O
of	O
QA	O
,	O
chitchat	O
,	O
recommendation	O
,	O
etc	O
.	O
without	O
pre	O
-	O
specified	O
goals	O
or	O
conversational	O
patterns	O
(	O
Dodge	O
et	O
al	O
,	O
2016	O
;	O
Akasaki	O
and	O
Kaji	O
,	O
2017	O
;	O
.	O
Selecting	O
proper	O
domain	O
knowledge	O
to	O
support	O
response	B-TaskName
generation	I-TaskName
at	O
all	O
the	O
different	O
situations	O
is	O
challenging	O
(	O
Milward	O
and	O
Beveridge	O
,	O
2003	O
;	O
Shen	O
et	O
al	O
,	O
2019	O
)	O
.	O
In	O
this	O
work	O
,	O
we	O
direct	O
our	O
focus	O
to	O
the	O
movie	O
domain	O
and	O
present	O
a	O
large	O
-	O
scale	O
,	O
crowdsourced	O
Chinese	O
dataset	O
with	O
fine	O
-	O
grained	O
annotations	O
in	O
hope	O
of	O
boosting	O
the	O
study	O
towards	O
a	O
human	O
-	O
like	O
closed	O
-	O
domain	O
chatbot	B-TaskName
.	O
A	O
variety	O
of	O
dialogue	O
datasets	O
with	O
grounded	O
domain	O
knowledge	O
have	O
already	O
been	O
proposed	O
.	O
However	O
,	O
they	O
are	O
collected	O
either	O
through	O
(	O
1	O
)	O
online	O
forum	O
crawling	O
(	O
Dodge	O
et	O
al	O
,	O
2016	O
;	O
Ghazvininejad	O
et	O
al	O
,	O
2018	O
;	O
Liu	O
et	O
al	O
,	O
2018	O
;	O
Zhou	O
et	O
al	O
,	O
2018a	O
;	O
,	O
which	O
are	O
noisy	O
,	O
multi	O
-	O
party	O
,	O
mostly	O
contain	O
only	O
single	O
-	O
exchange	O
QA	O
,	O
or	O
(	O
2	O
)	O
crowdsourced	O
(	O
Zhu	O
et	O
al	O
,	O
2017	O
;	O
Zhou	O
et	O
al	O
,	O
2018b	O
;	O
Moon	O
et	O
al	O
,	O
2019	O
;	O
,	O
which	O
are	O
small	O
-	O
scale	O
and	O
often	O
created	O
in	O
an	O
overconstrained	O
setting	O
like	O
teacher	O
-	O
student	O
(	O
Moghe	O
et	O
al	O
,	O
2018	O
)	O
.	O
Even	O
for	O
datasets	O
crowd	O
-	O
sourced	O
in	O
unconstrained	O
scenarios	O
,	O
suggestive	O
domain	O
knowledge	O
is	O
provided	O
for	O
humans	O
before	O
an	O
utterance	O
is	O
provided	O
.	O
This	O
would	O
inevitably	O
prompt	O
humans	O
to	O
utilize	O
these	O
knowledge	O
deliberately	O
,	O
yielding	O
unnatural	O
conversations	O
simply	O
connecting	O
the	O
knowledge	O
(	O
Dinan	O
et	O
al	O
,	O
2019	O
;	O
.	O
We	O
show	O
examples	O
from	O
other	O
datasets	O
in	O
Appendix	O
Table	O
10	O
.	O
In	O
comparison	O
,	O
our	O
dataset	O
has	O
the	O
following	O
advantages	O
:	O
1	O
.	O
Natural	O
:	O
Crowdworkers	O
chat	O
in	O
a	O
free	O
environment	O
without	O
further	O
constraint	O
or	O
prompt	O
in	O
order	O
to	O
mimic	O
the	O
human	O
daily	O
conversations	O
to	O
the	O
largest	O
extent	O
.	O
2	O
.	O
Large	O
-	O
scale	O
:	O
It	O
covers	O
270k	O
human	O
dialogues	O
with	O
over	O
3	O
M	O
utterances	O
,	O
which	O
is	O
at	O
least	O
one	O
order	O
of	O
magnitude	O
larger	O
than	O
all	O
the	O
other	O
crowd	O
-	O
sourced	O
datasets	O
.	O
3	O
.	O
Annotated	O
:	O
Utterances	O
are	O
labeled	O
with	O
entity	O
information	O
and	O
dialogue	O
acts	O
classified	O
into	O
15	O
fine	O
-	O
grained	O
aspects	O
,	O
based	O
on	O
which	O
linked	O
into	O
different	O
types	O
of	O
knowledge	O
.	O
Different	O
from	O
previous	O
crowd	O
-	O
sourced	O
works	O
,	O
our	O
annotation	O
process	O
is	O
conducted	O
posteriori	O
so	O
that	O
it	O
will	O
not	O
interfere	O
with	O
human	O
conversations	O
,	O
e.g.	O
,	O
prompt	O
them	O
to	O
overuse	O
suggested	O
knowledge	O
.	O
Built	O
upon	O
our	O
dataset	O
,	O
we	O
propose	O
a	O
simple	O
unified	O
language	O
model	O
approach	O
to	O
push	O
the	O
limits	O
of	O
movie	O
-	O
domain	O
chatbots	O
.	O
The	O
model	O
is	O
first	O
pretrained	O
on	O
2.2B	O
words	O
collected	O
from	O
various	O
general	O
-	O
domain	O
conversational	O
resources	O
,	O
then	O
finetuned	O
on	O
the	O
movie	O
dataset	O
with	O
additional	O
knowledge	O
and	O
dialogue	O
acts	O
incorporated	O
.	O
We	O
pool	O
all	O
components	O
like	O
intent	O
prediction	O
and	O
knowledge	O
retrieval	O
into	O
a	O
sequence	O
prediction	O
task	O
and	O
solve	O
them	O
with	O
a	O
unified	O
language	O
model	O
architecture	O
.	O
It	O
avoids	O
designing	O
complex	O
systems	O
for	O
individual	O
components	O
separately	O
and	O
all	O
subtasks	O
can	O
be	O
easily	O
trained	O
simultaneously	O
(	O
Hosseini	O
-	O
Asl	O
et	O
al	O
,	O
2020	O
;	O
Peng	O
et	O
al	O
,	O
2020	O
)	O
.	O
We	O
show	O
our	O
simple	O
unified	O
approach	O
outperforms	O
strong	O
baselines	O
for	O
each	O
separate	O
subtask	O
.	O
Knowledge	O
retrieval	O
,	O
dialogue	O
acts	O
prediction	O
and	O
general	O
-	O
domain	O
pretrain	O
benefit	O
from	O
each	O
other	O
and	O
altogether	O
bring	O
improvement	O
to	O
the	O
generation	O
quality	O
.	O
In	O
the	O
online	O
interactive	O
test	O
,	O
our	O
best	O
model	O
succeeds	O
at	O
chatting	O
with	O
humans	O
for	O
11.4	O
turns	O
without	O
being	O
detected	O
to	O
be	O
a	O
machine	O
,	O
outperforming	O
even	O
commercial	O
chatbots	O
Mitsuku	O
2	O
and	O
Microsoft	O
XiaoIce	O
3	O
which	O
further	O
rely	O
on	O
complex	O
rules	O
.	O
By	O
analyzing	O
the	O
limitations	O
of	O
our	O
model	O
,	O
we	O
find	O
it	O
especially	O
has	O
difficulty	O
at	O
dealing	O
with	O
in	O
-	O
depth	O
discussions	O
over	O
long	O
turns	O
.	O
Future	O
research	O
can	O
consider	O
employing	O
larger	O
knowledge	O
base	O
or	O
explicit	O
state	O
tracking	O
.	O
In	O
summary	O
,	O
our	O
main	O
contributions	O
are	O
(	O
1	O
)	O
presenting	O
a	O
high	O
-	O
quality	O
,	O
large	O
-	O
scale	O
Chinese	O
conversational	O
corpus	O
with	O
fine	O
-	O
grained	O
annotations	O
in	O
the	O
movie	O
domain	O
to	O
benefit	O
future	O
study	O
,	O
(	O
2	O
)	O
showing	O
that	O
a	O
simple	O
unified	O
neural	O
model	O
trained	O
on	O
the	O
high	O
-	O
quality	O
dataset	O
can	O
approach	O
human	O
performance	O
and	O
even	O
outperform	O
commercial	O
systems	O
replying	O
on	O
complex	O
rules	O
,	O
and	O
(	O
3	O
)	O
studying	O
the	O
shortcomings	O
of	O
current	O
techniques	O
,	O
providing	O
suggestive	O
directions	O
for	O
future	O
research	O
.	O

Language	O
models	O
have	O
demonstrated	O
impressive	O
performance	O
as	O
a	O
universal	O
learner	O
across	O
NLP	O
tasks	O
(	O
Shen	O
et	O
al	O
,	O
2017	O
;	O
Peters	O
et	O
al	O
,	O
2018	O
;	O
Radford	O
et	O
al	O
,	O
2019	O
;	O
Brown	O
et	O
al	O
,	O
2020	O
)	O
.	O
Inspired	B-DatasetName
by	O
this	O
,	O
our	O
dialogue	B-TaskName
generation	I-TaskName
model	O
is	O
implemented	O
as	O
a	O
Transformer	B-MethodName
-	O
based	O
language	O
model	O
like	O
GPT2	O
(	O
Radford	O
et	O
al	O
,	O
2019	O
;	O
.	O
It	O
contains	O
a	O
pipeline	O
process	O
of	O
movie	O
tracker	O
,	O
intent	O
prediction	O
,	O
knowledge	O
retrieval	O
and	O
text	O
gener	O
-	O
7	O
We	O
only	O
consider	O
recommending	O
movies	O
as	O
for	O
the	O
DA	O
about	O
recommendation	O
.	O
Recommending	O
other	O
aspects	O
require	O
assembling	O
recommendation	B-TaskName
systems	I-TaskName
of	O
different	O
domains	O
,	O
which	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
paper	O
.	O

The	O
knowledge	O
retrieval	O
component	O
is	O
similar	O
to	O
the	O
classical	O
DSSM	O
model	O
(	O
Huang	O
et	O
al	O
,	O
2013	O
)	O
.	O
We	O
replace	O
the	O
MLP	B-DatasetName
with	O
our	O
language	O
model	O
encoder	O
to	O
get	O
the	O
embedding	O
for	O
knowledge	O
.	O
Note	O
that	O
we	O
only	O
select	O
knowledge	O
from	O
the	O
current	O
movie	O
,	O
which	O
can	O
be	O
obtained	O
from	O
the	O
movie	O
tracker	O
,	O
so	O
it	O
is	O
possible	O
to	O
"	O
will	O
be	O
fed	O
to	O
the	O
language	O
model	O
to	O
generate	O
the	O
response	O
.	O
To	O
make	O
it	O
consistent	O
with	O
the	O
pretrained	O
general	O
-	O
domain	O
dialogue	O
,	O
the	O
position	O
embedding	O
of	O
the	O
decoded	O
response	O
will	O
skip	O
the	O
concatenated	O
intent	O
and	O
knowledge	O
and	O
directly	O
follow	O
the	O
dialogue	O
context	O
.	O
We	O
find	O
this	O
beneficial	O
when	O
combined	O
with	O
pretrained	O
models	O
.	O
The	O
objective	O
also	O
follows	O
the	O
pretrained	O
model	O
mixing	O
maximum	O
lilkelihood	O
and	O
unlikelihood	O
training	O
.	O

We	O
present	O
MovieChats	O
:	O
a	O
movie	O
-	O
domain	O
chatbot	B-TaskName
built	O
upon	O
a	O
large	O
-	O
scale	O
,	O
high	O
-	O
quality	O
conversational	O
corpus	O
with	O
fine	O
-	O
grained	O
annotations	O
.	O
The	O
model	O
can	O
be	O
trained	O
end	O
-	O
to	O
-	O
end	O
with	O
a	O
simple	O
unified	O
language	O
model	O
architecture	O
.	O
We	O
show	O
that	O
our	O
model	O
,	O
powered	O
by	O
well	O
-	O
defined	O
knowledge	O
grounding	O
,	O
is	O
able	O
to	O
approach	O
human	O
performance	O
in	O
some	O
perspective	O
,	O
though	O
still	O
lagged	O
behind	O
when	O
it	O
comes	O
to	O
dealing	O
with	O
detailed	O
knowledge	O
or	O
long	O
-	O
turn	O
consistency	O
.	O

We	O
thank	O
anonymous	O
reviewers	O
and	O
the	O
dialogue	O
system	O
team	O
at	O
Wechat	B-DatasetName
AI	O
for	O
their	O
valuable	O
comments	O
.	O
Xiaoyu	O
Shen	O
was	O
funded	O
by	O
IMPRS	O
-	O
CS	B-DatasetName
fellowship	O
.	O
Ernie	O
Chang	O
is	O
supported	O
by	O
SFB	O
248	O
"	O
Foundations	O
of	O
Perspicuous	O
Software	O
Systems	O
"	O
(	O
E2	O
)	O
.	O

Suicide	O
is	O
amongst	O
the	O
most	O
pressing	O
public	O
health	O
issues	O
facing	O
today	O
's	O
society	O
,	O
stressing	O
the	O
need	O
for	O
rapid	O
and	O
effective	O
detection	O
tools	O
.	O
As	O
people	O
are	O
increasingly	O
self	O
-	O
expressing	O
their	O
distress	O
on	O
social	O
media	O
,	O
an	O
unprecedented	O
volume	O
of	O
data	O
is	O
currently	O
available	O
to	O
detect	O
a	O
person	O
's	O
suicide	O
risk	O
(	O
Roy	O
et	O
al	O
,	O
2020	O
;	O
Tadesse	O
et	O
al	O
,	O
2020	O
;	O
Luo	O
et	O
al	O
,	O
2020	O
)	O
.	O
In	O
this	O
shared	O
task	O
,	O
we	O
aim	O
to	O
construct	O
tools	O
to	O
identify	O
suicidal	O
Twitter	O
users	O
(	O
who	O
attempted	O
suicide	O
)	O
based	O
on	O
their	O
tweets	O
collected	O
from	O
spans	O
of	O
30	O
-	O
days	O
(	O
subtask	O
1	O
)	O
and	O
six	O
months	O
(	O
subtask	O
2	O
)	O
before	O
the	O
adverse	O
event	O
's	O
occurrence	O
date	O
(	O
Macavaney	O
et	O
al	O
,	O
2021	O
)	O
.	O
The	O
small	O
number	O
of	O
users	O
in	O
the	O
labeled	O
collections	O
of	O
subtask	O
1	O
(	O
57	O
suicidal/57	O
control	O
)	O
and	O
subtask	O
2	O
(	O
82	O
suicidal/82	O
control	O
)	O
and	O
the	O
scarcity	O
of	O
tweets	O
for	O
some	O
users	O
pose	O
these	O
tasks	O
as	O
small	O
-	O
dataset	O
classification	O
challenges	O
.	O
On	O
that	O
note	O
,	O
Coppersmith	O
et	O
al	O
(	O
2018	O
)	O
reported	O
high	O
performance	O
with	O
deep	O
learning	O
(	O
DL	O
)	O
methods	O
on	O
these	O
collections	O
after	O
enriching	O
them	O
with	O
additional	O
data	O
(	O
418	O
suicidal/418	O
control	O
)	O
.	O
When	O
formulating	O
the	O
strategy	O
to	O
attack	O
the	O
challenge	O
,	O
we	O
were	O
motivated	O
by	O
the	O
real	O
-	O
life	O
applicability	O
of	O
the	O
methods	O
.	O
Some	O
social	O
media	O
domains	O
already	O
started	O
implementing	O
auto	O
-	O
detection	O
tools	O
to	O
prevent	O
suicide	O
(	O
Ji	O
et	O
al	O
,	O
2020	O
)	O
.	O
These	O
tools	O
continuously	O
monitor	O
the	O
presence	O
of	O
suicide	O
risk	O
in	O
new	O
posts	O
.	O
Therefore	O
,	O
we	O
chose	O
to	O
train	O
the	O
models	O
at	O
the	O
tweet	O
level	O
.	O
Next	O
,	O
we	O
develop	O
a	O
majority	O
voting	O
scheme	O
over	O
the	O
classified	O
tweets	O
to	O
report	O
an	O
overall	O
suicide	O
risk	O
score	O
for	O
a	O
user	O
.	O
We	O
employ	O
simple	O
machine	O
learning	O
(	O
ML	O
)	O
methods	O
and	O
create	O
an	O
ensemble	O
.	O
We	O
also	O
experiment	O
with	O
DL	O
methods	O
to	O
assess	O
whether	O
complexity	O
would	O
improve	O
the	O
results	O
.	O
Since	O
successful	O
ML	O
applications	O
thrive	O
on	O
feature	B-TaskName
engineering	I-TaskName
(	O
Domingos	O
,	O
2012	O
)	O
,	O
we	O
conduct	O
feature	B-MethodName
selection	I-MethodName
to	O
evaluate	O
and	O
determine	O
the	O
best	O
feature	O
sets	O
for	O
the	O
models	O
.	O
Our	O
experiments	O
suggest	O
that	O
majority	O
voting	O
(	O
MV	O
)	O
over	O
tweet	O
-	O
level	O
classification	O
scores	O
is	O
a	O
viable	O
approach	O
for	O
the	O
short	O
-	O
term	O
prediction	O
of	O
suicide	O
risk	O
.	O
We	O
observe	O
that	O
DL	O
methods	O
require	O
plentiful	O
resources	O
despite	O
the	O
small	O
size	O
of	O
the	O
datasets	O
.	O
Simple	O
ML	O
methods	O
with	O
feature	B-MethodName
selection	I-MethodName
return	O
satisfactory	O
results	O
,	O
and	O
the	O
performance	O
further	O
improves	O
by	O
the	O
ensemble	O
classifier	O
.	O
We	O
also	O
observe	O
that	O
the	O
MV	O
approach	O
falls	O
short	O
on	O
the	O
six	O
-	O
month	O
-	O
long	O
data	O
regardless	O
of	O
the	O
applied	O
model	O
.	O
Yet	O
this	O
limitation	O
provides	O
the	O
invaluable	O
insight	O
that	O
suicidal	O
ideation	O
signals	O
are	O
more	O
significant	O
when	O
the	O
date	O
of	O
the	O
suicidal	O
event	O
is	O
closer	O
,	O
which	O
stresses	O
the	O
need	O
for	O
more	O
complex	O
,	O
noise	O
immune	O
models	O
for	O
longer	O
time	O
-	O
spanning	O
data	O
.	O
In	O
this	O
context	O
,	O
we	O
consider	O
a	O
noise	O
-	O
immune	O
model	O
as	O
a	O
suicidal	O
ideation	O
detection	O
model	O
that	O
is	O
not	O
affected	O
by	O
tweets	O
lacking	O
suicidal	O
ideation	O
.	O

Before	O
ML	O
experiments	O
,	O
we	O
initially	O
explore	O
a	O
simple	O
approach	O
that	O
constructs	O
graphs	O
from	O
training	O
sets	O
and	O
computes	O
how	O
well	O
the	O
given	O
texts	O
match	O
the	O
graphs	O
(	O
Bayram	O
et	O
al	O
,	O
2018	O
)	O
.	O
However	O
,	O
tweets	O
proved	O
to	O
be	O
unfit	O
for	O
the	O
method	O
due	O
to	O
low	O
word	O
counts	O
.	O
As	O
most	O
ML	O
methods	O
depend	O
on	O
learning	O
from	O
features	O
,	O
we	O
select	O
n	O
-	O
gram	O
features	O
where	O
n	O
≤	O
2	O
for	O
their	O
popularity	O
in	O
suicide	O
studies	O
(	O
O'Dea	O
et	O
al	O
,	O
2015	O
;	O
De	O
Choudhury	O
et	O
al	O
,	O
2016	O
;	O
Pestian	O
et	O
al	O
,	O
2020	O
)	O
.	O
For	O
bigrams	O
(	O
n	O
=	O
2	O
)	O
,	O
we	O
apply	O
a	O
sliding	O
window	O
over	O
concurrent	O
words	O
using	O
the	O
NLTK	O
library	O
(	O
Bird	O
et	O
al	O
,	O
2009	O
)	O
.	O
Next	O
,	O
we	O
eliminate	O
infrequent	O
n	O
-	O
grams	O
from	O
the	O
training	O
set	O
to	O
reduce	O
uninformative	O
features	O
(	O
occurring	O
in	O
≤3	O
tweets	O
in	O
30days	O
,	O
≤10	O
tweets	O
in	O
182	O
-	O
days	O
training	O
sets	O
)	O
.	O
Subsequently	O
,	O
we	O
scale	O
the	O
features	O
by	O
row	O
-	O
normalizing	O
them	O
with	O
the	O
root	O
of	O
the	O
sum	O
of	O
the	O
square	O
(	O
i.e.	O
variation	O
)	O
of	O
the	O
feature	O
values	O
.	O
Among	O
the	O
popular	O
ML	O
methods	O
in	O
suicide	O
literature	O
is	O
logistic	B-MethodName
regression	I-MethodName
(	O
LR	O
)	O
(	O
Walsh	O
et	O
al	O
,	O
2017	O
;	O
De	O
Choudhury	O
et	O
al	O
,	O
2016	O
;	O
O'Dea	O
et	O
al	O
,	O
2015	O
)	O
.	O
We	O
select	O
the	O
"	O
liblinear	O
"	O
solver	O
with	O
default	O
settings	O
for	O
being	O
recommended	O
for	O
small	O
datasets	O
(	O
Buitinck	O
et	O
al	O
,	O
2013	O
)	O
.	O
To	O
cover	O
diverse	O
mathematical	O
frameworks	O
and	O
assumptions	O
,	O
we	O
also	O
include	O
two	O
naive	O
Bayes	O
methods	O
(	O
Gaussian	O
(	O
GNB	O
)	O
and	O
Multinomial	O
(	O
MNB	O
)	O
with	O
default	O
settings	O
)	O
(	O
Buitinck	O
et	O
al	O
,	O
2013	O
)	O
.	O
We	O
also	O
experiment	O
with	O
K	B-MethodName
-	I-MethodName
Nearest	I-MethodName
Neighbors	I-MethodName
with	O
different	O
distance	O
(	O
uniform	O
,	O
weighted	O
)	O
and	O
neighborhood	O
(	O
k	O
{	O
3	O
,	O
5	O
,	O
8	O
}	O
)	O
settings	O
,	O
but	O
we	O
eliminate	O
it	O
for	O
low	O
within	O
-	O
dataset	O
results	O
.	O
Similarly	O
,	O
ensemblelearning	O
methods	O
(	O
Adaboost	O
,	O
XGBoost	O
,	O
Random	O
Forest	O
)	O
also	O
return	O
underwhelming	O
performance	O
despite	O
the	O
parameter	O
tuning	O
,	O
and	O
thus	O
,	O
were	O
eliminated	O
.	O
Additionally	O
,	O
we	O
evaluate	O
support	O
vector	O
machines	O
(	O
SVM	B-MethodName
)	O
for	O
their	O
popularity	O
in	O
suicide	O
research	O
(	O
Zhu	O
et	O
al	O
,	O
2020	O
;	O
Pestian	O
et	O
al	O
,	O
2020	O
;	O
O'Dea	O
et	O
al	O
,	O
2015	O
)	O
.	O
SVM	B-MethodName
with	O
rbf	O
kernel	O
proves	O
to	O
be	O
successful	O
but	O
requires	O
costly	O
parameter	O
tuning	O
,	O
while	O
linear	O
SVM	B-MethodName
(	O
lSVM	O
)	O
shows	O
success	O
on	O
withindataset	O
evaluations	O
with	O
less	O
cost	O
.	O
Consequently	O
,	O
we	O
select	O
lSVM	O
of	O
sklearn	O
(	O
default	O
settings	O
)	O
for	O
the	O
shared	O
task	O
(	O
Buitinck	O
et	O
al	O
,	O
2013	O
)	O
,	O
which	O
returns	O
only	O
binary	O
classification	O
results	O
.	O
To	O
convert	O
them	O
to	O
probabilities	O
,	O
we	O
apply	O
probability	O
calibration	O
with	O
logistic	B-MethodName
regression	I-MethodName
(	O
CalibratedClassifierCV	O
)	O
.	O
Feature	B-MethodName
selection	I-MethodName
:	O
Following	O
the	O
ML	O
method	O
selections	O
,	O
we	O
evaluate	O
the	O
effect	O
of	O
feature	B-MethodName
selection	I-MethodName
on	O
ML	O
performance	O
.	O
To	O
compute	O
feature	B-TaskName
importance	I-TaskName
scores	O
,	O
we	O
also	O
use	O
the	O
LR	O
.	O
For	O
each	O
selected	O
number	O
of	O
features	O
,	O
we	O
gather	O
top	O
suicidal	O
and	O
control	O
features	O
.	O
Next	O
,	O
we	O
train	O
and	O
evaluate	O
the	O
ML	O
methods	O
in	O
a	O
leave	O
-	O
one	O
-	O
out	O
(	O
LOO	O
)	O
framework	O
using	O
those	O
features	O
.	O
The	O
feature	B-MethodName
selection	I-MethodName
results	O
of	O
the	O
selected	O
ML	O
methods	O
for	O
two	O
subtasks	O
are	O
in	O
Figure	O
2	O
.	O
We	O
select	O
the	O
best	O
ML	O
models	O
from	O
these	O
plots	O
.	O
Experiments	O
with	O
Ensemble	O
:	O
Ensemble	O
classifiers	O
previously	O
showed	O
success	O
in	O
ML	O
challenges	O
(	O
Niculescu	O
-	O
Mizil	O
et	O
al	O
,	O
2009	O
)	O
.	O
Since	O
every	O
classifier	O
renders	O
predicted	O
probabilities	O
for	O
every	O
data	O
point	O
,	O
we	O
build	O
an	O
ensemble	O
classifier	O
to	O
optimize	O
the	O
results	O
of	O
four	O
selected	O
ML	O
methods	O
(	O
LR	O
,	O
GNB	O
,	O
MNB	O
,	O
lSVM	O
)	O
.	O
We	O
adopt	O
a	O
weighting	O
ensemble	O
method	O
where	O
the	O
weight	O
of	O
each	O
classifier	O
is	O
set	O
proportional	O
to	O
its	O
performance	O
(	O
Rokach	O
,	O
2010	O
)	O
.	O
We	O
call	O
this	O
method	O
weighted	O
Ensemble	O
(	O
wEns	O
)	O
.	O

To	O
measure	O
whether	O
re	O
-	O
sults	O
would	O
improve	O
with	O
complexity	O
,	O
we	O
also	O
evaluate	O
shallow	O
DL	O
methods	O
.	O
We	O
use	O
the	O
pre	O
-	O
trained	O
transformer	O
model	O
Bert	O
-	O
base	O
-	O
uncased	O
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
to	O
catch	O
the	O
linguistics	O
features	O
of	O
the	O
tweets	O
.	O
The	O
embeddings	O
are	O
then	O
fed	O
to	O
a	O
DL	O
Recurrent	O
Units	O
-	O
based	O
architecture	O
to	O
learn	O
text	O
sequence	O
orders	O
.	O
We	O
experiment	O
with	O
two	O
types	O
of	O
recurrent	O
neural	O
networks	O
(	O
RNNs	O
)	O
:	O
Long	O
Short	O
Term	O
Memory	O
(	O
LSTM	B-MethodName
)	O
(	O
Gers	O
et	O
al	O
,	O
1999	O
)	O
,	O
and	O
Gated	B-MethodName
Recurrent	I-MethodName
Unit	I-MethodName
(	O
GRU	B-MethodName
)	O
known	O
for	O
overcoming	O
vanishing	O
and	O
exploding	O
gradient	O
problems	O
faced	O
by	O
vanilla	O
RNNs	O
during	O
training	O
(	O
Cho	O
et	O
al	O
,	O
2014	O
)	O
.	O
After	O
assessing	O
various	O
configurations	O
of	O
both	O
architectures	O
,	O
we	O
settle	O
on	O
a	O
multi	O
-	O
layer	O
bi	O
-	O
directional	O
GRU	B-MethodName
with	O
the	O
following	O
characteristics	O
:	O
embedding	O
dimen	O
-	O
sion=256	O
,	O
number	O
of	O
layers=2	O
,	O
batch	O
size=32	O
.	O
We	O
call	O
this	O
model	O
GRU	B-MethodName
-	O
Bert	O
.	O
We	O
include	O
a	O
drop	O
-	O
out	O
to	O
regularise	O
learning	O
and	O
a	O
fully	O
connected	O
layer	O
with	O
a	O
Sigmoid	B-MethodName
activation	I-MethodName
to	O
produce	O
the	O
classification	O
for	O
each	O
tweet	O
.	O
Finally	O
,	O
we	O
include	O
the	O
same	O
majority	O
voting	O
framework	O
to	O
infer	O
the	O
classification	O
on	O
the	O
user	O
level	O
.	O
We	O
use	O
Pytorch	O
(	O
Paszke	O
et	O
al	O
,	O
2019	O
)	O
and	O
scikit	O
-	O
learn	O
(	O
Buitinck	O
et	O
al	O
,	O
2013	O
)	O
libraries	O
for	O
implementation	O
.	O

In	O
subtask	O
1	O
,	O
the	O
test	O
set	O
results	O
show	O
that	O
feature	B-MethodName
selection	I-MethodName
can	O
considerably	O
enhance	O
the	O
performance	O
of	O
ML	O
models	O
compared	O
to	O
the	O
baseline	O
.	O
We	O
also	O
find	O
that	O
the	O
ensemble	O
classifier	O
is	O
comparably	O
better	O
than	O
the	O
baseline	O
in	O
this	O
subtask	O
.	O
Meanwhile	O
,	O
though	O
the	O
baseline	O
of	O
CLPsych2021	O
is	O
the	O
same	O
as	O
our	O
LR	O
,	O
our	O
additional	O
MV	O
and	O
feature	B-MethodName
selection	I-MethodName
together	O
enable	O
LR	O
to	O
substantially	O
outperform	O
the	O
baseline	O
.	O
These	O
successes	O
of	O
simple	O
ML	O
methods	O
indicate	O
that	O
a	O
collection	O
of	O
tweets	O
from	O
within	O
the	O
30	O
-	O
days	O
of	O
a	O
suicidal	O
event	O
is	O
good	O
enough	O
to	O
capture	O
the	O
existence	O
of	O
suicidal	O
ideation	O
,	O
which	O
is	O
an	O
important	O
finding	O
for	O
future	O
real	O
-	O
life	O
suicide	O
prevention	O
applications	O
.	O
In	O
contrast	O
to	O
the	O
observations	O
from	O
subtask	O
1	O
,	O
our	O
test	O
results	O
on	O
subtask	O
2	O
are	O
unsatisfactory	O
.	O
Yet	O
,	O
they	O
provide	O
the	O
valuable	O
insight	O
that	O
suicidal	O
signals	O
are	O
more	O
significant	O
in	O
the	O
short	O
-	O
term	O
,	O
and	O
older	O
tweets	O
lacking	O
suicidal	O
ideation	O
generate	O
noise	O
.	O
This	O
insight	O
suggests	O
the	O
need	O
to	O
account	O
for	O
a	O
time	O
-	O
domain	O
aspect	O
.	O
To	O
investigate	O
the	O
viability	O
of	O
this	O
claim	O
,	O
we	O
experiment	O
with	O
a	O
simple	O
time	O
-	O
decay	O
coefficient	O
in	O
the	O
MV	O
framework	O
and	O
evaluate	O
it	O
through	O
LR	O
on	O
the	O
test	O
set	O
.	O
We	O
multiply	O
each	O
vote	O
by	O
the	O
coefficient	O
2	O
−timeDif	O
f	O
half	O
Lif	O
e	O
where	O
timeDif	O
f	O
is	O
the	O
number	O
of	O
days	O
between	O
the	O
current	O
and	O
last	O
tweets	O
,	O
and	O
half	O
Lif	O
e	O
(	O
=	O
7	O
days	O
)	O
is	O
a	O
hyperparameter	O
that	O
reflects	O
the	O
weight	O
of	O
a	O
vote	O
in	O
the	O
final	O
suicide	O
risk	O
score	O
of	O
a	O
user	O
.	O
Initial	O
experiments	O
show	O
that	O
even	O
this	O
simple	O
time	O
-	O
decay	O
coefficient	O
improves	O
the	O
test	O
results	O
significantly	O
.	O
This	O
observation	O
suggests	O
that	O
tweet	O
dates	O
are	O
critical	O
features	O
for	O
this	O
subtask	O
and	O
should	O
be	O
included	O
in	O
future	O
work	O
.	O
Notwithstanding	O
,	O
on	O
both	O
subtasks	O
,	O
the	O
shallow	O
DL	O
methods	O
we	O
experimented	O
with	O
perform	O
poorly	O
.	O
These	O
results	O
could	O
be	O
attributed	O
to	O
overfitting	O
on	O
the	O
small	O
dataset	O
and	O
noise	O
sensitivity	O
for	O
the	O
larger	O
time	O
-	O
spanning	O
dataset	O
.	O
Additionally	O
,	O
regardless	O
of	O
the	O
dataset	O
size	O
,	O
these	O
methods	O
proved	O
to	O
be	O
computationally	O
expensive	O
.	O
As	O
within	O
-	O
dataset	O
experiments	O
using	O
simple	O
ML	O
methods	O
outperformed	O
these	O
expensive	O
shallow	O
DL	O
methods	O
,	O
we	O
excluded	O
the	O
latter	O
from	O
the	O
test	O
set	O
evaluation	O
.	O
Future	O
work	O
on	O
DL	O
will	O
include	O
deeper	O
,	O
more	O
complex	O
,	O
and	O
noise	O
immune	O
methods	O
that	O
could	O
integrate	O
Convolutional	O
neural	O
networks	O
(	O
CNN	O
)	O
,	O
deeper	O
LSTM	B-MethodName
or	O
GRU	B-MethodName
layers	O
,	O
and	O
experiments	O
with	O
various	O
word	O
embedding	O
models	O
.	O
If	O
we	O
compare	O
our	O
findings	O
with	O
those	O
in	O
Coppersmith	O
et	O
al	O
(	O
2018	O
)	O
,	O
we	O
observe	O
different	O
results	O
in	O
terms	O
of	O
short	O
-	O
term	O
versus	O
long	O
-	O
term	O
dataset	O
classifications	O
.	O
We	O
attribute	O
these	O
different	O
outcomes	O
to	O
the	O
fact	O
that	O
the	O
original	O
study	O
optimizes	O
the	O
design	O
for	O
detecting	O
trait	O
-	O
level	O
(	O
relevant	O
to	O
risk	O
for	O
any	O
point	O
in	O
time	O
)	O
suicide	O
risk	O
when	O
we	O
endeavor	O
to	O
identify	O
suicidal	O
ideation	O
at	O
the	O
state	O
level	O
(	O
immediate	O
risk	O
presence	O
)	O
.	O
This	O
design	O
choice	O
,	O
along	O
with	O
tweet	O
-	O
level	O
classification	O
,	O
enabled	O
our	O
model	O
to	O
recognize	O
suicidal	O
nuances	O
in	O
short	O
-	O
term	O
tweets	O
.	O
Meanwhile	O
,	O
we	O
were	O
unable	O
to	O
detect	O
any	O
suicidal	O
ideation	O
through	O
manual	O
inspection	O
(	O
reading	O
and	O
interpreting	O
the	O
tweets	O
)	O
over	O
most	O
of	O
these	O
tweets	O
due	O
to	O
their	O
noisy	O
and	O
ambiguous	O
nature	O
.	O

In	O
this	O
shared	O
task	O
,	O
we	O
investigate	O
various	O
models	O
for	O
identifying	O
suicide	O
risk	O
based	O
on	O
user	O
's	O
tweets	O
.	O
Inspired	B-DatasetName
by	O
real	O
-	O
life	O
applications	O
,	O
we	O
focus	O
on	O
assessing	O
suicide	O
risk	O
on	O
the	O
tweet	O
level	O
.	O
Experimental	O
results	O
reveal	O
that	O
the	O
ensemble	O
classifier	O
can	O
identify	O
suicidal	O
users	O
from	O
30	O
-	O
days	O
tweets	O
with	O
a	O
high	O
performance	O
rate	O
,	O
demonstrating	O
the	O
power	O
of	O
majority	O
voting	O
over	O
tweet	O
-	O
level	O
classifications	O
for	O
short	O
-	O
term	O
suicide	O
risk	O
detection	O
.	O
Meanwhile	O
,	O
we	O
construe	O
from	O
the	O
underwhelming	O
results	O
on	O
the	O
six	O
-	O
month	O
dataset	O
that	O
these	O
models	O
were	O
more	O
sensitive	O
to	O
the	O
signals	O
relevant	O
to	O
short	O
term	O
risk	O
than	O
those	O
relevant	O
to	O
long	O
term	O
risk	O
.	O
In	O
future	O
work	O
,	O
we	O
will	O
incorporate	O
a	O
temporal	O
aspect	O
to	O
improve	O
the	O
noise	O
immunity	O
of	O
our	O
models	O
,	O
and	O
we	O
will	O
continue	O
experimenting	O
with	O
more	O
complex	O
models	O
.	O

Preface	O
:	O
General	B-DatasetName
Chair	O

In	O
addition	O
,	O
oral	O
presentations	O
were	O
shortened	O
to	O
fourteen	O
(	O
twelve	O
)	O
minutes	O
for	O
long	O
(	O
short	O
)	O
papers	O
,	O
plus	O
time	O
for	O
questions	O
.	O
While	O
this	O
places	O
a	O
greater	O
demand	O
on	O
speakers	O
to	O
be	O
concise	O
,	O
we	O
believe	O
it	O
is	O
worth	O
the	O
effort	O
,	O
allowing	O
far	O
more	O
work	O
to	O
be	O
presented	O
orally	O
.	O
We	O
also	O
took	O
advantage	O
of	O
the	O
many	O
halls	O
available	O
and	O
expanded	O
the	O
number	O
of	O
parallel	O
talks	O
to	O
five	O
during	O
most	O
of	O
the	O
conference	O
sessions	O
.	O
In	O
keeping	O
with	O
changes	O
introduced	O
in	O
the	O
ACL	O
community	O
from	O
last	O
year	O
,	O
we	O
continued	O
the	O
practice	O
of	O
recognizing	O
outstanding	O
papers	O
at	O
ACL	O
.	O
The	O
22	O
outstanding	O
papers	O
(	O
15	O
long	O
,	O
7	O
short	O
,	O
1.6	O
%	O
of	O
submissions	O
)	O
represent	O
a	O
broad	O
spectrum	O
of	O
exciting	O
contributions	O
and	O
have	O
been	O
specially	O
placed	O
on	O
the	O
final	O
day	O
of	O
the	O
main	O
conference	O
where	O
the	O
program	O
is	O
focused	O
into	O
two	O
parallel	O
sessions	O
of	O
these	O
outstanding	O
contributions	O
.	O
From	O
these	O
,	O
a	O
best	O
paper	O
and	O
a	O
best	O
short	O
paper	O
those	O
will	O
be	O
announced	O
in	O
the	O
awards	O
session	O
on	O
Wednesday	O
afternoon	O
.	O
Chris	O
has	O
already	O
mentioned	O
our	O
introduction	O
of	O
the	O
chairs	O
'	O
blog	O
2	O
,	O
where	O
we	O
strove	O
to	O
make	O
the	O
selection	O
process	O
of	O
the	O
internal	O
workings	O
of	O
the	O
scientific	O
committee	O
more	O
transparent	O
.	O
We	O
have	O
publicly	O
documented	O
our	O
calls	O
for	O
area	O
chairs	O
,	O
reviewers	O
and	O
accepted	O
papers	O
selection	O
process	O
.	O
Via	O
the	O
blog	O
,	O
we	O
communicated	O
several	O
innovations	O
in	O
the	O
conference	O
organization	O
workflow	O
,	O
of	O
which	O
we	O
would	O
call	O
attention	O
to	O
two	O
key	O
ones	O
here	O
.	O
In	O
the	O
review	O
process	O
,	O
we	O
pioneered	O
the	O
use	O
of	O
the	O
Toronto	O
Paper	O
Matching	O
System	O
,	O
a	O
topic	O
model	O
based	O
approach	O
to	O
the	O
assignment	O
of	O
reviewers	O
to	O
papers	O
.	O
We	O
hope	O
this	O
decision	O
will	O
spur	O
other	O
program	O
chairs	O
to	O
adopt	O
the	O
system	O
,	O
as	O
increased	O
coverage	O
will	O
better	O
the	O
reviewer	O
/	O
submission	O
matching	O
process	O
,	O
ultimately	O
leading	O
to	O
a	O
higher	O
quality	O
program	O
.	O
For	O
posterity	O
,	O
we	O
also	O
introduced	O
the	O
usage	O
of	O
hyperlinks	O
in	O
the	O
bibliography	O
reference	O
sections	O
of	O
papers	O
,	O
1	O
These	O
numbers	O
exclude	O
papers	O
that	O
were	O
not	O
reviewed	O
due	O
to	O
formatting	O
,	O
anonymity	O
,	O
or	O
double	O
submission	O
violations	O
or	O
that	O
were	O
withdrawn	O
prior	O
to	O
review	O
,	O
which	O
was	O
unfortunately	O
a	O
substantial	O
number	O
.	O
2	O
https://chairs	O
-	O
blog.acl2017.org/	O
viii	O
and	O
have	O
worked	O
with	O
the	O
ACL	O
Anthology	O
to	O
ensure	O
that	O
digital	O
object	O
identifiers	O
(	O
DOIs	O
)	O
appear	O
in	O
the	O
footer	O
of	O
each	O
paper	O
.	O
These	O
steps	O
will	O
help	O
broaden	O
the	O
long	O
-	O
term	O
impact	O
of	O
the	O
work	O
that	O
our	O
community	O
has	O
on	O
the	O
scientific	O
world	O
at	O
large	O
.	O
There	O
are	O
many	O
individuals	O
we	O
wish	O
to	O
thank	O
for	O
their	O
contributions	O
to	O
ACL	O
2017	O
,	O
some	O
multiple	O
times	O
:	O
The	O
61	O
area	O
chairs	O
who	O
volunteered	O
for	O
our	O
extra	O
duty	O
.	O
They	O
recruited	O
reviewers	O
,	O
led	O
discussions	O
on	O
each	O
paper	O
,	O
replied	O
to	O
authors	O
'	O
direct	O
comments	O
to	O
them	O
and	O
carefully	O
assessed	O
each	O
submission	O
.	O
Their	O
input	O
was	O
instrumental	O
in	O
guiding	O
the	O
final	O
decisions	O
on	O
papers	O
and	O
selecting	O
the	O
outstanding	O
papers	O
.	O
Our	O
full	O
program	O
committee	O
of	O
BUG	B-DatasetName
hard	O
-	O
working	O
individuals	O
who	O
reviewed	O
the	O
conference	O
's	O
1	O
,	O
318	O
submissions	O
(	O
including	O
secondary	O
reviewers	O
)	O
.	O
TACL	O
editors	O
-	O
in	O
-	O
chief	O
Mark	O
Johnson	O
,	O
Lillian	O
Lee	O
,	O
and	O
Kristina	O
Toutanova	O
,	O
for	O
coordinating	O
with	O
us	O
on	O
TACL	O
presentations	O
at	O
ACL	O
.	O
Noah	O
Smith	O
and	O
Katrin	O
Erk	O
,	O
program	O
co	O
-	O
chairs	O
of	O
ACL	O
2016	O
and	O
Ani	O
Nenkova	O
and	O
Owen	O
Rambow	O
,	O
program	O
co	O
-	O
chairs	O
of	O
NAACL	O
2016	O
,	O
who	O
we	O
consulted	O
several	O
times	O
on	O
short	O
order	O
for	O
help	O
and	O
advice	O
.	O
Wei	O
Lu	O
and	O
Sameer	O
Singh	O
,	O
our	O
well	O
-	O
organized	O
publication	O
chairs	O
,	O
with	O
direction	O
and	O
oversight	O
from	O
publication	O
chair	O
mentor	O
Meg	O
Mitchell	O
.	O
Also	O
,	O
Christian	O
Federmann	O
who	O
helped	O
with	O
the	O
local	O
handbook	O
.	O
The	O
responsive	O
team	O
at	O
Softconf	O
led	O
by	O
Rich	O
Gerber	O
,	O
who	O
worked	O
quickly	O
to	O
resolve	O
problems	O
and	O
who	O
strove	O
to	O
integrate	O
the	O
use	O
of	O
the	O
Toronto	O
Paper	O
Matching	O
System	O
(	O
TPMS	O
)	O
for	O
our	O
use	O
.	O
Priscilla	O
Rasmussen	O
and	O
Anoop	O
Sarkar	O
and	O
the	O
local	O
organization	O
team	O
,	O
especially	O
webmaster	O
Nitin	O
Madnani	O
.	O
Christopher	O
Calliston	O
-	O
Burch	O
,	O
our	O
general	O
chair	O
,	O
who	O
kept	O
us	O
coordinated	O
with	O
the	O
rest	O
of	O
the	O
ACL	O
2017	O
team	O
and	O
helped	O
us	O
free	O
our	O
time	O
to	O
concentrate	O
on	O
the	O
key	O
duty	O
of	O
organizing	O
the	O
scientific	O
program	O
.	O
Key	O
-	O
Sun	O
Choi	O
,	O
Jing	O
Jiang	O
,	O
Graham	O
Neubig	O
,	O
Emily	O
Pitler	O
,	O
and	O
Bonnie	O
Webber	O
who	O
carefully	O
reviewed	O
papers	O
under	O
consideration	O
for	O
best	O
paper	O
recognition	O
.	O
Our	O
senior	O
correspondents	O
for	O
the	O
blog	O
,	O
who	O
contributed	O
guest	O
posts	O
and	O
advice	O
for	O
writing	O
and	O
reviewing	O
:	O
Waleed	O
Ammar	O
,	O
Yoav	O
Artzi	O
,	O
Tim	O
Baldwin	O
,	O
Marco	O
Baroni	O
,	O
Claire	O
Cardie	O
,	O
Xavier	O
Carreras	O
,	O
Hal	O
Daumé	O
,	O
Kevin	O
Duh	O
,	O
Chris	O
Dyer	O
,	O
Marti	O
Hearst	O
,	O
Mirella	O
Lapata	O
,	O
Emily	O
M.	O
Bender	O
,	O
Aurélien	O
Max	O
,	O
Kathy	O
McKeown	O
,	O
Ray	O
Mooney	O
,	O
Ani	O
Nenkova	O
,	O
Joakim	O
Nivre	O
,	O
Philip	O
Resnik	O
,	O
and	O
Joel	O
Tetreault	O
.	O
Without	O
them	O
,	O
the	O
participation	O
of	O
the	O
community	O
through	O
the	O
productive	O
comments	O
,	O
and	O
without	O
you	O
the	O
readership	O
,	O
our	O
blog	O
for	O
disseminating	O
information	O
about	O
the	O
decision	O
processes	O
would	O
not	O
have	O
been	O
possible	O
and	O
a	O
success	O
.	O
With	O
twin	O
upward	O
trends	O
in	O
the	O
interest	O
in	O
computational	O
linguistics	O
and	O
natural	O
language	O
processing	O
and	O
the	O
size	O
of	O
our	O
annual	O
meeting	O
,	O
ACL	O
has	O
begun	O
the	O
practice	O
of	O
recognizing	O
outstanding	O
papers	O
that	O
represent	O
a	O
select	O
cross	O
-	O
section	O
of	O
the	O
entire	O
field	O
,	O
as	O
nominated	O
by	O
reviewers	O
and	O
vetted	O
by	O
the	O
area	O
chairs	O
and	O
program	O
co	O
-	O
chairs	O
.	O
These	O
papers	O
have	O
been	O
centrally	O
located	O
in	O
the	O
program	O
,	O
on	O
the	O
last	O
day	O
of	O
our	O
meeting	O
,	O
in	O
a	O
more	O
focused	O
two	O
parallel	O
tracks	O
format	O
.	O
This	O
year	O
,	O
we	O
have	O
nominated	O
15	O
long	O
papers	O
and	O
7	O
short	O
papers	O
,	O
representing	O
1.8	O
%	O
of	O
all	O
submissions	O
and	O
approximately	O
5	O
%	O
of	O
the	O
accepted	O
ACL	O
program	O
.	O
Congratulations	O
,	O
authors	O
!	O
(	O
in	O
alphabetical	O
order	O
by	O
first	O
author	O
surname	O
)	O
Long	O
Papers	O
Jan	O
Buys	O
and	O
Phil	O
Blunsom	O
.	O
Robust	O
Incremental	O
Neural	O
Semantic	O
Graph	O
Parsing	O
.	O
Xinchi	O
Chen	O
,	O
Zhan	O
Shi	O
,	O
Xipeng	O
Qiu	O
and	O
Xuanjing	O
Huang	O
.	O
Adversarial	O
Multi	O
-	O
Criteria	O
Learning	O
for	O
Chinese	B-TaskName
Word	I-TaskName
Segmentation	I-TaskName
.	O

The	O
computational	O
linguistics	O
and	O
natural	O
language	O
processing	O
community	O
is	O
experiencing	O
an	O
episode	O
of	O
deep	O
fascination	O
with	O
representation	B-TaskName
learning	I-TaskName
.	O
Like	O
many	O
other	O
presenters	O
at	O
this	O
conference	O
,	O
I	O
will	O
describe	O
new	O
ways	O
to	O
use	O
representation	B-TaskName
learning	I-TaskName
in	O
models	O
of	O
natural	O
language	O
.	O
Noting	O
that	O
a	O
data	O
-	O
driven	O
model	O
always	O
assumes	O
a	O
theory	O
(	O
not	O
necessarily	O
a	O
good	O
one	O
)	O
,	O
I	O
will	O
argue	O
for	O
the	O
benefits	O
of	O
language	O
-	O
appropriate	O
inductive	O
bias	O
for	O
representation	O
-	O
learning	O
-	O
infused	O
models	O
of	O
language	O
.	O
Such	O
bias	O
often	O
comes	O
in	O
the	O
form	O
of	O
assumptions	O
baked	O
into	O
a	O
model	O
,	O
constraints	O
on	O
an	O
inference	O
algorithm	O
,	O
or	O
linguistic	O
analysis	O
applied	O
to	O
data	O
.	O
Indeed	O
,	O
many	O
decades	O
of	O
research	O
in	O
linguistics	O
(	O
including	O
computational	O
linguistics	O
)	O
put	O
our	O
community	O
in	O
a	O
strong	O
position	O
to	O
identify	O
promising	O
inductive	O
biases	O
.	O
The	O
new	O
models	O
,	O
in	O
turn	O
,	O
may	O
allow	O
us	O
to	O
explore	O
previously	O
unavailable	O
forms	O
of	O
bias	O
,	O
and	O
to	O
produce	O
findings	O
of	O
interest	O
to	O
linguistics	O
.	O
I	O
will	O
focus	O
on	O
new	O
models	O
of	O
documents	O
and	O
of	O
sentential	O
semantic	O
structures	O
,	O
and	O
I	O
will	O
emphasize	O
abstract	O
,	O
reusable	O
components	O
and	O
their	O
assumptions	O
rather	O
than	O
applications	O
.	O
In	O
this	O
talk	O
I	O
will	O
argue	O
that	O
in	O
order	O
to	O
render	O
electronic	O
data	O
more	O
accessible	O
to	O
individuals	O
and	O
computers	O
alike	O
,	O
new	O
types	O
of	O
translation	O
models	O
need	O
to	O
be	O
developed	O
.	O
I	O
will	O
focus	O
on	O
three	O
examples	O
,	O
text	B-TaskName
simplification	I-TaskName
,	O
source	O
code	B-TaskName
generation	I-TaskName
,	O
and	O
movie	O
summarization	B-TaskName
.	O
I	O
will	O
illustrate	O
how	O
recent	O
advances	O
in	O
deep	O
learning	O
can	O
be	O
extended	O
in	O
order	O
to	O
induce	O
general	O
representations	O
for	O
different	O
modalities	O
and	O
learn	O
how	O
to	O
translate	O
between	O
these	O
and	O
natural	O
language	O
.	O

Mirella	O
Lapata	O
is	O
professor	O
of	O
natural	O
language	O
processing	O
in	O
the	O
School	O
of	O
Informatics	O
at	O
the	O
University	O
of	O
Edinburgh	O
.	O
Her	O
research	O
focuses	O
on	O
getting	O
computers	O
to	O
understand	O
,	O
reason	O
with	O
,	O
and	O
generate	O
.	O
She	O
is	O
as	O
an	O
associate	O
editor	O
of	O
the	O
Journal	O
of	O
Artificial	O
Intelligence	O
Research	O
and	O
has	O
served	O
on	O
the	O
editorial	O
boards	O
of	O
Transactions	O
of	O
the	O
ACL	O
and	O
Computational	O
Linguistics	O
.	O
She	O
was	O
the	O
first	O
recipient	O
of	O
the	O
Karen	O
Sparck	O
Jones	O
award	O
of	O
the	O
British	O
Computer	O
Society	O
,	O
recognizing	O
key	O
contributions	O
to	O
NLP	O
and	O
information	B-TaskName
retrieval	I-TaskName
.	O
She	O
received	O
two	O
EMNLP	O
best	O
paper	O
awards	O
and	O
currently	O
holds	O
a	O
prestigious	O
Consolidator	O
Grant	O
from	O
the	O
European	O
Research	O
Council	O
.	O
xxiii	O

Pretrained	B-TaskName
language	I-TaskName
models	I-TaskName
have	O
served	O
as	O
the	O
backbone	O
for	O
many	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
NLP	O
results	O
.	O
These	O
models	O
are	O
large	O
and	O
expensive	O
to	O
train	O
.	O
Recent	O
work	O
suggests	O
that	O
continued	O
pretraining	O
on	O
task	O
-	O
specific	O
data	O
is	O
worth	O
the	O
effort	O
as	O
pretraining	O
leads	O
to	O
improved	O
performance	O
on	O
downstream	O
tasks	O
.	O
We	O
explore	O
alternatives	O
to	O
full	O
-	O
scale	O
task	O
-	O
specific	O
pretraining	O
of	O
language	O
models	O
through	O
the	O
use	O
of	O
adapter	O
modules	O
,	O
a	O
parameter	O
-	O
efficient	O
approach	O
to	O
transfer	B-TaskName
learning	I-TaskName
.	O
We	O
find	O
that	O
adapter	O
-	O
based	O
pretraining	O
is	O
able	O
to	O
achieve	O
comparable	O
results	O
to	O
task	O
-	O
specific	O
pretraining	O
while	O
using	O
a	O
fraction	O
of	O
the	O
overall	O
trainable	O
parameters	O
.	O
We	O
further	O
explore	O
direct	O
use	O
of	O
adapters	O
without	O
pretraining	O
and	O
find	O
that	O
the	O
direct	O
finetuning	O
performs	O
mostly	O
on	O
par	O
with	O
pretrained	O
adapter	O
models	O
,	O
contradicting	O
previously	O
proposed	O
benefits	O
of	O
continual	B-TaskName
pretraining	I-TaskName
in	O
full	O
pretraining	O
fine	O
-	O
tuning	O
strategies	O
.	O
Lastly	O
,	O
we	O
perform	O
an	O
ablation	O
study	O
on	O
task	O
-	O
adaptive	O
pretraining	O
to	O
investigate	O
how	O
different	O
hyperparameter	O
settings	O
can	O
change	O
the	O
effectiveness	O
of	O
the	O
pretraining	O
.	O

Pre	O
-	O
trained	O
language	O
model	O
We	O
use	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
,	O
a	O
Transformer	B-MethodName
-	O
based	O
language	O
model	O
that	O
is	O
pre	O
-	O
trained	O
on	O
a	O
massive	O
text	O
corpus	O
,	O
following	O
Gururangan	O
et	O
al	O
,	O
2020	O
.	O
RoBERTa	B-MethodName
is	O
an	O
extension	O
of	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
with	O
optimized	O
hyperparameters	O
and	O
a	O
modification	O
of	O
the	O
pretraining	O
objective	O
,	O
which	O
excludes	O
next	O
sentence	O
prediction	O
and	O
only	O
uses	O
the	O
randomly	O
masked	O
tokens	O
in	O
the	O
input	O
sentence	O
.	O
To	O
evaluate	O
the	O
performance	O
of	O
RoBERTa	B-MethodName
on	O
a	O
certain	O
task	O
,	O
a	O
classification	O
layer	O
is	O
appended	O
on	O
top	O
of	O
the	O
language	O
model	O
after	O
the	O
pretraining	O
and	O
all	O
the	O
parameters	O
in	O
RoBERTa	B-MethodName
are	O
trained	O
in	O
a	O
supervised	O
way	O
using	O
the	O
label	O
of	O
the	O
dataset	O
.	O
In	O
this	O
paper	O
,	O
training	O
word	O
representations	O
using	O
RoBERTa	B-MethodName
on	O
a	O
masked	B-TaskName
language	I-TaskName
modeling	I-TaskName
task	O
will	O
be	O
referred	O
to	O
as	O
pretraining	O
.	O
Further	O
,	O
taking	O
this	O
pretrained	O
model	O
and	O
adding	O
a	O
classification	O
layer	O
with	O
additional	O
updates	O
to	O
the	O
language	O
model	O
parameters	O
will	O
be	O
referred	O
to	O
as	O
fine	O
-	O
tuning	O
.	O
Task	O
-	O
adaptive	O
pretraining	O
(	O
TAPT	O
)	O
Although	O
RoBERTa	B-MethodName
achieves	O
strong	O
performance	O
by	O
simply	O
fine	O
-	O
tuning	O
the	O
PLMs	O
on	O
a	O
target	O
task	O
,	O
there	O
can	O
be	O
a	O
distributional	O
mismatch	O
between	O
the	O
pretraining	O
and	O
target	O
corpora	O
.	O
To	O
address	O
this	O
issue	O
,	O
pretraining	O
on	O
the	O
target	O
task	O
or	O
the	O
domain	O
of	O
the	O
target	O
task	O
can	O
be	O
usefully	O
employed	O
to	O
adapt	O
the	O
language	O
models	O
to	O
the	O
target	O
task	O
and	O
it	O
further	O
improves	O
the	O
performance	O
of	O
the	O
PLMs	O
.	O
Such	O
methods	O
can	O
be	O
referred	O
to	O
as	O
Domain	O
-	O
Adaptive	O
Pretraining	O
(	O
DAPT	O
)	O
or	O
Task	O
Adaptive	O
-	O
Pretraining	O
(	O
TAPT	O
)	O
(	O
Gururangan	O
et	O
al	O
,	O
2020	O
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
limit	O
the	O
scope	O
of	O
our	O
works	O
to	O
TAPT	O
as	O
domain	O
text	O
corpus	O
is	O
not	O
always	O
available	O
for	O
each	O
task	O
,	O
whereas	O
TAPT	O
can	O
be	O
easily	O
applied	O
by	O
directly	O
using	O
the	O
dataset	O
of	O
the	O
target	O
task	O
while	O
its	O
performance	O
often	O
matches	O
with	O
DAPT	O
(	O
Gururangan	O
et	O
al	O
,	O
2020	O
)	O
.	O
In	O
TAPT	O
,	O
the	O
second	O
phase	O
of	O
pretraining	O
is	O
per	O
-	O
Figure	O
1	O
:	O
The	O
adapter	O
achitecture	O
in	O
the	O
Transformer	B-MethodName
layer	O
(	O
Pfeiffer	O
et	O
al	O
,	O
2020a	O
)	O
formed	O
with	O
RoBERTa	B-MethodName
using	O
the	O
unlabeled	O
text	O
corpus	O
of	O
the	O
target	O
task	O
,	O
and	O
then	O
it	O
is	O
fine	O
-	O
tuned	O
on	O
the	O
target	O
task	O
.	O
Adapter	B-MethodName
Adapter	B-MethodName
modules	O
have	O
been	O
employed	O
as	O
a	O
feature	O
extractor	O
in	O
computer	O
vision	O
(	O
Rebuffi	O
et	O
al	O
,	O
2017	O
)	O
and	O
have	O
been	O
recently	O
adopted	O
in	O
the	O
NLP	O
literature	O
as	O
an	O
alternative	O
approach	O
to	O
fully	O
fine	O
-	O
tuning	O
PLMs	O
.	O
Adapters	O
are	O
sets	O
of	O
new	O
weights	O
that	O
are	O
typically	O
embedded	O
in	O
each	O
transformer	O
layer	O
of	O
PLMs	O
and	O
consist	O
of	O
feed	O
-	O
forward	O
layers	O
with	O
normalizations	O
,	O
residual	O
connections	O
,	O
and	O
projection	O
layers	O
.	O
The	O
architectures	O
of	O
adapters	O
vary	O
with	O
respect	O
to	O
the	O
different	O
configuration	O
settings	O
.	O
We	O
use	O
the	O
configuration	O
proposed	O
by	O
Pfeiffer	O
et	O
al	O
,	O
2020a	O
in	O
Figure	O
1	O
,	O
which	O
turned	O
out	O
to	O
be	O
effective	O
on	O
diverse	O
NLP	O
tasks	O
,	O
and	O
add	O
the	O
adapter	O
layer	O
to	O
each	O
transformer	O
layer	O
.	O
Pfeiffer	O
et	O
al	O
,	O
2020c	O
use	O
two	O
types	O
of	O
adapter	O
:	O
language	O
-	O
specific	O
adapters	O
and	O
taskspecific	O
adapters	O
for	O
cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
transfer	I-TaskName
.	O
These	O
two	O
types	O
of	O
adapter	O
modules	O
have	O
similar	O
architecture	O
as	O
in	O
Figure	O
1	O
.	O
However	O
,	O
the	O
language	O
adapters	O
involve	O
invertible	O
adapters	O
after	O
the	O
embedding	O
layer	O
to	O
capture	O
token	O
-	O
level	O
language	O
representation	O
when	O
those	O
are	O
trained	O
via	O
masked	B-TaskName
language	I-TaskName
modeling	I-TaskName
in	O
the	O
pretraining	O
stage	O
,	O
whereas	O
the	O
task	O
adapters	O
are	O
simply	O
embedded	O
in	O
each	O
transformer	O
layer	O
and	O
trained	O
in	O
the	O
fine	O
-	O
tuning	O
stage	O
to	O
learn	O
the	O
task	O
representation	O
.	O
Following	O
Pfeiffer	O
et	O
al	O
,	O
2020c	O
,	O
we	O
employ	O
language	O
adapter	O
modules	O
with	O
invertible	O
adapter	O
layers	O
to	O
perform	O
pretraining	O
adapters	O
on	O
the	O
unlabeled	O
target	O
dataset	O
.	O
However	O
,	O
we	O
perform	O
fine	O
-	O
tuning	O
pre	O
-	O
trained	O
parameters	O
of	O
the	O
language	O
adapter	O
modules	O
for	O
evaluation	O
to	O
align	O
with	O
(	O
Maas	O
et	O
al	O
,	O
2011	O
)	O
)	O
and	O
low	O
-	O
resource	O
(	O
CHEMPROT	B-DatasetName
(	O
Kringelum	O
et	O
al	O
,	O
2016	O
)	O
,	O
ACL	B-DatasetName
-	I-DatasetName
ARC	I-DatasetName
(	O
Jurgens	O
et	O
al	O
,	O
2018	O
)	O
,	O
SCIERC	B-DatasetName
(	O
Luan	O
et	O
al	O
,	O
2018	O
)	O
,	O
HYPERPARTISAN	O
(	O
Kiesel	O
et	O
al	O
,	O
2019	O
)	O
settings	O
.	O
TAPT	O
,	O
whereas	O
Pfeiffer	O
et	O
al	O
,	O
2020c	O
employ	O
both	O
the	O
language	O
and	O
the	O
task	O
adapters	O
by	O
stacking	O
task	O
adapters	O
on	O
top	O
of	O
the	O
language	O
adapters	O
.	O

We	O
now	O
propose	O
an	O
adapter	O
-	O
based	O
approach	O
that	O
is	O
a	O
parameter	O
efficient	O
variant	O
of	O
Task	O
-	O
Adaptive	O
Pretraining	O
(	O
TAPT	O
)	O
and	O
measure	O
the	O
margin	O
of	O
the	O
performance	O
between	O
the	O
pre	O
-	O
trained	O
adapter	O
model	O
and	O
the	O
adapter	O
model	O
without	O
pretraining	O
.	O
For	O
pretraining	O
adapters	O
,	O
we	O
added	O
the	O
adapter	O
module	O
in	O
each	O
transformer	O
layer	O
of	O
RoBERTa	B-MethodName
using	O
adaptertransformer	O
(	O
Pfeiffer	O
et	O
al	O
,	O
2020b	O
)	O
1	O
and	O
continued	O
pretraining	O
all	O
the	O
weights	O
in	O
adapter	O
layers	O
on	O
target	O
text	O
corpus	O
while	O
keeping	O
the	O
original	O
parameters	O
in	O
RoBERTa	B-MethodName
fixed	O
.	O
After	O
finishing	O
the	O
second	O
phase	O
of	O
pretraining	O
,	O
we	O
performed	O
fine	O
-	O
tuning	O
of	O
RoBERTa	B-MethodName
by	O
training	O
the	O
weights	O
in	O
the	O
adapters	O
and	O
the	O
final	O
classification	O
layers	O
while	O
keeping	O
all	O
of	O
the	O
parameters	O
in	O
RoBERTa	B-MethodName
frozen	O
.	O

Following	O
Gururangan	O
et	O
al	O
,	O
2020	O
2	O
,	O
we	O
consider	O
8	O
classification	O
tasks	O
from	O
4	O
different	O
domains	O
.	O
The	O
specification	O
of	O
each	O
task	O
is	O
shown	O
in	O
Table	O
1	O
.	O
We	O
covered	O
news	O
and	O
review	O
texts	O
that	O
are	O
similar	O
to	O
the	O
pretraining	O
corpus	O
of	O
RoBERTa	B-MethodName
as	O
well	O
as	O
scientific	O
domains	O
in	O
which	O
text	O
corpora	O
can	O
have	O
largely	O
different	O
distributions	O
from	O
those	O
of	O
RoBERTa	B-MethodName
.	O
Furthermore	O
,	O
the	O
pretraining	O
corpora	O
of	O
the	O
target	O
tasks	O
include	O
both	O
large	O
and	O
small	O
cases	O
to	O
determine	O
whether	O
the	O
adapter	O
-	O
based	O
approach	O
can	O
be	O
applicable	O
in	O
both	O
low	O
and	O
high	O
-	O
resource	O
settings	O
.	O
1	O
https://github.com/Adapter	O
-	O
Hub/	O
adapter	O
-	O
transformers	O
2	O
Downloadble	O
link	O
for	O
task	O
dataset	O
:	O
https://github	O
.	O
com	O
/	O
allenai	O
/	O
dont	O
-	O
stop	O
-	O
pretraining	O

Our	O
work	O
demonstrates	O
that	O
adapters	O
provide	O
a	O
competitive	O
alternative	O
to	O
large	O
-	O
scale	O
task	O
-	O
adaptive	O
pretraining	O
for	O
NLP	O
classification	O
tasks	O
.	O
We	O
show	O
that	O
it	O
is	O
possible	O
to	O
achieve	O
similar	O
performance	O
to	O
TAPT	O
with	O
pretraining	O
training	O
just	O
1.32	O
%	O
of	O
the	O
parameters	O
through	O
pretraining	O
with	O
adapters	O
.	O
However	O
,	O
the	O
most	O
computationally	O
efficient	O
option	O
is	O
to	O
skip	O
pretraining	O
and	O
only	O
perform	O
fine	O
-	O
tuning	O
with	O
adapters	O
.	O
We	O
found	O
that	O
skipping	O
pretraining	O
altogether	O
and	O
just	O
fine	O
-	O
tuning	O
with	O
adapters	O
outperforms	O
or	O
performs	O
mostly	O
on	O
par	O
with	O
TAPT	O
and	O
the	O
adapter	O
model	O
with	O
pretraining	O
across	O
our	O
tasks	O
while	O
substantially	O
reducing	O
the	O
training	O
time	O
.	O
Xiang	O
Zhang	O
,	O
Junbo	O
Zhao	O
,	O
and	O
Yann	O
LeCun	O
.	O
2015	O
.	O
Character	O
-	O
level	O
convolutional	O
networks	O
for	O
text	B-TaskName
classification	I-TaskName
.	O
In	O
Advances	O
in	O
Neural	O
Information	O
Processing	O
Systems	O
,	O
volume	O
28	O
,	O
pages	O
649	O
-	O
657	O
.	O
Curran	O
Associates	O
,	O
Inc.	O

Comparing	O
CRF	B-MethodName
and	O
LSTM	B-MethodName
performance	O
on	O
the	O
task	O
of	O
morphosyntactic	O
tagging	O
of	O
non	O
-	O
standard	O
varieties	O
of	O
South	O
Slavic	O
languages	O

This	O
paper	O
presents	O
two	O
systems	O
taking	O
part	O
in	O
the	O
Morphosyntactic	O
Tagging	O
of	O
Tweets	O
shared	O
task	O
on	O
Slovene	O
,	O
Croatian	O
and	O
Serbian	O
data	O
,	O
organized	O
inside	O
the	O
VarDial	O
Evaluation	O
Campaign	O
.	O
While	O
one	O
system	O
relies	O
on	O
the	O
traditional	O
method	O
for	O
sequence	O
labeling	O
(	O
conditional	O
random	O
fields	O
)	O
,	O
the	O
other	O
relies	O
on	O
its	O
neural	O
alternative	O
(	O
bidirectional	O
long	B-MethodName
short	I-MethodName
-	I-MethodName
term	I-MethodName
memory	I-MethodName
)	O
.	O
We	O
investigate	O
the	O
similarities	O
and	O
differences	O
of	O
these	O
two	O
approaches	O
,	O
showing	O
that	O
both	O
methods	O
yield	O
very	O
good	O
and	O
quite	O
similar	O
results	O
,	O
with	O
the	O
neural	O
model	O
outperforming	O
the	O
traditional	O
one	O
more	O
as	O
the	O
level	O
of	O
non	O
-	O
standardness	O
of	O
the	O
text	O
increases	O
.	O
Through	O
an	O
error	O
analysis	O
we	O
show	O
that	O
the	O
neural	O
system	O
is	O
better	O
at	O
long	O
-	O
range	O
dependencies	O
,	O
while	O
the	O
traditional	O
system	O
excels	O
and	O
slightly	O
outperforms	O
the	O
neural	O
system	O
at	O
the	O
local	O
ones	O
.	O
We	O
present	O
in	O
the	O
paper	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
in	O
morphosyntactic	O
annotation	O
of	O
non	O
-	O
standard	O
text	O
for	O
Slovene	O
,	O
Croatian	O
and	O
Serbian	O
.	O

In	O
this	O
paper	O
we	O
present	O
two	O
systems	O
taking	O
part	O
in	O
the	O
MTT	O
(	O
Morphosyntactic	O
Tagging	O
of	O
Tweets	O
)	O
shared	O
task	O
,	O
part	O
of	O
the	O
VarDial	O
Evaluation	O
Campaign	O
(	O
Zampieri	O
et	O
al	O
,	O
2018	O
)	O
.	O
In	O
the	O
task	O
,	O
general	O
-	O
domain	O
and	O
indomain	O
datasets	O
with	O
tokens	O
manually	O
annotated	O
with	O
morphosyntactic	O
descriptions	O
(	O
MSDs	O
)	O
,	O
are	O
given	O
,	O
together	O
with	O
large	O
web	O
-	O
based	O
datasets	O
,	O
for	O
three	O
South	O
Slavic	O
languages	O
:	O
Slovene	O
,	O
Croatian	O
and	O
Serbian	O
.	O
The	O
challenge	O
of	O
the	O
task	O
is	O
to	O
exploit	O
similarity	O
of	O
standard	O
vs.	O
non	O
-	O
standard	O
variants	O
,	O
as	O
well	O
as	O
the	O
overall	O
proximity	O
of	O
the	O
three	O
languages	O
in	O
question	O
.	O
While	O
the	O
first	O
system	O
,	O
JANES	O
,	O
relies	O
on	O
the	O
traditional	O
method	O
for	O
sequence	O
labeling	O
,	O
namely	O
conditional	O
random	O
fields	O
(	O
CRF	B-MethodName
)	O
,	O
the	O
second	O
system	O
,	O
JSI	O
,	O
relies	O
on	O
the	O
currently	O
hugely	O
popular	O
neural	O
networks	O
,	O
more	O
precisely	O
bidirectional	O
long	O
short	O
-	O
term	O
memories	O
(	O
BiLSTM	B-MethodName
)	O
.	O
The	O
contributions	O
of	O
this	O
paper	O
are	O
the	O
following	O
:	O
(	O
1	O
)	O
a	O
direct	O
comparison	O
of	O
CRFs	O
and	O
BiLSTMs	O
on	O
a	O
series	O
of	O
datasets	O
,	O
where	O
CRFs	O
are	O
equipped	O
with	O
carefully	O
engineered	O
features	O
,	O
not	O
generic	O
ones	O
,	O
and	O
(	O
2	O
)	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
tagging	O
non	O
-	O
standard	O
varieties	O
of	O
the	O
three	O
languages	O
in	O
question	O
.	O
2	O
System	O
Descriptions	O
2.1	O
Datasets	O
Distributed	O
inside	O
the	O
Shared	O
Task	O
Before	O
we	O
describe	O
our	O
two	O
systems	O
participating	O
in	O
the	O
task	O
,	O
we	O
quickly	O
quantify	O
the	O
available	O
resources	O
through	O
token	O
number	O
in	O
Table	O
1	O
as	O
these	O
heavily	O
influence	O
our	O
decisions	O
in	O
the	O
system	O
setup	O
.	O
The	O
twitter	O
.	O
*	O
datasets	O
come	O
from	O
the	O
Janes	O
-	O
Tag	O
manually	O
annotated	O
dataset	O
of	O
Slovene	O
computermediated	O
communication	O
(	O
Erjavec	O
et	O
al	O
,	O
2017	O
)	O
and	O
the	O
ReLDI	O
-	O
NormTagNER	O
-	O
*	O
manually	O
annotated	O
datasets	O
of	O
Croatian	O
(	O
Ljubešić	O
et	O
al	O
,	O
2017b	O
)	O
and	O
Serbian	O
(	O
Ljubešić	O
et	O
al	O
,	O
2017c	O
)	O
tweets	O
.	O
They	O
are	O
all	O
similar	O
in	O
size	O
,	O
with	O
cca	O
.	O
40	O
thousand	O
tokens	O
available	O
for	O
training	O
,	O
8	O
thousand	O
for	O
development	O
and	O
20	O
thousand	O
for	O
testing	O
.	O
The	O
standard.train	O
datasets	O
mostly	O
cover	O
the	O
general	O
domain	O
.	O
While	O
the	O
Slovene	O
and	O
Croatian	O
datasets	O
are	O
similar	O
in	O
size	O
with	O
around	O
500	O
thousand	O
tokens	O
,	O
the	O
Serbian	O
dataset	O
is	O
significantly	O
smaller	O
with	O
only	O
87	O
thousand	O
tokens	O
.	O
The	O
web.auto	O
datasets	O
are	O
large	O
web	O
-	O
based	O
datasets	O
,	O
slWac	O
for	O
Slovene	O
,	O
hrWaC	O
for	O
Croatian	O
and	O
srWaC	O
for	O
Serbian	O
(	O
Ljubešić	O
and	O
Klubička	O
,	O
2014	O
)	O
.	O
These	O
are	O
automatically	O
annotated	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
taggers	O
of	O
standard	O
language	O
for	O
Slovene	O
(	O
Ljubešić	O
and	O
Erjavec	O
,	O
2016	O
)	O
and	O
Croatian	O
and	O
Serbian	O

The	O
JSI	O
system	O
(	O
the	O
name	O
comes	O
from	O
the	O
name	O
of	O
our	O
current	O
employer	O
,	O
the	O
Jožef	O
Stefan	O
Institute	O
)	O
is	O
an	O
adaptation	O
of	O
the	O
BiLSTM	B-MethodName
tagger	O
written	O
in	O
pytorch	O
2	O
,	O
with	O
some	O
added	O
modifications	O
.	O
The	O
architecture	O
of	O
the	O
submitted	O
system	O
is	O
the	O
following	O
:	O
a	O
character	O
-	O
level	O
subnetwork	O
,	O
consisting	O
of	O
a	O
character	O
embedding	O
layer	O
of	O
16	O
dimensions	O
and	O
a	O
BiLSTM	B-MethodName
layer	O
with	O
25	O
units	O
the	O
main	O
network	O
concatenating	O
the	O
character	O
-	O
level	O
representation	O
of	O
a	O
word	O
from	O
the	O
subnetwork	O
described	O
above	O
(	O
25	O
*	O
2	O
,	O
i.e.	O
,	O
50	O
dimensions	O
)	O
,	O
and	O
the	O
word	O
embedding	O
layer	O
(	O
100	O
dimensions	O
)	O
feeding	O
this	O
concatenated	O
150	O
-	O
dimensional	O
character	O
-	O
and	O
word	O
-	O
level	O
representation	O
into	O
a	O
BiL	O
-	O
STM	O
layer	O
with	O
100	O
units	O
the	O
per	O
-	O
token	O
BiLSTM	B-MethodName
output	O
being	O
fed	O
to	O
a	O
fully	O
-	O
connected	O
layer	O
with	O
256	O
units	O
and	O
a	O
final	O
softmax	B-MethodName
layer	O
for	O
prediction	O
While	O
developing	O
this	O
architecture	O
,	O
we	O
investigated	O
the	O
impact	O
of	O
various	O
setups	O
on	O
the	O
Slovene	O
dataset	O
.	O
The	O
results	O
of	O
experimenting	O
with	O
(	O
1	O
)	O
different	O
pretrained	O
word	B-TaskName
embeddings	I-TaskName
,	O
(	O
2	O
)	O
the	O
impact	O
of	O
adding	O
different	O
character	O
-	O
level	O
representations	O
,	O
(	O
3	O
)	O
fine	O
-	O
tuning	O
the	O
model	O
on	O
in	O
-	O
domain	O
data	O
and	O
(	O
4	O
)	O
pretraining	O
the	O
character	O
-	O
level	O
encoder	O
on	O
a	O
inflectional	O
lexicon	O
,	O
are	O
shown	O
in	O
Table	O
2	O
.	O
We	O
performed	O
our	O
experiments	O
on	O
each	O
of	O
the	O
above	O
mentioned	O
issues	O
subsequently	O
,	O
always	O
propagating	O
to	O
the	O
next	O
experiment	O
set	O
the	O
setup	O
achieving	O
best	O
results	O
in	O
the	O
previous	O
one	O
.	O
The	O
setup	O
we	O
start	O
with	O
consists	O
only	O
of	O
the	O
main	O
network	O
,	O
without	O
the	O
character	O
-	O
level	O
subnetwork	O
.	O

The	O
third	O
experiment	O
considers	O
the	O
impact	O
of	O
not	O
training	O
the	O
network	O
on	O
a	O
simple	O
merge	O
of	O
all	O
the	O
available	O
relevant	O
training	O
data	O
,	O
but	O
also	O
fine	O
-	O
tuning	O
the	O
network	O
exclusively	O
on	O
in	O
-	O
domain	O
data	O
.	O
Running	O
three	O
epochs	O
on	O
the	O
concatenation	O
of	O
all	O
datasets	O
,	O
and	O
then	O
additional	O
two	O
epochs	O
only	O
on	O
the	O
in	O
-	O
domain	O
Twitter	O
data	O
,	O
consistently	O
improved	O
the	O
results	O
for	O
around	O
half	O
an	O
accuracy	B-MetricName
point	O
.	O
This	O
method	O
is	O
somewhat	O
similar	O
to	O
the	O
oversampling	O
method	O
applied	O
on	O
the	O
JANES	O
system	O
.	O
It	O
is	O
,	O
however	O
,	O
more	O
elegant	O
as	O
it	O
gives	O
greater	O
control	O
over	O
the	O
amount	O
and	O
order	O
of	O
data	O
fed	O
into	O
the	O
system	O
.	O

In	O
this	O
section	O
we	O
perform	O
an	O
analysis	O
of	O
confusion	O
matrices	O
of	O
the	O
JANES	O
and	O
the	O
JSI	O
-	O
simpler	O
system	O
.	O
We	O
perform	O
the	O
analysis	O
on	O
the	O
output	O
of	O
the	O
system	O
on	O
the	O
Croatian	O
test	O
set	O
.	O
We	O
analyze	O
and	O
compare	O
the	O
10	O
most	O
frequent	O
confusions	O
for	O
each	O
system	O
,	O
which	O
covers	O
roughly	O
20	O
%	O
of	O
all	O
errors	O
done	O
by	O
each	O
of	O
the	O
systems	O
.	O
The	O
confusion	O
pairs	O
are	O
given	O
in	O
Table	O
4	O
.	O
Both	O
systems	O
make	O
similar	O
most	O
frequent	O
mistakes	O
,	O
some	O
of	O
which	O
are	O
typical	O
for	O
morphosyntactic	O
tagging	O
of	O
standard	O
varieties	O
of	O
South	O
Slavic	O
languages	O
,	O
other	O
being	O
more	O
specific	O
for	O
the	O
Twitter	O
variety	O
.	O
The	O
typical	O
mistakes	O
on	O
the	O
standard	O
language	O
include	O
confusing	O
nominative	O
masculinum	O
common	O
nouns	O
(	O
Ncmsn	O
)	O
for	O
accusative	O
masculinum	O
common	O
nouns	O
(	O
Ncmsan	O
)	O
and	O
vice	O
versa	O
,	O
confusing	O
the	O
word	O
"	O
i	O
"	O
(	O
English	O
"	O
and	O
"	O
)	O
in	O
its	O
coordinating	O
conjunction	O
(	O
Cc	O
)	O
and	O
particle	O
(	O
Qo	O
)	O
usage	O
,	O
confusing	O
adverbs	O
(	O
Rgp	O
)	O
for	O
adjectives	O
(	O
Agpnsny	O
for	O
instance	O
)	O
and	O
confusing	O
the	O
word	O
"	O
kada	O
"	O
(	O
English	O
"	O
when	O
"	O
)	O
in	O
its	O
subordinative	O
conjunction	O
(	O
Cs	O
)	O
and	O
adverbial	O
(	O
Rgp	O
)	O
usage	O
.	O
The	O
errors	O
that	O
are	O
more	O
due	O
to	O
the	O
specificity	O
of	O
the	O
Twitter	O
variety	O
are	O
confusing	O
proper	O
names	O
(	O
Np	O
.	O
*	O
)	O
or	O
common	O
nouns	O
(	O
Nc	O
.	O
*	O
)	O
for	O
foreign	O
residuals	O
(	O
mostly	O
foreign	O
words	O
or	O
foreign	O
sequences	O
of	O
words	O
,	O
Xf	O
)	O
and	O
vice	O
versa	O
.	O
When	O
comparing	O
the	O
most	O
frequent	O
errors	O
between	O
the	O
two	O
systems	O
,	O
the	O
JANES	O
CRF	B-MethodName
-	O
based	O
system	O
seems	O
to	O
have	O
more	O
problems	O
with	O
the	O
traditional	O
discrimination	O
between	O
different	O
context	O
-	O
dependent	O
cases	O
of	O
nouns	O
,	O
which	O
points	O
to	O
the	O
direction	O
that	O
BiLSTMs	O
are	O
better	O
at	O
modeling	O
long	O
-	O
range	O
dependencies	O
as	O
discriminating	O
between	O
the	O
nominative	O
and	O
the	O
accusative	O
case	O
often	O
requires	O
a	O
very	O
wide	O
context	O
.	O
On	O
the	O
other	O
hand	O
,	O
what	O
the	O
BiLSTM	B-MethodName
system	O
seems	O
to	O
be	O
worse	O
at	O
is	O
discriminating	O
between	O
different	O
cases	O
for	O
prepositions	O
,	O
which	O
heavily	O
depends	O
on	O
the	O
following	O
adjective	O
or	O
noun	O
.	O
While	O
confusing	O
an	O
accusative	O
preposition	O
(	O
Sa	O
)	O
for	O
a	O
locative	O
one	O
(	O
Sl	O
)	O
the	O
BiLSTM	B-MethodName
system	O
did	O
23	O
times	O
,	O
this	O
happened	O
to	O
the	O
CRF	B-MethodName
system	O
17	O
times	O
.	O
In	O
the	O
opposite	O
direction	O
,	O
the	O
BiLSTM	B-MethodName
system	O
did	O
19	O
mistakes	O
while	O
the	O
CRF	B-MethodName
system	O
did	O
one	O
mistake	O
less	O
,	O
namely	O
18	O
of	O
them	O
.	O
While	O
it	O
is	O
clear	O
why	O
CRFs	O
excel	O
at	O
predicting	O
prepositional	O
cases	O
correctly	O
as	O
this	O
dependence	O
is	O
in	O
the	O
scope	O
of	O
the	O
local	O
features	O
,	O
it	O
seems	O
that	O
the	O
BiLSTMs	O
trade	O
more	O
mistakes	O
in	O
the	O
local	O
context	O
for	O
less	O
mistakes	O
in	O
a	O
wider	O
one	O
.	O

An	O
Empirical	O
Investigation	O
of	O
Word	B-TaskName
Alignment	I-TaskName
Supervision	O
for	O
Zero	O
-	O
Shot	O
Multilingual	O
Neural	O
Machine	B-TaskName
Translation	I-TaskName

Zero	O
-	O
shot	O
translations	O
is	O
a	O
fascinating	O
feature	O
of	O
Multilingual	O
Neural	O
Machine	B-TaskName
Translation	I-TaskName
(	O
MNMT	O
)	O
systems	O
.	O
These	O
MNMT	O
models	O
are	O
usually	O
trained	O
on	O
English	O
-	O
centric	O
data	O
,	O
i.e.	O
English	O
either	O
as	O
the	O
source	O
or	O
target	O
language	O
,	O
and	O
with	O
a	O
language	O
label	O
prepended	O
to	O
the	O
input	O
indicating	O
the	O
target	O
language	O
.	O
However	O
,	O
recent	O
work	O
has	O
highlighted	O
several	O
flaws	O
of	O
these	O
models	O
in	O
zero	O
-	O
shot	O
scenarios	O
where	O
language	O
labels	O
are	O
ignored	O
and	O
the	O
wrong	O
language	O
is	O
generated	O
or	O
different	O
runs	O
show	O
highly	O
unstable	O
results	O
.	O
In	O
this	O
paper	O
,	O
we	O
investigate	O
the	O
benefits	O
of	O
an	O
explicit	O
alignment	O
to	O
language	O
labels	O
in	O
Transformer	B-MethodName
-	O
based	O
MNMT	O
models	O
in	O
the	O
zero	O
-	O
shot	O
context	O
,	O
by	O
jointly	O
training	O
one	O
cross	O
attention	O
head	O
with	O
word	B-TaskName
alignment	I-TaskName
supervision	O
to	O
stress	O
the	O
focus	O
on	O
the	O
target	O
language	O
label	O
.	O
We	O
compare	O
and	O
evaluate	O
several	O
MNMT	O
systems	O
on	O
three	O
multilingual	O
MT	O
benchmarks	O
of	O
different	O
sizes	O
,	O
showing	O
that	O
simply	O
supervising	O
one	O
cross	O
attention	O
head	O
to	O
focus	O
both	O
on	O
word	O
alignments	O
and	O
language	O
labels	O
reduces	O
the	O
bias	O
towards	O
translating	O
into	O
the	O
wrong	O
language	O
,	O
improving	O
the	O
zero	O
-	O
shot	O
performance	O
overall	O
.	O
Moreover	O
,	O
as	O
an	O
additional	O
advantage	O
,	O
we	O
find	O
that	O
our	O
alignment	O
supervision	O
leads	O
to	O
more	O
stable	O
results	O
across	O
different	O
training	O
runs	O
.	O

Multilingual	O
Neural	O
Machine	B-TaskName
Translation	I-TaskName
(	O
MNMT	O
)	O
focuses	O
on	O
translation	O
between	O
multiple	O
language	O
pairs	O
through	O
a	O
single	O
optimized	O
neural	O
model	O
,	O
and	O
has	O
been	O
explored	O
from	O
different	O
angles	O
witnessing	O
a	O
rapid	O
progress	O
in	O
recent	O
years	O
(	O
Arivazhagan	O
et	O
al	O
,	O
2019b	O
;	O
Dabre	O
et	O
al	O
,	O
2020	O
;	O
Lin	O
et	O
al	O
,	O
2021	O
)	O
.	O
Besides	O
the	O
great	O
flexibility	O
MNMT	O
models	O
offer	O
,	O
they	O
are	O
also	O
highlighted	O
by	O
their	O
so	O
called	O
zero	O
-	O
shot	O
translation	O
capabilities	O
,	O
i.e.	O
,	O
translating	O
between	O
all	O
combinations	O
of	O
languages	O
available	O
in	O
the	O
training	O
data	O
,	O
including	O
those	O
with	O
no	O
parallel	O
data	O
seen	O
at	O
training	O
time	O
(	O
Ha	O
et	O
al	O
,	O
2016	O
;	O
Firat	O
et	O
al	O
,	O
2016	O
;	O
Johnson	O
et	O
al	O
,	O
2017	O
)	O
.	O
Many	O
studies	O
have	O
investigated	O
this	O
feature	O
,	O
focusing	O
on	O
the	O
impact	O
of	O
both	O
,	O
the	O
model	O
architecture	O
design	O
(	O
Arivazhagan	O
et	O
al	O
,	O
2019a	O
;	O
and	O
data	O
pre	O
-	O
processing	O
(	O
Lee	O
et	O
al	O
,	O
2017	O
;	O
Wang	O
et	O
al	O
,	O
2019	O
;	O
Rios	O
et	O
al	O
,	O
2020	O
;	O
.	O
Broadly	O
speaking	O
,	O
MNMT	O
architectures	O
are	O
categorized	O
according	O
to	O
their	O
degree	O
of	O
parameter	O
sharing	O
,	O
from	O
fully	O
shared	O
(	O
Johnson	O
et	O
al	O
,	O
2017	O
)	O
to	O
the	O
use	O
of	O
language	O
-	O
specific	O
components	O
(	O
Vázquez	O
et	O
al	O
,	O
2020	O
;	O
Escolano	O
et	O
al	O
,	O
2021	O
;	O
Zhang	O
et	O
al	O
,	O
2021	O
)	O
.	O
The	O
Johnson	O
et	O
al	O
(	O
2017	O
)	O
MNMT	O
model	O
is	O
widely	O
used	O
,	O
due	O
to	O
its	O
simplicity	O
and	O
good	O
translation	O
quality	O
.	O
It	O
uses	O
the	O
fully	O
shared	O
parameters	O
setting	O
,	O
and	O
relies	O
on	O
appending	O
an	O
artificial	O
language	O
label	O
to	O
each	O
input	O
sentence	O
to	O
indicate	O
the	O
target	O
language	O
.	O
While	O
this	O
method	O
allows	O
for	O
zeroshot	O
translation	O
,	O
several	O
works	O
have	O
highlighted	O
two	O
major	O
flaws	O
:	O
i	O
)	O
its	O
failure	O
to	O
reliably	O
generalize	O
to	O
unseen	O
language	O
pairs	O
,	O
ending	O
up	O
with	O
the	O
so	O
called	O
off	O
-	O
target	O
issue	O
,	O
where	O
the	O
language	O
label	O
is	O
ignored	O
and	O
the	O
wrong	O
target	O
language	O
is	O
produced	O
as	O
a	O
result	O
,	O
ii	O
)	O
its	O
lack	O
of	O
stability	O
in	O
translation	O
results	O
between	O
different	O
training	O
runs	O
(	O
Rios	O
et	O
al	O
,	O
2020	O
)	O
.	O
In	O
this	O
work	O
,	O
we	O
investigate	O
the	O
role	O
of	O
guided	O
alignment	O
in	O
the	O
Johnson	O
et	O
al	O
(	O
2017	O
)	O
setting	O
,	O
by	O
jointly	O
training	O
one	O
cross	O
attention	O
head	O
to	O
explicitly	O
focus	O
on	O
the	O
target	O
language	O
label	O
.	O
We	O
show	O
that	O
alignment	O
supervision	O
mitigates	O
the	O
off	O
-	O
target	O
translation	O
issue	O
in	O
the	O
zero	O
-	O
shot	O
case	O
.	O
Our	O
method	O
improves	O
the	O
zero	O
-	O
shot	O
translation	O
performance	O
and	O
results	O
in	O
more	O
stable	O
results	O
across	O
different	O
training	O
runs	O
.	O

In	O
this	O
work	O
we	O
present	O
an	O
empirical	O
comparative	O
evaluation	O
of	O
integrating	O
different	O
alignment	O
methods	O
in	O
Transformer	B-MethodName
-	O
based	O
models	O
for	O
highly	O
multilingual	O
English	O
-	O
centric	O
MT	O
setups	O
.	O
Our	O
extensive	O
evaluation	O
over	O
three	O
alignment	O
variants	O
shows	O
that	O
adding	O
alignment	O
supervision	O
between	O
corresponding	O
words	O
and	O
the	O
language	O
label	O
consistently	O
improves	O
the	O
stability	O
of	O
the	O
models	O
,	O
resulting	O
in	O
stable	O
performance	O
across	O
different	O
runs	O
and	O
mitigating	O
the	O
off	O
-	O
target	O
translation	O
issue	O
in	O
the	O
zero	O
-	O
shot	O
scenario	O
.	O
We	O
believe	O
that	O
our	O
work	O
will	O
pave	O
the	O
way	O
for	O
designing	O
new	O
and	O
better	O
multilingual	O
MT	O
models	O
to	O
improve	O
their	O
generalization	O
in	O
zero	O
-	O
shot	O
setups	O
.	O
As	O
future	O
work	O
,	O
we	O
intend	O
to	O
analyze	O
the	O
quality	O
of	O
the	O
learned	O
alignments	O
and	O
their	O
effect	O
on	O
the	O
other	O
attention	O
weights	O
in	O
both	O
supervised	O
and	O
zeroshot	O
evaluation	O
data	O
(	O
Raganato	O
and	O
Tiedemann	O
,	O
2018	O
;	O
Tang	O
et	O
al	O
,	O
2018	O
;	O
Mareček	O
and	O
Rosa	O
,	O
2019	O
;	O
Voita	O
et	O
al	O
,	O
2019	O
)	O
.	O
Finally	O
,	O
we	O
plan	O
to	O
explore	O
other	O
mechanisms	O
to	O
inject	O
prior	O
knowledge	O
to	O
better	O
handle	O
zero	O
-	O
shot	O
translations	O
(	O
Deshpande	O
and	O
Narasimhan	O
,	O
2020	O
;	O
Song	O
et	O
al	O
,	O
2020	O
)	O
.	O

A.1	O
Data	O
TED	O
Talks	O
(	O
Qi	O
et	O
al	O
,	O
2018	O
)	O
.	O
This	O
parallel	O
corpus	O
includes	O
59	O
language	O
pairs	O
from	O
and	O
to	O
English	O
.	O
It	O
is	O
a	O
highly	O
imbalanced	O
benchmark	O
,	O
ranging	O
from	O
less	O
than	O
4	O
K	O
up	O
to	O
215	O
K	O
training	O
sentences	O
.	O
We	O
use	O
the	O
same	O
languages	O
as	O
Aharoni	O
et	O
al	O
(	O
2019	O
)	O
for	O
both	O
supervised	O
testing	O
and	O
zero	O
-	O
shot	O
evaluation	O
.	O
As	O
supervised	O
test	O
sets	O
,	O
we	O
use	O
{	O
Azerbeijani	O
,	O
Belarusian	O
,	O
Galician	O
,	O
Slovak	O
,	O
Arabic	O
,	O
German	O
,	O
Hebrew	O
,	O
Italian	O
}	O
↔English	O
.	O
As	O
zero	O
-	O
shot	O
test	O
sets	O
,	O
we	O
use	O
Arabic↔French	O
,	O
and	O
Ukrainian↔Russian	O
.	O
WMT	O
-	O
2018	O
(	O
Bojar	O
et	O
al	O
,	O
2018	O
)	O
.	O
We	O
use	O
training	O
and	O
testing	O
data	O
as	O
provided	O
by	O
the	O
WMT	B-DatasetName
2018	I-DatasetName
news	I-DatasetName
translation	O
task	O
organizers	O
.	O
The	B-DatasetName
benchmark	I-DatasetName
contains	O
a	O
total	O
of	O
14	O
language	O
pairs	O
:	O
{	O
Chinese	O
,	O
Czech	O
,	O
Estonian	O
,	O
Finnish	O
,	O
German	O
,	O
Russian	O
,	O
Turkish	O
}	O
↔English	O
.	O
For	O
training	O
,	O
we	O
use	O
up	O
to	O
5	O
M	O
parallel	O
sentences	O
per	O
language	O
pair	O
,	O
with	O
Turkish↔English	O
,	O
Estonian↔English	O
,	O
and	O
Finnish↔English	O
,	O
having	O
only	O
200	O
K	O
,	O
1	O
M	O
,	O
and	O
2.7	O
M	O
training	O
sentences	O
,	O
respectively	O
.	O
For	O
zeroshot	O
test	O
sets	O
,	O
we	O
use	O
the	O
test	O
data	O
from	O
Tiedemann	O
(	O
2020	O
)	O
,	O
using	O
the	O
following	O
24	O
language	O
directions	O
:	O
Czech	O
↔	O
German	O
,	O
German	O
↔	O
Russian	O
,	O
German	O
↔	O
Chinese	O
,	O
Finnish	O
↔	O
German	O
,	O
Finnish	O
↔	O
Turkish	O
,	O
Russian	O
↔	O
Finnish	O
,	O
Russian	O
↔	O
Chinese	O
,	O
Turkish	O
↔	O
Chinese	O
,	O
Czech	O
↔	O
Russian	O
,	O
German	O
↔	O
Turkish	O
,	O
Estonian	O
↔	O
Russian	O
,	O
Russian	O
↔	O
Turkish	O
OPUS	O
-	O
100	O
.	O
OPUS	O
-	O
100	O
is	O
a	O
recent	O
benchmark	O
consisting	O
of	O
55	O
M	O
Englishcentric	O
sentence	O
pairs	O
covering	O
100	O
languages	O
.	O
The	O
data	O
is	O
collected	O
from	O
movie	O
subtitles	O
,	O
GNOME	O
documentation	O
,	O
and	O
the	O
Bible	O
.	O
Out	O
of	O
99	O
language	O
pairs	O
,	O
44	O
have	O
1	O
M	O
sentences	O
,	O
73	O
have	O
at	O
least	O
100	O
K	O
sentences	O
,	O
and	O
95	O
at	O
least	O
10K.	O
It	O
provides	O
also	O
zero	O
-	O
shot	O
test	O
sets	O
,	O
pairing	O
the	O
following	O
languages	O
:	O
Arabic	O
,	O
Chinese	O
,	O
Dutch	O
,	O
French	O
,	O
German	O
,	O
and	O
Russian	O
.	O

Question	B-TaskName
Answering	I-TaskName
(	O
QA	O
)	O
involves	O
constructing	O
an	O
answer	O
for	O
a	O
given	O
question	O
in	O
either	O
an	O
extractive	O
or	O
an	O
abstractive	O
manner	O
.	O
QA	O
systems	O
are	O
central	O
to	O
other	O
Natural	O
Language	O
Processing	O
(	O
NLP	O
)	O
applications	O
like	O
search	O
engines	O
,	O
and	O
dialogue	O
.	O
Recently	O
,	O
QA	O
based	O
solutions	O
have	O
also	O
been	O
proposed	O
to	O
evaluate	O
factuality	O
(	O
Wang	O
et	O
al	O
,	O
2020	O
)	O
and	O
faithfulness	O
(	O
Durmus	O
et	O
al	O
,	O
2020	O
)	O
of	O
abstractive	O
summarization	B-TaskName
systems	O
.	O
In	O
addition	O
to	O
popular	O
QA	O
benchmarks	O
like	O
SQuAD	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
,	O
and	O
MRQA	B-DatasetName
-	O
2019	O
(	O
Fisch	O
et	O
al	O
,	O
2019	O
)	O
,	O
we	O
have	O
seen	O
QA	O
challenges	O
that	O
require	O
reasoning	O
over	O
human	O
dialogue	O
.	O
Some	O
notable	O
examples	O
being	O
QuAC	B-DatasetName
(	O
Choi	O
et	O
al	O
,	O
2018	O
)	O
and	O
CoQA	B-DatasetName
(	O
Reddy	O
et	O
al	O
,	O
2019	O
)	O
.	O
These	O
datasets	O
require	O
the	O
model	O
to	O
attend	O
to	O
the	O
entire	O
dialogue	O
context	O
in	O
the	O
process	O
of	O
retrieving	O
an	O
answer	O
.	O
In	O
this	O
work	O
,	O
we	O
are	O
interesting	O
in	O
building	O
a	O
QA	O
system	O
to	O
help	O
with	O
human	O
dialogue	O
.	O
Feng	O
et	O
al	O
(	O
2020	O
)	O
introduced	O
a	O
new	O
dataset	O
of	O
goal	O
-	O
oriented	O
dialogues	O
(	O
Doc2Dial	B-DatasetName
)	O
that	O
are	O
grounded	O
in	O
the	O
associated	O
documents	O
.	O
Each	O
sample	O
in	O
the	O
dataset	O
consists	O
of	O
an	O
information	O
-	O
seeking	O
conversation	O
between	O
a	O
user	O
and	O
an	O
agent	B-DatasetName
where	O
agent	B-DatasetName
's	O
responses	O
are	O
grounded	O
in	O
FAQ	O
-	O
like	O
webpages	O
.	O
DialDoc	O
shared	O
task	O
derives	O
its	O
training	O
data	O
from	O
the	O
Doc2Dial	B-DatasetName
dataset	O
and	O
proposes	O
two	O
subtasks	O
which	O
require	O
the	O
participants	O
to	O
(	O
1	O
)	O
identify	O
the	O
grounding	O
knowledge	O
in	O
form	O
of	O
document	O
span	O
for	O
the	O
next	O
agent	B-DatasetName
turn	O
;	O
and	O
(	O
2	O
)	O
generate	O
the	O
next	O
agent	B-DatasetName
response	O
in	O
natural	O
language	O
.	O
In	O
this	O
paper	O
,	O
we	O
describe	O
our	O
solution	O
to	O
the	O
subtask	O
1	O
.	O
This	O
subtask	O
is	O
formulated	O
as	O
a	O
span	O
selection	O
problem	O
.	O
Therefore	O
,	O
we	O
leverage	O
a	O
transformerbased	O
extractive	O
question	O
-	O
answering	O
model	O
Lan	O
et	O
al	O
,	O
2019	O
)	O
to	O
extract	O
the	O
relevant	O
spans	O
from	O
the	O
document	O
.	O
We	O
pretrain	O
our	O
model	O
on	O
different	O
QA	O
datasets	O
like	O
SQuAD	B-DatasetName
,	O
different	O
subsets	O
of	O
MRQA	B-DatasetName
-	O
2019	O
training	O
set	O
,	O
and	O
conversational	O
QA	O
datasets	O
like	O
CoQA	B-DatasetName
and	O
QuAC	B-DatasetName
.	O
We	O
find	O
that	O
models	O
pretrained	O
on	O
out	O
-	O
of	O
-	O
domain	O
QA	O
datasets	O
substantially	O
outperform	O
the	O
baseline	O
.	O
Our	O
experiments	O
suggest	O
that	O
conversational	O
QA	O
datasets	O
are	O
more	O
useful	O
than	O
MRQA	B-DatasetName
-	O
2019	O
data	O
or	O
its	O
subsets	O
.	O
In	O
the	O
following	O
sections	O
,	O
we	O
first	O
present	O
an	O
overview	O
of	O
the	O
DialDoc	O
shared	O
task	O
(	O
2	O
)	O
,	O
followed	O
by	O
our	O
system	O
description	O
(	O
3	O
)	O
and	O
a	O
detailed	O
account	O
of	O
our	O
experimental	O
results	O
,	O
and	O
ablation	O
studies	O
(	O
4	O
,	O
5	O
)	O
.	O

Dataset	O
used	O
in	O
the	O
DialDoc	O
shared	O
-	O
task	O
is	O
derived	O
from	O
Doc2Dial	B-DatasetName
dataset	O
(	O
Feng	O
et	O
al	O
,	O
2020	O
)	O
,	O
a	O
new	O
dataset	O
with	O
goal	O
-	O
oriented	O
document	O
-	O
grounded	O
dialogue	O
.	O
It	O
includes	O
a	O
set	O
of	O
documents	O
and	O
conversations	O
between	O
a	O
user	O
and	O
an	O
agent	B-DatasetName
grounded	O
in	O
the	O
associated	O
document	O
.	O
The	O
authors	O
provide	O
annotations	O
for	O
dialogue	O
acts	O
for	O
each	O
utterance	O
in	O
the	O
dialogue	O
flow	O
,	O
along	O
with	O
the	O
span	O
in	O
the	O
document	O
that	O
acts	O
as	O
the	O
reference	O
of	O
it	O
.	O
The	O
dataset	O
shared	O
during	O
the	O
shared	O
task	O
was	O
divided	O
into	O
train	O
/	O
validation	O
/	O
testdev	O
/	O
test	O
splits	O
.	O
Train	O
and	O
validation	O
splits	O
were	O
provided	O
to	O
the	O
participants	O
to	O
facilitate	O
model	O
development	O
.	O
During	O
phase	O
1	O
,	O
the	O
models	O
were	O
evaluated	O
on	O
testdev	O
whereas	O
,	O
the	O
final	O
ranking	O
was	O
done	O
on	O
the	O
performance	O
on	O
the	O
test	O
set	O
.	O
Pre	O
-	O
processing	O
Using	O
the	O
pre	O
-	O
processing	O
scripts	O
provided	O
by	O
the	O
task	O
organizers	O
,	O
we	O
converted	O
the	O
Doc2Dial	B-DatasetName
dataset	O
into	O
SQuAD	B-DatasetName
v2.0	O
format	O
with	O
questions	O
containing	O
the	O
latest	O
user	O
utterance	O
as	O
well	O
as	O
all	O
previous	O
turns	O
in	O
the	O
conversation	O
.	O
This	O
is	O
in	O
line	O
with	O
previous	O
work	O
from	O
(	O
Feng	O
et	O
al	O
,	O
2020	O
)	O
which	O
showed	O
that	O
including	O
the	O
entire	O
conversational	O
history	O
performs	O
better	O
than	O
just	O
considering	O
the	O
current	O
user	O
utterance	O
.	O
Dialogue	O
context	O
is	O
concatenated	O
with	O
the	O
latest	O
user	O
utterance	O
in	O
the	O
reverse	O
time	O
order	O
.	O
The	O
output	O
of	O
this	O
pre	O
-	O
processing	O
step	O
consisted	O
of	O
20431	O
training	O
,	O
3972	O
validation	O
,	O
727	O
testdev	O
,	O
and	O
2824	O
test	O
instances	O
.	O

Recent	O
work	O
(	O
Gururangan	O
et	O
al	O
,	O
2020	O
)	O
has	O
shown	O
that	O
multi	O
-	O
phase	O
domain	O
adaptive	O
pretraining	O
of	O
transformer	O
-	O
based	O
encoders	O
on	O
related	O
datasets	O
(	O
and	O
tasks	O
)	O
benefits	O
the	O
overall	O
performance	O
of	O
the	O
model	O
on	O
the	O
downstream	O
task	O
.	O
Motivated	O
by	O
this	O
,	O
we	O
experimented	O
with	O
further	O
pretraining	O
the	O
QA	O
model	O
on	O
different	O
out	O
-	O
of	O
-	O
domain	O
QA	O
datasets	O
to	O
gauge	O
its	O
benefits	O
on	O
Doc2Dial	B-DatasetName
(	O
Table	O
1	O
)	O
.	O

Our	O
first	O
set	O
of	O
results	O
portray	O
the	O
differential	O
benefits	O
of	O
different	O
out	O
-	O
of	O
-	O
domain	O
QA	O
datasets	O
when	O
used	O
to	O
pretrain	O
the	O
transformer	O
encoder	O
.	O
Experiments	O
with	O
bert	O
-	O
base	O
-	O
uncased	O
on	O
the	O
validation	O
set	O
(	O
Table	O
2	O
)	O
portray	O
that	O
pretraining	O
on	O
different	O
QA	O
datasets	O
is	O
indeed	O
beneficial	O
.	O
Datasets	O
like	O
SQuAD	B-DatasetName
,	O
NewsQA	B-DatasetName
,	O
and	O
NaturalQuestions	O
are	O
more	O
useful	O
than	O
SearchQA	B-DatasetName
,	O
and	O
Trivi	O
-	O
aQA	O
.	O
However	O
,	O
pretraining	O
on	O
complete	O
MRQA	B-DatasetName
-	O
2019	O
training	O
set	O
does	O
not	O
outperform	O
the	O
individual	O
datasets	O
suggesting	O
that	O
merely	O
introducing	O
more	O
pretraining	O
data	O
might	O
not	O
result	O
in	O
improved	O
performance	O
.	O
Furthermore	O
,	O
conversational	O
QA	O
datasets	O
like	O
CoQA	B-DatasetName
and	O
QuAC	B-DatasetName
,	O
which	O
are	O
more	O
similar	O
in	O
their	O
setup	O
to	O
DialDoc	O
,	O
perform	O
substantially	O
better	O
than	O
any	O
of	O
the	O
other	O
MRQA	B-DatasetName
-	O
2019	O
training	O
datasets	O
.	O
We	O
observe	O
similar	O
trends	O
with	O
larger	O
transformers	O
(	O
Table	O
3	O
)	O
.	O
Models	O
pretrained	O
on	O
QuAC	B-DatasetName
or	O
CoQA	B-DatasetName
outperform	O
those	O
pretrained	O
on	O
SQuAD	B-DatasetName
.	O
However	O
,	O
combining	O
CoQA	B-DatasetName
and	O
QuAC	B-DatasetName
during	O
pretraining	O
does	O
not	O
seem	O
to	O
help	O
with	O
the	O
performance	O
on	O
validation	O
or	O
testdev	O
split	O
.	O
Analyzing	O
Different	O
Transformer	B-MethodName
Variants	O
Table	O
3	O
also	O
contains	O
the	O
results	O
for	O
experiments	O
where	O
albert	O
-	O
xl	O
is	O
used	O
to	O
encode	O
the	O
questioncontext	O
pair	O
.	O
We	O
find	O
that	O
albert	O
-	O
xl	O
-	O
based	O
models	O
outperform	O
their	O
bert	O
counterparts	O
on	O
validation	O
set	O
.	O
However	O
,	O
they	O
do	O
not	O
generalize	O
well	O
to	O
the	O
Testdev	O
set	O
,	O
which	O
contains	O
about	O
30	O
%	O
of	O
the	O
test	O
instances	O
but	O
is	O
much	O
smaller	O
than	O
validation	O
set	O
in	O
size	O
(	O
727	O
samples	O
in	O
testdev	O
vs	O
3972	O
in	O
validation	O
set	O
)	O
.	O

In	O
this	O
work	O
,	O
we	O
tackle	O
the	O
task	O
of	O
question	B-TaskName
answering	I-TaskName
(	O
QA	O
)	O
for	O
English	O
language	O
text	O
.	O
While	O
we	O
believe	O
that	O
the	O
proposed	O
methods	O
can	O
be	O
effective	O
in	O
other	O
languages	O
,	O
we	O
leave	O
this	O
exploration	O
for	O
future	O
work	O
.	O
We	O
also	O
acknowledge	O
that	O
QA	O
systems	O
suffer	O
from	O
bias	O
(	O
Li	O
et	O
al	O
,	O
2020	O
)	O
,	O
which	O
often	O
lead	O
to	O
unintended	O
real	O
-	O
world	O
consequences	O
.	O
For	O
the	O
purpose	O
of	O
the	O
shared	O
task	O
,	O
we	O
focused	O
solely	O
on	O
the	O
modeling	O
techniques	O
,	O
but	O
a	O
study	O
of	O
model	O
bias	O
in	O
our	O
systems	O
is	O
necessary	O
.	O

P	O
-	O
Stance	O
:	O
A	O
Large	O
Dataset	O
for	O
Stance	B-TaskName
Detection	I-TaskName
in	O
Political	O
Domain	O

The	O
most	O
common	O
stance	B-TaskName
detection	I-TaskName
task	O
on	O
social	O
media	O
is	O
target	O
-	O
specific	O
stance	B-TaskName
detection	I-TaskName
(	O
ALDayel	O
and	O
Magdy	O
,	O
2021	O
)	O
which	O
aims	O
to	O
identify	O
the	O
stance	O
toward	O
a	O
set	O
of	O
figures	O
or	O
topics	O
(	O
Hasan	O
and	O
Ng	O
,	O
2014	O
;	O
Mohammad	O
et	O
al	O
,	O
2016a	O
;	O
Xu	O
et	O
al	O
,	O
2016	O
;	O
Taulé	O
et	O
al	O
,	O
2017	O
;	O
Swami	O
et	O
al	O
,	O
2018	O
;	O
Zotova	O
et	O
al	O
,	O
2020	O
;	O
Conforti	O
et	O
al	O
,	O
2020b	O
;	O
Lai	O
et	O
al	O
,	O
2020	O
;	O
Vamvas	O
and	O
Sennrich	O
,	O
2020	O
;	O
Conforti	O
et	O
al	O
,	O
2020a	O
)	O
.	O
Besides	O
target	O
-	O
specific	O
stance	B-TaskName
detection	I-TaskName
,	O
multi	O
-	O
target	O
stance	B-TaskName
detection	I-TaskName
Darwish	O
et	O
al	O
,	O
2017	O
;	O
Li	O
and	O
Caragea	O
,	O
2021a	O
)	O
,	O
and	O
claimbased	O
stance	B-TaskName
detection	I-TaskName
(	O
Qazvinian	O
et	O
al	O
,	O
2011	O
;	O
Derczynski	B-DatasetName
et	O
al	O
,	O
2015	O
;	O
Ferreira	O
and	O
Vlachos	O
,	O
2016	O
;	O
Bar	O
-	O
Haim	O
et	O
al	O
,	O
2017	O
;	O
Rao	O
and	O
Pomerleau	O
,	O
2017	O
;	O
Derczynski	B-DatasetName
et	O
al	O
,	O
2017	O
;	O
Gorrell	O
et	O
al	O
,	O
2019	O
)	O
are	O
other	O
popular	O
trends	O
of	O
stance	B-TaskName
detection	I-TaskName
.	O
Multitarget	O
stance	B-TaskName
detection	I-TaskName
aims	O
to	O
jointly	O
identify	O
the	O
stance	O
toward	O
two	O
or	O
more	O
targets	O
in	O
the	O
same	O
text	O
.	O
Unlike	O
the	O
target	O
-	O
specific	O
stance	B-TaskName
detection	I-TaskName
and	O
multi	O
-	O
target	O
stance	B-TaskName
detection	I-TaskName
where	O
the	O
target	O
is	O
usually	O
a	O
prominent	O
figure	O
or	O
topic	O
,	O
in	O
claimbased	O
stance	B-TaskName
detection	I-TaskName
the	O
target	O
is	O
a	O
claim	O
,	O
which	O
could	O
be	O
an	O
article	O
headline	O
or	O
a	O
rumor	O
's	O
post	O
.	O
Interestingly	O
,	O
despite	O
substantial	O
progress	O
on	O
stance	B-TaskName
detection	I-TaskName
,	O
large	O
-	O
scale	O
annotated	O
datasets	O
are	O
limited	O
.	O
We	O
compare	O
our	O
P	O
-	O
STANCE	O
dataset	O
with	O
some	O
existing	O
stance	B-TaskName
detection	I-TaskName
datasets	O
in	O
Table	O
2	O
.	O
We	O
can	O
observe	O
that	O
the	O
sizes	O
of	O
existing	O
stance	B-TaskName
detection	I-TaskName
datasets	O
are	O
smaller	O
than	O
ours	O
except	O
for	O
the	O
WT	B-DatasetName
-	I-DatasetName
WT	I-DatasetName
dataset	O
(	O
Conforti	O
et	O
al	O
,	O
2020b	O
)	O
in	O
the	O
financial	O
domain	O
.	O
However	O
,	O
the	O
average	O
tweet	O
length	O
of	O
WT	B-DatasetName
-	I-DatasetName
WT	I-DatasetName
is	O
much	O
shorter	O
when	O
compared	O
with	O
our	O
P	O
-	O
STANCE	O
.	O
Moreover	O
,	O
more	O
explicit	O
mentions	O
of	O
targets	O
and	O
lexical	O
cues	O
of	O
stance	O
appear	O
in	O
the	O
sentences	O
of	O
WT	B-DatasetName
-	I-DatasetName
WT	I-DatasetName
dataset	O
.	O
In	O
our	O
work	O
,	O
we	O
focus	O
on	O
the	O
political	O
domain	O
and	O
our	O
P	O
-	O
STANCE	O
,	O
which	O
contains	O
much	O
longer	O
sentences	O
and	O
less	O
surfacelevel	O
lexical	O
cues	O
,	O
can	O
serve	O
as	O
a	O
new	O
challenging	O
benchmark	O
for	O
stance	B-TaskName
detection	I-TaskName
tasks	O
.	O
Different	O
from	O
classifying	O
the	O
stance	B-TaskName
detection	I-TaskName
tasks	O
by	O
target	O
type	O
(	O
i.e.	O
,	O
one	O
specific	O
target	O
,	O
multiple	O
targets	O
,	O
or	O
a	O
claim	O
)	O
,	O
we	O
can	O
also	O
categorize	O
the	O
stance	B-TaskName
detection	I-TaskName
as	O
in	O
-	O
target	O
and	O
cross	O
-	O
target	O
stance	B-TaskName
detection	I-TaskName
by	O
the	O
training	O
setting	O
.	O
Most	O
previous	O
works	O
focused	O
on	O
the	O
in	O
-	O
target	O
stance	B-TaskName
detection	I-TaskName
where	O
a	O
classifier	O
is	O
trained	O
and	O
validated	O
on	O
the	O
same	O
target	O
(	O
Mohammad	O
et	O
al	O
,	O
2016b	O
;	O
Zarrella	O
and	O
Marsh	O
,	O
2016	O
;	O
Wei	O
et	O
al	O
,	O
2016	O
;	O
Vijayaraghavan	O
et	O
al	O
,	O
2016	O
;	O
Du	O
et	O
al	O
,	O
2017	O
;	O
Sun	O
et	O
al	O
,	O
2018	O
;	O
Wei	O
et	O
al	O
,	O
2018	O
;	O
Caragea	O
,	O
2019	O
,	O
2021b	O
)	O
.	O
However	O
,	O
sufficient	O
annotated	O
data	O
are	O
usually	O
hard	O
to	O
obtain	O
and	O
conventional	O
models	O
on	O
stance	B-TaskName
detection	I-TaskName
perform	O
poorly	O
on	O
generalizing	O
to	O
the	O
data	O
of	O
new	O
targets	O
,	O
which	O
motivates	O
the	O
studies	O
of	O
cross	O
-	O
target	O
stance	B-TaskName
detection	I-TaskName
(	O
Augenstein	O
et	O
al	O
,	O
2016	O
;	O
Xu	O
et	O
al	O
,	O
2018	O
;	O
Wei	O
and	O
Mao	O
,	O
2019	O
;	O
Zhang	O
et	O
al	O
,	O
2020	O
)	O
.	O
Most	O
previous	O
studies	O
evaluated	O
the	O
cross	O
-	O
target	O
models	O
on	O
the	O
SemEval	O
-	O
2016	O
dataset	O
(	O
Mohammad	O
et	O
al	O
,	O
2016a	O
)	O
,	O
which	O
is	O
a	O
small	O
dataset	O
and	O
thus	O
may	O
make	O
the	O
conclusions	O
less	O
convincing	O
.	O
In	O
this	O
paper	O
,	O
we	O
show	O
that	O
our	O
P	O
-	O
STANCE	O
dataset	O
can	O
be	O
also	O
used	O
to	O
evaluate	O
the	O
model	O
performance	O
of	O
cross	O
-	O
target	O
stance	B-TaskName
detection	I-TaskName
and	O
provides	O
opportunities	O
for	O
exploring	O
more	O
crosstarget	O
tasks	O
by	O
interacting	O
with	O
previous	O
SemEval	O
-	O
2016	O
(	O
Mohammad	O
et	O
al	O
,	O
2016a	O
)	O
and	O
Multi	O
-	O
Target	O
stance	O
datasets	O
.	O
In	O
addition	O
,	O
P	O
-	O
STANCE	O
enables	O
the	O
exploration	O
of	O
largescale	O
deep	O
learning	O
models	O
including	O
pre	O
-	O
trained	O
language	O
models	O
,	O
e.g.	O
,	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
and	O
BERTweet	O
(	O
Nguyen	O
et	O
al	O
,	O
2020	O
)	O
.	O
We	O
fine	O
-	O
tune	O
the	O
BERT	B-MethodName
and	O
BERTweet	O
models	O
on	O
our	O
dataset	O
and	O
compare	O
them	O
with	O
other	O
strong	O
baselines	O
.	O

In	O
this	O
section	O
,	O
we	O
detail	O
the	O
creation	O
and	O
the	O
particularities	O
of	O
P	O
-	O
STANCE	O
,	O
our	O
large	O
political	O
stance	B-TaskName
detection	I-TaskName
dataset	O
composed	O
of	O
21	O
,	O
574	O
tweets	O
collected	O
during	O
the	O
2020	O
U.S.	O
presidential	O
election	O
.	O

We	O
collected	O
tweets	O
using	O
the	O
Twitter	O
streaming	O
API	O
.	O
Similar	O
to	O
prior	O
works	O
(	O
Mohammad	O
et	O
al	O
,	O
2016a	O
;	O
)	O
that	O
target	O
presidential	O
candidates	O
,	O
we	O
focus	O
our	O
attention	O
on	O
three	O
political	O
figures	O
2	O
in	O
the	O
presidential	O
race	O
of	O
2020	O
:	O
"	O
Donald	O
Trump	O
,	O
"	O
"	O
Joe	O
Biden	O
,	O
"	O
and	O
"	O
Bernie	O
Sanders	O
.	O
"	O
We	O
used	O
a	O
set	O
of	O
query	O
hashtags	O
as	O
seeds	B-DatasetName
to	O
collect	O
target	O
-	O
related	O
tweets	O
,	O
which	O
can	O
be	O
categorized	O
as	O
favor	O
hashtags	O
,	O
against	O
hashtags	O
and	O
neutral	O
hashtags	O
(	O
Mohammad	O
et	O
al	O
,	O
2016a	O
)	O
.	O
We	O
show	O
examples	O
of	O
these	O
query	O
hashtags	O
in	O
Table	O
3	O
.	O
In	O
total	O
,	O
we	O
gathered	O
around	O
2.8	O
million	O
tweets	O
for	O
all	O
three	O
targets	O
combined	O
.	O

To	O
ensure	O
the	O
quality	O
of	O
this	O
dataset	O
,	O
we	O
performed	O
several	O
preprocessing	O
steps	O
:	O
1	O
)	O
We	O
removed	O
tweets	O
with	O
less	O
than	O
10	O
,	O
or	O
more	O
than	O
128	O
words	O
.	O
According	O
to	O
our	O
observations	O
,	O
tweets	O
with	O
less	O
than	O
10	O
words	O
are	O
either	O
too	O
easy	O
for	O
detecting	O
the	O
stance	O
or	O
too	O
noisy	O
,	O
and	O
tweets	O
with	O
more	O
than	O
128	O
words	O
usually	O
contain	O
duplicate	O
expressions	O
.	O
2	O
)	O
We	O
removed	O
duplicates	O
and	O
retweets	O
.	O
Twitter	O
data	O
are	O
noisy	O
not	O
only	O
due	O
to	O
the	O
creative	O
spellings	O
,	O
slang	O
and	O
URLs	O
,	O
but	O
also	O
because	O
of	O
the	O
duplicate	O
tweets	O
.	O
Since	O
these	O
duplicate	O
data	O
reduce	O
our	O
ability	O
to	O
build	O
reliable	O
models	O
,	O
we	O
need	O
to	O
clean	O
the	O
dataset	O
by	O
removing	O
duplicates	O
.	O
3	O
)	O
We	O
kept	O
only	O
the	O
tweets	O
in	O
English	O
because	O
our	O
goal	O
in	O
this	O
work	O
is	O
to	O
build	O
an	O
English	O
stance	B-TaskName
detection	I-TaskName
dataset	O
.	O
We	O
leave	O
multilingual	O
stance	B-TaskName
detection	I-TaskName
as	O
future	O
work	O
.	O
After	O
data	O
preprocessing	O
,	O
the	O
size	O
of	O
our	O
corpus	O
reduces	O
to	O
around	O
2	O
million	O
examples	O
.	O
In	O
Table	O
4	O
,	O
we	O
show	O
the	O
number	O
of	O
tweets	O
before	O
and	O
after	O
preprocessing	O
for	O
each	O
political	O
figure	O
.	O
We	O
will	O
provide	O
this	O
large	O
-	O
scale	O
repository	O
of	O
tweets	O
(	O
which	O
we	O
call	O
P	O
-	O
STANCE	O
-	O
EXT	O
)	O
alongside	O
P	O
-	O
STANCE	O
,	O
in	O
hope	O
that	O
it	O
will	O
spur	O
further	O
research	O
in	O
the	O
field	O
of	O
semisupervised	O
learning	O
for	O
stance	B-TaskName
detection	I-TaskName
.	O
Finally	O
,	O
we	O
sampled	O
10	O
,	O
000	O
tweets	O
for	O
each	O
political	O
figure	O
,	O
obtaining	O
30	O
,	O
000	O
tweets	O
for	O
annotation	O
in	O
total	O
.	O

Stance	O
-	O
exposing	O
hashtags	O
that	O
may	O
expose	O
the	O
stance	O
directly	O
,	O
e.g.	O
,	O
#	O
NeverBernie	O
,	O
can	O
be	O
observed	O
in	O
the	O
data	O
.	O
A	O
model	O
can	O
detect	O
the	O
stance	O
from	O
these	O
hashtags	O
without	O
extracting	O
effective	O
representations	O
for	O
the	O
meanings	O
of	O
sentences	O
,	O
which	O
makes	O
stance	B-TaskName
detection	I-TaskName
easier	O
.	O
To	O
remove	O
the	O
stance	O
-	O
exposing	O
hashtags	O
and	O
ensure	O
the	O
data	O
quality	O
,	O
we	O
performed	O
the	O
following	O
steps	O
after	O
the	O
data	O
annotation	O
:	O
1	O
)	O
We	O
manually	O
built	O
a	O
hashtag	O
lexicon	O
that	O
contains	O
stance	O
-	O
exposing	O
hashtags	O
for	O
each	O
target	O
.	O
Then	O
we	O
removed	O
all	O
hashtags	O
that	O
are	O
appended	O
at	O
the	O
end	O
of	O
a	O
sentence	O
if	O
they	O
are	O
in	O
the	O
hashtag	O
lexicon	O
.	O
The	O
reason	O
of	O
only	O
removing	O
the	O
appended	O
hashtags	O
is	O
that	O
a	O
hashtag	O
may	O
serve	O
as	O
a	O
constituent	O
of	O
a	O
sentence	O
,	O
so	O
it	O
would	O
introduce	O
more	O
noise	O
if	O
we	O
simply	O
remove	O
all	O
stance	O
-	O
exposing	O
hashtags	O
.	O
2	O
)	O
To	O
address	O
the	O
stance	O
-	O
exposing	O
hashtag	O
that	O
is	O
a	O
constituent	O
of	O
a	O
sentence	O
,	O
we	O
replaced	O
stance	O
-	O
exposing	O
hashtags	O
that	O
contain	O
the	O
target	O
name	O
with	O
a	O
neutral	O
hashtag	O
,	O
e.g.	O
,	O
#	O
NeverBernie	O
#	O
Bernie	O
.	O
These	O
steps	O
ensure	O
the	O
high	O
quality	O
of	O
our	O
P	O
-	O
STANCE	O
dataset	O
.	O
In	O
addition	O
,	O
P	O
-	O
STANCE	O
is	O
a	O
challenging	O
dataset	O
for	O
the	O
following	O
reasons	O
:	O
1	O
)	O
Targets	O
in	O
P	O
-	O
STANCE	O
are	O
referred	O
to	O
in	O
a	O
more	O
implicit	O
way	O
.	O
Consider	O
the	O
second	O
example	O
in	O
Table	O
1	O
,	O
the	O
target	O
name	O
only	O
appears	O
at	O
the	O
end	O
of	O
the	O
sentence	O
and	O
it	O
is	O
hard	O
to	O
correctly	O
identify	O
the	O
stance	O
without	O
any	O
knowledge	O
about	O
the	O
political	O
figures	O
mentioned	O
in	O
the	O
content	O
and	O
background	O
immigration	O
policy	O
.	O
Similarly	O
,	O
for	O
the	O
third	O
example	O
,	O
it	O
is	O
difficult	O
to	O
correctly	O
identify	O
the	O
stance	O
if	O
the	O
classifier	O
fails	O
to	O
connect	O
the	O
target	O
with	O
relevant	O
events	O
,	O
i.e.	O
,	O
climate	O
change	O
or	O
medicare	O
for	O
all	O
residents	O
.	O
2	O
)	O
The	O
average	O
length	O
of	O
tweets	O
in	O
previous	O
datasets	O
is	O
short	O
,	O
and	O
there	O
are	O
more	O
explicit	O
mentions	O
of	O
targets	O
and	O
rich	O
sentiment	O
and	O
emotion	B-DatasetName
words	O
that	O
can	O
easily	O
reveal	O
the	O
stance	O
toward	O
the	O
target	O
.	O
The	O
average	O
tweet	O
length	O
is	O
17	O
in	O
Mohammad	O
et	O
al	O
(	O
2016a	O
)	O
,	O
21	O
in	O
and	O
16	O
in	O
Conforti	O
et	O
al	O
(	O
2020b	O
)	O
.	O
However	O
,	O
our	O
P	O
-	O
STANCE	O
has	O
a	O
much	O
longer	O
average	O
length	O
of	O
30	O
and	O
more	O
implicit	O
mentions	O
of	O
targets	O
and	O
context	O
words	O
,	O
which	O
indicates	O
that	O
our	O
dataset	O
is	O
more	O
difficult	O
.	O
In	O
addition	O
,	O
P	O
-	O
STANCE	O
covers	O
more	O
target	O
-	O
relevant	O
events	O
.	O
These	O
characteristics	O
contribute	O
to	O
making	O
P	O
-	O
STANCE	O
a	O
challenging	O
dataset	O
for	O
stance	B-TaskName
detection	I-TaskName
.	O

In	O
this	O
section	O
,	O
we	O
first	O
introduce	O
two	O
benchmark	O
datasets	O
of	O
stance	B-TaskName
detection	I-TaskName
in	O
4.1	O
.	O
The	O
union	O
of	O
these	O
datasets	O
and	O
our	O
P	O
-	O
STANCE	O
dataset	O
provides	O
opportunities	O
for	O
studying	O
the	O
cross	O
-	O
target	O
stance	B-TaskName
detection	I-TaskName
(	O
5.2	O
)	O
and	O
cross	O
-	O
topic	O
stance	B-TaskName
detection	I-TaskName
(	O
5.3	O
)	O
.	O
Then	O
we	O
discuss	O
the	O
evaluation	O
metrics	O
in	O
4.2	O
and	O
introduce	O
the	O
baseline	O
methods	O
in	O
4.3	O
.	O

SemEval	O
-	O
2016	O
(	O
Mohammad	O
et	O
al	O
,	O
2016a	O
)	O
and	O
Multi	O
-	O
Target	O
stance	O
datasets	O
are	O
two	O
benchmark	O
datasets	O
in	O
which	O
political	O
figures	O
are	O
chosen	O
as	O
the	O
targets	O
.	O
SemEval	O
-	O
2016	O
contains	O
six	O
targets	O
:	O
"	O
Atheism	O
,	O
"	O
"	O
Climate	O
Change	O
is	O
a	O
Real	O
Concern	O
,	O
"	O
"	O
Feminist	O
Movement	O
,	O
"	O
"	O
Hillary	O
Clinton	O
,	O
"	O
"	O
Legalization	O
of	O
Abortion	O
,	O
"	O
and	O
"	O
Donald	O
Trump	O
.	O
"	O
The	O
dataset	O
is	O
annotated	O
for	O
detecting	O
the	O
stance	O
toward	O
a	O
given	O
target	O
.	O
The	O
data	O
distribution	O
of	O
SemEval	O
-	O
2016	O
is	O
shown	O
in	O
Table	O
7	O
.	O
Multi	O
-	O
Target	O
stance	O
dataset	O
contains	O
three	O
sets	O
of	O
tweets	O
corresponding	O
to	O
three	O
target	O
pairs	O
:	O
"	O
Donald	O
Trump	O
and	O
Hillary	O
Clinton	O
,	O
"	O
"	O
Donald	O
Trump	O
and	O
Ted	O
Cruz	O
,	O
"	O
"	O
Hillary	O
Clinton	O
and	O
Bernie	O
Sanders	O
"	O
for	O
2016	O
U.S.	O
presidential	O
election	O
.	O
The	O
task	O
aims	O
at	O
detecting	O
the	O
stances	O
toward	O
two	O
targets	O
for	O
each	O
data	O
.	O
The	O
data	O
distribution	O
of	O
Multi	O
-	O
Target	O
stance	O
dataset	O
is	O
shown	O
in	O
Table	O
8	O
.	O
In	O
the	O
next	O
section	O
,	O
we	O
show	O
how	O
to	O
perform	O
various	O
stance	B-TaskName
detection	I-TaskName
tasks	O
with	O
the	O
union	O
of	O
these	O
datasets	O
and	O
our	O
P	O
-	O
STANCE	O
dataset	O
.	O

In	O
this	O
section	O
,	O
we	O
present	O
the	O
set	O
of	O
experiments	O
performed	O
on	O
various	O
stance	B-TaskName
detection	I-TaskName
tasks	O
on	O
our	O
dataset	O
and	O
show	O
the	O
results	O
obtained	O
by	O
using	O
the	O
aforementioned	O
baselines	O
.	O
Each	O
result	O
is	O
the	O
average	O
of	O
seven	O
runs	O
with	O
different	O
initializations	O
.	O

In	O
-	O
target	O
stance	B-TaskName
detection	I-TaskName
is	O
a	O
stance	B-TaskName
detection	I-TaskName
task	O
where	O
a	O
classifier	O
is	O
trained	O
and	O
validated	O
on	O
the	O
same	O
target	O
.	O
Most	O
previous	O
works	O
adopt	O
an	O
"	O
Adhoc	O
"	O
training	O
strategy	O
by	O
training	O
one	O
model	O
for	O
each	O
target	O
and	O
evaluate	O
it	O
on	O
the	O
test	O
set	O
of	O
that	O
target	O
(	O
i.e.	O
,	O
we	O
train	O
three	O
different	O
models	O
if	O
there	O
are	O
three	O
targets	O
in	O
the	O
dataset	O
)	O
.	O
However	O
,	O
the	O
model	O
is	O
more	O
likely	O
to	O
predict	O
the	O
stance	O
by	O
following	O
specific	O
patterns	O
without	O
fully	O
considering	O
the	O
target	O
information	O
and	O
overfit	O
.	O
Therefore	O
,	O
to	O
better	O
evaluate	O
the	O
performance	O
of	O
baselines	O
,	O
we	O
propose	O
a	O
"	O
Merged	O
"	O
training	O
strategy	O
by	O
training	O
and	O
validating	O
a	O
model	O
on	O
all	O
targets	O
and	O
testing	O
it	O
on	O
separate	O
targets	O
to	O
be	O
compared	O
with	O
the	O
"	O
Ad	O
-	O
hoc	O
"	O
setting	O
.	O
Experimental	O
results	O
of	O
these	O
two	O
different	O
settings	O
are	O
shown	O
in	O
Table	O
9	O
.	O
First	O
,	O
we	O
can	O
observe	O
that	O
BERTweet	O
performs	O
best	O
in	O
both	O
settings	O
and	O
significantly	O
outperforms	O
the	O
second	O
best	O
results	O
,	O
demonstrating	O
the	O
effectiveness	O
of	O
this	O
model	O
.	O
Second	O
,	O
performance	O
drops	O
can	O
be	O
observed	O
on	O
all	O
models	O
in	O
the	O
"	O
Merged	O
"	O
setting	O
and	O
models	O
(	O
BiL	O
-	O
STM	O
and	O
CNN	O
)	O
that	O
do	O
not	O
consider	O
target	O
information	O
suffer	O
the	O
most	O
severe	O
drops	O
,	O
which	O
means	O
our	O
proposed	O
training	O
strategy	O
can	O
serve	O
as	O
a	O
better	O
evaluation	O
method	O
to	O
test	O
whether	O
the	O
model	O
learns	O
target	O
-	O
specific	O
representations	O
.	O
Moreover	O
,	O
we	O
can	O
observe	O
that	O
both	O
BERTweet	O
and	O
BERT	B-MethodName
perform	O
well	O
and	O
have	O
the	O
minimum	O
performance	O
drops	O
compared	O
with	O
the	O
other	O
baselines	O
,	O
which	O
demonstrates	O
that	O
self	O
-	O
attention	O
mechanism	O
can	O
better	O
capture	O
target	O
-	O
specific	O
representations	O
.	O

Despite	O
substantial	O
progress	O
on	O
the	O
stance	B-TaskName
detection	I-TaskName
,	O
sufficient	O
annotated	O
data	O
are	O
usually	O
hard	O
to	O
obtain	O
and	O
conventional	O
models	O
on	O
stance	B-TaskName
detection	I-TaskName
perform	O
poorly	O
on	O
generalizing	O
to	O
the	O
data	O
of	O
new	O
targets	O
,	O
which	O
motivates	O
the	O
studies	O
of	O
crosstarget	O
stance	B-TaskName
detection	I-TaskName
.	O
The	O
model	O
of	O
cross	O
-	O
target	O
stance	B-TaskName
detection	I-TaskName
is	O
first	O
trained	O
and	O
validated	O
on	O
a	O
source	O
target	O
,	O
and	O
then	O
tested	O
on	O
a	O
destination	O
target	O
.	O
In	O
this	O
subsection	O
,	O
we	O
show	O
that	O
our	O
P	O
-	O
STANCE	O
dataset	O
can	O
be	O
also	O
used	O
to	O
evaluate	O
the	O
model	O
performance	O
of	O
cross	O
-	O
target	O
stance	B-TaskName
detection	I-TaskName
and	O
provides	O
opportunities	O
for	O
exploring	O
more	O
cross	O
-	O
target	O
tasks	O
by	O
interacting	O
with	O
previous	O
SemEval	O
-	O
2016	O
and	O
Multi	O
-	O
Target	O
stance	O
datasets	O
.	O
We	O
use	O
five	O
targets	O
for	O
our	O
experiments	O
:	O
"	O
Donald	O
Trump	O
"	O
(	O
DT	O
)	O
,	O
"	O
Joe	O
Biden	O
"	O
(	O
JB	O
)	O
,	O
"	O
Bernie	O
Sanders	O
"	O
(	O
BS	O
)	O
,	O
"	O
Hillary	O
Clinton	O
"	O
(	O
HC	O
)	O
,	O
and	O
"	O
Ted	O
Cruz	O
"	O
(	O
TC	O
)	O
.	O
Experimental	O
results	O
of	O
cross	O
-	O
target	O
stance	B-TaskName
detection	I-TaskName
are	O
shown	O
in	O
Table	O
10	O
.	O
For	O
the	O
first	O
half	O
of	O
Table	O
10	O
,	O
only	O
targets	O
of	O
P	O
-	O
STANCE	O
dataset	O
are	O
used	O
to	O
evaluate	O
the	O
model	O
performance	O
.	O
However	O
,	O
for	O
the	O
second	O
half	O
,	O
targets	O
of	O
SemEval	O
-	O
2016	O
and	O
Multi	O
-	O
Target	O
datasets	O
also	O
serve	O
as	O
destination	O
targets	O
,	O
which	O
makes	O
it	O
a	O
more	O
challenging	O
task	O
since	O
the	O
target	O
-	O
related	O
topics	O
in	O
2016	O
are	O
quite	O
different	O
from	O
the	O
ones	O
in	O
2020	O
.	O
More	O
specifically	O
,	O
we	O
train	O
and	O
validate	O
the	O
model	O
on	O
a	O
source	O
target	O
of	O
P	O
-	O
STANCE	O
dataset	O
and	O
test	O
it	O
on	O
the	O
data	O
of	O
a	O
destination	O
target	O
,	O
which	O
is	O
a	O
combination	O
of	O
train	O
,	O
validation	O
,	O
and	O
test	O
sets	O
of	O
previous	O
datasets	O
.	O
Note	O
that	O
we	O
merge	O
the	O
data	O
from	O
SemEval	O
-	O
2016	O
and	O
Multi	O
-	O
Target	O
datasets	O
if	O
these	O
two	O
datasets	O
share	O
the	O
same	O
target	O
,	O
e.g.	O
,	O
Hillary	O
Clinton	O
.	O
For	O
the	O
cross	O
-	O
target	O
tasks	O
only	O
on	O
the	O
P	O
-	O
STANCE	O
dataset	O
,	O
first	O
,	O
we	O
can	O
observe	O
from	O
the	O
Table	O
10	O
that	O
BERTweet	O
achieves	O
the	O
best	O
performance	O
on	O
all	O
target	O
configurations	O
,	O
demonstrating	O
its	O
effectiveness	O
.	O
Moreover	O
,	O
BERTweet	O
shows	O
greater	O
improvement	O
over	O
the	O
best	O
baseline	O
when	O
training	O
on	O
the	O
data	O
of	O
two	O
targets	O
.	O
The	O
reason	O
is	O
that	O
BERTweet	O
learns	O
more	O
universal	O
representations	O
by	O
leveraging	O
the	O
data	O
from	O
two	O
targets	O
.	O
Second	O
,	O
we	O
see	O
that	O
Cross	O
-	O
Net	O
outperforms	O
BiCE	O
on	O
almost	O
all	O
target	O
configurations	O
,	O
which	O
is	O
consistent	O
with	O
the	O
observations	O
of	O
previous	O
studies	O
(	O
Xu	O
et	O
al	O
,	O
2018	O
;	O
Zhang	O
et	O
al	O
,	O
2020	O
)	O
.	O
Third	O
,	O
we	O
find	O
that	O
models	O
achieve	O
better	O
performance	O
on	O
JB	O
BS	O
and	O
BS	O
JB	O
.	O
One	O
potential	O
explanation	O
is	O
that	O
targets	O
"	O
Joe	O
Biden	O
"	O
and	O
"	O
Bernie	O
Sanders	O
"	O
are	O
from	O
the	O
same	O
party	O
and	O
thus	O
share	O
more	O
similar	O
topics	O
.	O
For	O
the	O
second	O
half	O
of	O
Table	O
10	O
,	O
we	O
observe	O
a	O
significant	O
drop	O
in	O
performance	O
on	O
all	O
models	O
,	O
which	O
verifies	O
that	O
it	O
is	O
more	O
challenging	O
to	O
transfer	O
the	O
knowledge	O
to	O
a	O
destination	O
target	O
with	O
more	O
diverse	O
topics	O
in	O
the	O
past	O
.	O
BERTweet	O
still	O
achieves	O
the	O
best	O
performance	O
on	O
almost	O
all	O
target	O
configurations	O
,	O
making	O
it	O
a	O
highly	O
competitive	O
model	O
for	O
crosstarget	O
stance	B-TaskName
detection	I-TaskName
task	O
.	O
Interestingly	O
,	O
we	O
can	O
observe	O
that	O
both	O
BiCE	O
,	O
CrossNet	O
,	O
and	O
BERTweet	O
show	O
better	O
performance	O
on	O
target	O
"	O
Ted	O
Cruz	O
.	O
"	O
A	O
possible	O
reason	O
is	O
that	O
the	O
data	O
of	O
"	O
Ted	O
Cruz	O
"	O
contain	O
more	O
universal	O
expressions	O
and	O
topics	O
.	O

Obtaining	O
sufficient	O
annotated	O
data	O
of	O
specific	O
target	O
from	O
most	O
recent	O
past	O
is	O
challenging	O
.	O
However	O
,	O
sometimes	O
historical	O
annotated	O
data	O
of	O
the	O
same	O
target	O
are	O
available	O
.	O
Therefore	O
,	O
motivated	O
by	O
a	O
desire	O
to	O
improve	O
the	O
models	O
'	O
generalization	O
ability	O
to	O
transfer	O
knowledge	O
from	O
historical	O
data	O
,	O
we	O
come	O
up	O
with	O
a	O
new	O
stance	B-TaskName
detection	I-TaskName
task	O
,	O
named	O
cross	O
-	O
topic	O
stance	B-TaskName
detection	I-TaskName
.	O
Specifically	O
,	O
in	O
this	O
task	O
,	O
the	O
model	O
of	O
cross	O
-	O
topic	O
stance	B-TaskName
detection	I-TaskName
is	O
first	O
trained	O
on	O
the	O
data	O
of	O
a	O
target	O
(	O
e.g.	O
,	O
Donald	O
Trump	O
)	O
in	O
2016	O
,	O
and	O
then	O
validated	O
and	O
tested	O
on	O
the	O
data	O
of	O
the	O
same	O
target	O
in	O
2020	O
.	O
Note	O
that	O
the	O
annotated	O
data	O
of	O
year	O
2016	O
are	O
the	O
same	O
with	O
the	O
data	O
used	O
in	O
5.2	O
.	O
The	O
results	O
are	O
shown	O
in	O
Table	O
11	O
.	O
Since	O
target	O
"	O
Joe	O
Biden	O
"	O
is	O
absent	O
from	O
the	O
previous	O
stance	B-TaskName
detection	I-TaskName
datasets	O
,	O
we	O
use	O
targets	O
"	O
Donald	O
Trump	O
"	O
and	O
"	O
Bernie	O
Sanders	O
"	O
for	O
evaluation	O
.	O
We	O
can	O
observe	O
that	O
BERTweet	O
still	O
performs	O
best	O
on	O
this	O
task	O
and	O
the	O
overall	O
model	O
performance	O
of	O
cross	O
-	O
topic	O
stance	B-TaskName
detection	I-TaskName
is	O
better	O
than	O
that	O
of	O
cross	O
-	O
target	O
stance	B-TaskName
detection	I-TaskName
due	O
to	O
the	O
use	O
of	O
the	O
same	O
target	O
in	O
evaluation	O
stage	O
.	O
Moreover	O
,	O
we	O
see	O
that	O
models	O
perform	O
relatively	O
poorly	O
on	O
target	O
"	O
Bernie	O
Sanders	O
"	O
.	O
One	O
possible	O
explanation	O
is	O
that	O
some	O
topics	O
,	O
e.g.	O
healthcare	O
and	O
climate	O
change	O
,	O
appear	O
rarely	O
in	O
previous	O
datasets	O
.	O

In	O
this	O
paper	O
,	O
we	O
introduced	O
P	O
-	O
STANCE	O
,	O
an	O
English	O
stance	B-TaskName
detection	I-TaskName
dataset	O
in	O
the	O
political	O
domain	O
,	O
which	O
is	O
larger	O
and	O
more	O
challenging	O
compared	O
with	O
previous	O
datasets	O
for	O
stance	B-TaskName
detection	I-TaskName
.	O
Composed	O
of	O
21	O
,	O
574	O
tweets	O
that	O
were	O
collected	O
during	O
the	O
2020	O
USA	O
election	O
,	O
P	O
-	O
STANCE	O
can	O
serve	O
as	O
a	O
new	O
benchmark	O
for	O
stance	B-TaskName
detection	I-TaskName
and	O
enable	O
future	O
research	O
in	O
other	O
stance	B-TaskName
detection	I-TaskName
tasks	O
,	O
e.g.	O
,	O
cross	O
-	O
target	O
stance	B-TaskName
detection	I-TaskName
and	O
cross	O
-	O
topic	O
stance	B-TaskName
detection	I-TaskName
.	O
Experimental	O
results	O
show	O
that	O
the	O
BERTweet	O
model	O
significantly	O
outperforms	O
other	O
strong	O
baselines	O
not	O
only	O
on	O
intarget	O
stance	B-TaskName
detection	I-TaskName
,	O
but	O
also	O
on	O
cross	O
-	O
target	O
and	O
cross	O
-	O
topic	O
stance	B-TaskName
detection	I-TaskName
.	O
Moreover	O
,	O
the	O
performance	O
of	O
BERTweet	O
can	O
be	O
further	O
improved	O
by	O
using	O
semi	O
-	O
supervised	O
learning	O
.	O
Future	O
work	O
includes	O
constructing	O
another	O
large	O
dataset	O
for	O
a	O
more	O
challenging	O
task	O
,	O
i.e.	O
,	O
multi	O
-	O
target	O
stance	B-TaskName
detection	I-TaskName
,	O
and	O
studying	O
the	O
multilingual	O
stance	B-TaskName
detection	I-TaskName
with	O
the	O
union	O
of	O
P	O
-	O
STANCE	O
and	O
other	O
multilingual	O
datasets	O
.	O

An	O
Empirical	O
Study	O
of	O
Incorporating	O
Pseudo	O
Data	O
into	O
Grammatical	B-TaskName
Error	I-TaskName
Correction	I-TaskName

The	O
incorporation	O
of	O
pseudo	O
data	O
in	O
the	O
training	O
of	O
grammatical	B-TaskName
error	I-TaskName
correction	I-TaskName
models	O
has	O
been	O
one	O
of	O
the	O
main	O
factors	O
in	O
improving	O
the	O
performance	O
of	O
such	O
models	O
.	O
However	O
,	O
consensus	O
is	O
lacking	O
on	O
experimental	O
configurations	O
,	O
namely	O
,	O
choosing	O
how	O
the	O
pseudo	O
data	O
should	O
be	O
generated	O
or	O
used	O
.	O
In	O
this	O
study	O
,	O
these	O
choices	O
are	O
investigated	O
through	O
extensive	O
experiments	O
,	O
and	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
is	O
achieved	O
on	O
the	O
CoNLL	O
-	O
2014	O
test	O
set	O
(	O
F	O
0.5	O
=	O
65.0	O
)	O
and	O
the	O
official	O
test	O
set	O
of	O
the	O
BEA	O
-	O
2019	O
shared	O
task	O
(	O
F	O
0.5	O
=	O
70.2	O
)	O
without	O
making	O
any	O
modifications	O
to	O
the	O
model	O
architecture	O
.	O

To	O
date	O
,	O
many	O
studies	O
have	O
tackled	O
grammatical	B-TaskName
error	I-TaskName
correction	I-TaskName
(	O
GEC	O
)	O
as	O
a	O
machine	B-TaskName
translation	I-TaskName
(	O
MT	O
)	O
task	O
,	O
in	O
which	O
ungrammatical	O
sentences	O
are	O
regarded	O
as	O
the	O
source	O
language	O
and	O
grammatical	O
sentences	O
are	O
regarded	O
as	O
the	O
target	O
language	O
.	O
This	O
approach	O
allows	O
cutting	O
-	O
edge	O
neural	O
MT	O
models	O
to	O
be	O
adopted	O
.	O
For	O
example	O
,	O
the	O
encoder	O
-	O
decoder	O
(	O
EncDec	O
)	O
model	O
(	O
Sutskever	O
et	O
al	O
,	O
2014	O
;	O
Bahdanau	O
et	O
al	O
,	O
2015	O
)	O
,	O
which	O
was	O
originally	O
proposed	O
for	O
MT	O
,	O
has	O
been	O
applied	O
widely	O
to	O
GEC	O
and	O
has	O
achieved	O
remarkable	O
results	O
in	O
the	O
GEC	O
research	O
field	O
(	O
Ji	O
et	O
al	O
,	O
2017	O
;	O
Chollampatt	O
and	O
Ng	O
,	O
2018	O
;	O
.	O
However	O
,	O
a	O
challenge	O
in	O
applying	O
EncDec	O
to	O
GEC	O
is	O
that	O
EncDec	O
requires	O
a	O
large	O
amount	O
of	O
training	O
data	O
(	O
Koehn	O
and	O
Knowles	O
,	O
2017	O
)	O
,	O
but	O
the	O
largest	O
set	O
of	O
publicly	O
available	O
parallel	O
data	O
in	O
GEC	O
has	O
only	O
two	O
million	O
sentence	O
pairs	O
(	O
Mizumoto	O
et	O
al	O
,	O
2011	O
)	O
.	O
Consequently	O
,	O
the	O
method	O
of	O
augmenting	O
the	O
data	O
by	O
incorporating	O
pseudo	O
training	O
data	O
has	O
been	O
studied	O
intensively	O
(	O
Xie	O
et	O
al	O
,	O
2018	O
;	O
Ge	O
et	O
al	O
,	O
2018	O
;	O
Lichtarge	O
et	O
al	O
,	O
2019	O
;	O
Zhao	O
et	O
al	O
,	O
2019	O
)	O
.	O
*	O
Current	O
affiliation	O
:	O
Future	O
Corporation	O
When	O
incorporating	O
pseudo	O
data	O
,	O
several	O
decisions	O
must	O
be	O
made	O
about	O
the	O
experimental	O
configurations	O
,	O
namely	O
,	O
(	O
i	O
)	O
the	O
method	O
of	O
generating	O
the	O
pseudo	O
data	O
,	O
(	O
ii	O
)	O
the	O
seed	O
corpus	O
for	O
the	O
pseudo	O
data	O
,	O
and	O
(	O
iii	O
)	O
the	O
optimization	O
setting	O
(	O
Section	O
2	O
)	O
.	O
However	O
,	O
consensus	O
on	O
these	O
decisions	O
in	O
the	O
GEC	O
research	O
field	O
is	O
yet	O
to	O
be	O
formulated	O
.	O
For	O
example	O
,	O
Xie	O
et	O
al	O
(	O
2018	O
)	O
found	O
that	O
a	O
variant	O
of	O
the	O
backtranslation	O
(	O
Sennrich	O
et	O
al	O
,	O
2016b	O
)	O
method	O
(	O
BACKTRANS	O
(	O
NOISY	O
)	O
)	O
outperforms	O
the	O
generation	O
of	O
pseudo	O
data	O
from	O
raw	O
grammatical	O
sentences	O
(	O
DIRECTNOISE	O
)	O
.	O
By	O
contrast	O
,	O
the	O
current	O
state	O
of	O
the	O
art	O
model	O
(	O
Zhao	O
et	O
al	O
,	O
2019	O
)	O
uses	O
the	O
DIRECTNOISE	O
method	O
.	O
In	O
this	O
study	O
,	O
we	O
investigate	O
these	O
decisions	O
regarding	O
pseudo	O
data	O
,	O
our	O
goal	O
being	O
to	O
provide	O
the	O
research	O
community	O
with	O
an	O
improved	O
understanding	O
of	O
the	O
incorporation	O
of	O
pseudo	O
data	O
.	O
Through	O
extensive	O
experiments	O
,	O
we	O
determine	O
suitable	O
settings	O
for	O
GEC	O
.	O
We	O
justify	O
the	O
reliability	O
of	O
the	O
proposed	O
settings	O
by	O
demonstrating	O
their	O
strong	O
performance	O
on	O
benchmark	O
datasets	O
.	O
Specifically	O
,	O
without	O
any	O
task	O
-	O
specific	O
techniques	O
or	O
architecture	O
,	O
our	O
model	O
outperforms	O
not	O
only	O
all	O
previous	O
single	O
-	O
model	O
results	O
but	O
also	O
all	O
ensemble	O
results	O
except	O
for	O
the	O
ensemble	O
result	O
by	O
Grundkiewicz	O
et	O
al	O
(	O
2019	O
)	O
1	O
.	O
By	O
applying	O
task	O
-	O
specific	O
techniques	O
,	O
we	O
further	O
improve	O
the	O
performance	O
and	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
the	O
CoNLL	O
-	O
2014	O
test	O
set	O
and	O
the	O
official	O
test	O
set	O
of	O
the	O
BEA	O
-	O
2019	O
shared	O
task	O
.	O

The	O
BEA	O
-	O
2019	O
workshop	O
official	O
dataset	O
4	O
is	O
the	O
origin	O
of	O
the	O
training	O
and	O
validation	O
data	O
of	O
our	O
experiments	O
.	O
Hereinafter	O
,	O
we	O
refer	O
to	O
the	O
training	O
data	O
as	O
BEA	O
-	O
train	O
.	O
We	O
create	O
validation	O
data	O
(	O
BEA	O
-	O
valid	O
)	O
by	O
randomly	O
sampling	O
sentence	O
pairs	O
from	O
the	O
official	O
validation	O
split	O
5	O
.	O
As	O
a	O
seed	O
corpus	O
T	O
,	O
we	O
use	O
SimpleWiki	O
6	O
,	O
Wikipedia	O
7	O
or	O
Gigaword	O
8	O
.	O
We	O
apply	O
the	O
noizing	O
methods	O
described	O
in	O
Section	O
3	O
to	O
each	O
corpus	O
and	O
generate	O
pseudo	O
data	O
D	O
p	O
.	O
The	O
characteristics	O
of	O
each	O
dataset	O
are	O
summarized	O
in	O
Table	O
1	O
.	O
Evaluation	O
We	O
report	O
results	O
on	O
BEA	O
-	O
valid	O
,	O
the	O
official	O
test	O
set	O
of	O
the	O
BEA	O
-	O
2019	O
shared	O
task	O
(	O
BEA	O
-	O
test	O
)	O
,	O
the	O
CoNLL	O
-	O
2014	O
test	O
set	O
(	O
CoNLL	O
-	O
2014	O
)	O
(	O
Ng	O
et	O
al	O
,	O
2014	O
)	O
,	O
and	O
the	O
JFLEG	B-DatasetName
test	O
set	O
(	O
JFLEG	B-DatasetName
)	O
(	O
Napoles	O
et	O
al	O
,	O
2017	O
)	O
.	O
All	O
reported	O
results	O
(	O
except	O
ensemble	O
)	O
are	O
the	O
average	O
of	O
five	O
distinct	O
trials	O
using	O
five	O
different	O
random	O
seeds	B-DatasetName
.	O
We	O
report	O
the	O
scores	O
measured	O
by	O
ERRANT	O
(	O
Bryant	O
et	O
al	O
,	O
2017	O
;	O
Felice	O
et	O
al	O
,	O
2016	O
)	O
for	O
BEA	O
-	O
valid	O
,	O
BEA	O
-	O
test	O
,	O
and	O
CoNLL	O
-	O
2014	O
.	O
As	O
the	O
reference	O
sentences	O
of	O
BEAtest	O
are	O
publicly	O
unavailable	O
,	O
we	O
evaluate	O
the	O
model	O
outputs	O
on	O
CodaLab	O
9	O
for	O
BEA	O
-	O
test	O
.	O
We	O
also	O
report	O
results	O
measured	O
by	O
the	O
M	O
2	O
scorer	O
(	O
Dahlmeier	O
and	O
Ng	O
,	O
2012	O
)	O
on	O
CoNLL	O
-	O
2014	O
to	O
compare	O
them	O
with	O
those	O
of	O
previous	O
studies	O
.	O
We	O
use	O
the	O
GLEU	O
metric	O
(	O
Napoles	O
et	O
al	O
,	O
2015	O
(	O
Napoles	O
et	O
al	O
,	O
,	O
2016	O
for	O
JFLEG	B-DatasetName
.	O
Model	O
We	O
adopt	O
the	O
Transformer	B-MethodName
EncDec	O
model	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
using	O
the	O
fairseq	O
toolkit	O
(	O
Ott	O
et	O
al	O
,	O
2019	O
)	O
and	O
use	O
the	O
"	O
Transformer	B-MethodName
(	O
big	O
)	O
"	O
settings	O
of	O
Vaswani	O
et	O
al	O
(	O
2017	O
)	O
.	O
Optimization	O
For	O
the	O
JOINT	O
setting	O
,	O
we	O
opti	O
-	O
mize	O
the	O
model	O
with	O
Adam	B-MethodName
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
.	O
For	O
the	O
PRETRAIN	O
setting	O
,	O
we	O
pretrain	O
the	O
model	O
with	O
Adam	B-MethodName
and	O
then	O
fine	O
-	O
tune	O
it	O
on	O
BEA	O
-	O
train	O
using	O
Adafactor	B-MethodName
(	O
Shazeer	O
and	O
Stern	O
,	O
2018	O
)	O
10	O
.	O

Adobe	O
AMPS	O
's	O
Submission	O
for	O
Very	O
Low	O
Resource	O
Supervised	O
Translation	B-TaskName
Task	O
at	O
WMT20	O

This	O
paper	O
describes	O
our	O
submissions	O
to	O
the	O
shared	O
task	O
on	O
Very	O
Low	O
Resource	O
Supervised	O
Machine	B-TaskName
Translation	I-TaskName
at	O
WMT	B-DatasetName
2020	I-DatasetName
.	O
The	O
task	O
involved	O
a	O
single	O
language	O
pair	O
:	O
Upper	O
Sorbian	O
-	O
German	O
.	O
We	O
submit	O
supervised	O
neural	O
machine	B-TaskName
translation	I-TaskName
(	O
NMT	O
)	O
systems	O
for	O
both	O
translation	O
directions	O
,	O
Upper	O
Sorbian	O
German	O
and	O
German	O
Upper	O
Sorbian	O
.	O
NMT	O
models	O
(	O
Sutskever	O
et	O
al	O
,	O
2014	O
;	O
Bahdanau	O
et	O
al	O
,	O
2015	O
;	O
Cho	O
et	O
al	O
,	O
2014a	O
)	O
have	O
achieved	O
stateof	O
-	O
the	O
-	O
art	O
performance	O
on	O
benchmark	O
datasets	O
for	O
multiple	O
language	O
pairs	O
.	O
A	O
big	O
advantage	O
of	O
such	O
systems	O
over	O
phrase	O
-	O
based	O
statistical	O
machine	B-TaskName
translation	I-TaskName
(	O
PBSMT	O
)	O
(	O
Koehn	O
et	O
al	O
,	O
2003	O
)	O
models	O
is	O
that	O
they	O
can	O
be	O
trained	O
end	O
-	O
to	O
-	O
end	O
.	O
The	O
bulk	O
of	O
the	O
development	O
,	O
however	O
,	O
has	O
been	O
limited	O
to	O
a	O
handful	O
of	O
high	O
-	O
resource	O
language	O
pairs	O
.	O
The	O
primary	O
reason	O
is	O
that	O
training	O
a	O
well	O
-	O
performing	O
NMT	O
system	O
requires	O
a	O
large	O
amount	O
of	O
parallel	O
training	O
data	O
,	O
which	O
means	O
a	O
lot	O
of	O
equivalent	O
investment	O
in	O
terms	O
of	O
resources	O
.	O
Koehn	O
and	O
Knowles	O
(	O
2017	O
)	O
show	O
that	O
when	O
compared	O
to	O
PBSMT	O
approaches	O
,	O
NMT	O
models	O
need	O
more	O
training	O
data	O
to	O
achieve	O
the	O
same	O
level	O
of	O
performance	O
.	O
1	O
One	O
of	O
the	O
most	O
popular	O
ways	O
to	O
increase	O
the	O
amount	O
of	O
parallel	O
training	O
data	O
for	O
supervised	O
training	O
is	O
backtranslation	O
(	O
Sennrich	O
et	O
al	O
,	O
2016a	O
)	O
.	O
We	O
utilize	O
this	O
approach	O
to	O
improve	O
upon	O
the	O
performance	O
of	O
our	O
baseline	O
models	O
.	O
All	O
of	O
our	O
systems	O
follow	O
the	O
Transformer	B-MethodName
architecture	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
Our	O
primary	O
system	O
is	O
a	O
supervised	O
NMT	O
model	O
trained	O
on	O
the	O
original	O
training	O
bitext	O
.	O
We	O
also	O
report	O
our	O
results	O
on	O
experiments	O
with	O
backtranslation	O
,	O
which	O
were	O
completed	O
post	O
the	O
shared	O
task	O
and	O
hence	O
not	O
a	O
part	O
of	O
our	O
primary	O
submissions	O
.	O
We	O
use	O
the	O
backtranslated	O
data	O
in	O
two	O
distinct	O
ways	O
-	O
as	O
a	O
standalone	O
parallel	O
corpus	O
,	O
and	O
to	O
create	O
a	O
combined	O
parallel	O
corpus	O
by	O
mixing	O
in	O
a	O
1:1	O
ratio	O
with	O
the	O
provided	O
training	O
data	O
.	O
We	O
also	O
report	O
the	O
performance	O
of	O
fine	O
-	O
tuned	O
models	O
originally	O
trained	O
only	O
on	O
the	O
backtranslated	O
data	O
.	O
In	O
the	O
following	O
sections	O
,	O
we	O
begin	O
by	O
briefly	O
describing	O
the	O
Transformer	B-MethodName
architecture	O
and	O
backtranslation	O
.	O
We	O
then	O
discuss	O
our	O
experimental	O
setup	O
as	O
well	O
as	O
our	O
experiments	O
with	O
backtranslation	O
.	O
We	O
conclude	O
with	O
a	O
discussion	O
of	O
our	O
results	O
and	O
possible	O
future	O
work	O
.	O

We	O
used	O
the	O
complete	O
parallel	O
training	O
corpus	O
for	O
our	O
primary	O
systems	O
.	O
In	O
addition	O
,	O
we	O
also	O
made	O
use	O
of	O
monolingual	O
data	O
from	O
each	O
language	O
for	O
2	O
https://github.com/pytorch/fairseq	O
two	O
purposes	O
-	O
learning	O
Byte	O
Pair	O
Encodings	O
(	O
BPE	B-MethodName
)	O
(	O
Sennrich	O
et	O
al	O
,	O
2016b	O
)	O
and	O
backtranslation	O
.	O
For	O
Upper	O
Sorbian	O
(	O
hsb	O
)	O
,	O
we	O
used	O
the	O
monolingual	O
corpora	O
provided	O
by	O
the	O
Sorbian	O
Institute	O
and	O
by	O
the	O
Witaj	O
Sprachzentrum	O
.	O
To	O
control	O
the	O
quality	O
of	O
the	O
backtranslated	O
data	O
,	O
we	O
chose	O
not	O
to	O
use	O
the	O
data	O
scraped	O
from	O
the	O
web	O
.	O
For	O
the	O
German	O
(	O
de	O
)	O
side	O
,	O
we	O
made	O
use	O
of	O
the	O
News	O
Crawl	O
3	O
2009	O
dataset	O
,	O
as	O
it	O
is	O
large	O
enough	O
to	O
satisfy	O
the	O
requirements	O
for	O
our	O
experiments	O
.	O

No	O
.	O
of	O
sentences	O
hsb	O
-	O
de	O
,	O
bitext	O
58	O
,	O
389	O
hsb	O
,	O
monolingual	O
540	O
,	O
994	O
de	O
,	O
monolingual	O
2	O
,	O
000	O
,	O
000	O
Moses	O
toolkit	O
(	O
Koehn	O
et	O
al	O
,	O
2007	O
)	O
was	O
used	O
for	O
tokenization	O
and	O
punctuation	O
normalization	O
for	O
all	O
data	O
.	O
Before	O
doing	O
any	O
additional	O
preprocessing	O
,	O
we	O
learned	O
separate	O
truecaser	O
models	O
using	O
the	O
toolkit	O
.	O
For	O
this	O
purpose	O
,	O
we	O
took	O
first	O
500	O
K	O
sentences	O
from	O
each	O
of	O
the	O
monolingual	O
corpora	O
and	O
aggregated	O
them	O
with	O
the	O
corresponding	O
portion	O
from	O
the	O
training	O
bitext	O
.	O
After	O
tokenizing	O
and	O
truecasing	O
,	O
we	O
joined	O
the	O
parallel	O
training	O
corpus	O
with	O
the	O
same	O
monolingual	O
data	O
.	O
We	O
learned	O
joint	O
BPE	B-MethodName
4	O
with	O
32	O
K	O
merge	O
operations	O
over	O
this	O
corpus	O
and	O
applied	O
them	O
to	O
the	O
parallel	O
training	O
data	O
to	O
get	O
vocabularies	O
for	O
each	O
language	O
.	O
Additionally	O
,	O
we	O
used	O
the	O
clean	O
-	O
corpus	O
-	O
n.perl	O
script	O
within	O
Moses	O
to	O
filter	O
out	O
sentences	O
from	O
the	O
parallel	O
corpus	O
with	O
more	O
than	O
250	O
subwords	O
as	O
well	O
as	O
sentence	O
length	O
ratio	O
over	O
1.5	O
in	O
either	O
direction	O
.	O
Final	O
corpus	O
statistics	O
are	O
presented	O
in	O
Table	O
1	O
.	O

NNEMBs	O
at	O
SemEval	O
-	O
2017	O
Task	O
4	O
:	O
Neural	O
Twitter	O
Sentiment	O
Classification	B-TaskName
:	O
a	O
Simple	O
Ensemble	O
Method	O
with	O
Different	O
Embeddings	O

Twitter	O
sentiment	O
classification	O
has	O
attracted	O
a	O
lot	O
of	O
attention	O
(	O
Dong	O
et	O
al	O
,	O
2015	O
;	O
Nakov	O
et	O
al	O
,	O
2016	O
;	O
Rosenthal	O
et	O
al	O
,	O
2017	O
)	O
,	O
which	O
aims	O
to	O
classify	O
a	O
tweet	O
into	O
three	O
sentiment	O
categories	O
:	O
negative	O
,	O
neutral	O
,	O
and	O
positive	O
.	O
Tweet	O
text	O
has	O
several	O
features	O
:	O
written	O
by	O
the	O
informal	O
language	O
,	O
hash	O
-	O
tags	O
and	O
emoticons	O
indicate	O
sentiments	O
,	O
and	O
sometimes	O
is	O
sarcasm	O
,	O
which	O
make	O
decisions	O
of	O
tweet	O
sentiment	O
hard	O
for	O
machines	O
.	O
With	O
releases	O
of	O
annotated	O
datasets	O
,	O
more	O
researchers	O
prefer	O
to	O
use	O
the	O
1	O
https://github.com/zwjyyc/NNEMBs	O
twitter	O
sentiment	O
classification	O
as	O
one	O
testbed	O
to	O
evaluate	O
their	O
proposed	O
models	O
.	O
Traditional	O
methods	O
(	O
Mohammad	O
et	O
al	O
,	O
2013	O
)	O
for	O
twitter	O
sentiment	O
classification	O
use	O
a	O
variety	O
of	O
hand	O
-	O
crafted	O
features	O
including	O
surface	O
-	O
form	O
,	O
semantic	O
and	O
sentiment	O
lexicons	O
.	O
The	O
performances	O
of	O
these	O
methods	O
often	O
depend	O
on	O
the	O
quality	O
of	O
feature	B-TaskName
engineering	I-TaskName
work	O
,	O
and	O
building	O
a	O
state	O
-	O
ofthe	O
-	O
art	O
system	O
is	O
difficult	O
for	O
novices	O
.	O
Moreover	O
,	O
these	O
designed	O
features	O
are	O
presented	O
by	O
the	O
onehot	O
representation	O
which	O
can	O
not	O
capture	O
the	O
semantic	O
relativeness	O
of	O
different	O
features	O
and	O
proposes	O
a	O
problem	O
of	O
feature	O
sparsity	O
.	O
To	O
address	O
this	O
,	O
Tang	O
et	O
al	O
(	O
2014	O
)	O
induced	O
sentiment	O
-	O
specific	O
low	O
-	O
dimensional	O
,	O
real	O
-	O
valued	O
embedding	O
features	O
for	O
twitter	O
classification	O
,	O
which	O
encode	O
both	O
semantics	O
and	O
sentiments	O
of	O
words	O
.	O
In	O
the	O
experiments	O
,	O
the	O
embedding	O
features	O
and	O
hand	O
-	O
crafted	O
features	O
obtain	O
similar	O
results	O
,	O
and	O
also	O
they	O
are	O
complementary	O
for	O
each	O
other	O
in	O
the	O
system	O
.	O
With	O
the	O
developments	O
of	O
neural	O
networks	O
in	O
natural	O
language	O
processing	O
,	O
neural	O
sentiment	O
classification	O
(	O
Severyn	O
and	O
Moschitti	O
,	O
2015	O
;	O
Deriu	O
et	O
al	O
,	O
2016	O
)	O
has	O
attracted	O
a	O
lot	O
of	O
attention	O
recently	O
and	O
become	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
arts	O
.	O
These	O
methods	O
first	O
learn	O
word	B-TaskName
embeddings	I-TaskName
from	O
large	O
-	O
scale	O
twitter	O
corpus	O
,	O
then	O
tune	O
neural	O
networks	O
by	O
the	O
tweets	O
which	O
have	O
distant	O
labels	O
,	O
and	O
finally	O
fine	O
-	O
tune	O
the	O
proposed	O
models	O
by	O
the	O
annotated	O
datasets	O
.	O
Learning	O
word	B-TaskName
embeddings	I-TaskName
using	O
in	O
-	O
domain	O
data	O
is	O
an	O
effective	O
way	O
to	O
boost	O
model	O
performances	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
;	O
Yin	O
et	O
al	O
,	O
2016	O
)	O
.	O
However	O
,	O
collecting	O
large	O
-	O
scale	O
twitter	O
corpus	O
is	O
often	O
time	O
-	O
consuming	O
.	O
In	O
this	O
paper	O
,	O
we	O
use	O
the	O
different	O
word	O
embedding	O
sets	O
to	O
boost	O
the	O
performances	O
of	O
our	O
neural	O
networks	O
,	O
which	O
only	O
include	O
released	O
different	O
word	B-TaskName
embeddings	I-TaskName
sets	O
and	O
the	O
word	O
embedding	O
set	O
derived	O
from	O
the	O
released	O
Yelp	O
large	O
-	O
scale	O
datasets	O
by	O
Skip	O
-	O
gram	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
.	O
A	O
simple	O
and	O
effective	O
ensemble	O
method	O
is	O
proposed	O
,	O
which	O
takes	O
different	O
word	O
embedding	O
sets	O
as	O
input	O
to	O
train	O
neural	O
networks	O
and	O
predicts	O
labels	O
of	O
testing	O
tweets	O
by	O
merging	O
all	O
output	O
of	O
neural	O
models	O
.	O
Our	O
ensemble	O
method	O
show	O
its	O
effectiveness	O
in	O
SemEval	O
2017	O
,	O
though	O
most	O
of	O
used	O
word	O
embedding	O
sets	O
are	O
not	O
learned	O
from	O
twitter	O
corpus	O
,	O
which	O
can	O
be	O
explained	O
that	O
different	O
embedding	O
sets	O
has	O
different	O
vocabularies	O
and	O
encode	O
different	O
parts	O
of	O
sentiment	O
knowledge	O
.	O
Moreover	O
,	O
we	O
conduct	O
additional	O
experiments	O
to	O
analyze	O
our	O
model	O
.	O

We	O
have	O
many	O
choices	O
of	O
neural	O
networks	O
(	O
e.g.	O
,	O
LSTM	B-MethodName
,	O
RNN	O
and	O
GRU	B-MethodName
)	O
for	O
our	O
method	O
,	O
here	O
we	O
consider	O
RCNN	O
(	O
Lei	O
et	O
al	O
,	O
2016	O
)	O
in	O
our	O
method	O
.	O
RCNN	O
has	O
non	O
-	O
consecutive	O
convolution	B-MethodName
and	O
adaptive	O
gated	O
decay	O
,	O
which	O
aims	O
to	O
capture	O
longerrange	O
,	O
non	O
-	O
consecutive	O
patterns	O
in	O
a	O
weighted	O
manner	O
.	O
Given	O
a	O
sequence	O
of	O
words	O
which	O
are	O
denoted	O
as	O
{	O
x	O
i	O
}	O
l	O
i=1	O
,	O
the	O
corresponding	O
word	B-TaskName
embeddings	I-TaskName
{	O
x	O
i	O
}	O
l	O
i=1	O
are	O
derived	O
using	O
the	O
embedding	O
matrix	O
E.	O
Then	O
,	O
RCNN	O
obtains	O
their	O
corresponding	O
hidden	O
vectors	O
{	O
h	O
i	O
}	O
l	O
i=1	O
using	O
the	O
convolution	B-MethodName
operation	O
and	O
gating	O
mechanism	O
.	O
After	O
obtaining	O
hidden	O
vectors	O
,	O
RCNN	O
uses	O
a	O
pooling	O
operation	O
to	O
get	O
fixed	O
-	O
sized	O
vector	O
presentation	O
,	O
which	O
is	O
fed	O
into	O
softmax	B-MethodName
layer	O
to	O
finish	O
the	O
prediction	O
.	O
The	O
ngram	O
convolution	B-MethodName
operation	O
and	O
gating	O
decay	O
are	O
described	O
as	O
follows	O
:	O
λt	O
=	O
σ	O
(	O
W	O
λ	O
xt	O
+	O
U	O
λ	O
ht−1	O
+	O
b	O
λ	O
)	O
,	O
c	O
(	O
1	O
)	O
t	O
=	O
λt	O
c	O
(	O
1	O
)	O
t−1	O
+	O
(	O
1	O
−	O
λt	O
)	O
(	O
W1xt	O
)	O
,	O
c	O
(	O
2	O
)	O
t	O
=	O
λt	O
c	O
(	O
2	O
)	O
t−1	O
+	O
(	O
1	O
−	O
λt	O
)	O
(	O
c	O
(	O
1	O
)	O
t−1	O
+	O
W2xt	O
)	O
,	O
,	O
c	O
(	O
n	O
)	O
t	O
=	O
λt	O
c	O
(	O
n	O
)	O
t−1	O
+	O
(	O
1	O
−	O
λt	O
)	O
(	O
c	O
(	O
n−1	O
)	O
t−1	O
+	O
Wnxt	O
)	O
,	O
ht	O
=	O
tanh	O
(	O
c	O
(	O
n	O
)	O
t	O
+	O
b	O
)	O
,	O
where	O
W	O
λ	O
,	O
U	O
λ	O
,	O
b	O
λ	O
,	O
b	O
and	O
W	O
*	O
are	O
learnable	O
parameters	O
,	O
σ	O
is	O
sigmoid	O
function	O
which	O
rescales	O
the	O
value	O
into	O
(	O
0	B-DatasetName
,	O
1	O
)	O
,	O
is	O
dot	O
product	O
,	O
λ	O
t	O
is	O
gating	O
value	O
determining	O
how	O
much	O
information	O
of	O
x	O
t	O
and	O
previous	O
patterns	O
is	O
added	O
into	O
the	O
hidden	O
vector	O
,	O
c	O
(	O
i	O
)	O
t	O
refer	O
to	O
the	O
vector	O
for	O
accumulated	O
previous	O
patterns	O
which	O
are	O
ended	O
with	O
x	O
t	O
include	O
i	O
consecutive	O
tokens	O
.	O
When	O
λ	O
t	O
=	O
0	B-DatasetName
,	O
the	O
convolution	B-MethodName
becomes	O
a	O
standard	O
n	O
-	O
gram	O
convolution	B-MethodName
.	O
We	O
also	O
can	O
build	O
a	O
deep	O
RCNN	O
by	O
adding	O
several	O
convolution	B-MethodName
layer	O
on	O
top	O
of	O
hidden	O
vectors	O
derived	O
from	O
the	O
bottom	O
convolution	B-MethodName
layer	O
.	O
Here	O
we	O
consider	O
the	O
RCNN	O
with	O
d	O
convolution	B-MethodName
layers	O
,	O
which	O
outputs	O
{	O
h	O
d	O
i	O
}	O
l	O
i=1	O
.	O
Then	O
,	O
a	O
last	O
pooling	O
operation	O
is	O
conducted	O
on	O
hidden	O
vectors	O
to	O
obtain	O
text	O
representation	O
r.	O
Finally	O
,	O
text	O
representation	O
is	O
fed	O
into	O
a	O
softmax	B-MethodName
layer	O
.	O
The	O
softmax	B-MethodName
layer	O
outputs	O
the	O
probability	O
distribution	O
over	O
|	O
Y	O
|	O
categories	O
for	O
the	O
distributed	O
representation	O
,	O
which	O
is	O
defined	O
as	O
:	O
p	O
(	O
r	O
)	O
=	O
softmax	B-MethodName
(	O
W	O
class	O
k	O
r	O
)	O
.	O
The	O
cross	O
-	O
entropy	O
objective	O
function	O
is	O
used	O
to	O
optimize	O
the	O
RCNN	O
model	O
.	O

To	O
improve	O
the	O
accuracy	B-MetricName
of	O
predicateargument	O
structure	O
(	O
PAS	O
)	O
analysis	O
,	O
large	O
-	O
scale	O
training	O
data	O
and	O
knowledge	O
for	O
PAS	O
analysis	O
are	O
indispensable	O
.	O
We	O
focus	O
on	O
a	O
specific	O
domain	O
,	O
specifically	O
Japanese	O
blogs	O
on	O
driving	O
,	O
and	O
construct	O
two	O
wide	O
-	O
coverage	O
datasets	O
as	O
a	O
form	O
of	O
QA	O
using	O
crowdsourcing	O
:	O
a	O
PAS	O
-	O
QA	O
dataset	O
and	O
a	O
reading	B-TaskName
comprehension	I-TaskName
QA	O
(	O
RC	O
-	O
QA	O
)	O
dataset	O
.	O
We	O
train	O
a	O
machine	O
comprehension	O
(	O
MC	O
)	O
model	O
based	O
on	O
these	O
datasets	O
to	O
perform	O
PAS	O
analysis	O
.	O
Our	O
experiments	O
show	O
that	O
a	O
stepwise	O
training	O
method	O
is	O
the	O
most	O
effective	O
,	O
which	O
pre	O
-	O
trains	O
an	O
MC	O
model	O
based	O
on	O
the	O
RC	O
-	O
QA	O
dataset	O
to	O
acquire	O
domain	O
knowledge	O
and	O
then	O
fine	O
-	O
tunes	O
based	O
on	O
the	O
PAS	O
-	O
QA	O
dataset	O
.	O

We	O
construct	O
a	O
driving	O
-	O
domain	O
RC	O
-	O
QA	O
dataset	O
in	O
the	O
same	O
way	O
as	O
SQuAD	B-DatasetName
1.1	O
.	O
We	O
extract	O
a	O
document	O
from	O
the	O
Driving	O
Experience	O
Corpus	O
and	O
ask	O
three	O
crowdworkers	O
to	O
write	O
questions	O
and	O
their	O
answers	O
about	O
the	O
document	O
.	O
After	O
that	O
,	O
we	O
ask	O
another	O
five	O
crowdworkers	O
to	O
answer	O
a	O
question	O
to	O
validate	O
its	O
answerability	O
and	O
adopt	O
questions	O
with	O
three	O
or	O
more	O
same	O
answers	O
.	O
We	O
randomly	O
extracted	O
200	O
questions	O
from	O
the	O
RC	O
-	O
QA	O
dataset	O
and	O
judged	O
the	O
question	O
types	O
.	O
The	O
result	O
is	O
shown	O
in	O
Table	O
2	O
.	O
A	O
question	O
was	O
classified	O
according	O
to	O
whether	O
it	O
is	O
a	O
question	O
asking	O
for	O
any	O
argument	O
of	O
nominative	O
,	O
accusative	O
or	O
dative	O
,	O
and	O
if	O
applicable	O
,	O
whether	O
it	O
is	O
an	O
omission	O
or	O
not	O
.	O
As	O
shown	O
in	O
Table	O
2	O
,	O
the	O
RC	O
-	O
QA	O
dataset	O
contains	O
nearly	O
40	O
%	O
of	O
questions	O
asking	O
arguments	O
of	O
nominative	O
,	O
accusative	O
and	O
dative	O
,	O
and	O
a	O
few	O
questions	O
asking	O
for	O
omitted	O
arguments	O
,	O
which	O
are	O
similar	O
to	O
the	O
PAS	O
-	O
QA	O
dataset	O
.	O
There	O
are	O
various	O
other	O
questions	O
asking	O
for	O
arguments	O
other	O
than	O
nominative	O
,	O
accusative	O
and	O
dative	O
,	O
and	O
questions	O
using	O
why	O
and	O
how	O
.	O

We	O
constructed	O
driving	O
-	O
domain	O
PAS	O
-	O
QA	O
and	O
RC	O
-	O
QA	O
datasets	O
using	O
crowdsourcing	O
5	O
.	O
We	O
also	O
proposed	O
an	O
MC	O
-	O
based	O
PAS	O
analysis	O
method	O
.	O
In	O
particular	O
,	O
the	O
stepwise	O
training	O
method	O
based	O
on	O
BERT	B-MethodName
was	O
the	O
most	O
effective	O
,	O
which	O
outperformed	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
NN	O
-	O
PAS	O
model	O
.	O
In	O
the	O
future	O
,	O
we	O
will	O
pre	O
-	O
train	O
an	O
MC	O
model	O
based	O
on	O
datasets	O
other	O
than	O
the	O
RC	O
-	O
QA	O
dataset	O
to	O
acquire	O
domain	O
knowledge	O
.	O
A	O
Details	O
of	O
PAS	O
-	O
QA	O
Dataset	O
Construction	O
We	O
construct	O
the	O
PAS	O
-	O
QA	O
dataset	O
asking	O
for	O
omitted	O
nominative	O
arguments	O
using	O
the	O
following	O
procedure	O
:	O
1	O
.	O
We	O
extract	O
four	O
consecutive	O
sentences	O
that	O
satisfy	O
the	O
following	O
conditions	O
from	O
the	O
Driving	O
Experience	O
Corpus	O
constructed	O
by	O
Iwai	O
et	O
al	O
(	O
2019	O
)	O
.	O
The	O
Driving	O
Experience	O
extracting	O
CRF	B-MethodName
tool	O
(	O
Iwai	O
et	O
al	O
,	O
2018	O
)	O
judges	O
that	O
three	O
or	O
more	O
sentences	O
out	O
of	O
four	O
sentences	O
are	O
driving	O
experience	O
.	O
Each	O
sentence	O
contains	O
at	O
least	O
one	O
PAS	O
.	O
The	O
PAS	O
analyzer	O
,	O
KNP	O
,	O
judges	O
that	O
there	O
is	O
a	O
PAS	O
whose	O
nominative	O
argument	O
is	O
omitted	O
in	O
the	O
fourth	O
sentence	O
.	O
Sentences	O
include	O
at	O
least	O
one	O
"	O
Driving	O
Characteristic	O
Word	O
"	O
(	O
Iwai	O
et	O
al	O
,	O
2019	O
)	O
.	O
2	O
.	O
We	O
automatically	O
make	O
crowdsourcing	O
tasks	O
using	O
an	O
extracted	O
document	O
and	O
a	O
PAS	O
whose	O
nominative	O
argument	O
is	O
omitted	O
(	O
See	O
Figure	O
5	O
and	O
Figure	O
6	O
)	O
.	O
Each	O
task	O
consists	O
of	O
a	O
document	O
,	O
a	O
question	O
and	O
answer	O
choices	O
.	O
Answer	O
choices	O
consist	O
of	O
nouns	O
extracted	O
from	O
the	O
document	O
and	O
special	O
symbols	O
,	O
"	O
author	O
,	O
"	O
"	O
other	O
,	O
"	O
and	O
"	O
not	O
sure	O
.	O
"	O
For	O
nominative	O
PAS	O
-	O
QA	O
questions	O
,	O
the	O
special	O
symbol	O
"	O
author	O
"	O
can	O
often	O
be	O
an	O
answer	O
,	O
but	O
it	O
is	O
not	O
explicitly	O
expressed	O
in	O
the	O
document	O
.	O
So	O
we	O
add	O
it	O
to	O
the	O
choices	O
.	O
We	O
add	O
"	O
other	O
"	O
so	O
that	O
it	O
can	O
be	O
selected	O
when	O
there	O
is	O
an	O
appropriate	O
answer	O
besides	O
the	O
choices	O
.	O
We	O
add	O
"	O
not	O
sure	O
"	O
so	O
that	O
workers	O
can	O
select	O
it	O
if	O
they	O
can	O
not	O
find	O
an	O
answer	O
.	O
We	O
add	O
more	O
explanations	O
to	O
crowdsourcing	O
answer	O
screen	O
(	O
See	O
Figure	O
5	O
and	O
Figure	O
6	O
)	O
.	O
3	O
.	O
Using	O
crowdsourcing	O
,	O
we	O
ask	O
five	O
crowdworkers	O
per	O
question	O
to	O
select	O
one	O
or	O
more	O
appropriate	O
answers	O
from	O
the	O
choices	O
.	O
We	O
asked	O
five	O
crowdworkers	O
per	O
question	O
using	O
Yahoo	O
!	O
crowdsourcing	O
.	O
We	O
adopted	O
triplets	O
with	O
three	O
or	O
more	O
votes	O
if	O
they	O
are	O
not	O
"	O
not	O
sure	O
.	O
"	O
If	O
they	O
are	O
"	O
other	O
,	O
"	O
we	O
handled	O
them	O
as	O
described	O
in	O
the	O
main	O
paper	O
.	O
We	O
finally	O
record	O
the	O
answers	O
as	O
spans	O
in	O
a	O
document	O
or	O
NULL	O
.	O

Image	O
-	O
text	O
retrieval	O
(	O
ITR	O
)	O
has	O
been	O
widely	O
studied	O
as	O
a	O
staple	O
benchmark	O
task	O
in	O
both	O
NLP	O
and	O
computer	O
vision	O
communities	O
.	O
Traditional	O
ITR	O
search	O
engines	O
typically	O
deploy	O
ranking	O
-	O
based	O
models	O
built	O
upon	O
visual	O
-	O
semantic	O
embedding	O
matching	O
(	O
Faghri	O
et	O
al	O
,	O
2017	O
;	O
Huang	O
et	O
al	O
,	O
2018	O
)	O
or	O
deep	O
cross	O
-	O
modal	O
fusion	O
with	O
attention	O
mechanism	O
(	O
Lee	O
et	O
al	O
,	O
2018	O
;	O
Li	O
et	O
al	O
,	O
2020a	O
,	O
b	O
)	O
.	O
Earliest	O
works	O
(	O
Kiros	O
et	O
al	O
,	O
2014	O
;	O
Faghri	O
et	O
al	O
,	O
2017	O
(	O
a	O
)	O
Early	O
work	O
(	O
Faghri	O
et	O
al	O
,	O
2017	O
)	O
using	O
dot	O
product	O
to	O
learn	O
the	O
similarity	O
between	O
global	O
image	O
features	O
and	O
global	O
text	O
features	O
.	O
(	O
b	O
)	O
Later	O
study	O
(	O
Lee	O
et	O
al	O
,	O
2018	O
)	O
applying	O
cross	O
-	O
attention	O
between	O
the	O
features	O
of	O
each	O
region	O
and	O
each	O
word	O
.	O
(	O
c	O
)	O
Pre	O
-	O
trained	O
V+L	O
models	O
with	O
deep	O
Transformer	B-MethodName
.	O
(	O
d	O
)	O
LightningDOT	O
without	O
cross	O
-	O
attention	O
.	O
CMR	O
,	O
SMRM	O
and	O
VMLM	O
refer	O
to	O
different	O
pre	O
-	O
training	O
tasks	O
,	O
which	O
will	O
be	O
introduced	O
later	O
in	O
method	O
section	O
.	O
employ	O
separate	O
image	O
encoder	O
(	O
e.g.	O
,	O
CNN	O
)	O
and	O
text	O
encoder	O
(	O
e.g.	O
,	O
RNN	O
)	O
,	O
the	O
embeddings	O
from	O
which	O
are	O
then	O
measured	O
by	O
doc	O
product	O
for	O
similarity	O
matching	O
(	O
Figure	O
1	O
(	O
a	O
)	O
)	O
.	O
Later	O
studies	O
(	O
Lee	O
et	O
al	O
,	O
2018Wang	O
et	O
al	O
,	O
2019	O
;	O
improve	O
this	O
paradigm	O
by	O
employing	O
advanced	O
region	O
-	O
level	O
visual	O
encoder	O
(	O
e.g.	O
,	O
Faster	O
-	O
RCNN	O
)	O
and	O
applying	O
cross	O
-	O
attention	O
between	O
word	O
features	O
and	O
region	O
features	O
for	O
multimodal	O
fusion	O
(	O
Figure	O
1	O
(	O
b	O
)	O
)	O
.	O
With	O
the	O
advent	O
of	O
Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
and	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
crossmodal	O
retrieval	O
tasks	O
are	O
more	O
recently	O
dominated	O
by	O
vision	O
-	O
and	O
-	O
language	O
(	O
V+L	O
)	O
pre	O
-	O
trained	O
models	O
,	O
such	O
as	O
ViLBERT	B-MethodName
,	O
UNITER	B-MethodName
,	O
OSCAR	B-MethodName
(	O
Li	O
et	O
al	O
,	O
2020b	O
)	O
,	O
and	O
VILLA	O
.	O
Large	O
-	O
scale	O
pre	O
-	O
trained	O
models	O
learned	O
from	O
massive	O
corpus	O
of	O
image	O
-	O
text	O
pairs	O
can	O
power	O
heterogeneous	O
downstream	O
tasks	O
that	O
take	O
diverse	O
modalities	O
as	O
inputs	O
(	O
e.g.	O
,	O
text	O
,	O
image	O
,	O
video	O
,	O
audio	O
)	O
.	O
These	O
models	O
benefit	O
from	O
the	O
self	O
-	O
attention	O
mechanism	O
in	O
Transformer	B-MethodName
architecture	O
,	O
learning	O
joint	O
image+text	O
embeddings	O
through	O
pre	O
-	O
training	O
objectives	O
such	O
as	O
masked	B-TaskName
language	I-TaskName
modeling	I-TaskName
(	O
MLM	B-DatasetName
)	O
and	O
masked	O
region	O
modeling	O
(	O
MRM	O
)	O
(	O
Figure	O
1	O
(	O
c	O
)	O
)	O
.	O
However	O
,	O
the	O
very	O
ingredient	O
that	O
engenders	O
the	O
success	O
of	O
these	O
pre	O
-	O
trained	O
models	O
,	O
crossmodal	O
attention	O
between	O
two	O
modalities	O
(	O
through	O
self	O
-	O
attention	O
)	O
,	O
also	O
destines	O
the	O
inevitable	O
latency	O
and	O
huge	O
computation	O
cost	O
in	O
training	O
and	O
deploying	O
such	O
massive	O
-	O
scale	O
models	O
.	O
For	O
example	O
,	O
UNITER	B-MethodName
builds	O
upon	O
12/24	O
Transformer	B-MethodName
layers	O
,	O
and	O
trains	O
over	O
10	O
million	O
image+text	O
pairs	O
.	O
The	O
inference	O
time	O
of	O
such	O
large	O
models	O
with	O
110	O
million	O
parameters	O
is	O
48	O
seconds	O
on	O
average	O
for	O
text	O
query	O
from	O
COCO	B-DatasetName
dataset	O
(	O
Chen	O
et	O
al	O
,	O
2015	O
)	O
,	O
not	O
scalable	O
in	O
real	O
-	O
life	O
applications	O
serving	O
millions	O
of	O
queries	O
per	O
second	O
.	O
To	O
make	O
real	O
-	O
time	O
ITR	O
possible	O
with	O
low	O
latency	O
,	O
we	O
ask	O
a	O
bold	O
question	O
:	O
can	O
we	O
go	O
back	O
to	O
the	O
beginning	O
,	O
reverting	O
to	O
simple	O
dot	O
product	O
for	O
efficient	O
cross	B-TaskName
-	I-TaskName
modal	I-TaskName
retrieval	I-TaskName
?	O
To	O
make	O
this	O
retro	O
experiment	O
feasible	O
,	O
we	O
rely	O
on	O
Transformer	B-MethodName
to	O
pre	O
-	O
train	O
high	O
-	O
quality	O
image	O
and	O
text	O
encoders	O
,	O
but	O
use	O
efficient	O
dot	O
product	O
for	O
multimodal	O
fusion	O
instead	O
of	O
computationally	O
heavy	O
self	O
-	O
attention	O
.	O
To	O
still	O
facilitate	O
effective	O
cross	O
-	O
modal	O
embedding	O
learning	O
,	O
we	O
use	O
a	O
special	O
[	O
CLS	O
]	O
token	O
on	O
both	O
encoders	O
,	O
which	O
transfers	O
the	O
learned	O
embedding	O
from	O
the	O
other	O
modality	O
(	O
Figure	O
1	O
(	O
d	O
)	O
)	O
.	O
We	O
name	O
this	O
new	O
paradigm	O
LightningDOT	O
,	O
for	O
its	O
lightening	O
speed	O
benefiting	O
from	O
dot	O
product	O
computation	O
.	O
By	O
removing	O
the	O
time	O
-	O
consuming	O
cross	O
-	O
attention	O
between	O
modalities	O
,	O
the	O
model	O
can	O
learn	O
visualsemantic	O
embeddings	O
without	O
extensive	O
matching	O
between	O
each	O
image	O
-	O
text	O
pair	O
during	O
inference	O
,	O
as	O
used	O
in	O
existing	O
pre	O
-	O
trained	O
models	O
Li	O
et	O
al	O
,	O
2020b	O
;	O
.	O
Further	O
,	O
by	O
eliminating	O
the	O
dependency	O
on	O
real	O
-	O
time	O
computation	O
over	O
image	O
-	O
text	O
pairs	O
,	O
we	O
can	O
compute	O
all	O
image	O
and	O
text	O
embeddings	O
independently	O
offline	O
just	O
for	O
once	O
,	O
and	O
reuse	O
these	O
embeddings	O
as	O
cached	O
indexes	O
for	O
new	O
queries	O
on	O
the	O
fly	O
(	O
Figure	O
2	O
)	O
.	O
For	O
model	O
training	O
,	O
we	O
propose	O
three	O
learning	O
objectives	O
to	O
jointly	O
train	O
two	O
Transformer	B-MethodName
blocks	O
:	O
Image	O
Encoder	O
and	O
Language	O
Encoder	O
.	O
Specifically	O
,	O
Visual	O
-	O
embedding	O
fused	O
MLM	B-DatasetName
(	O
namely	O
VMLM	O
)	O
and	O
Semantic	O
-	O
embedding	O
fused	O
MRM	O
(	O
namely	O
SMRM	O
)	O
ensure	O
cross	O
-	O
modal	O
information	O
is	O
harnessed	O
even	O
without	O
cross	O
-	O
modality	O
self	O
-	O
attention	O
.	O
A	O
cross	B-TaskName
-	I-TaskName
modal	I-TaskName
retrieval	I-TaskName
objective	O
(	O
namely	O
CMR	O
)	O
encourages	O
the	O
model	O
to	O
learn	O
multimodal	O
fusion	O
through	O
pre	O
-	O
training	O
.	O
To	O
maintain	O
competitive	O
model	O
performance	O
,	O
we	O
further	O
introduce	O
a	O
reranking	O
mechanism	O
to	O
bring	O
back	O
the	O
benefit	O
of	O
cross	O
-	O
attention	O
methods	O
.	O
In	O
summary	O
,	O
LightningDOT	O
is	O
designed	O
with	O
late	O
fusion	O
to	O
learn	O
visual	O
-	O
semantic	O
embeddings	O
.	O
Experiments	O
on	O
popular	O
ITR	O
benchmarks	O
show	O
that	O
LightningDOT	O
is	O
600/1900	O
times	O
faster	O
than	O
existing	O
pre	O
-	O
trained	O
models	O
on	O
Flickr30k	B-DatasetName
/	O
COCO	B-DatasetName
,	O
while	O
achieving	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
.	O
When	O
retrieving	O
from	O
larger	O
candidate	O
pool	O
(	O
>	O
120	O
K	O
images	O
)	O
,	O
LightningDOT	O
is	O
23	O
,	O
000	O
times	O
faster	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
known	O
effort	O
on	O
improving	O
V+L	O
model	O
efficiency	O
.	O

For	O
pre	O
-	O
training	O
,	O
we	O
use	O
pre	O
-	O
processed	O
data	O
provided	O
by	O
,	O
including	O
4.2	O
million	O
8	O
The	O
computation	O
time	O
of	O
LightningDOT	O
is	O
negligible	O
compared	O
to	O
that	O
of	O
UNITER	B-MethodName
.	O
Therefore	O
,	O
the	O
empirical	O
speed	O
is	O
proportional	O
to	O
the	O
number	O
of	O
pairs	O
UNITER	B-MethodName
has	O
to	O
rank	O
:	O
constant	O
M	O
for	O
LightningDOT	O
+	O
UNITER	B-MethodName
vs.	O
the	O
whole	O
database	O
(	O
index	O
)	O
size	O
for	O
UNITER	B-MethodName
only	O
.	O
images	O
with	O
9.5	O
million	O
associated	O
captions	O
from	O
COCO	B-DatasetName
(	O
Chen	O
et	O
al	O
,	O
2015	O
)	O
,	O
VG	O
(	O
Krishna	O
et	O
al	O
,	O
2017	O
)	O
,	O
Conceptual	B-DatasetName
Captions	I-DatasetName
(	O
Sharma	O
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
SBU	O
captions	O
(	O
Ordonez	O
et	O
al	O
,	O
2011	O
)	O
.	O
For	O
evaluation	O
,	O
we	O
use	O
Flickr30k	B-DatasetName
(	O
Plummer	O
et	O
al	O
,	O
2015	O
)	O
and	O
COCO	B-DatasetName
(	O
Lin	O
et	O
al	O
,	O
2014	O
)	O
datasets	O
,	O
which	O
include	O
31K/123	O
K	O
images	O
,	O
respectively	O
,	O
each	O
associated	O
with	O
5	O
human	O
-	O
written	O
captions	O
.	O
Following	O
(	O
Faghri	O
et	O
al	O
,	O
2017	O
)	O
,	O
we	O
split	O
COCO	B-DatasetName
into	O
114K/5K/5	O
K	O
and	O
Flickr30	O
K	O
into	O
29K/1k/1k	O
images	O
for	O
train	O
,	O
validation	O
and	O
test	O
.	O
Downstream	O
performance	O
is	O
measured	O
by	O
recall	O
at	O
K	O
(	O
R@K	O
)	O
for	O
both	O
image	O
and	O
text	O
retrieval	O
tasks	O
.	O
We	O
also	O
use	O
an	O
additional	O
metric	O
"	O
AR	O
"	O
,	O
the	O
average	O
of	O
R@K	O
for	O
all	O
K	O
across	O
both	O
image	O
and	O
sentence	O
retrieval	O
tasks	O
.	O

We	O
conduct	O
ablation	O
studies	O
on	O
Flickr30	O
K	O
(	O
Table	O
4	O
)	O
and	O
compare	O
LightningDOT	O
(	O
L4	O
)	O
against	O
3	O
ablated	O
instances	O
:	O
(	O
i	O
)	O
"	O
R	B-MethodName
-	I-MethodName
CNN	I-MethodName
only	O
"	O
(	O
L1	O
)	O
:	O
image	O
representations	O
are	O
extracted	O
from	O
Faster	B-MethodName
R	I-MethodName
-	I-MethodName
CNN	I-MethodName
directly	O
,	O
with	O
no	O
image	O
encoder	O
applied	O
;	O
(	O
ii	O
)	O
"	O
+	O
Image	O
Encoder	O
"	O
(	O
L2	O
)	O
:	O
regional	O
features	O
are	O
encoded	O
with	O
a	O
12	O
-	O
layer	O
Transformer	B-MethodName
as	O
the	O
image	O
encoder	O
;	O
(	O
iii	O
)	O
"	O
+	O
PT	O
†	O
"	O
(	O
L3	O
)	O
:	O
our	O
model	O
is	O
pre	O
-	O
trained	O
with	O
MLM+MRM+CMR	O
,	O
then	O
finetuned	O
on	O
Flickr30K.	O
Note	O
that	O
the	O
difference	O
between	O
MLM	B-DatasetName
vs.	O
VMLM	O
and	O
MRM	O
vs.	O
SMRM	O
is	O
whether	O
the	O
predictions	O
of	O
masked	O
tokens	O
(	O
regions	O
)	O
rely	O
on	O
infused	O
embeddings	O
from	O
the	O
other	O
modality	O
.	O
Multi30	O
K	O
and	O
COCO	B-DatasetName
datasets	O
.	O
We	O
compare	O
with	O
task	O
-	O
specific	O
methods	O
:	O
S	O
-	O
LIWE	O
(	O
Wehrmann	O
et	O
al	O
,	O
2019	O
)	O
,	O
MULE	O
,	O
SMALR	O
(	O
Burns	O
et	O
al	O
,	O
2020	O
)	O
,	O
pre	O
-	O
trained	O
method	O
M	O
3	O
P	O
(	O
Huang	O
et	O
al	O
,	O
2020a	O
)	O
Results	O
show	O
that	O
"	O
R	B-MethodName
-	I-MethodName
CNN	I-MethodName
only	O
"	O
is	O
not	O
sufficient	O
in	O
learning	O
good	O
image	O
representations	O
for	O
ITR	O
task	O
,	O
while	O
image	O
encoder	O
with	O
Transformer	B-MethodName
architecture	O
can	O
effectively	O
learn	O
contextualized	O
image	O
representations	O
,	O
hence	O
achieving	O
better	O
performance	O
.	O
Pre	O
-	O
trained	O
models	O
(	O
L3	O
-	O
4	O
)	O
generally	O
achieve	O
better	O
performance	O
,	O
compared	O
to	O
nonpretrained	O
models	O
(	O
L1	O
-	O
2	O
)	O
.	O
Comparing	O
"	O
+	O
PT	O
†	O
"	O
to	O
the	O
full	O
instance	O
of	O
LightningDOT	O
,	O
dependency	O
on	O
the	O
other	O
modality	O
in	O
VMLM	O
and	O
SMRM	O
brings	O
universal	O
performance	O
lift	O
across	O
all	O
metrics	O
.	O
This	O
indicates	O
that	O
these	O
cross	O
-	O
modal	O
dependencies	O
introduced	O
by	O
VMLM	O
and	O
SMRM	O
are	O
effective	O
in	O
learning	O
the	O
association	O
between	O
image	O
and	O
text	O
inputs	O
.	O
In	O
addition	O
,	O
we	O
investigate	O
the	O
effectiveness	O
of	O
each	O
pre	O
-	O
training	O
task	O
in	O
Table	O
5	O
.	O
Comparing	O
to	O
baseline	O
without	O
pre	O
-	O
training	O
,	O
pre	O
-	O
training	O
with	O
CMR	O
alone	O
lifts	O
+1.4	O
on	O
AR	O
.	O
Pre	O
-	O
training	O
with	O
all	O
three	O
tasks	O
achieves	O
the	O
best	O
performance	O
,	O
indicating	O
that	O
the	O
learning	O
of	O
contextualized	O
word	O
and	O
region	O
representations	O
promotes	O
better	O
global	O
alignment	O
between	O
image	O
and	O
text	O
,	O
and	O
these	O
three	O
pre	O
-	O
training	O
tasks	O
work	O
collaboratively	O
to	O
yield	O
better	O
visual	O
-	O
semantic	O
embeddings	O
.	O

We	O
show	O
an	O
example	O
of	O
image	B-TaskName
retrieval	I-TaskName
results	O
here	O
at	O
figure	O
4	O
for	O
query	O
as	O
"	O
Sky	O
view	O
of	O
a	O
blue	O
and	O
yellow	O
biplane	O
flying	O
near	O
each	O
other	O
"	O
.	O
In	O
addition	O
to	O
the	O
ground	O
truth	O
image	O
in	O
the	O
red	O
rectangle	O
,	O
all	O
the	O
10	O
images	O
retrieved	O
by	O
our	O
model	O
are	O
valid	O
retrieval	O
since	O
multiple	O
keywords	O
(	O
"	O
sky	O
"	O
,	O
"	O
blue	O
"	O
,	O
"	O
yellow	O
"	O
,	O
"	O
airplane	O
"	O
,	O
"	O
near	O
"	O
)	O
are	O
captured	O
for	O
each	O
image	O
.	O
Please	O
see	O
the	O
appendix	O
A.4	O
for	O
more	O
examples	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
pre	O
-	O
training	O
framework	O
that	O
learns	O
joint	O
visual	O
-	O
semantic	O
embedding	O
without	O
any	O
cross	O
-	O
attention	O
between	O
modalities	O
.	O
Light	O
-	O
ningDOT	O
outperforms	O
previous	O
state	O
of	O
the	O
art	O
,	O
while	O
significantly	O
speeding	O
up	O
inference	O
time	O
by	O
600	O
-	O
2000×	O
on	O
Flickr30	O
K	O
and	O
COCO	B-DatasetName
image	O
-	O
text	O
retrieval	O
benchmarks	O
.	O
Future	O
work	O
includes	O
extending	O
the	O
efficient	O
training	O
framework	O
to	O
other	O
V+L	O
tasks	O
.	O

When	O
evaluating	O
on	O
ITR	O
under	O
the	O
multilingual	O
setting	O
,	O
we	O
consider	O
two	O
benchmarks	O
:	O
Multi30	O
K	O
(	O
Elliott	O
et	O
al	O
,	O
2016	O
(	O
Elliott	O
et	O
al	O
,	O
,	O
2017Barrault	O
et	O
al	O
,	O
2018	O
)	O
and	O
COCO	B-DatasetName
Japanese	O
(	O
Yoshikawa	O
et	O
al	O
,	O
2017	O
)	O
and	O
Chinese	O
(	O
Li	O
et	O
al	O
,	O
2019b	O
)	O
.	O
Multi30	O
K	O
is	O
constructed	O
by	O
manually	O
translating	O
English	O
captions	O
in	O
Flickr30	O
K	O
(	O
Plummer	O
et	O
al	O
,	O
2015	O
)	O
to	O
German	O
,	O
French	O
,	O
and	O
Czech	O
.	O
Each	O
image	O
in	O
Multi30	O
K	O
is	O
paired	O
with	O
5	O
captions	O
in	O
German	O
,	O
1	O
caption	O
in	O
French	O
and	O
Czech	O
.	O
We	O
adopt	O
the	O
same	O
train	O
/	O
val	O
/	O
test	O
split	O
as	O
in	O
Flickr30K.	O
COCO	B-DatasetName
Japanese	O
(	O
Yoshikawa	O
et	O
al	O
,	O
2017	O
)	O
collected	O
820	O
K	O
Japanese	O
captions	O
for	O
165	O
K	O
COCO	B-DatasetName
images	O
(	O
Lin	O
et	O
al	O
,	O
2014	O
)	O
.	O
We	O
use	O
the	O
same	O
train	O
/	O
dev	O
/	O
test	O
splits	O
for	O
COCO	B-DatasetName
Japanese	O
as	O
in	O
Karpathy	O
and	O
Fei	O
-	O
Fei	O
(	O
2015	O
)	O
,	O
and	O
present	O
results	O
on	O
the	O
1	O
K	O
test	O
set	O
.	O
Similarly	O
,	O
Li	O
et	O
al	O
(	O
2019b	O
)	O
collected	O
1	O
-	O
2	O
Chinese	O
captions	O
per	O
image	O
for	O
20	O
K	O
COCO	B-DatasetName
images	O
to	O
build	O
COCO	B-DatasetName
Chinese	O
.	O
We	O
follow	O
the	O
original	O
split	O
defined	O
in	O
Li	O
et	O
al	O
(	O
2019b	O
)	O
.	O

We	O
present	O
the	O
detailed	O
inference	O
time	O
of	O
UNITERbase	O
,	O
SCAN	B-DatasetName
the	O
proposed	O
LightningDOT	O
and	O
LightningDOT	O
with	O
UNITER	B-MethodName
-	O
base	O
re	O
-	O
ranker	O
in	O
Table	O
7	O
,	O
measured	O
by	O
seconds	O
/	O
query	O
.	O
UNITER	B-MethodName
clearly	O
is	O
the	O
slowest	O
,	O
as	O
the	O
12	O
-	O
layer	O
Transformer	B-MethodName
model	O
inference	O
needs	O
to	O
be	O
run	O
between	O
each	O
query	O
and	O
all	O
images	O
.	O
Comparing	O
between	O
Flickr30k	B-DatasetName
-	O
test	O
and	O
COCO	B-DatasetName
-	O
test	O
,	O
its	O
inference	O
time	O
scales	O
up	O
linearly	O
with	O
the	O
number	O
of	O
images	O
.	O
With	O
the	O
lightweight	O
GRU	B-MethodName
(	O
Chung	O
et	O
al	O
,	O
2014	O
)	O
,	O
SCAN	B-DatasetName
is	O
∼1.9×	O
faster	O
than	O
UNITER	B-MethodName
.	O
Across	O
all	O
settings	O
,	O
LightningDOT	O
is	O
significantly	O
faster	O
than	O
both	O
cross	O
-	O
attention	O
methods	O
(	O
UNITER	B-MethodName
-	O
base	O
and	O
SCAN	B-DatasetName
)	O
.	O
When	O
adding	O
UNITER	B-MethodName
-	O
base	O
as	O
the	O
re	O
-	O
ranker	O
,	O
our	O
method	O
slows	O
down	O
by	O
∼10	O
,	O
but	O
still	O
achieves	O
decent	O
speedup	O
.	O

We	O
show	O
several	O
qualitative	O
results	O
of	O
image	B-TaskName
retrieval	I-TaskName
(	O
top	O
-	O
10	O
)	O
.	O
All	O
results	O
are	O
retrieved	O
from	O
COCO	B-DatasetName
-	O
Full	O
dataset	O
(	O
123k	O
images	O
in	O
total	O
)	O
.	O
Our	O
model	O
can	O
well	O
understand	O
the	O
underlying	O
semantic	O
meaning	O
.	O
For	O
example	O
,	O
"	O
romantic	O
"	O
only	O
appears	O
twice	O
in	O
the	O
whole	O
COCO	B-DatasetName
dataset	O
annotations	O
,	O
yet	O
the	O
top	O
retrieved	O
images	O
are	O
all	O
topic	O
-	O
related	O
(	O
Figure	O
5	O
)	O
.	O
With	O
multiple	O
keywords	O
,	O
our	O
model	O
attempts	O
to	O
retrieve	O
the	O
combinations	O
of	O
them	O
(	O
if	O
not	O
all	O
)	O
.	O
For	O
example	O
,	O
for	O
the	O
query	O
"	O
blue	O
girl	O
boy	O
ball	O
"	O
with	O
four	O
keywords	O
,	O
our	O
model	O
retrieves	O
images	O
Figure	O
5	O
:	O
Retrieved	O
top	O
-	O
10	O
images	O
for	O
query	O
"	O
romantic	O
"	O
.	O
Figure	O
6	O
:	O
Retrieved	O
top	O
-	O
10	O
images	O
for	O
query	O
"	O
blue	O
girl	O
boy	O
ball	O
"	O
that	O
capture	O
at	O
least	O
three	O
keywords	O
(	O
Figure	O
6	O
)	O
.	O
We	O
also	O
present	O
image	B-TaskName
retrieval	I-TaskName
results	O
where	O
the	O
text	O
query	O
is	O
sampled	O
from	O
COCO	B-DatasetName
dataset	O
.	O
We	O
randomly	O
sample	O
3	O
queries	O
and	O
present	O
the	O
results	O
as	O
below	O
(	O
ground	O
truth	O
on	O
the	O
top	O
,	O
retrieved	O
top	O
-	O
10	O
images	O
at	O
the	O
bottom	O
)	O
.	O
Clearly	O
,	O
our	O
model	O
retrieves	O
related	O
images	O
from	O
the	O
full	O
dataset	O
.	O

Back	O
in	O
1997	O
,	O
Hearst	O
tried	O
to	O
detect	O
the	O
structure	O
of	O
text	O
using	O
patterns	O
of	O
lexical	O
co	O
-	O
occurrence	O
to	O
identify	O
paragraphs	O
related	O
to	O
the	O
same	O
topic	O
(	O
Hearst	O
,	O
1997	O
)	O
.	O
In	O
this	O
case	O
,	O
term	O
repetition	O
proved	O
to	O
be	O
enough	O
to	O
detect	O
subtopics	O
in	O
explanatory	O
texts	O
,	O
but	O
did	O
not	O
include	O
consideration	O
about	O
other	O
traits	O
of	O
the	O
discourse	O
(	O
e.g.	O
syntactic	O
constructions	O
,	O
verb	O
tenses	O
,	O
number	O
of	O
adjectives	O
in	O
each	O
region	O
)	O
neither	O
recovering	O
more	O
meaning	O
further	O
than	O
topic	O
identification	O
,	O
as	O
could	O
be	O
the	O
purpose	O
intended	O
on	O
the	O
paragraph	O
(	O
s	O
)	O
.	O
Besides	O
,	O
the	O
author	O
remarked	O
that	O
the	O
results	O
had	O
proved	O
highly	O
valuable	O
when	O
applied	O
to	O
explanatory	O
text	O
,	O
but	O
they	O
would	O
be	O
less	O
significant	O
for	O
other	O
text	O
types	O
.	O
From	O
another	O
point	O
of	O
view	O
,	O
Bachand	O
(	O
Bachand	O
et	O
al	O
,	O
2014	O
)	O
develops	O
a	O
research	O
focused	O
on	O
the	O
relations	O
between	O
text	O
-	O
type	O
,	O
discourse	O
structures	O
and	O
rhetorical	O
relations	O
.	O
Again	O
,	O
the	O
experiments	O
conducted	O
are	O
implemented	O
on	O
a	O
single	O
type	O
of	O
feature	O
,	O
this	O
time	O
rhetorical	O
relations	O
and	O
markers	O
.	O
The	O
good	O
results	O
obtained	O
by	O
the	O
author	O
indicate	O
that	O
our	O
approach	O
,	O
which	O
is	O
grounded	O
in	O
similar	O
intuitions	O
,	O
can	O
reach	O
comparable	O
developments	O
that	O
we	O
expect	O
will	O
enrich	O
our	O
capacity	O
for	O
generating	O
accurate	O
document	O
plans	O
.	O
Regarding	O
reviews	O
,	O
most	O
of	O
the	O
work	O
developed	O
refers	O
to	O
sentiment	B-TaskName
analysis	I-TaskName
or	O
polarity	O
classifica	O
-	O
tion	O
(	O
Cambria	O
et	O
al	O
,	O
2013	O
)	O
.	O
A	O
few	O
research	O
works	O
have	O
been	O
focused	O
on	O
the	O
structure	O
related	O
to	O
textual	O
genres	O
,	O
relying	O
on	O
the	O
Systemic	O
Functional	O
Theory	O
(	O
Taboada	O
,	O
2011	O
)	O
.	O
The	O
relations	O
of	O
different	O
parts	O
of	O
the	O
text	O
with	O
several	O
purposes	O
are	O
revealed	O
,	O
focusing	O
their	O
analysis	O
on	O
the	O
domain	O
of	O
movie	O
reviews	O
,	O
and	O
showing	O
at	O
the	O
same	O
time	O
the	O
variability	O
of	O
the	O
ordering	O
in	O
such	O
type	O
of	O
documents	O
.	O
Finally	O
,	O
a	O
special	O
mention	O
must	O
be	O
done	O
to	O
the	O
Systemic	O
Functional	O
Theory	O
(	O
Halliday	O
et	O
al	O
,	O
2014	O
)	O
.	O
It	O
provides	O
a	O
notion	O
of	O
genre	O
that	O
connects	O
situation	O
types	O
with	O
semantic	O
/	O
lexico	O
-	O
grammatic	O
patterns	O
from	O
a	O
conception	O
of	O
language	O
highly	O
related	O
to	O
its	O
socio	O
-	O
semiotic	O
origin	O
.	O
A	O
textual	O
typology	O
is	O
depicted	O
on	O
this	O
terms	O
,	O
connected	O
as	O
well	O
with	O
the	O
context	O
of	O
the	O
discourse	O
and	O
the	O
semantic	O
choices	O
to	O
organise	O
it	O
.	O
On	O
the	O
other	O
hand	O
,	O
and	O
as	O
a	O
more	O
precise	O
example	O
,	O
the	O
typology	O
of	O
processes	O
that	O
Halliday	O
and	O
Mathiessen	O
describe	O
,	O
directly	O
influences	O
the	O
classification	O
accomplished	O
by	O
ADESSE	O
,	O
one	O
of	O
the	O
resources	O
applied	O
in	O
our	O
experiments	O
over	O
Spanish	O
reviews	O
,	O
explained	O
in	O
the	O
next	O
section	O
.	O

We	O
believe	O
that	O
,	O
in	O
order	O
to	O
become	O
more	O
meaningful	O
,	O
the	O
quality	O
of	O
features	O
could	O
be	O
improved	O
by	O
means	O
of	O
some	O
resources	O
rooted	O
in	O
Web	O
Semantic	O
technologies	O
.	O
There	O
is	O
some	O
research	O
related	O
to	O
genres	O
that	O
can	O
be	O
useful	O
in	O
our	O
project	O
.	O
In	O
the	O
ADESSE	O
verb	O
senses	O
Mental	O
,	O
material	O
,	O
relational	O
,	O
verbal	O
,	O
existential	O
and	O
modulation	O
FREELING	O
features	O
PoS	O
tagging	O
:	O
noun	O
,	O
adjective	O
,	O
pronoun	O
,	O
verb	O
(	O
tense	O
,	O
aspect	O
,	O
...	O
)	O
,	O
etc	O
.	O
realm	O
of	O
reviews	O
,	O
opinion	O
and	O
sentiment	O
annotation	O
,	O
we	O
can	O
take	O
advantage	O
for	O
example	O
of	O
MARL	O
Ontology	B-MethodName
Specification	O
1	O
,	O
a	O
data	O
schema	O
that	O
has	O
been	O
used	O
in	O
the	O
EuroSentiment	O
Project	O
(	O
Buitelaar	O
et	O
al	O
,	O
2013	O
)	O
or	O
directly	O
related	O
to	O
reviews	O
from	O
a	O
Sentiment	B-TaskName
Analysis	I-TaskName
perspective	O
(	O
Santosh	O
and	O
Vardhan	O
,	O
2015	O
)	O
.	O
Other	O
genres	O
have	O
been	O
targeted	O
for	O
similar	O
developments	O
.	O
With	O
regard	O
to	O
news	O
genre	O
,	O
in	O
order	O
to	O
obtain	O
more	O
significant	O
annotation	O
of	O
the	O
documents	O
,	O
BBC	O
provides	O
a	O
set	O
of	O
ontologies	O
related	O
to	O
their	O
contents	O
.	O
DBPedia	B-DatasetName
has	O
been	O
already	O
proved	O
useful	O
for	O
Wikipedia	O
articles	O
researchers	O
.	O
Drammar	O
(	O
Lombardo	O
and	O
Damiano	O
,	O
2012	O
)	O
and	O
OntoMedia	O
(	O
Jewell	O
et	O
al	O
,	O
2005	O
)	O
are	O
ontology	B-MethodName
-	O
based	O
models	O
for	O
annotating	O
features	O
of	O
media	O
and	O
cultural	O
narratives	O
.	O
All	O
of	O
them	O
represent	O
resources	O
that	O
may	O
lead	O
to	O
different	O
results	O
in	O
our	O
clustering	O
task	O
and	O
analysis	O
.	O

Pre	O
-	O
trained	O
language	O
models	O
have	O
been	O
found	O
to	O
capture	O
a	O
surprisingly	O
rich	O
amount	O
of	O
lexical	O
knowledge	O
,	O
ranging	O
from	O
commonsense	O
properties	O
of	O
everyday	O
concepts	O
to	O
detailed	O
factual	O
knowledge	O
about	O
named	O
entities	O
.	O
Among	O
others	O
,	O
this	O
makes	O
it	O
possible	O
to	O
distill	O
high	O
-	O
quality	O
word	O
vectors	O
from	O
pre	O
-	O
trained	O
language	O
models	O
.	O
However	O
,	O
it	O
is	O
currently	O
unclear	O
to	O
what	O
extent	O
it	O
is	O
possible	O
to	O
distill	O
relation	O
embeddings	O
,	O
i.e.	O
vectors	O
that	O
characterize	O
the	O
relationship	O
between	O
two	O
words	O
.	O
Such	O
relation	O
embeddings	O
are	O
appealing	O
because	O
they	O
can	O
,	O
in	O
principle	O
,	O
encode	O
relational	O
knowledge	O
in	O
a	O
more	O
finegrained	O
way	O
than	O
is	O
possible	O
with	O
knowledge	B-TaskName
graphs	I-TaskName
.	O
To	O
obtain	O
relation	O
embeddings	O
from	O
a	O
pre	O
-	O
trained	O
language	O
model	O
,	O
we	O
encode	O
word	O
pairs	O
using	O
a	O
(	O
manually	O
or	O
automatically	O
generated	O
)	O
prompt	O
,	O
and	O
we	O
fine	O
-	O
tune	O
the	O
language	O
model	O
such	O
that	O
relationally	O
similar	O
word	O
pairs	O
yield	O
similar	O
output	O
vectors	O
.	O
We	O
find	O
that	O
the	O
resulting	O
relation	O
embeddings	O
are	O
highly	O
competitive	O
on	O
analogy	O
(	O
unsupervised	O
)	O
and	O
relation	B-TaskName
classification	I-TaskName
(	O
supervised	O
)	O
benchmarks	O
,	O
even	O
without	O
any	O
task	O
-	O
specific	O
fine	O
-	O
tuning	O
.	O
1	O

One	O
of	O
the	O
most	O
widely	O
studied	O
aspects	O
of	O
word	B-TaskName
embeddings	I-TaskName
is	O
the	O
fact	O
that	O
word	O
vector	O
differences	O
capture	O
lexical	O
relations	O
(	O
Mikolov	O
et	O
al	O
,	O
2013a	O
)	O
.	O
While	O
not	O
being	O
directly	O
connected	O
to	O
downstream	O
performance	O
on	O
NLP	O
tasks	O
,	O
this	O
ability	O
of	O
word	B-TaskName
embeddings	I-TaskName
is	O
nonetheless	O
important	O
.	O
For	O
instance	O
,	O
understanding	O
lexical	O
relations	O
is	O
an	O
important	O
prerequisite	O
for	O
understanding	O
the	O
meaning	O
of	O
compound	O
nouns	O
.	O
Moreover	O
,	O
the	O
ability	O
of	O
word	O
vectors	O
to	O
capture	O
semantic	O
relations	O
has	O
enabled	O
a	O
wide	O
range	O
of	O
applications	O
beyond	O
NLP	O
,	O
including	O
flexible	O
querying	O
of	O
relational	O
databases	O
(	O
Bordawekar	O
and	O
Shmueli	O
,	O
2017	O
)	O
,	O
schema	O
match	O
-	O
1	O
Source	O
code	O
to	O
reproduce	O
our	O
experimental	O
results	O
and	O
the	O
model	O
checkpoints	O
are	O
available	O
in	O
the	O
following	O
repository	O
:	O
https://github.com/asahi417/relbert	O
ing	O
(	O
Fernandez	O
et	O
al	O
,	O
2018	O
)	O
,	O
completion	O
and	O
retrieval	O
of	O
Web	O
tables	O
(	O
Zhang	O
et	O
al	O
,	O
2019	O
)	O
,	O
ontology	B-MethodName
completion	O
(	O
Bouraoui	O
and	O
Schockaert	O
,	O
2019	O
)	O
and	O
information	B-TaskName
retrieval	I-TaskName
in	O
the	O
medical	B-DatasetName
domain	I-DatasetName
(	O
Arguello	O
Casteleiro	O
et	O
al	O
,	O
2020	O
)	O
.	O
More	O
generally	O
,	O
relational	O
similarity	O
(	O
or	O
analogy	O
)	O
plays	O
a	O
central	O
role	O
in	O
computational	O
creativity	O
(	O
Goel	O
,	O
2019	O
)	O
,	O
legal	O
reasoning	O
(	O
Ashley	O
,	O
1988	O
;	O
Walton	O
,	O
2010	O
)	O
,	O
ontology	B-MethodName
alignment	O
(	O
Raad	O
and	O
Evermann	O
,	O
2015	O
)	O
and	O
instance	O
-	O
based	O
learning	O
(	O
Miclet	O
et	O
al	O
,	O
2008	O
)	O
.	O
Given	O
the	O
recent	O
success	O
of	O
pre	O
-	O
trained	O
language	O
models	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
;	O
Liu	O
et	O
al	O
,	O
2019	O
;	O
Brown	O
et	O
al	O
,	O
2020	O
)	O
,	O
we	O
may	O
wonder	O
whether	O
such	O
models	O
are	O
able	O
to	O
capture	O
lexical	O
relations	O
in	O
a	O
more	O
faithful	O
or	O
fine	O
-	O
grained	O
way	O
than	O
traditional	O
word	B-TaskName
embeddings	I-TaskName
.	O
However	O
,	O
for	O
language	O
models	O
(	O
LMs	O
)	O
,	O
there	O
is	O
no	O
direct	O
equivalent	O
to	O
the	O
word	O
vector	O
difference	O
.	O
In	O
this	O
paper	O
,	O
we	O
therefore	O
propose	O
a	O
strategy	O
for	O
extracting	O
relation	O
embeddings	O
from	O
pre	O
-	O
trained	O
LMs	O
,	O
i.e.	O
vectors	O
encoding	O
the	O
relationship	O
between	O
two	O
words	O
.	O
On	O
the	O
one	O
hand	O
,	O
this	O
will	O
allow	O
us	O
to	O
gain	O
a	O
better	O
understanding	O
of	O
how	O
well	O
lexical	O
relations	O
are	O
captured	O
by	O
these	O
models	O
.	O
On	O
the	O
other	O
hand	O
,	O
this	O
will	O
also	O
provide	O
us	O
with	O
a	O
practical	O
method	O
for	O
obtaining	O
relation	O
embeddings	O
in	O
applications	O
such	O
as	O
the	O
ones	O
mentioned	O
above	O
.	O
Since	O
it	O
is	O
unclear	O
how	O
LMs	O
store	O
relational	O
knowledge	O
,	O
rather	O
than	O
directly	O
extracting	O
relation	O
embeddings	O
,	O
we	O
first	O
fine	O
-	O
tune	O
the	O
LM	O
,	O
such	O
that	O
relation	O
embeddings	O
can	O
be	O
obtained	O
from	O
its	O
output	O
.	O
To	O
this	O
end	O
,	O
we	O
need	O
a	O
prompt	O
,	O
i.e.	O
a	O
template	O
to	O
convert	O
a	O
given	O
word	O
pair	O
into	O
a	O
sentence	O
,	O
and	O
some	O
training	O
data	O
to	O
fine	O
-	O
tune	O
the	O
model	O
.	O
To	O
illustrate	O
the	O
process	O
,	O
consider	O
the	O
word	O
pair	O
Paris	O
-	O
France	O
.	O
As	O
a	O
possible	O
input	O
to	O
the	O
model	O
,	O
we	O
could	O
use	O
a	O
sentence	O
such	O
as	O
"	O
The	O
relation	O
between	O
Paris	O
and	O
France	O
is	O
<	O
mask	O
>	O
"	O
.	O
Note	O
that	O
our	O
aim	O
is	O
to	O
find	O
a	O
strategy	O
that	O
can	O
be	O
applied	O
to	O
any	O
pair	O
of	O
words	O
,	O
hence	O
the	O
way	O
in	O
which	O
the	O
input	O
is	O
represented	O
needs	O
to	O
be	O
sufficiently	O
generic	O
.	O
We	O
then	O
fine	O
-	O
tune	O
the	O
LM	O
such	O
that	O
its	O
output	O
corresponds	O
to	O
a	O
relation	O
embedding	O
.	O
To	O
this	O
end	O
,	O
we	O
use	O
a	O
crowdsourced	O
dataset	O
of	O
relational	O
similarity	O
judgements	O
that	O
was	O
collected	O
in	O
the	O
context	O
of	O
SemEval	O
2012	O
Task	O
2	O
(	O
Jurgens	O
et	O
al	O
,	O
2012	O
)	O
.	O
Despite	O
the	O
relatively	O
small	O
size	O
of	O
this	O
dataset	O
,	O
we	O
show	O
that	O
the	O
resulting	O
fine	O
-	O
tuned	O
LM	O
allows	O
us	O
to	O
produce	O
high	O
-	O
quality	O
relation	O
embeddings	O
,	O
as	O
confirmed	O
in	O
our	O
extensive	O
evaluation	O
in	O
analogy	O
and	O
relation	B-TaskName
classification	I-TaskName
tasks	O
.	O
Importantly	O
,	O
this	O
also	O
holds	O
for	O
relations	O
that	O
are	O
of	O
a	O
different	O
nature	O
than	O
those	O
in	O
the	O
SemEval	O
dataset	O
,	O
showing	O
that	O
this	O
process	O
allows	O
us	O
to	O
distill	O
relational	O
knowledge	O
that	O
is	O
encoded	O
in	O
the	O
pre	O
-	O
trained	O
LM	O
,	O
rather	O
than	O
merely	O
generalising	O
from	O
the	O
examples	O
that	O
were	O
used	O
for	O
fine	O
-	O
tuning	O
.	O

In	O
this	O
section	O
,	O
we	O
present	O
our	O
main	O
experimental	O
results	O
,	O
testing	O
the	O
relation	O
embeddings	O
learned	O
by	O
RelBERT	O
on	O
analogy	O
questions	O
(	O
Section	O
5.1	O
)	O
and	O
relation	B-TaskName
classification	I-TaskName
(	O
Section	O
5.2	O
)	O
.	O

In	O
our	O
main	O
experiments	O
,	O
RelBERT	O
is	O
trained	O
using	O
the	O
SemEval	O
2012	O
Task	O
2	O
dataset	O
.	O
This	O
dataset	O
contains	O
a	O
broad	O
range	O
of	O
semantic	O
relations	O
,	O
including	O
hypernymy	O
and	O
meronymy	O
relations	O
.	O
This	O
raises	O
an	O
important	O
question	O
:	O
Does	O
RelBERT	O
provide	O
us	O
with	O
a	O
way	O
to	O
extract	O
relational	O
knowledge	O
from	O
the	O
parameters	O
of	O
the	O
As	O
a	O
further	O
analysis	O
,	O
Table	O
5	O
shows	O
a	O
breakdown	O
of	O
the	O
Google	B-DatasetName
and	O
BATS	O
analogy	O
results	O
,	O
showing	O
the	O
average	O
performance	O
on	O
each	O
of	O
the	O
top	O
-	O
level	O
categories	O
from	O
these	O
datasets	O
.	O
10	O
While	O
RelBERT	O
is	O
outperformed	O
by	O
FastText	B-MethodName
on	O
the	O
morphological	O
relations	O
,	O
it	O
should	O
be	O
noted	O
that	O
the	O
differences	O
are	O
small	O
,	O
while	O
such	O
relations	O
are	O
of	O
a	O
very	O
different	O
nature	O
than	O
those	O
from	O
the	O
SemEval	O
dataset	O
.	O
This	O
confirms	O
that	O
RelBERT	O
is	O
able	O
to	O
model	O
a	O
broad	O
range	O
of	O
relations	O
,	O
although	O
it	O
can	O
be	O
expected	O
that	O
better	O
results	O
would	O
be	O
possible	O
by	O
including	O
task	O
-	O
specific	O
training	O
data	O
into	O
the	O
fine	O
-	O
tuning	O
process	O
(	O
e.g.	O
including	O
morphological	O
relations	O
for	O
tasks	O
where	O
such	O
relations	O
matter	O
)	O
.	O

Nearest	O
Neighbors	O
barista	O
:	O
coffee	O
baker	O
:	O
bread	O
,	O
brewer	O
:	O
beer	O
,	O
bartender	O
:	O
cocktail	O
,	O
winemaker	O
:	O
wine	O
,	O
bartender	O
:	O
drink	O
,	O
baker	O
:	O
cake	O
bag	O
:	O
plastic	O
bottle	O
:	O
plastic	O
,	O
bag	O
:	O
leather	O
,	O
container	O
:	O
plastic	O
,	O
box	O
:	O
plastic	O
,	O
jug	O
:	O
glass	O
,	O
bottle	O
:	O
glass	O
duck	O
:	O
duckling	O
chicken	O
:	O
chick	O
,	O
pig	O
:	O
piglet	O
,	O
cat	O
:	O
kitten	O
,	O
ox	O
:	O
calf	O
,	O
butterfly	O
:	O
larvae	O
,	O
bear	O
:	O
cub	O
cooked	O
:	O
raw	O
raw	O
:	O
cooked	O
,	O
regulated	O
:	O
unregulated	O
,	O
sober	O
:	O
drunk	O
,	O
loaded	O
:	O
unloaded	O
,	O
armed	O
:	O
unarmed	O
,	O
published	O
:	O
unpublished	O
chihuahua	O
:	O
dog	O
dachshund	O
:	O
dog	O
,	O
poodle	O
:	O
dog	O
,	O
terrier	O
:	O
dog	O
,	O
chinchilla	B-MethodName
:	O
rodent	O
,	O
macaque	O
:	O
monkey	O
,	O
dalmatian	O
:	O
dog	O
dog	O
:	O
dogs	O
cat	O
:	O
cats	O
,	O
horse	O
:	O
horses	O
,	O
pig	O
:	O
pigs	O
,	O
rat	O
:	O
rats	O
,	O
wolf	O
:	O
wolves	O
,	O
monkey	O
:	O
monkeys	O
spy	O
:	O
espionage	O
pirate	O
:	O
piracy	O
,	O
robber	O
:	O
robbery	O
,	O
lobbyist	O
:	O
lobbying	O
,	O
scout	O
:	O
scouting	O
,	O
terrorist	O
:	O
terrorism	O
,	O
witch	O
:	O
witchcraft	O

Figure	O
3	O
compares	O
the	O
performance	O
of	O
RelBERT	O
with	O
that	O
of	O
the	O
vanilla	O
pre	O
-	O
trained	O
RoBERTa	B-MethodName
model	O
(	O
i.e.	O
when	O
only	O
the	O
prompt	O
is	O
optimized	O
)	O
.	O
As	O
can	O
be	O
seen	O
,	O
the	O
fine	O
-	O
tuning	O
process	O
is	O
critical	O
for	O
achieving	O
good	O
results	O
.	O
In	O
Figure	O
3	O
,	O
we	O
also	O
compare	O
the	O
performance	O
of	O
our	O
main	O
RelBERT	O
model	O
,	O
which	O
is	O
based	O
on	O
RoBERTa	B-MethodName
,	O
with	O
versions	O
that	O
were	O
instead	O
initialized	O
with	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
and	O
ALBERT	B-MethodName
(	O
Lan	O
et	O
al	O
,	O
2019	O
)	O
.	O
11	O
RoBERTa	B-MethodName
clearly	O
outperforms	O
the	O
other	O
two	O
LMs	O
,	O
which	O
is	O
in	O
accordance	O
with	O
findings	O
from	O
the	O
literature	O
suggesting	O
that	O
RoBERTa	B-MethodName
captures	O
more	O
semantic	O
knowledge	O
Warstadt	O
et	O
al	O
,	O
2020	O
)	O
.	O

To	O
give	O
further	O
insight	O
into	O
the	O
nature	O
of	O
RelBERT	O
embeddings	O
,	O
Table	O
6	O
shows	O
the	O
nearest	O
neighbors	O
of	O
some	O
selected	O
word	O
pairs	O
from	O
the	O
evaluation	O
datasets	O
.	O
To	O
this	O
end	O
,	O
we	O
computed	O
RelBERT	O
relation	O
vectors	O
for	O
all	O
pairs	O
in	O
the	O
Wikipedia	O
pretrained	O
RELATIVE	O
vocabulary	O
(	O
over	O
1	O
M	O
pairs	O
)	O
.	O
12	O
The	O
neighbors	O
are	O
those	O
word	O
pairs	O
whose	O
Rel	O
-	O
BERT	B-MethodName
embedding	O
has	O
the	O
highest	O
cosine	O
similarity	O
within	O
the	O
full	O
pair	O
vocabulary	O
.	O
As	O
can	O
be	O
seen	O
,	O
the	O
neighbors	O
mostly	O
represent	O
word	O
pairs	O
that	O
are	O
relationally	O
similar	O
,	O
even	O
for	O
morphological	O
relations	O
(	O
e.g.	O
dog	O
:	O
dogs	O
)	O
,	O
which	O
are	O
not	O
present	O
in	O
the	O
SemEval	O
dataset	O
.	O
A	O
more	O
extensive	O
qualitative	O
analysis	O
,	O
including	O
a	O
comparison	O
with	O
RELATIVE	O
,	O
is	O
provided	O
in	O
the	O
appendix	O
.	O

We	O
have	O
proposed	O
a	O
strategy	O
for	O
learning	O
relation	O
embeddings	O
,	O
i.e.	O
vector	O
representations	O
of	O
pairs	O
of	O
words	O
which	O
capture	O
their	O
relationship	O
.	O
The	O
main	O
idea	O
is	O
to	O
fine	O
-	O
tune	O
a	O
pre	O
-	O
trained	O
language	O
model	O
using	O
the	O
relational	O
similarity	O
dataset	O
from	O
SemEval	O
2012	O
Task	O
2	O
,	O
which	O
covers	O
a	O
broad	O
range	O
of	O
semantic	O
relations	O
.	O
In	O
our	O
experimental	O
results	O
,	O
we	O
found	O
the	O
resulting	O
relation	O
embeddings	O
to	O
be	O
of	O
high	O
quality	O
,	O
outperforming	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
on	O
several	O
analogy	O
and	O
relation	B-TaskName
classification	I-TaskName
benchmarks	O
.	O
Among	O
the	O
models	O
tested	O
,	O
we	O
obtained	O
the	O
best	O
results	O
with	O
RoBERTa	B-MethodName
,	O
when	O
using	O
manually	O
defined	O
templates	O
for	O
encoding	O
word	O
pairs	O
.	O
Importantly	O
,	O
we	O
found	O
that	O
high	O
-	O
quality	O
relation	O
embeddings	O
can	O
be	O
obtained	O
even	O
for	O
relations	O
that	O
are	O
unlike	O
those	O
from	O
the	O
SemEval	O
dataset	O
,	O
such	O
as	O
morphological	O
and	O
encyclopedic	O
relations	O
.	O
This	O
suggests	O
that	O
the	O
knowledge	O
captured	O
by	O
our	O
relation	O
embeddings	O
is	O
largely	O
distilled	O
from	O
the	O
pre	O
-	O
trained	O
language	O
model	O
,	O
rather	O
than	O
being	O
acquired	O
during	O
training	O
.	O

Table	O
9	O
shows	O
additional	O
results	O
of	O
word	B-TaskName
embeddings	I-TaskName
on	O
analogy	O
test	O
together	O
with	O
RelBERT	O
results	O
.	O
We	O
concatenate	O
the	O
RELATIVE	O
and	O
pair2vec	O
vectors	O
with	O
the	O
word	O
vector	O
difference	O
.	O
However	O
,	O
this	O
does	O
not	O
lead	O
to	O
better	O
results	O
.	O

Table	O
11	O
shows	O
the	O
best	O
hyperparameters	O
in	O
the	O
validation	O
set	O
of	O
the	O
MLPs	O
for	O
relation	B-TaskName
classification	I-TaskName
.	O

Towards	O
Unsupervised	B-TaskName
Text	I-TaskName
Classification	I-TaskName
Leveraging	O
Experts	O
and	O
Word	B-TaskName
Embeddings	I-TaskName

Document	B-TaskName
classification	I-TaskName
is	O
a	O
standard	O
task	O
in	O
machine	O
learning	O
(	O
Joachims	O
,	O
1999	O
;	O
Sebastiani	O
,	O
2002	O
)	O
.	O
Its	O
applications	O
span	O
a	O
variety	O
of	O
"	O
use	O
cases	O
and	O
contexts	O
,	O
e.g.	O
,	O
email	O
filtering	O
,	O
news	O
article	O
clustering	O
,	O
clinical	O
document	B-TaskName
classification	I-TaskName
,	O
expertquestion	O
matching	O
"	O
.	O
The	O
standard	O
process	O
for	O
text	B-TaskName
categorization	I-TaskName
relies	O
on	O
supervised	O
and	O
semisupervised	O
approaches	O
.	O
The	O
motivation	O
for	O
the	O
present	O
effort	O
comes	O
from	O
the	O
banking	O
sector	O
,	O
in	O
particular	O
the	O
management	O
of	O
operational	O
risks	O
.	O
This	O
category	O
of	O
risks	O
corresponds	O
to	O
the	O
broad	O
set	O
of	O
incidents	O
that	O
are	O
neither	O
credit	O
nor	O
market	O
risk	O
and	O
includes	O
issues	O
related	O
to	O
internal	O
and	O
external	O
fraud	O
,	O
cybersecurity	O
,	O
damages	O
on	O
physical	O
assets	O
,	O
natural	O
disasters	O
,	O
etc	O
.	O
The	O
practical	O
management	O
of	O
operational	O
risk	O
is	O
partially	O
based	O
on	O
the	O
management	O
of	O
a	O
dataset	O
of	O
historical	O
operational	O
risk	O
incidents	O
where	O
each	O
incident	O
is	O
described	O
in	O
details	O
and	O
that	O
is	O
shared	O
on	O
a	O
regular	O
basis	O
with	O
regulators	O
.	O
Historically	O
,	O
all	O
incident	O
reports	O
have	O
been	O
mapped	O
to	O
about	O
twenty	O
categories	O
of	O
risk	O
issued	O
from	O
the	O
regulator	O
.	O
However	O
,	O
from	O
an	O
operational	O
perspective	O
,	O
a	O
higher	O
number	O
of	O
risk	O
categories	O
is	O
relevant	O
to	O
better	O
capture	O
the	O
nuances	O
around	O
the	O
incidents	O
and	O
enable	O
relevant	O
comparisons	O
.	O
This	O
led	O
to	O
the	O
creation	O
of	O
a	O
new	O
internal	O
risk	O
taxonomy	O
of	O
risk	O
composed	O
of	O
264	O
categories	O
,	O
each	O
described	O
by	O
a	O
label	O
(	O
a	O
few	O
words	O
)	O
.	O
To	O
make	O
it	O
operational	O
,	O
the	O
stock	O
of	O
all	O
internal	O
and	O
external	O
incident	O
reports	O
had	O
to	O
be	O
classified	O
into	O
categories	O
from	O
the	O
new	O
internal	O
taxonomy	O
.	O
However	O
,	O
since	O
it	O
had	O
never	O
been	O
used	O
before	O
,	O
we	O
had	O
no	O
labeled	O
samples	O
readily	O
available	O
.	O
As	O
hundreds	O
of	O
thousands	O
of	O
incidents	O
had	O
to	O
be	O
processed	O
,	O
text	B-TaskName
classification	I-TaskName
seemed	O
a	O
promising	O
approach	O
to	O
assist	O
in	O
that	O
mapping	O
task	O
.	O
Indeed	O
,	O
given	O
the	O
specificity	O
of	O
the	O
domain	O
and	O
the	O
lack	O
of	O
availability	O
of	O
experts	O
,	O
it	O
was	O
not	O
conceivable	O
to	O
obtain	O
many	O
labeled	O
examples	O
for	O
each	O
category	O
as	O
would	O
be	O
required	O
for	O
supervised	O
approaches	O
.	O
This	O
is	O
the	O
issue	O
addressed	O
in	O
this	O
paper	O
where	O
describe	O
our	O
work	O
towards	O
an	O
unsupervised	O
approach	O
to	O
classify	O
documents	O
into	O
a	O
set	O
of	O
categories	O
described	O
by	O
a	O
short	O
sentence	O
(	O
label	O
)	O
.	O
While	O
the	O
inspiration	O
of	O
this	O
paper	O
is	O
the	O
classification	O
of	O
incident	O
reports	O
in	O
operational	O
risk	O
,	O
our	O
approach	O
aims	O
to	O
be	O
readily	O
transferable	O
to	O
other	O
domains	O
.	O
For	O
that	O
purpose	O
,	O
we	O
tested	O
it	O
on	O
standard	O
text	B-TaskName
classification	I-TaskName
corpora	O
.	O
The	O
underlying	O
idea	O
is	O
altogether	O
simple	O
.	O
We	O
emulate	O
the	O
approach	O
that	O
a	O
domain	O
expert	O
would	O
follow	O
to	O
manually	O
assign	O
an	O
input	O
document	O
(	O
incident	O
report	O
,	O
client	O
review	O
,	O
news	O
article	O
,	O
etc	O
.	O
)	O
to	O
a	O
given	O
category	O
.	O
Specifically	O
this	O
entails	O
developing	O
an	O
understanding	O
of	O
the	O
categories	O
semantic	O
fields	O
and	O
then	O
,	O
for	O
each	O
document	O
,	O
to	O
classify	O
it	O
into	O
the	O
closest	O
category	O
.	O
The	O
novelty	O
of	O
our	O
method	O
hinges	O
on	O
the	O
diversity	O
of	O
enrichment	O
techniques	O
of	O
the	O
categories	O
label	O
,	O
including	O
expert	O
input	O
that	O
assists	O
the	O
semantic	O
expansion	O
and	O
the	O
use	O
of	O
word	B-TaskName
embeddings	I-TaskName
,	O
both	O
generic	O
and	O
domain	O
specific	O
.	O
The	O
remainder	O
of	O
this	O
paper	O
is	O
organized	O
as	O
follows	O
.	O
In	O
Section	O
2	O
,	O
we	O
provide	O
an	O
overview	O
of	O
the	O
relevant	O
literature	O
.	O
Section	O
3	O
contains	O
a	O
detailed	O
description	O
of	O
our	O
approach	O
.	O
Sections	O
4	O
and	O
5	O
describe	O
the	O
results	O
of	O
its	O
application	O
to	O
standard	O
corpora	O
and	O
operational	O
risks	O
incidents	O
respectively	O
.	O
We	O
conclude	O
in	O
Section	O
6	O
.	O

Our	O
approach	O
for	O
unsupervised	B-TaskName
text	I-TaskName
classification	I-TaskName
is	O
based	O
on	O
the	O
choice	O
to	O
model	O
the	O
task	O
as	O
a	O
text	B-TaskName
similarity	I-TaskName
problem	O
between	O
two	O
sets	O
of	O
words	O
:	O
One	O
containing	O
the	O
most	O
relevant	O
words	O
in	O
the	O
document	O
and	O
another	O
containing	O
keywords	O
derived	O
from	O
the	O
label	O
of	O
the	O
target	O
category	O
.	O
While	O
the	O
key	O
advantage	O
of	O
this	O
approach	O
is	O
its	O
simplicity	O
,	O
its	O
success	O
hinges	O
on	O
the	O
good	O
definition	O
of	O
a	O
dictionary	O
of	O
words	O
for	O
each	O
category	O
.	O
Figure	O
1	O
provides	O
an	O
overview	O
of	O
the	O
main	O
steps	O
included	O
in	O
our	O
method	O
.	O
On	O
the	O
document	O
side	O
,	O
we	O
simply	O
perform	O
standard	O
cleaning	O
steps	O
.	O
On	O
the	O
category	O
labels	O
side	O
,	O
besides	O
the	O
same	O
initial	O
processing	O
,	O
we	O
implement	O
a	O
series	O
of	O
enrichment	O
steps	O
so	O
as	O
to	O
iteratively	O
expand	O
label	O
dictionaries	O
.	O
Before	O
proceeding	O
to	O
the	O
comparison	O
of	O
documents	O
and	O
labels	O
via	O
a	O
similarity	O
metric	O
,	O
we	O
have	O
added	O
a	O
consolidation	O
step	O
which	O
considers	O
all	O
expanded	O
label	O
dictionaries	O
and	O
makes	O
adjustments	O
so	O
that	O
they	O
are	O
as	O
discriminating	O
as	O
possible	O
.	O
We	O
compare	O
documents	O
and	O
labels	O
by	O
computing	O
a	O
similarity	O
metric	O
between	O
cleaned	O
documents	O
and	O
dictionaries	O
.	O
We	O
provide	O
further	O
details	O
into	O
each	O
of	O
these	O
main	O
steps	O
in	O
the	O
following	O
subsections	O
.	O
In	O
terms	O
of	O
notation	O
,	O
we	O
refer	O
to	O
the	O
unlabeled	O
corpus	O
as	O
C	O
,	O
its	O
vocabulary	O
as	O
V	O
and	O
and	O
assume	O
that	O
we	O
have	O
M	O
text	O
categories	O
to	O
which	O
documents	O
in	O
C	O
need	O
to	O
be	O
mapped	O
.	O

Cleaning	O
of	O
either	O
documents	O
or	O
category	O
labels	O
is	O
done	O
as	O
follows	O
:	O
After	O
tokenization	O
,	O
we	O
start	O
by	O
replacing	O
a	O
list	O
of	O
common	O
abbreviations	O
,	O
e.g.	O
,	O
Mgt	O
,	O
Mngt	O
,	O
IT	O
,	O
ATM	O
provided	O
by	O
business	O
with	O
their	O
associated	O
expansions	O
.	O
Similarly	O
we	O
spell	O
out	O
negative	O
contractions	O
.	O
We	O
then	O
remove	O
uninformative	O
tokens	O
including	O
(	O
i	O
)	O
isolated	O
and	O
special	O
characters	O
such	O
as	O
i	O
,	O
a	O
,	O
o	O
,	O
op	O
,	O
@	O
,	O
*	O
,	O
(	O
ii	O
)	O
punctuation	O
(	O
iii	O
)	O
stopwords	O
(	O
based	O
on	O
stopword	O
lists	O
from	O
NLTK	O
's	O
list	O
of	O
english	O
stopwords	O
,	O
scikit	O
-	O
learn	O
version	O
0.18.2	O
,	O
spaCy	O
version	O
1.8.2	O
)	O
(	O
iv	O
)	O
common	O
words	O
across	O
documents	O
such	O
as	O
risky	O
,	O
dangerous	O
,	O
based	O
on	O
the	O
highest	O
Term	O
Frequency	O
(	O
top	O
3	O
%	O
)	O
(	O
v	O
)	O
uncommon	O
words	O
,	O
i.e.	O
,	O
top	O
3	O
%	O
in	O
terms	O
of	O
Inverse	O
Term	O
Frequency	O
(	O
vi	O
)	O
specific	O
tokens	O
such	O
as	O
dates	O
,	O
nationalities	O
,	O
countries	O
,	O
regions	O
,	O
bank	O
names	O
.	O
For	O
instance	O
,	O
to	O
extract	O
dates	O
,	O
we	O
use	O
both	O
regular	O
expression	O
and	O
fuzzy	O
matching	O
to	O
identify	O
all	O
sorts	O
of	O
date	O
-	O
like	O
strings	O
(	O
e.g.	O
,	O
February	O
can	O
also	O
be	O
written	O
as	O
Feb	O
or	O
Febr	O
)	O
.	O
Regarding	O
nationalities	O
and	O
bank	O
names	O
,	O
we	O
combined	O
different	O
lists	O
coming	O
from	O
Wikipedia	O
,	O
business	O
experts	O
and	O
fuzzy	O
matching	O
(	O
e.g.	O
,	O
BNP	O
Paribas	O
could	O
be	O
found	O
as	O
BNP	O
,	O
BNPParibas	O
,	O
BNP	O
Securities	O
,	O
BNP	O
Trading	O
,	O
BNP	O
Group	O
,	O
etc	O
.	O
)	O
.	O
As	O
the	O
taxonomy	O
is	O
designed	O
to	O
be	O
universal	O
,	O
such	O
tokens	O
are	O
not	O
relevant	O
to	O
the	O
text	B-TaskName
classification	I-TaskName
task	O
and	O
are	O
thus	O
removed	O
.	O
To	O
give	O
a	O
concrete	O
example	O
,	O
the	O
following	O
snippet	O
of	O
operational	O
incident	O
"	O
On	O
18	O
June	O
2013	O
the	O
US	O
Commodity	O
Futures	O
Trading	O
Commission	O
(	O
CFTC	O
)	O
fined	O
ABN	O
AMRO	O
Clearing	O
Chicago	O
USD	O
1	O
million	O
(	O
EUR	O
748	O
,	O
000	O
)	O
for	O
failing	O
to	O
segregate	O
or	O
secure	O
sufficient	O
customer	O
funds	O
,	O
failing	O
to	O
meet	O
the	O
minimum	O
net	O
capital	O
requirements	O
,	O
failing	O
to	O
maintain	O
accurate	O
books	O
and	O
records	O
,	O
and	O
failing	O
to	O
supervise	O
its	O
employees	O
...	O
"	O
would	O
have	O
been	O
transformed	O
into	O
"	O
fine	O
fail	O
segreg	O
secur	O
suffici	O
custom	O
fund	O
fail	O
meet	O
minimum	O
net	O
capit	O
requir	O
fail	O
maintain	O
accur	O
book	O
record	O
fail	O
supervis	O
employe	O
..	O
"	O

As	O
mentioned	O
previously	O
,	O
once	O
we	O
have	O
clean	O
labels	O
,	O
we	O
make	O
a	O
series	O
of	O
enrichment	O
steps	O
.	O
First	O
,	O
we	O
make	O
use	O
of	O
Expert	O
Knowledge	O
,	O
i.e.	O
,	O
a	O
human	O
expert	O
is	O
asked	O
to	O
provide	O
3	O
to	O
5	O
additional	O
words	O
for	O
each	O
label	O
.	O
While	O
this	O
constitutes	O
a	O
small	O
amount	O
of	O
manual	O
effort	O
,	O
there	O
are	O
multiple	O
ways	O
to	O
approximate	O
this	O
task	O
without	O
human	O
intervention	O
,	O
for	O
example	O
,	O
by	O
querying	O
Wikipedia	O
or	O
the	O
web	O
with	O
the	O
category	O
name	O
and	O
performing	O
token	O
counts	O
over	O
retrieved	O
entries	O
.	O
Before	O
proceeding	O
to	O
the	O
next	O
enrichment	O
step	O
,	O
we	O
also	O
add	O
to	O
the	O
label	O
dictionaries	O
all	O
the	O
spelling	O
variants	O
of	O
the	O
expert	O
-	O
provided	O
words	O
that	O
can	O
be	O
found	O
in	O
the	O
document	O
corpus	O
.	O
We	O
also	O
remove	O
any	O
word	O
whose	O
stem	O
is	O
not	O
in	O
the	O
document	O
corpus	O
.	O
Second	O
,	O
we	O
leverage	O
WordNet	O
(	O
Fellbaum	O
,	O
1998	O
)	O
to	O
obtain	O
knowledge	O
-	O
based	O
synonyms	O
.	O
For	O
every	O
word	O
obtained	O
in	O
the	O
previous	O
step	O
,	O
we	O
add	O
to	O
the	O
label	O
dictionary	O
all	O
the	O
associated	O
synonym	O
sets	O
(	O
English	O
nouns	O
,	O
verbs	O
,	O
and	O
adjectives	O
)	O
.	O
Again	O
,	O
once	O
this	O
step	O
is	O
completed	O
,	O
we	O
remove	O
all	O
words	O
where	O
the	O
stem	O
is	O
not	O
in	O
the	O
vocabulary	O
V.	O
Third	O
,	O
we	O
bootstrap	O
the	O
label	O
dictionary	O
obtained	O
upon	O
this	O
point	O
by	O
making	O
use	O
of	O
representative	O
documents	O
.	O
A	O
representative	O
sentence	O
for	O
a	O
given	O
category	O
is	O
defined	O
by	O
Ko	O
and	O
Seo	O
(	O
2000	O
)	O
as	O
a	O
sentence	O
in	O
the	O
document	O
corpus	O
that	O
contains	O
manually	O
pre	O
-	O
defined	O
keywords	O
of	O
the	O
category	O
in	O
its	O
content	O
words	O
.	O
In	O
this	O
work	O
,	O
we	O
extend	O
this	O
definition	O
to	O
apply	O
to	O
documents	O
instead	O
of	O
sentences	O
and	O
to	O
include	O
all	O
categories	O
'	O
keywords	O
obtained	O
at	O
this	O
stage	O
.	O
Therefore	O
we	O
calculate	O
a	O
similarity	O
score	O
between	O
each	O
pair	O
of	O
input	O
document	O
-	O
category	O
label	O
keywords	O
using	O
cosine	O
distance	O
and	O
Latent	O
Semantic	O
Analysis	O
.	O
The	O
text	B-TaskName
similarity	I-TaskName
metric	O
will	O
be	O
details	O
in	O
section	O
3.4	O
.	O
For	O
this	O
step	O
,	O
we	O
use	O
an	O
empirically	O
identified	O
similarity	O
threshold	O
(	O
70	O
%	O
)	O
.	O
Then	O
,	O
for	O
each	O
identified	O
representative	O
document	O
,	O
we	O
add	O
all	O
its	O
words	O
to	O
the	O
label	O
dictionary	O
.	O
Finally	O
,	O
we	O
make	O
use	O
of	O
word	B-TaskName
embeddings	I-TaskName
(	O
Bengio	O
et	O
al	O
,	O
2003	O
;	O
Mikolov	O
et	O
al	O
,	O
2013a	O
,	O
b	O
)	O
to	O
further	O
capture	O
semantically	O
similar	O
words	O
to	O
the	O
ones	O
belonging	O
to	O
each	O
label	O
dictionary	O
.	O
We	O
first	O
proceed	O
with	O
pre	O
-	O
trained	O
models	O
which	O
enable	O
to	O
identify	O
semantically	O
similar	O
words	O
used	O
in	O
the	O
general	O
domain	O
.	O
In	O
our	O
case	O
,	O
we	O
used	O
Glove	O
1	O
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
,	O
The	O
model	O
is	O
pre	O
-	O
trained	O
on	O
a	O
corpus	O
using	O
Wikipedia2014	O
and	O
Gigaword5	O
,	O
with	O
a	O
330	O
vocabulary	O
of	O
the	O
top	O
400	O
,	O
000	O
most	O
frequent	O
words	O
and	O
a	O
context	O
window	O
size	O
of	O
10	O
.	O
Furthermore	O
,	O
we	O
also	O
seek	O
to	O
obtain	O
similar	O
words	O
as	O
used	O
in	O
the	O
specific	O
domain	O
of	O
the	O
corpus	O
.	O
Since	O
the	O
neighbors	O
of	O
each	O
keyword	O
are	O
semantically	O
related	O
in	O
embedding	O
space	O
(	O
Mikolov	O
et	O
al	O
,	O
2013b	O
)	O
,	O
we	O
train	O
a	O
Word2Vec	O
model	O
,	O
trained	O
on	O
all	O
input	O
documents	O
cleaned	O
then	O
joined	O
together	O
.	O
In	O
this	O
work	O
,	O
we	O
tested	O
its	O
two	O
main	O
architectures	O
:	O
1	O
https://nlp.stanford.edu/projects/glove/	O
Continous	O
Bag	O
of	O
words	O
(	O
CBOW	O
)	O
that	O
predicts	O
a	O
word	O
based	O
on	O
its	O
context	O
defined	O
by	O
a	O
sliding	O
window	O
of	O
words	O
and	O
Skip	O
-	O
Gram	O
(	O
SG	O
)	O
which	O
predicts	O
the	O
context	O
given	O
the	O
target	O
word	O
.	O
Experimental	O
settings	O
will	O
be	O
detailed	O
in	O
section	O
4.3	O
.	O

Once	O
all	O
labels	O
have	O
been	O
associated	O
with	O
dictionaries	O
,	O
we	O
perform	O
a	O
final	O
step	O
in	O
order	O
to	O
reduce	O
keyword	O
overlap	O
among	O
all	O
dictionaries	O
.	O
In	O
essence	O
,	O
we	O
favor	O
words	O
that	O
are	O
representative	O
(	O
salient	O
)	O
for	O
the	O
category	O
in	O
the	O
sense	O
that	O
they	O
have	O
the	O
ability	O
to	O
distinguish	O
the	O
category	O
label	O
from	O
the	O
other	O
categories	O
.	O
We	O
adapt	O
the	O
Function	O
-	O
aware	O
Component	O
(	O
FAC	O
)	O
originally	O
used	O
in	O
supervised	O
document	B-TaskName
classification	I-TaskName
(	O
Liu	O
et	O
al	O
,	O
2018	O
)	O
.	O
F	O
AC	O
(	O
w	O
,	O
c	O
)	O
=	O
T	O
F	O
(	O
w	O
,	O
c	O
)	O
−	O
1	O
M	O
1≤k≤M	O
T	O
F	O
(	O
w	O
,	O
k	O
)	O
var	O
(	O
T	O
F	O
−c	O
(	O
w	O
)	O
)	O
(	O
1	O
)	O
where	O
T	O
F	O
−c	O
(	O
w	O
)	O
is	O
the	O
collection	O
of	O
term	O
frequencies	O
except	O
the	O
c	O
-	O
th	O
category	O
and	O
var	O
(	O
)	O
is	O
the	O
variance	O
.	O
The	O
consolidation	O
step	O
consists	O
in	O
computing	O
the	O
above	O
metric	O
for	O
every	O
word	O
in	O
the	O
label	O
dictionaries	O
and	O
to	O
filter	O
out	O
those	O
whose	O
associated	O
metric	O
is	O
below	O
a	O
given	O
threshold	O
.	O
This	O
latter	O
threshold	O
depends	O
on	O
two	O
main	O
constraints	O
:	O
The	O
maximum	O
number	O
of	O
categories	O
that	O
contain	O
a	O
given	O
word	O
and	O
the	O
minimum	O
word	O
frequency	O
in	O
the	O
label	O
dictionaries	O
.	O
Regarding	O
the	O
first	O
constraint	O
,	O
in	O
our	O
practical	O
case	O
of	O
operational	O
risk	O
taxonomy	O
,	O
we	O
have	O
264	O
target	O
categories	O
that	O
could	O
be	O
grouped	O
into	O
16	O
broad	O
categories	O
:	O
cyber	O
-	O
security	O
,	O
fraud	O
,	O
compliance	O
,	O
human	O
resources	O
,	O
etc	O
.	O
Thresholds	O
are	O
determined	O
so	O
as	O
to	O
tolerate	O
overlap	O
within	O
each	O
broad	O
category	O
and	O
to	O
minimize	O
it	O
outside	O
.	O
More	O
generally	O
,	O
we	O
start	O
by	O
identifying	O
the	O
maximum	O
number	O
of	O
semantically	O
similar	O
categories	O
,	O
i.e.	O
,	O
where	O
we	O
would	O
expect	O
some	O
overlap	O
and	O
we	O
set	O
the	O
threshold	O
consequently	O
.	O
By	O
construction	O
,	O
keywords	O
in	O
a	O
given	O
dictionary	O
occur	O
at	O
least	O
one	O
time	O
.	O
We	O
decided	O
not	O
to	O
set	O
an	O
additional	O
constraint	O
on	O
word	O
frequency	O
per	O
category	O
label	O
so	O
as	O
to	O
keep	O
highly	O
specific	O
words	O
with	O
a	O
low	O
frequency	O
,	O
generally	O
captured	O
by	O
the	O
Word2vec	O
model	O
trained	O
on	O
the	O
input	O
corpus	O
.	O

In	O
order	O
to	O
evaluate	O
our	O
approach	O
,	O
we	O
conduct	O
experiments	O
on	O
five	O
standard	O
text	B-TaskName
classification	I-TaskName
corpora	O
,	O
described	O
listed	O
in	O
Table	O
1	O
.	O
As	O
we	O
use	O
an	O
unsupervised	O
approach	O
for	O
text	B-TaskName
classification	I-TaskName
,	O
we	O
make	O
use	O
of	O
the	O
whole	O
corpus	O
of	O
each	O
dataset	O
by	O
aggregating	O
training	O
and	O
test	O
sets	O
.	O
We	O
describe	O
each	O
corpus	O
briefly	O
:	O
(	O
1	O
)	O
The	O
20NewsGroup	O
2	O
dataset	O
consists	O
of	O
18	O
,	O
846	O
news	O
articles	O
divided	O
almost	O
evenly	O
among	O
20	O
different	O
UseNet	O
discussion	O
groups	O
.	O
Some	O
of	O
the	O
newsgroups	O
are	O
closely	O
related	O
(	O
e.g.	O
,	O
comp.sys.ibm.pc.hardware	O
and	O
comp.sys.mac.hardware	O
)	O
.	O
While	O
each	O
document	O
may	O
discuss	O
multiple	O
topics	O
,	O
it	O
needs	O
to	O
be	O
assigned	O
to	O
a	O
single	O
category	O
.	O
(	O
2	O
)	O
The	O
AG	O
's	O
Corpus	O
of	O
news	O
articles	O
3	O
is	O
a	O
collection	O
of	O
more	O
than	O
1	O
million	O
news	O
articles	O
.	O
We	O
used	O
the	O
version	O
created	O
by	O
Zhang	O
et	O
al	O
(	O
2015	O
)	O
who	O
selected	O
4	O
largest	O
classes	O
from	O
AG	B-DatasetName
news	I-DatasetName
corpus	O
on	O
the	O
web	O
with	O
each	O
instance	O
containing	O
class	O
index	O
,	O
title	O
and	O
description	O
fields	O
.	O
(	O
3	O
)	O
The	O
Yahoo	O
-	O
Answers	O
4	O
corpus	O
contains	O
4	O
,	O
483	O
,	O
032	O
questions	O
and	O
their	O
corresponding	O
answers	O
from	O
Yahoo	B-DatasetName
!	I-DatasetName
Answers	I-DatasetName
service	O
as	O
of	O
10/25/2007	O
.	O
We	O
used	O
the	O
version	O
constructed	O
by	O
Zhang	O
et	O
al	O
(	O
2015	O
)	O
using	O
10	O
largest	O
main	O
categories	O
and	O
the	O
best	O
answer	O
content	O
from	O
all	O
the	O
answers	O
.	O
(	O
4	O
)	O
The	O
5AbstractsGroup	O
5	O
dataset	O
is	O
a	O
collection	O
of	O
academic	O
papers	O
from	O
five	O
different	O
domains	O
collected	O
from	O
Web	O
of	O
Science	O
namely	O
,	O
business	O
,	O
artificial	O
intelligence	O
,	O
sociology	O
,	O
transport	O
and	O
law	O
.	O
We	O
extracted	O
the	O
abstract	O
and	O
title	O
fields	O
of	O
each	O
paper	O
as	O
a	O
document	O
.	O
(	O
5	O
)	O
The	O
Google	B-DatasetName
-	O
Snippets	O
6	O
dataset	O
contains	O
the	O
web	O
search	O
results	O
related	O
to	O
8	O
different	O
domains	O
such	O
as	O
business	O
,	O
computers	O
and	O
engineering	O
.	O

We	O
apply	O
multiple	O
variants	O
of	O
our	O
method	O
to	O
each	O
of	O
the	O
above	O
corpora	O
.	O
Note	O
first	O
that	O
using	O
representative	O
documents	O
(	O
Section	O
3.2	O
)	O
to	O
enrich	O
label	O
dictionaries	O
is	O
suitable	O
for	O
categories	O
whose	O
labels	O
take	O
the	O
form	O
of	O
a	O
structured	O
sentence	O
containing	O
more	O
than	O
10	O
words	O
before	O
cleaning	O
.	O
In	O
the	O
application	O
to	O
operational	O
risk	O
incidents	O
(	O
Section	O
5	O
)	O
,	O
it	O
allowed	O
to	O
enrich	O
13	O
%	O
of	O
dictionaries	O
.	O
In	O
the	O
standard	O
text	B-TaskName
classification	I-TaskName
datasets	O
used	O
in	O
our	O
experiments	O
,	O
category	O
labels	O
contain	O
less	O
than	O
5	O
words	O
so	O
representative	O
documents	O
were	O
not	O
relevant	O
in	O
the	O
enrichment	O
process	O
.	O
Thus	O
none	O
of	O
the	O
configurations	O
discussed	O
in	O
this	O
section	O
include	O
this	O
step	O
.	O
Overall	O
,	O
in	O
addition	O
to	O
the	O
full	O
pipeline	O
,	O
which	O
we	O
refer	O
to	O
as	O
all	O
keywords	O
,	O
we	O
also	O
investigated	O
whether	O
semantic	O
expansion	O
solely	O
through	O
word	B-TaskName
embeddings	I-TaskName
could	O
improve	O
performance	O
.	O
We	O
thus	O
tested	O
with	O
either	O
generic	O
embeddings	O
(	O
pre	O
-	O
trained	O
Glove	O
)	O
or	O
corpus	O
-	O
based	O
embeddings	O
(	O
Word2Vec	O
)	O
.	O
Finally	O
,	O
for	O
each	O
configuration	O
,	O
we	O
tested	O
with	O
and	O
without	O
the	O
function	O
aware	O
component	O
(	O
FAC	O
)	O
for	O
consolidation	O
of	O
the	O
label	O
dictionaries	O
.	O
We	O
also	O
implemented	O
simple	O
baselines	O
for	O
comparison	O
.	O
On	O
the	O
unsupervised	O
side	O
,	O
(	O
1	O
)	O
we	O
calculated	O
a	O
text	B-TaskName
similarity	I-TaskName
score	O
between	O
each	O
docu	O
-	O
ment	O
and	O
the	O
set	O
of	O
expert	O
provided	O
keywords	O
(	O
2	O
)	O
we	O
enriched	O
this	O
list	O
of	O
initial	O
keywords	O
with	O
their	O
synonyms	O
from	O
WordNet	O
.	O
On	O
the	O
supervised	O
side	O
,	O
we	O
use	O
Multinomial	O
Naïve	O
Bayes	O
as	O
a	O
basic	O
baseline	O
where	O
we	O
represented	O
each	O
document	O
as	O
TF	O
-	O
IDF	O
vector	O
(	O
bag	O
of	O
words	O
)	O
,	O
cleaned	O
the	O
input	O
corpus	O
in	O
the	O
same	O
way	O
as	O
in	O
our	O
proposed	O
approach	O
and	O
split	O
each	O
dataset	O
into	O
a	O
training	O
set	O
(	O
2/3	O
)	O
and	O
a	O
test	O
set	O
(	O
1/3	O
)	O
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
a	O
method	O
for	O
unsupervised	B-TaskName
text	I-TaskName
classification	I-TaskName
based	O
on	O
computing	O
the	O
similarity	O
between	O
the	O
documents	O
to	O
be	O
classified	O
and	O
a	O
rich	O
description	O
of	O
the	O
categories	O
label	O
.	O
The	O
category	O
label	O
enrichment	O
starts	O
with	O
humanexpert	O
provided	O
keywords	O
but	O
is	O
then	O
expanded	O
through	O
the	O
use	O
of	O
word	B-TaskName
embeddings	I-TaskName
.	O
We	O
also	O
investigated	O
whether	O
a	O
consolidation	O
step	O
that	O
removes	O
non	O
discriminant	O
words	O
from	O
the	O
label	O
dictionaries	O
could	O
have	O
an	O
effect	O
on	O
performance	O
.	O
We	O
have	O
not	O
explored	O
whether	O
recent	O
advances	O
in	O
word	B-TaskName
embeddings	I-TaskName
from	O
instance	O
ELMO	B-MethodName
(	O
Peters	O
et	O
al	O
,	O
2018	O
)	O
and	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
could	O
add	O
further	O
benefits	O
.	O
This	O
is	O
certainly	O
an	O
avenue	O
that	O
we	O
seek	O
to	O
explore	O
.	O
However	O
,	O
for	O
our	O
application	O
domain	O
,	O
we	O
expect	O
that	O
it	O
may	O
not	O
lead	O
to	O
increased	O
performance	O
as	O
words	O
are	O
used	O
to	O
a	O
large	O
extent	O
with	O
the	O
same	O
sense	O
across	O
the	O
corpus	O
.	O

Human	O
language	O
encompasses	O
more	O
than	O
just	O
text	O
;	O
it	O
also	O
conveys	O
emotions	O
through	O
tone	O
and	O
gestures	O
.	O
We	O
present	O
a	O
case	O
study	O
of	O
three	O
simple	O
and	O
efficient	O
Transformer	B-MethodName
-	O
based	O
architectures	O
for	O
predicting	O
sentiment	O
and	O
emotion	B-DatasetName
in	O
multimodal	O
data	O
.	O
The	O
Late	O
Fusion	O
model	O
merges	O
unimodal	O
features	O
to	O
create	O
a	O
multimodal	O
feature	O
sequence	O
,	O
the	O
Round	O
Robin	O
model	O
iteratively	O
combines	O
bimodal	O
features	O
using	O
cross	O
-	O
modal	O
attention	O
,	O
and	O
the	O
Hybrid	O
Fusion	O
model	O
combines	O
trimodal	O
and	O
unimodal	O
features	O
together	O
to	O
form	O
a	O
final	O
feature	O
sequence	O
for	O
predicting	O
sentiment	O
.	O
Our	O
experiments	O
show	O
that	O
our	O
small	O
models	O
are	O
effective	O
and	O
outperform	O
the	O
publicly	O
released	O
versions	O
of	O
much	O
larger	O
,	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
multimodal	B-TaskName
sentiment	I-TaskName
analysis	I-TaskName
systems	O
.	O

Language	O
is	O
composed	O
of	O
three	O
different	O
modalities	O
:	O
text	O
,	O
audio	O
,	O
and	O
video	O
.	O
These	O
three	O
modalities	O
together	O
make	O
it	O
easier	O
for	O
humans	O
to	O
convey	O
emotion	B-DatasetName
and	O
sentiment	O
.	O
Thus	O
,	O
a	O
machine	O
learning	O
model	O
for	O
sentiment	B-TaskName
analysis	I-TaskName
needs	O
to	O
learn	O
the	O
features	O
and	O
interactions	O
of	O
all	O
three	O
modalities	O
.	O
For	O
example	O
,	O
a	O
frown	O
in	O
the	O
video	O
can	O
alter	O
the	O
emotion	B-DatasetName
expressed	O
in	O
the	O
text	O
transcript	O
,	O
or	O
audio	O
intensity	O
can	O
help	O
determine	O
if	O
a	O
speaker	O
is	O
getting	O
agitated	O
.	O
Multimodal	O
learning	O
has	O
recently	O
received	O
a	O
good	O
deal	O
of	O
attention	O
from	O
the	O
natural	O
language	O
processing	O
community	O
[	O
Sun	O
et	O
al	O
,	O
2016	O
,	O
Chen	O
et	O
al	O
,	O
2018	O
,	O
Pham	O
et	O
al	O
,	O
2019	O
.	O
The	O
Transformer	B-MethodName
network	O
[	O
Vaswani	O
et	O
al	O
,	O
2017	O
]	O
,	O
with	O
its	O
self	O
-	O
attention	O
modules	O
,	O
has	O
achieved	O
strong	O
performance	O
in	O
multimodal	O
learning	O
;	O
attention	O
provides	O
a	O
natural	O
way	O
to	O
model	O
the	O
relationship	O
between	O
pairs	O
of	O
modalities	O
.	O
In	O
this	O
work	O
we	O
investigate	O
three	O
small	O
,	O
lightweight	O
,	O
Transformer	B-MethodName
-	O
based	O
architectures	O
for	O
multimodal	B-TaskName
sentiment	I-TaskName
analysis	I-TaskName
and	O
emotion	B-DatasetName
recog	O
-	O
nition	O
.	O
Our	O
first	O
model	O
is	O
an	O
implementation	O
of	O
the	O
Late	O
Fusion	O
model	O
commonly	O
used	O
as	O
a	O
baseline	O
system	O
,	O
which	O
assigns	O
individual	O
Transformer	B-MethodName
blocks	O
to	O
each	O
of	O
the	O
three	O
modalities	O
for	O
feature	O
extraction	O
and	O
then	O
combines	O
these	O
unimodal	O
features	O
to	O
learn	O
cross	O
-	O
modal	O
interactions	O
.	O
The	O
second	O
model	O
is	O
an	O
implementation	O
of	O
the	O
Round	O
Robin	O
approach	O
;	O
the	O
model	O
generates	O
bimodal	O
features	O
by	O
using	O
cross	O
-	O
modal	O
attention	O
to	O
combine	O
pairs	O
of	O
modalities	O
,	O
one	O
pair	O
at	O
a	O
time	O
.	O
Our	O
last	O
model	O
is	O
a	O
Hybrid	O
of	O
the	O
early	O
and	O
late	O
fusion	O
schemes	O
.	O
This	O
model	O
merges	O
the	O
features	O
extracted	O
using	O
a	O
late	O
fusion	O
pipeline	O
,	O
as	O
well	O
as	O
those	O
from	O
an	O
early	O
fusion	O
pipeline	O
,	O
where	O
the	O
three	O
modalities	O
are	O
concatenated	O
and	O
passed	O
through	O
a	O
single	O
Transformer	B-MethodName
block	O
for	O
feature	O
extraction	O
;	O
.	O
We	O
present	O
experiments	O
using	O
these	O
three	O
models	O
on	O
three	O
multimodal	O
datasets	O
:	O
IEMOCAP	B-DatasetName
[	O
Busso	O
et	O
al	O
,	O
2008	O
]	O
,	O
an	O
emotion	B-TaskName
recognition	I-TaskName
dataset	O
,	O
and	O
CMU	O
-	O
MOSI	B-DatasetName
[	O
Zadeh	O
et	O
al	O
,	O
2016	O
]	O
and	O
CMU	B-DatasetName
-	I-DatasetName
MOSEI	I-DatasetName
[	O
Zadeh	O
et	O
al	O
,	O
2018b	O
]	O
,	O
two	O
multimodal	B-TaskName
sentiment	I-TaskName
analysis	I-TaskName
datasets	O
.	O
Our	O
results	O
show	O
that	O
our	O
small	O
models	O
are	O
competitive	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
that	O
use	O
much	O
more	O
complex	O
architectures	O
.	O
Our	O
main	O
contributions	O
are	O
as	O
follows	O
:	O
We	O
present	O
three	O
lightweight	O
architectures	O
for	O
multimodal	B-TaskName
sentiment	I-TaskName
analysis	I-TaskName
that	O
achieve	O
comparable	O
results	O
to	O
much	O
larger	O
,	O
state	O
-	O
ofthe	O
-	O
art	O
models	O
.	O
We	O
analyze	O
the	O
effect	O
of	O
removing	O
or	O
simplifying	O
components	O
of	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
multimodal	O
architectures	O
.	O
We	O
conduct	O
experiments	O
on	O
small	O
training	O
sets	O
,	O
demonstrating	O
the	O
ability	O
of	O
our	O
lightweight	O
architectures	O
to	O
leverage	O
limited	O
training	O
data	O
and	O
computational	O
resources	O
.	O

We	O
do	O
not	O
give	O
an	O
exhaustive	O
list	O
of	O
prior	O
work	O
in	O
multimodal	B-TaskName
sentiment	I-TaskName
analysis	I-TaskName
,	O
but	O
focus	O
on	O
recent	O
neural	O
approaches	O
that	O
achieved	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
at	O
their	O
times	O
of	O
publication	O
.	O

The	O
Memory	O
Fusion	O
Network	O
(	O
MFN	O
)	O
of	O
Zadeh	O
et	O
al	O
[	O
2018a	O
]	O
uses	O
a	O
separate	O
LSTM	B-MethodName
to	O
encode	O
each	O
of	O
the	O
three	O
modalities	O
and	O
then	O
uses	O
attention	O
to	O
model	O
cross	O
-	O
modal	O
interactions	O
for	O
different	O
combinations	O
of	O
modalities	O
.	O
The	O
Recurrent	O
Attended	O
Variation	O
Embedding	O
Network	O
(	O
RAVEN	B-DatasetName
)	O
of	O
Wang	O
et	O
al	O
[	O
2019	O
]	O
encodes	O
the	O
audio	O
and	O
video	O
features	O
using	O
two	O
recurrent	O
neural	O
networks	O
;	O
these	O
features	O
are	O
combined	O
with	O
the	O
textual	O
input	O
using	O
cross	O
-	O
modal	O
attention	O
in	O
a	O
Gated	O
Modality	O
Mixing	O
Network	O
.	O
The	O
Multi	O
-	O
Attention	O
Recurrent	O
Network	O
(	O
MARN	O
)	O
of	O
Zadeh	O
et	O
al	O
[	O
2018c	O
]	O
is	O
an	O
LSTM	B-MethodName
-	O
based	O
architecture	O
that	O
stores	O
representations	O
of	O
each	O
of	O
the	O
three	O
modalities	O
,	O
which	O
are	O
then	O
combined	O
using	O
a	O
multi	O
-	O
attention	O
block	O
.	O
Finally	O
,	O
the	O
Multimodal	O
Cyclic	O
Translation	B-TaskName
Network	O
(	O
MCTN	O
)	O
of	O
Pham	O
et	O
al	O
[	O
2019	O
]	O
produces	O
multimodal	O
features	O
by	O
translating	O
one	O
modality	O
into	O
another	O
,	O
learning	O
a	O
joint	O
encoding	O
in	O
that	O
direction	O
,	O
and	O
then	O
back	O
-	O
translating	O
to	O
learn	O
a	O
joint	O
encoding	O
in	O
the	O
other	O
direction	O
.	O

The	O
Transformer	B-MethodName
network	O
[	O
Vaswani	O
et	O
al	O
,	O
2017	O
]	O
has	O
been	O
used	O
widely	O
in	O
neural	O
machine	B-TaskName
translation	I-TaskName
[	O
Tubay	O
and	O
Costa	O
-	O
jussà	O
,	O
2018	O
,	O
Edunov	O
et	O
al	O
,	O
2018	O
,	O
Xia	O
et	O
al	O
,	O
2019	O
,	O
Devlin	O
et	O
al	O
,	O
2019	O
and	O
has	O
proven	O
effective	O
for	O
sentiment	B-TaskName
analysis	I-TaskName
and	O
emotion	B-TaskName
recognition	I-TaskName
.	O
However	O
,	O
existing	O
architectures	O
are	O
very	O
dense	O
compared	O
to	O
our	O
three	O
lightweight	O
models	O
.	O
The	O
Multimodal	O
Transformer	B-MethodName
(	O
MuLT	O
)	O
of	O
Tsai	O
et	O
al	O
[	O
2019	O
]	O
modifies	O
the	O
Transformer	B-MethodName
block	O
to	O
compute	O
cross	O
-	O
modal	O
attention	O
for	O
two	O
modalities	O
at	O
a	O
time	O
.	O
It	O
combines	O
modalities	O
in	O
directed	O
pairs	O
,	O
using	O
a	O
total	O
of	O
six	O
Transformers	O
,	O
whose	O
outputs	O
are	O
then	O
merged	O
into	O
a	O
single	O
multimodal	O
representation	O
.	O
Unlike	O
other	O
works	O
,	O
MuLT	O
is	O
able	O
to	O
handle	O
cases	O
where	O
the	O
three	O
modalities	O
are	O
not	O
aligned	O
at	O
the	O
word	O
level	O
;	O
it	O
learns	O
soft	O
alignments	O
via	O
the	O
cross	O
-	O
modal	O
attention	O
weights	O
for	O
each	O
pair	O
of	O
modalities	O
.	O
The	O
model	O
works	O
well	O
in	O
the	O
unaligned	O
case	O
,	O
and	O
in	O
the	O
aligned	O
case	O
,	O
it	O
gives	O
state	O
of	O
the	O
art	O
performance	O
the	O
Happy	O
emotion	B-DatasetName
in	O
IEMO	O
-	O
CAP	B-DatasetName
.	O
The	O
Factorized	O
Multimodal	O
Transformer	B-MethodName
(	O
FMT	O
)	O
of	O
introduces	O
Factorized	O
Multimodal	O
Self	O
-	O
Attention	O
(	O
FSM	O
)	O
modules	O
,	O
which	O
compute	O
self	O
-	O
attention	O
over	O
unimodal	O
,	O
bimodal	O
,	O
and	O
trimodal	O
inputs	O
in	O
parallel	O
.	O
FMT	O
gives	O
state	O
of	O
the	O
art	O
performance	O
in	O
the	O
word	O
-	O
aligned	O
case	O
on	O
CMU	O
-	O
MOSI	B-DatasetName
and	O
on	O
the	O
Sad	O
,	O
Angry	O
,	O
and	O
Neutral	O
emotions	O
in	O
IEMOCAP	B-DatasetName
.	O
We	O
use	O
FMT	O
,	O
along	O
with	O
the	O
word	O
-	O
aligned	O
version	O
of	O
MuLT	O
,	O
as	O
baselines	O
for	O
comparison	O
in	O
our	O
experiments	O
.	O

Our	O
three	O
lightweight	O
architectures	O
are	O
comprised	O
of	O
Transformer	B-MethodName
blocks	O
[	O
Vaswani	O
et	O
al	O
,	O
2017	O
]	O
,	O
which	O
are	O
non	O
-	O
recurrent	O
neural	O
networks	O
that	O
can	O
process	O
sequential	O
data	O
.	O
It	O
consists	O
of	O
alternating	O
attention	O
and	O
linear	O
layers	O
.	O
The	O
attention	O
block	O
of	O
a	O
Transformer	B-MethodName
uses	O
multi	B-MethodName
-	I-MethodName
head	I-MethodName
attention	I-MethodName
,	O
where	O
each	O
head	O
computes	O
scaled	O
dot	O
product	O
attention	O
:	O
attn	O
(	O
Q	O
,	O
K	O
,	O
V	O
)	O
=	O
softmax	B-MethodName
QK	O
T	O
√	O
d	O
k	O
V	O
head	O
i	O
=	O
attn	O
QW	O
Q	O
i	O
,	O
KW	O
K	O
i	O
,	O
V	O
W	O
V	O
i	O
multi	O
(	O
Q	O
,	O
K	O
,	O
V	O
)	O
=	O
[	O
head	O
1	O
;	O
.	O
.	O
.	O
;	O
head	O
h	O
]	O
W	O
O	O
where	O
Q	O
,	O
K	O
,	O
V	O
represent	O
the	O
query	O
,	O
key	O
and	O
value	O
;	O
d	O
k	O
is	O
the	O
key	O
dimension	O
size	O
;	O
W	O
Q	O
i	O
,	O
W	O
K	O
i	O
,	O
W	O
V	O
i	O
are	O
learned	O
projection	O
matrices	O
for	O
head	O
i	O
;	O
and	O
W	O
O	O
is	O
a	O
learned	O
projection	O
matrix	O
for	O
the	O
attention	O
block	O
.	O
In	O
addition	O
,	O
Vaswani	O
et	O
al	O
note	O
that	O
positional	O
encodings	O
must	O
be	O
added	O
to	O
Transformer	B-MethodName
input	O
because	O
there	O
is	O
no	O
sequential	O
information	O
present	O
in	O
the	O
Transformer	B-MethodName
itself	O
:	O
P	O
E	O
(	O
pos	O
,	O
2i	O
)	O
=	O
sin	O
(	O
pos/10000	O
2i	O
/	O
d	O
model	O
)	O
P	O
E	O
(	O
pos	O
,	O
2i+1	O
)	O
=	O
cos	O
(	O
pos/10000	O
2i	O
/	O
d	O
model	O
)	O
X	O
=	O
X	O
+	O
P	O
E	O

Figure	O
1	O
shows	O
our	O
Late	O
Fusion	O
architecture	O
.	O
Three	O
unimodal	O
Transformers	O
learn	O
high	O
-	O
level	O
features	O
from	O
the	O
low	O
-	O
level	O
input	O
features	O
of	O
each	O
modality	O
.	O
The	O
outputs	O
of	O
these	O
unimodal	O
Transformers	O
are	O
then	O
merged	O
together	O
using	O
a	O
simple	O
summation	O
,	O
rather	O
than	O
the	O
merge	O
layer	O
used	O
in	O
previous	O
work	O
[	O
Tsai	O
et	O
al	O
,	O
2019	O
]	O
,	O
and	O
passed	O
to	O
a	O
residual	B-MethodName
network	I-MethodName
of	O
linear	O
layers	O
[	O
Xie	O
et	O
al	O
,	O
2017	O
]	O
for	O
sentiment	O
prediction	O
.	O
Figure	O
2	O
shows	O
our	O
Round	O
Robin	O
architecture	O
,	O
which	O
is	O
a	O
simplification	O
of	O
MuTL	O
[	O
Tsai	O
et	O
al	O
,	O
2019	O
]	O
.	O
Three	O
cross	O
-	O
modal	O
Transformers	O
learn	O
bimodal	O
feaatures	O
for	O
ordered	O
pairs	O
of	O
modalities	O
,	O
where	O
the	O
query	O
is	O
one	O
modality	O
and	O
the	O
key	O
/	O
value	O
is	O
the	O
other	O
.	O
We	O
use	O
only	O
three	O
pairs	O
-	O
text	O
query	O
and	O
audio	O
key	O
/	O
value	O
,	O
audio	O
query	O
and	O
video	O
key	O
/	O
value	O
,	O
and	O
video	O
query	O
and	O
text	O
key	O
/	O
valuewith	O
bimodal	O
information	O
flowing	O
in	O
only	O
one	O
direction	O
;	O
in	O
contrast	O
,	O
MuLT	O
uses	O
six	O
pairs	O
of	O
crossmodal	O
Transformers	O
,	O
with	O
information	O
flowing	O
in	O
both	O
directions	O
.	O
MuLT	O
also	O
uses	O
three	O
Transformers	O
,	O
one	O
for	O
each	O
modality	O
,	O
to	O
merge	O
the	O
two	O
pairs	O
sharing	O
that	O
modality	O
as	O
key	O
/	O
value	O
;	O
our	O
pairwise	O
features	O
are	O
simply	O
concatenated	O
and	O
passed	O
to	O
the	O
output	O
residual	B-MethodName
network	I-MethodName
.	O
Figure	O
3	O
shows	O
our	O
Hybrid	O
Fusion	O
architecture	O
,	O
which	O
uses	O
both	O
an	O
early	O
fusion	O
approach	O
that	O
concatenates	O
the	O
inputs	O
and	O
passes	O
them	O
to	O
a	O
single	O
Transformer	B-MethodName
to	O
learn	O
trimodal	O
features	O
,	O
as	O
well	O
as	O
a	O
late	O
fusion	O
approach	O
that	O
passes	O
each	O
modality	O
through	O
a	O
separate	O
Transformer	B-MethodName
to	O
learn	O
unimodal	O
features	O
.	O
The	O
trimodal	O
and	O
unimodal	O
features	O
are	O
concatenated	O
together	O
and	O
merged	O
using	O
a	O
layer	O
of	O
Gated	O
Recurrent	O
Units	O
.	O

Text	O
Features	O
:	O
For	O
word	O
-	O
level	O
textual	O
features	O
we	O
use	O
the	O
pretrained	O
,	O
300	O
-	O
dimensional	O
,	O
Common	B-DatasetName
Crawl	I-DatasetName
GloVe	B-MethodName
embeddings	I-MethodName
[	O
Pennington	O
et	O
al	O
,	O
2014	O
]	O
.	O
Audio	O
features	O
,	O
including	O
Mel	O
-	O
frequency	O
cepstral	O
coefficients	O
and	O
transformations	O
thereof	O
,	O
as	O
well	O
as	O
harmonic	O
,	O
percussive	O
,	O
and	O
glottal	O
source	O
parameters	O
.	O
We	O
also	O
use	O
COVERAP	O
[	O
Degottex	O
et	O
al	O
,	O
2014	O
]	O
to	O
extract	O
pitch	O
tracking	O
and	O
voiced	O
/	O
unvoiced	O
sloping	O
parameters	O
,	O
peak	O
slope	O
parameters	O
,	O
and	O
maximum	O
dispersion	O
quotients	O
.	O
Video	O
Features	O
:	O
We	O
extract	O
35	O
facial	O
units	O
using	O
Facet	O
[	O
iMotions	O
,	O
2017	O
]	O
,	O
as	O
well	O
as	O
35	O
facial	O
action	O
units	O
and	O
30	O
facial	O
landmark	O
and	O
gaze	O
fea	O
-	O
tures	O
using	O
OpenFace	O
[	O
Baltrusaitis	O
et	O
al	O
,	O
2018	O
]	O
.	O

We	O
compare	O
our	O
results	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
Multimodal	O
Transformer	B-MethodName
(	O
MuLT	O
)	O
1	O
[	O
Tsai	O
et	O
al	O
,	O
2019	O
]	O
and	O
Factorized	O
Multimodal	O
Transformer	B-MethodName
(	O
FMT	O
)	O
,	O
as	O
well	O
as	O
Memory	O
Fusion	O
Network	O
(	O
MFN	O
)	O
[	O
Zadeh	O
et	O
al	O
,	O
2018a	O
]	O
,	O
Recurrent	O
Attended	O
Variation	O
Embedding	O
Network	O
(	O
RAVEN	B-DatasetName
)	O
[	O
Wang	O
et	O
al	O
,	O
2019	O
]	O
,	O
Multi	O
-	O
Attention	O
Recurrent	O
Network	O
(	O
MARN	O
)	O
[	O
Zadeh	O
et	O
al	O
,	O
2018c	O
]	O
,	O
and	O
Multimodal	O
Cyclic	O
Translation	B-TaskName
Network	O
(	O
MCTN	O
)	O
[	O
Pham	O
et	O
al	O
,	O
2019	O
]	O
.	O
These	O
systems	O
are	O
described	O
in	O
Section	O
2	O
;	O
all	O
attained	O
state	O
of	O
the	O
art	O
on	O
at	O
least	O
one	O
of	O
the	O
evaluation	O
datasets	O
at	O
their	O
times	O
of	O
publication	O
,	O
and	O
all	O
use	O
a	O
similar	O
feature	O
set	O
to	O
our	O
work	O
.	O

We	O
perform	O
ablation	O
experiments	O
on	O
our	O
models	O
using	O
the	O
IEMOCAP	B-DatasetName
dataset	O
;	O
ablation	O
results	O
for	O
CMU	O
-	O
MOSI	B-DatasetName
and	O
CMU	B-DatasetName
-	I-DatasetName
MOSEI	I-DatasetName
are	O
omitted	O
due	O
to	O
space	O
constraints	O
,	O
but	O
exhibit	O
similar	O
trends	O
.	O
Table	O
6	O
presents	O
the	O
results	O
of	O
modality	O
ablation	O
on	O
the	O
simplest	O
Late	O
Fusion	O
model	O
;	O
it	O
clearly	O
shows	O
that	O
unimodal	O
and	O
bimodal	O
models	O
are	O
unable	O
to	O
match	O
the	O
performance	O
of	O
a	O
full	O
multimodal	O
model	O
.	O
This	O
demonstrates	O
the	O
importance	O
of	O
considering	O
all	O
modalities	O
when	O
analyzing	O
spoken	O
language	O
,	O
since	O
some	O
of	O
the	O
emotions	O
or	O
sentiment	O
may	O
be	O
dependent	O
more	O
on	O
the	O
audio	O
or	O
the	O
visual	O
actions	O
of	O
the	O
speaker	O
,	O
rather	O
than	O
the	O
text	O
.	O
Examining	O
the	O
unimodal	O
results	O
,	O
we	O
see	O
that	O
the	O
Text	O
modality	O
is	O
the	O
most	O
informative	O
for	O
predicting	O
Happy	O
,	O
Sad	O
,	O
and	O
Neutral	O
,	O
while	O
Audio	O
is	O
the	O
most	O
informative	O
for	O
Angry	O
.	O
However	O
,	O
the	O
bimodal	O
results	O
do	O
not	O
always	O
match	O
the	O
unimodal	O
results	O
.	O
The	O
best	O
-	O
performing	O
bimodal	O
model	O
for	O
Happy	O
is	O
[	O
V	O
,	O
A	O
]	O
,	O
despite	O
Video	O
being	O
the	O
worst	O
-	O
performing	O
single	O
modality	O
,	O
and	O
[	O
T	O
,	O
A	O
]	O
is	O
the	O
worst	O
-	O
performing	O
bimodal	O
model	O
,	O
despite	O
both	O
Text	O
and	O
Audio	O
outperforming	O
Video	O
individually	O
.	O
Considering	O
the	O
other	O
three	O
emotions	O
,	O
we	O
see	O
that	O
the	O
best	O
bimodal	O
model	O
varies	O
between	O
[	O
T	O
,	O
A	O
]	O
and	O
[	O
V	O
,	O
A	O
]	O
,	O
with	O
[	O
T	O
,	O
V	O
]	O
generally	O
performing	O
the	O
worst	O
.	O
Table	O
7	O
shows	O
the	O
results	O
of	O
modality	O
ablation	O
on	O
the	O
Round	O
Robin	O
model	O
;	O
as	O
the	O
architecture	O
does	O
not	O
support	O
unimodal	O
experiments	O
,	O
only	O
bimodal	O
results	O
are	O
shown	O
.	O
Comparing	O
Table	O
6	O
to	O
Table	O
7	O
,	O
we	O
see	O
that	O
the	O
cross	O
-	O
modal	O
Transformers	O
of	O
the	O
full	O
Round	O
Robin	O
model	O
are	O
outperformed	O
by	O
the	O
full	O
Late	O
Fusion	O
model	O
.	O
However	O
,	O
the	O
relative	O
performance	O
among	O
modality	O
pairs	O
is	O
consistent	O
across	O
Tables	O
6	O
and	O
7	O
.	O
Finally	O
,	O
Table	O
8	O
shows	O
the	O
results	O
of	O
modality	O
ablation	O
on	O
the	O
Hybrid	O
Fusion	O
model	O
,	O
where	O
we	O
compare	O
the	O
relative	O
contributions	O
of	O
the	O
early	O
fusion	O
and	O
late	O
fusion	O
halves	O
of	O
the	O
architecture	O
.	O
The	O
top	O
of	O
the	O
table	O
shows	O
the	O
results	O
of	O
reducing	O
the	O
early	O
fusion	O
half	O
to	O
only	O
two	O
modalities	O
while	O
retaining	O
all	O
three	O
modalities	O
in	O
the	O
late	O
fusion	O
half	O
,	O
and	O
the	O
bottom	O
shows	O
the	O
results	O
of	O
reducing	O
the	O
late	O
fusion	O
half	O
to	O
two	O
modalities	O
while	O
retaining	O
all	O
three	O
in	O
the	O
early	O
fusion	O
half	O
;	O
in	O
both	O
sets	O
of	O
experiments	O
,	O
the	O
overall	O
model	O
has	O
access	O
to	O
all	O
three	O
modalities	O
,	O
but	O
only	O
through	O
either	O
the	O
early	O
fusion	O
path	O
or	O
the	O
late	O
fusion	O
path	O
.	O
Surprisingly	O
,	O
although	O
standalone	O
early	O
fusion	O
models	O
are	O
outperformed	O
by	O
standalone	O
late	O
fusion	O
models	O
[	O
Tsai	O
et	O
al	O
,	O
2019	O
]	O
,	O
we	O
find	O
that	O
a	O
hybrid	O
model	O
containing	O
a	O
full	O
,	O
trimodal	O
early	O
fusion	O
half	O
is	O
more	O
robust	O
to	O
modality	O
ablation	O
in	O
its	O
late	O
fusion	O
half	O
than	O
a	O
model	O
with	O
a	O
full	O
late	O
fusion	O
half	O
is	O
to	O
an	O
ablated	O
early	O
fusion	O
half	O
.	O
Our	O
results	O
in	O
this	O
experiment	O
also	O
show	O
greater	O
variability	O
among	O
modality	O
pairs	O
.	O
The	O
[	O
T	O
,	O
A	O
]	O
combination	O
,	O
which	O
gave	O
the	O
best	O
performance	O
in	O
the	O
Late	O
Fusion	O
and	O
Round	O
Robin	O
experiments	O
,	O
remains	O
the	O
strongest	O
modality	O
pair	O
for	O
the	O
full	O
early	O
fusion	O
,	O
bimodal	O
late	O
fusion	O
model	O
.	O
In	O
contrast	O
,	O
for	O
the	O
bimodal	O
early	O
fusion	O
,	O
full	O
late	O
fusion	O
model	O
,	O
[	O
T	O
,	O
A	O
]	O
is	O
outperformed	O
by	O
one	O
of	O
the	O
two	O
Video	O
-	O
based	O
modality	O
pairs	O
,	O
[	O
T	O
,	O
V	O
]	O
or	O
[	O
V	O
,	O
A	O
]	O
,	O
on	O
each	O
of	O
the	O
four	O
emotions	O
,	O
suggesting	O
that	O
the	O
performance	O
gap	O
of	O
early	O
versus	O
late	O
fusion	O
differs	O
across	O
modalities	O
.	O

We	O
have	O
presented	O
three	O
lightweight	O
architectures	O
for	O
multimodal	B-TaskName
sentiment	I-TaskName
analysis	I-TaskName
and	O
emotion	B-TaskName
recognition	I-TaskName
.	O
The	O
Late	O
Fusion	O
model	O
merges	O
unimodal	O
features	O
,	O
the	O
Round	O
Robin	O
model	O
iteratively	O
combines	O
bimodal	O
features	O
,	O
and	O
the	O
Hybrid	O
Early	O
-	O
Late	O
Fusion	O
model	O
combines	O
early	O
-	O
fusion	O
trimodal	O
and	O
late	O
-	O
fusion	O
unimodal	O
features	O
.	O
Our	O
proposed	O
models	O
are	O
much	O
smaller	O
in	O
size	O
compared	O
to	O
existing	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
;	O
they	O
are	O
able	O
to	O
attain	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
scores	O
on	O
the	O
CMU	O
-	O
MOSI	B-DatasetName
and	O
CMU	B-DatasetName
-	I-DatasetName
MOSEI	I-DatasetName
datasets	O
on	O
two	O
metrics	O
,	O
while	O
remaining	O
competitive	O
on	O
the	O
others	O
.	O
Further	O
,	O
our	O
experiments	O
analyzing	O
the	O
relative	O
contribution	O
of	O
modalities	O
and	O
architecture	O
components	O
in	O
our	O
models	O
suggest	O
new	O
directions	O
for	O
developing	O
multimodal	O
systems	O
.	O
We	O
hope	O
that	O
our	O
simple	O
architectures	O
for	O
sentiment	O
and	O
emotion	B-DatasetName
detection	O
,	O
currently	O
the	O
fastest	O
and	O
best	O
-	O
performing	O
publicly	O
available	O
system	O
,	O
as	O
well	O
as	O
the	O
insights	O
revealed	O
in	O
our	O
experimental	O
results	O
,	O
can	O
be	O
useful	O
for	O
further	O
research	O
in	O
the	O
field	O
.	O

Deep	O
Multi	B-TaskName
-	I-TaskName
Task	I-TaskName
Learning	I-TaskName
for	O
Aspect	O
Term	B-TaskName
Extraction	I-TaskName
with	O
Memory	O
Interaction	O
*	O

We	O
propose	O
a	O
novel	O
LSTM	B-MethodName
-	O
based	O
deep	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
framework	O
for	O
aspect	O
term	B-TaskName
extraction	I-TaskName
from	O
user	O
review	O
sentences	O
.	O
Two	O
LSTMs	O
equipped	O
with	O
extended	O
memories	O
and	O
neural	O
memory	O
operations	O
are	O
designed	O
for	O
jointly	O
handling	O
the	O
extraction	O
tasks	O
of	O
aspects	O
and	O
opinions	O
via	O
memory	O
interactions	O
.	O
Sentimental	O
sentence	O
constraint	O
is	O
also	O
added	O
for	O
more	O
accurate	O
prediction	O
via	O
another	O
LSTM	B-MethodName
.	O
Experiment	O
results	O
over	O
two	O
benchmark	O
datasets	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
framework	O
.	O
*	O
The	O
work	O
described	O
in	O
this	O
paper	O
is	O
substantially	O
supported	O
by	O
a	O
grant	O
from	O
the	O
Research	O
Grant	O
Council	O
of	O
the	O
Hong	O
Kong	O
Special	O
Administrative	O
Region	O
,	O
China	O
(	O
Project	O
Code	O
:	O
14203414	O
)	O
.	O
We	O
thank	O
Lidong	O
Bing	O
and	O
Piji	O
Li	O
for	O
their	O
helpful	O
comments	O
on	O
this	O
draft	O
and	O
the	O
anonymous	O
reviewers	O
for	O
their	O
valuable	O
feedback	O
.	O

The	O
aspect	B-TaskName
-	I-TaskName
based	I-TaskName
sentiment	I-TaskName
analysis	I-TaskName
(	O
ABSA	O
)	O
task	O
is	O
to	O
identify	O
opinions	O
expressed	O
towards	O
specific	O
entities	O
such	O
as	O
laptop	O
or	O
attributes	O
of	O
entities	O
such	O
as	O
price	O
(	O
Liu	O
,	O
2012a	O
)	O
.	O
This	O
task	O
involves	O
three	O
subtasks	O
:	O
Aspect	O
Term	B-TaskName
Extraction	I-TaskName
(	O
ATE	O
)	O
,	O
Aspect	O
Polarity	O
Detection	O
and	O
Aspect	B-TaskName
Category	I-TaskName
Detection	I-TaskName
.	O
As	O
a	O
fundamental	O
subtask	O
in	O
ABSA	O
,	O
the	O
goal	O
of	O
the	O
ATE	O
task	O
is	O
to	O
identify	O
opinionated	O
aspect	O
expressions	O
.	O
One	O
of	O
most	O
important	O
characteristics	O
is	O
that	O
opinion	O
words	O
can	O
provide	O
indicative	O
clues	O
for	O
aspect	O
detection	O
since	O
opinion	O
words	O
should	O
co	O
-	O
occur	O
with	O
aspect	O
words	O
.	O
Most	O
publicly	O
available	O
datasets	O
contain	O
the	O
gold	O
standard	O
annotations	O
for	O
opinionated	O
aspects	O
,	O
but	O
the	O
ground	O
truth	O
of	O
the	O
corresponding	O
opinion	O
words	O
is	O
not	O
commonly	O
provided	O
.	O
Some	O
works	O
tackling	O
the	O
ATE	O
task	O
ignore	O
the	O
consideration	O
of	O
opinion	O
words	O
and	O
just	O
focus	O
on	O
aspect	O
term	O
modeling	O
and	O
learning	O
(	O
Jin	O
et	O
al	O
,	O
2009	O
;	O
Jakob	O
and	O
Gurevych	O
,	O
2010	O
;	O
Toh	O
and	O
Wang	O
,	O
2014	O
;	O
Chernyshevich	O
,	O
2014	O
;	O
Manek	O
et	O
al	O
,	O
2017	O
;	O
San	O
Vicente	O
et	O
al	O
,	O
2015	O
;	O
Liu	O
et	O
al	O
,	O
2015	O
;	O
Poria	O
et	O
al	O
,	O
2016	O
;	O
Toh	O
and	O
Su	O
,	O
2016	O
;	O
Yin	O
et	O
al	O
,	O
2016	O
)	O
.	O
They	O
fail	O
to	O
leverage	O
opinion	O
information	O
which	O
is	O
supposed	O
to	O
be	O
useful	O
clues	O
.	O
Some	O
works	O
tackling	O
the	O
ATE	O
task	O
consider	O
opinion	O
information	O
(	O
Hu	O
and	O
Liu	O
,	O
2004a	O
,	O
b	O
;	O
Popescu	O
and	O
Etzioni	O
,	O
2005	O
;	O
Zhuang	O
et	O
al	O
,	O
2006	O
;	O
Qiu	O
et	O
al	O
,	O
2011	O
;	O
Liu	O
et	O
al	O
,	O
2012bLiu	O
et	O
al	O
,	O
,	O
2013aLiu	O
et	O
al	O
,	O
,	O
b	O
,	O
2014	O
in	O
an	O
unsupervised	O
or	O
partially	O
supervised	O
manner	O
.	O
Qiu	O
et	O
al	O
(	O
2011	O
)	O
proposed	O
Double	O
Propagation	O
(	O
DP	O
)	O
to	O
collectively	O
extract	O
aspect	O
terms	O
and	O
opinion	O
words	O
based	O
on	O
information	O
propagation	O
over	O
a	O
dependency	O
graph	O
.	O
One	O
drawback	O
is	O
that	O
it	O
heavily	O
relies	O
on	O
the	O
dependency	O
parser	O
,	O
which	O
is	O
prone	O
to	O
generate	O
mistakes	O
when	O
applying	O
on	O
informal	O
online	O
reviews	O
.	O
Liu	O
et	O
al	O
(	O
2014	O
)	O
modeled	O
relation	O
between	O
aspects	O
and	O
opinions	O
by	O
constructing	O
a	O
bipartite	O
heterogenous	O
graph	O
.	O
It	O
can	O
not	O
perform	O
well	O
without	O
a	O
high	O
-	O
quality	O
phrase	O
chunker	O
and	O
POS	O
tagger	O
reducing	O
its	O
flexibility	O
.	O
As	O
unsupervised	O
or	O
partially	O
supervised	O
frameworks	O
can	O
not	O
take	O
the	O
full	O
advantages	O
of	O
aspect	O
annotations	O
commonly	O
found	O
in	O
the	O
training	O
data	O
,	O
the	O
above	O
methods	O
lead	O
to	O
deficiency	O
in	O
leveraging	O
the	O
data	O
.	O
Recently	O
,	O
Wang	O
et	O
al	O
(	O
2016	O
)	O
considered	O
relation	O
between	O
opinion	O
words	O
and	O
aspect	O
words	O
in	O
a	O
supervised	O
model	O
named	O
RNCRF	O
.	O
However	O
,	O
RNCRF	O
tends	O
to	O
suffer	O
from	O
parsing	O
errors	O
since	O
the	O
structure	O
of	O
the	O
recursive	O
network	O
hinges	O
on	O
the	O
dependency	O
parse	O
tree	O
.	O
CMLA	O
(	O
Wang	O
et	O
al	O
,	O
2017a	O
)	O
used	O
a	O
multilayer	O
neural	O
model	O
where	O
each	O
layer	O
consists	O
of	O
aspect	O
attention	O
and	O
opinion	O
attention	O
.	O
However	O
CMLA	O
merely	O
employs	O
standard	O
GRU	B-MethodName
without	O
extended	O
memories	O
.	O
We	O
propose	O
MIN	O
(	O
Memory	O
Interaction	O
Network	O
)	O
,	O
a	O
novel	O
LSTM	B-MethodName
-	O
based	O
deep	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
framework	O
for	O
the	O
ATE	O
task	O
.	O
Two	O
LSTMs	O
with	O
extended	O
memory	O
are	O
designed	O
for	O
handling	O
the	O
extraction	O
tasks	O
of	O
aspects	O
and	O
opinions	O
.	O
The	O
aspect	O
-	O
opinion	O
relationship	O
is	O
established	O
based	O
on	O
neural	O
memory	O
interactions	O
between	O
aspect	B-TaskName
extraction	I-TaskName
and	O
opinion	O
extraction	O
where	O
the	O
global	O
indicator	O
score	O
of	O
opinion	O
terms	O
and	O
local	O
positional	O
relevance	O
between	O
aspects	O
and	O
opinions	O
are	O
considered	O
.	O
To	O
ensure	O
that	O
aspects	O
are	O
from	O
sentimental	O
sentences	O
,	O
MIN	O
employs	O
a	O
third	O
LSTM	B-MethodName
for	O
sentimental	O
sentence	B-TaskName
classification	I-TaskName
facilitating	O
more	O
accurate	O
aspect	O
term	B-TaskName
extraction	I-TaskName
.	O
Experiment	O
results	O
over	O
two	O
benchmark	O
datasets	O
show	O
that	O
our	O
framework	O
achieves	O
superior	O
performance	O
.	O

Let	O
an	O
input	O
review	O
sentence	O
with	O
T	O
word	O
tokens	O
and	O
the	O
corresponding	O
distributed	O
representations	O
be	O
w	O
=	O
{	O
w	O
1	O
,	O
...	O
,	O
w	O
T	O
}	O
and	O
x	O
=	O
{	O
x	O
1	O
,	O
...	O
,	O
x	O
T	O
}	O
respectively	O
.	O
The	O
ATE	O
task	O
is	O
treated	O
as	O
a	O
sequence	O
labeling	O
task	O
with	O
BIO	O
tagging	O
scheme	O
and	O
the	O
set	O
of	O
aspect	O
tags	O
for	O
the	O
word	O
w	O
t	O
is	O
y	O
A	O
t	O
{	O
B	O
,	O
I	O
,	O
O	O
}	O
,	O
where	O
B	O
,	O
I	O
,	O
O	O
represent	O
beginning	O
of	O
,	O
inside	O
and	O
outside	O
of	O
the	O
aspect	O
span	O
respectively	O
.	O
Commonly	O
found	O
training	O
data	O
contains	O
gold	O
annotations	O
for	O
aspect	O
terms	O
and	O
opinionated	O
sentences	O
,	O
but	O
the	O
gold	O
standard	O
of	O
opinion	O
words	O
are	O
usually	O
not	O
available	O
.	O
In	O
our	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
framework	O
,	O
three	O
tasks	O
are	O
involved	O
:	O
(	O
1	O
)	O
aspect	O
term	B-TaskName
extraction	I-TaskName
(	O
ATE	O
)	O
,	O
(	O
2	O
)	O
opinion	O
word	O
extraction	O
and	O
(	O
3	O
)	O
sentimental	O
sentence	B-TaskName
classification	I-TaskName
.	O
We	O
design	O
a	O
taskspecific	O
LSTM	B-MethodName
,	O
namely	O
,	O
A	O
-	O
LSTM	B-MethodName
,	O
O	O
-	O
LSTM	B-MethodName
and	O
S	O
-	O
LSTM	B-MethodName
,	O
for	O
tackling	O
each	O
of	O
the	O
above	O
tasks	O
respectively	O
.	O
The	O
first	O
component	O
of	O
our	O
proposed	O
framework	O
consists	O
of	O
A	O
-	O
LSTM	B-MethodName
and	O
O	O
-	O
LSTM	B-MethodName
where	O
we	O
equip	O
LSTMs	O
with	O
extended	O
operational	O
memories	O
and	O
some	O
operations	O
are	O
defined	O
over	O
the	O
memories	O
for	O
task	O
-	O
level	O
memory	O
interactions	O
.	O
The	O
second	O
component	O
is	O
to	O
determine	O
if	O
a	O
review	O
sentence	O
is	O
sentimental	O
.	O
This	O
is	O
achieved	O
by	O
employing	O
a	O
vanilla	O
LSTM	B-MethodName
,	O
namely	O
,	O
S	O
-	O
LSTM	B-MethodName
.	O

We	O
conduct	O
experiments	O
on	O
two	O
benchmark	O
datasets	O
from	O
SemEval	O
ABSA	O
challenge	O
(	O
Pontiki	O
et	O
al	O
,	O
2014	O
(	O
Pontiki	O
et	O
al	O
,	O
,	O
2016	O
as	O
shown	O
in	O
Table	O
1	O
.	O
D	O
1	O
(	O
Se	O
-	O
mEval	O
2014	O
)	O
contains	O
reviews	O
from	O
the	O
laptop	O
domain	O
and	O
D	O
2	O
(	O
SemEval	O
2016	O
)	O
contains	O
reviews	O
from	O
the	O
restaurant	O
domain	O
.	O
In	O
these	O
datasets	O
,	O
aspect	O
terms	O
have	O
been	O
labeled	O
and	O
sentences	O
containing	O
at	O
least	O
one	O
golden	O
truth	O
aspect	O
are	O
regarded	O
as	O
sentimental	O
sentences	O
.	O
As	O
gold	O
standard	O
annotations	O
for	O
opinion	O
words	O
are	O
not	O
provided	O
,	O
we	O
select	O
words	O
with	O
strong	O
subjectivity	O
from	O
MPQA	B-DatasetName
1	O
as	O
potential	O
opinion	O
words	O
.	O
Apart	O
from	O
the	O
common	O
opinion	O
words	O
in	O
the	O
sentiment	O
lexicon	O
,	O
we	O
also	O
treat	O
words	O
,	O
which	O
directly	O
depend	O
on	O
gold	O
standard	O
aspect	O
terms	O
through	O
highprecision	O
dependency	O
rules	O
,	O
as	O
opinion	O
words	O
.	O

Table	O
2	O
depicts	O
experiment	O
results	O
.	O
Compared	O
to	O
the	O
best	O
systems	O
in	O
SemEval	O
challenge	O
,	O
MIN	O
achieves	O
3.0	O
%	O
and	O
1.1	O
%	O
absolute	O
gains	O
on	O
D	O
1	O
and	O
D	O
2	O
respectively	O
.	O
Besides	O
,	O
our	O
MIN	O
outperforms	O
WDEmb	O
,	O
a	O
strong	O
CRF	B-MethodName
-	O
based	O
system	O
benefiting	O
from	O
several	O
kinds	O
of	O
useful	O
word	B-TaskName
embeddings	I-TaskName
,	O
by	O
2.1	O
%	O
on	O
D	O
1	O
.	O
With	O
memory	O
interactions	O
and	O
consideration	O
of	O
sentimental	O
sentence	O
,	O
our	O
MIN	O
boosts	O
the	O
performance	O
of	O
vanilla	O
bi	O
-	O
directional	O
LSTM	B-MethodName
(	O
+2.0	O
%	O
and	O
+1.7	O
%	O
respectively	O
)	O
.	O
It	O
validates	O
the	O
effectiveness	O
of	O
the	O
manually	O
designed	O
memory	O
operations	O
and	O
the	O
proposed	O
memory	O
interaction	O
mechanism	O
.	O
MIN	O
also	O
outperforms	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
RNCRF	O
on	O
each	O
dataset	O
suggesting	O
that	O
memory	O
interactions	O
can	O
be	O
an	O
alternative	O
strategy	O
instead	O
of	O
syntactic	O
parsing	O
.	O
To	O
further	O
study	O
the	O
impact	O
of	O
each	O
element	O
in	O
MIN	O
,	O
we	O
conduct	O
ablation	O
experiments	O
.	O
As	O
shown	O
in	O
Table	O
3	O
,	O
removing	O
bi	O
-	O
directionality	O
decreases	O
the	O
extraction	O
performances	O
(	O
-	O
2.0	O
%	O
and	O
-	O
1.0	O
%	O
)	O
.	O
The	O
soft	O
sentimental	O
constraint	O
proves	O
to	O
be	O
useful	O
since	O
MIN	O
is	O
1.5	O
%	O
and	O
1.0	O
%	O
superior	O
than	O
the	O
framework	O
without	O
S	O
-	O
LSTM	B-MethodName
on	O
D	O
1	O
and	O
D	O
2	O
respectively	O
.	O
O	O
-	O
LSTM	B-MethodName
brings	O
in	O
the	O
largest	O
performance	O
gains	O
on	O
D	O
2	O
compared	O
with	O
ablated	O
framework	O
(	O
i.e.	O
,	O
MIN	O
without	O
O	O
-	O
LSTM	B-MethodName
)	O
,	O
verifying	O
our	O
postulation	O
that	O
aspect	O
-	O
opinion	O
"	O
interaction	O
"	O
is	O
more	O
effective	O
than	O
only	O
considering	O
aspect	O
terms	O
.	O
We	O
also	O
observe	O
that	O
the	O
contribution	O
of	O
O	O
-	O
LSTM	B-MethodName
is	O
less	O
significant	O
than	O
that	O
of	O
bi	O
-	O
directionality	O
on	O
D	O
1	O
(	O
+1.6	O
%	O
vs	O
+2.0	O
%	O
)	O
.	O
This	O
is	O
reasonable	O
since	O
using	O
opinion	O
words	O
as	O
adjective	O
modifiers	O
placed	O
after	O
the	O
aspects	O
is	O
common	O
in	O
English	O
.	O

We	O
propose	O
Memory	O
Interaction	O
Network	O
(	O
MIN	O
)	O
,	O
a	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
framework	O
,	O
to	O
detect	O
aspect	O
terms	O
from	O
the	O
online	O
user	O
reviews	O
.	O
Compared	O
with	O
previous	O
studies	O
,	O
our	O
MIN	O
has	O
following	O
features	O
:	O
Co	O
-	O
occurrence	O
pattern	O
between	O
aspects	O
and	O
opinions	O
is	O
captured	O
via	O
memory	O
interactions	O
,	O
where	O
the	O
neural	O
memory	O
operations	O
are	O
designed	O
to	O
summarize	O
task	O
-	O
level	O
information	O
and	O
perform	O
interactions	O
.	O
A	O
novel	O
LSTM	B-MethodName
unit	O
with	O
extended	O
memories	O
is	O
developed	O
for	O
memory	O
interactions	O
.	O

Contextual	O
and	O
Non	O
-	O
Contextual	O
Word	B-TaskName
Embeddings	I-TaskName
:	O
an	O
in	O
-	O
depth	O
Linguistic	O
Investigation	O

In	O
this	O
paper	O
we	O
present	O
a	O
comparison	O
between	O
the	O
linguistic	O
knowledge	O
encoded	O
in	O
the	O
internal	O
representations	O
of	O
a	O
contextual	O
Language	O
Model	O
(	O
BERT	B-MethodName
)	O
and	O
a	O
contextual	O
-	O
independent	O
one	O
(	O
Word2vec	O
)	O
.	O
We	O
use	O
a	O
wide	O
set	O
of	O
probing	O
tasks	O
,	O
each	O
of	O
which	O
corresponds	O
to	O
a	O
distinct	O
sentence	O
-	O
level	O
feature	O
extracted	O
from	O
different	O
levels	O
of	O
linguistic	O
annotation	O
.	O
We	O
show	O
that	O
,	O
although	O
BERT	B-MethodName
is	O
capable	O
of	O
understanding	O
the	O
full	O
context	O
of	O
each	O
word	O
in	O
an	O
input	O
sequence	O
,	O
the	O
implicit	O
knowledge	O
encoded	O
in	O
its	O
aggregated	O
sentence	O
representations	O
is	O
still	O
comparable	O
to	O
that	O
of	O
a	O
contextualindependent	O
model	O
.	O
We	O
also	O
find	O
that	O
BERT	B-MethodName
is	O
able	O
to	O
encode	O
sentence	O
-	O
level	O
properties	O
even	O
within	O
single	O
-	O
word	B-TaskName
embeddings	I-TaskName
,	O
obtaining	O
comparable	O
or	O
even	O
superior	O
results	O
than	O
those	O
obtained	O
with	O
sentence	O
representations	O
.	O

Distributional	O
word	O
representations	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
trained	O
on	O
large	O
-	O
scale	O
corpora	O
have	O
rapidly	O
become	O
one	O
of	O
the	O
most	O
prominent	O
component	O
in	O
modern	O
NLP	O
systems	O
.	O
In	O
this	O
context	O
,	O
the	O
recent	O
development	O
of	O
context	O
-	O
dependent	O
embeddings	O
(	O
Peters	O
et	O
al	O
,	O
2018	O
;	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
has	O
shown	O
that	O
such	O
representations	O
are	O
able	O
to	O
achieve	O
state	O
-	O
ofthe	O
-	O
art	O
performance	O
in	O
many	O
complex	O
NLP	O
tasks	O
.	O
However	O
,	O
the	O
introduction	O
of	O
such	O
models	O
made	O
the	O
interpretation	O
of	O
the	O
syntactic	O
and	O
semantic	O
properties	O
learned	O
by	O
their	O
inner	O
representations	O
more	O
complex	O
.	O
Recent	O
studies	O
have	O
begun	O
to	O
study	O
these	O
models	O
in	O
order	O
to	O
understand	O
whether	O
they	O
encode	O
linguistic	O
phenomena	O
even	O
without	O
being	O
explicitly	O
designed	O
to	O
learn	O
such	O
properties	O
(	O
Marvin	O
and	O
Linzen	O
,	O
2018	O
;	O
Goldberg	O
,	O
2019	O
;	O
Warstadt	O
et	O
al	O
,	O
2019	O
)	O
.	O
Much	O
of	O
this	O
work	O
focused	O
on	O
the	O
definition	O
of	O
probing	O
models	O
trained	O
to	O
predict	O
simple	O
linguistic	O
properties	O
from	O
unsupervised	O
representations	O
.	O
In	O
particular	O
,	O
those	O
work	O
provided	O
evidences	O
that	O
contextualized	O
Neural	O
Language	O
Models	O
(	O
NLMs	O
)	O
are	O
able	O
to	O
capture	O
a	O
wide	O
range	O
of	O
linguistic	O
phenomena	O
(	O
Adi	O
et	O
al	O
,	O
2016	O
;	O
Perone	O
et	O
al	O
,	O
2018	O
;	O
Tenney	O
et	O
al	O
,	O
2019b	O
)	O
and	O
even	O
to	O
organize	O
this	O
information	O
in	O
a	O
hierarchical	O
manner	O
(	O
Belinkov	O
et	O
al	O
,	O
2017	O
;	O
Lin	O
et	O
al	O
,	O
2019	O
;	O
Jawahar	O
et	O
al	O
,	O
2019	O
)	O
.	O
Despite	O
this	O
,	O
less	O
study	O
focused	O
on	O
the	O
analysis	O
and	O
the	O
comparison	O
of	O
contextual	O
and	O
non	O
-	O
contextual	O
NLMs	O
according	O
to	O
their	O
ability	O
to	O
encode	O
implicit	O
linguistic	O
properties	O
in	O
their	O
representations	O
.	O
In	O
this	O
paper	O
we	O
perform	O
a	O
large	O
number	O
of	O
probing	O
experiments	O
to	O
analyze	O
and	O
compare	O
the	O
implicit	O
knowledge	O
stored	O
by	O
a	O
contextual	O
and	O
a	O
non	O
-	O
contextual	O
model	O
within	O
their	O
inner	O
representations	O
.	O
In	O
particular	O
,	O
we	O
define	O
two	O
research	O
questions	O
,	O
aimed	O
at	O
understanding	O
:	O
(	O
i	O
)	O
which	O
is	O
the	O
best	O
method	O
for	O
combining	O
BERT	B-MethodName
and	O
Word2vec	O
word	O
representations	O
into	O
sentence	B-TaskName
embeddings	I-TaskName
and	O
how	O
they	O
differently	O
encode	O
properties	O
related	O
to	O
the	O
linguistic	O
structure	O
of	O
a	O
sentence	O
;	O
(	O
ii	O
)	O
whether	O
such	O
sentence	O
-	O
level	O
knowledge	O
is	O
preserved	O
within	O
BERT	B-MethodName
single	O
-	O
word	O
representations	O
.	O
To	O
answer	O
our	O
questions	O
,	O
we	O
rely	O
on	O
a	O
large	O
suite	O
of	O
probing	O
tasks	O
,	O
each	O
of	O
which	O
codifies	O
a	O
particular	O
propriety	O
of	O
a	O
sentence	O
,	O
from	O
very	O
shallow	O
features	O
(	O
such	O
as	O
sentence	O
length	O
and	O
average	O
number	O
of	O
characters	O
per	O
token	O
)	O
to	O
more	O
complex	O
aspects	O
of	O
morphosyntactic	O
and	O
syntactic	O
structure	O
(	O
such	O
as	O
the	O
depth	O
of	O
the	O
whole	O
syntactic	O
tree	O
)	O
,	O
thus	O
making	O
them	O
as	O
suitable	O
to	O
assess	O
the	O
implicit	O
knowledge	O
encoded	O
by	O
a	O
NLM	O
at	O
a	O
deep	O
level	O
of	O
granularity	O
.	O
The	O
remainder	O
of	O
the	O
paper	O
is	O
organized	O
as	O
follows	O
.	O
First	O
we	O
present	O
related	O
work	O
(	O
Sec	O
.	O
2	O
)	O
,	O
then	O
,	O
after	O
briefly	O
presenting	O
our	O
approach	O
(	O
Sec	O
.	O
3	O
)	O
,	O
we	O
describe	O
in	O
more	O
details	O
the	O
data	O
(	O
Sec	O
.	O
3.1	O
)	O
,	O
our	O
set	O
of	O
probing	O
features	O
(	O
Sec	O
.	O
3.2	O
)	O
and	O
the	O
models	O
used	O
for	O
the	O
experiments	O
(	O
Sec	O
.	O
3.3	O
)	O
.	O
Experiments	O
and	O
results	O
are	O
described	O
in	O
Sec	O
.	O
4	O
and	O
5	O
.	O
To	O
conclude	O
,	O
in	O
Sec	O
.	O
6	O
we	O
summarize	O
the	O
main	O
findings	O
of	O
the	O
study	O
.	O
Contributions	O
In	O
this	O
paper	O
:	O
(	O
i	O
)	O
we	O
perform	O
an	O
in	O
-	O
depth	O
study	O
aimed	O
at	O
understanding	O
the	O
linguistic	O
knowledge	O
encoded	O
in	O
a	O
contextual	O
(	O
BERT	B-MethodName
)	O
and	O
a	O
contextual	O
-	O
independent	O
(	O
Word2vec	O
)	O
Neural	O
Language	O
Model	O
;	O
(	O
ii	O
)	O
we	O
evaluate	O
the	O
best	O
method	O
for	O
obtaining	O
sentence	O
-	O
level	O
representations	O
from	O
BERT	B-MethodName
and	O
Word2vec	O
according	O
to	O
a	O
wide	O
spectrum	O
of	O
probing	O
tasks	O
;	O
(	O
iii	O
)	O
we	O
compare	O
the	O
results	O
obtained	O
by	O
BERT	B-MethodName
and	O
Word2vec	O
according	O
to	O
the	O
different	O
combining	O
methods	O
;	O
(	O
iv	O
)	O
we	O
study	O
whether	O
BERT	B-MethodName
is	O
able	O
to	O
encode	O
sentence	O
-	O
level	O
properties	O
within	O
its	O
single	O
word	O
representations	O
.	O

In	O
the	O
last	O
few	O
years	O
,	O
several	O
methods	O
have	O
been	O
devised	O
to	O
open	O
the	O
black	O
box	O
and	O
understand	O
the	O
linguistic	O
information	O
encoded	O
in	O
NLMs	O
(	O
Belinkov	O
and	O
Glass	O
,	O
2019	O
)	O
.	O
They	O
range	O
from	O
techniques	O
to	O
examine	O
the	O
activations	O
of	O
individual	O
neurons	O
(	O
Karpathy	O
et	O
al	O
,	O
2015	O
;	O
Li	O
et	O
al	O
,	O
2016	O
;	O
Kádár	O
et	O
al	O
,	O
2017	O
)	O
to	O
more	O
domain	O
specific	O
approaches	O
,	O
such	O
as	O
interpreting	O
attention	O
mechanisms	O
(	O
Raganato	O
and	O
Tiedemann	O
,	O
2018	O
;	O
Kovaleva	O
et	O
al	O
,	O
2019	O
;	O
Vig	O
and	O
Belinkov	O
,	O
2019	O
)	O
or	O
designing	O
specific	O
probing	O
tasks	O
that	O
a	O
model	O
can	O
solve	O
only	O
if	O
it	O
captures	O
a	O
precise	O
linguistic	O
phenomenon	O
using	O
the	O
contextual	O
word	O
/	O
sentence	B-TaskName
embeddings	I-TaskName
of	O
a	O
pre	O
-	O
trained	O
model	O
as	O
training	O
features	O
(	O
Conneau	O
et	O
al	O
,	O
2018	O
;	O
Zhang	O
and	O
Bowman	O
,	O
2018	O
;	O
Hewitt	O
and	O
Liang	O
,	O
2019	O
)	O
.	O
These	O
latter	O
studies	O
demonstrated	O
that	O
NLMs	O
are	O
able	O
to	O
encode	O
a	O
wide	O
range	O
of	O
linguistic	O
information	O
in	O
a	O
hierarchical	O
manner	O
(	O
Belinkov	O
et	O
al	O
,	O
2017	O
;	O
Blevins	O
et	O
al	O
,	O
2018	O
;	O
Tenney	O
et	O
al	O
,	O
2019b	O
)	O
and	O
even	O
to	O
support	O
the	O
extraction	O
of	O
dependency	O
parse	O
trees	O
(	O
Hewitt	O
and	O
Manning	O
,	O
2019	O
)	O
.	O
Jawahar	O
et	O
al	O
(	O
2019	O
)	O
investigated	O
the	O
representations	O
learned	O
at	O
different	O
layers	O
of	O
BERT	B-MethodName
,	O
showing	O
that	O
lower	O
layer	O
representations	O
are	O
usually	O
better	O
for	O
capturing	O
surface	O
features	O
,	O
while	O
embeddings	O
from	O
higher	O
layers	O
are	O
better	O
for	O
syntactic	O
and	O
semantic	O
properties	O
.	O
Using	O
a	O
suite	O
of	O
probing	O
tasks	O
,	O
Tenney	O
et	O
al	O
(	O
2019a	O
)	O
found	O
that	O
the	O
linguistic	O
knowledge	O
encoded	O
by	O
BERT	B-MethodName
through	O
its	O
12/24	O
layers	O
follows	O
the	O
traditional	O
NLP	O
pipeline	O
:	O
POS	O
tagging	O
,	O
parsing	O
,	O
NER	B-TaskName
,	O
semantic	O
roles	O
and	O
then	O
coreference	O
.	O
,	O
instead	O
,	O
quantified	O
differences	O
in	O
the	O
transferability	O
of	O
individual	O
layers	O
between	O
different	O
models	O
,	O
showing	O
that	O
higher	O
layers	O
of	O
RNNs	O
(	O
ELMo	B-MethodName
)	O
are	O
more	O
task	O
-	O
specific	O
(	O
less	O
general	O
)	O
,	O
while	O
transformer	O
layers	O
(	O
BERT	B-MethodName
)	O
do	O
not	O
exhibit	O
this	O
increase	O
in	O
task	O
-	O
specificity	O
.	O
Closer	O
to	O
our	O
study	O
,	O
Adi	O
et	O
al	O
(	O
2016	O
)	O
proposed	O
a	O
method	O
for	O
analyzing	O
and	O
comparing	O
different	O
sentence	O
representations	O
and	O
different	O
dimensions	O
,	O
exploring	O
the	O
effect	O
of	O
the	O
dimensionality	O
on	O
the	O
resulting	O
representations	O
.	O
In	O
particular	O
,	O
they	O
showed	O
that	O
sentence	O
representations	O
based	O
on	O
averaged	O
Word2vec	O
embeddings	O
are	O
particularly	O
effective	O
and	O
encode	O
a	O
wide	O
amount	O
of	O
information	O
regarding	O
sentence	O
length	O
,	O
while	O
LSTM	B-MethodName
auto	O
-	O
encoders	O
are	O
very	O
effective	O
at	O
capturing	O
word	O
order	O
and	O
word	O
content	O
.	O
Similarly	O
,	O
but	O
focused	O
on	O
the	O
resolution	O
of	O
specific	O
downstream	O
tasks	O
,	O
Shen	O
et	O
al	O
(	O
2018	O
)	O
compared	O
a	O
Single	O
Word	O
Embedding	O
-	O
based	O
model	O
(	O
SWEM	O
-	O
based	O
)	O
with	O
existing	O
recurrent	O
and	O
convolutional	O
networks	O
using	O
a	O
suite	O
of	O
17	O
NLP	O
datasets	O
,	O
demonstrating	O
that	O
simple	O
pooling	O
operations	O
over	O
SWEM	O
-	O
based	O
representations	O
exhibit	O
comparable	O
or	O
even	O
superior	O
performance	O
in	O
the	O
majority	O
of	O
cases	O
considered	O
.	O
On	O
the	O
contrary	O
,	O
Joshi	O
et	O
al	O
(	O
2019	O
)	O
showed	O
that	O
,	O
in	O
the	O
context	O
of	O
three	O
different	O
classification	O
problems	O
in	O
health	O
informatics	O
,	O
context	O
-	O
based	O
representations	O
are	O
a	O
better	O
choice	O
than	O
word	O
-	O
based	O
representations	O
to	O
create	O
vectors	O
.	O
Focusing	O
instead	O
on	O
the	O
geometry	O
of	O
the	O
representation	O
space	O
,	O
Ethayarajh	O
(	O
2019	O
)	O
first	O
showed	O
that	O
the	O
contextualized	O
word	O
representations	O
of	O
ELMo	B-MethodName
,	O
BERT	B-MethodName
and	O
GPT	B-MethodName
-	O
2	O
produce	O
more	O
context	O
specific	O
representations	O
in	O
the	O
upper	O
layers	O
and	O
then	O
proposed	O
a	O
method	O
for	O
creating	O
a	O
new	O
type	O
of	O
static	O
embedding	O
that	O
outperforms	O
GloVe	B-MethodName
and	O
FastText	B-MethodName
on	O
many	O
benchmarks	O
,	O
by	O
simply	O
taking	O
the	O
first	O
principal	O
component	O
of	O
contextualized	O
representations	O
in	O
lower	O
layers	O
of	O
BERT	B-MethodName
.	O
Differently	O
from	O
those	O
latter	O
work	O
,	O
our	O
aim	O
is	O
to	O
investigate	O
the	O
implicit	O
linguistic	O
knowledge	O
encoded	O
in	O
pre	O
-	O
trained	O
contextual	O
and	O
contextualindependent	O
models	O
both	O
at	O
sentence	O
and	O
word	O
levels	O
.	O

We	O
studied	O
how	O
layer	O
-	O
wise	O
internal	O
representations	O
of	O
BERT	B-MethodName
encode	O
a	O
wide	O
spectrum	O
of	O
linguistic	O
properties	O
and	O
how	O
such	O
implicit	O
knowledge	O
differs	O
from	O
that	O
learned	O
by	O
a	O
context	O
-	O
independent	O
model	O
such	O
as	O
Word2vec	O
.	O
Following	O
the	O
probing	O
task	O
approach	O
as	O
defined	O
in	O
Conneau	O
et	O
al	O
(	O
2018	O
)	O
,	O
we	O
proposed	O
a	O
suite	O
of	O
68	O
probing	O
tasks	O
,	O
each	O
of	O
which	O
corresponds	O
to	O
a	O
distinct	O
linguistic	O
feature	O
capturing	O
raw	O
-	O
text	O
,	O
lexical	O
,	O
morpho	O
-	O
syntactic	O
and	O
syntactic	O
characteristics	O
of	O
a	O
sentence	O
.	O
More	O
specifically	O
,	O
we	O
defined	O
two	O
sets	O
of	O
experiments	O
.	O
The	O
first	O
consists	O
in	O
evaluating	O
which	O
is	O
the	O
best	O
method	O
for	O
generating	O
sentence	O
-	O
level	O
embeddings	O
using	O
BERT	B-MethodName
and	O
Word2vec	O
single	O
-	O
word	O
representations	O
.	O
In	O
particular	O
,	O
we	O
defined	O
a	O
simple	O
probing	O
model	O
that	O
takes	O
as	O
input	O
layer	O
-	O
wise	O
BERT	B-MethodName
and	O
Word2vec	O
combined	O
representations	O
for	O
each	O
sentence	O
of	O
a	O
gold	O
standard	O
Universal	B-DatasetName
Dependencies	I-DatasetName
(	O
UD	B-DatasetName
)	O
(	O
Nivre	O
et	O
al	O
,	O
2016	O
)	O
English	O
dataset	O
and	O
predicts	O
the	O
actual	O
value	O
of	O
a	O
given	O
probing	O
feature	O
.	O
Moreover	O
,	O
we	O
compared	O
the	O
results	O
to	O
understand	O
which	O
model	O
performs	O
better	O
according	O
to	O
different	O
levels	O
of	O
linguistic	O
sophistication	O
.	O
In	O
the	O
second	O
set	O
of	O
experiments	O
,	O
we	O
measured	O
how	O
many	O
sentence	O
-	O
level	O
properties	O
are	O
encoded	O
in	O
single	O
-	O
word	O
representations	O
.	O
To	O
do	O
so	O
,	O
we	O
performed	O
our	O
set	O
of	O
probing	O
tasks	O
using	O
the	O
embeddings	O
extracted	O
from	O
both	O
BERT	B-MethodName
and	O
Word2vec	O
individual	O
tokens	O
.	O
In	O
particular	O
,	O
we	O
considered	O
the	O
word	O
representations	O
corresponding	O
to	O
the	O
first	O
,	O
last	O
and	O
two	O
internal	O
tokens	O
for	O
each	O
sentence	O
of	O
the	O
UD	B-DatasetName
dataset	O
.	O

In	O
order	O
to	O
perform	O
the	O
probing	O
experiments	O
on	O
gold	O
annotated	O
sentences	O
,	O
we	O
relied	O
on	O
the	O
Universal	B-DatasetName
Dependencies	I-DatasetName
(	O
UD	B-DatasetName
)	O
English	O
dataset	O
.	O
The	O
dataset	O
includes	O
three	O
UD	B-DatasetName
English	O
treebanks	O
:	O
UD	B-DatasetName
English	O
-	O
ParTUT	O
,	O
a	O
conversion	O
of	O
a	O
multilin	O
-	O
gual	O
parallel	O
treebank	O
consisting	O
of	O
a	O
variety	O
of	O
text	O
genres	O
,	O
including	O
talks	O
,	O
legal	O
texts	O
and	O
Wikipedia	O
articles	O
(	O
Sanguinetti	O
and	O
Bosco	O
,	O
2015	O
)	O
;	O
the	O
Universal	B-DatasetName
Dependencies	I-DatasetName
version	O
annotation	O
from	O
the	O
GUM	B-DatasetName
corpus	O
(	O
Zeldes	O
,	O
2017	O
)	O
;	O
the	O
English	B-DatasetName
Web	I-DatasetName
Treebank	I-DatasetName
(	O
EWT	O
)	O
,	O
a	O
gold	O
standard	O
universal	B-DatasetName
dependencies	I-DatasetName
corpus	O
for	O
English	O
(	O
Silveira	O
et	O
al	O
,	O
2014	O
)	O
.	O
Overall	O
,	O
the	O
final	O
dataset	O
consists	O
of	O
23	O
,	O
943	O
sentences	O
.	O

As	O
previously	O
mentioned	O
,	O
our	O
method	O
is	O
in	O
line	O
with	O
the	O
probing	O
tasks	O
approach	O
defined	O
in	O
Conneau	O
et	O
al	O
(	O
2018	O
)	O
,	O
which	O
aims	O
to	O
capture	O
linguistic	O
information	O
from	O
the	O
representations	O
learned	O
by	O
a	O
NLM	O
.	O
Specifically	O
,	O
in	O
our	O
work	O
,	O
each	O
probing	O
task	O
correspond	O
to	O
predict	O
the	O
value	O
of	O
a	O
specific	O
linguistic	O
feature	O
automatically	O
extracted	O
from	O
the	O
POS	O
tagged	O
and	O
dependency	O
parsed	O
sentences	O
in	O
the	O
English	O
UD	B-DatasetName
dataset	O
.	O
The	O
set	O
of	O
features	O
is	O
based	O
on	O
the	O
ones	O
described	O
in	O
Brunato	O
et	O
al	O
(	O
2020	O
)	O
and	O
it	O
includes	O
characteristics	O
acquired	O
from	O
raw	O
,	O
morphosyntactic	O
and	O
syntactic	O
levels	O
of	O
annotation	O
.	O
As	O
described	O
in	O
Brunato	O
et	O
al	O
(	O
2020	O
)	O
,	O
this	O
set	O
of	O
features	O
has	O
been	O
shown	O
to	O
have	O
a	O
highly	O
predictive	O
role	O
when	O
leveraged	O
by	O
traditional	O
learning	O
models	O
on	O
a	O
variety	O
of	O
classification	O
problems	O
,	O
covering	O
different	O
aspects	O
of	O
stylometric	O
and	O
complexity	O
analysis	O
.	O
As	O
shown	O
in	O
Table	O
1	O
,	O
these	O
features	O
capture	O
sev	O
-	O
eral	O
linguistic	O
phenomena	O
ranging	O
from	O
the	O
average	O
length	O
of	O
words	O
and	O
sentence	O
,	O
to	O
morpho	O
-	O
syntactic	O
information	O
both	O
at	O
the	O
level	O
of	O
POS	O
distribution	O
and	O
about	O
the	O
inflectional	O
properties	O
of	O
verbs	O
.	O
More	O
complex	O
aspects	O
of	O
sentence	O
structure	O
are	O
derived	O
from	O
syntactic	O
annotation	O
and	O
model	O
global	O
and	O
local	O
properties	O
of	O
parsed	O
tree	O
structure	O
,	O
with	O
a	O
focus	O
on	O
subtrees	O
of	O
verbal	O
heads	O
,	O
the	O
order	O
of	O
subjects	O
and	O
objects	O
with	O
respect	O
to	O
the	O
verb	O
,	O
the	O
distribution	O
of	O
UD	B-DatasetName
syntactic	O
relations	O
and	O
features	O
referring	O
to	O
the	O
use	O
of	O
subordination	O
.	O

We	O
relied	O
on	O
a	O
pre	O
-	O
trained	O
English	O
version	O
of	O
BERT	B-MethodName
(	O
BERT	B-MethodName
-	O
base	O
uncased	O
,	O
12	O
layers	O
)	O
for	O
the	O
extraction	O
of	O
the	O
contextual	O
word	B-TaskName
embeddings	I-TaskName
.	O
To	O
obtain	O
the	O
representations	O
for	O
our	O
sentence	O
-	O
level	O
tasks	O
we	O
experimented	O
the	O
activation	O
of	O
the	O
first	O
input	O
token	O
(	O
[	O
CLS	O
]	O
)	O
1	O
and	O
four	O
different	O
combining	O
methods	O
:	O
Max	O
-	O
pooling	O
,	O
Min	O
-	O
pooling	O
,	O
Mean	O
and	O
Sum	O
.	O
Each	O
of	O
this	O
four	O
combining	O
methods	O
returns	O
a	O
single	O
s	O
vector	O
,	O
such	O
that	O
each	O
s	O
n	O
is	O
obtained	O
by	O
combining	O
the	O
n	O
th	O
components	O
w	O
1n	O
,	O
w	O
2n	O
,	O
...	O
,	O
w	O
mn	O
of	O
the	O
embedding	O
of	O
each	O
word	O
in	O
the	O
input	O
sentence	O
.	O
In	O
order	O
to	O
conduct	O
a	O
comparison	O
of	O
contextbased	O
and	O
word	O
-	O
based	O
representations	O
when	O
solving	O
our	O
set	O
of	O
probing	O
tasks	O
,	O
we	O
performed	O
all	O
the	O
probing	O
experiments	O
using	O
also	O
the	O
embeddings	O
extracted	O
from	O
a	O
pre	O
-	O
trained	O
version	O
of	O
Word2vec	O
.	O
In	O
particular	O
,	O
we	O
trained	O
the	O
model	O
on	O
the	O
English	O
Wikipedia	O
dataset	O
(	O
dump	O
of	O
March	O
2020	O
)	O
,	O
resulting	O
in	O
300	O
-	O
dimensional	O
vectors	O
.	O
In	O
the	O
same	O
manner	O
as	O
BERT	B-MethodName
's	O
contextual	O
representations	O
,	O
we	O
experimented	O
four	O
combining	O
methods	O
:	O
Max	O
-	O
pooling	O
,	O
Min	O
-	O
pooling	O
,	O
Mean	O
and	O
Sum	O
.	O
We	O
used	O
a	O
linear	O
Support	O
Vector	O
Regression	O
model	O
(	O
LinearSVR	O
)	O
as	O
probing	O
model	O
.	O

Once	O
we	O
have	O
probed	O
the	O
linguistic	O
knowledge	O
encoded	O
by	O
BERT	B-MethodName
and	O
Word2vec	O
using	O
different	O
strategies	O
for	O
computing	O
sentence	B-TaskName
embeddings	I-TaskName
,	O
we	O
investigated	O
how	O
much	O
information	O
about	O
the	O
structure	O
of	O
a	O
sentence	O
is	O
encoded	O
within	O
single	O
-	O
word	O
contextual	O
representations	O
.	O
For	O
doing	O
so	O
,	O
we	O
performed	O
our	O
sentence	O
-	O
level	O
probing	O
tasks	O
using	O
a	O
single	O
BERT	B-MethodName
word	O
embedding	O
for	O
each	O
sentence	O
in	O
the	O
UD	B-DatasetName
dataset	O
.	O
We	O
tested	O
four	O
different	O
words	O
,	O
corresponding	O
to	O
the	O
first	O
,	O
the	O
last	O
and	O
two	O
internal	O
tokens	O
for	O
each	O
sentence	O
in	O
the	O
UD	B-DatasetName
dataset	O
.	O
In	O
particular	O
,	O
we	O
extracted	O
the	O
embeddings	O
from	O
the	O
output	O
layer	O
(	O
-	O
1	O
)	O
and	O
from	O
the	O
layer	O
that	O
achieved	O
best	O
results	O
in	O
the	O
previous	O
experiments	O
(	O
-	O
8	O
)	O
.	O
We	O
used	O
probing	O
scores	O
obtained	O
with	O
Word2vec	O
embeddings	O
for	O
the	O
same	O
tokens	O
as	O
baseline	O
.	O
In	O
Table	O
5	O
we	O
report	O
average	O
ρ	O
scores	O
obtained	O
by	O
BERT	B-MethodName
(	O
BERT	B-MethodName
-	O
*	O
)	O
and	O
Word2vec	O
(	O
Word2vec	O
-	O
*	O
)	O
according	O
to	O
word	O
-	O
level	O
representations	O
extracted	O
from	O
the	O
four	O
tokens	O
mentioned	O
above	O
.	O
Results	O
were	O
computed	O
aggregating	O
all	O
probing	O
results	O
(	O
All	O
)	O
and	O
according	O
to	O
raw	O
text	O
(	O
Raw	O
)	O
,	O
morphosyntactic	O
(	O
Morphosyntax	O
)	O
and	O
syntatic	O
(	O
Syntax	O
)	O
levels	O
of	O
annotation	O
.	O
For	O
comparison	O
,	O
we	O
also	O
report	O
average	O
scores	O
obtained	O
with	O
the	O
[	O
CLS	O
]	O
token	O
.	O
As	O
a	O
first	O
remark	O
,	O
we	O
can	O
clearly	O
notice	O
that	O
even	O
with	O
a	O
single	O
-	O
word	O
embedding	O
BERT	B-MethodName
is	O
able	O
to	O
encode	O
a	O
wide	O
spectrum	O
of	O
sentence	O
-	O
level	O
linguistic	O
properties	O
.	O
This	O
result	O
allows	O
us	O
to	O
highlight	O
the	O
main	O
potential	O
of	O
contextual	O
representations	O
,	O
i.e.	O
the	O
capability	O
of	O
capturing	O
linguistic	O
phenomena	O
that	O
refer	O
to	O
the	O
entire	O
input	O
sequence	O
within	O
single	O
-	O
word	O
representations	O
.	O
An	O
interesting	O
observation	O
is	O
that	O
,	O
except	O
for	O
the	O
raw	O
text	O
features	O
,	O
for	O
which	O
the	O
best	O
scores	O
are	O
achieved	O
using	O
[	O
CLS	O
]	O
,	O
higher	O
performance	O
are	O
obtained	O
with	O
the	O
embeddings	O
corresponding	O
to	O
BERT	B-MethodName
-	O
4	O
,	O
i.e.	O
the	O
last	O
token	O
of	O
each	O
sentence	O
.	O
This	O
result	O
seems	O
to	O
indicate	O
that	O
[	O
CLS	O
]	O
,	O
although	O
being	O
used	O
for	O
classification	O
predictions	O
,	O
does	O
not	O
necessarily	O
correspond	O
to	O
the	O
most	O
linguistically	O
informative	O
token	O
within	O
each	O
input	O
sequence	O
.	O
Comparing	O
the	O
results	O
with	O
those	O
achieved	O
using	O
Word2vec	O
word	B-TaskName
embeddings	I-TaskName
,	O
we	O
notice	O
that	O
BERT	B-MethodName
scores	O
greatly	O
outperform	O
Word2vec	O
for	O
all	O
the	O
probing	O
tasks	O
.	O
This	O
is	O
a	O
straightforward	O
result	O
and	O
can	O
be	O
easily	O
explained	O
by	O
the	O
fact	O
that	O
the	O
lack	O
of	O
contextual	O
knowledge	O
does	O
not	O
allow	O
singleword	O
representations	O
to	O
encode	O
information	O
that	O
are	O
related	O
to	O
the	O
structure	O
of	O
the	O
whole	O
sentence	O
.	O
Since	O
the	O
latter	O
results	O
demonstrated	O
that	O
BERT	B-MethodName
is	O
capable	O
of	O
encoding	O
many	O
sentence	O
-	O
level	O
properties	O
within	O
its	O
single	O
word	O
representations	O
,	O
as	O
a	O
last	O
analysis	O
,	O
we	O
decided	O
to	O
compare	O
these	O
results	O
with	O
the	O
ones	O
obtained	O
using	O
sentence	B-TaskName
embeddings	I-TaskName
.	O
In	O
particular	O
,	O
Figure	O
3	O
reports	O
probing	O
scores	O
obtained	O
by	O
BERT	B-MethodName
single	O
word	O
(	O
tok	O
*	O
)	O
and	O
Mean	O
sentence	O
representations	O
(	O
sent	O
)	O
extracted	O
from	O
the	O
output	O
layer	O
(	O
-	O
1	O
)	O
and	O
from	O
the	O
layer	O
that	O
achieved	O
best	O
results	O
in	O
average	O
(	O
-	O
8	O
)	O
.	O
As	O
already	O
mentioned	O
,	O
for	O
many	O
of	O
these	O
probing	O
tasks	O
,	O
word	B-TaskName
embeddings	I-TaskName
performance	O
is	O
comparable	O
to	O
that	O
obtained	O
with	O
the	O
aggregated	O
sentence	O
representations	O
.	O
Nevertheless	O
,	O
there	O
are	O
several	O
cases	O
in	O
which	O
the	O
difference	O
between	O
performance	O
is	O
particularly	O
significant	O
.	O
Interestingly	O
,	O
we	O
can	O
notice	O
that	O
aggregated	O
sentence	O
representations	O
are	O
generally	O
better	O
for	O
predicting	O
properties	O
belonging	O
to	O
the	O
left	O
heatmap	B-MethodName
,	O
i.e.	O
to	O
the	O
group	O
of	O
features	O
more	O
related	O
to	O
syntactic	O
properties	O
.	O
This	O
is	O
particularly	O
noticeable	O
for	O
the	O
average	O
number	O
of	O
tokens	O
per	O
clause	O
(	O
avg	O
token	O
per	O
clause	O
)	O
or	O
the	O
distribution	O
of	O
subordinate	O
chains	O
by	O
length	O
(	O
subord	O
dist	O
)	O
,	O
for	O
which	O
we	O
observe	O
an	O
improvement	O
from	O
word	O
-	O
level	O
to	O
sentence	O
-	O
level	O
representations	O
of	O
more	O
than	O
.10	O
ρ	O
points	O
.	O
On	O
the	O
contrary	O
,	O
probing	O
features	O
belonging	O
to	O
the	O
right	O
heatmap	B-MethodName
,	O
therefore	O
more	O
close	O
to	O
raw	O
text	O
and	O
morphosyntactic	O
properties	O
,	O
are	O
generally	O
better	O
predicted	O
using	O
single	O
word	B-TaskName
embeddings	I-TaskName
,	O
especially	O
when	O
considering	O
the	O
inner	O
representations	O
corresponding	O
to	O
the	O
last	O
token	O
in	O
each	O
sentence	O
(	O
tok	O
4	O
)	O
.	O
The	O
property	O
most	O
affected	O
by	O
the	O
difference	O
in	O
scores	O
between	O
wordand	O
sentence	O
-	O
level	O
embeddings	O
is	O
the	O
the	O
distribution	O
of	O
periods	O
(	O
xpos	O
dist	O
.	O
)	O
.	O
Focusing	O
instead	O
on	O
differences	O
in	O
performance	O
between	O
the	O
two	O
considered	O
layers	O
,	O
we	O
can	O
notice	O
that	O
regardless	O
of	O
the	O
method	O
used	O
to	O
predict	O
each	O
feature	O
,	O
the	O
representations	O
learned	O
by	O
BERT	B-MethodName
tend	O
to	O
lose	O
their	O
precision	O
in	O
encoding	O
our	O
set	O
of	O
linguistic	O
properties	O
,	O
most	O
likely	O
because	O
the	O
model	O
is	O
storing	O
task	O
-	O
specific	O
information	O
(	O
Masked	B-TaskName
Language	I-TaskName
Modeling	I-TaskName
task	O
)	O
at	O
the	O
expense	O
of	O
its	O
ability	O
to	O
encode	O
general	B-TaskName
knowledge	I-TaskName
about	O
the	O
language	O
.	O

In	O
this	O
paper	O
we	O
studied	O
the	O
linguistic	O
knowledge	O
implicitly	O
encoded	O
in	O
the	O
internal	O
representations	O
of	O
a	O
contextual	O
Language	O
Model	O
(	O
BERT	B-MethodName
)	O
and	O
a	O
contextual	O
-	O
independent	O
one	O
(	O
Word2vec	O
)	O
.	O
Using	O
a	O
suite	O
of	O
68	O
probing	O
tasks	O
and	O
testing	O
different	O
methods	O
for	O
combining	O
word	B-TaskName
embeddings	I-TaskName
into	O
sentence	O
representations	O
,	O
we	O
showed	O
that	O
BERT	B-MethodName
and	O
Word2vec	O
encode	O
a	O
wide	O
set	O
of	O
sentence	O
-	O
level	O
linguistic	O
properties	O
in	O
a	O
similar	O
manner	O
.	O
Nevertheless	O
,	O
we	O
found	O
that	O
for	O
Word2vec	O
the	O
best	O
method	O
for	O
obtaining	O
sentence	O
representations	O
is	O
the	O
Sum	O
,	O
while	O
BERT	B-MethodName
is	O
more	O
effective	O
when	O
averaging	O
all	O
the	O
single	O
-	O
word	O
representations	O
(	O
Mean	O
method	O
)	O
.	O
Moreover	O
,	O
we	O
showed	O
that	O
BERT	B-MethodName
is	O
able	O
in	O
storing	O
features	O
that	O
are	O
mainly	O
related	O
to	O
raw	O
text	O
and	O
syntactic	O
properties	O
,	O
while	O
Word2vec	O
is	O
good	O
at	O
predicting	O
morphosyntactic	O
characteristics	O
.	O
Finally	O
,	O
we	O
showed	O
that	O
BERT	B-MethodName
is	O
able	O
to	O
encode	O
sentence	O
-	O
level	O
linguistic	O
phenomena	O
even	O
within	O
single	O
-	O
word	B-TaskName
embeddings	I-TaskName
,	O
exhibiting	O
comparable	O
or	O
even	O
superior	O
performance	O
than	O
those	O
obtained	O
with	O
aggregated	O
sentence	O
representations	O
.	O
Moreover	O
,	O
we	O
found	O
that	O
,	O
at	O
least	O
for	O
morphosyntactic	O
and	O
syntactic	O
characteristics	O
,	O
the	O
most	O
informative	O
word	O
representation	O
is	O
the	O
one	O
that	O
correspond	O
to	O
the	O
last	O
token	O
of	O
each	O
input	O
sequence	O
and	O
not	O
,	O
as	O
might	O
be	O
expected	O
,	O
to	O
the	O
[	O
CLS	O
]	O
special	O
token	O
.	O

Many	O
works	O
related	O
to	O
automatic	O
detection	O
of	O
misogyny	O
,	O
hate	O
,	O
sexism	O
on	O
social	O
media	O
and	O
web	O
have	O
been	O
proposed	O
.	O
Abir	O
Rahali	O
(	O
Rahali	O
et	O
al	O
,	O
2021	O
)	O
proposed	O
a	O
approach	O
for	O
automatic	O
misogyny	O
detection	O
in	O
social	O
media	O
using	O
attention	O
based	O
bidirectional	B-MethodName
LSTM	I-MethodName
.	O
Endang	O
Wahyu	O
Pamungkas	O
(	O
Pamungkas	O
et	O
al	O
,	O
2020	O
)	O
proposed	O
a	O
method	O
for	O
Automatic	O
Identification	O
of	O
Misogyny	O
in	O
English	O
and	O
Italian	O
Tweets	O
at	O
EVALITA	O
2018	O
with	O
a	O
Multilingual	O
Hate	O
Lexicon	O
.	O
Mario	O
Anzovino	O
,	O
Elisabetta	O
Fersini	O
(	O
Anzovino	O
et	O
al	O
,	O
2018	O
)	O
proposed	O
a	O
method	O
for	O
Automatic	O
Identification	O
and	O
Classification	B-TaskName
of	O
Misogynistic	O
Language	O
on	O
Twitter	O
.	O
The	O
main	O
contribution	O
of	O
this	O
paper	O
is	O
two	O
-	O
fold	O
:	O
(	O
1	O
)	O
a	O
corpus	O
of	O
misogynous	O
tweets	O
,	O
labelled	O
from	O
different	O
perspective	O
and	O
(	O
2	O
)	O
an	O
exploratory	O
investigation	O
on	O
NLP	O
features	O
and	O
ML	O
models	O
for	O
detecting	O
and	O
classifying	O
misogynistic	O
language	O
.	O
Rachael	O
Fulper	O
(	O
Fulper	O
et	O
al	O
,	O
2014	O
)	O
proposed	O
a	O
relation	O
between	O
misogynistic	O
language	O
in	O
twitter	O
and	O
sexual	O
Violence	O
.	O
In	O
their	O
paper	O
they	O
consider	O
all	O
50	O
states	O
in	O
Washington	O
DC	O
.	O
Lakes	O
Goenaga	O
,	O
Aitziber	O
(	O
Goenaga	O
et	O
al	O
,	O
2018	O
)	O
Atutxa	O
proposed	O
a	O
Automatic	O
misogyny	O
identification	O
using	O
neural	O
networks	O
.	O
In	O
this	O
paper	O
they	O
focus	O
on	O
recurrent	O
neural	O
network	O
(	O
RNN	O
)	O
approach	O
using	O
a	O
Bidirectional	O
Long	O
Short	O
Term	O
Memory	O
(	O
Bi	O
-	O
LSTM	B-MethodName
)	O
.	O

First	O
,	O
we	O
removed	O
all	O
the	O
punctuations	O
,	O
numbers	O
,	O
links	O
and	O
stop	O
words	O
.	O
We	O
have	O
used	O
lemmatization	B-TaskName
for	O
grouping	O
together	O
the	O
different	O
forms	O
of	O
a	O
word	O
into	O
a	O
single	O
word	O
.	O
NLTK	O
wordnet	O
(	O
Loper	O
and	O
Bird	O
,	O
2002	O
)	O
is	O
used	O
for	O
lemmatization	B-TaskName
.	O

TfidfVectorizer	O
(	O
Kumar	B-DatasetName
and	O
Subba	O
,	O
2020	O
)	O
is	O
used	O
for	O
converting	O
the	O
text	O
into	O
numerical	O
features	O
.	O
Pipeline	O
1	O
is	O
used	O
for	O
doing	O
TfidfVectorizer	O
and	O
classification	O
in	O
pipelined	O
manner	O
.	O
Tokenizer	O
by	O
keras	O
library	O
is	O
used	O
for	O
LSTM	B-MethodName
and	O
Bert	O
.	O
For	O
Logistic	B-MethodName
regression	I-MethodName
and	O
SVM	B-MethodName
we	O
have	O
used	O
TfidfVectorizer	O
from	O
scikit	O
-	O
learn	O
library	O
.	O

Multivalent	O
Entailment	O
Graphs	O
for	O
Question	B-TaskName
Answering	I-TaskName

Drawing	O
inferences	O
between	O
open	O
-	O
domain	O
natural	O
language	O
predicates	O
is	O
a	O
necessity	O
for	O
true	O
language	O
understanding	O
.	O
There	O
has	O
been	O
much	O
progress	O
in	O
unsupervised	O
learning	O
of	O
entailment	O
graphs	O
for	O
this	O
purpose	O
.	O
We	O
make	O
three	O
contributions	O
:	O
(	O
1	O
)	O
we	O
reinterpret	O
the	O
Distributional	O
Inclusion	O
Hypothesis	O
to	O
model	O
entailment	O
between	O
predicates	O
of	O
different	O
valencies	O
,	O
like	O
DEFEAT	O
(	O
Biden	O
,	O
Trump	O
)	O
WIN	O
(	O
Biden	O
)	O
;	O
(	O
2	O
)	O
we	O
actualize	O
this	O
theory	O
by	O
learning	O
unsupervised	O
Multivalent	O
Entailment	O
Graphs	O
of	O
open	O
-	O
domain	O
predicates	O
;	O
and	O
(	O
3	O
)	O
we	O
demonstrate	O
the	O
capabilities	O
of	O
these	O
graphs	O
on	O
a	O
novel	O
question	B-TaskName
answering	I-TaskName
task	O
.	O
We	O
show	O
that	O
directional	O
entailment	O
is	O
more	O
helpful	O
for	O
inference	O
than	O
non	O
-	O
directional	O
similarity	O
on	O
questions	O
of	O
fine	O
-	O
grained	O
semantics	O
.	O
We	O
also	O
show	O
that	O
drawing	O
on	O
evidence	O
across	O
valencies	O
answers	O
more	O
questions	O
than	O
by	O
using	O
only	O
the	O
same	O
valency	O
evidence	O
.	O
*	O
Now	O
at	O
Google	B-DatasetName
Research	O
.	O
1	O
The	O
murder	O
mystery	O
board	O
game	O
Clue	O
(	O
also	O
known	O
as	O
Cluedo	O
)	O
lends	O
inspiration	O
to	O
this	O
project	O
.	O

We	O
are	O
reading	O
a	O
mystery	O
about	O
a	O
dark	O
and	O
foreboding	O
manor	O
and	O
have	O
one	O
question	O
:	O
"	O
is	O
Mr.	O
Boddy	O
dead	O
?	O
"	O
1	O
Our	O
text	O
might	O
say	O
"	O
Colonel	O
Mustard	O
killed	O
Mr.	O
Boddy	O
,	O
"	O
or	O
"	O
Mr.	O
Boddy	O
was	O
murdered	O
in	O
the	O
kitchen	O
with	O
a	O
candlestick	O
,	O
"	O
either	O
of	O
which	O
answers	O
the	O
question	O
,	O
but	O
only	O
via	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
.	O
An	O
Entailment	O
Graph	O
(	O
EG	O
)	O
is	O
a	O
structure	O
of	O
meaning	O
postulates	O
supporting	O
these	O
inferences	O
such	O
as	O
"	O
if	O
A	O
kills	O
B	O
,	O
then	O
B	O
is	O
dead	O
.	O
"	O
Entailment	O
Graphs	O
contain	O
vertices	O
of	O
opendomain	O
natural	O
language	O
predicates	O
and	O
entailments	O
between	O
them	O
are	O
represented	O
as	O
directed	O
edges	O
.	O
Previous	O
models	O
learn	O
predicates	O
of	O
a	O
single	O
valency	O
,	O
the	O
number	O
and	O
types	O
of	O
arguments	O
controlled	O
by	O
the	O
predicate	O
.	O
Commonly	O
these	O
are	O
binary	O
graphs	O
,	O
which	O
can	O
not	O
model	O
single	O
-	O
argument	O
predicates	O
like	O
the	O
entity	O
states	O
"	O
is	O
dead	O
"	O
or	O
"	O
is	O
an	O
author	O
.	O
"	O
This	O
means	O
they	O
miss	O
a	O
variety	O
of	O
entailments	O
in	O
text	O
that	O
could	O
be	O
used	O
to	O
answer	O
questions	O
such	O
as	O
our	O
example	O
.	O
The	O
Distributional	O
Inclusion	O
Hypothesis	O
(	O
DIH	O
)	O
(	O
Dagan	O
et	O
al	O
,	O
1999	O
;	O
Kartsaklis	O
and	O
Sadrzadeh	O
,	O
2016	O
)	O
is	O
a	O
theory	O
which	O
has	O
been	O
used	O
effectively	O
in	O
unsupervised	O
learning	O
of	O
these	O
same	O
-	O
valency	O
entailment	O
graphs	O
(	O
Geffet	O
and	O
Dagan	O
,	O
2005	O
;	O
Berant	O
et	O
al	O
,	O
2010	O
;	O
Hosseini	O
,	O
2021	O
)	O
.	O
In	O
this	O
work	O
the	O
DIH	O
is	O
reinterpreted	O
in	O
a	O
way	O
which	O
supports	O
learning	O
entailments	O
between	O
predicates	O
of	O
different	O
valencies	O
such	O
as	O
KILL	O
(	O
Mustard	O
,	O
Boddy	O
)	O
DIE	O
(	O
Boddy	O
)	O
.	O
We	O
extend	O
the	O
work	O
of	O
Hosseini	O
et	O
al	O
(	O
2018	O
)	O
and	O
develop	O
a	O
new	O
Multivalent	O
Entailment	O
Graph	O
(	O
MGraph	O
)	O
where	O
vertices	O
may	O
be	O
predicates	O
of	O
different	O
valencies	O
.	O
This	O
results	O
in	O
new	O
kinds	O
of	O
entailments	O
that	O
answer	O
a	O
broader	O
range	O
of	O
questions	O
including	O
entity	O
state	O
.	O
We	O
further	O
pose	O
a	O
true	O
-	O
false	O
question	B-TaskName
answering	I-TaskName
task	O
generated	O
automatically	O
from	O
news	O
text	O
.	O
Our	O
model	O
draws	O
inferences	O
across	O
propositions	O
of	O
different	O
valencies	O
to	O
answer	O
more	O
questions	O
than	O
using	O
same	O
-	O
valence	O
entailment	O
graphs	O
.	O
We	O
also	O
compare	O
with	O
several	O
baselines	O
,	O
including	O
unsupervised	O
pretrained	B-TaskName
language	I-TaskName
models	I-TaskName
,	O
and	O
show	O
that	O
our	O
directional	O
entailment	O
graphs	O
succeed	O
over	O
non	O
-	O
directional	O
similarity	O
measures	O
in	O
answering	O
questions	O
of	O
fine	O
-	O
grained	O
semantics	O
.	O
Advantageously	O
,	O
EGs	O
are	O
structures	O
designed	O
to	O
be	O
queried	O
,	O
so	O
they	O
are	O
inherently	O
explainable	O
.	O
This	O
research	O
is	O
conducted	O
in	O
English	O
,	O
but	O
as	O
an	O
unsupervised	O
algorithm	O
it	O
may	O
be	O
applied	O
to	O
other	O
languages	O
given	O
a	O
parser	O
and	O
named	O
entity	O
linker	O
.	O

The	O
task	O
of	O
recognizing	O
textual	O
entailment	O
(	O
Dagan	O
et	O
al	O
,	O
2006	O
)	O
requires	O
models	O
to	O
predict	O
a	O
relation	O
between	O
a	O
text	O
T	O
and	O
hypothesis	O
H	O
;	O
"	O
T	O
entails	O
H	O
if	O
,	O
typically	O
,	O
a	O
human	O
reading	O
T	O
would	O
infer	O
that	O
H	O
is	O
most	O
likely	O
true	O
.	O
"	O
From	O
here	O
,	O
research	O
has	O
moved	O
in	O
several	O
directions	O
.	O
We	O
study	O
predicates	O
,	O
including	O
verbs	O
and	O
phrases	O
that	O
apply	O
to	O
arguments	O
.	O
Research	O
in	O
predicate	O
entailment	O
graphs	O
has	O
evolved	O
from	O
"	O
local	O
"	O
learning	O
of	O
entailment	O
rules	O
(	O
Geffet	O
and	O
Dagan	O
,	O
2005	O
;	O
Szpektor	O
and	O
Dagan	O
,	O
2008	O
)	O
to	O
later	O
work	O
on	O
joint	O
learning	O
of	O
"	O
globalized	O
"	O
rules	O
,	O
overcoming	O
sparsity	O
in	O
local	O
graphs	O
(	O
Berant	O
et	O
al	O
,	O
2010	O
;	O
Hosseini	O
et	O
al	O
,	O
2018	O
)	O
.	O
These	O
graphs	O
frequently	O
rely	O
on	O
the	O
DIH	O
for	O
the	O
local	O
learning	O
step	O
to	O
learn	O
initial	O
predicate	O
entailments	O
.	O
The	O
DIH	O
states	O
that	O
for	O
some	O
predicates	O
p	O
and	O
q	O
,	O
if	O
the	O
contextual	O
features	O
of	O
p	O
are	O
included	O
in	O
those	O
of	O
q	O
,	O
then	O
p	O
entails	O
q	O
(	O
Geffet	O
and	O
Dagan	O
,	O
2005	O
)	O
.	O
In	O
previous	O
work	O
predicate	O
arguments	O
are	O
successfully	O
used	O
as	O
these	O
contextual	O
features	O
,	O
but	O
only	O
predicates	O
of	O
the	O
same	O
valency	O
are	O
considered	O
(	O
e.g.	O
binary	O
predicates	O
entail	O
binary	O
;	O
unary	O
entail	O
unary	O
)	O
,	O
and	O
further	O
research	O
computes	O
additional	O
edges	O
in	O
these	O
same	O
-	O
valency	O
graphs	O
such	O
as	O
with	O
link	B-TaskName
prediction	I-TaskName
(	O
Hosseini	O
et	O
al	O
,	O
2019	O
)	O
.	O
However	O
,	O
this	O
leaves	O
out	O
crucial	O
inferences	O
that	O
cross	O
valencies	O
such	O
as	O
the	O
kill	O
/	O
die	O
example	O
,	O
which	O
are	O
easy	O
for	O
humans	O
.	O
We	O
generalize	O
the	O
DIH	O
to	O
learn	O
entailments	O
within	O
and	O
across	O
valencies	O
.	O
Typing	O
is	O
very	O
helpful	O
for	O
entailment	O
graph	B-TaskName
learning	I-TaskName
(	O
Berant	O
et	O
al	O
,	O
2010	O
;	O
Lewis	O
and	O
Steedman	O
,	O
2013	O
;	O
Hosseini	O
et	O
al	O
,	O
2018	O
)	O
.	O
Inducing	O
a	O
type	O
for	O
each	O
entity	O
such	O
as	O
"	O
person	O
,	O
"	O
"	O
location	O
,	O
"	O
etc	O
.	O
enables	O
generalized	O
learning	O
across	O
instances	O
and	O
disambiguates	O
word	O
sense	O
,	O
e.g.	O
"	O
running	O
a	O
company	O
"	O
has	O
different	O
entailments	O
than	O
"	O
running	O
code	O
.	O
"	O
We	O
compare	O
our	O
model	O
to	O
several	O
baselines	O
,	O
including	O
strong	O
pretrained	B-TaskName
language	I-TaskName
models	I-TaskName
in	O
an	O
unsupervised	O
setting	O
using	O
similarity	O
.	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
generates	O
impressive	O
word	O
representations	O
,	O
even	O
unsupervised	O
(	O
Petroni	O
et	O
al	O
,	O
2019	O
)	O
,	O
which	O
we	O
compare	O
with	O
on	O
a	O
task	O
of	O
predicate	O
inference	O
.	O
We	O
further	O
test	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
to	O
show	O
the	O
impact	O
of	O
robust	O
in	O
-	O
domain	O
pretraining	O
on	O
the	O
same	O
architecture	O
.	O
These	O
non	O
-	O
directional	O
similarity	O
models	O
provide	O
a	O
strong	O
baseline	O
for	O
evaluating	O
directional	O
entailment	O
graphs	O
.	O

We	O
define	O
an	O
Entailment	O
Graph	O
as	O
a	O
directed	O
graph	O
of	O
predicates	O
and	O
their	O
entailments	O
,	O
G	O
=	O
(	O
V	O
,	O
E	O
)	O
.	O
The	O
vertices	O
V	O
are	O
the	O
set	O
of	O
predicates	O
,	O
where	O
each	O
argument	O
has	O
a	O
type	O
from	O
the	O
set	O
of	O
49	O
FIGER	B-DatasetName
base	O
types	O
T	O
,	O
e.g.	O
TRAVEL.TO	O
(	O
:	O
person	O
,	O
:	O
location	O
)	O
V	O
,	O
and	O
:	O
person	O
,	O
:	O
location	O
T	O
.	O
The	O
directed	O
edges	O
are	O
E	O
=	O
{	O
(	O
v	O
1	O
,	O
v	O
2	O
)	O
|	O
v	O
1	O
,	O
v	O
2	O
V	O
if	O
v	O
1	O
v	O
2	O
}	O
,	O
or	O
all	O
entailments	O
between	O
vertices	O
in	O
V	O
.	O
In	O
Multivalent	O
Entailment	O
Graphs	O
we	O
expand	O
V	O
to	O
contain	O
predicates	O
of	O
both	O
1	O
-	O
and	O
2	O
-	O
valency	O
,	O
and	O
E	O
to	O
edges	O
between	O
these	O
vertices	O
,	O
described	O
as	O
follows	O
.	O
Let	O
b	O
i	O
,	O
b	O
j	O
V	O
be	O
distinct	O
binary	O
predicates	O
and	O
u	O
i	O
,	O
u	O
j	O
V	O
be	O
unary	O
predicates	O
.	O
Define	O
E	O
as	O
the	O
set	O
of	O
all	O
entities	O
in	O
the	O
world	O
,	O
and	O
some	O
particular	O
entities	O
x	O
,	O
y	O
E	O
to	O
illustrate	O
argument	O
slots	O
.	O
E	O
contains	O
these	O
patterns	O
of	O
entailment	O
:	O
1	O
.	O
b	O
i	O
(	O
x	O
,	O
y	O
)	O
b	O
j	O
(	O
x	O
,	O
y	O
)	O
or	O
b	O
i	O
(	O
x	O
,	O
y	O
)	O
b	O
j	O
(	O
y	O
,	O
x	O
)	O
Binary	O
entails	O
binary	O
(	O
BB	O
entailments	O
)	O
2	O
.	O
b	O
i	O
(	O
x	O
,	O
y	O
)	O
u	O
i	O
(	O
x	O
)	O
or	O
b	O
i	O
(	O
x	O
,	O
y	O
)	O
u	O
i	O
(	O
y	O
)	O
Binary	O
entails	O
unary	O
of	O
one	O
argument	O
(	O
BU	O
)	O
3	O
.	O
u	O
i	O
(	O
x	O
)	O
u	O
j	O
(	O
x	O
)	O
Unary	O
entails	O
unary	O
(	O
UU	O
)	O
Predicates	O
with	O
valence	O
>	O
2	O
are	O
sparse	O
in	O
the	O
text	O
,	O
but	O
are	O
also	O
included	O
in	O
the	O
MGraph	O
by	O
decomposing	O
them	O
into	O
binary	O
relations	O
between	O
pairs	O
of	O
entities	O
.	O
This	O
is	O
another	O
application	O
of	O
our	O
Multivalent	O
DIH	O
.	O
We	O
maintain	O
argument	O
roles	O
,	O
so	O
each	O
binary	O
is	O
a	O
window	O
into	O
its	O
higher	O
-	O
valency	O
predicate	O
,	O
allowing	O
higher	O
-	O
valency	O
predicates	O
to	O
entail	O
lower	O
binaries	O
and	O
unaries	O
.	O
To	O
learn	O
these	O
new	O
kinds	O
of	O
connections	O
we	O
develop	O
a	O
method	O
of	O
local	O
entailment	O
rule	O
learning	O
using	O
the	O
MDIH	O
.	O
As	O
in	O
2	O
,	O
the	O
local	O
step	O
learns	O
the	O
initial	O
directed	O
edges	O
of	O
the	O
entailment	O
graph	O
,	O
which	O
are	O
further	O
improved	O
with	O
global	O
learning	O
.	O
Our	O
local	O
step	O
learns	O
entailments	O
by	O
machine	O
-	O
reading	O
the	O
NewsSpike	O
corpus	O
(	O
2.3	O
GB	O
)	O
,	O
which	O
contains	O
550	O
K	O
news	O
articles	O
,	O
or	O
over	O
20	O
M	O
sentences	O
(	O
Zhang	O
and	O
Weld	O
,	O
2013	O
)	O
.	O
NewsSpike	O
consists	O
of	O
multi	O
-	O
source	O
news	O
articles	O
collected	O
within	O
a	O
fixed	O
timeframe	O
,	O
and	O
due	O
to	O
these	O
properties	O
the	O
articles	O
frequently	O
discuss	O
the	O
same	O
events	O
but	O
phrased	O
in	O
different	O
ways	O
,	O
providing	O
appropriate	O
training	O
evidence	O
.	O

Our	O
pipeline	O
processes	O
raw	O
article	O
text	O
into	O
a	O
list	O
of	O
propositions	O
:	O
predicates	O
with	O
associated	O
typed	O
arguments	O
.	O
We	O
use	O
the	O
MoNTEE	O
system	O
(	O
Bijl	O
de	O
Vroe	O
et	O
al	O
,	O
2021	O
)	O
to	O
extract	O
natural	O
language	O
relations	O
between	O
entities	O
from	O
raw	O
text	O
2	O
.	O
This	O
system	O
first	O
parses	O
sentences	O
using	O
the	O
RotatingCCG	O
parser	O
(	O
Stanojević	O
and	O
Steedman	O
,	O
2019	O
)	O
(	O
Combinatory	O
Categorial	O
Grammar	O
;	O
Steedman	O
,	O
2000	O
)	O
and	O
then	O
forms	O
dependency	O
graphs	O
from	O
the	O
parses	O
.	O
Fi	O
-	O
nally	O
,	O
it	O
traverses	O
these	O
graphs	O
to	O
extract	O
the	O
relations	O
,	O
each	O
consisting	O
of	O
a	O
predicate	O
and	O
its	O
arguments	O
.	O
Figure	O
1	O
shows	O
an	O
example	O
dependency	O
graph	O
and	O
the	O
relation	O
extracted	O
from	O
it	O
.	O
Arguments	O
may	O
be	O
either	O
named	O
entities	O
3	O
or	O
general	O
entities	O
(	O
noun	O
phrases	O
)	O
.	O
These	O
entities	O
are	O
mapped	O
to	O
types	O
by	O
linking	O
to	O
their	O
Freebase	O
IDs	O
(	O
Bollacker	O
et	O
al	O
,	O
2008	O
)	O
using	O
AIDA	O
-	O
Light	O
(	O
Nguyen	O
et	O
al	O
,	O
2014	O
)	O
,	O
and	O
mapping	O
the	O
IDs	O
to	O
the	O
49	O
base	O
FIGER	B-DatasetName
types	O
(	O
Ling	O
and	O
Weld	O
,	O
2012	O
)	O
.	O
Both	O
binary	O
and	O
unary	O
relations	O
are	O
extracted	O
from	O
the	O
corpus	O
if	O
they	O
contain	O
at	O
least	O
one	O
named	O
entity	O
,	O
which	O
helps	O
anchor	O
to	O
a	O
real	O
-	O
world	O
event	O
.	O
This	O
poses	O
a	O
challenge	O
as	O
noted	O
by	O
Szpektor	O
and	O
Dagan	O
(	O
2008	O
)	O
.	O
While	O
binary	O
predicates	O
may	O
be	O
extracted	O
from	O
dependency	O
paths	O
between	O
two	O
entities	O
,	O
unary	O
predicates	O
only	O
have	O
one	O
endpoint	O
,	O
so	O
we	O
must	O
carefully	O
apply	O
linguistic	O
knowledge	O
to	O
extract	O
meaningful	O
unary	O
relations	O
.	O
We	O
extract	O
these	O
neo	O
-	O
Davidsonian	O
event	O
cases	O
:	O
One	O
-	O
argument	O
verbs	O
including	O
intransitives	O
,	O
e.g.	O
"	O
Knowles	O
sang	O
"	O
⇒	O
SING.1	O
(	O
Knowles	O
)	O
and	O
passivized	O
transitives	O
,	O
e.g.	O
"	O
Bill	O
H.R.	O
1	O
was	O
passed	O
"	O
⇒	O
PASS.2	O
(	O
Bill	O
-	O
HR1	O
)	O
Copular	O
constructions	O
,	O
where	O
copular	O
"	O
be	O
"	O
acts	O
as	O
the	O
main	O
verb	O
,	O
e.g.	O
"	O
Chiang	O
is	O
an	O
author	O
"	O
⇒	O
BE.AUTHOR.1	O
(	O
Chiang	O
)	O
and	O
where	O
it	O
does	O
not	O
,	O
e.g.	O
"	O
Phelps	O
seems	O
to	O
be	O
the	O
winner	O
"	O
⇒	O
SEEM.TO.BE.WINNER.1	O
(	O
Phelps	O
)	O
As	O
with	O
binaries	O
in	O
earlier	O
work	O
,	O
unary	O
predicates	O
are	O
lemmatized	O
,	O
and	O
tense	O
,	O
aspect	O
,	O
modality	O
,	O
and	O
other	O
auxiliaries	O
are	O
stripped	O
.	O
The	O
CCG	O
argument	O
position	O
which	O
corresponds	O
to	O
its	O
case	O
(	O
e.g.	O
1	O
for	O
nominative	O
,	O
2	O
for	O
accusative	O
)	O
,	O
is	O
appended	O
to	O
the	O
predicate	O
.	O
Passive	O
predicates	O
are	O
mapped	O
to	O
active	O
ones	O
.	O
Modifiers	O
such	O
as	O
negation	O
and	O
predicates	O
like	O
"	O
planned	O
to	O
"	O
as	O
in	O
"	O
Professor	O
Plum	O
planned	O
to	O
attend	O
"	O
are	O
also	O
extracted	O
in	O
the	O
predicate	O
.	O
We	O
pay	O
special	O
attention	O
to	O
copular	O
constructions	O
,	O
which	O
always	O
introduce	O
stative	O
predicates	O
,	O
rather	O
than	O
events	O
(	O
Vendler	O
,	O
1967	O
)	O
.	O
These	O
are	O
interesting	O
for	O
modeling	O
the	O
properties	O
of	O
entities	O
.	O

We	O
pose	O
an	O
automatically	O
generated	O
QA	O
task	O
to	O
evaluate	O
our	O
model	O
explicitly	O
for	O
directional	O
inference	O
between	O
binary	O
and	O
unary	O
predicates	O
,	O
as	O
we	O
are	O
not	O
aware	O
of	O
any	O
standard	O
datasets	O
for	O
this	O
problem	O
.	O
Our	O
task	O
is	O
to	O
answer	O
true	O
-	O
false	O
questions	O
about	O
real	O
events	O
that	O
are	O
discussed	O
in	O
the	O
news	O
,	O
for	O
example	O
,	O
"	O
Was	O
Biden	O
elected	O
?	O
"	O
These	O
types	O
of	O
questions	O
are	O
surprisingly	O
difficult	O
and	O
frequently	O
require	O
inference	O
to	O
answer	O
(	O
Clark	O
et	O
al	O
,	O
2019	O
)	O
.	O
For	O
this	O
,	O
entailment	O
is	O
especially	O
useful	O
:	O
we	O
must	O
decide	O
if	O
the	O
question	O
(	O
hypothesis	O
)	O
is	O
true	O
given	O
a	O
list	O
of	O
propositions	O
from	O
limited	O
news	O
text	O
(	O
premises	O
)	O
,	O
which	O
are	O
all	O
likely	O
to	O
be	O
phrased	O
differently	O
.	O
This	O
task	O
is	O
designed	O
independently	O
of	O
the	O
MGraph	O
as	O
a	O
challenge	O
in	O
information	B-TaskName
retrieval	I-TaskName
.	O
Positive	O
questions	O
made	O
from	O
binary	O
and	O
unary	O
predicates	O
are	O
selected	O
directly	O
from	O
the	O
news	O
text	O
using	O
special	O
criteria	O
,	O
and	O
are	O
then	O
removed	O
.	O
From	O
these	O
positives	O
we	O
automatically	O
generate	O
false	O
events	O
to	O
use	O
as	O
negatives	O
,	O
which	O
are	O
designed	O
to	O
mimic	O
real	O
,	O
newsworthy	O
events	O
.	O
The	O
remaining	O
news	O
text	O
is	O
used	O
to	O
answer	O
the	O
questions	O
.	O
We	O
at	O
-	O
tempt	O
to	O
make	O
every	O
question	O
answerable	O
,	O
but	O
since	O
they	O
are	O
generated	O
automatically	O
there	O
is	O
no	O
guarantee	O
.	O
However	O
,	O
the	O
task	O
is	O
fair	O
as	O
all	O
models	O
are	O
given	O
the	O
same	O
information	O
.	O
The	O
additive	O
effects	O
of	O
multivalent	O
entailment	O
should	O
be	O
demonstrated	O
:	O
with	O
more	O
kinds	O
of	O
entailment	O
,	O
the	O
MGraph	O
should	O
find	O
more	O
textual	O
support	O
and	O
answer	O
more	O
questions	O
.	O
The	O
task	O
is	O
presented	O
on	O
a	O
text	O
sample	O
from	O
NewsCrawl	O
,	O
a	O
multi	O
-	O
source	O
corpus	O
of	O
news	O
articles	O
,	O
to	O
be	O
published	O
separately	O
.	O
A	O
test	O
set	O
is	O
extracted	O
which	O
contains	O
700	O
K	O
sentences	O
from	O
articles	O
over	O
a	O
period	O
of	O
several	O
months	O
,	O
and	O
also	O
a	O
development	O
set	O
from	O
a	O
further	O
500	O
K	O
sentences	O
.	O
We	O
generate	O
questions	O
balanced	O
to	O
a	O
ratio	O
of	O
50	O
%	O
binary	O
questions	O
/	O
50	O
%	O
unary	O
;	O
and	O
within	O
each	O
50	O
%	O
positive	O
/	O
50	O
%	O
negative	O
.	O
Table	O
2	O
shows	O
a	O
sample	O
from	O
the	O
dev	O
set	O
.	O
We	O
generate	O
34	O
,	O
394	O
questions	O
on	O
the	O
test	O
set	O
:	O
17	O
,	O
256	O
unary	O
questions	O
and	O
17	O
,	O
138	O
binary	O
.	O

For	O
realism	O
,	O
questions	O
should	O
be	O
both	O
interesting	O
and	O
answerable	O
using	O
the	O
corpus	O
.	O
A	O
multi	O
-	O
step	O
process	O
extracts	O
questions	O
from	O
the	O
news	O
text	O
itself	O
.	O
1	O
.	O
Partitioning	O
.	O
First	O
,	O
the	O
articles	O
are	O
grouped	O
by	O
publication	O
date	O
such	O
that	O
each	O
partition	O
covers	O
a	O
timespan	O
of	O
up	O
to	O
3	O
consecutive	O
days	O
of	O
news	O
(	O
49	O
partitions	O
in	O
the	O
test	O
set	O
)	O
.	O
We	O
ask	O
yes	O
-	O
no	O
questions	O
about	O
events	O
drawn	O
from	O
the	O
partition	O
,	O
and	O
the	O
news	O
text	O
within	O
this	O
3	O
-	O
day	O
window	O
is	O
used	O
as	O
evidence	O
to	O
answer	O
them	O
.	O
We	O
ask	O
questions	O
as	O
if	O
happening	O
presently	O
in	O
this	O
time	O
window	O
to	O
control	O
for	O
the	O
variable	O
of	O
time	O
,	O
so	O
we	O
can	O
ask	O
ambiguous	O
questions	O
like	O
"	O
Did	O
the	O
Patriots	O
win	O
the	O
Superbowl	O
?	O
"	O
which	O
may	O
be	O
"	O
true	O
"	O
or	O
not	O
depending	O
on	O
the	O
date	O
and	O
timespan	O
.	O
The	O
small	O
3	O
-	O
day	O
window	O
size	O
was	O
chosen	O
so	O
multiple	O
news	O
stories	O
about	O
an	O
event	O
appear	O
together	O
,	O
increasing	O
the	O
chances	O
of	O
finding	O
question	O
answers	O
.	O
Within	O
each	O
partition	O
we	O
do	O
relation	B-TaskName
extraction	I-TaskName
in	O
a	O
process	O
mirroring	O
4.1	O
.	O
2	O
.	O
Selecting	O
Positives	O
.	O
We	O
adapt	O
a	O
selection	O
process	O
from	O
Poon	O
and	O
Domingos	O
(	O
2009	O
)	O
to	O
choose	O
good	O
questions	O
which	O
are	O
interesting	O
to	O
a	O
human	O
and	O
answerable	O
from	O
the	O
partition	O
text	O
.	O
First	O
,	O
we	O
identify	O
repeated	O
entities	O
that	O
star	O
in	O
the	O
events	O
of	O
the	O
articles	O
;	O
these	O
will	O
yield	O
interesting	O
questions	O
as	O
well	O
as	O
ample	O
textual	O
evidence	O
for	O
answering	O
them	O
.	O
In	O
each	O
partition	O
we	O
count	O
the	O
mentions	O
of	O
each	O
entity	O
pair	O
(	O
from	O
binary	O
propositions	O
)	O
and	O
single	O
entities	O
(	O
from	O
unary	O
and	O
binary	O
ones	O
)	O
.	O
The	O
most	O
frequent	O
entities	O
and	O
pairs	O
mentioned	O
more	O
than	O
5	O
times	O
in	O
the	O
partition	O
are	O
selected	O
.	O
Predicates	O
which	O
are	O
mentioned	O
across	O
the	O
entire	O
news	O
corpus	O
10	O
times	O
or	O
fewer	O
are	O
filtered	O
out	O
;	O
we	O
assume	O
those	O
remaining	O
are	O
popular	O
to	O
report	O
in	O
news	O
and	O
thus	O
are	O
interesting	O
to	O
a	O
human	O
questioner	O
.	O
We	O
randomly	O
select	O
propositions	O
featuring	O
both	O
a	O
star	O
entity	O
and	O
predicate	O
to	O
use	O
as	O
questions	O
,	O
and	O
remove	O
them	O
from	O
the	O
partition	O
.	O
3	O
.	O
Generating	O
Negatives	O
.	O
A	O
simple	O
strategy	O
for	O
producing	O
negatives	O
might	O
seem	O
to	O
be	O
substituting	O
random	O
predicates	O
into	O
the	O
positive	O
questions	O
.	O
However	O
,	O
this	O
is	O
unsatisfactory	O
because	O
modern	O
techniques	O
in	O
NLP	O
excel	O
at	O
detecting	O
unrelated	O
words	O
.	O
For	O
example	O
,	O
a	O
neural	O
model	O
will	O
easily	O
distinguish	O
a	O
random	O
negative	O
like	O
DETONATE	O
(	O
Google	B-DatasetName
,	O
YouTube	O
)	O
from	O
a	O
news	O
text	O
discussing	O
Google	B-DatasetName
's	O
acquisition	O
of	O
YouTube	O
,	O
classifying	O
it	O
as	O
a	O
false	O
event	O
on	O
grounds	O
of	O
dissimilarity	O
alone	O
.	O
To	O
be	O
a	O
meaningful	O
test	O
of	O
inference	O
this	O
task	O
requires	O
that	O
negatives	O
be	O
difficult	O
to	O
discriminate	O
from	O
positives	O
:	O
they	O
should	O
be	O
semantically	O
related	O
but	O
should	O
not	O
logically	O
follow	O
from	O
what	O
is	O
stated	O
in	O
the	O
text	O
.	O
To	O
this	O
end	O
we	O
derive	O
negative	O
questions	O
from	O
the	O
selected	O
positives	O
using	O
linguistic	O
relations	O
in	O
WordNet	O
(	O
Fellbaum	O
,	O
1998	O
)	O
.	O
We	O
assume	O
that	O
news	O
text	O
follows	O
the	O
Gricean	O
cooperative	O
principle	O
of	O
communication	O
(	O
Davis	O
,	O
2019	O
)	O
,	O
such	O
that	O
it	O
will	O
report	O
what	O
facts	O
are	O
known	O
and	O
nothing	O
more	O
.	O
To	O
this	O
end	O
,	O
noun	O
hyponyms	O
and	O
their	O
verbal	O
equivalent	O
,	O
troponyms	O
,	O
are	O
mined	O
from	O
the	O
first	O
sense	O
of	O
each	O
positive	O
in	O
WordNet	O
.	O
For	O
example	O
,	O
we	O
extract	O
"	O
burn	O
"	O
as	O
a	O
troponym	O
of	O
"	O
hurt	O
"	O
and	O
the	O
phrase	O
"	O
inherit	O
from	O
"	O
as	O
a	O
troponym	O
of	O
"	O
receive	O
from	O
.	O
"	O
We	O
therefore	O
expect	O
that	O
these	O
specific	O
relations	O
will	O
be	O
untrue	O
of	O
the	O
argument	O
tuple	O
in	O
question	O
and	O
may	O
be	O
used	O
as	O
negatives	O
.	O
We	O
also	O
considered	O
antonyms	O
and	O
other	O
WordNet	O
relations	O
,	O
but	O
these	O
are	O
much	O
sparser	O
in	O
English	O
and	O
have	O
low	O
coverage	O
.	O
For	O
fairness	O
,	O
generated	O
negatives	O
which	O
actually	O
occur	O
in	O
the	O
current	O
partition	O
are	O
screened	O
out	O
(	O
0.1	O
%	O
of	O
proposed	O
negatives	O
)	O
,	O
as	O
well	O
as	O
negatives	O
which	O
never	O
occur	O
in	O
the	O
entire	O
corpus	O
(	O
76.8	O
%	O
of	O
proposed	O
negatives	O
)	O
.	O
Only	O
challenging	O
negatives	O
are	O
left	O
,	O
which	O
actually	O
do	O
occur	O
in	O
real	O
news	O
text	O
.	O
See	O
Table	O
2	O
for	O
a	O
sample	O
of	O
questions	O
.	O
In	O
the	O
error	O
analysis	O
we	O
find	O
these	O
negatives	O
to	O
be	O
of	O
good	O
quality	O
:	O
they	O
are	O
uncommonly	O
inferable	O
from	O
the	O
text	O
,	O
accounting	O
for	O
a	O
small	O
percentage	O
of	O
false	O
positives	O
.	O

In	O
each	O
partition	O
,	O
models	O
receive	O
factual	O
propositions	O
extracted	O
from	O
3	O
days	O
of	O
news	O
text	O
to	O
use	O
as	O
evidence	O
for	O
answering	O
true	O
-	O
false	O
questions	O
.	O
A	O
model	O
scores	O
how	O
strongly	O
it	O
can	O
infer	O
the	O
question	O
proposition	O
from	O
each	O
evidence	O
proposition	O
,	O
and	O
we	O
take	O
the	O
maximum	O
score	O
as	O
the	O
model	O
confidence	O
of	O
a	O
"	O
true	O
"	O
answer	O
.	O
Exact	O
-	O
Match	O
.	O
Our	O
text	O
is	O
multi	O
-	O
source	O
news	O
articles	O
,	O
so	O
world	O
events	O
are	O
often	O
discussed	O
multiple	O
times	O
in	O
the	O
data	O
,	O
even	O
with	O
the	O
same	O
phrasing	O
.	O
We	O
compute	O
an	O
"	O
exact	O
-	O
match	O
"	O
baseline	O
which	O
shows	O
how	O
many	O
questions	O
can	O
be	O
answered	O
from	O
an	O
exact	O
string	O
match	O
in	O
the	O
text	O
;	O
the	O
rest	O
require	O
inference	O
.	O
Binary	O
Entailment	O
Graph	O
.	O
Our	O
BB	O
model	O
is	O
roughly	O
equivalent	O
to	O
the	O
state	O
of	O
the	O
art	O
binary	O
-	O
tobinary	O
entailment	O
graph	O
(	O
Hosseini	O
et	O
al	O
,	O
2018	O
)	O
,	O
so	O
it	O
serves	O
as	O
a	O
baseline	O
for	O
the	O
overall	O
model	O
.	O
5	O
All	O
graph	O
models	O
look	O
for	O
directed	O
entailments	O
from	O
evidence	O
propositions	O
to	O
the	O
question	O
proposition	O
.	O
For	O
example	O
,	O
"	O
Was	O
YouTube	O
sold	O
to	O
Google	B-DatasetName
?	O
"	O
can	O
be	O
answered	O
affirmatively	O
by	O
reading	O
"	O
Google	B-DatasetName
bought	O
YouTube	O
"	O
using	O
the	O
graph	O
edge	O
BUY	O
(	O
x	O
,	O
y	O
)	O
SELL.TO	O
(	O
y	O
,	O
x	O
)	O
.	O
BInc	O
scores	O
range	O
from	O
0	B-DatasetName
to	O
1	O
;	O
if	O
no	O
entailments	O
are	O
found	O
we	O
assume	O
it	O
is	O
false	O
(	O
score	O
of	O
0	B-DatasetName
)	O
.	O
Multivalent	O
Entailment	O
Graph	O
.	O
The	O
MGraph	O
is	O
made	O
of	O
3	O
component	O
models	O
:	O
(	O
1	O
)	O
the	O
BB	O
model	O
which	O
uses	O
binary	O
evidence	O
to	O
answer	O
binary	O
questions	O
;	O
(	O
2	O
)	O
the	O
UU	O
model	O
which	O
uses	O
unary	O
evidence	O
to	O
answer	O
unary	O
questions	O
;	O
and	O
(	O
3	O
)	O
the	O
BU	O
model	O
which	O
uses	O
binary	O
evidence	O
to	O
answer	O
unary	O
questions	O
.	O
The	O
MGraph	O
is	O
able	O
to	O
answer	O
questions	O
using	O
evidence	O
across	O
valencies	O
,	O
e.g.	O
"	O
Is	O
J.K.	O
Rowling	O
an	O
author	O
?	O
"	O
is	O
affirmed	O
by	O
reading	O
"	O
J.K.	O
Rowling	O
wrote	O
The	O
Sorcerer	O
's	O
Stone	O
"	O
using	O
the	O
graph	O
edge	O
WRITE	O
(	O
x	O
,	O
y	O
)	O
BE.AUTHOR	O
(	O
x	O
)	O
.	O
Individually	O
,	O
each	O
model	O
answers	O
only	O
binary	O
or	O
unary	O
qustions	O
,	O
not	O
both	O
.	O
By	O
combining	O
them	O
all	O
kinds	O
of	O
questions	O
can	O
be	O
answered	O
using	O
all	O
available	O
evidence	O
.	O
At	O
each	O
precision	O
level	O
if	O
any	O
component	O
model	O
predicts	O
true	O
,	O
the	O
overall	O
model	O
does	O
too	O
.	O
In	O
some	O
test	O
instances	O
the	O
entity	O
typer	O
may	O
make	O
an	O
error	O
,	O
and	O
so	O
we	O
fail	O
to	O
find	O
the	O
question	O
predicate	O
in	O
the	O
typed	O
subgraph	O
.	O
Similarly	O
to	O
Hosseini	O
et	O
al	O
(	O
2018	O
)	O
,	O
in	O
these	O
cases	O
we	O
back	O
off	O
,	O
querying	O
all	O
subgraphs	O
for	O
the	O
untyped	O
predicate	O
and	O
averaging	O
the	O
entailment	O
scores	O
found	O
.	O
We	O
find	O
5	O
%	O
more	O
unary	O
questions	O
and	O
18	O
%	O
more	O
binaries	O
.	O
Similarity	O
Models	O
.	O
BERT	B-MethodName
and	O
RoBERTa	B-MethodName
predicate	O
embeddings	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
;	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
are	O
used	O
in	O
an	O
unsupervised	O
manner	O
to	O
answer	O
questions	O
based	O
on	O
similarity	O
to	O
the	O
evidence	O
.	O
We	O
encode	O
the	O
question	O
into	O
a	O
representation	O
vector	O
,	O
and	O
each	O
evidence	O
proposition	O
with	O
the	O
same	O
arguments	O
.	O
We	O
compute	O
the	O
cosine	O
similarity	O
between	O
the	O
question	O
and	O
each	O
evidence	O
vector	O
,	O
adjusted	O
to	O
a	O
scale	O
of	O
0	B-DatasetName
to	O
1	O
:	O
sim	O
(	O
p	O
,	O
q	O
)	O
=	O
(	O
cos	O
(	O
p	O
,	O
q	O
)	O
+	O
1	O
)	O
/2	O
.	O
To	O
compute	O
each	O
vector	O
encoding	O
we	O
construct	O
a	O
simple	O
natural	O
language	O
sentence	O
from	O
the	O
proposition	O
using	O
its	O
predicate	O
and	O
arguments	O
and	O
encode	O
it	O
with	O
the	O
language	O
model	O
.	O
Our	O
representation	O
includes	O
only	O
the	O
encoding	O
for	O
the	O
predicate	O
in	O
the	O
context	O
of	O
its	O
arguments	O
,	O
but	O
not	O
the	O
arguments	O
themselves	O
to	O
make	O
this	O
a	O
true	O
test	O
of	O
predicate	O
similarity	O
.	O
We	O
average	O
all	O
final	O
hidden	O
-	O
state	O
vectors	O
from	O
the	O
model	O
corresponding	O
to	O
the	O
predicate	O
,	O
excluding	O
those	O
of	O
the	O
arguments	O
.	O
We	O
test	O
the	O
basic	O
BERT	B-MethodName
model	O
and	O
RoBERTa	B-MethodName
model	O
,	O
which	O
has	O
robustly	O
pretrained	O
on	O
160	O
GB	O
of	O
text	O
(	O
76	O
GB	O
news	O
)	O
.	O
PPDB	O
.	O
Though	O
supervised	O
,	O
PPDB	O
2.0	O
(	O
we	O
use	O
XXXL	O
)	O
(	O
Pavlick	O
et	O
al	O
,	O
2015	O
)	O
is	O
a	O
useful	O
comparison	O
as	O
it	O
is	O
a	O
large	O
,	O
well	O
-	O
understood	O
resource	O
for	O
phrasal	O
entailment	O
.	O
PPDB	O
relations	O
come	O
from	O
bilingual	O
pivoting	O
and	O
are	O
categorized	O
using	O
textbased	O
features	O
,	O
which	O
is	O
very	O
different	O
from	O
our	O
argument	O
-	O
tracking	O
method	O
.	O
We	O
view	O
PPDB	O
as	O
a	O
kind	O
of	O
Entailment	O
Graph	O
with	O
9	O
M	O
predicate	O
phrases	O
(	O
vertices	O
)	O
and	O
33	O
M	O
"	O
Equivalence	O
"	O
and	O
"	O
ForwardEntailment	O
"	O
edges	O
.	O
We	O
convert	O
evidence	O
and	O
question	O
propositions	O
into	O
a	O
natural	O
text	O
format	O
and	O
extract	O
a	O
PPDB	O
relation	O
score	O
from	O
each	O
evidence	O
phrase	O
to	O
the	O
question	O
.	O

We	O
sample	O
300	O
false	O
positives	O
(	O
100	O
for	O
each	O
model	O
)	O
and	O
report	O
analyses	O
in	O
Table	O
4	O
.	O
In	O
all	O
models	O
spurious	O
entailments	O
are	O
the	O
largest	O
issue	O
,	O
and	O
may	O
occur	O
due	O
to	O
normalization	O
of	O
predicates	O
during	O
learning	O
,	O
or	O
incidental	O
correlations	O
in	O
the	O
data	O
.	O
The	O
UU	O
and	O
BU	O
models	O
also	O
suffer	O
during	O
relation	B-TaskName
extraction	I-TaskName
(	O
parsing	O
)	O
.	O
When	O
we	O
fail	O
to	O
parse	O
a	O
second	O
argument	O
for	O
a	O
predicate	O
we	O
assume	O
it	O
only	O
has	O
one	O
and	O
extract	O
a	O
malformed	O
unary	O
,	O
which	O
can	O
interfere	O
with	O
question	B-TaskName
answering	I-TaskName
(	O
e.g.	O
reporting	O
verbs	O
"	O
explain	O
,	O
"	O
"	O
announce	O
,	O
"	O
etc	O
.	O
which	O
fail	O
to	O
parse	O
with	O
their	O
quote	O
)	O
.	O
We	O
also	O
find	O
relatively	O
few	O
poorly	O
generated	O
negatives	O
,	O
which	O
are	O
actually	O
true	O
given	O
the	O
text	O
.	O
In	O
these	O
cases	O
the	O
model	O
finds	O
an	O
entailment	O
which	O
the	O
authors	O
judge	O
to	O
be	O
correct	O
.	O

The	O
MDIH	O
is	O
shown	O
as	O
an	O
effective	O
theory	O
of	O
unsupervised	O
,	O
open	O
-	O
domain	O
predicate	O
entailment	O
,	O
which	O
crosses	O
valencies	O
by	O
respecting	O
argument	O
roles	O
.	O
Our	O
multivalent	O
entailment	O
graph	O
's	O
performance	O
has	O
been	O
demonstrated	O
on	O
a	O
question	B-TaskName
answering	I-TaskName
task	O
requiring	O
fine	O
-	O
grained	O
semantic	O
understanding	O
.	O
Our	O
method	O
is	O
able	O
to	O
answer	O
a	O
broader	O
variety	O
of	O
questions	O
than	O
earlier	O
entailment	O
graphs	O
,	O
aided	O
by	O
drawing	O
on	O
evidence	O
across	O
valencies	O
.	O
We	O
outperform	O
baseline	O
models	O
including	O
a	O
strong	O
similarity	O
measure	O
using	O
unsupervised	O
BERT	B-MethodName
and	O
RoBERTa	B-MethodName
,	O
while	O
using	O
far	O
less	O
training	O
data	O
.	O
This	O
shows	O
that	O
directional	O
entailment	O
is	O
more	O
helpful	O
for	O
inference	O
on	O
such	O
a	O
task	O
than	O
non	O
-	O
directional	O
similarity	O
,	O
even	O
with	O
robust	O
,	O
in	O
-	O
domain	O
pretraining	O
.	O
We	O
also	O
noted	O
a	O
complementarity	O
between	O
unsupervised	O
methods	O
.	O
Our	O
symbolic	O
graph	O
method	O
achieves	O
high	O
precision	O
for	O
learned	O
predicates	O
,	O
while	O
sub	O
-	O
symbolic	O
neural	O
models	O
achieve	O
high	O
recall	O
by	O
generalizing	O
to	O
unseen	O
predicates	O
.	O
Future	O
work	O
may	O
leverage	O
our	O
MDIH	O
signal	O
to	O
train	O
a	O
directional	O
neural	O
classifier	O
and	O
combine	O
benefits	O
.	O

This	O
work	O
was	O
supported	O
in	O
part	O
by	O
ERC	O
H2020	O
Advanced	O
Fellowship	O
GA	B-MethodName
742137	O
SEMANTAX	O
,	O

MLEC	O
-	O
QA	O
:	O
A	O
Chinese	O
Multi	O
-	O
Choice	O
Biomedical	O
Question	B-TaskName
Answering	I-TaskName
Dataset	O

Question	B-TaskName
Answering	I-TaskName
(	O
QA	O
)	O
has	O
been	O
successfully	O
applied	O
in	O
scenarios	O
of	O
human	O
-	O
computer	O
interaction	O
such	O
as	O
chatbots	O
and	O
search	O
engines	O
.	O
However	O
,	O
for	O
the	O
specific	O
biomedical	O
domain	O
,	O
QA	O
systems	O
are	O
still	O
immature	O
due	O
to	O
expert	O
-	O
annotated	O
datasets	O
being	O
limited	O
by	O
category	O
and	O
scale	O
.	O
In	O
this	O
paper	O
,	O
we	O
present	O
MLEC	O
-	O
QA	O
,	O
the	O
largest	O
-	O
scale	O
Chinese	O
multi	O
-	O
choice	O
biomedical	O
QA	O
dataset	O
,	O
collected	O
from	O
the	O
National	O
Medical	O
Licensing	O
Examination	O
in	O
China	O
.	O
The	O
dataset	O
is	O
composed	O
of	O
five	O
subsets	O
with	O
136	O
,	O
236	O
biomedical	O
multi	O
-	O
choice	O
questions	O
with	O
extra	O
materials	O
(	O
images	O
or	O
tables	O
)	O
annotated	O
by	O
human	O
experts	O
,	O
and	O
first	O
covers	O
the	O
following	O
biomedical	O
sub	O
-	O
fields	O
:	O
Clinic	O
,	O
Stomatology	O
,	O
Public	O
Health	O
,	O
Traditional	O
Chinese	O
Medicine	O
,	O
and	O
Traditional	O
Chinese	O
Medicine	O
Combined	O
with	O
Western	O
Medicine	O
.	O
We	O
implement	O
eight	O
representative	O
control	O
methods	O
and	O
open	O
-	O
domain	O
QA	O
methods	O
as	O
baselines	O
.	O
Experimental	O
results	O
demonstrate	O
that	O
even	O
the	O
current	O
best	O
model	O
can	O
only	O
achieve	O
accuracies	O
between	O
40	O
%	O
to	O
55	O
%	O
on	O
five	O
subsets	O
,	O
especially	O
performing	O
poorly	O
on	O
questions	O
that	O
require	O
sophisticated	O
reasoning	O
ability	O
.	O
We	O
hope	O
the	O
release	O
of	O
the	O
MLEC	O
-	O
QA	O
dataset	O
can	O
serve	O
as	O
a	O
valuable	O
resource	O
for	O
research	O
and	O
evaluation	O
in	O
open	O
-	O
domain	O
QA	O
,	O
and	O
also	O
make	O
advances	O
for	O
biomedical	O
QA	O
systems	O
.	O
1	O

As	O
a	O
branch	O
of	O
the	O
QA	O
task	O
,	O
Biomedical	O
Question	B-TaskName
Answering	I-TaskName
(	O
BQA	O
)	O
enables	O
effectively	O
perceiving	O
,	O
accessing	O
,	O
and	O
understanding	O
complex	O
biomedical	O
knowledge	O
by	O
innovative	O
applications	O
,	O
which	O
makes	O
BQA	O
an	O
important	O
QA	O
application	O
in	O
the	O
biomedical	O
domain	O
(	O
Jin	O
et	O
al	O
,	O
2021	O
)	O
.	O
Such	O
a	O
task	O
has	O
recently	O
attracted	O
considerable	O
attention	O
from	O
the	O
NLP	O
community	O
(	O
Zweigenbaum	O
,	O
2003	O
;	O
He	O
et	O
al	O
,	O
2020b	O
;	O
Jin	O
et	O
al	O
,	O
2020	O
)	O
,	O
but	O
is	O
still	O
confronted	O
with	O
the	O
following	O
three	O
key	O
challenges	O
:	O
1	O
https://github.com/Judenpech/MLEC	O
-	O
QA	O
(	O
1	O
)	O
Most	O
work	O
attempt	O
to	O
build	O
BQA	O
systems	O
with	O
deep	O
learning	O
and	O
neural	O
network	O
techniques	O
(	O
Ben	O
Abacha	O
et	O
al	O
,	O
2017	O
,	O
2019bPampari	O
et	O
al	O
,	O
2018	O
)	O
and	O
are	O
thus	O
data	O
-	O
hungry	O
.	O
However	O
,	O
annotating	O
large	O
-	O
scale	O
biomedical	O
question	O
-	O
answer	O
pairs	O
with	O
high	O
quality	O
is	O
prohibitively	O
expensive	O
.	O
As	O
a	O
result	O
,	O
current	O
expert	O
-	O
annotated	O
BQA	O
datasets	O
are	O
small	O
in	O
size	O
.	O
(	O
2	O
)	O
Multi	O
-	O
choice	O
QA	O
is	O
a	O
typical	O
format	O
type	O
of	O
BQA	O
dataset	O
.	O
Most	O
previous	O
work	O
focus	O
on	O
such	O
format	O
type	O
of	O
datasets	O
in	O
which	O
contents	O
are	O
in	O
the	O
field	O
of	O
clinical	O
medicine	O
(	O
Zhang	O
et	O
al	O
,	O
2018b	O
;	O
Jin	O
et	O
al	O
,	O
2020	O
)	O
and	O
consumer	O
health	O
(	O
Zhang	O
et	O
al	O
,	O
2017	O
(	O
Zhang	O
et	O
al	O
,	O
,	O
2018aHe	O
et	O
al	O
,	O
2019	O
;	O
Tian	O
et	O
al	O
,	O
2019	O
)	O
.	O
However	O
,	O
there	O
are	O
many	O
other	O
specialized	O
sub	O
-	O
fields	O
in	O
biomedicine	O
that	O
have	O
not	O
been	O
studied	O
before	O
(	O
e.g.	O
,	O
Stomatology	O
)	O
.	O
(	O
3	O
)	O
Ideal	O
BQA	O
systems	O
should	O
not	O
only	O
focus	O
on	O
raw	O
text	O
data	O
,	O
but	O
also	O
fully	O
utilize	O
various	O
types	O
of	O
biomedical	O
resources	O
,	O
such	O
as	O
images	O
and	O
tables	O
.	O
Unfortunately	O
,	O
most	O
BQA	O
datasets	O
are	O
either	O
texts	O
(	O
Tsatsaronis	O
et	O
al	O
,	O
2015	O
;	O
Pampari	O
et	O
al	O
,	O
2018	O
;	O
Jin	O
et	O
al	O
,	O
2019	O
)	O
or	O
images	O
(	O
Lau	O
et	O
al	O
,	O
2018	O
;	O
Ben	O
Abacha	O
et	O
al	O
,	O
2019a	O
;	O
He	O
et	O
al	O
,	O
2020a	O
)	O
;	O
as	O
a	O
result	O
,	O
BQA	O
datasets	O
that	O
are	O
composed	O
by	O
fusing	O
different	O
biomedical	O
resources	O
are	O
relatively	O
limited	O
.	O
To	O
push	O
forward	O
the	O
variety	O
of	O
BQA	O
datasets	O
,	O
we	O
present	O
MLEC	O
-	O
QA	O
,	O
the	O
largest	O
-	O
scale	O
Chinese	O
multi	O
-	O
choice	O
BQA	O
dataset	O
.	O
Questions	O
in	O
MLEC	O
-	O
QA	O
are	O
collected	O
from	O
the	O
National	O
Medical	O
Licensing	O
Examination	O
in	O
China	O
(	O
NMLEC	O
)	O
2	O
,	O
which	O
are	O
carefully	O
designed	O
by	O
human	O
experts	O
to	O
evaluate	O
professional	O
knowledge	O
and	O
skills	O
for	O
those	O
who	O
want	O
to	O
be	O
medical	O
practitioners	O
in	O
China	O
.	O
The	O
NMLEC	O
has	O
a	O
total	O
number	O
of	O
24	O
categories	O
of	O
exams	O
,	O
but	O
only	O
five	O
of	O
them	O
have	O
the	O
written	O
exams	O
in	O
Chinese	O
.	O
Every	O
year	O
,	O
only	O
around	O
18	O
-	O
22	O
%	O
of	O
applicants	O
can	O
pass	O
one	O
of	O
these	O
exams	O
,	O
showing	O
the	O
complexity	O
and	O
difficulty	O
of	O
passing	O
them	O
even	O
for	O
skilled	O
humans	O
.	O
There	O
are	O
three	O
main	O
properties	O
of	O
MLEC	O
-	O
QA	O
:	O
(	O
1	O
)	O
MLEC	O
-	O
QA	O
is	O
the	O
largest	O
-	O
scale	O
Chinese	O
multichoice	O
BQA	O
dataset	O
,	O
containing	O
136	O
,	O
236	O
questions	O
with	O
extra	O
materials	O
(	O
images	O
or	O
tables	O
)	O
,	O
Table	O
1	O
shows	O
an	O
example	O
.	O
(	O
2	O
)	O
MLEC	O
-	O
QA	O
first	O
covers	O
the	O
following	O
biomedical	O
sub	O
-	O
fields	O
:	O
Clinic	O
,	O
Stomatology	O
,	O
Public	O
Health	O
,	O
Traditional	O
Chinese	O
Medicine	O
,	O
and	O
Traditional	O
Chinese	O
Medicine	O
Combined	O
with	O
Western	O
Medicine	O
(	O
denoted	O
as	O
Chinese	O
Western	O
Medicine	O
)	O
.	O
Only	O
one	O
(	O
Clinic	O
)	O
of	O
them	O
has	O
been	O
studied	O
in	O
previous	O
research	O
.	O
(	O
3	O
)	O
MLEC	O
-	O
QA	O
provides	O
extra	O
labels	O
of	O
five	O
question	O
types	O
(	O
A1	O
,	O
A2	O
,	O
A3	O
/	O
A4	O
and	O
B1	O
)	O
for	O
each	O
question	O
,	O
and	O
an	O
in	O
-	O
depth	O
analysis	O
of	O
the	O
most	O
frequent	O
reasoning	O
types	O
of	O
the	O
questions	O
in	O
MLEC	O
-	O
QA	O
,	O
such	O
as	O
lexical	O
matching	O
,	O
multi	O
-	O
sentence	O
reading	O
and	O
concept	O
summary	O
,	O
etc	O
.	O
Detailed	O
analysis	O
can	O
be	O
found	O
in	O
Section	O
3.2	O
.	O
Examples	O
of	O
sub	O
-	O
fields	O
and	O
question	O
types	O
are	O
summarized	O
in	O
Table	O
2	O
.	O
We	O
set	O
each	O
example	O
of	O
five	O
question	O
types	O
corresponding	O
to	O
one	O
of	O
the	O
subfields	O
due	O
to	O
page	O
limits	O
.	O
As	O
an	O
attempt	O
to	O
solve	O
MLEC	O
-	O
QA	O
and	O
provide	O
strong	O
baselines	O
,	O
we	O
implement	O
eight	O
representative	O
control	O
methods	O
and	O
open	O
-	O
domain	O
QA	O
methods	O
by	O
a	O
two	O
-	O
stage	O
retriever	O
-	O
reader	O
framework	O
:	O
(	O
1	O
)	O
A	O
retriever	O
finding	O
documents	O
that	O
(	O
might	O
)	O
contain	O
an	O
answer	O
from	O
a	O
large	O
collection	O
of	O
documents	O
.	O
We	O
adopt	O
Chinese	O
Wikipedia	O
dumps	O
3	O
as	O
our	O
information	O
sources	O
,	O
and	O
use	O
a	O
distributed	O
search	O
and	O
analytics	O
engine	O
,	O
ElasticSearch	O
4	O
,	O
as	O
the	O
document	O
store	O
and	O
document	O
retriever	O
.	O
(	O
2	O
)	O
A	O
reader	O
finding	O
the	O
answer	O
in	O
given	O
documents	O
retrieved	O
by	O
the	O
retriever	O
.	O
We	O
fine	O
-	O
tune	O
five	O
pre	O
-	O
trained	O
language	O
models	O
for	O
machine	B-TaskName
reading	I-TaskName
comprehension	I-TaskName
as	O
the	O
reader	O
.	O
Experimental	O
results	O
show	O
that	O
even	O
the	O
current	O
best	O
model	O
can	O
only	O
achieve	O
accuracies	O
of	O
53	O
%	O
,	O
44	O
%	O
,	O
40	O
%	O
,	O
55	O
%	O
,	O
and	O
50	O
%	O
on	O
the	O
five	O
categories	O
of	O
subsets	O
:	O
Clinic	O
,	O
Stomatology	O
,	O
Public	O
Health	O
,	O
Traditional	O
Chinese	O
Medicine	O
,	O
and	O
Chinese	O
Western	O
Medicine	O
,	O
respectively	O
.	O
The	O
models	O
especially	O
perform	O
poorly	O
on	O
questions	O
that	O
require	O
understanding	O
comprehensive	O
biomedical	O
concepts	O
and	O
handling	O
complex	O
reasoning	O
.	O
In	O
summary	O
,	O
the	O
major	O
contributions	O
of	O
this	O
paper	O
are	O
threefold	O
:	O
We	O
present	O
MLEC	O
-	O
QA	O
,	O
the	O
largest	O
-	O
scale	O
Chinese	O
multi	O
-	O
choice	O
BQA	O
dataset	O
with	O
extra	O
materials	O
,	O
and	O
it	O
first	O
covers	O
five	O
biomedical	O
sub	O
-	O
fields	O
,	O
only	O
one	O
of	O
which	O
has	O
been	O
studied	O
in	O
previous	O
research	O
.	O
We	O
conduct	O
an	O
in	O
-	O
depth	O
analysis	O
on	O
MLEC	O
-	O
QA	O
,	O
revealing	O
that	O
both	O
comprehensive	O
biomedical	O
knowledge	O
and	O
sophisticated	O
reasoning	O
ability	O
are	O
required	O
to	O
answer	O
questions	O
.	O
We	O
implement	O
eight	O
representative	O
methods	O
as	O
baselines	O
and	O
show	O
the	O
performance	O
of	O
existing	O
methods	O
on	O
MLEC	O
-	O
QA	O
,	O
and	O
provide	O
an	O
outlook	O
for	O
future	O
research	O
directions	O
.	O

Open	O
-	O
Domain	O
BQA	O
The	O
Text	O
REtrieval	O
Conference	O
(	O
TREC	B-DatasetName
)	O
(	O
Voorhees	O
and	O
Tice	O
,	O
2000	O
)	O
has	O
triggered	O
the	O
open	O
-	O
domain	O
BQA	O
research	O
.	O
At	O
the	O
time	O
,	O
most	O
traditional	O
BQA	O
systems	O
were	O
employing	O
complex	O
pipelines	O
with	O
question	O
processing	O
,	O
document	O
/	O
passage	B-TaskName
retrieval	I-TaskName
,	O
and	O
answer	O
processing	O
modules	O
.	O
Examples	O
of	O
such	O
systems	O
include	O
EPoCare	O
(	O
Niu	O
et	O
al	O
,	O
2003	O
)	O
,	O
MedQA	O
(	O
Yu	O
et	O
al	O
,	O
2007	O
;	O
Terol	O
et	O
al	O
,	O
2007	O
;	O
Wang	O
et	O
al	O
,	O
2007	O
)	O
and	O
AskHERMES	O
(	O
Cao	O
et	O
al	O
,	O
2011	O
)	O
.	O
With	O
the	O
introduction	O
of	O
various	O
BQA	O
datasets	O
that	O
are	O
focused	O
on	O
specific	O
biomedical	O
topics	O
,	O
such	O
as	O
BioASQ	B-DatasetName
(	O
Tsatsaronis	O
et	O
al	O
,	O
2015	O
)	O
,	O
emrQA	B-DatasetName
(	O
Pampari	O
et	O
al	O
,	O
2018	O
)	O
and	O
PubMedQA	B-DatasetName
(	O
Jin	O
et	O
al	O
,	O
2019	O
)	O
,	O
pioneered	O
by	O
Chen	O
et	O
al	O
(	O
2017	O
)	O
,	O
the	O
modern	O
open	O
-	O
domain	O
BQA	O
systems	O
largely	O
simplified	O
the	O
traditional	O
BQA	O
pipeline	O
to	O
a	O
two	O
-	O
stage	O
retriever	O
-	O
reader	O
framework	O
by	O
combining	O
information	B-TaskName
retrieval	I-TaskName
and	O
machine	B-TaskName
reading	I-TaskName
comprehension	I-TaskName
models	O
(	O
Ben	O
Abacha	O
et	O
al	O
,	O
2017	O
,	O
2019b	O
.	O
Moreover	O
,	O
the	O
extensive	O
use	O
of	O
medical	O
images	O
(	O
e.g.	O
,	O
CT	O
)	O
and	O
tables	O
(	O
e.g.	O
,	O
laboratory	O
examination	O
)	O
has	O
improved	O
results	O
in	O
real	O
-	O
world	O
clinical	O
scenarios	O
,	O
making	O
the	O
BQA	O
a	O
task	O
lying	O
at	O
the	O
intersection	O
of	O
Computer	O
Vision	O
(	O
CV	O
)	O
and	O
NLP	O
.	O
However	O
,	O
most	O
BQA	O
models	O
focus	O
on	O
either	O
texts	O
or	O
images	O
(	O
Lau	O
et	O
al	O
,	O
2018	O
;	O
Ben	O
Abacha	O
et	O
al	O
,	O
2019a	O
;	O
He	O
et	O
al	O
,	O
2020a	O
)	O
;	O
as	O
a	O
result	O
,	O
BQA	O
datasets	O
that	O
are	O
composed	O
by	O
fusing	O
different	O
biomedical	O
resources	O
are	O
relatively	O
limited	O
.	O
Open	O
-	O
Domain	O
Multi	O
-	O
Choice	O
BQA	O
Datasets	O
With	O
rapidly	O
increasing	O
numbers	O
of	O
consumers	O
asking	O
health	O
-	O
related	O
questions	O
on	O
online	O
medical	O
consultation	O
websites	O
,	O
cMedQA	O
(	O
Zhang	O
et	O
al	O
,	O
2017	O
(	O
Zhang	O
et	O
al	O
,	O
,	O
2018a	O
,	O
webMedQA	O
(	O
He	O
et	O
al	O
,	O
2019	O
)	O
and	O
ChiMed	O
(	O
Tian	O
et	O
al	O
,	O
2019	O
)	O
exploit	O
patient	O
-	O
doctor	O
QA	O
data	O
to	O
build	O
consumer	O
health	O
QA	O
datasets	O
.	O
However	O
,	O
the	O
quality	O
problems	O
in	O
such	O
datasets	O
are	O
that	O
the	O
answers	O
are	O
written	O
by	O
online	O
-	O
doctors	O
and	O
the	O
data	O
itself	O
has	O
intrinsic	O
noise	O
.	O
By	O
contrast	O
,	O
medical	O
licensing	O
examinations	O
,	O
which	O
are	O
designed	O
by	O
human	O
medical	O
experts	O
,	O
often	O
take	O
the	O
form	O
of	O
multi	O
-	O
choice	O
questions	O
,	O
and	O
contain	O
a	O
significant	O
number	O
of	O
questions	O
that	O
require	O
comprehensive	O
biomedical	O
knowledge	O
and	O
multiple	O
reasoning	O
ability	O
.	O
Such	O
exams	O
are	O
the	O
perfect	O
data	O
source	O
to	O
push	O
the	O
development	O
of	O
BQA	O
systems	O
.	O
Several	O
datasets	O
have	O
been	O
released	O
that	O
exploit	O
such	O
naturally	O
existing	O
BQA	O
data	O
,	O
which	O
are	O
summarized	O
in	O
Table	O
3	O
.	O
Collecting	O
from	O
the	O
Spain	O
public	O
healthcare	O
specialization	O
examination	O
,	O
HEAD	O
-	O
QA	O
(	O
Vilares	O
and	O
Gómez	O
-	O
Rodríguez	O
,	O
2019	O
)	O
contains	O
multichoice	O
questions	O
from	O
six	O
biomedical	O
categories	O
,	O
including	O
Medicine	O
,	O
Pharmacology	O
,	O
Psychology	O
,	O
Nursing	O
,	O
Biology	O
and	O
Chemistry	O
.	O
NLPEC	O
(	O
Li	O
et	O
al	O
,	O
2020	O
)	O
collects	O
21.7k	O
multi	O
-	O
choice	O
questions	O
with	O
human	O
-	O
annotated	O
answers	O
from	O
the	O
National	O
Licensed	O
Pharmacist	O
Examination	O
in	O
China	O
,	O
but	O
only	O
a	O
small	O
number	O
of	O
sample	O
data	O
is	O
available	O
for	O
public	O
use	O
.	O
Last	O
but	O
not	O
least	O
,	O
clinical	O
medicine	O
,	O
as	O
one	O
of	O
the	O
24	O
categories	O
in	O
NMLEC	O
,	O
has	O
been	O
previously	O
studied	O
by	O
MedQA	O
(	O
Zhang	O
et	O
al	O
,	O
2018b	O
)	O
and	O
MEDQA	O
(	O
Jin	O
et	O
al	O
,	O
2020	O
)	O
.	O
However	O
,	O
the	O
former	O
did	O
not	O
release	O
any	O
data	O
or	O
code	O
,	O
and	O
the	O
latter	O
only	O
focused	O
on	O
clinical	O
medicine	O
with	O
34k	O
questions	O
in	O
their	O
cross	O
-	O
lingual	O
studies	O
,	O
questions	O
with	O
images	O
or	O
tables	O
were	O
not	O
included	O
,	O
and	O
none	O
of	O
the	O
remaining	O
categories	O
in	O
MLEC	O
-	O
QA	O
were	O
studied	O
.	O
Basically	O
,	O
as	O
shown	O
in	O
Table	O
2	O
and	O
Table	O
4	O
,	O
the	O
questions	O
in	O
MLEC	O
-	O
QA	O
are	O
divided	O
into	O
five	O
types	O
including	O
:	O
A1	O
:	O
single	O
statement	O
question	O
;	O
B1	O
:	O
similar	O
to	O
A1	O
,	O
with	O
a	O
group	O
of	O
options	O
shared	O
in	O
multiple	O
questions	O
;	O
A2	O
:	O
questions	O
accompanied	O
by	O
a	O
clinical	O
scenario	O
;	O
A3	O
:	O
similar	O
to	O
A2	O
,	O
with	O
information	O
shared	O
among	O
multiple	O
independent	O
questions	O
;	O
A4	O
:	O
similar	O
to	O
A3	O
,	O
with	O
information	O
shared	O
among	O
multiple	O
questions	O
,	O
new	O
information	O
can	O
be	O
gradually	O
added	O
.	O
We	O
further	O
classify	O
these	O
questions	O
into	O
Knowledge	O
Questions	O
(	O
KQ	O
)	O
and	O
Case	O
Questions	O
(	O
CQ	O
)	O
,	O
where	O
KQ	O
(	O
A1+B1	O
)	O
focus	O
on	O
the	O
definition	O
and	O
comprehension	O
of	O
biomedical	O
knowledge	O
,	O
while	O
CQ	O
(	O
A2+A3	O
/	O
A4	O
)	O
require	O
analysis	O
and	O
practical	O
application	O
for	O
real	O
-	O
world	O
medical	O
scenarios	O
.	O
Both	O
types	O
of	O
questions	O
require	O
multiple	O
reasoning	O
ability	O
to	O
answer	O
.	O
For	O
the	O
Train	O
/	O
Dev	O
/	O
Test	O
split	O
,	O
randomly	O
splitting	O
may	O
cause	O
data	O
imbalance	O
because	O
the	O
number	O
of	O
the	O
five	O
question	O
types	O
are	O
various	O
from	O
each	O
other	O
(	O
e.g.	O
,	O
A1	O
is	O
far	O
more	O
than	O
others	O
)	O
.	O
To	O
ensure	O
that	O
the	O
subsets	O
have	O
the	O
same	O
distribution	O
of	O
the	O
question	O
types	O
,	O
we	O
split	O
the	O
data	O
based	O
on	O
the	O
question	O
types	O
,	O
with	O
80	O
%	O
training	O
,	O
10	O
%	O
development	O
,	O
and	O
10	O
%	O
test	O
.	O
The	O
overall	O
statistics	O
of	O
the	O
MLEC	O
-	O
QA	O
dataset	O
are	O
summarized	O
in	O
Table	O
5	O
.	O
We	O
can	O
see	O
that	O
the	O
length	O
of	O
the	O
questions	O
and	O
the	O
vocabulary	O
size	O
in	O
Clinic	O
are	O
larger	O
than	O
the	O
rest	O
of	O
the	O
subsets	O
,	O
explaining	O
that	O
clinical	O
medicine	O
may	O
involve	O
more	O
medical	O
subjects	O
than	O
other	O
specialties	O
.	O

Both	O
examination	O
counseling	O
books	O
and	O
Wikipedia	O
have	O
been	O
used	O
as	O
the	O
source	O
of	O
supporting	O
materials	O
in	O
previous	O
research	O
(	O
Zhong	O
et	O
al	O
,	O
2020	O
;	O
Jin	O
et	O
al	O
,	O
2020	O
;	O
Vilares	O
and	O
Gómez	O
-	O
Rodríguez	O
,	O
2019	O
)	O
.	O
However	O
,	O
because	O
examination	O
counseling	O
books	O
are	O
designed	O
to	O
help	O
examinees	O
pass	O
the	O
examination	O
,	O
knowledge	O
is	O
highly	O
simplified	O
and	O
summarized	O
;	O
even	O
the	O
easily	O
confused	O
knowledge	O
points	O
are	O
compared	O
.	O
Using	O
examination	O
counseling	O
books	O
as	O
information	O
sources	O
may	O
make	O
the	O
retriever	O
-	O
reader	O
more	O
likely	O
to	O
exploit	O
shallow	O
text	B-TaskName
matching	I-TaskName
,	O
and	O
complex	O
reasoning	O
is	O
seldom	O
involved	O
.	O
Therefore	O
,	O
to	O
help	O
better	O
understand	O
the	O
improvement	O
coming	O
from	O
future	O
models	O
,	O
we	O
adopt	O
Chinese	O
Wikipedia	O
dumps	O
as	O
our	O
information	O
sources	O
,	O
which	O
contain	O
a	O
wealth	O
of	O
information	O
(	O
over	O
1	O
million	O
articles	O
)	O
of	O
real	O
-	O
world	O
facts	O
.	O
Building	O
upon	O
the	O
whole	O
Chinese	O
Wikipedia	O
data	O
,	O
we	O
use	O
a	O
distributed	O
search	O
and	O
analytics	O
engine	O
,	O
Elas	B-DatasetName
-	O
ticSearch	O
,	O
as	O
the	O
document	O
store	O
and	O
document	O
retriever	O
,	O
which	O
supports	O
very	O
fast	O
full	O
-	O
text	O
searches	O
.	O
The	O
similarity	O
scoring	O
function	O
used	O
in	O
Elasticsearch	O
is	O
the	O
BM25	O
algorithm	O
(	O
Robertson	O
and	O
Zaragoza	O
,	O
2009	O
)	O
,	O
which	O
measures	O
the	O
relevance	O
of	O
documents	O
to	O
a	O
given	O
search	O
query	O
.	O
As	O
defined	O
in	O
Appendix	O
C	O
,	O
the	O
larger	O
this	O
BM25	O
score	O
,	O
the	O
stronger	O
the	O
relevance	O
between	O
document	O
and	O
query	O
.	O
Specifically	O
,	O
for	O
each	O
question	O
Q	O
i	O
and	O
each	O
candidate	O
option	O
O	O
ij	O
where	O
j	O
{	O
A	O
,	O
B	O
,	O
C	O
,	O
D	O
,	O
E	O
}	O
,	O
we	O
define	O
Q	O
i	O
O	O
ij	O
=	O
Q	O
i	O
+	O
O	O
ij	O
as	O
a	O
search	O
query	O
to	O
Elasticsearch	O
and	O
is	O
repeated	O
for	O
all	O
options	O
.	O
The	O
document	O
with	O
the	O
highest	O
BM25	O
score	O
returned	O
by	O
each	O
query	O
is	O
selected	O
as	O
supporting	O
materials	O
for	O
the	O
next	O
stage	O
machine	B-TaskName
reading	I-TaskName
comprehension	I-TaskName
task	O
.	O

We	O
apply	O
an	O
unified	O
framework	O
UER	O
-	O
py	O
(	O
Zhao	O
et	O
al	O
,	O
2019	O
)	O
to	O
fine	O
-	O
tuning	O
pre	O
-	O
trained	O
language	O
models	O
on	O
the	O
machine	B-TaskName
reading	I-TaskName
comprehension	I-TaskName
task	O
as	O
our	O
reader	O
.	O
We	O
consider	O
the	O
following	O
five	O
pre	O
-	O
trained	O
language	O
models	O
:	O
Chinese	O
BERT	B-MethodName
-	O
Base	O
(	O
denoted	O
as	O
BERT	B-MethodName
-	O
Base	O
)	O
and	O
Multilingual	O
Uncased	O
BERT	B-MethodName
-	O
Base	O
(	O
denoted	O
as	O
BERT	B-MethodName
-	O
Base	O
-	O
Multilingual	O
)	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
Chinese	O
BERT	B-MethodName
-	O
Base	O
with	O
whole	O
word	O
masking	O
and	O
pre	O
-	O
trained	O
over	O
larger	O
corpora	O
(	O
denoted	O
as	O
BERT	B-MethodName
-	O
wwm	O
-	O
ext	O
)	O
(	O
Cui	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
the	O
robustly	O
optimized	O
BERTs	O
:	O
Chinese	O
RoBERTa	B-MethodName
-	O
wwm	O
-	O
ext	O
and	O
Chinese	O
RoBERTa	B-MethodName
-	O
wwm	O
-	O
ext	O
-	O
large	O
(	O
Cui	O
et	O
al	O
,	O
2019	O
)	O
.	O
Specifically	O
,	O
given	O
the	O
i	O
th	O
question	O
Q	O
i	O
,	O
retrieved	O
question	O
relevant	O
documents	O
D	O
i	O
,	O
and	O
a	O
candidate	O
option	O
O	O
ij	O
,	O
where	O
j	O
{	O
A	O
,	O
B	O
,	O
C	O
,	O
D	O
,	O
E	O
}	O
.	O
The	O
input	O
sequence	O
for	O
the	O
framework	O
is	O
constructed	O
by	O
concatenating	O
[	O
CLS	O
]	O
,	O
tokens	O
in	O
D	O
i	O
,	O
[	O
SEP	O
]	O
,	O
tokens	O
in	O
Q	O
i	O
,	O
[	O
SEP	O
]	O
,	O
tokens	O
in	O
an	O
option	O
O	O
ij	O
,	O
and	O
[	O
SEP	O
]	O
,	O
where	O
[	O
CLS	O
]	O
is	O
the	O
classifier	O
token	O
,	O
and	O
[	O
SEP	O
]	O
is	O
the	O
sentence	O
separator	O
in	O
pre	O
-	O
trained	O
language	O
models	O
.	O
We	O
pass	O
each	O
of	O
the	O
five	O
options	O
in	O
turn	O
,	O
and	O
the	O
model	O
outputs	O
the	O
hidden	O
state	O
representation	O
S	O
ij	O
R	O
1×H	O
of	O
the	O
input	O
sequence	O
,	O
then	O
performs	O
the	O
classification	O
and	O
output	O
an	O
unnormalized	O
log	O
probability	O
P	O
ij	O
R	O
of	O
each	O
option	O
O	O
ij	O
being	O
correct	O
by	O
P	O
ij	O
=	O
S	O
ij	O
W	O
T	O
,	O
where	O
W	O
R	O
1×H	O
is	O
the	O
weight	O
matrix	O
.	O
Finally	O
,	O
we	O
pass	O
the	O
unnormalized	O
log	O
probabilities	O
of	O
each	O
option	O
through	O
a	O
softmax	B-MethodName
layer	O
and	O
obtain	O
the	O
option	O
with	O
the	O
highest	O
probability	O
as	O
the	O
predicted	O
answer	O
A	O
′	O
i	O
.	O

Tables	O
8	O
and	O
Figure	O
3	O
show	O
the	O
performance	O
of	O
baselines	O
as	O
well	O
as	O
the	O
performance	O
on	O
KQ	O
and	O
CQ	O
questions	O
.	O
As	O
we	O
can	O
see	O
,	O
among	O
control	O
methods	O
,	O
the	O
correct	O
option	O
has	O
a	O
slight	O
tendency	O
to	O
appear	O
in	O
the	O
middle	O
(	O
C	O
and	O
D	O
)	O
of	O
candidate	O
options	O
,	O
but	O
the	O
margins	O
are	O
small	O
.	O
The	O
performance	O
of	O
the	O
Mixed	O
method	O
is	O
slightly	O
better	O
than	O
a	O
random	O
guess	O
,	O
which	O
indicates	O
that	O
the	O
flexible	O
use	O
of	O
guessing	O
skills	O
may	O
add	O
wings	O
to	O
the	O
tiger	O
as	O
humans	O
can	O
exclude	O
some	O
certain	O
wrong	O
options	O
,	O
but	O
if	O
the	O
cart	O
before	O
the	O
horse	O
is	O
reversed	O
,	O
it	O
is	O
impossible	O
to	O
pass	O
the	O
exam	O
only	O
through	O
opportunistic	O
guessing	O
.	O
RoBERTa	B-MethodName
-	O
wwm	O
-	O
ext	O
-	O
large	O
and	O
BERTwwm	O
-	O
ext	O
perform	O
better	O
than	O
other	O
models	O
on	O
five	O
subsets	O
.	O
However	O
,	O
even	O
the	O
best	O
-	O
performing	O
model	O
can	O
only	O
achieve	O
accuracies	O
between	O
40	O
%	O
to	O
55	O
%	O
on	O
five	O
subsets	O
,	O
so	O
there	O
is	O
still	O
a	O
gap	O
to	O
pass	O
the	O
exams	O
.	O
Comparing	O
the	O
performance	O
between	O
KQ	O
and	O
CQ	O
questions	O
,	O
most	O
models	O
achieve	O
better	O
performance	O
on	O
CQ	O
,	O
which	O
is	O
positively	O
correlated	O
with	O
CQ	O
's	O
better	O
retrieval	O
performance	O
.	O
Among	O
different	O
subsets	O
,	O
the	O
subset	O
TCM	O
is	O
the	O
easiest	O
(	O
54.95	O
%	O
)	O
one	O
to	O
answer	O
across	O
the	O
board	O
,	O
while	O
the	O
subset	O
PH	O
is	O
the	O
hardest	O
(	O
40.04	O
%	O
)	O
,	O
which	O
does	O
not	O
totally	O
correspond	O
to	O
their	O
retrieval	O
performance	O
as	O
shown	O
in	O
Table	O
7	O
.	O
The	O
possible	O
reason	O
is	O
that	O
the	O
diagnosis	O
and	O
treatment	O
of	O
diseases	O
in	O
traditional	O
Chinese	O
medicine	O
are	O
characterized	O
by	O
"	O
Homotherapy	O
for	O
Heteropathy	O
"	O
,	O
that	O
is	O
,	O
treating	O
different	O
diseases	O
with	O
the	O
same	O
method	O
,	O
which	O
may	O
result	O
in	O
some	O
patterns	O
or	O
mechanisms	O
that	O
can	O
be	O
used	O
by	O
the	O
models	O
to	O
reach	O
such	O
results	O
.	O

Since	O
the	O
data	O
is	O
designed	O
by	O
a	O
team	O
of	O
anonymous	O
human	O
healthcare	O
experts	O
,	O
we	O
are	O
not	O
able	O
to	O
directly	O
reach	O
them	O
for	O
inclusion	O
in	O
this	O
dataset	O
and	O
thus	O
could	O
not	O
be	O
asked	O
for	O
demographic	O
information	O
.	O
It	O
is	O
expected	O
that	O
most	O
of	O
the	O
speakers	O
come	O
from	O
China	O
with	O
professionals	O
working	O
in	O
the	O
area	O
of	O
biomedicine	O
,	O
and	O
speak	O
Chinese	O
as	O
a	O
native	O
language	O
.	O
No	O
direct	O
information	O
is	O
available	O
about	O
age	B-DatasetName
and	I-DatasetName
gender	I-DatasetName
distribution	O
.	O

For	O
five	O
subsets	O
in	O
MLEC	O
-	O
QA	O
,	O
we	O
collect	O
2006	O
to	O
2020	O
Sprint	O
Paper	O
for	O
the	O
National	O
Medical	O
Licensing	O
Examination	O
-	O
Tianjin	O
Science	O
and	O
Technology	O
Press	O
in	O
PDF	O
format	O
,	O
and	O
then	O
converted	O
them	O
into	O
digital	O
format	O
via	O
Optical	B-TaskName
Character	I-TaskName
Recognition	I-TaskName
(	O
OCR	O
)	O
.	O
We	O
manually	O
checked	O
and	O
corrected	O
the	O
OCR	O
results	O
with	O
confidence	O
less	O
than	O
0.99	O
to	O
ensure	O
the	O
quality	O
of	O
our	O
dataset	O
.	O
We	O
also	O
scraped	O
practice	O
exercises	O
from	O
offcn	O
(	O
http://www.offcn.com/yixue/yszg/	O
)	O
,	O
which	O
are	O
freely	O
accessible	O
online	O
for	O
public	O
usage	O
.	O

NLP	O
systems	O
for	O
machine	B-TaskName
translation	I-TaskName
,	O
summarization	B-TaskName
,	O
paraphrasing	O
,	O
and	O
other	O
tasks	O
often	O
fail	O
to	O
preserve	O
the	O
compositional	O
semantics	O
of	O
sentences	O
and	O
documents	O
because	O
they	O
model	O
language	O
as	O
bags	O
of	O
words	O
,	O
or	O
at	O
best	O
syntactic	O
trees	O
.	O
To	O
preserve	O
semantics	O
,	O
they	O
must	O
model	O
semantics	O
.	O
In	O
pursuit	O
of	O
this	O
goal	O
,	O
several	O
datasets	O
have	O
been	O
produced	O
which	O
pair	O
natural	O
language	O
with	O
compositional	O
semantic	O
representations	O
in	O
the	O
form	O
of	O
directed	O
acyclic	O
graphs	O
(	O
DAGs	O
)	O
,	O
including	O
the	O
Abstract	O
Meaning	O
Representation	O
Bank	O
(	O
AMR	O
;	O
Banarescu	O
et	O
al	O
2013	O
)	O
,	O
the	O
Prague	O
Czech	O
-	O
English	O
Dependency	O
Treebank	O
(	O
Hajič	O
et	O
al	O
,	O
2012	O
)	O
,	O
Deepbank	O
(	O
Flickinger	O
et	O
al	O
,	O
2012	O
)	O
,	O
and	O
the	O
Universal	O
Conceptual	O
Cognitive	O
Annotation	O
(	O
Abend	O
and	O
Rappoport	O
,	O
2013	O
)	O
.	O
To	O
make	O
use	O
of	O
this	O
data	O
,	O
we	O
require	O
models	O
of	O
graphs	O
.	O
Consider	O
how	O
we	O
might	O
use	O
compositional	O
semantic	O
representations	O
in	O
machine	B-TaskName
translation	I-TaskName
(	O
Jones	O
et	O
al	O
,	O
2012	O
)	O
.	O
The	O
edge	O
labels	O
identify	O
'	O
cat	O
'	O
as	O
the	O
object	O
of	O
the	O
verb	O
'	O
miss	O
'	O
,	O
'	O
Anna	O
'	O
as	O
the	O
subject	O
of	O
'	O
miss	O
'	O
and	O
'	O
Anna	O
'	O
as	O
the	O
possessor	O
of	O
'	O
cat	O
'	O
.	O
Edges	O
whose	O
head	O
nodes	O
are	O
not	O
attached	O
to	O
any	O
other	O
edge	O
are	O
interpreted	O
as	O
node	O
labels	O
.	O
(	O
Figure	O
1	O
)	O
,	O
a	O
two	O
-	O
step	O
process	O
in	O
which	O
semantic	O
analysis	O
is	O
followed	O
by	O
generation	O
.	O
Jones	O
et	O
al	O
(	O
2012	O
)	O
observe	O
that	O
this	O
decomposition	O
can	O
be	O
modeled	O
with	O
a	O
pair	O
of	O
synchronous	O
grammars	O
,	O
each	O
defining	O
a	O
relation	O
between	O
strings	O
and	O
graphs	O
.	O
Necessarily	O
,	O
one	O
projection	O
of	O
this	O
synchronous	O
grammar	O
produces	O
strings	O
,	O
while	O
the	O
other	O
produces	O
graphs	O
,	O
i.e.	O
,	O
is	O
a	O
graph	O
grammar	O
.	O
A	O
consequence	O
of	O
this	O
representation	O
is	O
that	O
the	O
complete	O
translation	O
process	O
can	O
be	O
realized	O
by	O
parsing	O
:	O
to	O
analyze	O
a	O
sentence	O
,	O
we	O
parse	O
the	O
input	O
string	O
with	O
the	O
string	O
-	O
generating	O
projection	O
of	O
the	O
synchronous	O
grammar	O
,	O
and	O
read	O
off	O
the	O
synchronous	O
graph	O
from	O
the	O
resulting	O
parse	O
.	O
To	O
generate	O
a	O
sentence	O
,	O
we	O
parse	O
the	O
graph	O
,	O
and	O
read	O
off	O
the	O
synchronous	O
string	O
from	O
the	O
resulting	O
parse	O
.	O
In	O
this	O
paper	O
,	O
we	O
focus	O
on	O
the	O
latter	O
problem	O
:	O
using	O
graph	O
grammars	O
to	O
parse	O
input	O
graphs	O
.	O
We	O
call	O
this	O
graph	O
recognition	O
to	O
avoid	O
confusion	O
with	O
other	O
parsing	O
problems	O
.	O
Recent	O
work	O
in	O
NLP	O
has	O
focused	O
primarily	O
on	O
hyperedge	O
replacement	O
grammar	O
(	O
HRG	O
;	O
Drewes	O
et	O
al	O
1997	O
)	O
,	O
a	O
context	O
-	O
free	O
graph	O
grammar	O
formalism	O
that	O
has	O
been	O
studied	O
in	O
an	O
NLP	O
context	O
by	O
several	O
researchers	O
(	O
Chiang	O
et	O
al	O
,	O
2013	O
;	O
Peng	O
et	O
al	O
,	O
2015	O
;	O
Bauer	O
and	O
Rambow	O
,	O
2016	O
)	O
.	O
In	O
particular	O
,	O
Chiang	O
et	O
al	O
(	O
2013	O
)	O
propose	O
that	O
HRG	O
could	O
be	O
used	O
to	O
represent	O
semantic	O
graphs	O
,	O
and	O
precisely	O
characterize	O
the	O
complexity	O
of	O
a	O
CKY	O
-	O
style	O
algorithm	O
for	O
graph	O
recognition	O
from	O
Lautemann	O
(	O
1990	O
)	O
to	O
be	O
polynomial	O
in	O
the	O
size	O
of	O
the	O
input	O
graph	O
.	O
HRGs	O
are	O
very	O
expressive	O
-	O
they	O
can	O
generate	O
graphs	O
that	O
simulate	O
non	O
-	O
context	O
-	O
free	O
string	O
languages	O
(	O
Engelfriet	O
and	O
Heyker	O
,	O
1991	O
;	O
Bauer	O
and	O
Rambow	O
,	O
2016	O
)	O
.	O
This	O
means	O
they	O
are	O
likely	O
more	O
expressive	O
than	O
we	O
need	O
to	O
represent	O
the	O
linguistic	O
phenomena	O
that	O
appear	O
in	O
existing	O
semantic	O
datasets	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
the	O
use	O
of	O
Regular	O
Graph	O
Grammars	O
(	O
RGG	O
;	O
Courcelle	O
1991	O
)	O
a	O
subfamily	O
of	O
HRG	O
that	O
,	O
like	O
its	O
regular	O
counterparts	O
among	O
string	O
and	O
tree	O
languages	O
,	O
is	O
less	O
expressive	O
than	O
context	O
-	O
free	O
grammars	O
but	O
may	O
admit	O
more	O
practical	O
algorithms	O
.	O
By	O
analogy	O
to	O
Chiang	O
's	O
CKY	O
-	O
style	O
algorithm	O
for	O
HRG	O
.	O
We	O
develop	O
an	O
Earley	O
-	O
style	O
recognition	O
algorithm	O
for	O
RGLs	O
that	O
is	O
linear	O
in	O
the	O
size	O
of	O
the	O
input	O
graph	O
.	O

To	O
recognize	O
RGG	O
,	O
we	O
exploit	O
the	O
property	O
that	O
every	O
nonterminal	O
including	O
the	O
start	O
symbol	O
has	O
rank	O
at	O
least	O
one	O
(	O
Definition	O
5	O
)	O
,	O
and	O
we	O
assume	O
that	O
the	O
corresponding	O
external	O
node	O
is	O
identified	O
in	O
the	O
input	O
graph	O
.	O
This	O
mild	O
assumption	O
may	O
be	O
reasonable	O
for	O
applications	O
like	O
AMR	B-TaskName
parsing	I-TaskName
,	O
where	O
grammars	O
could	O
be	O
designed	O
so	O
that	O
the	O
external	O
node	O
is	O
always	O
the	O
unique	O
root	O
.	O
Later	O
we	O
relax	O
this	O
assumption	O
.	O
The	O
availability	O
of	O
an	O
identifiable	O
external	O
node	O
suggests	O
a	O
top	O
-	O
down	O
algorithm	O
,	O
and	O
we	O
take	O
in	O
-	O
spiration	O
from	O
a	O
top	O
-	O
down	O
recognition	O
algorithm	O
for	O
the	O
predictive	O
top	O
-	O
down	O
parsable	O
grammars	O
,	O
another	O
subclass	O
of	O
HRG	O
(	O
Drewes	O
et	O
al	O
,	O
2015	O
)	O
.	O
These	O
grammars	O
,	O
the	O
graph	O
equivalent	O
of	O
LL	O
(	O
1	O
)	O
string	O
grammars	O
,	O
are	O
incomparable	O
to	O
RGG	O
,	O
but	O
the	O
algorithms	O
are	O
related	O
in	O
their	O
use	O
of	O
top	O
-	O
down	O
prediction	O
and	O
in	O
that	O
they	O
both	O
fix	O
an	O
order	O
of	O
the	O
edges	O
in	O
the	O
right	O
-	O
hand	O
side	O
of	O
each	O
production	O
.	O

[	O
b	O
(	O
G	O
)	O
,	O
p	O
S	O
:	O
S	O
*	O
S	O
G	O
,	O
φ	O
p	O
S	O
]	O
can	O
be	O
proved	O
from	O
the	O
axiom	O
[	O
ext	O
G	O
,	O
p	O
S	O
:	O
S	O
*	O
S	O
G	O
,	O
φ	O
p	O
S	O
[	O
ext	O
R	O
(	O
p	O
S	O
)	O
=	O
ext	O
G	O
]	O
]	O
if	O
and	O
only	O
if	O
G	O
L	O
(	O
G	O
)	O
.	O
Proof	O
.	O
We	O
prove	O
that	O
for	O
each	O
X	O
N	O
G	O
,	O
[	O
b	O
(	O
G	O
)	O
,	O
p	O
X	O
:	O
X	O
*	O
X	O
,	O
φ	O
p	O
X	O
]	O
can	O
be	O
proved	O
from	O
[	O
ext	O
G	O
,	O
p	O
X	O
:	O
X	O
*	O
X	O
,	O
φ	O
p	O
X	O
[	O
ext	O
R	O
(	O
p	O
X	O
)	O
=	O
ext	O
G	O
]	O
]	O
if	O
and	O
only	O
if	O
G	O
L	O
X	O
(	O
G	O
)	O
where	O
the	O
dummy	O
nonterminal	O
X	O
*	O
was	O
added	O
to	O
the	O
set	O
of	O
nonterminals	O
and	O
p	O
X	O
:	O
X	O
*	O
X	O
was	O
added	O
to	O
the	O
set	O
of	O
productions	O
.	O
We	O
prove	O
this	O
by	O
induction	O
on	O
the	O
number	O
of	O
edges	O
in	O
G.	O
We	O
assume	O
that	O
each	O
production	O
in	O
the	O
grammar	O
contains	O
at	O
least	O
one	O
terminal	O
edge	O
.	O
If	O
the	O
HRG	O
is	O
not	O
in	O
this	O
form	O
,	O
it	O
can	O
be	O
converted	O
into	O
this	O
form	O
and	O
in	O
the	O
case	O
of	O
RGGs	O
they	O
are	O
already	O
in	O
this	O
form	O
by	O
definition	O
.	O
Base	O
Case	O
:	O
Let	O
G	O
consist	O
of	O
a	O
single	O
edge	O
.	O
If	O
:	O
Assume	O
G	O
L	O
X	O
(	O
G	O
)	O
.	O
Since	O
G	O
consists	O
of	O
one	O
edge	O
,	O
there	O
must	O
be	O
a	O
production	O
q	O
:	O
X	O
G.	O
Apply	O
PREDICT	O
to	O
the	O
axiom	O
and	O
p	O
X	O
:	O
X	O
*	O
X	O
to	O
obtain	O
the	O
item	O
[	O
φ	O
p	O
X	O
(	O
X	O
)	O
,	O
q	O
:	O
X	O
G	O
,	O
φ	O
0	B-DatasetName
q	O
[	O
ext	O
G	O
=	O
φ	O
p	O
X	O
(	O
X	O
)	O
]	O
]	O
.	O
Apply	O
SCAN	B-DatasetName
to	O
the	O
single	O
terminal	O
edge	O
that	O
makes	O
up	O
G	O
to	O
obtain	O
[	O
b	O
(	O
G	O
)	O
,	O
q	O
:	O
X	O
G	O
,	O
φ	O
q	O
]	O
and	O
finally	O
apply	O
COMPLETE	O
to	O
this	O
and	O
the	O
axiom	O
reach	O
the	O
goal	O
[	O
b	O
(	O
G	O
)	O
,	O
p	O
X	O
:	O
X	O
*	O
X	O
,	O
φ	O
p	O
X	O
]	O
.	O
Only	O
if	O
:	O
Assume	O
the	O
goal	O
can	O
be	O
reached	O
from	O
the	O
axiom	O
and	O
G	O
=	O
e.	O
Then	O
the	O
item	O
[	O
b	O
(	O
e	O
)	O
,	O
q	O
:	O
X	O
e	O
,	O
φ	O
q	O
]	O
must	O
have	O
been	O
reached	O
at	O
some	O
point	O
for	O
some	O
q	O
P	O
G	O
.	O
Therefore	O
q	O
:	O
X	O
e	O
is	O
a	O
production	O
and	O
so	O
e	O
=	O
G	O
L	O
X	O
(	O
G	O
)	O
.	O
Assumption	O
:	O
Assume	O
that	O
the	O
proposition	O
holds	O
when	O
G	O
has	O
fewer	O
than	O
k	O
edges	O
.	O
Inductive	O
Step	O
:	O
Assume	O
G	O
has	O
k	O
edges	O
.	O
If	O
:	O
Assume	O
G	O
L	O
X	O
(	O
G	O
)	O
,	O
then	O
there	O
is	O
a	O
production	O
q	O
:	O
X	O
H	O
where	O
H	O
has	O
nonterminals	O
Y	O
1	O
,	O
.	O
.	O
.	O
,	O
Y	O
n	O
and	O
there	O
are	O
graphs	O
H	O
1	O
,	O
.	O
.	O
.	O
,	O
H	O
n	O
such	O
that	O
G	O
=	O
H	O
[	O
Y	O
1	O
/H	O
1	O
]	O
.	O
.	O
.	O
[	O
Y	O
n	O
/H	O
n	O
]	O
.	O
Each	O
graph	O
H	O
i	O
for	O
i	O
[	O
n	O
]	O
has	O
fewer	O
than	O
k	O
edges	O
and	O
so	O
we	O
apply	O
the	O
inductive	O
hypothesis	O
to	O
show	O
that	O
we	O
can	O
prove	O
the	O
items	O
[	O
b	O
(	O
H	O
i	O
)	O
,	O
r	O
i	O
:	O
Y	O
i	O
J	O
i	O
,	O
φ	O
r	O
i	O
]	O
for	O
each	O
i	O
[	O
n	O
]	O
.	O
By	O
applying	O
COMPLETE	O
to	O
each	O
such	O
item	O
and	O
applying	O
SCAN	B-DatasetName
to	O
each	O
terminal	O
edge	O
of	O
H	O
we	O
reach	O
the	O
goal	O
[	O
b	O
(	O
G	O
)	O
,	O
p	O
X	O
:	O
X	O
*	O
X	O
,	O
φ	O
p	O
X	O
]	O
.	O
Only	O
If	O
:	O
Assume	O
the	O
goal	O
can	O
be	O
proved	O
from	O
the	O
axiom	O
.	O
Then	O
we	O
must	O
have	O
at	O
some	O
point	O
reached	O
an	O
item	O
of	O
the	O
form	O
[	O
b	O
(	O
G	O
)	O
,	O
q	O
:	O
X	O
H	O
,	O
φ	O
q	O
]	O
and	O
that	O
H	O
has	O
nonterminals	O
Y	O
1	O
,	O
.	O
.	O
.	O
,	O
Y	O
n	O
.	O
This	O
means	O
that	O
there	O
are	O
graphs	O
H	O
1	O
,	O
.	O
.	O
.	O
,	O
H	O
n	O
such	O
that	O
[	O
b	O
(	O
H	O
i	O
)	O
,	O
p	O
Y	O
i	O
:	O
Y	O
*	O
i	O
Y	O
i	O
,	O
φ	O
p	O
Y	O
i	O
]	O
for	O
each	O
i	O
[	O
n	O
]	O
and	O
G	O
=	O
H	O
[	O
Y	O
1	O
/H	O
1	O
]	O
.	O
.	O
.	O
[	O
Y	O
n	O
/H	O
n	O
]	O
.	O
Since	O
each	O
H	O
i	O
has	O
fewer	O
than	O
k	O
edges	O
,	O
we	O
apply	O
the	O
inductive	O
hypothesis	O
to	O
get	O
that	O
H	O
i	O
L	O
Y	O
i	O
(	O
G	O
)	O
for	O
each	O
i	O
[	O
n	O
]	O
and	O
therefore	O
G	O
L	O
X	O
(	O
G	O
)	O
.	O
Example	O
5	O
.	O
Using	O
the	O
RGG	O
in	O
Table	O
1	O
,	O
we	O
show	O
how	O
to	O
recognize	O
the	O
graph	O
in	O
Figure	O
7	O
,	O
which	O
can	O
be	O
derived	O
by	O
applying	O
production	O
s	O
followed	O
by	O
production	O
u	O
,	O
where	O
the	O
external	O
nodes	O
of	O
Y	O
are	O
(	O
v	O
3	O
,	O
v	O
2	O
)	O
.	O
Assume	O
the	O
ordering	O
of	O
the	O
edges	O
in	O
production	O
s	O
is	O
arg1	O
,	O
arg0	O
,	O
Z	O
;	O
the	O
top	O
node	O
isv	O
1	O
;	O
the	O
bottom	O
node	O
isv	O
2	O
;	O
and	O
the	O
node	O
on	O
the	O
right	O
isv	O
3	O
;	O
and	O
that	O
the	O
marker	O
node	O
is	O
not	O
in	O
this	O
subgraphwe	O
elide	O
reference	O
to	O
it	O
for	O
simplicity	O
.	O
Letv	O
4	O
be	O
the	O
top	O
node	O
of	O
R	O
(	O
u	O
)	O
andv	O
5	O
be	O
the	O
bottom	O
node	O
of	O
R	O
(	O
u	O
)	O
.	O
The	O
external	O
nodes	O
of	O
Y	O
are	O
determined	O
top	O
-	O
down	O
,	O
so	O
the	O
recognize	O
of	O
this	O
subgraph	O
is	O
triggered	O
by	O
this	O
item	O
:	O
[	O
{	O
v	O
3	O
,	O
v	O
2	O
}	O
,	O
Y	O
arg1	O
arg0	O
Z	O
,	O
φ	O
0	B-DatasetName
s	O
[	O
ext	O
R	O
(	O
s	O
)	O
=	O
(	O
v	O
3	O
,	O
v	O
2	O
)	O
]	O
]	O
(	O
2	O
)	O
where	O
φ	O
s	O
(	O
arg1	O
)	O
=	O
(	O
v	O
1	O
,	O
v	O
3	O
)	O
,	O
φ	O
s	O
(	O
arg0	O
)	O
=	O
(	O
v	O
1	O
,	O
v	O
2	O
)	O
,	O
and	O
φ	O
s	O
(	O
Z	O
)	O
=	O
(	O
v	O
1	O
)	O
.	O
Table	O
3	O
shows	O
how	O
we	O
can	O
prove	O
the	O
item	O
[	O
{	O
v	O
3	O
,	O
v	O
2	O
}	O
,	O
{	O
e	O
3	O
,	O
e	O
2	O
}	O
,	O
Y	O
arg1arg0Z	O
,	O
φ	O
]	O
The	O
boundary	O
representation	O
{	O
v	O
3	O
,	O
v	O
2	O
}	O
,	O
{	O
e	O
3	O
,	O
e	O
2	O
}	O
in	O
this	O
item	O
represents	O
the	O
whole	O
subgraph	O
shown	O
in	O
Figure	O
7	O
.	O
v	O
1	O
v	O
4	O
v	O
2	O
v	O
3	O
.	O
.	O
.	O
.	O
.	O
.	O
need	O
(	O
e1	O
)	O
arg0	O
(	O
e2	O
)	O
arg1	O
(	O
e3	O
)	O

Our	O
algorithm	O
requires	O
a	O
fixed	O
ordering	O
of	O
the	O
edges	O
in	O
the	O
right	O
-	O
hand	O
sides	O
of	O
each	O
production	O
.	O
We	O
will	O
constrain	O
this	O
ordering	O
to	O
exploit	O
the	O
structure	O
of	O
RGG	O
productions	O
,	O
allowing	O
us	O
to	O
bound	O
recognition	O
complexity	O
.	O
If	O
s	O
=	O
ē	O
1	O
.	O
.	O
.ē	O
n	O
is	O
an	O
order	O
,	O
define	O
s	O
i	O
:	O
j	O
=	O
ē	O
i	O
.	O
.	O
.ē	O
j	O
.	O
Definition	O
7	O
.	O
Let	O
s	O
=	O
ē	O
1	O
,	O
.	O
.	O
.	O
,	O
ē	O
n	O
be	O
an	O
edge	O
order	O
of	O
a	O
right	O
-	O
hand	O
side	O
of	O
a	O
production	O
.	O
Then	O
s	O
is	O
normal	O
if	O
it	O
has	O
the	O
following	O
properties	O
:	O
1.ē	O
1	O
is	O
connected	O
to	O
an	O
external	O
node	O
,	O
2	O
.	O
s	O
1	O
:	O
j	O
is	O
a	O
connected	O
graph	O
for	O
all	O
j	O
[	O
n	O
]	O
3	O
.	O
ifē	O
i	O
is	O
nonterminal	O
,	O
each	O
endpoint	O
ofē	O
i	O
must	O
be	O
incident	O
with	O
some	O
terminal	O
edgeē	O
j	O
for	O
which	O
j	O
<	O
i.	O
Example	O
6	O
.	O
The	O
ordering	O
of	O
the	O
edges	O
of	O
production	O
s	O
in	O
Example	O
5	O
is	O
normal	O
.	O
Arbitrary	O
HRGs	O
do	O
not	O
necessarily	O
admit	O
a	O
normal	O
ordering	O
.	O
For	O
example	O
,	O
the	O
graph	O
in	O
Figure	O
8	O
can	O
not	O
satisfy	O
Properties	O
2	O
and	O
3	O
simultaneously	O
.	O
However	O
,	O
RGGs	O
do	O
admit	O
a	O
normal	O
ordering	O
.	O
Current	O
Item	O
Reason	O
1	O
.	O
[	O
{	O
v3	O
,	O
v2	O
}	O
,	O
Y	O
arg1arg0Z	O
,	O
φ	O
0	B-DatasetName
s	O
[	O
ext	O
R	O
(	O
s	O
)	O
=	O
(	O
v3	O
,	O
v2	O
)	O
]	O
]	O
Equation	O
2	O
2	O
.	O
[	O
{	O
v3	O
,	O
v2	O
,	O
v1	O
}	O
,	O
{	O
e3	O
}	O
,	O
Y	O
arg1	O
arg0Z	O
,	O
φs	O
[	O
att	O
(	O
arg1	O
)	O
=	O
(	O
v1	O
,	O
v3	O
)	O
]	O
]	O
SCAN	B-DatasetName
:	O
1	O
.	O
and	O
e3	O
=	O
edg	O
arg1	O
(	O
v1	O
,	O
v3	O
)	O
3	O
.	O
[	O
{	O
v3	O
,	O
v2	O
,	O
v1	O
}	O
,	O
{	O
e3	O
,	O
e2	O
}	O
,	O
Y	O
arg1arg0	O
Z	O
,	O
φs	O
[	O
att	O
(	O
arg0	O
)	O
=	O
(	O
v1	O
,	O
v2	O
)	O
]	O
]	O
SCAN	B-DatasetName
:	O
2	O
.	O
and	O
e2	O
=	O
edg	O
arg0	O
(	O
v1	O
,	O
v2	O
)	O
]	O
4	O
.	O
[	O
(	O
v1	O
)	O
,	O
Z	O
need	O
,	O
φ	O
0	B-DatasetName
u	O
[	O
ext	O
R	O
(	O
u	O
)	O
=	O
(	O
v1	O
)	O
]	O
]	O
PREDICT	O
:	O
3	O
.	O
and	O
Z	O
need	O
5	O
.	O
[	O
{	O
v1	O
,	O
v4	O
}	O
,	O
{	O
e1	O
}	O
,	O
Z	O
need	O
,	O
φu	O
[	O
att	O
(	O
need	O
)	O
=	O
(	O
v1	O
,	O
v4	O
)	O
]	O
]	O
SCAN	B-DatasetName
:	O
4	O
.	O
and	O
e1	O
=	O
edg	O
need	O
(	O
v1	O
,	O
v4	O
)	O
6	O
.	O
[	O
{	O
v3	O
,	O
v2	O
}	O
,	O
{	O
e3	O
,	O
e2	O
}	O
,	O
Y	O
arg1arg0Z	O
,	O
φs	O
[	O
att	O
(	O
Z	O
)	O
=	O
(	O
v1	O
)	O
]	O
]	O
COMPLETE	O
:	O
3	O
.	O
and	O
5	O
.	O
Table	O
3	O
:	O
The	O
steps	O
of	O
recognizing	O
that	O
the	O
subgraph	O
shown	O
in	O
Figure	O
7	O
is	O
derived	O
from	O
productions	O
r2	O
and	O
u	O
in	O
the	O
grammar	O
in	O
Table	O
1	O
.	O
Proposition	O
2	O
.	O
If	O
G	O
is	O
an	O
RGG	O
,	O
for	O
every	O
p	O
P	O
G	O
,	O
there	O
is	O
a	O
normal	O
ordering	O
of	O
the	O
edges	O
in	O
R	O
(	O
p	O
)	O
.	O
Proof	O
.	O
If	O
R	O
(	O
p	O
)	O
contains	O
a	O
single	O
node	O
then	O
it	O
must	O
be	O
an	O
external	O
node	O
and	O
it	O
must	O
have	O
a	O
terminal	O
edge	O
attached	O
to	O
it	O
since	O
R	O
(	O
p	O
)	O
must	O
contain	O
at	O
least	O
one	O
terminal	O
edge	O
.	O
If	O
R	O
(	O
p	O
)	O
contains	O
multiple	O
nodes	O
then	O
by	O
C2	O
there	O
must	O
be	O
terminal	O
internal	O
paths	O
between	O
all	O
of	O
them	O
,	O
so	O
there	O
must	O
be	O
a	O
terminal	O
edge	O
attached	O
to	O
the	O
external	O
node	O
,	O
which	O
we	O
use	O
to	O
satisfy	O
Property	O
1	O
.	O
To	O
produce	O
a	O
normal	O
ordering	O
,	O
we	O
next	O
select	O
terminal	O
edges	O
once	O
one	O
of	O
their	O
endpoints	O
is	O
connected	O
to	O
an	O
ordered	O
edge	O
,	O
and	O
nonterminal	O
edges	O
once	O
all	O
endpoints	O
are	O
connected	O
to	O
ordered	O
edges	O
,	O
possible	O
by	O
C2	O
.	O
Therefore	O
,	O
Properties	O
2	O
and	O
3	O
are	O
satisfied	O
.	O
A	O
normal	O
ordering	O
tightly	O
constrains	O
the	O
recognition	O
of	O
edges	O
.	O
Property	O
3	O
ensures	O
that	O
when	O
we	O
apply	O
PREDICT	O
,	O
the	O
external	O
nodes	O
of	O
the	O
predicted	O
edge	O
are	O
all	O
bound	O
to	O
specific	O
nodes	O
in	O
the	O
graph	O
.	O
Properties	O
1	O
and	O
2	O
ensure	O
that	O
when	O
we	O
apply	O
SCAN	B-DatasetName
,	O
at	O
least	O
one	O
endpoint	O
of	O
the	O
edge	O
is	O
bound	O
(	O
fixed	O
)	O
.	O

Assume	O
a	O
normally	O
-	O
ordered	O
RGG	O
.	O
Let	O
the	O
maximum	O
number	O
of	O
edges	O
in	O
the	O
right	O
-	O
hand	O
side	O
of	O
any	O
production	O
be	O
m	O
;	O
the	O
maximum	O
number	O
of	O
nodes	O
in	O
any	O
right	O
-	O
hand	O
side	O
of	O
a	O
production	O
k	O
;	O
the	O
maximum	O
degree	O
of	O
any	O
node	O
in	O
the	O
input	O
graph	O
d	O
;	O
and	O
the	O
number	O
of	O
nodes	O
in	O
the	O
input	O
graph	O
n.	O
As	O
previously	O
mentioned	O
,	O
Drewes	O
et	O
al	O
(	O
2015	O
)	O
also	O
propose	O
a	O
HRG	O
recognizer	O
which	O
can	O
recognize	O
a	O
subclass	O
of	O
HRG	O
(	O
incomparable	O
to	O
RGG	O
)	O
called	O
the	O
predictive	O
top	O
-	O
down	O
parsable	O
grammars	O
.	O
Their	O
recognizer	O
in	O
this	O
case	O
runs	O
in	O
O	O
(	O
n	O
2	O
)	O
time	O
.	O
A	O
well	O
-	O
known	O
bottom	O
-	O
up	O
recognizing	O
algorithm	O
for	O
HRG	O
was	O
first	O
proposed	O
by	O
Lautemann	O
(	O
1990	O
)	O
.	O
In	O
this	O
paper	O
,	O
the	O
recognizer	O
is	O
shown	O
to	O
be	O
polynomial	O
in	O
the	O
size	O
of	O
the	O
input	O
graph	O
.	O
Later	O
,	O
Chiang	O
et	O
al	O
(	O
2013	O
)	O
formulate	O
the	O
same	O
algorithm	O
more	O
precisely	O
and	O
show	O
that	O
the	O
recognizing	O
complexity	O
is	O
O	O
(	O
(	O
3	O
d	O
×	O
n	O
)	O
k+1	O
)	O
where	O
k	O
in	O
their	O
case	O
is	O
the	O
treewidth	O
of	O
the	O
grammar	O
.	O
Remark	O
1	O
.	O
The	O
maximum	O
number	O
of	O
nodes	O
in	O
any	O
right	O
-	O
hand	O
side	O
of	O
a	O
production	O
(	O
k	O
)	O
is	O
also	O
the	O
maximum	O
number	O
of	O
boundary	O
nodes	O
for	O
any	O
subgraph	O
in	O
the	O
recognizer	O
.	O
COMPLETE	O
combines	O
subgraphs	O
I	O
and	O
J	O
only	O
when	O
the	O
entire	O
subgraph	O
derived	O
from	O
Y	O
has	O
been	O
recognized	O
.	O
Boundary	O
nodes	O
of	O
J	O
are	O
also	O
boundary	O
nodes	O
of	O
I	O
because	O
they	O
are	O
nodes	O
in	O
the	O
terminal	O
subgraph	O
of	O
R	O
(	O
p	O
)	O
where	O
Y	O
connects	O
.	O
The	O
boundary	O
nodes	O
of	O
I	O
∪	O
J	O
are	O
also	O
bounded	O
by	O
k	O
since	O
form	O
a	O
subset	O
of	O
the	O
boundary	O
nodes	O
of	O
I.	O
Remark	O
2	O
.	O
Given	O
a	O
boundary	O
node	O
,	O
there	O
are	O
at	O
most	O
(	O
d	O
m	O
)	O
k−1	O
ways	O
of	O
identifying	O
the	O
remaining	O
boundary	O
nodes	O
of	O
a	O
subgraph	O
that	O
is	O
isomorphic	O
to	O
the	O
terminal	O
subgraph	O
of	O
the	O
right	O
-	O
hand	O
side	O
of	O
a	O
production	O
.	O
The	O
terminal	O
subgraph	O
of	O
each	O
production	O
is	O
connected	O
by	O
C2	O
,	O
with	O
a	O
maximum	O
path	O
length	O
of	O
m.	O
For	O
each	O
edge	O
in	O
the	O
path	O
,	O
there	O
are	O
at	O
most	O
d	O
subsequent	O
edges	O
.	O
Hence	O
for	O
the	O
k	O
−	O
1	O
remaining	O
boundary	O
nodes	O
there	O
are	O
(	O
d	O
m	O
)	O
k−1	O
ways	O
of	O
choosing	O
them	O
.	O
We	O
count	O
instantiations	O
of	O
COMPLETE	O
for	O
an	O
upper	O
bound	O
on	O
complexity	O
(	O
McAllester	O
,	O
2002	O
)	O
,	O
using	O
similar	O
logic	O
to	O
(	O
Chiang	O
et	O
al	O
,	O
2013	O
)	O
.	O
The	O
number	O
of	O
boundary	O
nodes	O
of	O
I	O
,	O
J	O
and	O
I	O
∪	O
J	O
is	O
at	O
most	O
k.	O
Therefore	O
,	O
if	O
we	O
choose	O
an	O
arbitrary	O
node	O
to	O
be	O
some	O
boundary	O
node	O
of	O
I	O
∪	O
J	O
,	O
there	O
are	O
at	O
most	O
(	O
d	O
m	O
)	O
k−1	O
ways	O
of	O
choosing	O
its	O
remaining	O
boundary	O
nodes	O
.	O
For	O
each	O
of	O
these	O
nodes	O
,	O
there	O
are	O
at	O
most	O
(	O
3	O
d	O
)	O
k	O
states	O
of	O
their	O
attached	O
boundary	O
edges	O
:	O
in	O
I	O
,	O
in	O
J	O
,	O
or	O
in	O
neither	O
.	O
The	O
total	O
number	O
of	O
instantiations	O
is	O
O	O
(	O
n	O
(	O
d	O
m	O
)	O
k−1	O
(	O
3	O
d	O
)	O
k	O
)	O
,	O
linear	O
in	O
the	O
number	O
of	O
input	O
nodes	O
and	O
exponential	O
in	O
the	O
degree	O
of	O
the	O
input	O
graph	O
.	O
Note	O
that	O
in	O
the	O
case	O
of	O
the	O
AMR	O
dataset	O
(	O
Banarescu	O
et	O
al	O
2013	O
)	O
,	O
the	O
maximum	O
node	O
degree	O
is	O
17	O
and	O
the	O
average	O
is	O
2.12	O
.	O
We	O
observe	O
that	O
RGGs	O
could	O
be	O
relaxed	O
to	O
produce	O
graphs	O
with	O
no	O
external	O
nodes	O
by	O
adding	O
a	O
dummy	O
nonterminal	O
S	O
with	O
rank	O
0	B-DatasetName
and	O
a	O
single	O
production	O
S	O
S.	O
To	O
adapt	O
the	O
recognition	O
algorithm	O
,	O
we	O
would	O
first	O
need	O
to	O
guess	O
where	O
the	O
graph	O
starts	O
.	O
This	O
would	O
add	O
a	O
factor	O
of	O
n	O
to	O
the	O
complexity	O
as	O
the	O
graph	O
could	O
start	O
at	O
any	O
node	O
.	O

We	O
have	O
presented	O
RGG	O
as	O
a	O
formalism	O
that	O
could	O
be	O
useful	O
for	O
semantic	O
representations	O
and	O
we	O
have	O
provided	O
a	O
top	O
-	O
down	O
recognition	O
algorithm	O
for	O
them	O
.	O
The	O
constraints	O
of	O
RGG	O
enable	O
more	O
efficient	O
recognition	O
than	O
general	O
HRG	O
,	O
and	O
this	O
tradeoff	O
is	O
reasonable	O
since	O
HRG	O
is	O
very	O
expressive	O
-	O
when	O
generating	O
strings	O
,	O
it	O
can	O
express	O
non	O
-	O
context	O
-	O
free	O
languages	O
(	O
Engelfriet	O
and	O
Heyker	O
,	O
1991	O
;	O
Bauer	O
and	O
Rambow	O
,	O
2016	O
)	O
,	O
far	O
more	O
power	O
than	O
needed	O
to	O
express	O
semantic	O
graphs	O
.	O
On	O
the	O
other	O
hand	O
,	O
RGG	O
is	O
so	O
constrained	O
that	O
it	O
may	O
not	O
be	O
expressive	O
enough	O
:	O
it	O
would	O
be	O
more	O
natural	O
to	O
derive	O
the	O
graph	O
in	O
Figure	O
4	O
from	O
outermost	O
to	O
innermost	O
predicate	O
;	O
but	O
constraint	O
C2	O
makes	O
it	O
difficult	O
to	O
express	O
this	O
,	O
and	O
the	O
grammar	O
in	O
Table	O
1	O
A	O
possible	O
alternative	O
would	O
be	O
to	O
consider	O
Restricted	B-DatasetName
DAG	O
Grammars	O
(	O
RDG	O
;	O
Björklund	O
et	O
al	O
2016	O
)	O
.	O
Parsing	O
for	O
a	O
fixed	O
such	O
grammar	O
can	O
be	O
achieved	O
in	O
quadratic	O
time	O
with	O
respect	O
to	O
the	O
input	O
graph	O
.	O
It	O
is	O
known	O
that	O
for	O
a	O
fixed	O
HRG	O
generating	O
k	O
-	O
connected	O
hypergraphs	O
consisting	O
of	O
hyperedges	O
of	O
rank	O
k	O
only	O
,	O
parsing	O
can	O
be	O
carried	O
out	O
in	O
cubic	O
time	O
(	O
k	O
-	O
HRG	O
;	O
(	O
Drewes	O
,	O
1993	O
)	O
)	O
.	O
More	O
general	O
than	O
RDGs	O
a	O
is	O
the	O
class	O
of	O
graph	O
languages	O
recognized	O
by	O
DAG	O
automata	O
(	O
DA	O
-	O
GAL	O
;	O
Blum	O
and	O
Drewes	O
2016	O
)	O
,	O
for	O
which	O
the	O
deterministic	O
variant	O
provides	O
polynomial	O
time	O
parsing	O
.	O
Note	O
that	O
RGGs	O
can	O
generate	O
graph	O
languages	O
of	O
unbounded	O
node	O
degree	O
.	O
With	O
respect	O
to	O
expressive	O
power	O
,	O
RDGs	O
and	O
k	O
-	O
HRGs	O
are	O
incomparable	O
to	O
RGGs	O
.	O
Figure	O
9	O
shows	O
the	O
relationships	O
between	O
the	O
context	O
-	O
free	O
and	O
regular	O
languages	O
for	O
strings	O
,	O
trees	O
and	O
graphs	O
.	O
Monadic	O
-	O
second	O
order	O
logic	O
(	O
MSOL	O
;	O
Courcelle	O
and	O
Engelfriet	O
2011	O
)	O
is	O
a	O
form	O
of	O
logic	O
which	O
when	O
restricted	O
to	O
strings	O
gives	O
us	O
exactly	O
the	O
regular	O
string	O
languages	O
and	O
when	O
restricted	O
to	O
trees	O
gives	O
us	O
exactly	O
the	O
regular	O
tree	O
languages	O
.	O
RGLs	O
lie	O
in	O
the	O
intersection	O
of	O
HRG	O
and	O
MSOL	O
on	O
graphs	O
but	O
they	O
do	O
not	O
make	O
up	O
this	O
entire	O
intersection	O
.	O
Courcelle	O
(	O
1991	O
)	O
defined	O
(	O
non	O
-	O
constructively	O
)	O
this	O
intersection	O
to	O
be	O
the	O
strongly	O
context	O
-	O
free	O
languages	O
(	O
SCFL	O
)	O
.	O
We	O
believe	O
that	O
there	O
may	O
be	O
other	O
formalisms	O
that	O
are	O
subfamilies	O
of	O
SCFL	O
which	O
may	O
be	O
useful	O
for	O
semantic	O
representations	O
.	O
All	O
inclusions	O
shown	O
in	O
Figure	O
9	O
are	O
strict	O
.	O
For	O
instance	O
,	O
RGL	O
can	O
not	O
produce	O
"	O
star	O
graphs	O
"	O
(	O
one	O
node	O
that	O
has	O
edges	O
to	O
n	O
other	O
nodes	O
)	O
,	O
while	O
DAGAL	O
and	O
HRL	O
can	O
produce	O
such	O
graphs	O
.	O
It	O
is	O
well	O
-	O
known	O
that	O
HRL	O
and	O
MSOL	O
are	O
incomparable	O
.	O
There	O
is	O
a	O
language	O
in	O
RGL	O
that	O
is	O
not	O
in	O
DAGAL	O
,	O
for	O
instance	O
,	O
"	O
ladders	O
"	O
(	O
two	O
string	O
graphs	O
of	O
n	O
nodes	O
each	O
,	O
with	O
an	O
edge	O
between	O
the	O
ith	O
node	O
of	O
each	O
string	O
)	O
.	O
Another	O
alternative	O
formalism	O
to	O
RGG	O
that	O
is	O
defined	O
as	O
a	O
restriction	O
of	O
HRG	O
are	O
Tree	O
-	O
like	O
Grammars	O
(	O
TLG	O
;	O
Matheja	O
et	O
al	O
2015	O
)	O
.	O
They	O
define	O
a	O
subclass	O
of	O
SCFL	O
,	O
i.e.	O
,	O
they	O
are	O
MSO	O
definable	O
.	O
TLGs	O
have	O
been	O
considered	O
for	O
program	O
verification	O
,	O
where	O
closure	O
under	O
intersection	O
of	O
the	O
formalism	O
is	O
essential	O
.	O
Note	O
that	O
RGGs	O
are	O
also	O
closed	O
under	O
intersection	O
.	O
While	O
TLG	O
and	O
RDG	O
are	O
both	O
incomparable	O
to	O
RGG	O
,	O
they	O
share	O
important	O
characteristics	O
,	O
including	O
the	O
fact	O
that	O
the	O
terminal	O
subgraph	O
of	O
every	O
production	O
is	O
connected	O
.	O
This	O
means	O
that	O
our	O
top	O
-	O
down	O
recognition	O
algorithm	O
is	O
applicable	O
to	O
both	O
.	O
In	O
the	O
future	O
we	O
would	O
like	O
to	O
investigate	O
larger	O
,	O
less	O
restrictive	O
(	O
and	O
more	O
linguistically	O
expressive	O
)	O
subfamilies	O
of	O
SCFL	O
.	O
We	O
plan	O
to	O
implement	O
and	O
evaluate	O
our	O
algorithm	O
experimentally	O
.	O

This	O
work	O
was	O
supported	O
in	O
part	O
by	O
the	O
EPSRC	O
Centre	O
for	O
Doctoral	O
Training	O
in	O
Data	O
Science	O
,	O
funded	O
by	O
the	O
UK	O
Engineering	O
and	O
Physical	O
Sciences	O
Research	O
Council	O
(	O
grant	O
EP	O
/	O
L016427/1	O
)	O
and	O
the	O
University	O
of	O
Edinburgh	O
;	O
and	O
in	O
part	O
by	O
a	O
Google	B-DatasetName
faculty	O
research	O
award	O
(	O
to	O
AL	O
)	O
.	O
We	O
thank	O
Clara	O
Vania	O
,	O
Sameer	O
Bansal	O
,	O
Ida	O
Szubert	O
,	O
Federico	O
Fancellu	O
,	O
Antonis	O
Anastasopoulos	O
,	O
Marco	O
Damonte	O
,	O
and	O
the	O
anonymous	O
reviews	O
for	O
helpful	O
discussion	O
of	O
this	O
work	O
and	O
comments	O
on	O
previous	O
drafts	O
of	O
the	O
paper	O
.	O

Neural	O
Machine	B-TaskName
Translation	I-TaskName
with	O
the	O
Transformer	B-MethodName
and	O
Multi	O
-	O
Source	O
Romance	O
Languages	O
for	O
the	O
Biomedical	O
WMT	B-DatasetName
2018	I-DatasetName
task	O

The	O
Transformer	B-MethodName
model	O
is	O
the	O
first	O
NMT	O
model	O
relying	O
entirely	O
on	O
self	O
-	O
attention	O
to	O
compute	O
representations	O
of	O
its	O
input	O
and	O
output	O
without	O
using	O
recurrent	O
neural	O
networks	O
(	O
RNN	O
)	O
or	O
convolutional	O
neural	O
networks	O
(	O
CNN	O
)	O
.	O
RNNs	O
read	O
one	O
word	O
at	O
a	O
time	O
,	O
having	O
to	O
perform	O
multiple	O
steps	O
before	O
generating	O
an	O
output	O
that	O
depends	O
on	O
words	O
that	O
are	O
far	O
away	O
.	O
But	O
it	O
has	O
been	O
demonstrated	O
that	O
the	O
more	O
steps	O
required	O
,	O
the	O
harder	O
it	O
is	O
to	O
the	O
network	O
to	O
learn	O
how	O
to	O
make	O
these	O
decisions	O
(	O
Bahdanau	O
et	O
al	O
,	O
2015	O
)	O
.	O
In	O
addition	O
,	O
given	O
the	O
sequential	O
nature	O
of	O
the	O
RNNs	O
,	O
it	O
is	O
difficult	O
to	O
fully	O
take	O
advantage	O
of	O
modern	O
computing	O
devices	O
such	O
as	O
Tensor	O
Processing	O
Units	O
(	O
TPUs	O
)	O
or	O
Graphics	O
Processing	O
Units	O
(	O
GPUs	O
)	O
which	O
rely	O
on	O
parallel	O
processing	O
.	O
The	O
Transformer	B-MethodName
is	O
an	O
encoder	O
-	O
decoder	O
model	O
that	O
was	O
conceived	O
to	O
solve	O
these	O
problems	O
.	O
The	O
encoder	O
is	O
composed	O
of	O
three	O
stages	O
.	O
In	O
the	O
first	O
stage	O
input	O
words	O
are	O
projected	O
into	O
an	O
embedded	O
vector	O
space	O
.	O
In	O
order	O
to	O
capture	O
the	O
notion	O
of	O
token	O
position	O
within	O
the	O
sequence	O
,	O
a	O
positional	O
encoding	O
is	O
added	O
to	O
the	O
embedded	O
input	O
vectors	O
.	O
Without	O
positional	O
encodings	O
,	O
the	O
output	O
of	O
the	O
multi	B-MethodName
-	I-MethodName
head	I-MethodName
attention	I-MethodName
network	O
would	O
be	O
the	O
same	O
for	O
the	O
sentences	O
"	O
I	O
love	O
you	O
more	O
than	O
her	O
"	O
and	O
"	O
I	O
love	O
her	O
more	O
than	O
you	O
"	O
.	O
The	O
second	O
stage	O
is	O
a	O
multi	O
-	O
head	O
self	O
-	O
attention	O
.	O
Instead	O
of	O
computing	O
a	O
single	O
attention	O
,	O
this	O
stage	O
computes	O
multiple	O
attention	O
blocks	O
over	O
the	O
source	O
,	O
concatenates	O
them	O
and	O
projects	O
them	O
linearly	O
back	O
onto	O
a	O
space	O
with	O
the	O
initial	O
dimensionality	O
.	O
The	O
individual	O
attention	O
blocks	O
compute	O
the	O
scaled	B-MethodName
dot	I-MethodName
-	I-MethodName
product	I-MethodName
attention	I-MethodName
with	O
different	O
linear	O
projections	O
.	O
Finally	O
a	O
position	O
-	O
wise	O
fully	O
connected	O
feed	O
-	O
forward	O
network	O
is	O
used	O
,	O
which	O
consists	O
of	O
two	O
linear	O
transformations	O
with	O
a	O
ReLU	B-MethodName
activation	O
(	O
Vinod	O
Nair	O
,	O
2010	O
)	O
The	O
decoder	O
operates	O
similarly	O
,	O
but	O
generates	O
one	O
word	O
at	O
a	O
time	O
,	O
from	O
left	O
to	O
right	O
.	O
It	O
is	O
composed	O
of	O
five	O
stages	O
.	O
The	O
first	O
two	O
are	O
similar	O
to	O
the	O
encoder	O
:	O
embedding	O
and	O
positional	O
encoding	O
and	O
a	O
masked	O
multi	O
-	O
head	O
self	O
-	O
attention	O
,	O
which	O
unlike	O
in	O
the	O
encoder	O
,	O
forces	O
to	O
attend	O
only	O
to	O
past	O
words	O
.	O
The	O
third	O
stage	O
is	O
a	O
multi	B-MethodName
-	I-MethodName
head	I-MethodName
attention	I-MethodName
that	O
not	O
only	O
attends	O
to	O
these	O
past	O
words	O
,	O
but	O
also	O
to	O
the	O
final	O
representations	O
generated	O
by	O
the	O
encoder	O
.	O
The	O
fourth	O
stage	O
is	O
another	O
position	O
-	O
wise	O
feed	O
-	O
forward	O
network	O
.	O
Finally	O
,	O
a	O
softmax	B-MethodName
layer	O
allows	O
to	O
map	O
target	O
word	O
scores	O
into	O
target	O
word	O
probabilities	O
.	O
For	O
more	O
specific	O
details	O
about	O
the	O
architecture	O
,	O
refer	O
to	O
the	O
original	O
paper	O
.	O

Multi	O
-	O
source	O
translation	O
consists	O
in	O
exploiting	O
multiple	O
text	O
inputs	O
to	O
improve	O
NMT	O
(	O
Zoph	O
and	O
Knight	O
,	O
2016	O
)	O
.	O
In	O
our	O
case	O
,	O
we	O
are	O
using	O
this	O
approach	O
in	O
the	O
Transformer	B-MethodName
architecture	O
described	O
above	O
and	O
using	O
only	O
inputs	O
from	O
the	O
same	O
language	O
family	O
.	O

The	O
experimental	O
framework	O
is	O
the	O
Biomedical	O
Translation	B-TaskName
Task	O
(	O
WMT18	O
)	O
2	O
.	O
The	O
corpus	O
used	O
to	O
train	O
the	O
model	O
are	O
the	O
one	O
provided	O
for	O
the	O
task	O
for	O
the	O
selected	O
languages	O
pairs	O
:	O
Spanishto	O
-	O
English	O
(	O
es2en	O
)	O
,	O
French	O
-	O
to	O
-	O
English	O
(	O
fr2en	O
)	O
and	O
Portuguese	O
-	O
to	O
-	O
English	O
(	O
pt2en	O
)	O
.	O
Sources	O
are	O
mainly	O
from	O
Scielo	O
and	O
Medline	O
and	O
detailed	O
in	O
Table	O
3	O
.	O
Validation	O
sets	O
were	O
taken	O
from	O
Khresmoi	O
development	O
data	O
3	O
,	O
as	O
recommended	O
in	O
the	O
task	O
description	O
.	O
Each	O
validation	B-DatasetName
dataset	I-DatasetName
contains	O
500	O
sentence	O
pairs	O
.	O
Test	O
sets	O
were	O
the	O
ones	O
provides	O
by	O
the	O
task	O
for	O
the	O
previous	O
year	O
competition	O
(	O
WMT17	O
4	O
)	O
.	O
Preprocessing	O
relied	O
on	O
three	O
basic	O
steps	O
:	O
tokenization	O
,	O
truecasing	O
and	O
limiting	O
sentence	O
length	O
to	O
80	O
words	O
.	O
Words	O
were	O
segmented	O
by	O
means	O
of	O
Byte	O
-	O
Pair	O
Encoding	O
(	O
BPE	B-MethodName
)	O
(	O
Sennrich	O
et	O
al	O
,	O
2015	O
)	O
.	O

Neural	O
Relation	B-TaskName
Extraction	I-TaskName
for	O
Knowledge	O
Base	O
Enrichment	O

Inspired	B-DatasetName
by	O
the	O
work	O
of	O
Brin	O
(	O
1998	O
)	O
,	O
state	O
-	O
of	O
-	O
theart	O
methods	O
employ	O
distant	O
supervision	O
by	O
leveraging	O
seed	O
facts	O
from	O
an	O
existing	O
KG	O
(	O
Mintz	O
et	O
al	O
,	O
2009	O
;	O
Suchanek	O
et	O
al	O
,	O
2009	O
;	O
Carlson	O
et	O
al	O
,	O
2010	O
)	O
.	O
These	O
methods	O
learn	O
extraction	O
patterns	O
from	O
seed	O
facts	O
,	O
apply	O
the	O
patterns	O
to	O
extract	O
new	O
fact	O
candidates	O
,	O
iterate	O
this	O
principle	O
,	O
and	O
finally	O
use	O
statistical	O
inference	O
(	O
e.g.	O
,	O
a	O
classifier	O
)	O
for	O
reducing	O
the	O
false	O
positive	O
rate	O
.	O
Some	O
of	O
these	O
methods	O
hinge	O
on	O
the	O
assumption	O
that	O
the	O
co	O
-	O
occurrence	O
of	O
a	O
seed	O
fact	O
's	O
entities	O
in	O
the	O
same	O
sentence	O
is	O
an	O
indicator	O
of	O
expressing	O
a	O
semantic	O
relationship	O
between	O
the	O
entities	O
.	O
This	O
is	O
a	O
potential	O
source	O
of	O
wrong	O
labeling	O
.	O
Follow	O
-	O
up	O
studies	O
(	O
Hoffmann	O
et	O
al	O
,	O
2010	O
;	O
Riedel	O
et	O
al	O
,	O
2010Riedel	O
et	O
al	O
,	O
,	O
2013Surdeanu	O
et	O
al	O
,	O
2012	O
)	O
overcome	O
this	O
limitation	O
by	O
various	O
means	O
,	O
including	O
the	O
use	O
of	O
relation	O
-	O
specific	O
lexicons	O
and	O
latent	O
factor	O
models	O
.	O
Still	O
,	O
these	O
methods	O
treat	O
entities	O
by	O
their	O
surface	O
forms	O
and	O
disregard	O
their	O
mapping	O
to	O
existing	O
entities	O
in	O
the	O
KG	O
.	O
Suchanek	O
et	O
al	O
(	O
2009	O
)	O
and	O
Sa	O
et	O
al	O
(	O
2017	O
)	O
used	O
probabilistic	O
-	O
logical	O
inference	O
to	O
eliminate	O
false	O
positives	O
,	O
based	O
on	O
constraint	O
solving	O
or	O
Monte	O
Carlo	O
sampling	O
over	O
probabilistic	O
graphical	O
models	O
,	O
respectively	O
.	O
These	O
methods	O
integrate	O
entity	B-TaskName
linking	I-TaskName
(	O
i.e.	O
,	O
NED	O
)	O
into	O
their	O
models	O
.	O
However	O
,	O
both	O
have	O
high	O
computational	O
complexity	O
and	O
rely	O
on	O
modeling	O
constraints	O
and	O
appropriate	O
priors	O
.	O
Recent	O
studies	O
employ	O
neural	O
networks	O
to	O
learn	O
the	O
extraction	O
of	O
triples	O
.	O
Nguyen	O
and	O
Grish	O
-	O
None	O
of	O
these	O
neural	O
models	O
is	O
geared	O
for	O
KG	O
enrichment	O
,	O
as	O
the	O
canonicalization	O
of	O
entities	O
is	O
out	O
of	O
their	O
scope	O
.	O

Figure	O
1	O
illustrates	O
the	O
overall	O
solution	O
framework	O
.	O
Our	O
framework	O
consists	O
of	O
three	O
components	O
:	O
data	O
collection	O
module	O
,	O
embedding	O
module	O
,	O
and	O
neural	O
relation	B-TaskName
extraction	I-TaskName
module	O
.	O
In	O
the	O
data	O
collection	O
module	O
(	O
detailed	O
in	O
Section	O
3.2	O
)	O
,	O
we	O
align	O
known	O
triples	O
in	O
an	O
existing	O
KB	O
with	O
sentences	O
that	O
contain	O
such	O
triples	O
from	O
a	O
text	O
corpus	O
.	O
The	O
aligned	O
pairs	O
of	O
sentences	O
and	O
triples	O
will	O
later	O
be	O
used	O
as	O
the	O
training	O
data	O
in	O
our	O
neural	O
relation	B-TaskName
extraction	I-TaskName
module	O
.	O
This	O
alignment	O
is	O
done	O
by	O
distant	O
supervision	O
.	O
To	O
obtain	O
a	O
large	O
number	O
of	O
high	O
-	O
quality	O
alignments	O
,	O
we	O
augment	O
the	O
process	O
with	O
a	O
co	O
-	O
reference	O
resolution	O
to	O
extract	O
sentences	O
with	O
implicit	O
entity	O
names	O
,	O
which	O
enlarges	O
the	O
set	O
of	O
candidate	O
sentences	O
to	O
be	O
aligned	O
.	O
We	O
further	O
use	O
dictionary	O
based	O
paraphrase	O
detection	O
to	O
filter	O
sentences	O
that	O
do	O
not	O
express	O
any	O
relationships	O
between	O
entities	O
.	O
In	O
the	O
embedding	O
module	O
(	O
detailed	O
in	O
Section	O
3.3	O
)	O
,	O
we	O
propose	O
a	O
joint	O
learning	O
of	O
word	O
and	O
entity	B-TaskName
embeddings	I-TaskName
by	O
combining	O
skip	O
-	O
gram	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
to	O
compute	O
the	O
word	B-TaskName
embeddings	I-TaskName
and	O
TransE	B-MethodName
(	O
Bordes	O
et	O
al	O
,	O
2013	O
)	O
to	O
compute	O
the	O
entity	B-TaskName
embeddings	I-TaskName
.	O
The	O
objective	O
of	O
the	O
joint	O
learning	O
is	O
to	O
capture	O
the	O
similarity	O
of	O
words	O
and	O
entities	O
that	O
helps	O
map	O
the	O
entity	O
names	O
into	O
the	O
related	O
entity	O
IDs	O
.	O
Moreover	O
,	O
the	O
resulting	O
entity	B-TaskName
embeddings	I-TaskName
are	O
used	O
to	O
train	O
a	O
triple	O
classifier	O
that	O
helps	O
filter	O
invalid	O
triples	O
generated	O
by	O
our	O
neural	O
relation	B-TaskName
extraction	I-TaskName
model	O
.	O
In	O
the	O
neural	O
relation	B-TaskName
extraction	I-TaskName
module	O
(	O
detailed	O
in	O
Section	O
3.4	O
)	O
,	O
we	O
propose	O
an	O
n	O
-	O
gram	O
based	O
attention	O
model	O
by	O
expanding	O
the	O
attention	O
mechanism	O
to	O
the	O
n	O
-	O
gram	O
token	O
of	O
a	O
sentence	O
.	O
The	O
ngram	O
attention	O
computes	O
the	O
n	O
-	O
gram	O
combination	O
of	O
attention	O
weight	O
to	O
capture	O
the	O
verbal	O
or	O
noun	O
phrase	O
context	O
that	O
complements	O
the	O
word	O
level	O
attention	O
of	O
the	O
standard	O
attention	O
model	O
.	O
This	O
expansion	O
helps	O
our	O
model	O
to	O
better	O
capture	O
the	O
multi	O
-	O
word	O
context	O
of	O
entities	O
and	O
relationships	O
.	O
The	O
output	O
of	O
the	O
encoder	O
-	O
decoder	O
model	O
is	O
a	O
sequence	O
of	O
the	O
entity	O
and	O
predicate	O
IDs	O
where	O
every	O
three	O
IDs	O
indicate	O
a	O
triple	O
.	O
To	O
generate	O
highquality	O
triples	O
,	O
we	O
propose	O
two	O
strategies	O
.	O
The	O
first	O
strategy	O
uses	O
a	O
modified	O
beam	O
search	O
that	O
computes	O
the	O
lexical	O
similarity	O
of	O
the	O
extracted	O
entities	O
with	O
the	O
surface	O
form	O
of	O
entity	O
names	O
in	O
the	O
input	O
sentence	O
to	O
ensure	O
the	O
correct	O
entity	O
prediction	O
.	O
The	O
second	O
strategy	O
uses	O
a	O
triple	O
classifier	O
that	O
is	O
trained	O
using	O
the	O
entity	B-TaskName
embeddings	I-TaskName
from	O
the	O
joint	O
learning	O
to	O
filter	O
the	O
invalid	O
triples	O
.	O
The	O
triple	O
generation	O
process	O
is	O
detailed	O
in	O
Section	O
3.5	O

We	O
aim	O
to	O
extract	O
triples	O
from	O
a	O
sentence	O
for	O
KB	O
enrichment	O
by	O
proposing	O
a	O
supervised	O
relation	B-TaskName
extraction	I-TaskName
model	O
.	O
To	O
train	O
such	O
a	O
model	O
,	O
we	O
need	O
a	O
large	O
volume	O
of	O
fully	O
labeled	O
training	O
data	O
in	O
the	O
form	O
of	O
sentence	O
-	O
triple	O
pairs	O
.	O
Following	O
Sorokin	O
and	O
Gurevych	O
(	O
2017	O
)	O
,	O
we	O
use	O
distant	O
supervision	O
(	O
Mintz	O
et	O
al	O
,	O
2009	O
)	O
to	O
align	O
sentences	O
in	O
Wikipedia	O
1	O
with	O
triples	O
in	O
Wikidata	O
2	O
(	O
Vrandecic	O
and	O
Krötzsch	O
,	O
2014	O
)	O
.	O
We	O
map	O
an	O
entity	O
mention	O
in	O
a	O
sentence	O
to	O
the	O
corresponding	O
entity	O
entry	O
(	O
i.e.	O
,	O
Wikidata	O
ID	O
)	O
in	O
Wikidata	O
via	O
the	O
hyperlink	O
associated	O
to	O
the	O
entity	O
mention	O
,	O
which	O
is	O
recorded	O
in	O
Wikidata	O
as	O
the	O
url	O
property	O
of	O
the	O
entity	O
entry	O
.	O
Each	O
pair	O
may	O
contain	O
one	O
sentence	O
and	O
multiple	O
triples	O
.	O
We	O
sort	O
the	O
order	O
of	O
the	O
triples	O
based	O
on	O
the	O
order	O
of	O
the	O
predicate	O
paraphrases	O
that	O
indicate	O
the	O
relationships	O
between	O
entities	O
in	O
the	O
sentence	O
.	O
We	O
collect	O
sentence	O
-	O
triple	O
pairs	O
by	O
extracting	O
sentences	O
that	O
contain	O
both	O
head	O
and	O
tail	O
entities	O
of	O
Wikidata	O
triples	O
.	O
To	O
generate	O
high	O
-	O
quality	O
sentence	O
-	O
triple	O
pairs	O
,	O
we	O
propose	O
two	O
additional	O
steps	O
:	O
(	O
1	O
)	O
extracting	O
sentences	O
that	O
contain	O
implicit	O
entity	O
names	O
using	O
co	O
-	O
reference	O
resolution	O
,	O
and	O
(	O
2	O
)	O
filtering	O
sentences	O
that	O
do	O
not	O
express	O
any	O
relationships	O
using	O
paraphrase	O
detection	O
.	O
We	O
detail	O
these	O
steps	O
below	O
.	O
Prior	O
to	O
aligning	O
the	O
sentences	O
with	O
triples	O
,	O
in	O
Step	O
(	O
1	O
)	O
,	O
we	O
find	O
the	O
implicit	O
entity	O
names	O
to	O
increase	O
the	O
number	O
of	O
candidate	O
sentences	O
to	O
be	O
aligned	O
.	O
We	O
apply	O
co	O
-	O
reference	O
resolution	O
(	O
Clark	O
and	O
Manning	O
,	O
2016	O
)	O
to	O
each	O
paragraph	O
in	O
a	O
Wikipedia	O
article	O
and	O
replace	O
the	O
extracted	O
co	O
-	O
references	O
with	O
the	O
proper	O
entity	O
name	O
.	O
We	O
observe	O
that	O
the	O
first	O
sentence	O
of	O
a	O
paragraph	O
in	O
a	O
Wikipedia	O
article	O
may	O
contain	O
a	O
pronoun	O
that	O
refers	O
to	O
the	O
main	O
entity	O
.	O
For	O
example	O
,	O
there	O
is	O
a	O
paragraph	O
in	O
the	O
Barack	O
Obama	O
article	O
that	O
starts	O
with	O
a	O
sentence	O
"	O
He	O
was	O
reelected	O
to	O
the	O
Illinois	O
Senate	O
in	O
1998	O
"	O
.	O
This	O
may	O
cause	O
the	O
standard	O
co	O
-	O
reference	O
resolution	O
to	O
miss	O
the	O
implicit	O
entity	O
names	O
for	O
the	O
rest	O
of	O
the	O
paragraph	O
.	O
To	O
address	O
this	O
problem	O
,	O
we	O
heuristically	O
replace	O
the	O
pronouns	O
in	O
the	O
first	O
sentence	O
of	O
a	O
paragraph	O
if	O
the	O
main	O
entity	O
name	O
of	O
the	O
Wikipedia	O
page	O
is	O
not	O
mentioned	O
.	O
For	O
the	O
sentence	O
in	O
the	O
previous	O
example	O
,	O
we	O
replace	O
"	O
He	O
"	O
with	O
"	O
Barack	O
Obama	O
"	O
.	O
The	O
intuition	O
is	O
that	O
a	O
Wikipedia	O
article	O
contains	O
content	O
of	O
a	O
single	O
entity	O
of	O
interest	O
,	O
and	O
that	O
the	O
pronouns	O
mentioned	O
in	O
the	O
first	O
sentence	O
of	O
a	O
paragraph	O
mostly	O
relate	O
to	O
the	O
main	O
entity	O
.	O
In	O
Step	O
(	O
2	O
)	O
,	O
we	O
use	O
a	O
dictionary	O
based	O
paraphrase	O
detection	O
to	O
capture	O
relationships	O
between	O
entities	O
in	O
a	O
sentence	O
.	O
First	O
,	O
we	O
create	O
a	O
dictionary	O
by	O
populating	O
predicate	O
paraphrases	O
from	O
three	O
sources	O
including	O
PATTY	O
(	O
Nakashole	O
et	O
al	O
,	O
2012	O
)	O
,	O
POLY	O
(	O
Grycner	O
and	O
Weikum	O
,	O
2016	O
)	O
,	O
and	O
PPDB	O
(	O
Ganitkevitch	O
et	O
al	O
,	O
2013	O
)	O
ship	O
"	O
place	O
of	O
birth	O
"	O
are	O
{	O
born	O
in	O
,	O
was	O
born	O
in	O
,	O
...	O
}	O
.	O
Then	O
,	O
we	O
use	O
this	O
dictionary	O
to	O
filter	O
sentences	O
that	O
do	O
not	O
express	O
any	O
relationships	O
between	O
entities	O
.	O
We	O
use	O
exact	O
string	O
matching	O
to	O
find	O
verbal	O
or	O
noun	O
phrases	O
in	O
a	O
sentence	O
which	O
is	O
a	O
paraphrases	O
of	O
a	O
predicate	O
of	O
a	O
triple	O
.	O
For	O
example	O
,	O
for	O
the	O
triple	O
Barack	O
Obama	O
,	O
place	O
of	O
birth	O
,	O
Honolulu	O
,	O
the	O
sentence	O
"	O
Barack	O
Obama	O
was	O
born	O
in	O
1961	O
in	O
Honolulu	O
,	O
Hawaii	O
"	O
will	O
be	O
retained	O
while	O
the	O
sentence	O
"	O
Barack	O
Obama	O
visited	O
Honolulu	O
in	O
2010	O
"	O
will	O
be	O
removed	O
(	O
the	O
sentence	O
may	O
be	O
retained	O
if	O
there	O
is	O
another	O
valid	O
triple	O
Barack	O
Obama	O
,	O
visited	O
,	O
Honolulu	O
)	O
.	O
This	O
helps	O
filter	O
noises	O
for	O
the	O
sentence	O
-	O
triple	O
alignment	O
.	O
The	O
collected	O
dataset	O
contains	O
255	O
,	O
654	O
sentence	O
-	O
triple	O
pairs	O
.	O
For	O
each	O
pair	O
,	O
the	O
maximum	O
number	O
of	O
triples	O
is	O
four	O
(	O
i.e.	O
,	O
a	O
sentence	O
can	O
produce	O
at	O
most	O
four	O
triples	O
)	O
.	O
We	O
split	O
the	O
dataset	O
into	O
train	O
set	O
(	O
80	O
%	O
)	O
,	O
dev	O
set	O
(	O
10	O
%	O
)	O
and	O
test	O
set	O
(	O
10	O
%	O
)	O
(	O
we	O
call	O
it	O
the	O
WIKI	O
test	O
dataset	O
)	O
.	O
For	O
stress	O
testing	O
(	O
to	O
test	O
the	O
proposed	O
model	O
on	O
a	O
different	O
style	O
of	O
text	O
than	O
the	O
training	O
data	O
)	O
,	O
we	O
also	O
collect	O
another	O
test	O
dataset	O
outside	O
Wikipedia	O
.	O
We	O
apply	O
the	O
same	O
procedure	O
to	O
the	O
user	O
reviews	O
of	O
a	O
travel	O
website	O
.	O
First	O
,	O
we	O
collect	O
user	O
reviews	O
on	O
100	O
popular	O
landmarks	O
in	O
Australia	O
.	O
Then	O
,	O
we	O
apply	O
the	O
adapted	O
distant	O
supervision	O
to	O
the	O
reviews	O
and	O
collect	O
1	O
,	O
000	O
sentence	O
-	O
triple	O
pairs	O
(	O
we	O
call	O
it	O
the	O
GEO	O
test	O
dataset	O
)	O
.	O
Table	O
2	O
summarizes	O
the	O
statistics	O
of	O
our	O
datasets	O
.	O

We	O
compare	O
our	O
proposed	O
model	O
3	O
with	O
three	O
existing	O
models	O
including	O
CNN	O
(	O
the	O
state	O
-	O
of	O
-	O
theart	O
supervised	O
approach	O
by	O
Lin	O
et	O
al	O
(	O
2016	O
)	O
)	O
,	O
MiniE	O
(	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
unsupervised	O
approach	O
by	O
Gashteovski	O
et	O
al	O
(	O
2017	O
)	O
)	O
,	O
and	O
ClausIE	O
by	O
Corro	O
and	O
Gemulla	O
(	O
2013	O
)	O
.	O
To	O
map	O
the	O
extracted	O
entities	O
by	O
these	O
models	O
,	O
we	O
use	O
two	O
state	O
-	O
of	O
-	O
theart	O
NED	O
systems	O
including	O
AIDA	O
(	O
Hoffart	O
et	O
al	O
,	O
2011	O
)	O
and	O
NeuralEL	O
(	O
Kolitsas	O
et	O
al	O
,	O
2018	O
)	O
.	O
The	O
precision	O
(	O
tested	O
on	O
our	O
test	O
dataset	O
)	O
of	O
AIDA	O
and	O
NeuralEL	O
are	O
70	O
%	O
and	O
61	O
%	O
respectively	O
.	O
To	O
map	O
the	O
extracted	O
predicates	O
(	O
relationships	O
)	O
of	O
the	O
unsupervised	O
approaches	O
output	O
,	O
we	O
use	O
the	O
dictionary	O
based	O
paraphrase	O
detection	O
.	O
We	O
use	O
the	O
same	O
dictionary	O
that	O
is	O
used	O
to	O
collect	O
the	O
dataset	O
(	O
i.e.	O
,	O
the	O
combination	O
of	O
three	O
paraphrase	O
dictionaries	O
including	O
PATTY	O
(	O
Nakashole	O
et	O
al	O
,	O
2012	O
)	O
,	O
POLY	O
(	O
Grycner	O
and	O
Weikum	O
,	O
2016	O
)	O
,	O
and	O
PPDB	O
(	O
Ganitkevitch	O
et	O
al	O
,	O
2013	O
)	O
)	O
.	O
We	O
replace	O
the	O
extracted	O
predicate	O
with	O
the	O
correct	O
predicate	O
ID	O
if	O
one	O
of	O
the	O
paraphrases	O
of	O
the	O
correct	O
predicate	O
(	O
i.e.	O
,	O
the	O
gold	O
standard	O
)	O
appear	O
in	O
the	O
extracted	O
predicate	O
.	O
Otherwise	O
,	O
we	O
replace	O
the	O
extracted	O
predicate	O
with	O
"	O
NA	O
"	O
to	O
indicate	O
an	O
unrecognized	O
predicate	O
.	O
We	O
also	O
compare	O
our	O
N	O
-	O
gram	O
Attention	O
model	O
with	O
two	O
encoder	O
-	O
decoder	O
based	O
models	O
including	O
the	O
Single	O
Attention	O
model	O
(	O
Bahdanau	O
et	O
al	O
,	O
2015	O
)	O
and	O
Transformer	B-MethodName
model	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O

CODEX	O
:	O
A	O
Comprehensive	O
Knowledge	B-TaskName
Graph	I-TaskName
Completion	I-TaskName
Benchmark	O

We	O
present	O
CODEX	O
,	O
a	O
set	O
of	O
knowledge	B-TaskName
graph	I-TaskName
COmpletion	I-TaskName
Datasets	O
EXtracted	O
from	O
Wikidata	O
and	O
Wikipedia	O
that	O
improve	O
upon	O
existing	O
knowledge	B-TaskName
graph	I-TaskName
completion	I-TaskName
benchmarks	O
in	O
scope	O
and	O
level	O
of	O
difficulty	O
.	O
In	O
terms	O
of	O
scope	O
,	O
CODEX	O
comprises	O
three	O
knowledge	B-TaskName
graphs	I-TaskName
varying	O
in	O
size	O
and	O
structure	O
,	O
multilingual	O
descriptions	O
of	O
entities	O
and	O
relations	O
,	O
and	O
tens	O
of	O
thousands	O
of	O
hard	O
negative	O
triples	O
that	O
are	O
plausible	O
but	O
verified	O
to	O
be	O
false	O
.	O
To	O
characterize	O
CODEX	O
,	O
we	O
contribute	O
thorough	O
empirical	O
analyses	O
and	O
benchmarking	O
experiments	O
.	O
First	O
,	O
we	O
analyze	O
each	O
CODEX	O
dataset	O
in	O
terms	O
of	O
logical	O
relation	O
patterns	O
.	O
Next	O
,	O
we	O
report	O
baseline	O
link	B-TaskName
prediction	I-TaskName
and	O
triple	B-TaskName
classification	I-TaskName
results	O
on	O
CODEX	O
for	O
five	O
extensively	O
tuned	O
embedding	O
models	O
.	O
Finally	O
,	O
we	O
differentiate	O
CODEX	O
from	O
the	O
popular	O
FB15	O
K	O
-	O
237	O
knowledge	B-TaskName
graph	I-TaskName
completion	I-TaskName
dataset	O
by	O
showing	O
that	O
CODEX	O
covers	O
more	O
diverse	O
and	O
interpretable	O
content	O
,	O
and	O
is	O
a	O
more	O
difficult	O
link	B-TaskName
prediction	I-TaskName
benchmark	O
.	O
Data	O
,	O
code	O
,	O
and	O
pretrained	O
models	O
are	O
available	O
at	O
https://bit.ly/2EPbrJs	O
.	O

Knowledge	B-TaskName
graphs	I-TaskName
are	O
multi	O
-	O
relational	O
graphs	O
that	O
express	O
facts	O
about	O
the	O
world	O
by	O
connecting	O
entities	O
(	O
people	O
,	O
places	O
,	O
things	O
,	O
concepts	O
)	O
via	O
different	O
types	O
of	O
relationships	O
.	O
The	O
field	O
of	O
automatic	O
knowledge	B-TaskName
graph	I-TaskName
completion	I-TaskName
(	O
KGC	O
)	O
,	O
which	O
is	O
motivated	O
by	O
the	O
fact	O
that	O
knowledge	B-TaskName
graphs	I-TaskName
are	O
usually	O
incomplete	O
,	O
is	O
an	O
active	O
research	O
direction	O
spanning	O
several	O
subfields	O
of	O
artificial	O
intelligence	O
(	O
Nickel	O
et	O
al	O
,	O
2015	O
;	O
Ji	O
et	O
al	O
,	O
2020	O
)	O
.	O
As	O
progress	O
in	O
artificial	O
intelligence	O
depends	O
heavily	O
on	O
data	O
,	O
a	O
relevant	O
and	O
high	O
-	O
quality	O
benchmark	O
is	O
imperative	O
to	O
evaluating	O
and	O
advancing	O
the	O
state	O
of	O
the	O
art	O
in	O
KGC	O
.	O
However	O
,	O
the	O
field	O
has	O
largely	O
remained	O
static	O
in	O
this	O
regard	O
over	O
the	O
past	O
decade	O
.	O
Outdated	O
subsets	O
of	O
Freebase	O
(	O
Bollacker	O
et	O
al	O
,	O
2008	O
)	O
are	O
most	O
commonly	O
used	O
for	O
evaluation	O
in	O
KGC	O
,	O
even	O
though	O
Freebase	O
had	O
known	O
quality	O
issues	O
(	O
Tanon	O
et	O
al	O
,	O
2016	O
)	O
and	O
was	O
eventually	O
deprecated	O
in	O
favor	O
of	O
the	O
more	O
recent	O
Wikidata	O
knowledge	O
base	O
(	O
Vrandečić	O
and	O
Krötzsch	O
,	O
2014	O
)	O
.	O
Indeed	O
,	O
KGC	O
benchmarks	O
extracted	O
from	O
Freebase	O
like	O
FB15	O
K	O
and	O
FB15	O
K	O
-	O
237	O
(	O
Bordes	O
et	O
al	O
,	O
2013	O
;	O
are	O
questionable	O
in	O
quality	O
.	O
For	O
example	O
,	O
FB15	O
K	O
was	O
shown	O
to	O
have	O
train	O
/	O
test	O
leakage	O
.	O
Later	O
in	O
this	O
paper	O
(	O
6.2	O
)	O
,	O
we	O
will	O
show	O
that	O
a	O
relatively	O
large	O
proportion	O
of	O
relations	O
in	O
FB15	O
K	O
-	O
237	O
can	O
be	O
covered	O
by	O
a	O
trivial	O
frequency	O
rule	O
.	O
To	O
address	O
the	O
need	O
for	O
a	O
solid	O
benchmark	O
in	O
KGC	O
,	O
we	O
present	O
CODEX	O
,	O
a	O
set	O
of	O
knowledge	B-TaskName
graph	I-TaskName
COmpletion	I-TaskName
Datasets	O
EXtracted	O
from	O
Wikidata	O
and	O
its	O
sister	O
project	O
Wikipedia	O
.	O
Inasmuch	O
as	O
Wikidata	O
is	O
considered	O
the	O
successor	O
of	O
Freebase	O
,	O
CODEX	O
improves	O
upon	O
existing	O
Freebase	O
-	O
based	O
KGC	O
benchmarks	O
in	O
terms	O
of	O
scope	O
and	O
level	O
of	O
difficulty	O
(	O
Table	O
1	O
)	O
.	O
Our	O
contributions	O
include	O
:	O
Foundations	O
We	O
survey	O
evaluation	O
datasets	O
in	O
encyclopedic	O
knowledge	B-TaskName
graph	I-TaskName
completion	I-TaskName
to	O
motivate	O
a	O
new	O
benchmark	O
(	O
2	O
and	O
Appendix	O
A	O
)	O
.	O
Data	O
We	O
introduce	O
CODEX	O
,	O
a	O
benchmark	O
consisting	O
of	O
three	O
knowledge	B-TaskName
graphs	I-TaskName
varying	O
in	O
size	O
and	O
structure	O
,	O
entity	O
types	O
,	O
multilingual	O
labels	O
and	O
descriptions	O
,	O
and	O
-	O
unique	O
to	O
CODEX	O
-	O
manually	O
verified	O
hard	O
negative	O
triples	O
(	O
3	O
)	O
.	O
To	O
better	O
understand	O
CODEX	O
,	O
we	O
analyze	O
the	O
logical	O
relation	O
patterns	O
in	O
each	O
of	O
its	O
datasets	O
(	O
4	O
)	O
.	O
Benchmarking	O
We	O
conduct	O
large	O
-	O
scale	O
model	B-TaskName
selection	I-TaskName
and	O
benchmarking	O
experiments	O
,	O
reporting	O
baseline	O
link	B-TaskName
prediction	I-TaskName
and	O
triple	B-TaskName
classification	I-TaskName
results	O
on	O
CODEX	O
for	O
five	O
widely	O
used	O
embedding	O
models	O
from	O
different	O
architectural	O
classes	O
(	O
5	O
)	O
.	O
Comparative	O
analysis	O
Finally	O
,	O
to	O
demonstrate	O
the	O
unique	O
value	O
of	O
CODEX	O
,	O
we	O
differentiate	O
Multi	O
-	O
domain	O
,	O
with	O
a	O
strong	O
focus	O
on	O
awards	O
,	O
entertainment	O
,	O
and	O
sports	O
(	O
6.1	O
and	O
Appendix	O
E	O
)	O
Multi	O
-	O
domain	O
,	O
with	O
focuses	O
on	O
writing	O
,	O
entertainment	O
,	O
music	O
,	O
politics	O
,	O
journalism	O
,	O
academics	O
,	O
and	O
science	O
(	O
6.1	O
and	O
Appendix	O
E	O
)	O
Scope	O
(	O
auxiliary	O
data	O
)	O
Various	O
decentralized	O
versions	O
of	O
FB15	O
K	O
with	O
,	O
e.g.	O
,	O
entity	O
types	O
(	O
Xie	O
et	O
al	O
,	O
2016	O
)	O
,	O
sampled	O
negatives	O
(	O
Socher	O
et	O
al	O
,	O
2013	O
)	O
,	O
and	O
more	O
(	O
Table	O
8	O
)	O
Centralized	O
repository	O
of	O
three	O
datasets	O
with	O
entity	O
types	O
,	O
multilingual	O
text	O
,	O
and	O
manually	O
annotated	O
hard	O
negatives	O
(	O
3	O
)	O
Level	O
of	O
difficulty	O
FB15	O
K	O
has	O
severe	O
train	O
/	O
test	O
leakage	O
from	O
inverse	O
relations	O
;	O
while	O
removal	O
of	O
inverse	O
relations	O
makes	O
FB15	O
K	O
-	O
237	O
harder	O
than	O
FB15	O
K	O
,	O
FB15	O
K	O
-	O
237	O
still	O
has	O
a	O
high	O
proportion	O
of	O
easy	O
-	O
to	O
-	O
predict	O
relational	O
patterns	O
(	O
6.2	O
)	O
Inverse	O
relations	O
removed	O
from	O
all	O
datasets	O
to	O
avoid	O
train	O
/	O
test	O
leakage	O
(	O
3.2	O
)	O
;	O
manually	O
annotated	O
hard	O
negatives	O
for	O
the	O
task	O
of	O
triple	B-TaskName
classification	I-TaskName
(	O
3.4	O
)	O
;	O
few	O
trivial	O
patterns	O
for	O
the	O
task	O
of	O
link	B-TaskName
prediction	I-TaskName
(	O
6.2	O
)	O
CODEX	O
from	O
FB15	O
K	O
-	O
237	O
in	O
terms	O
of	O
both	O
content	O
and	O
difficulty	O
(	O
6	O
)	O
.	O
We	O
show	O
that	O
CODEX	O
covers	O
more	O
diverse	O
and	O
interpretable	O
content	O
,	O
and	O
is	O
a	O
more	O
challenging	O
link	B-TaskName
prediction	I-TaskName
benchmark	O
.	O

In	O
addition	O
to	O
large	O
encyclopedic	O
knowledge	B-TaskName
graphs	I-TaskName
,	O
it	O
is	O
common	O
to	O
evaluate	O
KGC	O
methods	O
on	O
at	O
least	O
one	O
smaller	O
,	O
domain	O
-	O
specific	O
dataset	O
,	O
typically	O
drawn	O
from	O
the	O
WordNet	O
semantic	O
network	O
(	O
Miller	O
,	O
1998	O
;	O
Bordes	O
et	O
al	O
,	O
2013	O
)	O
.	O
Other	O
choices	O
include	O
the	O
Unified	O
Medical	O
Language	O
System	O
(	O
UMLS	B-DatasetName
)	O
database	O
(	O
McCray	O
,	O
2003	O
)	O
,	O
the	O
Alyawarra	O
kinship	O
dataset	O
(	O
Kemp	O
et	O
al	O
,	O
2006	O
)	O
,	O
the	O
Countries	O
dataset	O
(	O
Bouchard	O
et	O
al	O
,	O
2015	O
)	O
,	O
and	O
variants	O
of	O
a	O
synthetic	O
"	O
family	O
tree	O
"	O
(	O
Hinton	O
,	O
1986	O
)	O
.	O
As	O
our	O
focus	O
in	O
this	O
paper	O
is	O
encyclopedic	O
knowledge	O
,	O
we	O
do	O
not	O
cover	O
these	O
datasets	O
further	O
.	O
(	O
3.4	O
)	O
.	O
We	O
compute	O
multilingual	O
coverage	O
over	O
all	O
labels	O
,	O
descriptions	O
,	O
and	O
entity	O
Wikipedia	O
extracts	O
successfully	O
retrieved	O
for	O
the	O
respective	O
dataset	O
in	O
Arabic	O
(	O
ar	O
)	O
,	O
German	O
(	O
de	O
)	O
,	O
English	O
(	O
en	O
)	O
,	O
Spanish	O
(	O
es	O
)	O
,	O
Russian	O
(	O
ru	O
)	O
,	O
and	O
Chinese	O
(	O
zh	O
)	O
.	O

We	O
collected	O
an	O
initial	O
set	O
of	O
triples	O
using	O
a	O
type	O
of	O
snowball	O
sampling	O
(	O
Goodman	O
,	O
1961	O
)	O
.	O
We	O
first	O
manually	O
defined	O
a	O
broad	O
seed	O
set	O
of	O
entity	O
and	O
relation	O
types	O
common	O
to	O
13	O
domains	O
:	O
Business	O
,	O
geography	O
,	O
literature	O
,	O
media	O
and	O
entertainment	O
,	O
medicine	O
,	O
music	O
,	O
news	O
,	O
politics	O
,	O
religion	O
,	O
science	O
,	O
sports	O
,	O
travel	O
,	O
and	O
visual	O
art	O
.	O
Examples	O
of	O
seed	O
entity	O
types	O
include	O
airline	O
,	O
journalist	O
,	O
and	O
religious	O
text	O
;	O
corresponding	O
seed	O
relation	O
types	O
in	O
each	O
respective	O
domain	O
include	O
airline	O
alliance	O
,	O
notable	O
works	O
,	O
and	O
language	O
of	O
work	O
or	O
name	O
.	O
Table	O
9	O
in	O
Appendix	O
B	O
gives	O
all	O
seed	O
entity	O
and	O
relation	O
types	O
.	O
Using	O
these	O
seeds	B-DatasetName
,	O
we	O
retrieved	O
an	O
initial	O
set	O
of	O
380	O
,	O
038	O
entities	O
,	O
75	O
relations	O
,	O
and	O
1	O
,	O
156	O
,	O
222	O
triples	O
by	O
querying	O
Wikidata	O
for	O
statements	O
of	O
the	O
form	O
(	O
head	O
entity	O
of	O
seed	O
type	O
,	O
seed	O
relation	O
type	O
,	O
?	O
)	O
.	O

Knowledge	B-TaskName
graphs	I-TaskName
are	O
unique	O
in	O
that	O
they	O
only	O
contain	O
positive	O
statements	O
,	O
meaning	O
that	O
triples	O
not	O
observed	O
in	O
a	O
given	O
knowledge	O
graph	O
are	O
not	O
necessarily	O
false	O
,	O
but	O
merely	O
unseen	O
;	O
this	O
is	O
called	O
the	O
Open	O
World	O
Assumption	O
(	O
Galárraga	O
et	O
al	O
,	O
2013	O
)	O
.	O
However	O
,	O
most	O
machine	O
learning	O
tasks	O
on	O
knowledge	B-TaskName
graphs	I-TaskName
require	O
negatives	O
in	O
some	O
capacity	O
.	O
While	O
different	O
negative	O
sampling	O
strategies	O
exist	O
(	O
Cai	O
and	O
Wang	O
,	O
2018	O
)	O
,	O
the	O
most	O
common	O
approach	O
is	O
to	O
randomly	O
perturb	O
observed	O
triples	O
to	O
generate	O
negatives	O
,	O
following	O
Bordes	O
et	O
al	O
(	O
2013	O
)	O
.	O
While	O
random	O
negative	O
sampling	O
is	O
beneficial	O
and	O
even	O
necessary	O
in	O
the	O
case	O
where	O
a	O
large	O
number	O
of	O
negatives	O
is	O
needed	O
(	O
i.e.	O
,	O
training	O
)	O
,	O
it	O
is	O
not	O
necessarily	O
useful	O
for	O
evaluation	O
.	O
For	O
example	O
,	O
in	O
the	O
task	O
of	O
triple	B-TaskName
classification	I-TaskName
,	O
the	O
goal	O
is	O
to	O
discriminate	O
between	O
positive	O
(	O
true	O
)	O
and	O
negative	O
(	O
false	O
)	O
triples	O
.	O
As	O
we	O
show	O
in	O
5.5	O
,	O
triple	B-TaskName
classification	I-TaskName
over	O
randomly	O
generated	O
negatives	O
is	O
trivially	O
easy	O
for	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
because	O
random	O
negatives	O
are	O
generally	O
not	O
meaningful	O
or	O
plausible	O
.	O
Therefore	O
,	O
we	O
generate	O
and	O
manually	O
evaluate	O
hard	O
negatives	O
for	O
KGC	O
evaluation	O
.	O
Generation	O
To	O
generate	O
hard	O
negatives	O
,	O
we	O
used	O
each	O
pre	O
-	O
trained	O
embedding	O
model	O
from	O
5.2	O
to	O
predict	O
tail	O
entities	O
of	O
triples	O
in	O
CODEX	O
.	O
For	O
each	O
model	O
,	O
we	O
took	O
as	O
candidate	O
negatives	O
the	O
triples	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
for	O
which	O
(	O
i	O
)	O
the	O
type	O
of	O
the	O
predicted	O
tail	O
entityt	O
matched	O
the	O
type	O
of	O
the	O
true	O
tail	O
entity	O
t	O
;	O
(	O
ii	O
)	O
t	O
was	O
ranked	O
in	O
the	O
top	O
-	O
10	O
predictions	O
by	O
that	O
model	O
;	O
and	O
(	O
iii	O
)	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
was	O
not	O
observed	O
in	O
G.	O

To	O
give	O
an	O
idea	O
of	O
the	O
types	O
of	O
reasoning	O
necessary	O
for	O
models	O
to	O
perform	O
well	O
on	O
CODEX	O
,	O
we	O
analyze	O
the	O
presence	O
of	O
learnable	O
binary	O
relation	O
patterns	O
within	O
CODEX	O
.	O
The	O
three	O
main	O
types	O
of	O
such	O
patterns	O
in	O
knowledge	B-TaskName
graphs	I-TaskName
are	O
symmetry	O
,	O
inversion	O
,	O
and	O
compositionality	O
(	O
Trouillon	O
et	O
al	O
,	O
2019	O
;	O
.	O
We	O
address	O
symmetry	O
and	O
compositionality	O
here	O
,	O
and	O
omit	O
inversion	O
because	O
we	O
specifically	O
removed	O
inverse	O
relations	O
to	O
avoid	O
train	O
/	O
test	O
leakage	O
(	O
3.2	O
)	O
.	O

Compositionality	O
captures	O
path	O
rules	O
of	O
the	O
form	O
(	O
h	O
,	O
r	O
1	O
,	O
x	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
x	O
n	O
,	O
r	O
n	O
,	O
t	O
)	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
.	O
To	O
learn	O
these	O
rules	O
,	O
models	O
must	O
be	O
capable	O
of	O
"	O
multi	O
-	O
hop	O
"	O
reasoning	O
on	O
knowledge	B-TaskName
graphs	I-TaskName
(	O
Guu	O
et	O
al	O
,	O
2015	O
)	O
.	O
To	O
identify	O
compositional	O
paths	O
,	O
we	O
use	O
the	O
AMIE3	O
system	O
(	O
Lajus	O
et	O
al	O
,	O
2020	O
)	O
,	O
which	O
outputs	O
rules	O
with	O
confidence	O
scores	O
that	O
capture	O
how	O
many	O
times	O
those	O
rules	O
are	O
seen	O
versus	O
violated	O
,	O
to	O
identify	O
paths	O
of	O
lengths	O
two	O
and	O
three	O
;	O
we	O
omit	O
longer	O
paths	O
as	O
they	O
are	O
relatively	O
costly	O
to	O
compute	O
.	O
We	O
identify	O
26	O
,	O
44	O
,	O
and	O
93	O
rules	O
in	O
CODEX	O
-	O
S	O
,	O
CODEX	O
-	O
M	O
,	O
and	O
CODEX	O
-	O
L	O
,	O
respectively	O
,	O
with	O
average	O
confidence	O
(	O
out	O
of	O
1	O
)	O
of	O
0.630	O
,	O
0.556	O
,	O
and	O
0.459	O
.	O
Table	O
4	O
gives	O
the	O
percentage	O
of	O
triples	O
per	O
dataset	O
participating	O
in	O
a	O
discovered	O
rule	O
.	O
Evidently	O
,	O
composition	O
is	O
especially	O
prevalent	O
in	O
CODEX	O
-	O
L.	O
An	O
example	O
rule	O
in	O
CODEX	O
-	O
L	O
is	O
"	O
if	O
X	O
was	O
founded	O
by	O
Y	O
,	O
and	O
Y	O
's	O
country	O
of	O
citizenship	O
is	O
Z	O
,	O
then	O
the	O
country	O
[	O
i.e.	O
,	O
of	O
origin	O
]	O
of	O
X	O
is	O
Z	O
"	O
(	O
confidence	O
0.709	O
)	O
.	O
We	O
release	O
these	O
rules	O
as	O
part	O
of	O
CODEX	O
for	O
further	O
development	O
of	O
KGC	O
methodologies	O
that	O
incorporate	O
or	O
learn	O
rules	O
.	O

Next	O
,	O
we	O
benchmark	O
performance	O
on	O
CODEX	O
for	O
the	O
tasks	O
of	O
link	B-TaskName
prediction	I-TaskName
and	O
triple	B-TaskName
classification	I-TaskName
.	O
To	O
ensure	O
that	O
models	O
are	O
fairly	O
and	O
accurately	O
compared	O
,	O
we	O
follow	O
Ruffinelli	O
et	O
al	O
(	O
2020	O
)	O
,	O
who	O
conducted	O
what	O
is	O
(	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
)	O
the	O
largest	O
-	O
scale	O
hyperparameter	O
tuning	O
study	O
of	O
knowledge	B-TaskName
graph	I-TaskName
embeddings	I-TaskName
to	O
date	O
.	O
Note	O
that	O
CODEX	O
can	O
be	O
used	O
to	O
evaluate	O
any	O
type	O
of	O
KGC	O
method	O
.	O
However	O
,	O
we	O
focus	O
on	O
embeddings	O
in	O
this	O
section	O
due	O
to	O
their	O
widespread	O
usage	O
in	O
modern	O
NLP	O
(	O
Ji	O
et	O
al	O
,	O
2020	O
)	O
.	O

We	O
compare	O
the	O
following	O
embedding	O
methods	O
:	O
RESCAL	B-MethodName
(	O
Nickel	O
et	O
al	O
,	O
2011	O
)	O
,	O
TransE	B-MethodName
(	O
Bordes	O
et	O
al	O
,	O
2013	O
)	O
,	O
ComplEx	O
(	O
Trouillon	O
et	O
al	O
,	O
2016	O
)	O
,	O
ConvE	O
(	O
Dettmers	O
et	O
al	O
,	O
2018	O
)	O
,	O
andTuckER	O
(	O
Balazevic	O
et	O
al	O
,	O
2019b	O
)	O
.	O
These	O
models	O
represent	O
several	O
classes	O
of	O
architecture	O
,	O
from	O
linear	O
(	O
RESCAL	B-MethodName
,	O
TuckER	B-MethodName
,	O
ComplEx	O
)	O
to	O
translational	O
(	O
TransE	B-MethodName
)	O
to	O
nonlinear	O
/	O
learned	O
(	O
ConvE	O
)	O
.	O
Appendix	O
D	O
provides	O
more	O
specifics	O
on	O
each	O
model	O
.	O

We	O
first	O
compare	O
the	O
content	O
in	O
CODEX	O
-	O
M	O
,	O
which	O
is	O
extracted	O
from	O
Wikidata	O
,	O
with	O
that	O
of	O
FB15	O
K	O
-	O
237	O
,	O
which	O
is	O
extracted	O
from	O
Freebase	O
.	O
For	O
brevity	O
,	O
Figure	O
2	O
compares	O
the	O
top	O
-	O
15	O
relations	O
by	O
mention	O
count	O
in	O
the	O
two	O
datasets	O
.	O
Appendix	O
E	O
provides	O
more	O
content	O
comparisons	O
.	O
Diversity	O
The	O
most	O
common	O
relation	O
in	O
CODEX	O
-	O
M	O
is	O
occupation	O
,	O
which	O
is	O
because	O
most	O
people	O
on	O
Wikidata	O
have	O
multiple	O
occupations	O
listed	O
.	O
By	O
contrast	O
,	O
the	O
frequent	O
relations	O
in	O
FB15	O
K	O
-	O
237	O
are	O
mostly	O
related	O
to	O
awards	O
and	O
film	O
.	O
In	O
fact	O
,	O
over	O
25	O
%	O
of	O
all	O
triples	O
in	O
FB15	O
K	O
-	O
237	O
belong	O
to	O
the	O
/award	O
relation	O
domain	O
,	O
suggesting	O
that	O
CODEX	O
covers	O
a	O
more	O
diverse	O
selection	O
of	O
content	O
.	O
Interpretability	O
The	O
Freebase	O
-	O
style	O
relations	O
are	O
also	O
arguably	O
less	O
interpretable	O
than	O
those	O
in	O
Wikidata	O
.	O
Whereas	O
Wikidata	O
relations	O
have	O
concise	O
natural	O
language	O
labels	O
,	O
the	O
Freebase	O
relation	O
labels	O
are	O
hierarchical	O
,	O
often	O
at	O
five	O
or	O
six	O
levels	O
of	O
hierarchy	O
(	O
Figure	O
2	O
)	O
.	O
Moreover	O
,	O
all	O
relations	O
in	O
Wikidata	O
are	O
binary	O
,	O
whereas	O
some	O
Freebase	O
relations	O
are	O
n	O
-	O
nary	O
(	O
Tanon	O
et	O
al	O
,	O
2016	O
)	O
,	O
meaning	O
that	O
they	O
connect	O
more	O
than	O
two	O
entities	O
.	O
The	O
relations	O
containing	O
a	O
dot	O
(	O
"	O
.	O
"	O
)	O
are	O
such	O
n	O
-	O
nary	O
relations	O
,	O
and	O
are	O
difficult	O
to	O
reason	O
about	O
without	O
understanding	O
the	O
structure	O
of	O
Freebase	O
,	O
which	O
has	O
been	O
deprecated	O
.	O
We	O
further	O
discuss	O
the	O
impact	O
of	O
such	O
n	O
-	O
nary	O
relations	O
for	O
link	B-TaskName
prediction	I-TaskName
in	O
the	O
following	O
section	O
.	O

We	O
present	O
CODEX	O
,	O
a	O
set	O
of	O
knowledge	B-TaskName
graph	I-TaskName
COmpletion	I-TaskName
Datasets	O
EXtracted	O
from	O
Wikidata	O
and	O
Wikipedia	O
,	O
and	O
show	O
that	O
CODEX	O
is	O
suitable	O
for	O
multiple	O
KGC	O
tasks	O
.	O
We	O
release	O
data	O
,	O
code	O
,	O
and	O
pretrained	O
models	O
for	O
use	O
by	O
the	O
community	O
at	O
https://bit.ly/2EPbrJs	O
.	O
Some	O
promising	O
future	O
directions	O
on	O
CODEX	O
include	O
:	O
Better	O
model	O
understanding	O
CODEX	O
can	O
be	O
used	O
to	O
analyze	O
the	O
impact	O
of	O
hyperparameters	O
,	O
training	O
strategies	O
,	O
and	O
model	O
architectures	O
in	O
KGC	O
tasks	O
.	O
Revival	O
of	O
triple	B-TaskName
classification	I-TaskName
We	O
encourage	O
the	O
use	O
of	O
triple	B-TaskName
classification	I-TaskName
on	O
CODEX	O
in	O
addition	O
to	O
link	B-TaskName
prediction	I-TaskName
because	O
it	O
directly	O
tests	O
discriminative	O
power	O
.	O
Fusing	O
text	O
and	O
structure	O
Including	O
text	O
in	O
both	O
the	O
link	B-TaskName
prediction	I-TaskName
and	O
triple	B-TaskName
classification	I-TaskName
tasks	O
should	O
substantially	O
improve	O
performance	O
.	O
Furthermore	O
,	O
text	O
can	O
be	O
used	O
for	O
few	O
-	O
shot	O
link	B-TaskName
prediction	I-TaskName
,	O
an	O
emerging	O
research	O
direction	O
(	O
Xiong	O
et	O
al	O
,	O
2017	O
;	O
Shi	O
and	O
Weninger	O
,	O
2017	O
)	O
.	O
Overall	O
,	O
we	O
hope	O
that	O
CODEX	O
will	O
provide	O
a	O
boost	O
to	O
research	O
in	O
KGC	O
,	O
which	O
will	O
in	O
turn	O
impact	O
many	O
other	O
fields	O
of	O
artificial	O
intelligence	O
.	O

Table	O
8	O
provides	O
an	O
overview	O
of	O
knowledge	B-TaskName
graph	I-TaskName
embedding	I-TaskName
papers	O
with	O
respect	O
to	O
datasets	O
and	O
evaluation	O
tasks	O
.	O
In	O
our	O
review	O
,	O
we	O
only	O
consider	O
papers	O
published	O
between	O
2014	O
and	O
2020	O
in	O
the	O
main	O
proceedings	O
of	O
conferences	O
where	O
KGC	O
embedding	O
papers	O
are	O
most	O
likely	O
to	O
appear	O
:	O
Artificial	O
intelligence	O
(	O
AAAI	O
,	O
IJCAI	O
)	O
,	O
machine	O
learning	O
(	O
ICML	O
,	O
ICLR	O
,	O
NeurIPS	O
)	O
,	O
and	O
natural	O
language	O
processing	O
(	O
ACL	O
,	O
EMNLP	O
,	O
NAACL	O
)	O
.	O
The	O
main	O
evaluation	O
benchmarks	O
are	O
FB15	O
K	O
(	O
Bordes	O
et	O
al	O
,	O
2013	O
)	O
,	O
WN18	B-DatasetName
(	O
Bordes	O
et	O
al	O
,	O
2013	O
)	O
,	O
FB15	O
K	O
-	O
237	O
(	O
Toutanova	O
and	O
Chen	O
,	O
2015	O
)	O
,	O
WN18RR	B-DatasetName
(	O
Dettmers	O
et	O
al	O
,	O
2018	O
)	O
,	O
FB13	O
(	O
Socher	O
et	O
al	O
,	O
2013	O
)	O
,	O
WN11	O
(	O
Socher	O
et	O
al	O
,	O
2013	O
)	O
,	O
NELL	B-DatasetName
-	O
995	O
(	O
Xiong	O
et	O
al	O
,	O
2017	O
)	O
,	O
YAGO3	B-DatasetName
-	I-DatasetName
10	I-DatasetName
(	O
Dettmers	O
et	O
al	O
,	O
2018	O
,	O
Countries	O
(	O
Bouchard	O
et	O
al	O
,	O
2015	O
)	O
.	O
UMLS	B-DatasetName
(	O
McCray	O
,	O
2003	O
)	O
,	O
Kinship	B-DatasetName
(	O
Kemp	O
et	O
al	O
,	O
2006	O
)	O
,	O
Families	O
(	O
Hinton	O
,	O
1986	O
,	O
and	O
other	O
versions	O
of	O
NELL	B-DatasetName
(	O
Mitchell	O
et	O
al	O
,	O
2018	O
)	O
.	O

We	O
provide	O
the	O
annotation	O
guidelines	O
we	O
used	O
to	O
label	O
candidate	O
negative	O
triples	O
(	O
3.4	O
)	O
.	O
Task	O
You	O
must	O
label	O
each	O
triple	O
as	O
either	O
true	O
or	O
false	O
.	O
To	O
help	O
you	O
find	O
the	O
answer	O
,	O
we	O
have	O
provided	O
you	O
with	O
Wikipedia	O
and	O
Wikidata	O
links	O
for	O
the	O
entities	O
and	O
relations	O
in	O
each	O
triple	O
.	O
You	O
may	O
also	O
search	O
on	O
Google	B-DatasetName
for	O
the	O
answer	O
,	O
although	O
most	O
claims	O
should	O
be	O
resolvable	O
using	O
Wikipedia	O
and	O
Wikidata	O
alone	O
.	O
If	O
you	O
are	O
not	O
able	O
to	O
find	O
any	O
reliable	O
,	O
specific	O
,	O
clear	O
information	O
supporting	O
the	O
claim	O
,	O
choose	O
false	O
.	O
You	O
may	O
explain	O
your	O
reasoning	O
if	O
need	O
be	O
or	O
provide	O
sources	O
to	O
back	O
up	O
your	O
answer	O
in	O
the	O
optional	O
explanation	O
column	O
.	O
Examples	O
False	O
triples	O
may	O
have	O
problems	O
with	O
grammar	O
,	O
factual	O
content	O
,	O
or	O
both	O
.	O
Examples	O
of	O
grammatically	O
incorrect	O
triples	O
are	O
those	O
whose	O
entity	O
or	O
relation	O
types	O
do	O
not	O
make	O
sense	O
,	O
for	O
example	O
:	O
(	O
United	O
States	O
of	O
America	O
,	O
continent	O
,	O
science	O
fiction	O
writer	O
)	O
(	O
Mohandas	O
Karamchand	O
Gandhi	O
,	O
medical	O
condition	O
,	O
British	O
Raj	O
)	O
(	O
Canada	O
,	O
foundational	O
text	O
,	O
Vietnamese	O
cuisine	O
)	O
Examples	O
of	O
grammatically	O
correct	O
but	O
factually	O
false	O
triples	O
include	O
:	O
(	O
United	O
States	O
of	O
America	O
,	O
continent	O
,	O
Europe	O
)	O
(	O
Mohandas	O
Karamchand	O
Gandhi	O
,	O
country	O
of	O
citizenship	O
,	O
Argentina	O
)	O
(	O
Canada	O
,	O
foundational	O
text	O
,	O
Harry	O
Potter	O
and	O
the	O
Goblet	O
of	O
Fire	O
)	O
(	O
Alexander	O
Pushkin	O
,	O
influenced	O
by	O
,	O
Leo	O
Tolstoy	O
)	O
-	O
Pushkin	O
died	O
only	O
a	O
few	O
years	O
after	O
Tolstoy	O
was	O
born	O
,	O
so	O
this	O
sentence	O
is	O
unlikely	O
.	O
Notice	O
that	O
in	O
the	O
latter	O
examples	O
,	O
the	O
entity	O
types	O
match	O
up	O
,	O
but	O
the	O
statements	O
are	O
still	O
false	O
.	O
Tips	O
For	O
triples	O
about	O
people	O
's	O
occupation	O
and	O
genre	O
,	O
try	O
to	O
be	O
as	O
specific	O
as	O
possible	O
.	O
For	O
example	O
,	O
if	O
the	O
triple	O
says	O
(	O
<	O
person	O
>	O
,	O
occupation	O
,	O
guitarist	O
)	O
but	O
that	O
person	O
is	O
mainly	O
known	O
for	O
their	O
singing	O
,	O
choose	O
false	O
,	O
even	O
if	O
that	O
person	O
plays	O
the	O
guitar	O
.	O
Likewise	O
,	O
if	O
a	O
triple	O
says	O
(	O
<	O
person	O
>	O
,	O
genre	O
,	O
classical	O
)	O
but	O
they	O
are	O
mostly	O
known	O
for	O
jazz	O
music	O
,	O
choose	O
false	O
even	O
if	O
,	O
for	O
example	O
,	O
that	O
person	O
had	O
classical	O
training	O
in	O
their	O
childhood	O
.	O

We	O
briefly	O
overview	O
the	O
five	O
models	O
compared	O
in	O
our	O
link	B-TaskName
prediction	I-TaskName
and	O
triple	B-TaskName
classification	I-TaskName
tasks	O
.	O
RESCAL	B-MethodName
(	O
Nickel	O
et	O
al	O
,	O
2011	O
)	O
was	O
one	O
of	O
the	O
first	O
knowledge	B-TaskName
graph	I-TaskName
embedding	I-TaskName
models	O
.	O
Although	O
it	O
is	O
not	O
often	O
used	O
as	O
a	O
baseline	O
,	O
Ruffinelli	O
et	O
al	O
(	O
2020	O
)	O
showed	O
that	O
it	O
is	O
competitive	O
when	O
appropriately	O
tuned	O
.	O
RESCAL	B-MethodName
treats	O
relational	O
learning	O
as	O
tensor	O
decomposition	O
,	O
scoring	O
entity	B-TaskName
embeddings	I-TaskName
h	O
,	O
r	O
R	O
de	O
and	O
relation	O
embeddings	O
R	O
R	O
de×de	O
with	O
the	O
bilinear	O
form	O
h	O
Rt	O
.	O
P35	O
)	O
,	O
headquarters	O
location	O
(	O
P159	O
)	O
,	O
health	O
specialty	O
(	O
P1995	O
)	O
,	O
indigenous	O
to	O
(	O
P2341	O
)	O
,	O
industry	O
(	O
P452	O
)	O
,	O
influenced	O
by	O
(	O
P737	O
)	O
,	O
instance	O
of	O
(	O
P31	O
)	O
,	O
instrument	O
(	O
P1303	O
)	O
,	O
language	O
of	O
work	O
or	O
name	O
(	O
P407	O
)	O
,	O
languages	O
spoken	O
,	O
written	O
,	O
or	O
signed	O
(	O
P1412	O
)	O
,	O
legal	O
form	O
(	O
P1454	O
)	O
,	O
legislative	O
body	O
(	O
P194	O
)	O
,	O
located	O
in	O
the	O
administrative	O
terroritorial	O
entity	O
(	O
P131	O
)	O
,	O
location	O
of	O
formation	O
(	O
P740	O
)	O
,	O
medical	O
condition	O
(	O
P1050	O
)	O
,	O
medical	O
examinations	O
(	O
P923	O
)	O
,	O
member	O
of	O
(	O
P463	O
)	O
,	O
member	O
of	O
political	O
party	O
(	O
P102	O
)	O
,	O
member	O
of	O
sports	O
team	O
(	O
P54	O
)	O
,	O
mountain	O
range	O
(	O
P4552	O
)	O
,	O
movement	O
(	O
P135	O
)	O
,	O
named	O
after	O
(	O
P138	O
)	O
,	O
narrative	O
location	O
(	O
P840	O
)	O
,	O
notable	O
works	O
(	O
P800	O
)	O
,	O
occupant	O
(	O
P466	O
)	O
,	O
occupation	O
(	O
P106	O
)	O
,	O
official	O
language	O
(	O
P37	O
)	O
,	O
parent	O
organization	O
(	O
P749	O
)	O
,	O
part	O
of	O
(	O
P361	O
)	O
,	O
place	O
of	O
birth	O
(	O
P19	O
)	O
,	O
place	O
of	O
burial	O
(	O
P119	O
)	O
,	O
place	O
of	O
death	O
(	O
P20	O
)	O
,	O
practiced	O
by	O
(	O
P3095	O
)	O
,	O
product	O
or	O
material	O
produced	O
(	O
P1056	O
)	O
,	O
publisher	O
(	O
P123	O
)	O
,	O
record	O
label	O
(	O
P264	O
)	O
,	O
regulated	O
by	O
(	O
P3719	O
)	O
,	O
religion	O
(	O
P140	O
)	O
,	O
residence	O
(	O
P551	O
)	O
,	O
shares	O
border	O
with	O
(	O
P47	O
)	O
,	O
sibling	O
(	O
P3373	O
)	O
,	O
sport	O
(	O
P641	O
)	O
,	O
spouse	O
(	O
P26	O
)	O
,	O
studies	O
(	O
P2578	O
)	O
,	O
subclass	O
of	O
(	O
P279	O
)	O
,	O
symptoms	O
(	O
P780	O
)	O
,	O
time	O
period	O
(	O
P2348	O
)	O
,	O
tributary	O
(	O
P974	O
)	O
,	O
unmarried	O
partner	O
(	O
P451	O
)	O
,	O
use	O
(	O
P366	O
)	O
,	O
uses	O
(	O
P2283	O
)	O
TransE	B-MethodName
(	O
Bordes	O
et	O
al	O
,	O
2013	O
)	O
treats	O
relations	O
as	O
translations	O
between	O
entities	O
,	O
i.e.	O
,	O
h	O
+	O
r	O
≈	O
t	O
for	O
h	O
,	O
r	O
,	O
t	O
R	O
de	O
,	O
and	O
scores	O
embeddings	O
with	O
negative	O
Euclidean	O
distance	O
−	O
h	O
+	O
r	O
−	O
t	O
.	O
TransE	B-MethodName
is	O
likely	O
the	O
most	O
popular	O
baseline	O
for	O
KGC	O
tasks	O
and	O
the	O
most	O
influential	O
of	O
all	O
KGC	O
embedding	O
papers	O
.	O
ComplEx	O
(	O
Trouillon	O
et	O
al	O
,	O
2016	O
)	O
uses	O
a	O
bilinear	O
function	O
to	O
score	O
triples	O
with	O
a	O
diagonal	O
relation	O
embedding	O
matrix	O
and	O
complex	O
-	O
valued	O
embeddings	O
.	O
Its	O
scoring	O
function	O
is	O
re	O
h	O
diag	O
(	O
r	O
)	O
t	O
,	O
where	O
t	O
is	O
the	O
complex	O
conjugate	O
of	O
t	O
and	O
re	O
denotes	O
the	O
real	O
part	O
of	O
a	O
complex	O
number	O
.	O
ConvE	O
(	O
Dettmers	O
et	O
al	O
,	O
2018	O
)	O
is	O
one	O
of	O
the	O
first	O
and	O
most	O
popular	O
nonlinear	O
models	O
for	O
KGC	O
.	O
It	O
concatenates	O
head	O
and	O
relation	O
embeddings	O
h	O
and	O
r	O
into	O
a	O
two	O
-	O
dimensional	O
"	O
image	O
"	O
,	O
applies	O
a	O
pointwise	O
linearity	O
over	O
convolutional	O
and	O
fullyconnected	O
layers	O
,	O
and	O
multiplies	O
the	O
result	O
with	O
the	O
tail	O
embedding	O
t	O
to	O
obtain	O
a	O
score	O
.	O
Formally	O
,	O
its	O
scoring	O
function	O
is	O
given	O
as	O
f	O
(	O
vec	O
(	O
f	O
(	O
[	O
h	O
;	O
r	O
]	O
*	O
ω	O
)	O
)	O
W	O
)	O
t	O
,	O
where	O
f	O
is	O
a	O
nonlinearity	O
(	O
originally	O
,	O
ReLU	B-MethodName
)	O
,	O
[	O
h	O
;	O
r	O
]	O
denotes	O
a	O
concatenation	O
and	O
twodimensional	O
reshaping	O
of	O
the	O
head	O
and	O
relation	O
embeddings	O
,	O
ω	O
denotes	O
the	O
filters	O
of	O
the	O
convolutional	O
layer	O
,	O
and	O
vec	O
denotes	O
the	O
flattening	O
of	O
a	O
two	O
-	O
dimensional	O
matrix	O
.	O
TuckER	B-MethodName
(	O
Balazevic	O
et	O
al	O
,	O
2019b	O
)	O
is	O
a	O
linear	O
model	O
based	O
on	O
the	O
Tucker	O
tensor	O
decomposition	O
,	O
which	O
factorizes	O
a	O
tensor	O
into	O
three	O
lower	O
-	O
rank	O
matrices	O
and	O
a	O
core	O
tensor	O
.	O
The	O
TuckER	B-MethodName
scoring	O
function	O
for	O
a	O
single	O
triple	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
is	O
given	O
as	O
W	O
×	O
1	O
h	O
×	O
2	O
r	O
×	O
3	O
t	O
,	O
where	O
W	O
is	O
the	O
mode	O
-	O
three	O
core	O
tensor	O
that	O
is	O
shared	O
among	O
all	O
entity	O
and	O
relation	O
embeddings	O
,	O
and	O
×	O
n	O
denotes	O
the	O
tensor	O
product	O
along	O
the	O
nth	O
mode	O
of	O
the	O
tensor	O
.	O
TuckER	B-MethodName
can	O
be	O
seen	O
as	O
a	O
generalized	O
form	O
of	O
other	O
linear	O
KGC	O
embedding	O
models	O
like	O
RESCAL	B-MethodName
and	O
ComplEx	O
.	O

Traditional	O
IE	O
systems	O
assume	O
abundant	O
human	O
annotations	O
for	O
training	O
high	O
quality	O
machine	O
learning	O
models	O
,	O
which	O
is	O
impractical	O
when	O
trying	O
to	O
deploy	O
IE	O
systems	O
to	O
a	O
broad	O
range	O
of	O
domains	O
,	O
settings	O
and	O
languages	O
.	O
In	O
the	O
first	O
part	O
of	O
the	O
tutorial	O
,	O
we	O
introduce	O
how	O
to	O
extract	O
structured	O
facts	O
(	O
i.e.	O
,	O
entities	O
and	O
their	O
relations	O
of	O
different	O
types	O
)	O
from	O
text	O
corpora	O
to	O
construct	O
knowledge	O
bases	O
,	O
with	O
a	O
focus	O
on	O
methods	O
that	O
are	O
minimally	O
-	O
supervised	O
and	O
domain	O
-	O
independent	O
for	O
timely	O
knowledge	O
base	O
construction	O
across	O
various	O
application	O
domains	O
.	O
In	O
the	O
second	O
part	O
,	O
we	O
introduce	O
how	O
to	O
leverage	O
other	O
knowledge	O
,	O
such	O
as	O
the	O
distributional	O
statistics	O
of	O
characters	O
and	O
words	O
,	O
the	O
annotations	O
for	O
other	O
tasks	O
and	O
other	O
domains	O
,	O
and	O
the	O
linguistics	O
and	O
problem	O
structures	O
,	O
to	O
combat	O
the	O
problem	O
of	O
inadequate	O
supervision	O
,	O
and	O
conduct	O
low	O
-	O
resource	O
information	O
extraction	O
.	O
In	O
the	O
third	O
part	O
,	O
we	O
describe	O
recent	O
advances	O
in	O
knowledge	O
base	O
reasoning	O
.	O
We	O
start	O
with	O
the	O
gentle	O
introduction	O
to	O
the	O
literature	O
,	O
focusing	O
on	O
pathbased	B-DatasetName
and	O
embedding	O
based	O
methods	O
.	O
We	O
then	O
describe	O
DeepPath	O
,	O
a	O
recent	O
attempt	O
of	O
using	O
deep	O
reinforcement	O
learning	O
to	O
combine	O
the	O
best	O
of	O
both	O
worlds	O
for	O
knowledge	O
base	O
reasoning	O
.	O

The	O
success	O
of	O
data	O
mining	O
and	O
artificial	O
intelligence	O
technology	O
is	O
largely	O
attributed	O
to	O
the	O
efficient	O
and	O
effective	O
analysis	O
of	O
structured	O
data	O
.	O
The	O
construction	O
of	O
a	O
well	O
-	O
structured	O
,	O
machine	O
-	O
actionable	O
knowledge	O
base	O
(	O
KB	O
)	O
from	O
raw	O
(	O
unstructured	O
or	O
loosely	O
-	O
structured	O
)	O
data	O
sources	O
is	O
often	O
the	O
premise	O
of	O
consequent	O
applications	O
.	O
Although	O
the	O
majority	O
of	O
existing	O
data	O
generated	O
in	O
our	O
society	O
is	O
unstructured	O
,	O
big	O
data	O
leads	O
to	O
big	O
opportunities	O
to	O
uncover	O
structures	O
of	O
real	O
-	O
world	O
entities	O
(	O
e.g.	O
,	O
person	O
,	O
product	O
)	O
,	O
attributes	O
(	O
e.g.	O
,	O
age	O
,	O
weight	O
)	O
,	O
relations	O
(	O
e.g.	O
,	O
employee	O
of	O
,	O
manufacture	O
)	O
from	O
massive	O
text	O
corpora	O
.	O
By	O
integrating	O
these	O
semantic	O
structures	O
,	O
one	O
can	O
construct	O
a	O
powerful	O
KB	O
as	O
a	O
conceptual	O
abstraction	O
of	O
the	O
original	O
corpus	O
.	O
The	O
constructed	O
knowledge	O
base	O
will	O
facilitate	O
browsing	O
information	O
and	O
inferring	O
knowledge	O
that	O
are	O
otherwise	O
widely	O
scattered	O
in	O
the	O
text	O
corpora	O
.	O
Computational	O
machines	O
can	O
effectively	O
perform	O
algorithmic	O
analysis	O
at	O
a	O
large	O
scale	O
over	O
these	O
KBs	O
,	O
and	O
apply	O
the	O
new	O
insights	O
to	O
improve	O
human	O
productivity	O
in	O
various	O
downstream	O
tasks	O
.	O
Our	O
Focus	O
.	O
In	O
this	O
tutorial	O
,	O
we	O
focus	O
our	O
discussion	O
on	O
two	O
tightly	O
related	O
problems	O
:	O
automatic	O
construction	O
of	O
knowledge	O
bases	O
from	O
text	O
,	O
and	O
knowledge	O
reasoning	O
for	O
knowledge	B-TaskName
base	I-TaskName
completion	I-TaskName
.	O
While	O
traditional	O
information	O
extraction	O
techniques	O
have	O
heavy	O
reliance	O
on	O
human	O
-	O
annotated	O
data	O
,	O
our	O
tutorial	O
will	O
devote	O
more	O
time	O
on	O
introducing	O
methods	O
that	O
can	O
reduce	O
human	O
efforts	O
in	O
the	O
process	O
,	O
by	O
leveraging	O
external	O
knowledge	O
sources	O
(	O
e.g.	O
,	O
distant	O
supervision	O
)	O
and	O
exploiting	O
rich	O
data	O
redundancy	O
in	O
massive	O
text	O
corpora	O
(	O
e.g.	O
,	O
weak	O
supervision	O
)	O
.	O
We	O
also	O
discuss	O
how	O
data	O
sources	O
from	O
various	O
domains	O
and	O
languages	O
could	O
opens	O
up	O
tremendous	O
opportunities	O
to	O
leverage	O
and	O
transfer	O
existing	O
knowledge	O
about	O
domains	O
,	O
tasks	O
and	O
language	O
,	O
and	O
help	O
knowledge	O
extraction	O
in	O
low	O
-	O
resource	O
settings	O
with	O
minimal	O
supervision	O
.	O
In	O
the	O
reasoning	O
part	O
,	O
we	O
aim	O
to	O
leverage	O
the	O
existing	O
background	O
knowledge	O
and	O
design	O
various	O
algorithms	O
to	O
fill	O
in	O
the	O
missing	O
link	O
between	O
entities	O
in	O
the	O
KB	O
,	O
given	O
the	O
extracted	O
KBs	O
are	O
likely	O
incomplete	O
.	O
More	O
specifically	O
,	O
this	O
part	O
will	O
introduce	O
two	O
lines	O
of	O
research	O
for	O
KB	O
reasoning	O
:	O
path	O
-	O
based	O
and	O
embedding	O
-	O
based	O
methods	O
.	O
Topics	O
to	O
be	O
covered	O
in	O
this	O
tutorial	O
.	O
The	O
first	O
2/3	O
of	O
this	O
tutorial	O
presents	O
a	O
comprehensive	O
overview	O
of	O
the	O
information	O
extraction	O
techniques	O
developed	O
in	O
recent	O
years	O
for	O
constructing	O
knowledge	O
bases	O
(	O
see	O
also	O
Section	O
2	O
for	O
a	O
more	O
detailed	O
outline	O
)	O
.	O
We	O
will	O
discuss	O
the	O
following	O
key	O
issues	O
:	O
(	O
1	O
)	O
data	O
-	O
driven	O
approaches	O
for	O
mining	O
quality	O
phrases	O
from	O
massive	O
,	O
unstructured	O
text	O
corpora	O
;	O
(	O
2	O
)	O
entity	O
recognition	O
and	O
typing	O
:	O
preliminaries	O
,	O
challenges	O
,	O
and	O
methodologies	O
;	O
and	O
(	O
3	O
)	O
relation	B-TaskName
extraction	I-TaskName
:	O
previous	O
efforts	O
,	O
limitations	O
,	O
recent	O
progress	O
,	O
and	O
a	O
joint	B-TaskName
entity	I-TaskName
and	I-TaskName
relation	I-TaskName
extraction	I-TaskName
method	O
using	O
distant	O
supervision	O
;	O
(	O
4	O
)	O
multi	O
-	O
task	O
and	O
multi	O
-	O
domain	O
learning	O
for	O
lowresource	O
information	O
extraction	O
;	O
(	O
5	O
)	O
distill	O
linguistic	O
knowledge	O
into	O
neural	O
models	O
to	O
help	O
low	O
-	O
resource	O
information	O
extraction	O
.	O
The	O
second	O
half	O
of	O
the	O
tutorial	O
presents	O
a	O
comprehensive	O
overview	O
of	O
KB	O
reasoning	O
techniques	O
.	O
For	O
path	O
-	O
based	O
methods	O
,	O
we	O
will	O
first	O
describe	O
the	O
Path	O
-	O
Ranking	O
Algorithm	O
(	O
PRA	O
)	O
(	O
Lao	O
et	O
al	O
,	O
2011	O
)	O
and	O
briefly	O
describe	O
extensions	O
such	O
as	O
ProPPR	O
.	O
Our	O
tutorial	O
will	O
also	O
cover	O
the	O
recent	O
integration	O
of	O
PRA	O
with	O
recurrent	O
neural	O
networks	O
.	O
For	O
the	O
embedding	O
based	O
method	O
,	O
we	O
will	O
briefly	O
describe	O
RESCAL	B-MethodName
(	O
Nickel	O
et	O
al	O
,	O
2011	O
)	O
and	O
TransE	B-MethodName
(	O
Bordes	O
et	O
al	O
,	O
2013	O
)	O
.	O
Finally	O
,	O
we	O
discuss	O
DeepPath	O
(	O
Xiong	O
et	O
al	O
,	O
2017	O
)	O
,	O
a	O
novel	O
deep	O
reinforcement	O
learning	O
model	O
that	O
combines	O
the	O
embedding	O
and	O
path	O
-	O
based	O
approaches	O
for	O
the	O
learning	O
to	O
reason	O
problem	O
.	O
Research	O
Impact	O
.	O
Our	O
phrase	O
mining	O
tool	O
,	O
SegPhrase	O
(	O
Liu	O
et	O
al	O
,	O
2015	O
)	O
,	O
won	O
the	O
grand	O
prize	O
of	O
Yelp	O
Dataset	O
Challenge	O
1	O
and	O
was	O
used	O
by	O
TripAdvisor	O
in	O
productions	O
2	O
.	O
Our	O
entity	O
recognition	O
and	O
typing	O
system	O
,	O
ClusType	O
,	O
was	O
shipped	O
as	O
part	O
of	O
the	O
products	O
in	O
Microsoft	O
Bing	O
and	O
U.S.	O
Army	O
Research	O
Lab	O
.	O
We	O
built	O
the	O
first	O
named	O
entity	O
recognizer	O
on	O
Chinese	O
social	O
media	O
Dredze	O
,	O
2015	O
,	O
2016	O
)	O
and	O
closed	O
the	O
gap	O
between	O
NER	B-TaskName
on	O
English	O
and	O
Chinese	O
social	O
media	O
.	O
The	O
same	O
technique	O
was	O
applied	O
to	O
build	O
the	O
first	O
relation	O
extractor	O
for	O
cross	O
-	O
sentence	O
,	O
n	O
-	O
ary	O
relation	B-TaskName
extraction	I-TaskName
between	O
drug	O
,	O
gene	O
,	O
and	O
mutation	O
(	O
Peng	O
et	O
al	O
,	O
2017	O
)	O
.	O
Duration	O
and	O
Sessions	O
.	O
The	O
duration	O
of	O
the	O
tutorial	O
is	O
flexible	O
:	O
It	O
is	O
expected	O
to	O
be	O
3	O
hours	O
,	O
but	O
it	O
can	O
be	O
extended	O
into	O
6	O
hours	O
,	O
based	O
on	O
the	O
need	O
of	O
the	O
conference	O
.	O
The	O
outline	O
presented	O
here	O
is	O
for	O
the	O
3	O
-	O
hour	O
tutorial	O
.	O
For	O
longer	O
duration	O
of	O
the	O
tutorial	O
,	O
we	O
plan	O
to	O
extend	O
entity	O
and	O
relation	B-TaskName
extraction	I-TaskName
parts	O
,	O
and	O
add	O
in	O
more	O
case	O
studies	O
and	O
applications	O
.	O
Relevance	O
to	O
ACL	O
.	O
Machine	O
"	O
reading	O
"	O
and	O
"	O
reasoning	O
"	O
of	O
large	O
text	O
corpora	O
have	O
long	O
been	O
the	O
interests	O
to	O
CL	O
and	O
NLP	O
communities	O
,	O
especially	O
when	O
people	O
now	O
are	O
exposed	O
to	O
an	O
explosion	O
of	O
information	O
in	O
the	O
form	O
of	O
free	O
text	O
.	O
Extracting	O
structured	O
information	O
is	O
key	O
to	O
understanding	O
messy	O
and	O
scattered	O
raw	O
data	O
,	O
and	O
effective	O
reasoning	O
tools	O
are	O
critical	O
for	O
the	O
use	O
of	O
KBs	O
in	O
downstream	O
tasks	O
like	O
QA	O
.	O
This	O
tutorial	O
will	O
present	O
an	O
organized	O
picture	O
of	O
recent	O
research	O
on	O
knowledge	O
base	O
construction	O
and	O
reasoning	O
.	O
We	O
will	O
show	O
how	O
exciting	O
and	O
surprising	O
knowledge	O
can	O
be	O
discovered	O
from	O
your	O
own	O
not	O
so	O
well	O
-	O
structured	O
raw	O
corpora	O
,	O
and	O
such	O
incomplete	O
KBs	O
can	O
be	O
further	O
used	O
to	O
derive	O
new	O
insights	O
and	O
more	O
complex	O
knowledge	O
with	O
reasoning	O
techniques	O
.	O

Most	O
of	O
the	O
previous	O
tutorials	O
focused	O
exclusively	O
on	O
the	O
knowledge	O
base	O
construction	O
aspect	O
.	O
In	O
the	O
proposed	O
tutorial	O
,	O
we	O
will	O
give	O
a	O
systematic	O
discussion	O
on	O
the	O
problem	O
of	O
knowledge	O
base	O
reasoning	O
,	O
for	O
which	O
extensive	O
studies	O
have	O
been	O
conducted	O
recently	O
but	O
systematic	O
tutorials	O
are	O
lacking	O
.	O
This	O
tutorial	O
also	O
presents	O
recent	O
advances	O
in	O
applying	O
distant	O
and	O
weak	O
supervision	O
to	O
the	O
extraction	O
of	O
structured	O
facts	O
in	O
knowledge	O
base	O
construction	O
,	O
in	O
addition	O
to	O
the	O
traditional	O
supervised	O
techniques	O
and	O
rule	O
-	O
based	O
approaches	O
.	O
Target	O
audience	O
and	O
prerequisites	O
.	O
Researchers	O
and	O
practitioners	O
in	O
the	O
field	O
of	O
natural	O
language	O
processing	O
,	O
computational	O
linguistic	O
,	O
text	O
mining	O
,	O
information	B-TaskName
retrieval	I-TaskName
,	O
semantic	O
web	O
and	O
machine	O
learning	O
.	O
While	O
the	O
audience	O
with	O
a	O
good	O
background	O
in	O
these	O
areas	O
would	O
benefit	O
most	O
from	O
this	O
tutorial	O
,	O
we	O
believe	O
the	O
material	O
to	O
be	O
presented	O
would	O
give	O
general	O
audience	O
and	O
newcomers	O
an	O
introductory	O
pointer	O
to	O
the	O
current	O
work	O
and	O
important	O
research	O
topics	O
in	O
this	O
field	O
,	O
and	O
inspire	O
them	O
to	O
learn	O
more	O
.	O
Only	O
preliminary	O
knowledge	O
about	O
NLP	O
,	O
algorithms	O
and	O
their	O
applications	O
are	O
needed	O
.	O
We	O
expect	O
there	O
will	O
be	O
around	O
70	O
people	O
interested	O
in	O
our	O
tutorial	O
.	O
Tutorial	O
material	O
and	O
equipment	O
.	O
We	O
will	O
provide	O
attendees	O
a	O
website	O
and	O
upload	O
our	O
tutorial	O
materials	O
(	O
slides	O
,	O
references	O
,	O
softwares	O
)	O
.	O
There	O
is	O
no	O
copyright	O
issue	O
.	O
Standard	O
equipment	O
will	O
be	O
enough	O
for	O
our	O
tutorial	O
.	O

