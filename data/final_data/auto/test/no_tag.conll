In	O
this	O
paper	O
,	O
we	O
present	O
an	O
improved	O
graph	O
-	O
based	O
translation	O
model	O
which	O
segments	O
an	O
input	O
graph	O
into	O
node	O
-	O
induced	O
subgraphs	O
by	O
taking	O
source	O
context	O
into	O
consideration	O
.	O
Translations	O
are	O
generated	O
by	O
combining	O
subgraph	O
translations	O
leftto	O
-	O
right	O
using	O
beam	O
search	O
.	O
Experiments	O
on	O
Chinese	O
-	O
English	O
and	O
German	O
-	O
English	O
demonstrate	O
that	O
the	O
context	O
-	O
aware	O
segmentation	O
significantly	O
improves	O
the	O
baseline	O
graph	O
-	O
based	O
model	O
.	O

The	O
well	O
-	O
known	O
phrase	O
-	O
based	O
statistical	O
translation	O
model	O
(	O
Koehn	O
et	O
al	O
,	O
2003	O
)	O
extends	O
the	O
basic	O
translation	O
units	O
from	O
single	O
words	O
to	O
continuous	O
phrases	O
to	O
capture	O
local	O
phenomena	O
.	O
However	O
,	O
one	O
of	O
its	O
significant	O
weaknesses	O
is	O
that	O
it	O
can	O
not	O
learn	O
generalizations	O
Galley	O
and	O
Manning	O
,	O
2010	O
)	O
.	O
To	O
allow	O
discontinuous	O
phrases	O
(	O
any	O
subset	O
of	O
words	O
of	O
an	O
input	O
sentence	O
)	O
,	O
dependency	O
treelets	O
Xiong	O
et	O
al	O
,	O
2007	O
)	O
can	O
be	O
used	O
,	O
which	O
are	O
connected	O
subgraphs	O
on	O
trees	O
.	O
However	O
,	O
continuous	O
phrases	O
which	O
are	O
not	O
connected	O
on	O
trees	O
and	O
thus	O
excluded	O
could	O
in	O
fact	O
be	O
extremely	O
important	O
to	O
system	O
performance	O
(	O
Koehn	O
et	O
al	O
,	O
2003	O
;	O
Hanneman	O
and	O
Lavie	O
,	O
2009	O
)	O
.	O
To	O
make	O
use	O
of	O
the	O
merits	O
of	O
both	O
phrase	O
-	O
based	O
models	O
and	O
treelet	O
-	O
based	O
models	O
,	O
Li	O
et	O
al	O
(	O
2016	O
)	O
proposed	O
a	O
graph	O
-	O
based	O
translation	O
model	O
as	O
in	O
Equation	O
(	O
1	O
)	O
:	O
p	O
(	O
t	O
I	O
1	O
|	O
g	O
I	O
1	O
)	O
=	O
I	O
i=1	O
p	O
(	O
t	O
i	O
|	O
g	O
a	O
i	O
)	O
×	O
d	O
(	O
g	O
a	O
i	O
,	O
g	O
a	O
i−1	O
)	O
(	O
1	O
)	O
where	O
t	O
i	O
is	O
a	O
continuous	O
target	O
phrase	O
which	O
is	O
the	O
translation	O
of	O
a	O
node	O
-	O
induced	O
and	O
connected	O
source	O
subgraph	O
g	O
a	O
i	O
.	O
1	O
d	O
is	O
a	O
distance	O
-	O
based	O
reordering	O
function	O
which	O
penalizes	O
discontinuous	O
phrases	O
that	O
have	O
relatively	O
long	O
gaps	O
(	O
Galley	O
and	O
Manning	O
,	O
2010	O
)	O
.	O
The	O
model	O
translates	O
an	O
input	O
graph	O
by	O
segmenting	O
it	O
into	O
subgraphs	O
and	O
generates	O
a	O
complete	O
translation	O
by	O
combining	O
subgraph	O
translations	O
left	O
-	O
to	O
-	O
right	O
.	O
However	O
,	O
the	O
model	O
treats	O
different	O
graph	O
segmentations	O
equally	O
.	O
Therefore	O
,	O
in	O
this	O
paper	O
we	O
propose	O
a	O
contextaware	O
graph	O
segmentation	O
(	O
Section	O
2	O
)	O
:	O
(	O
i	O
)	O
we	O
add	O
contextual	O
information	O
to	O
each	O
translation	O
rule	O
during	O
training	O
(	O
Section	O
2.2	O
)	O
;	O
(	O
ii	O
)	O
during	O
decoding	O
,	O
when	O
a	O
rule	O
is	O
applied	O
,	O
the	O
input	O
context	O
should	O
match	O
with	O
the	O
rule	O
context	O
(	O
Section	O
2.3	O
)	O
.	O
Experiments	O
(	O
Section	O
3	O
)	O
on	O
Chinese	O
-	O
English	O
(	O
ZH	O
-	O
EN	O
)	O
and	O
German	O
-	O
English	O
(	O
DE	O
-	O
EN	O
)	O
tasks	O
show	O
that	O
our	O
method	O
significantly	O
improves	O
the	O
graphbased	O
model	O
.	O
As	O
observed	O
in	O
our	O
experiments	O
,	O
the	O
context	O
-	O
aware	O
segmentation	O
brings	O
two	O
benefits	O
to	O
our	O
system	O
:	O
(	O
i	O
)	O
it	O
helps	O
to	O
select	O
a	O
better	O
subgraph	O
to	O
translate	O
;	O
and	O
(	O
ii	O
)	O
it	O
selects	O
a	O
better	O
target	O
phrase	O
for	O
a	O
subgraph	O
.	O

Our	O
model	O
extends	O
the	O
graph	O
-	O
based	O
translation	O
model	O
by	O
considering	O
source	O
context	O
during	O
segmenting	O
input	O
graphs	O
,	O
as	O
in	O
Equation	O
(	O
2	O
)	O
:	O
p	O
(	O
t	O
I	O
1	O
|	O
g	O
I	O
1	O
)	O
=	O
I	O
i=1	O
p	O
(	O
t	O
i	O
|	O
g	O
a	O
i	O
,	O
c	O
a	O
i	O
)	O
×	O
d	O
(	O
g	O
a	O
i	O
,	O
g	O
a	O
i−1	O
)	O
(	O
2	O
)	O
where	O
c	O
a	O
i	O
denotes	O
the	O
context	O
of	O
the	O
subgraph	O
g	O
a	O
i	O
,	O
which	O
is	O
represented	O
as	O
a	O
set	O
of	O
connections	O
(	O
i.e.	O
edges	O
)	O
between	O
g	O
a	O
i	O
and	O
[	O
g	O
a	O
i+1	O
,	O
,	O
g	O
a	O
I	O
]	O
.	O
2010Nian	O
FIFA	O
Shijiebei	O
Zai	O
Nanfei	O
Chenggong	O
Juxing	O

The	O
graph	O
used	O
in	O
this	O
paper	O
combines	O
a	O
sequence	O
and	O
a	O
dependency	O
tree	O
as	O
in	O
Li	O
et	O
al	O
(	O
2016	O
)	O
.	O
Each	O
graph	O
contains	O
two	O
kinds	O
of	O
links	O
:	O
dependency	O
links	O
from	O
dependency	O
trees	O
which	O
model	O
syntactic	O
and	O
semantic	O
relations	O
between	O
words	O
,	O
and	O
bigram	O
links	O
which	O
provide	O
local	O
and	O
sequential	O
information	O
on	O
pairs	O
of	O
continuous	O
words	O
.	O
Figure	O
1	O
shows	O
an	O
example	O
graph	O
.	O
Given	O
such	O
graphs	O
,	O
we	O
can	O
make	O
use	O
of	O
both	O
continuous	O
and	O
linguistically	O
informed	O
discontinuous	O
phrases	O
as	O
long	O
as	O
they	O
are	O
connected	O
on	O
graphs	O
.	O
In	O
this	O
paper	O
,	O
we	O
do	O
not	O
distinguish	O
the	O
two	O
kinds	O
of	O
relations	O
,	O
because	O
our	O
preliminary	O
experiments	O
showed	O
no	O
improvement	O
when	O
considering	O
edge	O
types	O
.	O

Following	O
Li	O
et	O
al	O
(	O
2016	O
)	O
,	O
we	O
define	O
our	O
model	O
in	O
the	O
well	O
-	O
known	O
log	O
-	O
linear	O
framework	O
(	O
Och	O
and	O
Ney	O
,	O
2002	O
)	O
.	O
In	O
our	O
experiments	O
,	O
we	O
use	O
the	O
following	O
standard	O
features	O
:	O
two	O
translation	O
probabilities	O
p	O
(	O
g	O
,	O
c	O
|	O
t	O
)	O
and	O
p	O
(	O
t	O
|	O
g	O
,	O
c	O
)	O
,	O
two	O
lexical	O
translation	O
probabilities	O
p	O
lex	O
(	O
g	O
,	O
c	O
|	O
t	O
)	O
and	O
p	O
lex	O
(	O
t	O
|	O
g	O
,	O
c	O
)	O
,	O
a	O
language	O
model	O
p	O
(	O
t	O
)	O
,	O
a	O
rule	O
penalty	O
,	O
a	O
word	O
penalty	O
,	O
and	O
a	O
distortion	O
function	O
as	O
defined	O
in	O
Galley	O
and	O
Manning	O
(	O
2010	O
)	O
.	O
In	O
addition	O
,	O
we	O
add	O
one	O
more	O
feature	O
into	O
our	O
system	O
:	O
a	O
basic	O
-	O
rule	O
penalty	O
to	O
distinguish	O
basic	O
rules	O
from	O
segmenting	O
and	O
selecting	O
rules	O
.	O
Our	O
decoder	O
is	O
very	O
similar	O
to	O
the	O
one	O
in	O
the	O
conventional	O
graph	O
-	O
based	O
model	O
,	O
which	O
generates	O
hypotheses	O
left	O
-	O
to	O
-	O
right	O
using	O
beam	O
search	O
.	O
A	O
hypothesis	O
can	O
be	O
extended	O
on	O
the	O
right	O
by	O
translating	O
an	O
uncovered	O
source	O
subgraph	O
.	O
The	O
translation	O
process	O
ends	O
when	O
all	O
source	O
words	O
have	O
been	O
translated	O
.	O
However	O
,	O
when	O
extending	O
a	O
hypothesis	O
,	O
our	O
decoder	O
considers	O
the	O
context	O
of	O
the	O
translated	O
subgraph	O
,	O
i.e.	O
edges	O
connecting	O
it	O
with	O
the	O
remaining	O
untranslated	O
source	O
words	O
.	O
Figure	O
2	O
shows	O
a	O
derivation	O
which	O
translates	O
an	O
input	O
graph	O
in	O
Chinese	O
to	O
an	O
English	O
string	O
.	O
In	O
this	O
example	O
,	O
both	O
rules	O
r	O
1	O
and	O
r	O
2	O
are	O
segmenting	O
rules	O
.	O

We	O
conduct	O
experiments	O
on	O
ZH	O
-	O
EN	O
and	O
DE	O
-	O
EN	O
corpora	O
.	O

ZhiLi	O
BaoHu	O
He	O
GaiShan	O
JuZhu	O
HuanJing	O
.	O
Ref	O
:	O
GBMT	O
:	O
GBMTctx	O
:	O
we	O
are	O
also	O
committed	O
to	O
protect	O
and	O
improve	O
our	O
living	O
environment	O
.	O
we	O
have	O
worked	O
hard	O
to	O
protect	O
and	O
improve	O
the	O
living	O
environment	O
.	O
we	O
are	O
also	O
committed	O
to	O
protect	O
and	O
improve	O
the	O
living	O
environment	O
.	O
(	O
b	O
)	O
target	O
-	O
phrase	O
selection	O
Bold	O
figures	O
mean	O
a	O
system	O
is	O
significantly	O
better	O
than	O
the	O
one	O
only	O
using	O
basic	O
rules	O
at	O
p	O
≤	O
0.01	O
.	O
the	O
number	O
of	O
segmenting	O
rules	O
is	O
much	O
larger	O
than	O
the	O
number	O
of	O
selecting	O
rules	O
.	O
We	O
further	O
observed	O
that	O
,	O
while	O
our	O
system	O
achieves	O
the	O
best	O
performance	O
when	O
all	O
rules	O
are	O
used	O
on	O
ZH	O
-	O
EN	O
,	O
the	O
combination	O
of	O
basic	O
rules	O
and	O
segmenting	O
rules	O
on	O
DE	O
-	O
EN	O
results	O
in	O
the	O
best	O
system	O
.	O
This	O
is	O
probably	O
because	O
reordering	O
(	O
including	O
long	O
-	O
distance	O
reordering	O
)	O
is	O
performed	O
less	O
often	O
in	O
DE	O
-	O
EN	O
than	O
in	O
ZH	O
-	O
EN	O
(	O
Li	O
et	O
al	O
,	O
2016	O
)	O
which	O
makes	O
selecting	O
rules	O
less	O
preferable	O
on	O
DE	O
-	O
EN	O
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
a	O
graph	O
-	O
based	O
model	O
which	O
takes	O
subgraphs	O
as	O
the	O
basic	O
translation	O
units	O
and	O
considers	O
source	O
context	O
during	O
segmenting	O
graphs	O
into	O
subgraphs	O
.	O
Experiments	O
on	O
Chinese	O
-	O
English	O
and	O
German	O
-	O
English	O
show	O
that	O
our	O
model	O
is	O
significantly	O
better	O
than	O
the	O
conventional	O
graphbased	O
model	O
which	O
equally	O
treats	O
different	O
graph	O
segmentations	O
.	O
In	O
this	O
paper	O
,	O
source	O
context	O
is	O
used	O
as	O
hard	O
constraints	O
during	O
decoding	O
.	O
In	O
future	O
,	O
we	O
would	O
like	O
to	O
try	O
soft	O
constraints	O
.	O
In	O
addition	O
,	O
it	O
would	O
also	O
be	O
interesting	O
to	O
extend	O
this	O
model	O
using	O
a	O
synchronous	O
graph	O
grammar	O
.	O

This	O
research	O
has	O
received	O
funding	O
from	O
the	O
European	O
Union	O
's	O
Horizon	O
2020	O
research	O
and	O
innovation	O
programme	O
under	O
grant	O
agreement	O
n	O
o	O
645452	O
(	O
QT21	O
)	O
.	O
The	O
ADAPT	O
Centre	O
for	O
Digital	O
Content	O
Technology	O
is	O
funded	O
under	O
the	O
SFI	O
Research	O
Centres	O
Programme	O
(	O
Grant	O
13	O
/	O
RC/2106	O
)	O
and	O
is	O
cofunded	O
under	O
the	O
European	O
Regional	O
Development	O
Fund	O
.	O
The	O
authors	O
thank	O
all	O
anonymous	O
reviewers	O
for	O
their	O
insightful	O
comments	O
and	O
suggestions	O
.	O

We	O
consider	O
retrieval	O
-	O
based	O
QA	O
systems	O
,	O
which	O
are	O
mainly	O
constituted	O
by	O
(	O
i	O
)	O
a	O
search	O
engine	O
,	O
retrieving	O
documents	O
related	O
to	O
the	O
questions	O
;	O
and	O
(	O
ii	O
)	O
an	O
AS2	O
model	O
,	O
which	O
reranks	O
passages	O
/	O
sentences	O
extracted	O
from	O
the	O
documents	O
.	O
The	O
top	O
sentence	O
is	O
typically	O
used	O
as	O
final	O
answer	O
for	O
the	O
users	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
our	O
baseline	O
models	O
,	O
which	O
are	O
constituted	O
by	O
pointwise	O
,	O
pairwise	O
,	O
and	O
listwise	O
strategies	O
.	O

To	O
better	O
show	O
the	O
potential	O
of	O
our	O
approach	O
and	O
the	O
complexity	O
of	O
the	O
task	O
,	O
we	O
designed	O
three	O
joint	O
model	O
baselines	O
based	O
on	O
:	O
(	O
i	O
)	O
a	O
multiclassifier	O
approach	O
(	O
a	O
listwise	O
method	O
)	O
,	O
and	O
(	O
ii	O
)	O
a	O
pairwise	O
joint	O
model	O
operating	O
over	O
k	O
+	O
1	O
candidates	O
,	O
and	O
our	O
adaptation	O
of	O
KGAT	O
model	O
(	O
a	O
pairwise	O
method	O
)	O
.	O

We	O
proposed	O
the	O
Answer	O
Support	O
Reranker	O
(	O
ASR	O
)	O
,	O
which	O
uses	O
an	O
answer	O
pair	O
classifier	O
to	O
provide	O
evidence	O
to	O
a	O
target	O
answer	O
t.	O
Given	O
a	O
question	O
q	O
,	O
and	O
a	O
subset	O
of	O
its	O
top	O
-	O
k+1	O
ranked	O
answer	O
candidates	O
,	O
A	O
(	O
reranked	O
by	O
an	O
AS2	O
model	O
)	O
,	O
we	O
build	O
a	O
function	O
,	O
σ	O
:	O
Q	O
×	O
C	O
×	O
C	O
k	O
R	O
such	O
that	O
σ	O
(	O
q	O
,	O
t	O
,	O
A	O
\	O
{	O
t	O
}	O
)	O
provides	O
the	O
probability	O
of	O
t	O
to	O
be	O
correct	O
,	O
where	O
C	O
is	O
the	O
set	O
of	O
sentence	O
-	O
candidates	O
.	O
We	O
also	O
design	O
a	O
multi	O
-	O
classifier	O
MASR	O
,	O
which	O
combines	O
k	O
ASR	O
models	O
,	O
one	O
for	O
each	O
different	O
target	O
answer	O
.	O

In	O
these	O
experiments	O
,	O
we	O
compare	O
our	O
models	O
:	O
KGAT	O
,	O
ASR	O
and	O
MASR	O
with	O
pointwise	O
models	O
,	O
which	O
are	O
the	O
state	O
of	O
the	O
art	O
for	O
AS2	O
.	O
We	O
also	O
compare	O
them	O
with	O
our	O
joint	O
model	O
baselines	O
(	O
pairwise	O
and	O
listwise	O
)	O
.	O
Finally	O
,	O
we	O
provide	O
an	O
error	O
analysis	O
.	O

Compact	O
-	O
answer	O
Representation	O

Generative	O
adversarial	O
networks	O
(	O
GANs	O
)	O
(	O
Goodfellow	O
et	O
al	O
,	O
2014	O
)	O
are	O
a	O
framework	O
for	O
training	O
generative	O
models	O
based	O
on	O
game	O
theory	O
.	O
Unlike	O
the	O
original	O
GANs	O
,	O
which	O
generate	O
such	O
data	O
samples	O
as	O
images	O
and	O
compact	O
answers	O
from	O
noise	O
,	O
our	O
AGR	O
generates	O
useful	O
vector	O
representations	O
from	O
meaningful	O
text	O
passages	O
.	O
To	O
clarify	O
the	O
difference	O
,	O
we	O
explain	O
our	O
AGR	O
with	O
three	O
subnetworks	O
:	O
two	O
generators	O
,	O
F	O
and	O
R	O
,	O
and	O
a	O
discriminator	O
,	O
D	O
,	O
as	O
in	O
Fig	O
.	O
1	O
(	O
b	O
)	O
.	O
Generator	O
F	O
takes	O
as	O
input	O
passage	O
p	O
drawn	O
from	O
prior	O
passage	O
distribution	O
d	O
p	O
and	O
outputs	O
vectorp	O
as	O
a	O
fake	O
representation	O
of	O
a	O
compact	O
answer	O
.	O
We	O
call	O
F	O
a	O
fake	O
-	O
representation	O
generator	O
.	O
R	O
,	O
which	O
we	O
call	O
a	O
real	O
-	O
representation	O
generator	O
,	O
is	O
given	O
sample	O
c	O
taken	O
from	O
manually	O
created	O
compact	O
-	O
answers	O
and	O
provides	O
vector	O
c	O
as	O
a	O
real	O
-	O
representation	O
of	O
the	O
sampled	O
compactanswer	O
.	O
Discriminator	O
D	O
has	O
to	O
distinguish	O
fakerepresentationp	O
from	O
real	O
-	O
representationc	O
of	O
a	O
compact	O
answer	O
.	O
These	O
three	O
networks	O
play	O
an	O
adversarial	O
minimax	O
game	O
;	O
fake	O
-	O
representation	O
generator	O
F	O
creates	O
a	O
fake	O
compact	O
-	O
answer	O
representation	O
that	O
is	O
hard	O
for	O
the	O
discriminator	O
to	O
distinguish	O
from	O
the	O
representations	O
of	O
manually	O
created	O
compact	O
-	O
answers	O
,	O
and	O
discriminator	O
D	O
and	O
generator	O
R	O
simultaneously	O
try	O
to	O
avoid	O
being	O
duped	O
by	O
generator	O
F	O
.	O
These	O
processes	O
should	O
allow	O
generator	O
F	O
to	O
learn	O
how	O
to	O
generate	O
a	O
representation	O
of	O
a	O
proper	O
compact	O
-	O
answer	O
from	O
an	O
answer	O
passage	O
.	O
In	O
addition	O
,	O
since	O
passage	O
p	O
and	O
compact	O
answer	O
c	O
are	O
dependent	O
on	O
question	O
q	O
,	O
the	O
generation	O
of	O
the	O
compact	O
-	O
answer	O
representations	O
by	O
F	O
and	O
R	O
is	O
conditioned	O
by	O
question	O
q	O
,	O
like	O
in	O
the	O
conditional	O
GANs	O
(	O
Mirza	O
and	O
Osindero	O
,	O
2014	O
)	O
.	O
We	O
trained	O
our	O
AGR	O
with	O
the	O
following	O
minimax	O
objective	O
:	O
min	O
F	O
max	O
D	O
,	O
R	O
V	O
(	O
D	O
,	O
F	O
,	O
R	O
)	O
=	O
E	O
c∼dc	O
(	O
c	O
)	O
[	O
log	O
D	O
(	O
R	O
(	O
c	O
|	O
q	O
)	O
)	O
]	O
+	O
E	O
p∼dp	O
(	O
p	O
)	O
[	O
log	O
(	O
1	O
−	O
D	O
(	O
F	O
(	O
p	O
|	O
q	O
)	O
)	O
]	O
.	O

Are	O
Training	O
Samples	O
Correlated	O
?	O
Learning	O
to	O
Generate	O
Dialogue	O
Responses	O
with	O
Multiple	O
References	O

In	O
this	O
paper	O
,	O
we	O
tackle	O
the	O
one	O
-	O
to	O
-	O
many	O
queryresponse	O
mapping	O
problem	O
in	O
open	O
-	O
domain	O
conversation	O
and	O
propose	O
a	O
novel	O
two	O
-	O
step	O
generation	O
architecture	O
with	O
the	O
correlation	O
of	O
multiple	O
valid	O
responses	O
considered	O
.	O
Jointly	O
viewing	O
the	O
multiple	O
responses	O
as	O
a	O
response	O
bag	O
,	O
the	O
model	O
extracts	O
the	O
common	O
and	O
distinct	O
features	O
of	O
different	O
responses	O
in	O
two	O
generation	O
phases	O
respectively	O
to	O
output	O
multiple	O
diverse	O
responses	O
.	O
Experimental	O
results	O
illustrate	O
the	O
superior	O
performance	O
of	O
the	O
proposed	O
model	O
in	O
generating	O
diverse	O
and	O
appropriate	O
responses	O
compared	O
to	O
previous	O
representative	O
approaches	O
.	O
However	O
,	O
the	O
modeling	O
of	O
the	O
common	O
and	O
distinct	O
features	O
of	O
responses	O
in	O
our	O
method	O
is	O
currently	O
implicit	O
and	O
coarse	O
-	O
grained	O
.	O
Directions	O
of	O
future	O
work	O
may	O
be	O
pursuing	O
betterdefined	O
features	O
and	O
easier	O
training	O
strategies	O
.	O

We	O
would	O
like	O
to	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
constructive	O
comments	O
.	O
This	O
work	O
was	O
supported	O
by	O
the	O
National	O
Key	O
Research	O
and	O
Development	O
Program	O
of	O
China	O
(	O
No	O
.	O
2017YFC0804001	O
)	O
,	O
the	O
National	O
Science	O
Foundation	O
of	O
China	O
(	O
NSFC	O
No	O
.	O
61672058	O
;	O
NSFC	O
No	O
.	O
61876196	O
)	O
.	O
Rui	O
Yan	O
was	O
sponsored	O
by	O
CCF	O
-	O
Tencent	O
Open	O
Research	O
Fund	O
and	O
Alibaba	O
Innovative	O
Research	O
(	O
AIR	O
)	O
Fund	O
.	O

SGNMT	O
-	O
A	O
Flexible	O
NMT	O
Decoding	O
Platform	O
for	O
Quick	O
Prototyping	O
of	O
New	O
Models	O
and	O
Search	O
Strategies	O

SGNMT	O
allows	O
combining	O
any	O
number	O
of	O
predictors	O
and	O
even	O
multiple	O
instances	O
of	O
the	O
same	O
predictor	O
type	O
.	O
In	O
case	O
of	O
multiple	O
predictors	O
we	O
combine	O
the	O
predictor	O
scores	O
in	O
a	O
linear	O
model	O
.	O
The	O
following	O
list	O
illustrates	O
that	O
various	O
interesting	O
decoding	O
tasks	O
can	O
be	O
formulated	O
as	O
predictor	O
combinations	O
.	O
nmt	O
:	O
A	O
single	O
NMT	O
predictor	O
represents	O
pure	O
NMT	O
decoding	O
.	O
nmt	O
,	O
nmt	O
,	O
nmt	O
:	O
Using	O
multiple	O
NMT	O
predictors	O
is	O
a	O
natural	O
way	O
to	O
represent	O
ensemble	O
decoding	O
(	O
Hansen	O
and	O
Salamon	O
,	O
1990	O
;	O
in	O
our	O
framework	O
.	O
fst	O
,	O
nmt	O
:	O
NMT	O
decoding	O
constrained	O
to	O
an	O
FST	O
.	O
This	O
can	O
be	O
used	O
for	O
neural	O
lattice	O
rescoring	O
or	O
other	O
kinds	O
of	O
constraints	O
,	O
for	O
example	O
in	O
the	O
context	O
of	O
source	O
side	O
simplification	O
in	O
MT	O
or	O
chord	O
progressions	O
in	O
'	O
Bach	O
'	O
(	O
Tomczak	O
,	O
2016	O
)	O
.	O
The	O
fst	O
predictor	O
can	O
also	O
be	O
used	O
to	O
restrict	O
the	O
output	O
of	O
character	O
-	O
based	O
or	O
subword	O
-	O
unit	O
-	O
based	O
NMT	O
to	O
a	O
large	O
word	O
-	O
level	O
vocabulary	O
encoded	O
as	O
FSA	O
.	O
nmt	O
,	O
rnnlm	O
,	O
srilm	O
,	O
nplm	O
:	O
Combining	O
NMT	O
with	O
three	O
kinds	O
of	O
language	O
models	O
:	O
An	O
RNNLM	O
(	O
Zaremba	O
et	O
al	O
,	O
2014	O
)	O
,	O
a	O
Kneser	O
-	O
Ney	O
n	O
-	O
gram	O
LM	O
(	O
Heafield	O
et	O
al	O
,	O
2013	O
;	O
Stolcke	O
et	O
al	O
,	O
2002	O
)	O
,	O
and	O
a	O
feedforward	O
neural	O
network	O
LM	O
(	O
Vaswani	O
et	O
al	O
,	O
2013	O
)	O
.	O

This	O
paper	O
presented	O
our	O
SGNMT	O
platform	O
for	O
prototyping	O
new	O
approaches	O
to	O
MT	O
which	O
involve	O
both	O
neural	O
and	O
symbolic	O
models	O
.	O
SGNMT	O
supports	O
a	O
number	O
of	O
different	O
models	O
and	O
constraints	O
via	O
a	O
common	O
interface	O
(	O
predictors	O
)	O
,	O
and	O
various	O
search	O
strategies	O
(	O
decoders	O
)	O
.	O
Furthermore	O
,	O
SGNMT	O
focuses	O
on	O
minimizing	O
the	O
implementation	O
effort	O
for	O
adding	O
new	O
predictors	O
and	O
decoders	O
by	O
decoupling	O
scoring	O
modules	O
from	O
each	O
other	O
and	O
from	O
the	O
search	O
algorithm	O
.	O
SGNMT	O
is	O
actively	O
being	O
used	O
for	O
teaching	O
and	O
research	O
and	O
we	O
welcome	O
contributions	O
to	O
its	O
development	O
,	O
for	O
example	O
by	O
implementing	O
new	O
predictors	O
for	O
using	O
models	O
trained	O
with	O
other	O
frameworks	O
and	O
tools	O
.	O

This	O
work	O
was	O
supported	O
by	O
the	O
U.K.	O
Engineering	O
and	O
Physical	O
Sciences	O
Research	O
Council	O
(	O
EPSRC	O
grant	O
EP	O
/	O
L027623/1	O
)	O
.	O

Double	O
Perturbation	O
:	O
On	O
the	O
Robustness	O
of	O
Robustness	O
and	O
Counterfactual	O
Bias	O
Evaluation	O

Robustness	O
and	O
counterfactual	O
bias	O
are	O
usually	O
evaluated	O
on	O
a	O
test	O
dataset	O
.	O
However	O
,	O
are	O
these	O
evaluations	O
robust	O
?	O
If	O
the	O
test	O
dataset	O
is	O
perturbed	O
slightly	O
,	O
will	O
the	O
evaluation	O
results	O
keep	O
the	O
same	O
?	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
"	O
double	O
perturbation	O
"	O
framework	O
to	O
uncover	O
model	O
weaknesses	O
beyond	O
the	O
test	O
dataset	O
.	O
The	O
framework	O
first	O
perturbs	O
the	O
test	O
dataset	O
to	O
construct	O
abundant	O
natural	O
sentences	O
similar	O
to	O
the	O
test	O
data	O
,	O
and	O
then	O
diagnoses	O
the	O
prediction	O
change	O
regarding	O
a	O
single	O
-	O
word	O
substitution	O
.	O
We	O
apply	O
this	O
framework	O
to	O
study	O
two	O
perturbation	O
-	O
based	O
approaches	O
that	O
are	O
used	O
to	O
analyze	O
models	O
'	O
robustness	O
and	O
counterfactual	O
bias	O
in	O
English	O
.	O
(	O
1	O
)	O
For	O
robustness	O
,	O
we	O
focus	O
on	O
synonym	O
substitutions	O
and	O
identify	O
vulnerable	O
examples	O
where	O
prediction	O
can	O
be	O
altered	O
.	O
Our	O
proposed	O
attack	O
attains	O
high	O
success	O
rates	O
(	O
96.0	O
%	O
-	O
99.8	O
%	O
)	O
in	O
finding	O
vulnerable	O
examples	O
on	O
both	O
original	O
and	O
robustly	O
trained	O
CNNs	O
and	O
Transformers	O
.	O
(	O
2	O
)	O
For	O
counterfactual	O
bias	O
,	O
we	O
focus	O
on	O
substituting	O
demographic	O
tokens	O
(	O
e.g.	O
,	O
gender	O
,	O
race	O
)	O
and	O
measure	O
the	O
shift	O
of	O
the	O
expected	O
prediction	O
among	O
constructed	O
sentences	O
.	O
Our	O
method	O
is	O
able	O
to	O
reveal	O
the	O
hidden	O
model	O
biases	O
not	O
directly	O
shown	O
in	O
the	O
test	O
dataset	O
.	O
Our	O
code	O
is	O
available	O
at	O
https://github.com/chong	O
-	O
z/	O
nlp	O
-	O
second	O
-	O
order	O
-	O
attack	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
the	O
double	O
perturbation	O
framework	O
which	O
focuses	O
on	O
identifying	O
vulnerable	O
examples	O
within	O
a	O
small	O
neighborhood	O
of	O
the	O
test	O
dataset	O
.	O
The	O
framework	O
consists	O
of	O
a	O
neighborhood	O
perturbation	O
and	O
a	O
word	O
substitution	O
.	O
We	O
start	O
with	O
defining	O
word	O
substitutions	O
.	O

With	O
the	O
proposed	O
double	O
perturbation	O
framework	O
,	O
we	O
design	O
two	O
black	O
-	O
box	O
attacks	O
1	O
to	O
identify	O
vulnerable	O
examples	O
within	O
the	O
neighborhood	O
of	O
the	O
test	O
set	O
.	O
We	O
aim	O
at	O
evaluating	O
the	O
robustness	O
for	O
inputs	O
beyond	O
the	O
test	O
set	O
.	O

In	O
this	O
section	O
,	O
we	O
evaluate	O
the	O
second	O
-	O
order	O
robustness	O
of	O
existing	O
models	O
and	O
show	O
the	O
quality	O
of	O
our	O
constructed	O
vulnerable	O
examples	O
.	O

We	O

We	O
perform	O
human	O
evaluation	O
on	O
the	O
examples	O
constructed	O
by	O
SO	O
-	O
Beam	O
.	O
Specifically	O
,	O
we	O
randomly	O

In	O
addition	O
to	O
evaluating	O
second	O
-	O
order	O
robustness	O
,	O
we	O
further	O
extend	O
the	O
double	O
perturbation	O
framework	O
(	O
2	O
)	O
to	O
evaluate	O
counterfactual	O
biases	O
by	O
setting	O
p	O
to	O
pairs	O
of	O
protected	O
tokens	O
.	O
We	O
show	O
that	O
our	O
method	O
can	O
reveal	O
the	O
hidden	O
model	O
bias	O
.	O

In	O
this	O
section	O
,	O
we	O
use	O
gender	O
bias	O
as	O
a	O
running	O
example	O
,	O
and	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
method	O
by	O
revealing	O
the	O
hidden	O
model	O
bias	O
.	O
We	O
provide	O
additional	O
results	O
in	O
Appendix	O
A.4	O
.	O

First	O
-	O
order	O
robustness	O
evaluation	O
.	O
A	O
line	O
of	O
work	O
has	O
been	O
proposed	O
to	O
study	O
the	O
vulnerability	O
of	O
natural	O
language	O
models	O
,	O
through	O
transformations	O
such	O
as	O
character	O
-	O
level	O
perturbations	O
(	O
Ebrahimi	O
et	O
al	O
,	O
2018	O
)	O
,	O
word	O
-	O
level	O
perturbations	O
(	O
Jin	O
et	O
al	O
,	O
2019	O
;	O
Ren	O
et	O
al	O
,	O
2019	O
;	O
Cheng	O
et	O
al	O
,	O
2020	O
;	O
Li	O
et	O
al	O
,	O
2020	O
)	O
,	O
prepending	O
or	O
appending	O
a	O
sequence	O
(	O
Jia	O
and	O
Liang	O
,	O
2017	O
;	O
Wallace	O
et	O
al	O
,	O
2019a	O
)	O
,	O
and	O
generative	O
models	O
(	O
Zhao	O
et	O
al	O
,	O
2018b	O
)	O
.	O
They	O
focus	O
on	O
constructing	O
adversarial	O
examples	O
from	O
the	O
test	O
set	O
that	O
alter	O
the	O
prediction	O
,	O
whereas	O
our	O
methods	O
focus	O
on	O
finding	O
vulnerable	O
examples	O
beyond	O
the	O
test	O
set	O
whose	O
prediction	O
can	O
be	O
altered	O
.	O
Robustness	O
beyond	O
the	O
test	O
set	O
.	O
Several	O
works	O
have	O
studied	O
model	O
robustness	O
beyond	O
test	O
sets	O
but	O
mostly	O
focused	O
on	O
computer	O
vision	O
tasks	O
.	O
Zhang	O
et	O
al	O
(	O
2019	O
)	O
demonstrate	O
that	O
a	O
robustly	O
trained	O
model	O
could	O
still	O
be	O
vulnerable	O
to	O
small	O
perturbations	O
if	O
the	O
input	O
comes	O
from	O
a	O
distribution	O
only	O
slightly	O
different	O
than	O
a	O
normal	O
test	O
set	O
(	O
e.g.	O
,	O
images	O
with	O
slightly	O
different	O
contrasts	O
)	O
.	O
Hendrycks	O
and	O
Dietterich	O
(	O
2019	O
)	O
study	O
more	O
sources	O
of	O
common	O
corruptions	O
such	O
as	O
brightness	O
,	O
motion	O
blur	O
and	O
fog	O
.	O
Unlike	O
in	O
computer	O
vision	O
where	O
simple	O
image	O
transformations	O
can	O
be	O
used	O
,	O
in	O
our	O
natural	O
language	O
setting	O
,	O
generating	O
a	O
valid	O
example	O
beyond	O
test	O
set	O
is	O
more	O
challenging	O
because	O
language	O
semantics	O
and	O
grammar	O
must	O
be	O
maintained	O
.	O
Counterfactual	O
fairness	O
.	O
Kusner	O
et	O
al	O
(	O
2017	O
)	O
propose	O
counterfactual	O
fairness	O
and	O
consider	O
a	O
model	O
fair	O
if	O
changing	O
the	O
protected	O
attributes	O
does	O
not	O
affect	O
the	O
distribution	O
of	O
prediction	O
.	O
We	O
follow	O
the	O
definition	O
and	O
focus	O
on	O
evaluating	O
the	O
counterfactual	O
bias	O
between	O
pairs	O
of	O
protected	O
tokens	O
.	O
Existing	O
literature	O
quantifies	O
fairness	O
on	O
a	O
test	O
dataset	O
or	O
through	O
templates	O
(	O
Feldman	O
et	O
al	O
,	O
2015	O
;	O
Kiritchenko	O
and	O
Mohammad	O
,	O
2018	O
;	O
May	O
et	O
al	O
,	O
2019	O
;	O
.	O
For	O
instance	O
,	O
Garg	O
et	O
al	O
(	O
2019	O
)	O
quantify	O
the	O
absolute	O
counterfactual	O
token	O
fairness	O
gap	O
on	O
the	O
test	O
set	O
;	O
Prabhakaran	O
et	O
al	O
(	O
2019	O
)	O
study	O
perturbation	O
sensitivity	O
for	O
named	O
entities	O
on	O
a	O
given	O
set	O
of	O
corpus	O
.	O
Wallace	O
et	O
al	O
(	O
2019b	O
)	O
;	O
Sheng	O
et	O
al	O
(	O
2019Sheng	O
et	O
al	O
(	O
,	O
2020	O
study	O
how	O
language	O
generation	O
models	O
respond	O
differently	O
to	O
prompt	O
sentences	O
containing	O
mentions	O
of	O
different	O
demographic	O
groups	O
.	O
In	O
contrast	O
,	O
our	O
method	O
quantifies	O
the	O
bias	O
on	O
the	O
constructed	O
neighborhood	O
.	O

This	O
work	O
proposes	O
the	O
double	O
perturbation	O
framework	O
to	O
identify	O
model	O
weaknesses	O
beyond	O
the	O
test	O
dataset	O
,	O
and	O
study	O
a	O
stronger	O
notion	O
of	O
robustness	O
and	O
counterfactual	O
bias	O
.	O
We	O
hope	O
that	O
our	O
work	O
can	O
stimulate	O
the	O
research	O
on	O
further	O
improving	O
the	O
robustness	O
and	O
fairness	O
of	O
natural	O
language	O
models	O
.	O

We	O
thank	O
anonymous	O
reviewers	O
for	O
their	O
helpful	O
feedback	O
.	O
We	O
thank	O
UCLA	O
-	O
NLP	O
group	O
for	O
the	O
valuable	O
discussions	O
and	O
comments	O
.	O
The	O
research	O
is	O
supported	O
NSF	O
#	O
1927554	O
,	O
#	O
1901527	O
,	O
#	O
2008173	O
and	O
#	O
2048280	O
and	O
an	O
Amazon	O
Research	O
Award	O
.	O

Original	O
54	O
%	O
Positive	O
(	O
69	O
%	O
Positive	O
)	O
for	O
the	O
most	O
part	O
,	O
director	O
anne	O
-	O
sophie	O
birot	O
's	O
first	O
feature	O
is	O
a	O
sensitive	O
,	O
overly	O
(	O
extraordinarily	O
)	O
well	O
-	O
acted	O
drama	O
.	O
Vulnerable	O
53	O
%	O
Negative	O
(	O
62	O
%	O
Positive	O
)	O
for	O
the	O
most	O
part	O
,	O
director	O
anne	O
-	O
sophie	O
benoit	O
's	O
first	O
feature	O
is	O
a	O
sensitive	O
,	O
overly	O
(	O
extraordinarily	O
)	O
well	O
-	O
acted	O
drama	O
.	O
Original	O
73	O
%	O
Negative	O
(	O
56	O
%	O
Negative	O
)	O
the	O
cold	O
(	O
colder	O
)	O
turkey	O
would	O
'	O
ve	O
been	O
a	O
far	O
better	O
title	O
.	O
Vulnerable	O
61	O
%	O
Negative	O
(	O
62	O
%	O
Positive	O
)	O
the	O
cold	O
(	O
colder	O
)	O
turkey	O
might	O
'	O
ve	O
been	O
a	O
far	O
better	O
title	O
.	O

70	O
%	O
Negative	O
(	O
65	O
%	O
Negative	O
)	O
it	O
's	O
just	O
disappointingly	O
superficial	O
-	O
a	O
movie	O
that	O
has	O
all	O
the	O
elements	O
necessary	O
to	O
be	O
a	O
fascinating	O
,	O
involving	O
character	O
study	O
,	O
but	O
never	O
does	O
more	O
than	O
scratch	O
the	O
shallow	O
(	O
surface	O
)	O
.	O
Vulnerable	O
52	O
%	O
Negative	O
(	O
55	O
%	O
Positive	O
)	O
it	O
's	O
just	O
disappointingly	O
short	O
-	O
a	O
movie	O
that	O
has	O
all	O
the	O
elements	O
necessary	O
to	O
be	O
a	O
fascinating	O
,	O
involving	O
character	O
study	O
,	O
but	O
never	O
does	O
more	O
than	O
scratch	O
the	O
shallow	O
(	O
surface	O
)	O
.	O
Original	O
79	O
%	O
Negative	O
(	O
72	O
%	O
Negative	O
)	O
schaeffer	O
has	O
to	O
find	O
some	O
hook	O
on	O
which	O
to	O
hang	O
his	O
persistently	O
useless	O
movies	O
,	O
and	O
it	O
might	O
as	O
well	O
be	O
the	O
resuscitation	O
(	O
revival	O
)	O
of	O
the	O
middleaged	O
character	O
.	O
Vulnerable	O
57	O
%	O
Negative	O
(	O
57	O
%	O
Positive	O
)	O
schaeffer	O
has	O
to	O
find	O
some	O
hook	O
on	O
which	O
to	O
hang	O
his	O
persistently	O
entertaining	O
movies	O
,	O
and	O
it	O
might	O
as	O
well	O
be	O
the	O
resuscitation	O
(	O
revival	O
)	O
of	O
the	O
middleaged	O
character	O
.	O
Original	O
64	O
%	O
Positive	O
(	O
58	O
%	O
Positive	O
)	O
the	O
primitive	O
force	O
of	O
this	O
film	O
seems	O
to	O
bubble	O
up	O
from	O
the	O
vast	O
collective	O
memory	O
of	O
the	O
combatants	O
(	O
militants	O
)	O
.	O
Vulnerable	O
52	O
%	O
Positive	O
(	O
53	O
%	O
Negative	O
)	O
the	O
primitive	O
force	O
of	O
this	O
film	O
seems	O
to	O
bubble	O
down	O
from	O
the	O
vast	O
collective	O
memory	O
of	O
the	O
combatants	O
(	O
militants	O
)	O
.	O
Original	O
64	O
%	O
Positive	O
(	O
74	O
%	O
Positive	O
)	O
on	O
this	O
troublesome	O
(	O
tricky	O
)	O
topic	O
,	O
tadpole	O
is	O
very	O
much	O
a	O
step	O
in	O
the	O
right	O
direction	O
,	O
with	O
its	O
blend	O
of	O
frankness	O
,	O
civility	O
and	O
compassion	O
.	O
Vulnerable	O
55	O
%	O
Negative	O
(	O
56	O
%	O
Positive	O
)	O
on	O
this	O
troublesome	O
(	O
tricky	O
)	O
topic	O
,	O
tadpole	O
is	O
very	O
much	O
a	O
step	O
in	O
the	O
right	O
direction	O
,	O
losing	O
its	O
blend	O
of	O
frankness	O
,	O
civility	O
and	O
compassion	O
.	O

We	O
would	O
like	O
to	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
helpful	O
comments	O
.	O
This	O
research	O
is	O
partially	O
supported	O
by	O
Ministry	O
of	O
Education	O
,	O
Singapore	O
,	O
under	O
its	O
Academic	O
Research	O
Fund	O
(	O
AcRF	O
)	O
Tier	O
2	O
Programme	O
(	O
MOE	O
AcRF	O
Tier	O
2	O
Award	O
No	O
:	O
MOE2017	O
-	O
T2	O
-	O
1	O
-	O
156	O
)	O
.	O
Any	O
opinions	O
,	O
findings	O
and	O
conclusions	O
or	O
recommendations	O
expressed	O
in	O
this	O
material	O
are	O
those	O
of	O
the	O
authors	O
and	O
do	O
not	O
reflect	O
the	O
views	O
of	O
the	O
Ministry	O
of	O
Education	O
,	O
Singapore	O
.	O

Deep	O
learning	O
for	O
language	O
understanding	O
of	O
mental	O
health	O
concepts	O
derived	O
from	O
Cognitive	O
Behavioural	O
Therapy	O

While	O
our	O
main	O
emphasis	O
was	O
on	O
thinking	O
errors	O
and	O
emotions	O
,	O
we	O
also	O
defined	O
a	O
small	O
set	O
of	O
situations	O
.	O
The	O
list	O
of	O
situations	O
again	O
evolved	O
during	O
the	O
early	O
days	O
of	O
annotation	O
,	O
with	O
a	O
longer	O
original	O
list	O
being	O
reduced	O
down	O
,	O
for	O
simplicity	O
.	O
Again	O
,	O
it	O
is	O
possible	O
for	O
more	O
than	O
one	O
situation	O
(	O
for	O
example	O
Work	O
and	O
Relationships	O
)	O
to	O
apply	O
to	O
a	O
single	O
problem	O
.	O
The	O
considered	O
situations	O
are	O
given	O
in	O
Table	O
3	O
.	O

This	O
work	O
was	O
funded	O
by	O
EPSRC	O
project	O
Natural	O
speech	O
Automated	O
Utility	O
for	O
Mental	O
health	O
(	O
NAUM	O
)	O
,	O
award	O
reference	O
EP	O
/	O
P017746/1	O
.	O
The	O
authors	O
would	O
also	O
like	O
to	O
thank	O
anonymous	O
reviewers	O
for	O
their	O
valuable	O
comments	O
.	O
The	O
code	O
is	O
available	O
at	O
https://github.com/YinpeiDai/NAUM	O

This	O
work	O
was	O
made	O
possible	O
in	O
part	O
through	O
support	O
of	O
the	O
United	O
States	O
Agency	O
for	O
International	O
Development	O
.	O
The	O
opinions	O
expressed	O
herein	O
are	O
those	O
of	O
the	O
authors	O
and	O
do	O
not	O
necessarily	O
reflect	O
the	O
views	O
of	O
the	O
United	O
States	O
Agency	O
for	O
International	O
Development	O
or	O
the	O
US	O
Government	O
.	O

Will	O
I	O
Sound	O
Like	O
Me	O
?	O
Improving	O
Persona	O
Consistency	O
in	O
Dialogues	O
through	O
Pragmatic	O
Self	O
-	O
Consciousness	O

In	O
the	O
study	O
of	O
dialogue	O
agents	O
,	O
consistency	O
has	O
been	O
a	O
long	O
-	O
standing	O
issue	O
.	O
To	O
resolve	O
this	O
,	O
much	O
research	O
has	O
been	O
conducted	O
to	O
endow	O
dialogue	O
agents	O
with	O
personas	O
.	O
Li	O
et	O
al	O
(	O
2016	O
)	O
propose	O
to	O
encode	O
persona	O
in	O
embeddings	O
and	O
Zhang	O
et	O
al	O
(	O
2018	O
)	O
introduce	O
a	O
persona	O
-	O
conditioned	O
dialogue	O
dataset	O
.	O
On	O
top	O
of	O
these	O
works	O
,	O
many	O
efforts	O
have	O
been	O
made	O
to	O
improve	O
consistency	O
.	O
In	O
spite	O
of	O
such	O
recent	O
significant	O
progress	O
,	O
there	O
is	O
much	O
room	O
for	O
improving	O
persona	O
-	O
based	O
dialogue	O
agents	O
.	O
We	O
observe	O
that	O
even	O
the	O
best	O
performing	O
persona	O
-	O
based	O
generative	O
models	O
(	O
See	O
et	O
al	O
,	O
2019	O
;	O
Wolf	O
et	O
al	O
,	O
2019b	O
;	O
I	O
like	O
to	O
stay	O
at	O
home	O
.	O

We	O
would	O
like	O
to	O
thank	O
Reuben	O
Cohn	O
-	O
Gordon	O
,	O
Sean	O
Welleck	O
,	O
Junhyug	O
Noh	O
and	O
Jiwan	O
Chung	O
for	O
their	O
valuable	O
comments	O
.	O
We	O
also	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
thoughtful	O
suggestions	O
on	O
this	O
work	O
.	O

Analogical	O
Reasoning	O
on	O
Chinese	O
Morphological	O
and	O
Semantic	O
Relations	O

Morphology	O
concerns	O
the	O
internal	O
structure	O
of	O
words	O
.	O
There	O
is	O
a	O
common	O
belief	O
that	O
Chinese	O
is	O
a	O
morphologically	O
impoverished	O
language	O
since	O
a	O
morpheme	O
mostly	O
corresponds	O
to	O
an	O
orthographic	O
character	O
,	O
and	O
it	O
lacks	O
apparent	O
distinctions	O
between	O
roots	O
and	O
affixes	O
.	O
However	O
,	O
Packard	O
(	O
2000	O
)	O
suggests	O
that	O
Chinese	O
has	O
a	O
different	O
morphological	O
system	O
because	O
it	O
selects	O
different	O
"	O
settings	O
"	O
on	O
parameters	O
shared	O
by	O
all	O
languages	O
.	O
We	O
will	O
clarify	O
this	O
special	O
system	O
by	O
mapping	O
its	O
morphological	O
analogies	O
into	O
two	O
processes	O
:	O
reduplication	O
and	O
semi	O
-	O
affixation	O
.	O

Reduplication	O
means	O
a	O
morpheme	O
is	O
repeated	O
to	O
form	O
a	O
new	O
word	O
,	O
which	O
is	O
semantically	O
and/or	O
syntactically	O
distinct	O
from	O
the	O
original	O
morpheme	O
,	O
e.g.	O
the	O
word	O
"	O
tiān	O
-	O
tiān	O
"	O
(	O
day	O
day	O
)	O
in	O
Figure	O
1	O
(	O
b	O
)	O
means	O
"	O
everyday	O
"	O
.	O
By	O
analyzing	O
all	O
the	O
word	O
categories	O
in	O
Chinese	O
,	O
we	O
find	O
that	O
nouns	O
,	O
verbs	O
,	O
adjectives	O
,	O
adverbs	O
,	O
and	O
measure	O
words	O
have	O
reduplication	O
abilities	O
.	O
Given	O
distinct	O
morphemes	O
A	O
and	O
B	O
,	O
we	O
summarize	O
6	O
repetition	O
patterns	O
in	O
Figure	O
2	O
.	O
Each	O
pattern	O
may	O
have	O
one	O
or	O
more	O
morphological	O
functions	O
.	O
Taking	O
Pattern	O
1	O
(	O
A	O
AA	O
)	O
as	O
an	O
example	O
,	O
noun	O
morphemes	O
could	O
form	O
kinship	O
terms	O
or	O
yield	O
every	O
/	O
each	O
meaning	O
.	O
For	O
verbs	O
,	O
it	O
signals	O
doing	O
something	O
a	O
little	O
bit	O
or	O
things	O
happen	O
briefly	O
.	O
AA	O
reduplication	O
could	O
also	O
intensify	O
an	O
adjective	O
or	O
transform	O
it	O
to	O
an	O
adverb	O
.	O
bà	O
(	O
dad	O
)	O
bà	O
-	O
bà	O
(	O
dad	O
)	O
tiān	O
(	O
day	O
)	O
tiān	O
-	O
tiān	O
(	O
everyday	O
)	O
shuō	O
(	O
say	O
)	O
shuō	O
-	O
shuo	O
(	O
say	O
a	O
little	O
)	O
kàn	O
(	O
look	O
)	O
kàn	O
-	O
kàn	O
(	O
have	O
a	O
brief	O
look	O
)	O
dà	O
(	O
big	O
)	O
dà	O
-	O
dà	O
(	O
very	O
big	O
;	O
greatly	O
)	O
shēn	O
(	O
deep	O
)	O
shēn	O
-	O
shēn	O
(	O
deeply	O
)	O
1	O
https://github.com/Embedding/Chinese	O
-	O
Word	O
-	O
Vectors	O

Affixation	O
is	O
a	O
morphological	O
process	O
whereby	O
a	O
bound	O
morpheme	O
(	O
an	O
affix	O
)	O
is	O
attached	O
to	O
roots	O
or	O
stems	O
to	O
form	O
new	O
language	O
units	O
.	O
Chinese	O
is	O
a	O
typical	O
isolating	O
language	O
that	O
has	O
few	O
affixes	O
.	O
Liu	O
et	O
al	O
(	O
2001	O
)	O
points	O
out	O
that	O
although	O
affixes	O
are	O
rare	O
in	O
Chinese	O
,	O
there	O
are	O
some	O
components	O
behaving	O
like	O
affixes	O
and	O
can	O
also	O
be	O
used	O
as	O
independent	O
lexemes	O
.	O
They	O
are	O
called	O
semi	O
-	O
affixes	O
.	O
To	O
model	O
the	O
semi	O
-	O
affixation	O
process	O
,	O
we	O
uncover	O
21	O
semi	O
-	O
prefixes	O
and	O
41	O
semi	O
-	O
suffixes	O
.	O
These	O
semi	O
-	O
suffixes	O
can	O
be	O
used	O
to	O
denote	O
changes	O
of	O
meaning	O
or	O
part	O
of	O
speech	O
.	O
For	O
example	O
,	O
the	O
semi	O
-	O
prefix	O
"	O
dì	O
-	O
"	O
could	O
be	O
added	O
to	O
numerals	O
to	O
form	O
ordinal	O
numbers	O
,	O
and	O
the	O
semi	O
-	O
suffix	O
"	O
-	O
zi	O
"	O
is	O
able	O
to	O
nominalize	O
an	O
adjective	O
:	O
yī	O
(	O
one	O
)	O
dì	O
-	O
yī	O
(	O
first	O
)	O
èr	O
(	O
two	O
)	O
dì	O
-	O
èr	O
(	O
second	O
)	O
pàng	O
(	O
fat	O
)	O
pàng	O
-	O
zi	O
(	O
a	O
fat	O
man	O
)	O
shòu	O
(	O
thin	O
)	O
shòu	O
-	O
zi	O
(	O
a	O
thin	O
man	O
)	O

To	O
investigate	O
semantic	O
knowledge	O
reasoning	O
,	O
we	O
present	O
28	O
semantic	O
relations	O
in	O
four	O
aspects	O
:	O
geography	O
,	O
history	O
,	O
nature	O
,	O
and	O
people	O
.	O
Among	O
them	O
we	O
inherit	O
a	O
few	O
relations	O
from	O
English	O
datasets	O
,	O
e.g.	O
country	O
-	O
capital	O
and	O
family	O
members	O
,	O
while	O
the	O
rest	O
of	O
them	O
are	O
proposed	O
originally	O
on	O
the	O
basis	O
of	O
our	O
observation	O
of	O
Chinese	O
lexical	O
knowledge	O
.	O
For	O
example	O
,	O
a	O
Chinese	O
province	O
may	O
have	O
its	O
own	O
abbreviation	O
,	O
capital	O
city	O
,	O
and	O
representative	O
drama	O
,	O
which	O
could	O
form	O
rich	O
semantic	O
analogies	O
:	O
ān	O
-	O
huī	O
vs	O
zhè	O
-	O
jiāng	O
(	O
province	O
)	O
wǎn	O
vs	O
zhè	O
(	O
abbreviation	O
)	O
hé	O
-	O
féi	O
vs	O
háng	O
-	O
zhōu	O
(	O
capital	O
)	O
huáng	O
-	O
méi	O
-	O
xì	O
vs	O
yuè	O
-	O
jù	O
(	O
drama	O
)	O
We	O
also	O
address	O
novel	O
relations	O
that	O
could	O
be	O
used	O
for	O
other	O
languages	O
,	O
e.g.	O
scientists	O
and	O
their	O
findings	O
,	O
companies	O
and	O
their	O
founders	O
.	O

This	O
work	O
is	O
supported	O
by	O
the	O
Fundamental	O
Research	O
Funds	O
for	O
the	O
Central	O
Universities	O
,	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
with	O
Grant	O
(	O
No.61472428	O
)	O
and	O
Chinese	O
Testing	O
International	O
Project	O
(	O
No	O
.	O
CTI2017B12	O
)	O
.	O

.	O
277	O
.491	O
.625	O
.175	O
.199	O
.134	O
.251	O
.189	O
.146	O
.147	O
.250	O
.189	O
.181	O
Combination	O
15.9	O
G	O
.872	O
.994	O
.710	O
.223	O
.300	O
.234	O
.518	O
.321	O
.662	O
.293	O
.310	O
.307	O
.467	O

The	O
1992	O
Los	O
Angeles	O
riots	O
,	O
also	O
known	O
as	O
the	O
Rodney	O
King	O
riots	O
were	O
a	O
series	O
of	O
riots	O
,	O
lootings	O
,	O
arsons	O
,	O
and	O
civil	O
disturbances	O
that	O
occurred	O
in	O
Los	O
Angeles	O
County	O
,	O
California	O
in	O
April	O
and	O
May	O
1992	O
.	O
[	O
wiki	O
/	O
Los	O
Angeles	O
County	O
]	O
Los	O
Angeles	O
County	O
,	O
officially	O
the	O
County	O
of	O
Los	O
Angeles	O
,	O
is	O
the	O
most	O
populous	O
county	O
in	O
the	O
USA	O
.	O
assign	O
a	O
label	O
whether	O
,	O
given	O
the	O
evidence	O
,	O
the	O
claim	O
is	O
SUPPORTED	O
,	O
REFUTED	O
or	O
whether	O
there	O
is	O
NOTENOUGHINFO	O
in	O
Wikipedia	O
to	O
reach	O
a	O
conclusion	O
.	O
In	O
16.82	O
%	O
of	O
cases	O
,	O
claims	O
required	O
the	O
combination	O
of	O
more	O
than	O
one	O
sentence	O
as	O
supporting	O
or	O
refuting	O
evidence	O
.	O
An	O
example	O
is	O
provided	O
in	O
Figure	O
1	O
.	O

The	O
first	O
Fact	O
Extraction	O
and	O
VERification	O
shared	O
task	O
attracted	O
submissions	O
from	O
86	O
submissions	O
from	O
23	O
teams	O
.	O
19	O
of	O
these	O
teams	O
exceeded	O
the	O
score	O
of	O
the	O
baseline	O
presented	O
in	O
Thorne	O
et	O
al	O
(	O
2018	O
)	O
.	O
For	O
the	O
teams	O
which	O
provided	O
a	O
system	O
description	O
,	O
we	O
highlighted	O
the	O
approaches	O
,	O
identifying	O
commonalities	O
and	O
features	O
that	O
could	O
be	O
further	O
explored	O
.	O
Future	O
work	O
will	O
address	O
limitations	O
in	O
humanannotated	O
evidence	O
and	O
explore	O
other	O
subtasks	O
needed	O
to	O
predict	O
the	O
veracity	O
of	O
information	O
extracted	O
from	O
untrusted	O
sources	O
.	O

Our	O
system	O
was	O
developed	O
using	O
a	O
heuristicsbased	O
approach	O
for	O
evidence	O
extraction	O
and	O
a	O
modified	O
version	O
of	O
the	O
inference	O
model	O
by	O
Parikh	O
et	O
al	O
(	O
2016	O
)	O
for	O
classification	O
into	O
refute	O
,	O
support	O
,	O
or	O
not	O
enough	O
info	O
.	O
Our	O
process	O
is	O
broken	O
down	O
into	O
three	O
distinct	O
phases	O
.	O
First	O
,	O
potentially	O
relevant	O
documents	O
are	O
gathered	O
based	O
on	O
key	O
words	O
/	O
phrases	O
in	O
the	O
claim	O
that	O
appear	O
in	O
the	O
wiki	O
dump	O
.	O
Second	O
,	O
any	O
possible	O
evidence	O
sentences	O
inside	O
those	O
documents	O
are	O
extracted	O
by	O
breaking	O
down	O
the	O
claim	O
into	O
named	O
entities	O
plus	O
nouns	O
and	O
finding	O
any	O
sentences	O
which	O
match	O
those	O
entities	O
,	O
while	O
allowing	O
for	O
various	O
exceptions	O
and	O
additional	O
potential	O
criteria	O
to	O
increase	O
recall	O
.	O
Finally	O
,	O
every	O
sentences	O
is	O
classified	O
into	O
one	O
of	O
the	O
three	O
categories	O
by	O
the	O
inference	O
tool	O
,	O
after	O
additional	O
vectors	O
are	O
added	O
based	O
on	O
named	O
entity	O
types	O
.	O
NEI	O
sentences	O
are	O
discarded	O
and	O
the	O
highest	O
scored	O
label	O
of	O
the	O
remaining	O
sentences	O
is	O
assigned	O
to	O
the	O
claim	O
.	O

In	O
our	O
approach	O
we	O
used	O
a	O
sentence	O
wise	O
approach	O
in	O
all	O
components	O
.	O
To	O
find	O
the	O
sentences	O
we	O
set	O
up	O
a	O
Solr	O
database	O
and	O
indexed	O
every	O
sentence	O
including	O
information	O
about	O
the	O
article	O
where	O
the	O
sentence	O
is	O
from	O
.	O
We	O
created	O
queries	O
based	O
on	O
the	O
named	O
entities	O
and	O
noun	O
chunks	O
of	O
the	O
claims	O
.	O
For	O
the	O
entailment	O
task	O
we	O
used	O
a	O
Decomposable	O
Attention	O
Model	O
similar	O
to	O
the	O
one	O
used	O
in	O
the	O
baseline	O
approach	O
.	O
But	O
instead	O
of	O
comparing	O
the	O
claim	O
with	O
all	O
top	O
5	O
sentences	O
at	O
once	O
we	O
treat	O
every	O
sentence	O
separately	O
.	O
The	O
results	O
for	O
the	O
top	O
5	O
sentence	O
where	O
then	O
joined	O
with	O
an	O
ensemble	O
learner	O
incl	O
.	O
the	O
rank	O
of	O
the	O
sentence	O
retriever	O
of	O
the	O
wikipedia	O
sentences	O
.	O

You	O
Do	O
n't	O
Know	O
My	O
Favorite	O
Color	O
:	O
Preventing	O
Dialogue	O
Representations	O
from	O
Revealing	O
Speakers	O
'	O
Private	O
Personas	O

In	O
this	O
section	O
,	O
we	O
conduct	O
experiments	O
to	O
evaluate	O
the	O
performance	O
of	O
privacy	O
and	O
utility	O
for	O
the	O
proposed	O
defense	O
learning	O
strategies	O
.	O
In	O
Section	O
5.1	O
,	O
we	O
give	O
our	O
experimental	O
settings	O
in	O
detail	O
.	O
In	O
Section	O
5.2	O
,	O
we	O
show	O
the	O
attacking	O
performance	O
with	O
and	O
without	O
defense	O
.	O
In	O
Section	O
5.3	O
,	O
we	O
perform	O
ablation	O
study	O
on	O
defense	O
objectives	O
.	O
In	O
Section	O
5.4	O
,	O
we	O
use	O
automatic	O
metrics	O
to	O
evaluate	O
chatbots	O
'	O
utility	O
.	O
We	O
conduct	O
various	O
attack	O
setups	O
in	O
Section	O
5.5	O
and	O
perform	O
a	O
case	O
study	O
in	O
Section	O
5.6	O
.	O

To	O
show	O
an	O
intuition	O
view	O
on	O
utility	O
,	O
we	O
provide	O
one	O
generation	O
sample	O
shown	O
in	O
Figure	O
5	O
.	O
Both	O
LM	O
and	O
LM+KL+MI	O
are	O
able	O
to	O
generate	O
fluent	O
and	O
proper	O
relies	O
.	O
Moreover	O
,	O
they	O
tend	O
to	O
maintain	O
coherence	O
with	O
previous	O
contexts	O
.	O
For	O
example	O
,	O
it	O
is	O
mentioned	O
in	O
the	O
context	O
that	O
Human	O
B	O
is	O
a	O
vegan	O
and	O
both	O
chatbots	O
respond	O
that	O
they	O
do	O
not	O
eat	O
meat	O
for	O
the	O
food	O
preference	O
.	O
This	O
generation	O
example	O
shows	O
that	O
proposed	O
defense	O
learning	O
objectives	O
preserve	O
the	O
model	O
utility	O
.	O

Here	O
,	O
we	O
give	O
two	O
more	O
examples	O
of	O
the	O
persona	O
inference	O
attacks	O
in	O
Table	O
6	O
.	O
The	O
first	O
example	O
shows	O
one	O
successful	O
defense	O
.	O
For	O
the	O
second	O
example	O
,	O
both	O
attackers	O
with	O
and	O
without	O
defense	O
fail	O
to	O
predict	O
the	O
ground	O
truth	O
persona	O
.	O
Still	O
,	O
we	O
can	O
see	O
that	O
LM+KL+MI	O
predicts	O
personas	O
that	O
are	O
irrelevant	O
to	O
the	O
context	O
.	O
However	O
,	O
LM	O
's	O
output	O
"	O
I	O
know	O
how	O
to	O
play	O
the	O
guitar	O
.	O
"	O
is	O
much	O
closer	O
to	O
the	O
context	O
about	O
music	O
and	O
instruments	O
.	O
Without	O
any	O
defense	O
,	O
the	O
above	O
examples	O
show	O
that	O
the	O
attacker	O
model	O
can	O
still	O
predict	O
context	O
-	O
aware	O
personas	O
even	O
if	O
its	O
predictions	O
are	O
wrong	O
.	O
After	O
applying	O
the	O
proposed	O
defenses	O
,	O
the	O
attacker	O
model	O
can	O
not	O
predict	O
meaningful	O
personas	O
relevant	O
to	O
the	O
context	O
.	O

The	O
authors	O
of	O
this	O
paper	O
were	O
supported	O
by	O
the	O
NSFC	O
Fund	O
(	O
U20B2053	O
)	O
from	O
the	O
NSFC	O
of	O
China	O
,	O
the	O
RIF	O
(	O
R6020	O
-	O
19	O
and	O
R6021	O
-	O
20	O
)	O
and	O
the	O
GRF	O
(	O
16211520	O

Alignment	O
verification	O
to	O
improve	O
NMT	O
translation	O
towards	O
highly	O
inflectional	O
languages	O
with	O
limited	O
resources	O

The	O
present	O
article	O
studies	O
translation	O
quality	O
when	O
limited	O
training	O
data	O
is	O
available	O
to	O
translate	O
towards	O
morphologically	O
rich	O
languages	O
.	O
The	O
starting	O
point	O
is	O
a	O
neural	O
MT	O
system	O
,	O
used	O
to	O
train	O
translation	O
models	O
with	O
only	O
publicly	O
available	O
parallel	O
data	O
.	O
An	O
initial	O
analysis	O
of	O
the	O
translation	O
output	O
has	O
shown	O
that	O
quality	O
is	O
sub	O
-	O
optimal	O
,	O
mainly	O
due	O
to	O
the	O
insufficient	O
amount	O
of	O
training	O
data	O
.	O
To	O
improve	O
translation	O
,	O
a	O
hybridized	O
solution	O
is	O
proposed	O
,	O
using	O
an	O
ensemble	O
of	O
relatively	O
simple	O
NMT	O
systems	O
trained	O
with	O
different	O
metrics	O
,	O
combined	O
with	O
an	O
open	O
source	O
module	O
designed	O
for	O
low	O
-	O
resource	O
MT	O
that	O
measures	O
the	O
alignment	O
level	O
.	O
A	O
quantitative	O
analysis	O
based	O
on	O
established	O
metrics	O
is	O
complemented	O
by	O
a	O
qualitative	O
analysis	O
of	O
translation	O
results	O
.	O
These	O
show	O
that	O
over	O
multiple	O
test	O
sets	O
,	O
the	O
proposed	O
hybridized	O
method	O
confers	O
improvements	O
over	O
(	O
i	O
)	O
both	O
the	O
best	O
individual	O
NMT	O
and	O
(	O
ii	O
)	O
the	O
ensemble	O
system	O
provided	O
in	O
the	O
Marian	O
-	O
NMT	O
package	O
.	O
Improvements	O
over	O
Marian	O
-	O
NMT	O
are	O
in	O
many	O
cases	O
statistically	O
significant	O
.	O

Verification	O
Method	O
(	O
AVM	O
)	O

PAM	O
utilizes	O
a	O
limited	O
-	O
size	O
bilingual	O
lexicon	O
(	O
of	O
typically	O
30	O
to	O
40	O
thousand	O
token	O
pairs	O
)	O
together	O
with	O
a	O
publicly	O
available	O
parser	O
.	O
Details	O
on	O
these	O
resources	O
are	O
reported	O
in	O
section	O
4.4	O
,	O
as	O
their	O
choices	O
are	O
language	O
-	O
specific	O
.	O
Based	O
on	O
these	O
resources	O
,	O
PAM	O
establishes	O
for	O
the	O
set	O
of	O
parallel	O
sentences	O
the	O
alignment	O
of	O
both	O
words	O
and	O
phrases	O
from	O
SL	O
to	O
TL	O
,	O
in	O
three	O
hierarchically	O
ordered	O
stages	O
:	O
1	O
.	O
Within	O
the	O
first	O
stage	O
,	O
the	O
alignment	O
of	O
words	O
is	O
based	O
on	O
equivalences	O
provided	O
by	O
the	O
bilingual	O
lexicon	O
.	O
Dedicated	O
PAM	O
processes	O
resolve	O
cases	O
where	O
(	O
i	O
)	O
words	O
have	O
multiple	O
appearances	O
within	O
a	O
sentence	O
and	O
(	O
ii	O
)	O
multiple	O
potential	O
translations	O
of	O
an	O
SL	O
word	O
exist	O
in	O
the	O
TL	O
side	O
.	O
2	O
.	O
Within	O
the	O
second	O
stage	O
,	O
words	O
are	O
aligned	O
by	O
establishing	O
statistical	O
correspondences	O
between	O
grammatical	O
features	O
across	O
the	O
SL	O
and	O
TL	O
pair	O
.	O
These	O
correspondences	O
are	O
automatically	O
extracted	O
from	O
the	O
lexicon	O
.	O
3	O
.	O
Within	O
the	O
third	O
stage	O
,	O
any	O
remaining	O
words	O
are	O
aligned	O
and	O
grouped	O
into	O
phrases	O
on	O
the	O
basis	O
of	O
the	O
alignments	O
of	O
their	O
neighboring	O
words	O
that	O
are	O
successfully	O
aligned	O
.	O
To	O
implement	O
this	O
,	O
the	O
principle	O
of	O
locality	O
across	O
languages	O
is	O
adopted	O
(	O
words	O
at	O
a	O
small	O
distance	O
to	O
each	O
other	O
in	O
SL	O
also	O
tend	O
to	O
be	O
located	O
close	O
to	O
each	O
other	O
in	O
TL	O
)	O
.	O
The	O
key	O
PAM	O
principle	O
is	O
that	O
decisions	O
made	O
at	O
a	O
later	O
stage	O
have	O
a	O
lower	O
degree	O
of	O
confidence	O
than	O
those	O
made	O
at	O
an	O
earlier	O
stage	O
(	O
Troullinos	O
,	O
2013	O
)	O
.	O

Examples	O
Label	O
P	O
:	O
ᄂ	O
ᅥᄂ	O
ᅳ	O
ᆫ	O
ᄀ	O
ᅥᄀ	O
ᅵᄋ	O
ᅦ	O
ᄋ	O
ᅵ	O
ᆻᄋ	O
ᅳ	O
ᆯ	O
ᄑ	O
ᅵ	O
ᆯᄋ	O
ᅭ	O
ᄋ	O
ᅥ	O
ᆹᄋ	O
ᅥ	O
.	O
E	O
"	O
You	O
do	O
n't	O
have	O
to	O
stay	O
there	O
.	O
"	O
H	O
:	O
ᄀ	O
ᅡᄃ	O
ᅩ	O
ᄃ	O
ᅫ	O
.	O
"	O
You	O
can	O
leave	O
.	O
"	O
P	O
:	O
ᄂ	O
ᅥᄂ	O
ᅳ	O
ᆫ	O
ᄀ	O
ᅥᄀ	O
ᅵᄋ	O
ᅦ	O
ᄋ	O
ᅵ	O
ᆻᄋ	O
ᅳ	O
ᆯ	O
ᄑ	O
ᅵ	O
ᆯᄋ	O
ᅭ	O
ᄋ	O
ᅥ	O
ᆹᄋ	O
ᅥ	O
.	O
C	O
"	O
You	O
do	O
n't	O
have	O
to	O
stay	O
there	O
.	O
"	O
H	O
:	O
ᄂ	O
ᅥ	O
ᆫ	O
ᄌ	O
ᅥ	O
ᆼᄒ	O
ᅪ	O
ᆨᄒ	O
ᅵ	O
ᄀ	O
ᅳ	O
ᄌ	O
ᅡᄅ	O
ᅵᄋ	O
ᅦ	O
ᄋ	O
ᅵ	O
ᆻᄋ	O
ᅥᄋ	O
ᅣ	O
ᄒ	O
ᅢ	O
!	O
"	O
You	O
need	O
to	O
stay	O
in	O
this	O
place	O
exactly	O
!	O
"	O
P	O
:	O
ᄂ	O
ᅥᄂ	O
ᅳ	O
ᆫ	O
ᄀ	O
ᅥᄀ	O
ᅵᄋ	O
ᅦ	O
ᄋ	O
ᅵ	O
ᆻᄋ	O
ᅳ	O
ᆯ	O
ᄑ	O
ᅵ	O
ᆯᄋ	O
ᅭ	O
ᄋ	O
ᅥ	O
ᆹᄋ	O
ᅥ	O
.	O
N	O
"	O
You	O
do	O
n't	O
have	O
to	O
stay	O
there	O
.	O
"	O
are	O
almost	O
twice	O
as	O
long	O
as	O
the	O
hypotheses	O
,	O
as	O
reported	O
in	O
Conneau	O
et	O
al	O
(	O
2018	O
)	O
.	O
We	O
present	O
a	O
few	O
examples	O
in	O
Table	O
2	O
.	O
H	O
:	O
ᄂ	O
ᅦᄀ	O
ᅡ	O
ᄋ	O
ᅯ	O
ᆫᄒ	O
ᅡᄆ	O
ᅧ	O
ᆫ	O
ᄂ	O
ᅥ	O
ᆫ	O
ᄌ	O
ᅵ	O
ᆸᄋ	O
ᅦ	O
ᄀ	O
ᅡᄃ	O
ᅩ	O
ᄃ	O
ᅫ	O
.	O
"	O
You	O
can	O
go	O
home	O
if	O
you	O
like	O
.	O
"	O

We	O
thank	O
Pulip	O
Park	O
for	O
helping	O
with	O
hiring	O
and	O
contacting	O
with	O
the	O
professional	O
translators	O
.	O
We	O
would	O
also	O
like	O
to	O
acknowledge	O
Kakao	O
Brain	O
Cloud	O
,	O
which	O
we	O
used	O
for	O
our	O
baseline	O
experiments	O
.	O

DialPort	O
,	O
Gone	O
Live	O
:	O
An	O
Update	O
After	O
A	O
Year	O
of	O
Development	O

DialPort	O
collects	O
user	O
data	O
for	O
connected	O
spoken	O
dialog	O
systems	O
.	O
At	O
present	O
six	O
systems	O
are	O
linked	O
to	O
a	O
central	O
portal	O
that	O
directs	O
the	O
user	O
to	O
the	O
applicable	O
system	O
and	O
suggests	O
systems	O
that	O
the	O
user	O
may	O
be	O
interested	O
in	O
.	O
User	O
data	O
has	O
started	O
to	O
flow	O
into	O
the	O
system	O
.	O

The	O
portal	O
gives	O
users	O
feedback	O
for	O
:	O
available	O
topics	O
,	O
system	O
state	O
,	O
and	O
present	O
system	O
state	O
.	O
Skylar	O
does	O
n't	O
interrupt	O
the	O
dialog	O
with	O
a	O
list	O
of	O
topics	O
.	O
Rather	O
it	O
suggests	O
one	O
topic	O
every	O
few	O
turns	O
.	O
This	O
evenly	O
steers	O
users	O
to	O
all	O
of	O
the	O
ES	O
.	O
A	O
banner	O
at	O
the	O
bottom	O
of	O
the	O
screen	O
reminds	O
users	O
of	O
all	O
the	O
topics	O
that	O
can	O
be	O
discussed	O
.	O
Another	O
box	O
indicates	O
the	O
system	O
state	O
in	O
order	O
to	O
avoid	O
user	O
confusion	O
about	O
who	O
has	O
the	O
floor	O
.	O
It	O
shows	O
,	O
for	O
example	O
,	O
whether	O
the	O
system	O
is	O
processing	O
the	O
speech	O
or	O
is	O
still	O
waiting	O
for	O
them	O
to	O
talk	O
.	O
The	O
box	O
shows	O
:	O
idle	O
(	O
either	O
from	O
timeout	O
or	O
from	O
the	O
user	O
clicking	O
on	O
the	O
box	O
to	O
pause	O
the	O
system	O
)	O
;	O
listening	O
(	O
this	O
is	O
shown	O
from	O
the	O
instant	O
the	O
ASR	O
begins	O
to	O
process	O
speech	O
to	O
when	O
it	O
is	O
finished	O
)	O
;	O
speaking	O
(	O
from	O
when	O
the	O
TTS	O
begins	O
output	O
to	O
when	O
it	O
is	O
finished	O
)	O
;	O
thinking	O
(	O
from	O
when	O
the	O
ASR	O
output	O
is	O
sent	O
to	O
the	O
NLU	O
to	O
when	O
the	O
DM	O
issues	O
its	O
action	O
)	O
.	O
Finally	O
,	O
the	O
system	O
informs	O
the	O
user	O
of	O
the	O
present	O
state	O
of	O
the	O
dialog	O
.	O
Do	O
you	O
still	O
want	O
XX	O
(	O
e.g.	O
Pittsburgh	O
)	O
?	O
reveals	O
that	O
the	O
user	O
preference	O
for	O
Pittsburgh	O
has	O
not	O
been	O
used	O
for	O
a	O
while	O
,	O
and	O
Skylar	O
's	O
forgetting	O
curve	O
is	O
ready	O
to	O
eliminate	O
it	O
.	O
The	O
dynamic	O
choice	O
of	O
implicit	O
or	O
explicit	O
confirmation	O
covers	O
the	O
global	O
dialog	O
state	O
.	O

The	O
AdWord	O
experience	O
lead	O
us	O
to	O
published	O
a	O
Facebook	O
page	O
on	O
April	O
12	O
,	O
2017	O
.	O
The	O
page	O
was	O
to	O
attract	O
both	O
explorers	O
and	O
real	O
users	O
through	O
both	O
organic	O
(	O
friends	O
and	O
friends	O
of	O
friends	O
)	O
and	O
paid	O
distribution	O
.	O
Despite	O
the	O
short	O
time	O
(	O
4	O
-	O
12	O
to	O
4	O
-	O
20	O
)	O
that	O
it	O
has	O
been	O
published	O
,	O
there	O
have	O
been	O
a	O
total	O
of	O
51	O
dialogs	O
(	O
excluding	O
all	O
dialogs	O
from	O
participating	O
research	O
teams	O
)	O
.	O
As	O
of	O
April	O
20	O
,	O
Dial	O
-	O
Port	O
spent	O
about	O
$	O
52	O
in	O
advertising	O
to	O
reach	O
1776	O
individuals	O
getting	O
147	O
page	O
views	O
,	O
47	O
likes	O
and	O
346	O
engagements	O
(	O
shares	O
or	O
clicks	O
)	O
.	O
About	O
40	O
%	O
of	O
the	O
clicks	O
were	O
from	O
mobile	O
devices	O
as	O
opposed	O
to	O
computers	O
.	O
This	O
underlines	O
the	O
need	O
for	O
mobile	O
versions	O
of	O
DialPort	O
.	O
The	O
average	O
length	O
of	O
a	O
dialog	O
is	O
8.7turns	O
(	O
7.18	O
stdev	O
)	O
and	O
129.51s	O
(	O
stdev	O
138.03	O
)	O
.	O
There	O
were	O
14.9	O
%	O
return	O
users	O
,	O
although	O
another	O
person	O
could	O
be	O
using	O
that	O
computer	O
and	O
some	O
places	O
have	O
automatic	O
IP	O
assignment	O
.	O
52.9	O
%	O
of	O
the	O
dialogs	O
were	O
spoken	O
as	O
opposed	O
to	O
typed	O
.	O
The	O
average	O
ASR	O
delay	O
was	O
925.03ms	O
.	O
On	O
average	O
,	O
users	O
tried	O
4.8	O
systems	O
per	O
dialog	O
.	O
The	O
distribution	O
of	O
dialog	O
turns	O
per	O
ES	O
and	O
for	O
the	O
portal	O
over	O
time	O
is	O
shown	O
on	O
Figure	O
1	O
.	O
Some	O
systems	O
are	O
getting	O
less	O
use	O
than	O
others	O
.	O
This	O
will	O
be	O
countered	O
by	O
paid	O
advertising	O
campaigns	O
that	O
promote	O
each	O
specific	O
system	O
.	O

This	O
paper	O
has	O
presented	O
a	O
novel	O
portal	O
that	O
collects	O
spoken	O
dialog	O
data	O
for	O
connected	O
systems	O
.	O
It	O
has	O
begun	O
to	O
collect	O
data	O
for	O
the	O
present	O
seven	O
systems	O
.	O
In	O
order	O
to	O
improve	O
service	O
an	O
audio	O
server	O
is	O
under	O
construction	O
as	O
are	O
smartphone	O
and	O
tablet	O
versions	O
.	O
The	O
portal	O
welcomes	O
new	O
external	O
systems	O
.	O

This	O
work	O
is	O
partly	O
funded	O
by	O
National	O
Science	O
Foundation	O
grant	O
CNS	O
-	O
1512973	O
.	O
The	O
opinions	O
expressed	O
in	O
this	O
paper	O
do	O
not	O
necessarily	O
reflect	O
those	O
of	O
the	O
National	O
Science	O
Foundation	O
.	O

This	O
work	O
has	O
received	O
funding	O
from	O
the	O
European	O
Research	O
Council	O
(	O
ERC	O
)	O
,	O
under	O
the	O
European	O
Union	O
's	O
Horizon	O
2020	O
research	O
and	O
innovation	O
programme	O
(	O
FASTPARSE	O
,	O
grant	O
agreement	O
No	O
714150	O
)	O
,	O
from	O
MINECO	O
(	O
FFI2014	O
-	O
51978	O
-	O
C2	O
-	O
2	O
-	O
R	O
,	O
TIN2017	O
-	O
85160	O
-	O
C2	O
-	O
1	O
-	O
R	O
)	O
and	O
from	O
Xunta	O
de	O
Galicia	O
(	O
ED431B	O
2017/01	O
)	O
.	O

Disinformation	O
is	O
much	O
more	O
than	O
just	O
a	O
mild	O
inconvenience	O
for	O
society	O
;	O
it	O
has	O
resulted	O
in	O
needless	O
deaths	O
in	O
the	O
COVID	O
-	O
19	O
pandemic	O
,	O
and	O
has	O
fomented	O
violence	O
and	O
political	O
instability	O
all	O
over	O
the	O
globe	O
(	O
van	O
der	O
Linden	O
et	O
al	O
,	O
2020	O
)	O
.	O
Our	O
goal	O
in	O
this	O
paper	O
is	O
to	O
discover	O
exploitable	O
weaknesses	O
in	O
current	O
fact	O
-	O
checking	O
models	O
and	O
recommend	O
that	O
such	O
models	O
not	O
be	O
relied	O
upon	O
in	O
their	O
current	O
form	O
.	O
We	O
point	O
out	O
how	O
the	O
models	O
are	O
dependent	O
on	O
emotional	O
signals	O
in	O
the	O
texts	O
instead	O
of	O
exclusively	O
performing	O
textual	O
entailment	O
,	O
and	O
that	O
additional	O
research	O
needs	O
to	O
be	O
done	O
to	O
ensure	O
they	O
are	O
performing	O
the	O
proper	O
task	O
.	O
Harm	O
Minimization	O
Our	O
quantifying	O
of	O
the	O
effects	O
of	O
pre	O
-	O
processing	O
on	O
fact	O
-	O
checking	O
models	O
does	O
not	O
cause	O
any	O
harm	O
to	O
real	O
-	O
world	O
users	O
or	O
companies	O
.	O
Research	O
has	O
demonstrated	O
that	O
adversarial	O
attacks	O
could	O
result	O
in	O
disinformation	O
being	O
labeled	O
as	O
factual	O
news	O
.	O
Disinformation	O
has	O
become	O
increasingly	O
present	O
in	O
global	O
politics	O
,	O
as	O
some	O
nation	O
-	O
states	O
with	O
significant	O
resources	O
have	O
disseminated	O
propaganda	O
to	O
create	O
political	O
dissent	O
in	O
other	O
countries	O
(	O
Zhou	O
et	O
al	O
,	O
2019	O
)	O
.	O
Our	O
research	O
here	O
has	O
demonstrated	O
potential	O
risks	O
:	O
emotional	O
writing	O
could	O
be	O
used	O
as	O
an	O
exploit	O
to	O
circumvent	O
fact	O
-	O
checking	O
models	O
.	O
Thus	O
,	O
we	O
urge	O
others	O
to	O
further	O
illuminate	O
such	O
vulnerabilities	O
,	O
to	O
minimize	O
potential	O
harms	O
,	O
and	O
to	O
encourage	O
improvements	O
with	O
new	O
models	O
.	O
Deployment	O
Social	O
media	O
companies	O
often	O
deal	O
with	O
fake	O
news	O
by	O
placing	O
highly	O
visible	O
labels	O
.	O
However	O
,	O
simply	O
tagging	O
stories	O
as	O
false	O
can	O
make	O
readers	O
more	O
willing	O
to	O
believe	O
and	O
share	O
other	O
false	O
,	O
untagged	O
stories	O
.	O
This	O
unintended	O
consequence	O
-	O
in	O
which	O
the	O
selective	O
labeling	O
of	O
false	O
news	O
makes	O
other	O
news	O
stories	O
seem	O
more	O
legitimate	O
-	O
has	O
been	O
called	O
the	O
"	O
implied	O
-	O
truth	O
effect	O
"	O
(	O
Pennycook	O
et	O
al	O
,	O
2019	O
)	O
.	O
Thus	O
,	O
unless	O
these	O
models	O
become	O
so	O
accurate	O
that	O
they	O
catch	O
all	O
fake	O
news	O
presented	O
to	O
them	O
,	O
the	O
entire	O
basis	O
of	O
their	O
use	O
is	O
called	O
into	O
question	O
.	O
Despite	O
the	O
significant	O
progress	O
in	O
developing	O
models	O
to	O
correctly	O
identify	O
fake	O
news	O
,	O
the	O
real	O
elephant	O
in	O
the	O
room	O
is	O
that	O
many	O
people	O
simply	O
ignore	O
the	O
labels	O
(	O
Molina	O
et	O
al	O
,	O
2021	O
)	O
.	O
There	O
is	O
,	O
however	O
,	O
prior	O
work	O
supporting	O
the	O
idea	O
that	O
if	O
people	O
are	O
warned	O
that	O
a	O
headline	O
is	O
false	O
,	O
they	O
will	O
be	O
less	O
likely	O
to	O
believe	O
it	O
(	O
Ecker	O
et	O
al	O
,	O
2010	O
;	O
Lewandowsky	O
et	O
al	O
,	O
2012	O
)	O
.	O
Because	O
of	O
this	O
,	O
we	O
believe	O
this	O
research	O
represents	O
a	O
net	O
benefit	O
for	O
humanity	O
.	O
Warning	O
labels	O
are	O
just	O
one	O
way	O
of	O
dealing	O
with	O
properly	O
identified	O
fake	O
news	O
,	O
and	O
publishers	O
can	O
choose	O
to	O
simply	O
not	O
allow	O
it	O
on	O
their	O
platforms	O
.	O
Of	O
course	O
,	O
this	O
issue	O
leads	O
to	O
questions	O
of	O
censorship	O
.	O

Estonian	O
as	O
a	O
Second	O
Language	O
Teacher	O
's	O
Tools	O

The	O
paper	O
describes	O
"	O
Teacher	O
's	O
Tools	O
"	O
(	O
et	O
Opetaja	O
tööriistad	O
)	O
developed	O
by	O
the	O
Institute	O
of	O
the	O
Estonian	O
Language	O
for	O
teachers	O
and	O
specialists	O
of	O
Estonian	O
as	O
a	O
Second	O
Language	O
.	O
The	O
toolbox	O
includes	O
four	O
modules	O
:	O
vocabulary	O
,	O
grammar	O
,	O
communicative	O
language	O
activities	O
and	O
text	O
evaluation	O
.	O
The	O
vocabulary	O
module	O
provides	O
graded	O
word	O
lists	O
for	O
young	O
(	O
CEFR	O
levels	O
pre	O
-	O
A1	O
-	O
C1	O
)	O
and	O
adult	O
(	O
CEFR	O
levels	O
A1	O
-	O
C1	O
)	O
learners	O
.	O
The	O
grammar	O
module	O
provides	O
descriptions	O
of	O
young	O
learners	O
'	O
grammar	O
competence	O
.	O
The	O
communicative	O
language	O
activities	O
module	O
gives	O
teachers	O
an	O
overview	O
of	O
the	O
typical	O
situations	O
that	O
learners	O
should	O
be	O
able	O
to	O
cope	O
with	O
.	O
The	O
text	O
evaluation	O
module	O
marks	O
lemmas	O
in	O
texts	O
according	O
to	O
their	O
CEFR	O
assignment	O
in	O
the	O
vocabulary	O
module	O
.	O
So	O
far	O
the	O
grammar	O
and	O
the	O
communicative	O
language	O
activities	O
modules	O
have	O
been	O
developed	O
only	O
for	O
young	O
learners	O
(	O
CEFR	O
levels	O
pre	O
-	O
A1	O
-	O
B2	O
)	O
.	O
The	O
toolbox	O
is	O
aimed	O
at	O
providing	O
support	O
for	O
the	O
development	O
of	O
Estonian	O
as	O
an	O
L2	O
courses	O
,	O
educational	O
materials	O
,	O
exercises	O
and	O
tests	O
within	O
a	O
CEFR	O
-	O
based	O
framework	O
.	O
The	O
project	O
started	O
in	O
2017	O
and	O
is	O
ongoing	O
.	O

1	O
Introduction	O
"	O
Teacher	O
's	O
Tools	O
"	O
is	O
a	O
compatible	O
toolkit	O
developed	O
for	O
teachers	O
and	O
specialists	O
of	O
Estonian	O
as	O
a	O
Second	O
Language	O
,	O
providing	O
an	O
extensive	O
overview	O
of	O
the	O
development	O
of	O
lexical	O
and	O
grammatical	O
competence	O
in	O
Estonian	O
among	O
L2	O
learners	O
.	O
The	O
toolkit	O
is	O
accessible	O
as	O
a	O
sub	O
-	O
page	O
of	O
the	O
language	O
portal	O
Sõnaveeb	O
1	O
.	O
The	O
framework	O
of	O
the	O
project	O
is	O
based	O
on	O
the	O
Common	O
European	O
Framework	O
of	O
Reference	O
for	O
Languages	O
:	O
Learning	O
,	O
Teaching	O
,	O
Assessment	O
(	O
CEFR	O
,	O
2001	O
)	O
,	O
its	O
Companion	O
Volume	O
with	O
New	O
Descriptors	O
(	O
CEFR	O
/	O
CV	O
,	O
2018	O
)	O
and	O
the	O
Collated	O
Representative	O
Samples	O
of	O
Descriptors	O
of	O
Language	O
Competences	O
Developed	O
for	O
Young	O
1	O
https://sonaveeb.ee/teacher	O
-	O
tools	O
Learners	O
for	O
Ages	O
7	O
-	O
10	O
(	O
Szabo	O
,	O
2018a	O
)	O
and	O
11	O
-	O
15	O
(	O
Szabo	O
,	O
2018b	O
)	O
.	O
The	O
toolkit	O
distinguishes	O
between	O
adult	O
(	O
CEFR	O
levels	O
A1	O
-	O
C1	O
)	O
and	O
young	O
learners	O
(	O
CEFR	O
levels	O
pre	O
-	O
A1	O
-	O
C1	O
)	O
.	O
The	O
methodology	O
of	O
the	O
project	O
is	O
adapted	O
from	O
similar	O
projects	O
for	O
other	O
languages	O
(	O
e.g.	O
Capel	O
,	O
2010Capel	O
,	O
,	O
2012O'Keeffe	O
and	O
Geraldine	O
2017	O
;	O
Alfter	O
et	O
al	O
,	O
2019	O
)	O
.	O

There	O
are	O
quite	O
a	O
few	O
corpora	O
available	O
for	O
the	O
research	O
of	O
the	O
Estonian	O
Language	O
(	O
the	O
biggest	O
Estonian	O
corpus	O
is	O
the	O
"	O
Estonian	O
National	O
Corpus	O
2019	O
"	O
(	O
1	O
,	O
5	O
billions	O
words	O
)	O
)	O
2	O
.	O
However	O
,	O
at	O
the	O
beginning	O
of	O
the	O
project	O
in	O
2017	O
there	O
was	O
a	O
clear	O
lack	O
of	O
resources	O
available	O
for	O
the	O
research	O
of	O
Estonian	O
as	O
an	O
L2	O
.	O
In	O
order	O
to	O
fill	O
this	O
gap	O
,	O
the	O
Institute	O
of	O
Estonian	O
Language	O
compiled	O
two	O
types	O
of	O
corpora	O
:	O
1	O
)	O
coursebook	O
corpora	O
and	O
2	O
)	O
learner	O
corpora	O
.	O
As	O
Volodina	O
and	O
Kokkinakis	O
(	O
2013	O
)	O
point	O
out	O
,	O
the	O
first	O
type	O
of	O
corpora	O
provides	O
information	O
about	O
what	O
and	O
when	O
education	O
professionals	O
found	O
important	O
for	O
students	O
to	O
learn	O
(	O
passive	O
linguistic	O
competence	O
)	O
,	O
while	O
learner	O
corpora	O
provide	O
an	O
indication	O
of	O
active	O
linguistic	O
competence	O
.	O
Both	O
need	O
to	O
be	O
studied	O
since	O
the	O
second	O
is	O
directly	O
influenced	O
by	O
the	O
first	O
.	O
The	O
coursebook	O
corpora	O
were	O
compiled	O
in	O
several	O
stages	O
and	O
resulted	O
in	O
two	O
groups	O
of	O
corpora	O
:	O
"	O
Estonian	O
as	O
a	O
Second	O
Language	O
Coursebook	O
Content	O
Corpus	O
2017	O
"	O
3	O
(	O
contains	O
eight	O
coursebooks	O
for	O
adult	O
learners	O
at	O
levels	O
A1	O
-	O
C1	O
;	O
500	O
000	O
tokens	O
)	O
and	O
"	O
Estonian	O
as	O
a	O
Second	O
Language	O
School	O
Coursebook	O
Content	O
Corpus	O
2021	O
"	O
4	O
(	O
contains	O
27	O
school	O
coursebooks	O
for	O
grades	O
1	O
-	O
12	O
;	O
1	O
300	O
000	O
tokens	O
)	O
.	O
The	O
learner	O
corpus	O
was	O
compiled	O
in	O
2019	O
-	O
2021	O
and	O
contains	O
6700	O
Estonian	O
as	O
an	O
L2	O
young	O
learner	O
's	O
texts	O
.	O
These	O
are	O
texts	O
written	O
mostly	O
as	O
state	O
standard	O
-	O
determining	O
tests	O
(	O
grades	O
3	O
and	O
6	O
)	O
,	O
basic	O
school	O
final	O
examinations	O
The	O
data	O
is	O
available	O
for	O
analysis	O
through	O
the	O
interface	O
of	O
the	O
Estonian	O
Learner	O
Language	O
Corpus	O
EMMA	O
5	O
.	O
The	O
texts	O
were	O
not	O
corrected	O
or	O
altered	O
.	O
Both	O
types	O
of	O
corpora	O
were	O
used	O
for	O
the	O
development	O
of	O
"	O
Teacher	O
's	O
Tools	O
"	O
.	O
The	O
coursebook	O
corpora	O
were	O
used	O
primarily	O
to	O
create	O
passive	O
vocabulary	O
word	O
lists	O
and	O
for	O
the	O
analysis	O
of	O
explicit	O
and	O
implicit	O
grammar	O
teaching	O
.	O
The	O
learner	O
corpus	O
was	O
used	O
for	O
the	O
creation	O
and	O
analysis	O
of	O
active	O
vocabulary	O
word	O
lists	O
and	O
the	O
dynamics	O
of	O
grammar	O
acquisition	O
.	O
3	O
Modules	O
of	O
"	O
Teacher	O
's	O
Tools	O
"	O
"	O
Teacher	O
's	O
Tools	O
"	O
consists	O
of	O
four	O
modules	O
:	O
vocabulary	O
,	O
grammar	O
,	O
communicative	O
language	O
activities	O
and	O
text	O
evaluation	O
.	O

The	O
vocabulary	O
module	O
includes	O
young	O
and	O
adult	O
learners	O
'	O
word	O
lists	O
compiled	O
on	O
the	O
basis	O
of	O
coursebook	O
and	O
learner	O
corpus	O
word	O
lists	O
in	O
comparison	O
with	O
frequency	O
distribution	O
in	O
the	O
"	O
Estonian	O
National	O
Corpus	O
2017	O
"	O
6	O
.	O
Currently	O
,	O
the	O
vocabulary	O
list	O
for	O
adult	O
learners	O
includes	O
about	O
12	O
500	O
words	O
for	O
levels	O
A1	O
-	O
C1	O
.	O
The	O
young	O
learners	O
'	O
word	O
list	O
includes	O
approx	O
.	O
9000	O
words	O
for	O
levels	O
pre	O
-	O
A1	O
-	O
B2	O
.	O
All	O
words	O
are	O
presented	O
as	O
lemmas	O
.	O
Users	O
can	O
generate	O
word	O
lists	O
based	O
on	O
learner	O
type	O
(	O
young	O
or	O
adult	O
)	O
and	O
POS	O
.	O
In	O
addition	O
,	O
24	O
topics	O
(	O
clothes	O
,	O
furniture	O
,	O
birds	O
etc	O
.	O
)	O
are	O
compiled	O
.	O
The	O
search	O
results	O
can	O
be	O
sorted	O
alphabetically	O
,	O
by	O
level	O
or	O
by	O
POS	O
(	O
see	O
Figure	O
1	O
)	O
.	O

The	O
grammar	O
module	O
is	O
the	O
first	O
attempt	O
to	O
create	O
a	O
systematic	O
overview	O
of	O
Estonian	O
L2	O
learner	O
's	O
grammar	O
competence	O
development	O
.	O
The	O
general	O
methodology	O
was	O
adopted	O
from	O
the	O
English	O
Grammar	O
Profile	O
7	O
project	O
(	O
O'Keeffe	O
and	O
Geraldine	O
,	O
2017	O
)	O
.	O
Currently	O
,	O
the	O
grammar	O
module	O
provides	O
descriptions	O
of	O
grammar	O
competence	O
on	O
the	O
morphology	O
,	O
derivation	O
,	O
phrase	O
and	O
sentence	O
levels	O
,	O
from	O
level	O
pre	O
-	O
A1	O
up	O
to	O
level	O
B2	O
(	O
see	O
Figure	O
2	O
)	O
.	O
The	O
same	O
grammatical	O
category	O
(	O
e.g.	O
the	O
use	O
of	O
the	O
genetive	O
case	O
for	O
nouns	O
or	O
the	O
use	O
of	O
imperative	O
mood	O
for	O
verbs	O
)	O
may	O
be	O
described	O
at	O
all	O
levels	O
,	O
but	O
the	O
functions	O
and	O
usage	O
(	O
how	O
often	O
the	O
learner	O
makes	O
mistakes	O
and	O
what	O
kinds	O
of	O
mistakes	O
)	O
are	O
level	O
-	O
specific	O
.	O
Search	O
options	O
offer	O
users	O
choices	O
of	O
four	O
main	O
categories	O
:	O
morphology	O
,	O
derivation	O
,	O
phrase	O
formation	O
and	O
sentence	O
formation	O
.	O
The	O
main	O
categories	O
contain	O
19	O
subcategories	O
.	O
For	O
example	O
,	O
subcategories	O
of	O
morphology	O
are	O
verb	O
,	O
noun	O
,	O
adjective	O
,	O
numeral	O
,	O
pronoun	O
,	O
adverb	O
.	O
Every	O
subcategory	O
has	O
grammatical	O
markers	O
(	O
e.g.	O
number	O
and	O
case	O
for	O
noun	O
)	O
.	O
The	O
grammatical	O
markers	O
have	O
values	O
,	O
for	O
example	O
number	O
has	O
values	O
singular	O
and	O
plural	O
.	O
All	O
descriptions	O
are	O
equipped	O
with	O
example	O
sentences	O
compiled	O
either	O
by	O
experts	O
or	O
taken	O
from	O
coursebook	O
and	O
learner	O
's	O
corpora	O
.	O

The	O
communicative	O
language	O
activities	O
module	O
is	O
based	O
on	O
CEFR	O
/	O
CV	O
descriptor	O
scales	O
,	O
and	O
examples	O
are	O
taken	O
from	O
Szabo	O
(	O
2018a	O
,	O
b	O
)	O
,	O
which	O
include	O
reception	O
processes	O
and	O
strategies	O
of	O
written	O
and	O
spoken	O
texts	O
(	O
listening	O
and	O
reading	O
)	O
,	O
written	O
and	O
oral	O
text	O
production	O
and	O
production	O
strategies	O
,	O
and	O
written	O
and	O
spoken	O
interaction	O
and	O
interaction	O
strategies	O
.	O
It	O
covers	O
young	O
learners	O
at	O
levels	O
A1	O
-	O
B2	O
.	O
The	O
architecture	O
of	O
the	O
module	O
is	O
similar	O
to	O
that	O
of	O
the	O
grammar	O
module	O
,	O
containing	O
main	O
categories	O
(	O
reception	O
,	O
production	O
,	O
interaction	O
and	O
mediation	O
)	O
and	O
subcategories	O
,	O
which	O
are	O
followed	O
by	O
detailed	O
descriptions	O
,	O
including	O
limitations	O
:	O
when	O
and	O
how	O
well	O
the	O
language	O
user	O
can	O
manage	O
certain	O
communicative	O
activities	O
(	O
see	O
Figure	O
3	O
)	O
.	O
For	O
example	O
,	O
by	O
choosing	O
the	O
main	O
category	O
reception	O
and	O
the	O
subcategory	O
overall	O
listening	O
comprehension	O
,	O
a	O
teacher	O
can	O
see	O
that	O
at	O
the	O
A1	O
level	O
the	O
learner	O
should	O
be	O
able	O
to	O
understand	O
a	O
simple	O
description	O
with	O
short	O
sentences	O
,	O
for	O
example	O
about	O
the	O
weather	O
or	O
how	O
big	O
a	O
certain	O
object	O
is	O
.	O

Supporting	O
Spoken	O
Assistant	O
Systems	O
with	O
a	O
Graphical	O
User	O
Interface	O
that	O
Signals	O
Incremental	O
Understanding	O
and	O
Prediction	O
State	O

Current	O
virtual	O
personal	O
assistants	O
(	O
PAs	O
)	O
require	O
users	O
to	O
either	O
formulate	O
complex	O
intents	O
in	O
one	O
utterance	O
(	O
e.g.	O
,	O
"	O
call	O
Peter	O
Miller	O
on	O
his	O
mobile	O
phone	O
"	O
)	O
or	O
go	O
through	O
tedious	O
sub	O
-	O
dialogues	O
(	O
e.g.	O
,	O
"	O
phone	O
call	O
"	O
-	O
who	O
would	O
you	O
like	O
to	O
call	O
?	O
-	O
"	O
Peter	O
Miller	O
"	O
-	O
I	O
have	O
a	O
mobile	O
number	O
and	O
a	O
work	O
number	O
.	O
Which	O
one	O
do	O
you	O
want	O
?	O
)	O
.	O
This	O
is	O
not	O
how	O
one	O
would	O
interact	O
with	O
a	O
human	O
assistant	O
,	O
where	O
the	O
request	O
would	O
be	O
naturally	O
structured	O
into	O
smaller	O
chunks	O
that	O
individually	O
get	O
acknowledged	O
(	O
e.g.	O
,	O
"	O
Can	O
you	O
make	O
a	O
connection	O
for	O
me	O
?	O
"	O
-	O
sure	O
-	O
"	O
with	O
Peter	O
Miller	O
"	O
-	O
uh	O
huh	O
-	O
"	O
on	O
his	O
mobile	O
"	O
-	O
dialling	O
now	O
)	O
.	O
Current	O
PAs	O
signal	O
ongoing	O
understanding	O
by	O
displaying	O
the	O
state	O
of	O
the	O
recognised	O
speech	O
(	O
ASR	O
)	O
to	O
the	O
user	O
,	O
but	O
not	O
their	O
semantic	O
interpretation	O
of	O
it	O
.	O
Another	O
type	O
of	O
assistant	O
system	O
forgoes	O
enquiring	O
user	O
intent	O
altogether	O
and	O
infers	O
likely	O
intents	O
from	O
context	O
.	O
GoogleNow	O
,	O
for	O
example	O
,	O
might	O
present	O
traffic	O
information	O
to	O
a	O
user	O
picking	O
up	O
their	O
mobile	O
phone	O
at	O
their	O
typical	O
commute	O
time	O
.	O
These	O
systems	O
display	O
their	O
"	O
understanding	O
"	O
state	O
,	O
but	O
do	O
not	O
allow	O
any	O
type	O
of	O
interaction	O
with	O
it	O
apart	O
from	O
dismissing	O
the	O
provided	O
information	O
.	O
In	O
this	O
work	O
,	O
we	O
explore	O
adding	O
a	O
graphical	O
user	O
interface	O
(	O
GUI	O
)	O
modality	O
that	O
makes	O
it	O
possible	O
to	O
see	O
these	O
interaction	O
styles	O
as	O
extremes	O
on	O
a	O
continuum	O
,	O
and	O
to	O
realise	O
positions	O
between	O
these	O
extremes	O
and	O
present	O
a	O
mixed	O
graphical	O
/	O
voice	O
enabled	O
PA	O
that	O
can	O
provide	O
feedback	O
of	O
understanding	O
to	O
the	O
user	O
incrementally	O
as	O
the	O
user	O
's	O
utterance	O
unfolds	O
-	O
allowing	O
users	O
to	O
make	O
requests	O
in	O
instalments	O
instead	O
of	O
fully	O
thought	O
-	O
out	O
requests	O
.	O
It	O
does	O
this	O
by	O
signalling	O
ongoing	O
understanding	O
in	O
an	O
intuitive	O
tree	O
-	O
like	O
GUI	O
that	O
can	O
be	O
displayed	O
on	O
a	O
mobile	O
device	O
.	O
We	O
evaluate	O
our	O
system	O
by	O
directing	O
users	O
to	O
perform	O
tasks	O
using	O
it	O
under	O
nonincremental	O
(	O
i.e.	O
,	O
ASR	O
endpointing	O
)	O
and	O
incremental	O
conditions	O
and	O
then	O
compare	O
the	O
two	O
conditions	O
.	O
We	O
further	O
compare	O
a	O
non	O
-	O
adaptive	O
with	O
an	O
adaptive	O
(	O
i.e.	O
,	O
infers	O
likely	O
events	O
)	O
version	O
of	O
our	O
system	O
.	O
We	O
report	O
that	O
the	O
users	O
found	O
the	O
interface	O
intuitive	O
and	O
easy	O
to	O
use	O
,	O
and	O
that	O
users	O
were	O
able	O
to	O
perform	O
tasks	O
more	O
efficiently	O
with	O
incremental	O
as	O
well	O
as	O
adaptive	O
variants	O
of	O
the	O
system	O
.	O

This	O
work	O
builds	O
upon	O
several	O
threads	O
of	O
previous	O
research	O
:	O
Chai	O
et	O
al	O
(	O
2014	O
)	O
addressed	O
misalignments	O
in	O
understanding	O
(	O
i.e.	O
,	O
common	O
ground	O
(	O
Clark	O
and	O
Schaefer	O
,	O
1989	O
)	O
)	O
between	O
robots	O
and	O
humans	O
by	O
informing	O
the	O
human	O
of	O
the	O
internal	O
system	O
state	O
via	O
speech	O
.	O
We	O
take	O
this	O
idea	O
and	O
ap	O
-	O
ply	O
it	O
to	O
a	O
PA	O
by	O
displaying	O
the	O
internal	O
state	O
of	O
the	O
system	O
to	O
the	O
user	O
via	O
a	O
GUI	O
(	O
explained	O
in	O
Section	O
3.5	O
)	O
,	O
allowing	O
the	O
user	O
to	O
determine	O
if	O
system	O
understanding	O
has	O
taken	O
place	O
-	O
a	O
way	O
of	O
providing	O
feedback	O
and	O
backchannels	O
to	O
the	O
user	O
.	O
Dethlefs	O
et	O
al	O
(	O
2016	O
)	O
provide	O
a	O
good	O
review	O
of	O
work	O
that	O
show	O
how	O
backchannels	O
facilitate	O
grounding	O
,	O
feedback	O
,	O
and	O
clarifications	O
in	O
human	O
spoken	O
dialogue	O
,	O
and	O
apply	O
an	O
information	O
density	O
approach	O
to	O
determine	O
when	O
to	O
backchannel	O
using	O
speech	O
.	O
Because	O
we	O
do	O
n't	O
backchannel	O
using	O
speech	O
here	O
,	O
there	O
is	O
no	O
potential	O
overlap	O
between	O
the	O
user	O
and	O
the	O
system	O
;	O
rather	O
,	O
our	O
system	O
can	O
display	O
backchannels	O
and	O
ask	O
clarifications	O
without	O
frustrating	O
the	O
user	O
through	O
inadvertent	O
overlaps	O
.	O
Though	O
different	O
in	O
many	O
ways	O
,	O
our	O
work	O
is	O
similar	O
in	O
some	O
regards	O
to	O
Larsson	O
et	O
al	O
(	O
2011	O
)	O
,	O
which	O
displays	O
information	O
to	O
the	O
user	O
and	O
allows	O
the	O
user	O
to	O
navigate	O
the	O
display	O
itself	O
(	O
e.g.	O
,	O
by	O
saying	O
up	O
or	O
down	O
in	O
a	O
menu	O
list	O
)	O
-	O
functionality	O
that	O
we	O
intend	O
to	O
apply	O
to	O
our	O
GUI	O
in	O
future	O
work	O
.	O
Our	O
work	O
is	O
also	O
comparable	O
to	O
SDS	O
toolkits	O
such	O
as	O
IrisTK	O
(	O
Skantze	O
and	O
Moubayed	O
,	O
2012	O
)	O
and	O
Open	O
-	O
Dial	O
(	O
Lison	O
,	O
2015	O
)	O
which	O
enable	O
SDS	O
designers	O
to	O
visualise	O
the	O
internal	O
state	O
of	O
their	O
systems	O
,	O
though	O
not	O
for	O
end	O
user	O
interpretability	O
.	O
Some	O
of	O
the	O
work	O
here	O
is	O
inspired	O
by	O
the	O
Microsoft	O
Language	O
Understanding	O
Intelligent	O
Service	O
(	O
LUIS	O
)	O
project	O
(	O
Williams	O
et	O
al	O
,	O
2015	O
)	O
.	O
While	O
our	O
system	O
by	O
no	O
means	O
achieves	O
the	O
scale	O
that	O
LUIS	O
does	O
,	O
we	O
offer	O
here	O
an	O
additional	O
contribution	O
of	O
an	O
open	O
source	O
LUIS	O
-	O
like	O
system	O
(	O
with	O
the	O
important	O
addition	O
of	O
the	O
graphical	O
interface	O
)	O
that	O
is	O
authorable	O
(	O
using	O
JSON	O
files	O
;	O
we	O
leave	O
authoring	O
using	O
a	O
web	O
interface	O
like	O
that	O
of	O
LUIS	O
to	O
future	O
work	O
)	O
,	O
extensible	O
(	O
affordances	O
can	O
be	O
easily	O
added	O
)	O
,	O
incremental	O
(	O
in	O
that	O
respect	O
going	O
beyond	O
LUIS	O
)	O
,	O
trainable	O
(	O
i.e.	O
,	O
can	O
learn	O
from	O
examples	O
,	O
but	O
can	O
still	O
function	O
well	O
without	O
examples	O
)	O
,	O
and	O
can	O
learn	O
through	O
interacting	O
(	O
here	O
we	O
apply	O
a	O
user	O
model	O
that	O
learns	O
during	O
interaction	O
)	O
.	O

DM	O
w	O
1	O
...	O
w	O
n	O
w	O
1	O
...	O
w	O
n	O
d	O
1	O
d	O
m	O
s	O
1	O
:	O
v	O
1	O
s	O
m	O
:	O
v	O
m	O
...	O
s	O
1	O
:	O
v	O
1	O
s	O
m	O
:	O
v	O
m	O
...	O
...	O

Figure	O
1	O
:	O
Overview	O
of	O
system	O
made	O
up	O
of	O
ASR	O
which	O
takes	O
in	O
a	O
speech	O
signal	O
and	O
produces	O
transcribed	O
words	O
,	O
NLU	O
,	O
which	O
takes	O
words	O
and	O
produces	O
a	O
slots	O
in	O
a	O
frame	O
,	O
DM	O
which	O
takes	O
slots	O
and	O
produces	O
a	O
decision	O
for	O
each	O
,	O
and	O
the	O
GUI	O
which	O
displays	O
the	O
state	O
of	O
the	O
system	O
.	O

The	O
DM	O
plays	O
a	O
crucial	O
role	O
in	O
our	O
SDS	O
:	O
as	O
well	O
as	O
determining	O
how	O
to	O
act	O
,	O
the	O
DM	O
is	O
called	O
upon	O
to	O
decide	O
when	O
to	O
act	O
,	O
effectively	O
giving	O
the	O
DM	O
the	O
control	O
over	O
timing	O
of	O
actions	O
rather	O
than	O
relying	O
on	O
ASR	O
endpointing	O
-	O
further	O
separating	O
our	O
SDS	O
from	O
other	O
systems	O
.	O
The	O
DM	O
policy	O
is	O
based	O
on	O
a	O
confidence	O
score	O
derived	O
from	O
the	O
NLU	O
(	O
in	O
this	O
case	O
,	O
we	O
used	O
the	O
distribution	O
's	O
argmax	O
value	O
)	O
using	O
thresholds	O
for	O
the	O
actions	O
(	O
see	O
below	O
)	O
,	O
set	O
by	O
hand	O
(	O
i.e.	O
,	O
trial	O
and	O
error	O
)	O
.	O
At	O
each	O
word	O
and	O
resulting	O
distribution	O
from	O
NLU	O
,	O
the	O
DM	O
needs	O
to	O
choose	O
one	O
of	O
the	O
following	O
:	O
wait	O
-	O
wait	O
for	O
more	O
information	O
(	O
i.e.	O
,	O
for	O
the	O
next	O
word	O
)	O
select	O
-	O
as	O
the	O
NLU	O
is	O
confident	O
enough	O
,	O
fill	O
the	O
slot	O
can	O
with	O
the	O
argmax	O
from	O
NLU	O
request	O
-	O
signal	O
a	O
(	O
yes	O
/	O
no	O
)	O
clarification	O
request	O
on	O
the	O
current	O
slot	O
and	O
the	O
proposed	O
filler	O
confirm	O
-	O
act	O
on	O
the	O
confirmation	O
of	O
the	O
user	O
;	O
in	O
effect	O
,	O
select	O
the	O
proposed	O
slot	O
value	O
Though	O
the	O
thresholds	O
are	O
statically	O
set	O
,	O
we	O
applied	O
OpenDial	O
(	O
Lison	O
,	O
2015	O
)	O
as	O
an	O
IU	O
-	O
module	O
to	O
perform	O
the	O
task	O
of	O
the	O
DM	O
with	O
the	O
future	O
goal	O
that	O
these	O
values	O
could	O
be	O
adjusted	O
through	O
reinforcement	O
learning	O
(	O
which	O
OpenDial	O
could	O
provide	O
)	O
.	O
The	O
DM	O
processes	O
and	O
makes	O
a	O
decision	O
for	O
each	O
slot	O
,	O
with	O
the	O
assumption	O
that	O
only	O
one	O
slot	O
out	O
of	O
all	O
that	O
are	O
processed	O
will	O
result	O
in	O
an	O
non	O
-	O
wait	O
action	O
(	O
though	O
this	O
is	O
not	O
enforced	O
)	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
two	O
experiments	O
where	O
we	O
evaluated	O
our	O
system	O
.	O
It	O
is	O
our	O
primary	O
goal	O
to	O
show	O
that	O
our	O
GUI	O
is	O
useful	O
and	O
signals	O
understanding	O
to	O
the	O
user	O
.	O
We	O
also	O
wish	O
to	O
show	O
that	O
incremental	O
presentation	O
of	O
such	O
a	O
GUI	O
is	O
more	O
effective	O
than	O
an	O
endpointed	O
system	O
.	O
We	O
further	O
want	O
to	O
show	O
that	O
an	O
adaptive	O
system	O
is	O
more	O
effective	O
than	O
a	O
non	O
-	O
adaptive	O
system	O
(	O
though	O
both	O
would	O
process	O
incrementally	O
)	O
.	O
In	O
order	O
to	O
best	O
evaluate	O
our	O
system	O
,	O
we	O
recruited	O
participants	O
to	O
interact	O
with	O
our	O
system	O
in	O
varied	O
settings	O
to	O
compare	O
endpointed	O
(	O
i.e.	O
,	O
non	O
-	O
incremental	O
)	O
and	O
nonadaptive	O
as	O
well	O
as	O
adaptive	O
versions	O
.	O
We	O
describe	O
how	O
the	O
data	O
were	O
collected	O
from	O
the	O
participants	O
,	O
then	O
explain	O
each	O
experiment	O
and	O
give	O
results	O
.	O

In	O
this	O
section	O
we	O
report	O
the	O
results	O
of	O
the	O
evaluation	O
between	O
the	O
endpointed	O
(	O
i.e.	O
,	O
nonincremental	O
;	O
Phase	O
1	O
)	O
variant	O
vs	O
the	O
incremental	O
(	O
Phase	O
2	O
)	O
variant	O
of	O
our	O
system	O
.	O

We	O
applied	O
a	O
multinomial	O
test	O
of	O
significance	O
to	O
the	O
results	O
,	O
treating	O
all	O
four	O
possible	O
answers	O
as	O
equally	O
likely	O
(	O
with	O
Bonferroni	O
correction	O
of	O
10	O
)	O
.	O
The	O
item	O
The	O
interface	O
was	O
useful	O
and	O
easy	O
to	O
understand	O
with	O
the	O
answer	O
of	O
Both	O
was	O
significant	O
(	O
χ	O
2	O
(	O
4	O
,	O
N	O
=	O
12	O
)	O
=	O
9.0	O
,	O
p	O
<	O
.005	O
)	O
,	O
as	O
was	O
The	O
assistant	O
was	O
easy	O
and	O
intuitive	O
to	O
use	O
also	O
with	O
the	O
answer	O
Both	O
(	O
χ	O
2	O
(	O
4	O
,	O
N	O
=	O
12	O
)	O
=	O
9.0	O
,	O
p	O
<	O
.005	O
)	O
.	O
The	O
item	O
I	O
always	O
understood	O
what	O
the	O
system	O
wanted	O
from	O
me	O
was	O
also	O
answered	O
Both	O
significantly	O
more	O
times	O
than	O
other	O
answers	O
(	O
χ	O
2	O
(	O
4	O
,	O
N	O
=	O
14	O
)	O
=	O
9.0	O
,	O
p	O
<	O
.005	O
)	O
,	O
similarly	O
for	O
It	O
was	O
sometimes	O
unclear	O
to	O
me	O
if	O
the	O
assistant	O
understood	O
me	O
with	O
the	O
answer	O
of	O
Both	O
(	O
χ	O
2	O
(	O
4	O
,	O
N	O
=	O
12	O
)	O
=	O
10.0	O
,	O
p	O
<	O
.005	O
)	O
.	O
These	O
responses	O
tell	O
us	O
that	O
though	O
the	O
participants	O
did	O
not	O
report	O
preference	O
for	O
either	O
system	O
variant	O
,	O
they	O
reported	O
a	O
general	O
positive	O
impression	O
of	O
the	O
GUI	O
(	O
in	O
both	O
variants	O
)	O
.	O
This	O
is	O
a	O
nice	O
result	O
;	O
the	O
GUI	O
could	O
be	O
used	O
in	O
either	O
system	O
with	O
benefit	O
to	O
the	O
users	O
.	O

The	O
endpointed	O
(	O
Phase	O
1	O
)	O
and	O
incremental	O
(	O
Phase	O
2	O
)	O
columns	O
in	O
Table	O
1	O
show	O
the	O
results	O
of	O
the	O
objective	O
evaluation	O
.	O
incremental	O
variant	O
,	O
the	O
total	O
number	O
of	O
tasks	O
for	O
the	O
incremental	O
variant	O
was	O
higher	O
.	O
Manual	O
inspection	O
of	O
logs	O
indicate	O
that	O
participants	O
took	O
advantage	O
of	O
the	O
system	O
's	O
flexibility	O
of	O
understanding	O
instalments	O
(	O
i.e.	O
,	O
filling	O
frames	O
incrementally	O
)	O
.	O
This	O
is	O
evidenced	O
in	O
that	O
participants	O
often	O
uttered	O
words	O
understood	O
by	O
the	O
system	O
as	O
being	O
negative	O
(	O
e.g.	O
,	O
nein	O
/	O
no	O
)	O
,	O
either	O
as	O
a	O
result	O
of	O
an	O
explicit	O
confirmation	O
request	O
by	O
the	O
system	O
(	O
e.g.	O
,	O
Thai	O
?	O
)	O
or	O
after	O
a	O
slot	O
was	O
incorrectly	O
filled	O
(	O
something	O
very	O
easily	O
determined	O
through	O
the	O
GUI	O
)	O
.	O
This	O
is	O
a	O
desired	O
outcome	O
of	O
using	O
our	O
system	O
;	O
participants	O
were	O
able	O
to	O
repair	O
local	O
areas	O
of	O
misunderstanding	O
as	O
they	O
took	O
place	O
instead	O
of	O
needing	O
to	O
correct	O
an	O
entire	O
intent	O
(	O
i.e.	O
,	O
frame	O
)	O
.	O
However	O
,	O
we	O
can	O
not	O
fully	O
empirically	O
measure	O
these	O
tendencies	O
given	O
our	O
data	O
.	O

Incremental	O
-	O
Adaptive	O
In	O
this	O
section	O
we	O
report	O
results	O
for	O
the	O
evaluation	O
between	O
the	O
incremental	O
(	O
Phase	O
2	O
)	O
and	O
incremental	O
-	O
adaptive	O
(	O
henceforth	O
just	O
adaptive	O
;	O
Phase	O
3	O
)	O
systems	O
.	O

We	O
applied	O
the	O
same	O
significance	O
test	O
as	O
Experiment	O
1	O
(	O
with	O
Bonferroni	O
correction	O
of	O
12	O
)	O
.	O
The	O
item	O
The	O
interface	O
was	O
useful	O
and	O
easy	O
to	O
understand	O
was	O
answered	O
with	O
Both	O
significantly	O
(	O
χ	O
2	O
(	O
4	O
,	O
N	O
=	O
14	O
)	O
=	O
10.0	O
,	O
p	O
<	O
.0042	O
)	O
,	O
The	O
item	O
I	O
had	O
the	O
feeling	O
that	O
the	O
assistant	O
attempted	O
to	O
learn	O
about	O
me	O
was	O
answered	O
with	O
Neither	O
(	O
χ	O
2	O
(	O
4	O
,	O
N	O
=	O
14	O
)	O
=	O
8.0	O
,	O
p	O
<	O
.0042	O
)	O
,	O
though	O
Phase	O
3	O
was	O
also	O
marked	O
(	O
6	O
times	O
)	O
.	O
All	O
other	O
items	O
were	O
not	O
significant	O
.	O
Here	O
again	O
we	O
see	O
that	O
there	O
is	O
a	O
general	O
positive	O
impression	O
of	O
the	O
GUI	O
under	O
all	O
conditions	O
.	O
If	O
anyone	O
noticed	O
that	O
a	O
system	O
variant	O
was	O
attempting	O
to	O
learn	O
a	O
user	O
model	O
at	O
all	O
,	O
they	O
noticed	O
that	O
it	O
was	O
in	O
Phase	O
3	O
,	O
as	O
expected	O
.	O

The	O
incremental	O
(	O
Phase	O
2	O
)	O
and	O
adaptive	O
(	O
Phase	O
3	O
)	O
columns	O
in	O
Table	O
1	O
show	O
the	O
results	O
for	O
the	O
objective	O
evaluation	O
for	O
this	O
experiment	O
.	O
There	O
is	O
a	O
clear	O
difference	O
between	O
the	O
two	O
variants	O
,	O
with	O
the	O
adaptive	O
showing	O
more	O
completed	O
tasks	O
,	O
more	O
fully	O
correct	O
frames	O
,	O
and	O
a	O
higher	O
average	O
fscore	O
(	O
all	O
three	O
likely	O
due	O
to	O
the	O
fact	O
that	O
frames	O
were	O
potentially	O
pre	O
-	O
filled	O
)	O
.	O

While	O
the	O
responses	O
do	O
n't	O
express	O
any	O
preference	O
for	O
a	O
particular	O
system	O
variant	O
,	O
the	O
overall	O
impression	O
of	O
the	O
GUI	O
was	O
positive	O
.	O
The	O
objective	O
measures	O
show	O
that	O
there	O
are	O
gains	O
to	O
be	O
made	O
when	O
the	O
system	O
signals	O
understanding	O
at	O
a	O
more	O
finegrained	O
interval	O
than	O
at	O
the	O
utterance	O
level	O
,	O
due	O
to	O
the	O
higher	O
number	O
of	O
completed	O
tasks	O
and	O
locallymade	O
repairs	O
.	O
There	O
are	O
further	O
gains	O
to	O
be	O
made	O
when	O
the	O
system	O
applies	O
simple	O
user	O
modelling	O
(	O
i.e.	O
,	O
adaptivity	O
)	O
by	O
attempting	O
to	O
predict	O
what	O
the	O
user	O
might	O
want	O
to	O
do	O
in	O
a	O
chosen	O
domain	O
,	O
decreasing	O
the	O
possibility	O
of	O
user	O
error	O
and	O
allowing	O
the	O
system	O
to	O
accurately	O
and	O
quickly	O
complete	O
more	O
tasks	O
.	O
Participants	O
also	O
did	O
n't	O
just	O
get	O
used	O
to	O
the	O
system	O
over	O
time	O
,	O
as	O
the	O
average	O
time	O
per	O
episode	O
was	O
fairly	O
similar	O
in	O
all	O
three	O
phases	O
.	O
The	O
open	O
-	O
ended	O
questionnaire	O
sheds	O
additional	O
light	O
.	O
Most	O
of	O
the	O
suggestions	O
for	O
improvement	O
related	O
to	O
ASR	O
misrecognition	O
and	O
speed	O
(	O
i.e.	O
,	O
not	O
about	O
the	O
system	O
itself	O
)	O
.	O
Two	O
participants	O
suggested	O
an	O
ability	O
to	O
add	O
"	O
free	O
input	O
"	O
or	O
select	O
alternatives	O
from	O
the	O
tree	O
.	O
Two	O
participants	O
suggested	O
that	O
the	O
system	O
be	O
more	O
responsive	O
(	O
i.e.	O
,	O
in	O
wait	O
states	O
)	O
,	O
and	O
give	O
more	O
feedback	O
(	O
i.e.	O
,	O
backchannels	O
)	O
more	O
often	O
.	O
For	O
those	O
participants	O
that	O
expressed	O
preference	O
to	O
the	O
non	O
-	O
incremental	O
system	O
(	O
Phase	O
1	O
)	O
,	O
none	O
of	O
them	O
had	O
used	O
a	O
speech	O
-	O
based	O
PA	O
before	O
,	O
whereas	O
those	O
that	O
expressed	O
preference	O
to	O
the	O
incremental	O
versions	O
(	O
Phases	O
2	O
and	O
3	O
)	O
use	O
them	O
regularly	O
.	O
We	O
conjecture	O
that	O
people	O
without	O
SDS	O
experience	O
equate	O
understanding	O
with	O
ASR	O
,	O
whereas	O
those	O
that	O
are	O
more	O
familiar	O
with	O
PAs	O
know	O
that	O
perfect	O
ASR	O
does	O
n't	O
translate	O
to	O
perfect	O
understanding	O
-	O
hence	O
the	O
need	O
for	O
a	O
GUI	O
.	O
A	O
potential	O
remedy	O
would	O
be	O
to	O
display	O
ASR	O
with	O
the	O
tree	O
,	O
signalling	O
understanding	O
despite	O
ASR	O
errors	O
.	O

Acknowledgements	O
Thanks	O
to	O
the	O
anonymous	O
reviewers	O
who	O
provided	O
useful	O
comments	O
and	O
suggestions	O
.	O
Thanks	O
also	O
to	O
Julian	O
Hough	O
for	O
helping	O
with	O
experiments	O
.	O
We	O
acknowledge	O
support	O
by	O
the	O
Cluster	O
of	O
Excellence	O
"	O
Cognitive	O
Interaction	O
Technology	O
"	O
(	O
CITEC	O
;	O
EXC	O
277	O
)	O
at	O
Bielefeld	O
University	O
,	O
which	O
is	O
funded	O
by	O
the	O
German	O
Research	O
Foundation	O
(	O
DFG	O
)	O
,	O
and	O
the	O
BMBF	O
Kogni	O
-	O
Home	O
project	O
.	O

The	O
following	O
questions	O
were	O
asked	O
on	O
both	O
questionnaires	O
following	O
Phase	O
2	O
and	O
Phase	O
3	O
(	O
comparing	O
the	O
two	O
most	O
latest	O
used	O
system	O
versions	O
;	O
as	O
translated	O
into	O
English	O
)	O
:	O
The	O
interface	O
was	O
useful	O
and	O
easy	O
to	O
understand	O
.	O
The	O
assistant	O
was	O
easy	O
and	O
intuitive	O
to	O
use	O
.	O
The	O
assistant	O
understood	O
what	O
I	O
wanted	O
to	O
say	O
.	O
I	O
always	O
understood	O
what	O
the	O
system	O
wanted	O
from	O
me	O
.	O
The	O
assistant	O
made	O
many	O
mistakes	O
.	O
The	O
assistant	O
did	O
not	O
respond	O
while	O
I	O
spoke	O
.	O

Learning	O
an	O
embedding	O
for	O
every	O
possible	O
token	O
and	O
tag	O
combination	O
would	O
enormously	O
increase	O
the	O
model	O
's	O
learnable	O
parameter	O
count	O
.	O
Furthermore	O
,	O
training	O
data	O
is	O
likely	O
to	O
be	O
sparse	O
in	O
its	O
coverage	O
of	O
all	O
possible	O
pairs	O
,	O
but	O
not	O
in	O
its	O
coverage	O
of	O
the	O
token	O
and	O
tag	O
vocabularies	O
separately	O
.	O
Therefore	O
,	O
we	O
instead	O
learn	O
a	O
separate	O
embedding	O
vector	O
for	O
each	O
possible	O
token	O
and	O
each	O
possible	O
tag	O
,	O
effectively	O
concatenating	O
these	O
two	O
vocabularies	O
(	O
rather	O
than	O
taking	O
the	O
product	O
space	O
)	O
.	O
The	O
embedding	O
vectors	O
for	O
the	O
token	O
and	O
tag	O
at	O
each	O
position	O
are	O
then	O
added	O
to	O
combine	O
information	O
from	O
both	O
channels	O
into	O
a	O
single	O
vector	O
,	O
so	O
as	O
not	O
to	O
increase	O
the	O
size	O
of	O
subsequent	O
model	O
layers	O
and	O
the	O
capacity	O
of	O
the	O
model	O
,	O
apart	O
from	O
the	O
additional	O
tag	O
embedding	O
vectors	O
.	O

Word	O
tokenization	O
,	O
as	O
used	O
by	O
the	O
tagging	O
systems	O
,	O
is	O
most	O
straightforward	O
for	O
maintaining	O
one	O
-	O
to	O
-	O
one	O
alignments	O
between	O
tokens	O
and	O
their	O
assigned	O
tags	O
.	O
For	O
word	O
tokenization	O
experiments	O
,	O
vocabularies	O
of	O
size	O
35	O
,	O
012	O
for	O
German	O
and	O
17	O
,	O
196	O
for	O
English	O
were	O
selected	O
,	O
resulting	O
in	O
an	O
unknown	O
word	O
replacement	O
rate	O
of	O
3	O
%	O
.	O
This	O
unknown	O
word	O
replacement	O
was	O
considerably	O
higher	O
on	O
rare	O
word	O
categories	O
,	O
for	O
example	O
named	O
entities	O
saw	O
a	O
25	O
-	O
30	O
%	O
rate	O
of	O
unknown	O
words	O
outside	O
the	O
selected	O
word	O
vocabulary	O
.	O
To	O
alleviate	O
this	O
it	O
is	O
also	O
possible	O
to	O
consider	O
subword	O
(	O
Kudo	O
,	O
2018	O
)	O
vocabulary	O
of	O
32	O
,	O
000	O
subwords	O
,	O
built	O
from	O
the	O
training	O
split	O
and	O
used	O
to	O
tokenize	O
both	O
languages	O
.	O
After	O
subword	O
tokenization	O
,	O
the	O
BIOES	O
structure	O
of	O
named	O
entity	O
spans	O
was	O
propagated	O
across	O
subword	O
tokens	O
in	O
the	O
natural	O
way	O
to	O
maintain	O
spans	O
.	O
For	O
POS	O
tags	O
,	O
subwords	O
received	O
the	O
same	O
tag	O
as	O
their	O
parent	O
word	O
.	O

Many	O
thanks	O
go	O
to	O
my	O
colleagues	O
Jeesoo	O
Bang	O
,	O
Jaehun	O
Shin	O
,	O
and	O
Baikjin	O
Jung	O
in	O
the	O
Knowledge	O
and	O
Language	O
Engineering	O
Lab	O
(	O
POSTECH	O
)	O
for	O
their	O
many	O
hours	O
generously	O
spent	O
discussing	O
these	O
research	O
topics	O
.	O
These	O
results	O
would	O
not	O
have	O
been	O
possible	O
without	O
their	O
support	O
.	O
This	O

Māori	O
Loanwords	O
:	O
A	O
Corpus	O
of	O
New	O
Zealand	O
English	O
Tweets	O

Māori	O
loanwords	O
are	O
widely	O
used	O
in	O
New	O
Zealand	O
English	O
for	O
various	O
social	O
functions	O
by	O
New	O
Zealanders	O
within	O
and	O
outside	O
of	O
the	O
Māori	O
community	O
.	O
Motivated	O
by	O
the	O
lack	O
of	O
linguistic	O
resources	O
for	O
studying	O
how	O
Māori	O
loanwords	O
are	O
used	O
in	O
social	O
media	O
,	O
we	O
present	O
a	O
new	O
corpus	O
of	O
New	O
Zealand	O
English	O
tweets	O
.	O
We	O
collected	O
tweets	O
containing	O
selected	O
Māori	O
words	O
that	O
are	O
likely	O
to	O
be	O
known	O
by	O
New	O
Zealanders	O
who	O
do	O
not	O
speak	O
Māori	O
.	O
Since	O
over	O
30	O
%	O
of	O
these	O
words	O
turned	O
out	O
to	O
be	O
irrelevant	O
(	O
e.g.	O
,	O
mana	O
is	O
a	O
popular	O
gaming	O
term	O
,	O
Moana	O
is	O
a	O
character	O
from	O
a	O
Disney	O
movie	O
)	O
,	O
we	O
manually	O
annotated	O
a	O
sample	O
of	O
our	O
tweets	O
into	O
relevant	O
and	O
irrelevant	O
categories	O
.	O
This	O
data	O
was	O
used	O
to	O
train	O
machine	O
learning	O
models	O
to	O
automatically	O
filter	O
out	O
irrelevant	O
tweets	O
.	O

One	O
of	O
the	O
most	O
salient	O
features	O
of	O
New	O
Zealand	O
English	O
(	O
NZE	O
)	O
is	O
the	O
widespread	O
use	O
of	O
Māori	O
words	O
(	O
loanwords	O
)	O
,	O
such	O
as	O
aroha	O
(	O
love	O
)	O
,	O
kai	O
(	O
food	O
)	O
and	O
Aotearoa	O
(	O
New	O
Zealand	O
)	O
.	O
See	O
ex	O
.	O
(	O
1	O
)	O
specifically	O
from	O
Twitter	O
(	O
note	O
the	O
informal	O
,	O
conversational	O
style	O
and	O
the	O
Māori	O
loanwords	O
emphasised	O
in	O
bold	O
)	O
.	O
(	O
1	O
)	O
Led	O
the	O
waiata	O
for	O
the	O
manuhiri	O
at	O
the	O
pōwhiri	O
for	O
new	O
staff	O
for	O
induction	O
week	O
.	O
Was	O
told	O
by	O
the	O
kaumātua	O
I	O
did	O
it	O
with	O
mana	O
and	O
integrity	O
.	O
The	O
use	O
of	O
Māori	O
words	O
has	O
been	O
studied	O
intensively	O
over	O
the	O
past	O
thirty	O
years	O
,	O
offering	O
a	O
comprehensive	O
insight	O
into	O
the	O
evolution	O
of	O
one	O
of	O
the	O
youngest	O
dialects	O
of	O
English	O
-	O
New	O
Zealand	O
English	O
(	O
Calude	O
et	O
al	O
,	O
2017	O
;	O
Daly	O
,	O
2007Daly	O
,	O
,	O
2016	O
;	O
Davies	O
and	O
Maclagan	O
,	O
2006	O
;	O
De	O
Bres	O
,	O
2006	O
;	O
Degani	O
and	O
Onysko	O
,	O
2010	O
;	O
Kennedy	O
and	O
Yamazaki	O
,	O
1999	O
;	O
Macalister	O
,	O
2009Macalister	O
,	O
,	O
2006aOnysko	O
and	O
Calude	O
,	O
2013	O
)	O
.	O
One	O
aspect	O
which	O
is	O
missing	O
in	O
this	O
body	O
of	O
work	O
is	O
the	O
online	O
discourse	O
presence	O
of	O
the	O
loanwords	O
-	O
almost	O
all	O
studies	O
come	O
from	O
(	O
collaborative	O
)	O
written	O
language	O
(	O
highly	O
edited	O
,	O
revised	O
and	O
scrutinised	O
newspaper	O
language	O
,	O
Davies	O
and	O
Maclagan	O
2006	O
;	O
Macalister	O
2009Macalister	O
,	O
2006aOnysko	O
and	O
Calude	O
2013	O
,	O
and	O
picture	O
-	O
books	O
,	O
Daly	O
2007	O
,	O
2016	O
,	O
or	O
from	O
spoken	O
language	O
collected	O
in	O
the	O
late	O
1990s	O
(	O
Kennedy	O
and	O
Yamazaki	O
,	O
1999	O
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
build	O
a	O
corpus	O
of	O
New	O
Zealand	O
English	O
tweets	O
containing	O
Māori	O
loanwords	O
.	O
Building	O
such	O
a	O
corpus	O
has	O
its	O
challenges	O
(	O
as	O
discussed	O
in	O
Section	O
3.1	O
)	O
.	O
Before	O
we	O
discuss	O
these	O
,	O
it	O
is	O
important	O
to	O
highlight	O
the	O
uniqueness	O
of	O
the	O
language	O
contact	O
situation	O
between	O
Māori	O
and	O
(	O
NZ	O
)	O
English	O
.	O
The	O
language	O
contact	O
situation	O
in	O
New	O
Zealand	O
provides	O
a	O
unique	O
case	O
-	O
study	O
for	O
loanwords	O
because	O
of	O
a	O
number	O
of	O
factors	O
.	O
We	O
list	O
three	O
particularly	O
relevant	O
here	O
.	O
First	O
,	O
the	O
direction	O
of	O
lexical	O
transfer	O
is	O
highly	O
unusual	O
,	O
namely	O
,	O
from	O
an	O
endangered	O
indigenous	O
language	O
(	O
Māori	O
)	O
into	O
a	O
dominant	O
lingua	O
franca	O
(	O
English	O
)	O
.	O
The	O
large	O
-	O
scale	O
lexical	O
transfer	O
of	O
this	O
type	O
has	O
virtually	O
never	O
been	O
documented	O
elsewhere	O
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
(	O
see	O
summary	O
of	O
current	O
language	O
contact	O
situations	O
in	O
Stammers	O
and	O
Deuchar	O
2012	O
,	O
particularly	O
Table	O
1	O
,	O
p.	O
634	O
)	O
.	O
Secondly	O
,	O
because	O
Māori	O
loanwords	O
are	O
"	O
New	O
Zealand	O
's	O
and	O
New	O
Zealand	O
's	O
alone	O
"	O
(	O
Deverson	O
,	O
1991	O
,	O
p.	O
18	O
-	O
19	O
)	O
,	O
and	O
above	O
speakers	O
'	O
consciousness	O
,	O
their	O
ardent	O
study	O
over	O
the	O
years	O
provides	O
a	O
fruitful	O
comparison	O
of	O
the	O
use	O
of	O
loanwords	O
across	O
genres	O
,	O
contexts	O
and	O
time	O
.	O
Finally	O
,	O
the	O
aforementioned	O
body	O
of	O
previous	O
research	O
on	O
the	O
topic	O
is	O
rich	O
and	O
detailed	O
,	O
and	O
still	O
rapidly	O
changing	O
,	O
with	O
loanword	O
use	O
being	O
an	O
increasing	O
trend	O
(	O
Macalister	O
,	O
2006a	O
;	O
Kennedy	O
and	O
Yamazaki	O
,	O
1999	O
)	O
.	O
However	O
,	O
the	O
jury	O
is	O
still	O
out	O
regarding	O
the	O
reasons	O
for	O
the	O
loanword	O
use	O
(	O
some	O
hypotheses	O
have	O
been	O
put	O
forward	O
)	O
,	O
and	O
the	O
pat	O
-	O
terns	O
of	O
use	O
across	O
different	O
genres	O
(	O
it	O
is	O
unclear	O
how	O
language	O
formality	O
influences	O
loanword	O
use	O
)	O
.	O
We	O
find	O
that	O
Twitter	O
data	O
complements	O
the	O
growing	O
body	O
of	O
work	O
on	O
Māori	O
loanwords	O
in	O
NZE	O
,	O
by	O
adding	O
a	O
combination	O
of	O
institutional	O
and	O
individual	O
linguistic	O
exchanges	O
,	O
in	O
a	O
non	O
-	O
editable	O
online	O
platform	O
.	O
Social	O
media	O
language	O
shares	O
properties	O
with	O
both	O
spoken	O
and	O
written	O
language	O
,	O
but	O
is	O
not	O
exactly	O
like	O
either	O
.	O
More	O
specifically	O
,	O
Twitter	O
allows	O
for	O
creative	O
expression	O
and	O
lexical	O
innovation	O
(	O
Grieve	O
et	O
al	O
,	O
2017	O
)	O
.	O
Our	O
Twitter	O
corpus	O
was	O
created	O
by	O
following	O
three	O
main	O
steps	O
:	O
collecting	O
tweets	O
over	O
a	O
ten	O
-	O
year	O
period	O
using	O
"	O
query	O
words	O
"	O
(	O
Section	O
3.1	O
)	O
,	O
manually	O
labelling	O
thousands	O
of	O
randomly	O
-	O
sampled	O
tweets	O
as	O
"	O
relevant	O
"	O
or	O
"	O
irrelevant	O
"	O
(	O
Section	O
3.2	O
)	O
,	O
and	O
then	O
training	O
a	O
classifier	O
to	O
obtain	O
automatic	O
predictions	O
for	O
the	O
relevance	O
of	O
each	O
tweet	O
and	O
deploying	O
this	O
model	O
on	O
our	O
target	O
tweets	O
,	O
in	O
a	O
bid	O
to	O
filter	O
out	O
all	O
those	O
which	O
are	O
"	O
irrelevant	O
"	O
(	O
Section	O
3.3	O
)	O
.	O
As	O
will	O
be	O
discussed	O
in	O
Section	O
2	O
,	O
our	O
corpus	O
is	O
not	O
the	O
first	O
of	O
its	O
kind	O
but	O
is	O
the	O
first	O
corpus	O
of	O
New	O
Zealand	O
English	O
tweets	O
and	O
the	O
first	O
collection	O
of	O
online	O
discourse	O
built	O
specifically	O
to	O
analyse	O
the	O
use	O
of	O
Māori	O
loanwords	O
in	O
NZE	O
.	O
Section	O
4	O
outlines	O
some	O
preliminary	O
findings	O
from	O
our	O
corpus	O
and	O
Section	O
5	O
lays	O
out	O
plans	O
for	O
future	O
work	O
.	O

It	O
is	O
uncontroversial	O
that	O
Māori	O
loanwords	O
are	O
both	O
productively	O
used	O
in	O
NZE	O
and	O
increasing	O
in	O
popularity	O
(	O
Macalister	O
,	O
2006a	O
)	O
.	O
The	O
corpora	O
analysed	O
previously	O
indicate	O
that	O
loanword	O
use	O
is	O
highly	O
skewed	O
,	O
with	O
some	O
language	O
users	O
leading	O
the	O
way	O
-	O
specifically	O
Māori	O
women	O
(	O
Calude	O
et	O
al	O
,	O
2017	O
;	O
Kennedy	O
and	O
Yamazaki	O
,	O
1999	O
)	O
,	O
and	O
with	O
certain	O
topics	O
of	O
discourse	O
drawing	O
significantly	O
higher	O
counts	O
of	O
loanwords	O
than	O
others	O
-	O
specifically	O
those	O
related	O
to	O
Māori	O
people	O
and	O
Māori	O
affairs	O
,	O
Māoritanga	O
(	O
Degani	O
,	O
2010	O
)	O
.	O
The	O
type	O
of	O
loanwords	O
being	O
borrowed	O
from	O
Māori	O
is	O
also	O
changing	O
.	O
During	O
the	O
first	O
wave	O
of	O
borrowing	O
,	O
some	O
two	O
-	O
hundred	O
years	O
ago	O
,	O
many	O
flora	O
and	O
fauna	O
words	O
were	O
being	O
borrowed	O
;	O
today	O
,	O
it	O
is	O
social	O
culture	O
terms	O
that	O
are	O
increasingly	O
adopted	O
,	O
e.g.	O
,	O
aroha	O
(	O
love	O
)	O
,	O
whaea	O
(	O
woman	O
,	O
teacher	O
)	O
,	O
and	O
tangi	O
(	O
Māori	O
funeral	O
)	O
,	O
see	O
Macalister	O
(	O
2006a	O
)	O
.	O
However	O
,	O
the	O
data	O
available	O
for	O
loanword	O
analysis	O
is	O
either	O
outdated	O
(	O
Calude	O
et	O
al	O
,	O
2017	O
;	O
Kennedy	O
and	O
Yamazaki	O
,	O
1999	O
)	O
,	O
or	O
exclusively	O
formal	O
and	O
highly	O
edited	O
(	O
mainly	O
newspaper	O
language	O
,	O
Macalister	O
2006a	O
;	O
Davies	O
and	O
Maclagan	O
2006	O
;	O
Degani	O
2010	O
)	O
,	O
so	O
little	O
is	O
known	O
about	O
Māori	O
loanwords	O
in	O
recent	O
informal	O
NZE	O
interactions	O
-	O
a	O
gap	O
we	O
hope	O
to	O
address	O
here	O
.	O
With	O
the	O
availability	O
of	O
vast	O
amounts	O
of	O
data	O
,	O
building	O
Twitter	O
corpora	O
has	O
been	O
a	O
fruitful	O
endeavour	O
in	O
various	O
languages	O
,	O
including	O
Turkish	O
(	O
Ş	O
imşek	O
andÖzdemir	O
,	O
2012	O
;	O
Ç	O
etinoglu	O
,	O
2016	O
)	O
,	O
Greek	O
(	O
Sifianou	O
,	O
2015	O
)	O
,	O
German	O
(	O
Scheffler	O
,	O
2014	O
;	O
Cieliebak	O
et	O
al	O
,	O
2017	O
)	O
,	O
and	O
(	O
American	O
)	O
English	O
(	O
Huang	O
et	O
al	O
,	O
2016	O
)	O
(	O
though	O
notably	O
,	O
not	O
New	O
Zealand	O
English	O
,	O
while	O
a	O
modest	O
corpus	O
of	O
te	O
reo	O
Māori	O
tweets	O
does	O
exist	O
,	O
Keegan	O
et	O
al	O
2015	O
)	O
.	O
Twitter	O
corpora	O
of	O
mixed	O
languages	O
are	O
tougher	O
to	O
collect	O
because	O
it	O
is	O
not	O
straightforward	O
to	O
detect	O
mixed	O
language	O
data	O
automatically	O
.	O
Geolocations	O
can	O
help	O
to	O
some	O
extent	O
,	O
but	O
they	O
have	O
limitations	O
(	O
most	O
users	O
do	O
not	O
use	O
them	O
to	O
begin	O
with	O
)	O
.	O
Recent	O
work	O
on	O
Arabic	O
has	O
leveraged	O
the	O
presence	O
of	O
distinct	O
scripts	O
-	O
the	O
Roman	O
and	O
Arabic	O
alphabet	O
-	O
to	O
create	O
a	O
mixed	O
language	O
corpus	O
(	O
Voss	O
et	O
al	O
,	O
2014	O
)	O
,	O
but	O
this	O
option	O
is	O
not	O
available	O
to	O
us	O
.	O
Māori	O
has	O
traditionally	O
been	O
a	O
spoken	O
(	O
only	O
)	O
language	O
,	O
and	O
was	O
first	O
written	O
down	O
in	O
the	O
early	O
1800s	O
by	O
European	O
missionaries	O
in	O
conjunction	O
with	O
Māori	O
language	O
scholars	O
,	O
using	O
the	O
Roman	O
alphabet	O
(	O
Smyth	O
,	O
1946	O
)	O
.	O
Our	O
task	O
is	O
more	O
similar	O
to	O
studies	O
such	O
as	O
Das	O
andGambäck	O
(	O
2014	O
)	O
andÇ	O
etinoglu	O
(	O
2016	O
)	O
,	O
who	O
aim	O
to	O
find	O
a	O
mix	O
of	O
two	O
languages	O
which	O
share	O
the	O
same	O
script	O
(	O
in	O
their	O
case	O
,	O
Hindi	O
and	O
English	O
,	O
and	O
Turkish	O
and	O
German	O
,	O
respectively	O
)	O
,	O
but	O
our	O
method	O
for	O
collecting	O
tweets	O
is	O
not	O
user	O
-	O
based	O
;	O
instead	O
we	O
use	O
a	O
set	O
of	O
target	O
query	O
words	O
,	O
as	O
detailed	O
in	O
Section	O
3.1	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
the	O
process	O
of	O
building	O
the	O
Māori	O
Loanword	O
Twitter	O
Corpus	O
(	O
hereafter	O
,	O
the	O
MLT	O
Corpus	O
)	O
1	O
.	O
This	O
process	O
consists	O
of	O
three	O
main	O
steps	O
,	O
as	O
depicted	O
in	O
Figure	O
1	O
.	O

In	O
order	O
to	O
facilitate	O
the	O
collection	O
of	O
relevant	O
data	O
for	O
the	O
MLT	O
Corpus	O
,	O
we	O
compiled	O
a	O
list	O
of	O
116	O
target	O
loanwords	O
,	O
which	O
we	O
will	O
call	O
"	O
query	O
words	O
"	O
.	O
Most	O
of	O
these	O
are	O
individual	O
words	O
but	O
some	O
are	O
short	O
phrasal	O
units	O
(	O
tangata	O
whenua	O
,	O
people	O
of	O
the	O
land	O
;	O
kapa	O
haka	O
,	O
cultural	O
performance	O
)	O
.	O
The	O
list	O
is	O
largely	O
derived	O
from	O
Hay	O
(	O
2018	O
)	O
but	O
was	O
modified	O
to	O
exclude	O
function	O
words	O
(	O
such	O
as	O
numerals	O
)	O
and	O
most	O
proper	O
nouns	O
,	O
except	O
five	O
that	O
have	O
native	O
English	O
counterparts	O
:	O
Aotearoa	O
(	O
New	O
Zealand	O
)	O
,	O
Kiwi	O
(	O
s	O
)	O
(	O
New	O
Zealander	O
(	O
s	O
)	O
)	O
,	O
Māori	O
(	O
indigenous	O
New	O
Zealander	O
)	O
,	O
Pākehā	O
(	O
European	O
New	O
Zealander	O
)	O
,	O
non	O
-	O
Māori	O
(	O
non	O
-	O
indigenous	O
New	O
Zealander	O
)	O
.	O
We	O
also	O
added	O
three	O
further	O
loanwords	O
which	O
we	O
deemed	O
useful	O
for	O
increasing	O
our	O
data	O
,	O
namely	O
haurangi	O
(	O
drunk	O
)	O
,	O
wairangi	O
(	O
drugged	O
,	O
confused	O
)	O
,	O
and	O
pōrangi	O
(	O
crazy	O
)	O
.	O
Using	O
the	O
Twitter	O
Search	O
API	O
,	O
we	O
harvested	O
8	O
million	O
tweets	O
containing	O
at	O
least	O
one	O
query	O
word	O
(	O
after	O
converting	O
all	O
characters	O
to	O
lowercase	O
)	O
.	O
The	O
tweets	O
were	O
collected	O
diachronically	O
over	O
an	O
eleven	O
year	O
period	O
,	O
between	O
2007	O
-	O
2018	O
.	O
We	O
ensured	O
that	O
tweets	O
were	O
(	O
mostly	O
)	O
written	O
in	O
English	O
by	O
using	O
the	O
lang	O
:	O
en	O
parameter	O
.	O
A	O
number	O
of	O
exclusions	O
and	O
further	O
adjustments	O
were	O
made	O
.	O
With	O
the	O
aim	O
of	O
avoiding	O
redundancy	O
and	O
uninformative	O
data	O
,	O
retweets	O
and	O
tweets	O
with	O
URLs	O
were	O
discarded	O
.	O
Tweets	O
in	O
which	O
the	O
query	O
word	O
was	O
used	O
as	O
part	O
of	O
a	O
username	O
or	O
mention	O
(	O
e.g.	O
,	O
@happy	O
kiwi	O
)	O
were	O
also	O
discarded	O
.	O
For	O
those	O
query	O
words	O
which	O
contained	O
macrons	O
,	O
we	O
found	O
that	O
users	O
were	O
inconsistent	O
in	O
their	O
macron	O
use	O
.	O
Consequently	O
,	O
we	O
consolidated	O
the	O
data	O
by	O
adjusting	O
our	O
search	O
to	O
include	O
both	O
the	O
macron	O
and	O
the	O
non	O
-	O
macron	O
version	O
(	O
e.g.	O
,	O
both	O
Māori	O
and	O
Maori	O
)	O
.	O
We	O
also	O
removed	O
all	O
tweets	O
containing	O
fewer	O
than	O
five	O
tokens	O
(	O
words	O
)	O
,	O
due	O
to	O
insufficient	O
context	O
of	O
analysis	O
.	O
Owing	O
to	O
relaxed	O
spelling	O
conventions	O
on	O
Twitter	O
(	O
and	O
also	O
the	O
use	O
of	O
hashtags	O
)	O
,	O
certain	O
query	O
words	O
comprising	O
multiple	O
lexical	O
items	O
were	O
stripped	O
of	O
spaces	O
in	O
order	O
to	O
harvest	O
all	O
variants	O
of	O
the	O
phrasal	O
units	O
(	O
e.g.	O
,	O
kai	O
moana	O
and	O
kaimoana	O
)	O
.	O
As	O
kai	O
was	O
itself	O
a	O
query	O
word	O
(	O
in	O
its	O
own	O
right	O
)	O
,	O
we	O
excluded	O
tweets	O
containing	O
kai	O
moana	O
when	O
searching	O
for	O
tweets	O
containing	O
kai	O
(	O
and	O
repeated	O
this	O
process	O
with	O
similar	O
items	O
)	O
.	O
After	O
inspecting	O
these	O
tweets	O
,	O
it	O
was	O
clear	O
that	O
a	O
large	O
number	O
of	O
our	O
query	O
words	O
were	O
polysemous	O
(	O
or	O
otherwise	O
unrelated	O
to	O
NZE	O
)	O
,	O
and	O
had	O
introduced	O
a	O
significant	O
amount	O
of	O
noise	O
into	O
the	O
data	O
.	O
The	O
four	O
main	O
challenges	O
we	O
encountered	O
are	O
described	O
below	O
.	O
First	O
,	O
because	O
Twitter	O
contains	O
many	O
different	O
varieties	O
of	O
English	O
,	O
NZE	O
being	O
just	O
one	O
of	O
these	O
,	O
it	O
is	O
not	O
always	O
straightforward	O
to	O
disentangle	O
the	O
dialect	O
of	O
English	O
spoken	O
in	O
New	O
Zealand	O
from	O
other	O
dialects	O
of	O
English	O
.	O
This	O
could	O
be	O
a	O
problem	O
when	O
,	O
for	O
instance	O
,	O
a	O
Māori	O
word	O
like	O
Moana	O
(	O
sea	O
)	O
is	O
used	O
in	O
American	O
English	O
tweets	O
to	O
denote	O
the	O
Disney	O
movie	O
(	O
or	O
its	O
main	O
character	O
)	O
.	O
Secondly	O
,	O
Māori	O
words	O
have	O
cognate	O
forms	O
with	O
other	O
Austronesian	O
languages	O
,	O
such	O
as	O
Hawaiian	O
,	O
Samoan	O
and	O
Tongan	O
,	O
and	O
many	O
speakers	O
of	O
these	O
languages	O
live	O
and	O
work	O
(	O
and	O
tweet	O
)	O
in	O
New	O
Zealand	O
.	O
For	O
instance	O
,	O
the	O
word	O
wahine	O
(	O
woman	O
)	O
has	O
the	O
same	O
written	O
form	O
in	O
Māori	O
and	O
in	O
Hawaiian	O
.	O
But	O
cognates	O
are	O
not	O
the	O
only	O
problematic	O
words	O
.	O
Homographs	O
with	O
other	O
,	O
genealogically	O
-	O
unrelated	O
languages	O
can	O
also	O
pose	O
problems	O
.	O
For	O
instance	O
,	O
the	O
Māori	O
word	O
hui	O
(	O
meeting	O
)	O
is	O
sometimes	O
used	O
as	O
a	O
proper	O
noun	O
in	O
Chinese	O
,	O
as	O
can	O
be	O
seen	O
in	O
the	O
following	O
tweet	O
:	O
"	O
Yo	O
is	O
Tay	O
Peng	O
Hui	O
okay	O
with	O
the	O
tip	O
of	O
his	O
finger	O
?	O
"	O
.	O
Proper	O
nouns	O
constitute	O
a	O
third	O
problematic	O
aspect	O
in	O
our	O
data	O
.	O
As	O
is	O
typical	O
for	O
many	O
language	O
contact	O
situations	O
where	O
an	O
indigenous	O
language	O
shares	O
the	O
same	O
geographical	O
space	O
as	O
an	O
incoming	O
language	O
,	O
Māori	O
has	O
contributed	O
many	O
place	O
names	O
and	O
personal	O
names	O
to	O
NZE	O
,	O
such	O
as	O
Timaru	O
,	O
Aoraki	O
,	O
Titirangi	O
,	O
Hēmi	O
,	O
Mere	O
and	O
so	O
on	O
.	O
While	O
these	O
proper	O
nouns	O
theoretically	O
count	O
as	O
loanwords	O
,	O
we	O
are	O
less	O
interested	O
in	O
them	O
than	O
in	O
content	O
words	O
because	O
the	O
use	O
of	O
the	O
former	O
does	O
not	O
constitute	O
a	O
choice	O
,	O
whereas	O
the	O
use	O
of	O
the	O
latter	O
does	O
(	O
in	O
many	O
cases	O
)	O
.	O
The	O
"	O
choice	O
"	O
of	O
whether	O
to	O
use	O
a	O
loanword	O
or	O
whether	O
to	O
use	O
a	O
native	O
English	O
word	O
(	O
or	O
sometimes	O
a	O
native	O
English	O
phrase	O
)	O
is	O
interesting	O
to	O
study	O
because	O
it	O
provides	O
insights	O
into	O
idiolectal	O
lexical	O
preferences	O
(	O
which	O
words	O
different	O
speakers	O
or	O
writers	O
prefer	O
in	O
given	O
contexts	O
)	O
and	O
relative	O
borrowing	O
success	O
rates	O
(	O
Calude	O
et	O
al	O
,	O
2017	O
;	O
Zenner	O
et	O
al	O
,	O
2012	O
)	O
.	O
Finally	O
,	O
given	O
the	O
impromptu	O
and	O
spontaneous	O
nature	O
of	O
Twitter	O
in	O
general	O
,	O
we	O
found	O
that	O
certain	O
Māori	O
words	O
coincided	O
with	O
misspelled	O
versions	O
of	O
intended	O
native	O
English	O
words	O
,	O
e.g.	O
,	O
whare	O
(	O
house	O
)	O
instead	O
of	O
where	O
.	O
The	O
resulting	O
collection	O
of	O
tweets	O
,	O
termed	O
the	O
Original	O
Dataset	O
,	O
was	O
used	O
to	O
create	O
the	O
Raw	O
Corpus	O
,	O
as	O
explained	O
below	O
.	O

We	O
decided	O
to	O
address	O
the	O
"	O
noisy	O
"	O
tweets	O
in	O
our	O
data	O
using	O
supervised	O
machine	O
learning	O
.	O
Two	O
coders	O
manually	O
inspected	O
a	O
random	O
sample	O
of	O
30	O
tweets	O
for	O
each	O
query	O
word	O
,	O
by	O
checking	O
the	O
word	O
's	O
context	O
of	O
use	O
,	O
and	O
labelled	O
each	O
tweet	O
as	O
"	O
relevant	O
"	O
or	O
"	O
irrelevant	O
"	O
.	O
For	O
example	O
,	O
a	O
tweet	O
like	O
that	O
in	O
example	O
(	O
1	O
)	O
would	O
be	O
coded	O
as	O
relevant	O
and	O
one	O
like	O
"	O
awesome	O
!	O
!	O
Congrats	O
to	O
Tangi	O
:	O
)	O
"	O
,	O
would	O
be	O
coded	O
as	O
irrelevant	O
(	O
because	O
the	O
query	O
word	O
tangi	O
is	O
used	O
as	O
a	O
proper	O
noun	O
)	O
.	O
Since	O
39	O
of	O
the	O
query	O
words	O
consistently	O
yielded	O
irrelevant	O
tweets	O
(	O
at	O
least	O
90	O
%	O
of	O
the	O
time	O
)	O
,	O
these	O
(	O
and	O
the	O
tweets	O
they	O
occurred	O
in	O
)	O
were	O
removed	O
altogether	O
from	O
the	O
data	O
.	O
Our	O
annotators	O
produced	O
a	O
total	O
of	O
3	O
,	O
685	O
labelled	O
tweets	O
for	O
the	O
remaining	O
77	O
query	O
words	O
,	O
which	O
comprise	O
the	O
Labelled	O
Corpus	O
(	O
see	O
Tables	O
1	O
and	O
4	O
;	O
note	O
that	O
irrelevant	O
tweets	O
have	O
been	O
removed	O
from	O
the	O
latter	O
for	O
linguistic	O
analysis	O
)	O
.	O
Assuming	O
our	O
coded	O
samples	O
are	O
representative	O
of	O
the	O
real	O
distribution	O
of	O
relevant	O
/	O
irrelevant	O
tweets	O
that	O
occur	O
with	O
each	O
query	O
word	O
,	O
it	O
makes	O
sense	O
to	O
also	O
discard	O
the	O
39	O
"	O
noisy	O
"	O
query	O
words	O
from	O
our	O
Original	O
Dataset	O
.	O
In	O
this	O
way	O
,	O
we	O
created	O
the	O
(	O
unlabelled	O
)	O
Raw	O
Corpus	O
,	O
which	O
is	O
a	O
fifth	O
of	O
the	O
size	O
(	O
see	O
Table	O
4	O
)	O
.	O
We	O
computed	O
an	O
inter	O
-	O
rater	O
reliability	O
score	O
for	O
our	O
two	O
coders	O
,	O
based	O
on	O
a	O
random	O
sample	O
of	O
200	O
tweets	O
.	O
Using	O
Cohen	O
's	O
Kappa	O
,	O
we	O
calculated	O
this	O
value	O
to	O
be	O
0.87	O
(	O
"	O
strong	O
"	O
)	O
.	O
In	O
light	O
of	O
the	O
strong	O
agreement	O
between	O
the	O
initial	O
coders	O
,	O
no	O
further	O
coders	O
were	O
enlisted	O
for	O
the	O
task	O
.	O

As	O
we	O
are	O
only	O
just	O
beginning	O
to	O
sift	O
through	O
the	O
MLT	O
Corpus	O
,	O
we	O
note	O
two	O
particular	O
sets	O
of	O
preliminary	O
findings	O
.	O
First	O
,	O
even	O
though	O
our	O
corpus	O
was	O
primarily	O
geared	O
up	O
to	O
investigate	O
loanword	O
use	O
,	O
we	O
are	O
finding	O
that	O
,	O
unlike	O
other	O
NZE	O
genres	O
analysed	O
,	O
the	O
Twitter	O
data	O
exhibits	O
use	O
of	O
Māori	O
which	O
is	O
more	O
in	O
line	O
with	O
code	O
-	O
switching	O
than	O
with	O
loanword	O
use	O
,	O
see	O
ex	O
.	O
(	O
2	O
-	O
3	O
)	O
.	O
This	O
is	O
particularly	O
interesting	O
in	O
light	O
of	O
the	O
reported	O
increase	O
in	O
te	O
reo	O
Māori	O
language	O
tweets	O
(	O
Keegan	O
et	O
al	O
,	O
2015	O
)	O
.	O
(	O
2	O
)	O
Mōrena	O
e	O
hoa	O
!	O
We	O
must	O
really	O
meet	O
IRL	O
when	O
I	O
get	O
back	O
to	O
Tāmaki	O
Makaurau	O
!	O
You	O
have	O
a	O
fab	O
day	O
too	O
!	O
(	O
3	O
)	O
Heh	O
!	O
He	O
porangi	O
toku	O
ngeru	O
-	O
especially	O
at	O
5	O
in	O
the	O
morning	O
!	O
!	O
Ata	O
marie	O
e	O
hoa	O
ma	O
.	O
I	O
am	O
well	O
thank	O
you	O
.	O
Secondly	O
,	O
we	O
also	O
report	O
the	O
use	O
of	O
hybrid	O
hashtags	O
,	O
that	O
is	O
,	O
hashtags	O
which	O
contain	O
a	O
Māori	O
part	O
and	O
an	O
English	O
part	O
,	O
for	O
example	O
#	O
mycrazywhanau	O
,	O
#	O
reostories	O
,	O
#	O
Matarikistar	O
,	O
#	O
bringitonmana	O
,	O
#	O
growingupkiwi	O
,	O
#	O
kaitoputinmyfridge	O
.	O
To	O
our	O
knowledge	O
,	O
these	O
hybrid	O
hashtags	O
have	O
never	O
been	O
analysed	O
in	O
the	O
current	O
literature	O
.	O
Hybrid	O
hashtags	O
parallel	O
the	O
phenomenon	O
of	O
hybrid	O
compounds	O
discussed	O
by	O
Degani	O
and	O
Onysko	O
(	O
2010	O
)	O
.	O
Degani	O
and	O
Onysko	O
report	O
that	O
hybrid	O
compounds	O
are	O
both	O
productive	O
and	O
semantically	O
novel	O
,	O
showing	O
that	O
the	O
borrowed	O
words	O
take	O
on	O
reconceptualised	O
meanings	O
in	O
their	O
adoptive	O
language	O
(	O
2010	O
,	O
p.231	O
)	O
.	O

Relevant	O
tweets	O
f	O
(	O
x	O
)	O
<	O
0.5	O
Classified	O
irrelevant	O
Haka	O
ne	O
!	O
And	O
i	O
know	O
even	O
the	O
good	O
guys	O
get	O
blood	O
for	O
body	O
(	O
0.282	O
,	O
foreign	O
language	O
)	O
son	O
did	O
nt	O
get	O
my	O
chop	O
ciggies	O
2day	O
so	O
stopped	O
talking	O
2	O
him	O
.	O
he	O
just	O
walked	O
past	O
and	O
gave	O
me	O
the	O
maori	O
eyebrow	O
lift	O
and	O
a	O
smile	O
.	O
were	O
friends	O
(	O
0.337	O
)	O
Whare	O
has	O
the	O
year	O
gone	O
(	O
0.36	O
,	O
misspelling	O
)	O
Shorts	O
and	O
bare	O
feet	O
in	O
this	O
whare	O
(	O
0.41	O
)	O
chegar	O
na	O
morena	O
e	O
falar	O
can	O
i	O
be	O
your	O
girlfriend	O
can	O
i	O
(	O
0.384	O
,	O
foreign	O
language	O
)	O

The	O
authors	O
would	O
like	O
to	O
thank	O
former	O
Honours	O
student	O
Nicole	O
Chan	O
for	O
a	O
preliminary	O
study	O
on	O
Māori	O
Loanwords	O
in	O
Twitter	O
.	O
Felipe	O
Bravo	O
-	O
Marquez	O
was	O
funded	O
by	O
Millennium	O
Institute	O
for	O
Foundational	O
Research	O
on	O
Data	O
.	O
Andreea	O
S.	O
Calude	O
acknowledges	O
the	O
support	O
of	O
the	O
NZ	O
Royal	O
Society	O
Marsden	O
Grant	O
.	O
David	O
Trye	O
acknowledges	O
the	O
generous	O
support	O
of	O
the	O
Computing	O
and	O
Mathematical	O
Sciences	O
group	O
at	O
the	O
University	O
of	O
Waikato	O
.	O

Neural	O
Maximum	O
Subgraph	O
Parsing	O
for	O
Cross	O
-	O
Domain	O
Semantic	O
Dependency	O
Analysis	O

Some	O
recent	O
work	O
on	O
parsing	O
targets	O
the	O
graphstructured	O
semantic	O
representations	O
that	O
are	O
more	O
general	O
than	O
the	O
tree	O
representation	O
.	O
Existing	O
approaches	O
can	O
be	O
categorized	O
into	O
two	O
dominant	O
types	O
:	O
the	O
transition	O
-	O
based	O
(	O
Zhang	O
et	O
al	O
,	O
2016	O
;	O
Wang	O
et	O
al	O
,	O
2018	O
)	O
and	O
graph	O
-	O
based	O
,	O
i.e.	O
,	O
Maximum	O
Subgraph	O
(	O
Kuhlmann	O
and	O
Jonsson	O
,	O
2015	O
;	O
Cao	O
et	O
al	O
,	O
2017a	O
)	O
,	O
approaches	O
.	O
Previous	O
investigations	O
on	O
transition	O
-	O
based	O
string	O
-	O
to	O
-	O
semantic	O
-	O
graph	O
parsing	O
adopt	O
many	O
ideas	O
from	O
syntactic	O
string	O
-	O
totree	O
parsing	O
,	O
such	O
as	O
how	O
to	O
handle	O
crossing	O
arcs	O
and	O
how	O
to	O
perform	O
neural	O
disambiguation	O
.	O
Zhang	O
et	O
al	O
(	O
2016	O
)	O
introduced	O
two	O
transition	O
systems	O
that	O
can	O
generate	O
arbitrary	O
graphs	O
and	O
augmented	O
them	O
into	O
practical	O
semantic	O
dependency	O
parsers	O
with	O
a	O
structured	O
perceptron	O
model	O
.	O
Wang	O
et	O
al	O
(	O
2018	O
)	O
evaluated	O
the	O
effectiveness	O
of	O
deep	O
learning	O
techniques	O
for	O
transition	O
-	O
based	O
SDP	O
.	O
Kuhlmann	O
and	O
Jonsson	O
(	O
2015	O
)	O
proposed	O
to	O
formulate	O
SDP	O
as	O
the	O
search	O
for	O
the	O
maximum	O
subgraphs	O
for	O
some	O
particular	O
graph	O
classes	O
.	O
This	O
proposal	O
is	O
called	O
Maximum	O
Subgraph	O
parsing	O
,	O
which	O
is	O
a	O
generalization	O
of	O
the	O
graph	O
-	O
based	O
parsing	O
framework	O
for	O
syntactic	O
parsing	O
.	O
For	O
arbitrary	O
graphs	O
,	O
Du	O
et	O
al	O
(	O
2015a	O
)	O
proved	O
that	O
the	O
secondorder	O
Maximum	O
Subgraph	O
problem	O
is	O
an	O
NPhard	O
problem	O
.	O
Nevertheless	O
,	O
Almeida	O
and	O
Martins	O
(	O
2015	O
)	O
and	O
Du	O
et	O
al	O
(	O
2015a	O
)	O
showed	O
that	O
dual	O
decomposition	O
is	O
a	O
practical	O
technique	O
to	O
solve	O
the	O
problem	O
.	O
Considering	O
more	O
restricted	O
graph	O
classes	O
,	O
Kuhlmann	O
and	O
Jonsson	O
(	O
2015	O
)	O
introduced	O
a	O
dynamic	O
programming	O
algorithem	O
for	O
parsing	O
to	O
noncrossing	O
graphs	O
.	O
Cao	O
et	O
al	O
(	O
2017a	O
;	O
2017b	O
)	O
showed	O
that	O
1EC	O
/	O
P2	O
graphs	O
are	O
more	O
suitable	O
for	O
describing	O
semantic	O
graphs	O
than	O
the	O
noncrossing	O
graphs	O
,	O
and	O
they	O
also	O
allow	O
low	O
-	O
degree	O
dynamic	O
programming	O
algorithms	O
for	O
decoding	O
.	O

Previous	O
work	O
showed	O
that	O
the	O
Maximum	O
Subgraph	O
framework	O
is	O
not	O
only	O
elegant	O
in	O
theory	O
but	O
also	O
effective	O
in	O
practice	O
(	O
Kuhlmann	O
and	O
Jonsson	O
,	O
2015	O
;	O
Cao	O
et	O
al	O
,	O
2017a	O
,	O
b	O
)	O
.	O
In	O
particular	O
,	O
1EC	O
/	O
P2	O
graphs	O
are	O
an	O
appropriate	O
graph	O
class	O
for	O
modeling	O
semantic	O
dependency	O
structures	O
(	O
Cao	O
et	O
al	O
,	O
2017a	O
)	O
.	O
Figure	O
2	O
presents	O
an	O
example	O
to	O
illustrate	O
the	O
1	O
-	O
endpoint	O
-	O
crossing	O
property	O
,	O
while	O
Figure	O
3	O
shows	O
a	O
case	O
for	O
pagenumber	O
-	O
2	O
.	O
Below	O
we	O
present	O
the	O
formal	O
description	O
of	O
the	O
two	O
properties	O
that	O
are	O
adopted	O
from	O
Pitler	O
et	O
al	O
(	O
2013	O
)	O
and	O
Kuhlmann	O
and	O
Jonsson	O
(	O
2015	O
)	O
respectively	O
.	O
Definition	O
1	O
A	O
dependency	O
graph	O
is	O
1	O
-	O
Endpoint	O
-	O
Crossing	O
if	O
for	O
any	O
edge	O
e	O
,	O
all	O
edges	O
that	O
cross	O
e	O
share	O
an	O
endpoint	O
p	O
named	O
pencil	O
point	O
.	O
Definition	O
2	O
A	O
pagenumber	O
-	O
k	O
graph	O
means	O
it	O
consists	O
at	O
most	O
k	O
half	O
-	O
planes	O
,	O
and	O
arcs	O
on	O
each	O
half	O
-	O
plane	O
are	O
noncrossing	O
.	O
If	O
G	O
is	O
the	O
set	O
of	O
1	O
-	O
endpoint	O
-	O
crossing	O
graphs	O
or	O
more	O
restricted	O
1EC	O
/	O
P2	O
graphs	O
,	O
the	O
optimization	O
problem	O
(	O
1	O
)	O
in	O
the	O
first	O
-	O
order	O
case	O
can	O
be	O
solved	O
in	O
quintic	O
-	O
time	O
(	O
Cao	O
et	O
al	O
,	O
2017a	O
)	O
by	O
using	O
dynamic	O
programming	O
.	O
Furthermore	O
,	O
ignoring	O
one	O
linguistically	O
-	O
rare	O
structure	O
in	O
1EC	O
/	O
P2	O
graphs	O
descreases	O
the	O
complexity	O
to	O
O	O
(	O
n	O
4	O
)	O
(	O
Cao	O
et	O
al	O
,	O
2017a	O
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
implement	O
Cao	O
et	O
al	O
Cao	O
et	O
al	O
(	O
2017a	O
)	O
's	O
algorithm	O
as	O
the	O
basis	O
of	O
our	O
parser	O
.	O

A	O
semantic	O
graph	O
mainly	O
consists	O
of	O
two	O
parts	O
:	O
the	O
structural	O
part	O
and	O
the	O
label	O
part	O
.	O
The	O
former	O
describes	O
the	O
predicate	O
-	O
argument	O
relation	O
in	O
the	O
sentence	O
,	O
and	O
the	O
latter	O
describes	O
the	O
type	O
of	O
this	O
relation	O
.	O
In	O
our	O
model	O
,	O
the	O
structural	O
part	O
and	O
the	O
label	O
part	O
are	O
regarded	O
as	O
independent	O
of	O
each	O
other	O
.	O
We	O
use	O
a	O
coarse	O
-	O
to	O
-	O
fine	O
strategy	O
:	O
finding	O
the	O
maximum	O
unlabeled	O
subgraph	O
first	O
and	O
assigning	O
a	O
label	O
for	O
every	O
edge	O
in	O
this	O
subgraph	O
then	O
.	O
The	O
motivation	O
is	O
to	O
avoid	O
the	O
calculation	O
of	O
a	O
number	O
of	O
unnecessary	O
label	O
scores	O
in	O
order	O
to	O
improve	O
the	O
processing	O
efficiency	O
.	O
candidate	O
dependencies	O
as	O
well	O
as	O
their	O
relation	O
types	O
.	O
Figure	O
4	O
shows	O
the	O
architecture	O
of	O
our	O
system	O
.	O

This	O
work	O
was	O
supported	O
by	O
the	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
(	O
61772036	O
,	O
61331011	O
)	O
and	O
the	O
Key	O
Laboratory	O
of	O
Science	O
,	O
Technology	O
and	O
Standard	O
in	O
Press	O
Industry	O
(	O
Key	O
Laboratory	O
of	O
Intelligent	O
Press	O
Media	O
Technology	O
)	O
.	O
We	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
helpful	O
comments	O
.	O
Weiwei	O
Sun	O
is	O
the	O
corresponding	O
author	O
.	O

Our	O
OpenRE	O
framework	O
mainly	O
consists	O
of	O
two	O
modules	O
,	O
the	O
relation	O
similarity	O
calculation	O
module	O
and	O
the	O
relation	O
clustering	O
module	O
.	O
For	O
relation	O
similarity	O
calculation	O
,	O
we	O
propose	O
Relational	O
Siamese	O
Networks	O
(	O
RSNs	O
)	O
,	O
which	O
learn	O
to	O
predict	O
whether	O
two	O
sentences	O
mention	O
the	O
same	O
relation	O
.	O
To	O
utilize	O
large	O
-	O
scale	O
unsupervised	O
data	O
and	O
distantly	O
-	O
supervised	O
data	O
,	O
we	O
further	O
propose	O
Semi	O
-	O
supervised	O
RSN	O
and	O
Distantly	O
-	O
supervised	O
RSN	O
.	O
Finally	O
,	O
in	O
the	O
relation	O
clustering	O
module	O
,	O
with	O
the	O
learned	O
relation	O
metric	O
,	O
we	O
utilize	O
hierarchical	O
agglomerative	O
clustering	O
(	O
HAC	O
)	O
and	O
Louvain	O
clustering	O
algorithms	O
to	O
cluster	O
target	O
relation	O
instances	O
of	O
new	O
relation	O
types	O
.	O

In	O
this	O
section	O
,	O
we	O
conduct	O
several	O
experiments	O
on	O
real	O
-	O
world	O
RE	O
datasets	O
to	O
show	O
the	O
effectiveness	O
of	O
our	O
models	O
,	O
and	O
give	O
a	O
detailed	O
analysis	O
to	O
show	O
its	O
advantages	O
.	O

To	O
intuitively	O
evaluate	O
the	O
knowledge	O
transfer	O
effects	O
of	O
RSN	O
and	O
Semi	O
-	O
supervised	O
RSN	O
,	O
we	O
visualize	O
their	O
relational	O
knowledge	O
representation	O
spaces	O
in	O
the	O
last	O
layer	O
of	O
CNN	O
encoders	O
with	O
t	O
-	O
SNE	O
(	O
Maaten	O
and	O
Hinton	O
,	O
2008	O
)	O
in	O
Figure	O
4	O
.	O
We	O
also	O
compare	O
with	O
a	O
supervised	O
CNN	O
trained	O
on	O
9	O
,	O
600	O
labeled	O
instances	O
of	O
novel	O
relations	O
,	O
which	O
suggests	O
the	O
optimal	O
relational	O
knowledge	O
representation	O
.	O
In	O
each	O
figure	O
,	O
we	O
plot	O
402	O
relation	O
instances	O
of	O
4	O
randomly	O
-	O
chosen	O
relation	O
types	O
in	O
the	O
test	O
set	O
,	O
and	O
points	O
are	O
colored	O
according	O
to	O
their	O
ground	O
-	O
truth	O
labels	O
.	O
As	O
we	O
can	O
see	O
from	O
Figure	O
4	O
,	O
RSN	O
is	O
able	O
to	O
roughly	O
distinguish	O
different	O
relations	O
,	O
and	O
Semi	O
-	O
supervised	O
RSN	O
further	O
facilitated	O
knowledge	O
transfer	O
by	O
optimizing	O
the	O
margin	O
between	O
potential	O
relation	O
clusters	O
during	O
training	O
.	O
As	O
a	O
result	O
,	O
Semi	O
-	O
supervised	O
RSN	O
can	O
extract	O
more	O
distinguishable	O
novel	O
relations	O
,	O
and	O
gains	O
comparable	O
relational	O
knowledge	O
representation	O
ability	O
with	O
supervised	O
CNN	O
.	O

KALA	O
:	O
Knowledge	O
-	O
Augmented	O
Language	O
Model	O
Adaptation	O

The	O
performance	O
gap	O
between	O
KALA	O
(	O
relational	O
)	O
and	O
KALA	O
(	O
point	O
-	O
wise	O
)	O
shows	O
the	O
effectiveness	O
of	O
relational	O
retrieval	O
for	O
language	O
model	O
adaptation	O
,	O
which	O
allows	O
us	O
to	O
incorporate	O
relational	O
knowledge	O
into	O
the	O
PLM	O
.	O
The	O
relational	O
retrieval	O
also	O
helps	O
address	O
unseen	O
entities	O
,	O
as	O
discussed	O
in	O
Section	O
5.4	O
.	O

We	O
perform	O
an	O
ablation	O
study	O
to	O
see	O
how	O
much	O
each	O
component	O
contributes	O
to	O
the	O
performance	O
gain	O
.	O

One	O
remarkable	O
advantage	O
of	O
our	O
KALA	O
is	O
its	O
ability	O
to	O
represent	O
an	O
unseen	O
entity	O
by	O
aggregating	O
features	O
of	O
its	O
neighbors	O
from	O
a	O
given	O
KG	O
.	O
To	O
analyze	O
this	O
,	O
we	O
first	O
divide	O
all	O
contexts	O
into	O
one	O
of	O
Seen	O
and	O
Unseen	O
,	O
where	O
Seen	O
denotes	O
the	O
context	O
with	O
less	O
than	O
3	O
unseen	O
entities	O
,	O
and	O
then	O
measure	O
the	O
performance	O
on	O
the	O
two	O
subsets	O
.	O
As	O
shown	O
in	O
Figure	O
4	O
,	O
we	O
observe	O
that	O
the	O
performance	O
gain	O
of	O
KALA	O
over	O
the	O
baselines	O
is	O
much	O
larger	O
on	O
the	O
Unseen	O
subset	O
,	O
which	O
demonstrates	O
the	O
effectiveness	O
of	O
KALA	O
's	O
relational	O
retrieval	O
scheme	O
to	O
represent	O
unseen	O
entities	O
.	O
DAPT	O
also	O
largely	O
outperforms	O
fine	O
-	O
tuning	O
and	O
TAPT	O
as	O
it	O
is	O
trained	O
on	O
an	O
extremely	O
large	O
external	O
corpus	O
for	O
adaptive	O
pre	O
-	O
training	O
.	O
However	O
,	O
KALA	O
even	O
outperforms	O
DAPT	O
in	O
most	O
cases	O
,	O
verifying	O
that	O
our	O
knowledgeaugmentation	O
method	O
is	O
more	O
effective	O
for	O
tackling	O
domain	O
-	O
specific	O
tasks	O
.	O
The	O
visualization	O
of	O
embeddings	O
of	O
seen	O
and	O
unseen	O
entities	O
in	O
Figure	O
2	O
shows	O
that	O
KALA	O
embeds	O
the	O
unseen	O
entities	O
more	O
closely	O
to	O
the	O
seen	O
entities	O
4	O
,	O
which	O
explains	O
KALA	O
's	O
good	O
performance	O
on	O
the	O
Unseen	O
subset	O
.	O

To	O
better	O
see	O
how	O
our	O
KFM	O
(	O
3.2	O
)	O
works	O
,	O
we	O
show	O
the	O
context	O
and	O
its	O
fact	O
,	O
and	O
then	O
visualize	O
representations	O
from	O
the	O
PLM	O
modulated	O
by	O
the	O
KFM	O
.	O
As	O
shown	O
in	O
Figure	O
5	O
right	O
,	O
the	O
token	O
'	O
#	O
#	O
on	O
'	O
is	O
not	O
aligned	O
with	O
their	O
corresponding	O
tokens	O
,	O
such	O
as	O
'	O
ex	O
'	O
(	O
for	O
exon	O
)	O
and	O
'	O
cod	O
'	O
(	O
for	O
codon	O
)	O
,	O
in	O
the	O
baseline	O
.	O
However	O
,	O
with	O
our	O
feature	O
modulation	O
that	O
transforms	O
multiple	O
tokens	O
associated	O
with	O
the	O
single	O
entity	O
equally	O
,	O
the	O
two	O
tokens	O
(	O
e.g.	O
,	O
(	O
'	O
ex	O
'	O
,	O
'	O
#	O
#	O
on	O
'	O
)	O
)	O
,	O
composing	O
one	O
entity	O
,	O
are	O
closely	O
embedded	O
.	O
Also	O
,	O
while	O
the	O
baseline	O
can	O
not	O
handle	O
the	O
unseen	O
entity	O
consisting	O
of	O
three	O
tokens	O
:	O
're	O
'	O
,	O
'	O
#	O
#	O
tina	O
'	O
,	O
and	O
'	O
#	O
#	O
l	O
'	O
,	O
KALA	O
embeds	O
them	O
closely	O
by	O
representing	O
the	O
unseen	O
retinal	O
from	O
the	O
representation	O
of	O
its	O
neighborhood	O
gene	O
derived	O
by	O
the	O
domain	O
knowledge	O
-	O
(	O
retinal	O
,	O
instance	O
of	O
,	O
gene	O
)	O
.	O

Our	O
KALA	O
framework	O
is	O
also	O
applicable	O
to	O
encoder	O
-	O
decoder	O
PLMs	O
by	O
applying	O
the	O
KFM	O
to	O
the	O
encoder	O
.	O
Therefore	O
,	O
we	O
further	O
validate	O
KALA	O
's	O
effectiveness	O
on	O
the	O
encoder	O
-	O
decoder	O
PLMs	O
on	O
the	O
generative	O
QA	O
task	O
(	O
Lee	O
et	O
al	O
,	O
2021	O
)	O
with	O
T5small	O
(	O
Raffel	O
et	O
al	O
,	O
2020	O
)	O
.	O
Table	O
7	O
shows	O
that	O
KALA	O
largely	O
outperforms	O
baselines	O
even	O
with	O
such	O
a	O
generative	O
PLM	O
.	O

In	O
this	O
section	O
,	O
we	O
introduce	O
the	O
detailed	O
setups	O
for	O
our	O
models	O
and	O
baselines	O
used	O
in	O
Table	O
1	O
,	O
2	O
,	O
and	O
4	O
.	O

In	O
this	O
section	O
,	O
we	O
provide	O
the	O
analyses	O
on	O
the	O
forgetting	O
of	O
TAPT	O
,	O
entity	O
memory	O
,	O
number	O
of	O
entities	O
and	O
facts	O
,	O
location	O
of	O
the	O
KLM	O
layer	O
,	O
and	O
values	O
of	O
Gamma	O
and	O
Beta	O
.	O

In	O
Figure	O
1	O
,	O
we	O
observe	O
that	O
the	O
performance	O
of	O
TAPT	O
decreases	O
as	O
the	O
number	O
of	O
training	O
steps	O
increases	O
.	O
To	O
get	O
a	O
concrete	O
intuition	O
on	O
this	O
particular	O
phenomenon	O
,	O
we	O
analysis	O
what	O
happens	O
in	O
the	O
Pre	O
-	O
trained	O
Language	O
Model	O
(	O
PLM	O
)	O
,	O
when	O
we	O
further	O
pre	O
-	O
train	O
it	O
on	O
the	O
task	O
-	O
specific	O
corpus	O
.	O
Specifically	O
,	O
in	O
Figure	O
7	O

Here	O
we	O
provide	O
the	O
frequency	O
distribution	O
of	O
entities	O
,	O
additional	O
case	O
studies	O
,	O
and	O
more	O
illustrations	O
of	O
textual	O
examples	O
and	O
embedding	O
spaces	O
.	O

In	O
Figure	O
16	O
and	O
17	O
,	O
we	O
visualize	O
the	O
examples	O
of	O
the	O
context	O
with	O
its	O
seen	O
and	O
unseen	O
entities	O
and	O
its	O
relational	O
facts	O
.	O
We	O
first	O
confirm	O
that	O
the	O
quality	O
of	O
facts	O
is	O
moderate	O
to	O
use	O
.	O
For	O
instance	O
,	O
in	O
the	O
first	O
example	O
of	O
Figure	O
16	O
,	O
the	O
fact	O
in	O
the	O
context	O
that	O
Omar_bin_Laden	O
is	O
son	O
of	O
Osama_bin_Laden	O
,	O
is	O
also	O
appeared	O
in	O
the	O
knowledge	O
graph	O
.	O
In	O
addition	O
,	O
we	O
observe	O
that	O
there	O
are	O
facts	O
that	O
link	O
unseen	O
entities	O
to	O
the	O
seen	O
entities	O
in	O
both	O
Figure	O
16	O
and	O
17	O
.	O
Thus	O
,	O
while	O
some	O
of	O
the	O
facts	O
in	O
the	O
knowledge	O
graph	O
are	O
not	O
accurate	O
,	O
we	O
can	O
represent	O
the	O
unseen	O
entities	O
with	O
their	O
relation	O
to	O
the	O
seen	O
entities	O
.	O
We	O
expect	O
that	O
there	O
is	O
a	O
still	O
room	O
to	O
improve	O
in	O
terms	O
of	O
the	O
quality	O
of	O
KGs	O
,	O
allowing	O
our	O
KALA	O
to	O
modulate	O
the	O
entity	O
representation	O
more	O
accurately	O
.	O
We	O
leave	O
the	O
study	O
on	O
this	O
as	O
the	O
future	O
work	O
.	O

The	O
adenomatous	O
polyposis	O
coli	O
(	O
APC	O
)	O
tumour	O
-	O
suppressor	O
protein	O
controls	O
the	O
Wnt	O
signalling	O
pathway	O
by	O
forming	O
a	O
complex	O
with	O
glycogen	O
synthase	O
kinase	O
3beta	O
(	O
GSK	O
-	O
3beta	O
)	O
,	O
axin	O
/	O
conductin	O
and	O
betacatenin	O
.	O

(	O
complex	O
,	O
subclass	O
of	O
,	O
protein	O
)	O
(	O
GSK	O
,	O
instance	O
of	O
,	O
protein	O
)	O
(	O
glycogen	O
,	O
instance	O
of	O
,	O
protein	O
)	O
(	O
APC	O
,	O
instance	O
of	O
,	O
protein	O
)	O
Context	O
HLA	O
typing	O
for	O
HLA	O
-	O
B27	O
,	O
HLA	O
-	O
B60	O
,	O
and	O
HLA	O
-	O
DR1	O
was	O
performed	O
by	O
polymerase	O
chain	O
reaction	O
with	O
sequence	O
-	O
specific	O
primers	O
,	O
and	O
zygosity	O
was	O
assessed	O
using	O
microsatellite	O
markers	O
.	O

(	O
microsatellite	O
,	O
subclass	O
of	O
,	O
primers	O
)	O
(	O
DR1	O
,	O
instance	O
of	O
,	O
microsatellite	O
)	O
(	O
microsatellite	O
,	O
subclass	O
of	O
,	O
typing	O
)	O

We	O
identified	O
four	O
germline	O
mutations	O
in	O
three	O
breast	O
cancer	O
families	O
and	O
in	O
one	O
breast	O
-	O
ovarian	O
cancer	O
family	O
.	O
among	O
these	O
were	O
one	O
frameshift	O
mutation	O
,	O
one	O
nonsense	O
mutation	O
,	O
one	O
novel	O
splice	O
site	O
mutation	O
,	O
and	O
one	O
missense	O
mutation	O
.	O

We	O
count	O
the	O
number	O
of	O
each	O
type	O
of	O
tag	O
in	O
the	O
training	O
set	O
and	O
the	O
validation	O
set	O
,	O
and	O
obtain	O
the	O
data	O
distribution	O
of	O
Not	O
-	O
offensive	O
,	O
offensive	O
-	O
untargeted	O
,	O
offensive	O
-	O
targeted	O
-	O
individual	O
,	O
offensive	O
-	O
targeted	O
-	O
group	O
,	O
offensive	O
-	O
targeted	O
-	O
other	O
,	O
and	O
Not	O
-	O
in	O
-	O
indented	O
-	O
language	O
in	O
Tamil	O
,	O
Malayalam	O
,	O
and	O
Kannada	O
.	O
as	O
shown	O
in	O
Table	O
1	O
.	O

Right	O
for	O
the	O
Right	O
Reason	O
:	O
Evidence	O
Extraction	O
for	O
Trustworthy	O
Tabular	O
Reasoning	O

Trustworthy	O
inference	O
has	O
an	O
intrinsic	O
sequential	O
causal	O
structure	O
:	O
extract	O
evidence	O
first	O
,	O
then	O
predict	O
the	O
inference	O
label	O
using	O
the	O
extracted	O
evidence	O
data	O
,	O
knowledge	O
/	O
common	O
sense	O
,	O
and	O
perhaps	O
formal	O
reasoning	O
(	O
Herzig	O
et	O
al	O
,	O
2021	O
;	O
Paranjape	O
et	O
al	O
,	O
2020	O
)	O
7	O
.	O
To	O
operationalize	O
this	O
intuition	O
,	O
we	O
chose	O
a	O
two	O
-	O
stage	O
sequential	O
approach	O
which	O
consists	O
of	O
an	O
evidence	O
extraction	O
followed	O
by	O
the	O
NLI	O
classi	O
-	O
fication	O
,	O
as	O
shown	O
in	O
Figure	O
2	O
.	O
Notation	O
.	O
The	O
function	O
f	O
in	O
Eq	O
.	O
2	O
can	O
be	O
rewritten	O
with	O
functions	O
g	O
and	O
h	O
,	O
f	O
(	O
.	O
)	O
=	O
g	O
(	O
.	O
)	O
,	O
h	O
g	O
(	O
.	O
)	O
,	O
as	O
f	O
(	O
T	O
,	O
H	O
)	O
=	O
{	O
g	O
(	O
T	O
,	O
H	O
)	O
,	O
h	O
(	O
g	O
(	O
T	O
,	O
H	O
)	O
,	O
H	O
)	O
}	O
(	O
3	O
)	O
Here	O
,	O
g	O
extracts	O
the	O
evidence	O
rows	O
T	O
R	O
subset	O
of	O
T	O
,	O
and	O
h	O
uses	O
the	O
extracted	O
evidence	O
T	O
R	O
and	O
the	O
hypothesis	O
H	O
to	O
predict	O
the	O
inference	O
label	O
y	O
,	O
as	O
g	O
(	O
T	O
,	O
H	O
)	O
T	O
R	O
h	O
(	O
T	O
R	O
,	O
H	O
)	O
y	O
(	O
4	O
)	O
To	O
obtain	O
f	O
,	O
we	O
need	O
to	O
define	O
the	O
functions	O
g	O
and	O
h	O
,	O
and	O
a	O
flexible	O
representation	O
of	O
a	O
semi	O
-	O
structured	O
.	O
We	O
explore	O
unsupervised	O
(	O
$	O
4.1	O
)	O
and	O
supervised	O
(	O
$	O
4.2	O
)	O
methods	O
for	O
the	O
evidence	O
row	O
extractor	O
g.	O

The	O
unsupervised	O
approaches	O
extract	O
Top	O
-	O
K	O
rows	O
are	O
based	O
on	O
relevance	O
scores	O
,	O
where	O
K	O
is	O
a	O
hyperparameter	O
.	O
We	O
use	O
the	O
cosine	O
similarity	O
between	O
the	O
row	O
and	O
the	O
hypothesis	O
sentence	O
representations	O
to	O
score	O
rows	O
.	O
We	O
study	O
three	O
ways	O
to	O
define	O
relevance	O
described	O
next	O
.	O

For	O
most	O
instances	O
,	O
the	O
number	O
of	O
relevant	O
rows	O
(	O
K	O
)	O
is	O
much	O
lower	O
than	O
the	O
total	O
number	O
of	O
rows	O
(	O
m	O
)	O
;	O
most	O
examples	O
have	O
only	O
one	O
or	O
two	O
relevant	O
rows	O
.	O
We	O
constrained	O
the	O
sparsity	O
in	O
the	O
extraction	O
by	O
capping	O
the	O
value	O
of	O
K	O
to	O
S	O
m.	O

We	O
use	O
a	O
threshold	O
τ	O
to	O
select	O
rows	O
dynamically	O
Top	O
-	O
K	O
τ	O
based	O
on	O
the	O
hypothesis	O
,	O
rather	O
than	O
always	O
selecting	O
fixed	O
K	O
rows	O
.	O
We	O
only	O
select	O
rows	O
whose	O
similarity	O
(	O
after	O
Re	O
-	O
Ranking	O
)	O
to	O
the	O
hypothesis	O
sentence	O
representations	O
is	O
greater	O
than	O
a	O
threshold	O
τ	O
.	O
We	O
adopt	O
this	O
strategy	O
because	O
(	O
a	O
)	O
the	O
number	O
of	O
rows	O
in	O
the	O
premise	O
table	O
can	O
vary	O
across	O
examples	O
,	O
and	O
(	O
b	O
)	O
different	O
hypotheses	O
may	O
require	O
a	O
differing	O
number	O
of	O
evidence	O
rows	O
.	O

Our	O
experiments	O
assess	O
the	O
efficacy	O
of	O
evidence	O
extraction	O
(	O
$	O
4	O
)	O
and	O
its	O
impact	O
on	O
the	O
downstream	O
NLI	O
task	O
by	O
studying	O
the	O
following	O
questions	O
:	O
RQ1	O
:	O
What	O
is	O
the	O
efficacy	O
of	O
unsupervised	O
approaches	O
for	O
evidence	O
extraction	O
?	O
(	O
$	O
5.2	O
)	O

In	O
this	O
paper	O
,	O
we	O
introduced	O
the	O
problem	O
of	O
Trustworthy	O
Tabular	O
Inference	O
,	O
where	O
a	O
reasoning	O
model	O
both	O
extracts	O
evidence	O
from	O
a	O
table	O
and	O
predicts	O
an	O
inference	O
label	O
.	O
We	O
studied	O
a	O
two	O
-	O
stage	O
approach	O
,	O
comprising	O
an	O
evidence	O
extraction	O
and	O
an	O
inference	O
stage	O
.	O
We	O
explored	O
several	O
unsupervised	O
and	O
supervised	O
strategies	O
for	O
evidence	O
extraction	O
,	O
several	O
of	O
which	O
outperformed	O
prior	O
benchmarks	O
.	O
Finally	O
,	O
we	O
showed	O
that	O
by	O
using	O
only	O
extracted	O
evidence	O
as	O
the	O
premise	O
,	O
our	O
approach	O
outperforms	O
previous	O
baselines	O
on	O
the	O
downstream	O
tabular	O
inference	O
task	O
.	O

Since	O
many	O
hypothesis	O
sentences	O
(	O
especially	O
those	O
with	O
neutral	O
labels	O
)	O
require	O
out	O
-	O
of	O
-	O
table	O
information	O
for	O
inference	O
,	O
we	O
introduced	O
the	O
option	O
to	O
choose	O
out	O
-	O
of	O
-	O
table	O
(	O
OOT	O
)	O
pseudo	O
rows	O
,	O
which	O
are	O
highlighted	O
only	O
when	O
the	O
hypothesis	O
requires	O
information	O
that	O
is	O
not	O
common	O
(	O
i.e.	O
common	O
sense	O
)	O
and	O
missing	O
from	O
the	O
table	O
.	O
To	O
reduce	O
any	O
possible	O
bias	O
due	O
to	O
unintended	O
associations	O
between	O
the	O
NLI	O
label	O
and	O
the	O
row	O
selections	O
(	O
e.g.	O
,	O
using	O
OOT	O
for	O
neutral	O
examples	O
)	O
,	O
we	O
avoid	O
showing	O
inference	O
labels	O
to	O
the	O
annotators	O
15	O
.	O
To	O
assess	O
an	O
annotator	O
,	O
we	O
compare	O
their	O
annotations	O
with	O
the	O
majority	O
consensus	O
of	O
other	O
annotators	O
'	O
(	O
four	O
)	O
annotations	O
.	O
We	O
perform	O
this	O
comparison	O
at	O
two	O
levels	O
:	O
(	O
a	O
)	O
local	O
-	O
consensus	O
-	O
score	O
on	O
the	O
most	O
recent	O
batch	O
,	O
and	O
(	O
b	O
)	O
cumulative	O
-	O
consensusscore	O
on	O
all	O
batches	O
annotated	O
thus	O
far	O
.	O
We	O
use	O
these	O
consensus	O
scores	O
to	O
temporarily	O
(	O
local	O
-	O
consensus	O
-	O
score	O
)	O
or	O
permanently	O
(	O
cumulative	O
score	O
)	O
block	O
the	O
poor	O
annotators	O
from	O
the	O
task	O
.	O
We	O
also	O
review	O
the	O
annotations	O
manually	O
and	O
provide	O
feedback	O
with	O
more	O
detailed	O
instructions	O
and	O
personalized	O
examples	O
for	O
annotators	O
who	O
were	O
making	O
mistakes	O
due	O
to	O
ambiguity	O
in	O
the	O
task	O
.	O
We	O
give	O
incentives	O
to	O
annotators	O
who	O
received	O
high	O
consensus	O
scores	O
.	O
As	O
in	O
previous	O
work	O
,	O
we	O
removed	O
certain	O
annotators	O
'	O
annotations	O
that	O
have	O
a	O
poor	O
consensus	O
score	O
(	O
cumulative	O
score	O
)	O
and	O
published	O
a	O
second	O
validation	O
HIT	O
to	O
double	O
-	O
check	O
each	O
data	O
point	O
if	O
necessary	O
.	O

We	O
manually	O
inspect	O
the	O
Type	O
I	O
and	O
Type	O
II	O
error	O
instances	O
for	O
the	O
supervised	O
model	O
and	O
human	O
annotation	O
for	O
the	O
development	O
set	O
.	O
Below	O
,	O
we	O
show	O
some	O
of	O
these	O
examples	O
where	O
models	O
conflict	O
with	O
ground	O
-	O
truth	O
human	O
annotation	O
.	O
We	O
also	O
provide	O
a	O
possible	O
reason	O
behind	O
the	O
model	O
mistakes	O
.	O
Example	O
III	O
Row	O
:	O
The	O
trainer	O
of	O
Caveat	O
is	O
Woody	O
Stephens	O
.	O
Hypothesis	O
:	O
Caveat	O
won	O
more	O
in	O
winnings	O
than	O
it	O
took	O
to	O
raise	O
and	O
train	O
him	O
.	O
Model	O
Prediction	O
:	O
Relevant	O
Evidence	O
Human	O
Ground	O
Truth	O
:	O
Not	O
Relevant	O
.	O
Possible	O
Reason	O
:	O
Model	O
connects	O
the	O
'	O
raise	O
and	O
train	O
'	O
term	O
with	O
the	O
trainer	O
name	O
which	O
is	O
unrelated	O
and	O
has	O
no	O
connection	O
to	O
overall	O
,	O
winning	O
races	O
money	O
vs	O
spending	O
for	O
animal	O
.	O
Discussion	O
Based	O
on	O
the	O
observation	O
from	O
the	O
above	O
examples	O
as	O
also	O
stated	O
in	O
$	O
5.3	O
,	O
the	O
model	O
fails	O
on	O
many	O
examples	O
due	O
to	O
its	O
lack	O
of	O
knowledge	O
and	O
common	O
-	O
sense	O
reasoning	O
ability	O
.	O
One	O
possible	O
solution	O
to	O
mitigate	O
this	O
is	O
by	O
the	O
addition	O
of	O
implicit	O
and	O
explicit	O
knowledge	O
on	O
-	O
the	O
-	O
fly	O
for	O
evidence	O
extraction	O
,	O
as	O
done	O
for	O
inference	O
task	O
by	O
Neeraja	O
et	O
al	O
(	O
2021	O
)	O
.	O

We	O
manually	O
examine	O
the	O
human	O
-	O
annotated	O
evidence	O
in	O
the	O
development	O
set	O
.	O
We	O
discovered	O
the	O
existence	O
of	O
several	O
relevant	O
phrases	O
/	O
tokens	O
which	O
implicitly	O
indicate	O
the	O
presence	O
of	O
evidence	O
rows	O
.	O
E.g.	O
The	O
existence	O
of	O
tokens	O
such	O
as	O
married	O
,	O
husband	O
,	O
lesbian	O
,	O
and	O
wife	O
in	O
hypothesis	O
(	O
H	O
)	O
is	O
very	O
suggestive	O
of	O
the	O
row	O
Spouse	O
being	O
the	O
relevant	O
evidence	O
.	O
Learning	O
such	O
implicit	O
relevance	O
-	O
based	O
phrases	O
and	O
tokens	O
connection	O
is	O
easy	O
for	O
humans	O
and	O
large	O
pre	O
-	O
trained	O
supervision	O
models	O
.	O
It	O
is	O
a	O
challenging	O
task	O
for	O
similarity	O
-	O
based	O
unsupervised	O
extraction	O
methods	O
.	O
Below	O
,	O
we	O
show	O
implicit	O
relevance	O
,	O
indicating	O
token	O
and	O
the	O
corresponding	O
relevant	O
evidence	O
rows	O
.	O
Relevance	O
Indicating	O
Phrase	O
(	O
H	O
)	O
Relevant	O
Evidence	O
Rows	O
Key	O
(	O
T	O
)	O

The	O
authors	O
thank	O
Bloomberg	O
's	O
AI	O
Engineering	O
team	O
,	O
especially	O
Ketevan	O
Tsereteli	O
,	O
Anju	O
Kambadur	O
,	O
and	O
Amanda	O
Stent	O
for	O
helpful	O
feedback	O
and	O
directions	O
.	O
We	O
also	O
appreciate	O
the	O
useful	O
feedback	O
provided	O
by	O
Ellen	O
Riloff	O
and	O
the	O
Utah	O
NLP	O
group	O
.	O
Additionally	O
,	O
we	O
appreciate	O
the	O
helpful	O
inputs	O
provided	O
by	O
Atreya	O
Ghosal	O
,	O
Riyaz	O
A.	O
Bhat	O
,	O
Manish	O
Srivastava	O
,	O
and	O
Maneesh	O
Singh	O
.	O
Vivek	O
Gupta	O
acknowledges	O
support	O
from	O
Bloomberg	O
's	O
Data	O
Science	O
Ph.D.	O
Fellowship	O
.	O
This	O
work	O
was	O
supported	O
in	O
part	O
by	O
National	O
Science	O
Foundation	O
grants	O
#	O
1801446	O
(	O
SaTC	O
)	O
and	O
#	O
1822877	O
(	O
Cyberlearning	O
)	O
.	O
Finally	O
,	O
we	O
would	O
like	O
to	O
express	O
our	O
gratitude	O
to	O
the	O
reviewing	O
team	O
for	O
their	O
insightful	O
comments	O
.	O

X575	O
:	O
writing	O
rengas	O
with	O
web	O
services	O

Our	O
software	O
system	O
simulates	O
the	O
classical	O
collaborative	O
Japanese	O
poetry	O
form	O
,	O
renga	O
,	O
made	O
of	O
linked	O
haikus	O
.	O
We	O
used	O
NLP	O
methods	O
wrapped	O
up	O
as	O
web	O
services	O
.	O
This	O
approach	O
is	O
suitable	O
for	O
collaborative	O
human	O
-	O
AI	O
generation	O
,	O
as	O
well	O
as	O
purely	O
computer	O
-	O
generated	O
poetry	O
.	O
Evaluation	O
included	O
a	O
blind	O
survey	O
comparing	O
AI	O
and	O
human	O
haiku	O
.	O
To	O
gather	O
ideas	O
for	O
future	O
work	O
,	O
we	O
examine	O
related	O
research	O
in	O
semiotics	O
,	O
linguistics	O
,	O
and	O
computing	O
.	O

Computer	O
haikus	O
have	O
been	O
explored	O
in	O
practice	O
at	O
least	O
since	O
Lutz	O
(	O
1959	O
)	O
.	O
More	O
recently	O
,	O
haikus	O
have	O
been	O
used	O
by	O
Ventura	O
(	O
2016	O
)	O
as	O
the	O
testbed	O
for	O
a	O
thought	O
experiment	O
on	O
levels	O
of	O
computational	O
creativity	O
.	O
As	O
we	O
will	O
discuss	O
below	O
,	O
the	O
classic	O
haiku	O
traditionally	O
formed	O
the	O
starting	O
verse	O
of	O
a	O
longer	O
poetry	O
jam	O
,	O
resulting	O
in	O
a	O
poem	O
called	O
a	O
renga	O
.	O
A	O
computational	O
exploration	O
of	O
renga	O
writing	O
allows	O
us	O
to	O
return	O
to	O
some	O
of	O
the	O
classical	O
ideas	O
in	O
Japanese	O
poetry	O
via	O
thoroughly	O
modern	O
ideas	O
like	O
concept	O
blending	O
and	O
collaborative	O
AI	O
.	O
Ventura	O
's	O
creative	O
levels	O
range	O
from	O
randomisation	O
to	O
plagiarisation	O
,	O
memorisation	O
,	O
generalisation	O
,	O
filtration	O
,	O
inception	O
1	O
and	O
creation	O
.	O
Further	O
gradations	O
and	O
criteria	O
could	O
be	O
advanced	O
,	O
for	O
example	O
,	O
the	O
fitness	O
function	O
used	O
for	O
filtration	O
could	O
be	O
developed	O
and	O
refined	O
as	O
the	O
system	O
learns	O
.	O
Creativity	O
might	O
be	O
assessed	O
in	O
a	O
social	O
context	O
,	O
as	O
we	O
investigate	O
how	O
a	O
system	O
collaborates	O
.	O
While	O
self	O
-	O
play	O
was	O
a	O
good	O
way	O
for	O
the	O
recently	O
developed	O
board	O
game	O
-	O
playing	O
system	O
AlphaGo	O
to	O
transcend	O
its	O
training	O
data	O
(	O
Silver	O
et	O
al	O
,	O
2016	O
)	O
,	O
we	O
do	O
not	O
yet	O
have	O
computationally	O
robust	O
qualitative	O
evaluation	O
measures	O
for	O
the	O
poetry	O
domain	O
,	O
where	O
there	O
is	O
no	O
obvious	O
"	O
winning	O
condition	O
.	O
"	O
We	O
began	O
by	O
creating	O
a	O
program	O
for	O
generating	O
haikus	O
,	O
trained	O
on	O
a	O
small	O
corpus	O
.	O
Our	O
technical	O
aim	O
then	O
was	O
to	O
simulate	O
the	O
collaborative	O
creation	O
of	O
renga	O
,	O
i.e.	O
,	O
linked	O
haikus	O
.	O
There	O
are	O
several	O
forms	O
of	O
renga	O
with	O
varying	O
constraints	O
(	O
Carley	O
,	O
2015	O
)	O
,	O
for	O
example	O
the	O
20	O
stanza	O
"	O
Nijiun	O
"	O
renga	O
which	O
alternates	O
between	O
twoline	O
and	O
three	O
-	O
line	O
verses	O
,	O
with	O
a	O
focus	O
on	O
seasonal	O
symbolism	O
and	O
rules	O
against	O
repetition	O
.	O
2	O
Our	O
initial	O
effort	O
was	O
a	O
technical	O
success	O
,	O
however	O
the	O
rengas	O
we	O
produced	O
fail	O
to	O
fully	O
satisfy	O
classical	O
constraints	O
.	O
A	O
subsequent	O
experiment	O
is	O
more	O
convincing	O
in	O
this	O
regard	O
,	O
but	O
still	O
leaves	O
room	O
for	O
improvement	O
.	O
Our	O
discussion	O
considers	O
the	O
aesthetics	O
of	O
the	O
generated	O
poems	O
and	O
outlines	O
directions	O
for	O
future	O
research	O
.	O

Coleridge	O
considered	O
poetry	O
to	O
be	O
"	O
the	O
blossom	O
and	O
the	O
fragrance	O
of	O
all	O
human	O
knowledge	O
.	O
"	O
AI	O
researcher	O
Ruli	O
Manurung	O
defines	O
poetry	O
somewhat	O
more	O
drily	O
:	O
"	O
A	O
poem	O
is	O
a	O
natural	O
language	O
artefact	O
which	O
simultaneously	O
fulfils	O
the	O
properties	O
of	O
meaningfulness	O
,	O
grammaticality	O
and	O
poeticness	O
"	O
(	O
Manurung	O
,	O
2004	O
,	O
p.	O
8	O
)	O
.	O
The	O
haiku	O
as	O
we	O
know	O
it	O
was	O
originally	O
called	O
hokku	O
-	O
発句	O
,	O
literally	O
the	O
"	O
starting	O
verse	O
"	O
of	O
a	O
collaboratively	O
written	O
poem	O
,	O
hakai	O
no	O
renga	O
.	O
Typically	O
each	O
of	O
following	O
links	O
in	O
a	O
renga	O
take	O
the	O
familiar	O
5/7/5	O
syllable	O
form	O
.	O
Classical	O
rengas	O
vary	O
in	O
length	O
from	O
two	O
to	O
100	O
links	O
(	O
and	O
,	O
rarely	O
,	O
even	O
1000	O
)	O
.	O
The	O
starting	O
verse	O
is	O
traditionally	O
comprised	O
of	O
two	O
images	O
,	O
with	O
a	O
kireji	O
-	O
a	O
sharp	O
cut	O
-	O
between	O
them	O
.	O
The	O
term	O
haiku	O
introduced	O
by	O
the	O
19th	O
Century	O
poet	O
Masaoka	O
Shiki	O
supersedes	O
the	O
older	O
term	O
.	O
Stylistically	O
,	O
a	O
haiku	O
captures	O
a	O
moment	O
.	O
In	O
classical	O
renga	O
,	O
all	O
of	O
the	O
verses	O
after	O
the	O
first	O
have	O
additional	O
complex	O
constraints	O
,	O
such	O
as	O
requiring	O
certain	O
images	O
to	O
be	O
used	O
at	O
certain	O
points	O
,	O
but	O
disallowing	O
repetition	O
,	O
with	O
various	O
proximity	O
constraints	O
.	O
The	O
setting	O
in	O
which	O
rengas	O
were	O
composed	O
is	O
also	O
worth	O
commenting	O
on	O
.	O
A	O
few	O
poets	O
would	O
compose	O
together	O
in	O
party	O
atmosphere	O
,	O
with	O
one	O
honoured	O
guest	O
proposing	O
the	O
starting	O
haiku	O
,	O
then	O
the	O
next	O
responding	O
,	O
and	O
continuing	O
in	O
turn	O
,	O
subject	O
to	O
the	O
oversight	O
of	O
a	O
scribe	O
and	O
a	O
renga	O
master	O
.	O
These	O
poetry	O
parties	O
were	O
once	O
so	O
popular	O
and	O
time	O
consuming	O
that	O
they	O
were	O
viewed	O
as	O
a	O
major	O
decadence	O
.	O
Jin'Ichi	O
et	O
al	O
(	O
1975	O
)	O
offers	O
a	O
useful	O
overview	O
.	O
Because	O
of	O
the	O
way	O
we	O
've	O
constructed	O
our	O
haiku	O
generating	O
system	O
,	O
it	O
can	O
take	O
an	O
entire	O
haiku	O
as	O
its	O
input	O
topic	O
-	O
we	O
just	O
add	O
the	O
word	O
vectors	O
to	O
make	O
a	O
topic	O
model	O
-	O
and	O
compose	O
a	O
response	O
.	O
This	O
affords	O
AI	O
-	O
to	O
-	O
AI	O
collaboration	O
,	O
or	O
AI	O
-	O
human	O
collaboration	O
.	O
It	O
can	O
also	O
blend	O
two	O
inputs	O
-	O
for	O
example	O
,	O
the	O
previous	O
haiku	O
and	O
the	O
current	O
constraint	O
from	O
the	O
renga	O
ruleset	O
(	O
e.g.	O
,	O
the	O
requirement	O
to	O
allude	O
to	O
"	O
cherry	O
blossoms	O
"	O
or	O
"	O
the	O
moon	O
"	O
)	O
.	O

In	O
terms	O
of	O
Ventura	O
's	O
hierarchy	O
of	O
creative	O
levels	O
,	O
the	O
haiku	O
system	O
appears	O
to	O
be	O
in	O
the	O
"	O
generalisation	O
"	O
stage	O
.	O
Our	O
renga	O
-	O
writing	O
experiments	O
with	O
FloWr	O
brought	O
in	O
a	O
"	O
filtration	O
"	O
aspect	O
.	O
The	O
research	O
themes	O
discussed	O
above	O
point	O
to	O
directions	O
for	O
future	O
work	O
in	O
pursuit	O
of	O
the	O
"	O
inception	O
"	O
and	O
"	O
creativity	O
"	O
stages	O
.	O
Some	O
previous	O
work	O
with	O
haiku	O
,	O
e.g.	O
Netzer	O
et	O
al	O
(	O
2009	O
)	O
and	O
Rzepka	O
and	O
Araki	O
(	O
2015	O
)	O
,	O
have	O
addressed	O
the	O
problem	O
of	O
meaning	O
.	O
The	O
renga	O
form	O
brings	O
these	O
issues	O
to	O
the	O
fore	O
.	O
We	O
hope	O
this	O
early	O
work	O
has	O
motivated	O
further	O
interest	O
in	O
this	O
challenging	O
and	O
enjoyable	O
poetic	O
form	O
that	O
-	O
like	O
other	O
less	O
constrained	O
forms	O
of	O
dialogue	O
-	O
combines	O
themes	O
of	O
natural	O
language	O
generation	O
and	O
understanding	O
.	O
One	O
natural	O
next	O
step	O
would	O
be	O
a	O
series	O
of	O
experiments	O
in	O
collaborative	O
human	O
-	O
AI	O
generation	O
of	O
rengas	O
.	O
Our	O
haiku	O
software	O
is	O
available	O
for	O
future	O
experiments	O
.	O
7	O

This	O
research	O
was	O
supported	O
by	O
the	O
Future	O
and	O
Emerging	O
Technologies	O
(	O
FET	O
)	O
programme	O
within	O
the	O
Seventh	O
Framework	O
Programme	O
for	O
Research	O
of	O
the	O
European	O
Commission	O
,	O
under	O
FET	O
-	O
Open	O
Grant	O
number	O
611553	O
(	O
COINVENT	O
)	O
.	O

We	O
then	O
used	O
K	O
-	O
means	O
with	O
two	O
cluster	O
centroids	O
to	O
label	O
each	O
point	O
in	O
the	O
space	O
based	O
on	O
that	O
point	O
's	O
distance	O
from	O
the	O
nearest	O
cluster	O
centroid	O
.	O
We	O
did	O
this	O
with	O
both	O
the	O
dimensionalityreduced	O
sentence	O
representations	O
and	O
the	O
original	O
768	O
-	O
dimensional	O
vectors	O
.	O
The	O
sentence	O
representations	O
and	O
the	O
K	O
-	O
means	O
labels	O
were	O
then	O
evaluated	O
using	O
the	O
aforementioned	O
evaluation	O
metrics	O
.	O

The	O
evaluation	O
metrics	O
for	O
the	O
K	O
-	O
means	O
labeled	O
points	O
in	O
the	O
space	O
does	O
not	O
seem	O
to	O
correspond	O
to	O
the	O
IAA	O
values	O
.	O
The	O
lowest	O
scoring	O
dogwhistles	O
,	O
"	O
refugee	O
policy	O
"	O
and	O
"	O
remigration	O
"	O
,	O
cluster	O
fairly	O
well	O
compared	O
to	O
the	O
other	O
dogwhistles	O
with	O
higher	O
IAA	O
values	O
.	O

The	O
results	O
for	O
the	O
evaluation	O
metrics	O
on	O
the	O
human	O
labeled	O
points	O
indicate	O
that	O
there	O
is	O
an	O
overall	O
correspondence	O
between	O
the	O
IAA	O
and	O
those	O
measurements	O
:	O
the	O
lowest	O
rated	O
IAA	O
dogwhistles	O
always	O
have	O
the	O
lowest	O
clustering	O
score	O
.	O
This	O
indicates	O
that	O
there	O
is	O
a	O
semantic	O
distinction	O
between	O
in	O
-	O
group	O
responses	O
and	O
out	O
-	O
group	O
responses	O
that	O
is	O
captured	O
fairly	O
well	O
by	O
sentence	O
transformers	O
.	O

Funding	O
for	O
this	O
work	O
was	O
provided	O
by	O
the	O
Gothenburg	O
Research	O
Initiative	O
for	O
Politically	O
Emergent	O
Systems	O
(	O
GRIPES	O
)	O
supported	O
by	O
the	O
Marianne	O
and	O
Marcus	O
Wallenberg	O
Foundation	O
grant	O
2019.0214	O
.	O
Christoffer	O
Olssson	O
assisted	O
with	O
some	O
of	O
the	O
annotations	O
used	O
in	O
the	O
work	O
.	O

The	O
idea	O
of	O
weighting	O
training	O
examples	O
has	O
a	O
long	O
history	O
.	O
Importance	O
sampling	O
(	O
Kahn	O
and	O
Marshall	O
,	O
1953	O
)	O
assigns	O
weights	O
to	O
different	O
samples	O
and	O
changes	O
the	O
data	O
distribution	O
.	O
Boosting	O
algorithms	O
such	O
as	O
AdaBoost	O
(	O
Kanduri	O
et	O
al	O
,	O
2018	O
)	O
select	O
harder	O
examples	O
to	O
train	O
subsequent	O
classifiers	O
.	O
Similarly	O
,	O
hard	O
example	O
mining	O
(	O
Malisiewicz	O
et	O
al	O
,	O
2011	O
)	O
downsamples	O
the	O
majority	O
class	O
and	O
exploits	O
the	O
most	O
difficult	O
examples	O
.	O
Oversampling	O
(	O
Chen	O
et	O
al	O
,	O
2010	O
;	O
Chawla	O
et	O
al	O
,	O
2002	O
)	O
(	O
Jiang	O
et	O
al	O
,	O
2017	O
;	O
Fan	O
et	O
al	O
,	O
2018	O
)	O
proposed	O
to	O
learn	O
a	O
separate	O
network	O
to	O
predict	O
sample	O
weights	O
.	O

We	O
thank	O
all	O
anonymous	O
reviewers	O
,	O
as	O
well	O
as	O
Qinghong	O
Han	O
,	O
Wei	O
Wu	O
and	O
Jiawei	O
Wu	O
for	O
their	O
comments	O
and	O
suggestions	O
.	O
The	O
work	O
is	O
supported	O
by	O
the	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
(	O
NSFC	O
No	O
.	O
61625107	O
and	O
61751209	O
)	O
.	O

Using	O
Linguistic	O
Features	O
to	O
Predict	O
the	O
Response	O
Process	O
Complexity	O
Associated	O
with	O
Answering	O
Clinical	O
MCQs	O

This	O
study	O
examines	O
the	O
relationship	O
between	O
the	O
linguistic	O
characteristics	O
of	O
a	O
test	O
item	O
and	O
the	O
complexity	O
of	O
the	O
response	O
process	O
required	O
to	O
answer	O
it	O
correctly	O
.	O
Using	O
data	O
from	O
a	O
large	O
-	O
scale	O
medical	O
licensing	O
exam	O
,	O
clustering	O
methods	O
identified	O
items	O
that	O
were	O
similar	O
with	O
respect	O
to	O
their	O
relative	O
difficulty	O
and	O
relative	O
response	O
-	O
time	O
intensiveness	O
to	O
create	O
low	O
response	O
process	O
complexity	O
and	O
high	O
response	O
process	O
complexity	O
item	O
classes	O
.	O
Interpretable	O
models	O
were	O
used	O
to	O
investigate	O
the	O
linguistic	O
features	O
that	O
best	O
differentiated	O
between	O
these	O
classes	O
from	O
a	O
descriptive	O
and	O
predictive	O
framework	O
.	O
Results	O
suggest	O
that	O
nuanced	O
features	O
such	O
as	O
the	O
number	O
of	O
ambiguous	O
medical	O
terms	O
help	O
explain	O
response	O
process	O
complexity	O
beyond	O
superficial	O
item	O
characteristics	O
such	O
as	O
word	O
count	O
.	O
Yet	O
,	O
although	O
linguistic	O
features	O
carry	O
signal	O
relevant	O
to	O
response	O
process	O
complexity	O
,	O
the	O
classification	O
of	O
individual	O
items	O
remains	O
challenging	O
.	O

We	O
use	O
a	O
set	O
of	O
interpretable	O
linguistic	O
features	O
,	O
many	O
of	O
which	O
were	O
previously	O
used	O
for	O
predicting	O
item	O
difficulty	O
(	O
Ha	O
et	O
al	O
,	O
2019	O
)	O
and	O
response	O
time	O
in	O
the	O
domain	O
of	O
clinical	O
MCQs	O
.	O
These	O
features	O
were	O
extracted	O
using	O
code	O
made	O
available	O
by	O
Ha	O
et	O
al	O
(	O
2019	O
)	O
and	O
to	O
these	O
,	O
we	O
add	O
several	O
predictors	O
specifically	O
related	O
to	O
the	O
medical	O
content	O
of	O
the	O
items	O
,	O
as	O
well	O
as	O
standard	O
item	O
metadata	O
.	O

As	O
noted	O
,	O
this	O
study	O
replicates	O
the	O
feature	O
extraction	O
procedure	O
described	O
and	O
made	O
available	O
by	O
Ha	O
et	O
al	O
(	O
2019	O
)	O
.	O
Approximately	O
90	O
linguistic	O
features	O
were	O
extracted	O
from	O
each	O
item	O
's	O
text	O
(	O
the	O
full	O
item	O
including	O
answer	O
options	O
)	O
and	O
are	O
summarized	O
in	O
Table	O
2	O
.	O
They	O
span	O
several	O
levels	O
of	O
linguistic	O
processing	O
including	O
surface	O
lexical	O
and	O
syntactic	O
features	O
,	O
semantic	O
features	O
that	O
account	O
for	O
ambiguity	O
,	O
and	O
cognitively	O
motivated	O
features	O
that	O
capture	O
properties	O
such	O
as	O
imageability	O
and	O
familiarity	O
.	O
Common	O
readability	O
formulae	O
are	O
used	O
to	O
account	O
for	O
surface	O
reading	O
difficulty	O
.	O
The	O
organization	O
of	O
ideas	O
in	O
the	O
text	O
is	O
captured	O
through	O
text	O
cohesion	O
features	O
that	O
measure	O
the	O
number	O
and	O
types	O
of	O
connective	O
words	O
within	O
an	O
item	O
.	O
Finally	O
,	O
word	O
frequency	O
features	O
(	O
including	O
threshold	O
frequencies	O
)	O
measure	O
the	O
extent	O
to	O
which	O
items	O
utilize	O
frequent	O
vocabulary	O
.	O
Combinations	O
of	O
these	O
features	O
have	O
the	O
potential	O
to	O
capture	O
different	O
aspects	O
of	O
item	O
content	O
that	O
are	O
relevant	O
to	O
response	O
complexity	O
.	O
For	O
example	O
,	O
medical	O
terms	O
can	O
be	O
expected	O
to	O
have	O
lower	O
absolute	O
frequencies	O
and	O
familiarity	O
ratings	O
,	O
among	O
other	O
characteristics	O
,	O
and	O
combinations	O
of	O
these	O
features	O
may	O
suggest	O
a	O
higher	O
density	O
of	O
medical	O
terms	O
and	O
specialized	O
language	O
in	O
some	O
items	O
compared	O
to	O
others	O
.	O
Another	O
example	O
is	O
the	O
temporal	O
organization	O
of	O
the	O
information	O
about	O
the	O
patient	O
history	O
and	O
symptoms	O
described	O
in	O
the	O
item	O
and	O
captured	O
by	O
temporal	O
connectives	O
,	O
where	O
it	O
is	O
reasonable	O
to	O
expect	O
that	O
more	O
temporally	O
intricate	O
cases	O
would	O
require	O
higher	O
response	O
process	O
complexity	O
to	O
solve	O
.	O
Similarly	O
,	O
a	O
high	O
number	O
of	O
causal	O
connectives	O
would	O
indicate	O
a	O
higher	O
complexity	O
of	O
causal	O
relationships	O
among	O
the	O
events	O
that	O
led	O
to	O
the	O
patient	O
seeing	O
a	O
doctor	O
,	O
which	O
may	O
also	O
be	O
associated	O
with	O
higher	O
cognitive	O
demands	O
.	O

This	O
group	O
of	O
features	O
refers	O
to	O
metadata	O
describing	O
item	O
content	O
.	O
Presence	O
of	O
an	O
image	O
is	O
a	O
binary	O
categorical	O
variable	O
indicating	O
whether	O
the	O
item	O
includes	O
an	O
image	O
such	O
an	O
X	O
-	O
ray	O
or	O
an	O
MRI	O
that	O
needs	O
to	O
be	O
examined	O
.	O
Another	O
variable	O
is	O
Content	O
category	O
,	O
which	O
describes	O
18	O
generic	O
topic	O
categories	O
such	O
as	O
"	O
Cardiovascular	O
"	O
,	O
"	O
Gastrointestinal	O
"	O
,	O
"	O
Behavioral	O
Health	O
"	O
,	O
'	O
Immune	O
System	O
"	O
,	O
and	O
so	O
on	O
.	O
Another	O
variable	O
,	O
Physician	O
Task	O
describes	O
tasks	O
required	O
by	O
the	O
item	O
,	O
e.g.	O
,	O
determine	O
a	O
diagnosis	O
,	O
choose	O
the	O
correct	O
medicine	O
,	O
apply	O
foundational	O
science	O
concepts	O
,	O
and	O
others	O
.	O
Finally	O
,	O
we	O
also	O
include	O
the	O
Year	O
the	O
item	O
was	O
administered	O
as	O
a	O
predictor	O
(	O
2010	O
-	O
2015	O
)	O
to	O
account	O
for	O
potential	O
changes	O
in	O
response	O
process	O
complexity	O
and	O
examinee	O
samples	O
over	O
time	O
.	O

Three	O
classification	O
baselines	O
were	O
computed	O
to	O
benchmark	O
the	O
predictive	O
benefit	O
given	O
by	O
linguistics	O
features	O
over	O
standard	O
item	O
characteristics	O
:	O
Majority	O
Class	O
Baseline	O
:	O
Since	O
the	O
lowcomplexity	O
class	O
contains	O
a	O
higher	O
number	O
of	O
items	O
,	O
it	O
is	O
more	O
likely	O
that	O
an	O
item	O
would	O
be	O
correctly	O
predicted	O
as	O
belonging	O
to	O
this	O
class	O
.	O
Word	O
Count	O
:	O
This	O
baseline	O
examines	O
the	O
possibility	O
that	O
response	O
process	O
complexity	O
is	O
simply	O
a	O
function	O
of	O
item	O
length	O
.	O
Standard	O
Item	O
Features	O
:	O
This	O
baseline	O
comprises	O
Word	O
count	O
,	O
Presence	O
of	O
an	O
image	O
,	O
Content	O
category	O
,	O
Physician	O
task	O
and	O
Year	O
.	O
This	O
model	O
reflects	O
the	O
standard	O
item	O
characteristics	O
that	O
most	O
testing	O
organizations	O
would	O
routinely	O
store	O
.	O

The	O
output	O
of	O
the	O
selected	O
-	O
features	O
prediction	O
model	O
was	O
analyzed	O
further	O
in	O
order	O
to	O
get	O
insight	O
into	O
this	O
model	O
's	O
performance	O
.	O
As	O
could	O
be	O
expected	O
,	O
the	O
majority	O
class	O
of	O
low	O
-	O
complexity	O
items	O
was	O
predicted	O
more	O
accurately	O
than	O
the	O
highcomplexity	O
class	O
,	O
as	O
shown	O
by	O
the	O
confusion	O
matrix	O
in	O
Table	O
4	O
.	O
An	O
interesting	O
observation	O
was	O
made	O
during	O
a	O
follow	O
-	O
up	O
classification	O
experiment	O
,	O
which	O
showed	O
that	O
this	O
effect	O
remained	O
when	O
using	O
balanced	O
classes	O
4	O
.	O
This	O
shows	O
that	O
the	O
success	O
in	O
predicting	O
this	O
class	O
can	O
not	O
be	O
attributed	O
solely	O
to	O
its	O
prevalence	O
but	O
potentially	O
also	O
to	O
its	O
high	O
homogeneity	O
compared	O
to	O
the	O
high	O
-	O
complexity	O
class	O
.	O
Next	O
,	O
we	O
plot	O
the	O
model	O
errors	O
across	O
the	O
two	O
classes	O
of	O
low	O
-	O
complexity	O
and	O
high	O
-	O
complexity	O
items	O
,	O
as	O
shown	O
in	O
Figure	O
3	O
.	O
Notably	O
,	O
items	O
with	O
average	O
response	O
times	O
below	O
150	O
seconds	O
were	O
predicted	O
as	O
low	O
-	O
complexity	O
most	O
of	O
the	O
time	O
,	O
with	O
minimal	O
consideration	O
of	O
their	O
p	O
-	O
value	O
.	O
This	O
shows	O
that	O
what	O
the	O
model	O
effectively	O
learned	O
was	O
to	O
distinguish	O
between	O
items	O
with	O
long	O
and	O
short	O
mean	O
response	O
times	O
,	O
which	O
overpowered	O
its	O
ability	O
to	O
predict	O
the	O
p	O
-	O
value	O
parameter	O
.	O
This	O
finding	O
is	O
consistent	O
with	O
previous	O
work	O
,	O
where	O
response	O
times	O
in	O
were	O
predicted	O
more	O
successfully	O
than	O
p	O
-	O
value	O
using	O
a	O
similar	O
set	O
of	O
linguistic	O
features	O
in	O
Ha	O
et	O
al	O
(	O
2019	O
)	O
.	O
Finally	O
,	O
analysis	O
of	O
the	O
feature	O
distributions	O
across	O
these	O
four	O
classes	O
revealed	O
no	O
unexpected	O
patterns	O
.	O

The	O
experiments	O
presented	O
in	O
this	O
paper	O
are	O
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
the	O
first	O
investigation	O
of	O
the	O
relationship	O
between	O
item	O
text	O
and	O
response	O
process	O
complexity	O
.	O
The	O
results	O
showed	O
that	O
such	O
a	O
relationship	O
exists	O
.	O
To	O
the	O
extent	O
that	O
items	O
were	O
written	O
as	O
clearly	O
and	O
as	O
concisely	O
as	O
possible	O
,	O
the	O
findings	O
suggest	O
that	O
high	O
-	O
complexity	O
medical	O
items	O
generally	O
include	O
longer	O
phrases	O
,	O
more	O
medical	O
terms	O
,	O
and	O
more	O
specific	O
descriptions	O
.	O
While	O
the	O
models	O
outperformed	O
several	O
baselines	O
,	O
they	O
required	O
a	O
large	O
number	O
of	O
features	O
to	O
do	O
so	O
and	O
the	O
predictive	O
utility	O
remained	O
low	O
.	O
Ultimately	O
,	O
this	O
shows	O
the	O
challenging	O
nature	O
of	O
modeling	O
response	O
process	O
complexity	O
using	O
interpretable	O
models	O
and	O
the	O
lack	O
of	O
a	O
straightforward	O
way	O
to	O
manipulate	O
this	O
item	O
property	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
our	O
three	O
system	O
runs	O
.	O
The	O
ideas	O
behind	O
our	O
methods	O
are	O
independent	O
of	O
the	O
word	O
order	O
in	O
a	O
sentence	O
.	O
Our	O
first	O
method	O
is	O
unsupervised	O
,	O
whereas	O
the	O
other	O
two	O
methods	O
are	O
supervised	O
.	O
The	O
first	O
and	O
second	O
method	O
share	O
the	O
same	O
preprocessing	O
.	O

Our	O
first	O
method	O
is	O
unsupervised	O
.	O
It	O
measures	O
the	O
overlap	O
between	O
the	O
tokens	O
in	O
sentence	O
s	O
1	O
and	O
the	O
tokens	O
in	O
sentence	O
s	O
2	O
.	O

You	O
should	O
do	O
it	O
.	O
You	O
can	O
do	O
it	O
,	O
too	O
.	O
1	O
4.39817	O
Unfortunately	O
the	O
answer	O
to	O
your	O
question	O
is	O
we	O
simply	O
do	O
not	O
know	O
.	O
My	O
answer	O
to	O
your	O
question	O
is	O
"	O
Probably	O
Not	O
"	O
.	O
1	O
3.70982	O
P	O
(	O
A	O
|	O
B	O
)	O
is	O
the	O
conditional	O
probability	O
of	O
A	O
,	O
given	O
B.	O
P	O
(	O
B	O
|	O
A	O
)	O
is	O
the	O
conditional	O
probability	O
of	O
B	O
given	O
A.	O

This	O
work	O
was	O
partially	O
funded	O
by	O
the	O
PhD	O
program	O
Online	O
Participation	O
,	O
supported	O
by	O
the	O
North	O
Rhine	O
-	O
Westphalian	O
funding	O
scheme	O
Fortschrittskollegs	O
and	O
by	O
the	O
German	O
Federal	O
Ministry	O
of	O
Economics	O
and	O
Technology	O
under	O
the	O
ZIM	O
program	O
(	O
Grant	O
No	O
.	O
KF2846504	O
)	O
.	O
The	O
authors	O
want	O
to	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
suggestions	O
and	O
comments	O
.	O

Here	O
,	O
we	O
formalize	O
the	O
problem	O
of	O
answering	O
user	O
queries	O
from	O
product	O
specifications	O
.	O
Given	O
a	O
question	O
Q	O
about	O
a	O
product	O
P	O
and	O
the	O
list	O
of	O
M	O
specifications	O
{	O
s	O
1	O
,	O
s	O
2	O
,	O
...	O
,	O
s	O
M	O
}	O
of	O
P	O
,	O
our	O
objective	O
is	O
to	O
identify	O
the	O
specification	O
s	O
i	O
that	O
can	O
help	O
answer	O
Q.	O
Here	O
,	O
we	O
assume	O
that	O
the	O
question	O
is	O
answerable	O
from	O
specifications	O
.	O

Diversifying	O
Content	O
Generation	O
for	O
Commonsense	O
Reasoning	O
with	O
Mixture	O
of	O
Knowledge	O
Graph	O
Experts	O

The	O
KG	O
-	O
enhanced	O
generative	O
reasoning	O
module	O
is	O
illustrated	O
in	O
Figure	O
2	O
.	O
It	O
consists	O
of	O
four	O
steps	O
.	O
First	O
,	O
a	O
sequence	O
-	O
associated	O
subgraph	O
is	O
retrieved	O
from	O
the	O
KG	O
given	O
the	O
input	O
sequence	O
(	O
3.1.1	O
)	O
.	O
Then	O
,	O
a	O
multi	O
-	O
relational	O
graph	O
encoder	O
iteratively	O
updates	O
the	O
representation	O
of	O
each	O
node	O
by	O
aggregating	O
information	O
from	O
its	O
neighboring	O
nodes	O
and	O
edges	O
(	O
3	O
.	O
1.2	O
)	O
.	O
Next	O
,	O
the	O
model	O
selects	O
salient	O
concepts	O
that	O
should	O
be	O
considered	O
during	O
generation	O
(	O
3	O
.	O
1.3	O
)	O
.	O
Finally	O
,	O
the	O
model	O
generates	O
outputs	O
by	O
integrating	O
the	O
token	O
embeddings	O
of	O
both	O
the	O
input	O
sequence	O
and	O
the	O
top	O
-	O
ranked	O
concepts	O
(	O
3.1.4	O
)	O
.	O

To	O
facilitate	O
the	O
reasoning	O
process	O
,	O
we	O
resort	O
to	O
an	O
external	O
commonsense	O
knowledge	O
graph	O
G	O
=	O
{	O
V	O
,	O
E	O
}	O
,	O
where	O
V	O
denotes	O
the	O
concept	O
set	O
and	O
E	O
denotes	O
the	O
edges	O
with	O
relations	O
.	O
Since	O
direct	O
reasoning	O
on	O
the	O
entire	O
graph	O
is	O
intractable	O
,	O
we	O
extract	O
a	O
sequence	O
-	O
associated	O
subgraph	O
G	O
x	O
=	O
{	O
V	O
x	O
,	O
E	O
x	O
}	O
,	O
where	O
V	O
x	O
consists	O
of	O
the	O
concepts	O
extracted	O
from	O
the	O
input	O
sequence	O
(	O
denoted	O
as	O
C	O
x	O
)	O
and	O
their	O
inter	O
-	O
connected	O
concepts	O
within	O
two	O
hops	O
,	O
i.e.	O
,	O
V	O
x	O
=	O
{	O
C	O
x	O
∪	O
N	O
(	O
C	O
x	O
)	O
∪	O
N	O
(	O
N	O
(	O
C	O
x	O
)	O
)	O
}	O
.	O
For	O
exam	O
-	O
ple	O
,	O
in	O
Figure	O
2	O
,	O
C	O
x	O
=	O
{	O
piano	O
,	O
sport	O
,	O
kind	O
}	O
and	O
V	O
x	O
=	O
{	O
piano	O
,	O
sport	O
,	O
kind	O
,	O
art	O
,	O
music	O
,	O
press	O
,	O
...	O
}	O
.	O
Next	O
,	O
the	O
generation	O
task	O
is	O
to	O
maximize	O
the	O
conditional	O
probability	O
p	O
(	O
y	O
|	O
x	O
,	O
G	O
x	O
)	O
.	O

.	O
Distinct	O
-	O
k	O
(	O
Li	O
et	O
al	O
,	O
2016	O
)	O
measures	O
the	O
total	O
number	O
of	O
unique	O
k	O
-	O
grams	O
normalized	O
by	O
the	O
total	O
number	O
of	O
generated	O
k	O
-	O
gram	O
tokens	O
to	O
avoid	O
favoring	O
long	O
sentences	O
.	O
Entropyk	O
reflects	O
how	O
evenly	O
the	O
empirical	O
k	O
-	O
gram	O
distribution	O
is	O
for	O
a	O
given	O
sentence	O
when	O
word	O
frequency	O
is	O
considered	O
.	O

(	O
2	O
)	O
Billy	O
wanted	O
to	O
go	O
to	O
the	O
zoo	O
.	O
He	O
saw	O
elephants	O
.	O
(	O
3	O
)	O
Billy	O
went	O
to	O
the	O
store	O
and	O
bought	O
an	O
elephant	O
.	O
(	O
1	O
)	O
Billy	O
's	O
parents	O
sent	O
him	O
on	O
an	O
African	O
safari	O
for	O
a	O
reward	O
.	O
(	O
2	O
)	O
He	O
went	O
to	O
the	O
zoo	O
later	O
in	O
the	O
day	O
and	O
saw	O
elephants	O
.	O
(	O
1	O
)	O
Billy	O
wanted	O
to	O
go	O
to	O
the	O
zoo	O
and	O
see	O
elephants	O
.	O
(	O
2	O
)	O
Billy	O
was	O
excited	O
to	O
go	O
on	O
his	O
trip	O
to	O
the	O
zoo	O
.	O
(	O
3	O
)	O
Billy	O
went	O
to	O
the	O
zoo	O
to	O
see	O
the	O
animals	O
.	O

Nucleus	O

In	O
this	O
paper	O
,	O
we	O
proposed	O
a	O
novel	O
method	O
that	O
diversified	O
the	O
generative	O
reasoning	O
by	O
a	O
mixture	O
of	O
expert	O
strategy	O
on	O
commonsense	O
knowledge	O
graph	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
work	O
to	O
boost	O
diversity	O
in	O
NLG	O
by	O
diversifying	O
knowledge	O
reasoning	O
on	O
commonsense	O
knowledge	O
graph	O
.	O
Experiments	O
on	O
two	O
generative	O
commonsense	O
reasoning	O
benchmarks	O
demonstrated	O
that	O
MoKGE	O
outperformed	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
on	O
diversity	O
,	O
while	O
achieving	O
on	O
par	O
performance	O
on	O
quality	O
.	O

The	O
work	O
is	O
supported	O
by	O
NSF	O
IIS	O
-	O
1849816	O
,	O
CCF	O
-	O
1901059	O
,	O
IIS	O
-	O
2119531	O
and	O
IIS	O
-	O
2142827	O
.	O

We	O
have	O
collected	O
a	O
Swedish	O
dataset	O
,	O
henceforth	O
referred	O
to	O
as	O
SweQUAD	O
-	O
MC	O
,	O
consisting	O
of	O
texts	O
and	O
MCQs	O
for	O
the	O
given	O
texts	O
.	O
The	O
dataset	O
was	O
created	O
by	O
three	O
paid	O
linguistics	O
students	O
instructed	O
to	O
pose	O
unambiguous	O
and	O
independent	O
questions	O
.	O
They	O
were	O
also	O
asked	O
to	O
identify	O
the	O
key	O
with	O
at	O
least	O
two	O
distractors	O
,	O
all	O
of	O
which	O
are	O
contiguous	O
phrases	O
in	O
a	O
given	O
text	O
.	O
Additionally	O
,	O
as	O
the	O
distractors	O
were	O
required	O
to	O
be	O
in	O
the	O
same	O
grammatical	O
form	O
as	O
the	O
key	O
(	O
e.g.	O
,	O
both	O
in	O
plural	O
)	O
,	O
the	O
students	O
were	O
allowed	O
to	O
change	O
the	O
grammatical	O
form	O
of	O
phrases	O
if	O
they	O
constituted	O
plausible	O
distractors	O
after	O
this	O
change	O
.	O
The	O
exact	O
instructions	O
given	O
to	O
the	O
students	O
along	O
with	O
more	O
details	O
on	O
the	O
used	O
texts	O
are	O
provided	O
in	O
Appendix	O
A.	O
Each	O
datapoint	O
in	O
SweQUAD	O
-	O
MC	O
consists	O
of	O
a	O
base	O
text	O
and	O
an	O
MCQ	O
,	O
i.e.	O
a	O
stem	O
,	O
the	O
key	O
and	O
at	O
least	O
two	O
distractors	O
.	O
The	O
same	O
text	O
can	O
be	O
reused	O
for	O
different	O
MCQs	O
,	O
but	O
the	O
sets	O
of	O
texts	O
in	O
training	O
(	O
∼	O
80	O
%	O
)	O
,	O
development	O
(	O
∼	O
10	O
%	O
)	O
and	O
test	O
(	O
∼	O
10	O
%	O
)	O
datasets	O
are	O
disjoint	O
.	O
However	O
,	O
some	O
overlap	O
in	O
sentences	O
is	O
possible	O
,	O
since	O
the	O
texts	O
might	O
come	O
from	O
the	O
same	O
source	O
.	O
Descriptive	O
statistics	O
of	O
all	O
SweQUAD	O
-	O
MC	O
splits	O
is	O
provided	O
in	O
Table	O
1	O
.	O

We	O
have	O
used	O
publicly	O
available	O
texts	O
from	O
the	O
websites	O
of	O
Swedish	O
government	O
agencies	O
.	O
The	O
exact	O
list	O
of	O
URLs	O
is	O
provided	O
in	O
the	O
GitHub	O
repository	O
associated	O
with	O
the	O
paper	O
.	O
The	O
exact	O
instructions	O
given	O
to	O
students	O
recruited	O
to	O
collect	O
SweQUAD	O
-	O
MC	O
dataset	O
(	O
and	O
their	O
translation	O
to	O
English	O
)	O
are	O
presented	O
in	O
Figure	O
6	O
.	O
In	O
addition	O
to	O
the	O
given	O
instructions	O
,	O
the	O
students	O
were	O
also	O
given	O
the	O
opportunity	O
to	O
slightly	O
reformulate	O
the	O
distractors	O
found	O
in	O
the	O
text	O
in	O
order	O
to	O
align	O
the	O
syntactic	O
structure	O
with	O
that	O
of	O
the	O
key	O
.	O

In	O
addition	O
to	O
the	O
metrics	O
1	O
-	O
10	O
presented	O
in	O
Section	O
6.1	O
,	O
we	O
have	O
also	O
looked	O
at	O
the	O
following	O
ones	O
(	O
MCQ%	O
means	O
"	O
Percentage	O
of	O
MCQ	O
"	O
and	O
DIS	O
means	O
"	O
generated	O
distractor	O
(	O
s	O
)	O
"	O
)	O
The	O
rationale	O
behind	O
metric	O
11	O
was	O
that	O
capitalized	O
answers	O
are	O
named	O
entities	O
and	O
thus	O
one	O
would	O
like	O
distractors	O
also	O
to	O
be	O
named	O
entities	O
.	O
However	O
,	O
it	O
does	O
not	O
always	O
hold	O
.	O
For	O
instance	O
,	O
consider	O
the	O
stem	O
"	O
Who	O
gets	O
an	O
e	O
-	O
mail	O
with	O
a	O
confirmation	O
of	O
a	O
successful	O
submission	O
of	O
the	O
application	O
for	O
the	O
work	O
permit	O
?	O
"	O
and	O
the	O
key	O
"	O
you	O
and	O
your	O
employer	O
"	O
.	O
A	O
distractor	O
"	O
Migration	O
Agency	O
"	O
would	O
suit	O
the	O
question	O
perfectly	O
,	O
although	O
capitalization	O
is	O
clearly	O
different	O
.	O
Metrics	O
12	O
-	O
17	O
were	O
candidates	O
to	O
become	O
overfitting	O
indicators	O
.	O
However	O
,	O
metric	O
2	O
was	O
excluded	O
,	O
since	O
AnyDisFromTrainDis	O
is	O
more	O
informative	O
,	O
given	O
phrases	O
used	O
as	O
distractors	O
in	O
training	O
data	O
can	O
be	O
repeated	O
in	O
other	O
texts	O
.	O
Metrics	O
13	O
-	O
14	O
were	O
excluded	O
,	O
since	O
it	O
's	O
unclear	O
whether	O
the	O
higher	O
or	O
lower	O
values	O
are	O
better	O
.	O
For	O
instance	O
,	O
if	O
a	O
text	O
from	O
the	O
training	O
data	O
and	O
the	O
given	O
text	O
are	O
thematically	O
similar	O
,	O
would	O
copying	O
a	O
distractor	O
from	O
training	O
data	O
be	O
considered	O
overfitting	O
?	O
Metrics	O
15	O
-	O
17	O
were	O
rejected	O
as	O
too	O
strict	O
,	O
leaving	O
the	O
possibility	O
of	O
actually	O
missing	O
overfitting	O
if	O
only	O
2	O
of	O
3	O
distractors	O
would	O
meet	O
the	O
criteria	O
.	O

We	O
used	O
one	O
sample	O
t	O
-	O
test	O
for	O
conducting	O
our	O
analysis	O
and	O
thus	O
the	O
following	O
assumptions	O
were	O
checked	O
for	O
.	O
1	O
.	O
The	O
variable	O
under	O
study	O
should	O
be	O
either	O
an	O
interval	O
or	O
ratio	O
variable	O
.	O
Our	O
variable	O
,	O
the	O
number	O
of	O
correctly	O
answered	O
MCQs	O
,	O
is	O
clearly	O
on	O
a	O
ratio	O
scale	O
.	O

The	O
exact	O
guidelines	O
given	O
to	O
the	O
teachers	O
and	O
their	O
translation	O
to	O
English	O
,	O
are	O
presented	O
in	O
Figure	O
10	O
.	O

Vad	O
täckeröver	O
hälften	O
av	O
Sveriges	O
yta	O
?	O
(	O
What	O
covers	O
more	O
than	O
half	O
of	O
the	O
surface	O
of	O
Sweden	O
?	O
)	O

This	O
work	O
was	O
supported	O
by	O
Vinnova	O
(	O
Sweden	O
's	O
Innovation	O
Agency	O
)	O
within	O
project	O
2019	O
-	O
02997	O
.	O
We	O
would	O
like	O
to	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
comments	O
,	O
as	O
well	O
as	O
Gabriel	O
Skantze	O
and	O
Bram	O
Willemsen	O
for	O
their	O
helpful	O
feedback	O
prior	O
to	O
the	O
submission	O
of	O
the	O
paper	O
.	O

Much	O
Gracias	O
:	O
Semi	O
-	O
supervised	O
Code	O
-	O
switch	O
Detection	O
for	O
Spanish	O
-	O
English	O
:	O
How	O
far	O
can	O
we	O
get	O
?	O

In	O
this	O
section	O
,	O
we	O
will	O
first	O
describe	O
the	O
manually	O
annotated	O
code	O
-	O
switched	O
data	O
that	O
we	O
use	O
for	O
evaluating	O
our	O
models	O
,	O
then	O
we	O
describe	O
the	O
monolingual	O
data	O
that	O
we	O
will	O
use	O
as	O
"	O
training	O
"	O
data	O
.	O
It	O
should	O
be	O
noted	O
that	O
this	O
is	O
not	O
real	O
training	O
data	O
,	O
as	O
it	O
is	O
not	O
annotated	O
for	O
the	O
task	O
at	O
hand	O
(	O
thus	O
the	O
setting	O
is	O
semi	O
-	O
supervised	O
)	O
.	O

In	O
order	O
to	O
perform	O
semi	O
-	O
supervised	O
codeswitching	O
detection	O
,	O
we	O
use	O
Wikipedia	O
data	O
,	O
because	O
it	O
is	O
available	O
in	O
many	O
languages	O
and	O
easy	O
to	O
obtain	O
.	O
We	O
extracted	O
dumps	O
from	O
September	O
1st	O
2020	O
with	O
Wikiextractor	O
2	O
.	O
Without	O
punctuation	O
and	O
numbers	O
,	O
the	O
English	O
dataset	O
contains	O
420	O
K	O
distinct	O
words	O
and	O
the	O
Spanish	O
dataset	O
contains	O
610	O
K	O
distinct	O
words	O
.	O
It	O
should	O
be	O
noted	O
that	O
there	O
is	O
a	O
domain	O
difference	O
between	O
the	O
training	O
and	O
the	O
dev	O
/	O
test	O
data	O
.	O
However	O
,	O
collecting	O
monolingual	O
data	O
from	O
Twitter	O
2	O
https://github.com/attardi/	O
wikiextractor	O
is	O
non	O
-	O
trivial	O
.	O
3	O
Furthermore	O
,	O
it	O
should	O
be	O
noted	O
that	O
the	O
Wikipedia	O
datasets	O
are	O
not	O
100	O
%	O
monolingual	O
,	O
so	O
there	O
will	O
be	O
some	O
Spanish	O
data	O
in	O
the	O
English	O
dump	O
and	O
vice	O
-	O
versa	O
.	O
Both	O
of	O
these	O
artefacts	O
might	O
have	O
a	O
negative	O
effect	O
on	O
performance	O
.	O

Tokenization	O
of	O
the	O
raw	O
datasets	O
is	O
done	O
using	O
the	O
English	O
and	O
Spanish	O
SpaCy	O
tokenization	O
models	O
4	O
,	O
as	O
it	O
matches	O
the	O
tokenization	O
of	O
the	O
development	O
and	O
test	O
sets	O
.	O
Punctuation	O
and	O
non	O
-	O
word	O
tokens	O
(	O
the	O
other	O
class	O
)	O
are	O
identified	O
with	O
manually	O
designed	O
rules	O
using	O
regular	O
expressions	O
,	O
and	O
the	O
python	O
emoji	O
package	O
.	O
Tokens	O
that	O
are	O
not	O
identified	O
as	O
other	O
,	O
are	O
labeled	O
with	O
the	O
corresponding	O
label	O
based	O
on	O
the	O
language	O
of	O
the	O
wikipedia	O
.	O
3	O
Methods	O

We	O
first	O
clean	O
the	O
mono	O
-	O
lingual	O
Wikipedia	O
data	O
by	O
removing	O
XML	O
/	O
HTML	O
tags	O
from	O
the	O
articles	O
and	O
special	O
tokens	O
that	O
belong	O
to	O
the	O
other	O
class	O
.	O
We	O
calculate	O
the	O
word	O
probability	O
based	O
on	O
the	O
resulting	O
data	O
(	O
word	O
frequency	O
/	O
total	O
number	O
of	O
words	O
)	O
using	O
Laplace	O
smoothing	O
with	O
a	O
smoothing	O
factor	O
of	O
1	O
.	O

We	O
also	O
experiment	O
with	O
taking	O
a	O
larger	O
context	O
into	O
account	O
through	O
bi	O
-	O
grams	O
and	O
tri	O
-	O
grams	O
.	O
Here	O
,	O
we	O
divide	O
the	O
frequency	O
of	O
the	O
n	O
-	O
gram	O
containing	O
the	O
word	O
with	O
the	O
frequency	O
of	O
the	O
leading	O
(	O
n	O
−	O
1	O
)	O
gram	O
.	O
The	O
probability	O
is	O
computed	O
this	O
way	O
for	O
a	O
given	O
word	O
in	O
each	O
language	O
,	O
and	O
then	O
the	O
label	O
with	O
the	O
highest	O
probability	O
is	O
assigned	O
to	O
the	O
word	O
.	O
Laplace	O
smoothing	O
with	O
a	O
factor	O
of	O
1	O
is	O
used	O
.	O
In	O
our	O
initial	O
experiments	O
,	O
tri	O
-	O
grams	O
showed	O
very	O
low	O
performance	O
,	O
so	O
we	O
use	O
bi	O
-	O
grams	O
in	O
the	O
remainder	O
of	O
this	O
paper	O
.	O

For	O
this	O
model	O
,	O
we	O
calculate	O
the	O
joint	O
log	O
probability	O
of	O
words	O
based	O
on	O
the	O
monolingual	O
training	O
data	O
,	O
and	O
assign	O
the	O
most	O
probable	O
label	O
.	O
We	O
vary	O
the	O
n	O
-	O
gram	O
size	O
from	O
1	O
to	O
6	O
and	O
use	O
Laplace	O
smoothing	O
with	O
a	O
factor	O
of	O
1	O
.	O

We	O
also	O
experiment	O
with	O
ensembling	O
the	O
previous	O
methods	O
,	O
where	O
we	O
use	O
a	O
simple	O
majority	O
voting	O
.	O
We	O
compare	O
using	O
all	O
models	O
,	O
to	O
using	O
the	O
best	O
3	O
and	O
the	O
best	O
5	O
models	O
,	O
as	O
well	O
as	O
an	O
oracle	O
.	O

Universal	O
schemas	O
:	O
We	O
compared	O
our	O
method	O
with	O
semi	O
-	O
supervised	O
methods	O
based	O
on	O
universal	O
schemas	O
(	O
Toutanova	O
et	O
al	O
,	O
2015	O
;	O
Verga	O
et	O
al	O
,	O
2017	O
)	O
.	O
In	O
our	O
experiments	O
,	O
we	O
used	O
the	O
same	O
encoder	O
as	O
(	O
Song	O
et	O
al	O
,	O
2018	O
)	O
to	O
encode	O
each	O
surface	O
pattern	O
.	O
9	O
We	O
tested	O
two	O
types	O
of	O
scoring	O
functions	O
,	O
Model	O
F	O
and	O
Model	O
E	O
,	O
as	O
in	O
(	O
Toutanova	O
et	O
al	O
,	O
2015	O
)	O
.	O
10	O
,	O
11	O

We	O
thank	O
all	O
the	O
EMNLP	O
reviewers	O
and	O
Daniel	O
Andrade	O
for	O
their	O
valuable	O
comments	O
and	O
suggestions	O
to	O
improve	O
the	O
paper	O
.	O

To	O
gain	O
further	O
insights	O
,	O
we	O
compared	O
ZAR	O
results	O
with	O
English	O
translations	O
automatically	O
generated	O
by	O
the	O
corresponding	O
MT	O
model	O
.	O
Figure	O
4	O
gives	O
two	O
examples	O
.	O
It	O
is	O
no	O
great	O
surprise	O
that	O
the	O
translation	O
quality	O
was	O
not	O
satisfactory	O
because	O
we	O
did	O
not	O
fully	O
optimize	O
the	O
model	O
for	O
it	O
.	O
In	O
the	O
exmple	O
of	O
Figure	O
4	O
(	O
a	O
)	O
,	O
MT	O
seems	O
to	O
have	O
helped	O
ZAR	O
.	O
The	O
omitted	O
nominative	O
argument	O
of	O
"	O
あり	O
"	O
(	O
is	O
)	O
was	O
correctly	O
translated	O
as	O
"	O
the	O
school	O
"	O
,	O
and	O
the	O
model	O
successfully	O
identified	O
its	O
antecedent	O
"	O
学校	O
"	O
(	O
school	O
)	O
while	O
the	O
baseline	O
failed	O
.	O
Figure	O
4	O
(	O
b	O
)	O
illustrates	O
a	O
limitation	O
of	O
the	O
proposed	O
approach	O
.	O
The	O
omitted	O
nominative	O
argument	O
of	O
the	O
predicate	O
,	O
"	O
で	O
"	O
(	O
be	O
)	O
,	O
points	O
to	O
"	O
定吉	O
"	O
(	O
Sadakichi	O
,	O
the	O
father	O
of	O
Jutaro	O
)	O
.	O
Although	O
the	O
model	O
correctly	O
translated	O
the	O
zero	O
pronoun	O
as	O
"	O
He	O
"	O
,	O
it	O
failed	O
in	O
ZAR	O
.	O
This	O
is	O
probably	O
because	O
not	O
only	O
"	O
定吉	O
(	O
Sadakichi	O
)	O
"	O
but	O
also	O
"	O
龍馬	O
"	O
(	O
Ryoma	O
)	O
and	O
"	O
重太郎	O
"	O
(	O
Jutaro	O
)	O
can	O
be	O
referred	O
to	O
as	O
"	O
He	O
"	O
.	O
When	O
disambiguation	O
is	O
not	O
required	O
to	O
generate	O
an	O
overt	O
pronoun	O
,	O
MT	O
is	O
not	O
very	O
helpful	O
.	O

We	O
thank	O
the	O
Yomiuri	O
Shimbun	O
for	O
providing	O
Japanese	O
-	O
English	O
parallel	O
texts	O
.	O
We	O
are	O
grateful	O
for	O
Nobuhiro	O
Ueda	O
'	O
help	O
in	O
setting	O
up	O
the	O
baseline	O
model	O
.	O
We	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
insightful	O
comments	O
.	O

Although	O
we	O
followed	O
Ueda	O
et	O
al	O
(	O
2020	O
)	O
with	O
respect	O
to	O
hyper	O
-	O
parameter	O
settings	O
,	O
there	O
was	O
one	O
exception	O
.	O
Verbal	O
predicate	O
analysis	O
is	O
conventionally	O
divided	O
into	O
three	O
types	O
:	O
overt	O
,	O
covert	O
,	O
and	O
zero	O
.	O
While	O
Ueda	O
et	O
al	O
(	O
2020	O
)	O
excluded	O
the	O
easiest	O
overt	O
type	O
from	O
training	O
,	O
we	O
targeted	O
all	O
the	O
three	O
types	O
because	O
we	O
found	O
slight	O
performance	O
improvements	O
.	O
The	O
overt	O
type	O
covers	O
situations	O

Tables	O
9	O
and	O
10	O
show	O
the	O
performance	O
on	O
the	O
validation	O
sets	O
.	O

Resolution	O
and	O
Zero	O
Pronoun	O
Translatoin	O

As	O
it	O
was	O
mentioned	O
before	O
,	O
nowadays	O
there	O
are	O
numerous	O
avenues	O
for	O
event	O
data	O
acquisition	O
.	O
For	O
this	O
paper	O
Twitter	O
was	O
chosen	O
as	O
the	O
social	O
network	O
to	O
use	O
for	O
retrieving	O
data	O
since	O
this	O
data	O
is	O
easily	O
available	O
and	O
a	O
good	O
amount	O
of	O
it	O
is	O
related	O
to	O
events	O
of	O
different	O
categories	O
.	O
Twitter	O
's	O
REST	O
API	O
was	O
used	O
in	O
order	O
to	O
retrieve	O
data	O
related	O
to	O
these	O
events	O
:	O
Each	O
dataset	O
had	O
a	O
file	O
per	O
day	O
with	O
all	O
the	O
tweets	O
from	O
the	O
day	O
and	O
contained	O
only	O
the	O
text	O
that	O
represents	O
a	O
tweet	O
per	O
line	O
.	O

With	O
the	O
raw	O
data	O
ready	O
to	O
be	O
used	O
,	O
the	O
preprocessing	O
step	O
followed	O
.	O
The	O
sequence	O
followed	O
is	O
exposed	O
below	O
:	O
1	O
.	O
Removing	O
punctuation	O
and	O
unicode	O
only	O
characters	O
except	O
written	O
accents	O
.	O

Each	O
tokenized	O
tweet	O
al	O
o	O
contains	O
a	O
reference	O
to	O
the	O
original	O
,	O
unprocessed	O
tweet	O
,	O
which	O
will	O
be	O
used	O
on	O
Section	O
5	O
.	O

In	O
this	O
subsection	O
UDPipe	O
is	O
also	O
used	O
in	O
order	O
to	O
extract	O
the	O
syntactic	O
dependencies	O
,	O
in	O
particular	O
,	O
the	O
focus	O
is	O
to	O
obtain	O
'	O
dobj	O
'	O
and	O
'	O
iobj	O
'	O
objects	O
,	O
which	O
refer	O
to	O
direct	O
and	O
indirect	O
object	O
respectively	O
,	O
and	O
then	O
obtain	O
the	O
root	O
verb	O
they	O
stem	O
from	O
.	O
By	O
doing	O
this	O
a	O
verb	O
can	O
be	O
linked	O
to	O
each	O
object	O
and	O
furthermore	O
,	O
the	O
entities	O
related	O
to	O
verb	O
,	O
which	O
were	O
obtained	O
from	O
the	O
Formal	O
Context	O
,	O
can	O
be	O
linked	O
to	O
each	O
object	O
.	O
Doing	O
this	O
allows	O
us	O
to	O
add	O
activities	O
for	O
each	O
entity	O
,	O
as	O
well	O
as	O
create	O
a	O
relationship	O
between	O
two	O
entities	O
where	O
one	O
of	O
them	O
appears	O
as	O
an	O
object	O
in	O
the	O
action	O
of	O
another	O
.	O

We	O
want	O
to	O
augment	O
existing	O
labeled	O
utterances	O
by	O
generating	O
additional	O
novel	O
utterances	O
in	O
a	O
desired	O
target	O
language	O
.	O
In	O
our	O
case	O
,	O
existing	O
data	O
consists	O
of	O
feature	O
-	O
unrelated	O
data	O
(	O
intents	O
and	O
slots	O
already	O
supported	O
)	O
spanning	O
all	O
languages	O
and	O
featurerelated	O
data	O
,	O
which	O
is	O
available	O
in	O
a	O
source	O
language	O
but	O
is	O
small	O
(	O
few	O
-	O
shot	O
)	O
or	O
not	O
available	O
(	O
zero	O
shot	O
)	O
in	O
other	O
languages	O
.	O
For	O
generation	O
,	O
we	O
first	O
extract	O
the	O
intent	O
and	O
slot	O
types	O
from	O
the	O
available	O
data	O
.	O
We	O
then	O
generate	O
a	O
new	O
utterance	O
by	O
conditioning	O
a	O
multilingual	O
language	O
model	O
on	O
the	O
intent	O
,	O
slot	O
types	O
and	O
the	O
target	O
language	O
.	O
We	O
refer	O
to	O
utterances	O
that	O
have	O
the	O
same	O
intent	O
and	O
slot	O
types	O
as	O
paraphrases	O
of	O
each	O
other	O
since	O
they	O
convey	O
the	O
same	O
meaning	O
in	O
the	O
context	O
of	O
the	O
SLU	O
system	O
.	O

We	O
evaluate	O
our	O
approach	O
by	O
simulating	O
few	O
-	O
shot	O
and	O
zero	O
-	O
shot	O
feature	O
bootstrapping	O
scenarios	O
.	O

Paraphrases	O
generated	O
in	O
different	O
languages	O
for	O
a	O
given	O
input	O
are	O
shown	O
in	O
Table	O
6	O
.	O
The	O
intent	O
is	O
airline	O
and	O
the	O
slots	O
are	O
fromloc.city_name	O
for	O
columbus	O
and	O
toloc.city_name	O
for	O
minneapolis	O
.	O
For	O
this	O
intent	O
and	O
the	O
slots	O
,	O
the	O
generated	O
paraphrase	O
in	O
German	O
(	O
translated	O
to	O
English	O
)	O
is	O
Show	O
me	O
all	O
the	O
airlines	O
that	O
fly	O
from	O
Toronto	O
to	O
Boston	O
.	O
The	O
desired	O
intent	O
,	O
that	O
is	O
airline	O
is	O
realized	O
in	O
the	O
gener	O
-	O
ated	O
paraphrase	O
.	O
Additionally	O
,	O
Toronto	O
and	O
Boston	O
are	O
the	O
slot	O
values	O
respectively	O
for	O
the	O
slot	O
types	O
fromloc.city_name	O
and	O
toloc.city_name	O
.	O
For	O
Spanish	O
,	O
the	O
generated	O
paraphrase	O
(	O
translated	O
to	O
English	O
)	O
is	O
Which	O
Airlines	O
Fly	O
from	O
Atlanta	O
to	O
Philadelphia	O
.	O
The	O
airline	O
intent	O
is	O
realized	O
in	O
the	O
generated	O
paraphrase	O
and	O
also	O
Atlanta	O
and	O
Philadelphia	O
are	O
the	O
slot	O
values	O
produced	O
associated	O
with	O
the	O
desired	O
slot	O
types	O
.	O
As	O
illustrated	O
by	O
the	O
examples	O
,	O
the	O
model	O
is	O
free	O
to	O
pick	O
a	O
specific	O
slot	O
value	O
during	O
generation	O
,	O
leading	O
to	O
variations	O
across	O
languages	O
,	O
but	O
all	O
are	O
consistent	O
with	O
the	O
slot	O
type	O
.	O

We	O
would	O
like	O
to	O
thank	O
our	O
anonymous	O
reviewers	O
for	O
their	O
thoughtful	O
comments	O
and	O
suggestions	O
that	O
improved	O
the	O
final	O
version	O
of	O
this	O
paper	O
.	O

PBoS	O
:	O
Probabilistic	O
Bag	O
-	O
of	O
-	O
Subwords	O
for	O
Generalizing	O
Word	O
Embedding	O

show	O
that	O
word	O
vectors	O
generated	O
by	O
PBoS	O
have	O
better	O
quality	O
compared	O
to	O
previously	O
proposed	O
models	O
across	O
languages	O
(	O
Section	O
4.3	O
and	O
4.4	O
)	O
.	O

Exchanging	O
the	O
order	O
of	O
summations	O
in	O
Eq	O
.	O
(	O
5	O
)	O
from	O
segmentation	O
first	O
to	O
subword	O
first	O
,	O
we	O
get	O
w	O
=	O
s⊆w	O
a	O
s	O
|	O
w	O
s	O
(	O
7	O
)	O
where	O
a	O
s	O
|	O
w	O
∝	O
g	O
Segw	O
,	O
g	O
s	O
p	O
g	O
|	O
w	O
(	O
8	O
)	O
is	O
the	O
weight	O
accumulated	O
over	O
subword	O
s	O
,	O
summing	O
over	O
all	O
segmentations	O
of	O
w	O
that	O
contain	O
s.	O
5	O
Eq	O
.	O
(	O
7	O
)	O
provides	O
an	O
alternative	O
view	O
of	O
the	O
word	O
vector	O
composed	O
by	O
our	O
model	O
:	O
a	O
weighted	O
sum	O
of	O
all	O
the	O
word	O
's	O
subword	O
vectors	O
.	O
Comparing	O
to	O
BoS	O
,	O
we	O
assign	O
different	O
importance	O
a	O
s	O
|	O
w	O
,	O
instead	O
of	O
a	O
uniform	O
weight	O
,	O
to	O
each	O
subword	O
.	O
a	O
s	O
|	O
w	O
can	O
be	O
viewed	O
as	O
the	O
likelihood	O
of	O
subword	O
s	O
being	O
a	O
meaningful	O
segment	O
of	O
the	O
particular	O
word	O
w	O
,	O
considering	O
both	O
the	O
likelihood	O
of	O
s	O
itself	O
being	O
meaningful	O
,	O
and	O
at	O
the	O
same	O
time	O
how	O
likely	O
the	O
rest	O
of	O
the	O
word	O
can	O
still	O
be	O
segmented	O
into	O
meaningful	O
subwords	O
.	O
Example	O
.	O
Consider	O
the	O
contribution	O
of	O
subword	O
s	O
=	O
"	O
gher	O
"	O
in	O
word	O
w	O
=	O
"	O
higher	O
"	O
.	O
Possible	O
contributions	O
only	O
come	O
from	O
segmentations	O
that	O
contain	O
"	O
higher	O
"	O
:	O
g	O
1	O
=	O
(	O
"	O
h	O
"	O
,	O
"	O
i	O
"	O
,	O
"	O
gher	O
"	O
)	O
and	O
g	O
2	O
=	O
(	O
"	O
hi	O
"	O
,	O
"	O
gher	O
"	O
)	O
.	O
Each	O
segmentation	O
g	O
adds	O
weight	O
p	O
g	O
|	O
w	O
to	O
a	O
s	O
|	O
w	O
.	O
In	O
this	O
case	O
,	O
a	O
"	O
gher	O
"	O
|	O
w	O
will	O
be	O
smaller	O
than	O
a	O
"	O
er	O
"	O
|	O
w	O
because	O
both	O
p	O
g	O
1	O
|	O
w	O
and	O
p	O
g	O
2	O
|	O
w	O
would	O
be	O
rather	O
small	O
.	O

We	O
further	O
assess	O
the	O
quality	O
of	O
generated	O
word	O
embedding	O
via	O
the	O
extrinsic	O
task	O
of	O
POS	O
tagging	O
.	O
The	O
task	O
is	O
to	O
categorize	O
each	O
word	O
in	O
a	O
given	O
context	O
into	O
a	O
particular	O
part	O
of	O
speech	O
,	O
e.g.	O
noun	O
,	O
verb	O
,	O
and	O
adjective	O
.	O

Here	O
we	O
list	O
the	O
details	O
of	O
our	O
experiments	O
that	O
are	O
omitted	O
in	O
the	O
main	O
paper	O
due	O
to	O
space	O
constraints	O
.	O
We	O
run	O
all	O
our	O
experiments	O
on	O
a	O
machine	O
with	O
an	O
8	O
-	O
core	O
Intel	O
i7	O
-	O
6700	O
CPU	O
@	O
3.40GHz	O
,	O
32	O
GB	O
Memory	O
,	O
and	O
GeForce	O
GTX	O
970	O
GPU	O
.	O

The	O
meaning	O
of	O
hyperparameters	O
shown	O
in	O
Table	O
6	O
,	O
Table	O
7	O
and	O
Table	O
8	O
as	O
explained	O
as	O
follows	O
.	O

min	O
len	O
:	O
The	O
minimum	O
length	O
for	O
a	O
subword	O
to	O
be	O
considered	O
.	O
max	O
len	O
:	O
The	O
maximum	O
length	O
for	O
a	O
subword	O
to	O
be	O
considered	O
.	O
word	O
boundary	O
:	O
Whether	O
to	O
add	O
special	O
characters	O
to	O
annotate	O
word	O
boundaries	O
.	O

The	O
authors	O
would	O
like	O
to	O
thank	O
anonymous	O
reviewers	O
of	O
EMNLP	O
for	O
their	O
comments	O
.	O
ZJ	O
would	O
like	O
to	O
thank	O
Xuezhou	O
Zhang	O
,	O
Sidharth	O
Mudgal	O
,	O
Matt	O
Du	O
and	O
Harit	O
Vishwakarma	O
for	O
their	O
helpful	O
discussions	O
.	O

Ntrain	O
is	O
the	O
number	O
of	O
training	O
instances	O
for	O
the	O
POS	O
tagging	O
model	O
.	O
OOV	O
%	O
is	O
the	O
percentage	O
of	O
the	O
words	O
in	O
the	O
POS	O
tagging	O
testing	O
set	O
that	O
is	O
out	O
of	O
the	O
vocabulary	O
of	O
the	O
Polyglot	O
vectors	O
in	O
that	O
language	O
.	O
Experimental	O
results	O
are	O
included	O
for	O
convenience	O
.	O

Answering	O
complex	O
questions	O
that	O
involve	O
multiple	O
entities	O
and	O
multiple	O
relations	O
using	O
a	O
standard	O
knowledge	O
base	O
is	O
an	O
open	O
and	O
challenging	O
task	O
.	O
Most	O
existing	O
KBQA	O
approaches	O
focus	O
on	O
simpler	O
questions	O
and	O
do	O
not	O
work	O
very	O
well	O
on	O
complex	O
questions	O
because	O
they	O
were	O
not	O
able	O
to	O
simultaneously	O
represent	O
the	O
question	O
and	O
the	O
corresponding	O
complex	O
query	O
structure	O
.	O
In	O
this	O
work	O
,	O
we	O
encode	O
such	O
complex	O
query	O
structure	O
into	O
a	O
uniform	O
vector	O
representation	O
,	O
and	O
thus	O
successfully	O
capture	O
the	O
interactions	O
between	O
individual	O
semantic	O
components	O
within	O
a	O
complex	O
question	O
.	O
This	O
approach	O
consistently	O
outperforms	O
existing	O
methods	O
on	O
complex	O
questions	O
while	O
staying	O
competitive	O
on	O
simple	O
questions	O
.	O

Entity	O

In	O
this	O
section	O
,	O
we	O
introduce	O
the	O
QA	O
datasets	O
and	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
systems	O
that	O
we	O
compare	O
.	O
We	O
show	O
the	O
end	O
-	O
to	O
-	O
end	O
results	O
of	O
the	O
KBQA	O
task	O
,	O
and	O
perform	O
detail	O
analysis	O
to	O
investigate	O
the	O
importance	O
of	O
different	O
modules	O
used	O
in	O
our	O
approach	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
work	O
to	O
handle	O
complex	O
KBQA	O
task	O
by	O
explicitly	O
encoding	O
the	O
complete	O
semantics	O
of	O
a	O
complex	O
query	O
graph	O
using	O
neural	O
networks	O
.	O
We	O
stud	O
-	O
ied	O
different	O
methods	O
to	O
further	O
improve	O
the	O
performance	O
,	O
mainly	O
leveraging	O
dependency	O
parse	O
and	O
the	O
ensemble	O
method	O
for	O
linking	O
enrichment	O
.	O
Our	O
model	O
becomes	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
Com	O
-	O
plexQuestions	O
dataset	O
,	O
and	O
produces	O
competitive	O
results	O
on	O
other	O
simple	O
question	O
based	O
datasets	O
.	O
Possible	O
future	O
work	O
includes	O
supporting	O
more	O
complex	O
semantics	O
like	O
implicit	O
time	O
constraints	O
.	O

Kenny	O
Q.	O
Zhu	O
is	O
the	O
contact	O
author	O
and	O
was	O
supported	O
by	O
NSFC	O
grants	O
91646205	O
and	O
61373031	O
.	O
Thanks	O
to	O
the	O
anonymous	O
reviewers	O
for	O
their	O
valuable	O
feedback	O
.	O

Multi	O
-	O
Model	O
and	O
Crosslingual	O
Dependency	O
Analysis	O

We	O
would	O
like	O
to	O
thank	O
the	O
developpers	O
of	O
Bist	O
-	O
Parser	O
,	O
Eliyahu	O
Kiperwasser	O
and	O
Yoav	O
Goldberg	O
,	O
and	O
the	O
developper	O
of	O
the	O
CNN	O
library	O
(	O
Chris	O
Dyer	O
)	O
for	O
making	O
them	O
available	O
as	O
open	O
source	O
on	O
GitHub	O
.	O
Finally	O
we	O
would	O
like	O
to	O
thank	O
our	O
colleague	O
Ghislain	O
Putois	O
for	O
help	O
on	O
all	O
aspects	O
on	O
neural	O
networks	O
.	O

In	O
this	O
section	O
,	O
we	O
first	O
provide	O
the	O
conventional	O
definition	O
of	O
DST	O
and	O
then	O
extend	O
the	O
definition	O
to	O
the	O
noisy	O
label	O
learning	O
scenario	O
.	O

Let	O
X	O
=	O
{	O
(	O
R	O
1	O
,	O
U	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
R	O
T	O
,	O
U	O
T	O
)	O
}	O
denote	O
a	O
dialogue	O
of	O
T	O
turns	O
,	O
where	O
R	O
t	O
and	O
U	O
t	O
represent	O
the	O
system	O
response	O
and	O
user	O
utterance	O
at	O
turn	O
t	O
,	O
respectively	O
.	O
The	O
dialogue	O
state	O
at	O
turn	O
t	O
is	O
defined	O
as	O
B	O
t	O
=	O
{	O
(	O
s	O
,	O
v	O
t	O
)	O
|	O
s	O
S	O
}	O
,	O
where	O
S	O
denotes	O
the	O
set	O
of	O
predefined	O
slots	O
and	O
v	O
t	O
is	O
the	O
corresponding	O
value	O
of	O
slot	O
s.	O
Following	O
previous	O
work	O
Hu	O
et	O
al	O
,	O
2020	O
;	O
Ye	O
et	O
al	O
,	O
2021b	O
)	O
,	O
a	O
slot	O
in	O
this	O
paper	O
refers	O
to	O
the	O
concatenation	O
of	O
the	O
domain	O
name	O
and	O
slot	O
name	O
so	O
as	O
to	O
include	O
the	O
domain	O
information	O
.	O
For	O
example	O
,	O
we	O
use	O
"	O
hotel	O
-	O
name	O
"	O
to	O
represent	O
the	O
slot	O
"	O
name	O
"	O
in	O
the	O
hotel	O
domain	O
.	O
In	O
general	O
,	O
the	O
issue	O
of	O
DST	O
is	O
defined	O
as	O
learning	O
a	O
dialogue	O
state	O
tracker	O
F	O
:	O
X	O
t	O
B	O
t	O
that	O
takes	O
the	O
dialogue	O
context	O
X	O
t	O
as	O
input	O
and	O
predicts	O
the	O
dialogue	O
state	O
B	O
t	O
at	O
each	O
turn	O
t	O
as	O
accurately	O
as	O
possible	O
.	O
Here	O
,	O
X	O
t	O
represents	O
the	O
dialogue	O
history	O
up	O
to	O
turn	O
t	O
,	O
i.e.	O
,	O
X	O
t	O
=	O
{	O
(	O
R	O
1	O
,	O
U	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
R	O
t	O
,	O
U	O
t	O
)	O
}	O
.	O

We	O
introduce	O
a	O
general	O
framework	O
ASSIST	O
,	O
aiming	O
to	O
train	O
DST	O
models	O
robustly	O
from	O
noisy	O
labels	O
.	O
We	O
assume	O
that	O
a	O
small	O
clean	O
dataset	O
is	O
accessible	O
.	O
Based	O
on	O
this	O
dataset	O
,	O
ASSIST	O
first	O
trains	O
an	O
auxiliary	O
model	O
A.	O
Then	O
,	O
it	O
leverages	O
A	O
to	O
generate	O
pseudo	O
labels	O
for	O
each	O
sample	O
in	O
the	O
noisy	O
training	O
set	O
.	O
The	O
pseudo	O
state	O
annotations	O
are	O
represented	O
asB	O
t	O
=	O
{	O
(	O
s	O
,	O
v	O
t	O
)	O
|	O
s	O
S	O
}	O
,	O
wherev	O
t	O
denotes	O
the	O
pseudo	O
label	O
of	O
slot	O
s	O
at	O
turn	O
t.	O
Afterwards	O
,	O
both	O
the	O
generated	O
pseudo	O
labels	O
and	O
vanilla	O
noisy	O
labels	O
are	O
exploited	O
to	O
train	O
the	O
primary	O
model	O
F	O
*	O
.	O
That	O
is	O
,	O
we	O
intend	O
to	O
learn	O
F	O
*	O
:	O
X	O
t	O
C	O
(	O
B	O
t	O
,	O
B	O
t	O
)	O
,	O
where	O
C	O
(	O
B	O
t	O
,	O
B	O
t	O
)	O
is	O
a	O
combination	O
ofB	O
t	O
andB	O
t	O
.	O
Essentially	O
,	O
any	O
existing	O
DST	O
models	O
can	O
be	O
employed	O
as	O
the	O
auxiliary	O
model	O
.	O
However	O
,	O
these	O
models	O
may	O
lead	O
to	O
overfitting	O
due	O
to	O
the	O
small	O
size	O
of	O
the	O
clean	O
dataset	O
.	O
To	O
tackle	O
this	O
issue	O
,	O
we	O
propose	O
a	O
new	O
simple	O
model	O
as	O
the	O
auxiliary	O
model	O
1	O
.	O

The	O
slot	O
-	O
value	O
matching	O
module	O
is	O
utilized	O
to	O
predict	O
the	O
value	O
of	O
each	O
slot	O
s.	O
It	O
first	O
calculates	O
the	O
distance	O
between	O
the	O
slot	O
-	O
specific	O
representation	O
g	O
s	O
t	O
and	O
the	O
semantic	O
representation	O
of	O
each	O
candidate	O
value	O
v	O
V	O
s	O
,	O
i.e.	O
,	O
h	O
v	O
.	O
Then	O
,	O
the	O
candidate	O
value	O
with	O
the	O
smallest	O
distance	O
is	O
selected	O
as	O
the	O
prediction	O
.	O
The	O
2	O
norm	O
is	O
adopted	O
to	O
compute	O
the	O
distance	O
.	O
Denotingv	O
t	O
as	O
the	O
predicted	O
value	O
of	O
slot	O
s	O
at	O
turn	O
t	O
,	O
we	O
have	O
:	O
v	O
t	O
=	O
argmin	O
v	O
Vs	O
g	O
s	O
t	O
−	O
h	O
v	O
2	O
.	O

We	O
leverage	O
a	O
small	O
clean	O
dataset	O
to	O
train	O
the	O
auxiliary	O
model	O
.	O
Since	O
the	O
true	O
labels	O
are	O
available	O
,	O
the	O
auxiliary	O
model	O
is	O
directly	O
trained	O
to	O
maximize	O
the	O
joint	O
probability	O
of	O
all	O
slot	O
values	O
.	O
The	O
probability	O
of	O
the	O
true	O
value	O
v	O
t	O
of	O
slot	O
s	O
at	O
turn	O
t	O
is	O
defined	O
as	O
:	O
p	O
(	O
v	O
t	O
|	O
X	O
t	O
,	O
s	O
)	O
=	O
exp	O
(	O
−	O
g	O
s	O
t	O
−	O
h	O
v	O
t	O
2	O
)	O
v	O
Vs	O
exp	O
(	O
−	O
g	O
s	O
t	O
−	O
h	O
v	O
2	O
)	O
,	O
where	O
h	O
v	O
t	O
is	O
the	O
semantic	O
representation	O
of	O
v	O
t	O
.	O
Maximizing	O
the	O
joint	O
probability	O
Π	O
(	O
s	O
,	O
vt	O
)	O
Bt	O
p	O
(	O
v	O
t	O
|	O
X	O
t	O
,	O
s	O
)	O
is	O
equivalent	O
to	O
minimizing	O
the	O
following	O
objective	O
:	O
L	O
aux	O
=	O
(	O
s	O
,	O
vt	O
)	O
Bt	O
−	O
log	O
p	O
(	O
v	O
t	O
|	O
X	O
t	O
,	O
s	O
)	O
.	O

We	O
also	O
test	O
using	O
the	O
proposed	O
auxiliary	O
model	O
as	O
the	O
primary	O
model	O
.	O
For	O
the	O
sake	O
of	O
description	O
,	O
we	O
refer	O
to	O
this	O
model	O
as	O
AUX	O
-	O
DST	O
.	O

Considering	O
that	O
our	O
proposed	O
framework	O
ASSIST	O
relies	O
on	O
a	O
small	O
clean	O
dataset	O
to	O
train	O
the	O
auxiliary	O
model	O
that	O
is	O
further	O
leveraged	O
to	O
generate	O
pseudo	O
labels	O
for	O
the	O
training	O
set	O
,	O
it	O
is	O
valuable	O
to	O
explore	O
the	O
effects	O
of	O
the	O
size	O
of	O
the	O
clean	O
dataset	O
on	O
the	O
performance	O
of	O
the	O
primary	O
model	O
.	O
For	O
this	O
purpose	O
,	O
we	O
vary	O
the	O
number	O
of	O
dialogues	O
in	O
the	O
clean	O
dataset	O
from	O
500	O
to	O
1000	O
4	O
to	O
generate	O
different	O
pseudo	O
labels	O
.	O
We	O
then	O
combine	O
these	O
different	O
pseudo	O
labels	O
with	O
the	O
vanilla	O
labels	O
to	O
train	O
the	O
primary	O
model	O
AUX	O
-	O
DST	O
.	O
The	O
results	O
on	O
Multi	O
-	O
WOZ	O
2.4	O
are	O
reported	O
in	O
Figure	O
5	O
.	O
For	O
comparison	O
,	O
4	O
There	O
are	O
1000	O
dialogues	O
in	O
total	O
in	O
the	O
validation	O
set	O
.	O
we	O
also	O
include	O
the	O
results	O
when	O
only	O
the	O
pseudo	O
labels	O
or	O
only	O
the	O
vanilla	O
labels	O
are	O
used	O
to	O
train	O
the	O
primary	O
model	O
.	O
As	O
can	O
be	O
seen	O
,	O
the	O
size	O
of	O
the	O
clean	O
dataset	O
has	O
a	O
great	O
impact	O
on	O
the	O
performance	O
of	O
the	O
primary	O
model	O
.	O
Apparently	O
,	O
fewer	O
clean	O
data	O
will	O
lead	O
to	O
worse	O
performance	O
.	O
Nevertheless	O
,	O
as	O
long	O
as	O
the	O
pseudo	O
labels	O
are	O
combined	O
with	O
the	O
vanilla	O
labels	O
,	O
the	O
primary	O
model	O
can	O
consistently	O
demonstrate	O
the	O
strongest	O
performance	O
.	O

The	O
previous	O
experiments	O
have	O
proven	O
the	O
effectiveness	O
of	O
the	O
generated	O
pseudo	O
labels	O
in	O
training	O
robust	O
DST	O
models	O
.	O
In	O
this	O
part	O
,	O
we	O
provide	O
further	O
analyses	O
on	O
the	O
quality	O
of	O
the	O
pseudo	O
labels	O
to	O
gain	O
more	O
insights	O
into	O
why	O
they	O
can	O
be	O
beneficial	O
.	O

To	O
intuitively	O
understand	O
the	O
quality	O
of	O
the	O
pseudo	O
labels	O
,	O
we	O
show	O
four	O
dialogue	O
snippets	O
with	O
their	O
vanilla	O
labels	O
and	O
the	O
generated	O
pseudo	O
labels	O
in	O
Table	O
2	O
.	O
As	O
can	O
be	O
seen	O
,	O
the	O
vanilla	O
labels	O
of	O
the	O
first	O
two	O
dialogue	O
snippets	O
are	O
incomplete	O
,	O
while	O
all	O
the	O
missing	O
information	O
is	O
presented	O
in	O
the	O
pseudo	O
labels	O
.	O
For	O
the	O
third	O
dialogue	O
snippet	O
,	O
the	O
vanilla	O
labels	O
contain	O
an	O
unmentioned	O
slot	O
-	O
value	O
pair	O
"	O
(	O
trainarriveby	O
,	O
13:03	O
)	O
"	O
.	O
This	O
error	O
has	O
also	O
been	O
fixed	O
in	O
the	O
pseudo	O
labels	O
.	O
For	O
the	O
last	O
dialogue	O
snippet	O
,	O
the	O
vanilla	O
labels	O
are	O
correct	O
.	O
However	O
,	O
the	O
pseudo	O
labels	O
introduce	O
an	O
overconfident	O
prediction	O
of	O
the	O
value	O
of	O
slot	O
"	O
hotel	O
-	O
area	O
"	O
.	O
This	O
case	O
study	O
has	O
verified	O
again	O
that	O
the	O
pseudo	O
labels	O
can	O
be	O
utilized	O
to	O
fix	O
certain	O
errors	O
in	O
the	O
vanilla	O
labels	O
.	O
However	O
,	O
the	O
pseudo	O
labels	O
may	O
bring	O
about	O
some	O
new	O
errors	O
.	O
Hence	O
,	O
we	O
should	O
combine	O
the	O
two	O
types	O
of	O
labels	O
so	O
as	O
to	O
achieve	O
the	O
best	O
performance	O
.	O

In	O
this	O
section	O
,	O
we	O
briefly	O
review	O
related	O
work	O
on	O
DST	O
and	O
noisy	O
label	O
learning	O
.	O

Proof	O
.	O
Our	O
proof	O
is	O
based	O
on	O
the	O
bias	O
-	O
variance	O
decomposition	O
theorem	O
5	O
.	O
For	O
any	O
sample	O
X	O
t	O
in	O
the	O
noisy	O
training	O
set	O
D	O
n	O
,	O
the	O
approximation	O
error	O
with	O
respect	O
to	O
the	O
pseudo	O
labelv	O
t	O
of	O
slot	O
s	O
is	O
defined	O
as	O
]	O
,	O
which	O
,	O
according	O
to	O
the	O
biasvariance	O
decomposition	O
theorem	O
,	O
can	O
be	O
decomposed	O
into	O
a	O
bias	O
term	O
and	O
a	O
variance	O
term	O
,	O
i.e.	O
,	O
where	O
In	O
our	O
approach	O
,	O
the	O
auxiliary	O
model	O
is	O
a	O
BERTbased	O
model	O
,	O
which	O
has	O
more	O
than	O
110	O
M	O
parameters	O
.	O
Such	O
a	O
complex	O
model	O
is	O
expected	O
to	O
be	O
able	O
to	O
capture	O
all	O
the	O
samples	O
in	O
the	O
small	O
clean	O
dataset	O
D	O
c	O
.	O
Therefore	O
,	O
we	O
can	O
reasonably	O
assume	O
that	O
the	O
bias	O
term	O
is	O
close	O
to	O
zero	O
.	O
Then	O
,	O
we	O
have	O
:	O

Instead	O
of	O
using	O
only	O
two	O
languages	O
(	O
source	O
and	O
target	O
)	O
for	O
training	O
an	O
NMT	O
model	O
,	O
using	O
multiple	O
languages	O
has	O
been	O
shown	O
to	O
help	O
in	O
low	O
resource	O
scenarios	O
.	O
For	O
example	O
,	O
it	O
might	O
be	O
the	O
case	O
that	O
a	O
certain	O
pair	O
of	O
languages	O
have	O
very	O
little	O
parallel	O
data	O
between	O
them	O
,	O
but	O
there	O
exists	O
a	O
third	O
language	O
with	O
abundant	O
parallel	O
data	O
with	O
the	O
original	O
two	O
languages	O
.	O
This	O
third	O
language	O
acts	O
as	O
a	O
pivot	O
and	O
helps	O
in	O
improving	O
NMT	O
between	O
the	O
two	O
languages	O
(	O
Aharoni	O
et	O
al	O
,	O
2019	O
;	O
Gu	O
et	O
al	O
,	O
2018	O
;	O
Liu	O
et	O
al	O
,	O
2020	O
;	O
Zhang	O
et	O
al	O
,	O
2020	O
)	O
.	O

We	O
use	O
hi	O
↔	O
mr	O
and	O
es	O
↔	O
pt	O
language	O
pairs	O
for	O
our	O
experiments	O
.	O

We	O
thank	O
the	O
IIT	O
Delhi	O
HPC	O
facility	O
3	O
for	O
the	O
computational	O
resources	O
.	O
We	O
are	O
also	O
thankful	O
to	O
Ganesh	O
Ramakrishnan	O
and	O
Pawan	O
Goyal	O
for	O
initial	O
discussions	O
on	O
the	O
project	O
.	O
Parag	O
Singla	O
is	O
supported	O

To	O
perform	O
a	O
systematic	O
comparison	O
of	O
errors	O
across	O
different	O
models	O
,	O
we	O
investigate	O
the	O
predictions	O
based	O
on	O
the	O
following	O
criteria	O
.	O

We	O
analyze	O
the	O
impact	O
of	O
passage	O
length	O
on	O
errors	O
,	O
since	O
this	O
can	O
be	O
an	O
important	O
factor	O
in	O
determining	O
the	O
difficulty	O
of	O
understanding	O
the	O
passage	O
.	O
As	O
seen	O
in	O
Figure	O
1	O
,	O
DocQA	O
performs	O
the	O
best	O
on	O
shorter	O
passages	O
,	O
while	O
R	O
-	O
Net	O
and	O
BiDAF	O
are	O
observed	O
to	O
be	O
better	O
for	O
longer	O
passages	O
.	O
However	O
,	O
there	O
are	O
no	O
systematic	O
error	O
patterns	O
and	O
overall	O
error	O
rates	O
,	O
surprisingly	O
,	O
are	O
not	O
much	O
higher	O
for	O
longer	O
passages	O
.	O
This	O
means	O
that	O
predictions	O
on	O
long	O
passages	O
are	O
almost	O
as	O
good	O
as	O
on	O
short	O
(	O
presumably	O
easier	O
to	O
understand	O
)	O
passages	O
.	O

We	O
also	O
do	O
a	O
similar	O
error	O
analysis	O
for	O
questions	O
of	O
different	O
lengths	O
.	O
Since	O
there	O
are	O
very	O
few	O
questions	O
which	O
have	O
length	O
greater	O
than	O
30	O
,	O
the	O
estimate	O
for	O
range	O
30	O
-	O
34	O
is	O
not	O
very	O
reliable	O
.	O
In	O
Figure	O
2	O
,	O
we	O
observe	O
that	O
the	O
error	O
rate	O
first	O
decreases	O
and	O
then	O
increases	O
for	O
BiDAF	O
,	O
DrQA	O
and	O
DocQA	O
.	O
A	O
plausible	O
explanation	O
for	O
this	O
is	O
that	O
shorter	O
questions	O
contain	O
insufficient	O
information	O
in	O
order	O
to	O
be	O
able	O
to	O
select	O
the	O
correct	O
answer	O
span	O
and	O
can	O
hence	O
be	O
confusing	O
,	O
but	O
it	O
also	O
becomes	O
difficult	O
for	O
end	O
-	O
to	O
-	O
end	O
neural	O
models	O
to	O
learn	O
a	O
good	O
representation	O
when	O
the	O
question	O
becomes	O
longer	O
and	O
syntactically	O
more	O
complicated	O
.	O
However	O
,	O
R	O
-	O
Net	O
has	O
an	O
irregular	O
trend	O
with	O
respect	O
to	O
question	O
length	O
,	O
which	O
is	O
difficult	O
to	O
explain	O
.	O

For	O
answers	O
of	O
varying	O
lengths	O
,	O
the	O
error	O
rates	O
are	O
shown	O
in	O
Figure	O
3	O
.	O
Again	O
,	O
estimates	O
for	O
answers	O
with	O
length	O
>	O
16	O
are	O
not	O
very	O
reliable	O
since	O
data	O
is	O
sparse	O
for	O
high	O
answer	O
lengths	O
.	O
Here	O
,	O
we	O
observe	O
an	O
increasing	O
trend	O
initially	O
and	O
then	O
a	O
slight	O
decrease	O
(	O
bell	O
shape	O
)	O
.	O
This	O
conforms	O
to	O
the	O
hypothesis	O
that	O
shorter	O
answers	O
are	O
easier	O
to	O
predict	O
than	O
longer	O
answers	O
,	O
but	O
only	O
up	O
to	O
a	O
certain	O
answer	O
length	O
(	O
observed	O
to	O
be	O
around	O
7	O
for	O
most	O
models	O
)	O
.	O
The	O
slightly	O
better	O
performance	O
for	O
very	O
long	O
answers	O
is	O
likely	O
due	O
to	O
such	O
answers	O
having	O
a	O
higher	O
chance	O
of	O
being	O
(	O
almost	O
)	O
entire	O
sentences	O
with	O
simpler	O
questions	O
being	O
asked	O
about	O
them	O
.	O

Incorrect	O
answer	O
boundary	O
(	O
longer	O
)	O
:	O
This	O
error	O
category	O
includes	O
those	O
cases	O
where	O
the	O
predicted	O
span	O
is	O
longer	O
than	O
the	O
ground	O
truth	O
answer	O
,	O
but	O
contains	O
the	O
answer	O
.	O
Incorrect	O
answer	O
boundary	O
(	O
shorter	O
)	O
:	O
This	O
error	O
category	O
includes	O
those	O
cases	O
where	O
the	O
predicted	O
span	O
is	O
shorter	O
than	O
the	O
ground	O
truth	O
answer	O
,	O
and	O
is	O
a	O
substring	O
of	O
the	O
answer	O
.	O
Soft	O
Correct	O
:	O
This	O
error	O
category	O
includes	O
those	O
cases	O
where	O
the	O
prediction	O
is	O
actually	O
correct	O
,	O
but	O
due	O
to	O
inclusion	O
/	O
exclusion	O
of	O
certain	O
question	O
terms	O
(	O
such	O
as	O
units	O
)	O
along	O
with	O
the	O
answer	O
,	O
it	O
is	O
deemed	O
incorrect	O
.	O

Multi	O
-	O
Sentence	O
:	O
This	O
error	O
category	O
includes	O
those	O
cases	O
where	O
inference	O
is	O
required	O
to	O
be	O
performed	O
across	O
2	O
or	O
more	O
sentences	O
in	O
the	O
given	O
passage	O
to	O
be	O
able	O
to	O
arrive	O
at	O
the	O
answer	O
,	O
which	O
leads	O
to	O
an	O
incorrect	O
prediction	O
based	O
on	O
only	O
1	O
passage	O
sentence	O
.	O
Paraphrase	O
:	O
This	O
error	O
category	O
includes	O
those	O
cases	O
where	O
the	O
question	O
paraphrases	O
certain	O
parts	O
of	O
the	O
sentence	O
that	O
it	O
is	O
asking	O
about	O
which	O
makes	O
lexical	O
pattern	O
matching	O
difficult	O
and	O
leads	O
to	O
errors	O
in	O
prediction	O
.	O

This	O
error	O
category	O
includes	O
those	O
cases	O
where	O
the	O
question	O
is	O
about	O
an	O
entity	O
type	O
which	O
is	O
present	O
multiple	O
times	O
in	O
the	O
passage	O
and	O
the	O
model	O
returns	O
a	O
different	O
entity	O
than	O
the	O
ground	O
truth	O
entity	O
but	O
of	O
the	O
same	O
type	O
.	O
Requires	O
World	O
Knowledge	O
:	O
This	O
error	O
category	O
includes	O
questions	O
which	O
can	O
not	O
be	O
answered	O
using	O
the	O
given	O
passage	O
alone	O
and	O
require	O
external	O
knowledge	O
to	O
solve	O
,	O
leading	O
to	O
incorrect	O
predictions	O
.	O
Missing	O
Inference	O
:	O
This	O
category	O
includes	O
inference	O
-	O
related	O
errors	O
which	O
do	O
n't	O
belong	O
to	O
any	O
of	O
the	O
other	O
categories	O
mentioned	O
above	O
.	O

We	O
would	O
like	O
to	O
thank	O
Chaitanya	O
Malaviya	O
and	O
Abhishek	O
Chinni	O
for	O
their	O
valuable	O
feedback	O
,	O
and	O
the	O
Language	O
Technologies	O
Institute	O
at	O
CMU	O
for	O
the	O
GPU	O
resources	O
used	O
in	O
this	O
work	O
.	O
We	O
are	O
also	O
very	O
grateful	O
to	O
the	O
anonymous	O
reviewers	O
for	O
their	O
insightful	O
comments	O
and	O
suggestions	O
,	O
which	O
helped	O
us	O
polish	O
the	O
presentation	O
of	O
our	O
work	O
.	O

One	O
of	O
the	O
recent	O
challenges	O
in	O
machine	O
learning	O
(	O
ML	O
)	O
is	O
interpreting	O
the	O
predictions	O
made	O
by	O
models	O
,	O
especially	O
deep	O
neural	O
networks	O
.	O
Understanding	O
models	O
is	O
not	O
only	O
beneficial	O
,	O
but	O
necessary	O
for	O
wide	O
-	O
spread	O
adoption	O
of	O
more	O
complex	O
(	O
and	O
potentially	O
more	O
accurate	O
)	O
ML	O
models	O
.	O
From	O
healthcare	O
to	O
financial	O
domains	O
,	O
regulatory	O
agencies	O
mandate	O
entities	O
to	O
provide	O
explanations	O
for	O
their	O
decisions	O
(	O
Goodman	O
and	O
Flaxman	O
,	O
2016	O
)	O
.	O
Hence	O
,	O
most	O
machine	O
learning	O
progress	O
made	O
in	O
those	O
areas	O
is	O
hindered	O
by	O
a	O
lack	O
of	O
model	O
explainability	O
-	O
causing	O
practitioners	O
to	O
resort	O
to	O
simpler	O
,	O
potentially	O
low	O
-	O
performance	O
models	O
.	O
To	O
supply	O
for	O
this	O
demand	O
,	O
there	O
has	O
been	O
many	O
attempts	O
for	O
model	O
interpretation	O
in	O
recent	O
years	O
for	O
tree	O
-	O
based	O
algorithms	O
(	O
Lundberg	O
et	O
al	O
,	O
2018	O
)	O
and	O
deep	O
learning	O
algorithms	O
(	O
Lundberg	O
and	O
Lee	O
,	O
2017	O
;	O
Smilkov	O
et	O
al	O
,	O
2017	O
;	O
Sundararajan	O
et	O
al	O
,	O
2017	O
;	O
Bach	O
et	O
al	O
,	O
2015	O
;	O
Dhurandhar	O
et	O
al	O
,	O
2018	O
)	O
.	O

Integrated	O
Gradients	O
(	O
Sundararajan	O
et	O
al	O
,	O
2017	O
)	O
is	O
a	O
model	O
attribution	O
technique	O
applicable	O
to	O
all	O
models	O
that	O
have	O
differentiable	O
inputs	O
w.r.t	O
.	O
outputs	O
.	O
IG	O
produces	O
feature	O
attributions	O
relative	O
to	O
an	O
uninformative	O
baseline	O
.	O
This	O
baseline	O
input	O
is	O
designed	O
to	O
produce	O
a	O
high	O
-	O
entropy	O
prediction	O
representing	O
uncertainty	O
.	O
IG	O
,	O
then	O
,	O
interpolates	O
the	O
baseline	O
towards	O
the	O
actual	O
input	O
,	O
with	O
the	O
prediction	O
moving	O
from	O
uncertainty	O
to	O
certainty	O
in	O
the	O
process	O
.	O
Building	O
on	O
the	O
notion	O
that	O
the	O
gradient	O
of	O
a	O
function	O
,	O
f	O
,	O
with	O
respect	O
to	O
input	O
can	O
characterize	O
sensitivity	O
of	O
f	O
for	O
each	O
input	O
dimension	O
,	O
IG	O
simply	O
aggregates	O
the	O
gradients	O
of	O
f	O
with	O
respect	O
to	O
the	O
input	O
along	O
this	O
path	O
using	O
a	O
path	O
integral	O
.	O
The	O
crux	O
of	O
using	O
path	O
integral	O
rather	O
than	O
overall	O
gradient	O
at	O
the	O
input	O
is	O
that	O
f	O
's	O
gradients	O
might	O
have	O
been	O
saturated	O
around	O
the	O
input	O
and	O
integrating	O
over	O
a	O
path	O
alleviates	O
this	O
phenomenon	O
.	O
Even	O
though	O
there	O
can	O
be	O
infinitely	O
many	O
paths	O
from	O
a	O
baseline	O
to	O
input	O
point	O
,	O
Integrated	O
Gradients	O
takes	O
the	O
straight	O
path	O
between	O
the	O
two	O
.	O
We	O
give	O
the	O
formal	O
definition	O
from	O
the	O
original	O
paper	O
in	O
2.2	O
.	O
Definition	O
2.2	O
.	O
Given	O
an	O
input	O
x	O
and	O
baseline	O
x	O
,	O
the	O
integrated	O
gradient	O
along	O
the	O
i	O
th	O
dimension	O
is	O
defined	O
as	O
follows	O
.	O
IG	O
i	O
(	O
x	O
,	O
x	O
)	O
:	O
:	O
=	O
(	O
x	O
i	O
−	O
x	O
i	O
)	O
×	O
1	O
α=0	O
∂f	O
(	O
x	O
+	O
α×	O
(	O
x−x	O
)	O
)	O
∂x	O
i	O
dα	O
(	O
1	O
)	O
where	O
∂f	O
(	O
x	O
)	O
∂x	O
i	O
represents	O
the	O
gradient	O
of	O
f	O
along	O
the	O
i	O
th	O
dimension	O
at	O
x.	O
In	O
the	O
NLP	O
setting	O
,	O
x	O
is	O
the	O
concatenated	O
embedding	O
of	O
the	O
input	O
sequence	O
.	O
The	O
attribution	O
of	O
each	O
token	O
is	O
the	O
sum	O
of	O
the	O
attributions	O
of	O
its	O
embedding	O
.	O
There	O
are	O
other	O
explainability	O
methods	O
that	O
attribute	O
a	O
model	O
's	O
decision	O
to	O
its	O
features	O
,	O
but	O
we	O
chose	O
IG	O
in	O
this	O
framework	O
due	O
to	O
several	O
of	O
its	O
characteristics	O
.	O
First	O
,	O
it	O
is	O
both	O
theoretically	O
justified	O
(	O
Sundararajan	O
et	O
al	O
,	O
2017	O
)	O
and	O
proven	O
to	O
be	O
effective	O
in	O
NLP	O
-	O
related	O
tasks	O
(	O
Mudrakarta	O
et	O
al	O
,	O
2018	O
)	O
.	O
Second	O
,	O
the	O
IG	O
formula	O
in	O
2.2	O
is	O
differentiable	O
everywhere	O
with	O
respect	O
to	O
model	O
parameters	O
.	O
Lastly	O
,	O
it	O
is	O
lightweight	O
in	O
terms	O
of	O
implementation	O
and	O
execution	O
complexity	O
.	O

Base	O

We	O
thank	O
Salem	O
Haykal	O
,	O
Ankur	O
Taly	O
,	O
Diego	O
Garcia	O
-	O
Olano	O
,	O
Raz	O
Mathias	O
,	O
and	O
Mukund	O
Sundararajan	O
for	O
their	O
valuable	O
feedback	O
and	O
insightful	O
discussions	O
.	O

