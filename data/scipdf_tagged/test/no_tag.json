{"text": "In this paper , we present an improved graph - based translation model which segments an input graph into node - induced subgraphs by taking source context into consideration . Translations are generated by combining subgraph translations leftto - right using beam search . Experiments on Chinese - English and German - English demonstrate that the context - aware segmentation significantly improves the baseline graph - based model .", "entities": []}
{"text": "The well - known phrase - based statistical translation model ( Koehn et al , 2003 ) extends the basic translation units from single words to continuous phrases to capture local phenomena . However , one of its significant weaknesses is that it can not learn generalizations Galley and Manning , 2010 ) . To allow discontinuous phrases ( any subset of words of an input sentence ) , dependency treelets Xiong et al , 2007 ) can be used , which are connected subgraphs on trees . However , continuous phrases which are not connected on trees and thus excluded could in fact be extremely important to system performance ( Koehn et al , 2003 ; Hanneman and Lavie , 2009 ) . To make use of the merits of both phrase - based models and treelet - based models , Li et al ( 2016 ) proposed a graph - based translation model as in Equation ( 1 ) : p ( t I 1 | g I 1 ) = I i=1 p ( t i | g a i ) \u00d7 d ( g a i , g a i\u22121 ) ( 1 ) where t i is a continuous target phrase which is the translation of a node - induced and connected source subgraph g a i . 1 d is a distance - based reordering function which penalizes discontinuous phrases that have relatively long gaps ( Galley and Manning , 2010 ) . The model translates an input graph by segmenting it into subgraphs and generates a complete translation by combining subgraph translations left - to - right . However , the model treats different graph segmentations equally . Therefore , in this paper we propose a contextaware graph segmentation ( Section 2 ) : ( i ) we add contextual information to each translation rule during training ( Section 2.2 ) ; ( ii ) during decoding , when a rule is applied , the input context should match with the rule context ( Section 2.3 ) . Experiments ( Section 3 ) on Chinese - English ( ZH - EN ) and German - English ( DE - EN ) tasks show that our method significantly improves the graphbased model . As observed in our experiments , the context - aware segmentation brings two benefits to our system : ( i ) it helps to select a better subgraph to translate ; and ( ii ) it selects a better target phrase for a subgraph .", "entities": []}
{"text": "Our model extends the graph - based translation model by considering source context during segmenting input graphs , as in Equation ( 2 ) : p ( t I 1 | g I 1 ) = I i=1 p ( t i | g a i , c a i ) \u00d7 d ( g a i , g a i\u22121 ) ( 2 ) where c a i denotes the context of the subgraph g a i , which is represented as a set of connections ( i.e. edges ) between g a i and [ g a i+1 , , g a I ] . 2010Nian FIFA Shijiebei Zai Nanfei Chenggong Juxing", "entities": []}
{"text": "The graph used in this paper combines a sequence and a dependency tree as in Li et al ( 2016 ) . Each graph contains two kinds of links : dependency links from dependency trees which model syntactic and semantic relations between words , and bigram links which provide local and sequential information on pairs of continuous words . Figure 1 shows an example graph . Given such graphs , we can make use of both continuous and linguistically informed discontinuous phrases as long as they are connected on graphs . In this paper , we do not distinguish the two kinds of relations , because our preliminary experiments showed no improvement when considering edge types .", "entities": []}
{"text": "Following Li et al ( 2016 ) , we define our model in the well - known log - linear framework ( Och and Ney , 2002 ) . In our experiments , we use the following standard features : two translation probabilities p ( g , c | t ) and p ( t | g , c ) , two lexical translation probabilities p lex ( g , c | t ) and p lex ( t | g , c ) , a language model p ( t ) , a rule penalty , a word penalty , and a distortion function as defined in Galley and Manning ( 2010 ) . In addition , we add one more feature into our system : a basic - rule penalty to distinguish basic rules from segmenting and selecting rules . Our decoder is very similar to the one in the conventional graph - based model , which generates hypotheses left - to - right using beam search . A hypothesis can be extended on the right by translating an uncovered source subgraph . The translation process ends when all source words have been translated . However , when extending a hypothesis , our decoder considers the context of the translated subgraph , i.e. edges connecting it with the remaining untranslated source words . Figure 2 shows a derivation which translates an input graph in Chinese to an English string . In this example , both rules r 1 and r 2 are segmenting rules .", "entities": []}
{"text": "We conduct experiments on ZH - EN and DE - EN corpora .", "entities": []}
{"text": "ZhiLi BaoHu He GaiShan JuZhu HuanJing . Ref : GBMT : GBMTctx : we are also committed to protect and improve our living environment . we have worked hard to protect and improve the living environment . we are also committed to protect and improve the living environment . ( b ) target - phrase selection Bold figures mean a system is significantly better than the one only using basic rules at p \u2264 0.01 . the number of segmenting rules is much larger than the number of selecting rules . We further observed that , while our system achieves the best performance when all rules are used on ZH - EN , the combination of basic rules and segmenting rules on DE - EN results in the best system . This is probably because reordering ( including long - distance reordering ) is performed less often in DE - EN than in ZH - EN ( Li et al , 2016 ) which makes selecting rules less preferable on DE - EN .", "entities": []}
{"text": "In this paper , we present a graph - based model which takes subgraphs as the basic translation units and considers source context during segmenting graphs into subgraphs . Experiments on Chinese - English and German - English show that our model is significantly better than the conventional graphbased model which equally treats different graph segmentations . In this paper , source context is used as hard constraints during decoding . In future , we would like to try soft constraints . In addition , it would also be interesting to extend this model using a synchronous graph grammar .", "entities": []}
{"text": "This research has received funding from the European Union 's Horizon 2020 research and innovation programme under grant agreement n o 645452 ( QT21 ) . The ADAPT Centre for Digital Content Technology is funded under the SFI Research Centres Programme ( Grant 13 / RC/2106 ) and is cofunded under the European Regional Development Fund . The authors thank all anonymous reviewers for their insightful comments and suggestions .", "entities": []}
{"text": "We consider retrieval - based QA systems , which are mainly constituted by ( i ) a search engine , retrieving documents related to the questions ; and ( ii ) an AS2 model , which reranks passages / sentences extracted from the documents . The top sentence is typically used as final answer for the users .", "entities": []}
{"text": "In this section , we describe our baseline models , which are constituted by pointwise , pairwise , and listwise strategies .", "entities": []}
{"text": "To better show the potential of our approach and the complexity of the task , we designed three joint model baselines based on : ( i ) a multiclassifier approach ( a listwise method ) , and ( ii ) a pairwise joint model operating over k + 1 candidates , and our adaptation of KGAT model ( a pairwise method ) .", "entities": []}
{"text": "We proposed the Answer Support Reranker ( ASR ) , which uses an answer pair classifier to provide evidence to a target answer t. Given a question q , and a subset of its top - k+1 ranked answer candidates , A ( reranked by an AS2 model ) , we build a function , \u03c3 : Q \u00d7 C \u00d7 C k R such that \u03c3 ( q , t , A \\ { t } ) provides the probability of t to be correct , where C is the set of sentence - candidates . We also design a multi - classifier MASR , which combines k ASR models , one for each different target answer .", "entities": []}
{"text": "In these experiments , we compare our models : KGAT , ASR and MASR with pointwise models , which are the state of the art for AS2 . We also compare them with our joint model baselines ( pairwise and listwise ) . Finally , we provide an error analysis .", "entities": []}
{"text": "Compact - answer Representation", "entities": []}
{"text": "Generative adversarial networks ( GANs ) ( Goodfellow et al , 2014 ) are a framework for training generative models based on game theory . Unlike the original GANs , which generate such data samples as images and compact answers from noise , our AGR generates useful vector representations from meaningful text passages . To clarify the difference , we explain our AGR with three subnetworks : two generators , F and R , and a discriminator , D , as in Fig . 1 ( b ) . Generator F takes as input passage p drawn from prior passage distribution d p and outputs vectorp as a fake representation of a compact answer . We call F a fake - representation generator . R , which we call a real - representation generator , is given sample c taken from manually created compact - answers and provides vector c as a real - representation of the sampled compactanswer . Discriminator D has to distinguish fakerepresentationp from real - representationc of a compact answer . These three networks play an adversarial minimax game ; fake - representation generator F creates a fake compact - answer representation that is hard for the discriminator to distinguish from the representations of manually created compact - answers , and discriminator D and generator R simultaneously try to avoid being duped by generator F . These processes should allow generator F to learn how to generate a representation of a proper compact - answer from an answer passage . In addition , since passage p and compact answer c are dependent on question q , the generation of the compact - answer representations by F and R is conditioned by question q , like in the conditional GANs ( Mirza and Osindero , 2014 ) . We trained our AGR with the following minimax objective : min F max D , R V ( D , F , R ) = E c\u223cdc ( c ) [ log D ( R ( c | q ) ) ] + E p\u223cdp ( p ) [ log ( 1 \u2212 D ( F ( p | q ) ) ] .", "entities": []}
{"text": "Are Training Samples Correlated ? Learning to Generate Dialogue Responses with Multiple References", "entities": []}
{"text": "In this paper , we tackle the one - to - many queryresponse mapping problem in open - domain conversation and propose a novel two - step generation architecture with the correlation of multiple valid responses considered . Jointly viewing the multiple responses as a response bag , the model extracts the common and distinct features of different responses in two generation phases respectively to output multiple diverse responses . Experimental results illustrate the superior performance of the proposed model in generating diverse and appropriate responses compared to previous representative approaches . However , the modeling of the common and distinct features of responses in our method is currently implicit and coarse - grained . Directions of future work may be pursuing betterdefined features and easier training strategies .", "entities": []}
{"text": "We would like to thank the anonymous reviewers for their constructive comments . This work was supported by the National Key Research and Development Program of China ( No . 2017YFC0804001 ) , the National Science Foundation of China ( NSFC No . 61672058 ; NSFC No . 61876196 ) . Rui Yan was sponsored by CCF - Tencent Open Research Fund and Alibaba Innovative Research ( AIR ) Fund .", "entities": []}
{"text": "SGNMT - A Flexible NMT Decoding Platform for Quick Prototyping of New Models and Search Strategies", "entities": []}
{"text": "SGNMT allows combining any number of predictors and even multiple instances of the same predictor type . In case of multiple predictors we combine the predictor scores in a linear model . The following list illustrates that various interesting decoding tasks can be formulated as predictor combinations . nmt : A single NMT predictor represents pure NMT decoding . nmt , nmt , nmt : Using multiple NMT predictors is a natural way to represent ensemble decoding ( Hansen and Salamon , 1990 ; in our framework . fst , nmt : NMT decoding constrained to an FST . This can be used for neural lattice rescoring or other kinds of constraints , for example in the context of source side simplification in MT or chord progressions in ' Bach ' ( Tomczak , 2016 ) . The fst predictor can also be used to restrict the output of character - based or subword - unit - based NMT to a large word - level vocabulary encoded as FSA . nmt , rnnlm , srilm , nplm : Combining NMT with three kinds of language models : An RNNLM ( Zaremba et al , 2014 ) , a Kneser - Ney n - gram LM ( Heafield et al , 2013 ; Stolcke et al , 2002 ) , and a feedforward neural network LM ( Vaswani et al , 2013 ) .", "entities": []}
{"text": "This paper presented our SGNMT platform for prototyping new approaches to MT which involve both neural and symbolic models . SGNMT supports a number of different models and constraints via a common interface ( predictors ) , and various search strategies ( decoders ) . Furthermore , SGNMT focuses on minimizing the implementation effort for adding new predictors and decoders by decoupling scoring modules from each other and from the search algorithm . SGNMT is actively being used for teaching and research and we welcome contributions to its development , for example by implementing new predictors for using models trained with other frameworks and tools .", "entities": []}
{"text": "This work was supported by the U.K. Engineering and Physical Sciences Research Council ( EPSRC grant EP / L027623/1 ) .", "entities": []}
{"text": "Double Perturbation : On the Robustness of Robustness and Counterfactual Bias Evaluation", "entities": []}
{"text": "Robustness and counterfactual bias are usually evaluated on a test dataset . However , are these evaluations robust ? If the test dataset is perturbed slightly , will the evaluation results keep the same ? In this paper , we propose a \" double perturbation \" framework to uncover model weaknesses beyond the test dataset . The framework first perturbs the test dataset to construct abundant natural sentences similar to the test data , and then diagnoses the prediction change regarding a single - word substitution . We apply this framework to study two perturbation - based approaches that are used to analyze models ' robustness and counterfactual bias in English . ( 1 ) For robustness , we focus on synonym substitutions and identify vulnerable examples where prediction can be altered . Our proposed attack attains high success rates ( 96.0 % - 99.8 % ) in finding vulnerable examples on both original and robustly trained CNNs and Transformers . ( 2 ) For counterfactual bias , we focus on substituting demographic tokens ( e.g. , gender , race ) and measure the shift of the expected prediction among constructed sentences . Our method is able to reveal the hidden model biases not directly shown in the test dataset . Our code is available at https://github.com/chong - z/ nlp - second - order - attack .", "entities": []}
{"text": "In this section , we describe the double perturbation framework which focuses on identifying vulnerable examples within a small neighborhood of the test dataset . The framework consists of a neighborhood perturbation and a word substitution . We start with defining word substitutions .", "entities": []}
{"text": "With the proposed double perturbation framework , we design two black - box attacks 1 to identify vulnerable examples within the neighborhood of the test set . We aim at evaluating the robustness for inputs beyond the test set .", "entities": []}
{"text": "In this section , we evaluate the second - order robustness of existing models and show the quality of our constructed vulnerable examples .", "entities": []}
{"text": "We", "entities": []}
{"text": "We perform human evaluation on the examples constructed by SO - Beam . Specifically , we randomly", "entities": []}
{"text": "In addition to evaluating second - order robustness , we further extend the double perturbation framework ( 2 ) to evaluate counterfactual biases by setting p to pairs of protected tokens . We show that our method can reveal the hidden model bias .", "entities": []}
{"text": "In this section , we use gender bias as a running example , and demonstrate the effectiveness of our method by revealing the hidden model bias . We provide additional results in Appendix A.4 .", "entities": []}
{"text": "First - order robustness evaluation . A line of work has been proposed to study the vulnerability of natural language models , through transformations such as character - level perturbations ( Ebrahimi et al , 2018 ) , word - level perturbations ( Jin et al , 2019 ; Ren et al , 2019 ; Cheng et al , 2020 ; Li et al , 2020 ) , prepending or appending a sequence ( Jia and Liang , 2017 ; Wallace et al , 2019a ) , and generative models ( Zhao et al , 2018b ) . They focus on constructing adversarial examples from the test set that alter the prediction , whereas our methods focus on finding vulnerable examples beyond the test set whose prediction can be altered . Robustness beyond the test set . Several works have studied model robustness beyond test sets but mostly focused on computer vision tasks . Zhang et al ( 2019 ) demonstrate that a robustly trained model could still be vulnerable to small perturbations if the input comes from a distribution only slightly different than a normal test set ( e.g. , images with slightly different contrasts ) . Hendrycks and Dietterich ( 2019 ) study more sources of common corruptions such as brightness , motion blur and fog . Unlike in computer vision where simple image transformations can be used , in our natural language setting , generating a valid example beyond test set is more challenging because language semantics and grammar must be maintained . Counterfactual fairness . Kusner et al ( 2017 ) propose counterfactual fairness and consider a model fair if changing the protected attributes does not affect the distribution of prediction . We follow the definition and focus on evaluating the counterfactual bias between pairs of protected tokens . Existing literature quantifies fairness on a test dataset or through templates ( Feldman et al , 2015 ; Kiritchenko and Mohammad , 2018 ; May et al , 2019 ; . For instance , Garg et al ( 2019 ) quantify the absolute counterfactual token fairness gap on the test set ; Prabhakaran et al ( 2019 ) study perturbation sensitivity for named entities on a given set of corpus . Wallace et al ( 2019b ) ; Sheng et al ( 2019Sheng et al ( , 2020 study how language generation models respond differently to prompt sentences containing mentions of different demographic groups . In contrast , our method quantifies the bias on the constructed neighborhood .", "entities": []}
{"text": "This work proposes the double perturbation framework to identify model weaknesses beyond the test dataset , and study a stronger notion of robustness and counterfactual bias . We hope that our work can stimulate the research on further improving the robustness and fairness of natural language models .", "entities": []}
{"text": "We thank anonymous reviewers for their helpful feedback . We thank UCLA - NLP group for the valuable discussions and comments . The research is supported NSF # 1927554 , # 1901527 , # 2008173 and # 2048280 and an Amazon Research Award .", "entities": []}
{"text": "Original 54 % Positive ( 69 % Positive ) for the most part , director anne - sophie birot 's first feature is a sensitive , overly ( extraordinarily ) well - acted drama . Vulnerable 53 % Negative ( 62 % Positive ) for the most part , director anne - sophie benoit 's first feature is a sensitive , overly ( extraordinarily ) well - acted drama . Original 73 % Negative ( 56 % Negative ) the cold ( colder ) turkey would ' ve been a far better title . Vulnerable 61 % Negative ( 62 % Positive ) the cold ( colder ) turkey might ' ve been a far better title .", "entities": []}
{"text": "70 % Negative ( 65 % Negative ) it 's just disappointingly superficial - a movie that has all the elements necessary to be a fascinating , involving character study , but never does more than scratch the shallow ( surface ) . Vulnerable 52 % Negative ( 55 % Positive ) it 's just disappointingly short - a movie that has all the elements necessary to be a fascinating , involving character study , but never does more than scratch the shallow ( surface ) . Original 79 % Negative ( 72 % Negative ) schaeffer has to find some hook on which to hang his persistently useless movies , and it might as well be the resuscitation ( revival ) of the middleaged character . Vulnerable 57 % Negative ( 57 % Positive ) schaeffer has to find some hook on which to hang his persistently entertaining movies , and it might as well be the resuscitation ( revival ) of the middleaged character . Original 64 % Positive ( 58 % Positive ) the primitive force of this film seems to bubble up from the vast collective memory of the combatants ( militants ) . Vulnerable 52 % Positive ( 53 % Negative ) the primitive force of this film seems to bubble down from the vast collective memory of the combatants ( militants ) . Original 64 % Positive ( 74 % Positive ) on this troublesome ( tricky ) topic , tadpole is very much a step in the right direction , with its blend of frankness , civility and compassion . Vulnerable 55 % Negative ( 56 % Positive ) on this troublesome ( tricky ) topic , tadpole is very much a step in the right direction , losing its blend of frankness , civility and compassion .", "entities": []}
{"text": "We would like to thank the anonymous reviewers for their helpful comments . This research is partially supported by Ministry of Education , Singapore , under its Academic Research Fund ( AcRF ) Tier 2 Programme ( MOE AcRF Tier 2 Award No : MOE2017 - T2 - 1 - 156 ) . Any opinions , findings and conclusions or recommendations expressed in this material are those of the authors and do not reflect the views of the Ministry of Education , Singapore .", "entities": []}
{"text": "Deep learning for language understanding of mental health concepts derived from Cognitive Behavioural Therapy", "entities": []}
{"text": "While our main emphasis was on thinking errors and emotions , we also defined a small set of situations . The list of situations again evolved during the early days of annotation , with a longer original list being reduced down , for simplicity . Again , it is possible for more than one situation ( for example Work and Relationships ) to apply to a single problem . The considered situations are given in Table 3 .", "entities": []}
{"text": "This work was funded by EPSRC project Natural speech Automated Utility for Mental health ( NAUM ) , award reference EP / P017746/1 . The authors would also like to thank anonymous reviewers for their valuable comments . The code is available at https://github.com/YinpeiDai/NAUM", "entities": []}
{"text": "This work was made possible in part through support of the United States Agency for International Development . The opinions expressed herein are those of the authors and do not necessarily reflect the views of the United States Agency for International Development or the US Government .", "entities": []}
{"text": "Will I Sound Like Me ? Improving Persona Consistency in Dialogues through Pragmatic Self - Consciousness", "entities": []}
{"text": "In the study of dialogue agents , consistency has been a long - standing issue . To resolve this , much research has been conducted to endow dialogue agents with personas . Li et al ( 2016 ) propose to encode persona in embeddings and Zhang et al ( 2018 ) introduce a persona - conditioned dialogue dataset . On top of these works , many efforts have been made to improve consistency . In spite of such recent significant progress , there is much room for improving persona - based dialogue agents . We observe that even the best performing persona - based generative models ( See et al , 2019 ; Wolf et al , 2019b ; I like to stay at home .", "entities": []}
{"text": "We would like to thank Reuben Cohn - Gordon , Sean Welleck , Junhyug Noh and Jiwan Chung for their valuable comments . We also thank the anonymous reviewers for their thoughtful suggestions on this work .", "entities": []}
{"text": "Analogical Reasoning on Chinese Morphological and Semantic Relations", "entities": []}
{"text": "Morphology concerns the internal structure of words . There is a common belief that Chinese is a morphologically impoverished language since a morpheme mostly corresponds to an orthographic character , and it lacks apparent distinctions between roots and affixes . However , Packard ( 2000 ) suggests that Chinese has a different morphological system because it selects different \" settings \" on parameters shared by all languages . We will clarify this special system by mapping its morphological analogies into two processes : reduplication and semi - affixation .", "entities": []}
{"text": "Reduplication means a morpheme is repeated to form a new word , which is semantically and/or syntactically distinct from the original morpheme , e.g. the word \" ti\u0101n - ti\u0101n \" ( day day ) in Figure 1 ( b ) means \" everyday \" . By analyzing all the word categories in Chinese , we find that nouns , verbs , adjectives , adverbs , and measure words have reduplication abilities . Given distinct morphemes A and B , we summarize 6 repetition patterns in Figure 2 . Each pattern may have one or more morphological functions . Taking Pattern 1 ( A AA ) as an example , noun morphemes could form kinship terms or yield every / each meaning . For verbs , it signals doing something a little bit or things happen briefly . AA reduplication could also intensify an adjective or transform it to an adverb . b\u00e0 ( dad ) b\u00e0 - b\u00e0 ( dad ) ti\u0101n ( day ) ti\u0101n - ti\u0101n ( everyday ) shu\u014d ( say ) shu\u014d - shuo ( say a little ) k\u00e0n ( look ) k\u00e0n - k\u00e0n ( have a brief look ) d\u00e0 ( big ) d\u00e0 - d\u00e0 ( very big ; greatly ) sh\u0113n ( deep ) sh\u0113n - sh\u0113n ( deeply ) 1 https://github.com/Embedding/Chinese - Word - Vectors", "entities": []}
{"text": "Affixation is a morphological process whereby a bound morpheme ( an affix ) is attached to roots or stems to form new language units . Chinese is a typical isolating language that has few affixes . Liu et al ( 2001 ) points out that although affixes are rare in Chinese , there are some components behaving like affixes and can also be used as independent lexemes . They are called semi - affixes . To model the semi - affixation process , we uncover 21 semi - prefixes and 41 semi - suffixes . These semi - suffixes can be used to denote changes of meaning or part of speech . For example , the semi - prefix \" d\u00ec - \" could be added to numerals to form ordinal numbers , and the semi - suffix \" - zi \" is able to nominalize an adjective : y\u012b ( one ) d\u00ec - y\u012b ( first ) \u00e8r ( two ) d\u00ec - \u00e8r ( second ) p\u00e0ng ( fat ) p\u00e0ng - zi ( a fat man ) sh\u00f2u ( thin ) sh\u00f2u - zi ( a thin man )", "entities": []}
{"text": "To investigate semantic knowledge reasoning , we present 28 semantic relations in four aspects : geography , history , nature , and people . Among them we inherit a few relations from English datasets , e.g. country - capital and family members , while the rest of them are proposed originally on the basis of our observation of Chinese lexical knowledge . For example , a Chinese province may have its own abbreviation , capital city , and representative drama , which could form rich semantic analogies : \u0101n - hu\u012b vs zh\u00e8 - ji\u0101ng ( province ) w\u01cen vs zh\u00e8 ( abbreviation ) h\u00e9 - f\u00e9i vs h\u00e1ng - zh\u014du ( capital ) hu\u00e1ng - m\u00e9i - x\u00ec vs yu\u00e8 - j\u00f9 ( drama ) We also address novel relations that could be used for other languages , e.g. scientists and their findings , companies and their founders .", "entities": []}
{"text": "This work is supported by the Fundamental Research Funds for the Central Universities , National Natural Science Foundation of China with Grant ( No.61472428 ) and Chinese Testing International Project ( No . CTI2017B12 ) .", "entities": []}
{"text": ". 277 .491 .625 .175 .199 .134 .251 .189 .146 .147 .250 .189 .181 Combination 15.9 G .872 .994 .710 .223 .300 .234 .518 .321 .662 .293 .310 .307 .467", "entities": []}
{"text": "The 1992 Los Angeles riots , also known as the Rodney King riots were a series of riots , lootings , arsons , and civil disturbances that occurred in Los Angeles County , California in April and May 1992 . [ wiki / Los Angeles County ] Los Angeles County , officially the County of Los Angeles , is the most populous county in the USA . assign a label whether , given the evidence , the claim is SUPPORTED , REFUTED or whether there is NOTENOUGHINFO in Wikipedia to reach a conclusion . In 16.82 % of cases , claims required the combination of more than one sentence as supporting or refuting evidence . An example is provided in Figure 1 .", "entities": []}
{"text": "The first Fact Extraction and VERification shared task attracted submissions from 86 submissions from 23 teams . 19 of these teams exceeded the score of the baseline presented in Thorne et al ( 2018 ) . For the teams which provided a system description , we highlighted the approaches , identifying commonalities and features that could be further explored . Future work will address limitations in humanannotated evidence and explore other subtasks needed to predict the veracity of information extracted from untrusted sources .", "entities": []}
{"text": "Our system was developed using a heuristicsbased approach for evidence extraction and a modified version of the inference model by Parikh et al ( 2016 ) for classification into refute , support , or not enough info . Our process is broken down into three distinct phases . First , potentially relevant documents are gathered based on key words / phrases in the claim that appear in the wiki dump . Second , any possible evidence sentences inside those documents are extracted by breaking down the claim into named entities plus nouns and finding any sentences which match those entities , while allowing for various exceptions and additional potential criteria to increase recall . Finally , every sentences is classified into one of the three categories by the inference tool , after additional vectors are added based on named entity types . NEI sentences are discarded and the highest scored label of the remaining sentences is assigned to the claim .", "entities": []}
{"text": "In our approach we used a sentence wise approach in all components . To find the sentences we set up a Solr database and indexed every sentence including information about the article where the sentence is from . We created queries based on the named entities and noun chunks of the claims . For the entailment task we used a Decomposable Attention Model similar to the one used in the baseline approach . But instead of comparing the claim with all top 5 sentences at once we treat every sentence separately . The results for the top 5 sentence where then joined with an ensemble learner incl . the rank of the sentence retriever of the wikipedia sentences .", "entities": []}
{"text": "You Do n't Know My Favorite Color : Preventing Dialogue Representations from Revealing Speakers ' Private Personas", "entities": []}
{"text": "In this section , we conduct experiments to evaluate the performance of privacy and utility for the proposed defense learning strategies . In Section 5.1 , we give our experimental settings in detail . In Section 5.2 , we show the attacking performance with and without defense . In Section 5.3 , we perform ablation study on defense objectives . In Section 5.4 , we use automatic metrics to evaluate chatbots ' utility . We conduct various attack setups in Section 5.5 and perform a case study in Section 5.6 .", "entities": []}
{"text": "To show an intuition view on utility , we provide one generation sample shown in Figure 5 . Both LM and LM+KL+MI are able to generate fluent and proper relies . Moreover , they tend to maintain coherence with previous contexts . For example , it is mentioned in the context that Human B is a vegan and both chatbots respond that they do not eat meat for the food preference . This generation example shows that proposed defense learning objectives preserve the model utility .", "entities": []}
{"text": "Here , we give two more examples of the persona inference attacks in Table 6 . The first example shows one successful defense . For the second example , both attackers with and without defense fail to predict the ground truth persona . Still , we can see that LM+KL+MI predicts personas that are irrelevant to the context . However , LM 's output \" I know how to play the guitar . \" is much closer to the context about music and instruments . Without any defense , the above examples show that the attacker model can still predict context - aware personas even if its predictions are wrong . After applying the proposed defenses , the attacker model can not predict meaningful personas relevant to the context .", "entities": []}
{"text": "The authors of this paper were supported by the NSFC Fund ( U20B2053 ) from the NSFC of China , the RIF ( R6020 - 19 and R6021 - 20 ) and the GRF ( 16211520", "entities": []}
{"text": "Alignment verification to improve NMT translation towards highly inflectional languages with limited resources", "entities": []}
{"text": "The present article studies translation quality when limited training data is available to translate towards morphologically rich languages . The starting point is a neural MT system , used to train translation models with only publicly available parallel data . An initial analysis of the translation output has shown that quality is sub - optimal , mainly due to the insufficient amount of training data . To improve translation , a hybridized solution is proposed , using an ensemble of relatively simple NMT systems trained with different metrics , combined with an open source module designed for low - resource MT that measures the alignment level . A quantitative analysis based on established metrics is complemented by a qualitative analysis of translation results . These show that over multiple test sets , the proposed hybridized method confers improvements over ( i ) both the best individual NMT and ( ii ) the ensemble system provided in the Marian - NMT package . Improvements over Marian - NMT are in many cases statistically significant .", "entities": []}
{"text": "Verification Method ( AVM )", "entities": []}
{"text": "PAM utilizes a limited - size bilingual lexicon ( of typically 30 to 40 thousand token pairs ) together with a publicly available parser . Details on these resources are reported in section 4.4 , as their choices are language - specific . Based on these resources , PAM establishes for the set of parallel sentences the alignment of both words and phrases from SL to TL , in three hierarchically ordered stages : 1 . Within the first stage , the alignment of words is based on equivalences provided by the bilingual lexicon . Dedicated PAM processes resolve cases where ( i ) words have multiple appearances within a sentence and ( ii ) multiple potential translations of an SL word exist in the TL side . 2 . Within the second stage , words are aligned by establishing statistical correspondences between grammatical features across the SL and TL pair . These correspondences are automatically extracted from the lexicon . 3 . Within the third stage , any remaining words are aligned and grouped into phrases on the basis of the alignments of their neighboring words that are successfully aligned . To implement this , the principle of locality across languages is adopted ( words at a small distance to each other in SL also tend to be located close to each other in TL ) . The key PAM principle is that decisions made at a later stage have a lower degree of confidence than those made at an earlier stage ( Troullinos , 2013 ) .", "entities": []}
{"text": "Examples Label P : \u1102 \u1165\u1102 \u1173 \u11ab \u1100 \u1165\u1100 \u1175\u110b \u1166 \u110b \u1175 \u11bb\u110b \u1173 \u11af \u1111 \u1175 \u11af\u110b \u116d \u110b \u1165 \u11b9\u110b \u1165 . E \" You do n't have to stay there . \" H : \u1100 \u1161\u1103 \u1169 \u1103 \u116b . \" You can leave . \" P : \u1102 \u1165\u1102 \u1173 \u11ab \u1100 \u1165\u1100 \u1175\u110b \u1166 \u110b \u1175 \u11bb\u110b \u1173 \u11af \u1111 \u1175 \u11af\u110b \u116d \u110b \u1165 \u11b9\u110b \u1165 . C \" You do n't have to stay there . \" H : \u1102 \u1165 \u11ab \u110c \u1165 \u11bc\u1112 \u116a \u11a8\u1112 \u1175 \u1100 \u1173 \u110c \u1161\u1105 \u1175\u110b \u1166 \u110b \u1175 \u11bb\u110b \u1165\u110b \u1163 \u1112 \u1162 ! \" You need to stay in this place exactly ! \" P : \u1102 \u1165\u1102 \u1173 \u11ab \u1100 \u1165\u1100 \u1175\u110b \u1166 \u110b \u1175 \u11bb\u110b \u1173 \u11af \u1111 \u1175 \u11af\u110b \u116d \u110b \u1165 \u11b9\u110b \u1165 . N \" You do n't have to stay there . \" are almost twice as long as the hypotheses , as reported in Conneau et al ( 2018 ) . We present a few examples in Table 2 . H : \u1102 \u1166\u1100 \u1161 \u110b \u116f \u11ab\u1112 \u1161\u1106 \u1167 \u11ab \u1102 \u1165 \u11ab \u110c \u1175 \u11b8\u110b \u1166 \u1100 \u1161\u1103 \u1169 \u1103 \u116b . \" You can go home if you like . \"", "entities": []}
{"text": "We thank Pulip Park for helping with hiring and contacting with the professional translators . We would also like to acknowledge Kakao Brain Cloud , which we used for our baseline experiments .", "entities": []}
{"text": "DialPort , Gone Live : An Update After A Year of Development", "entities": []}
{"text": "DialPort collects user data for connected spoken dialog systems . At present six systems are linked to a central portal that directs the user to the applicable system and suggests systems that the user may be interested in . User data has started to flow into the system .", "entities": []}
{"text": "The portal gives users feedback for : available topics , system state , and present system state . Skylar does n't interrupt the dialog with a list of topics . Rather it suggests one topic every few turns . This evenly steers users to all of the ES . A banner at the bottom of the screen reminds users of all the topics that can be discussed . Another box indicates the system state in order to avoid user confusion about who has the floor . It shows , for example , whether the system is processing the speech or is still waiting for them to talk . The box shows : idle ( either from timeout or from the user clicking on the box to pause the system ) ; listening ( this is shown from the instant the ASR begins to process speech to when it is finished ) ; speaking ( from when the TTS begins output to when it is finished ) ; thinking ( from when the ASR output is sent to the NLU to when the DM issues its action ) . Finally , the system informs the user of the present state of the dialog . Do you still want XX ( e.g. Pittsburgh ) ? reveals that the user preference for Pittsburgh has not been used for a while , and Skylar 's forgetting curve is ready to eliminate it . The dynamic choice of implicit or explicit confirmation covers the global dialog state .", "entities": []}
{"text": "The AdWord experience lead us to published a Facebook page on April 12 , 2017 . The page was to attract both explorers and real users through both organic ( friends and friends of friends ) and paid distribution . Despite the short time ( 4 - 12 to 4 - 20 ) that it has been published , there have been a total of 51 dialogs ( excluding all dialogs from participating research teams ) . As of April 20 , Dial - Port spent about $ 52 in advertising to reach 1776 individuals getting 147 page views , 47 likes and 346 engagements ( shares or clicks ) . About 40 % of the clicks were from mobile devices as opposed to computers . This underlines the need for mobile versions of DialPort . The average length of a dialog is 8.7turns ( 7.18 stdev ) and 129.51s ( stdev 138.03 ) . There were 14.9 % return users , although another person could be using that computer and some places have automatic IP assignment . 52.9 % of the dialogs were spoken as opposed to typed . The average ASR delay was 925.03ms . On average , users tried 4.8 systems per dialog . The distribution of dialog turns per ES and for the portal over time is shown on Figure 1 . Some systems are getting less use than others . This will be countered by paid advertising campaigns that promote each specific system .", "entities": []}
{"text": "This paper has presented a novel portal that collects spoken dialog data for connected systems . It has begun to collect data for the present seven systems . In order to improve service an audio server is under construction as are smartphone and tablet versions . The portal welcomes new external systems .", "entities": []}
{"text": "This work is partly funded by National Science Foundation grant CNS - 1512973 . The opinions expressed in this paper do not necessarily reflect those of the National Science Foundation .", "entities": []}
{"text": "This work has received funding from the European Research Council ( ERC ) , under the European Union 's Horizon 2020 research and innovation programme ( FASTPARSE , grant agreement No 714150 ) , from MINECO ( FFI2014 - 51978 - C2 - 2 - R , TIN2017 - 85160 - C2 - 1 - R ) and from Xunta de Galicia ( ED431B 2017/01 ) .", "entities": []}
{"text": "Disinformation is much more than just a mild inconvenience for society ; it has resulted in needless deaths in the COVID - 19 pandemic , and has fomented violence and political instability all over the globe ( van der Linden et al , 2020 ) . Our goal in this paper is to discover exploitable weaknesses in current fact - checking models and recommend that such models not be relied upon in their current form . We point out how the models are dependent on emotional signals in the texts instead of exclusively performing textual entailment , and that additional research needs to be done to ensure they are performing the proper task . Harm Minimization Our quantifying of the effects of pre - processing on fact - checking models does not cause any harm to real - world users or companies . Research has demonstrated that adversarial attacks could result in disinformation being labeled as factual news . Disinformation has become increasingly present in global politics , as some nation - states with significant resources have disseminated propaganda to create political dissent in other countries ( Zhou et al , 2019 ) . Our research here has demonstrated potential risks : emotional writing could be used as an exploit to circumvent fact - checking models . Thus , we urge others to further illuminate such vulnerabilities , to minimize potential harms , and to encourage improvements with new models . Deployment Social media companies often deal with fake news by placing highly visible labels . However , simply tagging stories as false can make readers more willing to believe and share other false , untagged stories . This unintended consequence - in which the selective labeling of false news makes other news stories seem more legitimate - has been called the \" implied - truth effect \" ( Pennycook et al , 2019 ) . Thus , unless these models become so accurate that they catch all fake news presented to them , the entire basis of their use is called into question . Despite the significant progress in developing models to correctly identify fake news , the real elephant in the room is that many people simply ignore the labels ( Molina et al , 2021 ) . There is , however , prior work supporting the idea that if people are warned that a headline is false , they will be less likely to believe it ( Ecker et al , 2010 ; Lewandowsky et al , 2012 ) . Because of this , we believe this research represents a net benefit for humanity . Warning labels are just one way of dealing with properly identified fake news , and publishers can choose to simply not allow it on their platforms . Of course , this issue leads to questions of censorship .", "entities": []}
{"text": "Estonian as a Second Language Teacher 's Tools", "entities": []}
{"text": "The paper describes \" Teacher 's Tools \" ( et Opetaja t\u00f6\u00f6riistad ) developed by the Institute of the Estonian Language for teachers and specialists of Estonian as a Second Language . The toolbox includes four modules : vocabulary , grammar , communicative language activities and text evaluation . The vocabulary module provides graded word lists for young ( CEFR levels pre - A1 - C1 ) and adult ( CEFR levels A1 - C1 ) learners . The grammar module provides descriptions of young learners ' grammar competence . The communicative language activities module gives teachers an overview of the typical situations that learners should be able to cope with . The text evaluation module marks lemmas in texts according to their CEFR assignment in the vocabulary module . So far the grammar and the communicative language activities modules have been developed only for young learners ( CEFR levels pre - A1 - B2 ) . The toolbox is aimed at providing support for the development of Estonian as an L2 courses , educational materials , exercises and tests within a CEFR - based framework . The project started in 2017 and is ongoing .", "entities": []}
{"text": "1 Introduction \" Teacher 's Tools \" is a compatible toolkit developed for teachers and specialists of Estonian as a Second Language , providing an extensive overview of the development of lexical and grammatical competence in Estonian among L2 learners . The toolkit is accessible as a sub - page of the language portal S\u00f5naveeb 1 . The framework of the project is based on the Common European Framework of Reference for Languages : Learning , Teaching , Assessment ( CEFR , 2001 ) , its Companion Volume with New Descriptors ( CEFR / CV , 2018 ) and the Collated Representative Samples of Descriptors of Language Competences Developed for Young 1 https://sonaveeb.ee/teacher - tools Learners for Ages 7 - 10 ( Szabo , 2018a ) and 11 - 15 ( Szabo , 2018b ) . The toolkit distinguishes between adult ( CEFR levels A1 - C1 ) and young learners ( CEFR levels pre - A1 - C1 ) . The methodology of the project is adapted from similar projects for other languages ( e.g. Capel , 2010Capel , , 2012O'Keeffe and Geraldine 2017 ; Alfter et al , 2019 ) .", "entities": []}
{"text": "There are quite a few corpora available for the research of the Estonian Language ( the biggest Estonian corpus is the \" Estonian National Corpus 2019 \" ( 1 , 5 billions words ) ) 2 . However , at the beginning of the project in 2017 there was a clear lack of resources available for the research of Estonian as an L2 . In order to fill this gap , the Institute of Estonian Language compiled two types of corpora : 1 ) coursebook corpora and 2 ) learner corpora . As Volodina and Kokkinakis ( 2013 ) point out , the first type of corpora provides information about what and when education professionals found important for students to learn ( passive linguistic competence ) , while learner corpora provide an indication of active linguistic competence . Both need to be studied since the second is directly influenced by the first . The coursebook corpora were compiled in several stages and resulted in two groups of corpora : \" Estonian as a Second Language Coursebook Content Corpus 2017 \" 3 ( contains eight coursebooks for adult learners at levels A1 - C1 ; 500 000 tokens ) and \" Estonian as a Second Language School Coursebook Content Corpus 2021 \" 4 ( contains 27 school coursebooks for grades 1 - 12 ; 1 300 000 tokens ) . The learner corpus was compiled in 2019 - 2021 and contains 6700 Estonian as an L2 young learner 's texts . These are texts written mostly as state standard - determining tests ( grades 3 and 6 ) , basic school final examinations The data is available for analysis through the interface of the Estonian Learner Language Corpus EMMA 5 . The texts were not corrected or altered . Both types of corpora were used for the development of \" Teacher 's Tools \" . The coursebook corpora were used primarily to create passive vocabulary word lists and for the analysis of explicit and implicit grammar teaching . The learner corpus was used for the creation and analysis of active vocabulary word lists and the dynamics of grammar acquisition . 3 Modules of \" Teacher 's Tools \" \" Teacher 's Tools \" consists of four modules : vocabulary , grammar , communicative language activities and text evaluation .", "entities": []}
{"text": "The vocabulary module includes young and adult learners ' word lists compiled on the basis of coursebook and learner corpus word lists in comparison with frequency distribution in the \" Estonian National Corpus 2017 \" 6 . Currently , the vocabulary list for adult learners includes about 12 500 words for levels A1 - C1 . The young learners ' word list includes approx . 9000 words for levels pre - A1 - B2 . All words are presented as lemmas . Users can generate word lists based on learner type ( young or adult ) and POS . In addition , 24 topics ( clothes , furniture , birds etc . ) are compiled . The search results can be sorted alphabetically , by level or by POS ( see Figure 1 ) .", "entities": []}
{"text": "The grammar module is the first attempt to create a systematic overview of Estonian L2 learner 's grammar competence development . The general methodology was adopted from the English Grammar Profile 7 project ( O'Keeffe and Geraldine , 2017 ) . Currently , the grammar module provides descriptions of grammar competence on the morphology , derivation , phrase and sentence levels , from level pre - A1 up to level B2 ( see Figure 2 ) . The same grammatical category ( e.g. the use of the genetive case for nouns or the use of imperative mood for verbs ) may be described at all levels , but the functions and usage ( how often the learner makes mistakes and what kinds of mistakes ) are level - specific . Search options offer users choices of four main categories : morphology , derivation , phrase formation and sentence formation . The main categories contain 19 subcategories . For example , subcategories of morphology are verb , noun , adjective , numeral , pronoun , adverb . Every subcategory has grammatical markers ( e.g. number and case for noun ) . The grammatical markers have values , for example number has values singular and plural . All descriptions are equipped with example sentences compiled either by experts or taken from coursebook and learner 's corpora .", "entities": []}
{"text": "The communicative language activities module is based on CEFR / CV descriptor scales , and examples are taken from Szabo ( 2018a , b ) , which include reception processes and strategies of written and spoken texts ( listening and reading ) , written and oral text production and production strategies , and written and spoken interaction and interaction strategies . It covers young learners at levels A1 - B2 . The architecture of the module is similar to that of the grammar module , containing main categories ( reception , production , interaction and mediation ) and subcategories , which are followed by detailed descriptions , including limitations : when and how well the language user can manage certain communicative activities ( see Figure 3 ) . For example , by choosing the main category reception and the subcategory overall listening comprehension , a teacher can see that at the A1 level the learner should be able to understand a simple description with short sentences , for example about the weather or how big a certain object is .", "entities": []}
{"text": "Supporting Spoken Assistant Systems with a Graphical User Interface that Signals Incremental Understanding and Prediction State", "entities": []}
{"text": "Current virtual personal assistants ( PAs ) require users to either formulate complex intents in one utterance ( e.g. , \" call Peter Miller on his mobile phone \" ) or go through tedious sub - dialogues ( e.g. , \" phone call \" - who would you like to call ? - \" Peter Miller \" - I have a mobile number and a work number . Which one do you want ? ) . This is not how one would interact with a human assistant , where the request would be naturally structured into smaller chunks that individually get acknowledged ( e.g. , \" Can you make a connection for me ? \" - sure - \" with Peter Miller \" - uh huh - \" on his mobile \" - dialling now ) . Current PAs signal ongoing understanding by displaying the state of the recognised speech ( ASR ) to the user , but not their semantic interpretation of it . Another type of assistant system forgoes enquiring user intent altogether and infers likely intents from context . GoogleNow , for example , might present traffic information to a user picking up their mobile phone at their typical commute time . These systems display their \" understanding \" state , but do not allow any type of interaction with it apart from dismissing the provided information . In this work , we explore adding a graphical user interface ( GUI ) modality that makes it possible to see these interaction styles as extremes on a continuum , and to realise positions between these extremes and present a mixed graphical / voice enabled PA that can provide feedback of understanding to the user incrementally as the user 's utterance unfolds - allowing users to make requests in instalments instead of fully thought - out requests . It does this by signalling ongoing understanding in an intuitive tree - like GUI that can be displayed on a mobile device . We evaluate our system by directing users to perform tasks using it under nonincremental ( i.e. , ASR endpointing ) and incremental conditions and then compare the two conditions . We further compare a non - adaptive with an adaptive ( i.e. , infers likely events ) version of our system . We report that the users found the interface intuitive and easy to use , and that users were able to perform tasks more efficiently with incremental as well as adaptive variants of the system .", "entities": []}
{"text": "This work builds upon several threads of previous research : Chai et al ( 2014 ) addressed misalignments in understanding ( i.e. , common ground ( Clark and Schaefer , 1989 ) ) between robots and humans by informing the human of the internal system state via speech . We take this idea and ap - ply it to a PA by displaying the internal state of the system to the user via a GUI ( explained in Section 3.5 ) , allowing the user to determine if system understanding has taken place - a way of providing feedback and backchannels to the user . Dethlefs et al ( 2016 ) provide a good review of work that show how backchannels facilitate grounding , feedback , and clarifications in human spoken dialogue , and apply an information density approach to determine when to backchannel using speech . Because we do n't backchannel using speech here , there is no potential overlap between the user and the system ; rather , our system can display backchannels and ask clarifications without frustrating the user through inadvertent overlaps . Though different in many ways , our work is similar in some regards to Larsson et al ( 2011 ) , which displays information to the user and allows the user to navigate the display itself ( e.g. , by saying up or down in a menu list ) - functionality that we intend to apply to our GUI in future work . Our work is also comparable to SDS toolkits such as IrisTK ( Skantze and Moubayed , 2012 ) and Open - Dial ( Lison , 2015 ) which enable SDS designers to visualise the internal state of their systems , though not for end user interpretability . Some of the work here is inspired by the Microsoft Language Understanding Intelligent Service ( LUIS ) project ( Williams et al , 2015 ) . While our system by no means achieves the scale that LUIS does , we offer here an additional contribution of an open source LUIS - like system ( with the important addition of the graphical interface ) that is authorable ( using JSON files ; we leave authoring using a web interface like that of LUIS to future work ) , extensible ( affordances can be easily added ) , incremental ( in that respect going beyond LUIS ) , trainable ( i.e. , can learn from examples , but can still function well without examples ) , and can learn through interacting ( here we apply a user model that learns during interaction ) .", "entities": []}
{"text": "DM w 1 ... w n w 1 ... w n d 1 d m s 1 : v 1 s m : v m ... s 1 : v 1 s m : v m ... ...", "entities": []}
{"text": "Figure 1 : Overview of system made up of ASR which takes in a speech signal and produces transcribed words , NLU , which takes words and produces a slots in a frame , DM which takes slots and produces a decision for each , and the GUI which displays the state of the system .", "entities": []}
{"text": "The DM plays a crucial role in our SDS : as well as determining how to act , the DM is called upon to decide when to act , effectively giving the DM the control over timing of actions rather than relying on ASR endpointing - further separating our SDS from other systems . The DM policy is based on a confidence score derived from the NLU ( in this case , we used the distribution 's argmax value ) using thresholds for the actions ( see below ) , set by hand ( i.e. , trial and error ) . At each word and resulting distribution from NLU , the DM needs to choose one of the following : wait - wait for more information ( i.e. , for the next word ) select - as the NLU is confident enough , fill the slot can with the argmax from NLU request - signal a ( yes / no ) clarification request on the current slot and the proposed filler confirm - act on the confirmation of the user ; in effect , select the proposed slot value Though the thresholds are statically set , we applied OpenDial ( Lison , 2015 ) as an IU - module to perform the task of the DM with the future goal that these values could be adjusted through reinforcement learning ( which OpenDial could provide ) . The DM processes and makes a decision for each slot , with the assumption that only one slot out of all that are processed will result in an non - wait action ( though this is not enforced ) .", "entities": []}
{"text": "In this section , we describe two experiments where we evaluated our system . It is our primary goal to show that our GUI is useful and signals understanding to the user . We also wish to show that incremental presentation of such a GUI is more effective than an endpointed system . We further want to show that an adaptive system is more effective than a non - adaptive system ( though both would process incrementally ) . In order to best evaluate our system , we recruited participants to interact with our system in varied settings to compare endpointed ( i.e. , non - incremental ) and nonadaptive as well as adaptive versions . We describe how the data were collected from the participants , then explain each experiment and give results .", "entities": []}
{"text": "In this section we report the results of the evaluation between the endpointed ( i.e. , nonincremental ; Phase 1 ) variant vs the incremental ( Phase 2 ) variant of our system .", "entities": []}
{"text": "We applied a multinomial test of significance to the results , treating all four possible answers as equally likely ( with Bonferroni correction of 10 ) . The item The interface was useful and easy to understand with the answer of Both was significant ( \u03c7 2 ( 4 , N = 12 ) = 9.0 , p < .005 ) , as was The assistant was easy and intuitive to use also with the answer Both ( \u03c7 2 ( 4 , N = 12 ) = 9.0 , p < .005 ) . The item I always understood what the system wanted from me was also answered Both significantly more times than other answers ( \u03c7 2 ( 4 , N = 14 ) = 9.0 , p < .005 ) , similarly for It was sometimes unclear to me if the assistant understood me with the answer of Both ( \u03c7 2 ( 4 , N = 12 ) = 10.0 , p < .005 ) . These responses tell us that though the participants did not report preference for either system variant , they reported a general positive impression of the GUI ( in both variants ) . This is a nice result ; the GUI could be used in either system with benefit to the users .", "entities": []}
{"text": "The endpointed ( Phase 1 ) and incremental ( Phase 2 ) columns in Table 1 show the results of the objective evaluation . incremental variant , the total number of tasks for the incremental variant was higher . Manual inspection of logs indicate that participants took advantage of the system 's flexibility of understanding instalments ( i.e. , filling frames incrementally ) . This is evidenced in that participants often uttered words understood by the system as being negative ( e.g. , nein / no ) , either as a result of an explicit confirmation request by the system ( e.g. , Thai ? ) or after a slot was incorrectly filled ( something very easily determined through the GUI ) . This is a desired outcome of using our system ; participants were able to repair local areas of misunderstanding as they took place instead of needing to correct an entire intent ( i.e. , frame ) . However , we can not fully empirically measure these tendencies given our data .", "entities": []}
{"text": "Incremental - Adaptive In this section we report results for the evaluation between the incremental ( Phase 2 ) and incremental - adaptive ( henceforth just adaptive ; Phase 3 ) systems .", "entities": []}
{"text": "We applied the same significance test as Experiment 1 ( with Bonferroni correction of 12 ) . The item The interface was useful and easy to understand was answered with Both significantly ( \u03c7 2 ( 4 , N = 14 ) = 10.0 , p < .0042 ) , The item I had the feeling that the assistant attempted to learn about me was answered with Neither ( \u03c7 2 ( 4 , N = 14 ) = 8.0 , p < .0042 ) , though Phase 3 was also marked ( 6 times ) . All other items were not significant . Here again we see that there is a general positive impression of the GUI under all conditions . If anyone noticed that a system variant was attempting to learn a user model at all , they noticed that it was in Phase 3 , as expected .", "entities": []}
{"text": "The incremental ( Phase 2 ) and adaptive ( Phase 3 ) columns in Table 1 show the results for the objective evaluation for this experiment . There is a clear difference between the two variants , with the adaptive showing more completed tasks , more fully correct frames , and a higher average fscore ( all three likely due to the fact that frames were potentially pre - filled ) .", "entities": []}
{"text": "While the responses do n't express any preference for a particular system variant , the overall impression of the GUI was positive . The objective measures show that there are gains to be made when the system signals understanding at a more finegrained interval than at the utterance level , due to the higher number of completed tasks and locallymade repairs . There are further gains to be made when the system applies simple user modelling ( i.e. , adaptivity ) by attempting to predict what the user might want to do in a chosen domain , decreasing the possibility of user error and allowing the system to accurately and quickly complete more tasks . Participants also did n't just get used to the system over time , as the average time per episode was fairly similar in all three phases . The open - ended questionnaire sheds additional light . Most of the suggestions for improvement related to ASR misrecognition and speed ( i.e. , not about the system itself ) . Two participants suggested an ability to add \" free input \" or select alternatives from the tree . Two participants suggested that the system be more responsive ( i.e. , in wait states ) , and give more feedback ( i.e. , backchannels ) more often . For those participants that expressed preference to the non - incremental system ( Phase 1 ) , none of them had used a speech - based PA before , whereas those that expressed preference to the incremental versions ( Phases 2 and 3 ) use them regularly . We conjecture that people without SDS experience equate understanding with ASR , whereas those that are more familiar with PAs know that perfect ASR does n't translate to perfect understanding - hence the need for a GUI . A potential remedy would be to display ASR with the tree , signalling understanding despite ASR errors .", "entities": []}
{"text": "Acknowledgements Thanks to the anonymous reviewers who provided useful comments and suggestions . Thanks also to Julian Hough for helping with experiments . We acknowledge support by the Cluster of Excellence \" Cognitive Interaction Technology \" ( CITEC ; EXC 277 ) at Bielefeld University , which is funded by the German Research Foundation ( DFG ) , and the BMBF Kogni - Home project .", "entities": []}
{"text": "The following questions were asked on both questionnaires following Phase 2 and Phase 3 ( comparing the two most latest used system versions ; as translated into English ) : The interface was useful and easy to understand . The assistant was easy and intuitive to use . The assistant understood what I wanted to say . I always understood what the system wanted from me . The assistant made many mistakes . The assistant did not respond while I spoke .", "entities": []}
{"text": "Learning an embedding for every possible token and tag combination would enormously increase the model 's learnable parameter count . Furthermore , training data is likely to be sparse in its coverage of all possible pairs , but not in its coverage of the token and tag vocabularies separately . Therefore , we instead learn a separate embedding vector for each possible token and each possible tag , effectively concatenating these two vocabularies ( rather than taking the product space ) . The embedding vectors for the token and tag at each position are then added to combine information from both channels into a single vector , so as not to increase the size of subsequent model layers and the capacity of the model , apart from the additional tag embedding vectors .", "entities": []}
{"text": "Word tokenization , as used by the tagging systems , is most straightforward for maintaining one - to - one alignments between tokens and their assigned tags . For word tokenization experiments , vocabularies of size 35 , 012 for German and 17 , 196 for English were selected , resulting in an unknown word replacement rate of 3 % . This unknown word replacement was considerably higher on rare word categories , for example named entities saw a 25 - 30 % rate of unknown words outside the selected word vocabulary . To alleviate this it is also possible to consider subword ( Kudo , 2018 ) vocabulary of 32 , 000 subwords , built from the training split and used to tokenize both languages . After subword tokenization , the BIOES structure of named entity spans was propagated across subword tokens in the natural way to maintain spans . For POS tags , subwords received the same tag as their parent word .", "entities": []}
{"text": "Many thanks go to my colleagues Jeesoo Bang , Jaehun Shin , and Baikjin Jung in the Knowledge and Language Engineering Lab ( POSTECH ) for their many hours generously spent discussing these research topics . These results would not have been possible without their support . This", "entities": []}
{"text": "M\u0101ori Loanwords : A Corpus of New Zealand English Tweets", "entities": []}
{"text": "M\u0101ori loanwords are widely used in New Zealand English for various social functions by New Zealanders within and outside of the M\u0101ori community . Motivated by the lack of linguistic resources for studying how M\u0101ori loanwords are used in social media , we present a new corpus of New Zealand English tweets . We collected tweets containing selected M\u0101ori words that are likely to be known by New Zealanders who do not speak M\u0101ori . Since over 30 % of these words turned out to be irrelevant ( e.g. , mana is a popular gaming term , Moana is a character from a Disney movie ) , we manually annotated a sample of our tweets into relevant and irrelevant categories . This data was used to train machine learning models to automatically filter out irrelevant tweets .", "entities": []}
{"text": "One of the most salient features of New Zealand English ( NZE ) is the widespread use of M\u0101ori words ( loanwords ) , such as aroha ( love ) , kai ( food ) and Aotearoa ( New Zealand ) . See ex . ( 1 ) specifically from Twitter ( note the informal , conversational style and the M\u0101ori loanwords emphasised in bold ) . ( 1 ) Led the waiata for the manuhiri at the p\u014dwhiri for new staff for induction week . Was told by the kaum\u0101tua I did it with mana and integrity . The use of M\u0101ori words has been studied intensively over the past thirty years , offering a comprehensive insight into the evolution of one of the youngest dialects of English - New Zealand English ( Calude et al , 2017 ; Daly , 2007Daly , , 2016 ; Davies and Maclagan , 2006 ; De Bres , 2006 ; Degani and Onysko , 2010 ; Kennedy and Yamazaki , 1999 ; Macalister , 2009Macalister , , 2006aOnysko and Calude , 2013 ) . One aspect which is missing in this body of work is the online discourse presence of the loanwords - almost all studies come from ( collaborative ) written language ( highly edited , revised and scrutinised newspaper language , Davies and Maclagan 2006 ; Macalister 2009Macalister , 2006aOnysko and Calude 2013 , and picture - books , Daly 2007 , 2016 , or from spoken language collected in the late 1990s ( Kennedy and Yamazaki , 1999 ) . In this paper , we build a corpus of New Zealand English tweets containing M\u0101ori loanwords . Building such a corpus has its challenges ( as discussed in Section 3.1 ) . Before we discuss these , it is important to highlight the uniqueness of the language contact situation between M\u0101ori and ( NZ ) English . The language contact situation in New Zealand provides a unique case - study for loanwords because of a number of factors . We list three particularly relevant here . First , the direction of lexical transfer is highly unusual , namely , from an endangered indigenous language ( M\u0101ori ) into a dominant lingua franca ( English ) . The large - scale lexical transfer of this type has virtually never been documented elsewhere , to the best of our knowledge ( see summary of current language contact situations in Stammers and Deuchar 2012 , particularly Table 1 , p. 634 ) . Secondly , because M\u0101ori loanwords are \" New Zealand 's and New Zealand 's alone \" ( Deverson , 1991 , p. 18 - 19 ) , and above speakers ' consciousness , their ardent study over the years provides a fruitful comparison of the use of loanwords across genres , contexts and time . Finally , the aforementioned body of previous research on the topic is rich and detailed , and still rapidly changing , with loanword use being an increasing trend ( Macalister , 2006a ; Kennedy and Yamazaki , 1999 ) . However , the jury is still out regarding the reasons for the loanword use ( some hypotheses have been put forward ) , and the pat - terns of use across different genres ( it is unclear how language formality influences loanword use ) . We find that Twitter data complements the growing body of work on M\u0101ori loanwords in NZE , by adding a combination of institutional and individual linguistic exchanges , in a non - editable online platform . Social media language shares properties with both spoken and written language , but is not exactly like either . More specifically , Twitter allows for creative expression and lexical innovation ( Grieve et al , 2017 ) . Our Twitter corpus was created by following three main steps : collecting tweets over a ten - year period using \" query words \" ( Section 3.1 ) , manually labelling thousands of randomly - sampled tweets as \" relevant \" or \" irrelevant \" ( Section 3.2 ) , and then training a classifier to obtain automatic predictions for the relevance of each tweet and deploying this model on our target tweets , in a bid to filter out all those which are \" irrelevant \" ( Section 3.3 ) . As will be discussed in Section 2 , our corpus is not the first of its kind but is the first corpus of New Zealand English tweets and the first collection of online discourse built specifically to analyse the use of M\u0101ori loanwords in NZE . Section 4 outlines some preliminary findings from our corpus and Section 5 lays out plans for future work .", "entities": []}
{"text": "It is uncontroversial that M\u0101ori loanwords are both productively used in NZE and increasing in popularity ( Macalister , 2006a ) . The corpora analysed previously indicate that loanword use is highly skewed , with some language users leading the way - specifically M\u0101ori women ( Calude et al , 2017 ; Kennedy and Yamazaki , 1999 ) , and with certain topics of discourse drawing significantly higher counts of loanwords than others - specifically those related to M\u0101ori people and M\u0101ori affairs , M\u0101oritanga ( Degani , 2010 ) . The type of loanwords being borrowed from M\u0101ori is also changing . During the first wave of borrowing , some two - hundred years ago , many flora and fauna words were being borrowed ; today , it is social culture terms that are increasingly adopted , e.g. , aroha ( love ) , whaea ( woman , teacher ) , and tangi ( M\u0101ori funeral ) , see Macalister ( 2006a ) . However , the data available for loanword analysis is either outdated ( Calude et al , 2017 ; Kennedy and Yamazaki , 1999 ) , or exclusively formal and highly edited ( mainly newspaper language , Macalister 2006a ; Davies and Maclagan 2006 ; Degani 2010 ) , so little is known about M\u0101ori loanwords in recent informal NZE interactions - a gap we hope to address here . With the availability of vast amounts of data , building Twitter corpora has been a fruitful endeavour in various languages , including Turkish ( \u015e im\u015fek and\u00d6zdemir , 2012 ; \u00c7 etinoglu , 2016 ) , Greek ( Sifianou , 2015 ) , German ( Scheffler , 2014 ; Cieliebak et al , 2017 ) , and ( American ) English ( Huang et al , 2016 ) ( though notably , not New Zealand English , while a modest corpus of te reo M\u0101ori tweets does exist , Keegan et al 2015 ) . Twitter corpora of mixed languages are tougher to collect because it is not straightforward to detect mixed language data automatically . Geolocations can help to some extent , but they have limitations ( most users do not use them to begin with ) . Recent work on Arabic has leveraged the presence of distinct scripts - the Roman and Arabic alphabet - to create a mixed language corpus ( Voss et al , 2014 ) , but this option is not available to us . M\u0101ori has traditionally been a spoken ( only ) language , and was first written down in the early 1800s by European missionaries in conjunction with M\u0101ori language scholars , using the Roman alphabet ( Smyth , 1946 ) . Our task is more similar to studies such as Das andGamb\u00e4ck ( 2014 ) and\u00c7 etinoglu ( 2016 ) , who aim to find a mix of two languages which share the same script ( in their case , Hindi and English , and Turkish and German , respectively ) , but our method for collecting tweets is not user - based ; instead we use a set of target query words , as detailed in Section 3.1 .", "entities": []}
{"text": "In this section , we describe the process of building the M\u0101ori Loanword Twitter Corpus ( hereafter , the MLT Corpus ) 1 . This process consists of three main steps , as depicted in Figure 1 .", "entities": []}
{"text": "In order to facilitate the collection of relevant data for the MLT Corpus , we compiled a list of 116 target loanwords , which we will call \" query words \" . Most of these are individual words but some are short phrasal units ( tangata whenua , people of the land ; kapa haka , cultural performance ) . The list is largely derived from Hay ( 2018 ) but was modified to exclude function words ( such as numerals ) and most proper nouns , except five that have native English counterparts : Aotearoa ( New Zealand ) , Kiwi ( s ) ( New Zealander ( s ) ) , M\u0101ori ( indigenous New Zealander ) , P\u0101keh\u0101 ( European New Zealander ) , non - M\u0101ori ( non - indigenous New Zealander ) . We also added three further loanwords which we deemed useful for increasing our data , namely haurangi ( drunk ) , wairangi ( drugged , confused ) , and p\u014drangi ( crazy ) . Using the Twitter Search API , we harvested 8 million tweets containing at least one query word ( after converting all characters to lowercase ) . The tweets were collected diachronically over an eleven year period , between 2007 - 2018 . We ensured that tweets were ( mostly ) written in English by using the lang : en parameter . A number of exclusions and further adjustments were made . With the aim of avoiding redundancy and uninformative data , retweets and tweets with URLs were discarded . Tweets in which the query word was used as part of a username or mention ( e.g. , @happy kiwi ) were also discarded . For those query words which contained macrons , we found that users were inconsistent in their macron use . Consequently , we consolidated the data by adjusting our search to include both the macron and the non - macron version ( e.g. , both M\u0101ori and Maori ) . We also removed all tweets containing fewer than five tokens ( words ) , due to insufficient context of analysis . Owing to relaxed spelling conventions on Twitter ( and also the use of hashtags ) , certain query words comprising multiple lexical items were stripped of spaces in order to harvest all variants of the phrasal units ( e.g. , kai moana and kaimoana ) . As kai was itself a query word ( in its own right ) , we excluded tweets containing kai moana when searching for tweets containing kai ( and repeated this process with similar items ) . After inspecting these tweets , it was clear that a large number of our query words were polysemous ( or otherwise unrelated to NZE ) , and had introduced a significant amount of noise into the data . The four main challenges we encountered are described below . First , because Twitter contains many different varieties of English , NZE being just one of these , it is not always straightforward to disentangle the dialect of English spoken in New Zealand from other dialects of English . This could be a problem when , for instance , a M\u0101ori word like Moana ( sea ) is used in American English tweets to denote the Disney movie ( or its main character ) . Secondly , M\u0101ori words have cognate forms with other Austronesian languages , such as Hawaiian , Samoan and Tongan , and many speakers of these languages live and work ( and tweet ) in New Zealand . For instance , the word wahine ( woman ) has the same written form in M\u0101ori and in Hawaiian . But cognates are not the only problematic words . Homographs with other , genealogically - unrelated languages can also pose problems . For instance , the M\u0101ori word hui ( meeting ) is sometimes used as a proper noun in Chinese , as can be seen in the following tweet : \" Yo is Tay Peng Hui okay with the tip of his finger ? \" . Proper nouns constitute a third problematic aspect in our data . As is typical for many language contact situations where an indigenous language shares the same geographical space as an incoming language , M\u0101ori has contributed many place names and personal names to NZE , such as Timaru , Aoraki , Titirangi , H\u0113mi , Mere and so on . While these proper nouns theoretically count as loanwords , we are less interested in them than in content words because the use of the former does not constitute a choice , whereas the use of the latter does ( in many cases ) . The \" choice \" of whether to use a loanword or whether to use a native English word ( or sometimes a native English phrase ) is interesting to study because it provides insights into idiolectal lexical preferences ( which words different speakers or writers prefer in given contexts ) and relative borrowing success rates ( Calude et al , 2017 ; Zenner et al , 2012 ) . Finally , given the impromptu and spontaneous nature of Twitter in general , we found that certain M\u0101ori words coincided with misspelled versions of intended native English words , e.g. , whare ( house ) instead of where . The resulting collection of tweets , termed the Original Dataset , was used to create the Raw Corpus , as explained below .", "entities": []}
{"text": "We decided to address the \" noisy \" tweets in our data using supervised machine learning . Two coders manually inspected a random sample of 30 tweets for each query word , by checking the word 's context of use , and labelled each tweet as \" relevant \" or \" irrelevant \" . For example , a tweet like that in example ( 1 ) would be coded as relevant and one like \" awesome ! ! Congrats to Tangi : ) \" , would be coded as irrelevant ( because the query word tangi is used as a proper noun ) . Since 39 of the query words consistently yielded irrelevant tweets ( at least 90 % of the time ) , these ( and the tweets they occurred in ) were removed altogether from the data . Our annotators produced a total of 3 , 685 labelled tweets for the remaining 77 query words , which comprise the Labelled Corpus ( see Tables 1 and 4 ; note that irrelevant tweets have been removed from the latter for linguistic analysis ) . Assuming our coded samples are representative of the real distribution of relevant / irrelevant tweets that occur with each query word , it makes sense to also discard the 39 \" noisy \" query words from our Original Dataset . In this way , we created the ( unlabelled ) Raw Corpus , which is a fifth of the size ( see Table 4 ) . We computed an inter - rater reliability score for our two coders , based on a random sample of 200 tweets . Using Cohen 's Kappa , we calculated this value to be 0.87 ( \" strong \" ) . In light of the strong agreement between the initial coders , no further coders were enlisted for the task .", "entities": []}
{"text": "As we are only just beginning to sift through the MLT Corpus , we note two particular sets of preliminary findings . First , even though our corpus was primarily geared up to investigate loanword use , we are finding that , unlike other NZE genres analysed , the Twitter data exhibits use of M\u0101ori which is more in line with code - switching than with loanword use , see ex . ( 2 - 3 ) . This is particularly interesting in light of the reported increase in te reo M\u0101ori language tweets ( Keegan et al , 2015 ) . ( 2 ) M\u014drena e hoa ! We must really meet IRL when I get back to T\u0101maki Makaurau ! You have a fab day too ! ( 3 ) Heh ! He porangi toku ngeru - especially at 5 in the morning ! ! Ata marie e hoa ma . I am well thank you . Secondly , we also report the use of hybrid hashtags , that is , hashtags which contain a M\u0101ori part and an English part , for example # mycrazywhanau , # reostories , # Matarikistar , # bringitonmana , # growingupkiwi , # kaitoputinmyfridge . To our knowledge , these hybrid hashtags have never been analysed in the current literature . Hybrid hashtags parallel the phenomenon of hybrid compounds discussed by Degani and Onysko ( 2010 ) . Degani and Onysko report that hybrid compounds are both productive and semantically novel , showing that the borrowed words take on reconceptualised meanings in their adoptive language ( 2010 , p.231 ) .", "entities": []}
{"text": "Relevant tweets f ( x ) < 0.5 Classified irrelevant Haka ne ! And i know even the good guys get blood for body ( 0.282 , foreign language ) son did nt get my chop ciggies 2day so stopped talking 2 him . he just walked past and gave me the maori eyebrow lift and a smile . were friends ( 0.337 ) Whare has the year gone ( 0.36 , misspelling ) Shorts and bare feet in this whare ( 0.41 ) chegar na morena e falar can i be your girlfriend can i ( 0.384 , foreign language )", "entities": []}
{"text": "The authors would like to thank former Honours student Nicole Chan for a preliminary study on M\u0101ori Loanwords in Twitter . Felipe Bravo - Marquez was funded by Millennium Institute for Foundational Research on Data . Andreea S. Calude acknowledges the support of the NZ Royal Society Marsden Grant . David Trye acknowledges the generous support of the Computing and Mathematical Sciences group at the University of Waikato .", "entities": []}
{"text": "Neural Maximum Subgraph Parsing for Cross - Domain Semantic Dependency Analysis", "entities": []}
{"text": "Some recent work on parsing targets the graphstructured semantic representations that are more general than the tree representation . Existing approaches can be categorized into two dominant types : the transition - based ( Zhang et al , 2016 ; Wang et al , 2018 ) and graph - based , i.e. , Maximum Subgraph ( Kuhlmann and Jonsson , 2015 ; Cao et al , 2017a ) , approaches . Previous investigations on transition - based string - to - semantic - graph parsing adopt many ideas from syntactic string - totree parsing , such as how to handle crossing arcs and how to perform neural disambiguation . Zhang et al ( 2016 ) introduced two transition systems that can generate arbitrary graphs and augmented them into practical semantic dependency parsers with a structured perceptron model . Wang et al ( 2018 ) evaluated the effectiveness of deep learning techniques for transition - based SDP . Kuhlmann and Jonsson ( 2015 ) proposed to formulate SDP as the search for the maximum subgraphs for some particular graph classes . This proposal is called Maximum Subgraph parsing , which is a generalization of the graph - based parsing framework for syntactic parsing . For arbitrary graphs , Du et al ( 2015a ) proved that the secondorder Maximum Subgraph problem is an NPhard problem . Nevertheless , Almeida and Martins ( 2015 ) and Du et al ( 2015a ) showed that dual decomposition is a practical technique to solve the problem . Considering more restricted graph classes , Kuhlmann and Jonsson ( 2015 ) introduced a dynamic programming algorithem for parsing to noncrossing graphs . Cao et al ( 2017a ; 2017b ) showed that 1EC / P2 graphs are more suitable for describing semantic graphs than the noncrossing graphs , and they also allow low - degree dynamic programming algorithms for decoding .", "entities": []}
{"text": "Previous work showed that the Maximum Subgraph framework is not only elegant in theory but also effective in practice ( Kuhlmann and Jonsson , 2015 ; Cao et al , 2017a , b ) . In particular , 1EC / P2 graphs are an appropriate graph class for modeling semantic dependency structures ( Cao et al , 2017a ) . Figure 2 presents an example to illustrate the 1 - endpoint - crossing property , while Figure 3 shows a case for pagenumber - 2 . Below we present the formal description of the two properties that are adopted from Pitler et al ( 2013 ) and Kuhlmann and Jonsson ( 2015 ) respectively . Definition 1 A dependency graph is 1 - Endpoint - Crossing if for any edge e , all edges that cross e share an endpoint p named pencil point . Definition 2 A pagenumber - k graph means it consists at most k half - planes , and arcs on each half - plane are noncrossing . If G is the set of 1 - endpoint - crossing graphs or more restricted 1EC / P2 graphs , the optimization problem ( 1 ) in the first - order case can be solved in quintic - time ( Cao et al , 2017a ) by using dynamic programming . Furthermore , ignoring one linguistically - rare structure in 1EC / P2 graphs descreases the complexity to O ( n 4 ) ( Cao et al , 2017a ) . In this paper , we implement Cao et al Cao et al ( 2017a ) 's algorithm as the basis of our parser .", "entities": []}
{"text": "A semantic graph mainly consists of two parts : the structural part and the label part . The former describes the predicate - argument relation in the sentence , and the latter describes the type of this relation . In our model , the structural part and the label part are regarded as independent of each other . We use a coarse - to - fine strategy : finding the maximum unlabeled subgraph first and assigning a label for every edge in this subgraph then . The motivation is to avoid the calculation of a number of unnecessary label scores in order to improve the processing efficiency . candidate dependencies as well as their relation types . Figure 4 shows the architecture of our system .", "entities": []}
{"text": "This work was supported by the National Natural Science Foundation of China ( 61772036 , 61331011 ) and the Key Laboratory of Science , Technology and Standard in Press Industry ( Key Laboratory of Intelligent Press Media Technology ) . We thank the anonymous reviewers for their helpful comments . Weiwei Sun is the corresponding author .", "entities": []}
{"text": "Our OpenRE framework mainly consists of two modules , the relation similarity calculation module and the relation clustering module . For relation similarity calculation , we propose Relational Siamese Networks ( RSNs ) , which learn to predict whether two sentences mention the same relation . To utilize large - scale unsupervised data and distantly - supervised data , we further propose Semi - supervised RSN and Distantly - supervised RSN . Finally , in the relation clustering module , with the learned relation metric , we utilize hierarchical agglomerative clustering ( HAC ) and Louvain clustering algorithms to cluster target relation instances of new relation types .", "entities": []}
{"text": "In this section , we conduct several experiments on real - world RE datasets to show the effectiveness of our models , and give a detailed analysis to show its advantages .", "entities": []}
{"text": "To intuitively evaluate the knowledge transfer effects of RSN and Semi - supervised RSN , we visualize their relational knowledge representation spaces in the last layer of CNN encoders with t - SNE ( Maaten and Hinton , 2008 ) in Figure 4 . We also compare with a supervised CNN trained on 9 , 600 labeled instances of novel relations , which suggests the optimal relational knowledge representation . In each figure , we plot 402 relation instances of 4 randomly - chosen relation types in the test set , and points are colored according to their ground - truth labels . As we can see from Figure 4 , RSN is able to roughly distinguish different relations , and Semi - supervised RSN further facilitated knowledge transfer by optimizing the margin between potential relation clusters during training . As a result , Semi - supervised RSN can extract more distinguishable novel relations , and gains comparable relational knowledge representation ability with supervised CNN .", "entities": []}
{"text": "KALA : Knowledge - Augmented Language Model Adaptation", "entities": []}
{"text": "The performance gap between KALA ( relational ) and KALA ( point - wise ) shows the effectiveness of relational retrieval for language model adaptation , which allows us to incorporate relational knowledge into the PLM . The relational retrieval also helps address unseen entities , as discussed in Section 5.4 .", "entities": []}
{"text": "We perform an ablation study to see how much each component contributes to the performance gain .", "entities": []}
{"text": "One remarkable advantage of our KALA is its ability to represent an unseen entity by aggregating features of its neighbors from a given KG . To analyze this , we first divide all contexts into one of Seen and Unseen , where Seen denotes the context with less than 3 unseen entities , and then measure the performance on the two subsets . As shown in Figure 4 , we observe that the performance gain of KALA over the baselines is much larger on the Unseen subset , which demonstrates the effectiveness of KALA 's relational retrieval scheme to represent unseen entities . DAPT also largely outperforms fine - tuning and TAPT as it is trained on an extremely large external corpus for adaptive pre - training . However , KALA even outperforms DAPT in most cases , verifying that our knowledgeaugmentation method is more effective for tackling domain - specific tasks . The visualization of embeddings of seen and unseen entities in Figure 2 shows that KALA embeds the unseen entities more closely to the seen entities 4 , which explains KALA 's good performance on the Unseen subset .", "entities": []}
{"text": "To better see how our KFM ( 3.2 ) works , we show the context and its fact , and then visualize representations from the PLM modulated by the KFM . As shown in Figure 5 right , the token ' # # on ' is not aligned with their corresponding tokens , such as ' ex ' ( for exon ) and ' cod ' ( for codon ) , in the baseline . However , with our feature modulation that transforms multiple tokens associated with the single entity equally , the two tokens ( e.g. , ( ' ex ' , ' # # on ' ) ) , composing one entity , are closely embedded . Also , while the baseline can not handle the unseen entity consisting of three tokens : 're ' , ' # # tina ' , and ' # # l ' , KALA embeds them closely by representing the unseen retinal from the representation of its neighborhood gene derived by the domain knowledge - ( retinal , instance of , gene ) .", "entities": []}
{"text": "Our KALA framework is also applicable to encoder - decoder PLMs by applying the KFM to the encoder . Therefore , we further validate KALA 's effectiveness on the encoder - decoder PLMs on the generative QA task ( Lee et al , 2021 ) with T5small ( Raffel et al , 2020 ) . Table 7 shows that KALA largely outperforms baselines even with such a generative PLM .", "entities": []}
{"text": "In this section , we introduce the detailed setups for our models and baselines used in Table 1 , 2 , and 4 .", "entities": []}
{"text": "In this section , we provide the analyses on the forgetting of TAPT , entity memory , number of entities and facts , location of the KLM layer , and values of Gamma and Beta .", "entities": []}
{"text": "In Figure 1 , we observe that the performance of TAPT decreases as the number of training steps increases . To get a concrete intuition on this particular phenomenon , we analysis what happens in the Pre - trained Language Model ( PLM ) , when we further pre - train it on the task - specific corpus . Specifically , in Figure 7", "entities": []}
{"text": "Here we provide the frequency distribution of entities , additional case studies , and more illustrations of textual examples and embedding spaces .", "entities": []}
{"text": "In Figure 16 and 17 , we visualize the examples of the context with its seen and unseen entities and its relational facts . We first confirm that the quality of facts is moderate to use . For instance , in the first example of Figure 16 , the fact in the context that Omar_bin_Laden is son of Osama_bin_Laden , is also appeared in the knowledge graph . In addition , we observe that there are facts that link unseen entities to the seen entities in both Figure 16 and 17 . Thus , while some of the facts in the knowledge graph are not accurate , we can represent the unseen entities with their relation to the seen entities . We expect that there is a still room to improve in terms of the quality of KGs , allowing our KALA to modulate the entity representation more accurately . We leave the study on this as the future work .", "entities": []}
{"text": "The adenomatous polyposis coli ( APC ) tumour - suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta ( GSK - 3beta ) , axin / conductin and betacatenin .", "entities": []}
{"text": "( complex , subclass of , protein ) ( GSK , instance of , protein ) ( glycogen , instance of , protein ) ( APC , instance of , protein ) Context HLA typing for HLA - B27 , HLA - B60 , and HLA - DR1 was performed by polymerase chain reaction with sequence - specific primers , and zygosity was assessed using microsatellite markers .", "entities": []}
{"text": "( microsatellite , subclass of , primers ) ( DR1 , instance of , microsatellite ) ( microsatellite , subclass of , typing )", "entities": []}
{"text": "We identified four germline mutations in three breast cancer families and in one breast - ovarian cancer family . among these were one frameshift mutation , one nonsense mutation , one novel splice site mutation , and one missense mutation .", "entities": []}
{"text": "We count the number of each type of tag in the training set and the validation set , and obtain the data distribution of Not - offensive , offensive - untargeted , offensive - targeted - individual , offensive - targeted - group , offensive - targeted - other , and Not - in - indented - language in Tamil , Malayalam , and Kannada . as shown in Table 1 .", "entities": []}
{"text": "Right for the Right Reason : Evidence Extraction for Trustworthy Tabular Reasoning", "entities": []}
{"text": "Trustworthy inference has an intrinsic sequential causal structure : extract evidence first , then predict the inference label using the extracted evidence data , knowledge / common sense , and perhaps formal reasoning ( Herzig et al , 2021 ; Paranjape et al , 2020 ) 7 . To operationalize this intuition , we chose a two - stage sequential approach which consists of an evidence extraction followed by the NLI classi - fication , as shown in Figure 2 . Notation . The function f in Eq . 2 can be rewritten with functions g and h , f ( . ) = g ( . ) , h g ( . ) , as f ( T , H ) = { g ( T , H ) , h ( g ( T , H ) , H ) } ( 3 ) Here , g extracts the evidence rows T R subset of T , and h uses the extracted evidence T R and the hypothesis H to predict the inference label y , as g ( T , H ) T R h ( T R , H ) y ( 4 ) To obtain f , we need to define the functions g and h , and a flexible representation of a semi - structured . We explore unsupervised ( $ 4.1 ) and supervised ( $ 4.2 ) methods for the evidence row extractor g.", "entities": []}
{"text": "The unsupervised approaches extract Top - K rows are based on relevance scores , where K is a hyperparameter . We use the cosine similarity between the row and the hypothesis sentence representations to score rows . We study three ways to define relevance described next .", "entities": []}
{"text": "For most instances , the number of relevant rows ( K ) is much lower than the total number of rows ( m ) ; most examples have only one or two relevant rows . We constrained the sparsity in the extraction by capping the value of K to S m.", "entities": []}
{"text": "We use a threshold \u03c4 to select rows dynamically Top - K \u03c4 based on the hypothesis , rather than always selecting fixed K rows . We only select rows whose similarity ( after Re - Ranking ) to the hypothesis sentence representations is greater than a threshold \u03c4 . We adopt this strategy because ( a ) the number of rows in the premise table can vary across examples , and ( b ) different hypotheses may require a differing number of evidence rows .", "entities": []}
{"text": "Our experiments assess the efficacy of evidence extraction ( $ 4 ) and its impact on the downstream NLI task by studying the following questions : RQ1 : What is the efficacy of unsupervised approaches for evidence extraction ? ( $ 5.2 )", "entities": []}
{"text": "In this paper , we introduced the problem of Trustworthy Tabular Inference , where a reasoning model both extracts evidence from a table and predicts an inference label . We studied a two - stage approach , comprising an evidence extraction and an inference stage . We explored several unsupervised and supervised strategies for evidence extraction , several of which outperformed prior benchmarks . Finally , we showed that by using only extracted evidence as the premise , our approach outperforms previous baselines on the downstream tabular inference task .", "entities": []}
{"text": "Since many hypothesis sentences ( especially those with neutral labels ) require out - of - table information for inference , we introduced the option to choose out - of - table ( OOT ) pseudo rows , which are highlighted only when the hypothesis requires information that is not common ( i.e. common sense ) and missing from the table . To reduce any possible bias due to unintended associations between the NLI label and the row selections ( e.g. , using OOT for neutral examples ) , we avoid showing inference labels to the annotators 15 . To assess an annotator , we compare their annotations with the majority consensus of other annotators ' ( four ) annotations . We perform this comparison at two levels : ( a ) local - consensus - score on the most recent batch , and ( b ) cumulative - consensusscore on all batches annotated thus far . We use these consensus scores to temporarily ( local - consensus - score ) or permanently ( cumulative score ) block the poor annotators from the task . We also review the annotations manually and provide feedback with more detailed instructions and personalized examples for annotators who were making mistakes due to ambiguity in the task . We give incentives to annotators who received high consensus scores . As in previous work , we removed certain annotators ' annotations that have a poor consensus score ( cumulative score ) and published a second validation HIT to double - check each data point if necessary .", "entities": []}
{"text": "We manually inspect the Type I and Type II error instances for the supervised model and human annotation for the development set . Below , we show some of these examples where models conflict with ground - truth human annotation . We also provide a possible reason behind the model mistakes . Example III Row : The trainer of Caveat is Woody Stephens . Hypothesis : Caveat won more in winnings than it took to raise and train him . Model Prediction : Relevant Evidence Human Ground Truth : Not Relevant . Possible Reason : Model connects the ' raise and train ' term with the trainer name which is unrelated and has no connection to overall , winning races money vs spending for animal . Discussion Based on the observation from the above examples as also stated in $ 5.3 , the model fails on many examples due to its lack of knowledge and common - sense reasoning ability . One possible solution to mitigate this is by the addition of implicit and explicit knowledge on - the - fly for evidence extraction , as done for inference task by Neeraja et al ( 2021 ) .", "entities": []}
{"text": "We manually examine the human - annotated evidence in the development set . We discovered the existence of several relevant phrases / tokens which implicitly indicate the presence of evidence rows . E.g. The existence of tokens such as married , husband , lesbian , and wife in hypothesis ( H ) is very suggestive of the row Spouse being the relevant evidence . Learning such implicit relevance - based phrases and tokens connection is easy for humans and large pre - trained supervision models . It is a challenging task for similarity - based unsupervised extraction methods . Below , we show implicit relevance , indicating token and the corresponding relevant evidence rows . Relevance Indicating Phrase ( H ) Relevant Evidence Rows Key ( T )", "entities": []}
{"text": "The authors thank Bloomberg 's AI Engineering team , especially Ketevan Tsereteli , Anju Kambadur , and Amanda Stent for helpful feedback and directions . We also appreciate the useful feedback provided by Ellen Riloff and the Utah NLP group . Additionally , we appreciate the helpful inputs provided by Atreya Ghosal , Riyaz A. Bhat , Manish Srivastava , and Maneesh Singh . Vivek Gupta acknowledges support from Bloomberg 's Data Science Ph.D. Fellowship . This work was supported in part by National Science Foundation grants # 1801446 ( SaTC ) and # 1822877 ( Cyberlearning ) . Finally , we would like to express our gratitude to the reviewing team for their insightful comments .", "entities": []}
{"text": "X575 : writing rengas with web services", "entities": []}
{"text": "Our software system simulates the classical collaborative Japanese poetry form , renga , made of linked haikus . We used NLP methods wrapped up as web services . This approach is suitable for collaborative human - AI generation , as well as purely computer - generated poetry . Evaluation included a blind survey comparing AI and human haiku . To gather ideas for future work , we examine related research in semiotics , linguistics , and computing .", "entities": []}
{"text": "Computer haikus have been explored in practice at least since Lutz ( 1959 ) . More recently , haikus have been used by Ventura ( 2016 ) as the testbed for a thought experiment on levels of computational creativity . As we will discuss below , the classic haiku traditionally formed the starting verse of a longer poetry jam , resulting in a poem called a renga . A computational exploration of renga writing allows us to return to some of the classical ideas in Japanese poetry via thoroughly modern ideas like concept blending and collaborative AI . Ventura 's creative levels range from randomisation to plagiarisation , memorisation , generalisation , filtration , inception 1 and creation . Further gradations and criteria could be advanced , for example , the fitness function used for filtration could be developed and refined as the system learns . Creativity might be assessed in a social context , as we investigate how a system collaborates . While self - play was a good way for the recently developed board game - playing system AlphaGo to transcend its training data ( Silver et al , 2016 ) , we do not yet have computationally robust qualitative evaluation measures for the poetry domain , where there is no obvious \" winning condition . \" We began by creating a program for generating haikus , trained on a small corpus . Our technical aim then was to simulate the collaborative creation of renga , i.e. , linked haikus . There are several forms of renga with varying constraints ( Carley , 2015 ) , for example the 20 stanza \" Nijiun \" renga which alternates between twoline and three - line verses , with a focus on seasonal symbolism and rules against repetition . 2 Our initial effort was a technical success , however the rengas we produced fail to fully satisfy classical constraints . A subsequent experiment is more convincing in this regard , but still leaves room for improvement . Our discussion considers the aesthetics of the generated poems and outlines directions for future research .", "entities": []}
{"text": "Coleridge considered poetry to be \" the blossom and the fragrance of all human knowledge . \" AI researcher Ruli Manurung defines poetry somewhat more drily : \" A poem is a natural language artefact which simultaneously fulfils the properties of meaningfulness , grammaticality and poeticness \" ( Manurung , 2004 , p. 8 ) . The haiku as we know it was originally called hokku - \u767a\u53e5 , literally the \" starting verse \" of a collaboratively written poem , hakai no renga . Typically each of following links in a renga take the familiar 5/7/5 syllable form . Classical rengas vary in length from two to 100 links ( and , rarely , even 1000 ) . The starting verse is traditionally comprised of two images , with a kireji - a sharp cut - between them . The term haiku introduced by the 19th Century poet Masaoka Shiki supersedes the older term . Stylistically , a haiku captures a moment . In classical renga , all of the verses after the first have additional complex constraints , such as requiring certain images to be used at certain points , but disallowing repetition , with various proximity constraints . The setting in which rengas were composed is also worth commenting on . A few poets would compose together in party atmosphere , with one honoured guest proposing the starting haiku , then the next responding , and continuing in turn , subject to the oversight of a scribe and a renga master . These poetry parties were once so popular and time consuming that they were viewed as a major decadence . Jin'Ichi et al ( 1975 ) offers a useful overview . Because of the way we 've constructed our haiku generating system , it can take an entire haiku as its input topic - we just add the word vectors to make a topic model - and compose a response . This affords AI - to - AI collaboration , or AI - human collaboration . It can also blend two inputs - for example , the previous haiku and the current constraint from the renga ruleset ( e.g. , the requirement to allude to \" cherry blossoms \" or \" the moon \" ) .", "entities": []}
{"text": "In terms of Ventura 's hierarchy of creative levels , the haiku system appears to be in the \" generalisation \" stage . Our renga - writing experiments with FloWr brought in a \" filtration \" aspect . The research themes discussed above point to directions for future work in pursuit of the \" inception \" and \" creativity \" stages . Some previous work with haiku , e.g. Netzer et al ( 2009 ) and Rzepka and Araki ( 2015 ) , have addressed the problem of meaning . The renga form brings these issues to the fore . We hope this early work has motivated further interest in this challenging and enjoyable poetic form that - like other less constrained forms of dialogue - combines themes of natural language generation and understanding . One natural next step would be a series of experiments in collaborative human - AI generation of rengas . Our haiku software is available for future experiments . 7", "entities": []}
{"text": "This research was supported by the Future and Emerging Technologies ( FET ) programme within the Seventh Framework Programme for Research of the European Commission , under FET - Open Grant number 611553 ( COINVENT ) .", "entities": []}
{"text": "We then used K - means with two cluster centroids to label each point in the space based on that point 's distance from the nearest cluster centroid . We did this with both the dimensionalityreduced sentence representations and the original 768 - dimensional vectors . The sentence representations and the K - means labels were then evaluated using the aforementioned evaluation metrics .", "entities": []}
{"text": "The evaluation metrics for the K - means labeled points in the space does not seem to correspond to the IAA values . The lowest scoring dogwhistles , \" refugee policy \" and \" remigration \" , cluster fairly well compared to the other dogwhistles with higher IAA values .", "entities": []}
{"text": "The results for the evaluation metrics on the human labeled points indicate that there is an overall correspondence between the IAA and those measurements : the lowest rated IAA dogwhistles always have the lowest clustering score . This indicates that there is a semantic distinction between in - group responses and out - group responses that is captured fairly well by sentence transformers .", "entities": []}
{"text": "Funding for this work was provided by the Gothenburg Research Initiative for Politically Emergent Systems ( GRIPES ) supported by the Marianne and Marcus Wallenberg Foundation grant 2019.0214 . Christoffer Olssson assisted with some of the annotations used in the work .", "entities": []}
{"text": "The idea of weighting training examples has a long history . Importance sampling ( Kahn and Marshall , 1953 ) assigns weights to different samples and changes the data distribution . Boosting algorithms such as AdaBoost ( Kanduri et al , 2018 ) select harder examples to train subsequent classifiers . Similarly , hard example mining ( Malisiewicz et al , 2011 ) downsamples the majority class and exploits the most difficult examples . Oversampling ( Chen et al , 2010 ; Chawla et al , 2002 ) ( Jiang et al , 2017 ; Fan et al , 2018 ) proposed to learn a separate network to predict sample weights .", "entities": []}
{"text": "We thank all anonymous reviewers , as well as Qinghong Han , Wei Wu and Jiawei Wu for their comments and suggestions . The work is supported by the National Natural Science Foundation of China ( NSFC No . 61625107 and 61751209 ) .", "entities": []}
{"text": "Using Linguistic Features to Predict the Response Process Complexity Associated with Answering Clinical MCQs", "entities": []}
{"text": "This study examines the relationship between the linguistic characteristics of a test item and the complexity of the response process required to answer it correctly . Using data from a large - scale medical licensing exam , clustering methods identified items that were similar with respect to their relative difficulty and relative response - time intensiveness to create low response process complexity and high response process complexity item classes . Interpretable models were used to investigate the linguistic features that best differentiated between these classes from a descriptive and predictive framework . Results suggest that nuanced features such as the number of ambiguous medical terms help explain response process complexity beyond superficial item characteristics such as word count . Yet , although linguistic features carry signal relevant to response process complexity , the classification of individual items remains challenging .", "entities": []}
{"text": "We use a set of interpretable linguistic features , many of which were previously used for predicting item difficulty ( Ha et al , 2019 ) and response time in the domain of clinical MCQs . These features were extracted using code made available by Ha et al ( 2019 ) and to these , we add several predictors specifically related to the medical content of the items , as well as standard item metadata .", "entities": []}
{"text": "As noted , this study replicates the feature extraction procedure described and made available by Ha et al ( 2019 ) . Approximately 90 linguistic features were extracted from each item 's text ( the full item including answer options ) and are summarized in Table 2 . They span several levels of linguistic processing including surface lexical and syntactic features , semantic features that account for ambiguity , and cognitively motivated features that capture properties such as imageability and familiarity . Common readability formulae are used to account for surface reading difficulty . The organization of ideas in the text is captured through text cohesion features that measure the number and types of connective words within an item . Finally , word frequency features ( including threshold frequencies ) measure the extent to which items utilize frequent vocabulary . Combinations of these features have the potential to capture different aspects of item content that are relevant to response complexity . For example , medical terms can be expected to have lower absolute frequencies and familiarity ratings , among other characteristics , and combinations of these features may suggest a higher density of medical terms and specialized language in some items compared to others . Another example is the temporal organization of the information about the patient history and symptoms described in the item and captured by temporal connectives , where it is reasonable to expect that more temporally intricate cases would require higher response process complexity to solve . Similarly , a high number of causal connectives would indicate a higher complexity of causal relationships among the events that led to the patient seeing a doctor , which may also be associated with higher cognitive demands .", "entities": []}
{"text": "This group of features refers to metadata describing item content . Presence of an image is a binary categorical variable indicating whether the item includes an image such an X - ray or an MRI that needs to be examined . Another variable is Content category , which describes 18 generic topic categories such as \" Cardiovascular \" , \" Gastrointestinal \" , \" Behavioral Health \" , ' Immune System \" , and so on . Another variable , Physician Task describes tasks required by the item , e.g. , determine a diagnosis , choose the correct medicine , apply foundational science concepts , and others . Finally , we also include the Year the item was administered as a predictor ( 2010 - 2015 ) to account for potential changes in response process complexity and examinee samples over time .", "entities": []}
{"text": "Three classification baselines were computed to benchmark the predictive benefit given by linguistics features over standard item characteristics : Majority Class Baseline : Since the lowcomplexity class contains a higher number of items , it is more likely that an item would be correctly predicted as belonging to this class . Word Count : This baseline examines the possibility that response process complexity is simply a function of item length . Standard Item Features : This baseline comprises Word count , Presence of an image , Content category , Physician task and Year . This model reflects the standard item characteristics that most testing organizations would routinely store .", "entities": []}
{"text": "The output of the selected - features prediction model was analyzed further in order to get insight into this model 's performance . As could be expected , the majority class of low - complexity items was predicted more accurately than the highcomplexity class , as shown by the confusion matrix in Table 4 . An interesting observation was made during a follow - up classification experiment , which showed that this effect remained when using balanced classes 4 . This shows that the success in predicting this class can not be attributed solely to its prevalence but potentially also to its high homogeneity compared to the high - complexity class . Next , we plot the model errors across the two classes of low - complexity and high - complexity items , as shown in Figure 3 . Notably , items with average response times below 150 seconds were predicted as low - complexity most of the time , with minimal consideration of their p - value . This shows that what the model effectively learned was to distinguish between items with long and short mean response times , which overpowered its ability to predict the p - value parameter . This finding is consistent with previous work , where response times in were predicted more successfully than p - value using a similar set of linguistic features in Ha et al ( 2019 ) . Finally , analysis of the feature distributions across these four classes revealed no unexpected patterns .", "entities": []}
{"text": "The experiments presented in this paper are , to the best of our knowledge , the first investigation of the relationship between item text and response process complexity . The results showed that such a relationship exists . To the extent that items were written as clearly and as concisely as possible , the findings suggest that high - complexity medical items generally include longer phrases , more medical terms , and more specific descriptions . While the models outperformed several baselines , they required a large number of features to do so and the predictive utility remained low . Ultimately , this shows the challenging nature of modeling response process complexity using interpretable models and the lack of a straightforward way to manipulate this item property .", "entities": []}
{"text": "In this section , we describe our three system runs . The ideas behind our methods are independent of the word order in a sentence . Our first method is unsupervised , whereas the other two methods are supervised . The first and second method share the same preprocessing .", "entities": []}
{"text": "Our first method is unsupervised . It measures the overlap between the tokens in sentence s 1 and the tokens in sentence s 2 .", "entities": []}
{"text": "You should do it . You can do it , too . 1 4.39817 Unfortunately the answer to your question is we simply do not know . My answer to your question is \" Probably Not \" . 1 3.70982 P ( A | B ) is the conditional probability of A , given B. P ( B | A ) is the conditional probability of B given A.", "entities": []}
{"text": "This work was partially funded by the PhD program Online Participation , supported by the North Rhine - Westphalian funding scheme Fortschrittskollegs and by the German Federal Ministry of Economics and Technology under the ZIM program ( Grant No . KF2846504 ) . The authors want to thank the anonymous reviewers for their suggestions and comments .", "entities": []}
{"text": "Here , we formalize the problem of answering user queries from product specifications . Given a question Q about a product P and the list of M specifications { s 1 , s 2 , ... , s M } of P , our objective is to identify the specification s i that can help answer Q. Here , we assume that the question is answerable from specifications .", "entities": []}
{"text": "Diversifying Content Generation for Commonsense Reasoning with Mixture of Knowledge Graph Experts", "entities": []}
{"text": "The KG - enhanced generative reasoning module is illustrated in Figure 2 . It consists of four steps . First , a sequence - associated subgraph is retrieved from the KG given the input sequence ( 3.1.1 ) . Then , a multi - relational graph encoder iteratively updates the representation of each node by aggregating information from its neighboring nodes and edges ( 3 . 1.2 ) . Next , the model selects salient concepts that should be considered during generation ( 3 . 1.3 ) . Finally , the model generates outputs by integrating the token embeddings of both the input sequence and the top - ranked concepts ( 3.1.4 ) .", "entities": []}
{"text": "To facilitate the reasoning process , we resort to an external commonsense knowledge graph G = { V , E } , where V denotes the concept set and E denotes the edges with relations . Since direct reasoning on the entire graph is intractable , we extract a sequence - associated subgraph G x = { V x , E x } , where V x consists of the concepts extracted from the input sequence ( denoted as C x ) and their inter - connected concepts within two hops , i.e. , V x = { C x \u222a N ( C x ) \u222a N ( N ( C x ) ) } . For exam - ple , in Figure 2 , C x = { piano , sport , kind } and V x = { piano , sport , kind , art , music , press , ... } . Next , the generation task is to maximize the conditional probability p ( y | x , G x ) .", "entities": []}
{"text": ". Distinct - k ( Li et al , 2016 ) measures the total number of unique k - grams normalized by the total number of generated k - gram tokens to avoid favoring long sentences . Entropyk reflects how evenly the empirical k - gram distribution is for a given sentence when word frequency is considered .", "entities": []}
{"text": "( 2 ) Billy wanted to go to the zoo . He saw elephants . ( 3 ) Billy went to the store and bought an elephant . ( 1 ) Billy 's parents sent him on an African safari for a reward . ( 2 ) He went to the zoo later in the day and saw elephants . ( 1 ) Billy wanted to go to the zoo and see elephants . ( 2 ) Billy was excited to go on his trip to the zoo . ( 3 ) Billy went to the zoo to see the animals .", "entities": []}
{"text": "Nucleus", "entities": []}
{"text": "In this paper , we proposed a novel method that diversified the generative reasoning by a mixture of expert strategy on commonsense knowledge graph . To the best of our knowledge , this is the first work to boost diversity in NLG by diversifying knowledge reasoning on commonsense knowledge graph . Experiments on two generative commonsense reasoning benchmarks demonstrated that MoKGE outperformed state - of - the - art methods on diversity , while achieving on par performance on quality .", "entities": []}
{"text": "The work is supported by NSF IIS - 1849816 , CCF - 1901059 , IIS - 2119531 and IIS - 2142827 .", "entities": []}
{"text": "We have collected a Swedish dataset , henceforth referred to as SweQUAD - MC , consisting of texts and MCQs for the given texts . The dataset was created by three paid linguistics students instructed to pose unambiguous and independent questions . They were also asked to identify the key with at least two distractors , all of which are contiguous phrases in a given text . Additionally , as the distractors were required to be in the same grammatical form as the key ( e.g. , both in plural ) , the students were allowed to change the grammatical form of phrases if they constituted plausible distractors after this change . The exact instructions given to the students along with more details on the used texts are provided in Appendix A. Each datapoint in SweQUAD - MC consists of a base text and an MCQ , i.e. a stem , the key and at least two distractors . The same text can be reused for different MCQs , but the sets of texts in training ( \u223c 80 % ) , development ( \u223c 10 % ) and test ( \u223c 10 % ) datasets are disjoint . However , some overlap in sentences is possible , since the texts might come from the same source . Descriptive statistics of all SweQUAD - MC splits is provided in Table 1 .", "entities": []}
{"text": "We have used publicly available texts from the websites of Swedish government agencies . The exact list of URLs is provided in the GitHub repository associated with the paper . The exact instructions given to students recruited to collect SweQUAD - MC dataset ( and their translation to English ) are presented in Figure 6 . In addition to the given instructions , the students were also given the opportunity to slightly reformulate the distractors found in the text in order to align the syntactic structure with that of the key .", "entities": []}
{"text": "In addition to the metrics 1 - 10 presented in Section 6.1 , we have also looked at the following ones ( MCQ% means \" Percentage of MCQ \" and DIS means \" generated distractor ( s ) \" ) The rationale behind metric 11 was that capitalized answers are named entities and thus one would like distractors also to be named entities . However , it does not always hold . For instance , consider the stem \" Who gets an e - mail with a confirmation of a successful submission of the application for the work permit ? \" and the key \" you and your employer \" . A distractor \" Migration Agency \" would suit the question perfectly , although capitalization is clearly different . Metrics 12 - 17 were candidates to become overfitting indicators . However , metric 2 was excluded , since AnyDisFromTrainDis is more informative , given phrases used as distractors in training data can be repeated in other texts . Metrics 13 - 14 were excluded , since it 's unclear whether the higher or lower values are better . For instance , if a text from the training data and the given text are thematically similar , would copying a distractor from training data be considered overfitting ? Metrics 15 - 17 were rejected as too strict , leaving the possibility of actually missing overfitting if only 2 of 3 distractors would meet the criteria .", "entities": []}
{"text": "We used one sample t - test for conducting our analysis and thus the following assumptions were checked for . 1 . The variable under study should be either an interval or ratio variable . Our variable , the number of correctly answered MCQs , is clearly on a ratio scale .", "entities": []}
{"text": "The exact guidelines given to the teachers and their translation to English , are presented in Figure 10 .", "entities": []}
{"text": "Vad t\u00e4cker\u00f6ver h\u00e4lften av Sveriges yta ? ( What covers more than half of the surface of Sweden ? )", "entities": []}
{"text": "This work was supported by Vinnova ( Sweden 's Innovation Agency ) within project 2019 - 02997 . We would like to thank the anonymous reviewers for their comments , as well as Gabriel Skantze and Bram Willemsen for their helpful feedback prior to the submission of the paper .", "entities": []}
{"text": "Much Gracias : Semi - supervised Code - switch Detection for Spanish - English : How far can we get ?", "entities": []}
{"text": "In this section , we will first describe the manually annotated code - switched data that we use for evaluating our models , then we describe the monolingual data that we will use as \" training \" data . It should be noted that this is not real training data , as it is not annotated for the task at hand ( thus the setting is semi - supervised ) .", "entities": []}
{"text": "In order to perform semi - supervised codeswitching detection , we use Wikipedia data , because it is available in many languages and easy to obtain . We extracted dumps from September 1st 2020 with Wikiextractor 2 . Without punctuation and numbers , the English dataset contains 420 K distinct words and the Spanish dataset contains 610 K distinct words . It should be noted that there is a domain difference between the training and the dev / test data . However , collecting monolingual data from Twitter 2 https://github.com/attardi/ wikiextractor is non - trivial . 3 Furthermore , it should be noted that the Wikipedia datasets are not 100 % monolingual , so there will be some Spanish data in the English dump and vice - versa . Both of these artefacts might have a negative effect on performance .", "entities": []}
{"text": "Tokenization of the raw datasets is done using the English and Spanish SpaCy tokenization models 4 , as it matches the tokenization of the development and test sets . Punctuation and non - word tokens ( the other class ) are identified with manually designed rules using regular expressions , and the python emoji package . Tokens that are not identified as other , are labeled with the corresponding label based on the language of the wikipedia . 3 Methods", "entities": []}
{"text": "We first clean the mono - lingual Wikipedia data by removing XML / HTML tags from the articles and special tokens that belong to the other class . We calculate the word probability based on the resulting data ( word frequency / total number of words ) using Laplace smoothing with a smoothing factor of 1 .", "entities": []}
{"text": "We also experiment with taking a larger context into account through bi - grams and tri - grams . Here , we divide the frequency of the n - gram containing the word with the frequency of the leading ( n \u2212 1 ) gram . The probability is computed this way for a given word in each language , and then the label with the highest probability is assigned to the word . Laplace smoothing with a factor of 1 is used . In our initial experiments , tri - grams showed very low performance , so we use bi - grams in the remainder of this paper .", "entities": []}
{"text": "For this model , we calculate the joint log probability of words based on the monolingual training data , and assign the most probable label . We vary the n - gram size from 1 to 6 and use Laplace smoothing with a factor of 1 .", "entities": []}
{"text": "We also experiment with ensembling the previous methods , where we use a simple majority voting . We compare using all models , to using the best 3 and the best 5 models , as well as an oracle .", "entities": []}
{"text": "Universal schemas : We compared our method with semi - supervised methods based on universal schemas ( Toutanova et al , 2015 ; Verga et al , 2017 ) . In our experiments , we used the same encoder as ( Song et al , 2018 ) to encode each surface pattern . 9 We tested two types of scoring functions , Model F and Model E , as in ( Toutanova et al , 2015 ) . 10 , 11", "entities": []}
{"text": "We thank all the EMNLP reviewers and Daniel Andrade for their valuable comments and suggestions to improve the paper .", "entities": []}
{"text": "To gain further insights , we compared ZAR results with English translations automatically generated by the corresponding MT model . Figure 4 gives two examples . It is no great surprise that the translation quality was not satisfactory because we did not fully optimize the model for it . In the exmple of Figure 4 ( a ) , MT seems to have helped ZAR . The omitted nominative argument of \" \u3042\u308a \" ( is ) was correctly translated as \" the school \" , and the model successfully identified its antecedent \" \u5b66\u6821 \" ( school ) while the baseline failed . Figure 4 ( b ) illustrates a limitation of the proposed approach . The omitted nominative argument of the predicate , \" \u3067 \" ( be ) , points to \" \u5b9a\u5409 \" ( Sadakichi , the father of Jutaro ) . Although the model correctly translated the zero pronoun as \" He \" , it failed in ZAR . This is probably because not only \" \u5b9a\u5409 ( Sadakichi ) \" but also \" \u9f8d\u99ac \" ( Ryoma ) and \" \u91cd\u592a\u90ce \" ( Jutaro ) can be referred to as \" He \" . When disambiguation is not required to generate an overt pronoun , MT is not very helpful .", "entities": []}
{"text": "We thank the Yomiuri Shimbun for providing Japanese - English parallel texts . We are grateful for Nobuhiro Ueda ' help in setting up the baseline model . We thank the anonymous reviewers for their insightful comments .", "entities": []}
{"text": "Although we followed Ueda et al ( 2020 ) with respect to hyper - parameter settings , there was one exception . Verbal predicate analysis is conventionally divided into three types : overt , covert , and zero . While Ueda et al ( 2020 ) excluded the easiest overt type from training , we targeted all the three types because we found slight performance improvements . The overt type covers situations", "entities": []}
{"text": "Tables 9 and 10 show the performance on the validation sets .", "entities": []}
{"text": "Resolution and Zero Pronoun Translatoin", "entities": []}
{"text": "As it was mentioned before , nowadays there are numerous avenues for event data acquisition . For this paper Twitter was chosen as the social network to use for retrieving data since this data is easily available and a good amount of it is related to events of different categories . Twitter 's REST API was used in order to retrieve data related to these events : Each dataset had a file per day with all the tweets from the day and contained only the text that represents a tweet per line .", "entities": []}
{"text": "With the raw data ready to be used , the preprocessing step followed . The sequence followed is exposed below : 1 . Removing punctuation and unicode only characters except written accents .", "entities": []}
{"text": "Each tokenized tweet al o contains a reference to the original , unprocessed tweet , which will be used on Section 5 .", "entities": []}
{"text": "In this subsection UDPipe is also used in order to extract the syntactic dependencies , in particular , the focus is to obtain ' dobj ' and ' iobj ' objects , which refer to direct and indirect object respectively , and then obtain the root verb they stem from . By doing this a verb can be linked to each object and furthermore , the entities related to verb , which were obtained from the Formal Context , can be linked to each object . Doing this allows us to add activities for each entity , as well as create a relationship between two entities where one of them appears as an object in the action of another .", "entities": []}
{"text": "We want to augment existing labeled utterances by generating additional novel utterances in a desired target language . In our case , existing data consists of feature - unrelated data ( intents and slots already supported ) spanning all languages and featurerelated data , which is available in a source language but is small ( few - shot ) or not available ( zero shot ) in other languages . For generation , we first extract the intent and slot types from the available data . We then generate a new utterance by conditioning a multilingual language model on the intent , slot types and the target language . We refer to utterances that have the same intent and slot types as paraphrases of each other since they convey the same meaning in the context of the SLU system .", "entities": []}
{"text": "We evaluate our approach by simulating few - shot and zero - shot feature bootstrapping scenarios .", "entities": []}
{"text": "Paraphrases generated in different languages for a given input are shown in Table 6 . The intent is airline and the slots are fromloc.city_name for columbus and toloc.city_name for minneapolis . For this intent and the slots , the generated paraphrase in German ( translated to English ) is Show me all the airlines that fly from Toronto to Boston . The desired intent , that is airline is realized in the gener - ated paraphrase . Additionally , Toronto and Boston are the slot values respectively for the slot types fromloc.city_name and toloc.city_name . For Spanish , the generated paraphrase ( translated to English ) is Which Airlines Fly from Atlanta to Philadelphia . The airline intent is realized in the generated paraphrase and also Atlanta and Philadelphia are the slot values produced associated with the desired slot types . As illustrated by the examples , the model is free to pick a specific slot value during generation , leading to variations across languages , but all are consistent with the slot type .", "entities": []}
{"text": "We would like to thank our anonymous reviewers for their thoughtful comments and suggestions that improved the final version of this paper .", "entities": []}
{"text": "PBoS : Probabilistic Bag - of - Subwords for Generalizing Word Embedding", "entities": []}
{"text": "show that word vectors generated by PBoS have better quality compared to previously proposed models across languages ( Section 4.3 and 4.4 ) .", "entities": []}
{"text": "Exchanging the order of summations in Eq . ( 5 ) from segmentation first to subword first , we get w = s\u2286w a s | w s ( 7 ) where a s | w \u221d g Segw , g s p g | w ( 8 ) is the weight accumulated over subword s , summing over all segmentations of w that contain s. 5 Eq . ( 7 ) provides an alternative view of the word vector composed by our model : a weighted sum of all the word 's subword vectors . Comparing to BoS , we assign different importance a s | w , instead of a uniform weight , to each subword . a s | w can be viewed as the likelihood of subword s being a meaningful segment of the particular word w , considering both the likelihood of s itself being meaningful , and at the same time how likely the rest of the word can still be segmented into meaningful subwords . Example . Consider the contribution of subword s = \" gher \" in word w = \" higher \" . Possible contributions only come from segmentations that contain \" higher \" : g 1 = ( \" h \" , \" i \" , \" gher \" ) and g 2 = ( \" hi \" , \" gher \" ) . Each segmentation g adds weight p g | w to a s | w . In this case , a \" gher \" | w will be smaller than a \" er \" | w because both p g 1 | w and p g 2 | w would be rather small .", "entities": []}
{"text": "We further assess the quality of generated word embedding via the extrinsic task of POS tagging . The task is to categorize each word in a given context into a particular part of speech , e.g. noun , verb , and adjective .", "entities": []}
{"text": "Here we list the details of our experiments that are omitted in the main paper due to space constraints . We run all our experiments on a machine with an 8 - core Intel i7 - 6700 CPU @ 3.40GHz , 32 GB Memory , and GeForce GTX 970 GPU .", "entities": []}
{"text": "The meaning of hyperparameters shown in Table 6 , Table 7 and Table 8 as explained as follows .", "entities": []}
{"text": "min len : The minimum length for a subword to be considered . max len : The maximum length for a subword to be considered . word boundary : Whether to add special characters to annotate word boundaries .", "entities": []}
{"text": "The authors would like to thank anonymous reviewers of EMNLP for their comments . ZJ would like to thank Xuezhou Zhang , Sidharth Mudgal , Matt Du and Harit Vishwakarma for their helpful discussions .", "entities": []}
{"text": "Ntrain is the number of training instances for the POS tagging model . OOV % is the percentage of the words in the POS tagging testing set that is out of the vocabulary of the Polyglot vectors in that language . Experimental results are included for convenience .", "entities": []}
{"text": "Answering complex questions that involve multiple entities and multiple relations using a standard knowledge base is an open and challenging task . Most existing KBQA approaches focus on simpler questions and do not work very well on complex questions because they were not able to simultaneously represent the question and the corresponding complex query structure . In this work , we encode such complex query structure into a uniform vector representation , and thus successfully capture the interactions between individual semantic components within a complex question . This approach consistently outperforms existing methods on complex questions while staying competitive on simple questions .", "entities": []}
{"text": "Entity", "entities": []}
{"text": "In this section , we introduce the QA datasets and state - of - the - art systems that we compare . We show the end - to - end results of the KBQA task , and perform detail analysis to investigate the importance of different modules used in our approach .", "entities": []}
{"text": "To the best of our knowledge , this is the first work to handle complex KBQA task by explicitly encoding the complete semantics of a complex query graph using neural networks . We stud - ied different methods to further improve the performance , mainly leveraging dependency parse and the ensemble method for linking enrichment . Our model becomes the state - of - the - art on Com - plexQuestions dataset , and produces competitive results on other simple question based datasets . Possible future work includes supporting more complex semantics like implicit time constraints .", "entities": []}
{"text": "Kenny Q. Zhu is the contact author and was supported by NSFC grants 91646205 and 61373031 . Thanks to the anonymous reviewers for their valuable feedback .", "entities": []}
{"text": "Multi - Model and Crosslingual Dependency Analysis", "entities": []}
{"text": "We would like to thank the developpers of Bist - Parser , Eliyahu Kiperwasser and Yoav Goldberg , and the developper of the CNN library ( Chris Dyer ) for making them available as open source on GitHub . Finally we would like to thank our colleague Ghislain Putois for help on all aspects on neural networks .", "entities": []}
{"text": "In this section , we first provide the conventional definition of DST and then extend the definition to the noisy label learning scenario .", "entities": []}
{"text": "Let X = { ( R 1 , U 1 ) , . . . , ( R T , U T ) } denote a dialogue of T turns , where R t and U t represent the system response and user utterance at turn t , respectively . The dialogue state at turn t is defined as B t = { ( s , v t ) | s S } , where S denotes the set of predefined slots and v t is the corresponding value of slot s. Following previous work Hu et al , 2020 ; Ye et al , 2021b ) , a slot in this paper refers to the concatenation of the domain name and slot name so as to include the domain information . For example , we use \" hotel - name \" to represent the slot \" name \" in the hotel domain . In general , the issue of DST is defined as learning a dialogue state tracker F : X t B t that takes the dialogue context X t as input and predicts the dialogue state B t at each turn t as accurately as possible . Here , X t represents the dialogue history up to turn t , i.e. , X t = { ( R 1 , U 1 ) , . . . , ( R t , U t ) } .", "entities": []}
{"text": "We introduce a general framework ASSIST , aiming to train DST models robustly from noisy labels . We assume that a small clean dataset is accessible . Based on this dataset , ASSIST first trains an auxiliary model A. Then , it leverages A to generate pseudo labels for each sample in the noisy training set . The pseudo state annotations are represented asB t = { ( s , v t ) | s S } , wherev t denotes the pseudo label of slot s at turn t. Afterwards , both the generated pseudo labels and vanilla noisy labels are exploited to train the primary model F * . That is , we intend to learn F * : X t C ( B t , B t ) , where C ( B t , B t ) is a combination ofB t andB t . Essentially , any existing DST models can be employed as the auxiliary model . However , these models may lead to overfitting due to the small size of the clean dataset . To tackle this issue , we propose a new simple model as the auxiliary model 1 .", "entities": []}
{"text": "The slot - value matching module is utilized to predict the value of each slot s. It first calculates the distance between the slot - specific representation g s t and the semantic representation of each candidate value v V s , i.e. , h v . Then , the candidate value with the smallest distance is selected as the prediction . The 2 norm is adopted to compute the distance . Denotingv t as the predicted value of slot s at turn t , we have : v t = argmin v Vs g s t \u2212 h v 2 .", "entities": []}
{"text": "We leverage a small clean dataset to train the auxiliary model . Since the true labels are available , the auxiliary model is directly trained to maximize the joint probability of all slot values . The probability of the true value v t of slot s at turn t is defined as : p ( v t | X t , s ) = exp ( \u2212 g s t \u2212 h v t 2 ) v Vs exp ( \u2212 g s t \u2212 h v 2 ) , where h v t is the semantic representation of v t . Maximizing the joint probability \u03a0 ( s , vt ) Bt p ( v t | X t , s ) is equivalent to minimizing the following objective : L aux = ( s , vt ) Bt \u2212 log p ( v t | X t , s ) .", "entities": []}
{"text": "We also test using the proposed auxiliary model as the primary model . For the sake of description , we refer to this model as AUX - DST .", "entities": []}
{"text": "Considering that our proposed framework ASSIST relies on a small clean dataset to train the auxiliary model that is further leveraged to generate pseudo labels for the training set , it is valuable to explore the effects of the size of the clean dataset on the performance of the primary model . For this purpose , we vary the number of dialogues in the clean dataset from 500 to 1000 4 to generate different pseudo labels . We then combine these different pseudo labels with the vanilla labels to train the primary model AUX - DST . The results on Multi - WOZ 2.4 are reported in Figure 5 . For comparison , 4 There are 1000 dialogues in total in the validation set . we also include the results when only the pseudo labels or only the vanilla labels are used to train the primary model . As can be seen , the size of the clean dataset has a great impact on the performance of the primary model . Apparently , fewer clean data will lead to worse performance . Nevertheless , as long as the pseudo labels are combined with the vanilla labels , the primary model can consistently demonstrate the strongest performance .", "entities": []}
{"text": "The previous experiments have proven the effectiveness of the generated pseudo labels in training robust DST models . In this part , we provide further analyses on the quality of the pseudo labels to gain more insights into why they can be beneficial .", "entities": []}
{"text": "To intuitively understand the quality of the pseudo labels , we show four dialogue snippets with their vanilla labels and the generated pseudo labels in Table 2 . As can be seen , the vanilla labels of the first two dialogue snippets are incomplete , while all the missing information is presented in the pseudo labels . For the third dialogue snippet , the vanilla labels contain an unmentioned slot - value pair \" ( trainarriveby , 13:03 ) \" . This error has also been fixed in the pseudo labels . For the last dialogue snippet , the vanilla labels are correct . However , the pseudo labels introduce an overconfident prediction of the value of slot \" hotel - area \" . This case study has verified again that the pseudo labels can be utilized to fix certain errors in the vanilla labels . However , the pseudo labels may bring about some new errors . Hence , we should combine the two types of labels so as to achieve the best performance .", "entities": []}
{"text": "In this section , we briefly review related work on DST and noisy label learning .", "entities": []}
{"text": "Proof . Our proof is based on the bias - variance decomposition theorem 5 . For any sample X t in the noisy training set D n , the approximation error with respect to the pseudo labelv t of slot s is defined as ] , which , according to the biasvariance decomposition theorem , can be decomposed into a bias term and a variance term , i.e. , where In our approach , the auxiliary model is a BERTbased model , which has more than 110 M parameters . Such a complex model is expected to be able to capture all the samples in the small clean dataset D c . Therefore , we can reasonably assume that the bias term is close to zero . Then , we have :", "entities": []}
{"text": "Instead of using only two languages ( source and target ) for training an NMT model , using multiple languages has been shown to help in low resource scenarios . For example , it might be the case that a certain pair of languages have very little parallel data between them , but there exists a third language with abundant parallel data with the original two languages . This third language acts as a pivot and helps in improving NMT between the two languages ( Aharoni et al , 2019 ; Gu et al , 2018 ; Liu et al , 2020 ; Zhang et al , 2020 ) .", "entities": []}
{"text": "We use hi \u2194 mr and es \u2194 pt language pairs for our experiments .", "entities": []}
{"text": "We thank the IIT Delhi HPC facility 3 for the computational resources . We are also thankful to Ganesh Ramakrishnan and Pawan Goyal for initial discussions on the project . Parag Singla is supported", "entities": []}
{"text": "To perform a systematic comparison of errors across different models , we investigate the predictions based on the following criteria .", "entities": []}
{"text": "We analyze the impact of passage length on errors , since this can be an important factor in determining the difficulty of understanding the passage . As seen in Figure 1 , DocQA performs the best on shorter passages , while R - Net and BiDAF are observed to be better for longer passages . However , there are no systematic error patterns and overall error rates , surprisingly , are not much higher for longer passages . This means that predictions on long passages are almost as good as on short ( presumably easier to understand ) passages .", "entities": []}
{"text": "We also do a similar error analysis for questions of different lengths . Since there are very few questions which have length greater than 30 , the estimate for range 30 - 34 is not very reliable . In Figure 2 , we observe that the error rate first decreases and then increases for BiDAF , DrQA and DocQA . A plausible explanation for this is that shorter questions contain insufficient information in order to be able to select the correct answer span and can hence be confusing , but it also becomes difficult for end - to - end neural models to learn a good representation when the question becomes longer and syntactically more complicated . However , R - Net has an irregular trend with respect to question length , which is difficult to explain .", "entities": []}
{"text": "For answers of varying lengths , the error rates are shown in Figure 3 . Again , estimates for answers with length > 16 are not very reliable since data is sparse for high answer lengths . Here , we observe an increasing trend initially and then a slight decrease ( bell shape ) . This conforms to the hypothesis that shorter answers are easier to predict than longer answers , but only up to a certain answer length ( observed to be around 7 for most models ) . The slightly better performance for very long answers is likely due to such answers having a higher chance of being ( almost ) entire sentences with simpler questions being asked about them .", "entities": []}
{"text": "Incorrect answer boundary ( longer ) : This error category includes those cases where the predicted span is longer than the ground truth answer , but contains the answer . Incorrect answer boundary ( shorter ) : This error category includes those cases where the predicted span is shorter than the ground truth answer , and is a substring of the answer . Soft Correct : This error category includes those cases where the prediction is actually correct , but due to inclusion / exclusion of certain question terms ( such as units ) along with the answer , it is deemed incorrect .", "entities": []}
{"text": "Multi - Sentence : This error category includes those cases where inference is required to be performed across 2 or more sentences in the given passage to be able to arrive at the answer , which leads to an incorrect prediction based on only 1 passage sentence . Paraphrase : This error category includes those cases where the question paraphrases certain parts of the sentence that it is asking about which makes lexical pattern matching difficult and leads to errors in prediction .", "entities": []}
{"text": "This error category includes those cases where the question is about an entity type which is present multiple times in the passage and the model returns a different entity than the ground truth entity but of the same type . Requires World Knowledge : This error category includes questions which can not be answered using the given passage alone and require external knowledge to solve , leading to incorrect predictions . Missing Inference : This category includes inference - related errors which do n't belong to any of the other categories mentioned above .", "entities": []}
{"text": "We would like to thank Chaitanya Malaviya and Abhishek Chinni for their valuable feedback , and the Language Technologies Institute at CMU for the GPU resources used in this work . We are also very grateful to the anonymous reviewers for their insightful comments and suggestions , which helped us polish the presentation of our work .", "entities": []}
{"text": "One of the recent challenges in machine learning ( ML ) is interpreting the predictions made by models , especially deep neural networks . Understanding models is not only beneficial , but necessary for wide - spread adoption of more complex ( and potentially more accurate ) ML models . From healthcare to financial domains , regulatory agencies mandate entities to provide explanations for their decisions ( Goodman and Flaxman , 2016 ) . Hence , most machine learning progress made in those areas is hindered by a lack of model explainability - causing practitioners to resort to simpler , potentially low - performance models . To supply for this demand , there has been many attempts for model interpretation in recent years for tree - based algorithms ( Lundberg et al , 2018 ) and deep learning algorithms ( Lundberg and Lee , 2017 ; Smilkov et al , 2017 ; Sundararajan et al , 2017 ; Bach et al , 2015 ; Dhurandhar et al , 2018 ) .", "entities": []}
{"text": "Integrated Gradients ( Sundararajan et al , 2017 ) is a model attribution technique applicable to all models that have differentiable inputs w.r.t . outputs . IG produces feature attributions relative to an uninformative baseline . This baseline input is designed to produce a high - entropy prediction representing uncertainty . IG , then , interpolates the baseline towards the actual input , with the prediction moving from uncertainty to certainty in the process . Building on the notion that the gradient of a function , f , with respect to input can characterize sensitivity of f for each input dimension , IG simply aggregates the gradients of f with respect to the input along this path using a path integral . The crux of using path integral rather than overall gradient at the input is that f 's gradients might have been saturated around the input and integrating over a path alleviates this phenomenon . Even though there can be infinitely many paths from a baseline to input point , Integrated Gradients takes the straight path between the two . We give the formal definition from the original paper in 2.2 . Definition 2.2 . Given an input x and baseline x , the integrated gradient along the i th dimension is defined as follows . IG i ( x , x ) : : = ( x i \u2212 x i ) \u00d7 1 \u03b1=0 \u2202f ( x + \u03b1\u00d7 ( x\u2212x ) ) \u2202x i d\u03b1 ( 1 ) where \u2202f ( x ) \u2202x i represents the gradient of f along the i th dimension at x. In the NLP setting , x is the concatenated embedding of the input sequence . The attribution of each token is the sum of the attributions of its embedding . There are other explainability methods that attribute a model 's decision to its features , but we chose IG in this framework due to several of its characteristics . First , it is both theoretically justified ( Sundararajan et al , 2017 ) and proven to be effective in NLP - related tasks ( Mudrakarta et al , 2018 ) . Second , the IG formula in 2.2 is differentiable everywhere with respect to model parameters . Lastly , it is lightweight in terms of implementation and execution complexity .", "entities": []}
{"text": "Base", "entities": []}
{"text": "We thank Salem Haykal , Ankur Taly , Diego Garcia - Olano , Raz Mathias , and Mukund Sundararajan for their valuable feedback and insightful discussions .", "entities": []}
