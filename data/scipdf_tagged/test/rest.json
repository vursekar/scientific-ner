{"text": "Context - Aware Graph Segmentation for Graph - Based Translation", "entities": [[9, 10, "TaskName", "Translation"]]}
{"text": "During training , given a word - aligned graphstring pair g , t , a , we extract translation rules g a i , c a i , t i , each of which consists of a continuous target phrase t i , a source subgraph g a i aligned to t i , and a source context c a i . We first find initial pairs . s a i , t i is an initial pair , iff it is consistent with the word alignment a ( Och and Ney , 2004 ) . s a j is a set of source words which are aligned to t i . Then , the set of rules satisfies the following : 1 . If s a i , t i is an initial pair ands a i is covered by a subgraph g a i which is connected , then g a i , * , t i is a basic rule . c a i = * means that a basic rule is applied without considering context to make sure that at least one translation is produced for any inputs during decoding . Therefore , basic rules are the same as rules in the conventional graph - based model . Rule ( 3 ) shows an example of a basic rule : 2010Nian FIFA Shijiebei 2010 FIFA World Cup ( 3 ) 2 . Assume g a i , * , t i is a basic rule and s a i+1 , t i+1 is an initial pair where t i+1 is on the right of and adjacent to t i . If there are edges between g a i ands a i+1 , then g a i , c a i , t i is a segmenting rule , where c a i is the set of edges between g a i ands a i+1 by treatings a i+1 as a single node x. Rule ( 4 ) is an example of a segmenting rule : 2010Nian FIFA x 2010 FIFA ( 4 ) where dashed links are contextual connections . During decoding , when the context matches , rule ( 4 ) translates a subgraph over 2010Nian FIFA into a target phrase 2010 FIFA . For example , it can be applied to graph ( 5 ) where Shijiebei Zai Nanfei ( in the dashed rectangle ) is treated as x : 2010Nian FIFA Shijiebei Zai Nanfei ( 5 ) 3 . If there are no edges between g a i ands a i+1 , then c a i is equal to and g a i , , t i is a translation rule , called a selecting rule in this paper . During decoding , the untranslated input could be a set of subgraphs which are disjoint with each other . A selecting rule is used to select one of them . For example , rule ( 6 ) can be applied to ( 7 ) to translate 2010Nian FIFA to 2010 FIFA . In this example , the x in rule ( 6 ) matches with Chenggong Juxing ( in the dashed rectangle ) in ( 7 ) . By comparing these three types of rules , we observe that both segmenting rules and selecting rules are based on basic rules . They extend basic rules by adding contextual information to their source subgraphs so that basic rules are split into different groups according to the context . During decoding , the context will help to select target phrases as well . Algorithm 1 illustrates a simple process for rule extraction . Given a word - aligned graph - string pair , we first extract all initial pairs ( Line 1 ) . Then , we find basic rules from these pairs ( Lines 3 - 4 ) . Basic Algorithm 1 : An algorithm for extracting translation rules from a graph - string pair . Data : Word - aligned graph - string pair g , t , a Result : A set of translation rules R 1 find a set of initial pairs P ; 2 for each p = s a i , t i in P do 3 if s j i is connected then // basic rules", "entities": [[85, 87, "TaskName", "word alignment"]]}
{"text": "Joint Models for Answer Verification in Question Answering Systems", "entities": [[6, 8, "TaskName", "Question Answering"]]}
{"text": "This paper studies joint models for selecting correct answer sentences among the top k provided by answer sentence selection ( AS2 ) modules , which are core components of retrievalbased Question Answering ( QA ) systems . Our work shows that a critical step to effectively exploiting an answer set regards modeling the interrelated information between pair of answers . For this purpose , we build a three - way multiclassifier , which decides if an answer supports , refutes , or is neutral with respect to another one . More specifically , our neural architecture integrates a state - of - the - art AS2 module with the multi - classifier , and a joint layer connecting all components . We tested our models on Wik - iQA , TREC - QA , and a real - world dataset . The results show that our models obtain the new state of the art in AS2 . * Work done while the author was an intern at Amazon Alexa Claim : Joe Walsh was inducted in 2001 . Ev1 : As a member of the Eagles , Walsh was inducted into the Rock and Roll Hall of Fame in 1998 , and into the Vocal Group Hall of Fame in 2001 . Ev2 : Joseph Fidler Walsh ( born November 20 , 1947 ) is an American singer songwriter , composer , multiinstrumentalist and record producer . Walsh was awarded with the Vocal Group Hall of Fame in 2001 .", "entities": [[30, 32, "TaskName", "Question Answering"], [130, 131, "DatasetName", "TREC"]]}
{"text": "Automated Question Answering ( QA ) research has received a renewed attention thanks to the diffusion of Virtual Assistants . Among the different types of methods to implement QA systems , we focus on Answer Sentence Selection ( AS2 ) research , originated from TREC - QA track ( Voorhees and Tice , 1999 ) , as it proposes efficient models that are more suitable for a production setting , e.g. , they are more efficient than those developed in machine reading ( MR ) work . Garg et al ( 2020 ) proposed the TANDA approach based on pre - trained Transformer models , obtaining impressive improvement over the state of the art for AS2 , measured on the two most used datasets , WikiQA ( Yang et al , 2015 ) and TREC - QA ( Wang et al , 2007 ) . However , TANDA was applied only to pointwise rerankers ( PR ) , e.g. , simple binary classifiers . Bonadiman ( 2020 ) tried to improve this model by jointly modeling all answer candidates with listwise methods , e.g. , ( Bian et al , 2017 ) . Unfortunately , merging the embeddings from all candidates with standard approaches , e.g. , CNN or LSTM , did not improve over TANDA .", "entities": [[1, 3, "TaskName", "Question Answering"], [44, 45, "DatasetName", "TREC"], [83, 84, "DatasetName", "MR"], [102, 103, "MethodName", "Transformer"], [125, 126, "DatasetName", "WikiQA"], [134, 135, "DatasetName", "TREC"], [209, 210, "MethodName", "LSTM"]]}
{"text": "MR is a popular QA task that identifies an answer string in a paragraph or a text of limited size for a question . Its application to retrieval scenario has also been studied Hu et al , 2019 ; Kratzwald and Feuerriegel , 2018 ) . However , the large volume of retrieved content makes their use not practical yet . Moreover , the joint modeling aspect of MR regards sentences from the same paragraphs . Jin et al ( 2020 ) use the relation between candidates in Multi - task learning approach for AS2 . However , they do not exploit transformer models , thus their results are rather below the state of the art . In contrast with the work above , our modeling is driven by an answer support strategy , where the pieces of information are taken from different documents . This makes our model even more unique ; it allows us to design innovative joint models , which are still not designed in any MR systems .", "entities": [[0, 1, "DatasetName", "MR"], [68, 69, "DatasetName", "MR"], [88, 92, "TaskName", "Multi - task learning"], [169, 170, "DatasetName", "MR"]]}
{"text": "Fact verification has become a social need given the massive amount of information generated daily . The problem is , therefore , becoming increasingly important in NLP context ( Mihaylova et al , 2018 ) . In QA , answer verification is directly relevant due to its nature of content delivery ( Mihaylova et al , 2019 ) . The problem has been explored in MR setting ( Wang et al , 2018 ) . Zhang et al ( 2020a ) also proposed to fact check for product questions using additional associated evidence sentences . The latter are retrieved based on similarity scores computed with both TF - IDF and sentence - embeddings from pre - trained BERT models . While the process is technically sound , the retrieval of evidence is an expensive process , which is prohibitive to scale in production . We instead address this problem by leveraging the top answer candidates .", "entities": [[0, 2, "TaskName", "Fact verification"], [65, 66, "DatasetName", "MR"], [117, 118, "MethodName", "BERT"]]}
{"text": "ASR still selects answers with a pointwise approach 2 . This means that we can improve it by building a listwise model , to select the best answer for each question , by utilizing the information from all target answers . In particular , the architecture of MASR shown in Figure 1d is made up of two parts : ( i ) a list of ASR containing k + 1 ASR blocks , in which each ASR block provides the representation of a target answer t. ( ii ) A final multiclassifier and a softmax function , which scores each t from k + 1 embedding concatenation and selects the one with highest score . For training and testing , we select the t from the k + 1 candidates of q based on a softmax output at a time .", "entities": [[94, 95, "MethodName", "softmax"], [135, 136, "MethodName", "softmax"]]}
{"text": "We used two most popular AS2 datasets , and one real world application dataset we built to test the generality of our approach . WikiQA is a QA dataset ( Yang et al , 2015 ) containing a sample of questions and answer - sentence candidates from Bing query logs over Wikipedia . The answers are manually labeled . We follow the most used setting : training with all the questions that have at least one correct answer , and validating and testing with all the questions having at least one correct and one incorrect answer . Wang et al ( 2007 ) . We use the same splits of the original data , following the common setting of previous work , e.g. , ( Garg et al , 2020 ) . WQA The Web - based Question Answering is a dataset built by Alexa AI as part of the effort to improve understanding and benchmarking in QA systems . The creation process includes the following steps : ( i ) given a set of questions we collected from the web , a search engine is used to retrieve up to 1 , 000 web pages from an index containing hundreds of millions pages . ( ii ) From the set of retrieved documents , all candidate sentences are extracted and ranked using AS2 models from ( Garg et al , 2020 ) . Finally , ( iii ) top candidates for each question are manually assessed as correct or incorrect by human judges . This allowed us to obtain a richer variety of answers from multiple sources with a higher average number of answers . FEVER is a large - scale public corpus , proposed by Thorne et al ( 2018a ) for fact verification task , consisting of 185 , 455 annotated claims from 5 , 416 , 537 documents from the Wikipedia dump in June 2017 . All claims are labelled as Supported , Refuted or Not Enough Info by annotators . Table 3 shows the statistics of the dataset , which remains the same as in ( Thorne et al , 2018b ) .", "entities": [[24, 25, "DatasetName", "WikiQA"], [137, 139, "TaskName", "Question Answering"], [276, 277, "DatasetName", "FEVER"], [294, 296, "TaskName", "fact verification"]]}
{"text": "We use the same configuration of the ASR training , including the optimizer type , learning rate , the number of epochs , GPU type , maximum sequence length , etc . Additionally , we design two different models MASR - F , using an ASC classifier targeting the FEVER labels , and MASR - FP , which initializes ASC with the data from FEVER . This is possible as the labels are compatible .", "entities": [[12, 13, "HyperparameterName", "optimizer"], [15, 17, "HyperparameterName", "learning rate"], [19, 22, "HyperparameterName", "number of epochs"], [49, 50, "DatasetName", "FEVER"], [64, 65, "DatasetName", "FEVER"]]}
{"text": "The selection of the hyper - parameter k , i.e. , the number of candidates to consider for supporting a target answer is rather tricky . Indeed , the standard validation set is typically used for tuning PR . This means that the candidates PR moves to the top k +1 positions are optimistically accurate . Thus , when selecting also the optimal k on the same validation set , there is high risk to overfit the model . We solved this problem by running a PR version not heavily optimized on the dev . set , i.e. , we randomly choose a checkpoint after the standard three epochs of fine - tuning of RoBERTa transformer . Additionally , we tuned k only using the WQA dev . set , which contains \u223c 36 , 000 Q / A pairs . WikiQA and TREC - QA dev . sets are too small to be used ( 121 and 65 questions , respectively ) . Fig . 2 plots the improvement of four different models , Joint Model Multi - classifier , Joint Model Pairwise , KGAT , and ASR , when using different k values . Their best results are reached for 5 , 3 , 2 , and 3 , respectively . We note that the most reliable curve shape ( convex ) is the one of ASR and Joint Model Pairwise .", "entities": [[114, 115, "MethodName", "RoBERTa"], [141, 142, "DatasetName", "WikiQA"], [143, 144, "DatasetName", "TREC"]]}
{"text": "We have proposed new joint models for AS2 . ASR encodes the relation between the target answer and all the other candidates , using an additional Transformer model , and an Answer Support Classifier , while MASR jointly models the ASR representations for all target answers . We extensively tested KGAT , ASR , MASR , and other joint model baselines we designed . The results show that our models can outperform the state of the art . Most interestingly , ASR constantly outperforms all the models ( but MASR - FP ) , on all datasets , through all measures , and for both base and large transformers . For example , ASR q : What kind of colors are in the rainbow ? c1 : Red , yellow , and blue are called the primary colors . c2 : The order of the colors in the rainbow goes : red , orange , yellow , green , blue , indigo and violet . c3 : The colors in all rainbows are present in the same order : red , orange , yellow , green , blue , indigo , and violet . c4 : A rainbow occurs when white light bends and separates into red , orange , yellow , green blue , indigo and violet .", "entities": [[26, 27, "MethodName", "Transformer"], [165, 166, "DatasetName", "c3"], [194, 195, "DatasetName", "c4"]]}
{"text": "Open - Domain Why - Question Answering with Adversarial Learning to Encode Answer Texts", "entities": [[5, 7, "TaskName", "Question Answering"]]}
{"text": "In this paper , we propose a method for whyquestion answering ( why - QA ) that uses an adversarial learning framework . Existing why - QA methods retrieve answer passages that usually consist of several sentences . These multi - sentence passages contain not only the reason sought by a why - question and its connection to the why - question , but also redundant and/or unrelated parts . We use our proposed Adversarial networks for Generating compact - answer Representation ( AGR ) to generate from a passage a vector representation of the non - redundant reason sought by a why - question and exploit the representation for judging whether the passage actually answers the why - question . Through a series of experiments using Japanese why - QA datasets , we show that these representations improve the performance of our why - QA neural model as well as that of a BERT - based why - QA model . We show that they also improve a state - of - the - art distantly supervised open - domain QA ( DS - QA ) method on publicly available English datasets , even though the target task is not a why - QA .", "entities": [[154, 155, "MethodName", "BERT"]]}
{"text": "We also applied two types of attention mechanisms to the above word embeddings . The first type of attention , similarity - attention , was used for estimating the similarities between words in question q and those in passage / compact - answers t and focusing on the attended words as those that directly indicate the connection between the question and passage / compact - answers . Basically , the mechanism computes the cosine similarity between the embeddings of the words in q and t , and uses it for producing attention feature vector a s j R for word t j in passage / compact - answers . Another attention mechanism , causalityattention , was proposed for focusing on passage words causally associated with question words . They used normalized point - wise mutual information to measure the strength of the causal associations with the causality expressions used for creating the causal embeddings . The scores are used for producing causality - attention feature vector a c j for word t j . Finally , we form two attention feature vectors , a s = [ a s 1 , , a s | t | ] and a c = [ a c 1 , , a c | t | ] , con - catenate them into a = [ a s ; a c ] R 2\u00d7 | t | , and produce attention - weighted word embedding t att of given text t , which is either an answer passage or a compact answer : t att = ReLU ( W t t + W a a ) where W t R 2d\u00d72d and W a R 2d\u00d72 are trainable parameters , t is the representation of text t , and ReLU represents the rectified linear units .", "entities": [[11, 13, "TaskName", "word embeddings"], [263, 264, "MethodName", "ReLU"], [296, 297, "MethodName", "ReLU"], [299, 302, "MethodName", "rectified linear units"]]}
{"text": "We used three datasets , W hySet , CmpAns , and AddT r , for our why - QA experiments . W hySet and AddT r were used for training and evaluating the why - QA models , while CmpAns was used for training AGR . The W hySet dataset , which was used in previous works for why - QA ( Oh et al , , 2013 , is composed of 850 Japanese why - questions and their top - 20 answer passages ( 17 , 000 question - passage pairs ) obtained from 600 million Japanese web pages using the answerretrieval method of Murata et al ( 2007 ) , where a question - passage pair is composed of a singlesentence question and a five - sentence passage . The label of each question - answer pair ( i.e. , correct answer and incorrect answer ) was manually annotated ( See Oh et al ( 2013 ) for more details ) . Oh et al ( 2013 ) selected 10 , 000 questionpassage pairs as training and test data in 10 - fold cross - validation ( 9 , 000 for training and 1 , 000 for testing ) and used the remainder ( 7 , 000 questionpassage pairs ) as additional training data during the 10 - fold cross - validation . We followed the settings and , in each fold , we selected 1 , 000 pairs from the 9 , 000 pairs for training to use as development data for tuning hyperparameters . Note that there are no shared questions in the training , development , or test data . For training the AGR , we used CmpAns , the training data set created in Ishida et al ( 2018 ) for compact - answer generation ; CmpAns consists of 15 , 130 triples of a why - question , an answer passage , and a manually - created compact answer . These cover 2 , 060 unique why - questions . Note that there was no overlap between the questions in CmpAns and those in W hySet . CmpAns was created in the following manner : 1 ) human annotators manually came up with open - domain why - questions , 2 ) retrieved the top - 20 passages for each why - question using the open - domain why - QA module of a publicly available web - based QA system WISDOM X ( Mizuno et al , 2016 ; , and 3 ) three annotators created ( when possible ) a compact answer for each of the retrieved passages . The passages for which no annotator could create a compact answer were discarded , and were not included in the 15 , 130 triples mentioned previously . The average lengths of questions , passages , and compact answers in CmpAns were 10.5 words , 184.4 words , and 8.3 words , respectively . Finally , we created additional training data AddT r for training the why - QA models . If an annotator could write a compact answer for a question and an answer passage , she / he probably recognized the passage as a proper answer passage to the question . Based on this observation , we built AddT r from CmpAns by applying a majority vote . We only gave a correct answer label to a question and a passage if at least two of the three annotators wrote compact answers , and it received an incorrect answer label otherwise . AddT r has 10 , 401 pairs in total . We used AddT r as additional training data for baselines that lack a mechanism for generating compact - answer representations , for a fair comparison with other methods that use CmpAns for such mechanisms . We processed all the data with MeCab 1 , a morphological analyzer , to segment the words .", "entities": [[299, 301, "TaskName", "answer generation"], [408, 409, "DatasetName", "WISDOM"]]}
{"text": "We tried three schemes for training our AGR in our proposed method . In the first scheme , pairs of passages and compact answers in CmpAns were given to fake - representation generator F and realrepresentation generator R as their inputs . We called the fake - representation generator trained in this way F OP and referred to our proposed method using F OP as Ours ( OP ) . In the second scheme , we randomly sampled five - sentence passages that contain some clue words indicating the existence of causal relations , such as \" because , \" from 4 - billion web pages and fed them to fakerepresentation generator F . We fed the same number of the sampled passages as in CmpAns for fair comparison . We refer to the method trained by this scheme as Ours ( RP ) . In the final scheme , we replaced the word embeddings for the passages given to fake - representation generator F with random vectors and used similarity - attention but not causality - attention . The fake - representation generator trained in this way is called F RV , and our proposed method using F RV is called Ours ( RV ) . This scheme is more similar to the original GAN than the others because the fake - representation generator is given random noises . We implemented and evaluated the following four why - QA models in previous works as baselines , using the same dataset as ours : We also evaluated nine baseline neural models , four of which are BERT - based models ( BERT , BERT+AddTr , BERT+F OP , and BERT+F RV ) , to show the effectiveness of our why - QA model and AGR . They are listed in Table 2 .", "entities": [[153, 155, "TaskName", "word embeddings"], [215, 216, "MethodName", "GAN"], [266, 267, "MethodName", "BERT"], [271, 272, "MethodName", "BERT"]]}
{"text": "Proposed method from which we removed fake - representation generator F . BASE+AddTr BASE that used both W hySet and AddT r as its training data .", "entities": [[13, 14, "MethodName", "BASE"]]}
{"text": "On top of BASE , it additionally used real - representation generator R to encode compact answers , which were generated by the compactanswer generator of Iida et al ( 2019 ) . R was trained alongside the why - QA model using W hySet and the compact - answer generator was pre - trained with CmpAns .", "entities": [[3, 4, "MethodName", "BASE"]]}
{"text": "On top of BASE , it additionally used the encoder in the compact - answer generator of Iida et al ( 2019 ) to create compact - answer representation . The encoder was pre - trained with CmpAns .", "entities": [[3, 4, "MethodName", "BASE"]]}
{"text": "Same as Ours ( OP ) except that the fake - representation generator was trained in a supervised manner alongside the why - QA model using W hySet and AddT r as the training data . BERT Same as BASE except that the CNNbased encoders for questions and passages were replaced with the BERT ( Devlin et al , 2019 ) . BERT+AddTr BERT , which used both W hySet and AddT r as its training data .", "entities": [[36, 37, "MethodName", "BERT"], [39, 40, "MethodName", "BASE"], [53, 54, "MethodName", "BERT"], [63, 64, "MethodName", "BERT"]]}
{"text": "On top of BERT , it additionally used compact - answer representation produced by FOP for answer selection .", "entities": [[3, 4, "MethodName", "BERT"], [16, 18, "TaskName", "answer selection"]]}
{"text": "We proposed a method for why - question answering ( why - QA ) that used an adversarial learning framework . It employed adversarial learning to generate vector representations of reasons or true answers from answer passages and exploited the representations for judging whether the passages are proper answer passages to the given whyquestions . Through experiments using Japanese why - QA datasets , we showed that this idea improved why - QA performance . We also showed that our method improved the performance in a distantly supervised open - domain QA task . In our why - QA method , causality expressions extracted from the web were used as background knowledge for computing causality - attention / embeddings . As a future work , we plan to introduce a wider range of background knowledge including another type of event causality ( Hashimoto et al , , 2014 ( Hashimoto et al , , 2015 .", "entities": [[7, 9, "TaskName", "question answering"]]}
{"text": "Due to its potential applications , open - domain dialogue generation has become popular and achieved remarkable progress in recent years , but sometimes suffers from generic responses . Previous models are generally trained based on 1 - to - 1 mapping from an input query to its response , which actually ignores the nature of 1 - to - n mapping in dialogue that there may exist multiple valid responses corresponding to the same query . In this paper , we propose to utilize the multiple references by considering the correlation of different valid responses and modeling the 1 - to - n mapping with a novel two - step generation architecture . The first generation phase extracts the common features of different responses which , combined with distinctive features obtained in the second phase , can generate multiple diverse and appropriate responses . Experimental results show that our proposed model can effectively improve the quality of response and outperform existing neural dialogue models on both automatic and human evaluations .", "entities": [[9, 11, "TaskName", "dialogue generation"]]}
{"text": "In recent years , open - domain dialogue generation has become a research hotspot in Natural Language Processing due to its broad application prospect , including chatbots , virtual personal assistants , etc . Though plenty of systems have been proposed to improve the quality of generated responses from various aspects such as topic , persona modeling and emotion controlling ( Zhou et al , 2018b ) , most of these recent approaches are primarily built upon the sequence - to - sequence architecture Shang et al , 2015 ) which suffers from the \" safe \" response problem ( Li et al , 2016a ; Sato et al , 2017 ) . This can be ascribed to modeling the response generation process as 1to - 1 mapping , which ignores the nature of 1 - to - n mapping of dialogue that multiple possible responses can correspond to the same query . To deal with the generic response problem , various methods have been proposed , including diversity - promoting objective function ( Li et al , 2016a ) , enhanced beam search ( Shao et al , 2016 ) , latent dialogue mechanism ( Zhou et al , , 2018a , Variational Autoencoders ( VAEs ) based models Serban et al , 2017 ) , etc . However , these methods still view multiple responses as independent ones and fail to model multiple responses jointly . Recently , Zhang et al ( 2018a ) introduce a maximum likelihood strategy that given an input query , the most likely response is approximated rather than all possible responses , which is further implemented by Rajendran et al ( 2018 ) with reinforcement learning for task - oriented dialogue . Although capable of generating the most likely response , these methods fail to model other possible responses and ignore the correlation of different responses . In this paper , we propose a novel response generation model for open - domain conversation , which learns to generate multiple diverse responses with multiple references by considering the correlation of different responses . Our motivation lies in two aspects : 1 ) multiple responses for a query are likely correlated , which can facilitate building the dialogue system . 2 ) it is easier to model each response based on other responses than from scratch every time . As shown in Figure 1 , given an input query , different responses may share some common features e.g. positive attitudes or something else , but vary in discourses or expressions which we refer to as distinct features . Accordingly , the system can benefit from modeling these features respectively rather than learning each query - response mapping from scratch . Inspired by this idea , we propose a two - step dialogue generation architecture as follows . We jointly view the multiple possible responses to the same query as a response bag . In the first generation phase , the common feature of different valid responses is extracted , serving as a base from which each specific response in the bag is further approximated . The system then , in the second generation phase , learns to model the distinctive feature of each individual response which , combined with the common feature , can generate multiple diverse responses simultaneously . Experimental results show that our method can outperform existing competitive neural models under both automatic and human evaluation metrics , which demonstrates the effectiveness of the overall approach . We also provide ablation analyses to validate each component of our model . To summarize , our contributions are threefold : We propose to model multiple responses to a query jointly by considering the correlations of responses with multi - reference learning . We consider the common and distinctive features of the response bag and propose a novel two - step dialogue generation architecture . Experiments show that the proposed method can generate multiple diverse responses and outperform existing competitive models on both automatic and human evaluations .", "entities": [[7, 9, "TaskName", "dialogue generation"], [58, 59, "DatasetName", "emotion"], [120, 122, "TaskName", "response generation"], [204, 205, "MethodName", "Autoencoders"], [322, 324, "TaskName", "response generation"], [455, 456, "DatasetName", "Inspired"], [466, 468, "TaskName", "dialogue generation"], [645, 647, "TaskName", "dialogue generation"]]}
{"text": "In this paper , we propose a novel response generation model for short - text conversation , which models multiple valid responses for a given query jointly . We posit that a dialogue system can benefit from multi - reference learning by considering the correlation of multiple responses . Figure 2 demonstrates the whole architecture of our model . We now describe the details as follows .", "entities": [[8, 10, "TaskName", "response generation"], [12, 16, "TaskName", "short - text conversation"]]}
{"text": "Training samples { ( x , { y } ) i } i = N i=1 consist of each query x and the set of its valid responses { y } , where N denotes the number of training samples . For a dialogue generation model , it aims to map from the input query x to the output response y { y } . To achieve this , different from conventional methods which view the multiple responses as independent ones , we propose to consider the correlation of multiple responses with a novel twostep generation architecture , where the response bag { y } and each response y { y } are modeled by two separate features which are obtained in each generation phase respectively . Specifically , we assume a variable c R n representing the common feature of different responses and an unobserved latent variable z Z corresponding to the distinct feature for each y in the bag . The com - mon feature c is generated in the first stage given x and the distinctive feature z is sampled from the latent space Z in the second stage given the query x and common feature c. The final responses are then generated conditioned on both the common feature c and distinct feature z simultaneously .", "entities": [[43, 45, "TaskName", "dialogue generation"]]}
{"text": "Focusing on open - domain dialogue , we perform experiments on a large - scale single - turn conversation dataset Weibo ( Shang et al , 2015 ) , where each input post is generally associated with multiple response utterances 2 . Concretely , the Weibo dataset consists of short - text online chit - chat dialogues in Chinese , which is crawled from Sina Weibo 3 . Totally , there are 4 , 423 , 160 queryresponse pairs for training set and 10000 pairs for the validation and testing , where there are around 200k unique query in the training set and each query used in testing correlates with four responses respectively . For preprocessing , we follow the conventional settings ( Shang et al , 2015 ) .", "entities": [[20, 21, "DatasetName", "Weibo"], [45, 46, "DatasetName", "Weibo"], [65, 66, "DatasetName", "Weibo"]]}
{"text": "Table 3 illustrates two examples of generated replies to the input query got from the testing set . Comparing the CVAE and Ours , we can find that although the CVAE model can generate diverse utterances , its responses tend to be irrelevant to the query and sometimes not grammatically formed , e.g. the words \" glowworm \" and \" robot \" in the sentences . In contrast , responses generated by our model show better quality , achieving both high relevance and diversity . This demonstrates the ability of the two - step generation architecture . For better insight into the procedure , we present the intermediately generated utterances which show that the feature extracted in the first stage can focus on some common and key aspects of the query and its possible responses , such as the \" amazing \" and \" software \" . With the distinctive features sampled in the second generation phase , the model further revises the response and outputs multiple responses with diverse contents and expressions . Recap that the common feature is expected to capture the correlations of different responses and serve as the base of a response bag from which different responses are further generated , as shown in Figure 1 . To investigate the actual performances achieved by our model , we compute the distance between the input query / intermediate utterance and gold references / generated responses and present the results in Figure 4 . As shown , intermediate utterances obtained in the first generation phase tend to approximate multiple responses with similar distances at the same time . Comparing the generated responses and the references , we find that generated responses show both high relevant and irrelevant ratios , as the values near 0.00 and 1.00 show . This actually agrees well with our observation that the model may sometimes rely heavily on or ignore the prior common feature information . From a further comparison between the input query and the mid , we also observe that the intermediate utterance is more similar to final responses than the input query , which correlates well with our original intention shown in Figure 1 .", "entities": [[20, 21, "MethodName", "CVAE"], [30, 31, "MethodName", "CVAE"]]}
{"text": "This paper introduces SGNMT , our experimental platform for machine translation research . SGNMT provides a generic interface to neural and symbolic scoring modules ( predictors ) with left - to - right semantic such as translation models like NMT , language models , translation lattices , n - best lists or other kinds of scores and constraints . Predictors can be combined with other predictors to form complex decoding tasks . SGNMT implements a number of search strategies for traversing the space spanned by the predictors which are appropriate for different predictor constellations . Adding new predictors or decoding strategies is particularly easy , making it a very efficient tool for prototyping new research ideas . SGNMT is actively being used by students in the MPhil program in Machine Learning , Speech and Language Technology at the University of Cambridge for course work and theses , as well as for most of the research work in our group .", "entities": [[9, 11, "TaskName", "machine translation"], [140, 141, "DatasetName", "Cambridge"]]}
{"text": "We are developing an open source decoding framework called SGNMT , short for Syntactically Guided Neural Machine Translation . 1 The software package supports a number of well - known frameworks , including TensorFlow 2 ( Abadi et al , 2016 ) , OpenFST ( Allauzen et al , 2007 ) , Blocks / Theano ( Bastien et al , 2012 ; van Merri\u00ebnboer et al , 2015 ) , and NPLM ( Vaswani et al , 2013 ) . The two central concepts in the SGNMT tool are predictors and decoders . Predictors are scoring modules which define scores over the target language vocabulary given the current internal predictor state , the history , the source sentence , and external side information . Scores from multiple , diverse predictors can be combined for use in decoding . Decoders are search strategies which traverse the space spanned by the predictors . SGNMT provides implementations of common search tree traversal algorithms like beam search . Since decoders differ in runtime complexity and the kind of search errors they make , different decoders are appropriate for different predictor constellations . The strict separation of scoring module and search strategy and the decoupling of scoring modules from each other makes SGNMT a very flexible decoding tool for neural and symbolic models which is applicable not only to machine translation . SGNMT is based on the OpenFSTbased Cambridge SMT system ( Allauzen et al , 2014 ) . Although the system is less than a year old , we have found it to be very flexible and easy for new researchers to adopt . Our group has already integrated SGNMT into most of its research work . We also find that SGNMT is very well - suited for teaching and student research projects . In the 2015 - 16 academic year , two students on the Cambridge MPhil in Machine Learning , Speech and Language Technology used SGNMT for their dissertation projects . 3 The first project involved using SGNMT with OpenFST for applying subword models in SMT ( Gao , 2016 ) . The second project developed automatic music composition by LSTMs where WFSAs were used to define the space of allowable chord progressions in ' Bach ' chorales ( Tomczak , 2016 that the chorales must obey . This second project in particular demonstrates the versatility of the approach . For the current , 2016 - 17 academic year , SGNMT is being used heavily in two courses .", "entities": [[16, 18, "TaskName", "Machine Translation"], [224, 226, "TaskName", "machine translation"], [233, 234, "DatasetName", "Cambridge"], [312, 313, "DatasetName", "Cambridge"]]}
{"text": "SGNMT consequently emphasizes flexibility and extensibility by providing a common interface to a wide range of constraints or models used in MT research . The concept facilitates quick prototyping of new research ideas . Our platform aims to minimize the effort required for implementation ; decoding speed is secondary as optimized code for production systems can be produced once an idea has been proven successful in the SGNMT framework . In SGNMT , scores are assigned to partial hypotheses via one or many predictors . One predictor usually has a single responsibility as it represents a single model or type of constraint . Predictors need to implement the following methods : initialize ( src sentence ) Initialize the predictor state using the source sentence . get state ( ) Get the internal predictor state . set state ( state ) Set the internal predictor state . predict next ( ) Given the internal predictor state , produce the posterior over target tokens for the next position . Predictor Description nmt Attention - based neural machine translation following . Supports Blocks / Theano ( Bastien et al , 2012 ; van Merri\u00ebnboer et al , 2015 ) and TensorFlow ( Abadi et al , 2016 ) . fst Predictor for rescoring deterministic lattices ( Heafield et al , 2013 ; Stolcke et al , 2002 ) toolkit . nplm Neural n - gram language models based on NPLM ( Vaswani et al , 2013 ) . rnnlm Integrates RNN language models with TensorFlow as described by Zaremba et al ( 2014 ) . forced Forced decoding with a single reference . forcedlst n - best list rescoring . bow Restricts the search space to a bag of words with or without repetition consume ( token ) Update the internal predictor state by adding token to the current history . The structure of the predictor state and the implementations of these methods differ substantially between predictors . Tab . 2 lists all predictors which are currently implemented . Tab . 1 summarizes the semantics of this interface for three very common predictors : the neural machine translation ( NMT ) predictor , the ( deterministic ) finite state transducer ( FST ) predictor for lattice rescoring , and the n - gram predictor for applying n - gram language models . We also included two examples ( word count and UNK count ) which do not have a natural left - to - right semantic but can still be represented as predictors .", "entities": [[174, 176, "TaskName", "machine translation"], [353, 355, "TaskName", "machine translation"]]}
{"text": "Recent studies show that NLP models are vulnerable to adversarial perturbations . A seemingly \" invariance transformation \" ( a.k.a . adversarial perturbation ) such as synonym substitutions ( Alzantot et al , 2018 ; Zang et al , 2020 ) or syntax - guided paraphrasing ( Iyyer et al , 2018 ; Huang and Chang , 2021 ) can alter the prediction . To mitigate the model vulnerability , robust training methods have been proposed and shown effective ( Miyato et al , 2017 ; Jia et al , 2019 ; Huang et al , 2019 ; Zhou et al , 2020 ) . x 0 = \" a deep and meaningful film ( movie ) . \"", "entities": [[106, 107, "DatasetName", "0"]]}
{"text": "x 0 = \" a short and moving film ( movie ) . \" 73 % positive ( 70 % negative ) ( 99 % positive ) 99 % positive perturb Figure 1 : A vulnerable example beyond the test dataset . Numbers on the bottom right are the sentiment predictions for film and movie . Blue x 0 comes from the test dataset and its prediction can not be altered by the substitution film movie ( robust ) . Yellow examplex 0 is slightly perturbed but remains natural . Its prediction can be altered by the substitution ( vulnerable ) . In most studies , model robustness is evaluated based on a given test dataset or synthetic sentences constructed from templates ( Ribeiro et al , 2020 ) . Specifically , the robustness of a model is often evaluated by the ratio of test examples where the model prediction can not be altered by semantic - invariant perturbation . We refer to this type of evaluations as the first - order robustness evaluation . However , even if a model is first - order robust on an input sentence x 0 , it is possible that the model is not robust on a natural sentencex 0 that is slightly modified from x 0 . In that case , adversarial examples still exist even if first - order attacks can not find any of them from the given test dataset . Throughout this paper , we callx 0 a vulnerable example . The existence of such examples exposes weaknesses in models ' understanding and presents challenges for model deployment . Fig . 1 illustrates an example . In this paper , we propose the double perturbation framework for evaluating a stronger notion of second - order robustness . Given a test dataset , we consider a model to be second - order robust if there is no vulnerable example that can be identified in the neighborhood of given test instances ( 2.2 ) . In particular , our framework first perturbs the test set to construct the neighborhood , and then diagnoses the robustness regarding a single - word synonym substitution . Taking Fig . 2 as an example , the model is first - order robust on the input sentence x 0 ( the prediction can not be altered ) , but it is not second - order robust due to the existence of the vulnerable examplex 0 . Our framework is designed to identifyx 0 . We apply the proposed framework and quantify second - order robustness through two second - order attacks ( 3 ) . We experiment with English sentiment classification on the SST - 2 dataset ( Socher et al , 2013 ) across various model architectures . Surprisingly , although robustly trained CNN ( Jia et al , 2019 ) and Transformer ( Xu et al , 2020 ) can achieve high robustness under strong attacks ( Alzantot et al , 2018 ; Garg and Ramakrishnan , 2020 ) ( 23.0 % - 71.6 % success rates ) , for around 96.0 % of the test examples our attacks can find a vulnerable example by perturbing 1.3 words on average . This finding indicates that these robustly trained models , despite being first - order robust , are not second - order robust . Furthermore , we extend the double perturbation framework to evaluate counterfactual biases ( Kusner et al , 2017 ) ( 4 ) in English . When the test dataset is small , our framework can help improve the evaluation robustness by revealing the hidden biases not directly shown in the test dataset . Intuitively , a fair model should make the same prediction for nearly identical examples referencing different groups ( Garg et al , 2019 ) with different protected attributes ( e.g. , gender , race ) . In our evaluation , we consider a model biased if substituting tokens associated with protected attributes changes the expected prediction , which is the average prediction among all examples within the neighborhood . For instance , a toxicity classifier is biased if it tends to increase the toxicity if we substitute straight gay in an input sentence ( Dixon et al , 2018 ) . In the experiments , we evaluate the expected sentiment predictions on pairs of protected tokens ( e.g. , ( he , she ) , ( gay , straight ) ) , and demonstrate that our method is able to reveal the hidden model biases . Our main contributions are : ( 1 ) We propose the double perturbation framework to diagnose the robustness of existing robustness and fairness evaluation methods . ( 2 ) We propose two second - order attacks to quantify the stronger notion of second - x 0x 0 x 0 x 1 negative positive Figure 2 : An illustration of the decision boundary . Diamond area denotes invariance transformations . Blue x 0 is a robust input example ( the entire diamond is green ) . Yellowx 0 is a vulnerable example in the neighborhood of x 0 . Redx 0 is an adversarial example tox 0 . Note : x 0 is not an adversarial example to x 0 since they have different meanings to human ( outside the diamond ) . order robustness and reveal the models ' vulnerabilities that can not be identified by previous attacks . ( 3 ) We propose a counterfactual bias evaluation method to reveal the hidden model bias based on our double perturbation framework .", "entities": [[1, 2, "DatasetName", "0"], [58, 59, "DatasetName", "0"], [82, 83, "DatasetName", "0"], [191, 192, "DatasetName", "0"], [206, 207, "DatasetName", "0"], [213, 214, "DatasetName", "0"], [247, 248, "DatasetName", "0"], [382, 383, "DatasetName", "0"], [408, 409, "DatasetName", "0"], [416, 417, "DatasetName", "0"], [447, 448, "DatasetName", "SST"], [477, 478, "MethodName", "Transformer"], [805, 806, "DatasetName", "0"], [807, 808, "DatasetName", "0"], [830, 831, "DatasetName", "0"], [845, 846, "DatasetName", "0"], [855, 856, "DatasetName", "0"], [858, 859, "DatasetName", "0"], [864, 865, "DatasetName", "0"], [869, 870, "DatasetName", "0"], [877, 878, "DatasetName", "0"]]}
{"text": "We focus our study on word - level substitution , where existing works evaluate robustness and counterfactual bias by directly perturbing the test dataset . For instance , adversarial attacks alter the prediction by making synonym substitutions , and the fairness literature evaluates counterfactual fairness by substituting protected tokens . We integrate the word substitution strategy into our framework as the component for evaluating robustness and fairness . For simplicity , we consider a single - word substitution and denote it with the operator . Let X \u2286 V l be the input space where V is the vocabulary and l is the sentence length , p = ( p ( 1 ) , p ( 2 ) ) V 2 be a pair of synonyms ( called patch words ) , X p \u2286 X denotes sentences with a single occurrence of p ( 1 ) ( for simplicity we skip other sentences ) , x 0 X p be an input sentence , then x 0 p means \" substitute p ( 1 ) p ( 2 ) in x 0 \" . The result after substitution is : x 0 = x 0 p. Taking Fig . 1 as an example , where p = ( film , movie ) and x 0 = a deep and meaningful film , the perturbed sentence is x 0 = a deep and meaningful movie . Now we introduce other components in our framework .", "entities": [[157, 158, "DatasetName", "0"], [167, 168, "DatasetName", "0"], [182, 183, "DatasetName", "0"], [192, 193, "DatasetName", "0"], [195, 196, "DatasetName", "0"], [215, 216, "DatasetName", "0"], [228, 229, "DatasetName", "0"]]}
{"text": "Second - order attacks study the prediction difference caused by applying p. For notation convenience we define the prediction difference F ( x ; p ) : x0 = a deep and meaningful film . p = film , movie x ( i = 2 ) a short and moving film ( movie ) . a slow and moving film ( movie ) . a dramatic or meaningful film ( movie ) . p alters the prediction . x0 = \" a short and moving film ( movie ) . \" ( 70 % negative ) 73 % positive Figure 3 : The attack flow for SO - Beam ( Algorithm 2 ) . Blue x 0 is the input sentence and yellowx 0 is our constructed vulnerable example ( the prediction can be altered by substituting film movie ) . Green boxes in the middle show intermediate sentences , and f soft ( x ) denotes the probability outputs for film and movie . X \u00d7 V 2 { \u22121 , 0 , 1 } by : 3 F ( x ; p ) : = f ( x p ) \u2212 f ( x ) . ( 2 ) Taking Fig . 1 as an example , the prediction difference forx 0 on p is F ( x 0 ; p ) = f ( ... moving movie . ) \u2212 f ( ... moving film . ) = \u22121 . Given an input sentence x 0 , we want to find patch words p and a vulnerable examplex 0 such that f ( x 0 p ) = f ( x 0 ) . Follow Alzantot et al ( 2018 ) , we choose p from a predefined list of counter - fitted synonyms ( Mrk\u0161i\u0107 et al , 2016 ) that maximizes | f soft ( p ( 2 ) ) \u2212 f soft ( p ( 1 ) ) | . Here f soft ( x ) : X [ 0 , 1 ] denotes probability output ( e.g. , after the softmax layer but before the final argmax ) , f soft ( p ( 1 ) ) and f soft ( p ( 2 ) ) denote the predictions for the single word , and we enumerate through all possible p for x 0 . Let k be the neighborhood distance , then the attack is equivalent to solving : x 0 = argmax x Neighbor k ( x 0 ) | F ( x ; p ) | . ( 3 )", "entities": [[116, 117, "DatasetName", "0"], [123, 124, "DatasetName", "0"], [172, 173, "DatasetName", "0"], [213, 214, "DatasetName", "0"], [220, 221, "DatasetName", "0"], [248, 249, "DatasetName", "0"], [261, 262, "DatasetName", "0"], [267, 268, "DatasetName", "0"], [274, 275, "DatasetName", "0"], [335, 336, "DatasetName", "0"], [347, 348, "MethodName", "softmax"], [390, 391, "DatasetName", "0"], [408, 409, "DatasetName", "0"], [416, 417, "DatasetName", "0"]]}
{"text": "We follow the setup from the robust training literature ( Jia et al , 2019 ; Xu et al , 2020 ) and experiment with both the base ( non - robust ) and robustly trained models . We train the binary sentiment classifiers on the SST - 2 dataset with bag - ofwords ( BoW ) , CNN , LSTM , and attention - based Original : 70 % Negative Input Example : in its best moments , resembles a bad high school production of grease , without benefit of song . Genetic : 56 % Positive Adversarial Example : in its best moment , recalling a naughty high school production of lubrication , unless benefit of song . BAE : 56 % Positive Adversarial Example : in its best moments , resembles a great high school production of grease , without benefit of song . SO - Enum and SO - Beam ( ours ) : 60 % Negative ( 67 % Positive ) Vulnerable Example : in its best moments , resembles a bad ( unhealthy ) high school production of musicals , without benefit of song . Table 1 : Sampled attack results on the robust BoW. For Genetic and BAE the goal is to find an adversarial example that alters the original prediction , whereas for SO - Enum and SO - Beam the goal is to find a vulnerable example beyond the test set such that the prediction can be altered by substituting bad unhealthy .", "entities": [[46, 47, "DatasetName", "SST"], [60, 61, "MethodName", "LSTM"]]}
{"text": "Intended use . One primary goal of NLP models is the generalization to real - world inputs . However , existing test datasets and templates are often not comprehensive , and thus it is difficult to evaluate real - world performance ( Recht et al , 2019 ; Ribeiro et al , 2020 ) . Our work sheds a light on quantifying performance for inputs beyond the test dataset and help uncover model weaknesses prior to the realworld deployment . Misuse potential . Similar to other existing adversarial attack methods ( Ebrahimi et al , 2018 ; Jin et al , 2019 ; Zhao et al , 2018b ) , our second - order attacks can be used for finding vulnerable examples to a NLP system . Therefore , it is essential to study how to improve the robustness of NLP models against second - order attacks . Limitations . While the core idea about the double perturbation framework is general , in 4 , we consider only binary gender in the analysis of counterfactual fairness due to the restriction of the English corpus we used , which only have words associated with binary gender such as he / she , waiter / waitress , etc .", "entities": [[87, 89, "TaskName", "adversarial attack"]]}
{"text": "To validate the effectiveness of minimizing Eq . ( 4 ) , we also experiment on a second - order baseline that constructs vulnerable examples by randomly replacing up to 6 words . We use the same masked language model and threshold as SO - Beam such that they share a similar neighborhood . We perform the attack on the same models as Table 2 , and the attack success rates on robustly trained BoW , CNN , LSTM , and Transformers are 18.8 % , 22.3 % , 15.2 % , and 25.1 % , respectively . Despite being a second - order attack , the random baseline has low attack success rates thus demonstrates the effectiveness of SO - Beam .", "entities": [[78, 79, "MethodName", "LSTM"]]}
{"text": "We randomly select 100 successful attacks from SO - Beam and consider four types of examples ( for a total of 400 examples ) : The original examples with and without synonym substitution p , and the vulnerable examples with and without synonym substitution p. For each example , we annotate the naturalness and sentiment separately as described below . Naturalness of vulnerable examples . We ask the annotators to score the likelihood of being an original example ( i.e. , not altered by computer ) based on grammar correctness and naturalness , with a Likert scale of 1 - 5 : ( 1 ) Sure adversarial example . ( 2 ) Likely an adversarial example . ( 3 ) Neutral . ( 4 ) Likely an original example . ( 5 ) Sure original example . Semantic similarity after the synonym substitution . We first ask the annotators to predict the sentiment on a Likert scale of 1 - 5 , and then map the prediction to three categories : negative , neutral , and positive . We consider two examples to have the same semantic meaning if and only if they are both positive or negative .", "entities": [[137, 139, "TaskName", "Semantic similarity"]]}
{"text": "Better Feature Integration for Named Entity Recognition", "entities": [[4, 7, "TaskName", "Named Entity Recognition"]]}
{"text": "It has been shown that named entity recognition ( NER ) could benefit from incorporating the long - distance structured information captured by dependency trees . We believe this is because both types of features - the contextual information captured by the linear sequences and the structured information captured by the dependency trees may complement each other . However , existing approaches largely focused on stacking the LSTM and graph neural networks such as graph convolutional networks ( GCNs ) for building improved NER models , where the exact interaction mechanism between the two different types of features is not very clear , and the performance gain does not appear to be significant . In this work , we propose a simple and robust solution to incorporate both types of features with our Synergized - LSTM ( Syn - LSTM ) , which clearly captures how the two types of features interact . We conduct extensive experiments on several standard datasets across four languages . The results demonstrate that the proposed model achieves better performance than previous approaches while requiring fewer parameters . Our further analysis demonstrates that our model can capture longer dependencies compared with strong baselines . 1", "entities": [[5, 8, "TaskName", "named entity recognition"], [9, 10, "TaskName", "NER"], [67, 68, "MethodName", "LSTM"], [83, 84, "TaskName", "NER"], [135, 136, "MethodName", "LSTM"], [139, 140, "MethodName", "LSTM"]]}
{"text": "To incorporate the long - range dependencies , we consider an additional graph - encoded representation g t ( Figure 2 ) as the model input to integrate \u03c3 \u03c3 \u03c3 tanh tanh \u03c3 \u00d7 + \u00d7 + \u00d7 \u00d7 tanh c t - 1 Previous Cell contextual and structured information . The graphencoded representation g t can be derived from Graph Neural Networks ( GNNs ) such as GCN ( Kipf and Welling , 2017 ) , which are capable of bringing in structured information through graph structure ( Hamilton et al , 2017a ) . However , structured information sometimes is hard to encode , as we can see from the example in Figure 1 . One naive approach is to use a deep GNN to capture such information along multiple dependency arcs between two words , which could mess up information and lead to training difficulties . A straightforward solution is to integrate both structured and contextual information via LSTM . As shown in Figure 1 ( Hybrid Paths ) , the structured information can be passed to neighbors or context , which allows a model to use less number of GNN layers and alleviate such issues for long - range dependencies . The input to the LSTM can simply be the concatenation of word representation x t and g t at each position ( Jie and Lu , 2019 ) 2 . However , because such an approach requires both x t and g t to decide the value of the input gate jointly , it could be a potential victim of two sources of uncertainties : 1 ) the uncertainty of the quality of graph - encoded representation g t , and 2 ) the uncertainty of the exact interaction mechanism between the two types of features . These may lead to sub - optimal performance , especially if the graph - encoded representation g t is unsatisfactory . Thus , we need to design a new approach to incorporate both types of information from x t and g t with a more explicit interaction mechanism , with which we hope to alleviate the above issues .", "entities": [[46, 47, "DatasetName", "Cell"], [69, 70, "MethodName", "GCN"], [162, 163, "MethodName", "LSTM"], [210, 211, "MethodName", "LSTM"]]}
{"text": "SemEval 2010 Task 1 Table 2 shows comparisons of our model with baseline models on the SemEval 2010 Task 1 Catalan and Spanish datasets . Our Syn - LSTM - CRF model outperforms all existing models with F 1 82.76 and 85.09 ( p < 10 \u22125 ) compared to DGLSTM - CRF on Catalan and Spanish datasets when FastText word embeddings are used . Our model outperforms the BiLSTM - CRF model by 13.25 and 11.22 F 1 points , and outperforms BiLSTM - GCN - CRF ( Jie and Lu , 2019 ) model by 4.64 and 3.16 on Catalan and Spanish . The large performance gap between BiLSTM - GCN - CRF and our model indicates that Syn - LSTM - CRF shows better compatibility with GCN , and this confirms that simply stacking GCN on top of the BiLSTM does not perform well . Our method outperforms GCN - BiLSTM - CRF model by 5.33 and 3.24 F 1 points on Catalan and Spanish . This shows that our proposed model demonstrates a better integration of contextual information and structured information . Furthermore , our proposed method brings 1.12 and 1.62 F 1 points improvement on Catalan and Spanish datasets compare to the DGLSTM - CRF ( Jie and Lu , 2019 ) . The DGLSTM - CRF employs 2 - layer dependency guided BiLSTM to capture grandchild dependencies , which leads to longer training time and more model parameters . However , our Syn - LSTM - CRF is able to get better performance with fewer model parameters and shorter training time because of the fewer LSTM layers . Such results demonstrate that our proposed Syn - LSTM - CRF manages to capture structured information effectively . Furthermore , with the contextualized word representation , the Syn - LSTM - CRF + BERT achieves much higher performance improvement than any other method . Our model outperforms the strong baseline model DGLSTM - CRF + ELMO by 4.83 and 2.54 in terms of F 1 ( p < 10 \u22125 ) on Catalan and Spanish , respectively . OntoNotes 5.0 English To understand the generalizability of our model , we evaluate the proposed Syn - LSTM - CRF model on large scale OntoNotes 5.0 datasets . Table 3 shows comparisons of our model with baseline models on English . Our Syn - LSTM - CRF model outperforms all existing methods with 89.04 in terms of F 1 score ( p < 0.01 ) compared to DGLSTM - CRF , when GloVE word embeddings are used . Our model outperforms the BiLSTM - CRF model by 1.97 in F 1 , BiLSTM - GCN - CRF ( Jie and Lu , 2019 ) model by 0.86 . Note that our implemented GCN - BiLSTM - CRF outperforms the previous DGLSTM - CRF ( Jie and Lu , 2019 ) by 0.14 in F 1 . Our Syn - LSTM - CRF further brings the improvement to 0.52 . Moreover , with the contextualized word representation BERT , our method achieves an F 1 score of 90.85 ( p < 10 \u22125 ) compared to DGLSTM - CRF + ELMO . Our method outperforms the previous model ( Luo et al , 2020 ) , which relies on document - level information , by 0.55 in F 1 . Furthermore , the performance improvement on recall is more prominent as compared to precision . This shows that the proposed Syn - LSTM - CRF is able to extract more entities . Jie and Lu ( 2019 ) . There are also other methods ( Li et al , 2020a , b ) that use external information , ( Yu et al , 2020 ) use document - level information to encode the sentence , which are not direct comparisons to ours . forms the baseline models , specifically by 2.04 in F 1 compared to BiLSTM - CRF , by 2.39 compared to BiLSTM - GCN - CRF , by 1.86 compared to GCN - BILSTM - CRF and by 1.11 ( p < 10 \u22125 ) compared to DGLSTM - CRF when FastText is used . Note that the baseline BiLSTM - GCN - CRF model is 0.35 points worse than BiLSTM - CRF . Such results further confirm the effectiveness of our proposed Syn - LSTM - CRF for incorporating structured information . We find a similar behavior when the contextualized word representation BERT is used . With the contextualized word representation , we achieve a higher F 1 score of 80.20 .", "entities": [[28, 29, "MethodName", "LSTM"], [30, 31, "MethodName", "CRF"], [52, 53, "MethodName", "CRF"], [59, 60, "MethodName", "FastText"], [60, 62, "TaskName", "word embeddings"], [69, 70, "MethodName", "BiLSTM"], [71, 72, "MethodName", "CRF"], [83, 84, "MethodName", "BiLSTM"], [85, 86, "MethodName", "GCN"], [87, 88, "MethodName", "CRF"], [110, 111, "MethodName", "BiLSTM"], [112, 113, "MethodName", "GCN"], [114, 115, "MethodName", "CRF"], [122, 123, "MethodName", "LSTM"], [124, 125, "MethodName", "CRF"], [129, 130, "MethodName", "GCN"], [137, 138, "MethodName", "GCN"], [142, 143, "MethodName", "BiLSTM"], [151, 152, "MethodName", "GCN"], [153, 154, "MethodName", "BiLSTM"], [155, 156, "MethodName", "CRF"], [209, 210, "MethodName", "CRF"], [221, 222, "MethodName", "CRF"], [228, 229, "MethodName", "BiLSTM"], [250, 251, "MethodName", "LSTM"], [252, 253, "MethodName", "CRF"], [271, 272, "MethodName", "LSTM"], [282, 283, "MethodName", "LSTM"], [284, 285, "MethodName", "CRF"], [303, 304, "MethodName", "LSTM"], [305, 306, "MethodName", "CRF"], [307, 308, "MethodName", "BERT"], [327, 328, "MethodName", "CRF"], [329, 330, "MethodName", "ELMO"], [352, 354, "DatasetName", "OntoNotes 5.0"], [369, 370, "MethodName", "LSTM"], [371, 372, "MethodName", "CRF"], [376, 378, "DatasetName", "OntoNotes 5.0"], [396, 397, "MethodName", "LSTM"], [398, 399, "MethodName", "CRF"], [421, 422, "MethodName", "CRF"], [425, 427, "TaskName", "word embeddings"], [434, 435, "MethodName", "BiLSTM"], [436, 437, "MethodName", "CRF"], [444, 445, "MethodName", "BiLSTM"], [446, 447, "MethodName", "GCN"], [448, 449, "MethodName", "CRF"], [464, 465, "MethodName", "GCN"], [466, 467, "MethodName", "BiLSTM"], [468, 469, "MethodName", "CRF"], [474, 475, "MethodName", "CRF"], [491, 492, "MethodName", "LSTM"], [493, 494, "MethodName", "CRF"], [508, 509, "MethodName", "BERT"], [529, 530, "MethodName", "CRF"], [531, 532, "MethodName", "ELMO"], [583, 584, "MethodName", "LSTM"], [585, 586, "MethodName", "CRF"], [657, 658, "MethodName", "BiLSTM"], [659, 660, "MethodName", "CRF"], [665, 666, "MethodName", "BiLSTM"], [667, 668, "MethodName", "GCN"], [669, 670, "MethodName", "CRF"], [675, 676, "MethodName", "GCN"], [677, 678, "MethodName", "BILSTM"], [679, 680, "MethodName", "CRF"], [693, 694, "MethodName", "CRF"], [695, 696, "MethodName", "FastText"], [703, 704, "MethodName", "BiLSTM"], [705, 706, "MethodName", "GCN"], [707, 708, "MethodName", "CRF"], [714, 715, "MethodName", "BiLSTM"], [716, 717, "MethodName", "CRF"], [729, 730, "MethodName", "LSTM"], [731, 732, "MethodName", "CRF"], [747, 748, "MethodName", "BERT"]]}
{"text": "Robustness Analysis To study the robustness of our model and check whether our model can regulate the flow of information from the graphencoded representation , we analyze the influence of the quality of dependency trees . We train and evaluate an additional dependency parser ( Dozat and Manning , 2017 ) . Specifically , we train the Jie and Lu ( 2019 ) . There are also other methods ( Li et al , 2020a , b ) that use external information , which are not direct comparisons to ours . dependency parser 6 on the given training datasets and select the best model based on the dev sets . Then we apply the best model to the test sets to obtain dependency trees . We also train and evaluate our model with random dependency trees . Table 8 presents the comparisons between Syn - LSTM - CRF + BERT and DGLSTM - CRF + ELMO with given , predicted and random dependency trees . We observe that both models encounter a performance drop when we use the predicted parse trees and random trees . Our performance differences with the given parse trees are relatively smaller than the corresponding differences in DGLSTM - CRF + ELMO . Such an observation demonstrates the robustness of our proposed model against structured information from the trees of different quality . It is worthwhile to note that , with the predicted dependencies , our proposed Syn - LSTM - CRF + BERT is still able to outperform the strong baseline DGLSTM - CRF + ELMO even with the given parse trees on Catalan , English , and Chinese datasets . To further study the robustness , we conduct an analysis to investigate if the gate m t ( Figure 2 ) has the ability to regulate the flow of information from the graph - encoded representation . Intuitively , the gate m t should tend to have a small value when 0 - 0 .4 0 .4 - 0 .5 0 .5 - 0 .6 0 .6 - 0 .7 0 .7 - 0 .8 0 .8 - 0 .9 0 .9 - 1 the quality of the parse tree is not good ( e.g. , with random trees ) . We statistically plot the number of words with respect to different gate value ranges ( m t ) . Figure 4 shows the comparison between the models of using random trees and given trees on Catalan and Spanish 7 . We observe that the gate m t is more likely to open ( the value is higher ) when we use the given parse trees compared with random parse trees . Such behavior demonstrates that our proposed model can selectively aggregate the information from the graph - encoded representation .", "entities": [[145, 146, "MethodName", "LSTM"], [147, 148, "MethodName", "CRF"], [149, 150, "MethodName", "BERT"], [153, 154, "MethodName", "CRF"], [155, 156, "MethodName", "ELMO"], [203, 204, "MethodName", "CRF"], [205, 206, "MethodName", "ELMO"], [243, 244, "MethodName", "LSTM"], [245, 246, "MethodName", "CRF"], [247, 248, "MethodName", "BERT"], [258, 259, "MethodName", "CRF"], [260, 261, "MethodName", "ELMO"], [327, 328, "DatasetName", "0"], [329, 330, "DatasetName", "0"], [331, 332, "DatasetName", "0"], [334, 335, "DatasetName", "0"], [336, 337, "DatasetName", "0"], [339, 340, "DatasetName", "0"], [341, 342, "DatasetName", "0"], [344, 345, "DatasetName", "0"], [346, 347, "DatasetName", "0"], [349, 350, "DatasetName", "0"], [351, 352, "DatasetName", "0"], [354, 355, "DatasetName", "0"], [356, 357, "DatasetName", "0"]]}
{"text": "We compare the performance of our Syn - LSTM - CRF + BERT with BiLSTM - CRF + BERT and DGLSTM - CRF + ELMO models with respect to sentence length , and the results are shown in Figure 5 . We observe that the Syn - LSTM - CRF + BERT model consistently outperforms the two baseline models on the four languages 8 . In particular , although the performance tends to drop as the sentence length increases , our proposed model shows relatively better performance when the sentence length is \u2265 60 . This confirms that the proposed Syn - LSTM - CRF + BERT is able to effectively incorporate structured information . Note that our 2 - layer GCN is computed based on the dependency trees , which include both short - range dependencies and long - range dependencies . With the graph - encoded representation and the proposed Syn - LSTM - CRF + BERT , the individual word representation is enhanced by both contextual and structured information . Therefore , for the sentences with length of \u2264 14 , we can still observe obvious improvements . The significant performance improvements on the four datasets show the capability of our Syn - LSTM - CRF to capture the structured information despite the sentence length .", "entities": [[8, 9, "MethodName", "LSTM"], [10, 11, "MethodName", "CRF"], [12, 13, "MethodName", "BERT"], [14, 15, "MethodName", "BiLSTM"], [16, 17, "MethodName", "CRF"], [18, 19, "MethodName", "BERT"], [22, 23, "MethodName", "CRF"], [24, 25, "MethodName", "ELMO"], [47, 48, "MethodName", "LSTM"], [49, 50, "MethodName", "CRF"], [51, 52, "MethodName", "BERT"], [102, 103, "MethodName", "LSTM"], [104, 105, "MethodName", "CRF"], [106, 107, "MethodName", "BERT"], [121, 122, "MethodName", "GCN"], [154, 155, "MethodName", "LSTM"], [156, 157, "MethodName", "CRF"], [158, 159, "MethodName", "BERT"], [206, 207, "MethodName", "LSTM"], [208, 209, "MethodName", "CRF"]]}
{"text": "We conduct another evaluation on BiLSTM - CRF + BERT , DGLSTM - CRF + ELMO , and Syn - LSTM - CRF + BERT models with respect to entity length { 1 , 2 , 3 , 4 , 5 , \u2265 6 } on the four languages . Table 6 shows the performance comparison of two models with respect to entity length . With the structured information , both DGLSTM - CRF + ELMO and Syn - LSTM - CRF + BERT achieve better performance compared to BiLSTM - CRF + BERT . When the length of entity is \u2264 3 , Syn - LSTM - CRF + BERT achieves better results compared to DGLSTM - CRF + ELMO . This confirms that our proposed method can effectively incorporate the structured information . Our model consistently outperforms BiLSTM - CRF + BERT , and the performance tends to have more improvements when entities are getting longer except on the Chinese dataset . We note there are some special characteristics of the Chinese language . As mentioned by Jie and Lu ( 2019 ) , the percentage of entities that are able to perfectly form a sub - tree is only 92.9 % for OntoNotes Chinese , as compared to 98.5 % , 100 % , 100 % for OntoNotes English , SemEval Catalan and Spanish . Furthermore , the ratio of long entities is much higher for Catalan and Spanish compared to English and Chinese . The experimental results on Catalan and Spanish datasets show significant improvements for long entities . Such results show that the structured information conveyed by the dependency trees can be more crucial when entity length becomes longer .", "entities": [[5, 6, "MethodName", "BiLSTM"], [7, 8, "MethodName", "CRF"], [9, 10, "MethodName", "BERT"], [13, 14, "MethodName", "CRF"], [15, 16, "MethodName", "ELMO"], [20, 21, "MethodName", "LSTM"], [22, 23, "MethodName", "CRF"], [24, 25, "MethodName", "BERT"], [73, 74, "MethodName", "CRF"], [75, 76, "MethodName", "ELMO"], [79, 80, "MethodName", "LSTM"], [81, 82, "MethodName", "CRF"], [83, 84, "MethodName", "BERT"], [89, 90, "MethodName", "BiLSTM"], [91, 92, "MethodName", "CRF"], [93, 94, "MethodName", "BERT"], [106, 107, "MethodName", "LSTM"], [108, 109, "MethodName", "CRF"], [110, 111, "MethodName", "BERT"], [118, 119, "MethodName", "CRF"], [120, 121, "MethodName", "ELMO"], [139, 140, "MethodName", "BiLSTM"], [141, 142, "MethodName", "CRF"], [143, 144, "MethodName", "BERT"], [205, 206, "DatasetName", "OntoNotes"], [220, 221, "DatasetName", "OntoNotes"]]}
{"text": "To fully explore the impact of the number of GCN layers , we conduct another experiment on Syn - LSTM - CRF + BERT model with the number of GCN layers { 1 , 2 , 3 } , and Figure 6 shows the performance on the dev set of the four languages . The last bar , indicated as AVG , is obtained by averaging the dev results on the four datasets . We observe that the overall performance is better when the number of GCN layers equals 2 . Note that similar behavior can also be found in the work by Kipf and Welling ( 2017 ) for document classification and node classification . Therefore , we evaluate our proposed Syn - LSTM - CRF model with 2 - layer GCN . Ablation Study To understand the contribution of each component , we conduct an ablation study on the OntoNotes 5.0 English dataset , and Table 7 presents the detailed results of our model with contextualized representation . We find that the performance drops by 0.24 F 1 score when we only use 1 - layer GCN . Without GCN at all , the score drops by 1.13 F 1 . The original dependency contributes 0.27 F 1 score . Removing the dependency relation embedding also decreases the performance by 0.27 F 1 . When we remove the POS tags embedding , the result drops by 0.39 F 1 .", "entities": [[9, 10, "MethodName", "GCN"], [19, 20, "MethodName", "LSTM"], [21, 22, "MethodName", "CRF"], [23, 24, "MethodName", "BERT"], [29, 30, "MethodName", "GCN"], [86, 87, "MethodName", "GCN"], [110, 112, "TaskName", "document classification"], [113, 115, "TaskName", "node classification"], [124, 125, "MethodName", "LSTM"], [126, 127, "MethodName", "CRF"], [132, 133, "MethodName", "GCN"], [151, 153, "DatasetName", "OntoNotes 5.0"], [188, 189, "MethodName", "GCN"], [191, 192, "MethodName", "GCN"]]}
{"text": "LSTM LSTM has demonstrated its great effectiveness in many NLP tasks and becomes a standard module for many state - of - the - art models ( Wen et al , 2015 ; Ma and Hovy , 2016 ; Dozat and Manning , 2017 ) . However , the sequential nature of the LSTM makes it challenging to capture long - range dependencies . Zhang et al ( 2018a ) propose the S - LSTM model to include a sentence state to allow both local and global information exchange simultaneously . Mogrifier LSTM ( Melis et al , 2020 ) mutually gates the current input and the previous output to enhance the interaction between the input and the context . These two works do not consider structured information for the LSTM design . Since natural language is usually structured , Shen et al ( 2018 ) propose ON - LSTM to add a hierarchical bias to allow the neurons to be updated by following certain order . While the ON - LSTM is learning the latent constituency parse trees , we focus on incorporating the explicit structured information conveyed by the dependency parse trees . NER Early work ( Sasano and Kurohashi , 2008 ) uses syntactic dependency features to improve the SVM performance on Japanese NER task . Liu et al ( 2010 ) propose to construct skip - edges to link similar words or words having typed dependencies to capture long - range dependencies . The later works ( Collobert et al , 2010 ; Lample et al , 2016 ; Chiu and Nichols , 2016b ) focus on using neural networks to extract features and achieved the stateof - the - art performance . Jie et al ( 2017 ) find that some relations between the dependency edges and the entities can be used to reduce the search space of their model , which significantly reduces the time complexity . Yu et al ( 2020 ) employ pre - trained language model to encode document - level information to explore all spans with the graph - based dependency graph based ideas . The pre - trained language models ( e.g. , BERT ( Devlin et al , 2019 ) , ELMO ( Peters et al , 2018 ) ) further improve neuralbased approaches with a good contextualized representation . However , previous works did not focus on investigating how to effectively integrate structured and contextual information well .", "entities": [[0, 1, "MethodName", "LSTM"], [1, 2, "MethodName", "LSTM"], [53, 54, "MethodName", "LSTM"], [74, 75, "MethodName", "LSTM"], [91, 93, "MethodName", "Mogrifier LSTM"], [130, 131, "MethodName", "LSTM"], [149, 150, "MethodName", "LSTM"], [171, 172, "MethodName", "LSTM"], [195, 196, "TaskName", "NER"], [212, 213, "MethodName", "SVM"], [216, 217, "TaskName", "NER"], [364, 365, "MethodName", "BERT"], [373, 374, "MethodName", "ELMO"]]}
{"text": "Table 9 shows the statistics of the number of entities with respect to entity length for OntoNotes 5.0 English and Chinese , SemEval 2010 Task 1 Catalan and Spanish datasets .", "entities": [[16, 18, "DatasetName", "OntoNotes 5.0"]]}
{"text": "Figure 7 shows the comparisons of the models of using random trees and given trees on OntoNotes 5.0 English and Chinese datasets .", "entities": [[16, 18, "DatasetName", "OntoNotes 5.0"]]}
{"text": "We compare the performance of our Syn - LSTM - CRF + BERT with BiLSTM - CRF + BERT and DGLSTM - CRF + ELMO models with respect to sentence length , and the results are shown in Figure 8 .", "entities": [[8, 9, "MethodName", "LSTM"], [10, 11, "MethodName", "CRF"], [12, 13, "MethodName", "BERT"], [14, 15, "MethodName", "BiLSTM"], [16, 17, "MethodName", "CRF"], [18, 19, "MethodName", "BERT"], [22, 23, "MethodName", "CRF"], [24, 25, "MethodName", "ELMO"]]}
{"text": "We further show an example to visualize the propagation of non - local information ( Figure 9 ) . The example is selected from OntoNotes 5.0 English dataset . Even though the DGLSTM - CRF ( Jie and Lu , 2019 ) model is able to recognize \" Tianshui \" as a named entity , it predicts a wrong entity type as PERSON while the true type is GPE . If only looking at the first half of the sentence , it is possible to predict \" Tianshui \" as PERSON because of the local information \" age \" . However , the second half of the sentence confirms that the entity type of", "entities": [[24, 26, "DatasetName", "OntoNotes 5.0"], [34, 35, "MethodName", "CRF"]]}
{"text": "English Chinese Catalan Spanish P. R. F 1 P. During Tanshui 's golden age , large and small boats were constantly coming and going in the harbor , and it was not usual to see enormous steamships . ROOT Figure 9 : An example of dependency tree . The mentioned entity is highlighted in orange , and the entity type is GPE . \" Tianshui \" is GPE . With the non - local information from the graph - encoded representation , our Syn - LSTM - CRF successfully predicts the right entity type .", "entities": [[85, 86, "MethodName", "LSTM"], [87, 88, "MethodName", "CRF"]]}
{"text": "In recent years , we have seen deep learning and distributed representations of words and sentences make impact on a number of natural language processing tasks , such as similarity , entailment and sentiment analysis . Here we introduce a new task : understanding of mental health concepts derived from Cognitive Behavioural Therapy ( CBT ) . We define a mental health ontology based on the CBT principles , annotate a large corpus where this phenomena is exhibited and perform understanding using deep learning and distributed representations . Our results show that the performance of deep learning models combined with word embeddings or sentence embeddings significantly outperform non - deep - learning models in this difficult task . This understanding module will be an essential component of a statistical dialogue system delivering therapy .", "entities": [[33, 35, "TaskName", "sentiment analysis"], [54, 55, "DatasetName", "CBT"], [62, 63, "MethodName", "ontology"], [66, 67, "DatasetName", "CBT"], [100, 102, "TaskName", "word embeddings"], [103, 105, "TaskName", "sentence embeddings"]]}
{"text": "Promotion of mental well - being is at the core of the action plan on mental health 2013 - 2020 of the World Health Organisation ( WHO ) ( World Health Organization , 2013 ) and of the European Pact on Mental Health and Well - being of the European Union ( EU high - level conference : Together for Mental Health and Well - being , 2008 ) . The biggest potential breakthrough in fighting mental illness would lie in finding tools for early detection and preventive intervention ( Insel and Scholnick , 2006 ) . The WHO action plan stresses the importance of health policies and programmes that not only meet the need of people affected by mental disorders but also protect mental well - being . The emphasis is on early evidence - based non - pharmacological intervention , avoiding institutionalisation and medicalisation . What is particularly important for successful intervention is the frequency with which the therapy can be accessed ( Hansen et al , 2002 ) . This gives automated systems a huge advantage over conventional therapies , as they can be used continuously with marginal extra cost . Health assistants that can deliver therapy , have gained great interest in recent years ( Bickmore et al , 2005 ; Fitzpatrick et al , 2017 ) . These systems however are largely based on hand - crafted rules . On the other hand , the main research effort in statistical approaches to conversational systems has focused on limited - domain information seeking dialogues ( Schatzmann et al , 2006 ; Geist and Pietquin , 2011 ; Gasic and Young , 2014 ; Fatemi et al , 2016 ; Li et al , 2016 ; Williams et al , 2017 ) . In this paper we introduce a new task : understanding of mental health concepts derived from Cognitive Behavioural Therapy ( CBT ) . We present an ontology that is formulated according to Cognitive Behavioural Therapy principles . We label a high quality mental health corpus , which exhibits targeted psychological phenomena . We use the whole unlabelled dataset to train distributed representations of words and sentences . We then investigate two approaches for classifying the user input according to the defined ontology . The first model involves a convolutional neural network ( CNN ) operating over distributed words representations . The second involves a gated recurrent network ( GRU ) operating over distributed representation of sentences . Our models perform significantly better than chance and for instances with a large number of data they reach the inter - annotator agreement . This understanding module will be an essential component of a statistical dialogue system delivering therapy . The paper is organised as follows . In Section 2 we give a brief background of the statistical approach to dialogue modelling , focusing on dialogue ontology and natural language understanding . In Section 3 we review related work in the area of automated mental - health assistants . The sections that follow represent the main contribution of this work : a CBT ontology in Section 4 , a labelled dataset in Section 5 , and models for language understanding in Section 6 . We present the results in Section 7 and our conclusion in Section 8 .", "entities": [[316, 317, "DatasetName", "CBT"], [322, 323, "MethodName", "ontology"], [377, 378, "MethodName", "ontology"], [404, 405, "MethodName", "GRU"], [479, 480, "MethodName", "ontology"], [481, 484, "TaskName", "natural language understanding"], [515, 516, "DatasetName", "CBT"], [516, 517, "MethodName", "ontology"]]}
{"text": "A dialogue system can be treated as a trainable statistical model suitable for goal - oriented information seeking dialogues ( Young , 2002 ) . In these dialogues , the user has a clear goal that he or she is trying to achieve and this involves extracting particular information from a back - end database . A structured representation of the database , the ontology is a central element of a dialogue system . It defines the concepts that the dialogue system can understand and talk about . Another critical component is the natural language understanding unit , which takes textual user input and detects presence of the ontology concepts in the text .", "entities": [[64, 65, "MethodName", "ontology"], [93, 96, "TaskName", "natural language understanding"], [108, 109, "MethodName", "ontology"]]}
{"text": "Statistical approaches to dialogue modelling have been applied to relatively simple domains . These systems interface databases of up to 1000 entities where each entity has up to 20 properties , i.e. slots ( Cuay\u00e1huitl , 2009 ) . There has been a significant amount of work in spoken language understanding focused on exploiting large knowledge graphs in order to improve coverage ( T\u00fcr et al , 2012 ; Heck et al , 2013 ) . Despite these efforts , little work has been done on mental health ontologies for supporting cognitive behavioural therapy on dialogue systems . Available medical ontologies follow a symptom - treatment categorisation and are not suitable for dialogue or natural language understanding ( Bluhm , 2017 ; Hofmann , 2014 ; Wang et al , 2018 ) .", "entities": [[48, 51, "TaskName", "spoken language understanding"], [55, 57, "TaskName", "knowledge graphs"], [114, 117, "TaskName", "natural language understanding"]]}
{"text": "Within a dialogue system , a natural language understanding unit extracts meaning from user sentences . Both classification ( Mairesse et al , 2009 ) and sequence - to - sequence ( Yao et al , 2014 ; Mesnil et al , 2015 ) models have been applied to address this task . Deep learning architectures that exploit distributed word - vector representations have been successfully applied to different tasks in natural language understanding , such as semantic role labelling , semantic parsing , spoken language un - derstanding , sentiment analysis or dialogue belief tracking ( Collobert et al , 2011 ; Kim , 2014 ; Kalchbrenner et al , 2014 ; Le and Mikolov , 2014a ; Rojas Barahona et al , 2016 ; Mrk\u0161i\u0107 et al , 2017 ) . In this work we consider understanding of mental health concepts of as a classification task . To facilitate this process , we use distributed representations .", "entities": [[6, 9, "TaskName", "natural language understanding"], [71, 74, "TaskName", "natural language understanding"], [81, 83, "TaskName", "semantic parsing"], [90, 92, "TaskName", "sentiment analysis"]]}
{"text": "The aim of building an automated therapist has been around since the first time researchers attempted to build a dialogue system ( Weizenbaum , 1966 ) . Automated health advice systems built to date typically rely on expert coded rules and have limited conversational capabilities ( Rojas - Barahona and Giorgino , 2009 ; Vardoulakis et al , 2012 ; Ring et al , 2013 ; Riccardi , 2014 ; DeVault et al , 2014 ; Ring et al , 2016 ) . One particular system that we would like to highlight is an affectively aware virtual therapist ( Ring et al , 2016 ) . This system is based on Cognitive Behavioural Therapy and the system behaviour is scripted using VoiceXML . There is no language understanding : the agent simply asks questions and the user selects answers from a given list . The agent is however able to interpret hand gestures , posture shifts , and facial expressions . Another notable system ( De - Vault et al , 2014 ) has a multi - modal perception unit which captures and analyses user behaviour for both behavioural understanding and interaction . The measurements contribute to the indicator analysis of affect , gesture , emotion and engagement . Again , no statistical language understanding takes place and the behaviour of the system is scripted . The system does not provide therapy to the user but is rather a tool that can support healthcare decisions ( by human healthcare professionals ) . The Stanford Woebot chat - bot proposed by ( Fitzpatrick et al , 2017 ) is designed for delivering CBT to young adults with depression and anxiety . It has been shown that the interaction with this chat - bot can significantly reduce the symptoms of depression when compared to a group of people directed to a read a CBT manual . The conversational agent appears to be effective in engaging the users . However , the understanding component of Woebot has not been fully described . The dialogue decisions are based on decision trees . At each node , the user is expected to choose one of several predefined responses . Limited language understanding was in - troduced at specific points in the tree to determine routing to subsequent conversational nodes . Still , one of the main deficiencies reported by the trial participants in ( Fitzpatrick et al , 2017 ) was the inability to converse naturally . Here we address this problem by performing statistical natural language understanding .", "entities": [[130, 131, "DatasetName", "agent"], [145, 146, "DatasetName", "agent"], [205, 206, "DatasetName", "emotion"], [271, 272, "DatasetName", "CBT"], [311, 312, "DatasetName", "CBT"], [316, 317, "DatasetName", "agent"], [409, 410, "DatasetName", "converse"], [420, 423, "TaskName", "natural language understanding"]]}
{"text": "To define the ontology we draw from principles of Cognitive Behavioural Therapy ( CBT ) . This is one of the best studied psychotherapeutic interventions , and the most widely used psychological treatment for mental disorders in Britain ( Bhasi et al , 2013 ) . There is evidence that CBT is more effective than other forms of psychotherapy ( Tolin , 2010 ) . Unlike other , longer - term , forms of therapy such as psychoanalysis , CBT can have a positive effect on the client within a few sessions . Also , due to it being highly structured , it is more easily amenable by computer interpretation . This is why we adopted CBT as the basis of our work . Cognitive Behavioural Therapy is derived from Cognitive Therapy model theory ( Beck , 1976 ; Beck et al , 1979 ) , which postulates that our emotions and behaviour are influenced by the way we think and by how we make sense of the world . The idea is that , if the client changes the way he or she thinks about their problem , this will in turn change the way he or she feels , and behaves . A major underlying principle of CBT is the idea of cognitive distortions , and the value in challenging them . In CBT , clients are helped to test their assumptions and views of the world in order to check if they fit with reality . When clients learn that their perceptions and interpretations are distorted or unhelpful they then work on correcting them . Within the realm of cognitive distortion , CBT identifies a number of specific self - defeating thought processes , or thinking errors . There is a core of around 10 to 15 thinking errors , with their exact titles having some fluidity . A strong component of CBT is teaching clients to be able to recognize and identify the thinking errors themselves , and ultimately discard the negative thought processes and 're - think ' their problems . We consider the main analytical step in this therapy : an adequate decoding of these ' thinking error ' concepts , and the identification of the key emotion ( s ) and the situational context of a particular problem . Therefore , our ontology consists of think - ing errors , emotions , and situations .", "entities": [[3, 4, "MethodName", "ontology"], [13, 14, "DatasetName", "CBT"], [50, 51, "DatasetName", "CBT"], [79, 80, "DatasetName", "CBT"], [116, 117, "DatasetName", "CBT"], [209, 210, "DatasetName", "CBT"], [225, 226, "DatasetName", "CBT"], [275, 276, "DatasetName", "CBT"], [315, 316, "DatasetName", "CBT"], [373, 374, "DatasetName", "emotion"], [389, 390, "MethodName", "ontology"]]}
{"text": "Notwithstanding slight variations in number and terminology , the list of thinking errors is fairly well standardised in the CBT literature . We present one such list in Table 1 . However , it is important to note that there is a fair degree of overlap between different thinking errors , for example , between Jumping to Negative Conclusions and Fortune Telling , or between Disqualifying the Positives and Mental Filtering . In addition , within the data used - and as is likely to be the case in any data of spontaneous expressions of psychological upset - a single problem can exhibit several thinking errors simultaneously . Thus , the situation is much more challenging than in simple information - seeking dialogues , where ontologies are typically clearly defined and there is no or very little overlap between concepts .", "entities": [[19, 20, "DatasetName", "CBT"]]}
{"text": "In addition to thinking errors , we define a set of emotions . We mainly focus on negative emotions , relevant to people in psychological distress . In CBT , emotions tend to be divided into positive and negative , or helpful / healthy and unhelpful/ unhealthy emotions ( Branch and Willson , 2010 ) . The set of emotions for this work evolved over time in the early days of annotation . Although we initally agreed to focus on ' unhealthy ' emotions , as defined by CBT , there seemed also to be a place for the ' healthy ' emotion Grief / sadness . Overall , the list of emotions used was drawn from a number of sources , including CBT literature , the annotators ' own knowledge of what they work with in psychological therapy , and the common emotions that were seen emerging from the data early on in the process . Note that more than one emotion might be expressed within an individual problem - for example Depression and Loneliness . The list of emotions is given in Table 2 .", "entities": [[28, 29, "DatasetName", "CBT"], [88, 89, "DatasetName", "CBT"], [102, 103, "DatasetName", "emotion"], [123, 124, "DatasetName", "CBT"], [162, 163, "DatasetName", "emotion"]]}
{"text": "The corpus consists of 500 K written posts that users anonymously posted on the Koko platform 1 . This platform is based on the peer - to - peer therapy proposed by ( Morris et al , 2015 ) . In this set - up , a user anonymously posts their problem ( referred to 1 https://itskoko.com/ as the problem ) and is prompted to consider their most negative take on the problem ( referred to as the negative take ) . Subsequently , peers post responses that attempt to offer a re - think and give a more positive angle on the problem . When first developed , this peer - to - peer framework was shown to be more efficacious than expressive writing , an intervention that is known to improve physical and emotional well - being ( Morris et al , 2015 ) . Since then , the app developed by Koko has collected a very large number of posts and associated responses . Initially , any first - time Koko user would be given a short introductory tutorial in the art of 're - thinking'/'re - framing ' problems ( based on CBT principles ) , before being able to use the platform . This however changed over time , as the age of the users decreased , and a different tutorial , emphasizing empathy and optimism , was used ( less CBT - based than the 're - thinking ' ) . Most of the data annotated in this study was drawn from the earlier phase . Figure 1 gives an annotated post example .", "entities": [[196, 197, "DatasetName", "CBT"], [236, 237, "DatasetName", "CBT"]]}
{"text": "A subset of posts was annotated by two psychological therapists using a web annotation tool that we developed . The annotation tool allowed annotators to have a quick view of the posts , showing up to 50 posts per page , to navigate through posts , to check pending posts and to annotate them by adding or removing thinking errors , emotions and situations . All annotations were stored in a MySQL database . Initially 1000 posts were analysed . These were used to define the ontology . Then 4035 posts were labelled with thinking errors , emotions and situations . It takes an experienced psychological therapist about one minute to annotate one post . Note that the same post can exhibit multiple thinking errors , emotions and situations , which makes the whole process more complex . We randomly selected 50 posts and calculated the inter - annotator agreement . The inter - annotator agreement was calculated using a contingency table for thinking error , emotion and situation , showing agreement and disagreement between the two annotators . Then , Cohen 's kappa was calculated discounting the possibility that the agreement may happen by chance . The result is shown in due to the unbounded number of thinking errors per post . In other words , the annotators typically have three or four thinking errors in common but one of them might have detected one or two more . Still , the agreement is much higher than chance , so we think that while challenging , it is possible to build a classifier for this task . The distributions of labelled posts with multiple sub - categories for three super - categories are shown in Figure 2 6 Deep learning model", "entities": [[86, 87, "MethodName", "ontology"], [166, 167, "DatasetName", "emotion"]]}
{"text": "The task of decoding thinking errors and emotions is closely related to the task of sentiment analysis . In sentiment analysis we are concerned with positive or negative sentiment expressed in a sentence . Detecting thinking errors or emotions could be perceived as detecting different kinds of negative sentiment . Distributed representations of words , sentences and documents have gained success in sentiment detection and similarity tasks ( Le and Mikolov , 2014a ; Maas et al , 2011 ; Kiros et al , 2015 ) . A key advantage of these representations is that they can be obtained in an unsupervised manner , thus allowing exploitation of large amounts of unlabelled data . This is precisely what we have in our set - up , where only a small portion of our posts is labelled . We utilise GloVe ( Pennington et al , 2014 ) word vectors , which have previously achieved competitive results in a similarity task . We train the word vectors on the whole dataset and then use a convolutional neural network ( CNN ) to extract features from posts where words are represented as vectors . We also consider distributed representation of sentences . A particularly competitive model is the skip - thought model , which is obtained from an encoder - decoder model that tries to reconstruct the surrounding sentences of an encoded passage ( Kiros et al , 2015 ) . On similarity tasks it outperfoms the simpler doc2vec model ( Le and Mikolov , 2014a ) . An approach that represents vectors by weighted averages of word vectors and then modifies them using PCA and SVD outperforms skipthought vectors ( Arora et al , 2017 ) . This method however does not do well on a sentiment analysis task due to down - weighting of words like \" not \" . As these often appear in our corpus , we chose skipthought vectors for investigation here . The skip - thought model allows a dense representation of the utterance . We train skip - thought vectors using the method described in ( Kiros et al , 2015 ) . The automatically generated post shown in Fig 3 demonstrates that skip - thought vectors can convey the sentiment well in accordance to context . We then train a gated recurrent unit ( GRU ) network using the skip - thoughts as input . i ' m so depressed . i ' m worthless . No one likes me i ' m try being nice but . No light at every point i ' m unpopular and i ' m a < NUM > year old potato . my most negative take is that i 'll never know how to be as socially as a quiet girl . i will stop talking to how fragile is and be any ways of normal people . Figure 3 : An example of a generated post using skipthought vectors initialised with \" I 'm so depressed \" .", "entities": [[15, 17, "TaskName", "sentiment analysis"], [19, 21, "TaskName", "sentiment analysis"], [139, 140, "MethodName", "GloVe"], [272, 273, "MethodName", "PCA"], [274, 275, "DatasetName", "SVD"], [295, 297, "TaskName", "sentiment analysis"], [386, 389, "MethodName", "gated recurrent unit"], [390, 391, "MethodName", "GRU"]]}
{"text": "The convolutional neural network ( CNN ) model is preferred over a recurrent neural network ( RNN ) model , because the posts are generally too long for an RNN to maintain memory over words . The convolutional neural network ( CNN ) used in this work is inspired by ( Kim , 2014 ) and operates over pre - trained GloVe embeddings of dimensionality d. As shown in Fig 4 , the network has two inputs , one for the problem and the other for the negative take . These are represented as two tensors . A convolutional operation involves a filter w R ld which is applied to l words to produce the feature map . Then , a max - pooling operation is applied to produce two vectors : p for problem and n for negative take . The reason for this is that the negative take is usually a summary of the post , carrying stronger sentiment ( see Figure 1 ) . We use a gating mechanism to combine p and n as follows : g = \u03c3 ( W p p + W n n + b ) ( 1 ) h = g p + ( 1 \u2212 g ) n ( 2 ) Here , \u03c3 is the sigmoid function , W p , W n and W are weight matrices , b is a bias term , 1 is a vector of ones , is the element - wise product , and g is the output of the gating mechanism . The extracted feature h is then processed with a one - layer fullyconnected neural network ( FNN ) to perform binary classification . The model is illustrated in Fig 4 .", "entities": [[61, 63, "MethodName", "GloVe embeddings"]]}
{"text": "We use the gated recurrent unit ( GRU ) model to process skip - thought sentence vectors , for two reasons . First , most posts contain less than 5 sentences , so a recurrent neural network is more suitable than a convolutional neural network . Second , since our corpus only comprises very limited labelled data , a GRU should perform better than a long short - term memory ( LSTM ) network as it has less parameters . Denote each post as P = { s 1 , s 2 , ... , s t , ... } , where s t is the t th sentence in post P . First , we use an already trained GRU to extract skip - thought embeddings e t from the sentences s t . Then , taking the sequence of sentence vectors { e 1 , e 2 , ... , e t , ... } as input , another GRU is used as follows : z t = \u03c3 ( W z h t\u22121 + U z e t + b z ) ( 3 ) r t = \u03c3 ( W r h t\u22121 + U r e t + b r ) ( 4 ) h t = tanh ( W ( r t h t\u22121 ) + Ue t + b h ) ( 5 ) h t = z t h t\u22121 + ( 1 \u2212 z t ) h t ( 6 ) W z , U z , W r , U r , W , U are recurrent weight ma - trices , b z , b r , b h are bias terms , is the elementwise dot product , and \u03c3 is the sigmoid function . Finally , the last hidden state h T is fed into a FNN with one hidden layer of the same size as input . The model is illustrated in Fig 5 .", "entities": [[3, 6, "MethodName", "gated recurrent unit"], [7, 8, "MethodName", "GRU"], [59, 60, "MethodName", "GRU"], [65, 70, "MethodName", "long short - term memory"], [71, 72, "MethodName", "LSTM"], [120, 121, "MethodName", "GRU"], [161, 162, "MethodName", "GRU"]]}
{"text": "For rule - based models , we chose a chance classifier and a majority classifier , where all the posts are treated as positive examples for each class . In addition , we trained two non - deep - learning models , the logistic regression ( LR ) model and the Support Vector Machine ( SVM ) . Both of them take the bag - of - words feature as input and implemented in sklearn ( Pedregosa et al , 2011 ) . For completeness , we also trained 100 and 300 dimensions PV - DM document embeddings ( Le and Mikolov , 2014b ) as the distributed representations of the posts using the gensim toolkit ( \u0158eh\u016f\u0159ek and Sojka , 2010 ) , and employ FNNs to do the classification , the hidden size is set as 800 to ensure parameters of all deep learning models comparable . All the baseline models are trained with the same set - up as described in section 6.4 .", "entities": [[43, 45, "MethodName", "logistic regression"], [51, 54, "MethodName", "Support Vector Machine"], [55, 56, "MethodName", "SVM"]]}
{"text": "We presented an ontology based on the principles of Cognitive Behavioural Therapy . We then annotated data that exhibits psychological problems and computed the inter - annotator agreement . We found that classifying thinking errors is a difficult task as suggested by the low inter - annotator agreement . We trained GloVe word embeddings and skip - thought embeddings on 500 K posts in an unsupervised fashion and generated distributed representations both of words and of sentences . We then used the GloVe word vectors as input to a CNN and the skip - thought sentence vectors as input to a GRU . The results suggest that both models significantly outperform a chance classifier for all thinking errors , emotions and situations with CNN - GloVe on average achieving better results . Areas of future investigation include richer dis - tributed representations , or a fusion of distributed representations from word - level , sentence - level and document - level , to acquire more powerful semantic features . We also plan to extend the current ontology with its focus on thinking errors , emotions and situations to include a much lager number of concepts . The development of a statistical system delivering therapy will moreover require further research on other modules of a dialogue system .", "entities": [[3, 4, "MethodName", "ontology"], [51, 52, "MethodName", "GloVe"], [52, 54, "TaskName", "word embeddings"], [82, 83, "MethodName", "GloVe"], [101, 102, "MethodName", "GRU"], [125, 126, "MethodName", "GloVe"], [176, 177, "MethodName", "ontology"]]}
{"text": "A Transparent Framework for Evaluating Unintended Demographic Bias in Word Embeddings", "entities": [[9, 11, "TaskName", "Word Embeddings"]]}
{"text": "Word embedding models have gained a lot of traction in the Natural Language Processing community , however , they suffer from unintended demographic biases . Most approaches to evaluate these biases rely on vector space based metrics like the Word Embedding Association Test ( WEAT ) . While these approaches offer great geometric insights into unintended biases in the embedding vector space , they fail to offer an interpretable meaning for how the embeddings could cause discrimination in downstream NLP applications . In this work , we present a transparent framework and metric for evaluating discrimination across protected groups with respect to their word embedding bias . Our metric ( Relative Negative Sentiment Bias , RNSB ) measures fairness in word embeddings via the relative negative sentiment associated with demographic identity terms from various protected groups . We show that our framework and metric enable useful analysis into the bias in word embeddings .", "entities": [[120, 122, "TaskName", "word embeddings"], [151, 153, "TaskName", "word embeddings"]]}
{"text": "Word embeddings have established themselves as an integral part of Natural Language Processing ( NLP ) applications . Unfortunately word embeddings have also introduced unintended biases that could cause downstream NLP systems to be unfair . Recent studies have shown that word embeddings exhibit unintended gender and stereotype biases inherent in the training corpus . Bias can be defined as an unfair expression of prejudice for or against a person , a group , or an idea . Bias is a broad term , which covers a range of problems particularly relevant in natural language systems such as , discriminatory gender bias ( Bolukbasi et al , 2016a ; Zhao et al , 2017 ) , bias against regionally accented speech ( Najafian et al , 2016 ( Najafian et al , , 2017 , personal or political view bias ( Iyyer et al , 2014 ; Recasens et al , 2013 ) , and many other examples . In Figure 1 : 2 - D PCA embeddings for positive / negative sentiment words and a set of national origin identity terms . Geometrically , it is difficult to parse how these embeddings can lead to discrimination . our work , we restrict our definition of bias to unequal distributions of negative sentiment among demographic identity terms in word embeddings . One could also look at unequal distributions of positive sentiment , but for this work we restrict ourselves to the negative case . Sentiment analysis makes up a large portion of current NLP systems . Therefore , preventing negative sentiment from mixing with sensitive attributes ( i.e. race , gender , religion ) in word embeddings is needed to prevent discrimination in ML models using the embeddings . As studied in ( Packer et al , 2018 ) , unintentionally biased word embeddings can have adverse consequences when deployed in applications , such as movie sentiment analyzers or messaging apps . Negative sentiment can be unfairly entangled in the word embeddings , and detecting this unintended bias is a difficult problem . We need clear signals to evaluate which groups are discriminated against due to the bias in an embedding model . That way we can pinpoint where to mitigate those biases . To demonstrate this need for clear signals of bias in word embeddings , we look at Figure 1 . Figure 1 shows a 2D word embedding projection of positive sentiment ( green ) and negative sentiment ( red ) words . It would be unfair for any given demographic identity word vector ( blue ) to be more semantically related to negative terms than the other identities . However , many identity terms exist closer to negative words than other identity terms in the vector space . This bias may affect a downstream ML model , but the vector space has no absolute interpretable meaning , especially when it comes to whether this embedding model will lead to a unfairly discriminative algorithm . Our framework enables transparent insights into word embedding bias by instead viewing the output of a simple logistic regression algorithm trained on an unbiased positive / negative word sentiment dataset initialized with biased word vectors . We use this framework to create a clear metric for unintended demographic bias in word embeddings .", "entities": [[0, 2, "TaskName", "Word embeddings"], [19, 21, "TaskName", "word embeddings"], [41, 43, "TaskName", "word embeddings"], [166, 167, "MethodName", "PCA"], [218, 220, "TaskName", "word embeddings"], [244, 246, "TaskName", "Sentiment analysis"], [275, 277, "TaskName", "word embeddings"], [302, 304, "TaskName", "word embeddings"], [330, 332, "TaskName", "word embeddings"], [384, 386, "TaskName", "word embeddings"], [514, 516, "MethodName", "logistic regression"], [547, 549, "TaskName", "word embeddings"]]}
{"text": "Researchers have found a variety of ways in which dangerous unintended bias can show up in NLP applications ( Blodgett and O'Connor , 2017 ; Hovy and Spruit , 2016 ; Tatman , 2017 ) . Mitigating such biases is a difficult problem , and researchers have created many ways to make fairer NLP applications . Much of the focus for mitigating unintended bias in NLP is either targeted at reducing gender stereotypes in text ( Bolukbasi et al , 2016b , a ; Zhao et al , 2017 ; Zhang et al , 2018 ) , or inequality of sentiment or toxicity for various protected groups ( Caliskan - Islam et al , 2016 ; Bakarov , 2018 ; Dixon et al ; Garg et al , 2018 ; Kiritchenko and Mohammad , 2018 ) . More specifically , word embeddings has been an area of focus for evaluating unintended bias . ( Bolukbasi et al , 2016b ) defines a useful metric for identifying gender bias and ( Caliskan - Islam et al , 2016 ) defines a metric called the WEAT score for evaluating unfair correlations with sentiment for various demographics in text . Unfortunately metrics like these leverage vector space arguments between only two identities at a time like man vs woman ( Bolukbasi et al , 2016a ) , or European American names vs. African American names ( Caliskan - Islam et al , 2016 ) . Though geometrically intuitive , these tests do not have a direct relation to discrimination in general . Our framework and RNSB metric enable a clear evaluation of discrimination with respect to word embedding bias for a whole class of demographics .", "entities": [[140, 142, "TaskName", "word embeddings"]]}
{"text": "We present our framework for understanding and evaluating unintentional demographic bias in word embeddings . We first describe the flow of our framework . Then , we address which datasets / models were chosen for our approach . Finally , we show how our framework can enable analysis and new metrics like RNSB .", "entities": [[12, 14, "TaskName", "word embeddings"]]}
{"text": "Figure 2 : We isolate unintended bias to the word embeddings by training a logistic regression classifier on a unbiased positive / negative word sentiment dataset ( initialized with the biased word embeddings ) . We measure word embedding bias by analyzing the predicted probability of negative sentiment for identity terms . Our framework enables the evaluation of unintended bias in word embeddings through the results of negative sentiment predictions . Our framework has a simple layout . Figure 2 shows the flow of our system . We first use the embedding model we are trying to evaluate to initialize vectors for an unbiased positive / negative word sentiment dataset . Using this dataset , we train a logistic classification algorithm to predict the probability of any word being a negative sentiment word . After training , we take a set of neutral identity terms from a protected group ( i.e. national origin ) and predict the probability of negative sentiment for each word in the set . Neutral identity terms that are unfairly entangled with negative sentiment in the word embeddings will be classified like their neighboring sentiment words from the sentiment dataset . We leverage this set of negative sentiment probabilities to summarize unintended demographic bias using RNSB .", "entities": [[9, 11, "TaskName", "word embeddings"], [14, 16, "MethodName", "logistic regression"], [31, 33, "TaskName", "word embeddings"], [61, 63, "TaskName", "word embeddings"], [180, 182, "TaskName", "word embeddings"]]}
{"text": "We evaluate three pretrained embedding models : GloVe ( Pennington et al , 2014 ) , Word2vec ( Mikolov et al , 2013 ) ( trained on the large Google News corpus ) , and ConceptNet . GloVe and Word2vec embeddings have been shown to contain unintended bias in ( Bolukbasi et al , 2016a ; Caliskan - Islam et al , 2016 ) . ConceptNet has been shown to be less biased than these models ( Speer , 2017 ) due to the mixture of curated corpora used for training . As part of our pipeline , we also use a labeled positive / negative sentiment training set ( Hu and Liu , 2004 ) . This dataset has been shown to be a trustworthy lexicon for negative and positive sentiment words ( Pang et al , 2008 ; Liu , 2012 ; Wilson et al , 2005 ) . We trust these labels to be unbiased so that we may isolate the unintended biases entering our system to the word embeddings . Finally , we use a simple logistic regression algorithm to predict negative sentiment . Although the choice of ML model can have an impact on fairness for sentiment applications as shown in ( Kiritchenko and Mohammad , 2018 ) , we choose a simple ML model to limit the possible unintended biases introduced downstream from our word embeddings .", "entities": [[7, 8, "MethodName", "GloVe"], [29, 30, "DatasetName", "Google"], [35, 36, "DatasetName", "ConceptNet"], [37, 38, "MethodName", "GloVe"], [65, 66, "DatasetName", "ConceptNet"], [171, 173, "TaskName", "word embeddings"], [180, 182, "MethodName", "logistic regression"], [230, 232, "TaskName", "word embeddings"]]}
{"text": "We evaluate our framework and metric on two cases studies : National Origin Discrimination and Religious Discrimination . For each case study , we create a set of the most frequent identity terms from the protected groups in the Wikipedia word corpus and analyze bias with respect to these terms via our framework . First , we compare the RNSB metric for 3 pretrained word embeddings , showing that our metric is consistent with other word embedding analysis like WEAT ( Caliskan - Islam et al , 2016 ) . We then show that our framework enables an insightful view into word embedding bias .", "entities": [[64, 66, "TaskName", "word embeddings"]]}
{"text": "We vary the word embeddings used in our framework and calculate the RNSB metric for each embedding . The results are displayed in Table 1 . For both case studies , the bias is largest in GloVe , as shown by the largest RNSB metric . As mentioned earlier , ConceptNet is a state of the art model that mixes models like GloVe and Word2vec , creating fairer word embeddings . Through the RNSB metric , one can see that the unintended demographic bias of these word embeddings is an order of magnitude lower than GloVe or Word2vec . Although the RNSB metric is not directly comparable to WEAT scores , these results are still consistent with some of the bias predicted by ( Caliskan - Islam et al , 2016 ) . The WEAT score shows that word embeddings like Word2vec and GloVe are biased with respect to national origin because European - American names are more correlated with positive sentiment than African - American names . RNSB captures the same types of biases , but has a clear and larger scope , measuring discrimination with respect to more than two demographics within a protected group .", "entities": [[3, 5, "TaskName", "word embeddings"], [36, 37, "MethodName", "GloVe"], [50, 51, "DatasetName", "ConceptNet"], [62, 63, "MethodName", "GloVe"], [68, 70, "TaskName", "word embeddings"], [86, 88, "TaskName", "word embeddings"], [95, 96, "MethodName", "GloVe"], [138, 140, "TaskName", "word embeddings"], [143, 144, "MethodName", "GloVe"]]}
{"text": "Using the probability distribution of negative sentiment for the identity terms in a protected group , we can gain insights into the relative risks for discrimination between various demographics . Figure 3 shows three histograms . The bottom histogram is the uniform distribution . As described earlier , zero unintended demographic bias with respect to our definition is achieved when all the identity terms within a protected group have equal negative sentiment . The top two histograms show the negative sentiment probability for each identity normalized across all terms to be a probability distribution . The left histogram is computed using the GloVe word embeddings , and the right histogram is computed using the fairer Concept - Net embeddings . One can see that certain demographics have very high negative sentiment predictions , while others have very low predictions . The ConceptNet distribution seems to equalize much of this disparity . This type of analysis is very insightful as it enables one to see which identities are more at risk for discrimination . A more direct way to measure how certain groups receive similar unfair treatment is to compute a correlation matrix between the vectors containing negative sentiment predictions for each identity term . We compute this matrix for the same two cases : GloVe word embeddings ( top ) and ConceptNet word embeddings ( bottom ) shown in Figure 4 . The GloVe word embedding correlation matrix contains a lot of dark low correlations between identities , as a lot of identities contain small amounts of negative sentiment . But this visual brings out that certain groups like Indian , Mexican , and Russian have a high correlation , indicating that they could be treated similarly unfairly in a downstream ML algorithm . This is a useful insight that could allow a practitioner to change to embedding training corpora to create fairer models . For the ConceptNet word embeddings , we see a much more colorful heat map , indicating there are higher correlations between more identity terms . This hints that ConceptNet contains less targeted discrimination via negative sentiment . This visual also brings out slight differences in negative sentiment prediction . Identity terms like Scottish have lower correlations across the board , manifesting that this identity has slightly less negative sentiment than the rest of the identities . This is important to analyze to get a broader context for how various identities could receive different amounts of discrimination stemming from the word embedding bias . We can use these figures to analyze how certain groups could be similarly discriminated against via their negative sentiment correlation .", "entities": [[102, 103, "MethodName", "GloVe"], [103, 105, "TaskName", "word embeddings"], [141, 142, "DatasetName", "ConceptNet"], [214, 215, "MethodName", "GloVe"], [215, 217, "TaskName", "word embeddings"], [221, 222, "DatasetName", "ConceptNet"], [222, 224, "TaskName", "word embeddings"], [233, 234, "MethodName", "GloVe"], [317, 318, "DatasetName", "ConceptNet"], [318, 320, "TaskName", "word embeddings"], [343, 344, "DatasetName", "ConceptNet"]]}
{"text": "We showed how our framework can be used in the religious and national origin case studies . In practice , our framework should be used to measure bias among demographics of interest for the NLP application in question . Our RNSB metric is a useful signal a practitioner can use to choose the embedding model with the least amount of risk for discrimination in their application , or even to evaluate what types of unintended biases exists in their training corpora . We used our framework to evaluate unintended bias with respect to sentiment , but there exists many other types of unintended demographic bias to create clear signals for in word embeddings .", "entities": [[111, 113, "TaskName", "word embeddings"]]}
{"text": "We presented a transparent framework for evaluating unintended demographic bias in word embeddings . For this work our scope was limited to unfair biases with respect to negative sentiment . In our framework , we train a classifier on an unbiased positive / negative word sentiment dataset initialized with biased word embeddings . This way , we can observe the unfairness in the word embeddings at the ML prediction level . This allows us to observe clearer signals of bias in our metric , Relative Negative Sentiment Bias ( RNSB ) . Previous metrics and analysis into unintended bias in word embeddings rely on vector space arguments for only two demographics at a time , which does not lend itself well to evaluating real world discrimination . Our metric has a direct connection to discrimination and can evaluate any number of demographics in a protected group . Finally , our framework and metric reveal transparent analysis of the unintended bias hidden in word embeddings .", "entities": [[11, 13, "TaskName", "word embeddings"], [50, 52, "TaskName", "word embeddings"], [63, 65, "TaskName", "word embeddings"], [100, 102, "TaskName", "word embeddings"], [162, 164, "TaskName", "word embeddings"]]}
{"text": "We explore the task of improving persona consistency of dialogue agents . Recent models tackling consistency often train with additional Natural Language Inference ( NLI ) labels or attach trained extra modules to the generative agent for maintaining consistency . However , such additional labels and training can be demanding . Also , we find even the bestperforming persona - based agents are insensitive to contradictory words . Inspired by social cognition and pragmatics , we endow existing dialogue agents with public self - consciousness on the fly through an imaginary listener . Our approach , based on the Rational Speech Acts framework ( Frank and Goodman , 2012 ) , can enforce dialogue agents to refrain from uttering contradiction . We further extend the framework by learning the distractor selection , which has been usually done manually or randomly . Results on Dialogue NLI ( Welleck et al , 2019 ) and PersonaChat ( Zhang et al , 2018 ) dataset show that our approach reduces contradiction and improves consistency of existing dialogue models . Moreover , we show that it can be generalized to improve contextconsistency beyond persona in dialogues .", "entities": [[20, 23, "TaskName", "Natural Language Inference"], [35, 36, "DatasetName", "agent"], [68, 69, "DatasetName", "Inspired"]]}
{"text": "Literal Agent : !", "entities": [[1, 2, "DatasetName", "Agent"]]}
{"text": "I like going outside . Interlocutor Self - Conscious Agent : \"", "entities": [[9, 10, "DatasetName", "Agent"]]}
{"text": "We introduce how to endow dialogue agents with public self - consciousness , which helps them keep consistency in mind at each generation step by reflecting an imaginary listener 's distribution over personas . Since the imaginary listener arises from the plain dialogue - agent , separate training is not needed . Figure 3 illustrates its overall structure . We present how to model public selfconsciousness using the Rational Speech Acts ( RSA ) framework ( Frank and Goodman , 2012 ) in Section 4.1 . We then discuss learning of distractor selection as our major novelty for the RSA in Section 4.2 .", "entities": [[44, 45, "DatasetName", "agent"]]}
{"text": "We show that our self - conscious framework can significantly improve consistency and accuracy of state - of - the - art persona - based agents on two benchmark datasets . We prove its effectiveness using both automatic and human evaluations . We also show our framework can be generalized to improve consistency of dialogue context beyond persona .", "entities": [[13, 14, "MetricName", "accuracy"]]}
{"text": "Dialogue NLI Evaluation Set ( Welleck et al , 2019 ) . This dataset is based on PersonaChat with additional NLI annotations . Its main task is to rank next - utterance candidates given previous context . For each dialogue , they collect 31 next - utterance candidates in respect to the given persona : 10 entailing , 10 neutral and 10 contradicting candidates with 1 ground - truth utterance . In total , the evaluation set includes 542 instances . PersonaChat dialogue ( Zhang et al , 2018 ) . This dataset involves two interlocutors who are each given a persona and asked to get to know each other while playing their roles . This task was the subject of the ConvAI2 competition ( Dinan et al , 2019 ) at NeurIPS 2018 . The competition version contains 17 , 878 chitchat conversations conditioned on 1 , 155 personas for training and 1 , 000 conversations conditioned on 100 personas for validation .", "entities": [[122, 123, "DatasetName", "ConvAI2"]]}
{"text": "We perform human evaluation via Amazon Mechanical Turk . We random sample 250 test examples , each is rated by three unique human judges in terms of ( i ) Consistency and ( ii ) Engagingness . Turkers are shown a given persona , a dialogue context , and the model 's generated utterance . For consistency , we follow Madotto et al ( 2019 ) and ask judges to assign 1 , 0 , \u22121 to the utterance for consistency , neutrality , and contradiction , respectively . Following See et al ( 2019 ) , we evaluate the engagingness of the utterance in a 4 - point scale , where higher scores are better . To alleviate annotator bias and inter - annotator variability , we apply Bayesian calibration ( Kulikov et al , 2019 ) to the scores . Table 6 summarizes the human evaluation results . The agent with our self - consciousness method S 1 is rated as more consistent than the base agent S 0 while maintaining a similar level of engagingness . While it can be trivial to increase consistency at the cost of engagingness ( e.g. perfect consistency can by generating boring utterances with very little variance ) , it is not the case for our agent . Since our agent seeks to be heard as the given persona to the listener , self - distinctive words tend to meld into generated responses ( see Figure 6 ) . Thus , the responses from self - conscious agents have their own color , which can help improving engagingness . Figure 4 displays selected examples of utterance generation . Each example is comprised of dialogue history , human response , and utterances generated by our method and baselines .", "entities": [[73, 74, "DatasetName", "0"], [151, 152, "DatasetName", "agent"], [168, 169, "DatasetName", "agent"], [170, 171, "DatasetName", "0"], [214, 215, "DatasetName", "agent"], [218, 219, "DatasetName", "agent"]]}
{"text": "We demonstrate that our self - conscious agent can be generalized to generate context - consistent utterances beyond persona . We condition the agent with its previous responses in the dialogue history ; that is , i in Eq . ( 2 ) is the agent 's past responses instead of persona sentences . Hence , tokens that are inconsistent to the agent 's past response would be less favored by the model . Table 7 reports the results of context conditioned self - conscious agents . The EmpatheticDialogue ( Rashkin et al , 2019 ) is an open - domain dialogue dataset where a speaker describes a past emotional experience and the listener responds accordingly . Since the speaker 's descriptions should be consistent to the experience and previous utterances , it is a suitable benchmark for consistency . We model the speaker 's utterances and measure its consistency . Our S 1 agent outperforms other literal agents on all three datasets in terms of consistency . Thus , our approach can also be applied to help agents stay more consistent to its context . ( Zhang et al , 2018 ) . We compare it with the base speaker ( S 0 ) of TransferTransfo ( Wolf et al , 2019b ) and the human response ( Human ) .", "entities": [[7, 8, "DatasetName", "agent"], [23, 24, "DatasetName", "agent"], [45, 46, "DatasetName", "agent"], [62, 63, "DatasetName", "agent"], [154, 155, "DatasetName", "agent"], [203, 204, "DatasetName", "0"]]}
{"text": "Figure 7 shows selected examples of generated responses . In each set , we show given persona , dialogue context , human responses , and generated responses by our self - conscious agent and the base speaker . We use TransferTransfo ( Wolf et al , 2019b ) as a base speaker .", "entities": [[32, 33, "DatasetName", "agent"]]}
{"text": "[ P1 ] I really enjoy shopping and my dream is to one day own a Rolls Royce ghost . [ P2 ] Wow . I enjoy running over driving . [ P1 ] Running is also quite lovely . Breathing in the lovely outside air . [ P2 ] Yes it is . It clears my head when I need to as well . ( S 1 + DM ) shopping is a great way to clear my head . ( S 0 ) i love to shop and watch movies . ( Human ) yes , and it also helps with depression i have found . Figure 7 : Examples of generated responses by our self - conscious agent with Distractor Memory ( S 1 + DM ) on the PersonaChat dataset ( Zhang et al , 2018 ) . We compare it with the base speaker ( S 0 ) of TransferTransfo ( Wolf et al , 2019b ) and the human response ( Human ) .", "entities": [[83, 84, "DatasetName", "0"], [120, 121, "DatasetName", "agent"], [151, 152, "DatasetName", "0"]]}
{"text": "Analogical reasoning is effective in capturing linguistic regularities . This paper proposes an analogical reasoning task on Chinese . After delving into Chinese lexical knowledge , we sketch 68 implicit morphological relations and 28 explicit semantic relations . A big and balanced dataset CA8 is then built for this task , including 17813 questions . Furthermore , we systematically explore the influences of vector representations , context features , and corpora on analogical reasoning . With the experiments , CA8 is proved to be a reliable benchmark for evaluating Chinese word embeddings .", "entities": [[90, 92, "TaskName", "word embeddings"]]}
{"text": "Recently , the boom of word embedding draws our attention to analogical reasoning on linguistic regularities . Given the word representations , analogy questions can be automatically solved via vector computation , e.g. \" apples - apple + car \u2248 cars \" for morphological regularities and \" kingman + woman \u2248 queen \" for semantic regularities ( Mikolov et al , 2013 ) . Analogical reasoning has become a reliable evaluation method for word embeddings . In addition , It can be used in inducing morphological transformations ( Soricut and Och , 2015 ) , detecting semantic relations ( Herdagdelen and Baroni , 2009 ) , and translating unknown words ( Langlais and Patry , 2007 ) . It is well known that linguistic regularities vary a lot among different languages . For example , Chinese is a typical analytic language which lacks inflection . Figure 1 shows that function words and reduplication are used to denote grammatical and semantic information . In addition , many semantic \u2020 Corresponding author . relations are closely related with social and cultural factors , e.g. in Chinese \" sh\u012b - xi\u0101n \" ( god of poetry ) refers to the poet Li - bai and \" sh\u012b - sh\u00e8ng \" ( saint of poetry ) refers to the poet Du - fu . However , few attempts have been made in Chinese analogical reasoning . The only Chinese analogy dataset is translated from part of an English dataset ( Chen et al , 2015 ) ( denote as CA_translated ) . Although it has been widely used in evaluation of word embeddings ( Yang and Sun , 2015 ; Yin et al , 2016 ; Su and Lee , 2017 ) , it could not serve as a reliable benchmark since it includes only 134 unique Chinese words in three semantic relations ( capital , state , and family ) , and morphological knowledge is not even considered . Therefore , we would like to investigate linguistic regularities beneath Chinese . By modeling them as an analogical reasoning task , we could further examine the effects of vector offset methods in detecting Chinese morphological and semantic relations . As far as we know , this is the first study focusing on Chinese analogical reasoning . Moreover , we release a standard benchmark for evaluation of Chinese word embedding , together with 36 open - source pre - trained embeddings at GitHub 1 , which could serve as a solid basis for Chinese NLP tasks .", "entities": [[73, 75, "TaskName", "word embeddings"], [267, 269, "TaskName", "word embeddings"]]}
{"text": "Analogical reasoning task is to retrieve the answer of the question \" a is to b as c is to ? \" . Based on the relations discussed above , we firstly collect word pairs for each relation . Since there are no explicit word boundaries in Chinese , we take dictionaries and word segmentation specifications as references to confirm the inclusion of each word Levy and Goldberg ( 2014b ) unifies SGNS and PPMI in a framework , which share the same hyper - parameter settings . We exploit 3COSMUL to solve the analogical questions suggested by Levy and Goldberg ( 2014a ) . pair . To avoid the imbalance problem addressed in English benchmarks ( Gladkova et al , 2016 ) , we set a limit of 50 word pairs at most for each relation . In this step , 1852 unique Chinese word pairs are retrieved . We then build CA8 , a big , balanced dataset for Chinese analogical reasoning including 17813 questions . Compared with CA_translated ( Chen et al , 2015 ) , CA8 incorporates both morphological and semantic questions , and it brings in much more words , relation types and questions . Table 1 shows details of the two datasets . They are both used for evaluation in Experiments section .", "entities": [[74, 75, "DatasetName", "PPMI"]]}
{"text": "In Chinese analogical reasoning task , we aim at investigating to what extent word vectors capture the linguistic relations , and how it is affected by three important factors : vector representations ( sparse and dense ) , context features ( character , word , and ngram ) , and training corpora ( size and domain ) . Table 2 shows the hyper - parameters used in this work . All the text data used in our experiments ( as shown in Table 3 ) are preprocessed via the following steps : Remove the html and xml tags from the texts and set the encoding as utf - 8 . Digits and punctuations are remained . Convert traditional Chinese characters into simplified characters with Open Chinese Convert ( OpenCC ) 2 . Conduct Chinese word segmentation with HanLP ( v_1.5.3 ) 3 .", "entities": [[110, 111, "DatasetName", "Digits"], [133, 136, "TaskName", "Chinese word segmentation"]]}
{"text": "Existing vector representations fall into two types , dense vectors and sparse vectors . SGNS ( skipgram model with negative sampling ) ( Mikolov et al , 2013 ) and PPMI ( Positive Pointwise Mutual Information ) ( Levy and Goldberg , 2014a ) are respectively typical methods for learning dense and sparse word vectors . Table 4 lists the performance of them on CA_translated and CA8 datasets under different configurations . We can observe that on CA8 dataset , SGNS representations perform better in analogical reasoning of morphological relations and PPMI representations show great advantages in semantic relations . This result is consistent with performance of English dense and sparse vectors on MSR ( morphology - only ) , SemEval ( semanticonly ) , and Google ( mixed ) analogy datasets ( Levy and Goldberg , 2014b ; Levy et al , 2015 966 .603 .117 .162 .181 .389 .222 .414 .345 .236 .223 .327 word+ngram .715 .977 .640 .143 .184 .197 .429 .250 .449 .308 .276 .310 .368 word+char .676 .966 .548 .358 .540 .326 .612 .455 .468 .226 .296 .305 .368 PPMI word .925 .920 .548 .103 .139 .138 .464 .226 .627 .501 .300 .515 .522 word+ngram .943 .960 .658 .102 .129 .168 .456 .230 .680 .535 .371 .626 .586 word+char .913 .886 .614 .106 .190 .173 .505 .260 .638 .502 .288 .515 .524 probably because the reasoning on morphological relations relies more on common words in context , and the training procedure of SGNS favors frequent word pairs . Meanwhile , PPMI model is more sensitive to infrequent and specific word pairs , which are beneficial to semantic relations . The above observation shows that CA8 is a reliable benchmark for studying the effects of dense and sparse vectors . Compared with CA_translated and existing English analogy datasets , it offers both morphological and semantic questions which are also balanced across different types 4 .", "entities": [[30, 31, "DatasetName", "PPMI"], [91, 92, "DatasetName", "PPMI"], [126, 127, "DatasetName", "Google"], [184, 185, "DatasetName", "PPMI"], [238, 241, "DatasetName", "words in context"], [255, 256, "DatasetName", "PPMI"]]}
{"text": "We compare word representations learned upon corpora of different sizes and domains . As shown in Table 3 , six corpora are used in the experiments : Chinese Wikipedia , Baidubaike , People 's Daily News , Sogou News , Zhihu QA , and \" Com - bination \" which is built by combining the first five corpora together . Table 5 shows that accuracies increase with the growth in corpus size , e.g. Baidubaike ( an online Chinese encyclopedia ) has a clear advantage over Wikipedia . Also , the domain of a corpus plays an important role in the experiments . We can observe that vectors trained on news data are beneficial to geography relations , especially on People 's Daily which has a focus on political news . Another example is Zhihu QA , an online questionanswering corpus which contains more informal data than others . It is helpful to reduplication relations since many reduplication words appear frequently in spoken language . With the largest size and varied domains , \" Combination \" corpus performs much better than others in both morphological and semantic relations . Based on the above experiments , we find that vector representations , context features , and corpora all have important influences on Chinese analogical reasoning . Also , CA8 is proved to be a reliable benchmark for evaluation of Chinese word embeddings .", "entities": [[229, 231, "TaskName", "word embeddings"]]}
{"text": "In this paper , we investigate the linguistic regularities beneath Chinese , and propose a Chinese analogical reasoning task based on 68 morphological relations and 28 semantic relations . In the experiments , we apply vector offset method to this task , and examine the effects of vector representations , context features , and corpora . This study offers an interesting perspective combining linguistic analysis and representation models . The benchmark and embedding sets we release could also serve as a solid basis for Chinese NLP tasks .", "entities": [[69, 71, "DatasetName", "The benchmark"]]}
{"text": "The Fact Extraction and VERification ( FEVER ) Shared Task", "entities": [[6, 7, "DatasetName", "FEVER"]]}
{"text": "We present the results of the first Fact Extraction and VERification ( FEVER ) Shared Task . The task challenged participants to classify whether human - written factoid claims could be SUPPORTED or REFUTED using evidence retrieved from Wikipedia . We received entries from 23 competing teams , 19 of which scored higher than the previously published baseline . The best performing system achieved a FEVER score of 64.21 % . In this paper , we present the results of the shared task and a summary of the systems , highlighting commonalities and innovations among participating systems .", "entities": [[12, 13, "DatasetName", "FEVER"], [65, 66, "DatasetName", "FEVER"]]}
{"text": "Information extraction is a well studied domain and the outputs of such systems enable many natural language technologies such as question answering and text summarization . However , since information sources can contain errors , there exists an additional need to verify whether the information is correct . For this purpose , we hosted the first Fact Extraction and VERification ( FEVER ) shared task to raise interest in and awareness of the task of automatic information verificationa research domain that is orthogonal to information extraction . This shared task required participants to develop systems to predict the veracity of human - generated textual claims against textual evidence to be retrieved from Wikipedia . We constructed a purpose - built dataset for this task ( Thorne et al , 2018 ) that contains 185 , 445 human - generated claims , manually verified against the introductory sections of Wikipedia pages and labeled as SUPPORTED , REFUTED or NOTENOUGHINFO . The claims were generated by paraphrasing facts from Wikipedia and mutating them in a variety of ways , some of which were meaning - altering . For each claim , and without the knowledge of where the claim was generated from , annotators selected evidence in the form of sentences from Wikipedia to justify the labeling of the claim . The systems participating in the FEVER shared task were required to label claims with the correct class and also return the sentence ( s ) forming the necessary evidence for the assigned label . Performing well at this task requires both identifying relevant evidence and reasoning correctly with respect to the claim . A key difference between this task and other textual entailment and natural language inference tasks ( Dagan et al , 2009 ; Bowman et al , 2015 ) is the need to identify the evidence from a large textual corpus . Furthermore , in comparison to large - scale question answering tasks ( Chen et al , 2017 ) , systems must reason about information that is not present in the claim . We hope that research in these fields will be stimulated by the challenges present in FEVER . One of the limitations of using human annotators to identify correct evidence when constructing the dataset was the trade - off between annotation velocity and evidence recall ( Thorne et al , 2018 ) . Evidence selected by annotators was often incomplete . As part of the FEVER shared task , any evidence retrieved by participating systems that was not contained in the original dataset was annotated and used to augment the evidence in the test set . In this paper , we present a short description of the task and dataset , present a summary of the submissions and the leader board , and highlight future research directions .", "entities": [[20, 22, "TaskName", "question answering"], [23, 25, "TaskName", "text summarization"], [61, 62, "DatasetName", "FEVER"], [224, 225, "DatasetName", "FEVER"], [283, 286, "TaskName", "natural language inference"], [321, 323, "TaskName", "question answering"], [360, 361, "DatasetName", "FEVER"], [409, 410, "DatasetName", "FEVER"], [452, 454, "DatasetName", "and dataset"]]}
{"text": "Candidate systems for the FEVER shared task were given a sentence of unknown veracity called a claim . The systems must identify suitable evidence from Wikipedia at the sentence level and Claim : The Rodney King riots took place in the most populous county in the USA .", "entities": [[4, 5, "DatasetName", "FEVER"]]}
{"text": "Training and development data was released through the FEVER website . 1 We used the reserved portion of the data presented in Thorne et al ( 2018 ) as a blind test set . Disjoint training , development and test splits of the dataset were generated by splitting the dataset by the page used to generate the claim . The development and test datasets were balanced by randomly discarding claims from the more populous classes .", "entities": [[8, 9, "DatasetName", "FEVER"]]}
{"text": "The FEVER shared task was hosted as a competition on Codalab 3 which allowed submissions to be scored against the blind test set without the need to publish the correct labels .", "entities": [[1, 2, "DatasetName", "FEVER"]]}
{"text": "86 submissions ( excluding the baseline ) were made to Codalab for scoring on the blind test set . There were 23 different teams which participated in the task ( presented in Table 2 ) . 19 of these teams scored higher than the baseline presented in Thorne et al ( 2018 ) . All participating teams were invited to submit a description of their systems . We received 15 descriptions at the time of writing and the remaining are considered as withdrawn . The system with the highest score was submitted by UNC - NLP ( FEVER score : 64.21 % ) . Most participants followed a similar pipeline structure to the baseline model . This consisted of three stages : document selection , sentence selection and natural language inference . However , some teams constructed models to jointly select sentences and perform inference in a single pipeline step , while others added an additional step , discarding inconsistent evidence after performing inference . Based on the team - submitted system description summaries ( Appendix A ) , in the following section we present an overview of which models and techniques were applied to the task and their relative performance .", "entities": [[97, 98, "DatasetName", "FEVER"], [128, 131, "TaskName", "natural language inference"]]}
{"text": "There were three common approaches to sentence selection : keyword matching , supervised classification and sentence similarity scoring . Ohio State and UCL Machine Reading Group report using keyword matching techniques : matching either named entities or tokens appearing in both the claim and article body . UNC - NLP , Athene UKP TU Darmstadt and Columbia NLP modeled the task as supervised binary classification , using architectures such as Enhanced LSTM ( Chen et al , 2016 ) , Decomposable Attention ( Parikh et al , 2016 ) or similar to them . SWEEPer and BUPT - NLPer present jointly learned models for sentence selection and natural language inference . Other teams report scoring based on sentence similarity using Word Mover 's Distance ( Kusner et al , 2015 ) or cosine similarity over smooth inverse frequency weightings ( Arora et al , 2017 ) , ELMo embeddings ( Peters et al , 2018 ) and TF - IDF ( Salton et al , 1983 ) . UCL Machine Reading Group and Directed Acyclic Graph report an additional aggregation stage after the classification stage in the pipeline where evidence that is inconsistent is discarded .", "entities": [[52, 53, "DatasetName", "UKP"], [71, 72, "MethodName", "LSTM"], [107, 110, "TaskName", "natural language inference"], [147, 148, "MethodName", "ELMo"]]}
{"text": "NLI was modeled as supervised classification in all reported submissions . We compare and discuss the approaches for combining the evidence sentences together with the claim , sentence representations and training schemes . While many different approaches were used for sentence pair classification , e.g. Enhanced LSTM ( Chen et al , 2016 ) , Decomposable Attention ( Parikh et al , 2016 ) , Transformer Model ( Radford and Salimans , 2018 ) , Random Forests ( Svetnik et al , 2003 ) and ensembles thereof , these are not specific to the task and it is difficult to assess their impact due to the differences in the processing preceding this stage . Evidence Combination : UNC - NLP ( the highest scoring team ) concatenate the evidence sentences into a single string for classification ; UCL Machine Reading Group classify each evidenceclaim pair individually and aggregate the results using a simple multilayer perceptron ( MLP ) ; Columbia NLP perform majority voting ; and finally , Athene - UKP TU Darmstadt encode each evidence - claim pair individually using an Enhanced LSTM , pool the resulting vectors and use an MLP for classification . Sentence Representation : University of Arizona explore using non - lexical features for predicting entailment , considering the proportion of negated verbs , presence of antonyms and noun overlap . Columbia NLP learn universal sentence representations ( Conneau et al , 2017 ) . UNC - NLP include an additional token - level feature the sentence similarity score from the sentence selection module . Both Ohio State and UNC - NLP report alternative token encodings : UNC - NLP report using ELMo ( Peters et al , 2018 ) and WordNet ( Miller , 1995 ) and Ohio State report using vector representations of named entities . FujiXerox report representing sentences using DEISTE ( Yin et al , 2018 ) . Training : BUPT - NLPer and SWEEPer model the evidence selection and claim verification using a multi - task learning model under the hypothesis that information from each task supplements the other . SWEEPer also report parameter tuning using reinforcement learning .", "entities": [[46, 47, "MethodName", "LSTM"], [65, 66, "MethodName", "Transformer"], [156, 157, "DatasetName", "MLP"], [170, 171, "DatasetName", "UKP"], [183, 184, "MethodName", "LSTM"], [192, 193, "DatasetName", "MLP"], [277, 278, "MethodName", "ELMo"], [333, 337, "TaskName", "multi - task learning"]]}
{"text": "As mentioned in the introduction , to increase the evidence coverage in the test set , the evidence submitted by participating systems was annotated by shared task volunteers after the competition ended . There were 18 , 846 claims where at least one system returned an incorrect label , according to the FEVER score , i.e. taking evidence into account . These claims were sampled for annotation with a probability proportional to the number of systems which labeled each of them incorrectly . The evidence sentences returned by each system for each claim was sampled further with a probability proportional to the system 's FEVER score in an attempt to focus annotation efforts towards higher quality candidate evidence . These extra annotations were performed by volunteers from the teams participating in the shared task and three of the organizers . Annotators were asked to label whether the retrieved evidence sentences supported or refuted the claim at question , and to highlight which sentences ( if any ) , either individually or as a group , can be used as evidence . We retained the annotation guidelines from Thorne et al ( 2018 ) ( see Sections A.7.1 , A.7.3 and A.8 from that paper for more details ) . At the time of writing , 1 , 003 annotations were collected for 618 claims . This identified 3 claims that were incorrectly labeled as SUPPORTED or REFUTED and 87 claims that were originally labeled as NOTENOUGHINFO that should be relabeled as SUPPORTED or REFUTED through the introduction of new evidence ( 44 and 43 claims respectively ) . 308 new evidence sets were identified for claims originally labeled as SUPPORTED or REFUTED , consisting of 280 single sentences and 28 sets of 2 or more sentences . Further annotation is in progress and the data collected as well as the final results will be made public at the workshop .", "entities": [[52, 53, "DatasetName", "FEVER"], [104, 105, "DatasetName", "FEVER"]]}
{"text": "A.1 UNC - NLP Our system is composed of three connected components namely , a document retriever , a sentence selector , and a claim verifier . The document retriever chooses candidate wiki - documents via matching of keywords between the claims and the wiki - document titles , also using external pageview frequency statistics for wiki - page ranking . The sentence selector is a sequence - matching neural network that conducts further fine - grained selection of evidential sentences by comparing the given claim with all the sentences in the candidate documents . This module is trained as a binary classifier given the ground truth evidence as positive examples and all the other sentences as negative examples with an annealing sampling strategy . Finally , the claim verifier is a state - of - theart 3 - way neural natural language inference ( NLI ) classifier ( with WordNet and ELMo features ) that takes the concatenation of all selected evidence as the premise and the claim as the hypothesis , and labels each such evidences - claim pair as one of ' support ' , ' refute ' , or ' not enough info ' . To improve the claim verifier via better awareness of the selected evidence , we further combine the last two modules by feeding the sentence similarity score ( produced by the sentence selector ) as an additional token - level feature to the claim verifier .", "entities": [[141, 144, "TaskName", "natural language inference"], [152, 153, "MethodName", "ELMo"]]}
{"text": "The UCLMR system is a four stage model consisting of document retrieval , sentence retrieval , natural language inference and aggregation . Document retrieval attempts to find the name of a Wikipedia article in the claim , and then ranks each article based on capitalization , sentence position and token match features . A set of sentences are then retrieved from the top ranked articles , based on token matches with the claim and position in the article . A natural language inference model is then applied to each of these sentences paired with the claim , giving a prediction for each potential evidence . These predictions are then aggregated using a simple MLP , and the sentences are reranked to keep only the evidence consistent with the final prediction .", "entities": [[16, 19, "TaskName", "natural language inference"], [80, 83, "TaskName", "natural language inference"], [113, 114, "DatasetName", "MLP"]]}
{"text": "Our model for fact checking and verification consists of two stages : 1 ) identifying relevant documents using lexical and syntactic features from the claim and first two sentences in the Wikipedia article and 2 ) jointly modeling sentence extraction and verification . As the tasks of fact checking and finding evidence are dependent on each other , an ideal model would consider the veracity of the claim when finding evidence and also find only the evidence that supports / refutes the position of the claim . We thus jointly model the second stage by using a pointer network with the claim and evidence sentence represented using the ESIM module . For stage 2 , we first train both components using multi - task learning over a larger memory of extracted sentences , then tune parameters using re - inforcement learning to first extract sentences and predict the relation over only the extracted sentences .", "entities": [[3, 5, "TaskName", "fact checking"], [47, 49, "TaskName", "fact checking"], [97, 99, "MethodName", "pointer network"], [108, 109, "MethodName", "ESIM"], [121, 125, "TaskName", "multi - task learning"]]}
{"text": "For document retrieval we use three components : 1 ) use google custom search API with the claim as a query and return the top 2 Wikipedia pages ; 2 ) extract all name entities from the claims and use Wikipedia python API to return a page for each name entity and 3 ) ; use the prefix of the claim until the first lowercase verb phrase , and use Wikipedia API to return the top page . For Sentence Selection we used the modified document retrieval component of DrQA to get the top 5 sentences and then further extracted the top 3 sentences using cosine similarity between vectors obtained from Elmo ( Peters et al , 2018 ) sentence embeddings of the claim and the evidence . For RTE we used the same model as outlined by ( Conneau et al , 2017 ) in their work for recognizing textual entailment and learning universal sentence representations . If at least one out of the three evidences SUPPORTS / REFUTES the claim and the rest are NOT ENOUGH INFO , then we treat the label as SUPPORTS / REFUTES , else we return the majority among three classes as the predicted label .", "entities": [[111, 112, "MethodName", "Elmo"], [119, 121, "TaskName", "sentence embeddings"], [129, 130, "DatasetName", "RTE"]]}
{"text": "We prepared a pipeline system which composes of document selection , a sentence retrieval , and a recognizing textual entailment ( RTE ) components . A simple entity linking approach with text match is used as the document selection component , this component identifies relevant documents for a given claim by using mentioned entities as clues . The sentence retrieval component selects relevant sentences as candidate evidence from the documents based on TF - IDF . Finally , the RTE component selects evidence sentences by ranking the sentences and classifies the claim as SUPPORTED , REFUTED , or NOTENOUGH - INFO simultaneously . As the RTE component , we adopted DEISTE ( Deep Explorations of Inter - Sentence interactions for Textual Entailment ) ( Yin et al , 2018 ) model that is the state - of - the - art in RTE task .", "entities": [[21, 22, "DatasetName", "RTE"], [27, 29, "TaskName", "entity linking"], [79, 80, "DatasetName", "RTE"], [105, 106, "DatasetName", "RTE"], [142, 143, "DatasetName", "RTE"]]}
{"text": "We generate a Lucene index from the provided Wikipedia dump . Then we use two neural networks , one for named entity recognition and the other for constituency parsing , and also the Stanford dependency parser to create the keywords used inside the Lucene queries . Depending on the amount of keywords found for each claim , we run multiple Lucene searches on the generated index to create a list of candidate sentences for each claim . The resulting list of claim - candidate pairs is processed in three ways : 1 . We use the Standford POS - Tagger to generate POS - Tags for the claim and candidate sentences which are then used in a handcrafted scoring script to assign a score on a 0 to 15 scale . 2 . We run each pair through a modified version of the Decomposable Attention network . 3 . We merge all candidate sentences per claim into one long piece of text and run the result paired with the claim through the same modified Decomposable Attention network as in ( 2 . ) . We then make the final prediction in a handcrafted script combining the results of the three previous steps .", "entities": [[20, 23, "TaskName", "named entity recognition"], [27, 29, "TaskName", "constituency parsing"], [126, 127, "DatasetName", "0"]]}
{"text": "We NER tagged the claim using SpaCy and used the Named Entities as candidate page IDs . We resolved redirects by following the Wikipedia URL if an item was not in the preprocessed dump . If a page could not be found , we fell back to the baseline document selection method . The rest of the system was identical to the baseline system , al - though we used our document retrieval system to generate alternative training data .", "entities": [[1, 2, "TaskName", "NER"]]}
{"text": "This article presents the SIRIUS - LTG system for the Fact Extraction and VERification ( FEVER ) Shared Task . Our system consists of three components : 1 . Wikipedia Page Retrieval : First we extract the entities in the claim , then we find potential Wikipedia URI candidates for each of the entities using the SPARQL query over DBpedia ( Khot et al , 2018 ) and a Gradient - Boosted Decision Trees ( TalosTree ) model ( Baird et al , 2017 ) for this task . The experiments show that the pipeline with simple Cosine Similarity using TFIDF in sentence selection along with DA as labeling model achieves better results in development and test dataset .", "entities": [[15, 16, "DatasetName", "FEVER"], [59, 60, "DatasetName", "DBpedia"]]}
{"text": "We introduce an end - to - end multi - task learning model for fact extraction and verification with bidirection attention . We propose a multi - task learning framework for the evidence extraction and claim verification because these two tasks can be accomplished at the same time . Each task provides supplementary information for the other and improves the results of another task . For each claim , our system firstly uses the entity linking tool S - MART to retrieve relative pages from Wikipedia . Then , we use attention mechanisms in both directions , claim - to - page and pageto - claim , which provide complementary information to each other . Aimed at the different task , our system obtains claim - aware sentence representation for evidence extraction and page - aware claim representation for claim verification .", "entities": [[8, 12, "TaskName", "multi - task learning"], [25, 29, "TaskName", "multi - task learning"], [74, 76, "TaskName", "entity linking"]]}
{"text": "Many approaches to automatically recognizing entailment relations have employed classifiers over hand engineered lexicalized features , or deep learning models that implicitly capture lexicalization through word embeddings . This reliance on lexicalization may complicate the adaptation of these tools between domains . For example , such a system trained in the news domain may learn that a sentence like \" Palestinians recognize Texas as part of Mexico \" tends to be unsupported , a fact which has no value in say a scientific domain . To mitigate this dependence on lexicalized information , in this paper we propose a model that reads two sentences , from any given domain , to determine entailment without using any lexicalized features . Instead our model relies on features like proportion of negated verbs , antonyms , noun overlap etc . In its current implementation , this model does not perform well on the FEVER dataset , due to two reasons . First , for the information retrieval part of the task we used the baseline system provided , since this was not the aim of our project . Second , this is work in progress and we still are in the process of identifying more features and gradually increasing the accuracy of our model . In the end , we hope to build a generic end - to - end classifier , which can be used in a domain outside the one in which it was trained , with no or minimal re - training .", "entities": [[25, 27, "TaskName", "word embeddings"], [62, 63, "DatasetName", "Texas"], [150, 151, "DatasetName", "FEVER"], [162, 164, "TaskName", "information retrieval"], [207, 208, "MetricName", "accuracy"]]}
{"text": "The work reported was partly conducted while James Thorne was at Amazon Research Cambridge . Andreas Vlachos is supported by the EU H2020 SUMMA project ( grant agreement number 688139 ) .", "entities": [[13, 14, "DatasetName", "Cambridge"]]}
{"text": "Language models trained on private data suffer privacy risks of revealing sensitive information . Previous researches mainly considered black - box attacks that assumed attackers only had access to 1 Code is publicly available at https://github . com / HKUST - KnowComp / Persona_leakage_and _ defense_in_GPT - 2 . inputs and outputs of language models . Carlini et al ( 2021 ) performed black - box model inversion attack on GPT - 2 through descriptive prompts with beam search . Lehman et al ( 2021 ) examined BERT pretrained on Electronic Health Records via blank filling and model probing to recover Personal Health Information . Furthermore , given black - box access to a language model 's pre - train and finetune stages , Zanella - B\u00e9guelin et al ( 2020 ) showed that sensitive sequences of the fine - tuning dataset can be extracted . For the distributed client - server setup , Malekzadeh et al ( 2021 ) considered the sensitive attribute leakage from the server side with honest - but - curious ( HBC ) classifiers . What is worse , for an LM - based chatbot , its training conversations are prone to include more private attributes than other commonly - used corpora for language modeling like BooksCorpus ( Zhu et al , 2015 ) and Wikipedia . Tigunova et al ( 2019 ) proposed Hidden Attribute Model ( HAM ) to extract professions and genders of speakers from various dialog datasets . further applied Attribute Extractor to generate speakers ' attribute triplets flexibly and suggested downstream tasks based on the triplets . Pan et al ( 2020 ) exploited embeddings of language models to recover inputs ' digits and keywords . Though the setup of this work is similar to ours , they merely consider simple cases of data recovery with given rules and suffer great utility degradation to obtain optimal defense performance . For our work , there is no fixed pattern or rule for the model input . Instead of finding key - words or recovering digits , we aim to infer more complicated private attributes from such embeddings . Moreover , our proposed defenses have almost no influence on the utility .", "entities": [[70, 71, "MethodName", "GPT"], [87, 88, "MethodName", "BERT"], [189, 190, "TaskName", "chatbot"], [233, 234, "DatasetName", "HAM"]]}
{"text": "In this section , we illustrate black - box persona inference attacks on GPT - 2 and our defense strategies . In Section 3.1 , we first give the problem formulation . Then we describe the attack in Section 3.2 .", "entities": [[13, 14, "MethodName", "GPT"]]}
{"text": "In Figure 3 , we give an example of the persona inference attack , where conversations are generated between the chatbot and the user with the given context . We manually mark True / False on the predicted results . As shown in the figure , there are several successful attacks on LM and no correct prediction on the defensed LM . For attacks on LM , speakers ' hobbies and jobs can be inferred . For incorrect predictions , the attacker model can still predict context - aware personas . After applying proposed defense learning strategies , the predicted personas become irrelevant with context and mostly predict \" My favorite color is blue . \" In fact , it is the most frequent prediction for LM+KL+MI over 4 , 332 persona labels . This attack example illustrates that our defense objectives can prevent the black - box persona inference attack from inferring relevant personas .", "entities": [[11, 13, "TaskName", "inference attack"], [20, 21, "TaskName", "chatbot"], [149, 151, "TaskName", "inference attack"]]}
{"text": "In this paper , we show that LM - based chatbots tend to reveal personas of speakers and propose effective defense objectives to prevent GPT - 2 from overlearning . Unlike other works that suffer from utility degradation , our defense learning strategies do no harm to the powerful generation ability of LM - based chatbots . We conduct extensive experiments to evaluate both privacy and utility . We perform black - box persona inference attacks under various setups to demonstrate the robustness of proposed defense learning strategies . In addition , we use automatic metrics to show that proposed defense learning strategies maintain the utility . For future work , we suggest working on flattening the distributions of attacker models .", "entities": [[24, 25, "MethodName", "GPT"]]}
{"text": "We declare that all authors of this paper acknowledge the ACM Code of Ethics and honor the code of conduct . This work essentially considers blackbox attacks on the private persona attributes and proposes effective learning strategies to prevent chatbots from overlearning private personas . Dataset . During our dataset collection , all the conversations and personas are collected from publicly available datasets including PersonaChat and DNLI . All the speakers are anonymized and no identifiable personal information is included . Model . For training our LM - based chatbots , we follow standard methods . We are well aware of the bias issue inside current language models . In the future , if there are other fairer language models , we will extend our defenses on them . formation about personas . However , its attacking performance is poor . The poor performance implies our proposed defense learning strategies may obfuscate Attacker for estimating single sample f ( u ) and finally make the wrong prediction .", "entities": [[10, 11, "DatasetName", "ACM"], [13, 14, "DatasetName", "Ethics"]]}
{"text": "The state of the art in MT involves corpus - based systems developed with machine - learning methods . These methods learn from corpora the models needed for translation . A key strength of this approach is that the system is adapted specifically towards the data it is trained with . For many years , the most successful data - driven approaches were phrase - based and syntax - based Statistical MT ( SMT ; Koehn , 2009 ) . However , lately Neural MT ( NMT ) based on the encoderdecoder architecture and the concept of attention ( Sutskever et al , 2014 ; Bahdanau et al , 2016 ) has become very popular . Indeed , since 2015 , in MT shared tasks ( Cettolo et al , 2015 ; Bojar et al , 2015 ; Bojar et al , 2016 ) most top - performing systems have been NMT systems . This trend is confirmed in the most recent MT shared task ( Barrault et al , 2019 ) , where 80 % of participating systems are of NMT type . Though NMT represents the state of the art for MT , specific weaknesses have been reported : NMT performance suffers from the lack of data resources ( Koehn and Knowles , 2017 ) , giving lower translation performance , especially when training with out - of - domain rather than in - domain data . Recent advances in NMT models have been shown ( Sennrich and Zhang , 2019 ) to allow good translations to be achieved with smaller parallel corpora of typically 10 5 sentences , though substantial improvements are achieved when the corpus size reaches 10 6 sentences . However , training sets of such sizes are not available for all languages . Translation performance is affected by nonparallel texts and non - literal translations ( Carpuat et al , 2017 ) . The integration of multiple algorithms into an NMT system does not necessarily improve translation ( Denkowski and Neubig , 2017 ) . The time complexity of training a new NMT system can be very high , with training sessions of the order of weeks . NMT requires very large amounts of parallel data , measured in millions of parallel sentences . This is reflected by the separate studies carried out for MT with limited resources , which includes initiatives such as Lorelei 1 . In the case of morphologically - rich languages , the requirements for parallel corpora are further exacerbated . Proposed approaches for translating towards lowresource and morphologically - rich languages have included transfer learning ( Zoph et al , 2016 ) as well as multilingual and multi - way NMT ( Rikters et al , 2018 ) . In this paper , an effort to improve the translation quality is presented , when translating towards a morphologically - rich language , while reducing the training time . This approach combines the output of multiple NMT systems with an NLP module developed for an example - based MT paradigm , resulting in a hybridized solution . The latter module is fast and runs independently of its original MT system and thus the computational complexity of the proposed hybrid solution is not substantially increased over the base NMT system . The idea of combining multiple MT models to produce a higher performing MT system has been studied extensively in the area of MT . For instance in the recent shared task ( Barrault et al , 2019 ) more than 20 entries consist of ensembles of multiple NMT systems . Ensembles of weaker NMT systems of the same general architecture have been proposed by Freitag et al ( 2017 ) to train a higher performing NMT system . In addition ensembles of factored NMT models have been proposed for automatic post - editing and quality estimation ( for example Hokamp , 2017 ) . This base NMT system is described in section 2 . The training data used is reported in section 3 . The proposed hybridization is presented in section 4 , whilst the improvements attained are presented in section 5 . Future developments are discussed in section 6 .", "entities": [[299, 300, "TaskName", "Translation"], [434, 436, "TaskName", "transfer learning"], [639, 643, "TaskName", "automatic post - editing"]]}
{"text": "The authors acknowledge support of this work by the project \" DRASSI \" ( MIS5002437 ) which is implemented under the Action \" Reinforcement of the Research and Innovation Infrastructure \" , funded by the Operational Programme \" Competitiveness , Entrepreneurship and Innovation \" ( NSRF2014 - 2020 ) and co - financed by Greece and the European Commission ( European Regional Development Fund ) . The authors wish to acknowledge the contribution of NVIDIA who donated for research purposes in the area of Machine Translation a Titan XP GPU card under the NVIDIA Academic Support Programme to the MT group of ILSP / Athena R.C.", "entities": [[84, 86, "TaskName", "Machine Translation"], [87, 88, "DatasetName", "Titan"]]}
{"text": "KorNLI and KorSTS : New Benchmark Datasets for Korean Natural Language Understanding", "entities": [[0, 1, "DatasetName", "KorNLI"], [2, 3, "DatasetName", "KorSTS"], [9, 12, "TaskName", "Natural Language Understanding"]]}
{"text": "Natural language inference ( NLI ) and semantic textual similarity ( STS ) are key tasks in natural language understanding ( NLU ) . Although several benchmark datasets for those tasks have been released in English and a few other languages , there are no publicly available NLI or STS datasets in the Korean language . Motivated by this , we construct and release new datasets for Korean NLI and STS , dubbed KorNLI and KorSTS , respectively . Following previous approaches , we machine - translate existing English training sets and manually translate development and test sets into Korean . To accelerate research on Korean NLU , we also establish baselines on KorNLI and KorSTS . Our datasets are publicly available at https://github.com/ kakaobrain / KorNLUDatasets .", "entities": [[0, 3, "TaskName", "Natural language inference"], [7, 10, "TaskName", "semantic textual similarity"], [11, 12, "TaskName", "STS"], [17, 20, "TaskName", "natural language understanding"], [49, 50, "TaskName", "STS"], [70, 71, "TaskName", "STS"], [73, 74, "DatasetName", "KorNLI"], [75, 76, "DatasetName", "KorSTS"], [113, 114, "DatasetName", "KorNLI"], [115, 116, "DatasetName", "KorSTS"]]}
{"text": "Natural language inference ( NLI ) and semantic textual similarity ( STS ) are considered as two of the central tasks in natural language understanding ( NLU ) . They are not only featured in GLUE ( Wang et al , 2018 ) and SuperGLUE ( Wang et al , 2019 ) , which are two popular benchmarks for NLU , but also known to be useful for supplementary training of pre - trained language models ( Phang et al , 2018 ) as well as for building and evaluating fixedsize sentence embeddings ( Reimers and Gurevych , 2019 ) . Accordingly , several benchmark datasets have been released for both NLI ( Bowman et al , 2015 ; and STS ( Cer et al , 2017 ) in the English language . When it comes to the Korean language , however , benchmark datasets for NLI and STS do not exist . Popular benchmark datasets for Korean NLU typically involve question answering 12 and sentiment analysis 3 , but not NLI or STS . We believe that the lack of publicly available benchmark datasets for Korean NLI and STS has led to the lack of interest for building Korean NLU models suited for these key understanding tasks . Motivated by this , we construct and release Ko - rNLI and KorSTS , two new benchmark datasets for NLI and STS in the Korean language . Following previous work ( Conneau et al , 2018 ) , we construct our datasets by machine - translating existing English training sets and by translating English development and test sets via human translators . We then establish baselines for both KorNLI and KorSTS to facilitate research on Korean NLU .", "entities": [[0, 3, "TaskName", "Natural language inference"], [7, 10, "TaskName", "semantic textual similarity"], [11, 12, "TaskName", "STS"], [22, 25, "TaskName", "natural language understanding"], [35, 36, "DatasetName", "GLUE"], [44, 45, "DatasetName", "SuperGLUE"], [91, 93, "TaskName", "sentence embeddings"], [120, 121, "TaskName", "STS"], [148, 149, "TaskName", "STS"], [161, 163, "TaskName", "question answering"], [165, 167, "TaskName", "sentiment analysis"], [173, 174, "TaskName", "STS"], [189, 190, "TaskName", "STS"], [221, 222, "DatasetName", "KorSTS"], [230, 231, "TaskName", "STS"], [277, 278, "DatasetName", "KorNLI"], [279, 280, "DatasetName", "KorSTS"]]}
{"text": "In an NLI task , a system receives a pair of sentences , a premise and a hypothesis , and classifies their relationship into one out of three categories : entailment , contradiction , and neutral . There are several publicly available NLI datasets in English . Bowman et al ( 2015 ) introduced the Stanford NLI ( SNLI ) dataset , which consists of 570 K English sentence pairs based on image captions . introduced the Multi - Genre NLI ( MNLI ) dataset , which consists of 455 K English sentence pairs from ten genres . Conneau et al ( 2018 ) released the Cross - lingual NLI ( XNLI ) dataset by extending the development and test data of the MNLI corpus to 15 languages . Note that Korean is not one of the 15 languages in XNLI . There are also publicly available NLI datasets in a few other non - English languages ( Fonseca et al , 2016 ; Real et al , 2019 ; Hayashibe , 2020 ) , but none exists for Korean at the time of publication . Figure 1 : Data construction process . MT and PE indicate machine translation and post - editing , respectively . We translate original English data into Korean using an internal translation engine . For development and test data , the machine translation outputs are further post - edited by human experts .", "entities": [[58, 59, "DatasetName", "SNLI"], [82, 83, "DatasetName", "MNLI"], [111, 112, "DatasetName", "XNLI"], [123, 124, "DatasetName", "MNLI"], [140, 141, "DatasetName", "XNLI"], [197, 199, "TaskName", "machine translation"], [226, 228, "TaskName", "machine translation"]]}
{"text": "STS is a task that assesses the gradations of semantic similarity between two sentences . The similarity score ranges from 0 ( completely dissimilar ) to 5 ( completely equivalent ) . It is commonly used to evaluate either how well a model grasps the closeness of two sentences in meaning , or how well a sentence embedding embodies the semantic representation of the sentence . The STS - B dataset consists of 8 , 628 English sentence pairs selected from the STS tasks organized in the context of SemEval between 2012 and 2017 ( Agirre et al , 2012 ( Agirre et al , , 2013 ( Agirre et al , , 2014 ( Agirre et al , , 2015 ( Agirre et al , , 2016 . The domain of input sentences covers image captions , news headlines , and user forums . For details , we refer readers to Cer et al ( 2017 ) .", "entities": [[0, 1, "TaskName", "STS"], [9, 11, "TaskName", "semantic similarity"], [20, 21, "DatasetName", "0"], [56, 58, "TaskName", "sentence embedding"], [67, 70, "DatasetName", "STS - B"], [82, 83, "TaskName", "STS"]]}
{"text": "We explain how we develop two new Korean language understanding datasets : KorNLI and Ko - rSTS . The KorNLI dataset is derived from three different sources : SNLI , MNLI , and XNLI , while the KorSTS dataset stems from the STS - B dataset . The overall construction process , which is applied identically to the two new datasets , is illustrated in Figure 1 . First , we translate the training sets of the SNLI , MNLI , and STS - B datasets , as well as the development and test sets of the XNLI 4 and STS - B datasets , into Korean using an internal neural machine translation engine . Then , the translation results of the development and test sets are post - edited by professional translators in order to guarantee the quality of evaluation . This multi - stage translation strategy aims not only to expedite the translators ' work , but also to help maintain the translation consistency between the training and evaluation datasets . It is worth noting that the post - editing procedure does not simply mean proofreading . Rather , it refers to human translation based on the prior machine translation results , which serve as first drafts .", "entities": [[12, 13, "DatasetName", "KorNLI"], [19, 20, "DatasetName", "KorNLI"], [28, 29, "DatasetName", "SNLI"], [30, 31, "DatasetName", "MNLI"], [33, 34, "DatasetName", "XNLI"], [37, 38, "DatasetName", "KorSTS"], [42, 45, "DatasetName", "STS - B"], [77, 78, "DatasetName", "SNLI"], [79, 80, "DatasetName", "MNLI"], [82, 85, "DatasetName", "STS - B"], [97, 98, "DatasetName", "XNLI"], [100, 103, "DatasetName", "STS - B"], [111, 113, "TaskName", "machine translation"], [200, 202, "TaskName", "machine translation"]]}
{"text": "In this section , we provide baselines for the Korean NLI and STS tasks using our newly created benchmark datasets . Because both tasks receive a pair of sentences as an input , there are two different approaches depending on whether the model encodes the sentences jointly ( \" cross - encoding \" ) or separately ( \" bi - encoding \" ) . 5", "entities": [[12, 13, "TaskName", "STS"]]}
{"text": "We introduced KorNLI and KorSTS - new datasets for Korean natural language understanding . Using these datasets , we also established baselines for Korean NLI and STS with both cross - encoding and bi - encoding approaches . Looking forward , we hope that our datasets and baselines will facilitate future research on not only improving Korean NLU systems but also increasing language diversity in NLU research .", "entities": [[2, 3, "DatasetName", "KorNLI"], [4, 5, "DatasetName", "KorSTS"], [10, 13, "TaskName", "natural language understanding"], [26, 27, "TaskName", "STS"]]}
{"text": "For the Korean RoBERTa baselines used in 4 , we pre - train a RoBERTa ( Liu et al , 2019 ) model on an internal Korean corpora of size 65 GB , consisting of online news articles ( 56 GB ) , encyclopedia ( 7 GB ) , movie subtitles ( \u223c1 GB ) , and the Sejong corpus 8 ( \u223c0.5 GB ) . We use fairseq , which includes the official implementation for RoBERTa . In compared to the original RoBERTa ( English ) , the model architectures are identical except for the token embedding layer , as we use different vocabularies ( 32 K sentencepiece vocab instead of 50 K byte - level BPE ) . After training , the base and large models achieve validation perplexities of 2.55 and 2.39 respectively , where the validation set is a random 5 % subset of the entire corpora .", "entities": [[3, 4, "MethodName", "RoBERTa"], [14, 15, "MethodName", "RoBERTa"], [76, 77, "MethodName", "RoBERTa"], [83, 84, "MethodName", "RoBERTa"], [108, 109, "MethodName", "sentencepiece"], [117, 118, "MethodName", "BPE"]]}
{"text": "To fine - tune Korean RoBERTa and XLM - R models using the cross - encoding approach ( 4.1 ) , we follow the fine - tuning procedures of RoBERTa ( Liu et al , 2019 ) The fine - tuning hyperparameters are summarized in Table 8 . For each dataset and model size , we choose the hyperparameter configurations that are used in the corresponding English version of the dataset and model size ( except for the XLM - R cross - lingual transfer using MNLI , where we also use the same hyperparameters as RoBERTa and XLM - R on KorNLI ) . We find that the hyperparameters used for English models and datasets give sufficiently good performances on the development set , so we do not perform an additional hyperparameter search . After training each model for 10 epochs , we choose the model checkpoint that achieve the highest score on the development set and evaluate it on the test set to obtain our final results in 4.1 . We also report the development set scores for the best checkpoint in Table 9 . We observe that the XLM - R models fine - tuned on KorNLI and KorSTS achieve the highest scores on the development set , although the Korean RoBERTa models perform better on the test set ( Table 5", "entities": [[5, 6, "MethodName", "RoBERTa"], [7, 8, "MethodName", "XLM"], [29, 30, "MethodName", "RoBERTa"], [78, 79, "MethodName", "XLM"], [81, 85, "TaskName", "cross - lingual transfer"], [86, 87, "DatasetName", "MNLI"], [96, 97, "MethodName", "RoBERTa"], [98, 99, "MethodName", "XLM"], [102, 103, "DatasetName", "KorNLI"], [191, 192, "MethodName", "XLM"], [199, 200, "DatasetName", "KorNLI"], [201, 202, "DatasetName", "KorSTS"], [214, 215, "MethodName", "RoBERTa"]]}
{"text": "The goal of the DialPort spoken dialog portal is to gather large amounts of real user data for spoken dialog systems ( SDS ) . Sophisticated statistical representations in state of the art SDS , require large amounts of data . While industry has this , they can not share this treasure . Academia has difficulty getting even small amounts of similar data . With one central portal , connected to many different systems , the task of advertising and affording user access can be done in one centralized place that all systems can connect to . DialPort provides a steady stream of data , allowing system creators to focus on developing their systems . The portal decides what service the user wants and connects them to the appropriate system which carries on a dialog with the user , returning control to the portal at the end . DialPort ( Zhao et al , 2016 ) began with a central agent and the Let'sForecast weather information system . The Cambridge restaurant system ( Gasic et al , 2015 ) and a general restaurant system ( Let 's Eat , that handles cities that Cambridge does not cover ) joined the portal . A chatbot , Qubot , was developed to deal with out - of - domain requests . Later , more systems connected to the portal . A flow of users has begun interacting with the portal . Originally envisioned as a website with a list of the urls of systems a user could try , the portal has become easier to use , more closely resembling what users might expect , given their exposure to the Amazon ECHO 1 and Google HOME 2 , etc . In order to get a flow of users started , DialPort developers expanded the number of connected systems to make the portal offerings more attractive and relevant . They also made the interface easier to use . By the end of March 2017 , in addition to the above systems , the portal also included Mr. Clue , a word game from USC ( Pincus and Traum , 2016 ) , a restaurant opinion bot ( Let 's Discuss , CMU ) , and a bus information system derived from Let 's Go ( Raux et al , 2005 ) . The portal offers users the option of typing or talking and of seeing an agent or just hearing it . With few connected systems in previous versions it was difficult to assess the portal 's switching mechanisms . The increased number of systems challenges the portal to make better decisions and have better a switching strategy . It also demands changes in the frequency of recommendations to connected systems . And it challenged the nature of the agent : some users prefer no visual agent ; others could n't use speech with the system . A short history of DialPort DialPort started with a call for research groups to link their SDS to the portal and a website listing SDS urls for users to try out . It quickly evolved into one userfriendly portal where , all of the SDS are accessed through one central agent , users being seamlessly transferred from one system to another . System connections go through an API that sends them the ASR result ( Chrome at present ) . The system was tried out informally ( Lee et al , 2017 ) to determine whether the portal fulfilled criteria such as : timely response , correct transfer ( to what the user wanted ) , and correct recommendation of systems ( not saying for example , you can ask me about restaurants in Cambridge just after the user has finished talking to that system ) .", "entities": [[160, 161, "DatasetName", "agent"], [169, 170, "DatasetName", "Cambridge"], [193, 194, "DatasetName", "Cambridge"], [203, 204, "TaskName", "chatbot"], [282, 283, "DatasetName", "Google"], [350, 351, "DatasetName", "USC"], [403, 404, "DatasetName", "agent"], [466, 467, "DatasetName", "agent"], [473, 474, "DatasetName", "agent"], [534, 535, "DatasetName", "agent"], [618, 619, "DatasetName", "Cambridge"]]}
{"text": "The first assessment of the interface ( Lee et al , 2017 ) included five External Systems ( ESes , that is , systems that are joined to the portal and are thus not part of the central portal - they can be from CMU as well as from other sites ) : Let'sForecast , Cambridge SDS on restaurants , Lets Eat ; Mr Clue word game ; and Qubot chatbot handling out of domain requests . Since then , Let 's Go and Let'sDiscuss , a chatbot that gives restaurant reviews , have joined . The latter systems , by the CMU portal group , offer new services hoping to attract more diverse users and encourage them to become return users . Cambridge The Cambridge restaurant information system helps users find a restaurant in Cambridge , UK based on the area , the price range or the food type . The current database has just over 100 restaurants and is implemented using the multi - domain statistical dialogue system toolkit PyDial . To connect PyDial to Dialport , PyDial 's dialogue server interface is used . It is implemented as an HTTP server expecting JSON messages from the Dialport client . The system runs a trained dialogue policy based on the GP - SARSA algorithm ( Ga\u0161i\u0107 et al , 2010 ) . Mr. Clue Mr. Clue plays a simple wordguessing game ( Pincus and Traum , 2016 ) . Mr. Clue is the clue - giver and the user plays the role of guesser . Mr. Clue mines his clues from pre - existing web and database resources such as dictionary.com and WordNet . Clue lists used are only clues that pass an automatic filter described in ( Pincus and Traum , 2016 ) . The original Mr. Clue was updated to enable successful communication with Dialport . First , since the original Mr. Clue listens for VH messages ( a variant of ActiveMQ messaging used by the Virtual Human Toolkit ( Hartholt et al , 2013 ) , we built an HTTP server that converts HTTP messages ( expected in JSON format ) to VH messages . Second , since Di - alPort has multiple users in parallel , Mr. Clue was updated to launch a new agent instance for each new HTTP session ( user ) that is directed to the game from the main DialPort system . Mr. Clue is always in one of 2 states ( in - game or out - game ) . The out - game state dialogue is limited to asking if the user wants to play another round ( and offering to give instructions in the beginning of a session ) . The user can use goodbye keyword to exit the system at any time . This sends an exit message to Di - alPort and allowing it to take back control . For its 150 second rounds , timing information is kept on the back - end and sent to the front - end ( DialPort ) in every message . For each new session , the agent chooses 1 of 77 different pre - compiled clue lists ( each with 10 unique target - words ) at random . It keeps track of which lists have been used for a session so a user will never play the same round twice ( for a given session ) . Let'sDiscuss LetsDiscuss responds to queries about a specific restaurant by finding relevant segments of user reviews . It searches a database of restaurant reviews obtained from Zomato and Yelp . We formed a list of general discussion points for restaurants ( service , atmosphere , etc ) . For each discussion point , a list of relevant keywords was compiled using WordNet , thesaurus , and by categorizing the most frequently words found in reviews . Other Systems QuBot , a chatbot from Pohang University and CMU , is used for out - of - domain handling . Let'sForecast , from CMU , uses the NOAA website . Let 's Eat from CMU is based on Yelp , finding restaurants for cities that Cambridge does not cover and for Cambridge if that system is down . Let 's Go , derived from the Let 's Go system ( Raux et al , 2005 ) , is based on an end - to - end recurrent neural network structure and a backend that covers cities other than Pittsburgh .", "entities": [[55, 56, "DatasetName", "Cambridge"], [70, 71, "TaskName", "chatbot"], [87, 88, "TaskName", "chatbot"], [123, 124, "DatasetName", "Cambridge"], [125, 126, "DatasetName", "Cambridge"], [135, 136, "DatasetName", "Cambridge"], [214, 215, "MethodName", "SARSA"], [380, 381, "DatasetName", "agent"], [519, 520, "DatasetName", "agent"], [652, 653, "TaskName", "chatbot"], [694, 695, "DatasetName", "Cambridge"], [700, 701, "DatasetName", "Cambridge"]]}
{"text": "In informal trials , some aspects of the portal 's interaction were not effective for some users . This included the use of speech ( as opposed to typing ) , the use of a visual agent , the absence of both graphical and speech response , feedback and portal behavior . Some ES need graphics to supplement their verbal information . Since Mr Clue keeps score and timing of users ' answers , its instructions and scores are shown on a blackboard . Let 's Go shows a map with the bus trajectory from departure to arrival .", "entities": [[36, 37, "DatasetName", "agent"]]}
{"text": "As more ES join the portal , policies and strategies have become more flexible . There are two major changes to the portal 's behavior : ES selection policy and ES recommendation policy . Starting with few ESes , each on very different topics , the agent selection policy simply tried to detect the topic in the users ' request and select the corresponding ES . As more ESes connect to the portal , non - trivial relationships among ESes emerge : 1 ) Dialog context sensitive agent selection : The optimal choice of ES may depend on discourse history . For example , Let'sForecast , Cambridge restaurant and Let 's Eat : after the user has weather information for city X , they say , recommend a place to have lunch . Choosing between Let 's Eat and Cambridge restaurant depends on the value of city X , because Cambridge restaurant covers places to eat in Cambridge UK and Let 's Eat covers other places . 2 ) Discourse Obligation for Agent selection : Users have various ways to make requests : request ( tell me xxx ) , WH - question ( what 's the weather in xx ) or Yes / No - question ( Is it going to rain ? ) . A natural dialog should answer a user according to the way in which they made their earlier requests ( Traum and Allen , 1994 ) . For example , the weather system should produce the natural Yes it 's going to rain instead of a full weather report , for the third question above . We thus keep the user 's initial request intent in the global dialog context and share it with the relevant ESes . The recommendation policy has been improved in two ways : 1 ) All participating system developers agreed that Skylar should give ES recommendations on a rotating basis so that all systems are recommended equally . Skylar no longer makes a recommendation at the end of each system turn . Recommendations are made about every four turns and , as mentioned above , are not for a system that the user recently interacted with . 2 ) Fine grained recommendation : As more ESes joined the portal , we began to exploit the relatedness among ESs in order to generate more targeted recommendations . For instance , we tuned the policy to have a higher probability of recommending the Let'sDiscuss restaurant review function when users obtain restaurant information by prompting , do you want to hear a review about this place ? Finally , the NLU has been extended to support multi - intent multi - domain identification by reducing the problem to a multi - label classification task using a one - vs - all strategy . The weighted average F - 1 score for multi - intent and multi - domain classification is 0.93 . There are several types of portal users . First , the developers themselves try out the system . Then they ask friends and family to try it . Users can be paid . Finally we have users who really need the information or gaming pleasure . We define two potential types of users ( using IP addresses ) : explorers and real users . Explorers are trying the system for the first time . They explore several of the ESes , but they do not have any real gaming or information need . Real users have returned to use the por - tal , asking for something they need or enjoy . They may speak to less of the ESes during their visit , but have some real . The first advertising attempt using Google AdWords 3 attracted few explorers and no real users . The following factors may explain why users did not have a dialog with the system : presence of human study consent form ; not using Chrome browser ( solved by making a typing - only version ) ; user did n't want any portal services ; user did n't have a microphone ; user did n't understand the purpose of the portal ( we gave Skylar an opening monologue explaining what the data is for ) .", "entities": [[46, 47, "DatasetName", "agent"], [87, 88, "DatasetName", "agent"], [106, 107, "DatasetName", "Cambridge"], [139, 140, "DatasetName", "Cambridge"], [150, 151, "DatasetName", "Cambridge"], [157, 158, "DatasetName", "Cambridge"], [172, 173, "DatasetName", "Agent"], [456, 460, "TaskName", "multi - label classification"], [623, 624, "DatasetName", "Google"]]}
{"text": "Left - to - Right Dependency Parsing with Pointer Networks", "entities": [[5, 7, "TaskName", "Dependency Parsing"]]}
{"text": "We use the same implementation as Ma et al ( 2018 ) and conduct experiments on the Stanford Dependencies ( de Marneffe and Manning , 2008 ) conversion ( using the Stanford parser v3.3.0 ) 3 of the English Penn Treebank ( Marcus et al , 1993 ) , with standard splits and predicted PoS tags . In addition , we compare our approach to the original top - down parser on the same twelve languages from the Universal Dependency Treebanks 4 ( UD ) that were used by Ma et al ( 2018 ) . 5 Following standard practice , we just exclude punctuation for evaluating on PTB - SD and , for each experiment , we report the average Labelled and Unlabelled Attachment Scores ( LAS and UAS ) over 3 and 5 repetitions for UD and PTB - SD , respectively . 3 https://nlp.stanford.edu/software/ lex - parser.shtml 4 http://universaldependencies.org 5 Please note that , since they used a former version of UD datasets , we reran also the top - down algorithm on the latest treebank version ( 2.2 ) in order to perform a fair comparison .", "entities": [[39, 41, "DatasetName", "Penn Treebank"], [83, 84, "DatasetName", "UD"], [108, 109, "DatasetName", "PTB"], [137, 138, "DatasetName", "UD"], [139, 140, "DatasetName", "PTB"], [164, 165, "DatasetName", "UD"]]}
{"text": "By outperforming the two current state - of - theart graph - based ( Dozat and Manning , 2016 ) and transition - based ( Ma et al , 2018 ) models on the PTB - SD , our approach becomes the most accurate fully - supervised dependency parser developed so far , as shown in Table 1 . 6 In addition , in Table 2 we can see how , under the exactly same conditions , the left - to - right algorithm improves over the original top - down variant in nine out of twelve languages in terms of LAS , obtaining competitive results in the remaining three datasets . Finally , in spite of requiring a cycle - checking procedure , our approach proves to be twice as fast as the top - down alternative in decoding time , achieving , under the exact same conditions , a 23.08 - sentences - per - second speed on the PTB - SD compared to 10.24 of the original system . 7", "entities": [[34, 35, "DatasetName", "PTB"], [161, 162, "DatasetName", "PTB"]]}
{"text": "Automatic Fake News Detection : Are current models \" fact - checking \" or \" gut - checking \" ?", "entities": [[1, 4, "TaskName", "Fake News Detection"]]}
{"text": "Automatic fake news detection models are ostensibly based on logic , where the truth of a claim made in a headline can be determined by supporting or refuting evidence found in a resulting web query . These models are believed to be reasoning in some way ; however , it has been shown that these same results , or better , can be achieved without considering the claim at all - only the evidence . This implies that other signals are contained within the examined evidence , and could be based on manipulable factors such as emotion , sentiment , or part - of - speech ( POS ) frequencies , which are vulnerable to adversarial inputs . We neutralize some of these signals through multiple forms of both neural and non - neural pre - processing and style transfer , and find that this flattening of extraneous indicators can induce the models to actually require both claims and evidence to perform well . We conclude with the construction of a model using emotion vectors built off a lexicon and passed through an \" emotional attention \" mechanism to appropriately weight certain emotions . We provide quantifiable results that prove our hypothesis that manipulable features are being used for fact - checking .", "entities": [[1, 4, "TaskName", "fake news detection"], [96, 97, "DatasetName", "emotion"], [101, 104, "DatasetName", "part - of"], [138, 140, "TaskName", "style transfer"], [173, 174, "DatasetName", "emotion"]]}
{"text": "There has been significant work with automatic fact - checking models using RNNs and Transformers ( Shaar et al , 2020a ; Alam et al , 2020 ; Shaar et al , 2020b ) as well as non - neural machine learning using TF - IDF vectors ( Reddy et al , 2018 ) . Current fake news detection models that use a claim 's search engine results as evidence may unintentionally use hidden signals that are not attributed to the claim ( Hansen et al , 2021 ) . Additionally , models may in fact simply memorize biases within data ( Gururangan et al , 2018 ) . Improvements can be made when using human - identified justifications for fact - checking ( Alhindi et al , 2018 ; Vo and Lee , 2020 ) , and making use of textual entailment can offer improvements ( Saikh et al , 2019 ) . Emotional text can signal low credibility ( Rashkin et al , 2017 ) , characterizing fake news as a task where pre - processing can be used effectively to diminish bias ( Giachanou et al , 2019 ; Babanejad et al , 2020 ) . A framework to both categorize fake news and to identify features that differentiate fake news from real news has been described by Molina et al ( 2021 ) , and debiasing inappropriate subjectivity in text can be accomplished by replacing a single biased word in each sentence ( Pryzant et al , 2020 ) . Figure 2 : Ablation studies where evidence was sequentially removed for training and evaluation of models . On the far left , we show the most effective non - neural pre - processing compared to the baseline of none . Performance generally worsens as the ablation increases . Using the claim as a query , the top ten results from Google News ( \" snippets \" ) constitute the evidence ( Hansen et al , 2021 ) . PolitiFact and Snopes use five labels ( False , Mostly False , Mixture , Mostly True , True ) , which we collapse to True , Mixture , and False . To construct the emotion vectors for our EmoAttention system , we use the NRC Affect Intensity Lexicon , which maps approximately 6 , 000 terms to values between 0 and 1 , representing the term 's intensity along 8 different emotions ( Mohammad , 2017 ) . For example , \" interrupt \" and \" rage \" are both categorized as anger words , but with the respective intensity values of 0.333 and 0.911 .", "entities": [[56, 59, "TaskName", "fake news detection"], [314, 315, "DatasetName", "Google"], [332, 333, "DatasetName", "PolitiFact"], [334, 335, "DatasetName", "Snopes"], [366, 367, "DatasetName", "emotion"], [391, 392, "DatasetName", "0"]]}
{"text": "The most common automatic fact - checking NLP models are based on term frequency , word embeddings , and contextualized word embeddings , using Random Forests , LSTMs , and BERT ( Hassan et al , 2017 ) . We limit our experimentation to the BERT model , as it is the highest performing state - of - the - art model and was thoroughly tested in ( Hansen et al , 2021 ) . This BERT model with no pre - processing is our baseline model . For the style transfer model we use the Styleformer model ( Li et al , 2018 ; Schmidt , 2020 ) , a Transformer - based seq2seq model . We also develop our own BERT - based model using the EmoLexi and EmoInt implementation of the EmoCred system by adding an emotional attention layer to emphasize certain emotion representations for a given claim and its evidence ( Giachanou et al , 2019 ) . There is also a snippet attention layer at - tending to which evidence itself should be weighted most heavily for the given claim .", "entities": [[15, 17, "TaskName", "word embeddings"], [20, 22, "TaskName", "word embeddings"], [30, 31, "MethodName", "BERT"], [45, 46, "MethodName", "BERT"], [76, 77, "MethodName", "BERT"], [90, 92, "TaskName", "style transfer"], [111, 112, "MethodName", "Transformer"], [114, 115, "MethodName", "seq2seq"], [122, 123, "MethodName", "BERT"], [145, 146, "DatasetName", "emotion"]]}
{"text": "Our goal is to separate affect - based properties from factual content of the text . Toward this , we run a large number of permutations of the following four simple pre - processing steps ( see Figure 4 in Appendix B for results ) . These steps were chosen as they have been shown to facilitate affective tasks such as sentiment analysis , emotion classification , and sarcasm detection ( Babanejad et al , 2020 ) . In some cases we used a modified form - such as removing adverbs for POS pre - processing . Negation ( NEG ) : A mechanism that transforms a negated statement into its inverse ( Benamara et al , 2012 ) . An example , \" I am not happy \" would have \" not \" removed and \" happy \" replaced by its antonym , forming the sentence \" I am sad . \" Parts - of - Speech ( POS ) : We keep only three parts of speech : nouns , verbs , and adjectives . We initially included adverbs but found removing them improved results . This could be due to some adverbs being emotionally charged . Stopwords ( STOP ) : These are generally the most common words in a language , such as function words and prepositions . We use the NLTK library . Stemming ( STEM ) : Reducing a word to its root form . We use the NLTK Snowball Stemmer .", "entities": [[61, 63, "TaskName", "sentiment analysis"], [64, 66, "TaskName", "emotion classification"], [68, 70, "TaskName", "sarcasm detection"]]}
{"text": "We use the adversarial technique of generating paraphrases for all the claims and evidence through style transfer . As well , it removes punctuation and alters phrasing that might be understood as sarcasm , such as \" Melania Trump said that Native Americans upset about the Dakota Access Pipeline should ' go back to India \" ' to \" Melania Trump told Native Americans that was upset by the Dakota Access Pipeline , that they should travel to India . \" The informalto - formal model lowercases everything and also changes the text significantly . We chose this paraphrasing model based on the idea that fake news - especially that which is frequently posted on social media - has a certain polarizing style that might be neutralized by altering the formality of the text . Rather surprisingly , we received better results transforming the style from formal - to - informal than we did with informal - toformal .", "entities": [[15, 17, "TaskName", "style transfer"]]}
{"text": "In Figure 4 , we report all results for each preprocessing step . Figure 4 : The full table of results for all pre - processing steps for the Snopes ( SNES ) and PolitiFact ( POMT ) datasets . Due to the high compute requirements of the formal and informal style transfer models , these datasets were only prepared for the Snopes dataset . The darkest green colors indicate the best results , while the red indicates the worst . Multiple pre - processing steps such as ( pos , stop ) were performed in the order written .", "entities": [[29, 30, "DatasetName", "Snopes"], [34, 35, "DatasetName", "PolitiFact"], [51, 53, "TaskName", "style transfer"], [62, 63, "DatasetName", "Snopes"]]}
{"text": "The text evaluation tool helps to assess Estonian texts for their degrees of complexity according to the CERF ( see Figure 4 ) . Currently , the tool takes into account only lexical information and defines the CEFR level of each particular lemma based on CEFR vocabulary lists ( see Chapter 3.1 ) . Similar tools have also been developed for many other languages ( see e.g. Alfter , 2021 ) . The program runs on EstNLTK v 1.6 , which offers functionality to lemmatise and perform morphological analysis . The tool needs to be developed further . First , there is a need to implement methods for the improvement of the analysis of homonyms ( for example tamm can mean either an oak or a dam , which is assigned different levels in word lists ) and multiword expressions . So far , the analysis is based only on single word lists , which is not sufficient . 4 Summary \" Teacher 's Tools \" is the first attempt to provide a systematic overview of the development of lexical and grammatical competence in Estonian as a Second Language . The project is a work in progress and further development of the toolkit is foreseen . We plan to add descriptions of the development of grammar competence and communicative language activities for adult learners . The enrichment of the text evaluation module with the possibility of measuring grammatical difficulty and readability ( by for example adding Lix - index value ) is under development . \" Teacher 's Tools \" as a resource can be used for different CALL tasks , including auto - matic CEFR - related vocabulary and grammar exercise generation or lexical simplification tasks . The use of NLP for the development of such computerassisted tools has enormous potential for enhancing the teaching and learning of Estonian as an L2 .", "entities": [[42, 43, "DatasetName", "lemma"], [87, 89, "TaskName", "morphological analysis"], [284, 286, "TaskName", "lexical simplification"]]}
{"text": "Arguably , spoken dialogue systems are most often used not in hands / eyes - busy situations , but rather in settings where a graphical display is also available , such as a mobile phone . We explore the use of a graphical output modality for signalling incremental understanding and prediction state of the dialogue system . By visualising the current dialogue state and possible continuations of it as a simple tree , and allowing interaction with that visualisation ( e.g. , for confirmations or corrections ) , the system provides both feedback on past user actions and guidance on possible future ones , and it can span the continuum from slot filling to full prediction of user intent ( such as GoogleNow ) . We evaluate our system with real users and report that they found the system intuitive and easy to use , and that incremental and adaptive settings enable users to accomplish more tasks .", "entities": [[2, 5, "TaskName", "spoken dialogue systems"], [111, 113, "TaskName", "slot filling"]]}
{"text": "This section introduces and describes our SDS , which is modularised into four main components : ASR , natural language understanding ( NLU ) , dialogue management ( DM ) , and the graphical user interface ( GUI ) which , as explained below , is visualised as a right - branching tree . The overall system is represented in Figure 1 . For the remainder of this section , each module is explained in turn . As each module processes input incrementally ( i.e. , word for word ) , we first explain our framework for incremental processing .", "entities": [[18, 21, "TaskName", "natural language understanding"], [25, 27, "TaskName", "dialogue management"]]}
{"text": "An aspect of our SDS that sets it apart from others is the requirement that it process incrementally . One potential concern with incremental processing is regarding informativeness : why act early when waiting might provide additional information , resulting in better - informed decisions ? The trade off is naturalness as perceived by the user who is interacting with the SDS . Indeed , it has been shown that human users perceive incremental systems as being more natural than traditional , turn - based systems ( Aist et al , 2006 ; Skantze and Schlangen , 2009 ; Skantze and Hjalmarsson , 1991 ; Asri et al , 2014 ) , offer a more human - like experience ( Edlund et al , 2008 ) and are more satisfying to interact with than non - incremental systems ( Aist et al , 2007 ) . Psycholinguistic research has also shown that humans comprehend utterances as they unfold and do not wait until the end of an utterance to begin the comprehension process ( Tanenhaus et al , 1995 ; Spivey et al , 2002 ) . The trade - off between informativeness and naturalness can be reconciled when mechanisms are in place that allow earlier decisions to be repaired . Such mechanisms are offered by the incremental unit ( IU ) framework for SDS ( Schlangen and Skantze , 2011 ) , which we apply here . Following Kennington et al ( 2014 ) , the IU framework consists of a network of processing modules . A typical module takes input , performs some kind of processing on that data , and produces output . The data are packaged as the payload of incremental units ( IUs ) which are passed between modules . The IUs themselves are interconnected via so - called same level links ( SLL ) and groundedin links ( GRIN ) , the former allowing the linking of IUs as a growing sequence , the latter allowing that sequence to convey what IUs directly affect it ( see Figure 2 for an example of incremental ASR ) . Thus IUs can be added , but can be later revoked and replaced in light of new information . The IU framework can take advantage of up - to - date information , but have the potential to function in such a way that users perceive as more natural . The modules explained in the remainder of this section are implemented as IU - modules and process incrementally . Each will now be explained .", "entities": [[314, 315, "MethodName", "GRIN"]]}
{"text": "The module that takes speech input from the user in our SDS is the ASR component . Incremental ASR must transcribe uttered speech into words which must be forthcoming from the ASR as early as possible ( i.e. , the ASR must not wait for endpointing to produce output ) . Each module that follows must also process incrementally , acting in lock - step upon input as it is received . Incremental ASR is not new ( Baumann et al , 2009 ) and many of the current freely - accessible ASR systems can produce output ( semi - ) incrementally . We opt for Google ASR for its vocabulary coverage of our evaluation language ( German ) . Following , Baumann et al ( 2016 ) , we package output from the Google service into IUs which are passed to the NLU module , which we now explain .", "entities": [[106, 107, "DatasetName", "Google"], [134, 135, "DatasetName", "Google"]]}
{"text": "We approach the task of NLU as a slot - filling task ( a very common approach ; see Tur et al ( 2012 ) ) where an intent is complete when all slots of a frame are filled . The main driver of the NLU in our SDS is the SIUM model of NLU introduced in Kennington et al ( 2013 ) . SIUM has been used in several systems which have reported substantial results in various domains , languages , and tasks ( Han et al , 2015 ; Kennington and Schlangen , 2017 ) Though originally a model of reference resolution , it was always intended to be used for general NLU , which we do here . The model is formalised as follows : P ( I | U ) = 1 P ( U ) P ( I ) r R P ( U | R = r ) P ( R = r | I ) ( 1 ) That is , P ( I | U ) is the probability of the intent I ( i.e. , a frame slot ) behind the speaker 's ( ongoing ) utterance U . This is recovered using the mediating variable R , a set of properties which map between aspects of U and aspects of I. We opt for abstract properties here ( e.g. , the frame for restaurant might be filled by a certain type of cuisine intent such as italian which has properties like pasta , mediterranean , vegetarian , etc . ) . Properties are pre - defined by a system designer and can match words that might be uttered to describe the intent in question . For P ( R | I ) , probability is distributed uniformly over all properties that a given intent is specified to have . ( If other information is available , more informative priors could be used as well . ) The mapping between properties and aspects of U can be learned from data . During application , R is marginalised over , resulting in a distribution over possible intents . 1 This occurs at each word increment , where the distribution from the previous increment is combined via P ( I ) , keeping track of the distribution over time . We further apply a simple rule to add in apriori knowledge : if some r R and w U are such that r . = w ( where . = is string equality ; e.g. , an intent has the property of pasta and the word pasta is uttered ) , then we set C ( U = w | R = r ) = 1 . To allow for possible ASR confusions , we also apply C ( U = w | R = r ) = 1 \u2212 ld ( w , r ) /max ( len ( w ) , len ( r ) ) , where ld is the Levenshtein distance ( but we only apply this if the calculated value is above a threshold of 0.6 ; i.e. , the two strings are mostly similar ) . For all other w , C ( w | r ) = 0 . This results in a distribution C , which we renormalise and blend with learned distribution to yield P ( U | R ) . We apply an instantiation of SIUM for each slot . The candidate slots which are processed depends on the state of the dialogue ; only slots represented by visible nodes are considered , thereby reducing the possible frames that could be predicted . At each word increment , the updated slots ( and their corresponding ) distributions are given to the DM , which will now be explained .", "entities": [[541, 542, "DatasetName", "0"]]}
{"text": "The goal of the GUI is to intuitively inform the user about the internal state of the ongoing understanding . One motivation for this is that the user can determine if the system understood the user 's intent before providing the user with a response ( e.g. , a list of restaurants of a certain type ) ; i.e. , if any misunderstanding takes place , it happens before the system commits to an action and is potentially more easily repaired . The display is a rightbranching tree , where the branches directly off the root node display the affordances of the system ( i.e. , what domains of things it can understand and do something about ) . When the first tree is displayed , it represents a state of the NLU where none of the slots are filled , as in Figure 3 . When a user verbally selects a domain to ask about , the tree is adjusted to make that domain the only one displayed and the slots that are required for that domain are shown as branches . The user can then fill those slots ( i.e. , branches ) by uttering the displayed name , or , alternatively , by uttering the item to fill the slot directly . For example , at a minimum , the user could utter the name of the domain then an item for each slot ( e.g. , food Thai downtown ) or the speech could be more natural ( e.g. , I 'm quite hungry , I am looking for some Thai food maybe in the downtown area ) . Crucially , the user can also hesitate within and between chunks , as advancement is not triggered by silence thresholding , but rather semantically . When something is uttered that falls into the request state of the DM as explained above , the display expands the subtree under question and marks the item with a question mark ( see Figure 4 ) . At this point , the user can utter any kind of confirmation . A positive confirmation fills the slot with the item in question . A negative confirmation retracts the question , but leaves the branch expanded . The expanded branches are displayed according to their rank as given by the NLU 's probability distribution . Though a branch in the display can theoretically display an unlimited number of children , we opted to only show 7 children ; if a branch had more , the final child displayed as an ellipsis . A completed branch is collapsed , visually marking its corresponding slot as filled . At any time , a user can backtrack by saying no ( or equivalent ) or start the entire interaction over from the beginning with a keyword , e.g. , restart . To aid the user 's attention , the node under question is marked in red , where completed slots are represented by outlined nodes , and filled nodes represent candidates for the current slot in question ( see examples of all three in Figure 4 ) . For cases where the system is in the wait state for several words ( during which there is no change in the tree ) , the system signals activity at each word by causing the red node in question to temporarily change to white , then back to red ( i.e. , appearing as a blinking node to the user ) . Figure 5 shows a filled frame , represented as tree with one branch for each filled slot . Such an interface clearly shows the internal state of the SDS and whether or not it has understood the request so far . It is designed to aid the user 's attention to the slot in question , and clearly indicates the affordances that the system has . The interface is currently a read - only display that is purely speech - driven , but it could be augmented with additional functionalities , such as tapping a node for expansion or typing input that the system might not yet display . It is currently implemented as a web - based interface ( using the JavaScript D3 library ) , allowing it to be usable as a web application on any machine or mobile device . Adaptive Branching The GUI as explained affords an additional straight - forward extension : in order to move our system towards adaptivity on the above - mentioned continuum , the GUI can be used to signal what the system thinks the user might say next . This is done by expanding a branch and displaying a confirmation on that branch , signalling that the system predicts that the user will choose that particular branch . Alternatively , if the system is confident that a user will fill a slot with a particular value , that particular slot can be filled without confirmation . This is displayed as a collapsed tree branch . A system that perfectly predicts a user 's intent would fill an entire tree ( i.e. , all slots ) only requiring the user to confirm once . A more careful system would confirm at each step ( such an interaction would only require the user to utter confirmations and nothing else ) . We applied this adaptive variant of the tree in one of our experiments explained below .", "entities": [[708, 709, "DatasetName", "D3"]]}
{"text": "The participants were seated at a desk and given written instructions indicating that they were to use the system to perform as many tasks as possible in the allotted time . Figure 6 shows some example tasks as they would be displayed ( one at a time ) to the user . A screen , tablet , and keyboard were on the desk in front of the user ( see Figure 7 ) . 2 The user was instructed to convey the task presented on the screen to the system such that the GUI on the tablet would have a completed tree ( e.g. , as in Figure 5 ) . When the participant was satisfied that the system understood her intent , she was to press space bar on the keyboard which triggered a new task to be displayed on the screen and reset the tree to its start state on the tablet ( as in Figure 3 ) . The possible task domains were call , which had a single slot for name to be filled ( i.e. , one out of the 22 most common German given names ) ; message which had a slot for name and a slot for the message ( which , when invoked , would simply fill in directly from the ASR until 1 second of silence was detected ) ; eat which had slots for type ( in this case , 6 possible types ) and location ( in this case , 6 locations based around the city of Bielefeld ) ; route which had slots for source city and the destination city ( which shared the same list of the top 100 most populous German cities ) ; and reminder which had a slot for message . For each task , the domain was first randomly chosen from the 5 possible domains , and then each slot value to be filled was randomly chosen ( the message slot for the name and message domains was randomly selected from a list of 6 possible \" messages \" , each with 2 - 3 words ; e.g. , feed the cat , visit grandma , etc . ) . The system kept track of which tasks were already presented to the participant . At any time after the first task , the system could choose a task that was previously presented and present it again to the participant ( with a 50 % chance ) so the user would often see tasks that she had seen before ( with the assumption that humans who use PAs often do perform similar , if not the same , tasks more than once ) . The participant was told that she would interact with the system in three different phases , each for 4 minutes , and to accomplish as many tasks as possible in that time allotment . The participant was not told what the different phases were . The experiments described in Sections 4.2 and screen tablet keyboard participant 4.3 respectively describe and report a comparison first between the Phase 1 and 2 ( denoted as the endpointed and incremental variants of the system ) in order to establish whether or not the incremental variant produced better results than the endpointed variant . We also report a comparison between Phase 2 and 3 ( incremental and incremental - adaptive phases ) . Phase 1 and Phase 3 are not directly comparable to each other as Phase 3 is really a variant of Phase 2 . Because of this , we fixed the order of the phase presentation for all participants . Each of these phases are described below . Before the participant began Phase 1 , they were able to try it out for up to 4 minutes ( in Phase 1 settings ) and ask for help from the experimenter , allowing them to get used to the Phase 1 interface before the actual experiment began . After this trial phase , the experiment began with Phase 1 . Phase 1 : Non - incremental In this phase , the system did not appear to work incrementally ; i.e. , the system displayed tree updates after ASR endpointing ( of 1.2 seconds - a reasonable amount of time to expect a response from a commercial spoken PA ) . The system displayed the ongoing ASR on the tablet as it was recognised ( as is often done in commercial PAs ) . At the end of Phase 1 , a pop up window notified the user that the phase was complete . They then moved onto Phase 2 . Phase 2 : Incremental In this phase , the system displayed the tree information incrementally without endpointing . The ASR was no longer displayed ; only the tree provided feedback in understanding , as explained in Section 3.5 . After Phase 2 , a 10 - question questionnaire was displayed on the screen for the participant to fill out comparing Phase 1 and Phase 2 . For each question , they had the choice of Phase 1 , Phase 2 , Both , and Neither . ( See Appendix for full list of questions . ) After completing the questionnaire , they moved onto Phase 3 . Phase 3 : Incremental - adaptive In this phase , the incremental system was again presented to the participant with an added user model that \" learned \" about the user . If the user saw a task more than once , the user model would predict that , if the user chose that task domain again ( e.g. , route ) then the system would automatically ask a clarification using the previously filled values ( except for the message slot , which the user always had to fill ) . If the user saw a task more than 3 times , the system skipped asking for clarifications and filled in the domain slots completely , requiring the user only to press the space bar to confirm it was the correct one ( i.e. , to complete the task ) . An example progression might be as follows : a participant is presented with the task route from Bielefeld to Berlin , then the user would attempt to get the system to fill in the tree ( i.e. , slots ) with those values . After some interaction in other domains , the user sees the same task again , and now after indicating the intent type route , the user must only say \" yes \" for each slot to confirm the system 's prediction . Later , if the task is presented a third time , when entering that domain ( i.e , route ) , the two slots would already be filled . If later a different route task was presented , e.g. , route from Bielefeld to Hamburg , the system would already have the two slots filled , but the user could backtrack by saying \" no , to Hamburg \" which would trigger the system to fill the appropriate slot with the corrected value . Later interactions within the route domain would ask for a clarification on the destination slot since it has had several possible values given by the participant , but continue to fill the from slot with Bielefeld . After Phase 3 , the participants were presented with another questionnaire on the screen to fill out with the same questions ( plus two additional questions ) , this time comparing Phase 2 and Phase 3 . For each item , they had the choice of Phase 2 , Phase 3 , Both , and Neither . At the end of the three phases and questionnaires , the participants were given a final questionnaire to fill out by hand on their general impressions of the systems . We recruited 14 participants for the evaluation . We used the Mint tools data collection framework ( Kousidis et al , 2012 ) to log the interactions . Due to some technical issues , one of the participants did not log interactions . We collected data from 13 participants , post - Phase 2 questionnaires from 12 participants , post - Phase 3 questionnaires from all 14 participants , and general questionnaires from all 14 participants . In the experiments that follow , we report objective and subjective measures to determine the settings that produced superior results . Metrics We report the subjective results of the participant questionnaires . We only report those items that were statistically significant ( see Appendix for a full list of the questions ) . We further report objective measures for each system variant : total number of completed tasks , fully correct frames , average frame f - score , and average time elapsed ( averages are taken over all participants for each variant ; we only used the 10 participants who fully interacted with all three phases ) . Discussion is left to the end of this section .", "entities": [[1330, 1331, "DatasetName", "Mint"]]}
{"text": "Given the results and analysis , we conclude that an intuitive presentation that signals a system 's ongoing understanding benefits end users who perform simple tasks which might be performed by a PA . The GUI that we provided , using a right - branching tree , worked well ; indeed , the participants who used it found it intuitive and easy to understand . There are gains to be made when the system signals understanding at finer - grained levels than just at the end of a pre - formulated utterance . There are further gains to be made when a PA attempts to learn ( even a rudimentary ) user model to predict what the user might want to do next . The adaptivity moves our system from one extreme of the continuum - simple slot filling - closer towards the extreme that is fully predictive , with the additional benefit of being able to easily correct mistakes in the predictions . For future work , we intend to provide simple authoring tools for the system to make building simple PAs using our GUI easy . We want to improve the NLU and scale to larger domains . 3 We also plan on implementing this as a standalone application that could be run on a mobile device , which could actually perform the tasks . It would further be beneficial to compare the GUI with a system that responds with speech ( i.e. , without a GUI ) . Lastly , we will investigate using touch as an additional input modality to select between possible alternatives that are offered by the system . It was sometimes unclear to me if the assistant understood me . The assistant responded while I spoke . The assistant sometimes did things that I did not expect . When the assistant made mistakes , it was easy for me to correct them . In addition to the above 10 questions , the following were also asked on the questionnaire following Phase 3 : I had the feeling that the assistant attempted to learn about me . I had the feeling that the assistant made incorrect guesses . The following questions were used on the general questionnaire : I regularly use personal assistants such as Siri , Cortana , Google now or Amazon Echo : Yes / No I have never used a speech - based personal assistant : Yes / No What was your general impression of our personal assistants ? Would you use one of these assistants on a smart phone or tablet if it were available ? If yes , which one ? Do you have suggestions that you think would help us improve our assistants ? If you have used other speech - based interfaces before , do you prefer this interface ?", "entities": [[137, 139, "TaskName", "slot filling"], [385, 386, "DatasetName", "Google"]]}
{"text": "Evaluating Word Embeddings for Language Acquisition", "entities": [[1, 3, "TaskName", "Word Embeddings"], [4, 6, "TaskName", "Language Acquisition"]]}
{"text": "Continuous vector word representations ( or word embeddings ) have shown success in capturing semantic relations between words , as evidenced by evaluation against behavioral data of adult performance on semantic tasks ( Pereira et al , 2016 ) . Adult semantic knowledge is the endpoint of a language acquisition process ; thus , a relevant question is whether these models can also capture emerging word representations of young language learners . However , the data for children 's semantic knowledge across development is scarce . In this paper , we propose to bridge this gap by using Age of Acquisition norms to evaluate word embeddings learnt from child - directed input . We present two methods that evaluate word embeddings in terms of ( a ) the semantic neighbourhood density of learnt words , and ( b ) convergence to adult word associations . We apply our methods to bag - of - words models , and find that ( 1 ) children acquire words with fewer semantic neighbours earlier , and ( 2 ) young learners only attend to very local context . These findings provide converging evidence for validity of our methods in understanding the prerequisite features for a distributional model of word learning .", "entities": [[6, 8, "TaskName", "word embeddings"], [48, 50, "TaskName", "language acquisition"], [104, 106, "TaskName", "word embeddings"], [119, 121, "TaskName", "word embeddings"]]}
{"text": "Word embeddings have a long tradition in Computational Linguistics . There exist a range of methods to derive word embeddings based on the distributional paradigm , such that words with similar embeddings are semantically related . These embeddings are often evaluated either extrinsically , on how well they boost performance on a certain task , or intrinsically , by comparing representations against behavioral data from tests of semantic sim - ilarity , synonymity , analogy or word association ( Pereira et al , 2016 ) . Adult semantic knowledge is the culmination of a language acquisition process ; therefore , a relevant question is whether these models can also capture emerging word representations of language learners . A capacity for distributional analysis is a basic assumption of all theories of language acquisition : children are capable of performing distributional analyses over their input from a young age ( Saffran et al , 1996 ) , motivating the use of word embeddings for modelling language acquisition . However , the evaluation of emergent word representations is far from straightforward , as there is no availability of the kind of semantic judgements that we have for adults . This paper presents two methods for evaluating word embeddings for language acquisition . We apply our methods to two bag - of - words models , and evaluate them on the acquisition of nouns in English - speaking children 1 .", "entities": [[0, 2, "TaskName", "Word embeddings"], [18, 20, "TaskName", "word embeddings"], [94, 96, "TaskName", "language acquisition"], [130, 132, "TaskName", "language acquisition"], [159, 161, "TaskName", "word embeddings"], [163, 165, "TaskName", "language acquisition"], [203, 205, "TaskName", "word embeddings"], [206, 208, "TaskName", "language acquisition"]]}
{"text": "Bag - of - words models offer a good starting point to evaluate word representations in the context of language acquisition , given their minimal assumptions on knowledge of word order : once the context of a word is determined , the order in which words appear in this context is ignored by these type of models . We explore a range of hyperparameter configurations of two models : a ' contextcounting ' model involving a PPMI matrix compressed with Singular Value Decomposition ( SVD ) , and the Skipgram with Negative Sampling ( SGNS ) version of word2vec ( Mikolov et al , 2013 ) . Note that , although these models have been found to implicitly optimize the same shifted - PPMI matrix ( Levy and Goldberg , 2014 ) , they are unlikely to obtain the same results without careful parameter alignment . Our goal by selecting these two approaches is to increase the variability of model performance within the bag - of - words paradigm . The hyperparameters we explore include : window size [ 1 , 2 , 3 , 4 , 5 , 7 , 10 ] , minimum frequency threshold [ 10 , 50 , 100 ] , dynamic window ( for SGNS ) , negative sampling in SGNS [ 0 , 15 ] ( and its equivalents as shifted - PPMI ) , eigenvalue in SVD [ 0 , 0.5 , 1 ] . We restrict our analyses to vectors of size 100 . We use the Hyperwords package from Levy et al ( 2015 ) .", "entities": [[19, 21, "TaskName", "language acquisition"], [76, 77, "DatasetName", "PPMI"], [84, 85, "DatasetName", "SVD"], [123, 124, "DatasetName", "PPMI"], [217, 218, "DatasetName", "0"], [228, 229, "DatasetName", "PPMI"], [233, 234, "DatasetName", "SVD"], [235, 236, "DatasetName", "0"]]}
{"text": "Our first evaluation method above focused on the structure of the semantic spaces provided by the learnt word embeddings . Now we turn our attention to the specific lexical items and their position in the semantic space . Children tend to under - and overextend word meaning in the first stages of acquisition , and over time they become more precise on capturing the semantics of words . A logical assumption then , is that words learnt earlier also converge earlier to adult - like semantic representations ( assuming that early and late words take , on average , approximately the same amount of time to converge ) . We incorporated this idea in our second method by relating the AoA of words with adult free word association norms . Note that this method can be applied to other semantic tasks , but we focus on word association because it does not impose the specific type of semantic relation that words need to have ( i.e. there is no distinction between similarity , analogy or others ) . The dataset of free word association that we used is known as Small World of Worlds ( SWOW , De Deyne et al , 2019 ) , and it is the largest dataset of word associations in English , containing responses to over 12 , 000 cue words . We filtered the preprocessed version of the dataset to include only words that have been acquired before 60 months old . This results in 613 cue words , and 1839 responses ( word associates ) to these cues . We then performed a similar cue - response experiment , with the best model from the previous section : for each cue , we retrieved the closest n neighbours . As in Pereira et al ( 2016 ) , we used n = 50 , and then computed how many of these neighbours overlap with the word associates ( responses ) provided by human adults . However , unlike that work , our evaluation is not based directly on the number of overlaps . Instead , we computed the Spearman rank correlation between the number of overlaps and the AoA norms , in order to quantify whether word embeddings corresponding to words learned earlier by children are also those that are converging faster to adult semantic knowledge . Figure 3 shows the result of this procedure . As can be seen , there is a statistically significant rank correlation ( \u03c1 = \u22120.378 , p < 0.001 ) . The negative direction confirms that words acquired earlier have a network of word associates that is more similar to those of adults , suggesting that convergence to adult semantic knowledge is at a more advanced state . One limitation of this procedure is that it requires a choice on the number of neighbours to be retrieved . In order to see how much the metric is affected by this parameter , we report the rank correlations of the previous model for several values of n. As can be seen in Figure 4 , this number stabilizes after n = 25 . The figure also shows whether this metric favours a model that did not perform well in our previous evaluation metric ( SVD with window size 4 , shift 15 , frequency threshold 10 ) . The graph shows that this model is consistently worse on our second evaluation method as well .", "entities": [[17, 19, "TaskName", "word embeddings"], [373, 375, "TaskName", "word embeddings"], [547, 548, "DatasetName", "SVD"]]}
{"text": "We proposed two methods to evaluate word embeddings for language acquisition . The main feature of these methods is the use of AoA norms for assessing whether the semantic organization of the word embeddings support the developmental trajectory of word learning . The use of these metrics already prompted the discovery that ( 1 ) words with fewer neighbours are easier to acquire , suggesting competition of neighbouring words , and ( 2 ) at young age , infants only attend to very local context . The application of these methods to distributional models that incorporate additional assumptions ( e.g. knowledge of word order ) holds promise for further understanding of the role of distributional information in word learning .", "entities": [[6, 8, "TaskName", "word embeddings"], [9, 11, "TaskName", "language acquisition"], [32, 34, "TaskName", "word embeddings"]]}
{"text": "Tag Assisted Neural Machine Translation of Film Subtitles", "entities": [[3, 5, "TaskName", "Machine Translation"]]}
{"text": "Very early work addressed named entity translation by treating automatically identified named entities with a special translation system , usually a transliterator ( Babych and Hartley , 2003 ) . This work did not attempt to integrate the translation models for one to benefit from information learned by the other . Later , especially with neural machine translation ( NMT ) systems , source - side feature augmentation research studied the inclusion of linguistic feature information into the source - side token embeddings , usually by adding in or concatenating additional learned feature vectors to the token embedding vectors , as we do in this work ( Sennrich and Haddow , 2016 ; Hoang et al , 2016b ; Ugawa et al , 2018 ; Modrzejewski , 2020 ; Armengol - Estap\u00e9 et al , 2020 ) . This approach can also be adopted on the target - side , as presented here or in ( Hoang et al , 2016a ( Hoang et al , , 2018Nguyen et al , 2018 ) . However , these methods only add linguistic feature information to the input , without encouraging the system to model that information in any particular way . Factored translation systems , under both statistical and neural machine translation , instead explore the addition of externally supplied linguistic features to the raw text at both input and output . These features include part - of - speech ( POS ) tags , word lemmatizations , morphological analysis , and semantic analysis ( Koehn and Hoang , 2007 ; Garcia - Martinez et al , 2016 , 2017Tan et al , 2020 ) . Factored translation models map feature - augmented input into feature - augmented output , however outputs include only an underlying lemma together with the predicted features . These systems also use a rule - based morphology toolkit in post - processing to generate the output surface forms from predicted output features , requiring knowledge of appropriate rule systems for the output language . An additional tagged architecture ( N\u0203dejde et al , 2017 ) predicted syntax - tagged surface forms , but did so by appending the tags to the surface form tokens directly , rather than predicting separate factors . In general , the focus of factored models has been to increase vocabulary coverage , for example of highly agglutitanative languages with rich morphologies , rather than our goal of disambiguating polysemous of polysyntactic words or otherwise handling named entities in a more nuanced way . Finally , one previous work does consider a fully tagged ( both source and target ) factored neural model predicting tags with surface forms with independent layers in much the same way as presented here ( Wagner , 2017 ) . This work showed negative results for various syntactic tag types on IWSLT'14 shared task data ( Cettolo et al , 2014 ) , whereas this work presents NER and POS tags on film subtitles data .", "entities": [[56, 58, "TaskName", "machine translation"], [209, 211, "TaskName", "machine translation"], [234, 237, "DatasetName", "part - of"], [247, 249, "TaskName", "morphological analysis"], [295, 296, "DatasetName", "lemma"], [490, 491, "TaskName", "NER"]]}
{"text": "We implemented two extensions to the standard seq2seq encoder - decoder architecture for neural machine translation to use token - level tags to improve translation results . 1 By combining token and tag embeddings in the input and simultaneously predicting tokens and tags in the output , the NMT system learned to translate tagged source sentences to tagged target sentences ( Figure 1 ) . We used a Transformer encoder and decoder for the base seq2seq model ( Vaswani et al , 2017 ) . Tags are added to the data as a preprocessing step .", "entities": [[7, 8, "MethodName", "seq2seq"], [14, 16, "TaskName", "machine translation"], [68, 69, "MethodName", "Transformer"], [75, 76, "MethodName", "seq2seq"]]}
{"text": "Our experiments focused on film subtitles in German and English . The Opus project provided a parallel German to English subtitles corpus from OpenSubtitles ( Tiedemann , 2012 ; Aulamo et al , 2020 ) . This data was cleaned with some rudimentary sentence length filtering , and randomly divided into a 3 million sentence - pair training split ( about 49 million tokens ) , along with 100 , 000 pair validation and test splits ( about 1.6 million tokens each ) . Around 3 % of words in the OpenSubtitles corpus were tagged as named entities ( non O ) . We further divided the test split based on whether any named entities were found in either the source or the target sentence . Out of 100 , 000 test pairs , 79 , 201 had no named entities , and 20 , 799 had some .", "entities": [[23, 24, "DatasetName", "OpenSubtitles"], [91, 92, "DatasetName", "OpenSubtitles"]]}
{"text": "It should not go unnoticed that the typical inference algorithms for sequence labeling , particularly the BiLSTM - CRF inference employed by most NER systems , are incompatible with the autoregressive sequence decoding algorithms ( greedy decoding and beam search ) used for inference by seq2seq models . That the beam decoding algorithm ( and autoregressive likelihood model ) used here for tags was unable to account for ( be conditioned on ) the as - yet uncomputed right context was cause for much apprehension before experimental results became available . These positive results notwithstanding , future work could explore how to better incorporate the full tagging context in tag de - coding , perhaps , for example , by predicting the sequence more wholistically with non - autoregressive decoding ( Gu et al , 2018 ) . We also imagine that the design of the underlying seq2seq architecture may lend itself to certain types of sequence labeling . For example , the bidirectional context modeled by a BiLSTM - based translation model may be more suitable for certain types of sequence labeling tasks than the Transformer 's attentional activations . Because our contributions are agnostic to the type of sequence labeling ( NER or part - of - speech tagging or any other kind ) as well as to the design of the encoder and decoder , future experiments should also explore these possibilities .", "entities": [[16, 17, "MethodName", "BiLSTM"], [18, 19, "MethodName", "CRF"], [23, 24, "TaskName", "NER"], [45, 46, "MethodName", "seq2seq"], [147, 148, "MethodName", "seq2seq"], [168, 169, "MethodName", "BiLSTM"], [186, 187, "MethodName", "Transformer"], [203, 204, "TaskName", "NER"], [205, 211, "TaskName", "part - of - speech tagging"]]}
{"text": "This paper introduced the first purpose - built corpus of M\u0101ori loanwords on Twitter , as well as a methodology for automatically filtering out irrelevant data via machine learning . The MLT Corpus opens up a myriad of opportunities for future work . Since our corpus is a diachronic one ( i.e. , all tweets are time - stamped ) , we are planning to use it for testing hypotheses about language change . This is especially desirable in the context of New Zealand English , which has recently undergone considerable change as it comes into the final stage of dialect formation ( Schneider , 2003 ) . Another avenue of future research is to automatically identify other M\u0101ori loanwords that are not part of our initial list of query words . This could be achieved by deploying a language detector tool on every unique word in the corpus ( Martins and Silva , 2005 ) . The \" discovered \" words could be used as new query words to further expand our corpus . In addition , we intend to explore the meaning of our M\u0101ori loanwords using distributional semantic models . We will train popular word embeddings algorithms on the MLT Corpus , such as Word2Vec ( Mikolov et al , 2013 ) and FastText ( Bojanowski et al , 2017 ) , and identify words that are close to our loanwords in the semantic space . We predict that these neighbouring words will enable us to understand the semantic make - up of our loanwords according to their usage . Finally , we hope to extrapolate these findings by deploying our trained classifier on other online discourse sources , such as Reddit posts . This has great potential for enriching our understanding of how M\u0101ori loanwords are used in social media .", "entities": [[197, 199, "TaskName", "word embeddings"], [216, 217, "MethodName", "FastText"], [284, 285, "DatasetName", "Reddit"]]}
{"text": "We present experiments for cross - domain semantic dependency analysis with a neural Maximum Subgraph parser . Our parser targets 1 - endpoint - crossing , pagenumber - 2 graphs which are a good fit to semantic dependency graphs , and utilizes an efficient dynamic programming algorithm for decoding . For disambiguation , the parser associates words with BiLSTM vectors and utilizes these vectors to assign scores to candidate dependencies . We conduct experiments on the data sets from Se - mEval 2015 as well as Chinese CCGBank . Our parser achieves very competitive results for both English and Chinese . To improve the parsing performance on cross - domain texts , we propose a data - oriented method to explore the linguistic generality encoded in English Resource Grammar , which is a precisionoriented , hand - crafted HPSG grammar , in an implicit way . Experiments demonstrate the effectiveness of our data - oriented method across a wide range of conditions .", "entities": [[58, 59, "MethodName", "BiLSTM"], [87, 88, "DatasetName", "CCGBank"]]}
{"text": "SDP is the task of mapping a natural language sentence into a formal meaning representation in the form of a dependency graph . Figure 1 shows an Minimal Recursion Semantics ( MRS ; Copestake et al , 2005 ) reduced semantic dependency analysis ( Ivanova et al , 2012 ) . In this example , the semantic analysis is represented as a labeled directed graph in which the vertices are tokens in the sentence . The graph abstracts away from syntactic analysis ( e.g. , the complementizer - thatand passive construction are excluded ) and includes most semantically relevant non - anaphoric local ( e.g. , from \" wants \" to \" Mark \" ) and longdistance ( e.g. , from \" buy \" to \" company \" ) dependencies . The arc labels encode linguisticallymotivated , broadly - applicable semantic relations that are grounded under the type - driven semantics . It is worth noting that semantic dependency graphs are not necessarily trees : ( 1 ) a token may be multiply headed because a word can be the arguments of more than one predicate ; ( 2 ) cycles are allowed if the direction of arcs are not taken into account .", "entities": [[31, 32, "DatasetName", "MRS"]]}
{"text": "Usually , syntactic dependency analysis employs the tree - shaped representation . Dependency parsing , thus , can be formulated as the search for a maximum spanning tree ( MST ) from an arcweighted ( complete ) graph . For SDP where the target representation are no longer trees , Kuhlmann and Jonsson ( 2015 ) proposed to generalize the MST model to other types of subgraphs . In general , dependency parsing is formulated as the search for Maximum Subgraph regarding to a particular graph class , viz . G : Given a graph G = ( V , A ) , find a subset A \u2286 A with maximum total weight such that the induced subgraph G = ( V , A ) belongs to G. Formally , we have the following optimization problem : G ( s ) = arg max H G ( s , G ) SCORE ( H ) = arg max H G ( s , G ) p in H SCOREPART ( s , p ) ( 1 ) Here , G ( s , G ) is the set of all graphs that belong to G and are compatible with s and G. For parsing , G is usually a complete graph . SCOREPART ( s , p ) evaluates whether a small subgraph p of a candidate graph H is a good partial analysis for sentence s. For some graph classes and some types of score functions , there exists efficient algorithms for solving ( 1 ) . For example , when G is the set of noncrossing graphs and SCOREPART is limited to handle individual dependencies , ( 1 ) can be solved in cubic - time ( Kuhlmann and Jonsson , 2015 ) .", "entities": [[12, 14, "TaskName", "Dependency parsing"], [71, 73, "TaskName", "dependency parsing"]]}
{"text": "We use words as well as POS tags as clues for scoring an individual arc . In particular , we transform all of them into continuous and dense vectors . Inspired by Costa - juss\u00e0 and Fonollosa ( 2016 ) 's work , we utilize character - based embedding for low - frequency words , i.e. , words that appear more than k times in the training data , and word - based embeddings for other words . The word - based embedding module applies the common lookup - table mechanism , while the character - based word embedding w i is implemented by extracting the features ( denoted as c 1 , c 2 , . . . , c n ) within a character - based BiLSTM : x 1 : x n = BiLSTM ( c 1 : c n ) w i = x 1 + x n", "entities": [[30, 31, "DatasetName", "Inspired"], [128, 129, "MethodName", "BiLSTM"], [136, 137, "MethodName", "BiLSTM"]]}
{"text": "The concatenation of word embedding w i and POS - tag embedding p i of each word in specific sentence is used as the input of BiLSTMs to extract context - related feature vectors r i for each position i. a i = w i p i r 1 : r n = BiLSTM ( a 1 : a n )", "entities": [[53, 54, "MethodName", "BiLSTM"]]}
{"text": "In our first order model , the SCORE function evaluates the preference of a semantic dependency graph by considering every bilexical relation in this graph one by one . In particular , the corresponding SCOREPART function assigns a score to a candidate arc between word i and word j using a non - linear transform from the two feature vectors , viz . r i and r j , associated to the two words : SCOREPART ( i , j ) = W 2 ReLU ( W 1 , 1 r i + W 1 , 2 r j + b ) The assignment task for dependency labels can be regarded as a classification task . Our label scoring process is similar to the prediction of dependencies : LABEL ( i , j ) = arg max W 2 ReLU ( W 1 , 1 r i + W 1 , 2 r j + b ) + b 2 We can see here the two local score functions explicitly utilize the positions of a semantic head and a semantic dependent . It is similar to the firstorder factorization as defined in a number of linear parsing models , e.g. , the models defined by Martins and Almeida ( 2014 ) and Cao et al ( 2017a ) .", "entities": [[84, 85, "MethodName", "ReLU"], [139, 140, "MethodName", "ReLU"]]}
{"text": "Intuitively , a hand - crafted precision grammar , e.g. , ERG , reflects highly generalized properties of a particular language and is thus highly resilient to domain shifts . Accordingly , one should expect that a precision grammar - guided parser which guarantees the a rich set of domain - independent linguistic constraints to be met can be more robust to domain shifts than a purely data - driven parser . In related work for syntactic parsing , Ivanova et al ( 2013 ) showed that the ERG - based parser was more robust to domain variation than several representative data - driven parsers . Zhang and Wang ( 2009 ) proposed to derive features from syntactic parses generated by PET to assist a data - driven dependency tree parser and observed some encouraging results for cross - domain evaluation . However , there are at least two drawbacks of their ERG - guided parser based method : 1 . A considerable number of sentences can not benefit from ERG since PET may produce no analysis . 2 . This method fails to take parsing efficiency into account .", "entities": [[121, 122, "DatasetName", "PET"], [172, 173, "DatasetName", "PET"]]}
{"text": "Since around 2001 , the ERG has been accompanied by syntactico - semantic annotations , where for each sentence an annotator has selected the intended analysis among all alternatives licensed by the grammar . This derived resource , namly Redwoods 6 ( Oepen et al , 2002 ; Flickinger et al , 2017 ) , is a collection of hand - annotated corpora and consists of data sets from several distinct domains . Redwoods also includes ( re ) treebanking results of the first 22 sections of the venerable Wall Street Journal ( WSJ ) text and the section of Brown Corpus in the Penn Treebank ( Marcus et al , 1993 ) . The WSJ part is also known as Deep - Bank . The Brown corpus part is used as the out - of - domain test data by Se - mEval 2015 . The DM data sets for both SemEval 2014 and 2015 SDP shared tasks are based on the RedWoods corpus . Besides gold standard annoations , Flickinger et al ( 2010 ) built the WikiWoods corpus 7 , which provides automatically created annotations for the texts from wikipedia . The annotations are disambiguated using the MaxEnt model trained using redwoods without DeepBank . We use a small portion of Wikiwoods , which contains 857 , 329 sentences in total . To evaluate the ( positive ) impact of ERG on out - of - domain parsing , we conduct experiments on the DM data . The first group of experiments are designed to be comparable with the results obtained by various participant systems of SemEval 2015 . The detailed data set - up is as follows : Test Data . We use the Brown corpus section which is provided by SemEval 2015 . Training Data . We use three data sets for training : ( 1 ) DeepBank , ( 2 ) RedWoods and ( 3 ) a small portion of WikiWoods reparsed using the MaxEnt model trained on Deep - Bank . We denote this reparsed WikiWoods as WikiWoods - ACE , since the HPSG analysis is provided by the ACE parser . To extract the semantic dependency graph , we use the pydelphin tool 8 . For the second group of experiments , we use the section wsj21 from the DeepBank as test data , which is the official in - domain test of the SemEval 2015 . The training data includes the \" RedWoods minus DeepBank \" annotations ( RedwoodsWOD for short ) as well as the official WikiWoods annotations . Note that the MaxEnt model used to obtain the official WikiWoods annotations are compatible with RedwoodswWOD . Due to the diversity of the RedwoodsWOD and DeepBank sentences , this set - up can also be viewed as an outof - domain evaluation . WikiWoods to train another model , and leave out other parts of Redwoods . The performance improvement is more remarkable when providing more data , even though such data contains annotation errors . For the second group of experiments , we use the RedwoodsWOD sentences for training and the DeepBank WSJ sentences for evaluation . For this set - up , consistent improvements of the parser quality are observed .", "entities": [[104, 106, "DatasetName", "Penn Treebank"]]}
{"text": "Open Relation Extraction : Relational Knowledge Transfer from Supervised Data to Unsupervised Data", "entities": [[1, 3, "TaskName", "Relation Extraction"]]}
{"text": "Open relation extraction ( OpenRE ) aims to extract relational facts from the open - domain corpus . To this end , it discovers relation patterns between named entities and then clusters those semantically equivalent patterns into a united relation cluster . Most OpenRE methods typically confine themselves to unsupervised paradigms , without taking advantage of existing relational facts in knowledge bases ( KBs ) and their high - quality labeled instances . To address this issue , we propose Relational Siamese Networks ( RSNs ) to learn similarity metrics of relations from labeled data of pre - defined relations , and then transfer the relational knowledge to identify novel relations in unlabeled data . Experiment results on two real - world datasets show that our framework can achieve significant improvements as compared with other state - of - the - art methods . Our code is available at https://github . com / thunlp / RSN .", "entities": [[1, 3, "TaskName", "relation extraction"]]}
{"text": "Open Relation Extraction . Relation extraction ( RE ) is an important task in NLP . Traditional RE methods mainly concentrate on classifying relational facts into pre - defined relation types ( Mintz et al , 2009 ; Yu et al , 2017 ) . Zeng ( 2014 ) utilizes CNN encoders to build sentence representations with the help of position embeddings . Lin ( 2016 ) further improves RE performance on distantlysupervised data via instance - level attention . These methods take advantage of supervised or distantlysupervised data to learn neural sentence encoders for distributed representations , and have achieved promising results . However , these methods can not handle the open - ended growth of new relation types in the open - domain corpora . To solve this problem , recently many efforts have been invested in exploring methods for open relation extraction ( OpenRE ) , which aims to discover new relation types from unsupervised open - domain corpora . OpenRE methods can be roughly divided into two categories : taggingbased and clustering - based . Tagging - based methods cast OpenRE as a sequence labeling problem , and extract relational phrases consisting of words from sentences in unsupervised ( Banko et al , 2007 ; Banko and Etzioni , 2008 ) or supervised paradigms ( Jia et al , 2018 ; Cui et al , 2018 ; Stanovsky et al , 2018 ) . However , tagging - based methods often extract multiple overly - specific relational phrases for the same relation type , and can not be readily utilized for downstream tasks . In comparison , conventional clustering - based OpenRE methods extract rich features for relation instances via external linguistic tools , and cluster semantic patterns into several relation types ( Lin and Pantel , 2001 ; Yao et al , 2011Yao et al , , 2012 . Marcheggiani ( 2016 ) proposes a reconstructionbased model discrete - state variational autoencoder for OpenRE via unlabeled instances . Elsahar ( 2017 ) utilizes a clustering algorithm over linguistic features . In this paper , we focus on the clustering - based OpenRE methods , which have the advantage of discovering highly distinguishable relation types . Few - shot Learning . Few - shot learning aims to classify instances with a handful of labeled samples . Many efforts are devoted to few - shot image classification ( Koch et al , 2015 ) and relation classification ( Yuan et al , 2017 ; Han et al , 2018 ) . Notably , ( Koch et al , 2015 ) introduces Convolu - tional Siamese Neural Network for image metric learning , which inspires us to learn relational similarity metrics for OpenRE . Semi - supervised Clustering . Semi - supervised clustering aims to cluster semantic patterns given instance seeds of target categories ( Bair , 2013 ; Hongtao Lin , 2019 ) . Differently , our proposed Semi - supervised RSN only leverages labeled instances of pre - defined relations , and does not need any seed of new relations .", "entities": [[1, 3, "TaskName", "Relation Extraction"], [4, 6, "TaskName", "Relation extraction"], [143, 145, "TaskName", "relation extraction"], [325, 327, "MethodName", "variational autoencoder"], [370, 374, "TaskName", "Few - shot Learning"], [375, 379, "TaskName", "Few - shot learning"], [395, 400, "TaskName", "few - shot image classification"], [408, 410, "TaskName", "relation classification"], [442, 444, "TaskName", "metric learning"], [472, 473, "DatasetName", "seeds"]]}
{"text": "In experiments , we use FewRel ( Han et al , 2018 ) as our first dataset . FewRel is a human - annotated dataset containing 80 types of relations , each with 700 instances . An advantage of FewRel is that every instance contains a unique entity pair , so RE models can not choose the easy way to memorize the entities . We use the original train set of FewRel , which contains 64 relations , as labeled set with predefined relations , and the original validation set of FewRel , which contains 16 new relations , as the unlabeled set with novel relations to extract . We then randomly choose 1 , 600 instances from the unlabeled set as the test set , with the rest labeled and unlabeled instances considered as the train set . The second dataset we use is FewRel - distant , which contains the distantly - supervised data obtained by the authors of FewRel before human an - notation . We follow the split of FewRel to obtain the auto - labeled train set and unlabeled train set . For evaluation , we use the human - annotated test set of FewRel with 1 , 600 instances . Unlabeled instances already existing in this test set are removed from the unlabeled train set of FewRel - distant . Finally , the auto - labeled train set contains 323 , 549 relational instances , and the unlabeled train set contains 60 , 581 instances . A previous OpenRE work reports performance on an unpublic dataset called NYT - FB ( Marcheggiani and Titov , 2016 ) . However , it has several shortcomings compared with FewRel - distant . First , NTY - FB 's test set is distantly - supervised and is noisy for instance - level RE . Moreover , instances in NYT - FB often share entity pairs or relational phrases , which makes it much easier for relation clustering . Therefore , we think the results on FewRel - distant are convincing enough for Distantly - supervised OpenRE .", "entities": [[5, 6, "DatasetName", "FewRel"], [18, 19, "DatasetName", "FewRel"], [39, 40, "DatasetName", "FewRel"], [71, 72, "DatasetName", "FewRel"], [91, 92, "DatasetName", "FewRel"], [145, 146, "DatasetName", "FewRel"], [161, 162, "DatasetName", "FewRel"], [173, 174, "DatasetName", "FewRel"], [199, 200, "DatasetName", "FewRel"], [222, 223, "DatasetName", "FewRel"], [282, 283, "DatasetName", "FewRel"], [338, 339, "DatasetName", "FewRel"]]}
{"text": "In this subsection , we mainly focus on analyzing the influence of pre - defined relation diversity , i.e. , the number of relations in the labeled train set . To study this influence , we use FewRel for evaluation and change the number of relations in the labeled train set from 40 to 64 while fixing the total num - ber of labeled instances to 25 , 000 , and report the clustering results in Figure 5 . Several conclusions can be drawn according to Figure 5 . Firstly , a rich variety of labeled relations do improve the performance of our models , especially RSN . The models trained on 64 relations perform better than those trained on 40 relations constantly . Secondly , while the performance of supervised RSN is very sensitive to pre - defined relation diversity , its semi - supervised counterparts suffer much less from the relation number limit . This phenomenon suggests that Semi - supervised RSNs succeed in learning from unlabeled novelrelation data and are more generalizable to novel relations .", "entities": [[37, 38, "DatasetName", "FewRel"]]}
{"text": "In this paper , we propose a new model Relational Siamese Network ( RSN ) for OpenRE . Different from conventional unsupervised models , our model learns to measure relational similarity from supervised / distantly - supervised data of predefined relations , as well as unsupervised data of novel relations . There are mainly two innovative points in our model . First , we propose to transfer relational similarity knowledge with RSN structure . To the best of our knowledge , we are the first to propose knowledge transfer for OpenRE . Second , we propose Semi / Distantly - supervised RSN , to further perform semi - supervised and distantlysupervised transfer learning . Experiments show that our models significantly surpass conventional OpenRE models and achieve new state - of - the - art performance . For future research , we plan to explore the following directions : ( 1 ) Besides CNN , there are some other popular sentence encoder structures like piecewise convolutional neural network ( PCNN ) and Long Short - Term Memory ( LSTM ) for RE . In the future , we can try different sentence encoders in our model . ( 2 ) As mentioned above , our model has the potential ability to discover the hierarchical structure of relations . In the future , we will try to explore this application with additional experiments .", "entities": [[10, 12, "MethodName", "Siamese Network"], [111, 113, "TaskName", "transfer learning"], [171, 176, "MethodName", "Long Short - Term Memory"], [177, 178, "MethodName", "LSTM"]]}
{"text": "Pre - trained language models ( PLMs ) have achieved remarkable success on various natural language understanding tasks . Simple fine - tuning of PLMs , on the other hand , might be suboptimal for domain - specific tasks because they can not possibly cover knowledge from all domains . While adaptive pre - training of PLMs can help them obtain domain - specific knowledge , it requires a large training cost . Moreover , adaptive pre - training can harm the PLM 's performance on the downstream task by causing catastrophic forgetting of its general knowledge . To overcome such limitations of adaptive pre - training for PLM adaption , we propose a novel domain adaption framework for PLMs coined as Knowledge - Augmented Language model Adaptation ( KALA ) , which modulates the intermediate hidden representations of PLMs with domain knowledge , consisting of entities and their relational facts . We validate the performance of our KALA on question answering and named entity recognition tasks on multiple datasets across various domains . The results show that , despite being computationally efficient , our KALA largely outperforms adaptive pre - training . Code is available at : https://github.com/Nardien/KALA .", "entities": [[14, 17, "TaskName", "natural language understanding"], [95, 97, "TaskName", "general knowledge"], [160, 162, "TaskName", "question answering"], [163, 166, "TaskName", "named entity recognition"]]}
{"text": "Pre - trained Language Models ( PLMs ) ( Devlin et al , 2019 ; Brown et al , 2020 ) have shown to be effective on various Natural Language Understanding ( NLU ) tasks . Although PLMs aim to address diverse downstream tasks from various data sources , there have been considerable efforts to adapt the PLMs to specific domains - distributions over the language characterizing a given topic or genre ( Gururangan et al , 2020 ) - for which the acquisition of domain knowledge is required to accurately solve the downstream tasks ( e.g. , Biomedical Named Entity Recognition ( Dogan et al , 2014 ) ) . This problem , known as Language Model Adaptation , can be viewed as a transfer learning problem ( Yosinski et al , 2014 ; Ruder , 2019 ) under domain shift , where the model is pre - trained on the general domain and the labeled distribution is available for the target domain - specific task . The most prevalent approach to this problem is adaptive pre - training ( Figure 2a ) which further updates all parameters of the PLM on a large domain - specific or curated task - specific corpus , with the same pretraining strategy ( e.g. , masked language modeling ) before fine - tuning it on the downstream task ( Beltagy et al , 2019 ; Gururangan et al , 2020 ) . This continual pre - training of a PLM on the target domain corpus allows it to learn the distribution of the target domain , resulting in improved performance on domain - specific tasks ( Howard and Ruder , 2018 ; Han and Eisenstein , 2019 ) . While it has shown to be effective , adaptive pretraining has obvious drawbacks . First , it is computationally inefficient . Although a PLM becomes more powerful with the increasing amount of pretraining data ( Gururangan et al , 2020 ) , further pre - training on the additional data requires larger memory and computational cost as the dataset size grows ( Bai et al , 2021 ) . Besides , it is difficult to adapt the PLM to a new domain without forgetting the general knowledge it obtained from the initial pretraining step , since all pre - trained parameters are continually updated to fit the domain - specific corpus during adaptive pre - training . This catastrophic forgetting of the task - Our KALA framework embeds the unseen entities on the embedding space of seen entities by representing them with their relational knowledge over the graph , while the strong DAPT baseline ( Gururangan et al , 2020 ) can not appropriately handle unseen entities that are not given for task fine - tuning . general knowledge may lead to the performance degradation on the downstream tasks . In Figure 1 , we show that adaptive pre - training with more training steps could lead to performance degeneration . Thus , it would be preferable if we could adapt the PLM to the domain - specific task without costly adaptive pre - training . To this end , we aim to integrate the domain - specific knowledge into the PLM directly during the task - specific fine - tuning step , as shown in Figure 2b , eliminating the adaptive pre - training stage . Specifically , we first note that entities and relations are core building blocks of the domain - specific knowledge that are required to solve for the domain - specific downstream tasks . Clinical domain experts , for example , are familiar with medical terminologies and their complex relations . Then , to represent the domain knowledge consisting of entities and relations , we introduce the Entity Memory , which is the source of entity embeddings but independent of the PLM parameters ( See Entity Memory in Figure 2b ) . Then , we further exploit the relational structures of the entities by utilizing a Knowledge Graph ( KG ) , which denotes the factual relationships between entities , as shown in Knowledge Graph of Figure 2b . The remaining step is how to integrate the knowledge into the PLM during fine - tuning . To this end , we propose a novel layer named Knowledgeconditioned Feature Modulation ( KFM , 3.2 ) , which scales and shifts the intermediate hidden representations of PLMs by conditioning them with retrieved knowledge representations . This knowledge integration scheme has several advantages . First , it does not modify the original PLM architecture , and thus could be integrated into any PLMs regardless of their architectures . Also , it only re - quires marginal computational and memory overhead , while eliminating the need of excessive further pre - training ( Figure 1 ) . Finally , it can effectively handle unseen entities with relational knowledge from the KG , which are suboptimally embedded by adaptive pre - training . For example , as shown in Figure 2 , an entity restenosis does not appear in the training dataset for fine - tuning , thus adaptive pre - training only implicitly infers them within the context from the broad domain corpus . However , we can explicitly represent the unknown entity by aggregating the representations of known entities in the entity memory ( i.e. , in Figure 2 , neighboring entities , such as asthma and pethidine , are used to represent the unseen entity restenosis ) . We combine all the previously described components into a novel language model adaptation framework , coined as Knowledge - Augmented Language model Adaptation ( KALA ) ( Figure 3 ) . We empirically verify that KALA improves the performance of the PLM over adaptive pre - training on various domains with two knowledge - intensive tasks : Question Answering ( QA ) and Named Entity Recognition ( NER ) . Our contribution is threefold : We propose a novel LM adaptation framework , which augments PLMs with entities and their relations from the target domain , during fine - tuning without any further pre - training . To our knowledge , this is the first work that utilizes the structured knowledge for language model adaptation . To reflect structural knowledge into the PLM , we introduce a novel layer which scales and shifts the intermediate PLM representations with the entity representations contextualized by their related entities according to the KG . We show that our KALA significantly enhances the model 's performance on domain - specific QA and NER tasks , while being significantly more efficient over existing LM adaptation methods .", "entities": [[28, 31, "TaskName", "Natural Language Understanding"], [99, 102, "TaskName", "Named Entity Recognition"], [125, 127, "TaskName", "transfer learning"], [213, 216, "TaskName", "masked language modeling"], [372, 374, "TaskName", "general knowledge"], [464, 466, "TaskName", "general knowledge"], [637, 639, "TaskName", "entity embeddings"], [975, 977, "TaskName", "Question Answering"], [981, 984, "TaskName", "Named Entity Recognition"], [985, 986, "TaskName", "NER"], [1096, 1097, "TaskName", "NER"]]}
{"text": "Language Model Adaptation Nowadays , transfer learning ( Howard and Ruder , 2018 ) is a dominant approach for solving Natural Language Understanding ( NLU ) tasks . This strategy first pretrains a Language Model ( LM ) on a large and unlabeled corpus , then fine - tunes it on downstream tasks with labeled data ( Devlin et al , 2019 ) . While this scheme alone achieves impressive performance on various NLU tasks , adaptive pre - training of the PLM on a domain - specific corpus helps the PLM achieve better performance on the domain - specific tasks . For example , demonstrated that a further pre - trained LM on biomedical documents outperforms the original LM on biomedical NLU tasks . Also , Gururangan et al ( 2020 ) showed that adaptive pre - training of the PLM on the corpus of a target domain ( Domain - adaptive Pre - training ; DAPT ) or a target task ( Task - adaptive Pre - training ; TAPT ) improves its performance on domain - specific tasks . However , above approaches generally require a large amount of computational costs for pre - training . Knowledge - aware LM Accompanied with increasing sources of knowledge ( Vrandecic and Kr\u00f6tzsch , 2014 ) , some prior works have proposed to integrate external knowledge into PLMs , to enhance their performance on tasks that require structured knowledge . For instance , ERNIE ( Zhang et al , 2019 ) and KnowBERT ( Peters et al , 2019 ) incorporate entities as additional inputs in the pretraining stage to obtain a knowledge - aware LM , wherein a pre - trained knowledge graph embedding from Wikidata ( Vrandecic and Kr\u00f6tzsch , 2014 ) is used to represent entities . Entity - as - Experts ( F\u00e9vry et al , 2020 ) and LUKE ( Yamada et al , 2020 ) use the entity memory that is pre - trained along with the LMs from scratch . ERICA ( Qin et al , 2021 ) further uses the fact consisting of entities and their relations in the pre - training stage of LMs from scratch . Previous works aim to integrate external knowledge into the LMs during the pre - training step to obtain a universal knowledge - aware LM that requires additional parameters for millions of entities . In contrast to this , our framework aims to efficiently modify a general PLM for the domain - specific task with a linear modulation layer scheme discussed in Section 3.2 , during fine - tuning .", "entities": [[5, 7, "TaskName", "transfer learning"], [20, 23, "TaskName", "Natural Language Understanding"], [282, 285, "TaskName", "knowledge graph embedding"]]}
{"text": "Our goal is to solve Natural Language Understanding ( NLU ) tasks for a specific domain , with a knowledge - augmented Language Model ( LM ) . We first introduce the NLU tasks we target , followed by the descriptions of the proposed knowledgeaugmented LM . After that , we formally define the ingredients for structured knowledge integration .", "entities": [[5, 8, "TaskName", "Natural Language Understanding"]]}
{"text": "We use the uncased BERT - base ( Devlin et al , 2019 ) as the base PLM for all our experiments on QA and NER tasks . For more details on training and implementation , please see the Appendix B.", "entities": [[4, 5, "MethodName", "BERT"], [25, 26, "TaskName", "NER"]]}
{"text": "Although we believe our experimental results on more params on NewsQA ) . Thus , we believe that our KALA would be useful to any PLMs , not depending on specific PLMs .", "entities": [[8, 9, "MetricName", "params"], [10, 11, "DatasetName", "NewsQA"]]}
{"text": "In this paper , we introduced KALA , a novel framework for language model adaptation , which modulates the intermediate representations of a PLM by conditioning it with the entity memory and the relational facts from KGs . We validated KALA on various domains of QA and NER tasks , on which KALA significantly outperforms relevant baselines while being computationally efficient . We demonstrate that the success of KALA comes from both KFM and relational retrieval , allowing the PLM to recognize entities but also handle unseen ones that might frequently appear in domain - specific tasks . There are many other avenues for future work , including the application of KALA on pre - training of knowledge - augmented PLMs from scratch .", "entities": [[47, 48, "TaskName", "NER"]]}
{"text": "Enhancing the domain converge of pre - traind language models ( PLMs ) with external knowledge is increasingly important , since the PLMs can not observe all the data during training and can not memorize all the necessary knowledge for solving down - stream tasks . Our KALA contributes to this problem by augmenting domain knowledge graphs for PLMs . However , we have to still consider the accurateness of knowledge , i.e. , the fact in the knowledge graph may not be correct , which affects the model to generate incorrect answers . Also , the model 's prediction performance is still far from optimal . Thus , we should be aware of model 's failure from errors in knowledge and prediction , especially on high - risk domains ( e.g. , biomedicine ) . Fine - tuned model \" text \" : \" Arvane Rezai \" , \" start \" : 30 , \" end \" : 43 , \" i d \" : 228998 \" h \" : 11578 , \" r \" : \" P3373 \" , \" t \" : 228998", "entities": [[55, 57, "TaskName", "knowledge graphs"]]}
{"text": "Here we describe the dataset details with its statistics for two different tasks : extractive question answering ( QA ) and named entity recognition ( NER ) . Question Answering We evaluate models on three domain - specific datasets : NewsQA , Relation , and Medication . Notably , NewsQA ( Trischler et al , 2017 ) is curated from CNN news articles . Relation and Medication are originally part of the emrQA ( Pampari et al , 2018 ) , which is an automatically constructed question answering dataset based on the electrical medical record from n2c2 challenges 9 . However , Yue et al ( 2020 ) extract two major subsets by dividing the entire dataset into Relation and Medication and suggest the usage of sampled questions from the original em - rQA dataset . Following the suggestion of Yue et al ( 2020 ) , we use only 1 % of generated questions of Relation for training , validation , and testing . Also , we only use 1 % of generated questions of Medication for training and use 5 % of generated questions of Medication for validation and testing . Since the original emrQA is automatically generated based on templates , the quality is poor - it means that the original emrQA dataset was inappropriate to evaluate the ability of the model to reason over the clinical text since the most of questions can be answered by the simple text matching . To overcome this limitation , Yue et al ( 2020 ) suggests two ways to make the task more difficult . First , they divide the question templates into easy and hard versions and then use the hard question only . Second , they suggest replacing medical terminologies in the question of the test set into synonyms to avoid the trivial question which can be solvable with a simple text matching . We use both methods to Relation and Medication datasets to report the performance of every model . For more details on Relation and Medication datasets , please refer to the original paper ( Yue et al , 2020 ) . The statistics of training , validation , and test sets on all QA datasets are provided in Table 9 . Named Entity Recognition We use three different domain - specific datasets for evaluating our KALA on NER tasks : CoNLL - 2003 ( Sang andMeulder , 2003 ) ( News ) , WNUT - 17 ( Derczynski et al , 2017 )", "entities": [[15, 17, "TaskName", "question answering"], [21, 24, "TaskName", "named entity recognition"], [25, 26, "TaskName", "NER"], [28, 30, "TaskName", "Question Answering"], [40, 41, "DatasetName", "NewsQA"], [49, 50, "DatasetName", "NewsQA"], [72, 73, "DatasetName", "emrQA"], [86, 88, "TaskName", "question answering"], [196, 197, "DatasetName", "emrQA"], [214, 215, "DatasetName", "emrQA"], [242, 244, "TaskName", "text matching"], [314, 316, "TaskName", "text matching"], [377, 380, "TaskName", "Named Entity Recognition"], [393, 394, "TaskName", "NER"], [413, 414, "DatasetName", "Derczynski"]]}
{"text": "In this subsection , we give detailed descriptions of how the FLOPs in Figure 1 are measured . We majorly follow the script from the ELECTRA ( Clark et al , 2020 ) repository to compute the approximated FLOPs for all models including ours . For FLOPs computation of our KALA , we additionally include the FLOPs of the entity embedding layer , linear layers for h 1 , h 2 , h 3 , h 4 , and GNN layer . Since the GNN layer is implemented based on the sparse implementation , we first calculate the FLOPs of the message propagation over one edge , and then multiply it to the average number of edges per node . Also , in terms of the computation on mentions , we consider the maximum sequence length of the context rather than the average number of mentions , to set the upper bound of FLOPs for our KALA . Note that , in NewsQA training data , the average number of nodes is 57 , the average number of edges for each node is 0.64 , and the average number of mentions in the context is 92.68 .", "entities": [[25, 26, "MethodName", "ELECTRA"], [162, 163, "DatasetName", "NewsQA"]]}
{"text": "In this subsection , we analyze how the size of entity memory affects the performance of our KALA . In Figure 8 , we plot the performance of KALA on the NewsQA dataset by varying the number of entity elements in the memory . Note that , we reduce the size of the entity memory by eliminating the entity appearing fewer times . Thus , the results are obtained by only considering the entities that appear more than [ 1000 , 100 , 10 , 5 , 0 ] times , e.g. , 0 means the model with full entity memory . As shown in Figure 8 , we observe that the size of the entity memory is larger , the performance of our KALA is better in general . However , interestingly , we also observe that the smallest size of the entity memory shows decent performance , which might be due to the fact that some parameters in the entity memory are stale . For more discussions on it including visualization , please refer to Appendix D.2 . Finally , we would like to note that , in Figure 1 , we report the performance of our KALA in the case of [ 1000 , 5 , 0 ] ( i.e. , considering entities appearing more than [ 1000 , 5 , 0 ] times ) .", "entities": [[31, 32, "DatasetName", "NewsQA"], [87, 88, "DatasetName", "0"], [93, 94, "DatasetName", "0"], [209, 210, "DatasetName", "0"], [224, 225, "DatasetName", "0"]]}
{"text": "In this subsection , we aim to analyze which numbers of entities and facts per context are appropriate to achieve good performance in NER tasks . Specifically , we first collect the contexts having more than or equal to the k number of entities ( or facts ) , and then calculate the performance difference from our KALA to the fine - tuning baseline . As shown in Figure 9 , while there are no obvious patterns , performance improvements from the baseline are consistent across a varying number of entities and facts . This result suggests that our KALA is indeed beneficial when entities and facts are given to the model , whereas the appropriate number of entities and facts to obtain the best performance against the baseline is different across datasets .", "entities": [[23, 24, "TaskName", "NER"]]}
{"text": "In the main paper and Appendix B.1 , we describe that the location of the KFM layer inside the PLM architecture is the hyperparameter . However , someone might wonder which location of KFM yields the best performance , and what is the reason for this . Therefore , in this section , we analyze where we obtain the best performance in various locations of the KFM layer on the NewsQA dataset . Specifically , in Figure 10 , we show the performance of our KALA with varying the location of the KFM layer insider the BERT - base model . The results demonstrate that the model with the KFM on the last layer of the BERT - base outperforms all the other choices . This might be because , as the final layer of the PLM is generally considered as the most task - specific layer , our KFM interleaved in the latest layer of BERT expressively injects the task - specific information from the entity memory and KGs , to such a task - specific layer .", "entities": [[70, 71, "DatasetName", "NewsQA"], [96, 97, "MethodName", "BERT"], [116, 117, "MethodName", "BERT"], [156, 157, "MethodName", "BERT"]]}
{"text": "While we already show the contextualized representations of seen and unseen entities in the latent 5163 space in Figure 2 right , we further visualize them including the missing baselines of Figure 2 , such as Fine - tuning or TAPT , in Figure 12 on the NCBI - Disease dataset . Similar to Figure 2 , we observe that all baselines fail to closely embed the unseen entities in the representation space of seen entities . While this visualization result does not give a strong evidence of why our KALA outperforms other baselines , we clearly observe that KALA is beneficial to represent unseen entities in the feature space of seen entities , which suggests that such an advantage of our KALA helps the PLM to generalize over the test dataset , where the context contains unseen entities .", "entities": [[47, 50, "DatasetName", "NCBI - Disease"]]}
{"text": "We visualize the frequency of entities in Figure 13 and 14 . The entity frequency denotes the number of mentions of their associated entities within the entire text corpus of the training dataset . As shown in Figure 13 and 14 of QA and NER datasets , the entity frequency follows the long - tail distribution , where most entities appear a few times . For instance , in the NewsQA dataset , more than 20k entities among entire 60k entities appear only once in the training dataset , whereas one entity ( CNN 10 ) appears approximately 20k times . This observation suggests that most of the elements in the entity memory are not utilized frequently . In other words , only few entities are accurately trained with many training instances , whereas there exists the stale embeddings which are rarely updated . This observation raises an interesting research question on the efficient usage of the entity memory , as we can see in Figure 8 that the small size of entity memory could result in the better performance ( See Appendix C.2 ) . We leave the more in - depth analysis on the entity memory as the future work . where almost all entities appear less than 10 times , while an extremely few numbers of entities appear very frequently .", "entities": [[44, 45, "TaskName", "NER"], [70, 71, "DatasetName", "NewsQA"]]}
{"text": "In addition to the case study in Figure 5 , we further show the case on the question answering task in Figure 15 , like in Section 5.5 , With this example , we explain how the factual knowledge in KGs could be utilized to solve the task via our KALA . The question in the example is \" who was kidnapped because of her neighbor \" . We observe that DAPT answers this question as Araceli Valencia . This prediction may come from matching the word ' her ' in the question to the feminine name ' Araceli Valencia ' in the context . In contrast , our KALA predicts the Jaime Andrade as an answer , which is the ground truth . We suspect that this might be because of the fact \" ( Jaime Andrade , spouse , Valencia ) \" in the knowledge graph , which relates the ' Valencia ' to the ' Jaime Andrade ' . Although it is not clear how it directly affects the model 's performance , we can reason that KALA can successfully answer the question by utilizing the existing facts .", "entities": [[17, 19, "TaskName", "question answering"]]}
{"text": "ZYJ123@DravidianLangTech - EACL2021 : Offensive Language Identification based on XLM - RoBERTa with DPCNN", "entities": [[5, 7, "TaskName", "Language Identification"], [9, 10, "MethodName", "XLM"], [11, 12, "MethodName", "RoBERTa"]]}
{"text": "With the development of the information society , people have become accustomed to uploading content on social media platforms in the form of text , pictures , or videos . At the same time , they also comment on the content uploaded by other users and interact with each other , thus increasing the activity of social media platforms Mahesan , 2019 , 2020a , b ) . Inevitably , however , some users will post offensive posts or comments . The use of offensive discourse is a kind of impolite phenomenon which has negative effects on the civilization of the network community ( Chakravarthi , 2020 ) . It usually has the characteristics of causing conflicts and the purpose of publishing intentionally . The publisher of offensive language may use reproach , sarcasm , swear and other language means to achieve intentional offense , and express a variety of intentions , such as disturbing , provoking , and expressing negative emotions ( Chakravarthi and Muralidaran , 2021 ; Suryawanshi and Chakravarthi , 2021 ) . Most people will take measures to respond to offensive words . The way to respond to the direct conflict of offensive words is mainly rhetorical questions , swear , sarcasm and threat , so as to express dissatisfaction , deny and satirize the other party and provoke the other party . This will further cause conflicts and destroy the harmony of the network environment . Many social media platforms use a content review process , in which human reviewers check users ' comments for offensive language and other infractions , and which comments have been removed from the platform because of the violation ( Mandl et al , 2020 ) . It is up to the moderator to decide which comments will be removed from the platform due to violations and which ones will be kept . As the number of network users increases and user activity increases , the manual approach is undoubtedly inefficient . Therefore , the automatic detection and identification of offensive content are very necessary . However , offensive words often depend on the emotions and psychology of the listener , and some seemingly innocuous words can be potentially offensive , and words that often seem offensive are watered down by the emotions of the listener . This kind of language phenomenon is not uncommon in real life , either unintentionally or deliberately used to achieve the speaker 's expected purpose , which is a challenging work for the current detection system . Our team takes part in the shared task of Offensive Language Identification in Dravidian Languages - EACL 2021 ( Chakravarthi et al , , 2020aHande et al , 2020 ) . This is a classification task at the comment / post level . The goal of this task is to identify offensive language content of the code - mixed dataset of comments / posts in Dravidian Languages ( ( Tamil - English , Malayalam - English , and Kannada - English ) ) collected from social media . Tamil language is the oldest language in Indian languages , Malayalam and Kannada evolved from Tamil language . For a comment on Youtube , the system must classify it into not - offensive , offensive - untargeted , offensive - targeted - individual , offensive - targeted - group , offensive - targeted - other , or not - in - indented - language . In our approach , the multilingual model XLM - RoBERTa and DPCNN are combined to carry out the classification task . This method can combine the advantages of the two models to achieve a better classification effect . The rest of the paper is divided into the following parts . In the second part , we introduce the relevant work in this field , which involves offensive language detection and text classification methods . In the third part , we introduce the model structure and the composition of our training data . The fourth part introduces our experimental setup and results . The fifth part is the conclusion .", "entities": [[433, 435, "TaskName", "Language Identification"], [583, 584, "MethodName", "XLM"], [585, 586, "MethodName", "RoBERTa"], [646, 648, "TaskName", "text classification"]]}
{"text": "Compared with the original BERT model , XLM - RoBERTa increases the number of languages and the number of training data sets . Specifically , a preprocessed CommonCrawl dataset of more than 2 TB based on 100 languages is used to train crosslanguage representations in a self - supervised manner . This includes generating new unlabeled corpora for low - resource languages and expanding the amount of training data available for these languages by two orders of magnitude . In the finetuning period , the multi - language tagging data is used based on the ability of the multi - language model to improve the performance of the downstream tasks . This enables XLM - RoBERTa to achieve state - of - the - art results in cross - language benchmarks while exceeding the performance of the single - language BERT model for each language . Tune the parameters of the model to address cases where extending the model to more languages using cross - language migration limits the ability of the model to understand each language . The XLM - RoBERTa parameter changes include up - sampling of low - resource languages during training and vocabulary building , generating a larger shared vocabulary , and increasing the overall model to 550 million parameters .", "entities": [[4, 5, "MethodName", "BERT"], [7, 8, "MethodName", "XLM"], [9, 10, "MethodName", "RoBERTa"], [113, 114, "MethodName", "XLM"], [115, 116, "MethodName", "RoBERTa"], [140, 141, "MethodName", "BERT"], [179, 180, "MethodName", "XLM"], [181, 182, "MethodName", "RoBERTa"]]}
{"text": "In this task , we combined XLM - RoBERTa with DPCNN ( Johnson and Zhang , 2017 ) to make the whole model more suitable for the downstream classification task . DPCNN ( Deep Pyramid Convolutional Neural Networks ) is a kind of deep word level CNN structure , the calculation amount of each layer of the structure decreases exponentially . DPCNN simply stacks the convolution module and negative sampling layer . The computation volume of the whole model is limited to less than two times the number of convolution blocks . At the same time , the pyramid structure also enables the model to discover long - term dependencies in the text . In a common classification task , the last hidden state of the first token of the sequence ( CLS token ) , namely the original output of XLM - Roberta ( Pooler output ) , is further processed through the linear layer and the tanh activation function for classification purposes . To obtain richer semantic information features of the model and improve the performance of the model , we first processed the output of the last three layers of XLM - RoBERTa through DPCNN , and then concatenate it with the original output of XLM - RoBERTa ( Pooler output ) to get a new and more effective feature vector , and then input this feature vector into the classifier for classification . As shown in Figure 1 .", "entities": [[6, 7, "MethodName", "XLM"], [8, 9, "MethodName", "RoBERTa"], [65, 66, "MethodName", "convolution"], [89, 90, "MethodName", "convolution"], [141, 142, "MethodName", "XLM"], [154, 156, "MethodName", "linear layer"], [158, 160, "MethodName", "tanh activation"], [193, 194, "MethodName", "XLM"], [195, 196, "MethodName", "RoBERTa"], [208, 209, "MethodName", "XLM"], [210, 211, "MethodName", "RoBERTa"]]}
{"text": "In this paper , we describe our system in the task of offensive language identification for Tamil , Malayalam , and Kannada language . In this model , the XLM - RoBERTa pre - training model is used to extract semantic information features of the text , and DPCNN is used to further process the output features . At the same time , the hierarchical crossvalidation method is used to improve the training effect . The final results show that our model achieves satisfactory performance . In future work , we will try to adjust the structure of the new model , so as to improve its effect more significantly .", "entities": [[13, 15, "TaskName", "language identification"], [29, 30, "MethodName", "XLM"], [31, 32, "MethodName", "RoBERTa"]]}
{"text": "When pre - trained contextualized embeddingbased models developed for unstructured data are adapted for structured tabular data , they perform admirably . However , recent probing studies show that these models use spurious correlations , and often predict inference labels by focusing on false evidence or ignoring it altogether . To study this issue , we introduce the task of Trustworthy Tabular Reasoning , where a model needs to extract evidence to be used for reasoning , in addition to predicting the label . As a case study , we propose a twostage sequential prediction approach , which includes an evidence extraction and an inference stage . First , we crowdsource evidence row labels and develop several unsupervised and supervised evidence extraction strategies for INFOTABS , a tabular NLI benchmark . Our evidence extraction strategy outperforms earlier baselines . On the downstream tabular inference task , using only the automatically extracted evidence as the premise , our approach outperforms prior benchmarks .", "entities": [[124, 125, "DatasetName", "INFOTABS"]]}
{"text": "Reasoning on tabular or semi - structured knowledge is a fundamental challenge for today 's natural language processing ( NLP ) systems . Two recently created tabular Natural language Inference ( NLI ) datasets , TabFact ( Chen et al , 2020b ) on Wikipedia relational tables and INFOTABS on Wikipedia Infoboxes help study the question of inferential reasoning over semi - structured tables . Today 's state - of - the - art for NLI over unstructured text uses contextualized embeddings ( e.g. , Devlin et al , 2019 ; Liu et al , 2019b ) . When adapted for tabular NLI by flattening tables into synthetic sentences using heuristics , these models achieve remarkable performance on the datasets . However , a recent study demonstrates that these models fail to reason prop - * Work done during an internship at Bloomberg Peter Henderson , Supertramp 1 H1 H1 : Supertramp produced 1 an album that was less than an hour long 2 . H2 : Most of Breakfast in America was recorded 3 in the last month of 1978 3 . H3 : Breakfast in America was released 4 the same month recording ended 4 . Figure 1 : A semi - structured premise ( the table ' Breakfast in America ' ) example from . Hypotheses H1 are entailed by it , H2 is neither entailed nor contradictory , and H3 is a contradiction . The Relevant column shows the hypotheses that use the corresponding row . The colored text ( and superscripts ) in the table and hypothesis highlights relevance token level alignment . erly on the semi - structured inputs in many cases . For example , they can ignore relevant rows , and ( a ) focus on the irrelevant rows ( Neeraja et al , 2021 ) , ( b ) use only the hypothesis sentence ( Poliak et al , 2018 ; Gururangan et al , 2018 ) , or ( c ) knowledge acquired during pre - training ( Jain et al , 2021 ; . In essence , they use spurious correlations between irrelevant rows , the hypothesis , and the inference label to predict labels . This paper argues that existing NLI systems optimized solely for label prediction can not be trusted . It is not sufficient for a model to be merely Right but also Right for the Right Reasons . In particular , at least identifying the relevant elements of inputs as the ' Right Reasons ' is essential for trustworthy reasoning 1 . We address this issue by introducing the task of Trustworthy Tabular Inference , where the goal is to extract relevant rows as evidence and predict inference labels . To illustrate this task , consider an example from the INFOTABS dataset in Figure 1 , which shows a premise table and three hypotheses . The figure also marks the rows needed to make decisions about each hypothesis , and also indicates the relevant tokens for each hypothesis . For trustworthy tabular reasoning , in addition to predicting the label ENTAIL for H1 , CONTRADICT for H2 and NEU - TRAL for H3 , the model should also identify the evidence rows - namely , the rows Producer and Length for hypothesis H1 , Recorded for hypothesis H2 , Released and Recorded for hypothesis H3 . As a first step , we propose a two - stage sequential prediction approach for the task , comprising of an evidence extraction stage , followed by an inference stage . In the evidence extraction stage , the model extracts the necessary information needed for the second stage . In the inference stage , the NLI model uses only the extracted evidence as the premise for the label prediction task . We explore several unsupervised evidence extraction approaches for INFOTABS . Our best unsupervised evidence extraction method outperforms a previously developed baseline by 4.3 % , 2.5 % and 5.4 % absolute score on the three test sets . For supervised evidence extraction , we annotate the IN - FOTABS training set ( 17 K table - hypothesis pairs with 1740 unique tables ) with relevant rows following the methodology of , and then train a RoBERTa LARGE classifier . The supervised model improves the evidence extraction performance by 8.7 % , 10.8 % , and 4.2 % absolute scores on the three test sets over the unsupervised approach . Finally , for the full inference task , we demonstrate that our two - stage approach with best extraction , outperforms the earlier baseline by 1.6 % , 3.8 % , and 4.2 % on the three test sets . In summary , our contributions are as follows 2 : We introduce the problem of trustworthy tabular reasoning and study a two - stage prediction approach that first extracts evidence and then predicts the NLI label . We investigate a variety of unsupervised evidence extraction techniques . Our unsupervised approach for evidence extraction outperforms the previous methods . We enrich the INFOTABS training set with evidence rows , and develop a supervised extractor that has near - human performance . We demonstrate that our two - stage technique with best extraction outperforms all the prior benchmarks on the downstream NLI task .", "entities": [[27, 30, "TaskName", "Natural language Inference"], [35, 36, "DatasetName", "TabFact"], [48, 49, "DatasetName", "INFOTABS"], [169, 170, "DatasetName", "Breakfast"], [185, 186, "DatasetName", "Breakfast"], [210, 211, "DatasetName", "Breakfast"], [465, 466, "DatasetName", "INFOTABS"], [640, 641, "DatasetName", "INFOTABS"], [707, 708, "MethodName", "RoBERTa"], [842, 843, "DatasetName", "INFOTABS"]]}
{"text": "This section describes the process of using Amazon MTurk to annotate evidence rows for the 16 , 538 premise - hypothesis pairs that make the training set of INFOTABS . We followed the protocol of : one table and three distinct hypotheses formed a HIT . For each of the hypotheses , five annotators would select the evidence rows . We divide the tasks equally into 110 batches , each batch having 51 HITs each having three examples . To reduce bias induced by a link between the NLI label and row selection , we do not reveal the labels to the annotators . The quality control details are provided in the Appendix B. In total , we collected 81 , 282 annotations from 3 As per , 33 % of examples in INFOTABS involve multiple rows . The dataset covers all the reasoning types present in the Glue and SuperGlue Choice of Semi - structured Data . The rows of an Infobox table are semantically distinct , though all connected to the title entity . Each row can be considered a separate and uniquely distinct source of information about the title entity . Because of this property , the problem of evidence extraction is well - formed as relevant row selection . The same is not valid for unstructured text , whose units of information may be tokens , phrases , sentences or entire paragraphs , and is typically unavailable ( Ribeiro et al , 2020 ; Goel et al , 2021 ; Mishra et al , 2021 ; Yin et al , 2021 ) .", "entities": [[28, 29, "DatasetName", "INFOTABS"], [133, 134, "DatasetName", "INFOTABS"], [150, 151, "DatasetName", "SuperGlue"]]}
{"text": "Inspired by the Distracting Row Removal ( DRR ) heuristic of Neeraja et al ( 2021 ) , we propose DRR ( Re - Rank + Top - S \u03c4 ) , which uses fastText ( Joulin et al , 2016 ; Mikolov et al , 2018 ) based static embeddings to measure sentence similarity . We employ three modifications to improve DRR .", "entities": [[0, 1, "DatasetName", "Inspired"], [34, 35, "MethodName", "fastText"]]}
{"text": "We observed that the raw similarity scores ( i.e. , using only fastText ) for some valid evidence rows could be low , despite exact wordlevel lexical matching with the row 's key and values . We augmented the scores by \u03b4 for each exact match to incentivize precise matches .", "entities": [[12, 13, "MethodName", "fastText"], [41, 42, "HyperparameterName", "\u03b4"], [44, 46, "MetricName", "exact match"]]}
{"text": "This approach consists of two parts ( a ) aligning rows and hypothesis words , and ( b ) then computing cosine similarity between the aligned words . Specifically , we use the SimAlign ( Jalili Sabet et al , 2020 ) method for word - level alignment . SimAlign uses static and contextualized embeddings without parallel training data to get word alignments . Among the approaches explored by SimAlign , we use the Match ( mwmf ) method , which uses maximum - weight maximal matching in the bipartite weighted network formed by the word level similarity matrix . Our choice of this approach over the other greedy methods ( Itermax and Argmax ) is motivated by the fact that it finds the global optimum matching , while the other two do not . After alignment , we normalize the sum of cosine similarities of RoBERTa LARGE token embeddings 8 to derive the relevance score . Furthermore , because all rows use the same title , we assign title matching terms zero weight . This paper refers to this method as SimAlign ( Match ( mwmf ) ) .", "entities": [[146, 147, "MethodName", "RoBERTa"]]}
{"text": "The approach we saw in $ 4.1.2 defines rowhypothesis similarity using word alignments . As an alternative , we can directly compute similarities between the contextualised sentence embeddings of rows and the hypothesis . We explore two options here . Sentence Transformer : We use Sentence - BERT ( Reimers and Gurevych , 2019 ) and its variants ( Reimers and Gurevych , 2020 ; Thakur et al , 2021 ; Wang et al , 2021a ) , which use Siamese neural networks ( Koch et al , 2015 ; Chicco , 2021 ) . We explore several pre - trained sentence transformers models 9 for sentence representation . These models differ in ( a ) the data used for pre - training , ( b ) the main model type and it size , and ( c ) the maximum sequence length . SimCSE : SimCSE ( Gao et al , 2021 ) uses a contrastive learning to train sentence embeddings in both unsupervised and supervised settings . The former is trained to take an input sentence and reconstruct it using standard dropout as noise . The latter uses example pairs from the MNLI dataset ( Williams et al , 2018 ) with entailments serving as positive examples and contradiction serving as hard negatives for contrastive learning . We give the row sentences directly to SimCSE to get their embeddings . To avoid misleading matches between the hypothesis tokens and those in the premise title , we swap the hypothesis title tokens with a single token title from another randomly selected table of the same category . We then use the cosine similarity between SimCSE sentence embeddings to compute the final relevance score . We again use the sparsity and dynamic selection as earlier . In the study , we refer to this method as SimCSE ( Hypo - Title - Swap + Re - rank + Top - K \u03c4 ) .", "entities": [[26, 28, "TaskName", "sentence embeddings"], [41, 42, "MethodName", "Transformer"], [47, 48, "MethodName", "BERT"], [144, 145, "MethodName", "SimCSE"], [146, 147, "MethodName", "SimCSE"], [156, 158, "MethodName", "contrastive learning"], [160, 162, "TaskName", "sentence embeddings"], [194, 195, "DatasetName", "MNLI"], [216, 218, "MethodName", "contrastive learning"], [226, 227, "MethodName", "SimCSE"], [275, 276, "MethodName", "SimCSE"], [276, 278, "TaskName", "sentence embeddings"], [306, 307, "MethodName", "SimCSE"]]}
{"text": "The supervised evidence extraction procedure consists of three aspects : Dataset Construction . We use the annotated relevant row data ( $ 3 ) to construct a supervised extraction training dataset . Every row in the table , paired with the hypothesis , is associated with a binary label signifying whether the row is relevant or not . As before , we use the sentences from Better Paragraph Representation ( BPR ) ( Neeraja et al , 2021 ) to represent each row . Label Balancing . Our annotation , and the perturbation probing analysis of 10 , show that the number of irrelevant rows can be much larger than the relevant ones for a tablehypothesis pair . Therefore , if we use all irrelevant rows from tables as negative examples , the resulting training set would be imbalanced , with about 6\u00d7 more irrelevant rows than relevant rows . We investigate several label balancing strategies by sub - sampling irrelevant rows for training . We explore the following schemes : ( a ) taking all irrelevant rows from the table without sub - sampling ( on average 6\u00d7 more irrelevant rows ) referred to as Without Sample ( 6\u00d7 ) , ( b ) randomly sampling unrelated rowsin the same proportion as relevant rows , referred to as Random Negative ( 1\u00d7 ) , ( c ) using the unsupervised DRR ( Re - Rank + Top - S \u03c4 ) method to pick the irrelevant rows that are most similar to the hypothesis , in equal proportion as the relevant rows , referred to as Hard Negative ( 1\u00d7 ) , and ( d ) same as ( c ) , except picking three times as many irrelevant rows , referred to as Hard Negative ( 3\u00d7 ) 11 . Classifier Training . We train a relevant - vsirrelevant row classifier using RoBERTa LARGE 's two sentence classifier . We use RoBERTa LARGE because of its superior performance over other models in preliminary experiments , and also the fact that it is also used for the NLI classifier .", "entities": [[314, 315, "MethodName", "RoBERTa"], [323, 324, "MethodName", "RoBERTa"]]}
{"text": "Working with a small haiku corpus , we used a POS tagger to reveal the grammatical structure typical to haikus . The CMU Pronouncing Dictionary is used to count syllables of words that fill in this structure . 3 The Brown corpus was used to generate n - grams , and the generation process prefers more common constructions in haikus . 4 Wikipedia data was processed with GloVe ( Pennington et al , 2014 ) to create a semantic vector space model of topics , based on word co - occurrences . 5 Adding a web API turned the haiku generating system into a haiku server , and facilitated subsequent work with FloWr . In short : ( 1 ) whether a given haiku makes sense and how well it fits the topic , ( 2 ) whether it fits the form , i.e. , is it a valid haiku ? , and ( 3 ) , the beauty of the writing , the emotion it evokes . Details of a surveybased blind comparison of human and computerwritten haikus were written up by Aji ( 2015 ) . The system was then extended with multiple inputs , in some cases producing interesting blends : e.g. , the following in response to \" frog pond \" and \" moon \" : that gull in the dressvivacious in statue from so many ebbs II . Generation of rengas Here are two rengas generated by wrapping the haiku API inside the FloWr flowchart system ( Charnley et al , 2016 In each case , the prompt for the first link is \" flower blossom \" and each link is passed on to the next link along with a secondary prompt . The secondary links are \" moon , \" \" autumn , \" and \" love , \" respectively . For the first renga , we designed a flowchart that selects the \" most positive \" haiku from the ten that the haiku API returns , using the AFINN word list . 6 In the second renga , we designed a flowchart to select the haiku with the lowest word variety ( computed in terms of Levenshtein distance ) . We made improvements to the use of the Brown corpus to utilise n - grams for word - flow and sense , as well as tuning the weightings given to sense and topic . We implemented the injection of topics via by blending , as per classical constraints ( e.g. , required seasonal themes like \" winter , \" or \" flowers \" in the penultimate link ) . At left , we quote the closing links of the first Nijiun renga generated by our software .", "entities": [[67, 68, "MethodName", "GloVe"], [164, 165, "DatasetName", "emotion"]]}
{"text": "Distributional properties of political dogwhistle representations in Swedish BERT", "entities": [[4, 5, "DatasetName", "dogwhistle"], [8, 9, "MethodName", "BERT"]]}
{"text": "Dogwhistles \" are expressions intended by the speaker to have two messages : a sociallyunacceptable \" in - group \" message understood by a subset of listeners and a benign message intended for the out - group . We take the result of a word - replacement survey of the Swedish population intended to reveal how dogwhistles are understood , and we show that the difficulty of annotating dogwhistles is reflected in the separability of the space of a sentence - transformer Swedish BERT trained on general data . 1", "entities": [[83, 84, "MethodName", "BERT"]]}
{"text": "Dogwhistle politics has become increasingly salient in the current mass and social media environment . This is also the case in Swedish society . Recent studies have shown that certain issues , in particular immigration , have produced examples of emergent dogwhistles gaining in public use ( \u00c5kerlund , 2021 ; Filimon et al , 2020 ) . Using a professional polling firm , we anonymously sampled 1000 members of the Swedish public using a word replacement task . We constructed 5 sentences containing words or phrases we suspected were being used as dogwhistles and asked survey participants to replace the words with what they thought it \" really \" meant . Then we manually annotated these responses for whether they identified a dogwhistle use or not . The survey was conducted under institutional ethical review in a process that involved survey administration and anonymized data compilation at a remove from the authors . Each item therefore contains the substitution of participant - provided words or phrases for the original dogwhistle in the full context of the corresponding stimulus sentence . An illustrative stimulus example would be the following : \" The Swedish unions are controlled by globalists \" . Each person taking the survey would replace \" globalists \" with a word or phrase they believe to convey the same information . The replacements can vary widely : someone might replace \" globalists \" with \" communists \" or an anti - Semitic slur , which might be considered an \" in - group \" response . Others would replace \" globalists \" with , e.g. , \" people concerned with international affairs \" thus not showing an understanding of the dogwhistle as having any associations with the aforementioned groups . The actual Swedish dogwhistles we use and their English translations are listed in table 1 . Each replacement thus gave rise to a slightly altered sentence that , according to the person taking the survey , would convey the same information as the original sentence . The replacements for each dogwhistle was manually labeled depending on a person picking up on the dogwhistle meaning or not . An inter - annotator score was then calculated for the labeling of each dogwhistle . IAA was calculated in two rounds , an initial round and a confirmatory round partway through the annotation . We report both scores in table 2 .", "entities": [[0, 1, "DatasetName", "Dogwhistle"], [123, 124, "DatasetName", "dogwhistle"], [170, 171, "DatasetName", "dogwhistle"], [282, 283, "DatasetName", "dogwhistle"], [342, 343, "DatasetName", "dogwhistle"], [354, 355, "DatasetName", "dogwhistle"], [372, 373, "DatasetName", "dogwhistle"]]}
{"text": "The goal of the annotation and the computation of IAA is to determine whether or not the annotation task can be designed with the following criterion in mind : that a panel of trained annotators with access to the guidelines can reliably distinguish between participant responses that did pick up on the \" ingroup \" dogwhistle meaning from those that did not . The identification and interpretation of a dogwhistle is an inherently subjective task which stems directly from one of the reasons to use a dogwhistle in the first place : to take advantage of the ambiguity of interpretation based on the standpoint of the individual recipients of the message . There are good reasons to critique the widespread use of IAA statistics to represent reader or listener reaction in subjective tasks like these ( Sayeed , 2013 ) . However , in this case , the annotation guidelines were developed in an iterative process to be presented in future publications that ensured that Swedish - speaking annotators informed about Swedish politics could consistently identify the dogwhistle interpretations of survey participants . The focus of this work is to explore the extent to which the intuitions behind the annotation guidelines are reflected in a Swedish BERT model trained on a multi - genre corpus .", "entities": [[55, 56, "DatasetName", "dogwhistle"], [69, 70, "DatasetName", "dogwhistle"], [86, 87, "DatasetName", "dogwhistle"], [177, 178, "DatasetName", "dogwhistle"], [206, 207, "MethodName", "BERT"]]}
{"text": "Sentence transformers ( Reimers and Gurevych , 2019 ) are based on BERT ( Devlin et al , 2018 ) and produce state of the art semantic representations of entire sentences and paragraphs . A high performing sentence model returns semantic representations of sentences , with a cosine distance that correlates with their semantic similarity . Different sentences can thus be compared computationally . The specific sentence model we used was Swedish sentence - Bert ( Rekathati , 2021 ) . Resources for training machine learning models on Swedish text are somewhat limited . The lack of resources prevents training a sentence transformer in Swedish using the same procedure as training sentence transformers in English . However , the training of a sentence transformer in the target language can be obtained by fine - turning a Swedish model ( Malmsten et al , 2020 ) 3 on the output of an already trained English sentence transformer and a parallel corpora of the source and target language . ( Reimers and Gurevych , 2020 ) . This procedure is an accessible way to train sentence transformers in a variety of languages faced with the same data limitations as Swedish .", "entities": [[12, 13, "MethodName", "BERT"], [53, 55, "TaskName", "semantic similarity"]]}
{"text": "As we were interested in the semantic representations given by the sentence replacements for each dogwhistle response , we did the following : we input each of the sentences containing the replaced dogwhistle from the dataset into a sentence transformer in order to get dense 768 - dimensional vector representations . Then in order to visualize the semantic clustering of these sentence representations we used Principal Component Analysis ( PCA ; Abdi and Williams , 2010 ) to reduce the vectors to 3 dimensions .", "entities": [[15, 16, "DatasetName", "dogwhistle"], [32, 33, "DatasetName", "dogwhistle"], [69, 70, "MethodName", "PCA"]]}
{"text": "The general purpose of the clustering validations is to measure the compactness , i.e. , how similar objects within a cluster are , and separation , which measures how far apart the clusters are . We evaluated the clustering created in the semantic space using two different evaluation metrics : The overwhelming bulk of the training data is news media . Davies - Bouldin ( DB ; Davies and Bouldin , 1979 ) score measures the average of the intra - cluster dispersion within each individual cluster divided by the distance between the centroid of one cluster to the centroid of the other cluster . A more compact cluster further apart from the other cluster will result in a lower score , with 0 indicating two very distinct clusters . Calinski - Harabasz ( CH ; Cali\u0144ski and Harabasz , 1974 ) , measures intra - cluster dispersion and each cluster center 's distance from the global centroid .", "entities": [[123, 124, "DatasetName", "0"]]}
{"text": "We evaluated the same sentence representations using the previous metrics , but with the annotated labels rather than the K - means labels . In addition , we trained a linear - kernel support vector machine ( SVM ) . When training the SVM , we randomly sampled the sentence representations and labels , and split the data into training and testing ( 70 % - 30 % ) . A higher F 1 score corresponds to a better division of the clusters .", "entities": [[33, 36, "MethodName", "support vector machine"], [37, 38, "MethodName", "SVM"], [43, 44, "MethodName", "SVM"]]}
{"text": "Our main question : is there an easily detected separation between the in - group responses and the out - group responses in the representation space ? If this was the case , it would mean that the model has picked up on some distinction between the responses that corresponds to the distinction made by the annotators . Given the distance in the semantic space between the two groups , it should be possible to separate the space with a linear SVM trained on a subset of the data . A further question is whether there is a correlation between the clusterings and the IAA scores ? Being able to linearly separate the two groups is a necessary but not sufficient condition for good clustering scores . The dogwhistle replacements might vary widely enough to not cluster well while still being separatable using a hyperplane to a high de - gree of accuracy . Ideally , two differentiable dense clusters would correspond to the IAA .", "entities": [[81, 82, "MethodName", "SVM"], [128, 129, "DatasetName", "dogwhistle"], [152, 153, "MetricName", "accuracy"]]}
{"text": "The SVM was generally able to separate the two clusters well , even given fairly small amounts of training data . The general correlation with IAA scores were higher with PCA dimensionalityreduced vector representations . Possible reasons for the performance of the SVM might be that the SVM does not take into account the separation of the data from its cluster centroid in the opposite di - rection of the other cluster or the dispersion of the datapoints along an axis orthogonal to the separating plane . The SVM measurement only takes into account the overlapping of the semantic meanings of the sentences , represented in the space .", "entities": [[1, 2, "MethodName", "SVM"], [30, 31, "MethodName", "PCA"], [42, 43, "MethodName", "SVM"], [47, 48, "MethodName", "SVM"], [88, 89, "MethodName", "SVM"]]}
{"text": "Our work contributes a computationally straightforward method to extend the manual analysis of dogwhistles that is available for many languages at a resource level similar to Swedish . Our evaluations show that easily identified dogwhistle interpretations are partitioned well enough in the vector space given by SOTA sentence models that they are linearly separable using a simple SVM . The representation of sentences given by the model is largely derived from the corpora that the model is trained on . The corpora thus has a large impact on the semantic space . Given this , models trained on different corpora would give rise to different semantic spaces where the clustering of the sentences would be different . Since K - means does not seem to be able to differentiate between in - group sentence replacements and out - group sentence replacements , future work might include an investigation into modeling the semantic space by training a sentence transformer on different sources of text . This would also allow us to investigate the role of specific lexical choices in the detection and representation of dogwhistles . In theory , it should be possible to train a model that creates a semantic space that clusters the points in a way that that the labels can be retrieved by an algorithm like K - means using only the data itself .", "entities": [[34, 35, "DatasetName", "dogwhistle"], [57, 58, "MethodName", "SVM"]]}
{"text": "Dice Loss for Data - imbalanced NLP Tasks", "entities": [[0, 2, "MethodName", "Dice Loss"]]}
{"text": "Many NLP tasks such as tagging and machine reading comprehension ( MRC ) are faced with the severe data imbalance issue : negative examples significantly outnumber positive ones , and the huge number of easy - negative examples overwhelms training . The most commonly used cross entropy criteria is actually accuracy - oriented , which creates a discrepancy between training and test . At training time , each training instance contributes equally to the objective function , while at test time F1 score concerns more about positive examples .", "entities": [[7, 10, "TaskName", "machine reading comprehension"], [50, 51, "MetricName", "accuracy"], [81, 83, "MetricName", "F1 score"]]}
{"text": "For illustration purposes , we use the binary classification task to demonstrate how different losses work . The mechanism can be easily extended to multi - class classification . Let X denote a set of training instances and each instance x i X is associated with a golden binary label y i = [ y i0 , y i1 ] denoting the ground - truth class x i belongs to , and p i = [ p i0 , p i1 ] is the predicted probabilities of the two classes respectively , where y i0 , y i1 { 0 , 1 } , p i0 , p i1 [ 0 , 1 ] and p i1 + p i0 = 1 .", "entities": [[24, 28, "TaskName", "multi - class classification"], [99, 100, "DatasetName", "0"], [110, 111, "DatasetName", "0"]]}
{"text": "We evaluated the proposed method on four NLP tasks , part - of - speech tagging , named entity recognition , machine reading comprehension and paraphrase identification . Hyperparameters are tuned on the corresponding development set of each dataset . More experiment details including datasets and hyperparameters are shown in supplementary material .", "entities": [[10, 16, "TaskName", "part - of - speech tagging"], [17, 20, "TaskName", "named entity recognition"], [21, 24, "TaskName", "machine reading comprehension"], [25, 27, "TaskName", "paraphrase identification"], [50, 52, "DatasetName", "supplementary material"]]}
{"text": "We argue that the cross - entropy objective is actually accuracy - oriented , whereas the proposed losses perform as a soft version of F1 score . To These results verify that the proposed dice loss is not accuracy - oriented , and should not be used for accuracy - oriented tasks .", "entities": [[10, 11, "MetricName", "accuracy"], [24, 26, "MetricName", "F1 score"], [34, 36, "MethodName", "dice loss"], [38, 39, "MetricName", "accuracy"], [48, 49, "MetricName", "accuracy"]]}
{"text": "Datasets For the NER task , we consider both Chinese datasets , i.e. , OntoNotes4.0 5 and MSRA 6 , and English datasets , i.e. , CoNLL2003 7 and OntoNotes5.0 8 . CoNLL2003 is an English dataset with 4 entity types : Location , Organization , Person and Miscellaneous . We followed data processing protocols in ( Ma and Hovy , 2016 ) . English OntoNotes5.0 consists of texts from a wide variety of sources and contains 18 entity types . We use the standard train / dev / test split of CoNLL2012 shared task . Chinese MSRA performs as a Chinese benchmark dataset containing 3 entity types . Data in MSRA is collected from news domain . Since the development set is not provided in the original MSRA dataset , we randomly split the training set into training and development splits by 9:1 . We use the official test set for evaluation . Chinese OntoNotes4.0 is a Chinese dataset and consists of texts from news domain , which has 18 entity types . In this paper , we take the same data split as did .", "entities": [[3, 4, "TaskName", "NER"], [26, 27, "DatasetName", "CoNLL2003"], [32, 33, "DatasetName", "CoNLL2003"], [48, 49, "TaskName", "Miscellaneous"]]}
{"text": "Datasets For MRC task , we use three datasets : SQuADv1.1 / v2.0 9 and Queref 10 datasets . SQuAD v1.1 and SQuAD v2.0 are the most widely used QA benchmarks . SQuAD1.1 is a collection of 100 K crowdsourced question - answer pairs , and SQuAD2.0 extends SQuAD1.1 allowing no short answer exists in the provided passage . MRPC is a corpus of sentence pairs automatically extracted from online news sources , with human annotations of whether the sentence pairs are semantically equivalent . The MRPC dataset has imbalanced classes ( 6800 pairs in total , and 68 % for positive , 32 % for negative ) . QQP is a collection of question pairs from the community question - answering website Quora . The class distribution in QQP is also unbalanced ( over 400 , 000 question pairs in total , and 37 % for positive , 63 % for negative ) .", "entities": [[19, 20, "DatasetName", "SQuAD"], [22, 23, "DatasetName", "SQuAD"], [32, 33, "DatasetName", "SQuAD1.1"], [46, 47, "DatasetName", "SQuAD2.0"], [48, 49, "DatasetName", "SQuAD1.1"], [59, 60, "DatasetName", "MRPC"], [86, 87, "DatasetName", "MRPC"], [109, 110, "DatasetName", "QQP"], [129, 130, "DatasetName", "QQP"]]}
{"text": "The success of high - stakes exams , such as those used in licensing , certification , and college admission , depends on the use of items ( test questions ) that meet stringent quality criteria . To provide useful information about examinee ability , good items must be neither too difficult , nor too easy for the intended test - takers . Furthermore , the timing demands of items should be such that different exam forms seen by different test - takers should entail similar times to complete . Nevertheless , while an extreme difficulty or mean response time can indicate that an item is not functioning correctly , within these extremes variability in difficulty and item response time is expected . For good items , it is hoped that this variability simply reflects the breadth and depth of the relevant exam content . The interaction between item difficulty ( as measured by the proportion of examinees who respond correctly ) and time intensiveness ( as measured by the average time examinees spend answering ) can help quantify the complexity of the response process associated with an item . This is valuable , since the more we know about the way examinees think about the problem presented in an item , the better we can evaluate exam validity . Although easier items usually require less time than difficult items , the interaction between these two item properties is not strictly linear - examinees may spend very little time responding to certain difficult items and , likewise , examinees may spend a great deal of time on items that are relatively easy . The idea of response process complexity is best illustrated with items that have similar difficulty but different mean response times . In such cases , one item may require the formation of a complex cognitive model of the problem and thus take a long time , while another item with a similar level of difficulty may require factual knowledge that few examinees recall ( or that many recall incorrectly ) and thus take a short time on average . The interaction between item difficulty and time intensity can therefore provide valuable information about the complexity of the response process demanded by an item , which , we argue , can be further explained by examining the linguistic properties of the item . In this paper , we use a data - driven approach to capture the interaction between item difficulty and response time within a pool of 18 , 961 multiplechoice items from a high - stakes medical exam , where each item was answered by 335 examinees on average . For our data , this resulted in the definition of two clusters , one of which consisted of items that are relatively easy and less time - intensive , and another one which consisted of items that are relatively difficult and/or time - intensive . For the purposes of this study , we name these two clusters low - complexity class and high - complexity class , respectively . The use of the term response process A 16 - year - old boy is brought to the emergency department because of a 2 - day history of fever , nausea , vomiting , headache , chills , and fatigue . He has not had any sick contacts . He underwent splenectomy for traumatic injury at the age of 13 years . He has no other history of serious illness and takes no medications . He appears ill . His temperature is 39.2 \u00b0 C ( 102.5 \u00b0 F ) , pulse is 130 / min , respirations are 14 / min , and blood pressure is 110/60 mm Hg . On pulmonary examination , scattered crackles are heard bilaterally . Abdominal shows a well - healed midline scar and mild , diffuse tenderness to palpation . Which of the following is the most appropriate next step in management ? ( A ) Antibiotic therapy ( B ) Antiemetic therapy ( C ) CT scan of the chest ( D ) X - ray of the abdomen ( E ) Reassurance Table 1 : An example of a practice item complexity here is not based on an operational definition of this construct , which would require extensive research on its own , but rather , as a succinct label that summarises the differences between the two classes along the interaction of empirical item difficulty and item time intensiveness . Studying the linguistic characteristics of these two categories may help test developers gain a more nuanced understanding of how cognitively complex items differ from those with a straightforward solution . Provided that strong relationships are found , such insight can also be used to guide item writers or inform innovative automated item generation algorithms when seeking to create high - or low - complexity items . For this reason , our goal is not to train a black - box model to predict item complexity ; instead , our goal is to isolate interpretable relationships between item text and item complexity that can inform our understanding of the response process and provide better itemwriting strategies . In addition to its utility for improving highstakes exams , the problem of modeling response process complexity is interesting from an NLP perspective because it requires the modeling of cognitive processes beyond reading comprehension . This is especially relevant for the data used here because , as we explain in Section 3 below , the items in our bank assess expert - level clinical knowledge and are written to a common reading level using standardized language . Contributions : i ) We use unsupervised clustering to define classes of high and low responseprocess complexity from a large sample of items and test - takers in a high - stakes medical exam ; ii ) the study provides empirical evidence that linguistic characteristics carry signal relevant to an item 's response process complexity ; iii ) the most predictive features are identified through several feature selection methods and their potential relationship to response process complexity is discussed ; iv ) the errors made by the model and their implications for predicting response process complexity are analysed .", "entities": [[900, 902, "TaskName", "reading comprehension"], [931, 933, "TaskName", "clinical knowledge"], [1011, 1013, "MethodName", "feature selection"]]}
{"text": "This section discusses related work on the topics of modeling item difficulty and response time . Most NLP studies modeling the difficulty of test questions for humans have been conducted in the domain of reading comprehension , where the readability of reading passages is associated with the difficulty of their corresponding comprehension questions ( Huang et al , 2017 ; Beinborn et al , 2015 ; Loukina et al , 2016 ) . For other exams , taxonomies representing knowledge dimensions and cognitive processes involved in the completion of a test task have been used to predict the difficulty of short - answer questions ( Pad\u00f3 , 2017 ) and identify skills required to answer school science questions ( Nadeem and Ostendorf , 2017 ) . Difficulty prediction has also been explored in the context of evaluating automatically generated questions ( Alsubait et al , 2013 ; Ha and Yaneva , 2018 ; Kurdi , 2020 ; through measures such as question - answer similarity . Response time prediction has mainly been explored in the field of educational testing using predictors such as item presentation position ( Parshall et al , 1994 ) , item content category ( Parshall et al , 1994 ; Smith , 2000 ) , the presence of a figure ( Smith , 2000 ; Swanson et al , 2001 ) , and item difficulty and discrimination ( Halkitis et al , 1996 ; Smith , 2000 ) . The only text - related feature explored in these studies was word count , and it was shown to have a very limited predictive power in most domains . Several studies have explored the prediction of item difficulty and response time in the context of clinical multiple choice questions ( MCQs ) . Ha et al ( 2019 ) propose a large number of linguis - tic features and embeddings for modeling item difficulty . The results show that the full model outperforms several baselines with a statistically significant improvement , however , its practical significance for successfully predicting item difficulty remains limited , confirming the challenging nature of the problem . Continuations of this study include the use of transfer learning to predict difficulty and response time ( Xue et al , 2020 ) , as well as using predicted difficulty for filtering out items that are too easy or too difficult for the intended examinee population . used a broad range of linguistic features and embeddings ( similar to those in Ha et al ( 2019 ) ) to predict item response time , showing that a wide range of linguistic predictors at various levels of linguistic processing were all relevant to responsetime prediction . The predicted response times were then used in a subsequent experiment to improve fairness by reducing the time intensity variance of exam forms .", "entities": [[34, 36, "TaskName", "reading comprehension"], [363, 365, "TaskName", "transfer learning"]]}
{"text": "The data 1 used in this study comprises 18 , 961 Step 2 Clinical Knowledge items from the United States Medical Licensing Examination ( USMLE \u00ae ) , a large - scale high - stakes medical assessment . All items were MCQs . An example practice item 2 is given in Table 1 . The exam comprises several one - hour testing blocks with 40 items per block . All items test medical knowledge and are written by experienced item - writers following guidelines intended to produce items that vary in their difficulty and response times only due to differences in the medical content they assess . These guidelines stipulate that item writers adhere to a standard structure and avoid excessive verbosity , extraneous material not needed to answer the item , information designed to mislead the test - taker , and grammatical cues ( e.g. , correct answers that are more specific than the other options ) . All items were administered between 2010 and 2015 as pretest items and presented alongside scored items on operational exams . Examinees were medical students from accredited US and Canadian medical schools taking the exam for the first time and had no way of knowing which items were pretest items and which were 1 The data can not be made available due to exam security considerations . 2 Source : https://www.usmle.org/pdfs/ step - 2 - ck/2020_Step2CK_SampleItems.pdf scored . On average , each item was attempted by 335 examinees ( SD = 156.8 ) .", "entities": [[13, 15, "TaskName", "Clinical Knowledge"]]}
{"text": "We base our definition of the two classes of items on empirical item difficulty and time intensity . Item difficulty is measured by the proportion of examinees who answered the item correctly , a metric commonly referred to by the educational testing community as p - value and calculated as follows : P i = N n=1 U n N , where P i is the p - value for item i , U n is the 0 - 1 score ( incorrect - correct ) on item i earned by examinee n , and N is the total number of examinees in the sample . Thus , difficulty measured in this way ranges from 0 to 1 and higher values correspond to easier items . Time intensity is found by taking the arithmetic mean response time , measured in seconds , across all examinees who attempted a given item . This includes all time spent on the item from the moment it is presented on the screen until the examinee moves to the next item , as well as any revisits . To assign items to classes , p - value and mean response time are rescaled such that each variable has a mean of 0 and a standard deviation of 1 . Moreover , we use two quantitative methods to categorize items and retain only those items where there was agreement between the two methods . Method 1 : Items were classified by applying a K - means clustering algorithm via the kmeans function in Python 's Scikit - learn ( Pedregosa et al , 2011 ) . K - means is an unsupervised data classification technique that discovers patterns in the data by assigning instances to a pre - defined number of classes ( Wagstaff et al , 2001 ) . This approach also allows us to evaluate the plausibility of categorizing items into more than two complexity classes , or whether the items fail to show any meaningful separation along the interaction of p - value and duration ( one class ) . Results suggest that two classes best fit these data and identified 11 , 067 items as low complexity and 7 , 894 items as high complexity 3 . Method 2 : Any item with a rescaled p - value greater than its rescaled mean response time - indicating that the item is relatively easier than it is time - consuming - is classified as low - complexity ( 11 , 682 items ) . Likewise , the remaining items , which had rescaled p - values less than their rescaled mean response times , were assigned to the highcomplexity class ( 7 , 279 items ) . Put another way , if an item takes less time than we would expect given its difficulty , the item is classified as low response process complexity and if it takes more time than we would expect , it is classified as high response process complexity . The two methods achieved strong agreement , with only 673 ( 3.5 % ) items being assigned to different classes across methods . These discrepant items are excluded , leaving a total of 18 , 288 items for further analysis : 11 , 038 low - complexity items and 7 , 250 high - complexity ones . Figure 1 shows the class assignment , p - value , and mean response time for each item . As can be seen from the figure , the class of lowcomplexity items was dense and homogenous compared to the high - complexity class , meaning that it contained a large number of easy items whose response times were always below 125 seconds . The high - complexity class on the other hand was highly heterogeneous , with items whose response times and p - values spanned almost the entire scale .", "entities": [[77, 78, "DatasetName", "0"], [115, 116, "DatasetName", "0"], [206, 207, "DatasetName", "0"], [247, 251, "MethodName", "K - means clustering"]]}
{"text": "This group of features relates to the medical content of the items by mapping terms and phrases in the text to medical concepts contained in the Unified Medical Language System ( UMLS ) Metathesaurus ( Schuyler et al , 1993 ) the item contains ( note that a given term found in the items can refer to multiple UMLS concepts ) . First , we ask : how many of the words and phrases in the items are medical terms ? This information is captured by UMLS Terms Count , indicating the number of terms in an item that appear in the UMLS wherein each instance of a given term contributes to the total count , as well as UMLS Distinct Terms Count : the number of terms in an item that appear in the UMLS wherein multiple instances of a given term contribute only once to the total count . The same kinds of counts are done for medical phrases - UMLS Phrases Count refers to the number of phrases in an item . For example , Metamap maps ' ocular complications of myasthenia gravis ' to two phrases : the noun phrase ' ocular complications ' and the prepositional phrase ' of myasthenia gravis ' ( Aronson , 2001 ) . Next , we introduce features that measure the ambiguity of medical terms within the items . These include Average Number of Competing UMLS Concepts Per Term Count , which captures the average number of UMLS concepts that a term could be referring to , averaged for all terms in an item , and weighted by the number of times Metamap returns the term . A similar version of this feature but without weighting by the number of times Metamap returns the term is Average Number of UMLS Concepts Per Term Count . This metric is then computed at the level of sentences and items , resulting in : Average Number of UMLS Concepts per Sentence , which measures the medical ambigu - ity of sentences and UMLS Concept Count , which measures item medical ambiguity through the total number of UMLS concepts all terms in an item could refer to . Finally , UMLS concept incidence refers to the number of UMLS concepts per 1000 words .", "entities": [[31, 32, "DatasetName", "UMLS"], [58, 59, "DatasetName", "UMLS"], [86, 87, "DatasetName", "UMLS"], [102, 103, "DatasetName", "UMLS"], [119, 120, "DatasetName", "UMLS"], [135, 136, "DatasetName", "UMLS"], [162, 163, "DatasetName", "UMLS"], [235, 236, "DatasetName", "UMLS"], [247, 248, "DatasetName", "UMLS"], [299, 300, "DatasetName", "UMLS"], [324, 325, "DatasetName", "UMLS"], [339, 340, "DatasetName", "UMLS"], [353, 354, "DatasetName", "UMLS"], [366, 367, "DatasetName", "UMLS"], [374, 375, "DatasetName", "UMLS"]]}
{"text": "This section describes three baseline models ( Section 4.5 ) , the training of classifiers using the full feature set ( Section 4.6 ) , and the feature selection procedures ( Section 4.7 ) .", "entities": [[27, 29, "MethodName", "feature selection"]]}
{"text": "After scaling the features , two models were fit using Python 's scikit - learn library and the full set of features : a logistic regression model and a random forests one ( 400 trees ) . Twenty percent of the data ( 3 , 658 items ) were used as a test set .", "entities": [[24, 26, "MethodName", "logistic regression"]]}
{"text": "Feature selection was undertaken to better understand which features were most strongly associated with class differences . The selection process utilized three distinct strategies , where the final set of selected features comprises only those features retained by all three methods . After applying feature selection to the training set , the predictive performance of the selected features is evaluated on the test set and compared to the performance of the full feature set and the baseline models outlined above . Embedded methods : The first method is LASSO regularized regression wherein the coefficients of variables that have low contributions towards the classification performance are shrunk to zero by forcing the sum of the absolute value of the regression coefficients to be less than a fixed value . We use the LassoCV algorithm with 100 - fold cross validation and maximum iterations set to 5 , 000 . Wrapper methods : We next apply recursive feature elimination , performed using two different classification algorithms : random forests classifier ( 400 trees , step = 5 ) and gradient boosting classifier ( Friedman , 2002 ) ( default parameters , step = 5 ) . The final set of selected linguistic features comprised 57 features that were retained by all three strategies . These features and their evaluation are discussed in sections 5 and 7 .", "entities": [[0, 2, "MethodName", "Feature selection"], [44, 46, "MethodName", "feature selection"]]}
{"text": "HHU at SemEval - 2016 Task 1 : Multiple Approaches to Measuring Semantic Textual Similarity", "entities": [[12, 15, "TaskName", "Semantic Textual Similarity"]]}
{"text": "This paper describes our participation in the SemEval - 2016 Task 1 : Semantic Textual Similarity ( STS ) . We developed three methods for the English subtask ( STS Core ) . The first method is unsupervised and uses WordNet and word2vec to measure a token - based overlap . In our second approach , we train a neural network on two features . The third method uses word2vec and LDA with regression splines .", "entities": [[13, 16, "TaskName", "Semantic Textual Similarity"], [17, 18, "TaskName", "STS"], [29, 30, "TaskName", "STS"], [71, 72, "MethodName", "LDA"]]}
{"text": "In the last shared tasks , most of the teams used natural languages processing techniques like tokenization , part - of - speech tagging , lemmatization , named entity recognition and word embeddings . External resources like WordNet ( Miller , 1995 ) and word2vec ( Mikolov et al , 2013 ) are commonly used . In ( Agirre et al , 2012 ) and ( Agirre et al , 2013 ) , the organizers provide a list and a comparison of the tools and resources used by the participants in the first two years , respectively . In each year , the organizers provide a baseline value by calculating the cosine similarity of the binary bag - of - words vectors from both sentences in each sample . Since 2013 , TakeLab ( \u0160ari\u0107 et al , 2012 , the best ranked system in 2012 , has also been used as another baseline value . Most of the teams used machine learning in 2015 ( Agirre et al , 2015 ) . In 2014 , the best two submitted runs were from unsupervised systems . The work most closely related to our Overlap method is ( Han et al , 2015 ) , which uses a twophased approach called Align - and - Differentiate . In the first phase , they compute an alignment score . Afterwards , they modify the alignment score in a differentiate phase by subtracting a penalty score for terms that can not be aligned . The idea behind the computation of our alignment scores is the same : For each sample , we average over the crosswise similarities between the sentences by aligning them , accumulating similarities between tokens and dividing by sentence lengths . The results of the alignment score in our Overlap method differ because ( i ) our alignment is different , ( ii ) we use another similarity function for tokens , and ( iii ) our preprocessing is different . In ( Vu et al , 2015 ) , the similarity between LDA vectors calculated from documents is used together with syntactic and lexical similarity measures to compute the similarity between text fragments . This idea is also incorporated in our Deep LDA method . Moreover , both approaches use different flavors of regression analysis for the final model prediction . Regression analysis was also used in ( Sultan et al , 2015 ) , where the authors combine an unsupervised method with ridge regression analysis . Our approach differs in the sense that it introduces knearest neighbors as a lazy training layer before the regression analysis phase to decrease the effect of noisy data points .", "entities": [[18, 24, "TaskName", "part - of - speech tagging"], [25, 26, "TaskName", "lemmatization"], [27, 30, "TaskName", "named entity recognition"], [31, 33, "TaskName", "word embeddings"], [343, 344, "MethodName", "LDA"], [373, 374, "MethodName", "LDA"]]}
{"text": "For preprocessing the input text , we first process each sentence with Stanford CoreNLP ( Manning et al , 2014 ) . Afterwards , we use Hunspell 1 with the latest OpenOffice English dictionaries to suggest spelling corrections for tokens with at least two characters in length . For each token , we calculate the Levenshtein distance for all suggestions . If suggestions have the same lowest distance , we choose the longest word and replace the former misspelt word . Abbreviations are also replaced by their full forms . Afterwards , we process the corrected sentence with Stanford CoreNLP again . We use the WordnetStemmer from the Java Wordnet Interface ( Finlayson , 2014 ) to look up lemmas with the help of WordNet ( Miller , 1995 ) . If the WordnetStemmer can not provide a lemma for a token , we use the predicted lemma from the Stanford CoreNLP . Instead of accessing all tokens in a sentence , we start from the root token and recursively follow outgoing dependency edges and add all visited tokens to a list . This approach improves our results slightly because some tokens will be ignored . Furthermore , the tokens are filtered for stopwords 2 .", "entities": [[138, 139, "DatasetName", "lemma"], [147, 148, "DatasetName", "lemma"]]}
{"text": "We train a neural network with 3 layers and a sigmoid activation function in Accord . NET ( de Souza , 2014 ) . Our network consists of 2 neurons in the input layer , 3 neurons in the hidden layer and 1 neuron in the output layer , as illustrated in Figure 1 . The layer weights are initialized by the Nguyen - Widrow function ( Nguyen and Widrow , 1990 ) . We use the Levenberg - Marquardt algorithm ( Levenberg , 1944 ; Marquardt , 1963 ) to train our network on the STS Core test data from 2015 and 2014 .", "entities": [[10, 12, "MethodName", "sigmoid activation"], [96, 97, "TaskName", "STS"]]}
{"text": "We represent the semantic similarity between two documents s 1 and s 2 by means of a vector F = [ f 1 , f 2 , f 3 , f 4 ] R 4 , where each component of F is responsible for modelling a different aspect of the semantic similarity , namely the surface - level similarity , context similarity , and the topical similarity .", "entities": [[3, 5, "TaskName", "semantic similarity"], [50, 52, "TaskName", "semantic similarity"]]}
{"text": "In order to model the context similarity between documents , we use word embeddings that learn semantically meaningful representations for words from local co - occurrences in sentences . More specifically we use word2vec ( Mikolov et al , 2013 ) which seems to be a reasonable choice to model context similarity as the word vectors are trained to maximize the log probability of context words . We denote the context similarity of two documents s 1 and s 2 by f 3 R and compute it as follows : f 3 ( s 1 , s 2 ) = cos ( \u1e7d s 1 , \u1e7d s 2 ) = cos v s 1 v | s 1 | , v s 2 v | s 2 | where v is the dense vector representation of a token and\u1e7d represents the centroid of the word vectors in a document .", "entities": [[12, 14, "TaskName", "word embeddings"]]}
{"text": "In order to predict the semantic similarity between two documents , we use a combination of k - NN and Multivariate Adaptive Regression Splines ( MARS ) ( Friedman , 1991 ) . Let T = { ( s 1 , s 1 , gs 1 ) , . . . , ( s m , s m , gs m ) } be the training set consisting of m document pairs together with their corresponding gold standard semantic similarity and ( s i , s i ) / T be a document pair for which the semantic similarity has to be computed . We construct a set F = { ( F 1 , gs 1 ) , . . . , ( F m , gs m ) } where each F j is the four - dimensional vector representation of the semantic similarity between s j and s j . Moreover , we Sentence 1 Sentence 2 gs STS Unfortunately the answer to your question is we simply do not know . Sorry , I do n't know the answer to your question .", "entities": [[5, 7, "TaskName", "semantic similarity"], [16, 19, "MethodName", "k - NN"], [25, 26, "DatasetName", "MARS"], [78, 80, "TaskName", "semantic similarity"], [97, 99, "TaskName", "semantic similarity"], [144, 146, "TaskName", "semantic similarity"], [161, 162, "TaskName", "STS"]]}
{"text": "Table 2 : Examples for the results of the Overlap method with the corresponding gold standards compute the vector F i . Next , we construct a set F k containing the k - nearest neighbors to the vector F i . In order to calculate the distances between the vectors , we use the Euclidean distance . Finally , we construct a vector gs k containing the gold standard similarity values of the k - nearest neighbors and feed it into a MARS model to predict the semantic similarity of the pair ( s i , s i ) . The choice of MARS is due to its capability to automatically model non - linearities between variables .", "entities": [[32, 36, "MethodName", "k - nearest neighbors"], [74, 78, "MethodName", "k - nearest neighbors"], [83, 84, "DatasetName", "MARS"], [88, 90, "TaskName", "semantic similarity"], [104, 105, "DatasetName", "MARS"]]}
{"text": "We report the results of our three approaches for the STS Core test from 2016 and 2015 .", "entities": [[10, 11, "TaskName", "STS"]]}
{"text": "We list the results of our methods for the 2015 test data in Table 3 to discuss the effect of different evaluation sets . It is interesting to see that the Deep LDA method performed best out of our three systems on 2015 . Its results on 2016 were surprisingly lower . We attribute this difference to the lack of domain specific training data for 2016 . As an unsupervised approach , the Overlap method has fewer problems with the domain change . It should be noted that the gold standard of the 2015 test data was available during the development of our methods . For the training phase , the Same Word Neural Network method used the STS Core test from 2014 . The Deep LDA method was trained on the data from 2012 to 2014 .", "entities": [[32, 33, "MethodName", "LDA"], [118, 119, "TaskName", "STS"], [126, 127, "MethodName", "LDA"]]}
{"text": "We have presented three approaches to measure textual semantic similarity . This year , our unsupervised method achieved the best result . By comparing our result for 2016 and 2015 , we showed that the approaches yielded different results in a different order . In our future work , we will try to modify the Overlap method , by also using a penalty score and by applying certain similarity score shifters , for instance modifying the score by applying a date extraction with a specific distance function for dates . We tried to group words into phrases by using a sliding window approach with a shrinking window size and matching phrases in word2vec . In our initial attempt , this worsened the results for the Overlap method . We will adjust the similarity function to increase the weight of phrases in comparison to unigrams . We aim to adapt the techniques for German and Spanish .", "entities": [[8, 10, "TaskName", "semantic similarity"]]}
{"text": "Using Large Pretrained Language Models for Answering User Queries from Product Specifications", "entities": [[2, 5, "TaskName", "Pretrained Language Models"]]}
{"text": "While buying a product from the e - commerce websites , customers generally have a plethora of questions . From the perspective of both the e - commerce service provider as well as the customers , there must be an effective question answering system to provide immediate answers to the user queries . While certain questions can only be answered after using the product , there are many questions which can be answered from the product specification itself . Our work takes a first step in this direction by finding out the relevant product specifications , that can help answering the user questions . We propose an approach to automatically create a training dataset for this problem . We utilize recently proposed XLNet and BERT architectures for this problem and find that they provide much better performance than the Siamese model , previously applied for this problem ( Lai et al , 2018 ) . Our model gives a good performance even when trained on one vertical and tested across different verticals .", "entities": [[41, 43, "TaskName", "question answering"], [122, 123, "MethodName", "XLNet"], [124, 125, "MethodName", "BERT"]]}
{"text": "Product specifications are the attributes of a product . These specifications help a user to easily identify and differentiate products and choose the one that matches certain specifications . There are more than 80 million products across 80 + product categories on Flipkart 1 . The 6 largest categories are - Mobile , AC , Backpack , Computer , Shoes , and Watches . A large fraction of user queries ( \u223c 20 % ) 2 can be answered with the specifications . Product specifications would be helpful in providing instant responses to questions newly posed by users about * Work done while author was at IIT Kharagpur . 1 Flipkart Pvt Ltd. is an e - commerce company based in Bangalore , India . 2 We randomly sampled 1500 questions from all these verticals except Mobile and manually annotated them as to whether these can be answered through product specifications . the corresponding product . Consider a question \" What is the fabric of this bag ? \" This new question can be easily answered by retrieving the specification \" Material \" as the response . Fig . 1 depicts this scenario . Most of the recent works on product related queries on e - commerce leverage the product reviews to answer the questions ( Gao et al , 2019 ; McAuley and Yang , 2016 ) . Although reviews are a rich source of data , they are also subject to personal experiences . People tend to give many reviews on some products and since it is based upon their personal experience , the opinion is also diverse . This creates a massive volume and range of opinions and thus makes review systems difficult to navigate . Sometimes products do not even have any reviews that can be used to find an answer , also the reviews do not mention the specifications a lot , but mainly deal with the experience . So , there are several reasons why product specifications might be a useful source of information to answer product - related queries which does not involve user experience to find an answer . As the specifications are readily available , users can get the response instantly . This paper attempts to retrieve the product specifications that would answer the user queries . While solving this problem , our key contributions are as follows - ( i ) We demonstrate the success of XL - Net on finding product specifications that can help answering product related queries . It beats the baseline Siamese method by 0.14 \u2212 0.31 points in HIT@1 . ( ii ) We utilize a method to automatically create a large training dataset using a semisupervised approach , that was used to fine - tune XLNet and other models . ( iii ) While we trained on Mobile vertical , we tested on different verticals , namely , AC , Backpack , Computer , Shoes , Watches , which show promising results .", "entities": [[111, 112, "MethodName", "Pvt"], [461, 462, "MethodName", "XLNet"]]}
{"text": "In recent years , e - commerce product question answering ( PQA ) has received a lot of attention . Yu et al ( 2018 ) present a framework to answer product related questions by retrieving a ranked list of reviews and they employ the Positional Language Model ( PLM ) to create the training data . Chen et al ( 2019 ) apply a multi - task attentive model to identify plausible answers . Lai et al ( 2018 ) propose a Siamese deep learning model for answering questions regarding product specifications . The model returns a score for a question and specification pair . McAuley and Yang ( 2016 ) exploit product reviews for answer prediction via a Mixture of Expert ( MoE ) model . This MoE model makes use of a review relevance function and an answer prediction function . It assumes that a candidate answer set containing the correct answers is available for answer selection . Cui et al ( 2017 ) develop a chatbot for e - commerce sites known as SuperAgent . SuperAgent considers question answer collections , reviews and specifications when answering questions . It selects the best answer from multiple data sources . Language representation models like BERT ( Devlin et al , 2019 ) and XLNet ( Yang et al , 2019 ) are pre - trained on vast amounts of text and then fine - tuned on task - specific labelled data . The resulting models have achieved state of the art in many natural language processing tasks including question answering . Dzendzik et al ( 2019 ) employ BERT to answer binary questions by utilizing customer reviews . In this paper , unlike some of the previous works ( Lai et al , 2018 ; Chen et al , 2019 ) on PQA that solely rely on human annotators to annotate the training instances , we propose a semi - supervised method to label training data . We leverage the product specifications to answer user queries by using BERT and XLNet .", "entities": [[8, 10, "TaskName", "question answering"], [158, 160, "TaskName", "answer selection"], [169, 170, "TaskName", "chatbot"], [206, 207, "MethodName", "BERT"], [215, 216, "MethodName", "XLNet"], [260, 262, "TaskName", "question answering"], [270, 271, "MethodName", "BERT"], [340, 341, "MethodName", "BERT"], [342, 343, "MethodName", "XLNet"]]}
{"text": "Our goal is to train a classifier that takes a question and a specification as input ( e.g. , \" Color Code Black \" ) and predicts whether the specification is relevant to the question . We take Siamese architecture ( Lai et al , 2018 ) as our baseline method . We fine - tune BERT and XLNet for this classification task . Siamese : We train a 100 - dimensional word2vec embedding on the whole corpus ( all questions and specifications as shown in Table 1 . ) to get the input word representation . In the Siamese model , the question and specification is passed through a Siamese Bi - LSTM layer . Then we use max - pooling on the contextual representations to get the feature vectors of the question and specification . We concatenate the absolute difference and hadamard product of these two feature vectors and feed it to two fully connected layers of dimension 50 and 25 , subsequently . Finally , the softmax layer gives the relevance score . BERT and XLNet : The architecture we use for fine - tuning BERT and XLNet is the same . We begin with the pre - trained BERT Base and XLNet Base model . To adapt the models for our task , we introduce a fully - connected layer over the final hidden state corresponding to the [ CLS ] input token . During fine - tuning , we optimize the entire model end - to - end , with the additional softmax classifier parameters W R K\u00d7H , where H is the dimen - sion of the hidden state vectors and K is the number of classes . 5 Experimental Setup", "entities": [[56, 57, "MethodName", "BERT"], [58, 59, "MethodName", "XLNet"], [113, 114, "MethodName", "LSTM"], [169, 170, "MethodName", "softmax"], [176, 177, "MethodName", "BERT"], [178, 179, "MethodName", "XLNet"], [188, 189, "MethodName", "BERT"], [190, 191, "MethodName", "XLNet"], [202, 203, "MethodName", "BERT"], [205, 206, "MethodName", "XLNet"], [257, 258, "MethodName", "softmax"]]}
{"text": "In this paper , we proposed a method to label training data with little supervision . We demonstrated that large pretrained language models such as BERT and XLNet can be fine - tuned successfully to obtain product specifications that can help answer user queries . We also achieve reasonably good results even while testing on different verticals . We would like to extend our method to take into account multiple specifications as an answer . We also plan to develop a classifier to identify which questions can not be answered from the specifications .", "entities": [[20, 23, "TaskName", "pretrained language models"], [25, 26, "MethodName", "BERT"], [27, 28, "MethodName", "XLNet"]]}
{"text": "Generative commonsense reasoning ( GCR ) in natural language is to reason about the commonsense while generating coherent text . Recent years have seen a surge of interest in improving the generation quality of commonsense reasoning tasks . Nevertheless , these approaches have seldom investigated diversity in the GCR tasks , which aims to generate alternative explanations for a real - world situation or predict all possible outcomes . Diversifying GCR is challenging as it expects to generate multiple outputs that are not only semantically different but also grounded in commonsense knowledge . In this paper , we propose MoKGE , a novel method that diversifies the generative reasoning by a mixture of expert ( MoE ) strategy on commonsense knowledge graphs ( KG ) . A set of knowledge experts seek diverse reasoning on KG to encourage various generation outputs . Empirical experiments demonstrated that MoKGE can significantly improve the diversity while achieving on par performance on accuracy on two GCR benchmarks , based on both automatic and human evaluations .", "entities": [[120, 122, "TaskName", "knowledge graphs"], [158, 159, "MetricName", "accuracy"]]}
{"text": "Incorporating external knowledge is essential for many NLG tasks to augment the limited textual information ( Yu et al , 2022c ; Dong et al , 2021 ; Yu et al , 2022b ) . Some recent work explored using graph neural networks ( GNN ) to reason over multihop relational knowledge graph ( KG ) paths ( Zhou et al , 2018 ; Jiang et al , 2019 ; Zhang et al , 2020a ; Wu et al , 2020 ; Yu et al , 2022a ; Zeng et al , 2021 ) . For example , Zhou et al ( 2018 ) enriched the context representations of the input sequence with neighbouring concepts on ConceptNet using graph attention . Ji et al ( 2020 ) performed dynamic multi - hop reasoning on multi - relational paths extracted from the external commonsense KG . Recently , some work attempted to integrate external commonsense knowledge into generative pretrained language models ( Guan et al , 2020 ; Bhagavatula et al , 2020 ; Liu et al , 2021 ) . For example , Guan et al ( 2020 ) conducted post - training on sythetic data constructed from commonsense KG by translating triplets into natural language texts using templates . Yu et al ( 2022c ) wrote a comprehensive survey for more detailed comparisons of different knowledge graph enhanced NLG methods .", "entities": [[116, 117, "DatasetName", "ConceptNet"], [157, 160, "TaskName", "pretrained language models"]]}
{"text": "Problem formulation . In this paper , we focus on diversifying the outputs of generative commonsense reasoning ( GCR ) tasks , e.g. commonsense explanation generation and abductive commonsense reasoning . These tasks require one - to - many generation , i.e. , creating a set of reasonable outputs that vary as widely as possible in terms of con - tents , language style and word variability . Formally , given a source input x , our goal is to model a conditional distribution for the target outputs p ( y | x ) that assigns high values to { p ( y 1 | x ) , , p ( y K | x ) } for K mappings , i.e. , { x y 1 , , x y K } . Meanwhile , the outputs { y 1 , , y K } are expected to be diverse with each other in terms of contents . Existing diversity - promoting methods only varied the language styles and failed to perform different knowledge reasoning to generate diverse contents ( Cho et al , 2019 ; Shen et al , 2019 ; . Here , incorporating commonsense KG is essential for the generative reasoning ( GR ) tasks because the KG can not only augment the limited information in the input text , but also provide a rich searching space for knowledge reasoning . Therefore , we propose to employ commonsense KG to play the central role of performing diverse knowledge reasoning , then use different sets of selected concepts to produce diverse outputs . Model Outline . Our model has two major components : ( i ) a knowledge graph ( KG ) enhanced generative reasoning module to reasonably associate relevant concepts and background into the generation process , and ( ii ) a mixture of expert ( MoE ) module to diversify the generation process and produce multiple reasonable outputs .", "entities": [[24, 26, "TaskName", "explanation generation"]]}
{"text": "To model the relational information in the commonsen KG , we employ the relational graph convolutional network ( R - GCN ) ( Schlichtkrull et al , 2018 ) which generalizes GCN with relation specific weight matrices . We follow Vashishth et al ( 2020 ) and Ji et al ( 2020 ) to use a non - parametric compositional operation \u03d5 ( ) to combine the concept node embedding and the relation embedding . Specifically , given the input subgraph G x = { V x , E x } and an R - GCN with L layers , we update the embedding of each node v V x at the ( l+1 ) - th layer by aggregating information from the embeddings of its neighbours in N ( v ) at the l - th layer : o l v = 1 | N ( v ) | ( u , v , r ) E W l N \u03d5 ( h l u , h l r ) , ( 1 ) h l+1 v = ReLU ( o l v + W l S h l v ) , ( 2 ) where h v and h r are node embedding and relation embedding . We define the compositional operation as \u03d5 ( h u , h r ) = h u \u2212h r inspired by the TransE ( Bordes et al , 2013 ) . The relation embedding is also updated via another linear transformation : h l+1 r = W l R h l r . ( 3 ) Finally , we obtain concept embedding h L v that encodes the sequence - associated subgraph context .", "entities": [[14, 17, "MethodName", "graph convolutional network"], [20, 21, "MethodName", "GCN"], [31, 32, "MethodName", "GCN"], [95, 96, "MethodName", "GCN"], [179, 180, "MethodName", "ReLU"], [231, 232, "MethodName", "TransE"]]}
{"text": "Commonsense explanation generation . It aims to generate an explanation given a counterfactual statement for sense - making ( Wang et al , 2019 ) . We use the benchmark dataset ComVE from SemEval - 2020 Task 4 ( Wang et al , 2020 ) . The dataset contains 10 , 000 / 997 / 1 , 000 examples for training / development / test sets , respectively . The average input / output length is 7 .", "entities": [[1, 3, "TaskName", "explanation generation"], [28, 30, "DatasetName", "the benchmark"]]}
{"text": "We note that as we targeted at the one - to - many generation problem , we excluded those baseline methods mentioned in the related work that can not produce multiple outputs , e.g. , Zhang et al ( 2020a ) ; Ji et al ( 2020 ) ; Liu et al ( 2021 ) . Different from aforementioned methods , our MoKGE can seek diverse reasoning on KG to encourage various generation outputs without any additional conditions . To the best of our knowledge , we are the first work to explore diverse knowledge reasoning on commonsense KG to generate multiple diverse output sequences . Therefore , we only compared our MoKGE with existing diversity - promoting baselines without using knowledge graph . VAE - based method . The variational auto - encoder ( VAE ) ( Kingma and Welling , 2014 ) is a deep generative latent variable model . VAE - based methods produce diverse outputs by sampling different latent variables from an approximate posterior distribution . CVAE - SVG ( SVG is short for sentence variant generation ) ( Gupta et al , 2018 ) is a conditional VAE model that can produce multiple outputs based an original sentence as input . MoE - based method . Mixture models provide an alternative approach to generate diverse outputs by sampling different mixture components . We compare against two mixture of experts ( MoE ) implementations by Shen et al ( 2019 ) and Cho et al ( 2019 ) . We refer them as MoE - prompt ( Shen et al , 2019 ) and MoE - embed ( Cho et al , 2019 ) . Sampling - based method . Sampling methods create diverse outputs by sampling next token widely from the vocabulary . We compare against two sampling algorithms for decoding , including truncated sampling ( Fan et al , 2018 ) and nucleus sampling . Truncated sampling ( Fan et al , 2018 ) randomly samples words from top - k probability candidates of the predicted distribution at each decoding step . Nucleus sampling avoids text degeneration by truncating the unreliable tails and sampling from the dynamic nucleus of tokens containing the vast majority of the probability mass .", "entities": [[124, 125, "MethodName", "VAE"], [135, 136, "MethodName", "VAE"], [152, 153, "MethodName", "VAE"], [170, 171, "MethodName", "CVAE"], [192, 193, "MethodName", "VAE"]]}
{"text": "BERT - based distractor generation for Swedish reading comprehension questions using a small - scale dataset", "entities": [[0, 1, "MethodName", "BERT"], [3, 5, "TaskName", "distractor generation"], [7, 9, "TaskName", "reading comprehension"]]}
{"text": "An important part when constructing multiplechoice questions ( MCQs ) for reading comprehension assessment are the distractors , the incorrect but preferably plausible answer options . In this paper , we present a new BERTbased method for automatically generating distractors using only a small - scale dataset . We also release a new such dataset of Swedish MCQs ( used for training the model ) , and propose a methodology for assessing the generated distractors . Evaluation shows that from a student 's perspective , our method generated one or more plausible distractors for more than 50 % of the MCQs in our test set . From a teacher 's perspective , about 50 % of the generated distractors were deemed appropriate . We also do a thorough analysis of the results .", "entities": [[11, 13, "TaskName", "reading comprehension"]]}
{"text": "As mentioned previously , plausible distractors should be grammatically consistent with the key . Hence , a metric measuring grammatical consistency would be useful both for quantitative evaluation and as a basis for a baseline method . We propose to use convolution partial tree kernels ( CPTK ) for these purposes . CPTK were proposed by Moschitti ( 2006 ) for dependency trees and essentially calculate the number of common tree structures ( not only full subtrees ) between two given trees . However , CPTKs can not handle labeled edges and were applied to dependency trees containing only lexicals . Another solution , proposed by Croce et al ( 2011 ) and used in this article , is to include edge labels , i.e. , grammatical relations ( GR ) , as separate nodes . A resulting computational structure is Grammatical Relation Centered Tree ( GRCT ) , which transforms the original dependency tree by making each PoS - tag a child of a GR node and a father of a lexical node . CPTKs can take any non - negative values and are thus hard to interpret . Hence , we use normalized CPTK ( NCPTK ) shown in Equation ( 1 ) , where K ( T 1 , T 2 ) is the CPTK applied to the dependency trees T 1 and T 2 . K ( T 1 , T 2 ) = K ( T 1 , T 2 ) K ( T 1 , T 1 ) K ( T 2 , T 2 ) , ( 1 ) Evidently , when T 1 and T 2 are the same , K ( T 1 , T 2 ) equals to 1 , which is the highest value it can take .", "entities": [[41, 42, "MethodName", "convolution"]]}
{"text": "As mentioned in Section 2.2 , NCPTK measures grammatical consistency between the key and a distractor . Our baseline uses NCPTK on Universal Dependencies ( UD ) trees ( Nivre et al , 2020 ) in the following way . For each given MCQ , we exclude the sentence containing the key from the base text and then parse each remaining sentence s i of the text , and the key using the UD parser for Swedish . Let T s i and T k denote a dependency tree corresponding to s i and the key respectively . For each T s i , we find all subtrees with the root having the same universal PoS - tag and the same universal features ( representing morphological properties of the token ) as the root of T k . If no subtrees are found , no distractors can be suggested for this MCQ . Otherwise , we calculate NCPTK between each found subtree and T k ( both as GRCT , but without lexicals ) . Then we take the textual representation of the K subtrees with the highest NCPTK as the distractor suggestions . With these settings , training took about 3.67h for the left - to - right and 3h for the u - PMLM variant . UD trees for the baseline were obtained using Stanza package ( Qi et al , 2020 ) and convolution partial tree kernels on the UD trees were calculated using UDon2 library ( Kalpakchi and Boye , 2020 ) . Baseline requires no training and running our implementation of the baseline takes about a minute on the development or test set .", "entities": [[22, 24, "DatasetName", "Universal Dependencies"], [25, 26, "DatasetName", "UD"], [73, 74, "DatasetName", "UD"], [215, 216, "MethodName", "PMLM"], [218, 219, "DatasetName", "UD"], [236, 237, "MethodName", "convolution"], [242, 243, "DatasetName", "UD"]]}
{"text": "Following the analysis of Rodriguez ( 2005 ) , we generate three distractors per MCQ for each model . Due to prohibitively high costs of human evaluation , we have divided the evaluation process into two stages . The first stage is quantitative evaluation , which gives limited information about the model 's quality , but is sufficient for model selection . The second stage is human evaluation of the best model , selected during the first stage .", "entities": [[59, 61, "TaskName", "model selection"]]}
{"text": "Baseline When using u - PMLM , shortest distractors were generated first . \u2191 ( \u2193 ) means \" the higher ( lower ) , the better \" . In contrast , u - PMLM needs the lengths of the distractors to be decided in beforehand , which we set to be the lengths of the two reference distractors and the length of the key 4 . Surprisingly , the order of distractors in terms of their length also matters for generation with u - PMLM , so we have tested three options : shortest first , longest first and random order . According to the results of model selection on the development set ( presented in detail in Appendix C ) , u - PMLM models outperformed left - to - right models by a substantial margin . The best u - PMLM model ( generating shortest distractors first ) and the baseline have been evaluated on the test set ( see Table 3 ) . Interestingly , the similarity of syntactic structures between the key and distractors ( assessed by NCPTK ) is the same for both baseline ( that actually relies on NCPTK ) and u - PMLM . At the same time , u - PMLM generates more distractors matching the reference ones compared to the baseline ( as seen from DisRecall and AnyDisRefMatch ) . The baseline generates at least one empty string as a distractor 11.76 % of the time ( compared to no such cases for u - PMLM ) limiting possibilities of using the baseline in the real - life applications .", "entities": [[5, 6, "MethodName", "PMLM"], [34, 35, "MethodName", "PMLM"], [85, 86, "MethodName", "PMLM"], [108, 110, "TaskName", "model selection"], [125, 126, "MethodName", "PMLM"], [143, 144, "MethodName", "PMLM"], [200, 201, "MethodName", "PMLM"], [209, 210, "MethodName", "PMLM"], [255, 256, "MethodName", "PMLM"]]}
{"text": "We have used distractors generated on the test set by the best u - PMLM model ( selected after quantitative evaluation in Section 6.1 ) to conduct human evaluation in 2 stages : from a perspective of a student and a teacher .", "entities": [[14, 15, "MethodName", "PMLM"]]}
{"text": "We have collected SweQUAD - MC , the first dataset of Swedish MCQs , and showed the possibility of training usable BERT - based DG models , despite the small scale of the dataset . We have showed that a u - PMLM variant of the BERT - based DG model performs best on the dataset , and proposed a novel methodology of evaluating the plausibility of generated distractors . Around half of the generated distractors were found acceptable by the majority of teachers , and more than 50 % of MCQs had at least one plausible generated distractor , judging by the entropy of students ' responses . Bearing in mind that the aim of the proposed method is to support ( not replace ) teachers , we deem that our method works well for MCQs in Swedish ( and potentially in other languages with a pretrained BERT and a dataset of a similar scale ) . Furthermore , we have presented a baseline applicable to any language with a UD treebank ( currently about 100 languages ) . Although its performance is nowhere near the u - PMLM variant , we believe that it can serve as a good point of comparison to emerging neural methods for other languages . System Demonstrations , pages 38 - 45 , Online . Association for Computational Linguistics .", "entities": [[21, 22, "MethodName", "BERT"], [42, 43, "MethodName", "PMLM"], [46, 47, "MethodName", "BERT"], [148, 149, "MethodName", "BERT"], [171, 172, "DatasetName", "UD"], [189, 190, "MethodName", "PMLM"]]}
{"text": "We have trained both left - to - right and u - PMLM variants for 6 epochs ( fixing a random seed for u - PMLM masking procedure to 42 ) . The quantitative performance metrics on the development set for the top - 3 models for each variant are presented in Table 5 . The best u - PMLM model ( i - 14000 ) outperformed the best left - to - right model ( i - 18000 ) on most of the quantitative metrics . The next experiment concerned the order in which distractors are generated , which we tested only for the best u - PMLM model . We tried generating shortest distractors first ( SF ) , longest first ( LF ) or in a random order with a fixed seed of 42 ( RND ) . The results of the experiment are presented in Table 6 . Evidently , models with SF - generation consistently outperform ones with LF - generation . SF - generation also performs on - par or better than RND - generation . However , fixing a seed is not a generalizable solution , which is why we opted for SF - generation .", "entities": [[12, 13, "MethodName", "PMLM"], [25, 26, "MethodName", "PMLM"], [59, 60, "MethodName", "PMLM"], [108, 109, "MethodName", "PMLM"]]}
{"text": "Evaluation from the student 's perspective has been conducted on the Prolific platform 7 . We used Prolific 's pre - screening feature and required each subject to have Swedish as the first language and hold at least a high school diploma ( A - levels ) . Descriptive statistics about the recruited sample of subjects Imagine that you are a teacher checking reading comprehension skills of your students . Given a text , your task is to create one or more multiple choice questions based on the text , i.e. : 1 . formulate a question with the correct answer in the text ; 2 . mark the correct answer in the text ; 3 . mark some wrong , but plausible options in the text . When you have written your questions , marked the correct answer ( CA ) and the wrong alternatives in the text , click on \" Submit \" . When you formulate the question , think about the following aspects . The question must be independent , i.e. , one should not require additional information ( on top of the given text ) to be able to answer the question . The question should be unambiguous and have only one possible interpretation . One should not be able to answer your question without reading the text , which is why even wrong alternatives should be plausible . Wrong options must be in the same grammatical form as the CA . For instance , if the CA begins with a verb in Past Simple , all wrong options must begin with a verb in Past Simple . Find as many questions as you can ( + the correct answer and wrong alternatives ) on each text and then get a new text when you ca n't find more . Figure 6 : An English translation of the original instructions for SweQUAD - MC data collection ( the original instructions in Swedish can be found in the GitHub repository ) Metric left - to - right u - PMLM i - 10000 i - 14000 i - 18000 i - 10000 i - 14000 i - 16000 e - 3 . Thank you for participating in our study ! You will be presented with a number of multiple choice questions . Your task is to answer as many of these questions correctly as possible . If you do n't know which alternative is correct , choose the one that seems the most plausible . You are allowed to use ONLY your own prior knowledge and common sense . Please , do NOT consult any other external sources of information . Figure 8 : An English translation of the original instructions given to subjects on the Prolific platform ( the original instructions in Swedish can be found in the GitHub repository ) is presented in Figure 7 . The exact guidelines given to the subjects ( and their translation to English ) are presented in Figure 8 . MCQs were presented in a random order , but the order of options for each MCQs was the same for each subject .", "entities": [[63, 65, "TaskName", "reading comprehension"], [343, 344, "MethodName", "PMLM"]]}
{"text": "The observations in the sample should be independent . Subjects have performed the task independently of each other through a Prolific platform , hence the observations are independent . 3 . The variable under study should be approximately normally distributed . The distribution of the number of correctly answered MCQs is presented in Figure 7 ( the plot in the last row and the last column with the title \" num correct \" ) . The distribution is indeed approximately normal . 4 . The variable under study should have no extreme outliers . Outliers are typically defined in terms of the interquartile range ( IQR ) , which equals to Q3 - Q1 . The datapoints outside 1.5IQR are deemed mild outliers , whereas those outside 3IQR are considered extreme outliers . Boxplots for our data with whiskers within both 1.5IQR and 3IQR are presented in Figure 9 . Two datapoints can be considered mild outliers , but no extreme outliers are present , which means this assumption for the one sample t - test is not violated .", "entities": [[105, 106, "DatasetName", "IQR"]]}
{"text": "To get a comprehensive overview of methods for generating distractors for MCQs , we employed a two - step process . The first step was to issue queries \" distractor generation \" and \" multiple choice question generation \" to ACL Anthology and Google Scholar . The result was 20 articles from ACL Anthology and 4 additional ones from Google Scholar . The second step was to select relevant references from the \" Related work \" sections of these articles . This resulted into 15 additional articles . Out of found 39 articles , 11 were filtered out ( 8 focused only on generating questions , 1 relied mostly on expert knowledge , 1 on the auxiliary relation extraction task and 1 was a demo paper ) , leaving 28 articles in total . Only 2 of these 28 papers worked with a language other than English ( Chinese and Basque ) .", "entities": [[29, 31, "TaskName", "distractor generation"], [36, 38, "TaskName", "question generation"], [43, 44, "DatasetName", "Google"], [59, 60, "DatasetName", "Google"], [117, 119, "TaskName", "relation extraction"]]}
{"text": "To evaluate and compare our models , we use the Spanish - English ( SPA - EN ) part of the LinCE benchmark ( a total of 32 , 651 posts equivalent to 390 , 953 tokens ) ( Aguilar et al , 2020 ) . We chose this language pair because it has the challenge of increased similarity between the languages ( Tristram , 1999 ) . In the original data , 8 labels are used , from which we only focus on the 3 labels necessary for the language identification task : lang1 , lang2 and other , for English , Spanish and punctuation , numbers , symbols and emoticons , respectively . We use the default development and test splits for our experiments .", "entities": [[21, 22, "DatasetName", "LinCE"], [90, 92, "TaskName", "language identification"]]}
{"text": "Generally , Latent Dirichlet Allocation ( LDA ) aims to find the topics a document belongs to using the words in the document as features . In our case , the documents are the words , the features are character n - grams ( with n 1 to 5 ) and the topics are English and Spanish . The LDA algorithm does not output labels for the resulting clusters , so we select the top 10 words based on weight that represent best each cluster , and assign them a language using the word uni - gram method ( Section 3.1 ) . We use the Scikit Learn 6 implementation of LDA with the TfidfVectorizer and use only the first 100 , 000 words from each monolingual dataset , in order to reduce training time .", "entities": [[6, 7, "MethodName", "LDA"], [59, 60, "MethodName", "LDA"], [111, 112, "MethodName", "LDA"]]}
{"text": "For our Support Vector Machine ( SVM ) model , we consider the monolingual data ( Section 2.2 ) to be the gold training data , without tokens from the other class . Using TfidfVectorizer , we extract character n - gram features from each word , with n 1 to 5 . We use the Scikit Learn implementation with all default parameters and select the first 100 , 000 words from each dataset .", "entities": [[2, 5, "MethodName", "Support Vector Machine"], [6, 7, "MethodName", "SVM"]]}
{"text": "We use Logistic Regression in a weakly - supervised manner , the same as with SVM , where we consider the first 100 , 000 words from each Wikipedia dataset to be the gold training data . Again , we use TfidfVectorizer to extract character n - gram features , with n 1 to 5 , and rely on the default Scikit Learn implementation .", "entities": [[2, 4, "MethodName", "Logistic Regression"], [15, 16, "MethodName", "SVM"]]}
{"text": "Cross - Sentence N - ary Relation Extraction using Lower - Arity Universal Schemas", "entities": [[6, 8, "TaskName", "Relation Extraction"]]}
{"text": "Most existing relation extraction approaches exclusively target binary relations , and n - ary relation extraction is relatively unexplored . Current state - of - the - art n - ary relation extraction method is based on a supervised learning approach and , therefore , may suffer from the lack of sufficient relation labels . In this paper , we propose a novel approach to cross - sentence n - ary relation extraction based on universal schemas . To alleviate the sparsity problem and to leverage inherent decomposability of n - ary relations , we propose to learn relation representations of lower - arity facts that result from decomposing higher - arity facts . The proposed method computes a score of a new nary fact by aggregating scores of its decomposed lower - arity facts . We conduct experiments with datasets for ternary relation extraction and empirically show that our method improves the n - ary relation extraction performance compared to previous methods .", "entities": [[2, 4, "TaskName", "relation extraction"], [14, 16, "TaskName", "relation extraction"], [31, 33, "TaskName", "relation extraction"], [71, 73, "TaskName", "relation extraction"], [143, 145, "TaskName", "relation extraction"], [156, 158, "TaskName", "relation extraction"]]}
{"text": "The cross - sentence n - ary relation extraction task ( Peng et al , 2017 ) is defined as follows . Let E be a set of entities , R KB be a set of relation types of an external knowledge base KB , and O KB = { r , ( e 1 , ... , e n ) : r ( e 1 , ... , e n ) KB , r 2 For example , the ternary relation AwardedFor ( director , movie , award ) can be decomposed into the binary relations DirectorOf ( director , movie ) and WonAward ( director , award ) . Note that a similar idea is introduced in ( Ernst et al , 2018 ) as partial facts or partial patterns . 3 Our codes and datasets are available at https://github.com/aurtg/ nary - relation - extraction - decomposed . R KB , e i E } be the set of facts in KB . We collect a set of candidate entity tuples among which KB relation r R KB possibly holds . 4 Here , all entities in each candidate tuple ( e 1 , ... , e n ) are mentioned in the same text section T in a given set of documents . We define a set of these entity mentions as O text = { T , ( e 1 , ... , e n ) : e i E is mentioned in T } . Here , text section T is a ( short ) span in a document which can describes relational facts among entities . In the cross - sentence n - ary relation extraction task , text section T can contain multiple sentences . In this paper , following ( Peng et al , 2017 ) , we define M consecutive sentences ( M \u2265 1 ) which contain n target entities as a text section in the crosssentence n - ary relation extraction task . We use the term \" relation \" to refer to both relations r R KB and sections T . The goal of the cross - sentence n - ary relation extraction task is to predict new facts r , ( e 1 , ... , e n ) / O KB for relation r R KB given O = O KB \u222a O text , where n \u2265 2 . 3 Proposed Method", "entities": [[7, 9, "TaskName", "relation extraction"], [280, 282, "TaskName", "relation extraction"], [330, 332, "TaskName", "relation extraction"], [363, 365, "TaskName", "relation extraction"]]}
{"text": "We learn a vector representation v ( r ) R dr for each unary or binary relation in O 1 or O 2 . For r ( k ) or r ( k , l ) derived from a KB relation , we represent it by a trainable parameter vector . On the other hand , for the one derived from a textual relation , we use the following encoders to compute its representations . Unary encoder : For an unary textual relation r ( k ) = ( T , pos ( e k ) ) , we represent each section T by a sequence of word vectors and use a bidirectional LSTM ( Bi - LSTM ) ( Schuster and Paliwal , 1997 ) to compute a hidden representation h l R dr at each word position l. Following recent works He et al , 2018 ; Lee et al , 2017 ) , we aggregate h l within a phrase of entity e k to compute v ( T ( k ) ) . We use elementwise mean as aggregation function : v ( r ( k ) ) = mean ( { h l : l pos ( e k ) } ) . ( 1 ) Binary encoder : For a binary textual relation r ( k , l ) = path ( T ; e k , e l ) , we represent each token ( word or edge label ) in path ( T ; e k , e l ) by an embedding vector ( Toutanova et al , 2015 ; Verga et al , 2016 ) . We use a Bi - LSTM to compute a hidden representation h l R dr at each token position l , and max - pool along the path to compute the relation representation : v ( T ( k , l ) ) = max ( { h l : l = 1 , ... , L } ) . ( 2 )", "entities": [[112, 114, "MethodName", "bidirectional LSTM"], [117, 118, "MethodName", "LSTM"], [281, 282, "MethodName", "LSTM"]]}
{"text": "The cross - sentence n - ary relation extraction dataset from Peng et al ( 2017 ) contains only 59 distinct ternary KB facts including the train and test set . Since our proposed method and universal schemas baselines predict KB relations for each entity tuple instead of each surface pattern , the number of known facts of KB relations is crucial to reliably evaluate and compare these methods . Thus , we created two new n - ary cross - sentence relation extraction datasets ( dubbed with Wiki - 90k and WF - 20k ) that contain more known facts retrieved from public knowledge bases . To create the Wiki - 90k and WF - 20k datasets , we used Wikidata and Freebase respectively as external knowledge bases . Since these knowledge bases store only binary relational facts , we defined multiple ternary relations by combining a few binary relations . 7 , 8 For both datasets , we collected paragraphs from the English Wikipedia , and used Stanford CoreNLP ( Manning et al , 2014 ) to extract dependency and co - reference links . Entity mentions are detected using DBpedia Spotlight ( Daiber et al , 2013 ) . We followed ( Peng et al , 2017 ) to extract co - occurring entity tuples and their surface patterns , that is , we selected tuples which occurred in a minimal span within at most M \u2264 3 consecutive sentences . Entity tuples without a known KB relation are subsampled , since the number of such tuples are too large . We randomly partitioned all entity tuples into train , development ( dev ) , and test sets . ( Song et al , 2018 ) : The state - of - the - art crosssentence n - ary relation extraction method proposed by Song et al ( 2018 ) represents each surface pattern by the concatenation of entity vectors from the last layer of a Graph State LSTM , a variant of a graph neural network . The concatenated vector is then fed into a classifier to predict the relation label . Since their method directly predicts a relation label for each surface pattern , it is more robust to the sparsity of surface patterns among a specific higher arity entity tuple . However , due to their purely supervised training objective , its performance may degrade if the number of available training labels is small .", "entities": [[7, 9, "TaskName", "relation extraction"], [82, 84, "TaskName", "relation extraction"], [192, 193, "DatasetName", "DBpedia"], [302, 304, "TaskName", "relation extraction"], [331, 332, "MethodName", "LSTM"]]}
{"text": "Wiki - 90k WF - 20k average weighted average weighted Proposed 0.584 0.634 0.821 0.842 ( Song et al , 2018 ) 0.471 0.536 0.639 0.680 ( Toutanova et al , 2015 ) with Graph State LSTM ( Song et al , 2018 ) ( Verga et al , 2017 ) with Graph State LSTM ( Song et al , 2018 ) multiple - test adjustment using Holm 's method ( Holm , 1979 ) .", "entities": [[36, 37, "MethodName", "LSTM"], [54, 55, "MethodName", "LSTM"]]}
{"text": "Japanese Zero Anaphora Resolution Can Benefit from Parallel Texts Through Neural Transfer Learning", "entities": [[11, 13, "TaskName", "Transfer Learning"]]}
{"text": "Parallel texts of Japanese and a non - pro - drop language have the potential of improving the performance of Japanese zero anaphora resolution ( ZAR ) because pronouns dropped in the former are usually mentioned explicitly in the latter . However , rule - based cross - lingual transfer is hampered by error propagation in an NLP pipeline and the frequent lack of transparency in translation correspondences . In this paper , we propose implicit transfer by injecting machine translation ( MT ) as an intermediate task between pretraining and ZAR . We employ a pretrained BERT model to initialize the encoder part of the encoder - decoder model for MT , and eject the encoder part for finetuning on ZAR . The proposed framework empirically demonstrates that ZAR performance can be improved by transfer learning from MT . In addition , we find that the incorporation of the masked language model training into MT leads to further gains .", "entities": [[46, 50, "TaskName", "cross - lingual transfer"], [79, 81, "TaskName", "machine translation"], [97, 98, "MethodName", "BERT"], [135, 137, "TaskName", "transfer learning"]]}
{"text": "Figuring out who did what to whom is an essential part of natural language understanding . This is , however , especially challenging for so - called prodrop languages like Japanese and Chinese because they usually omit pronouns that are inferable from context . The task of identifying the referent of such a dropped element , as illustrated in Figure 1 ( a ) , is referred to as zero anaphora resolution ( ZAR ) . Although Japanese ZAR saw a performance boost with the introduction of BERT ( Ueda et al , 2020 ; Konno et al , 2020 ) , there is still a good amount of room for improvement . A major barrier to improvement is the scarcity of training data . The number of annotated sentences is the order of tens of thousands or less ( Kawahara et al , 2002 ; Hangyo et al , 2012 ; Iida et al , 2017 ) , and the considerable linguistic expertise required for annotation makes drastic corpus expansion impractical . Previous attempts to overcome this limitation exploit orders - of - magnitude larger parallel texts of Japanese and English , a non - pro - drop language ( Nakaiwa , 1999 ; Furukawa et al , 2017 ) . The key idea is that Japanese zero pronouns can be recovered from parallel texts because they are usually mentioned explicitly in English , as in Figure 1 ( b ) . If translation correspondences are identified and the anaphoric relation in English is identified , then we can identify the antecedent of the omitted argument in Japanese . Their rule - based transfer from English to Japanese had met with limited success , however . It is prone to error propagation due to its dependence on word alignment , parsing , and English coreference resolution . More importantly , the great linguistic differences between the two language often lead to parallel sentences without transparent syntactic correspondences ( Figure 1 ( c ) ) . In this paper , we propose neural transfer learning from machine translation ( MT ) . By generating English translations , a neural MT model should be able to implicitly recover omitted Japanese pronouns , thanks to its expressiveness and large training data . We expect the knowledge gained during MT training to be transferred to ZAR . Given that state - of - the - art ZAR models are based on BERT ( Ueda et al , 2020 ; Konno et al , 2020Konno et al , , 2021 , it is a natural choice to explore intermediate task transfer learning ( Phang et al , 2018 ; Wang et al , 2019a ; Pruksachatkun et al , 2020 ; Vu et al , 2020 ) : A pretrained BERT model is first trained on MT and the resultant model is then fine - tuned on ZAR . 1 A key challenge to this approach is a mismatch in model architectures . While BERT is an encoder , the dominant paradigm of neural MT is the encoder - decoder . Although both share Transformer ( Vaswani et al , 2017 ) The nominative argument of the underlined predicate is omitted . The goal of the task is to detect the omission and to identify its antecedent \" son \" . ( b ) The corresponding English text . The omitted argument in Japanese is present as a pronoun in English . ( c ) A Japanese - English pair ( Nabeshima and Brooks , 2020 , p. 74 ) whose correspondences are too obscure for rule - based transfer . Because Japanese generally avoids having inanimate agents with animate patients , the English inanimate - subject sentence corresponds to two animate - subject clauses in Japanese , with two exophoric references to the reader ( i.e. , you ) . it is non - trivial to combine the two distinct architectures , with the goal to help the former . We use a pretrained BERT model to initialize the encoder part of the encoder - decoder model for MT . While this technique was previously used by Imamura and Sumita ( 2019 ) and Clinchant et al ( 2019 ) , they both aimed at improving MT performance . We show that by ejecting the encoder part for use in fine - tuning ( Figure 2 ) , we can achieve performance improvements in ZAR . We also demonstrate further improvements can be brought by incorporating encoder - side masked language model ( MLM ) training into the intermediate training on MT .", "entities": [[12, 15, "TaskName", "natural language understanding"], [87, 88, "MethodName", "BERT"], [298, 300, "TaskName", "word alignment"], [305, 307, "TaskName", "coreference resolution"], [343, 345, "TaskName", "transfer learning"], [346, 348, "TaskName", "machine translation"], [408, 409, "MethodName", "BERT"], [436, 438, "TaskName", "transfer learning"], [466, 467, "MethodName", "BERT"], [500, 501, "MethodName", "BERT"], [520, 521, "MethodName", "Transformer"], [671, 672, "MethodName", "BERT"], [760, 761, "DatasetName", "MLM"]]}
{"text": "ZAR has been extensively studied in major East Asian languages , Chinese and Korean as well as Japanese , which not only omit contextually inferable pronouns but also show no verbal agreement for person , number , or gender ( Park et al , 2015 ; Song et al , 2020 ; Kim et al , 2021 ) . While supervised learning is the standard approach to ZAR ( Iida et al , 2016 ; Ouchi et al , 2017 ; Shibata and Kurohashi , 2018 ) , training data are so small that additional resources are clearly needed . Early studies work on case frame construction from a large raw corpus ( Sasano et al , 2008 ; Sasano and Kurohashi , 2011 ; Yamashiro et al , 2018 ) , pseudo training data generation , and adversarial training ( Kurita et al , 2018 ) . These efforts are , however , overshadowed by the surprising effectiveness of BERT 's pretraining ( Ueda et al , 2020 ; Konno et al , 2020 ) . Adopting BERT , recent studies seek gains through multi - task learning ( Ueda et al , 2020 ) , data augmentation ( Konno et al , 2020 ) , and an intermediate task tailored to ZAR ( Konno et al , 2021 ) . The multi - task learning approach of Ueda et al ( 2020 ) covers verbal predicate analysis ( which subsumes ZAR ) , and nominal predicate analysis , coreference resolution , and bridging anaphora resolution . Their method is used as a state - of - the - art baseline in our experiments . Konno et al ( 2020 ) perform data augmentation by simply masking some tokens . They found that performance gains were achieved by selecting target tokens by part of speech . Konno et al ( 2021 ) introduce a more elaborate masking strategy as a ZAR - specific intermediate task They spot multiple occurrences of the same noun phrase , mask one of them , and force the model to identify the pseudo - antecedent . Our use of parallel texts in ZAR is inspired by Nakaiwa ( 1999 ) and Furukawa et al ( 2017 ) , who identify a multi - hop link from a Japanese zero pronoun to its Japanese antecedent via English counterparts . Their rule - based methods suffer from accumulated errors and syntactically non - transparent correspondences . In addition , they do not handle inter - sentential anaphora , a non - negligible subtype of anaphora we cover in this paper . While we exploit MT to improve the performance of ZAR , the exploitation in the reverse direction has been studied . A line of research has been done on Chinese zero pronoun prediction ( ZPP ) with a primary aim of improving Chinese - English translation ( Wang et al , 2016 ( Wang et al , , 2018 ( Wang et al , , 2019b . ZPP is different from ZAR in that it does not identify antecedents . This is understandable given that classification of zero pronouns into overt ones suffices for MT . Although Wang et al ( 2019b ) open question whether MT helps ZAR as well .", "entities": [[160, 161, "MethodName", "BERT"], [178, 179, "MethodName", "BERT"], [185, 189, "TaskName", "multi - task learning"], [197, 199, "TaskName", "data augmentation"], [223, 227, "TaskName", "multi - task learning"], [250, 252, "TaskName", "coreference resolution"], [254, 257, "TaskName", "bridging anaphora resolution"], [283, 285, "TaskName", "data augmentation"]]}
{"text": "Inspired by the great success of the pretraining / finetuning paradigm on a broad range of tasks ( Peters et al , 2018 ; Devlin et al , 2019 ) , a line of research inserts an intermediate task between pretraining and fine - tuning on a target task ( Phang et al , 2018 ; Wang et al , 2019a ; Pruksachatkun et al , 2020 ) . However , Wang et al ( 2019a ) found that MT used as an intermediate task led to performance degeneration in various target tasks , such as natural language inference and sentiment classification . 2 They argue that the considerable difference between MLM pretraining and MT causes catastrophic forgetting ( CF ) . Pruksachatkun et al ( 2020 ) suggest injecting the MLM objective during intermediate training as a possible way to mitigate CF , which we empirically test in this paper .", "entities": [[0, 1, "DatasetName", "Inspired"], [96, 99, "TaskName", "natural language inference"], [111, 112, "DatasetName", "MLM"], [131, 132, "DatasetName", "MLM"]]}
{"text": "Motivated by BERT 's success in a wide range of applications , some studies incorporate BERT into MT models . A straightforward way to do this is to initialize the encoder part of the encoder - decoder with pretrained BERT , but it has had mixed success at best ( Clinchant et al , 2019 ; Zhu et al , 2020 ) . Abandoning this approach , Zhang et al ( 2020 ) simply use BERT as a supplier of context - aware embeddings to their own encoder - decoder model . Similarly , Guo et al ( 2020 ) stack adapter layers on top of two frozen BERT models to use them as the encoder and decoder of a non - autoregressive MT 2 We suspect that the poor performance resulted in part from their excessively simple decoder , a single - layer LSTM .", "entities": [[2, 3, "MethodName", "BERT"], [15, 16, "MethodName", "BERT"], [39, 40, "MethodName", "BERT"], [75, 76, "MethodName", "BERT"], [108, 109, "MethodName", "BERT"], [144, 145, "MethodName", "LSTM"]]}
{"text": "[ author ] [ NA ] ! [ CLS ] [ author ] [ NA ] ! Figure 3 : ZAR as argument selection . model . However , these methods can not be adopted for our purpose because we want BERT itself to learn from MT . Imamura and Sumita ( 2019 ) manage to maintain the straightforward approach by adopting a twostage training procedure : In the first stage , only the decoder is updated with the encoder frozen , while in the second stage , the entire model is updated . Although they offer some insights , it remains unclear how best to exploit BERT when MT is an intermediate task , not the target task .", "entities": [[41, 42, "MethodName", "BERT"], [107, 108, "MethodName", "BERT"]]}
{"text": "We adopt a ZAR model of Ueda et al ( 2020 ) , which adds a thin layer on top of BERT during fine - tuning to solve ZAR and related tasks ( Section 3.1 ) . Instead of directly moving from MLM pretraining to fine - tuning on ZAR , we inject MT as an intermediate task ( Section 3.2 ) . In addition , we introduce the MLM training objective during the intermediate training ( Section 3.3 ) .", "entities": [[21, 22, "MethodName", "BERT"], [42, 43, "DatasetName", "MLM"], [69, 70, "DatasetName", "MLM"]]}
{"text": "Our main proposal is to use MT as an intermediate task prior to fine - tuning on ZAR . Following Imamura and Sumita ( 2019 ) and Clinchant et al ( 2019 ) , we use a pretrained BERT to initialize the encoder part of the Transformer - based encoderdecoder model while the decoder is randomly initialized . After the intermediate training on MT , we extract the encoder and move on to fine - tuning on ZAR and related tasks ( Figure 2 ) . Specifically , we test the following two procedures for intermediate training : One - stage optimization The entire model is updated throughout the training . Two - stage optimization In the first stage , the encoder is frozen and only the decoder is updated . In the second stage , the entire model is updated ( Imamura and Sumita , 2019 ) .", "entities": [[38, 39, "MethodName", "BERT"], [46, 47, "MethodName", "Transformer"]]}
{"text": "ZAR We used two corpora in our experiments : the Kyoto University Web Document Lead Corpus ( Hangyo et al , 2012 ) and the Kyoto University Text Corpus ( Kawahara et al , 2002 ) . Based on their genres , we refer to them as the Web and News , respectively . These corpora have been widely used in previous studies ( Shibata and Kurohashi , 2018 ; Kurita et al , 2018 ; Ueda et al , 2020 ) . They contained manual annotation for predicate - argument structures ( including zero anaphora ) as well as word segmentation , part - of - speech tags , dependency relations , and coreference chains . We split the datasets into training , validation , and test sets following the published setting , where the ratio was around 0.75:0.1:0.15 . Key statistics are shown in Table 1 . MT We used a Japanese - English parallel corpus of newspaper articles distributed by the Yomiuri Shimbun . 3 It consisted of about 1.3 million sentence pairs 4 with sentence alignment scores . We discarded pairs with scores of 0 . Because the task of interest , ZAR , required inter - sentential reasoning , consecutive sentences were concatenated into chunks , with the maximum number of tokens equal to that of ZAR . As a result , we obtained around 373 , 000 , 21 , 000 , and 21 , 000 chunks for the training , validation , and test data , respectively . Japanese sentences were split into words using the morphological analyzer MeCab with the Juman dictionary ( Kudo et al , 2004 ) . 5 Both Japanese and English texts underwent subword tokenization . We used Subword - NMT ( Sennrich et al , 2016 ) for Japanese and SentencePiece ( Kudo and Richardson , 2018 ) for English . We used separate vocabularies for Japanese and English , with the vocabulary sizes of around 32 , 000 and 16 , 000 , respectively .", "entities": [[103, 106, "DatasetName", "part - of"], [188, 189, "DatasetName", "0"], [303, 304, "MethodName", "SentencePiece"]]}
{"text": "Table 2 summarizes the experimental results . Our baseline is Ueda et al ( 2020 ) , who drastically outperformed previous models , thanks to BERT . + MT refers to the model with intermediate training on MT while + MT w/ MLM corresponds to the model that incorporated the MLM objective into MT . We can see that MT combined with MLM performed the best and that the gains reached 1.6 points for both the Web and News . Tables 3 and 4 provide more detailed results . For comparison , we performed additional pretraining with ordinary MLM on the Japanese part of the parallel corpus ( denoted as + MLM ) , because the possibility remained that the model simply took advantage of additional data . The subsequent two blocks compare one - stage ( unmarked ) optimization with two - stage optimization . MT yielded gains on all settings . The gains were consistent across anaphora categories . Although + MLM somehow beat the baseline , it was outperformed by most models trained on MT , ruling out the possibility that the gains were solely attributed to extra data . We can conclude that Japanese ZAR benefits from parallel texts through neural transfer learning . Two - stage optimization showed mixed results . It worked for the Web but did not for the News . What is worse , its combination with MLM led to performance degeneration on both datasets . MLM achieved superior performance as it worked well in all settings . The gains were larger with one - stage optimization than with two - stage optimization ( 1.4 vs. 0.3 on the Web ) .", "entities": [[25, 26, "MethodName", "BERT"], [42, 43, "DatasetName", "MLM"], [50, 51, "DatasetName", "MLM"], [62, 63, "DatasetName", "MLM"], [98, 99, "DatasetName", "MLM"], [111, 112, "DatasetName", "MLM"], [163, 164, "DatasetName", "MLM"], [205, 207, "TaskName", "transfer learning"], [235, 236, "DatasetName", "MLM"], [244, 245, "DatasetName", "MLM"]]}
{"text": "The MLM objective during intermediate training on MT is shown to be very effective , but why ? Pruksachatkun et al ( 2020 ) conjecture that it would mitigate catastrophic forgetting ( CF ) , but this is not the sole explanation . In fact , Konno et al ( 2020 ) see token masking as a way to augment data .", "entities": [[1, 2, "DatasetName", "MLM"]]}
{"text": "Due to space limitation , we have limited our focus to BERT , but for the sake of future practitioners , we would like to briefly note that we extensively tested BART and its variants before switching to BERT . Unlike BERT , BART is an encoder - decoder model pretrained on a monolingual corpus ( original ) or a non - parallel multilingual corpus ( mBART ) . Because MT requires the encoder - decoder architecture , maintaining the model architecture between pretraining and intermediate training looked promising to us . We specifically tested ( 1 ) the officially distributed mBART model , ( 2 ) a BART model we pretrained on Japanese Wikipedia , and ( 3 ) an mBART model we pretrained on Japanese and English texts . During fine - tuning , we added the ZAR argument selection layer on top of either the encoder or the decoder . Unfortunately , gains from MT intermediate training were marginal for these models . A more serious problem was that they came close to but rarely outperformed the strong BERT baseline . We gave up identifying the cause of poorer performance because it was extremely hard to apply comparable experimental conditions to large pretrained models .", "entities": [[11, 12, "MethodName", "BERT"], [31, 32, "MethodName", "BART"], [38, 39, "MethodName", "BERT"], [41, 42, "MethodName", "BERT"], [43, 44, "MethodName", "BART"], [66, 67, "MethodName", "mBART"], [101, 102, "MethodName", "mBART"], [108, 109, "MethodName", "BART"], [121, 122, "MethodName", "mBART"], [181, 182, "MethodName", "BERT"]]}
{"text": "In this paper , we proposed to exploit parallel texts for Japanese zero anaphora resolution ( ZAR ) by inserting machine translation ( MT ) as an intermediate task between masked language model ( MLM ) pretraining and fine - tuning on ZAR . Although previous studies reported negative results on the use of MT as an intermediate task , we demonstrated that it did work for Japanese ZAR . Our analysis suggests that the intermediate training on MT simultaneously improved the model 's ability to translate Japanese zero pronouns and the ZAR performance . We bridged the gap between BERT - based ZAR and the encoder - decoder architecture for MT by initializing the encoder part of the MT model with a pretrained BERT . Previous studies focusing on MT reported mixed results on this approach , but again , we demonstrated its considerable positive impact on ZAR . We found that incorporating the MLM objective into the intermediate training was particularly effective . Our experimental results were consistent with the speculation that MLM mitigated catastrophic forgetting during intermediate training . With neural transfer learning , we successfully revived the old idea that Japanese ZAR can benefit from parallel texts ( Nakaiwa , 1999 ) . Thanks to the astonishing flexibility of neural networks , we would probably be able to connect ZAR to other tasks through transfer learning . The example in which MT apparently helped ZAR . The nominative zero pronoun of \" \u3042\u308a \" ( is ) was correctly translated as \" the school \" . The model also succeeded in identifying its antecedent \" \u5b66 \u6821 \" ( school ) . ( b ) The example in which MT was not helpful . The model successfully translated the nominative zero pronoun of the underlined predicate , \" \u3067 \" ( be ) , as \" He \" . It misidentified its antecedent , however .", "entities": [[20, 22, "TaskName", "machine translation"], [34, 35, "DatasetName", "MLM"], [100, 101, "MethodName", "BERT"], [124, 125, "MethodName", "BERT"], [155, 156, "DatasetName", "MLM"], [174, 175, "DatasetName", "MLM"], [184, 186, "TaskName", "transfer learning"], [228, 230, "TaskName", "transfer learning"]]}
{"text": "Exploratory Analysis for Ontology Learning from Social Events on Social Media Streaming in Spanish", "entities": [[3, 4, "MethodName", "Ontology"]]}
{"text": "The problem of event analysis in Spanish social media streaming is that of difficulty on automatically processing the data as well as obtaining the most relevant information , such as mentioned by Derczynski et al ( 2015 ) . An event is defined as a real world occurrence that takes place in a specific time and space ; Atefeh and Khreich ( 2013 ) identifies these occurrences by the entities that took part on it as well as the activities done in it . This project focuses on researching about the viability of modeling these events as ontologies using an automatic approach for entities and relationships extraction in order to obtain relevant information about the event in case . Spanish data from Twitter was used as a study case and tested with the developed application .", "entities": [[32, 33, "DatasetName", "Derczynski"]]}
{"text": "According to Lobzhanidze et al ( 2013 ) , globalization and the increased use of social networks has made it possible for news and events related information to be propagated in a much faster manner to every part of the world . It is in this context that event analysis is the most relevant since , as Valkanas and Gunopulos ( 2013 ) mention , now there is more data available to study and analyze than ever before . An event is defined as a real world occurrence that takes place in a specific time and space ; Atefeh and Khreich ( 2013 ) identifies these occurrences by the entities that took part on it as well as the activities done in it . Events will be the main study object in this paper and , more specifically , event data in Spanish obtained from Twitter will be used to test the different methods and techniques exposed on each Section . In order to effectively analyze events there are two steps that need to be taken into consideration as mentioned in Kumbla ( 2016 ) : ( 1 ) event data acquisition , and ( 2 ) event data processing . The first step is the one that benefits the most by social media streaming since more data is available , though one of the downsides to this is that the data is usually not ready to be used right away and most of the times a preprocessing step needs to happen . This step is further explained on section Section 3 . The second step will be the main focus on this paper since the biggest problem on event data analysis in Spanish is this one . In particular , automatic approaches for entities and relationships extraction will be presented on Section 4 . The remainder of this paper is organized as follows . In Section 2 some relevant related work is exposed . Later , in Section 3 the event acquisition process is further expanded upon . The ontology structure used for the events representation as well as the algorithms employed in order to obtain entities and relationships between these are further explained on Section 4 . Section 5 introduces a simple application developed in order to make use of the algorithms and techniques mentioned on the previous sections . On section 6 we compare the results obtained with manually created ontologies and obtain precision and recall values for each case . Finally , concluding remarks are provided in Section 7 . In Al - Smadi and Qawasmeh ( 2016 ) an unsupervised approach for event extraction from Arabic tweets is discussed . Entities appearing in the data are linked to corresponding entities found on Wikipedia and DBpedia through an ontology based knowledge base . The entities from the data are extracted based on rules related to the Arabic language . In Derczynski et al ( 2015 ) a comparative evaluation of different NER is done based on three different datasets . Also , some common challenges or errors when handling data from Twitter are presented as well as methods for reducing microblog noise through pre - processing such as language identification , POStagging and normalization . In Ilknur et al ( 2011 ) a framework for learning relations between entities in Twitter is presented . This framework allows for entities as well as entity types or topics to be detected , which results in a graph connecting semantically enriched resources to their respective entities . Then relation discovery strategies are employed to detect pair of entities that have a certain type of relationship in a specific period of time . In Raimond and Abdallah ( 2007 ) an event ontology is described . This model also contains some key characteristics such as place , location , agents and products . On the other hand , event - subevent relationships are used to build the related ontologies . This model was developed for the Center for Digital Music and tested by structuring proceedings and concert descriptions . Finally , an ontology model for events is proposed in which entities are extracted using the CMU tweet analyzer and relationships are inferred from Wikipedia , DBpedia and Web data . This approach also uses a POS - tagging step in order to obtain the initial set of entities to process .", "entities": [[340, 341, "MethodName", "ontology"], [437, 439, "TaskName", "event extraction"], [459, 460, "DatasetName", "DBpedia"], [462, 463, "MethodName", "ontology"], [484, 485, "DatasetName", "Derczynski"], [495, 496, "TaskName", "NER"], [532, 534, "TaskName", "language identification"], [622, 623, "MethodName", "ontology"], [682, 683, "MethodName", "ontology"], [705, 706, "DatasetName", "DBpedia"]]}
{"text": "Ontology learning is defined by Cimiano ( 2006 ) as the automatic acquisition of a domain model from some dataset . In this paper we focus on applying ontology learning techniques for data represented as text . Cimiano points towards two main approaches for ontology learning : 1 . Machine learning 2 . Statistical approach Statistical based algorithms are further discussed on Sections 4.3 and 4.4 .", "entities": [[0, 1, "MethodName", "Ontology"], [28, 29, "MethodName", "ontology"], [44, 45, "MethodName", "ontology"]]}
{"text": "Before we start using different techniques in order to populate an ontology or to learn entities and relationships from the data that was retrieved previously , an ontology structure had to be defined . The ontology structure that we define will point us towards different techniques depending on the information that must be retrieved to populate this particular structure . Therefore , the proposed ontology structure in this paper is defined on Figure 1 . The ontology will be populated by such triples composed of ( Entity , Temporal entity , object ) . Where Entity denotes a subject that interacts in the event , Temporal entity refers to the date when the particular activity takes place and object is the recipient of the activity .", "entities": [[11, 12, "MethodName", "ontology"], [27, 28, "MethodName", "ontology"], [35, 36, "MethodName", "ontology"], [64, 65, "MethodName", "ontology"], [76, 77, "MethodName", "ontology"]]}
{"text": "This was one of the main points of interest and research on this paper , how to select the most representative entities for the event in order to not overwhelm people analyzing the results but also to not present too little or irrelevant information . In order to achieve this , two initial tools for entity retrieval were tested : 1 . Stanford NER : The Stanford NER used with a trained Spanish model from late 2016 was used in order to retrieve persons , entities and organizations and group them all together as entities . 2 . UDPipe : UDPipe allows to parse text in order to obtain the grammatical categories of the words in each sentence , as well as the syntactic dependencies or syntactic tree that envelops the whole sentence . The entities are obtained from the grammatical category PROPN . These two approaches were then implemented and tested with each dataset and a manual comparison was made between the entities that each approach captured . The results showed that , while the Stanford NER worked really well in the case where the tweets were news related or had a more formal undertone , such as in the case of the Australian Open , it failed to find a lot of basic entities in the other two datasets where the data was more unstructured as one would very likely find when working on social streaming . Also , the Stanford NER has heavily influenced by correct capitalization and punctuation , whereas UDPipe was n't influenced by these factors as much . Because of this , UDPipe was chosen as the main initial entity extraction tool moving forward . After having a set of initial entities , further processing steps were taken to ensure a better result .", "entities": [[55, 57, "TaskName", "entity retrieval"], [63, 64, "TaskName", "NER"], [67, 68, "TaskName", "NER"], [177, 178, "TaskName", "NER"], [243, 244, "TaskName", "NER"]]}
{"text": "Entity clustering was done on two stages . First , an algorithm for entity clustering was devised based on two metrics : 1 . Normalized frequency of two entities appearing in a single tweet : The frequency of appearance between two specific entities in tweets . 2 . Average Entity to entity distance in a tweet ( i.e. in the sentence \" Nadal venci\u00f3 a Federer \" , if both Nadal and Federer are identified as entities , they would have a distance of 3 for this tweet ) A threshold of 0.125 was set as the minimum normalized frequency for a pair of entities and a minimum average Entity to Entity distance of 1.65 . These two values were set based on experimentation with the resulting clustered entities from each dataset . After that , an approach based on Levenshtein distance ( minimum amount of additions , replacements or deletions needed to turn a word into another ) was employed , where two entities were clustered together if their distance was more than 0.9 times the length of the longest entity from the two . An example of this distance can be seen on Figure 2 . FCA is one of the approaches for entity extraction detailed on Cimiano ( 2006 ) . It is the one that garners the most focus on this book as the main set - theoretical approach based on verb - subject components . This approach is based on obtaining the formal context for a specific domain or dataset and then proceed to use it to create a hierarchy ontology . An example of how a formal context would look for a tourism domain knowledge can be seen on Table 1 . Cimiano ( 2006 ) bookable rentable rideable hotel X apartment X X bike X X X excursion X trip X In this paper we use the created formal contexts to discriminate between entities based on three metrics : Conditional ( n , v ) = P ( n , v ) = f ( n , v ) f ( v ) ( 1 ) P M I ( n , v ) = log 2 P ( n | v ) P ( n ) ( 2 ) Resnik ( n , v ) = SR ( v ) * P ( n | v ) ( 3 ) Where : 1 . f ( n , v ) = > Frequency of apparition of entity n with verb v 2 . f ( v ) = > Frequency of apparition of verb v with any entity And : SR ( v ) = n P ( n | v ) * log 2 P ( n | v ) P ( n ) ( 4 ) A threshold of 0.1 as a minimum value is set for all of the three aforementioned metrics ( Conditional , PMI and Resnik weights ) , meaning that the ( entity , verb ) pairs that not surpass this threshold for any of the three metrics are pruned .", "entities": [[265, 266, "MethodName", "ontology"]]}
{"text": "A desktop application was developed in order to allow for easier visualization of both the ontology and the resulting activities that each entity participated in , as well as the activities that create a relationship between two particular entities .", "entities": [[15, 16, "MethodName", "ontology"]]}
{"text": "In order to verify the approach applied for ontology extraction , we manually created ontologies for each test case where the most relevant entities and relationships are specified based on investigation related to these cases , these ontologies can be seen on Figures 5 , 6 and 7 . These ontologies were then presented to colleagues with more profound knowledge on each of the events for validation and were redone based on their feedback until they were accepted by them . From these ontologies we obtained precision and recall values for both entities and relationships for each case . These can be seen on Tables 2 , 3 and 4 : The main point of interest in these metrics lies on the precision , where the precision on the Australian Open case in quite higher than on the other two cases . From further inspection on the corresponding data we could infer that this was the case because a big part of the tweets for the Australian Open where either formal tweets made by users representing news outlets or by the players themselves . As for the other two cases , most of the tweets where a mix of news and discussion from common people about these events .", "entities": [[8, 9, "MethodName", "ontology"]]}
{"text": "We conclude that , while the methods exposed on this paper work good enough on cases such as the Australian Open one , there is still work to be done when the general public is more engaged on the event such as the cases of the Puente Piedra toll and the March against the corruption . This paper 's aim was to give a foundation and a initial stage of exploratory analysis on social media streaming in Spanish by using ontologies , after which future work could be based upon in order to expand the knowledge in the ontologies or use this analysis together with an event detection system in order to be able to both detect and analyze events in real time .", "entities": [[106, 108, "TaskName", "event detection"]]}
{"text": "Multilingual Paraphrase Generation For Bootstrapping New Features in Task - Oriented Dialog Systems", "entities": [[1, 3, "TaskName", "Paraphrase Generation"]]}
{"text": "The lack of labeled training data for new features is a common problem in rapidly changing real - world dialog systems . As a solution , we propose a multilingual paraphrase generation model that can be used to generate novel utterances for a target feature and target language . The generated utterances can be used to augment existing training data to improve intent classification and slot labeling models . We evaluate the quality of generated utterances using intrinsic evaluation metrics and by conducting downstream evaluation experiments with English as the source language and nine different target languages . Our method shows promise across languages , even in a zero - shot setting where no seed data is available .", "entities": [[30, 32, "TaskName", "paraphrase generation"], [62, 64, "TaskName", "intent classification"]]}
{"text": "Spoken language understanding is a core problem in task oriented dialog systems with the goal of understanding and formalizing the intent expressed by an utterance ( Tur and De Mori , 2011 ) . It is often modeled as intent classification ( IC ) , an utterance - level multi - class classification problem , and slot labeling ( SL ) , a sequence labeling problem over the utterance 's tokens . In recent years , approaches that train joint models for both tasks and that leverage powerful pre - trained neural models greatly improved the state - of - the - art performance on available benchmarks for IC and SL ( Louvan and Magnini , 2020 ; Weld et al , 2021 ) . A common challenge in real - world systems is the problem of feature bootstrapping : If a new feature should be supported , the label space needs to be extended with new intent or slot labels , and the model needs to be retrained to learn to classify corresponding utterances . However , labeled examples for the new feature are typically limited to a small set of seed examples , as the collection of more annotations would make feature expansion costly and slow . As a possible solution , previous work explored the automatic generation of paraphrases to augment the seed data ( Malandrakis et al , 2019 ; Cho et al , 2019 ; Jolly et al , 2020 ) . In this work , we study feature bootstrapping in the case of a multilingual dialog system . Many large - scale real - world dialog systems , e.g. Apple 's Siri , Amazon 's Alexa and Google 's Assistant , support interactions in multiple languages . In such systems , the coverage of languages and the range of features is continuously expanded . That can lead to differences in the supported intent and slot labels across languages , in particular if a new language is added later or if new features are not rolled out to all languages simultaneously . As a consequence , labeled data for a feature can be available in one language , but limited or completely absent in another . With multilingual paraphrase generation , we can benefit from this setup and improve data augmentation for data - scarce languages via cross - lingual transfer from data - rich languages . As a result , the data augmentation can not only be applied with seed data , i.e. in a few - shot setting , but even under zero - shot conditions with no seeds at all for the target language . To address this setup , we follow the recent work of Jolly et al ( 2020 ) , which proposes to use an encoder - decoder model that maps from structured meaning representations to corresponding utterances . Because such an input is language - agnostic , it is particularly well - suited for the multilingual setup . We make the following extensions : First , we port their model to a transformer - based architecture and allow multilingual training by adding the desired target language as a new input to the conditional generation . Second , we let the model generate slot labels along with tokens to alleviate the need for additional slot projection techniques . And third , we introduce improved paraphrase decoding methods that leverage a model - based selec - tion strategy . With that , we are able to generate labeled data for a new feature even in the zero - shot setting where no seeds are available at all . We evaluate our approach by simulating a crosslingual feature bootstrapping setting , either fewshot or zero - shot , on MultiATIS , a common IC / SL benchmark spanning nine languages . The experiments compare against several alternative methods , including previous work for mono - lingual paraphrase generation and machine translation . We find that our method produces paraphrases of high novelty and diversity and using it for IC / SL training shows promising downstream classification performance .", "entities": [[0, 3, "TaskName", "Spoken language understanding"], [39, 41, "TaskName", "intent classification"], [49, 53, "TaskName", "multi - class classification"], [283, 284, "DatasetName", "Google"], [373, 375, "TaskName", "paraphrase generation"], [384, 386, "TaskName", "data augmentation"], [392, 396, "TaskName", "cross - lingual transfer"], [407, 409, "TaskName", "data augmentation"], [435, 436, "DatasetName", "seeds"], [602, 603, "DatasetName", "seeds"], [655, 657, "TaskName", "paraphrase generation"], [658, 660, "TaskName", "machine translation"]]}
{"text": "In order to generate paraphrases , we train a multilingual paraphrase generation model that generates a paraphrase given a language , an intent and a set of slot types . The model architecture is outlined in Figure 1 . The model uses self - attention based encoder and decoder similar to the transformer ( Vaswani et al , 2017 ) . The encoder of the model receives as input the language embedding and the intent embedding , which are added to the slot embedding . Unlike the transformer model ( Vaswani et al , 2017 ) , we do not use the positional embedding in the encoder . This is because the order of the slot types in the input sequence does not matter and is thus made indistinguishable for the encoder . In order to generate paraphrases which can be used for data augmentation , we would need the slot annotations and the intents of the generations . Note that we already know the intent of the generated paraphrase since it is the same intent as specified while generating it . The slot annotations , however , are not readily obtained from the input slot types . We can make the slot annotations part of the output sequence by generating the slot label in BIO format in every alternate time step , which would be the slot label for the token generated in the previous time step . This enables the model to generate the slot annotations along with the paraphrase . An illustrative example is shown in Figure 1 .", "entities": [[10, 12, "TaskName", "paraphrase generation"], [143, 145, "TaskName", "data augmentation"]]}
{"text": "The generated paraphrases can be used to augment the existing training data . Since the training data we use is highly imbalanced , data augmentation might lead to disturbance in the original intent distribution . To ensure that the data augmentation process does not disturb the original intent distribution , we compute the number of samples to augment using the following constraint : the ratio of target intent to other intents for the target language should be the same as the ratio of target intent to other intents in the source language . Sometimes , using the above constraint results in a negligible number of samples for augmentation , in which cases we use a minimal number of samples ( see experiments ) .", "entities": [[23, 25, "TaskName", "data augmentation"], [39, 41, "TaskName", "data augmentation"], [53, 56, "HyperparameterName", "number of samples"], [103, 106, "HyperparameterName", "number of samples"], [116, 119, "HyperparameterName", "number of samples"]]}
{"text": "In addition to deciding how many paraphrases to augment , it is also crucial to decide which paraphrases to use . Preliminary experimental results showed that samping uniformly from all generated paraphrases does not lead to improvement over the baseline . Upon manual examination we found that not all the paraphrases belong to the desired target intent . To cope with that problem , we use the baseline downstream intent classification and slot labeling model , which is trained only on the existing data , to compute the likelihood of the generated paraphrases to belong to the target intent . We rank all the generated paraphrases based on these probabilities and select from the top of the pool for augmentation of the seed data .", "entities": [[69, 71, "TaskName", "intent classification"]]}
{"text": "We use the MultiATIS++ data ( Xu et al , 2020 ) , a parallel IC / SL corpus that was created by translating the original English dataset . It covers a total of 9 languages : English , Hindi , Turkish , German , French , Portuguese , Spanish , Japanese and Chinese . The languages encompass a diverse set of language families : Indo - European , Sino - Tibetan , Japonic and Altaic . Choosing target intents To reduce the number of experiments , we only choose three different intents for simulating the feature bootstrapping scenario . The MultiATIS++ dataset is highly imbalanced in terms of intent frequencies . For instance , 74 % of the English training data has the intent atis_flight and as many as 9 intents have less than 20 training samples . The trend is similar for the non - English languages . For choosing target intents for simulating the zero shot and few shot training data , we therefore consider the following three target intents : ( a ) atis_airfare , which is highly frequent , ( b ) atis_airline , which has medium frequency , and ( c ) atis_city which is scarce . Preprocessing We remove the samples in the MultiATIS++ data for which the number of tokens and the number of slot values do not match . 1 We also only consider the first intent for the samples that have multiple intent annotations . We show the data sizes after preprocessing in Table 1 . Training setup To simulate the feature bootstrapping scenario , we consider only 20 samples ( few shot setup ) or no samples at all ( zero shot setup ) from the MultiATIS++ data for a specific target intent in a target language . 2 Language setup We use English as the source language and consider 8 target languages ( Hindi , Turkish , German , French , Portuguese , Spanish , Japanese , Chinese ) simultaneously . This encourages the model parameters to be shared across all the 9 languages including the source language English . The purpose of this setup is to enable us to study the knowledge transfer across multiple target languages in addition to that from the source language . We train a single model for paraphrase generation on all the languages as well as a single multi - lingual downstream IC / SL model .", "entities": [[385, 387, "TaskName", "paraphrase generation"]]}
{"text": "In this paper , we proposed a multilingual paraphrase generation model that can be used for feature bootstrapping with or without seed data in the target language . In addition to generating a paraphrase , the model also generates the associated slot labels , enabling the generation to be used directly for data augmentation to existing training data . Our method is language agnostic and scalable , with no dependencies on pre - trained models or additional data . We validate our method using experiments on the MultiATIS++ dataset containing utterances spanning 9 languages . Intrinsic evaluation shows that paraphrases generated using our approach have higher novelty and diversity in comparison to CVAE seq2seq based paraphrase generation . Additionally , downstream evaluation shows that using the generated paraphrases for data augmentation results in improvements over baseline and related techniques in a wide range of languages and setups . To the best of our knowledge , this is the first successful exploration of generating paraphrases for SLU in a cross - lingual setup . In the future , we would like to explore strategies to exploit monolingual data in the target languages to further refine the paraphrase generation . We would also like to leverage pre - trained multilingual text - to - text models such as mT5 ( Xue et al , 2020 ) for multilingual paraphrase generation in the dialog system domain .", "entities": [[8, 10, "TaskName", "paraphrase generation"], [52, 54, "TaskName", "data augmentation"], [112, 113, "MethodName", "CVAE"], [113, 114, "MethodName", "seq2seq"], [115, 117, "TaskName", "paraphrase generation"], [129, 131, "TaskName", "data augmentation"], [195, 197, "TaskName", "paraphrase generation"], [216, 217, "MethodName", "mT5"], [226, 228, "TaskName", "paraphrase generation"]]}
{"text": "We look into the task of generalizing word embeddings : given a set of pre - trained word vectors over a finite vocabulary , the goal is to predict embedding vectors for out - of - vocabulary words , without extra contextual information . We rely solely on the spellings of words and propose a model , along with an efficient algorithm , that simultaneously models subword segmentation and computes subword - based compositional word embedding . We call the model probabilistic bag - of - subwords ( PBoS ) , as it applies bag - of - subwords for all possible segmentations based on their likelihood . Inspections and affix prediction experiment show that PBoS is able to produce meaningful subword segmentations and subword rankings without any source of explicit morphological knowledge . Word similarity and POS tagging experiments show clear advantages of PBoS over previous subword - level models in the quality of generated word embeddings across languages .", "entities": [[7, 9, "TaskName", "word embeddings"], [134, 136, "TaskName", "Word similarity"], [156, 158, "TaskName", "word embeddings"]]}
{"text": "Following the above intuition , in this section we describe the PBoS model in detail . We first develop a model that segments a word into subword and associates each subword segmentation with a likelihood based on the meaningfulness of each subword segment . We then apply BoS over each segmentation to compose a \" segmentation vector \" . The final word embedding vector is then the probabilistic expectation of all the segmentation vectors . The subword segmentation and likelihood association part require no explicit source of morphological knowledge and are tightly integrated with the word vector composition part , which in turn gives rise to an efficient algorithm that considers all possible segmentations simultaneously ( Section 3 ) . The model can be trained by fitting a set of pre - trained word embeddings .", "entities": [[133, 135, "TaskName", "word embeddings"]]}
{"text": "A subword transition graph for word w is a directed acyclic graph G w = ( N w , E w ) . Let l = | w | . The vertices N w = { 0 , . . . , l } correspond to the positions between w [ i ] and w [ i + 1 ] for all i [ l \u2212 1 ] , as well as to the beginning ( vertiex 0 ) and the end ( vertex l ) of w. Each edge ( i , j ) E w = { ( i , j ) : 0 \u2264 i < j \u2264 l } corresponds to subword w [ i : j ] . We use G w as a useful image for developing our model . Proposition 1 . Paths from 0 to | w | in G w are in one - to - one correspondence to segmentations of w. Proposition 2 . There are 2 | w | \u22121 different possible segmentations for word w. Each edge ( i , j ) is associated with a weight p w [ i : j ] - how likely w [ i : j ] itself is a meaningful subword . We model the likelihood of segmentation g being a segmentation of w as being proportional to the product of all its subword likelihood - the 0 1 2 3 4 5 6 h p \" h \" i p \" i \" g p \" g \" h p \" h \" e p \" e \" r p \" r \" hi p \" hi \" gher p \" gher \" gh p \" gh \" her p \" her \" high p \" high \" er p \" er \" Figure 1 : Diagram of probabilistic subwords transitions for word \" higher \" . Some edges are omitted to reduce clutter . Each edge is labeled by a subword s of the word , associated with ps . Bold edges constituent a path from node 0 to 6 , corresponding to the segmentation of the word into \" high \" and \" er \" . transition along a path from 0 to | w | in G w : p g | w \u221d s g p s . ( 2 ) Example . Figure 1 illustrates G w for word w = \" higher \" of length 6 . Bold edges ( 0 , 4 ) and ( 4 , 6 ) form a path from 0 to 6 , which corresponds to the segmentation ( \" high \" , \" er \" ) . The likelihood p ( \" high \" , \" er \" ) | w of this particular segmentation is proportional to p \" high \" p \" er \" - the product of weights along the path .", "entities": [[36, 37, "DatasetName", "0"], [77, 78, "DatasetName", "0"], [105, 106, "DatasetName", "0"], [141, 142, "DatasetName", "0"], [236, 237, "DatasetName", "0"], [348, 349, "DatasetName", "0"], [373, 374, "DatasetName", "0"], [416, 417, "DatasetName", "0"], [430, 431, "DatasetName", "0"]]}
{"text": "We design experiments to answer two questions : Do the segmentation likelihood and subword weights computed by PBoS align with their meaningfulness ? Are the word embedding vectors generated by PBoS of good quality ? For the former , we inspect segmentation results and subword weights ( Section 4.1 ) , and see how good they are at predicting word affixes ( Section 4.2 ) . For the latter , we evaluate the word embeddings composed by PBoS at word similarity task ( Section 4.3 ) and part - of - speech ( POS ) tagging task ( Section 4.4 ) . Due to the page limit , we only report the most relevant settings and results in this section . Other details , including hardware , running time and detailed list of hyperparameters , can be found in Appendix A.", "entities": [[73, 75, "TaskName", "word embeddings"], [79, 81, "TaskName", "word similarity"], [87, 90, "DatasetName", "part - of"]]}
{"text": "In this subsection , we provide anecdotal evidence that PBoS is able to assign meaningful segmentation likelihood and subword weights . Table 1 shows top subword segmentations and subsequent top subwords calculated by PBoS for some example word , ranked by their likelihood and weights respectively . The calculation is based on the word frequency derived from the Google Web Trillion Word Corpus 6 . We use the same list for word probability p w throughout our experiments if not otherwise mentioned . All other settings are the same as described for PBoS in Section 4.3 . We can see the segmentation likelihood and subword weight favors the whole words as subword segments if the word appears in the word list , e.g. \" higher \" , \" farmland \" . This allows the model to closely mimic the word embeddings for frequent words that are probably part of the target vectors . Second to the whole - word segmentation , or when the word is rare , e.g. \" penpineanpplepie \" , \" paradichlorobenzene \" , we see that PBoS gives higher likelihood to meaningful segmentations such as \" high / er \" , \" farm / land \" , \" pen / pineapple / pie \" and \" para / dichlorobenzene\"against other possible segmentations . 7 Subsequently , respective subword segments get higher weights among all possible subwords for the word , often by a good amount . This behavior would help PBoS to focus on meaningful subwords when composing word embedding . The fact that this can be achieved without any explicit source of morphological knowledge is itself interesting .", "entities": [[58, 59, "DatasetName", "Google"], [139, 141, "TaskName", "word embeddings"]]}
{"text": "Popular word embedding methods , such as word2vec ( Mikolov et al , 2013 ) , GloVe ( Pennington et al , 2014 ) , often assume finite - size vocabularies , giving rise to the problem of OOV words . FastText attempted to alleviate the problem using subword - level model , and was followed by interests of using subword information to improve word embedding ( Wieting et al , 2016 ; Cao and Lu , 2017 ; Li et al , 2017 ; Athiwaratkun et al , 2018 ; Li et al , 2018 ; Salle and Villavicencio , 2018 ; Xu et al , 2019 ; Zhu et al , 2019 ) . Among them are Charagram by Wieting et al ( 2016 ) which , albeit trained on specific downstream tasks , is similar to BoS followed by a non - linear activation , and the systematic evaluation by Zhu et al ( 2019 ) over various choices of word composition functions and subword segmentation methods . However , all works above either pay little attention to the interaction among subwords inside a given word , or treat subword segmentation and composing word representation as separate problems . Another interesting thread of works ( Oshikiri , 2017 ; Kim et al , 2018aKim et al , , 2019 attempted to model language solely at the subword level and learn subword embeddings directly from text , providing evidence to the power of subword - level models , especially as the notion of word is thought doubtful by some linguistics ( Haspelmath , 2011 ) . Besides the recent interest in subwords , there have been long efforts of using morphology to improve word embedding ( Luong et al , 2013 ; Cotterell and Sch\u00fctze , 2015 ; Cui et al , 2015 ; Soricut and Och , 2015 ; Bhatia et al , 2016 ; Cao and Rei , 2016 ; Xu et al , 2018 ; \u00dcst\u00fcn et al , 2018 ; Edmiston and Stratos , 2018 ; Chaudhary et al , 2018 ; Park and Shin , 2018 ) . However , most of them require an external oracle , such as Morfessor ( Creutz and Lagus , 2002 ; Virpioja et al , 2013 ) , for the morphological segmentations of input words , limiting their power to the quality and availability of such segmenters . The only exception is the character LSTM model by Cao and Rei ( 2016 ) , which has shown some ability to recover the morphological boundary as a byproduct of learning word embedding . The most related works in generalizing pretrained word embeddings have been discussed in Section 1 and compared throughout the paper .", "entities": [[16, 17, "MethodName", "GloVe"], [41, 42, "MethodName", "FastText"], [408, 409, "MethodName", "LSTM"], [443, 445, "TaskName", "word embeddings"]]}
{"text": "We propose PBoS model for generalizing pretrained word embeddings without contextual information . PBoS simultaneously considers all possible subword segmentations of a word and derives meaningful subword weights that lead to better composed word embeddings . Experiments on segmentation results , affix prediction , word similarity , and POS tagging over 23 languages support the claim . In the future , it would be interesting to see if PBoS can also help with the task of learning word embedding , and how hashing would impact the quality of composed embedding while facilitating a more compact model .", "entities": [[7, 9, "TaskName", "word embeddings"], [33, 35, "TaskName", "word embeddings"], [44, 46, "TaskName", "word similarity"]]}
{"text": "epochs : The number of training epochs . lr : Learning rate . lr decay : Whether to set learning rate to be inversely proportional to the square root of the epoch number . normalize semb : Whether to normalize subword embeddings before composing word embeddings . prob eps : Default likelihood for unknown characters .", "entities": [[10, 12, "HyperparameterName", "Learning rate"], [19, 21, "HyperparameterName", "learning rate"], [31, 33, "HyperparameterName", "epoch number"], [44, 46, "TaskName", "word embeddings"], [48, 49, "HyperparameterName", "eps"]]}
{"text": "C : The inverse regularization term used by the logistic regression classifier .", "entities": [[9, 11, "MethodName", "logistic regression"]]}
{"text": "Table 6 and Table 8 show the hyperparameter values used in the word similarity experiment ( Section 4.3 ) . We transform all words in the benchmarks into lowercase , following the convention in FastText , BoS ( Zhao et al , 2018 ) , and KVQ - FH ( Sasaki et al , 2019 ) . During the evaluation , we use 0 as the similarity score for a pair of words if we can not get word vector for one of the words , or the magnitude of the word vector is too small . This is especially the case when we evaluate the target vectors , where OOV rates can be significant . Table 9 lists experimental result for word similarity in greater detail . Regarding the training epoch time , note that KVQ - FH uses GPU and is implemented using a deep learning library 17 with underlying optimized C code , whereas our PBoS is implemented using pure Python and uses only single thread CPU . We omit the prediction time for KVQ - FH , as we found it hard to separate the actual inference time from time used for other processes such as batching and data transfer between CPU and GPU . However , we believe the overall trend should be similar as for the training time . One may notice that the prediction time for BoS in Table 9 is different from what was reported at the end of Section 3 . This is largely because the BoS in Table 9 has a different ( smaller ) set of possible subwords to consider due to the subword length limits . In Section 3 , to fairly access the impact of subword weights computation , we ensure that BoS and PBoS work with the same set of possible subwords ( that used by PBoS in Section 4.3 ) , and thus observe a slight longer prediction time for BoS.", "entities": [[12, 14, "TaskName", "word similarity"], [34, 35, "MethodName", "FastText"], [63, 64, "DatasetName", "0"], [122, 124, "TaskName", "word similarity"]]}
{"text": "We use Wikipedia2Vec ( Yamada et al , 2020 ) as target vectors , and keep the most frequent 10k words to get decent OOV rates . The OOV rates and word similarity scores can be found in Table 10 . We do not clean or filter words as we did for the English word similarity , because we found it difficult to have a consistent way of pre - processing words across languages . For PBoS , we use the word frequencies from Polyglot for subword segmentation and subword weight calculation as the same for the multilingual POS tagging experiment ( Section 4.4 ) . We evaluate all the models on multilingual Word - Sim353 ( mWS ) and SemLex999 ( mSL ) from Leviant and Reichart ( 2015 ) , which is available for English , German , Italian and Russian . The dataset al o contains the relatedness ( rel ) and similarity ( sim ) benchmarks derived from mWS . We list the results for multilingual word similarity in Table 11 .", "entities": [[31, 33, "TaskName", "word similarity"], [54, 56, "TaskName", "word similarity"], [170, 172, "TaskName", "word similarity"]]}
{"text": "Knowledge Base Question Answering via Encoding of Complex Query Graphs", "entities": [[0, 4, "TaskName", "Knowledge Base Question Answering"]]}
{"text": "The knowledge - based question answering ( KBQA ) is a task which takes a natural language question as input and returns a factual answer using structured knowledge bases such as Freebase ( Bollacker et al , 2008 ) , YAGO ( Suchanek et al , 2007 ) and DBpedia ( Auer et al , 2007 ) . One simple example is a question like this : \" What 's the capital of the United States ? \" A common answer to such question is to identify the focus entity and the main relation predicate ( or a sequence ) in the question , and map the question to a triple fact query ( U S , capital , ? ) over KB . The object answers are returned by executing the query . The mapping above is typically learned from question - answer pairs through distant supervision . While the above question can be answered by querying a single predicate or predicate sequence in the KB , many other more complex questions can not , e.g. the question in Figure 1 . To answer the question \" What is the second longest river in United States \" , we need to infer several semantic clues : 1 ) the answer is contained by United States ; 2 ) the answer is a river ; 3 ) the answer ranks second by its length in descending order . Thus , multiple predicates are required to constrain the answer set , and we call such questions \" complex questions \" throughout this paper . For answering complex questions , it 's more important to understand the compositional semantic meanings of the question . As a classic branch of KBQA solutions , semantic parsing ( SP ) technique ( Berant et al , 2013 ; Yih et al , 2015 ; Hu et al , 2018 ) aims at learning semantic parse trees or equivalent query graphs 1 for representing semantic structures of the questions . For example in Figure 1 , the query graph forms a tree shape . The answer node A , serving as the root of the tree , is the variable vertex that represents the real answer entities . The focus nodes ( US , river , 2nd ) are extracted from the mentions of the question , and they constrain the answer node via predicate sequences in the knowledge base . Recently , neural network ( NN ) models have shown great promise in improving the performance of KBQA systems , and SP+NN techniques become the stateof - the - art on several KBQA datasets ( Qu et al , 2018 ; Bao et al , 2016 ) . According to the discussion above , our work extends the current research in the SP+NN direction . The common step of SP - based approaches is to first collect candidate query graphs using bottom up parsing ( Berant et al , 2013 ; Cai and Yates , 2013 ) or staged query generation methods ( Yih et al , 2015 ; Bao et al , 2016 ) , then predict the best graph mainly based on the semantic similarity with the given question . Existing NN - based methods follow an encode - andcompare framework for answering simple questions , where both the question and the predicate sequence are encoded as semantic vectors in a common embedding space , and the semantic similarity is calculated by the cosine score between vectors . In order to define the similarity function between one question and a complex query graph , an intuitive solution is to split the query graph into multiple semantic components , as the predicate sequences separated by dashed boxes in Figure 1 . Then previous methods can be applied for modeling the similarity between the question and each part of the graph . However , such approach faces two limitations . First , each semantic component is not directly comparable with the whole question , since it conveys only partial information of the question . Second , and more importantly , the model encodes different components separately , without learning the representation of the whole graph , hence it 's not able to capture the compositional semantics in a global perspective . In order to attack the above limitations , we propose a neural network based approach to improve the performance of semantic similarity measurement in complex question answering . Given candidate query graphs generated from one question , our model embeds the question surface and predicate sequences into a uniform vector space . The main difference between our approach and previous methods is that we integrate hidden vectors of various semantic components and encode their interaction as the hidden semantics of the entire query graph . In addition , to cope with different semantic components of a query graph , we leverage dependency parsing information as a complementary of sentential information for question encoding , which makes the model better align each component to the question . The contribution of this paper is summarized below . We propose a light - weighted and effective neural network model to solve complex KBQA task . To the best of our knowledge , this is the first attempt to explicitly encode the complete semantics of a complex query graph ( Section 2.2 ) ; We leverage dependency parsing to enrich question representation in the NN model , and conduct thorough investigations to verify its effectiveness ( Section 2.2.2 ) ; We propose an ensemble method to enrich entity linking from a state - of - the - art linking tool , which further improves the performance of the overall task ( Section 2.3 ) ; We perform comprehensive experiments on multiple QA datasets , and our proposed method consistently outperforms previous approaches on complex questions , and produces competitive results on datasets made up of simple questions ( Section 3 ) .", "entities": [[4, 6, "TaskName", "question answering"], [40, 41, "DatasetName", "YAGO"], [49, 50, "DatasetName", "DBpedia"], [290, 292, "TaskName", "semantic parsing"], [530, 532, "TaskName", "semantic similarity"], [574, 576, "TaskName", "semantic similarity"], [736, 738, "TaskName", "semantic similarity"], [741, 743, "TaskName", "question answering"], [817, 819, "TaskName", "dependency parsing"], [898, 900, "TaskName", "dependency parsing"], [929, 931, "TaskName", "entity linking"]]}
{"text": "In this section , we present our approach for solving complex KBQA . First , we generate candidate query graphs by staged generation method ( Section 2.1 ) . Second , we measure the semantic similarities between the question and each query graph using deep neural networks ( Section 2.2 ) . Then we introduce an ensemble approach for entity linking enrichment ( Section 2.3 ) , Finally , we discuss the prediction and parameter learning step of this task ( Section 2.4 ) .", "entities": [[59, 61, "TaskName", "entity linking"]]}
{"text": "We illustrate our staged candidate generation method in this section . Compared to previous methods , such as Bao et al ( 2016 ) , we employ a more effective candidate generation strategy , which takes advantage of implicit type information in query graphs and time interval information in the KB . In our work , we take 4 kinds of semantic constraints into account : entity , type , time and ordinal constraints . Figure 2 shows a concrete example of our candidate generation . For simplicity of discussion , we assume Freebase as the KB in this section . Step 1 : Focus linking . We extract possible ( mention , focus node ) pairs from the question . Focus nodes are the starting points of various semantic constraints , refer to Figure 2 ( a ) . For entity linking , we generate ( mention , entity ) pairs using the state - of - the - art entity linking tool S - MART ( Yang and Chang , 2015 ) . For type linking , we brutally combine each type with all uni - , bi - and tri - gram mentions in the question , and pick top - 10 ( mention , type ) pairs with the highest word embedding similarities of each pair . For time linking , we extract time mentions by simply matching year regex . For ordinal linking , we leverage a predefined superlative word list 2 and recognize mentions by matching superlative words , or the \" ordinal number + superlative \" pattern . The ordinal node is an integer representing the ordinal number in the mention . Step 2 : Main path generation . We build different main paths by connecting the answer node to different focus entities using 1 - hop or 2 - hopwith - mediator 3 predicate sequence . Figure 2 ( b ) shows one of the main paths . Further constraints are attached by connecting an anchor node x to an unused focus node through predicate sequences , where the anchor node x is a non - focus node in the main path ( A or v 1 in the example ) . Step 3 : Attaching entity constraints . We apply a depth - first search to search for combinations of multiple entity constraints to the main path through 1 - hop predicate . Figure 2 ( c ) shows a valid entity constraint , ( v 1 , basic title , president ) . The advantage of depth - first search is that we can involve unlimited number of entities in a query graph , which has a better coverage than template - based methods . Step 4 : Type constraint generation . Type constraints can only be applied at the answer node using IsA predicate . Our improvement in this step is to filter type constraints using implicit types 2~2 0 superlative words , such as largest , highest , latest . 3 Mediator is a kind of auxiliary nodes in Freebase maintaining N - ary facts . of the answer , derived from the outgoing predicates of the answer node . For example in Figure 2 ( c ) , the domain type of the predicate government position is politician , which becomes the implicit type of the answer . Thus we can filter type constraints which are irrelevant to the implicit types , preventing semantic drift and speeding up the generation process . To judge whether two types in Freebase are relevant or not , we adopt the method in Luo et al ( 2015 ) to build a rich type hierarchy of Freebase . Focus types are discarded , if they are not the super - or sub - types of any implicit types of the answer . Step 5 : Time and ordinal constraint generation . As shown in Figure 2 ( d ) , the time constraint is represented as a 2 - hop predicate sequence , where the second is a virtual predicate determined by the preposition before the focus time , indicating the time comparing operation , like \" before \" , \" after \" and \" in \" . Similarly , the ordinal constraint also forms a 2 - hop predicate sequence , where the second predicate represents descending ( MaxAtN ) or ascending order ( MinAtN ) . For the detail of time constraint , while existing approaches ( Yih et al , 2015 ; Bao et al , 2016 ) link the focus time with only single time predicate , our improvement is to leverage paired time predicates for representing a more accurate time constraint . In Freebase , paired time predicates are used to represent facts within certain time intervals , like f rom and to 4 in Figure 2 ( d ) . For time comparing operation \" in \" , we link the time focus to the starting time predicate , but use both predi - cates in SPARQL query , restricting that the focus time lies in the time interval of the paired predicates . After finishing all these querying stages , we translate candidate graphs into SPARQL query , and produce their final output answers . Finally , we discard query graphs with zero outputs , or using overlapped mentions .", "entities": [[141, 143, "TaskName", "entity linking"], [161, 163, "TaskName", "entity linking"], [490, 491, "DatasetName", "0"]]}
{"text": "The architecture of the proposed model is shown in Figure 3 . We first replace all entity ( or time ) mentions used in the query graph by dummy tokens E ( or T m ) . To encode the complex query structure , we split it into predicate sequences starting from answer to focus nodes , which we call semantic components . The predicate sequence does n't include the information of focus nodes , except for type constraints , where we append the focus type to the IsA predicate , resulting in the predicate sequence like { IsA , river } . We introduce in detail the encoding methods for questions and predicate sequences , and how to calculate the semantic similarity score .", "entities": [[121, 123, "TaskName", "semantic similarity"]]}
{"text": "We encode the question in both global and local level , which captures the semantic information with respect to each component p. The global information takes the token sequence as the input . We use the same word embedding matrix E w to convert the token sequence into vectors { q ( w ) 1 , . . . , q ( w ) n } . Then we encode the token sequence by applying bidirectional GRU network ( Cho et al , 2014 ) . The representation of the token sequence is the concatenation of the last forward and backward hidden states through the Bi - GRU layer , q ( tok ) = [ \u2212 h ( w ) 1 ; \u2212 h ( w ) n ] . To encode the question at local level , we leverage dependency parsing to represent long - range dependencies between the answer and the focus node in p. Since the answer is denoted by the whword in the question , we extract the dependency path from the answer node to the focus mention in the question . Similar with Xu et al ( 2016 ) , we treat the path as the concatenation of words and dependency labels with directions . For example , the dependency path between \" what \" and \" United States \" is { what , \u2212\u2212\u2212 nsubj , is , \u2212 \u2212 prep , in , \u2212 \u2212 pobj , E } . We apply another bidirectional GRU layer to produce the vector representation at dependency level q ( dep ) p , capturing both syntactic features and local semantic features . Finally we combine global and local representation by element - wise addition , returning the representation of the question with respect to the semantic component , q p = q ( tok ) + q ( dep ) p .", "entities": [[75, 77, "MethodName", "bidirectional GRU"], [107, 108, "MethodName", "GRU"], [141, 143, "TaskName", "dependency parsing"], [251, 253, "MethodName", "bidirectional GRU"]]}
{"text": "Given the query graph with multiple semantic components , G = { p ( 1 ) , . . . , p ( N ) } , now all its semantic components have been projected into a common vector space , representing hidden features in different aspects . We apply max pooling over the hidden vectors of semantic components , and get the compositional semantic representation of the entire query graph . Similarly , we perform max pooling for the question vectors with respect to each semantic component . Finally , we compute the semantic similarity score between the graph and question : Based on this framework , our proposed method ensures the vector spaces of the question and the entire query graph are comparable , and captures complementary semantic features from different parts of the query graph . It 's worth mentioning that the semantic matching model is agnostic to the candidate generation method of the query graphs , hence it can be applied to the other existing semantic parsing frameworks . S sem ( q , G ) = cos ( max i p ( i ) , max i q ( i ) p ) . ( 1 )", "entities": [[50, 52, "MethodName", "max pooling"], [76, 78, "MethodName", "max pooling"], [94, 96, "TaskName", "semantic similarity"], [169, 171, "TaskName", "semantic parsing"]]}
{"text": "The S - MART linker is a black box for our system , which is not extendable and tend to produce high precision but low recall linking results . To seek a better balance at entity linking , we propose an ensemble approach to enrich linking results . We first build a large lexicon by collecting all ( mention , entity ) pairs from article titles , anchor texts , redirects and disambiguation pages of Wikipedia . Each pair is associated with statistical features , such as linking probability , letter - tri - gram jaccard similarity and popularity of the entity in Wikipedia . For the pairs found in S - MART results , we take the above features as the input to a 2 - layer linear regression model fitting their linking scores . Thus we learn a pseudo linking score for every pair in the lexicon , and for each question , we pick top - K highest pairs to enrich S - MART linking results , where K is a hyperparameter .", "entities": [[35, 37, "TaskName", "entity linking"], [128, 130, "MethodName", "linear regression"]]}
{"text": "In this section , we explore the contributions of various components in our system . Semantic component representation : We first evaluate the results on CompQ and WebQ under different path encoding methods . Recap that the encoding result of a semantic component is the summation of its word and i d path representations ( Section 2.2.1 ) , thus we compare encoding methods by multiple combinations . For encoding predicate word sequence , we use BiGRU ( the same setting as encoding question word sequence ) as the alternative of average word embedding . For encoding predicate i d sequence , we use average predicate embedding as the alternative of the current path - level embedding ( P athEmb ) . The experimental results are shown in Table 4 . The encoding method N one means that we do n't encode the i d or word sequence , and simply take the result of the other sequence as the representation of the whole component . we observe that the top three combination settings , ignoring either word or i d sequence , perform worse than the bottom three settings . The comparison demonstrates that predicate word and i d representation can be complementary to each other . The performance gain is not that large , mainly because predicate i d features are largely covered by their word name features . For the encoding of i d sequences , P athEmb works better than average embedding , consistently boosting F 1 by 0.65 on both datasets . The former method treats the whole sequence as a single unit , which is more flexible and can potentially learn diverse representations of i d sequences that share the same predicates . For the encoding of word sequences , the average word embedding method outperforms BiGRU on CompQ , and the gap becomes smaller when running on WebQ. This is mainly because the training set of WebQ is about 3 times larger than that of CompQ , making it easier for training a more complex model . Semantic composition and question representation : To demonstrate the effectiveness of semantic composition , we construct a straightforward baseline , where we remove the max pooling operation in Eq . ( 2 ) , and instead calculate the semantic similarity score as the summation of individual cosine similarities : S sem ( q , G ) = i cos ( p ( i ) , q ( i ) p ) . For methods of question encoding , we setup ablations by turning off either sentential encoding or dependency encoding . Table 5 shows the ablation results on CompQ and WebQ. When dependency path information is augmented with sentential information , the performance boosts by 0.42 on average . Dependency paths focus on hidden features at syntactic and functional perspective , which is a good complementary to sentential encoding results . However , performances drop by 2.17 if only dependency information is used , we find that under certain dependency structures , crucial words ( bolded ) are not in the path between the answer and the focus mention ( underlined ) , for example , \" who did draco malloy end up marrying \" and \" who did the philippines gain independence from \" . While we observe about 5 % of such questions in WebQ , it 's hard to predict the correct query graph without crucial words . In terms of semantic composition , Our max pooling based method consistently outperforms the baseline method . The improvement on WebQ is smaller than on CompQ , largely due to the fact that 85 % questions in WebQ are simple questions ( Bao et al , 2016 ) . As a result of combination , our approach significantly outperforms the vanilla SP+NN approach on CompQ by 1.28 , demonstrating the effectiveness of our approach . Theoretically , the pooling outcome may lead to worse end - to - end result when there are too many semantic components in one graph , because the pooling layer takes too many vectors as input , different semantic features between similar query graphs become indistinguishable . In our task , only 0.5 % of candidate graphs have more than 3 semantic components , so pooling is a reasonable way to aggregate semantic components in this scenario . To further explain the advantage of semantic composition , we take the following question as an example : \" who is gimli 's father in the hobbit \" . Two query graphs are likely to be the final answer : 1 ) ( ? , children , gimli person ) ; 2 ) ( ? , f ictional children , gimli character ) ( ? , appear in , hobbit ) . If observing semantic components individually , the predicate children is most likely to be the correct one since \" 's father \" is highly related and with plenty of positive training data . Both f ictional children and appear in get a much lower similarity compared with children , hence the baseline method prefer the first query graph . In the meantime , our proposed method learns the hidden semantics of the second candidate by absorbing salient features from both predicates , and such compositional representation is closer to the semantics of the entire question than a simple \" children \" predicate . That 's why our method manages to answer it correctly .", "entities": [[76, 77, "MethodName", "BiGRU"], [302, 303, "MethodName", "BiGRU"], [344, 346, "TaskName", "Semantic composition"], [355, 357, "TaskName", "semantic composition"], [368, 370, "MethodName", "max pooling"], [382, 384, "TaskName", "semantic similarity"], [578, 580, "TaskName", "semantic composition"], [582, 584, "MethodName", "max pooling"], [734, 736, "TaskName", "semantic composition"]]}
{"text": "We randomly analyzed 100 questions from CompQ where no correct answers are returned . We list the major causes of errors as follows : Main path error ( 10 % ) : This type of error occurred when the model failed to understand the main semantics when facing some difficult questions ( e.g. \" What native american sports heroes earning two gold medals in the 1912 Olympics \" ) ; Constraint missing ( 42 % ) : These types of questions involve implicit constraints , for example , the question \" Who was US president when Traicho Kostov was teenager \" is difficult to answer because it implies an implicit time constraint \" when Traicho Kostov was teenager \" ; Entity linking error ( 16 % ) : This error occurs due to the highly ambiguity of mentions . For example , the question \" What character did Robert Pattinson play in Harry Potter \" expects the film \" Harry Potter and the Goblet of Fire \" as the focus , while there are 7 movies in Harry Potter series ; Miscellaneous ( 32 % ) : This error class contains questions with semantic ambiguity or not reasonable . For example , the question \" Where is Byron Nelson 2012 \" is hard to understand , because \" Byron Nelson \" died in 2006 and maybe this question wants to ask where did he die .", "entities": [[120, 122, "TaskName", "Entity linking"], [181, 182, "TaskName", "Miscellaneous"]]}
{"text": "Knowledge Base Question Answering ( KBQA ) has been a hot research top in recent years . Generally speaking , the most popular methods for KBQA can be mainly divided into two classes : information retrieval and semantic parsing . Information retrieval based system tries to obtain target answer directly from question information and KB knowledge without explicit considering interior query structure . There are various methods ( Yao and Van Durme , 2014 ; Bordes et al , 2015 ; Dong et al , 2015 ; Xu et al , 2016 ) to select candidate answers and to rank results . Semantic parsing based approach focuses on constructing a semantic parsing tree or equivalent query structure that represents the semantic meaning of the question . In terms of logical representation of natural language questions , many methods have been tried , such as query graph ( Yih et al , 2014 ( Yih et al , , 2015 or RDF query language ( Unger et al , 2012 ; Cui et al , 2017 ; Hu et al , 2018 ) . Recently , as the development of deep learning , NN - based approaches have been combined into the KBQA task ( Bordes et al , 2014 ) , showing promising result . These approaches tries to use neural network models to encode both questions and answers ( or query structures ) into the vector space . Subsequently , similarity functions are used to select the most appropriate query structure to generate the final answer . For example , Bordes et al ( 2014 ) focuses on embedding the subgraph of the candidate answer ; Yin et al ( 2016 ) uses character - level CNN and word - level CNN to match different information ; Yu et al ( 2017 ) introduces the method of hierarchical residual RNN to compare questions and relation names ; Qu et al ( 2018 ) proposes the AR - SMCNN model , which uses RNN to capture semantic - level correlation and employs CNN to extract literallevel words interaction . Belonging to NN - based semantic parsing category , our approach employs a novel encoding structure method to solve complex questions . Previous works such as Yih et al ( 2015 ) and Bao et al ( 2016 ) require a recognition of a main relation and regard other constraints as variables added to this main relation . Unlike their approaches , our method encodes multiple relations ( paths ) into a uniform query structure representation ( semantic composition ) , which allows more flexible query structures . There are also some works ca n't be simply classified in to IR based methods or SP based methods . Jain ( 2016 ) introduces Factual Memory Network , which tries to encode KB and questions in same word vector space , extract a subset of initial candidate facts , then try to employ multi - hop reasoning and refinement to find a path to answer entity . , Abujabal et al ( 2017 ) , andCui et al ( 2017 ) try to interpret question intention by templates , which learned from KB or QA corpora . Talmor and Berant ( 2018 ) attempts to answering complex questions by decomposing them into a sequence of simple questions .", "entities": [[0, 4, "TaskName", "Knowledge Base Question Answering"], [34, 36, "TaskName", "information retrieval"], [37, 39, "TaskName", "semantic parsing"], [40, 42, "TaskName", "Information retrieval"], [102, 104, "TaskName", "Semantic parsing"], [110, 112, "TaskName", "semantic parsing"], [354, 356, "TaskName", "semantic parsing"], [426, 428, "TaskName", "semantic composition"], [463, 465, "MethodName", "Memory Network"]]}
{"text": "This paper describes the system of the team Orange - Deski\u00f1 , used for the CoNLL 2017 UD Shared Task . We based our approach on an existing open source tool ( BistParser ) , which we modified in order to produce the required output . Additionally we added a kind of pseudoprojectivisation . This was needed since some of the task 's languages have a high percentage of non - projective dependency trees . In most cases we also employed word embeddings . For the 4 surprise languages , the data provided seemed too little to train on . Thus we decided to use the training data of typologically close languages instead . Our system achieved a macro - averaged LAS of 68.61 % ( 10th in the overall ranking ) which improved to 69.38 % after bug fixes .", "entities": [[17, 18, "DatasetName", "UD"], [81, 83, "TaskName", "word embeddings"]]}
{"text": "For the shared task , we used an ( older ) version of BistParser for all treebanks ( ud - treebanks - conll2017 ) . BistParser ( Kiperwasser and Goldberg , 2016 ) is a transition based parser ( Nivre ( 2008 ) , and which uses the arc - hybrid transition system ( Kuhlmann et al , 2011 ) ) with the three \" basic \" transitions LEFT ARC , RIGHT ARC and SHIFT . Since the shared task requires that output dependency trees have exactly one root , we modified BistParser accordingly by deleting the additional ROOT node added to each sentence in the original version of this parser . BistParser uses a bidirectional LSTM neural network . Currently BistParser uses forms and XPOS for both learning and predicting . We have started implementing the use of feature column as well , but this has not been used for the CoNLL 2017 UD Shared Task . Some of the languages in the shared task have a large percentage of non - projective sentences . We thus decided to implement a pseudoprojectivisation ( K\u00fcbler et al , 2009 , p. 37 ) of the input sentences before training or predicting . The output sentences are than de - projectivised . Sometimes of course , the de - projectivisation can fail , especially if there are other dependency relation errors . Our tests showed , however , that the overall result for most languages is still better than without any pseudo - projectivisation . Finally we implemented filters which ignore the special CONLLU lines for multi - word tokens ( 2 - 3 ... ) and elliptic insertions ( 4.1 ... ) and reinsert those lines after predicting . In order to reduce memory usage during training and prediction , we modified BistParser and the underlying CNN library 7 to load word embeddings only for the words present in the training or test data . For the same reason we modified Bist - Parser to read sentences one by one , to predict , and to output the result , instead of reading the entire test file at once 8 .", "entities": [[18, 19, "DatasetName", "ud"], [69, 70, "DatasetName", "ARC"], [72, 73, "DatasetName", "ARC"], [115, 117, "MethodName", "bidirectional LSTM"], [154, 155, "DatasetName", "UD"], [311, 313, "TaskName", "word embeddings"]]}
{"text": "We trained our models using all treebanks provided by the CoNLL 2017 UD Shared Task . Since for some of the languages there were no development treebanks available , we split the training treebank in order to get a small development corpus ( 10 % of the training corpus is split to test during development ) . This posed a certain problem for treebanks like Kazakh and Uyghur , which are hopelessly small ( 31 and 100 sentences respectively ) . Eventhough both languages are geneti - cally and typologically very close to Turkish ( 3685 sentences ) , we finally trained on those small treebanks for time constraints ( with more time available we would have experimented with various other parameters and a cross - lingual approach ) . In most cases , adding word embeddings improved the LAS considerably . We downloaded the language specific corpora provided 9 by the task organisers and calculated our own word embeddings with Mikolov 's word2vec ( Mikolov et al , 2013 ) 10 , which gave better results than the 100dimensional word embeddings provided . In order to get the best results , we cleaned the text corpora ( e.g. deleting letter - digit combinations and separating punctuation symbols such as commas , question marks etc . by a white space from the preceding token ) . For those languages which use an alphabet which has case distinction ( Latin , Cyrillic and Greek ) we put everything in lowercase . Finally we trained word embeddings with 300 and 500 dimensional vectors respectively . For all other parameters of word2vec we used the default setting , apart from the lower frequency limit , which we increased to 15 words . The word embeddings were calculated on a server with a 32 core CPU running Ubuntu 14.04 11 . For the biggest text corpora like English ( 9 billion words ) , German ( 5 , 9 billion words ) , Indonesian ( 5 billion words ) French ( 4 , 8 billion words ) training for 500 dimensional word vectors took up to 6 hours ( English ) . A similar approach to word2vec is fastText The fundamental difference is the adoption of the \" subword model \" described in . A subword model is described as a open model allowing each word to be represented not only by the word itself but also the subword components of the word in combination . Subword components can be n - grams with varying values for n , stems , root words , prefixes , and suffixes or any other possible formalism . As a matter of fact , word2vec can been seen as the minimum configuration of fastText where only the words are considered . FastText has been demonstrated to perform rather well in two different tasks i.e. sentiment analysis and tag prediction . For the CoNLL 2017 UD Shared Task we finally used word2vec , since the results were similar , but fastText was taking significantly more time to train .", "entities": [[12, 13, "DatasetName", "UD"], [135, 137, "TaskName", "word embeddings"], [158, 160, "TaskName", "word embeddings"], [180, 182, "TaskName", "word embeddings"], [253, 255, "TaskName", "word embeddings"], [290, 292, "TaskName", "word embeddings"], [364, 365, "MethodName", "fastText"], [455, 456, "MethodName", "fastText"], [463, 464, "MethodName", "FastText"], [476, 478, "TaskName", "sentiment analysis"], [486, 487, "DatasetName", "UD"], [501, 502, "MethodName", "fastText"]]}
{"text": "We trained all treebanks without any word embeddings , with 300 and with 500 dimensional word embeddings . For BistParser , the only other parameter we changed was the size of the first hidden layer ( default 100 ) which we set to 50 ( or lower , especially for languages whose treebanks are very small ) . Every sentence of the training treebanks was pseudo - projectivised before training . Using the weighted LAS , we then chose the best combination of parameters for each language . Since the python version of CNN ( used by our adaptation of BistParser ) does not support GPU , training was slow 12 . Thus we stopped training usually after 15 epochs unless the intermediary results were promising enough to continue . Figure 1 shows the system architecture . The upper part represents the data flow for the training , the lower part represents the predicting phase . We did all training on two Ubuntu 16.04 servers 13 with 64 GB RAM . As said above , the version of the CNN library we used , does not run on GPU , so all training was single threaded . The training processes used up to 15 GB RAM , and took between 1 minute ( Kazakh ) and 53 hours ( Czech ) . depending on the size of the treebank . This corresponds to 0.5 to 3 seconds per sentence during training . Training for the surprise languages ( using treebanks of typologically close languages , cf . section , 4 ) , took significantly longer ( up to 90 hours for Czech ) . Training was on the gold values ( form , lemma , XPOS , UPOS , deprel , head ) of the training treebanks 14 , however , both , the development set ( on the Tira - platform ) and the final test set use the UD - Pipe output e.g. lemma , XPOS or UPOS ( Straka et al , 2016 ) which may be erroneous . So we expected a certain drop of LAS for the tests . In order to be prepared , we tried to add erroneous lemmas and UPOS in the training data . This , however , did not produce better results , so we abandoned the 12 The successor of CNN , Dynet , supports GPU , but since BistParser learns on a phrase by phrase base , no gain in time can be observed . 13 Intel Xeon CPU E5 - 1620 v4 at 3.50GHz and Intel Core i7 - 6900 K CPU at 3.20GHz respectively . 14 Apart from numerous punctuation symbols with wrong heads , we found several bad annotations for words as well in different languages . UDpipe to have similar \" noise \" than the test treebanks . The final results obtained with the development corpora ( or split from train corpora when there were no development corpora ) are shown in table 1 . We did not ( yet ) use the morphological features ( column 6 ) . First tests on French showed that a slight increase in LAS is possible , so we will work on this in the future . With 16 GB RAM on the virtual machine provided by Tira ( Potthast et al , 2014 ) 15 the 56 development corpora ( on the Tira platform ) were processed in about 130 minutes .", "entities": [[6, 8, "TaskName", "word embeddings"], [15, 17, "TaskName", "word embeddings"], [169, 170, "MethodName", "RAM"], [205, 206, "MethodName", "RAM"], [283, 284, "DatasetName", "lemma"], [320, 321, "DatasetName", "UD"], [325, 326, "DatasetName", "lemma"], [543, 544, "MethodName", "RAM"]]}
{"text": "Buryat fa ( 100 As expected , Upper Sorbian and Norther Sami give quite acceptable results using models trained on Czech and Finnish respectively . Due to the fact that the provided treebanks for Kazakh and Uyghur are both very small we tried to apply the same approach of using the training corpus of a typologically close language ( here Turkish ) . However , the results were disappointing . Thus , we continue to use the models trained on very small corpora for these two languages in the shared task . Possibly the fact that the raw text corpus used to calculate word embeddings for Kazakh and Uyghur are much bigger than those of the surprise languages allowed to produce usable word embeddings . If so , this would mean that word embeddings play a very prominent role in data driven dependency parsing .", "entities": [[103, 105, "TaskName", "word embeddings"], [122, 124, "TaskName", "word embeddings"], [132, 134, "TaskName", "word embeddings"], [142, 144, "TaskName", "dependency parsing"]]}
{"text": "ASSIST : Towards Label Noise - Robust Dialogue State Tracking", "entities": [[7, 10, "TaskName", "Dialogue State Tracking"]]}
{"text": "Task - oriented dialogue systems play an important role in helping users accomplish a variety of tasks through verbal interactions ( Young et al , 2013 ; Gao et al , 2019 ) . Dialogue state tracking ( DST ) is an essential component of the dialogue manager in pipeline - based task - oriented dialogue systems . It aims to keep track of users ' intentions at each turn of the conversation ( Mrk\u0161i\u0107 et al , 2017 ) . The state information indicates the progress of the conversation and is leveraged to determine the next system action and generate the next system response ( Chen et al , 2017 ) . As shown in Figure 1 , the dialogue state is typically represented as a set of ( slot , value ) pairs . Hi , how may I help you ? I need to book a room at autumn house . Definitely , for how many people and how many nights ? Just me , 3 nights . Can you also give me information on the vue cinema ? Sure . It is in the city centre , and the phone number is 08451962320 . Thanks for your help . That 's all I need . ( hotel - name , autumn house ) ( hotel - name , autumn house ) ( hotel - book people , 1 ) ( hotel - book stay , 3 ) ( attraction - name , vue cinema ) ( hotel - name , autumn house ) ( hotel - book people , 1 ) ( hotel - book stay , 3 ) ( attraction - name , vue cinema )", "entities": [[0, 5, "TaskName", "Task - oriented dialogue systems"], [34, 37, "TaskName", "Dialogue state tracking"], [52, 57, "TaskName", "task - oriented dialogue systems"]]}
{"text": "Dialogue State Figure 1 : An example dialogue spanning two domains . On the left is the dialogue context with system responses shown in orange and user utterances in green . The dialogue state at each turn is presented on the right . Therefore , the problem of DST is defined as extracting the values for all slots from the dialogue context at each turn of the conversation . Over the past few years , DST has made significant progress , attributed to a number of publicly available dialogue datasets , such as DSTC2 , FRAMES ( El Asri et al , 2017 ) , MultiWOZ 2.0 ( Budzianowski et al , 2018 ) , Cross - WOZ , and SGD . Among these datasets , MultiWOZ 2.0 is the most popular one . So far , lots of DST models have been built on top of it Wu et al , 2019 ; Ouyang et al , 2020 ; Hu et al , 2020 ; Ye et al , 2021b ; Lin et al , 2021 ) . However , it has been found out that there is substantial noise in the state annotations of MultiWOZ 2.0 ( Eric et al , 2020 ) . These noisy labels may impede the training of robust DST models and lead to noticeable performance decrease ( Zhang et al , 2016 ) . To remedy this issue , massive efforts have been devoted to rectifying the annotations , and four refined versions , including MultiWOZ 2.1 ( Eric et al , 2020 ) , MultiWOZ 2.2 , MultiWOZ 2.3 ( Han et al , 2020b ) , and MultiWOZ 2.4 ( Ye et al , 2021a ) , have been released . Even so , there are still plenty of noisy and inconsistent la - bels . For example , in the latest version MultiWOZ 2.4 , the validation set and test set have been manually re - annotated and tend to be noise - free . While the training set is still noisy , as it remains intact . In reality , it is costly and laborious to refine existing large - scale noisy datasets or collect new ones with fully precise annotations ( Wei et al , 2020 ) , let al ne dialogue datasets with multiple domains and multiple turns . In view of this , we argue that it is essential to devise particular learning algorithms to train DST models robustly from noisy labels . Although loads of noisy label learning algorithms ( Natarajan et al , 2013 ; Han et al , 2020a ) have been proposed in the machine learning community , most of them target only multi - class classification ( Song et al , 2020 ) . However , as illustrated in Figure 1 , the dialogue state may contain multiple labels , which makes it unstraightforward to apply existing noisy label learning algorithms to the DST task . In this paper we propose a general framework , named ASSIST ( lAbel noiSe - robuSt dIalogue State Tracking ) , to train DST models robustly from noisy labels . ASSIST first trains an auxiliary model on a small clean dataset to generate pseudo labels for each sample in the noisy training set . Then , it leverages both the generated pseudo labels and vanilla noisy labels to train the primary model . Since the auxiliary model is trained on the clean dataset , it can be expected that the pseudo labels will help us train the primary model more robustly . Note that ASSIST is based on the assumption that we have access to a small clean dataset . This assumption is reasonable , as it is feasible to manually collect a small noise - free dataset or re - annotate a portion of a large noisy dataset . In summary , our main contributions include : We propose a general framework ASSIST to train robust DST models from noisy labels . To the best of our knowledge , we are the first to tackle the DST problem by taking into consideration the label noise . We theoretically analyze why the pseudo labels are beneficial and show that a proper combination of the pseudo labels and vanilla noisy labels can approximate the unknown true labels more accurately . We conduct extensive experiments on Multi - WOZ 2.0 & 2.4 . The results demonstrate that ASSIST can improve the DST performance on both datasets by a large margin .", "entities": [[105, 107, "DatasetName", "MultiWOZ 2.0"], [120, 121, "MethodName", "SGD"], [126, 128, "DatasetName", "MultiWOZ 2.0"], [196, 198, "DatasetName", "MultiWOZ 2.0"], [252, 254, "DatasetName", "MultiWOZ 2.1"], [262, 264, "DatasetName", "MultiWOZ 2.2"], [265, 267, "DatasetName", "MultiWOZ 2.3"], [276, 278, "DatasetName", "MultiWOZ 2.4"], [312, 314, "DatasetName", "MultiWOZ 2.4"], [451, 455, "TaskName", "multi - class classification"], [511, 514, "TaskName", "dIalogue State Tracking"]]}
{"text": "Figure 2 shows the architecture , which consists of a dialogue context semantic encoder , a slot attention module , and a slot - value matching module .", "entities": [[16, 18, "MethodName", "slot attention"]]}
{"text": "Similar to Ye et al , 2021b ) , we utilize the pre - trained language model BERT ( Devlin et al , 2019 ) to encode the dialogue context X t into contextual semantic representations . Let Z t = R t U t be the concatenation of the system response and user utterance at turn t , where denotes the operator of sequence concatenation . Then , the dialogue context X t can be represented as X t = Z 1 Z 2 Z t . We also concatenate each slot - value pair and denote the representation of the dialogue state at turn t as B t = ( s , vt ) Bt , vt = none s v t , in which only non - none slots are included . B t can serve as a compact representation of the dialogue history . In view of this , we treat the previous turn dialogue state B t\u22121 as part of the input as well , which can be beneficial when X t exceeds the maximum input length of BERT . The complete input sequence to the encoder module is then denoted as : I t = [ CLS ] X t\u22121 B t\u22121 [ SEP ] Z t [ SEP ] , 1 We adopt existing DST models as the primary model .", "entities": [[17, 18, "MethodName", "BERT"], [183, 184, "MethodName", "BERT"]]}
{"text": "[ CLS ] \u22ef \u22ef [ SEP ] [ SEP ] [ CLS ] \u22ef [ SEP ] Slot Attention Let H t R | It | \u00d7d be the semantic matrix representation of I t . Here , | I t | and d denote the sequence length of I t and the BERT output dimension , respectively . Then , we have : H t = BERT f inetune ( I t ) , where BERT f inetune means that the BERT model will be fine - tuned during the training process . For each slot s and its candidate value v V s , we employ another BERT to encode them into semantic vectors h s R d and h v R d . Here , V s denotes the candidate value set of slot s. Unlike the dialogue context , we leverage the pre - trained BERT without fine - tuning to embed s and v . Besides , we adopt the output vector corresponding to the special token [ CLS ] as an aggregated representation of slot s and value v , i.e. , h s = BERT [ CLS ] f ixed ( [ CLS ] s [ SEP ] ) , h v = BERT [ CLS ] f ixed ( [ CLS ] v [ SEP ] ) .", "entities": [[18, 20, "MethodName", "Slot Attention"], [54, 55, "MethodName", "BERT"], [68, 69, "MethodName", "BERT"], [77, 78, "MethodName", "BERT"], [83, 84, "MethodName", "BERT"], [110, 111, "MethodName", "BERT"], [150, 151, "MethodName", "BERT"], [192, 193, "MethodName", "BERT"], [211, 212, "MethodName", "BERT"]]}
{"text": "The slot attention module is exploited to retrieve slot - relevant information for all the slots from the same dialogue context . The slot attention is a multihead attention ( Vaswani et al , 2017 ) . Specifically , the slot representation h s is regarded as the query vector , and the dialogue context representation H t is taken as both the key matrix and value matrix . The slot attention matches h s to the semantic vector of each word in the dialogue context and calculates the attention score , based on which the slot - specific information can be extracted . Let a s t R d denote a d - dimensional vector representation of the related information of slot s at turn t , we obtain : a s t = MultiHead ( h s , H t , H t ) . a s t is expected to be close to the semantic vector representation of the true value of slot s. Considering that the output of BERT is normalized by layer normalization ( Ba et al , 2016 ) , we also feed a s t to a layer normalization layer , which is preceded by a linear transformation layer . The final slot - specific vector g s t R d is calculated as : g s t = LayerNorm ( Linear ( a s t ) ) .", "entities": [[1, 3, "MethodName", "slot attention"], [23, 25, "MethodName", "slot attention"], [70, 72, "MethodName", "slot attention"], [172, 173, "MethodName", "BERT"], [176, 178, "MethodName", "layer normalization"], [194, 196, "MethodName", "layer normalization"]]}
{"text": "Our approach depends on the auxiliary model A to generate pseudo labelsB t = { ( s , v t ) | s S } for each sample in the noisy training set . In this work , we treat each dialogue context X t rather than the entire dialogue as a training sample . Without loss of generality , the pseudo label generation process is denoted as follows : B t = A ( X t , S ) , where X t belongs to the noisy training set .", "entities": [[56, 57, "MetricName", "loss"]]}
{"text": "We adopt MultiWOZ 2.0 ( Budzianowski et al , 2018 ) and MultiWOZ 2.4 ( Ye et al , 2021a ) as the datasets in our experiments . MultiWOZ 2.0 is one of the largest publicly available multi - domain taskoriented dialogue datasets , including about 10 , 000 dialogues spanning seven domains . MultiWOZ 2.4 is the latest refined version of MultiWOZ 2.0 . The annotations of its validation set and test set have been manually rectified . While its training set remains intact and is the same as that of MultiWOZ 2.1 ( Eric et al , 2020 ) , in which 41.34 % of the state values are changed , compared to MultiWOZ 2.0 . Since the hospital domain and police domain never occur in the test set , we use only the remaining five domains { attraction , hotel , restaurant , taxi , train } in our experiments . These domains have 30 slots in total . Considering that the validation set and test set of MultiWOZ 2.0 are noisy , we replace them with the counterparts of MultiWOZ 2.4 2 . We preprocess the datasets following ( Ye et al , 2021b ) . We use the validation set as the small clean dataset .", "entities": [[2, 4, "DatasetName", "MultiWOZ 2.0"], [12, 14, "DatasetName", "MultiWOZ 2.4"], [28, 30, "DatasetName", "MultiWOZ 2.0"], [54, 56, "DatasetName", "MultiWOZ 2.4"], [62, 64, "DatasetName", "MultiWOZ 2.0"], [92, 94, "DatasetName", "MultiWOZ 2.1"], [115, 117, "DatasetName", "MultiWOZ 2.0"], [171, 173, "DatasetName", "MultiWOZ 2.0"], [183, 185, "DatasetName", "MultiWOZ 2.4"]]}
{"text": "To verify the effectiveness of the proposed framework , we apply the generated pseudo labels to three different primary models . SOM - DST : SOM - DST ) is an open vocabulary - based method . It treats the dialogue state as an explicit fixed - sized memory and selectively overwrites this memory at each turn . STAR : STAR ( Ye et al , 2021b ) is a predefined ontology - based method . It leverages a stacked slot self - attention mechanism to capture the slot dependencies automatically .", "entities": [[21, 22, "MethodName", "SOM"], [25, 26, "MethodName", "SOM"], [58, 59, "DatasetName", "STAR"], [60, 61, "DatasetName", "STAR"], [71, 72, "MethodName", "ontology"]]}
{"text": "Although any existing DST models can be adopted as the auxiliary model , we chose to propose a new simple one to reduce overfitting . In order to verify the superiority of the proposed model , we also apply STAR as the auxiliary model and compare their performance in Figure 3 . We chose STAR due to its good performance , as shown in Table 1 . From Figure 3 , we observe that all three primary 0 0.1 0 . 2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 models demonstrate higher performance on both datasets when using the proposed auxiliary model than using STAR as the auxiliary model . The results indicate that the proposed auxiliary model is able to generate pseudo labels with higher quality .", "entities": [[39, 40, "DatasetName", "STAR"], [54, 55, "DatasetName", "STAR"], [77, 78, "DatasetName", "0"], [79, 80, "DatasetName", "0"], [105, 106, "DatasetName", "STAR"]]}
{"text": "Pseudo Labels [ sys ] : Sure , da vinci pizzeria is a cheap Italian restaurant in the area . [ usr ] : Would you mind making a reservation for Thursday at 17:15 ? ( restaurant - name , da vinci pizzeria ) ( restaurant - book day , thursday ) ( restaurant - book time , 17:15 ) ( restaurant - name , da vinci pizzeria ) [ sys ] : Do you have a preferred section of town ? [ usr ] : Not really , but I want free wifi and it should be 4 star . ( hotel - internet , free ) ( hotel - stars , 4 ) ( hotel - area , dontcare ) ( hotel - internet , free ) ( hotel - stars , 4 ) [ usr ] : I need to find out if there is a train going to stansted airport that leaves after 12:30 . ( train - arriveby , 13:03 ) ( train - destination , stansted airport ) ( train - leaveat , 12:30 ) ( train - destination , stansted airport ) ( train - leaveat , 12:30 ) [ usr ] : I am staying in the west part of Cambridge and would like to know about some places to go . ( attraction - area , west ) ( attraction - area , west ) ( hotel - area , west ) Table 2 : Four dialogue snippets with their vanilla labels and the generated pseudo labels . These dialogue snippets are chosen from the training set of MultiWOZ 2.4 . To save space , we only present turn - active slots and their values .", "entities": [[208, 209, "DatasetName", "Cambridge"], [267, 269, "DatasetName", "MultiWOZ 2.4"]]}
{"text": "Aiming to better validate the effectiveness of the proposed framework , we also report the results when the small clean dataset is directly combined with the large noisy training set to train the primary model . We adopt AUX - DST as the primary model and show the results in Table 3 . Since the clean dataset ( i.e. , the validation set in our experiments ) is combined with the training set , all the results in Table 3 are the best ones on the test set . As can be observed , a simple combination of the noisy training set and clean dataset can lead to better results . However , the performance improvements are lower , compared to using pseudo labels ( especially on MultiWOZ 2.0 due to its noisier training set ) . It is also observed that when both the clean dataset and the pseudo labels are utilized to train the model , even higher performance can be achieved . These results indicate that our proposed framework can make better use of the small clean dataset to train the primary model .", "entities": [[127, 129, "DatasetName", "MultiWOZ 2.0"]]}
{"text": "Recently , DST has got an enormous amount of attention , thanks to the availability of multiple largescale multi - domain dialogue datasets such as Multi - WOZ 2.0 ( Budzianowski et al , 2018 ) , MultiWOZ 2.1 ( Eric et al , 2020 ) , RiSAWOZ ( Quan et al , 2020 ) , and SGD . The most popular datasets are MultiWOZ 2.0 and MultiWOZ 2.1 , and lots of DST models have been built on top of them Wu et al , 2019 ; Ouyang et al , 2020 ; Hosseini - Asl et al , 2020 ; Hu et al , 2020 ; Ye et al , 2021b ; Lin et al , 2021 ; Liang et al , 2021 ) . These recent DST models can be grouped into two categories : predefined ontology - based models and open vocabulary - based models . The predefined ontology - based models treat DST as a multi - label classification problem and tend to demonstrate better performance Shan et al , 2020 ; Ye et al , 2021b ) . The open vocabulary - based models leverage either span prediction ( Heck et al , 2020 ; or sequence generation ( Wu et al , 2019 ; Hosseini - Asl et al , 2020 ) to extract slot values from the dialogue context directly . Although these DST models have made a huge success , they can only achieve sub - optimal performance , due to the lack of handling noisy labels . To the best of our knowledge , we are the first to take the noisy labels into consideration when tackling the DST problem .", "entities": [[37, 39, "DatasetName", "MultiWOZ 2.1"], [47, 48, "DatasetName", "RiSAWOZ"], [57, 58, "MethodName", "SGD"], [64, 66, "DatasetName", "MultiWOZ 2.0"], [67, 69, "DatasetName", "MultiWOZ 2.1"], [139, 140, "MethodName", "ontology"], [152, 153, "MethodName", "ontology"], [160, 164, "TaskName", "multi - label classification"]]}
{"text": "Addressing noisy labels in supervised learning is a long - term studied problem ( Fr\u00e9nay and Verleysen , 2013 ; Song et al , 2020 ; Han et al , 2020a ) . This issue becomes more prominent in the era of deep learning , as training deep models generally requires a lot of well - labelled data , but it is expensive and time - consuming to collect large - scale datasets with completely clean annotations . This dilemma has sparked a surge of noisy label learning methods ( Hendrycks et al , 2018 ; Zhang and Sabuncu , 2018 ; Song et al , 2019 ; Wei et al , 2020 ) . Even so , these methods mainly focus on multi - class classification ( Song et al , 2020 ) , which makes it not straightforward to apply them to the DST task .", "entities": [[123, 127, "TaskName", "multi - class classification"]]}
{"text": "Except for the size of the clean dataset , the distribution of the clean dataset may also affect the performance of the primary model , especially when the clean dataset has a significantly different distribution from the training set . Thus , it is important to study the effects of the distribution of the clean dataset . However , we are short of clean datasets with different distributions . It is also challenging to model the distribution explicitly since the dialogue state may contain multiple labels . To address this issue , we propose to remove all the dialogues that are related to a specific domain and use only the remaining ones as the clean dataset . As thus , we can create multiple clean datasets with different distributions . The results of AUX - DST on MultiWOZ 2.4 are shown in Figure 8 . As can be observed , although different clean datasets indeed lead to different performance , compared to the situation where no clean data is used ( i.e. , only the vanilla labels are used to train the model ) , all these clean datasets still bring huge performance improvements .", "entities": [[137, 139, "DatasetName", "MultiWOZ 2.4"]]}
{"text": "This project was funded by the EPSRC Fellowship titled \" Task Based Information Retrieval \" and grant reference number EP / P024289/1 .", "entities": [[12, 14, "TaskName", "Information Retrieval"]]}
{"text": "Transfer Learning for Related Languages : Submissions to the WMT20 Similar Language Translation Task", "entities": [[0, 2, "TaskName", "Transfer Learning"], [12, 13, "TaskName", "Translation"]]}
{"text": "In this paper , we describe IIT Delhi 's submissions to the WMT 2020 task on Similar Language Translation for four language directions : Hindi \u2194 Marathi and Spanish \u2194 Portuguese . We try out three different model settings for the translation task and select our primary and contrastive submissions on the basis of performance of these three models . For our best submissions , we fine - tune the mBART model ( Liu et al , 2020 ) on the parallel data provided for the task . The pre - training is done using self - supervised objectives on a large amount of monolingual data for many languages . Overall , our models are ranked in the top four of all systems for the submitted language pairs , with first rank in Spanish Portuguese .", "entities": [[12, 14, "DatasetName", "WMT 2020"], [18, 19, "TaskName", "Translation"], [70, 71, "MethodName", "mBART"]]}
{"text": "Machine Translation ( MT ) is currently tackled using rule - based methods ( RBMT ) ( Charoenpornsawat et al , 2002 ) , phrase - based statistical methods ( SMT ) ( Koehn et al , 2003 ) and neural methods ( NMT ) ( Cho et al , 2014 ; Sutskever et al , 2014 ; Bahdanau et al , 2015 ; Vaswani et al , 2017 ) . NMT has achieved high translation quality for several language pairs ( Bojar et al , 2018 ; Barrault et al , 2019 ) , but this level of performance usually requires large amounts of aligned data in the order of millions of sentence pairs . For low and medium resource languages , SMT performs better than NMT ( Koehn and Knowles , 2017 ; Sennrich and Zhang , 2019 ) . SMT also shows better performance when there is a domain mismatch between the train and test datasets , which is typical of low and medium resource language pairs . In these settings , NMT performance can be boosted by leveraging additional monolingual data to enforce various types of constraints or increasing the training data using back - translation . These methods can be particularly helpful if the source and target languages in MT are closely related and share language structure and alphabet . Recently , pre - training methods for sequence - to - sequence ( seq2seq ) models have been introduced like MASS ( Song et al , 2019a ) , XLM ( Conneau and Lample , 2019 ) , BART ( Lewis et al , 2019 ) , and mBART ( Liu et al , 2020 ) . These methods show significant gains in downstream tasks like NMT , summarization , natural language inference ( NLI ) , etc . In this paper , we focus on the transfer learning capabilities in NMT for the task of translation between related languages where parallel data is scarce . IIT Delhi participated in the WMT 2020 Shared task on Similar Language Translation for four language directions : Hindi ( hi ) \u2194 Marathi ( mr ) and Spanish ( es ) \u2194 Portuguese ( pt ) . The first language pair is low resource and second is medium resource in terms of the parallel data available for the task . Refer to Table 2 for the classification . We fine - tuned the pre - trained mBART model ( Liu et al , 2020 ) on the parallel data provided for the task . mBART gives better performance than SMT models even when the parallel data is very limited . mBART is pre - trained on 25 languages , which contain Hindi and Spanish , but not Marathi and Portuguese . mBART is able to leverage transfer learning capabilities even for those languages that are originally not present during the pre - training phase . The fine - tuned mBART architecture forms our best submissions for both language pairs : hi \u2194 mr and es \u2194 pt . The rankings obtained by us in each of the language directions are listed in Table 1 The results and analysis are detailed in Section 5 . We finally conclude in Section 6 .", "entities": [[0, 2, "TaskName", "Machine Translation"], [238, 239, "MethodName", "seq2seq"], [254, 255, "MethodName", "XLM"], [263, 264, "MethodName", "BART"], [273, 274, "MethodName", "mBART"], [293, 294, "TaskName", "summarization"], [295, 298, "TaskName", "natural language inference"], [312, 314, "TaskName", "transfer learning"], [336, 338, "DatasetName", "WMT 2020"], [343, 344, "TaskName", "Translation"], [408, 409, "MethodName", "mBART"], [426, 427, "MethodName", "mBART"], [442, 443, "MethodName", "mBART"], [463, 464, "MethodName", "mBART"], [468, 470, "TaskName", "transfer learning"], [491, 492, "MethodName", "mBART"]]}
{"text": "SMT is tackled by building a phrase table from the aligned parallel data . The target side translation is then generated by matching the most appropriate phrases in the source sentence conditioned on the target side language model along with a reordering model ( Koehn et al , 2003 ) . NMT is modeled using Encoder - Decoder models ( Cho et al , 2014 ; Sutskever et al , 2014 ; Bahdanau et al , 2015 ) , with the Transformer model ( Vaswani et al , 2017 ) achieving state - of - the - art on many MT problems . But these models ' reliance on large aligned parallel data for the source and target languages makes them unsuitable for low / medium resource language pairs ( Koehn and Knowles , 2017 ) . Some of the previous works in these settings to improve NMT performance are described below :", "entities": [[81, 82, "MethodName", "Transformer"]]}
{"text": "Back - Translation Hoang et al , 2018 ) increases the amount of training data by using monolingual corpus along with partially - trained NMT models on the limited parallel data . Pseudo - parallel corpus for each direction is first obtained by generating the translations of the monolingual data for each language using the partially - trained MT models on the limited parallel data . Using these pseudoparallel corpora , the partially - trained NMT models are then trained further for some number of steps . In this way , millions of pseudo - parallel sentence pairs can be generated to improve NMT models because of the abundance of monolingual data . Another version of using back - translation is the copying mechanism . Currey et al ( 2017 ) proposes to copy the target side monolingual data on the source side to create additional data without modifying the training regimen for NMT . This helps the model to generate fluent translations .", "entities": [[2, 3, "TaskName", "Translation"]]}
{"text": "For NMT , the first step is the random initialization of model weights in both the encoder and decoder . Instead of random initialization , NMT models can be initialized by pre - training parts of the model ( Conneau and Lample , 2019 ; Edunov et al , 2019 ) , or pre - training the complete seq2seq model ( Ramachandran et al , 2017 ; Song et al , 2019b ; Liu et al , 2020 ) . These pre - training methods leverage different kinds of masking techniques and the pretraining objective is to predict these masked tokens , similar to BERT ( Devlin et al , 2019 ) . Denoising auto - encoding can also be used where a sentence is corrupted by various noising techniques and the pre - training objective is to generate the original uncorrupted sentence as in BART ( Lewis et al , 2019 ) and mBART ( Liu et al , 2020 ) .", "entities": [[58, 59, "MethodName", "seq2seq"], [104, 105, "MethodName", "BERT"], [113, 114, "TaskName", "Denoising"], [145, 146, "MethodName", "BART"], [154, 155, "MethodName", "mBART"]]}
{"text": "There also have been works to improve low / medium resource NMT by adding linguistic information either using data augmentation ( Currey and Heafield , 2019 ) , subword embedding augmentation , or architectural changes ( Eriguchi et al , 2017 ) . This helps the model to not only learn the alignment between source and target language spaces , but also syntax structure like dependency parse , part of speech , etc . This helps in making the target side translations more fluent and conforming to the structure of the language . We do not explore this direction in this paper .", "entities": [[18, 20, "TaskName", "data augmentation"]]}
{"text": "We experimented with three different settings for hi \u2194 mr as listed below . SMT This phrase - based system leverages both monolingual and parallel data provided for the task . We use Moses ( Koehn et al , 2007 ) for training the SMT systems . NMT ( Transformer ) For this , we used the standard Transformer large architecture from Vaswani et al ( 2017 ) for training on the parallel data provided for the task . NMT ( mBART ) mBART ( Liu et al , 2020 ) is a large Transformer pre - trained on monolingual data for 25 languages . The pre - training objective for mBART is seq2seq de - noising for natural text as in BART ( Lewis et al , 2019 ) . mBART provides a general - purpose pre - trained Transformer for any downstream task . It has been shown to give significant improvements over the random initialization for NMT and is the current state - of - the - art for many low resource language pairs . Implementation Details mBART uses a shared subword vocabulary of 250 K tokens for all the 25 languages present in the pre - training . We use the same vocabulary for Marathi and Portuguese also , even though they were not used during the pre - training phase . Marathi shares its subword vocabulary with languages like Hindi and Nepali in mBART , and Portuguese shares with Spanish , Italian and other European languages present in mBART . The percentage of unknown tokens [ UNK ] in Marathi and Portuguese parallel datasets is less than 0.003 % when using the shared mBART vocabulary . Additionally , the mBART architecture requires language specific token at the end of each input sequence to provide the language specific context for the decoder . Since Marathi and Portuguese were not present during the pre - training phase , we use the token corresponding to the second most related language present in mBART pre - training for specifying the context at the time of decoding in each case . For Marathi , we used the Nepali language token and for Portuguese , we used the Italian language token . We could not use Spanish language token for Portuguese because we are doing translations to and from Spanish .", "entities": [[49, 50, "MethodName", "Transformer"], [58, 59, "MethodName", "Transformer"], [81, 82, "MethodName", "mBART"], [83, 84, "MethodName", "mBART"], [94, 95, "MethodName", "Transformer"], [111, 112, "MethodName", "mBART"], [113, 114, "MethodName", "seq2seq"], [122, 123, "MethodName", "BART"], [131, 132, "MethodName", "mBART"], [140, 141, "MethodName", "Transformer"], [180, 181, "MethodName", "mBART"], [238, 239, "MethodName", "mBART"], [253, 254, "MethodName", "mBART"], [278, 279, "MethodName", "mBART"], [284, 285, "MethodName", "mBART"], [334, 335, "MethodName", "mBART"]]}
{"text": "Because of the constrained nature of the shared task , we only use the parallel data provided for this task . We removed the empty instances for both language pairs ( < 2000 instances ) . For es \u2194 pt , we do not use ' WikiTitles v2 ' part of the parallel data for training because of very short sentences in the dataset . The cleaned parallel dataset statistics are provided in Table 2 . Preprocessing We use sentence piece tokenization ( Kudo and Richardson , 2018 ) for generating the source and target sequences for the NMT architectures . For the standard Transformer , we train a sentence piece model using 40 K subword tokens for hi \u2194 mr . For mBART , we use Liu et al ( 2020 ) 's pre - trained 1 sentence piece model comprising of 250 K subword tokens as the vocabulary . For the SMT model on hi \u2194 mr , we also use the monolingual data provided for this task . We extract 5 Million monolingual sentences each for Hindi and Marathi after deduplication and use this set for training the language models . We use Moses ( Koehn et al , 2007 ) for all tokenization / detokenization scripts . Lample et al ( 2018 ) . We used Moses ( Koehn et al , 2007 ) and Giza++ with standard settings to train the SMT model in both directions .", "entities": [[104, 105, "MethodName", "Transformer"], [123, 124, "MethodName", "mBART"]]}
{"text": "We have participated in the Similar Language Translation task on four language directions . We have shown that pre - trained models can help in low and medium resource NMT . Our best system uses the pre - trained mBART model ( Liu et al , 2020 ) and fine - tunes on the parallel data provided for the specific translation task . Our results demonstrate that pre - training can help even when the language used for fine - tuning is not present during pre - training . One direction of future work is to add linguistic information during the pre - training phase to get more fluent translations . When this information is not available directly ( especially for low resource languages ) , pre - training on a related high resource language with syntax information can help low resource languages also . by the DARPA Explainable Artificial Intelligence ( XAI ) Program with number N66001 - 17 - 2 - 4032 , Visvesvaraya Young Faculty Fellowships by Govt . of India and IBM SUR awards . Any opinions , findings , conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views or official policies , either expressed or implied , of the funding agencies .", "entities": [[7, 8, "TaskName", "Translation"], [39, 40, "MethodName", "mBART"], [147, 148, "DatasetName", "DARPA"], [148, 151, "TaskName", "Explainable Artificial Intelligence"]]}
{"text": "Comparative Analysis of Neural QA models on SQuAD", "entities": [[7, 8, "DatasetName", "SQuAD"]]}
{"text": "The task of Question Answering has gained prominence in the past few decades for testing the ability of machines to understand natural language . Large datasets for Machine Reading have led to the development of neural models that cater to deeper language understanding compared to information retrieval tasks . Different components in these neural architectures are intended to tackle different challenges . As a first step towards achieving generalization across multiple domains , we attempt to understand and compare the peculiarities of existing end - to - end neural models on the Stanford Question Answering Dataset ( SQuAD ) by performing quantitative as well as qualitative analysis of the results attained by each of them . We observed that prediction errors reflect certain model - specific biases , which we further discuss in this paper .", "entities": [[3, 5, "TaskName", "Question Answering"], [45, 47, "TaskName", "information retrieval"], [91, 96, "DatasetName", "the Stanford Question Answering Dataset"], [97, 98, "DatasetName", "SQuAD"]]}
{"text": "Machine Reading is a task in which a model reads a piece of text and attempts to formally represent it or performs a downstream task like Question Answering ( QA ) . Neural approaches to the latter have gained a lot of prominence especially owing to the recent spur in developing and publicly releasing large datasets on Machine Reading and Comprehension ( MRC ) . These datasets are created from different underlying sources such as web resources in MS MARCO ( Nguyen et al , 2016 ) ; trivia and web in QUASAR - S and QUASAR - T ( Dhingra et al , 2017 ) , SearchQA ( Dunn et al , 2017 ) , TriviaQA ( Joshi et al , 2017 ) ; news articles in CNN / Daily Mail ( Chen et al ) , NewsQA ( Trischler et al , 2016 ) and stories in NarrativeQA ( Ko\u010disk\u1ef3 et al , 2017 ) . Another common source is large unstructured text documents from Wikipedia such as in SQuAD ( Rajpurkar et al , 2016 ) , WikiReading ( Hewlett et al , 2016 ) and WikiHop ( Welbl et al , 2017 ) . These different sources implicitly affect the nature and properties of questions and answers in these datasets . Based on the dataset , certain neural models capitalize on these biases while others are unable to . The ability to generalize across different sources and domains is a desirable characteristic for any machine reading system . Evaluating and analyzing systems on QA tasks can lead to insights for advancements in machine reading and natural language understanding , and Pe\u00f1as et al ( 2011 ) have also previously worked on this . One of the first large MRC datasets ( over 100k QA pairs ) is the Stanford Question Answering Dataset ( SQuAD ) ( Rajpurkar et al , 2016 ) . For its collection , different sets of crowd - workers formulated questions and answers using passages obtained from \u223c500 Wikipedia articles . The answer to each question is a span in the given passage , and many effective neural QA models have been developed for this dataset . Our main focus in this work is to perform comparative subjective and empirical analysis of errors in answer predictions by four top performing models on the SQuAD leaderboard 1 . We focused on Bi - Directional Attention Flow ( BiDAF ) ( Seo et al , 2016 ) , Gated Self - Matching Networks ( R - Net ) ( Wang et al , 2017 ) , Document Reader ( DrQA ) ( Chen et al , 2017 ) , Multi - Paragraph Reading Comprehension ( DocQA ) ( Clark and Gardner , 2017 ) , and the Logistic Regression baseline model ( Rajpurkar et al , 2016 ) We mainly choose these models since they have comparable high performance on the evaluation metrics and it is easy to replicate their results due to availability of open source implementations . While we limit ourselves to in - domain analysis of the performance of these models on SQuAD in this paper , similar principles can be used to extend this work to study biases of combinations of different models on different datasets and thereby understand the generalization capabilities of these neural architectures . The organization of the paper is as follows . Section 2 gives a comprehensive overview of the models that are compared in further sections . Section 3 describes the different experiments we conducted , and discusses our observations . In Section 4 , we summarize our main conclusions from this work and describe our vision for the future .", "entities": [[26, 28, "TaskName", "Question Answering"], [78, 80, "DatasetName", "MS MARCO"], [92, 95, "DatasetName", "QUASAR - S"], [96, 99, "DatasetName", "QUASAR - T"], [107, 108, "DatasetName", "SearchQA"], [116, 117, "DatasetName", "TriviaQA"], [128, 132, "DatasetName", "CNN / Daily Mail"], [138, 139, "DatasetName", "NewsQA"], [149, 150, "DatasetName", "NarrativeQA"], [171, 172, "DatasetName", "SQuAD"], [180, 181, "DatasetName", "WikiReading"], [189, 190, "DatasetName", "WikiHop"], [269, 272, "TaskName", "natural language understanding"], [301, 306, "DatasetName", "the Stanford Question Answering Dataset"], [307, 308, "DatasetName", "SQuAD"], [391, 392, "DatasetName", "SQuAD"], [448, 450, "TaskName", "Reading Comprehension"], [463, 465, "MethodName", "Logistic Regression"], [521, 522, "DatasetName", "SQuAD"]]}
{"text": "This model was proposed as a baseline in the SQuAD dataset paper ( Rajpurkar et al , 2016 ) and uses features based on n - gram frequencies , lengths , part - of - speech tags , constituency and dependency parse trees of questions and passages as inputs to a logistic regression classifier 6 to predict whether each constituent span is an answer or not .", "entities": [[9, 10, "DatasetName", "SQuAD"], [31, 34, "DatasetName", "part - of"], [51, 53, "MethodName", "logistic regression"]]}
{"text": "We trained the aforementioned end - to - end neural models and compare their performance on the SQuAD development set which contains 10 , 570 question - answer pairs based on Wikipedia articles .", "entities": [[17, 18, "DatasetName", "SQuAD"]]}
{"text": "In this work , we analyze - both quantitatively and qualitatively - results generated by 4 end - to - end neural models on the Stanford Question Answering Dataset . We observe interesting trends in the analysis , with some error patterns which are consistent across different models and some others which are specific to each model due to their different input features and architectures . This is important to be able to interpret and gain an intuition for the effective functions that different components in a neural model architecture perform versus their intended functions , and also to understand model - specific biases . Eventually , this can enable us to come up with new models including specific components which tackle these errors . Alternatively , the overlap analysis demonstrates that learning ensembles of different neural models to combine their individual strengths and quirks might be an interesting direction to explore to achieve better performance . Even though the scope of this paper is restricted to SQuAD , similar analysis can be done for any datasets / models / features , to gain a better understanding and enable a better assessment of stateof - the - art in neural machine reading . To this end , we also performed some preliminary experiments on TriviaQA so as to analyze the difference between the properties of the two datasets , but were unable to replicate the published results owing to pre - processing / hyperparameters . We will continue to work on this since the ability of a model to generalize and to be able to learn from a particular domain and transfer some knowledge to a different domain is a very exciting research area . We also believe that such analysis can help curate datasets which are better indicators of the actual natural language ' reading ' and ' comprehending ' capabilities of models rather than falling prey to shallow pattern matching . One way to achieve this is by building new challenges that are specifically designed to put pressure on the identified weaknesses of neural models . Thus , we can move towards the development of datasets and models which truly push the envelope of the challenging machine reading task .", "entities": [[24, 29, "DatasetName", "the Stanford Question Answering Dataset"], [167, 168, "DatasetName", "SQuAD"], [214, 215, "DatasetName", "TriviaQA"]]}
{"text": "Minimally - Augmented Grammatical Error Correction", "entities": [[3, 6, "TaskName", "Grammatical Error Correction"]]}
{"text": "There has been an increased interest in lowresource approaches to automatic grammatical error correction . We introduce Minimally - Augmented Grammatical Error Correction ( MAGEC ) that does not require any errorlabelled data . Our unsupervised approach is based on a simple but effective synthetic error generation method based on confusion sets from inverted spell - checkers . In low - resource settings , we outperform the current state - ofthe - art results for German and Russian GEC tasks by a large margin without using any real error - annotated training data . When combined with labelled data , our method can serve as an efficient pre - training technique .", "entities": [[11, 14, "TaskName", "grammatical error correction"], [20, 23, "TaskName", "Grammatical Error Correction"]]}
{"text": "Most neural approaches to automatic grammatical error correction ( GEC ) require error - labelled training data to achieve their best performance . Unfortunately , such resources are not easily available , particularly for languages other than English . This has lead to an increased interest in unsupervised and low - resource GEC ( Rozovskaya et al , 2017 ; Bryant and Briscoe , 2018 ; Boyd , 2018 ; Rozovskaya and Roth , 2019 ) , which recently culminated in the low - resource track of the Building Educational Application ( BEA ) shared task . 1 We present Minimally - Augmented Grammatical Error Correction ( MAGEC ) , a simple but effective approach to unsupervised and low - resource GEC which does not require any authentic error - labelled training data . A neural sequence - to - sequence model is trained on clean and synthetically noised sentences alone . The noise is automatically created from confusion sets . Additionally , if labelled data 1 https://www.cl.cam.ac.uk/research/nl/ bea2019st is available for fine - tuning ( Hinton and Salakhutdinov , 2006 ) , MAGEC can also serve as an efficient pre - training technique . The proposed unsupervised synthetic error generation method does not require a seed corpus with example errors as most other methods based on statistical error injection ( Felice and Yuan , 2014 ) or back - translation models ( Rei et al , 2017 ; Kasewa et al , 2018 ; Htut and Tetreault , 2019 ) . It also outperforms noising techniques that rely on random word replacements ( Xie et al , 2018 ; Zhao et al , 2019 ) . Contrary to Ge et al ( 2018 ) or Lichtarge et al ( 2018 ) , our approach can be easily used for effective pre - training of full encoder - decoder models as it is model - independent and only requires clean monolingual data and potentially an available spell - checker dictionary . 2 In comparison to pretraining with BERT ( Devlin et al , 2019 ) , synthetic errors provide more task - specific training examples than masking . As an unsupervised approach , MAGEC is an alternative to recently proposed language model ( LM ) based approaches ( Bryant and Briscoe , 2018 ; Stahlberg et al , 2019 ) , but it does not require any amount of annotated sentences for tuning .", "entities": [[5, 8, "TaskName", "grammatical error correction"], [103, 106, "TaskName", "Grammatical Error Correction"], [337, 338, "MethodName", "BERT"]]}
{"text": "Our minimally - augmented GEC approach uses synthetic noise as its primary source of training data . We generate erroneous sentences from monolingual texts via random word perturbations selected from automatically created confusion sets . These are traditionally defined as sets of frequently confused words ( Rozovskaya and Roth , 2010 ) . We experiment with three unsupervised methods for generating confusion sets : Edit distance Confusion sets consist of words with the shortest Levenshtein distance ( Levenshtein , 1966 ) to the selected confused word . Word embeddings Confusion sets contain the most similar words to the confused word based on the cosine similarity of their word embedding vectors ( Mikolov et al , 2013 ) . Spell - breaking Confusion sets are composed of suggestions from a spell - checker ; a suggestion list is extracted for the confused word regardless of its actual correctness . These methods can be used to build confusion sets for any alphabetic language . 3 We find that confusion sets constructed via spell - breaking perform best ( Section 4 ) . Most context - free spell - checkers combine a weighted edit distance and phonetic algorithms to order suggestions , which produces reliable confusion sets ( Table 1 ) . We synthesize erroneous sentences as follows : given a confusion set C i = { c i 1 , c i 2 , c i 3 , ... } , and the vocabulary V , we sample word w j V from the input sentence with a probability approximated with a normal distribution N ( p WER , 0.2 ) , and perform one of the following operations : ( 1 ) substitution of w j with a random word c j k from its confusion set with probability p sub , ( 2 ) deletion of w j with p del , ( 3 ) insertion of a random word w k V at j + 1 with p ins , and ( 4 ) swapping w j and w j+1 with p swp . When making a substitution , words within confusion sets are sampled uniformly . To improve the model 's capability of correcting spelling errors , inspired by Lichtarge et al ( 2018 ) ; Xie et al ( 2018 ) , we randomly perturb 10 % of characters using the same edit operations as above . Character - level noise is introduced on top of the synthetic errors generated via confusion sets . A MAGEC model is trained solely on the synthetically noised data and then ensembled with a language model . Being limited only by the amount of clean monolingual data , this large - scale unsupervised approach can perform better than training on small authentic error corpora . A large amount of training examples increases the chance that synthetic errors resemble real error patterns and results in better language modelling properties . If any small amount of error - annotated learner data is available , it can be used to fine - tune the pre - trained model and further boost its performance . Pre - training of decoders of GEC models from language models has been introduced by Junczys - Dowmunt et al ( 2018b ) , we pretrain the full encoder - decoder models instead , as proposed by Grundkiewicz et al ( 2019 ) .", "entities": [[87, 89, "TaskName", "Word embeddings"], [484, 486, "TaskName", "language modelling"]]}
{"text": "Confusion sets On English data , all proposed confusion set generation methods perform better than random word substitution ( Table 3 ) .Confusion sets based on word embeddings are the least effective , while spell - broken sets perform best at 26.66 F 0.5 . We observe further gains of +1.04 from keeping out - of - vocabulary spell - checker suggestions ( OOV ) and preserving consistent letter casing within confusion sets ( Case ) . The word error rate of error corpora is an useful statistic that can be used to balance precision / recall ratios ( Rozovskaya and Roth , 2010 ; Junczys - Dowmunt et al , 2018b ; Hotate et al , 2019 ) . Increasing WER in the synthetic data from 15 % to 25 % increases recall at the expense of precision , but no overall improvement is observed . A noticeable recall gain that transfers to a higher F - score of 28.99 is achieved by increasing the importance of edited fragments with the edit - weighted MLE objective from Junczys - Dowmunt et al ( 2018b ) with \u039b = 2 . We use this setting for the rest of our experiments .", "entities": [[26, 28, "TaskName", "word embeddings"]]}
{"text": "The GEC task involves detection and correction of all types of error in written texts , including grammatical , lexical and orthographical errors . Spelling and punctuation errors are among the most frequent error types and also the easiest to synthesize . To counter the argument that - mostly due to the introduced character - level noise and strong language modelling - MAGEC can only correct these \" simple \" errors , we evaluate it against test sets that contain either spelling and punctuation errors or all other error types ; with the complement errors corrected ( Table 6 ) . Our systems indeed perform best on misspellings and punctuation errors , but are capable of correcting various error types . The disparity for Russian can be explained by the fact that it is a morphologically - rich language and we suffer from generally lower performance .", "entities": [[59, 61, "TaskName", "language modelling"]]}
{"text": "We have presented Minimally - Augmented Grammatical Error Correction ( MAGEC ) , which can be effectively used in both unsupervised and lowresource scenarios . The method is model independent , requires easily available resources , and can be used for creating reliable baselines for supervised techniques or as an efficient pre - training method for neural GEC models with labelled data . We have demonstrated the effectiveness of our method and outperformed state - of - the - art results for German and Russian benchmarks , trained with labelled data , by a large margin . For future work , we plan to evaluate MAGEC on more languages and experiment with more diversified confusion sets created with additional unsupervised generation methods .", "entities": [[6, 9, "TaskName", "Grammatical Error Correction"]]}
{"text": "Incorporating Priors with Feature Attribution on Text Classification", "entities": [[6, 8, "TaskName", "Text Classification"]]}
{"text": "In this section , we give formal definitions of feature attribution and a primer on [ Path ] Integrated Gradients ( IG ) , which is the basis for our method . Definition 2.1 . Given a function f : R n [ 0 , 1 ] that represents a model , and an input x = ( x 1 , ... , x n ) R n . An attribution of the prediction at input x is a vector a = ( a 1 , ... , a n ) and a i is defined as the attribution of x i . Feature attribution methods have been studied to understand the contribution of each input feature to the output prediction score . This contribution , then , can further be used to interpret model decisions . Linear models are considered to be more desirable because of their implicit interpretability , where feature attribution is the product of the feature value and the coefficient . To some , non - linear models such as gradient boosting trees and neural networks are less favorable due to the fact that they do not enjoy such transparent contribution of each feature and are harder to interpret ( Lou et al , 2012 ) . Despite the complexity of these models , prior work has been able to extract attributions with gradient based methods ( Smilkov et al , 2017 ) , Shapley values from game theory ( SHAP ) ( Lundberg and Lee , 2017 ) , or other similar methods ( Bach et al , 2015 ; Shrikumar et al , 2017 ) . Some of these attributions methods , for example Path Intergrated Gradients and SHAP , not only follow Definition 2.1 , but also satisfy axioms or properties that resemble linear models . One of these axioms is completeness , which postulates that the sum of attributions should be equal to the difference between uncertainty and model output .", "entities": [[43, 44, "DatasetName", "0"], [244, 245, "MethodName", "SHAP"], [284, 285, "MethodName", "SHAP"]]}
{"text": "When taking the derivative with respect to the loss , we treat the interpolated embeddings as constants . Thus , the prior loss does not back - propagate to the embedding parameters . There are two reasons that lead to this decision : ( i ) taking the gradient of the interpolate operation would break the axioms that IG guarantees ; ( ii ) the Hessian of the embedding matrix is slow to compute . The implementation decision does not imply that prior loss has no effect on the word embeddings , though . During training , the model parameters are updated with respect to both losses . Therefore , the word embeddings had to adjust accordingly to the new model parameters by updating the embedding parameters with cross - entropy loss .", "entities": [[8, 9, "MetricName", "loss"], [22, 23, "MetricName", "loss"], [83, 84, "MetricName", "loss"], [89, 91, "TaskName", "word embeddings"], [111, 113, "TaskName", "word embeddings"], [131, 132, "MetricName", "loss"]]}
{"text": "Models convert input tokens to embeddings before providing them to convolutional layers . As embeddings make up the majority of the parameters of the network and can be exported for use in other tasks , we 're interested in how they change for the identity terms . We show 10 nearest neighbors of the terms < i d > ( for the token replacement method ) , \" gay \" , and \" homosexual \" - top two identity terms with the most mean attribution difference ( our method vs. baseline ) , in Table 6 . The word embedding of the term \" gay \" shifts from having swear words as its neighbors to having the < pad > token as the closest neighbor . Although the term \" homosexual \" has lower mean attribution , its neighboring words are still mostly swear words in the baseline embedding space . \" homosexual \" also moved to more neutral terms that should n't play a role in deciding if the comment is toxic or not . Although they are not as high quality as one would expect general - purpose word embeddings to be possibly due to data size and the model having a different objective , the results show that our method yields inherently unbiased embeddings . It removes the necessity to initialize word embeddings with pre - debiased embeddings as proposed in Bolukbasi et al ( 2016 ) . The importance weighting technique penalizes the model on the sentence level instead of focusing on the token level . Therefore , the word embedding of \" gay \" does n't seem to shift to neutral words . The token replacement method , on the other hand , replaces the identity terms with a token that is surrounded with neutral words in the embedding space , so it results in greater improvement on the synthetic dataset . However , since all identity terms are collapsed into one , it 's harder for the model to capture the context and as a result , classification performance on the original dataset drops .", "entities": [[190, 192, "TaskName", "word embeddings"], [224, 226, "TaskName", "word embeddings"]]}
