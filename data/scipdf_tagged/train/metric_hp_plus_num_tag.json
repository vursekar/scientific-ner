{"text": "This paper describes the result of team - Maoqin at DravidianLangTech - EACL2021 . The provided task consists of three languages ( Tamil , Malayalam , and Kannada ) , I only participate in one of the language task - Malayalam . The goal of this task is to identify offensive language content of the code - mixed dataset of comments / posts in Dravidian Languages ( Tamil - English , Malayalam - English , and Kannada - English ) collected from social media . This is a classification task at the comment / post level . Given a Youtube comment , systems have to classify it into Notoffensive , Offensive - untargeted , Offensivetargeted - individual , Offensive - targeted - group , Offensive - targeted - other , or Not - in - indentedlanguage . I use the transformer - based language model with BiGRU - Attention to complete this task . To prove the validity of the model , I also use some other neural network models for comparison . And finally , the team ranks 5th in this task with a weighted average F1 score of 0.93 on the private leader board .", "entities": [[146, 147, "MethodName", "BiGRU"], [186, 188, "MetricName", "average F1"]]}
{"text": "Offensive language refers to direct or indirect use of verbal abuse , slander , contempt , ridicule , and other means to infringe or damage the dignity , spiritual world , and mental health of others . It will seriously affect the mental state of others , disrupt work , the life and learning order of others , and seriously pollute the public opinion environment of the entire network ( Schmidt and Wiegand , 2017 ) . Due to the development of the Internet and the popularity of anonymous comments , many offensive languages have spread on the Internet and caused trouble to relevant personnel Mahesan , 2019 , 2020a , b ) . Relevant organizations should take measures to prevent this from happening . It is unrealistic to judge whether online sentences are completely offended by humans . There - fore , mechanical methods must be used to distinguish whether the language is offensive . The task is to directly test whether the system can distinguish offensive language in Dravidian languages . Dravidian languages are a group of languages spoken by 220 million people , predominantly in southern India and northern Sri Lanka , but also in other areas of South Asia . The Dravidian languages were first recorded in Tamili script inscribed on cave walls in Tamil Nadu 's Madurai and Tirunelveli districts in the 6th century BCE . The Dravidian languages are closely related languages the are under - resourced ( Chakravarthi , 2020 ) . Existing deep learning and pre - training models have achieved good results on other tasks ( Zampieri et al , 2019 ) , so I use the deep learning method to deal with the related task . According to the latest related research progress , the transformer - based language model has become my preferred model . Because the pre - trained and fine - tuned transformersbased models have shown excellent performance in many NLP problems , such as sentiment classification and automatic extraction of text summaries . So I choose ALBERT ( Lan et al , 2019 ) as my basic model in this task . To get a more effective and higher accuracy model , BiGRU combined with attention . To prove the effectiveness of this model , I have also done comparative experiments with other neural networks . In this task , my model is an effective way to perform well . To obtain as much effective information as possible from the limited data , I also use the 5 - fold cross - validation method . my model achieves the desired result . The rest of this article is structured as follows . Section 2 introduces related work . Model and data preparation are described in Section 3 . Experiments and evaluation are described in Section 4 . Section 5 describes the results of my work . The conclusions and future work are drawn in Section 6 .", "entities": [[340, 341, "MethodName", "ALBERT"], [363, 364, "MetricName", "accuracy"], [366, 367, "MethodName", "BiGRU"]]}
{"text": "There are many competitions about offensive language detection ( such as HASOC ( Chakravarthi et al , 2020c ; Mandl et al , 2020 ) and TRAC ( Kumar et al , 2018 ) ) , and many corresponding methods have been produced . People often tend to abstract this task into a text classification task ( Howard and Ruder , 2018 ) . Text classification is called extracting features from original text data and predicting the category of text data based on these features . In the past few decades , many models for text classification have been proposed ( Qian , 2020 ) . From the 1960s to the 2010s , text classification models based on shallow learning dominated . Shallow learning means statistical - based models such as Naive Bayes ( NB ) , K Nearest Neighbors ( KNN ) ( Cover and Hart , 1967 ) and Support Vector Machines ( SVM ) . Compared with earlier rulebased methods , this method has obvious advantages in accuracy and stability . However , these methods still require functional design , which is time - consuming and expensive . In addition , they usually ignore the natural order structure or context information in the text data , which makes learning the semantic information of words difficult . Since the 2010s , text classification has gradually changed from a shallow learning model to a deep learning model . Compared with methods based on shallow learning , deep learning methods avoid the manual design of rules and functions and automatically provide semantically meaningful representations for text mining . Therefore , most of the text classification research work is based on DNN ( Yu et al , 2013 ) , which is a data - driven method with high computational complexity . Few studies have focused on shallow learning models to solve the limitations of computation and data . The shallow learning model speeds up the text classification speed , improves the accuracy , and expands the application range of shallow learning . The shallow learning method is a type of machine learning . It learns from data , which is a predefined function that is important to the performance of the predicted value . However , element engineering is an arduous and giant job . Before training the classifier , we need to collect knowledge or experience to extract features from the original text . The shallow learning method trains the initial classifier based on various text features extracted from the original text . For small data sets , under the limita -", "entities": [[28, 29, "DatasetName", "Kumar"], [53, 55, "TaskName", "text classification"], [64, 66, "TaskName", "Text classification"], [95, 97, "TaskName", "text classification"], [113, 115, "TaskName", "text classification"], [155, 156, "MethodName", "SVM"], [170, 171, "MetricName", "accuracy"], [223, 225, "TaskName", "text classification"], [273, 275, "TaskName", "text classification"], [325, 327, "TaskName", "text classification"], [331, 332, "MetricName", "accuracy"]]}
{"text": "The ALBERT model belongs to transformer - based language models . The ALBERT model is improved on the basis of Bidirectional Encoder Representations for Transformers ( BERT ) ( Devlin et al , 2018 ) model . It has designed a parameter reduction method to reduce memory consumption by changing the result of the original embedding parameter P ( the product of the vocabulary size V and the hidden layer size H ) . V * H = P V * E + E * H = P ( 1 ) E represents the size of the low - dimensional embedding space . In BERT , E = H. While in AL - BERT , H > > E , so the number of parameters will be greatly reduced . At the same time , the self - supervised loss is used to focus on the internal coherence in the construction of sentences . The ALBERT model implements three embedding layers : word embedding , position embedding , and segment embedding . The token embedding layer predicts each word as a fixed - size vector . Position embedding is used to retain position information , use a vector to randomly initialize each position , add model training , and finally obtain an embedding containing position information . Segment embedding helps BERT distinguish between paired input sequences .", "entities": [[1, 2, "MethodName", "ALBERT"], [12, 13, "MethodName", "ALBERT"], [26, 27, "MethodName", "BERT"], [68, 71, "HyperparameterName", "hidden layer size"], [104, 105, "MethodName", "BERT"], [113, 114, "MethodName", "BERT"], [122, 125, "HyperparameterName", "number of parameters"], [139, 140, "MetricName", "loss"], [155, 156, "MethodName", "ALBERT"], [220, 221, "MethodName", "BERT"]]}
{"text": "In this task , I use the ALBERT model to pre - train the task . For the ALBERT model , the main hyperparameters I pay attention to are the training step size , batch size and learning rate . The parameters of my model are shown in Table 2 . I have obtained good performance using the ALBERT - BASE . 1 model . Considering that BiGRU - Attention can capture contextual information well and extract text information features more accurately ( Radford et al , 2018 ) , I add it after AL - BERT . I use the development data set to verify the performance of the models . The standard of judgment is a weighted F1 - score , and this standard is the judgment standard used for my task . Table3 lists the results of various models described previously . The best performance is in bold . My model gets the best performance of 0.93 . As shown in the table my model can greatly improve the performance and my overall approach achieved 5th place on the final leader board .", "entities": [[7, 8, "MethodName", "ALBERT"], [18, 19, "MethodName", "ALBERT"], [31, 33, "HyperparameterName", "step size"], [34, 36, "HyperparameterName", "batch size"], [37, 39, "HyperparameterName", "learning rate"], [58, 59, "MethodName", "ALBERT"], [60, 61, "MethodName", "BASE"], [67, 68, "MethodName", "BiGRU"], [96, 97, "MethodName", "BERT"], [119, 122, "MetricName", "F1 - score"]]}
{"text": "Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs . Recent advances in GPU hardware have led to the emergence of bi - directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) . Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency . This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction . Unlike LSTMs whose sequential processing on sentences of length N requires O ( N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents . We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF . Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8x faster test time speeds .", "entities": [[60, 61, "TaskName", "NER"], [71, 72, "MethodName", "CRF"], [104, 105, "TaskName", "NER"], [128, 130, "TaskName", "structured prediction"], [194, 195, "MetricName", "accuracy"], [200, 201, "MethodName", "LSTM"], [202, 203, "MethodName", "CRF"]]}
{"text": "In order to democratize large - scale NLP and information extraction while minimizing our environmental footprint , we require fast , resource - efficient methods for sequence tagging tasks such as part - of - speech tagging and named entity recognition ( NER ) . Speed is not sufficient of course : they must also be expressive enough to tolerate the tremendous lexical variation in input data . The massively parallel computation facilitated by GPU hardware has led to a surge of successful neural network architectures for sequence labeling ( Ling et al , 2015 ; Ma and Hovy , 2016 ; Chiu and Nichols , 2016 ; Lample et al , 2016 ) . While these models are expressive and accurate , they fail to fully exploit the parallelism opportunities of a GPU , and thus their speed is limited . Specifically , they employ either recurrent neural networks ( RNNs ) for feature extraction , or Viterbi inference in a structured output model , both of which require sequential computation across the length of the input . Instead , parallelized runtime independent of the length of the sequence saves time and energy costs , maximizing GPU resource usage and minimizing the amount of time it takes to train and evaluate models . Convolutional neural networks ( CNNs ) provide exactly this property ( Kim , 2014 ; Kalchbrenner et al , 2014 ) . Rather than composing representations incrementally over each token in a sequence , they apply filters in parallel across the entire sequence at once . Their computational cost grows with the number of layers , but not the input size , up to the memory and threading limitations of the hardware . This provides , for example , audio generation models that can be trained in parallel . Despite the clear computational advantages of CNNs , RNNs have become the standard method for composing deep representations of text . This is because a token encoded by a bidirectional RNN will incorporate evidence from the entire input sequence , but the CNN 's representation is limited by the effective input width 1 of the network : the size of the input context which is observed , directly or indirectly , by the representation of a token at a given layer in the network . Specifically , in a network composed of a series of stacked convolutional layers of convolution width w , the number r of context tokens incorporated into a token 's representation at a given layer l , is given by r = l ( w \u2212 1 ) + 1 . The number of layers required to incorporate the entire input context grows linearly with the length of the sequence . To avoid this scaling , one could pool representations across the sequence , but this is not appropriate for sequence labeling , since it reduces the output resolution of the representation . In response , this paper presents an application of dilated convolutions ( Yu and Koltun , 2016 ) for sequence labeling ( Figure 1 ) . For dilated convolutions , the effective input width can grow exponentially with the depth , with no loss in resolution at each layer and with a modest number of parameters to estimate . Like typical CNN layers , dilated convolutions operate on a sliding window of context over the sequence , but unlike conventional convolutions , the context need not be consecutive ; the dilated window skips over every dilation width d inputs . By stacking layers of dilated convolutions of exponentially increasing dilation width , we can expand the size of the effective input width to cover the entire length of most sequences using only a few layers : The size of the effective input width for a token at layer l is now given by 2 l+1 \u22121 . More concretely , just four stacked dilated convolutions of width 3 produces token representations with a n effective input width of 31 tokens - longer than the average sentence length ( 23 ) in the Penn TreeBank . Our overall iterated dilated CNN architecture ( ID - CNN ) repeatedly applies the same block of dilated convolutions to token - wise representations . This parameter sharing prevents overfitting and also provides opportunities to inject supervision on intermediate activations of the network . Similar to models that use logits produced by an RNN , the ID - CNN provides two methods for performing prediction : we can predict each token 's label independently , or by running Viterbi inference in a chain structured graphical model .", "entities": [[31, 37, "TaskName", "part - of - speech tagging"], [38, 41, "TaskName", "named entity recognition"], [42, 43, "TaskName", "NER"], [266, 269, "HyperparameterName", "number of layers"], [293, 295, "TaskName", "audio generation"], [402, 403, "MethodName", "convolution"], [439, 442, "HyperparameterName", "number of layers"], [533, 534, "MetricName", "loss"], [543, 546, "HyperparameterName", "number of parameters"], [682, 684, "DatasetName", "Penn TreeBank"]]}
{"text": "1 What we call effective input width here is known as the receptive field in the vision literature , drawing an analogy to the visual receptive field of a neuron in the retina . 5.0 English NER , we demonstrate significant speed gains of our ID - CNNs over various recurrent models , while maintaining similar F1 performance . When performing prediction using independent classification , the ID - CNN consistently outperforms a bidirectional LSTM ( Bi - LSTM ) , and performs on par with inference in a CRF with logits from a Bi - LSTM ( Bi - LSTM - CRF ) . As an extractor of per - token logits for a CRF , our model out - performs the Bi - LSTM - CRF . We also apply ID - CNNs to entire documents , where independent token classification is as accurate as the Bi - LSTM - CRF while decoding almost 8\u00d7 faster . The clear accuracy gains resulting from incorporating broader context suggest that these models could similarly benefit many other contextsensitive NLP tasks which have until now been limited by the computational complexity of existing context - rich models . 2 2 Background", "entities": [[36, 37, "TaskName", "NER"], [56, 57, "MetricName", "F1"], [73, 75, "MethodName", "bidirectional LSTM"], [78, 79, "MethodName", "LSTM"], [89, 90, "MethodName", "CRF"], [96, 97, "MethodName", "LSTM"], [100, 101, "MethodName", "LSTM"], [102, 103, "MethodName", "CRF"], [115, 116, "MethodName", "CRF"], [125, 126, "MethodName", "LSTM"], [127, 128, "MethodName", "CRF"], [141, 143, "TaskName", "token classification"], [150, 151, "MethodName", "LSTM"], [152, 153, "MethodName", "CRF"], [161, 162, "MetricName", "accuracy"]]}
{"text": "CNNs in NLP are typically one - dimensional , applied to a sequence of vectors representing tokens rather than to a two - dimensional grid of vectors representing pixels . In this setting , a convolutional neural network layer is equivalent to applying an affine transformation , W c to a sliding window of width r tokens on either side of each token in the sequence . Here , and throughout the paper , we do not explicitly write the bias terms in affine transformations . The convolutional operator applied to each token x t with output c t is defined as : c t = W c r k=0 x t\u00b1k , ( 3 ) where is vector concatenation . Dilated convolutions perform the same operation , except rather than transforming adjacent in - puts , the convolution is defined over a wider effective input width by skipping over \u03b4 inputs at a time , where \u03b4 is the dilation width . We define the dilated convolution operator : c t = W c r k=0 x t\u00b1k\u03b4 . ( 4 ) A dilated convolution of width 1 is equivalent to a simple convolution . Using the same number of parameters as a simple convolution with the same radius ( i.e. W c has the same dimensionality ) , the \u03b4 > 1 dilated convolution incorporates broader context into the representation of a token than a simple convolution .", "entities": [[138, 139, "MethodName", "convolution"], [150, 151, "HyperparameterName", "\u03b4"], [157, 158, "HyperparameterName", "\u03b4"], [166, 168, "MethodName", "dilated convolution"], [184, 186, "MethodName", "dilated convolution"], [194, 195, "MethodName", "convolution"], [199, 202, "HyperparameterName", "number of parameters"], [205, 206, "MethodName", "convolution"], [221, 222, "HyperparameterName", "\u03b4"], [224, 226, "MethodName", "dilated convolution"], [238, 239, "MethodName", "convolution"]]}
{"text": "We can leverage the ability of dilated convolutions to incorporate global context without losing important local information by stacking dilated convolutions of increasing width . First described for pixel classification in computer vision , Yu and Koltun ( 2016 ) achieve state - of - the - art results on image segmentation benchmarks by stacking dilated convolutions with exponentially increasing rates of dilation , a technique they refer to as multiscale context aggregation . By feeding the outputs of each dilated convolution as the input to the next , increasingly non - local information is incorporated into each pixel 's representation . Performing a dilation - 1 convolution in the first layer ensures that no pixels within the effective input width of any pixel are excluded . By doubling the dilation width at each layer , the size of the effective input width grows exponentially while the number of parameters grows only linearly with the number of layers , so a pixel representation quickly incorporates rich global evidence from the entire image . Unfortunately , simply increasing the depth of stacked dilated CNNs causes considerable overfitting in our experiments . In response , we present Iterated Dilated CNNs ( ID - CNNs ) , which instead apply the same small stack of dilated convolutions multiple times , each iterate taking as input the result of the last application . Repeatedly employing the same parameters in a recurrent fashion provides both broad effective input width and desirable generalization capabilities . We also obtain significant accuracy gains with a training objective that strives for accurate labeling after each iterate , allowing follow - on iterations to observe and resolve dependency violations .", "entities": [[80, 82, "MethodName", "dilated convolution"], [107, 108, "MethodName", "convolution"], [147, 150, "HyperparameterName", "number of parameters"], [155, 158, "HyperparameterName", "number of layers"], [253, 254, "MetricName", "accuracy"]]}
{"text": "The network takes as input a sequence of T vectors x t , and outputs a sequence of per - class scores h t , which serve either as the local conditional distributions of Eqn . ( 1 ) or the local factors \u03c8 t of Eqn . ( 2 ) . We denote the jth dilated convolutional layer of dilation width \u03b4 as D ( j ) \u03b4 . The first layer in the net - work is a dilation - 1 convolution D ( 0 ) 1 that transforms the input to a representation i t : i t = D ( 0 ) 1 x t ( 5 ) Next , L c layers of dilated convolutions of exponentially increasing dilation width are applied to i t , folding in increasingly broader context into the embedded representation of x t at each layer . Let r ( ) denote the ReLU activation function ( Glorot et al , 2011 ) . Beginning with c t ( 0 ) = i t we define the stack of layers with the following recurrence : c t ( j ) = r D ( j\u22121 ) 2 Lc\u22121 c t ( j\u22121 ) ( 6 ) and add a final dilation - 1 layer to the stack : c t ( Lc+1 ) = r D ( Lc ) 1 c t ( Lc ) ( 7 ) We refer to this stack of dilated convolutions as a block B ( ) , which has output resolution equal to its input resolution . To incorporate even broader context without over - fitting , we avoid making B deeper , and instead iteratively apply B L b times , introducing no extra parameters . Starting with b t ( 1 ) = B ( i t ) : b t ( k ) = B b t ( k\u22121 ) ( 8 ) We apply a simple affine transformation W o to this final representation to obtain per - class scores for each token x t : h t ( L b ) = W o b t ( L b ) ( 9 )", "entities": [[62, 63, "HyperparameterName", "\u03b4"], [68, 69, "HyperparameterName", "\u03b4"], [83, 84, "MethodName", "convolution"], [86, 87, "DatasetName", "0"], [104, 105, "DatasetName", "0"], [153, 154, "MethodName", "ReLU"], [154, 156, "HyperparameterName", "activation function"], [169, 170, "DatasetName", "0"]]}
{"text": "Our main focus is to apply the ID - CNN an encoder to produce per - token logits for the first conditional model described in Sec . 2.1 , where tags are conditionally independent given deep features , since this will enable prediction that is parallelizable across the length of the input sequence . Here , maximum likelihood training is straightforward because the likelihood decouples into the sum of the likelihoods of independent logistic regression problems for every tag , with natural parameters given by Eqn . ( 9 ) : 1 T T t=1 log P ( y t | h t ( L b ) ) ( 10 ) We can also use the ID - CNN as logits for the CRF model ( Eqn . ( 2 ) ) , where the partition function and its gradient are computed using the forward - backward algorithm . We next present an alternative training method that helps bridge the gap between these two techniques . Sec . 2.1 identifies that the CRF has preferable sample complexity and accuracy since prediction directly reasons in the space of structured outputs . In response , we compile some of this reasoning in output space into ID - CNN feature extraction . Instead of explicit reasoning over output labels during inference , we train the network such that each block is predictive of output labels . Subsequent blocks learn to correct dependency violations of their predecessors , refining the final sequence prediction . To do so , we first define predictions of the model after each of the L b applications of the block . Let h t ( k ) be the result of applying the matrix W o from ( 9 ) to b t ( k ) , the output of block k. We minimize the average of the losses for each application of the block : 1 L b L b k=1 1 T T t=1 log P ( y t | h t ( k ) ) . ( 11 ) By rewarding accurate predictions after each application of the block , we learn a model where later blocks are used to refine initial predictions . The loss also helps reduce the vanishing gradient problem ( Hochreiter , 1998 ) for deep architectures . Such an approach has been applied in a variety of contexts for training very deep networks in computer vision ( Romero et al , 2014 ; Szegedy et al , 2015 ; Lee et al , 2015 ; G\u00fcl\u00e7ehre and Bengio , 2016 ) , but not to our knowledge in NLP . We apply dropout ( Srivastava et al , 2014 ) to the raw inputs x t and to each block 's output b t b ) to help prevent overfitting . The version of dropout typically used in practice has the undesirable property that the randomized predictor used at train time differs from the fixed one used at test time . Ma et al ( 2017 ) present dropout with expectationlinear regularization , which explicitly regularizes these two predictors to behave similarly . All of our best reported results include such regularization . This is the first investigation of the technique 's effectiveness for NLP , including for RNNs . We encourage its further application .", "entities": [[73, 75, "MethodName", "logistic regression"], [123, 124, "MethodName", "CRF"], [172, 173, "MethodName", "CRF"], [178, 179, "MetricName", "accuracy"], [369, 370, "MetricName", "loss"]]}
{"text": "The state - of - the art models for sequence labeling include an inference step that searches the space of possible output sequences of a chain - structured graphical model , or approximates this search with a beam ( Collobert et al , 2011 ; Weiss et al , 2015 ; Lample et al , 2016 ; Ma and Hovy , 2016 ; Chiu and Nichols , 2016 ) . These outperform similar systems that use the same features , but independent local predictions . On the other hand , the greedy sequential prediction ( Daum\u00e9 III et al , 2009 ) approach of Ratinov and Roth ( 2009 ) , which employs lexicalized features , gazetteers , and word clusters , outperforms CRFs with similar features . LSTMs ( Hochreiter and Schmidhuber , 1997 ) were used for NER as early as the CoNLL shared task in 2003 ( Hammerton , 2003 ; Tjong Kim Sang and De Meulder , 2003 ) . More recently , a wide variety of neural network architectures for NER have been proposed . Collobert et al ( 2011 ) employ a one - layer CNN with pre - trained word embeddings , capitalization and lexicon features , and CRF - based prediction . Huang et al ( 2015 ) achieved state - of - the - art accuracy on partof - speech , chunking and NER using a Bi - LSTM - CRF . Lample et al ( 2016 ) proposed two models which incorporated Bi - LSTM - composed character embeddings alongside words : a Bi - LSTM - CRF , and a greedy stack LSTM which uses a simple shift - reduce grammar to compose words into labeled entities . Their Bi - LSTM - CRF obtained the state - of - the - art on four languages without word shape or lexicon features . Ma and Hovy ( 2016 ) use CNNs rather than LSTMs to compose characters in a Bi - LSTM - CRF , achieving state - ofthe - art performance on part - of - speech tagging and CoNLL NER without lexicons . Chiu and Nichols ( 2016 ) evaluate a similar network but propose a novel method for encoding lexicon matches , presenting results on CoNLL and OntoNotes NER . Yang et al ( 2016 ) use GRU - CRFs with GRUcomposed character embeddings of words to train a single network on many tasks and languages . In general , distributed representations for text can provide useful generalization capabilities for NER systems , since they can leverage unsupervised pre - training of distributed word representations ( Turian et al , 2010 ; Collobert et al , 2011 ; Passos et al , 2014 ) . Though our models would also likely benefit from additional features such as character representations and lexicons , we focus on simpler models which use word - embeddings alone , leaving more elaborate input representations to future work . In these NER approaches , CNNs were used for low - level feature extraction that feeds into alternative architectures . Overall , end - to - end CNNs have mainly been used in NLP for sentence classification , where the output representation is lower resolution than that of the input Kim ( 2014 ) ; Kalchbrenner et al ( 2014 ) ; ; Toutanova et al ( 2015 ) . Lei et al ( 2015 ) present a CNN variant where convolutions adaptively skip neighboring words . While the flexibility of this model is powerful , its adaptive behavior is not well - suited to GPU acceleration . Our work draws on the use of dilated convolutions for image segmentation in the computer vision community ( Yu and Koltun , 2016 ; Chen et al , 2015 ) . Similar to our block , Yu and Koltun ( 2016 ) employ a context - module of stacked dilated convolutions of exponentially increasing dilation width . Dilated convolutions were recently applied to the task of speech generation ( van den , and concurrent with this work , posted a pre - print describing the similar ByteNet network for machine translation that uses dilated convolutions in the encoder and decoder components . Our basic model architecture is similar to that of the ByteNet encoder , except that the inputs to our model are tokens and not bytes . Additionally , we present a novel loss and parameter sharing scheme to facilitate training models on much smaller datasets than those used by . We are the first to use dilated convolutions for sequence labeling . The broad effective input width of the ID - CNN helps aggregate document - level context . Ratinov and Roth ( 2009 ) incorporate document context in their greedy model by adding features based on tagged entities within a large , fixed window of tokens . Prior work has also posed a structured model that couples predictions across the whole document ( Bunescu and Mooney , 2004 ; Sutton and McCallum , 2004 ; Finkel et al , 2005 ) .", "entities": [[139, 140, "TaskName", "NER"], [175, 176, "TaskName", "NER"], [196, 198, "TaskName", "word embeddings"], [205, 206, "MethodName", "CRF"], [224, 225, "MetricName", "accuracy"], [230, 231, "TaskName", "chunking"], [232, 233, "TaskName", "NER"], [237, 238, "MethodName", "LSTM"], [239, 240, "MethodName", "CRF"], [254, 255, "MethodName", "LSTM"], [265, 266, "MethodName", "LSTM"], [267, 268, "MethodName", "CRF"], [273, 274, "MethodName", "LSTM"], [292, 293, "MethodName", "LSTM"], [294, 295, "MethodName", "CRF"], [332, 333, "MethodName", "LSTM"], [334, 335, "MethodName", "CRF"], [344, 350, "TaskName", "part - of - speech tagging"], [352, 353, "TaskName", "NER"], [381, 382, "DatasetName", "OntoNotes"], [382, 383, "TaskName", "NER"], [391, 392, "MethodName", "GRU"], [424, 425, "TaskName", "NER"], [431, 435, "TaskName", "unsupervised pre - training"], [499, 500, "TaskName", "NER"], [532, 534, "TaskName", "sentence classification"], [694, 696, "TaskName", "machine translation"], [739, 740, "MetricName", "loss"]]}
{"text": "We describe experiments on two benchmark English named entity recognition datasets . On CoNLL - 2003 English NER , our ID - CNN performs on par with a Bi - LSTM not only when used to produce per - token logits for structured inference , but the ID - CNN with greedy decoding also performs on - par with the Bi - LSTM - CRF while running at more than 14 times the speed . We also observe a performance boost in almost all models when broadening the context to incorporate entire documents , achieving an average F1 of 90.65 on CoNLL - 2003 , out - performing the sentence - level model while still decoding at nearly 8 times the speed of the Bi - LSTM - CRF .", "entities": [[7, 10, "TaskName", "named entity recognition"], [17, 18, "TaskName", "NER"], [30, 31, "MethodName", "LSTM"], [62, 63, "MethodName", "LSTM"], [64, 65, "MethodName", "CRF"], [96, 98, "MetricName", "average F1"], [126, 127, "MethodName", "LSTM"], [128, 129, "MethodName", "CRF"]]}
{"text": "We evaluate using labeled data from the CoNLL - 2003 shared task ( Tjong Kim Sang and De Meulder , 2003 ) and OntoNotes 5.0 ( Hovy et al , 2006 ; Pradhan et al , 2006 ) . Following previous work , we use the same OntoNotes data split used for co - reference resolution in the CoNLL - 2012 shared task ( Pradhan et al , 2012 ) . For both datasets , we convert the IOB boundary encoding to BILOU as previous work found this encoding to result in improved performance ( Ratinov and Roth , 2009 ) . As in previous work we evaluate the performance of our models using segment - level micro - averaged F1 score . Hyperparameters that resulted in the best performance on the validation set were selected via grid search . A more detailed description of the data , evaluation , optimization and data pre - processing can be found in the Appendix .", "entities": [[23, 25, "DatasetName", "OntoNotes 5.0"], [47, 48, "DatasetName", "OntoNotes"], [120, 122, "MetricName", "F1 score"]]}
{"text": "We compare our ID - CNN against strong LSTM and CNN baselines : a Bi - LSTM with local decoding , and one with CRF decoding ( Bi - LSTM - CRF ) . We also compare against a non - dilated CNN architecture with the same number of convolutional layers as our dilated network ( 4 - layer CNN ) and one with enough layers to incorporate an effective input width of the same size as that of the dilated network ( 5 - layer CNN ) to demonstrate that the dilated convolutions more effectively aggregate contextual information than simple convolutions ( i.e. using fewer parameters ) . We also compare our document - level ID - CNNs to a baseline which does not share parameters between blocks ( noshare ) and one that computes loss only at the last block , rather than after every iterated block of dilated convolutions ( 1 - loss ) . We do not compare with deeper or more elaborate CNN architectures for a number of reasons : 1 ) Fast train and test performance are highly desirable for NLP practitioners , and deeper models require more computation time 2 ) more complicated models tend to over - fit on this relatively small dataset and 3 ) most accurate deep CNN architectures repeatedly up - sample and down - sample the inputs . We do not compare to stacked LSTMs for similar reasons - a single LSTM is already slower than a 4 - layer CNN . Since our task is sequence labeling , we desire a model that maintains the token - level resolution of the input , making dilated convolutions an elegant solution .", "entities": [[8, 9, "MethodName", "LSTM"], [16, 17, "MethodName", "LSTM"], [24, 25, "MethodName", "CRF"], [29, 30, "MethodName", "LSTM"], [31, 32, "MethodName", "CRF"], [136, 137, "MetricName", "loss"], [155, 156, "MetricName", "loss"], [243, 244, "MethodName", "LSTM"]]}
{"text": "Table 1 lists F1 scores of models predicting with sentence - level context on CoNLL - 2003 . For models that we trained , we report F1 and standard deviation obtained by averaging over 10 random restarts . The Viterbi - decoding Bi - LSTM - CRF and ID - CNN - CRF and greedy ID - CNN obtain the highest average scores , with the ID - CNN - CRF outperforming the Bi - LSTM - CRF by 0.11 points of F1 on average , and the Bi - LSTM - CRF out - performing the greedy ID - CNN by 0.11 as well . Our greedy ID - CNN outperforms the Bi - LSTM and the 4 - layer CNN , which uses the same number of parameters as the ID - CNN , and performs similarly to the 5 - layer CNN which uses more parameters but covers the same effective input width . All CNN models out - perform the Bi - Model F1 Ratinov and Roth ( 2009 ) 86.82 Collobert et al ( 2011 ) 86.96 Lample et al ( 2016 ) 90.33 Bi - LSTM 89.34 \u00b1 0.28 4 - layer CNN 89.97 \u00b1 0.20 5 - layer CNN 90.23 \u00b1 0.16 ID - CNN 90.32 \u00b1 0.26 Collobert et al ( 2011 ) 88.67 Passos et al ( 2014 ) 90.05 Lample et al ( 2016 ) 90.20 Bi - LSTM - CRF ( re - impl ) 90.43 \u00b1 0.12 ID - CNN - CRF 90.54 \u00b1 0.18 LSTM when paired with greedy decoding , suggesting that CNNs are better token encoders than Bi - LSTMs for independent logistic regression . When paired with Viterbi decoding , our ID - CNN performs on par with the Bi - LSTM , showing that the ID - CNN is also an effective token encoder for structured inference . Our ID - CNN is not only a better token encoder than the Bi - LSTM but it is also faster . Table 2 lists relative decoding times on the CoNLL development set , compared to the Bi - LSTM - CRF . We report decoding times using the fastest batch size for each method . 3 The ID - CNN model decodes nearly 50 % faster than the Bi - LSTM . With Viterbi decoding , the gap closes somewhat but the ID - CNN - CRF still comes out ahead , about 30 % faster than the Bi - LSTM - CRF . The most vast speed improvements come when comparing the greedy ID - CNN to the Bi - LSTM - CRF - our ID - CNN is more than 14 times faster than the Bi - LSTM - CRF at test time , with comparable accuracy . The 5 - layer CNN , which observes the same effective input width as the ID - CNN but with more parameters , performs at about the same speed as the ID - CNN in our experiments . With a better implementation of dilated convolutions than currently included in TensorFlow , we would expect the ID - CNN to be notably faster than 3 : Comparison of models trained with and without expectation - linear dropout regularization ( DR ) . DR improves all models . the 5 - layer CNN . We emphasize the importance of the dropout regularizer of Ma et al ( 2017 ) in Table 3 , where we observe increased F1 for every model trained with expectation - linear dropout regularization . Dropout is important for training neural network models that generalize well , especially on relatively small NLP datasets such as CoNLL - 2003 . We recommend this regularizer as a simple and helpful tool for practitioners training neural networks for NLP .", "entities": [[3, 4, "MetricName", "F1"], [26, 27, "MetricName", "F1"], [44, 45, "MethodName", "LSTM"], [46, 47, "MethodName", "CRF"], [52, 53, "MethodName", "CRF"], [70, 71, "MethodName", "CRF"], [75, 76, "MethodName", "LSTM"], [77, 78, "MethodName", "CRF"], [82, 83, "MetricName", "F1"], [90, 91, "MethodName", "LSTM"], [92, 93, "MethodName", "CRF"], [115, 116, "MethodName", "LSTM"], [127, 130, "HyperparameterName", "number of parameters"], [167, 168, "MetricName", "F1"], [191, 192, "MethodName", "LSTM"], [238, 239, "MethodName", "LSTM"], [240, 241, "MethodName", "CRF"], [253, 254, "MethodName", "CRF"], [257, 258, "MethodName", "LSTM"], [277, 279, "MethodName", "logistic regression"], [297, 298, "MethodName", "LSTM"], [330, 331, "MethodName", "LSTM"], [354, 355, "MethodName", "LSTM"], [356, 357, "MethodName", "CRF"], [365, 367, "HyperparameterName", "batch size"], [386, 387, "MethodName", "LSTM"], [402, 403, "MethodName", "CRF"], [416, 417, "MethodName", "LSTM"], [418, 419, "MethodName", "CRF"], [437, 438, "MethodName", "LSTM"], [439, 440, "MethodName", "CRF"], [455, 456, "MethodName", "LSTM"], [457, 458, "MethodName", "CRF"], [464, 465, "MetricName", "accuracy"], [581, 582, "MetricName", "F1"], [593, 594, "MethodName", "Dropout"]]}
{"text": "In Table 4 we show that adding document - level context improves every model on CoNLL - 2003 . Incorporating document - level context further improves our greedy ID - CNN model , attaining 90.65 average F1 . We believe this model sees greater improvement with the addition of document - level context than the Bi - LSTM - CRF due to the ID - CNN learning a feature function better suited for representing broad context , in contrast with the Bi - LSTM which , though better than a simple RNN at encoding long memories of sequences , may reach its limit when provided with sequences more than 1 , 000 tokens long such as entire documents . We also note that our combination of training objective ( Eqn . 11 ) and tied parameters ( Eqn . 5 compares models trained to incorporate entire document context using the document baselines described in Section 6.2 . In Table 6 we show that , in addition to being more accurate , our ID - CNN model is also much faster than the Bi - LSTM - CRF when incorporating context from entire documents , decoding at almost 8 times the speed . On these long sequences , it also tags at more than 4.5 times the speed of the greedy Bi - LSTM , demonstrative of the benefit of our ID - CNNs context - aggregating computation that does not depend on the length of the sequence .", "entities": [[35, 37, "MetricName", "average F1"], [57, 58, "MethodName", "LSTM"], [59, 60, "MethodName", "CRF"], [83, 84, "MethodName", "LSTM"], [184, 185, "MethodName", "LSTM"], [186, 187, "MethodName", "CRF"], [222, 223, "MethodName", "LSTM"]]}
{"text": "We observe similar patterns on OntoNotes as we do on CoNLL . icalized greedy model of Ratinov and Roth ( 2009 ) , and our ID - CNN out - performs the Bi - LSTM as well as the more complex model of Durrett and Klein ( 2014 ) which leverages the parallel coreference annotation available in the OntoNotes corpus to predict named entities jointly with entity linking and co - reference . Our greedy model is out - performed by the Bi - LSTM - CRF reported in Chiu and Nichols ( 2016 ) as well as our own re - implementation , which appears to be the new state - of - the - art on this dataset . The gap between our greedy model and those using Viterbi decoding is wider than on CoNLL . We believe this is due to the more diverse set of entities in OntoNotes , which also tend to be much longer - the average length of a multi - token named entity segment in CoNLL is about one token shorter than in OntoNotes . These long entities benefit more from explicit structured constraints enforced in Viterbi decoding . Still , our ID - CNN outperforms all other greedy methods , achieving our goal of learning a better token encoder for structured prediction . Incorporating greater context significantly boosts the score of our greedy model on OntoNotes , whereas the Bi - LSTM - CRF performs more poorly . In Table 7 , we also list the F1 of our ID - CNN model and the Bi - LSTM - CRF model trained on entire document context . For the first time , we see the score decrease when more context is added to the Bi - LSTM - CRF model , though the ID - CNN , whose sentence model a lower score than that of the Bi - LSTM - CRF , sees an increase . We believe the decrease in the Bi - LSTM - CRF model occurs because of the nature of the OntoNotes dataset compared to CoNLL - 2003 : CoNLL - 2003 contains a particularly high proportion of ambiguous entities , 7 perhaps leading to more benefit from document context that helps with disambiguation . In this scenario , adding the wider context may just add noise to the high - scoring Bi - LSTM - CRF model , whereas the less accurate dilated model can still benefit from the refined predictions of the iterated dilated convolutions .", "entities": [[5, 6, "DatasetName", "OntoNotes"], [34, 35, "MethodName", "LSTM"], [58, 59, "DatasetName", "OntoNotes"], [66, 68, "TaskName", "entity linking"], [84, 85, "MethodName", "LSTM"], [86, 87, "MethodName", "CRF"], [151, 152, "DatasetName", "OntoNotes"], [181, 182, "DatasetName", "OntoNotes"], [219, 221, "TaskName", "structured prediction"], [234, 235, "DatasetName", "OntoNotes"], [240, 241, "MethodName", "LSTM"], [242, 243, "MethodName", "CRF"], [255, 256, "MetricName", "F1"], [266, 267, "MethodName", "LSTM"], [268, 269, "MethodName", "CRF"], [295, 296, "MethodName", "LSTM"], [297, 298, "MethodName", "CRF"], [318, 319, "MethodName", "LSTM"], [320, 321, "MethodName", "CRF"], [334, 335, "MethodName", "LSTM"], [336, 337, "MethodName", "CRF"], [345, 346, "DatasetName", "OntoNotes"], [398, 399, "MethodName", "LSTM"], [400, 401, "MethodName", "CRF"]]}
{"text": "Named entity recognition ( NER ) is one of the foundations of many downstream tasks such as relation extraction , event detection , and knowledge graph construction . NER models require vast amounts of labeled data to learn and identify patterns that humans can not continuously . It is really about getting accurate data to train the models . When end - to - end neural models achieve excellent performance on NER in various domains ( Lample et al , 2016 ; Liu et al , 2018 ; Luan et al , 2018 ; Zeng et al , , 2021 , building useful and challenging NER benchmarks , such as CoNLL03 , WNUT16 , and SCIERC , contributes significantly to the research community . Data annotation plays a crucial role in building benchmarks and ensuring NLP models are trained with the correct information to learn from ( Luan et al , 2018 ; . Producing the necessary annotation from any asset at scale is a challenge , mainly because of the complexity involved with annotation . Getting the most accurate labels demands time and expertise . Label mistakes can hardly be avoided , especially when the labeling process splits the data into multiple sets for distributed annotation . The mistakes cause label inconsistency between subsets of annotated data ( e.g. , training set and test set or multiple training subsets ) . For example , in the CoNLL03 dataset ( Sang and De Meulder , 2003 ) , a standard NER benchmark that has been cited over 2 , 300 times , label mistakes were found in 5.38 % of the test set ( Wang et al , 2019 ) . Note that the stateof - the - art results on CoNLL03 have achieved an F1 score of \u223c .93 . So even if the label mistakes make up a tiny part , they can not be negligible when researchers are trying to improve the results further . In the work of Wang et al , five annotators were recruited to correct the label mistakes . Compared to the original test set results , the corrected test set results are more accurate and stable . However , two critical issues were not resolved in this process : i ) How to identify label inconsistency between the subsets of annotated data ? ii ) How to validate that the label consistency was recovered by the correction ? Another example is SCIERC ( Luan et al , 2018 ) ( cited \u223c50 times ) which is a multi - task ( including NER ) benchmark in AI domain . It has 1 , 861 sentences for training , 455 for dev , and 551 for test . When we looked at the false predictions given by SCIIE which was a multi - task model released along with the SCIERC dataset , we found that as many as 147 ( 26.7 % of the test set ) sentences were not properly annotated . ( We also recruited five annotators and counted a mistake when all the annotators report it . ) Three examples are given in Table 1 : two of them have wrong entity types ; the third has a wrong span boundary . As shown in the experiments section , after the correction , the NER performance becomes more accurate and stable . Table 1 : Three examples to compare original and corrected annotation in the test set of the SCIERC dataset . If the annotation on the test set consistently followed the \" codebook \" that was used to annotate training data , the entities in the first two examples would be labelled as \" Task \" ( not \" Method \" ) for sure .", "entities": [[0, 3, "TaskName", "Named entity recognition"], [4, 5, "TaskName", "NER"], [17, 19, "TaskName", "relation extraction"], [20, 22, "TaskName", "event detection"], [25, 27, "TaskName", "graph construction"], [28, 29, "TaskName", "NER"], [71, 72, "TaskName", "NER"], [105, 106, "TaskName", "NER"], [110, 111, "DatasetName", "CoNLL03"], [115, 116, "DatasetName", "SCIERC"], [237, 238, "DatasetName", "CoNLL03"], [250, 251, "TaskName", "NER"], [291, 292, "DatasetName", "CoNLL03"], [295, 297, "MetricName", "F1 score"], [409, 410, "DatasetName", "SCIERC"], [430, 431, "TaskName", "NER"], [476, 477, "DatasetName", "SCIERC"], [554, 555, "TaskName", "NER"], [579, 580, "DatasetName", "SCIERC"]]}
{"text": "We design a neural ranking model to score all the candidates that underwent splitting and deletion , V = { v 1 , v 2 , . . . , v n } , then feed the top - ranked one to the lexical paraphrasing model for the final output . We train the model on a standard text simplification corpus consisting of pairs of complex sentence x and manually simplified reference y. Scoring Function . To assess the \" goodness \" of each candidate v i during training , we define the gold scoring function g * as a length - penalized BERTscore : g * ( v i , y ) = e \u2212\u03bb | | \u03c6v i \u2212\u03c6y | | \u00d7 BERT Score ( v i , y ) ( 1 ) BERTScore ( Zhang et al , 2020b ) is a text similarity metric that uses BERT ( Devlin et al , 2019 ) embeddings to find soft matches between word pieces ( Wu et al , 2016 ) instead of exact string matching . We introduce a length penalty to favor the candidates that are of similar length to the human reference y and penalize those that deviate from the target compression ratio \u03c6 y . \u03bb defines the extent of penalization and is set to 1 in our experiments . \u03c6 v i represents the compression ratios of v i compared to the input x. In principle , other similarity metrics can also be used for scoring . Pairwise Ranking Model . We train the ranking model in a pairwise setup since BERTScore is sensitive to the relative rather than absolute similarity , when comparing multiple candidates with the same reference . We transform the gold ranking of V ( | V | = n ) into n 2 pairwise comparisons for every candidate pair , and learn to minimize the pairwise ranking violations using hinge loss : L M R = 1 m m k=1 1 n 2 k n k i=1 n k j=1 , i = j max ( 0 , 1 \u2212 l k ij d k ij ) d k ij = g ( v k i ) \u2212 g ( v k j ) l k ij = sign g * ( v k i , y k ) \u2212 g * ( v k j , y k ) ( 2 ) where g ( . ) is a feedforward neural network , m is the number of training complex - simple sentence pairs , k is the index of training examples , and n k represents the number of generated candidates ( 2.1 ) . On average , n k is about 14.5 for a sentence of 30 words , and can be larger for longer sentences . We consider 10 randomly sampled candidates for each complex sentence during training . Features . For the feedforward network g ( . ) , we use the following features : number of words in v i and x , compression ratio of v i with respect to x , Jaccard similarity between v i and x , the rules applied on x to obtain v i , and the number of rule applications . We vectorize all the real - valued features using Gaussian binning ( Maddela and Xu , 2018 ) , which has shown to help neural models trained on numerical features ( Liu et al , 2016 ; Sil et al , 2017 ; . We concatenate these vectors before feeding them to the ranking model . We score each candidate v i separately and rank them in the decreasing order of g ( v i ) . We provide implementation details in Appendix A.", "entities": [[58, 60, "TaskName", "text simplification"], [124, 125, "MethodName", "BERT"], [125, 126, "MetricName", "Score"], [145, 147, "TaskName", "text similarity"], [150, 151, "MethodName", "BERT"], [322, 323, "MetricName", "loss"], [348, 349, "DatasetName", "0"], [489, 491, "MethodName", "feedforward network"]]}
{"text": "We then paraphrase the top - ranked candidatev V to generate the final simplification output\u0177 . Our paraphrase generation model can explicitly control the extent of lexical paraphrasing by specifying the percentage of words to be copied from the input sentence as a soft constraint . We also introduce a data augmentation method to encourage our model to generate more diverse outputs . Base Model . Our base generation model is a Transformer encoder - decoder initialized by the BERT checkpoint ( ? ) , which achieved the best reported performance on text simplification in the recent work . We enhance this model with an attention - based copy mechanism to encourage lexical paraphrasing , while remaining faithful to the input . Copy Control . Given the input candidatev = ( v 1 , v 2 , . . . , v l ) of l words and the percentage of copying cp ( 0 , 1 ] , our goal is to paraphrase the rest of ( 1 \u2212 cp ) \u00d7 l words inv to a simpler version . To achieve this , we convert cp into a vector of the same dimension as BERT embeddings using Gaussian binning ( Maddela and Xu , 2018 ) and add it to the beginning of the input sequencev . The Transformer encoder then produces a sequence of context - aware hidden states H = ( h 1 , h 2 . . . h l ) , where h i corresponds to the hidden state ofv i . Each h i is fed into the copy network which predicts the probability p i that wordv i should be copied to output . We create a new hidden stateh i by adding h i to a vector u scaled according to p i . In other words , the scaled version of u informs the decoder whether the word should be copied . A single vector u is used across all sentences and hidden states , and is randomly initialized then updated during training . More formally , the encoding process can be described as follows : ( h 1 , h 2 , . . . , h l ) = encoder ( [ cp ; v 1 , v 2 , . . . , v l ] ) h i = h i + p i u , H = ( h 1 , h 2 , . . . , h l ) ( 3 ) The Transformer decoder generates the output sequence fromH. Our copy mechanism is incorporated into the encoder rather than copying the input words during the decoding steps ( Gu et al , 2016 ; See et al , 2017 ) . Unless otherwise specified , we use the average copy ratio of the training dataset , 0.7 , for our experiments . Multi - task Training . We train the paraphrasing model and the copy network in a multi - task learning setup , where predicting whether a word should be copied serves as an auxiliary task . The gold labels for this task are obtained by checking if each word in the input sentence also appears in the human reference . When a word occurs multiple times in the input , we rely on the monolingual word alignment results from JacanaAlign ( Yao et al , 2013 ) to determine which occurrence is the one that gets copied . We train the Transformer model and the copy network jointly by minimizing the cross - entropy loss for both decoder generation and binary word classification . We provide implementation and training details in Appendix A. Data Augmentation . The sentence pairs in the training corpus often exhibit a variable mix of splitting and deletion operations along with paraphras - ing ( see Figure 1 for an example ) , which makes it difficult for the encoder - decoder models to learn paraphrases . Utilizing DisSim , we create additional training data that focuses on lexical paraphrasing For each sentence pair x , y , we first generate a set of candidates V = { v 1 , v 2 , . . . , v n } by applying DisSim to x , as described in 2.1 . Then , we select a a subset of V , called V = { v 1 , v 2 , . . . , v n } ( V V ) that are fairly close to the reference y , but have only undergone splitting and deletion . We score each candidate v i using the length - penalized BERTScore g * ( v i , y ) in Eq . ( 1 ) , and discard those with scores lower than 0.5 . While calculating g * , we set \u03c6 y and \u03bb to 1 and 2 respectively to favor candidates of similar length to the reference y. We also discard the candidates that have different number of split sentences with respect to the reference . Finally , we train our model on the filtered candidate - reference sentence pairs v 1 , y , v 2 , y , . . . , v n , y , which focus on lexical paraphrasing , in addition to x , y .", "entities": [[17, 19, "TaskName", "paraphrase generation"], [50, 52, "TaskName", "data augmentation"], [72, 73, "MethodName", "Transformer"], [79, 80, "MethodName", "BERT"], [92, 94, "TaskName", "text simplification"], [154, 155, "DatasetName", "0"], [196, 197, "MethodName", "BERT"], [220, 221, "MethodName", "Transformer"], [420, 422, "MethodName", "Transformer decoder"], [496, 500, "TaskName", "multi - task learning"], [555, 557, "TaskName", "word alignment"], [581, 582, "MethodName", "Transformer"], [594, 595, "MetricName", "loss"], [613, 615, "TaskName", "Data Augmentation"]]}
{"text": "We train and evaluate our models on Newsela ( Xu et al , 2015 ) 3 and Wikipedia copora ( Zhu et al , 2010 ; Woodsend and Lapata , 2011 ; Coster and Kauchak , 2011 Table 2 : Automatic evaluation results on NEWSELA - AUTO test set . We report SARI , the main automatic metric for simplification , and its three edit scores namely precision for delete ( del ) and F1 scores for add and keep operations . We also report FKGL ( FK ) , average sentence length ( SLen ) , output length ( OLen ) , compression ratio ( CR ) , self - BLEU ( s - BL ) , percentage of sentence splits ( % split ) , average percentage of new words added to the output ( % new ) , and percentage of sentences identical to the input ( % eq ) . Bold typeface denotes the best performances ( i.e. , closest to the reference ) . articles with each article rewritten by professional editors for students in different grades . We used the complex - simple sentence pairs automatically aligned by , called the NEWSELA - AUTO dataset . To capture sentence splitting , we joined the adjacent sentences in the simple article that are aligned to the same sentence in the complex article . Following \u0160tajner et al ( 2015 ) , we removed the sentence pairs with high ( > 0.9 ) and low ( < 0.1 ) BLEU ( Papineni et al , 2002 ) scores , which mostly correspond to the near identical and semantically divergent sentence pairs respectively . The final dataset consists of 259 , 778 train , 32 , 689 validation and 33 , 391 test complex - simple sentence pairs , where \u223c30 % of pairs involve sentence splitting . Besides Newsela , we also provide the details of experiments on Wikipedia corpus in Appendix F , which show similar trends . To demonstrate that our model can be controlled to generate diverse simplifications , we evaluate under the following settings : ( i ) Standard evaluation on the NEWSELA - AUTO test set similar to the methodology in the recent literature Dong et al , 2019 ; Zhang and Lapata , 2017 ) , and ( ii ) Evaluation on different subsets of the NEWSELA - AUTO test set that concentrate on a specific operation . We selected 9 , 356 sentence pairs with sentence splits for split - focused evaluation . Similarly , we chose 9 , 511 sentence pairs with compression ratio < 0.7 and without sentences splits to evaluate delete - focused simplification . We created a new dataset , called NEWSELA - TURK , to evaluate lexical paraphrasing . 4 Similar to the WIKIPEDIA - TURK benchmark corpus ( Xu et al , 2016 ) , NEWSELA - TURK consists of human - written references focused on lexical para - phrasing . We first selected sentence pairs from the NEWSELA - AUTO test set of roughly similar length ( compression ratio between 0.8 and 1.2 ) and no sentence splits because they more likely involve paraphrasing . Then , we asked Amazon Mechanical Turk workers to simplify the complex sentence without any loss in meaning . 5 To ensure the quality of simplifications , we manually selected the workers using the qualification test proposed in Alva - Manchego et al ( 2020 ) , during which the workers were asked to simplify three sentences . We selected top 35 % of the 300 workers that participated in the test . We periodically checked the submissions and removed the bad workers . In the end , we collected 500 sentences with 4 references for each sentence .", "entities": [[7, 8, "DatasetName", "Newsela"], [44, 45, "DatasetName", "NEWSELA"], [74, 75, "MetricName", "F1"], [111, 112, "MetricName", "BLEU"], [197, 198, "DatasetName", "NEWSELA"], [253, 254, "MetricName", "BLEU"], [312, 313, "DatasetName", "Newsela"], [360, 361, "DatasetName", "NEWSELA"], [396, 397, "DatasetName", "NEWSELA"], [456, 457, "DatasetName", "NEWSELA"], [482, 483, "DatasetName", "NEWSELA"], [505, 506, "DatasetName", "NEWSELA"], [548, 549, "MetricName", "loss"]]}
{"text": "Metrics . We report SARI ( Xu et al , 2016 ) , which averages the F1 / precision of n - grams ( n { 1 , 2 , 3 , 4 } ) inserted , deleted and kept when compared to human references . More specifically , it computes the F1 score for the n - grams that are added ( add ) , 8 which is an important indicator if a model is good at paraphrasing . The model 's deletion capability is measured by the F1 score for n - grams that are kept ( keep ) and precision for those deleted ( del ) . 9 To evaluate a model 's para - 8 We slightly improved the SARI implementation by Xu et al ( 2016 ) to exclude the spurious ngrams while calculating the F1 score for add . For example , if the input contains the phrase \" is very beautiful \" , the phrase \" is beautiful \" is treated as a new phrase in the original implementation even though it is caused by the delete operation . 9 SARI score of a reference with itself may not always be 100 as it considers 0 divided by 0 as 0 , instead of 1 , when calculating n - gram precision and recall . This avoids the inflation of del scores when the input is same as the output . phrasing capability and diversity , we calculate the BLEU score with respect to the input ( s - BL ) , the percentage of new words ( % new ) added , and the percentage of system outputs identical to the input ( % eq ) . Low s - BL , % eq , or high % new indicate that the system is less conservative . We also report Flesch - Kincaid ( FK ) grade level readability ( Kincaid and Chissom , 1975 ) , average sentence length ( SLen ) , the percentage of splits ( % split ) , compression ratio ( CR ) , and average output length ( OLen ) . We do not report BLEU because it often does not correlate with simplicity ( Sulem et al , 2018a , b ; Xu et al , 2016 ) . Table 6 : Human evaluation of 100 random simplifications from the NEWSELA - AUTO test set and the split - focused subset of the same test set . Has Split and Correct Split denote the percentage of the output sentences that have undergone splitting and the percentage of coherent splits respectively . * denotes that our model is significantly better than the corresponding baseline ( according to a t - test with p < 0.05 ) .", "entities": [[16, 17, "MetricName", "F1"], [52, 54, "MetricName", "F1 score"], [89, 91, "MetricName", "F1 score"], [140, 142, "MetricName", "F1 score"], [202, 203, "DatasetName", "0"], [205, 206, "DatasetName", "0"], [207, 208, "DatasetName", "0"], [246, 248, "MetricName", "BLEU score"], [359, 360, "MetricName", "BLEU"], [395, 396, "DatasetName", "NEWSELA"]]}
{"text": "deletion as they show high self - BLEU ( > 66.5 ) and FK ( > 8.8 ) scores despite having compression ratios similar to other systems . Transformer model alone is rather conservative and copies 10.2 % of the sentences directly to the output . Although Hybrid - NG makes more changes than any other baselines , its SARI and add scores are 3.7 and 1.7 points lower than our model indicating that it generates more errors . Our model achieves the lowest self - BLEU ( 48.7 ) , FK ( 7.9 ) , and percentage of sentences identical to the input ( 0.4 ) , and the highest add ( 3.3 ) score and percentage of new words ( 16.2 % ) . In other words , our system is the least conservative , generates more good paraphrases , and mimics the human references better . We provide examples of system outputs in Table 9 and Appendix C. Tables 3 , 4 , and 5 show the results on NEWSELA - TURK , split - focused , and delete - focused subsets of NEWSELA - AUTO test set respectively . For these experiments , we configure our model to focus on specific operations ( details in 2.4 ) . Our model again outperforms the existing systems according to SARI , add score , and percentage of new words , which means that our model is performing more meaningful paraphrasing . We show that we can control the extent of paraphrasing by varying the copy ratio ( cp ) . Our model splits 93.5 % of the sentences , which is substantially better than the other models .", "entities": [[7, 8, "MetricName", "BLEU"], [28, 29, "MethodName", "Transformer"], [86, 87, "MetricName", "BLEU"], [172, 173, "DatasetName", "NEWSELA"], [186, 187, "DatasetName", "NEWSELA"]]}
{"text": "China 's air pollution is very unhealthy . Hybrid - NG experts say the government 's air pollution exacts a toll on human health . LSTM experts say china 's air pollution exacts a tremendous toll on human health . Transformer bert experts say china 's pollution has a tremendous effect on human health . EditNTS experts say china 's air pollution can cause human health . Our Model ( cp = 0.6 ) experts say china 's air pollution is a big problem for human health . Our Model ( cp = 0.7 ) experts say china 's air pollution can cause a lot of damage on human health . Our Model ( cp = 0.8 ) experts say china 's air pollution is a huge toll on human health . Table 9 : Examples of system outputs . Red marks the errors ; blue marks good paraphrases . cp is a soft constraint that denotes the percentage of words that can be copied from the input . Kriz et al , 2019 ; Dong et al , 2019 ; at the cost of controllability and performance as shown in this paper . Controllable text simplification has been attempted before , but only with limited capability . Scarton and Specia ( 2018 ) and added additional tokens to the input representing grade level , length , lexical , and structural complexity constraints . Nishihara et al ( 2019 ) proposed a loss which controls word complexity , while Mallinson and Lapata ( 2019 ) concatenated constraints to each word embedding . Kumar et al ( 2020 ) proposed a linguistic scoring function to control the edits to the input . Another long body of research focuses on a single simplification operation and can be broadly divided into three categories : ( 1 ) Lexical Simplification ( Specia et al , 2012 ; Horn et al , 2014 ; Glava\u0161 and \u0160tajner , 2015 ; Paetzold andSpecia , 2017 , 2015 ; Maddela and Xu , 2018 ; Qiang et al , 2020 ) , where complex words are substituted with simpler words . ( 2 ) Syntactic Simplification ( Siddharthan , 2006 ; Aharoni and Goldberg , 2018 ; Botha et al , 2018 ; Niklaus et al , 2019 ) , which deals exclusively with sentence splitting , and ( 3 ) Sentence Compression ( Filippova et al , 2015 ; Rush et al , 2015 ; Nallapati et al , 2016 ; See et al , 2017 ; Baziotis et al , 2019 ) , where the goal is to shorten the input sentence by removing its irrelevant content .", "entities": [[25, 26, "MethodName", "LSTM"], [40, 41, "MethodName", "Transformer"], [195, 197, "TaskName", "text simplification"], [242, 243, "MetricName", "loss"], [262, 263, "DatasetName", "Kumar"], [304, 306, "TaskName", "Lexical Simplification"], [394, 396, "DatasetName", "Sentence Compression"]]}
{"text": "We implemented two separate Transformer models for neural deletion and split component ( 2.1 ) and paraphrase generation ( 2.3 ) using the Fairseq 12 toolkit . Both the encoder and decoder follow BERT base 13 architecture , while the encoder is also initialized with BERT base checkpoint . For neural deletion and split component , we used a beam search of width 10 to generate candidates . The copy attention mechanism is a feedforward network containing 3 hidden layers , 1000 nodes in each layer with tanh activation , and a single linear output node with sigmoid activation . We used Adam optimizer ( Kingma and Ba , 2014 ) with a learning rate of 0.0001 , linear learning rate warmup of 40k steps , and 100k training steps . We used a batch size of 64 . We used BERT WordPiece tokenizer . During inference , we constrained the beam - search to not repeat trigrams and emitted sentences that avoided aggressive deletion ( compression ratio [ 0.9 , 1.2 ] . We chose the best checkpoint based on the SARI score ( Xu et al , 2016 ) on the dev set . We saved a checkpoint after every epoch . We did not perform any hyperparameter search and directly used the hyperparameters of the BERT - initialized Transformer described in ? . The model takes 10 hours to train on 1 NVIDIA GeForce GPU . Our pairwise ranking model , implemented using the PyTorch framework , consists of 3 hidden layers , 100 nodes in each layer , tanh activation , and a single linear output node . We used Adam optimizer with a learning rate of 0.01 and 10 epochs . We applied a dropout of 0.2 . For Gaussian binning , we vectorized the numerical features into 10 dimensional vectors . The model takes half hour to train on 1 NVIDIA GeForce GPU . We do not perform any extensive hyperparameter tuning . We just examined few values for learning rate ( 0.001 , 0.01 and 0.1 ) and chose the best based on the SARI score on the dev set . We used the original code for DisSim . 14", "entities": [[4, 5, "MethodName", "Transformer"], [16, 18, "TaskName", "paraphrase generation"], [33, 34, "MethodName", "BERT"], [45, 46, "MethodName", "BERT"], [74, 76, "MethodName", "feedforward network"], [87, 89, "MethodName", "tanh activation"], [97, 99, "MethodName", "sigmoid activation"], [102, 103, "MethodName", "Adam"], [103, 104, "HyperparameterName", "optimizer"], [113, 115, "HyperparameterName", "learning rate"], [119, 121, "HyperparameterName", "learning rate"], [134, 136, "HyperparameterName", "batch size"], [141, 142, "MethodName", "BERT"], [142, 143, "MethodName", "WordPiece"], [218, 219, "MethodName", "BERT"], [221, 222, "MethodName", "Transformer"], [262, 264, "MethodName", "tanh activation"], [274, 275, "MethodName", "Adam"], [275, 276, "HyperparameterName", "optimizer"], [278, 280, "HyperparameterName", "learning rate"], [335, 337, "HyperparameterName", "learning rate"]]}
{"text": "the room echoed with the sounds of song , the beat of drums , the voices of young men who are hungry and legs . Our Model ( cp = 0.6 ) the sound of the room was full of sounds of young men and the voices of cellos . Our Model ( cp = 0.7 ) the sound of the room sounded like a lot of music , and the voices of young men . Our Model ( cp = 0.8 ) the sound of the room sounded like a song , the beat of drums , and the voices of young men . Table 11 : Automatic evaluation results on a subset of Newsela test set that focuses on paraphrasing ( 8371 complexsimple sentence with compression ratio > 0.9 and no splits ) . We control the extent of paraphrasing of our models by specifying the percentage of words to be copied ( cp ) from the input as a soft constraint . 36.1 2.5 67.4 38.5 11.7 20.9 22.4 1.02 6.4 63.5 13.5 0.0 Our Model 35.9 4.7 63.6 39.6 9.2 14.7 19.8 0.9 33.7 63.2 12.9 9.2 Our Model ( no split ; cp = 0.6 ) 36.5 4.9 63.2 41.4 10.8 18.6 19.9 0.89 6.7 61.9 12.4 3.9 Our Model ( no split ; cp = 0.7 ) 37 . 5 4.3 68.8 39.4 11.2 19.1 20.9 0.94 8.9 72.6 8.6 12.3 Our Model ( no split ; cp = 0.8 ) 37 . 0 3.8 72.0 35.3 11.7 19.8 21.7 0.97 8.4 80.4 6.6 24.5 Table 12 : Automatic evaluation results on TURK dataset ( Xu et al , 2015 ) that focuses on lexical paraphrasing . ( Alva - Manchego et al , 2020 ) dataset that contains all the three simplification operations . We use the complex - simple sentence pairs from WIKI - AUTO , which contains 138 , 095 article pairs and 604k non - identical aligned and partially - aligned sentence pairs . To capture sentence splitting , we join the sentences in the simple article mapped to the same sentence in the complex article . Similar to Newsela , we remove the sentence pairs with high ( > 0.9 ) and low ( < 0.1 ) BLEU ( Papineni et al , 2002 ) scores . For validation and testing purposes , we use the following two corpora : ( i ) TURK corpus ( Xu et al , 2015 ) for lexical paraphrasing and ( ii ) ASSET corpus ( Alva - Manchego et al , 2020 ) for multiple rewrite operations . While the former corpus has 8 humanwritten references for 2000 validation and 359 test sentences , the latter corpus provides 10 references for the same sentences . We remove the validation and test sentences from the training corpus . Tables 12 and 13 show the results on TURK and ASSET respectively .", "entities": [[115, 116, "DatasetName", "Newsela"], [249, 250, "DatasetName", "0"], [359, 360, "DatasetName", "Newsela"], [378, 379, "MetricName", "BLEU"], [420, 422, "DatasetName", "ASSET corpus"], [485, 486, "DatasetName", "ASSET"]]}
{"text": "We use various neural network learned embeddings as similarity feature in our system . The system uses Siamese network to learn these similarity measures . Siamese nets were first introduced in the early 1990s by ( Bromley et al , 1993 ) to solve signature verification as an image matching problem . A siamese neural network consists of twin networks which accept distinct inputs but are joined by an energy function at the top . This function computes some metric between the highest level feature representation on each side . The weights between both the networks are shared generally , so that they project the similar texts not far in the embedding dimension . We use contrastive loss described in ( Chopra et al , 2005 ) as the loss function to the Siamese network . Glove pretrained vectors ( 300 dimension ) are fed as input to the neural network . The final neural embeddings are generated by various architectures . Figure 1 shows a siamese network , where X 1 represents the original question text and X 2 represents the candidate question text . G W represents a complex nonlinear function which is represented by neural network having weights W . The euclidean distance of the vectors is used to compute the contrastive loss . The goal is to minimize the distance in the embedding space of the similar question text and maximize for non similar pairs . The contrastive loss can be given by following equation : L = Y | | G W ( X 1 ) , G W ( X 2 ) | | 2 + ( 1 \u2212 Y ) max ( 0 , m \u2212 | | G W ( X 1 ) , G W ( X 2 ) | | 2 ) where Y is annotated tag , 1 if X 1 and X 2 are similar , 0 otherwise . m is margin parameter for hinge loss , which is kept 1 for all our networks . We use following networks to generate text embedding : Long Short Term Memory LSTM ( Hochreiter and Schmidhuber , 1997 ) are popular variant of the the recurrent neural network architecture that captures the long term dependency in text and deals with vanishing gradient problem . Recently LSTMs have been very successful in various NLP tasks .", "entities": [[17, 19, "MethodName", "Siamese network"], [48, 50, "TaskName", "image matching"], [111, 113, "HyperparameterName", "embedding dimension"], [117, 118, "MetricName", "loss"], [129, 130, "MetricName", "loss"], [133, 135, "MethodName", "Siamese network"], [166, 168, "MethodName", "siamese network"], [215, 216, "MetricName", "loss"], [242, 243, "MetricName", "loss"], [279, 280, "DatasetName", "0"], [318, 319, "DatasetName", "0"], [327, 328, "MetricName", "loss"], [351, 352, "MethodName", "LSTM"]]}
{"text": "Figure 2 shows a bidirectional recurrent neural network architecture . Bi - directional RNN processes the text in both directions with separate hidden units and these hidden representations are concatenated together to create final hidden embedding . For bi - directional LSTM , the hidden unit is a LSTM cell combining of various gates . We use a bidirectional LSTM to generate a 256 dimensional vector for pair of text and train the model by back propagation using contrastive loss . Gated Recurrent Unit Gated recurrent unit ( GRU ) ( Chung et al , 2014 ) is another variant of RNN which were introduced recently as compared to LSTM . They also have seen similar success as LSTM in various NLP tasks . We use Bi - GRU as another network to generate the neural embeddings trained by siamese network similar to Bi - LSTM . The final hidden embedding size is 256 dimension for our Bi - GRU network also . Convolution Net We also use convolution networks as another neural network architecture to generate embeddings inside the siamese network . We use 1D - convolution with 128 kernels , stride of 5 followed by 1D - max pool with pool - size of 5 and finally a dense layer to create a 128 dimension vector . Implementation Details We use Keras 1 library with Theano ( Theano Development Team , 2016 ) backend to train above 3 models . The batch size is set to 64 and dropout rate is 0.25 . We run 25 epochs for each of these 3 networks training . It takes couple of hours to train on CPU . Instead of using the entire vectors into our final classifier , we compute cosine similarity of learned vectors of both the question text ( for each of the 3 networks ) and use that as a feature in our system .", "entities": [[41, 42, "MethodName", "LSTM"], [48, 49, "MethodName", "LSTM"], [58, 60, "MethodName", "bidirectional LSTM"], [79, 80, "MetricName", "loss"], [81, 84, "MethodName", "Gated Recurrent Unit"], [84, 87, "MethodName", "Gated recurrent unit"], [88, 89, "MethodName", "GRU"], [109, 110, "MethodName", "LSTM"], [118, 119, "MethodName", "LSTM"], [128, 129, "MethodName", "GRU"], [139, 141, "MethodName", "siamese network"], [145, 146, "MethodName", "LSTM"], [159, 160, "MethodName", "GRU"], [163, 164, "MethodName", "Convolution"], [168, 169, "MethodName", "convolution"], [180, 182, "MethodName", "siamese network"], [187, 188, "MethodName", "convolution"], [243, 245, "HyperparameterName", "batch size"]]}
{"text": "Apart from the neural network learned semantic features , we also employ semantic similarity between question text generated by semantic net . We use the sentence similarity described in ( Li et al , 2006 ) using WordNet as semantic net . The paper describes various heuristics used to generate this sentence similarity . First , word pair similarity is generated as a function of the shortest path between the words and height of their lowest common subsumer ( LCS ) . This combines the word 1 https://github.com/fchollet/keras similarity with their specificity ( abstract vs specific concept ) . Then the sentence similarity is obtained as a linear combination of semantic similarity and the word order similarity . To generate semantic similarity , cosine between semantic vectors is obtained . The semantic vectors are generated by creating sentence vector of word presence and their similarity . Word order similarity is computed in the similar way as semantic similarity but the position of word in the sentence is used to generate the word order vector . Finally a linear combination of these two similarity features is used as the similarity measure between question texts . We use the same hyper - parameters as original paper that give the best results i.e. \u03b1 = 0.2 , \u03b2 = 0.45 , \u03b7 = 0.4 , \u03c6 = 0.2 , \u03b4 = 0.85 . The feature encodes semantic similarity and gives boost to system , shown in the results table .", "entities": [[12, 14, "TaskName", "semantic similarity"], [110, 112, "TaskName", "semantic similarity"], [120, 122, "TaskName", "semantic similarity"], [156, 158, "TaskName", "semantic similarity"], [210, 211, "HyperparameterName", "\u03b1"], [214, 215, "HyperparameterName", "\u03b2"], [226, 227, "HyperparameterName", "\u03b4"], [233, 235, "TaskName", "semantic similarity"]]}
{"text": "There has been a lot of research in machine translation and summarization community to find metrics that correlate with human judgement on these tasks . We compute BLEU ( Papineni et al , 2002 ) metrics for 1 , 2 , 3 and 4 grams and compute a weighted addition ( weights = 0.1 , 0.1 , 0.3 , 0.5 ) . We also compute ROUGE - L ( Lin , 2004 ) , which is recall oriented similarity measure based on longest common subsequence ( LCS ) , as a feature in our system .", "entities": [[8, 10, "TaskName", "machine translation"], [11, 12, "TaskName", "summarization"], [27, 28, "MetricName", "BLEU"], [65, 68, "MetricName", "ROUGE - L"]]}
{"text": "The results generated by the system on test data were submitted as an entry to SemEval - 2017 task 3 subtask B. Our primary entry achieved second place on the MAP which was official metric for ranking . Also it achieved highest MRR amongst all the primary submissions . Table 1 shows the dev and test set accuracy for our system with each feature applied incrementally . Our both contrastive submissions trained on SVM achieved better test accuracy than training on Logistic Regression . Thus the Ranking - SVM is able to generalize better . We also experimented with pointwise learning to rank method and got inferior results thus corroborating the fact that pairwise methods are helping our system in achieving better accuracy .", "entities": [[30, 31, "DatasetName", "MAP"], [42, 43, "MetricName", "MRR"], [57, 58, "MetricName", "accuracy"], [73, 74, "MethodName", "SVM"], [77, 78, "MetricName", "accuracy"], [81, 83, "MethodName", "Logistic Regression"], [88, 89, "MethodName", "SVM"], [122, 123, "MetricName", "accuracy"]]}
{"text": "Pre - trained feature extractors , such as BERT for natural language processing and VGG for computer vision , have become effective methods for improving deep learning models without requiring more labeled data . While effective , these feature extractors may be prohibitively large for some deployment scenarios . We explore weight pruning for BERT and ask : how does compression during pretraining affect transfer learning ? We find that pruning affects transfer learning in three broad regimes . Low levels of pruning ( 30 - 40 % ) do not affect pre - training loss or transfer to downstream tasks at all . Medium levels of pruning increase the pre - training loss and prevent useful pre - training information from being transferred to downstream tasks . High levels of pruning additionally prevent models from fitting downstream datasets , leading to further degradation . Finally , we observe that finetuning BERT on a specific task does not improve its prunability . We conclude that BERT can be pruned once during pre - training rather than separately for each task without affecting performance .", "entities": [[8, 9, "MethodName", "BERT"], [14, 15, "MethodName", "VGG"], [54, 55, "MethodName", "BERT"], [64, 66, "TaskName", "transfer learning"], [72, 74, "TaskName", "transfer learning"], [95, 96, "MetricName", "loss"], [113, 114, "MetricName", "loss"], [151, 152, "MethodName", "BERT"], [165, 166, "MethodName", "BERT"]]}
{"text": "Pre - trained feature extractors , such as BERT ( Devlin et al , 2018 ) for natural language processing and VGG ( Simonyan and Zisserman , 2014 ) for computer vision , have become effective methods for improving the performance of deep learning models . In the last year , models similar to BERT have become state - of - the - art in many NLP tasks , including natural language inference ( NLI ) , named entity recognition ( NER ) , sentiment analysis , etc . These models follow a pre - training paradigm : they are trained on a large amount of unlabeled text via a task that resembles language modeling ( Yang et al , 2019 ; Chan et al , 2019 ) and are then fine - tuned on a smaller amount of \" downstream \" data , which is labeled for a specific task . Pre - trained models usually achieve higher accuracy than any model trained on downstream data alone . The pre - training paradigm , while effective , still has some problems . While some claim that language model pre - training is a \" universal language learning task \" ( Radford et al , 2019 ) , there is no theoretical justification for this , only empirical evidence . Second , due to the size of the pre - training dataset , BERT models tend to be slow and require impractically large amounts of GPU memory . BERT - Large can only be used with access to a Google TPU , and BERT - Base requires some optimization tricks such as gradient checkpointing or gradient accumulation to be trained effectively on consumer hardware ( Sohoni et al , 2019 ) . Training BERT - Base from scratch costs \u223c$7k and emits \u223c1438 pounds of CO 2 ( Strubell et al , 2019 ) . Model compression ( Bucila et al , 2006 ) , which attempts to shrink a model without losing accuracy , is a viable approach to decreasing GPU usage . It might also be used to trade accuracy for memory in some low - resource cases , such as deploying to smartphones for real - time prediction . The main questions this paper attempts to answer are : Does compressing BERT impede it 's ability to transfer to new tasks ? And does fine - tuning make BERT more or less compressible ? To explore these questions , we compressed English BERT using magnitude weight pruning ( Han et al , 2015 ) and observed the results on transfer learning to the General Language Understanding Evaluation ( GLUE ) benchmark , a diverse set of natural language understanding tasks including sentiment analysis , NLI , and textual similarity evaluation . We chose magnitude weight pruning , which compresses models by removing weights close to 0 , because it is one of the most fine - grained and effective compression methods and because there are many interesting ways to view pruning , which we explore in the next section . Our findings are as follows : Low levels of pruning ( 30 - 40 % ) do not increase pre - training loss or affect transfer to downstream tasks at all . Medium levels of pruning increase the pre - training loss and prevent useful pre - training information from being transferred to downstream tasks . This information is not equally useful to each task ; tasks degrade linearly with pre - train loss , but at different rates . High levels of pruning , depending on the size of the downstream dataset , may additionally degrade performance by preventing models from fitting downstream datasets . Finally , we observe that fine - tuning BERT on a specific task does not improve its prunability or change the order of pruning by a meaningful amount . To our knowledge , prior work had not shown whether BERT could be compressed in a taskgeneric way , keeping the benefits of pre - training while avoiding costly experimentation associated with compressing and re - training BERT multiple times . Nor had it shown whether BERT could be over - pruned for a memory / accuracy trade - off for deployment to low - resource devices . In this work , we conclude that BERT can be pruned prior to distribution without affecting it 's universality , and that BERT may be over - pruned during pre - training for a reasonable accuracy trade - off for certain tasks . 2 Pruning : Compression , Regularization , Architecture Search Neural network pruning involves examining a trained network and removing parts deemed to be unnecessary by some heuristic saliency criterion . One might remove weights , neurons , layers , channels , attention heads , etc . depending on which heuristic is used . Below , we describe three different lenses through which we might interpret pruning . Compression Pruning a neural network decreases the number of parameters required to specify the model , which decreases the disk space required to store it . This allows large models to be deployed on edge computing devices like smartphones . Pruning can also increase inference speed if whole neurons or convolutional channels are pruned , which reduces GPU usage . 1 Regularization Pruning a neural network also regularizes it . We might consider pruning to be a form of permanent dropout ( Molchanov et al , 2017 ) or a heuristic - based L0 regularizer ( Louizos et al , 2018 ) . Through this lens , pruning decreases the complexity of the network and therefore narrows the range of possible functions it can express . 2 The main difference between L0 or L1 regularization and weight pruning is that the former induce sparsity via a penalty on the loss function , which is learned during gradient descent via stochastic relaxation . It 's not clear which approach is more principled or preferred . ( Gale et al , 2019 ) Sparse Architecture Search Finally , we can view neural network pruning as a type of sparse architecture search . Liu et al ( 2019b ) and Frankle and Carbin ( 2019 ) show that they can train carefully re - initialized pruned architectures to similar performance levels as dense networks . Under this lens , stochastic gradient descent ( SGD ) induces network sparsity , and pruning simply makes that sparsity explicit . These sparse architectures , along with the appropriate initializations , are sometimes referred to as \" lottery tickets . \" 3", "entities": [[8, 9, "MethodName", "BERT"], [21, 22, "MethodName", "VGG"], [54, 55, "MethodName", "BERT"], [70, 73, "TaskName", "natural language inference"], [77, 80, "TaskName", "named entity recognition"], [81, 82, "TaskName", "NER"], [84, 86, "TaskName", "sentiment analysis"], [159, 160, "MetricName", "accuracy"], [233, 234, "MethodName", "BERT"], [248, 249, "MethodName", "BERT"], [259, 260, "DatasetName", "Google"], [263, 264, "MethodName", "BERT"], [272, 274, "MethodName", "gradient checkpointing"], [293, 294, "MethodName", "BERT"], [315, 317, "TaskName", "Model compression"], [333, 334, "MetricName", "accuracy"], [351, 352, "MetricName", "accuracy"], [384, 385, "MethodName", "BERT"], [401, 402, "MethodName", "BERT"], [415, 416, "MethodName", "BERT"], [432, 434, "TaskName", "transfer learning"], [436, 437, "DatasetName", "General"], [441, 442, "DatasetName", "GLUE"], [449, 452, "TaskName", "natural language understanding"], [454, 456, "TaskName", "sentiment analysis"], [478, 479, "DatasetName", "0"], [535, 536, "MetricName", "loss"], [554, 555, "MetricName", "loss"], [586, 587, "MetricName", "loss"], [627, 628, "MethodName", "BERT"], [658, 659, "MethodName", "BERT"], [685, 686, "MethodName", "BERT"], [694, 695, "MethodName", "BERT"], [704, 705, "MetricName", "accuracy"], [723, 724, "MethodName", "BERT"], [738, 739, "MethodName", "BERT"], [751, 752, "MetricName", "accuracy"], [769, 771, "TaskName", "network pruning"], [833, 836, "HyperparameterName", "number of parameters"], [959, 961, "MethodName", "L1 regularization"], [975, 976, "MetricName", "loss"], [1016, 1018, "TaskName", "network pruning"], [1062, 1065, "MethodName", "stochastic gradient descent"], [1066, 1067, "MethodName", "SGD"]]}
{"text": "In this work , we focus on weight magnitude pruning because it is one of the most fine - grained and effective pruning methods . It also has a compelling saliency criterion ( Han et al , 2015 ) : if a weight is close to zero , then its input is effectively ignored , which means the weight can be pruned . Magnitude weight pruning itself is a simple procedure : 1 . Pick a target percentage of weights to be pruned , say 50 % . 2 . Calculate a threshold such that 50 % of weight magnitudes are under that threshold . 3 . Remove those weights . 4 . Continue training the network to recover any lost accuracy . 5 . Optionally , return to step 1 and increase the percentage of weights pruned . This procedure is conveniently implemented in a Tensorflow ( Abadi et al , 2016 ) package 4 , which we use ( Zhu and Gupta , 2017 ) . Calculating a threshold and pruning can be done for all network parameters holistically ( global pruning ) or for each weight matrix individually ( matrix - local pruning ) . Both methods will prune to the same sparsity , but in global pruning the sparsity might be unevenly distributed across weight matrices . We use matrix - local pruning because it is more popular in the community . 5 For information on other pruning techniques , we recommend Gale et al ( 2019 ) and Liu et al ( 2019b ) .", "entities": [[121, 122, "MetricName", "accuracy"]]}
{"text": "BERT - Base consists of 12 encoder layers , each of which contains 6 prunable matrices : 4 for the multiheaded self - attention and 2 for the layer 's output feed - forward network . Recall that self - attention first projects layer inputs into key , query , and value embeddings via linear projections . While there is a separate key , query , and value projection matrix for each attention head , implementations typically \" stack \" matrices from each attention head , resulting in only 3 parameter matrices : one for key projections , one for value projections , and one for query projections . We prune each of these matrices separately , calculating a threshold for each . We also prune the linear output projection , which combines outputs from each attention head into a single embedding . 6 We prune word embeddings in the same way we prune feed - foward networks and self - attention parameters . 7 The justification is similar : if a word embedding value is close to zero , we can assume it 's zero and store the rest in a sparse matrix . This is useful because token / subword embeddings tend to account for a large portion of a natural language model 's memory . In BERT - Base specifically , 5 The weights in almost every matrix in BERT - Base are approximately normally distributed with mean 0 and variance between 0.03 and 0.05 ( Table A ) . This similarity may imply that global pruning would perform similarly to matrix - local pruning . 6 We could have calculated a single threshold for the entire self - attention layer or for each attention head separately . Similar to global pruning vs. matrix - local pruning , it 's not clear which one should be preferred . 7 Interestingly , pruning word embeddings is slightly more interpretable that pruning other matrices . See Figure ? ? for a heatmap of embedding magnitudes , which shows that shorter subwords tend to be pruned more than longer subwords and that certain dimensions are almost never pruned in any subword . the embeddings account for \u223c21 % of the model 's memory . Our experimental code for pruning BERT , based on the public BERT repository , is available here . 8", "entities": [[0, 1, "MethodName", "BERT"], [36, 37, "MetricName", "Recall"], [146, 148, "TaskName", "word embeddings"], [219, 220, "MethodName", "BERT"], [232, 233, "MethodName", "BERT"], [241, 242, "DatasetName", "0"], [315, 317, "TaskName", "word embeddings"], [332, 333, "MethodName", "heatmap"], [379, 380, "MethodName", "BERT"], [385, 386, "MethodName", "BERT"]]}
{"text": "We perform weight magnitude pruning on a pretrained BERT - Base model . 9 We select sparsities from 0 % to 90 % in increments of 10 % and gradually prune BERT to this sparsity over the first 10k steps of training . We continue pre - training on English Wikipedia and BookCorpus for another 90k steps to regain any lost accuracy . 10 The resulting pre - training losses are shown in Table 1 . We then fine - tune these pruned models on tasks from the General Language Understanding Evaluation ( GLUE ) benchmark , which is a standard set of 9 tasks that include sentiment analysis , natural language inference , etc . We avoid WNLI , which is known to be problematic . 11 We also avoid tasks with less than 5k training examples because the results tend to be noisy ( RTE , MRPC , STS - B ) . We fine - tune a separate model on each of the remaining 5 GLUE tasks for 3 epochs and try 4 learning rates : [ 2 , 3 , 4 , 5 ] \u00d7 10 \u22125 . The best evaluation accuracies are averaged and plotted in Figure 1 . Individual task results are in Table 1 . BERT can be used as a static feature - extractor or as a pre - trained model which is fine - tuned endto - end . In all experiments , we fine - tune weights in all layers of BERT on downstream tasks .", "entities": [[8, 9, "MethodName", "BERT"], [18, 19, "DatasetName", "0"], [31, 32, "MethodName", "BERT"], [52, 53, "DatasetName", "BookCorpus"], [61, 62, "MetricName", "accuracy"], [88, 89, "DatasetName", "General"], [93, 94, "DatasetName", "GLUE"], [107, 109, "TaskName", "sentiment analysis"], [110, 113, "TaskName", "natural language inference"], [118, 119, "DatasetName", "WNLI"], [146, 147, "DatasetName", "RTE"], [148, 149, "DatasetName", "MRPC"], [150, 153, "DatasetName", "STS - B"], [168, 169, "DatasetName", "GLUE"], [212, 213, "MethodName", "BERT"], [251, 252, "MethodName", "BERT"]]}
{"text": "Pruning involves two steps : it deletes the information stored in a weight by setting it to 0 and then regularizes the model by preventing that weight from changing during further training . To disentangle these two effects ( model complexity restriction and information deletion ) , we repeat the experiments from Section 3.2 with an identical pre - training setup , but instead of pruning we simply set the weights to 0 and allow them to vary during downstream training . This deletes the pre - training information associated with the weight but does not prevent the model from fitting downstream datasets by keeping the weight at zero during downstream training . We also fine - tune on downstream tasks until training loss becomes comparable to models with no pruning . We trained most models for epochs rather than 3 . Models with 70 - 90 % information deletion required 15 epochs to fit the training data . The results are also included in Figure 1 and Table 1 .", "entities": [[17, 18, "DatasetName", "0"], [72, 73, "DatasetName", "0"], [123, 124, "MetricName", "loss"]]}
{"text": "We might expect that BERT would be more compressible after downstream fine - tuning . Intuitively , the information needed for downstream tasks is a subset of the information learned during pretraining ; some tasks require more semantic information than syntactic , and vice - versa . We should be able to discard the \" extra \" information and only keep what we need for , say , parsing ( Li and Eisner , 2019 ) . For magnitude weight pruning specifically , we might expect downstream training to change the distribution of weights in the parameter matrices . This , in turn , changes the sort - order of the absolute values of those weights , which changes the order that we prune them in . This new pruning order , hypothetically , would be less degrading to our specific downstream task . To test this , we fine - tuned pre - trained BERT - Base on downstream data for 3 epochs . We then pruned at various sparsity levels and continued training for 5 more epochs ( 7 for 80/90 % sparsity ) , at which point the training losses became comparable to those of models pruned during pretraining . We repeat this for learning rates in [ 2 , 3 , 4 , 5 ] \u00d710 \u22125 and show the results with the best development accuracy in Figure 1 / Table 1 . We also measure the difference in which weights are selected for pruning during pre - training vs. downstream fine - tuning and plot the results in Figure 3 .", "entities": [[4, 5, "MethodName", "BERT"], [155, 156, "MethodName", "BERT"], [229, 230, "MetricName", "accuracy"]]}
{"text": "Figure 1 shows that the first 30 - 40 % of weights pruned by magnitude weight pruning do not impact pre - training loss or inference on any downstream task . These weights can be pruned either before or after fine - tuning . This makes sense from the perspective of pruning as sparse architecture search : when we initialize BERT - Base , we initialize many possible subnetworks . SGD selects the best one for pre - training and pushes the rest of the weights to 0 . We can then prune those weights without affecting the output of the network . 12", "entities": [[23, 24, "MetricName", "loss"], [60, 61, "MethodName", "BERT"], [70, 71, "MethodName", "SGD"], [87, 88, "DatasetName", "0"]]}
{"text": "Past 40 % pruning , performance starts to degrade . Pre - training loss increases as we prune weights necessary for fitting the pre - training data ( Table 1 ) . Feature activations of the hidden layers start to diverge from models with low levels of pruning ( Figure 2 ) . 13 Downstream accuracy also begins to degrade at this point . Why does pruning at these levels hurt downstream performance ? On one hand , pruning deletes pre - training information by setting weights to 0 , preventing the transfer of the useful inductive biases learned during pre - training . On the other hand , pruning regularizes the model by keeping certain weights at zero , which might prevent fitting downstream datasets . Figure 1 and Table 1 show information deletion is the main cause of performance degradation between 40 - 60 % sparsity , since pruning and information deletion degrade models by the same amount . Information deletion would not be a problem if pretraining and downstream datasets contained similar information . However , pre - training is effective precisely because the pre - training dataset is much larger than the labeled downstream dataset , which allows learning of more robust representations . We see that the main obstacle to compressing pre - trained models is maintaining the inductive bias of the model learned during pre - training . Encoding this bias requires many more weights than fitting downstream datasets , and it can not be recovered due to a fundamental information gap between pretraining and downstream datasets . 14 This leads us to believe that the amount a model can be pruned 12 We know , however , that increasing the size of BERT to BERT - Large improves performance . This view does not fully explain why even an obviously under - parameterized model should become sparse . This may be caused by dropout , or it may be a general property of our training regime ( SGD ) . Perhaps an extension of to under - parameterized models would provide some insight . 13 We believe this observation may point towards a more principled stopping criterion for pruning . Currently , the only way to know how much to prune is by trial and ( dev - set ) error . Predictors of performance degradation while pruning might help us decide which level of sparsity is appropriate for a given trained network without trying many at once . 14 We might consider finding a lottery ticket for BERT , which we would expect to fit the GLUE training data just as well as pre - trained BERT . However , we predict that the lottery - ticket will not reach similar generalization levels unless the lottery ticket encodes enough information to close the information gap . Also shown are models with information deletion during pre - training ( orange ) , models pruned after downstream fine - tuning ( green ) , and models pruned randomly during pre - training instead of by lowest magnitude ( red ) . 30 - 40 % of weights can be pruned using magnitude weight pruning without decreasing dowsntream accuracy . Notice that information deletion fits the training data better than un - pruned models at all sparsity levels but does not fully recover evaluation accuracy . Also , models pruned after downstream fine - tuning have the same or worse development accuracy , despite achieving lower training losses . Note : none of the pruned models are overfitting because un - pruned models have the lowest training loss and the highest development accuracy . While the results for individual tasks are in Table 1 , each task does not vary much from the average trend , with an exception discussed in Section 4.3 . Figure 2 : ( Left ) Pre - training loss predicts information deletion GLUE accuracy linearly as sparsity increases . We believe the slope of each line tells us how much a bit of BERT is worth to each task . ( CoLA at 90 % is excluded from the line of best fit . ) ( Right ) The cosine similarities of features extracted for a subset of the pre - training development data before and after pruning . Features are extracted from activations of all 12 layers of BERT and compared layer - wise to a model that has not been pruned . As performance degrades , cosine similarities of features decreases . is limited by the largest dataset the model has been trained on : in this case , the pre - training dataset . 15", "entities": [[13, 14, "MetricName", "loss"], [55, 56, "MetricName", "accuracy"], [88, 89, "DatasetName", "0"], [289, 290, "MethodName", "BERT"], [291, 292, "MethodName", "BERT"], [334, 335, "MethodName", "SGD"], [425, 426, "MethodName", "BERT"], [434, 435, "DatasetName", "GLUE"], [444, 445, "MethodName", "BERT"], [533, 534, "MetricName", "accuracy"], [559, 560, "MetricName", "accuracy"], [576, 577, "MetricName", "accuracy"], [602, 603, "MetricName", "loss"], [607, 608, "MetricName", "accuracy"], [648, 649, "MetricName", "loss"], [652, 653, "DatasetName", "GLUE"], [653, 654, "MetricName", "accuracy"], [673, 674, "MethodName", "BERT"], [681, 682, "DatasetName", "CoLA"], [729, 730, "MethodName", "BERT"]]}
{"text": "At 70 % sparsity and above , models with information deletion recover some accuracy w.r.t . pruned models , so complexity restriction is a secondary cause of performance degradation . However , these models do not recover all evaluation accuracy , despite matching un - pruned model 's training loss . Table 1 shows that on the MNLI and QQP tasks , which have the largest amount of training data , information deletion performs much better than pruning . In contrast , models do not recover as well on SST - 2 and CoLA , which have less data . We believe this is because the larger datasets require larger models to fit , so complexity restriction becomes an issue earlier . We might be concerned that poorly performing models are over - fitting , since they have lower training losses than unpruned models . But the best performing information - deleted models have the lowest training error of all , so overfitting seems unlikely . 16", "entities": [[13, 14, "MetricName", "accuracy"], [39, 40, "MetricName", "accuracy"], [49, 50, "MetricName", "loss"], [57, 58, "DatasetName", "MNLI"], [59, 60, "DatasetName", "QQP"], [89, 90, "DatasetName", "SST"], [93, 94, "DatasetName", "CoLA"]]}
{"text": "We 've seen that over - pruning BERT deletes information useful for downstream tasks . Is this information equally useful to all tasks ? We might consider the pre - training loss as a proxy for how much pre - training information we 've deleted in total . Similarly , the performance of informationdeletion models is a proxy for how much of that information was useful for each task . Figure 2 shows that the pre - training loss linearly predicts the effects of information deletion on downstream accuracy . For every bit of information we delete from BERT , it appears only a fraction is useful for CoLA , and an even smaller fraction useful for QQP . 17 This relationship should be taken into account when considering the memory / accuracy trade - off of overpruning . Pruning an extra 30 % of BERT 's weights Figure 3 : ( Top ) The measured difference in pruning masks between models pruned during pre - training and models pruned during downstream fine - tuning . As predicted , the differences are less than 6 % , since finetuning only changes the magnitude sorting order of weights locally , not globally . ( Bottom ) The average GLUE development accuracy and pruning mask difference for models trained on downstream datasets before pruning 60 % at learning rate 5e - 5 . After pruning , models are trained for an additional 2 epochs to regain accuracy . We see that training between 3 and 12 epochs before pruning does not change which weights are pruned or improve performance . is worth only one accuracy point on QQP but 10 points on CoLA . It 's unclear , however , whether this is because the pre - training task is less relevant to QQP or whether QQP simply has a bigger dataset with more information content . 18", "entities": [[8, 9, "MethodName", "BERT"], [32, 33, "MetricName", "loss"], [80, 81, "MetricName", "loss"], [90, 91, "MetricName", "accuracy"], [100, 101, "MethodName", "BERT"], [110, 111, "DatasetName", "CoLA"], [119, 120, "DatasetName", "QQP"], [134, 135, "MetricName", "accuracy"], [147, 148, "MethodName", "BERT"], [209, 210, "DatasetName", "GLUE"], [211, 212, "MetricName", "accuracy"], [227, 229, "HyperparameterName", "learning rate"], [246, 247, "MetricName", "accuracy"], [274, 275, "MetricName", "accuracy"], [277, 278, "DatasetName", "QQP"], [282, 283, "DatasetName", "CoLA"], [303, 304, "DatasetName", "QQP"], [306, 307, "DatasetName", "QQP"]]}
{"text": "Since pre - training information deletion plays a central role in performance degradation while overpruning , we might expect that downstream fine - tuning would improve prunability by making important weights more salient ( increasing their magnitude ) . However , Figure 1 shows that models pruned after downstream fine - tuning do not surpass the development accuracies of models pruned during pre - training , despite achieving similar training losses . Figure 3 shows fine - tuning changes which weights are pruned by less than 6 % . Why does n't fine - tuning change which weights are pruned much ? Table 2 shows that the magnitude sorting order of weights is mostly preserved ; weights move on average 0 - 4 % away from their starting positions in the sort order . We also see that high magnitude weights are more stable than lower ones ( Figure 6 ) . Our experiments suggest that training on downstream data before pruning is too blunt an instrument to improve prunability . Even so , we might consider simply training on the downstream tasks for much longer , which would increase the difference in weights pruned . However , Figure 4 shows that even after an epoch of downstream fine - tuning , weights quickly re - stabilize in a new sorting order , meaning longer downstream training will have only a marginal effect on which weights are pruned . Indeed , Figure 3 shows that the weights selected for 60 % pruning quickly stabilize and evaluation accuracy does not improve with more training before pruning .", "entities": [[120, 121, "DatasetName", "0"], [256, 257, "MetricName", "accuracy"]]}
{"text": "Compressing BERT for Specific Tasks Section 5 showed that downstream fine - tuning does not increase prunability . However , several alternative compression approaches have been proposed to discard non - task - specific information . Li and Eisner ( 2019 ) used an information bottleneck to discard non - syntactic information . Tang et al ( 2019 ) used BERT as a knowledge distillation teacher to compress relevant information into smaller Bi - LSTMs , while Kuncoro et al ( 2019 ) took a similar distillation approach . While fine - tuning does not increase prunability , task - specific knowledge might be extracted from BERT with other methods . Attention Head Pruning previously showed redundancy in transformer models by pruning entire attention heads . Michel et al ( 2019 ) showed that after fine - tuning on MNLI , up to 40 % of attention heads can be pruned from BERT without affecting test accuracy . They show redundancy in BERT after fine - tuning on a single downstream task ; in contrast , our work emphasizes the interplay between compression and transfer learning to many tasks , pruning both before and after finetuning . Also , magnitude weight pruning allows us to additionally prune the feed - foward networks and sub - word embeddings in BERT ( not just selfattention ) , which account for \u223c72 % of BERT 's total memory usage . We suspect that attention head pruning and weight pruning remove different redundancies from BERT . Figure 4 shows that weight pruning does not prune any specific attention head much more than the pruning rate for the whole model . It is not clear , however , whether weight pruning and recovery training makes attention heads less prunable by distributing functionality to unused heads .", "entities": [[1, 2, "MethodName", "BERT"], [60, 61, "MethodName", "BERT"], [63, 65, "MethodName", "knowledge distillation"], [106, 107, "MethodName", "BERT"], [139, 140, "DatasetName", "MNLI"], [152, 153, "MethodName", "BERT"], [156, 157, "MetricName", "accuracy"], [162, 163, "MethodName", "BERT"], [184, 186, "TaskName", "transfer learning"], [215, 217, "TaskName", "word embeddings"], [218, 219, "MethodName", "BERT"], [231, 232, "MethodName", "BERT"], [250, 251, "MethodName", "BERT"]]}
{"text": "We 've shown that encoding BERT 's inductive bias requires many more weights than are required to fit downstream data . Future work on compressing pre - trained models should focus on maintaining that inductive bias and quantifying its relevance to various tasks during accuracy / memory trade - offs . For magnitude weight pruning , we 've shown that 30 - 40 % of the weights do not encode any useful inductive bias and can be discarded without affecting BERT 's universality . The relevance of the rest of the weights vary from task to task , and fine - tuning on downstream tasks does not change the nature of this trade - off by changing which weights are pruned . In future work , we will investigate the factors that influence language modeling 's relevance to downstream tasks and how to improve compression in a task - general way . It 's reasonable to believe that these conclusions will generalize to other pre - trained language models such as Kermit ( Chan et al , 2019 ) , XLNet ( Yang et al , 2019 ) , GPT - 2 ( Radford et al , 2019 ) , RoBERTa ( Liu et al , 2019a ) or ELMO ( Peters et al , 2018 ) . All of these learn some variant of language modeling , and most use Transformer architectures . While it remains to be shown in future work , viewing pruning as architecture search implies these models will be prunable due to the training dynamics inherent to neural networks . 1 . Pre - training losses are omitted for models pruned after downstream fine - tuning because it is not clear how to measure their performance on the pre - training task in a fair way . Figure 5 : The sum of weights pruned at each sparsity level for one shot pruning of BERT . Given the motivation for our saliency criterion , it seems strange that such a large magnitude of weights can be pruned without decreasing accuracy . LR MNLI QQP QNL SST - 2 CoLA 2e - 5 1.91 \u00b1 1.81 1.82 \u00b1 1.72 1.27 \u00b1 1.22 1.06 \u00b1 1.03 0.79 \u00b1 0.77 3e - 5 2.68 \u00b1 2.51 2.56 \u00b1 2.40 1.79 \u00b1 1.69 1.54 \u00b1 1.47 1.06 \u00b1 1.03 4e - 5 3.41 \u00b1 3.18 3.30 \u00b1 3.10 2.31 \u00b1 2.19 1.99 \u00b1 1.89 1.11 \u00b1 1.09 5e - 5 4.12 \u00b1 3.83 4.02 \u00b1 3.74 2.77 \u00b1 2.62 2.38 \u00b1 2.29 1.47 \u00b1 1.43 Table 2 : We compute the magnitude sorting order of each weight before and after downstream fine - tuning . If a weight 's original position is 59 / 100 before fine - tuning and 63 / 100 after fine - tuning , then that weight moved 4 % in the sorting order . We then list the average movement of weights in each model , along with the standard deviation . Sorting order changes mostly locally across tasks : a weight moves , on average , 0 - 4 % away from its starting position . As expected , larger datasets and larger learning rates have more movement ( per epoch ) . We also see that higher magnitude weights are more stable than lower weights , see Figure 6 . Figure 6 : We show how weight sort order movements are distributed during fine - tuning , given a weight 's starting magnitude . We see that higher magnitude weights are more stable than lower magnitude weights and do not move as much in the sort order . This plot is nearly identical for every model and learning rate , so we only show it once . Figure 7 : A heatmap of the weight magnitudes of the 12 horizontally stacked self - attention key projection matrices for layer 1 . A banding pattern can be seen : the highest values of the matrix tend to cluster in certain attention heads . This pattern appears in most of the self - attention parameter matrices , but it does not cause pruning to prune one head more than another . However , it may prove to be a useful heuristic for attention head pruning , which would not require making many passes over the training data . ( Right ) A heatmap of the weight magnitudes of BERT 's subword embeddings . Interestingly , pruning BERT embeddings are more interpretable ; we can see shorter subwords ( top rows ) have smaller magnitude values and thus will be pruned earlier than other subword embeddings .", "entities": [[6, 7, "MethodName", "BERT"], [45, 46, "MetricName", "accuracy"], [82, 83, "MethodName", "BERT"], [182, 183, "MethodName", "XLNet"], [191, 192, "MethodName", "GPT"], [202, 203, "MethodName", "RoBERTa"], [211, 212, "MethodName", "ELMO"], [233, 234, "MethodName", "Transformer"], [321, 322, "MethodName", "BERT"], [346, 347, "MetricName", "accuracy"], [349, 350, "DatasetName", "MNLI"], [350, 351, "DatasetName", "QQP"], [352, 353, "DatasetName", "SST"], [355, 356, "DatasetName", "CoLA"], [515, 516, "DatasetName", "0"], [617, 619, "HyperparameterName", "learning rate"], [631, 632, "MethodName", "heatmap"], [730, 731, "MethodName", "heatmap"], [736, 737, "MethodName", "BERT"], [744, 745, "MethodName", "BERT"]]}
{"text": "QG often uses standard evaluation metrics from text summarization and machine translation ( BLEU ( Papineni et al , 2002 ) , ROUGE ( Lin , 2004 ) , METEOR ( Banerjee and Lavie , 2005 ) , etc . ) . However , such metrics do not provide an accurate evaluation for QG task ( Novikova et al , 2017 ) , especially when the input passage is long ( and many acceptable questions that differ from the gold question can be generated ) . Thus , to alleviate shortcomings associated with n - gram based similarity metrics , we use BLEURT ( Sellam et al , 2020 ) ( BLEURT - 20 ) , which is state - of - the - art evaluation metric in WMT Metrics shared task . BLEURT is a BERT - based model that uses multi - task learning to evaluate a generated text by giving it a value mostly between 0.0 and 1.0 . In our experiments , we consider BLEURT as the main metric for the evaluation . We also report standard MT metric BLEU ( 1 - 4 ngrams ) , and perform an additional manual evaluation . Manual evaluation is required in our collected dataset , because teachers wrote a single question per skill for a given story , where the model might generate other possible questions for the same skill .", "entities": [[7, 9, "TaskName", "text summarization"], [10, 12, "TaskName", "machine translation"], [13, 14, "MetricName", "BLEU"], [29, 30, "DatasetName", "METEOR"], [136, 137, "MethodName", "BERT"], [142, 146, "TaskName", "multi - task learning"], [183, 184, "MetricName", "BLEU"]]}
{"text": "We fine - tune a T5 model ( t5 - base from Hugging - Face library ) using the Adam optimizer with a batch size of 8 and a learning rate of 1e\u22124 . We use a maximum sequence length of 512 for the encoder , and 128 for the decoder 7 . We tested the T5 - large model , but we did not notice any improvements considering BLEURT metric . We train all models for a maximum of ten epochs with an early stopping value of 1 ( patience ) based on the validation loss . We use a single NVIDIA TITAN RTX with 24 G RAM . For HTA , we validate on a combined version of the validation sets from both datasets ( SQuAD and CosmosQA ) . Regarding the collected dataset validation set , we use stratified sampling : we took a random 10 % of stories from each skill since the dataset is unbalanced . We apply the same strategy with the test set but with a value of 20 % .", "entities": [[5, 6, "MethodName", "T5"], [8, 9, "MethodName", "t5"], [19, 20, "MethodName", "Adam"], [20, 21, "HyperparameterName", "optimizer"], [23, 25, "HyperparameterName", "batch size"], [29, 31, "HyperparameterName", "learning rate"], [56, 57, "MethodName", "T5"], [84, 86, "MethodName", "early stopping"], [96, 97, "MetricName", "loss"], [103, 104, "DatasetName", "TITAN"], [108, 109, "MethodName", "RAM"], [127, 128, "DatasetName", "SQuAD"], [129, 130, "DatasetName", "CosmosQA"]]}
{"text": "Table 2 presents the results of the proposed HTA - WTA method with the baselines . We can see that out of the baselines , T5 - WTA performs best in terms of BLEURT score ( 32.96 % ) , followed by NQG - Max with a value of 31.78 % . Given its high BLEURT score , it is surprising that T5 - WTA model has low BLEU - 4 . This implies that the generated questions use rich vocabulary , making them different from the gold in terms of overlapping ngrams , but semantically similar leading to higher BLEURT score . As shown in the table , HTA - WTA 's BLEURT score outperforms all of the previous QG models by a noticeable margin , showing that including the skill name information plays an important role in generating the intended questions . Also , training on more QG datasets improves the performance . We also noted that the CGC - QG model achieves a higher BLEU - 1 than our HTA - WTA model . We argue that this is because the Clue Words Prediction Module learns important cues , increasing the uni - gram overlap with the gold references ( BLEU - 1 ) . Regarding the generated questions type , in Table 3 we show the performance of the T5 - based models per question type ( inferential and literal ) . Though One - Step and HTA - WTA models were trained on the same amount of data , the results show that HTA - WTA model clearly performs better than the One - Step model , especially on inferential questions . We see a similar scenario when comparing One - Step and T5 - WTA models , yet , the gap is smaller . In general , we can notice that the performance gaps for the inferential questions are larger than the literal ones . Thus , we can conclude that HTA - WTA is generating more correct inferential questions , which is challenging . This experiment concludes that transformers - based models are capable of asking questions beyond the literal meaning of the text . This confirms what was shown by Liu et al ( 2021a ) regarding the skills that language models can acquire . Additionally , as some training questions directly quote text from the given story . The T5 model was able to learn how to quote the proper segment of the passage when generating questions . The One - Step model performs similarly to the baselines , although it has been trained using the T5 model and on all three datasets . This may be due to the fact that we did not include the skill name in the encoder , which guides the model to generate skill related questions . To better understand the differences between the outputs of One - Step and HTA - WTA models , we used human evaluation . This evaluation is to assess the quality of the generated question in terms of 1 . Answerability ( Ay ) , 2 . Fluency ( Fy ) , and 3 . Grammaticality ( Gy ) categories , following Harrison and Walker ( 2018 ) ; Azevedo et al ( 2020 ) . We include these three criteria as questions may have high Fluency and Grammaticality scores , but not be answerable . We select a sample of 110 story - question pairs from the test dataset , for both models . Then , we perform a human evaluation using crowdworkers on Amazon Mechanical Turk . We use a \" master \" qualification criteria to restrict the participation of workers in our evaluation study to those who have a high historical HIT accuracy , and workers are required to be located in an English speaking country . Each HIT was answered by three workers . Each worker needs reads the story , and provides ratings ( 1 - 5 , low to high ) for the generated questions , and the three criteria . Table 4 shows the average rating assigned by the workers for the 3 criteria . Originally , we hypothesized that adding the skill name to the input would force the model to formulate a specific SBRCS question , even if it is not applicable to the current passage . Omitting the skill name may allow the model score high values as it has been left to decide the question . The results show that both models are similar in terms of the given categories , except that HTA - WTA performs slightly better in all of the three categories . However , these results refute our claim and show that adding the skill information makes the model generates slightly better questions in terms of quality . In Section A.4 , we present an ablation test and discuss some causes of errors in generating questions . Impact of Skill Name Token . In order to quantify the impact of skill name in the input , we do another human manual evaluation to assess how beneficial the skill name token is when we add it to the HTA - WTA model . Thus , we ask two professional persons who were involved in the annotation process to assign skill names to the generated questions of both One - Step and HTA - WTA models . We selected these models as they were trained on the same amount of data ; the only difference between them is that the HTA - WTA model uses the skill name token . We utilize the same question sample that was used in the previous human evaluation experiment . Few annotation conflicts were found and were solved after a discussion . We evaluate the results using accuracy ( see Table 4 ) . The result for One - Step model is 0.16 , and 0.8 for HTA - WTA model . We can clearly see a large gap in accuracy between both models , and this becomes clear with the skills that have a low number of instances in the dataset ( e.g. Figurative Language , Predicting , etc . ) . This result shows that , in addition to using the skill name token to control the skill of the generated questions , it helps the model to learn the underrepresented skills in the dataset . Table 6 in Appendix A.5 presents the F1 scores per skill name . We also notice that HTA - WTA model performed perfectly on the given sample of Predicting and Figurative Language ( F1 is 1.0 for each skill ) . This is an interesting result given that the type of the questions for both skills is inferential , which is harder to generate compared to the literal questions . Few - Shot Generation . The process of manually writing questions to assess humans SBRCS is difficult . In some stories , professional writers find obstacles in writing questions for some skills as those skills require high attention and advanced reasoning skills to be written . We can see that in our own dataset , as some skills have fewer questions ( e.g. Predicting , Visualizing , etc . ) . Thus , in this experiment , we evaluate the performance of HTA - WTA model when we inject a low percentage of the skills ' instances into the training set . This experiment will simulate the case when training a model on a dataset that contains few skills ' instances . We use the stratified sampling technique when sampling fewer instances from the collected dataset . Figure 3 shows that injecting only 10 % of the data led to a boost in performance of 5.99 ( BLEURT ) . The result at 10 % ( 33.21 % ) exceeds the results of most of the baselines and is higher than T5 - WTA and NQG - MAX models when trained on all the datasets ( see Table 2 ) . In Table A.6 in the appendix , we present the results considering other models and metrics . In most cases , the performance gradually improves as data grows . We notice a small drop when we move from 10 % to 30 % . This behaviour was previously reported by Stappen et al ( 2020 ) . Further research is needed to investigate the causes of this behaviour .", "entities": [[25, 26, "MethodName", "T5"], [62, 63, "MethodName", "T5"], [68, 69, "MetricName", "BLEU"], [167, 168, "MetricName", "BLEU"], [203, 204, "MetricName", "BLEU"], [223, 224, "MethodName", "T5"], [288, 289, "MethodName", "T5"], [398, 399, "MethodName", "T5"], [435, 436, "MethodName", "T5"], [626, 627, "MetricName", "accuracy"], [967, 968, "MetricName", "accuracy"], [1000, 1001, "MetricName", "accuracy"], [1075, 1076, "MetricName", "F1"], [1101, 1102, "MetricName", "F1"], [1319, 1320, "MethodName", "T5"]]}
{"text": "Ablation Test . The results of our experiments confirmed the importance of both the skill name token and the two - steps training method . To quantify the impact of including the skill name token , we run T5 - WTA without including the skill name token ( T5 - WTA - unskilled ) . We compare the T5 - WTAunskilled to the One - Step model ; the only difference between these models is that One - Step model includes SQuAD and CosmosQA datasets in the training data . The ablation test results in Table 5 shows that the skill name token and the additional training data both increase model performance . T5 - WTA - unskilled BLEURT performance is lower than the BLEURT scores of the other two models . Error Analysis . Here we are interested in further understanding the HTA - WTA model 's performance . We manually examined several generated questions to understand the sources of its errors . Given the unbalanced status of the dataset , we found that the model does not always generate an appropriate question for a given skill name , especially when that skill is underrepresented in the data ( e.g. Visualizing , Figurative Language , etc . ) . In some cases , the model learned the style of the skill 's questions , but in the given context , the generated question could not be answered . As an example , the following generated figurative language question quoted a sentence from a story about the space . The sentence is an event in the story and not a figurative language : Which figurative language technique is being used in the phrase \" The first safe trip into space \" ? This happens even for very common skill categories , again due to the difficulty ( or even impossibility ) of generating questions for some skill and story pairs . The other kind of error is the subjectivity in selecting the \" correct \" words from the story . For instance , giving the following Vocabulary question from the dataset : What is the correct definition of the word \" decoy \" as it is used in the story ? For this kind of question , annotators chose words that can have multiple meanings , some of which may be unfamiliar to school children . The process of choosing those words is subjective . Although both annotators agreed on the word in the previous example , the model chose to select another word from the story ( \" panting \" ) . In other cases , the question asks about the definition of a word within a sentence from the story ( e.g. What is the meaning of \" word \" as it is used in this sentence : \" quoted sentence \" ) . We noted that when the model generated the question , it selects the correct word but sometimes used a randomly quoted sentence from the story that did n't contain the word .", "entities": [[38, 39, "MethodName", "T5"], [48, 49, "MethodName", "T5"], [58, 59, "MethodName", "T5"], [81, 82, "DatasetName", "SQuAD"], [83, 84, "DatasetName", "CosmosQA"], [113, 114, "MethodName", "T5"], [132, 133, "MetricName", "Error"]]}
{"text": "In Table 7 , we show the few - shot experiment 's results considering both scoring metrics ( BLEU , and BLUERT ) . We do not experiment with One - Step model as we need to sample SQuAD and Cos - mosQA datasets when we sample the collected data ; it is hard to set up a fair comparison here as , for instance , sampling 10 % of SQuAD dataset is larger than the whole collected dataset .", "entities": [[18, 19, "MetricName", "BLEU"], [38, 39, "DatasetName", "SQuAD"], [70, 71, "DatasetName", "SQuAD"]]}
{"text": "Text Generation Model Text generation models learn to generate a sentence Y = ( y 1 , . . . , y T ) of length T , possibly conditioned on some context X. Here each y t is a token from vocabulary A. Starting from the initial state s 0 , a recurrent neural network ( RNN ) produces a sequence of states ( s 1 , . . . , s T ) given an input sentence - feature representation ( e ( y 1 ) , . . . , e ( y T ) ) , where e ( ) denotes a word embedding function mapping a token to its ddimensional feature representation . The states are recursively updated with a function known as the cell : s t = h ( s t\u22121 , e ( y t ) ) . One typically assigns the following probability to an observation y at location t : p ( y | Y < t ) = [ softmax ( g ( s t ) ) ] y . Together ( g , h ) specifies a probabilistic model \u03c0 , i.e. , log \u03c0 ( Y ) = t log p ( y t | Y < t ) . ( 1 ) To train the model \u03c0 , one typically uses maximum likelihood estimation ( MLE ) , via minimizing the cross - entropy loss , i.e. , J MLE ( \u03c0 ) = \u2212E [ log \u03c0 ( Y ) ] . In order to generate sentence Y s from a ( trained ) model , one iteratively applies the following operations : y s t+1 \u223c Multi ( 1 , softmax ( g ( s t ) ) ) , ( 2 ) s t = h ( s t\u22121 , e ( y s t ) ) , where Multi ( 1 , ) denotes one draw from a multinomial distribution . Model - Based Imitation Learning Text generation can be considered as an RL problem with a large number of discrete actions , deterministic transitions , and deterministic terminal rewards . It can be formulated as a Markov decision process ( MDP ) M = S , A , P , r , \u03b3 , where S is the state space , A is the action space , P is the deterministic environment dynamics , r ( s , y ) is a reward function , and \u03b3 ( 0 , 1 ) is the discrete - time discount factor . The policy \u03c0 \u03c6 , parameterized by \u03c6 , maps each state s S to a probability distribution over A. The objective is to maximize the expected reward : J ( \u03c0 ) = t=1 E P , \u03c0 \u03b3 t\u22121 r ( s t , y t ) . In model - based imitation learning ( Baram et al , 2017 ; Cheng et al , 2019 ) , a model is built to make predictions for future state s t+ t conditioned on the current state 1 , which can be used for action selection , e.g. , next - token generation . This model is typically a discrete - time system , taking the current state - action pair ( s t , y t ) as input , and outputting an estimate of the future state s t+ t at time t + t. At each step t , y t is chosen based on the model , and the model will re - plan with the updated information from the dynamics . This control scheme is different from a standard model - based method , and is referred to as model - predictive control ( MPC ) ( Nagabandi et al , 2017 ) . Note that in our setting , the state in RL typically corresponds to the current generated sentences Y 1 , ... , t instead of the RNN state of generator ( decoder ) .", "entities": [[0, 2, "TaskName", "Text Generation"], [3, 5, "TaskName", "Text generation"], [50, 51, "DatasetName", "0"], [170, 171, "MethodName", "softmax"], [238, 239, "MetricName", "loss"], [286, 287, "MethodName", "softmax"], [332, 334, "TaskName", "Imitation Learning"], [334, 336, "TaskName", "Text generation"], [381, 382, "HyperparameterName", "\u03b3"], [415, 416, "HyperparameterName", "\u03b3"], [417, 418, "DatasetName", "0"], [468, 469, "HyperparameterName", "\u03b3"], [483, 485, "TaskName", "imitation learning"]]}
{"text": "The guider network , implemented as an RNN with LSTM units , is adopted to model environment dynamics to assist text generation . The idea is to train a guider network such that its predicted sentence features at each time step are used to assist next - word generation and construct intermediate rewards , which in turn are used to optimize the sentence generator . Denote the guider network as G \u03c8 ( s G t\u22121 , f t ) , with parameters \u03c8 and input arguments ( s G t\u22121 , f t ) at time t , to explicitly write out the dependency on the guider network latent state s G t\u22121 from the previous time step . Here f t is the input to the LSTM guider , which represents the feature of the current generated sentence extracted 1 t > 1 ; the model predicts future states based on the collected trajectories . f G t < l a t e x i t s h a 1 _ b a s e 6 4 = \" z U c 7 C 4 R M y k u C + q Q K u z B Z q w S L z z 0 = \" > A A A B / n i c b V B P S 8 M w H E 3 n v z n / V c W T l + A Q P I 1 2 D v Q 4 8 K D H C W 4 O t l r S N N 3 C 0 q Q k q T B K w a / i x Y M i X v 0 c 3 v w 2 p l s P u v k g 5 P H e 7 0 d e X p A w q r T j f F u V l d W 1 9 Y 3 q Z m 1 r e 2 d 3 z 9 4 / 6 C m R S k y 6 W D A h + w F S h F F O u p p q R v q J J C g O G L k P J l e F f / 9 I p K K C 3 + l p Q r w Y j T i N K E b a S L 5 9 l A 0 D w U I 1 j c 0 F o 9 z P d P 5 w 7 d t 1 p + H M A J e J W 5 I 6 K N H x 7 a 9 h K H A a E 6 4 x Q 0 o N X C f R X o a k p p i R v D Z M F U k Q n q A R G R j K U U y U l 8 3 i 5 / D U K C G M h D S H a z h T f 2 9 k K F Z F Q D M Z I z 1 W i 1 4 h / u c N U h 1 d e h n l S a o J x / O H o p R B L W D R B Q y p J F i z q S E I S 2 q y Q j x G E m F t G q u Z E t z F L y + T X r P h n j e a t 6 1 6 u 1 X W U Q X H 4 A S c A R d c g D a 4 A R 3 Q B R h k 4 B m 8 g j f r y X q x 3 q 2 P + W j F K n c O w R 9 Y n z / E B Z X 0 < / l a t e x i t > ' < l a t e x i t s h a 1 _ b a s e 6 4 = \" f F W B I b p 6 2 Q E 4 8 9 e p H A 8 8 w i M y o m E = \" > A A A B 7 X i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e y K o N 6 K X j x W c G 2 h X U o 2 z b a h 2 W x I s o W y 9 E d 4 8 a D i 1 f / j z X 9 j 2 u 5 B W x 8 M P N 6 b Y W Z e p A Q 3 1 v O + U W l t f W N z q 7 x d 2 d n d 2 z + o H h 4 9 m T T T l A U 0 F a l u R 8 Q w w S U L L L e C t Z V m J I k E a 0 W j u 5 n f G j N t e C o f 7 U S x M C E D y W N O i X V S q z s m W g 1 5 r 1 r z 6 t 4 c e J X 4 B a l B g W a v + t X t p z R L m L R U E G M 6 v q d s m B N t O R V s W u l m h i l C R 2 T A O o 5 K k j A T 5 v N z p / j M K X 0 c p 9 q V t H i u / p 7 I S W L M J I l c Z 0 L s 0 C x 7 M / E / r 5 P Z + D r M u V S Z Z Z I u F s W Z w D b F s 9 9 x n 2 t G r Z g 4 Q q j m 7 l Z M h 0 Q T a l 1 C F R e C v / z y K g k u 6 j d 1 / + G y 1 r g t 0 i j D C Z z C O f h w B Q 2 4 h y Y E Q G E E z / A K b 0 i h F / S O P h a t J V T M H M M f o M 8 f 5 y 2 P e g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" f F W B I b p 6 2 Q E 4 8 9 e p H A 8 8 w i M y o m E = \" > A A A B 7 X i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e y K o N 6 K X j x W c G 2 h X U o 2 z b a h 2 W x I s o W y 9 E d 4 8 a D i 1 f / j z X 9 j 2 u 5 B W x 8 M P N 6 b Y W Z e p A Q 3 1 v O + U W l t f W N z q 7 x d 2 d n d 2 z + o H h 4 9 m T T T l A U 0 F a l u R 8 Q w w S U L L L e C t Z V m J I k E a 0 W j u 5 n f G j N t e C o f 7 U S x M C E D y W N O i X V S q z s m W g 1 5 r 1 r z 6 t 4 c e J X 4 B a l B g W a v + t X t p z R L m L R U E G M 6 v q d s m B N t O R V s W u l m h i l C R 2 T A O o 5 K k j A T 5 v N z p / j M K X 0 c p 9 q V t H i u / p 7 I S W L M J I l c Z 0 L s 0 C x 7 M / E / r 5 P Z + D r M u V S Z Z Z I u F s W Z w D b F s 9 9 x n 2 t G r Z g 4 Q q j m 7 l Z M h 0 Q T a l 1 C F R e C v / z y K g k u 6 j d 1 / + G y 1 r g t 0 i j D C Z z C O f h w B Q 2 4 h y Y E Q G E E z / A K b 0 i h F / S O P h a t J V T M H M M f o M 8 f 5 y 2 P e g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" f F W B I b p 6 2 Q E 4 8 9 e p H A 8 8 w i M y o m E = \" > A A A B 7 X i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e y K o N 6 K X j x W c G 2 h X U o 2 z b a h 2 W x I s o W y 9 E d 4 8 a D i 1 f / j z X 9 j 2 u 5 B W x 8 M P N 6 b Y W Z e p A Q 3 1 v O + U W l t f W N z q 7 x d 2 d n d 2 z + o H h 4 9 m T T T l A U 0 F a l u R 8 Q w w S U L L L e C t Z V m J I k E a 0 W j u 5 n f G j N t e C o f 7 U S x M C E D y W N O i X V S q z s m W g 1 5 r 1 r z 6 t 4 c e J X 4 B a l B g W a v + t X t p z R L m L R U E G M 6 v q d s m B N t O R V s W u l m h i l C R 2 T A O o 5 K k j A T 5 v N z p / j M K X 0 c p 9 q V t H i u / p 7 I S W L M J I l c Z 0 L s 0 C x 7 M / E / r 5 P Z + D r M u V S Z Z Z I u F s W Z w D b F s 9 9 x n 2 t G r Z g 4 Q q j m 7 l Z M h 0 Q T a l 1 C F R e C v / z y K g k u 6 j d 1 / + G y 1 r g t 0 i j D C Z z C O f h w B Q 2 4 h y Y E Q G E E z / A K b 0 i h F / S O P h a t J V T M H M M f o M 8 f 5 y 2 P e g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" f F W B I b p 6 2 Q E 4 8 9 e p H A 8 8 w i M y o m E = \" > A A A B 7 X i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e y K o N 6 K X j x W c G 2 h X U o 2 z b a h 2 W x I s o W y 9 E d 4 8 a D i 1 f / j z X 9 j 2 u 5 B W x 8 M P N 6 b Y W Z e p A Q 3 1 v O + U W l t f W N z q 7 x d 2 d n d 2 z + o H h 4 9 m T T T l A U 0 F a l u R 8 Q w w S U L L L e C t Z V m J I k E a 0 W j u 5 n f G j N t e C o f 7 U S x M C E D y W N O i X V S q z s m W g 1 5 r 1 r z 6 t 4 c e J X 4 B a l B g W a v + t X t p z R L m L R U E G M 6 v q d s m B N t O R V s W u l m h i l C R 2 T A O o 5 K k j A T 5 v N z p / j M K X 0 c p 9 q V t H i u / p 7 I S W L M J I l c Z 0 L s 0 C x 7 M / E / r 5 P Z + D r M u V S Z Z Z I u F s W Z w D b F s 9 9 x n 2 t G r Z g 4 Q q j m 7 l Z M h 0 Q T a l 1 C F R e C v / z y K g k u 6 j d 1 / + G y 1 r g t 0 i j D C Z z C O f h w B Q 2 4 h y Y E Q G E E z / A K b 0 i h F / S O P h a t J V T M H M M f o M 8 f 5 y 2 P e g = = < / l a t e x i t > y t < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 G q 4 R 7 h 6 x w e U t W p N B q A 5 V P E x B C Y = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o N 4 K X j x W t B / Q h r L Z b t q l m 0 3 Y n Q i h 9 C d 4 8 a C I V 3 + R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K F Q d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o Z e J U M 9 5 k s Y x 1 J 6 C G S 6 F 4 E w V K 3 k k 0 p 1 E g e T s Y 3 8 7 8 9 h P X R s T q E b O E + x E d K h E K R t F K D 1 k f + + W K W 3 X n I K v E y 0 k F c j T 6 5 a / e I G Z p x B U y S Y 3 p e m 6 C / o R q F E z y a a m X G p 5 Q N q Z D 3 r V U 0 Y g b f z I / d U r O r D I g Y a x t K S R z 9 f f E h E b G Z F F g O y O K I 7 P s z c T / v G 6 K 4 b U / E S p J k S u 2 W B S m k m B M Z n + T g d C c o c w s o U w L e y t h I 6 o p Q 5 t O y Y b g L b + 8 S l q 1 q n d R r d 1 f V u o 3 e R x F O I F T O A c P r q A O d 9 C A J j A Y w j O 8 w p s j n R f n 3 f l Y t B a c f O Y Y / s D 5 / A F x W 4 3 f < / l a t e x i t > y t < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 G q 4 R 7 h 6 x w e U t W p N B q A 5 V P E x B C Y = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o N 4 K X j x W t B / Q h r L Z b t q l m 0 3 Y n Q i h 9 C d 4 8 a C I V 3 + R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K F Q d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o Z e J U M 9 5 k s Y x 1 J 6 C G S 6 F 4 E w V K 3 k k 0 p 1 E g e T s Y 3 8 7 8 9 h P X R s T q E b O E + x E d K h E K R t F K D 1 k f + + W K W 3 X n I K v E y 0 k F c j T 6 5 a / e I G Z p x B U y S Y 3 p e m 6 C / o R q F E z y a a m X G p 5 Q N q Z D 3 r V U 0 Y g b f z I / d U r O r D I g Y a x t K S R z 9 f f E h E b G Z F F g O y O K I 7 P s z c T / v G 6 K 4 b U / E S p J k S u 2 W B S m k m B M Z n + T g d C c o c w s o U w L e y t h I 6 o p Q 5 t O y Y b g L b + 8 S l q 1 q n d R r d 1 f V u o 3 e R x F O I F T O A c P r q A O d 9 C A J j A Y w j O 8 w p s j n R f n 3 f l Y t B a c f O Y Y / s D 5 / A F x W 4 3 f < / l a t e x i t > d1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" x f 5 j g l o e i p z E z b j N z t U C c V 3 7 h E Y = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l q Q Y 8 F L x 4 r 2 g 9 o Q 9 l s t u 3 S z S b s T o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I J H C o O t + O 4 W N z a 3 t n e J u a W / / 4 P C o f H z S N n G q G W + x W M a 6 G 1 D D p V C 8 h Q I l 7 y a a 0 y i Q v B N M b u d + 5 4 l r I 2 L 1 i N O E + x E d K T E U j K K V H s K B N y h X 3 K q 7 A F k n X k 4 q k K M 5 K H / 1 w 5 i l E V f I J D W m 5 7 k J + h n V K J j k s 1 I / N T y h b E J H v G e p o h E 3 f r Y 4 d U Y u r B K S Y a x t K S Q L 9 f d E R i N j p l F g O y O K Y 7 P q z c X / v F 6 K w x s / E y p J k S u 2 X D R M J c G Y z P 8 m o d C c o Z x a Q p k W 9 l b C x l R T h j a d k g 3 B W 3 1 5 n b R r V e + q W r u v V x r 1 P I 4 i n M E 5 X I I H 1 9 C A O 2 h C C x i M 4 B l e 4 c 2 R z o v z 7 n w s W w t O P n M K f + B 8 / g D q Q Y 2 C < / l a t e x i t > d2 < l a t e x i t s h a 1 _ b a s e 6 4 = \" F v 7 l i 5 4 W D O 5 8 8 U B Z M 8 e W 5 Q H B X G Q = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l q Q Y 8 F L x 4 r 2 g 9 o Q 9 l s p u 3 S z S b s b o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 7 d z v P K H S P J a P Z p q g H 9 G R 5 E P O q L H S Q z i o D c o V t + o u Q N a J l 5 M K 5 G g O y l / 9 M G Z p h N I w Q b X u e W 5 i / I w q w 5 n A W a m f a k w o m 9 A R 9 i y V N E L t Z 4 t T Z + T C K i E Z x s q W N G S h / p 7 I a K T 1 N A p s Z 0 T N W K 9 6 c / E / r 5 e a 4 Y 2 f c Z m k B i V b L h q m g p i Y z P 8 m I V f I j J h a Q p n i 9 l b C x l R R Z m w 6 J R u C t / r y O m n X q t 5 V t X Z f r z T q e R x F O I N z u A Q P r q E B d 9 C E F j A Y w T O 8 w p s j n B f n 3 f l Y t h a c f O Y U / s D 5 / A H r x Y 2 D < / l a t e x i t > dN < l a t e x i t s h a 1 _ b a s e 6 4 = \" s 5 E Z Q S S l O i a F e 5 s r L h m U a j D J V c Q = \" > A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k t 6 L H g x Z N U t B / Q h r L Z T N q l m 0 3 Y 3 Q i l 9 C d 4 8 a C I V 3 + R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S q 4 N q 7 7 7 a y t b 2 x u b R d 2 i r t 7 + w e H p a P j l k 4 y x b D J E p G o T k A 1 C i 6 x a b g R 2 E k V 0 j g Q 2 A 5 G N z O / / Y R K 8 0 Q + m n G K f k w H k k e c U W O l h 7 B / 1 y + V 3 Y o 7 B 1 k l X k 7 K k K P R L 3 3 1 w o R l M U r D B N W 6 6 7 m p 8 S d U G c 4 E T o u 9 T G N K 2 Y g O s G u p p D F q f z I / d U r O r R K S K F G 2 p C F z 9 f f E h M Z a j + P A d s b U D P W y N x P / 8 7 q Z i a 7 9 C Z d p Z l C y x a I o E 8 Q k Z P Y 3 C b l C Z s T Y E s o U t 7 c S N q S K M m P T K d o Q v O W X V 0 m r W v E u K 9 X 7 W r l e y + M o w C m c w Q V 4 c A V 1 u I U G N I H B A J 7 h F d 4 c 4 b w 4 7 8 7 H o n X N y W d O 4 A + c z x 8 W R I 2 f < / l a t e x i t > . . . < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 e 3 5 2 g W f r l v f 1 6 w M E b X 2 S 1 Z U Q C Q = \" > A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B I v g q S R V U G 8 F L x 4 r 2 A 9 o Q 9 l s N u 3 a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o Z V S m K W t S J Z T u h M Q w w S V r I k f B O q l m J A k F a 4 e j 2 5 n f f m L a c C U f c J y y I C E D y W N O C V q p 1 R O R Q t M v V 7 y q N 4 e 7 S v y c V C B H o 1 / + 6 k W K Z g m T S A U x p u t 7 K Q Y T o p F T w a a l X m Z Y S u i I D F j X U k k S Z o L J / N q p e 2 a V y I 2 V t i X R n a u / J y Y k M W a c h L Y z I T g 0 y 9 5 M / M / r Z h h f B x M u 0 w y Z p I t F c S Z c V O 7 s d T f i m l E U Y 0 s I 1 d z e 6 t I h 0 Y S i D a h k Q / C X X 1 4 l r V r V v 6 j W 7 i 8 r 9 Z s 8 j i K c w C m c g w 9 X U I c 7 a E A T K D z C M 7 z C m 6 O c F + f d + V i 0 F p x 8 5 h j + w P n 8 A b q 4 j z M = < / l a t e x i t > d1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" x f 5 j g l o e i p z E z b j N z t U C c V 3 7 h E Y = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l q Q Y 8 F L x 4 r 2 g 9 o Q 9 l s t u 3 S z S b s T o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I J H C o O t + O 4 W N z a 3 t n e J u a W / / 4 P C o f H z S N n G q G W + x W M a 6 G 1 D D p V C 8 h Q I l 7 y a a 0 y i Q v B N M b u d + 5 4 l r I 2 L 1 i N O E + x E d K T E U j K K V H s K B N y h X 3 K q 7 A F k n X k 4 q k K M 5 K H / 1 w 5 i l E V f I J D W m 5 7 k J + h n V K J j k s 1 I / N T y h b E J H v G e p o h E 3 f r Y 4 d U Y u r B K S Y a x t K S Q L 9 f d E R i N j p l F g O y O K Y 7 P q z c X / v F 6 K w x s / E y p J k S u 2 X D R M J c G Y z P 8 m o d C c o Z x a Q p k W 9 l b C x l R T h j a d k g 3 B W 3 1 5 n b R r V e + q W r u v V x r 1 P I 4 i n M E 5 X I I H 1 9 C A O 2 h C C x i M 4 B l e 4 c 2 R z o v z 7 n w s W w t O P n M K f + B 8 / g D q Q Y 2 C < / l a t e x i t > d2 < l a t e x i t s h a 1 _ b a s e 6 4 = \" F v 7 l i 5 4 W D O 5 8 8 U B Z M 8 e W 5 Q H B X G Q = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l q Q Y 8 F L x 4 r 2 g 9 o Q 9 l s p u 3 S z S b s b o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 7 d z v P K H S P J a P Z p q g H 9 G R 5 E P O q L H S Q z i o D c o V t + o u Q N a J l 5 M K 5 G g O y l / 9 M G Z p h N I w Q b X u e W 5 i / I w q w 5 n A W a m f a k w o m 9 A R 9 i y V N E L t Z 4 t T Z + T C K i E Z x s q W N G S h / p 7 I a K T 1 N A p s Z 0 T N W K 9 6 c / E / r 5 e a 4 Y 2 f c Z m k B i V b L h q m g p i Y z P 8 m I V f I j J h a Q p n i 9 l b C x l R R Z m w 6 J R u C t / r y O m n X q t 5 V t X Z f r z T q e R x F O I N z u A Q P r q E B d 9 C E F j A Y w T O 8 w p s j n B f n 3 f l Y t h a c f O Y U / s D 5 / A H r x Y 2 D < / l a t e x i t > dN < l a t e x i t s h a 1 _ b a s e 6 4 = \" s 5 E Z Q S S l O i a F e 5 s r L h m U a j D J V c Q = \" > A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k t 6 L H g x Z N U t B / Q h r L Z T N q l m 0 3 Y 3 Q i l 9 C d 4 8 a C I V 3 + R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S q 4 N q 7 7 7 a y t b 2 x u b R d 2 i r t 7 + w e H p a P j l k 4 y x b D J E p G o T k A 1 C i 6 x a b g R 2 E k V 0 j g Q 2 A 5 G N z O / / Y R K 8 0 Q + m n G K f k w H k k e c U W O l h 7 B / 1 y + V 3 Y o 7 B 1 k l X k 7 K k K P R L 3 3 1 w o R l M U r D B N W 6 6 7 m p 8 S d U G c 4 E T o u 9 T G N K 2 Y g O s G u p p D F q f z I / d U r O r R K S K F G 2 p C F z 9 f f E h M Z a j + P A d s b U D P W y N x P / 8 7 q Z i a 7 9 C Z d p Z l C y x a I o E 8 Q k Z P Y 3 C b l C Z s T Y E s o U t 7 c S N q S K M m P T K d o Q v O W X V 0 m r W v E u K 9 X 7 W r l e y + M o w C m c w Q V 4 c A V 1 u I U G N I H B A J 7 h F d 4 c 4 b w 4 7 8 7 H o n X N y W d O 4 A + c z x 8 W R I 2 f < / l a t e x i t > . . . < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 e 3 5 2 g W f r l v f 1 6 w M E b X 2 S 1 Z U Q C Q = \" > A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B I v g q S R V U G 8 F L x 4 r 2 A 9 o Q 9 l s N u 3 a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o Z V S m K W t S J Z T u h M Q w w S V r I k f B O q l m J A k F a 4 e j 2 5 n f f m L a c C U f c J y y I C E D y W N O C V q p 1 R O R Q t M v V 7 y q N 4 e 7 S v y c V C B H o 1 / + 6 k W K Z g m T S A U x p u t 7 K Q Y T o p F T w a a l X m Z Y S u i I D F j X U k k S Z o L J / N q p e 2 a V y I 2 V t i X R n a u / J y Y k M W a c h L Y z I T g 0 y 9 5 M / M / r Z h h f B x M u 0 w y Z p I t F c S Z c V O 7 s d T f i m l E U Y 0 s I 1 d z e 6 t I h 0 Y S i D a h k Q / C X X 1 4 l r V r V v 6 j W 7 i 8 r 9 Z s 8 j i K c w C m c g w 9 X U I c 7 a E A T K D z C M 7 z C m 6 O c F + f d + V i 0 F p x 8 5 h j + w P n 8 A b q 4 j z M = < / l a t e x i t > d1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" x f 5 j g l o e i p z E z b . . . < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 e 3 5 2 g W f r l v f 1 6 w M E b X 2 S 1 Z U Q C Q = \" > A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B I v g q S R V U G 8 F L x 4 r 2 A 9 o Q 9 l s N u 3 a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0 s 7 u 3 j N z t U C c V 3 7 h E Y = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l q Q Y 8 F L x 4 r 2 g 9 o Q 9 l s t u 3 S z S b s T o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I J H C o O t + O 4 W N z a 3 t n e J u a W / / 4 P C o f H z S N n G q G W + x W M a 6 G 1 D D p V C 8 h Q I l 7 y a a 0 y i Q v B N M b u d + 5 4 l r I 2 L 1 i N O E + x E d K T E U j K K V H s K B N y h X 3 K q 7 A F k n X k 4 q k K M 5 K H / 1 w 5 i l E V f I J D W m 5 7 k J + h n V K J j k s 1 I / N T y h b E J H v G e p o h E 3 f r Y 4 d U Y u r B K S Y a x t K S Q L 9 f d E R i N j p l F g O y O K Y 7 P q z c X / v F 6 K w x s / E y p J k S u 2 X D R M J c G Y z P 8 m o d C c o Z x a Q p k W 9 l b C x l R T h j a d k g 3 B W 3 1 5 n b R r V e + q W r u v V x r 1 P I 4 i n M E 5 X I I H 1 9 C A O 2 h C C x i M 4 B l e 4 c 2 R z o v z 7 n w s W w t O P n M K f + B 8 / g D q Q Y 2 C < / l a t e x i t > d2 < l a t e x i t s h a 1 _ b a s e 6 4 = \" F v 7 l i 5 4 W D O 5 8 8 U B Z M 8 e W 5 Q H B X G Q = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l q Q Y 8 F L x 4 r 2 g 9 o Q 9 l s p u 3 S z S b s b o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 7 d z v P K H S P J a P Z p q g H 9 G R 5 E P O q L H S Q z i o D c o V t + o u Q N a J l 5 M K 5 G g O y l / 9 M G Z p h N I w Q b X u e W 5 i / I w q w 5 n A W a m f a k w o m 9 A R 9 i y V N E L t Z 4 t T Z + T C K i E Z x s q W N G S h / p 7 I a K T 1 N A p s Z 0 T N W K 9 6 c / E / r 5 e a 4 Y 2 f c Z m k B i V b L h q m g p i Y z P 8 m I V f I j J h a Q p n i 9 l b C x l R R Z m w 6 J R u C t / r y O m n X q t 5 V t X Z f r z T q e R x F O I N z u A Q P r q E B d 9 C E F j A Y w T O 8 w p s j n B f n 3 f l Y t h a c f O Y U / s D 5 / A H r x Y 2 D < / l a t e x i t > dN < l a t e x i t s h a 1 _ b a s e 6 4 = \" s 5 E Z Q S S l O i a F e 5 s r L h m U a j D J V c Q = \" > A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k t 6 L H g x Z N U t B / Q h r L Z T N q l m 0 3 Y 3 Q i l 9 C d 4 8 a C I V 3 + R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S q 4 N q 7 7 7 a y t b 2 x u b R d 2 i r t 7 + w e H p a P j l k 4 y x b D J E p G o T k A 1 C i 6 x a b g R 2 E k V 0 j g Q 2 A 5 G N z O / / Y R K 8 0 Q + m n G K f k w H k k e c U W O l h 7 B / 1 y + V 3 Y o 7 B 1 k l X k 7 K k K P R L 3 3 1 w o R l M U r D B N W 6 6 7 m p 8 S d U G c 4 E T o u 9 T G N K 2 Y g O s G u p p D F q f z I / d U r O r R K S K F G 2 p C F z 9 f f E h M Z a j + P A d s b U D P W y N x P / 8 7 q Z i a 7 9 C Z d p Z l C y x a I o E 8 Q k Z P Y 3 C b l C Z s T Y E s o U t 7 c S N q S K M m P T K d o Q v O W X V 0 m r W v E u K 9 X 7 v 5 B + f C o Z V S m K W t S J Z T u h M Q w w S V r I k f B O q l m J A k F a 4 e j 2 5 n f f m L a c C U f c J y y I C E D y W N O C V q p 1 R O R Q t M v V 7 y q N 4 e 7 S v y c V C B H o 1 / + 6 k W K Z g m T S A U x p u t 7 K Q Y T o p F T w a a l X m Z Y S u i I D F j X U k k S Z o L J / N q p e 2 a V y I 2 V t i X R n a u / J y Y k M W a c h L Y z I T g 0 y 9 5 M / M / r Z h h f B x M u 0 w y Z p I t F c S Z c V O 7 s d T f i m l E U Y 0 s I 1 d z e 6 t I h 0 Y S i D a h k Q / C X X 1 4 l r V r V v E Q F 9 V Y Q w a O C / Z C m h M 1 2 U h c 3 m 7 g 7 E U v o v / D i X / H i Q R G v e v P f u K 0 5 a P X B s o / 3 Z p i Z F 6 Z S G H T d T 2 d i c m p 6 Z n Z u v r S w u L S 8 U l 5 d a 5 g k 0 x z q P J G J b o X M g B Q K 6 i h Q Q i v V w O J Q Q j O 8 P h 7 6 z V v Q R i T q A v s p d G L W U y I S n K G V g n I 1 9 8 N E d k 0 / t h + N B g F S H 7 V g q i f h x l K 4 Q 8 T 8 R P H B 9 m W A O 0 G 5 4 l b d E e h f 4 h W k Q g q c B e U P v 5 v w L A a F X D J j 2 p 6 b Y i d n G g W X M C j 5 m Y G U 8 W v W g 7 a l i s V g O v n o r g H d s k q X R o m 2 T y E d q T 8 7 c h a b 4 e a 2 M m Z 4 Z c a 9 o f i f 1 8 4 w O u z k Q q U Z g j 1 t N C j K J M W E D k O i X a G B o + x b w r g W d l f K r 5 h m H G 2 U J R u C N 3 7 y X 9 L Y r X p 7 1 d 3 z / U r t q I h j j m y Q T b J N P H J A a u S U n J E 6 4 e S e P J J n 8 u I 8 O E / O q / P 2 X T r h F D 3 r 5 B e c 9 y / a 2 q B N < / l a t e x i t > CNN MLP w t < l a t e x i t s h a 1 _ b a s e 6 4 = \" q 3 U T 9 y t x i 6 J a G M 3 Y 4 F l 9 w H X p O G M = \" > A A A B 6 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I 6 q 3 o x W N F Y w t t K J v t p l 2 6 2 Y T d i V J C f 4 I X D y p e / U f e / D d u 2 x y 0 9 c H A 4 7 0 Z Z u a F q R Q G X f f b W V p e W V 1 b L 2 2 U N 7 e 2 d 3 Y r e / s P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W c x q H k z X B 4 P f G b j 1 w b k a h 7 H K U 8 i G l f i U g w i l a 6 e + p i t 1 J 1 a + 4 U Z J F 4 B a l C g U a 3 8 t X p J S y L u U I m q T F t z 0 0 x y K l G w S Q f l z u Z 4 S l l Q 9 r n b U s x 1 y S l m D u A P n M 8 f 3 T m N t g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" q 3 U T 9 y t x i 6 J a G M 3 Y 4 F l 9 w H X p O G M = \" > A A A B 6 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I 6 q 3 o x W N F Y w t t K J v t p l 2 6 2 Y T d i V J C f 4 I X D y p e / U f e / D d u 2 x y 0 9 c H A 4 7 0 Z Z u a F q R Q G X f f b W V p e W V 1 b L 2 2 U N 7 e 2 d 3 Y r e / s P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W c x q H k z X B 4 P f G b j 1 w b k a h 7 H K U 8 i G l f i U g w i l a 6 e + p i t 1 J 1 a + 4 U Z J F 4 B a l C g U a 3 8 t X p J S y L u U I m q T F t z 0 0 x y K l G w S Q f l z u Z 4 S l l Q 9 r n b U s x 1 y S l m D u A P n M 8 f 3 T m N t g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" q 3 U T 9 y t x i 6 J a G M 3 Y 4 F l 9 w H X p O G M = \" > A A A B 6 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I 6 q 3 o x W N F Y w t t K J v t p l 2 6 2 Y T d i V J C f 4 I X D y p e / U f e / D d u 2 x y 0 9 c H A 4 7 0 Z Z u a F q R Q G X f f b W V p e W V 1 b L 2 2 U N 7 e 2 d 3 Y r e / s P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W c x q H k z X B 4 P f G b j 1 w b k a h 7 H K U 8 i G l f i U g w i l a 6 e + p i t 1 J 1 a + 4 U Z J F 4 B a l C g U a 3 8 t X p J S y L u U I m q T F t z 0 0 x y K l G w S Q f l z u Z 4 S l l Q 9 r n b U s V j b k J 8 u m p Y 3 J s l R 6 J E m 1 L I Z m q v y d y G h s z i k P b G V M c m H l v I v 7 n t T O M L o J c q D R D r t h s U Z R J g g m Z / E 1 6 Q n O G c m Q J Z V r Y W w k b U E 0 Z 2 n T K N g R v / u V F 4 p / W L m v e 7 V m 1 f l W k U Y V j b k J 8 u m p Y 3 J s l R 6 J E m 1 L I Z m q v y d y G h s z i k P b G V M c m H l v I v 7 n t T O M L o J c q D R D r t h s U Z R J g g m Z / E 1 6 Q n O G c m Q J Z V r Y W w k b U E 0 Z 2 n T K N g R v / u V F 4 p / W L m v e 7 V m 1 f l W k U Y V j b k J 8 u m p Y 3 J s l R 6 J E m 1 L I Z m q v y d y G h s z i k P b G V M c m H l v I v 7 n t T O M L o J c q D R D r t h s U Z R J g g m Z / E 1 6 Q n O G c m Q J Z V r Y W w k b U E 0 Z 2 n T K N g R v / u V F 4 p / W L m v e 7 V m 1 f l W k U Y g J w = \" > A A A B 2 H i c b Z D N S g M x F I X v 1 L 8 6 V q 1 r N 8 E i u C o z b t S d 4 M Z l B c c W 2 q F k M n f a 0 E x m S O 4 I p f Q F X L h R f D B 3 v o 3 p z 0 K t B w I f 5 y T k 3 p O U S l o K g i + v t r W 9 s 7 t X 3 / c P G v 7 h 0 X G z 8 W S L y g i M R K E K 0 0 u 4 R S U 1 R i R J Y a 8 0 y P N E Y T e Z 3 C 3 y 7 j M a K w v 9 S N M S 4 5 y P t M y k 4 O S s z r D Z C t r B U m w T w j W 0 Y K 1 h 8 3 O Q F q L K U Z N Q 3 N p + G J Q U z 7 g h K R T O / U F l s e R i w k f Y d 6 h 5 j j a e L c e c s 3 P n p C w r j D u a 2 N L 9 + W L G c 2 u n e e J u 5 p z G 9 m + 2 M P / L + h V l 1 / F M 6 r I i 1 G L 1 U V Y p R g V b 7 M x S a V C Q m j r g w k g 3 K x N j b r g g 1 4 z v O g j / b r w J 0 W X 7 p h 0 + B F C H U z i D C w j h C m 7 h H j o Q g Y A U X u D N G 3 u v 3 v u q q p q 3 7 u w E f s n 7 + A a q K Y o N < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 8 L i q q k O r 4 y R e y v W h W M T n L M j k C 0 c = \" > A A A B 3 n i c b Z D N S g M x F I X v + F t r 1 e r W T b A I r s q M G 3 U n u H F Z 0 b G F d i i Z 9 E 4 b m s k M y R 2 l l D 6 C G x c q P p Y 7 3 8 b 0 Z 6 G t B w I f 5 y T k 3 h P n S l r y / W 9 v b X 1 j c 2 u 7 t F P e r e z t H 1 Q P K 4 8 2 K 4 z A U G Q q M 6 2 Y W 1 R S Y 0 i S F L Z y g z y N F T b j 4 c 0 0 b z 6 h s T L T D z T K M U p 5 X 8 t E C k 7 O u n / u U r d a 8 K e / V + 5 i 3 t e Y t a j u C P / I + f w D A + Y x i < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 8 T P j 8 p W 5 m g 7 r I V N u D P D 9 n l C 0 6 8 k = \" > A A A B 6 X i c b V B N T 8 J A E J 3 i F + I X 6 t H L R m L i i b R e 1 B v R i 0 e M V k i g I d t l C x u 2 2 2 Z 3 q i E N P 8 G L B z V e / U f e / D c u 0 I O C L 5 n k 5 b 2 Z z M w L U y k M u u 6 3 U 1 p Z X V v f K G 9 W t r Z 3 d v e q + w c P J s k 0 4 z 5 L Z K L b I T V c C s V 9 F C h 5 O 9 W c x q H k r X B 0 P f V b j 1 w b k a h 7 H K c 8 i O l A i U g w i l a 6 e + p h r 1 p z 6 + 4 M Z J l 4 B a l B g W a v + t X t J y y L u U I m q T E d z 0 + v + T G w V g g X U Y K F G t / r V 6 W W i S F G T U N z 0 x y K l G w S S f V L q Z 4 S l l I z r g H U s V j b k J 8 t m p E 3 J i l T 6 J E m 1 L I Z m p v y d y G h s z j k P b G V M c m k V v K v 7 n d T K M L o J c q D R D r t h 8 U Z R J g g m Z / k 3 6 Q n O G c m w J Z V r Y W w k b U k 0 Z 2 n Q q N g R v 8 e V l 4 p / V L + v e r V t r X B V p l O E I j u E U P D i H B t x A E 3 x g M I B n e I U 3 R z o v z r v z M W 8 t O c X M I f y B 8 / k D 2 / m N s g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" q 3 U T 9 y t x i 6 J a G M 3 Y 4 F l 9 w H X p O G M = \" > A A A B 6 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I 6 q 3 o x W N F Y w t t K J v t p l 2 6 2 Y T d i V J C f 4 I X D y p e / U f e / D d u 2 x y 0 9 c H A 4 7 0 Z Z u a F q R Q G X f f b W V p e W V 1 b L 2 2 U N 7 e 2 d 3 Y r e / s P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W c x q H k z X B 4 P f G b j 1 w b k a h 7 H K U 8 i G l f i U g w i l a 6 e + p i t 1 J 1 a + 4 U Z J F 4 B a l C g U a 3 8 t X p J S y L u U I m q T F t z 0 0 x y K l G w S Q f l z u Z 4 S l l Q 9 r n b U s x 1 y S l m D u A P n M 8 f 3 T m N t g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" q 3 U T 9 y t x i 6 J a G M 3 Y 4 F l 9 w H X p O G M = \" > A A A B 6 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I 6 q 3 o x W N F Y w t t K J v t p l 2 6 2 Y T d i V J C f 4 I X D y p e / U f e / D d u 2 x y 0 9 c H A 4 7 0 Z Z u a F q R Q G X f f b W V p e W V 1 b L 2 2 U N 7 e 2 d 3 Y r e / s P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W c x q H k z X B 4 P f G b j 1 w b k a h 7 H K U 8 i G l f i U g w i l a 6 e + p i t 1 J 1 a + 4 U Z J F 4 B a l C g U a 3 8 t X p J S y L u U I m q T F t z 0 0 x y K l G w S Q f l z u Z 4 S l l Q 9 r n b U s x 1 y S l m D u A P n M 8 f 3 T m N t g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" q 3 U T 9 y t x i 6 J a G M 3 Y 4 F l 9 w H X p O G M = \" > A A A B 6 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I 6 q 3 o x W N F Y w t t K J v t p l 2 6 2 Y T d i V J C f 4 I X D y p e / U f e / D d u 2 x y 0 9 c H A 4 7 0 Z Z u a F q R Q G X f f b W V p e W V 1 b L 2 2 U N 7 e 2 d 3 Y r e / s P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W c x q H k z X B 4 P f G b j 1 w b k a h 7 H K U 8 i G l f i U g w i l a 6 e + p i t 1 J 1 a + 4 U Z J F 4 B a l C g U a 3 8 t X p J S y L u U I m q T F t z 0 0 x y K l G w S Q f l z u Z 4 S l l Q 9 r n b U s x 1 y S l m D u A P n M 8 f 3 T m N t g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" q 3 U T 9 y t x i 6 J a G M 3 Y 4 F l 9 w H X p O G M = \" > A A A B 6 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I 6 q 3 o x W N F Y w t t K J v t p l 2 6 2 Y T d i V J C f 4 I X D y p e / U f e / D d u 2 x y 0 9 c H A 4 7 0 Z Z u a F q R Q G X f f b W V p e W V 1 b L 2 2 U N 7 e 2 d 3 Y r e / s P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W c x q H k z X B 4 P f G b j 1 w b k a h 7 H K U 8 i G l f i U g w i l a 6 e + p i t 1 J 1 a + 4 U Z J F 4 B a l C g U a 3 8 t X p J S y L u U I m q T F t z 0 0 V j b k J 8 u m p Y 3 J s l R 6 J E m 1 L I Z m q v y d y G h s z i k P b G V M c m H l v I v 7 n t T O M L o J c q D R D r t h s U Z R J g g m Z / E 1 6 Q n O G c m Q J Z V r Y W w k b U E 0 Z 2 n T K N g R v / u V F 4 p / W L m v e 7 V m 1 f l W k U Y V j b k J 8 u m p Y 3 J s l R 6 J E m 1 L I Z m q v y d y G h s z i k P b G V M c m H l v I v 7 n t T O M L o J c q D R D r t h s U Z R J g g m Z / E 1 6 Q n O G c m Q J Z V r Y W w k b U E 0 Z 2 n T K N g R v / u V F 4 p / W L m v e 7 V m 1 f l W k U Y V j b k J 8 u m p Y 3 J s l R 6 J E m 1 L I Z m q v y d y G h s z i k P b G V M c m H l v I v 7 n t T O M L o J c q D R D r t h s U Z R J g g m Z / E 1 6 Q n O G c m Q J Z V r Y W w k b U E 0 Z 2 n T K N g R v / u V F 4 p / W L m v e 7 V m 1 f l W k U Y x y K l G w S Q f l z u Z 4 S l l Q 9 r n b U s V j b k J 8 u m p Y 3 J s l R 6 J E m 1 L I Z m q v y d y G h s z i k P b G V M c m H l v I v 7 n t T O M L o J c q D R D r t h s U Z R J g g m Z / E 1 6 Q n O G c m Q J Z V r Y W w k b U E 0 Z 2 n T K N g R v / u V F 4 p / W L m v e 7 V m 1 f l W k U Y J D O I I T 8 O A c 6 n A D D f C B Q R + e 4 R X e H O m 8 O O / O x 6 x 1 y S l m D u A P n M 8 f 3 T m N t g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" q 3 U T 9 y t x i 6 J a G M 3 Y 4 F l 9 w H X p O G M = \" > A A A B 6 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I 6 q 3 o x W N F Y w t t K J v t p l 2 6 2 Y T d i V J C f 4 I X D y p e / U f e / D d u 2 x y 0 9 c H A 4 7 0 Z Z u a F q R Q G X f f b W V p e W V 1 b L 2 2 U N 7 e 2 d 3 Y r e / s P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W c x q H k z X B 4 P f G b j 1 w b k a h 7 H K U 8 i G l f i U g w i l a 6 e + p i t 1 J 1 a + 4 U Z J F 4 B a l C g U a 3 8 t X p J S y L u U I m q T F t z 0 0 x y K l G w S Q f l z u Z 4 S l l Q 9 r n b U s V j b k J 8 u m p Y 3 J s l R 6 J E m 1 L I Z m q v y d y G h s z i k P b G V M c m H l v I v 7 n t T O M L o J c q D R D r t h s U Z R J g g m Z / E 1 6 Q n O G c m Q J Z V r Y W w k b U E 0 Z 2 n T K N g R v / u V F 4 p / W L m v e 7 V m 1 f l W k U Y J D O I I T 8 O A c 6 n A D D f C B Q R + e 4 R X e H O m 8 O O / O x 6 x 1 y S l m D u A P n M 8 f 3 T m N t g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" q 3 U T 9 y t x i 6 J a G M 3 Y 4 F l 9 w H X p O G M = \" > A A A B 6 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I 6 q 3 o x W N F Y w t t K J v t p l 2 6 2 Y T d i V J C f 4 I X D y p e / U f e / D d u 2 x y 0 9 c H A 4 7 0 Z Z u a F q R Q G X f f b W V p e W V 1 b L 2 2 U N 7 e 2 d 3 Y r e / s P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W c x q H k z X B 4 P f G b j 1 w b k a h 7 H K U 8 i G l f i U g w i l a 6 e + p i t 1 J 1 a + 4 U Z J F 4 B a l C g U a 3 8 t X p J S y L u U I m q T F t z 0 0 R 3 i G V 3 i z n q w X 6 9 3 6 m I 9 W r H L n A P 7 A + v w B y q i T 5 A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" U 0 R 3 i G V 3 i z n q w X 6 9 3 6 m I 9 W r H L n A P 7 A + v w B y q i T 5 A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" U 0 R 3 i G V 3 i z n q w X 6 9 3 6 m I 9 W r H L n A P 7 A + v w B y q i T 5 A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" X / B b P x y K l G w S Q f l z u Z 4 S l l Q 9 r n b U s V j b k J 8 u m p Y 3 J s l R 6 J E m 1 L I Z m q v y d y G h s z i k P b G V M c m H l v I v 7 n t T O M L o J c q D R D r t h s U Z R J g g m Z / E 1 6 Q n O G c m Q J Z V r Y W w k b U E 0 Z 2 n T K N g R v / u V F 4 p / W L m v e 7 V m 1 f l W k U Y g M h Y y t y h J 9 O i N I M Q F 4 c M z J C Y s = \" > A A A B + X i c b V C 9 T s M w G P x S / k r 5 S 2 F k s a i Q m K o E I Q F b B Q t j k Q i t 1 E S V 4 7 i t V c e J b A d U h T 4 K C w M g V t 6 E j b f B a T N A y 0 m W T 3 f f J 5 8 v T D l T 2 n G + r c r K 6 t r 6 R n W z t r W 9 s 7 t n 1 / f v V Z J J Q j 2 S 8 E R 2 Q 6 w o Z 4 J 6 m m l O u 6 m k O A 4 5 7 Y T j 6 8 L v P F C p W C L u 9 C S l Q Y y H g g 0 Y w d p I f b v u 5 3 6 Y 8 E h N Y n O h r j / t 2 w 2 n 6 c y A l o l b k g a U a P f t L z 9 K S B Z T o Q n H S v V c J 9 V B j q V m h N N g M h Y y t y h J 9 O i N I M Q F 4 c M z J C Y s = \" > A A A B + X i c b V C 9 T s M w G P x S / k r 5 S 2 F k s a i Q m K o E I Q F b B Q t j k Q i t 1 E S V 4 7 i t V c e J b A d U h T 4 K C w M g V t 6 E j b f B a T N A y 0 m W T 3 f f J 5 8 v T D l T 2 n G + r c r K 6 t r 6 R n W z t r W 9 s 7 t n 1 / f v V Z J J Q j 2 S 8 E R 2 Q 6 w o Z 4 J 6 m m l O u 6 m k O A 4 5 7 Y T j 6 8 L v P F C p W C L u 9 C S l Q Y y H g g 0 Y w d p I f b v u 5 3 6 Y 8 E h N Y n O h r j / t 2 w 2 n 6 c y A l o l b k g a U a P f t L z 9 K S B Z T o Q n H S v V c J 9 V B j q V m h N N g M h Y y t y h J 9 O i N I M Q F 4 c M z J C Y s = \" > A A A B + X i c b V C 9 T s M w G P x S / k r 5 S 2 F k s a i Q m K o E I Q F b B Q t j k Q i t 1 E S V 4 7 i t V c e J b A d U h T 4 K C w M g V t 6 E j b f B a T N A y 0 m W T 3 f f J 5 8 v T D l T 2 n G + r c r K 6 t r 6 R n W z t r W 9 s 7 t n 1 / f v V Z J J Q j 2 S 8 E R 2 Q 6 w o Z 4 J 6 m m l O u 6 m k O A 4 5 7 Y T j 6 8 L v P F C p W C L u 9 C S l Q Y y H g g 0 Y w d p I f b v u 5 3 6 Y 8 E h N Y n O h r j / t 2 w 2 n 6 c y A l o l b k g a U a P f t L z 9 K S B Z T o Q n H S v V c J 9 V B j q V m h N N P Q R M 1 p m B h x d K 1 e n S b L + g J w = \" > A A A B 2 H i c b Z D N S g M x F I X v 1 L 8 6 V q 1 r N 8 E i u C o z b t S d 4 M Z l B c c W 2 q F k M n f a 0 E x m S O 4 I p f Q F X L h R f D B 3 v o 3 p z 0 K t B w I f 5 y T k 3 p O U S l o K g i + v t r W 9 s 7 t X 3 / c P G v 7 h 0 X G z 8 W S L y g i M R K E K 0 0 u 4 R S U 1 R i R J Y a 8 0 y P N E Y T e Z 3 C 3 y 7 j M a K w v 9 S N M S 4 5 y P t M y k 4 O S s z r D Z C t r B U m w T w j W 0 Y K 1 h 8 3 O Q F q L K U Z N Q 3 N p + G J Q U z 7 g h K R T O / U F l s e R i w k f Y d 6 h 5 j j a e L c e c s 3 P n p C w r j D u a 2 N L 9 + W L G c 2 u n e e J u 5 p z G 9 R 3 i G V 3 i z n q w X 6 9 3 6 m I 9 W r H L n A P 7 A + v w B y q i T 5 A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" U 0 R 3 i G V 3 i z n q w X 6 9 3 6 m I 9 W r H L n A P 7 A + v w B y q i T 5 A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" U 0 R 3 i G V 3 i z n q w X 6 9 3 6 m I 9 W r H L n A P 7 A + v w B y q i T 5 A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" U 0 R 3 i G V 3 i z n q w X 6 9 3 6 m I 9 W r H L n A P 7 A + v w B y q i T 5 A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" U 0 R 3 i G V 3 i z n q w X 6 9 3 6 m I 9 W r H L n A P 7 A + v w B y q i T 5 A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" U 0 R 3 i G V 3 i z n q w X 6 9 3 6 m I 9 W r H L n A P 7 A + v w B y q i T 5 A = = < / l a t e x i t > y 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" J R + a W z 1 b K b 9 6 X A l h t 4 q X n K 5 V 3 S k = \" < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 G q 4 R 7 h 6 x w e U t W p N B q A 5 . . . m + 2 M P / L + h V l 1 / F M 6 r I i 1 G L 1 U V Y p R g V b 7 M x S a V C Q m j r g w k g 3 K x N j b r g g 1 4 z v O g j / b r w J 0 W X 7 p h 0 + B F C H U z i D C w j h C m 7 h H j o Q g Y A U X u D N G 3 u v 3 v u q q p q 3 7 u w E f s n 7 + A a q K Y o N < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" k v u 1 4 B G D B I h R l F R I G u 9 Z 3 v M Y U y g = \" > A A A B 7 n i c b V C 9 T s M w G P x S / k o p k L K y W F R I T F X C A m x I L I x F I r R S E 1 W O 4 7 R W H T u y H V A V + i g s D I B 4 H D b e B q f t A C 0 n f f L p z p b v u z j n T B v P + 3 Z q G 5 t b 2 z v 1 3 c Z e c / / g 0 G 0 1 H 7 Q s F K E B k V y q f o w 1 5 U z Q w D D D a T 9 X F G c x p 7 1 4 c l P 5 v U e q N J P i 3 k x z G m V 4 J F j K C D Z W G r q t s A x j y R M 9 z e y B + u F s 6 L a 9 j j c H W i f + k r R h i e 7 Q / Q o T S Y q M C k M 4 1 n r g e 7 m J S q w M I 5 z O G m G h a Y 7 J B I / o w F K B M 6 q j c h 5 9 h k 6 t k q B U K j v C o L n 6 + 0 W J M 1 1 l s z c z b M Z 6 1 a v E / 7 x B Y d L L q G Q i L w w V Z P F R W n B k J K p 6 Q A l T l B g + t Q Q T x W x W R M Z Y Y W J s W w 1 b g r + 6 8 j o J z j t X H f / O g z o c w w m c g Q 8 X c A 2 3 0 I U A C D z B C 7 z B u / P s v D o f i 7 Z q z r K 2 I / g D 5 / M H b U 6 S g A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" k v u 1 4 B G D B I h R l F R I G u 9 Z 3 v M Y U y g = \" > A A A B 7 n i c b V C 9 T s M w G P x S / k o p k L K y W F R I T F X C A m x I L I x F I r R S E 1 W O 4 7 R W H T u y H V A V + i g s D I B 4 H D b e B q f t A C 0 n f f L p z p b v u z j n T B v P + 3 Z q G 5 t b 2 z v 1 3 c Z e c / / g 0 G 0 1 H 7 Q s F K E B k V y q f o w 1 5 U z Q w D D D a T 9 X F G c x p 7 1 4 c l P 5 v U e q N J P i 3 k x z G m V 4 J F j K C D Z W G r q t s A x j y R M 9 z e y B + u F s 6 L a 9 j j c H W i f + k r R h i e 7 Q / Q o T S Y q M C k M 4 1 n r g e 7 m J S q w M I 5 z O G m G h a Y 7 J B I / o w F K B M 6 q j c h 5 9 h k 6 t k q B U K j v C o L n 6 + 0 W J M 1 1 l s z c z b M Z 6 1 a v E / 7 x B Y d L L q G Q i L w w V Z P F R W n B k J K p 6 Q A l T l B g + t Q Q T x W x W R M Z Y Y W J s W w 1 b g r + 6 8 j o J z j t X H f / O g z o c w w m c g Q 8 X c A 2 3 0 I U A C D z B C 7 z B u / P s v D o f i 7 Z q z r K 2 I / g D 5 / M H b U 6 S g A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" S d O L O h y v h a t 7 G h d S U z X L f g 4 p i J 4 = \" > A A A B + X i c b V C 9 T s M w G P z C b y l / K Y w s F h U S U 5 W w A F s F C 2 O R C K 3 U R J X j O K 1 V J 4 5 s B 1 S F P g o L A y B W 3 o S N t 8 F p M 0 D L S Z Z P d 9 8 n n y / M O F P a c b 6 t l d W 1 9 Y 3 N 2 l Z 9 e 2 d 3 b 9 9 u H N w r k U t C P S K 4 k L 0 Q K 8 p Z S j 3 N N K e 9 T F K c h J x 2 w / F 1 6 X c f q F R M p H d 6 k t E g w c O U x Y x g b a S B 3 f A L P x Q 8 U p P E X K j n T w d 2 0 2 k 5 M 6 B l 4 l a k C R U 6 A / v L j w T J E 5 p q w r F S f d f J d F B g q R n h d F r 3 c 0 U z T M Z 4 S P u G p j i h K i h m 0 a f o x C g R i o U 0 J 9 V o p v 7 e K H C i y m x m M s F 6 p B a 9 U v z P 6 + c 6 v g g K l m a 5 p i m Z P x T n H G m B y h 5 Q x C Q l m k 8 M w U Q y k x W R E Z a Y a N N W 3 Z T g L n 5 5 m X h n r c u W e + s 0 2 1 d V G z U 4 g m M 4 B R f O o Q 0 3 0 A E P C D z C M 7 z C m / V k v V j v 1 s d 8 d M W q d g 7 h D 6 z P H 8 l o k + A = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" U 0 g M h Y y t y h J 9 O i N I M Q F 4 c M z J C Y s = \" > A A A B + X i c b V C 9 T s M w G P x S / k r 5 S 2 F k s a i Q m K o E I Q F b B Q t j k Q i t 1 E S V 4 7 i t V c e J b A d U h T 4 K C w M g V t 6 E j b f B a T N A y 0 m W T 3 f f J 5 8 v T D l T 2 n G + r c r K 6 t r 6 R n W z t r W 9 s 7 t n 1 / f v V Z J J Q j 2 S 8 E R 2 Q 6 w o Z 4 J 6 m m l O u 6 m k O A 4 5 7 Y T j 6 8 L v P F C p W C L u 9 C S l Q Y y H g g 0 Y w d p I f b v u 5 3 6 Y 8 E h N Y n O h r j / t 2 w 2 n 6 c y A l o l b k g a U a P f t L z 9 K S B Z T o Q n H S v V c J 9 V B j q V m h N N g M h Y y t y h J 9 O i N I M Q F 4 c M z J C Y s = \" > A A A B + X i c b V C 9 T s M w G P x S / k r 5 S 2 F k s a i Q m K o E I Q F b B Q t j k Q i t 1 E S V 4 7 i t V c e J b A d U h T 4 K C w M g V t 6 E j b f B a T N A y 0 m W T 3 f f J 5 8 v T D l T 2 n G + r c r K 6 t r 6 R n W z t r W 9 s 7 t n 1 / f v V Z J J Q j 2 S 8 E R 2 Q 6 w o Z 4 J 6 m m l O u 6 m k O A 4 5 7 Y T j 6 8 L v P F C p W C L u 9 C S l Q Y y H g g 0 Y w d p I f b v u 5 3 6 Y 8 E h N Y n O h r j / t 2 w 2 n 6 c y A l o l b k g a U a P f t L z 9 K S B Z T o Q n H S v V c J 9 V B j q V m h N N g M h Y y t y h J 9 O i N I M Q F 4 c M z J C Y s = \" > A A A B + X i c b V C 9 T s M w G P x S / k r 5 S 2 F k s a i Q m K o E I Q F b B Q t j k Q i t 1 E S V 4 7 i t V c e J b A d U h T 4 K C w M g V t 6 E j b f B a T N A y 0 m W T 3 f f J 5 8 v T D l T 2 n G + r c r K 6 t r 6 R n W z t r W 9 s 7 t n 1 / f v V Z J J Q j 2 S 8 E R 2 Q 6 w o Z 4 J 6 m m l O u 6 m k O A 4 5 7 Y T j 6 8 L v P F C p W C L u 9 C S l Q Y y H g g 0 Y w d p I f b v u 5 3 6 Y 8 E h N Y n O h r j / t 2 w 2 n 6 c y A l o l b k g a U a P f t L z 9 K S B Z T o Q n H S v V c J 9 V B j q V m h N N g M h Y y t y h J 9 O i N I M Q F 4 c M z J C Y s = \" > A A A B + X i c b V C 9 T s M w G P x S / k r 5 S 2 F k s a i Q m K o E I Q F b B Q t j k Q i t 1 E S V 4 7 i t V c e J b A d U h T 4 K C w M g V t 6 E j b f B a T N A y 0 m W T 3 f f J 5 8 v T D l T 2 n G + r c r K 6 t r 6 R n W z t r W 9 s 7 t n 1 / f v V Z J J Q j 2 S 8 E R 2 Q 6 w o Z 4 J 6 m m l O u 6 m k O A 4 5 7 Y T j 6 8 L v P F C p W C L u 9 C S l Q Y y H g g 0 Y w d p I f b v u 5 3 6 Y 8 E h N Y n O h r j / t 2 w 2 n 6 c y A l o l b k g a U a P f t L z 9 K S B Z T o Q n H S v V c J 9 V B j q V g M h Y y t y h J 9 O i N I M Q F 4 c M z J C Y s = \" > A A A B + X i c b V C 9 T s M w G P x S / k r 5 S 2 F k s a i Q m K o E I Q F b B Q t j k Q i t 1 E S V 4 7 i t V c e J b A d U h T 4 K C w M g V t 6 E j b f B a T N A y 0 m W T 3 f f J 5 8 v T D l T 2 n G + r c r K 6 t r 6 R n W z t r W 9 s 7 t n 1 / f v V Z J J Q j 2 S 8 E R 2 Q 6 w o Z 4 J 6 m m l O u 6 m k O A 4 5 7 Y T j 6 8 L v P F C p W C L u 9 C S l Q Y y H g g 0 Y w d p I f b v u 5 3 6 Y 8 E h N Y n O h r j / t 2 w 2 n 6 c y A l o l b k g a U a P f t L z 9 K S B Z T o Q n H S v V c J 9 V B j q V g M h Y y t y h J 9 O i N I M Q F 4 c M z J C Y s = \" > A A A B + X i c b V C 9 T s M w G P x S / k r 5 S 2 F k s a i Q m K o E I Q F b B Q t j k Q i t 1 E S V 4 7 i t V c e J b A d U h T 4 K C w M g V t 6 E j b f B a T N A y 0 m W T 3 f f J 5 8 v T D l T 2 n G + r c r K 6 t r 6 R n W z t r W 9 s 7 t n 1 / f v V Z J J Q j 2 S 8 E R 2 Q 6 w o Z 4 J 6 m m l O u 6 m k O A 4 5 7 Y T j 6 8 L v P F C p W C L u 9 C S l Q Y y H g g 0 Y w d p I f b v u 5 3 6 Y 8 E h N Y n O h r j / t 2 w 2 n 6 c y A l o l b k g a U a P f t L z 9 K S B Z T o Q n H S v V c J 9 V B j q V > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o N 4 K X j x W t B / Q h r L Z b t q l m 0 3 Y n Q i h 9 C d 4 8 a C I V 3 + R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K F Q d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o Z e J U M 9 5 k s Y x 1 J 6 C G S 6 F 4 E w V K 3 k k 0 p 1 E g e T s Y 3 8 7 8 9 h P X R s T q E b O E + x E d K h E K R t F K D 1 n f 6 5 c r b t W d g 6 w S L y c V y N H o l 7 9 6 g 5 i l E V f I J D W m 6 7 k J + h O q U T D J p 6 V e a n h C 2 Z g O e d d S R S N u / M n 8 1 C k 5 s 8 q A h L G 2 p Z D M 1 d 8 T E x o Z k 0 W B 7 Y w o j s y y N x P / 8 7 o p h t f + R K g k R a 7 Y Y l G Y S o I x m f 1 N B k J z h j K z h D I t 7 K 2 E j a i m D G 0 6 J R u C t / z y K m n V q t 5 F t X Z / W a n f V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o N 4 K X j x W t B / Q h r L Z b t q l m 0 3 Y n Q i h 9 C d 4 8 a C I V 3 + R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K F Q d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o Z e J U M 9 5 k s Y x 1 J 6 C G S 6 F 4 E w V K 3 k k 0 p 1 E g e T s Y 3 8 7 8 9 h P X R s T q E b O E + x E d K h E K R t F K D 1 m / 1 i 9 X 3 K o 7 B 1 k l X k 4 q k K P R L 3 / 1 B j F L I 6 6 Q S W p M 1 3 M T 9 C d U o 2 C S T 0 u 9 1 P C E s j E d 8 q 6 l i k b c + J P 5 q V N y Z p U B C W N t S y G Z q 7 8 n J j Q y J o s C 2 x l R H J l l b y b + 5 3 V T D K / 9 i V B J i l y x x a I w l Q R j M v u b D I T m D G V m C W V a 2 F s J G 1 F N G d p 0 S j Y E b / n l V d K q V V P E x B C Y = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o N 4 K X j x W t B / Q h r L Z b t q l m 0 3 Y n Q i h 9 C d 4 8 a C I V 3 + R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K F Q d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o Z e J U M 9 5 k s Y x 1 J 6 C G S 6 F 4 E w V K 3 k k 0 p 1 E g e T s Y 3 8 7 8 9 h P X R s T q E b O E + x E d K h E K R t F K D 1 k f + + W K W 3 X n I K v E y 0 k F c j T 6 5 a / e I G Z p x B U y S Y 3 p e m 6 C / o R q F E z y a a m X G p 5 Q N q Z D 3 r V U 0 Y g b f z I / d U r O r D I g Y a x t K S R z 9 f f E h E b G Z F F g O y O K I 7 P s z c T / v G 6 K 4 b U / E S p J k S < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 e 3 5 2 g W f r l v f 1 6 w M E b X 2 S 1 Z U Q C Q = \" > A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B I v g q S R V U G 8 F L x 4 r 2 A 9 o Q 9 l s N u 3 a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o Z V S m K W t S J Z T u h M Q w w S V r I k f B O q l m J A k F a 4 e j 2 5 n f f m L a c C U f c J y y I C E D y W N O C V q p 1 R O R Q t M v V 7 y q N 4 e 7 S v y c V C B H o 1 / + 6 k W K Z g m T S A U x p u t 7 K Q Y T o p F T w a a l X m Z Y S u i I D F j X U k k S Z o L J / N q p e 2 a V y I 2 V t i X R n a u / J y Y k M W a c h L Y z I T g 0 y 9 5 M / M / r Z h h f B x M u 0 w y Z p I t F c S Z c V O 7 s d T f i m l E U Y 0 s I 1 d z e 6 t I h 0 Y S i D a h k Q / C X X 1 4 l r V r V v 6 j W 7 i 8 r 9 Z s 8 j i K c w C m c g w 9 X U I c 7 a E A T K D z C M 7 z C m 6 O c F + f d + V i 0 F p x 8 5 h j + w P n 8 A b q 4 j z M = < / l a t e x i t > . . . < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 e 3 5 2 g W f r l v f 1 6 w M E b X 2 S 1 Z U Q C Q = \" > A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B I v g q S R V U G 8 F L x 4 r 2 A 9 o Q 9 l s N u 3 a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o Z V S m K W t S J Z T u h M Q w w S V r I k f B O q l m J A k F a 4 e j 2 5 n f f m L a c C U f c J y y I C E D y W N O C V q p 1 R O R Q t M v V 7 y q N 4 e 7 S v y c V C B H o 1 / + 6 k W K Z g m T S A U x p u t 7 K Q Y T o p F T w a a l X m Z Y S u i I D F j X U k k S Z o L J / N q p e 2 a V y I 2 V t i X R n a u / J y Y k M W a c h L Y z I T g 0 y 9 5 M / M / r Z h h f B x M u 0 w y Z p I t F c S Z c V O 7 s d T f i m l E U Y 0 s I 1 d z e 6 t I h 0 Y S i D a h k Q / C X X 1 4 l r V r V v 6 j W 7 i 8 r 9 Z s 8 j i K c w C m c g w 9 X U I c 7 a E A T K D z C M 7 z C m 6 O c F + f d + V i 0 F p x 8 5 h j + w P n 8 A b q 4 j z M = < / l a t e x i t > . . . < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 e 3 5 2 g W f r l v f 1 6 w M E b X 2 S 1 Z U Q C Q = \" > A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B I v g q S R V U G 8 F L x 4 r 2 A 9 o Q 9 l s N u 3 a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o Z V S m K W t S J Z T u h M Q w w S V r I k f B O q l m J A k F a 4 e j 2 5 n f f m L a c C U f c J y y I C E D y W N O C V q p 1 R O R Q t M v V 7 y q N 4 e 7 S v y c V C B H o 1 / + 6 k W K Z g m T S A U x p u t 7 K Q Y T o p F T w a a l X m Z Y S u i I D F j X U k k S Z o L J / N q p e 2 a V y I 2 V t i X R n a u / J y Y k M W a c h L Y z I T g 0 y 9 5 M / M / r Z h h f B x M u 0 w y Z p I t F c S Z c V O 7 s d T f i m l E U Y 0 s I 1 d z e 6 t I h 0 Y S i D a h k Q / C X X 1 4 l r V r V v 6 j W 7 i 8 r 9 Z s 8 j i K c w C m c g w 9 X U I c 7 a E A T K D z C M 7 z C m 6 O c F + f d + V i 0 F p x 8 5 h j + w P n 8 A b q 4 j z M = < / l a t e x i t > . . . < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 e 3 5 2 g W f r l v f 1 6 w M E b X 2 S 1 Z U Q C Q = \" > A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B I v g q S R V U G 8 F L x 4 r 2 A 9 o Q 9 l s N u 3 a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o Z V S m K W t S J Z T u h M Q w w S V r I k f B O q l m J A k F a 4 e j 2 5 n f f m L a c C U f c J y y I C E D y W N O C V q p 1 R O R Q t M v V 7 y q N 4 e 7 S v y c V C B H o 1 / + 6 k W K Z g m T S A U x p u t 7 K Q Y T o p F T w a a l X m Z Y S u i I D F j X U k k S Z o L J / N q p e 2 a V y I 2 V t i X R n a u / J y Y k M W a c h L Y z I T g 0 y 9 5 M / M / r Z h h f B x M u 0 w y Z p I t F c S Z c V O 7 s d T f i m l E U Y 0 s I 1 d z e 6 t I h 0 Y S i D a h k Q / C X X 1 4 l r V r V v 6 j W 7 i 8 r 9 Z s 8 j i K c w C m c g w 9 X U I c 7 a E A T K D z C M 7 z C m 6 O c F + f d + V i 0 F p x 8 5 h j + w P n 8 A b q 4 j z M = < / l a t e x i t > . . . < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 e 3 5 2 g W f r l v f 1 6 w M E b X 2 S 1 Z U Q C Q = \" > A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B I v g q S R V U G 8 F L x 4 r 2 A 9 o Q 9 l s N u 3 a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o Z V S m K W t S J Z T u h M Q w w S V r I k f B O q l m J A k F a 4 e j 2 5 n f f m L a c C U f c J y y I C E D y W N O C V q p 1 R O R Q t M v V 7 y q N 4 e 7 S v y c V C B H o 1 / + 6 k W K Z g m T S A U x p u t 7 K Q Y T o p F T w a a l X m Z Y S u i I D F j X U k k S Z o L J / N q p e 2 a V y I 2 V t i X R n a u / J y Y k M W a c h L Y z I T g 0 y 9 5 M / M / r Z h h f B x M u 0 w y Z p I t F c S Z c V O 7 s d T f i m l E U Y 0 s I 1 d z e 6 t I h 0 Y S i D a h k Q / C X X 1 4 l r V r V v v S B h V 2 n G + r b X 1 j c 2 t 7 c p O d X d v / + D Q P j r u K Z F K T L p Y M C E H A V K E U U 6 6 m m p G B o k k K A 4 Y 6 Q f T 6 8 L v P x K p q O D 3 O k u I F 6 M x p x H F S B v J t 2 v 5 K B A s V F l s L q h m v n 6 4 8 e 2 6 0 3 D m g K v E L U k d l O j 4 9 t c o F D i N C d e Y I a W G r p N o L 0 d S U 8 z I r D p K F U k Q n q I x G R r K U U y U l 8 / D z + C Z U U I Y C W k O 1 3 C u / t 7 I U a y K e G Y y R n q i l r 1 C / M 8 b p j q 6 8 n L K k 1 Q T j h c P R S m D W s C i C R h S S b B m m S E I S 2 q y Q j x B E m F t + q q a E t z l L 6 + S X r P h X j S a d 6 1 6 u 1 X W U Q E n 4 B S c A x d c g j a 4 B R 3 Q B R h k 4 B m 8 g j f r y X q x 3 q 2 P x e i a V e 7 U w B 9 Y n z 8 F g Z T 1 < / l a t e x i t > s G 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" l c G u L g G f K D o + l T 5 W 7 Y v r A Q H M V G I = \" > A A A B / H i c b V D N S 8 M w H E 3 9 n P O r u q O X 4 B A 8 j X Y O 9 D j w o M c J 7 g O 2 W t I 0 3 c L S p C S p U M r 8 V 7 x 4 U M S r f 4 g 3 / x v T r Q f d f B D y e O / 3 I y 8 v S B h V 2 n G + r b X 1 j c 2 t 7 c p O d X d v / + D Q P j r u K Z F K T L p Y M C E H A V K E U U 6 6 m m p G B o k k K A 4 Y 6 Q f T 6 8 L v P x K p q O D 3 O k u I F 6 M x p x H F S B v J t 2 v 5 K B A s V F l s L q h m v v t w 4 9 t 1 p + H M A V e J W 5 I 6 K N H x 7 a 9 R K H A a E 6 4 x Q 0 o N X S f R X o 6 k p p i R W X W U K p I g P E V j M j S U o 5 g o L 5 + H n 8 E z o 4 Q w E t I c r u F c / b 2 R o 1 g V 8 c x k j P R E L X u F + J 8 3 T H V 0 5 e W U J 6 k m H C 8 e i l I G t Y B F E z C k k m D N M k M Q l t R k h X i C J M L a 9 F U 1 J b j L X 1 4 l v W b D v W g 0 7 1 r 1 d q u s o w J O w C k 4 B y 6 4 B G 1 w C z q g C z D I w D N 4 B W / W k / V i v V v S B h V 2 n G + r b X 1 j c 2 t 7 c p O d X d v / + D Q P j r u K Z F K T L p Y M C E H A V K E U U 6 6 m m p G B o k k K A 4 Y 6 Q f T 6 8 L v P x K p q O D 3 O k u I F 6 M x p x H F S B v J t 2 v 5 K B A s V F l s L q h m f v P h x r f r T s O Z A 6 4 S t y R 1 U K L j 2 1 + j U O A 0 J l x j h p Q a u k 6 i v R x J T T E j s + o o V S R B e I r G Z G g o R z F R X j 4 P P 4 N n R g l h J K Q 5 X M O 5 + n s j R 7 E q 4 p n J G O m J W v Y K 8 T 9 v m O r o y s s p T 1 J N O F 4 8 F K U M a g G L J m B I J c G a Z Y Y g L K n J C v E E S Y S 1 6 a t q S n C X v 7 x K e s 2 G e 9 F o 3 r X q 7 V Z Z R w W c g F N w D l x w C d r g F n R A F 2 C Q g W f w C t 6 s J + v F e r c + F q N r V r l T A 3 9 g f f 4 A o O a U s w = = < / l a t e x i t > f G 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" G / j y b d u z g T n G + M Y L R r v P d H T m X 8 w = \" > A A A B / n i c b V B P S 8 M w H E 3 n v z n / V c W T l + A Q P I 1 2 D v Q 4 8 K D H C W 4 O t l r S N N 3 C 0 q Q k q T B K w a / i x Y M i X v 0 c 3 v w 2 p l s P u v k g 5 P H e 7 0 d e X p A w q r T j f F u V l d W 1 9 Y 3 q Z m 1 r e 2 d 3 z 9 4 / 6 C m R S k y 6 W D A h + w F S h F F O u p p q R v q J J C g O G L k P J l e F f / 9 I p K K C 3 + l p Q r w Y j T i N K E b a S L 5 9 l A 0 D w U I 1 j c 0 F o 9 z P 3 P z h 2 r f r T s O Z A S 4 T t y R 1 U K L j 2 1 / D U O A 0 J l x j h p Q a u E 6 i v Q x J T T E j e W 2 Y K p I g P E E j M j C U o 5 g o L 5 v F z + G p U U I Y C W k O 1 3 C m / t 7 I U K y K g G Y y R n q s F r 1 C / M 8 b p D q 6 9 D L K k 1 Q T j u c P R S m D W s C i C x h S S b B m U 0 M Q l t R k h X i M J M L a N F Y z J b i L X 1 4 m v W b D P W 8 0 b 1 v 1 d q u s o w q O w Q k 4 A y 6 4 A G 1 w A z q g C z D I w D N 4 B W / W k / V i v V s f 8 9 G K V e 4 c g j + w P n 8 A X b C V s Q = = < / l a t e x i t > f G 2 < l a t e x i t s h a 1 _ b a s e 6 4 = \" y L 3 s 0 4 B p / V q G k J 3 0 G 8 q C M a m W T Q g = \" > A A A B / n i c b V B P S 8 M w H E 3 n v z n / V c W T l + A Q P I 1 2 D v Q 4 8 K D H C W 4 O t l r S N N 3 C 0 q Q k q T B K w a / i x Y M i X v 0 c 3 v w 2 p l s P u v k g 5 P H e 7 0 d e X p A w q r T j f F u V l d W 1 9 Y 3 q Z m 1 r e 2 d 3 z 9 4 / 6 C m R S k y 6 W D A h + w F S h F F O u p p q R v q J J C g O G L k P J l e F f / 9 I p K K C 3 + l p Q r w Y j T i N K E b a S L 5 9 l A 0 D w U I 1 j c 0 F o 9 z P m v n D t W / X n Y Y z A 1 w m b k n q o E T H t 7 + G o c B p T L j G D C k 1 c J 1 E e x m S m m J G 8 t o w V S R B e I J G Z G A o R z F R X j a L n 8 N T o 4 Q w E t I c r u F M / b 2 R o V g V A c 1 k j P R Y L X q F + J 8 3 S H V 0 6 W W U J 6 k m H M 8 f i l I G t Y B F F z C k k m D N p o Y g L K n J C v E Y S Y S 1 a a x m S n A X v J i g N G h s H 8 t v S H j 0 Q q K v i D z h L i x W j K a U Q x 0 k b y 7 U Y + C Q Q L V R a b C 6 r C z 5 3 C t 5 t O y 1 k A r h O 3 I k 1 Q o e f b X 5 N Q 4 D Q m X G O G l B q 7 T q K 9 H E l N M S N F f Z I q k i A 8 R 1 M y N p S j m C g v X 4 Q v 4 I V R Q h g J a Q 7 X c K H + 3 s h R r M p 4 Z j J G e q Z W v V L 8 z x u n O r r x c s q T V B O O l w 9 F K Y N a w L I J G F J J s G a Z I Q h L a r J C P E M S Y W 3 6 q p s S 3 N U v r 5 N B u + V e t d r 3 n W a 3 U 9 V R A 2 f g H F w C F 1 y D L r g D P d A H G G T g G b y C N + v J e r H e r Y / l 6 I Z V 7 T T A H 1 i f P x w d l Q Q = < / l a t e x i t > s G 0 < l a t e x i t s h a 1 _ b a s e 6 4 = \" w 7 + O A h R 3 E 4 2 B D J q r e W 7 O K f c p C Q 0 = \" > A A A B / n i c b V B P S 8 M w H E 3 n v z n / V c W T l + A Q P I 1 2 D v Q 4 8 K D H C W 4 O t l r S N N 3 C 0 q Q k q T B K w a / i x Y M i X v 0 c 3 v w 2 p l s P u v k g 5 P H e 7 0 d e X p A w q r T j f F u V l d W 1 9 Y 3 q Z m 1 r e 2 d 3 z 9 4 / 6 C m R S k y 6 W D A h + w F S h F F O u p p q R v q J J C g O G L k P J l e F f / 9 I p K K C 3 + l p Q r w Y j T i N K E b a S L 5 9 l A 0 D w U I 1 j c 0 F V e 5 n T v 5 w 7 d t 1 p + H M A J e J W 5 I 6 K N H x 7 a 9 h K H A a E 6 4 x Q 0 o N X C f R X o a k p p i R v D Z M F U k Q n q A R G R j K U U y U l 8 3 i 5 / D U K C G M h D S H a z h T f 2 9 k K F Z F Q D M Z I z 1 W i 1 4 h / u c N U h 1 d e h n l S a o J x / O H o p R B L W D R B Q y p J F i z q S E I S 2 q y Q j x G E m F t G q u Z E t z F L y + T X r P h n j e a t 6 1 6 u 1 X W U Q X H 4 A S c A R d c g D a 4 A R 3 Q B R h k 4 B m 8 g j f r y X q x 3 q 2 P + W j F K n c O w R 9 Y n z 9 w O J W 9 < / l a t e x i t > f G t < l a t e x i t s h a 1 _ b a s e 6 4 = \" z U c 7 C 4 R M y k u C + q Q K u z B Z q w S L z z 0 = \" > A A A B / n i c b V B P S 8 M w H E 3 n v z n / V c W T l + A Q P I 1 2 D v Q 4 8 K D H C W 4 O t l r S N N 3 C 0 q Q k q T B K w a / i x Y M i X v 0 c 3 v w 2 p l s P u v k g 5 P H e 7 0 d e X p A w q r T j f F u V l d W 1 9 Y 3 q Z m 1 r e 2 d 3 z 9 4 / 6 C m R S k y 6 W D A h + w F S h F F O u p p q R v q J J C g O G L k P J l e F f / 9 I p K K C 3 + l p Q r w Y j T i N K E b a S L 5 9 l A 0 D w U I 1 j c 0 F o 9 z P d P 5 w 7 d t 1 p + H M A J e J W 5 I 6 K N H x 7 a 9 h K H A a E 6 4 x Q 0 o N X C f R X o a k p p i R v D Z M F U k Q n q A R G R j K U U y U l 8 3 i 5 / D U K C G M h D S H a z h T f 2 9 k K F Z F Q D M Z I z 1 W i 1 4 h / u c N U h 1 d e h n l S a o J x / O H o p R B L W D R B Q y p J F i z q S E I S 2 q y Q j x G E m F t G q u Z E t z F L y + T X r P h n j e a t 6 1 6 u 1 X W U Q X H 4 A S c A R d c g D a 4 A R 3 Q B R h k 4 B m 8 g j f r y X q x 3 q 2 P + W j F K n c O w R 9 Y n z / E B Z X 0 < / l a t e x i t > J i g N G h s H 8 t v S H j 0 Q q K v i D z h L i x W j K a U Q x 0 k b y 7 U Y + C Q Q L V R a b C 0 a F n 7 u F b z e d l r M A X C d u R Z q g Q s + 3 v y a h w G l M u M Y M K T V 2 n U R 7 O Z K a Y k a K + i R V J E F 4 j q Z k b C h H M V F e v g h f w A u j h D A S 0 h y u 4 U L 9 v Z G j W J X x z G S M 9 E y t e q X 4 n z d O d X T j 5 Z Q n q S Y c L x + K U g a 1 g G U T M K S S Y M 0 y Q x C W 1 G S F e I Y k w t r 0 V T c l u K t f X i e D d s u 9 a r X v O 8 1 u p 6 q j B s 7 A O b g E L r g G X X A H e q A P M M j A M 3 g F b 9 a T 9 W K 9 W x / L 0 Q 2 r 2 m m A P 7 A + f w A J r Z T 4 < / l a t e x i t > f 2 < l a t e x i t s h a 1 _ b a s e 6 4 = \" v 5 4 x g q V m / n q 9 D y N z G r d c 1 F u 6 O P g = \" > A A A B / H i c b V D N S 8 M w H E 3 9 n P O r u q O X 4 B A 8 j X Y O 9 D j w 4 n G C + 4 C t l D R N t 7 A 0 K U k q l F L / F S 8 e F P H q H + L N / 8 Z 0 6 0 E 3 H 4 Q 8 3 v v 9 y M s L E k a V d p x v a 2 N z a 3 t n t 7 Z X 3 z 8 4 P D q 2 T 0 4 H S q Q S k z 4 W T M h R g B R h l J O + p p q R U S I J i g N G h s H 8 t v S H j 0 Q q K v i D z h L i x W j K a U Q x 0 k b y 7 U Y + C Q Q L V R a b C 0 a F n 7 c L 3 2 4 6 L W c B u E 7 c i j R B h Z 5 v f 0 1 C g d O Y c I 0 Z U m r s O o n 2 c i Q 1 x Y w U 9 U m q S I L w H E 3 J 2 F C O Y q K 8 f B G + g B d G C W E k p D l c w 4 X 6 e y N H s S r j m c k Y 6 Z l a 9 U r x P 2 + c 6 u j G y y l P U k 0 4 X j 4 U p Q x q A c s m Y E g l w Z p l h i A s q c k K 8 Q x J h L X p q 2 5 K c F e / v E 4 G 7 Z Z 7 1 W r f d 5 r d T l V H D Z y B c 3 A J X H A N u u A O 9 E A f Y J C B Z / A K 3 q w n 6 8 V 6 t z 6 W o x t W t d M A f 2 B 9 / g A L M p T 5 < / l a t e x i t > f t < l a t e x i t s h a 1 _ b a s e 6 4 = \" g 7 5 7 F + X y u G B m K w U j f a Z q y B p Y F O g = \" > A A A B / H i c b V D N S 8 M w H E 3 9 n P O r u q O X 4 B A 8 j X Y O 9 D j w 4 n G C + 4 C t l D R N t 7 A 0 K U k q l F L / F S 8 e F P H q H + L N / 8 Z 0 6 0 E 3 H 4 Q 8 3 v v 9 y M s L E k a V d p x v a 2 N z a 3 t n t 7 Z X 3 z 8 4 P D q 2 T 0 4 H S q Q S k z 4 W T M h R g B R h l J O + p p q R U S I J i g N G h s H 8 t v S H j 0 Q q K v i D z h L i x W j K a U Q x 0 k b y 7 U Y + C Q Q L V R a b C 0 a F n + v C t 5 t O y 1 k A r h O 3 I k 1 Q o e f b X 5 N Q 4 D Q m X G O G l B q 7 T q K 9 H E l N M S N F f Z I q k i A 8 R 1 M y N p S j m C g v X 4 Q v 4 I V R Q h g J a Q 7 X c K H + 3 s h R r M p 4 Z j J G e q Z W v V L 8 z x u n O r r x c s q T V B O O l w 9 F K Y N a w L I J G F J J s G a Z I Q h L a r J C P E M S Y W 3 6 q p s S 3 N U v r 5 N B u + V e t d r 3 n W a 3 U 9 V R A 2 f g H F w C F 1 y D L r g D P d A H G G T g G b y C N + v J e r H e r Y / l 6 I Z V 7 T T A H 1 i f P 2 9 8 l T s = < / l a t e x i t > = . . . < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 e 3 5 2 g W f r l v f 1 6 w M E b X 2 S 1 Z U Q C Q = \" > A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B I v g q S R V U G 8 F L x 4 r 2 A 9 o Q 9 l s N u 3 a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o Z V S m K W t S J Z T u h M Q w w S V r I k f B O q l m J A k F a 4 e j 2 5 n f f m L a c C U f c J y y I C E D y W N O C V q p 1 R O R Q t M v V 7 y q N 4 e 7 S v y c V C B H o 1 / + 6 k W K Z g m T S A U x p u t 7 K Q Y T o p F T w a a l X m Z Y S u i I D F j X U k k S Z o L J / N q p e 2 a V y I 2 V t i X R n a u / J y Y k M W a c h L Y z I T g 0 y 9 5 M / M / r Z h h f B x M u 0 w y Z p I t F c S Z c V O 7 s d T f i m l E U Y 0 s I 1 d z e 6 t I h 0 Y S i D a h k Q / C X X 1 4 l r V r V v 6 j W 7 i 8 r 9 Z s 8 j i K c w C m c g w 9 X U I c 7 a E A T K D z C M 7 z C m 6 O c F + f d + V i 0 F p x 8 5 h j + w P n 8 A b q 4 j z M = < / l a t e x i t > . . . < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 e 3 5 2 g W f r l v f 1 6 w M E b X 2 S 1 Z U Q C Q = \" > A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B I v g q S R V U G 8 F L x 4 r 2 A 9 o Q 9 l s N u 3 a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0 s 7 u 3 x p u t 7 K Q Y T o p F T w a a l X m Z Y S u i I D F j X U k k S Z o L J / N q p e 2 a V y I 2 V t i X R n a u / J y Y k M W a c h L Y z I T g 0 y 9 5 M / M / r Z h h f B x M u 0 w y Z p I t F c S Z c V O 7 s d T f i m l E U Y 0 s I 1 d z e 6 t I h 0 Y S i D a h k Q / C X X 1 4 l r V r V v 6 j W 7 i 8 r 9 Z s 8 j i K c w C m c g w 9 X U I c 7 a E A T K D z C M 7 z C m 6 O c F + f d + V i 0 F p x 8 5 h j + w P n 8 A b q 4 j z M = < / l a t e x i t > . . . < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 e 3 5 2 g W f r l v f 1 6 w M E b X 2 S 1 Z U Q C Q = \" > A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B I v g q S R V U G 8 F L x 4 r 2 A 9 o Q 9 l s N u 3 a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o Z V S m K W t S J Z T u h M Q w w S V r I k f B O q l m J A k F a 4 e j 2 5 n f f m L a c C U f c J y y I C E D y W N O C V q p 1 R O R Q t M v V 7 y q N 4 e 7 S v y c V C B H o 1 / + 6 k W K Z g m T S A U x p u t 7 K Q Y T o p F T w a a l X m Z Y S u i I D F j X U k k S Z o L J / N q p e 2 a V y I 2 V t i X R n a u / J y Y k M W a c h L Y z I T g 0 y 9 5 M / M / r Z h h f B x M u 0 w y Z p I t F c S Z c V O 7 s d T f i m l E U Y 0 s I 1 d z e 6 t I h 0 Y S i D a h k Q / C X X 1 4 l r V r V v 6 j W 7 i 8 r 9 Z s 8 j i K c w C m c g w 9 X U I c 7 a E A T K D z C M 7 z C m 6 O c F + f d + V i 0 F p x 8 5 h j + w P n 8 A b q 4 j z M = < / l a t e x i t > . . . < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 e 3 5 2 g W f r l v f 1 6 w M E b X 2 S 1 Z U Q C Q = \" > A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B I v g q S R V U G 8 F L x 4 r 2 A 9 o Q 9 l s N u 3 a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o Z V S m K W t S J Z T u h M Q w w S V r I k f B O q l m J A k F a 4 e j 2 5 n f f m L a c C U f c J y y I C E D y W N O C V q p 1 R O R Q t M v V 7 y q N 4 e 7 S v y c V C B H o 1 / + 6 k W K Z g m T S A U x p u t 7 K Q Y T o p F T w a a l X m Z Y S u i I D F j X U k k S Z o L J / N q p e 2 a V y I 2 V t i X R n a u / J y Y k M W a c h L Y z I T g 0 y 9 5 M / M / r Z h h f B x M u 0 w y Z p I t F c S Z c V O 7 s d T f i m l E U Y 0 s I 1 d z e 6 t I h 0 Y S i D a h k Q / C X X 1 4 l r V r V v 6 j W 7 i 8 r 9 Z s 8 j i K c w C m c g w 9 X U I c 7 a E A T K D z C M 7 z C m 6 O c F + f d + V i 0 F p x 8 5 h j + w P n 8 A b q 4 j z M = < / l a t e x i t > . . . < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 e 3 5 2 g W f r l v f 1 6 w M E b X 2 S 1 Z U Q C Q = \" > A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B I v g q S R V U G 8 F L x 4 r 2 A 9 o Q 9 l s N u 3 a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o Z V S m K W t S J Z T u h M Q w w S V r I k f B O q l m J A k F a 4 e j 2 5 n f f m L a c C U f c J y y I C E D y W N O C V q p 1 R O R Q t M v V 7 y q N 4 e 7 S v y c V C B H o 1 / + 6 k W K Z g m T S A U x p u t 7 K Q Y T o p F T w a a l X m Z Y S u i I D F j X U k k S Z o L J / N q p e 2 a V y I 2 V t i X R n a u / J y Y k M W a c h L Y z I T g 0 y 9 5 M / M / r Z h h f B x M u 0 w y Z p I t F c S Z c V O 7 s d T f i m l E U Y 0 s I 1 d z e 6 t I h 0 Y S i D a h k Q / C X X 1 4 l r V r V v 6 j W 7 i 8 r 9 Z s 8 j i K c w C m c g w 9 X U I c 7 a E A T K D z C M 7 z C m 6 O c F + f d + V i 0 F p x 8 5 h j + w P n 8 A b q 4 j z M = < / l a t e x i t > . . . < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 e 3 5 2 g W f r l v f 1 6 w M E b X 2 S 1 Z U Q C Q = \" > A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B I v g q S R V U G 8 F L x 4 r 2 A 9 o Q 9 l s N u 3 a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o Z V S m K W t S J Z T u h M Q w w S V r I k f B O q l m J A k F a 4 e j 2 5 n f f m L a c C U f c J y y I C E D y W N O C V q p 1 R O R Q t M v V 7 y q N 4 e 7 S v y c V C B H o 1 / + 6 k W K Z g m T S A U x p u t 7 K Q Y T o p F T w a a l X m Z Y S u i I D F j X U k k S Z o L J / N q p e 2 a V y I 2 V t i X R n a u / J y Y k M W a c h L Y z I T g 0 y 9 5 M / M / r Z h h f B x M u 0 w y Z p I t F c S Z c V O 7 s d T f i m l E U Y 0 s I 1 d z e 6 t I h 0 Y S i D a h k Q / C X X 1 4 l r V r V v 6 j W 7 i 8 r 9 Z s 8 j i K c w C m c g w 9 X U I c 7 a E A T K D z C M 7 z C m 6 O c F + f d + V i 0 F p x 8 5 h j + w P n 8 A b q 4 j z M = < / l a t e x i t > s 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" m Q 9 7 r i 6 n c S F 3 A L P b D O J n M 3 9 1 o r k = \" > A A A B + n i c b V C 7 T s M w F L 0 p r 1 J e K Y w s F h U S U 5 U U J G C r x M J Y J P q Q 2 i h y H L e 1 6 j i R 7 Y C q 0 E 9 h Y Q A h V r 6 E j b / B a T O t+1 = g ( s t ) v 5 B + f C o Z V S m K W t S J Z T u h M Q w w S V r I k f B O q l m J A k F a 4 e j 2 5 n f f m L a c C U f c J y y I C E D y W N O C V q p 1 R O R Q t M v V 7 y q N 4 e 7 S v y c V C B H o 1 / + 6 k W K Z g m T S A U x p u t 7 K Q Y T o p F T w a a l X m Z Y S u i I D F j X U k k S Z o L J / N q p e 2 a V y I 2 V t i X R n a u / J y Y k M W a c h L Y z I T g 0 y 9 5 M / M / r Z h h f B x M u 0 w y Z p I t F c S Z c V O 7 s d T f i m l E U Y 0 s I 1 d z e 6 t I h 0 Y S i D a h k Q / C X X 1 4 l r V r V v < l a t e x i t s h a 1 _ b a s e 6 4 = \" l + S u E U v F h Z d t P G c f L D R X 6 U r 4 l + s = \" > A by an encoder network . Specifically , let the current generated sentence be Y 1 ... t ( encouraged to be the same as parts of a training sentence in training ) , with f t calculated as : A A C F X i c b V D L S g M x F M 3 U V 6 2 v U Z d u g k W o K G W m C u p C K L h x Z w X 7 g H Y Y M p m 0 D c 0 8 S O 4 I Z Z i f c O O v u H G h i F v B n X 9 j + l j Y 1 g M h h 3 P u 5 d 5 7 v F h w B Z b 1 Y + S W l l d W 1 / L r h Y 3 N r e 0 d c 3 e v o a J E U l a n k Y h k y y O K C R 6 y O n A Q r B V L R g J P s K Y 3 u B n 5 z U c m F Y / C B x j G z A l I L + R d T g l o y T V P 0 4 4 X C V 8 N A / 3 h u 8 x N 4 c T O 8 D X u l W Y c l b l w 7 J p F q 2 y N g R e J P S V F N E X N N b 8 7 f k S T g I V A B V G q b V s x O C m R w K l g W a G T K B Y T O i A 9 1 t Y 0 J A F T T j q + K s N H W v F x N 5 L 6 h Y D H 6 t + O l A R q t J 2 u D A j 0 1 b w 3 E v / z 2 g l 0 L 5 2 U h 3 E C L K S T Q d 1 E Y I j w K C L s c 8 k o i K E m h E q u d 8 W 0 T y S h o I M s 6 B D s + Z M X S a N S t s / K l f v z Y v V q G k c f t = Enc ( Y 1 ... t ) . The initial state of the guider network is the encoded feature of a true input sentence by the same convolutional neural network ( CNN ) , i.e. , s G 0 = Enc ( X ) , where Enc ( ) denotes the encoder transformation , implemented with a CNN . Importantly , the input to the guider network , at each time point , is defined by features from the entire sentence generated to that point . This provides an important \" guide \" to the LSTM decoder , accounting for the global properties of the generated text . Text Generation with Planning We first explain how one uses the guider network to guide next - word generation for the generator ( the LSTM decoder in Figure 1 ) . Our framework is inspired by the MPC method ( Nagabandi et al , 2017 ) , and can be regarded as a type of plan - ahead attention mechanism . Given the feature f t at time t from the current input sentence , the guider network produces a prediction G \u03c8 ( s G t\u22121 , f t ) as a future feature representation , by feeding f t into the LSTM guider . Since the training of the guider network is based on real data ( detailed in the next paragraph ) , the predicted feature contains global - structure information of the training sentences . To utilize such information to predict the next word , we combine the predicted feature with the output of the decoder by constructing an attention - like mechanism . Specifically , we first apply a linear transformation \u03d5 on the predicted feature G \u03c8 ( s G t\u22121 , f t ) , forming a weight vector w t \u03d5 G \u03c8 ( s G t\u22121 , f t ) . The weight w t is applied to the output O t of the LSTM decoder by an element - wise multiplication operation . The result is then fed into a softmax layer to generate the next token y t . Formally , the generative process is written as : O t = g ( s t\u22121 ) , w t = \u03d5 ( G \u03c8 ( s G t\u22121 , f t ) ) , ( 5 ) y t \u223c Multi ( 1 , softmax ( O t w t ) ) , ( 6 ) s G t = h G ( s G t\u22121 , f t ) , s t = h ( s t\u22121 , e ( y t ) ) . ( 7 ) Guider Network Training Given a sentence of feature representations ( f 1 , f 2 , . . . f T ) for a training sentence , we seek to update the guider network such that it is able to predict f t+c given f t , where c > 0 is the number of steps that are looked ahead . We implement this by forcing the predicted feature , G \u03c8 ( s G t , f t ) , to match both the sentence feature f t+c ( first term in ( 8 ) ) and the corresponding feature - changing direction ( second term in ( 8 ) ) . This is formalized by maximizing an objective function of the following form at time t : J \u03c8 G = D cos f t+c , G \u03c8 ( s G t\u22121 , f t ) ( 8 ) + D cos f t+c \u2212 f t , G \u03c8 ( s G t\u22121 , f t ) \u2212 f t , where D cos ( , ) denotes the cosine similarity 2 . By maximizing ( 8 ) , an ideal guider network should be able to predict the true next words conditioned on the current word in a sentence . As a result , the prediction is used to construct an intermediate reward , used to update the generator ( the LSTM decoder ) , as described further below .", "entities": [[9, 10, "MethodName", "LSTM"], [20, 22, "TaskName", "text generation"], [128, 129, "MethodName", "LSTM"], [207, 208, "DatasetName", "0"], [267, 268, "DatasetName", "0"], [285, 286, "DatasetName", "0"], [304, 305, "DatasetName", "0"], [409, 410, "DatasetName", "0"], [417, 418, "DatasetName", "0"], [460, 461, "DatasetName", "0"], [674, 675, "DatasetName", "0"], [872, 873, "DatasetName", "0"], [898, 899, "DatasetName", "0"], [1015, 1016, "DatasetName", "0"], [1038, 1039, "DatasetName", "0"], [1041, 1042, "DatasetName", "0"], [1094, 1095, "DatasetName", "0"], [1125, 1126, "DatasetName", "0"], [1154, 1155, "DatasetName", "0"], [1381, 1382, "DatasetName", "0"], [1407, 1408, "DatasetName", "0"], [1524, 1525, "DatasetName", "0"], [1547, 1548, "DatasetName", "0"], [1550, 1551, "DatasetName", "0"], [1603, 1604, "DatasetName", "0"], [1634, 1635, "DatasetName", "0"], [1663, 1664, "DatasetName", "0"], [1890, 1891, "DatasetName", "0"], [1916, 1917, "DatasetName", "0"], [2033, 2034, "DatasetName", "0"], [2056, 2057, "DatasetName", "0"], [2059, 2060, "DatasetName", "0"], [2112, 2113, "DatasetName", "0"], [2143, 2144, "DatasetName", "0"], [2172, 2173, "DatasetName", "0"], [2399, 2400, "DatasetName", "0"], [2425, 2426, "DatasetName", "0"], [2542, 2543, "DatasetName", "0"], [2565, 2566, "DatasetName", "0"], [2568, 2569, "DatasetName", "0"], [2621, 2622, "DatasetName", "0"], [2652, 2653, "DatasetName", "0"], [2681, 2682, "DatasetName", "0"], [2808, 2809, "DatasetName", "0"], [2832, 2833, "DatasetName", "0"], [2861, 2862, "DatasetName", "0"], [2891, 2892, "DatasetName", "0"], [2930, 2931, "DatasetName", "0"], [2984, 2985, "DatasetName", "0"], [3035, 3036, "DatasetName", "0"], [3315, 3316, "DatasetName", "0"], [3339, 3340, "DatasetName", "0"], [3368, 3369, "DatasetName", "0"], [3398, 3399, "DatasetName", "0"], [3437, 3438, "DatasetName", "0"], [3491, 3492, "DatasetName", "0"], [3542, 3543, "DatasetName", "0"], [3821, 3822, "DatasetName", "0"], [3944, 3945, "DatasetName", "0"], [4327, 4328, "DatasetName", "0"], [4410, 4411, "DatasetName", "0"], [4591, 4592, "DatasetName", "0"], [4857, 4858, "DatasetName", "0"], [4886, 4887, "DatasetName", "0"], [4956, 4957, "DatasetName", "0"], [4973, 4974, "DatasetName", "0"], [5169, 5170, "DatasetName", "0"], [5394, 5395, "DatasetName", "0"], [5424, 5425, "DatasetName", "0"], [5611, 5612, "DatasetName", "0"], [5628, 5629, "DatasetName", "0"], [5653, 5654, "DatasetName", "0"], [5664, 5665, "DatasetName", "0"], [5735, 5736, "DatasetName", "0"], [5851, 5852, "DatasetName", "0"], [5974, 5975, "DatasetName", "0"], [6357, 6358, "DatasetName", "0"], [6440, 6441, "DatasetName", "0"], [6621, 6622, "DatasetName", "0"], [6887, 6888, "DatasetName", "0"], [6916, 6917, "DatasetName", "0"], [6986, 6987, "DatasetName", "0"], [7003, 7004, "DatasetName", "0"], [7199, 7200, "DatasetName", "0"], [7424, 7425, "DatasetName", "0"], [7454, 7455, "DatasetName", "0"], [7641, 7642, "DatasetName", "0"], [7658, 7659, "DatasetName", "0"], [7683, 7684, "DatasetName", "0"], [7694, 7695, "DatasetName", "0"], [7765, 7766, "DatasetName", "0"], [7972, 7973, "DatasetName", "0"], [8002, 8003, "DatasetName", "0"], [8056, 8057, "DatasetName", "0"], [8179, 8180, "DatasetName", "0"], [8562, 8563, "DatasetName", "0"], [8645, 8646, "DatasetName", "0"], [8826, 8827, "DatasetName", "0"], [9092, 9093, "DatasetName", "0"], [9121, 9122, "DatasetName", "0"], [9191, 9192, "DatasetName", "0"], [9208, 9209, "DatasetName", "0"], [9404, 9405, "DatasetName", "0"], [9597, 9598, "DatasetName", "0"], [9614, 9615, "DatasetName", "0"], [9639, 9640, "DatasetName", "0"], [9650, 9651, "DatasetName", "0"], [9720, 9721, "DatasetName", "0"], [9770, 9771, "DatasetName", "0"], [9840, 9841, "DatasetName", "0"], [9875, 9876, "DatasetName", "0"], [10137, 10138, "DatasetName", "MLP"], [10274, 10275, "DatasetName", "0"], [10281, 10282, "DatasetName", "0"], [10322, 10323, "DatasetName", "0"], [10424, 10425, "DatasetName", "0"], [10425, 10426, "DatasetName", "0"], [10616, 10617, "DatasetName", "0"], [10623, 10624, "DatasetName", "0"], [10664, 10665, "DatasetName", "0"], [10766, 10767, "DatasetName", "0"], [10767, 10768, "DatasetName", "0"], [10958, 10959, "DatasetName", "0"], [10965, 10966, "DatasetName", "0"], [11006, 11007, "DatasetName", "0"], [11108, 11109, "DatasetName", "0"], [11109, 11110, "DatasetName", "0"], [11229, 11230, "DatasetName", "0"], [11356, 11357, "DatasetName", "0"], [11483, 11484, "DatasetName", "0"], [11577, 11578, "DatasetName", "0"], [11602, 11603, "DatasetName", "0"], [11641, 11642, "DatasetName", "0"], [11657, 11658, "DatasetName", "0"], [11658, 11659, "DatasetName", "0"], [11672, 11673, "DatasetName", "0"], [11724, 11725, "DatasetName", "0"], [11881, 11882, "DatasetName", "0"], [11887, 11888, "DatasetName", "0"], [12001, 12002, "DatasetName", "0"], [12050, 12051, "DatasetName", "0"], [12085, 12086, "DatasetName", "0"], [12151, 12152, "DatasetName", "0"], [12168, 12169, "DatasetName", "0"], [12169, 12170, "DatasetName", "0"], [12285, 12286, "DatasetName", "0"], [12288, 12290, "HyperparameterName", "k ="], [12333, 12334, "DatasetName", "0"], [12372, 12373, "DatasetName", "0"], [12423, 12424, "DatasetName", "0"], [12455, 12456, "DatasetName", "0"], [12525, 12526, "DatasetName", "0"], [12556, 12557, "DatasetName", "0"], [12676, 12677, "DatasetName", "0"], [12910, 12911, "DatasetName", "0"], [12917, 12918, "DatasetName", "0"], [12958, 12959, "DatasetName", "0"], [13060, 13061, "DatasetName", "0"], [13061, 13062, "DatasetName", "0"], [13252, 13253, "DatasetName", "0"], [13259, 13260, "DatasetName", "0"], [13300, 13301, "DatasetName", "0"], [13402, 13403, "DatasetName", "0"], [13403, 13404, "DatasetName", "0"], [13594, 13595, "DatasetName", "0"], [13601, 13602, "DatasetName", "0"], [13642, 13643, "DatasetName", "0"], [13744, 13745, "DatasetName", "0"], [13745, 13746, "DatasetName", "0"], [13936, 13937, "DatasetName", "0"], [13943, 13944, "DatasetName", "0"], [13984, 13985, "DatasetName", "0"], [14086, 14087, "DatasetName", "0"], [14087, 14088, "DatasetName", "0"], [14183, 14184, "DatasetName", "0"], [14310, 14311, "DatasetName", "0"], [14437, 14438, "DatasetName", "0"], [14588, 14589, "DatasetName", "0"], [14822, 14823, "DatasetName", "0"], [14829, 14830, "DatasetName", "0"], [14870, 14871, "DatasetName", "0"], [14972, 14973, "DatasetName", "0"], [14973, 14974, "DatasetName", "0"], [15093, 15094, "DatasetName", "0"], [15327, 15328, "DatasetName", "0"], [15334, 15335, "DatasetName", "0"], [15375, 15376, "DatasetName", "0"], [15477, 15478, "DatasetName", "0"], [15478, 15479, "DatasetName", "0"], [15551, 15552, "DatasetName", "0"], [15624, 15625, "DatasetName", "0"], [15820, 15821, "DatasetName", "0"], [15965, 15966, "DatasetName", "0"], [16066, 16067, "DatasetName", "0"], [16252, 16253, "DatasetName", "0"], [16353, 16354, "DatasetName", "0"], [16539, 16540, "DatasetName", "0"], [16640, 16641, "DatasetName", "0"], [16794, 16795, "DatasetName", "0"], [16819, 16820, "DatasetName", "0"], [16858, 16859, "DatasetName", "0"], [16874, 16875, "DatasetName", "0"], [16875, 16876, "DatasetName", "0"], [16889, 16890, "DatasetName", "0"], [16941, 16942, "DatasetName", "0"], [17103, 17104, "DatasetName", "0"], [17176, 17177, "DatasetName", "0"], [17249, 17250, "DatasetName", "0"], [17322, 17323, "DatasetName", "0"], [17395, 17396, "DatasetName", "0"], [17495, 17497, "HyperparameterName", "k ="], [17609, 17610, "DatasetName", "0"], [17615, 17616, "DatasetName", "0"], [17816, 17817, "DatasetName", "0"], [17854, 17855, "DatasetName", "0"], [17856, 17857, "DatasetName", "0"], [18031, 18032, "DatasetName", "0"], [18138, 18139, "DatasetName", "0"], [18325, 18326, "DatasetName", "0"], [18363, 18364, "DatasetName", "0"], [18365, 18366, "DatasetName", "0"], [18540, 18541, "DatasetName", "0"], [18647, 18648, "DatasetName", "0"], [18833, 18834, "DatasetName", "0"], [18890, 18891, "DatasetName", "0"], [18964, 18965, "DatasetName", "0"], [19013, 19014, "DatasetName", "0"], [19032, 19033, "DatasetName", "0"], [19043, 19044, "DatasetName", "0"], [19143, 19144, "DatasetName", "0"], [19162, 19163, "DatasetName", "0"], [19164, 19165, "DatasetName", "0"], [19240, 19241, "DatasetName", "0"], [19354, 19355, "DatasetName", "0"], [19455, 19456, "DatasetName", "0"], [19641, 19642, "DatasetName", "0"], [19742, 19743, "DatasetName", "0"], [19928, 19929, "DatasetName", "0"], [20029, 20030, "DatasetName", "0"], [20215, 20216, "DatasetName", "0"], [20316, 20317, "DatasetName", "0"], [20498, 20499, "DatasetName", "0"], [20599, 20600, "DatasetName", "0"], [20781, 20782, "DatasetName", "0"], [20882, 20883, "DatasetName", "0"], [20985, 20986, "DatasetName", "0"], [21009, 21010, "DatasetName", "0"], [21038, 21039, "DatasetName", "0"], [21068, 21069, "DatasetName", "0"], [21107, 21108, "DatasetName", "0"], [21245, 21246, "DatasetName", "0"], [21310, 21311, "DatasetName", "0"], [21360, 21361, "DatasetName", "0"], [21384, 21385, "DatasetName", "0"], [21413, 21414, "DatasetName", "0"], [21443, 21444, "DatasetName", "0"], [21482, 21483, "DatasetName", "0"], [21572, 21573, "DatasetName", "0"], [21686, 21687, "DatasetName", "0"], [21743, 21744, "DatasetName", "0"], [21767, 21768, "DatasetName", "0"], [21796, 21797, "DatasetName", "0"], [21826, 21827, "DatasetName", "0"], [21865, 21866, "DatasetName", "0"], [21919, 21920, "DatasetName", "0"], [21970, 21971, "DatasetName", "0"], [22169, 22170, "DatasetName", "0"], [22199, 22200, "DatasetName", "0"], [22386, 22387, "DatasetName", "0"], [22403, 22404, "DatasetName", "0"], [22428, 22429, "DatasetName", "0"], [22439, 22440, "DatasetName", "0"], [22510, 22511, "DatasetName", "0"], [22681, 22682, "DatasetName", "0"], [22711, 22712, "DatasetName", "0"], [22898, 22899, "DatasetName", "0"], [22915, 22916, "DatasetName", "0"], [22940, 22941, "DatasetName", "0"], [22951, 22952, "DatasetName", "0"], [23022, 23023, "DatasetName", "0"], [23193, 23194, "DatasetName", "0"], [23223, 23224, "DatasetName", "0"], [23410, 23411, "DatasetName", "0"], [23427, 23428, "DatasetName", "0"], [23452, 23453, "DatasetName", "0"], [23463, 23464, "DatasetName", "0"], [23534, 23535, "DatasetName", "0"], [23705, 23706, "DatasetName", "0"], [23735, 23736, "DatasetName", "0"], [23922, 23923, "DatasetName", "0"], [23939, 23940, "DatasetName", "0"], [23964, 23965, "DatasetName", "0"], [23975, 23976, "DatasetName", "0"], [24046, 24047, "DatasetName", "0"], [24217, 24218, "DatasetName", "0"], [24247, 24248, "DatasetName", "0"], [24434, 24435, "DatasetName", "0"], [24451, 24452, "DatasetName", "0"], [24476, 24477, "DatasetName", "0"], [24487, 24488, "DatasetName", "0"], [24629, 24630, "DatasetName", "0"], [24666, 24667, "DatasetName", "0"], [24988, 24989, "DatasetName", "0"], [25182, 25183, "DatasetName", "0"], [25264, 25265, "DatasetName", "0"], [25328, 25329, "DatasetName", "0"], [25519, 25520, "DatasetName", "0"], [25864, 25865, "DatasetName", "0"], [25882, 25883, "DatasetName", "0"], [25901, 25902, "DatasetName", "0"], [26006, 26007, "DatasetName", "0"], [26014, 26015, "DatasetName", "0"], [26051, 26052, "DatasetName", "0"], [26169, 26170, "DatasetName", "0"], [26203, 26204, "DatasetName", "0"], [26314, 26315, "DatasetName", "0"], [26325, 26326, "DatasetName", "0"], [26396, 26397, "DatasetName", "0"], [26414, 26415, "DatasetName", "0"], [26433, 26434, "DatasetName", "0"], [26538, 26539, "DatasetName", "0"], [26546, 26547, "DatasetName", "0"], [26671, 26672, "DatasetName", "0"], [26739, 26740, "DatasetName", "0"], [26758, 26759, "DatasetName", "0"], [27042, 27043, "DatasetName", "0"], [27090, 27091, "DatasetName", "0"], [27150, 27151, "DatasetName", "0"], [27168, 27169, "DatasetName", "0"], [27187, 27188, "DatasetName", "0"], [27292, 27293, "DatasetName", "0"], [27300, 27301, "DatasetName", "0"], [27343, 27344, "DatasetName", "0"], [27618, 27619, "DatasetName", "0"], [27678, 27679, "DatasetName", "0"], [27696, 27697, "DatasetName", "0"], [27715, 27716, "DatasetName", "0"], [27820, 27821, "DatasetName", "0"], [27828, 27829, "DatasetName", "0"], [27871, 27872, "DatasetName", "0"], [28085, 28086, "DatasetName", "0"], [28110, 28111, "DatasetName", "0"], [28129, 28130, "DatasetName", "0"], [28146, 28147, "DatasetName", "0"], [28233, 28234, "DatasetName", "0"], [28295, 28296, "DatasetName", "0"], [28312, 28313, "DatasetName", "0"], [28381, 28382, "DatasetName", "0"], [28519, 28520, "DatasetName", "0"], [28543, 28544, "DatasetName", "0"], [28545, 28546, "DatasetName", "0"], [28589, 28590, "DatasetName", "0"], [28633, 28634, "DatasetName", "0"], [28652, 28653, "DatasetName", "0"], [28669, 28670, "DatasetName", "0"], [28697, 28698, "DatasetName", "0"], [28706, 28707, "DatasetName", "0"], [28798, 28799, "DatasetName", "0"], [29042, 29043, "DatasetName", "0"], [29066, 29067, "DatasetName", "0"], [29068, 29069, "DatasetName", "0"], [29112, 29113, "DatasetName", "0"], [29156, 29157, "DatasetName", "0"], [29175, 29176, "DatasetName", "0"], [29192, 29193, "DatasetName", "0"], [29598, 29599, "DatasetName", "0"], [29628, 29629, "DatasetName", "0"], [29815, 29816, "DatasetName", "0"], [29832, 29833, "DatasetName", "0"], [29857, 29858, "DatasetName", "0"], [29868, 29869, "DatasetName", "0"], [29939, 29940, "DatasetName", "0"], [30110, 30111, "DatasetName", "0"], [30140, 30141, "DatasetName", "0"], [30217, 30218, "DatasetName", "0"], [30234, 30235, "DatasetName", "0"], [30259, 30260, "DatasetName", "0"], [30270, 30271, "DatasetName", "0"], [30341, 30342, "DatasetName", "0"], [30512, 30513, "DatasetName", "0"], [30542, 30543, "DatasetName", "0"], [30729, 30730, "DatasetName", "0"], [30746, 30747, "DatasetName", "0"], [30771, 30772, "DatasetName", "0"], [30782, 30783, "DatasetName", "0"], [30853, 30854, "DatasetName", "0"], [31024, 31025, "DatasetName", "0"], [31054, 31055, "DatasetName", "0"], [31241, 31242, "DatasetName", "0"], [31258, 31259, "DatasetName", "0"], [31283, 31284, "DatasetName", "0"], [31294, 31295, "DatasetName", "0"], [31365, 31366, "DatasetName", "0"], [31536, 31537, "DatasetName", "0"], [31566, 31567, "DatasetName", "0"], [31753, 31754, "DatasetName", "0"], [31770, 31771, "DatasetName", "0"], [31795, 31796, "DatasetName", "0"], [31806, 31807, "DatasetName", "0"], [31877, 31878, "DatasetName", "0"], [32048, 32049, "DatasetName", "0"], [32078, 32079, "DatasetName", "0"], [32265, 32266, "DatasetName", "0"], [32282, 32283, "DatasetName", "0"], [32307, 32308, "DatasetName", "0"], [32318, 32319, "DatasetName", "0"], [32389, 32390, "DatasetName", "0"], [32469, 32471, "HyperparameterName", "k ="], [32491, 32492, "DatasetName", "0"], [32537, 32538, "DatasetName", "0"], [32745, 32746, "DatasetName", "0"], [32762, 32763, "DatasetName", "0"], [32787, 32788, "DatasetName", "0"], [32798, 32799, "DatasetName", "0"], [32964, 32965, "DatasetName", "0"], [32967, 32968, "DatasetName", "0"], [33035, 33036, "DatasetName", "0"], [33109, 33110, "DatasetName", "0"], [33204, 33205, "DatasetName", "0"], [33244, 33245, "DatasetName", "0"], [33256, 33257, "DatasetName", "0"], [33296, 33297, "DatasetName", "0"], [33373, 33374, "DatasetName", "0"], [33430, 33431, "MethodName", "LSTM"], [33443, 33445, "TaskName", "Text Generation"], [33467, 33468, "MethodName", "LSTM"], [33546, 33547, "MethodName", "LSTM"], [33666, 33667, "MethodName", "LSTM"], [33683, 33684, "MethodName", "softmax"], [33738, 33739, "MethodName", "softmax"], [33833, 33834, "DatasetName", "0"], [34018, 34019, "MethodName", "LSTM"]]}
{"text": "As in many RL - based text - generation methods , such as SeqGAN and LeakGAN ( Guo et al , 2017 ) , the generator is updated based on policy - gradient methods . As a result , collecting rewards in the generation process is critical . Though SeqGAN has proposed to use rollout to get rewards for each generated word , the variance of the rewards is typically too high to be useful practically . In addition , the computational cost may be too high for practical use . We below describe how to use the proposed guider network to define intermediate rewards , leading to a definition of feature - matching reward . Feature - Matching Rewards We first define an intermediate reward to generate a particular word . The idea is to match the ground - truth features from the CNN encoder in Figure 1 with those generated from the guider network . Equation ( 8 ) indicates that the further the generated feature is from the true feature , the smaller the reward should be . To this end , for each time t , we define the intermediate reward for generating the current word as : r g t = 1 2c c i=1 ( D cos ( f t , f t ) + D cos ( f t \u2212 f t\u2212i , f t \u2212 f t\u2212i ) ) , wheref t = G \u03c8 ( s G t\u2212c\u22121 , f t\u2212c ) is the predicted feature . Intuitively , f t \u2212 f t\u2212i measures the difference between the generated sentences in feature space ; the reward is high if it matches the predicted feature transitionf t \u2212 f t\u2212i from the guider network . At the last step of text generation , i.e. , t = T , the corresponding reward measures the quality of the whole generated sentence , thus it is called a final reward . The final reward is defined differently from the intermediate reward , discussed below for both the unconditional - and conditional - generation cases . Note that a token generated at time t will influence not only the rewards received at that time but also the rewards at subsequent time steps . Thus we propose to define the cumulative reward , T i = t \u03b3 i r g i with \u03b3 a discount factor , as a featurematching reward . Intuitively , this encourages the generator to focus on achieving higher long - term rewards . Finally , in order to apply policy gradient to update the generator , we combine the featurematching reward with the problem - specific final reward , to form a Q - value reward specified below . Similar to SeqGAN , the final reward is defined as the output of a discriminator , evaluating the quality of the whole generated sentence , i.e. , the smaller the output , the less likely the generation is a true sentence . As a result , we combine the adversarial reward r f [ 0 , 1 ] by the discriminator ( Yu et al Generate a sequence Y 1 ... T \u223c \u03c0 \u03c6 .", "entities": [[299, 301, "TaskName", "text generation"], [392, 393, "HyperparameterName", "\u03b3"], [398, 399, "HyperparameterName", "\u03b3"], [514, 515, "DatasetName", "0"]]}
{"text": "Compute Q t , and update \u03c0 \u03c6 . 8 : end while 2017 ) with the guider - matching rewards , to define a Q - value reward as Q t = ( T i = t \u03b3 i r g i ) \u00d7 r f . Generator Optimization The generator is initialized by pre - training on sentences with an autoencoder structure , based on MLE training . After that , the final Q - value reward Q t is used as a reward for each time t , with standard policy gradient optimization methods to update the generator . Specifically , the policy gradient is \u2207 \u03c6 J = E ( s t\u22121 , yt ) \u223c\u03c1\u03c0 [ Q t \u2207 \u03c6 log p ( y t | s t\u22121 ; \u03c6 , \u03d5 ) ] , \u2207 \u03d5 J = E ( s t\u22121 , yt ) \u223c\u03c1\u03c0 [ Q t \u2207 \u03d5 log p ( y t | s t\u22121 ; \u03c6 , \u03d5 ) ] , where p ( y t | s t\u22121 ; \u03c6 , \u03d5 ) is the probability of generating y t given s t\u22121 in the generator . Algorithm 1 describes the proposed model - based imitation learning framework for text generation . Model - based or Model - free Text generation seeks to generate the next word ( action ) given the current ( sub - ) sentence ( state ) . The generator is considered as an agent that learns a policy to predict the next word given its current state . In previous work ( Ranzato et al , 2016 ) , a metric reward is given and the generator is trained to only maximize the metric reward by trial , thus this is model - free learning . In the proposed method , the guider network models the environment dynamics , and is trained by minimizing the cosine similarity between the prediction and the ground truth on real text . For generator training , it maximizes the reward which is determined by the metric and guider network , and thus is model - free learning with model - based boosting ( Gu et al , 2016 ) . The model predictive control scheme is included in our method , where the guider network is used to help next - word selection at each time - step . I x L 1 g O O U B z E d K B E J R t F K r X E v x z N v 0 q v W 3 L o 7 A 1 k m X k F q U K D Z q 3 5 1 + w n L Y q 6 Q S W p M x 3 N T D H K q U T D J J 5 V u Z n h K 2 Y g O e M d S R W N u g n x 2 7 o S c W K V P o k T b U k h m 6 u + J n M b G j O P Q d s Y U h 2 b R m 4 r / e Z 0 M o 6 s g F y r N k C s 2 X x R l k m B C p r + T v t C c o R x b Q p k W 9 l b C h l R T h j a h i g 3 B W 3 x 5 m f j n 9 e u 6 d 3 9 R a 9 w U a Z T h C I 7 h F D y 4 h A b c Q R N 8 Y D C C Z 3 i F N y d 1 X p x 3 5 2 P e W n K K m U P 4 A + f z B 3 1 D j z Q = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 a k G l X l B 3 d d W H T a 2 P N m t m K s T F H I = \" > A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g n o r e v F Y w d h C G 8 p m u 2 m X b j Z h d y K U 0 B / h x Y O K V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q F Q d f 9 d k o r q 2 v r G + X N y t b 2 z u 5 e d f / g 0 S S Z Z t x n i U x 0 O 6 S G S 6 G 4 j w I l b 6 e a 0 z i U v B W O b q d + 6 4 l r I x L 1 g O O U B z E d K B E J R t F K r X E v x z N v 0 q v W 3 L o 7 A 1 k m X k F q U K D Z q 3 5 1 + w n L Y q 6 Q S W p M x 3 N T D H K q U T D J J 5 V u Z n h K 2 Y g O e M d S R W N u g n x 2 7 o S c W K V P o k T b U k h m 6 u + J n M b G j O P Q d s Y U h 2 b R m 4 r / e Z 0 M o 6 s g F y r N k C s 2 X x R l k m B C p r + T v t C c o R x b Q p k W 9 l b C h l R T h j a h i g 3 B W 3 x 5 m f j n 9 e u 6 d 3 9 R a 9 w U a Z T h C I 7 h F D y 4 h A b c Q R N 8 Y D C C Z 3 i F N y d 1 X p x 3 5 2 P e W n K K m U P 4 A + f z B 3 1 D j z Q = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 a k G l X l B 3 d d W H T a 2 P N m t m K s T F H I = \" > A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g n o r e v F Y w d h C G 8 p m u 2 m X b j Z h d y K U 0 B / h x Y O K V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q F Q d f 9 d k o r q 2 v r G + X N y t b 2 z u 5 e d f / g 0 S S Z Z t x n i U x 0 O 6 S G S 6 G 4 j w I l b 6 e a 0 z i U v B W O b q d + 6 4 l r I x L 1 g O O U B z E d K B E J R t F K r X E v x z N v 0 q v W 3 L o 7 A 1 k m X k F q U K D Z q 3 5 1 + w n L Y q 6 Q S W p M x 3 N T D H K q U T D J J 5 V u Z n h K 2 Y g O e M d S R W N u g n x 2 7 o S c W K V P o k T b U k h m 6 u + J n M b G j O P Q d s Y U h 2 b R m 4 r / e Z 0 M o 6 s g F y r N k C s 2 X x R l k m B C p r + T v t C c o R x b Q p k W 9 l b C h l R T h j a h i g 3 B W 3 x 5 m f j n 9 e u 6 d 3 9 R a 9 w U a Z T h C I 7 h F D y 4 h A b c Q R N 8 Y D C C Z 3 i F N y d 1 X p x 3 5 2 P e W n K K m U P 4 A + f z B 3 1 D j z Q = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 a k G l X l B 3 d d W H T a 2 P N m t m K s T F H I = \" > A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g n o r e v F Y w d h C G 8 p m u 2 m X b j Z h d y K U 0 B / h x Y O K V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q F Q d f 9 d k o r q 2 v r G + X N y t b 2 z u 5 e d f / g 0 S S Z Z t x n i U x 0 O 6 S G S 6 G 4 j w I l b 6 e a 0 z i U v B W O b q d + 6 4 l r I x L 1 g O O U B z E d K B E J R t F K r X E v x z N v 0 q v W 3 L o 7 A 1 k m X k F q U K D Z q 3 5 1 + w n L Y q 6 Q S W p M x 3 N T D H K q U T D J J 5 V u Z n h K 2 Y g O e M d S R W N u g n x 2 7 o S c W K V P o k T b U k h m 6 u + J n M b G j O P Q d s Y U h 2 b R m 4 r / e Z 0 M o 6 s g F y r N k C s 2 X x R l k m B C p r + T v t C c o R x b Q p k W 9 l b C h l R T h j a h i g 3 B W 3 x 5 m f j n 9 e u 6 d 3 9 R a 9 w U a Z T h C I 7 h F D y 4 h A b c Q R N 8 Y D C C Z 3 i F N y d 1 X p x 3 5 2 P e W n K K m U P 4 A + f z B 3 1 D j z Q = < / l a t e x i t > w t < l a t e x i t s h a 1 _ b a s e 6 4 = \" q 3 U T 9 y t x i 6 J a G M 3 Y 4 F l 9 w H X p O G M = \" > A A A B 6 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I 6 q 3 o x W N F Y w t t K J v t p l 2 6 2 Y T d i V J C f 4 I X D y p e / U f e / D d u 2 x y 0 9 c H A 4 7 0 Z Z u a F q R Q G X f f b W V p e W V 1 b L 2 2 U N 7 e 2 d 3 Y r e / s P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W c x q H k z X B 4 P f G b j 1 w b k a h 7 H K U 8 i G l f i U g w i l a 6 e + p i t 1 J 1 a + 4 U Z J F 4 B a l C g U a 3 8 t X p J S y L u U I m q T F t z 0 0 x y K l G w S Q f l z u Z 4 S l l Q 9 r n b U s V j b k J 8 u m p Y 3 J s l R 6 J E m 1 L I Z m q v y d y G h s z i k P b G V M c m H l v I v 7 n t T O M L o J c q D R D r t h s U Z R J g g m Z / E 1 6 Q n O G c m Q J Z V r Y W w k b U E 0 Z 2 n T K N g R v / u V F 4 p / W L m v e 7 V m 1 f l W k U Y J D O I I T 8 O A c 6 n A D D f C B Q R + e 4 R X e H O m 8 O O / O x 6 x 1 y S l m D u A P n M 8 f 3 T m N t g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" q 3 U T 9 y t x i 6 J a G M 3 Y 4 F l 9 w H X p O G M = \" > A A A B 6 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I 6 q 3 o x W N F Y w t t K J v t p l 2 6 2 Y T d i V J C f 4 I X D y p e / U f e / D d u 2 x y 0 9 c H A 4 7 0 Z Z u a F q R Q G X f f b W V p e W V 1 b L 2 2 U N 7 e 2 d 3 Y r e / s P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W c x q H k z X B 4 P f G b j 1 w b k a h 7 H K U 8 i G l f i U g w i l a 6 e + p i t 1 J 1 a + 4 U Z J F 4 B a l C g U a 3 8 t X p J S y L u U I m q T F t z 0 0 x y K l G w S Q f l z u Z 4 S l l Q 9 r n b U s V j b k J 8 u m p Y 3 J s l R 6 J E m 1 L I Z m q v y d y G h s z i k P b G V M c m H l v I v 7 n t T O M L o J c q D R D r t h s U Z R J g g m Z / E 1 6 Q n O G c m Q J Z V r Y W w k b U E 0 Z 2 n T K N g R v / u V F 4 p / W L m v e 7 V m 1 f l W k U Y J D O I I T 8 O A c 6 n A D D f C B Q R + e 4 R X e H O m 8 O O / O x 6 x 1 y S l m D u A P n M 8 f 3 T m N t g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" q 3 U T 9 y t x i 6 J a G M 3 Y 4 F l 9 w H X p O G M = \" > A A A B 6 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I 6 q 3 o x W N F Y w t t K J v t p l 2 6 2 Y T d i V J C f 4 I X D y p e / U f e / D d u 2 x y 0 9 c H A 4 7 0 Z Z u a F q R Q G X f f b W V p e W V 1 b L 2 2 U N 7 e 2 d 3 Y r e / s P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W c x q H k z X B 4 P f G b j 1 w b k a h 7 H K U 8 i G l f i U g w i l a 6 e + p i t 1 J 1 a + 4 U Z J F 4 B a l C g U a 3 8 t X p J S y L u U I m q T F t z 0 0 x y K l G w S Q f l z u Z 4 S l l Q 9 r n b U s V j b k J 8 u m p Y 3 J s l R 6 J E m 1 L I Z m q v y d y G h s z i k P b G V M c m H l v I v 7 n t T O M L o J c q D R D r t h s U Z R J g g m Z / E 1 6 Q n O G c m Q J Z V r Y W w k b U E 0 Z 2 n T K N g R v / u V F 4 p / W L m v e 7 V m 1 f l W k U Y J D O I I T 8 O A c 6 n A D D f C B Q R + e 4 R X e H O m 8 O O / O x 6 x 1 y S l m D u A P n M 8 f 3 T m N t g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" X / B b P P Q R M 1 p m B h x d K 1 e n S b L + g J w = \" > A A A B 2 H i c b Z D N S g M x F I X v 1 L 8 6 V q 1 r N 8 E i u C o z b t S d 4 M Z l B c c W 2 q F k M n f a 0 E x m S O 4 I p f Q F X L h R f D B 3 v o 3 p z 0 K t B w I f 5 y T k 3 p O U S l o K g i + v t r W 9 s 7 t X 3 / c P G v 7 h 0 X G z 8 W S L y g i M R K E K 0 0 u 4 R S U 1 R i R J Y a 8 0 y P N E Y T e Z 3 C 3 y 7 j M a K w v 9 S N M S 4 5 y P t M y k 4 O S s z r D Z C t r B U m w T w j W 0 Y K 1 h 8 3 O Q F q L K U Z N Q 3 N p + G J Q U z 7 g h K R T O / U F l s e R i w k f Y d 6 h 5 j j a e L c e c s 3 P n p C w r j D u a 2 N L 9 + W L G c 2 u n e e J u 5 p z G 9 m + 2 M P / L + h V l 1 / F M 6 r I i 1 G L 1 U V Y p R g V b 7 M x S a V C Q m j r g w k g 3 K x N j b r g g 1 4 z v O g j / b r w J 0 W X 7 p h 0 + B F C H U z i D C w j h C m 7 h H j o Q g Y A U X u D N G 3 u v 3 v u q q p q 3 7 u w E f s n 7 + A a q K Y o N < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 8 L i q q k O r 4 y R e y v W h W M T n L M j k C 0 c = \" > A A A B 3 n i c b Z D N S g M x F I X v + F t r 1 e r W T b A I r s q M G 3 U n u H F Z 0 b G F d i i Z 9 E 4 b m s k M y R 2 l l D 6 C G x c q P p Y 7 3 8 b 0 Z 6 G t B w I f 5 y T k 3 h P n S l r y / W 9 v b X 1 j c 2 u 7 t F P e r e z t H 1 Q P K 4 8 2 K 4 z A U G Q q M 6 2 Y W 1 R S Y 0 i S F L Z y g z y N F T b j 4 c 0 0 b z 6 h s T L T D z T K M U p 5 X 8 t E C k 7 O u n / u U r d a 8 + v + T G w V g g X U Y K F G t / r V 6 W W i S F G T U N z a d u D n F I 2 5 I S k U T s q d w m L O x Z D 3 s e 1 Q 8 x R t N J 6 N O m G n z u m x J D P u a G I z 9 / e L M U + t H a W x u 5 l y G t j l b G r + l 7 U L S i 6 j s d R 5 Q a j F / K O k U I w y N t 2 b 9 a R B Q W r k g A s j 3 a x M D L j h g l w 7 Z V d C s L z y K o T n 9 a t 6 c O d D C Y 7 h B M 4 g g A u 4 h l t o Q A g C + v A C b / D u K e / V + 5 i 3 t e Y t a j u C P / I + f w D A + Y x i < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 8 L i q q k O r 4 y R e y v W h W M T n L M j k C 0 c = \" > A A A B 3 n i c b Z D N S g M x F I X v + F t r 1 e r W T b A I r s q M G 3 U n u H F Z 0 b G F d i i Z 9 E 4 b m s k M y R 2 l l D 6 C G x c q P p Y 7 3 8 b 0 Z 6 G t B w I f 5 y T k 3 h P n S l r y / W 9 v b X 1 j c 2 u 7 t F P e r e z t H 1 Q P K 4 8 2 K 4 z A U G Q q M 6 2 Y W 1 R S Y 0 i S F L Z y g z y N F T b j 4 c 0 0 b z 6 h s T L T D z T K M U p 5 X 8 t E C k 7 O u n / u U r d a 8 + v + T G w V g g X U Y K F G t / r V 6 W W i S F G T U N z a d u D n F I 2 5 I S k U T s q d w m L O x Z D 3 s e 1 Q 8 x R t N J 6 N O m G n z u m x J D P u a G I z 9 / e L M U + t H a W x u 5 l y G t j l b G r + l 7 U L S i 6 j s d R 5 Q a j F / K O k U I w y N t 2 b 9 a R B Q W r k g A s j 3 a x M D L j h g l w 7 Z V d C s L z y K o T n 9 a t 6 c O d D C Y 7 h B M 4 g g A u 4 h l t o Q A g C + v A C b / D u K e / V + 5 i 3 t e Y t a j u C P / I + f w D A + Y x i < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 8 T P j 8 p W 5 m g 7 r I V N u D P D 9 n l C 0 6 8 k = \" > A A A B 6 X i c b V B N T 8 J A E J 3 i F + I X 6 t H L R m L i i b R e 1 B v R i 0 e M V k i g I d t l C x u 2 2 2 Z 3 q i E N P 8 G L B z V e / U f e / D c u 0 I O C L 5 n k 5 b 2 Z z M w L U y k M u u 6 3 U 1 p Z X V v f K G 9 W t r Z 3 d v e q + w c P J s k 0 4 z 5 L Z K L b I T V c C s V 9 F C h 5 O 9 W c x q H k r X B 0 P f V b j 1 w b k a h 7 H K c 8 i O l A i U g w i l a 6 e + p h r 1 p z 6 + 4 M Z J l 4 B a l B g W a v + t X t J y y L u U I m q T E d z 0 0 x y K l G w S S f V L q Z 4 S l l I z r g H U s V j b k J 8 t m p E 3 J i l T 6 J E m 1 L I Z m p v y d y G h s z j k P b G V M c m k V v K v 7 n d T K M L o J c q D R D r t h 8 U Z R J g g m Z / k 3 6 Q n O G c m w J Z V r Y W w k b U k 0 Z 2 n Q q N g R v 8 e V l 4 p / V L + v e r V t r X B V p l O E I j u E U P D i H B t x A E 3 x g M I B n e I U 3 R z o v z r v z M W 8 t O c X M I f y B 8 / k D 2 / m N s g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" q 3 U T 9 y t x i 6 J a G M 3 Y 4 F l 9 w H X p O G M = \" > A A A B 6 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I 6 q 3 o x W N F Y w t t K J v t p l 2 6 2 Y T d i V J C f 4 I X D y p e / U f e / D d u 2 x y 0 9 c H A 4 7 0 Z Z u a F q R Q G X f f b W V p e W V 1 b L 2 2 U N 7 e 2 d 3 Y r e / s P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W c x q H k z X B 4 P f G b j 1 w b k a h 7 H K U 8 i G l f i U g w i l a 6 e + p i t 1 J 1 a + 4 U Z J F 4 B a l C g U a 3 8 t X p J S y L u U I m q T F t z 0 0 x y K l G w S Q f l z u Z 4 S l l Q 9 r n b U s V j b k J 8 u m p Y 3 J s l R 6 J E m 1 L I Z m q v y d y G h s z i k P b G V M c m H l v I v 7 n t T O M L o J c q D R D r t h s U Z R J g g m Z / E 1 6 Q n O G c m Q J Z V r Y W w k b U E 0 Z 2 n T K N g R v / u V F 4 p / W L m v e 7 V m 1 f l W k U Y J D O I I T 8 O A c 6 n A D D f C B Q R + e 4 R X e H O m 8 O O / O x 6 x 1 y S l m D u A P n M 8 f 3 T m N t g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" q 3 U T 9 y t x i 6 J a G M 3 Y 4 F l 9 w H X p O G M = \" > A A A B 6 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I 6 q 3 o x W N F Y w t t K J v t p l 2 6 2 Y T d i V J C f 4 I X D y p e / U f e / D d u 2 x y 0 9 c H A 4 7 0 Z Z u a F q R Q G X f f b W V p e W V 1 b L 2 2 U N 7 e 2 d 3 Y r e / s P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W c x q H k z X B 4 P f G b j 1 w b k a h 7 H K U 8 i G l f i U g w i l a 6 e + p i t 1 J 1 a + 4 U Z J F 4 B a l C g U a 3 8 t X p J S y L u U I m q T F t z 0 0 x y K l G w S Q f l z u Z 4 S l l Q 9 r n b U s V j b k J 8 u m p Y 3 J s l R 6 J E m 1 L I Z m q v y d y G h s z i k P b G V M c m H l v I v 7 n t T O M L o J c q D R D r t h s U Z R J g g m Z / E 1 6 Q n O G c m Q J Z V r Y W w k b U E 0 Z 2 n T K N g R v / u V F 4 p / W L m v e 7 V m 1 f l W k U Y J D O I I T 8 O A c 6 n A D D f C B Q R + e 4 R X e H O m 8 O O / O x 6 x 1 y S l m D u A P n M 8 f 3 T m N t g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" q 3 U T 9 y t x i 6 J a G M 3 Y 4 F l 9 w H X p O G M = \" > A A A B 6 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I 6 q 3 o x W N F Y w t t K J v t p l 2 6 2 Y T d i V J C f 4 I X D y p e / U f e / D d u 2 x y 0 9 c H A 4 7 0 Z Z u a F q R Q G X f f b W V p e W V 1 b L 2 2 U N 7 e 2 d 3 Y r e / s P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W c x q H k z X B 4 P f G b j 1 w b k a h 7 H K U 8 i G l f i U g w i l a 6 e + p i t 1 J 1 a + 4 U Z J F 4 B a l C g U a 3 8 t X p J S y L u U I m q T F t z 0 0 o K g i + v t r W 9 s 7 t X 3 / c P G v 7 h 0 X G z 8 W S L y g i M R K E K 0 0 u 4 R S U 1 R i R J Y a 8 0 y P N E Y T e Z 3 C 3 y 7 j M a K w v 9 S N M S 4 5 y P t M y k 4 O S s z r D Z C t r B U m w T w j W 0 Y K 1 h 8 3 O Q F q L K U Z N Q 3 N p + G J Q U z 7 g h K R T O / U F l s e R i w k f Y d 6 h 5 j j a e L c e c s 3 P n p C w r j D u a 2 N L 9 + W L G c 2 u n e e J u 5 p z G 9 m + 2 M P / L + h V l 1 / F M 6 r I i 1 G L 1 U V Y p R g V b 7 M x S a V C Q m j r g w k g 3 K x N j b r g g 1 4 z v O g j / b r w J 0 W X 7 p h 0 + B F C H U z i D C w j h C m 7 h H j o Q g Y A U X u D N G 3 u v 3 v u q q p q 3 7 u w E f s n 7 + A a q K Y o N < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" Y f B p l r X h t V g c d Z q 6 r a 0 D R Y N h O c A = \" > A A A B 4 H i c b Z D N S g M x F I X v + F t r 1 e r W T b A I r s q M G 3 U n u N B l B c c W 2 r F k 0 j t t a C Y z J H e E U v o M b l y o + F T u f B v T n 4 W 2 H g h 8 n J O Q e 0 + c K 2 n J 9 7 + 9 t f W N z a 3 t 0 k 5 5 t 7 K 3 f 1 A 9 r D z a r D A C Q 5 G p z L R i b l F J j S F J U t j K D f I 0 V t i M h z f T v P m M x s p M P 9 A o x y j l f S 0 T K T g 5 K 0 y 6 9 H T b r d b 8 u j 8 T W 4 V g A T V Y q N G t f n V 6 m S h S 1 C Q U t 7 Y d + D l F Y 2 5 I C o W T c q e w m H M x 5 H 1 s O 9 Q 8 R R u N Z 8 N O 2 K l z e i z J j D u a 2 M z 9 / W L M U 2 t H a e x u p p w G d j m b m v 9 l 7 Y K S y 2 g s d V 4 Q a j H / K C k U o 4 x N N 2 c 9 a V C Q G j n g w k g 3 K x M D b r g g 1 0 / Z l R A s r 7 w K 4 X n 9 q h 7 c + 1 C C Y z i B M w j g A q 7 h D h o Q g g A J L / A G 7 5 7 2 X r 2 P e V t r 3 q K 2 I / g j 7 / M H 5 m a N C A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" Y f B p l r X h t V g c d Z q 6 r a 0 D R Y N h O c A = \" > A A A B 4 H i c b Z D N S g M x F I X v + F t r 1 e r W T b A I r s q M G 3 U n u N B l B c c W 2 r F k 0 j t t a C Y z J H e E U v o M b l y o + F T u f B v T n 4 W 2 H g h 8 n J O Q e 0 + c K 2 n J 9 7 + 9 t f W N z a 3 t 0 k 5 5 t 7 K 3 f 1 A 9 r D z a r D A C Q 5 G p z L R i b l F J j S F J U t j K D f I 0 V t i M h z f T v P m M x s p M P 9 A o x y j l f S 0 T K T g 5 K 0 y 6 9 H T b r d b 8 u j 8 T W 4 V g A T V Y q N G t f n V 6 m S h S 1 C Q U t 7 Y d + D l F Y 2 5 I C o W T c q e w m H M x 5 H 1 s O 9 Q 8 R R u N Z 8 N O 2 K l z e i z J j D u a 2 M z 9 / W L M U 2 t H a e x u p p w G d j m b m v 9 l 7 Y K S y 2 g s d V 4 Q a j H / K C k U o 4 x N N 2 c 9 a V C Q G j n g w k g 3 K x M D b r g g 1 0 / Z l R A s r 7 w K 4 X n 9 q h 7 c + 1 C C Y z i B M w j g A q 7 h D h o Q g g A J L / A G 7 5 7 2 X r 2 P e V t r 3 q K 2 I / g j 7 / M H 5 m a N C A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" y o u q m G / h y L p 9 / 9 P M T / p R g i x y K l G w S Q f l z u Z 4 S l l Q 9 r n b U s V j b k J 8 u m p Y 3 J s l R 6 J E m 1 L I Z m q v y d y G h s z i k P b G V M c m H l v I v 7 n t T O M L o J c q D R D r t h s U Z R J g g m Z / E 1 6 Q n O G c m Q J Z V r Y W w k b U E 0 Z 2 n T K N g R v / u V F 4 p / W L m v e 7 V m 1 f l W k U Y", "entities": [[38, 39, "HyperparameterName", "\u03b3"], [62, 63, "MethodName", "autoencoder"], [207, 209, "TaskName", "imitation learning"], [211, 213, "TaskName", "text generation"], [221, 223, "TaskName", "Text generation"], [250, 251, "DatasetName", "agent"], [430, 431, "DatasetName", "0"], [544, 545, "DatasetName", "0"], [803, 804, "DatasetName", "0"], [825, 826, "DatasetName", "0"], [867, 868, "DatasetName", "0"], [878, 879, "DatasetName", "0"], [895, 896, "DatasetName", "0"], [939, 940, "DatasetName", "0"], [1053, 1054, "DatasetName", "0"], [1312, 1313, "DatasetName", "0"], [1334, 1335, "DatasetName", "0"], [1376, 1377, "DatasetName", "0"], [1387, 1388, "DatasetName", "0"], [1404, 1405, "DatasetName", "0"], [1448, 1449, "DatasetName", "0"], [1562, 1563, "DatasetName", "0"], [1821, 1822, "DatasetName", "0"], [1843, 1844, "DatasetName", "0"], [1885, 1886, "DatasetName", "0"], [1896, 1897, "DatasetName", "0"], [1913, 1914, "DatasetName", "0"], [1957, 1958, "DatasetName", "0"], [2071, 2072, "DatasetName", "0"], [2351, 2352, "DatasetName", "0"], [2358, 2359, "DatasetName", "0"], [2399, 2400, "DatasetName", "0"], [2501, 2502, "DatasetName", "0"], [2502, 2503, "DatasetName", "0"], [2622, 2623, "DatasetName", "0"], [2856, 2857, "DatasetName", "0"], [2863, 2864, "DatasetName", "0"], [2904, 2905, "DatasetName", "0"], [3006, 3007, "DatasetName", "0"], [3007, 3008, "DatasetName", "0"], [3127, 3128, "DatasetName", "0"], [3361, 3362, "DatasetName", "0"], [3368, 3369, "DatasetName", "0"], [3409, 3410, "DatasetName", "0"], [3511, 3512, "DatasetName", "0"], [3512, 3513, "DatasetName", "0"], [3632, 3633, "DatasetName", "0"], [3839, 3840, "DatasetName", "0"], [3864, 3865, "DatasetName", "0"], [3903, 3904, "DatasetName", "0"], [3919, 3920, "DatasetName", "0"], [3920, 3921, "DatasetName", "0"], [3934, 3935, "DatasetName", "0"], [3986, 3987, "DatasetName", "0"], [4143, 4144, "DatasetName", "0"], [4149, 4150, "DatasetName", "0"], [4263, 4264, "DatasetName", "0"], [4312, 4313, "DatasetName", "0"], [4347, 4348, "DatasetName", "0"], [4413, 4414, "DatasetName", "0"], [4430, 4431, "DatasetName", "0"], [4431, 4432, "DatasetName", "0"], [4756, 4757, "DatasetName", "0"], [4805, 4806, "DatasetName", "0"], [4840, 4841, "DatasetName", "0"], [4906, 4907, "DatasetName", "0"], [4923, 4924, "DatasetName", "0"], [4924, 4925, "DatasetName", "0"], [5247, 5248, "DatasetName", "0"], [5250, 5252, "HyperparameterName", "k ="], [5295, 5296, "DatasetName", "0"], [5334, 5335, "DatasetName", "0"], [5385, 5386, "DatasetName", "0"], [5417, 5418, "DatasetName", "0"], [5487, 5488, "DatasetName", "0"], [5488, 5489, "DatasetName", "0"], [5608, 5609, "DatasetName", "0"], [5842, 5843, "DatasetName", "0"], [5849, 5850, "DatasetName", "0"], [5890, 5891, "DatasetName", "0"], [5992, 5993, "DatasetName", "0"], [5993, 5994, "DatasetName", "0"], [6113, 6114, "DatasetName", "0"], [6347, 6348, "DatasetName", "0"], [6354, 6355, "DatasetName", "0"], [6395, 6396, "DatasetName", "0"], [6497, 6498, "DatasetName", "0"], [6498, 6499, "DatasetName", "0"], [6618, 6619, "DatasetName", "0"], [6852, 6853, "DatasetName", "0"], [6859, 6860, "DatasetName", "0"], [6900, 6901, "DatasetName", "0"], [7002, 7003, "DatasetName", "0"], [7003, 7004, "DatasetName", "0"], [7026, 7027, "DatasetName", "0"], [7042, 7043, "DatasetName", "0"], [7043, 7044, "DatasetName", "0"], [7057, 7058, "DatasetName", "0"], [7109, 7110, "DatasetName", "0"], [7266, 7267, "DatasetName", "0"], [7272, 7273, "DatasetName", "0"], [7379, 7380, "DatasetName", "0"], [7443, 7444, "DatasetName", "0"], [7484, 7485, "DatasetName", "0"], [7503, 7504, "DatasetName", "0"], [7546, 7547, "DatasetName", "0"], [7573, 7574, "DatasetName", "0"], [7580, 7581, "DatasetName", "0"], [7748, 7749, "DatasetName", "0"], [7876, 7877, "DatasetName", "0"], [7940, 7941, "DatasetName", "0"], [7981, 7982, "DatasetName", "0"], [8000, 8001, "DatasetName", "0"], [8043, 8044, "DatasetName", "0"], [8070, 8071, "DatasetName", "0"], [8077, 8078, "DatasetName", "0"], [8245, 8246, "DatasetName", "0"], [8496, 8497, "DatasetName", "0"]]}
{"text": "We first review related works that combine RL and GAN for text generation . As one of the most rep - resentative models in this direction , SeqGAN adopts Monte - Carlo search to calculate rewards . However , such a method introduces high variance in policy optimization . There are a number of works proposed subsequently to improve the reward - generation process . For example , RankGAN ( Lin et al , 2017 ) proposes to replace the reward from the GAN discriminator with a rankingbased reward , MaliGAN ( Che et al , 2017 ) modifies the GAN objective and proposes techniques to reduce gradient variance , MaskGAN uses a filling technique to define a Q - value reward for sentence completion , RelGAN ( Nie et al , 2019 ) uses a relational memory based generator for the long - distance dependency modeling , FM - GAN uses a feature mover distance to match features of real and generated sentences inspired by optimal transport ( Chen et al , 2019 ; , and LeakGAN ( Guo et al , 2017 ) tries to address the sparse - reward issue for long - text generation with hierarchical RL by utilizing the leaked information from a GAN discriminator . One problem of LeakGAN is that it tends to overfit the training data , yielding generated sentences that are often not diverse . By contrast , by relying on a model - based imitation learning approach , our method learns global - structure information , which generates more - diverse sentences , and can be extended to conditional text generation . designed a differentiable nested Wasserstein distance for semantic matching , which can be applied for further improvement . RL techniques can also be used in other ways for text generation ( Bachman and Precup , 2015 ) . For example , Ranzato et al ( 2016 ) trained a Seq2Seq model by directly optimizing the BLEU / ROUGE scores with the REINFORCE algorithm . To reduce variance of the vanilla REINFORCE , Bahdanau et al ( 2017 ) adopted the actor - critic framework for sequence prediction . Furthermore , Rennie et al ( 2016 ) trained a baseline algorithm with a greedy decoding scheme for the REINFORCE method . Note that all these methods can only obtain reward after a whole sentence is generated . Planning techniques in RL have also been explored to improve text generation ( Gulcehre et al , 2017 ; Serdyuk et al , 2018 ) . introduced the selfimitation scheme to exploit historical high - quality sentences for enhanced exploration . Compared to these related works , the proposed guider network can provide a planning mechanism and intermediate rewards . generation . More details of GMGAN are provided in Appendix D.", "entities": [[9, 10, "MethodName", "GAN"], [11, 13, "TaskName", "text generation"], [83, 84, "MethodName", "GAN"], [100, 101, "MethodName", "GAN"], [123, 125, "TaskName", "sentence completion"], [150, 151, "MethodName", "GAN"], [196, 198, "TaskName", "text generation"], [208, 209, "MethodName", "GAN"], [244, 246, "TaskName", "imitation learning"], [268, 271, "TaskName", "conditional text generation"], [300, 302, "TaskName", "text generation"], [321, 322, "MethodName", "Seq2Seq"], [327, 328, "MetricName", "BLEU"], [333, 334, "MethodName", "REINFORCE"], [342, 343, "MethodName", "REINFORCE"], [379, 380, "MethodName", "REINFORCE"], [408, 410, "TaskName", "text generation"]]}
{"text": "We use the COCO Image Captions Dataset , in which most sentences have a length of about 10 words . Since we consider unconditional text generation , only image captions are used as the training data . After preprocessing , we use 120 , 000 random sample sentences as the training set , and 10 , 000 as the test set . The BLEU scores with different methods are listed in Table 1 . We observe that GM - GAN performs significantly better than the baseline models . Specifically , besides achieving higher test - BLEU scores , the proposed method also generates samples with very good diversity in terms of self - BLEU scores . LeakGAN represents the state - of - the - art in adversarial text generation , however , its diversity measurement is relatively poor ( Zhu et al , 2018 ) . We suspect that the high BLEU score achieved by LeakGAN is due to its mode collapse on some good samples , resulting in high self - BLEU scores . Other baselines achieve lower self - BLEU scores since they can not generate reasonable sentences . Long Text Generation : EMNLP2017 WMT Following ( Zhu et al , 2018 ) Human Evaluation Simply relying on the above metrics is not sufficient to evaluate the proposed method ( Caccia et al , 2018 ) . Following previous work ( Guo et al , 2017 ) , we perform human evaluations using Amazon Mechnical Turk , evaluating the text quality based on readability and meaningfulness ( whether sentences make sense ) on the EMNNLP2017 WMT News dataset . We ask the worker to rate the input sentence with scores scal - ing from 1 to 5 , with 1 as the worst score and 5 as the best . The detailed criteria is listed in Table 3 . We require all the workers to be native English speakers , with approval rate higher than 90 % and at least 100 assignments completed . We randomly sample 100 sentences generated by each model . Ten native English speakers on Amazon Mechanical Turk are asked to rate each sentence . The average human rating scores are shown in Table 4 , indicating GMGAN achieves higher human scores compared to other methods . As examples , Table 5 illustrates some generated samples by GMGAN and its baselines . The performance on the two datasets indicates that the generated sentences of GMGAN are of higher global consistency and better readability than SeqGAN and LeakGAN . More generated examples are provided in the Appendix .", "entities": [[3, 4, "DatasetName", "COCO"], [24, 26, "TaskName", "text generation"], [63, 64, "MetricName", "BLEU"], [79, 80, "MethodName", "GAN"], [95, 96, "MetricName", "BLEU"], [113, 114, "MetricName", "BLEU"], [127, 129, "TaskName", "adversarial text"], [152, 154, "MetricName", "BLEU score"], [173, 174, "MetricName", "BLEU"], [182, 183, "MetricName", "BLEU"], [193, 195, "TaskName", "Text Generation"]]}
{"text": "( 1 ) Bicycles are parked near a row of large trees near a sidewalk . ( 2 ) A married couple posing in front of a piece of birthday cake . ( 1 ) \" Sometimes decisions are big , but they 're easy to make , \" he told The Sunday Times in the New Year . ( 2 ) A BBC star has been questioned by police on suspicion of sexual assault against a 23 - year - old man , it was reported last night . It is grammatically wrong to select ' was ' for the generator , thus the guider network gives a small reward . We can see that the rewards become lower with more time steps , which is consistent with the exposure bias . Model Acc ( % ) BLEU BLEU - ref CVAE 73.9 20.7 7.8 Controllable ( Hu et al , 2017 ) 86.7 58.4 - BackTrans ( Prabhumoye et al , 2018 ) 91.2 2.8 2.0 DeleteAndRetrieval ( Li et al , 2018a ) 88.9 36.8 14.7 Guider ( Ours ) 92.7 52.1 25.4", "entities": [[134, 135, "MetricName", "Acc"], [138, 139, "MetricName", "BLEU"], [139, 140, "MetricName", "BLEU"], [142, 143, "MethodName", "CVAE"]]}
{"text": "We test the proposed framework on the non - parallel text - style - transfer task , where the goal is to transfer one sentence in one style ( e.g. , positive ) to a similar sentence but with a different style ( e.g. , negative ) . Pair - wise information should be inferred from the training data , which becomes more challenging . For a fair comparison , we use the same data and its split method as in . Specifically , there are 444 , 000 , 63 , 500 , and 127 , 000 sentences with either positive or negative sentiments in the training , validation and test sets , respectively . To measure whether the original sentences ( in the test set ) have been transferred to the desired sentiment , we follow the settings of and employ a pretrained CNN classifier , which achieves an accuracy of 97.4 % on the validation set , to evaluate the transferred sentences . We also report the BLEU scores with original sentences ( BLEU ) and human references ( BLEU - ref ) ( Li et al , 2018a ) , to evaluate the content preservation of transferred sentences . Results are summarized in Table 7 . Our proposed model exhibits higher transfer accuracy and better content preservation , indicating the guider network provides good sentiment guidance to better preserve the content information .", "entities": [[151, 152, "MetricName", "accuracy"], [170, 171, "MetricName", "BLEU"], [176, 177, "MetricName", "BLEU"], [182, 183, "MetricName", "BLEU"], [216, 217, "MetricName", "accuracy"]]}
{"text": "We conduct experiments on image captioning ( Karpathy and Fei - Fei , 2015 ) , investigating benefits brought by the Guider network . In image captioning , instead of using a discriminator to define final rewards for generated sentence , we adopt evaluation metrics computed based on human references . The final rewards appear more important as they contain reference ( ground - truth ) information . Feature - matching rewards work as a regularizer of the final rewards . We call our model in this setting a guider - matching sequence training ( GMST ) model . An overview of GMST is provided in the Appendix . We test our proposed model on the MS COCO dataset ( Karpathy and Fei - Fei , 2015 ) , containing 123 , 287 images in total . Each image is annotated with at least 5 captions . Following Karpathy 's split ( Karpathy and Fei - Fei , 2015 ) , 5 , 000 images are used for both validation and testing . We report BLEU - k ( k from 1 to 4 ) , CIDEr ( Vedantam et al , 2015 ) , and ME - TEOR ( Banerjee and Lavie , 2005 ) scores . We consider two settings : ( i ) using a pre - trained 152layer ResNet ( He et al , 2016 ) for feature extraction , where we take the output of the 2048 - way pool5 layer from ResNet - 152 , pretrained on the ImageNet dataset ; and ( ii ) using semantic tags detected from the image as features . We use an LSTM with 512 hidden units with mini - batches of size 64 . Adam ( Kingma and Ba , 2014 ) is used for optimization , with learning rate 2 \u00d7 10 \u22124 . We pretrain the captioning model for the maximum 20 epochs , then use the reinforcement learning to train it for 20 epochs and test on the best model on the validation set . The results are summarized in comparing an AutoEncoder ( AE ) with a variant implemented by adding a guider network ( Guider ) , improvements are observed . We compare the proposed GMST with SCST . Note the main difference between GMST and SCST is that the former employs our proposed feature - matching reward , while the latter only considers the final reward provided by evaluation metrics . GMST achieves higher scores compared with SCST on its optimized metrics . The gain of GMST compared with SCST comes from the immediate rewards , which can maintain the semantic consistency and sentence structure , preventing language - fluency damage caused by only focusing on evaluation metrics . Specifically , the average length of generated sentence with a Guider is 15.7 , and 12.9 for traditional generator . Comparison with MLE The guider network models the long - term dependency and overcome the issue of sparse reward inspired by model predictive control ( MPC ) . The experiments aim to quantify the gain when incorporating MPC for imitation learning , i.e. , MLE and RL finetune . We provide an additional comparison with Caccia et al ( 2018 ) and evaluate the diversity and quality with BLEU scores . We also report the F1 - BLEU which considers both diversity and quality in Table 10 . ( Caccia et al , 2018 ) .", "entities": [[4, 6, "TaskName", "image captioning"], [25, 27, "TaskName", "image captioning"], [117, 118, "DatasetName", "COCO"], [175, 176, "MetricName", "BLEU"], [186, 187, "MetricName", "CIDEr"], [222, 223, "MethodName", "ResNet"], [241, 242, "DatasetName", "2048"], [247, 248, "MethodName", "ResNet"], [254, 255, "DatasetName", "ImageNet"], [274, 275, "MethodName", "LSTM"], [287, 288, "MethodName", "Adam"], [301, 303, "HyperparameterName", "learning rate"], [348, 349, "MethodName", "AutoEncoder"], [350, 351, "MethodName", "AE"], [375, 376, "MethodName", "SCST"], [384, 385, "MethodName", "SCST"], [416, 417, "MethodName", "SCST"], [428, 429, "MethodName", "SCST"], [517, 519, "TaskName", "imitation learning"], [546, 547, "MetricName", "BLEU"], [553, 554, "MetricName", "F1"], [555, 556, "MetricName", "BLEU"]]}
{"text": "For Image COCO , the learning rate of the generator is 0.0002 , the learning rate of the guider 0.0002 , the maximum length of sequence is 25 . For WMT , the learning rate of the guider 0.0002 , the learning rate of the guider 0.0002 , the maximum length of sequence is 50 . We use c = 4 chosen from [ 2 , 3 , 4 , 5 , 8 ] and \u03b3 = 0.25 chosen from [ 0.1 , 0.25 , 0.5 , 0.75 , 0.99 ] . We use Adam ( Kingma and Ba , 2014 ) optimization algorithm to train the guider , generator and discriminator . For both tasks , the LSTM state of dimension for the generator is 300 , and the LSTM state of dimension for the generator is 300 . The dimension of word - embedding is 300 . The output dimension of the linear transformation connecting guider and generator is 600\u00d710 . The learning rate of Discriminator is 0.001 .", "entities": [[2, 3, "DatasetName", "COCO"], [5, 7, "HyperparameterName", "learning rate"], [14, 16, "HyperparameterName", "learning rate"], [33, 35, "HyperparameterName", "learning rate"], [41, 43, "HyperparameterName", "learning rate"], [75, 76, "HyperparameterName", "\u03b3"], [94, 95, "MethodName", "Adam"], [118, 119, "MethodName", "LSTM"], [130, 131, "MethodName", "LSTM"], [164, 166, "HyperparameterName", "learning rate"]]}
{"text": "For Image Captioning , the learning rate of the guider 0.0002 , the learning rate of the guider 0.0002 , the maximum length of sequence is 25 . For Style transfer , the learning rate of the guider 0.0001 , the learning rate of the guider 0.0001 , the maximum length of sequence is 15 .", "entities": [[1, 3, "TaskName", "Image Captioning"], [5, 7, "HyperparameterName", "learning rate"], [13, 15, "HyperparameterName", "learning rate"], [29, 31, "TaskName", "Style transfer"], [33, 35, "HyperparameterName", "learning rate"], [41, 43, "HyperparameterName", "learning rate"]]}
{"text": "Algorithm 2 Guider Matching Generative Adversarial Network ( GMGAN ) Require : generator policy \u03c0 \u03c6 ; discriminator D \u03b8 ; guider network G \u03c8 ; a sequence dataset S = { X 1 ... T } . 1 : Initialize G \u03c8 , \u03c0 \u03c6 , D \u03b8 with random weights . 2 : Pretrain generator \u03c0 \u03c6 , guider G \u03c8 and discriminator D \u03b8 with MLE loss . 3 : repeat 4 : for g - steps do 5 : Generate a sequence Y 1 ... T \u223c \u03c0 \u03c6 .", "entities": [[4, 7, "MethodName", "Generative Adversarial Network"], [19, 20, "HyperparameterName", "\u03b8"], [48, 49, "HyperparameterName", "\u03b8"], [66, 67, "HyperparameterName", "\u03b8"], [69, 70, "MetricName", "loss"]]}
{"text": "Test - BLEU - 2 3 4 5 Self - BLEU - 2 3 4 SeqGAN 0.820 0.604 0.361 0.211 0.807 0.577 0.278 RankGAN ( Lin et al , 2017 ) 0.852 0.637 0.389 0.248 0.822 0.592 0.230 GSGAN ( Kusner and Miguel , 2016 ) 0.810 0.566 0.335 0.197 0.785 0.522 0.230 TextGAN 0.910 0.728 0.484 0.306 0.806 0.548 0.217 LeakGAN ( Guo et al , 2017 ) 0.922 0.797 0.602 0.416 0.912 0.825 0.689 MLE ( Caccia et al , 2018 ) 0.902 0.706 0.470 0.392 0.787 0.646 0.485 GMGAN ( ours ) 0.949 0.823 0.635 0.421 0.746 0.511 0.319 ( Lin et al , 2017 ) 0.723 0.440 0.210 0.107 0.672 0.346 0.119 GSGAN ( Kusner and Miguel , 2016 ) 0.723 0.440 0.210 0.107 0.807 0.680 0.450 TextGAN 0.777 0.529 0.305 0.161 0.806 0.662 0.448 LeakGAN ( Guo et al , 2017 ) 0.923 0.757 0.546 0.335 0.837 0.683 0.513 MLE ( Caccia et al , 2018 ) 0.902 0.706 0.470 0.392 0.787 0.646 0.485 GMGAN ( ours ) 0.923 0.727 0.491 0.303 0.814 0.576 0.328", "entities": [[2, 3, "MetricName", "BLEU"], [10, 11, "MetricName", "BLEU"]]}
{"text": "Encoder as the feature extractor For unconditional generation , the feature extractor that generates inputs for the guider network shares the CNN part of the encoder . We stop gradients from the guider network to the encoder CNN in the training process . For conditional generation , we use a pretrained feature extractor , trained similarly to the unconditional generation . Training procedure As with many imitationlearning models ( Bahdanau et al , 2017 ; Rennie et al , 2016 ; Sutskever et al , 2014 ) , we first train the encoder - decoder part based on the off - policy data with an MLE loss . Then we use RL training to fine - tune the trained generator . We adaptively transfer the training from MLE loss to RL loss , similar to ( Paulus et al , 2017 ; Ranzato et al , 2016 ) .", "entities": [[106, 107, "MetricName", "loss"], [128, 129, "MetricName", "loss"], [131, 132, "MetricName", "loss"]]}
{"text": "We focus on adversarial text generation , and compare our approach with a number of related works ( Guo et al , 2017 ; Lin et al , 2017 ; Zhu et al , 2018 ) . In this setting , a discriminator in the GAN framework is added to the model in Figure 1 to guide the generator to generate high - quality sentences . This is implemented by defining the final reward to be the output of the discriminator . All baseline experiments are implemented on the texygen platform ( Zhu et al , 2018 ) . We adopt the BLEU score , referenced by the test set ( test - BLEU , higher value implies better quality ) and itself ( self - BLEU , lower value implies better diversity ) ( Zhu et al , 2018 ) to evaluate quality of generated samples , where test - BLEU evaluates the reality of generated samples , and self - BLEU measures the diversity . A good generator should achieve both a high test - BLEU score and a low self - BLEU score . In practice , we use t = c = 4 and \u03b3 = 0.25 . We call the proposed method guidermatching GAN ( GMGAN ) for unconditional text Res152 - SCST : a group of zebras standing in a eld . Res152 - GMST : a herd of zebras standing in a eld of grass . Tag - SCST : a zebra and a zebra drinking water from a eld of grass . Tag - GMST : a group of zebras drinking water in the eld of grass . Res152 - SCST : a group of people walking down a skateboard . Res152 - GMST : a group of people standing on a street with a skateboard . Tag - SCST : a woman walking down a street with a skateboard . Tag - GMST : a black and white photo of a man riding a skateboard . Res152 - SCST : a baby sing next to a baby girae . Res152 - GMST : a lile baby sing next to a baby holding a teddy bear . Tag - SCST : a black and white photo of a woman holding a teddy bear . Tag - GMST : a black and white photo of a man and a woman holding a teddy bear . Res152 - SCST : a trac light on a street with a in the . Res152 - GMST : a trac light on the side of a street . Tag - SCST : a trac light on a street with a green . Tag - GMST : a red trac light sing on the side of a road . Generate a sequence Y 1 ... T \u223c \u03c0 \u03c6 .", "entities": [[3, 5, "TaskName", "adversarial text"], [45, 46, "MethodName", "GAN"], [89, 91, "DatasetName", "texygen platform"], [102, 104, "MetricName", "BLEU score"], [113, 114, "MetricName", "BLEU"], [126, 127, "MetricName", "BLEU"], [151, 152, "MetricName", "BLEU"], [162, 163, "MetricName", "BLEU"], [177, 179, "MetricName", "BLEU score"], [184, 186, "MetricName", "BLEU score"], [198, 199, "HyperparameterName", "\u03b3"], [208, 209, "MethodName", "GAN"], [217, 218, "MethodName", "SCST"], [245, 246, "MethodName", "SCST"], [278, 279, "MethodName", "SCST"], [307, 308, "MethodName", "SCST"], [337, 338, "MethodName", "SCST"], [367, 368, "MethodName", "SCST"], [404, 405, "MethodName", "SCST"], [433, 434, "MethodName", "SCST"]]}
{"text": "Following the few other research and asked for \" based on the store to protect older , nor this . But there , nor believe that it has reached a the person to know what never - he needed . The trump administration later felt the alarm was a their doctors are given . We have been the time of single things what people do not need to get careful with too hurt after wells then . If he was waited same out the group of fewer friends a more injured work under it . It will access like the going on an \" go back there and believe . Premier as well as color looking to put back on a his is . So , even though : \" don ' t want to understand it at an opportunity for our work . I was shocked , nor don ' t know if mate , don ' t have survived , So one point like ten years old , but a sure , nor with myself more people substantial . And if an way of shoes of crimes the processes need to run the billionaire . Now that their people had trained and people the children live an actor , nor what trump had . However , heavily she been told at about four during an innocent person . LeakGAN The country has a reputation for cheap medical costs and high - attack on a oil for more than to higher its - wage increase to increase access to the UK the UK women from the UK ' s third nuclear in the last couple of weeks . I ' ve been watching it through , and when the most important time it is going to be so important . I ' m hopeful that as that process moves along , that the U . S . Attorney will share as much as far as possible . The main thing for should go in with the new contract , so the rest of the Premier League is there to grow up and be there , \" she said . I think the main reason for their sudden is however , I didn ' t get any big thing , \" he says , who is the whole problem on the U . S . Supreme Court and rule had any broken . The average age of Saudi citizens is still very potential for the next year in the past year , over the last year he realised he has had his massive and family and home . \" I think Ted is under a lot of people really want a \" and then the opportunity to put on life for security for them to try and keep up . The new website , set to launch March 1 , but the U . S is to give up the time the case can lead to a more than three months of three months to be new home . It ' s a pub ; though it was going to be that , but , not , but I am not the right thing to live , \" she said . \" I ' m not saying method writing is the only way to get in the bedroom to get through the season and we ' ll be over again , \" he says . I ' m not suggesting that our jobs or our love our years because I have a couple of games where I want it to be . The German government said 31 suspects were briefly detained for questioning after the New Year ' s Eve trouble , among them not allowed to stay in the long - term . It was a punishment carried out by experts in violence , and it was hard to me he loved the man and he ' s got off to support me in the future . \" I ' ve known him , all that just over the last two weeks and for the last 10 years , I ' ll have one day of my life , \" she said . The main idea behind my health and I think we saw in work of our country was in big fourth - up come up with a little you ' ve ever . he Kings had needed scoring from the left side , too , and King has provided that since his return are the of the first three quarters of the game . It ' s going to be a good test for us and we are on the right way to be able to get through it on every day on the year . GMGAN But it ' s grown up a little now , and might be ready for actually putting into your house . More than a dozen Republicans and a handful of Democrats have announced they are running for their party ' s 2016 presidential nomination , and when they were wealthy in 2010 right , what he has . And with a growing following of more than 45 , 000 people on Facebook , awareness of their work is on the rise . In all age groups , for instance , more people cited retirement as the reason for being out of the labour force , and it wasn ' t a problem in big . I had to train really , really hard and that ' s the advice I can give , because if you don ' t work hard somebody else will . I am picking up two cars tomorrow and taking them down south tomorrow if all goes according to plan , \" he said . The team looked into the influence of marriage on weight loss after surgery - as well as the effects of surgery on the quality of his administration and rest on the world . Two former prime ministers were set to face off in the second round of a presidential election in New Hampshire . A third more complaints were made about the accounts between April and December last year than in the whole of 2014 / 15 . United Airlines subsequently worked to get those passengers back in the air so they could get to Colorado , the airline spokesman said . Mr Brown was standing in the kitchen when he started to feel a bit cold - and he noticed the door had disappeared . She has focused instead on where she parts ways with her rival on other issues , like to have someone with a president has revealed . Once , an ex - boyfriend and I lived with her for two months after we came back from travelling . He had faced 10 years in prison on the charges but the first government have been made at the recent peak . \" We weren ' t exposed to things we didn ' t have in the same way kids these days are , \" said Obama . I have no idea what it is , but there is definitely an intelligence - a higher intelligence - at work you have you want to make sure you are going into the local community . His current club have confirmed they would be willing to listen to offers for the attacking midfielder , but we did not have the right manager - there ' s summer to be in a big . We are in the last 16 and the target is always to win in the Champions League and will continue at the best level to be the coach . People are seeing that you can go into real estate and do really well and do something we want and if we make the right decision , and how we will be doing it is . Table 13 : Generated Examples on EMNLP2017 WMT . Original : i ' m so lucky to have found this place ! Guider : i ' m so embarrassed that i picked this place . Original : awesome place , very friendly staff and the food is great ! Guider : disgusting place , horrible staff and extremely rude customer service . Original : this was my first time trying thai food and the waitress was amazing ! Guider : this was my first experience with the restaurant and we were absolutely disappointed . Original : thanks to this place ! Guider : sorry but this place is horrible . Original : the staff was warm and friendly . Guider : the staff was slow and rude . Original : great place and huge store . Guider : horrible place like ass screw . Original : the service is friendly and quick especially if you sit in the bar . Guider : the customer service is like ok - definitely a reason for never go back .. Original : everything is always delicious and the staff is wonderful . Guider : everything is always awful and their service is amazing . Original : best place to have lunch and or dinner . Guider : worst place i have ever eaten . Original : best restaurant in the world ! Guider : worst dining experience ever ! Original : you 'll be back ! Guider : you 're very disappointed ! Original : you will be well cared for here ! Guider : you will not be back to spend your money . Original : they were delicious ! Guider : they were overcooked . Original : seriously the best service i ' ve ever had . Guider : seriously the worst service i ' ve ever experienced . Original : it 's delicious ! Guider : it 's awful . this place is phenomenal . Original : this was bad experience from the start . Guider : the food here was amazing good . Original : very rude lady for testing my integrity . Guider : very nice atmosphere for an amazing lunch ! Original : they recently renovated rooms but should have renovated management and staff . Guider : great management and the staff is friendly and helpful . Original : this store is not a good example of sprint customer service though . Guider : this store is always good , consistent and they 're friendly . Original : one of my least favorite ross locations . Guider : one of my favorite spots . Original : horrible in attentive staff . Guider : great front desk staff ! Original : the dining area looked like a hotel meeting room . Guider : the dining area is nice and cool . Original : never ever try to sell your car at co part ! Guider : highly recommend to everyone and recommend this spot for me ! Original : i ordered the filet mignon and it was not impressive at all . Guider : i had the lamb and it was so good .", "entities": [[979, 980, "MetricName", "loss"]]}
{"text": "One line of work in style transfer attempts to learn disentangled latent representation for style and content , and transfer style by manipulating latent representation of style ( Shen et al , 2017 ) . Although these approaches perform well with one style at a time , they do not trivially scale to multidimensional style transfer . Several other works develop unsupervised approach for style transfer by employing Denoising Autoencoding ( DAE ) ( Fu et al , 2017 ) and back - translation ( BT ) ( Lample et al , 2018 ) loss to develop interaction and hence transfer between the source and target domain . Subramanian et al ( 2018 ) extend this approach to multiple styles by conditioning on average of embedding of each target attribute and using combination of DAE and back - translation techniques . DAE takes as input a sentence x from style s and tries to reconstruct sentence x from its corrupted versionx . This relies on the assumption that the input sentence x is from a certain style combination s = { s 1 , s 2 , . . . , s k } . Similarly back translation ( BT ) objective with input sentence x from style s , first estimates x = f ( x , s ) , where s = s and then reconstruct x fromx = f ( x , s ) . Thus , these approaches are inherently dependent on knowledge of annotation of each sentence with all the style combinations . Dai et al ( 2019 ) achieve state - ofthe - art style transfer in single style dimensions by employing transformer - based model in conjunction with classifier - based discriminator . In addition to discriminator losses , their proposed technique uses self - reconstruction and cycle reconstruction losses , which similar to DAE and BT losses are also reliant on availability of jointly annotated data to be extendable to multiple style setup . Language modeling is integral to several natural language generation ( NLG ) tasks like text summarization , spelling correction , image captioning , etc . The model architecture for these tasks has evolved from n - gram based methods to Recurrent Neural Networks to transformer architectures . The introduction of Transformer - based architecture accompanied with generative pre - training ( Radford , 2018 ) capabilities have led to strong improvements in many downstream generation and GLUE ( Wang et al , 2018 ) tasks . Generative pre - training aims to adapt a large Transformer language model to large unsupervised corpus . This capability of generative pre - training is exploited in many large language models like BERT ( Devlin et al , 2019 ) , GPT - 2 ( Radford et al , 2018 ) , ERNIE 2.0 ( Sun et al , 2020 ) which have the ability to perform tasks like reading comprehension ( Xu et al , 2019 ) , summarization ( Liu and Lapata , 2019 ) , question - answering ( Rajpurkar et al , 2016 ) and translation ( Clinchant et al , 2019 ) in zero - shot and few - shot settings . Recently these pre - trained generative language models have been explored in translation ( Conneau and Lample , 2019 ) and style transfer tasks ( Syed et al , 2020 ) . Conneau and Lample ( 2019 ) develop cross - lingual models for unsupervised machine translation by initializing encoder and decoder with a pre - trained language model trained on Masked Language Modeling ( MLM ) ( Devlin et al , 2019 ) objective and fine - tuning the encoderdecoder framework with adversarial training . Syed et al ( 2020 ) extend this to stylized re - writing task by employing DAE during fine - tuning . The joint encoder - decoder framework learns to reconstruct sentences in target - domain from its noisy version using DAE objective . As previously discussed , the DAE objective is reliant on the corpus being tagged for the target domain style ( or combination of style ) and restricts the generalization of this setup to multiple attributes . We overcome this by employing discriminative language models to assist the decoder with feedback for various target styles . Shen et al ( 2017 ) show that even with nonparallel data , the content distribution across source and target style is shared . Based on this , a language model trained on target style will have high perplexity on transferred text if it does not match target style and low perplexity otherwise . Yang et al ( 2018 ) exploit this ability of language models to replace standard binary classifier - based discriminators with an implicitly trained language model as discriminator . They show that using the language model as structured discriminator allows for more stable training by eliminating the adversarial step . We extend this idea to a multi - discriminator approach . Training a LM on combination of target styles is not possible in absence of jointly labelled dataset . Due to this , we attempt to use multiple discriminators for each of the target styles . Since with multiple styles , the underlying corpus is independently acquired , the variation in content distribution across different styles is more noticeable . Consequently , an independently trained LM on one of the target styles might have high perplexity even if the transferred sentence fits in the corresponding target style , due to the content space of source sentence . To equip discriminative LM with more generalized notion of content , we use large transformer - based LM pre - trained on large unsupervised corpus to establish generic content distribution before style - oriented fine - tuning .", "entities": [[5, 7, "TaskName", "style transfer"], [54, 56, "TaskName", "style transfer"], [64, 66, "TaskName", "style transfer"], [68, 69, "TaskName", "Denoising"], [94, 95, "MetricName", "loss"], [270, 272, "TaskName", "style transfer"], [346, 348, "TaskName", "text summarization"], [349, 351, "TaskName", "spelling correction"], [352, 354, "TaskName", "image captioning"], [382, 383, "MethodName", "Transformer"], [408, 409, "DatasetName", "GLUE"], [427, 428, "MethodName", "Transformer"], [450, 451, "MethodName", "BERT"], [459, 460, "MethodName", "GPT"], [487, 489, "TaskName", "reading comprehension"], [497, 498, "TaskName", "summarization"], [556, 558, "TaskName", "style transfer"], [579, 582, "TaskName", "unsupervised machine translation"], [596, 599, "TaskName", "Masked Language Modeling"], [600, 601, "DatasetName", "MLM"], [758, 759, "MetricName", "perplexity"], [771, 772, "MetricName", "perplexity"], [909, 910, "MetricName", "perplexity"]]}
{"text": "Our proposed approach has two key elementsa Transformer - based encoder - decoder model initialized with a pre - trained Transformer Language Model and fine - tuned on DAE loss to achieve style transfer ( Section 3.1 ) and the multiple language models as discriminators stacked together to enable multi - style transfer ( Section 3.2 ) .", "entities": [[7, 8, "MethodName", "Transformer"], [20, 21, "MethodName", "Transformer"], [29, 30, "MetricName", "loss"], [32, 34, "TaskName", "style transfer"], [51, 53, "TaskName", "style transfer"]]}
{"text": "Similar to Syed et al ( 2020 ) , we first pre - train a Transformer - based language model with Masked Language Modeling ( MLM ) objective on English Wikipedia data extracted using WikiExtractor . 1 This equips LM with the ability to predict masked words over a large corpus . Masked Language Modeling leverages bidirectional context of the input , thus enabling better language understanding . Following Masked Language Modeling objective from Devlin et al ( 2019 ) , we randomly sample 15 % of the tokens from the text stream and replace them with the [ MASK ] token 80 % of the time , by a random token 10 % of the time and keep them unchanged 10 % of the time , with the objective of predicting the original identity of the masked word based on its bidirectional context . To enable style transfer from a given sentence to target style , we use independently trained language models ( LMs ) to initialize the encoder and decoder and connect these with randomly initialized attention layers to arrive at a encoder - decoder setup . As discussed by Syed et al ( 2020 ) , the Transformer architecture ( Vaswani et al , 2017 ) allows such independent initialization by implicitly aligning encoder - decoder layers via attention mechanism . Pre - training an encoder only transformer on generative task and then leveraging it to initialize as both encoder and decoder as opposed to pretraining a joint encoder - decoder model has several advantages . Transformer - based models with encoder - only ( Devlin et al , 2019 ) or decoder - only blocks have been shown to perform well in generative pre - training task . Clearly , pre - training a single transformer block on generative task and then utilizing it as both encoder and decoder blocks has lower computational cost than training the entire encoder - decoder block jointly . Moreover , this also enables us to use the same pre - trained model to initialize both style transfer module and the discriminator models , explained in the following section . This is not only computationally more efficient but it also closely ties the underlying language distribution of the two modules . This is expected to make the discriminative feedback more effective while fine tuning the transfer model for multiple styles . In Syed et al ( 2020 ) 's setup , both encoder and decoder in the style transfer module are initialized with the pre - trained language model ( trained on MLM objective ) . Instead , we initialize the decoder with the language model fine - tuned with the target style using Causal Language Modeling ( CLM ) objective , before training the joint encoder - decoder model , as detailed in Section 3.2 . The encoder is initialized with the pre - trained model directly . Aligning the decoder to the distribution of the tar - . . . . get style helps speed up the fine - tuning process as decoder is more adept at generating stylized outputs . This does not add to computational overhead as these fine - tuned models are repurposed as discriminators for stylistic feedback ( Section 3.2 ) . To instill style - awareness to the encoder - decoder setup initialized with pre - trained Transformer models , we fine - tune it with Denoising Autoencoder ( DAE ) loss using the target - domain corpus . In case of multiple styles , we use a randomized mixture of target - domain corpus from each of the target styles . Under the DAE objective , the encoder takes a noisy masked versionx of the text x as input and attempts to fill in the mask token as per the MLM objective that it was pre - trained on . In turn , the decoder re - creates stylistic version of original sentence from this noisy output from the encoder . The overall training objective is L DAE ( \u03b8 G ) = E x\u223cT [ \u2212 log P \u03b8 G ( x | x ) ] , ( 1 ) where \u03b8 G are the trainable parameters of the encoder - decoder model . The noisy version of sentence x from the target corpus T is obtained after dropping tokens from x with probability p drop and masking with a probability of p mask . In conjunction , the encoder and decoder enable style transfer to the target style . The noteworthy aspect here is that the model has no sense of source style and is trained to generate sentences to match the style of the target - domain corpus with which it is trained .", "entities": [[15, 16, "MethodName", "Transformer"], [21, 24, "TaskName", "Masked Language Modeling"], [25, 26, "DatasetName", "MLM"], [52, 55, "TaskName", "Masked Language Modeling"], [69, 72, "TaskName", "Masked Language Modeling"], [147, 149, "TaskName", "style transfer"], [178, 180, "HyperparameterName", "attention layers"], [200, 201, "MethodName", "Transformer"], [259, 260, "MethodName", "Transformer"], [345, 348, "MethodName", "style transfer module"], [416, 419, "MethodName", "style transfer module"], [431, 432, "DatasetName", "MLM"], [563, 564, "MethodName", "Transformer"], [572, 574, "MethodName", "Denoising Autoencoder"], [577, 578, "MetricName", "loss"], [637, 638, "DatasetName", "MLM"], [676, 677, "HyperparameterName", "\u03b8"], [686, 687, "HyperparameterName", "\u03b8"], [699, 700, "HyperparameterName", "\u03b8"], [751, 753, "TaskName", "style transfer"]]}
{"text": "To extend the single - dimensional style transfer setup above to multi - dimensional setting , we use language models as discriminators to provide the feedback to the model for partially annotated nature of input data . As opposed to a classifier - based discriminator , the language model as discriminator takes into account the wider language distribution of the target style . Additionally , such a setup allows us to use only the target style corpus for training the transfer model , whereas the classifier would require both source and target style corpus to distinguish between a sentence as being from one style or another . Inspired by Yang et al ( 2018 ) , we fine - tune a language model on the target style s i , so that the language model is equipped with language distribution of target domain data . This entails generating the probability of next token , given the previous tokens - also known as Causal Language Modeling objective ( Conneau and Lample , 2019 ) . The training loss for the LM for target style s i with corresponding corpus T i is Ex\u223cT i n t=1 [ \u2212 log PLM ( xt | x1 , . . . , xt\u22121 ) ] ( 2 ) We show in our experiments that such a finetuning step transforms language distribution of this language model to style s i and hence serve as softdiscriminator for our framework . We exploit this capability of language models to imbibe style of fine - tuning corpus by employing language models as style discriminators for transferred sentences . This is based on the idea that if the transferred sentence does not fit well in the target style , then the perplexity of language model fine - tuned on that style will be high ( Section 4.1 ) . For k - dimensional style transfer with target styles s = { s 1 , s 2 , . . . , s k } , we independently finetune k language models on each of the target styles . As discussed in Yang et al ( 2018 ) , we are able to forgo the adversarial training for the discriminator , since the fine - tuned discriminative language model is implicitly capable of assigning high perplexity to negative samples ( out - of - style samples ) , as shown in Section 4.1 . For the transferred sentence x , the training objective for each target style s i is , argmin \u03b8 G L s i = E x\u223cT , x \u223cP \u03b8 G ( x ) n t=1 \u2212 log P LM i ( x t | x 1 , .. , x t\u22121 ) ( 3 ) This dictates that transferred sentence x has low perplexity on the language model fine - tuned on style s i , for each target style s i . However , we can not directly find the argmin \u03b8 G using gradient descent because of discrete sampling of x \u223c P \u03b8 G ( x ) . To account for this , we use a policy gradient reinforcement learning approach using REINFORCE algorithm ( Sutton et al , 1999 ) . The reward for an input sequence x to the style discriminator LM i is calculated as , r ( x ) = n t=1 log P LM i ( x t | x 1 , .. , x t\u22121 ) ( 4 ) Using these rewards , the RL objective is to minimize the loss L s i given by , L s i = E x\u223cT , x \u223cP \u03b8 G ( x ) ( r ( x ) \u2212 r ( x ) ) [ \u2212 log P \u03b8 G ( x | x ) ] ( 5 ) for style s i , where P \u03b8 G ( x | x ) is as in Equation 1and r ( x ) is the reward in the Equation 4 for the transferred sentence x . The rewards r ( x ) represents the baseline reward of greedily sampling the input sequence x by the style discriminator LM i . For the style combination s = { s 1 , s 2 , . . . , s k } , the joint encoder - decoder model is trained on randomized mixture of data from each of the targetdomain corpus . The mixture is thus agnostic of individual style of each of the sentence and the discriminative LM for each style guides the generation towards that specific style by rewarding style adherence in the transferred sentence . Randomized mixture of training corpus across styles allows for unified and cohesive understanding of multiple styles by diversifying rewards from different discriminators across samples . The overall training loss for the joint encoder - decoder model is L = \u03bbDAEEx\u223cT [ \u2212 log P \u03b8 ( x | x ) ] + k i=1 \u03bbiL s i , ( 6 ) where L s i is as defined in Equation 5 , and \u03bb DAE and { \u03bb i } k i=1 are hyper - parameters . The overall training process is summarized in Figure 1 . First , we pre - train a transformer model with Masked language modeling objective as shown in Figure 1 ( Left ) . We then initialize discriminator model with this pre - trained language model and fine - tune it with Causal language modeling objective , shown in Figure 1 ( Right ) , for each target style . Finally , we initialize the encoder and decoder of the style transfer module with the pretrained and style - specific fine - tuned language models , respectively . In case of multiple styles , the decoder can be initialized with the language model which is fine - tuned with CLM loss on the mixture of data from target styles , i.e. , CLM loss in Equation 2 with x \u223c T . The joint encoder - decoder model ( Figure 1 ( Centre ) ) is then trained with a combination of DAE objective and rewards from fine - tuned discriminators of respective target styles .", "entities": [[6, 8, "TaskName", "style transfer"], [107, 108, "DatasetName", "Inspired"], [176, 177, "MetricName", "loss"], [292, 293, "MetricName", "perplexity"], [314, 316, "TaskName", "style transfer"], [385, 386, "MetricName", "perplexity"], [422, 423, "HyperparameterName", "\u03b8"], [433, 434, "HyperparameterName", "\u03b8"], [468, 469, "MetricName", "perplexity"], [497, 498, "HyperparameterName", "\u03b8"], [510, 511, "HyperparameterName", "\u03b8"], [530, 531, "MethodName", "REINFORCE"], [594, 595, "MetricName", "loss"], [610, 611, "HyperparameterName", "\u03b8"], [630, 631, "HyperparameterName", "\u03b8"], [648, 649, "HyperparameterName", "\u03b8"], [806, 807, "MetricName", "loss"], [822, 823, "HyperparameterName", "\u03b8"], [885, 888, "TaskName", "Masked language modeling"], [944, 947, "MethodName", "style transfer module"], [984, 985, "MetricName", "loss"], [997, 998, "MetricName", "loss"]]}
{"text": "We experiment with a combination of sentiment and formality styles . For sentiment , we use a mixture of IMDB ( Maas et al , 2011 ) and Yelp dataset ( Li et al , 2018 ) with 300k examples in the positive and negative sentiment each . For formality , we use GYAFC corpus ( Rao and Tetreault , 2018 ) which has 104k examples in each formal and informal class . The test set has 3000 and 4849 examples for sentiment and formality respectively , following the data split available in Dai et al ( 2019 ) ; Rao and Tetreault ( 2018 ) . For both datasets , the training corpus is non - parallel and the test corpus has human written references available , which we use for content evaluation ( Section 4.2 ) . For pre - training , we use 12 - layer Transformer model with 512 hidden units , 16 heads , a dropout rate of 0.1 and learned positional embedding . We train our models with the Adam optimizer , and", "entities": [[19, 20, "DatasetName", "IMDB"], [53, 54, "DatasetName", "GYAFC"], [149, 150, "MethodName", "Transformer"], [175, 176, "MethodName", "Adam"], [176, 177, "HyperparameterName", "optimizer"]]}
{"text": "To evaluate style variation across language models fine - tuned on different styles , we compare the generations of the fine - tuned models . For singledimensional style evaluation , we generate sentences from models fine - tuned on negative corpus and positive corpus and compare the style accuracy of generated sentences . The style accuracy is evaluated by employing a FastText ( Joulin et al , 2016 ) classifier trained on the corresponding style dimension . For instance , the classifier for evaluating sentiment accuracy is trained on sentiment corpus tagged with positive and negative class in IMDB and Yelp data . Table 1 shows the accuracy of sentences generated by a model fine - tuned on style s i as belonging to the class s i . For both sentiment and formality , the fine - tuned language models are able to generate text faithful to the target style dimension . Thus , we conclude that the language models trained on style s i are able to capture the essence of the corresponding style reasonably well . These accuracies are an indication of the style awareness in these fine - tuned LMs . We , therefore , employ the perplexities of these fine - tuned language models to gauge the style of the input text to guide our style transfer model . As discussed in discriminative modeling ( Section 3.2 ) , the model fine - tuned with corpus from a certain style is expected to have high perplexity on sentence not from that style and low perplexity otherwise . To this end , we experiment with two models independently finetuned on positive and negative corpus . We calculate the perplexity of each of these models on the test corpus from the same style and from the opposite style . As seen in Table 2 , the perplexity for each model is substantially lower on the same corpus as compared to that on the opposite corpus . This implies that a language model fine - tuned on positive corpus shows higher perplexity for negative sentences and lower for positive sentences and vice versa . This corroborates the effectiveness of these fine - tuned language models to serve as discriminators for training the style transfer module .", "entities": [[48, 49, "MetricName", "accuracy"], [55, 56, "MetricName", "accuracy"], [61, 62, "MethodName", "FastText"], [85, 86, "MetricName", "accuracy"], [98, 99, "DatasetName", "IMDB"], [107, 108, "MetricName", "accuracy"], [220, 222, "TaskName", "style transfer"], [250, 251, "MetricName", "perplexity"], [259, 260, "MetricName", "perplexity"], [282, 283, "MetricName", "perplexity"], [309, 310, "MetricName", "perplexity"], [343, 344, "MetricName", "perplexity"], [374, 377, "MethodName", "style transfer module"]]}
{"text": "We measure the performance of our model and the baselines based on the style control , content preservation and fluency . To measure the accuracy of style transfer , we train two Fasttext 2 classifiers independently for sentiment and formality using the train corpus , as described in Section 4.1 . These classifiers have accuracy of 93.74 % and 88.95 % respectively on test corpus of respective datasets . We note that formality as a style is more intricately designed , so we also check lexical scoring by Brooke et al ( 2010 ) to evaluate formality , which uses a formality lexicon to assign formality score between \u22121 ( informal ) and 1 ( formal ) to each word and averages it . We scale these scores between 0 - 100 , where higher ( 100 ) lexical score signifies formal style and lower ( 0 ) score signifies informal style . For informal target style , we report lexical score as 100 \u2212 n , so that a higher average lexical score signifies a better transfer for either polarity . To measure content preservation on transfer , we calculate the BLEU score ( Papineni et al , 2002 ) between the transferred sentence and the input sentence ( self - BLEU ) . Besides this , we also calculate BLEU score between the transferred sentence generated by our model and the corresponding human reference transferred sentence , available for GYAFC and Yelp corpus ( ref - BLEU ) . Since both these corpus account for transfer across only one style dimension each , the provided references are only partial indication of expected outcome . This Dai et al , 2019 ) , Cascaded Discriminative LM method and multi - style transfer using Adapted Rewriting LM ( Syed et al , 2020 ) . The upward arrow signifies that higher is better and vice versa . Score of near 100 on formality lexical scoring imply the transferred text is close in formality to the target corpus . is also apparent from low ref - BLEU scores for our model as well as baselines . Since , the results are presented on aggregated dataset from both these style dimensions , this evaluation is still able to provide reasonable indication of content preservation . To measure the fluency of the text , we calculate perplexity assigned to the generated text sequence by a language model trained on the train corpus , as is standard in style transfer literature ( Dai et al , 2019 ; Subramanian et al , 2018 ) . The perplexity is the measure of log likelihood of the generated sentence on the language model . A lower perplexity is indicative of a more fluent sentence . We use a generative transformer - based language model trained on the dataset combined from two styles . Dai et al ( 2019 ) use transformer - based model ( Style Transformer ) for single - dimensional style transfer . We train two independent Style Transformer models for sentiment and formality transfer and then perform transfer one after another to compare results with our model . We term this as Cascaded Style Transformer setup . The Style Transformer model is shown to have state - of - the - art performance in single - dimensional style transfer ; thus it provides an estimate of the performance of sequential single style transfer . We also experiment with Adapted Rewriting LM ( Syed et al , 2020 ) as another baseline . Their work on style rewriting to match author - specific style does not require explicit annotations for the various aspects that constitutes an author 's style , but is based on the assumption that the training corpus reflects the target style . In this context , we train their framework on the mixture of data from the respective target styles and report the performance . These are the closest baselines to our proposed approach , since other works dealing with multi - style transfer assume presence of jointly annotated dataset , which is a stronger assumption that we aim to relax . In addition to our proposed model with multiple style transfer , we also train our encoder - decoder architecture with single discriminative LM for one style at a time and perform two stage transfer , similar to one with Cascaded Style Transformer ( Dai et al , 2019 ) setup .", "entities": [[24, 25, "MetricName", "accuracy"], [26, 28, "TaskName", "style transfer"], [32, 33, "MethodName", "Fasttext"], [54, 55, "MetricName", "accuracy"], [129, 130, "DatasetName", "0"], [146, 147, "DatasetName", "0"], [192, 194, "MetricName", "BLEU score"], [212, 213, "MetricName", "BLEU"], [221, 223, "MetricName", "BLEU score"], [241, 242, "DatasetName", "GYAFC"], [248, 249, "MetricName", "BLEU"], [291, 293, "TaskName", "style transfer"], [317, 318, "MetricName", "Score"], [345, 346, "MetricName", "BLEU"], [393, 394, "MetricName", "perplexity"], [414, 416, "TaskName", "style transfer"], [432, 433, "MetricName", "perplexity"], [450, 451, "MetricName", "perplexity"], [490, 491, "MethodName", "Transformer"], [496, 498, "TaskName", "style transfer"], [504, 505, "MethodName", "Transformer"], [531, 532, "MethodName", "Transformer"], [536, 537, "MethodName", "Transformer"], [554, 556, "TaskName", "style transfer"], [568, 570, "TaskName", "style transfer"], [671, 673, "TaskName", "style transfer"], [699, 701, "TaskName", "style transfer"], [732, 733, "MethodName", "Transformer"]]}
{"text": "The results in Table 3 show that our model achieves better style control than the Cascaded Style Transformer ( Dai et al , 2019 ) as well as the joint transfer using Syed et al ( 2020 ) for both sentiment and formality . As seen in Table 3 , cascaded style transfer models perform poorly on content preservation . This is because transferring style one after other leads to huge loss in content , thus both the two - stage models score lower on content preservation metrics , both w.r.t . the input text and the reference transferred text . This demonstrates the advantage of using single model to control for multiple styles . The effect can also be observed in Table 4 which demonstrates qualitative results for Cascaded Style Transformer model and our model . We can see in many cases content loses the underlying meaning of source sentence during the twostage transfer , whereas our model is able to retain original meaning of the sentence well , corroborating the findings of automatic evaluation . Among the cascaded models , the Discriminative LM scores marginally better on content preservation than the Style Transformer model . We attribute this to initialization with the same pre - trained LM resulting in shared content space in the underlying single style transfer models . However , due to independent training of the two single style transfer models , they are not able to model interplay between these styles and hence perform worse on style control than our proposed model trained jointly on multiple styles . Our model also scores better on fluency , as seen in Table 3 . This is also apparent from the exam - Give your brother some money and tell him to take a hike . Just give your brother some time and it will be good again . Give your brother some money and request him to leave .", "entities": [[17, 18, "MethodName", "Transformer"], [51, 53, "TaskName", "style transfer"], [71, 72, "MetricName", "loss"], [131, 132, "MethodName", "Transformer"], [194, 195, "MethodName", "Transformer"], [218, 220, "TaskName", "style transfer"], [232, 234, "TaskName", "style transfer"]]}
{"text": "To augment automatic evaluation results , we conduct a human study to evaluate the model outputs across various dimensions such as content preservation , style control , fluency , and overall trans - fer quality . Based on comparable style control in Cascaded Style Transformer and our proposed approach on automatic metrics , we compare the transfer quality across these two models by a small - scale human study . We select 40 sentences , with 10 examples from each combinations of sentiment and formality as target style , and collect annotations from 4 - 5 participants for each example . Out of resulting annotations , more than 85 % annotations favoured our results over baseline . The average participant rating across different dimensions is shown in Table 5 . We test the statistical significance of these results using z - test statistic . With \u03b1 = 0.05 , the preferences indicated in human study are significant across all metrics . These results are in line with our automatic evaluations and add confidence to the efficacy of our proposed approach in achieving style transfer across multiple dimensions .", "entities": [[44, 45, "MethodName", "Transformer"], [145, 146, "HyperparameterName", "\u03b1"], [182, 184, "TaskName", "style transfer"]]}
{"text": "Numerical reasoning skills are essential for complex question answering ( CQA ) over text . It requires opertaions including counting , comparison , addition and subtraction . A successful approach to CQA on text , Neural Module Networks ( NMNs ) , follows the programmer - interpreter paradigm and leverages specialised modules to perform compositional reasoning . However , the NMNs framework does not consider the relationship between numbers and entities in both questions and paragraphs . We propose effective techniques to improve NMNs ' numerical reasoning capabilities by making the interpreter questionaware and capturing the relationship between entities and numbers . On the same subset of the DROP dataset for CQA on text , experimental results show that our additions outperform the original NMNs by 3.0 points for the overall F1 score .", "entities": [[7, 9, "TaskName", "question answering"], [108, 109, "DatasetName", "DROP"], [131, 133, "MetricName", "F1 score"]]}
{"text": "Complex Question Answering ( CQA ) is a challenging task , requiring a model to perform compositional and numerical reasoning . Originally proposed for the visual question answering ( VQA ) task , Neural Module Networks ( NMNs ) ( Andreas et al , 2016 ) have recently been adopted to tackle the CQA problem over text ( Gupta et al , 2020 ) . The NMNs is an end - to - end differentiable model in the programmer - interpreter paradigm ( Guo et al , 2020 ; Hua et al , 2020a , b ) . Briefly , the programmer learns to map each question into a program , i.e. a sequence of neural modules , and the interpreter then \" executes \" the program , operationalized by modules , on the paragraph to yield the answer for different types of complex questions . NMNs achieves the best performance on a subset of the challenging DROP dataset ( Dua et al , 2019 ) and is interpertable by nature . However , NMNs ' performance advantage is not consistent , as it underperforms in some types of questions that require numerical reasoning . For instance , for date - compare questions , MTMSN ( Hu et al , 2019 ) achieves an F1 score of 85.2 1 , whereas NMNs ' performance is 82.6 . Similarly , for count questions , the F1 score is 61.6 for MTMSN and 55.7 for NMNs . This performance gap stems from two deficiencies of NMNs , which we describe below with the help of two examples in Figure 1 . Firstly , NMNs ' interpreter is oblivious to the question when executing number - related modules . For executing number - related modules , the interpreter only receives the paragraph as input , but not the question . Such a lack of direct interactions with the question impairs model performance : the entities in the question , which may also occur in the paragraph , can help locate significant and relevant numbers to produce the final answer . In the first example in Figure 1 , if the interpreter is aware of the correct event mentioned in the question ( i.e. \" the Constituent Assembly being elected \" ) , it can easily find the same event in the paragraph and further locate its date ( \" 12 November \" ) precisely . Without this knowledge , the original NMNs found the wrong event ( i.e. \" dissolved the Constituent Assembly \" ) , thus the wrong date ( \" January 1918 \" ) , leading to an incorrect answer . Secondly , NMNs disregards the relative positioning of entities and their related numbers in the paragraph . Although NMNs can learn separate distributions over numbers extracted from a paragraph , it does not have an effective mechanism to identify the number that connects to a given entity . Such an ability to recognise the association among numbers and entities is vital for learning numerical reasoning skills : the operation between numbers is meaningful only when they refer to the same entity or the same type of entities . The second example in Figure 1 illustrates the positioning of entities and their related numbers . With only a constraint on a window around an entity , the NMNs ' interpreter tends to identify the nearest number as the related one to a given entity ( \" August 1996 to December 1997 \" for entity \" PUK and KDP later co - operated \" ) , resulting in wrong predictions . Figure 1 : Two examples in the DROP ( Dua et al , 2019 ) dataset that demonstrate the deficienties of NMNs . Tokens pertinent to our discussion are highlighted in red , and their relevant numbers are highlighted in orange . Solid blue lines are predictions of our model , while dotted blue lines show the predictions of NMNs . We propose three simple and effective mechanisms to improve NMNs ' numerical reasoning capabilities . Firstly , we improve the interpreter to make it questionaware . By explicitly conditioning the execution on the question , the interpreter can exploit the information contained in the question . Secondly , we propose an intuitive constraint to better relate numbers and their corresponding entities in the paragraph . Finally , we strengthen the auxiliary loss to increase attention values of entities in closer vicinity within a sentence . Experimental results show that our modifications significantly improve NMNs ' numerical reasoning performance by up to 3.0 absolute F1 points . With minor modification , these mechanisms are simple enough to be applied to other modular approaches .", "entities": [[1, 3, "TaskName", "Question Answering"], [25, 28, "DatasetName", "visual question answering"], [29, 30, "TaskName", "VQA"], [157, 158, "DatasetName", "DROP"], [214, 216, "MetricName", "F1 score"], [234, 236, "MetricName", "F1 score"], [606, 607, "DatasetName", "DROP"], [731, 732, "MetricName", "loss"], [763, 764, "MetricName", "F1"]]}
{"text": "In this section , we will discuss the deficiencies of NMNs described in Section 1 and propose three techniques to overcome these problems . Considering the importance of questions while executing programs , we incorporate a question - to - paragraph alignment matrix to form a question - aware interpreter in Section 3.1 . In Section 3.2 , the correspondence between numbers and their related entities is enhanced with a simple and effective constraint on number - related modules . In Section 3.3 , we strengthen the auxiliary loss function in NMNs to further concentrate attention in the same sentence .", "entities": [[88, 89, "MetricName", "loss"]]}
{"text": "The interpreter in the NMNs framework is responsible for executing specialised modules given the context ( i.e. paragraph ) . For number - related modules such as \" find - num \" , the question is not taken into account , which limits NMNs ' performance on numerical reasoning , as information in the question is not taken into account . As an example , let us take a clear look at the \" find - num \" module in NMNs . find - num ( P ) T 2 . This module takes as input the distribution over paragraph tokens , and produces output an distribution over the numbers : S n ij = P i T W n P n j , ( 1 ) A n i = softmax ( S n i ) , ( 2 ) T = i P i A n i , ( 3 ) where input P and output T are distributions over paragraph tokens and numbers respectively , P is the paragraph token representations , i is the index of the i th paragraph token , n j is the index of the j th number token , and W n is a learnable matrix . Note that when computing the similarity matrix between the paragraph token P i and the number token P n j in Equation 1 , there is no interaction with the question . When the correct number types or related entities can be easily found in the question , incorporating the question in \" find - num \" can help narrow down the search of numbers in the paragraph . The first example in Figure 1 shows that the NMNs fails to locate the correct number as the wrong event is recognized , without interacting with the question . Inspired by this idea , we propose the question - toparagraph alignment modification to number - related modules . Specifically , the definition of \" find - num \" is modified as follows : find - num ( P , Q ) T n , where the additional input Q obtained from the programmer represents the distribution over question tokens , and the new output is represented by T n . Additional computational steps ( Equation 4 to 7 below ) are added after Equation 3 : S n kj = Q k T W n P n j , ( 4 ) A n k = softmax ( S n k ) , ( 5 ) T = k Q k A n k , ( 6 ) T n = \u03bb T + ( 1\u2212\u03bb ) T , ( 7 ) where Q is the question token representations and k is the index of the k th question token . As can be seen from the above equations , the input of the improved \" find - num \" module is extended to include not only paragraph but also question token distributions instead of only the paragraph . More precisely , T is another alignment matrix between all question tokens and number tokens , using the same form of Bi - linear attention computation as T . Finally , the new distribution T n is produced by the weighted sum of T and T with an additional hyperparameters \u03bb . Here we fix \u03bb=0.5 so that NMNs treats the paragraph and the question equally . Other numberrelated modules are also revised in a similar way , e.g. \" find - date \" , \" compare - num - lt - than \" , \" find - max - num \" .", "entities": [[131, 132, "MethodName", "softmax"], [304, 305, "DatasetName", "Inspired"], [409, 411, "HyperparameterName", "k ="], [411, 412, "MethodName", "softmax"]]}
{"text": "Gupta et al ( 2020 ) employed an auxiliary loss to constrain the relative positioning of output tokens with respect to input tokens in the \" find - num \" , \" find - date \" and \" relocate \" modules . For instance , the auxiliary loss for the \" find - num \" module is as follows : H n loss = \u2212 m i=1 log ( Nt j=0 1 n j [ i\u00b1W ] A n ij ) , ( 10 ) where A n ij is from Equation 2 . The loss enables the model to concentrate the attention mass of output tokens within a window of size W ( e.g. W = 10 ) . However , these loss functions still allow irrelevant numbers to have spuriously high attention values . Taking the second line in Figure 1 as an example , based on the loss computation procedures , the number \" December 1997 \" will be also \" found \" and connected to the entity \" PUK and KDP \" in NMNs . Obviously , this irrelevant year information should not be taken into consideration . Therefore , we propose to strengthen the auxiliary loss to further concentrate attention mass to those tokens within the same sentence : H n loss = \u2212 m i=1 log ( Nt j=0 1 ( n j st ) ( i st ) A n ij ) , ( 11 ) where the s t is the token index set for the t th sentence in the paragraph . In this way , the year \" 2003 \" is the only consideration for the previous example .", "entities": [[9, 10, "MetricName", "loss"], [47, 48, "MetricName", "loss"], [62, 63, "MetricName", "loss"], [95, 96, "MetricName", "loss"], [123, 124, "MetricName", "loss"], [150, 151, "MetricName", "loss"], [200, 201, "MetricName", "loss"], [216, 217, "MetricName", "loss"]]}
{"text": "We evaluate model performance on the same subset of the DROP dataset used by the original NMNs ( Gupta et al , 2020 ) , which contains approx . 19 , 500 QA pairs for training , 440 for validation and 1 , 700 for testing . The training procedures and hyper - parameter settings are the same as the original NMNs ( Gupta et al , 2020 ) . We report F1 and Exact Match ( EM ) scores following the literature ( Dua et al , 2019 ; Gupta et al , 2020 ) .", "entities": [[10, 11, "DatasetName", "DROP"], [72, 73, "MetricName", "F1"], [74, 76, "MetricName", "Exact Match"], [77, 78, "MetricName", "EM"]]}
{"text": "Table 1 shows the main results , where \" original \" represents the performance of the original NMNs ( Gupta et al , 2020 ) . Row 4 , \" + qai+nepc+aux \" , is our full model , which includes the question - aware interpreter ( + qai ) , the number - entity positional constraint ( + nepc ) , and the improved auxiliary loss ( + aux ) . It can be observed that compared to \" original \" , our full model achieves significantly higher performance with F1 of 80.4 and EM of 76.6 , representing an increase of 3.0 and 2.6 absolute points respectively . Besides , our significant test shows p\u22640.01 .", "entities": [[66, 67, "MetricName", "loss"], [91, 92, "MetricName", "F1"], [95, 96, "MetricName", "EM"]]}
{"text": "original ( Gupta et al , 2020 ) We also conduct an ablation study to discuss the contribution of individual technique . The second line , \" + qai \" , is the results with the question - aware interpreter employed only . For this variant , the F1 and EM scores improve on the original baseline by 1.6 and 0.9 points respectively . With the addition of the number - entity positional constraint , \" + nepc \" , results show an improvement of 2.5 and 2.0 points for F1 and EM when comparing with \" original \" . These results show that all of the three techniques are effective in improving numerical reasoning skills for NMNs . We also report performance by subsets of different question types in Table 2 . Except for the numbercompare type , our model improves on the original NMNs across all other types of questions significantly , by at least 3.2 absolute points for F1 . In addition , our model outperforms aforementioned MTMSN ( Hu et al , 2019 )", "entities": [[48, 49, "MetricName", "F1"], [50, 51, "MetricName", "EM"], [90, 91, "MetricName", "F1"], [92, 93, "MetricName", "EM"], [161, 162, "MetricName", "F1"]]}
{"text": "Neural Moudule Networks ( NMNs ) represent an interpretable state - of - the - art approach to complex question answering over text . In this paper , we further improve NMNs ' numerical reasoning capabilities , by making the interpreter question - aware and placing stronger constraints on the relative positioning of entities and their related numbers . Experimental results show that our approach significantly improves NMNs ' numerical reasoning ability , with an increase in F1 of 3.0 absolute points .", "entities": [[19, 21, "TaskName", "question answering"], [77, 78, "MetricName", "F1"]]}
{"text": "Due to the page limitation , we did n't include more baselines , such as NAQANet ( Dua et al , 2019 ) . After running on the same split of DROP dataset , the F1 and EM scores by NAQANet are 62.1 % and 57.9 % respectively , which are substantially lower than our results in Table 1 , by over 17 % for both scores . And we did apply these components in Section 3 to other modules , such as the \" extract - argument \" module ( extracts spans or tokens from paragraphs ) , and also obtained better results ( 0.5 % F1 increase ) . Besides , for different question types , their statistics on the test set can be found in Table 4 . Current NMNs ( Gupta et al , 2020 ) does not support other arithmetic datasets , since some arithmetic operations , including addition , are not supported . Extending related arithmetic modules is one of our future work , based on which the NMNs could be trained on other datsets .", "entities": [[31, 32, "DatasetName", "DROP"], [35, 36, "MetricName", "F1"], [37, 38, "MetricName", "EM"], [107, 108, "MetricName", "F1"]]}
{"text": "Widely adopted Transformer architecture ( Vaswani et al , 2017 ) has obviated the need for sequential processing of the input that is enforced in traditional Recurrent Neural Networks ( RNN ) . As a result , compared to a single - layered LSTM or RNN model , a single - layered Transformer model is computationally more efficient , reflecting in a relatively shorter training time ( Vaswani et al , 2017 ) . This advantage encourages the training of deep Transformer - based language models on largescale datasets . Their learning on large corpora has already attained state - of - the - art ( SOTA ) performances in many downstream Natural Language Processing ( NLP ) tasks . A large number of SOTA machine learning systems even beyond NLP ( Lu et al , 2019 ) are inspired by the building blocks of Transformer that is multi - head self - attention ( Radford et al , 2018 ; Devlin et al , 2018 ) . A model employing an attention - based mechanism generates a probability distribution a = { a 1 , . . . , a n } over the n input units z = { z 1 , . . . , z n } . The idea is to perform a weighted sum of inputs , denoted by n i=1 a i z i , to produce a more context - involved output . The attention vector , a , are commonly interpreted as scores signifying the relative importance of input units . However , counter - intuitively , it is recently observed that the weights generated in the model do not provide meaningful explanations ( Jain and Wallace , 2019 ; Wiegreffe and Pinter , 2019 ) . Attention weights are ( structurally ) identifiable if we can uniquely determine them from the output of the attention unit ( Brunner et al , 2019 ) . Identifiability of the attention weights is critical to the model 's prediction to be interpretable and replicable . If the weights are not unique , explanatory insights from them might be misleading . The self - attention transforms an input sequence of vectors z = { z 1 , . . . , z n } to a contextualized output sequence y = { y 1 , . . . , y n } , where y k = n i=1 a ( k , i ) z i . The scalar a ( k , i ) captures how much of the i th token contributes to the contextualization of k th token . A Transformer layer consists of multiple heads , where each head performs selfattention computations , we break the head computations in two phases : Phase 1 : Calculation of attention weights a ( k , i ) . It involves mapping input tokens to key and query vectors . The dot product of k th query vector and i th key vector gives a ( k , i ) . Phase 2 : Calculation of a contextualized representation for each token . It involves mapping input tokens to the value vectors . The contextualized representation for k th token can be computed by the weighted average of the value vectors , where the weight of i th token is a ( k , i ) computed in first phase . The identifiability in Transformer has been recently studied by Brunner et al ( 2019 ) which provides theoretical claims that under mild conditions of input length , attention weights are not unique to the head 's output . Essentially their proof was dedicated to the analysis of the computations in the second phase , i.e. , token contextualization . However , the theoretical analysis ignored the crucial first phase where the attention weights are generated . Intrinsic to their analysis , the attention identifiability can be studied by studying only the second phase of head computations . However , even if we find another set of weights from the second phase , it depends on the first phase if those weights can be generated as the part of key - query multiplication . In this work , we probe the identifiability of attention weights in Transformer from a perspective that was ignored in Brunner et al ( 2019 ) . We explore the previously overlooked first phase of selfattention for its contribution to the identifiability in Transformer . During our analysis of the first phase , we uncover the critical constraint imposed by the size of the key vector 1 d k . The flow of analysis can be described as We first show that the attention weights are identifiable for the input sequence length d s no longer than the size of value vector d v ( 3.1 ) ( Brunner et al , 2019 ) 2 . For the case when d s > d v , we analyse the attention weights as raw dot - product ( logits ) and the softmaxed dot - product ( probability simplex ) , independently . An important theoretical finding is that both versions are prone to be unidentifiable . In the case of attention weights as logits ( 3.2.1 ) , we analytically construct another set of attention weights to claim the unidentifiability . In the case of attention weights as 1 The size of key and query vector is expected to be the same due to the subsequent dot product operation 2 The sequence length denotes number of tokens at input . softmaxed logits ( 3.2.2 ) , we find the attention identifiability to be highly dependent on d k . Thus , the size of key vector plays an important role in the identifiability of the self - attention head . The pieces of evidence suggest that the current analysis in Brunner et al ( 2019 ) ignored the crucial constraints from the first phase in their analysis . To resolve the unidentifiability problem , we propose two simple solutions ( 4 ) . For the regular setting of the Transformer encoder where d v depends on the number of attention heads and token embedding dimension , we propose to reduce d k . This may lead to more identifiable attention weights . Alternatively , as a more concrete solution , we propose to set d v equal to token embedding dimension while adding head outputs as opposed to the regular approach of concatenation ( Vaswani et al , 2017 ) . Embedding dimension can be tuned according to the sequence length up to which identifiability is desired . We evaluate the performance of the proposed variants on varied text classification tasks comprising of ten datasets ( 5 ) . In this paper , our goal is to provide concrete theoretical analysis , experimental observations , and possible simple solutions to identifiability of attention weights in Transformer . The idea behind identifiable variants of the Transformer is - the harder it is to obtain alternative attention weights , the likelier is they are identifiable , which is a desirable property of the architecture . Thus , our contribution are as follows : We provide a concrete theoretical analysis of identifiability of attention weights which was missing in the previous work by Brunner et al ( 2019 ) . We provide Transformer variants that are identifiable and validate them empirically by analysing the numerical rank of the attention matrix generated in the self - attention head of the Transformer encoder . The variants have strong mathematical support and simple to adopt in the standard Transformer settings . We provide empirical evaluations on varied text classification tasks that show higher identifiability does not compromise with the task 's performance . 2 Background", "entities": [[2, 3, "MethodName", "Transformer"], [43, 44, "MethodName", "LSTM"], [52, 53, "MethodName", "Transformer"], [81, 82, "MethodName", "Transformer"], [145, 146, "MethodName", "Transformer"], [401, 403, "HyperparameterName", "k ="], [440, 441, "MethodName", "Transformer"], [572, 573, "MethodName", "Transformer"], [714, 715, "MethodName", "Transformer"], [745, 746, "MethodName", "Transformer"], [1021, 1022, "MethodName", "Transformer"], [1035, 1037, "HyperparameterName", "embedding dimension"], [1071, 1073, "HyperparameterName", "embedding dimension"], [1093, 1095, "HyperparameterName", "Embedding dimension"], [1120, 1122, "TaskName", "text classification"], [1157, 1158, "MethodName", "Transformer"], [1166, 1167, "MethodName", "Transformer"], [1231, 1232, "MethodName", "Transformer"], [1258, 1259, "MethodName", "Transformer"], [1274, 1275, "MethodName", "Transformer"], [1283, 1285, "TaskName", "text classification"]]}
{"text": "We base our analysis on the building block of Transformer , i.e. , the encoder layer ( Vaswani et al , 2017 ) . The layer has two sub - layers . First sublayer performs the multi - head self - attention , and second is feed - forward network . Given a sequence of tokens { x 1 , . . . , x ds } , an embedding layer transforms it to a set of vector { z 1 , . . . , z ds } R de , where d e denotes token embedding dimension . To this set , we add vectors encoding positional information of tokens { p 1 , . . . , p ds } R de . Multi - head Attention . Input to a head of multihead self - attention module is W R ds\u00d7de , i.e. , a sequence of d s tokens lying in a d e - dimensional embedding space . Tokens are projected to d q - size query , d k - size key , and d v - size value vectors using linear layers , resulting in the respective matrices - Query Q R ds\u00d7dq , Key K R ds\u00d7d k , and Value V R ds\u00d7dv . The attention weights A R ds\u00d7ds can be computed by A = softmax Q K T d q . ( 1 ) The ( i , j ) th element of A shows how much of i th token is influenced by j th token . The output of a head H R ds\u00d7de is given by H = A V D = A T , ( 2 ) where D R dv\u00d7de is a linear layer and the matrix T R ds\u00d7de denotes the operation V D. The R ds\u00d7de output of multi - head attention can be expressed as a summation over H obtained for each head 3 . The i th row of multi - head output matrix corresponds to the d e dimensional contextualized representation of i th input token . In the original work , Vaswani et al ( 2017 ) , the multi - head operation is described as the concatenation of A V obtained from each head followed by a linear transformation D R de\u00d7de . Both the explanations are associated with the same sequence of matrix operations as shown in fig . 1 . In regular Transformer setting , a token vector is t i { ( z j + p j ) } ds i=1 is d e = 512 dimensional , number of heads h=8 , size of d k = d q = d v = d e /h=64 . Feed - Forward Network . This sub - layer performs the following transformations on each token representation at the output of a head : y 1 = Linear 1 ( Norm ( t i + head output for t i ) ) y 2 = Norm ( t i + ReLU ( Linear 2 ( y 1 ) ) ) Linear 1 and Linear 2 are linear layers with 2048 and 512 nodes , respectively . Norm denotes minibatch layer normalization .", "entities": [[9, 10, "MethodName", "Transformer"], [97, 99, "HyperparameterName", "embedding dimension"], [126, 130, "MethodName", "Multi - head Attention"], [226, 227, "MethodName", "softmax"], [289, 291, "MethodName", "linear layer"], [307, 311, "MethodName", "multi - head attention"], [408, 409, "MethodName", "Transformer"], [443, 445, "HyperparameterName", "k ="], [505, 506, "MethodName", "ReLU"], [524, 525, "DatasetName", "2048"], [534, 536, "MethodName", "layer normalization"]]}
{"text": "Since the logits matrix A is obtained from the product of Q and K T , we can assert that rank ( A ) \u2264 min rank ( Q ) , rank ( K T ) \u2264 min d e , d k , d q , d e = d k . ( 8 ) Therefore , the rank of attention matrix producible by the head in the first phase of self - attention can at most be equal to the size of key vectors d k . On this basis , the head can produce only those A + \u00c3 satisfying rank ( A + \u00c3 ) \u2264 d k ( constraint - R2 ) Proposition 3.3 . There exists a non - trivial\u00c3 that satisfy ( A + \u00c3 ) T = A T and constraint - R2 . Hence , A is unidentifiable . Proof . Let a 1 , . . . , a ds and\u00e3 1 , . . . , \u00e3 ds denote rows of A and\u00c3 , respectively . Without the loss of generality , let a 1 , . . . , a d k be linearly independent rows . For all j > d k , a j can be represented as a linear combination d k i=1 \u03bb j i a i , where \u03bb j i is a scalar . Next , we independently choose first k rows of\u00c3 that are { \u00e3 1 , . . . , \u00e3 d k } from LN ( T ) . From the same set of coefficients of linear combination \u03bb j i for i { 1 , . . . , d k } and j { d k+1 , . . . , d s } , we can construct j th row of\u00c3 as\u00e3 j = d k i=1 \u03bb j i\u00e3 i . Now , since we can construct the j th row of ( A + \u00c3 ) from the linear combination of its first d k rows as d k i=1 \u03bb j i ( a i + \u00e3 i ) , the rank of ( A + \u00c3 ) is not more than d k . For a set of vectors lying in a linear space , a vector formed by their linear combination should also lie in the same space . Thus , the artificially constructed rows of A belongs to LN ( T ) . Therefore , there exist an\u00c3 that establishes the proposition which claims the unidentifiability of A.", "entities": [[179, 180, "MetricName", "loss"]]}
{"text": "The softmax over attention logits generates attention weights with each row of A ( i.e. , a i 's ) is constrained to be a probability distribution . Hence , we can define constraint over\u00c3 as ( A + \u00c3 ) \u2265 0 ( P1 ) A T = 0 ( P2 ) A 1 = 0 . ( P3 ) P1 is non - negativity constraint on ( A + \u00c3 ) as it is supposed to be the output of softmax ; P2 de - notes\u00c3 LN ( T ) ; P3 can be derived from the fact ( A + \u00c3 ) 1 = 1 = \u21d2 ( A 1 + \u00c3 1 ) = 1 = \u21d2\u00c3 1 = 0 as ( A 1 = 1 ) . Where 1 R ds is the vector of ones . The constraint in P2 and P3 can be combined and reformulated as\u00c3 [ T , 1 ] = 0 . Following the similar analysis as in eq . ( 7 ) , we can obtain dim LN ( [ T , 1 ] ) = max d s \u2212 ( d v + 1 ) , 0 . Disregarding the extreme cases when a i is a one - hot distribution , Brunner et al ( 2019 ) proved the existence and construction of non - trivial\u00c3 's satisfying all the constraints P1 , P2 , and P3 . 5 However , the proof by Brunner et al ( 2019 ) missed the constraint - R2 , hence the existence of a non - trivial\u00c3 satisfying only the set of constraints P1 , P2 and P3 may not be a valid proposition to claim attention weights unidentifiability . Essentially , the work largely ignored the constraints coming from the rank of the matrix that produces A after softmax 6 . Let A l denote logits Q K T \u221a dq and softmax ( A l ) = ( A + \u00c3 ) , where softmax is operated over each row of A l . We add an extra constraint on A l rank ( A l ) \u2264 d k . ( P4 ) The constraint P4 confirms if there exists a logit matrix A l that can generate ( A + \u00c3 ) , given constraints P1 , P2 , and P3 are satisfied . The possibility of such an A l will provide sufficient evidence that A is unidentifiable . Next , we investigate how the existence of\u00c3 is impacted by the size of key vector d k ( query and key vector sizes are the same , i.e. , d q = d k ) . Let ( A + \u00c3 ) ( i , k ) denotes ( i , k ) th element of the matrix . We can retrieve the set of matrices A l such that softmax ( A l ) = A + \u00c3 , where for some arbitrary c i R ; log denotes natural logarithm . As shown in fig . 3 , the column vectors of A l can be written as c + \u00e2 1 , . . . , c + \u00e2 ds . A l ( i , k ) = c i + log ( A + \u00c3 ) ( i , k ) ( 9 ) For an arbitrarily picked\u00c3 satisfying constraint P1 , P2 , and P3 , the dimensions of affine span S of { \u00e2 1 , . . . , \u00e2 ds } could be as high as d s \u2212 1 ( fig . 4 ) . In such cases , the best one could do is to choose a c a S such that the dimension of the linear span of { \u00e2 1 \u2212 c a , . . . , \u00e2 ds \u2212 c a } , i.e. , rank ( A l ) is d s \u2212 1 . Hence , to satisfy P4 , d s \u2212 1 \u2264 d k = \u21d2 d s \u2264 d k + 1 . Thus , the set of ( A + \u00c3 ) satisfying constraint P1 , P2 and P3 are not always obtainable from attention head for d s > d k . We postulate Although it is easier to construct\u00c3 satisfying constraints P1 , P2 and P3 , it is hard to construct\u00c3 satisfying constraint P4 over the rank of logit matrix A l . Therefore , A becomes more identifiable as the size of key vector decreases . Figure 4 : This is a simplified illustration for the case d s = 3 . Affine space ( translated linear subspace ) spanned by vectors\u00e2 1 , \u00e2 2 and\u00e2 3 . c a can be any arbitrary vector in affine space . By putting c = \u2212c a , we can obtain a linear subspace whose rank is equal to rank of the affine subspace . Experimental evidence . We conduct an experiment to validate the minimum possible numerical rank of A l by constructing\u00c3. For\u00c3 to be obtainable from the phase 1 , the minimum possible rank of A l should not be higher than d k . From IMDB dataset ( 5 ) , we randomly sample a set of reviews with token sequence length d s ranging from 66 to 128 7 . For each review , we construct 1000\u00c3 's satisfying constraints P1 , P2 , and P3 - First , we train a Transformer encoder - based IMDB review sentiment classifier ( 6 ) . We obtain an orthonormal basis for the left null space of [ T , 1 ] using singular value decomposition . To form an\u00c3 , we generate d s random linear combinations of the basis vectors ( one for each of its row ) . Each set of linear combination coefficients is sampled uniformly from [ \u221210 , 10 ] . All the rows are then scaled to satisfy the constraint P1 as mentioned in Brunner et al ( 2019 ) . Using eq . ( 9 ) , we obtain a minimum rank matrix A l 's by putting c = \u2212\u00e2 1 . Figure 5 depicts the obtained numerical rank of A l . We observed all the obtained A l from ( A + \u00c3 ) ( using eq . ( 9 ) ) are full - row rank matrices . However , from the first phase of self - attention , the maximum obtainable rank of A l is d k = 64 . Thus , the experimentally constructed A l 's do not claim unidentifiability of A as it fails to satisfy the constraint P4 , while for Brunner et al ( 2019 ) , it falls under the solution set to prove unidentifiability as it meets constraints P1 , P2 and P3 .", "entities": [[1, 2, "MethodName", "softmax"], [42, 43, "DatasetName", "0"], [49, 50, "DatasetName", "0"], [56, 57, "DatasetName", "0"], [59, 60, "DatasetName", "P3"], [82, 83, "MethodName", "softmax"], [93, 94, "DatasetName", "P3"], [123, 124, "DatasetName", "0"], [147, 148, "DatasetName", "P3"], [160, 161, "DatasetName", "0"], [198, 199, "DatasetName", "0"], [239, 240, "DatasetName", "P3"], [277, 278, "DatasetName", "P3"], [309, 310, "MethodName", "softmax"], [323, 324, "MethodName", "softmax"], [336, 337, "MethodName", "softmax"], [394, 395, "DatasetName", "P3"], [485, 486, "MethodName", "softmax"], [575, 576, "DatasetName", "P3"], [678, 680, "HyperparameterName", "k ="], [705, 706, "DatasetName", "P3"], [734, 735, "DatasetName", "P3"], [879, 880, "DatasetName", "IMDB"], [920, 921, "DatasetName", "P3"], [927, 928, "MethodName", "Transformer"], [931, 932, "DatasetName", "IMDB"], [1103, 1105, "HyperparameterName", "k ="], [1156, 1157, "DatasetName", "P3"]]}
{"text": "Based on the Identifiability analysis in 3 , we propose basic solutions to make Transformer 's attention weights identifiable . Decoupling d k . Contrary to the regular Transformer setting where d k = d v , a simple approach is to decrease the value of d k that is the size of the key and query vector . It will reduce the possible solutions of\u00c3 by putting harder constraints on the rank of attention logits , i.e. , A l in eq . ( 9 ) . However , theoretically , d k decides the upper bound on dimensions of the space to which token embeddings are projected before the dot product . Higher the upper bound , more degree of freedom to choose the subspace dimensions as compared to the lower d k variants . Thus , there is a plausible trade - off when choosing between d k induced identifiability and the upper bound on the dimension of projected space . Head Addition . To resolve the unidentifiability issue when sequence length exceeds the size of value vector , we propose to keep the value vector size and token embedding dimension to be more than ( or equal to ) the maximum allowed input tokens , i.e. , d v \u2265 d s - max . In Vaswani et al ( 2017 ) , d v was bound to be equal to d e /h , where d e is token embedding dimension and h is number of heads . This constraint on d v is because of the concatenation of h self - attention heads to produce d e - sized output at the first sub - layer of the encoder . Thus , to decouple d v from this constraint , we keep d v = d e and add each head 's output . 8", "entities": [[14, 15, "MethodName", "Transformer"], [28, 29, "MethodName", "Transformer"], [32, 34, "HyperparameterName", "k ="], [192, 194, "HyperparameterName", "embedding dimension"], [244, 246, "HyperparameterName", "embedding dimension"]]}
{"text": "Setting up the encoder . We normalize the text by lower casing , removing special characters , etc . 9 For each task , we construct separate 1 - Gram vocabulary ( U ) and initialize a trainable randomly sampled token embedding ( U \u00d7 d e ) from N ( 0 , 1 ) . Similarly , we randomly initialize a ( d s - max \u00d7 d e ) positional embedding . The encoder ( 2.2 ) takes input a sequence of token vectors ( d s \u00d7 d e ) with added positional vectors . The input is then projected to key and query vector of size d k { 1 , 2 , 4 , 8 , 16 , 32 , 64 , 128 , 256 } . For the regular Transformer setting , we fix the number of heads h to 8 and the size of value vector d v = d e /h that is 64 . For each token at the input , the outputs of attention heads are concatenated to generate a d e - sized vector . For the identifiable variant of the Transformer encoder , d v = d e = 512 , this is equal to d s - max to keep it identifiable up to the maximum permissible number of tokens . The outputs of all the heads are then added . Each token 's contextualized representations ( added head outputs ) are then passed through the feed - forward network ( 2.2 ) . For classification , we use the encoder layer 's output for the first token and pass it through a linear classification layer . In datasets with more than two classes , the classifier output is softmaxed . In the case of SNLI , we use the shared encoder for both premise and hypothesis ; the output of their first tokens is then concatenated just before the final classification layer . We use Adam optimizer , with learning rate = 0.001 , to minimize the cross - entropy loss between the target and predicted label . For all the experiments , we keep the batch size as 256 and train for 20 epochs . We report the test accuracy obtained at the epoch with the best validation accuracy . Numerical rank . To generate the numerical rank plot on IMDB dataset as shown in fig . 2 , we train a separate Transformer encoder - based classifier . For a particular d s value , we sample 100 reviews from the dataset with token length \u2265 d s and clip each review to the maximum length d s . The clipping will ensure the number of tokens is d s before feeding it to the encoder . The numerical rank is calculated for T 's obtained from the first head of the encoder .", "entities": [[51, 52, "DatasetName", "0"], [135, 136, "MethodName", "Transformer"], [192, 193, "MethodName", "Transformer"], [298, 299, "DatasetName", "SNLI"], [329, 330, "MethodName", "Adam"], [330, 331, "HyperparameterName", "optimizer"], [333, 335, "HyperparameterName", "learning rate"], [344, 345, "MetricName", "loss"], [360, 362, "HyperparameterName", "batch size"], [374, 375, "MetricName", "accuracy"], [383, 384, "MetricName", "accuracy"], [395, 396, "DatasetName", "IMDB"], [408, 409, "MethodName", "Transformer"]]}
{"text": "For the identifiable variant , similar to 3.1 , we plot the numerical rank of T with input sequence length as shown in fig . 6 . Unlike fig . 2 , where dim LN ( T ) linearly increases after d s = 64 , we find the dimension is zero for a larger d s ( \u223c 380 ) . The zero dimensional ( left ) null space of T con - firms there exist no nontrivial solution to the constraint constraint - R2 , i.e. , \u00c3 = { 0 } . Thus , the attention weights A are identifiable for a larger range of length of the input sequence . Figure 6 : Scatter plots in red and blue show rank ( T ) and dim LN ( T ) , respectively , for matrices T obtained from the second phase of attention by feeding IMDB samples to the encoder . The green line shows the desired rank ( T ) for which dim LN ( T ) = 0 and thus attention weights are identifiable . It is important that the identifiability of attention weights should not come at the cost of reduced performance of the model . To investigate this issue , we compare the performance of the identifiable Transformer encoder against its regular settings ( 6 ) on varied text classification tasks . For the regular setting , as discussed in 4 as one of the solutions , the Transformer can be made identifiable by decreasing the size of the key vector d k . The rows of the Table 1 corresponding to Con denotes regular Transformer setting with varying size of key vector . We observe the classification accuracy at the lower d k is comparable or higher than large d k values , thus , the enhanced identifiability does not compromise with the model 's classification accuracy . However , we notice a general performance decline with an increase in the size of the key vector . We speculate that for simple classification tasks , the lower - dimensional projection for key and query vector works well . However , as the task becomes more involved , a higher dimension for the projected subspace could be essential . Nonetheless , as we do not have strong theoretical findings , we leave this observation for future work . Another solution to identifiability is to increase d v to d e and add the heads ' outputs . This setting corresponds to the Add rows in the Table 1 . For key vector size d k = 1 , 2 , and 4 , We find the identifiable Transformer 's performance is comparable to the regular settings . For d k \u2265 8 , as a general observation , we find the performance of Add does not drop as drastically as Con with an increase in d k . This could be due to the larger size of value vector leading to the more number of parameters in Add that compensate for the significant reduction in the model 's accuracy . On the large - scale datasets , we observe that Add performs slightly better than Con . Intuitively , as shown in fig . 1 , we can increase the size of value vector to increase the dimension of the space on which each token is projected . A higher dimensional subspace can contain more semantic information to perform the specific task . Even though the theoretical analysis shows the possibility of a full row rank of T and identifiable attention weights , the T obtained from a trained model might not contain all the rows linearly independent as d s increases . We can explain this from the semantic similarities between words cooccurring together ( Harris , 1954 ) . The similarity is captured as the semantic relationship , such as dot product , between vectors in a linear space . As the number of tokens in a sentence , i.e. , d s increases , it becomes more likely to obtain a token vector from the linear combination of other tokens .", "entities": [[92, 93, "DatasetName", "0"], [149, 150, "DatasetName", "IMDB"], [173, 174, "DatasetName", "0"], [215, 216, "MethodName", "Transformer"], [226, 228, "TaskName", "text classification"], [246, 247, "MethodName", "Transformer"], [273, 274, "MethodName", "Transformer"], [286, 287, "MetricName", "accuracy"], [315, 316, "MetricName", "accuracy"], [432, 434, "HyperparameterName", "k ="], [445, 446, "MethodName", "Transformer"], [501, 504, "HyperparameterName", "number of parameters"], [516, 517, "MetricName", "accuracy"]]}
{"text": "We introduce NLQuAD , the first data set with baseline methods for non - factoid long question answering , a task requiring documentlevel language understanding . In contrast to existing span detection question answering data sets , NLQuAD has non - factoid questions that are not answerable by a short span of text and demanding multiple - sentence descriptive answers and opinions . We show the limitation of the F1 score for evaluation of long answers and introduce Intersection over Union ( IoU ) , which measures position - sensitive overlap between the predicted and the target answer spans . To establish baseline performances , we compare BERT , RoBERTa , and Longformer models . Experimental results and human evaluations show that Longformer outperforms the other architectures , but results are still far behind a human upper bound , leaving substantial room for improvements . NLQuAD 's samples exceed the input limitation of most pretrained Transformer - based models , encouraging future research on long sequence language models . 1", "entities": [[16, 18, "TaskName", "question answering"], [32, 34, "TaskName", "question answering"], [69, 71, "MetricName", "F1 score"], [82, 83, "MetricName", "IoU"], [107, 108, "MethodName", "BERT"], [109, 110, "MethodName", "RoBERTa"], [112, 113, "MethodName", "Longformer"], [122, 123, "MethodName", "Longformer"], [155, 156, "MethodName", "Transformer"]]}
{"text": "Over the last few years , there have been remarkable improvements in the area of Machine Reading Comprehension ( MRC ) and open - domain Question Answering ( QA ) due to the availability of large scale data sets such as SQuAD ( Rajpurkar et al , 2016 ) and pre - trained language models such as BERT ( Devlin et al , 2018 ) . Although non - factoid questions represent a large number of real - life questions , current QA data sets barely cover this area . The reason is that context passages in existing QA data sets are mostly very short and questions mostly factoid , i.e. , can be answered by simple facts or entities such as a person name and location ( Jurafsky and Martin , 2019 ) . Little attention has been 1 Dataset and Models : github.com/asoleimanib/NLQuAD Question : How are people coping in the lockdown ? Headline : China coronavirus : Death toll rises as more cities restrict travel Document : China has widened its travel restrictions in Hubei province - the centre of the coronavirus outbreak - as the death toll climbed to 26 . The restrictions will affect at least 20 million people across 10 cities , including the capital , Wuhan , where the virus emerged . On Thursday , a coronavirus patient died in northern Hebei province - making it the first death outside Hubei . [ ... ] We now know this is not a virus that will burn out on its own and disappear . [ ... ] And we still do n't know when people are contagious . Is it before symptoms appear , or only after severe symptoms emerge ? One is significantly harder to stop spreading than the other . [ ... ] One doctor , who requested anonymity , describes the conditions at a hospital in Wuhan . [ ... ] \" I was planning to stay in my apartment because I 'm scared to go to the gym , and I 'm scared to go to out in public , and not many people are willing to go out . \" ( 141 words ) . Vietnam and Singapore were on Thursday added to the nations recording confirmed cases , joining Thailand , the US , Taiwan and South Korea . [ ... ] Taiwan has banned people arriving from Wuhan and the US state department warned American travellers to exercise increased caution in China . ( document length : 921 words ) Figure 1 : A question - answer pair in NLQuAD . QA models must predict the answer span within the context document . The correct answer span is bolded . We extract questions and answers , respectively , from the subheadings and the sub - section bodies from real - word English news articles . Two other questions based on the same article : Can the Coronavirus be stopped ? What 's the global situation ? paid to non - factoid and open - ended questions that require complex answers such as descriptions or opinions ( Hashemi et al , 2020 ) . Answers to nonfactoid questions extend to multiple sentences or paragraphs having few words overlapping with the question ( Cohen and Croft , 2016 ) . Non - factoid QA facilitates document assistance systems , where for example , journalists can seek assistance to highlight relevant opinions and interpretations . It can further motivate more research on long sequence language models . Therefore , a high - quality data set in this area is clearly desired . To support research towards non - factoid and long QA tasks and to address the existing shortcomings as identified above , we have built NLQuAD , a non - factoid long question answering data set . NLQuAD contains 31k non - factoid questions and long answers collected from 13k BBC news articles . We extract questions and answers from the articles ' sub - headings and the following body paragraphs of the sub - headings ( see Figure 1 ) . Questions in NLQuAD are not answerable by a short span of text within the documents . This is in contrast to existing long - context but factoid QA data sets such as NewsQA ( Trischler et al , 2017 ) , TriviaQA ( Joshi et al , 2017 ) , NarrativeQA ( Ko\u010disk\u00fd et al , 2018 ) , DuoRC ( Saha et al , 2018 ) , HotpotQA ( Yang et al , 2018 ) , and Natural Questions ( Kwiatkowski et al , 2019 ) . Although these data sets contain long documents , questions are answerable by short entities or a span of entities . In particular , Natural Questions covers two types of short and long answers . However , due to its factoid questions , most long answers are still sections containing exactly the short answers and so are trivial ( e.g. , \" Where is the world 's largest ice sheet ... ? \" , Short : \" Antarctica \" ; Long : \" The Antarctic ice sheet is the largest single mass of ice on Earth ... \" ) . Furthermore , although a small portion ( 13 % ) of Natural Questions samples have only long answers , they are still spans of simple facts . For example , \" Who is the author of the book Arabian Nights ? \" has no short answer simply because there are multiple authors : \" The work was collected over many centuries by various authors , translators ... \" . In contrast , we address non - factoid questions requiring complex answers like opinions and explanations . NLQuAD 's answers are open and not predefined . Figure 3 and Table 3 present our question types . NLQuAD 's questions are also not self - contained . For example , \" How are people coping in the lockdown ? \" or \" What 's the global situation ? \" can not be answered without the context from the document ( see Figure 1 ) . Section 3.2 discusses our question types in detail . In most existing QA data sets such as SQuAD , crowd - workers generate questions based on provided short passages and extract answers from the passages ( Rajpurkar et al , 2016 ) . This method of question generation can make QA samples trivial because models can simply detect the most related span to the question by guessing based on shallow pattern matching ( Ko\u010disk\u00fd et al , 2018 ) . In contrast , all annotations in NLQuAD are done automatically and directly based on the news articles themselves . NLQuAD , unlike MS MARCO ( Bajaj et al , 2016 ) and ELI5 ( Fan et al , 2019 ) , does not use information retrieval ( IR ) methods to collect supporting documents . Retrieved documents in these data sets are not guaranteed to contain all facts required to answer the question or they occasionally just contain information related to the question but no answers . NLQuAD requires document - level language understanding . With an average document length and answer length of 877 and 175 words , respectively , it exceeds the maximum input length of the state of the art QA models such as BERT ( Devlin et al , 2018 ) and RoBERTa ( Liu et al , 2019 ) due to their memory and computational requirements . Thus , training and evaluating the ( document , question , answer ) tuples is impossible using such models in an end - to - end manner . It is worth noting that it is also harder to perform pre - selection methods before the final span detection because our answers are long . Meanwhile , most of our questions are not self - contained . For example , to answer the question \" How are people coping in the lockdown ? \" ( Figure 1 ) , the system needs to read the document to interpret the concept of \" lockdown \" and then locate the information regarding the people 's behaviour . We also show the shortcomings of the F1 score and ROUGE - N scores in evaluating long sequences . There is a higher chance of overlap between the word N - grams in two long sequences causing F1 and ROUGE - N to over - estimate the performance . Therefore , we propose to use Intersection over Union ( IoU ) measuring position - sensitive overlap between two spans . In summary , our contributions are as follows : ( 1 ) We introduce a new data set for non - factoid long QA that to the best of our knowledge is the first data set requiring long answer span detection given non - self - contained and non - factoid questions ; ( 2 ) We show the limitations of the F1 score in evaluating long answers and propose a new evaluation metric ; ( 3 ) To establish baseline results , we experiment with three state - of - the - art models : BERT , RoBERTa , and Longformer , and compare them with human performance . To handle the input length limitations of BERT and RoBERTa , we pro - pose to train these models in a sliding - window approach ; ( 4 ) We finally show that the state - of - the - art models have limited performance in the non - factoid long QA task .", "entities": [[15, 18, "TaskName", "Machine Reading Comprehension"], [22, 27, "TaskName", "open - domain Question Answering"], [41, 42, "DatasetName", "SQuAD"], [57, 58, "MethodName", "BERT"], [635, 637, "TaskName", "question answering"], [717, 718, "DatasetName", "NewsQA"], [726, 727, "DatasetName", "TriviaQA"], [735, 736, "DatasetName", "NarrativeQA"], [744, 745, "DatasetName", "DuoRC"], [753, 754, "DatasetName", "HotpotQA"], [763, 765, "DatasetName", "Natural Questions"], [796, 798, "DatasetName", "Natural Questions"], [883, 885, "DatasetName", "Natural Questions"], [1042, 1043, "DatasetName", "SQuAD"], [1071, 1073, "TaskName", "question generation"], [1127, 1129, "DatasetName", "MS MARCO"], [1137, 1138, "DatasetName", "ELI5"], [1149, 1151, "TaskName", "information retrieval"], [1232, 1233, "MethodName", "BERT"], [1241, 1242, "MethodName", "RoBERTa"], [1378, 1380, "MetricName", "F1 score"], [1408, 1409, "MetricName", "F1"], [1430, 1431, "MetricName", "IoU"], [1503, 1505, "MetricName", "F1 score"], [1537, 1538, "MethodName", "BERT"], [1539, 1540, "MethodName", "RoBERTa"], [1542, 1543, "MethodName", "Longformer"], [1558, 1559, "MethodName", "BERT"], [1560, 1561, "MethodName", "RoBERTa"]]}
{"text": "Exact Match ( EM ) and the macro - averaged F1 score are the two main evaluation metrics in the span detection QA task ( Rajpurkar et al , 2016 ) . Exact Match determines if the prediction exactly matches the target which can be a too strict criterion for long answers . The F1 score measures the overlap between the words in the prediction and the target . It treats sequences as a bag of words . Unfortunately , in long answers , it is highly likely that a random , long span shares a considerable number of tokens with the target span . The ROUGE - N scores ( Lin and Och , 2004 ) , which are primarily used for sequence generation evaluation , have the same drawback in long sequences . ROUGE - N measures the N - gram overlap between the prediction and target . High chances of overlap of unigrams and bigrams in long sequences cause ROUGE - 1 and ROUGE - 2 to over - estimate performance . The same holds for ROUGE - L with the Longest Common Sub - sequence ( LCS ) because of a high chance of longer LCSs between two long sequences . To better take sequence similarities into account , we propose to evaluate models with the Intersection over Union ( IoU ) score , also known as Jaccard Index . IoU is defined as follows : IoU = | p \u2229 t | | p \u222a t | Question : How did we get here ? Headline : Eta disarms : French police find 3.5 tonnes of weapons Target Answer : Slowly , and with many false starts . Eta used parts of south - western France as a base , even though most of its operations were against Spanish targets in Spain . The group has , however , killed some French policemen , but mostly during police raids on members of the group . Eta\u015b first ceasefire was in 1998 , but collapsed the following year . A similar declaration in 2006 only lasted a matter of months , ending when Eta bombed an airport car park , killing two people . Four years later , in 2010 , Eta announced it would not carry out further attacks and in January 2011 , it declared a permanent and \" internationally verifiable \" ceasefire but refused to disarm . In recent years , police in France and Spain have arrested hundreds of Eta figures and seized many of its weapons . Eta\u015b political wing , Herri Batasuna , was banned by the Spanish government , which argued that the two groups were inextricably linked . Prediction : The group was set up more than 50 years ago in the era of Spanish dictator General Franco , who repressed the Basques politically and culturally . Eta 's goal was to create an independent Basque state out of territory in south - west France and northern Spain . Its first known killing was in 1968 , when a secret police chief was shot dead in the Basque city of San Sebastian . France and Spain refuse to negotiate with Eta , which is on the EU blacklist of terrorist organisations . Figure 5 : A prediction span that is semantically different from the target span but has a F1=30 % ( Prec.=43 % , Rec.=23 % ) and IoU=0 . Red shows the overlapping words in the prediction span with the target . Articles ( a , an , the ) and punctuations are discarded before overlapping calculation . ( ROUGE - 1=32 % , ROUGE - 2=4 % , ROUGE - L=24 % ) where p and t and are the predicted and target contiguous intervals over the context document , containing the positions of the tokens . Intersec - tion ( p \u2229 t = { x | x p and x t } ) measures the overlapping interval and union ( \u222a ) is defined as p \u222a t = { x | x p or x t } . Figure 4 ( left / middle ) compares the F1 and ROUGE - N scores and IoU for the Longformer model on the development set . The F1 and ROUGE - N scores are always higher than IoU , but the metrics perform similarly in their higher values . Somewhat surprisingly , the F1 score can be up to 40 % while there is no overlap between the two spans and IoU=0 . We manually inspected the spans with F1>0 and IoU=0 and saw no significant semantic similarity between the predicted answer span and the target span . The same pattern repeats for the ROUGE - N scores . ROUGE - 1 similar to F1 can reach 40 % while IoU=0 , but ROUGE - 2 and ROUGE - L are less prone to such over - estimation due to lower chance of overlap of bigrams than unigrams and shorter LCSs in two random non - overlapping sequences . Figure 4 ( right ) indicates that the F1 and ROUGE - N scores are higher than IoU for longer answers reiterating the fact that these scores over - estimate more for longer sequences . Figure 5 shows two spans in a document with high F1 and ROUGE - N percentages , but different meanings .", "entities": [[0, 2, "MetricName", "Exact Match"], [3, 4, "MetricName", "EM"], [10, 12, "MetricName", "F1 score"], [32, 34, "MetricName", "Exact Match"], [54, 56, "MetricName", "F1 score"], [179, 182, "MetricName", "ROUGE - L"], [224, 225, "MetricName", "IoU"], [234, 235, "MetricName", "IoU"], [240, 241, "MetricName", "IoU"], [468, 469, "DatasetName", "General"], [695, 696, "MetricName", "F1"], [702, 703, "MetricName", "IoU"], [705, 706, "MethodName", "Longformer"], [713, 714, "MetricName", "F1"], [723, 724, "MetricName", "IoU"], [739, 741, "MetricName", "F1 score"], [772, 774, "TaskName", "semantic similarity"], [800, 801, "MetricName", "F1"], [813, 816, "MetricName", "ROUGE - L"], [853, 854, "MetricName", "F1"], [862, 863, "MetricName", "IoU"], [890, 891, "MetricName", "F1"]]}
{"text": "We use the BM25L ranking function ( Trotman et al , 2014 ) to investigate how a basic IR approach can detect answer spans using TF - IDF features . We adopt a sliding window approach with a window size of 512 and a stride of one sentence . We compare BM25L with random window ( span ) selection and the first and last window selection in the documents . Table 4 presents the results of the ranking functions . In the BM25L - oracle , we set the window size to the target answer span size . BM25L - oracle outperforms the other methods but the results are far from perfect . There is no significant difference between BM25L and other methods . The results restate the fact that there is little word overlap between non - factoid questions and their answers . We analyze the performance of BERT and RoBERTa with different hyper - parameters on the development set in Table 5 . Smaller strides , i.e. , higher overlap between the segments , and warm - up contribute to better performances . RoBERTa constantly outperforms BERT , which is to be expected as RoBERTa is optimized robustly during the pretraining . We use the HuggingFace 's Transformers ( Wolf et al , 2019 ) code 4 and train the base and large models on 2 and 4 GPUs , respectively . We have to use a batch size of 12 and 8 , respectively , for the base and large models because of the long input sequence size and memory limitations . We use the official AllenAI Longformer code 5 to train Longformer on NLQuAD . We use the same batch size of 12 ( batch size of 1 and gradient accumulation over 12 batches ) and learning rate warmup for the first 1 , 000 steps . Due to memory requirements , we limit the experiments to only the Longformer base model ( the large model can not fit on our GPUs even with a batch size of 1 ) . We ran the experiments on 2 NVIDIA P40 ( 24 GB GPU memory ) for about one day for 5 epochs . Similarly , we choose the best epoch based on the performance on the development set . Table 6 summarizes the scores obtained by the baseline systems on the NLQuAD evaluation set . While Longformer significantly outperforms BERT and RoBERTa , its performance , particularly in terms of IoU and EM , is far from perfect . This demonstrates that NLQuAD and non - factoid QA is still an open problem for state - of - the - art models .", "entities": [[149, 150, "MethodName", "BERT"], [151, 152, "MethodName", "RoBERTa"], [185, 186, "MethodName", "RoBERTa"], [188, 189, "MethodName", "BERT"], [196, 197, "MethodName", "RoBERTa"], [239, 241, "HyperparameterName", "batch size"], [270, 271, "MethodName", "Longformer"], [275, 276, "MethodName", "Longformer"], [283, 285, "HyperparameterName", "batch size"], [288, 290, "HyperparameterName", "batch size"], [300, 302, "HyperparameterName", "learning rate"], [323, 324, "MethodName", "Longformer"], [339, 341, "HyperparameterName", "batch size"], [400, 401, "MethodName", "Longformer"], [403, 404, "MethodName", "BERT"], [405, 406, "MethodName", "RoBERTa"], [414, 415, "MetricName", "IoU"], [416, 417, "MetricName", "EM"]]}
{"text": "To ensure that the samples are of high quality , in addition to the initial investigation and pre - processing steps , we asked four volunteers to investigate 50 random samples from the evaluation set . They rated the goodness of answers on a 3 - point scale : ( 1 : Irrelevant answer ; 2 : Good answer after adding or removing some sentences ; 3 : Perfect answer ) . The average score is 2.56 indicating the high quality of NLQuAD 's QA samples . In order to benchmark human performance , we asked the four volunteers to answer 50 questions , a randomly sampled subset of evaluation set . They were given unlimited time to detect the answers , but on average , it took them about 270 seconds to answer a question . ing the best human answer in terms of our primary evaluation metric ( IoU ) for each sample . While NLQuAD is a challenging task both for humans and the state of the art QA models , the human upper bound performance significantly outperforms the models . We suspect that the mediocre average of human performance , considering the high score of the target answers , might be because volunteers are not familiar with the articles ' writing style or they might have become exhausted by reading long articles . Furthermore , we asked another volunteer to compare the target answers with the predicted answers in a pairwise comparison for 100 samples . Figure 6 shows that the target answers are preferred in 37 % and 64 % of cases over the Longformer and RoBERTa predictions , respectively . The human evaluation is in line with the results shown in Table 6 and Table 7 .", "entities": [[150, 151, "MetricName", "IoU"], [269, 270, "MethodName", "Longformer"], [271, 272, "MethodName", "RoBERTa"]]}
{"text": "Figure 7 compares the performance of BERT , RoBERTa , and Longformer for instances with different document and answer lengths . As expected , both longer documents and longer answers are harder for the models . Surprisingly , BERT and RoBERTa outperform Longformer for longer answers . The same pattern occurs for F1 and EM ( not shown in the figure ) . Figure 7 ( right ) shows that RoBERTa and BERT behave completely differently compared to Longformer for longer answer lengths . The former models have a bias to predict longer spans while Longformer under - estimates the length of the answer span . This different behaviour might be due to the sliding window approach and the prediction aggregation in the RoBERTa and BERT models and the attention dilation strategy in Longformer .", "entities": [[6, 7, "MethodName", "BERT"], [8, 9, "MethodName", "RoBERTa"], [11, 12, "MethodName", "Longformer"], [38, 39, "MethodName", "BERT"], [40, 41, "MethodName", "RoBERTa"], [42, 43, "MethodName", "Longformer"], [52, 53, "MetricName", "F1"], [54, 55, "MetricName", "EM"], [70, 71, "MethodName", "RoBERTa"], [72, 73, "MethodName", "BERT"], [78, 79, "MethodName", "Longformer"], [95, 96, "MethodName", "Longformer"], [123, 124, "MethodName", "RoBERTa"], [125, 126, "MethodName", "BERT"], [133, 134, "MethodName", "Longformer"]]}
{"text": "We introduce NLQuAD , a non - factoid long question answering data set from BBC news articles . NLQuAD 's question types and the long lengths of its context documents as well as answers , make it a challenging real - world task . We propose to use Intersection over Union ( IoU ) as an evaluation metric for long question answering . To establish a baseline performance , we experimented with the BERT , RoBERTa , and Longformer question answering models . Longformer outperforms the other methods with an IoU of 73.57 % , but the results show that the performance of state - of - the - art question answering systems is far from perfect . We hope NLQuAD will inspire more research in the area of document - level language understanding and question answering .", "entities": [[9, 11, "TaskName", "question answering"], [52, 53, "MetricName", "IoU"], [60, 62, "TaskName", "question answering"], [73, 74, "MethodName", "BERT"], [75, 76, "MethodName", "RoBERTa"], [78, 79, "MethodName", "Longformer"], [79, 81, "TaskName", "question answering"], [83, 84, "MethodName", "Longformer"], [90, 91, "MetricName", "IoU"], [110, 112, "TaskName", "question answering"], [135, 137, "TaskName", "question answering"]]}
{"text": "There can be many possible POLAR dimensions , which requires to select the most suitable ones . That is , we want to define a limited set of opposites that best describes words or emoji w.r.t . interpretability across the whole embedding . Extremal Word Score ( EWSO ) . We propose a new metric to measure the quality of polar dimensions complementing heuristics from ( Mathew et al , 2020 ) . It measures the embedding confidence and consistency along available differentials . The idea of POLAR \u03c1 is that directions represent semantics within the input embedding . We determine embedded terms shortest distance to these axes via orthogonal projection ; we use resulting intersections as the position w.r.t . the directions . That is , as a new heuristic , for each of our differentials dir i , we look out for k = 10 embedded words at the extremes ( having the highest scores in each direction ) and take their average cosine distance within the original embedding D to the differential as a measure . This results in the average similarity of existing extremal words on our scale - a heuristic that represents the skew - whiffiness within the extremes on a differential scale .", "entities": [[45, 46, "MetricName", "Score"], [144, 146, "HyperparameterName", "k ="]]}
{"text": "Preprocessing . We tokenize sentences with spaCy and remove stopwords . To increase amounts of available data , we remove all emoji modifiers ( skin tone and gender ) : { , , } . Due to German language , we keep capitalization . Original Embedding . We use gensim implementation of Word2Vec ( W2V ) . A qualitative investigation suggests that skip - gram works better than CBOW ( better word analogy ) . We kept training parameters largely at defaults including negative sampling , opting for d = 300 dimensions . Interpretable Embedding . The actual application of embedding transformation is simple . We create the matrix of differentials dir , the POLAR subspace , according to our antonym - set P words \u222a P emoji ( 3.2 ) . After normalizing the subspace vectors , we create all embedding vectors via projec - tion with interpret W Mixed E Words ( W / W ) ( W / M ) ( W / E ) Emoji ( E / W ) ( E / M ) ( E / E ) ( \u2212 E v = d ir T \u2212 W v , \u2200v V. Though normalization requires careful later additions to the PO - LAR space , we opted for standard normalization , E stdnrm = [ E \u2212 mean ( E ) ] std ( E ) \u22121 , to ensure that the whole embedding space aligns properly around the center of gravity on each differential scale . We select the best suited opposites for a given embedding space by using the Extremal Word Score ( 2.3 ) for d=500 + 44 dimensions ( words + emoji ) .", "entities": [[269, 270, "MetricName", "Score"]]}
{"text": "Emoji . As a byproduct , we also show if emoji opposites are preferred over words . That is , we focus on the mixed campaigns describing words and emoji with words and emoji ( * /M ) . We establish a baseline by filtering the counts for all non - POLAR \u03c1 randomly chosen dimensions being word or emoji representing a Bernoulli experiment . I.e. , along the random dimensions , our coders chose 228 vs. 221 and 167 vs. 187 words over emoji . Applying chi - squared statistics indicates , that both types ( words and emoji ) are chosen equally often at least can not be rejected . We next analyze the POLAR \u03c1 chosen dimensions in the mixed campaigns . Here , coders chose words over emoji as follows : 465 vs. 336 in the ( W / M ) , and 414 vs. 482 in the ( E / M ) campaign . We find statistically significant favors for words to interpret words and emoji to describe emoji . Scale Usage . We find no evidence for any directional biases within our preference test ( cf . 3c ) . Coder Agreement . While the aggregate results are compelling , we use the Krippendorff - alpha metric to measure coder agreement along all six campaigns as shown Tab . 2 ; higher scores depict better agreement . We split the overall results by test first ( Selection & Preference ) , but also show additional agreement results for preferences along POLAR \u03c1 chosen dimensions and their random counterpart . Most agreement is within the moderate regime . This observation does not come unexpected from our five non - expert classifiers per task . Overall , we find that coders agree better for well - performing campaigns . We identify the best agreement scores for interpreting emoji with emoji ( E / E ) ; coders agree least in the worst performing explaining words with emoji campaign ( W / E ) . For the preference test , we subdivide our results into POLAR \u03c1 chosen differentials and compare them to the randomly chosen ones . While the agreement on the random opposites is only fair , the agreement on POLAR \u03c1 chosen opposites is consistently better : Estimating differential scale directions via POLAR \u03c1 for words yields moderate agreement , whereas coders consistently align substantially in interpreting emoji . We presume emoji may convey limited ideas , but are easier to grasp , have better readability ; the campaings interpreting emoji ( E/ * ) were generally accomplished faster .", "entities": [[211, 212, "HyperparameterName", "alpha"]]}
{"text": "This paper describes the NoahNMT system submitted to the WMT 2021 shared task of Very Low Resource Supervised Machine Translation . The system is a standard Transformer model equipped with our recent technique of dual transfer . It also employs widely used techniques that are known to be helpful for neural machine translation , including iterative backtranslation , selected finetuning , and ensemble . The final submission achieves the top BLEU for three translation directions .", "entities": [[18, 20, "TaskName", "Machine Translation"], [26, 27, "MethodName", "Transformer"], [51, 53, "TaskName", "machine translation"], [70, 71, "MetricName", "BLEU"]]}
{"text": "In this paper , we describe the NoahNMT system submitted to one of the WMT 2021 shared tasks . The shared task features both unsupervised machine translation and very low resource supervised machine translation . As our core technique is mainly suitable for low resource supervised machine translation , we participated in four translation directions between Chuvash - Russian ( chv - ru ) and Upper Sorbian - German ( hsb - de ) . Our core technique is called dual transfer ( Zhang et al , 2021 ) , which belongs to the family of transfer learning . It transfers from both high resource neural machine translation model and pretrained language model to improve the quality of low resource machine translation . During the preparation for the shared task , we conducted additional experiments that supplement the original paper , including the choice of parent language , the validation of Transformer big model , and the usage of dual transfer along with iterative back - translation . In addition , we also applied proven techniques to strengthen the quality of our system , including selected finetuning and ensemble . Our final submission achieves the top BLEU on the blind test sets for three translation directions : chv ru , ru chv , and hsb de .", "entities": [[24, 27, "TaskName", "unsupervised machine translation"], [32, 34, "TaskName", "machine translation"], [46, 48, "TaskName", "machine translation"], [96, 98, "TaskName", "transfer learning"], [106, 108, "TaskName", "machine translation"], [120, 122, "TaskName", "machine translation"], [151, 152, "MethodName", "Transformer"], [196, 197, "MetricName", "BLEU"]]}
{"text": "Selected finetuning aims to deal with the domain difference that may exist between the test set and the training set . Given the source side of the test set , we try to select similar source sentences from the training set , and then finetune the translation model on the selected subset of training sentence pairs . We use BM25 ( Robertson and Zaragoza , 2009 ) to calculate the similarity between two sentences for retrieval . The BM25 score between a query sentence Q and a sentence D in the corpus for parent language chv ru BLEU kk 18.47 en 18.61 retrieval C is given by s ( D , Q ) = L Q i=1 IDF ( q i ) ( k + 1 ) TF ( q i , D ) k 1 \u2212 b + b L D Lavg + TF ( q i , D ) , where the query sentence Q is a sequence of L Q subwords { q i } L Q i=1 , IDF ( q i ) is the Inverse Docu - ment Frequency for q i in the corpus C , TF ( q i , D ) is the Term Frequency for q i in the sentence D , L D is the length of the sentence D , L avg is the average length of the corpus C , k and b are hyperparameters , which are set as 1.5 and 0.75 , respectively . Based on the BM25 score , we calculate the similarity between a source test sentence ( as the query sentence ) and the source sentences in the training set to obtain the top 500 sentences . After performing the selection for all the source test sentences , we merge them and remove duplicates to obtain the set for finetuning . 3 Experimental Setup", "entities": [[97, 98, "MetricName", "BLEU"]]}
{"text": "We collected allowed data for the involved languages and followed the same preprocessing pipeline of punctuation normalization and tokenization , using scripts from Moses 2 . The English monolingual data came from the English original side of ru - en back - translated news 3 , but its automatic translation to Russian was discarded . The provided Chuvash - Russian dictionary was not used . Each language was encoded with byte pair encoding ( BPE ) ( Sennrich et al , 2016b ) . The BPE codes and vocabularies were learned on each language 's monolingual data , and then used to segment parallel data . We used 32k merge operations for all languages . After BPE segmentation , we discarded sentences with more than 128 subwords , and cleaned parallel data with length ratio 1.5 . Training data statistics is provided in Table 1 . Note that we experimented with Kazakh ( kk ) data ( Section 4.1 ) , but did not use it for our final submission . Evaluation on test sets is given by SacreBLEU 4 ( Post , 2018 ) , after BPE removal and detokenization .", "entities": [[70, 73, "MethodName", "byte pair encoding"], [74, 75, "MethodName", "BPE"], [85, 86, "MethodName", "BPE"], [116, 117, "MethodName", "BPE"], [178, 179, "MetricName", "SacreBLEU"], [187, 188, "MethodName", "BPE"]]}
{"text": "We use Transformer ( Vaswani et al , 2017 ) as our translation model , but with slight modifications that follow the implementation of BERT 5 . The absolute position embeddings are also learned as in BERT . The encoder and decoder embeddings are independent because each language manages its own vocabulary , but we tie the decoder input and output embeddings ( Press and Wolf , 2017 ) . We apply dropout with probability 0.1 . We use LazyAdam as the optimizer . Learning rate warms up for 16 , 000 steps and then follows inverse square root decay . The peak learning rate is 5 \u00d7 10 \u22124 for parent translation models , and 1 \u00d7 10 \u22124 for child translation models . Early stopping occurs when the validation BLEU does not improve for 10 checkpoints . We set checkpoint frequency to 2 , 000 updates for parent translation models and 1 , 000 updates for child translation models . The batch size is 6 , 144 tokens per GPU and 8 NVIDIA V100 GPUs are used . Hyperparameters for BERT are the same as in the original paper ( Zhang et al , 2021 ) . For selected finetuning , we use stochastic gradient descent as the optimizer , and the learning rate is 1 \u00d7 10 \u22125 . We finetune for 10 , 000 updates , and save a checkpoint every 100 updates . The checkpoint with the highest validation BLEU is kept .", "entities": [[2, 3, "MethodName", "Transformer"], [24, 25, "MethodName", "BERT"], [36, 37, "MethodName", "BERT"], [82, 83, "HyperparameterName", "optimizer"], [84, 86, "HyperparameterName", "Learning rate"], [103, 105, "HyperparameterName", "learning rate"], [125, 127, "MethodName", "Early stopping"], [131, 132, "MetricName", "BLEU"], [163, 165, "HyperparameterName", "batch size"], [182, 183, "MethodName", "BERT"], [205, 208, "MethodName", "stochastic gradient descent"], [210, 211, "HyperparameterName", "optimizer"], [214, 216, "HyperparameterName", "learning rate"], [244, 245, "MetricName", "BLEU"]]}
{"text": "We ran five iterations of iterative back - translation . Results are shown in Table 5 . The best BLEU scores are attained with two or three iterations . Another observation is that iterative back - translation brings larger improvements for chv ru and hsb de than ru chv and de hsb . This is probably because the monolingual data for chv and hsb are small in quantity .", "entities": [[19, 20, "MetricName", "BLEU"]]}
{"text": "We validate the effectiveness of ensemble on hsb de and de hsb , by performing ensemble decoding from the five models from iterative back - translation . Results in Table 7 demonstrate that ensemble gives BLEU improvements of about 0.8 .", "entities": [[35, 36, "MetricName", "BLEU"]]}
{"text": "Baseline ( Brun and Nikoulina , 2018 ) - 38.10 TAS - LPM - CRF ( Wan et al , 2020 ) 54.76 64.66 TAS - SW - CRF ( Wan et al , 2020 ) 57.51 65.89 TAS - SW - TO ( Wan et al , 2020 ) 58.09 65.44 all experiments . T5 closely follows the original encoder - decoder architecture of the Transformer model , with some slight differences such as different position embedding schemes . Therefore , the encoder and decoder of it have similar parameter size as the BERT - BASE model . For all tasks , we use similar experimental settings for simplicity : we train the model with the batch size of 16 and accumulate gradients every two batches . The learning rate is set to be 3e - 4 . The model is trained up to 20 epochs for the AOPE , UABSA , and ASTE task and 30 epochs for the TASD task .", "entities": [[12, 13, "MethodName", "LPM"], [14, 15, "MethodName", "CRF"], [28, 29, "MethodName", "CRF"], [55, 56, "MethodName", "T5"], [66, 67, "MethodName", "Transformer"], [94, 95, "MethodName", "BERT"], [96, 97, "MethodName", "BASE"], [117, 119, "HyperparameterName", "batch size"], [129, 131, "HyperparameterName", "learning rate"]]}
{"text": "The main results for the AOPE , UABSA , ASTE , TASD task are reported in Tables 1 , 2 , 3 , 4 respectively . For our proposed GAS framework , we also present the raw results without the proposed prediction normalization strategy ( with the suffix \" - R \" ) . All results are the average F1 scores across 5 runs with different random seeds . It is noticeable that our proposed methods , based on either annotation - style or extraction - style modeling , establish new state - of - the - art results in almost all cases . The only exception is on the Rest15 dataset for the AOPE task , our method is still on par with the previous best performance . It shows that tackling various ABSA tasks with the proposed unified generative method is an effective solution . Moreover , we can see that our method performs especially well on the ASTE and TASD tasks , the proposed extraction - style method outperforms the previous best models by 7.6 and 3.7 average F1 scores ( across different datasets ) on them respectively . It implies that incorporating the label semantics and appropriately modeling the interactions among those sentiment elements are essential for tackling complex ABSA problems .", "entities": [[58, 60, "MetricName", "average F1"], [67, 68, "DatasetName", "seeds"], [180, 182, "MetricName", "average F1"]]}
{"text": "State - of - the - art dialogue models still often stumble with regards to factual accuracy and self - contradiction . Anecdotally , they have been observed to fail to maintain character identity throughout discourse ; and more specifically , may take on the role of their interlocutor . In this work we formalize and quantify this deficiency , and show experimentally through human evaluations that this is indeed a problem . In contrast , we show that discriminative models trained specifically to recognize who is speaking can perform well ; and further , these can be used as automated metrics . Finally , we evaluate a wide variety of mitigation methods , including changes to model architecture , training protocol , and decoding strategy . Our best models reduce mistaken identity issues by nearly 65 % according to human annotators , while simultaneously improving engagingness . Despite these results , we find that maintaining character identity still remains a challenging problem .", "entities": [[16, 17, "MetricName", "accuracy"]]}
{"text": "The exchange of stories from one 's past , or descriptions of activities in one 's present , are a fundamental part of human discourse . Trustworthy human conversationalists keep their stories roughly straight within a conversation . An interlocutor taking on your own stories and persona as theirs is especially jarring and unnatural . However , despite the improvements in state - of - the - art open - domain dialogue modeling , both in terms of distributional accuracy metrics like perplexity , and subjectively in terms of human judgements ( Adiwardana et al , 2020 ; Roller et al , 2021 ) , interactions with those agents reveal that they can not keep their stories straight . In particular , they are likely to take on the role of their interlocutor ; for example , if an agent 's partner says they are a software engineer , the agent is likely to say it is a software engineer too ( Roller et al , 2021 ) , or worse , appropriate their partners just told tale of a trip to NAACL as their own . Some ( Roller et al , 2021 ) and fine - tuned on LIGHT ( Urbanek et al , 2019 ) . The bold words in red highlight the model mistaking its identity for its partner 's . ( Top ) The model believes it is a thief , rather than a guest . ( Bottom ) The model believes it is a hunter rather than a helper . Token probabilities are given at the position of the mistake for the two names . example failure cases are given in Table 1 , where models incorrectly take on the name , role or activities of their partner instead of their assigned role . These failures are related to the general problems of repetition in language models ( Holtzman et al , 2020 ) , the weak influence of word order ( Sinha et al , 2021 ) and inability to avoid contradictions ( Nie et al , 2021 ) . In this work we formalize and quantify this behavior , show that to some extent it can be detected automatically with a specifically trained classifier , and then study a wide variety of mitigations . These include multi - objective training , unlikelihood training , classifier - assisted re - ranking based generation , and several forms modifying the attention mechanisms of the decoder in a sequence to sequence model . Our best methods can reduce mistaken identity issues by 65 % , while simultaneously improving inconversation engagingness ; indeed , our models that can stick to their role in conversation are judged by humans to be significantly more engaging than their baseline counterparts . Despite these advances , we find that there is still considerable space to improve these results further in future work . We make publicly available both our trained models and code to reproduce results 1 .", "entities": [[79, 80, "MetricName", "accuracy"], [82, 83, "MetricName", "perplexity"], [139, 140, "DatasetName", "agent"], [150, 151, "DatasetName", "agent"], [413, 416, "MethodName", "sequence to sequence"]]}
{"text": "We first define a metric , role - playing accuracy ( RPA ) , to denote how often a model 's responses are \" in - character \" ; by this , we mean how often the model 's response could feasibly be said by their character , given their assigned character identity . Measuring RPA is a non - trivial task for a variety of reasons . First , some conversations involve pairs that can reasonably say similar things ( priest vs. priestess , man vs. woman , wizard vs. witch ) . Second , opening lines are often more generic ( \" hello \" , \" how fare your travels today \" ) , so either character can say it in conversation . The third reason stems from the data that we study ; we are relying on crowdsourced data in which humans are required to portray their characters . Some crowdworkers may be better than others , and there may be some noise in the dataset in which , e.g. , a horse may proclaim its love for a queen , or a knight may discuss at length the kingdom 's tax collecting . Given the difficulties above , our primary measure of RPA involves human annotation of model responses , specifically evaluating whether a candidate response fits a given model 's character . We thus have human crowdworkers chat with each model in a LIGHT setting ; each is given a character and asked to role - play , while the human an - notates each model response , determining whether the model is in character : we denote this metric as \" Mistaken Identity \" in our experiments , and other utterance - level annotations are collected . Further details regarding human evaluation are outlined in Section 4.7 . Despite the efficacy of human evaluation , it is both costly and slow ; as a proxy , we thus train models specifically designed to identify whether a candidate response from a model fits the model 's role , and denote these as \" RPA Classifiers \" . We employ poly - encoder transformers ( Humeau et al , 2020 ) to learn this metric , and structure the task as a ranking one ; the model receives the LIGHT setting and prior utterances of dialogue as input , as well as the response currently under consideration , and the model must choose the correct character from a fixed set of candidates . We also explore RPA classifiers trained on all partially complete sequences of labels , such that the classifiers can determine the character speaking without requiring the full utterance ; we call these left - to - right ( LTR ) RPA classifiers . Further details about how our RPA classifiers are built are given in Appendix B.", "entities": [[9, 10, "MetricName", "accuracy"]]}
{"text": "We can employ an RPA classifier in response generation by using it to rank candidate model outputs . Utterance Re - ranking : Given a set of candidate responses , the RPA classifier can re - score the set and return the response yielding the highest probability of staying in character ( according to the RPA score on the complete candidate generations ) . The dialogue models employ beam - search to generate responses , and the candidates for re - ranking are the beams within beam - search . We also try nucleus sampling ( Holtzman et al , 2020 ) and delayed beam - search ( Massarelli et al , 2020 ) to see whether more diverse candidates have any effect . Partial And Complete Efficient Re - ranking ( PACER ) : Re - ranking only the final beam candidates may be suboptimal because it is well known that those candidates are not very diverse ( Kulikov et al , 2019 ) , meaning there may not be any good candidates to choose from in this final set . In order to generate utterances that agree with our classifiers , a possible improvement is to generate the utterance such that partial generations also agree with the classifier when generating left - to - right , ensuring that good candidates are surfaced . With access to LTR RPA classifiers , we can apply re - ranking to partial sequences . Unfortunately , re - ranking at every step of beam search , for every token , requires significant computation , such as in the recent FUDGE method ( Yang and Klein , 2021 ) . FUDGE re - scores tokens at each decoding step by multiplying the classifier probability with each token probability , and renormalizing , which is used for control tasks with lightweight classifiers in order to be tractable . In our proposed approach , called PACER , we re - score candidate tokens , for each beam , according to the probability that their inclusion yields the appropriate character classification , and then finally re - rank the complete candidate beams . To make this efficient , we crucially score only a small proportion of decoding steps ( e.g. , 5 % of token positions ) as well as for only a few candidate rescored tokens ( e.g. , top 10 only ) . We can control these hyperparameters to explore the speed vs. accuracy trade - off .", "entities": [[7, 9, "TaskName", "response generation"], [409, 410, "MetricName", "accuracy"]]}
{"text": "Table 4 gives results for RPA - based re - ranking of generation models . Automated results show a slight bump in F1 on the LIGHT valid set , and indeed a bump in RPA . Including the intra - generation re - ranking with PACER yields an even higher RPA score . Table 3 contains the results of varying the candidate tokens re - ranked per intra - generation step ( # Toks ) and number of partial re - ranking steps ( Freq ) , both in terms of generation metrics / RPA and relative computational cost compared to reranking . Increasing # of toks or increasing the frequency can lead to improved F1 and RPA , but with significant latency increase for too high values ( e.g. over 11x when applying re - ranking for every partial step using the top 10 tokens each time ) . Applying both partial and final complete ranking helps performance . Note that re - ranker models use the same model to re - rank that is being used to measure RPA afterwards , making that metric biased . Hence , human evaluations are required for this model , which will be detailed in Section 4.7 , and which will indicate that re - ranking does in fact help .", "entities": [[22, 23, "MetricName", "F1"], [115, 116, "MetricName", "F1"]]}
{"text": "Results of unlikelihood ( UL ) training are also given in Table 4 . We apply UL loss to the 128 - truncation model in two different ways : ( 1 ) Top - 1 : apply the loss on the token that yields the most incorrect partial sequence RPA classification ; ( 2 ) All : apply the loss to all tokens that yield an incorrect RPA classification on partial sequences . The RPA UL methods suffer compared to the baselines in terms of PPL and F1 , yet they retain similar RPA metrics . We hypothesize that while the UL loss can adjust the model to refrain from generating outof - character responses , there are still far too many other tokens that may yield similar outcomes that are not penalized . Table 12 in Appendix D includes similar results with the 1024 - truncation model .", "entities": [[17, 18, "MetricName", "loss"], [38, 39, "MetricName", "loss"], [59, 60, "MetricName", "loss"], [87, 88, "MetricName", "F1"], [102, 103, "MetricName", "loss"]]}
{"text": "Multi - objective training results are in Table 5 , where the base model is a 1024 - truncation model . We measure generation metrics in terms of RPA ( with PPL and F1 in Table 13 in Appendix E ) , and classification metrics in terms of Hits@1/427 as before . The model is able to predict the appropriate character using either the decoder outputs or the en - coder+decoder outputs . hits@1 for the best model ) , this does not translate to substantial RPA improvements over the baseline .", "entities": [[33, 34, "MetricName", "F1"], [73, 74, "MetricName", "hits@1"]]}
{"text": "We show results for the automated grounding of expanded attention in Table 7 . Attempting to use the decoder attention weights to select expanded attention context yields no additional benefits , which is not surprising : if the model could identify the pertinent components of the input beforehand , it would not require a reattention . The trainable mask does not yield any benefits either . However , using the RPA classifier attention weights to inform the model which tokens to re - attend to yields improved performance across all three metrics compared to the baseline , and PPL is nearly the same as profile grounding ( 12.19 vs. 12.18 ) , while RPA trails slightly behind ( 91.11 vs. 91.79 ) . We also include the usage of the bottom - k tokens from the classifier weights to emphasize that there is indeed signal from the top - k , as using the bottom tokens does not help . Automated Grounding + Multi - Objective Table 5 shows that combining automated grounding with the multi - objective task yields higher hits@1 compared to not using the trainable mask , especially in the first stage of multi - objective training . However , RPA scores are only fractionally better than the baseline . Appendix E includes results across more settings ( see Table 13 and Table 14 ) . Expanded Attention + RPA Re - ranking The expanded attention and RPA re - ranker methods can also both be applied to obtain effective models . Results are in Table 4 ; indeed , the combination yields the highest F1 and RPA scores .", "entities": [[181, 182, "MetricName", "hits@1"], [268, 269, "MetricName", "F1"]]}
{"text": "We further explored three decoding settings : standard beam - search , delayed beam search ( Massarelli et al , 2020 ) and nucleus sampling ( Holtzman et al , 2020 ) , both in a re - ranking setting and not . When considering performance on automated metrics ( provided in Table 20 in the Appendix ) , we see that generation settings other than beam search , when using a re - ranker , yield lower F1 scores but higher RPA scores , as the RPA re - ranker has more diversity of candidate responses from which to choose ; however , these methods perform worse in human evaluations , with nucleus sampling reranking yielding far more problems and far lower engagingness ratings . Qualitative analysis of outputs on the test set are in Appendix J.1 .", "entities": [[78, 79, "MetricName", "F1"]]}
{"text": "The RPA classifier models are trained with a cross - entropy loss over the correct label , with 99 random negatives chosen from the training set ; we ensured that each character in conversation showed up in the set of candidate labels . The models were trained with a batch size of 16 on 4 32 GB GPUs , with early stopping on the validation set according to valid accuracy . We used the Adam optimizer ( Kingma and Ba , 2015 ) with weight decay ( Loshchilov and Hutter , 2019 ) , sweeping over learning rates { 1e \u2212 5 , 5e \u2212 6 } . Generative Models All variants of generative models were trained using 8 32 GB GPUs , with early stopping on perplexity on the validation set . We used the Adam optimizer , sweeping over learning rates { 1e \u2212 5 , 7e \u2212 6 } , training with a batch size of 128 for the short - truncation models , and 32 for the long - truncation models . For the multiobjective models , we used the same loss ( and negative - sampling ) setup as the RPA classifiers for the character accuracy objective . During inference , unless otherwise specified , we generated using beam - search with beam size of 10 , enforcing a minimum length of 20 , and with tri - gram blocking with respect to both the context and the current generation .", "entities": [[11, 12, "MetricName", "loss"], [49, 51, "HyperparameterName", "batch size"], [60, 62, "MethodName", "early stopping"], [69, 70, "MetricName", "accuracy"], [74, 75, "MethodName", "Adam"], [75, 76, "HyperparameterName", "optimizer"], [84, 86, "MethodName", "weight decay"], [124, 126, "MethodName", "early stopping"], [127, 128, "MetricName", "perplexity"], [136, 137, "MethodName", "Adam"], [137, 138, "HyperparameterName", "optimizer"], [156, 158, "HyperparameterName", "batch size"], [185, 186, "MetricName", "loss"], [200, 201, "MetricName", "accuracy"]]}
{"text": "In Table 12 , we compare UL models across different truncation lengths ; the same story applies to the 1024 - truncation models . We additionally include a third method , Random - 3 , where we apply the loss randomly to 3 tokens that yield incorrect RPA classifications . This method performs about the same as the Top - 1 method , but the RPA is lower , indicating that the Top - 1 method at least is providing some signal . E Multi - Objective : Additional Results", "entities": [[39, 40, "MetricName", "loss"]]}
{"text": "Table 13 displays full PPL and F1 scores corresponding to the models in Table 5 .", "entities": [[6, 7, "MetricName", "F1"]]}
{"text": "We experiment with various generation settings , with or without re - rankers ; results are in Table 20 . For the baseline and re - ranker models , beam search yields the highest F1 scores ; RPA can be improved with the other inference methods when combined with a re - ranker . We believe this may be due to the higher diversity of candidate responses generated from those methods .", "entities": [[34, 35, "MetricName", "F1"]]}
{"text": "We analyze the correlation between human annotations and the automatic metrics collected on the LIGHT validation set , as shown in Figure 3 ; we note some interesting trends : Perplexity perplexity appears to be positively correlated with mistaken identity , and negatively correlated with engagingness . So , perplexity is a good indicator of how fluent and engaging the model is in conversation , and can indirectly point to a better understanding of the role - playing task . An important note is that we only tested this amongst models of the same size , and only for the models we tested , so it is not clear that larger models will necessarily bring improvements . F1 F1 word overlap is positively correlated with engagingness as well , so F1 may be a good proxy of model performance . Correlation with mistaken identity is negative here , implying that better F1 corresponds with better role - playing ability . However , we note that F1 is not a catch - all metric ( Liu et al , 2016 ) . RPA RPA appears to be strongly negatively correlated with mistaken identity , indicating that it is indeed a good measure of the model 's ability to stay in character . It is weakly negatively correlated with the other issues , and is somewhat positively correlated with engagingness as well . These correlations give us confidence that our RPA classifiers are adequately measuring role - playing ability within models .", "entities": [[30, 31, "MetricName", "Perplexity"], [31, 32, "MetricName", "perplexity"], [49, 50, "MetricName", "perplexity"], [117, 118, "MetricName", "F1"], [118, 119, "MetricName", "F1"], [130, 131, "MetricName", "F1"], [151, 152, "MetricName", "F1"], [165, 166, "MetricName", "F1"]]}
{"text": "In Figure 1 , we see RPA results across turns of conversation for a wider variety of models . Human The human outputs are most often correct on the first turn , with gradual decay of accuracy throughout the conversation ( according to RPA ) .", "entities": [[36, 37, "MetricName", "accuracy"]]}
{"text": "The vanilla baseline suffers a pretty dramatic drop off after the first couple of turns ; the long - context model achieves slightly higher character accuracy overall but we see similar drop offs farther down the conversation . RPA UL The unlikelihood models seem to recover somewhat in the initial turns of conversation , however later turns still yield sharp drop offs in RPA . Multi - objective Similarly to the UL case , we see the most gains in initial turns compare to the vanilla baselines ; however , we see even more dramatic drop offs towards the end of the conversation . Expanded Attention With profile grounding , we see near - human performance , with even better performance towards the end of the conversation . The automatic grounding improves over the baseline but is slightly worse than profile grounding . Combining automated grounding with multi - objective training leads to some benefits in earlier turns , but later turns still suffer . Re - ranking Although we 're using the same RPA classifier to both re - ranker and score the model outputs , it is still interesting to examine on which turns the re - ranker benefits the model the most . We see in the last set of graphs that beam re - ranking Figure 4 : Vanilla Attention . The speaker here is the mermaid , whose partner is a sea - witch . The last utterance from the sea - witch is , \" What are you doing on the turquoise shore ? \" . The mermaid responds , \" I 've been catching waves with the dolphins all morning . What kind of victims do you expect to find in a tranquil place like this ? \" . The vanilla model spreads its attention across the whole context ; blue boxes at the top are attentions over the character descriptions , while the bottom box is attention over the word \" victims \" . Figure 5 : Profile Expanded Attention . The speaker here is the mermaid , whose partner is a sea - witch . The last utterance from the sea - witch is , \" What are you doing on the turquoise shore ? \" . The mermaid responds , \" I 've been catching waves with the dolphins all morning . What kind of victims do you expect to find in a tranquil place like this ? \" . Left original attention over the full context ; Right expanded attention over the additional context . The top two boxes are the partner name and self name ; the bottom box on the left refers to \" victims \" , and on the right refers to the \" dolphins \" .", "entities": [[25, 26, "MetricName", "accuracy"]]}
{"text": "Simultaneous Translation aims to translate the speech of a source language into a target language as quickly as possible without interrupting the speaker . Typically , a simultaneous translation system is comprised of an auto - speech - recognition ( ASR ) model and a machine translation ( MT ) model . The ASR model transforms the audio signal into the text of source language and the MT model translates the source text into the target language . Recent studies on simultaneous translation ( Cho and Esipova , 2016 ; Ma et al , 2019 ; Arivazhagan et al , 2019 ) focus on the trade - off between translation quality and latency . They explore a policy that determines when to begin translating with the input of a stream of transcription . However , there is a gap between transcription and ASR that some ASR model does n't provide punctuations or can not provide accurate punctuation in realtime , while the transcription is always well - formed . See Figure 1 for illustration . Without sentence boundaries , the state - of - the - art wait - k model takes insufficient text as input and produces an incorrect translation . Therefore , sentence boundary detection ( or sentence segmentation ) 1 plays an important role to narrow the gap between the ASR and transcription . A good segmentation will not only improve translation quality but also reduce latency . Studies of sentence segmentation falls into one of the following two bins : The strategy performs segmentation from a speech perspective . F\u00fcgen et al ( 2007 ) and Bangalore et al ( 2012 ) used prosodic pauses in speech recognition as segmentation boundaries . This method is effective in dialogue scenarios , with clear silence during the conversation . However , it does not work well in long speech audio , such as lecture scenarios . According to Venuti ( 2012 ) , silence - based chunking accounts for only 6.6 % , 10 % , and 17.1 % in English , French , and German , respectively . Indicating that in most cases , it can not effectively detect boundaries for streaming words . The strategy takes segmentation as a standard text processing problem . The studies considered the problem as classification or sequence labeling , based on SVM , conditional random filed ( CRFs ) ( Lu and Ng , 2010 ; Wang et al , 2012 ; Ueffing et al , 2013 ) . Other researches utilized language model , either based on N - gram ( Wang et al , 2016 ) or recurrent neural network ( RNN ) ( Tilk and Alum\u00e4e , 2015 ) . In this paper , we use classification to solve the problem of sentence segmentation from the perspective of text . Instead of predicting a sentence boundary for a certain position , we propose a multiposition boundary prediction approach . Specifically , for a source text x = { x 1 , ... , x T } , we calculate the probability of predicting sentence boundary Figure 1 : An English - to - German example that translates from a streaming source with and without sentence boundaries . We take the wait - K model ( Ma et al , 2019 ) for illustration , K=3 here . The wait3 model first performs three READ ( wait ) action at the beginning of each sentence ( as shown in blue ) , and then alternating one READ with one WRITE action in the following steps . Given the input source without sentence boundaries ( in the 4 th line ) , the wait3 model ( in the 5 th line ) does n't take the three READ action at the beginning of following sentences . Therefore , the English phrase \" it 's going to \" , which should have been translated as \" wird \" , produced a meaningless translation \" es ist geht dass \" with limited context during wait3 model inference . after x t , t = T , T \u2212 1 , ... , T \u2212 M . Thus the latency of translation can be controlled within L+M words , where L is the length of the sentence . Inspired by the recent pre - training techniques ( Devlin et al , 2019 ; Sun et al , 2019 ) that successfully used in many NLP tasks , we used a pre - trained model for initialization and fine - tune the model on the source side of the sentence . Overall , the contributions are as follows : We propose a novel sentence segmentation method based on pre - trained language representations , which have been successfully used in various NLP tasks . Our method dynamically predicts the boundary at multiple locations , rather than a specific location , achieving high accuracy with low latency .", "entities": [[1, 2, "TaskName", "Translation"], [36, 39, "TaskName", "speech - recognition"], [45, 47, "TaskName", "machine translation"], [205, 207, "TaskName", "boundary detection"], [209, 211, "TaskName", "sentence segmentation"], [243, 245, "TaskName", "sentence segmentation"], [280, 282, "TaskName", "speech recognition"], [328, 329, "TaskName", "chunking"], [391, 392, "MethodName", "SVM"], [465, 467, "TaskName", "sentence segmentation"], [716, 717, "DatasetName", "Inspired"], [780, 782, "TaskName", "sentence segmentation"], [819, 820, "MetricName", "accuracy"]]}
{"text": "Given a streaming input x = { x 1 , ... , x t , ... , x T } , the task of sentence segmentation is to determine whether x t x is the end of a sentence . Thus the task can be considered as a classification problem , that is p ( y t | x , \u03b8 ) , where y t { 0 , 1 } . However , in simultaneous translation scenario , the latency is unacceptable if we take the full source text as contextual information . Thus we should limit the context size and make a decision dynamically . As the input is a word streaming , the sentence boundary detection problem can be transformed as , whether there exists a sentence boundary until the current word x t . Thus we can use the word streaming as a context to make a prediction . We propose a multi - class classification model to predict the probability of a few words before x t as sentence boundaries ( Section 3.1 ) . We use the ERNIE framework to first pre - train a language representation and then fine - tune it to sentence boundary detection ( Section 3.2 ) . We also propose a dynamic voted inference strategy ( Section 3.3 ) .", "entities": [[24, 26, "TaskName", "sentence segmentation"], [60, 61, "HyperparameterName", "\u03b8"], [67, 68, "DatasetName", "0"], [99, 101, "HyperparameterName", "context size"], [117, 119, "TaskName", "boundary detection"], [156, 160, "TaskName", "multi - class classification"], [201, 203, "TaskName", "boundary detection"]]}
{"text": "Our training data is extracted from paragraphs . Question marks , exclamation marks , and semicolons are mapped to periods and all other punctuation symbols are removed from the corpora . Then for every two adjacent sentences in a paragraph , we concatenate them to form a long sequence , x. We record the position of the period as r and then remove the period from the sequence . For x = ( x 1 , x 2 , ... , x N ) with N words , we generate r + M samples for t = 1 , 2 , ... , ( r + M ) , in the form of < ( x 1 , ... , x t ) , y t > , where y t is the label that : y t = \u03c6 , if t < r \u2212 ( t \u2212 r ) , if t [ r , r + M ] ( 1 ) Note that if the length of the second sentence is less than M , we concatenate subsequent sentences until r + M samples are collected . Then we define the loss function as follows : J ( \u03b8 ) = ( x , r ) D log ( r\u22121 t=1 p ( y t = \u03c6 | x \u2264t ; \u03b8 ) + r+M t = r p ( y t = \u2212 ( t \u2212 r ) ) | x \u2264t ; \u03b8 ) ) ( 2 ) where D is the dataset that contains pairs of concatenated sentences x and its corresponding position of the removed periods r. M is a hyperparameter denotes the number of waiting words . Note that our method differs from previous work in the manner of classification . predicts whether a word x t labeled as the end of a sentence or not by a binary classification : p ( y t = 0 | x t+2 t\u22122 ) + p ( y t = 1 | x t+2 t\u22122 ) = 1 ( 3 ) where y t = 0 means x t is not the end of a sentence and y t = 1 means x t is the end . x t+2 t\u22122 denotes 5 words x t\u22122 , x t\u22121 , ... , x t+2 . Some other language - model based work ( Wang et al , 2016 ) calculates probabilities over all words in the vocabulary including the period : w V \u222a \" . \" p ( y t = w | x \u2264t ) = 1 ( 4 ) and decides whether x t is a sentence boundary by comparing the probability of y t = \" . \" and y t = The performance of these methods is limited by incomplete semantics , without considering global boundary detection . In our methods , we leverage more future words and restrict classes globally : x t+1 . 1 2 \u2026 \u22124 \u22123 \u22122 1 2 \u2026 \u22124 \u22123 \u22122 \u22121 1 2 \u2026 \u22124 \u22123 \u22122 \u22121 = 0 | 1 , \u2026 , \u22122 = \u22121 | 1 , \u2026 , \u22121 = \u22122 | 1 , \u2026 , p ( y t = \u03c6 | x \u2264t ) + M m=0 p ( y t = \u2212m | x \u2264t ) = 1 ( 5 ) The restriction is motivated that in a lecture scenario , where a sentence could not be very short that contains only 1 or 2 words . Thus , the probability distribution prohibits that adjacent words to be the end of sentences at the same time .", "entities": [[194, 195, "MetricName", "loss"], [201, 202, "HyperparameterName", "\u03b8"], [224, 225, "HyperparameterName", "\u03b8"], [247, 248, "HyperparameterName", "\u03b8"], [324, 325, "DatasetName", "0"], [351, 352, "DatasetName", "0"], [476, 478, "TaskName", "boundary detection"], [517, 518, "DatasetName", "0"]]}
{"text": "At inference time , we predict sentence boundaries sequentially with a dynamic voting strategy . Each time a new word x t is received , we predict the probability of M + 1 classes as shown in the bottom of Figure 3 , then calculate if the probability of previous M + 1 positions ( x t\u2212M , x t\u2212M +1 , x t ) is larger then a threshold \u03b8 T h . If yes , we add a sentence boundary at the corresponding position . Otherwise , we continue to receive new words . Note that the probability is adopted as the voted probability . While the probability of adding a sentence boundary after x t\u2212M has M + 1 probabilities to calculate the average , the number of probabilities to determine whether it is a sentence boundary at subsequent positions is less than M + 1 . Here we use the voted average of existing probabilities . Specifically , to judge whether x t is a sentence boundary , it needs t \u2212 t + 1 probabilities : 1 t \u2212 t + 1 t\u2212t m=0 p ( y = \u2212m | x 1 , ... , x t+m ) ( 6 ) where t [ t \u2212 M , t ] . If more than one sentence boundary probabilities for x t\u2212M , ... , x t exceeds the threshold \u03b8 T h at the same time , we choose the front - most position as a sentence boundary . This is consistent with our training process , that is , if there is a sample of two or more sentence boundaries , we ignore the following and label the class y t according to the first boundary . This is because we generate samples with each period in the original paragraph as depicted in Section 3.2 . From another point of view , the strategy can also compensate for some incorrect suppression of adjacent boundaries , thereby improving online prediction accuracy .", "entities": [[70, 71, "HyperparameterName", "\u03b8"], [234, 235, "HyperparameterName", "\u03b8"], [335, 336, "MetricName", "accuracy"]]}
{"text": "Experiments are conducted on English - German ( En - De ) simultaneous translation . We evaluate 1 ) the F - score 2 of sentence boundary detection and 2 ) case - sensitive tokenized 4 - gram BLEU ( Papineni et al , 2002 ) as the final translation effect of the segmented sentences . To reduce the impact of the ASR system , we use the transcription without punctuation in both training and evaluation . The datasets used in our experiments are listed in Table 1 . We use two parallel corpus from machine translation task : WMT 14 3 and IWSLT 14 4 . WMT 14 is a text translation corpus including 4.4 M sentences , mainly on news and web sources . And IWSLT 14 is a speech translation corpus of TED lectures with transcribed text and corresponding translation . Here we only use the text part in it , containing 0.19 M sentences in the training set . We train the machine translation model on WMT 14 with the base version of the Transformer model ( Vaswani et al , 2017 ) , achieving a BLEU score of 27.2 on newstest2014 . And our sentence boundary detection model is trained on the source transcription of IWSLT 14 unless otherwise specified ( Section 4.3 ) . To evaluate the system performance , we merge the IWSLT test set of 4 years ( 2010 ) ( 2011 ) ( 2012 ) ( 2013 ) ( 2014 ) to construct a big test set of 7040 sentences . The overall statistics of our dataset is shown in Table 1 . We evaluate our model and two existing methods listed below : dynamic - base is our proposed method that detect sentence boundaries dynamically using a multi - class classification . dynamic - force adds a constraint on dynamicbase . In order to keep in line with ( Wang et al , 2016 ) , we add a constraint that sentence should be force segmented if longer than \u03b8 l . N - gram is the method using an N - gram language model to compare the probability of adding vs. not adding a boundary at x t after receiving x t\u2212N +1 , ... , x t . We implement according to ( Wang et al , 2016 ) . T - LSTM uses a RNN - based classification model with two classes . We implement a unidirectional RNN and perform training according to ( Tilk and Alum\u00e4e , 2015 ) 5 . Our classifier in dynamic - base and dynamicforce is trained under ERNIE base framework . We use the released 6 parameters obtained at pretraining step as initialization . In the fine - tuning stage , we use a learning rate of 2e \u22125 . 5 we only keep the two classes of period and \u03c6 in this work 6 https://github.com/PaddlePaddle/ERNIE", "entities": [[26, 28, "TaskName", "boundary detection"], [38, 39, "MetricName", "BLEU"], [95, 97, "TaskName", "machine translation"], [166, 168, "TaskName", "machine translation"], [178, 179, "MethodName", "Transformer"], [190, 192, "MetricName", "BLEU score"], [200, 202, "TaskName", "boundary detection"], [297, 301, "TaskName", "multi - class classification"], [339, 340, "HyperparameterName", "\u03b8"], [394, 395, "MethodName", "LSTM"], [463, 465, "HyperparameterName", "learning rate"]]}
{"text": "Table 2 reports the results of source sentence segmentation on En - De translation , where the latency is measured by Consecutive Wait ( CW ) ( Gu et al , 2017 ) , the number of words between two translate actions . To eliminate the impact of the different policies in simultaneous translation , we only execute translation at the end of each sentence . Therefore , the CW here denotes the sentence length L plus the number of future words M . We calculate its average and maximum value as \" avgCW \" and \" maxCW \" , respectively . Better performance expect high F - score , BLEU , and low latency ( CW ) . The translation effect obtained by using the groundtruth period as the sentence segmentation is shown in the first line of Oracle . The N - gram method calculate the probability of add ( p add ) and not add ( p not ) period at each position , and decide whether to chunk by comparing whether p add /p not exceeds \u03b8 T h . The N - gram method without threshold tuning ( with \u03b8 T h = e 0.0 ) divides sentences into small pieces , achieving the lowest average latency of 6.64 . However , the Fscore of segmentation is very low because of the incomplete essence of the n - gram feature . Notable , the precision and recall differs much ( precision = 0.33 , recall = 0.78 ) in this setup . Therefore , we need to choose a better threshold by grid search ( Wang et al , 2016 ) . With \u03b8 T h equals to e 2.0 , the F - score of N - gram method increased a little bit ( 0.46 0.48 ) , with a more balanced precision and recall ( precision = 0.51 , recall = 0.48 ) . However , the max latency runs out of control , resulting in a maximum of 161 words in a sentence . We also tried to shorten the latency of the N - gram method by force segmentation ( Wang et al , 2016 ) , but the result was very poor ( precision = 0.33 , recall = 0.40 ) . The T - LSTM method with the hidden size of 256 performs better than N - gram , but the F - score and BLEU is still limited . On the contrary , our dynamic - based approaches with M = 1 achieve the best F - score at 0.74 and the final translation is very close to the result of Oracle . In particular , the precision and recall reached about 0.72 and 0.77 in both dynamic - force and dynamic - base , respectively . Accurate sentence segmentation brings better performance in translation , bringing an improvement of 1.55 over T - LSTM . Moreover , our approach is not inferior in terms of latency . Both average latency and max latency is controlled at a relatively low level . It is interesting to note that , dynamic - force performs better than dynamic - base , in terms of latency and BLEU . This suggests the effectiveness of the force segmentation strategy , that is , select the chunking location with a sentence length limitation will not affect the accuracy of segmentation , and would enhance the translation effect .", "entities": [[7, 9, "TaskName", "sentence segmentation"], [110, 111, "MetricName", "BLEU"], [130, 132, "TaskName", "sentence segmentation"], [180, 181, "HyperparameterName", "\u03b8"], [194, 195, "HyperparameterName", "\u03b8"], [278, 279, "HyperparameterName", "\u03b8"], [385, 386, "MethodName", "LSTM"], [406, 407, "MetricName", "BLEU"], [470, 472, "TaskName", "sentence segmentation"], [486, 487, "MethodName", "LSTM"], [536, 537, "MetricName", "BLEU"], [553, 554, "TaskName", "chunking"], [564, 565, "MetricName", "accuracy"]]}
{"text": "According to Section 3.2 , the order between sentences of original corpora would affect the generation of training samples . In this section , we investigate the effect of various data reordering strategies . A basic method is to use the original sentence order of speech corpora , denote as Basic . However , the samples generated is limited , which makes the model easy to over - fit . To overcome this problem , we adopt two methods to expand data scale : 1 ) Duplicate the original data multiple times or 2 ) Add Synthetic adjacent sentences , through randomly selecting two sentences from the corpora . These two methods greatly expand the total amount of data , but the gain to the model is uncertain . As an alternative , we explore a Sort method , to sort sentences according to alphabetic order . The performance of the four training data organization methods is shown in Figure 4 , all built on IWSLT2014 and conducted under the setup of M = 1 and \u03b8 l = 40 . It is clear that Basic , Duplicate and Synthetic are all involved in the problem of over - fitting . They quickly achieved their best results and then gradually declined . Surprisingly , the Sort approach is prominent in both segmentation accuracy and translation performance . This may be due to the following reasons : 1 ) Sentence classification is not a difficult task , especially when M = 1 for 3 - class classification ( y [ \u03c6 , 0 , \u22121 ] ) , making the task easy to over - fit . 2 ) Compared with Basic , Duplicate is more abundant in the sample combination in batch training , but there is no essential difference between the two methods . 3 ) Synthetic hardly profits our model , because the synthesized data may be very simple due to random selection . 4 ) Sort may simulate difficult cases in real scenes and train them pertinently , bringing it a poor performance at start but not prone to overfit . There are many samples with identical head and tail words in the sorted data , such as : \" and it gives me a lot of hope and ... \" and \" that means there 's literally thousands of new ideas that ... \" . Even human beings find it difficult to determine whether the words before is sentence boundaries of these samples . In Basic , Duplicate and Synthetic methods , such samples are usually submerged in a large quantity of simple samples . However , the data organization mode of Sort greatly strengthens the model 's ability to learn these difficult samples . There is no need to worry that the Sort method can not cover simple samples . Because we sort by rows in source file , and some of the rows contain multiple sentences ( an average of 1.01 sentences per row ) , which are in real speech order . We argue that these sentences are sufficient to model the classification of simple samples , based on the rapid overfit performance of the other three methods .", "entities": [[176, 177, "HyperparameterName", "\u03b8"], [222, 223, "MetricName", "accuracy"], [238, 240, "TaskName", "Sentence classification"], [261, 262, "DatasetName", "0"]]}
{"text": "Next , we turn to the question that how does the domain of training corpus affects results . With the test set unchanged , we compare the sentence boundary detections model trained on out - of - domain corpora WMT 14 and in - domain corpora IWSLT 14 , respectively . As mentioned before , WMT 14 is a larger text translation corpus mainly on news and web sources . But the test set comes from IWSLT , which contains transcriptions of TED lectures of various directions . Intuitively , larger dataset provides more diverse samples , but due to domain changes , it does not necessarily lead to improvements in accuracy . The performance of various models trained on WMT14 is shown in Table 3 . Dynamic - force also achieves the best translation performance with a relatively small latency on average and limited the max latency within 40 words . However , it underperforms the same model trained on IWSLT2014 ( as shown in Table 2 ) , demonstrating its sensitivity to the training domain . On the contrary , N - gram and T - LSTM is hardly affected . For N - gram , one possible reason is the before mentioned weakness of the N - gram : segmentation depends on only N previous words , which is more steady compared to the whole sentence , thus eliminating the perturbation of whole sentence brought by the domain variation . For T - LSTM , it even improves a little compared with its in - domain performance . This may be due to the lack of training samples . 0.19 M sentences of IWSLT2014 is insufficient to fit the parameters of T - LSTM . Thus the model would benefit from increasing the corpus size . However , our method needs less data in training because our model has been pre - trained . Based on a powerful representation , we need only a small amount of training data in fine - tuning , which is best aligned with the test set in the domain .", "entities": [[111, 112, "MetricName", "accuracy"], [120, 121, "DatasetName", "WMT14"], [188, 189, "MethodName", "LSTM"], [246, 247, "MethodName", "LSTM"], [286, 287, "MethodName", "LSTM"]]}
{"text": "Next , we discuss the effect of changing \u03b8 . The performance of dynamic - force with varying \u03b8 l is shown in Table 4 . Smaller \u03b8 l brings shorter latency , as well as worse performance . The effect is extremely poor with \u03b8 l = 10 . There are two possible reasons : 1 ) Constraint sentence length less than \u03b8 l is too harsh under small \u03b8 l , 2 ) The discrepancy between the unrestricted training and length - restricted testing causes the poor effect . We first focus on the second possible reason . While the difference between dynamic - base and dynamic - force is only in prediction , we want to know whether we can achieve better results by controlling the length of training samples . Accordingly , we only use the samples shorter than a fixed value : \u03b8 l in training phrase . At inference time , we use both dynamic - force with the same sentence length constraint \u03b8 l and dynamic - base to predict sentence boundaries . As elaborated in Figure 5 , For each pair of curves with a same \u03b8 l , dynamic - force and dynamic - base present similar performance . This demonstrates the main reason for the poor performance with small \u03b8 l is not the training - testing discrepancy but lies in the first reason that the force constraint is too harsh . Moreover , it is interesting to find that the performance of \u03b8 l = 80 is similar with \u03b8 l = 40 at the beginning but falls a little during training . This probably because the setup with \u03b8 l = 40 can filter some inaccurate cases , as the average number of words in IWSLT2014 training set is 20.26 .", "entities": [[8, 9, "HyperparameterName", "\u03b8"], [18, 19, "HyperparameterName", "\u03b8"], [27, 28, "HyperparameterName", "\u03b8"], [45, 46, "HyperparameterName", "\u03b8"], [63, 64, "HyperparameterName", "\u03b8"], [70, 71, "HyperparameterName", "\u03b8"], [147, 148, "HyperparameterName", "\u03b8"], [169, 170, "HyperparameterName", "\u03b8"], [194, 195, "HyperparameterName", "\u03b8"], [219, 220, "HyperparameterName", "\u03b8"], [253, 254, "HyperparameterName", "\u03b8"], [260, 261, "HyperparameterName", "\u03b8"], [280, 281, "HyperparameterName", "\u03b8"]]}
{"text": "We investigate whether can we achieve better performance with more or less future words . We experiment with M from 0 to 5 . The result is shown in Table 5 . Reducing M to zero means that do not refer to any future words in prediction . This degrades performance a lot , proving the effectiveness of adding future words in prediction . Increase M from 1 to 2 also promote the performance in both sentence boundary detection f - score and the system BLEU . However , as more future words added ( increase M to 3 and 4 ) , the improvement becomes less obvious .", "entities": [[20, 21, "DatasetName", "0"], [77, 79, "TaskName", "boundary detection"], [85, 86, "MetricName", "BLEU"]]}
{"text": "Some work takes a fixed size of words as input . Focus on utilizing a limited size of the streaming input , they predict the probability of putting a boundary at a specific position x t by a N - gram lan - guage model ( Wang et al , 2016 ) or a classification model Yarmohammadi et al , 2013 ) . The language - model based method make decision depends on N words ( x t\u2212N +2 , ... , x t+1 ) and compares its probability with ( x t\u2212N +2 , ... , x t , \" . \" ) . The classification model takes features of N words around x t and classifies to two classes denoting x t is a sentence boundary or not . The main deficiency of this method is that the dependencies outside the input window are lost , resulting in low accuracy .", "entities": [[151, 152, "MetricName", "accuracy"]]}
{"text": "Some other work focuses on restoring punctuation and capitalization using the whole sentence . To improve the sentence boundary classification accuracy , some work upgrade the N - gram input to variable - length input by using recurrent neural network ( RNN ) ( Tilk and Alum\u00e4e , 2015 ; Salloum et al , 2017 ) . Some other work takes punctuation restoration as a sequence labeling problem and investigates using Conditional Random Fields ( CRFs ) ( Lu and Ng , 2010 ; Wang et al , 2012 ; Ueffing et al , 2013 ) . Peitz et al ( 2011 ) and Cho et al ( 2012 ) treats this problem as a machine translation task , training to translate non - punctuated transcription into punctuated text . However , all these methods utilize the whole sentence information , which is not fit for the simultaneous translation scenario . Moreover , the translation model based methods require multiple steps of decoding , making it unsuitable for online prediction .", "entities": [[20, 21, "MetricName", "accuracy"], [115, 117, "TaskName", "machine translation"]]}
{"text": "In this paper , we propose an online sentence boundary detection approach . With the input of streaming words , our model predicts the probability of multiple positions rather than a certain position . By adding this adjacent position constraint and using dynamic prediction , our method achieves higher accuracy with lower latency . We also incorporate the pre - trained technique , ERNIE to implement our classification model . The empirical results on IWSLT2014 demonstrate that our approach achieves significant improvements of 0.19 F - score on sentence segmentation and 1.55 BLEU points compared with the language - model based methods .", "entities": [[9, 11, "TaskName", "boundary detection"], [49, 50, "MetricName", "accuracy"], [88, 90, "TaskName", "sentence segmentation"], [92, 93, "MetricName", "BLEU"]]}
{"text": "Computational approaches to noun ellipsis resolution has been sparse , with only a naive rulebased approach that uses syntactic feature constraints for marking noun ellipsis licensors and selecting their antecedents . In this paper , we further the ellipsis research by exploring several statistical and neural models for both the subtasks involved in the ellipsis resolution process and addressing the representation and contribution of manual features proposed in previous research . Using the best performing models , we build an end - to - end supervised Machine Learning ( ML ) framework for this task that improves the existing F1 score by 16.55 % for the detection and 14.97 % for the resolution subtask . Our experiments demonstrate robust scores through pretrained BERT ( Bidirectional Encoder Representations from Transformers ) embeddings for word representation , and more so the importance of manual features - once again highlighting the syntactic and semantic characteristics of the ellipsis phenomenon . For the classification decision , we notice that a simple Multilayar Perceptron ( MLP ) works well for the detection of ellipsis ; however , Recurrent Neural Networks ( RNN ) are a better choice for the much harder resolution step .", "entities": [[99, 101, "MetricName", "F1 score"], [122, 123, "MethodName", "BERT"], [170, 171, "DatasetName", "MLP"]]}
{"text": "We detection task , we take the annotated 946 positive samples ( exophoric ) and randomly choose 946 negative samples . Similarly , for the resolution task , we take 438 positive samples ( endophoric ) and 438 randomly chosen negative samples . We perform a standard 70 - 10 - 20 split to obtain the train , development and test set respectively , and follow the 5 - fold cross validation procedure to capture both classes properly in each case . For MLP , we take a simple , two - layer feedforward network ( FFNN ) or two layers of multiple computational units interconnected in a feed - forward way without loops . We have a single hidden layer with 768 neurons and a sigmoid function . A unidirectional weight connection exists between the two successive layers . The classification decision is made by turning the input vector representations of a word with its context into a score . The network has a softmax output layer . For bi - LSTM , we have embedding layer , time - distributed translate layer , Bi - LSTM ( RNN ) layer , batch normalization layer , dropout layer and prediction layer . The activation used is Softmax . The loss function is calculated with cross entropy . We train in batch sizes of 16 and early stopping with max epochs of 100 . In early stopping the patience is kept to be 10 and the optimizer used is Adam . We use default values for the learning rate . We use Keras ( Chollet et al , 2015 ) for coding the models .", "entities": [[83, 84, "DatasetName", "MLP"], [93, 95, "MethodName", "feedforward network"], [165, 166, "MethodName", "softmax"], [172, 173, "MethodName", "LSTM"], [187, 188, "MethodName", "LSTM"], [193, 195, "MethodName", "batch normalization"], [207, 208, "MethodName", "Softmax"], [210, 211, "MetricName", "loss"], [226, 228, "MethodName", "early stopping"], [235, 237, "MethodName", "early stopping"], [246, 247, "HyperparameterName", "optimizer"], [249, 250, "MethodName", "Adam"], [257, 259, "HyperparameterName", "learning rate"]]}
{"text": "We evaluate the performance of our models in terms of F1 - score , computed by taking an average F1 - scores obtained from the 5 - folds results . We experiment with sixteen models each for the noun ellipsis detection and resolution . The results on the testset for Precision , Recall and F1 - Score values are presented in Table .2 . As expected , the neural models perform significantly better than the statistical ones for both the subtasks . Our experiments show that for the detection task , BERT embeddings with a simple MLP gives best scores . This is expected because , BERT currently provides the most powerful contextual word representations , using 12 separate attention mechanism for each layer , where , at each layer , each token can focus on 12 distinct aspects of other tokens . Since Transformers ( Vaswani et al , 2017 ) use many distinct attention heads ( 12 * 12=144 for the base BERT model ) , each head can focus on a different kind of constituent combinations , making BERT broadly attending over the whole sentence . In our task , the ( Khullar et al , 2019 ) and the neural model presented in this paper . input and output , but they are not innately designed to capture temporal relationships within a sentence . Hence , although they perform well for a task like detection that needs local information , they are outperformed by bi - LSTMs on the resolution task that requires capturing a deeper relationship between the antecedent and the elided noun . We also note that manual feature addition boosts results greatly for all models , highlighting that ellipsis is a syntactically constrained phenomenon . We finally integrate the best models for each subtask into an end - to - end pipeline , as in Figure 2 . Now , instead of the gold vectors ( from the annotations ) , the resolution model is fed the ouput licensor vector from the detection model . This obviously results into error propagation into the second model , and lowers the precision value to 82.52 % , recall to 78.66 % and consequently , the F1 - score to 80.55 % of the final system . The error in the final system comes from failing to detect actual licensors , wrongly identifying non - licensor words and correct licensor detection but failed antecedent resolution . We run our final system on the curated dataset prepared by ( Khullar et al , 2019 ) and compare the results with their rule - based approach . As expected , this model improves the F1 - score by 16.55 % for noun ellipsis detection and 14.97 % for noun ellipsis res - olution . See Table 3 . The even higher accuracy on the curated dataset can be explained by the nature of the sentences in this dataset which are from textbooks , and , hence , free of grammatical errors , etc . - resulting into improved parser performance in the pre - processing step . Although , the presented models achieve high scores on both the tasks separately and in the pipeline process , the results can be further improved with hyper - parameter tuning and additional regularization .", "entities": [[10, 13, "MetricName", "F1 - score"], [18, 20, "MetricName", "average F1"], [50, 51, "MetricName", "Precision"], [52, 53, "MetricName", "Recall"], [54, 57, "MetricName", "F1 - Score"], [91, 92, "MethodName", "BERT"], [96, 97, "DatasetName", "MLP"], [106, 107, "MethodName", "BERT"], [164, 165, "MethodName", "BERT"], [181, 182, "MethodName", "BERT"], [370, 373, "MetricName", "F1 - score"], [446, 449, "MetricName", "F1 - score"], [473, 474, "MetricName", "accuracy"]]}
{"text": "In this section , we survey related attention mechanisms ( Bahdanau et al , 2014 ) and review the most relevant studies on information bottleneck ( IB ) ( Tishby et al , 1999 ) . Attention has been proved can help explain the internals of neural models ( Li et al , 2016 ; Wiegreffe and Pinter , 2019 ) though it is limited ( Jain and Wallace , 2019 ) . Many researchers try to improve the interpretability of the attention mechanisms . leveraged small amounts of word - level annotations to regularize attention . Kim et al ( 2017 ) introduced a structured attention mechanism to learn attention variants from explicit probabilistic semantics . Barrett et al ( 2018 ) ; Bao et al ( 2018 ) aligned explanations with human - provided rationales to improve the explanation of attention . Unlike these methods that require prior attributions or human explanations , the VAT method enforces the attention to learn the vital information while filter the noise via IB . A series of studies motivate us to utilize IB to improve the explanations of attention mechanisms . Li and Eisner ( 2019 ) compressed the pre - trained embedding ( e.g. , BERT , ELMO ) , remaining only the information that helps a discriminative parser through variational IB . Zhmoginov et al ( 2019 ) utilized the IB approach to discover the salient region . Some works ( Jiang et al , 2020 ; Chen et al , 2018 ; Guan et al , 2019 ; Schulz et al , 2020 ; Bang et al , 2019 ) proposed to identify vital features or attributions via IB . Moreover , Chen and Ji ( 2020 ) designed a variational mask strategy to delete the useless words in the text . As far as we are aware , we are the first ones to leverage IB into attention mechanisms to train more interpretable attention with better accuracy .", "entities": [[205, 206, "MethodName", "BERT"], [207, 208, "MethodName", "ELMO"], [329, 330, "MetricName", "accuracy"]]}
{"text": "Our framework is composed of a learner and a compressor , which performs fine - tuning and compressing iteratively ( Figure 1 ) . The learner aims to learn a task - specific contextual word representation by fine - tuning . The compressor enforces the model to learn task - relevant information while reduce irrelevant information via IB . We iteratively perform the learner and compressor ( fine - tuning and compressing ) to improve each other . Learner . We adopt a basic attention - based neural network model as a learner to learn representations of the words based on the good attention weights learned by the compressor . The model is optimized by cross - entropy loss to learn the label - relevant information . In this phase , we fix the attention 's parameters so that the model will focus on updating the encoder to learn word representations . Compressor . To restrict the attention to capture the vital information while reduce the noise , we integrate IB into attention mechanisms to compress the text attentive representation . We fix the encoder 's parameters so that the model will focus on learning the attention weights based on current representations obtained from the learner .", "entities": [[118, 119, "MetricName", "loss"]]}
{"text": "In this section , we describe our learner , which is an attention - based neural network model . First , given a text T \" tw 1 , w 2 , ... , w | T | u , where | T | is the length of text T , we feed it into an encoder with a First , we obtain the input text 's word representations X via an encoder trained by the learner . Then , we calculate Z by compressing the text representation R that is the weighted sum of X based on the attention \u03b1 , while remaining the maximum information to judge Y by inputting Z into a MLP classifier for predicting . word embedding layer . We adopt LSTM and BERT models as our encoder , and other models can also be applied to our framework . We obtain the contextaware word representations x \" rx 1 , x 2 , ... , x | T | s , where x i is the hidden vector of the word w i . x \" encoderpT , \u03b8 encoder q , ( ) 1 where \u03b8 encoder is the parameters of the encoder . Based on the contextual word representations , attention mechanism ( Bahdanau et al , 2014 ) 2 is utilized to capture the important parts in the text and obtain the text representation R , which is calculated as , R \" n \u00ff i\"1 \u03b1 i x i \u03b1 i \" softmaxpv J a tanhpW a x i qq ( 2 ) where \u03b8 attention \" tv a , W a u is the trainable parameters of the attention , which is not updated in this step to learn the word representation x based the good attention learned by the compressor . \u03b1 \" r\u03b1 1 , \u03b1 2 , ... , \u03b1 | T | s is the attention weights . Finally , we input the text representation R into a multi - layer perceptron ( MLP ) to predict the probability . The cross - entropy loss is used to optimize the model .", "entities": [[100, 101, "HyperparameterName", "\u03b1"], [115, 116, "DatasetName", "MLP"], [126, 127, "MethodName", "LSTM"], [128, 129, "MethodName", "BERT"], [184, 185, "HyperparameterName", "\u03b8"], [192, 193, "HyperparameterName", "\u03b8"], [245, 246, "HyperparameterName", "\u03b1"], [249, 250, "HyperparameterName", "\u03b1"], [264, 265, "HyperparameterName", "\u03b8"], [303, 304, "HyperparameterName", "\u03b1"], [308, 309, "HyperparameterName", "\u03b1"], [313, 314, "HyperparameterName", "\u03b1"], [338, 339, "DatasetName", "MLP"], [349, 350, "MetricName", "loss"]]}
{"text": "The learner optimizes the sentence representations by minimizing the cross - entropy loss , which does not restrict the model to ignore the useless information . Thus , we compress sentence representations R into a latent representation Z that retains most useful information to infer the label Y . We propose to accomplish this by integrating VIB into the attention mechanism ( Figure 2 ) . To ensure Z contains maximum ability to predict Y ( IpZ ; Y q ) while has the least redundant information form R ( \u00b4 IpZ ; Rq ) , we use the standard IB theory ( Tishby et al , 1999 ) where KLr\u00a8 } \u00a8s represents Kullback - Leibler divergence . Specifically , we regard ppyq as constant and then minimize E p \u03b8 py , zq rlog q \u03c6 py | zqs . Since we must first sample r to sample y , z from p \u03b8 pr , y , zq , the lower bound of IpZ ; Y q is computed as , IpZ ; Y q \u011b E ppr , yq rE p \u03b8 pz | rq rlog q \u03c6 py | zqss ( 5 ) We calculate the upper bound of IpZ ; Rq by replacing p \u03b8 pzq with a variational distribution r \u03c8 pzq , 3 We give the main steps as follows and the detailed derivation is provided in supplementary materials . 4 Y \u00d1 R \u00d1 Z : Y and Z are independent given R. Then , we obtain the lower bound L of IB by substituting Equation 5 and 7 into Equation 3 : L \" E ppr , yq rE p \u03b8 pz | rq rlog q \u03c6 py | zq\u015b \u03b2\u00a8KLrp \u03b8 pz | rq } r \u03c8 pzqss ( 8 ) The first component in L is to keep the most useful information in p \u03b8 pz | rq for inferring y , while the second one is to regularize p \u03b8 pz | rq with a predefined prior distribution r \u03c8 pzq ( e.g. , Gaussian distribution ) . To compute p \u03b8 pz | rq , we adopt the reparametrization trick for multivariate Gaussians ( Rezende et al , 2014 ) , which obtains the gradient of parameters that derive z from a random noise . z \" u`\u03c3 d , \" N p0 , Iq ( 9 ) where d means element - wise multiplication . u and \u03c3 denote the mean and covariance defined by two functions of R , where R \" \u03b1\u00a8x that is learned based on attention . In particular , two MLP are used to predict u and \u03c3 . Finally , we input the z into a MLP to predict q \u03c6 py | zq and optimize the attention 's parameter via Equation 8 .", "entities": [[12, 13, "MetricName", "loss"], [131, 132, "HyperparameterName", "\u03b8"], [155, 156, "HyperparameterName", "\u03b8"], [185, 186, "HyperparameterName", "\u03b8"], [210, 211, "HyperparameterName", "\u03b8"], [280, 281, "HyperparameterName", "\u03b8"], [291, 292, "HyperparameterName", "\u03b8"], [316, 317, "HyperparameterName", "\u03b8"], [332, 333, "HyperparameterName", "\u03b8"], [354, 355, "HyperparameterName", "\u03b8"], [440, 441, "DatasetName", "MLP"], [457, 458, "DatasetName", "MLP"]]}
{"text": "We report the accuracy of our VAT and baselines based on LSTM and BERT ( Table 2 ) . From these results , we find the following observations : 1 ) our models ( LSTM / BERT - VAT ) outperform all the corresponding baselines over all the eight datasets , which denotes the effectiveness of our VAT on both LSTM and BERT - based models ; 2 ) compared with attention - based models ( LSTM / BERT - ATT ) , our models obtain better results . It indicates reducing the irrelevant information in input via VAT can improve the performance of the models . Furthermore , we visualize the sentence representations obtained from LSTM / BERT - ATT and - VAT models ( Figure 3 ) . We randomly select 1000 samples from the test set for each dataset . We can find that our VAT model can reduce the distance of the samples in a class and add the distance of the samples in different classes . For example , it is hard to split the positive samples from the negative ones based on the representations obtained from LSTM - ATT for the IMDB dataset , while the divider line based on our VAT is clear . These ob - servations show our VAT model can learn a better task - specific representation by enforcing the model to reduce the task - irrelevant information .", "entities": [[3, 4, "MetricName", "accuracy"], [11, 12, "MethodName", "LSTM"], [13, 14, "MethodName", "BERT"], [34, 35, "MethodName", "LSTM"], [36, 37, "MethodName", "BERT"], [60, 61, "MethodName", "LSTM"], [62, 63, "MethodName", "BERT"], [76, 77, "MethodName", "LSTM"], [78, 79, "MethodName", "BERT"], [116, 117, "MethodName", "LSTM"], [118, 119, "MethodName", "BERT"], [192, 193, "MethodName", "LSTM"], [197, 198, "DatasetName", "IMDB"]]}
{"text": "In this section , we evaluate our VAT model using two metrics , AOPC and post - hoc accuracy , which are widely used for explanations ( Chen and Ji , 2020 ) . Note that well - trained LSTM / BERT - base is used for evaluating the performance of classification . AOPC . To evaluate the faithfulness of explanations to our models , we adopt the area over the perturbation curve ( AOPC ) ( Nguyen , 2018 ; Samek et al , 2016 ) metric . It calculates the average change of accuracy over test data by deleting top K words via attentive weights . The larger the value of AOPC , the better the explanations of the models . Table 3 displays the results with K \" 5 . We compare our models with random and basic attention - based models . From the results , we observe that : 1 ) basic attention - based models ( LSTM / BERT - ATT ) can find the important words in the sentence to some extent . Comparing with random ( Random ) , LSTM / BERT - ATT obtains significant improvement ; 2 ) Our models ( LSTM / BERT - VAT ) outperform the standard attention - based models . It indicates that integrating VIB into the attention mechanism can help improve the interpretability of the models by filtering the useless information ; 3 ) BERT model is sensitive to the context ; deleting the words will destroy the semantic information of the sentence and significantly affect the model 's performance . We also explore the influence of top - K ( Figure 4 ) . Intuitively , the more words we delete , the larger accuracy the models reduce . Our models reduce more performance than random and attention - based models . For the IMDB dataset , when deleting top 20 words ( average length is 268 ) , the accuracy reduces about 19 points for our LSTM - VAT model while it is about 2 points for the random model . Post - hoc Accuracy . We also adopt the post - hoc accuracy ( Chen et al , 2018 ) to evaluate the influence of task - specific essential words on the performance of LSTM - based and BERT - based models . For each test sample , we select the top K words based on their attentive weights as input to make a prediction and compare it with the ground truth . Table 4 presents the performance with K \" 5 . First , it is interesting to find that the post - hoc accuracy with five most important words on Sbuj dataset ( 89.10 ) is even better than the original sentence ( 89.00 ) . Additionally , we obtain comparable results with only five words for SST - 1 , SST - 2 , and Twitter datasets . These show that our model can reduce the noise information since most of the words are useless for predictions in some cases . Second , for BERT - based models , the context words are also important for classification even though they may not be task - specific . Similarly , we investigate the influence of top - K for post - hoc ( Figure 5 ) . The LSTM - base model with top - 10 words selected by our LSTM - VAT model can achieve comparable results with the original samples in most cases . Additionally , for the IMDB dataset , the accuracy of LSTM - base with one word selected by our VAT model is even better than the one with 20 words selected randomly .", "entities": [[18, 19, "MetricName", "accuracy"], [39, 40, "MethodName", "LSTM"], [41, 42, "MethodName", "BERT"], [95, 96, "MetricName", "accuracy"], [162, 163, "MethodName", "LSTM"], [164, 165, "MethodName", "BERT"], [187, 188, "MethodName", "LSTM"], [189, 190, "MethodName", "BERT"], [201, 202, "MethodName", "LSTM"], [203, 204, "MethodName", "BERT"], [240, 241, "MethodName", "BERT"], [291, 292, "MetricName", "accuracy"], [311, 312, "DatasetName", "IMDB"], [327, 328, "MetricName", "accuracy"], [334, 335, "MethodName", "LSTM"], [352, 353, "MetricName", "Accuracy"], [361, 362, "MetricName", "accuracy"], [383, 384, "MethodName", "LSTM"], [387, 388, "MethodName", "BERT"], [444, 445, "MetricName", "accuracy"], [478, 479, "DatasetName", "SST"], [482, 483, "DatasetName", "SST"], [516, 517, "MethodName", "BERT"], [559, 560, "MethodName", "LSTM"], [571, 572, "MethodName", "LSTM"], [591, 592, "DatasetName", "IMDB"], [595, 596, "MetricName", "accuracy"], [597, 598, "MethodName", "LSTM"]]}
{"text": "We perform semi - supervised word - level sentiment detection in Twitter ( Rosenthal et al , 2015 ( Rosenthal et al , , 2014 to evaluate the interpretability of our VAT . This task requires to detect the sentiment words in a tweet via the sentiment polarity of the whole tweet . In the following example from the dataset , positive words ( \" good \" and \" fantastic \" ) are marked with a bold font and the overall polarity of the tweet is positive : Good morning becky ! Thursday is going to be fantastic ! We use the SemEval 2013 Twitter dataset , which contains word - level sentiment annotation . We remove the samples with the neutral sentiment . We report word - level precision , recall , and F - measure for evaluating the models ( Table 5 ) , the same as . Note that we select the top - K ( we set it as 1 and 5 here ) words according to the attention weights as the sentiment words . We compare our VAT model with random and attention - based models . The results show attentionbased models can capture the important words in the text , to a certain extent . Since our VAT can reduce irrelevant information , it performs better than the standard attention model . Also , LSTM - based models outperform BERT - based models for this task in most cases . It is because that BERT learns much semantic information from the text , and context information plays a vital role in prediction .", "entities": [[102, 104, "DatasetName", "SemEval 2013"], [134, 137, "MetricName", "F - measure"], [230, 231, "MethodName", "LSTM"], [235, 236, "MethodName", "BERT"], [250, 251, "MethodName", "BERT"]]}
{"text": "We propose to train the learner and compressor iteratively so that the learner optimizes the word representations based on the good attention , and the compressor optimizes the attention based on the good word representations . To have a deep look at how it works , we first provide our VAT model 's accuracy with different iterations ( Table 6 ) . From the results , we can find that the model 's performance will improve at first , then it will converge . Positive Negative P@1 R@1 F1@1 P@5 R@5 F1@5 P@1 R@1 F1@1 P@5 R@ Also , we draw change of the sentence representation with different iterations ( Figure 6 ) . Similarly , we observe that fine - tuning and compressing iteratively can improve the sentence representations . The samples with the same class are close , and the samples with different classes have a large distance .", "entities": [[53, 54, "MetricName", "accuracy"], [86, 87, "MetricName", "P@1"], [87, 88, "MetricName", "R@1"], [90, 91, "MetricName", "R@5"], [92, 93, "MetricName", "P@1"], [93, 94, "MetricName", "R@1"]]}
{"text": "The predominant neural architectures in machine translation are recurrent encoder - decoder networks ( Graves , 2012 ; Cho et al , 2014 ) . The encoder is a recurrent neural network ( RNN ) based on gated recurrent units ( Hochreiter and Schmidhuber , 1997 ; Cho et al , 2014 ) to map the input sequence into a vector representation . Often a bi - directional RNN ( Schuster and Paliwal , 1997 ) is used , which consists of two RNNs that process the input in opposite directions , and the final states of both RNNs are concatenated as the input encoding . The decoder consists of a second RNN , which takes the input encoding , and sequentially samples the output sequence one to - ken at a time whilst updating its state . While best known for their use in visual recognition models , ( Oord et al , 2016a ; Salimans et al , 2017 ; Reed et al , 2017 ; Oord et al , 2016c ) . Recent works also introduced convolutional networks to natural language processing . The first convolutional apporaches to encoding variablelength sequences consist of stacking word vectors , applying 1D convolutions then aggregating with a max - pooling operator over time ( Collobert and Weston , 2008 ; Kalchbrenner et al , 2014 ; Kim , 2014 ) . For sequence generation , the works of Ranzato et al ( 2016 ) ; Bahdanau et al ( 2017 ) ; Gehring et al ( 2017a ) mix a convolutional encoder with an RNN decoder . The first entirely convolutional encoder - decoder models where introduced by , but they did not improve over state - of - the - art recurrent architectures . Gehring et al ( 2017b ) outperformed deep LSTMs for machine translation 1D CNNs with gated linear units ( Meng et al , 2015 ; Oord et al , 2016c ; Dauphin et al , 2017 ) in both the encoder and decoder modules . Such CNN - based models differ from their RNN - based counterparts in that temporal connections are placed between layers of the network , rather than within layers . See Figure 2 for a conceptual illustration . This apparently small difference in connectivity has two important consequences . First , it makes the field of view grow linearly across layers in the convolutional network , while it is unbounded within layers in the recurrent network . Second , while the activations in the RNN can only be computed in a sequential manner , they can be computed in parallel across the temporal dimension in the convolutional case . In all the recurrent or convolutional models mentioned above , each of the input and output sequences are processed separately as a onedimensional sequence by the encoder and decoder respectively . Attention mechanisms ( Bahdanau et al , 2015 ; Luong et al , 2015 ; Xu et al , 2015 ) were introduced as an interface between the encoder and decoder modules . During encoding , the attention model finds which hidden states from the source code are the most salient for generating the next target token . This is achieved by evaluating a \" context vector \" which , in its most basic form , is a weighted average of the source features . The weights of the summation are predicted by a small neural network that scores these features condi - Vaswani et al ( 2017 ) propose an architecture relying entirely on attention . Positional input coding together with self - attention ( Parikh et al , 2016 ; replaces recurrent and convolutional layers . Huang et al ( 2018 ) use an attentionlike gating mechanism to alleviate an assumption of monotonic alignment in the phrase - based translation model of . treat the sentence alignment as a latent variable which they infer using a variational inference network during training to optimize a variational lower - bound on the log - likelihood . Beyond uni - dimensional encoding / decoding . proposed a 2D LSTM model similar to our 2D CNN for machine translation . Like our model , a 2D grid is defined across the input and output sequences , as in Figure 1 . In their model , each cell takes input from its left and bottom neighbor . In a second LSTM stream , each cell takes input from its left and top neighbor , as well as from the corresponding cell in the first stream . They also observed that such a structure implements an implicit form of attention , by producing an input encoding that depends on the output sequence produced so far . Wu et al ( 2017 ) used a CNN over the 2D source - target representation as in our work , but only as a discriminator in an adversarial training setup . They do not use masked convolutions , since their CNN is used to predict if a given sourcetarget pair is a human or machine translation . A standard encoder - decoder model with attention is used to generate translations .", "entities": [[5, 7, "TaskName", "machine translation"], [305, 307, "TaskName", "machine translation"], [657, 659, "MethodName", "variational inference"], [671, 674, "MetricName", "log - likelihood"], [686, 687, "MethodName", "LSTM"], [694, 696, "TaskName", "machine translation"], [736, 737, "MethodName", "LSTM"], [846, 848, "TaskName", "machine translation"]]}
{"text": "We use the DenseNet convolutional architecture , which is the state of the art for image classification tasks . Layers are densely connected , meaning that each layer takes as input the activations of all the preceding layers , rather than just the last one , to produce its g feature maps . The parameter g is called the \" growth rate \" as it is the number of appended channels to the network 's output at each layer . The long - distance connections in the network improve gradient flow to early network layers during training , which is beneficial for deeper networks . Each layer first batch - normalizes ( Ioffe and Szegedy , 2015 ) its input and apply a ReLU ( Nair and Hinton , 2010 ) non - linearity . To reduce the computation cost , each layer first computes 4 g channels using a 1\u00d71 convolution from the f 0 + ( l \u2212 1 ) g input channels to layer l { 1 , . . . , L } . This is followed by a second batch - normalization and ReLU non - linearity . The second convolution has ( k \u00d7 k 2 ) kernels , i.e. masked as illustrated in Figure 1 , and generates the g output features maps to which we apply dropout ( Srivastava et al , 2014 ) . The architecture of the densely connected network is illustrated in Figure 3 . We optionally use gated linear units ( Dauphin et al , 2017 ) Target sequence prediction . Starting from the initial f 0 feature maps , each layer l { 1 , . . . , L } of our DenseNet produces a tensor H l of size | t | \u00d7 | s | \u00d7 f l , where f l is the number of output channels of that layer . To compute a distribution over the tokens in the output vocabulary , we need to collapse the second dimension of the tensor , which is given by the variable length of the input sequence , to retrieve a unique encoding for each target position . The simplest aggregation approach is to apply max - pooling over the input sequence to obtain a tensor H pool R | t | \u00d7f L , i.e. H pool i d = max j { 1 , ... , | s | } H L ijd . ( 2 ) Alternatively , we can use average - pooling over the input sequence : H pool i d = 1 | s | j { 1 , ... , | s | } H L ijd . ( 3 ) The scaling with the inverse square - root of the source length acts as a variance stabilization term , which we find to be more effective in practice than a simple averaging . The pooled features are then transformed to predictions over the output vocabulary V , by linearly mapping them with a matrix E R | V | \u00d7f L to the vocabulary dimension | V | , and then applying a soft - max . Thus the probability distribution over V for the i - th output token is obtained as p i = SoftMax ( EH pool i ) . ( 4 ) Alternatively , we can use E to project to dimension d t , and then multiply with the target word embedding matrix used to define the input tensor . This reduces the number of parameters and generally improves the performance . Implicit sentence alignment . For a given output token position i , the max - pooling operator of Eq . ( 2 ) partitions the f L channels by assigning them across the source tokens j. Let us define B ij = { d { 1 , . . . , f L } | j = arg max ( H L ijd ) } as the channels assigned to source token j for output token i. The energy that enters into the softmax to predict token w V for the i - th output position is given by e iw = d { 1 , ... , f L } E wd H pool i d ( 5 ) = j { 1 , ... , | s | } d B ij E wd H L ijd . ( 6 ) The total contribution of the j - th input token is thus given by \u03b1 ij = d B ij E wd H L ijd , ( 7 ) where we dropped the dependence on w for simplicity . As we will show experimentally in the next section , visualizing the values \u03b1 ij for the groundtruth output tokens , we can recover an implicit sentence alignment used by the model .", "entities": [[3, 4, "MethodName", "DenseNet"], [15, 17, "TaskName", "image classification"], [123, 124, "MethodName", "ReLU"], [151, 152, "MethodName", "convolution"], [155, 156, "DatasetName", "0"], [188, 189, "MethodName", "ReLU"], [195, 196, "MethodName", "convolution"], [268, 269, "DatasetName", "0"], [286, 287, "MethodName", "DenseNet"], [549, 550, "MethodName", "SoftMax"], [591, 594, "HyperparameterName", "number of parameters"], [683, 684, "MethodName", "softmax"], [757, 758, "HyperparameterName", "\u03b1"], [795, 796, "HyperparameterName", "\u03b1"]]}
{"text": "Data and pre - processing . We experiment with the IWSLT 2014 bilingual dataset ( Cettolo et al , 2014 ) , which contains transcripts of TED talks aligned at sentence level , and translate between German ( De ) and English ( En ) in both directions . Following the setup of ( Edunov et al , 2018 ) , sentences longer than 175 tokens and pairs with length ratio exceeding 1.5 were removed from the original data . There are 160 + 7 K training sentence pairs , 7 K of which are separated and used for validation / development . We report results on a test set of 6 , 578 pairs obtained by concatenating dev2010 and tst2010 - 2013 . We tokenized and lowercased all data using the standard scripts from the Moses toolkit ( Koehn et al , 2007 ) . For open - vocabulary translation , we segment sequences using joint byte pair encoding ( Sennrich et al , 2016 ) with 14 K merge operations on the concatenation of source and target languages . This results in a German and English vocabularies of around 12 K and 9 K types respectively . Implementation details . Unless stated otherwise , we use DenseNets with masked convolutional filters of size 5 \u00d7 3 , as given by the light blue area in Figure 1 . To train our models , we use maximum likelihood estimation ( MLE ) with Adam ( \u03b2 1 = 0.9 , \u03b2 2 = 0.999 , = 1e \u22128 ) starting with a learning rate of 5e \u22124 that we scale by a factor of 0.8 if no improvement ( \u03b4 \u2264 0.01 ) is noticed on the validation loss after three evaluations , we evaluate every 8 K updates . After training all models up to 40 epochs , the best performing model on the validation set is used for decoding the test set . We use a beam - search of width 5 without any length or coverage penalty and measure translation quality using the BLEU metric ( Papineni et al , 2002 ) . Baselines . For comparison with state - of - theart architectures , we implemented a bidirectional LSTM encoder - decoder model with dotproduct attention ( Bahdanau et al , 2015 ; Luong et al , 2015 ) using PyTorch ( Paszke et al , 2017 ) , and used Facebook AI Research Sequence - to - Sequence Toolkit ( Gehring et al , 2017b ) to train the ConvS2S and Transformer ( Vaswani et al , 2017 ) models on our data . For the Bi - LSTM encoder - decoder , the encoder is a single layer bidirectional LSTM with input embeddings of size 128 and a hidden state of size with different pooling operators and using gated convolutional units .", "entities": [[157, 160, "MethodName", "byte pair encoding"], [244, 245, "MethodName", "Adam"], [246, 247, "HyperparameterName", "\u03b2"], [251, 252, "HyperparameterName", "\u03b2"], [263, 265, "HyperparameterName", "learning rate"], [280, 281, "HyperparameterName", "\u03b4"], [289, 290, "MetricName", "loss"], [347, 348, "MetricName", "BLEU"], [372, 374, "MethodName", "bidirectional LSTM"], [427, 428, "MethodName", "Transformer"], [444, 445, "MethodName", "LSTM"], [455, 457, "MethodName", "bidirectional LSTM"]]}
{"text": ") . The decoder is a single layer LSTM with similar input size and a hidden size of 256 , the target input embeddings are also used in the pre - softmax projection . For regularization , we apply a dropout of rate 0.2 to the inputs of both encoder and decoder and to the output of the decoder prior to softmax . As in ( Bahdanau et al , 2015 ) , we refer to this model as RNNsearch . The ConvS2S model we trained has embeddings of dimension 256 , a 16 - layers encoder and 12 - layers decoder . Each convolution uses 3\u00d71 filters and is followed by a gated linear unit with a total of 2 \u00d7 256 channels . Residual connections link the input of a convolutional block to its output . We first trained the default architecture for this dataset as suggested in FairSeq ( Gehring et al , 2017b ) , which has only 4 layers in the encoder and 3 in the decoder , but achieved better results with the deeper version described above . The model is trained with MLE using Nesterov accelerated gradient with a momentum of 0.99 and an initial learning rate of 0.25 decaying by a factor of 0.1 every epoch . ConvS2S is also regularized with a dropout rate of 0.2 . For the transformer model , use the settings of ( Vaswani et al , 2017 ) . We use token embeddings of dimension 512 , and the encoder and decoder have 6 layers and 8 attention heads . For the inner layer in the per - position feed - forawrd network we use d f f = 2048 . For MLE training we use Adam ( \u03b2 1 = 0.9 , \u03b2 2 = 0.98 , = 1e \u22128 ) ( Kingma and Ba , 2015 ) , and a learning rate starting from 1e \u22125 that is increased during 4 , 000 warm - up steps then used a learning rate of 5e \u22124 that follows an inverse - square - root schedule afterwards ( Vaswani et al , 2017 ) . Similar to previous models we set the dropout rate to 0.2 .", "entities": [[8, 9, "MethodName", "LSTM"], [31, 32, "MethodName", "softmax"], [61, 62, "MethodName", "softmax"], [104, 105, "MethodName", "convolution"], [113, 116, "MethodName", "gated linear unit"], [191, 194, "MethodName", "Nesterov accelerated gradient"], [202, 204, "HyperparameterName", "learning rate"], [283, 284, "DatasetName", "2048"], [290, 291, "MethodName", "Adam"], [292, 293, "HyperparameterName", "\u03b2"], [297, 298, "HyperparameterName", "\u03b2"], [316, 318, "HyperparameterName", "learning rate"], [336, 338, "HyperparameterName", "learning rate"]]}
{"text": "Architecture evaluation . In this section we explore the impact of several parameters of our model : the token embedding dimension , depth , growth rate and filter sizes . We also evaluate different aggregation mechanisms across the source dimension : max - pooling , average - pooling , and attention . In each chosen setting , we train five models with different initializations and report the mean and standard deviation of the BLEU scores . We also state the number of parameters of each model and the computational cost of training , estimated in a similar way as Vaswani et al ( 2017 ) , based on the wall clock time of training and the GPU single precision specs . In Table 1 we see that using max - pooling instead average - pooling across the source dimension increases the performance with around 2 BLEU points . Scaling the average representation with | s | Eq . ( 3 ) helped improving the performance but it is still largely outperformed by the max - pooling . Adding gated linear units on top of each convolutional layer does not improve the BLEU scores , but increases the variance due to the additional parameters . Stand - alone selfattention i.e. weighted average - pooling is slightly better than uniform average - pooling but it is still outperformed by max - pooling . Concatenating the max - pooled features ( Eq . ( 2 ) ) with the representation obtained with self - attention ( Eq . ( 9 ) ) leads to a small but significant increase in performance , from 33.70 to 33.81 . In the remainder of our experiments we only use max - pooling for simplicity , unless stated otherwise . In Figure 4 we consider the effect of the token embedding size , the growth rate of the network , and its depth . The token embedding size together with the growth rate g control the number of features that are passed though the pooling operator along the source dimension , and that can be used used for token prediction . Using the same embedding size d = d t = d s on both source and target , the total number of features for token prediction produced by the network is f L = 2d + gL. In Figure 4 we see that for token embedding sizes between 128 to 256 lead to BLEU scores vary between 33.5 and 34 . Smaller embedding sizes quickly degrade the performance to 32.2 for embeddings of size 64 . The growth rate ( g ) has an important impact on performance , increasing it from 8 to 32 increases the BLEU scrore by more than 2.5 point . Beyond g = 32 performance saturates and we observe only a small improvement . For a good trade - off between performance and computational cost we choose g = 32 for the remaining experiments . The depth of the network also has an important impact on performance , increasing the BLEU score by about 2 points when increasing the depth from 8 to 24 layers . Beyond this point performance drops due to over - fitting , which means we should either increase the dropout rate or add another level of regularization before considering deeper networks . The receptive field of our model is controlled by its depth and the filter size . In Table 2 , we note that narrower receptive fields are better than larger ones with less layers at equivalent complextities e.g. comparing ( k = 3 , L = 20 ) to ( k = 5 , L = 12 ) , and ( k = 5 , L = 16 ) with ( k = 7 , L = 12 ) . Comparison to the state of the art . We compare our results to the state of the art in Ta - ble 3 for both directions German - English ( De - En ) and English - German ( En - De ) . We refer to our model as Pervasive Attention . Unless stated otherwise , the parameters of all models are trained using maximum likelihood estimation ( MLE ) . For some models we additionally report results obtained with sequence level estimation ( SLE , e.g. using reinforcement learning approaches ) , typically aiming directly to optimize the BLEU measure rather than the likelihood of correct translation . First of all we find that all results obtained using byte - pair encodings ( BPE ) are superior to wordbased results . Our model has about the same number of parameters as RNNsearch , yet improves performance by almost 3 BLEU points . It is also better than the recent work of on recurrent architectures with variational attention . Our model outperforms both the recent transformer approach of Vaswani et al ( 2017 ) and the convolutional model of Gehring et al ( 2017b ) in both translation directions , while having about 3 to 8 times fewer parameters . Our model has an equivalent training cost to the transformer ( as implemented in fairseq ) while the convs2s implementation is well optimized with fast running 1dconvolutions leading to shorter training times . Performance across sequence lengths . In Figure 5 we consider translation quality as a function of sentence length , and compare our model to RNNsearch , ConvS2S and Transformer . Our model gives the best results across all sentence lengths , except for the longest ones where ConvS2S and Transformer are better . Overall , our model combines the strong performance of RNNsearch on short sentences with good perfor - Implicit sentence alignments . Following the method described in Section 3 , we illustrate in Figure 6 the implicit sentence alignments the maxpooling operator produces in our model . For reference we also show the alignment produced by our model using self - attention . We see that with both max - pooling and attention qualitatively similar implicit sentence alignments emerge . Notice in the first example how the max - pool model , when writing I 've been working , looks at arbeite but also at seit which indicates the past tense of the former . Also notice some cases of non - monotonic alignment . In the first example for some time occurs at the end of the English sentence , but seit einiger zeit appears earlier in the German source . For the second example there is non - monotonic alignment around the negation at the start of the sentence . The first example illustrates the ability of the model to translate proper names by breaking them down into BPE units . In the second example the German word Karriereweg is broken into the four BPE units karri , er , e , weg . The first and the fourth are mainly used to produce the English a carreer , while for the subsequent path the model looks at weg . Finally , we can observe an interesting pattern in the alignment map for several phrases across the three examples . A rough lower triangular pattern is observed for the English phrases for some time , and it 's fantastic , and it 's not , a little step , and in that direction . In all these cases the phrase seems to be decoded as a unit , where features are first taken across the entire corresponding source Figure 6 : Implicit BPE token - level alignments produced by our Pervasive Attention model . For the maxpooling aggregation we visualize \u03b1 obtained with Eq . ( 7 ) and for self - attention the weights \u03c1 of Eq . ( 8 ) . ( Bahdanau et al , 2017 ) 27.56 Bi - GRU ( MLE+SLE ) ( Bahdanau et al , 2017 ) 28.53 Conv - LSTM ( deep+pos ) ( Gehring et al , 2017a ) 30.4 NPMT + language model ( Huang et al , 2018 ) 30.08 25.36", "entities": [[19, 21, "HyperparameterName", "embedding dimension"], [73, 74, "MetricName", "BLEU"], [80, 83, "HyperparameterName", "number of parameters"], [145, 146, "MetricName", "BLEU"], [191, 192, "MetricName", "BLEU"], [407, 408, "MetricName", "BLEU"], [451, 452, "MetricName", "BLEU"], [509, 511, "MetricName", "BLEU score"], [596, 598, "HyperparameterName", "k ="], [606, 608, "HyperparameterName", "k ="], [617, 619, "HyperparameterName", "k ="], [627, 629, "HyperparameterName", "k ="], [736, 737, "MetricName", "BLEU"], [761, 762, "MethodName", "BPE"], [775, 778, "HyperparameterName", "number of parameters"], [787, 788, "MetricName", "BLEU"], [908, 909, "MethodName", "Transformer"], [929, 930, "MethodName", "Transformer"], [1123, 1124, "MethodName", "BPE"], [1139, 1140, "MethodName", "BPE"], [1257, 1258, "MethodName", "BPE"], [1275, 1276, "HyperparameterName", "\u03b1"], [1308, 1309, "MethodName", "GRU"], [1322, 1323, "MethodName", "LSTM"]]}
{"text": "Our NMT systems were standard base Transformer models trained using the Marian NMT framework ( Junczys - Dowmunt et al , 2018 ) . We trained separate , unidirectional models for each language pair . Hyperparameters such as label smoothing , dropout , learning rate , batch size , number of encoder / decoder layers , number of attention heads , embedding dimensionality , etc . , were held fixed across all language pairs . The validation frequency was every 500 updates , and training was continued for 50 epochs or until the primary validation metric ( ce - mean - words , or mean word cross - entropy score ) failed to improve for five consecutive checkpoints . Our models were trained on AWS P3 instances using 4 NVIDIA Tesla V100 GPUs .", "entities": [[6, 7, "MethodName", "Transformer"], [38, 40, "MethodName", "label smoothing"], [43, 45, "HyperparameterName", "learning rate"], [46, 48, "HyperparameterName", "batch size"], [125, 126, "DatasetName", "P3"]]}
{"text": "Our results show that for most language pairs , a shared vocabulary of size 8 , 000 achieved the best performance . For the Korean Japanese and Japanese Korean language pairs , using a vocabulary size of 32 , 000 produced better results . Using a split vocabulary for these language pairs also resulted in better performance , whereas a shared vocabulary was advantageous for all other language pairs . In all cases , the inclusion of back translated training data resulted in higher validation scores . Table 1 shows our results in terms of BLEU scores ( Papineni et al , 2002 ) as calculated on our local machines . Due to differences in processing , these scores do not match the scores reported by the Organizers .", "entities": [[95, 96, "MetricName", "BLEU"]]}
{"text": "This paper describes the system for our participation of team Wanghao - ftd - SJTU in the CoNLL 2017 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies . In this work , we design a system based on UDPipe 1 for universal dependency parsing , where transitionbased models are trained for different treebanks . Our system directly takes raw texts as input , performing several intermediate steps like tokenizing and tagging , and finally generates the corresponding dependency trees . For the special surprise languages for this task , we adopt a delexicalized strategy and predict based on transfer learning from other related languages . In the final evaluation of the shared task , our system achieves a result of 66.53 % in macro - averaged LAS F1 - score .", "entities": [[28, 30, "DatasetName", "Universal Dependencies"], [45, 47, "TaskName", "dependency parsing"], [101, 103, "TaskName", "transfer learning"], [130, 133, "MetricName", "F1 - score"]]}
{"text": "Universal Dependencies ( UD ) ( Nivre et al , 2016 ( Nivre et al , , 2017b and universal dependency parsing take efforts to build cross - linguistically treebank annotation and develop cross - lingual learning to parse many languages even low - resource languages . Universal Dependencies release 2.0 2 ( Nivre et al , 2017b ) includes rich languages and treebanks resources and the parsing task in CoNLL 2017 is based on this dataset . In fact , dependency parsing has been adopted as topic of the shared task in CoNLL - X andCoNLL - 2007 ( Buchholz andMarsi , 2006 ; Nivre et al , 2007 ) , which have been the milestones for the researching field of parsing . This time , the task is taking a universal annotation version and trying to exploit cross - linguistic similarities between various languages . In this paper , we describe the system of team Wanghao - ftd - SJTU for the CoNLL 2017 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies ( Zeman et al , 2017 ) . For this task , we only use provided treebanks to train models without any other resources including pretrained embeddings . For dependency parsing , there have been two major parsing methods : graph - based and transition - based . The former searches for the final tree through graph algorithms by decomposing trees into factors , utilizing ingenious dynamic programming algorithms ( Eisner , 1996 ; McDonald et al , 2005 ; McDonald and Pereira , 2006 ) ; while the latter parses sentences by making a series of shift - reduce decisions ( Yamada and Matsumoto , 2003 ; Nivre , 2003 ) . In our system , we will utilize the transition - based system for its simplicity and relatively lower computation cost . Transition - based dependency parsing takes linear time complexity and utilizes rich features to make structural prediction ( Zhang and Clark , 2008 ; Zhang and Nivre , 2011 ) . Specifically , a buffer for input words , a stack for partially built structure and shift - reduce actions are basic elements in a transition - based dependency parsing . For the transition systems of dependency parsing , there have been two major ones : arc - standard and arc - eager ( Nivre , 2008 ) . Our system adopts the former , whose basic algorithm can be described as following : Start : \u03c3 = [ ROOT ] , \u03b2 = w 1 , ... , w n , A = 1 . Shift : \u03c3 , w i | \u03b2 , A \u03c3 | w i , \u03b2 , A 2 . Left - Arc r : \u03c3 | w i | w j , \u03b2 , A \u03c3 | w j , \u03b2 , A \u222a r ( w j , w i ) 3 . Right - Arc r : \u03c3 | w i | w j , \u03b2 , A \u03c3 | w i , \u03b2 , A \u222a r ( w i , w j ) Finish : \u03c3 = [ w ] , \u03b2 = where \u03c3 , \u03b2 , A represent the stack , queue and the actions respectively . One major difference for parsing between the situation of current and that of ten years ago is that recently we have seen a rising of neural network based methods in the field of Natural Language Processing and parsing has also been greatly changed by the neural methods . With distributed representation for words and sentences and the powerful non - linear calculation ability of the neural networks , we could explore deeper syntactic and maybe semantic meaning in text analysis , and both graph - based ( Pei et al , 2015 ; Wang and Chang , 2016 ) and transition - based ( Chen and Manning , 2014 ; Weiss et al , 2015 ; Dyer et al , 2015 ; Andor et al , 2016 ) parsing have benefited a lot from neural representation learnings . In our system , the model , which is trained by UDPipe , for the transition action predictor is also based on neural network , which is similar to the one of Chen and Manning ( 2014 ) . For this shared task , our system is built based on UDpipe ( Straka et al , 2016 ) , which provides a pipeline from raw text to dependency structures , including a tokenizer , taggers and the dependency predictor . We trained and tuned the models on different treebanks , and in the final evaluation , a score of 66.53 % in macro - averaged LAS F1 - score measurement is achieved . In the task , there are several surprise languages which lack of annotated resources , which means it is hard to train specified models for those languages . To tackle this problem , we exploit the universal part - of - speech ( POS ) tags , which could be represented as crosslingual knowledge to avoid language - specific information , and adopting a delexicalized and crosslingual method , which relies solely on universal POS tags and annotated data in close - related languages . The rest of the paper is organized as follows : Section 2 describes our system overview , Section 3 elaborates the components of the system , Section 4 shows the experiments and results for our participation in the shared task , and Section 5 concludes this paper .", "entities": [[0, 2, "DatasetName", "Universal Dependencies"], [3, 4, "DatasetName", "UD"], [20, 22, "TaskName", "dependency parsing"], [47, 49, "DatasetName", "Universal Dependencies"], [81, 83, "TaskName", "dependency parsing"], [175, 177, "DatasetName", "Universal Dependencies"], [206, 208, "TaskName", "dependency parsing"], [311, 316, "TaskName", "Transition - based dependency parsing"], [366, 371, "TaskName", "transition - based dependency parsing"], [377, 379, "TaskName", "dependency parsing"], [423, 424, "HyperparameterName", "\u03b2"], [444, 445, "HyperparameterName", "\u03b2"], [452, 453, "HyperparameterName", "\u03b2"], [470, 471, "HyperparameterName", "\u03b2"], [478, 479, "HyperparameterName", "\u03b2"], [505, 506, "HyperparameterName", "\u03b2"], [513, 514, "HyperparameterName", "\u03b2"], [533, 534, "HyperparameterName", "\u03b2"], [538, 539, "HyperparameterName", "\u03b2"], [795, 798, "MetricName", "F1 - score"], [839, 842, "DatasetName", "part - of"]]}
{"text": "In this paper , we describe the universal dependency parser for our participation in the CoNLL 2017 shared task . The official evaluation shows that our system achieves 66.53 % in macroaveraged LAS F1 - score measurement on the official blind test . Further improvements could be obtained by more carefully fine - tuning models and adopting more sophisticated neural models .", "entities": [[33, 36, "MetricName", "F1 - score"]]}
{"text": "The proposed method produces pseudo label\u015d p ( y | x j ) for unlabeled documents { x j } m j=1 . When true labels { ( x i , y i ) } n i=1 are available , we can train a new discriminative logistic regression classifier p \u03b8 ( y | x ) using both true and pseudo labels ( \u03b8 is the model parameter ) : J ( \u03b8 ) = n i=1 y Y \u22121 { y i = y } log p \u03b8 ( y | x i ) + \u03bb \u03b8 2 + \u00b5 m j=1 y Y \u2212p ( y | x j ) log p \u03b8 ( y | x j ) . ( 5 ) To find the balance of pseudo vs. true labels in ( 5 ) , we search the hyperparameter \u00b5 on a 5point grid { 10 \u22122 , 10 \u22121 , 0.4 , 0.7 , 1 } . We expect pseudo labels to have comparable importance as true labels when n is small ( fine granularity for \u00b5 [ 10 \u22121 , 1 ] ) , and their importance will diminish as n gets large ( \u00b5 = 10 \u22122 ) . \u00b5 is automatically selected such that it gives the best 5 - fold crossvalidation accuracy on n true labels .", "entities": [[46, 48, "MethodName", "logistic regression"], [50, 51, "HyperparameterName", "\u03b8"], [63, 64, "HyperparameterName", "\u03b8"], [72, 73, "HyperparameterName", "\u03b8"], [88, 89, "HyperparameterName", "\u03b8"], [97, 98, "HyperparameterName", "\u03b8"], [114, 115, "HyperparameterName", "\u03b8"], [220, 221, "MetricName", "accuracy"]]}
{"text": "Retrieval - based methods . We use language modeling retrieval function with Dirichlet smoothing ( Zhai and Lafferty , 2001 ) ( \u00b5 = 2500 ) to match a document to class labels ( IR ) . The top 10 results are then used as pseudo - labeled documents to retrain three classifiers : IR+Roc : a Rocchio classifier ( \u03b1 = 1 , \u03b2 = 0.5 , \u03b3 = 0 ) ; IR+NB : a multinomial naive Bayes classifier ( Laplace smoothing , \u03b1 = 0.01 ) ; IR+LR a logistic regression classifier ( linear kernel , C = 1 ) . Semi - supervised methods . ST - 0 : the initial self - training classifier using class labels as \" training documents \" ( multinomial na\u00efve Bayes , Laplace smoothing \u03b1 = 0.01 ) . ST - 1 : ST - 0 retrained on 10 most confident documents predicted by itself . GE : a logistic regression classifier trained using generalized expectation criteria ( Druck et al , 2008 ) . Class labels are used as labeled features . sLDA : a supervised topic model trained using seeded LDA ( Jagarlamudi et al , 2012 ) . Besides k seeded topics ( k is the number of classes ) , we use an extra topic to account for other content in the corpus . Word embedding - based methods . Cosine : a centroid - based classifier , where class definitions and documents are represented as average of word vectors . WENB : The proposed method ( Section 3 ) . WENB+LR : a logistic regression classifier trained only on pseudo labels produced by WENB ( Section 3.1 , n = 0 ) . For general domain tasks , we take raw text from English Wikipedia , English news crawl ( WMT , 2014 ) , and 1 billion word news corpus ( Chelba et al , 2013 ) to train word vectors . For medical domain tasks , we take raw text from MEDLINE abstracts ( NLM , 2018 ) to train word vectors . We find 50 - dimensional skip - gram word vectors perform reasonably well in the experiments .", "entities": [[60, 61, "HyperparameterName", "\u03b1"], [64, 65, "HyperparameterName", "\u03b2"], [68, 69, "HyperparameterName", "\u03b3"], [70, 71, "DatasetName", "0"], [84, 85, "HyperparameterName", "\u03b1"], [91, 93, "MethodName", "logistic regression"], [110, 111, "DatasetName", "0"], [133, 134, "HyperparameterName", "\u03b1"], [144, 145, "DatasetName", "0"], [158, 160, "MethodName", "logistic regression"], [191, 192, "MethodName", "LDA"], [267, 269, "MethodName", "logistic regression"], [284, 285, "DatasetName", "0"], [328, 330, "DatasetName", "medical domain"]]}
{"text": "Label savings . Table 2 shows that overall , class labels can train text classifiers remarkably better than majority guess . This is no small feat considering that the classifier has not seen any labeled documents yet . Such performance gain essentially comes \" for free \" , as any text classification task has to start by defining classes . In Table 3 , we report the number of true labels needed for a logistic regression model to achieve the same performance as WENB+LR . The most significant savings happen on short documents : class labels are equivalent to hundreds to thousands of labeled documents at the beginning of the training process . Effect of document length . On short documents ( Wiki Titles , News Titles , Y Questions ) , leveraging unlabeled data does not help with most semi - supervised methods due to severe vocabulary mismatch . The proposed methods ( WENB and WENB+LR ) show robust performance , because pretrained word vectors can capture semantic similarity even without any word overlap between a class label and a document . This prior knowledge is essential when documents are short . On long documents ( 20 News , Reuters , Med WSD ) , leveraging unlabeled data helps , since long documents have richer content and are more likely to contain not only label words themselves , but also other topic - specific words . Retrieval - based and semi - supervised methods are able to learn these words by exploiting intra - document word co - occurrences . Performance of other methods . Learning from class labels themselves provides very limited help ( IR and ST - 0 ) . Using class labels as search queries and labeled documents are closely related : IR and ST - 0 perform similarly ; so do IR+NB and ST - 1 . When using class labels as search queries , 2 shows a salient warm - start effect on a balanced binary classification task in 20 News . The weight \u00b5 of pseudo labels increases when true labels are few ( initial classifier as an informative prior ) . As expected , \u00b5 decreases when true labels become abundant . Figure 3 shows another binary classification task in 20 News where the warm - start effect is limited . Correspondingly , \u00b5 quickly diminishes as more true labels are available . With 100 or more true labels , pseudo labels have a negligible weight ( \u00b5 = 10 \u22122 ) . In machine learning terms , these pseudo labels specify an incorrect prior that the model should quickly forget , so that it will not hinder the overall learning process . A closer investigation reveals that the word vector for mideast ( the class label of one topic in Figure 3 ) is not well - trained . This is because in general text corpus , the word mideast is rather infrequent compared to commonly used alternatives , such as middle east . The word vector of mideast is surrounded by other infrequent words or misspellings ( such as hizballah , jubeir , saudis , isreal ) as opposed to more frequent and relevant ones ( such as israel , israeli , saudi , arab ) . Since WENB uses the semantic knowledge in word vectors to infer pseudo labels , the quality of class label word vectors will affect the pseudo label accuracy .", "entities": [[50, 52, "TaskName", "text classification"], [74, 76, "MethodName", "logistic regression"], [168, 170, "TaskName", "semantic similarity"], [280, 281, "DatasetName", "0"], [300, 301, "DatasetName", "0"], [573, 574, "MetricName", "accuracy"]]}
{"text": "Slot - filling , Translation , Intent classification , and Language identification , or STIL , is a newly - proposed task for multilingual Natural Language Understanding ( NLU ) . By performing simultaneous slot filling and translation into a single output language ( English in this case ) , some portion of downstream system components can be monolingual , reducing development and maintenance cost . Results are given using the multilingual BART model ( Liu et al , 2020 ) fine - tuned on 7 languages using the MultiATIS++ dataset . When no translation is performed , mBART 's performance is comparable to the current state of the art system ( Cross - Lingual BERT by Xu et al ( 2020 ) ) for the languages tested , with better average intent classification accuracy ( 96.07 % versus 95.50 % ) but worse average slot F1 ( 89.87 % versus 90.81 % ) . When simultaneous translation is performed , average intent classification accuracy degrades by only 1.7 % relative and average slot F1 degrades by only 1.2 % relative .", "entities": [[4, 5, "TaskName", "Translation"], [6, 8, "TaskName", "Intent classification"], [10, 12, "TaskName", "Language identification"], [24, 27, "TaskName", "Natural Language Understanding"], [34, 36, "TaskName", "slot filling"], [72, 73, "MethodName", "BART"], [98, 99, "MethodName", "mBART"], [115, 116, "MethodName", "BERT"], [132, 134, "TaskName", "intent classification"], [134, 135, "MetricName", "accuracy"], [146, 147, "MetricName", "F1"], [162, 164, "TaskName", "intent classification"], [164, 165, "MetricName", "accuracy"], [174, 175, "MetricName", "F1"]]}
{"text": "The multilingual BART ( mBART ) model architecture was used ( Liu et al , 2020 ) , as well as the pretrained mBART.cc25 model described in the same paper . The model consists of 12 encoder layers , 12 decoder layers , a hidden layer size of 1 , 024 , and 16 attention heads , yielding a parameter count of 680M. The mBART.cc25 model was trained on 25 languages for 500k steps using a 1.4 TB corpus of scraped website data taken from Common Crawl ( Wenzek et al , 2019 ) . The model was trained to reconstruct masked tokens and to rearrange scrambled sentences . SentencePiece tokenization ( Kudo and Richardson , 2018 ) was used for mBART.cc25 with a sub - word vocabulary size of 250k .", "entities": [[2, 3, "MethodName", "BART"], [4, 5, "MethodName", "mBART"], [44, 47, "HyperparameterName", "hidden layer size"], [85, 87, "DatasetName", "Common Crawl"], [109, 110, "MethodName", "SentencePiece"]]}
{"text": "The same vocabulary as that of the pretrained model was used for this work , and SentencePiece tokenization was performed on the full sequence , including the slot tags , intent tags , and language tags . For all mBART experiments and datasets , data from all languages were shuffled together . The fairseq library was used for all experimentation ( Ott et al , 2019 ) . Training was performed on 8 Nvidia V100 GPUs ( 16 GB ) using a batch size of 32 , layer normalization for both the encoder and the decoder ( Xu et al , 2019 ) ; label smoothed cross entropy with = 0.2 ( Szegedy et al , 2016 ) ; the ADAM optimizer with \u03b2 1 = 0.9 and \u03b2 2 = 0.999 ( Kingma and Ba , 2014 ) ; an initial learning rate of 3 \u00d7 10 \u22125 with polynomial decay over 20 , 000 updates after 1 epoch of warmup ; attention dropout of 0.1 and dropout of 0.2 elsewhere ; and FP16 type for weights . Each model was trained for 19 epochs , which took 5 - 6 hours .", "entities": [[16, 17, "MethodName", "SentencePiece"], [39, 40, "MethodName", "mBART"], [82, 84, "HyperparameterName", "batch size"], [87, 89, "MethodName", "layer normalization"], [120, 121, "DatasetName", "ADAM"], [121, 122, "HyperparameterName", "optimizer"], [123, 124, "HyperparameterName", "\u03b2"], [128, 129, "HyperparameterName", "\u03b2"], [142, 144, "HyperparameterName", "learning rate"], [163, 165, "MethodName", "attention dropout"]]}
{"text": "Results from the models are given in Table 3 . Statistical significance was evaluated using the Wilson method ( Wilson , 1927 ) with 95 % confidence . Xu et al ( 2020 ) Examining the first training configuration ( 1 , 496 samples for Hindi and 626 for Turkish ) , the nontranslated mBART 's macro - averaged intent classification ( 96.07 % ) outperforms Cross - Lingual BERT by Xu et al ( 2020 ) ( 95.50 % ) , but slot F1 is worse ( 89.87 % for non - translated mBART and 90.81 % for Cross - Lingual BERT ) . The differences are statistically significant in both cases .", "entities": [[54, 55, "MethodName", "mBART"], [59, 61, "TaskName", "intent classification"], [69, 70, "MethodName", "BERT"], [84, 85, "MetricName", "F1"], [94, 95, "MethodName", "mBART"], [102, 103, "MethodName", "BERT"]]}
{"text": "When translation is performed ( the STIL task ) , intent classification accuracy degrades by 1.7 % relative from 96.07 % to 94.40 % , and slot F1 degrades by 1.2 % relative from 89.87 % to 88.79 % . The greatest degradation occurred for utterances involving flight number , airfare , and airport name ( in that order ) .", "entities": [[10, 12, "TaskName", "intent classification"], [12, 13, "MetricName", "accuracy"], [27, 28, "MetricName", "F1"]]}
{"text": "Adding 105 more Hindi and 12 more Turkish training examples results in improved performance for the translated , STIL mBART model . Macro - averaged intent classification improves from 94.40 % to 95.94 % , and slot F1 improves from 88.79 % to 90.10 % , both of which are statistically significant . By adding these 117 samples , the STIL mBART model matches the performance ( within confidence intervals ) of the non - translated mBART model . This finding suggests that the STIL models may require more training data than traditional , non - translated slot filling models . Additionally , by adding more Hindi and Turkish data , both the intent accuracy and the slot filling F1 improves for every individual language of the translated , STIL models , suggesting that some portion of the internal , learned representation is language agnostic . Finally , the results suggest that there is a trainingsize - dependent performance advantage in using a single output language , as contrasted with the nontranslated mBART model , for which the intent classification accuracy and slot F1 does not improve ( with statistical significance ) when using the additional Hindi and Turkish training samples .", "entities": [[19, 20, "MethodName", "mBART"], [25, 27, "TaskName", "intent classification"], [37, 38, "MetricName", "F1"], [61, 62, "MethodName", "mBART"], [76, 77, "MethodName", "mBART"], [97, 99, "TaskName", "slot filling"], [114, 115, "MetricName", "accuracy"], [117, 119, "TaskName", "slot filling"], [119, 120, "MetricName", "F1"], [172, 173, "MethodName", "mBART"], [178, 180, "TaskName", "intent classification"], [180, 181, "MetricName", "accuracy"], [183, 184, "MetricName", "F1"]]}
{"text": "Language identification F1 is above 99.7 % for all languages , with perfect performance in many cases . ( Qin et al , 2019 ) 97.5 Joint BERT + CRF ( Chen et al , 2019 ) 97.9 Non - translated mBART , with Perfect performance on Chinese and Hindi is unsurprising given their unique scripts versus the other languages tested .", "entities": [[0, 2, "TaskName", "Language identification"], [2, 3, "MetricName", "F1"], [27, 28, "MethodName", "BERT"], [29, 30, "MethodName", "CRF"], [41, 42, "MethodName", "mBART"]]}
{"text": "Compositor attribution , the clustering of pages in a historical printed document by the individual who set the type , is a bibliographic task that relies on analysis of orthographic variation and inspection of visual details of the printed page . In this paper , we introduce a novel unsupervised model that jointly describes the textual and visual features needed to distinguish compositors . Applied to images of Shakespeare 's First Folio , our model predicts attributions that agree with the manual judgements of bibliographers with an accuracy of 87 % , even on text that is the output of OCR .", "entities": [[87, 88, "MetricName", "accuracy"]]}
{"text": "Our computational approach to compositor attribution operates on the sources of evidence that have been considered by bibliographers . In particular , we focus on jointly modeling patterns of orthographic variation and spacing preferences across pages of a document , treating compositor assignments as latent variables in a generative model . We assume access to a diplomatic transcription of the document ( a transcription faith - ful to the original orthography ) , which we automatically align with a modernized version . 2 We experiment with both manually and automatically ( OCR ) produced transcriptions , and assume access to pixel - level spacing information on each page , which can be extracted using OCR as described in Section 4 . Figure 2 shows the generative process . In our model , each of I total pages is generated independently . The compositor assignment for the ith page is represented by the variable c i { 1 , . . . , C } and is generated from a multinomial prior . For page i , each diplomatic word , d ij , is generated conditioned on the corresponding modern word , m ij , and the compositor who set the page , c i . Finally , the model produces the pixel width of the space after each medial comma , s ik , again conditioned on the compositor , c i . The joint distribution for page i , conditioned on modern text , takes the following form : P ( { d ij } , { s ik } , c i | { m ij } ) = P ( c i ) [ Prior on compositors ] J i j=1 P ( d ij | m ij , c i ; w c i ) [ Orthographic model ] K i k=1 P ( s ik | c i ; \u03b8 c i ) [ Whitespace model ]", "entities": [[315, 316, "HyperparameterName", "\u03b8"]]}
{"text": "Manual analysis of spacing has revealed differences across pages . In particular , the choice of spaced or non - spaced punctuation marks is hypothesized by biobliographers to be indicative of compositor preference and specific typecase . We add whitespace distance to our model to capture those observations . While bibliographers only made a coarse distinction between spaced or nonspaced commas , in our model we generate medial comma spacing widths , s ik , that are measured in pixels to enable finer - grained analysis . We use a simple multinomial parameterization where each pixel width is treated as a separate outcome up to some maximum allowable width : s ik | c i \u223c M ult ( \u03b8 c i ) Here , \u03b8 c represents the vector of multinomial spacing parameters corresponding to compositor c. We choose this parameterization because it can capture non - unimodal whitespace preference distributions , as depicted in Figure 2 , and it makes learning simple .", "entities": [[119, 120, "HyperparameterName", "\u03b8"], [125, 126, "HyperparameterName", "\u03b8"]]}
{"text": "Modern and diplomatic words and spacing variables are observed , while compositor assignments are latent . In order to fit the model to an input document we estimate the orthographic preference parameters , w c , and spacing preference parameters , \u03b8 c , for each compositor using EM . The E - step is accomplished via a tractable sum over compositor assignments , while the M - step for w c is accomplished via gradient ascent ( Berg - Kirkpatrick et al , 2010 ) . The M - step for spacing parameters , \u03b8 c , uses the standard multinomial update . Predicting compositor groups is accomplished via an Model Setup", "entities": [[41, 42, "HyperparameterName", "\u03b8"], [48, 49, "MetricName", "EM"], [95, 96, "HyperparameterName", "\u03b8"]]}
{"text": "Ocular OCR Transcription Hinman Attr Blayney Attr Hinman Attr Blayney Attr 1 - to - 1 M - to - 1 1 - to - 1 M - to - 1 1 - to - 1 M - to - 1 1 - to - 1 M - to - 1 ( Hinman , 1963 ; Howard - Hill , 1973 , 1976 , 1980Taylor , 1981 ; O'Connor , 1975 ; Werstine , 1982 ) . We also evaluate our system against an earlier , highly influential model proposed by Hinman ( 1963 ) , which we approximate by reverting certain compositor divisions in Blayney 's attribution . Hinman 's attribution posited five compositors , while Blayney 's posited eight . In experiments , we set the model 's maximum number of compositors to C = 5 when evaluating on Hinman 's attribution , and use C = 8 with Blayney 's . We compute the one - to - one and many - to - one accuracy , mapping the recovered page groups to the gold compositors to maximize accuracy , as is standard for many unsupervised clustering tasks , e.g. POS induction ( see Christodoulopoulos et al ( 2010 ) ) . BASIC model variant : We evaluate a simple baseline model that uses a multinomial parameterization for generating diplomatic words and does not incorporate spacing information . We use two different options for selection of spelling variants to be considered by the model . First , we consider only the three words selected by Hinman : do , go and here ( referred to as HINMAN ) . Second , we use a larger , automatically selected , word list ( referred to as AUTO ) . Here , we select all modern words with frequency greater than 70 that are not names and that exhibit sufficient variance in diplomatic spellings ( most common diplomatic spelling occurs in less than 80 % of aligned tokens ) . For our full model , described in the next section , we always use the larger AUTO word list . FEAT model variant : We run experiments with several variants of our full model , described in Section 3 ( referred to as FEAT since they use a feature - based parameterization of diplomatic word generation . ) We try ablations of WORD and EDIT features , as well as model variants with and without the spacing generation component ( referred to as SPACE . ) We refer to the full model that includes both types of features and spacing generation as ALL .", "entities": [[168, 169, "MetricName", "accuracy"], [181, 182, "MetricName", "accuracy"], [393, 394, "DatasetName", "WORD"], [414, 415, "DatasetName", "SPACE"]]}
{"text": "Our experimental results are presented in Table 1 . The BASIC variant , modeled after Hinman 's original procedure , substantially outperforms the random baseline , with the HINMAN word list outperforming the larger AUTO word list . However , use ! ! ! ! ! ! ! ! ! u o u w ! u DEL ! B A C E D ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! Comp B Comp A Comp C Comp E Comp D ! ! ! ! Figure 3 : Learned behaviors of the Folio compositors . Our model only detected the presence of five compositors ( ranked according to number of pages the compositor set in our model 's prediction ) . Compositor D 's habit of omitting u ( yong vs. young ) and compositor C 's usage of spaced medial commas were also noticed in Taylor ( 1981 ) . of the larger word list with feature - based models yields large gains in all scenarios , including evaluation on Hinman 's original attributions and while using OCR diplomatic transcriptions . The bestperforming model for both manually transcribed and OCR text uses EDIT features in conjunction with spacing generation and achieves an accuracy of up to 87 % . Including WORD features on top of this leads to slightly reduced performance , perhaps as a result of the substantially increased number of free parameters . In the OCR scenario , the addition of WORD features on top of EDIT decreases accuracy , unlike the same experiment with the manual transcription . This is possibly a result of the reduced reliability of full word forms due to mistakes in OCR . Particularly interesting is the result that spacing , rarely a factor considered in NLP models , improves the accuracy significantly for our system when compared with EDIT features alone . Because pixel - level visual information and arbitrary orthographic patterns are also the most difficult features to measure manually , our results give strong evidence to the assertion that NLP - style models can aid bibliographers .", "entities": [[219, 220, "MetricName", "accuracy"], [227, 228, "DatasetName", "WORD"], [260, 261, "DatasetName", "WORD"], [267, 268, "MetricName", "accuracy"], [315, 316, "MetricName", "accuracy"]]}
{"text": "The input is a liberal news corpus D L = { d L i } | D L | i=1 and a conservative news corpus D R = { d R i } | D R | i=1 ( L denotes \" Left \" and R denotes \" Right \" ) , where d L i is an article from D L and d R i is an article from D R . A news article is represented as a sequence of tokens : d k = ( w k i ) | d k | i=1 . Given a topic model trained on the combined corpus D C = D L \u222a D R with a set of modeled topics T = { t i } K i=1 where t i represents a topic , we aim to learn a model f We train an LDA model on the combined corpus and extract 2 topics . Top - 4 keywords on topic t 1 are \" briefing \" , \" trump \" , \" president \" and \" white_house \" . Top - 2 most relevant documents on topic t 1 are d L 1 and d L 2 for CNN and d R 1 and d R 2 for Fox . d L 3 and d R 3 are not among the most relevant documents of this topic and are excluded in the embedding generation step . Note that we set K = 2 ( No . of topics ) , m = 4 ( No . of keywords ) , and n = 2 ( No . of documents ) , just for clear demonstration . ( b ) Partisanship learning . We finetune a pretrained language model to classify the partisanship ( liberal vs. conservative ) of input documents . ( c ) Topic embedding generation and similarity measuring . We provide a step by step illustration of DC keyword embedding DC topic embedding CC topic embedding on topic t 1 . In the two input corpora , the tokens that are among the top - 4 keywords of topic t 1 are highlighted in bold . Take document d L 1 from CNN as an example . The weighted average of the DC keyword embeddings ( H d L 1 ( president ) , H d L 1 ( trump ) , and H d L 1 ( briefing ) ) is defined as the DC topic embedding H d L 1 ( t 1 ) with keyword coefficients given by Eq . 2 ; note that H d L 1 ( criticize ) is excluded because \" criticize \" is not among the top - 4 keywords of topic t 1 . Similarly we can obtain the DC topic embeddings for d L 2 , d R 1 and d R 2 . The DC topic embeddings are further aggregated into CC topic embeddings H D L ( t 1 ) and H D R ( t 1 ) ( document coefficients are from Eq . 3 ) and the cosine distance between them is used as a measure of polarization of the two corpora on topic t 1 . that is able to detect the topic polarization between D L and D R on topics in T and output a ranking of topics based on polarization , such that f ( D L , D R , T ) = ( t k ) K k=1 , i > j \u21d4 \u03b2 ( t i , D L , D R ) < \u03b2 ( t j , D L , D R ) , ( 1 ) where \u03b2 ( t , D L , D R ) represents the polarization score of topic t between D L and D R .", "entities": [[85, 87, "HyperparameterName", "k ="], [146, 147, "MethodName", "LDA"], [243, 245, "HyperparameterName", "K ="], [588, 589, "HyperparameterName", "\u03b2"], [600, 601, "HyperparameterName", "\u03b2"], [616, 617, "HyperparameterName", "\u03b2"]]}
{"text": "As we will see in Sections 3.4 and 3.5 , the contextualized topic embeddings are generated from a pretrained language model ( Devlin et al , 2018 ) and cosine distance between the topic embeddings from two corpora are used as a measure of topic polarization . The idea is inspired by static word embedding models like GloVe ( Pennington et al , 2014 ) , where the authors measure the similarity between words by the cosine similarity between the word embeddings . However , to apply this measure of similarity , the model should be fitted on the target corpus . To fit the pretrained language model on the news corpora , we can use one of the two training tasks : masked language modeling or partisanship recognition . We decide on the second task because 1 ) it is more time efficient ; 2 ) it informs the language model of the partisan divisions between different news sources , enhancing the language model 's ability to encode the polarization arising from partisan differences in its output . This idea is similar to ( Webson et al , 2020 ) where the authors call the embedding space of the language model after finetuning as \" connotation space \" . As a result , given a document d D C , the model is optimized to classify whether it is from D L or D R by a binary cross - entropy loss , where the [ CLS ] embedding is used to represent the document , as shown in Figure 1 ( b ) .", "entities": [[57, 58, "MethodName", "GloVe"], [80, 82, "TaskName", "word embeddings"], [123, 126, "TaskName", "masked language modeling"], [242, 243, "MetricName", "loss"]]}
{"text": "After obtaining the CC topic embeddings H D L ( t i ) and H D R ( t i ) of the two corpora D L and D R on topic t i , using two different sets of top - n most relevant documents from D L and D R respectively , we measure the ideology similarity ( and then polarization ) based on the cosine similarity between them , such that c = cos_sim ( H D L ( t i ) , H D R ( t i ) ) , \u03b2 ( D L , D R , t i ) = 0.5 * ( 1 \u2212 c ) [ 0 , 1 ] . ( 6 ) A higher value of \u03b2 indicates more polarization . Therefore , the polarization - based ranked topic list f ( D L , D R , T ) is computed based on the corresponding polarization scores ( \u03b2 ( D L , D R , t i ) ) K i=1 . 4 Experiments and Results", "entities": [[95, 96, "HyperparameterName", "\u03b2"], [115, 116, "DatasetName", "0"], [127, 128, "HyperparameterName", "\u03b2"], [160, 161, "HyperparameterName", "\u03b2"]]}
{"text": "Data Preprocessing . We build a global vocabulary containing unigrams and bigrams from the six news sources . We perform lemmatization via LDA Topic Modeling . We train the topic model using articles from all six sources to create a global topic set . The number of topics K is selected from a grid search in [ 10 , 50 ] and the model with K = 39 produces the best coherence value ( R\u00f6der et al , 2015 ) . From the 39 topics we remove 9 of them regarding advertisements , sport events , gossip news and recipes , and 30 topics are left ; the removed topics are more factual and contain less ideologies from the news media , which is less worth studying . Different from ( Demszky et al , 2019 ) that assigns only one topic with the highest probability to a document , we allow a document to be assigned multiple topics with different probabilities . We represent each topic with its top - 10 keywords because given a topic t i we empirically find that 10 j=1 p ij > 0.95 ; and we keep the top - 10 most relevant documents to represent a topic because on some topics , the documents beyond the top - 10 list are obviously irrelevant and will bias the polarization study regarding the topic . In Table 1 we show the top - 10 keywords of topics that are discussed in this paper . For a complete list of topics please refer to Appendix B. Learning Partisanship . We finetune the pretrained bert - base - uncased model from huggingface Transformers ( Wolf et al , 2020 ) to classify the news articles according to their political leanings , or partisanship . To smooth over the differences in style and writing between the sources and render the model primarily sensitive to political divisions , we aggregate CNN , Huff , and NYP to create a holistic Liberal corpus , and similarly aggregate Fox , Breit and NYP to create a holistic Conservative corpus and optimize the model to classify whether an article is from Liberal or Conservative . In fact , finetuning a BERT model to recognize differences only between CNN vs. Fox is likely to make it end up capturing the writing style differences and ignoring political differences , since the former is an easier task . For more details about the training process please refer to Appendix C. Idx Top - 10 keywords ( and two defined stances )", "entities": [[20, 21, "TaskName", "lemmatization"], [22, 23, "MethodName", "LDA"], [65, 67, "HyperparameterName", "K ="], [368, 369, "MethodName", "BERT"]]}
{"text": "As ground truth for the evaluation of PaCTE , we annotate the topic polarization scores on a subset of the 30 modeled topics . We asked three annotators to select 10 topics and define two polarized political stances on each selected topic , and they reached an agreement on T labeled = { t 1 , t 2 , t 8 , t 9 , t 10 , t 11 , t 12 , t 27 , t 30 , t 33 } , as shown in Table 1 . Then on each topic in T labeled , we selected 60 relevant documents ( 10 from each of the six sources ) , and asked three annotators to decide which stance they belong to ( label it as 0/1 ) . If the document does not have a clear stance , it was labeled as \u22121 . On each document , the majority label from the annotations was used as the final annotation . Please refer to Appendix D for more details about the annotation process . Denoting the number of negative labels ( 0 ) and positive labels ( 1 ) in corpus D on topic t as N t D ( 0 ) and N t D ( 1 ) respectively , the leaning of the corpus on the topic is quantified as le ( D , t ) = ( N t D ( 1 ) \u2212N t D ( 0 ) ) / | D | [ \u22121 , 1 ] . ( 7 ) Intuitively , le ( D , t ) reflects how much the corpus is aligned with the stance labeled as 1 . Notably , the documents labeled with \u22121 are not counted because they do not display a clear political standing . Accordingly , the ground - truth polarization score between a liberal corpus D L and a conservative corpus D R on topic t is computed as the difference between the leanings of the two corpora , such that \u03b1 ( D L , D R , t ) = | le ( D L , t ) \u2212 le ( D R , t ) | /2 [ 0 , 1 ] . ( 8 ) A higher value of \u03b1 signifies more polarization . As a result , the ground - truth polarization - based topic ranked list l gt ( D L , D R , T labeled ) between a liberal corpus D L and a conservative corpus D R is computed based on the corresponding ground - truth polarization scores ( \u03b1 ( D L , D R , t ) | t T labeled ) .", "entities": [[184, 185, "DatasetName", "0"], [203, 204, "DatasetName", "0"], [243, 244, "DatasetName", "0"], [339, 340, "HyperparameterName", "\u03b1"], [369, 370, "DatasetName", "0"], [381, 382, "HyperparameterName", "\u03b1"], [436, 437, "HyperparameterName", "\u03b1"]]}
{"text": "The news articles are split into the training set comprising topical documents and validation set comprising non - topical documents . Non - topical documents have small probabilities ( < 0.15 ) categorized to all topics . We do such a split because all documents are assigned a partisanship label , but not all of them are topical . For the topical documents from which we will generate contextualized topic embeddings , we use them as the training data to finetune the language model during the training phase . As a result , train set has 30 , 571 documents and the validation set has 35 , 797 documents . The model is trained for 30 epochs and we pick the one with the best performance on validation set for the subsequent topic embedding generation . We train the model using Adam optimizer , with learning rate 1e - 5 and weight decay 5e - 4 . We use a batch size of 64 and train the model on 4 RTX 2080 GPUs . Each epoch takes about 10 minutes . The best validation F1 score on classifying partisanship is 91.3 .", "entities": [[141, 142, "MethodName", "Adam"], [142, 143, "HyperparameterName", "optimizer"], [145, 147, "HyperparameterName", "learning rate"], [151, 153, "MethodName", "weight decay"], [160, 162, "HyperparameterName", "batch size"], [184, 186, "MetricName", "F1 score"]]}
{"text": "To sample clips that were likely candidates , we trained a simple autoencoder for audio clips of single words synthesized using the Amazon Polly speech synthesis system . Treating the autoencoder 's low - dimensional latent space as a proxy for perceptual space , we searched for clips that travel through more of the space as the playback rate is slowed from 1.0\u00d7 to 0.6\u00d7. Intuitively , a longer path through encoder space should correspond to a more dramatic change in perception as the clip is slowed down ( Section 3 presents some data supporting this ) . Concretely , we computed a score S proportional to the length of the curve swept by the encoder E in latent space as the clip is slowed down , normalized by the straight - line distance traveled : that is , we define S ( c ) = 0.6\u00d7 r=1.0\u00d7 | | dE ( c , r ) /dr | | dr | | E ( c , 0.6\u00d7 ) \u2212E ( c , 1.0\u00d7 ) | | . Then , with probability proportional to e 0.2 S , we importancesampled 200 clips from the set of audio clips of the top 10 , 000 English words , each spoken by all 16 voices offered by Amazon Polly ( spanning American , British , Indian , Australian , and Welsh accents , and male and female voices ) . The distributions of S in the population and our sample is shown in Figure 2 . Autoencoder details Our autoencoder operates on one - second audio clips sampled at 22 , 050 Hz , which are converted to spectrograms with a window size of 256 and then flattened to vectors in R 90 , 000 . The encoder is a linear map to R 512 with ReLU activations , and the decoder is a linear map back to R 90 , 000 space with pointwise squaring . We used an Adam optimizer with lr=0.01 , training on a corpus of 16 , 000 clips ( randomly resampled to between 0.6x and 1.0x the original speed ) for 70 epochs with a batch size of 16 ( \u2248 8 hours on an AWS c5.4xlarge EC2 instance ) .", "entities": [[12, 13, "MethodName", "autoencoder"], [24, 26, "TaskName", "speech synthesis"], [30, 31, "MethodName", "autoencoder"], [253, 254, "MethodName", "Autoencoder"], [256, 257, "MethodName", "autoencoder"], [303, 304, "MethodName", "ReLU"], [327, 328, "MethodName", "Adam"], [328, 329, "HyperparameterName", "optimizer"], [358, 360, "HyperparameterName", "batch size"]]}
{"text": "To improve the robustness of our model , our team apply cross - validation for training . Firstly , by using different random seeds , we divided the training set which included all three languages ten times . Through this process , we obtained 10 folds of data , which contain 15768 training samples and 1751 validation samples in each fold . During the finetuning process , we used random search to optimize hyper - parameters like epochs , learning rate , and batch size . By using F1 - Score as our evaluation metric , the best model at all the ten - fold of training is saved . Finally , by making predictions on the test set , we save the mean of the probability of all ten best - saved models . This result is our final output of ERNIE - M. Cross - validation process is shown in Figure 1 .", "entities": [[23, 24, "DatasetName", "seeds"], [69, 71, "MethodName", "random search"], [79, 81, "HyperparameterName", "learning rate"], [83, 85, "HyperparameterName", "batch size"], [88, 91, "MetricName", "F1 - Score"]]}
{"text": "Predicting user intent and detecting the corresponding slots from text are two key problems in Natural Language Understanding ( NLU ) . Since annotated datasets are only available for a handful of languages , our work focuses particularly on a zero - shot scenario where the target language is unseen during training . In the context of zero - shot learning , this task is typically approached using representations from pre - trained multilingual language models such as mBERT or by fine - tuning on data automatically translated into the target language . We propose a novel method which augments monolingual source data using multilingual code - switching via random translations , to enhance generalizability of large multilingual language models when fine - tuning them for downstream tasks . Experiments on the Mul - tiATIS++ benchmark show that our method leads to an average improvement of +4.2 % in accuracy for the intent task and +1.8 % in F1 for the slot - filling task over the state - of - the - art across 8 typologically diverse languages . We also study the impact of code - switching into different families of languages on downstream performance . Furthermore , we present an application of our method for crisis informatics using a new human - annotated tweet dataset of slot filling in English and Haitian Creole , collected during the Haiti earthquake . 1", "entities": [[15, 18, "TaskName", "Natural Language Understanding"], [57, 61, "TaskName", "zero - shot learning"], [78, 79, "MethodName", "mBERT"], [149, 150, "MetricName", "accuracy"], [158, 159, "MetricName", "F1"], [219, 221, "TaskName", "slot filling"]]}
{"text": "Joint training is traditionally used for intent prediction and slot filling to exploit the correlation between the two tasks . This is done by feeding the feature vectors of one model to another or by sharing layers of a neural network followed by training the tasks together . So , a standard joint model loss can be defined as a combination of intent ( L i ) and slot ( L sl ) losses . i.e. , L = \u03b1L i + \u03b2L sl , where \u03b1 and \u03b2 are corresponding task weights . Prior works ( Goo et al , 2018 ; Schuster et al , 2019 ; Liu and Lane , 2016 ; Haihong et al , 2019 ) that use BiL - STM or RNN are now modified to BERT - based implementations explored in more recent works Hardalov et al , 2020 ; . A standard Joint model consists of BERT outputs from the final hidden state ( classification ( CLS ) token for intent and m word tokens for slots ) fed to linear layers to get intent and slot predictions . Assuming h cls represents the CLS token and h m represents a token from the remaining word - level tokens , the BERT model outputs are defined as : p i = sof tmax ( W i h cls + b i ) p sl m = sof tmax ( W sl hm + b sl ) \u2200m ( 1 ) with a multi - class cross - entropy loss 3 for both intent ( L i ) and slots ( L sl ) . We will use this model as 2 Each of the Sino - Tibetan , Koreanic , and Japonic families have a single high - resource member ( Chinese , Korean , Japanese respectively ) . We only group them as an additional interesting data point , not because we ascribe to any theories that link them typologically . 3 L = \u2212 1 n \u2211\ufe01 n i=1 [ y log \u0177 ] our baseline for joint training . Our goal will be to show that code - switching on top of joint training improves the performance . The output of Algorithm 1 will be the input used for joint training on BERT for code - switched experiments . 3 Datasets Benchmark Dataset . We use the latest multilingual benchmark dataset of MultiATIS++ , which was created by manually translating the original ATIS ( Price , 1990 ) dataset from English ( en ) to 8 other languages : Spanish ( es ) , Portuguese ( pt ) , German ( de ) , French ( fr ) , Chinese ( zh ) , Japanese ( ja ) , Hindi ( hi ) , and Turkish ( tr ) . The dataset consists of utterances for each language with an ' intent ' label for ' flight intent ' and ' slot ' labels for the word tokens in BIO format . A sample datapoint in English is shown in Figure 2 . model on the validation set . Consistent with the metrics reported for intent prediction and slot filling evaluation in the past , we also accuracy for intent and micro F1 5 to measure slot performance .", "entities": [[9, 11, "TaskName", "slot filling"], [54, 55, "MetricName", "loss"], [86, 87, "HyperparameterName", "\u03b1"], [88, 89, "HyperparameterName", "\u03b2"], [132, 133, "MethodName", "BERT"], [154, 155, "MethodName", "BERT"], [209, 210, "MethodName", "BERT"], [219, 220, "DatasetName", "sof"], [234, 235, "DatasetName", "sof"], [256, 257, "MetricName", "loss"], [383, 384, "MethodName", "BERT"], [413, 414, "DatasetName", "ATIS"], [529, 531, "TaskName", "slot filling"], [538, 539, "MetricName", "accuracy"], [542, 544, "MetricName", "micro F1"]]}
{"text": "Effect of Multilingual Code - Switching . Table 3 describes performance evaluation on the Multi - ATIS++ dataset . When compared to the state - ofthe - art jointly trained English - only baseline , we see a +4.2 % boost in intent accuracy and +1.8 % boost in slot F1 scores on average by augmenting the dataset via multilingual code - switching without requiring the target language . From the significance tests , except for Spanish and German , all other languages were helped by code - switching for intent detection . For slot filling , improvement on Portuguese and French is insignificant . This suggests that code - switching primarily helped languages that are morphologically more different from the source language ( English ) . For example , Hindi and Turkish have the highest intent performance improvement of +16.1 % and +9.8 % respectively . And for slots , Hindi and Chinese with +6.0 % and +4.3 % respectively . Japanese showed +4 % improvement for intent and +3.4 % for slots . The runtime of the models in Table 5 ( Appendix B ) shows that code - switching is expensive , taking up to five hours for five augmentation rounds ( k = 5 ) . This is because there are k times more data compared to the monolingual source data . Increasing the number of code - switchings ( k ) for a sentence from 5 to 50 improves the performance by +1 % , while increasing the run - time by a large margin . Hence , such tradeoffs should be considered when picking k for real - world applications where time to deployment might be of the essense . In the translate - train ( upper bound ) scenario , it is not immediately clear if augmentation helps , since data in the same language as the target are always preferable to other language or code - switched data . At a minimum , augmentation does not hinder upper - bound performance ( Table 3 ) . For both intent and slot performance , the chunklevel model remained robust across the languages . For intent , the difference between word - level and sentence - level was insignificant . For slot , sentencelevel was in par with chunk - level on average . Thus , we think that code - switching at chunk - level is safer for avoiding semantic discrepancies ( which are a danger in the word - level ) while also better encouraging intra - sentence language neutrality . Evaluation on Disaster Dataset . We found that disaster data is more challenging than the ATIS dataset for transfer learning in NLU . The predictive performance is shown in Table 4 . Code - Switching improved intent accuracy by +12.5 % and slot F1 by +2.3 % , which is quite promising considering the domain mismatch ( tweets vs airline guides ) . Joint training added +0.9 % improvement to intent accuracy , however did not seem to help slot F1 . This might imply a weaker correlation between the two tasks in real - world data , i.e. a mention of ' food ' or ' shelter ' in a tweet may not always mean that there is a ' request ' or vice - versa . The upper bound of translate - train method did not perform any better than the randomly code - switched model which seemed counter - intuitive . This might be due to the lack of strong representation for Haitian Creole in the pre - trained model , although it is similar to French , or due to the limitation of the machine translation system . Impact of Language Families . Results of language family analysis are shown in Figure 3 for the 4 languages that showed significant improvements for both intent and slots in Table 3 . The input in English is independently code - switched using 6 different language families . Note that the target language is always excluded from the group when evaluating on the same , i.e. Hindi is excluded from Indo - Aryan family when that family is being evaluated on it . Translate - train model is provided as a frame of reference and upper bound . Generally , as expected , we found that language families helped their corresponding languages , i.e. Romance helped Spanish , Germanic helped German , and so on . An exception is our loose group of Sino - Tibetan , Koreanic , and Japonic languages - for both Chinese and Japanese , languages from the Turkic language family helped more than others . On the other hand , the Sino - tibetan , Japonic , and Koreanic group helps Hindi more than other Indo - European languages . We believe this highlights the necessity for methods like the one of Xia et al ( 2020 ) that can a priori identify the best helper language or group of languages that can benefit downstream tasks for low resource languages . Control Experiments on k. Hyperparameter k controls the amount of code - switched data . k = 0 represents original size with no code - switching , k = 1 represents original size with code - switching , and k = 10 means 10 - times more code - switched data than the original . The main experiments in Table 3 use k = 5 . Figure 4 shows how varying k affects performance . For this analysis , we consider 4 target languages on which code - switching produced significant results in Table 3 on both Intent Accuracy and Slot F1 : Chinese , Japanese , Hindi , and Turkish . Intuitively , we observe that as k increases , too much code - switching becomes expensive in terms of runtime , while performance improvement slowly plateaus . For Slot F1 performance in all four cases , unlike Intent , we observe an interesting dip when k = 1 , which represents the augmentation having just one copy of codeswitching ( without the original non - code - switched data ) , as compared to k = 0 . Adding the original data to one round of code - switched data ( k = 2 ) leads to big improvements . Overall , we see improvement for both tasks , with Slot F1 plateauing earlier . Table 5 and Figure 10 in Appendix B show the impact of code - switching on training runtime , which increases as k increases . Thus , finding an optimal value of k and specific language groups are essential for downstream applications . mBERT versus XLM - R. Additional performance evaluations and benefits of code - switching on XLM - R ( Conneau et al , 2020a ) , a stronger multilingual language model , are provided in Appendix A. Note that XLM - R is trained using Common - Crawl and is likely to be exposed to some code - switched data . Thus , we focus primarily on mBERT which largely remains monolingual at the sentence - level to identify the unbiased impact of code - switching during fine - tuning . Furthermore , runtime and hyperparameter tuning along with insights into layers to freeze before training are shown in Appendix B. Error Analysis . Selecting intent classes with support > 10 , Figure 5 shows how each class is positively or negatively impacted by code - switching . Improvement was primarily on ' airfare ' , ' distance ' ' capacity ' , ' airline ' , and ' ground_service ' which had longer sentences such as ' Please tell me which airline has the most departures from Atlanta ' when compared to ' abbreviations ' and ' airport ' classes that included very short phrases like ' What does EA mean ? ' However , note that Spanish and German did not improve much , aligning with our results in Table 3 . For slot labels in Figure 6 , we selected the ones with support > 50 and that have different characteristics , e.g. ' name ' , ' code ' , etc . The overall trend in slot performance shows improvements for labels such as ' day_name ' , ' airport_code ' , and ' city_name ' and slight variations in labels such as ' fight_number ' and ' period_of_day ' , implying textual slots benefiting over numeric ones .", "entities": [[43, 44, "MetricName", "accuracy"], [50, 51, "MetricName", "F1"], [90, 92, "TaskName", "intent detection"], [94, 96, "TaskName", "slot filling"], [205, 207, "HyperparameterName", "k ="], [431, 432, "DatasetName", "Disaster"], [444, 445, "DatasetName", "ATIS"], [447, 449, "TaskName", "transfer learning"], [466, 467, "MetricName", "accuracy"], [472, 473, "MetricName", "F1"], [500, 501, "MetricName", "accuracy"], [509, 510, "MetricName", "F1"], [617, 619, "TaskName", "machine translation"], [861, 863, "HyperparameterName", "k ="], [863, 864, "DatasetName", "0"], [873, 875, "HyperparameterName", "k ="], [885, 887, "HyperparameterName", "k ="], [908, 910, "HyperparameterName", "k ="], [944, 945, "MetricName", "Accuracy"], [947, 948, "MetricName", "F1"], [987, 988, "MetricName", "F1"], [1003, 1005, "HyperparameterName", "k ="], [1032, 1034, "HyperparameterName", "k ="], [1034, 1035, "DatasetName", "0"], [1049, 1051, "HyperparameterName", "k ="], [1069, 1070, "MetricName", "F1"], [1116, 1117, "MethodName", "mBERT"], [1118, 1119, "MethodName", "XLM"], [1131, 1132, "MethodName", "XLM"], [1155, 1156, "MethodName", "XLM"], [1183, 1184, "MethodName", "mBERT"], [1227, 1228, "MetricName", "Error"]]}
{"text": "We conduct an additional analysis on XLM - R ( Conneau et al , 2020a ) and compare it with mBERT ( Devlin et al , 2019 ) . The implementation is very similar in PyTorch ( Paszke et al , 2019 ) but using the pre - trained xlm - roberta - base with RobertaForSequenceClassification ( Wolf et al , 2020 ) as the XLM - R model . We observe that , setting k = 5 , XLM - R outperforms mBERT on average ( by 2 % Intent Accuracy and 1.5 % Slot F1 ) . Individually , XLM - R improved Chinese , Japanese , Portuguese , and Turkish for Intent Prediction and German , Chinese , Japanese , Portuguese , and Hindi for Slot Filling as shown in Figure 7 . We observe a trend similar to mBERT with k on XLM - R shown in Figure 8 . However , for XLM - R , we observe that randomized code - switching did not help Chinese for Intent Prediction and Hindi for Slot F1 . If codeswitched to a specific language family , instead of switching to random languages , it might improve their performance . A deeper dive into XLM - R and language families are left for future work .", "entities": [[6, 7, "MethodName", "XLM"], [20, 21, "MethodName", "mBERT"], [49, 50, "MethodName", "xlm"], [65, 66, "MethodName", "XLM"], [75, 77, "HyperparameterName", "k ="], [79, 80, "MethodName", "XLM"], [83, 84, "MethodName", "mBERT"], [91, 92, "MetricName", "Accuracy"], [96, 97, "MetricName", "F1"], [101, 102, "MethodName", "XLM"], [128, 130, "TaskName", "Slot Filling"], [142, 143, "MethodName", "mBERT"], [146, 147, "MethodName", "XLM"], [157, 158, "MethodName", "XLM"], [179, 180, "MetricName", "F1"], [206, 207, "MethodName", "XLM"]]}
{"text": "For joint training with same task weights , we tuned \u03b1 and \u03b2 using grid search to see the strength of correlation between the tasks . For intent , the ( \u03b1 , \u03b2 ) combination of ( 1.0 , 0.6 ) performed well , while ( 1.0 , 1.0 ) for slots . This suggests that intent benefiting slot might be slightly more than slot benefiting intent . Additionally , during fine - tuning , freezing the layers of the transformer affected the model performance as shown in Figure 9 . Keeping the first 8 layers frozen gave the best performance . By freezing the earlier layers , the transformer can retain its most fundamental feature information gained from the massive pre - training step , and by unfreezing some top layers , it can undergo fine - tuning . Additionally , latency for training a code - switched model is shown in Table 5 and how runtime varies with an increase in k is shown in Figure 10 .", "entities": [[10, 11, "HyperparameterName", "\u03b1"], [12, 13, "HyperparameterName", "\u03b2"], [31, 32, "HyperparameterName", "\u03b1"], [33, 34, "HyperparameterName", "\u03b2"]]}
{"text": "Document - level information extraction ( IE ) tasks have recently begun to be revisited in earnest using the end - to - end neural network techniques that have been successful on their sentence - level IE counterparts . Evaluation of the approaches , however , has been limited in a number of dimensions . In particular , the precision / recall / F1 scores typically reported provide few insights on the range of errors the models make . We build on the work of Kummerfeld and Klein ( 2013 ) to propose a transformation - based framework for automating error analysis in document - level event and ( N - ary ) relation extraction . We employ our framework to compare two state - of - theart document - level template - filling approaches on datasets from three domains ; and then , to gauge progress in IE since its inception 30 years ago , vs. four systems from the MUC - 4 ( 1992 ) evaluation . 1", "entities": [[63, 64, "MetricName", "F1"], [113, 115, "TaskName", "relation extraction"]]}
{"text": "Although information extraction ( IE ) research has almost uniformly focused on sentence - level relation and event extraction ( Grishman , 2019 ) , the earliest research in the area formulated the task at the document level . Consider , for example , the first large - scale ( for the time ) evaluations of IE systems - e.g. MUC - 3 ( 1991 ) and MUC - 4 ( 1992 ) . Each involved a complex document - level event extraction task : there were 24 types of events , over a dozen event arguments ( or roles ) to be identified for each event ; documents could contain zero to tens of events , and extracting argument entities ( or role fillers ) required noun phrase coreference resolution to ensure interpretability for the end - user ( e.g. to ensure that multiple distinct mentions of the same entity in the output were not misinterpreted as references to distinct entities ) . The task was challenging : information relevant for a single event could be scattered across the document or repeated in multiple places ; relevant information might need to be shared across multiple events ; information regarding different events could be intermingled . In Figure 1 , for example , the DISEASE \" Newcastle \" is mentioned well before its associated event is mentioned ( via the triggering phrase \" the disease has killed \" ) ; that same mention of \" Newcastle \" must again be recognized as the DISEASE in a second event ; and the COUNTRY of the first event ( \" Honduras \" ) appears only in the sentence describing the second event . In fact , the problem of document - level information extraction has only recently begun to be revisited ( Quirk and Poon , 2017 ; Jain et al , 2020 ; Du et al , 2021b , a ; Li et al , 2021 ; Du , 2021 ; Yang et al , 2021 ) in part in an attempt to test the power of end - to - end neural network techniques that have been so successful on their sentence - level counterparts . 2 Evaluation , however , has been limited in a number of ways . First , despite the relative complexity of the task , approaches are only evaluated with respect to their overall performance scores ( e.g. precision , recall , and F1 ) . Even though scores at the role level are sometimes included , no systematic analysis or characterization of the types of errors that occur is typically done . The latter is needed to determine strategies to improve performance , to obtain more informative cross - system and cross - genre comparisons , and to identify and track broader advances in the field as the underlying approaches evolve . To date , for example , there has been no attempt to directly compare the error landscape and distribution of [ Trigger ] [ Trigger ] Input Document", "entities": [[17, 19, "TaskName", "event extraction"], [78, 83, "TaskName", "document - level event extraction"], [129, 131, "TaskName", "coreference resolution"], [408, 409, "MetricName", "F1"]]}
{"text": "As in Jurafsky and Martin ( 2021 ) , we will refer to document - level information extraction tasks as template - filling tasks and use the term going forward to refer to both event extraction and documentlevel relation extraction tasks . Given a document , D , and an IE template specification consisting of a predetermined list of roles R 1 , R 2 , ... , R i associated with each type of relevant event for the task of interest , the goal for template filling is to extract from D , one output template , T for every relevant event / relation e 1 , e 2 , . . . , e n present in the document . Notably , in the general case , n \u2265 0 and is not specified as part of the input . In each output template , its roles are filled with the corresponding role filler ( s ) , which can be inferred or extracted from the document depending on the predetermined role types . We consider two role types here : 5 Set - fill roles , which must be filled with exactly one role filler from a finite set supplied in the template specification . An example of a set - fill role in Figure 1 is STATUS , which can be confirmed , possible , or suspected . String - fill roles , whose role filler ( s ) are spans extracted from the document , or left empty if no corresponding role filler is found in the document . VICTIMS , DISEASE and COUNTRY are string - fill roles in Figure 1 . Some string - fill roles allow multiple fillers ; for example , there might be more than one VICTIMS . Importantly , for document - level template filling , exactly one string should be included for each role filler entity ( typically a canonical mention of the entity ) , i.e. coreferent mentions of the same entity are not permitted . Evaluation . We use the standard ( exact - match ) F1 score ( Chinchor , 1991 ) to evaluate the output 5 There are potentially more role types depending on the dataset ( e.g. normalized dates , times , locations ) ; we will not consider those here . produced by a template - filling system : F 1 = 2 Precision Recall Precision + Recall", "entities": [[34, 36, "TaskName", "event extraction"], [38, 40, "TaskName", "relation extraction"], [131, 132, "DatasetName", "0"], [350, 352, "MetricName", "F1 score"], [401, 402, "MetricName", "Precision"], [402, 403, "MetricName", "Recall"], [403, 404, "MetricName", "Precision"], [405, 406, "MetricName", "Recall"]]}
{"text": "The first stage of the error analysis tool involves matching each system - predicted template to the best - matching gold template for each document in the dataset . In particular , the overall F1 score for a given document can vary based on how a predicted template is individually matched with a gold template ( or left unmatched ) . Specifically , for each document , we recursively generate all possible template matchings - where each predicted template is matched ( if possible ) to a gold template . In particular , for a document with P predicted templates and G gold templates , the total number of possible template matchings is : Note that template matching can result in unmatched predicted templates ( Spurious Templates ) , as well as unmatched gold templates ( Missing Templates ) . 1 + P 1 G + P 2 G ( G \u2212 1 ) + ... + G ! ( G \u2212 P ) ! , if G \u2212 P \u2265 0 1 + P 1 G + P 2 G ( G \u2212 1 ) + ... + P G G ! , if G \u2212 P < 0 = min ( P , G ) i=0 P i G ! ( G \u2212 i ) ! Next , for each predicted - gold pair in a template matching , we iterate through all its roles and recursively generate all possible mention matchings , in each of which a predicted role filler is matched ( if possible ) to a set of coreferent gold role fillers . Similar to template matching , the process of mention matching can also result in unmatched predicted role fillers ( Spurious Role Fillers ) and unmatched coreferent sets of gold role fillers ( Missing Role Fillers ) . Through the process , each predicted role filler increases the denominator of the total precision by 1 , and each set of coreferent gold role fillers increases the denominator of total recall by 1 . Whenever there is a matched mention pair in which the predicted role filler has an exact match to an element of the set of coreferent gold role fillers , this adds 1 to the numerator of both precision and recall . These counts are calculated for each template matching . Using precision and recall , the total F1 score across all the slots / roles is calculated and the template matching with the highest total F1 score is chosen . If there are ties , the template matching with the fewest errors is chosen ( see Section 4.3 ) .", "entities": [[34, 36, "MetricName", "F1 score"], [171, 172, "DatasetName", "0"], [199, 200, "DatasetName", "0"], [355, 357, "MetricName", "exact match"], [397, 399, "MetricName", "F1 score"], [415, 417, "MetricName", "F1 score"]]}
{"text": "The second part of the error analysis tool involves changing the predicted templates to the desired gold templates with the help of a fixed set of transformations as detailed below . Alter Span transforms a role filler into the gold role filler with the lowest span comparison score ( SCS ) . The tool provides two options for computing the SCS between two spans , and each depends only on the starting and ending indices of the spans . 6 SCS can be interpreted as distance and is 0 between two identical spans , and 1 for non - overlapping spans . The two modes are given as follows : a ) absolute : This mode captures the ( positive ) distance between the starting ( and ending ) character offsets of spans x and y in the document , and scales that value by the sum of the lengths of x and y , capping it at a maximum of 1 . SCS = max 1 , | xstart\u2212ystart | + | x end \u2212y end | length ( x ) + length ( y ) b ) geometric mean : This mode captures the degree of disjointedness between spans x and y by dividing the length of the overlap between the two spans with respect to each of their lengths , multiplying those two fractions , and subtracting the final result from 1 . If si is the length of the intersection of x and y , and neither x nor y have length 0 , SCS is calculated as shown below ; otherwise , SCS is 1 . overlap = min ( x end , y end ) \u2212 max ( x start , y start ) si = max ( 0 , overlap ) SCS = 1 \u2212 si 2 length ( x ) * length ( y ) Thus , if the predicted role filler is an exact match for the gold role filler , the SCS is 0 . If there is some overlap between the spans , the SCS is between 0 and 1 ( not inclusive ) , and if there is no overlap between the spans , the SCS is 1 . The order of comparison of the spans does n't change the SCS score for both modes . As the absolute mode is less sensitive to changes in span indices as compared to the geometric mean , we chose geometric mean for our analysis , as tiny changes in index positions result in a bigger change in the SCS score . Alter Role changes the role of a role filler to another role within the same template . Remove Duplicate Role Filler removes a role filler that is coreferent to an already matched role filler . Remove Cross Template Spurious Role Filler removes a role filler that would be correct if present in another template ( in the same role ) . Remove Spurious Role Filler removes a role filler that has not been mentioned in any of the gold templates for a given document . Introduce Role Filler introduces a role filler that was not present in the predicted template but was required to be present in the matching gold template . Remove Template removes a predicted template that could not be matched to any gold template for a given document . Introduce Template introduces a template that can be matched to an unmatched gold template for a given document . For a given document , all singleton Alter Span and Alter Role transformations , as well as sets of Alter Span + Alter Role transformations , are applied first . The other transformations are applied in the order in which they were detected , which is dependent on the order of predicted and gold template pairs in the optimized matching and the order of the slots / roles in the template .", "entities": [[88, 89, "DatasetName", "0"], [256, 257, "DatasetName", "0"], [294, 295, "DatasetName", "0"], [322, 324, "MetricName", "exact match"], [333, 334, "DatasetName", "0"], [348, 349, "DatasetName", "0"]]}
{"text": "Table 2 shows the results of evaluating DyGIE++ and GTT on the SciREX , ProMED , and MUC - 4 datasets . We can see that all models perform substantially worse on scientific texts ( ProMED , SciREX ) as compared to news ( MUC - 4 ) , likely because the model base is pretrained for generalpurpose NLP applications ( BERT ) or there are not enough examples of scientific - style text in the pretraining corpus ( SciBERT ) . In addition , models seem to perform better on the news - style ProMED dataset than the scientific - paper - based long - text SciREX dataset . This can be explained by the fact that all four models handle a maximum of 512 tokens as inputs , while the average length of a SciREX document is 5401 tokens . Thus , a majority of the text is truncated and , hence , unavailable to the models . Nevertheless , we see an increase in F1 scores for all SciBERT - based models when compared to their BERT counterparts for the SciREX dataset . The same trend is seen for DyGIE++ for ProMED , but not for GTT . This can be explained by the fact that GTT ( SciBERT ) has more Missing Template errors than GTT ( BERT ) . So even if GTT ( SciBERT ) performs better on the scientific slot VICTIMS , i.e. it extracts more scientific information , it does not identify relevant events as well as GTT ( BERT ) , reducing the F1 score across the remaining slots . From the error count results in Figure 4 , we see that GTT makes fewer Missing Template errors than DyGIE++ on the MUC - 4 dataset ( 86 vs. 97 ) . However , there is no significant difference in the number of missing templates between the two models on the ProMED and SciREX datasets . This could be because DyGIE++ is prone to overgeneration - there are significantly more Spurious Role Filler and Spurious Template errors as compared to the results of GTT . Since we use heuristics that create templates based on the extracted role fillers , this increases the probability that there was a possible match to a gold template , reducing the number of Missing Template Errors . We can also conclude that DyGIE++ is worse at coreference resolution when compared to GTT as DyGIE++ makes more Duplicate Role Filler errors across all datasets . Overall , we find that the major source of error for both GTT and DyGIE++ across all the datasets is missing recall in the form of Missing Role Filler and Missing Template errors .", "entities": [[12, 13, "DatasetName", "SciREX"], [37, 38, "DatasetName", "SciREX"], [61, 62, "MethodName", "BERT"], [107, 108, "DatasetName", "SciREX"], [136, 137, "DatasetName", "SciREX"], [167, 168, "MetricName", "F1"], [179, 180, "MethodName", "BERT"], [183, 184, "DatasetName", "SciREX"], [221, 222, "MethodName", "BERT"], [257, 258, "MethodName", "BERT"], [262, 264, "MetricName", "F1 score"], [322, 323, "DatasetName", "SciREX"], [400, 402, "TaskName", "coreference resolution"]]}
{"text": "Table 3 presents the precision , recall , and F1 performance on the MUC - 4 dataset for early models from 1992 alongside those of the more recent DyGIE++ and GTT models . We summarize key findings below . The best of the early models ( GE NLToolset ) performs better than either of the modern models . It does so by doing a better job balancing precision and recall , whereas GTT and DyGIE++ exhibit much higher precision and much lower recall . Table 4 : Span Errors in early models . The differences between the predicted mention and its best gold mention match according to our analysis tool are in bold . The early models have more span errors than the modern DyGIE++ and GTT models . The representative kinds of span errors from the 1992 model outputs are shown in Table 4 . One interesting difference between the span errors in the early models and the modern models is that the predicted mentions include longer spans with more information than is indicated in the best gold mention match . Some could be due to errors in dataset annotation ; for example , maoist shining path group versus shining path but a significant number of the span errors occur as the early models seem to extract the entire sentence or clause which contains the desired role filler mention . The modern models tend to leave off parts of the desired spans , and if they do predict larger spans than required , are only off by a few words . The early models have fewer Missing Template and Missing Role Filler errors as compared to the modern models . However , the former also have more Spurious Template and Spurious Role Filler errors than the latter , indicating these models mitigate the issue of Missing Templates through over - generation . The early models have fewer Incorrect Role errors as compared to modern models . However , since all the models make relatively few such errors , it suggests that role classification for predicted mentions is not a major problem for modern models . The main source of error for both early and modern models is missing recall due to missing templates and missing role fillers . This strongly suggests future systems can maximize their performance by being less conservative in role filler detection and focusing on improvement of the recall , even at the expense of potentially decreasing some precision .", "entities": [[9, 10, "MetricName", "F1"]]}
{"text": "This work explores subtypes of Spurious Role Filler errors extensively , however , we would like to further analyze Missing Role Filler and templatelevel errors for more fine - grained error subtypes and the linguistic reasons behind why they occur . Due to the pairwise comparisons between all predicted and gold mentions in a role for all pairs of predicted and gold templates in an example , the error analysis tool is slow when the number of both the predicted and gold templates as well as the number of role fillers in the templates is high . Thus , we would also like to improve the time complexity of our template ( and mention ) matching algorithms using an approach like bipartite matching ( Yang et al , 2021 ) . Currently , the error analysis tool reports exact match precision / recall / F1 which is more suitable for string - fill roles . We would like to extend the tool to further analyze set - fill roles by implementing metrics such as false - positive rate . We used a limited number of models in this paper as we aimed to develop and test the usability of our error analysis tool . In the future , however , we would like to test our tool on a wider range of models , in addition to running more experiments in order to reach more generalizable conclusions about the behavior of IE models .", "entities": [[138, 140, "MetricName", "exact match"], [144, 145, "MetricName", "F1"]]}
{"text": "The specific list of transformations applied in the error correction process . ( 1 ) Span Error . Each singleton Alter Span transformation is mapped to a Span Error . A Span Error occurs when a predicted role filler becomes an exact match to the a gold role filer only upon span alteration . ( 2 ) Duplicate Role Filler . Each singleton Remove Duplicate Role Filler transformation is mapped to a Duplicate Role Filler error . A Duplicate Role Filler error occurs when a spurious role filler is coreferent to an already matched role filler and is treated as a separate entity . This happens when the system fails at coreference resolution . ( 3 ) Duplicate Partially Matched Role Filler ( Spurious ) . Same as ( 2 ) above , but with an added Alter Span transformation applied first to account for partial matching . This happens when the system fails at correct span extraction and coreference resolution . ( 4 ) Spurious Role Filler . Each singleton Remove Spurious Role Filler transformation is mapped to a Spurious Role Filler error . A Spurious Role Filler error occurs when a mention is extracted from the text with no connection to the gold templates . ( 5 ) Missing Role Filler . Each singleton Introduce Role Filler transformation is mapped to a Missing Role Filler error . A Missing Role Filler error occurs when a role filler is present in the gold template but not the predicted template for a given role . ( 6 ) Incorrect Role . Each singleton Alter Role transformation is mapped to an Incorrect Role . An Incorrect Role occurs when a spurious role filler is assigned to the incorrect role within the same template , i.e. the role filler would have been correct if present filled in another slot / role in the same template . This happens when the system fails at correct role assignment . ( 7 ) Incorrect Role + Partially Matched Filler . Same as ( 4 ) above , but with an added Alter Span transformation applied first to account for partial matching . This happens when the system fails at correct span extraction and role assignment . ( 8 ) Wrong Template for Role Filler . Each singleton Remove Cross Template Spurious Role Filler transformation is mapped to a Wrong Template for Role Filler error . A Wrong Template for Role Filler occurs when a spurious role filler in one template can be assigned to the correct role in another template , i.e. it would be correct if it had been placed in another template . This happens when the system fails at correct event assignment . ( 9 ) Wrong Template for Partially Matched Role Filler . Same as ( 6 ) above , but with an added Alter Span transformation applied first to account for partial matching . This happens when the system fails at correct span extraction and event assignment . ( 10 ) Wrong Template + Wrong Role . An Alter Role and Remove Cross Template Spurious Role Filler transformation are applied to the same predicted role filler in that order to be mapped to a Wrong Template + Wrong Role error . A Wrong Template + Wrong Role error occurs when a spurious role filler can be assigned to another role in another template . This happens when the system fails at correct role assignment and event assignment . ( 11 ) Wrong Template + Wrong Role + Partially Matched Filler . Same as ( 8 ) above , but with an added Alter Span transformation applied first to account for partial matching . This happens when the system fails at correct span extraction , role assignment and event assignment . (", "entities": [[16, 17, "MetricName", "Error"], [28, 29, "MetricName", "Error"], [32, 33, "MetricName", "Error"], [41, 43, "MetricName", "exact match"], [111, 113, "TaskName", "coreference resolution"], [159, 161, "TaskName", "coreference resolution"]]}
{"text": "Quality assurance for data . Nuanced understanding of data is requisite for drawing sound scientific conclusions . In particular , without evaluating for the quality and accuracy of data used to test models , it is impossible to be certain that progress is being made and that successive iterations of models truly make progress on the underlying task or linguistic phenomena of interest . Within NLP , iconic datasets such as the Penn Treebank ( Marcus et al , 1993 ) have sustained subareas such as language modelling , part - of - speech tagging , and syntactic parsing for years due to the painstaking annotation efforts put into making these high - fidelity resources . And in the context of summarization , initial datasets , such as those produced during the Document Understanding Conference ( DUC ) and Text Analysis Conference ( TAC ) evaluations , implemented fine - grained verification of data quality . 2 In part due to the emergence of data - hungry modelling techniques , the demands for larger datasets often render quality assurance procedures of this standard to be impractical and infeasible . Nonetheless , several recent natural language understanding datasets ( Bowman et al , 2015 ; Rajpurkar et al , 2016 ; Suhr et al , 2017 ) institute explicit qualitycontrol procedures in crowd - sourcing dataset construction ( Zaidan and Callison - Burch , 2011 ; Yan et al , 2014 ; Callison - Burch et al , 2015 ) , such as using additional annotators to validate annotations ( c.f . Geva et al , 2019 ) . In the sibling subfield of machine translation , which often shares similar modelling challenges and evaluation regimes as summarization due to the shared nature of being sequence - to - sequence natural language generation tasks , the annual WMT conference 3 consistently furnishes high quality data . In summary , ensuring data quality is both crucial and challenging . And in comparison with other subareas of NLP , we argue that summarization has lagged behind in rigorously ensuring the quality of widely - used datasets . Relating data quality and model quality . The correctness and quality of data inherently bounds what can be learned from the data about the task of interest . From an information - theoretic perspective , this can be made fully formal as follows : For fully learning - based methods , especially those with weak / minimal inductive biases such as neural networks , I ( S ; A ) is approximately zero . While I ( S ; P ) may be greater than zero ( e.g. language modelling pretraining provides statistical information that may facilitate a model to avoid a priori unlikely summaries ) , standard pretraining regimes such as large - scale language modelling over generic text corpora ( Devlin et al , 2019 ; Raffel et al , 2019 ) are likely insufficient to meaningfully learn to summarize . Under these assumptions , the mutual information between S and M is critically upper - bounded in terms of I ( S ; T ) . We hypothesize that the quality of the training dataset T is highly correlated with its mutual information with respect to the summarization task S , I ( S ; T ) . One size does not fit all . Sp\u00e4rck Jones ( 1999 ) famously argued that summarization systems should be understood conditional on the context in which they will be used . In recent years , the field has significantly departed from this perspective and primarily studied \" general - purpose summarization \" ( Kryscinski et al , 2019 ) , which she denounced as ignis fatuus . With our work , we adopt the perspective that for all datasets it is strictly preferable to have all properties quantified ; it is the responsibility of practitioners building summarization systems to accurately weight different metrics based on their ultimate goals and use cases . As such , we refrain from providing prescriptive domain - agnostic or context - agnostic notions of summarization .", "entities": [[26, 27, "MetricName", "accuracy"], [72, 74, "DatasetName", "Penn Treebank"], [86, 88, "TaskName", "language modelling"], [89, 95, "TaskName", "part - of - speech tagging"], [121, 122, "TaskName", "summarization"], [193, 196, "TaskName", "natural language understanding"], [235, 238, "DatasetName", "Yan et al"], [273, 275, "TaskName", "machine translation"], [286, 287, "TaskName", "summarization"], [339, 340, "TaskName", "summarization"], [442, 444, "TaskName", "language modelling"], [469, 471, "TaskName", "language modelling"], [544, 545, "TaskName", "summarization"], [570, 571, "TaskName", "summarization"], [605, 606, "TaskName", "summarization"], [651, 652, "TaskName", "summarization"], [684, 685, "TaskName", "summarization"]]}
{"text": "In this work , we evaluate the quality of a dataset by aggregating scores for each example in the dataset . We conjecture that for many NLP tasks , estimating the quality of a particular data example is of similar complexity as correctly performing the task on the example . 5 Nevertheless , for summarization , our insight is that various aspects of a summarization example ( a document - summary pair ) can be reliably estimated by re - purposing existing NLP methods . We are guided by pioneering work ( Luhn , 1958 ; Edmundson , 1969 ; Mani , 1999 ) that defined core properties of summarization systems and influential sub - sequent work ( Radev et al , 2002 ; Nenkova , 2006 ; Nenkova and McKeown , 2012 ; Peyrard , 2019a ) that refined and extended these properties . From this literature , we specifically study compression , topic similarity , abstractivity , redundancy , and semantic coherence as these properties are of recurring and sustained interest . 6 For each abstract property , numerous concrete methods can be proposed to quantify it . In Appendix A , we describe alternatives we considered and detail how we decided which methods performed best . We restrict discussion to the bestperforming approaches in the main paper . Notation . Our metrics will assume indexed sets D , S such that summary S i S summarizes document D i D. The length in words of a sequence s is | s | and the length in sentences is s . Each metric assigns a value x [ 0 , 1 ] to every ( D i , S i ) where 1 is the maximal score and example - level scores are averaged to yield a dataset - level score . Compression . We quantify compression at the word ( w ) and sentence ( s ) levels : CMP w ( D i , S i ) = 1 \u2212 | S i | | D i | ( 1 ) CMP s ( D i , S i ) = 1 \u2212 S i D i ( 2 ) Topic Similarity . We learn a topic model M on training corpus T with k topics using LDA ( Blei et al , 2003 ) and quantify topic similarity by comparing the inferred topic distributions \u03b8 D i | M , \u03b8 S i | M for a given summary and document : TS ( D i , S i ) = 1 \u2212 JS ( \u03b8 D i | M , \u03b8 S i | M ) ( 3 ) where JS is the Jensen - Shannon distance . We set k = 20 and T = D. Abstractivity . Grusky et al ( 2018 ) introduced fragments F ( D i , S i ) , which are greedily - matched spans shared between D i and S i . We quantify abstractivity as a normalized function of the aggregate fragment length ; our definition generalizes the definition of Grusky et al ( 2018 ) . We set p = 1 . ABS p ( D i , S i ) = 1 \u2212 f F ( D i , S i ) | f | p | S i | p ( 4 ) Redundancy . ROUGE ( Lin , 2004 ) implicitly penalizes redundancy but underestimates its detrimental impacts ( Chaganty et al , 2018 ) . However , we find that ROUGE is effective for identifying redundancy given the definitional focus on overlapping spans . We quantify redundancy as the average ROUGE - L 6 Different names and interpretations have been given for these properties in the literature . We revisit this in Appendix A in discussing alternate metrics . F - score for all pairs of distinct sentences in the summary . RED ( S i ) = mean ( x , y ) S i \u00d7S i , x = y ROUGE ( x , y ) ( 5 ) Semantic Coherence . We evaluate the semantic coherence of multi - sentence summaries by predicting the probability of each successive sentence conditioned on the previous one using a powerful language model , BERT ( Devlin et al , 2019 ) , pretrained with precisely this objective . SC ( S i ) = | | S | | j=2 1 BERT ( S j i | S j\u22121 i ) | | S i | | \u2212 1 ( 6 ) 4 Data We study the following 10 summarization datasets that have been frequently used in recent years . 7 Table 1 contains standard dataset statistics in the upper half and our aspect - level scores in the lower half ; datasets are grouped by domain . CNN - DM ( Hermann et al , 2015 ; Nallapati et al , 2016 ) is a dataset composed of CNN and Daily Mail news articles with summaries that are a concatenated list of highlight bullet points . NYT ( Sandhaus , 2008 ) is a dataset of curated New York Times articles paired with abstracts written by library scientists . NWS ( Grusky et al , 2018 ) is the Newsroom dataset of news articles drawn from 38 top English publishers paired with multi - sentence summaries written by the original authors and editors . GW ( Graff and Cieri , 2003 ) is the Gigaword headline generation dataset that some refer to as a summarization dataset ( Rush et al , 2015 ; Chopra et al , 2016 ) . Examples in the dataset are drawn from seven news sources and are the article prefix paired with its headline . XSum ( Narayan et al , 2018 ) is an extreme summarization dataset where BBC articles are paired with single - sentence summaries written generally by the author of the article that tries to motivate the BBC audience to read the article by answering \" What is the article about ? \" . PeerRead ( Kang et al , 2018 ) is a dataset of paper drafts from top - tier computer science venues as well as arXiv . 8 Consistent with its use in the summarization community , we consider the full introduction to be the source document and the ab - stract to be the target summary . PubMed ( Cohan et al , 2018 ) is a dataset of papers drawn from the biomedical and life sciences . Unlike PeerRead , the full paper is taken as the document but the summary is still specified as the abstract . TL ; DR ( V\u00f6lske et al , 2017 ) is a dataset of userwritten articles from the social media platform Reddit along with the author - provided courtesy summaries that tend to be multi - sentence . V\u00f6lske et al ( 2017 ) applied a series of preprocessing procedures to filter out bot - generated content . AMI ( Carletta et al , 2005 ) is a dataset of transcribed meetings , some which are naturally occurring and the rest of which are elicited , with handannotated summaries . The transcription process has multiple steps that are described extensively by Carletta et al ( 2005 ) . Various additional data provided within the AMI dataset is neglected in this work . MovieScript ( Gorinski and Lapata , 2015 ) is a dataset of movie scripts drawn from the Script - Base corpus that are aligned with user - written summaries sourced either from Wikipedia or IMDB . Various additional data provided within the Movi - eScript dataset is neglected in this work .", "entities": [[54, 55, "TaskName", "summarization"], [64, 65, "TaskName", "summarization"], [109, 110, "TaskName", "summarization"], [270, 271, "DatasetName", "0"], [381, 382, "MethodName", "LDA"], [399, 400, "HyperparameterName", "\u03b8"], [405, 406, "HyperparameterName", "\u03b8"], [417, 418, "MethodName", "TS"], [430, 431, "HyperparameterName", "\u03b8"], [436, 437, "HyperparameterName", "\u03b8"], [456, 458, "HyperparameterName", "k ="], [610, 613, "MetricName", "ROUGE - L"], [652, 653, "DatasetName", "RED"], [713, 714, "MethodName", "BERT"], [741, 742, "MethodName", "BERT"], [769, 770, "TaskName", "summarization"], [808, 811, "DatasetName", "CNN - DM"], [925, 926, "TaskName", "summarization"], [961, 962, "DatasetName", "XSum"], [971, 973, "TaskName", "extreme summarization"], [1014, 1015, "DatasetName", "PeerRead"], [1038, 1039, "DatasetName", "arXiv"], [1047, 1048, "TaskName", "summarization"], [1093, 1094, "DatasetName", "PeerRead"], [1134, 1135, "DatasetName", "Reddit"], [1269, 1270, "DatasetName", "IMDB"], [1277, 1278, "DatasetName", "Movi"]]}
{"text": "In the main paper , we compute topic similarity using the Jensen - Shannon distance . We initially considered the Kullback - Leibler ( KL ) divergence . While the JS distance and/or divergence has been more frequently used in the context of similarity in topic modelling , the KL divergence is also frequently considered . Intuitively and under some interpretations , the asymmetry of the KL divergence may be desirable as the extent to which a summary is topically similar to a document may not be the same as the extent to which a document is topically similar to a summary . In spite of this , in viewing the results using KL , we found that the measure lacked discriminative power in disambiguating examples we believed were more topically similar than others . We qualitatively found the judgments via the JS distance to be accurate . That said , the judgments between the measures tended to be highly correlated as the Spearman rank correlation coefficient was \u03c1 \u2265 0.7 for all topic modelling settings and in most cases exceeded 0.8 . We also considered a topic model learned using both the documents and summaries D \u222a S and just the documents D. Both are natural choices , with using the documents being more general in some sense as the topic similarity of a summary should be able to be assigned without requiring the summary collection . We further considered several choices for the number of topics as well . In Table 5 , we report the full results for all pairs of ( training corpus T , # topics k ) for all ( T , k ) { D \u222a S , D } \u00d7 { 10 , 20 , 50 , 100 } . In all cases , the number of training examples is truncated to 20000 ( hence 10000 summaries and 10000 documents when using the training corpus of D \u222a S ) . We fix the number of training documents across datasets to attempt to control for the confound of larger datasets inducing higher quality topic models . We did not observe significant changes in the result by relaxing this ( i.e. using the full datasets instead of just 20000 examples ) . We find that there is significant variation in crossdataset rankings with respect to these two parameters . We chose to report the results corresponding to k = 20 , T = D. We chose the value for k based on qualitative judgments about topic quality for CNN - DM , PeerRead , and AMI , as we considered these to be a diverse subset of all 10 datasets . The topics we observed were highly disjoint and reasonably aligned with our intuitions about what sensible topics should be . We chose the value for T based on the generality referenced previously . While the results are substantially different for D versus D \u222a S , we did not find any consistent and interpretable discriminative properties between the two .", "entities": [[351, 353, "TaskName", "topic models"], [404, 406, "HyperparameterName", "k ="], [425, 428, "DatasetName", "CNN - DM"], [429, 430, "DatasetName", "PeerRead"]]}
{"text": "In the main paper , we compute redundancy scores for each distinct sentence pair using ROUGE - L Fmeasure and then average these individual values to get a score for the entire summary . Alternatively , we considered other ROUGE scores ( specifically ROUGE - 1 and ROUGE - 2 ) as well as max pooling the sentence pair scores . We report these results below in Table 7 . We do not observe significant changes with the specific ROUGE metric considered ( i.e. a Spearman \u03c1 of 1.0 which indicates a perfect correlation in the case of max pooling across the ROUGE variants ) . We do see substantial differences between averaging and max pooling ; we find that max pooling turns out to precisely correlate ( \u03c1 = 1.0 ) with the average summary length . This is somewhat expected , given that the max - pooled redundancy estimates does n't inherently control for summary length . We therefore chose to report redundancy scores using averaging as we also qualitatively found them to be more useful and characteristic , especially for datasets such as AMI and the Scientific datasets as max pooling was overly aggressive . While the nuances of the specific ROUGE variant did not significantly impact trends in redundancy scores , we chose to report the ROUGE - L scores in the main paper as we ( highly subjectively ) found the values to be most interpretable / consistent with values we would have assigned .", "entities": [[15, 18, "MetricName", "ROUGE - L"], [54, 56, "MethodName", "max pooling"], [98, 100, "MethodName", "max pooling"], [114, 116, "MethodName", "max pooling"], [120, 122, "MethodName", "max pooling"], [192, 194, "MethodName", "max pooling"], [220, 223, "MetricName", "ROUGE - L"]]}
{"text": "We lowercase all terms , remove stopwords using the list specified in NLTK ( Loper and Bird , 2002 ) , and lemmatize using SpaCy ( Honnibal and Montani , 2017 ) . We only retain words tagged with a POS category in { NOUN , ADJ , VERB , ADV } by the SpaCy POS tagger . We use LDA ( Blei et al , 2003 ) to learn all topic models and rely on the implementation in Gensim ( \u0158eh\u016f\u0159ek and Sojka , 2010 ) based on specification of Hoffman et al ( 2010 ) . All hyperparameters are set as default and we discussed the number of topics k and training corpus T in A.2 with the results in the main paper using k = 20 and T = D where T is truncated to be at most 20000 documents . We compute the Jensen - Shannon distance using SciPy ( Virtanen et al , 2020 ) .", "entities": [[60, 61, "MethodName", "LDA"], [71, 73, "TaskName", "topic models"], [126, 128, "HyperparameterName", "k ="]]}
{"text": "We make use of the native Python reimplementation of ROUGE ( Lin , 2004 ) , easy - rouge . 19 All scores reported in the main paper use ROUGE - L and use the computed F - measure score .", "entities": [[29, 32, "MetricName", "ROUGE - L"], [36, 39, "MetricName", "F - measure"]]}
{"text": "All metrics reported in the main paper can be computed over all datasets in less than 10 ten hours on a single CPU . The only model with a nontrivial number of parameters used in this work is the bert - base - uncased models we use in measuring semantic coherence . We refer readers to Devlin et al ( 2019 ) for more details and to the HuggingFace implementation we reference previously .", "entities": [[30, 33, "HyperparameterName", "number of parameters"]]}
{"text": "We use the SimulEval toolkit . The toolkit provides a simple interface for evaluation of simultaneous ( speech ) translation . It reports the quality metric BLEU ( Papineni et al , 2002 ; Post , 2018 ) and latency metrics Average Proportion ( AP , Cho and Esipova 2016 ) , Average Lagging ( AL , Ma et al 2019 ) , and Differentiable Average Lagging ( DAL , Cherry and Foster 2019 ) modified for speech source . Specifically , we implement an Agent class . We have to implement two important functions : policy ( state ) and predict ( state ) , where state is the state of the agent ( e.g. , read processed input , emitted tokens , ... ) . The policy function returns the action of the agent : ( 1 ) READ to request more input , ( 2 ) WRITE to emit new hypothesis tokens . We implement the policy as specified in Algorithm 1 . The default action is READ . If there is a new chunk , we perform the inference and use the pref ix ( W c ) function to find the stable prefix . If there are new tokens to display ( i.e. , | pref ix ( W c ) | > | pref ix ( W c\u22121 ) | ) , we return the WRITE action . As soon as our agent emits an endof - sequence ( EOS ) token , the inference of the utterance is finished by the SimulEval . We noticed that our model was emitting the EOS token quite often , especially in the early chunks . Hence , we ignore the EOS if returned by our model and continue the inference until the end of the source . 3 Algorithm 1 Policy function Require : state if state.new_input > chunk_size then hypothesis predict ( state ) if | hypothesis | > 0 then return W RIT E end if end if return READ", "entities": [[26, 27, "MetricName", "BLEU"], [44, 45, "DatasetName", "AP"], [85, 86, "DatasetName", "Agent"], [113, 114, "DatasetName", "agent"], [135, 136, "DatasetName", "agent"], [238, 239, "DatasetName", "agent"], [324, 325, "DatasetName", "0"]]}
{"text": "As we could see in Section 5.1 , the shorter chunk sizes tend to perform worse . One of the reasons might be the limited context of the early chunks . 6 To increase the early context , we prolong the first chunk to 2 seconds . The results are in Table 1 . We see a slight ( 0.3 BLEU ) increase in quality for a chunk size of 250 ms , though the initial wait does not improve the BLEU and a considerable increase in the latency . The performance seems to be influenced mainly by the chunk size . The reason for smaller chunks ' under - performance might be caused by ( 1 ) acoustic uncertainty towards the end of a chunk ( e.g. , words often get cut in the middle ) , or ( 2 ) insufficient information difference between two consecutive chunks . This is supported by the observation in Figure 3 . Increasing the number of consecutive chunks ( i.e. , increasing the context for the decision ) considered in the local agreement strategy ( LA - 2 , 3 , 4 ) , improves the quality , while it adds latency .", "entities": [[60, 61, "MetricName", "BLEU"], [81, 82, "MetricName", "BLEU"]]}
{"text": "Interestingly , we noticed that some of the strategies achieved negative average lagging ( e.g. , LA - 2 in Section 5.1 ) with a chunk size of 250 ms has AL of - 36 ms ) . After a closer examination of the outputs , we found that the negative AL is in utterances where the hypothesis is significantly longer than the reference . Recall the average latency for speech input defined by : AL speech = 1 \u03c4 \u2032 ( | X | ) \u03c4 \u2032 ( | X | ) i=1 d i \u2212 d * i , ( 4 ) where d i = j k=1 T k , j is the index of raw audio segment that has been read when generating y i , T k is duration of raw audio segment , \u03c4 \u2032 ( | X | ) = min { i | d i = | X | j=1 T j } and d * i are the delays of an ideal policy : d * i = ( i \u2212 1 ) \u00d7 | X | j=1 T j / | Y * | , ( 5 ) where Y * is reference translation . If the hypothesis is longer than the reference , then d * i > d i , making the sum argument in Equation ( 4 ) negative . On the other hand , if we use the length of the hypothesis instead , then a shorter hypothesis would benefit . 7 We , therefore , suggest using the maximum of both to prevent the advantage of either a shorter or a longer hypothesis : d * i = ( i \u2212 1 ) \u00d7 | X | j=1 T j /max ( | Y | , | Y * | ) . ( 6 )", "entities": [[65, 66, "MetricName", "Recall"]]}
{"text": "In this section , we describe the submitted system . We follow the allowed training data and pretrained models and therefore our submission is constrained ( see Section 4.2.1 for model description ) . For stable hypothesis detection , we decided to use the local agreement strategy with n = 2 . As shown in Section 5.2 , the LA - 2 has the best latencyquality trade - off along with other LA - n strategies . To achieve the different latency regimes , we use various chunk sizes , depending on the language pair . We decided not to use larger n > 2 to control the latency , as it increases the computation complexity while having the same effect as using a different chunk size . The results on MuST - C tst - COMMON are in Table 2 . The quality - latency trade - off is in Figure 4 . From Table 2 and Figure 4 , we can see that the proposed method works well on two different models and various language pairs . We see that an improvement in the offline model ( offline BLEU of 31.36 and 33.14 for Model A and B , respectively ) leads to improvement in the online regime . Finally , we see that our method beats the best IWSLT 2021 system ( USTC - NELSLIP ( Liu et al , 2021 ) ) in medium and high latency regimes using both models ( i.e. , a model trained from scratch and a model based on pretrained wav2vec and mBART ) , and is almost on par in the low latency regime ( Model A is losing 0.35 BLEU and Model B is losing 0.47 BLEU ) .", "entities": [[131, 134, "DatasetName", "MuST - C"], [190, 191, "MetricName", "BLEU"], [261, 262, "MethodName", "mBART"], [280, 281, "MetricName", "BLEU"], [287, 288, "MetricName", "BLEU"]]}
{"text": "This paper describes our submission to the constrained track of WMT21 shared news translation task . We focus on the three relatively low resource language pairs Bengali \u2194 Hindi , English \u2194 Hausa and Xhosa \u2194 Zulu . To overcome the limitation of relatively low parallel data we train a multilingual model using a multitask objective employing both parallel and monolingual data . In addition , we augment the data using back translation . We also train a bilingual model incorporating back translation and knowledge distillation then combine the two models using sequence - to - sequence mapping . We see around 70 % relative gain in BLEU point for En \u2194 Ha and around 25 % relative improvements for Bn \u2194 Hi and Xh \u2194 Zu compared to bilingual baselines .", "entities": [[84, 86, "MethodName", "knowledge distillation"], [107, 108, "MetricName", "BLEU"]]}
{"text": "System combination or ensembling is known to improve the performance over individual systems . There are many ways to create an ensemble ( Liu et al , 2018 ; Dabre et al , 2019 ) . For example , individual models obtained from different checkpoints during the same training or by training models sharing the same vocab and architecture using different data or simply different random seeds can be combined using model averaging techniques . Here , we opt to combine different models since it generally leads to better performance because different models tend to be more complementary . To this end , we propose a simple and effective method to combine completely different architectures . The proposed method could be also used in conjunction with checkpoint and model averaging for further gains , but we have n't tried this in our experiments due to time limitations . The basic idea of our combination is very simple . Assume we have the translation pair x y where y is the reference translation . The output of model 1 is the pair x y1 and the output of model 2 is the pair x y2 . This can be generalized to multiple systems but we limited our combination to only two models . We train a new model that takes the set of hypotheses ( possibly augmented by the source sentence ) from the two models to generate the target sentence . Thus this model combines the outputs of two models in the ensemble to produce a translation closer to the original target sentence i.e. < HY P > y1 < HY P > y2 y. We also experimented with adding the source to the input i.e. < SRC > x < HY P > y1 < HY P > y2 y which led to around 0.3 BLEU improvement for Ha En , but we have n't tried on other pairs due to time limitation . All combination models use 6 layers encoder and decoder and a 64 K vocabulary similar to the multilingual system . These combination models use the full bitext and dev data provided in WMT21 as shown in Table 1 . The system combination is outlined in Figure 1 . This ensembling technique can be thought of as providing both system combination and post - editing capabilities .", "entities": [[66, 67, "DatasetName", "seeds"], [306, 307, "MetricName", "BLEU"]]}
{"text": "In this section , we describe the results of our intermediate and final systems . We report Sacre - BLEU ( Post , 2018 ) on the validation set released in WMT21 , and both SacreBLEU and COMET ( Rei et al , 2020 ) using the available implementation 3 on the official test set released in WMT21 . The results for the six submitted language pairs are in Tables 3 - 5 . The first row in each table shows the bilingual baseline which performs relatively poor due to the limited amount of parallel data for each pair . This is followed by the four multilingual systems with different objectives . It is clear that adding a monolingual objective brings nice improvements for all language pairs . The M T + DAE and M T + M LM + DAE perform closely for all language pairs indicating that target monolingual data is most important . The next two rows show the results of adding back - translated data to the multilingual model and a bilingual baseline using back - translated and knowledge distilled data generated from the best multilingual model . As expected adding back translation brings significant improvement to all language pairs . Also using the multilingual model to create data for a bilingual model shows excellent results that outperform the multilingual model . Finally , the ensemble , as expected , performs better than the individual models . The significant difference between reported improvements in Ha \u2194 En and other directions shows the effectiveness of adding De \u2194 En parallel and monolingual data that helps English centric directions more than other directions . We evaluated the final submitted systems on the official test set released in WMT21 as shown in Table 6 .", "entities": [[19, 20, "MetricName", "BLEU"], [35, 36, "MetricName", "SacreBLEU"]]}
{"text": "This paper describes our submission to the constrained track of WMT21 . We focus on the three relatively low resource language pairs Bn \u2194 Hi , En \u2194 Ha and Xh \u2194 Zu . To overcome the limitation of relatively low parallel data we train a multilingual model using a multitask objective recently proposed in ( Wang et al , 2020 ) . In addition , we augment the data using back translation . We also use the resulting multilingual model to create a bilingual model incorporating back translation and knowledge distillation . Finally , we combine the two models , using a flexible sequence - to - sequence approach , to yield our submitted systems . We see large gains up to 8 - 10 BLEU points for En \u2194 Ha and nice improvements of up to 2 - 3 BLEU points for Bn \u2194 Hi and Xh \u2194 Zu .", "entities": [[90, 92, "MethodName", "knowledge distillation"], [126, 127, "MetricName", "BLEU"], [141, 142, "MetricName", "BLEU"]]}
{"text": "Exemplar - based generative models ( Wu et al , 2019 ; Cai et al , 2019b ; Gupta et al , 2021 ) for open - domain conversation combine a retrieval model ( Humeau et al , 2019 ; Mazare et al , 2018 ; Kim et al , 2021 ) and a generative model ( Adiwardana et al , 2020 ; Roller et al , 2021 ; Figure 1 : Responses generated by the three exemplarbased generative models . RetNRef ignores the exemplar during response generation , RetNRef \u03b1 generates the response highly over - fitted to the exemplar , and RetNRef trained with our training method ( CORGE ) well utilizes the exemplar to produce a more fluent response than that of the others . Zhang et al , 2020 ; Brown et al , 2020 ) into a single framework to generate responses in two steps : ( 1 ) the retriever searches an exemplar using the given context as a query , and ( 2 ) the generator produces a response based on the given context and the retrieved exemplar . Exemplar - based generative models produce more specific responses than vanilla generative models while being more fluent than retrieval models . Despite their success , exemplar - based generative models have two major shortcomings . Primitive exemplar - based generative models Cai et al , 2019a ) tend to entirely ignore the exemplars and produce responses similar to those of vanilla generative models . This is due to the one - to - many problem ( Li et al , 2016 ) where there are many possible responses for each dialogue context . During the training phase , the retrieved exemplar is not helpful for generating the gold response when the exemplar retrieved for the given context is significantly different from the gold response . This leads exemplar - based generative models to ignore the exemplar while generating responses , as shown in Figure 1 ( a ) . To address this issue , recent exemplar - based generative models utilize the gold response ( Roller et al , 2021 ) or the slightly perturbed gold response ( Cai et al , 2019b ) as an exemplar in the training phase . However , these training methods cause the generator to rely heavily on the retrieved exemplar , i.e. the generator resorts to copying the provided tokens , as shown in Figure 1 ( b ) . These two disadvantages of existing exemplar - based generative models can adversely affect the quality of the generated response . Therefore , we introduce CORGE ( COnnecting Retriever and GEnerator ) , a simple training method of exemplar - based generative models considering the one - to - many problem of the open - domain conversation . As inspired by Wu et al ( 2019 ) , CORGE first utilizes the gold response instead of dialogue context as the query for the retriever to select exemplars that are similar to the gold response . The retrieved exemplars ensure that exemplar - based generative models utilize their semantics while generating the gold response at the training phase . Since the exemplars are retrieved by the gold response , some of them are lexically identical or too similar to the gold response . These exemplars lead exemplar - based generative models to be trained to depend on the exemplar heavily . Thus , CORGE then eliminates the exemplars based on the distance between the exemplars and the gold response to alleviate the dependency of the generative models on the exemplars . Here , we employ Jaccard similarity to measure the distance ( Guu et al , 2018 ; Cai et al , 2019a ; Wu et al , 2019 ) . However , as the selected exemplars solely depend on the gold response , some of them may be irrelevant to the given context , which results in exemplar - based generative models still ignoring the retrieved exemplar . To solve this , CORGE utilizes the relevance scores between the context and the exemplar to weight the relevant exemplars and penalizes irrelevant exemplars to the given context . Extensive experiments show that CORGE is generally applicable to the existing exemplar - based generative models and improves the quality of generated responses regarding appropriateness and informativeness .", "entities": [[86, 88, "TaskName", "response generation"], [90, 91, "HyperparameterName", "\u03b1"]]}
{"text": "As we select the exemplar totally based on the gold response , some of kNE could be relevant to the gold response r i but irrelevant to the given context c i . Therefore , we condition the generator with the relevance score of kNE to reward the relevant exemplars and penalize irrelevant exemplars . Using the retriever R , we calculate the relevance score S R ( z i , j , c i ) per each selected exemplar z i , j , then apply the softmax function to the relevance score to 1 Note that S R ( z , c ) and S R \u2032 ( z , ri ) use the same retriever , but they are computed differently . Please refer to how we calculate the score S R \u2032 ( z , ri ) and S R ( z , c ) in the Supplementary Materials . obtain the normalized relevance score P R ( z i , j , c i ) . Then we replace the traditional likelihood with the weighted likelihood using the normalized score . Our final training objective is to minimize the loss function L = n i=1 L ( r i , c i ) where : L ( ri , ci ) = \u2212 log z Z i P R ( z , ci ) P G ( ri | ci , z ) ( 1 ) The gradient of the generator G is calculated as follows : \u2207 G L ( ri , ci ) = \u2212\u03b1 z Z i P R ( z , ci ) \u2207 G ( P G ( ri | ci , z ) ) , ( 2 ) where \u03b1 \u22121 = z Z i P R ( z , c i ) P G ( r i | c i , z ) . This equation demonstrates that the gradient of the generator G is scaled by the normalized relevance score P R ( z , c i ) , which indicates that the generator is less updated when the retrieved exemplar z is not relevant to the given context c i . This procedure helps the model ignore the irrelevant exemplars . Thus , the generator learns to fetch tokens from the exemplar more easily , which is relevant to the gold response . Difference between CORGE and Knowledgegrounded generative models The way of leveraging the relevance scores is already employed by knowledge - grounded generative models ( Lewis et al , 2020 ; Sachan et al , 2021 ) in open - domain question answering . However , there is a significant difference between our CORGE and knowledgegrounded generative models . CORGE uses the relevance score P R ( z , c i ) to penalize the irrelevant exemplars z to the given context c i since the exemplars are retrieved by S R \u2032 ( z , r i ) . Knowledgegrounded generative models use it as the latent variable to jointly train the retriever R and generator G. Especially , knowledge - grounded generative models also tend to ignore the retrieved exemplars due Context Retrieval indicates the exemplar retrieved by using the context as a query , and kNE shows the exemplars selected by using the gold response as a query . Sim measures the lexical similarity between the gold response and the exemplar and P R ( z , c ) indicates the normalized relevance score calculated by retriever . to the one - to - many nature in open - domain conversation when the retriever and generator are jointly trained . On the other hand , we do not perform the joint learning of the retriever and the generator , but freeze the retriever while training the generator .", "entities": [[88, 89, "MethodName", "softmax"], [194, 195, "MetricName", "loss"], [290, 291, "HyperparameterName", "\u03b1"], [434, 439, "TaskName", "open - domain question answering"]]}
{"text": "Retrieval and Generative Models Bi - encoder 256 M ( Mazare et al , 2018 ) and Blender 90 M ( Roller et al , 2021 ) are considered as a baseline retrieval model and a baseline generative model . Further , they are also employed as a retriever and a generator of the following exemplarbased generative baselines , respectively . ( Roller et al , 2021 ) , and MatToGen ( Cai et al , 2019b ) , as baselines . RetNRef concatenates the retrieved exemplar with the given context as the input of the generator to produce the response . RetNRef \u03b1 is the dialogue retrieval version of RetNRef , which adopts \u03b1 - blending to escape from simply ignoring the retrieved exemplars ( \u03b1 = 0.5 ) . MatToGen extracts the meaningful tokens from the exemplar to provide them to the generator . To verify the effectiveness of our training method , we apply CORGE to RetNRef and Mat - ToGen instead of their training method . They are denoted as RetNRef + CORGE and MatTo - Gen+CORGE , respectively . Knowledge - grounded Generative Models Although RAG ( Lewis et al , 2020 ) and KIF ( Fan et al , 2021 ) are proposed to perform knowledgegrounded generation tasks , we employ RAG and KIF as baselines since they have a similar form with exemplar - based generative models . Our experiments demonstrate that these knowledge - grounded generative models can not be directly applied to the open - domain conversation .", "entities": [[17, 18, "MethodName", "Blender"], [103, 104, "HyperparameterName", "\u03b1"], [114, 115, "HyperparameterName", "\u03b1"], [126, 127, "HyperparameterName", "\u03b1"], [190, 191, "MethodName", "RAG"], [217, 218, "MethodName", "RAG"]]}
{"text": "To verify the effectiveness of our training method CORGE , we conduct a pair - wise comparison through the human evaluation following . We use two criteria : Appropriateness and Informativeness . Appropriateness measures how the generated response is fluent , logical , and appropriate to the given context . Informativeness measures how the generated response has meaningful information relevant to the given context . We use Amazon Mechanical Turk to collect the annotations , and more details are described in the Supplementary Material . We also employ the automatic evaluation metrics , Perplexity ( PPL ) , Dist - n , and BLEU ( Papineni et al , 2002 ) , to analyze the generated responses of each model . PPL measures how well the model predicts a response based on the given input context , and lower PPL indicates that the model predicts the response better . To analyze how much the exemplar - based generative model leverages the retrieved exemplar , we introduce two variants of PPL by utilizing conditional probability when exemplars are given : ( 1 ) PPL gold uses the conditional probability P G ( r | c , r ) , which assumes the situation when the gold response is given as an exemplar , and ( 2 ) PPL ret uses the conditional probability P G ( r | c , z ) where z is the retrieved exemplar by using S R \u2032 ( z , r ) . Lower PPL gold denotes that the exemplar - based generative model predicts the gold response well when the gold response is given as an exemplar . Lower PPL ret indicates that the exemplar - based generative model well leverages the provided exemplar to predict the gold response . Dist - n ( Li et al , 2016 ) is the ratio of distinct ngrams to a total number of n - grams for all the generated responses , which measures the degree of the diversity of the generated responses . BLEU ( z , r ) is adopted to measure the degree of the token overlap between the provided exemplar and the generated response pair ( z , r ) . A higher BLEU ( z , r ) score indicates that the generator copies more from the provided exemplar while generating the response .", "entities": [[82, 84, "DatasetName", "Supplementary Material"], [93, 94, "MetricName", "Perplexity"], [103, 104, "MetricName", "BLEU"], [338, 339, "MetricName", "BLEU"], [371, 372, "MetricName", "BLEU"]]}
{"text": "We provide the details of our implementation in the Supplementary Material . We will the source codes of CORGE for the reproducibility of the conducted experiments . 6 Experimental Results PPL ret than RetNRef + CORGE . This result demonstrates that RetNRef \u03b1 does not make good use of the retrieved exemplar except when the gold response is given as the retrieved exemplar . From this observation , we claim that RetNRef \u03b1 generates a response highly over - fitted to the selected exemplar , which is caused by utilizing the gold response as an exemplar in the training phase . The same goes for MatToGen , where applying CORGE mitigates the over - fitting issue .", "entities": [[9, 11, "DatasetName", "Supplementary Material"], [42, 43, "HyperparameterName", "\u03b1"], [72, 73, "HyperparameterName", "\u03b1"]]}
{"text": "Higher Dist - n of RetNRef + CORGE and Mat - ToGen+CORGE compared to Blender 90 M shows that our exemplar - based generative models produce more diverse responses than the vanilla generative model . Moreover , RetNRef + CORGE has higher Dist - n than RetNRef , which shows that utilizing the exemplars helps the generator diversify the responses . Although RetNRef \u03b1 is the only one that achieves comparable Dist - n to that of the vanilla retrieval model , Bi - encoder 256 M , it is derived from an over - fitting to the exemplar considering the gap between PPL gold and PPL ret , resulting in the degradation of appropriateness and informativeness in human evaluation . Average BLEU ( z , r ) scores implicitly measure the overlap between the retrieved exemplar and the generated response ; thus , a higher degree of BLEU ( z , r ) indicates that the generator depends more on the retrieved exemplar . RetNRef shows a negligible BLEU ( z , r ) score , which reaffirms that the model is almost not utilizing the retrieved exemplar . RetNRef \u03b1 and MatToGen have higher BLEU ( z , r ) scores compared to RetNRef + CORGE and MatToGen+CORGE , respectively , which verifies that the former depends more on the retrieved exemplar than the latter .", "entities": [[14, 15, "MethodName", "Blender"], [63, 64, "HyperparameterName", "\u03b1"], [122, 123, "MetricName", "BLEU"], [148, 149, "MetricName", "BLEU"], [169, 170, "MetricName", "BLEU"], [191, 192, "HyperparameterName", "\u03b1"], [196, 197, "MetricName", "BLEU"]]}
{"text": "The automatic evaluation results in Table 3 confirm that knowledge - grounded generative models are ignoring the exemplar . PPL gold , PPL ret , and Dist - n of RAG and KIF have a similar degree to those of Blender 90 M , which implies that the exemplars are not providing useful information while generating the response . The average BLEU ( z , r ) score also has a poor degree , indicating almost no overlap between the retrieved exemplars and the generated responses . We explain that these results are originated from the difference between the open - domain conversation and knowledge - grounded generation tasks . While training knowledge - grounded generative models , they use P R ( z , c ) to fetch the external knowledge . However , the generator also ignores the retrieved exemplar due to the one - to - many nature of the open - domain conversation . In addition , we observe that jointly training the retriever with the generator causes the retriever stuck in the local minima . As shown in Figure 4 , the standard deviation of normalized relevance scores P R ( z , c ) computed by the retriever almost gets near zero when the retriever of RAG is jointly trained . A smaller standard deviation means the relevance scores are getting flattened . Although knowledge - grounded generative models empirically have shown that jointly training the retriever and generator improves the performance in knowledge - intensive NLP tasks ( Lewis et al , 2020 ) , in open - domain conversation , the retrieved exemplars are ignored . Thus , the retriever learns to produce an uninformative relevance score . As a result , the retriever collapses , which means the retriever may return inappropriate exemplars to the generator ( also shown in the example of KIF and RAG in Table 4 ) . Intriguingly , jointly training the retriever with CORGE also causes the retriever scores to be flattened , as shown in Figure 4 , and we empirically observe the minor collapse of the retriever as we experienced in RAG as well . Thus , CORGE does not jointly train the retriever .", "entities": [[30, 31, "MethodName", "RAG"], [40, 41, "MethodName", "Blender"], [61, 62, "MetricName", "BLEU"], [212, 213, "MethodName", "RAG"], [314, 315, "MethodName", "RAG"], [357, 358, "MethodName", "RAG"]]}
{"text": "For automatic metrics , we calculate the metric for each case and take the average of those values . When calculating BLEU , we use sentence_bleu function in nltk python package ( Loper and Bird , 2002 ) .", "entities": [[21, 22, "MetricName", "BLEU"]]}
{"text": "We investigate the utility of sequence - to - sequence models with attention ( Bahdanau et al , 2015 ) to generate concrete realizations of abstract task descriptions . We hypothesize that models that learn explicit alignments are particularly amenable to interpretable analysis on the task . Therefore , in addition to using the global attention model of ( Luong et al , 2015 ) , we adapt the transducer model proposed by Yu et al ( 2016 ) , which uses learned latent discrete variables to model phraseto - phrase alignments . In contrast to many standard neural models , this approach enables us to incorporate prior knowledge about the alignment structure , and to extract interpretable alignments between task phrases . Closely related architectures have been proposed for segmental sequence modeling ( Wang et al , 2017 ) and phrase - based neural machine translation ( Huang et al , 2018 ) . We train the transducer models using Viterbi EM ( after doing marginal likelihood training for the initial iterations ) , as we found it gave higher predictive accuracy than marginal likelihood training only . Following Yu et al ( 2016 ) we experiment with both a fixed alignment transition probability model and a transition model with a neural parameterization . Cloze task prediction is performed greedily . 3 At each slot the Viterbi alignment of the prefix of the sequence up to that slot is computed . See appendix 7 for model details . 4 We also evaluate the performance of a language modelling baseline and a seq2seq model without attention ( Sutskever et al , 2014 ) , to compare the effect of not modeling alignment at all . We expect all the models to implicitly capture aspects of world knowledge . However , the discrete latent variable models provide Viterbi alignments over the training data , from which we can compile a look - up table with the extracted knowledge . In neural attention models , this knowledge is only weakly recoverable : extracting information requires hand tuning attention thresholds and there is no direct way to extract contiguous alignments for multi - word phrases .", "entities": [[145, 147, "TaskName", "machine translation"], [162, 163, "MetricName", "EM"], [182, 183, "MetricName", "accuracy"], [257, 259, "TaskName", "language modelling"], [262, 263, "MethodName", "seq2seq"]]}
{"text": "During generation , we provide the model with the number of words in each blank to be predicted . We consider two setups for evaluating examples with multiple blanks , both assuming that predictions are made left - to - right : Oracle , where the gold prediction of each blank is fed into the model to condition on for future predictions , and Greedy , where the model prediction is used for future predictions . We compute the proportion of exact word matches over each blank and the precision of the top k = 5 predictions for both setups . Additionally we compute the average surprisal of the gold prediction ( conditioning on oracle predictions ) . The surprisal of a word ( Attneave , 1959 ; Hale , 2001 ) is its negative log probability under the model : \u2212log ( P ( w i | w 1 : i\u22121 ) ) . The higher the probability of the ground truth , the lower the model 's \" surprise \" at seeing it in that context . Finally , as a quantitative proxy for interpretability , we report the length of the transducer models ' average Viterbi alignment span : our goal is a model which balances low average alignment lengths and high matching or ranking scores .", "entities": [[93, 95, "HyperparameterName", "k ="]]}
{"text": "We report results on the prediction task in Table 4 . First we consider models trained only on our dataset : All the models that incorporate a notion of alignment do substantially better than those who do not . We see that our transducer model with fixed alignment transition probabilities performs best in terms of predictive accuracy ( exact match and top - 5 precision ) , while the seqseq model with attention is the next best in most comparisons . The model with parameterized transitions has the lowest surprisal though , as it is more confident about the alignment predictions it is making . Using average alignment length to quantify whether the phrase alignments exhibit desirable structure , we see that the alignments found by the unparameterized transition model ( average length 6.18 ) are significantly shorter than those of the parameterized model ( average length 16.61 ) . Investigation shows that the paramaterized model mostly learns degenerate alignments , aligning most of the concrete sequence to either the start or end of the abstract sentence . In contrast , qualitative analysis of the unparameterized transition model show that its alignments learn desirable correspondences ( see Figure 2 ) . Therefore among our proposed models ( trained on in - domain data only ) the transducer with unparameterized transitions satisfies our desiderata of displaying both good predictive power for word generation , and learning interpretable alignments . Given the recent success of massively pre - ... 7 E 6 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 H n f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T J J p x p s s k Y n u h N R w K R R v o k D J O 6 n m N A 4 l b 4 f j 2 5 n f f u L a i E Q 9 4 i T l Q U y H S k S C U b T S g + u 6 / W r N c 7 0 5 y C r x C 1 K D A o 1 + 9 a s 3 S F g W c 4 V M U m O 6 v p d i k F O N g k k + r f Q y w 1 P K x n T I u 5 Y q G n M T 5 P N T p + T M K g M S J d q W Q j J X f 0 / k N D Z m E o e 2 M 6 Y 4 M s v e T P z P 6 2 Y Y X Q e 5 U G m G X L H F o i i 7 E 6 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 H n f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T J J p x p s s k Y n u h N R w K R R v o k D J O 6 n m N A 4 l b 4 f j 2 5 n f f u L a i E Q 9 4 i T l Q U y H S k S C U b T S g + u 6 / W r N c 7 0 5 y C r x C 1 K D A o 1 + 9 a s 3 S F g W c 4 V M U m O 6 v p d i k F O N g k k + r f Q y w 1 P K x n T I u 5 Y q G n M T 5 P N T p + T M K g M S J d q W Q j J X f 0 / k N D Z m E o e 2 M 6 Y 4 M s v e T P z P 6 2 Y Y X Q e 5 U G m G X L H F o i i 7 E 6 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 H n f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T J J p x p s s k Y n u h N R w K R R v o k D J O 6 n m N A 4 l b 4 f j 2 5 n f f u L a i E Q 9 4 i T l Q U y H S k S C U b T S g + u 6 / W r N c 7 0 5 y C r x C 1 K D A o 1 + 9 a s 3 S F g W c 4 V M U m O 6 v p d i k F O N g k k + r f Q y w 1 P K x n T I u 5 Y q G n M T 5 P N T p + T M K g M S J d q W Q j J X f 0 / k N D Z m E o e 2 M 6 Y 4 M s v e T P z P 6 2 Y Y X Q e 5 U G m G X L H F o i i 7 G 6 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T A X X x v O + n d L a + s b m V n m 7 s r O 7 t 3 / g H h 6 1 d J I p h k 2 W i E R 1 Q q p R c I l N w 4 3 A T q q Q x q H A d j i + n f n t J 1 S a J / L R T F I M Y j q U P O K M G i s 9 Y N / r u 1 W v 5 s 1 B V o l f k C o U a P T d r 9 4 g Y V m M 0 j B B t e 7 6 X m q C n C r D m c B p p Z d p T C k b 0 y F 2 L Z U 0 R h 3 k 8 1 O n 5 M w q A x I l y p Y 0 Z K 7 + n s h p r P U k D m 1 n T M 1 I L 3 s z 8 T + v m 5 n o O s i 5 T D O D k i 0 W R Z k g J i G z v 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p 2 K D c F f f n m V t C 5 q v l f z 7 y + r 9 Z s i j j K c w C m c g w 9 X U I c 7 a E A T G A z h G V 7 h z R H O i / P u f C x a S 0 4 x c w x / 4 H z + A O 0 r j Y w = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 N S V A 7 X C 5 w l a 7 N G f X i t f H c X t Z N 8 = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / o h 6 9 L B b B U 0 l E 0 G P R i 8 e K 9 g P a U D b b S b t 0 s w m 7 G 6 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T A X X x v O + n d L a + s b m V n m 7 s r O 7 t 3 / g H h 6 1 d J I p h k 2 W i E R 1 Q q p R c I l N w 4 3 A T q q Q x q H A d j i + n f n t J 1 S a J / L R T F I M Y j q U P O K M G i s 9 Y N / r u 1 W v 5 s 1 B V o l f k C o U a P T d r 9 4 g Y V m M 0 j B B t e 7 6 X m q C n C r D m c B p p Z d p T C k b 0 y F 2 L Z U 0 R h 3 k 8 1 O n 5 M w q A x I l y p Y 0 Z K 7 + n s h p r P U k D m 1 n T M 1 I L 3 s z 8 T + v m 5 n o O s i 5 T D O D k i 0 W R Z k g J i G z v 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p 2 K D c F f f n m V t C 5 q v l f z 7 y + r 9 Z s i j j K c w C m c g w 9 X U I c 7 a E A T G A z h G V 7 h z R H O i / P u f C x a S 0 4 x c w x / 4 H z + A O 0 r j Y w = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 N S V A 7 X C 5 w l a 7 N G f X i t f H c X t Z N 8 = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / o h 6 9 L B b B U 0 l E 0 G P R i 8 e K 9 g P a U D b b S b t 0 s w m 7 G 6 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T A X X x v O + n d L a + s b m V n m 7 s r O 7 t 3 / g H h 6 1 d J I p h k 2 W i E R 1 Q q p R c I l N w 4 3 A T q q Q x q H A d j i + n f n t J 1 S a J / L R T F I M Y j q U P O K M G i s 9 Y N / r u 1 W v 5 s 1 B V o l f k C o U a P T d r 9 4 g Y V m M 0 j B B t e 7 6 X m q C n C r D m c B p p Z d p T C k b 0 y F 2 L Z U 0 R h 3 k 8 1 O n 5 M w q A x I l y p Y 0 Z K 7 + n s h p r P U k D m 1 n T M 1 I L 3 s z 8 T + v m 5 n o O s i 5 T D O D k i 0 W R Z k g J i G z v 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p 2 K D c F f f n m V t C 5 q v l f z 7 y + r 9 Z s i j j K c w C m c g w 9 X U I c 7 a E A T G A z h G V 7 h z R H O i / P u f C x a S 0 4 x c w x / 4 H z + A O 0 r j Y w = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 N S V A 7 X C 5 w l a 7 N G f X i t f H c X t Z N 8 = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / o h 6 9 L B b B U 0 l E 0 G P R i 8 e K 9 g P a U D b b S b t 0 s w m 7 G 6 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T A X X x v O + n d L a + s b m V n m 7 s r O 7 t 3 / g H h 6 1 d J I p h k 2 W i E R 1 Q q p R c I l N w 4 3 A T q q Q x q H A d j i + n f n t J 1 S a J / L R T F I M Y j q U P O K M G i s 9 Y N / r u 1 W v 5 s 1 B V o l f k C o U a P T d r 9 4 g Y V m M 0 j B B t e 7 6 X m q C n C r D m c B p p Z d p T C k b 0 y F 2 L Z U 0 R h 3 k 8 1 O n 5 M w q A x I l y p Y 0 Z K 7 + n s h p r P U k D m 1 n T M 1 I L 3 s z 8 T + v m 5 n o O s i 5 T D O D k i 0 W R Z k g J i G z v 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p 2 K D c F f f n m V t C 5 q v l f z 7 y + r 9 Z s i j j K c w C m c g w 9 X U I c 7 a E A T G A z h G V 7 h z R H O i / P u f C x a S 0 4 x c w x / 4 H z + A O 0 r j Y w = < / l a t e x i t > e6 < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 2 M f j m J D q M 2 1 J 4 r d 2 G a r P c w F A s o = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q s e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s t H M 0 n Q j + h Q 8 p A z a q z 0 g P 1 a v 1 x x q + 4 c Z J V 4 O a l A j k a / / N U b x C y N U B o m q N Z d z 0 2 M n 1 F l O B M 4 L f V S j Q l l Y z r E r q W S R q j 9 b H 7 q l J x Z Z U D C W N m S h s z V 3 x M Z j b S e R I H t j K g Z 6 W V v J v 7 n d V M T X v s Z l 0 l q U L L F o j A V x M R k 9 j c Z c I X M i I k l l C l u b y V s R B V l x q Z T s i F 4 y y + v k t Z F 1 X O r 3 v 1 l p X 6 T x 1 G E E z i F c / D g C u p w B w 1 o A o M h P M M r v D n C e X H e n Y 9 F a 8 H J Z 4 7 h D 5 z P H / Z D j Z I = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 2 M f j m J D q M 2 1 J 4 r d 2 G a r P c w F A s o = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q s e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s t H M 0 n Q j + h Q 8 p A z a q z 0 g P 1 a v 1 x x q + 4 c Z J V 4 O a l A j k a / / N U b x C y N U B o m q N Z d z 0 2 M n 1 F l O B M 4 L f V S j Q l l Y z r E r q W S R q j 9 b H 7 q l J x Z Z U D C W N m S h s z V 3 x M Z j b S e R I H t j K g Z 6 W V v J v 7 n d V M T X v s Z l 0 l q U L L F o j A V x M R k 9 j c Z c I X M i I k l l C l u b y V s R B V l x q Z T s i F 4 y y + v k t Z F 1 X O r 3 v 1 l p X 6 T x 1 G E E z i F c / D g C u p w B w 1 o A o M h P M M r v D n C e X H e n Y 9 F a 8 H J Z 4 7 h D 5 z P H / Z D j Z I = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 2 M f j m J D q M 2 1 J 4 r d 2 G a r P c w F A s o = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q s e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s t H M 0 n Q j + h Q 8 p A z a q z 0 g P 1 a v 1 x x q + 4 c Z J V 4 O a l A j k a / / N U b x C y N U B o m q N Z d z 0 2 M n 1 F l O B M 4 L f V S j Q l l Y z r E r q W S R q j 9 b H 7 q l J x Z Z U D C W N m S h s z V 3 x M Z j b S e R I H t j K g Z 6 W V v J v 7 n d V M T X v s Z l 0 l q U L L F o j A V x M R k 9 j c Z c I X M i I k l l C l u b y V s R B V l x q Z T s i F 4 y y + v k t Z F 1 X O r 3 v 1 l p X 6 T x 1 G E E z i F c / D g C u p w B w 1 o A o M h P M M r v D n C e X H e n Y 9 F a 8 H J Z 4 7 h D 5 z P H / Z D j Z I = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 2 M f j m J D q M 2 1 J 4 r d 2 G a r P c w F A s o = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q s e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s t H M 0 n Q j + h Q 8 p A z a q z 0 g P 1 a v 1 x x q + 4 c Z J V 4 O a l A j k a / / N U b x C y N U B o m q N Z d z 0 2 M n 1 F l O B M 4 L f V S j Q l l Y z r E r q W S R q j 9 b H 7 q l J x Z Z U D C W N m S h s z V 3 x M Z j b S e R I H t j K g Z 6 W V v J v 7 n d V M T X v s Z l 0 l q U L L F o j A V x M R k 9 j c Z c I X M i I k l l C l u b y V s R B V l x q Z T s i F 4 y y + v k t Z F 1 X O r 3 v 1 l p X 6 T x 1 G E E z i F c / D g C u p w B w 1 o A o M h P M M r v D n C e X H e n Y 9 F a 8 H J Z 4 7 h D 5 z P H / Z D j Z I = < / l a t e x i t > ... 7 G 6 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T A X X x v O + n d L a + s b m V n m 7 s r O 7 t 3 / g H h 6 1 d J I p h k 2 W i E R 1 Q q p R c I l N w 4 3 A T q q Q x q H A d j i + n f n t J 1 S a J / L R T F I M Y j q U P O K M G i s 9 Y N / r u 1 W v 5 s 1 B V o l f k C o U a P T d r 9 4 g Y V m M 0 j B B t e 7 6 X m q C n C r D m c B p p Z d p T C k b 0 y F 2 L Z U 0 R h 3 k 8 1 O n 5 M w q A x I l y p Y 0 Z K 7 + n s h p r P U k D m 1 n T M 1 I L 3 s z 8 T + v m 5 n o O s i 5 T D O D k i 0 W R Z k g J i G z v 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p 2 K D c F f f n m V t C 5 q v l f z 7 y + r 9 Z s i j j K c w C m c g w 9 X U I c 7 a E A T G A z h G V 7 h z R H O i / P u f C x a S 0 4 x c w x / 4 H z + A O 0 r j Y w = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 N S V A 7 X C 5 w l a 7 N G f X i t f H c X t Z N 8 = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / o h 6 9 L B b B U 0 l E 0 G P R i 8 e K 9 g P a U D b b S b t 0 s w m 7 G 6 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T A X X x v O + n d L a + s b m V n m 7 s r O 7 t 3 / g H h 6 1 d J I p h k 2 W i E R 1 Q q p R c I l N w 4 3 A T q q Q x q H A d j i + n f n t J 1 S a J / L R T F I M Y j q U P O K M G i s 9 Y N / r u 1 W v 5 s 1 B V o l f k C o U a P T d r 9 4 g Y V m M 0 j B B t e 7 6 X m q C n C r D m c B p p Z d p T C k b 0 y F 2 L Z U 0 R h 3 k 8 1 O n 5 M w q A x I l y p Y 0 Z K 7 + n s h p r P U k D m 1 n T M 1 I L 3 s z 8 T + v m 5 n o O s i 5 T D O D k i 0 W R Z k g J i G z v 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p 2 K D c F f f n m V t C 5 q v l f z 7 y + r 9 Z s i j j K c w C m c g w 9 X U I c 7 a E A T G A z h G V 7 h z R H O i / P u f C x a S 0 4 x c w x / 4 H z + A O 0 r j Y w = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 N S V A 7 X C 5 w l a 7 N G f X i t f H c X t Z N 8 = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / o h 6 9 L B b B U 0 l E 0 G P R i 8 e K 9 g P a U D b b S b t 0 s w m 7 G 6 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T A X X x v O + n d L a + s b m V n m 7 s r O 7 t 3 / g H h 6 1 d J I p h k 2 W i E R 1 Q q p R c I l N w 4 3 A T q q Q x q H A d j i + n f n t J 1 S a J / L R T F I M Y j q U P O K M G i s 9 Y N / r u 1 W v 5 s 1 B V o l f k C o U a P T d r 9 4 g Y V m M 0 j B B t e 7 6 X m q C n C r D m c B p p Z d p T C k b 0 y F 2 L Z U 0 R h 3 k 8 1 O n 5 M w q A x I l y p Y 0 Z K 7 + n s h p r P U k D m 1 n T M 1 I L 3 s z 8 T + v m 5 n o O s i 5 T D O D k i 0 W R Z k g J i G z v 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p 2 K D c F f f n m V t C 5 q v l f z 7 y + r 9 Z s i j j K c w C m c g w 9 X U I c 7 a E A T G A z h G V 7 h z R H O i / P u f C x a S 0 4 x c w x / 4 H z + A O 0 r j Y w = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 N S V A 7 X C 5 w l a 7 N G f X i t f H c X t Z N 8 = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / o h 6 9 L B b B U 0 l E 0 G P R i 8 e K 9 g P a U D b b S b t 0 s w m 7 G 6 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T A X X x v O + n d L a + s b m V n m 7 s r O 7 t 3 / g H h 6 1 d J I p h k 2 W i E R 1 Q q p R c I l N w 4 3 A T q q Q x q H A d j i + n f n t J 1 S a J / L R T F I M Y j q U P O K M G i s 9 Y N / r u 1 W v 5 s 1 B V o l f k C o U a P T d r 9 4 g Y V m M 0 j B B t e 7 6 X m q C n C r D m c B p p Z d p T C k b 0 y F 2 L Z U 0 R h 3 k 8 1 O n 5 M w q A x I l y p Y 0 Z K 7 + n s h p r P U k D m 1 n T M 1 I L 3 s z 8 T + v m 5 n o O s i 5 T D O D k i 0 W R Z k g J i G z v 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p 2 K D c F f f n m V t C 5 q v l f z 7 y + r 9 Z s i j j K c w C m c g w 9 X U I c 7 a E A T G A z h G V 7 h z R H O i / P u f C x a S 0 4 x c w x / 4 H z + A O 0 r j Y w = < / l a t e x i t > e6 < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 2 M f j m J D q M 2 1 J 4 r d 2 G a r P c w F A s o = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q s e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s t H M 0 n Q j + h Q 8 p A z a q z 0 g P 1 a v 1 x x q + 4 c Z J V 4 O a l A j k a / / N U b x C y N U B o m q N Z d z 0 2 M n 1 F l O B M 4 L f V S j Q l l Y z r E r q W S R q j 9 b H 7 q l J x Z Z U D C W N m S h s z V 3 x M Z j b S e R I H t j K g Z 6 W V v J v 7 n d V M T X v s Z l 0 l q U L L F o j A V x M R k 9 j c Z c I X M i I k l l C l u b y V s R B V l x q Z T s i F 4 y y + v k t Z F 1 X O r 3 v 1 l p X 6 T x 1 G E E z i F c / D g C u p w B w 1 o A o M h P M M r v D n C e X H e n Y 9 F a 8 H J Z 4 7 h D 5 z P H / Z D j Z I = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 2 M f j m J D q M 2 1 J 4 r d 2 G a r P c w F A s o = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q s e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s t H M 0 n Q j + h Q 8 p A z a q z 0 g P 1 a v 1 x x q + 4 c Z J V 4 O a l A j k a / / N U b x C y N U B o m q N Z d z 0 2 M n 1 F l O B M 4 L f V S j Q l l Y z r E r q W S R q j 9 b H 7 q l J x Z Z U D C W N m S h s z V 3 x M Z j b S e R I H t j K g Z 6 W V v J v 7 n d V M T X v s Z l 0 l q U L L F o j A V x M R k 9 j c Z c I X M i I k l l C l u b y V s R B V l x q Z T s i F 4 y y + v k t Z F 1 X O r 3 v 1 l p X 6 T x 1 G E E z i F c / D g C u p w B w 1 o A o M h P M M r v D n C e X H e n Y 9 F a 8 H J Z 4 7 h D 5 z P H / Z D j Z I = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 2 M f j m J D q M 2 1 J 4 r d 2 G a r P c w F A s o = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q s e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s t H M 0 n Q j + h Q 8 p A z a q z 0 g P 1 a v 1 x x q + 4 c Z J V 4 O a l A j k a / / N U b x C y N U B o m q N Z d z 0 2 M n 1 F l O B M 4 L f V S j Q l l Y z r E r q W S R q j 9 b H 7 q l J x Z Z U D C W N m S h s z V 3 x M Z j b S e R I H t j K g Z 6 W V v J v 7 n d V M T X v s Z l 0 l q U L L F o j A V x M R k 9 j c Z c I X M i I k l l C l u b y V s R B V l x q Z T s i F 4 y y + v k t Z F 1 X O r 3 v 1 l p X 6 T x 1 G E E z i F c / D g C u p w B w 1 o A o M h P M M r v D n C e X H / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s t H M 0 n Q j + h Q 8 p A z a q z 0 g P 1 a v 1 x x q + 4 c Z J V 4 O a l A j k a / / N U b x C y N U B o m q N Z d z 0 2 M n 1 F l O B M 4 L f V S j Q l l Y z r E r q W S R q j 9 b H 7 q l J x Z Z U D C W N m S h s z V 3 x M Z j b S e R I H t j K g Z 6 W V v J v 7 n d V M T X v s Z l 0 l q U L L F o j A V x M R k 9 j c Z c I X M i I k l l C l u b y V s R B V l x q Z T s i F 4 y y + v k t Z F 1 X O r 3 v 1 l p X 6 T x 1 G E E z i F c / D g C u p w B w 1 o A o M h P M M r v D n C e X H e n Y 9 F a 8 H J Z 4 7 h D 5 z P H / Z D j Z I = < / l a t e x i t > trained language models ( Peters et al , 2018 ) , we are interested if these approaches transfer to our cloze task . We evaluate the OpenAI GPT transformer language model ( Radford et al , 2018 ) with and without fine - tuning . Without fine - tuning this model does slightly worse than our best domainspecific model . With fine - tuning , its accuracy is substantially higher , but it still suffers from the same fundamental limitations as our other models ( see Table 5 ) . The transformer ( Vaswani et al , 2017 ) attention is multi - headed and multi - layered which prohibits direct interpretability .", "entities": [[56, 57, "MetricName", "accuracy"], [58, 60, "MetricName", "exact match"], [390, 391, "DatasetName", "0"], [473, 474, "DatasetName", "0"], [658, 659, "DatasetName", "0"], [741, 742, "DatasetName", "0"], [926, 927, "DatasetName", "0"], [1009, 1010, "DatasetName", "0"], [1216, 1217, "DatasetName", "0"], [1244, 1245, "DatasetName", "0"], [1251, 1252, "DatasetName", "0"], [1271, 1272, "DatasetName", "0"], [1314, 1315, "DatasetName", "0"], [1417, 1418, "DatasetName", "0"], [1430, 1431, "DatasetName", "0"], [1530, 1531, "DatasetName", "0"], [1533, 1534, "DatasetName", "0"], [1552, 1553, "DatasetName", "0"], [1721, 1722, "DatasetName", "0"], [1749, 1750, "DatasetName", "0"], [1756, 1757, "DatasetName", "0"], [1776, 1777, "DatasetName", "0"], [1819, 1820, "DatasetName", "0"], [1922, 1923, "DatasetName", "0"], [1935, 1936, "DatasetName", "0"], [2035, 2036, "DatasetName", "0"], [2038, 2039, "DatasetName", "0"], [2057, 2058, "DatasetName", "0"], [2226, 2227, "DatasetName", "0"], [2254, 2255, "DatasetName", "0"], [2261, 2262, "DatasetName", "0"], [2281, 2282, "DatasetName", "0"], [2324, 2325, "DatasetName", "0"], [2427, 2428, "DatasetName", "0"], [2440, 2441, "DatasetName", "0"], [2540, 2541, "DatasetName", "0"], [2543, 2544, "DatasetName", "0"], [2562, 2563, "DatasetName", "0"], [2731, 2732, "DatasetName", "0"], [2759, 2760, "DatasetName", "0"], [2766, 2767, "DatasetName", "0"], [2786, 2787, "DatasetName", "0"], [2829, 2830, "DatasetName", "0"], [2932, 2933, "DatasetName", "0"], [2945, 2946, "DatasetName", "0"], [3046, 3047, "DatasetName", "0"], [3060, 3061, "DatasetName", "0"], [3074, 3075, "DatasetName", "0"], [3141, 3142, "DatasetName", "0"], [3190, 3191, "DatasetName", "0"], [3204, 3205, "DatasetName", "0"], [3246, 3247, "DatasetName", "0"], [3330, 3331, "DatasetName", "0"], [3551, 3552, "DatasetName", "0"], [3565, 3566, "DatasetName", "0"], [3579, 3580, "DatasetName", "0"], [3646, 3647, "DatasetName", "0"], [3695, 3696, "DatasetName", "0"], [3709, 3710, "DatasetName", "0"], [3751, 3752, "DatasetName", "0"], [3835, 3836, "DatasetName", "0"], [4056, 4057, "DatasetName", "0"], [4070, 4071, "DatasetName", "0"], [4084, 4085, "DatasetName", "0"], [4151, 4152, "DatasetName", "0"], [4200, 4201, "DatasetName", "0"], [4214, 4215, "DatasetName", "0"], [4256, 4257, "DatasetName", "0"], [4340, 4341, "DatasetName", "0"], [4561, 4562, "DatasetName", "0"], [4575, 4576, "DatasetName", "0"], [4589, 4590, "DatasetName", "0"], [4656, 4657, "DatasetName", "0"], [4705, 4706, "DatasetName", "0"], [4719, 4720, "DatasetName", "0"], [4761, 4762, "DatasetName", "0"], [4845, 4846, "DatasetName", "0"], [5148, 5149, "DatasetName", "0"], [5176, 5177, "DatasetName", "0"], [5183, 5184, "DatasetName", "0"], [5203, 5204, "DatasetName", "0"], [5246, 5247, "DatasetName", "0"], [5349, 5350, "DatasetName", "0"], [5362, 5363, "DatasetName", "0"], [5462, 5463, "DatasetName", "0"], [5465, 5466, "DatasetName", "0"], [5484, 5485, "DatasetName", "0"], [5653, 5654, "DatasetName", "0"], [5681, 5682, "DatasetName", "0"], [5688, 5689, "DatasetName", "0"], [5708, 5709, "DatasetName", "0"], [5751, 5752, "DatasetName", "0"], [5854, 5855, "DatasetName", "0"], [5867, 5868, "DatasetName", "0"], [5967, 5968, "DatasetName", "0"], [5970, 5971, "DatasetName", "0"], [5989, 5990, "DatasetName", "0"], [6158, 6159, "DatasetName", "0"], [6186, 6187, "DatasetName", "0"], [6193, 6194, "DatasetName", "0"], [6213, 6214, "DatasetName", "0"], [6256, 6257, "DatasetName", "0"], [6359, 6360, "DatasetName", "0"], [6372, 6373, "DatasetName", "0"], [6472, 6473, "DatasetName", "0"], [6475, 6476, "DatasetName", "0"], [6494, 6495, "DatasetName", "0"], [6663, 6664, "DatasetName", "0"], [6691, 6692, "DatasetName", "0"], [6698, 6699, "DatasetName", "0"], [6718, 6719, "DatasetName", "0"], [6761, 6762, "DatasetName", "0"], [6864, 6865, "DatasetName", "0"], [6877, 6878, "DatasetName", "0"], [6978, 6979, "DatasetName", "0"], [6992, 6993, "DatasetName", "0"], [7006, 7007, "DatasetName", "0"], [7073, 7074, "DatasetName", "0"], [7122, 7123, "DatasetName", "0"], [7136, 7137, "DatasetName", "0"], [7178, 7179, "DatasetName", "0"], [7262, 7263, "DatasetName", "0"], [7483, 7484, "DatasetName", "0"], [7497, 7498, "DatasetName", "0"], [7511, 7512, "DatasetName", "0"], [7578, 7579, "DatasetName", "0"], [7627, 7628, "DatasetName", "0"], [7641, 7642, "DatasetName", "0"], [7683, 7684, "DatasetName", "0"], [7767, 7768, "DatasetName", "0"], [7988, 7989, "DatasetName", "0"], [8002, 8003, "DatasetName", "0"], [8016, 8017, "DatasetName", "0"], [8083, 8084, "DatasetName", "0"], [8132, 8133, "DatasetName", "0"], [8146, 8147, "DatasetName", "0"], [8188, 8189, "DatasetName", "0"], [8272, 8273, "DatasetName", "0"], [8381, 8382, "DatasetName", "0"], [8430, 8431, "DatasetName", "0"], [8444, 8445, "DatasetName", "0"], [8486, 8487, "DatasetName", "0"], [8570, 8571, "DatasetName", "0"], [8734, 8735, "MethodName", "GPT"], [8773, 8774, "MetricName", "accuracy"]]}
{"text": "Next we briefly describe the dynamic program used to marginalize over alignments during training and to find the most likely alignments of a given alignment during inference ; we refer the reader to Yu , Buys , and Blunsom ( 2016 ) for a more thorough treatment . The forward variable \u03b1 i ( j ) representing p ( y 1 : j , a j = i | x 1 : i ) is recursively as \u03b1i ( j ) = p ( yj | i , x1 : i , y1 : j\u22121 ) \u00d7 i k=1 \u03b1 k ( j \u2212 1 ) p ( aj = i | k , x 1 : k , y1 : j\u22121 ) . ( 7 ) The marginal likelihood objective is to train the model to optimize \u03b1 m ( n ) = p ( y 1 : n , a n = m | x 1 : m ) . The gradients are computed with automatic differentiation ; as this is is equivalent to using the forward - backward algorithm to estimate the gradients ( Eisner , 2016 ) , only the forward algorithm has to be implemented . To make the implementation GPU - efficient , we vectorize the computation of \u03b1 . The computation iterates through decoding steps , each of which can be generated from an alignment to any of the encoder tokens . We can efficiently construct a transition matrix T , corresponding to all possible encoder states performing all possible shifts , and emission matrix E j which is a gather by word index j. To compute the forward probabilities at each timestep , the current forward probabilities are first multiplied by all possible transitions . A sum in logspace collapses all paths , and the emission ( word generation ) probabilities are multiplied to obtain the new forward probabilities . When fixed transition probabilities are used , T is precomputed .", "entities": [[51, 52, "HyperparameterName", "\u03b1"], [99, 100, "HyperparameterName", "\u03b1"], [138, 139, "HyperparameterName", "\u03b1"], [214, 215, "HyperparameterName", "\u03b1"]]}
{"text": "Latent variable models can be trained either through directly optimizing the likelihood objective through gradient descent ( as described above ) , or with the Expectation Maximization ( EM ) algorithm ( Dempster et al , 1977 ) , which alternates between calculating expectations over the values of the latent variables given the current parameters , and maximizing the expected complete data log likelihood given those expectations . We consider training our alignment model with Viterbi EM ( Brown et al , 1993 ) , also known as \" hard \" EM , where at each iteration the most likely assignment of the hidden variables ( alignments ) are found and the parameters are updated to optimize the log likelihood given those alignments . Viterbi EM has been shown to give superior performance to standard EM on unsupervised parsing ( Spitkovsky et al , 2010 ) , due to better convergence properties in practice by making the distribution more peaked . We perform batched Viterbi EM training by computing the Viterbi alignments for a batch , and then performing a gradient step based on treating those alignments as observations . We follow a two - stage training procedure : we first directly optimize the marginal likelihood with batched SGD to find a reasonable initial distribu - tion over alignments , before switching to Viterbi EM training . Such a strategy has been shown to reduce the chance that the model will get stuck in local optima ( Spitkovsky et al , 2011 ) .", "entities": [[28, 29, "MetricName", "EM"], [76, 77, "MetricName", "EM"], [91, 92, "MetricName", "EM"], [125, 126, "MetricName", "EM"], [135, 136, "MetricName", "EM"], [165, 166, "MetricName", "EM"], [208, 209, "MethodName", "SGD"], [224, 225, "MetricName", "EM"]]}
{"text": "We apply the trained models to multiple inference problems to evaluate how well they are capturing script knowledge . The first is finding the most likely alignment given a pair of abstract and concrete sequences . We use the standard Viterbi algorithm , in which we replace the sum in equation ( 7 ) with max , and keep track of the index corresponding to each value of \u03b1 during the forward computation . The most likely alignment can then be traced back from a n = m. The second inference problem is slot - filling , for application to the cloze task . Given an abstract sentence and a partially - filled concrete sequence , we want to use the model to predict words to fill the given blanks . To make the prediction , we sample 5 candidate sequences by predicting words for each slot , in left - to - right order , and then choosing the sequence with the highest overall probability . Words are predicted by sampling with temperature 0.1 , in order to peak the distribution while still allowing some diversity in the samples . The motivation for selecting the final output from multiple samples is that the original samples are biased , as they are only conditioned on the left context . At the start of the prediction for each slot , the Viterbi alignment of the prefix of the sequence up to the start of that slot is re - predicted , independent of previous alignment predictions . Consequently alignment decisions can be revised , and the slot alignments are no longer constrained to be monotonic , which makes the slot prediction model more flexible . For the parameterized transition model , the slot alignment is predicted greedily by incrementing the last predicted alignment while the shift probability is greater than 0.5 . The fixed transition model assumes that the alignment of the word preceding the slot is shared across the slot .", "entities": [[68, 69, "HyperparameterName", "\u03b1"]]}
{"text": "Deep energy - based models are powerful , but pose challenges for learning and inference ( Belanger and McCallum , 2016 ) . Tu and Gimpel ( 2018 ) developed an efficient framework for energy - based models by training \" inference networks \" to approximate structured inference instead of using gradient descent . However , their alternating optimization approach suffers from instabilities during training , requiring additional loss terms and careful hyperparameter tuning . In this paper , we contribute several strategies to stabilize and improve this joint training of energy functions and inference networks for structured prediction . We design a compound objective to jointly train both costaugmented and test - time inference networks along with the energy function . We propose joint parameterizations for the inference networks that encourage them to capture complementary functionality during learning . We empirically validate our strategies on two sequence labeling tasks , showing easier paths to strong performance than prior work , as well as further improvements with global energy terms .", "entities": [[68, 69, "MetricName", "loss"], [97, 99, "TaskName", "structured prediction"]]}
{"text": "Energy - based modeling ( LeCun et al , 2006 ) associates a scalar compatibility measure to each configuration of input and output variables . Belanger and McCallum ( 2016 ) formulated deep energy - based models for structured prediction , which they called structured prediction energy networks ( SPENs ) . SPENs use arbitrary neural networks to define the scoring function over input / output pairs . However , this flexibility leads to challenges for learning and inference . The original work on SPENs used gradient descent for structured inference ( Belanger and McCallum , 2016 ; Belanger et al , 2017 ) . Gimpel ( 2018 , 2019 ) found improvements in both speed and accuracy by replacing the use of gradient descent with a method that trains a neural network ( called an \" inference network \" ) to do inference directly . Their formulation , which jointly trains the inference network and energy function , is similar to training in generative adversarial networks ( Goodfellow et al , 2014 ) , which is known to suffer from practical difficulties in training due to the use of alternating optimization ( Salimans et al , 2016 ) . To stabilize training , Tu and Gimpel ( 2018 ) experimented with several additional terms in the training objectives , finding performance to be dependent on their inclusion . Moreover , when using the approach of Tu and Gimpel ( 2018 ) , there is a mismatch between the training and test - time uses of the trained inference network . During training with hinge loss , the inference network is actually trained to do \" costaugmented \" inference . However , at test time , the goal is to simply minimize the energy without any cost term . Tu and Gimpel ( 2018 ) fine - tuned the cost - augmented network to match the test - time criterion , but found only minimal change from this fine - tuning . This suggests that the cost - augmented network was mostly acting as a test - time inference network by convergence , which may be hindering the potential contributions of cost - augmented inference in max - margin structured learning ( Tsochantaridis et al , 2004 ; Taskar et al , 2004 ) . In this paper , we contribute a new training objective for SPENs that addresses the above concern and also contribute several techniques for stabilizing and improving learning . We empirically validate our strategies on two sequence labeling tasks from natural language processing ( NLP ) , namely part - of - speech tagging and named entity recognition . We show easier paths to strong performance than prior work , as well as further improvements with global energy terms . We summarize our list of contributions as follows . We design a compound objective under the SPEN framework to jointly train the \" trainingtime \" cost - augmented inference network and test - time inference network ( Section 3 ) . We propose shared parameterizations for the two inference networks so as to encourage them to capture complementary functionality while reducing the total number of trained parameters ( Section 3.1 ) . Quantitative and qualitative analysis shows clear differences in the characteristics of the two networks ( Table 3 ) . We present three methods to streamline and stabilize training that help with both the old and new objectives ( Section 4 ) . We propose global energy terms to capture long - distance dependencies and obtain further improvements ( Section 5 ) . While SPENs have been used for multiple NLP tasks , including multi - label classification ( Belanger and McCallum , 2016 ) , part - of - speech tagging ( Tu and Gimpel , 2018 ) , and semantic role labeling ( Belanger et al , 2017 ) , they are not widely used in NLP . Structured prediction is extremely common in NLP , but is typically approached using methods that are more limited than SPENs ( such as conditional random fields ) or models that suffer from a train / test mismatch ( such as most auto - regressive models ) . SPENs offer a maximally expressive framework for structured prediction while avoiding the train / test mismatch and therefore offer great potential for NLP . However , the training and inference have deterred NLP researchers . While we have found benefit from training inference networks for machine translation in recent work ( Tu et al , 2020b ) , that work assumed a fixed , pretrained energy function . Our hope is that the methods in this paper will enable SPENs to be applied to a larger set of applications , including generation tasks in the future .", "entities": [[38, 40, "TaskName", "structured prediction"], [44, 46, "TaskName", "structured prediction"], [117, 118, "MetricName", "accuracy"], [265, 266, "MetricName", "loss"], [432, 438, "TaskName", "part - of - speech tagging"], [439, 442, "TaskName", "named entity recognition"], [609, 613, "TaskName", "multi - label classification"], [621, 627, "TaskName", "part - of - speech tagging"], [636, 639, "TaskName", "semantic role labeling"], [655, 657, "TaskName", "Structured prediction"], [709, 711, "TaskName", "structured prediction"], [747, 749, "TaskName", "machine translation"]]}
{"text": "We denote the input space by X . For an input x X , we denote the structured output space by Y ( x ) . The entire space of structured outputs is denoted Y = \u222a x X Y ( x ) . A SPEN ( Belanger and McCallum , 2016 ) defines an energy function E \u0398 : X \u00d7 Y R parameterized by \u0398 that computes a scalar energy for an input / output pair . At test time , for a given input x , prediction is done by choosing the output with lowest energy : y = arg min y Y ( x ) E \u0398 ( x , y ) ( 1 ) However , solving equation ( 1 ) requires combinatorial algorithms because Y is a structured , discrete space . This becomes intractable when E \u0398 does not decompose into a sum over small \" parts \" of y. Belanger and McCallum ( 2016 ) relaxed this problem by allowing the discrete vector y to be continuous ; Y R denotes the relaxed output space . They solved the relaxed problem by using gradient descent to iteratively minimize the energy with respect to y. The energy function parameters \u0398 are trained using a structured hinge loss which requires repeated cost - augmented inference during training . Using gradient descent for the repeated costaugmented inference steps is time - consuming and makes learning unstable ( Belanger et al , 2017 ) . Tu and Gimpel ( 2018 ) replaced gradient descent with a neural network trained to do efficient inference . This \" inference network \" A \u03a8 : X Y R is parameterized by \u03a8 and trained with the goal that A \u03a8 ( x ) \u2248 arg min y Y R ( x ) E \u0398 ( x , y ) ( 2 ) When training the energy function parameters \u0398 , Tu and Gimpel ( 2018 ) replaced the cost - augmented inference step in the structured hinge loss from Belanger and McCallum ( 2016 ) with a costaugmented inference network F \u03a6 : F \u03a6 ( x ) \u2248 arg min y Y R ( x ) ( E \u0398 ( x , y ) \u2212 ( y , y * ) ) ( 3 ) where is a structured cost function that computes the distance between its two arguments . We use L1 distance for . This inference problem involves finding an output with low energy but high cost relative to the gold standard . Thus , it is not wellaligned with the test - time inference problem . Here is the specific objective to jointly train \u0398 ( parameters of the energy function ) and \u03a6 ( parameters of the cost - augmented inference network ) : min \u0398 max \u03a6 x i , y i D [ ( F \u03a6 ( x i ) , y i ) \u2212 E \u0398 ( x i , F \u03a6 ( x i ) ) + E \u0398 ( x i , y i ) ] + ( 4 ) where D is the set of training pairs , [ h ] + = max ( 0 , h ) , and is a structured cost function that computes the distance between its two arguments . Tu and Gimpel ( 2018 ) alternatively optimized \u0398 and \u03a6 , which is similar to training in generative adversarial networks ( Goodfellow et al , 2014 ) . The inference network is analogous to the generator and the energy function is analogous to the discriminator . As alternating optimization can be difficult in practice ( Salimans et al , 2016 ) , Tu & Gimpel experimented with including several additional terms in the above objective to stabilize training . After the training of the energy function , an inference network A \u03a8 for test - time prediction is finetuned with the goal shown in Eq . ( 2 ) . More specifically , for the fine - tuning step , we first initialize \u03a8 with \u03a6 ; next , we do gradient descent according to the following objective to learn \u03a8 : \u03a8 arg min \u03a8 x X E \u0398 ( x , A \u03a8 ( x ) ) where X is a set of training or validation inputs . It could also be the test inputs in a transductive setting . 3 An Objective for Joint Learning of Inference Networks One challenge with the above optimization problem is that it requires training a separate inference network A \u03a8 for test - time prediction after the energy function is trained . In this paper , we propose an alternative that trains the energy function and both inference networks jointly . In particular , we use a \" compound \" objective that combines two widely - used losses in structured prediction . We first present it without inference networks : min \u0398 x i , y i D max y ( ( y , y i ) \u2212E \u0398 ( x i , y ) + E \u0398 ( x i , y i ) ) + margin - rescaled hinge loss + \u03bb max y ( \u2212E \u0398 ( x i , y ) + E \u0398 ( x i , y i ) ) + perceptron loss ( 5 ) As indicated , this loss can be viewed as the sum of the margin - rescaled hinge and perceptron losses for SPENs . Two different inference problems are represented . The margin - rescaled hinge loss contains cost - augmented inference , shown as part of Eq . ( 3 ) . The perceptron loss contains the test - time inference problem , which is shown in Eq . ( 1 ) . Tu and Gimpel ( 2018 ) used a single inference network for solving both problems , so it was trained as a cost - augmented inference network during training and then fine - tuned as a test - time inference network afterward . We avoid this issue by training two inference networks , A \u03a8 for test - time inference and F \u03a6 for cost - augmented inference : min \u0398 max \u03a6 , \u03a8 x i , y i D [ ( F \u03a6 ( x i ) , y i ) \u2212E \u0398 ( x i , F \u03a6 ( x i ) ) + E \u0398 ( x i , y i ) ] + + \u03bb [ \u2212E \u0398 ( x i , A \u03a8 ( x i ) ) + E \u0398 ( x i , y i ) ] + ( 6 ) We treat this optimization problem as a minimax game and find a saddle point for the game similar to Tu and Gimpel ( 2018 ) and Goodfellow et al ( 2014 ) . We use minibatch stochastic gradient descent and alternately optimize \u0398 , \u03a6 , and \u03a8. The objective for the energy parameters \u0398 in minibatch M is : \u0398 arg min \u0398 x i , y i M ( F \u03a6 ( x i ) , y i ) \u2212E \u0398 ( x i , F \u03a6 ( x i ) ) + E \u0398 ( x i , y i ) + + \u03bb \u2212E \u0398 ( x i , A \u03a8 ( x i ) ) + E \u0398 ( x i , y i ) + When we remove 0 - truncation ( see Sec . 4.1 for the motivation ) , the objective for the inference network parameters in minibatch M is : \u03a8 , \u03a6 arg max \u03a8 , \u03a6 x i , y i M ( F \u03a6 ( x i ) , y i ) \u2212 E \u0398 ( x i , F \u03a6 ( x i ) ) \u2212 \u03bbE \u0398 ( x i , A \u03a8 ( x i ) )", "entities": [[58, 59, "HyperparameterName", "\u0398"], [66, 67, "HyperparameterName", "\u0398"], [110, 111, "HyperparameterName", "\u0398"], [143, 144, "HyperparameterName", "\u0398"], [206, 207, "HyperparameterName", "\u0398"], [213, 214, "MetricName", "loss"], [304, 305, "HyperparameterName", "\u0398"], [319, 320, "HyperparameterName", "\u0398"], [338, 339, "MetricName", "loss"], [370, 371, "HyperparameterName", "\u0398"], [449, 450, "HyperparameterName", "\u0398"], [471, 472, "HyperparameterName", "\u0398"], [494, 495, "HyperparameterName", "\u0398"], [508, 509, "HyperparameterName", "\u0398"], [537, 538, "DatasetName", "0"], [565, 566, "HyperparameterName", "\u0398"], [707, 708, "HyperparameterName", "\u0398"], [816, 818, "TaskName", "structured prediction"], [828, 829, "HyperparameterName", "\u0398"], [845, 846, "HyperparameterName", "\u0398"], [854, 855, "HyperparameterName", "\u0398"], [868, 869, "MetricName", "loss"], [875, 876, "HyperparameterName", "\u0398"], [884, 885, "HyperparameterName", "\u0398"], [895, 896, "MetricName", "loss"], [903, 904, "MetricName", "loss"], [934, 935, "MetricName", "loss"], [953, 954, "MetricName", "loss"], [1042, 1043, "HyperparameterName", "\u0398"], [1066, 1067, "HyperparameterName", "\u0398"], [1080, 1081, "HyperparameterName", "\u0398"], [1094, 1095, "HyperparameterName", "\u0398"], [1108, 1109, "HyperparameterName", "\u0398"], [1157, 1160, "MethodName", "stochastic gradient descent"], [1163, 1164, "HyperparameterName", "\u0398"], [1175, 1176, "HyperparameterName", "\u0398"], [1181, 1182, "HyperparameterName", "\u0398"], [1184, 1185, "HyperparameterName", "\u0398"], [1203, 1204, "HyperparameterName", "\u0398"], [1217, 1218, "HyperparameterName", "\u0398"], [1229, 1230, "HyperparameterName", "\u0398"], [1243, 1244, "HyperparameterName", "\u0398"], [1255, 1256, "DatasetName", "0"], [1307, 1308, "HyperparameterName", "\u0398"], [1321, 1322, "HyperparameterName", "\u0398"]]}
{"text": "If we were to train independent inference networks A \u03a8 and F \u03a6 , this new objective could be much slower than the approach of Tu and Gimpel ( 2018 ) . However , the compound objective offers several natural options for defining joint parameterizations of the two inference networks . We consider three options which are visualized in Figure 1 and described below : separated : F \u03a6 and A \u03a8 are two independent networks with their own architectures and parameters as shown in Figure 1 ( a ) . shared : F \u03a6 and A \u03a8 share a \" feature \" network as shown in Figure 1 ( b ) . We consider this option because both F \u03a6 and A \u03a8 are trained to produce output labels with low energy . However F \u03a6 also needs to produce output labels with high cost ( i.e. , far from the gold standard ) . stacked : the cost - augmented network F \u03a6 is a function of the output of the test - time network A \u03a8 and the gold standard output y. That is , F \u03a6 ( x , y ) = q ( A \u03a8 ( x ) , y ) where q is a parameterized function . This is depicted in Figure 1 ( c ) . We block the gradient at A \u03a8 when updating \u03a6. For the q function in the stacked option , we use an affine transformation on the concatenation of the inference network label distribution and the gold standard one - hot vector . That is , denoting the vector at position t of the cost - augmented network output by F \u03a6 ( x , y ) t , we have : F \u03a6 ( x , y ) t = softmax ( W q [ A \u03a8 ( x ) t ; y ( t ) ] + b q ) where semicolon ( ; ) is vertical concatenation , y ( t ) ( position t of y ) is an L - dimensional one - hot vector , A \u03a8 ( x ) t is the vector at position t of A \u03a8 ( x ) , W q is an L \u00d7 2L matrix , and b q is a bias . One motivation for these parameterizations is to reduce the total number of parameters in the procedure . Generally , the number of parameters is expected to decrease when moving from separated to shared to stacked . We will compare the three options empirically in our experiments , in terms of both accuracy and number of parameters . Another motivation , specifically for the third option , is to distinguish the two inference networks in terms of their learned functionality . With all three parameterizations , the cost - augmented network will be trained to produce an output that differs from the gold standard , due to the presence of the ( ) term in the combined objective . However , Tu and Gimpel ( 2018 ) found that the trained cost - augmented network was barely affected by fine - tuning for the test - time inference objective . This suggests that the cost - augmented network was mostly acting as a test - time inference network by the time of convergence . With the stacked parameterization , however , we explicitly provide the gold standard y to the cost - augmented network , permitting it to learn to change the predictions of the test - time network in appropriate ways to improve the energy function .", "entities": [[303, 304, "MethodName", "softmax"], [398, 401, "HyperparameterName", "number of parameters"], [408, 411, "HyperparameterName", "number of parameters"], [439, 440, "MetricName", "accuracy"], [441, 444, "HyperparameterName", "number of parameters"]]}
{"text": "Tu and Gimpel ( 2018 ) used the following objective for the cost - augmented inference network ( maximizing it with respect to \u03a6 ) : l 0 = [ ( F \u03a6 ( x ) , y ) \u2212 E \u0398 ( x , F \u03a6 ( x ) ) + E \u0398 ( x , y ) ] + where [ h ] + = max ( 0 , h ) . However , there are two potential reasons why l 0 will equal zero and trigger no gradient update . First , E \u0398 ( the energy function , corresponding to the discriminator in a GAN ) may already be well - trained , and it can easily separate the gold standard output from the costaugmented inference network output . Second , the cost - augmented inference network ( corresponding to the generator in a GAN ) could be so poorly trained that the energy of its output is very large , leading the margin constraints to be satisfied and l 0 to be zero . In standard margin - rescaled max - margin learning in structured prediction ( Taskar et al , 2004 ; Tsochantaridis et al , 2004 ) , the cost - augmented inference step is performed exactly ( or approximately with reasonable guarantee of effectiveness ) , ensuring that when l 0 is zero , the energy parameters are well trained . However , in our case , l 0 may be zero simply because the cost - augmented inference network is undertrained , which will be the case early in training . Then , when using zero truncation , the gradient of the inference network parameters will be 0 . This is likely why Tu and Gimpel ( 2018 ) found it important to add several stabilization terms to the l 0 objective . We find that by instead removing the truncation , learning stabilizes and becomes less dependent on these additional terms . Note that we retain the truncation at zero when updating the energy parameters \u0398. As shown in Figure 2 ( a ) , without any stabilization terms and with truncation , the inference network will barely move from its starting point and learning fails overall . However , without truncation , the inference network can work well even without any stabilization terms .", "entities": [[27, 28, "DatasetName", "0"], [41, 42, "HyperparameterName", "\u0398"], [53, 54, "HyperparameterName", "\u0398"], [69, 70, "DatasetName", "0"], [83, 84, "DatasetName", "0"], [96, 97, "HyperparameterName", "\u0398"], [108, 109, "MethodName", "GAN"], [148, 149, "MethodName", "GAN"], [174, 175, "DatasetName", "0"], [189, 191, "TaskName", "structured prediction"], [228, 229, "DatasetName", "0"], [246, 247, "DatasetName", "0"], [286, 287, "DatasetName", "0"], [309, 310, "DatasetName", "0"]]}
{"text": "Tu and proposed adding a local cross entropy ( CE ) loss , which is the sum of the label cross entropy losses over all positions in the sequence , to stabilize inference network training . We similarly find this term to help speed up convergence and improve accuracy . Figure 2 ( b ) shows faster convergence to high accuracy when adding the local CE term . See Section 7 for more details .", "entities": [[11, 12, "MetricName", "loss"], [48, 49, "MetricName", "accuracy"], [60, 61, "MetricName", "accuracy"]]}
{"text": "When training SPENs with inference networks , the inference network parameters are nested within the energy function . We found that the gradient components of the inference network parameters consequently have smaller absolute values than those of the energy function parameters . So , we alternate between k \u2265 1 steps of optimizing the inference network parameters ( \" I steps \" ) and one step of optimizing the energy function parameters ( \" E steps \" ) . We find this strategy especially helpful when using complex inference network architectures . To analyze , we compute the cost - augmented loss l 1 = ( F \u03a6 ( x ) , y ) \u2212 E \u0398 ( x , F \u03a6 ( x ) ) and the margin - rescaled hinge loss With k = 1 , the setting used by Tu and Gimpel ( 2018 ) , the inference network lags behind the energy , making the energy parameter updates very small , as shown by the small norms in Fig . 3 ( c ) . The inference network gradient norm ( Fig . 3 ( d ) ) remains high , indicating underfitting . However , increasing k too much also harms learning , as evi - denced by the \" plateau \" effect in the l 1 curves for k = 50 ; this indicates that the energy function is lagging behind the inference network . Using k = 5 leads to more of a balance between l 1 and l 0 and gradient norms that are mostly decreasing during training . We treat k as a hyperparameter that is tuned in our experiments . l 0 = [ ( F \u03a6 ( x ) , y ) \u2212 E \u0398 ( x , F \u03a6 ( x ) ) + E \u0398 ( x , y ) ] + averaged There is a potential connection between our use of multiple I steps and a similar procedure used in GANs ( Goodfellow et al , 2014 ) . In the GAN objective , the discriminator D is updated in the inner loop , and they alternate between multiple update steps for D and one update step for G. In this section , we similarly found benefit from multiple steps of inner loop optimization for every step of the outer loop . However , the analogy is limited , since GAN training involves sampling noise vectors and using them to generate data , while there are no noise vectors or explicitly - generated samples in our framework .", "entities": [[101, 102, "MetricName", "loss"], [116, 117, "HyperparameterName", "\u0398"], [132, 133, "MetricName", "loss"], [134, 136, "HyperparameterName", "k ="], [224, 226, "HyperparameterName", "k ="], [242, 244, "HyperparameterName", "k ="], [256, 257, "DatasetName", "0"], [281, 282, "DatasetName", "0"], [295, 296, "HyperparameterName", "\u0398"], [307, 308, "HyperparameterName", "\u0398"], [345, 346, "MethodName", "GAN"], [404, 405, "MethodName", "GAN"]]}
{"text": "For our sequence labeling experiments in this paper , the input x is a length - T sequence of tokens , and the output y is a sequence of labels of length T . We use y t to denote the output label at position t , where y t is a vector of length L ( the number of labels in the label set ) and where y t , j is the jth entry of the vector y t . In the original output space Y ( x ) , y t , j is 1 for a single j and 0 for all others . In the relaxed output space Y R ( x ) , y t , j can be interpreted as the probability of the tth position being labeled with label j. We then use the following energy for sequence labeling ( Tu and Gimpel , 2018 ) : E \u0398 ( x , y ) = \u2212 T t=1 L j=1 y t , j U j b ( x , t ) + T t=1 y t\u22121 W y t ( 7 ) where U j R d is a parameter vector for label j and the parameter matrix W R L\u00d7L contains label - pair parameters . Also , b ( x , t ) R d denotes the \" input feature vector \" for position t. We define b to be the d - dimensional BiLSTM ( Hochreiter and Schmidhuber , 1997 ) hidden vector at t. The full set of energy parameters \u0398 includes the U j vectors , W , and the parameters of the BiLSTM . Global Energies for Sequence Labeling . In addition to new training strategies , we also experiment with several global energy terms for sequence labeling . Eq . ( 7 ) shows the base energy , and to capture long - distance dependencies , we include global energy ( GE ) terms in the form of Eq . ( 8 ) . We use h to denote an LSTM tag language model ( TLM ) that takes a sequence of labels as input and returns a distribution over next labels . We define y t = h ( y 0 , . . . , y t\u22121 ) to be the distribution given the preceding label vectors ( under a LSTM language model ) . Then , the energy term is : E TLM ( y ) = \u2212 T +1 t=1 log y t y t ( 8 ) where y 0 is the start - of - sequence symbol and y T +1 is the end - of - sequence symbol . This energy returns the negative log - likelihood under the TLM of the candidate output y. Tu and Gimpel ( 2018 ) pretrained their h on a large , automatically - tagged corpus and fixed its parameters when optimizing \u0398. Our approach has one critical difference . We instead do not pretrain h , and its parameters are learned when optimizing \u0398. We show that even without pretraining , our global energy terms are still able to capture useful additional information . We also propose new global energy terms . Define y t = h ( y 0 , . . . , y t\u22121 ) where h is an LSTM TLM that takes a sequence of labels as input and returns a distribution over next labels . First , we add a TLM in the backward direction ( denoted y t analogously to the forward TLM ) . Second , we include words as additional inputs to forward and backward TLMs . We define y t = g ( x 0 , ... , x t\u22121 , y 0 , ... , y t\u22121 ) where g is a forward LSTM TLM . We define the backward version similarly ( denoted y t ) . The global energy is therefore E GE ( y ) = \u2212 T +1 t=1 log ( y t y t ) + log ( y t y t ) + \u03b3 log ( y t y t ) + log ( y t y t ) ( 9 ) Here \u03b3 is a hyperparameter that is tuned . We experiment with three settings for the global energy : GE ( a ) : forward TLM as in Tu and Gimpel ( 2018 ) ; GE ( b ) : forward and backward TLMs ( \u03b3 = 0 ) ; GE ( c ) : all four TLMs in Eq . ( 9 ) .", "entities": [[103, 104, "DatasetName", "0"], [156, 157, "HyperparameterName", "\u0398"], [245, 246, "MethodName", "BiLSTM"], [263, 264, "HyperparameterName", "\u0398"], [277, 278, "MethodName", "BiLSTM"], [346, 347, "MethodName", "LSTM"], [377, 378, "DatasetName", "0"], [398, 399, "MethodName", "LSTM"], [430, 431, "DatasetName", "0"], [457, 460, "MetricName", "log - likelihood"], [549, 550, "DatasetName", "0"], [562, 563, "MethodName", "LSTM"], [623, 624, "DatasetName", "0"], [631, 632, "DatasetName", "0"], [643, 644, "MethodName", "LSTM"], [689, 690, "HyperparameterName", "\u03b3"], [709, 710, "HyperparameterName", "\u03b3"], [753, 754, "HyperparameterName", "\u03b3"], [755, 756, "DatasetName", "0"]]}
{"text": "We consider two sequence labeling tasks : Twitter part - of - speech ( POS ) tagging ( Gimpel et al , 2011 ) and named entity recognition ( NER ; Tjong Kim Sang and De Meulder , 2003 ) . Twitter Part - of - Speech ( POS ) Tagging . We use the Twitter POS data from Gimpel et al ( 2011 ) and Owoputi et al ( 2013 ) which contain 25 tags . We use 100 - dimensional skip - gram ( Mikolov et al , 2013 ) embeddings from Tu et al ( 2017 ) . Like Tu and Gimpel ( 2018 ) , we use a BiLSTM to compute the input feature vector for each position , using hidden size 100 . We also use BiLSTMs for the inference networks . The output of the inference network is a softmax function , so the inference network will produce a distribution over labels at each position . The \u2206 is L1 distance . We train the inference network using stochastic gradient descent ( SGD ) with momentum and train the energy parameters using Adam ( Kingma and Ba , 2014 ) . We also explore training the inference network using Adam when not using the local CE loss . 1 In experiments with the local CE term , its weight is set to 1 . Named Entity Recognition ( NER ) . We use the CoNLL 2003 English dataset ( Tjong Kim Sang and De Meulder , 2003 ) . We use the BIOES tagging scheme , following previous work ( Ratinov and Roth , 2009 ) , resulting in 17 NER labels . We use 100 - dimensional pretrained GloVe embeddings ( Pennington et al , 2014 ) . The task is evaluated using F1 score computed with the conlleval script . The architectures for the feature networks in the energy function and inference networks are all BiLSTMs . The architectures for tag language models are LSTMs . We use a dropout keep - prob of 0.7 for all LSTM cells . The hidden size for all LSTMs is 128 . We use Adam ( Kingma and Ba , 2014 ) and do early stopping on the development set . We use a learning rate of 5 10 \u22124 . Similar to above , the weight for the CE term is set to 1 . We consider three NER modeling configurations . NER uses only words as input and pretrained , fixed Tu and Gimpel ( 2018 ) . The inference network architecture is a one - layer BiLSTM . GloVe embeddings . NER+ uses words , the case of the first letter , POS tags , and chunk labels , as well as pretrained GloVe embeddings with fine - tuning . NER++ includes everything in NER+ as well as character - based word representations obtained using a convolutional network over the character sequence in each word . Unless otherwise indicated , our SPENs use the energy in Eq . ( 7 ) .", "entities": [[8, 11, "DatasetName", "part - of"], [25, 28, "TaskName", "named entity recognition"], [29, 30, "TaskName", "NER"], [42, 45, "DatasetName", "Part - of"], [112, 113, "MethodName", "BiLSTM"], [145, 146, "MethodName", "softmax"], [174, 177, "MethodName", "stochastic gradient descent"], [178, 179, "MethodName", "SGD"], [188, 189, "MethodName", "Adam"], [205, 206, "MethodName", "Adam"], [212, 213, "MetricName", "loss"], [230, 233, "TaskName", "Named Entity Recognition"], [234, 235, "TaskName", "NER"], [240, 242, "DatasetName", "CoNLL 2003"], [276, 277, "TaskName", "NER"], [285, 287, "MethodName", "GloVe embeddings"], [300, 302, "MetricName", "F1 score"], [345, 346, "MethodName", "LSTM"], [359, 360, "MethodName", "Adam"], [369, 371, "MethodName", "early stopping"], [379, 381, "HyperparameterName", "learning rate"], [404, 405, "TaskName", "NER"], [408, 409, "TaskName", "NER"], [434, 435, "MethodName", "BiLSTM"], [436, 438, "MethodName", "GloVe embeddings"], [461, 463, "MethodName", "GloVe embeddings"]]}
{"text": "Effect of Zero Truncation and Local CE Loss . Table 1 shows results for zero truncation and the local CE term . Training fails for both tasks when using zero truncation without CE . Removing truncation makes learning succeed and leads to effective models even without using CE . However , when using the local CE term , truncation has little effect on performance . The importance of CE in prior work ( Tu and Gimpel , 2018 ) is likely due to the fact that truncation was being used . The local CE term is useful for both tasks , though it appears more helpful for tagging . 2 This may be because POS tagging is a more local task . Regardless , for both tasks , as shown in Section 4.2 , the inclusion of the CE term speeds convergence and improves training stability . For example , on NER , using the CE term reduces the number of epochs chosen by early stopping from \u223c100 to \u223c25 . For POS , using the CE term reduces the number of epochs from \u223c150 to \u223c60 . Effect of Compound Objective and Joint Parameterizations . The compound objective is the sum of the margin - rescaled and perceptron losses , and outperforms them both ( see Table 2 ) . Across all tasks , the shared and stacked parameterizations are more accurate than the previous objectives . For the separated parameterization , the performance drops slightly for NER , likely due to the larger number of parameters . The shared and stacked options have fewer parameters to train than the separated option , and the stacked version processes examples at the fastest rate during training . The top part of Table 3 shows how the performance of the test - time inference network A \u03a8 and the cost - augmented inference network F \u03a6 vary when using the new compound objective . The differences between F \u03a6 and A \u03a8 are larger than in the baseline configuration , showing that the two are learning complementary functionality . With the stacked parameterization , the cost - augmented network F \u03a6 receives as an additional input the gold standard label sequence , which leads to the largest differences as the cost - augmented network can explicitly favor incorrect labels . 3 The bottom part of Table 3 shows qualitative differences between the two inference networks . On the POS development set , we count the differences between the predictions of A \u03a8 and F \u03a6 when A \u03a8 makes the correct prediction . 4 F \u03a6 tends to output tags that are highly confusable with those output by A \u03a8 . For example , it often outputs proper noun when the gold standard is common noun or vice versa . It also captures the ambiguities among adverbs , adjectives , and prepositions . Global Energies . The results are shown in Table 4 . Adding the backward ( b ) and word - augmented TLMs ( c ) improves over using only the forward TLM from Tu and Gimpel ( 2018 ) . With the global energies , our performance is comparable to several strong results ( 90.94 of Lample et al , 2016 and91.37 of Ma andHovy , 2016 ) . However , it is still lower than the state of the art ( Akbik et al , 2018 ; Devlin et al , 2019 ) , likely due to the lack of contextualized embeddings . In other work , we proposed and evaluated several other high - order energy terms for sequence labeling using our framework ( Tu et al , 2020a ) .", "entities": [[151, 152, "TaskName", "NER"], [159, 162, "HyperparameterName", "number of epochs"], [164, 166, "MethodName", "early stopping"], [180, 183, "HyperparameterName", "number of epochs"], [248, 249, "TaskName", "NER"], [255, 258, "HyperparameterName", "number of parameters"]]}
{"text": "There are several efforts aimed at stabilizing and improving learning in generative adversarial networks ( GANs ) ( Goodfellow et al , 2014 ; Salimans et al , 2016 ; Zhao et al , 2017 ; from overcoming learning difficulties by modifying loss functions and optimization , and GANs have become more successful and popular as a result . Notably , Wasserstein GANs provided the first convergence measure in GAN training using Wasserstein distance . To compute Wasserstein distance , the discriminator uses weight clipping , which limits network capacity . Weight clipping was subsequently replaced with a gradient norm constraint ( Gulrajani et al , 2017 ) . Miyato et al ( 2018 ) proposed a novel weight normalization technique called spectral normalization . These methods may be applicable to the similar optimization problems solved in learning SPENs . Another direction may be to explore alternative training objectives for SPENs , such as those that use weaker supervision than complete structures ( Rooshenas et al , 2018 ( Rooshenas et al , , 2019Naskar et al , 2020 ) .", "entities": [[42, 43, "MetricName", "loss"], [69, 70, "MethodName", "GAN"], [118, 120, "MethodName", "weight normalization"], [122, 124, "MethodName", "spectral normalization"]]}
{"text": "We linearize the constituency parsing outputs , similar to Tran et al ( 2018 ) . We use the following equation plus global energy in the form of Eq . ( 8 ) as the energy function : E \u0398 ( x , y ) = \u2212 T t=1 L j=1 y t , j U j b ( x , t ) + T t=1 y t\u22121 W y t Here , b has a seq2seq - with - attention architecture identical to Tran et al ( 2018 ) . In particular , here is the list of implementation decisions . We can write b = g f where f ( which we call the \" feature network \" ) takes in an input sentence , passes it through the encoder , and passes the encoder output to the decoder feature layer to obtain hidden states ; g takes in the hidden states and passes them into the rest of the layers in the decoder . In our experiments , the cost - augmented inference network F \u03a6 , test - time inference network A \u03a8 , and b of the energy function above share the same feature network ( defined as f above ) . The feature network ( f ) component of b is pretrained using the feed - forward local crossentropy objective . The cost - augmented inference network F \u03a6 and the test - time inference network A \u03a8 are both pretrained using the feed - forward local cross - entropy objective . The seq2seq baseline achieves 82.80 F1 on the development set in our replication of Tran et al ( 2018 ) . Using a SPEN with our stacked parameterization , we obtain 83.22 F1 .", "entities": [[3, 5, "TaskName", "constituency parsing"], [39, 40, "HyperparameterName", "\u0398"], [76, 77, "MethodName", "seq2seq"], [259, 260, "MethodName", "seq2seq"], [263, 264, "MetricName", "F1"], [290, 291, "MetricName", "F1"]]}
{"text": "Subtask 1 requires a detection model that uses only the textual features of the meme content and detects which of the 20 propaganda techniques were used . This is a multi - label classification problem for text , based on the pre - trained ALBERT model and added a Text - CNN layer . As illustrated in Figure 2 , the proposed model includes an ALBERT layer , a Text - CNN layer , a fully connected layer , and an output layer . ALBERT ( Lan et al , 2020 ) is a lite BERT for self - supervised learning of language representations , which uses layer - to - layer parameter sharing to reduce the number of parameters of the model , which not only speeds up the model training but also outperforms BERT on certain datasets . With our model , the pretrained ALBERT model is fine - tuned to obtain a 512 \u00d7 768 hidden representation matrix for subsequent multi - label classification of text . Text - CNN ( Kim , 2014 ) is a convolutional neural network applied to a text classification task , using multiple kernels of different sizes to extract key information in sentences , and is thus able to better capture the local relevance . In this layer , we used three different sizes of onedimensional convolution kernels , i.e. , 3 , 4 , and 5 , to extract information from the hidden representation matrix output from the ALBERT layer for the final multi - label text classification .", "entities": [[30, 34, "TaskName", "multi - label classification"], [44, 45, "MethodName", "ALBERT"], [65, 66, "MethodName", "ALBERT"], [84, 85, "MethodName", "ALBERT"], [95, 96, "MethodName", "BERT"], [97, 101, "TaskName", "self - supervised learning"], [117, 120, "HyperparameterName", "number of parameters"], [135, 136, "MethodName", "BERT"], [146, 147, "MethodName", "ALBERT"], [163, 167, "TaskName", "multi - label classification"], [186, 188, "TaskName", "text classification"], [225, 226, "MethodName", "convolution"], [248, 249, "MethodName", "ALBERT"], [253, 258, "TaskName", "multi - label text classification"]]}
{"text": "All models used the TensorFlow2 backend , and all BERT - based models were implemented using the HuggingFace Transformers toolkit ( Wolf et al , 2020 ) . The Adam optimizer ( Ba and Kingma , 2015 ) was used to update all trainable parameters . The loss functions in subtasks 1 and 3 were binary cross - entropy , and subtask 2 was categorical cross - entropy . The hyper - parameters in the model training process were obtained using a grid - search strategy , as shown in Table 1 . Once the optimal settings of the parameters were obtained , they were used for classification on the test sets of different corpora .", "entities": [[9, 10, "MethodName", "BERT"], [29, 30, "MethodName", "Adam"], [30, 31, "HyperparameterName", "optimizer"], [47, 48, "MetricName", "loss"]]}
{"text": "Table 2 presents the results of Subtask 1 . We conducted experiments on several pre - trained models including BERT , RoBERTa ( Liu et al , 2019 ) , and ALBERT combined with the Text - CNN layer , and observed that the ALBERT and Text - CNN models achieved the best performance , the reason for which may be that the training datasets are small , and a serious overfitting will occur by directly finetuning BERT . Furthermore , the experiments show that the ALBERT model has fewer parameters and performs better on small datasets . Adding a Text - CNN layer after the BERT - based model can better extract the local relevance information of the text , which not only effectively alleviates the overfitting phenomenon it also effectively improves the model performance . In subtask 2 , the results of our proposed multitask sequence labeling model on the dev set are F 1 - score of 0.215 , Precision of 0.378 , and Recall of 0.151 . The results on the test set are F 1 - score of 0.091 , Precision of 0.186 , and Recall of 0.061 . Table 3 shows the results of Subtask 3 . It can be observed that ResNet18 works better than VGG16 when using both ALBERT and ALBERT - Text - CNN models . The performance was improved by adding a Text - CNN layer to the text channel . Considering that the micro F 1 - scores are relatively close , we selected the models with the top - three F 1 - scores and used hard voting to generate the results for comparison . For all three subtasks , the proposed systems achieved micro F 1 - scores of 0.492 , 0.091 , and 0.446 on the test set , respectively . The results of all models exceeded the baseline . However , there is a considerable decrease compared to the scores of 0.625 , 0.215 , and 0.636 achieved on the dev set .", "entities": [[19, 20, "MethodName", "BERT"], [21, 22, "MethodName", "RoBERTa"], [31, 32, "MethodName", "ALBERT"], [44, 45, "MethodName", "ALBERT"], [77, 78, "MethodName", "BERT"], [86, 87, "MethodName", "ALBERT"], [106, 107, "MethodName", "BERT"], [162, 163, "MetricName", "Precision"], [167, 168, "MetricName", "Recall"], [185, 186, "MetricName", "Precision"], [190, 191, "MetricName", "Recall"], [216, 217, "MethodName", "ALBERT"], [218, 219, "MethodName", "ALBERT"]]}
{"text": "Mclarty et al ( 2018 ) trained a Support Vector Machine ( SVM ) on pre - vocalic /r/ and vowels , and their approach did quite well in classifying prevocalic /r / s. They then took this pre - trained model and applied it to classifying postvocalic /r/ tokens , which classified 84 % as vowels , and 15 % as /r/. As they describe , this is likely because all postvocalic segments still contain vowel - like properties ; furthermore , their training set excluded postvocalic /r/ so the accuracy is expected to decrease . However , their method did not perform as well in comparison to humans . On tokens where there was no ground truth , humans only agreed with the SVM classification about 55 % of the time .", "entities": [[8, 11, "MethodName", "Support Vector Machine"], [12, 13, "MethodName", "SVM"], [91, 92, "MetricName", "accuracy"], [125, 126, "MethodName", "SVM"]]}
{"text": "In early testing , we attempted classification into rful , r - less , and unknown , but this did not provide strong results so we simplified to a binary classification . From the beginning of this project , we knew we wanted to use a machine learning approach , so before using neural networks we tried some easier classifiers . However , we did not get encouraging results . For example , our Random Forest Classifier only gave about 54 % accuracy . When we tried simpler neural networks , these gave much more promising results to we chose to pursue this method .", "entities": [[82, 83, "MetricName", "accuracy"]]}
{"text": "Following standard methods of Automatic Speech Recognition , we converted the audio to 12 Mel - Frequency - Cepstral - Coefficients ( MFCCs ) . We used the 12 MFCCs , similar to Mclarty et al . For each vowel+ ( r ) sequence , we normalized across the length to extract 100 time - points per token , as shown in figure 1 . In the training , MFCCs were more effective than traditional sociophonetic /r/ correlates F2 and F3 ( Thomas , 2011 ) . These samples were used in the model architecture as shown in figure 1 , where there are 100 samples for each vowel + /r/ sequence . The Gated Recurrent Unit is shown in more detail in figure 2 , where we can see the input from the previous timestep and layer , and how this is filtered through gates using tanh and sigmoid activation functions . Importantly , no work on coding rhoticity has made use of Recurrent Neural Networks , and we believe our methods are a promising step . We used Gated Recurrent Units Chung et al , 2014 ) to train our system to classify vowel+ ( r ) tokens as r - ful or r - less . Following standard methods in machine - learning , we split the data in order to train with 80 % of the data and test with 20 % . We chose hyperparameters based on a grid search using 3 - fold cross validation ( only 3 due to the small dataset ) . We saved the test set to validate results . The hidden layer size was 50 nodes , and dense layer size was 200 nodes . For regularization we used a kernel L2 regularization for the dense layer and we used both activation L2 and Recurrent L2 for the GRU layer . All of the alphas for this regularization are 0.01 . The optimization method was RMSprop , and the learning rate was 0.001 .", "entities": [[4, 7, "TaskName", "Automatic Speech Recognition"], [114, 117, "MethodName", "Gated Recurrent Unit"], [149, 151, "MethodName", "sigmoid activation"], [271, 274, "HyperparameterName", "hidden layer size"], [292, 294, "HyperparameterName", "L2 regularization"], [309, 310, "MethodName", "GRU"], [326, 327, "MethodName", "RMSprop"], [330, 332, "HyperparameterName", "learning rate"]]}
{"text": "In figure 3 , we see the Normalized Confusion Matrix , which summarizes our results by lining up true labels and predicted labels for our rhotic and non - rhotic tokens . We consider this binary classification either rhotic ( positive ) or non - rhotic ( negative ) . In this way we can see the proportion of true positives ( predicted to be rhotic and indeed truly rhotic ) , false positive ( predicted to be rhotic but actually non - rhotic ) , true negative ( predicted to be non - rhotic and actually non - rhotic ) , and false negative ( predicted to be non - rhotic and actually rhotic ) . In deciding which model to use , we tried a few different configurations . We used the sampled MFCCs ( as described earlier , figure 1 ) as well as Bark measurements that were extracted also at 100 time - points across the vowel . Because our MFCC data is multi - dimensional and time - dependent , we wanted to see how a Convolutional Neural Network would perform ( table 1 ) , but it turned out not to be as high in performance as our earlier model . Figure 4 shows the Receiver Operating Characteristic ( ROC ) for our model ( created using scikitlearn ) , which is fairly good by machine learning standards . The Area Under the Curve ( AUC , as noted in Table 1 ) is 0.892 , and as evident from the graph , is much closer to 1 . Our system had 81.1 % accuracy with the human analysts in judging tokens as r - less or r - ful , scoring 0.829 for F - measure . We also used the Heselwood et al approach ( section 3.1 ) of classifying front or back vowels to see how accurately it would perform on the same test dataset . This classification gave an average speaker accuracy of 63.3 % and an average token accuracy of 62.1 % ( Table 2 ) , much lower than our best model 's overall accuracy ( i.e. average across all tokens ) of 81.1 % ( Table 1 ) . Average Speaker Accuracy 63.3 % Average Token Accuracy 62.1 %", "entities": [[241, 242, "MetricName", "AUC"], [270, 271, "MetricName", "accuracy"], [290, 293, "MetricName", "F - measure"], [331, 332, "MetricName", "accuracy"], [339, 340, "MetricName", "accuracy"], [355, 357, "MetricName", "overall accuracy"], [374, 375, "MetricName", "Accuracy"], [379, 380, "MetricName", "Accuracy"]]}
{"text": "The initial results of this study are promising . Our results are quite strong , as shown by the metrics in Table 1 . When testing the Heselwood et al approach ( Table 2 ) , it only predicted correctly approximately 60 % of the time ; our model performs significantly better , at an accuracy of 81.1 % ( Table 1 ) . It seems that we are also slightly better at predicting rhotic tokens than non - rhotic ( Figure 3 ) , which likely has to do with the fact that we have more rhotic tokens in total . We aimed to reach human levels - considering that analyst agreement is 89.9 % for our dataset ( as mentioned above ) , our accuracy of 81.1 % is quite good . However , these numbers are not strictly comparable as we discarded tokens that proved difficult for human analysts . In future development of this method , we want to consider any sources of error on our part . For example , some audio and text files could be misaligned so we might consider hand - correcting these alignments . However , the nature of the neural network could correct for this in that it learns to forget irrelevant or noisy data . By gathering more data , we would expect that our accuracy would improve and eventually reach a plateau where additional speakers would not affect anything . Additionally , a study that involves cross - corpus analysis could provide greater insight into how this model might be applicable on a larger scale , and how well our model actually performs . Furthermore , if we had 3 analysts rather than 2 , we could have used a majority vote for classifying tokens , and would not have to discard tokens where rhoticity was ambiguous . A shortcoming of this study is that it only involves speech that is elicited through readingideally future studies would involve free speech in order to use more natural speech . R - dropping is a crucial sociolinguistic variable for English dialect research in the US Northeast , Great Britain , Australia , New Zealand , Singapore , and other locations . Our neural network model takes a significant step toward automation of this key variable . In the future , we will continue optimizing and improving our model . Other groups have studied automated methods for coding sociolinguistic variables ( Yuan and Liberman , 2011 ; Bailey , 2016 ) , and there are great ideas to be found in these works . When automated methods for rhoticity reach the accuracy level of humans , along with consistency and full replicability , this will open the floodgates to large amounts of /r/ data and greatly expand sociolinguistic knowledge of dialect variation around the world , efficiently allowing studies to be replicated across research groups .", "entities": [[55, 56, "MetricName", "accuracy"], [126, 127, "MetricName", "accuracy"], [226, 227, "MetricName", "accuracy"], [440, 441, "MetricName", "accuracy"]]}
{"text": "We introduce PubMedQA , a novel biomedical question answering ( QA ) dataset collected from PubMed abstracts . The task of Pub - MedQA is to answer research questions with yes / no / maybe ( e.g. : Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting ? ) using the corresponding abstracts . PubMedQA has 1k expert - annotated , 61.2k unlabeled and 211.3k artificially generated QA instances . Each PubMedQA instance is composed of ( 1 ) a question which is either an existing research article title or derived from one , ( 2 ) a context which is the corresponding abstract without its conclusion , ( 3 ) a long answer , which is the conclusion of the abstract and , presumably , answers the research question , and ( 4 ) a yes / no / maybe answer which summarizes the conclusion . Pub - MedQA is the first QA dataset where reasoning over biomedical research texts , especially their quantitative contents , is required to answer the questions . Our best performing model , multi - phase fine - tuning of BioBERT with long answer bag - of - word statistics as additional supervision , achieves 68.1 % accuracy , compared to single human performance of 78.0 % accuracy and majority - baseline of 55.2 % accuracy , leaving much room for improvement . PubMedQA is publicly available at https://pubmedqa.github.io .", "entities": [[2, 3, "DatasetName", "PubMedQA"], [7, 9, "TaskName", "question answering"], [56, 57, "DatasetName", "PubMedQA"], [73, 74, "DatasetName", "PubMedQA"], [205, 206, "MetricName", "accuracy"], [215, 216, "MetricName", "accuracy"], [223, 224, "MetricName", "accuracy"], [231, 232, "DatasetName", "PubMedQA"]]}
{"text": "We fine - tune BioBERT on Pub - MedQA as a baseline . BioBERT is initialized with BERT ( Devlin et al , 2018 ) and further pretrained on PubMed abstracts and PMC 7 articles . Expectedly , it vastly outperforms BERT in various biomedical NLP tasks . We denote the original transformer weights of BioBERT as \u03b8 0 . While fine - tuning , we feed PubMedQA questions and contexts ( or long answers ) , separated 7 https://www.ncbi.nlm.nih.gov/pmc/ by the special [ SEP ] token , to BioBERT . The yes / no / maybe labels are predicted using the special [ CLS ] embedding using a softmax function . Cross - entropy loss of predicted and true label distribution is denoted as L QA .", "entities": [[17, 18, "MethodName", "BERT"], [41, 42, "MethodName", "BERT"], [57, 58, "HyperparameterName", "\u03b8"], [58, 59, "DatasetName", "0"], [67, 68, "DatasetName", "PubMedQA"], [109, 110, "MethodName", "softmax"], [115, 116, "MetricName", "loss"]]}
{"text": "Under reasoning - required setting , long answers are available in training but not inference phase . We use them as an additional signal for training : similar to Ma et al ( 2018 ) regularizing neural machine translation models with binary bag - of - word ( BoW ) statistics , we fine - tune BioBERT with an auxiliary task of predicting the binary BoW statistics of the long answers , also using the special [ CLS ] embedding . We minimize binary crossentropy loss of this auxiliary task : L BoW = \u2212 1 N i b i logb i + ( 1 \u2212 b i ) log ( 1 \u2212b i ) where b i andb i are ground - truth and predicted probability of whether token i is in the long answers ( i.e. : b i { 0 , 1 } andb i [ 0 , 1 ] ) , and N is the BoW vocabulary size . The total loss is : L = L QA + \u03b2L BoW", "entities": [[37, 39, "TaskName", "machine translation"], [85, 86, "MetricName", "loss"], [142, 143, "DatasetName", "0"], [149, 150, "DatasetName", "0"], [165, 166, "MetricName", "loss"]]}
{"text": "Training ( Reasoning - required ) ( q A , c A ) , l A ( q U , c U ) , l U pseudo ( q L , c L ) , l L ( q A , a A ) , l A ( q L , a L ) , l L ( q U , a U ) \u2713 0 \u2713 0 \u2713 I \u2713 II \u2713 F \u2713 B 1 \u2713 B 2 Fine - tuning Supervision Pseudolabeling eq . ( 1 ) eq . ( 2 ) eq . ( 3 ) eq . ( 5 ) eq . ( 6 ) eq . ( 4 In reasoning - free setting which we use for bootstrapping , the regularization coefficient \u03b2 is set to 0 because long answers are directly used as input .", "entities": [[65, 66, "DatasetName", "0"], [67, 68, "DatasetName", "0"], [128, 129, "HyperparameterName", "\u03b2"], [132, 133, "DatasetName", "0"]]}
{"text": "Since PQA - A and PQA - U have different properties from the ultimate test set of PQA - L , BioBERT is fine - tuned in a multi - phase style on different subsets . Fig . 5 shows the architecture of this training schedule . We use q , c , a , l to denote question , context , long answer and yes / no / maybe label of instances , respectively . Their source subsets are indexed by the superscripts of A for PQA - A , U for PQA - U and L for PQA - L. Phase I Fine - tuning on PQA - A : PQA - A is automatically collected whose questions and labels are artificially generated . As a result , questions of PQA - A might differ a lot from those of PQA - U and PQA - L , and it only has yes / no labels with a very imbalanced distribution ( 92.8 % yes v.s. 7.2 % no ) . Despite these drawbacks , PQA - A has substantial training instances so models could still benefit from it as a pre - training step . Thus , in Phase I of multi - phase fine - tuning , we initialize BioBERT with \u03b8 0 , and fine - tune it on PQA - A using question and context as input : \u03b8 I argmin \u03b8 L ( BioBERT \u03b8 ( q A , c A ) , l A ) ( 1 ) Phase II Fine - tuning on Bootstrapped PQA - U : To fully utilize the unlabeled instances in PQA - U , we exploit the easiness of reasoning - free setting to pseudo - label these instances with a bootstrapping strategy : first , we initialize BioBERT with \u03b8 0 , and fine - tune it on PQA - A using question and long answer ( reasoning - free ) , \u03b8 B 1 argmin \u03b8 L ( BioBERT \u03b8 ( q A , a A ) , l A ) ( 2 ) then we further fine - tune BioBERT \u03b8 B 1 on PQA - L , also under the reasoning - free setting : \u03b8 B 2 argmin \u03b8 L ( BioBERT \u03b8 ( q L , a L ) , l L ) ( 3 ) We pseudo - label PQA - U instances using the most confident predictions of BioBERT \u03b8 B 2 for each class . Confidence is simply defined by the corresponding softmax probability and then we label a subset which has the same proportions of yes / no / maybe labels as those in the PQA - L : l U pseudo BioBERT \u03b8 B 2 ( q U , a U ) ( 4 ) In phase II , we fine - tune BioBERT \u03b8 I on the bootstrapped PQA - U using question and context ( under reasoning - required setting ) : \u03b8 II argmin \u03b8 L ( BioBERT \u03b8 ( q U , c U ) , l U pseudo ) ( 5 ) Final Phase Fine - tuning on PQA - L : In the final phase , we fine - tune BioBERT \u03b8 II on PQA - L : \u03b8 F argmin \u03b8 L ( BioBERT \u03b8 ( q L , c L ) , l L ) ( 6 ) Final predictions on instances of PQA - L validation and test sets are made using BioBERT \u03b8 F : l pred = BioBERT \u03b8 F ( q L , c L )", "entities": [[215, 216, "HyperparameterName", "\u03b8"], [216, 217, "DatasetName", "0"], [234, 235, "HyperparameterName", "\u03b8"], [237, 238, "HyperparameterName", "\u03b8"], [241, 242, "HyperparameterName", "\u03b8"], [304, 305, "HyperparameterName", "\u03b8"], [305, 306, "DatasetName", "0"], [327, 328, "HyperparameterName", "\u03b8"], [331, 332, "HyperparameterName", "\u03b8"], [335, 336, "HyperparameterName", "\u03b8"], [357, 358, "HyperparameterName", "\u03b8"], [373, 374, "HyperparameterName", "\u03b8"], [377, 378, "HyperparameterName", "\u03b8"], [381, 382, "HyperparameterName", "\u03b8"], [411, 412, "HyperparameterName", "\u03b8"], [425, 426, "MethodName", "softmax"], [457, 458, "HyperparameterName", "\u03b8"], [479, 480, "HyperparameterName", "\u03b8"], [499, 500, "HyperparameterName", "\u03b8"], [502, 503, "HyperparameterName", "\u03b8"], [506, 507, "HyperparameterName", "\u03b8"], [542, 543, "HyperparameterName", "\u03b8"], [549, 550, "HyperparameterName", "\u03b8"], [552, 553, "HyperparameterName", "\u03b8"], [556, 557, "HyperparameterName", "\u03b8"], [587, 588, "HyperparameterName", "\u03b8"], [594, 595, "HyperparameterName", "\u03b8"]]}
{"text": "Human performance is measured during the annotation : As shown in Algorithm 1 , annotations of annotator 1 and annotator 2 are used to calculate reasoning - free and reasoning - required human performance , respectively , against the discussed ground truth labels . Human performance on the test set of PQA - L is shown in Table 4 . We only test single - annotator performance due to limited resources . show that an ensemble of annotators perform significantly better than single - annotator , so the results reported in Table 4 are the lower bounds of human performance . Under reasoning - free setting where the annotator can see the conclusions , a single human achieves 90.4 % accuracy and 84.2 % macro - F1 . Under reasoning - required setting , the task be - comes much harder , but it 's still possible for humans to solve : a single annotator can get 78.0 % accuracy and 72.2 % macro - F1 .", "entities": [[120, 121, "MetricName", "accuracy"], [124, 127, "MetricName", "macro - F1"], [159, 160, "MetricName", "accuracy"], [163, 166, "MetricName", "macro - F1"]]}
{"text": "We report the test set performance of different models and training schedules in Table 5 . In general , multi - phase fine - tuning of BioBERT with additional supervision outperforms other baselines by large margins , but the results are still much worse than just single - human performance . Comparison of Models : A trend of BioBERT > ESIM w/ BioELMo > BiLSTM > shallow features > majority , conserves across different training schedules on both accuracy and macro - F1 . Fine - tuned BioBERT is better than state - of - theart recurrent model of ESIM w/ BioELMo , probably because BioELMo weights are fixed while all BioBERT parameters can be fine - tuned , which better benefit from the pre - training settings . Comparison of Training Schedules : Multiphase fine - tuning setting gets 5 out of 9 modelwise best accuracy / macro - F1 . Due to lack of annotated data , training only on the PQA - L ( final phase only ) generates similar results as the majority baseline . In phase I + Final setting where models are pre - trained on PQA - A , we observe significant improvements on accuracy and macro - F1 and some models even achieve their best accuracy under this setting . This indicates that a hard task with limited training instances can be at least partially solved by pre - training on a large automatically collected dataset when the tasks are similarly formatted . Improvements are also observed in phase II + Final setting , though less significant than those of phase I + Final . As expected , multi - phase finetuning schedule is better than single - phase , due to different properties of the subsets . Additional Supervision : Despite its simplicity , the auxiliary task of long answer BoW prediction clearly improves the performance : most results ( 28/40 ) are better with such additional supervision than without .", "entities": [[60, 61, "MethodName", "ESIM"], [64, 65, "MethodName", "BiLSTM"], [78, 79, "MetricName", "accuracy"], [80, 83, "MetricName", "macro - F1"], [99, 100, "MethodName", "ESIM"], [146, 147, "MetricName", "accuracy"], [148, 151, "MetricName", "macro - F1"], [201, 202, "MetricName", "accuracy"], [203, 206, "MetricName", "macro - F1"], [213, 214, "MetricName", "accuracy"]]}
{"text": "In this section we show the intermediate results of multi - phase fine - tuning schedule . Phase I : Results are shown in Table 6 . Phase I is fine - tuning on PQA - A using question and context . Since PQA - A is imbalanced due to its collection process , a trivial majority baseline gets 92.76 % accuracy . Other models have better accuracy and especially macro - F1 than majority baseline . Finetuned BioBERT performs best . Bootstrapping : Results are shown in Table 7 . Bootstrapping is a three - step process : fine - tuning on PQA - A , then on PQA - L and pseudo - labeling PQA - U. All three steps are using question and long answer as input . Expectedly , models perform better in this reasoning - free setting than they do in reasoning - required setting ( for PQA - A , Eq . 2 results in Table 7 are better than the performance in Table 6 ; for PQA - L , Eq . 3 results in Table 7 are better than the performance in Table 5 ) .", "entities": [[61, 62, "MetricName", "accuracy"], [67, 68, "MetricName", "accuracy"], [70, 73, "MetricName", "macro - F1"]]}
{"text": "Online misogyny is a pernicious social problem that risks making online platforms toxic and unwelcoming to women . We present a new hierarchical taxonomy for online misogyny , as well as an expert labelled dataset to enable automatic classification of misogynistic content . The dataset consists of 6 , 567 labels for Reddit posts and comments . As previous research has found untrained crowdsourced annotators struggle with identifying misogyny , we hired and trained annotators and provided them with robust annotation guidelines . We report baseline classification performance on the binary classification task , achieving accuracy of 0.93 and F1 of 0.43 . The codebook and datasets are made freely available for future researchers .", "entities": [[52, 53, "DatasetName", "Reddit"], [95, 96, "MetricName", "accuracy"], [99, 100, "MetricName", "F1"]]}
{"text": "Most previous classification work on online misogyny has used data from Twitter ( Waseem and Hovy , 2016 ; Jha and Mamidi , 2017 ) . However , social scientific and ethnographic research shows that Reddit is increasingly home to numerous misogynistic communities . Reddit is a social news website organised in to topic - based communities . Each subreddit acts as a message board where users make posts and hold discussions in comment threads on those posts . In recent years it has become a hub for anti - feminist activism online ( Massanari , 2017 ; Ging and Siapera , 2018 ) . It is also home to many misogynistic communities , particularly those associated with the ' manosphere ' , a loosely connected set of communities which perpetuate traditional forms of misogyny and develop new types of misogynistic discourse which in turn spread to other online spaces ( Ging , 2017 ; Zuckerberg , 2018 ; Ging et al , 2019 ; Farrell et al , 2019 ; Ribeiro et al , 2020 ) . Recent research suggests that the rate of misogynistic content in the Reddit manosphere is growing and such content is increasingly more violent ( Farrell et al , 2019 ) . Waseem and Hovy ( 2016 ) provided a widelyused dataset for abusive language classification . They used expert annotators to identify sexist and racist tweets based on a set of criteria drawn from critical race theory . The tweets were initially labelled by the authors then reviewed by a third annotator . The resulting dataset consists of 17k tweets , of which 20 % are labelled as sexist . However 85 % of the disagreements between annotators were over sexism labels , which shows that even experienced coders of abusive language can have difficultly identifying gendered abuse . Jha and Mamidi ( 2017 ) extended on the Waseem and Hovy ( 2016 ) dataset to distinguish between between ' benevolent ' and ' hostile ' sexism ( Glick and Fiske , 1997 ) . They classed all sexist labels in the previous dataset as ' Hostile ' and all non - sexist labels as ' Other ' . They then augmented the dataset by collecting tweets using keyword sampling on benevolently sexist phrases ( e.g. ' smart for a girl ' ) and extracted those manually identified as ' benevolent sexism ' . In the combined dataset of 10 , 095 unique tweets 712 were labelled as ' benevolent ' , 2 , 254 as ' hostile ' , and 7 , 129 as ' not sexist ' . They thus found that in the data hostile sexism was more than three times as common as the benevolent form . Their work highlights the need for greater attention to be placed on forms of ' subtle abuse ' , particularly for online misogyny ( Jurgens et al , 2019 ) . developed a taxonomy with five categories of misogyny , drawn from the work of Poland ( 2016 ) : Stereotype & Objectification , Dominance , Derailing , Sexual Harassment & Threats of Violence , Discredit . They used a combination of expert and crowdsourced annotation to apply the taxonomy and present a dataset of 4 , 454 tweets with balanced levels of misogynistic and non - misogynistic content . A shared task confirmed that the dataset could be used to distinguish misogynistic and non - misogynistic content with high accuracy , but performance was lower in differentiating between types of misogyny . Lynn et al ( 2019b ) provide a dataset of 2k Urban Dictionary definitions of which half are labelled as misogynistic . In Lynn et al ( 2019a ) they show that deep learning techniques had greater accuracy in detecting misogyny than conventional machine learning techniques .", "entities": [[35, 36, "DatasetName", "Reddit"], [44, 45, "DatasetName", "Reddit"], [189, 190, "DatasetName", "Reddit"], [219, 221, "TaskName", "abusive language"], [297, 299, "TaskName", "abusive language"], [578, 579, "MetricName", "accuracy"], [628, 629, "MetricName", "accuracy"]]}
{"text": "For the level one binary task the Fleiss ' Kappa is 0.484 and the Krippendorf 's alpha is 0.487 . By conventional NLP standards these results appear low . However they are equivalent to , or above , those of existing abusive content datasets . Sanguinetti et al ( 2018 ) report category - wise Kappas from k=0.37 for offence to k=0.54 for hate . Gomez et al ( 2020 ) have a Kappa of 0.15 in the \" MMH150 \" dataset of hateful memes . Fortuna and Nunes ( 2018 ) report a Kappa of 0.17 for a text - only task . Krippendorf 's alpha is similar to the 0.45 reported by Wulczyn et al ( 2017 ) . We also calculated level two category - wise Fleiss ' Kappas for each of the 17 sets of annotator groups , then took the mean across all groups ( Ravenscroft et al , 2016 ) . Table 1 shows the breakdown of Kappas per category . There was greatest agreement for Misogynistic pejoratives ( k=0.559 ) down to the lowest agreement for Misogynistic personal attacks ( k=0.145 ) .", "entities": [[16, 17, "HyperparameterName", "alpha"], [83, 85, "DatasetName", "hateful memes"], [106, 107, "HyperparameterName", "alpha"]]}
{"text": "As reference points for further research using our dataset , we provide three experimental baselines on the binary task of distinguishing between misogynistic and non - misogynistic content , i.e. level one of our taxonomy . As the simplest baseline , we evaluate a logistic unigram classifier . Further , we evaluate two uncased BERT - base models ( Devlin et al , 2019 ) - one unweighted , the other using class weights emphasising the minority class , i.e. misogynistic content , to account for class imbalance . For all models , we use the same stratified 80/20 train / test split of the dataset . Details on model training and parameters can be found in Appendix C. Performance of the three models is shown in Table 6 . All models perform poorly on misogynistic content , with the logistic classifier performing worst overall . The logistic classifier has the highest precision on misogynistic content ( 0.88 ) but very low recall ( 0.07 ) and a low F1 score ( 0.13 ) . The weighted BERT model has the highest recall ( 0.50 ) and F1 score ( 0.43 ) . Accuracy on all test cases , of which 91.9 % are non - misogynistic , is around 0.90 across models . The classification task is complicated by the relatively small size of our dataset ( n=6 , 385 unique cases ) as well as the relatively small proportion of misogynistic cases in it ( 8.1 % ) . These issues are common in abusive speech detection ( Fortuna and Nunes , 2018 ; Fortuna et al , 2020 ) . To address them , future research can leverage the typology and annotation process we introduced to collect additional cases , particularly misogynistic ones , thus growing and balancing the dataset .", "entities": [[54, 55, "MethodName", "BERT"], [169, 171, "MetricName", "F1 score"], [177, 178, "MethodName", "BERT"], [187, 189, "MetricName", "F1 score"], [193, 194, "MetricName", "Accuracy"]]}
{"text": "Discourse analysis is a crucial analytic level in NLP . In natural language discourse , speakers and writers often rely on implicit inference to signal the kind of contribution they are making to the conversation , as well as key relationships that justify their point of view . While early AI literature is full of case studies suggesting that this inference is complex , open - ended and knowledge - heavy ( e.g. , Charniak ( 1973 ) ; Schank and Abelson ( 1977 ) ) , recent work on computational discourse coherence offers a different approach . Take the following example from Pitler and Nenkova ( 2008 ) : ( 1 ) \" Alice thought the story was predictable . She found it boring . \" This discourse shows the classic pattern of implicit information . The overall point is that Alice had a negative opinion of the story : the underlying explanation is that the story was not interesting because it had no surprises . But given available lexical resources and sentiment detection methods , we can capture such inferences systematically by recognizing that they follow common general patterns , known as \" discourse relations \" , and are guided by shallow cues . An example of an instance in which discourse analysis can produce insights that may be missed by employing other NLP methods is this example from Taboada ( 2016 ) , where without discourse relations it may be difficult to capture sentiment : ( 2 ) \" While this book is totally different from any other book he has written to date , it did not disappoint me at all . \" This represents a Concession relation according to both Rhetorical Structure Theory and the Penn Discourse Treebank ( where it is notated as Comparison . Concession ) , resolving the incongruity of the first clause being negative and the second clause being positive by illustrating how the negative statement in the subordinate clause is reversed by the positive one in the main clause . The importance of discourse has led to active research based on predicting what coherence relations are present in text based on shallow information . The predicted relations are then used to draw inferences from the text . The value of predicting the semantic classes of coherence relations has been demonstrated in several applications , including sentiment analysis ( Marcu , 2000 ; Bhatia et al , 2015 ) , machine comprehension ( Narasimhan and Barzilay , 2015 ) , summarization ( Cohan et al , 2018 ; Marcu , 1999 ; Xu et al , 2019 ; Kikuchi et al , 2014 ) , and predicting instructor intervention in an online course discussion forum ( Chandrasekaran et al , 2017 ) . However , it is still the case that few works have so far found discourse relations as key features ( Zhong et al , 2020 ) . We argue that one reason for this gap between theory and empirical evidence is the quality of the parsers exacerbated by the distributional shifts in the texts they need to apply to . The necessity of discourse research has resulted in several shared tasks ( Xue et al , 2015 ( Xue et al , , 2016 and corpora development in multiple languages ( Zeyrek and Webber , 2008 ; Meyer et al , 2011 ; Danlos et al , 2012 ; Zhou et al , 2014 ; Zeyrek et al , 2020 ) . Yet shallow discourse parsing is a very difficult task ; more than 10 years after the introduction of the Penn Discourse Treebank ( Eleni Miltsakaki , 2004 ) , performance for English implicit discourse relation recognition has gone from 40.2 F - 1 ( Lin et al , 2009 ) to 47.8 ( Lee et al , 2020 ) , less than 8 percentage points ; a similar story could be said about the relation prediction performance of RST parsers . Such performance hinders the wider application of parsers . If downstream tasks are to use predicted relation senses , the data to which the systems are applied is typically different from their training data - the Wall Street Journal ( WSJ ) in a 3 - year window - to varying degrees . This tends to further aggravate the low performance observed . As a result , often we find that adding parsed discourse relations into models are unhelpful . Although domain difference is a recognized issue in shallow discourse parsing by existing work ( Braud et al , 2017 ; Liu et al , 2016 ) , we still have little understanding of the types of distributional shift that matter and by how much , even within one language . This position paper seeks to shed some light on our current state in discourse parsing in English . Surprisingly , we found that parsers have some issues even within the same news source as the training set ( WSJ ) ; the differences in accuracy were not significant between indomain and out - of - domain data for the qualitative examples that we looked at , although the distribution of errors tend to be different . This differs from other NLP tasks such as entity recognition , where training on data in the target domain increased the F1 score by over 20 points ( Bamman et al , 2019 ) . We further found that parsers perform differently on implicit discourse relations held within vs. across sentences . We believe these findings are strong evidence for the sensitivity of existing models to distributional shift in terms of both linguistic structure and vocabulary . Additionally , as part of our evaluation , we asked linguists to perform manual annotation , which allowed us to evaluate the accuracy of these parsers on plain , unlabeled text , and gain some insight about the mistakes made by the parsers . During the annotation process , we uncovered information that can guide future research , including but not limited to the critical role of context for implicit discourse sense classification . We discuss this need for context , hypothesize what scenarios may cause two arguments to need additional context , and provide some examples for which this is the case . We urge future researchers to consider developing contextaware models for shallow discourse parsing moving forward . We release our dataset to facilitate further discourse analysis under domain shift . 1", "entities": [[395, 397, "TaskName", "sentiment analysis"], [419, 420, "TaskName", "summarization"], [586, 588, "TaskName", "discourse parsing"], [754, 756, "TaskName", "discourse parsing"], [809, 811, "TaskName", "discourse parsing"], [840, 841, "MetricName", "accuracy"], [893, 895, "MetricName", "F1 score"], [971, 972, "MetricName", "accuracy"], [1064, 1066, "TaskName", "discourse parsing"]]}
{"text": "Transformer - based models perform better on linguistically different intra - sentential relations than they do on inter - sentential relations . As mentioned above , we aim to examine the results of distributional shifts in both vocabulary and linguistic structure . Here , we look at shifts in linguistic structure , namely , inter - vs. intra - sentence implicit discourse relations ( Hobbs , 1985 ) . The latter was introduced in the PDTB - 3 ( Liang et al , 2020 ) from which we show the following example : ( 3 ) ... Exxon Corp. built the plant but ( I m - plicit = then ) closed it in 1985 Unlike the inter - sentence relations that were annotated across adjacent sentences , implicit intrasentence relations do not occur at well - defined positions , but rather between varied types of syntactic constituents . Additionally , they often co - occur with explicit relations . Table 1 shows the accuracies of the base and large BERT model ( Chen et al , 2019 ) on the implicit relations in the two versions of the PDTB . The results on the PDTB - 3 are significantly better than those of the PDTB - 2 , and the model tested on the PDTB - 3 intra - sentential relations significantly outperformed both ( p<0.01 , t>11.172 ) . This mirrors the results found from running the baseline model in Liang et al ( 2020 ) on the PDTB - 2 , PDTB - 3 , and PDTB - 3 intra - sentential relations . Figure 1 shows the accuracy of the Wang et al ( 2017 ) parser on the inter - sentential and intrasentential relations in the RST , respectively . For the inter - sentential relations , we sampled only the relations between two sentences to have a \" fairer \" comparison ( it is well known that performance suffers on higher levels of the RST tree ) . As with the PDTB , these results show a significant improvement in performance when run on only the intra - sentential relations compared to only the intersentential relations . These results drive home the influence of the linguistic and structural differences between intra - and inter - sentence implicit relations on the performance of the parsers . We initially found this surprising since intra - sentence ones contain arguments with less information than their ( full - sentence ) intersentence counterparts . However , one explanation for this is that , while looking for relations within sentence boundaries is a problem that has been very explored , and to some extent solved , in various NLP tasks ( e.g. syntactic parsing ) , there are not as many rules regarding relations that occur across sentence boundaries . Regardless of the cause , these results illustrate that future shallow discourse parsers may benefit from accounting for such linguistic differences explicitly . Parsers struggle to identify implicit relations from less frequent classes . The second distributional shift we examine is a shift in vocabulary . In order to capture this , we measure the performance across several domain shifts from the PDTB - 2 using three datasets : WSJ articles from the COHA corpus ( Davies , 2012 ) , other news articles from COHA , and the GUM corpus ( Zeldes , 2017 ) . The WSJ articles are completely within the domain of the PDTB , but more shifted in timeline than the PDTB test set . The other news articles are in - domain as well , but not from the same source Figure 1 : F - 1 scores for running the Wang et al RST parser on the RST Discourse Treebank for inter - sentential ( yellow ) and intra - sentential ( blue ) relations ( * denotes that this relation was not included in the set of inter - sentential relations ) . We can see from this graph that the performance of the parser was improved for the intra - sentential relations compared to the inter - sentential relations . publication , and thus may be linguistically different . The GUM corpus , our out - of - domain dataset , contains data from eight domains : Academic , Bio , Fiction , Interview , News , Travel , How - to guides , and Forum Discussions . It contains gold RST annotations but no PDTB annotations . To quantitatively evaluate the performance of these parsing models , we examine the distribution of the parser predictions and how frequently different senses are predicted . From this , we noticed that only 5 out of the 16 PDTB - 2 level 2 senses were predicted at all by the Wang and Lan parser , and only 7 out of 16 were predicted by the DiscoEval parser . Of these classes , several were predicted less than 2 % of the time ( Table 6 ) . We can also see that in Tables 2 and 3 , the Wang et al parser predicted at least 38.7 % Contingency . Cause for all datasets and the DiscoEval parser predicted at least 44 % Contingency . Cause , although these percentages were often much higher . Because only 24.9 % of the total relations contained in the PDTB are Contingency , this overrepresentation of Contingency . Cause in the predictions indicates a strong bias towards Contingency . Indeed , many of the errors found during annotation occurred when the parser predicted Contingency . Cause , the most common level 2 sense , over a less represented class such as Comparison . Contrast ; the precision for Contingency . Cause was 0.33 , 0.14 , and 0.33 for WSJ articles , non - WSJ news articles , and the GUM corpus respectively . This likely contributed to the low accuracy for these documents . These results show us that if PDTB parsers are run on plain text documents , whether in - domain or slightly shifted , the results are likely to be overconfident with majority classes and unlikely to predict minority classes . We also obtained the predicted distributions of the RST relations ( Table 4 ) on the COHA news articles ; we examined these results for the set of WSJ articles as well as the other news articles . We found that relations that are highly represented in the RST Discourse Treebank such as Elaboration , Attribution , and Same Unit were predicted much more frequently than they appear in the RST . However , more minority classes were represented in 5 , where the results of the parsers are compared to the ground truth labels by the annotators . Across the three corpora , the annotators noticed that in many cases the relation type was labeled as EntRel or NoRel when it should n't have been , or vice versa . This led to discourse senses being predicted for relations that did not have a discourse sense and vice versa . The parsers also often had issues with argument segmentation . For the GUM corpus , segmentation was especially an issue in the travel genre , where headers or captions would be labeled as part of an argument . As is shown in Table 5 , the percentage of implicit relations that the parsers got right on the second level appeared to decrease on average as the domain shifted . However , this was a very slight decrease ; they had roughly the same level of accuracy across all datasets , which was very low . In fact , for all parsing models and datasets , a larger percentage of relations was predicted completely incorrectly . The results of running the state - of - the - art Wang et al ( 2017 ) parser on the gold labels of the RST and GUM corpus are shown in Figure 2 . These results make it clear that the RST parser performs much worse on out - of - domain data than it does on RST corpus data . This is expected ; it unsurprisingly does not generalize as well for text outside of its domain as for the news text contained within the corpus test set due to a change in vocabulary . However , in order for discourse parsers to be useful for applications outside of the news domain , models that can more easily adapt to the target domain must be developed . 0 - 2 % 1 2 2 2 3 3 2 - 5 % 1 0 0 2 2 2 > 5 % 3 3 3 3 3 3", "entities": [[0, 1, "MethodName", "Transformer"], [170, 171, "MethodName", "BERT"], [181, 183, "TaskName", "implicit relations"], [271, 272, "MetricName", "accuracy"], [382, 384, "TaskName", "implicit relations"], [498, 500, "TaskName", "implicit relations"], [560, 561, "DatasetName", "GUM"], [700, 701, "DatasetName", "GUM"], [719, 720, "DatasetName", "Bio"], [723, 724, "DatasetName", "Interview"], [975, 976, "DatasetName", "GUM"], [985, 986, "MetricName", "accuracy"], [1193, 1194, "DatasetName", "GUM"], [1229, 1231, "TaskName", "implicit relations"], [1266, 1267, "MetricName", "accuracy"], [1323, 1324, "DatasetName", "GUM"], [1425, 1426, "DatasetName", "0"], [1440, 1441, "DatasetName", "0"], [1441, 1442, "DatasetName", "0"]]}
{"text": "More context than the two arguments is needed to determine the correct discourse relation in many cases One potential way to mitigate the impact of domain shift on the performance of shallow discourse parsers is to incorporate context . With a few exceptions ( Dai and Huang , 2018 ; Shi and Demberg , 2019 ; Zhang et al , 2021 ) , existing models for shallow discourse parsing mostly do not Figure 3 : RST parse tree containing a segment of the relations that were examined in the qualitative analysis . The discourse sense labels on this tree that were examined in our analysis are marked red and green , where green is correct and red is incorrect use input beyond the two adjacent sentences that comprise the arguments of the relation ( Kishimoto et al , 2020 ; Chen et al , 2019 ) . We found that only considering these two sentences is not sufficient even for our expert linguist annotators . Specifically , while annotating the PDTB , the annotators found several examples where , when they looked at the larger context behind the arguments and the sentences where the arguments were contained , their annotations changed . Below , we describe a few examples that demonstrate the mistakes that can be made without the full context and their implications : ( 4 ) In this northern latitude it does n't get dark in summer until about 10:30 p.m. so lighting is operate except at some crazy time like 11:45 at night , whenever there is power , unless they have stand - by diesel generators . There 's a year 's supply of diesel oil here . This example is from the Wall Street Journal . At first glimpse , one would think to annotate this as Contingency . Factual present condition , but this does not capture the full context , which is shown below : ( 5 ) One housewife says : \" With an electric kitchen I have to do my whole day 's cook - ing the day before - and that during a couple of hours , not knowing from one minute to the next what time the power is coming on . \" In this northern latitude it does n't get dark in summer until about 10:30 p.m. so lighting is operate except at some crazy time like 11:45 at night , whenever there is power , unless they have stand - by diesel generators . There 's a year 's supply of diesel oil here . The additional context , that people in the country described are dealing with electricity issues despite there being a year 's worth of diesel supply , is now made clear in this passage . Thus we can conclude that the correct relation here is Comparison . Contrast . Without getting this context and just seeing the two sentences in which the arguments are contained , it is difficult to discern this as an annotator . This shows that by just getting exposure to the two arguments , without additional context , the sense may be marked incorrectly . The Wang and Lan ( 2015 ) parser and the DiscoEval parser both predicted this incorrectly , with the Wang and Lan ( 2015 ) parser predicting it as Contingency . Cause and the BERT parser predicting it as Expansion . Conjunction . Similarly , the following example , also contained in this passage , has a different true annotation than one would think from only seeing the arguments : ( 6 ) One housewife says : \" With an electric kitchen I have to do my whole day 's cooking the day before - and that during a couple of hours , not knowing from one minute to the next what time the power is coming on . \" In this northern latitude it does n't get dark in summer until about 10:30 p.m. so lighting is operate except at some crazy time like 11:45 at night , whenever there is power , unless they have stand - by diesel generators . The relation may be deemed as Expansion . Instantiation . However , by reading the full text , it is clear that it should be labeled as Contingency . Cause . Like the last example , a clearer view of the full text is needed to determine the proper annotation , not simply the two arguments . These observations provide insights as to why contextual embeddings with document context such as the next sentence prediction task helps with implicit discourse relation classification ( Shi and Demberg , 2019 ) . More generally , we believe future work on discourse parsing should look beyond only the arguments of a relation because of the different interpretations one would give when taking the relation in vs. out of context . We believe that argument pairs with low specificity and one or more pronouns may be especially in need of this extra context , but more experimentation will have to be done to confirm this hypothesis . Attachment issues tend to occur throughout the RST parse tree , and relations are often misclassified as Same - Unit and Elaboration . Regarding insights for the RST Discourse Treebank , a piece of the RST tree for this paragraph can be seen in 3 . Here , the EDU \" One housewife says \" should attach to the EDU after it , \" With an electric kitchen I have to do my whole day 's cooking the day before \" . However , it instead attaches to EDUs from the preceding sentences , which is incorrect , as these two sentences do not contain what the housewife says . We saw several other attachment issues in the text , including a couple where the attachment should go up / down by several levels . We also saw several instances of the relation being incorrectly tagged as Same - Unit or Elaboration , some of which can be seen in the diagram . Attachment issues are a particular problem for RST parsing due to its hierarchical nature ; one at - tachment issue can lead to error propagation where the accuracy of the attachments further in the tree is impacted by that of the current one . Reducing this error is of the utmost importance for future parsers .", "entities": [[67, 69, "TaskName", "discourse parsing"], [560, 561, "MethodName", "BERT"], [767, 771, "TaskName", "implicit discourse relation classification"], [787, 789, "TaskName", "discourse parsing"], [1042, 1043, "MetricName", "accuracy"]]}
{"text": "Neural machine translation ( NMT ) ( Kalchbrenner and Blunsom , 2013 ; Sutskever et al , 2014 ; Bahdanau et al , 2015 ; Gehring et al , 2017 ; Vaswani et al , 2017 ) has shown its superiority and drawn much attention in recent years . Although the NMT model can achieve promising results for highresource language pairs , it is unaffordable to train separate models for all the language pairs since there are thousands of languages in the world ( Tan et al , 2019 ; Aharoni et al , 2019 ; Arivazhagan et al , 2019 ) . A typical solution to reduce the model size and the training cost is to handle multiple languages in a single multilingual neural machine translation ( MNMT ) model ( Ha et al , 2016 ; Firat et al , 2016 ; Johnson et al , 2017 ; Gu et al , 2018 ) . The standard paradigm of MNMT proposed by Johnson et al ( 2017 ) contains a language - shared encoder and decoder with a special language indicator in the input sentence to determine the target language . Because different languages share all of the model parameters in the standard MNMT model , the model tends to converge to a region where there are low errors for all the languages . Therefore , the MNMT model trained on the combined data generally captures the general knowledge , but ignores the language - specific knowledge , rendering itself sub - optimal for the translation of a specific language ( Sachan and Neubig , 2018 ; Blackwood et al , 2018 ; Wang et al , 2020b ) . To retain the language - specific knowledge , some researches turn to augment the NMT model with language - specific modules , e.g. , the language - specific attention module ( Blackwood et al , 2018 ) , decoupled multilingual encoders and/or decoders ( V\u00e1zquez et al , 2019 ; Escolano et al , 2020 ) and the lightweight language adapters . However , these methods suffer from the parameter increment problem , because the number of parameters increases linearly with the number of languages . Besides , the structure , size , and location of the module have a large influence on the final performance , which requires specialized manual design . As a result , these problems often prevent the application of these methods in some scenarios . Based on the above , we aim to propose a method that can retain the general and language - specific knowledge , and keep a stable model size as the number of language - pair increases without introducing any specialized module . To achieve this , we propose to divide the model neurons into two parts based on their importance : the general neurons which are used to retain the general knowledge of all the languages , and the language - specific neurons which are used to retain the language - specific knowledge . Specifically , we first pre - train a standard MNMT model on all language data and then evaluate the importance of each neuron in each language pair . According to their importance , we divide the neurons into the general neurons and the language - specific neurons . After that , we finetune the translation model on all language pairs . In this process , only the general neurons and the corresponding language - specific neurons for the current language pair participate in training . Experimental results on different languages show that the proposed method outperforms several strong baselines . Our contributions can be summarized as follows : We propose a method that can improve the translation performance of the MNMT model without introducing any specialized modules or adding new parameters . We show that the similar languages share some common features that can be captured by some specific neurons of the MNMT model . We show that some modules tend to capture the general knowledge while some modules are more essential for capturing the languagespecific knowledge .", "entities": [[1, 3, "TaskName", "machine translation"], [125, 127, "TaskName", "machine translation"], [239, 241, "TaskName", "general knowledge"], [357, 360, "HyperparameterName", "number of parameters"], [482, 484, "TaskName", "general knowledge"], [670, 672, "TaskName", "general knowledge"]]}
{"text": "The basic idea of importance evaluation is to determine which neurons are essential to all languages while which neurons are responsible for some specific languages . For a neuron i , its average importance I across language pairs is defined as follow : I ( i ) = 1 M M m=1 \u0398 m ( i ) , ( 1 ) where the \u0398 ( ) denotes the importance evaluation function and M denotes the number of language pairs . This value correlates positively with how important the neuron is to all languages . For the importance evaluation function \u0398 ( ) , we adopt two schemes : one is based on the Taylor Expansion and the other is based on the Absolute Value . Taylor Expansion We adopt a criterion based on the Taylor Expansion ( Molchanov et al , 2017 ) , where we directly approximate the change in loss when removing a particular neuron . Let h i be the output produced from neuron i and H represents the set of other neurons . Assuming the independence of each neuron in the model , the change of loss when removing a certain neuron can be represented as : | \u2206L ( h i ) | = | L ( H , h i = 0 ) \u2212 L ( H , h i ) | , ( 2 ) where L ( H , h i = 0 ) is the loss value if the neuron i is pruned and L ( H , h i ) is the loss if it is not pruned . For the function L ( H , h i ) , its Taylor Expansion at point h i = a is : L ( H , h i ) = N n=0 L n ( H , a ) n ! ( h i \u2212 a ) n + R N ( h i ) , ( 3 ) where L n ( H , a ) is the n - th derivative of L ( H , h i ) evaluated at point a and R N ( h i ) is N - th remainder . Then , approximating L ( H , h i = 0 ) with a firstorder Taylor polynomial where h i equals zero : L ( H , h i = 0 ) = L ( H , h i ) \u2212 \u2202L ( H , h i ) \u2202h i h i \u2212R 1 ( h i ) . ( 4 ) The remainder R 1 can be represented in the form of Lagrange : R 1 ( h i ) = \u2202 2 L ( H , h i ) \u2202 2 \u03b4h i h 2 i , ( 5 ) where \u03b4 ( 0 , 1 ) . Considering the use of ReLU activation function ( Glorot et al , 2011 ) in the model , the first derivative of loss function tends to be constant , so the second order term tends to be zero in the end of training . Thus , we can ignore the remainder and get the importance evaluation function as follows : \u0398 TE ( i ) = | \u2206L ( h i ) | = \u2202L ( H , h i ) \u2202h i h i . ( 6 ) In practice , we need to accumulate the product of the activation and the gradient of the objective function w.r.t to the activation , which is easily computed during back - propagation . Finally , the evaluation function is shown as : \u0398 m TE ( i l ) = 1 T m t \u03b4L ( H , h l i ) \u03b4h l i h l i , ( 7 ) where h l i is the activation value of the i - th neuron of l - th layer and T m is the number of the training examples of language pair m. The criterion is computed on the data of language pair m and averaged over T m . Absolute Value We adopt the magnitude - based neuron importance evaluation scheme ( See et al , 2016 ) , where the absolute value of each neuron 's activation value is treated as the importance : \u0398 m AV ( i l ) = 1 T m t | h l i | . ( 8 ) The notations in the above equation are the same as those in the Equation 7 . After the importance of each neuron is evaluated on the combined data , we need to determine the role of each neuron in the fine - tuning step following the method in the next section .", "entities": [[52, 53, "HyperparameterName", "\u0398"], [63, 64, "HyperparameterName", "\u0398"], [99, 100, "HyperparameterName", "\u0398"], [151, 152, "MetricName", "loss"], [190, 191, "MetricName", "loss"], [217, 218, "DatasetName", "0"], [240, 241, "DatasetName", "0"], [244, 245, "MetricName", "loss"], [262, 263, "MetricName", "loss"], [377, 378, "DatasetName", "0"], [397, 398, "DatasetName", "0"], [470, 471, "HyperparameterName", "\u03b4"], [472, 473, "DatasetName", "0"], [481, 482, "MethodName", "ReLU"], [482, 484, "HyperparameterName", "activation function"], [499, 500, "MetricName", "loss"], [537, 538, "HyperparameterName", "\u0398"], [608, 609, "HyperparameterName", "\u0398"], [724, 725, "HyperparameterName", "\u0398"]]}
{"text": "According to the overall importance I ( i ) in Equation 1 , the value correlates positively with how important the neuron is to all languages . Therefore , we rank the neurons in each layer based on the importance and make the top \u03c1 percentage as general neurons that are responsible for capturing general knowledge . Language - specific Neurons Next , we regard other neurons except for the general neurons as the language - specific neurons and determine which language pair to assign them to . To achieve this , we compute an importance threshold for each neuron : \u03bb ( i ) = k \u00d7 max ( \u0398 m ( i ) ) , m { 1 , . . . , M } , k [ 0 , 1 ] ( 9 ) , where max ( \u0398 m ( i ) ) denotes the maximum importance of this neuron in all language pairs and k is a hyper - parameter . The neuron will be assigned to the language - pairs whose importance is larger than the threshold . When the importance of neurons is determined , the number of language pairs associated with each neuron can be adjusted according to k. The smaller the k , the more language - pairs will be associated with the specific neurons . In this way , we flexibly determine the language pairs assigned to each neuron according to its importance in different languages . Note that the neuron allocation is based on the importance of language pair . We have also tried other allocation variants , e.g. , based on the source language , target language , and find that the language pair - based method is the best among of these methods . The detailed results are listed in Appendix A. After this step , the model is continually finetuned on the combined multilingual data . If the training data is from a specific language pair , only the general neurons and the language - specific neurons for this language pair will participate in the forward computation and the parameters associated with them will be updated during the backward propagation .", "entities": [[54, 56, "TaskName", "general knowledge"], [110, 111, "HyperparameterName", "\u0398"], [130, 131, "DatasetName", "0"], [141, 142, "HyperparameterName", "\u0398"]]}
{"text": "For fair comparisons , we implement the proposed method and other contrast methods on the advanced Transformer model using the open - source toolkit Fairseq - py ( Ott et al , 2019 ) . We follow Vaswani et al ( 2017 ) to set the configurations of the NMT model , which consists of 6 stacked encoder / decoder layers with the layer size being 512 . All the models were trained on 4 NVIDIA 2080Ti GPUs where each was allocated with a batch size of 4 , 096 tokens for one - to - many scenario and 2 , 048 tokens for the many - to - many scenario . We train the baseline model using Adam optimizer ( Kingma and Ba , 2015 ) with \u03b2 1 = 0.9 , \u03b2 2 = 0.98 , and = 10 \u22129 . The proposed models are further trained with corresponding parameters initialized by the pre - trained baseline model . We vary the hyperparameter \u03c1 that controls the proportion of general neurons in each module from 80 % to 95 % and set it to 90 % in our main experiments according to the performance . The detailed results about this hyper - parameter are list in Appendix B. We set the hyper - parameter k to 0.7 and do more analysis on it in Section 5.3 . For evaluation , we use beam search with a beam size of 4 and length penalty \u03b1 = 0.6 .", "entities": [[16, 17, "MethodName", "Transformer"], [84, 86, "HyperparameterName", "batch size"], [118, 119, "MethodName", "Adam"], [119, 120, "HyperparameterName", "optimizer"], [128, 129, "HyperparameterName", "\u03b2"], [133, 134, "HyperparameterName", "\u03b2"], [245, 246, "HyperparameterName", "\u03b1"]]}
{"text": "The final translation is detokenized and then the quality is evaluated using the 4 - gram case - sensitive BLEU ( Papineni et al , 2002 ) with the SacreBLEU tool ( Post , 2018 ) . 4 Many - to - Many The results are given in Table 1 . We can see that the improvements brought by + TS and + Adapter methods are not large . For the + TS method , attention module may be not essential to capture language - specific knowledge , and thus it is difficult to converge to good optima . For the + Adapter method , adding an adapter module to the end of each layer may be not appropriate for some languages and hence has a loose capture to the specific features . In all language pairs , our method based on Taylor Expansion outperforms all the baselines in the datasets . Moreover , the parameters in our model are the same as the Multilingual system and less than other baselines . One - to - Many The results are given in Table 2 , our method exceeds the multilingual baseline in all language pairs and outperforms other baselines in most language pairs without capacity increment . When we expand the model capacity to the level of + Adapter , our approach can achieve better translation performance , which demonstrates the effectiveness of our method . Another finding is that the results of the individual baseline are worse than other baselines . The reason may be the training data is not big enough , individual baseline can not get a good enough optimization on 0.6 M sentences , while the MNMT model can be well trained with a total of 7.2 M data .", "entities": [[19, 20, "MetricName", "BLEU"], [29, 30, "MetricName", "SacreBLEU"], [60, 61, "MethodName", "TS"], [63, 64, "MethodName", "Adapter"], [72, 73, "MethodName", "TS"], [102, 103, "MethodName", "Adapter"], [218, 219, "MethodName", "Adapter"]]}
{"text": "In our method , we allocate neurons based on their importance for different languages . The rationality behind this mechanism is that different neurons should have distinct importance values so that these neurons can find their relevant language pairs . Therefore , we show the importance of neurons computed by Taylor Expansion in different modules for the one - to - many ( O2 M ) and many - to - many ( M2 M ) translation tasks . are Spanish and Portuguese , both of which belong to the Western Romance , the Romance branch of the Indo - European family , while the last one is Finnish , a member of the Finnish - Ugric branch of the Ural family . As we can see , the importance of Spanish and Portuguese are always similar in most neurons , but there is no obvious correlation between Finnish and the other two languages . It indicates that similar languages are also similar in the distribution of the neuron importance , which implies that the common features in similar languages can be captured by the same neurons . The results of M2 M are shown in Figure 2 ( c ) and Figure 2 ( d ) , and the language pairs are It En , Ro It , and En Ro , whose BLEU scores are 0.67 , 1 , and 1.7 higher than the multilingual baseline , respectively . In most neurons , the highest importance value is twice as high as the lowest and this high variance of importance provides the theoretical basis for later neuron allocation . Moreover , we can see a lot of importance peaks of the two language pairs : Ro It and En Ro , which means that these neurons are especially important for generating the translation for these language pairs . However , the fluctuation of It En is flat with almost no peaks , which means only a few neurons are specific to this language pair . This may be the reason why some language pairs have higher improvements , while some have lower improvements .", "entities": [[224, 225, "MetricName", "BLEU"]]}
{"text": "When the importance of neurons for different languages is determined , the number of language pairs associated with each neuron can be adjusted ac - Figure 5 : \u2206 BLEU over best performance when erasing the general or language - specific neurons randomly on the many - to - many translation task . cording to k. When k = 1.0 , the threshold is max ( \u0398 m ( i ) ) as computed by Equation 9 , so the neurons will only be allocated to the language pair with the highest importance , and when k = 0 , the threshold is 0 so the neurons will be shared across all language pairs just like the Multilingual baseline . To better show the overall impact of the hyperparameter k , we vary it from 0 to 1 and the results are shown in Figure 4 . As we can see , the translation performance of the two proposed approaches increases with the increment of k and reach the best performance when k equals 0.7 . As k continues to increase , the performance deteriorates , which indicates that the over - specific neurons are bad at capturing the common features shared by similar languages and will lead to performance degradation .", "entities": [[29, 30, "MetricName", "BLEU"], [57, 59, "HyperparameterName", "k ="], [66, 67, "HyperparameterName", "\u0398"], [96, 98, "HyperparameterName", "k ="], [98, 99, "DatasetName", "0"], [103, 104, "DatasetName", "0"], [135, 136, "DatasetName", "0"]]}
{"text": "The main idea of our method is to let the general knowledge and the language - specific knowledge be captured by different neurons of our method . To verify whether this goal has been achieved , we conduct the following experiments . For the general knowledge , we randomly erase 20 % general neurons of the best checkpoint of our method , which means we mask the output value of these neurons to 0 , then generate translation using it . For languagespecific knowledge , we randomly erase 50 % specific neurons and then generate translation . As shown in Figure 5 , when the general neurons are erased , the BLEU points of all the language pairs drop a lot ( about 15 to 20 BLEU ) , which indicates general neurons do capture the general knowledge across languages . For specific neurons , we show three language pairs for the sake of convenience . We can see that when the neurons associated with the current language pair are erased , the performance of this language pair decreases greatly . However , the performance of other language pairs only declines slightly , because the specific knowledge captured by these specific neurons are not so important for other languages .", "entities": [[10, 12, "TaskName", "general knowledge"], [44, 46, "TaskName", "general knowledge"], [73, 74, "DatasetName", "0"], [111, 112, "MetricName", "BLEU"], [126, 127, "MetricName", "BLEU"], [136, 138, "TaskName", "general knowledge"]]}
{"text": "We represent the whole corpus D with an undirected graph G = ( N , E ) , where N and E are nodes and edges in the graph respectively . To model both words and documents , each of them is represented as a node n i N , which gives rise to N = V + D nodes in total , where V is the size of vocabulary V and D is the number of documents in corpus D. An edge ( n i , n j ) indicates the relevance of node n i and n j , whose weight is determined by A i , j = \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 TF - IDF ij , i D and j V TF - IDF ji , i V and j D 1 , i = j 0 , otherwise ( 1 ) where A is the adjacency matrix of G and TF - IDF ij denotes the max - normalized TF - IDF ( Term Frequency - Inverse Document Frequency ) weight of word j in document i. Besides self - connections , we only apply positive weights to edges between documents and words , while rely on the model to capture higher - order relationships , e.g. doc - doc and word - word relationships , by applying graph convolutions on graph G. I X X T I \uf8ee \uf8f0 \uf8f9 \uf8fb E\u1e90 Z Dirichlet ( \u03b1 ) GX L rec ( X , X ) MMD ( P Z , Q\u1e90 ) Figure 1 : The framework of GTM . Circles denote neural networks . X , I , \u1e90 , X , Z are the TF - IDF matrix of the corpus , an identity matrix , latent topics of all documents , reconstructed word weights and topic distributions drawn from the Dirichlet prior respectively . L rec ( X , X ) and MMD ( P Z , Q Z ) are training objectives .", "entities": [[146, 147, "DatasetName", "0"], [247, 248, "HyperparameterName", "\u03b1"], [257, 258, "DatasetName", "MMD"], [327, 328, "DatasetName", "MMD"]]}
{"text": "The proposed GTM consists of an encoder E and a decoder G. The framework is shown in Figure 1 , and we detail the architecture in the following . The encoder network E maps nodes in G to their topic distributions by iteratively applying graph convolution to the node features . Following ( Kipf and Welling , 2016 ) , the layer - wise propagation rule of the graph convolution at layer l + 1 [ 1 , L ] is defined as H ( l+1 ) = \u03c3 ( D \u2212 1 2 AD \u2212 1 2 H ( l ) W ( l ) ) ( 2 ) where A R N \u00d7N is the adjacency matrix of G , D ii = j A ij , W ( l ) R d ( l ) \u00d7d ( l+1 ) is a layerspecific weight matrix where d ( l ) is the output size of layer l , and \u03c3 denotes an activation function that is LeakyReLU ( Maas et al , 2013 ) in this paper . H ( l ) R N \u00d7d ( l ) is the activations of all nodes at layer l and H ( 0 ) i is the embedding of node i. At each encoder layer , what the graph convolution does is aggregating node features from a node 's first - order neighborhood , which consequently enlarges the receptive field of the central node and enables the information propagation between relevant nodes . After successively applying L graph convolution layers , the encoding of a node essentially involves its L th - order neighborhood . With L \u2265 2 , doc - doc and word - word relationships are naturally captured in the topic inference process . We also add a batch normalization ( Ioffe and Szegedy , 2015 ) after each graph convolution . After the graph encoding , a softmax is further applied to the node features of a document to produce a multinomial topic distribution\u1e91 R K , where K is the topic number . Based on the inferred topic distribution\u1e91 , the decoder network G tries to restore the original document representations . To achieve this goal , we employ a 2 - layer MLP with LeakyReLU activation and batch normalization in the first layer . The output of the MLP decoder is then softmax - normalized to generate a word distributionx R V . The decoder is also used to interpret topics . In this case , we feed to the decoder an identity matrix I R K\u00d7K , and the decoder output G ( I ) i is the word distribution of the i - th topic .", "entities": [[45, 46, "MethodName", "convolution"], [69, 70, "MethodName", "convolution"], [164, 166, "HyperparameterName", "activation function"], [202, 203, "DatasetName", "0"], [219, 220, "MethodName", "convolution"], [258, 259, "MethodName", "convolution"], [301, 303, "MethodName", "batch normalization"], [313, 314, "MethodName", "convolution"], [321, 322, "MethodName", "softmax"], [378, 379, "DatasetName", "MLP"], [383, 385, "MethodName", "batch normalization"], [394, 395, "DatasetName", "MLP"], [398, 399, "MethodName", "softmax"]]}
{"text": "Based on the Wasserstein Autoencoder ( Tolstikhin et al , 2017 ) framework , the training objective of GTM is to minimize the document reconstruction loss when the latent topic space is constrained by a prior distribution . The reconstruction loss is defined as L rec ( X , X ) = \u2212E ( x logx ) , ( 3 ) where x denotes the TF - IDF of a document andx is the reconstructed word distribution corresponding to x. we use TF - IDF as the reconstruction target since TF - IDF basically preserves the relative importance of words and reduces some background noise that may hurt topic modeling , e.g. , stop words . We impose a Dirichlet prior , the conjugate prior of the multinomial distribution , to the latent topic distributions . Following W - LDA ( Nan et al , 2019 ) , we achieve this goal by minimizing the Maximum Mean Discrepancy ( MMD ) ( Gretton et al , 2012 ) between the distribution Q\u1e90 of inferred topic distributions\u1e91 and the Dirichlet prior P Z from which we draw multinomial noises z : MMD ( PZ , Q\u1e90 ) = 1 m ( m \u2212 1 ) i = j k ( z ( i ) , z ( j ) ) + 1 n ( n \u2212 1 ) i = j k ( \u1e91 ( i ) , \u1e91 ( j ) ) \u2212 2 mn i , j k ( z ( i ) , \u1e91 ( j ) ) , ( 4 ) where m and n are the number of samples from Z and\u1e90 respectively ( m and n are batch sizes and they are equal in our experiments ) , and k : Z\u00d7Z R is the kernel function . We use the information diffusion kernel ( Lebanon and Lafferty , 2003 ) as in W - LDA : k ( z , z ) = exp ( \u2212 arccos 2 ( K i=1 z i z i ) ) , ( 5 ) which is sensitive to points near the simplex boundary and thus more suitable for the sparse topic distributions .", "entities": [[4, 5, "MethodName", "Autoencoder"], [25, 26, "MetricName", "loss"], [40, 41, "MetricName", "loss"], [139, 140, "MethodName", "LDA"], [159, 160, "DatasetName", "MMD"], [190, 191, "DatasetName", "MMD"], [270, 273, "HyperparameterName", "number of samples"], [320, 321, "MethodName", "LDA"]]}
{"text": "We evaluate our model on three datasets : 20Newsgroups consisting of 11 , 259 documents , Grolier consisting of 29 , 762 documents , and NYTimes consisting of 99 , 992 documents . We use the preprocessed 20Newsgroups of ( Srivastava and Sutton , 2017 ) , and preprocessed Grolier and NYTimes of ( Wang et al , 2019a ) . We compare the performance of our model with LDA ( Blei et al , 2003 ) , NVDM ( Miao et al , 2016 ) , ProdLDA ( Srivastava and Sutton , 2017 ) , GraphBTM ( Zhu et al , 2018 ) , ATM ( Wang et al , 2019a ) and W - LDA ( Nan et al , 2019 ) using topic coherence measures ( R\u00f6der et al , 2015 ) . To quantify the understandability of the extracted topics , a topic coherence measure aggregates the relatedness scores of the topic words ( topweighted words ) of each topic , where the word relatedness scores are estimated based on word co - occurrence statistics on a large external corpus . For example , the NPMI coherence measure ( Aletras and Stevenson , 2013 ) applies a sliding window of size 10 over the Wikipedia corpus to calculate NPMI ( Bouma , 2009 ) for word pairs . We use three topic coherence measures in our experiments : C A ( Aletras and Stevenson , 2013 ) , C P ( R\u00f6der et al , 2015 ) , and NPMI . The topic coherence scores are calculated using Palmetto ( R\u00f6der et al , 2015 ) 1 . ( 20 , 30 , 50 , 75 , 100 ) . Bold values indicate the best performing model under the corresponding dataset / metric setting . We use 2 graph convolution layers with output dimensions of 100 and K respectively in the encoder . The hidden size of the decoder is also set to 100 . We use the RMSProp ( Hinton et al , 2012 ) optimizer with a learning rate of 0.01 to train the model for 100 epochs . Since the training datasets scale up to 100 K documents , i.e. , 100 K document nodes in the graph , it is hard to do batch training on a single GPU given the large memory requirements . We solve this issue by mini - batching the datasets and feeding to the model a subgraph consisting of 1000 document nodes and all word nodes at a training step , which results in efficient training ( The training time increases almost linearly with the number of documents ) and makes it possible to apply our model to even bigger datasets . The topic coherence results on the three datasets are shown in Table 1 , where each value is the average of 5 topic number settings : 20 , 30 , 50 , 75 , 100 . From Table 1 , we can observe that our proposed GTM is the best - performing model under all dataset / metric settings . W - LDA , ATM , LDA , and GraphBTM alternately achieve the second - best but they are always under - performed compared to our model . As described in section 2 , GTM is an extension to W - LDA with the main difference that GTM models topics in a larger context and incorporates more global information with the graph encoder . Therefore the improvements of GTM over W - LDA indicate the effectiveness of such information for topic modeling . We only experimented GraphBTM on 20Newsgroups because only 20Newsgroups preserves the sequential information that is necessary for GraphBTM to build graphs . GraphBTM performs well on the C A metric , which is reasonable since C A is a coherence measure based on a small sliding window of size 5 and consequently prefers models concentrating on a smaller context like GraphBTM . However , GraphBTM fails to achieve a high C P or NPMI score , which uses a bigger window ( 70 and 10 respectively ) . To explore how topic coherence results vary w.r.t . different topic numbers , we present in Figure 2 the topic coherence scores under different topic numbers settings . It can be observed in Figure 2 that GTM enjoys the best overall performance , achieving the highest scores in most settings . LDA has a slightly higher NPMI score on 20Newsgroups dataset with 75 and 100 topics , nevertheless , GTM outperforms all baseline models with a relatively large margin on other settings of 20Newsgroups . NVDM is apparently the worst - performing model , while performances of models other than GTM and NVDM are not so consistent . Notably , W - LDA , GraphBTM , and LDA obtain the second - best overall C P , C A , and NPMI scores respectively . Another observation from Figure 2 is that GTM performs better on smaller topics , probably due to the fact that topics become more discriminative against each other when the topic number is small . To gain an intuitive impression on the discovered topics , we present in Table 2 4 topics corresponding to 4 out of 20 ground - truth categories of 20Newsgroups . It can be observed that the topics discovered by GTM are more coherent and interpretable , containing few off - topic words . As a comparison , GraphBTM 's rec.autos topic mixes up automobiles and criminals , W - LDA 's misc.forsale topic is difficult to identify with too many offtopic words , while LDA can not distinguish between rec.autos and misc.forsale well thus recog - nizes them as the same topic . It can be observed that GTM learns more discriminative topics by examining topic words from overlapping topics , e.g. rec.autos and misc.forsale .", "entities": [[69, 70, "MethodName", "LDA"], [116, 117, "MethodName", "LDA"], [303, 304, "MethodName", "convolution"], [332, 333, "MethodName", "RMSProp"], [340, 341, "HyperparameterName", "optimizer"], [343, 345, "HyperparameterName", "learning rate"], [517, 518, "MethodName", "LDA"], [521, 522, "MethodName", "LDA"], [556, 557, "MethodName", "LDA"], [587, 588, "MethodName", "LDA"], [737, 738, "MethodName", "LDA"], [798, 799, "MethodName", "LDA"], [803, 804, "MethodName", "LDA"], [924, 925, "MethodName", "LDA"], [939, 940, "MethodName", "LDA"]]}
{"text": "As aforesaid , only a few studies handle this type of knowledge inference . The m - TransH method ( Wen et al , 2016 ) defines n - ary relations as the mappings from the attribute sequences to the attribute values . Each n - ary fact is an instance of the corresponding n - ary relation . Then , m - TransH generalizes TransH ( Wang et al , 2014 ) on binary facts to nary facts via attaching each n - ary relation with a hyperplane . RAE ( Zhang et al , 2018 ) further introduces the likelihood that two attribute values co - participate in a common n - ary fact , and adds the corresponding relatedness loss multiplied by a weight factor to the embedding loss of m - TransH. Specifically , RAE applies a fully - connected neural network to model the above likelihood . Differently , NaLP ( Guan et al , 2019 ) represents each n - ary fact as a set of attribute - value pairs directly . Then , convolution is adopted to get the embeddings of the attribute - value pairs , and a fully - connected neural network is applied to evaluate their relatedness and finally to obtain the validity score of the input n - ary fact . In these methods , the information in the same n - ary fact is equal - status . Actually , in each n - ary fact , a primary triple can usually be identified with other information as its auxiliary description ( s ) , as exemplified in Section 1 . Moreover , these methods are deliberately designed only for the inference on whole facts . They have not tackled any distinct inference task . In practice , the newly proposed flexible knowledge inference is also prevalent .", "entities": [[90, 91, "MethodName", "RAE"], [122, 123, "MetricName", "loss"], [131, 132, "MetricName", "loss"], [138, 139, "MethodName", "RAE"], [180, 181, "MethodName", "convolution"]]}
{"text": "The framework of NeuInfer is illustrated in Figure 1 , with the 5 - ary fact presented in Section 1 as an example . For an n - ary fact F ct , we look up the embeddings of its relation r and the attributes in A F ct from the embedding matrix M R R | R | \u00d7k of relations and attributes , where R is the set of all the relations and attributes , and k is the dimension of the latent vector space . The embeddings of h , t , and the attribute values in V F ct are looked up from the embedding matrix M E R | E | \u00d7k of entities and attribute values , where E is the set of all the entities and attribute values . In what follows , the embeddings are denoted with the same letters but in boldface by convention . As presented in Figure 1 , these embeddings are fed into the validity evaluation component ( the upper part of Figure 1 ) and the compatibility evaluation component ( the bottom part of Figure 1 ) to compute the validity score of ( h , r , t ) and the compatibility score of F ct , respectively . These two scores are used to generate the final score of F ct by weighted sum and further compute the loss . Note that , following RAE ( Zhang et al , 2018 ) and NaLP ( Guan et al , 2019 ) , we only apply fully - connected neural networks in NeuInfer .", "entities": [[233, 234, "MetricName", "loss"], [239, 240, "MethodName", "RAE"]]}
{"text": "The final score s F ct of F ct is the weighted sum of the above validity score and compatibility score : s F ct = val hrt comp F ct = w val hrt + ( 1 \u2212 w ) comp F ct , ( 6 ) where w ( 0 , 1 ) is the weight factor . If the arity of F ct is 2 , the final score is equal to the validity score of the primary triple ( h , r , t ) . Then , Equation ( 6 ) is reduced to : s F ct = val hrt . ( 7 ) Currently , we obtain the final score s F ct of F ct . In addition , F ct has its target score l F ct . By comparing s F ct with l F ct , we get the binary crossentropy loss : L F ct = \u2212l F ct logs F ct \u2212 ( 1\u2212l F ct ) log ( 1\u2212s F ct ) , ( 8 ) where l F ct = 1 , if F ct T , otherwise F ct T \u2212 , l F ct = 0 . Here , T is the training set and T \u2212 is the set of negative samples constructed by corrupting the n - ary facts in T . Specifically , for each n - ary fact in T , we randomly replace one of its elements with a random element in E / R to generate one negative sample not contained in T . We then optimize NeuInfer via backpropagation , and Adam ( Kingma and Ba , 2015 ) with learning rate \u03bb is used as the optimizer .", "entities": [[51, 52, "DatasetName", "0"], [152, 153, "MetricName", "loss"], [202, 203, "DatasetName", "0"], [275, 276, "MethodName", "Adam"], [284, 286, "HyperparameterName", "learning rate"], [291, 292, "HyperparameterName", "optimizer"]]}
{"text": "We conduct experiments on two n - ary datasets . The first one is JF17 K ( Wen et al , 2016 ; Zhang et al , 2018 ) , derived from Freebase ( Bollacker et al , 2008 ) . In JF17 K , an n - ary relation of a certain type is defined by a fixed number of ordered attributes . Then , any n - ary fact of this relation is denoted as an ordered sequence of attribute values corresponding to the attributes . For example , for all n - ary facts of the n - ary relation olympics.olympic medal honor , they all have four attribute values ( e.g. , 2008 Summer Olympics , U nited States , N atalie Coughlin , and Swimming at the 2008 Summer Olympics - W omen s 4\u00d7100 metre f reestyle relay ) , corresponding to the four ordered attributes of this n - ary relation . The second one is WikiPeople ( Guan et al , 2019 ) , derived from Wikidata ( Vrande\u010di\u0107 and Kr\u00f6tzsch , 2014 ) . Its n - ary facts are more diverse than JF17 K 's . For example , for all n - ary facts that narrate award - received , some have the attribute together - with , while some others do not . Thus , WikiPeople is more difficult . To run NeuInfer on JF17 K and WikiPeople , we transform the representation of their n - ary facts . For JF17 K , we need to convert each attribute value sequence of a specific n - ary relation to a primary triple coupled with a set of its auxiliary description ( s ) . The core of this process is to determine the primary triple , formed by merging the two primary attributes of the n - ary relation and the corresponding attribute values . The two primary attributes are selected based on RAE ( Zhang et al , 2018 ) . For each attribute of the n - ary relation , we count the number of its distinct attribute values from all the n - ary facts of this relation . The two attributes that correspond to the largest and second - largest numbers are chosen as the two primary attributes . For WikiPeople , since there is a primary triple for each n - ary fact in Wikidata , with its help , we simply reorganize a set of attribute - value pairs in WikiPeople to a primary triple coupled with a set of its auxiliary description ( s ) . The statistics of the datasets after conversion or reorganization are outlined in ciprocal Rank ( MRR ) and Hits@N . For each n - ary test fact , one of its elements is removed and replaced by all the elements in E / R. These corrupted n - ary facts are fed into NeuInfer to obtain the final scores . Based on these scores , the n - ary facts are sorted in descending order , and the rank of the n - ary test fact is stored . Note that , except the nary test fact , other corrupted n - ary facts existing in the training / validation / test set , are discarded before sorting . This process is repeated for all other elements of the n - ary test fact . Then , MRR is the average of these reciprocal ranks , and Hits@N is the proportion of the ranks less than or equal to N . Knowledge inference includes entity inference and relation inference . As presented in Table 1 , the number of relations and attributes in each dataset is far less than that of entities and attribute values ( on JF17 K , | R | = 501 , while | E | = 28 , 645 ; on WikiPeople , | R | = 193 , while | E | = 47 , 765 ) . That is , inferring a relation / attribute is much simpler than inferring an entity / attribute value . Therefore , we adopt MRR and Hits@ { 1 , 3 , 10 } on entity inference , while pouring attention to more finegrained metrics , i.e. , MRR and Hits@1 on relation inference .", "entities": [[326, 327, "MethodName", "RAE"], [451, 452, "MetricName", "MRR"], [573, 574, "MetricName", "MRR"], [693, 694, "MetricName", "MRR"], [717, 718, "MetricName", "MRR"], [719, 720, "MetricName", "Hits@1"]]}
{"text": "The hyper - parameters of NeuInfer are tuned via grid search in the following ranges : The embedding dimension k { 50 , 100 } , the batch size \u03b2 { 128 , 256 } , the learning rate \u03bb { 5e \u22126 , 1e \u22125 , 5e \u22125 , 1e \u22124 , 5e \u22124 , 1e \u22123 } , the numbers n 1 and n 2 of the neural network layers of \" hrt - FCNs \" and \" hrtav - FCNs \" in { 1 , 2 } , the dimension d of the interaction vector o hrta i v i in { 50 , 100 , 200 , 400 , 500 , 800 , 1000 , 1200 } , the weight factor w of the scores in { 0.1 , 0.2 , . . . , 0.9 } . The adopted optimal settings are : k = 100 , \u03b2 = 128 , \u03bb = 5e \u22125 , n 1 = 2 , n 2 = 1 , d = 1200 , and w = 0.1 for JF17 K ; k = 100 , \u03b2 = 128 , \u03bb = 1e \u22124 , n 1 = 1 , n 2 = 1 , d = 1000 , and w = 0.3 for WikiPeople .", "entities": [[17, 19, "HyperparameterName", "embedding dimension"], [27, 29, "HyperparameterName", "batch size"], [29, 30, "HyperparameterName", "\u03b2"], [37, 39, "HyperparameterName", "learning rate"], [148, 150, "HyperparameterName", "k ="], [152, 153, "HyperparameterName", "\u03b2"], [183, 185, "HyperparameterName", "k ="], [187, 188, "HyperparameterName", "\u03b2"]]}
{"text": "The experimental results of simple entity inference are reported in Table 2 . From the results , it can be observed that NeuInfer performs much better than the best baseline NaLP , which verifies the superiority of NeuInfer . Specifically , on JF17 K , the performance gap between NeuInfer and NaLP is significant . In essence , 0.151 on MRR , 14.6 % on Hits@1 , 16.2 % on Hits@3 , and 15.9 % on Hits@10 . On WikiPeople , NeuInfer also outperforms NaLP . It testifies the strength of NeuInfer treating the information in the same n - ary fact discriminatingly . By differentiating the primary triple from other auxiliary description ( s ) , NeuInfer considers the validity of the primary triple and the compatibility between the primary triple and its auxiliary description ( s ) to model each n - ary fact more appropriately and reasonably . Thus , it is not surprising that NeuInfer beats the baselines . And on simpler JF17 K ( see Section 5.1 ) , NeuInfer gains more significant performance improvement than on WikiPeople .", "entities": [[60, 61, "MetricName", "MRR"], [65, 66, "MetricName", "Hits@1"], [70, 71, "MetricName", "Hits@3"], [76, 77, "MetricName", "Hits@10"]]}
{"text": "To further analyze the effectiveness of the proposed NeuInfer method , we look into the breakdown of its performance on different arities , as well as on primary triples and auxiliary descriptions . Without loss of generality , here we report only the experimental results on simple entity inference . The test sets are grouped into binary and n - ary ( n > 2 ) categories according to the arities of the facts . Table 5 presents the experimental results of simple entity inference on these two categories of JF17 K and WikiPeople . From the tables , we can observe that NeuInfer consistently outperforms the baselines on both categories on simpler JF17K. On more difficult WikiPeople , NeuInfer is comparable to the best baseline NaLP on the binary category and gains much better performance on the n - ary category in terms of the fine - grained MRR and Hits@1 . In general , NeuInfer performs much better on JF17 K than on WikiPeople . We attribute this to the simplicity of JF17K. Where does the above performance improvement come from ? Is it from inferring the head / tail entities in primary triples or the attribute values in auxiliary descriptions ? To go deep into it , we study the performance of NeuInfer on inferring the head / tail entities and the attribute values and compare it with the best baseline NaLP . The detailed experimental results are demonstrated in Tables 6 and 7 . It can be observed that NeuInfer brings more performance gain on inferring attribute values . It indicates that combining the validity of the primary triple and the compatibility between the primary triple and its auxiliary description ( s ) to model each n - ary fact is more effective than only considering the relatedness of attribute - value pairs in NaLP , especially for inferring attribute values .", "entities": [[34, 35, "MetricName", "loss"], [149, 150, "MetricName", "MRR"], [151, 152, "MetricName", "Hits@1"]]}
{"text": "In this paper , we distinguished the information in the same n - ary fact and represented each n - ary fact as a primary triple coupled with a set of its auxiliary description ( s ) . We then proposed a neural network model , NeuInfer , for knowledge inference on n - ary facts . NeuInfer combines the validity evaluation of the primary triple and the compatibility evaluation of the n - ary fact to obtain the validity score of the n - ary fact . In this way , NeuInfer has the ability of well handling simple knowledge inference , which copes with the inference on whole facts . Furthermore , NeuInfer is capable of dealing with the newly proposed flexible knowledge inference , which tackles the inference on partial facts consisting of a primary triple coupled with any number of its auxiliary descriptive attributevalue pair ( s ) . Experimental results manifest the merits and superiority of NeuInfer . Particularly , on simple entity inference , NeuInfer outperforms the state - of - the - art method significantly in terms of all the metrics . NeuInfer improves the performance of Hits@3 even by 16.2 % on JF17K. In this paper , we use only n - ary facts in the datasets to conduct knowledge inference . For future works , to further improve the method , we will explore the introduction of additional information , such as rules and external texts .", "entities": [[194, 195, "MetricName", "Hits@3"]]}
{"text": "For the dependency analysis , we use an extended version of UDPipeFuture ( Straka , 2018 ) which showed its state of the art performance by becoming first in terms of the Morphology - aware Labeled Attachment Score ( MLAS ) 3 metric at the CoNLL Shared Task of dependency parsing in 2018 ( Zeman et al , 2018 ) . UDPipeFuture is a POS tagger and graph parser based dependency parser using a BiLSTM , inspired by Dozat et al ( 2017 ) . Our modification consisted in adding several contextual word embeddings ( with respect to the language ) . In order to find the best configuration we experimented with models like multilingual BERT ( Devlin et al , 2019 ) , XLM - R ( Conneau et al , 2019 ) ( for both , English and French ) , RoBERTA ( Liu et al , 2019 ) ( for English ) , FlauBERT ( Le et al , 2020 ) and CamemBERT ( Martin et al , 2019 ) ( for French ) during the training of the treebanks French - GSD and English - EWT 4 , of the Universal Dependencies project ( UD ) ( Nivre et al , 2016 ) 5 . Adding contextual word embedding increases significantly the results for all metrics , LAS , CLAS and MLAS ( cf . table 1 ) . This is the case for all languages ( of the CoNLL shared task ) , where language specific contextual embeddings or multingual ones ( as BERT or XLM - R ) improved parsing ( Heinecke , 2020 ) French ( Fr - GSD ) embeddings MLAS CLAS LAS Straka ( 2018 In order to parse simple , quiz - like questions , the training corpora of the two UD treebanks are not appropriate ( enough ) , since both treebanks do not contain many questions , if at all 6 . An explanation for bad performance on questions of parser models trained on standard UD is the fact , that in both languages , the syntax of questions differs from the syntax of declarative sentences : apart from wh question words , in English the to do periphrasis is nearly always used in questions . In French , subject and direct objects can be inversed and the est - ce que construction appears frequently . Both , the English to do periphrasis and the French est - ce que construction are absent in declarative sentences . Table 2 shows the ( much lower ) results when parsing questions using models trained only on the standard UD treebanks . In order to get a better analysis , we decided to and add this data to the basic treebanks . For English we annotated 309 questions ( plus 91 questions for validation ) from the QALD7 ( Usbeck et al , 2017 ) and QALD8 corpora 7 . For French we translated the QALD7 questions into French and formulated others ourselves ( 276 train , 66 validation ) . For the annotations we followed the general UD guidelines 8 as well as the treebank specific guidelines of En - EWT and Fr - GSD . As table 3 shows , the quality of the dependency analysis improves considerably . The contextual word embeddings CamemBERT ( for French ) and BERT ( English ) have the biggest impact . We rely on the UdpipeFuture version which we have improved with BERT ( for English ) /CamemBERT ( for French ) and which gives the best results in terms of dependency analysis , in order to proceed with the partitioning of the question into textual fragments ( also called chunks ) : Q = { c 1 , c 2 , . . . , c n } . If we take the example of the question What is the political party of the mayor of Paris ? , the set of textual fragments would be Q = { What , is , the political party of the mayor of Paris } .", "entities": [[37, 38, "MetricName", "Score"], [49, 51, "TaskName", "dependency parsing"], [74, 75, "MethodName", "BiLSTM"], [92, 94, "TaskName", "word embeddings"], [115, 116, "MethodName", "BERT"], [124, 125, "MethodName", "XLM"], [194, 196, "DatasetName", "Universal Dependencies"], [198, 199, "DatasetName", "UD"], [258, 259, "MethodName", "BERT"], [260, 261, "MethodName", "XLM"], [301, 302, "DatasetName", "UD"], [337, 338, "DatasetName", "UD"], [438, 439, "DatasetName", "UD"], [517, 518, "DatasetName", "UD"], [552, 554, "TaskName", "word embeddings"], [560, 561, "MethodName", "BERT"], [580, 581, "MethodName", "BERT"]]}
{"text": "The existing QAS test sets are more tailored to systems which generate the exact short answer to a question or more focused on the Machine Reading Comprehension task where the answer consists of a text passage from a document containing the short answer . Therefore , we have created a dataset which maps questions extracted from the QALD - 7 challenge dataset ( Usbeck et al , 2017 ) with natural language answers which were defined by a linguist and which we individually reviewed . This dataset called QUEREO consists of 150 questions with the short answers extracted by the QAS that we described above . We denote an average of three possible gold sanswers in natural language for each question . French and English versions were created for this dataset . As illustrated in figure 1 , two possible architectures of the approach proposed for answer generation have been evaluated . The first architecture A1 consists in generating all possible answer structures in order to have them evaluated afterwards by a LM which will identify the optimal answer structure to which we generate possible missing words . Architecture A2 starts with generating missing words for each structure in S which will then be evaluated by the LM . In this paper , we assume that there is only one missing word per structure . To evaluate the proposed approach , we have referred to standard metrics defined for NLG tasks such as Automatic Translation and Summarization , as they allow to assess to what extent a generated sentence is similar to the gold sentence . We con - sider three N - gram metrics ( BLEU , METEOR and ROUGE ) and the BERT score metric which exploits the pre - trained embeddings of BERT to calculate the similarity between the answer generated and the gold answer . To be able to compare the different configurations of the approach , we refer to Friedman 's test ( Milton , 1939 ) We also conducted a human evaluation study for the French and the English versions of the dataset , in which we asked 20 native speakers participants to evaluate the relevance of a generated answer ( correct or not correct ) regarding a given question while indicating the type of errors depicted ( grammar , wrong preposition , word order , extra word ( s ) , etc ) . Figure 3 presents the evaluation framework that we have implemented and provided to the participants . The results of each participant are saved in a json - file ( figure 4 ) . The inter - agreement rate between participants reached 70 % which indicates a substantial agreement . Through the human evaluation study , we wanted to explore to what extent the standard metrics are reliable to assess NLG approaches within the context of question - answering systems . Table 4 ( French dataset ) represents the obtained results for the first three best models according to the human evaluation ranking and the Friedman test ranking . We indicate between brackets each model 's rank according to the metric used . We note that the highest human accuracy score for French of about 85 % was scored with the first architecture coupled with BERT as the generation model ( GM ) and CamemBERT as the language model ( LM ) . We also notice that the architecture A1 , which considers the LM assessment of the structure before generating missing words , performs better . Surprisingly , as a generative model , the multi - Table 4 : Model ranking for French dataset according to the human evaluation study ( best in bold ) and the Friedman test ( best in yellow ) . \" BT \" in Column GM stands for BERT - base - multilingual - cased . In column LM we use \" CmBT \" for CamemBERT - base , \" BT - ml - c \" for BERT - base - multilingual - cased , \" XRob \" for XLM - RoBERTa - base , \" FBT - s - c \" for FlauBERT - small - cased , \" FBT - b - uc \" for FlauBERT - base - uncased and \" clm - 1024 \" for XLM - clmenfr - 1024 lingual BERT model predicts missing words better than CamemBERT for French sentences . These findings are also confirmed by the Friedman test where we can clearly see that the first ranked configuration maps the best configuration selected according to the human accuracy , with a very slight difference for the other four configurations . Let us see if that means that the four metrics are correlated with the human accuracy . According to table 6 which presents the Pearson correlation ( Benesty et al , 2009 ) of the human accuracy with the four metrics and to figure 2 which illustrates the ranking given by each evaluation metric along with the human judgement for each configuration ( i.e. configuration = GM \u00d7 architecture \u00d7 LM ) tested , we can clearly see that the human evaluation results are positively and strongly correlated with the BLEU , the METEOR and the BERT scores . These metrics are practically matching the human ranking and thus are obviously able to identify which configuration gives better results . The rouge metric , used for French question / answer evaluation , is moderately correlated with the human evaluation which means that we should not only rely on this metric when assessing such task . On the other hand , when the ROUGE metric is considered with the other metrics , it helps to get closer to the human judgement . Table 5 presents the results for the English dataset and shows that the best accuracy scored is about 72 % with A1 , BERT as the generative model and the Generative Pretrained Transformer ( GPT ) as the language model . According to the first three configurations , architecture A2 prevails and the GPT transformer takes over the other lan - guage models . These results are also confirmed by the Friedman test with a very slight difference on the ranking and also upheld with the correlation scores between the human assessment and each of the four metrics as shown by figure 5 and table 6 . These findings mean that we actually can rely on the use of these standard metrics to evaluate the answer generation task for question - answering . We also tried to analyse the errors indicated by the participants . As we can note from figure 6 , the most common error reported for both English and French datasets is the word order which sheds the light on a problem related to the language model assessment phase . The second most reported error addresses the generation process , whether to indicate that there are one or more missing words within the answer ( French ) or the presence of some odd words ( English ) . When trying to get an insight on the answers generated by the current intelligent systems such as Google assistant and Alexa , we noted that these systems are very accurate when extracting the correct answer to a question and can sometimes generate user - friendly answers that help recall the question context , specially with Alexa . However , we noticed that most of the answers generated by these systems are more verbose than necessary , we also found out that when addressing yes / no questions , these systems generally settle for just a yes or no without elaborating , or , on the other hand , present a text span extracted from a Web page and let the user guess the answer . Let us take for example the following question Was US president Jackson involved in a war ? Table 5 : Model ranking for English dataset according to the human evaluation study ( best in bold ) and the Friedman ranking ( best in yellow ) . In Column GM we use \" BT - ml \" for BERT - base - multilingual - cased and \" BT \" for BERTlarge - cased . In column LM \" GPT \" stands for for OpenAI - GPT , \" GPT2 - l \" for GPT2 - large , \" GPT2 - m \" for GPT2medium , \" GPT2 \" for GPT2 , \" BT - b - uc \" for BERT - base - uncased , \" mlm - 2048 \" for XLM - mlm - en - 2048 and \" BT - l - c \" for BERT - large - cased . [ { \" ID \" : \" quereo_5.4 \" , \" QUESTION \" : \" Quelles sont les companies d'\u00e9lectronique fond\u00e9es a Beijing ? \" , \" SHORT_ANSWER \" : [ \" Xiaomi \" , \" Lenovo \" ] , \" GENERATED_ANSWER \" : \" Les companies d'\u00e9lectronique fond\u00e9es\u00e0 beijing sont xiao xiaomi et lenovo \" , \" MISSING_WORD \" : \" Xiao \" , \" EVALUATION \" : \" correcte \" , \" ERROR \" : [ \" aucun \" ] , \" COMMENT \" : \" \" } , { \" ID \" : \" quereo_8.8 \" , \" QUESTION \" : \" Combien de films a r\u00e9alis\u00e9 Park Chan - wook ? \" , \" SHORT_ANSWER \" : [ \" quatorze \" ] , \" GENERATED_ANSWER \" : \" Quatorze films a r\u00e9alis\u00e9 park chan - wook \" , \" MISSING_WORD \" : \" . \" , \" EVALUATION \" : \" incorrecte \" , \" ERROR \" : [ \" ordre \" , \" accord \" ] , \" COMMENT \" : \" \" } , ... ] Here 's something I found on the Web . According to constitutioncenter.org : After the War of 1812 , Jackson led military forces against the Indians and was involved in treaties that led to the relocation of Indians . The user has to focus on the returned text fragment in order to guess that the answer to his question is actually yes . This issue was particularly noted when addressing French questions . If we also take the example How many grandchildren did Jacques Cousteau have ? the two systems answer as follows : Fabien Cousteau , Alexandra Cousteau , Philippe Cousteau Jr. , C\u00e9line Cousteau . Jacques Cousteau 's grandchildren were Philippe Cousteau Jr. , Alexandra ousteau , C\u00e9line Cousteau , and Fabien Cousteau However , the user is not asking about the names of Cousteau 's grand - children and has to guess by himself that the answer for this question is four . A more accurate answer should indicate the exact answer to the question and then elaborate Jacques Cousteau had four grand - children . But these systems perform better in case when the terms employed in the question are not necessarily relevant to the answer . If we take the example of the question who is the wife of Lance Bass , the approach that we propose will generate The wife of Lance Bass is Michael Turchin . As we can note the answer generated was not adapted to the actual answer , while the other systems are able to detect such nuance : Lance Bass is married to Michael Turchin . They have been married since 2014 . This issue has still to be addressed .", "entities": [[24, 27, "TaskName", "Machine Reading Comprehension"], [146, 148, "TaskName", "answer generation"], [244, 245, "TaskName", "Translation"], [246, 247, "TaskName", "Summarization"], [276, 277, "MetricName", "BLEU"], [278, 279, "DatasetName", "METEOR"], [284, 285, "MethodName", "BERT"], [295, 296, "MethodName", "BERT"], [529, 530, "MetricName", "accuracy"], [545, 546, "MethodName", "BERT"], [634, 635, "MethodName", "BERT"], [663, 664, "MethodName", "BERT"], [675, 676, "MethodName", "XLM"], [677, 678, "MethodName", "RoBERTa"], [715, 716, "MethodName", "XLM"], [721, 722, "MethodName", "BERT"], [761, 762, "MetricName", "accuracy"], [789, 790, "MetricName", "accuracy"], [798, 800, "MetricName", "Pearson correlation"], [810, 811, "MetricName", "accuracy"], [864, 865, "MetricName", "BLEU"], [867, 868, "DatasetName", "METEOR"], [870, 871, "MethodName", "BERT"], [969, 970, "MetricName", "accuracy"], [978, 979, "MethodName", "BERT"], [987, 988, "MethodName", "Transformer"], [989, 990, "MethodName", "GPT"], [1008, 1009, "MethodName", "GPT"], [1080, 1082, "TaskName", "answer generation"], [1193, 1194, "DatasetName", "Google"], [1358, 1359, "MethodName", "BERT"], [1378, 1379, "MethodName", "GPT"], [1385, 1386, "MethodName", "GPT"], [1419, 1420, "MethodName", "BERT"], [1426, 1427, "DatasetName", "mlm"], [1428, 1429, "DatasetName", "2048"], [1431, 1432, "MethodName", "XLM"], [1433, 1434, "DatasetName", "mlm"], [1437, 1438, "DatasetName", "2048"], [1447, 1448, "MethodName", "BERT"]]}
{"text": "We study model - generated answers from a state - of - the - art LFQA system ( Krishna et al , 2021 ) . 9 Their model uses passage retriever ( Guu et al , 2020 ) , and generates answers based on the retrieved passage with a routing transformer model . Q / A Validity We collect validity annotation on 193 model - generated answers , and 78 are considered invalid , significantly higher ratio than that of human written answers ( Table 2 ) . The Fleiss 's kappa of QA pair validity is 0.26 ( and 0.61 after collecting one more label ) , substantially lower than the agreement on human written answers ( 0.51 , 0.70 ) while annotated by the same set of annotators . Detailed distribution of invalid reason annotated can be found in Table 9 . Despite the low agreement , 60 of them are marked as \" No valid answer \" by at least two annotators . The flaw of automatic measures was also pointed out by prior work ( Krishna et al , 2021 ) , which compares ROUGE between humanwritten and model - generated answers . Our study reiterates that the generated answers received high ROUGE score without answering the question . Roles We proceed to collect sentence - level role annotations on 115 valid generated long - form answers following the same annotation setup in Section 3 , and hence our annotators were not asked to evaluate the correctness or the quality of the answers ( e.g. whether the generated example makes sense ) , focusing on the functional roles of sentences only . We found that the annotators disagree substantially more as compared to the humanwritten answers , with a Fleiss kappa of 0.31 ( vs. 0.45 for human - written answers ) , suggesting that the discourse structure of model - generated answers are less clear , even to our trained annotators . The answer role distribution of model - generated answers is very different from that of the human written answers ( Figure 4 ) . The generated answers contain more sentences which provide auxiliary information , and fewer summary sentences . This suggests that model - generated answers contain a higher portion of information tangentially related to what is asked in the question . Model - generated answers also contain fewer example and miscella - 9 We sampled from four different model configurations reported in their paper , i.e. combination of nucleus sampling threshold p= { 0.6 , 0.9 } , and generation conditioning on { predicted , random } passages . The answers we annotated achieved a ROUGE - L of 23.19 , higher than that of human - written answers on the same set of questions ( 21.28 ) . neous sentences . Examples of annotated modelgenerated answer can be found in Table 10 . Overall , our results suggest that machinegenerated long form answers are different from human - written answers , and judging their discourse structure is nontrivial for human annotators , resulting in lower agreement . Recent study ( Karpinska et al , 2021 ) also showed that expert annotators showed lower agreement and took longer time to evaluate the coherence of story generated from large - scale language model .", "entities": [[49, 51, "MethodName", "routing transformer"], [444, 447, "MetricName", "ROUGE - L"]]}
{"text": "Task and Data Given a question q and its longform answer consisting of sentences s 1 , s 2 ... s n , the goal is to assign each answer sentence s i one of the six roles defined in Section 2 . As we have annotated more examples from ELI5 dataset ( 411 answer paragraphs compared to around 100 paragraphs in other three datasets ( WebGPT , NQ and ELI5 - model ) ) , we randomly split the ELI5 longform answers into train , validation and test sets with a 70%/15%/15 % ratio , and train the model on the training portion . We use all other annotated datasets for evaluation only . For model - generated answers , we filtered 185 out of 1080 sentences where model - generated sentences do not have a majority role . This setup also allows us to study domain transfer of role classification model . Metrics We report accuracy with respect to the majority role label ( or adjudicated one , if majority does n't exist ) ( Acc ) , match on any label from three annotators ( Match ) , F1 score for each role and their macro average score Macro - F1 .", "entities": [[50, 51, "DatasetName", "ELI5"], [68, 69, "DatasetName", "NQ"], [70, 71, "DatasetName", "ELI5"], [80, 81, "DatasetName", "ELI5"], [157, 158, "MetricName", "accuracy"], [177, 178, "MetricName", "Acc"], [191, 193, "MetricName", "F1 score"], [201, 204, "MetricName", "Macro - F1"]]}
{"text": "Lower bounds We present two simple baselines to provide lower bounds : ( 1 ) Majority : We predict the most frequent labels in the training data : Answer - Summary . ( 2 ) Summary - lead : We predict first two sentences as Answer - Summary , and the rest of the sentences as Answer . Classification Models This baseline classifies each sentence independently . We use the [ CLS ] token from RoBERTa - Large model ( Liu et al , 2019 ) which encodes [ question < q > ans 1 ... < start > ans i < end > ... ] , where ans i is the i th sentence in the answer . The training batch size is set to 64 , with the initial learning rate as 5e \u2212 5 . We used AdamW optimizer and a linear learning rate schedule . We train the model for 10 epochs and report the result of the checkpoint with best validation accuracy , averaged across three random seeds . Seq2Seq Models We use two variations ( base , large ) of T5 model ( Raffel et al , 2020 ) , which take the concatenation of question and answer sentences , and output the roles for each sentence sequentially . This model predicts the roles of all sentences in the answer as a single sequence . The input sequence is [ question [ 1 ] ans 1 [ 2 ] ans 2 ... ] , where ans i denotes the i th sentence in the answer , and the target output sequence is set to [ [ 1 ] role 1 [ 2 ] role 2 [ 3 ] ... ] , where role i is the corresponding role for ans i ( e.g. \" Answer \" for the Answer role ) . We limit the input / output to be 512/128 tokens . For evaluating the predicted roles , we parse the output string to identify the role predicted for each sentence . We used the batch size of 16 , initial learning rate of 1e\u22124 with AdamW optimizer and a linear learning rate schedule . We train the model for 30 epochs and report the result of the checkpoint with the best validation accuracy , averaged across three random seeds . Human performance We provide two approximations for human performance : upperbound ( u ) and lowerbound ( l ) . ( 1 ) Human ( u ) : We compare each individual annotator 's annotation with the majority label . This inflates human performance as one 's own judgement affected the majority label . ( 2 ) Human ( l ) : We compare all pairs of annotation and calculate average F1 and accuracy of all pairs . For Match , we compute the match for each annotation against the other two annotations .", "entities": [[58, 59, "TaskName", "Classification"], [75, 76, "MethodName", "RoBERTa"], [121, 123, "HyperparameterName", "batch size"], [131, 133, "HyperparameterName", "learning rate"], [140, 141, "MethodName", "AdamW"], [141, 142, "HyperparameterName", "optimizer"], [145, 147, "HyperparameterName", "learning rate"], [166, 167, "MetricName", "accuracy"], [172, 173, "DatasetName", "seeds"], [174, 175, "MethodName", "Seq2Seq"], [186, 187, "MethodName", "T5"], [342, 344, "HyperparameterName", "batch size"], [348, 350, "HyperparameterName", "learning rate"], [353, 354, "MethodName", "AdamW"], [354, 355, "HyperparameterName", "optimizer"], [358, 360, "HyperparameterName", "learning rate"], [380, 381, "MetricName", "accuracy"], [386, 387, "DatasetName", "seeds"], [458, 460, "MetricName", "average F1"], [461, 462, "MetricName", "accuracy"]]}
{"text": "If not , the parents could just take the house and all the money ( provided the person did n't have a will Question classification model A difficulty in repurposing NQ is that not all questions with paragraph answers only actually need multiple sentences . To identify complex questions , we built a simple BERT - based classifier , trained to distinguish NQ questions with short answers ( i.e. , less than five tokens ) and ELI5 questions . We use the [ CLS ] token from BERT model to perform prediction . We use the original split from the ELI5 dataset , and split the NQ open 's validation set into val and test set . We preprocessed the questions by converting to lowercase and exclude punctuation to remove syntactic differences between ELI5 and NQ questions . We fine - tuned the bert - base - uncased model for 3 epochs , with an initial learning rate of 5e \u2212 5 and batch size of 32 . We use the model with the highest validation F1 as the question classifier , which achieves F1 of 0.97 and 0.94 on validation and test set respectively . We then run this classifier to select the non factoid questions from NQ questions with long - form answers , which classifies around 10 % , out of the 27 , 752 NQ long questions as non - factoid . Examples are in Table 7 .", "entities": [[30, 31, "DatasetName", "NQ"], [54, 55, "MethodName", "BERT"], [62, 63, "DatasetName", "NQ"], [76, 77, "DatasetName", "ELI5"], [87, 88, "MethodName", "BERT"], [100, 101, "DatasetName", "ELI5"], [106, 107, "DatasetName", "NQ"], [133, 134, "DatasetName", "ELI5"], [135, 136, "DatasetName", "NQ"], [156, 158, "HyperparameterName", "learning rate"], [163, 165, "HyperparameterName", "batch size"], [176, 177, "MetricName", "F1"], [184, 185, "MetricName", "F1"], [208, 209, "DatasetName", "NQ"], [228, 229, "DatasetName", "NQ"]]}
{"text": "In addition to use a pre - defined sentiment lexicon for word - level annotations , we also propose to learn the word - level sentiment supervision , based on PMI and SO . ( 1 ) PMI and SO Given a corpus with document - level class labels . We first compute the PMI score between each word t and two class labels P M I ( t , + ) = log p ( + | t ) p ( + ) , ( 1 ) P M I ( t , \u2212 ) = log p ( \u2212 | t ) p ( \u2212 ) , ( 2 ) where + and \u2212 denote the positive and negative document - level class labels , respectively . Second , we compute the SO score for each word t : SO ( t ) = P M I ( t , + ) \u2212 P M I ( t , \u2212 ) . ( 3 ) We call { t , SO ( t ) } as PMI - SO dictionary . The PMI - SO dictionary was widely used as a corpusbased sentiment lexicon for sentiment classification . By contrast , in our approach , it is the first step to learn the sentiment - aware word representation . Our approach supports two kinds of wordlevel sentiment annotations : 1 ) PMI - SO dictionary with hard sentiment annotation ; 2 ) PMI - SO dictionary with soft sentiment annotation . The bias of document - level softmax layer \u03b8t Weight of word - level softmax layer \u03b8 d Weight of document - level softmax layer p ( c | et ) The sentiment distribution of word t predicted by our model p ( c | de ) The sentiment distribution of document d predicted by our model p ( c | t ) The word - level sentiment annotation of word t with respect to class \u0109 p ( c | d ) The document - level sentiment annotation of document d with respect to class c [ p ( \u2212 | t ) , p ( + | t ) ] = \uf8f4 \uf8f2 \uf8f4 \uf8f3 [ 0 , 1 ] , if SO ( t ) > 0 [ 1 , 0 ] , if SO ( t ) < 0 random { [ 1 , 0 ] or [ 0 , 1 ] } , otherwise . ( 4 ) ( 3 ) PMI - SO lexicon with soft sentiment annotation \" Soft sentiment annotation \" means that the annotation is given by the probability of two sentiment polarities , rather than the class label . We first use the sigmoid function to map the SO score to the range of a probability , and then define [ p ( \u2212 | t ) , p ( + | t ) ] = [ 1 \u2212 \u03c3 ( SO ( t ) ) , \u03c3 ( SO ( t ) ) ] ( 5 ) as the PMI - SO soft sentiment distribution of the word t.", "entities": [[259, 260, "MethodName", "softmax"], [267, 268, "MethodName", "softmax"], [269, 270, "HyperparameterName", "\u03b8"], [276, 277, "MethodName", "softmax"], [371, 372, "DatasetName", "0"], [382, 383, "DatasetName", "0"], [386, 387, "DatasetName", "0"], [395, 396, "DatasetName", "0"], [401, 402, "DatasetName", "0"], [405, 406, "DatasetName", "0"]]}
{"text": "Till now we have obtained both document and word - level sentiment annotations , in the next step , we propose a neural network framework to learn the sentiment - aware word representation by integrating the sentiment supervision at both word and document granularities . We call it \" hierarchical sentiment supervision \" . The architecture of our model is shown in Figure 1 . We denote the corpus as D = { d 1 , d 2 , ... , d N } where N is the size of the corpus . Suppose d k is k - th document in D , and t i represents the i - th word in a document d. The parameters used in our neural network are described in Table 1 . We construct a embedding matrix C R V \u00d7M , of which each row represents the embedding of a word in the vocabulary , where V is the size of the vocabulary and M is the dimension of word embedding . We randomly initialize each element of matrix C with a normal distribution . ( 1 ) Word - Level Sentiment Supervision We use the word - level sentiment annotation [ p ( \u2212 | t ) , p ( + | t ) ] provided in Section 3.1 to supervise word representation learning at the word level . For each word in document d , we map it to a continuous representation as e C and feed e into our model to predict the sentiment distribution of the input word : p ( c | e ) = sof tmax ( \u03b8 t e + b t ) . ( 6 ) The cost function is defined as the average cross entropy that measures the difference between the sentiment distribution predicted in our model and the sentiment annotations at the word level : f word = \u2212 1 T N k=1 t d k c { + , \u2212 } p ( c | t ) log p ( c | e t ) ( 7 ) where T is the number of words in corpus . ( 2 ) Document - Level Sentiment Supervision We use the document - level sentiment annotations to supervise word representation learning at the document level . In order to obtain a continuous representation of a document d , we simply use the average embedding of words in d as de : de = 1 | d | t d e t . ( 8 ) We feed de into our model to predict the sentiment probability : t i is the i - th word in d. And e t i represents the embedding of the word t i . We take de , the average embedding of [ e t 1 , e t 2 , . . . , e tn ] , as the representation of document d. We get each embedding of words in d as input to predict its sentiment polarities . We also take de as input to predict the sentiment for document d one time per epoch . p ( c | de ) = sof tmax ( \u03b8 d de + b d ) . ( 9 ) Similarly , the cost function is defined as average cross entropy that measures the difference between the sentiment distribution predicted in our model and the sentiment annotation at the document level : f doc = \u2212 1 N N k=1 c { + , \u2212 } p ( c | d k ) log p ( c | de k ) ( 10 ) wherep ( c | d k ) is the sentiment annotation of doc - ument d k .p ( c | d k ) = 1 denotes the class label of d k is positive , otherwisep ( c | d k ) = 0 . ( 3 ) Word and Document - Level Joint Learning In order to learn the sentiment - aware word representation at both word and document levels , we integrate the cost function of two levels in a weighted combination way . The final cost function is defined as follows : f = \u03b1f word + ( 1 \u2212 \u03b1 ) f doc ( 11 ) where \u03b1 is a tradeoff parameter ( 0 \u2264 \u03b1 \u2264 1 ) . The weight of f word can be increased by choosing a lager value of \u03b1 . We train our neural model with stochastic gradient descent and use AdaGrad ( Duchi et al , 2011 ) to update the parameters .", "entities": [[222, 224, "TaskName", "representation learning"], [269, 270, "DatasetName", "sof"], [272, 273, "HyperparameterName", "\u03b8"], [377, 379, "TaskName", "representation learning"], [530, 531, "DatasetName", "sof"], [533, 534, "HyperparameterName", "\u03b8"], [652, 653, "DatasetName", "0"], [712, 713, "HyperparameterName", "\u03b1"], [720, 721, "HyperparameterName", "\u03b1"], [726, 727, "DatasetName", "0"], [728, 729, "HyperparameterName", "\u03b1"], [747, 748, "HyperparameterName", "\u03b1"], [755, 758, "MethodName", "stochastic gradient descent"], [760, 761, "MethodName", "AdaGrad"]]}
{"text": "We utilize the public distant - supervision corpus 2 ( Go et al , 2009 ) to learn our lexicons . We set M , the dimension of embedding , as 50 . The learning rate is 0.3 for stochastic gradient descent optimizer . We tune the hyper - parameter \u03b1 in the training process . We evaluate the sentiment lexicons in both supervised and unsupervised sentiment classification tasks , on the SemEval 2013 - 2016 datasets . The statistics of evaluation datasets are shown in Table 2 .", "entities": [[34, 36, "HyperparameterName", "learning rate"], [39, 42, "MethodName", "stochastic gradient descent"], [42, 43, "HyperparameterName", "optimizer"], [50, 51, "HyperparameterName", "\u03b1"], [72, 74, "DatasetName", "SemEval 2013"]]}
{"text": "To evaluate the effect of the sentiment lexicon in supervised sentiment classification , we report the supervised sentiment classification performance by using some pre - defined lexicon features . We follow ( Mohammad et al , 2013 ) to extract the lexicon features as follows : Total count of words in the tweet score of which is greater than 0 ; Total count of words in the tweet score of which is less than 0 ; The sum of scores for all word great than 0 ; 2 http://help.sentiment140.com/for - students The sum of scores for all word less than 0 ; The max score greater than 0 ; The min score less than 0 ; Non - zero score of the last positive word in the tweet ; Non - zero score of the last negative word in the tweet . We report the performance of SVM by using these lexicon features . The LIBSVM 3 toolkit is used with a linear kernel and the penalty parameter is set as the default value . The metric is F 1 score . Unsupervised Sentiment Classification Evaluation : For unsupervised sentiment classification , we sum up the scores of all sentiment words in the document , according to the sentiment lexicon . If the sum is greater than 0 , the document will be considered as positive , otherwise negative . The unsupervised learning evaluation metric is accuracy .", "entities": [[59, 60, "DatasetName", "0"], [74, 75, "DatasetName", "0"], [85, 86, "DatasetName", "0"], [100, 101, "DatasetName", "0"], [107, 108, "DatasetName", "0"], [114, 115, "DatasetName", "0"], [147, 148, "MethodName", "SVM"], [184, 185, "TaskName", "Classification"], [217, 218, "DatasetName", "0"], [236, 237, "MetricName", "accuracy"]]}
{"text": "We compare our HSSWE method with four sentiment lexicons generated by the related work proposed in recent years : Sentiment140 was constructed by Mohammad et al ( 2013 ) on tweet corpus based on PMI between each word and the emoticons . HIT was constructed by Tang et al ( 2014a ) with a representation learning approach . NN was constructed by Vo and Zhang ( 2016 ) with a neural network method . ETSL refers to SemEval - 2015 English Twitter Sentiment Lexicon 4 ( Rosenthal et al , 2015 ; Kiritchenko et al , 2014 ) , which is done using Best - Worst Scaling . Note that Tang et al ( 2014a ) all the comparison experiments on the complete benchmark datasets . Supervised Sentiment Classification : We first report the supervised sentiment classification F 1 score of five compared methods on the Semeval 2013 - 2016 datasets in Table 3 . It can be seen that our HSSWE method gets the best result on all four datasets . It outperforms Sentiment140 , HIT , NN and ETSL 1.7 , 2.8 , 1.9 , and 3.2 percentages on the average of four datasets . The improvements are significant according to the paired t - test . Unsupervised Sentiment Classification : We then report the unsupervised sentiment classification accuracy of five methods on the Semeval 2013 - 2016 datasets in Table 4 . In can be seen that HSSWE obtains the best performance on Semeval 2013 - 2015 . On the Semeval 2016 dataset , it is slightly lower than ETSL . Across four datasets , the average accuracy of HSSWE is 6.6 , 3.1 , 9.6 and 0.94 higher than Sentiment140 , HIT , NN and ET - SL , respectively .", "entities": [[19, 20, "DatasetName", "Sentiment140"], [54, 56, "TaskName", "representation learning"], [128, 129, "TaskName", "Classification"], [146, 148, "DatasetName", "Semeval 2013"], [174, 175, "DatasetName", "Sentiment140"], [211, 212, "TaskName", "Classification"], [220, 221, "MetricName", "accuracy"], [226, 228, "DatasetName", "Semeval 2013"], [246, 248, "DatasetName", "Semeval 2013"], [269, 271, "MetricName", "average accuracy"], [283, 284, "DatasetName", "Sentiment140"]]}
{"text": "In order to further verify the effectiveness of our method and analyze which part of our model contributes the most , we carried out the internal comparison within our model . We design the following two simplified versions of our model for comparison : PMI - SO denotes a PMI - SO based sentiment lexicon with soft sentiment annotation learned in Section 3.1 . Doc - Sup denotes the neural network system with only document - level sentiment supervision . It equals to HSSWE when \u03b1 = 0 . Actually , HSSWE can be viewed as a \" combination \" of PMI - SO and Doc - Sup . In Tables 5 and 6 , we report the comparison results on supervised and unsupervised sentiment classification respectively . Supervised Sentiment Classification : As is shown in Table 5 , two basic models PMI - SO and Doc - Sup show similar performance . They have distinct superiority across different datasets . But both are significantly lower than HSSWE . It shows that by combing the supervision at both document and word levels , it can indeed improve the quality of sentiment - aware word embedding and the subsequent sentiment lexicon . Unsupervised Sentiment Classification : As is shown in Table 6 , the conclusions are similar with that in supervised sentiment classification : HSS - WE achieves the significantly better performance .", "entities": [[85, 86, "HyperparameterName", "\u03b1"], [87, 88, "DatasetName", "0"], [130, 131, "TaskName", "Classification"], [203, 204, "TaskName", "Classification"]]}
{"text": "In this section , we discuss the tradeoff between two parts of supervisions by turning the tradeoff parameter \u03b1 . When \u03b1 is 0 , HSSWE only benefits from the document - level sentiment supervision and when \u03b1 is 1 , HSSWE benefits from only word - level sentiment supervision . We observe that HSSWE performs better when \u03b1 is in the range of [ 0.45 , 0.55 ] . By integrating two component parts of sentiment supervision , HSSWE has significant superiority over that learned from either one .", "entities": [[18, 19, "HyperparameterName", "\u03b1"], [21, 22, "HyperparameterName", "\u03b1"], [23, 24, "DatasetName", "0"], [37, 38, "HyperparameterName", "\u03b1"], [58, 59, "HyperparameterName", "\u03b1"]]}
{"text": "Named entity recognition is a key component of many text processing pipelines and it is thus essential for this component to be robust to different types of input . However , domain transfer of NER models with data from multiple genres has not been widely studied . To this end , we conduct NER experiments in three predictive setups on data from : a ) multiple domains ; b ) multiple domains where the genre label is unknown at inference time ; c ) domains not encountered in training . We introduce a new architecture tailored to this task by using shared and private domain parameters and multi - task learning . This consistently outperforms all other baseline and competitive methods on all three experimental setups , with differences ranging between +1.95 to +3.11 average F1 across multiple genres when compared to standard approaches . These results illustrate the challenges that need to be taken into account when building real - world NLP applications that are robust to various types of text and the methods that can help , at least partially , alleviate these issues .", "entities": [[0, 3, "TaskName", "Named entity recognition"], [34, 35, "TaskName", "NER"], [53, 54, "TaskName", "NER"], [107, 111, "TaskName", "multi - task learning"], [134, 136, "MetricName", "average F1"]]}
{"text": "We propose a new architecture based on the BiLSTM - CRF model tailored to the three proposed experimental setups . Our proposed architecture enhances the base architecture with three components : a ) domain - specific and - independent feed - forward layers that process the BiLSTM outputs ; b ) domain - specific and - independent feed forward layers CRFs ; c ) a multi - task learning objective that learns domain labels as an auxiliary task . The proposed architecture changes are motivated by the aim of capturing commonalities in which named entities are referred to , in any given genre , while still allowing for the model to tease apart and exploit domain - specific aspects . The architecture is also designed to capture these commonalities across label relationships , which can vary across domains . In addition , the multi - task objective further assists the model to leverage domaindependent and - independent components . The choice of input representation is orthogonal to the proposed architecture and our extensions to the architecture can be combined with any input repre - sentation . The model architecture is presented in Figure 1 and described below : Private and Shared Layers We rely on the shared - private paradigm where the model learns both a shared representation across all domains and is useful when the domain of the input is unknown or unseen in training , and a private domain representation that mostly helps tagging in that domain . We model the shared and private features at both the feature mapping stage connecting the BiLSTM outputs to the CRF ( s ) and at the CRF level . We expect the features extracted by the BiLSTM layers to model the structure of the input across all domains . The feed - forward layers capture the domainspecific and - independent information by using private output layers for each domain and one shared output layer . In training , the BiLSTM outputs are projected to both the shared layer and the private layer based on the domain label provided in training . The CRF layer is used to make a global decision for the entire tag sequence by modelling label dependencies . We expect that this decision is , at least partially , dependent on domain - specific relationships in the label space . Hence , each feedforward layer feeds into either private CRFs ( one for each domain ) or a shared CRF . The separation of the shared and private layers could happen before the CRF stage ( late separation ) or before the feed - forward layer stage ( early separation ) . We investigate the influence of each individual addition on the multi - domain performance in our analysis section through ablation studies . Given an input , both the shared and the private parameters are used in learning to predict the output . The set of private parameters for each domain are only updated by data from the same domain while the set of shared parameters are updated in a pooled way by taking all available data points in the training stage regardless of the domain characteristics . For a given data point , inference can be run either by : a ) passing it though the private components if the domain label is known ; b ) through the shared components if the domain label in unknown or the domain of the data is unseen in training . To this end , the objective function for the private and shared layers is : LNER SP ( x , y ) = LNER S ( x , y ) + LNER P ( x , y ) ( 1 ) where L N ER S and L N ER P stand for the shared layer loss and private layer loss respectively . Multi - Task Learning of Domain Labels Further , to better guide the learning process , we augment our architecture with a multi - task learning objective . Through this , the model learns to predict the domain label of each sample in training as an auxiliary task . The architecture uses average pooling on BiLSTM outputs followed by a fully connected layer . Finally , softmax is applied over the learned domain feature to obtain a probability distribution of all domain labels . The domain classification objective is to minimize the crossentropy loss L domain ( x , y d ) for an input x with domain label y d . The global objective function is the combination of the NER loss function and domain loss : L ( x ; y , y d ) = LNER SP ( x , y ) + L domain ( x , y d ) ( 2 ) 4 Experimental setup", "entities": [[8, 9, "MethodName", "BiLSTM"], [10, 11, "MethodName", "CRF"], [46, 47, "MethodName", "BiLSTM"], [65, 69, "TaskName", "multi - task learning"], [265, 266, "MethodName", "BiLSTM"], [269, 270, "MethodName", "CRF"], [276, 277, "MethodName", "CRF"], [286, 287, "MethodName", "BiLSTM"], [329, 330, "MethodName", "BiLSTM"], [352, 353, "MethodName", "CRF"], [412, 413, "MethodName", "CRF"], [426, 427, "MethodName", "CRF"], [639, 640, "MetricName", "loss"], [643, 644, "MetricName", "loss"], [646, 650, "TaskName", "Multi - Task Learning"], [668, 672, "TaskName", "multi - task learning"], [698, 700, "MethodName", "average pooling"], [701, 702, "MethodName", "BiLSTM"], [712, 713, "MethodName", "softmax"], [739, 740, "MetricName", "loss"], [767, 768, "TaskName", "NER"], [768, 769, "MetricName", "loss"], [772, 773, "MetricName", "loss"]]}
{"text": "For our experiments , we largely follow the training and evaluation procedure used in ( Akbik et al , 2018 ) . As hyperparameters , we follow most suggestions outlined in the in - depth study on model robustness ( Reimers and Gurevych , 2017 ) . Our training uses 256 hidden states for BiLSTM with mini - batch size of 32 . The model parameters are updated using back - propagation and Adam optimizer ( Kingma and Ba , 2014 ) . The learning rate is 1e \u22123 with weight decay value 1e \u22125 . The model is regularized with a locked dropout rate of 0.5 . We use 300 - dimensional pre - trained word embeddings as described in Section 3.1 , whereas the character LSTM is randomly initialized and has a hidden dimension of 64 . The embeddings are updated on the training data . When training the domain features together with the NER ( PoolDomain+DomainFeat ) , we set the domain embedding size to 128 . We train all models for 20 epochs and report the results for the model performing best on the joint development set of the open data set collection .", "entities": [[54, 55, "MethodName", "BiLSTM"], [56, 60, "HyperparameterName", "mini - batch size"], [73, 74, "MethodName", "Adam"], [74, 75, "HyperparameterName", "optimizer"], [84, 86, "HyperparameterName", "learning rate"], [90, 92, "MethodName", "weight decay"], [116, 118, "TaskName", "word embeddings"], [127, 128, "MethodName", "LSTM"], [156, 157, "TaskName", "NER"]]}
{"text": "First , we compare models when assuming the domain label of each test document is known at inference time . The results are listed in Table 2 . Our proposed method - MultDomain - SP - Aux ( P ) - obtains the best results across the entire test collection in both micro - average ( +0.43 ) and macro - average ( +1.94 ) compared to all other approaches and performs best on 7 out of the 8 domains . The second best method is the PoolDo - main+DomainFeat which uses the domain feature as input . Our method consistently surpasses the in - domain classifiers ( InDomain ) on microaverage ( +1.48 ) and macro - average ( +3.11 ) , showing the limitations of naive modeling approaches . Although increases exist across all domains , these are most prominent in domains like TC ( +5.36 ) that have a low density of named entities and where indomain models have access to limited amounts of data . However , the in - domain performance is better than the pooled method of training , which shows consistent drops in performance on some domains ( - 8.69 on WB , - 6.77 on BC , - 1.98 on CoNLL ) , where information from other domains did not benefit the model . Table 2 : Experimental results on the eight data sets , as well as micro ( \u00b5 - ) and macro ( M - ) averaged across data sets . Performance is measured using micro F1 score . The rows with indicate methods that can be applied when the domain label is not known at inference time . ( S ) and ( P ) denote if inference is done through the shared ( S ) or private ( P ) layers of the architecture . Results in bold are the best across all models , those underlined are best across methods that work with unknown domain labels .", "entities": [[256, 258, "MetricName", "micro F1"]]}
{"text": "We now focus on the experimental setup where domain labels are unknown for each data point at inference time . This is akin to a setup where the user is agnostic to the data the model was trained on . As only a subset of the models can perform inference in this scenario , the results are a subset of those in Table 2 . Our model - MultDomain - SP - Aux ( S ) - gains the best overall performance in this setup , with 1.95 macro - average F1 increase over the next best method ( InDomain+DomainClassifier ) . The other standard baseline for domain adaptation ( PoolDomain ) obtains a similar performance ( \u22122.19 compared to our method ) to the in - domain approach , which shows the benefits of multidomain adaptation . PoolDomain - Init is performing overall poorly , which shows that the INIT transfer learning strategy that is somewhat effective for source - target domain adaptation does not work well in the multidomain setup . Our intuition is that this technique is unable to learn robust features sequentially across N domains , as it performs poorly on the initial trained domains . PoolDomain - GradRev gains relatively weak performance overall , lower than the in - domain baseline .", "entities": [[90, 92, "MetricName", "average F1"], [107, 109, "TaskName", "domain adaptation"], [151, 153, "TaskName", "transfer learning"], [162, 164, "TaskName", "domain adaptation"]]}
{"text": "Finally , we show the results on the experimental setup where the test data is the four ' Zero - Shot Genres ' , which were not used in during training . labels , as we assume that in this setup , the end - user does not have knowledge about the domains used in training and which of these are most similar to the test point . Results show that our proposed method obtains again the best results , with a consistent margin of 2.24 macro - average F1 improvement over the next method . Pooling all data ( PoolDomain ) obtains better performance than building in - domain classifiers with domain classification ( InDomain+DomainClassifier ) unlike in the other setups . This also shows that the zero - shot domains we used are indeed different to any of the ones in training and pooling all data manages to build a slightly more robust model than individual ones trained on less data . The in - domain models perform 5.21 F1 points lower than our approach , the largest gap in all experimental setups , highlighting the robustness of the multi - domain modeling approach . The MultDomain - SP ( S ) model is second best , and as this is the base for our method , we discuss its performance in the ablation study from the next section .", "entities": [[88, 90, "MetricName", "average F1"], [171, 172, "MetricName", "F1"]]}
{"text": "In order to understand the limitations of the multidomain setup , we study whether the models we can build from the available data could theoretically achieve better overall performance . We use an oracle - based selection technique on the in - domain models to select , after the prediction and using the gold labels the model which performed best for each test instance , as selected using F1 score or , if there are no entities , the model with most O predictions . If multiple models are tied , we choose one at random . The oracle thus provides the counterfactually \" Optimal \" strategy of model selection for each test instance and represents an upper bound on strategies relying on InDomain models . Table 5 compares the oracle strategy predictions with the InDomain+DomainClassifier and the MultDomain - SP - Aux model . The results show that even though our model improves substantially over the in - domain models , an oracle selection method would push performance much higher ( +6.73 F1 on the open data ) . This highlights both the variability of NER models trained on different data sets and that there is potentially more room for improvements in the multi - domain setup .", "entities": [[68, 70, "MetricName", "F1 score"], [108, 110, "TaskName", "model selection"], [173, 174, "MetricName", "F1"], [186, 187, "TaskName", "NER"]]}
{"text": "The Supplementary Material shows a breakdown of the domain prediction labels for three methods : domain classification , domain prediction in the proposed MultDomain - SP - Aux model and the oracle in - domain choice on gold data . The oracle strategy selects the predictions from all in - domain models . Based on this , we analyzed the performance of each individual in - domain model when tested on all domains in Table 6 . We find that although the Oracle strategy uses a mix of models , any model alone is unable to generalize to other domains ( 67.19 vs. 84.68 best InDomain model compared to the best overall model ) . In the zero - shot genres , the Twitter model performs close to the MultDomain - SP - Aux model ( - 0.56 F1 ) , albeit it is 24 F1 lower on the multi - domain setup . This reinforces that learning shared domain features as opposed to learning individual models helps boost performance and is more robust to different types of inputs .", "entities": [[1, 3, "DatasetName", "Supplementary Material"], [138, 139, "MetricName", "F1"], [145, 146, "MetricName", "F1"]]}
{"text": "Finally , we compare the runtime difference across various methods listed in the experiment section to test the practical implications of using our pro - posed multi - domain modelling approach . In test phase , we set the batch size as 128 . Table 7 shows the average time of inference time used for each model . Our proposed model architecture takes 0.15 ms ( 33 % increase ) longer for inference than InDomain or PoolDomain models , which is a result of more model parameters . However , our proposed architecture is still 0.19 ms faster than using the InDomain+DomainClassifier approach . In addition to inference runtime , we also find that the training time is not significantly more than the combined training time of N in - domain models . The main additions are that of the shared layers and the auxiliary task to the components of the N in - domain models and is thus a constant addition in the number of parameters to the total of N indomain models . Hence , the model would scale by a constant with respect to the number of input domains ( N+1 number of components , where N is the number of domains ) . This should allow our pro - posed model to scale to a large number of domains . This highlights that the proposed MultDomain - SP - Aux model is a viable option for real - world applications .", "entities": [[39, 41, "HyperparameterName", "batch size"], [164, 167, "HyperparameterName", "number of parameters"]]}
{"text": "Robustness of NLP models is essential to their wider adoption and usability . Existing NER approaches are widely faced with limited scalability when applied to data that spans multiple domains . This paper introduced three experimental setups that provide a framework for evaluating the robustness of NER models . These include learning from data in multiple domains and testing on all domains , when the domain label of the test point is unknown and when this does not belong to a domain seen in training . Building on past research , we proposed a new neural architecture that achieves substantial improvements of up to 5 F1 points when compared to standard methods . Future work will focus on domain adaptation at the embedding layer . Finally , we plot the domain prediction distribution on the zero - shot genre data in Figure 3 . We find that similar to the confusion matrices , the oracle strategy has a more even spread in domain selection . We observe similar patterns to the confusion matrices for the InDomain+DomainClassifier and MultDomain - SP - Aux models .", "entities": [[14, 15, "TaskName", "NER"], [46, 47, "TaskName", "NER"], [105, 106, "MetricName", "F1"], [118, 120, "TaskName", "domain adaptation"]]}
{"text": "This paper presents a English - Korean parallel dataset that collects 381 K news articles where 1 , 400 of them , comprising 10 K sentences , are manually labeled for crosslingual named entity recognition ( NER ) . The annotation guidelines for the two languages are developed in parallel , that yield the inter - annotator agreement scores of 91 and 88 % for English and Korean respectively , indicating sublime quality annotation in our dataset . Three types of crosslingual learning approaches , direct model transfer , embedding projection , and annotation projection , are used to develop zero - shot Korean NER models . Our best model gives the F1 - score of 51 % that is very encouraging , considering the extremely distinct natures of these two languages . This is pioneering work that explores zero - shot crosslingual learning between English and Korean and provides rich parallel annotation for a core NLP task such as named entity recognition .", "entities": [[32, 35, "TaskName", "named entity recognition"], [36, 37, "TaskName", "NER"], [104, 105, "TaskName", "NER"], [112, 115, "MetricName", "F1 - score"], [160, 163, "TaskName", "named entity recognition"]]}
{"text": "We conduct a team of graduate students majoring in Data Science to manually tag named entities on all parallel sentences in the DEV and TST sets by taking the following 3 steps : 5 1 . For English , the pseudo - annotated entities are revised by the OntoNotes named entity guidelines ( BBN , 2014 ; Maekawa , 2018 ) , and missing entities are annotated as necessary . 2 . For Korean , the pseudo - annotated entities are revised to match the English tagset , and missing entities are annotated as necessary . 3 . Let E = { e 1 , . . . , e n } and K = { k 1 , . . . , k m } be the lists of entities from Steps 1 and 2 for a English and Korean sentence pair , respectively . Every entity pair ( e i , k j ) is linked in our dataset if e i is the translation of k j . Note that every article in DEV and TST consists of at least 5 sentences with at least 2 named entities . Table 3 shows the statistics of the gold annotation . Out of 22 , 367 and 21 , 892 named entities annotated in English and Korean sentences , 20 , 300 of them are linked across the languages ( above 90 % ) . To estimate the inter - annotator agreement , 10 news articles from each of the first 7 sections in Table 1 are randomly picked and double annotated ; the rest of DEV and TST are single annotated and sample checked . Table 4 shows the Cohen 's kappa scores measured for the English and Korean annotation . The high labeled matching scores of 90.9 and 88.3 are achieved for those two languages respectively , implying that the single annotation in this dataset is expected to be of high quality as well .", "entities": [[48, 49, "DatasetName", "OntoNotes"], [113, 115, "HyperparameterName", "K ="]]}
{"text": "Let S = { S 1 , . . . , S n } and T = { T 1 , . . . , T n } be lists of sentences in the source and target languages , and ( S i , T i ) be the i'th pair of parallel sentences in those two languages . Let S i = { s i1 , . . . , s in } and T i = { t i1 , . . . , t i m } where s i and t i are the i'th word in S and T . Then , annotation projection can be performed as proposed by Ni et al ( 2017 ) : 1 . Pseudo - label S \u2200i S using an existing model in the source language , in our case , ELIT ( 3.2 ) . 2 . Pseudo - align words in every ( S i , T i ) using an existing tool , in our case , GIZA++ ( 3.2 ) . If a consecutive word span S j , k i = { s ij , .. , s ik } is pseudo - labeled as the entity type as well as pseudo - aligned with a span T a , b i = { t ia , .. , t ib } , T a , b i is also pseudo - labeled with . The quality of pseudo annotation hugely depends on the performance of word alignment , which is generally not robust for the case of distant language pairs such as English and Korean . Thus , we propose a few constraints to filter out noisy annotation . Entity Matching Let \u03c8 be a boolean . If \u03c8 = F , all parallel sentences in ( S , T ) are used for training . If \u03c8 = T , ( S i , T i ) is selected for training only if all named entities in S i are properly labeled in T i by the above projection approach . Relative Frequency Let e be an entity term such as \" \ub3c4\ub110\ub4dc \ud2b8\ub7fc\ud504 ( Donald Trump ) \" in Figure 1 . Let L e be a set of entity types pseudo - annotated for all occurrences of e in the target language . Then , the relative frequency P ( | e ) for L e and e can be measured as follows , where COUNT ( , e ) is the number of occurrences for e being labeled as : P ( | e ) = COUNT ( , e ) Le COUNT ( , e ) Impurity Let F e be a set of unique terms in the source language that are pseudo - aligned with the term e labeled as in the target language such that | F e | \u2264 COUNT ( l , e ) . Then , the impurity M ( , e ) is measured as follows where \u03b1 is a smoothing factor : M ( , e ) = | F e | COUNT ( l , e ) + \u03b1 The relative frequency P ( | e ) and the impurity M ( , e ) are used to assess pseudo - annotation reliability .", "entities": [[253, 255, "TaskName", "word alignment"], [506, 507, "HyperparameterName", "\u03b1"], [529, 530, "HyperparameterName", "\u03b1"]]}
{"text": "E i = { ( 1 , e 1 ) , .. , ( q , e q ) } be a list of all ( entity term , label ) pairs in the target sentence T i . For each T i T , the following two scores , f ( T i ) and g ( T i ) , are measured to estimate the reliability of the pseudo - annotation in T i : f ( T i ) = \u2200 ( l , e ) E i P ( | e ) | E i | g ( T i ) = \u2200 ( l , e ) E i M ( | e ) | E i | Given the annotation reliability metrics , our data selection scheme heuristic is as follows : f ( T i ) \u2265 \u03c6 ; g ( T i ) \u2264 \u03b3 ; | E i | \u2265 \u00b5 ; \u03c8 = T | F Only the target sentences satisfying all of the above constraints are used for training given the hyperparameters \u03c8 , \u03b1 , \u03c6 , \u03b3 , and \u00b5.", "entities": [[152, 153, "HyperparameterName", "\u03b3"], [185, 186, "HyperparameterName", "\u03b1"], [189, 190, "HyperparameterName", "\u03b3"]]}
{"text": "The experimental settings of direct model transfer approach are identical with Wu and Dredze ( 2019 ) . We freeze the bottom n layers ( including n ) of mBERT , where layer 0 is the embedding layer . The cases of n are { - 1 , 0 , 3 , 6 , 9 } , where - 1 denotes fine - tuning all layers in mBERT . For word - level classification , a simple linear classification layer with softmax is added on mBERT . The hyperparameters we experiment on are the combitations of batch size { 16 , 32 } , learning rate { 2e - 5 , 3e - 5 , 5e - 5 } , and number of max epochs { 3 , 4 } .", "entities": [[29, 30, "MethodName", "mBERT"], [33, 34, "DatasetName", "0"], [48, 49, "DatasetName", "0"], [67, 68, "MethodName", "mBERT"], [81, 82, "MethodName", "softmax"], [85, 86, "MethodName", "mBERT"], [96, 98, "HyperparameterName", "batch size"], [104, 106, "HyperparameterName", "learning rate"]]}
{"text": "The annotation projection is performed to generate the pseudo - annotated Korean dataset ( Section 4.3 ) . The following 5 hyperparameters are tuned to filter out noisy annotation for training , where \u03c8 , \u03b1 , and \u03b3 are newly introduced by our work : \u03c8 : if True , keep only sentences whose entities are completely matching between the two languages . \u03b1 : the smoothing factor to measure the impurity . \u03c6 : retain sentences whose annotation reliability scores by relative frequency \u2265 this threshold . \u03b3 : retain sentences whose annotation reliability scores by impurity \u2264 this threshold . \u00b5 : retain sentences that contain named entities whose quantities are \u2265 this cutoff . Once the pseudo - annotation is created , all Korean sentences are encoded by mBERT to generate Korean embeddings that are fed into the NER model .", "entities": [[35, 36, "HyperparameterName", "\u03b1"], [38, 39, "HyperparameterName", "\u03b3"], [64, 65, "HyperparameterName", "\u03b1"], [89, 90, "HyperparameterName", "\u03b3"], [132, 133, "MethodName", "mBERT"], [142, 143, "TaskName", "NER"]]}
{"text": "Table 5 shows the best result of direct model transfer , mBERT fine - tuned on OntoNotes NER dataset and evaluated on our Korean TST set . All scores are reported in a form of mean ( \u00b1 standard deviation ) after three developments . The best model is built under the setting when all layers including the embedding layer of mBERT are fine - tuned . 6 shows the zero - shot results from the embedding projection models in Section 5.3 . Both mBERT and XLM - R models showed a performance improvement over 2 % with embedding transformation . F1 score improves 2.47 % ( 36.67 % to 39.14 % ) and 2.32 % ( 39.36 % to 41.68 % ) for mBERT and XLM - R , respectively . Both models showed the best performance with embedding transformation matrix made of 200k w . The number of parallel sentences used for training transformation matrix has a considerable impact on the Zero - shot learning . Table 7 shows the results from the annotation projection models with various configurations . About 9 % gain is shown by the best model using only the entity matching constraint \u03c8 that effectively filters out 55 % of the training data ( row 3 ) . A relative score of 50.25 % is achieved by the model using only 21 % of the training data , implying that a fair amount of noisy annotation is produced by the annotation projection approach . The overall results show that the Annotation Projection approach achieves the best performance , implying that considering the word order of the target language is critical in cross - lingual learning , especially in the case of distant language pairs . We expect further improvement of the annotation projection approach when adapting a more accurate word alignment tool or a data selection scheme , which we will further investigate . 6 Analysis", "entities": [[11, 12, "MethodName", "mBERT"], [16, 17, "DatasetName", "OntoNotes"], [17, 18, "TaskName", "NER"], [61, 62, "MethodName", "mBERT"], [84, 85, "MethodName", "mBERT"], [86, 87, "MethodName", "XLM"], [101, 103, "MetricName", "F1 score"], [124, 125, "MethodName", "mBERT"], [126, 127, "MethodName", "XLM"], [163, 167, "TaskName", "Zero - shot learning"], [305, 307, "TaskName", "word alignment"]]}
{"text": "The task specific NER model used : a 2 - layer Bi - LSTM with a hidden size of 768 followed by a CRF layer . A dropout rate of 0.5 is applied on the input and the output of the Bi - LSTM . Adam with default parameters and a learning rate of 0.0001 are used for optimization . We trained the model for 10 epoch with a batch size of 32 , and evaluate the model per a epoch . A.4 Comparison of NER performances ( Zero - shot VS Existing ) We compare our best performing Zero - shot Korean NER model with the existing Korean NER model 11 on TST .", "entities": [[3, 4, "TaskName", "NER"], [13, 14, "MethodName", "LSTM"], [23, 24, "MethodName", "CRF"], [43, 44, "MethodName", "LSTM"], [45, 46, "MethodName", "Adam"], [51, 53, "HyperparameterName", "learning rate"], [69, 71, "HyperparameterName", "batch size"], [85, 86, "TaskName", "NER"], [103, 104, "TaskName", "NER"], [109, 110, "TaskName", "NER"]]}
{"text": "Author name disambiguation ( AND ) refers to the problem of identifying each unique author entity record from all publication records in scholarly databases ( Ferreira et al , 2012 ) . It is also an important preprocessing step for a variety of problems . One example is processing author - related queries properly ( e.g. , identify all of a particular author 's publications ) in a digital library search engine . Another is to calculate author - related statistics such as an h - index , and collaboration relationships between authors . Typically , a clustering method is used to calculate AND . Such clustering calculates pairwise similarities between each possible pairs of records that then determines whether each pair should be in the same cluster . Since the number of possible pairs in a database with the number of records n is n ( n \u2212 1 ) /2 , it grows as O ( n 2 ) . Since n can be millions of authors in some databases such as PubMed , AND algorithms need methods that scale , such as a blocking function ( Christen , 2012 ) . The blocking function produces a reduced list of candidate pairs , and only the pairs on the list are considered for clustering . Blocking usually consists of blocking predicates . Each predicate is a logical binary function with a combination of an attribute and a similarity criterion . One example can be exact match of the last name . A simple but effective way of blocking involves manually selecting the predicates , with respect to the data characteristics . Much recent work on large - scale AND uses a heuristic that is the initial match of first name and exact match of last name ( Torvik and Smalheiser , 2009 ; Liu et al , 2014 ; Levin et al , 2012 ; Kim et al , 2016 ) . Although this gives reasonable completeness , it can be problematic when the database is extremely large , such as the author mentions in CiteSeerX ( 10 M publications , 32 M authors ) , PubMed ( 24 M publications , 88 M authors ) , and Web of Science ( 45 M publications , 163 M authors ) 1 . The blocking results on PubMed using this heuristic are shown in Table 1 . Note that most of the block sizes are less than 100 names , but a few blocks are extremely large . Since the number ( Kim et al , 2016 ) . To make matters worse , this problem increases in time , since the growth rates of publication records are rapidly increasing . To improve the blocking , there has been work on learning the blocking ( Bilenko et al , 2006 ; Michelson and Knoblock , 2006 ; Cao et al , 2011 ; Kejriwal and Miranker , 2013 ; Das Sarma et al , 2012 ; Fisher et al , 2015 ) . These can be categorized into two different methods . One is a disjoint blocking , where each block is separated so each record belongs to a single block . Another is non - disjoint blocking , where some blocks have shared records . Each has advantages . Disjoint blocking can make the clustering step easily parallelized , while non - disjoint blocking often produces smaller blocks . and also has more degrees of freedom from which to select the similarity criterion . Here , we propose to learn a non - disjoint blocking with a conjunctive normal form ( CNF ) . Our main contributions are : Propose a CNF blocking , which reduces more pairs compared to DNF blocking , in order to achieve a large number of pairs completeness . This also reduces the processing time , which benefits various applications such as online disambiguation , author search , etc . Extend the method to produce disjoint blocks , so that the AND clustering step can be easily parallelized . Compare different gain functions , which are used to find the best blocking predicates for each step of learning . Previous work is discussed in the next session . This is followed by problem definition . Next , we describe learning of CNF blocking and how to use it to ensure the production of disjoint blocks . Next , we evaluate our methods on the PubMed dataset . Finally , the last section consists of a summary work with possible future directions .", "entities": [[246, 248, "MetricName", "exact match"], [293, 295, "MetricName", "exact match"]]}
{"text": "Our work tackles the same problem with baseline DNF blocking ( Bilenko et al , 2006 ; Michelson and Knoblock , 2006 ) , but in a different way to get the optimized blocking function . Let R = { r 1 , r 2 , , r n } be the set of records in the database , where n is the number of records . Each record r has k attributes , and A be the attribute set A = { a 1 .a 2 , , a k } . A blocking predicate p is a combination of an attribute a and a similarity function s defined to a. An example of s is exact string match of a. A blocking predicate can be seen as a logical binary function applied to each pair of records , so p ( r x , r y ) = { 0 , 1 } , where r x , r y R. A blocking function f is a boolean logic formula consisting with blocking predicates p 1 , p 2 , , p n , and each predicate is connected with either conjunction or disjunction \u2228. An example is f example = ( p 1 p 2 ) \u2228 p 3 . Since it is made up of blocking predicates , f ( r x , r y ) = { 0 , 1 } for all r x , r y R. The goal is to find an optimal blocking function f * that covers a minimum number of record pairs while missing up to a fraction \u03b5 of total number of matching record pairs . To formalize it , f * = argmin f ( rx , ry ) R f ( r x , r y ) such that \u2265 ( 1 \u2212 \u03b5 ) \u00d7 | R + | ( 1 ) where R + is set of matching record pairs .", "entities": [[151, 152, "DatasetName", "0"], [232, 233, "DatasetName", "0"], [269, 270, "HyperparameterName", "\u03b5"], [307, 308, "HyperparameterName", "\u03b5"]]}
{"text": "Let N egCov be all l in N eg that satisfies T return CN F 40 : end function sample pairs , P are blocking predicates , and k is maximum number of predicates in each term . The algorithm first generates a set of negated candidate conjunction term T erms from all p in N egP ( line 19 - 22 ) . A dual of the original gain function CALCNEGGAIN selects a predicate for generating a negated candidate conjunction . Then , as in DNF blocking , the sequential covering algorithm is used to learn the negated DNF formula ( line 26 - 37 ) , which iteratively adds a negated conjunction term until it covers the desired number of samples . We select a negated conjunction term with a gain function , CALCNEGGAIN . Also , note that the termination condition of the loop ( line 26 ) is when \u03b5 of total positive samples are covered with the learned N egDN F . This ensures that we miss less than \u03b5 of the total number of positive samples in the final CNF formula . After getting the final N egDN F , it is negated to get the desired CNF .", "entities": [[120, 123, "HyperparameterName", "number of samples"], [153, 154, "HyperparameterName", "\u03b5"], [174, 175, "HyperparameterName", "\u03b5"]]}
{"text": "Disjoint blocking functions generate blocks for each record that resides in a single block ; thus such blocks are mutually exclusive . It has the advantage that parallelization can be performed efficiently after applying the blocking by running processes for each blocks separately . A blocking function is disjoint if and only if it satisfies the following conditions : 1 ) it only consists of pure conjunction ( logical AND ) , 2 ) all predicates use non - relative similarity measures . That is , measures that compare the absolute value of blocking key , e.g. exact match of first n characters . DNF and CNF blocking are both non - disjoint blocking due to the condition 1 above . We introduce a simple extension to ensure our CNF blocking can produce disjoint blocks . This is done by first producing two blocking functions . The first function learns a blocking function with only conjunctions based on our CNF blocking method using k = 1 and a limited set of predicates with nonrelative similarity measures . Then , CNF blocking is learned with our k - CNF method with the whole set of predicates for pairs remaining after applying 1 - CNF ( conjunction of single attributes ) . We first apply the 1 - CNF to the whole database to produce disjoint blocks . Then for each block , we apply the second k - CNF blocking function to filter out pairs not satisfies the k - CNF function . This is similar to applying a filter as in Gu and Baxter Khabsa et al ( 2015 ) . While they use a heuristic , our method automatically learns the optimal one . Note that this method still produces a CNF since it combines conjunction terms and k - CNF with logical AND .", "entities": [[97, 99, "MetricName", "exact match"], [163, 165, "HyperparameterName", "k ="]]}
{"text": "We evaluate our CNF blocking with reduction ratio ( RR ) , pairs completeness ( PC ) , and F - measure . These metrics are often used to evaluate blocking methods . Those metrics can be calculated as follows : RR = 1 \u2212 p + n P + N , ( 5 ) P C = p P , ( 6 ) F = 2 \u00d7 RR \u00d7 P C RR + P C . ( 7 ) where P , N are the numbers of positive and negative samples , and p , n are the numbers of positive and negative samples covered with the blocking function . RR measures the efficiency of the blocking function , PC measures the quality of the blocking function . F is the harmonic mean of RR and PC .", "entities": [[9, 10, "DatasetName", "RR"], [19, 22, "MetricName", "F - measure"], [41, 42, "DatasetName", "RR"], [68, 69, "DatasetName", "RR"], [72, 73, "DatasetName", "RR"], [111, 112, "DatasetName", "RR"], [135, 136, "DatasetName", "RR"]]}
{"text": "We first define the similarity criterion used for the experiments . We observed an important characteristic of the data : some attributes are empty ( e.g. year : 7.8 % , affiliation : 81.1 % ) or have only partial information ( 54.5 % has only initials for the first name ) . To deal with this , we add compatible to those blocking keys . Below is brief explanation of each similarity criterion . exact : Exact match . f irst ( n ) , last ( n ) : First / Last n character match , where n is an integer . We check { 1 , 3 , 5 , 7 } for name attributes . order : Assigns T rue if both records are first authors , last authors , or non - first and non - last authors . digit ( n ) : First n digit match . We check { 1 , 2 , 3 } for year . compatible : T rue if at least one of the records are empty ( Eq . 8 ) . If the key is name , it also checks if the initial matches if one of the records has only initial . compatible ( A , B ) = T rue if at least one is empty exact ( A , B ) otherwise ( 8 ) cos : Cosine distance of TF - IDF bag - ofwords vector . We check with threshold { 0.2 , 0.4 , 0.6 , 0.8 } . dif f : Year difference . We use the threshold { 2 , 5 , 10 } . Using those similarity measures , We define two different sets of blocking predicates . blocking predicates used for non - disjoint blocking . Disjoint blocking requires the use of predicates with non - relative similarity measures to ensure blocks are mutually exclusive . For disjoint blocking , we use the set of blocking predicates excluding the ones with the relative similarity measures ( exact , compatible , dif f ) in Table 3 .", "entities": [[77, 79, "MetricName", "Exact match"]]}
{"text": "The parameter \u03b5 is used to vary the PC . We tested values in [ 0 , 1 ] to get the PC - RR curve . k is selected experimentally to calculate the maximum reachable F - measure . We use k = 3 for further experiments .", "entities": [[2, 3, "HyperparameterName", "\u03b5"], [15, 16, "DatasetName", "0"], [24, 25, "DatasetName", "RR"], [36, 39, "MetricName", "F - measure"], [42, 44, "HyperparameterName", "k ="]]}
{"text": "The variational autoencoder ( Kingma and Welling , 2014 ) is an efficient way to handle ( continuous ) latent variables in neural models . We describe it briefly here , and interested readers can refer to Doersch ( 2016 ) for details . The VAE learns a generative model of the probability p ( x ) of observed data x. The generative process consists of first generating a continuous latent variable z conditioned on the observed data x , which is termed as the recognition model q ( z | x ) ( encoder ) and then use this latent variable to reconstruct the observation x known as the reconstruction ( decoder ) model p ( x | z ) . VAE uses the variational inference to approximate the intractable posterior by learning a parametric posterior distribution for all observations . Th learning objective function is the variational lower bound on the marginal log likelihood of data : log p \u03b8 ( x ) \u2265 E z\u223cq \u03c6 ( z | x ) [ log p \u03b8 ( x | z ) ] \u2212 KL ( q \u03c6 ( z | x ) | | p ( z ) ) ( 1 ) To optimize the parameters with gradient descent , Kingma and Welling ( 2014 ) introduce a reparameterization trick that allows for training using simple backpropagation w.r.t . the Gaussian latent variables z.", "entities": [[1, 3, "MethodName", "variational autoencoder"], [45, 46, "MethodName", "VAE"], [122, 123, "MethodName", "VAE"], [125, 127, "MethodName", "variational inference"], [161, 162, "HyperparameterName", "\u03b8"], [177, 178, "HyperparameterName", "\u03b8"]]}
{"text": "There are two cases to discuss when employing the variational encoder - decoder framework for labeled sequence transduction . First , when the labels of the inflected words are known as is the format of the training data in the shared task , we do n't need to bother introduction the discrete latent variables for the inflected labels . We maximize the variational lower bound on the conditional log likelihood of observing x ( t ) and y ( t ) as follows : log p \u03b8 ( x ( t ) , y ( t ) | x ( s ) ) \u2265 E z\u223cq \u03c6 ( z | x ( s ) ) log p \u03b8 ( x ( t ) , y ( t ) , z | x ( s ) ) q \u03c6 ( z | x ( s ) ) = E z\u223cq \u03c6 ( z | x ( s ) ) [ log p \u03b8 ( x ( t ) | y ( t ) , z ) + log p \u03c0 ( y ( t ) ) ] \u2212 KL ( q \u03c6 ( z | x ( s ) ) | | p ( z ) ) = L l ( x ( t ) , y ( t ) | x ( s ) ) ( 2 ) which is a simple extension to the vanilla variational auto - enocders . Second , in the case of unsupervised learning or when the labels of the inflected word is not observed , we only observe a word or a pair of words and we would like to maximize the log likelihood of the observed data by marginalizing over possible morphological labels , which is consisted to the supervised case above . In this scenario , we can introduce the discrete latent variables for the inflected labels which are used to infer the labels for the target word . Then when decoding the word , we condition both on the continuous and discrete latent variables . For the variational encoder - decoder ( MSVED ) , the variational lower bound on the conditional log likelihood is affected by the recognition model , and thus is computed as : log p \u03b8 ( x ( t ) | x ( s ) ) \u2265E ( y ( t ) , z ) \u223cq \u03c6 ( y ( t ) , z | x ( s ) , x ( t ) ) log p \u03b8 ( x ( t ) , y ( t ) , z | x ( s ) ) q \u03c6 ( y ( t ) , z | x ( s ) , x ( t ) ) = E y ( t ) \u223cq \u03c6 ( y ( t ) | x ( t ) ) [ E z\u223cq \u03c6 ( z | x ( s ) ) [ log p \u03b8 ( x ( t ) | y ( t ) , z ) ] \u2212 KL ( q \u03c6 ( z | x ( s ) ) | | p ( z ) ) + log p \u03c0 ( y ( t ) ) \u2212 log q \u03c6 ( y ( t ) | x ( t ) ) ] = L u ( x ( t ) | x ( s ) ) ( 3 ) While the unsupervised objective is trained by maximizing the following variational lower bound U ( x ) on the objective for unlabeled data : log p \u03b8 ( x ) \u2265 E ( y , z ) \u223cq \u03c6 ( y , z | x ) log p \u03b8 ( x , y , z ) q \u03c6 ( y , z | x ) = E y\u223cq \u03c6 ( y | x ) [ E z\u223cq \u03c6 ( z | x ) [ log p \u03b8 ( x | z , y ) ] \u2212 KL ( q \u03c6 ( z | x ) | | p ( z ) ) + log p \u03c0 ( y ) \u2212 log q \u03c6 ( y | x ) ] = U ( x ) ( 4 ) Note that when labels are not observed , the inference model q \u03c6 ( y | x ) has the form of a discriminative classifier , thus we can use observed labels as the supervision signal to learn a better classifier . In this case we also minimize the following cross entropy as the classification loss : D ( x , y ) = E ( x , y ) \u223cp l ( x , y ) [ \u2212 log q \u03c6 ( y | x ) ] ( 5 ) where p l ( x , y ) is the distribution of labeled data . To sum up , the semi - supervised model ( Semisup ) is trained to maximize the variational lower bounds and minimize the classification crossentropy error of 5 . L ( x ( s ) , x ( t ) , y ( t ) , x ) = \u03b1 U ( x ) + L u ( x ( s ) | x ( t ) ) + L l ( x ( t ) , y ( t ) | x ( s ) ) \u2212 D ( x ( t ) , y ( t ) ) ( 6 ) The weight \u03b1 controls the relative weight between the loss from unlabeled data and labeled data . 3 Learning MSVED", "entities": [[86, 87, "HyperparameterName", "\u03b8"], [117, 118, "HyperparameterName", "\u03b8"], [161, 162, "HyperparameterName", "\u03b8"], [378, 379, "HyperparameterName", "\u03b8"], [421, 422, "HyperparameterName", "\u03b8"], [494, 495, "HyperparameterName", "\u03b8"], [598, 599, "HyperparameterName", "\u03b8"], [620, 621, "HyperparameterName", "\u03b8"], [658, 659, "HyperparameterName", "\u03b8"], [764, 765, "MetricName", "loss"], [864, 865, "HyperparameterName", "\u03b1"], [920, 921, "HyperparameterName", "\u03b1"], [927, 928, "MetricName", "loss"]]}
{"text": "We observe that with the vanilla implementation the KL cost quickly decreases to near zero , setting q \u03c6 ( z | x ) equal to standard normal distribution . In this case , the RNN decoder can easily degenerate into an RNN language model . Hence , the latent variables are ignored by the decoder and can not encode any useful information . The latent variable z learns an undesirable distribution that coincides with the imposed prior distribution but has no contribution to the decoder . To force the decoder to use the latent variables , we take the following two approaches which are similar to Bowman et al ( 2016 ) . KL - Divergence Annealing : We add a coefficient \u03bb to the KL cost and gradually anneal it from zero to a predefined threshold \u03bb m . At the early stage of training , we set \u03bb to be zero and let the model first figure out how to project the representation of the source sequence to a roughly right point in the space and then regularize it with the KL cost . This technique can also be seen in ( Ko\u010disk\u1ef3 et al , 2016 ; Miao and Blunsom , 2016 ) . Input Dropout in the Decoder : Besides annealing the KL cost , we also randomly drop out the input token with a probability of \u03b2 at each time step of the decoder during learning . The previous ground - truth token embedding is replaced with a zero vector when dropped . In this way , the RNN decoder could not fully rely on the ground - truth previous token , which ensures that the decoder uses information encoded in the latent variables . k a l b \u2303 ( x ) \u00b5 ( x ) \u270f \u21e0 N ( 0 , 1 ) z < w > k k \u00e4 + y T 1 y T 2 y T 3 y T 4 .... ...... k a l b \u2303 ( x ) \u00b5 ( x ) \u270f \u21e0 N ( 0 , 1 ) z < w > k k a Multinomial Sampling + ...... y12", "entities": [[209, 210, "MethodName", "Dropout"], [232, 233, "HyperparameterName", "\u03b2"], [307, 308, "DatasetName", "0"], [349, 350, "DatasetName", "0"]]}
{"text": "The overall model architecture is shown in Fig . 1 . Each character and each label is associated with a continuous vector . We employ Gated Recurrent Units ( GRUs ) for the encoder and decoder . We use only single directional GRUs as the encoder for the input word x ( s ) . u is the hidden representation of x ( s ) which is the last hidden state of GRUs . and is used as the input for the inference model on z. We represent \u00b5 ( u ) and \u03c3 2 ( u ) as MLPs and sample z from N ( \u00b5 ( u ) , diag ( \u03c3 2 ( u ) ) ) , using z = \u00b5 + \u03c3 , where \u223c N ( 0 , I ) . Similarly , we can obtain the hidden representation of ( t ) and use this as input to the inference model on each label y x ( t ) i , which is also an MLP following a softmax layer to generate the categorical probabilities of target labels . Other experimental setups : We apply temperature annealing in the Gumble - Softmax with the scheme max ( 0.5 , exp ( \u22123e \u2212 5 t ) ) every 2000 updates where t is the update steps . We observe that our model is not sensitive to the temperature in this task . All hyperparameters are tuned on the validation set , and include the following : For KL cost annealing , \u03bb m is set to be 0.2 for all language settings . For character drop - out at the decoder , we empirically set \u03b2 to be 0.4 for all languages . We set the dimension of character embeddings to be 300 , tag label embeddings to be 200 , RNN hidden state to be 256 , and latent variable z to be 150 or 100 . We set \u03b1 the weight for the unsupervised loss to be 0.8 . We train the model with Adadelta ( Zeiler , 2012 ) and use early - stop with a patience of 5 . Our system is an ensemble of five models and the probability vector at each time step is obtained by averaging the output probabilities from each model 5 Experiments", "entities": [[132, 133, "DatasetName", "0"], [172, 173, "DatasetName", "MLP"], [175, 176, "MethodName", "softmax"], [198, 199, "MethodName", "Softmax"], [282, 283, "HyperparameterName", "\u03b2"], [327, 328, "HyperparameterName", "\u03b1"], [333, 334, "MetricName", "loss"], [343, 344, "MethodName", "Adadelta"]]}
{"text": "The results on the dev and test data of the 52 languages are presented in 1 . We obtain a generation accuracy above 80 % over more than 25 % languages and an average of 87.2 % for both dev and test data . The generation accuracy is almost consistent on the dev and test data except that the test data accuracy of Scottish - Gaelic drops by near 21 % . We find that only a medium volume of training data is provided for Scottish - Gaelic . This may be the reason why the model trained for Scottish - Gaelic can not generalize as well as other languages . We do not tune the hyper - parameters for each language manually . However , we test on different dimensions for the continuous latent variables . The dimension size we have used included 100 and 150 . And we observe significant improvement by using a larger dimension size of latent variables over a portion of languages including Faroese , Lithuanian , Navajo , Scottish - gaelic , Northern - sami , Slovene , Sorani , Slovak . However , we also observe that for some languages including Finnish , German , French , etc , the performance drops signficantly after increasing the size of continuous latent variable dimension . This indicates that for different languages , the continuous space required to encode the lemma and inflected information varies from language to language . We will further investigate this in the future work .", "entities": [[21, 22, "MetricName", "accuracy"], [46, 47, "MetricName", "accuracy"], [61, 62, "MetricName", "accuracy"], [234, 235, "DatasetName", "lemma"]]}
{"text": "Wiki Data While our performance was reasonable , it was not as good as that presented in our previous work ( Zhou and Neubig , 2017 ) , nor was it competitive with the highest - scoring models on the shared task . In order to examine the reason for this , we performed several ablations , the results of which are presented in Tab . 3 First , we first examined the effects of data augmentation and Wiki Data for semi - supervised learning on the performance of our model . By removing the augmented data from the training set , we observe a large gain in the generation accuracy . Besides , we find that Wiki Data for semisupervised learning does n't help much to increase the model 's performance . The reasons for this will be examined further in the following section . We additionally reimplemented a vanilla encoder - decoder model with attention that concatenates the input characters and target word tags together with a special token in the middle as the new input sequence to the encoder ( Kann and Sch\u00fctze , 2016 ) . The results show that the vanilla encoder - decoder works better than our model in some cases . We suspect that since task 1 is purely an inflection task and because semi - supervised learning did not provide a particularly large benefit , a simpler model that utilizes attention may be sufficient . This is in contrast to our previous findings , where semi - supervised learning was highly effective , and the proposed model out - performed the simpler attention - based baseline .", "entities": [[75, 77, "TaskName", "data augmentation"], [110, 111, "MetricName", "accuracy"]]}
{"text": "Autoregressive generation ( AG ) models achieve state - of - the - art performance on a wide range of text generation tasks , such as machine translation ( Vaswani et al , 2017 ) and text summarization ( Rush et al , 2015 ) . Such models generate a token sequence in a left - to - right , token - by - token fashion . The prediction for the next token is conditioned on all previously generated tokens . This characteristic makes it impossible to parallelize the computational overhead for token predictions in different positions , which leads to a relatively high latency in inference . On the other hand , non - autoregressive generation ( NAG ) models ( Gu et al , 2018 ) have emerged as a promising alternative due to their fast inference speed . NAG models omit the sequential dependencies within the output - side sequence and predict tokens in all positions simultaneously once the output length has been determined beforehand . While NAG models enjoy full parallelism and faster inference , the generation quality of NAG models often lags behind their autoregressive counterparts . In this work , we explore the potential of largescale pre - trained language models for improving the performance of non - autoregressive generation . Specifically , we utilize BERT ( Devlin et al , 2019 ) as the backbone for NAG modelling and extend the architecture of BERT with a CRF output layer ( Lafferty et al , 2001 ; for better capturing the output - side dependencies . In addition , we analyze two significant limitations that NAG models currently suffer from : ( 1 ) the inflexibility of prefixed output length , and ( 2 ) the conditional independence of individual token predictions . Accordingly , we devise two solutions to these two problems . First , prior NAG models require the output length to be determined before token generation , thus an extra module for output length prediction is always required . Nevertheless , the most likely length from the prediction module is not necessarily the best - suited one for the token generation model . To this end , previous works ( Gu et al , 2018 ; Ma et al , 2019 ) usually rely on length - parallel decoding ( LPD ) for performance enhancement ; that is , generating and re - ranking the results from different output length candidates . In this work , we propose a simple and elegant decoding mechanism that lets the model determine the output length on - the - fly . Specifically , our model dynamically adjusts the output sequence length via emitting an [ eos ] token at any output position to indicate the ending of the generated sequence . Therefore , we can avoid the additional efforts of output length prediction and results re - ranking . Second , most existing NAG models assume the token predictions in different positions are conditionally independent . As a consequence , they often tend to generate results that are ungrammatical with repetitions ( Wang et al , 2019b ) . To alleviate this problem , we propose a context - aware learning objective which impels the model to output different tokens at adjacent positions , thereby reducing the possibility of repetitive generation . Furthermore , for tasks like text summarization , the output sequence ( summary ) is known to be shorter than the source sequence ( article ) . In such cases , to further improve the model 's inference efficiency , we introduce a new ratio - first decoding strategy . Specifically , instead of performing inference on all source - side hidden states , ratio - first generates the result only based on a subset of source hidden states . The subset size is jointly determined by the source length T and a predefined ratio \u03b1 that is set based on our prior knowledge from the data statistics . In the experiments , we show that ratio - first can significantly improve the inference speed while maintaining the generation quality . We evaluate the proposed model on three typical text generation tasks , including text summarization , sentence compression and machine translation . Experimental results show that our model significantly outperforms many strong non - autoregressive baselines , and even performs competitively with several strong autoregressive models . In addition , we conduct extensive analysis experiments to study the effect of individual proposed components . In summary , our contributions are : ( 1 ) We propose a novel framework that utilizes BERT for text generation under the non - autoregressive generation paradigm ; ( 2 ) We propose a decoding mechanism that allows the model to dynamically determine the output length , and a new context - aware learning objective that reduces errors stemming from the output - side conditional independence assumption ; ( 3 ) We introduce a ratio - first decoding strategy that further improve the model 's inference efficiency .", "entities": [[20, 22, "TaskName", "text generation"], [26, 28, "TaskName", "machine translation"], [36, 38, "TaskName", "text summarization"], [221, 222, "MethodName", "BERT"], [240, 241, "MethodName", "BERT"], [243, 244, "MethodName", "CRF"], [563, 565, "TaskName", "text summarization"], [653, 654, "HyperparameterName", "\u03b1"], [697, 699, "TaskName", "text generation"], [702, 704, "TaskName", "text summarization"], [705, 707, "DatasetName", "sentence compression"], [708, 710, "TaskName", "machine translation"], [770, 771, "MethodName", "BERT"], [772, 774, "TaskName", "text generation"]]}
{"text": "We note that the outputs of BERT can be divided into two subsets . The first subset ranges from the beginning to the position where the first [ eos ] is emitted , and the second subset is the rest . For example , in Figure 2 , the first subset are those corresponding to the output sequence \" y ( 1 ) y ( 2 ) y ( 3 ) y ( 4 ) [ eos ] \" . As for the second part , we can see that it has little effect on the final output and removing it should not change the result . This indicates that it suffices to only consider the beginning part of BERT outputs for improving the inference speed . Especially , for tasks like summarization where the target is known to be shorter than the source sequence , we are safe to only use the first [ \u03b1 T ] outputs of BERT to perform inference . Here T denotes the source length , \u03b1 ( 0.0 , 1.0 ) is set based on the data statistics and [ ] is the integer rounding operation . Formally , given the source sequence X , the ratio - first decoding is defined as Y = arg max Y F ( X , Y , \u03b1 ) , = arg max Y { [ \u03b1 T ] i=1 \u03a6 y i ( h i ) + [ \u03b1 T ] i=2 t ( y i\u22121 , y i ) } . ( 10 ) When \u03b1 = 1.0 , ratio - first degenerates to the standard decoding strategy in CRF - based models . It should be noted that , [ \u03b1 T ] only constrains the maximum length of the generated result , and the actual output length ( after removing the generated [ eos ] tokens ) is still decided by the model itself . In the experiment section , we demonstrate that ratio - first can notably improve the inference speed whilst maintaining the generation quality .", "entities": [[6, 7, "MethodName", "BERT"], [119, 120, "MethodName", "BERT"], [132, 133, "TaskName", "summarization"], [155, 156, "HyperparameterName", "\u03b1"], [160, 161, "MethodName", "BERT"], [172, 173, "HyperparameterName", "\u03b1"], [221, 222, "HyperparameterName", "\u03b1"], [230, 231, "HyperparameterName", "\u03b1"], [243, 244, "HyperparameterName", "\u03b1"], [261, 262, "HyperparameterName", "\u03b1"], [275, 276, "MethodName", "CRF"], [287, 288, "HyperparameterName", "\u03b1"]]}
{"text": "Due to the conditional independence approximation on output tokens , NAG models often tend to generate repeated tokens ( Wang et al , 2019b ) . One way to alleviate this problem is to introduce implicit dependencies on the output side . In this work , we propose to use the unlikelihood formulation of Welleck et al ( 2020 ) in the context of NAG , where we define the set of negative candidate as the surrounding tokens within a predefined context window c. Formally , given the source sequence X and the target sequence Y with length T , the proposed context - aware objective is defined as : L CA ( Y | X ) = \u2212 T i=1 { log p \u03b8 ( y i | h i ; X ) + l CA ( i ) } , l CA ( i ) = j = i+c j = i\u2212c , y j = y i log ( 1.0 \u2212 p \u03b8 ( y j | h i ; X ) ) , ( 11 ) where h i is the model output state at position i. At position i , the proposed objective maximizes the probability of token y i while minimizing the probabilities of the surrounding tokens . In this way , it discourages the model from generating repetitive tokens at different time steps . The overall learning objective is then defined as L CRF = \u2212 log P CRF ( Y | X ) , L = L CRF + \u03bb L CA , ( 12 ) where \u03bb controls the importance of different loss terms and P CRF ( Y | X ) is described in Equation ( 8 ) .", "entities": [[124, 125, "HyperparameterName", "\u03b8"], [165, 166, "HyperparameterName", "\u03b8"], [240, 241, "MethodName", "CRF"], [245, 246, "MethodName", "CRF"], [255, 256, "MethodName", "CRF"], [271, 272, "MetricName", "loss"], [275, 276, "MethodName", "CRF"]]}
{"text": "We implement the proposed model with PyTorch ( Paszke et al , 2017 ) . The BERT model we use is the Huggingface implementation ( Wolf et al , 2019 ) ( bert - base - uncased ) . To approximate the transition matrix in the CRF layer , we set the dimension d of matrices E 1 and E 2 as 32 . For the normalizing factor Z ( X ) , we set the predefined beam size k as 256 . As for the overall learning objective , we set the window size c as 3 and \u03bb as 1.0 . In training , we use Adam optimizer ( Kingma and Ba , 2015 ) . To measure the relative speedup , we follow the standard setup which runs inference for each individual example separately . The model 's inference speed is computed by averaging the results of test cases . For a fair comparison , we measure the inference speed of all models on the same platform .", "entities": [[16, 17, "MethodName", "BERT"], [46, 47, "MethodName", "CRF"], [108, 109, "MethodName", "Adam"], [109, 110, "HyperparameterName", "optimizer"]]}
{"text": "Text summarization aims to automatically generate a compact summary that retains the most important content of the original text document ( Nenkova and McKeown , 2012 ) . In this experiment , we use the Gigawords dataset ( Rush et al , 2015 ) as our benchmark . For evaluation , standard metrics including ROUGE - 1 ( R - 1 ) , ROUGE - 2 ( R - 2 ) and ROUGE - L ( R - L ) ( Lin , 2004 ) are reported . We compare our model with several representative and the latest NAG models , including NAG - NMT ( Gu et al , 2018 ) , NAR - REG ( Wang et al , 2019b ) and NAG - CRF . Following previous works , during training , we train a length predictor to predict the output length . During inference , for each NAG baseline , we adopt the length - parallel decoding strategy ( LPD - k ) , that is , generating k results using the top - k possible output length predictions from the length predictor . The results are then re - ranked by a transformer model to get the final ouput . In the experiment , we report the results of different NAG baselines using LPD - 9 decoding . In addition , to better examine the effect of using BERT in NAG models , we add a BNAG - CRF baseline which adopts the same structure of the NAG - CRF model but using BERT as the encoder . We also compare our model with several strong autoregressive models , which are Luong - NMT ( Luong et al , 2015 ) , Pointer - Generator ( See et al , 2017 ) , DRGD ( Li et al , 2017 ) and Concept Pointer ( Wang et al , 2019a ) . To measure the relative inference speedup , we include transformer as a baseline model . The results are shown in Table 1 , from which we can see that , by using length - parallel decoding , the performance of all NAG baselines can be notably improved . However , such procedure significantly increases the inference latency . In contrast , our model can self - determine the output length without any re - ranking process . As shown in the results , our model outperforms the best NAG baseline ( with LPD ) and achieves performances that are comparable with several strong AG models . Comparing the results of BNAG - CRF and NAG - CRF , we can see that incorporating BERT as encoder helps to improve the model performance . Nonetheless , our model still outperforms BNAG - CRF with LPD - 9 decoding . This is because the dynamic length decoding mechanism allows our model to generate results with optimal length , leading to stronger model performances . Finally , we analyze the proposed ratio - first decoding . From the results , we observe a moderate performance drop when using ratio - first ( \u03b1 = 0.3 ) . It comes from the fact that , for some input documents with length T , the reference summary is longer than [ \u03b1 T ] . In such cases , ratio - first fails to generate the complete reference summary , leading to the drop of performance . On the other hand , we can see that , ratio - first can notably improve the inference speedup . With \u03b1 = 0.3 , our model achieves the highest inference speedup while still outperforms all compared NAG models .", "entities": [[0, 2, "TaskName", "Text summarization"], [72, 75, "MetricName", "ROUGE - L"], [113, 114, "DatasetName", "NAR"], [126, 127, "MethodName", "CRF"], [233, 234, "MethodName", "BERT"], [243, 244, "MethodName", "CRF"], [254, 255, "MethodName", "CRF"], [258, 259, "MethodName", "BERT"], [429, 430, "MethodName", "CRF"], [433, 434, "MethodName", "CRF"], [440, 441, "MethodName", "BERT"], [458, 459, "MethodName", "CRF"], [516, 517, "HyperparameterName", "\u03b1"], [543, 544, "HyperparameterName", "\u03b1"], [590, 591, "HyperparameterName", "\u03b1"]]}
{"text": "Sentence compression aims at compressing a long sentence into a short one by deleting redundant words . In this experiment , we use the Google sentence compression dataset ( Filippova and Altun , 2013 ) as our benchmark . For evaluation , we use the standard token - kept - F1 ( F1 ) score . In addition , We also report the results of other standard metrics including ROUGE - 1 , ROUGE - 2 and ROUGE - L. Models F1 R - 1 R - 2 R - L We compare the proposed model with the same NAG baselines as in the previous experiment . We also compare our model with several strong autoregressive models , including Bi - LSTM - Dep ( Filippova et al , 2015 ) , Tagger and Tagger+ILP , HiSAN - Dep and HiSAN ( Kamigaito et al , 2018 ) . To measure the inference speedup , we include transformer as a baseline model . The results are presented in Table 2 , from which we see that our model outperforms the best reported NAG baseline ( with LPD ) in terms of both the generation quality and inference speed . Comparing with the strong autoregressive models , our model can achieve competitive performance with a over 8.42\u00d7 inference speed up . We also report the results of our model using the ratio - first decoding strategy . By setting \u03b1 as 0.7 , it achieves a 10.00\u00d7 inference speedup while still outperforming other compared NAG baselines .", "entities": [[0, 2, "DatasetName", "Sentence compression"], [24, 25, "DatasetName", "Google"], [25, 27, "DatasetName", "sentence compression"], [50, 51, "MetricName", "F1"], [52, 53, "MetricName", "F1"], [81, 82, "MetricName", "F1"], [121, 122, "MethodName", "LSTM"], [238, 239, "HyperparameterName", "\u03b1"]]}
{"text": "Machine translation aims at translating text from the source language to the target language . In this task , we use the IWSLT14 German - to - English ( DE - EN ) dataset as our benchmark . Following previous works , we use the sequence - level knowledge distillation ( Gu et al , 2018 ) during training . For evaluation , we report results in BLEU scores ( Papineni et al , 2002 ) . In this experiment , we use the BERT model in German language . We compare our model with a range of strong NAG models , including NAG - NMT ( Gu et al , 2018 ) , ENAG - E and ENAG - P ( Guo et al , 2019 ) , NAG - REG ( Wang et al , 2019b ) , NAG - CRF and BNAG - CRF . For each NAG baseline , we also report the results using LPD - 9 decoding . In addition , we compare our model with several strong autoregressive models , including LSTM - based ( Wu et al , 2016 ) , CNN - based ( Gehring et al , 2017 ) and transformer model . The results are shown in Table 3 , from which we see that our model outperforms the best NAG baseline ( with LPD ) in terms of both the generation quality and inference speedup . Additionally , we also report the results using the ratio - first decoding . By setting \u03b1 as 0.8 , the inference speedup can be further boosted to 13.92\u00d7 while the generation quality is still higher than the best NAG baseline .", "entities": [[0, 2, "TaskName", "Machine translation"], [48, 50, "MethodName", "knowledge distillation"], [67, 68, "MetricName", "BLEU"], [84, 85, "MethodName", "BERT"], [142, 143, "MethodName", "CRF"], [146, 147, "MethodName", "CRF"], [178, 179, "MethodName", "LSTM"], [254, 255, "HyperparameterName", "\u03b1"]]}
{"text": "Context - Aware Objective In this part , we study the effect of the context - aware objective . As described in Equation ( 11 ) , it aims at alleviating the problem of repetitive generation . To give a quantitative analysis , we use the measurement of sentencelevel repetition ( Welleck et al , 2020 ) to compute the ratio of duplicate n - grams ( rep - n ) in the generated result . This metric is defined as rep - n ( Y ) = 100 \u00d7 ( 1.0 \u2212 | unique n - grams ( Y ) | | n - grams ( Y ) | ) . ( 13 ) For each generated result , rep - n is 0.0 when it has no repeating n - grams . The final result is computed by averaging over the entire evaluation set . We conduct experiments on Gigawords dataset to evaluate the n - gram repetitions ranging from uni - gram to 4 - gram . The results are shown in Table 5 , where w/o CA means the model is trained without using context - aware objective and R - L denotes the model 's ROUGE - L score . Additionally , we also show the results from transformer model for a direct comparison . Comparing the two variants of our model , we see that training with context - aware objective leads to a 42 % drop on rep - 3 metric ( 0.427 vs 0.741 ) and a 64 % drop on rep - 4 metric ( 0.106 vs 0.295 ) . The ROUGE - L results also indicate that the reduction in token repetition can effectively improve the model generation quality . Dynamic Length Determination Next , we examine the importance of the model 's ability to dynamically determine the length of the generated output . To this end , we train another model variant by removing the two [ eos ] tokens from the target sequence . In this way , the model is not able to self - determine the output length throughout the generation process . To perform inference , we use length - parallel decoding ( LPD ) with different number of length candidates . Formally , for each length candidate l , the model generates the result\u1ef8 as Y = arg max Y { l i=1 \u03a6 y i ( h i ) + l i=2 t ( y i\u22121 , y i ) } . ( 14 ) The final result is acquired by re - ranking the generated results with a transformer model . We conduct experiments on the IWSLT14 DE - EN dataset in which we try a different number of length candidates , including top - 1 , top - 5 and top - 10 . The results are shown in Table 6 , from which we can see , as the number of length candidates increases , the model performance increases as well . The reason is that a larger candidates set is more likely to contain the best - suited length for the generation model , leading to better performance . However , such decoding procedure inevitably increases the required computation overhead . We can see that , when setting k as 10 , the inference speedup decreases from 11.84\u00d7 to 6.01\u00d7. In contrast , our proposed model is able to determine the optimal output length by itself . Without any re - ranking process , it outperforms the model with LPD - 10 decoding and achieves the inference speedup that is comparable with the model using LPD - 1 decoding .", "entities": [[200, 203, "MetricName", "ROUGE - L"], [270, 273, "MetricName", "ROUGE - L"]]}
{"text": "We are also interested in the effect of the ratio - first decoding strategy . To provide a quantitative analysis , we perform inference on the Gigawords dataset using ratio - first with different \u03b1 . The experimental results with different \u03b1 are presented in Figure 3 . It can be observed that , when \u03b1 reaches 0.3 , the model approximately achieves its optimal performance . At the same time , a notable improvement can be observed in terms of the inference speedup ( 6.72\u00d7 9.31\u00d7 ) . Now we illustrate why the near optimal performance can be achieved when \u03b1 reaches 0.3 . In Figure 4 , we present the distribution of the target / source length ratio of every data instance in the Gigawords dataset . We can see that , for most cases , the ratio between the target length T and source length T is less than 0.3 . Recall the definition of ratio - first decoding in Equation ( 10 ) , the [ \u03b1 T ] constrains the maximum length of the generated result . Therefore , once we have a prior knowledge on the data statistic , we can easily choose a proper \u03b1 that both improves the inference speed whilst maintaining the generation quality . In this case , a proper \u03b1 could be 0.3 which is demonstrated by the results in Figure 3 and 4 . By setting different \u03b1 , ratio - first provides us an explicit way to control the balance between the inference speed and the generation quality . This property of ratio - first is especially favorable in real - life scenarios where the inference speed is the highest concern .", "entities": [[34, 35, "HyperparameterName", "\u03b1"], [41, 42, "HyperparameterName", "\u03b1"], [55, 56, "HyperparameterName", "\u03b1"], [101, 102, "HyperparameterName", "\u03b1"], [154, 155, "MetricName", "Recall"], [170, 171, "HyperparameterName", "\u03b1"], [201, 202, "HyperparameterName", "\u03b1"], [220, 221, "HyperparameterName", "\u03b1"], [239, 240, "HyperparameterName", "\u03b1"]]}
{"text": "We conduct an ablation study by removing user interaction in reviewing the model - generated edits . Then , we compare the overall quality of final revised documents with and without the human - in - the - loop component . In both HUMAN - HUMAN and SYSTEM - HUMAN setups where users interacted with the system , they were not informed whether the revisions were sampled from our collected iterative revision dataset , or generated by the underlying text revision models . User Study Design . We hired three linguistic experts ( English L1 , bachelor 's or higher degree in Linguistics ) to interact with our text revision system . Each user was presented with a text revision ( as shown in Figure 2d ) and asked to accept or reject each edit in the current revision ( users were informed which revision depth they were looking at ) . For a fair comparison , users were not informed about the source of the edits ( human - written vs. model - generated ) , and the experiments were conducted separately one after the other . Note that the users were only asked to accept or reject edits , and they had control neither over the number of iterations , nor over the stopping criteria . The stopping criteria for the experiment were set by us and designed as : ( 1 ) no new edits were made at the following revision depth , or ( 2 ) the maximum revision depth t max = 3 was reached . Data Details . We followed the prior work ( Du et al , 2022 ) to collect the text revision data across three domains : ArXiv , Wikipedia and Wikinews . This data was then used to train both the edit intention identification models and the text revision generation model . We split the data into training , validation and test set according to their document For the human evaluation data , we randomly sampled 10 documents with a maximum revision depth of 3 from each domain in the test set in Table 1 . For the evaluation of text revisions made by human writers ( HUMAN - HUMAN ) , we presented the existing ground - truth references from our collected dataset to users . Since we do not hire additional human writers to perform continuous revisions , we just presented the static human revisions from the original test set to users at each revision depth , and collected the user acceptance statistics as a baseline for our system . For the evaluation of text revisions made by our system ( SYSTEM - HUMAN ) , we only presented the original source document at the initial revision depth ( D 0 ) to our system , and let the system generate edits in the following revision depths , while incorporating the accept / reject decisions on modelgenerated edit suggestions by the users . Note that at each revision depth , the system will only incorporate the edits accepted by users and pass them to the next revision iteration . For text revisions made by our system without human - in - the - loop ( SYSTEM - ONLY ) , we let the system generate edits in an iterative way and accepted all model - generated edits at each revision depth . Model Details . For both edit intention identification models , we fine - tuned the RoBERTa - large pre - trained checkpoint from Hugging - Face ( Wolf et al , 2020 ) for 2 epochs with a learning rate of 1 \u00d7 10 \u22125 and batch size of 16 . The edit - For the text revision generation model , we finetuned the PEGASUS - LARGE ( Zhang et al , 2020 ) pre - trained checkpoint from HuggingFace . We set the edit intentions as new special tokens ( e.g. , < STYLE > , < FLUENCY > ) , and concatenated the edit intention and source sentence together as the input to the model . The output of the model is the revised sentence , and we trained the model with cross - entropy loss . We fine - tuned the model for 5 epochs with a learning rate of 3 \u00d7 10 \u22125 and batch size of 4 . Finally , our text revision generation model achieves 41.78 SARI score ( Xu et al , 2016 ) , 81.11 BLEU score ( Papineni et al , 2002 ) and 89.08 ROUGE - L score ( Lin , 2004 ) on the test set .", "entities": [[208, 211, "HyperparameterName", "number of iterations"], [286, 287, "DatasetName", "ArXiv"], [462, 463, "DatasetName", "0"], [579, 580, "MethodName", "RoBERTa"], [602, 604, "HyperparameterName", "learning rate"], [610, 612, "HyperparameterName", "batch size"], [701, 702, "MetricName", "loss"], [714, 716, "HyperparameterName", "learning rate"], [722, 724, "HyperparameterName", "batch size"], [747, 749, "MetricName", "BLEU score"], [758, 761, "MetricName", "ROUGE - L"]]}
{"text": "Since our research focus is to leverage learned feature importance to enhance predictions , we followed proven methods for hate speech classification ( Gamb\u00e4ck and Sikdar , 2017 ) . All tweets were converted into the Glove twitter embeddings ( Pennington et al , 2014 ) . These embeddings were passed to a Convolution Neural Network classifier which mirrored the same architecture used by Yoon Kim for sentence classification ( Kim , 2014 ) . We allowed for parameter tuning with random search , and the final CNN consisted of three convolutional layers of 75 filters with kernel sizes of 3 , 4 and 6 . These all received one dimensional max pooling and a dropout rate of 0.4 was applied . The output layer is a softmax with l2 regularization set at 0.029 . These parameters were selected by ranking validation AUC . This model served as both our baseline model and the input predictions of our predictions enhanced with XAI .", "entities": [[8, 10, "TaskName", "feature importance"], [19, 21, "DatasetName", "hate speech"], [53, 54, "MethodName", "Convolution"], [67, 69, "TaskName", "sentence classification"], [81, 83, "MethodName", "random search"], [111, 113, "MethodName", "max pooling"], [127, 128, "MethodName", "softmax"], [129, 131, "HyperparameterName", "l2 regularization"], [142, 143, "MetricName", "AUC"]]}
{"text": "NODE were introduced as a continuous depth alternative to Residual Networks ( ResNets ) ( He et al , 2016 ) . ResNets uses skip connections to avoid vanishing gradient problems when networks grow deeper . Residual block output is computed as h t+1 = h t + f ( h t , \u03b8 t ) , where f ( ) is a neural network ( NN ) parameterized by \u03b8 t and h t representing the hidden representation at depth t. This update is similar to a step in Euler numerical technique used for solving ordinary differential equations ( ODE ) dh ( t ) dt = f ( h ( t ) , t , \u03b8 ) . The sequence of residual block operations in ResNets can be seen as a solution to this ODE . Consequently , NODEs can be interpreted as a continuous equivalent of ResNets modeling the evolution of hidden representationsh ( t ) over time . For solving ODE , one can use fixed stepsize numerical techniques such as Euler , Runge - Kutta or adaptive step - size methods like Do - pri5 ( Dormand and Prince , 1980 ) . Solving an ODE requires one to specify an initial value h ( 0 ) ( input x or its transformation ) and can compute the value at t using an ODE solver ODESolverCompute ( f \u03b8 , h ( 0 ) , 0 , t ) . An ODE is solved until some end - time T to obtain the final hidden representation h ( T ) which is used to predict class labels\u0177 . For classification problems , cross - entropy loss is used and parameters are learnt through adjoint sensitivity method ( Zhuang et al , 2020 ; Chen et al , 2018 ) which provides efficient back - propagation and gradient computations .", "entities": [[0, 1, "MethodName", "NODE"], [36, 38, "MethodName", "Residual block"], [53, 54, "HyperparameterName", "\u03b8"], [70, 71, "HyperparameterName", "\u03b8"], [117, 118, "HyperparameterName", "\u03b8"], [123, 125, "MethodName", "residual block"], [210, 211, "DatasetName", "0"], [233, 234, "HyperparameterName", "\u03b8"], [237, 238, "DatasetName", "0"], [240, 241, "DatasetName", "0"], [280, 281, "MetricName", "loss"]]}
{"text": "LSTMs are popular for sequence classification but only considers the sequential nature of the data and ignore the temporal features associated with the data in its standard setting . As the posts occur in irregular intervals of time , the nature of a new post will be influenced by the recent posts , influence will be inversely proportional to the time gap . In these situations , it will be beneficial to use a model where the number of transformations depend on the time gap . We propose to use RNODE which considers the arrival time and accordingly the hidden representations are transformed across time . In RNODE , the transformation of a hidden representation h ( t i\u22121 ) at time t i\u22121 to h ( t i ) at time t i is governed by an ODE parameterized by a NN f ( ) . Unlike standard LSTMs where h ( t i ) is obtained from h ( t i\u22121 ) as a single NN transformation , RNODE first obtains a hidden representation h \u2032 ( t i ) as a solution to an ODE at time t i with initial value h ( t i\u22121 ) . The number of update steps in the numerical technique used to solve this ODE depends on the time gap t i \u2212t i\u22121 between the consecutive posts . The hidden representation h \u2032 ( t i ) and input post x i at time t i are passed through neural network transformation ( RNNCell ( ) ) to obtain final hidden representation h ( t i ) , i.e. , h ( t i ) = RNNCell ( h \u2032 ( t i ) , x i ) . The process is repeated for every element ( x i , t i ) in the sequence . The hidden representations associated with the elements in the sequence are then passed to a neural network ( NN ( ) ) to obtain the post labels . Using standard cross - entropy loss , the parameters of the models are learnt through backpropagation . Figure 1 provides the detailed architecture of the Bi - directional RNNs ( Schuster and Paliwal , 1997 ) such as Bi - LSTMS ( Graves et al , 2013 ) were proven to be successful in many sequence labeling tasks in natural language processing such as POS tagging ( Huang et al , 2015 ) . They use the information from the past and future to predict the label while standard LSTMs consider only information from the past . We propose a Bi - RNODE model , which uses the sequence of input observations from past and from the future to predict the post label at any time t. It assumes the hidden representation dynamics are influenced not only by the past posts but also by the futures posts . Unlike Bi - LSTMs , Bi - RNODE considers the exact time of the posts and their inter - arrival times in determining the transformations in the hidden representations . Bi - RNODE consists of two RNODE blocks , one performing transformations in the forward direction ( in the order of posting times ) and the other in the backward direction . The hidden representations H and H b computed by forward and backward RNODE respectively are aggregated either by concatenation or by averaging appropriately to obtain a final hidden representation and is passed through a NN to obtain the post labels . Bi - RNODE is useful when a sequence of posts with their time of occurrence needs to be classified together . Figure 2 provides an overview of Bi - RNODE model for post classification . For Bi - RNODE , an extra neural network f \u03b8 \u2032 ( ) is required to compute hidden representations h b ( t \u2032 i ) in the backward direction . Training in Bi - RNODE is done in a similar manner to RNODE , with cross - entropy loss and back - propagation to estimate parameters .", "entities": [[341, 342, "MetricName", "loss"], [632, 633, "HyperparameterName", "\u03b8"], [672, 673, "MetricName", "loss"]]}
{"text": "We conducted experiments to predict the stance of social media posts propagating in seen events and unseen events . - Seen Event Here we train , validate and test on tweets of the same event . Each event data is split 60:20:20 ratio in sequence of time . This setup helps in predicting the stance of unseen tweets of the same event . - Unseen Event : This setup helps in evaluating performance on an unseen event and training on a larger dataset . Here , training and validation data are formed using data from 3 events and testing is done on the 4 th event . Last 20 % of the training data ( after ordering based on time ) are set aside for validation . During training , mini - batches are formed only from the tweets belonging to the same event . Baselines : We compared results of our proposed RNODE and Bi - RNODE models with RNN based baselines such LSTM ( Kochkina et al , 2017 ) , Bi - LSTM ( Augenstein et al , 2016 ) , GRU ( Cho et al , 2014 ) , Bi - GRU , and Majority ( labelling with most frequent class ) baseline models . We also use a variant of LSTM baseline considering temporal information ( Zubiaga et al , 2018b ) , LSTM - timeGap where the time gap of consecutive data points is included as part of the input data . Evaluation Metrics : We consider the standard evaluation metrics such as precision , recall , F1 and in addition the AUC score to account for the data imbalance . We consider a weighted average of the evaluation metrics to compare the performance of models . Hyperparameters : All the models are trained for 50 epochs with 0.01 learning rate , Adam optimizer , dropout ( 0.2 ) regularizer , batchsize of 50 , hidden representation size of 64 and cross entropy as the loss function . Different hyperparameters like neural network layers ( 1 , 2 ) , numerical methods ( Euler , RK4 , Dopri5 for RNODE and Bi - RNODE ) and aggregation strategy ( concatenation or averaging for Bi - LSTM Bi - GRU and Bi - RNODE ) are used for all the models and the best configuration is selected from the validation data for different experimental setups and train / test data splits .", "entities": [[164, 165, "MethodName", "LSTM"], [175, 176, "MethodName", "LSTM"], [184, 185, "MethodName", "GRU"], [195, 196, "MethodName", "GRU"], [215, 216, "MethodName", "LSTM"], [228, 229, "MethodName", "LSTM"], [263, 264, "MetricName", "F1"], [268, 269, "MetricName", "AUC"], [305, 307, "HyperparameterName", "learning rate"], [308, 309, "MethodName", "Adam"], [309, 310, "HyperparameterName", "optimizer"], [331, 332, "MetricName", "loss"], [371, 372, "MethodName", "LSTM"], [374, 375, "MethodName", "GRU"]]}
{"text": "The results of seen event and unseen event experiment setup can be found in Table 1 , where the first and second rows for each model provides results on seen event and unseen event respectively . We can observe from Table 1 that for both seen event and unseen event experiment setup , RNODE and Bi - RNODE models performed better than the baseline models in general for all the 3 events 3 . In particular for the seen event setup , Bi - RNODE gives the best result outperforming RNODE and other models for most of the data sets and measures . Under seen event experiment on Syndneysiege event , we plot the ROC curve for all the models in Figure 3 . We can observe that AUC for Figures 3 ( a ) and 3 ( e ) corresponding to RNODE and Bi - RNODE respectively are higher than LSTM , GRU , Bi - LSTM , and Bi - GRU .", "entities": [[128, 129, "MetricName", "AUC"], [151, 152, "MethodName", "LSTM"], [153, 154, "MethodName", "GRU"], [157, 158, "MethodName", "LSTM"], [162, 163, "MethodName", "GRU"]]}
{"text": "We propose a new end - to - end model that treats AMR parsing as a series of dual decisions on the input sequence and the incrementally constructed graph . At each time step , our model performs multiple rounds of attention , reasoning , and composition that aim to answer two critical questions : ( 1 ) which part of the input sequence to abstract ; and ( 2 ) where in the output graph to construct the new concept . We show that the answers to these two questions are mutually causalities . We design a model based on iterative inference that helps achieve better answers in both perspectives , leading to greatly improved parsing accuracy . Our experimental results significantly outperform all previously reported SMATCH scores by large margins . Remarkably , without the help of any large - scale pre - trained language model ( e.g. , BERT ) , our model already surpasses previous state - of - the - art using BERT . With the help of BERT , we can push the state - of - the - art results to 80.2 % on LDC2017T10 ( AMR 2.0 ) and 75.4 % on LDC2014T12 ( AMR 1.0 ) .", "entities": [[12, 14, "TaskName", "AMR parsing"], [117, 118, "MetricName", "accuracy"], [151, 152, "MethodName", "BERT"], [167, 168, "MethodName", "BERT"], [173, 174, "MethodName", "BERT"], [191, 192, "DatasetName", "LDC2017T10"]]}
{"text": "Abstract Meaning Representation ( AMR ) ( Banarescu et al , 2013 ) is a broad - coverage semantic formalism that encodes the meaning of a sentence as a rooted , directed , and labeled graph , where nodes represent concepts and edges represent relations ( See an example in Figure 1 ) . AMR parsing is the task of transforming natural language text into AMR . One biggest challenge of AMR parsing is the lack of explicit alignments between nodes ( concepts ) in the graph and words in the text . This characteristic not only poses great difficulty in concept * The work described in this paper is substantially supported by grants from the Research Grant Council of the Hong Kong Special Administrative Region , China ( Project Code : 14204418 ) and the Direct Grant of the Faculty of Engineering , CUHK ( Project Code : 4055093 ) . prediction but also brings a close tie for concept prediction and relation prediction . While most previous works rely on a pre - trained aligner to train a parser , some recent attempts include : modeling the alignments as latent variables ( Lyu and Titov , 2018 ) , attention - based sequenceto - sequence transduction models ( Barzdins and Gosko , 2016 ; Konstas et al , 2017 ; van Noord and Bos , 2017 ) , and attention - based sequence - to - graph transduction models ( Cai and Lam , 2019 ; Zhang et al , 2019b ) . Sequence - to - graph transduction models build a semantic graph incrementally via spanning one node at every step . This property is appealing in terms of both computational efficiency and cognitive modeling since it mimics what human experts usually do , i.e. , first grasping the core ideas then digging into more details ( Banarescu et al , 2013 ; Cai and Lam , 2019 ) . Unfortunately , the parsing accuracy of existing works including recent state - of - the - arts ( Zhang et al , 2019a , b ) remain unsatisfactory compared to human - level performance , 1 especially in cases where the sentences are rather long and informative , which indicates substantial room for improvement . One possible reason for the deficiency is the inherent defect of one - pass prediction process ; that is , the lack of the modeling capability of the interactions between concept prediction and relation prediction , which is critical to achieving fullyinformed and unambiguous decisions . We introduce a new approach tackling AMR parsing , following the incremental sequence - tograph transduction paradigm . We explicitly characterize each spanning step as the efforts for finding which part to abstract with respect to the input sequence , and where to construct with respect to the partially constructed output graph . Equivalently , we treat AMR parsing as a series of dual decisions on the input sequence and the incrementally constructed graph . Intuitively , the answer of what concept to abstract decides where to construct ( i.e. , the relations to existing concepts ) , while the answer of where to construct determines what concept to abstract . Our proposed model , supported by neural networks with explicit structure for attention , reasoning , and composition , integrated with an iterative inference algorithm . It iterates between finding supporting text pieces and reading the partially constructed semantic graph , inferring more accurate and harmonious expansion decisions progressively . Our model is aligner - free and can be effectively trained with limited amount of labeled data . Experiments on two AMR benchmarks demonstrate that our parser outperforms the previous best parsers on both benchmarks . It achieves the best - reported SMATCH scores ( F1 ) : 80.2 % on LDC2017T10 and 75.4 % on LDC2014T12 , surpassing the previous state - of - the - art models by large margins .", "entities": [[54, 56, "TaskName", "AMR parsing"], [71, 73, "TaskName", "AMR parsing"], [144, 145, "DatasetName", "CUHK"], [327, 328, "MetricName", "accuracy"], [430, 432, "TaskName", "AMR parsing"], [481, 483, "TaskName", "AMR parsing"], [630, 631, "MetricName", "F1"], [636, 637, "DatasetName", "LDC2017T10"]]}
{"text": "At each sequence reasoning step t , the concept solver receives a state vector y t that carries the latest graph decision and the input sequence memories h 1 , . . . , h n from the sequence encoder , and aims to locate the proper parts in the input sequence to abstract and generate a new concept . We employ the scaled dot - product attention proposed in Vaswani et al ( 2017 ) to solve this problem . Concretely , we first calculate an attention distribution over all input tokens : \u03b1 t = softmax ( ( W Q y t ) T W K h 1 : n \u221a d k ) , where { W Q , W K } R d k \u00d7d denote learnable linear projections that transform the input vectors into the query and key subspace respectively , and d k represents the dimensionality of the subspace . The attention weights \u03b1 t R n provide a soft alignment between the new concept and the tokens in the input sequence . We then compute the probability distribution of the new concept label through a hybrid of three channels . First , \u03b1 t is fed through an MLP and softmax to obtain a probability distribution over a pre - defined vocabulary : MLP ( \u03b1 t ) = ( W V h 1 : n ) \u03b1 t + y t ( 1 ) P ( vocab ) = softmax ( W ( vocab ) MLP ( \u03b1 t ) + b ( vocab ) ) , where W V R d\u00d7d denotes the learnable linear projection that transforms the text memories into the value subspace , and the value vectors are averaged according to \u03b1 t for concept label prediction . Second , the attention weights \u03b1 t directly serve as a copy mechanism ( Gu et al , 2016 ; See et al , 2017 ) , i , e. , the probabilities of copying a token lemma from the input text as a node label . Third , to address the attribute values such as person names or numerical strings , we also use \u03b1 t for another copy mechanism that directly copies the original strings of input tokens . The above three channels are combined via a soft switch to control the production of the concept label from different sources : [ p 0 , p 1 , p 2 ] = softmax ( W ( switch ) MLP ( \u03b1 t ) ) , where MLP is the same as in Eq . 1 , and p 0 , p 1 and p 2 are the probabilities of three prediction channels respectively . Hence , the final prediction probability of a concept c is given by : P ( c ) = p 0 P ( vocab ) ( c ) + p 1 ( i L ( c ) \u03b1 t [ i ] ) + p 2 ( i T ( c ) \u03b1 t [ i ] ) , where [ i ] indexes the i - th element and L ( c ) and T ( c ) are index sets of lemmas and tokens respectively that have the surface form as c.", "entities": [[63, 68, "MethodName", "scaled dot - product attention"], [94, 95, "HyperparameterName", "\u03b1"], [97, 98, "MethodName", "softmax"], [159, 160, "HyperparameterName", "\u03b1"], [199, 200, "HyperparameterName", "\u03b1"], [205, 206, "DatasetName", "MLP"], [207, 208, "MethodName", "softmax"], [220, 221, "DatasetName", "MLP"], [222, 223, "HyperparameterName", "\u03b1"], [234, 235, "HyperparameterName", "\u03b1"], [247, 248, "MethodName", "softmax"], [253, 254, "DatasetName", "MLP"], [255, 256, "HyperparameterName", "\u03b1"], [293, 294, "HyperparameterName", "\u03b1"], [305, 306, "HyperparameterName", "\u03b1"], [337, 338, "DatasetName", "lemma"], [365, 366, "HyperparameterName", "\u03b1"], [405, 406, "DatasetName", "0"], [414, 415, "MethodName", "softmax"], [420, 421, "DatasetName", "MLP"], [422, 423, "HyperparameterName", "\u03b1"], [428, 429, "DatasetName", "MLP"], [440, 441, "DatasetName", "0"], [476, 477, "DatasetName", "0"], [493, 494, "HyperparameterName", "\u03b1"], [508, 509, "HyperparameterName", "\u03b1"]]}
{"text": "At each graph reasoning step t , the relation solver receives a state vector x t that carries the latest concept decision and the output graph memories s 0 , s 1 , . . . , s m from the graph encoder , and aims to point out the nodes in the current graph that have an immediate relation to the new concept ( source nodes ) and generate corresponding edges . Similar to Cai and Lam ( 2019 ) ; Zhang et al ( 2019b ) , we factorize the task as two stages : First , a relation identification module points to some preceding nodes as source nodes ; Then , the relation classification module predicts the relation type between the new concept and predicted source nodes . We leave the latter to be determined after iterative inference . AMR is a rooted , directed , and acyclic graph . The reason for AMR being a graph instead of a tree is that it allows reentrancies where a concept participates in multiple semantic relations with different semantic roles . Following Cai and Lam ( 2019 ) , we use multi - head attention for a more compact parsing procedure where multiple source nodes are simultaneously determined . 5 Formally , our relation identification module employs H different attention heads , for each head h , we calculate an attention distribution over all existing node ( including the dummy node s 0 ) : \u03b2 h t = softmax ( ( W Q h x t ) T W K h s 0 : m \u221a d k ) . Then , we take the maximum over different heads as the final edge probabilities : \u03b2 t [ i ] = H max h=1 \u03b2 h t [ i ] . Therefore , different heads may points to different nodes at the same time . Intuitively , each head represents a distinct relation detector for a particular set of relation types . For each attention head , it will point to a source node if certain relations exist between the new node and the existing graph , otherwise it will point to the dummy node . An example with four attention heads and three existing nodes ( excluding the dummy node ) is illustrated in Figure 3 .", "entities": [[28, 29, "DatasetName", "0"], [115, 117, "TaskName", "relation classification"], [192, 196, "MethodName", "multi - head attention"], [243, 244, "DatasetName", "0"], [246, 247, "HyperparameterName", "\u03b2"], [250, 251, "MethodName", "softmax"], [264, 265, "DatasetName", "0"], [287, 288, "HyperparameterName", "\u03b2"], [296, 297, "HyperparameterName", "\u03b2"]]}
{"text": "As described above , the concept solver and the relation solver are conceptually two attention mechanisms over the sequence and graph respectively , addressing the concept prediction and relation prediction separately . The key is to pass the decisions between the solvers so that they can examine each other 's answer and make harmonious decisions . Specifically , at each spanning step i , we start the iterative inference by setting x 0 = h 0 and solving f ( G i , x 0 ) . After the t - th graph reasoning , we compute the state vector y t , which will be handed over to the concept solver as g ( W , y t ) , as : y t = FFN ( y ) ( x t + ( W V h 1 : n ) \u03b1 t ) , where FFN ( y ) is a feed - forward network and W V projects text memories into a value space . Similarly , after the t - th sequence reasoning , we update the state vector from y t to x t+1 as : x t+1 = FFN ( x ) ( y t + H h=1 ( W V h s 0 : n ) \u03b2 h t ) , where FFN ( x ) is a feed - forward network and W V h projects graph memories into a value space for each head h. After N steps of iterative inference , i , e. , x 0 f ( G i , x 0 ) y 1 g ( W , y 1 ) x 1 f ( G i , x N \u22121 ) y N g ( W , y N ) x N , we finally employ a deep biaffine classifier ( Dozat and Manning , 2016 ) for edge label prediction . The Algorithm 1 AMR Parsing via Graph Sequence Iterative Inference Input : the input sentence W = ( w 1 , w 2 , . . . , w n ) Output : the corresponding AMR graph G // compute text memories 1 : h 0 , h 1 , . . . , h n = SequenceEncoder ( ( BOS , w 1 , . . . , w n ) ) // initialize graph 2 : G 0 = ( nodes= { BOG } , edges= ) // start graph expansions 3 : i = 0 4 : while True do 5 : s 0 , . . . , s i = GraphEncoder ( G i ) // the graph memories can be computed * incrementally * 6 : x 0 = h 0 // iterative inference 7 : for t 1 to N do 8 : y t = f ( G i , x t\u22121 ) // Seq . Graph 9 : x t = g ( W , y t ) // Graph Seq . i = i + 1 16 : end while 17 : return G i classifier uses a biaffine function to score each label , given the final concept representation x N and the node vector s 1 : m as input . The resulted concept , edge , and edge label predictions will added to the new graph G i+1 if the concept prediction is not EOG , a special concept that we add for indicating termination . Otherwise , the whole parsing process is terminated and the current graph is returned as final result . The complete parsing process adopting the iterative inference is described in Algorithm 1 .", "entities": [[72, 73, "DatasetName", "0"], [75, 76, "DatasetName", "0"], [84, 85, "DatasetName", "0"], [142, 143, "HyperparameterName", "\u03b1"], [209, 210, "DatasetName", "0"], [213, 214, "HyperparameterName", "\u03b2"], [256, 257, "DatasetName", "0"], [263, 264, "DatasetName", "0"], [319, 321, "TaskName", "AMR Parsing"], [361, 362, "DatasetName", "0"], [395, 396, "DatasetName", "0"], [413, 414, "DatasetName", "0"], [422, 423, "DatasetName", "0"], [449, 450, "DatasetName", "0"], [452, 453, "DatasetName", "0"]]}
{"text": "Our model is trained with the standard maximum likelihood estimate . The optimization objective is to maximize the sum of the decomposed step - wise log - likelihood , where each is the sum of concept , edge , and edge label probabilities . To facilitate training , we create a reference generation order of nodes by running a breadth - first - traversal over target AMR graphs , as it is cognitively appealing ( core - semantic - first principle , Cai and Lam , 2019 ) and the effectiveness of pre - order traversal is also empirically verified by Zhang et al ( 2019a ) in a depth - first setting . For the generation order for sibling nodes , we adopt the uniformly random order and the deterministic order sorted by the relation frequency in a 1 : 1 ratio at first then change to the deterministic order only in the final training steps . We empirically find that the deterministic - afterrandom strategy slightly improves performance . During testing , our model searches for the best output graph through beam search based on the log - likelihood at each spanning step . The time complexity of our model is O ( k | V | ) , where k is the beam size , and | V | is the number of nodes .", "entities": [[25, 28, "MetricName", "log - likelihood"], [188, 191, "MetricName", "log - likelihood"]]}
{"text": "The performance of AMR parsing is conventionally evaluated by SMATCH ( F1 ) metric . The left block of Table 1 shows the SMATCH scores on the AMR 2.0 test set of our models against the previous best approaches and recent competitors . On AMR 2.0 , we outperform the latest push from Zhang et al ( 2019b ) by 3.2 % and , for the first time , obtain a parser with over 80 % SMATCH score . Note that even without BERT , our model still outperforms the previous state - of - the - art approaches using BERT ( Zhang et al , 2019b , a ) with 77.3 % . This is particularly remarkable since running BERT is computationally expensive . As shown in most models trained on AMR 2.0 . The even more substantial performance gain on the smaller dataset suggests that our method is both effective and dataefficient . Besides , again , our model without BERT already surpasses previous state - of - the - art results using BERT . For ablated models , it can be observed that our models yield the best results in all settings if there are any competitors , indicating BERT and graph re - categorization are not the exclusive key for our superior performance .", "entities": [[3, 5, "TaskName", "AMR parsing"], [11, 12, "MetricName", "F1"], [83, 84, "MethodName", "BERT"], [100, 101, "MethodName", "BERT"], [120, 121, "MethodName", "BERT"], [162, 163, "MethodName", "BERT"], [175, 176, "MethodName", "BERT"], [202, 203, "MethodName", "BERT"]]}
{"text": "Effect of Iterative Inference We then turn to study the effect of our key idea , namely , the iterative inference design . To this end , we run a set of experiments with different values of the number of the inference steps N . The results on AMR 2.0 are shown in Figure 4 ( solid line ) . As seen , the performance generally goes up when the number of inference steps increases . The difference is most noticeable between 1 ( no iterative reasoning is performed ) and 2 , while later improvements gradually diminish . One important point here is that the model size in terms of the number of parameters is constant regardless of the number of inference steps , making it different from general over - parameterized problems . For a closer study on the effect of the inference steps with respect to the lengths of input sentences , we group sentences into three classes by length and also show the individual results in Figure 4 ( dashed lines ) . As seen , the iterative inference helps more for longer sentences , which confirms our intuition that longer and more complex input needs more reasoning . Another interesting observation is that the performance on shorter sentences reaches the peaks earlier . This observation suggests that the number of inference steps can be adjusted according to the input sentence , which we leave as future work .", "entities": [[112, 115, "HyperparameterName", "number of parameters"]]}
{"text": "We are also interested in the effect of beam size during testing . Ideally , if a model is able to make accurate predictions in the first place , it should rely less on the search algorithm . We vary the beam size and plot the curve in Figure 6 . The results show that the performance generally gets better with larger beam sizes . However , a small beam size of 2 already gets the most of the credits , which suggests that our model is robust enough for time - stressing environments . Visualization We visualize the iterative reasoning process with a case study in Figure 5 . We illustrate the values of \u03b1 t , \u03b2 t as the iterative inference progresses . As seen , the parser makes mistakes in the first step , but gradually corrects its decisions and finally makes the right predictions . Later reasoning steps typically provide a sharper attention distribution than earlier steps , narrowing down the most likely answer with more confidence . Speed We also report the parsing speed of our non - optimized code : With BERT , the parsing speed of our system is about 300 tokens / s , while without BERT , it is about 330 tokens / s on a single Nvidia P4 GPU . The absolute speed depends on various implementation choices and hardware performance . In theory , the time complexity of our parsing algorithm is O ( kbn ) , where k is the number of iterative steps , b is beam size , and n is the graph size ( number of nodes ) respectively . It is important to note that our algorithm is linear in the graph size .", "entities": [[115, 116, "HyperparameterName", "\u03b1"], [118, 119, "HyperparameterName", "\u03b2"], [188, 189, "MethodName", "BERT"], [205, 206, "MethodName", "BERT"]]}
{"text": "A Hyper - parameter Settings Table 3 lists the hyper - parameters used in our full models . Char - level CNNs and Transformer layers in the sentence encoder and the graph encoder share the same hyper - parameter settings . The BERT model ( Devlin et al , 2019 ) we used is the Huggingface 's implementation ( Wolf et al , 2019 ) ( bert - base - cased ) . To mitigate overfitting , we apply dropout ( Srivastava et al , 2014 ) with the drop rate 0.2 between different layers . We randomly mask ( replacing inputs with a special UNK token ) the input lemmas , POS tags , and NER tags with a rate of 0.33 . Parameter optimization is performed with the ADAM optimizer ( Kingma and Ba , 2014 ) with \u03b2 1 = 0.9 and \u03b2 2 = 0.999 . The learning rate schedule is similar to that in Vaswani et al ( 2017 ) , with warm - up steps being set to 2K. We use early stopping on the development set for choosing the best model .", "entities": [[23, 24, "MethodName", "Transformer"], [42, 43, "MethodName", "BERT"], [116, 117, "TaskName", "NER"], [130, 131, "DatasetName", "ADAM"], [131, 132, "HyperparameterName", "optimizer"], [140, 141, "HyperparameterName", "\u03b2"], [145, 146, "HyperparameterName", "\u03b2"], [151, 153, "HyperparameterName", "learning rate"], [177, 179, "MethodName", "early stopping"]]}
{"text": "Input strictly local ( ISL ) functions are a class of subregular transductions that have well - understood mathematical and computational properties and that are sufficiently expressive to account for a wide variety of attested morphological and phonological patterns ( e.g. , Chandlee , 2014 ; Chandlee , 2017 ; . In this study , we compared several approaches to learning ISL functions : the ISL function learning algorithm ( ISLFLA ; Chandlee , 2014 ; and the classic OSTIA learner to which it is related ( Oncina et al , 1993 ) ; the Minimal Generalization Learner ( MGL ; Hayes , 2002 , 2003 ) ; and a novel deep neural network model presented here ( DNN - ISL ) . The four models were evaluated on their ability to learn the mapping from feminine singular ( fem.sg . ) to masculine singular ( masc.sg . ) surface forms of Catalan adjectives ( and , separately , from provided underlying representations to fem.sg . and masc.sg . surface forms ) . The mappings to masc.sg . forms in Catalan involve several phonological modifications at the right edge of the word ( e.g. , Mascar\u00f3 , 1976 ) , the empirical focus of our study . The relevant processes include obstruent devoicing and strengthening ( e.g. , [ rOZ@ ] fem.sg . [ rOtS ] masc.sg . ' red ' ) , post - tonic n - Deletion ( e.g. , [ san@ ] [ sa ] ' healthy ' ) , and cluster simplification ( e.g. , [ blaNk@ ] [ blaN ] ' white ' ) . There are opaque , counterfeeding interactions among some of the processes ( e.g. , [ f@kund@ ] [ f@kun ] / * [ f@ku ] ' fertile ' ) , consistent with the idea that the mappings are input - rather than output - determined ( see . A small number of apparent lexical exceptions to the typical modification pattern ( e.g. , [ blan@ ] [ blan ] / * [ bla ] ' soft ' ) are problematic for ISL learners that assume perfect homogeneity . Our main findings were that the DNN - ISL learner achieved high accuracy on the Catalan data , with MGL coming in a close second , while ISLFLA and OSTIA performed much worse - either failing to learn any mapping at all or predicting the correct output for less than 5 % of held - out cases , even when lexical exceptions were removed from the data ( see Table 1 ) .", "entities": [[370, 371, "MetricName", "accuracy"]]}
{"text": "For the purposes of this abstract , we assume familiarity with ISLFLA , OSTIA , and MGL . We verified that the implementation of MGL learns only ISL phonological rules - rules conditioned on local phonological context in the result of a morphological operation such as affixation or truncation - a connection that has not previously been made in the literature . The deep neural network model proposed here ( DNN - ISL ) also applies morphological operations followed by phonological modifications , the latter being implemented with weighted constraints rather than rules . A phonological constraint as learned by DNN - ISL is defined by : a three - segment featural pattern specifying the input context to which the constraint applies ; a preference for one type of modification applied to the center segment of the context ( i.e. , deletion , epenthesis before / after , or feature change ) ; target output features in the case of epenthesis or change ; and a real - valued strength . Each constraint computes the degree to which its context matches every three - segment window in the input ( i.e. , it applies a novel feature based convolution operation to the input ) and imposes its preferred modification in proportion to the degree of match and its strength . These preferences are summed over constraints for each input position and applied to the positions independently to derive the phonological output . The parameters of the constraints are straightforwardly interpretable and visualizable as real - valued phonological feature coefficients , modification - type logits , and strengths . The model is fully differentiable and was trained with the Adagrad optimizer on small mini - batches for 20 epochs .", "entities": [[198, 199, "MethodName", "convolution"], [278, 279, "MethodName", "Adagrad"], [279, 280, "HyperparameterName", "optimizer"]]}
{"text": "Coreference resolution ( CR ) is an essential part of discourse analysis . Most recently , neural approaches have been proposed to improve over SOTA models from earlier paradigms . So far none of the published neural models leverage external semantic knowledge such as type information . This paper offers the first such model and evaluation , demonstrating modest gains in accuracy by introducing either gold standard or predicted types . In the proposed approach , type information serves both to ( 1 ) improve mention representation and ( 2 ) create a soft type consistency check between coreference candidate mentions . Our evaluation covers two different grain sizes of types over four different benchmark corpora .", "entities": [[0, 2, "TaskName", "Coreference resolution"], [61, 62, "MetricName", "accuracy"]]}
{"text": "In this section , we provide the results of our empirical experiments . Evaluation Metrics : We convert all three datasets into the CoNLL 2012 format and report the F1 score for MUC , B 3 , and CEAF metrics using the CoNLL - 2012 official scripts . The performances are compared on the average F1 of the abovementioned metrics . For EmailCoref , OntoNotes , and WikiCoref , we report the mean score of 5 independent runs of the model with different seeds . Whereas , for LitBank , we present the 10 - fold cross - validation results . 5", "entities": [[23, 25, "DatasetName", "CoNLL 2012"], [29, 31, "MetricName", "F1 score"], [54, 56, "MetricName", "average F1"], [64, 65, "DatasetName", "OntoNotes"], [67, 68, "DatasetName", "WikiCoref"], [83, 84, "DatasetName", "seeds"], [88, 89, "DatasetName", "LitBank"]]}
{"text": "In order to establish an upper bound for improvement through introduction of type information , our first experiment leverages the original list of entity - types annotated in different corpora ( + ET ( orig ) ) , using the gold standard labels for types . Inclusion of entity - type information improves over the baseline for all CR datasets . Table 2 presents the performance of the baseline model and the model with entity - type information . We find that entity - type information gives a boost of 0.96 Avg . F1 ( p < 0.01 ) on LitBank which is the new state - of - the - art score with goldmentions . This suggests that type information is helpful for CR on LitBank despite the heavily skewed distribution of entity - types in this corpus . Similarly , type information also benefits Email - Coref and WikiCoref resulting in an absolute improvement of 1.67 and 2.9 Avg . F1 points respectively ( p < 0.01 ) . We also see a 2.4 Avg . F1 improvement ( p < 0.01 ) on OntoNotes , the largest dataset in this study . This suggests that explicit access to type information is beneficial all over the board , despite the use of contextual representations which have been claimed to model realworld facts and relationships ( Petroni et al , 2019 ) . Ablation Results : To understand the contribution of the inclusion of type information to improve mention representation ( + ET - self ) and type consistency check between candidate mentions ( + ET - cross ) , we perform an ablation study ( Table 3 ) . We find that both components consistently provide significant performance boosts over the baseline . However , their combination ( + ET ) performs the best across all datasets .", "entities": [[93, 94, "MetricName", "F1"], [100, 101, "DatasetName", "LitBank"], [126, 127, "DatasetName", "LitBank"], [150, 151, "DatasetName", "WikiCoref"], [162, 163, "MetricName", "F1"], [178, 179, "MetricName", "F1"], [186, 187, "DatasetName", "OntoNotes"]]}
{"text": "Our hypothesis around the use of entity - types was to provide additional information to the model that could be leveraged to minimize errors due to type mismatch in CR . To evaluate if the F1 score improvements achieved by + ET models are because of fewer type mismatch errors , we report the number of coreference clusters detected by the model that contain at least one element with a type that is different from the others in the cluster . Since all of the datasets used in this work only consider identity coreferences ( Recasens et al , 2011 ) - with potentially varied definitions of identity ( Bamman et al , 2020 ; Pradhan et al , 2012 ) - where the mention is a linguistic \" re - packaging \" of its antecedent , this measure makes sense . As shown in Table 2 , the models that score lower on the impurity measure get a higher Avg F1 . This suggests that the aggregate performance improvements are at least partly due to the better mention - mention comparison in + ET systems .", "entities": [[35, 37, "MetricName", "F1 score"], [160, 162, "MetricName", "Avg F1"]]}
{"text": "Type Prediction : Our final evaluation of the use of types in coreference is perhaps the most important one as it uses predicted types rather than annotated types , thus demonstrating that the benefits can be achieved in practice . Here we use the Type Prediction Model described just above . We limit the length of the input sequence to 128 tokens and use BERT - base - cased model for our type - prediction experiments . We perform a fivefold cross - validation to predict the type for each mention in the dataset . Since all four datasets suffer from class - imbalance , we report both Macro F1 score as well as the accuracy for the model . The model is trained for 20 epochs , with earlystopping ( patience = 10 ) , and is fine - tuned on the development set for Macro F1 to give more importance to minority type categories . We do not consider NA as a separate class during type prediction for WikiCoref and OntoNotes . For evaluation of our type - prediction model , we ignore the mentions that do not have an associated gold type ( NA ) from the final numbers in Table 4 . As shown , our model performs well on Lit - Bank , EmailCoref , and Ontonotes due to their favorable size in terms of training samples for the BERT - based type predictor . WikiCoref , however , proves more challenging as the model only manages 38.0 Macro F1 points with original ( orig ) types and 45.0 with common types ( com ) , portraying its lack of ability to learn minority type categories with less data . Furthermore , our model finds it easier to predict the common ( com ) set of types for each dataset as combining multiple corpus - specific types into one partially alleviates the problem of class - imbalance . In line with our expectation , the largest improvement due to common types is seen for OntoNotes where the prob - lem reduces from an 18 - way classification to a 5way classification . Coreference Resolution : Each mention in the corpus occurs in the test - sets of the five - fold cross - validation type - prediction experiments exactly once . This allows us to infer the type of each mention using the model that is trained on a different subset of the dataset . These inferred types are used in the training and testing of the CR systems in a manner similar to the annotated types . Empirically , we found that the above configuration performs better than using the + ET models trained with annotated types and testing with predicted types , as the former exposes the CR models to the noisy types during training thus allowing them to learn weights that take this noise into account . We report the results for both original ( + ET - pred ( orig ) ) and common ( + ET - pred ( com ) ) type categories on each dataset . Table 5 shows the results for performance of the baseline and the type - informed models on the four datasets , where the types are inferred from the model described in Section 6.1 . We find that the improvements from type - information persist across LitBank , EmailCoref , and OntoNotes despite the use of predicted types , but , quite expectedly , remain smaller than the improvements from the gold annotated types . Scores on WikiCoref show no significant improvement over the baseline , which could be explained by the poor performance of the type prediction model on this dataset which reduces the potency of the feature for CR .", "entities": [[0, 2, "TaskName", "Type Prediction"], [44, 46, "TaskName", "Type Prediction"], [64, 65, "MethodName", "BERT"], [108, 110, "MetricName", "Macro F1"], [115, 116, "MetricName", "accuracy"], [146, 148, "MetricName", "Macro F1"], [167, 169, "TaskName", "type prediction"], [170, 171, "DatasetName", "WikiCoref"], [172, 173, "DatasetName", "OntoNotes"], [221, 222, "DatasetName", "Ontonotes"], [234, 235, "MethodName", "BERT"], [240, 241, "DatasetName", "WikiCoref"], [253, 255, "MetricName", "Macro F1"], [339, 340, "DatasetName", "OntoNotes"], [357, 359, "TaskName", "Coreference Resolution"], [563, 564, "DatasetName", "LitBank"], [568, 569, "DatasetName", "OntoNotes"], [594, 595, "DatasetName", "WikiCoref"], [613, 615, "TaskName", "type prediction"]]}
{"text": "As shown in Figure 1 , attention model in PBAN consists of two parts : including the aspect term to the position - aware sentence part and a position - aware sentence to the aspect term part . For the former part , we can obtain the different hidden contextual representation of a sentence according to different word in aspect term . For the later part , we can obtain the attention weights of the words in aspect term according to the position information , which is used for getting the final representation of a sentence . Details will be described in follwing sections . Aspect term to position - aware sentence attention : A sentence should be represented differently based on different words in aspect term , because different words may have different effects on the final representation of the sentence . We firstly get the hidden contextual representation of the aspect term by the left Bi - GRU , and get the hidden contextual representation of inputs ( i.e. , the concatenation of word embedding and position embedding ) by the right Bi - GRU structure . Here , we regard the position embedding as the part of the inputs , because it intuitively represents the relative distance of words in a sentence to the current aspect term as mentioned in section 2.1 . Then we calculate the attention weights by adopting hidden contextual representation of aspect term and inputs , obtaining the attention weight distribution of sentence corresponding to each word in this aspect term . It can be formulated as follows : s i = N j=1 \u03b1 ij h j ( 2 ) \u03b1 ij = exp ( f ( h j , h t i ) ) N k=1 exp ( f ( h k , h t i ) ) ( 3 ) f ( h j , h t i ) = tanh ( h T j W m h t i + b m ) ( 4 ) where \u03b1 ij indicates the attention weights from the word h t i in the aspect term to the j - th word in the inputs , and tanh is a non - liner activation function . W m is the weight matrix and b m is the bias . Subsequently , \u03b1 ij is used to compute a weighted sum of the hidden representation s i , producing a semantic vector that represents the input sequence . Position - aware sentence attention to aspect term : As we mentioned above , different words in aspect term will play different role in judging the sentiment polarity of aspect term . Since we obtain the hidden contextual representation of the inputs by the right Bi - GRU , we utilize both the position and semantic information for calculating the attention weights of different words in aspect term . The process can be formulated as follows : h R = M i=1 \u03b3 i s i ( 5 ) \u03b3 i = exp ( f ( h , h t i ) ) M k=1 exp ( f ( h , h t k ) ) ( 6 ) f ( h , h t i ) = tanh ( h T W n h t i + b n ) ( 7 ) h = 1 N N i=1 h i ( 8 ) where \u03b3 i stands for the attention weights from inputs to the words in aspect term , denoting which word in aspect term should be more focused . h is calculated by averagely pooling all Bi - GRU hidden states . Later , the sequence representation x is obtained by using a non - linear layer : x = tanh ( W R h R + b R ) ( 9 ) where W R and b R are weight matrix and bias respectively . We feed x into a linear layer , the length of whose output equals to the number of class labels S . Finally , we add a softmax layer to compute the probability distribution for judging the sentiment polarities as positive , negative or neutral : y = sof tmax ( W s x + b s ) ( 10 ) where W s and b s are the weight matrix and bias respectively for softmax layer .", "entities": [[159, 160, "MethodName", "GRU"], [186, 187, "MethodName", "GRU"], [271, 272, "HyperparameterName", "\u03b1"], [278, 279, "HyperparameterName", "\u03b1"], [338, 339, "HyperparameterName", "\u03b1"], [371, 373, "HyperparameterName", "activation function"], [389, 390, "HyperparameterName", "\u03b1"], [462, 463, "MethodName", "GRU"], [497, 498, "HyperparameterName", "\u03b3"], [504, 505, "HyperparameterName", "\u03b3"], [571, 572, "HyperparameterName", "\u03b3"], [607, 608, "MethodName", "GRU"], [624, 626, "MethodName", "linear layer"], [660, 662, "MethodName", "linear layer"], [682, 683, "MethodName", "softmax"], [703, 704, "DatasetName", "sof"], [730, 731, "MethodName", "softmax"]]}
{"text": "The PBAN model can be trained in an end - to - end way in a supervised learning framework , the aim of the training is to optimize all the parameters so as to minimize the objective function ( loss function ) as much as possible . In our work , let y i be the correct sentiment polarity , which is represented by one - hot vector , and y i denotes the predicted sentiment polarity for the given sentence . We regard the cross - entropy as the loss function , and the formula is as follows : loss = \u2212 S i=1 y i log ( y i ) + 1 2 \u03bb \u03b8 2 ( 11 ) where \u03bb is the regularization factor and \u03b8 contains all the parameters . Furthermore , in order to avoid over - fitting , we adopt the dropout strategy to enhance our PBAN model .", "entities": [[39, 40, "MetricName", "loss"], [90, 91, "MetricName", "loss"], [100, 101, "MetricName", "loss"], [116, 117, "HyperparameterName", "\u03b8"], [128, 129, "HyperparameterName", "\u03b8"]]}
{"text": "Parameters Setting : In our experiments , all word embedding are initialized by the pre - trained Glove vector 2 ( Pennington et al , 2014 ) . All the weight matrices are given the initial value by sampling from the uniform distribution U ( \u22120.1 , 0.1 ) , and all the biases are set to zero . The dimension of the word embedding and aspect term embedding are set to 300 , and the number of the hidden units are set to 200 . The dimension of position embedding is set to 100 , which is randomly initialized and updated during the training process . We use Tensorflow ( Abadi et al , 2016 ) to implement our proposed model and employ the Momentum as the training method , whose momentum parameter \u03b3 is set to 0.9 , \u03bb is set to 10 \u22126 , and the initial learning rate is set to 0.01 . Dataset : To evaluate our proposed methods , we conduct experiments on the dataset of SemEval 2014 Task4 3 , the SemEval 2014 dataset consists of reviews in Restaurant and Laptop datasets . Each review contains a list of aspect terms and corresponding polarities , which are labeled with { positive , negative , neutral } . Particularly , each aspect term has its character index in the sentence , so that when different aspect term have the same word in a sentence , we can mark the relative position distance of a sentence according to the current aspect term without confusion . Table 1 shows the training and test sample numbers in each sentiment polarity .", "entities": [[134, 135, "HyperparameterName", "\u03b3"], [150, 152, "HyperparameterName", "learning rate"]]}
{"text": "In order to evaluate the performance of our model , we compare our model with several baseline models , including LSTM , AE - LSTM , ATAE - LSTM , IAN ( Ma et al , 2017 ) and MemNet ( Tang et al , 2016 LSTM : LSTM takes the sentence as input so as to get the hidden representation of each word . Then it regards the average value of all hidden states as the representation of sentence , and puts it into softmax layer to predict the probability of each sentiment polarity . However , it can not capture any information of aspect term in sentence . AE - LSTM : AE - LSTM first models the words in sentence via LSTM network and concatenate the aspect embedding to the hidden contextual representation for calculating the attention weights , which are employed to produce the final representation for the input sentence to judge the sentiment polarity . ATAE - LSTM : ATAE - LSTM extended AE - LSTM by appending the aspect embedding to each word embedding so as to represent the input sentence , which highlights the role of aspect embedding . The other design of ATAE - LSTM is the same as AE - LSTM . IAN : IAN considers the separate modeling of aspect terms and sentences respectively . IAN is able to interactively learn attentions in the contexts and aspect terms , and generates the representations for aspect terms and contexts separately . Finally , it concatenates the aspect term representation and context representation for predicting the sentiment polarity of the aspect terms within its contexts ( Ma et al , 2017 ) . MemNet : MemNet applies attention multiple times on the word embedding , so that more abstractive evidences could be selected from the external memory . The output of the last attention layer is fed to a softmax layer for predictions ( Tang et al , 2016 Table 2 shows the performance of our model and other baseline models on datasets Restaurant and Laptop respectively . We can observe that our proposed PBAN model achieves the best performance among all methods . It is obvious that LSTM method gets the worst performance , because it treats aspect term and other words as the same , so that it can not take full advantage of the aspect term information and predicts the same polarity for different aspect terms in a sentence . Furthermore , both AE - LSTM and ATAE - LSTM perform better than LSTM model , because they all consider the importance of the aspect term , and utilize the attention mechanism . Specifically , ATAE - LSTM outperforms AE - LSTM since it appends the aspect embedding to each word embedding and takes them as inputs , which helps the model obtain more semantic information related to aspect term . IAN realizes the importance of interaction between aspect term and context , and models aspect term and context using two connected attention networks . Thus , IAN performs better than ATAE - LSTM , and achieves an improvement of 1.40 points and 3.40 points on Restaurant and Laptop datasets in Three - class respectively . MemNet ( 9 ) utilizes a more complex structure that containing nine computational layers , and it achieves better results compared to IAN since MemNet reads the useful information from external memory repeatedly . Although both IAN and MemNet models performance better than other methods , they all perform less competitive than our PBAN both on Restaurant and Laptop datasets . For IAN model , it interactively learns the attentions between the aspect term and its corresponding sentence , but this attention mechanism is coarse - grained and it does not fully consider the influence of different words in aspect term on the sentence . For MemNet model , although it utilizes the location information , it is mainly used for calculating the memory vectors . Nevertheless , PBAN utilizes the character index of the aspect term ( provided in the raw dataset ) and adopts relative distance to represent the position sequence . As we have mentioned in previous sections , an aspect term contains several words and different words in aspect term should have different contributions to the final representation of sentence . In PBAN , the position information is regarded as the inputs of the Bi - GRU , so it can help calculate the weights of different words in aspect term and improve the final representation of the sentence . Moreover , when different aspect terms contain the same word , our proposed position information can effectively identify the current aspect term without confusion while MemNet can not . Generally speaking , by integrating the position information and the bidirectional attention mechanism , PBAN achieves the state - of - the - art performances , and it can effectively judge the sentiment polarity of different aspect term in its corresponding sentence so as to improve the classification accuracy .", "entities": [[20, 21, "MethodName", "LSTM"], [22, 23, "MethodName", "AE"], [24, 25, "MethodName", "LSTM"], [28, 29, "MethodName", "LSTM"], [30, 31, "MethodName", "IAN"], [46, 47, "MethodName", "LSTM"], [48, 49, "MethodName", "LSTM"], [85, 86, "MethodName", "softmax"], [110, 111, "MethodName", "AE"], [112, 113, "MethodName", "LSTM"], [114, 115, "MethodName", "AE"], [116, 117, "MethodName", "LSTM"], [124, 125, "MethodName", "LSTM"], [162, 163, "MethodName", "LSTM"], [166, 167, "MethodName", "LSTM"], [168, 169, "MethodName", "AE"], [170, 171, "MethodName", "LSTM"], [202, 203, "MethodName", "LSTM"], [207, 208, "MethodName", "AE"], [209, 210, "MethodName", "LSTM"], [211, 212, "MethodName", "IAN"], [213, 214, "MethodName", "IAN"], [225, 226, "MethodName", "IAN"], [317, 318, "MethodName", "softmax"], [366, 367, "MethodName", "LSTM"], [414, 415, "MethodName", "AE"], [416, 417, "MethodName", "LSTM"], [420, 421, "MethodName", "LSTM"], [424, 425, "MethodName", "LSTM"], [448, 449, "MethodName", "LSTM"], [450, 451, "MethodName", "AE"], [452, 453, "MethodName", "LSTM"], [482, 483, "MethodName", "IAN"], [508, 509, "MethodName", "IAN"], [514, 515, "MethodName", "LSTM"], [559, 560, "MethodName", "IAN"], [573, 574, "MethodName", "IAN"], [599, 600, "MethodName", "IAN"], [737, 738, "MethodName", "GRU"], [838, 839, "MetricName", "accuracy"]]}
{"text": "Since a simple and effective method to learn distributed representation was proposed ( Mikolov et al , 2013 ) , neural networks enhance target - dependent sentiment analysis significantly . Vo and Zhang ( 2015 ) split a tweet into a left context and a right context according to a given target , using distributed word representations and neural pooling functions to extract features . Tang et al ( 2015 ) proposed TD - LSTM and TC - LSTM , where target information is automatically taken into account . These two models integrated the connections between target words and context words so as to significantly boost the classification accuracy . Zhang et al ( 2016 ) proposed two gated neural networks , one was used to capture tweet - level syntactic and semantic information , and the other was used to model the interactions between the left context and the right context of a given target . With the gating mechanism , the target influenced the selection of sentiment signals over the context .", "entities": [[26, 28, "TaskName", "sentiment analysis"], [74, 75, "MethodName", "LSTM"], [78, 79, "MethodName", "LSTM"], [108, 109, "MetricName", "accuracy"]]}
{"text": "Since it is intuitive that each language has its own characteristic , we set language - individual attention mechanisms for different languages . In the individual semantic space of the j - th language , we assign a query vector r j to each relation r R. The attention score for each sentence in S j = { x 1 j , x 2 j , . . . } is defined as follows , \u03b1 i j = exp ( r j y i j ) | S j | k=1 exp ( r j y k j ) . ( 7 ) The attention scores can be used to compute language - individual textual relation representations , sj = | S j | k=1 \u03b1 k j y k j . ( 8 )", "entities": [[75, 76, "HyperparameterName", "\u03b1"], [126, 127, "HyperparameterName", "\u03b1"]]}
{"text": "Besides language - individual attention mechanisms , we also adopt a language - consistent attention to take all sentences in all languages into consideration . In the consistent semantic space , we also assign a query vectorr to each relation r R and the attention score for each sentence is defined as follows , \u03b2 i j = exp ( r \u0233 i j ) n l=1 | S l | k=1 exp ( r \u0233 k l ) . ( 9 ) The attention scores can be used to compute language - consistent textual relation representations , s = n l=1 | S l | k=1 \u03b2 k l\u0233 k l . ( 10 )", "entities": [[54, 55, "HyperparameterName", "\u03b2"], [107, 108, "HyperparameterName", "\u03b2"]]}
{"text": "With the language - individual textual relation representations { s 1 , s 2 , . . . } and the language - consistent textual relation representations , we can estimate the probability p ( r | T ) over each relation r R , p ( r | T ) = p ( r | s ) n j=1 p ( r | sj ) . ( 11 ) p ( r | s ) and p ( r | s j ) can be defined as follows , p ( r | sj ) = softmax [ Rjsj + dj ] , p ( r | s ) = softmax [ Rs + d ] , ( 12 ) where d j andd are bias vectors , R j is the specific relation matrix of the j - th language , andR is the consistent relation matrix . We define the objective function to train the relation extractor as follows , min \u03b8 Lnre ( \u03b8 ) = \u2212 l log p ( r l | T l ) , ( 13 ) where \u03b8 is all parameters in the framework . In the training phase , p ( r | T ) is computed using the labeled relations as the attention queries . In the test phase , we need to use each possible relation as attention queries to compute p ( r | T ) for relation prediction since the relations are unknown in advance .", "entities": [[97, 98, "MethodName", "softmax"], [111, 112, "MethodName", "softmax"], [164, 165, "HyperparameterName", "\u03b8"], [167, 168, "HyperparameterName", "\u03b8"], [186, 187, "HyperparameterName", "\u03b8"]]}
{"text": "In our framework , we encode sentences of various languages into a consistent semantic space to grasp the consistency among languages . One possible situation is that sentences of different languages are aggregated in different places of the space and linearly separable . In this case , our purpose of mining substantially consistent relation patterns in different languages is difficult to be reached . Inspired by Ganin et al ( 2016 ) , we adopt adversarial training into our framework to address this problem . In the adversarial training , we define a discriminator to estimate which kind of languages the sentences from . The probability distributions over these sentences are formalized as follows , D ( s i j ) = softmax ( MLP ( s i j ) ) , ( 14 ) where MLP is a two - layer multilayer perceptron network . Contrary to the discriminator , the consistent sentence encoders are expected to produce sentence embeddings that can not be reliably predicted by the discriminator . Hence , the adversarial training process is a min - max game and can be formalized as follows , min \u03b8 C E max \u03b8 D n j=1 | S j | i=1 log [ D ( E C j ( x i j ) ) ] j , ( 15 ) where [ ] j is the j - th value of the vector . The formula means that given a sentence of any language , the corresponding sentence encoder of its language generates the sentence embedding to confuse the discriminator . Meanwhile , the discriminator tries its best to predict the language of the sentence according to the sentence embedding . After sufficient training , the encoders and the discriminator reach a balance , and sentences of different languages containing similar semantic information can be well encoded into adjacent places of the space . In training , we optimize the following loss functions instead of Eq . 15 , min \u03b8 C E L E adv ( \u03b8 C E ) = l S j T l x i j S j log [ D ( E C j ( x i j ) ) ] j , min \u03b8 D L D adv ( \u03b8 D ) = \u2212 l S j T l x i j S j log [ D ( E C j ( x i j ) ) ] j , ( 16 ) where \u03b8 C E and \u03b8 D are all parameters of the consistent sentence encoders and the discriminator . We notice that language - individual semantics could be wrongly encoded into the consistent semantic space , and may have negative effects on extracting language - consistent features . Inspired by Bousmalis et al ( 2016 ) , we adopt orthogonality constraints to alleviate this issue . We minimize the following penalty function : min \u03b8 E L penalty ( \u03b8 E ) = n j=1 I T j Cj F , ( 17 ) where I j and C j are two matrices whose row vectors are the embeddings of sentences in the j - th language encoded by E I j and E C j respectively . \u03b8 E is parameters of the all encoders . And F is the squared Frobenius norm .", "entities": [[64, 65, "DatasetName", "Inspired"], [122, 123, "MethodName", "softmax"], [124, 125, "DatasetName", "MLP"], [136, 137, "DatasetName", "MLP"], [159, 161, "TaskName", "sentence embeddings"], [191, 192, "HyperparameterName", "\u03b8"], [195, 196, "HyperparameterName", "\u03b8"], [257, 259, "TaskName", "sentence embedding"], [281, 283, "TaskName", "sentence embedding"], [324, 325, "MetricName", "loss"], [333, 334, "HyperparameterName", "\u03b8"], [340, 341, "HyperparameterName", "\u03b8"], [372, 373, "HyperparameterName", "\u03b8"], [378, 379, "HyperparameterName", "\u03b8"], [413, 414, "HyperparameterName", "\u03b8"], [417, 418, "HyperparameterName", "\u03b8"], [460, 461, "DatasetName", "Inspired"], [486, 487, "HyperparameterName", "\u03b8"], [491, 492, "HyperparameterName", "\u03b8"], [540, 541, "HyperparameterName", "\u03b8"]]}
{"text": "During training process , we combine the extraction and adversarial objective functions as follows , L = Lnre ( \u03b8 ) + \u03bb1L D adv ( \u03b8 D ) + \u03bb2L E adv ( \u03b8 C E ) + \u03bb3L penalty ( \u03b8 E ) , ( 18 ) where \u03bb 1 , \u03bb 2 , and \u03bb 3 are harmonic factors . All models are optimized using stochastic gradient descent ( SGD ) . In practice , we integrate \u03bb 1 and \u03bb 2 into the alternating ratio among the loss functions , and we calibrate a 1:1:5 ratio among L nre ( \u03b8 ) + \u03bb 3 L penalty ( \u03b8 E ) , L D adv ( \u03b8 D ) and L E adv ( \u03b8 C E ) . \u03bb 3 is set as 0.02 .", "entities": [[19, 20, "HyperparameterName", "\u03b8"], [26, 27, "HyperparameterName", "\u03b8"], [34, 35, "HyperparameterName", "\u03b8"], [42, 43, "HyperparameterName", "\u03b8"], [68, 71, "MethodName", "stochastic gradient descent"], [72, 73, "MethodName", "SGD"], [91, 92, "MetricName", "loss"], [104, 105, "HyperparameterName", "\u03b8"], [112, 113, "HyperparameterName", "\u03b8"], [120, 121, "HyperparameterName", "\u03b8"], [128, 129, "HyperparameterName", "\u03b8"]]}
{"text": "We evaluate our models on a multi - lingual relation extraction dataset developed by . We evaluate all models by the held - out evaluation following previous works ( Mintz et al , 2009 ; . In experiments , we report precision - recall curves of recall under 0.3 since we focus more on the performance of those top - ranked results . To give a complete view of the performance , we also report the area under the curve ( AUC ) .", "entities": [[9, 11, "TaskName", "relation extraction"], [81, 82, "MetricName", "AUC"]]}
{"text": "To evaluate the effectiveness of our proposed models AMNRE - CNN and AMNRE - RNN , we compare the proposed models with various neural methods : MNRE - CNN and MNRE - RNN are multi - lingual attention - based NRE models with CNN and RNN sentence encoders respectively ; CNN - EN and RNN - EN are vanilla selective - attention NRE models trained with English data , which are the state - of - the - art models in mono - lingual RE ( Lin et al , 2016 ) ; CNN - CN and RNN - CN are trained with Chinese data ; CNN - Joint and RNN - Joint are naive joint models which predict relations by directly summing up ranking scores of both English and Chinese ; CNN - Share and RNN - Share are another naive joint models which train English and Chinese models with shared relation embeddings . The results of precision - recall curves are shown in Figure 2 and the results of AUC are shown in Table 3 . From the results , we have the following observations : ( 1 ) Both for CNN and RNN , the models jointly utilizing English and Chinese sentences outperform the models only using mono - lingual sentences . This demonstrates that the rich information in multi - lingual data is useful and can significantly enhance existing NRE models . ( 2 ) The - Joint models achieve similar performance with the - Share models , and both of them underperform the MNRE and AMNRE models . They all benefit from the multi - lingual information , but the models with multi - lingual attentions can better take advantage of multi - lingual data . It indicates that designing targeted schemes to extract rich multi - lingual information is crucial . ( 3 ) AMNRE achieves the best results among all the baseline models over the entire range of recall in Figure 2 , even as compared with MNRE . AMNRE also outperforms MNRE with 3 percentage points increasing in the AUC results . It indicates our proposed framework which explicitly encodes languageconsistent and language - individual semantics better extract multi - lingual information , and therefore lead to the significant improvement in RE performance .", "entities": [[171, 172, "MetricName", "AUC"], [347, 348, "MetricName", "AUC"]]}
{"text": "To further verify that every mono - lingual RE models can benefit from our proposed framework , which explicitly consider language - consistent relation patterns , we train models with multi - lingual data and evaluate the performance of these models in the mono - lingual RE scenario . To show the results clearly , we report the precision - recall curves in Figure 3 and the AUC results in Table 4 . From the results , we can observe that : ( 1 ) As compared with the models directly learned with the mono - lingual data , the models exploiting the multi - lingual information perform better in the mono - lingual scenario . This demonstrates that there is latent consistency among languages , and grasping this consistency from multi - lingual data can provide additional information for models in each language to enhance their results in the mono - lingual scenario . ( 2 ) Our proposed models achieve the best precision over the entire range of recall and also significantly improve the AUC results as compared with both MNRE and mono - lingual RE models . It indicates that due to the consistent semantic space in our framework , language - consistent information lying in the multi - lingual data is better mined and serve the mono - lingual scenario .", "entities": [[67, 68, "MetricName", "AUC"], [176, 177, "MetricName", "AUC"]]}
{"text": "We adopt an adversarial training strategy to fuse the features from different languages to extract consistent relation patterns . Orthogonality constraints are also adopted to separate the consistent and individual feature spaces . To measure the effectiveness of them , we conduct an ablation study which compares the proposed models with the similar models but without adversarial training strategy ( AMNRE - noA ) , without orthogonality constraints ( AMNRE - noO ) , and without both of them ( AMNRE - noBoth ) . The AUC results are shown in Table 5 . We can observe that both the adversarial training strategy and orthogonality constraints have significant influence on the performance of our proposed model . This demonstrates the effectiveness of adversarial training strategy and orthogonality constraints for multi - lingual RE . To give a more intuitive picture of the effect of these two mechanisms , we visualize the distribution of sentence feature embeddings encoded by the individual and consistent encoders using t - SNE ( Maaten and Hinton , 2008 ) . The results are shown in Figure 4 . Figure 4 ( a ) shows that there are obvious differences between the feature embeddings encoded from the same sentences by individual and consistent encoders . It indicates the orthogonality constraints are effective to separate the individual and consistent latent spaces . From the comparison between Figure 4 ( c ) , we can observe that the feature embeddings from different languages are wellmixed due to the adversarial training strategy . We can more easily to grasp latent consistency among languages after multi - feature fusion .", "entities": [[86, 87, "MetricName", "AUC"]]}
{"text": "Lexical Antonyms . To detect whether an S i m - H int pair uses the lexical antonyms strategy , we first need to build a resource of lexical antonyms . We use the MPQA sentiment Lexicon ( Wilson et al , 2005 ) , 2004 ) . We consider a lexical antonym strategy if : 1 ) antonym words are aligned ; 2 ) they are the roots of the respective dependency trees or if the nodes modified by the lexical antonyms are the same in their respective trees ( e.g. , ' can you show any more of steelers \" ! \" show less of steelers \" , the candidate lexical antonyms are more and less and they are the objects of the same predicate in S i m - H int : show ) . Out of 211 S i m - H int pairs that are marked as having lexical antonym strategy ( dev set ) , 12 instances are identified by only the dependency parses , 67 instances by the word - alignments , and 100 instances by both ( P / R / F1 scores are 92.1 % , 77.7 % and 84.3 % ) , respectively on dev dataset . However , sometimes both dependency and wordalignment methods fail . In \" circling down the bowl . Yay \" ! \" circling down the bowl . awful \" , although the lexical antonyms yay and awful exist , neither the alignment nor the dependency trees can detect it ( 25 such instances in the dev set ) . To account for this , after having run the dependency and alignment methods , we also just search whether a S i m - H int pair contains a lexical antonym pair . This improves the final recall and on the dev set we achieve 89.0 % precision , 95.7 % recall , and 92.2 % F1 on dev dataset ( Lex ant Strategy ; Table 3 show results both on dev and the test sets ) . Note , just searching whether a lexical antonym pair is present in a S i m - H int pair results in low precision ( 58.6 % ) but high recall ( 80 % ) . Simple negation . This strategy ( denoted as Simple neg in Table 3 and Table 4 ) involves identifying the presence of negation and its scope . Here , however , the scope of negation is con - strained since generally Turkers negated only a single word ( i.e. , \" love \" ! \" not love \" ) . Thus our problem is easier than the general problem of finding the scope of negation ( Li and Lu , 2018 ; Qian et al , 2016 ; Fancellu et al , 2016 ) . We use 30 negation markers from Reitan et al ( 2015 ) to find negation scope in tweets . We first detect whether a negation marker appears in either H int or S i m , but not in both ( negation can appear in S i m for ironic blame ) If the marker is used , we extract its parent node from the dependency tree , and if this node is also present in the other utterance , then Negation strategy is selected . For instance , in \" looks just like me \" ! \" does not look like me \" , the negation not is modifying the main predicate looks in H int , which is also the main predicate in S i m ( words are lemmatized ) . In the next section , we discuss if the parent nodes are not the same but similar and with different sentiment strength . Weakening the intensity of sentiment . The first strategy - replacing words expressing a high degree of positive / negative sentiment with more neutral ones ( ' I love being sick \" ! \" I do n't like being sick ) - , is applied only in conjunction with the negation strategy . We measure the difference in strength using the Dictionary of Affect ( Whissell et al , 1986 ) . Out of 31 S i m - H int pairs in the dev set , we automatically identify 28 interpretations that use this approach . For the second strategy - removing the intensifier ( I am really happy \" ! \" I am disappointed ' ) - , we first determine whether the intensifier exists in S i m and is eliminated from H int . We use only adjective and adverb intensifiers from Taboada et al ( 2011 ) , primarily to discard conjunctions such as \" so \" ( \" no water so I ca n't wash . . . \" ) . This strategy is used together with both lexical antonyms and Simple negation strategies . For a candidate S i m - H int pair , if the lexical antonym strategy is selected and a S and a H are the lexical antonyms , we determine whether any intensifier modifies a S and no intensifier modifies a H . If the Negation strategy is se - lected , we identify the negated term in the H int and then search its aligned node from the S i m using the word - word alignment . Next , we search in the S i m if any intensifier is intensifying the aligned term . The strategies are denoted as AN weaksent in Table 3 and Table 4 . Interrogative to Declarative Transformation ( + Antonym / Neg ) . To capture this strategy we need to determine first if the verbal irony was expressed as a rhetorical question . To build a classifier to detect RQ , we collect two categories of tweets ( 4 K each ) ( 1 ) tweets labeled with # sarcasm or # irony that also contain \" ? \" , and ( 2 ) information seeking tweets containing \" ? \" . We train a binary classifier using SVM RBF Kernel with default parameters . The features are Twitter - trained word embeddings ( Ghosh et al , 2015 ) , modal verbs , pronouns , interrogative words , negations , and position of \" ? \" in a tweet . We evaluate the training model on the dev data and the P / R / F1 are 53.2 % , 65.4 % , and 58.6 % , respectively ( in future work we plan to develop more accurate models for RQ detection ) . Once we detect the ironic message was expressed as a RQ , we identify the specific interpretation strategy accompanying the transformation from interrogative to declarative form : antonym or negation . These combined strategies are denoted as AN I!D in Table 3 and Table 4 . Desiderative Constructions : Currently , we use a simple regular expression \" I [ w ] \u21e4 wish \" to capture counterfactual cases ( AN desiderative in Tables 3 and Table 4 ) . Note , when the Simple negation and lexical antonyms strategies are combined with other strategy ( e.g. , removing of intensifier ) , we consider this combined strategy for the interpretation of verbal irony and not the simple negation or lexical antonym strategy ( i.e. , we do not double count ) .", "entities": [[34, 35, "DatasetName", "MPQA"], [189, 190, "MetricName", "F1"], [321, 322, "MetricName", "F1"], [899, 901, "TaskName", "word alignment"], [1020, 1021, "MethodName", "SVM"], [1033, 1035, "TaskName", "word embeddings"], [1078, 1079, "MetricName", "F1"]]}
{"text": "The performance of the models is similar on both test and SIGN test sets , showing consistently good performance ( Table 3 ; 90 % F1 for all strategies , except the AntPhrase+PragInf and AN I!D ) . Given these results , we can now apply these models to study the distribution of these strategies in the entire datasets ( Table 4 ) . The strategy distribution between our dataset S i m - H int and SIGN dataset is similar and matches the distribution on the manual annotations on the dev dataset in Table 2 . The sum of the strategies can exceed the total number of the pairs since a tweet can contain several ironic sentences that are interpreted by Turkers . For instance , in \" Dave too nice . . . a nice fella \" ! \" Dave not nice . . . a mean fella \" we observe the application of two strategies , lexical antonyms ( e.g. , nice ! mean ) and negation ( e.g. , nice ! not nice ) .", "entities": [[25, 26, "MetricName", "F1"]]}
{"text": "Event schemas encode knowledge of stereotypical structures of events and their connections . As events unfold , schemas are crucial to act as a scaffolding . Previous work on event schema induction focuses either on atomic events or linear temporal event sequences , ignoring the interplay between events via arguments and argument relations . We introduce a new concept of Temporal Complex Event Schema : a graph - based schema representation that encompasses events , arguments , temporal connections and argument relations . In addition , we propose a Temporal Event Graph Model that predicts event instances following the temporal complex event schema . To build and evaluate such schemas , we release a new schema learning corpus containing 6 , 399 documents accompanied with event graphs , and we have manually constructed gold - standard schemas . Intrinsic evaluations by schema matching and instance graph perplexity , prove the superior quality of our probabilistic graph schema library compared to linear representations . Extrinsic evaluation on schema - guided future event prediction further demonstrates the predictive power of our event graph model , significantly outperforming human schemas and baselines by more than 23.8 % on", "entities": [[146, 147, "MetricName", "perplexity"]]}
{"text": "We use a Graph Neural Network ( GNN ) ( Kipf and Welling , 2017 ) to update node embeddings following the graph structure . Before we run the GNN on the graph , we first add virtual edges between the newly generated event and all previous events , and between new entities and previous entities , shown as dashed lines in Figure 2 . The virtual edges enable the representations of new nodes to aggregate the messages from previous nodes , which has been proven effective in ( Liao et al , 2019 ) . To capture rich semantics of edge types , we pass edge - aware messages during graph propagation . An intuitive way is to encode different edge types with different convolutional filters , which is similar to RGCN ( Schlichtkrull et al , 2018 ) . However , the number of RGCN parameters grows rapidly with the number of edge types and easily becomes unmanageable given the large number of relation types and argument roles in the IE ontology . 4 Instead , we learn a vector representation for each relation type r and argument role a. The message passed through each argument edge e i , a , v j is : m i , j = ReLU ( W a ( ( e i \u2212 v j ) a ) ) , where denotes concatenation operation . Similarly , the message between two entities v j and v k is : m j , k = ReLU ( W r ( ( v j \u2212 v k ) r ) ) . Considering that the direction of the temporal edge is important , we parametrize the message over this edge by assigning two separate weight matrices to the outgoing and incoming vertices : m i , l = ReLU ( W bfr e i \u2212 W aft e l ) . We aggregate the messages using edge - aware attention following ( Liao et al , 2019 ) : 5 \u03b1 i , j = \u03c3 ( MLP ( e i \u2212 e j ) ) , where \u03c3 is the sigmoid function , and MLP contains two hidden layers with ReLU nonlinearities . The event node representation e i is then updated using the messages from its local neighbors N ( e i ) , similar to entity node representations : e i GRU e i j N ( e i ) \u03b1 i , j m i , j .", "entities": [[132, 133, "MethodName", "RGCN"], [146, 147, "MethodName", "RGCN"], [173, 174, "MethodName", "ontology"], [213, 214, "MethodName", "ReLU"], [251, 253, "HyperparameterName", "k ="], [253, 254, "MethodName", "ReLU"], [305, 306, "MethodName", "ReLU"], [337, 338, "HyperparameterName", "\u03b1"], [344, 345, "DatasetName", "MLP"], [362, 363, "DatasetName", "MLP"], [368, 369, "MethodName", "ReLU"], [401, 402, "MethodName", "GRU"], [410, 411, "HyperparameterName", "\u03b1"]]}
{"text": "To predict the temporal dependencies between the new events and existing events , we connect them through temporal edges , as shown in Figure 2 . These edges are critical for message passing in predicting the next event . We build temporal edges in the last phase of generation , since it relies on the shared and related arguments . Considering that temporal edges are interdependent , we model the generation probability as a mixture of Bernoulli distributions following ( Liao et al , 2019 ) : p ( e i , e l | e i , e l ) = b \u03b3 b \u03b8 b , i , l , \u03b3 1 , , \u03b3 B = Softmax i , l MLP ( e i \u2212 e l ) , \u03b8 1 , i , l , , \u03b8 B , i , l = \u03c3 ( MLP \u03b8 ( e i \u2212 e l ) ) , where B is the number of mixture components . When B = 1 , the distribution degenerates to factorized Bernoulli , which assumes the independence of each potential temporal edge conditioned on the existing graph .", "entities": [[103, 104, "HyperparameterName", "\u03b3"], [105, 106, "HyperparameterName", "\u03b8"], [112, 113, "HyperparameterName", "\u03b3"], [116, 117, "HyperparameterName", "\u03b3"], [119, 120, "MethodName", "Softmax"], [123, 124, "DatasetName", "MLP"], [132, 133, "HyperparameterName", "\u03b8"], [140, 141, "HyperparameterName", "\u03b8"], [149, 150, "DatasetName", "MLP"], [150, 151, "HyperparameterName", "\u03b8"]]}
{"text": "We train the model by optimizing the negative loglikelihood loss , L = G G train \u2212 log 2 p ( G ) . To compose the schema library for each complex event scenario , we construct instance graphs from related documents to learn a graph model , and then obtain the schema using greedy decoding . 4 Evaluation Benchmark", "entities": [[9, 10, "MetricName", "loss"]]}
{"text": "To evaluate our temporal event graph model , we compute the instance graph perplexity by predicting the instance graphs in the test set , PP = 2 \u2212 1 | G test | G G test log 2 p ( G ) . ( 1 ) We calculate the full perplexity for the entire graph using Equation ( 1 ) , and event perplexity using only event nodes , emphasizing the importance of correctly predicting events .", "entities": [[13, 14, "MetricName", "perplexity"], [50, 51, "MetricName", "perplexity"], [63, 64, "MetricName", "perplexity"]]}
{"text": "Training Details . For our event graph model , the representation dimension is 128 , and we use a 2 - layer GNN . The value of B is 2 . The number of mixture components in temporal classifier is 2 . The learning rate is 1e - 4 . To train event language model baseline , instead of using LSTM - based architecture following ( Pichotta and Mooney , 2016 ) , we adopt the state - of - the - art auto - regressive language XLNet . In detail , we first linearize the graph using topological sort , and then train XLNet 9 using the dimension of 128 ( the same as our temporal event graph model ) , and the number of layers is 3 . The learning rate is 1e - 4 . We select the best model on the validation set . Both of our model and event language model baseline are trained on one Tesla V100 GPU with 16 GB DRAM . For sequential pattern mining , we perform random walk , starting from every node in instance graphs and ending at sink nodes , to obtain event type sequences , and then apply PrefixSpan ( Pei et al , 2001 ) 10 to rank sequential patterns . Evaluation Details . To compose the schema library , we use the first ranked sequence as the schema for these two models . To perform event prediction using baselines , we traverse the input graph to obtain event type sequences , and conduct prediction on all sequences to produce an averaged score . For human schemas , we first linearize them and the input graphs , and find the longest common subsequence between them .", "entities": [[43, 45, "HyperparameterName", "learning rate"], [60, 61, "MethodName", "LSTM"], [87, 88, "MethodName", "XLNet"], [104, 105, "MethodName", "XLNet"], [124, 127, "HyperparameterName", "number of layers"], [131, 133, "HyperparameterName", "learning rate"]]}
{"text": "Intrinsic Evaluation . In Table 3 , the significant gain on event match demonstrates the ability of our graph model to keep salient events . On sequence match , our approach achieves larger performance gain compared to baselines when the path length l is longer . It implies that the proposed model is capable of capturing longer and wider temporal dependencies . In the case of connection match , only sequential pattern mining in the baselines can predict connections between events . When compared against sequential pattern mining , our generation model significantly performs better since it considers the inter - dependency of arguments and encodes them with graph structures . Extrinsic Evaluation . On the task of schemaguided event prediction , our graph model obtains significant improvement ( see Table 4 . ) The low performance of human schema demonstrates the importance of probabilistically modeling schemas to support downstream tasks . Take Figure 3 as an example . Human schemas produce incorrect event types such as TRAILHEARING , since it matches the sequence ATTACK DIE TRAILHEARING , incapable of capturing the inter - dependencies between sequences . However , our model is able to customize the prediction to the global context of the input graph , and take into account that there is no AR - REST event or justice - related events in the input graph . Also , the human schema fails to predict INJURE and ATTACK , because it relies on the exact match of event sequences of lengths l \u2265 2 , and can not handle the variants of sequences . This problem can be solved by our probabilistic schema , via modeling the prediction probability conditioned on the existing graph . For example , even though AT - TACK mostly happens before DIE , we learn that ATTACK might repeat after DIE event if there are multiple ATTACK and DETONATE in the existing graph , which means the complex event is about a series of conflict events . Ablation Study . Removing argument generation ( \" w/o ArgumentGeneration \" ) generally lowers the performance on all evaluation tasks , since it ignores the coreferential arguments and their relations , but relies solely on the overly simplistic temporal order to connect events . This is especially apparent from the instance graph perplexity in Table 3 . Learning Corpus Size . An average of 113 instance graphs is used for each complex event type in the IED scenario , and 383 instance graphs to learn the schema model in the General scenario . The better performance on the IED dataset in Table 3 shows that the number of instance graphs increases the schema induction performance . Effect of Information Extraction Errors . Based on the error analysis for schemas induced in Table 1 , the effect of extraction errors can be categorized into : ( 1 ) temporal ordering errors : 43.3 % ; ( 2 ) missing events : 34.4 % ; ( 3 ) missing coreferential events : 8.8 % ; ( 4 ) incorrect event type : 7.7 % ; ( 5 ) missing coreferential arguments : 5.5 % . However , even on automatically extracted event graphs with extraction errors , our model significantly performs better on event prediction compared to humanconstructed schemas , as shown in Table 4 . It demonstrates that our schema induction method is robust and effective to support downstream tasks , even when only provided with noisy data with extraction errors .", "entities": [[246, 248, "MetricName", "exact match"], [386, 387, "MetricName", "perplexity"], [424, 425, "DatasetName", "General"]]}
{"text": "Not much . It was pretty dull . Blah , you did n't miss anything . Not anything that important . Very little , it was uneventful . Figure 1 : Diversity metric evaluation : we show two sets of responses to the same question , generated by crowdsourcing workers . While both sets are diverse in terms of form , only set A is diverse in terms of content . Each graph presents the distribution over a diversity metric for sets with high content diversity ( blue ) and low content diversity ( orange ) . Distributions are approximated over 200 sets . We observe that the human score metric ( absDHS ) separates the two distributions , while an n - gram based metric ( distinct - n ) fails , illustrating that it does not capture content diversity . The dotted lines correspond to the specific sets A and B presented above . all ) , making it difficult to compare competing approaches ( Hashimoto et al , 2019 ) . Having a principled and consensual diversity evaluation metric is hence fundamental for the field of NLG . A key challenge in developing diversity evaluation metrics , is the difficulty in determining their efficacy . Unlike metrics for evaluating the quality of generated text , where one can measure correlation between a metric ( such as BLEU ( Papineni et al , 2002 ) ) and human judgement ( Zhang et al , 2019a ; Sagarkar et al , 2018 ) , it is unknown if hu - mans can reliably estimate diversity . In this paper , we propose a framework for evaluating diversity metrics ( Figure 2 ) . We assume that a tester ( human or model ) is generating sets of sentences , conditioned on some diversity parameter that controls the diversity of the output sentences . We evaluate the diversity of the sentences using a proposed metric , and measure correlation between the metric and the diversity parameter . High correlation indicates that the metric captures how the diversity parameter affects the model output . We instantiate this framework with two tests . As a preliminary step , we introduce the decoding test : the tester is a neural generation model and the diversity parameter is a decoding parameter , such as softmax temperature ( Ackley et al , 1985 ) . This parameter controls the skewness of the distribution in every generated token , and has been shown to affect model diversity ( Holtzman et al , 2019 ; Caccia et al , 2018 ) . Then , we turn the focus to content diversity , introducing the content test ( Figure 1 ) . Here , the tester is a human , and the diversity parameter is a binary variable , where the human is instructed to generate sets of sentences with either high or low diversity in content . We evaluate three families of popular diversity metrics with these tests : ( a ) n - gram - based metrics that estimate diversity based on surface patterns in a set of generated sentences , ( b ) neural metrics : we propose a reduction from evaluating sentence similarity to evaluating diversity , then evaluate diversity using state - of - the - art sentence similarity models , and ( c ) human evaluation : we explore multiple ways in which humans can be asked to estimate diversity , resulting in multiple Human Diversity Score ( HDS ) variations . Applying our tests leads to several findings : ( i ) In the decoding test , n - gram - based metrics correlate well with decoding parameters , such as softmax temperature . While the goal of our framework is to evaluate diversity metrics , this result lets us reflect back on the tester itself and conclude that decoding parameters predominantly control the form of text rather than content . ( ii ) Conversely , n - gram - based metrics perform poorly in the content test . While neural metrics outperform n - gram - based metrics , humans are substantially better than any automatic metric at detecting content diversity . This is illustrated in Figure 1 , where a human clearly distinguishes between sets that have high ( blue ) and low ( orange ) content diver - sity , while n - gram - based metrics fail to do so . Due to this gap , we construct a large dataset focused on content - diversity metrics . We release the Metrics for content Diversity ( McDiv ) benchmark , a challenge for research in diversity evaluation . To conclude , our main contributions are : A framework for evaluating diversity metrics . Tests instantiating this framework , measuring the sensitivity of metrics to diversity , with a focus on content diversity . Best practices for obtaining diversity evaluations from crowdsourcing workers . Establishing that humans outperform current automatic metrics in detecting content diversity . The McDiv dataset - a benchmark for content diversity aware metrics . The collected data , test scores and code are publicly available , 1 and can be used to easily compare new diversity metrics to existing results in our framework .", "entities": [[229, 230, "MetricName", "BLEU"], [390, 391, "MethodName", "softmax"], [584, 585, "MetricName", "Score"], [620, 621, "MethodName", "softmax"]]}
{"text": "Recently , interest in diversity has increased ( Du and Black , 2019 ; Holtzman et al , 2019 ) , resulting in multiple proposals for its evaluation . We describe recent approaches , highlighting the need for a standard way to evaluate metrics . Perplexity is the standard metric in language modeling , measuring the proximity of a language model ( LM ) , P LM , to the true distribution , P ref , by approximating the cross - entropy H ( P ref , P LM ) with held - out data from P ref . Thus , perplexity captures to some extent diversity . For example , a dialog model that puts all probability mass on the output \" I do n't know \" for any given context will obtain infinite perplexity once it encounters any other response . This property makes perplexity popular in LM - based NLG models , and often it is the only reported measure for diversity ( Lewis et al , 2017 ; Fan et al , 2018 ; Wang et al , 2019 ; . However , perplexity does not purely measure diversity , and high perplexity does not entail low diversity . For example , a LM with a uniform distribution over the vocabulary for each decoded token has high diversity , but its perplexity will be extremely high , due to its low quality . Moreover , perplexity evaluates a LM , while the diversity of a NLG system is also strongly affected by the decoding procedure . For example , Top - k and nucleus sampling are popular decoding schemes that tradeoff quality and diversity by ignoring some of the LM probability mass ( Holtzman et al , 2019 ) . Last , some NLG models , such as Generative Adversarial Networks ( GANs ) ( Yu et al , 2017 ) are not language models . While one can approximate perplexity for such models ( Tevet et al , 2019 ) , ideally , a metric should not be tied to a model . N - gram - based metrics A popular metric is distinct n - grams ( Li et al , 2016b ) , which computes the proportion of unique n - grams out of the total number of n - grams in a set of generated sentences . Du\u0161ek et al ( 2020 ) calculated Shannon entropy ( Manning et al , 1999 ) based on different n - grams as a measure of lexical diversity . Self - BLEU ( Zhu et al , 2018 ; Shu et al , 2019 ) measures the BLEU score of a generated sentence with respect to another generated sentence ( rather than a gold reference ) . High average Self - BLEU indicates high similarity between generated sentences and low diversity . In 5 we expand this idea and suggest a reduction from any similarity metric to a diversity metric . By design , n - gram based metrics are sensitive to diversity in the form of language , rather than its meaning . Embedding - based metrics A new line of metrics suggests to embed generated sentences in latent space , then evaluate them in this space . Du and Black ( 2019 ) suggest to cluster the embedded sentences with k - means , then use its inertia as a measure for diversity . Recently , Lai et al ( 2020 ) suggested to consider the volume induced by the embedded sentences as a diversity metric . asked humans to evaluate the internal diversity of a generated essay . Ghandeharioun et al ( 2019 ) let crowdsourcing workers interact with a dialog chat - bot , then asked them to evaluate the diversity of a single conversation . In contrast , this paper focuses on the diversity of different responses given a context , as in Zhang et al ( 2019b ) .", "entities": [[45, 46, "MetricName", "Perplexity"], [101, 102, "MetricName", "perplexity"], [135, 136, "MetricName", "perplexity"], [146, 147, "MetricName", "perplexity"], [187, 188, "MetricName", "perplexity"], [196, 197, "MetricName", "perplexity"], [225, 226, "MetricName", "perplexity"], [239, 240, "MetricName", "perplexity"], [324, 325, "MetricName", "perplexity"], [426, 427, "MetricName", "BLEU"], [442, 444, "MetricName", "BLEU score"], [466, 467, "MetricName", "BLEU"]]}
{"text": "N - gram - based metrics We evaluate distinct ngrams ( distinct - n ) , as described in 2 . We also evaluate n - grams cosine similarity ( cos - sim ) : a similarity measure computing the cosine between the vectors representing two sentences , where each vector is a count vector over the n - grams that appear in the response . We use the reduction from 5 to convert this to a diversity measure . In both metrics , rather than choosing the order of the ngrams , we average over n { 1 , . . . , 5 } , which we found to outperform any single choice of n. Neural metrics We exploit existing BERT - based models ( Devlin et al , 2019 ) fine - tuned for estimating similarity between two sentences ( applying the reduction from 5 ) . BERT - STS ; A BERT model fine - tuned on Semantic Textual Similarity ( Cer et al , 2017 ) : a collection of sentence pairs annotated with scores from 1 - 5 denoting their semantic similarity . 3 BERT - Score ( Zhang et al , 2019a ) ; Originally a quality metric , BERT - Score uses BERT 's embeddings to measure similarity between two sen - tences . We used RoBERTa - large , as suggested by the authors . 4 Sentence - BERT ( sent - BERT ) ( Reimers and Gurevych , 2019 ) is a sentence - level embedding model based on BERT . We use the cosine similarity between the embeddings of two responses as a similarity metric . In our experiments we used bert - large - nli - stsb - mean - tokens . 5 Human Metrics We examine four methods for evaluating diversity with humans ( see 4 ) , to investigate best practices for obtaining diversity judgment from humans . In all metrics ( except ranking ) , ratings are from 5 ( highest diversity / similarity ) to 1 ( lowest ) . The original tasks presented to workers are in Appendix A. Absolute HDS ( absHDS ) ; Given a context c and a set of generated responses S c , rate the level of diversity of S c . Ranking HDS ( rnkHDS ) ; Given a context c and two sets S c , d 1 , S c , d 2 generated with different values of the diversity parameter d , rate which set is more diverse . Since this metric did not clearly outperform absHDS , we provide results in Appendix C only . Similarity HDS ( simHDS ) ; Given a context c and a set of generated responses S c , rate the similarity of each two sentences in S c , and then apply the reduction from 5 . Aspect HDS ( aspHDS ) ; Identical to absHDS , except we explicitly ask about a specific aspect of diversity , namely form and content . 6", "entities": [[122, 123, "MethodName", "BERT"], [150, 151, "MethodName", "BERT"], [152, 153, "TaskName", "STS"], [155, 156, "MethodName", "BERT"], [161, 164, "TaskName", "Semantic Textual Similarity"], [186, 188, "TaskName", "semantic similarity"], [190, 191, "MethodName", "BERT"], [192, 193, "MetricName", "Score"], [206, 207, "MethodName", "BERT"], [208, 209, "MetricName", "Score"], [210, 211, "MethodName", "BERT"], [224, 225, "MethodName", "RoBERTa"], [237, 238, "MethodName", "BERT"], [241, 242, "MethodName", "BERT"], [259, 260, "MethodName", "BERT"]]}
{"text": "In addition to Spearman 's \u03c1 , we report the optimal single - threshold classifier accuracy ( OCA ) , i.e. , the best achievable accuracy in predicting the class of a response set ( high or low content diversity ) for any threshold \u03b7 on m div , such that if m div ( S c ) > \u03b7 the classifier predicts high diversity , and otherwise predicts low diversity . Table 4 shows the results . N - gram - based metrics perform poorly , indicating they do not measure content diversity well . Neural models perform better than n - gram - based metrics ( especially sent - BERT ) , but there is still a clear gap between automatic metrics and humans . Figure 4 illustrates the typical distributions of n - gram , neural and human metrics . Clearly , HDS separates high and low content diversity better than neural metrics . In addition , n - gram - based metrics saturate both classes to near maximal values , similarly to decTest . Since conTest isolates content diversity , we used aspHDS to directly rate content and form diversity . Content aspHDS gets similar scores to ab - sHDS , suggesting little gain in asking directly on the tested aspect . Form aspHDS gets low scores compared to absHDS , validating that the form diversity of the two classes is similar . sets and 10 ratings per set for all experimentsthe minimal values in which results are confidently stable . Results are presented in Figure 5 .", "entities": [[15, 16, "MetricName", "accuracy"], [25, 26, "MetricName", "accuracy"], [111, 112, "MethodName", "BERT"]]}
{"text": "Comparing decTest results of storyGen to other tasks ( Table 2 ) , this task is characterised with noisier scores for all metrics ( Figures 3 and 6 ) , hence lower \u03c1 values and higher variance . A possible explanation is larger effect of c on the distribution P gen ( s | c ) in this task . Tables 3 , 6 and 7 , present decTest absolute scoring experiment using temperature , nucleus sampling and Top - k decoding parameters as d. Top - k consistently yields lower \u03c1 compared to other decoding parameters , especially for storyGen task . This implies that Top - k represents diversity less reliably than other methods . Ranking experiment To examine whether we can improve correlation by asking humans to rank diversity , rather than providing an absolute score , we designed a ranking version of decTest . Each context is given along with two sets ( 5 samples each ) , produced with different temperature values . We sweep over temperature differences instead of the absolute temperature values . The human metric in this setting is rnkHDS ( see 6.2 ) , and the automatic metrics are the difference between the scores each of the two sets got . We report two measures ; The first is Spearman 's \u03c1 between the metric and the temperature difference . The second is accuracy , i.e. , whether the metric can predict which set has higher temperature ( e.g. , in automatic metrics this is whether the sign of the temperature difference and the sign of metric score difference agree ) . 7 Table 5 summarizes the ranking test results . We observe that humans are better at ranking compared to giving absolute scores ( Table 2 ) , and are doing as well as automatic metrics . However , the scores of all automatic metrics also improve , making it difficult to separate between the different metrics .", "entities": [[232, 233, "MetricName", "accuracy"]]}
{"text": "As elaborated in 6 . tion between temperature differences and each metric score . Accuracy ( acc ) of classifying which set has the higher temperature . Standard deviation is up to 0.02 for all automatic metrics for both Spearman 's correlation and accuracy . metric to score zero correlation in conTest over this subset . The method of sub - sampling was meant to approximately equalize the distributions of the two classes , low and high content diversity , over the scores of distinct - n metric , and was performed as follows : Sort all collected samples ( from both low and high content diversity classes ) according to their distinct - n score . Divide the sorted samples to groups with fixed size ( 40 samples each in our case ) . From each such group , randomly sample the same amount of samples for each of the two classes . For example , if a group contains 5 low content diversity samples and 35 high content diversity samples , we can sample at most 5 samples for each class .", "entities": [[14, 15, "MetricName", "Accuracy"], [16, 17, "MetricName", "acc"], [43, 44, "MetricName", "accuracy"]]}
{"text": "Response set ( k = 3 ) Response set ( k = 32 ) Response set ( k = 318 ) Loud Noise . Jane was trying to take a nap . She heard a loud bang in the kitchen . It woke her up . A dish had fallen off the counter . Jane pulled over and started to clean herself . Jane was horrified and dropped her favorite food . Jane was able to finish her car and take a nap . Jane was able to finish her nap . Jane was able to finish her nap . Jane stopped at the store to buy a new one . Jane was able to grab a nap afterwards . Jane was unable to finish her nap since it was lost . Jane pulled over and started to clean up . Jane was able to finish her nap . Jane was able to catch a car using the seat . Jane stopped at the store to buy a new book . Jane was sad her cat dropped out of the kitchen . Jane screamed . Jane was horrified to find her car broken down on the floor . Jane was horrified and dropped her pay phone . Jane was easily able to grab a nap . Jane pulled over and started to cry . Jane pulled over and started to cry . Jane stopped at the store to buy a new dish from the store . Jane comes , noticed a lot of food left under it . Jane was horrified and dropped her book . Jane remembered to take a nap . Jane was since she took a nap while she waited for the refun Jane knew she had no time to finish her book . Jane was glad . Jane was annoyed and began to cry . Jane stopped at the store to buy a new one . Jane wanted to have her car back . Jane was monthed .", "entities": [[3, 5, "HyperparameterName", "k ="], [10, 12, "HyperparameterName", "k ="], [17, 19, "HyperparameterName", "k ="]]}
{"text": "Response set ( k = 3 ) Response set ( k = 32 ) Response set ( k = 318 ) watching curry play in his prime is truly a privilege i know i just do nt want him to play for us he has to be a good center for that he is a great center of football in his prime he s been playing in his prime for a long time he was a great back in the day he s been playing for a while now i do nt know about that he was pretty damn good at that i do nt think he was ever in his prime i do nt think he is a prime minister i do nt know why but i think he is a very good player and and his career as well agreed the way he s playing is awesome it has to be this is just called a job and then being on the field for the first time i do nt see him doing that often enough he just likes to party in the kitchen at this point he s going to be a great star for the rest of the only if he pays well the only thing that can make that kind of difference is how much time you yeah my feeling i mean we do nt like it but it happens all the you are one for real they still have a rule saying they might not be injured yet it really is a necessary thing to do finally some reason to continue watching him at some point yet that would be epic not to mention eating curry dinner is a privilege i just do nt want to turn over for this goal like he does in gt playing in his prime is truly a privilege ftfy so is saying he is in high school i m going this evening when she usually works i think you accidentally a word you are a good man i hope she works i m going to the same time as you when she usually works i am so sorry to hear that i hope she works for you i am so jealous of your work i am so jealous i hope you have fun i hope you get a job at a local bar i hope she works for you Response set ( \u03c4 = 0.25 ) Response set ( \u03c4 = 0.8 ) Response set ( \u03c4 = 1.1 ) Not the hacking . The hacking is the fact that the DNC was hacked . ! Not the hacking . The hacking is the real problem . The hacking is the ! Not the hacking of the DNC , but the leaks of the emails of the Democratic National Committee . ! Not the hacking , but the way it was done . The FBI 's investigation into the ! Not the hacking of the DNC , but the hacking of the emails of the Democratic National Committee . ! Not the hacking of the DNC , but the leaking of the emails . The DNC 's ! Not the hacking of the DNC . The hacking of the DNC was a \" false flag ! Not the hacking of the DNC . But the hacking of the RNC . The DNC hack ! Not the hacking . The hacking is the problem . The hacking is the problem ! Not the hacking of the DNC , but the leaking of the emails . The DNC was hacked , ! Not the hacking after all ? I 'm sure the nation - states that are involved in ! Not the hacking that happened on the internal networks of the Energy Department . In fact , according to ! Not the hacking of the American public but rather the fraudulent Heisenberg principle that seemed to be ! Not the hacking that took place in the DNC last year or the release of hacked emails during the ! Not the hacking futurists Cardboard inventor and self - described tinkerer Dennis ! Not the hacking alone . In the first half of the report , the hackers tried to create fake ! ( Top - k ) . Bold text is the 3 - words prompt context .", "entities": [[3, 5, "HyperparameterName", "k ="], [10, 12, "HyperparameterName", "k ="], [17, 19, "HyperparameterName", "k ="]]}
{"text": "Recent works have shown that supervised models often exploit data artifacts to achieve good test scores while their performance severely degrades on samples outside their training distribution . Contrast sets quantify this phenomenon by perturbing test samples in a minimal way such that the output label is modified . While most contrast sets were created manually , requiring intensive annotation effort , we present a novel method which leverages rich semantic input representation to automatically generate contrast sets for the visual question answering task . Our method computes the answer of perturbed questions , thus vastly reducing annotation cost and enabling thorough evaluation of models ' performance on various semantic aspects ( e.g. , spatial or relational reasoning ) . We demonstrate the effectiveness of our approach on the popular GQA dataset ( Hudson and Manning , 2019 ) and its semantic scene graph image representation . We find that , despite GQA 's compositionality and carefully balanced label distribution , two strong models drop 13 - 17 % in accuracy on our automatically - constructed contrast set compared to the original validation set . Finally , we show that our method can be applied to the training set to mitigate the degradation in performance , opening the door to more robust models . 1", "entities": [[80, 83, "DatasetName", "visual question answering"], [116, 118, "TaskName", "relational reasoning"], [130, 131, "DatasetName", "GQA"], [152, 153, "DatasetName", "GQA"], [170, 171, "MetricName", "accuracy"]]}
{"text": "We design a perturbation method which guarantees a change in the gold answer for each question template . For example , looking at Fig . 2 , for the question template are there X near the Y ? ( e.g. , \" Is there any fence near the players ? \" ) , we replace either X or Y with a probable distractor ( e.g. \" replace \" fence \" with \" trees \" ) . We use the scene graph to ensure that the answer to the question is indeed changed . In our example , this would entail grounding \" players \" in the question to the scene graph ( either via exact match or several other heuristics such as hard - coded lists of synonyms or co - hyponyms ) , locating its neighbors , and verifying that none of them are \" trees . \" We then apply heuristics to fix syntax ( e.g. , changing from singular to plural determiner , see Appendix A.3 ) , and verify that the perturbed sample Are there any cats near the boat ? Is there any bush near the boat ? Is the X Rel the Y ? Is the boy to the right of the man ? Is the boy to the left of the man ? Is the X Rel the Y ? Is the boy to the right of the man ? Is the zebra to the right of the man ? does not already exist in GQA . The specific perturbation is performed per question template . In question templates with two objects ( X and Y ) , we replace X with X ' , such that X ' is correlated with Y in other GQA scene graphs . In question templates with a single object X , we replace X with a textually - similar X ' . For example in the first row in Table 1 we replace dishwasher with dishes . Our perturbation code is publicly available . This process may yield an arbitrarily large number of contrasting samples per question , as there are many candidates for replacing objects participating in questions . We report experiments with up to 1 , 3 and 5 contrasting samples per question . Illustrating the perturbation process . Looking at Fig . 1 , we see the scene - graph information : objects have bounding - boxes around them in the image ( e.g. , zebra ) ; Objects have attributes ( wood is an attribute of the fence object ) ; and there are relationships between the objects ( the puddle is to the right of the zebra , and it is near the fence ) . The original ( question , answer ) pair is ( \" is there a fence near the puddle ? \" , \" Yes \" ) . We first identify the question template by regular expressions : \" Is there X near the Y \" , and isolate X = fence , Y = puddle . The answer is \" Yes \" , so we know that X is indeed near Y. We then use the existing information given in the scene - graph . We search for X ' that is not near Y. To achieve this , we sample a random object ( wall ) , and verify that it does n't exist in the set of scenegraph objects . This results in a perturbed example \" Is there a wall near the puddle ? \" , and now the ground truth is computed to be \" No \" . Consider a different example : ( \" Is the puddle to the left of the zebra ? \" , \" Yes \" ) . We identify the question template \" Is the X Rel the Y \" , where X = puddle , Rel = to the left , Y = zebra . The answer is \" Yes \" . Now we can easily change Rel'=to the right , resulting in the ( question , answer ) pair ( \" Is the puddle to the right of the zebra ? \" , \" No \" ) . We highlight the following : ( 1 ) This process is done entirely automatically ( we validate it in Section 2.3 ) ; ( 2 ) The answer is deterministic given the information in the scene - graph ; ( 3 ) We do not produce unanswerable questions . If we could n't find an alternative atom for which the presuppositions hold , we do not create the perturbed ( question , answer ) pair ; ( 4 ) Grounding objects from the question to the scene - graph can be tricky . It can involve exact match , number match ( dogs in the question , and dog in the scene - graph ) , hyponyms ( animal in the question , and dog in the scene - graph ) , and synonyms ( motorbike in the question , and motorcycle in the scene - graph ) . The details are in the published code ; ( 5 ) The only difference between the original and the perturbed instance is a single atom : an object , relationship , or attribute .", "entities": [[114, 116, "MetricName", "exact match"], [252, 253, "DatasetName", "GQA"], [292, 293, "DatasetName", "GQA"], [799, 801, "MetricName", "exact match"]]}
{"text": "We experiment with two top - performing GQA models , MAC ( Hudson and Manning , 2018 ) and LXMERT ( Tan and Bansal , 2019 ) , 3 to test their generalization on our automatic contrast sets , leading to various key observations . Models struggle with our contrast set . had the smallest performance drop , potentially because the models performance on such multi - class , subjective questions is relatively low to begin with . Training on perturbed set leads to more robust models . Previous works tried to mitigate spurious datasets biases by explicitly balancing labels during dataset construction ( Goyal et al , 2017 ; Zhu et al , 2016 ; Zhang et al , 2016 ) or using adversarial filtering ( Zellers et al , 2018 ( Zellers et al , , 2019 . In this work we take an inoculation approach ( Liu et al , 2019 ) This allows us to measure the contrast consistency of our contrast set , defined as the percentage of the contrast sets for which a model 's predictions are correct for all examples in the set ( including the original example ) . For example , in Fig . 1 the set size is 4 , and only 2/4 predictions are correct . We experiment with 1 , 3 , and 5 augmentations per question with the LXMERT model trained on the original GQA training set . Our results ( Table 4 ) show that sampling more objects leads to similar accuracy levels for the LXMERT model , indicating that quality of our contrast sets does not depend on the specific selection of replacements . However , we observe that consistency drops fast as the size of the contrast sets per QA instance grows , indicating that model success on a specific instance does not mean it can generalize robustly to perturbations .", "entities": [[7, 8, "DatasetName", "GQA"], [19, 20, "MethodName", "LXMERT"], [231, 232, "MethodName", "LXMERT"], [237, 238, "DatasetName", "GQA"], [255, 256, "MetricName", "accuracy"], [259, 260, "MethodName", "LXMERT"]]}
{"text": "Neural methods for abstractive summarization ( Rush et al , 2015 ; Nallapati et al , 2016 ; Lewis et al , 2020 ; formulate summarization as a sequenceto - sequence ( Seq2Seq ) problem ( Sutskever et al , 2014 ) , learning to generate the summary in an autoregressive manner . Such models are commonly trained with maximum likelihood estimation ( MLE ) , maximizing predictive probability of the reference output given the gold sub - sequence before it . However , during inference the model must also generate the output based on possibly erroneous previous steps . This can hurt model performance , a phenomenon often called exposure bias ( Bengio et al , 2015 ; Ranzato et al , 2016 ) . To maintain reasonable performance even in the case of a sub - sequence with errors , we argue that the The candidate summaries are generated by a pre - trained model ( BART ) , and we select the best and the worst candidates ( w.r.t . ROUGE scores ) for each of the samples . High and Low represent the average performance of the best and worst candidates respectively . R - 1/2 / L are the ROUGE - 1/2 / L scores . The original BART only achieves 54.80 % accuracy . model must accurately estimate relative quality of different generated outputs , since effective inference requires comparison among these candidates . To understand whether existing models can accurately perform such relative comparisons , we conducted a preliminary study on pre - trained BART ( Lewis et al , 2020 ) , first generating two candidate summaries from the model and observing whether a higher probability is assigned to the candidate with a higher ROUGE ( Lin , 2004 ) score . As Tab . 1 shows , the accuracy is far from ideal . This is likely due to the fact that MLE training only encourages the model to assign high probability to the reference summary , and is agnostic about any relative comparison between non - reference summaries . However , we argue that it is also important for the order of model scores to be coordinated with the actual quality metrics by which the summaries will be evaluated - higher model scores should indicate better quality summaries . In the following we will refer to models that have such scores as \" coordinated \" for conciseness . We introduce a training paradigm which requires the abstractive model to be able to be accurate with respect to predicting the tokens in the reference summaries and coordinated with respect to Figure 1 : Comparison of MLE loss ( LMLE ) and the contrastive loss ( LCtr ) in our method . MLE assumes a deterministic ( one - point ) distribution , in which the reference summary receives all the probability mass . Our method assumes a nondeterministic distribution in which system - generated summaries also receive probability mass according to their quality . The contrastive loss encourages the order of model - predicted probabilities of candidate summaries to be coordinated with the actual quality metric M by which the summaries will be evaluated . We assign the abstractive model a dual role - a single model could be used both as a generation model and a reference - free evaluation model . the candidate summaries . In other words , we give the abstractive model a dual role : as a generation model , it generates the output summaries in an autoregressive way ; as an evaluation model , it can be used to score the quality of candidate summaries by estimating a probability distribution over candidate outputs . The generation model is trained using the standard MLE loss , but to train the evaluation model we introduce a contrastive loss ( Hadsell et al , 2006 ) defined over different candidate summaries generated by pre - trained abstractive models ( Fig . 1 ) , following previous work on ranking - based or contrastive learning ( Hopkins and May , 2011 ; Zhong et al , 2020 ; Liu et al , 2021b ) . Our main contribution is to change the target distribution of abstractive models from a one - point deterministic distribution assumed by MLE training to a non - deterministic distribution in which candidate summaries are also assigned probability mass according to their quality . The new SOTA performance on CNN / DailyMail ( Hermann et al , 2015 ) and XSum ( Narayan et al , 2018 ) datasets demonstrated the effectiveness of our method . Our in - depth analysis also found that the abstractive models trained using our method can estimate the candidate summary quality more accurately , in concert with the the objective of our training paradigm .", "entities": [[4, 5, "TaskName", "summarization"], [25, 26, "TaskName", "summarization"], [32, 33, "MethodName", "Seq2Seq"], [158, 159, "MethodName", "BART"], [213, 214, "MethodName", "BART"], [218, 219, "MetricName", "accuracy"], [261, 262, "MethodName", "BART"], [307, 308, "MetricName", "accuracy"], [445, 446, "MetricName", "loss"], [452, 453, "MetricName", "loss"], [505, 506, "MetricName", "loss"], [628, 629, "MetricName", "loss"], [640, 641, "MetricName", "loss"], [674, 676, "MethodName", "contrastive learning"], [755, 756, "DatasetName", "XSum"]]}
{"text": "The goal of abstractive summarization is to create a function g that takes a source document D and generates an appropriate summary S S g ( D ) ( 1 ) Training Objective Neural abstractive summarization models aim to learn a neural model g that results in good summaries . Maximum likelihood estimation ( MLE ) is the standard training algorithm . It aims to maximize the likelihood of the reference summary S * , i.e. , \u03b8 * = argmax \u03b8 i log p g \u03b8 ( S * ( i ) | D ( i ) ; \u03b8 ) ( 2 ) where \u03b8 denotes the parameters of g and p g \u03b8 denotes the probability distribution entailed by these parameters . The summation is over the training set and { D ( i ) , S * ( i ) } is the i - th training sample . For a specific sample { D ( i ) , S * ( i ) } , Eq . 2 is equivalent to minimizing the sum of negative loglikelihoods of the tokens { s * 1 , , s * j , , s * l } in the reference summary S * whose length is l , which is the cross - entropy loss : Lxent = \u2212 l j=1 s ptrue ( s | D , S * < j ) log pg \u03b8 ( s | D , S * < j ; \u03b8 ) ( 3 ) where S * < j denotes the partial reference sequence { s * 0 , , s * j\u22121 } and s * 0 is a pre - defined start token . p true is a one - hot distribution under the standard MLE framework : ptrue ( s | D , S * < j ) = 1 s = s * j 0 s = s * j ( 4 ) In practice , label smoothing ( Szegedy et al , 2016 ) is a widely used and effective technique that modifies the target distribution in Eq . 4 to a \" soft \" label by assigning probability mass \u03b2 to other tokens : ptrue ( s | D , S * < j ) = 1 \u2212 \u03b2 s = s * j \u03b2 N \u22121 s = s * j ( 5 ) where N is the size of the dictionary . Inference and Exposure Bias During inference , the abstractive model g is used to generate the candidate summary in an autoregressive manner . It is intractable to enumerate all the possible candidate outputs , so in practice methods such as beam search are used to reduce the search space . One important step in search is estimating the probability of the next word s t given the previous predicted sequence S < t : p g \u03b8 ( s t | D , S < t ; \u03b8 ) ( 6 ) Comparing Eq . 6 with Eq . 3 , the major difference is that during inference the model makes new predictions based on its own previous predictions S < t instead of the reference S * < t . As a result , even if the generation model g achieves very high accuracy w.r.t . Eq . 3 , once S < t starts to deviate from S * , there is the risk that the performance of g will significantly degrade . This problem has been identified as the exposure bias ( Bengio et al , 2015 ) .", "entities": [[4, 5, "TaskName", "summarization"], [35, 36, "TaskName", "summarization"], [77, 78, "HyperparameterName", "\u03b8"], [81, 82, "HyperparameterName", "\u03b8"], [86, 87, "HyperparameterName", "\u03b8"], [99, 100, "HyperparameterName", "\u03b8"], [105, 106, "HyperparameterName", "\u03b8"], [114, 115, "HyperparameterName", "\u03b8"], [216, 217, "MetricName", "loss"], [237, 238, "HyperparameterName", "\u03b8"], [248, 249, "HyperparameterName", "\u03b8"], [266, 267, "DatasetName", "0"], [276, 277, "DatasetName", "0"], [317, 318, "DatasetName", "0"], [329, 331, "MethodName", "label smoothing"], [364, 365, "HyperparameterName", "\u03b2"], [383, 384, "HyperparameterName", "\u03b2"], [389, 390, "HyperparameterName", "\u03b2"], [485, 486, "HyperparameterName", "\u03b8"], [496, 497, "HyperparameterName", "\u03b8"], [553, 554, "MetricName", "accuracy"]]}
{"text": "Eq . 6 implies that the abstractive model g should be able to assign higher estimated probability to the better candidate summary during inference . However , this intuition is not directly captured in the standard MLE objective used in training - a model obtaining zero MLE loss would assign zero probability to any candidate summary different from the reference . This is obviously improper for any task where multiple reasonable generations may exist ( Khayrallah et al , 2020 ) , and also does not say anything about the ordering of two imperfect references . We therefore advocate for making the alternative assumption that the probability of one candidate should be well - correlated with its quality as evaluated by an automatic metric M . Since it is intractable to enumerate all the possible candidate outputs , we only require our model to be able to accurately predict the ranking order of a set of the most probable candidate summaries\u015c , which are its own beam search results . In order to achieve this objective , we slightly modify the conditions of Eq . 5 , maintaining the general functional form , but instead specifying the marginal probability of the non - reference candidates S to be \u03b2 , and encouraging coordination of probabilities and qualities among non - reference candidates as follows : \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 p true \u2020 ( S | D ) = 1 \u2212 \u03b2 S = S * S S p true \u2020 ( S | D ) = \u03b2 S = S * p true \u2020 ( Si | D ) > p true \u2020 ( Sj | D ) \u2200Si , Sj \u015c , M ( Si ) > M ( Sj ) ( 7 ) We next describe precisely how we encourage coordination through contrastive learning .", "entities": [[47, 48, "MetricName", "loss"], [208, 209, "HyperparameterName", "\u03b2"], [244, 245, "HyperparameterName", "\u03b2"], [260, 261, "HyperparameterName", "\u03b2"], [308, 310, "MethodName", "contrastive learning"]]}
{"text": "The candidate quality measure M can be defined in many ways . In this work we define it as the ROUGE ( Lin , 2004 ) score of a candidate summary S i given the reference summary S * . To coordinate a pre - trained abstractive model , we 1 ) use it to generate different candidate summaries with various levels of quality , 2 then 2 ) encourage the model to assign higher estimated probabilities to better candidates by fine - tuning the model with a contrastive loss , following the previous work ( Hopkins and May , 2011 ; Zhong et al , 2020 ) : Lctr = i j > i max ( 0 , f ( Sj ) \u2212 f ( Si ) + \u03bbij ) ( 8 ) where S i and S j are two different candidate summaries and ROUGE ( S i , S * ) > ROUGE ( S j , S * ) , \u2200i , j , i < j. \u03bb ij is the margin multiplied by the difference in rank between the candidates , i.e. , \u03bb ij = ( j \u2212 i ) * \u03bb . f ( S i ) is the length - normalized estimated log - probability 3 f ( S ) = l t=1 log p g \u03b8 ( s t | D , S < t ; \u03b8 ) | S | \u03b1 ( 9 ) where \u03b1 is the length penalty hyperparameter . This loss gives the abstractive model a dual purpose , first as a reference - free evaluation model , which can be used in a two - stage summarization pipeline , where it is used to score the candidates generated by a pre - trained generation model and select the final output from them . However , since the autoregressive generation depends on both the token - level prediction accuracy and sequencelevel coordination , the model fine - tuned with the contrastive loss alone can no longer be used as a generation model . Multi - task Fine - tuning Following Edunov et al ( 2018 ) , we combine the contrastive ( Eq . 8 ) and cross - entropy ( Eq . 3 ) losses to preserve the generation ability of the pre - trained abstractive model : L mul = L xent + \u03b3L ctr ( 10 ) where \u03b3 is the weight of the contrastive loss . We note that the contrastive and the cross - entropy loss can effectively complement each other - since the contrastive loss is defined on the sequence level , the token - level cross - entropy loss serves as a normalization to ensure that the model could assign balanced probability mass across the whole sequence .", "entities": [[89, 90, "MetricName", "loss"], [117, 118, "DatasetName", "0"], [224, 225, "HyperparameterName", "\u03b8"], [235, 236, "HyperparameterName", "\u03b8"], [240, 241, "HyperparameterName", "\u03b1"], [245, 246, "HyperparameterName", "\u03b1"], [253, 254, "MetricName", "loss"], [280, 281, "TaskName", "summarization"], [321, 322, "MetricName", "accuracy"], [334, 335, "MetricName", "loss"], [404, 405, "HyperparameterName", "\u03b3"], [411, 412, "MetricName", "loss"], [423, 424, "MetricName", "loss"], [433, 434, "MetricName", "loss"], [448, 449, "MetricName", "loss"]]}
{"text": "Training Methods of Seq2Seq Models In order to align the training objective and evaluation metric , structured losses have been used for the Seq2Seq model training . Among them , marginbased losses ( Herbrich et al , 1999 ; Taskar et al , 2004 ; Gimpel and Smith , 2010 ) , which require the model to assign higher probability to the better output , are a major category . Many margin - based losses used in modern seq2seq models ( Wiseman and Rush , 2016 ; Edunov et al , 2018 ) assume a deterministic ( one - point ) distribution : a model can achieve zero loss if it can assign a much higher probability to the ( pseudo ) - reference , regardless of relative comparisons of other candidate summaries . By contrast , our method has a non - deterministic assumption ( Eq . 7 ) , which focuses on the pair - wise ranking of a set of candidate summaries . One main challenge of directly optimizing a Seq2Seq model with quality scores of the output is that the discrete sampling process makes the loss non - differentiable . To circumvent this problem , reinforcement learning has been used to reformulate the conditional text generation tasks ( Ranzato et al , 2016 ; Bahdanau et al , 2016 ; Li et al , 2016 ; Paulus et al , 2018 ; Li et al , 2019 ) . Compared to this school of methods , our method is based on supervised learning , and it is more stable and less sensitive to the design choices ( e.g. reward shaping ) , which are well - known challenges of reinforcement learning methods . Minimum risk training ( Shen et al , 2016 ; Wieting et al , 2019 ) and other online sampling based methods ( Bengio et al , 2015 ; Norouzi et al , 2016 ; Zhang et al , 2019 ) belong to another school of methods used to circumvent the problem of non - differentiability . However , they also exhibit similar problems of stability as reinforcement learning . Contrastive Learning Recently , contrastive learning ( Hadsell et al , 2006 ) has been introduced into several conditional text generation tasks , such as machine translation Pan et al , 2021 ) , text summarization ( Cao and Wang , 2021 ; Xu et al , 2021 ; Sun and Li , 2021 ) , and other tasks ( Uehara et al , 2020 ; Cho et al , 2021 ; Lee et al , 2021b ) . Among these application scenarios , most work deployed contrastive learning in the latent representation space , following the framework proposed in . However , in this work we adopt contrastive learning over the discrete space of the generated texts . Besides , instead of constructing the contrastive learning examples by rule - based methods ( e.g. perturbing the reference output ) , we use the generation models to construct the examples , which makes the contrastive learning task closer to the generation task . Sun and Li ( 2021 ) also adopted contrastive learning on the generated texts . However , their formulation belongs to the margin - based losses . We have discussed the difference between our method and the margin - based losses in the previous paragraphs . Discriminative Reranking Discriminative reranking has been widely studied for conditional generation tasks Wan et al , 2015 ; Mizumoto and Matsumoto , 2016 ) . Some recent works Lee et al , 2021a ) have also explored discriminative reranking of candidates from neural natural language generation models , which adopt large pre - trained language models ( e.g. BERT ( Devlin et al , 2019 ) ) as the reranker . In this work , we factorize the Seq2Seq model ( e.g. , BART ) trained on the same dataset as the reranking model , which maximizes the parameter sharing across two stages . Besides , our approach contributes an instance of leveraging large pre - trained Seq2Seq models as a quality estimation model ( Yuan et al , 2021 ) .", "entities": [[3, 4, "MethodName", "Seq2Seq"], [23, 24, "MethodName", "Seq2Seq"], [78, 79, "MethodName", "seq2seq"], [108, 109, "MetricName", "loss"], [173, 174, "MethodName", "Seq2Seq"], [189, 190, "MetricName", "loss"], [207, 210, "TaskName", "conditional text generation"], [357, 359, "MethodName", "Contrastive Learning"], [361, 363, "MethodName", "contrastive learning"], [375, 378, "TaskName", "conditional text generation"], [382, 384, "TaskName", "machine translation"], [391, 393, "TaskName", "text summarization"], [444, 446, "MethodName", "contrastive learning"], [465, 467, "MethodName", "contrastive learning"], [482, 484, "MethodName", "contrastive learning"], [511, 513, "MethodName", "contrastive learning"], [528, 530, "MethodName", "contrastive learning"], [624, 625, "MethodName", "BERT"], [644, 645, "MethodName", "Seq2Seq"], [649, 650, "MethodName", "BART"], [683, 684, "MethodName", "Seq2Seq"]]}
{"text": "Datasets We mainly use three datasets in our experiments ( statistics in Appendix A ) . CNNDM 4 ( Hermann et al , 2015 ) is a large scale news dataset . Following Nallapati et al ( 2016 ) , we treat the news articles as the source documents and the associated highlights as the summaries . XSum 5 ( Narayan et al , 2018 ) is a highly abstractive dataset of articles from the British Broadcasting Corporation ( BBC ) . NYT 6 ( Sandhaus , 2008 ) contains articles from the New York Times and the associated summaries . We follow Kedzie et al ( 2018 ) for data preprocessing and splitting , and use the associated archival abstracts as the summaries . Baselines We choose a variety of related models with strong performance as baselines . BART ( Lewis et al , 2020 ) and PEGASUS are both large pre - trained Seq2Seq LMs standard in the literature . GSum ( Dou et al , 2021 ) is built on BART , and improves performance by using additional guidance from an extractive summarizer . SimCLS introduces a two - stage framework where the pre - trained BART model is used to generate candidates and a pre - trained RoBERTa model is fine - tuned as an evaluation model to score the candidate summaries and select from them . It achieves state - of - the - art performance on both CNNDM and XSum . GOLD ( Pang and He , 2021 ) uses offline reinforcement learning to train the BART model by treating the reference summaries as the demonstrations , a different formulation that can also improve the performance of the original BART . SeqCo ( Xu et al , 2021 ) and ConSum ( Sun and Li , 2021 ) are two recent methods that aim to leverage contrastive learning to improve the performance of the abstractive summarization model ( BART ) . Implementation Details In the following experiments , we use either BART or PEGASUS as a backbone . We label our proposed methods BRIO , with two variants : ( 1 ) BRIO - Ctr is fine - tuned with the contrastive loss ( Eq . 8 ) only ; ( 2 ) BRIO - Mul is fine - tuned with the multi - task loss ( Eq . 10 ) . We use BRIO - Ctr as an evaluation model that scores different candidate summaries generated by a Seq2Seq abstractive model and selects the final output from them , and BRIO - Mul as a standard Seq2Seq model that takes the source documents as input and generates the output in an autoregressive manner . Further details are in Appendix B.", "entities": [[57, 58, "DatasetName", "XSum"], [139, 140, "MethodName", "BART"], [155, 156, "MethodName", "Seq2Seq"], [173, 174, "MethodName", "BART"], [199, 200, "MethodName", "BART"], [211, 212, "MethodName", "RoBERTa"], [245, 246, "DatasetName", "XSum"], [262, 263, "MethodName", "BART"], [285, 286, "MethodName", "BART"], [312, 314, "MethodName", "contrastive learning"], [321, 322, "TaskName", "summarization"], [324, 325, "MethodName", "BART"], [337, 338, "MethodName", "BART"], [368, 369, "MetricName", "loss"], [391, 392, "MetricName", "loss"], [415, 416, "MethodName", "Seq2Seq"], [433, 434, "MethodName", "Seq2Seq"]]}
{"text": "We further perform some in - depth analyses from diverse perspectives on the CNNDM dataset to gain more insights into our proposed method . Table 3 : Model performance with different \u03b3 coefficients weighting the contrastive loss ( Eq . 10 ) on CNNDM . BRIO - Ctr is trained with the contrastive loss only , which no longer preserves its generation ability . We report its performance when it is used as an evaluation model to select from candidate summaries . R - 1/2 / L are the ROUGE - 1/2 / L F1 scores . Coefficients of the Multi - Task Loss The multitask loss ( Eq . 10 ) used to train our model contains two parts : the cross - entropy loss and the contastive loss . As shown in Tab . 3 , as the weight of the contrastive loss ( \u03b3 ) increases , the model 's performance improves . However , the cross - entropy loss is still necessary to preserve the model 's ability as a generation model . We argue that this is because the token level accuracy is still important during the autoregressive generation process , where the individual tokens are predicted sequentially . In addition , we also found that the model tends to achieve the best performance ( w.r.t the ROUGE scores on the development set ) faster with a higher \u03b3 . Specifically , it requires less than one entire epoch to achieve the best performance on CNNDM , making our approach an efficient fine - tuning method . Coefficient ( \u03b3 ) R - 1 R - 2 R - L 0 ( Generation - Finetuning as a Loop Since the fine - tuned model ( BRIO - Mul ) is still able to gen - ( Stahlberg and Byrne , 2019 ) , and the generator may not be able to differentiate them from high - quality candidates . In Tab . 5 , we compare the performance of the pre - trained BART and our model ( BRIO - Mul ) with different beam widths used during inference . We observe that the performance of BART goes down as the beam width increases . On the other hand , our model is able to achieve better performance with a larger number of beams , demonstrating that our training method can improve the coordination of the model by encouraging the model to assign estimated probabilities to candidate summaries wellcorrelated with their quality . Training with Different Evaluation Metrics In the previous experiments , we used ROUGE as the evaluation metric to define the target ordering of the candidate summaries ( Eq.7 ) . To evaluate our method 's performance beyond ROUGE , we use a model - based semantic similarity metric , BERTScore ( Zhang * et al , 2020 ) , 7 as the evaluation metric M in Eq.7 to compare the performance of different candidate summaries . Then , we trained another version of BRIO - Mul based on the order of candidate summaries calculated by BERTScore . Beams BART BRIO - Mul R - 1 R - 2 R - 1 R - The results in Tab . 6 show that ( 1 ) Our model can significantly improve the model performance when either ROUGE or BERTScore is used as the target evaluation metric for ordering candidate summaries . This suggests that it is possible to use our method to optimize any specific target metric , making our method an alternative to reinforcement learning or minimum risk training . ( 2 ) Our model that is trained on one evaluation metric ( e.g. BERTScore ) also achieves improvement on another metric ( e.g. ROUGE ) compared with the baseline model , which indicates that the improvement made by our model is not from exploiting the potential weaknesses of individual metrics . Besides , this result also demonstrates a non - trivial degree of agreement between ROUGE and BERTScore . Novel n - grams We compare the ratio of novel n - grams in reference , BRIO - Mul 's , and BART 's summaries . As Tab . 7 shows , our model is more \" abstractive \" compared to BART , although reference summaries still contain more novel n - grams . This is likely due to the fact that our model is optimized at the sequence - level , allowing more freedom for paraphrasing and compression . We further investigate the relation of the \" abstractiveness \" and model performance by com - 7 https://github.com/Tiiiger/bert_score . We use its default version for English texts . paring our model ( BRIO - Mul ) with the baseline model ( BART ) on different buckets of test examples grouped by the \" novelty \" of the reference summaries , 8 i.e. , Novelty ( D , S * ) = g G S * 1 ( g / GD ) | GS * | ( 11 ) where D and S * are the source document and reference summary respectively , G D and G S * are the sets of bigrams in D and S * , 1 is the indicator function . The results in Fig . 3 show that when novelty is higher , ( 1 ) all models ' performance decreases ; ( 2 ) our model achieves larger improvement over the baseline model . Rank Correlation We computed the rank correlation between the estimated probabilities of the candidate summaries calculated by the generators and the quality scores of the candidate summaries . We use Eq . 9 to calculate the estimated probabilities 9 and we use ROUGE - 1 as the quality score metric of the candidate summaries . We calculate Spearman 's rank correlation for each sample , and use the average score as the overall correlation , We investigated two specific settings : 1 ) ranking candidate summaries generated by a different model ( PEGASUS ) ; 2 ) ranking candidate summaries generated by themselves ( BART & BRIO - Mul ) . We use 16 candidates in total for calculation . As Tab . 8 shows , our model achieves better rank correlation on the candidate summaries generated by both itself and the independent model . This suggests that our model can better estimate the quality of candidate summaries .", "entities": [[31, 32, "HyperparameterName", "\u03b3"], [36, 37, "MetricName", "loss"], [53, 54, "MetricName", "loss"], [94, 95, "MetricName", "F1"], [106, 107, "MetricName", "loss"], [125, 126, "MetricName", "loss"], [129, 130, "MetricName", "loss"], [144, 145, "MetricName", "loss"], [146, 147, "HyperparameterName", "\u03b3"], [162, 163, "MetricName", "loss"], [186, 187, "MetricName", "accuracy"], [233, 234, "HyperparameterName", "\u03b3"], [264, 265, "HyperparameterName", "\u03b3"], [275, 276, "DatasetName", "0"], [338, 339, "MethodName", "BART"], [361, 362, "MethodName", "BART"], [463, 465, "TaskName", "semantic similarity"], [516, 517, "MethodName", "BART"], [689, 690, "MethodName", "BART"], [708, 709, "MethodName", "BART"], [788, 789, "MethodName", "BART"], [826, 827, "DatasetName", "GD"], [1011, 1012, "MethodName", "BART"]]}
{"text": "Calibration requires that a model 's confidence on its predictions is equal to the accuracy of these predictions ( Guo et al , 2017 ) . Previous work ( M\u00fcller et al , 2019 ; Kumar and Sarawagi , 2019 ; has found that a more calibrated text generation model tends to have better performance , and techniques like label smoothing can improve both the token - level calibration and sequence - level accuracy ( i.e. the ability of generating better results ) . One intuitive explanation of this phenomenon is to interpret the model 's estimated probability of a generated summary as the product of the model 's confidences on a series of tokenlevel predictions . Then , since a more calibrated model 's confidence estimates better the accuracy of its predictions , the model 's estimated probability of one sequence should be more indicative of the quality of this sequence , which is essential for the beam search during inference . However , the relation of token - level calibration and sequencelevel performance remains inconclusive ( M\u00fcller et al , 2019 ) . 10 For example , a generator that always predicts a uniform distribution over all tokens would be perfectly calibrated , however , such a model would not generate high - quality outputs . We investigate this relation from the opposite direction by evaluating whether our model ( BRIO - Mul ) , which is trained to have better sequencelevel performance , would also be more calibrated at the token - level compared with the baseline models that are trained using MLE and label smoothing . We follow previous work by using the Expected Calibration Error ( Naeini et al , 2015 ) ( ECE ) as the evaluation metric of calibration : ECE = M m=1 | B m | n | acc ( B m ) \u2212 conf ( B m ) | ( 12 ) where the samples are grouped into M equal - width buckets by confidence ( conf ) , B m denotes the m - th bucket , and n is the total number of samples . Following , we evaluate model calibration on the system - generated summaries during inference and use the tercom toolkit 11 to assign labels ( correct / incorrect ) to the system - generated summaries based on the reference summaries . The results in Tab . 9 show that BRIO - Mul is better calibrated compared to BART , suggesting that our method helps to improve the token - level calibration by explicitly encouraging the model to have more accurate sequence - level probability estimations . The reliability graph is shown in Fig . 4 . We found that ( 1 ) abstractive models are generally over - confident on their own predictions , ( 2 ) models are generally more calibrated on XSum than CNNDM . This is likely due to the fact that XSum has shorter summaries therefore it is less likely to be affected by the exposure bias .", "entities": [[14, 15, "MetricName", "accuracy"], [35, 36, "DatasetName", "Kumar"], [47, 49, "TaskName", "text generation"], [59, 61, "MethodName", "label smoothing"], [73, 74, "MetricName", "accuracy"], [129, 130, "MetricName", "accuracy"], [267, 269, "MethodName", "label smoothing"], [279, 280, "MetricName", "Error"], [307, 308, "MetricName", "acc"], [353, 356, "HyperparameterName", "number of samples"], [413, 414, "MethodName", "BART"], [479, 480, "DatasetName", "XSum"], [491, 492, "DatasetName", "XSum"]]}
{"text": "We use diverse beam search ( Vijayakumar et al , 2018 ) where warmup denotes the warmup steps , which is set to 10000 , step is the number of updating steps , lr is the learning rate . We set the length penalty factor \u03b1 in the scoring function ( Eq . 9 ) to the same value as used in the original beam search . We search the value of the margin \u03bb in the contrastive loss ( Eq . 8 ) within the range [ 1 \u00d7 10 \u22125 , 1 ] , and decide the value based on the model performance on the validation set . We also performed extensive search for the coefficient \u03b3 in Eq . 10 . The specific hyper - parameter setting is reported in Tab . 13 . We use the standard ROUGE ( Lin , 2004 ) Perl package 15 for evaluation . The command line parameters are ' - c 95 - r 1000 - n 2 - m ' . Before the 12 The checkpoint is \" facebook / bart - large - cnn \" , containing around 400 M parameters . 13 The checkpoint is \" google / pegasus - xsum \" \" containing around 568 M parameters . 14 The checkpoint is \" facebook / bart - large \" . 15 https://github.com/summanlp/evaluation/tree/master/ ROUGE - RELEASE - 1.5.5 Datasets \u03bb ( Eq . 8 ) \u03b1 ( Eq . 9 ) \u03b3 ( Eq . 10 ) CNNDM 0.001 2.0 100 XSum 0.1 0.6 100 NYT 0.001 2.0 100", "entities": [[36, 38, "HyperparameterName", "learning rate"], [45, 46, "HyperparameterName", "\u03b1"], [78, 79, "MetricName", "loss"], [118, 119, "HyperparameterName", "\u03b3"], [203, 204, "DatasetName", "xsum"], [238, 239, "HyperparameterName", "\u03b1"], [244, 245, "HyperparameterName", "\u03b3"], [254, 255, "DatasetName", "XSum"]]}
{"text": "We first train the student embeddings with the teacher model initialized from BERT LARGE . For a given input sequence , we mix the vocabularies by randomly selecting ( with probability p SV , a hyperparameter ) words from the sequence to segment using the student vocabulary , with the other words segmented using the teacher vocabulary . As in Figure 1 on the left , for input [ ' I ' , ' like ' , ' machine ' , ' learning ' ] , the words ' like ' and ' learning ' are segmented using the student vocabulary ( in blue ) , with the others using the teacher vocabulary ( in green ) . Similar to Lample and Conneau ( 2019 ) , this step seeks to align the student and teacher embeddings for the same tokens : the model learns to predict student tokens using context which is segmented using the teacher vocabulary , and vice versa . Note that since the student embeddings are set to a lower dimension than the teacher embeddings , as they are meant to be used in the smaller student model , we project the student embeddings up to the teacher embedding dimension using a trainable affine layer before these are input to the teacher BERT . We choose to keep the two embedding matrices separate despite the high token overlap : this is partly to keep our approach robust to lower vocabulary overlap settings , and partly due to empirical considerations described in Section 6 . Let \u03b8 s /eb s and \u03b8 t /eb t denote the transformer layer and embedding weights for the student and teacher models respectively . The loss defined in Equation 1 is the MLM cross entropy summed over masked positions M t in the teacher input . y i and c i denote the predicted and true tokens at position i respectively and can belong to either vocabulary . v i { s , t } denotes the vocabulary used to segment this token . Separate softmax layers P v i are used for token prediction , one for each vocabulary , depending on the segmenting vocabulary v i for token i. All teacher parameters ( \u03b8 t , eb t ) and student embeddings ( eb s ) are updated in this step . L s 1 = \u2212 i Mt ( logP v i ( y i = c i | \u03b8 t , eb s , eb t ) ) ( 1 ) Stage II ( Student Model Layers ) : With student embeddings initialized in stage I , we now train the student model normally i.e. , using only the student vocabulary and discarding the teacher model . Equation 2 shows the student MLM loss where M s is the set of positions masked in the student input . All student model parameters ( \u03b8 s , eb s ) are updated . L s 2 = \u2212 i M s logP s ( y i = c i | \u03b8 s , eb s ) ) ( 2 )", "entities": [[12, 13, "MethodName", "BERT"], [202, 204, "HyperparameterName", "embedding dimension"], [216, 217, "MethodName", "BERT"], [259, 260, "HyperparameterName", "\u03b8"], [264, 265, "HyperparameterName", "\u03b8"], [284, 285, "MetricName", "loss"], [291, 292, "DatasetName", "MLM"], [344, 345, "MethodName", "softmax"], [374, 375, "HyperparameterName", "\u03b8"], [411, 412, "HyperparameterName", "\u03b8"], [465, 466, "DatasetName", "MLM"], [466, 467, "MetricName", "loss"], [486, 487, "HyperparameterName", "\u03b8"], [512, 513, "HyperparameterName", "\u03b8"]]}
{"text": "Distillation : For all our models , we train the teacher model with mixed - vocabulary inputs ( stage I ) for 500 K steps , followed by 300 K steps of training just the student model ( stage II ) . We utilize the same corpora as the teacher model i.e. BooksCorpus ( Zhu et al , 2015 ) and English Wikipedia . For both stages , up to 20 input tokens were masked for MLM . In stage I , up to 10 of these masked tokens were tokenized using the teacher vocabulary , the rest using the student vocabulary . We optimize the loss using LAMB ( You et al , 2019 ) with a max learning rate of .00125 , linear warmup for the first 10 % of steps , batch size of 2048 and sequence length of 128 . Distillation was done on Cloud TPUs in a 8x8 pod configuration . p SV , the probability of segmenting a Stage I input word using the student vocabulary , is set to 0.5 . Finetuning : For all downstream task evaluations on GLUE , we finetune for 10 epochs using LAMB with a learning rate of 0.0001 and batch size of 64 . For all experiments on SNIPS , we use ADAM with a learning rate of 0.0001 and a batch size of 64 .", "entities": [[76, 77, "DatasetName", "MLM"], [106, 107, "MetricName", "loss"], [108, 109, "MethodName", "LAMB"], [119, 121, "HyperparameterName", "learning rate"], [124, 126, "MethodName", "linear warmup"], [134, 136, "HyperparameterName", "batch size"], [137, 138, "DatasetName", "2048"], [186, 187, "DatasetName", "GLUE"], [194, 195, "MethodName", "LAMB"], [197, 199, "HyperparameterName", "learning rate"], [202, 204, "HyperparameterName", "batch size"], [211, 212, "DatasetName", "SNIPS"], [215, 216, "DatasetName", "ADAM"], [218, 220, "HyperparameterName", "learning rate"], [224, 226, "HyperparameterName", "batch size"]]}
{"text": "Table 1 shows results on downstream GLUE tasks and model sizes for our proposed models , BERT BASE / LARGE , and baselines . Our models consistently improve upon the identically parameterized NoKD baselines , indicating mixedvocabulary training is better than training from scratch and avoids a large teacher - student performance gap . Compared with PKD / DistilBERT , our 6 - layer model outperforms PKD 3 while being > 7x smaller and our 12 - layer model is comparable to PKD 6 and DistilBERT 4 while being \u223c5 - 6x smaller . Interestingly , our models do particularly well on the MRPC task : the 6 - layer distilled model performs almost as well as PKD 6 while being over 10x smaller . This may be due to our smaller models being data - efficient on the smaller MRPC dataset . TinyBERT and Bert - of - Theseus are trained in task - specific fashion i.e. , a teacher model already finetuned on the downstream task is used for distillation . TinyBERT 's non - task - specific model results are reported on GLUE dev sets : these results are , therefore , not directly comparable with ours . Even so , our 12 - layer model performs credibly compared with the two , presenting a competitive size - accuracy tradeoff , particularly when compared to the 6x larger BERT - of - Theseus . MobileBERT performs strongly for the size while being task - agnostic . Our 12 - layer model , in comparison , retains \u223c98 % of its performance with 57 % fewer parameters and may thus be bettersuited for use on highly resource - limited devices . TinyBERT sees major gains from task - specific data augmentation and distillation , and Mobile - BERT from student architecture search and bottleneck layers . Notably , our technique targets the student vocabulary without conflicting with any of the above methods and can , in fact , be combined with these methods for even smaller models .", "entities": [[6, 7, "DatasetName", "GLUE"], [16, 17, "MethodName", "BERT"], [17, 18, "MethodName", "BASE"], [58, 59, "MethodName", "DistilBERT"], [85, 86, "MethodName", "DistilBERT"], [103, 104, "DatasetName", "MRPC"], [140, 141, "DatasetName", "MRPC"], [185, 186, "DatasetName", "GLUE"], [221, 222, "MetricName", "accuracy"], [231, 232, "MethodName", "BERT"], [237, 238, "MethodName", "MobileBERT"], [291, 293, "TaskName", "data augmentation"], [299, 300, "MethodName", "BERT"]]}
{"text": "Table 2 shows results on the SNIPS intent and slot tasks for our models and two state - of - theart baselines . Our smallest 6 - layer model retains over 95 % of the BERT BASE model 's slot filling F1 score ( Sang and Buchholz , 2000 ) while being 30x smaller ( < 10 MB w/o quantization ) and 57x faster on a mobile device , yet task - agnostic . Our other larger distilled models also demonstrate strong performance ( 0.2 - 0.5 % slot F1 higher than the respective NoKD baselines ) with small model sizes and latencies low enough for real - time inference . This indicates that small multi - task BERT models ( Tsai et al , 2019 ) present better trade - offs for on - device usage for size , accuracy and latency versus recurrent encoder - based models such as StackProp .", "entities": [[6, 7, "DatasetName", "SNIPS"], [35, 36, "MethodName", "BERT"], [36, 37, "MethodName", "BASE"], [39, 41, "TaskName", "slot filling"], [41, 43, "MetricName", "F1 score"], [59, 60, "TaskName", "quantization"], [89, 90, "MetricName", "F1"], [118, 119, "MethodName", "BERT"], [140, 141, "MetricName", "accuracy"]]}
{"text": "Impact of vocabulary size : We trained a model from scratch identical to BERT BASE except with our 5 K - WP student vocabulary . On the SST - 2 and MNLI - m dev sets , this model obtained 90.9 % and 83.7 % accuracy respectively - only 1.8 % and 0.7 % lower respectively compared to BERT BASE . Since embeddings account for a larger fraction of model parameters with fewer layers , we trained another model identical to our 6\u00d7256 model , but with a 30 K - WP vocabulary and teacher label dis - tillation . This model showed small gains ( 0.1 % / 0.5 % accuracy on SST - 2 / MNLI - m dev ) over our analogous distilled model , but with 30 % more parameters solely due to the larger vocabulary . This suggests that a small WordPiece vocabulary may be almost as effective for sequence classification / tagging tasks , especially for smaller BERT models and up to moderately long inputs . Curiously , increasing the student vocabulary size to 7 K or 10 K did not lead to an increase in performance on GLUE . We surmise that this may be due to underfitting owing to the embeddings accounting for a larger proportion of the model parameters . Alternative vocabulary pruning : Probing other strategies for a small - vocabulary model , we used the above 6\u00d7256 30 K - WP vanilla distilled model to obtain a smaller model by pruning the vocabulary to contain the intersection of the 30 K and 5 K vocabularies ( total 4629 WPs ) . This model is 1.2 % smaller than our 4928 - WP distilled model , but drops 0.8 % / 0.7 % on SST - 2 / MNLI - m dev sets . Furthermore , to exploit the high overlap in vocabularies , we tried running our distillation pipeline but with the embeddings for student tokens ( after projecting up to the teacher dimension ) also present in the teacher vocabulary tied to the teacher embeddings for those tokens . This model , however , dropped 0.7 % / 0.5 % on SST - 2 / MNLI - m compared to our analogous 6\u00d7256 distilled model . We also tried pretraining BERT LARGE from scratch with the 5 K vocabulary and doing vanilla distillation for a 6\u00d7256 student : this model dropped 1.2 % / 0.7 % for SST - 2 / MNLI - m over our similar distilled model , indicating the efficacy of mixed - vocabulary training over vanilla distillation .", "entities": [[13, 14, "MethodName", "BERT"], [14, 15, "MethodName", "BASE"], [27, 28, "DatasetName", "SST"], [31, 34, "DatasetName", "MNLI - m"], [45, 46, "MetricName", "accuracy"], [58, 59, "MethodName", "BERT"], [59, 60, "MethodName", "BASE"], [111, 112, "MetricName", "accuracy"], [113, 114, "DatasetName", "SST"], [117, 120, "DatasetName", "MNLI - m"], [146, 147, "MethodName", "WordPiece"], [163, 164, "MethodName", "BERT"], [194, 195, "DatasetName", "GLUE"], [294, 295, "DatasetName", "SST"], [298, 301, "DatasetName", "MNLI - m"], [363, 364, "DatasetName", "SST"], [367, 370, "DatasetName", "MNLI - m"], [382, 383, "MethodName", "BERT"], [409, 410, "DatasetName", "SST"], [413, 416, "DatasetName", "MNLI - m"]]}
{"text": "The utility of additional semantic information for the task of next utterance selection in an automated dialogue system is the focus of study in this paper . In particular , we show that additional information available in the form of dialogue acts - when used along with context given in the form of dialogue history - improves the performance irrespective of the underlying model being generative or discriminative . In order to show the model agnostic behavior of dialogue acts , we experiment with several well - known models such as sequence - to - sequence encoder - decoder model , hierarchical encoder - decoder model , and Siamese - based models with and without hierarchy ; and show that in all models , incorporating dialogue acts improves the performance by a significant margin . We , furthermore , propose a novel way of encoding dialogue act information , and use it along with hierarchical encoder to build a model that can use the sequential dialogue act information in a natural way . Our proposed model achieves an MRR of about 84.8 % for the task of next utterance selection on a newly introduced DailyDialog dataset , and outperform the baseline models . We also provide a detailed analysis of results including key insights that explain the improvement in MRR because of dialogue act information .", "entities": [[178, 179, "MetricName", "MRR"], [194, 195, "DatasetName", "DailyDialog"], [219, 220, "MetricName", "MRR"]]}
{"text": "An encoder - decoder is a generative model that works on the idea of obtaining a representation of an input and use it for generating an output . It has two main components , encoder and decoder . The encoder encodes the first K utterances , and the decoder uses that encoding to generate the next K + 1 th utterance . In a conversation , all words in first K utterances can be stringed together to form a single long chain and passed to an RNN encoder as following : e k = f 1 embed ( w k ) \u2200k 1 , 2 , . . . h e k = f 1 rnn ( h e k\u22121 , e k ) \u2200k 1 , 2 , . . . ( 1 ) where , f 1 embed represents the embedding layer , whereas f 1 rnn is the encoder ( RNN ) . Let v be the final output of the encoder which is considered as a representation of the entire context , and used to initialize the decoder ( another RNN ) . Mathematically , the sequence of operations at the decoder are as follows : h d 0 = v h d k = f 2 rnn ( h d k\u22121 , f 2 embed ( w k ) ) \u2200k 1 , 2 , . . . n \u2212 1 P k = Logistic ( h d k ) . ( 2 ) Here , f 2 embed represents the embedding layer . Logistic is the final layer , which outputs the probability distribution over the vocabulary . Encoder - decoder models are trained to maximize the likelihood of generating the next utterance , however , for the task of next utterance selection , they are tested based on the probability of generating the candidate utterances .", "entities": [[92, 94, "HyperparameterName", "k ="], [111, 113, "HyperparameterName", "k ="], [202, 203, "DatasetName", "0"], [207, 209, "HyperparameterName", "k ="], [237, 239, "HyperparameterName", "k ="]]}
{"text": "Dialogue acts are higher level abstractions assigned to utterances . In our problem setting , we are given a list of dialogue acts da 1 , da 2 , . . . da K , corresponding to first K utterances in the conversation . These dialogue acts are treated as an additional sequence of signals that can aid in the learning process , and are passed through an encoder , denoted as Dialog - Act encoder ( DA - encoder ) . The DA - encoder works on the same principle as the utterance encoder . It builds a dialogue act vocabulary and uses that to learn dialogue act embeddings . Similar to the utterance encoder , the input to the DA - encoder are one hot encodings of the dialogue acts , which are then passed through an embedding layer to learn DA embeddings . These DA embeddings are sent to an RNN to learn dialogue act representations . The sequence of operations for the DA - encoder are as follows : e da k = f 3 embed ( da k ) \u2200k 1 , 2 , . . . K h da k = f 4 rnn ( h da k\u22121 , e da k ) \u2200k 1 , 2 , . . . K. q K = h da K ( 4 ) The output of the DA - encoder at the last time step ( q K ) gives us the representation of the entire DA sequence which is then used in the further modeling process in generative and discriminative models . In generative models , it is used in the decoder by concatenating g K and q K , whereas in discriminative models , it is used along with encoder 's output by combining g K with q K through a linear combination .", "entities": [[175, 177, "HyperparameterName", "k ="], [195, 197, "HyperparameterName", "k ="], [219, 221, "HyperparameterName", "K ="]]}
{"text": "Our proposed model , i.e. Dialog - act - driven Hierarchical Siamese Model ( HSiamese - DA ) , uses the following three components : a hierarchical encoder to obtain a representation that captures the dependencies among K utterances ; an utterance encoder to obtain a representation of the candidate response , ( K + 1 ) th utterance ; a DA - encoder ( Equation 4 ) that captures the dependencies among the dialogue acts of the first K utterances . Let the representation obtained from the hierarchical encoder , DA - encoder and utterance encoder be g K , q K and v K+1 , respectively . The two representations , g K and q K , are linearly combined to obtained a compositional representation of the context , which is then used along with candidate representation to compute the probability of associating the candidate response with the context using following expression : d K = \u03b1 * g K + ( 1 \u2212 \u03b1 ) * q K p ( s | d K , v K+1 ) = \u03c3 ( d T K A v K+1 + b ) ( 5 ) The model is trained by minimizing the cross - entropy of all labeled conversations including positive and negative examples . At the test time , each conversation has K utterances followed by a set of 10 candidates responses . The system is tested in its ability to assign a higher rank to the true response .", "entities": [[156, 158, "HyperparameterName", "K ="], [158, 159, "HyperparameterName", "\u03b1"], [166, 167, "HyperparameterName", "\u03b1"]]}
{"text": "In our experiments , the parameters are tuned on validation set while the results are reported on test set . Each utterance in a mini - batch was padded to the maximum length for that batch . The maximum batch size allowed was 32 . The word vectors were initialized with the 300 - dimensional Glove embeddings ( Pennington et al , 2014 ) , and were also updated during training . For the generative models , the utterance encoder , conversation encoder , DA - Encoder and decoder are all GRUs with rnn size set to 1000 ( optimized over 100 to 1200 in steps of 100 ) . For the discriminative model , the utterance encoder , conversation encoder , and DA - Encoder are all GRUs with rnn size set to 300 ( optimized over 100 to 500 in steps of 100 ) . Dropout of 0.1 ( optimized over 0.0 to 0.7 in steps of 0.1 ) was applied to embeddings obtained from the output of conversation encoder . Note that , dropout was not used in the discriminative model and its variations . Models were trained to minimize cross entropy using Adam optimizer with learning rate of 0.0003 ( optimized over 0.0001 , 0.0003 , 0.0005 , 0.0007 , 0.001 ) . We found that a higher learning rate up - to 0.0005 helps the model to learn quickly , whereas learning rate greater than 0.0005 leads to oscillations .", "entities": [[39, 41, "HyperparameterName", "batch size"], [55, 57, "MethodName", "Glove embeddings"], [147, 148, "MethodName", "Dropout"], [196, 197, "MethodName", "Adam"], [197, 198, "HyperparameterName", "optimizer"], [199, 201, "HyperparameterName", "learning rate"], [222, 224, "HyperparameterName", "learning rate"], [236, 238, "HyperparameterName", "learning rate"]]}
{"text": "Since our problem formulation is retrieval based , we use standard IR metrics such as Mean Reciprocal Rank ( MRR ) and Recall@k as our evaluation metrics . MRR is calculated as the mean of the reciprocal rank of the true candidate response among other candidate responses . Recall@k measures whether the true candidate response appears in a ranked list of k responses . In this work , our hypothesis is that additional information about utterances available in the form of dialogue acts helps irrespective of the underlying model , i.e. generative or discriminative . Results in Table 3 support our hypothesis . These results clearly indicate that the MRR of the true candidate response improves when dialogue acts of previous utterances are provided . From these tables we see that for all underlying models , the dialogue act version performs better than non dialogue act version . These results furthermore indicate that hierarchical version performs better than non - hierarchical version for both generative and discriminative models . In the generative case , the plain ED has an MRR of 0.474 , whereas the same model , when conditioned with DA - Encoder , has an MRR of 0.54 , an improvement of 13.9 % . The hierarchical encoder - decoder HRED and HRED - DA has an MRR of 0.523 and 0.583 , respectively , an improvement of 11.4 % . Generative models are sequence - to - sequence models and rather complex in nature , so it is interesting to note that even a much simpler discriminative model , i.e. plain Siamese model , without any dialogue act information , has an MRR of 0.8 compared to 0.58 of the best performing generative model , i.e. HRED - DA . This observation demonstrates the strength of the discriminative models , and therefore is a motivation behind the proposed model . The proposed model improves these baseline numbers by incorporating hierarchy and dialogue act information , and pushes the MRR to 0.848 .", "entities": [[19, 20, "MetricName", "MRR"], [28, 29, "MetricName", "MRR"], [109, 110, "MetricName", "MRR"], [179, 180, "MetricName", "MRR"], [197, 198, "MetricName", "MRR"], [219, 220, "MetricName", "MRR"], [275, 276, "MetricName", "MRR"], [331, 332, "MetricName", "MRR"]]}
{"text": "While we have shown that using dialogue act information does help in the next utterance selection task , in this section , we dig deeper and understand reasons for it . In order to do that , we analyze the dialogue act distribution of the test data and model outputs . Although all K dialogue acts corresponding to K utterances in the context might play a role in ranking candidate utterances , the following analysis only uses the pairs of dialogue acts , i.e. dialogue acts of K th and ( K + 1 ) th utterances . Tables 4 ( a ) , 4 ( b ) and 4 ( c ) show the distribution of such dialogue act pairs for test data , HSiamese , and HSiamese - DA models respectively . Here , rows indicate the dialogue act of K th utterance , whereas columns indicate the dialogue act of ( K + 1 ) th utterance . A cell value indicates the count of utterance pairs with the respective dialogue act combinations where ( K + 1 ) th utterance was ranked 1 . Note that in the test data , ( K + 1 ) th utterance is the true candidate response and always have the rank 1 . For instance , there are 742 utterance pairs in the test data , where K th and ( K + 1 ) th utterances have dialogue acts Q and I , respectively , however , out of those 742 instances , HSiamese ranked only 605 as 1 while HSiamese - DA ranked 638 as 1 . From these tables , we draw following observations . Models Learn Dominant Patterns : The first is that there are certain dominant communication patterns that we observe in both , test data and model outputs ( See Table 4 ) , suggesting that models are able to learn these patterns and retain them in their outputs . We observe that a Question is often followed by an Information , whereas an Information can be followed by another Information or a Question . A Directive tends to be followed by Commissive . These communication patterns not only make sense intuitively but they are also in agreement with previous studies ( Li et al , 2017b ; Ribeiro et al , 2015 ) . Dialogue Acts Bring Uniformity : The second and a rather more important observation is that dialogue acts help the most for the dialogue act class ( DA - class ) when the utterances belonging to that class are non - uniform in their linguistic construct . In order to better exlpain this , we first compute the break - up of recall@1 according to the dialogue act classes . A DA - class of a conversation in the test data is defined based on the dialogue act of the last utterance ( K th utterance ) in the context . These numbers are shown in the last column of Tables 4 ( b ) and 4 ( c ) for the respective models . In Table 4 ( b ) , first row in recall@1 column is 0.65 , which indicates that out of the total number of test conversations where dialogue act of the last utterance of context was I , 65 % of true candidate responses were ranked 1 by the HSiamese model . Such a DA - class wise breakup of the recall@1 numbers helps us do an analysis with respect to individual DA - classes . From this break - up , it is clear that for the HSiamese model , Question DA - class has the best performance of 78 % whereas Directive has the worst performance of 63 % . This difference can be attributed to the fact that all utterances with dialogue act as Question have rather uniform construct . Some examples of Question utterances are , ' Q : Do you have a fever ? ' and ' Q : Why do you want to work for our company ? ' , while the examples of Directive utterances are , ' D : when we have the final results , we will call you . ' and ' D : we will take the trip . could you give us a pamphlet ? ' . From these examples , we observe that utterances belonging to DA - class Question have rather uniform construct in terms of linguistic features , whereas utterances belonging to DA - class Directive are ambiguous - some of the utterances of type Directive can be easily confused for Question . This uniformity makes the learning task easier for Question class , and thereby giving us better results in the next utterance selection task , even for the model that does not use the dialogue act information . This performance difference reduces when we provide the dialogue act information along with the textual content ( See 5 , we show the relative improvement of HSiamese - DA model over HSiamese . From this table , we observe that there are a total of 228 conversations where the proposed model was able to improve the ranking of true candidate response to 1 . We further observe that the biggest improvement is in I I , I Q , Q I , and D C , which make sense intuitively . These are dominant patterns observed in the training data which should be preserved in the model output as well , however these patterns will only be preserved when model is able to capture the correct dialogue act information . Since in many cases D and Q have similar construct , without explicit dialogue act information , a model may get confused and may learn patterns not observed in the training data . For example , Q I and D C are the dominant and right patterns in the training data , however in the absence of explicit dialogue act information , the model may get confused between D and Q and may learn D I and Q C instead of the dominant patterns i.e. Q I and D C. With the explicit dialogue act information , this ambiguity is alleviated and model learns the right patterns as demonstrated by Table 5 . Similar observations are true for other two constructs , i.e. Information and Commisive . Both are rather similar in construct , ' I : No , thank you ' , ' I : It does n't matter . it happens to everyone . ' and ' C : I knew you 'd see it my way . ' , ' C : Ok , i am ready to think of other things . ' , and there is no obvious distinguishing factor . However , providing explicit DA information helps in disambiguation , and learn the patterns that are observed in the training data such as I I , I Q.", "entities": [[453, 454, "MetricName", "recall@1"], [526, 527, "MetricName", "recall@1"], [577, 578, "MetricName", "recall@1"]]}
{"text": "For the task of next utterance selection , we show that dialogue acts helps achieve better performance irrespective of the underlying model , be it generative or discriminative . We also propose a novel discriminative model that leverages the hierarchical structure in a conversation and dialogue act information to produce much improved results , an MRR of 0.848 . Our results not only show the improvement in performance , but we also present key reasons for it by doing a detailed analysis and drawing key insights that the inclusion of dialogue act information induces uniformity and removes ambiguity .", "entities": [[55, 56, "MetricName", "MRR"]]}
{"text": "The Arabic dialects have a common written form and unified literary tradition , so it seems most logical to distinguish dialects on the basis of acoustics , and there is a fair amount of work there , including Hanani et al ( 2013Hanani et al ( , 2015 ; . - - Biadsy et al ( 2009 ) distinguish four Arabic dialects and MSA based on ( audio ) phone sequences ; the phones were obtained by phone recognizers for English , German , Japanese , Hindi , Mandarin , Spanish , and three different MSA phone - recognizer implementations . The dialects were distinguished by phoneme sequences , and the results of classifications based on each phonerecognizer were combined using a logistic regression classifier . They train on 150 hours per dialect of telephone recordings . They report 61 % accuracy on 5 - second segments , and 84 % accuracy on 120 second segments . Zaidan and Callison - Burch ( 2011 ) describe building a text corpus , based on reader commen - tary on newspaper websites , with significant dialect content ; the goal is to provide a corpus to improve machine translation for Arabic dialects . They used Amazon Mechanical Turk to provide annotation for a portion of the corpus . Zaidan and Callison - Burch ( 2014 ) describe the same work in greater detail , including dialect classifiers they built using the Mechanical Turk data for classes and origin metadata as additional features . They say these classifiers are ' approaching human quality . ' ElFardy and Diab ( 2013 ) classify EGY 3 and MSA sentences from the Zaidan and Callison - Burch ( 2011 ) corpus , that is , from text . Not only is this a binary task , but orthographic hints , including repeated long vowels , emojis and multiple punctuations , give strong clues of the register , and hence whether MSA is being employed . They do a number of experiments comparing various preprocessing schemes and different training sizes , ranging from 2 - 28 million tokens . They achieve 80 % - 86 % accuracy for all of their attempts . Malmasi et al ( 2015 ) do Arabic dialect identification from text corpora , including the Multi - Dialect Parallel Corpus of Arabic ( Bouamor et al , 2014 ) and the Arabic Online Commentary database ( Zaidan and Callison - Burch , 2011 ) . Hanani et al ( 2015 ) perform recognition of several Palestinian regional accents , evaluating four different acoustic models , achieving 81.5 % accuracy for their best system , an I - vector framework with 64 Gaussian components . developed the corpus on which the DSL Arabic shared task is based . Their own dialect detection efforts depended largely on acoustical cues . Arabic dialect recognition appeared in the 2016 edition of the VarDial workshop 's shared task ( Malmasi et al , 2016 ) . The shared task data was text - only . The best classifiers ( Malmasi et al , 2016 ; Ionescu and Popescu , 2016 ) for the shared task performed far below the best results reported by some of the preceding researchers , in particular which used some of the same data . Part of the reason must be that the amount of training data for the workshop is much smaller than that used by some of the other researchers ; the workshop data also did not include the audio recordings on which the transcripts are based . 3", "entities": [[122, 124, "MethodName", "logistic regression"], [141, 142, "MetricName", "accuracy"], [151, 152, "MetricName", "accuracy"], [195, 197, "TaskName", "machine translation"], [359, 360, "MetricName", "accuracy"], [374, 376, "TaskName", "dialect identification"], [435, 436, "MetricName", "accuracy"]]}
{"text": "We experimented with several neural networks . Our model for the S - 1 submission uses as input 26 features which correspond to one of our 26 pretrained dialect language models . Each feature represents the probability of a given sentence for one language model . The probability scores measure how close each sentence is to the dialect . We train Multilayer Perceptron ( MLP ) with one hidden ( dense ) layer with 400 units . The output of the hidden layer is passed to a final fullyconnected softmax layer . The output of the softmax layer is a probability distribution over all 26 classes . The class with the highest probability is predicted as a final output of our model . As an activation function in the hidden layer of the MLP a Rectified Linear Unit ( ReLu ) is employed . We also tried to combine character n - gram features with the language model features . The input is a sequence of first 200 character n - grams of a given text . Each sequence of character n - grams is used as a separate input followed by a randomly initialized embedding layer and then two layers of Bidirectional LSTM ( BiLSTM ) ( Graves and Schmidhuber , 2005 ) with 64 units are employed ( see Figure 1 ) . The output vector of the BiLSTM layers is concatenated with the language model features and this concatenated vector is passed to the MLP layer with 400 units ( the same as described above ) . All models were implemented by using Keras ( Chollet et al , 2015 ) with TensorFlow backend ( Abadi et al , 2015 )", "entities": [[64, 65, "DatasetName", "MLP"], [89, 90, "MethodName", "softmax"], [96, 97, "MethodName", "softmax"], [125, 127, "HyperparameterName", "activation function"], [133, 134, "DatasetName", "MLP"], [139, 140, "MethodName", "ReLu"], [202, 204, "MethodName", "Bidirectional LSTM"], [205, 206, "MethodName", "BiLSTM"], [230, 231, "MethodName", "BiLSTM"], [247, 248, "DatasetName", "MLP"]]}
{"text": "We tune all hyperparameters on the development data . We train our model with Adam ( Kingma and Ba , 2014 ) optimizer with learning rate 0.01 and without any dropout . The number of epochs is 800 and we do not use mini - batches or dropout regularization technique . The model with these hyperparameters achieves the best result ( 0.661 macro F 1 - score ) on the development data and was used for the final submission . We also experimented with the n - gram inputs . We tried a different number of character n - grams and we achieve the best result ( 0.555 macro F 1score ) on the development data using three inputs - character unigrams , bigrams and trigrams , with learning rate 0.005 , mini - batches of size 256 for 11 epochs and with the Adam optimizer .", "entities": [[14, 15, "MethodName", "Adam"], [22, 23, "HyperparameterName", "optimizer"], [24, 26, "HyperparameterName", "learning rate"], [33, 36, "HyperparameterName", "number of epochs"], [128, 130, "HyperparameterName", "learning rate"], [144, 145, "MethodName", "Adam"], [145, 146, "HyperparameterName", "optimizer"]]}
{"text": "Our tortuous classifier did less well on the tweet data , so we used a simpler classifier . The features are the kenlm language model scores for the 21 countries , computed for each of the training tweets , then exponentiated and normalized to sum to 1 . The tweets are classified using y_test = KNeighborsClassifier ( n_neighbors=31 ) .fit ( X_train , y_train ) .predict ( X_test ) The users are predicted based on the plurality prediction for all of their tweets , that is , the country to which the largest number of their tweets were assigned . There were a significant number of tweets unavailable , about 10 % in the training and development sets , and 12 % in the test set . After the submissions had closed we experimented with eliminating the unavailable and non - Arabic tweets from True labels 127 0 0 12 3 4 1 7 0 0 30 1 0 0 8 0 1 0 0 0 0 3 1 0 1 1 0 160 1 1 0 1 0 0 8 0 0 1 7 1 1 4 1 4 2 2 2 1 0 1 1 1 0 3 149 4 16 1 0 0 1 10 1 1 1 2 1 8 0 0 0 0 0 0 2 0 0 0 9 0 2 118 2 1 0 6 2 8 9 7 0 2 18 3 0 0 2 0 2 7 1 0 1 0 0 0 21 4 134 1 1 0 0 19 1 4 1 3 4 2 0 0 0 0 3 1 0 0 1 0 2 0 0 5 2 127 25 1 3 1 1 3 0 1 0 4 3 3 1 0 9 2 5 2 0 0 1 1 1 2 1 29 128 1 2 0 1 6 0 1 0 0 8 2 3 0 5 2 3 2 1 0 8 1 0 7 2 1 1 129 2 3 18 0 0 1 9 3 0 1 2 1 3 5 0 0 2 1 2 5 0 6 9 3 1 1 131 2 1 8 0 4 2 0 0 0 2 0 6 1 4 1 11 0 6 4 19 3 37 2 0 0 2 100 2 1 1 5 1 9 0 0 0 0 3 0 1 0 2 2 16 0 1 16 4 4 2 17 2 3 108 4 1 2 10 0 0 0 0 0 0 8 1 0 1 0 1 1 0 4 2 2 3 1 1 1 0 150 1 8 4 5 0 1 3 1 4 2 1 1 2 1 0 7 1 1 2 0 0 0 0 2 1 4 136 3 0 2 0 1 2 31 1 1 1 3 0 1 1 1 1 6 4 0 0 2 5 3 3 14 1 123 1 6 0 1 3 0 10 5 5 1 3 1 7 2 1 26 2 1 2 7 5 1 1 6 0 2 121 2 0 0 1 0 2 9 1 0 1 0 0 2 3 8 10 1 1 1 0 1 1 3 0 4 2 147 1 12 0 0 0 0 1 0 2 0 1 0 0 0 0 3 8 0 1 0 0 0 0 1 0 0 167 1 1 0 6 4 5 0 1 1 1 7 2 2 0 1 0 0 2 1 1 0 0 0 1 9 1 158 4 0 7 1 1 0 1 0 2 3 1 3 1 3 1 1 5 1 3 15 1 8 1 10 0 32 83 1 16 4 4 0 1 0 1 7 0 3 0 0 0 0 2 0 1 1 36 1 2 1 0 0 0 139 0 1 0 3 1 1 2 0 0 3 0 2 2 3 6 5 0 13 1 18 1 1 2 9 7 0 115 2 7 0 1 0 3 3 1 12 3 2 2 5 3 2 7 10 0 5 12 2 1 3 2 0 7 109 4 1 1 0 1 1 1 2 1 6 2 1 3 1 1 7 0 9 1 3 4 3 1 0 6 4 140 0 2 0 2 6 0 1 1 2 3 0 3 1 2 0 2 0 2 0 0 0 2 1 3 1 0 146 0 22 1 3 0 2 3 0 0 2 9 2 0 4 0 2 2 4 0 0 1 1 4 1 1 4 151 3 1 training and testing and choosing Saudi Arabia ( which is the origin for the plurality of tweets at 36 % ) for users with no remaining tweets . This improved tweet classification accuracy by about 5 % , but actually decreased user classification accuracy on the development set .", "entities": [[147, 148, "DatasetName", "0"], [148, 149, "DatasetName", "0"], [154, 155, "DatasetName", "0"], [155, 156, "DatasetName", "0"], [158, 159, "DatasetName", "0"], [159, 160, "DatasetName", "0"], [161, 162, "DatasetName", "0"], [163, 164, "DatasetName", "0"], [164, 165, "DatasetName", "0"], [165, 166, "DatasetName", "0"], [166, 167, "DatasetName", "0"], [169, 170, "DatasetName", "0"], [172, 173, "DatasetName", "0"], [176, 177, "DatasetName", "0"], [178, 179, "DatasetName", "0"], [179, 180, "DatasetName", "0"], [181, 182, "DatasetName", "0"], [182, 183, "DatasetName", "0"], [194, 195, "DatasetName", "0"], [198, 199, "DatasetName", "0"], [204, 205, "DatasetName", "0"], [205, 206, "DatasetName", "0"], [214, 215, "DatasetName", "0"], [215, 216, "DatasetName", "0"], [216, 217, "DatasetName", "0"], [217, 218, "DatasetName", "0"], [218, 219, "DatasetName", "0"], [219, 220, "DatasetName", "0"], [221, 222, "DatasetName", "0"], [222, 223, "DatasetName", "0"], [223, 224, "DatasetName", "0"], [225, 226, "DatasetName", "0"], [230, 231, "DatasetName", "0"], [236, 237, "DatasetName", "0"], [240, 241, "DatasetName", "0"], [241, 242, "DatasetName", "0"], [243, 244, "DatasetName", "0"], [247, 248, "DatasetName", "0"], [249, 250, "DatasetName", "0"], [250, 251, "DatasetName", "0"], [251, 252, "DatasetName", "0"], [257, 258, "DatasetName", "0"], [258, 259, "DatasetName", "0"], [266, 267, "DatasetName", "0"], [267, 268, "DatasetName", "0"], [268, 269, "DatasetName", "0"], [269, 270, "DatasetName", "0"], [272, 273, "DatasetName", "0"], [273, 274, "DatasetName", "0"], [275, 276, "DatasetName", "0"], [277, 278, "DatasetName", "0"], [278, 279, "DatasetName", "0"], [288, 289, "DatasetName", "0"], [290, 291, "DatasetName", "0"], [295, 296, "DatasetName", "0"], [300, 301, "DatasetName", "0"], [301, 302, "DatasetName", "0"], [311, 312, "DatasetName", "0"], [314, 315, "DatasetName", "0"], [316, 317, "DatasetName", "0"], [317, 318, "DatasetName", "0"], [321, 322, "DatasetName", "0"], [327, 328, "DatasetName", "0"], [330, 331, "DatasetName", "0"], [339, 340, "DatasetName", "0"], [340, 341, "DatasetName", "0"], [344, 345, "DatasetName", "0"], [350, 351, "DatasetName", "0"], [351, 352, "DatasetName", "0"], [356, 357, "DatasetName", "0"], [366, 367, "DatasetName", "0"], [369, 370, "DatasetName", "0"], [370, 371, "DatasetName", "0"], [371, 372, "DatasetName", "0"], [373, 374, "DatasetName", "0"], [379, 380, "DatasetName", "0"], [386, 387, "DatasetName", "0"], [387, 388, "DatasetName", "0"], [396, 397, "DatasetName", "0"], [397, 398, "DatasetName", "0"], [398, 399, "DatasetName", "0"], [399, 400, "DatasetName", "0"], [401, 402, "DatasetName", "0"], [403, 404, "DatasetName", "0"], [407, 408, "DatasetName", "0"], [421, 422, "DatasetName", "0"], [422, 423, "DatasetName", "0"], [423, 424, "DatasetName", "0"], [424, 425, "DatasetName", "0"], [425, 426, "DatasetName", "0"], [426, 427, "DatasetName", "0"], [429, 430, "DatasetName", "0"], [431, 432, "DatasetName", "0"], [434, 435, "DatasetName", "0"], [442, 443, "DatasetName", "0"], [448, 449, "DatasetName", "0"], [458, 459, "DatasetName", "0"], [463, 464, "DatasetName", "0"], [464, 465, "DatasetName", "0"], [465, 466, "DatasetName", "0"], [466, 467, "DatasetName", "0"], [472, 473, "DatasetName", "0"], [474, 475, "DatasetName", "0"], [482, 483, "DatasetName", "0"], [489, 490, "DatasetName", "0"], [490, 491, "DatasetName", "0"], [500, 501, "DatasetName", "0"], [503, 504, "DatasetName", "0"], [522, 523, "DatasetName", "0"], [526, 527, "DatasetName", "0"], [527, 528, "DatasetName", "0"], [529, 530, "DatasetName", "0"], [533, 534, "DatasetName", "0"], [535, 536, "DatasetName", "0"], [536, 537, "DatasetName", "0"], [544, 545, "DatasetName", "0"], [548, 549, "DatasetName", "0"], [554, 555, "DatasetName", "0"], [555, 556, "DatasetName", "0"], [556, 557, "DatasetName", "0"], [557, 558, "DatasetName", "0"], [559, 560, "DatasetName", "0"], [561, 562, "DatasetName", "0"], [563, 564, "DatasetName", "0"], [564, 565, "DatasetName", "0"], [565, 566, "DatasetName", "0"], [566, 567, "DatasetName", "0"], [569, 570, "DatasetName", "0"], [571, 572, "DatasetName", "0"], [572, 573, "DatasetName", "0"], [573, 574, "DatasetName", "0"], [574, 575, "DatasetName", "0"], [576, 577, "DatasetName", "0"], [577, 578, "DatasetName", "0"], [581, 582, "DatasetName", "0"], [585, 586, "DatasetName", "0"], [592, 593, "DatasetName", "0"], [594, 595, "DatasetName", "0"], [595, 596, "DatasetName", "0"], [599, 600, "DatasetName", "0"], [600, 601, "DatasetName", "0"], [601, 602, "DatasetName", "0"], [607, 608, "DatasetName", "0"], [611, 612, "DatasetName", "0"], [613, 614, "DatasetName", "0"], [630, 631, "DatasetName", "0"], [637, 638, "DatasetName", "0"], [639, 640, "DatasetName", "0"], [642, 643, "DatasetName", "0"], [644, 645, "DatasetName", "0"], [645, 646, "DatasetName", "0"], [646, 647, "DatasetName", "0"], [647, 648, "DatasetName", "0"], [649, 650, "DatasetName", "0"], [656, 657, "DatasetName", "0"], [657, 658, "DatasetName", "0"], [658, 659, "DatasetName", "0"], [660, 661, "DatasetName", "0"], [662, 663, "DatasetName", "0"], [667, 668, "DatasetName", "0"], [668, 669, "DatasetName", "0"], [670, 671, "DatasetName", "0"], [676, 677, "DatasetName", "0"], [685, 686, "DatasetName", "0"], [689, 690, "DatasetName", "0"], [691, 692, "DatasetName", "0"], [704, 705, "DatasetName", "0"], [711, 712, "DatasetName", "0"], [717, 718, "DatasetName", "0"], [730, 731, "DatasetName", "0"], [737, 738, "DatasetName", "0"], [741, 742, "DatasetName", "0"], [743, 744, "DatasetName", "0"], [746, 747, "DatasetName", "0"], [751, 752, "DatasetName", "0"], [755, 756, "DatasetName", "0"], [757, 758, "DatasetName", "0"], [759, 760, "DatasetName", "0"], [760, 761, "DatasetName", "0"], [761, 762, "DatasetName", "0"], [766, 767, "DatasetName", "0"], [768, 769, "DatasetName", "0"], [772, 773, "DatasetName", "0"], [775, 776, "DatasetName", "0"], [776, 777, "DatasetName", "0"], [780, 781, "DatasetName", "0"], [782, 783, "DatasetName", "0"], [786, 787, "DatasetName", "0"], [787, 788, "DatasetName", "0"], [829, 830, "MetricName", "accuracy"], [840, 841, "MetricName", "accuracy"]]}
{"text": "State - of - the - art neural machine translation methods employ massive amounts of parameters . Drastically reducing computational costs of such methods without affecting performance has been up to this point unsuccessful . To this end , we propose FullyQT : an allinclusive quantization strategy for the Transformer . To the best of our knowledge , we are the first to show that it is possible to avoid any loss in translation quality with a fully quantized Transformer . Indeed , compared to fullprecision , our 8 - bit models score greater or equal BLEU on most tasks . Comparing ourselves to all previously proposed methods , we achieve state - of - the - art quantization results .", "entities": [[8, 10, "TaskName", "machine translation"], [45, 46, "TaskName", "quantization"], [49, 50, "MethodName", "Transformer"], [71, 72, "MetricName", "loss"], [79, 80, "MethodName", "Transformer"], [96, 97, "MetricName", "BLEU"], [118, 119, "TaskName", "quantization"]]}
{"text": "The idea of using neural networks for machine translation was only recently proposed ( Kalchbrenner and Blunsom , 2013 ; Sutskever et al , 2014 ; . Nonetheless , the approach became the state - of - the - art in the field ( Ahmed et al , 2017 ; . A key element of this success was to allow the decoder to attend to all hidden states of the encoder . A few variations to this additive attention mechanism have been proposed , such as multiplicative and self - attention ( Luong et al , 2015 ; Cheng et al , 2016 ; Lin et al , 2017 ) . The latter formed the basis of the Transformer network ( Vaswani et al , 2017 ) , which achieved state - of - the - art results in machine translation . Inspiring a new wave of work , numerous natural language processing tasks reached new heights ( Devlin et al , 2018 ; Liu et al , 2019 ) . Unfortunately , these models use an enormous amount of parameters . Inference on resource - limited hardware such as edge - devices is thus impractical . A solution to reduce the computational burden of these networks is to lower numerical precision . Consequently , numerical values can be represented using fewer bits ( Tang and Kwan , 1993 ; Marchesi et al , 1993 ) . This method called quantization has the advantage of providing good compression rates with minimal loss in accuracy . It is also conveniently supported by most hardware . Properly quantizing the Transformer would allow computational speed gains at inference , as well as deployment on more constrained devices . In this work , we propose a quantization - aware training strategy for the entire Transformer architecture . Our method is easy to implement and results are consistent with the full - precision Transformer . We test our approach on multiple translation tasks such as WMT14 EN - FR and WMT14 EN - DE and obtain state - of - the - art quantization results . In comparison with full - precision , our quantized models score equal or higher BLEU on most tasks . We are , to the best of our knowledge , the first to show that the Transformer architecture can be fully quantized without impairing translation quality . We also perform an ablation study and show that quantizing specific components of the Transformer improves BLEU score .", "entities": [[7, 9, "TaskName", "machine translation"], [77, 79, "MethodName", "additive attention"], [118, 119, "MethodName", "Transformer"], [139, 141, "TaskName", "machine translation"], [240, 241, "TaskName", "quantization"], [251, 252, "MetricName", "loss"], [253, 254, "MetricName", "accuracy"], [267, 268, "MethodName", "Transformer"], [292, 293, "TaskName", "quantization"], [300, 301, "MethodName", "Transformer"], [318, 319, "MethodName", "Transformer"], [330, 331, "DatasetName", "WMT14"], [335, 336, "DatasetName", "WMT14"], [348, 349, "TaskName", "quantization"], [365, 366, "MetricName", "BLEU"], [386, 387, "MethodName", "Transformer"], [411, 412, "MethodName", "Transformer"], [413, 415, "MetricName", "BLEU score"]]}
{"text": "Over the years , a large range of methods have been proposed to quantize neural networks . These include , among many others , binary , ternary ( Lin et al , 2015 ; , uniform ( Jacob et al , 2017 ) and learned ( Zhang et al , 2018 ) quantization . These methods can be universally applied to any type of neural network . To maintain performance though , specific architectures usually require custom tailored quantization schemes . Several recent work explore recurrent neural network ( Jordan , 1990 ) quantization . Ott et al ( 2016 ) propose an exponential quantization method for RNN weights . They find ternary and exponential quantization to work well on language modeling and speech recognition , while binary weights seemed ineffective . quantize weights and activations of both RNNs and LSTMs ( Hochreiter and Schmidhuber , 1997 ) to 2 , 4 and 6bit . Meanwhile , propose modifications to the gates and interlinks of quantized LSTM and GRU cells , as well as a balanced quantization method for weights . successfully quantize a stacked sequence - tosequence LSTM to 8 - bit without any loss in translation quality . Most recently , Wang et al ( 2018 ) propose applying different quantization methods for different RNN components . With regards to CNNs ( LeCun et al , 1989 ) , various works have also explored quantizing these models . Gong et al ( 2014 ) compare matrix factorization , binarization , k - means clustering , product quantization and residual quantization of CNNs . Wu et al ( 2015 ) apply quantization to both kernels and fully connected layers of convolutional neural networks . Rastegari et al ( 2016 ) propose using binary weighted filters on AlexNet ( Krizhevsky et al , 2012 ) . Testing their method on ImageNet , they show classification accuracy to be on par with fullprecision . For faster inference and training , use low bitwidth weights , activations and gradients on CNNs . Quantization has been applied in tandem with other compression methods . Han et al ( 2015 ) combine pruning , quantization , weight sharing and Huffman coding . In another line of work , Polino et al ( 2018 ) employ quantization with knowledge distillation ( Hinton et al , 2015 ) for higher compression rates . Moreover , Chen et al ( 2018 ) blend quantization with block based low - rank matrix approximation of embeddings .", "entities": [[52, 53, "TaskName", "quantization"], [78, 79, "TaskName", "quantization"], [93, 94, "TaskName", "quantization"], [104, 105, "TaskName", "quantization"], [115, 116, "TaskName", "quantization"], [123, 125, "TaskName", "speech recognition"], [166, 167, "MethodName", "LSTM"], [168, 169, "MethodName", "GRU"], [176, 177, "TaskName", "quantization"], [188, 189, "MethodName", "LSTM"], [195, 196, "MetricName", "loss"], [212, 213, "TaskName", "quantization"], [250, 251, "TaskName", "binarization"], [252, 256, "MethodName", "k - means clustering"], [258, 259, "TaskName", "quantization"], [261, 262, "TaskName", "quantization"], [272, 273, "TaskName", "quantization"], [297, 298, "MethodName", "AlexNet"], [310, 311, "DatasetName", "ImageNet"], [315, 316, "MetricName", "accuracy"], [340, 341, "TaskName", "Quantization"], [360, 361, "TaskName", "quantization"], [381, 382, "TaskName", "quantization"], [383, 385, "MethodName", "knowledge distillation"], [406, 407, "TaskName", "quantization"]]}
{"text": "The pruning of neural networks for model compression has also been largely explored . LeCun et al ( 1990 ) were the first to propose a Hessian based method to prune neural net weights . Hassibi et al ( 1994 ) later improved the method . More recently , See et al ( 2016 ) show that pruning a fully trained model and then retraining it can increase performance over the original non - pruned model . Gradually pruning in tandem with training has also been shown to increase performance ( Zhu and Gupta , 2017 ) . To avoid sparse matrices , prune nodes instead of weights . They apply a penalty in the loss on the \u03b3 parameters of batch normalization layers . With a similar objective , Narang et al ( 2017b ) make better use of hardware by applying pruning and weight decay in blocks to minimize the number of loaded weight matrix chunks . Similarly to quantization , pruning methods have also been adapted to specific architectures . Liu et al ( 2015 ) propose an efficient sparse matrix multiplication algorithm for CNNs . As for RNNs , Narang et al ( 2017a ) show sparse pruning to work well on the architecture . In order to maintain dimension consistency , Wen et al ( 2017 ) propose to prune all basic LSTM structures concurrently . Lastly , Park et al ( 2018 ) introduce simple recurrent units ( SRUs ) for easy pruning of RNNs .", "entities": [[6, 8, "TaskName", "model compression"], [115, 116, "MetricName", "loss"], [118, 119, "HyperparameterName", "\u03b3"], [121, 123, "MethodName", "batch normalization"], [145, 147, "MethodName", "weight decay"], [161, 162, "TaskName", "quantization"], [227, 228, "MethodName", "LSTM"]]}
{"text": "Our quantization scheme was chosen to be uniform , meaning that the step size between two quantized values is constant . This choice , which is an additional constraint , was made for practical reasons . It indeed simplifies all computations required during inference , enabling the exploitation of hardware resources more efficiently . If the performance with uniform quantization is already on par with fullprecision , then more weighty methods are unnecessary . A brief overview of uniform quantization is given in this section . For more details , we refer the reader to Jacob et al ( 2017 ) . Given an element x of a tensor X , we apply the quantization function Q : Q ( x ) = clamp ( x ; x min , xmax ) \u2212 x min s * s + x min ( 1 ) s = xmax \u2212 x min 2 k \u2212 1 ( 2 ) where x min and x max defines the endpoints of the quantization interval . When quantization is applied to weights , these values are respectively min ( X ) and max ( X ) . However , when quantization is applied to activations , those values are running estimates . The latter are computed during training , where for every forward pass , the x min and x max variables are updated via an exponential moving average with a momentum of 0.9 . The clamp function associates all values outside of the [ x min , x max ] range to the closest endpoint and represents rounding to the nearest integer . The value k is simply the bit precision . For example , in the context of 8 - bit quantization , k = 8 . During backpropagation , we use the straightthrough estimator ( Hinton , 2012 ) and set the gradients of clamped values to zero . Once training is finished , s and x min are frozen along with the weights .", "entities": [[1, 2, "TaskName", "quantization"], [12, 14, "HyperparameterName", "step size"], [59, 60, "TaskName", "quantization"], [79, 80, "TaskName", "quantization"], [114, 115, "TaskName", "quantization"], [168, 169, "TaskName", "quantization"], [172, 173, "TaskName", "quantization"], [195, 196, "TaskName", "quantization"], [288, 289, "TaskName", "quantization"], [290, 292, "HyperparameterName", "k ="]]}
{"text": "We choose to quantize all operations which can provide a computational speed gain at inference . In this regard , we quantize all matrix multiplications , meaning that the inputs and weights of MatMuls will both be k - bit quantized . The other operations we quantize are divisions , but only if both the numerator and denominator are second or higher rank tensors . For all other operations , such as sums , the computational cost added by the quantization operation outweighs the benefit of performing the operation with reduced precision . Hence , we do not quantize such operations . More precisely , we quantize all weights of the Transformer , excluding biases . The latter are summed with the INT32 output of matrix multiplications and thus provide no additional computational efficiency from being quantized . Furthermore , the memory space of biases is insignificant in comparison to the weight matrices , representing less than 0.1 % of total weights . For positional embeddings , these are fixed and can thus be quantized once prior to training . The \u03b3 weights of Layer - Norms are also quantized . As for activations , we quantize the sum of the input embeddings with the positional encodings in both the encoder and decoder . In the Multi - Head Attention , we quantize the ( Q , K , V ) input , the softmax 's numerator , the softmax 's denominator , the softmax 's output and the Scaled Dot - Product Attention 's output . At inference , the softmax does not need to be computed in full - precision . Indeed , the exponential function can instead be replaced with a step function . For the position - wise feed - forward networks , we quantize the output of the ReLUs and of the feed - forwards themselves . Finally , for all LayerNorms , we quantize the numerator x\u2212\u00b5 , the denominator \u221a \u03c3 2 + , their quotient and the output of the LayerNorm . A visual guide is provided in appendix A.", "entities": [[80, 81, "TaskName", "quantization"], [111, 112, "MethodName", "Transformer"], [181, 182, "HyperparameterName", "\u03b3"], [216, 220, "MethodName", "Multi - Head Attention"], [234, 235, "MethodName", "softmax"], [239, 240, "MethodName", "softmax"], [244, 245, "MethodName", "softmax"], [249, 254, "MethodName", "Scaled Dot - Product Attention"], [261, 262, "MethodName", "softmax"]]}
{"text": "Instead of using a single set of ( s , x min ) per quantized tensor , we can quantize subsets of the latter with each its own set of ( s , x min ) ( Alistarh et al , 2016 ) . Even though this adds more scalars , the memory cost is insignificant overall . Furthermore , the added flexibility can greatly alleviate the precision loss resulting from all values being mapped to a single low numerical precision domain . We use this bucketing method for all weight matrices , with a number of subset equal to the output dimension . For activations , we use bucketing when quantizing : the sum of input embeddings with the positional encoding , the Q , K , V inputs , the Scaled Dot - Product Attention 's output , the feed - forward 's output , the LayerNorm 's numerator , quotient and output .", "entities": [[68, 69, "MetricName", "loss"], [132, 137, "MethodName", "Scaled Dot - Product Attention"]]}
{"text": "Recently , simple quantization solutions have been applied to the Transformer . Cheong and Daniel ( 2019 ) apply k - means quantization and binarization with two centroids over the weights of the network . For both methods , a look up table associated with each quantized layer is used to map indices to their corresponding centroids . Similarly , Fan ( 2019 ) compares binary , 4 and 8 - bit uniform quantization of the Transformer weights . A big disadvantage with quantizing only the weights of a network is that operations must still be performed in full - precision . Even though the parameters ' memory usage is reduced , these constantly have to be converted back to full - precision . Achieving quantization of both weights and activations is much more beneficial . The first attempt at doing so for the Transformer applies 8 - bit quantization on weights and inputs of feed forward layers and binarizes the ( Q , K ) input of the Multi - Head Attention ( Tierno , 2019 ) . The scaling factor \u221a d k is approximated by a constant which can be computed as a right bitshift . The method resulted in a huge drop in translation accuracy . Achieving better performance , Bhandare et al ( 2019 ) quantize certain MatMul operations and use the KL divergence to estimate the most suited parameters for each quantization range . They restrain from quantizing all MatMuls , reporting poorer results in accuracy . Aside from translation , the concurrent work by Zafrir et al ( 2019 ) quantizes the embedding and fully connected layers of BERT ( Devlin et al , 2018 ) . The Softmax and LayerNorm operations are kept in full - precision . On the GLUE benchmark , their loss in accuracy is minimal compared to the original model . All of these methods omit quantizing the whole Transformer architecture , resulting in suboptimal computational efficiency . Furthermore , these solutions all fail to avoid impairing translation quality . Our method achieves both .", "entities": [[3, 4, "TaskName", "quantization"], [10, 11, "MethodName", "Transformer"], [22, 23, "TaskName", "quantization"], [24, 25, "TaskName", "binarization"], [73, 74, "TaskName", "quantization"], [76, 77, "MethodName", "Transformer"], [125, 126, "TaskName", "quantization"], [144, 145, "MethodName", "Transformer"], [149, 150, "TaskName", "quantization"], [169, 173, "MethodName", "Multi - Head Attention"], [208, 209, "MetricName", "accuracy"], [237, 238, "TaskName", "quantization"], [251, 252, "MetricName", "accuracy"], [275, 276, "MethodName", "BERT"], [285, 286, "MethodName", "Softmax"], [298, 299, "DatasetName", "GLUE"], [302, 303, "MetricName", "loss"], [304, 305, "MetricName", "accuracy"], [321, 322, "MethodName", "Transformer"]]}
{"text": "We apply our quantization strategy on both the base and big Transformer ( Vaswani et al , 2017 ) . The training setup of all presented models is the same as in the original paper , with the exception that the dropout ratio is set to 0.1 in all cases . We refer readers to the original paper for experimental details . Our models were first evaluated on the WMT 2014 / 2017 English - to - German and WMT 2014 English - to - French translation tasks . Reported perplexity is per token and BLEU was measured with multi - bleu.pl 1 on the newstest2014 2 test set . We used beam search with a beam size of 4 and a length penalty of 0.6 . Unlike Vaswani et al ( 2017 ) , no checkpoint averaging was performed . We compare our results with the original Transformer and other 8 - bit quantization methods in Table 1 . All models are base Transformers . Original uncompressed size is the same in all cases . Most work do not report their compressed model size . For those , we give lower bounds based on their reports . Our BLEU score was computed on the test set using the checkpoint with the highest validation accuracy over In Table 2 , we show performance of our method on the WMT14 EN - DE and WMT14 EN - FR for a fixed amount of training steps . We compare our results with two full - precision Transformers : base and big variants . We also evaluate two other quantization approaches . The first one is the \" default \" approach , which is to naively quantize every possible operation . The second approach applies our quantization strategy post - training ( see section 5.3 ) . In all cases except for post - quantization , BLEU was computed on the test set using the checkpoint which scored the highest accuracy on the validation set . Towards the end of training , we ran one validation epoch for every 100 training steps . Baselines and FullyQT 8 - bit results were averaged over 5 trials . Standard deviation of the BLEU scores did not seem higher for any method and ranged between 0.09 and 0.51 . Training with quantization was about twice as slow as with the baselines . As for post - training quantization , the BLEU score was computed on the test set using the best validation performance out of 20 trials . The default approach 's nan in the EN - FR task is due to numerical instability . By quantizing every operation , zeros in the LayerNorm 's denominator are more frequent . Results on additional translation datasets can be found in Table 3 . All models were trained following the same setup as WMT14 EN - FR and WMT14 EN - DE . Vocabulary size is set to 32k for all models . Since there is no test set for WMT14 ES - EN , we used the validation set as a test set and omitted computing any validation epochs during training . Looking at all conducted experiments , including section 5.3 , translation quality of the 8 - bit Ful - lyQT models seems to be on par with full - precision . Most of the time , the highest BLEU was scored by the quantized model . For example in the case of WMT14 EN - DE , the maximum BLEU FullyQT base 8 - bit obtained was 26.98 , while the baseline 's highest was 26.64 . As for the big models , the max FullyQT scored was 27.95 , whereas the baseline 's was 27.43 . We looked at training and validation curves to see if quantization had any effect , but saw no discernible difference . All models use full - precision biases , s and x min . This amounts to 11.61 Mb in the base models and 23.15 Mb in the big models . In the case of 8 - bit , these represent less than 2.35 % of the total size . Without bucketing , this would amount to 2.18 Mb and 4.35 Mb respectively . We believe the small increase in model size to be worth it . Indeed , in section 5.2 , we show that training without bucketing leads to poorer translation . Although 6 - bit quantization seems to perform well , the compression advantage over 8 - bit is usually lost . Most hardware store INT6 using either 8 or 32 bits . Dedicated hardware is needed to get the full compression advantage . Unless 6 - bit quantization results in better models , 8 - bit seems like the best choice for most hardware .", "entities": [[3, 4, "TaskName", "quantization"], [11, 12, "MethodName", "Transformer"], [69, 71, "DatasetName", "WMT 2014"], [79, 81, "DatasetName", "WMT 2014"], [90, 91, "MetricName", "perplexity"], [95, 96, "MetricName", "BLEU"], [148, 149, "MethodName", "Transformer"], [154, 155, "TaskName", "quantization"], [199, 201, "MetricName", "BLEU score"], [214, 215, "MetricName", "accuracy"], [228, 229, "DatasetName", "WMT14"], [233, 234, "DatasetName", "WMT14"], [266, 267, "TaskName", "quantization"], [293, 294, "TaskName", "quantization"], [311, 312, "TaskName", "quantization"], [313, 314, "MetricName", "BLEU"], [327, 328, "MetricName", "accuracy"], [367, 368, "MetricName", "BLEU"], [385, 386, "TaskName", "quantization"], [401, 402, "TaskName", "quantization"], [404, 406, "MetricName", "BLEU score"], [475, 476, "DatasetName", "WMT14"], [480, 481, "DatasetName", "WMT14"], [502, 503, "DatasetName", "WMT14"], [563, 564, "MetricName", "BLEU"], [577, 578, "DatasetName", "WMT14"], [584, 585, "MetricName", "BLEU"], [632, 633, "TaskName", "quantization"], [740, 741, "TaskName", "quantization"], [783, 784, "TaskName", "quantization"]]}
{"text": "To better understand which operations are more sensitive to quantization , we evaluate such effect on single operations of the Transformer . By this , we mean quantizing the operation of a module for all Transformer layers . Table 4 shows results on the WMT14 EN - FR translation task for 8 - bit precision . The effect of bucketing was also evaluated . BLEU was computed on the test set after 100k steps of training . In 24 out of 27 experiments , performance was better than our full - precision baseline of 38.34 BLEU . Solely quantizing the LayerNorm 's denominator with no bucketing results in poor performance . The latter also can not be bucketed since all dimensions of the variance tensor vary per batch . To successfully quantize this element without causing any loss in performance , we suspect quantizing other elements in the network helps . To further validate our quantization scheme , we evaluated four models trained with alterations to our design choices . Results on the WMT14 EN - FR task are presented in Table 5 . All models are 8 - bit quantized base Transformers . Training procedure is the same as in section 5.1 .", "entities": [[9, 10, "TaskName", "quantization"], [20, 21, "MethodName", "Transformer"], [35, 36, "MethodName", "Transformer"], [44, 45, "DatasetName", "WMT14"], [64, 65, "MetricName", "BLEU"], [95, 96, "MetricName", "BLEU"], [137, 138, "MetricName", "loss"], [155, 156, "TaskName", "quantization"], [173, 174, "DatasetName", "WMT14"]]}
{"text": "Our method 's goal is to increase computational efficiency when inferring with the Transformer . To this end , our quantization scheme only requires us to learn s and x min . Although we do so throughout the whole training , this is not a necessity . Quantization could also be applied later during training . Results for different starting points are compared in Table 6 . The earliest we start quantizing is at 100 steps , since we need at least a few steps to assess the running estimates . All models were evaluated on the WMT14 EN - DE and WMT14 EN - FR translation tasks . BLEU was measured on the test set using the checkpoint which scored the highest accuracy on the validation set during training . Validation was computed every 100 training steps towards the end of training . From our observed results , quantizing the model early on seems preferable . Learning quantization parameters adds a significant computational cost during training . A major advantage to delaying quantization is to perform more training steps in the same given amount of time . Therefore , when training time is a constraint , a possible strategy is to train a model without quantization , perform more training steps and finally post - quantize the model . By the latter , we mean keeping all weights fixed and compute the s and x min over a few hundred steps . This is another advantage , since many trials can be performed in search of the best performing candidate . We found post - quantization BLEU scores to vary by about 0.2 BLEU .", "entities": [[13, 14, "MethodName", "Transformer"], [20, 21, "TaskName", "quantization"], [47, 48, "TaskName", "Quantization"], [97, 98, "DatasetName", "WMT14"], [102, 103, "DatasetName", "WMT14"], [109, 110, "MetricName", "BLEU"], [123, 124, "MetricName", "accuracy"], [158, 159, "TaskName", "quantization"], [173, 174, "TaskName", "quantization"], [206, 207, "TaskName", "quantization"], [266, 267, "TaskName", "quantization"], [267, 268, "MetricName", "BLEU"], [274, 275, "MetricName", "BLEU"]]}
{"text": "To evaluate if our quantization scheme generalizes well to other tasks , we evaluate it on two language modeling datasets : WikiText - 2 and WikiText - 103 . As the setup , we use PyTorch 's language modeling toy example 3 . The task consists of predicting the sequence { x t+1 , , x t+n+1 } from the input sequence { x t , , x t+n } . We trained four Transformer models , each with different precision . All models consist of two Transformer encoder layers , with the embedding and hidden size set to 200 . Multi - Head Attention has two heads with key and value size 64 . The final word projection layer 's weights are shared with the embedding layer . Models were trained for 10 epochs with a batch size of 20 and sequence length of 35 . Learning rate is set to 5 , dropout to 0.2 and gradient clipping to 0.25 . Loss is computed on every element of the output sequence . Results are presented in Table 7 . Validation was computed every epoch to determine the best candidate . Loss and perplexity are computed on the test set and averaged over 10 trials for WikiText - 2 and 3 trials for WikiText - 3 . See footnote 3 for any extra details .", "entities": [[4, 5, "TaskName", "quantization"], [74, 75, "MethodName", "Transformer"], [87, 88, "MethodName", "Transformer"], [101, 105, "MethodName", "Multi - Head Attention"], [137, 139, "HyperparameterName", "batch size"], [147, 149, "HyperparameterName", "Learning rate"], [158, 160, "MethodName", "gradient clipping"], [194, 195, "MetricName", "perplexity"]]}
{"text": "We experiment with node pruning our Transformer models . Once the model is fully trained and quantized , we can further compress it by removing useless nodes . By useless , we mean nodes which do not cause any loss in translation quality when removed . We choose to prune nodes instead of independently pruning weights . The latter method usually requires special hardware or software to leverage sparse weight matrices . Pruning nodes results in concretely shrunken models . When getting rid of a node , we remove its corresponding set of weights from the layer outputting it and the following layer receiving the node as input . The only nodes of the Transformer which can be removed without causing alterations to other components of the network are the nodes in between the two layers of each feed - forward network . Fortunately , these consist of a substantial portion of the model 's weights . In the case of the base Transformer , for a respective vocabulary of size 37000 and 32000 , 39.96 % and 41.65 % of the total weights are owned by the feed - foward networks . This number grows to 47.03 % and 48.18 % in the big Transformer . To evaluate which nodes can be safely pruned without affecting translation quality , we estimate x max for each node of the ReLU output over a few hundred steps . This is done on the training set , using the fully trained model and keeping all other weights frozen . These x max are computed before quantizing the ReLU output and do not replace the ones used by the quantization process . Figure 3 in the appendix shows the histogram of these running estimates for one ReLU layer in the encoder and one in the decoder . All other ReLU layers share the same pattern , where in the encoder there are always multiple x max close to 0 . This does not happen in the decoder . Once the running estimates are computed , we prune its corresponding node if x max < z\u03c3 where z is a hyperparameter and \u03c3 the standard deviation of the layer 's x max . We empirically found z = 0.025 to work well , with higher thresholds causing BLEU to quickly decay . No retraining of the model is performed after pruning nodes . Using this method , we can further compress the Transformer without affecting BLEU scores . Our approach has the advantage of being adaptive , meaning the number of nodes pruned per layer will differ as opposed to a fixed pruning ratio method . For example , in the case of the big Transformer trained on WMT14 EN - FR , 169 nodes were pruned in the first ReLU of the encoder , while in the second , 1226 were pruned . Nodes in the decoder rarely got pruned , at most 4 in the whole decoder . Results are presented in Table 8 . Reported results are averaged on the test set over a few trials . BLEU varied by about 0.01\u22120.02 . Other approaches usually decide the ratio first and then prune . We compared with two such methods . For each task , we fix their ratio to the average percentage of nodes pruned by our method and only prune nodes in the encoder . The first fixed pruning method uses L1 - norm to sort nodes in ascending weight order , while the second sorts the x max , also in ascending order .", "entities": [[6, 7, "MethodName", "Transformer"], [39, 40, "MetricName", "loss"], [114, 115, "MethodName", "Transformer"], [163, 164, "MethodName", "Transformer"], [205, 206, "MethodName", "Transformer"], [229, 230, "MethodName", "ReLU"], [265, 266, "MethodName", "ReLU"], [276, 277, "TaskName", "quantization"], [293, 294, "MethodName", "ReLU"], [306, 307, "MethodName", "ReLU"], [325, 326, "DatasetName", "0"], [383, 384, "MetricName", "BLEU"], [408, 409, "MethodName", "Transformer"], [411, 412, "MetricName", "BLEU"], [451, 452, "MethodName", "Transformer"], [454, 455, "DatasetName", "WMT14"], [466, 467, "MethodName", "ReLU"], [516, 517, "MetricName", "BLEU"]]}
{"text": "We proposed a full quantization strategy for the Transformer architecture . Our objective was to ex - ploit hardware resources as efficiently as possible , quantizing all operations which could provide a computational speed gain . With FullyQT , we achieve higher BLEU scores than all other quantization methods for the Transformer on multiple translation tasks and avoid any loss in BLEU compared to full - precision . Specifically , out of 35 experiments , 8 - bit quantization performed better than full - precision in 21 cases . If instead of minimizing inference time , one wants to maximize translation accuracy , then applying quantization to only certain components of the Transformer seems to be the best option . Indeed , our ablation study showed than BLEU score could increase even more when only specific elements of the Transformer were quantized . Further gains might be possible , but supplementary experiments would be necessary to determine the best combination . We plan on extending our work to variations of the Transformer , as well as further exploring the compression of these networks .", "entities": [[4, 5, "TaskName", "quantization"], [8, 9, "MethodName", "Transformer"], [42, 43, "MetricName", "BLEU"], [47, 48, "TaskName", "quantization"], [51, 52, "MethodName", "Transformer"], [59, 60, "MetricName", "loss"], [61, 62, "MetricName", "BLEU"], [78, 79, "TaskName", "quantization"], [101, 102, "MetricName", "accuracy"], [105, 106, "TaskName", "quantization"], [112, 113, "MethodName", "Transformer"], [127, 129, "MetricName", "BLEU score"], [139, 140, "MethodName", "Transformer"], [171, 172, "MethodName", "Transformer"]]}
{"text": "The difference between @ i A and # A is that while @ i A searches for an antecedent of type A in a given local context via anaphora resolution , # A introduces an object of type A via the following rule for # - elimination : ( 18 ) # - elimination : Let \u03d5 be a term containing # A as a subterm , where A is a type in which no @ - term nor # - term occurs . Suppose that we have a derivation of the form on the left , where \u03d5 : t is the first node that has type t and depends on # A : A , i.e. , no other judgement of the form \u03c8 : t appears in D 2 . Then the derivation can be transformed into the one on the right : . . . . D 1 A : t # A : A # . . . . D 2 \u03d5 : t . . . . D 1 A : t [ u : A ] n . . . . D 2 [ u/#A ] \u03d5 [ u/#A ] : t ( u : A ) \u00d7 \u03d5 [ u/#A ] : t \u03a3F , n By this rule , if there is a branch containing an underspecified term # A , one can close it by taking the existence of an object of type A as part of the asserted proposition represented as a \u03a3 - type of the form ( u : A ) \u00d7 \u03d5 [ u/#A ] , where \u03d5 [ u/#A ] is the expression obtained by replacing the occurrence of # A in \u03d5 by u. In the case of ( 16 ) , the initial derivation shown on the left in ( 19 ) is transformed to the derivation on the right by # - elimination , so we end up with the same representation as ( 6 ) . ( 19 ) [ x : e ] 1 man ( x ) : t man * : t \u03a3F , 1 # man * : man * # \u03c0 1 ( # man * ) : e \u03a3E enter ( \u03c0 1 ( # man * ) ) : t [ x : e ] 1 man ( x ) : t man * : t \u03a3F , 1 [ u : man * ] 1 \u03c0 1 ( u ) : e \u03a3E enter ( \u03c0 1 ( u ) ) : t ( u : man * ) \u00d7 enter ( \u03c0 1 ( u ) ) : t \u03a3F , 1 Thus # - elimination allows us to eliminate a # - term from a type and rewrite it to a \u03a3 - type . For notational convenience , we write this transformation as enter ( \u03c0 1 ( # man * ) ) # ( u : man * ) \u00d7 enter ( \u03c0 1 ( u ) ) . It should be clear from the above that DTS crucially makes use of underspecification in the interpretations of both pronouns and indefinites . We make the following two assumptions about the way underspecified terms are interpreted in the course of semantic composition : ( 20 ) a. Ban on the duplication of underspecified terms : In a well - formed semantic representation of DTS , an underspecified @ - term with the same index can appear at most once . b. Normal form requirement on compositionally derived semantic terms : At each step of semantic composition , the semantic term assigned to the derived linguistic expression is in \u03b2 - normal form . These restrictions can be thought of as embodying a general requirement that underspecification resolution is not totally unconstrained but is affected by the form of the sentences in which underspecified expressions occur . As such , these restrictions play key roles in the analysis of scope parallelism in the next section .", "entities": [[550, 552, "TaskName", "semantic composition"], [604, 606, "TaskName", "semantic composition"], [618, 619, "HyperparameterName", "\u03b2"]]}
{"text": "With the treatment of anaphora and indefinites introduced above , the interpretive parallelism exemplified by ( 1 ) and ( 2 ) falls out automatically as a consequence of the way underspecification is resolved in DTS . Unlike previous proposals ( Asudeh and Crouch 2002 ; Steedman 2012 ) , no extra assumption is needed beyond the simple restriction ( 20 ) on underspecification resolution introduced in the previous section . We start with the pronominal binding case . To avoid the issue of possessives ( which is itself a complex problem ) , we illustrate the analysis with the following example involving an embedded clause : ( 21 ) Every Englishman thinks , and every American believes , that he is a genius . One technical issue that needs to be addressed first is how to obtain the bound reading for the pronoun in the RNR'ed position to begin with . Note that given the prohibition on the duplication of underspecified terms in DTS , the simple derivation for ( 21 ) in ( 22 ) can not yield the bound reading for the pronoun . 5 ( 22 Here , for the pronoun in the RNR'ed S to be bound by the subject quantifiers in each conjunct , the term gen ( @ 1 e ) first needs to be substituted for variable p in each conjunct ( from where the antecedent is syntactically visible , given the definition of anaphora resolution from section 2 ) , but this is precisely the move that is prohibited by the ' no duplication of underspecified term ' restriction . This means that , in order to obtain the bound reading , we need a slightly more complex syntactic derivation involving ( syntactic ) type - lifting of both the RNR'ed material and the conjuncts . The effect in a nutshell is that , via type - lifting , we can ensure enough of the ' derivational structure ' of the sentence to be present in the ( beta - unreduced ) semantic translation to identify the ' possible binder ' of the pronoun before all the material is actually composed in the ( surface ) syntax . The derivation for the bound pronoun reading for ( 21 ) thus goes as in ( 23 ) . 5 We adopt the abbreviation A eng = def \u03bbP. [ ( u : eng * ) P ( \u03c01 ( u ) ) ] , etc . These abbreviations are unpacked at the end of the derivation ( via the step designated by the dotted line , which is not part of the syntactic derivation ) for clarity of presentation . Type checking for the semantic representation involves a branch to check the type of \u03bbR\u03bbx . R ( gen ( @1e ) ) ( x ) , which is shown on the left below . Here the assumption e true follows from the hypothesis x : e and thus by @ - elimination we can replace @ 1 e with x throughout the derivation . S / S that he is a genius ; gen ( @1e ) ; S /E every Englishman thinks and every American believes that he is a genius ; \u03bbp [ ( v : A eng ( \u03bby.think ( p ) ( y ) ) ) \u00d7 A am ( 5 [ \u03d54 ; R ; VP / S ] 4 that he is a genius ; gen ( @1e ) ; S /E \u03d54 that he is a genius ; R ( gen ( @1e ) ) ; VP \\E \u03d55 \u03d54 that he is a genius ; R ( gen ( @1e ) ) ( x ) ; S I 5 \u03d54 that he is a genius ; \u03bbx . R ( gen ( @1e ) ) ( x ) ; VP I 4 that he is a genius ; \u03bbR\u03bbx . R ( gen ( @1e ) ) ( x ) ; ( VP / S ) \\VP /E every Englishman thinks and every American believes that he is a genius ; \u03bbP [ ( v : A eng ( ( 24 ) [ x : e ] 2 e true @ 1 e : e @ gen ( @ 1 e ) : t [ x : e ] 2 R ( gen ( @ 1 e ) ) ( x ) : t [ R : t e t ] 1 \u03bbx . R ( gen ( @ 1 e ) ) ( x ) : e t \u03a0I , 2 \u03bbR\u03bbx . R ( gen ( @ 1 e ) ) ( x ) : ( t e t ) e t \u03a0I , 1 [ x : e ] 2 gen ( x ) : t [ x : e ] 2 R ( gen ( x ) ) ( x ) : t [ R : t e t ] 1 \u03bbx . R ( gen ( x ) ) ( x ) : e t \u03a0I , 2 \u03bbR\u03bbx . R ( gen ( x ) ) ( x ) : ( t e t ) e t \u03a0I , 1 Note crucially that here the underspecification for the pronoun term @ 1 e is resolved before the meaning contribution of the RNR'ed S which contains it as a subterm is copies into each conjunct via \u03b2 - reduction . The underspecified term identifies the ( \u03bb - bound ) subject x of the upstairs clause as its antecedent . After @ - elimination and \u03b2 - reduction , we obtain the final translation in ( 25 ) , which corresponds to the parallel bound reading for the sentence . For the parallel free pronoun reading , the simpler derivation in ( 22 ) would suffice . Since \u03b2 - reduction is prohibited before underspecification resolution , type checking for the underspecified term searches for an appropriate antecedent in the global context ( consisting of the previous linguistic discourse and extra - linguistic information ) . For concreteness , we assume that the previous utterance was Bobby Fisher is a famous American chess player , and that the judgement bf : e is in the global context . The previous sentence thus provides an antecedent and @ 1 e in ( 22 ) is bound to bf . By \u03b2 - reducing the term after anaphora resolution , we obtain ( 26 ) , where the pronoun refers to Bobby Fisher in each conjunct . ( 26 ) ( ( u : eng * ) think ( gen ( bf ) ( \u03c0 1 ( u ) ) ) ) \u00d7 ( ( u : am * ) believe ( gen ( bf ) ) ( \u03c0 1 ( u ) ) ) ) The quantifier scope case is somewhat different at the level of technical implementations , but at the broader conceptual level , is essentially similar to the pronoun case in that interpretive parallelism falls out from the constraints pertaining to underspecification resolution in the derivation of compositional semantics . Note first that , unlike the case for @ - terms , we do n't need to ensure that the derivationally obtained local context is ' large enough ' to contain the ' antecedent ' . Thus , the following simple derivation suffices to yield both the wide - scope and narrow - scope readings for the RNR'ed indefinite : ( 27 ) . . . Since # - terms do not carry indices , in the case of indefinites , interpretive parallelism follows not from the ban on duplicating indexed underspecified terms ( whose role was to ensure ' construal identity ' in anaphora resolution ) , but from an interaction of the normal form requirement for derived semantic terms and the locality requirement on underspecification resolution encoded in the # - elimination rule ( 18 ) . Specifically , there are two possible ways for resolving underspecification for the # - term in the semantic translation for the sentence obtained at the final line of ( 27 ) . If we resolve underspecification before \u03b2 - reducing the term , we obtain the wide scope reading for the indefinite as in ( 28 ) : ( 28 ) \u03bbx [ ( v : ( u : boy * ) admire ( x ) ( \u03c0 1 ( u ) ) ) \u00d7 ( ( u : girl * ) hate ( x ) ( \u03c0 1 ( u ) ) ) ] ( # sax * ) # ( t : sax * ) \u00d7 \u03bbx [ ( v : ( u : boy * ) admire ( x ) ( \u03c0 1 ( u ) ) ) \u00d7 ( ( u : girl * ) hate ( x ) ( \u03c0 1 ( u ) ) ) ] ( \u03c0 1 ( t ) ) \u03b2 ( t : sax * ) \u00d7 [ ( v : ( u : boy * ) admire ( \u03c0 1 ( t ) ) ( \u03c0 1 ( u ) ) ) \u00d7 ( ( u : girl * ) hate ( \u03c0 1 ( t ) ) ( \u03c0 1 ( u ) ) ) ] If , on the other hand , we first \u03b2 - reduce the term and then resolve underspecification , the \u03a3 - type that has the existential force associated with the indefinite is introduced in the smallest local context in each conjunct , via ( 18 ) . In this case , the distributive , narrow scope reading obtains for the sentence . ( 29 ) \u03bbx [ ( v : ( u : boy * ) admire ( x ) ( \u03c0 1 ( u ) ) ) \u00d7 ( ( u : girl * ) hate ( x ) ( \u03c0 1 ( u ) ) ) ] ( # sax * ) \u03b2 ( v : ( u : boy * ) admire ( # sax * ) ( \u03c0 1 ( u ) ) ) \u00d7 ( ( u : girl * ) hate ( # sax * ) ( \u03c0 1 ( u ) ) ) # ( v : ( u : boy * ) ( t : sax * ) \u00d7 admire ( \u03c0 1 ( t ) ) ( \u03c0 1 ( u ) ) ) \u00d7 ( ( u : girl * ) ( t : sax * ) \u00d7 hate ( \u03c0 1 ( t ) ) ( \u03c0 1 ( u ) ) ) One may wonder at this point why we impose the normal form requirement on compositionally derived semantic terms . To see why this requirement is needed , assume that no \u03b2 - reduction takes place in the course of the derivataion , and , ( as above ) that once the semantic representation for the whole sentence is obtained , there is no restriction on the order of \u03b2 - reduction and underspecification resolution for # terms . The following translation would then be assigned to the sentence , and via the underspecification resolution in ( 30 ) , a mixed scope reading would incorrectly be predicted to be available : In short , assumption ( 20b ) has the effect of eliminating unnecessary ' traces ' of derivational history to make unavailable intermediate scope positions that do not reflect the surface form of the sentence . Our proposal treats indefinites as underspecified terms and universals as true quantifiers , and in this respect , resembles the approach by Steedman ( 2012 ) . Unlike Steedman 's approach , which interleaves underspecification resolution with CCG syntactic combinatorics , our approach separates semantic underspecification resolution from syntax . Nonetheless , the similarity between the two is striking , and it is interesting to note that they both predict that mixed readings are available for examples involving indefinites as subjects and a universal quantifier in the RNR'ed position , such as the following : ( 31 ) Some boy loves , and some girl detests , every saxophonist . The judgments are somewhat subtle due to the independent pragmatic preference for parallel readings , but we follow Steedman ( 2012 ) in taking this prediction to be essentially correct . One translation that our analysis can assign to ( 31 ) is the following : Here , \u03b2 - conversion for the \u03bb - bound variables y , z and P can take place in any order , and the relative scope between the subject indefinites and the object universal depends on the order of application of \u03b2 - conversion and underspecification resolution for the two terms # boy and # girl .", "entities": [[337, 338, "HyperparameterName", "beta"], [908, 909, "HyperparameterName", "\u03b2"], [937, 938, "HyperparameterName", "\u03b2"], [980, 981, "HyperparameterName", "\u03b2"], [1071, 1072, "HyperparameterName", "\u03b2"], [1370, 1371, "HyperparameterName", "\u03b2"], [1502, 1503, "HyperparameterName", "\u03b2"], [1570, 1571, "HyperparameterName", "\u03b2"], [1676, 1677, "HyperparameterName", "\u03b2"], [1816, 1817, "HyperparameterName", "\u03b2"], [1854, 1855, "HyperparameterName", "\u03b2"], [2091, 2092, "HyperparameterName", "\u03b2"], [2131, 2132, "HyperparameterName", "\u03b2"]]}
{"text": "We train our TOD - BERT based on BERT architecture using two loss functions : masked language modeling ( MLM ) loss and response contrastive loss ( RCL ) . Note that the datasets we used can be used to pre - train any existing language model architecture , and here we select BERT because it is the most widely used model in NLP research . We use the BERT - base uncased model , which is a transformer self - attention encoder ( Vaswani et al , 2017 ) with 12 layers and 12 attention heads with its hidden size d B = 768 . To capture speaker information and the underlying interaction behavior in dialogue , we add two special tokens , [ USR ] and [ SYS ] , to the bytepair embeddings ( Mrk\u0161i\u0107 et al , 2016 ) . We prefix the special token to each user utterance and system response , and concatenate all the utterances in the same dialogue into one flat sequence , as shown in Figure 1 . For example , for a dialogue D = { S 1 , U 1 , . . . , S n , U n } , where n is the number of dialogue turns and each S i or U i contains a sequence of words , the input of the pre - training model is processed as \" [ SYS ] S 1 [ USR ] U 1 . . . \" with standard positional embeddings and segmentation embeddings . Masked language modeling is a common pretraining strategy for BERT - like architectures , in which a random sample of tokens in the input sequence is selected and replaced with the special token [ MASK ] . The MLM loss function is the crossentropy loss on predicting the masked tokens . In the original implementation , random masking and replacement are performed once in the beginning and saved for the training duration . Here we conduct token masking dynamically during batch training . TOD - BERT is initialized from BERT , a good starting parameter set , then is further pre - trained on those task - oriented corpora . The MLM loss function is L mlm = \u2212 M m=1 log P ( x m ) , ( 1 ) where M is the total number of masked tokens and P ( x m ) is the predicted probability of the token x m over the vocabulary size . Response contrastive loss can also be used for dialogue language modeling since it does not require any additional human annotation . Pretraining with RCL can bring us several advantages : 1 ) we can learn a better representation for the [ CLS ] token , as it is essential for all the downstream tasks , and 2 ) we encourage the model to capture underlying dialogue sequential order , structure information , and response similarity . Unlike the original next sentence prediction ( NSP ) objective in BERT pre - training , which concatenates two segments A and B to predict whether they are consecutive text with binary classification , we apply a dual - encoder approach ( Henderson et al , 2019a ) and simulate multiple negative samples . We first draw a batch of dialogues { D 1 , . . . , D b } and split each dialogue at a randomly selected turn t. For example , D 1 will be separated into two segments , one is the context { S 1 1 , U 1 1 , . . . , S 1 t , U 1 t } and the other is the response { S 1 t+1 } . We use TOD - BERT to encode all the contexts and their corresponding responses separately . Afterwards , we have a context matrix C R b\u00d7d B and a response matrix R R b\u00d7d B by taking the output [ CLS ] representations from the b dialogues . We treat other responses in the same batch as randomly selected negative samples . The RCL objective function is L rcl = \u2212 b i=1 log M i , i , M = Softmax ( CR T ) R b\u00d7b . ( 2 ) Increasing batch size to a certain amount can obtain better performance on downstream tasks , especially for the response selection . The Softmax function normalizes the vector per row . In our setting , increasing batch size also means changing the positive and negative ratio in the contrastive learning . Batch size is a hyper - parameter that may be limited by hardware . We also try different negative sampling strategies during pre - training such as local sampling ( Saeidi et al , 2017 ) , but do not observe significant change compared to random sampling . Overall pre - training loss function is the weighted - sum of L mlm and L rcl , and in our experiments , we simply sum them up . We gradually reduce the learning rate without a warm - up period . We train TOD - BERT with AdamW ( Loshchilov and Hutter , 2017 ) optimizer with a dropout ratio of 0.1 on all layers and attention weights . GELU activation functions ( Hendrycks and Gimpel , 2016 ) is used . Models are early - stopped using perplexity scores of a held - out development set , with mini - batches containing 32 sequences of maximum length 512 tokens . Experiments are conducted on two NVIDIA Tesla V100 GPUs .", "entities": [[5, 6, "MethodName", "BERT"], [8, 9, "MethodName", "BERT"], [12, 13, "MetricName", "loss"], [15, 18, "TaskName", "masked language modeling"], [19, 20, "DatasetName", "MLM"], [21, 22, "MetricName", "loss"], [25, 26, "MetricName", "loss"], [53, 54, "MethodName", "BERT"], [69, 70, "MethodName", "BERT"], [258, 261, "TaskName", "Masked language modeling"], [267, 268, "MethodName", "BERT"], [296, 297, "DatasetName", "MLM"], [297, 298, "MetricName", "loss"], [302, 303, "MetricName", "loss"], [343, 344, "MethodName", "BERT"], [347, 348, "MethodName", "BERT"], [369, 370, "DatasetName", "MLM"], [370, 371, "MetricName", "loss"], [374, 375, "DatasetName", "mlm"], [420, 421, "MetricName", "loss"], [505, 506, "MethodName", "BERT"], [629, 630, "MethodName", "BERT"], [706, 707, "MethodName", "Softmax"], [718, 720, "HyperparameterName", "batch size"], [739, 740, "MethodName", "Softmax"], [752, 754, "HyperparameterName", "batch size"], [764, 766, "MethodName", "contrastive learning"], [767, 769, "HyperparameterName", "Batch size"], [819, 820, "MetricName", "loss"], [828, 829, "DatasetName", "mlm"], [848, 850, "HyperparameterName", "learning rate"], [861, 862, "MethodName", "BERT"], [863, 864, "MethodName", "AdamW"], [871, 872, "HyperparameterName", "optimizer"], [885, 886, "MethodName", "GELU"], [904, 905, "MetricName", "perplexity"]]}
{"text": "We care the most in this paper whether TOD - BERT , a pre - trained language model using aggregated taskoriented corpora , can show any advantage over BERT . Therefore , we avoid adding too many additional components on top of its architecture when fine - tuning on each downstream task . Also , we use the same architecture with a similar number of parameters for a fair comparison . All the model parameters are updated with a gradient clipping to 1.0 using the same hyper - parameters during finetuning . We select four crucial task - oriented downstream tasks to evaluate : intent recognition , dialogue state tracking , dialogue act prediction , and response selection . All of them are core components in modularized task - oriented systems . Intent recognition task is a multi - class classification problem , where we input a sentence U and models predict one single intent class over I possible intents . P int = Softmax ( W 1 ( F ( U ) ) ) R I , ( 3 ) where F is a pre - trained language model and we use its [ CLS ] embeddings as the output representation . W 1 R I\u00d7d B is a trainable linear mapping . The model is trained with cross - entropy loss between the predicted distributions P int and the true intent labels . Dialogue state tracking can be treated as a multi - class classification problem using a predefined ontology . Unlike intent , we use dialogue history X ( a sequence of utterances ) as input and a model predicts slot values for each ( domain , slot ) pair at each dialogue turn . Each corresponding value v j i , the i - th value for the j - th ( domain , slot ) pair , is passed into a pre - trained model and fixed its representation during training . S j i = Sim ( G j ( F ( X ) ) , F ( v j i ) ) R 1 , ( 4 ) where Sim is the cosine similarity function , and S j R | v j | is the probability distribution of the j - th ( domain , slot ) pair over its possible values . G j is the slot projection layer of the j slot , and the number of layers | G | is equal to the number of ( domain , slot ) pairs . The model is trained with cross - entropy loss summed over all the pairs . Dialogue act prediction is a multi - label classification problem because a system response may contain multiple dialogue acts , e.g. , request and inform at the same time . Model take dialogue history as input and predict a binary result for each possible dialogue act : A = Sigmoid ( W 2 ( F ( X ) ) ) R N , ( 5 ) where W 2 R d B \u00d7N is a trainable linear mapping , N is the number of possible dialogue acts , and each value in A is between [ 0 , 1 ] after a Sigmoid layer . The model is trained with binary cross - entropy loss and the i - th dialogue act is considered as a triggered dialogue act if A i > 0.5 . Response selection is a ranking problem , aiming to retrieve the most relative system response from a candidate pool . We use a dual - encoder strategy ( Henderson et al , 2019b ) and compute similarity scores between source X and target Y , r i = Sim ( F ( X ) , F ( Y i ) ) R 1 , ( 6 ) where Y i is the i - th response candidate and r i is its cosine similarity score . Source X can be truncated , and we limit the context lengths to the most recent 256 tokens in our experiments . We randomly sample several system responses from the corpus as negative samples . Although it may not be a true negative sample , it is common to train a ranker and evaluate its results ( Henderson et al , 2019a ) .", "entities": [[10, 11, "MethodName", "BERT"], [28, 29, "MethodName", "BERT"], [63, 66, "HyperparameterName", "number of parameters"], [79, 81, "MethodName", "gradient clipping"], [104, 106, "TaskName", "intent recognition"], [107, 110, "TaskName", "dialogue state tracking"], [132, 134, "TaskName", "Intent recognition"], [137, 141, "TaskName", "multi - class classification"], [164, 165, "MethodName", "Softmax"], [222, 223, "MetricName", "loss"], [235, 238, "TaskName", "Dialogue state tracking"], [243, 247, "TaskName", "multi - class classification"], [251, 252, "MethodName", "ontology"], [405, 408, "HyperparameterName", "number of layers"], [432, 433, "MetricName", "loss"], [444, 448, "TaskName", "multi - label classification"], [535, 536, "DatasetName", "0"], [553, 554, "MetricName", "loss"]]}
{"text": "For each downstream task , we first conduct the experiments using the whole dataset , and then we simulate the few - shot setting to show the strength of our TOD - BERT . We run at least three times with different random seeds for each few - shot experiment to reduce data sampling variance , and we report its mean and standard deviation for these limited data scenarios . We investigate two versions of TOD - BERT ; one is TOD - BERT - mlm that only uses MLM loss during pre - training , and the other is TOD - BERT - jnt , which is jointly trained with the MLM and RCL objectives . We compare TOD - BERT with BERT and other baselines , including two other strong pre - training models GPT - 2 ( Radford et al , 2019 ) and DialoGPT ( Zhang et al , 2019 ) . For a GPT - based model , we use mean pooling of its hidden states as its output representation , which we found it is better than using only the last token .", "entities": [[32, 33, "MethodName", "BERT"], [43, 44, "DatasetName", "seeds"], [77, 78, "MethodName", "BERT"], [83, 84, "MethodName", "BERT"], [85, 86, "DatasetName", "mlm"], [89, 90, "DatasetName", "MLM"], [90, 91, "MetricName", "loss"], [102, 103, "MethodName", "BERT"], [112, 113, "DatasetName", "MLM"], [121, 122, "MethodName", "BERT"], [123, 124, "MethodName", "BERT"], [136, 137, "MethodName", "GPT"], [158, 159, "MethodName", "GPT"]]}
{"text": "TOD - BERT outperforms BERT and other strong baselines in one of the largest intent recognition Model Acc ( all ) Acc ( in ) Acc ( out ) Recall ( out ) 1 - Shot BERT 29.3 % \u00b1 3.4 % 35.7 % \u00b1 4.1 % 81.3 % \u00b1 0.4 % 0.4 % \u00b1 0.3 % TOD - BERT - mlm 38.9 % \u00b1 6.3 % 47.4 % \u00b1 7.6 % 81.6 % \u00b1 0.2 % 0.5 % \u00b1 0.2 % TOD - BERT - jnt 42.5 % \u00b1 0.1 % 52.0 % \u00b1 0.1 % 81.7 % \u00b1 0.1 % 0.1 % \u00b1 0.1 % 10 - Shot BERT 75.5 % \u00b1 1.1 % 88.6 % \u00b1 1.1 % 84.7 % \u00b1 0.3 % 16.5 % \u00b1 1.7 % TOD - BERT - mlm 76.6 % \u00b1 0.8 % 90.5 % \u00b1 1.2 % 84.3 % \u00b1 0.2 % 14.0 % \u00b1 1.3 % TOD - BERT - jnt 77.3 % \u00b1 0.5 % 91.0 % \u00b1 0.5 % 84.5 % \u00b1 0.4 % 15.3 % \u00b1 2.1 % datasets , as shown in Table 2 . We evaluate accuracy on all the data , the in - domain intents only , and the out - of - scope intent only . Note that there are two ways to predict out - of - scope intent , one is to treat it as an additional class , and the other is to set a threshold for prediction confidence . Here we report the results of the first setting . TOD - BERTjnt achieves the highest in - scope and out - of - scope accuracy . Besides , we conduct 1 - shot and 10 - shot experiments by randomly sampling one and ten utterances from each intent class in the training set . TOD - BERT - jnt has 13.2 % all - intent accuracy improvement and 16.3 % in - domain accuracy improvement compared to BERT in the 1 - shot setting .", "entities": [[2, 3, "MethodName", "BERT"], [4, 5, "MethodName", "BERT"], [14, 16, "TaskName", "intent recognition"], [17, 18, "MetricName", "Acc"], [21, 22, "MetricName", "Acc"], [25, 26, "MetricName", "Acc"], [29, 30, "MetricName", "Recall"], [36, 37, "MethodName", "BERT"], [59, 60, "MethodName", "BERT"], [61, 62, "DatasetName", "mlm"], [84, 85, "MethodName", "BERT"], [110, 111, "MethodName", "BERT"], [133, 134, "MethodName", "BERT"], [135, 136, "DatasetName", "mlm"], [158, 159, "MethodName", "BERT"], [191, 192, "MetricName", "accuracy"], [276, 277, "MetricName", "accuracy"], [308, 309, "MethodName", "BERT"], [317, 318, "MetricName", "accuracy"], [325, 326, "MetricName", "accuracy"], [329, 330, "MethodName", "BERT"]]}
{"text": "Two evaluation metrics are commonly used in dialogue state tracking task : joint goal accuracy and slot accuracy . The joint goal accuracy compares the predicted dialogue states to the ground truth at each dialogue turn . The ground truth includes slot values for all the possible ( domain , slot ) pairs . The output is considered as a correct prediction if and only if all the predicted values exactly match its ground truth values . On the other hand , the slot accuracy individually compares each ( domain , slot , value ) triplet to its ground truth label . In Table 5 , we compare BERT to TOD - BERTjnt on the MWOZ 2.1 dataset and find the latter has 2.4 % joint goal accuracy improvement . Since the original ontology provided by Budzianowski et al ( 2018 ) is not complete ( some labeled values are not included in the ontology ) , we create a new ontology of all the possible annotated values . We also list several well - known dialogue state trackers as reference , including DSTReader , HyST , TRADE , and ZSDST ( Rastogi et al , 2019 ) . We also report the few - shot experiments using 1 % , 5 % , 10 % , and 25 % data . Note that 1 % of data has around 84 dialogues . TOD - BERT outperforms BERT in all the setting , which further show the strength of task - oriented dialogue pre - training .", "entities": [[7, 10, "TaskName", "dialogue state tracking"], [14, 15, "MetricName", "accuracy"], [17, 18, "MetricName", "accuracy"], [22, 23, "MetricName", "accuracy"], [84, 85, "MetricName", "accuracy"], [108, 109, "MethodName", "BERT"], [127, 128, "MetricName", "accuracy"], [133, 134, "MethodName", "ontology"], [154, 155, "MethodName", "ontology"], [161, 162, "MethodName", "ontology"], [235, 236, "MethodName", "BERT"], [237, 238, "MethodName", "BERT"]]}
{"text": "We conduct experiments on three different datasets and report micro - F1 and macro - F1 scores for the dialogue act prediction task , a multi - label classification problem . For the MWOZ dataset , we remove the domain information from the original system dialogue act labels . For example , the \" taxi - inform \" will be simplified to \" inform \" . This process reduces the number of possible dialogue acts from 31 to 13 . For DSTC2 and GSIM corpora , we follow to apply universal dialogue act mapping that maps the original dialogue act labels to a general dialogue act format , resulting in 9 and 6 unique system dialogue acts in DSTC2 and GSIM , respectively . We run two other baselines , MLP and RNN , to further show the strengths of BERT - based MWOZ ( 13 ) DSTC2 ( 9 ) GSIM ( 6 ) micro - F1 macro - F1 micro - F1 macro - F1 micro - F1 macro - F1 1 % Data BERT 84.0 % \u00b1 0.6 % 66.7 % \u00b1 1.7 % 77.1 % \u00b1 2.1 % 25.8 % \u00b1 0.8 % 67.3 % \u00b1 1.4 % 26.9 % \u00b1 1.0 % TOD - BERT - mlm 87.5 % \u00b1 0.6 % 73.3 % \u00b1 1.5 % 79.6 % \u00b1 1.0 % 26.4 % \u00b1 0.5 % 82.7 % \u00b1 0.7 % 35.7 % \u00b1 0.3 % TOD - BERT - jnt 86.9 % \u00b1 0.2 % 72.4 % \u00b1 0.8 % 82.9 % \u00b1 0.4 % 28.0 % \u00b1 0.1 % 78.4 % \u00b1 3.2 % 32.9 % \u00b1 2.1 % 10 % Data BERT 89.7 % \u00b1 0.2 % 78.4 % \u00b1 0.3 % 88.2 % \u00b1 0.7 % 34.8 % \u00b1 1.3 % 98.4 % \u00b1 0.3 % 45.1 % \u00b1 0.2 % TOD - BERT - mlm 90.1 % \u00b1 0.2 % 78.9 % \u00b1 0.1 % 91.8 % \u00b1 1.7 % 39.4 % \u00b1 1.7 % 99.2 % \u00b1 0.1 % 45.6 % \u00b1 0.1 % TOD - BERT - jnt 90.2 % \u00b1 0.2 % 79.6 % \u00b1 0.7 % 90.6 % \u00b1 3.2 % 38.8 % \u00b1 2.2 % 99.3 % \u00b1 0.1 % 45.7 % \u00b1 0.0 %", "entities": [[9, 12, "MetricName", "micro - F1"], [13, 16, "MetricName", "macro - F1"], [25, 29, "TaskName", "multi - label classification"], [130, 131, "DatasetName", "MLP"], [140, 141, "MethodName", "BERT"], [155, 158, "MetricName", "micro - F1"], [158, 161, "MetricName", "macro - F1"], [161, 164, "MetricName", "micro - F1"], [164, 167, "MetricName", "macro - F1"], [167, 170, "MetricName", "micro - F1"], [170, 173, "MetricName", "macro - F1"], [176, 177, "MethodName", "BERT"], [209, 210, "MethodName", "BERT"], [211, 212, "DatasetName", "mlm"], [244, 245, "MethodName", "BERT"], [280, 281, "MethodName", "BERT"], [313, 314, "MethodName", "BERT"], [315, 316, "DatasetName", "mlm"], [348, 349, "MethodName", "BERT"]]}
{"text": "Slot Acc 1 % Data BERT 6.4 % \u00b1 1.4 % 84.4 % \u00b1 1.0 % TOD - BERT - mlm 9.9 % \u00b1 0.6 % 86.6 % \u00b1 0.5 % TOD - BERT - jnt 8.0 % \u00b1 1.0 % 85.3 % \u00b1 0.4 % 5 % Data BERT 19.6 % \u00b1 0.1 % 92.0 % \u00b1 0.5 % TOD - BERT - mlm 28.1 % \u00b1 1.6 % 93.9 % \u00b1 0.1 % TOD - BERT - jnt 28.6 % \u00b1 1.4 % 93.8 % \u00b1 0.3 % 10 % Data BERT 32.9 % \u00b1 0.6 % 94.7 % \u00b1 0.1 % TOD - BERT - mlm 39.5 % \u00b1 0.7 % 95.6 % \u00b1 0.1 % TOD - BERT - jnt 37.0 % \u00b1 0.1 % 95.2 % \u00b1 0.1 % 25 % Data BERT 40.8 % \u00b1 1.0 % 95.8 % \u00b1 0.1 % TOD - BERT - mlm 44.0 % \u00b1 0.4 % 96.4 % \u00b1 0.1 % TOD - BERT - jnt 44.3 % \u00b1 0.3 % 96.3 % \u00b1 0.2 % models . The MLP model simply takes bag - of - word embeddings to make dialogue act prediction , and the RNN model is a bi - directional GRU network . In Table 4 , one can observe that in full data scenario , TOD - BERT consistently works better than BERT and other baselines , no matter which datasets or which evaluation metrics . In the fewshot experiments , TOD - BERT - mlm outperforms BERT by 3.5 % micro - F1 and 6.6 % macro - F1 on MWOZ corpus in the 1 % data scenario . We also found that 10 % of training data can achieve good performance that is close to full data training .", "entities": [[1, 2, "MetricName", "Acc"], [5, 6, "MethodName", "BERT"], [18, 19, "MethodName", "BERT"], [20, 21, "DatasetName", "mlm"], [33, 34, "MethodName", "BERT"], [49, 50, "MethodName", "BERT"], [62, 63, "MethodName", "BERT"], [64, 65, "DatasetName", "mlm"], [77, 78, "MethodName", "BERT"], [93, 94, "MethodName", "BERT"], [106, 107, "MethodName", "BERT"], [108, 109, "DatasetName", "mlm"], [121, 122, "MethodName", "BERT"], [137, 138, "MethodName", "BERT"], [150, 151, "MethodName", "BERT"], [152, 153, "DatasetName", "mlm"], [165, 166, "MethodName", "BERT"], [181, 182, "DatasetName", "MLP"], [189, 191, "TaskName", "word embeddings"], [206, 207, "MethodName", "GRU"], [224, 225, "MethodName", "BERT"], [229, 230, "MethodName", "BERT"], [250, 251, "MethodName", "BERT"], [252, 253, "DatasetName", "mlm"], [254, 255, "MethodName", "BERT"], [258, 261, "MetricName", "micro - F1"], [264, 267, "MetricName", "macro - F1"]]}
{"text": "To evaluate response selection in task - oriented dialogues , we follow the k - to - 100 accuracy , which is becoming a research community standard ( Yang et al , 2018 ; Henderson et al , 2019a ) . The k - of - 100 MWOZ DSTC2 GSIM 1 - to - 100 3 - to - 100 1 - to - 100 3 - to - 100 1 - to - 100 3 - to - 100 1 % Data BERT 7.8 % \u00b1 2.0 % 20.5 % \u00b1 4.4 % 3.7 % \u00b1 0.6 % 9.6 % \u00b1 1.3 % 4.0 % \u00b1 0.4 % 10.3 % \u00b1 1.1 % TOD - BERT - mlm 13.0 % \u00b1 1.1 % 34.6 % \u00b1 0.4 % 12.5 % \u00b1 6.7 % 24.9 % \u00b1 10.7 % 7.2 % \u00b1 4.0 % 15.4 % \u00b1 8.0 % TOD - BERT - jnt - - 37.5 % \u00b1 0.6 % 55.9 % \u00b1 0.4 % 12.5 % \u00b1 0.9 % 26.8 % \u00b1 0.8 % 10 % Data BERT 20.9 % \u00b1 2.6 % 45.4 % \u00b1 3.8 % 8.9 % \u00b1 2.3 % 21.4 % \u00b1 3.1 % 9.8 % \u00b1 0.1 % 24.4 % \u00b1 1.2 % TOD - BERT - mlm 22.3 % \u00b1 3.2 % 48.7 % \u00b1 4.0 % 19.0 % \u00b1 16.3 % 33.8 % \u00b1 20.4 % 11.2 % \u00b1 2.5 % 26.0 % \u00b1 2.7 % TOD - BERT - jnt - - 49.7 % \u00b1 0.3 % 66.6 % \u00b1 0.1 % 23.0 % \u00b1 1.0 % 42.6 % \u00b1 1.0 % Table 6 : Response selection evaluation results on three corpora for 1 % , 10 % and full data setting . We report 1 - to - 100 and 3 - to - 100 accuracy , which is similar to recall1 and recall@3 given 100 candidates . metric is computed using a random batch of 100 examples so that responses from other examples in the same batch can be used as random negative candidates . This allows us to be compute the metric across many examples in batches efficiently . While it is not guaranteed that the random negatives will indeed be \" true \" negatives , the 1 - of - 100 metric still provides a useful evaluation signal . During inference , we run five different random seeds to sample batches and report the average results . In Table 6 , we conduct response selection experiments on three datasets , MWOZ , DSTC2 , and GSIM . TOD - BERT - jnt achieves 65.8 % 1 - to - 100 accuracy and 87.0 % 3 - to - 100 accuracy on MWOZ , which surpasses BERT by 18.3 % and 11.5 % , respectively . The similar results are also consistently observed in DSTC2 and GSIM datasets , and the advantage of the TOD - BERT - jnt is more evident in the few - shot scenario . We do not report TOD - BERT - jnt for MWOZ few - shot setting because it is not fair to compare them with others as the full MWOZ training set is used for response contrastive learning during pre - training stage . The response selection results are sensitive to the training batch size since the larger the batch size the harder the prediction . In our experiments , we set batch size equals to 25 for all the models .", "entities": [[18, 19, "MetricName", "accuracy"], [83, 84, "MethodName", "BERT"], [116, 117, "MethodName", "BERT"], [118, 119, "DatasetName", "mlm"], [151, 152, "MethodName", "BERT"], [179, 180, "MethodName", "BERT"], [212, 213, "MethodName", "BERT"], [214, 215, "DatasetName", "mlm"], [247, 248, "MethodName", "BERT"], [306, 307, "MetricName", "accuracy"], [401, 402, "DatasetName", "seeds"], [433, 434, "MethodName", "BERT"], [444, 445, "MetricName", "accuracy"], [453, 454, "MetricName", "accuracy"], [459, 460, "MethodName", "BERT"], [489, 490, "MethodName", "BERT"], [508, 509, "MethodName", "BERT"], [537, 539, "MethodName", "contrastive learning"], [554, 556, "HyperparameterName", "batch size"], [560, 562, "HyperparameterName", "batch size"], [573, 575, "HyperparameterName", "batch size"]]}
{"text": "In Figure 2 , we visualize the embeddings of BERT , TOD - BERT - mlm , and TOD - BERT - jnt given the same input from the MWOZ test set . Each sample point is a system response representation , which is passed through a pre - trained model and reduced its high - dimension features to a two - dimension point using the t - distributed stochastic neighbor embedding ( tSNE ) for dimension reduction . Since we know the true domain and dialogue act labels for each utterance , we use different colors to represent different domains and dialogue acts . As one can observe , TOD - BERT - jnt has more clear group boundaries than TOD - BERT - mlm , and two of them are better than BERT . To analyze the results quantitatively , we run Kmeans , a common unsupervised clustering algorithms , on top of the output embeddings of BERT and TOD - BERT . We set K for K - means equal to 10 and 20 . After the clustering , we can assign each utterance in the MWOZ test set to a predicted class . We then compute the normalized mutual information ( NMI ) between the clustering result and the actual domain label for each utterance . Here is what we observe : TOD - BERT consistently achieves higher NMI scores than BERT . For K=10 , TOD - BERT has a 0.143 NMI score , and BERT only has 0.094 . For K=20 , TOD - BERT achieves a 0.213 NMI score , while BERT has 0.109 .", "entities": [[9, 10, "MethodName", "BERT"], [13, 14, "MethodName", "BERT"], [15, 16, "DatasetName", "mlm"], [20, 21, "MethodName", "BERT"], [112, 113, "MethodName", "BERT"], [123, 124, "MethodName", "BERT"], [125, 126, "DatasetName", "mlm"], [134, 135, "MethodName", "BERT"], [159, 160, "MethodName", "BERT"], [163, 164, "MethodName", "BERT"], [205, 206, "MetricName", "NMI"], [228, 229, "MethodName", "BERT"], [232, 233, "MetricName", "NMI"], [235, 236, "MethodName", "BERT"], [242, 243, "MethodName", "BERT"], [246, 247, "MetricName", "NMI"], [250, 251, "MethodName", "BERT"], [260, 261, "MethodName", "BERT"], [264, 265, "MetricName", "NMI"], [268, 269, "MethodName", "BERT"]]}
{"text": "Vanilla Transformer . We describe the original Transformer architecture with positional encoding ( Vaswani et al , 2017 ) as formalized by P\u00e9rez et al ( 2019 ) , with some modifications . All vectors in this subsection are from Q d . The transformer , denoted Trans , is a seq - to - seq architecture . Its input consists of ( i ) a sequence X = ( x 1 , . . . , x n ) of vectors , ( ii ) a seed vector y 0 . The output is a sequence Y = ( y 1 , . . . , y r ) of vectors . The sequence X is obtained from the sequence ( s 1 , . . . , s n ) \u03a3 n of symbols by using the embedding mentioned earlier : x i = f ( f b ( s i ) , pos ( i ) ) . The transformer consists of composition of transformer encoder and transformer decoder . For the feedforward networks in the transformer layers we use the activation as in Siegelmann and Sontag ( 1992 ) , namely the saturated linear activation function \u03c3 ( x ) which takes value 0 for x < 0 , value x for 0 < x < 1 and value 1 for x > 1 . This activation can be easily replaced by the standard ReLU activation via \u03c3 ( x ) = ReLU ( x ) \u2212 ReLU ( x \u2212 1 ) . Self - attention . The self - attention mechanism takes as input ( i ) a query vector q , ( ii ) a sequence of key vectors K = ( k 1 , . . . , k n ) , and ( iii ) a sequence of value vectors V = ( v 1 , . . . , v n ) . The q - attention over K and V , denoted Att ( q , K , V ) , is a vector a = \u03b1 1 v 1 + \u03b1 2 v 2 + + \u03b1 n v n , where ( i ) ( \u03b1 1 , . . . , \u03b1 n ) = \u03c1 ( f att ( q , k 1 ) , . . . , f att ( q , k n ) ) . ( ii ) The normalization function \u03c1 : Q n Q n \u22650 is hardmax : for x = ( x 1 , . . . , x n ) Q n , if the maximum value occurs r times among x 1 , . . . , x n , then hardmax ( x ) i : = 1 / r if x i is a maximum value and hardmax ( x ) i : = 0 otherwise . In practice , the softmax is often used but its output values are in general not rational . ( iii ) For vanilla transformers , the scoring function f att used is a combination of multiplicative attention ( Vaswani et al , 2017 ) and a non - linear function : f att ( q , k i ) = \u2212 q , k i . This was also used by P\u00e9rez et al ( 2019 ) . Transformer encoder . A single - layer encoder is a function Enc ( X ; \u03b8 ) , with input X = ( x 1 , . . . , x n ) a sequence of vectors in Q d , and parameters \u03b8 . The output is another sequence Z = ( z 1 , . . . , z n ) of vectors in Q d . The parame - ters \u03b8 specify functions Q ( ) , K ( ) , V ( ) , and O ( ) , all of type Q d Q d . The functions Q ( ) , K ( ) , and V ( ) are linear transformations and O ( ) an FFN . For 1 \u2264 i \u2264 n , the output of the self - attention block is produced by a i = Att ( Q ( x i ) , K ( X ) , V ( X ) ) + x i ( 1 ) This operation is also referred to as the encoderencoder attention block . The output Z is computed by z i = O ( a i ) + a i for 1 \u2264 i \u2264 n. The addition operations + x i and + a i are the residual connections . The complete L - layer transformer encoder TEnc ( L ) ( X ; \u03b8 ) = ( K e , V e ) has the same input X = ( x 1 , . . . , x n ) as the single - layer encoder . In contrast , its output K e = ( k e 1 , . . . , k e n ) and V e = ( v e 1 , . . . v e n ) contains two sequences . TEnc ( L ) is obtained by composition of L singlelayer encoders : let X ( 0 ) : = X , and for 0 \u2264 \u2264 L \u2212 1 , let X ( +1 ) = Enc ( X ( ) ; \u03b8 ) and finally , K e = K ( L ) ( X ( L ) ) , V e = V ( L ) ( X ( L ) ) . Transformer decoder . The input to a singlelayer decoder is ( i ) ( K e , V e ) output by the encoder , and ( ii ) sequence Y = ( y 1 , . . . , y k ) of vectors for k \u2265 1 . The output is another sequence Z = ( z 1 , . . . , z k ) . Similar to the single - layer encoder , a singlelayer decoder is parameterized by functions Q ( ) , K ( ) , V ( ) and O ( ) and is defined by p t = Att ( Q ( y t ) , K ( Y t ) , V ( Y t ) ) + y t , ( 2 ) a t = Att ( p t , K e , V e ) + p t , ( 3 ) z t = O ( a t ) + a t , where 1 \u2264 t \u2264 k. The operation in ( 2 ) will be referred to as the decoder - decoder attention block and the operation in ( 3 ) as the decoder - encoder attention block . In ( 2 ) , positional masking is applied to prevent the network from attending over symbols which are ahead of them . An L - layer Transformer decoder TDec L ( ( K e , V e ) , Y ; \u03b8 ) = z is obtained by repeated application of L single - layer decoders each with its own parameters , and a transformation function F : Q d Q d applied to the last vector in the sequence of vectors output by the final decoder . Formally , for 0 \u2264 \u2264 L\u22121 and Y 0 : = Y we have Y +1 = Dec ( ( K e , V e ) , Y ; \u03b8 ) , z = F ( y L k ) . Note that while the output of a single - layer decoder is a sequence of vectors , the output of an L - layer Transformer decoder is a single vector . The complete Transformer . The output Trans ( X , y 0 ) = Y is computed by the recurrence\u1ef9 t+1 = TDec ( TEnc ( X ) , ( y 0 , y 1 , . . . , y t ) ) , for 0 \u2264 t \u2264 r \u2212 1 . We get y t+1 by adding positional encoding : y t+1 = \u1ef9 t+1 + pos ( t + 1 ) . Directional Transformer . We denote the Transformer with only positional masking and no positional encodings as Directional Transformer and use them interchangeably . In this case , we use standard multiplicative attention as the scoring function in our construction , i.e , f att ( q , k i ) = q , k i . The general architecture is the same as for the vanilla case ; the differences due to positional masking are the following . There are no positional encodings . So the input vectors x i only involve f b ( s i ) . Simi - larly , y t = \u1ef9 t . In ( 1 ) , Att ( ) is replaced by Att ( Q ( x i ) , K ( X i ) , V ( X i ) ) where X i : = ( x 1 , . . . , x i ) for 1 \u2264 i \u2264 n. Similarly , in ( 3 ) , Att ( ) is replaced by Att ( p t , K e t , V e t ) . Remark 1 . Our definitions deviate slightly from practice , hard - attention being the main one since hardmax keeps the values rational whereas softmax takes the values to irrational space . Previous studies have shown that soft - attention behaves like hard - attention in practice and Hahn ( 2020 ) discusses its practical relevance . Remark 2 . Transformer Networks with positional encodings are not necessarily equivalent in terms of their computational expressiveness ( Yun et al , 2020 ) to those with only positional masking when considering the encoder only model ( as used in BERT and GPT - 2 ) . Our results in Section 4.1 show their equivalence in terms of expressiveness for the complete seq - to - seq architecture . 4 Primary Results", "entities": [[1, 2, "MethodName", "Transformer"], [7, 8, "MethodName", "Transformer"], [90, 91, "DatasetName", "0"], [170, 172, "MethodName", "transformer decoder"], [198, 200, "HyperparameterName", "activation function"], [207, 208, "DatasetName", "0"], [211, 212, "DatasetName", "0"], [216, 217, "DatasetName", "0"], [238, 239, "MethodName", "ReLU"], [246, 247, "MethodName", "ReLU"], [251, 252, "MethodName", "ReLU"], [286, 288, "HyperparameterName", "K ="], [347, 348, "HyperparameterName", "\u03b1"], [352, 353, "HyperparameterName", "\u03b1"], [358, 359, "HyperparameterName", "\u03b1"], [368, 369, "HyperparameterName", "\u03b1"], [375, 376, "HyperparameterName", "\u03b1"], [481, 482, "DatasetName", "0"], [488, 489, "MethodName", "softmax"], [519, 521, "MethodName", "multiplicative attention"], [562, 563, "MethodName", "Transformer"], [577, 578, "HyperparameterName", "\u03b8"], [605, 606, "HyperparameterName", "\u03b8"], [635, 636, "HyperparameterName", "\u03b8"], [796, 797, "HyperparameterName", "\u03b8"], [887, 888, "DatasetName", "0"], [895, 896, "DatasetName", "0"], [914, 915, "HyperparameterName", "\u03b8"], [947, 949, "MethodName", "Transformer decoder"], [1178, 1180, "MethodName", "Transformer decoder"], [1193, 1194, "HyperparameterName", "\u03b8"], [1243, 1244, "DatasetName", "0"], [1249, 1250, "DatasetName", "0"], [1270, 1271, "HyperparameterName", "\u03b8"], [1306, 1308, "MethodName", "Transformer decoder"], [1315, 1316, "MethodName", "Transformer"], [1324, 1325, "DatasetName", "0"], [1344, 1345, "DatasetName", "0"], [1359, 1360, "DatasetName", "0"], [1390, 1391, "MethodName", "Transformer"], [1395, 1396, "MethodName", "Transformer"], [1406, 1407, "MethodName", "Transformer"], [1419, 1421, "MethodName", "multiplicative attention"], [1602, 1603, "MethodName", "softmax"], [1638, 1639, "MethodName", "Transformer"], [1676, 1677, "MethodName", "BERT"], [1678, 1679, "MethodName", "GPT"]]}
{"text": "In light of Theorem 3.1 , to prove that Transformers are Turing - complete , it suffices to show that they can simulate RNNs . We say that a Transformer simulates an RNN ( as defined in Sec . 3.1 ) if on every input s \u03a3 * , at each step t , the vector y t contains the hidden state h t as a subvector , i.e. y t = [ h t , ] , and halts at the same step as the RNN . Theorem 4.1 . The class of Transformers with positional encodings is Turing - complete . Proof Sketch . The input s 0 , . . . , s n \u03a3 * is provided to the transformer as the sequence of vectors x 0 , . . . , x n , where x i = [ 0 d h , f b ( s i ) , 0 d h , i , 1 ] , which has as sub - vector the given base embedding f b ( s i ) and the positional encoding i , along with extra coordinates set to constant values and will be used later . The basic observation behind our construction of the simulating Transformer is that the transformer decoder can naturally implement the recurrence operations of the type used by RNNs . To this end , the FFN O dec ( ) of the decoder , which plays the same role as the FFN component of the RNN , needs sequential access to the input in the same way as RNN . But the Transformer receives the whole input at the same time . We utilize positional encoding along with the attention mechanism to isolate x t at time t and feed it to O dec ( ) , thereby simulating the RNN . As stated earlier , we append the input s 1 , . . . , s n of the RNN with $ 's until it halts . Since the Transformer takes its input all at once , appending by $ 's is not possible ( in particular , we do not know how long the computation would take ) . Instead , we append the input with a single $ . After encountering a $ once , the Transformer will feed ( encoding of ) $ to O dec ( ) in subsequent steps until termination . Here we confine our discussion to the case t \u2264 n ; the t > n case is slightly different but simpler . The construction is straightforward : it has only one head , one encoder layer and one decoder layer ; moreover , the attention mechanisms in the encoder and the decoder - decoder attention block of the decoder are trivial as described below . The encoder attention layer does trivial computation in that it merely computes the identity function : z i = x i , which can be easily achieved , e.g. by using the residual connection and setting the value vectors to 0 . The fi - nal K ( 1 ) ( ) and V ( 1 ) ( ) functions bring ( K e , V e ) into useful forms by appropriate linear transformations : k i = [ 0 d b , 0 d b , 0 d b , \u22121 , i ] and v i = [ 0 d b , f b ( s i ) , 0 d b , 0 , 0 ] . Thus , the key vectors only encode the positional information and the value vectors only encode the input symbols . The output sequence of the decoder is y 1 , y 2 , . . .. Our construction will ensure , by induction on t , that y t contains the hidden states h t of the RNN as a sub - vector along with positional information : y t = [ h t , 0 d b , 0 d b , t + 1 , 1 ] . This is easy to arrange for t = 0 , and assuming it for t we prove it for t+1 . As for the encoder , the decoder - decoder attention block acts as the identity : p t = y t . Now , using the last but one coordinate in y t representing the time t + 1 , the attention mechanism Att ( p t , K e , V e ) can retrieve the embedding of the t - th input symbol x t . This is possible because in the key vector k i mentioned above , almost all coordinates other than the one representing the position i are set to 0 , allowing the mechanism to only focus on the positional information and not be distracted by the other contents of p t = y t : the scoring function has value f att ( p t , k i ) = \u2212 | p t , k i | = \u2212 | i \u2212 ( t + 1 ) | . For a given t , it is maximized at i = t + 1 for t < n and at i = n for t \u2265 n. This use of scoring function is similar to P\u00e9rez et al ( 2019 ) . At this point , O dec ( ) has at its disposal the hidden state h t ( coming from y t via p t and the residual connection ) and the input symbol x t ( coming via the attention mechanism and the residual connection ) . Hence O ( ) can act just like the FFN ( Lemma C.4 ) underlying the RNN to compute h t+1 and thus y t+1 , proving the induction hypothesis . The complete construction can be found in Sec . C.2 in the appendix . Theorem 4.2 . The class of Transformers with positional masking and no explicit positional encodings is Turing - complete . Proof Sketch . As before , by Theorem 3.1 it suffices to show that Transformers can simulate RNNs . The input s 0 , . . . , s n is provided to the transformer as the sequence of vectors x 0 , . . . , x n , where x i = [ 0 d h , 0 d h , f b ( s i ) , s i , 0 , 0 m , 0 m , 0 m ] . The general goal for the directional case is similar to the vanilla case , namely we would like the FFN O dec ( ) of the decoder to directly simulate the computation in the underlying RNN . In the vanilla case , positional encoding and the attention mechanism helped us feed input x t at the t - th iteration of the decoder to O dec ( ) . However , we no longer have explicit positional information in the input x t such as a coordinate with value t. The key insight is that we do not need the positional information explicitly to recover x t at step t : in our construction , the attention mechanism with masking will recover x t in an indirect manner even though it 's not able to \" zero in \" on the t - th position . Let us first explain this without details of the construction . We maintain in vector \u03c9 t Q m , with a coordinate each for symbols in \u03a3 , the fraction of times the symbol has occurred up to step t. Now , at a step t \u2264 n , for the difference \u03c9 t \u2212 \u03c9 t\u22121 ( which is part of the query vector ) , it can be shown easily that only the coordinate corresponding to s t is positive . Thus after applying the linearized sigmoid \u03c3 ( \u03c9 t \u2212 \u03c9 t\u22121 ) , we can isolate the coordinate corresponding to s t . Now using this query vector , the ( hard ) attention mechanism will be able to retrieve the value vectors for all indices j such that s j = s t and output their average . Crucially , the value vector for an index j is essentially x j which depends only on s j . Thus , all these vectors are equal to x t , and so is their average . This recovers x t , which can now be fed to O dec ( ) , simulating the RNN . We now outline the construction and relate it to the above discussion . As before , for simplicity we restrict to the case t \u2264 n. We use only one head , one layer encoder and two layer decoder . The encoder , as in the vanilla case , does very little other than pass information along . The vectors in ( K e , V e ) are obtained by the trivial attention mechanism followed by simple linear transformations : k e i = [ 0 d h , 0 d h , 0 d b , s i , 0 , 0 m , 0 m , 0 m ] and v e i = [ 0 d h , 0 d h , f b ( s i ) , 0 m , 0 , 0 m , s i , 0 m ] . Our construction ensures that at step t we have y t = [ h t\u22121 , 0 d h , 0 d b , 0 m , 1 2 t , 0 m , 0 m , \u03c9 t\u22121 ] . As before , the proof is by induction on t. In the first layer of decoder , the decoderdecoder attention block is trivial : p ( 1 ) t = y t . In the decoder - encoder attention block , we give equal attention to all the t + 1 values , which along with O enc ( ) , leads to z ( 1 ) t = [ h t\u22121 , 0 d h , 0 d b , \u03b4 t , 1 2 t+1 , 0 m , 0 m , \u03c9 t ] , where essentially \u03b4 t = \u03c3 ( \u03c9 t \u2212 \u03c9 t\u22121 ) , except with a change for the last coordinate due to special status of the last symbol $ in the processing of RNN . In the second layer , the decoder - decoder attention block is again trivial with p ( 2 ) t = z ( 1 ) t . We remark that in this construction , the scoring function is the standard multiplicative attention 3 . Now p ( 2 ) t , k e j = \u03b4 t , s j = \u03b4 t , j , which is positive if and only if s j = s t , as mentioned earlier . Thus attention weights in Att ( p ( 2 ) t , K e t , V e t ) satisfy hardmax ( p ( 2 ) t , k e 1 , . . . , p ( 2 ) t , k e t ) = 1 \u03bbt ( I ( s 0 = s t ) , I ( s 1 = s t ) , . . . , I ( s t = s t ) ) , where \u03bb t is a normalization constant and I ( ) is the indicator . See Lemma D.3 for more details . At this point , O dec ( ) has at its disposal the hidden state h t ( coming from z ( 1 ) t via p ( 2 ) t and the residual connection ) and the input symbol x t ( coming via the attention mechanism and the residual connection ) . Hence O dec ( ) can act just like the FFN underlying the RNN to compute h t+1 and thus y t+1 , proving the induction hypothesis . The complete construction can be found in Sec . D in the Appendix . In practice , found that for NMT , Transformers with only positional masking achieve comparable performance compared to the ones with positional encodings . Similar evidence was found by Tsai et al ( 2019 ) . Our proof for directional transformers entails that there is no loss of order information if positional information is only provided in the form of masking . However , we do not recommend using masking as a replacement for explicit encodings . The computational equivalence of encoding and masking given by our results implies that any differences in their performance must come from differences in learning dynamics .", "entities": [[29, 30, "MethodName", "Transformer"], [104, 105, "DatasetName", "Sketch"], [109, 110, "DatasetName", "0"], [130, 131, "DatasetName", "0"], [144, 145, "DatasetName", "0"], [155, 156, "DatasetName", "0"], [209, 210, "MethodName", "Transformer"], [213, 215, "MethodName", "transformer decoder"], [270, 271, "MethodName", "Transformer"], [339, 340, "MethodName", "Transformer"], [388, 389, "MethodName", "Transformer"], [505, 507, "MethodName", "residual connection"], [513, 514, "DatasetName", "0"], [553, 554, "DatasetName", "0"], [557, 558, "DatasetName", "0"], [561, 562, "DatasetName", "0"], [574, 575, "DatasetName", "0"], [585, 586, "DatasetName", "0"], [589, 590, "DatasetName", "0"], [591, 592, "DatasetName", "0"], [669, 670, "DatasetName", "0"], [673, 674, "DatasetName", "0"], [692, 693, "DatasetName", "0"], [800, 801, "DatasetName", "0"], [931, 933, "MethodName", "residual connection"], [948, 950, "MethodName", "residual connection"], [963, 964, "DatasetName", "Lemma"], [1018, 1019, "DatasetName", "Sketch"], [1039, 1040, "DatasetName", "0"], [1058, 1059, "DatasetName", "0"], [1072, 1073, "DatasetName", "0"], [1076, 1077, "DatasetName", "0"], [1090, 1091, "DatasetName", "0"], [1092, 1093, "DatasetName", "0"], [1095, 1096, "DatasetName", "0"], [1098, 1099, "DatasetName", "0"], [1536, 1537, "DatasetName", "0"], [1540, 1541, "DatasetName", "0"], [1544, 1545, "DatasetName", "0"], [1551, 1552, "DatasetName", "0"], [1553, 1554, "DatasetName", "0"], [1556, 1557, "DatasetName", "0"], [1559, 1560, "DatasetName", "0"], [1568, 1569, "DatasetName", "0"], [1572, 1573, "DatasetName", "0"], [1583, 1584, "DatasetName", "0"], [1586, 1587, "DatasetName", "0"], [1588, 1589, "DatasetName", "0"], [1594, 1595, "DatasetName", "0"], [1614, 1615, "DatasetName", "0"], [1618, 1619, "DatasetName", "0"], [1622, 1623, "DatasetName", "0"], [1629, 1630, "DatasetName", "0"], [1632, 1633, "DatasetName", "0"], [1712, 1713, "DatasetName", "0"], [1716, 1717, "DatasetName", "0"], [1720, 1721, "HyperparameterName", "\u03b4"], [1727, 1728, "DatasetName", "0"], [1730, 1731, "DatasetName", "0"], [1739, 1740, "HyperparameterName", "\u03b4"], [1814, 1816, "MethodName", "multiplicative attention"], [1829, 1830, "HyperparameterName", "\u03b4"], [1835, 1836, "HyperparameterName", "\u03b4"], [1911, 1912, "DatasetName", "0"], [1956, 1957, "DatasetName", "Lemma"], [1995, 1997, "MethodName", "residual connection"], [2012, 2014, "MethodName", "residual connection"], [2104, 2105, "MetricName", "loss"]]}
{"text": "The results for various components follow from our construction in Theorem 4.1 . Note that in both the encoder and decoder attention blocks , we need to compute the identity function . We can nullify the role of the attention heads by setting the value vectors to zero and making use of only the residual connections to implement the identity function . Thus , even if we remove those attention heads , the model is still Turing - complete . On the other hand , we can remove the residual connections around the attention blocks and make use of the attention heads to implement the identity function by using positional encodings . Hence , either the attention head or the residual connection is sufficient to achieve Turing - completeness . A similar argument can be made for the FFN in the encoder layer : either the residual connection or the FFN is sufficient for Turing - completeness . For the decoder - encoder attention head , since it is the only way for the decoder to obtain information about the input , it is necessary for the completeness . The FFN is the only component that can perform computations based on the input and the computations performed earlier via recurrence and hence , the model is not Turing - complete without it . Figure 2 summarizes the role of different components with respect to the computational expressiveness of the network . Proposition 4.3 . The class of Transformers without residual connection around the decoderencoder attention block is not Turing - complete . Proof Sketch . We confine our discussion to singlelayer decoder ; the case of multilayer decoder is similar . Without the residual connection , the decoder - encoder attention block produces a t = Att ( p t , K e , V e ) = n i=1 \u03b1 i v e i for some \u03b1 i 's such that n i \u03b1 i = 1 . Note that , without residual connection a t can take on at most 2 n \u2212 1 values . This is because by the definition of hard attention the vector ( \u03b1 1 , . . . , \u03b1 n ) is characterized by the set of zero coordinates and there are at most 2 n \u2212 1 such sets ( all coordinates can not be zero ) . This restriction on the number of values on a t holds regardless of the value of p t . If the task requires the network to produce values of a t that come from a set with size at least 2 n , then the network will not be able to perform the task . Here 's an example task : given a number \u2206 ( 0 , 1 ) , the network must produce numbers 0 , \u2206 , 2\u2206 , . . . , k\u2206 , where k is the maximum integer such that k\u2206 \u2264 1 . If the network receives a single input \u2206 , then it is easy to see that the vector a t will be a constant ( v e 1 ) at any step and hence the output of the network will also be constant at all steps . Thus , the model can not perform such a task . If the input is combined with n \u2212 1 auxiliary symbols ( such as # and $ ) , then in the network , each a t takes on at most 2 n \u2212 1 values . Hence , the model will be incapable of performing the task if \u2206 < 1/2 n . Such a limitation does not exist with a residual connection since the vector a t = n i=1 \u03b1 i v e i + p t can take arbitrary number of values depending on its prior computations in p t . For further details , see Sec . C.1 in the Appendix . Discussion . It is perhaps surprising that residual connection , originally proposed to assist in the learning ability of very deep networks , plays a vital role in the computational expressiveness of the network . Without it , the model is limited in its capability to make decisions based on predictions in the previous steps . We explore practical implications of this result in section 5 .", "entities": [[120, 122, "MethodName", "residual connection"], [146, 148, "MethodName", "residual connection"], [249, 251, "MethodName", "residual connection"], [263, 264, "DatasetName", "Sketch"], [283, 285, "MethodName", "residual connection"], [310, 311, "HyperparameterName", "\u03b1"], [317, 318, "HyperparameterName", "\u03b1"], [324, 325, "HyperparameterName", "\u03b1"], [333, 335, "MethodName", "residual connection"], [360, 361, "HyperparameterName", "\u03b1"], [367, 368, "HyperparameterName", "\u03b1"], [464, 465, "DatasetName", "0"], [474, 475, "DatasetName", "0"], [618, 620, "MethodName", "residual connection"], [628, 629, "HyperparameterName", "\u03b1"], [670, 672, "MethodName", "residual connection"]]}
{"text": "In this section , we explore the practical implications of our results . Our experiments are geared towards answering the following questions : Q1 . Are there any practical implications of the limitation of Transformers without decoder - encoder residual connections ? What tasks can they do or not do compared to vanilla Transformers ? Q2 . Is there any additional benefit of using positional masking as opposed to absolute positional encoding ( Vaswani et al , 2017 ) ? Although we showed that Transformers without decoder - encoder residual connection are not Turing complete , it does not imply that they are incapable of performing all the tasks . Our results suggest that they are limited in their capability to make inferences based on their previous computations , which is required for tasks such as counting and language modeling . However , it can be shown that the model is capable of performing tasks which rely only on information provided at a given step such as copying and mapping . For such tasks , given positional information at a particular step , the model can look up the corresponding input and map it via the FFN . We evaluate these hypotheses via our experiments . For our experiments on synthetic data , we consider two tasks , namely the copy task and the counting task . For the copy task , the goal of a model is to reproduce the input sequence . We sample sentences of lengths between 5 - 12 words from Penn Treebank and create a train - test split of 40k - 1k with all sentences belonging to the same range of length . In the counting task , we create a very simple dataset where the model is given one number between 0 and 100 as input and its goal is to predict the next five numbers . Since only a single input is provided to the encoder , it is necessary for the decoder to be able to make inferences based on its previous predictions to perform this task . The benefit of conducting these experiments on synthetic data is that they isolate the phenomena we wish to evaluate . For both these tasks , we compare vanilla Transformer with the one without decoder - encoder residual connection . As a baseline we also consider the model without decoder - decoder residual connection , since according to our results , that connection does not influence the computational power of the model . We implement a single layer encoderdecoder network with only a single attention head in each block . We then assess the influence of the limitation on Machine Translation which requires a model to do a combination of both mapping and inferring from computations in previous timesteps . We evaluate the models on IWSLT'14 German - English dataset and IWSLT'15 English - Vietnamese dataset . We again compare vanilla Transformer with the ones without decoder - encoder and decoder - decoder residual connection . While tuning the models , we vary the number of layers from 1 to 4 , the learning rate , warmup steps and the number of heads . Specifications of the models , experimental setup , datasets and sample outputs can be found in Sec . E in the Appendix . Results on the effect of residual connections on synthetic tasks can be found in Table 1 . As per our hypothesis , all the variants are able to perfectly perform the copy task . For the counting task , the one without decoder - encoder residual connection is incapable of performing it . However , the other two including the one without decoder - decoder residual connection are able to accomplish the task by learning to make decisions based on their prior predictions . Table 3 provides some illustrative sample outputs of the models . For the MT task , results can be found in Table 2 . While the drop from removing decoder - encoder residual connection is significant , it is still able to perform reasonably well since the task can be largely fulfilled by mapping different words from one sentence to another . For positional masking , our proof technique suggests that due to lack of positional encodings , the model must come up with its own mechanism to make order related decisions . Our hypothesis is that , if it is able to develop such a mechanism , it should be able to generalize to higher lengths and not overfit on the data it is provided . To evaluate this claim , we simply extend the copy task upto higher lengths . The training set remains the same as before , containing sentences of length 5 - 12 words . We create 5 different validation sets each containing 1k sentences each . The first set contains sentences within the same length as seen in training ( 5 - 12 words ) , the second set contains sentences of length 13 - 15 words while the third , fourth and fifth sets contain sentences of lengths 15 - 20 , 21 - 25 and 26 - 30 words respectively . We consider two models , one which is provided absolute positional encodings and one where only positional masking is applied . Figure 3 shows the performance of these models across various lengths . The model with positional masking clearly generalizes up to higher lengths although its performance too degrades at extreme lengths . We found that the model with absolute positional encodings during training overfits on the fact that the 13th token is always the terminal symbol . Hence , when evalu - ated on higher lengths it never produces a sentence of length greater than 12 . Other encoding schemes such as relative positional encodings ( Shaw et al , 2018 ; Dai et al , 2019 ) can generalize better , since they are inherently designed to address this particular issue . However , our goal is not to propose masking as a replacement of positional encodings , rather it is to determine whether the mechanism that the model develops during training is helpful in generalizing to higher lengths . Note that , positional masking was not devised by keeping generalization or any other benefit in mind . Our claim is only that , the use of masking does not limit the model 's expressiveness and it may benefit in other ways , but during practice one should explore each of the mechanisms and even a combination of both . showed that a combination of both masking and encodings is better able to learn order information as compared to explicit encodings .", "entities": [[89, 91, "MethodName", "residual connection"], [255, 257, "DatasetName", "Penn Treebank"], [298, 299, "DatasetName", "0"], [375, 376, "MethodName", "Transformer"], [383, 385, "MethodName", "residual connection"], [398, 400, "MethodName", "residual connection"], [445, 447, "TaskName", "Machine Translation"], [487, 488, "MethodName", "Transformer"], [499, 501, "MethodName", "residual connection"], [510, 513, "HyperparameterName", "number of layers"], [519, 521, "HyperparameterName", "learning rate"], [598, 600, "MethodName", "residual connection"], [618, 620, "MethodName", "residual connection"], [669, 671, "MethodName", "residual connection"]]}
{"text": "Denote the set { 1 , 2 , . . . , n } by [ n ] . Functions defined for scalars are extended to vectors in the natural way : for a function F defined on a set A , for a sequence ( a 1 , . . . , a n ) of elements in A , we set F ( a 1 , . . . , a n ) : = ( F ( a 1 ) , . . . , F ( a n ) ) . Indicator I ( P ) is 1 , if predicate P is true and is 0 otherwise . For a sequence X = ( x n , . . . , x n ) for some n \u2265 0 , we set X j : = ( x n , . . . , x j ) for j { n , i+1 , . . . , n } . We will work with an alphabet \u03a3 = { \u03b2 1 , . . . , \u03b2 m } , with \u03b2 1 = # and \u03b2 m = $ . The special symbols # and $ correspond to the beginning and end of the input sequence , resp . For a vector v , by 0 v we mean the all - 0 vector of the same dimension as v. Lett : = min { t , n }", "entities": [[110, 111, "DatasetName", "0"], [133, 134, "DatasetName", "0"], [175, 176, "HyperparameterName", "\u03b2"], [182, 183, "HyperparameterName", "\u03b2"], [187, 188, "HyperparameterName", "\u03b2"], [192, 193, "HyperparameterName", "\u03b2"], [222, 223, "DatasetName", "0"], [229, 230, "DatasetName", "0"]]}
{"text": "Here we describe the original transformer architecture due to ( Vaswani et al , 2017 ) as formalized by ( P\u00e9rez et al , 2019 ) . While our notation and definitions largely follow ( P\u00e9rez et al , 2019 ) , they are not identical . The transformer here makes use of positional encoding ; later we will discuss the transformer variant using directional attention but without using positional encoding . The transformer , denoted Trans , is a sequenceto - sequence architecture . Its input consists of ( i ) a sequence X = ( x 1 , . . . , x n ) of vectors in Q d , ( ii ) a seed vector y 0 Q d . The output is a sequence Y = ( y 1 , . . . , y r ) of vectors in Q d . The sequence X is obtained from the sequence ( s 0 , . . . , s n ) \u03a3 n+1 of symbols by using the embedding mentioned earlier : x i = f ( f b ( s i ) , pos ( i ) ) for 0 \u2264 i \u2264 n. The transformer consists of composition of transformer encoder and a transformer decoder . The transformer encoder is obtained by composing one or more single - layer encoders and similarly the transformer decoder is obtained by composing one or more single - layer decoders . For the feed - forward networks in the transformer layers we use the activation as in ( Siegelmann and Sontag , 1992 ) , namely the saturated linear activation function : \u03c3 ( x ) = \uf8f4 \uf8f2 \uf8f4 \uf8f3 0 if x < 0 , x if 0 \u2264 x \u2264 1 , 1 if x > 1 . ( 4 ) As mentioned in the main paper , we can easily work with the standard ReLU activation via \u03c3 ( x ) = ReLU ( x ) \u2212 ReLU ( x \u2212 1 ) . In the following , after defining these components , we will put them together to specify the full transformer architecture . But we begin with self - attention mechanism which is the central feature of the transformer . Self - attention . The self - attention mechanism takes as input ( i ) a query vector q , ( ii ) a sequence of key vectors K = ( k 1 , . . . , k n ) , and ( iii ) a sequence of value vectors V = ( v 1 , . . . , v n ) . All vectors are in Q d . The q - attention over keys K and values V , denoted by Att ( q , K , V ) , is a vector a given by ( \u03b1 1 , . . . , \u03b1 n ) = \u03c1 ( f att ( q , k 1 ) , . . . , f att ( q , k n ) ) , a = \u03b1 1 v 1 + \u03b1 2 v 2 + + \u03b1 n v n . The above definition uses two functions \u03c1 and f att which we now describe . For the normalization function \u03c1 : Q n Q n \u22650 we will use hardmax : for x = ( x 1 , . . . , x n ) Q n , if the maximum value occurs r times among x 1 , . . . , x n , then hardmax ( x ) i : = 1 / r if x i is a maximum value and hardmax ( x ) i : = 0 otherwise . In practice , the softmax is often used but its output values are in general not rational . The names soft - attention and hard - attention are used for the attention mechanism depending on which normalization function is used . For the Turing - completeness proof of vanilla transformers , the scoring function f att used is a combination of multiplicative attention ( Vaswani et al , 2017 ) and a non - linear function : f att ( q , k i ) = \u2212 q , k i . For directional trans - formers , the standard multiplicative attention is used , that is , f att ( q , k i ) = q , k i . Transformer encoder . A single - layer encoder is a function Enc ( X ; \u03b8 ) , where \u03b8 is the parameter vector and the input X = ( x 1 , . . . , x n ) is a sequence of vector in Q d . The output is another sequence Z = ( z 1 , . . . , z n ) of vectors in Q d . The parameters \u03b8 specify functions Q ( ) , K ( ) , V ( ) , and O ( ) , all of type Q d Q d . The functions Q ( ) , K ( ) , and V ( ) are usually linear transformations and this will be the case in our constructions : Q ( x i ) = x T i W Q , K ( x i ) = x T i W K , V ( x i ) = x T i W V , where W Q , W K , W V Q d\u00d7d . The function O ( ) is a feed - forward network . The single - layer encoder is then defined by a i = Att ( Q ( x i ) , K ( X ) , V ( X ) ) + x i , ( 5 ) z i = O ( a i ) + a i . The addition operations + x i and + a i are the residual connections . The operation in ( 5 ) is called the encoder - encoder attention block . The complete L - layer transformer encoder TEnc ( L ) ( X ; \u03b8 ) has the same input X = ( x 1 , . . . , x n ) as the single - layer encoder . By contrast , its output consists of two sequences ( K e , V e ) , each a sequence of n vectors in Q d . The encoder TEnc ( L ) ( ) is obtained by repeated application of single - layer encoders , each with its own parameters ; and at the end , two trasformation functions K L ( ) and V L ( ) are applied to the sequence of output vectors at the last layer . Functions K ( L ) ( ) and V ( L ) ( ) are linear transformations in our constructions . Formally , for 1 \u2264 \u2264 L \u2212 1 and X 1 : = X , we have X +1 = Enc ( X ; \u03b8 ) , K e = K ( L ) ( X L ) , V e = V ( L ) ( X L ) . The output of the L - layer Transformer encoder ( K e , V e ) = TEnc ( L ) ( X ) is fed to the Transformer decoder which we describe next . Transformer decoder . The input to a singlelayer decoder is ( i ) ( K e , V e ) , the sequences of key and value vectors output by the encoder , and ( ii ) a sequence Y = ( y 1 , . . . , y k ) of vectors in Q d . The output is another sequence Z = ( z 1 , . . . , z k ) of vectors in Q d . Similar to the single - layer encoder , a singlelayer decoder is parameterized by functions Q ( ) , K ( ) , V ( ) and O ( ) and is defined by p t = Att ( Q ( y t ) , K ( Y t ) , V ( Y t ) ) + y t , ( 6 ) a t = Att ( p t , K e , V e ) + p t , ( 7 ) z t = O ( a t ) + a t . The operation in ( 6 ) will be referred to as the decoder - decoder attention block and the operation in ( 7 ) as the decoder - encoder attention block . In the decoder - decoder attention block , positional masking is applied to prevent the network from attending over symbols which are ahead of them . An L - layer Transformer decoder is obtained by repeated application of L single - layer decoders each with its own parameters and a transformation function F : Q d Q d applied to the last vector in the sequence of vectors output by the final decoder . Formally , for 1 \u2264 \u2264 L \u2212 1 and Y 1 = Y we have Y +1 = Dec ( ( K e , V e ) , Y ; \u03b8 ) , z = F ( y L t ) . We use z = TDec L ( ( K e , V e ) , Y ; \u03b8 ) to denote an L - layer Transformer decoder . Note that while the output of a single - layer decoder is a sequence of vectors , the output of an L - layer Transformer decoder is a single vector . The complete Transformer . A Transformer network receives an input sequence X , a seed vector y 0 , and r N. For t \u2265 0 its output is a sequence Y = ( y 1 , . . . , y r ) defined b\u1ef9 y t+1 = TDec TEnc ( X ) , ( y 0 , y 1 , . . . , y t ) . We get y t+1 by adding positional encoding : y t+1 = \u1ef9 t+1 + pos ( t + 1 ) . We denote the complete Transformer by Trans ( X , y 0 ) = Y . The Transformer \" halts \" when y T H , where H is a prespecified halting set . a t = Att ( p t , K e , V e ) + p t The result follows from the observation that without the residual connections , a t = Att ( p t , K e , V e ) , which leads to a t = n i=1 \u03b1 i v e i for some \u03b1 i s such that n i \u03b1 i = 1 . Since v e i is produced from the encoder , the vector a t will have no information about its previous hidden state values . Since the previous hidden state information was computed and stored in p t , without the residual connection , the information in a t depends solely on the output of the encoder . One could argue that since the attention weights \u03b1 i s depend on the query vector p t , it could still use it gain the necessary information from the vectors v e i s. However , note that by definition of hard attention , the attention weights \u03b1 i in a t = n i=1 \u03b1 i v e i can either be zero or some nonzero value depending on the attention logits . Since the attention weights \u03b1 i are such that n i \u03b1 i = 1 and all the nonzero weights are equal to each other . Thus given the constraints there are 2 n \u22121 ways to attend over n inputs excluding the case where no input is attended over . Hence , the network without decoder - encoder residual connection with n inputs can have at most 2 n \u22121 distinct a t values . This implies that the model will be unable to perform a task that takes n inputs and has to produce more than 2 n \u2212 1 outputs . Note that , such a limitation will not exist with a residual connection since the vector a t = \u03a3 n i=1 \u03b1 i v e i + p t can take arbitrary number of values depending on its prior computations in p t . As an example to illustrate the limitation , consider the following simple problem , given a value \u2206 , where 0 \u2264 \u2206 \u2264 1 , the network must produce the values 0 , \u2206 , 2\u2206 , . . . , k\u2206 , where k is the maximum integer such that k\u2206 \u2264 1 . If the network receives a single input \u2206 , the encoder will produce only one particular output vector and regardless of what the value of the query vector p t is , the vector a t will be constant at every timestep . Since a t is fed to feedforward network which maps it to z t , the output of the decoder will remain the same at every timestep and it can not produce distinct values . If the input is combined with n \u2212 1 auxiliary symbols ( such as # and $ ) , then the network can only produce 2 n \u22121 outputs . Hence , the model will be incapable of performing the task if \u2206 < 1/2 n . Thus the model can not perform the task defined above which RNNs and Vanilla Transformers can easily do with a simple counting mechanism via their recurrent connection . For the case of multilayer decoder , consider any L layer decoder model . If the residual connection is removed , the output of decoder - encoder attention block at each layer is a ( ) t = n i=1 \u03b1 ( ) i v e i for 1 \u2264 \u2264 L. Observe , that since output of the decoder - encoder attention block in the last ( L - th ) layer of the decoder is a ( L ) t = n i=1 \u03b1 ( L ) i v e i . Since the output of the L layer decoder will be a feedforward network over a ( L ) t , the computation reduces to the single layer decoder case . Hence , similar to the single layer case , if the task requires the network to produce values of a t that come from a set with size at least 2 n , then the network will not be able to perform the task . This implies that the model without decoderencoder residual connection is limited in its capability to perform tasks which requires it to make inferences based on previously generated outputs .", "entities": [[120, 121, "DatasetName", "0"], [158, 159, "DatasetName", "0"], [196, 197, "DatasetName", "0"], [211, 213, "MethodName", "transformer decoder"], [231, 233, "MethodName", "transformer decoder"], [273, 275, "HyperparameterName", "activation function"], [285, 286, "DatasetName", "0"], [289, 290, "DatasetName", "0"], [293, 294, "DatasetName", "0"], [322, 323, "MethodName", "ReLU"], [330, 331, "MethodName", "ReLU"], [335, 336, "MethodName", "ReLU"], [408, 410, "HyperparameterName", "K ="], [481, 482, "HyperparameterName", "\u03b1"], [488, 489, "HyperparameterName", "\u03b1"], [519, 520, "HyperparameterName", "\u03b1"], [524, 525, "HyperparameterName", "\u03b1"], [530, 531, "HyperparameterName", "\u03b1"], [627, 628, "DatasetName", "0"], [634, 635, "MethodName", "softmax"], [691, 693, "MethodName", "multiplicative attention"], [730, 732, "MethodName", "multiplicative attention"], [752, 753, "MethodName", "Transformer"], [767, 768, "HyperparameterName", "\u03b8"], [771, 772, "HyperparameterName", "\u03b8"], [827, 828, "HyperparameterName", "\u03b8"], [1036, 1037, "HyperparameterName", "\u03b8"], [1190, 1191, "HyperparameterName", "\u03b8"], [1224, 1225, "MethodName", "Transformer"], [1245, 1247, "MethodName", "Transformer decoder"], [1252, 1254, "MethodName", "Transformer decoder"], [1493, 1495, "MethodName", "Transformer decoder"], [1568, 1569, "HyperparameterName", "\u03b8"], [1597, 1598, "HyperparameterName", "\u03b8"], [1605, 1607, "MethodName", "Transformer decoder"], [1632, 1634, "MethodName", "Transformer decoder"], [1641, 1642, "MethodName", "Transformer"], [1644, 1645, "MethodName", "Transformer"], [1656, 1657, "DatasetName", "0"], [1664, 1665, "DatasetName", "0"], [1696, 1697, "DatasetName", "0"], [1735, 1736, "MethodName", "Transformer"], [1742, 1743, "DatasetName", "0"], [1748, 1749, "MethodName", "Transformer"], [1817, 1818, "HyperparameterName", "\u03b1"], [1824, 1825, "HyperparameterName", "\u03b1"], [1831, 1832, "HyperparameterName", "\u03b1"], [1877, 1879, "MethodName", "residual connection"], [1902, 1903, "HyperparameterName", "\u03b1"], [1942, 1943, "HyperparameterName", "\u03b1"], [1950, 1951, "HyperparameterName", "\u03b1"], [1973, 1974, "HyperparameterName", "\u03b1"], [1980, 1981, "HyperparameterName", "\u03b1"], [2028, 2030, "MethodName", "residual connection"], [2084, 2086, "MethodName", "residual connection"], [2095, 2096, "HyperparameterName", "\u03b1"], [2138, 2139, "DatasetName", "0"], [2150, 2151, "DatasetName", "0"], [2223, 2225, "MethodName", "feedforward network"], [2343, 2345, "MethodName", "residual connection"], [2367, 2368, "HyperparameterName", "\u03b1"], [2412, 2413, "HyperparameterName", "\u03b1"], [2432, 2434, "MethodName", "feedforward network"], [2503, 2505, "MethodName", "residual connection"]]}
{"text": "Proof of Lemma C.3 . We construct a single - layer encoder achieving the desired K e and V e . We make use of the residual connections and via trivial selfattention we get that z i = x i . More specifically for i [ n ] we have V ( 1 ) ( x i ) = 0 , a i = 0 + x i , O ( a i ) = 0 , z i = 0 + a i = x i . V ( 1 ) ( x i ) = 0 can be achieved by setting the weight matrix as the all - 0 matrix . Recall that x i is defined as x i = [ 0 h , s i , 0 h , i , 1 ] . We then apply linear transformations in K ( z i ) = z i W k and V ( z i ) = z i W v , where W T k = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 0 0 0 0 . . . . . . . . . . . . 0 0 0 0 0 0 0 1 0 0 \u22121 0 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb , and W k Q d\u00d7d , and similarly one can obtain v i by setting the submatrix of W v Q d\u00d7d formed by the first d \u2212 2 rows and columns to the identity matrix , and the rest of the entries to zeros . Lemma C.5 . Let q t Q d be a query vector such that q = [ , . . . , , t + 1 , 1 ] where t N and ' ' denotes an arbitrary value . Then we have Att ( q t , K e , V e ) = [ 0 h , s t+1 , 0 h , 0 , 0 ] . ( 11 ) Proof . Recall that p t = y t = [ h t , 0 , . . . , 0 , t + 1 , 1 ] and k i = [ 0 , 0 , . . . , 0 , \u22121 , i ] and hence p t , k i = i \u2212 ( t + 1 ) , f att ( p t , k i ) = \u2212 | i \u2212 ( t + 1 ) | . Thus , for i [ n ] , the scoring functionf att ( p t , k i ) has the maximum value 0 at index i = t + 1 if t < n ; for t \u2265 n , the maximum value t + 1 \u2212 n is achieved for i = n. Therefore Att ( p t , K e , V e ) = s t+1 . Proof of Lemma C.4 . Recall that a t = [ h t , s t+1 , 0 h , t + 1 , 1 ] Network O ( a t ) is of the form O ( a t ) = W 2 \u03c3 ( W 1 a t + b 1 ) , where W i Q d\u00d7d and b Q d and W 1 = d h d e d h 2 d h d e d h 2 \uf8ee \uf8ef \uf8ef \uf8ef \uf8f0 W h W x 0 0 0 I 0 0 I 0 0 0 0 0 0 I \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb and b 1 = [ b h , 0 s , 0 h , 0 , 0 ] . Hence \u03c3 ( W 1 a t + b 1 ) = [ \u03c3 ( W h h t + W x s t+1 + b ) , s t+1 , h t , t + 1 , 1 ] Next we define W 2 by W 2 = d h d e d h 2 d h d e d h 2 \uf8ee \uf8ef \uf8ef \uf8ef \uf8f0 I 0 \u2212I 0 0 \u2212I 0 0 0 0 0 0 0 0 0 \u2212I \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb . This leads to O ( a t ) = W 2 \u03c3 ( W 1 a t + b 1 ) = [ \u03c3 ( W h h t + W x s t+1 + b ) \u2212 h t , \u2212s t+1 , 0 h , \u2212 ( t + 1 ) , \u22121 ] , which is what we wanted to prove .", "entities": [[2, 3, "DatasetName", "Lemma"], [59, 60, "DatasetName", "0"], [64, 65, "DatasetName", "0"], [75, 76, "DatasetName", "0"], [80, 81, "DatasetName", "0"], [97, 98, "DatasetName", "0"], [110, 111, "DatasetName", "0"], [113, 114, "MetricName", "Recall"], [124, 125, "DatasetName", "0"], [130, 131, "DatasetName", "0"], [169, 171, "HyperparameterName", "k ="], [179, 180, "DatasetName", "0"], [180, 181, "DatasetName", "0"], [181, 182, "DatasetName", "0"], [182, 183, "DatasetName", "0"], [195, 196, "DatasetName", "0"], [196, 197, "DatasetName", "0"], [197, 198, "DatasetName", "0"], [198, 199, "DatasetName", "0"], [199, 200, "DatasetName", "0"], [200, 201, "DatasetName", "0"], [201, 202, "DatasetName", "0"], [203, 204, "DatasetName", "0"], [204, 205, "DatasetName", "0"], [206, 207, "DatasetName", "0"], [262, 263, "DatasetName", "Lemma"], [318, 319, "DatasetName", "0"], [324, 325, "DatasetName", "0"], [327, 328, "DatasetName", "0"], [329, 330, "DatasetName", "0"], [337, 338, "MetricName", "Recall"], [349, 350, "DatasetName", "0"], [355, 356, "DatasetName", "0"], [368, 369, "DatasetName", "0"], [370, 371, "DatasetName", "0"], [376, 377, "DatasetName", "0"], [442, 443, "DatasetName", "0"], [492, 493, "DatasetName", "Lemma"], [495, 496, "MetricName", "Recall"], [507, 508, "DatasetName", "0"], [581, 582, "DatasetName", "0"], [582, 583, "DatasetName", "0"], [583, 584, "DatasetName", "0"], [585, 586, "DatasetName", "0"], [586, 587, "DatasetName", "0"], [588, 589, "DatasetName", "0"], [589, 590, "DatasetName", "0"], [590, 591, "DatasetName", "0"], [591, 592, "DatasetName", "0"], [592, 593, "DatasetName", "0"], [593, 594, "DatasetName", "0"], [608, 609, "DatasetName", "0"], [611, 612, "DatasetName", "0"], [614, 615, "DatasetName", "0"], [616, 617, "DatasetName", "0"], [688, 689, "DatasetName", "0"], [690, 691, "DatasetName", "0"], [691, 692, "DatasetName", "0"], [693, 694, "DatasetName", "0"], [694, 695, "DatasetName", "0"], [695, 696, "DatasetName", "0"], [696, 697, "DatasetName", "0"], [697, 698, "DatasetName", "0"], [698, 699, "DatasetName", "0"], [699, 700, "DatasetName", "0"], [700, 701, "DatasetName", "0"], [701, 702, "DatasetName", "0"], [753, 754, "DatasetName", "0"]]}
{"text": "The models under consideration are the vanilla Transformer , the one without decoder - encoder residual connection and the one without decoderdecoder residual connection . For the synthetic tasks , we implement a single layer encoder - decoder network with only a single attention head in each block . Our implementation of the Transformer is adapted from the implementation of ( Rush , 2018 ) . Table 4 provides some illustrative sample outputs of the models for the copy task . For the machine translation task , we use Open - NMT ( Klein et al , 2017 ) for our implementation . For preprocessing the German - English dataset we used the script from fairseq . The dataset contains about 153k training sentences , 7k development sentences and 7k test sentences . The hyperparameters to train the vanilla Transformer were obtained from fairseq 's guidelines . We tuned the parameters on the validation set for the two baseline model . To preprocess the English - Vietnamese dataset , we follow Luong and Manning ( 2015 ) . The dataset contains about 133k training sentences . We use the tst2012 dataset containing 1.5k sentences for validation and tst2013 containing 1.3k sentences as test set . We use noam optimizer in all our experiments . While tuning the network , we vary the number of layer from 1 to 4 , the learning rate , the number of heads , the warmup steps , embedding size and feedforward embedding size .", "entities": [[7, 8, "MethodName", "Transformer"], [15, 17, "MethodName", "residual connection"], [22, 24, "MethodName", "residual connection"], [53, 54, "MethodName", "Transformer"], [83, 85, "TaskName", "machine translation"], [139, 140, "MethodName", "Transformer"], [208, 209, "HyperparameterName", "optimizer"], [231, 233, "HyperparameterName", "learning rate"]]}
{"text": "Our implementation for directional transformer is based on but we use only unidirectional masking as opposed to bidirectional used in their setup . While tuning the models , we vary the layers from 1 to 4 , the learning rate , warmup steps and the number of heads .", "entities": [[38, 40, "HyperparameterName", "learning rate"]]}
{"text": "vectors h i s are reserved for computation related to hidden states of RNNs , s i s are reserved for input embeddings and x i s are reserved for scalar values related to positional operations . Given an input sequence s 0 s 1 s 2 s n \u03a3 * where s 0 = # and s n = $ , we use an embedding function f : \u03a3 Q d defined as Unlike ( P\u00e9rez et al , 2019 ) , we use the dot product as our scoring function as used in Vaswani et al ( 2017 ) in the attention mechanism in our construction , f att ( q i , k j ) = q i , k j . For the computation of the Transformer , we also use a vector sequence in Q | \u03a3 | defined by where 0 \u2264 t \u2264 n. The vector \u03c9 t = ( \u03c9 t , 1 , . . . , \u03c9 t , | \u03a3 | ) contains the proportion of each input symbol till step t for 0 \u2264 t \u2264 n. Set \u03c9 \u22121 = 0 . From the defintion of \u03c9 t , it follows that at any step 1 \u2264 k \u2264 | \u03a3 | we have where \u03c6 t , k denotes the number of times the k - th symbol \u03b2 k in \u03a3 has appeared till the t - th step . Note that \u03c9 t , 0 = 1 t+1 since the first coordinate corresponds to the proportion of the start symbol # which appears only once at t = 0 . Similarly , \u03c9 t , | \u03a3 | = 0 for 0 \u2264 t < n and \u03c9 t , | \u03a3 | = 1/ ( t + 1 ) for t \u2265 n , since the end symbol $ does n't appear till the end of the input and it appears only once at t = n. We define two more sequences of vectors in Here \u2206 t denotes the difference in the proportion of symbols between the t - th and ( t \u2212 1 ) - th steps , with the applicatin of sigmoid activation . In vector \u03b4 t , the last coordinate of \u2206 t has been replaced with 1/2 t+1 . The last coordinate in \u03c9 t indicates the proportion of the terminal symbol $ and hence the last value in \u2206 t denotes the change in proportion of $ . We set the last coordinate in \u03b4 t to an exponentially decreasing sequence so that after n steps we always have a nonzero score for the terminal symbol and it is taken as input in the underlying RNN . Different and perhaps simpler choices for the last coordinate of \u03b4 t may be possible . Note that 0 \u2264 \u2206 t , k \u2264 1 and 0 \u2264 \u03b4 t , k \u2264 1 for 0 \u2264 t \u2264 n and 1 \u2264 k \u2264 | \u03a3 | . Construction of TEnc . The input to the network DTrans M is the sequence ( s 0 , s 1 , . . . , s n\u22121 , s n ) where s 0 = # and s n = $ . Our encoder is a simple single layer network such that TEnc ( x 0 , x 1 , . . . , x n ) = ( K e , V e ) where K e = ( k e 0 , . . . , k e n ) and V e = ( v e 0 , . . . , v e n ) such that , Similar to our construction of the encoder for vanilla transformer ( Lemma C.3 ) , the above K e and V e can be obtained by making the output of Att ( ) = 0 by choosing the V ( ) to always evaluate to 0 and similarly for O ( ) , and using residual connections . Then one can produce K e and V e via simple linear transformations using K ( ) and V ( ) . Construction of TDec . At the t - th step we denote the input to the decoder as y t = \u1ef9 t , where 0 \u2264 t \u2264 r , where r is the step where the decoder halts . Let h \u22121 = 0 h and h 0 = 0 h . We will prove by induction on t that for 0 \u2264 t \u2264 r we have This is true for t = 0 by the choice of seed vector : Assuming the truth of ( 14 ) for t , we show it for t + 1 . Layer 1 . Similar to the construction in Lemma C.3 , in the decoder - decoder attention block we set V ( 1 ) ( ) = 0 d and use the residual connections to set p ( 1 ) t = y t . At the t - th step in the decoder - encoder attention block of layer 1 we have where ( \u03b1 t , k \u0113 t = hardmax ( 0 , . . . , 0 ) where In Lemma D.2 we construct feed - forward network Layer 2 . In the first block of layer 2 , we set the value transformation function to identically zero similar to Lemma C.3 , i.e. V ( 2 ) ( ) = 0 which leads to the output of Att ( ) to be 0 and then using the residual connection we get p In the final block of the decoder in the second layer , the computation for RNN takes place . In Lemma D.4 below we construct the feed - forward proving the induction hypothesis ( 14 ) for t + 1 , and completing the simulation of RNN .", "entities": [[42, 43, "DatasetName", "0"], [53, 54, "DatasetName", "0"], [130, 131, "MethodName", "Transformer"], [146, 147, "DatasetName", "0"], [184, 185, "DatasetName", "0"], [193, 194, "DatasetName", "0"], [232, 233, "HyperparameterName", "\u03b2"], [250, 251, "DatasetName", "0"], [274, 275, "DatasetName", "0"], [285, 286, "DatasetName", "0"], [287, 288, "DatasetName", "0"], [372, 374, "MethodName", "sigmoid activation"], [377, 378, "HyperparameterName", "\u03b4"], [429, 430, "HyperparameterName", "\u03b4"], [472, 473, "HyperparameterName", "\u03b4"], [480, 481, "DatasetName", "0"], [489, 490, "DatasetName", "0"], [491, 492, "HyperparameterName", "\u03b4"], [498, 499, "DatasetName", "0"], [528, 529, "DatasetName", "0"], [545, 546, "DatasetName", "0"], [567, 568, "DatasetName", "0"], [594, 595, "DatasetName", "0"], [611, 612, "DatasetName", "0"], [635, 636, "DatasetName", "Lemma"], [658, 659, "DatasetName", "0"], [669, 670, "DatasetName", "0"], [729, 730, "DatasetName", "0"], [749, 750, "DatasetName", "0"], [753, 754, "DatasetName", "0"], [755, 756, "DatasetName", "0"], [767, 768, "DatasetName", "0"], [780, 781, "DatasetName", "0"], [814, 815, "DatasetName", "Lemma"], [833, 834, "DatasetName", "0"], [871, 872, "HyperparameterName", "\u03b1"], [880, 881, "DatasetName", "0"], [886, 887, "DatasetName", "0"], [890, 891, "DatasetName", "Lemma"], [920, 921, "DatasetName", "Lemma"], [931, 932, "DatasetName", "0"], [943, 944, "DatasetName", "0"], [948, 950, "MethodName", "residual connection"], [973, 974, "DatasetName", "Lemma"]]}
{"text": "which is what we wanted to prove . where t \u2265 0 and ' ' denotes an arbitrary value . Then we have Proof . Let t , k \u0113 t be the vector of normalized attention scores in the decoder - encoder attention block of layer 2 at time t. Then We claim that Claim 1 . For t \u2265 0 we have where \u03bb t is a normalization factor given by \u03bb t = n\u22121 j=0 I ( s j = s t ) . We now prove the lemma assuming the claim above . Denote the L.H.S. in ( 16 ) by \u03b3 t . Note that if s j = s t , then v e j = \u03b3 t . Now we hav\u0113 completing the proof of the lemma modulo the proof of the claim , which we prove next . Proof . ( of Claim 1 ) For 0 < t \u2264 n , the vector \u03c9 t \u2212 \u03c9 t\u22121 has the form The last inequality used our assumption that s 0 = # and that # does not occur at any later time and therefore \u03c6 t\u22121 , j < t. On the other hand , if s t = \u03b2 k , then \u2264 0 . This leads to , In words , the change in the proportion of a symbol is positive from step t \u2212 1 to t if and only if it is the input symbol at the t - th step . For 0 \u2264 t \u2264 n and 1 \u2264 k \u2264 | \u03a3 | , this leads to Recall that p ( 2 ) t = z ( 1 ) t which comes from ( 15 ) , and k e j is defined in ( 13 ) . We reproduce these for convenience : \u03b4t , 1 2 t+1 , 0 \u03c9 , 0 \u03c9 , \u03c9t ] , k e j = [ 0 h , 0 h , 0 s , s j , 0 , 0 \u03c9 , 0 \u03c9 , 0 \u03c9 ] . It now follows that for 0 < t < n , if 0 \u2264 j \u2264 t is such that s j = s t , then t , k e j = \u03b4 t , s j = \u03b4 t , i = 0 . And for 0 < t < n , if 0 \u2264 j \u2264 t is such that s j = s t = \u03b2 i , then t , k e j = \u03b4 t , s j = \u03b4 t , i = t \u2212 \u03c6 t\u22121 , j t ( t + 1 ) \u2265 1 t ( t + 1 ) . Thus , for 0 \u2264 t < n , in the vector p ( 2 ) t , k e 0 , . . . , p t , k e t , the largest coordinates are the ones indexed by j with s j = s t and they all equal t\u2212\u03c6 t\u22121 , i t ( t+1 ) . All other coordinates are 0 . For t \u2265 n , only the last coordinate p", "entities": [[11, 12, "DatasetName", "0"], [61, 62, "DatasetName", "0"], [91, 92, "DatasetName", "lemma"], [105, 106, "HyperparameterName", "\u03b3"], [122, 123, "HyperparameterName", "\u03b3"], [133, 134, "DatasetName", "lemma"], [154, 155, "DatasetName", "0"], [178, 179, "DatasetName", "0"], [208, 209, "HyperparameterName", "\u03b2"], [213, 214, "DatasetName", "0"], [256, 257, "DatasetName", "0"], [273, 274, "MetricName", "Recall"], [316, 317, "DatasetName", "0"], [319, 320, "DatasetName", "0"], [330, 331, "DatasetName", "0"], [333, 334, "DatasetName", "0"], [336, 337, "DatasetName", "0"], [342, 343, "DatasetName", "0"], [344, 345, "DatasetName", "0"], [347, 348, "DatasetName", "0"], [350, 351, "DatasetName", "0"], [359, 360, "DatasetName", "0"], [366, 367, "DatasetName", "0"], [387, 388, "HyperparameterName", "\u03b4"], [393, 394, "HyperparameterName", "\u03b4"], [398, 399, "DatasetName", "0"], [402, 403, "DatasetName", "0"], [409, 410, "DatasetName", "0"], [423, 424, "HyperparameterName", "\u03b2"], [433, 434, "HyperparameterName", "\u03b4"], [439, 440, "HyperparameterName", "\u03b4"], [468, 469, "DatasetName", "0"], [485, 486, "DatasetName", "0"], [530, 531, "DatasetName", "0"]]}
{"text": "Multimodal sentiment analysis ( MSA ) has been an emerging research field for its potential applications in human - computer interaction . How to effectively fuse multimodal information including textual , acoustic , and visual to predict the sentiment is a very challenging problem and has been addressed by many previous studies . Some works focus on introducing additional information into the fusing model , such as the alignment information between different modal features and unimodal sentiment labels ( Yu et al , 2021 ) . And other works consider the semantic gaps between multimodal data and adopt the adversarial learning ( Mai et al , 2020 ) and multi - task learning ( Hazarika et al , 2020 ) to map different modal features into a shared subspace . Despite the apparent success of the current state - of - the - art models , their performance decreases sharply , when deployed in the real world . The reason is that the input texts are provided by the ASR models , which usually are with errors because of the limitation of model capacity . To further analyze this problem , we build three real - world multimodal sentiment analysis datasets based on the existing dataset , CMU - MOSI ( Zadeh et al , 2016 ) . Specifically , we adopt three widely used ASR APIs including SpeechBrain , IBM , and iFlytek to process the original audios and obtain the recognized texts . Then , we replace the gold texts in CMU - MOSI with the ASR results and get three realworld datasets , namely MOSI - SpeechBrain , MOSI - IBM , and MOSI - iFlytek . We evaluate the current state - of - the - art model , Self - MM ( Yu et al , 2021 ) , and report the mean absolute error ( MAE ) on the multimodal sentiment analysis task . As we can see in Figure 1 ( a ) , when the model is deployed in the real world , there is an obvious drop in model performance . The further in - depth analysis of ASR errors shows that the sentiment word substitution error can hurt the MSA model directly . The reason is that the sentiment words in the text are the most important clues in the textual modality for detecting sentiment and incorrectly recognizing them could change the sentiment conveyed by the text . To have an intuitive understanding of the sentiment word substitution error , we take an example in Figure 1 ( b ) . The gold text is \" And I was really upset about it \" , but the ASR model ( SpeechBrain ) recognizes the sentiment word \" upset \" wrongly as \" set \" , which results in the change of the sentiment semantics of the text and directly affects the MSA model performance . We list the percentages of the sentiment word substitution error on the MOSI dataset for three ASR APIs in Figure 1 ( b ) . The percentage of the sentiment word substitution error on the MOSI - IBM is 17.6 % , which means about 17 of 100 utterances have this type of error . To further demonstrate the negative effect of the substitution error on the MSA models , we split the test data of MOSI - IBM into two groups by whether there is a substitution error . We evaluate Self - MM on the test data and observe that the misclassification rate of the group in which the substitution error exists is higher than the other group ( 29.9 % vs 15.8 % ) . This result indicates that the sentiment word substitution error could hurt the state - of - the - art MSA model . To tackle this problem , we propose the sentiment word aware multimodal refinement model , which can detect the positions of the sentiment words in the text and dynamically refine the word embeddings in the detected positions by incorporating multimodal clues . The basic idea of our approach is shown in Figure 1 ( c ) . We consider leveraging the multimodal sentiment information , namely the negative sentiment conveyed by the low voice and sad face , and textual context information to help the model reconstruct the sentiment semantics for the input embeddings . Specifically , we first use the sentiment word location module to detect the positions of sentiment words and meanwhile utilize the strong language model , BERT , to generate the candidate sentiment words . Then we propose the multimodal sentiment word refinement module to refine the word embeddings based on the multimodal context information . The refinement process consists of two parts , filtering and adding . We apply the multimodal gating network to filter out useless information from the input word embeddings in the filtering process and use the multimodal sentiment word attention network to leverage the useful information from candidate sentiment words as the supplement to the filtered word embeddings in the adding process . Finally , the refined sentiment word embeddings are used for multimodal feature fusion . We conduct extensive experiments on the MOSI - SpeechBrain , MOSI - IBM , and MOSI - iFlytek datasets to demonstrate the effectiveness of our proposed model . The experimental results show that : ( 1 ) There is an obvious performance drop for the state - of - the - art MSA model , when the model is deployed in the real world taking the ASR outputs as the input of textual modality ; ( 2 ) Our proposed model outperforms all baselines , which can dynamically refine the sentiment word embeddings by leveraging multimodal information . The main contributions of this work are as follows : ( 1 ) We propose a novel sentiment word aware multimodal refinement model for multimodal sentiment analysis , which can dynamically reconstruct the sentiment semantics of the ASR texts with errors by utilizing the multimodal sentiment information resulting in more robust sentiment prediction ; ( 2 ) We validate the negative effect of the sentiment word substitution error on the state - of - the - art MSA model through the in - depth analysis ; ( 3 ) We evaluate our model on three real - world datasets , and the experimental results demonstrate that our model outperforms all baselines .", "entities": [[0, 3, "TaskName", "Multimodal sentiment analysis"], [109, 113, "TaskName", "multi - task learning"], [197, 200, "TaskName", "multimodal sentiment analysis"], [209, 210, "DatasetName", "MOSI"], [233, 234, "DatasetName", "iFlytek"], [255, 256, "DatasetName", "MOSI"], [267, 268, "DatasetName", "MOSI"], [271, 272, "DatasetName", "MOSI"], [276, 277, "DatasetName", "MOSI"], [278, 279, "DatasetName", "iFlytek"], [311, 312, "MetricName", "MAE"], [315, 318, "TaskName", "multimodal sentiment analysis"], [497, 498, "DatasetName", "MOSI"], [520, 521, "DatasetName", "MOSI"], [561, 562, "DatasetName", "MOSI"], [666, 668, "TaskName", "word embeddings"], [755, 756, "MethodName", "BERT"], [776, 778, "TaskName", "word embeddings"], [811, 813, "TaskName", "word embeddings"], [840, 842, "TaskName", "word embeddings"], [852, 854, "TaskName", "word embeddings"], [867, 868, "DatasetName", "MOSI"], [871, 872, "DatasetName", "MOSI"], [876, 877, "DatasetName", "MOSI"], [878, 879, "DatasetName", "iFlytek"], [952, 954, "TaskName", "word embeddings"], [983, 986, "TaskName", "multimodal sentiment analysis"]]}
{"text": "We build three real - world datasets including MOSI - SpeechBrain , MOSI - IBM , and MOSI - iFlytek , on CMU - MOSI ( Zadeh et al , 2016 ) . CMU - MOSI CMU multimodal opinion - level sentiment intensity ( CMU - MOSI ) consists of 93 videos collected from the YouTube website . The length of the videos varies from 2 - 5 mins . These videos are split into 2 , 199 short video clips and labeled with sentiment scores from - 3 ( strongly negative ) to 3 ( strongly positive ) . For multimodal features , we extract the visual features using Facet , which can extract the facial action units ( Ekman et al , 1980 ) from each frame . The acoustic features are obtained by applying COVAREP ( Degottex et al , 2014 ) , which includes 12 Mel - frequency cepstral coefficients ( MFCCs ) and other low - level features . However , the provided texts of the utterances in the MOSI dataset are manually transcribed from the corresponding videos by the expert transcribers , which is unrealistic for the real - world applications to obtain the texts in such a way . To evaluate the models in the real world , we replace the manually gold texts in the dataset with the texts output by the ASR models . We adopt a strong ASR model and two widely used commercial APIs to produce the texts . The utilized ASR model released by Ravanelli et al ( 2021 ) is built on the transformer encoder - decoder framework and trained on the Librispeech dataset ( Panayotov et al , 2015 ) . The commercial APIs used by us are IBM 2 and iFlytek 3 speech - to - text APIs , which are wildly used by researchers and software developers . Finally , we apply the three ASR models to transcribe the videos into texts and construct three new datasets , namely MOSI - SpeechBrain , MOSI - IBM , and MOSI - iFlytek . We report the WER results of the adopted ASR models on MOSI in Appendix A. Datasets Models Evaluation Metrics Has0 - Acc \u2191 Has0 - F1 \u2191 Non0 - Acc \u2191 Non0 - F1 \u2191 MAE \u2193 Corr \u2191 MOSI - Noted that , we do not adopt MOSEI , because it does not provide the original video clips for the extracted features and annotated sentiment labels , and we can not process the original audios .", "entities": [[8, 9, "DatasetName", "MOSI"], [12, 13, "DatasetName", "MOSI"], [17, 18, "DatasetName", "MOSI"], [19, 20, "DatasetName", "iFlytek"], [24, 25, "DatasetName", "MOSI"], [35, 36, "DatasetName", "MOSI"], [46, 47, "DatasetName", "MOSI"], [174, 175, "DatasetName", "MOSI"], [275, 276, "DatasetName", "Librispeech"], [295, 296, "DatasetName", "iFlytek"], [335, 336, "DatasetName", "MOSI"], [339, 340, "DatasetName", "MOSI"], [344, 345, "DatasetName", "MOSI"], [346, 347, "DatasetName", "iFlytek"], [359, 360, "DatasetName", "MOSI"], [369, 370, "MetricName", "Acc"], [373, 374, "MetricName", "F1"], [377, 378, "MetricName", "Acc"], [381, 382, "MetricName", "F1"], [383, 384, "MetricName", "MAE"], [387, 388, "DatasetName", "MOSI"]]}
{"text": "We use Adam as the optimizer and the learning rate is 5e - 5 . The batch size is 64 . The sentiment threshold is set to 0.5 while detecting the sentiment word position . The number of the candidate words k is 50 . The other hyper - parameters of the model are reported in Appendix B. All experiments are run on an Nvidia Tesla P100 GPU . We run five times and report the average performance . The random seeds we used are 1111 , 1112 , 1113 , 1114 , and 1115 .", "entities": [[2, 3, "MethodName", "Adam"], [5, 6, "HyperparameterName", "optimizer"], [8, 10, "HyperparameterName", "learning rate"], [16, 18, "HyperparameterName", "batch size"], [81, 82, "DatasetName", "seeds"]]}
{"text": "For the MOSI - SpeechBrain , MOSI - IBM , and MOSI - iFlytek datasets , following previous work ( Yu et al , 2021 ) , we take 2 - class accuracy ( Acc - 2 ) , F1 score ( F1 ) , mean absolute error ( MAE ) , and correlation ( Corr ) as our evaluation metrics . And for Acc - 2 and F1 - Score , we calculate them in two ways , negative / non - negative ( Non0 - Acc , Non0 - F1 ) and negative / positive ( Has0 - Acc , Has0 - F1 ) . As the prediction results are real values , we obtain the sentiment classification labels by mapping the sentiment scores into labels .", "entities": [[2, 3, "DatasetName", "MOSI"], [6, 7, "DatasetName", "MOSI"], [11, 12, "DatasetName", "MOSI"], [13, 14, "DatasetName", "iFlytek"], [32, 33, "MetricName", "accuracy"], [34, 35, "MetricName", "Acc"], [39, 41, "MetricName", "F1 score"], [42, 43, "MetricName", "F1"], [49, 50, "MetricName", "MAE"], [64, 65, "MetricName", "Acc"], [68, 71, "MetricName", "F1 - Score"], [87, 88, "MetricName", "Acc"], [91, 92, "MetricName", "F1"], [100, 101, "MetricName", "Acc"], [104, 105, "MetricName", "F1"]]}
{"text": "In this section , we describe the experiments carried out for the different subtasks and slots and the datasets provided by the organization . These datasets are composed of several reviews , splitted in sentences , for restaurants and laptops topics . The performance of slots 1 and 2 , for both subtasks , are measured by means of the F - score , while slot 3 is evaluated by means of the accuracy . Table 1 represents the precision , recall and Fscore obtained for restaurants datasets and all the slots submitted . For English language , an unconstrained system was presented , while for Spanish language both constrained and unconstrained systems were submitted . The constrained approaches do not need any external resources , but only the training files provided , while in the unconstrained ones , food and drinks lexicon was used in the preprocessing step for identifying different foods and drinks . It can be seen that there is not much difference between constrained and unconstrained systems for Spanish language , so we can assume that the recognition of different names of foods or drinks does not increase the knowledge of the classifiers , perform - ing almost equally . Moreover , we can state that our system perfoms as well for English as for Spanish language . In Table 2 , the detailed scores for slot 3 are shown in English language , for restaurants dataset , likewise in Table 3 As it can be seen in Table 2 and Table 3 , the results obtained for the sentiment slot are not quite competitive with the other teams . This can be due to the fact that our system is fully unsupervised , while the others are usually supervised systems , based on training . Moreover , we performed a simple adaptation from our original system , made for sentiment analysis in Twitter , presented to SemEval 2015 , so there is still a lot of improvement on this field .", "entities": [[73, 74, "MetricName", "accuracy"], [314, 316, "TaskName", "sentiment analysis"]]}
{"text": "Below , we define our DIG formulation that allows interpolations along non - linear paths : DIG i ( x ) = x i x k i = x i \u2202F x k \u2202x i dx k i . ( 1 ) Here , x k i refers to the i th dimension of the k th interpolated point between input x and baseline x and F is a neural network . The only constraint on x k i 's is that each interpolation should be monotonic between x i and x i , i.e. , \u2200j , k { 1 , ... , m } ; j < k , x i \u2264 x j i \u2264 x k i \u2264 x i if x i \u2264 x i , x i \u2265 x j i \u2265 x k i \u2265 x i otherwise . ( 2 ) Here m is the total number of steps for interpolation . This constraint is essential because it allows approximating the integral in Eq . 1 using Riemann summation 2 which requires monotonic paths . We note that the interpolation points used by IG naturally satisfy this constraint since they lie along a straight line joining x and x . The key distinction of our formulation from IG is that DIG is agnostic of any fixed step size parameter \u03b1 and thus allows non - linear interpolation paths in the embedding space . The integral approximation of DIG is defined as follows : DIG approx i ( x ) = \u03a3 m k=1 \u2202F x k \u2202x i \u00d7 x k+1 i \u2212 x k i , ( 3 ) where m is the total number of steps considered for the approximation .", "entities": [[224, 226, "HyperparameterName", "step size"], [227, 228, "HyperparameterName", "\u03b1"]]}
{"text": "In this step , given an anchor word embedding a , we modify the non - monotonic dimensions of a such that they become monotonic w.r.t . w and w . The monotonic dimensions of a vector a is given by : M a = { j | w j \u2264 a j \u2264 w j , j { 1 , ... , D } } \u222a { j | w j \u2265 a j \u2265 w j , j { 1 , ... , D } } , where D is the word embedding dimension . The number of monotonic dimensions is given by the size of the set defined as | M a | . Thus , the non - monotonic dimensions M a is the set complement of the monotonic dimensions , i.e. , M a = { 1 , ... , D } \u2212 M a , where the subtraction is the setdiff operation . Let the final monotonic vector be c. We define the MONOTONIZE operations as follows : c [ M a ] a [ M a ] , c [ M a ] w [ M a ] \u2212 1 m \u00d7 ( w [ M a ] \u2212 w [ M a ] ) , where m is the total number of interpolation points we want to select in the path . It can be easily seen that c is monotonic w.r.t . w and w according to the definition in Equation 2 . ANCHORSEARCH : First , we preprocess the word embedding in V to find the top - K nearest neighbor for each word . We consider this neighborhood for candidate anchor selection . Let us denote the Kneighbors for a word w by KN N V ( w ) . We define two heuristics to search for the next anchor word : GREEDY and MAXCOUNT . In the GREEDY heuristic , we first compute the monotonic embedding corresponding to each word in the neighborhood KN N V ( w ) using the MONO - TONIZE step . Then , we select the anchor word a that is closest to its corresponding monotonic embedding obtained from the above step . This can be thought of as minimizing the WAE metric for a single interpolated word . The key intuition here is to locally optimize for smallest perturbations at each iterative selection step . This heuristic is depicted in Figure 2a and the algorithm is presented in Algorithm 1 in Appendix . In the MAXCOUNT heuristic , we select the anchor a as the word in KN N V ( w ) with the highest number of monotonic dimensions . Precisely , the anchor is given by : a = arg max a KN N V ( w ) | M a | . The intuition of this heuristic is that the vector with highest number of monotonic dimensions would require the minimum number of dimensions being perturbed in the MONOTONIZE step and hence , would be close to a word in the vocabulary . This heuristic is depicted in Figure 2b and the algorithm is presented in Algorithm 2 in Appendix .", "entities": [[93, 96, "HyperparameterName", "word embedding dimension"]]}
{"text": "In this section , we describe the datasets and models used for evaluating our proposed algorithm . ( Wolf et al , 2020b ) . Method DistilBERT RoBERTa BERT LO \u2193 Comp \u2191 Suff \u2193 WAE \u2193 LO \u2193 Comp \u2191 Suff \u2193 WAE \u2193 LO \u2193 Comp \u2191 Suff \u2193 WAE \u2193 Grad * Method DistilBERT RoBERTa BERT LO \u2193 Comp \u2191 Suff \u2193 WAE \u2193 LO \u2193 Comp \u2191 Suff \u2193 WAE \u2193 LO \u2193 Comp \u2191 Suff \u2193 WAE \u2193 Grad * Language Models . We use pre - trained BERT ( Devlin et al , 2019 ) , DistilBERT ( Sanh et al , 2020 ) , and RoBERTa ( Liu et al , 2019 ) text classification models individually fine - tuned for SST2 , IMDB , and RT datasets . 4 The fine - tuned checkpoints used are provided by the HuggingFace library ( Wolf et al , 2020a ) . Evaluation Metrics . Following prior literature , we use the following three automated metrics : Log - odds ( LO ) score ( Shrikumar et al , 2017 ) is defined as the average difference of the negative logarithmic probabilities on the predicted class before and after masking the top k% words with zero padding . Lower scores are better . Comprehensiveness ( Comp ) score ( DeYoung et al , 2020 ) is the average difference of the change in predicted class probability before and after removing the top k% words . Similar to Log - odds , this measures the influence of the top - attributed words on the model 's prediction . Higher scores are better . Sufficiency ( Suff ) score ( DeYoung et al , 2020 ) is defined as the average difference of the change in predicted class probability before and after keeping only the top k% words . This measures the adequacy of the top k% attributions for model 's prediction . Please refer to Appendix C for more details about the evaluation metrics . We use k = 20 % in our experiments . In Appendix D we further analyze the effect of changing top - k% on the metrics . Additionally , we use our proposed word - approximation error ( WAE ) metric to compare DIG with IG . Method DistilBERT RoBERTa BERT LO \u2193 Comp \u2191 Suff \u2193 WAE \u2193 LO \u2193 Comp \u2191 Suff \u2193 WAE \u2193 LO \u2193 Comp \u2191 Suff \u2193 WAE \u2193 Grad *", "entities": [[26, 27, "MethodName", "DistilBERT"], [27, 28, "MethodName", "RoBERTa"], [28, 29, "MethodName", "BERT"], [56, 57, "MethodName", "DistilBERT"], [57, 58, "MethodName", "RoBERTa"], [58, 59, "MethodName", "BERT"], [93, 94, "MethodName", "BERT"], [102, 103, "MethodName", "DistilBERT"], [112, 113, "MethodName", "RoBERTa"], [120, 122, "TaskName", "text classification"], [128, 129, "DatasetName", "SST2"], [130, 131, "DatasetName", "IMDB"], [341, 343, "HyperparameterName", "k ="], [387, 388, "MethodName", "DistilBERT"], [388, 389, "MethodName", "RoBERTa"], [389, 390, "MethodName", "BERT"]]}
{"text": "We compare DIG with four representative gradientbased explanation methods - Gradient*Input ( Grad*Inp ) ( Shrikumar et al , 2016 ) , DeepLIFT ( Shrikumar et al , 2017 ) , GradShap ( Lundberg and Lee , 2017 ) , and integrated gradients ( Sundararajan et al , 2017 ) . For the IMDB and RT datasets , we randomly sample a subset of 2 , 000 reviews from the public test sets to compare the different methods , due to computation costs . For the SST2 dataset , we use the complete set of 1 , 821 test sentences . The results are shown in Tables 1 , 2 , and 3 for SST2 , IMDB , and Rotten Tomatoes respectively . Comparison with baselines . First , we observe that across the nine different settings we studied ( three language models per dataset ) , DIG consistently outperforms the baselines on eight of the settings . This is valid for all the metrics . We also note that the WAE metric is lower for all variants of DIG compared to IG . This validates that our proposed interpolation strategies for DIG is able to considerably reduce the word - approximation error in the interpolated paths and consistently improving performance on all three explanation evaluation metrics considered . Comparison between variants of DIG . Second , we observe that on average , DIG - GREEDY performs better than DIG - MAXCOUNT . Specifically , we find that DIG - MAXCOUNT does n't outperform DIG - GREEDY by significantly large margins on any setting ( while the opposite is true for one setting - RoBERTa fine - tuned on IMDB dataset ) . This could be because the DIG - GREEDY strategy ensures that the monotonic point c is always close to the anchor a due to the locally greedy selection at each step which is not explicitly guaranteed by DIG - MAXCOUNT . But overall , we do not find any Analysis . Finally , though we are able to achieve good reductions in WAE , we note that the WAE for our interpolation algorithms are not close to zero yet . This leaves some scope to design better interpolation algorithms in future . Moreover , we find that the average Pearson correlation between log - odds and WAE is 0.32 and the correlation is 0.45 if we consider the eight settings where we outperform IG . We discuss the correlations of all the settings in Appendix E. While this suggests a weak correlation between the two metrics , it is hard to comment if there is a causality between the two . This is partially because we believe selection of interpolation points should also take the semantics of the perturbed sentences into consideration , which we do n't strongly enforce in our strategies . Hence , we think that constraining interpolations in a semantically meaningful way is a promising direction to explore .", "entities": [[53, 54, "DatasetName", "IMDB"], [86, 87, "DatasetName", "SST2"], [114, 115, "DatasetName", "SST2"], [116, 117, "DatasetName", "IMDB"], [274, 275, "MethodName", "RoBERTa"], [279, 280, "DatasetName", "IMDB"], [382, 384, "MetricName", "Pearson correlation"]]}
{"text": "Integrated gradients ( IG ) ( Sundararajan et al , 2017 ) for an input x along the i th dimension is defined as follows : IG i ( x ) = ( x i \u2212 x i ) \u00d7 1 \u03b1=0 \u2202F ( x + \u03b1\u00d7 ( x\u2212x ) ) \u2202x i d\u03b1 . ( 5 ) Here , F is the neural network , x is a baseline embedding , and \u03b1 is the step size . Simply put , integrated gradients algorithm works by sampling points at a uniform spacing along a straight - line between the input and the baseline , and summing the model 's gradient at the inputs for each interpolated points . To compute this integral efficiently , the authors propose a Riemann summation approximation defined below : IG approx i ( x ) = ( xi \u2212 x i ) \u00d7 \u03a3 m k=1 \u2202F ( x + k m \u00d7 ( x\u2212x ) ) ) \u2202x i \u00d7 1 m , ( 6 ) where m is the total number of steps considered for the approximation . Next , we briefly describe how IG is used to explain a model 's prediction which takes a sentence as input ( for example , the model can be a text classification network ) . Let S = [ w 0 .. w n ] be a sentence of length n and w i be the i th word embedding of the sentence . Also , let F be a text - classification model , i.e. , y = F ( S ) . Then , IG calculates the attribution for each dimension of a word embedding w i . The interpolation points required for Equation 6 are generated by linearly interpolating the word embedding between w i and a baseline word embedding ( usually chosen as the pad embedding ) . Then , using Eq . 6 , the attribution for the i th dimension of w is calculated . The final word attribution is the sum of the attributions for each dimension of the word embedding .", "entities": [[73, 74, "HyperparameterName", "\u03b1"], [76, 78, "HyperparameterName", "step size"], [216, 218, "TaskName", "text classification"], [226, 227, "DatasetName", "0"]]}
{"text": "It is easy to see that the approximation of integrated gradients is a special case of DIG . Note that the k th linear interpolation of the i th dimension of input x for IG can be represented as : x k i = x i + k m \u00d7 ( x i \u2212 x i ) . ( 7 ) Substituting Eq . 7 in Eq . 3 gives us Eq . 6 . Sundararajan et al ( 2017 ) define path methods as the general form of integrated gradients that are applicable for all monotonic paths between the input and the baseline . Our DIG approach is a reformulation of the path method where the paths are not necessarily parameterized by \u03b1 , making it more applicable for discrete data domain . Hence , DIG also satisfies all the theoretical properties applicable for path methods - Implementation Invariance , Sensitivity , Linearity , and Completeness . We refer the readers to Proposition 2 in Sundararajan et al ( 2017 ) for more technical details .", "entities": [[123, 124, "HyperparameterName", "\u03b1"]]}
{"text": "We compute the Pearson correlation between logodds and WAE for each dataset + LM pair . For this , we consider the metric values for IG , DIG - GREEDY , and DIG - MAXCOUNT and report the correlations for each setting in Table 6 . We observe that , there is a strong correlation on average for DistilBERT . For BERT and RoBERTa we find a weak positive and negative correlation respectively .", "entities": [[3, 5, "MetricName", "Pearson correlation"], [58, 59, "MethodName", "DistilBERT"], [61, 62, "MethodName", "BERT"], [63, 64, "MethodName", "RoBERTa"]]}
{"text": "Typically , performance evaluations of automated event coding engines are carried out with respect to benchmarks made of annotated linguistic units ( e.g. clause , sentence or document ) . While this is crucial in order to factorize the individual , linguistic subtasks composing the event extraction process , it does not estimate the overall usability of machinecoded event data sets for micro - level modelling of social processes , particularly in the domain of socio - political and armed conflict , where spatial analysis has become standard . The complex dynamics of the Black Lives Matter movement and its varied media coverage by news outlets and social media make it a particularly relevant use case for assessing the capability of automated , Event Extraction systems to model socio - political processes . The Task 3 : \" Discovering Black Lives Matter Events \" 1 organized in the context of the Challenges and Applications of Automated Extraction of Socio - political Events from Text ( CASE ) 2021 workshop aims at doing so by challenging Event Extraction ( EE ) engines to extract a collection of protest events from two heterogeneous text collections ( i.e. , news and social media ) and then measuring a number of spatiotemporal correlation coefficients against a curated Gold Standard data set of protest incidents from the BLM movement . During May and June of 2020 , protests occurred across the globe in response to the murder of George Floyd , an unarmed Black man , by Derek Chauvin , a white police officer . In the U.S. , the number of locations holding demonstrations related to this murder outnumbered any other demonstration in U.S. history ( Putnam et al , 2020 ) . These events were more often than not associated with the Black Lives Matter ( BLM ) movement , either ( 1 ) directly through organizing or ( 2 ) indirectly through the slogan \" Black Lives Matter \" or shared political agendas such as police abolition and protests against police violence towards Black communi - ties . Since its inception in 2013 , the Black Lives Matter movement , a loose network of affiliated organizations , has organized demonstrations around a large number of police shootings and killings and sought to raise awareness of systematic violence against Black communities . While support for Black Lives Matter has varied over its lifetime ( Horowitz , 2020 ) , the work done over the past years laid the foundation for the global response seen in the wake of George Floyd 's murder . This task is the third in a series of tasks at CASE 2021 workshop ( H\u00fcrriyetoglu et al , 2021b ) . The first task is concerned with protest news detection at multiple text resolutions ( e.g. , the document and sentence level ) and in multiple languages : English , Hindi , Portuguese , and Spanish ( H\u00fcrriyetoglu et al , 2021a ) . Teams which participated in Task 1 were invited to participate in this third task : \" Discovering Black Lives Matter Events in the United States \" . This task is an evaluation only task , where all models are ( 1 ) trained on the data supplied in Task 1 , ( 2 ) applied to the news and social media data ( i.e , New York Times and Twitter data ) , and ( 3 ) evaluated on a manually curated , Gold Standard BLM protest event list . Each team 's system is compared to simple baselines in order to properly evaluate their accuracy .", "entities": [[45, 47, "TaskName", "event extraction"], [123, 125, "TaskName", "Event Extraction"], [175, 177, "TaskName", "Event Extraction"], [600, 601, "MetricName", "accuracy"]]}
{"text": "Summary measures such as precision , recall , and F1 are limited in their capacity to inform about the quality of the predictions of an automated system ( Derczynski , 2016 ; Yacouby and Axman , 2020 ) . Moreover , evaluating capabilities of a system on detecting socio - political events from text requires additional metrics such as spatio - temporal correlation of the system output and the actual distribution of the events ( Wang et al , 2016 ; Althaus et al , 2021 ) . Several studies focused on assessing the correlation of machine - coded event data sets with Gold Standards based on disaggregated event counts , for example Ward et al ( 2013 ) andSchrodt andAnalytics ( 2015 ) . Hammond and Weidmann ( 2014 ) applied disaggregation of events incidents across PRIO - GRID geographical cells ( Tollefsen et al , 2012 ) to assess the Global Database of Events , Language and Tone ( GDELT ) data approximation of the spatio - temporal pattern of conflicts . Zavarella et al ( 2020 ) adapted this method to administrative units for measuring the impact of event de - duplication on increasing correlation with the Armed Conflict Location and Event Data ( ACLED ) data sets for a number of conflicts in Africa . In this report we report on an evaluation task , which we refer as Task 3 , we provide a detailed analysis of the capabilities of the best performing systems on Task 1 ( H\u00fcrriyetoglu et al , 2021a ) in this respect . We believe this effort will shed light on system performances beyond precision , recall , and F1 .", "entities": [[9, 10, "MetricName", "F1"], [28, 29, "DatasetName", "Derczynski"], [279, 280, "MetricName", "F1"]]}
{"text": "The first analysis only considers the total number of \" activated \" cells ( i.e. , for Figure 1 : The geo - referenced BLM protest event records from Gold Standard ( small yellow dots ) overlaid with the PRIO - GRID cells over the US . The larger red and blue dots represent events recognized by the Baseline system from NYT and Twitter , respectively . which at least one Protest event was recorded ) , in the system output and Gold Standard data set . This time series analysis is sufficient to estimate how well the automatic systems capture the time trends of the protest movement . However , it does not compute accuracy of system data in estimating the spatial variation of the target process .", "entities": [[88, 91, "TaskName", "time series analysis"], [115, 116, "MetricName", "accuracy"]]}
{"text": "Table 1 shows the Pearson r , Spearman correlation coefficient \u03c1 , and Root Mean Squared Error ( RMSE ) for the total daily protest cell counts of the Baseline and participant systems , over the 35 days target time range . When a run for both source types exists for a system , we also evaluate the union of the two event sets ( noted as \" Merged \" in Tables ) . Here , the correlations are between the total number of cells per day where the system found an event vs. the number of cells where event happened according to the Gold Standard ( i.e. , temporal patterns and not spatial patterns ) . These correlation measures are tolerant to errors in geocoding ( as far as the events are located in U.S. ) and evaluate the capability of the system to detect protest events in the news and social media , independent of their location . We see the following : ( 1 ) NoConflict surpasses the Baseline with the NYT , Twitter , and Merged data in both Pearson r and Spearman \u03c1 , and ( 2 ) EventMiner and HandShakes surpasses Baseline with Twitter data in Pearson r ( both systems have lower Spearman \u03c1 than Baseline ) . Additionally , NoConflict surpasses the NexusDdpl system ( using NYT , Twitter , and Merged data ) , and the HandShakes system surpasses the NexusDdpl system using Twitter data . Table 2 reports Pearson r , Spearman correlation coefficient \u03c1 , and Root Mean Squared Error ( RMSE ) over cell - day event counts of the Baseline and participant systems with respect to Gold Standard , for the 35 days time range . Here the variables range over the whole set of PRIO - GRID cells included in the US territory and , thus , shows the correlation of event numbers across geo - cells , thus evaluating the system 's geolocation capabilities . NoConflict ( NYT ) had the highest Pearson r and lowest RMSE across all systems , as well as the highest Spearman \u03c1 ( with the Merged data ) . Using Twitter data alone , the Baseline and NexusDdpl systems outperformed all others in terms of Pearson r , however NexusDdpl had a higher Spearman \u03c1 . However , when looking at both correlation metrics simultaneously , no system is above the NexusDdpl baseline . In Figure 2 we plot the time series of total daily protest cells for the best performing instance of each system on New York Times ( left ) and Twitter ( right ) data , respectively . We see the systems evaluated on the NYT data failing to pick up both variation in the temporal patterns ( i.e. , a large number of protests early in late May and early June , which gradually declines with weekly spikes ) and the magnitude of the events ( i.e , most systems pick up less than 100 events per day ) . Systems evaluated on Twitter data pick up more events in late May and early June , but still fail to pick up the magnitude of the events . A more lenient representation of the agreement with Gold Standard is shown in Table 3 . Here we report the confusion matrix between grid cells that Gold Standard and system runs code as experiencing at least a protest event . It can be observed that only few of the cells classified as Protest by Gold Standard are detected by the automatic systems , which on the other hand incorrectly classified as Protest several additional cells .", "entities": [[7, 9, "MetricName", "Spearman correlation"], [16, 17, "MetricName", "Error"], [18, 19, "MetricName", "RMSE"], [251, 253, "MetricName", "Spearman correlation"], [260, 261, "MetricName", "Error"], [262, 263, "MetricName", "RMSE"], [341, 342, "MetricName", "RMSE"], [411, 413, "TaskName", "time series"]]}
{"text": "The goal of the \" Discovering Black Lives Matter Events \" Shared Task was to explore novel performance evaluations of pretrained event detection systems . These systems were applied to large noisy , multi - modal text data sets ( i.e. , news articles and social media data ) related to a specific protest movement , namely , Black Lives Matter . Thus , the systems are being evaluated out - of - domain in terms of both data type ( i.e. , the systems are trained on news data and evaluated on both news and social media ) and protest movement context ( i.e. , the training data are not necessarily related to BLM ) . Systems are evaluated in their ability to identify both events across time as well as events their distribution across space . This evaluation scenario proved difficult for all systems participating in the shared task . A major problem , as shown on ( Hettiarachchi et al , 2021 ) and NoConflict ( Hu and Stoehr , 2021 ) , with precisions of 56.0 and 73.6 , respectively . The low recall at this years shared task may well be due to the low coverage of protest events of the highly diffused BLM movement both in the NYT and Twitter corpus , so the upper bound of the recall may turn out not to be much higher than the system performance . One possible explanation for this is that a significant part of the BLM events in the Gold standard are located in small towns , for which NYT has a limited coverage and also they were not in the focus of social media , due to their small scale . NexusDdpl turned out to be quite high both in terms of event detection accuracy , as well as geo - coding correlation . While no single system outperformed all others in tracking both temporal and spatial trends , NoConflict had a clear advantage ( i.e. , the highest scoring system in 2 out of 3 metrics ) in terms of tracking daily events . 3 : Confusion matrix of grid cells experiencing at least one Protest event ( true ) versus inactive cells ( false ) , for the Gold Standard , Baseline and participant systems . Unless denoted by a superscript , all systems use the \" merged \" version ( i.e. , both NYT and Twitter data sets ) except for HandShakes system which uses only Twitter data .", "entities": [[21, 23, "TaskName", "event detection"], [298, 300, "TaskName", "event detection"], [300, 301, "MetricName", "accuracy"]]}
{"text": "Based on the SCM , this section describes how to resolve the trigger curse via causal intervention . Context Intervention . To block the backdoor path , we intervene on the context C and the new context - intervened SCM is shown in Figure 1 ( b ) . Given support set s , event set e of s , context set C of s and query instance q , we optimize the interventional distribution P ( Y | do ( C = C ) , E = e , Q = q ) rather than P ( Y | S = s , Q = q ) , where do ( ) denotes causal intervention operation . By interven - ing , the learning objective of models changes from optimizing correlation to optimizing causality . Backdoor Adjustment . Backdoor adjustment is used to estimate the interventional distribution 4 : P ( Y | do ( C = C ) , E = e , Q = q ) = t T s S P ( Y | s , q ) P ( s | C , t ) P ( t | e ) , ( 1 ) where P ( s | C , t ) denotes the generation of s from the trigger and contexts . P ( s | C , t ) = 1/ | C | if and only if the context of s in C and the trigger of s is t. P ( Y | s , q ) \u221d \u03c6 ( s , q ; \u03b8 ) is the matching model between q and s parametrized by \u03b8 . Estimating P ( t | e ) via Contextualized Prediction . The confounder distribution P ( t | e ) is unknown because E is a hidden variable . Since the event argument information is contained in C , we argue that P ( t | e ) \u221d M ( t | C ) where M ( | C ) indicates a masked token prediction task ( Taylor , 1953 ) which is constructed by masking triggers in the support set . In this paper , we use masked language model to calculate P ( t | e ) by first generating a set of candidate triggers through the context : T c = { t i | i = 1 , 2 , . . . } \u222a { t 0 } , where t i is the i - th predicted token and t 0 is the original trigger of the support set instance , then P ( t | e ) is estimated by averaging logit obtained from the MLM : P ( ti | e ) = \uf8f4 \uf8f2 \uf8f4 \uf8f3 \u03bb i = 0 ( 1 \u2212 \u03bb ) exp ( li ) j exp ( lj ) i = 0 ( 2 ) where l i is the logit for the i th token . To reduce the noise introduced by MLM , we assign an additional hyperparameter \u03bb ( 0 , 1 ) to t 0 . Optimizing via Representation Learning . Given the interventional distribution , FSED model can be learned by minimizing the loss function on it : L ( \u03b8 ) = \u2212 q Q f ( P ( Y | do ( C ) , e , q ; \u03b8 ) ) = \u2212 q Q f ( t T s S P ( Y | s , q ; \u03b8 ) P ( s | C , t ) P ( t | e ) ) ( 3 ) where Q is training queries and f is a strict monotonically increasing function . However , the optimization of L ( \u03b8 ) needs to calculate every P ( Y | s , q ; \u03b8 ) , which is quite time - consuming . To this end , we propose a surrogate learning criteria L SG ( \u03b8 ) to optimize the causal relation based on representation learning : 4 The proof is shown in Appendix LSG ( \u03b8 ) = \u2212 q Q g ( R ( q ; \u03b8 ) , t T s S P ( s | C , t ) P ( t | e ) R ( s ; \u03b8 ) ) Here R is a representation model which inputs s or q and outputs a dense representation . g ( , ) is a distance metric measuring the similarity between two representations . Such loss function is widely used in many metric - based methods ( e.g. , Prototypical Networks and Relation Networks ) . In the Appendix , we prove L SG ( \u03b8 ) is equivalent to L ( \u03b8 ) . 2 ) FS - ClusterLoss ( Lai et al , 2020 ) , which add two auxiliary loss functions when training . Furthermore , we compare our method with models finetuned with support set ( Finetune ) and pretrained using the training set ( Pretrain ) . BERT base ( uncased ) is used as the encoder for all models and MLM for trigger collection .", "entities": [[265, 266, "HyperparameterName", "\u03b8"], [277, 278, "HyperparameterName", "\u03b8"], [411, 412, "DatasetName", "0"], [426, 427, "DatasetName", "0"], [452, 453, "DatasetName", "MLM"], [468, 469, "DatasetName", "0"], [485, 486, "DatasetName", "0"], [507, 508, "DatasetName", "MLM"], [516, 517, "DatasetName", "0"], [522, 523, "DatasetName", "0"], [526, 528, "TaskName", "Representation Learning"], [542, 543, "MetricName", "loss"], [549, 550, "HyperparameterName", "\u03b8"], [570, 571, "HyperparameterName", "\u03b8"], [591, 592, "HyperparameterName", "\u03b8"], [632, 633, "HyperparameterName", "\u03b8"], [646, 647, "HyperparameterName", "\u03b8"], [669, 670, "HyperparameterName", "\u03b8"], [678, 680, "TaskName", "representation learning"], [690, 691, "HyperparameterName", "\u03b8"], [702, 703, "HyperparameterName", "\u03b8"], [727, 728, "HyperparameterName", "\u03b8"], [753, 755, "HyperparameterName", "distance metric"], [763, 764, "MetricName", "loss"], [793, 794, "HyperparameterName", "\u03b8"], [800, 801, "HyperparameterName", "\u03b8"], [820, 821, "MetricName", "loss"], [850, 851, "MethodName", "BERT"], [864, 865, "DatasetName", "MLM"]]}
{"text": "The performance of our method and all baselines is shown in Table 1 . We can see that : 1 ) By intervening on the context in SCM and using backdoor adjustment during training , our method can effectively learn FSED models . Compared with the original metric - based models , our method achieves 8.7 % and 1.6 % micro - F1 ( average ) improvement in prototypical network and relation network respectively . 2 ) The causal theory is a promising technique for resolving the trigger cruse problem . Notice that FS - LexFree can not achieve the competitive performance with the original FS models , which indicates that trigger information is import and underfitting triggers will hurt the detection performance . This verifies that trigger curse is very challenging and causal intervention can effectively resolve it . 3 ) Our method can achieve state - of - the - art FSED performance . Compared with best score in baselines , our method gains 7.5 % , 1.0 % , and 2.0 % micro - F1 improvements on ACE05 , MAVEN and KBP17 datasets respectively .", "entities": [[60, 63, "MetricName", "micro - F1"], [175, 178, "MetricName", "micro - F1"], [182, 183, "DatasetName", "MAVEN"]]}
{"text": "One - way K - Shot Settings . We adopt One - way K - shot setting in our experiments , in which the support set in an episode contains one event type ( called concerned event ) and the query can contain any event type . The model aims to detect triggers of the concerned event in query and all types will be evaluated by traversing each event type . The support set and query in an episode can be formulated as follows : S = { ( S 1 , E , Y 1 ) , . . . , ( S K , E , Y K ) } where S is the support set , E is the concerned event , S i = { s i 1 , s i 2 , . . . , s i n i } is the i - th sentence in support , s i j is the j - th token in S i , Y i = { y i 1 , y i 2 , . . . , y i n } is the labels of tokens in S i and y i j = 1 only if t i is the trigger ( or part of trigger ) of concerned event , otherwise y i j = 0 . Q = { Q 1 , Q 2 , . . . , Q M } where Q is the set of query and Q i = { q i 1 , q i 2 , . . . , q i m i } is the i - th query sentence and q i j is the j - th token in Q i The model is expected to output the concerned event in Q : O Q = { ( Q 1 , E , T 1 1 ) , . . . , ( Q 1 , E , T 1 n 1 ) , ( Q 2 , E , T 2 1 ) , . . . , ( Q 2 , E , T 2 n 2 ) , . . . , ( Q M , E , T M 1 ) , . . . , ( Q M , E , T M n M ) } where O Q is the set of triggers of concerned event detected in Q , T i k is the k - th trigger of concerned event in sentence Q i and n i \u2265 0 means the number of triggers of concerned event in Q i . Evaluation We improve the traditional episode evaluation setting by evaluating the full test set . For each event type in test set , we randomly sample K instances as support set and all other instances are used as query . Following previous event detection works ( Chen et al , 2015 ) , the predicted trigger is correct if its event type and offsets match those of a gold trigger . We evaluate all methods using macro - F1 and micro - F1 scores , and micro - F1 is taken as the primary measure .", "entities": [[224, 225, "DatasetName", "0"], [427, 428, "DatasetName", "0"], [482, 484, "TaskName", "event detection"], [516, 519, "MetricName", "macro - F1"], [520, 523, "MetricName", "micro - F1"], [526, 529, "MetricName", "micro - F1"]]}
{"text": "We use two metric - base methods in our experiments : Prototypical network ( Snell et al , 2017 ) and Relation network ( Sung et al , 2018 ) , which contain an encoder component and a classifier component . Encoder We use BERT ( Devlin et al , 2019 ) to encoder the support set and the query . Given a sentece X = { x 1 , x 2 , . . . , x n } , BERT encodes the sequence and output the represent of each token in X : R = { r 1 , r 2 , . . . , r n } . After obtaining the feature representation of the support set , we calculate the prototype of the categories ( concerned event and other ) : p i = 1 | R i | r R i r , i = 0 , 1 where p i is the prototype of category i , R i is the set of feature representation of tokens that labeled with y = i in support set . Classifier The models classify each token in query based on its similarity to the prototype . We first calculate the similarity between prototype and token in query . s i , j , k = g ( p k , q i j ) , k = 0 , 1 ( 6 ) where g ( x , y ) measures the similarity between x and y , q i j is the represent of j - th token in i - th query sentence . Then we calculate the probability distribution of token q i j : P ( Y | q i j , S ) = Softmax ( s i , j , 0 , s i , j , 1 ) ( 7 ) During training , we use the Cross - Entropy loss on each token of query . And the support set and the query are randomly sampled from the training set . When evaluating , we treat the labels as IO tagging schemes , and adjacent I are considered to be the same trigger so that we can handle a trigger with multiple tokens . Similarity Functions For prototypical network , the similarity in Equation 6 is Euclidean distance . For relation network , we calculate similarity using neural networks . Unlike the original paper , we find the following calculation to be more efficient : g ( p k , q j i ) = F ( p k q j i | p k \u2212 q j i | ) where means concatenation vectors and F is two - layer feed - forward neural networks with a ReLU function on the first layer .", "entities": [[44, 45, "MethodName", "BERT"], [81, 82, "MethodName", "BERT"], [151, 152, "DatasetName", "0"], [217, 219, "HyperparameterName", "k ="], [229, 231, "HyperparameterName", "k ="], [231, 232, "DatasetName", "0"], [293, 294, "MethodName", "Softmax"], [300, 301, "DatasetName", "0"], [321, 322, "MetricName", "loss"], [460, 461, "MethodName", "ReLU"]]}
{"text": "Different reviews in ( Arevalo et al , 2017 ; Poria et al , 2016Poria et al , , 2017bGhosal et al , 2018 ; Morency et al , 2011a ; Zadeh et al , 2018a ; Mihalcea , 2012 ; Lee et al , 2018 ; Tsai et al , 2018 ) suggest that multi - modal sentiment and emotion analysis are relatively new areas as compared to uni - modal analysis . Feature selection ( fusion ) is a challenging and important task for any multi - modal analysis . Poria et al ( 2016 ) proposed a multi - kernel learning based feature selection method for multimodal sentiment and emotion recognition . A convolutional deep belief network ( CDBN ) is proposed in ( Ranganathan et al , 2016 ) to learn salient multi - modal features of low - intensity expressions of emotions , whereas Lee et al ( 2018 ) introduced a convolutional attention network to learn multimodal feature representation between speech and text data for multi - modal emotion recognition . A feature level fusion vector was built , and then a Support Vector Machine ( SVM ) classifier was used to detect the emotional duality and mixed emotional experience in ( Patwardhan , 2017 ) . Similar work on feature - level fusion based on self - attention mechanism is reported in ( Hazarika et al , 2018 ) . Fu et al ( 2017 ) introduced an enhanced sparse local discriminative canoni - cal correlation analysis approach ( En - SLDCCA ) to learn the multi - modal shared feature representation . Tzirakis et al ( 2017 ) introduced a Long Short Term Memory ( LSTM ) based end - to - end multi - modal emotion recognition system in which convolutional neural network ( CNN ) and a deep residual network are used to capture the emotional content for various styles of speaking , robust features . Poria et al ( 2017a ) presented a literature survey on various affect dimensions e.g. , sentiment analysis , emotion analysis , etc . , for the multi - modal analysis . A multi - modal fusion - based approach is proposed in ( Blanchard et al , 2018 ) for sentiment classification . The author used exclusively high - level fusion of visual and acoustic features to classify the sentiment . Zadeh et al ( 2016 ) presented the multi - modal dictionary - based technique to capture the interaction between spoken words and facial expression better when expressing the sentiment . In another work , proposed a Tensor Fusion Network ( TFN ) to capture the inter - modality and intra - modality dynamics between the multi - modalities ( i.e. , text , visual , and acoustic ) . These works did not take contextual information into account . Poria et al ( 2017b ) introduced an Long Short Term Memory ( LSTM ) based framework for sentiment classification which uses contextual information to capture interrelationships between the utterances . In another work , Poria et al ( 2017c ) proposed a user opinion based framework to combine all the multi - modal inputs ( i.e. , visual , acoustic , and textual ) by applying a multi - kernel learning - based approach . Contextual inter - modal attention mechanism was not explored in much details until recently . Zadeh et al ( 2018a ) introduced a multi - attention blocks based model for multi - modal sentiment classification but did not account for contextual information , whereas Ghosal et al ( 2018 ) proposed a contextual inter - modal attention based framework for multi - modal sentiment classification . Recently , Zadeh et al ( 2018c ) introduced the largest multimodal dataset namely CMU - MOSEI for sentiment and emotion analysis . Author effectively fused the multi - modality inputs i.e. , text , visual , and acoustic through a dynamic fusion graph and reported competitive performance w.r.t . various state - ofthe - art systems for both sentiment and emotion analysis . Very recently , Akhtar et al ( 2019 ) in - troduced an attention based multi - task learning framework for sentiment and emotion classification on the CMU - MOSEI dataset . In comparison to the existing systems , our proposed approach aims to exploits the interaction between the input modalities through an autoencoder based inter - modal interactive module . The interactive module learns the joint representation for the participating modalities , which are further utilized to capture the contributing contextual utterances in a context - aware attention module . 3 Context - aware Interactive Attention ( CIA ) Affect Analysis In this section , we describe our proposed approach for the effective fusion of multi - modal input sources . We propose an end - to - end Contextaware Interactive Attention ( CIA ) based recurrent neural network for sentiment and emotion analysis . As discussed earlier , one of the main challenges for multi - modal information analysis is to exploit the interaction among the input modalities . Therefore , we introduce an Inter - modal Interactive Module ( IIM ) that aims to learn the interaction between any two modalities through an auto - encoder like structure . For the text - acoustic pair of modalities , we aim to decode the acoustic representation through the encoded textual representation . After training of IIM , we extract the encoded representation for further processing . We argue that the encoded representation learns the interaction between the text and acoustic modalities . Similarly , we compute the interaction among all the other pairs ( i.e. , acoustic - text , text - visual , visualtext , acoustic - visual , and visual - acoustic ) . Next , we extract the sequential pattern of the utterances through a Bi - directional Gated Recurrent Unit ( Bi - GRU ) ( Cho et al , 2014 ) ) . For each pair of modalities , the two representations denoting the interactions between them are combined through a mean operation . For an instance , we compute the mean of the text - acoustic and acoustic - text representations for text and acoustic modalities . The mean operation ensures that the network utilizes the two distinct representations by keeping the minimal dimension . In our network , we , additionally , learn the interaction among the modalities through a feedforward network . At first , all the three modalities are passed through a separate Bi - GRU . Then , pair - Algorithm 1 Inter - modal Interactive Module for Multi - modal Sentiment and Emotion Recognition ( IIM - MMSE ) procedure IIM - MMSE ( t , v , a ) for i 1 , ... , K do K = # modalities for j 1 , ... , K do \u2200x , y [ T , V , A ] , x = y and i \u2264 j C x i y j IIM ( x i , y j ) C x i y j biGRU ( C x i y j ) C x i biGRU ( x i ) for i , j 1 , ... , K do \u2200x , y [ T , V , A ] , and x = y M x i , y j M ean ( C x i y j , C y i x j ) cat x i , y j Concatenate ( C x i , C y j ) BI x i , y j F ullyConnected ( cat x i , y j ) A x i , y j CAM ( M x i , y j , BI x i , y j ) Rep [ A T V , A T A , A AV ] polarity Sent ( Rep ) /Emo ( Rep ) return polarity Algorithm 2 Inter - Modal Interactive Module ( IIM ) procedure IIM ( X , Y ) C XY IIM Encoder ( X , Y ) Y IIM Decoder ( C XY ) loss cross entropy ( Y , Y ) Backpropagation to update the weights return C XY Algorithm 3 Context - aware Attention Module ( CAM ) procedure CAM ( M , BI ) P M.BI T Cross product for i , j 1 , ... , u do u = # utterances N ( i , j ) e P ( i , j ) u k=1 e P ( i , k ) O N.BI A O M Multiplicative gating . return A wise concatenation is performed over the output of Bi - GRU and passed through a fully - connected layer to extract the bi - modal interaction ( BI ) . Further , we employ a Context - aware Attention Module ( CAM ) to exploit the correspondence among the neighboring utterances . The inputs to the CAM are the two representations for each pair of modalities , e.g. , mean representation M T A and bi - modal interaction BI T A for the text - acoustic pair . The attention module assists the network in attending the contributing features by putting weights to the current and the neighboring utterances in a video . In the end , the pair - wise ( i.e. , text - acoustic , text - visual , and acoustic - visual ) attended representations are concatenated and fed to an output layer for the prediction . We depict and summarize the proposed approach in Figure 1 and Algorithm 1 , 2 , and 3 . The source code is available at http://www.iitp . ac.in/\u02dcai - nlp - ml / resources.html .", "entities": [[60, 61, "DatasetName", "emotion"], [74, 76, "MethodName", "Feature selection"], [105, 107, "MethodName", "feature selection"], [112, 114, "TaskName", "emotion recognition"], [117, 120, "MethodName", "deep belief network"], [174, 176, "TaskName", "emotion recognition"], [188, 191, "MethodName", "Support Vector Machine"], [192, 193, "MethodName", "SVM"], [283, 284, "MethodName", "LSTM"], [294, 296, "TaskName", "emotion recognition"], [308, 310, "MethodName", "residual network"], [342, 344, "TaskName", "sentiment analysis"], [345, 346, "DatasetName", "emotion"], [491, 492, "MethodName", "LSTM"], [634, 637, "DatasetName", "CMU - MOSEI"], [640, 641, "DatasetName", "emotion"], [681, 682, "DatasetName", "emotion"], [699, 703, "TaskName", "multi - task learning"], [707, 709, "TaskName", "emotion classification"], [711, 714, "DatasetName", "CMU - MOSEI"], [737, 738, "MethodName", "autoencoder"], [827, 828, "DatasetName", "emotion"], [987, 990, "MethodName", "Gated Recurrent Unit"], [993, 994, "MethodName", "GRU"], [1083, 1085, "MethodName", "feedforward network"], [1100, 1101, "MethodName", "GRU"], [1119, 1121, "TaskName", "Emotion Recognition"], [1145, 1147, "HyperparameterName", "K ="], [1193, 1194, "MethodName", "biGRU"], [1204, 1205, "MethodName", "biGRU"], [1293, 1294, "MethodName", "CAM"], [1366, 1367, "MetricName", "loss"], [1390, 1391, "MethodName", "CAM"], [1393, 1394, "MethodName", "CAM"], [1460, 1461, "MethodName", "GRU"], [1491, 1492, "MethodName", "CAM"], [1506, 1507, "MethodName", "CAM"]]}
{"text": "For the evaluation of our proposed approach , we employ five multi - modal benchmark datasets 1 covering two affect analysis tasks , i.e. , sentiment and emotion . Table 1 : Results of sentiment and emotion analysis for the proposed approach . T : Text , V : Visual , A : Acoustic . Weighted accuracy as a metric is chosen due to unbalanced samples across various emotions and it is also in line with the other existing works ( Zadeh et al , 2018c", "entities": [[27, 28, "DatasetName", "emotion"], [36, 37, "DatasetName", "emotion"], [56, 57, "MetricName", "accuracy"]]}
{"text": "We implement our proposed model on the Pythonbased Keras deep learning library . As the evaluation metric , we employ accuracy ( weighted accu - racy ( Tong et al , 2017 ) ) and F1 - score for the classification problems , while for the intensity prediction task , we compute Pearson correlation scores and mean - absolute - error ( MAE ) . We evaluate our proposed CIA model on five benchmark datasets i.e. , MOUD , MOSI , YouTube , ICT - MMMO , and MOSEI . For all the datasets , we perform grid search to find the optimal hyperparameters ( c.f . Table 4 ) . Though we push for a generic hyper - parameter configuration for all datasets , in some cases , a different choice of the parameter has a significant effect . Therefore , we choose different parameters for different datasets for our experiments . Details of hyper - parameters for different datasets are depicted in Table 4 . We use different activation functions for the various modules in our model . We use tanh as the activation function for the inter - modal interactive module ( IIM ) , while we employ ReLu for the context - aware attention module . For each dataset , we use Adam as optimizer . In this paper , we address three multi - modal affective analysis problems , namely i.e. , sentiment classification ( S C ) , sentiment intensity ( S I ) and emotion classification ( E C ) . We use softmax as a classifier for sentiment classification , while optimizing the categorical cross - entropy as a loss function . In comparison , we use sigmoid for prediction and binary cross - entropy as the loss function for the emotion classification . As the emotions in the dataset are multi - labeled , we apply a threshold over the predicted sigmoid outputs for each emotion and consider all the emotions as present whose respective values are above the threshold . We cross - validate and optimize both We evaluate our proposed approach for all the possible input combinations i.e. , uni - modal ( T , A , V ) , bi - modal ( T+V , T+A , A+V ) and tri - modal ( T+V+A ) . We depict our obtained results in Table 1 . For MOSEI dataset , with tri - modal inputs , our proposed system reports 79.02 % F1 - score and 62.97 % weighted - accuracy for emotion classification . For sentiment classification , we obtain 78.23 % , 80.37 % , 49.15 % and 50.14 % as F1 - score for two - class , five - class and seven - class , respectively . For sentiment intensity prediction task , our proposed system yields MAE and Pearson score of 0.683 and 0.594 , respectively . We also observe that the proposed approach yields better performance for the tri - modal inputs than the bi - modal and uni - modal input combinations . This improvement implies that our proposed CIA architecture utilizes the interaction among the input modalities very effectively . Furthermore , for the other datasets , i.e. , MOSI , ICT - MMMO , YouTube , and MOUD , we also observe a similar phenomenon as well ( c.f . Table 1 ) . To show that our proposed IIM module , indeed , learns the interaction among the distinct modalities , we also perform an ablation study of the proposed CIA architecture . Consequently , we omit the IIM module from our architecture and compute the self - attention on the pair - wise fully - connected representations for the prediction . We observe that , for all the datasets , the performance of this modified architecture ( i.e. , CIA - IIM ) is constantly inferior ( with 1 % to 7 % F - score points ) to the proposed CIA architecture . This performance degradation suggests that the IIM module is , indeed , an important component of our proposed architecture . In Table 3 , we depict the evaluation results for both - with and without IIM .", "entities": [[20, 21, "MetricName", "accuracy"], [35, 38, "MetricName", "F1 - score"], [52, 54, "MetricName", "Pearson correlation"], [62, 63, "MetricName", "MAE"], [79, 80, "DatasetName", "MOSI"], [185, 187, "HyperparameterName", "activation function"], [201, 202, "MethodName", "ReLu"], [216, 217, "MethodName", "Adam"], [218, 219, "HyperparameterName", "optimizer"], [251, 253, "TaskName", "emotion classification"], [260, 261, "MethodName", "softmax"], [277, 278, "MetricName", "loss"], [295, 296, "MetricName", "loss"], [299, 301, "TaskName", "emotion classification"], [324, 325, "DatasetName", "emotion"], [414, 417, "MetricName", "F1 - score"], [422, 423, "MetricName", "accuracy"], [424, 426, "TaskName", "emotion classification"], [445, 448, "MetricName", "F1 - score"], [473, 474, "MetricName", "MAE"], [539, 540, "DatasetName", "MOSI"]]}
{"text": "In this section , we present our comparative studies against several existing and recent state - ofthe - art systems . For each dataset , we report three best systems for the comparisons 2 . In particular , we compare with the following systems : Bag of Feature - Multimodal Sentiment Analysis ( BoF - MSA ) ( Blanchard et al , 2018 ) , Memory Fusion Network ( MFN ) ( Zadeh et al , 2018b ) ( Zadeh et al , 2018c ) , Tensor Fusion Network ( TFN ) , Random Forest ( RF ) ( Breiman , 2001 ) , Support Vector Machine ( Zadeh et al , 2016 ) , Multi - Attention Recurrent Network ( MARN ) ( Zadeh et al , 2018a ) , Dynamic Fusion Graph ( DFG ) ( Zadeh et al , 2018c ) , Multi Modal Multi Utterance - Bimodal Attention ( MMMU - BA ) ( Ghosal et al , 2018 ) , Bi - directional Contextual LSTM ( BC - LSTM ) ( Poria et al , 2017b ) and Multimodal Factorization Model ( MFM ) ( Tsai et al , 2018 ) . We show the comparative results in Table 5a and Table 5b for emotion and sentiment analysis , respectively . We observe that the proposed CIA framework yields better performance against the state - of - the - art for all the cases . For emotion classification , our proposed approach achieves approximately 3 and 0.6 percentage higher F1 - ( Zadeh et al , 2018c ) . \u2020 Values are taken from ( Tsai et al , 2018 ) . Significance T - test ( < 0.05 ) signifies that the obtained results are statistically significant over the existing systems with 95 % confidence score . score and weighted accuracy , respectively , than the state - of - the - art DFG ( Zadeh et al , 2018c ) system . Furthermore , we also see improvements for most of the individual emotion classes as well . In sentiment analysis ( c.f . Table 5b ) , for all the five datasets and different experimental setups , the proposed CIA framework obtains the improved accuracies for the classification tasks . For intensity prediction , our proposed framework yields lesser mean - absolute - error with high Pearson correlation scores . On average , we observe 1 to 5 % improvement in accuracy values in comparison to the next best systems . Similarly , for the intensity prediction task , we report approximately 0.03 and 0.04 points improvement in mean - absolute - error and Pearson score , respectively . We perform statistical significance test ( paired T - test ) on the obtained results and observe that performance improvement in the proposed model over the state - of - the - art is significant with 95 % confidence ( i.e. , p - value < 0.05 ) .", "entities": [[49, 52, "TaskName", "Multimodal Sentiment Analysis"], [104, 107, "MethodName", "Support Vector Machine"], [155, 156, "DatasetName", "BA"], [169, 170, "MethodName", "LSTM"], [173, 174, "MethodName", "LSTM"], [209, 210, "DatasetName", "emotion"], [211, 213, "TaskName", "sentiment analysis"], [241, 243, "TaskName", "emotion classification"], [254, 255, "MetricName", "F1"], [306, 307, "MetricName", "accuracy"], [340, 341, "DatasetName", "emotion"], [346, 348, "TaskName", "sentiment analysis"], [394, 396, "MetricName", "Pearson correlation"], [409, 410, "MetricName", "accuracy"]]}
{"text": "Inspired by Gui et al ( 2020 ) , we successively introduce two classifiers - initial classifier and iterative classifier to construct IRSE - Graph . Both classifiers are constructed using slightly adapted GRN and utilized to deal with different scenarios , respectively . In this way , we can fully exploit the potential of iterative classifier to predict better pairwise orderings . We will give a detail introduction to the slightly adapted GRN in Section 4.3 . To better understand the procedure of constructing IRSE - Graph , we provide the details in Algorithm 1 . During this procedure , pairwise orderings are iteratively predicted and gradually incorporated to refine IRSE - Graph . Here we introduce a set VP ( k ) to collect sentence node pairs with uncertain pairwise orderings at the k - th iteration . First , we bulid an initial classifier based on the initial IRSE - Graph , where the learned sentence representations are used to predict pairwise orderings between any two linked sentences only once ( Lines 2 - 6 ) . Note that in the initial IRSE - Graph , all weights of ss - edges are set to 0.5 . In this case , IRSE - Graph degrades to the conventional SE - Graph . Concretely , for any two linked sentence nodes v i and v i , we concatenate their vector representations \u03ba i and \u03ba i as [ \u03ba i ; \u03ba i ] and [ \u03ba i ; \u03ba i ] , which are fed into an MLP classifier to obtain two probabilities . Then , we normalize and convert these two probabilities into ss - edge weights w i , i and w i , i . If both w i , i and w i , i are within a prefixed interval [ \u03b4 min , \u03b4 max ] , we consider ( v i , v i ) as a sentence node pair with uncertain pairwise ordering and add it into VP ( 0 ) . Moreover , we replace both w i , i and w i , i with 0.5 , indicating that they will be repredicted in the next iteration . In the following , we also construct an iterative classifier based on IRSE - Graph . However , in an easy - to - hard manner , we use iterative classifier to perform pairwise ordering predictions , where the ss - edge weights of IRSE - Graph are continously updated with previously - predicted pairwise orderings with high confidence ( Lines 13 - 26 ) . By doing so , graph representations can be continously refined for better subsequent predictions . More specifically , the k - th iteration of this classifier mainly involve three steps : In Step 1 , based on the current IRSE - Graph , we employ the adapted GRN to conduct graph encoding to learn sentence representations ( Line 15 ) . In Step 2 , on the top of learned sentence representations , we stack an MLP classifier to predict pairwise orderings for sentence node pairs in VP ( k ) ( Lines 16 - 19 ) . Likewise , we collect sentence node pairs with uncertain pairwise orderings to form VP ( k+1 ) , and reassign their corresponding ss - edge weights as 0.5 , so as to avoid the negative effect of these uncertain ss - edge weights during the next Algorithm 1 The procedure of constructing IRSE - Graph Input : the initial IRSE - Graph : G= ( V , E , W ) with all w i , i = 0 ; two thresholds : \u03b4min , \u03b4max Output : the final IRSE - Graph : G = ( V , E , W ) 1 : VP ( 0 ) 2 : { \u03bai } I i=1 GRN ( G ) 3 : for any linked sentence node pair ( vi , v i ) & & i < i do 4 : w i , i InitialClassifer ( [ \u03bai ; \u03ba i ] ) 5 : w i , i InitialClassifer ( [ \u03ba i ; \u03bai ] ) 6 : w i , i , w i , i Normalize ( w i , i , w i , i ) 7 : if \u03b4min \u2264 w i , i \u2264 \u03b4max then 8 : VP ( 0 ) VP ( 0 ) \u222a { ( vi , v i ) } 9 : w i , i 0.5 , w i , i 0.5 10 : end if 11 : end for 12 : k 0 13 : repeat 14 : iteration ( Lines 20 - 23 ) . VP ( k+1 ) 15 : { \u03bai } I i=1 GRN ( G ) 16 : for ( vi , v i ) VP ( k ) do 17 : w i , i IterativeClassifer ( [ \u03bai ; \u03ba i ] ) 18 : w i , i IterativeClassifer ( [ \u03ba i ; \u03bai ] ) 19 : w i , i , w i , i Normalize ( w i , i , w i , i ) 20 : if \u03b4min \u2264 w i , i \u2264 \u03b4max then 21 : VP ( k+1 ) VP ( k+1 ) \u222a { ( vi , v i ) } 22 : w i , i 0.5 , w i , i 0.5 23 : end if 24 : end for 25 : k k + 1 26 : until VP ( k+1 ) = = VP ( k ) | | VP ( k ) = = 27 : return G In Step 3 , if VP ( k+1 ) is equal to VP ( k ) or empty , we believe the learning of IRSE - Graph G has converged and thus return it ( Lines 26 - 27 ) . Although both of our classifiers are constructed using IRSE - Graph , their training procedures are slightly different . As for initial classifier , we directly train it on the initial IRSE - Graph without any pairwise ordering information ( all ss - edge weights are set to 0.5 ) . By contrast , we train iterative classifier on IRSE - Graph with partial pairwise orderings . To enable iterative classifier generalizable to any IRSE - Graph with partial predicted pairwise orderings , we first set al ss - edge weights to 1 or 0 according to their ground - truth pairwise orderings , and then train the classifier to correctly predict pari - wise orderings for other pairs . Concretely , if s i appears before s i , we set w i , i = 1 and w i , i = 0 , vice versa . For example , in the left part of Figure 3 , the ground - truth sentence sequence is s 1 , s 2 , s 3 , s 4 , and thus we assign the ss - edge weights of linked sentence node pairs ( v 1 , v 2 ) , ( v 3 , v 2 ) , ( v 3 , v 4 ) , ( v 2 , v 4 ) as follows : w 1 , 2 = 1 , w 2 , 3 = 1 , w 2 , 4 = 1 , w 3 , 4 = 1 , and w 2 , 1 = 0 , w 3 , 2 = 0 , w 4 , 2 = 0 , w 4 , 3 = 0 . Moreover , to enhance the robustness of the iterative classifier , we randomly select a certain ratio \u03b7 of sentence pairs and assign their ss - edges with incorrect weights . Let us revisit Figure 3 , for the randomly selected sentence node pair ( v 1 , v 2 ) , we assign ss - edges weights w 1 , 2 and w 2 , 1 with randomly generated noisy values 0.3 and 0.7 respectively . In this way , we expect that iterative classifier can conduct correct predictions even given incorrect previously - predicted pairwise orderings .", "entities": [[0, 1, "DatasetName", "Inspired"], [261, 262, "DatasetName", "MLP"], [309, 310, "HyperparameterName", "\u03b4"], [312, 313, "HyperparameterName", "\u03b4"], [340, 341, "DatasetName", "0"], [513, 514, "DatasetName", "MLP"], [613, 614, "DatasetName", "0"], [642, 643, "DatasetName", "0"], [744, 745, "DatasetName", "0"], [748, 749, "DatasetName", "0"], [783, 784, "DatasetName", "0"], [1097, 1098, "DatasetName", "0"], [1147, 1148, "DatasetName", "0"], [1264, 1265, "DatasetName", "0"], [1271, 1272, "DatasetName", "0"], [1278, 1279, "DatasetName", "0"], [1285, 1286, "DatasetName", "0"]]}
{"text": "Datasets . Following previous work ( Yin et al , 2020 ; Cui et al , 2018 ; Yin et al , 2021 ) , we carry out experiments on five benchmark datasets : SIND , ROCStory . SIND is a visual storytelling dataset and ROCStory ( Mostafazadeh et al , 2016 ) is about commonsense stories . Both two datasets are composed of 5 - sentence stories and randomly split by 8:1:1 for the training / validation / test sets . NIPS Abstract , AAN Abstract , arXiv Abstract . These three datasets consist of abstracts from research papers , which are collected from NIPS , ACL anthology and arXiv , respectively ( Radev et al , 2016 ; ( Yin et al , 2021 ) for our model and its variants . Specifically , we apply 100 - dimensional GloVe word embeddings , and set the sizes of Bi - LSTM hidden states , sentence node states , and entity node states as 512 , 512 and 150 , respectively . The recurrent step of GRN is 3 . We empirically set thresholds \u03b4 min and \u03b4 max as 0.2 and 0.8 , and set \u03b7 as 20 % , 15 % , 25 % , 15 % , 15 % according to accuracies of initial classifier on validation sets . Besides , we individually set the coefficient \u03bb ( See Equation 18 in ( Yin et al , 2020 ) ) as 0.5 , 0.5 , 0.2 , 0.4 , 0.5 on the five datasets . We adopt Adadelta ( Zeiler , 2012 ) with = 10 \u22126 , \u03c1 = 0.95 and initial learning rate 1.0 as the optimizer . We employ L2 weight decay with coefficient 10 \u22125 , batch size of 16 and dropout rate of 0.5 . When constructing our model based on BERT , we use the same settings as ( Cui et al , 2020 ) . Concretely , we set sizes of hidden states and node states to 768 , the learning rate of BERT as 3e - 3 , the batch size as 16 , 32 , 128 , 128 , 64 for the five datasets . Baselines . To demonstrate the effectiveness of our model ( IRSE - GRN ) , we compare it with SE - GRN ( Yin et al , 2021 ) . Besides , we report the performance of following sentence ordering models : 1 ) Pairwise models : Pairwise Model , RankTxNet ( Kumar et al , 2020 ) , andB - TSort ( Prabhumoye et al , 2020 ) , ConsGraph ( Zhu et al , 2021 ) ; 2 ) Set - to - sequence models : HAN ( Wang and Wan , 2019 ) , LSTM+PtrNet ( Gong et al , 2016 ) , V - LSTM+PtrNet ( Logeswaran et al , 2018 ) , ATTOrderNet ( Cui et al , 2018 ) , TGCM ( Oh et al , 2019 ) , SE - GRN ( Yin et al , 2019 ) , SE - GRN ( Yin et al , 2021 ) 2020 ) and BERSON ( Cui et al , 2020 ) . Furthermore , to examine the compatibility of other technologies with our model , we report the performance of IRSE - GRN equipped with some effective components : 1 ) IRSE - GRN+FHDecoder . In this variant , we equip our model with FHDecoder ( Yin et al , 2020 ) , where pairwise ordering information is incorporated ; 2 ) IRSE - GRN+BERT+FHDecoder . In addition to FHDecoder , we construct the sentence encoder based on BERT , where the mean - pooling outputs of all learned word representations are used to initialize sentence nodes . Evaluation Metrics . Following previous work ( Oh et al , 2019 ; Cui et al , 2020 ; Prabhumoye et al , 2020 ; Zhu et al , 2021 ; Yin et al , 2021 ) , we use the following three metrics : 1 ) Kendall 's Tau ( \u03c4 ) : Formally , this metric is calculated as 1 - 2\u00d7 ( number of inversions ) / I 2 , where I denotes the sequence length and number of inversions is the number of pairs in the predicted sequence with incorrect relative order ( Lapata , 2003 ) ; 2 ) Perfect Match Ratio ( PMR ) : This metric calculates the ratio of samples where the entire sequence is correctly predicted ; 3 ) Accuracy ( Acc ) : This metric measures the percentage of sentences , whose absolute positions are correctly predicted ( Logeswaran et al , 2018 ) .", "entities": [[34, 35, "DatasetName", "SIND"], [38, 39, "DatasetName", "SIND"], [41, 43, "TaskName", "visual storytelling"], [88, 89, "DatasetName", "arXiv"], [110, 111, "DatasetName", "arXiv"], [141, 142, "MethodName", "GloVe"], [142, 144, "TaskName", "word embeddings"], [152, 153, "MethodName", "LSTM"], [185, 186, "HyperparameterName", "\u03b4"], [188, 189, "HyperparameterName", "\u03b4"], [261, 262, "MethodName", "Adadelta"], [277, 279, "HyperparameterName", "learning rate"], [282, 283, "HyperparameterName", "optimizer"], [287, 289, "MethodName", "weight decay"], [294, 296, "HyperparameterName", "batch size"], [310, 311, "MethodName", "BERT"], [341, 343, "HyperparameterName", "learning rate"], [344, 345, "MethodName", "BERT"], [351, 353, "HyperparameterName", "batch size"], [406, 408, "TaskName", "sentence ordering"], [420, 421, "DatasetName", "Kumar"], [612, 613, "MethodName", "BERT"], [760, 761, "MetricName", "Accuracy"], [762, 763, "MetricName", "Acc"]]}
{"text": "Modern e - commerce catalogs contain millions of references , associated with textual and visual information that is of paramount importance for the products to be found via search or browsing . Of particular significance is the book category , where the author name ( s ) field poses a significant challenge . Indeed , books written by a given author might be listed with different authors ' names due to abbreviations , spelling variants and mistakes , among others . To solve this problem at scale , we design a composite system involving open data sources for books , as well as deep learning components , such as approximate match with Siamese networks and name correction with sequence - tosequence networks . We evaluate this approach on product data from the e - commerce website Rakuten France , and find that the top proposal of the system is the normalized author name with 72 % accuracy .", "entities": [[156, 157, "MetricName", "accuracy"]]}
{"text": "We want to learn a mapping that assigns a similarity score to a pair of author names such that name variants of the same entity will have high similarity , and names that belong to different entities will have low similarity . Once learned , this mapping will enable us to assign an entity to any given name . To this end , we might use a classical string metric such as the Levenshtein distance or the n - gram distance ( Kondrak , 2005 ) . However , those are not specific to people 's names , and might return a large distance ( low similarity ) in cases such as the inversion between first name and last name or the abbreviation of the first name to an initial . Thus , we want to use the dataset of name entities to learn a specialized notion of similarity - this is known as distance metric learning ( Kulis et al , 2013 ) . To this purpose , we use a pair of neural networks with shared weights , or Siamese neural network ( Bromley et al , 1994 ) . Each network is a recurrent neural network ( RNN ) composed of a character - level embedding layer with 256 units , a bidirectional long shortterm memory ( LSTM ) ( Hochreiter and Schmidhuber , 1997 ) with 2 \u00d7 128 units , and a dense layer with 256 units . Each network takes a name as input and outputs a representation - the two representations are then compared using cosine similarity with a target value equal to 1 for name variants of the same entity , and to 0 otherwise . We preprocess the input by representing all characters in ASCII and lowercase . We consider a sequence length of 32 using zero padding . The Siamese network is trained with contrastive loss ( Hadsell et al , 2006 ) in order to push the similarity towards 1 for similar pairs , and below a certain margin ( that we set to 0 ) for dissimilar pairs . The optimization is done using Adam ( Kingma and Ba , 2014 ) , with a learning rate of 10 \u22123 and a gradient clipping value of 5 . We use batches of 512 samples , consider a negative to positive pairs ratio of 4 : 1 , and randomly generate new negative pairs at every epoch . At test time , we search for the canonical name whose representation is closest to that of the query , using only the high - quality name entities from DBpedia , BnF , and JRC - names . To this end , we do approximate nearest neighbor search using Annoy 7 .", "entities": [[154, 156, "HyperparameterName", "distance metric"], [220, 221, "MethodName", "LSTM"], [281, 282, "DatasetName", "0"], [309, 311, "MethodName", "Siamese network"], [315, 316, "MetricName", "loss"], [345, 346, "DatasetName", "0"], [356, 357, "MethodName", "Adam"], [367, 369, "HyperparameterName", "learning rate"], [374, 376, "MethodName", "gradient clipping"], [438, 439, "DatasetName", "DBpedia"]]}
{"text": "We use a generative model to correct and normalize authors ' names directly . The dataset of name entities is again employed to train a sequence - to - sequence ( seq2seq ) model ( Sutskever et al , 2014 ) to produce the canonical form of a name from one of its variants . The dataset is further augmented by including additional variants where the first name is abbreviated to an initial . The seq2seq model is an encoder - decoder using RNNs , with a character embedding layer , as in the case of the Siamese network . The encoder is a bi - directional LSTM with 2 \u00d7 256 units , while the decoder is a plain LSTM with 512 units connected to a softmax layer that computes a probability distribution over the characters . The training is performed by minimizing the categorical cross - entropy loss , using teacher forcing ( Williams and Zipser , 1989 ) . The optimization setting is identical to that of the Siamese nework , with batches of 1024 samples . For inference , we collect the 10 output sequences with highest probability using beam search .", "entities": [[31, 32, "MethodName", "seq2seq"], [75, 76, "MethodName", "seq2seq"], [97, 99, "MethodName", "Siamese network"], [107, 108, "MethodName", "LSTM"], [120, 121, "MethodName", "LSTM"], [127, 128, "MethodName", "softmax"], [149, 150, "MetricName", "loss"]]}
{"text": "The three machine learning components discussed in the previous section have been individually evaluated on their specific task . Furthermore the final system has been evaluated in terms of correctly normalized book authors in a real case scenario . Siamese approximate name matching We evaluate the Siamese network on a held out test set , and compare it to an n - gram distance , by checking that the nearest neighbor of a name variant is the canonical name of the entity to which it belongs . We find an accuracy of 79.8 % for the Siamese network , against 71.1 % for the n - gram baseline with n = 3 . We have also checked metrics when introducing a threshold distance above which we consider that no matching entity is found , and found systematic improvement over the baseline . In the final system , we set the threshold to infinity . Siamese networks are more effective than simpler rule - based approaches and more specifically they perform better than the n - gram baseline on the following cases : Vittorio Hugo Victor Hugo : capturing name variants in different languages ; Bill Shakespeare William Shakespeare : capturing common nicknames Name correction with seq2seq networks Similarly to the previous approach , the seq2seq network is evaluated on a held out test set by checking that one of the generated name variants is the canonical name of the entity to which it belongs . As expected , name normalization using seq2seq network gives poorer performances than approximate matching within a dataset of known authors , but constitutes a complementary approach that is useful in case of formatting issues or incomplete names . This approach alone reaches a top - 10 accuracy of 42 % on the entire test set , 26 % on a test set containing only names with initials , and 53 % on a test set containing only minor spelling mistakes . Some examples where seq2seq performs better than the other methods are as follows : V. Hugo Victor Hugo : first name prediction for authors we do n't have in the canonical database ; Vicor Hugo Victor Hugo : misspelling correction for authors we do n't have in the canonical database . Ranking of the proposals With a decision threshold of p = 0.5 , the trained classifier has an accuracy of 93 % for both positive and negative candidates in the test set . The coefficients of the estimator reveal the importance of the features and , thus , of the related components . The three most important contributions are the match with the Siamese network , the match via ISBN in Babelio , and the similarity with the input catalog name , confirming the relevance of a multi - approach design choice . Global system In order to reflect the actual use of the global system on e - commerce catalog data , the final evaluation is performed at the book level , by considering all the proposals provided by the different components for a given book . The metric used is the top - k accuracy on the ranked list of proposals for each book ; results are summarized in Table 2 . We find that 72 % of the books have the author 's name normalized by the highest ranked proposal . Excluding from the evaluation books where the ground truth for the author 's name equals the catalog value , this accuracy drops to 49 % . In the case of books without ISBN or that do not match on any of the bibliographic resources , thus relying on machine learning - based components only , we find that 50 % of the books are normalized by the top proposal . Finally , for the combination of the above two restrictions , we find a top - 1 accuracy of 35 % .", "entities": [[46, 48, "MethodName", "Siamese network"], [90, 91, "MetricName", "accuracy"], [96, 98, "MethodName", "Siamese network"], [205, 206, "MethodName", "seq2seq"], [214, 215, "MethodName", "seq2seq"], [251, 252, "MethodName", "seq2seq"], [291, 292, "MetricName", "accuracy"], [329, 330, "MethodName", "seq2seq"], [395, 396, "MetricName", "accuracy"], [440, 442, "MethodName", "Siamese network"], [523, 524, "MetricName", "accuracy"], [581, 582, "MetricName", "accuracy"], [648, 649, "MetricName", "accuracy"]]}
{"text": "There is a long line of work on author name disambiguation for the case of bibliographic citation records ( Hussain and Asghar , 2017 ) . While related , this problem differs from the one of book authors . Indeed , unlike most books , research publications usually have several authors , each of them having published papers with other researchers . The relationships among authors , which can be represented as a graph , may be used to help disambiguate the bibliographic citations . Named entity linking ( Shen et al , 2015 ) , where one aims at determining the identity of entities ( such as a person 's name ) mentioned in text , is another related problem . The crucial difference with the disambiguation of book authors is that entity linking systems leverage the context of the named entity mention to link unambiguously to an entity in a pre - populated knowledge base . The conformity of truth in web resources is also a related problem , addressed in the literature by TruthFinder ( Yin et al , 2008 ) algorithms . Similarly , the proposed global model in which we combine sources learns to some extent the level of trust of the different sources . Unlike our technique , the TruthFinder approach needs to start from a book we can unambiguously identify in several sources and , thus , needs its ISBN . Distance metric learning with neural networks has been used for merging datasets on names ( Srinivas et al , 2018 ) , for normalizing job titles ( Neculoiu et al , 2016 ) , and for the disambiguation of researchers ( Zhang et al , 2018 ) . Sequence - to - sequence learning has been used for the more general task of text normalization ( Sproat and Jaitly , 2016 ) , and for sentence - level grammar error identification ( Schmaltz et al , 2016 ) . To the best of our knowledge , the problem of normalization of book authors name has not been tackled in the previous literature , except for a work on named entity linking for French writers ( Frontini et al , 2015 ) .", "entities": [[86, 88, "TaskName", "entity linking"], [133, 135, "TaskName", "entity linking"], [238, 240, "HyperparameterName", "Distance metric"], [357, 359, "TaskName", "entity linking"]]}
{"text": "One way to evaluate the robustness of a machine learning model is to search for inputs that produce incorrect outputs . Inputs intentionally designed to fool deep learning models are referred to as adversarial examples ( Goodfellow et al , 2017 ) . Adversarial examples have successfully tricked deep neural networks for image classification : two images that look exactly the same to a human receive \u21e4 * Equal contribution 1 Our code and datasets are available here . Figure 1 : An adversarial example generated by TFAD - JUSTED for BERT fine - tuned on the Rotten Tomatoes sentiment analysis dataset . Swapping a single word causes the prediction to change from positive to negative . completely different predictions from the classifier ( Goodfellow et al , 2014 ) . While applicable in the image case , the idea of an indistinguishable change lacks a clear analog in text . Unlike images , two different sequences of text are never entirely indistinguishable . This raises the question : if indistinguishable perturbations are not possible , what are adversarial examples in text ? The literature contains many potential answers to this question , proposing varying definitions for successful adversarial examples ( Zhang et al , 2019 ) . Even attacks with similar definitions of success often measure it in different ways . The lack of a consistent definition and standardized evaluation has hindered the use of adversarial examples to understand and improve NLP models . 2 Therefore , we propose a unified definition for successful adversarial examples in natural language : perturbations that both fool the model and fulfill a set of linguistic constraints . In Section 2 , we present four categories of constraints NLP adversarial examples may follow , depending on the context : semantics , grammaticality , overlap , and non - suspicion to human readers . By explicitly laying out categories of constraints adversarial examples may follow , we introduce a shared vocabulary for discussing constraints on adversarial attacks . In Section 4 , we suggest options for human and automatic evaluation methods for each category . We use these methods to evaluate two SOTA synonym substitution attacks : GENETICATTACK by Alzantot et al ( 2018 ) and TEXTFOOLER by Jin et al ( 2019 ) . Human surveys show that the perturbed examples often fail to fulfill semantics and non - suspicion constraints . Additionally , a grammar checker detects 39 % more errors in the perturbed examples than in the original inputs , including many types of errors humans almost never make . In Section 5 , we produce TFADJUSTED , an attack with the same search process as TEXTFOOLER , but with constraint enforcement tuned to generate higher quality adversarial examples . To enforce semantic preservation , we tighten the thresholds on the cosine similarity between embeddings of swapped words and between the sentence encodings of original and perturbed sentences . To enforce grammaticality , we validate perturbations with a grammar checker . As in TEXTFOOLER , these constraints are applied at each step of the search . Human evaluation shows that TFADJUSTED generates perturbations that better preserve semantics and are less noticeable to human judges . However , with stricter constraints , the attack success rate decreases from over 80 % to under 20 % . When used for adversarial training , TEXTFOOLER 's examples decreased model accuracy , but TFADJUSTED 's examples did not . Without a shared vocabulary for discussing constraints , past work has compared the success rate of search methods with differing constraint application techniques . Jin et al ( 2019 ) reported a higher attack success rate for TEXTFOOLER than Alzantot et al ( 2018 ) did for GENETICATTACK , but it was not clear whether the improvement was due to a better search method 3 or more lenient constraint application 4 . In Section 6 we compare the search methods with constraint application held constant . We find that GENETICATTACK 's search method is more successful than TEXTFOOLER 's , contrary to the implications of Jin et al ( 2019 ) . The five main contributions of this paper are : A definition for constraints on adversarial perturbations in natural language and suggest evaluation methods for each constraint . Constraint evaluations of two SOTA synonymsubstitution attacks , revealing that their perturbations often do not preserve semantics , grammaticality , or non - suspicion . Evidence that by aligning automatic constraint application with human judgment , it is possible for attacks to produce successful , valid adversarial examples . Demonstration that reported differences in attack success between TEXTFOOLER and GENET - ICATTACK are the result of more lenient constraint enforcement . Our framework enables fair comparison between attacks , by separating effects of search methods from effects of loosened constraints .", "entities": [[52, 54, "TaskName", "image classification"], [91, 92, "MethodName", "BERT"], [99, 101, "TaskName", "sentiment analysis"], [565, 566, "MetricName", "accuracy"], [772, 773, "MethodName", "GENET"]]}
{"text": "Overlap constraints restrict the similarity between x and x adv at the character level . This in - cludes constraints like Levenshtein distance as well as n - gram based measures such as BLEU , ME - TEOR and chRF ( Papineni et al , 2002 ; Denkowski and Lavie , 2014 ; Popovi\u0107 , 2015 ) . Setting a maximum edit distance is useful when the attacker is willing to introduce misspellings . Additionally , the edit distance constraint is sometimes used when improving the robustness of models . For example , Huang et al ( 2019 ) uses Interval Bound Propagation to ensure model robustness to perturbations within some edit distance of the input .", "entities": [[33, 34, "MetricName", "BLEU"]]}
{"text": "Automatic evaluation of semantic similarity is a well - studied NLP task . The STS Benchmark is used as a common measurement ( Cer et al , 2017 ) . Michel et al ( 2019 ) explored the use of common evaluation metrics for machine translation as a proxy for semantic similarity in the attack setting . While n - gram overlap based approaches are computationally cheap and work well in the machine translation setting , they do not correlate with human judgment as well as sentence encoders . Some attacks have used sentence encoders to encode two sentences into a pair of fixed - length vectors , then used the cosine distance between the vectors as a proxy for semantic similarity . TEXTFOOLER uses the Universal Sentence Encoder ( USE ) , which achieved a Pearson correlation score of 0.782 on the STS benchmark ( Cer et al , 2018 ) . Another option is BERT fine - tuned for semantic similarity , which achieved a score of 0.865 ( Devlin et al , 2018 ) . Additionally , synonym substitution methods , including TEXTFOOLER and GENETICATTACK , often require that words be substituted only with neighbors in the counter - fitted embedding space , which is designed to push synonyms together and antonyms apart ( Mrksic et al , 2016 ) . These automatic metrics of similarity produce a score that represents the similarity between x and x adv . Attacks depend on a minimum threshold value for each metric to determine whether the changes between x and x adv preserve semantics . Human evaluation is needed to find threshold values such that people generally \" agree \" that semantics is preserved .", "entities": [[3, 5, "TaskName", "semantic similarity"], [14, 16, "DatasetName", "STS Benchmark"], [44, 46, "TaskName", "machine translation"], [50, 52, "TaskName", "semantic similarity"], [72, 74, "TaskName", "machine translation"], [120, 122, "TaskName", "semantic similarity"], [130, 131, "MethodName", "USE"], [136, 138, "MetricName", "Pearson correlation"], [143, 145, "DatasetName", "STS benchmark"], [156, 157, "MethodName", "BERT"], [161, 163, "TaskName", "semantic similarity"]]}
{"text": "The simplest way to automatically evaluate grammatical correctness is with a rule - based grammar checker . Free grammar checkers are available online in many languages . One popular checker is LanguageTool , an open - source proofreading tool ( Naber , 2003 ) . LanguageTool ships with thousands of human - curated rules for the English language and provides an interface for identifying grammatical errors in sentences . LanguageTool uses rules to detect grammatical errors , statistics to detect uncommon sequences of words , and language model perplexity to detect commonly confused words .", "entities": [[88, 89, "MetricName", "perplexity"]]}
{"text": "Automatic evaluation may be used to guess whether or not an adversarial example is suspicious . Models can be trained to classify passages as real or perturbed , just as human judges do . For example , Warstadt et al ( 2018 ) trained sentence encoders on a real / fake task as a proxy for evaluation of linguistic acceptability . Recently , Zellers et al ( 2019 ) demonstrated that GROVER , a transformer - based text generation model , could classify its own generated news articles as human or machine - written with high accuracy .", "entities": [[58, 60, "TaskName", "linguistic acceptability"], [77, 79, "TaskName", "text generation"], [96, 97, "MetricName", "accuracy"]]}
{"text": "We presented a shuffled mix of real and perturbed examples to human judges and asked if they were real or computer - altered . As this is a time - consuming task for long documents , we only evaluated adversarial examples generated by TEXTFOOLER on the sentence - level MR dataset . If all generated examples were non - suspicious , judges would average 50 % accuracy , as they would not be able to distinguish between real and perturbed examples . In this case , judges achieved 69.2 % accuracy .", "entities": [[49, 50, "DatasetName", "MR"], [66, 67, "MetricName", "accuracy"], [90, 91, "MetricName", "accuracy"]]}
{"text": "Since all examples produced by TFADJUSTED are checked with LanguageTool , no perturbation can introduce grammatical errors . 10 Non - suspicion . We repeated the non - suspicion study from Section 4.3 with the examples generated by TFADJUSTED . Participants were able to guess with 58.8 % accuracy whether inputs were computer - altered . The accuracy is over 10 % lower than the accuracy on the examples generated by TEXTFOOLER . Attack success . For each of the three datasets , the attack success rate decreased by at least 71 percentage points ( see last row of Table 5 ) .", "entities": [[48, 49, "MetricName", "accuracy"], [57, 58, "MetricName", "accuracy"], [65, 66, "MetricName", "accuracy"]]}
{"text": "Quality Examples Using the 9 , 595 samples in the MR training set as seed inputs , TEXTFOOLER generated 7 , 382 adversarial examples , while TFADJUSTED generated just 825 . We append each set of adversarial examples to a copy of the original MR training set and fine - tuned a pre - trained BERT model for 10 epochs . Figure 2 plots the test accuracy over 10 training epochs , averaged over 5 random seeds per dataset . While neither training method strongly impacts accuracy , the augmentation using TFADJUSTED has a better impact than that of TEXTFOOLER . We then re - ran the two attacks using 1000 examples from the MR test set as seeds . Again averaging over 5 random seeds , we found no significant change in robustness . That is , models trained on the original MR dataset were approximately as robust as those trained on the datasets augmented with TEXTFOOLER and TFADJUSTED examples . This corroborates the findings of Alzantot et al ( 2018 ) and contradicts those of Jin et al ( 2019 ) . We include further analysis along with some hypotheses for the discrepancies in adversarial training results in A.4 .", "entities": [[10, 11, "DatasetName", "MR"], [44, 45, "DatasetName", "MR"], [55, 56, "MethodName", "BERT"], [66, 67, "MetricName", "accuracy"], [76, 77, "DatasetName", "seeds"], [86, 87, "MetricName", "accuracy"], [114, 115, "DatasetName", "MR"], [118, 119, "DatasetName", "seeds"], [125, 126, "DatasetName", "seeds"], [143, 144, "DatasetName", "MR"]]}
{"text": "A case - based reasoning ( CBR ) system solves a new problem by retrieving ' cases ' that are similar to the given problem . If such a system can achieve high accuracy , it is appealing owing to its simplicity , interpretability , and scalability . In this paper , we demonstrate that such a system is achievable for reasoning in knowledgebases ( KBs ) . Our approach predicts attributes for an entity by gathering reasoning paths from similar entities in the KB . Our probabilistic model estimates the likelihood that a path is effective at answering a query about the given entity . The parameters of our model can be efficiently computed using simple path statistics and require no iterative optimization . Our model is non - parametric , growing dynamically as new entities and relations are added to the KB . On several benchmark datasets our approach significantly outperforms other rule learning approaches and performs comparably to state - of - the - art embedding - based approaches . Furthermore , we demonstrate the effectiveness of our model in an \" open - world \" setting where new entities arrive in an online fashion , significantly outperforming state - ofthe - art approaches and nearly matching the best offline method . 1", "entities": [[33, 34, "MetricName", "accuracy"]]}
{"text": "We live in an evolving world with a lot of heterogeneity as well as new entities being created continuously . For example , scientific papers and Wikipedia pages describing facts about new entities , are being constantly added ( e.g. . These new findings further trigger the inference of newer facts , each with its own diverse reasoning . We are interested in developing such automated reasoning systems for large knowledge - bases ( KBs ) . In machine learning , non - parametric methods hold the promise of handling evolving data ( Cover 1 Code available at https://github.com/ ameyagodbole / Prob - CBR and Hart , 1967 ; Rasmussen , 2000 ) . Most current KG completion models learn low dimensional parametric representation of entities and relations via tensor factorization or sophisticated neural approaches ( Nickel et al , 2011 ; Bordes et al , 2013 ; Socher et al , 2013 ; Sun et al , 2019 ; Vashishth et al , 2020 ) . Another line of work learns Hornclause style reasoning rules from the KG and stores them in its parameters Das et al , 2018 ; Minervini et al , 2020 ) . However , these parametric approaches work with a fixed set of entities and it is unclear how these models will adapt to new entities . This paper presents a k - nearest neighbor ( KNN ) based approach for KG reasoning that is reminiscent of case - based reasoning ( CBR ) in classical AI . A CBR system solves a new problem by retrieving ' cases ' that are similar to the given problem , revising the solution to retrieved cases ( if necessary ) and reusing it for the new problem ( Schank , 1982 ; Leake , 1996 , inter - alia ) . For the task of finding a target entity given a source entity and binary KG relation ( e.g. ( JOHN VON NEU - MAN , PLACE OF DEATH , ? ) in Figure 1 ) , our approach first retrieves k similar entities ( cases ) to the query entity . Next , for each retrieved entity , it finds multiple KG paths 2 ( each path is a solution to retrieved cases ) to the entity they are connected by the query relation ( e.g. paths between ( RICHARD FEYNMAN , USA ) ) . However , one solution seldom works for all queries . For example , even though the path ' BORN IN ' is predictive of ' PLACE OF DEATH ' for US - born scientists ( figure 1 ) , it does not work for scientists who have immigrated to USA . To handle this , we present a probabilistic CBR approach which learns to weighs paths with respect to an estimate of its prior and its precision , given the query . The prior of a path rep - Figure 1 : Given the query , ( JON VON NEUMANN , PLACE OF DEATH , ? ) , our model gathers reasoning paths from similar entities such as other scientists . However , not all gathered paths work for a query e.g. the path ( ' BORN ( x , y ) ' ) would not work for VON NEUMANN . This highlights the importance of learning path weights for clusters of similar entities . Even though ' BORN IN ' could be a reasonable path for predicting PLACE OF DEATH , this does not apply for VON NEUMANN and other scientists in his cluster . The precision parameter of the path given the cluster helps in penalizing the ' BORN IN ' path . Note that the node USA is repeated twice in the figure to reduce clutter . resents its frequency while the precision represents the likelihood that the path will lead to a correct answer entity . To obtain robust estimates of the path parameters , we cluster similar entities together and compute them by simple count statistics ( 2.2.3 ) . Apart from computing these estimates , our method needs no further training . Overall , our simple approach outperforms several recent parametric rule learning methods ( Das et al , 2018 ; Minervini et al , 2020 ) and performs competitively with various state - of - the - art KG completion approaches ( Dettmers et al , 2018 ) on multiple datasets . An advantage of non - parametric models is that it can adapt to growing data by adjusting its number of parameters . In the same spirit , we show that our model can seamlessly handle an ' open - world ' setting in which new entities arrive in the KG . This is made possible by several design choices such as ( a ) representing entities as sparse ( non - learned ) vector of its relation types ( 2.2.1 ) , ( b ) our use of an online non - parametric hierarchical clustering algorithm ( Monath et al , 2019 ) that can efficiently recompute changes in cluster assignments because of the newly added entity ( 2.3 ) , ( c ) and a simple and efficient way of recomputing the prior and precision parameters for paths per cluster ( 2.2.3 ) . Current models for KG completion that learn entity representations for a fixed set of entities can not handle the open - world setting . In fact we show that , retraining the models continually with new data leads to severe degradation of the model performance with models forgetting what it had learned before . For example , the performance ( MRR ) of ROTATE model ( Sun et al , 2019 ) drops by 11 points ( absolute ) on WN18RR in this setting ( 3.4 ) . On the other hand , we show that with new data , the performance of our model is consistent as it is able to seamlessly reason with the newly arrived data . Our work is most closely related to a recent concurrent work by Das et al ( 2020 ) where they propose a model that gathers paths from entities similar to the query entity . However , Das et al ( 2020 ) encourages path that occur frequently in the KG and does not learn to weigh paths differently for queries . This often leads to wrong inference leading to low performance . For example , on the test - II evaluation subset of FB122 where all triples can be inferred by logical rules , Das et al ( 2020 ) scores quite low ( 63 MRR ) because of learning incorrect rules . On the other hand , we score significantly higher ( 94.83 MRR ) demonstrating that we can learn more effective rules . In fact , we consistently and significantly outperform Das et al ( 2020 ) on several benchmark datasets . Also , unlike us , they do not test themselves in the challenging open - world setting . The contributions of this paper are as follows : ( a ) We present a KNN based approach for KG completion that gathers reasoning paths from entities that are similar to the query entity . Following a principled probabilistic approach ( 2.2 ) , our model weighs each path by its likelihood of reaching a correct answer which penalizes paths that are spurious in nature . ( b ) The parameters of our model grow with data and can be estimated efficiently using simple count statistics ( 2.3 ) . Apart from this , our approach needs no training . We show that our simple approach significantly outperforms various rule learning methods ( Das et al , 2018 ; Minervini et al , 2020 ; Das et al , 2020 ) on many benchmark datasets . ( c ) We also show that our model can easily handle addition of facts about new entities and is able to seamlessly integrate and reason with the newly added data significantly outperforming parametric embedding based models . 2 Non - parametric Reasoning in KGs", "entities": [[758, 761, "HyperparameterName", "number of parameters"], [945, 946, "MetricName", "MRR"], [965, 966, "DatasetName", "WN18RR"], [1088, 1089, "DatasetName", "FB122"], [1110, 1111, "MetricName", "MRR"], [1129, 1130, "MetricName", "MRR"]]}
{"text": "A great benefit of non - parametric models is that it can seamlessly handle growing data by adding new parameters . New entities constantly arrive in the world ( e.g. new Wikipedia articles about entities are frequently created ) . We consider a setting ( Figure 2 ) in which new entities with few facts ( edges ) about them keep getting added to the KG . This setting is challenging for parametric models ( Das et al , 2018 ; Sun et al , 2019 ) as it is unclear how these models can incorporate new entities without retraining from scratch . However , retraining to obtain entity embeddings on industrial scale KGs might be impractical ( e.g. consider Facebook social graph where new users are joining continuously ) . Next , we show that our approach can handle this setting efficiently in the following way : ( a ) Adding / updating entity representations : First we need to create entity representations for the newly arrived entities . Also , for some existing entities for which new edges were added ( e.g. BILL GATES , DURHAM , etc . in figure 2 ) , their representations need to be updated . Recall , that we represent entities as a sparse vector of its edge types and hence this step is trivial for our approach . ( b ) Updating cluster assignments : Next the new entities needs to be added to clusters of similar entities . Also , the cluster assingments of entities that got updated can also change as well and their change can further trigger changes to the clustering of other entities . To handle this , one could naively cluster all entities in the KG , however that could be wasteful and time - consuming for large KGs . Instead , we use an online hierarchical clustering algorithm - GRINCH ( Monath et al , 2019 ) , which has shown to perform as well as agglomerative clustering in the online setting . GRINCH observes one entity at a time , placing it next to its nearest neighbor and performing local re - arrangements in the form of rotations of tree nodes and global rearrangments in the form of grafting a subtrees from part of the tree to another . Entities can be deleted from a hierarchy by simply removing the corresponding leaf node . We first use GRINCH to delete the entities whose representations had changed because of the addition of the new node and then incrementally add those entities back along with the newly added entities in the KG . We extract a flat clustering from the hierarchical clustering built ( c ) Re - estimating new parameters : After reassigning clusters , the final step is to estimate the per - cluster parameters . This computation is efficient as it is clear from equations 2 and 3 that the contribution from each entity in a cluster can be computed independently ( and hence can be easily parallelized ) . However , even for each entity , this computation needs path traversal in the KG which is expensive . We show that we do not have to recompute for all entities in the clusters . Let n denote the maximum length of a reasoning path considered by our model . For every new entity e i added to the KG , we need to recompute statistics for entities that lie within cycles of length up to ( n + 1 ) starting from e i . Please refer to appendix ( A.4 ) for a justification of this result .", "entities": [[108, 110, "TaskName", "entity embeddings"], [203, 204, "MetricName", "Recall"]]}
{"text": "Data . We evaluate on the following KBC datasets : NELL - 995 , FB122 ( Guo et al , 2016 ) , WN18RR ( Dettmers et al , 2018 ) . FB122 is a subset of the dataset derived from Freebase , FB15 K ( Bordes et al , 2013 ) , containing 122 relations regarding people , locations , and sports . NELL - 995 ( Xiong et al , 2017 ) a subset of the NELL derived from the 995th iteration of the system . WN18RR was created by Dettmers et al ( 2018 ) from WN18 by removing inverse relation test - leakage . Evaluation metrics . Following previous work , we evaluate our method using HITS@N and mean reciprocal rank ( MRR ) , which are standard metrics for evaluating a ranked list .", "entities": [[10, 11, "DatasetName", "NELL"], [14, 15, "DatasetName", "FB122"], [23, 24, "DatasetName", "WN18RR"], [32, 33, "DatasetName", "FB122"], [64, 65, "DatasetName", "NELL"], [78, 79, "DatasetName", "NELL"], [88, 89, "DatasetName", "WN18RR"], [99, 100, "DatasetName", "WN18"], [126, 127, "MetricName", "MRR"]]}
{"text": "Figure 3 reports the result for this task . We report results on the RotatE model with randomly initialized embeddings for new entities ( RotatE ) and the model with systematic initialization of new entity embeddings ( RotatE+ ) . We experiment with m = { 10 % , 30 % } of previously seen edges and retrain on them . We find that not including previously seen edges leads to severe degradation of overall performance due to the model forgetting what it had learned in the past . We also report results with freezing the already seen entity representations and only learning representations for new entities ( RotatE - Freeze ) . All models were trained till the validation set ( containing both new and old triples ) performance stopped improving . For our approach , we also report results for an oracle setting where we re - cluster all entities as new data arrives and re - estimate all parameters from scratch ( instead of using GRINCH and recomputing only required parameters ( 2.3 ) . For both datasets , the offline - best results were obtained by RotatE ( 47.1 for FB122 test - I , 48 for WN18RR ) . We report performance on the entire evaluation set ( full ) and also on the set containing the newly added edges ( new ) . The main summary of the results are ( i ) RotatE model converges to a much lower performance in the online setting losing at least 8 MRR points in FB122 and at least 11 points in WN18RR . On FB122 , we observe that the model prefers to learn new information more by sacrificing previously learned facts ( 2nd subfigure in figure 3 ) ( ii ) In the freeze setting , the model performance deteriorates quickly after a certain point indicating saturation , i.e. it becomes hard for the model to learn new information about arriving entities by keeping the parameters of the existing entities fixed . ( iii ) On the full evaluation , RotatE+ performs better than RotatE showing that bad initialization deteriorates performance over time , however , there is still a large gap between the best performance ( iv ) Our approach almost matches our performance in oracle setting indicating the effectiveness of the online clustering and fast parameter approximation . ( v ) Lastly , we perform closest to the offline best results outperforming all variants of RotatE.", "entities": [[14, 15, "MethodName", "RotatE"], [24, 25, "MethodName", "RotatE"], [34, 36, "TaskName", "entity embeddings"], [108, 109, "MethodName", "RotatE"], [190, 191, "MethodName", "RotatE"], [194, 195, "DatasetName", "FB122"], [201, 202, "DatasetName", "WN18RR"], [239, 240, "MethodName", "RotatE"], [255, 256, "MetricName", "MRR"], [258, 259, "DatasetName", "FB122"], [265, 266, "DatasetName", "WN18RR"], [268, 269, "DatasetName", "FB122"], [349, 350, "MethodName", "RotatE"]]}
{"text": "Computing Infrastructure : All our experiments were run on a Xeon E5 - 2680 v4 @ 2.40GHz CPU with 128 GB RAM . No GPUs were needed for the experiments . The results on the validation set are reported in table 8 and avg . of 3 runs are reported in table 9 . The NELL - 995 does not come with a validation set , and therefore we selected 3000 edges randomly from the full NELL KB . As a result , many of the query relations were different from what was present in the splits of NELL - 995 and hence is not a good representative . However , we report test results for the best hyper - parameter values that we got on this validation set . The fixed number of parameters in our model are essentially the sparse non - learned entity vectors ( which can be easily stored in COO format without taking much space ) . Other than that , our model is non - parametric with the number of parameters tied to the data . For experiments on WN18RR : Inference time : 18.9 queries / s ( total of 6268 queries ) Train time : around 20 mins . Best Hyper - parameters : - For experiments on FB122 : Inference time :", "entities": [[21, 22, "MethodName", "RAM"], [55, 56, "DatasetName", "NELL"], [76, 77, "DatasetName", "NELL"], [98, 99, "DatasetName", "NELL"], [132, 135, "HyperparameterName", "number of parameters"], [174, 177, "HyperparameterName", "number of parameters"], [185, 186, "DatasetName", "WN18RR"], [216, 217, "DatasetName", "FB122"]]}
{"text": "The purpose of this step is to aggregate semantic information at the inter - groups level . The forex trade data and the finance news are highly relevant : the trade data represents the history movement of forex , and the finance news represents the environmental variable . So the combination of them can help us model the forex movement better . In a certain input time , news groups have different impacts on forex movement . So we employ the trade data as a query to calculate the attention weights of news groups . Then the weighted sum of news groups and the trade data representation are finally fused to predict the forex movement . For forex trade data y , we apply a 3 - layer perceptron to access the trade data representation R t , and each layer is a non - linear transform with Relu activation function . Then we calculate the attention weight between R t and G i : g ( i ) = Relu ( R t * W a * G i ) ( 5 ) att i = e g ( i ) L i=1 e g ( i ) ( 6 ) Where att ( i ) is the i - th news group 's attention weight to trade data . Then we sum the news groups representations up to get the final news semantic representation R s : R s = L i=1 G i * att i ( 7 ) To fuse the news semantic and trade data representations effectively , we choose the fusion function used in Mou et al , 2016 ) to fuse R s and R t and predict the movement : R = [ R t ; R s ; R t \u2212 R s ; R t R s ] ( 8 ) p ( f | x , y ) = softmax ( W p * R + b p ) ( 9 ) means element - wise multiplication .", "entities": [[148, 149, "MethodName", "Relu"], [149, 151, "HyperparameterName", "activation function"], [170, 171, "MethodName", "Relu"], [320, 321, "MethodName", "softmax"]]}
{"text": "The loss function of the proposed model includes two parts : the negative log - likelihood training loss and the L 2 regularization item : Loss = \u2212f * log p ( f | x , y , \u03b8 ) + \u03bb 2 \u03b8 2 2 ( 10 ) \u03b8 is the model parameters . Experiments show that the performance improves after adding L 2 regularization . We train three models with different news grouping methods : time , topic and category , and we call them BHAM - Time , BHAM - Topic , BHAM - Category , respectively .", "entities": [[1, 2, "MetricName", "loss"], [13, 16, "MetricName", "log - likelihood"], [17, 18, "MetricName", "loss"], [38, 39, "HyperparameterName", "\u03b8"], [43, 44, "HyperparameterName", "\u03b8"], [49, 50, "HyperparameterName", "\u03b8"]]}
{"text": "The experiment dataset is accessed from the professional finance news providers Reuters 2 . We collect forex trade data of four major currency pairs ( USD - EUR , USD - JPY , USD - RMB , USD - GBP ) from 2013 to 2017 . We collect the open / close / high / low trade price for each trade minute . As for the finance news data , we collect all the English news happened in trade time released by Reuters and match the news with target currency pairs according to news region . For example , we match USD - EUR with news related to US , Europe or both of them . The raw data contains both news headline and body , and we utilize the headline only since the headline contains the most valuable information and has less noise . The forex movement label f is decided by the comparison of prediction time price and the input window ending price . We design the symbol USD - EUR ( 20 - 10 ) to represent the prediction for the USD - EUR exchange rate with 20 minutes input time and 10 minutes prediction delay . To access more data for training , we overlap the input time of samples . For example , when overlap - rate is 50 % , two consecutive samples ' input time will be 8:00 - 8:20 am and 8:10 - 8:30 am . Then the data samples will be twice as large as no overlap condition ( In the USD - EUR ( 20 - 10 ) dataset , the number of samples will increase from 31k to 62k ) . We reserve 5k samples for developing and 5k samples for testing . All the rest of samples are applied for training .", "entities": [[271, 274, "HyperparameterName", "number of samples"]]}
{"text": "We choose the pytorch - pretrained - BERT 3 as BERT implement and choose the bert - baseuncased version in which there are 12 layers , 768 hidden states and 12 attention heads in the transformer . We truncate the BERT input to 256 tokens and fine - tune the BERT parameters during training . We adopt the Adam ( Kingma and Ba , 2014 ) optimizer with the initial learning rate of 0.001 . We apply the dropout ( Srivastava et al , 2014 ) regularization with the dropout probability of 0.2 to reduce over - fitting . The batch size is 32 . The training epoch is 60 with early stop . The weight of L 2 regularization is 0.015 . The learning rate begins to decay after 10 epoch . The overlap rate of data samples is 50 % , and the number of selected news in each group is 3 . When splitting the dataset , we guarantee that the samples in train set are previous to samples in valid set and test set to avoid the possible information leakage . We tune the hyper - parameters on the development set and test model on the test set . The forex prediction is conducted as a binary classification task ( up or down ) . The evaluation metrics are macro - F1 and Matthews Correlation Coefficient ( MCC ) . MCC is often reported in stock movement forecast ( Xu and Cohen , 2018 ; Ding et al , 2016 ) because it can overcome the data imbalance issue .", "entities": [[7, 8, "MethodName", "BERT"], [10, 11, "MethodName", "BERT"], [40, 41, "MethodName", "BERT"], [50, 51, "MethodName", "BERT"], [58, 59, "MethodName", "Adam"], [66, 67, "HyperparameterName", "optimizer"], [70, 72, "HyperparameterName", "learning rate"], [100, 102, "HyperparameterName", "batch size"], [124, 126, "HyperparameterName", "learning rate"], [223, 226, "MetricName", "macro - F1"]]}
{"text": "In this section , we analyze the influence of two crucial time parameters on model performance , which are input time and predic - tion delay . We choose the input time { 10 , 20 , 30 , 40 , 50 , 60 } ( minutes ) , the prediction delay { 5 , 10 , 15 , 20 , 25 , 30 } ( minutes ) and experiment all combinations . We take the USD - JPY for example to analyze the time effect of forex trading , and we observe similar results in other currency pairs . The Figure 4 shows BHAM - Category model 's performances ( macro - F1 % ) on USD - JPY pair under different combinations of input time and prediction delay . We can observe that with the increase of input time from 10 minutes to 40 minutes , the model performance improves too . However , when we increase the input time continuously , the model performance begins to decrease . Too less text is not enough to support the prediction , but too many texts may bring much noise . The ideal input time is around 40 minutes . Besides , at all input time conditions , the model 's performances decline with the increase of prediction delay because events happened in the prediction delay time may also influence the forex movement . We can also conclude that forex movement pays more attention to the latest news because when masking the latest news input ( such as USD - JPY ( 40 - 05 ) and USD - JPY ( 30 - 15 ) , the latter one can be seen as the former one masking the lastest 10 minutes input ) , the model performance declines obviously at almost all conditions .", "entities": [[111, 114, "MetricName", "macro - F1"]]}
{"text": "In this work , we propose a BERT - based Hierarchical Aggregation Model to summarize a large amount of finance news for forex movement prediction . Experiments show that our model outperforms all the baselines by a large margin , which proves the effectiveness of the proposed framework . We design three grouping news methods : time , topic and category and experiments show that the category - based method performs best , which shows that the semantic information of forex related news is mostly aggregated by category . Experiments about time effect prove that the proper input time is about 40 minutes and the prediction accuracy declines with the increase of prediction delay . Besides , we analyze the influence of news attributes on forex trading and observe some interesting conclusions : Business Sectors news has the most influence on USD - EUR trading and Politics / International Affairs news effects USD - RMB trading most . Besides , both USD - JPY trading and USD - GBP trading pay most attention to news from US . All these influence patterns can help forex traders handle different news more wisely and make better decisions . To our knowledge , this is the first work to utilize the advanced NLP pre - train technology in the enormous forex market and the results show the potential of this research area . Promising future studies may include designing more suitable grouping methods or combining news grouping and market predicting in an end2end model .", "entities": [[7, 8, "MethodName", "BERT"], [106, 107, "MetricName", "accuracy"]]}
{"text": "In our first model , we use the lexical translation model and probability function p t in Equation 1 as the weighting function , which can be learned efficiently off - line using the EM algorithm . When attempting to use the SSSP procedure to compute this equation for a given source input x , we immediately have the problem that such a computation requires a complete component representation z ( Knight and Al - Onaizan , 1998 ) . We use an approximation 1 that involves ignoring the normalizer | A | and exploiting the word independence assumption of the model , which allows us to incrementally compute translation scores for individual source words given output translations corresponding to shortest paths during the SSSP search . The full decoding algorithm in shown in Algorithm 1 , where the red highlights the adjustments made to the standard SSSP search as presented in Cormen et al ( 2009 ) . The main modification involves adding a data structure s R | V | \u00d7 | x | ( initialized as 0.0 | V | \u00d7 | x | at line 2 ) that stores a running sum of source word scores given the best translations at each node , which can be used for computing the inner sum in Equation 1 . For example , given an input utterance ceiling function , s 6 in Figure 2 contains the independent translation scores for words ceiling and function given the edge label numeric and p t . Later on in the search , these scores are used to compute s 7 , which will provide translation scores for each word given the edge sequence numeric math . Taking the product over any given s j ( as done in line 7 to get score ) will give the probability of the shortest path translation at the particular point j. Here , the transformation into \u2212 log space is used to find the minimum incoming path . Standardly , the data structure \u03c0 can be used to retrieve the shortest path back to the source node b ( done via the FINDPATH method ) .", "entities": [[34, 35, "MetricName", "EM"]]}
{"text": "The second component is a decoder network , which directly computes the conditional distribution p ( z | x ) as follows : p ( z | x ) = | z | i=1 log p \u0398 ( z i | z < i , x ) ( 3 ) p \u0398 ( z i | z < i , x ) \u223c softmax ( f ( \u0398 , z < i , x ) ) ( 4 ) where f is a non - linear function that encodes information about the sequence z < i and the input x given the model parameters \u0398. We can think of this model as an ordinary recurrent language model that is additionally conditioned on the input x using information from our encoder . We implement the function f in the following way : f ( \u0398 , z < i , x ) = W o \u03b7 i + b o ( 5 ) \u03b7 i = MLP ( c i , g i ) ( 6 ) g i = LSTM dec ( g i\u22121 , E out z i\u22121 , c i ) ( 7 ) where MLP is a multi - layer perceptron model with a single hidden layer , E out R | \u03a3 dec | \u00d7e is a randomly initialized embedding matrix , g i is the decoder 's hidden state at step i , and c i is a contextvector that encodes information about the input x and the encoder annotations . Each context vector c i in turn is a weighted sum of each annotation h j against an attention vector \u03b1 i , j , or c i = | x | j=1 \u03b1 i , j h j , which is jointly learned using an additional single layered multi - layer perceptron defined in the following way : \u03b1 i , j \u221d exp ( e i , j ) ; e i , j = MLP ( g i\u22121 , h j ) ( 8 ) Lexical Bias and Copying In contrast to standard MT tasks , we are dealing with a relatively low - resource setting where the sparseness of the target vocabulary is an issue . For this reason , we experimented with integrating lexical translation scores using a biasing technique from Arthur et al ( 2016 ) . Their method is based on the following computation for each token z i : bias i = \uf8ee \uf8f0 p t ( 1 : d [ V [ G ] ] , d [ b ] o , \u03c0 [ V [ G ] ] N il 2 : s [ V [ G ] ] N il Path state information 3 : s [ b ] InitState ( ) Initialize source state 4 : for each vertex u \u2265 b V [ G ] in sorted order do 5 : if isinf ( d [ u ] ) then continue 6 : p s [ u ] Current state at node u , or z < i 7 : L 1 [ l ] arg max ( v 1 , ... , v k ) Adj [ u ] softmax ( f ( \u0398 , p , x ) )", "entities": [[36, 37, "HyperparameterName", "\u0398"], [51, 52, "HyperparameterName", "\u0398"], [63, 64, "MethodName", "softmax"], [67, 68, "HyperparameterName", "\u0398"], [143, 144, "HyperparameterName", "\u0398"], [165, 166, "DatasetName", "MLP"], [179, 180, "MethodName", "LSTM"], [197, 198, "DatasetName", "MLP"], [276, 277, "HyperparameterName", "\u03b1"], [289, 290, "HyperparameterName", "\u03b1"], [315, 316, "HyperparameterName", "\u03b1"], [333, 334, "DatasetName", "MLP"], [539, 540, "MethodName", "softmax"], [543, 544, "HyperparameterName", "\u0398"]]}
{"text": "for each vertex and label ( v , z ) L do 9 : score \u2212 log p\u0398 ( z | p , x ) + d [ u ] 10 : if d [ v ] > score then 11 : d [ v ] score , \u03c0 [ v ] u 12 : s [ v ] UpdateState ( p , z ) 13 : return FINDPATH ( \u03c0 , | V | , b ) The first matrix uses the inverse ( p t ) of the lexical translation function p t already introduced to compute the probability of each word in the target vocabulary \u03a3 dec ( the columns ) with each word in the input x ( the rows ) , which is then weighted by the attention vector from Equation 8 . bias i is then used to modify Equation 5 in the following way : f bias ( \u0398 , z < i , x ) = W o \u03b7 i + b o + log ( bias i + ) where is a hyper - parameter that helps to preserve numerical stability and biases more heavily on the lexical model when set lower . We also experiment with the copying mechanism from Jia and Liang ( 2016 ) , which works by allowing the decoder to choose from a set of latent actions , a j , that includes writing target words according to Equation 5 , as done standardly , or copying source words from x , or copy [ x i ] according to the attention scores in Equation 8 . A distribution is then computed over these actions using a softmax function and particular actions are chosen accordingly during training and decoding .", "entities": [[155, 156, "HyperparameterName", "\u0398"], [281, 282, "MethodName", "softmax"]]}
{"text": "The full decoding procedure is shown in Algorithm 2 , where the differences with the standard SSSP are again shown in red . We change the data structure s to contain the decoder 's RNN state at each node . We also modify the scoring ( line 7 , which uses Equation 4 ) to consider only the top l edges or translations at that point , as opposed to imposing a full search . When l is set to 1 , for example , the procedure does a greedy search through the graph , whereas when l is large the procedure is closer to a full search . In general terms , the decoder described above works like an ordinary neural decoder with the difference that each decision ( i.e. , new target - side word translation ) is constrained ( in line 7 ) by the transitions allowed in the underlying graph in order to ensure wellformedness of each component output . Standardly , we optimize these models using stochastic gradient descent with the objective of finding parameters\u0398 that minimize the negative conditional log - likelihood of the training dataset .", "entities": [[136, 138, "TaskName", "word translation"], [171, 174, "MethodName", "stochastic gradient descent"], [185, 188, "MetricName", "log - likelihood"]]}
{"text": "For the technical datasets , the goal is to see if our model generates correct signature representations from unobserved descriptions using exact match . We follow exactly the experimental setup and data splits from Richardson and Kuhn ( 2017b ) , and measure the accuracy at 1 ( Acc@1 ) , accuracy in top 10 ( Acc@10 ) , and MRR . For the GeoQuery and Sportscaster experiments , the goal is to see if our models can generate correct meaning representations for unseen input . For GeoQuery , we follow Andreas et al ( 2013 ) in evaluating extrinsically by checking that each representation evaluates to the same answer as the gold representation when executed against the Geobase database . For Sportscaster , we evaluate by exact match to a gold representation . Richardson and Kuhn ( 2017b , a ) , or RK", "entities": [[21, 23, "MetricName", "exact match"], [44, 45, "MetricName", "accuracy"], [51, 52, "MetricName", "accuracy"], [60, 61, "MetricName", "MRR"], [127, 129, "MetricName", "exact match"]]}
{"text": "Technical Documentation Results Table 1 shows the results for Stdlib and Py27 . In the monolingual case , we compare against the best performing models in Richardson and Kuhn ( 2017b , a ) . As summarized in Figure 3 , our experiments show that training polyglot models on multiple datasets can lead to large improvements over training individual models , especially on the Py27 datasets where using a polyglot model resulted in a nearly 9 % average increase in accuracy @1 . In both cases , however , the best performing lexical models are those trained only on the datasets they are evaluated on , as opposed to training on all datasets ( i.e. , + more ) . This is surprising given that training on all datasets doubles the size of the training data , and shows that adding more data does not necessarily boost performance when the additional data is from another distribution .", "entities": [[80, 81, "MetricName", "accuracy"]]}
{"text": "Acc@1 Acc@10 UBL ( Kwiatkowski et al , 2010 ) 74.2 - TreeTrans ( Jones et al , 2012 ) 76.8 - nHT ( Susanto and Lu , 2017 ) 83 . The neural models are strongly outperformed by all other models both in the monolingual and polyglot case ( only the latter results shown ) , even when lexical biasing is applied . While surprising , this is consistent with other studies on lowresource neural MT ( Zoph et al , 2016 ; \u00d6stling and Tiedemann , 2017 ) , where datasets of comparable size to ours ( e.g. , 1 million tokens or less ) typically fail against classical SMT models . This result has also been found in relation to neural AMR semantic parsing , where similar issues of sparsity are encountered ( Peng et al , 2017 ) . Even by doubling the amount of training data by training on all datasets ( results not shown ) , this did not improve the accuracy , suggesting that much more data is needed ( more discussion below ) . Beyond increases in accuracy , our polyglot models support zero - shot translation as shown in Figure 4 , which can be used for translating between unobserved language pairs ( e.g. , ( es , Clojure ) , ( ru , Haskell ) as shown in 1 - 2 ) , or for finding related functionality across different software projects ( as shown in 3 ) . These results were obtained by running our decoder model without specifying the output language . We note , however , that the decoder can be constrained to selectively translate to any specific programming language or project ( e.g. , in a QA setting ) . Future work will further investigate the decoder 's polyglot capabilities , which is currently hard to evaluate since we do not have an annotated set of function equivalences between different APIs .", "entities": [[125, 127, "TaskName", "semantic parsing"], [167, 168, "MetricName", "accuracy"], [185, 186, "MetricName", "accuracy"]]}
{"text": "Having results across related SP tasks allows us to reflect on the nature of the main technical documentation task . Consistent with recent findings ( Dong and Lapata , 2016 ) , we show that relatively simple neural sequence models are competitive with , and in some cases outperform , traditional grammar - based SP methods on bench - mark SP tasks . However , this result is not observed in our technical documentation task , in part because this problem is much harder for neural learners given the sparseness of the target data and lack of redundancy . For this reason , we believe our datasets provide new challenges for neural - based SP , and serve as a cautionary tale about the scalability and applicability of commonly used neural models to lower - resource SP problems . In general , we believe that focusing on polyglot and mixed language decoding is not only of interest to applications ( e.g , mixed language API QA ) but also allows for new forms of SP evaluation that are more revealing than only translation accuracy . When comparing the accuracy of the best monolingual Geo model and the worst performing neural polyglot model , one could mistakingly think that these models have equal abilities , though the polyglot model is much more robust and general . Moving forward , we hope that our work helps to motivate more diverse evaluations of this type .", "entities": [[183, 184, "MetricName", "accuracy"], [188, 189, "MetricName", "accuracy"]]}
{"text": "To accompany our text editing task we present a novel dataset of nearly 12 million sentence - level edits , WikiDocEdits . These edits were extracted from the revision histories in the February 1 , 2020 dump of English Wikipedia . 3 For a given Wikipedia page , a revision consists of a source and target text , corresponding to the old and new versions of the page . Each revision is also accompanied by an editor comment , which we will use as a proxy for the user command . For a given revision , we split the source and target texts into sentences and then attempt to match the sentences between source and target . For efficiency , we only look at a k - sentence neighborhood . Unmatched sentences are candidates for edits . A source sentence s and target sentence t form an edit pair s \u2212 t if f ( s , t ) > , where f is sentencelevel BLEU 4 without smoothing and = 0.1 in our case . If an unmatched source sentence does not form an edit pair with any target sentence , we consider it to be a sentence deletion . This can also be thought of as matching to an empty sentence . We identify sentence insertions in an analogous manner . Importantly , we only consider revisions that contain a single sentence - level edit . Otherwise , the editor comment that accompanies each revision may only describe one of the possibly many sentence - level edits . See appendix A for a detailed description of our processing pipeline .", "entities": [[20, 21, "DatasetName", "WikiDocEdits"], [165, 166, "MetricName", "BLEU"]]}
{"text": "We formalize our model , which we refer to as Interactive Editor , as a standard auto - regressive sequence to sequence model . Because our data only contains single - sentence edits , we assume that the sentence to be edited in the source document is given as an input to the model . Given a source sentence s D , the context around s , which we will refer to as D by abuse of notation , a user command q , a grounding corpus G , and a candidate target sentence s , the model , f , computes f ( s , s , D , q , G ) = P ( s | s , D , q , G ) = i P ( s i | s < i , s , D , q , G ) , where s < i = { s 0 , ... , s i\u22121 } are the tokens preceding s i in s . We use the same encoder - decoder architecture as T5 ( Raffel et al , 2020 ) and initialize our model with pretrained language model weights . The encoder - decoder architecture allows us to perform full attention over the inputs s , D , q , and G , while the decoder allows us to auto - regressively generate s . Meanwhile , initializing with pretrained weights has been shown to achieve state - of - the - art results on many NLP tasks ( Raffel et al , 2020 ) . In order to adapt T5 for our task , we represent all our inputs as sequences of tokens . We then concatenate these sequences together using separator tokens , truncating and padding them to fixed lengths . This is straightforward since all our inputs are text . See fig . 2 for reference . We also use the standard cross - entropy loss to train .", "entities": [[19, 22, "MethodName", "sequence to sequence"], [154, 155, "DatasetName", "0"], [179, 180, "MethodName", "T5"], [267, 268, "MethodName", "T5"], [325, 326, "MetricName", "loss"]]}
{"text": "We train our model on a subset of \u223c1 , 020 K edits from WikiDocEdits . We use a training / validation / test split of 1 , 000K/10K/10 K edits , and train for 3 epochs with a fixed learning rate of 0.0001 , and a batch size of 128 . We use the T5 - base implementation from Huggingface ( Wolf et al , 2020 ) , and finetune all weights in the model . We validate every 200 steps and select the model with the lowest validation loss .", "entities": [[14, 15, "DatasetName", "WikiDocEdits"], [40, 42, "HyperparameterName", "learning rate"], [47, 49, "HyperparameterName", "batch size"], [55, 56, "MethodName", "T5"], [90, 91, "MetricName", "loss"]]}
{"text": "For inference we use beam search with a beam width of 5 , and keep the 5 highest ranked candidates , excluding any generation that parrots the source as this corresponds to making no edits . Metrics We consider several metrics to evaluate our model . One natural metric to consider is BLEU ( ( Papineni et al , 2002 ) ) . BLEU shows high correlation with human judgement on machine translation ( Papineni et al , 2002 ; Doddington , 2002 ) . While this should not a priori transfer to evaluating different tasks , our task in fact bears a high similarity to machine translation because of how the output is constrained by the inputs . If , for example , the source sentence in an English to German translation task is \" Sally met Lucy \" , the German translation must in some way mention Sally and Lucy . Similarly , in our task , if the source sentence is \" Barack Obama was the 44th President of the United States \" , and the command is \" add birth date \" , the edit must somehow mention a birth date somewhere . Thus , in our setting , BLEU makes sense as a metric since in principle a good model output should not deviate too far from the reference . We use macro - averaged Comment added class of ' 13", "entities": [[52, 53, "MetricName", "BLEU"], [63, 64, "MetricName", "BLEU"], [71, 73, "TaskName", "machine translation"], [106, 108, "TaskName", "machine translation"], [203, 204, "MetricName", "BLEU"]]}
{"text": "Entitled \" It Feels Like Home ( Re Invented ) Tour 2011 \" , it included many remakes of Alliage hits as well as some of his newer songs . sentence - level BLEU with epsilon smoothing and equally weighted n - grams , with n up to 4 . One issue with BLEU is that the source and target sentences in our task are already very similar , so a model that simply parrots back the source sentence could achieve an unduly high score . Therefore , we also evaluate model outputs by comparing the word - level edits made by the model against the reference , where a word - level edit is a tuple of an operation , either insertion or deletion , a position , and a word . For example , in the edit \" Barack Obama was the 44 th President of the United States \" \u2212 \" Barack Obama , born August 4 th 1961 , was the 44 th President of the United States \" , the set of word edits would look like { ( insert , 2 , \" , \" ) , ( insert , 3 , \" born \" ) , ... } . Now , denote the set of word edits between two sentences a and b as WE ( a , b ) . Then , with s the source sentence , s the reference target sentence and h the target sentence generated by the model , we compute the precision P WE ( s , h , s ) = | WE ( s , s ) \u2229 WE ( h , s ) | | WE ( h , s ) | , recall , R WE ( s , h , s ) = | WE ( s , s ) \u2229 WE ( h , s ) | | WE ( s , s ) | , and F1 score , F 1 , WE ( s , h , s ) = 2 P WE R WE P WE + R WE . Finally , we compute sentence - level accuracy , which reports the proportion of edits for which the model output exactly matched the reference . Baselines We use two baselines to compare our model to . First , we consider the parrot baseline that simply outputs the source sentence as is . The second baseline attempts to delete the source sentence and replace it with a new sentence . We use a pretrained GPT - 2 model ( Radford et al , 2019 ) that generates a sentence given the left context .", "entities": [[33, 34, "MetricName", "BLEU"], [35, 36, "HyperparameterName", "epsilon"], [53, 54, "MetricName", "BLEU"], [326, 328, "MetricName", "F1 score"], [359, 360, "MetricName", "accuracy"], [393, 394, "MethodName", "parrot"], [425, 426, "MethodName", "GPT"]]}
{"text": "Table 5 presents our main results . Notice that the parrot baseline is able to achieve a considerably high BLEU score , as expected , while the GPT - 2 baseline surprisingly achieves a high word edit recall score . Our interactive neural editor model is able to beat both baselines across all metrics , as would be expected . Even on a harsh metric like accuracy our model achieves a nontrivial score , although we suspect most of the edits that the model gets exactly right are fluency edits . See table 6 for Comment Added more marriage info .", "entities": [[10, 11, "MethodName", "parrot"], [19, 21, "MetricName", "BLEU score"], [27, 28, "MethodName", "GPT"], [66, 67, "MetricName", "accuracy"]]}
{"text": "They are more frequent than primary brain tumors . Secondary brain tumors are more frequent than primary brain tumors . a breakdown by edit type , and table 4 for example model outputs . Ablations The middle rows of Table 5 show the results for three ablations of our model . The first ablation removes everything but the source sentence s. This is similar to the paraphrase setting ( Gupta et al , 2018 ) , and the editing setting in Faruqui et al ( 2018 ) and Yin et al ( 2018 ) . We can see that including the context , grounding , and command as additional inputs yields significant improvements over only using the source sentence . We can also see from the second ablation that the commands are a crucial element in the model 's performance . This is not surprising since without a command the model must guess what type of edit to make . Similarly , the model without grounding performs considerably worse than the full model , showing that the grounding is equally important as the command . Surprisingly , the last two ablations perform only marginally better than the first , meaning that removing the grounding in addition to the commands , or vice - versa , does not lead to a large drop in performance . This seems to suggest a synergistic effect between the command and the grounding , which makes sense since the model would not know what to do with the grounding without a command , and likewise , the model would not have access to the right information without the grounding , even if it knew what to edit from the command . Breakdown by edit type The results of our full model are broken down by edit intention labels in Table 6 . The columns report the same metrics as in our main table of results , with the exception of S - BLEU , which reports the BLEU score between the source sentence and target , and the last column , which reports the number of test edits that were classified into each category . With the caveat that intention labels come from an automatic classifier and not human annotation , we can observe that our model has varying performance across different types of edits . The model performs very well on fluency edits , but worse on content edits . This comes at no surprise given that fluency ed - its should be easier as they usually correct minor mistakes , which a language model should be able to detect from pretraining . Content edits , on the other hand , require pulling the correct information from the grounding and incorporating it in the correct manner into the sentence . The S - BLEU scores confirm this since the source sentences in the fluency examples are much more similar to the target sentences than for the content edits . In fact , when looking at the absolute improvement of the BLEU over the S - BLEU scores , the model performs equally well on both types of edits .", "entities": [[327, 328, "MetricName", "BLEU"], [332, 334, "MetricName", "BLEU score"], [469, 470, "MetricName", "BLEU"], [506, 507, "MetricName", "BLEU"], [511, 512, "MetricName", "BLEU"]]}
{"text": "We also evaluated the overall quality of model outputs . We considered our full model , and our ablated model that only takes the source sentence as input . We also considered showing and hiding the edit commands , for a total of 4 settings . For a given setting , raters were asked whether they found each of the top 3 model outputs satisfactory . Table 8 presents the results for the top model outputs , with bootstrapped pvalues for pairwise comparisons . We use a Bonferroni corrected \u03b1 = 0.0125 to determine significance . Note that our full model outperforms our ablated model in the first two comparisons . Inter - 9 The high percentage of Neutral judgments here may be partially attributable to other factors . Majority Neutral judgments are observed for approximately 65 % of those examples that received at least one Neutral judgment . This suggests many commands may not be readily interpretable to judges . 10 Appendix E presents some additional automatic metrics to measure the faithfulness of the model to the grounding . estingly , the difference is smaller when the raters are not shown the commands . Additionally , only the ablated model is rated differently depending on whether the commands are shown . This is to be expected since the ablated model is not likely to be faithful to the commands . In addition to reporting the mean scores from the raters , we can also look at the number of examples where at least one of the top model outputs was found satisfactory by human judges ( i.e. scored higher than 3 ) . We find that , when showing the edit commands , at least one of the outputs from our full model was satisfactory in 85.83 % of cases versus 60.17 % for the ablated model .", "entities": [[89, 90, "HyperparameterName", "\u03b1"]]}
{"text": "In addition to human evaluations , we also used automatic metrics to evaluate how faithful our model is to the grounding . BERT Recall Similarly to the coverage analysis in appendix D , we can use R BERT , with the grounding as C , to assess how well each word inserted by the model is supported by the grounding . The only difference is that the model output now replaces the reference target s in the formula for R BERT . Table 14 gives the summary statistics for R BERT across our test set , computed on the outputs of our full model , and the ablated model without grounding . Note that we only consider edits where the model makes at least one insertion . The ablated model serves as a baseline to compare the grounded model to . This baseline achieves a high R BERT score , likely because of spurious matches with the grounding . Nevertheless , our grounded model is still more faithful to the grounding , as expected . Grounding Usage While R BERT attempts to measure how faithful the model is to the grounding ( i.e. is the information inserted by the model found in the grounding ? ) , we can also attempt to measure how much the grounding is used ( i.e. how much of the information inserted by the model is only found in the grounding ? ) . One simple approach is to look at how many words inserted by the model are found in the grounding but not in the rest of the inputs . While this is n't obvious to compute similarities between BERT embeddings , we can use exact word matches instead . For the model without grounding we find that in 30.48 % of edits in the test set ( with at least one insertion ) , at least one of the words inserted by the model is found in the grounding but not in the rest of the inputs . For the full model , this number increases to 48.66 % as expected . The ablated model appears to insert words exclusive to the grounding in a high proportion of edits . However , this could be due to fluency edits , where the model might insert a functional word that happens to only appear in the grounding . If we restrict our attention to content edits , as defined in section 3.2 , the ablated model inserts grounding - exclusive words in only 36.85 % of edits , and 65.40 % for the full model .", "entities": [[22, 23, "MethodName", "BERT"], [23, 24, "MetricName", "Recall"], [37, 38, "MethodName", "BERT"], [80, 81, "MethodName", "BERT"], [90, 91, "MethodName", "BERT"], [147, 148, "MethodName", "BERT"], [179, 180, "MethodName", "BERT"], [276, 277, "MethodName", "BERT"]]}
{"text": "This section describes our pipeline to obtain atomic edits from Wikipedia revisions in more detail . We start by filtering the revisions in the data . In particular , following ( Zhang et al , 2019b ) , we only keep revisions that affect a single section , and we exclude revisions that do not contain an editor comment . We also exclude certain page types like talk or user pages . We then strip the Wikipedia markup in the retrieved text , using the WikiExtractor script ( Attardi , 2015 ) . This removes most markup and Wikimedia templates from the text . Because the markup language used on Wikipedia is not completely formalized 11 , and because malformed markup often appears in intermediate versions of Wikipedia pages , there is no guarantee that we can remove all the markup from the text . We then split each section into sentences using the Punkt sentence tokenizer ( Kiss and Strunk , 2006 ) provided in the NLTK python package ( Bird et al , 2009 ) . After splitting into sentences , we attempt to match the sentences from the pre - edit ( source ) document to the sentences in the post - edit ( target ) document . Unmatched sentences will be candidates for edits . Similarly to ( Faruqui et al , 2018 ) , for each sentence s i in the source document , we only look at the target sentences { t i\u2212k , ... , t i , ... , t i+k } , with k = 20 . This avoids the quadratic complexity of looking at all matches . We then filter out revisions that contain more than one sentence - level edit to ensure that the comment is relevant . If there is a single unmatched source , respectively target , sentence , we consider it a sentence deletion , respectively insertion . Because we do not look at all matches between source and target sentences , a sentence may remain unmatched if , in the target document , it was moved more than k sentences away compared to the source document . Thus we only keep a sentence insertion or deletion if the total number of source and target sentences differ by one . If there are both an unmatched source sentence s and target sentence t , we consider them to form an edit s \u2212 t if f ( s , t ) > , where f is the BLEU score and = 0.1 . As a final step , we filter out edits that involve sentences with markup punctuation . We have", "entities": [[262, 264, "HyperparameterName", "k ="], [421, 423, "MetricName", "BLEU score"]]}
{"text": "We employ the Transformer model implemented in the Sockeye toolkit . The number of layer in both the encoder and decoder is set to N=6 , the number of head is set to 8 , and the number of hidden unit in the feed - forward network is set to 1024 . We use an embedding size of both the source and target words of 512 dimension , and use a batch size of 128 sentences . The maximum sentence length is set to 100 tokens with 0.1 label smoothing . We apply layer normalization and add dropout to the embedding and transformer layers with 0.1 probability . Moreover , we use the Adam optimizer ( Kingma and Ba , 2015 ) with an initial learning rate of 0.0002 , and save the checkpoint every 1500 updates . Model training process stops after 8 checkpoints without improvements on the validation perplexity . Following Niu et al ( 2018a ) , we select the 4 best checkpoint based on the validation perplexity values and combine them in a linear ensemble for decoding . Decoding is performed by using beam search with a beam size of 5 . We evaluate the machine translation performance by using the case - sensitive BLEU score ( Papineni et al , 2002 ) with standard tokenization .", "entities": [[3, 4, "MethodName", "Transformer"], [71, 73, "HyperparameterName", "batch size"], [88, 90, "MethodName", "label smoothing"], [93, 95, "MethodName", "layer normalization"], [113, 114, "MethodName", "Adam"], [114, 115, "HyperparameterName", "optimizer"], [125, 127, "HyperparameterName", "learning rate"], [150, 151, "MetricName", "perplexity"], [170, 171, "MetricName", "perplexity"], [199, 201, "TaskName", "machine translation"], [208, 210, "MetricName", "BLEU score"]]}
{"text": "Table 4 shows the BLEU scores of the general NMT model and baseline NMT model on machine translation task . We can observe that the baseline NMT model is comparable to the general NMT model , and it achieves the highest BLEU scores on almost all the test datasets in both directions , which indicates that the NMT baseline based on our proposed segmentation method is competitive .", "entities": [[4, 5, "MetricName", "BLEU"], [16, 18, "TaskName", "machine translation"], [41, 42, "MetricName", "BLEU"]]}
{"text": "Table 5 shows the BLEU scores of the baseline NMT model , bi - directional NMT model , and multi - task neural model on the machine translation task between Turkish and English . The table shows that the multi - task neural model outperforms both the baseline NMT model and bi - directional NMT model , and it achieves the highest BLEU scores on almost all the test datasets in both directions , which suggests that the multi - task neural model is capable of improving the bi - directional translation quality on agglutinative languages . The main reason is that compared with the bi - directional NMT model , our proposed multi - task neural model additionally employs the stemming task for the agglutinative language , which is effective for the NMT model to learn both the source - side semantic information and the target - side language modeling .", "entities": [[4, 5, "MetricName", "BLEU"], [26, 28, "TaskName", "machine translation"], [62, 63, "MetricName", "BLEU"]]}
{"text": "The university was emulating its lives . multi - task The university was imitating life . The function of epochs and perplexity values on the validation dataset in different neural translation models are shown in Figure 3 . We can see that the perplexity values are consistently lower on the multi - task neural model , and it converges rapidly . Table 6 shows a translation example for the different models on Turkish - English . We can see that the translation result of the multi - task neural model is more accurate . The Turkish word \" taklit \" means \" imitate \" in English , both the baseline NMT and bi - directional NMT translate it into a synonym \" emulate \" . However , they are not able to express the meaning of the sentence correctly . The main reason is that the auxiliary task of stemming forces the proposed model to focus more strongly on the core meaning of each word ( or stem ) , therefore helping the model make the correct lexical choices and capture the in - depth semantic information .", "entities": [[21, 22, "MetricName", "perplexity"], [25, 27, "DatasetName", "validation dataset"], [43, 44, "MetricName", "perplexity"]]}
{"text": "Moreover , we evaluate the multi - task neural model on using external monolingual data for Turkish stemming task . We employ the parallel sentences and the monolingual data in a 1 - 1 ratio , and shuffle them randomly before each training epoch . More details about the data are shown below : Table 7 shows the BLEU scores of the proposed multi - task neural model on using different external monolingual data . We can see that there is no obvious difference on Turkish - English translation performance by using different monolingual data , whether the data is in - domain or out - of - domain to the test dataset . However , for the English - Turkish machine translation task , which can be seen as agglutinative language generation task , using the mixed data of talks and news achieves further improvements of the BLEU scores on almost all the test datasets . The main reason is that the proposed multi - task neural model incorporates many morphological and linguistic information of Turkish rather than that of English , which mainly pays attention to the source - side representation ability on agglutinative languages rather than the target - side language modeling . We also evaluate the translation performance of the general NMT model , baseline NMT model , and multi - task neural model with external news data on the machine translation task between Uyghur and Chinese . The experimental results are shown in Table 8 . The results indicate that the multi - task neural model achieves the highest BLEU scores on the test dataset by utilizing external monolingual data for the stemming task on Uyghur sentences .", "entities": [[58, 59, "MetricName", "BLEU"], [121, 123, "TaskName", "machine translation"], [148, 149, "MetricName", "BLEU"], [234, 236, "TaskName", "machine translation"], [264, 265, "MetricName", "BLEU"]]}
{"text": "In this paper , we propose a multi - task neural model for translation task from and into a low - resource and morphologically - rich agglutinative language . The model jointly learns to perform bi - directional translation and agglutinative language stemming by utilizing the shared encoder and decoder under standard NMT framework . Extensive experimental results show that the proposed model is beneficial for the agglutinative language machine translation , and only a small amount of the agglutinative data can improve the translation performance in both directions . Moreover , the proposed approach with external monolingual data is more useful for translating into the agglutinative language , which achieves an improvement of +1.42 BLEU points for translation from English into Turkish and +1.45 BLEU points from Chinese into Uyghur . In future work , we plan to utilize other word segmentation methods for model training . We also plan to combine the proposed multi - task neural model with back - translation method to enhance the ability of the NMT model on target - side language modeling .", "entities": [[69, 71, "TaskName", "machine translation"], [115, 116, "MetricName", "BLEU"], [125, 126, "MetricName", "BLEU"]]}
{"text": "To study negative interference , we compare multilingual models with monolingual baselines . Without loss of generality , we focus on analyzing bilingual models to minimize confounding factors . For two languages lg 1 and lg 2 , we pretrain a single bilingual model and two monolingual models . We then assess their performance on downstream tasks using two different settings . To examine negative interference , we evaluate both monolingual and multilingual models using the withinlanguage monolingual setting , such that the pretrained model is finetuned and tested on the same language . For instance , if the monolingual model of lg 1 outperforms the bilingual model on lg 1 , we know that lg 2 induces negative impact on lg 1 in the bilingual model . Besides , since multilingual models are trained to enable cross - lingual transfer , we also report their performance on the zero - shot cross - lingual transfer setting , where the model is only finetuned on the source language , say lg 1 , and tested on the target language lg 2 . We hypothesize that the following factors play important roles in causing negative interference and study each individually : guages , we hypothesize that it can also occur for languages with less resources . We study the impact of training data size per language on negative interference . We subsample a high - resource language , say lg 1 , to create a \" low - resource version \" . We then retrain the monolingual and bilingual models and compare with results of their high - source counterparts . Particularly , we test if reducing lg 1 's training size also reduces negative interference on lg 2 . Language Similarity Language similarity has been shown important for effective transfer in multilingual models . shows that bilingual models trained on more similar language pairs result in better zero - shot transfer performance . We thus expect it to play a critical role in negative interference as well . For a specific language lg 1 , we pair it with languages that are closely and distantly related . We then compare these bilingual models ' performance on lg 1 to investigate if more similar languages cause less severe interference . In addition , we further add a third language lg 3 that is similar to lg 1 and train a trilingual model on lg 1 - lg 2 - lg 3 . We compare the trilingual model with the bilingual model to examine if adding lg 3 can mitigate negative interference on lg 1 .", "entities": [[14, 15, "MetricName", "loss"], [137, 141, "TaskName", "cross - lingual transfer"], [149, 156, "TaskName", "zero - shot cross - lingual transfer"]]}
{"text": "State - of - the - art multilingual models aim to share as many parameters as possible in the hope of learning a languageuniversal model for all languages . While prior studies measure the latent embedding similarity between languages , we instead examine model parameters directly . The idea is to test whether model parameters are language - universal or language - specific . To achieve this , we prune multilingual models for each language using relaxed L 0 norm regularization ( Louizos et al , 2017 ) , and compare parameter similarities between languages . Formally , for a model f ( ; \u03b8 ) parameterized by \u03b8 = { \u03b8 i } n i=1 where each \u03b8 i represents an individual parameter or a group of parameters , the method introduces a set of binary masks z , drawn from some distribution q ( z | \u03c0 ) parametrized by \u03c0 , and learns a sparse model f ( ; \u03b8 z ) by optimizing : min \u03c0 E q ( z | \u03c0 ) 1 N N i=1 L ( f ( x i ; \u03b8 ) , y i ) + \u03bb \u03b8 0 s.t.\u03b8 = \u03b8 z , ( 1 ) where is the Hadamard ( elementwise ) product , L ( ) is some task loss and \u03bb is a hyper - parameter . We follow the work of ( Louizos et al , 2017 ) and use the Hard Concrete distribution for the binary mask z , such that the above objective is fully differentiable . Then , for each bilingual model , we freeze its pretrained parameter weights and learn binary masks z for each language independently . As a result , we obtain two independent sets of mask parameters \u03c0 which can be used to determine parameter importance . Intuitively , for each parameter group , it is language - universal if both languages consider it important ( positive \u03c0 values ) . On the other hand , if one language assigns positive value while the other assigns negative , it shows that the parameter group is language - specific . We compare them across languages and layers to analyze parameter similarity in multilingual models .", "entities": [[78, 79, "DatasetName", "0"], [104, 105, "HyperparameterName", "\u03b8"], [108, 109, "HyperparameterName", "\u03b8"], [111, 112, "HyperparameterName", "\u03b8"], [118, 119, "HyperparameterName", "\u03b8"], [162, 163, "HyperparameterName", "\u03b8"], [188, 189, "HyperparameterName", "\u03b8"], [196, 197, "HyperparameterName", "\u03b8"], [197, 198, "DatasetName", "0"], [200, 201, "HyperparameterName", "\u03b8"], [221, 222, "MetricName", "loss"]]}
{"text": "We focus on standard multilingual masked language modeling ( MLM ) used in mBERT and XLM . We first pretrain models and then evaluate their performance on four NLP benchmarks . For pretraining , we mainly follow the setup and implementation of XLM ( Lample and Conneau , 2019 ) . We focus on monolingual and bilingual models for a more controllable comparison , which we refer to as Mono and JointPair respectively . In particular , we always include English ( En ) in bilingual models to compare on zero - shot transfer settings with prior work . Besides , we consider three adpt 86.8 86.7 84.3 88.6 86.1 76.0 84.8 89.3 76.4 93.5 95.2 88.2 88 high - resource languages { Arabic ( Ar ) , French ( Fr ) , Russian ( Ru ) } and three low - resource languages { Hindi ( Hi ) , Swahili ( Sw ) , Telugu ( Te ) } ( see Table 1 for their statistics ) . We choose these six languages based their data availability in downstream tasks . We use Wikipedia as training data with statistics shown in Table 1 . For each model , we use BPE ( Sennrich et al , 2016 ) to learn 32k subword vocabulary shared between languages . For multilingual models , we sample language proportionally to P i = ( L i j L j ) 1 T , where L i is the size of the training corpus for i - th language pair and T is the temperature . Each model is a standard Transformer ( Vaswani et al , 2017 ) with 8 layers , 12 heads , 512 embedding size and 2048 hidden dimension for the feedforward layer . Notice that we specifically consider a smaller model capacity to be comparable with existing models with larger capacity but also include much more ( over 100 ) languages . We use the Adam optimizer ( Kingma and Ba , 2014 ) and exploit the same learning rate schedule as Lample and Conneau ( 2019 ) . We train each model with 4 NVIDIA V100 GPUs with 32 GB of memory . Using mixed precision , we fit a batch of 128 for each GPU and the total batch size is 512 . Each epoch contains 10k steps and we train for 50 epochs . For evaluation , we consider four downstream tasks : named entity recognition ( NER ) , part - ofspeech tagging ( POS ) , question answering ( QA ) , and natural language inference ( NLI ) . ( See Appendix A for finetuning details . )", "entities": [[5, 8, "TaskName", "masked language modeling"], [9, 10, "DatasetName", "MLM"], [13, 14, "MethodName", "mBERT"], [15, 16, "MethodName", "XLM"], [42, 43, "MethodName", "XLM"], [201, 202, "MethodName", "BPE"], [267, 268, "MethodName", "Transformer"], [286, 287, "DatasetName", "2048"], [326, 327, "MethodName", "Adam"], [327, 328, "HyperparameterName", "optimizer"], [339, 341, "HyperparameterName", "learning rate"], [381, 383, "HyperparameterName", "batch size"], [407, 410, "TaskName", "named entity recognition"], [411, 412, "TaskName", "NER"], [422, 424, "TaskName", "question answering"], [429, 432, "TaskName", "natural language inference"]]}
{"text": "We use the WikiAnn ( Pan et al , 2017 ) dataset , which is a sequence labelling task built automatically from Wikipedia . A linear layer with softmax classifier is added on top of pretrained models to predict the label for each word based on its first subword . We report the F1 score .", "entities": [[3, 4, "DatasetName", "WikiAnn"], [25, 27, "MethodName", "linear layer"], [28, 29, "MethodName", "softmax"], [53, 55, "MetricName", "F1 score"]]}
{"text": "Similar to NER , POS is also a sequence labelling task but with a focus on synthetic knowledge . In particular , we use the Universal Dependencies treebanks ( Nivre et al , 2018 ) . Task - specific layers are the same and we report F1 , as in NER .", "entities": [[2, 3, "TaskName", "NER"], [25, 27, "DatasetName", "Universal Dependencies"], [46, 47, "MetricName", "F1"], [50, 51, "TaskName", "NER"]]}
{"text": "We choose to use the TyDiQA - GoldP dataset ( Clark et al , 2020 ) that covers typologically diverse languages . Similar to popular QA dataset such as SQuAD ( Rajpurkar et al , 2018 ) , this is a span prediction task where task - specific linear classifiers are used to predict start / end positions of the answer . Standard metrics of F1 and Exact Match ( EM ) are reported . NLI XNLI ( Conneau et al , 2018 ) is probably the most popular cross - lingual benchmark . Notice that the original dataset only contains training data for English . Consequently , we only evaluate this task on the zero - shot transfer setting while we consider both settings for the rest of other tasks .", "entities": [[5, 8, "DatasetName", "TyDiQA - GoldP"], [29, 30, "DatasetName", "SQuAD"], [65, 66, "MetricName", "F1"], [67, 69, "MetricName", "Exact Match"], [70, 71, "MetricName", "EM"], [76, 77, "DatasetName", "XNLI"]]}
{"text": "In Table 2 and 3 gual models outperform bilingual models for all languages except Swahili on all three tasks . In fact , monolingual models even perform better than XLM on four out of six languages including hi and te , despite that XLM is much larger in model sizes and trained with much more resources . This shows that negative interference can occur on low - resource languages as well . While the negative impact is expected to be more prominent on high - resource languages , we demonstrate that it may occur for languages with resources fewer than commonly believed . The existence of negative interference confirms that state - of - the - art multilingual models can not generalize equally well on all languages , and there is still a gap compared to monolingual models on certain languages . We next turn to dissect negative interference by studying the four factors described in Section 3.1 . Training Corpus Size By comparing the validation perplexity on Swahili and Telugu in Figure 2 , we find that while both monolingual models outperform bilingual models in the first few epochs , the Swahili model 's perplexity starts to increase and is eventually surpassed by the bilingual model in later epochs . This matches the intuition that monolingual models may overfit when training data size is small . To verify this , we subsample French and Russian to 100k sentences to create a \" low - resource version \" of them ( denoted as fr l /ru l ) . As shown in lingual models for fr l /ru l , in contrast for fr / ru . This suggests that multilingual models can stimulate positive transfer for low - resource languages when monolingual models overfit . On the other hand , when we compare bilingual models on English , models trained using different sizes of fr / ru data obtain similar performance , indicating that the training size of the source language has little impact on negative interference on the target language ( English in this case ) . While more training data usually implies larger vocabulary and more diverse linguistic phenomena , negative interference seems to arise from more fundamental conflicts contained in even small training corpus . Language Similarity As illustrated by Table 5 , the in - language performance on English drops as the paired language becomes more distantly related ( French vs Russian ) . This verifies that transferring from more distant languages results in more severe negative interference . It is therefore natural to ask if adding more similar languages can mitigate negative interference , especially for low - resource languages . We then train two trilingual models , adding Marathi to English - Hindi , and Kannada to English - Telugu . Compared to their bilingual counterparts ( Table 4 ) , trilingual models obtain similar within - language performance , which indicates that adding similar languages can not mitigate negative interference . \" En - En \" refers to gradients of two English batches within the Ar - En model , while \" Ar - En \" and \" Fr - En \" refer to gradients of two batches , one from each language , within Ar - En and Fr - En models respectively . However , they do improve zero - shot cross - lingual performance . One possible explanation is that even similar languages can fight for language - specific capacity but they may nevertheless benefit the generalization of the shared knowledge . Gradient Conflict In Figure 3 , we plot the gradient cosine similarity between Arabic - English and French - English in their corresponding bilingual models over the first 25 epochs . We also plot the similarity within English , measured using two independently sampled batches 2 . Specifically , gradients between two different languages are indeed less similar than those within the same language . The gap is more evident in the early few epochs , where we observe negative gradient similarities for Ar - En and Fr - En while those for En - En are positive . In addition , gradients in Ar - En are less similar than those in Fr - En , indicating that distant language pair can cause more severe gradient conflicts . These results confirm that gradient conflict exists in multilingual models and is correlated to per language performance , suggesting it may introduce optimization challenge that results in negative interference .", "entities": [[29, 30, "MethodName", "XLM"], [43, 44, "MethodName", "XLM"], [166, 167, "MetricName", "perplexity"], [195, 196, "MetricName", "perplexity"]]}
{"text": "The existence of gradient 2 Notice that we use gradient accumulation to sample an effectively larger batch of 4096 sentences to calculate the gradient similarity . conflicts may imply that languages are fighting for capacity . Thus , we next study how languageuniversal these multilingual parameters are . Figure 4a shows the cosine similarity of mask parameters \u03c0 across different layers . We observe that within - language similarity ( En - En ) is near perfect , which validates the pruning method 's robustness . The trend shows that model parameters are better shared in the bottom layers than the upper ones . Besides , it also demonstrates that parameters in multi - head attention layers obtain higher similarities than those in feedforward layers , suggesting that attention mechanism might be more languageuniversal . We additionally inspect \u03c0 parameters with the highest absolute values and plot those values for Ar ( Figure 4b ) , together with their En counterparts . A more negative value indicates that the parameter is more likely to be pruned for that language and vice versa . Interestingly , while many parameters with positive values ( on the right ) are language - universal as both languages assign very positive values , parameters with negative values ( on the left ) are mostly language - specific for Ar as En assigns positive values . We observe similar patterns for other languages as well . These results demonstrate that language - specific parameters do exist in multilingual models . Having language - specific capacity in shared parameters is sub - optimal . It is less transferable and thus can hinder cross - lingual performance . Moreover , it may also take over capacity budgets for other languages and degrade their within - language performance , i.e. , causing negative interference . A natural next question is whether explicitly adding language - specific capacity into multilingual models can alleviate negative interference . We thus train variants of bilingual models that contain language - specific components for each language . Particularly , we consider adding language - specific feedforward layers , attention layers , and residual adapter layers ( Rebuffi et al , 2017 ; Houlsby et al , 2019 ) , denoted as ffn , attn and adpt respectively . For each type of component , we create two separate copies in each Transformer layer , one designated for each language , while the rest of the network remains unchanged . As shown in Table 2 and 3 , adding language - specific capacity does mitigate negative interference and improve monolingual performance . We also find that language - specific feedforward layers obtain larger performance gains compared to attention layers , consistent with our prior analysis . However , these gains come at a cost of cross - lingual transferability , such that their zeroshot performance drops tremendously . Our results suggest a tension between addressing interference versus improving transferability . In the next section , we investigate how to address negative interference in a manner that can improve performance on both within - language tasks and cross - lingual benchmarks . 4 Mitigating Negative Interference via Meta Learning", "entities": [[112, 116, "MethodName", "multi - head attention"], [354, 356, "HyperparameterName", "attention layers"], [397, 398, "MethodName", "Transformer"], [452, 454, "HyperparameterName", "attention layers"]]}
{"text": "In the previous section , we demonstrated that while explicitly adding language - specific components can alleviate negative interference , it can also hinder cross - lingual transferability . We notice that a critical shortcoming of language - specific capacity is that they are agnostic of the rest of other languages , since by design they are trained on the designated language only . They are thus more likely to overfit and can induce optimization challenges for shared capacity as well . Inspired by recent work in meta learning ( Flennerhag et al , 2019 ) that utilizes meta parameters to improve gradient geometry of the base network , we propose a novel meta - learning formulation of multilingual models that exploits language - specific parameters to i m - prove generalization of shared parameters . For a model with some predefined languagespecific parameters \u03c6 = { \u03c6 i } L i=1 , where \u03c6 i is designated for the i - th language , and shared parameters \u03b8 , our solution is to treat \u03c6 as meta parameters and \u03b8 as base parameters . Ideally , we want \u03c6 to store non - transferable language - specific knowledge to resolve conflicts and improve generalization of \u03b8 in all languages ( a.k.a . mitigate negative interference and improve cross - lingual transferability ) . Therefore , we train \u03c6 based on the following principle : if \u03b8 follows the gradients on training data for a given \u03c6 , the resulting \u03b8 should obtain a good validation performance on all languages . This implies a bilevel optimization problem ( Colson et al , 2007 ) formally written as : min \u03c6 1 L L i=1 L i val ( \u03b8 * , \u03c6 i ) s.t . \u03b8 * = arg min \u03b8 1 L L i=1 L i train ( \u03b8 , \u03c6 i ) , ( 2 ) where L i val and L i train denote the training and the validation MLM loss for the i - th language . Since directly solving this problem can be prohibitive due to the expensive inner optimization , we approximate \u03b8 * by adapting the current \u03b8 ( t ) using a single gradient step , similar to techniques used in prior meta - learning methods ( Finn et al , 2017 ) . This results in a two - phase iterative training process shown in Algorithm 1 ( See Appendix B ) . To be specific , at each training step t on the ith language during pretraining , we first adapt a gradient step on \u03b8 to obtain a new \u03b8 and update \u03c6 i Update language - specific parameters as : \u03c6 ( t+1 ) i GradientUpdate ( \u03c6 ( t ) i , \u2207 \u03c6 ( t ) i 1 L L j=1 L j val ( \u03b8 ( t ) i \u2212 \u03b2\u2207 \u03b8 ( t ) L i train ( \u03b8 ( t ) , \u03c6 ( t ) i ) , \u03c6 ( t ) j ) ) 7 : Update shared parameters as : t ) , \u03c6 ( t+1 ) ) ) 8 : end while based on the \u03b8 's validation MLM loss : \u03b8 ( t+1 ) GradientUpdate ( \u03b8 ( t ) , \u2207 \u03b8 ( t ) L train ( \u03b8 ( \u03c6 ( t+1 ) i = \u03c6 ( t ) i \u2212 \u03b1\u2207 \u03c6 ( t ) i 1 L L j=1 L j val ( \u03b8 , \u03c6 ( t ) j ) \u03b8 = \u03b8 ( t ) \u2212 \u03b2\u2207 \u03b8 ( t ) L i train ( \u03b8 ( t ) , \u03c6 ( t ) i ) , ( 3 ) where \u03b1 and \u03b2 are learning rates . Notice that \u03b8 is a function of \u03c6 ( t ) i and thus this optimization requires computing the gradient of gradient . Particularly , by applying chain rule to the gradient of \u03c6 ( t ) i , we can observe that it contains a higher - order term : \u2207 2 \u03c6 ( t ) i , \u03b8 ( t ) L i train ( \u03b8 ( t ) , \u03c6 ( t ) i ) \u2207 \u03b8 1 L L j=1 L j val ( \u03b8 , \u03c6 ( t ) j ) ( 4 ) This is important , since it shows that \u03c6 i can obtain information from other languages through higherorder gradients . In other words , language - specific parameters are not agnostic of other languages anymore without violating the language - specific requirement . This is because , in Eq . 3 , while \u2207 \u03b8 ( t ) is based on the i - th language only , the validation loss is computed for all languages . Finally , in the second phase , we update \u03b8 based on the new \u03c6 ( t+1 ) : \u03b8 ( t+1 ) = \u03b8 ( t ) \u2212 \u03b2\u2207 \u03b8 ( t ) L train ( \u03b8 ( t ) , \u03c6 ( t+1 ) ) ( 5 )", "entities": [[82, 83, "DatasetName", "Inspired"], [113, 116, "TaskName", "meta - learning"], [168, 169, "HyperparameterName", "\u03b8"], [180, 181, "HyperparameterName", "\u03b8"], [206, 207, "HyperparameterName", "\u03b8"], [236, 237, "HyperparameterName", "\u03b8"], [250, 251, "HyperparameterName", "\u03b8"], [288, 289, "HyperparameterName", "\u03b8"], [296, 297, "HyperparameterName", "\u03b8"], [301, 302, "HyperparameterName", "\u03b8"], [310, 311, "HyperparameterName", "\u03b8"], [333, 334, "DatasetName", "MLM"], [334, 335, "MetricName", "loss"], [359, 360, "HyperparameterName", "\u03b8"], [365, 366, "HyperparameterName", "\u03b8"], [381, 384, "TaskName", "meta - learning"], [436, 437, "HyperparameterName", "\u03b8"], [441, 442, "HyperparameterName", "\u03b8"], [480, 481, "HyperparameterName", "\u03b8"], [487, 488, "HyperparameterName", "\u03b8"], [495, 496, "HyperparameterName", "\u03b8"], [537, 538, "HyperparameterName", "\u03b8"], [540, 541, "DatasetName", "MLM"], [541, 542, "MetricName", "loss"], [543, 544, "HyperparameterName", "\u03b8"], [549, 550, "HyperparameterName", "\u03b8"], [555, 556, "HyperparameterName", "\u03b8"], [562, 563, "HyperparameterName", "\u03b8"], [590, 591, "HyperparameterName", "\u03b8"], [598, 599, "HyperparameterName", "\u03b8"], [600, 601, "HyperparameterName", "\u03b8"], [606, 607, "HyperparameterName", "\u03b8"], [614, 615, "HyperparameterName", "\u03b8"], [630, 631, "HyperparameterName", "\u03b1"], [632, 633, "HyperparameterName", "\u03b2"], [639, 640, "HyperparameterName", "\u03b8"], [696, 697, "HyperparameterName", "\u03b8"], [704, 705, "HyperparameterName", "\u03b8"], [716, 717, "HyperparameterName", "\u03b8"], [725, 726, "HyperparameterName", "\u03b8"], [790, 791, "HyperparameterName", "\u03b8"], [806, 807, "MetricName", "loss"], [822, 823, "HyperparameterName", "\u03b8"], [832, 833, "HyperparameterName", "\u03b8"], [837, 838, "HyperparameterName", "\u03b8"], [843, 844, "HyperparameterName", "\u03b8"], [850, 851, "HyperparameterName", "\u03b8"]]}
{"text": "While our method is generic , we evaluate it applied on bilingual models with adapter networks . Adapters have been effectively utilized in multilingual models , and we choose them for practical consideration of limiting perlanguage capacity . Unlike prior works that finetune adapters for adaptation , we train them jointly with shared parameters during pretraining . We follow Houlsby et al ( 2019 ) and insert language - specific adapters after attention and feedforward layers . We leave a more thorough investigation of how to better pick language - specific structures for future work . For downstream task evaluation , we finetune all layers . Notice that computing the gradient of gradient in Eq . 3 doubles the memory requirement . In practice , we utilize the finite difference approximation ( Appendix B ) . By evaluating their performance on the zeroshot transfer settings ( Table 2 , 3 and 6 ) , we observe that our method , denoted as meta adpt , consistently improves the performance over JointPair baselines , while ordinary adapters ( adpt ) perform worse than JointPair . This shows that , the proposed method can effectively utilize the added language - specific adapters to improve generalization of shared parameters across languages . At the same time , our method also mitigates negative interference and outperforms JointPair on withinlanguage performance , closing the gap with monolingual models . In particular , it performs better than ordinary adapters in both settings . We hypothesize that this is because it alleviates language conflicts during training and thus converges more robustly . For example , we plot training loss in the early stage in Figure 4c , which shows that ordinary adapters converge slower than JointPair due to overfitting of language - specific adapters while meta adapters converge much faster . For ablation studies , we also report results for JointPair trained with adapters shared between two languages , denoted as share adpt . Unlike language - specific adapters that can hinder transferability , shared adapters improve both within - language and cross - lingual performance with the extra capacity . However , meta adapters still obtain better performance . These results show that mitigating negative interference can improve multilingual representations .", "entities": [[271, 272, "MetricName", "loss"]]}
{"text": "Notice that XNLI only has training data in available in English so we only evaluate zero - shot crosslingual performance on it . Following ( Hu et al , 2020 ) , we finetune the model for 10 epochs for NER and POS , 2 epochs for QA and 200 epochs for XNLI . For NER , POS and QA , we search the following hyperparameters : batch size { 16 , 32 } ; learning rate { 2e - 5 , 3e - 5 , 5e - 5 } . We use English dev set for zero - shot cross - lingual setting and the target language dev set for within - language monolingual setting . For XNLI , we search for : batch size { 4 , 8 } ; encoder learning rate { 1e - 6 , 5e - 6 , 2e - 5 } ; classifier learning rate { 5e - 6 , 2e - 5 , 5e - 5 } . For models with language - specific components , we test freezing these components or finetuning them together . We discover that finetuning the whole network always yields better results . For all experiments , we save checkpoint after each epoch .", "entities": [[2, 3, "DatasetName", "XNLI"], [40, 41, "TaskName", "NER"], [52, 53, "DatasetName", "XNLI"], [55, 56, "TaskName", "NER"], [67, 69, "HyperparameterName", "batch size"], [75, 77, "HyperparameterName", "learning rate"], [118, 119, "DatasetName", "XNLI"], [124, 126, "HyperparameterName", "batch size"], [133, 135, "HyperparameterName", "learning rate"], [150, 152, "HyperparameterName", "learning rate"]]}
{"text": "Let z i be the output of the i - th layer of dimension d. The residual adapter network ( Rebuffi et al , 2017 ; Houlsby et al , 2019 ; ) is a bottleneck layer that first projects z i to an inner layer with dimension b : h i = g ( W z i z i ) ( 6 ) where W z i R d\u00d7b and g is some activation function such as relu . It is then projected back to the original input dimension d with a residual connection : o i = W h i h i + z i ( 7 ) where W h i R b\u00d7d . In our experiments , we fix b = 1 4 d. For a bilingual model of lg 1 and lg 2 , we inject two langauge - specific adapters after each attention and feedforward layer , one for each language . For example , if the input text is in lg 1 , the network will be routed to adapters designated for lg 1 . The rest of the network and training protocol remain unchanged . The injected adapter layers mimic the warp layers interleaved between base network layers in Flennerhag et al ( 2019 ) . Warp layers are meta parameters that aim to improve the performance of the base network . They precondition base network gradients to obtain better gradient geometry . In our experiments , we treat language - specific adapters as meta parameters to improve generalization of the shared network . The algorithm is outlined in Algorithm 1 . The adapters are updated according to Eq 3 , which doubles the memory requirement . In particular , the high - order term in Eq 4 requires computing the gradient of gradient . In practice , we approximate this term using the finite difference approximation as : \u2207 \u03c6 ( t ) i L i train ( \u03b8 + , \u03c6 ( t ) i ) \u2212 \u2207 \u03c6 ( t ) i L i train ( \u03b8 \u2212 , \u03c6 ( t ) i ) 2 ( 8 ) where \u03b8 \u00b1 = \u03b8 ( t ) \u00b1 \u2207 \u03b8 1 L L j=1 L j val ( \u03b8 , \u03c6 ( t ) j ) and is a small scalar . We use the same value for learning rates \u03b1 and \u03b2 in Eq 3 , to be consistent with standard learning rate schedule used in XLM ( Lample and Conneau , 2019 ) .", "entities": [[74, 76, "HyperparameterName", "activation function"], [78, 79, "MethodName", "relu"], [93, 95, "MethodName", "residual connection"], [327, 328, "HyperparameterName", "\u03b8"], [347, 348, "HyperparameterName", "\u03b8"], [361, 362, "HyperparameterName", "\u03b8"], [364, 365, "HyperparameterName", "\u03b8"], [370, 371, "HyperparameterName", "\u03b8"], [379, 380, "HyperparameterName", "\u03b8"], [401, 402, "HyperparameterName", "\u03b1"], [403, 404, "HyperparameterName", "\u03b2"], [413, 415, "HyperparameterName", "learning rate"], [418, 419, "MethodName", "XLM"]]}
{"text": "The goal of Semantic Text Similarity ( STS ) is to find the degree of overlap in the meaning of two pieces of text . This ranges from text fragments that are exact semantic equivalents , to others that have no semantic relation . STS has a wide variety of applications , including text summarisation ( Aliguliyev , 2009 ) , machine translation ( Kauchak and Barzilay , 2006 ) , and search optimisation ( Sriram et al , 2010 ) . The STS task , which has been set by the SemEval conference for the past number of years ( Agirre et al , 2014 ; Agirre et al , 2015 ) , requires that submitted systems assign a score between 0 ( the sentences are on different topics ) and 5 ( the sentences mean exactly the same thing ) that reflects how similar two sentences are . Most systems that tackled SemEval 's STS task in previous years have involved three main approaches : The first is text alignment , based on the content words ' meaning ( Sultan et al , 2015 ; Sultan et al , 2014b ) . The second represents text as vectors , which are used to find the similarity score using a vector similarity metric ( such as cosine ) . Third , machine learning approaches are used to compute multiple lexical , semantic , and syntactic features to classify each sentence pair 's similarity . We make use of both text alignments and vector representations , while limiting comparisons to words of the same Type ( Section 4.1 ) , a novel concept we introduce in addition to methods of phrase linking ( Section 4.2 ) and establishing common noun importance ( Section 4.3 ) . These , combined with several different weight combinations we pick for each word Type , provide us with 8 , 370 semantic relation measures ( Section 5 ) . The overall algorithm for generating the several similarity measures is presented in Algorithm 1 . We choose a subset of these measures using methods detailed in Section 6.1 , combine them with a limited set of features and use Support Vector Regression and Kernel Ridge Regression to generate a Similarity Score ( Section 6.2 ) . Our approach also handles definitions separately from arbitrary sentences , as we observed that their structure is significantly different . Since the test data provided this year did not contain a definition data set , this paper focuses on our generic approach , with definition similarity discussed briefly in Section 7 .", "entities": [[4, 6, "TaskName", "Text Similarity"], [7, 8, "TaskName", "STS"], [44, 45, "TaskName", "STS"], [61, 63, "TaskName", "machine translation"], [83, 84, "TaskName", "STS"], [122, 123, "DatasetName", "0"], [156, 157, "TaskName", "STS"], [375, 376, "MetricName", "Score"]]}
{"text": "Algorithm 1 provides an overview of the system we use to generate the various Similarity Scores , We call each combination that generates a score a \" Method \" . We use thirty weights for Types 3 , while providing the option of dividing the scores by the number of WordNet Synsets ( UseSSToWeight ) , which captures any dilution due to a word 's different senses . We also scale word2vec scores by different values . This gives us a total of 8 , 370 \" Methods \" . In calculating the similarity score , we capture the fraction of each Type that is aligned and scale it by the weight of that Type . This is captured in Equation 2 where score t represents the Similarity Score assigned to Type t by either of the measures detailed in Section 3 , count t represents the number of words of Type t in both sentences , w t the weight of Type t in the current iteration , and T is the total number of Types .", "entities": [[128, 129, "MetricName", "Score"]]}
{"text": "We first select a subset of the Methods , which are then passed on to either the SVR or KRR model . To do this , each of our Methods is ranked using three metrics with respect to the training set : The first is by use of the Pearson Correlation ( a criterion we call \" Method \" ) , the second is by the sum of the absolute error between Similarity Scores ( a criterion we call \" Error \" ) . The third metric aggregates the the rankings from the two criterion described above , and we call this criterion \" Combine \" . We select the top 50 methods using one of the three selection criterion .", "entities": [[49, 51, "MetricName", "Pearson Correlation"], [80, 81, "MetricName", "Error"]]}
{"text": "In order to find similarities between definitions , we first identify the word that a definition is defining . We achieve this by use of OneLook 's reverse dictionary search 5 , which returns a number of candidate words for a given definition . For each definition , the similarity of the top 10 candidates is then computed using Word2Vec and five similarity metrics provided by WordNet : Path distance , Leacock - Chodorow , Wu and Palmer , Jiang - Conrath and Lin . The final score is scaled between 0 and 5 and averaged across the 10 candidates returned by OneLook . We found this method of calculating similarities between definitions to be very good at telling if two definitions refer to the same word , but not ideally suited for measuring how similar they are . As a consequence , we found that results were clustered around 0 and 5 . The system produced a Pearson correlation of 0.69 on the SemEval 2014 definitions data set . We list the performance of our system in Table 2 . Our system 's poor performance on the ans - ans and ques - ques datasets can be attributed to our choice of training data , which , although well suited for previous years , was not well suited for these datasets . However , our system produces State of the Art results on the 2015 Test Sets . A breakdown of each of the run 's performance against the 2015 STS data set is provided in Table 3 . We note that the results we have reported for previous State of Art for individual data sources are not the results from just the winning system but the State of Art across all Systems for that data source . Our system also achieves comparable results ( 0.7793 ) to that presented by Sultan et al ( 2015 ) ( 0.779 ) on the 2014 STS dataset . The weighted mean reported by us does not include definitions as we decided to consider them independently . Table 4 on their 2015 System .", "entities": [[91, 92, "DatasetName", "0"], [150, 151, "DatasetName", "0"], [158, 160, "MetricName", "Pearson correlation"], [251, 252, "TaskName", "STS"], [324, 325, "TaskName", "STS"]]}
{"text": "String Kernels ( Lodhi et al , 2001 ) provide a way of comparing two documents , based on the inner product generated by all substrings of length n , typically known as character n - grams . Being relatively simple to use and implement , this technique has many applications according to the literature ( Cozma et al , 2018 ; Gim\u00e9nez - P\u00e9rez et al , 2017 ; Masala et al , 2017 ; Ionescu et al , 2014Popescu and Ionescu , 2013 ) , with emphasis on dialect identification and the good results obtained for this task in previous VarDial evaluation campaigns ( Butnaru and Ionescu , 2018b ; Ionescu and Butnaru , 2017 ; . Similar to our last year 's submission for the SMG - CH subtask ( G\u0203man and Ionescu , 2020 ) , we employ the string kernels computed by the efficient algorithm introduced by Popescu et al ( 2017 ) . This gives us a dual representation of the data , through a kernel matrix where the cell on row i and column j represents the similarity between two text samples x i and x j . In our experiments , we consider the presence bits string kernel ( Popescu and Ionescu , 2013 ) as the similarity function . For two strings x i and x j over a set of characters S , the presence bits string kernel is defined as follows : where n is the length of n - grams and # ( x , g ) is a function that returns 1 when the number of occurrences of n - gram g in x is greater than 1 , and 0 otherwise . k 0/1 ( x i , x j ) = g S n # ( x i , g ) # ( x j , g ) , ( 1 ) The resulting kernel matrix is plugged into a \u03bd - Support Vector Regression ( \u03bd - SVR ) model . SVR ( Drucker et al , 1997 ) is a modified Support Vector Machines ( SVM ) ( Cortes and Vapnik , 1995 ) model that is repurposed for regression . Similar to SVM , SVR uses the notion of support vectors and margin in order to find an optimal estimator . However , instead of a separating hyperplane , SVR aims to find a hyperplane that estimates the data points ( support vectors ) within the margin with minimal error . In our experiments , we employ an equivalent SVR formulation known as \u03bd - SVR ( Chang and Lin , 2002 ) , where \u03bd is the configurable proportion of support vectors to keep with respect to the number of samples in the data set . Using \u03bd - SVR , the optimal solution can converge to a sparse model , with only a few support vectors . This is especially useful in our case , as the data set provided for the SMG - CH subtask does not contain too many samples . Another reason to employ \u03bd - SVR in our regression task is that it was found to surpass other regression methods for other use cases , such as complex word identification ( Butnaru and Ionescu , 2018a ) .", "entities": [[90, 92, "TaskName", "dialect identification"], [283, 284, "DatasetName", "0"], [352, 353, "MethodName", "SVM"], [370, 371, "MethodName", "SVM"], [457, 460, "HyperparameterName", "number of samples"], [541, 544, "TaskName", "complex word identification"]]}
{"text": "Characters are the base units in building words that exist in the vocabulary of most languages . Among the advantages of working at the character level , we enumerate ( i ) the neutrality with respect to language theory ( independence of word boundaries , semantic structure or syntax ) and ( ii ) the robustness to spelling errors and words that are outside the vocabulary ( Ballesteros et al , 2015 ) . These explain the growing interest in using characters as features in various language modeling setups ( Al - Rfou et al , 2019 ; Ballesteros et al , 2015 ; Sutskever et al , 2011 ; Wood et al , 2009 ; Zhang et al , 2015 ) . Word embeddings are vectorial word representations that associate similar vectors to semanti - cally related words , allowing us to express semantic relations mathematically in the generated embedding space . From the initial works of Bengio et al ( 2003 ) and Sch\u00fctze ( 1993 ) to the recent improvements in the quality of the embedding and the training time ( Collobert and Weston , 2008 ; Mikolov et al , 2013a , b ; Pennington et al , 2014 ) , generating meaningful representations of words became a hot topic in the NLP research community . These improvements , and many others not mentioned here , have been extensively used in various NLP tasks ( Garg et al , 2018 ; Glorot et al , 2011 ; Musto et al , 2016 ) . Considering the sometimes orthogonal benefits of character and word embeddings , an intuitive idea has emerged , namely that of combining the character and word representations , which should complement each other in various aspects and provide better meaningful cues in the learning process of hybrid neural architectures ( Liang et al , 2017 ) . Thus , throughout the experiments performed in this work , we choose to employ a hybrid convolutional neural network working at both the character level ( Zhang et al , 2015 ) and the word level ( Kim , 2014 ) . The hybrid architecture concatenates two CNNs , out of which one is equipped with a character embedding layer and the other has an analogous word embeddings layer . The networks are able to automatically learn a 2D representation of text formed of either character or word embedding vectors , that are further processed by convolutional and fullyconnected layers . The last convolutional activation maps of our two CNNs sharing similar architectural choices are concatenated in what we call a hybrid network ( Liang et al , 2017 ) , with the aim of accurately and simultaneously predicting the two location components required for the geolocation task . The first component of the hybrid network is a characterlevel CNN , which takes the first and last 250 characters in the input and encodes each character with its position in the alphabet , then learns end - to - end embeddings for each character , as vectors of 128 components . The second CNN used as part of the hybrid network operates at the word level and it receives as input each sample encoded , initially , as an array of 100 indexes , corresponding to the position of each word in the vocabulary . As part of the pre - processing for the word - level CNN , we split the initial text into words , keeping the first 50 words and the last 50 words in the sample . In the end , we employ the German Snowball Stemmer ( Weissweiler and Fraser , 2018 ) to reduce each word to its stem , in an effort to reduce the vocabulary size by mapping variations of the same word to a single vocabulary entry . The word - level CNN is also equipped with an embedding layer , learning end - to - end word representations as vectors of length 128 . Each of the two CNNs has three convolutional ( conv ) layers placed after the initial embedding layer . The number of filters decreases from 1024 for the first conv layer to 728 for the second conv layer and to 512 for the third conv layer . Each conv layer is equipped with Rectified Linear Units ( ReLU ) ( Nair and Hinton , 2010 ) as the activation function . The convolutional filter sizes differ across the two convolutional architectures . Hence , we use kernels of sizes 9 , 7 and 7 for the char CNN . In the same time , we choose 7 , 5 and 3 as appropriate filter sizes for the conv layers of the word CNN . In the char CNN , each conv layer is followed by a max - pooling layer with filters of size 3 . In the word CNN , we add max - pooling layers only after the first two conv layers . The pooling filter sizes are 3 for the first pooling layer and 2 for the second pooling layer . The activation maps resulting after the last conv blocks of the char and the word CNNs are concatenated and the hybrid network continues with four fully - connected ( fc ) layers with ReLU activations . The fc layers are formed of 512 , 256 , 128 and 64 individual neural units , respectively .", "entities": [[123, 125, "TaskName", "Word embeddings"], [266, 268, "TaskName", "word embeddings"], [380, 382, "TaskName", "word embeddings"], [721, 724, "MethodName", "Rectified Linear Units"], [725, 726, "MethodName", "ReLU"], [736, 738, "HyperparameterName", "activation function"], [885, 886, "MethodName", "ReLU"]]}
{"text": "Gradient tree boosting ( Friedman , 2001 ) is based on training a tree ensemble model in an additive fashion . This technique has been successfully used in classification ( Li , 2010 ) and ranking ( Burges , 2010 ) problems , obtaining notable results in reputed competitions such as the Netflix Challenge ( Bennett and Lanning , 2007 ) . Furthermore , gradient tree boosting is the ensemble method of choice in some real - world pipelines running in production ( He et al , 2014 ) . XGBoost ( Chen and Guestrin , 2016 ) is a tree boosting model targeted at solving large - scale tasks with limited computational resources . This approach aims at parallelizing tree learning while also trying to handle various sparsity patterns . Overfitting is addressed through shrinkage and column subsampling . Shrinkage acts as a learning rate , reducing the influence of each individual tree . Column subsampling is borrowed from Random Forests ( Breiman , 2001 ) , bearing the advantage of speeding up the computations . In the experiments , we employ XGBoost as a metalearner over the individual predictions of each of the models described above . We opted for XG - Boost in detriment of average voting and a \u03bd - SVR meta - learner , both providing comparatively lower performance levels in a set of preliminary ensemble experiments .", "entities": [[144, 146, "HyperparameterName", "learning rate"]]}
{"text": "SVR based on string kernels . We compute the presence bits string kernel using the efficient algorithm proposed by Popescu et al ( 2017 ) . In order to find the optimal range of n - grams , we experiment with multiple blended spectrum string kernels based on various n - gram ranges that include n - grams from 3 to 7 characters long . The best performance in terms of both mean absolute error ( MAE ) and mean squared error ( MSE ) was attained by a string kernel based on the blended spectrum of 3 to 5 character n - grams . These results are consistent with those reported by Ionescu and Butnaru ( 2017 ) and G\u0203man and Ionescu ( 2020 ) , suggesting that the 3 - 5 n - gram range is optimal for German dialect identification . The resulting kernel matrix is used as input for two \u03bd - SVR models , optimized for predicting the latitude and longitude ( in degrees ) , respectively . For each of the two regressors , we tune the regularization penalty C , in a range of values from 10 \u22124 to 10 4 . Similarly , for the proportion of support vectors \u03bd , we consider 10 values uniformly covering the interval ( 0 , 1 ] with a step of 0.1 . To search for the optimal combination of hyperparameters , we apply grid search . Consistent with our previous study that inspired this choice of model and features ( G\u0203man and Ionescu , 2020 ) , for both regression models , the best hyperparameter combination is C = 10 and \u03bd = 0.5 . Hybrid CNN . As regularization for each conv block of the two CNNs , we introduce Gaussian noise and spatial dropout . We try many different magnitudes in the two regularization techniques , obtaining a few slightly different variations of the same model . These are compared against each other using grid search , selecting the best model to be included in our final ensemble . The rates used for the spatial dropout ( Tompson et al , 2015 ) are in the range [ 0.001 , 0.2 ] , with a step of 0.001 , with the best dropout rate deemed to be 0.007 . For the Gaussian noise , we consider standard deviations in the range [ 0.0001 , 0.01 ] , with the best results obtained with a standard deviation of 0.001 . After each of the four fully - connected layers , we employ plain dropout ( Srivastava et al , 2014 ) with a rate of 0.005 , deemed the best choice in the range [ 0.001 , 0.5 ] . In the optimization phase , common evaluation metrics such as the mean squared error ( MSE ) and the mean absolute error ( MAE ) are typically used to measure performance . In addition to these two metrics , we consider another error function as candidate for the loss to be minimized , namely the Huber loss . Although minimizing the Huber loss tends to give optimal values for the classical evaluation metrics , we finally use MSE as loss , given that it seems to converge to better results in terms of the median distance , which is the official evaluation metric in the SMG shared task . We optimize the hybrid CNN using the Adam optimization algorithm ( Kingma and Ba , 2015 ) with an initial learning rate of 10 \u22123 , chosen from the range [ 10 \u22125 , 10 \u22122 ] , a weight decay of 10 \u22127 , selected in the initial range [ 10 \u22129 , 10 \u22126 ] , and the learning rate decay of 0.999 . We train the hybrid architecture on mini - batches of 96 samples for 1000 epochs with early stopping . The network included in the final ensemble converged in 136 epochs . Fine - Tuned German BERT . We fine - tune three BERT models in slightly different setups . The pretrained base model used as starting point is the cased German BERT ( Wolf et al , 2020 ) . We set the maximum sequence length to 128 , applying zero - padding to reach the fixed length for the samples that are shorter . The batch size , unanimously used , is 32 , with a training that goes on for a maximum of 50 epochs . We optimize the models using Adam , with an initial learning rate \u03b1 = 5 10 \u22125 . The division by zero is prevented by the introduction of = 10 \u22128 . Moving to the particularities of each of the three fine - tuned BERT models , we note a difference in the choice of the loss function , such that the first model employs the L 2 loss ( MSE ) , while the other two models use the L 1 loss ( MAE ) . Another variation is introduced by the text truncation choices , such that for the first and third models , we do not enforce truncation , while for the second model , all the samples are enforced to align to the provided maximum sequence length . These differences are reflected in the number epochs required for complete convergence : 20 , 18 and 33 , respectively . We monitor the median distance for early stopping and observe that the best performance upon convergence is obtained by the third model , with a median distance of 30.17 km on the validation set , followed by the results of the first ( 30.63 km ) and second ( 33.86 km ) models , respectively . In the ensemble , we include only the top scoring BERT model . Extreme Gradient Boosting . We employed XG - Boost as a meta - learner , training it over the predictions of all the other models described in Section 3 . We selected XGBoost as the submission candidate , as it provided the best results . The XGBoost regressor , deemed optimal by the hyperparameter tuning , has default values for most parameters 2 , except for the number of estimators and the maximum tree depth . We set the number of estimators to 100 for the latitude regressor and to 1000 for the longitude regressor . Similarly , the maximum depth of the trees is 10 for the latitude model and 20 for the longitude one .", "entities": [[76, 77, "MetricName", "MAE"], [83, 84, "MetricName", "MSE"], [141, 143, "TaskName", "dialect identification"], [218, 219, "DatasetName", "0"], [472, 473, "MetricName", "MSE"], [480, 481, "MetricName", "MAE"], [505, 506, "MetricName", "loss"], [513, 514, "MetricName", "loss"], [519, 520, "MetricName", "loss"], [534, 535, "MetricName", "MSE"], [536, 537, "MetricName", "loss"], [573, 574, "MethodName", "Adam"], [586, 588, "HyperparameterName", "learning rate"], [605, 607, "MethodName", "weight decay"], [626, 628, "HyperparameterName", "learning rate"], [648, 650, "MethodName", "early stopping"], [667, 668, "MethodName", "BERT"], [674, 675, "MethodName", "BERT"], [693, 694, "MethodName", "BERT"], [728, 730, "HyperparameterName", "batch size"], [755, 756, "MethodName", "Adam"], [760, 762, "HyperparameterName", "learning rate"], [762, 763, "HyperparameterName", "\u03b1"], [794, 795, "MethodName", "BERT"], [806, 807, "MetricName", "loss"], [818, 819, "MetricName", "loss"], [820, 821, "MetricName", "MSE"], [832, 833, "MetricName", "loss"], [834, 835, "MetricName", "MAE"], [909, 911, "MethodName", "early stopping"], [969, 970, "MethodName", "BERT"], [1039, 1042, "HyperparameterName", "number of estimators"], [1051, 1054, "HyperparameterName", "number of estimators"], [1071, 1073, "HyperparameterName", "maximum depth"]]}
{"text": "As evaluation metrics we use the Strict F1 score , which is commonly adopted for this task ( Segura - Bedmar et al , 2013 ) . It is computed at the entity level , and assigns a hit only in case of perfect match between the labels assigned by the model and the labels in the gold annotation . In CADEC around 10 % of mentions are discontinuous ( Dai et al , 2020 ) and it is possible to have overlaps and intersections of discontinuous spans . We performed data tidying by merging overlapping ADE mentions , keeping only the longer span ( as it is customary in the literature ) and splitting discontinuous spans in multiple continuous spans .", "entities": [[7, 9, "MetricName", "F1 score"]]}
{"text": "Apart from the original BERT , we experimented with SpanBERT , for its peculiar pretraining procedure which focuses on predicting and encoding spans instead of single words , and with four BERT variants with in - domain knowledge , which differ from each other both for the corpus they were trained on and for the kind of pretraining . BERT Standard model , pretrained on general purpose texts ( Wikipedia and BookCorpus ) . SpanBERT This model is pretrained using the same corpus as the original BERT , so it comes with no in - domain knowledge . But the pretraining procedure makes its embeddings more appropriate for NER - like tasks . as it introduces an additional loss called Span Boundary Objective ( SBO ) , alongside the traditional Masked Language Modelling ( MLM ) used for BERT . Let us consider a sentence S = [ w 1 , w 2 , . . . , w k ] and its substring S m : n = [ w m , . . . , w n ] . w m\u22121 and w n+1 are the boundaries of S m : n ( the words immediately preceding and following it ) . We mask S by replacing all the words in S m : n with the [ MASK ] token . SpanBERT reads the masked version of S and returns an embedding for each word . The MLM loss measures if it is possible to reconstruct each original word w i S m : n from the corresponding embedding . The SBO loss measures if it is possible to reconstruct each w i S m : n using the embeddings of the boundary words w m\u22121 and w n+1 . BioBERT ( Lee et al , 2020 ) , pretrained from a BERT checkpoint , on PubMed abstracts . The authors of BioBERT provide different versions of the model , pretrained on different corpora . We selected the version which seemed to have the greatest advantage on this task , according to the results by Lee et al ( 2020 ) . We chose BioBERT v1.1 ( + PubMed ) , which outperformed other BioBERT v1.0 versions ( including the ones trained on full texts ) in NER tasks involving Diseases and Drugs . Preliminary experiments against BioBERT v.1.0 ( + PubMed+PMC ) confirmed this behaviour ( see Appendix D ) . BioClinicalBERT ( Alsentzer et al , 2019 ) , pretrained from a BioBERT checkpoint , on clinical texts from the MIMIC - III database . SciBERT ( Beltagy et al , 2019 ) , pretrained from scratch , on papers retrieved from Semantic Scholar ( 82 % of medical domain ) . PubMedBERT ( Gu et al , 2020 ) , pretrained from scratch , on PubMed abstracts and full text articles from PubMed Central . This model was created to prove that pretraining from scratch on a single domain produces substantial gains on in - domain downstream tasks . Gu et al ( 2020 ) compared it with various other models pretrained on either general texts , mixed - domain texts or in - domain texts starting from a general - purpose checkpoint ( e.g. BioBERT ) , showing that PubMedBERT outperforms them on several tasks based on medical language . The vocabulary of PubMedBERT contains more in - domain medical words than any other model under consideration . However , it should be kept in mind that ADE detection requires an understanding of both medical terms and colloquial language , as both can occur in social media text . Notice that two in - domain architectures were pretrained from scratch ( SciBERT and PubMed - BERT ) , meaning that they have a unique vocabulary tailored on their pretraining corpus , and include specific embeddings for in - domain words . BioBERT and BioClinicalBERT were instead pretrained starting from a BERT and BioBERT checkpoint , respectively . This means that the vocabularies are built from general - domain texts ( similarly to BERT ) and the embeddings are initialized likewise .", "entities": [[4, 5, "MethodName", "BERT"], [31, 32, "MethodName", "BERT"], [59, 60, "MethodName", "BERT"], [71, 72, "DatasetName", "BookCorpus"], [86, 87, "MethodName", "BERT"], [108, 109, "TaskName", "NER"], [118, 119, "MetricName", "loss"], [131, 133, "TaskName", "Language Modelling"], [134, 135, "DatasetName", "MLM"], [138, 139, "MethodName", "BERT"], [240, 241, "DatasetName", "MLM"], [241, 242, "MetricName", "loss"], [265, 266, "MetricName", "loss"], [305, 306, "MethodName", "BERT"], [380, 381, "TaskName", "NER"], [425, 428, "DatasetName", "MIMIC - III"], [447, 449, "DatasetName", "Semantic Scholar"], [453, 455, "DatasetName", "medical domain"], [622, 623, "MethodName", "BERT"], [657, 658, "MethodName", "BERT"], [679, 680, "MethodName", "BERT"]]}
{"text": "As a strong baseline , we used the TMRLeiden architecture ( Dirkson and Verberne , 2019 ) , which achieved the 2nd best Strict F1 - Score in the latest SMM4H shared task ( Weissenbacher et al , 2019 ) and is composed of a BiLSTM taking as input a concatenation of BERT and Flair embeddings ( Akbik et al , 2019 ) . We chose this baseline since the TMRLeiden code is publicly available .", "entities": [[24, 27, "MetricName", "F1 - Score"], [30, 31, "DatasetName", "SMM4H"], [45, 46, "MethodName", "BiLSTM"], [52, 53, "MethodName", "BERT"]]}
{"text": "TMRLeiden was re - implemented starting from its the original code 1 and trained according to the details in the paper . As for the Transformers , all experiments were performed using the TRANSFORMERS library ( Wolf et al , 2019 ) ( see Appendix C ) . Parameter - tuning was done via grid - search , using different learning rates ( [ 5e\u22124 , 5e\u22125 , 5e\u22126 ] ) and dropout rates ( from 0.15 to 0.30 , increments of 0.05 ) . All the architectures were trained for 50 epochs on the training set . Learning rate , dropout rate and maximum epoch were chosen evaluating the models on the validation set . During evaluation all the models were then trained using the best hyperparameters on the concatenation of the training set and the validation set , and tested on the test set . This procedure was repeated five times with different random seeds , and finally we averaged the results over the five runs .", "entities": [[98, 100, "HyperparameterName", "Learning rate"], [156, 157, "DatasetName", "seeds"]]}
{"text": "The results for the two datasets are shown in Table 1 ( we focus on the F1 - score , but Precision and Recall are reported in Appendix D ) . For reference , we reported the scores of the best architecture by Dai et al ( 2020 ) , which is the state - of - the - art system on CADEC . At a glance , all systems perform better on CADEC , whose texts belong to a more standardized variety of language . SpanBERT and PubMedBERT emerge as the top performing models , with close F1 - scores , and in particular , the SpanBERT models achieve the top score on both datasets , proving that modeling spans gives an important advantage for the identification of ADEs . For both models , the addition of CRF generally determines a slight improvement on CADEC , while it is detrimental on SMM4H. On SMM4H , the F1 - scores of BioBERT , SciBERT and Bio - ClinicalBERT consistently improve over the standard BERT , but they are outperformed by its CRFaugmented version , while on CADEC they perform closely to the standard model . The results suggest that in - domain knowledge is consistently useful only when training is done on in - domain text from scratch , instead of using general domain text first . SciBERT is also trained from scratch , but on a corpus that is less specific for the biomedical domain than the PubMedBERT one ( Gu et al , 2020 ) . The models are also being compared with TM - RLeiden : we can notice that both versions of SpanBERT and PubMedBERT outperform it on CADEC ( the differences are also statistically significant for the McNemar test at p < 0.001 ) , while only the basic versions of the same models retain an advantage on it on SMM4H ( also in this case , the difference is significant at p < 0.001 ) .", "entities": [[16, 19, "MetricName", "F1 - score"], [21, 22, "MetricName", "Precision"], [23, 24, "MetricName", "Recall"], [98, 99, "MetricName", "F1"], [138, 139, "MethodName", "CRF"], [154, 155, "DatasetName", "SMM4H"], [157, 158, "MetricName", "F1"], [165, 166, "DatasetName", "Bio"], [173, 174, "MethodName", "BERT"], [315, 316, "DatasetName", "SMM4H"]]}
{"text": "We analyzed the differences between the ADE entities correctly identified by the models and those that were missed , using the text statistics that we previously extracted with TEXTSTAT . As it was 1 @hospitalpatient have been on humira 2years now n get on off chest infections that sometimes need 2diff pills 2sort out should i b worried ? 4 i have had no side effects been taking arthrotec a little over a year , have not noticed any side effects . it does help alot i noticed that when there are times when i forget to take it i ca n't stand or walk for any lengths of time . 2 had a great few hours on my bike but exercise drives my olanzapine # munchies . getting fed up with not being able to fit into summer wardrobe 5 works just fine . if there are any side effects , they are definitely not noticeable . what 's with all these older people ( 70 's ) complaining about the lack of sex drive ? how much of what you are complaining about is simply related to getting older ? 3 this new baccy is just making my cough so much worse but ahh well need my nicotine 6 what a great store @walmart is : i loss iq points , gained weight & got addicted to nicotine - all in under 15 min from going in ! ! SpanBERT ( underlined in red ) . Actual ADEs in bold with gray background . The Samples belong to SMM4H ( 1 - 3 , 6 ) and CADEC ( 4 - 5 ) . predictable , it turns out that longer ADE spans are more difficult to identify : for all models , we extracted the average word length of correct and missed spans and we compared them with a twotailed Mann - Whitney U test , finding that the latter are significantly longer ( Z = - 6.176 , p < 0.001 ) . We also extracted the average number of difficult words in the correct and in the missed spans , defined as words with more than two syllables that are not included in the TEXTSTAT list of words of common usage in standard English . We took this as an approximation of the number of \" technical \" terms in the dataset instances . However , the average values for correct and missed instances do not differ ( Z = 0.109 , p > 0.1 ) , suggesting that the presence of difficult or technical words in a given instance does not represent an inherent factor of difficulty or facilitation . Still , for some of the models - including SpanBERT , PubMedBERT and TMRLeiden - this difference reaches a marginal significance ( p < 0.05 ) exclusively on the SMM4H dataset , where correctly identified spans have more difficult words . A possible interpretation is that , as the tweets ' language is more informal , such words represent a stronger ADE cue , compared to the more technical language of the CADEC dataset . Finally , we performed a qualitative analysis , comparing the predictions of SpanBERT and Pub - MedBERT . We selected the samples on which one of the architectures performed significantly better than the other one in terms of F1 - Score , and analyzed them manually . Some significant samples can be found in Table 2 . We observed that most of the samples in which PubMedBERT performed better than SpanBERT contained medical terms , which SpanBERT had completely ignored ( e.g. Sample 1 ) . The samples in which SpanBERT outperformed the in - domain model contained instead long ADE mentions , often associated with informal descriptions ( e.g. Samples 2 , 3 ) . As regards false positives , both models make similar errors , which fit into two broad categories : ( 1 ) extracting diseases or symptoms of a disease ( e.g. Samples 4 , 6 ) ; ( 2 ) not being able to handle general mentions , hypothetical language , negations and similar linguistic constructs ( e.g. Sample 5 ) . While the second kind of error requires a deeper reflection , the first one might be addressed by training the model to extract multiple kinds of entities ( e.g. both ADEs and Diseases ) .", "entities": [[219, 220, "MetricName", "loss"], [260, 261, "DatasetName", "SMM4H"], [475, 476, "DatasetName", "SMM4H"], [559, 562, "MetricName", "F1 - Score"]]}
{"text": "Since there is little research on determining the best fitting bias for stance detection , we explore three different classifiers for the stance classification , support vector machines ( SVM ) , random forest , and gradient boosting decision trees ( GBDT ) . For all three classifiers , we use the implementations in Scikit - Learn ( Pedregosa et al , 2011 ) . We choose SVM because it is the most widely used machine learning model for text classification and sentiment analysis ( e.g. , ( Pil\u00e1szy , 2005 ) ) . Additionally , it has been shown to be robust with high dimensional features ( e.g. , ( Joachims , 1998 ) ) . Random forest is adopted because of its capability of reducing overfitting by performing sampling on data points and on feature subspaces . GBDT is selected because it works well with continuous numerical features such as word vectors . We train individual classifiers for each target . Parameters are optimized in a 5 - fold cross - validation over the training data . SVM and random forest are trained on different numbers of selected unigrams for each target : 1 , 700 for Abortion , 1 , 535 for Atheism , 1 , 381 for Climate , 1 , 749 for Feminist , and 1 , 704 for Hillary . GBDT is trained on the word vectors : 300 dimensions for word2vec and 250 dimensions for GloVe . Additional experiments are performed with a standard feed - forward neural network on word vectors . These showed better performance on the training set for some targets , but overall , GBDT prove to be more reliable . SVM Our initial experiments using cross validation on training data showed that linear kernel performed better than non - linear ones , and that the LinearSVC implementation ( one - vs - rest strategy for multi - class ) outperformed SVC ( one - vs - one strategy ) . The optimal parameters differ for each target : 0.015 - 0.3 for the slack variable ; standard hinge or squared hinge for the loss function ; and L2 norm for the penalty term .", "entities": [[12, 14, "TaskName", "stance detection"], [22, 24, "TaskName", "stance classification"], [29, 30, "MethodName", "SVM"], [67, 68, "MethodName", "SVM"], [79, 81, "TaskName", "text classification"], [82, 84, "TaskName", "sentiment analysis"], [179, 180, "MethodName", "SVM"], [242, 243, "MethodName", "GloVe"], [282, 283, "MethodName", "SVM"], [355, 356, "MetricName", "loss"]]}
{"text": "The parameters for random forest are : 50 , 70 , or 90 for the number of trees ; 500 or All for the number of features to consider when looking for the best split ; 200 , 500 , or unlimited for the maximum depth of trees . GBDT The gradient boosting decision trees ( GBDT ) classifier is used in combination with word vector features . Our initial experiments showed that GBDT handles word vector features better than SVM and random forest . The optimal parameter range for different targets are : 80 - 100 for number of estimators ; 0.05 - 0.3 for learning rate ; false for warm start ; and 0.5 - 1.0 for subsample ratio . Ensemble Classifier Since initial experiments with the three classifiers showed considerable differences across targets and stances , we investigate whether an ensemble classifier would benefit from aggregating their predictions . For the ensemble classifier , we choose a memory - based learner , TiMBL , because of the need to operate on a small set of rather abstract features : stance predictions and confidence scores from the three classifiers along with the global features ( see section 2.2.3 ) . We use TiMBL ( Daelemans et al , 2009 ) version 6.4.2 , and perform 5 - fold jackknifing to generate the training set for this ensemble classifier . Parameter optimization is performed on the five folds . The best parameters are different in each target : 7 - 29 for the number of neighbors ; default minority voting for class voting in most cases ; Modified Value Distance , Jeffrey divergence , and cosine distance for distance metric ; and gain ratio for feature weight in most cases .", "entities": [[44, 46, "HyperparameterName", "maximum depth"], [80, 81, "MethodName", "SVM"], [98, 101, "HyperparameterName", "number of estimators"], [106, 108, "HyperparameterName", "learning rate"], [279, 281, "HyperparameterName", "distance metric"]]}
{"text": "While the official scorer averages the results over all five targets , we are interested in whether our classifiers show a stable performance across targets , and why the ensemble model benefits from combining all individual classifiers . For this reason , we modified the scorer so that it would calculate accuracy , precision , and recall for individual stances per target separately . The results are shown in table 5 . The official metric is the macro - averaged F - measure on Favor and Against while accuracy is equivalent to the micro - averaged F - measure based on all classes . The results show a more diverse picture : For the individual classifiers , GBDT reaches the highest ac - curacies for the targets Climate and Feminist , random forest for Atheism and Hillary , and they tie for Abortion . For the ensembles , the version without global features reaches higher accuracies for Abortion , Climate , and Hillary , the version with global features has a higher accuracy for Feminist , and they tie for Atheism . EnsembleNG , which reaches the best score across all targets , only reaches the best score for two targets : Abortion and Feminist . It reaches lower results than the best individual classifier for 3 targets : Atheism , Climate , and Hillary . However , since the best results for the latter 3 targets are reached by different individual classifiers ( random forest for Atheism and Hillary ; GBDT for Climate ) , we assume that the ensemble provides the best compromise . In order to obtain a better understanding of the differences in performance of classifiers across targets , we have analyzed the distribution of stances per target . One hypothesis that could be drawn from the analysis above is that the GBDT model is better suited for finding examples of the majority classes while random forest is better at finding minority class examples . However , when we compare the targets Abortion and Atheism , the class distribution is similar , but the performance of the two classifiers is vastly different : For Abortion , GBDT reaches higher recall for the majority class ( Against ) and higher precision for Favor . For Atheism , it has a higher precision for the majority class and a higher recall for Favor . The reasons for these different behaviors need to be determined in future work .", "entities": [[51, 52, "MetricName", "accuracy"], [80, 83, "MetricName", "F - measure"], [88, 89, "MetricName", "accuracy"], [96, 99, "MetricName", "F - measure"], [172, 173, "MetricName", "accuracy"]]}
{"text": "This paper describes our contribution to SemEval - 2021 Task 5 : Toxic Spans Detection . Our solution is built upon RoBERTa language model and Conditional Random Fields ( CRF ) . We pre - trained RoBERTa on Civil Comments dataset , enabling it to create better contextual representation for this task . We also employed the semi - supervised learning technique of selftraining , which allowed us to extend our training dataset . In addition to these , we also identified some pre - processing steps that significantly improved our F1 score . Our proposed system achieved a rank of 41 with an F1 score of 66.16 % .", "entities": [[21, 22, "MethodName", "RoBERTa"], [29, 30, "MethodName", "CRF"], [36, 37, "MethodName", "RoBERTa"], [91, 93, "MetricName", "F1 score"], [104, 106, "MetricName", "F1 score"]]}
{"text": "Toxic comments have a different language construct from the general language . Their slang and obfuscated content ( van Aken et al , 2018 ) make it difficult for the language models pre - trained on broader datasets to understand them . Similar to other domain - specific models ( Beltagy et al , 2019 ; Lee et al , 2020 ; Paraschiv et al , 2020 ) , we pretrained the RoBERTa - base model on the Civil comments dataset using Masked Language Modelling ( MLM ) ( Devlin et al , 2018 ) to provide the necessary domain knowledge and created our model RoBERTa ( p ) . The original weights of RoBERTabase served as the starting point for the pre - training . The pre - training was done for 0.2 million steps with a batch size of 32 and a learning rate of 2e - 5 . 1 https://github.com/jain - abhinav02/ Toxic_Spans_Detection", "entities": [[72, 73, "MethodName", "RoBERTa"], [83, 85, "TaskName", "Language Modelling"], [86, 87, "DatasetName", "MLM"], [105, 106, "MethodName", "RoBERTa"], [138, 140, "HyperparameterName", "batch size"], [144, 146, "HyperparameterName", "learning rate"]]}
{"text": "The best performing model on the manually annotated dataset ( gold dataset ) was used to generate toxic spans for the unannotated dataset . When selecting the unannotated data , we followed the process similar to the one used for creating the gold dataset ( Pavlopoulos et al , 2021 ) that is , filter the most toxic samples ( toxicity \u2265 0.80 ) from the Civil Comments dataset and select a random set of 10 , 000 samples . This process allowed the silver data to have similar toxicity distribution as the gold data . The newly generated annotations ( silver dataset ) were then used along with the gold dataset to train a new model . The model trained on the combined gold and silver dataset gave better performance ( F1 score : 66.13 % ) than the one trained only on the gold dataset ( F1 score : 65.66 % ) . We repeated this process for one more iteration with another random set of 10 , 000 samples ( F1 score : 66.34 % ) . Figure 2 gives a simplistic idea of selftraining . 3 Experimental Setup Data : Each training example consisted of a text sample in English , and its ground truth toxic span provided as a list of character offsets ( possibly empty ) . The posts were sampled from the publicly available Civil Comments dataset . The training set consisted of 7939 samples . We randomly sampled 20 % of it as the development set . The test set for the evaluation phase had 2000 samples . In the training dataset , sample length varies from 1 to 421 tokens , with an average length of 47 tokens when tokenized using the RoBERTa - base tokenizer . Nearly 10 % of all tokens in the training dataset are marked as toxic . The mean span length is 17.5 characters and 485 samples have empty spans . Further details about the dataset can be found in the task description paper ( Pavlopoulos et al , 2021 ) .", "entities": [[132, 134, "MetricName", "F1 score"], [148, 150, "MetricName", "F1 score"], [173, 175, "MetricName", "F1 score"], [291, 292, "MethodName", "RoBERTa"]]}
{"text": "The evaluation measure for a sample is the F1 score between the predicted spans and the ground truth spans as defined in the SemEval - 2021 Task 5 paper ( Pavlopoulos et al , 2021 ) . The overall score is obtained by taking the mean of the F1 score of all samples in the test set . by Huggingface 2 . The RoBERTa model was followed by two dense layers with 512 and 128 units with relu ( Agarap , 2018 ) as the activation function and a dropout rate of 0.1 . The output layer had two or three labels depending on the tagging scheme . We applied the post - processing steps mentioned in section 2.2 for all the model variants .", "entities": [[8, 10, "MetricName", "F1 score"], [48, 50, "MetricName", "F1 score"], [63, 64, "MethodName", "RoBERTa"], [77, 78, "MethodName", "relu"], [85, 87, "HyperparameterName", "activation function"]]}
{"text": "Table 1 shows that our RoBERTa ( p ) model outperforms the original RoBERTa model . As suggested earlier , domain - specific pre - training allows the model to understand the language construct of toxic comments better . Additionally , we observe a significant increase in performance by adding preprocessing steps as it makes the model more robust to the noise present in the text samples . Adding the CRF layer further improves the F1 score by eliminating the problem of independent label prediction . It is evident from table 1 that the BIO tagging scheme performs better than the IO tagging scheme when working with CRF , suggesting it can better understand the span nature of the output . Finally , using two rounds of self - training helped us achieve our best F1 score , 66.34 % 3 . One interesting observation that can be drawn from Table 1 is that for almost all the models , the recall remains constant and improvement in F1 is due to improvement in precision . The constancy of recall indicates that few spans are not captured as toxic by any of the models .", "entities": [[5, 6, "MethodName", "RoBERTa"], [13, 14, "MethodName", "RoBERTa"], [70, 71, "MethodName", "CRF"], [75, 77, "MetricName", "F1 score"], [107, 108, "MethodName", "CRF"], [135, 137, "MetricName", "F1 score"], [167, 168, "MetricName", "F1"]]}
{"text": "Figure 3 shows the variation of the F1 score across different toxic span lengths on the test dataset . Our model achieved a very high F1 score when one ( Span Length 1 - 9 , Mean F1 Score : 83.17 % ) or two ( Span Length 10 - 17 , Mean F1 Score : 74.44 % ) words are marked as toxic in a text sample . As the number of characters marked as toxic increases , the F1 score falls drastically , reaching as low as 24.82 % when more than 58 characters are marked as toxic . There are two main reasons for this . First , it is easier for the model to capture short - term dependencies than long - term dependencies . Second , only 10 % of the training data has a span length of more than 25 characters making the model less equipped to capture such toxic spans . To investigate our model 's most problematic cases , we analysed the samples for which our model gave a zero F1 score . There were 447 such samples , of which 349 samples did not have any toxic span in the ground truth . This is also reflected in Figure 3 , as the mean F1 score of all the samples with zero span length is 11.42 % . Further analysis revealed that our model tends to mark those tokens as toxic , which were frequently found to be toxic elsewhere . A few samples with empty toxic spans had doubtful gold annotations . However , in other samples , our model failed to capture the sentence 's context precisely and predicts tokens that were not used in a toxic sense . Table 2 shows other standard errors our model makes . It seems that our model has a problem with small sentences . More often than not , it misses the toxic span present in it and returns an empty span . A similar case occurs when it encounters text samples with rare toxic words . These words may be present in very few examples or be completely absent from the training dataset , making our model less endowed to understand them . Other than these , our model sometimes misses the non - swear words in a toxic span .", "entities": [[7, 9, "MetricName", "F1 score"], [25, 27, "MetricName", "F1 score"], [37, 39, "MetricName", "F1 Score"], [53, 55, "MetricName", "F1 Score"], [80, 82, "MetricName", "F1 score"], [178, 180, "MetricName", "F1 score"], [213, 215, "MetricName", "F1 score"]]}
{"text": "This paper described our system developed for SemEval - 2021 Task 5 : Toxic Span Detection . We built our solution on the RoBERTa language model and Conditional Random Fields ( CRF ) . Though RoBERTa alone can achieve great results , we highlighted the benefits of using external datasets and the performance improvements it can help us achieve . We pre - trained RoBERTa on the Civil Comments dataset to impart domain - specific knowledge to it . We also employed the semisupervised learning technique of self - training to extend our training dataset . In addition to these , we also discovered some pre - processing steps that significantly improved our F1 score . Experimenting with different tagging schemes , we found out that the BIO scheme works the best with CRF . In future , we plan to experiment with other language models such as T5 ( Raffel et al , 2019 ) , XL - Net ( Yang et al , 2019 ) and DeBERTa ( He et al , 2020 ) . The system could also benefit from the addition of syntactic and semantic features at the word and sentence level .", "entities": [[23, 24, "MethodName", "RoBERTa"], [31, 32, "MethodName", "CRF"], [35, 36, "MethodName", "RoBERTa"], [64, 65, "MethodName", "RoBERTa"], [113, 115, "MetricName", "F1 score"], [133, 134, "MethodName", "CRF"], [148, 149, "MethodName", "T5"], [168, 169, "MethodName", "DeBERTa"]]}
{"text": "Pre - training has improved model accuracy for both classification and generation tasks at the cost of introducing much larger and slower models . Pruning methods have proven to be an effective way of reducing model size , whereas distillation methods are proven for speeding up inference . We introduce a block pruning approach targeting both small and fast models . Our approach extends structured methods by considering blocks of any size and integrates this structure into the movement pruning paradigm for fine - tuning . We find that this approach learns to prune out full components of the underlying model , such as attention heads . Experiments consider classification and generation tasks , yielding among other results a pruned model that is a 2.4x faster , 74 % smaller BERT on SQuAD v1 , with a 1 % drop on F1 , competitive both with distilled models in speed and pruned models in size .", "entities": [[6, 7, "MetricName", "accuracy"], [78, 80, "MethodName", "movement pruning"], [130, 131, "MethodName", "BERT"], [132, 133, "DatasetName", "SQuAD"], [141, 142, "MetricName", "F1"]]}
{"text": "Pre - trained transformer models are the standard for NLP tasks in both classification and generation tasks ( Devlin et al , 2019 ; Lewis et al , 2020 ) . The recent trend is for models to continue to grow in size while yielding improved performance on standard benchmarks ( Rosset , 2020 ) . This development highlights the need to reduce the storage size and increase the efficiency of pre - trained models . Pruning methods have shown to be extremely effective at reducing the storage size of models fine - tuned for a specific task . Approaches such as magnitude pruning ( Han et al , 2015 ) , L0 regularization ( Louizos et al , 2018 ) , lottery ticket hypothesis ( Frankle and Carbin , 2018 ) , diff pruning ( Guo et al , 2020 ) , and movement pruning ( Sanh et al , 2020 ) have demonstrated remarkable reductions in model size . Movement pruning produces 77 % savings in parameter storage for a 1 % drop in accuracy on SQuAD v1.1 . However , these models yield very little actual efficiency benefits , as to run them in standard hardware often requires reconstructing the original dense shape . On the other hand distillation methods have been more effective at producing faster models as has been shown by DistilBERT ( Sanh et al , 2019 ) , TinyBERT ( Jiao et al , 2019 ) or MobileBERT ( Sun et al , 2020 ) . These approaches utilize targeted distillation to produce smaller models with a dense structure that is fast on standard hardware . However without careful engineering and size selection these models are much larger than pruned ones . In this work , we target closing this gap through block pruning . Unlike pruning individual parameters , this approach encourages pruning that can be optimized on dense hardware . It is a less rigid approach than row or column - based pruning typically used in structured approaches ( McCarley , 2019 ) , which have been difficult to apply effectively to transformers . We integrate this approach with Movement pruning ( Sanh et al , 2020 ) , a simple method for pruning pre - trained models during fine - tuning . The final method 1 has few additional hyperparameters or training requirements . Experiments consider a large variety of different benchmark datasets comparing accuracy and efficiency . We find a surprising result that despite utilizing sub - row square blocks during training , the approach learns to eliminate full components of the model , effectively dropping a large number of attention heads . This effect allows the model to achieve speedups even beyond standard structured pruning of feed - forward layers . Results show a 2.4x speedup on SQuAD v1.1 with a 1 % drop of F1 , and a 2.3x speedup on QQP with a 1 % loss of F1 . Experiments on summarization also show a 1.39x speedup for an average of 2 points drop on all ROUGE metrics on CNN / DailyMail , and for a reduction of decoder weights of 3.5x . 1 Available at https://github.com/ huggingface / nn_pruning", "entities": [[144, 146, "MethodName", "movement pruning"], [161, 163, "MethodName", "Movement pruning"], [176, 177, "MetricName", "accuracy"], [178, 179, "DatasetName", "SQuAD"], [226, 227, "MethodName", "DistilBERT"], [244, 245, "MethodName", "MobileBERT"], [358, 360, "MethodName", "Movement pruning"], [404, 405, "MetricName", "accuracy"], [469, 470, "DatasetName", "SQuAD"], [477, 478, "MetricName", "F1"], [484, 485, "DatasetName", "QQP"], [489, 490, "MetricName", "loss"], [491, 492, "MetricName", "F1"], [495, 496, "TaskName", "summarization"]]}
{"text": "Starting with a transformer model with parameters \u03b8 , our goal is to produce a set of parameters \u03b8 that are both fine - tuned for a specific end - task and smaller in such a way that inference can be efficiently computed on parallel hardware . The two largest lines in the transformer parameter budget are the feed - forward network sublayer ( FFN ) and the multi - head attention sub - layer ( MHA ) . The FFN parameters consist of two matrices ( W 1 and W 2 ) of transposed shape R d model \u00d7d ff and R d ff \u00d7d model where d model is the hidden size and d ff d model is the inner size . These are used in the standard fashion by the network . The MHA parameters consist of 4 projection matrices ( W q , W k , W v and W o ) of size R d model \u00d7d model ( query , key , value , out ) . These are used to project the hidden vector to and from the component attention parts . In implementations , this projection is made with the matrices in their folded tensor form In standard fine - tuning , starting from \u03b8 , we optimize the loss L ( for instance , cross - entropy for classification ) : arg min \u03b8 L ( \u03b8 ) Score - based pruning methods ( Ramanujan et al , 2019 ) modify the model by introducing score parameters S for each parameter i and replace the original parameter matrices with a masked version W = W M ( S ) . For instance , in the simplest version of magnitude pruning , the mask would just zero - out parameters with low absolute values . Movement pruning ( Sanh et al , 2020 ) is a scorebased pruning approach that encourages the model to optimize these score parameters . Specifically , we focus on the soft - movement variant of movement pruning that sets M ( S ) = 1 ( S > \u03c4 ) for a threshold parameter \u03c4 , and optimizes a regularized objective , arg min \u03b8 , S L ( \u03b8 ) + \u03bb \u03c3 ( S ) where \u03bb is a hyper - parameter , A = i , j A i , j and \u03c3 is the sigmoid function . This pruning objective encourages the model to fine - tune the parameters while lowering the scores of unimportant parameters and thus encouraging more sparsity . In order to train through the threshold , a straight - through estimator ( Bengio et al , 2013 ) is used . Movement pruning , combined with distillation , has shown to be a very effective method to reduce the number of parameters in an existing model yielding 94 % pruning in our tests for a F1 of 87.5 on SQuAD v1.1 ( BERT - base is 88.5 ) . This results in significantly smaller models than distillation alone . However , even with this sparsity level , the model is not substantially faster when run on most standard hardware that can not significantly take advantage of this style of sparse matrix - vector product .", "entities": [[7, 8, "HyperparameterName", "\u03b8"], [18, 19, "HyperparameterName", "\u03b8"], [68, 72, "MethodName", "multi - head attention"], [212, 213, "HyperparameterName", "\u03b8"], [217, 218, "MetricName", "loss"], [232, 233, "HyperparameterName", "\u03b8"], [235, 236, "HyperparameterName", "\u03b8"], [237, 238, "MetricName", "Score"], [303, 305, "MethodName", "Movement pruning"], [338, 340, "MethodName", "movement pruning"], [367, 368, "HyperparameterName", "\u03b8"], [372, 373, "HyperparameterName", "\u03b8"], [452, 454, "MethodName", "Movement pruning"], [470, 473, "HyperparameterName", "number of parameters"], [486, 487, "MetricName", "F1"], [490, 491, "DatasetName", "SQuAD"], [493, 494, "MethodName", "BERT"]]}
{"text": "We conduct experiments on five ( English ) tasks commonly used to evaluate pre - trained language models : question answering ( SQuAD v1.1 Rajpurkar et al , 2016 ) and ( SQuAD v2 Rajpurkar et al , 2018 ) , natural language inference ( MNLI Williams et al , 2018 ) , sentence similarity ( QQP Chen et al , 2018 ) , sentiment classification ( SST - 2 Socher et al , 2013 ) and abstractive summarization ( CNN / DailyMail Hermann et al , 2015 ) . These datasets respectively contain 87k , 130k , 392k , 363k , 67k and 287k training examples , and are downloaded from the Hugging Face datasets hub . SQuAD is formulated as a span - extraction task , MNLI and QQP are sentence pairs classification tasks , SST - 2 is a sentence classification task and CNN / DailyMail ( \" CNN \" ) is formulated as a conditional generation task . We report the performance on the development set as measured by the accuracy for MNLI and SST - 2 , F1 for QQP , the exact match ( EM ) and F1 for SQuAD and ROUGE for CNN / DailyMail . We experiment with task - specific pruning of transformer language models . We use BERT ( Devlin et al , 2019 ) We compare against several baselines . Movement pruning is a fully unstructured approach and gives an upper bound on the sparsity trade - offs we hope to achieve , even if it provides little speed benefit . We also compare our results against state - ofthe - art approaches developed for fast inference of transformer - based language models . DistilBERT ( Sanh et al , 2019 ) is obtained by distilling through pre - training a pre - trained BERT into a smaller model . TinyBERT ( Jiao et al , 2019 ) distills a finetuned model while using data augmentation . Mo - bileBERT ( Sun et al , 2020 ) is the result of a large architecture search . dBART ( Shleifer and Rush , 2020 ) is obtained by arbitrarily copying equally spaced layers of a large model to a smaller one . To measure inference speed on GPU , we use a 24 GB 3090 RTX and an Intel i7 CPU , using a large batch size ( 128 ) for evaluation and using PyTorch CUDA timing primitives . We measure the speed of other models in this same setup . Results may be different from original papers , as latency and throughput characteristics are different for each platform . We also provide the number of parameters in the linear layers of the Transformer layers for each of our models and for the reference ones : as the linear layers represent most of the FLOPS , this is a good proxy for the computation required and to some extent for the compute time , when the model characteristics are equivalent .", "entities": [[19, 21, "TaskName", "question answering"], [22, 23, "DatasetName", "SQuAD"], [32, 33, "DatasetName", "SQuAD"], [41, 44, "TaskName", "natural language inference"], [45, 46, "DatasetName", "MNLI"], [56, 57, "DatasetName", "QQP"], [67, 68, "DatasetName", "SST"], [78, 79, "TaskName", "summarization"], [118, 119, "DatasetName", "SQuAD"], [128, 129, "DatasetName", "MNLI"], [130, 131, "DatasetName", "QQP"], [137, 138, "DatasetName", "SST"], [142, 144, "TaskName", "sentence classification"], [174, 175, "MetricName", "accuracy"], [176, 177, "DatasetName", "MNLI"], [178, 179, "DatasetName", "SST"], [182, 183, "MetricName", "F1"], [184, 185, "DatasetName", "QQP"], [187, 189, "MetricName", "exact match"], [190, 191, "MetricName", "EM"], [193, 194, "MetricName", "F1"], [195, 196, "DatasetName", "SQuAD"], [217, 218, "MethodName", "BERT"], [231, 233, "MethodName", "Movement pruning"], [285, 286, "MethodName", "DistilBERT"], [305, 306, "MethodName", "BERT"], [325, 327, "TaskName", "data augmentation"], [395, 397, "HyperparameterName", "batch size"], [444, 447, "HyperparameterName", "number of parameters"], [453, 454, "MethodName", "Transformer"], [474, 475, "MetricName", "FLOPS"]]}
{"text": "The pruning approaches are shown in Table 1 . Block pruning use square block sizes throughout all the linear layers , as an extension of the original movement pruning for which the block size is 1 . Hybrid pruning jointly removes hidden dimensions in feed - forward layers W 1 and W 2 , using movement pruning to create the dimension mask . This corresponds to full rows or columns in the parameter matrices . The pruned W 1 and W 2 can then be \" compacted \" to become fully dense : we perform dense operations on cropped matrices . For the attention layers , pruning only some rows or columns in W q , W k , W v and W o can not be practically exploited . This is because the structure of the computation makes the additional cost of resizing the tensor inefficient . We , therefore , use square block pruning on the attention layer , with a block size of ( 32 , 32 ) which showed the best tradeoff between performance and accuracy . Struct pruning uses the same methods for FFN layers but aims to remove model attention heads directly . To do so , we choose a block size on attention that equals the head size while still using the same soft movement pruning strategy . For this approach , we use a \u03bb att equals to 1/32 , as there are 32 times more parameters than in an attention block than in a feed - forward dimension . When Block Pruning does not fully remove a component such as an attention head , as shown in Figure 1 , we can not speed up the model . But we can reclaim some of the performance at no speed cost and at marginal cost on sparsity by making use of those zero weights . Hybrid Filled pruning allows the model to reinitialize these reclaimed weights uniformly at random and continue fine - tuning the smaller model for a few steps . We also explore \" rewinding \" ( Frankle and Carbin , 2018 ) by identifying weights that should not be pruned ( because they are part of a non - empty attention head ) and re - fine - pruning the pre - trained model : the first run marks the attention heads that were not pruned , and the second uses this information to create a positive mask of weights that are protected from pruning . We did not find a significant difference between the two methods . The results presented here do not use rewinding .", "entities": [[27, 29, "MethodName", "movement pruning"], [55, 57, "MethodName", "movement pruning"], [103, 105, "HyperparameterName", "attention layers"], [179, 180, "MetricName", "accuracy"], [221, 223, "MethodName", "movement pruning"]]}
{"text": "Main Results We begin by observing the highlevel impact of the different pruning methods . Figure 1 shows the effect on attention and feedforward layers for the different block pruning methods . We find that all the different block sizes learn to prune out entire dimensions in the FFN layers . Interestingly we find that the block methods can also learn to remove entire heads from the MHA . This pruning pattern makes it possible to remove entire heads from the model during inference . For this reason , we focus on the Hybrid approach as our main method , which can both eliminate feed - forward dimensions while using blocks to remove attention heads gradually . Results on SQuAD are shown in Figure 2 , which compares our approach for speed and density to baseline BERT - Base tuned models such as TinyBERT - 6 and DistilBERT ( MobileBERT is discussed below ) . The main result is that the Hybrid Pruning model is as fast as the baseline and approaches the same accuracy while at the same time producing significantly smaller models in terms of density . Moving to the Hybrid Filled model leads to a further gain in speed at a small cost in model density . For instance , for the same F1 performance of 87.5 , Hybrid Filled models display a 2.5x speedup against 1.88 for TinyBERT . TinyBERT and Distil - BERT have 50 % of BERT 's encoder parameters , whereas Hybrid Filled models have 25 % BERT parameters for the same level of accuracy . The figures also include two intrinsic baselines : our reimplementation of Movement pruning and pure Block pruning . We find that our implementation of Movement pruning is highly effective at producing sparse models ( even leading to a small increase in accuracy ) but does not produce significant speedups . Square Block pruning does better , but not as well as hybrid blocks . Table 2 gives a full comparison of models with different compression rates . As linear layers represent a very large part of the flops of a transformer model , this compression rate is actually a good measure of the maximum achievable speedup . This number is much higher than the actually measured speedup . This indicates that our setup for measur - BERT performance on other tasks . Comparison with MobileBERT All methods can be improved further using a larger teacher model . For these experiments , we compare with MobileBERT , which uses a BERT - large teacher and reaches an F1 of 90.0 on SQuAD v1.1 on its fastest version . It should be noted that Mo - bileBERT makes use of additional optimizations not present in the original BERT - large we are using : LayerNorms are replaced by purely linear NoNorms , and GeLUs are replaced by ReLUs . For these experiments , we use a BERT - large teacher to perform meaningful comparisons , using our best method Hybrid Filled . Figure 2 shows that we have comparable results on SQuAD v1.1 , with a simpler optimization approach : we get a slightly better model ( F1=90.3 ) for the same speedup of 1.6x , and we get a speedup of 2.2x at BERT - base accuracy ( F1=88.5 ) . We observe that using a large teacher is beneficial even at high levels of pruning : up to 80 % of sparsity , the resulting student model has better accuracy for the same number of parameters when using a BERTlarge teacher instead of a base one . This trend reverses after this point : a larger teacher is detrimental to accuracy when the student is very heavily pruned . Encoder - Decoder Finally , we apply these methods to two encoder - decoder architectures , BARTbase and BART - large for the task of summarization . For these architectures , the decoder parameters are responsible for a majority of the computational costs , so these are our main focus . Voita et al ( 2019 ) observed that for machine translation models , encoder heads were much easier to prune than decoder ones . We found similar results , e.g. for identical \u03bb att and \u03bb ffn , the encoder was systematically more pruned than the decoder , for both MHA and FFN sub - layers . In order to increase speedup gain , we applied twice as much weight on the decoder compression , which resulted in even pruning ratios among the encoder and decoder . Table 4 shows the main results . We see that Hybrid pruning leads to large decoder compression ratios ( 3.4 on BART - base and 3.5 BART - large ) with only a small drop in ROUGE score . Speedups reach 1.4 times of the original speed . ( Given the large decoder compression rates , we would expect larger speedups to be possible with further engineering of the inference . ) There is less comparable work for pre - trained encoder - decoders . We compare our approach with a distillation - based approach dBART ( Shleifer and Rush , 2020 ) . This approach yields a similar speedup gain with a smaller drop in performance but less sparsity . For models of comparable sizes ( 158 M for our Hybrid NT vs 176 M for dBART - 6 - 6 ) , we observe a drop of 0.7 in R2 and 0.4 in RL against 0.9 in R2 and 1.3 in RL for dBART - 6 - 6 . As with encoder - only models , the two approaches could likely be combined to yield even faster , smaller models . 3", "entities": [[119, 120, "DatasetName", "SQuAD"], [136, 137, "MethodName", "BERT"], [147, 148, "MethodName", "DistilBERT"], [149, 150, "MethodName", "MobileBERT"], [174, 175, "MetricName", "accuracy"], [216, 217, "MetricName", "F1"], [237, 238, "MethodName", "BERT"], [242, 243, "MethodName", "BERT"], [254, 255, "MethodName", "BERT"], [261, 262, "MetricName", "accuracy"], [274, 276, "MethodName", "Movement pruning"], [287, 289, "MethodName", "Movement pruning"], [304, 305, "MetricName", "accuracy"], [389, 390, "MethodName", "BERT"], [397, 398, "MethodName", "MobileBERT"], [417, 418, "MethodName", "MobileBERT"], [422, 423, "MethodName", "BERT"], [429, 430, "MetricName", "F1"], [433, 434, "DatasetName", "SQuAD"], [458, 459, "MethodName", "BERT"], [487, 488, "MethodName", "BERT"], [512, 513, "DatasetName", "SQuAD"], [545, 546, "MethodName", "BERT"], [548, 549, "MetricName", "accuracy"], [582, 583, "MetricName", "accuracy"], [586, 589, "HyperparameterName", "number of parameters"], [613, 614, "MetricName", "accuracy"], [640, 641, "MethodName", "BART"], [647, 648, "TaskName", "summarization"], [682, 684, "TaskName", "machine translation"], [781, 782, "MethodName", "BART"], [786, 787, "MethodName", "BART"]]}
{"text": "Large Model Pruning To test that this approach scales to large models , we apply Hybrid pruning on BERT - large on SQuAD v1.1 . We observe similar results : a 18 % dense BERT - large has a F1 of 90.2 , with a speedup of 3.2x compared to BERT - large with a F1 of 93.2 . This pruned model is actually faster than a BERT - base model ( Table 5 ) . We can compare Hybrid Pruning of SQuAD v2 BERT - large models with the results of the structured pruning method described in McCarley ( 2019 ) . For a 17 % dense model , we obtain a F1 of 82.6 , whereas structured pruning gets a 25 % dense model with a F1 of 81.5 . This result is in line with Li et al ( 2020 ) : the larger the model , the more pruning is effective . When pruning a larger model , the final model is actually better than a smaller one with the same absolute number of parameters . Block Size Influence Figure 3 shows the impact of different block sizes on Block pruning : pruning is done on attention layers and FFNs with the same square block size , from ( 4 , 4 ) to ( 32 , 32 ) , with a BERT - base teacher . We can see that we reach the BERT - base original F1 for all block sizes from 4 to 32 , but with a speedup that increases with the block size . The maximum reachable speedup without F1 drop is 1.3 for a block size of 32 . But when some drop of F1 is allowed , the speedup increases quickly with the block size and plateau when reaching 16 . We then reach a speedup of 1.75 for an F1 drop of 2 % and a block size of 32 . We also note that , with the original Movement Pruning method , we see some speedup due to full dimension pruning . This likely comes from our improved set of hyper - parameters ( compared to the original paper ) , allowing us to remove some empty rows and columns in the FFN layers . However we see that using blocks leads to a significant speed improvement compared to Movement Pruning . Quantization Quantization is often of critical importance for practical applications . We , therefore , wanted to check that our networks could be subjected to quantization without significant loss of accuracy , especially when considering the issues that could arise with the high level of sparsity of some FFNs . Table 6 shows the results of full 8 - bit quantization tests on our models . These indicate that the method is compatible with quantization , and the models using quantization on top of our pruning method achieve very high gains in terms of size ( as well as speed ) .", "entities": [[18, 19, "MethodName", "BERT"], [22, 23, "DatasetName", "SQuAD"], [34, 35, "MethodName", "BERT"], [39, 40, "MetricName", "F1"], [50, 51, "MethodName", "BERT"], [55, 56, "MetricName", "F1"], [67, 68, "MethodName", "BERT"], [82, 83, "DatasetName", "SQuAD"], [84, 85, "MethodName", "BERT"], [113, 114, "MetricName", "F1"], [128, 129, "MetricName", "F1"], [176, 179, "HyperparameterName", "number of parameters"], [200, 202, "HyperparameterName", "attention layers"], [226, 227, "MethodName", "BERT"], [238, 239, "MethodName", "BERT"], [242, 243, "MetricName", "F1"], [268, 269, "MetricName", "F1"], [284, 285, "MetricName", "F1"], [311, 312, "MetricName", "F1"], [331, 333, "MethodName", "Movement Pruning"], [392, 394, "MethodName", "Movement Pruning"], [395, 396, "TaskName", "Quantization"], [396, 397, "TaskName", "Quantization"], [420, 421, "TaskName", "quantization"], [423, 424, "MetricName", "loss"], [425, 426, "MetricName", "accuracy"], [455, 456, "TaskName", "quantization"], [469, 470, "TaskName", "quantization"], [475, 476, "TaskName", "quantization"]]}
{"text": "Compress EM F1 As shown in Table 7 , combining hybrid pruning with distillation always performs better than pruning alone , but that it is not critical for the approach to work . The distillation effect is larger for smaller datasets such as SST - 2 , which are prone to over - fitting . We believe that the regularization brought by pruning and distillation counters overfitting caused by the additional number of steps needed for pruning .", "entities": [[1, 2, "MetricName", "EM"], [2, 3, "MetricName", "F1"], [43, 44, "DatasetName", "SST"]]}
{"text": "We show here the effect of the pattern on the head number reduction : using block instead of row / column pruning leads to a much larger number of pruned heads while improving accuracy , here on the SST - 2 task . We are using Block Movement pruning for each model , with different block patterns , pruning only the attention layers . Compression measures the reduction of the number of non - zero parameters in attention linear layers , whereas head compression measures the reduction of the number of complete non - zero heads .", "entities": [[33, 34, "MetricName", "accuracy"], [38, 39, "DatasetName", "SST"], [47, 49, "MethodName", "Movement pruning"], [61, 63, "HyperparameterName", "attention layers"]]}
{"text": "We select speed as our main metric to compare with other techniques , as it is the major practical measure of inference efficiency . On this metric , we decided to compare our models to the best models available i.e. the distilled models ( MobileBERT , TinyBERT ) , even though the method is different , as they are the strongest \" speed / accuracy \" baseline available . In Table 9 we compare with TinyBERT ( Jiao et al , 2019 ) and MobileBERT ( Sun et al , 2020 We compare as well to Hybrid pruning , with and without a teacher , with the unstructured methods from Sanh et al ( 2020 ) ( the original Movement Pruning method we are using ) and Gordon et al ( 2020 )", "entities": [[44, 45, "MethodName", "MobileBERT"], [64, 65, "MetricName", "accuracy"], [84, 85, "MethodName", "MobileBERT"], [119, 121, "MethodName", "Movement Pruning"]]}
{"text": "Automatic evaluation scores do not reflect the quality improvements . APE for MT has been using automatic metrics , such as BLEU , to benchmark progress ( Libovick\u1ef3 et al , 2016 ) . However , classic automatic evaluation metrics fail to capture the signal in human judgments for the proposed visual story post - editing task . We first use the human - edited stories as references , but all the automatic evaluation metrics generate lower scores when human judges give a higher rating ( Table 3 We then switch to use the human - written stories ( VIST test set ) as references , but again , all the automatic evaluation metrics generate lower scores even when the editing was done by human ( Table 4 . ) Table 5 further shows the Spearman rank - order correlation \u03c1 between the automatic evaluation scores ( sum of all six aspects ) and human judgment calculated using different data combination . In row { of Table 5 , the reported correlation \u03c1 of METEOR is consistent with the findings in Huang et al ( 2016 ) , which suggests that METEOR could be useful when comparing among stories generated by the same visual storytelling model . However , when comparing among machine - edited stories ( row y and | ) , among pre - and post - edited stories ( row z and } ) , or among any combinations of them ( row~ , and ) , all metrics result in weak correlations with human judgments . These results strongly suggest the need of a new automatic evaluation metric for visual story postediting task . Some new metrics have recently been introduced using linguistic ( Roemmele and Gor - don , 2018a ) or story features ( Purdy et al , 2018 ) to evaluate story automatically . More research is needed to examine whether these metrics are useful for story post - editing tasks too .", "entities": [[10, 11, "DatasetName", "APE"], [21, 22, "MetricName", "BLEU"], [99, 100, "DatasetName", "VIST"], [174, 175, "DatasetName", "METEOR"], [191, 192, "DatasetName", "METEOR"], [203, 205, "TaskName", "visual storytelling"]]}
{"text": "The approach we used to build acoustic word embeddings is inspired from the one proposed in ( Bengio and Heigold , 2014 ) . The deep neural architecture depicted in figure 1 is used to train the acoustic word embeddings . It relies on a convolutional neural network ( CNN ) classifier over words and on a deep neural network ( DNN ) trained by using a triplet ranking loss ( Bengio and Heigold , 2014 ; Wang et al , 2014 ; Weston et al , 2011 ) . The two architectures are trained using different inputs : speech signal and orthographic representation of the word , which are detailed as follows . The convolutional neural network classifier is trained independently to predict a word given a speech signal as input . It is composed of convolution and pooling layers , followed by fully connected layers which feed the final softmax layer . The embedding layer is the fully connected layer just below the softmax one , named s in the figure 1 . This representation contains a compact representation of the acoustic signal . It tends to preserve acoustic similarity between words , such that words are close in this space if they sound alike . The feedforward neural network ( DNN ) is used with the purpose to build an acoustic word embedding for a word not observed in the audio training corpus , based on its orthographic representation . It is trained using the triplet ranking loss function in order to project orthographic word representations to the same space as the acoustic embeddings s. The orthographic word representation consists in a bag of n - grams ( n \u2264 3 ) of letters , with additional special symbols [ and ] to specify the start and the end of a word . The size of this bag of ngrams vector is reduced using an auto - encoder . During the training process , this model takes as inputs acoustic embeddings s selected randomly from the training set and , for each signal acoustic embedding , the orthographic representation of the matching word o + , and the orthographic representation of a randomly selected word different to the first word o \u2212 . These two orthographic representations supply shared parameters in the DNN . The resulting DNN model can then be used to build an acoustic word embedding ( w + ) from any word , as long as one can extract an orthographic representation from it . This acoustic word embedding can be perceived as a canonical acoustic representation for a word , since different pronunciations imply different signal embeddings s.", "entities": [[7, 9, "TaskName", "word embeddings"], [38, 40, "TaskName", "word embeddings"], [69, 70, "MetricName", "loss"], [137, 138, "MethodName", "convolution"], [151, 152, "MethodName", "softmax"], [165, 166, "MethodName", "softmax"], [250, 251, "MetricName", "loss"]]}
{"text": "In the literature ( Kamper et al , 2015 ; Levin et al , 2013 ; Carlin et al , 2011 ) , a word discrimination task was used to evaluate acoustic embeddings s. Given a pair of acoustic segments , this task consists on deciding whether the segments correspond to the same words or not . This evalua - tion task can be performed on many ways , for example through the use of a dynamic time warping ( DTW ) to quantify the similarity between two segments when using frame level embeddings ( Thiolliere et al , 2015 ) , or by using the euclidean distance or the cosine similarity between embeddings representing the segments . In ( Kamper et al , 2015 ) the evaluation was conducted on two collections of words ( train and test ) coming from the Switchboard English corpus . After training the model on the training corpus , the cosine similarity is computed between the embeddings of each pair of words in the test set . These pairs are classified as similar or different by applying a threshold on their distance , and a precisionrecall curve is obtained by varying the threshold . In this study , we propose two approaches to evaluate acoustic word embeddings w + . We suggest to build different evaluation sets in order to assess the acoustic word embeddings ( w + ) performances on orthographic and phonetic similarity and homophones detection tasks . We remind that the acoustic word embedding w + is a projection of an orthographic word representation o + into the space of acoustic signal embeddings s. In our evaluation , we would like to measure the loss of orthographic information carried by w + and the potential gain of acoustic information due to this projection , in comparison to the information carried by o + . The evaluation sets are built as follows : given a list L of n frequent words ( candidate words ) in the vocabulary composed of m words , a list of n \u00d7 m word pairs was created . Then , two alignments were performed between each word pair based on their orthographic ( letters ) and phonetic ( phonemes ) representations , using the sclite 1 tool . From these alignment two edition distances are computed with respect to the alignment results of orthographic and phonetic representations . The Edition distance is computed as follows : SER = # In + # Sub + # Del # symbols in the ref erence word \u00d7 100 ( 1 ) where SER stands for Symbol Error rate , symbols correspond to the letters for orthographic representations , and to the phonemes for phonetic ones , and In , Sub and Del correspond respectively to insertion , substitution and deletion . Next , we compute two similarity scores that correspond to the orthographic and phonetic similarity scores sim score attributed for each pair of words , which are defined as : sim score = 10 \u2212 min ( 10 , SER/10 ) ( 2 ) where min ( ) is a function used to have an edition distance between 0 and 10 . Then , for each candidate word in the list L we extract its orthographically and phonetically 10 nearest words . This results in two lists for orthographic and phonetic similarity tasks . For each candidate word in the list L , the Orthographic list contains its ten closest words in terms of orthographic similarity scores and the Phonetic list contains its ten closest words in terms of phonetic similarity scores . Finally , the Homophones list , used for the homophone detection task , contains the homophone words ( i.e. sharing the same phonetic representation ) . Table 1 shows an example of the content of the three lists .", "entities": [[76, 79, "MethodName", "dynamic time warping"], [80, 81, "MethodName", "DTW"], [212, 214, "TaskName", "word embeddings"], [230, 232, "TaskName", "word embeddings"], [284, 285, "MetricName", "loss"], [438, 439, "MetricName", "Error"], [531, 532, "DatasetName", "0"]]}
{"text": "With the acquirement of the compressed local and global hyperbolic capsule set { u 1 , . . . , u P } in layer , let { v 1 , . . . , v Q } denote the label - aware hyperbolic capsule set in the next layer +1 , where Q equals to the number of labels . Following ( Sabour et al , 2017 ) , the compressed hyperbolic capsules are firstly transformed into a set of prediction capsules { \u00fb j | 1 , . . . , \u00fb j | P } for the j - th label - aware capsule , each of them is calculated by\u00fb j | i = W ij \u2297 u i B d , ( 10 ) where W ij is a learnable parameter . Then v j is calculated as a weighted M\u00f6bius summation over all the prediction capsules by v j = M u j | i { \u00fb j | 1 , ... , \u00fb j | P } c ij \u2297\u00fb j | i , ( 11 ) where c ij denotes the coupling coefficient that indicates the connection strength between\u00fb j | i and v j . The coupling coefficient c ij is iteratively updated during the HDR procedure and computed by the routing softmax c ij = exp ( b ij ) k exp ( b ik ) , ( 12 ) where the logits b ij are the log prior probabilities between capsule i and j , which are initialized as 0 . Once the label - aware hyperbolic capsules are produced , each b ij is then updated by b ij = b ij + K ( d B ( v j , \u00fb j | i ) ) , ( 13 ) where d B ( , ) denotes the Poincar\u00e9 distance , which can be written as d B ( u , v ) = cosh \u22121 ( 1 + 1 2 \u03bb u \u03bb v u \u2212 v 2 ) . ( 14 ) And K is a Epanechnikov kernel function ( Wand and Jones , 1994 ) with K = \u03b3 \u2212 x , x [ 0 , \u03b3 ) 0 , x \u2265 \u03b3 ( 15 ) where \u03b3 is the maximum Poincar\u00e9 distance between two points in the Poincar\u00e9 ball , which is d B ( p , 0 ) with p = 1 \u2212 ( = 10 \u22125 ) to avoid numerical errors . HDR is summarized in Algorithm 1 . Different from the routing procedure described in ( Sabour et al , 2017 ) , HDR does not require the squashing function since all the hyperbolic capsules are constrained in the Poincar\u00e9 ball .", "entities": [[221, 222, "MethodName", "softmax"], [260, 261, "DatasetName", "0"], [362, 364, "HyperparameterName", "K ="], [364, 365, "HyperparameterName", "\u03b3"], [370, 371, "DatasetName", "0"], [372, 373, "HyperparameterName", "\u03b3"], [374, 375, "DatasetName", "0"], [378, 379, "HyperparameterName", "\u03b3"], [383, 384, "HyperparameterName", "\u03b3"], [404, 405, "DatasetName", "0"]]}
{"text": "The large amount of labels in MLC is one major source of the computational complexity for the routing procedure . Since most of the labels are unrelated to a document , calculating the label - aware hyperbolic capsules for all the unrelated labels is redundant . Therefore , encoding based adaptive routing layer is used to efficiently decide the candidate labels for the document . The adaptive routing layer produces the candidate probability of each label by Initialize \u2200i , j : b ij 0 c = \u03c3 ( W c 1 T e i E e i + b c ) , ( 16 ) 3 : for r iterations do 4 : for all capsule i in layer and capsule j in layer + 1 : c ij softmax ( b ij ) Eq . 12 5 : for all capsule j in layer ( + 1 ) : v j M i c ij \u2297\u00fb j | i 6 : for all capsule i in layer and capsule j in layer + 1 : b ij b ij + K ( d B ( v j , \u00fb j | i ) ) 7 : return v j where \u03c3 denotes the Sigmoid function . W c and the bias b c are learnable parameters updated by minimizing the binary cross - entropy loss ( Liu et al , 2017 ) L c = \u2212 Q j=1 y j log ( c j ) + ( 1 \u2212 y j ) log ( 1 \u2212 c j ) , ( 17 ) where c j [ 0 , 1 ] is the j - th element in c and y j { 0 , 1 } denotes the ground truth about label j. The adaptive routing layer selects the candidate labels during test . Label - aware hyperbolic capsules are then constructed via HDR to predict probabilities of these candidate labels . During the training process , negative sampling is used to improve the the scalability of HYPERCAPS . Let N + denote the true label set and N \u2212 denote the set of randomly selected negative labels , the loss function is derived as L f = \u2212 j N + log ( a j ) + j N \u2212 log ( 1 \u2212 a j ) , ( 18 ) where a j = \u03c3 ( d B ( v j , 0 ) ) is activations of the j - th label - aware capsules , which is proportional to the distance from the origin of the Poincar\u00e9 ball .", "entities": [[84, 85, "DatasetName", "0"], [130, 131, "MethodName", "softmax"], [226, 227, "MetricName", "loss"], [269, 270, "DatasetName", "0"], [285, 286, "DatasetName", "0"], [363, 364, "MetricName", "loss"], [407, 408, "DatasetName", "0"]]}
{"text": "The proposed HYPERCAPS is evaluated on the four benchmark datasets by comparing with the six baselines in terms of P@k and nDCG@k with k = 1 , 3 , 5 . Results on all the labels averaged over the test instances are shown in Table 2 . nDCG@1 is omitted since it gives the same value as P@1 . It is notable that HYPERCAPS obtains competitive results on the four datasets . The encoding - based FASTTEXT is generally inferior to the other baselines as it applies the average pooling on word vector representations , which ig - nores word order for the construction of document representations . The typical MLC method SLEEC takes advantage of label correlations by embedding the label co - occurrence graph . However , SLEEC uses TF - IDF vectors to represent documents , thus word order is also ignored . XML - CNN uses a dynamic pooling technique to aggregate the local contextual features extracted by CNN , while SGM uses attention mechanism to aggregate the global contextual features extracted by LSTM . REGGNN is generally superior to both of them as it combines the local and global contextual information dynamically and takes label correlations into consideration using a regularized loss . However , the two capsulebased methods NLP - CAP and HYPERCAPS consistently outperform all the other methods owing to dynamic routing , which aggregates the fine - grained capsule features in a label - aware manner . Moreover , NLP - CAP only uses CNN to extract the local contextual information , while HYPER - CAPS benefits from the parallel combination of local and global contextual information . In addi - tion , NLP - CAP applies the non - linear squashing function for capsules in the Euclidean space , while HDR is designed for hyperbolic capsules , which take advantage of the representation capacity of the hyperbolic space . Therefore , HYPERCAPS outperforms NLP - CAP as expected . This result further confirms that the proposed HYPERCAPS with HDR is effective to learn the label - aware hyperbolic capsules for MLC .", "entities": [[23, 25, "HyperparameterName", "k ="], [57, 58, "MetricName", "P@1"], [76, 77, "MethodName", "FASTTEXT"], [88, 90, "MethodName", "average pooling"], [177, 178, "MethodName", "LSTM"], [206, 207, "MetricName", "loss"], [216, 217, "DatasetName", "CAP"], [249, 250, "DatasetName", "CAP"], [283, 284, "DatasetName", "CAP"], [324, 325, "DatasetName", "CAP"]]}
{"text": "In MLC , tail labels have low occurring frequency and hence are hard to predict compared to head labels . The performance on tail labels of the four benchmark datasets is evaluated in terms of nDCG@k with k = 1 , 3 , 5 . Figure 4 shows the results of the five deep learning based MLC methods , i.e. XML - CNN , SGM , REGGNN , NLP - CAP and HYPERCAPS . nDCG@1 is smaller than nDCG@3 on AAPD , RCV1 and ZHIHU since most of their test instances contain less than three tail labels . It is remarkable that HYPERCAPS outperforms all the other methods on tail labels . REGGNN takes advantage of the local and global contextual information and label correlations , thus it outperforms XML - CNN and SGM . The two capsule - based methods NLP - CAP and HYPERCAPS are both superior to the other methods , which indicates that the label - aware dynamic routing is effective for the prediction on tail labels . In addition , the fact that HYPERCAPS significantly improves the prediction performance compared to NLP - CAP implies that the representation capacity of the hyperbolic space and the combination of local and global contextual information are helpful for learning on tail labels . The results demonstrate the superiority of the proposed HYPERCAPS on tail labels for MLC .", "entities": [[37, 39, "HyperparameterName", "k ="], [70, 71, "DatasetName", "CAP"], [82, 83, "DatasetName", "RCV1"], [143, 144, "DatasetName", "CAP"], [188, 189, "DatasetName", "CAP"]]}
{"text": "An ablation test would be informative to analyze the effect of varying different components of the proposed HYPERCAPS , which can be taken apart as local Euclidean capsules only ( denoted as L ) , global Euclidean capsules only ( denoted as G ) , a combination of the local and global Euclidean capsules ( denoted as L + G ) , and a combination of the local and global hyperbolic capsules ( denoted as L + G + H ) . Euclidean capsules ( in L , G and L + G ) are aggregated via the origin dynamic routing ( Sabour et al , 2017 ) , while hyperbolic capsules ( in L + G + H ) are aggregated via our HDR . Figure 5 shows the results on EUR - LEX57 K in terms of P@k with k = 1 , 3 , 5 . In order to make the comparison fair , the number of total compressed capsules is equally set to 256 for all the four models . Adaptive routing is also applied with the maximum candidate label number set equally to 200 . Generally , the proposed combination of local and global contextual information contributes to the effectiveness of the model ( L + G ) . Therefore , it is practical to combine the local and global contextual information via dynamic routing . HDR furthermore improves the performance by making use of the representation capacity of the hyperbolic space . Overall , each of the components benefits the performance of HYPERCAPS for MLC . In summary , extensive experiments are carried out on four MLC benchmark datasets with various scales . The results demonstrate that the proposed HYPERCAPS can achieve competitive performance compared with the baselines . In particular , effectiveness of HYPERCAPS is shown on tail labels . The ablation test furthermore confirms that the combination of local and global contextual information is practical and HYPERCAPS benefits from the representation capacity of the hyperbolic space . 6 Related Work", "entities": [[141, 143, "HyperparameterName", "k ="]]}
{"text": "Multi - label classification ( MLC ) aims at assigning multiple relevant labels to one document . The MLC label set is large compared to Multi - class classification ( MCC ) . Besides , the correlations of labels ( e.g. hierarchical label structures ( Banerjee et al , 2019 ) ) and the existence of tail labels make MLC a hard task ( Bhatia et al , 2015 ) . As data sparsity and scalability issues arise with the large number of labels , XML - CNN ( Liu et al , 2017 ) employs CNN as efficient feature extractor , whereas it ignores label correlations , which are often used to deal with tail labels . The traditional MLC method SLEEC ( Bhatia et al , 2015 ) makes use of label correlations by embedding the label co - occurrence graph . The seq2seq model SGM ( Yang et al , 2018b ) uses the attention mechanism to consider the label correlations , while REGGNN ( Xu et al , 2019 ) applies a regularized loss specified for label co - occurrence . REGGNN additionally chooses to dynamically combine the local and global contextual information to construct document representations .", "entities": [[0, 4, "TaskName", "Multi - label classification"], [25, 29, "TaskName", "Multi - class classification"], [145, 146, "MethodName", "seq2seq"], [177, 178, "MetricName", "loss"]]}
{"text": "Capsule networks are recently proposed to address the representation limitations of CNN and RNN . The concept of capsule is first introduced by ( Hinton et al , 2011 ) . ( Sabour et al , 2017 ) replaces the scalar output features of CNN with vector capsules and pooling with dynamic routing . ( Hinton et al , 2018 ) proposes the EM algorithm based routing procedure between capsule layers . ( Gong et al , 2018 ) proposes to regard dynamic routing as an information aggregation procedure , which is more effective than pooling . ( Yang et al , 2018a ) and ( Du et al , 2019a ) investigate capsule networks for text classification . ( Zhao et al , 2019 ) then presents a capsule compression method and reformulates the routing procedure to fit for MLC . Our work is different from the predecessors as we design the Hyperbolic Dynamic Routing ( HDR ) to aggregate the parallel combination of local and global contextual information in form of hyperbolic capsules , which are constrained in the hyperbolic space without the requirement of non - linear squashing function . In addition , adaptive routing is proposed to improve the scalability for large number of labels .", "entities": [[63, 64, "MetricName", "EM"], [116, 118, "TaskName", "text classification"]]}
{"text": "Before going to the details of our method , we provide some background on the RL - based dialog manager in this section . Fig . 2 shows an overview of such dialog manager . We describe each of the parts briefly below . Feature Extraction At the t - th turn of a dialog , the user input u t is parsed into domain specific intents and slots to form a semantic frame a u t by a language understanding ( LU ) module . o u t and o s t\u22121 are the one - hot representations of such semantic frames for the current user input and the last system output respectively . Alternatively , o u t can be a simple n - grams representation of u t . But the vocabulary size is relatively large in real - world applications . It will yield slow convergence in the absence of a LU module . Based on the slot - value pair output with the highest probability , a query is sent to a database to retrieve user requested information . o db t is the one - hot representation of the database result . As a result , the observable information x t is the concatenation of o u t , o s t\u22121 and o db t . State Representation Based on the extracted feature vector x t and previous internal state s t\u22121 , recurrent neural networks ( RNNs ) are used to infer the latent representation of dialog state s t at step t. Current state s t can be interpreted as the summary of dialog history h t up to current step . Dialog Policy Next , the dialog state representation s t is fed into a policy network . The output \u03c0 ( a | h t ; \u03b8 ) of the policy network is a probability distribution over a predefined system action set A s . Lastly , the system samples an action a s t A s based on \u03c0 ( a | h t ; \u03b8 ) and receives a new observation x t+1 with an assigned reward r t . The policy parameters \u03b8 can be learned by maximizing the expected discounted cumulative rewards : J ( \u03b8 ) = E T \u2212t k=0 \u03b3 k r t+k ( 1 ) where T is the maximal step , and \u03b3 is the discount factor . Usually the parameters \u03b8 can be iteratively updated by policy gradient ( Williams , 1992 ) approach . The policy gradient can be empirically estimated as : \u2207 \u03b8 J ( \u03b8 ) \u2248 1 N N i=1 T t=1 \u2207 \u03b8 log \u03c0 ( a s i , t | hi , t ; \u03b8 ) ( Gi , t \u2212b ) ( 2 ) where N is the number of sampled episodes in a batch , G i , t = T \u2212t k=0 \u03b3 k r i , t+k is the sum of discounted reward at step t in the episode i , and b is a baseline to estimate the average reward of current policy .", "entities": [[307, 308, "HyperparameterName", "\u03b8"], [347, 348, "HyperparameterName", "\u03b8"], [366, 367, "HyperparameterName", "\u03b8"], [380, 381, "HyperparameterName", "\u03b8"], [387, 388, "HyperparameterName", "\u03b3"], [402, 403, "HyperparameterName", "\u03b3"], [411, 412, "HyperparameterName", "\u03b8"], [436, 437, "HyperparameterName", "\u03b8"], [439, 440, "HyperparameterName", "\u03b8"], [449, 450, "HyperparameterName", "\u03b8"], [463, 464, "HyperparameterName", "\u03b8"], [494, 495, "HyperparameterName", "\u03b3"]]}
{"text": "Although the ontology of the new system is different from the original one , the extended dialog manager can still reuse dialog policy of the illconsidered system circuitously . Given user logs D and the original dialog manager \u03c0 ( \u03b8 ) , we define a loss L ( \u03b8 ; D , \u03b8 ) to minimize the difference between new dialog manager \u03c0 ( \u03b8 ) and the old one : L ( \u03b8 ; D , \u03b8 ) = d D | d | t=1 KL ( \u03c0 ( a | ht ; \u03b8 ) | | \u03c0 ( a | ht ; \u03b8 ) ) ( 3 ) where \u03c0 ( a | h t ; \u03b8 ) and \u03c0 ( a | h t ; \u03b8 ) are the policy distributions over A s and A s given the same dialog history h t . | d | means turns of a specific dialog d D. To deal with unsupported user actions , A s will be a subset of A s . As a result , the KL term in equation ( 3 ) can be defined as follows : KL ( \u03c0 ( a | ht ; \u03b8 ) | | \u03c0 ( a | ht ; \u03b8 ) ) = | As | k=1 \u03c0 ( a k | ht ; \u03b8 ) log\u03c0 ( a k | ht ; \u03b8 ) \u2212 log\u03c0 ( a k | ht ; \u03b8 ) ( 4 ) As the original policy parameters \u03b8 are fixed , the loss function in equation ( 3 ) can be rewritten as : L ( \u03b8 ; D , \u03b8 ) = \u2212 d D | d | t=1 | As | k=1 \u03c0 ( a k | ht ; \u03b8 ) log\u03c0 ( a k | ht ; \u03b8 ) ( 5 ) This objective will transfer knowledge of the original system to the \" student \" at the turn level . Under the guidance of the original system , the extended system will be equipped with the primary strategy to complete a task .", "entities": [[2, 3, "MethodName", "ontology"], [40, 41, "HyperparameterName", "\u03b8"], [46, 47, "MetricName", "loss"], [49, 50, "HyperparameterName", "\u03b8"], [53, 54, "HyperparameterName", "\u03b8"], [65, 66, "HyperparameterName", "\u03b8"], [74, 75, "HyperparameterName", "\u03b8"], [78, 79, "HyperparameterName", "\u03b8"], [95, 96, "HyperparameterName", "\u03b8"], [105, 106, "HyperparameterName", "\u03b8"], [119, 120, "HyperparameterName", "\u03b8"], [129, 130, "HyperparameterName", "\u03b8"], [203, 204, "HyperparameterName", "\u03b8"], [213, 214, "HyperparameterName", "\u03b8"], [228, 229, "HyperparameterName", "\u03b8"], [237, 238, "HyperparameterName", "\u03b8"], [247, 248, "HyperparameterName", "\u03b8"], [257, 258, "HyperparameterName", "\u03b8"], [262, 263, "MetricName", "loss"], [276, 277, "HyperparameterName", "\u03b8"], [280, 281, "HyperparameterName", "\u03b8"], [301, 302, "HyperparameterName", "\u03b8"], [310, 311, "HyperparameterName", "\u03b8"]]}
{"text": "It 's easy for the developers to give logic rules on the system responses to handle new user actions . For example , if users ask to confirm a slot , the system should inform the value of that slot immediately . Note that these system actions which handle new user actions may not exist in the old model . It means the architecture of the new system is different from the old one . We define a set of logic constraints R = { ( h l , a l ) } L l=1 , where h l H R indicates the dialog context condition in the l - th rule , and a l A s is the corresponding system action . The number of logic rules L is equal to the number of new user actions . These rules can be seen as triggers : if dialog context h t in current turn t meets the context condition h l defined in logic rules , then the system should execute a l . In our work , we use the output of the LU module to judge whether the current dialog context meets the condition defined by logic rules . An alternative method is simple rules matching . To distill the knowledge of rules to a new system , we define a loss function L ( \u03b8 ; D , R ) to embed such constraints in the new system : L ( \u03b8 ; D , R ) = \u2212 d D | d | t=1 h l H R 1 { ht = h l } \u00d7 | A s | k=1 1 { a k = a l } log \u03c0 ( a k | ht ; \u03b8 ) ( 6 ) Where 1 { } is the indicate function . Equation ( 6 ) suggests the new dialog manager \u03c0 ( \u03b8 ) will be penalized if it violates the instructions defined by the dialog rules . Note that , for simplicity , we assume these rules are absolutely correct and mutually exclusive . Although this hypothesis may lead to a non - optimal dialog system , these rules define reasonable system actions to corresponding dialog contexts . It implies that the new system can be further refined by reinforcement learning once a new interaction environment is available .", "entities": [[225, 226, "MetricName", "loss"], [229, 230, "HyperparameterName", "\u03b8"], [246, 247, "HyperparameterName", "\u03b8"], [280, 282, "HyperparameterName", "k ="], [293, 294, "HyperparameterName", "\u03b8"], [318, 319, "HyperparameterName", "\u03b8"]]}
{"text": "In the absence of a new training environment , learning is made possible by exploiting structure that holds in the new dialog manager . On one hand , we expect the new system can complete tasks like the original one . On the other hand , it should satisfy the constraints defined by dialog rules . So , the learning objective of new dialog manager \u03c0 ( \u03b8 ) can be defined as follows : L ( \u03b8 ; D , \u03b8 , R ) = L ( \u03b8 ; D , R ) if ht HR ; L ( \u03b8 ; D , \u03b8 ) else ( 7 ) When the dialog context h t in the t - th turn satisfies a condition defined in H R , we distill knowledge of rules into the new system . Otherwise , we distill knowledge of the original system into the new one . Instead of retraining from scratch , developers can extend RL - based systems by reusing existing resources .", "entities": [[67, 68, "HyperparameterName", "\u03b8"], [77, 78, "HyperparameterName", "\u03b8"], [81, 82, "HyperparameterName", "\u03b8"], [88, 89, "HyperparameterName", "\u03b8"], [100, 101, "HyperparameterName", "\u03b8"], [104, 105, "HyperparameterName", "\u03b8"]]}
{"text": "For the original RL - based dialog system , a feature vector x t of size 191 is extracted . This vector is the concatenation of encodings of LU results , the previous system reply , database results and the current turn number . The LU module is implemented with an SVM 5 for intent detection and a CRF 6 for slot filling . The language generation module is implemented by a rule - based method . The hidden dialog state representation is inferred by a GRU ( Chung et al , 2014 ) . We set the hidden states of the GRU to be 120 . The policy network is implemented as a Multilayer Perceptron ( MLP ) with one hidden layer . The size of the hidden layer is 80 . The output dimension of policy network is 15 , which corresponds to the number of system actions . To encourage shorter interaction , we set a small per - turn negative reward R turn = \u22121 . The maximal turn is set to be 40 . If the user goal is satisfied , the policy will be encouraged by a large positive reward R succ = 10 ; otherwise the policy will be penalized by a negative reward R f ail = \u221210 . Discounted factor \u03b3 = 0.9 . The baseline b of current policy is estimated on sampled episodes in a batch . The batch size N is set to be 32 . Adadelta ( Zeiler , 2012 ) method is used to update model parameters . The original system S 1 is trained by interacting with Sim1 . After about 2400 interactions , the performance of S 1 starts to converge .", "entities": [[51, 52, "MethodName", "SVM"], [54, 56, "TaskName", "intent detection"], [58, 59, "MethodName", "CRF"], [61, 63, "TaskName", "slot filling"], [86, 87, "MethodName", "GRU"], [102, 103, "MethodName", "GRU"], [117, 118, "DatasetName", "MLP"], [219, 220, "HyperparameterName", "\u03b3"], [239, 241, "HyperparameterName", "batch size"], [248, 249, "MethodName", "Adadelta"]]}
{"text": "We also provide a count for the number of items in which ( within tables in Appendix Train a model on the pseudo - parallel data ; 5 Fine - tune the model on L ; 6 until convergence or maximum iteration ; 7 Reranking with reverse models In the syntax - semantics interface , the parsing task is usually to build a correct semantic ( or syntactic ) representation of a sentence . One can consider this task with respect to neural networks - which operate on sequences - straightforwardly by reversing the order of the parallel data : the source sequence ( meaning ) becomes the target , and the target sequence ( text ) becomes the source . Following the terminology of Li and White ( 2020 ) , we call such models reverse models , while models that generate text from meaning representations are forward models . 3 We can rerank the output of a forward model with the help of its corresponding reverse model . Given several outputs of a beam search of the forward model , we select the one that makes the best meaning representation if it is given to a reverse model as an input . Here , best means the one that has lowest perplexity with respect to forced decoding . One can combine self - training and reranking : Train forward and reverse models on the parallel data and then train forward and reverse models on the pseudo - parallel data . Afterwards finetune them again on the initial parallel data . Subsequently , use the reverse models to rerank the output of the forward models . Train forward and reverse models on the pseudo - parallel data ; 5 Fine - tune both models on L ; 6 until convergence or maximum iteration ;", "entities": [[213, 214, "MetricName", "perplexity"]]}
{"text": "On the challenge test , no model achieved perfect accuracy . The best performances are by RST - SM and RST - LG . Their performance is similar . It is worth noting that in the case of FACT - SM , reranking with self - training gave results comparable to RST - SM ( there is no significance difference in terms of Fisher 's test with significance at 5 % ) . This is not the case for FACT - LG and RST - LG models . RST - LG - ST - RMR outperforms the best FACT - LG model ( see Fig . 5 ) . From these experiments , we see that on the standard test set , RST - Large and RST - Small models performed best in terms of producing the correct discourse connective for SIMILARITY ( respectively CONTRAST ) . While errors occurred - sometimes matching the results of the corresponding FACT models - RST models correctly distinguish between producing the lexeme for SIMILARITY versus CONTRAST , while FACT models sometimes confuse SIMILARITY with CONTRAST . On the challenge data every model made errors . The RST models outperformed the corresponding FACT models , significantly in the case of RST - LG over RST - LG , as seen in Fig . 5 . Though the RST models yielded less dramatic improvements on comparisons in the challenge set , it is worth emphasizing that the RST models produce significantly fewer repetitions , omissions and hallucination compared to the FACT models ( Figs . 6 and 7 , Appendix C ) , further supporting the conclusion that the RST input produces better output . This result is interesting , since the content plans in the FACT models are shorter than those in RST models , yet still prompt the former models to produce more words than RST models do .", "entities": [[9, 10, "MetricName", "accuracy"]]}
{"text": "This paper proposes an improved custom model for WNUT task 2 : Identification of Informative COVID - 19 English Tweet . We improve experiment with the effectiveness of fine - tuning methodologies for state - of - the - art language model RoBERTa ( Liu et al , 2019 ) . We make a preliminary instantiation of this formal model for the text classification approaches . With appropriate training techniques , our model is able to achieve 0.9218 F1score on public validation set and the ensemble version settles at top 9 F1 - score ( 0.9005 ) and top 2 Recall ( 0.9301 ) on private test set .", "entities": [[42, 43, "MethodName", "RoBERTa"], [62, 64, "TaskName", "text classification"], [91, 94, "MetricName", "F1 - score"], [100, 101, "MetricName", "Recall"]]}
{"text": "Since the outbreak of COVID - 19 pandemic , frequently updated information becomes a huge problem of concern . Social media platforms consequently become real - time sources for news about flare - up data . In any case , the flare - up has been spreading quickly , we observe a monstrous amount of information on social networks , for example around 4 million COVID - 19 English Tweets every day on Twitter , in which most of these Tweets are uninformative . Therefore , it is crucial to collect the informative ones ( for example Corona Virus Tweets identified with new cases or dubious cases ) for downstream applications . In any case , manual ways to deal with recognizing useful Tweets require critical human endeavors , and hence are expensive . Based on the dataset provided in WNUT - 2020 Task 2 : Identification of informative COVID - 19 English Tweets ( Nguyen et al , 2020 ) , we propose a fine - tuning strategy to adopt the universal language model RoBERTa as an backbone model for text classification purposes . We also conduct several experiments in varied fine - tuning architectures on the pre - trained RoBERTa . Our best model results in a high F1 - score of 0.9005 on the task 's private test dataset and that of 0.9218 on the public validation set with Multilayer Perceptron Head .", "entities": [[175, 176, "MethodName", "RoBERTa"], [181, 183, "TaskName", "text classification"], [201, 202, "MethodName", "RoBERTa"], [210, 213, "MetricName", "F1 - score"]]}
{"text": "One of the most important parts in text classification problems is input representation . Traditional methods construct context - independent embeddings for words . Mikolov et al ( Mikolov et al , 2013 ) introduce an open - source Word2Vec , which consists of two models : Continuous Bag of Words ( CBOW ) and Skip - gram model . The models were trained on 1.6 billion words to learn linguistic contexts of words . While Word2Vec is a selfsupervised algorithm , GloVe ( Pennington et al , 2014 ) is trained unsupervised to form word embeddings . GloVe factorizes co - occurrence matrix of words , resulting in dense word vectors . However , both GloVe and Word2Vec fail representing rare or out - of - vocabulary words . FastText ( Mikolov et al , 2018 ) mitigates this problem by decomposing words as a sum of character n - grams . This handles unseen words very well because these character n - grams may still occur in other words . In contrast to context - independent embeddings , modern language models encode word semantics within contexts . Word vectors obtained from these methods achieve better results on downstream tasks because a word in different contexts expresses different meanings . Bidirectional Encoder Representations from Transformers ( Devlin et al , 2018 ) , or BERT for short , outperforms the previous best result with GLUE score of 80.4 % , which is 7.6 % improvement . There are two variants of BERT : base and large ; the large model is a stack of 24 Transformers ' encoders for a total of 340 M parameters while the base one has only 12 encoders . GPT - 2 ( Radford et al , 2019 ) by OpenAI is a gigantic model with 1.5 billion parameters and 48 layers , setting new state - of - the - art results on 7 out of 8 datasets . Face - book Research team improves training procedures for BERT , introducing RoBERTa ( Liu et al , 2019 ) . The improvements include extended training time on a ten - times bigger dataset , increased batch size , using byte - level encoding with larger vocabulary , excluding next sentence predicting task , and dynamic masking pattern modifying .", "entities": [[7, 9, "TaskName", "text classification"], [82, 83, "MethodName", "GloVe"], [95, 97, "TaskName", "word embeddings"], [98, 99, "MethodName", "GloVe"], [116, 117, "MethodName", "GloVe"], [130, 131, "MethodName", "FastText"], [225, 226, "MethodName", "BERT"], [235, 236, "DatasetName", "GLUE"], [252, 253, "MethodName", "BERT"], [285, 286, "MethodName", "GPT"], [335, 336, "MethodName", "BERT"], [338, 339, "MethodName", "RoBERTa"], [362, 364, "HyperparameterName", "batch size"]]}
{"text": "Figure 1 illustrates our process . For MLM tuning we propose hierarchical tuning process that consists of two steps : Domain adaptation using extra COVID data and Task adaptation using the given training data . After MLM Tuning , we utilize different training techniques for text classification such as back translation , warm - up learning rate , layer freezing and layer - wise learning rates . This section provides details of this pipeline .", "entities": [[7, 8, "DatasetName", "MLM"], [20, 22, "TaskName", "Domain adaptation"], [36, 37, "DatasetName", "MLM"], [45, 47, "TaskName", "text classification"], [55, 57, "HyperparameterName", "learning rate"]]}
{"text": "RoBERTa apparently is an excellent language model since it was trained on a huge dataset in a broad domain . However , the general domain is also a drawback when it comes to downstream tasks with completely different domains such as classifying users ' tweets on Twitter . Therefore , in order to produce high - quality outputs from the model , there is a need of fine - tuning MLM task on the task dataset for RoBERTa . This adapts the universal language model into our narrow domain , giving it prior knowledge for later classification training . Choosing learning rate is the key factor for the convergence . If learning rate is too small , the model may converge too slow causing harder to fit to new data distribution . On the other hand , large learning rate can lead to the problem of useful feature forgetting . Hence , we employ warm - up learning rate scheduler ( Howard and Ruder , 2018 ) to help the model converge faster while preserving its good initialization .", "entities": [[0, 1, "MethodName", "RoBERTa"], [70, 71, "DatasetName", "MLM"], [77, 78, "MethodName", "RoBERTa"], [100, 102, "HyperparameterName", "learning rate"], [111, 113, "HyperparameterName", "learning rate"], [138, 140, "HyperparameterName", "learning rate"], [157, 159, "HyperparameterName", "learning rate"]]}
{"text": "Layer freezing helps preserving useful knowledge that a pre - trained neural network has learned . Since RoBERTa has been trained on a huge dataset , we would not want the model to derive too far from its pre - train weights . The training procedure is divided into 2 steps : Step 1 : We freeze RoBERTa to train the classification head for the first epoch . Warm - up learning rate ( Section 3.2.1 ) is also applied . Because RoBERTa 's weights are already well trained , this step helps escape from narrow local optimum . Step 2 : RoBERTa is unfrozen , a whole network is trained . In RoBERTa , upper layers produce embeddings with more contextspecific than lower layers . This motivates us to further apply layer - wise learning rate : set a small learning rate for the shallowest layer , increase the learning rate as the layer goes deeper .", "entities": [[17, 18, "MethodName", "RoBERTa"], [57, 58, "MethodName", "RoBERTa"], [71, 73, "HyperparameterName", "learning rate"], [82, 83, "MethodName", "RoBERTa"], [102, 103, "MethodName", "RoBERTa"], [113, 114, "MethodName", "RoBERTa"], [135, 137, "HyperparameterName", "learning rate"], [141, 143, "HyperparameterName", "learning rate"], [150, 152, "HyperparameterName", "learning rate"]]}
{"text": "When training a huge neural network on a relatively small dataset , overconfidence is a problem leading to bad behaviours of the model . This phenomenon occurs when the model gives predictions with confidence higher than its accuracy . While there have been a lot of studies for overfitting reduction , overconfidence problem attracts less attention from researchers . In this study , we employ label smoothing ( Szegedy et al , 2015 ) to prevent model from being too certain about its predictions . Instead of assigning \" hard \" one - hot encoded ground truth , label smoothing adds a small perturbation into the label by a smoothing parameter \u03b1 . y k = y k ( 1 \u2212 \u03b1 ) + \u03b1 / K , where y k is output probabilities of K classes . Moreover , label smoothing also helps stabilize the training process . When using cross - entropy loss , one - hot encoded labels cause numerical instabilities if the prediction is close to one - hot form . In that case , the loss will become 1 log 0 = \u2212 . By setting \u03b1 = 0 , this problem can be solved .", "entities": [[37, 38, "MetricName", "accuracy"], [65, 67, "MethodName", "label smoothing"], [98, 100, "MethodName", "label smoothing"], [111, 112, "HyperparameterName", "\u03b1"], [114, 116, "HyperparameterName", "k ="], [121, 122, "HyperparameterName", "\u03b1"], [124, 125, "HyperparameterName", "\u03b1"], [140, 142, "MethodName", "label smoothing"], [154, 155, "MetricName", "loss"], [180, 181, "MetricName", "loss"], [185, 186, "DatasetName", "0"], [191, 192, "HyperparameterName", "\u03b1"], [193, 194, "DatasetName", "0"]]}
{"text": "Our set - up is proceeded as following instruction . We trained our networks with PyTorch framework on GPU GeForce GTX 2080Ti with batch size 32 for 20 epochs . We used AdamW ( Loshchilov and Hutter , 2017 ) for the optimization and a learning rate of 3e \u2212 5 , decayed 0.01 except for LayerNorm layers . Label smoothing hyperparameter \u03b1 was empirically experimented with multiple values of 0 , 0.1 , 0.15 , 0.2 and the last value possessed promising results . The numbers of hidden units of MLP Head and BiLSTM Head to 768 and 256 respectively .", "entities": [[23, 25, "HyperparameterName", "batch size"], [32, 33, "MethodName", "AdamW"], [45, 47, "HyperparameterName", "learning rate"], [59, 61, "MethodName", "Label smoothing"], [62, 63, "HyperparameterName", "\u03b1"], [70, 71, "DatasetName", "0"], [91, 92, "DatasetName", "MLP"], [94, 95, "MethodName", "BiLSTM"]]}
{"text": "Evaluation metrics for assessing are Accuracy , F1score , Recall and Precision metrics on public validation set . Accuracy can be used when the class distribution is similar while F1 - score is a better choice of metric when there are imbalanced classes . precision = T P T P + F P recall = T P T P + F N F 1 = 2 precision \u22121 + recall \u22121 , where T P : True Positive , F P : False Positive , F N : False Negative", "entities": [[5, 6, "MetricName", "Accuracy"], [9, 10, "MetricName", "Recall"], [11, 12, "MetricName", "Precision"], [18, 19, "MetricName", "Accuracy"], [29, 32, "MetricName", "F1 - score"]]}
{"text": "Table 1 compares the performance of multiple trial architectures training with pre - trained method using RoBERTa in our base settings . The original RoBERTa with MLP Head shows the better result than LSTM head , but the difference is not really noticeable ( 0.9082 vs. 0.9061 ) . When applying direct tuning MLM and label smoothing , the gap has been widened , specifically , 0.9218 for MLP Head and 0.9170 for LSTM Head . In the table , hierarchical tuning and back translation method did not yield better results than direct tuning without back translation one . Nevertheless , we expect this method can generalize well on many different distributed datasets and thus , we ensembled the two versions with voting and submitted to the private test benchmark . We ended up at top 9 on the leaderboard with 0.9005 F1 - score and", "entities": [[16, 17, "MethodName", "RoBERTa"], [24, 25, "MethodName", "RoBERTa"], [26, 27, "DatasetName", "MLP"], [33, 34, "MethodName", "LSTM"], [53, 54, "DatasetName", "MLM"], [55, 57, "MethodName", "label smoothing"], [68, 69, "DatasetName", "MLP"], [73, 74, "MethodName", "LSTM"], [142, 145, "MetricName", "F1 - score"]]}
{"text": "While automatic evaluation metrics are very important and invaluable tools for rapid development of machine translation ( MT ) systems , they are only a substitution for human assessment of translation quality . Various methods have been proposed and used for the human evaluation of MT quality by assigning overall scores to MT outputs , such as ( AL - PAC , 1966 ; White et al , 1994 ; Koehn and Monz , 2006 ; Callison - Burch et al , 2007 ; Roturier and Bensadoun , 2011 ; Graham et al , 2013 ; Barrault et al , 2019 ) , and all of them rely on at least one of the three translation quality criteria : comprehensibility ( comprehension , intelligibility ) , adequacy ( fidelity , semantic accuracy ) , and fluency ( grammaticality ) . Comprehensibility reflects the degree to which a translated text can be understood , adequacy reflects the degree to which the translation conveys the meaning of the original text in the source language , and fluency reflect the grammar of the translated text . The raters are usually asked to assign an overall score for the given translation criterion . In order to get more details about translation performance , error classification and analysis emerged in the field of MT ( Vilar et al , 2006 ; Lommel et al , 2014 ; Klubi\u010dka et al , 2018 ; Van Brussel et al , 2018 ) . However , there is less work dealing with human perception of MT quality and errors . For statistical phrase - based MT systems ( SMT ) , Kirchhoff et al ( 2014 ) and Federico et al ( 2014 ) were identifying error types which are mostly disliked by readers . In the last five years , systems based on artificial neural networks ( NMT ) have become the new state of the art . Several evaluation studies , such as ( Castilho et al , 2017 ; Klubi\u010dka et al , 2018 ; Van Brussel et al , 2018 ) reported that these systems are able to produce more fluent and readable translations , but that they are still sufferring from adequacy issues . In addition , many participants mentioned that good fluency of NMT outputs makes it more difficult to spot adequacy errors such as omissions or mistranslations . Such \" fluently inadequate \" errors may mislead readers into trusting the content based on fluency alone , especially when surrounded by fluent and adequate parts of a text ( Martindale and Carpuat , 2018 ) . Automatic identification of such errors for both SMT and NMT systems has been investigated in ( Martindale et al , 2019 ) and it is confirmed that these errors appear much more often in NMT system . To the best of our knowlegde , comprehensibility , while being a very important translation quality factor , has not been investigated in depth yet . It should be stressed that comprehensibility is very different from fluency - a fluent text can be incomprehensible ( for example \" Colorless green ideas sleep furiously . \" ) , and vice versa ( for example \" All these experiment was carry out this year . \" ) . Our main research questions are : RQ1 Are there \" comprehensible inadequate \" translations which are misleading human readers so that they fully trust the MT output despide adequacy errors ? In other words : how many adequacy errots are perceived as comprehensible ? RQ2 If the answer to the RQ1 is \" yes \" , which types of translation errors are mainly related to these translations ? As a first step , a group of evaluators annotated problematic parts of the given machine translated text . They were not asked to assign any error labels , only to mark the parts of the text which they perceived as problematic for the given translation criterion . They first annotated all comprehensibility issues , and after about two weeks , all adequacy issues . For each criterion , they were asked to distinguish major and minor issues . We then analysed all major issues in order to examine relations between comprehensibility and adequacy and identify error types . The analysis was carried out on English user reviews ( as a case of \" mid - way \" genre between formal and informal written language ) translated into Croatian and Serbian ( as a case of mid - size less - resourced morphologically rich European languages ) . It is worth noting that the aim of this work is not to compare MT systems , nor to estimate their overall performance for the given language pairs and domain in order to potentially improve them . The aim of this work is to explore relations between two aspects of human perception of translation quality .", "entities": [[14, 16, "TaskName", "machine translation"], [131, 132, "MetricName", "accuracy"]]}
{"text": "We propose an LSTM - based model with hierarchical architecture on named entity recognition from code - switching Twitter data . Our model uses bilingual character representation and transfer learning to address out - of - vocabulary words . In order to mitigate data noise , we propose to use token replacement and normalization . In the 3rd Workshop on Computational Approaches to Linguistic Code - Switching Shared Task , we achieved second place with 62.76 % harmonic mean F1 - score for English - Spanish language pair without using any gazetteer and knowledge - based information .", "entities": [[3, 4, "MethodName", "LSTM"], [11, 14, "TaskName", "named entity recognition"], [28, 30, "TaskName", "transfer learning"], [79, 82, "MetricName", "F1 - score"]]}
{"text": "In this section , we describe our model architecture and hyper - parameters setting . Bilingual Char - RNN : This is one of the approaches to learn character - level embeddings without needing of any lexical hand - crafted features . We use an RNN for representing the word with character - level information ( Lample et al , 2016 ) . Figure 1 shows the model architecture . The inputs are characters extracted from a word and every character is embedded with d dimension vector . Then , we use it as the input for a Bidirectional LSTM as character encoder , wherein every time step , a character is input to the network . Consider a t as the hidden states for word t. a t = ( a 1 1 , a 2 t , ... , a V t ) where V is the character length . The representation of the word is obtained by taking a V t which is the last hidden state . Figure 2 presents the overall architecture of the system . The input layers receive word and character - level representations from English and Spanish pre - trained Fast - Text word vectors and Bilingual Char - RNN . Consider X as the input sequence : X = ( x 1 , x 2 , ... , x N ) where N is the length of the sequence . We fix the word embedding parameters . Then , we concatenate both vectors to get a richer word representation u t . Afterwards , we pass the vectors to bidirectional LSTM . u t = x t a t \u2212 h t = \u2212 \u2212\u2212\u2212 LSTM ( u t , \u2212 \u2212 h t\u22121 ) , \u2212 h t = \u2212\u2212\u2212 \u2212 LSTM ( u t , \u2212 \u2212 h t\u22121 ) c t = \u2212 h t \u2212 h t where denotes the concatenation operator . Dropout is applied to the recurrent layer . At each time step we make a prediction for the entity of the current token . A softmax function is used to calculate the probability distribution of all possible named - entity tags . y t = e ct T j=1 e c j , where j = 1 , .. , T where y t is the probability distribution of tags at word t and T is the maximum time step . Since there is a variable number of sequence length , we padded the sequence and applied mask when calculating cross - entropy loss function . Our model does not use any gazetteer and knowledge - based information , and it can be easily adapted to another language pair .", "entities": [[98, 100, "MethodName", "Bidirectional LSTM"], [268, 270, "MethodName", "bidirectional LSTM"], [284, 285, "MethodName", "LSTM"], [301, 302, "MethodName", "LSTM"], [326, 327, "MethodName", "Dropout"], [351, 352, "MethodName", "softmax"], [429, 430, "MetricName", "loss"]]}
{"text": "We trained our LSTM models with a hidden size of 200 . We used batch size equals to 64 . The sentences were sorted by length in descending order . Our embedding size is 300 for word and 150 for characters . Dropout ( Srivastava et al , 2014 ) of 0.4 was applied to all LSTMs . Adam Optimizer was chosen with an initial learning rate of 0.01 . We applied time - based decay of \u221a 2 decay rate and stop after two consecutive epochs without improvement . We tuned our model with the development set and evaluated our best model with the test set using harmonic mean F1 - score metric with the script provided by Aguilar et al ( 2018 ) .", "entities": [[3, 4, "MethodName", "LSTM"], [14, 16, "HyperparameterName", "batch size"], [42, 43, "MethodName", "Dropout"], [58, 59, "MethodName", "Adam"], [59, 60, "HyperparameterName", "Optimizer"], [65, 67, "HyperparameterName", "learning rate"], [79, 81, "HyperparameterName", "decay rate"], [110, 113, "MetricName", "F1 - score"]]}
{"text": "Our criteria and basis for evaluating measurements of originality are : 1 . Can we tell whether a generated sentence is an original use of language ? 2 . Can we tell whether the sentence contains a fragment from the ground truth that is a candidate for protection as intellectual property ? Therefore , when measuring generation originality by comparing the generated sentence with the sentences in the ground truth , then the answers to numbers 1 and 2 above are binary . Either the generated sentence is an original use of language or it is not . Either the generation is at risk of plagiarism or it is not . However , if we consider that the ground truth may not be representative of all the sentences that have ever been generated , then there is a measure of uncertainty that may be added to the binary outcome . There are no standard automatic measures for novelty and originality in stylized language generation ( Mou and Vechtomova , 2020 ) . High perplexity ( PPL ) and a low BLEU ( Papineni et al , 2002 ) score may suggest novelty , but they are not sufficient for testing for originality . High PPL and a low BLEU score may be achieved when there is little overlap between the generated language and the ground truth , but nonesense and off - topic sentences are rewarded . While nonesense sentences may be novel , they may be grammatically incorrect , and sentences that are grammatically correct will likely have some overlap with fragments ( n - grams ) in the ground truth , such as using phrases like \" she said that \" . So , we want a generation originality test that does n't penalize n - gram overlap . ( An original use of language may combine common n - grams in a new way . ) We also want a generation originality test that flags potential plagiarism of original fragments in the ground truth , which neither BLEU nor PPL does . We propose a generation originality test ( GOT ) that addresses original uses of language . It does not necessarily address original ideas . GOT is equally appropriate for stylized text generation , where novelty is desirable , and for other generation tasks where there is not an imposed style but the generation is open - ended , including summarization tasks .", "entities": [[173, 174, "MetricName", "perplexity"], [180, 181, "MetricName", "BLEU"], [208, 210, "MetricName", "BLEU score"], [340, 341, "MetricName", "BLEU"], [375, 377, "TaskName", "text generation"], [404, 405, "TaskName", "summarization"]]}
{"text": "The following complexity analysis is with respect to Algorithm 1 . We are representing F and O with balanced binary search trees ( e.g. , red - black tree ( Guibas and Sedgewick , 1978 ; OKASAKI , 1999 ) ) where the comparator is lexicographic ordering . Searching , insertion and deletion in such trees take \u03b8 ( log n ) comparisons . Since the length of fragments is assumed to be constant on average , then each comparison takes constant time , implying that each search / insert / delete operation in O and F take \u03b8 ( log n ) time . Given our representation of F and O with balanced binary search trees , consider the following time complexity analysis : Let n = number of sentences in the dataset . The first for - loop ( line 1 ) iterates n times . Let c = the average length ( i.e. , number of tokens ) of a sentence in our ground truth . We found that c = 25 , a fairly small constant . Therefore , the two for - loops in Steps 4 and 5 iterate on average a constant number of times . The binary search in F ( line 10 ) has a runtime complexity of \u03b8 ( log n ) . Depending on the result of the binary search of F ( line 10 ) there may be an insertion to F ( line 14 ) which has a runtime complexity of \u03b8 ( log n ) . Then the number of calculations in lines 1 - 20 is the following function of n : 2c 2 n log n. The code segment of lines 21 - 26 takes \u03b8 ( n ) time because the number of wl - token fragments in the ground truth dataset ( of n sentences where each sentence consists of c tokens on average ) is at most cn . Therefore , the runtime complexity is : \u03b8 ( n log n ) . This algorithm would be executed before generation tasks , but may also be executed whenever the Algorithm 1 Find Original Fragments in the Ground Truth Require : Input S , the sentences in the ground truth to evaluate Require : Input F , list of fragments already discovered , may be empty set ; Require : Input CountP erF rag ( f ) , for all f F Require : O , list of original fragments Count per o O should always be 1 1 : for each s S do 2 : l = number of tokens in sentence s 3 : sentP arts = set of tokens in s 4 : for each wl in range 2 to l do wl = length of window 5 : for each i in range 0 to l \u2212 wl + 1 do assume zero - based indexing 6 : if sentP arts [ i ] or sentP arts [ i + wl \u2212 1 ] = special token 2 then 7 : Continue to next i 8 : else 9 : f rag = sentP arts [ i : i + wl ] 10 : if f rag F then binary search of F 11 : CountP erF rag [ f rag ] = CountP erF rag [ f rag ] + 1 12 : Break from for - loop in line 5 13 : else frag was not found in F 14 : Add f rag to F 15 : CountP erF rag [ f rag ] = 1 16 : end if 17 : end if 18 : end for 19 : end for 20 : end for 21 : Set O to the empty set ; 22 : for each f rag in F do 23 : if CountP erF rag [ f rag ] = = 1 then 24 : Add f rag to O ; 25 : end if 26 : end for reference set changes or is updated ( for example , based on generated language ) .", "entities": [[57, 58, "HyperparameterName", "\u03b8"], [98, 99, "HyperparameterName", "\u03b8"], [216, 217, "HyperparameterName", "\u03b8"], [253, 254, "HyperparameterName", "\u03b8"], [290, 291, "HyperparameterName", "\u03b8"], [334, 335, "HyperparameterName", "\u03b8"], [475, 476, "DatasetName", "0"]]}
{"text": "Question Difficulty Estimation ( QDE ) , also referred to as \" calibration \" , is a crucial task in education . Indeed , since question difficulty can be leveraged to assess the skill level of students under examination , wrongly calibrated questions are cause of erroneous estimations . Also , an accurate calibration enables to detect and discard questions that are too easy or too difficult for certain students . Traditionally , QDE is performed manually or with pretesting . Manual calibration is intrinsically subjective and inconsistent . Pretesting leads indeed to an accurate and consistent calibration , but i ) introduces a long delay between question generation and when the question can be used to score students , and ii ) requires to deploy the new questions before actually using them for scoring . To address the issues of the traditional approaches to calibration , recent research tried to leverage Natural Language Processing ( NLP ) techniques to perform QDE from question text : the idea is to use some pretested questions to train a model that performs QDE from text and thus eliminates ( or at least reduces ) the need for pretesting of new questions . Although such works showed promising results , none of them experimented with the latest NLP research , such as Transformer - based models ( Vaswani et al , 2017 ) . In this work , we aim at filling that gap , studying how different Transformers , also trained with different approaches , compare with the current state of the art . We evaluate several models built upon the pre - trained BERT ( Devlin et al , 2019 ) and DistilBERT ( Sanh et al , 2019 ) language models on the publicly available ASSISTments dataset and the private CloudAcademy dataset . By using data from two different domains , we add to the growing body of evidence showing that item text can be used to perform QDE . More precisely , we show that - after being fine - tuned on the task of QDE from text - Transformer models are capable of calibrating newly generated questions more accurately than previously proposed approaches . On top of that , we explore the possibility of leveraging additional textual information which might be available ( e.g. transcript of video lectures ) to perform an additional pre - training of the Transformers before fine - tuning them for QDE , and show that such approach can be used to further improve the accuracy . Overall , Transformer - based models are capable of reducing the RMSE by up tp 6.5 % , with respect to previous approaches . Lastly , we perform an analysis of the best performing model to under - stand whether some question characteristics ( e.g. text length , question type ) particularly affect its performance . Code is available at https://github . com / aradelli / transformers - for - qde .", "entities": [[107, 109, "TaskName", "question generation"], [219, 220, "MethodName", "Transformer"], [272, 273, "MethodName", "BERT"], [281, 282, "MethodName", "DistilBERT"], [350, 351, "MethodName", "Transformer"], [421, 422, "MetricName", "accuracy"], [425, 426, "MethodName", "Transformer"], [434, 435, "MetricName", "RMSE"]]}
{"text": "There is a large interest in understanding how textual features affect item difficulty ( El Masri et al , 2017 ; Hickendorff , 2013 ) , and this is not limited to the educational domain : for instance Wang et al ( 2014 ) focus on difficulty estimation in community question answering systems . The first works addressing QDE from text focused on MCQs and used deterministic approaches based on bag of words and similarities between question , answer , and distractors ( Alsubait et al , 2013 ; Yaneva et al , 2018 ; Kurdi et al , 2016 ) . Recent works mostly use machine learning approaches . Huang et al ( 2017 ) proposed a neural network for QDE of \" reading \" problems , in which the answer has to be found in a text provided together with the question . The model receives as input both the text of the question and the document , thus it actually estimates the difficulty of finding the correct answer in the provided document . This is a major difference from all other works , in which the difficulty depends on the questions only . Yaneva et al ( 2019 ) introduced a model to estimate the p - value of questions from text . The p - value of a question is defined as the fraction of students who correctly answered it and does not account for different skill levels . This model was trained using the text of questions and a large dataset of medical documents , thus it can not be used if an analogous dataset is not available . Similarly , the model proposed in ( Qiu et al , 2019 ) requires for training a dataset of medical documents in addition to the question texts . The model is made of two neural architectures to estimate the wrongness ( i.e. 1 \u2212 p - value ) of newly - generated MCQs , considering it as made of two components which indicate i ) how difficult it is to choose between the possible choices , and ii ) how difficult it is to recall the knowledge required to answer the question . Benedetto et al ( 2020b ) proposed R2DE , a model that estimates the difficulty and discrimination , as defined in Item Response Theory ( IRT ) ( Hambleton et al , 1991 ) , of newly generated MCQs . R2DE computes TF - IDF features from the text of the questions and the text of the possible choices , and feeds two Random Forest regressors with such features . The performance of R2DE was improved in ( Benedetto et al , 2020a ) , with the addition of readability and linguistics features . To the best of our knowledge , ( Xue et al , 2020 ) is the first work that explored the effects of transfer learning for QDE from text . Specifically , the authors fine - tune pre - trained ELMo embeddings ( Peters et al , 2018 ) for the task of response time prediction , and subsequently perform a second finetuning for the task of p - value estimation . Differently from all the other models , this one and R2DE can be trained using only question text , without needing an additional dataset of related documents . Our approach differs from previous research in several ways . First of all , we adopt transfer learning on Transformers , which are yet to be explored for QDE from text . Secondly , similarly to ( Benedetto et al , 2020b , a ) , we perform the estimation of the IRT difficulty which , differently from wrongness and p - value , models students ' skill levels as well . Specifically , we consider the one - parameter model ( Rasch , 1960 ) , a logistic model that associates a skill level to each student and a difficulty level to each question ( both represented as scalars ) . A brief introduction to the one - parameter IRT model is given in Appendix A. Thirdly , the Transformer models presented here do not necessarily require an additional dataset of documents from the same topics as the questions , and they can be trained using only question texts ; however , if such dataset is available , it can be leveraged to further improve the accuracy of calibration . Lastly , each previous work experimented on one private dataset ; we experiment on two datasets ( one being publicly available ) , showing that Transformers can be successfully used for QDE in different domains .", "entities": [[49, 52, "TaskName", "community question answering"], [484, 486, "TaskName", "transfer learning"], [501, 502, "MethodName", "ELMo"], [577, 579, "TaskName", "transfer learning"], [691, 692, "MethodName", "Transformer"], [738, 739, "MetricName", "accuracy"]]}
{"text": "BERT ( Devlin et al , 2019 ) is a pre - trained language model that reached state of the art performance in many language tasks . Its key technical innovation was the application of the Transformer , a popular self - attention model ( Vaswani et al , 2017 ) , to language modeling . BERT is originally trained to address two tasks : Masked Language Modeling ( MLM ) and Next Sentence Prediction ( NSP ) . MLM consists in removing one word from the input text and asking the model to fill the gap , while in NSP the model is asked - given two input sentences - to tell whether the second sentence is a reasonable continuation to the first one . Crucially , BERT can be used for many different downstream tasks , as we do here for QDE : starting from the pre - trained model , it is sufficient to stack an additional layer on top of the original network and then retrain it on the desired task ( process named \" finetuning \" ) . During fine - tuning , the internal weights of the pre - trained model are updated and adapted to the desired task ( together with the weights of the added layer ) , which is both more efficient than training the whole network from scratch and capable of better results , as the knowledge of the pre - trained model is not discarded . BERT is a large model and therefore requires many resources for training and fine - tuning . For this reason , we also experiment with DistilBERT ( Sanh et al , 2019 ) , which is a language model obtained by distilling BERT . Knowledge distillation is a compression technique in which a small model is trained to reproduce the full output distribution of a larger model ( Hinton et al , 2015 ) . With this approach , DistilBERT is able to retain 95 % of BERT 's performance on several language understanding tasks using about half the number of parameters of BERT . Similarly to BERT , Distil - BERT can be fine - tuned on downstream tasks and it is therefore worth exploring for QDE from text .", "entities": [[0, 1, "MethodName", "BERT"], [36, 37, "MethodName", "Transformer"], [56, 57, "MethodName", "BERT"], [65, 68, "TaskName", "Masked Language Modeling"], [69, 70, "DatasetName", "MLM"], [79, 80, "DatasetName", "MLM"], [128, 129, "MethodName", "BERT"], [247, 248, "MethodName", "BERT"], [272, 273, "MethodName", "DistilBERT"], [289, 290, "MethodName", "BERT"], [291, 293, "MethodName", "Knowledge distillation"], [326, 327, "MethodName", "DistilBERT"], [334, 335, "MethodName", "BERT"], [346, 349, "HyperparameterName", "number of parameters"], [350, 351, "MethodName", "BERT"], [354, 355, "MethodName", "BERT"], [358, 359, "MethodName", "BERT"]]}
{"text": "Before comparing the Transformer models with the state of the art , we show here the performance of the different configurations , both with and without the additional pre - training on MLM . Table 1 displays the Mean Absolute Error ( MAE ) and the Root Mean Squared Error ( RMSE ) of the different configurations ; the error is the difference between the IRT difficulty and the estimation of the Transformer model . The input configurations are the ones pre - : i ) Q only , ii ) Q+correct , iii ) Q+all . We show the mean of the errors , measured over three independent runs with different random initializations . The results shown here are obtained on Q TEST for CloudAcademy ; indeed , for ASSISTments the text of the possible choices is not avail - able and therefore the only possible input configuration is Q only . Even though there is not one model configuration which clearly outperforms all the others , it can be seen that both the additional pre - training and the textual information of the possible choices are helpful in improving the accuracy of the estimation . For DistilBERT without the additional MLM pre - training the best configuration is Q+all , while in all the other cases the best input configuration is Q+correct .", "entities": [[3, 4, "MethodName", "Transformer"], [32, 33, "DatasetName", "MLM"], [40, 41, "MetricName", "Error"], [42, 43, "MetricName", "MAE"], [49, 50, "MetricName", "Error"], [51, 52, "MetricName", "RMSE"], [72, 73, "MethodName", "Transformer"], [192, 193, "MetricName", "accuracy"], [198, 199, "MethodName", "DistilBERT"], [202, 203, "DatasetName", "MLM"]]}
{"text": "As baselines , we use : i ) ZeroR : predicts for all the questions the average difficulty of the training questions . ii ) R2DE ( Benedetto et al , 2020b ) : performs difficulty estimation in two steps : the input texts are converted into feature arrays with TF - IDF and then the feature arrays are given as input to a Random Forest regression model , that performs the actual estimation . We implemented R2DE using the available code 9 . iii ) ELMo ( Xue et al , 2020 ) : we re - implement this model to adapt it to our experimental datasets , and show here the results obtained with all the input configurations ( on our dataset , the best performing input configuration is different than the one in the original paper ) . Table 2 and Table 3 show the results of the experiments on QDE for CloudAcademy and ASSISTments , by displaying the MAE and the RMSE obtained on Q TEST by the Transformer models and the chosen baselines ( all the possible input configurations are considered for the baselines ) . For each Transformer model , only one input configuration is shown , as obtained in subsection 7.1 . In order to understand how well the models estimate the difficulty of very easy and very challenging questions , we also show the MAE and RMSE measured on the questions whose difficulty b is such that | b | > 2 ( also referred to as \" extreme \" questions ) . The results shown are the mean and the standard deviation of the errors over three independent runs . Table 2 shows that also R2DE and ELMo are able to leverage the text of the possible choices to improve the accuracy of the estimation ; indeed , the best performing input configuration is Q+all for R2DE and Q+correct for ELMo . The table shows that the Transformer models generally outperform the proposed baselines on both metrics , both with and without the additional pre - training on MLM . The model that consistently and significantly outperforms all the others is BERT with Q+correct and the additional MLM pre - training . It is also interesting to remark that ELMo seems to perform estimations a bit biased towards high and low difficulties : indeed , considering overall MAE and RMSE it performs at the same level of R2DE but it is better for the estimation of \" extreme \" questions . This might also be the reason why ELMo performs better than DistilBERT without MLM on \" extreme \" questions but worse overall . All models have larger errors on \" extreme \" questions than on general ones , but the increase is different for each of them : the best performing model has an increase in the MAE of 1.19 , which is lower than the other Transformers ( from 1.28 to 1.41 ) , ELMo ( 1.37 ) and R2DE ( 1.52 ) . Results are similar for the RMSE : the increase is 1.19 for the best model , between 1.19 and 1.30 for the other Transformers , 1.23 for ELMo ( Q+correct ) and 1.37 for R2DE ( Q+all ) . Table 3 shows results similar to Table 2 : BERT is the best performing model and both Transformer models outperform the baselines . However , we can see that the errors are larger than in the previous experiment , thus suggesting that all the models are less capable at estimating the difficulty of the questions in ASSISTments . There could be several reasons for this , but we believe that this limitation is mainly due to two aspects : i ) the platform allows the creation of question with images ( not available to us ) ; ii ) the language used in the dataset is \" less natural \" than in CloudAcademy ( e.g. many questions are equations with no additional text ) .", "entities": [[86, 87, "MethodName", "ELMo"], [162, 163, "MetricName", "MAE"], [165, 166, "MetricName", "RMSE"], [172, 173, "MethodName", "Transformer"], [193, 194, "MethodName", "Transformer"], [232, 233, "MetricName", "MAE"], [234, 235, "MetricName", "RMSE"], [286, 287, "MethodName", "ELMo"], [300, 301, "MetricName", "accuracy"], [319, 320, "MethodName", "ELMo"], [326, 327, "MethodName", "Transformer"], [347, 348, "DatasetName", "MLM"], [360, 361, "MethodName", "BERT"], [366, 367, "DatasetName", "MLM"], [378, 379, "MethodName", "ELMo"], [396, 397, "MetricName", "MAE"], [398, 399, "MetricName", "RMSE"], [427, 428, "MethodName", "ELMo"], [431, 432, "MethodName", "DistilBERT"], [433, 434, "DatasetName", "MLM"], [477, 478, "MetricName", "MAE"], [495, 496, "MethodName", "ELMo"], [510, 511, "MetricName", "RMSE"], [532, 533, "MethodName", "ELMo"], [553, 554, "MethodName", "BERT"], [561, 562, "MethodName", "Transformer"]]}
{"text": "We analyze here some of the characteristics of the best performing model ( i.e. BERT ) , trying to understand whether there are some question properties which particularly influence its accuracy . We perform the same analysis for R2DE , to understand whether such characteristics are a peculiarity of BERT or are shared among different models ; the choice of R2DE is motivated by the fact that it is the second best performing model on the CloudAcademy dataset ( excluding the other Transformer models ) and it uses TF - IDF to create the features , thus a non - neural approach . We report here the results of three analyses , studying possible differences in the accuracy of QDE depending on i ) input length and question difficulty , ii ) number of correct choices , and iii ) whether it is a cloze question . First of all , in Figure 3 we show for the two datasets the distribution of questions depending on the input length and the true difficulty ; the number of questions in each bin is represented by its color . The two figures show that the distribution is far for uniform and in many areas there are not enough questions to obtain significant results from this analysis . We do not show here the distribution of CloudAcademy for the Q+all input config - uration , which is the one used by R2DE , but it is qualitatively similar to the ones displayed . Considering this distribution , we decided to focus only on some of the questions while analyzing the error depending on the input length and the true difficulty . Specifically , we keep questions i ) with | b | < 3 and len < = 110 for CloudAcademy and BERT ( 96.4 % of the questions ) ; ii ) with | b | < 3 and len < = 110 for CloudAcademy and R2DE ( 96.9 % ) ; iii ) with | b | < 4 and len < = 80 for ASSISTments ( 94.3 % ) , there is no difference between BERT and R2DE because they use the same input configuration . Figure 4 shows how the estimation error ( represented by the color ) of BERT and R2DE depends on the input length and the target difficulty of the questions . The error heavily depends on the target difficulty : this suggests that the two models tend to estimate difficulties closer to 0 than the target values ( especially R2DE ) . Indeed , we have observed that both the target difficulties and the estimated difficulties follow a Gaussian distribution , with higher variance for the target difficulties . There is no clear correlation between error and input length , but in some cases it seems that the error increases with the input length ( e.g. row b = \u22121 in BERT ) . Figure 5 shows the same analysis as Figure 4 for ASSIST - ments . The findings are very similar , but it is even more evident the fact that R2DE tends to perform predictions close to 0 ; the error of BERT depends less heavily on the difficulty . Again , there are no clear correlations between the input length and the accuracy of the estimation . In CloudAcademy , there are i ) cloze questions , in which the correct choice goes in place of an underscore in the text , and ii ) questions with a question mark at the end . Of the 1259 test questions , 222 are cloze questions . From the average errors for the different types of questions , we observed that both BERT and R2DE perform slightly worse on cloze questions : BERT 's MAE is 0.804 on cloze questions and 0.756 on the other questions ; similarly , R2DE 's MAE is 0.893 on cloze questions and 0.794 on the other questions . Lastly , we looked at the average error depending on the number of correct choices : we compared questions with multiple correct choices ( there are 141 of them in the test set ) and the questions with one correct choice . For BERT , the overall MAE is 0.774 and on the questions with multiple correct choices it is 0.764 ; in the case of R2DE , the MAE is 0.813 overall and 0.750 for questions with multiple correct choices . This difference between the two models might be due to the nature of R2DE itself : indeed , it uses a bag of words approach thus it does not care about the position of each word . Instead , BERT uses contextual embeddings which depend on the position of each word and the encoding of multiple correct choices we performed ( i.e. concatenation ) might not be the better choice , especially considering that probably there are not enough questions to learn the encoding .", "entities": [[14, 15, "MethodName", "BERT"], [30, 31, "MetricName", "accuracy"], [49, 50, "MethodName", "BERT"], [82, 83, "MethodName", "Transformer"], [117, 118, "MetricName", "accuracy"], [299, 300, "MethodName", "BERT"], [355, 356, "MethodName", "BERT"], [380, 381, "MethodName", "BERT"], [417, 418, "DatasetName", "0"], [486, 487, "MethodName", "BERT"], [525, 526, "DatasetName", "0"], [530, 531, "MethodName", "BERT"], [551, 552, "MetricName", "accuracy"], [619, 620, "MethodName", "BERT"], [629, 630, "MethodName", "BERT"], [631, 632, "MetricName", "MAE"], [648, 649, "MetricName", "MAE"], [704, 705, "MethodName", "BERT"], [708, 709, "MetricName", "MAE"], [730, 731, "MetricName", "MAE"], [782, 783, "MethodName", "BERT"]]}
{"text": "For fine - tuning on QDE from text we select the hyperparameters from the following pool of candi - dates : batch size = 16 , 32 , 64 ; learning rate = 1e - 5 , 2e - 5 , 3 - 5 ; patience early stopping = 10 epochs ; dropout additional layer = 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ; internal dropout = 0.1 , 0.2 , 0.3 , 0.4 , 0.5 . For the additional pre - training on MLM , the hyperparameters are selected between the following candidates : batch size = 64 ; learning rate = 1e - 5 ; number of epochs = 4 , 12 , 24 , 36 ; dropout = 0.1 . In both cases we use sequence length = 256 and the Adam optimizer .", "entities": [[21, 23, "HyperparameterName", "batch size"], [30, 32, "HyperparameterName", "learning rate"], [46, 48, "MethodName", "early stopping"], [86, 87, "DatasetName", "MLM"], [97, 99, "HyperparameterName", "batch size"], [102, 104, "HyperparameterName", "learning rate"], [109, 112, "HyperparameterName", "number of epochs"], [136, 137, "MethodName", "Adam"], [137, 138, "HyperparameterName", "optimizer"]]}
{"text": "We model question difficulty as defined in the oneparameter IRT model ( also named Rasch model ( Rasch , 1960 ) ) , which associates a skill level \u03b8 to each student and a difficulty level b to each question . For a given question j , its latent trait b j define the item response function ( i.r.f . ) , which indicates the probability ( P C ) that a student i with skill level \u03b8 i correctly answers the question . The formula of the i.r.f . is as follows : According to the intuition , a student with a given skill \u03b8 i has a lower probability of correctly answering more difficult questions . Also , if a question is too difficult or too easy ( i.e. b j or b j \u2212 ) , all the students will answer in the same way ( i.e. P C 0 or P C 1 ) regardless of \u03b8 i . Given some students ' answers to a set of questions , with IRT it is possible to estimate both the", "entities": [[28, 29, "HyperparameterName", "\u03b8"], [77, 78, "HyperparameterName", "\u03b8"], [105, 106, "HyperparameterName", "\u03b8"], [152, 153, "DatasetName", "0"], [160, 161, "HyperparameterName", "\u03b8"]]}
{"text": "The anchor algorithm computes the topic matrix A , where A v , k is the conditional probability of observing word v given topic k , e.g. , the probability of seeing the word \" lens \" given the camera topic in a corpus of Amazon product reviews . Arora et al ( 2012a ) find these probabilities by assuming that every topic contains at least one ' anchor ' word which has a non - zero probability only in that topic . Anchor words make computing the topic matrix A tractable because the occurrence pattern of the anchor word mirrors the occurrence pattern of the topic itself . To recover the topic matrix A using anchor words , we first compute a V \u00d7 V cooccurrence matrix Q , where Q i , j is the conditional probability p ( w j | w i ) of seeing word type w j after having seen w i in the same document . A form of the Gram - Schmidt process on Q finds anchor words { g 1 . . . g k } ( Arora et al , 2013 ) . Once we have the set of anchor words , we can compute the probability of a topic given a word ( the inverse of the conditioning in A ) . This coefficient matrix C is defined row - wise for each word i C * i , = argmin C i , D KL Q i , K k=1 C i , k Q g k , , ( 1 ) which gives the best reconstruction ( based on Kullback - Leibler divergence D KL ) of non - anchor words given anchor words ' conditional probabilities . For example , in our product review data , a word such as \" battery \" is a convex combination of the anchor words ' contexts ( Q g k , ) such as \" camera \" , \" phone \" , and \" car \" . Solving each row of C is fast and is embarrassingly parallel . Finally , we apply Bayes ' rule to recover the topic matrix A from the coefficient matrix C. The anchor algorithm can be orders of magnitude faster than probabilistic inference ( Arora et al , 2013 ) . The construction of Q has a runtime of O ( DN 2 ) where D is the number of documents and N is the average number of tokens per document . This computation requires only a single pass over the data and can be pre - computed for interactive use - cases . Once Q is constructed , topic recovery requires O ( KV 2 + K 2 V I ) , where K is the number of topics , V is the vocabulary size , and I is the average number of iterations ( typically 100 - 1000 ) . In contrast , traditional topic ( Hoffman et al , 2010 ) or Stochastic Variation Inference ( Hoffman et al , 2013 ) improves this to a single pass over the entire data . However , from Heaps ' law ( Heaps , 1978 ) it follows that V 2 DN for large datasets , leading to much faster inference times for anchor methods compared to probabilistic topic modeling . Further , even if online were to be adapted to incorporate human guidance , a single pass is not tractable for interactive use .", "entities": [[479, 482, "HyperparameterName", "number of iterations"]]}
{"text": "Our first evaluation is a classification task to predict documents ' newsgroup membership . Thus , we do not aim for state - of - the - art accuracy , 2 but the experiment shows title - based tandem anchors yield topics closer to the underlying classes than Gram - Schmidt anchors . After randomly splitting the data into test and training sets we learn topics from the test data using both the title - based tandem anchors and the Gram - Schmidt single word anchors . 3 For multiword anchors , we use each of the combiner functions from Section 2.2 . The anchor algorithm only gives the topic - word distributions and not word - level topic assignments , so we infer token - level topic assignments using LDA Latent Dirichlet Allocation ( Blei et al , 2003 ) with fixed topics discovered by the anchor method . We use our own implementation of Gibbs sampling with fixed topics and a symmetric documenttopic Dirichlet prior with concentration \u03b1 = .01 . Since the topics are fixed , this inference is very fast and can be parallelized on a per - document basis . We then train a hinge - loss linear classifier on the newsgroup labels using Vowpal Wabbit 4 with topic - word pairs as features . Finally , we infer topic assignments in the test data and evaluate the classification using those topic - word features . For both training and test , we exclude words outside the LDA vocabulary . The topics created from multiword anchor facets are more accurate than Gram - Schmidt topics ( Figure 1 ) . This is true regardless of the combiner function . However , harmonic mean is more accurate than the other functions . 5 Since 20NEWS has twenty classes , accuracy alone does not capture confusion between closely related newsgroups . For example , accuracy penalizes a classifier just as much for labeling a document from ' rec.sport.baseball ' with ' rec.sport.hockey ' as with ' alt.atheism ' despite the similarity between sports newsgroups . Consequently , after building a confusion matrix between the predicted and true classes , external clustering metrics reveal confusion between classes . The first clustering metric is the adjusted Rand index ( Yeung and Ruzzo , 2001 ) , which is akin to accuracy for clustering , as it gives the percentage of correct pairing decisions from a reference clustering . Adjusted Rand index ( ARI ) also accounts for chance groupings of documents . Next we use F - measure , which also considers pairwise groups , balancing the contribution of false negatives , but without the true negatives . Finally , we use variation of information ( VI ) . This metric measures the amount of information lost by switching from the gold standard labels to the predicted labels ( Meil\u0203 , 2003 ) . Since we are measuring the amount of information lost , lower variation of information is better . Based on these clustering metrics , tandem anchors can yield superior topics to those created using single word anchors ( Figure 1 ) . As with accuracy , this is true regardless of which combination function we use . Furthermore , harmonic mean produces the least confusion between classes . 5 The final evaluation is topic coherence by Newman et al ( 2010 ) , which measures whether the topics make sense , and correlates with human judgments of topic quality . Given V , the set of the n most probable words of a topic , coherence is v 1 , v 2 V log D ( v 1 , v 2 ) + D ( v 2 ) ( 7 ) where D ( v 1 , v 2 ) is the co - document frequency of 5 Significant at p < 0.01/4 when using two - tailed t - tests with a Bonferroni correction . For each of our evaluations , we verify the normality of our data ( D'Agostino and Pearson , 1973 ) and use two - tailed t - tests with Bonferroni correction to determine whether the differences between the different methods are significant . Figure 1 : Using metadata can improve anchor - based topic models . For all metrics , the unsupervised Gram - Schmidt anchors do worse than creating anchors based on Newsgroup titles ( for all metrics except VI , higher is better ) . For coherence , Gram - Schmidt does better than two functions for combining anchor words , but not the element - wise min or harmonic mean . word types v 1 and v 2 , and D ( v 2 ) is the document frequency of word type v 2 . A smoothing parameter prevents zero logarithms . Figure 1 also shows topic coherence . Although title - based anchor facets produce better classification features , topics from Gram - Schmidt anchors have better coherence than title - based anchors with the vector average or the or - operator . However , when using the harmonic mean combiner , title - based anchors produce the most human interpretable topics . 6 Harmonic mean beats other combiner functions because it is robust to ambiguous or irrelevant term cooccurrences an anchor facet . Both the vector average and the or - operator are swayed by large outliers , making them sensitive to ambiguous terms in an anchor facet . Element - wise min also has this robustness , but harmonic mean is also able to better characterize anchor facets as it has more centralizing tendency than the min .", "entities": [[28, 29, "MetricName", "accuracy"], [130, 131, "MethodName", "LDA"], [169, 170, "HyperparameterName", "\u03b1"], [201, 202, "MetricName", "loss"], [252, 253, "MethodName", "LDA"], [303, 304, "MetricName", "accuracy"], [317, 318, "MetricName", "accuracy"], [391, 392, "MetricName", "accuracy"], [413, 414, "MetricName", "ARI"], [426, 429, "MetricName", "F - measure"], [528, 529, "MetricName", "accuracy"], [713, 715, "TaskName", "topic models"]]}
{"text": "We now validate our main result that for interactive topic modeling , tandem anchors yields better topics than single word anchors . Like our titlebased experiments in Section 3 , topics generated from users become features to train and test a classifier for the 20NEWS dataset . We choose this dataset for easier comparison with the Interactive Topic Modeling result of . Basedsie on our results with title - based anchors , we use the harmonic mean combiner in our analysis . As before , we report not only accuracy , but also multiple clustering metrics using the confusion matrix from the classification task . Finally , we report topic coherence . Figure 3 summarizes the results of our quantitative evaluation . While we only compare user generated anchors in our analysis , we include the unsupervised Gram - Schmidt anchors as a baseline . Some of the data violate assumptions of normality . Therefore , we use Wilcoxon 's signed - rank test ( Wilcoxon , 1945 ) to determine if the differences between multiword anchors and single word anchors are significant . Topics from user generated multiword anchors yield higher classification accuracy ( Figure 3 ) . Not only is our approach more scalable than the Interactive Topic Model , but we also achieve Figure 3 : Classification accuracy and coherence using topic features gleaned from user provided multiword and single word anchors . Grahm - Schmidt anchors are provided as a baseline . For all metrics except VI , higher is better . Except for coherence , multiword anchors are best . higher classification accuracy than Hu et al ( 2014 ) . 9 Tandem anchors also improve clustering metrics . 10 While user selected tandem anchors produce better classification features than single word anchors , users selected single word anchors produce topics with similar topic coherence scores . 11 To understand this phenomenon , we use quality metrics ( AlSumait et al , 2009 ) for ranking topics by their correspondence to genuine themes in the data . Significant topics are likely skewed towards a few related words , so we measure the distance of each topic - word distribution from the uniform distribution over words . Topics which are close to the underlying word distribution of the entire data are likely to be vacuous , so we also measure the distance of each topic - word distribution from the underlying word distribution . Finally , background topics are likely to appear in a wide range of documents , while meaningful topics will appear in a smaller subset of the data . Figure 4 reports our topic significance findings . For all three significance metrics , multiword anchors produce more significant topics than single word anchors . 10 Topic coherence is based solely on the top n words of a topic , while both accuracy and topic significance depend on the entire topicword distributions . With single word anchors , topics with good coherence may still be too general . Tandem anchors enables users to produce topics with more specific word distributions which are better features for classification . Users are able to combine words to create more specific topics with tandem anchors .", "entities": [[89, 90, "MetricName", "accuracy"], [193, 194, "MetricName", "accuracy"], [219, 220, "TaskName", "Classification"], [220, 221, "MetricName", "accuracy"], [267, 268, "MetricName", "accuracy"], [478, 479, "MetricName", "accuracy"]]}
{"text": "Following ( Augenstein et al , 2017 ; Taill\u00e9 et al , 2020a ) , we partition the entity mentions in the test set based on lexical overlap with the training set . We distinguish Seen and Unseen mentions and also extend this partition to relations . We denote a relation as an Exact Match if the same ( head , predicate , tail ) triple appears in the train set ; as a Partial 1 More implementation details in Appendix A Match if one of its arguments appears in the same position in a training relation of same type ; and as New otherwise . We implement a naive Retention Heuristic that tags an entity mention or a relation exactly present in the training set with its majority label . We report micro - averaged Precision , Recall and F1 scores for both NER and RE in Table 1 . An entity mention is considered correct if both its boundaries and type have been correctly predicted . For RE , we report scores in the Boundaries and Strict settings ( Bekoulis et al , 2018 ; Taill\u00e9 et al , 2020b ) . In the Boundaries setting , a relation is correct if its type is correct and the boundaries of its arguments are correct , without considering the detection of their types . The Strict setting adds the requirement that the entity type of both argument is correct .", "entities": [[53, 55, "MetricName", "Exact Match"], [136, 137, "MetricName", "Precision"], [138, 139, "MetricName", "Recall"], [140, 141, "MetricName", "F1"], [144, 145, "TaskName", "NER"]]}
{"text": "As expected , this first evaluation setting enables to expose an important lexical overlap bias , already SpERT 89.0 0.1 74.1 1.0 86.5 0.2 77.0 1.1 52.2 1.1 38.9 1.0 57.0 0.8 75.1 1.2 48.4 1.0 36.3 2.0 53.9 0.8 SpERT 89.4 0.2 74.2 0.8 86.8 0.2 84.8 0.8 59.6 0.7 42.3 1.1 64.0 0.6 82.6 0.8 55.6 0.7 38.4 1.1 60.6 0.5 TABTO 89.7 0.1 77.4 0.8 87.5 0.2 85.9 0.9 62.6 1.8 44.6 2.9 66.4 1.3 81.6 1.5 58.1 1.6 38.5 3.1 61.7 1.1 PURE 90.5 0.2 80.0 0.3 88.7 0.1 86.0 1.3 60.5 1.0 47.1 1.6 65.1 0.7 84.1 1.1 57.9 1.3 44.0 2.0 62.6 discussed in NER , in end - to - end Relation Extraction . On every dataset and for every model micro F1 scores are the highest for Exact Match relations , then Partial Match and finally totally unseen relations . This is a first confirmation that retention plays an important role in the measured overall performance of end - to - end RE models .", "entities": [[110, 111, "TaskName", "NER"], [118, 120, "TaskName", "Relation Extraction"], [128, 130, "MetricName", "micro F1"], [135, 137, "MetricName", "Exact Match"]]}
{"text": "Several works on generalization of NER models mention lexical overlap with the training as a key indicator of performance . Augenstein et al ( 2017 ) separate mentions in the test set as seen and unseen during training and measure out - of - domain generalization in an extensive study of two CRF based models and SENNA combining a Convolutional Neural Network with a CRF ( Collobert and Weston , 2011 ) . Taill\u00e9 et al ( 2020a ) compare the effect of introducing contextual embeddings in the classical BiLSTM - CRF architecture in a similar setting and show that they help close the performance gap on unseen mentions and domains . Arora et al ( 2020 ) ; Fu et al ( 2020b , a ) study the influence of several properties such as lexical overlap , label consistency and entity length on state - of - the - art models performance . They model these properties as continuous scores associated to each mention and bucketized for evaluation . Lexical overlap has also been mentioned in Coreference Resolution ( Moosavi and Strube , 2017 ) where coreferent mentions tend to co - occur in the test and train sets . In this line of works , the impact of lexical overlap is measured either by separating performance depending on the property of mentions ( seen or unseen ) or with outof - domain evaluation with a test set from a different dataset with lower lexical overlap with the train set . Another recently proposed method for finegrained evaluation of NLP models beyond a single benchmark score is to modify the test sentences in a controlled manner . McCoy et al ( 2019 ) expose lexical overlap as a shallow heuristic adopted by state - of - the - art Natural Language Inference models , especially by swapping subject and object of verbs in the hypothesis of some examples where the premise entails the hypothesis . While such a modification changes the label of these examples to non - entailment , all models tested show a spectacular drop of accuracy on these models . Ribeiro et al ( 2020 ) propose a broader set of test set modifications to individually test robustness of NLP models to several patterns such as the introduction of negation , swapping words with synonyms , changing tense and much more . In pipeline RE where ground truth candidate arguments are given , models often use intermediate representations based on entity types that reduce lexical overlap issues . However , Rosenman et al ( 2020 ) show that they still tend to adopt shallow heuristics based on the type of the arguments and the presence of triggers indicative of the presence of a relation . They propose hard cases with several mentions of same types for which Relation Classifiers struggle connecting the correct pair . Concurrently , Peng et al ( 2020 ) confirm that RE benchmarks present shallow cues such as the type of the candidate arguments that can be used alone to infer the relation . We propose to extend previous work on NER and RE to the more realistic end - to - end RE setting with two of the previously described approaches : 1 ) separating performance by lexical overlap of mentions or argument pairs and 2 ) modifying some CoNLL04 test examples by swapping relations heads and tails .", "entities": [[5, 6, "TaskName", "NER"], [44, 46, "TaskName", "domain generalization"], [52, 53, "MethodName", "CRF"], [64, 65, "MethodName", "CRF"], [89, 90, "MethodName", "BiLSTM"], [91, 92, "MethodName", "CRF"], [177, 179, "TaskName", "Coreference Resolution"], [300, 303, "TaskName", "Natural Language Inference"], [349, 350, "MetricName", "accuracy"], [519, 520, "TaskName", "NER"]]}
{"text": "For every model , we use the original code associated with the papers with the default best performing hyperparameters unless stated otherwise . We run 5 runs on a single NVIDIA 2080Ti GPU for each of them on each dataset . For CoNLL04 and ACE05 , we train each model with both the cased and uncased versions of BERT BASE and only keep the best performing setting . PURE ( Zhong and Chen , 2021 ) 1 We use the approximation model and limit use a context window of 0 to only use the current sentence for prediction and be able to compare with other models . For ACE05 , we use the standard bert - base - uncased LM but use the bert - base - cased version on CoNLL04 which results in a significant +2.4 absolute improvement in RE Strict micro F1 score . SpERT ( Eberts and Ulges , 2020 ) 2 We use the original implementation as is with bert - base - cased for both ACE05 and CoNLL04 since the uncased version is not beneficial , even on ACE05 where there are fewer proper nouns . For the Ent - SpERT ablation , we simply remove the max - pooled context representation from the final concatenation in the RE module . This modifies the RE classifier 's input dimension from the original 2354 to 1586 . Two are better than one ( TABTO ) ( Wang and Lu , 2020 ) 3 We use the original implementation with bert - base - uncased for both ACE05 and CoNLL04 since the cased version is not beneficial on CoNLL04 .", "entities": [[58, 59, "MethodName", "BERT"], [59, 60, "MethodName", "BASE"], [89, 90, "DatasetName", "0"], [142, 144, "MetricName", "micro F1"]]}
{"text": "We present general datasets statistics in Table 4 . We also compute average values of some entity and relation attributes inspired by ( Fu et al , 2020a ) and reported in Table 5 . We report two of their entity attributes : entity length in number of tokens ( eLen ) and entity label consistency ( eCon ) . Given a test entity mention , its label consistency is the number of occurrences in the training set with the same type divided by its total number of occurrences . It is zero for unseen mentions . Because eCon reflects both the ambiguity of labels for seen entities and the proportion of unseen entities , we propose to introduce the eCon * score that only averages label consistency of seen mentions and eLex , the proportion of entities with lexical overlap with the train set . We introduce similar scores for relations . Relation label consistency ( rCon ) extends label consistency for triples . Argument types label constitency ( aCon ) considers the labels of every pair of mentions of corresponding types in the training set . Because pairs of types are all seen during training we do not decompose aCon into aCon * and aLex . Argument length ( aLen ) is the sum of the lengths of the head and tail mentions . Argument distance ( aDist ) is the number of tokens between the head and the tail of a relation . We present a more complete report of overall Precision , Recall and F1 scores that can be interpreted in light of these statistics in Table 6 .", "entities": [[254, 255, "MetricName", "Precision"], [256, 257, "MetricName", "Recall"], [258, 259, "MetricName", "F1"]]}
{"text": "Relation extraction ( RE ) is an essential topic in natural language processing and has attracted extensive attention . Current RE approaches achieve fantastic results on common datasets , while they still struggle on practical applications . In this paper , we analyze the above performance gap , the underlying reason of which is that practical applications intrinsically have more hard cases . To make RE models more robust on such practical hard cases , we propose a case - oriented construction framework to build a Hard Case Relation Extraction Dataset ( HacRED ) . The proposed HacRED consists of 65 , 225 relational facts annotated from 9 , 231 documents with sufficient and diverse hard cases . Notably , HacRED is one of the largest Chinese document - level RE datasets and achieves a high 96 % F1 score on data quality . Furthermore , we apply the stateof - the - art RE models on this dataset and conduct a thorough evaluation . The results show that the performance of these models is far lower than humans , and RE applying on practical hard cases still requires further efforts . Ha - cRED is publicly available at https://github . com / qiaojiim / HacRED .", "entities": [[0, 2, "TaskName", "Relation extraction"], [88, 90, "TaskName", "Relation Extraction"], [138, 140, "MetricName", "F1 score"]]}
{"text": "Relation extraction ( RE ) is one of the core NLP tasks and plays an increasingly important role in knowledge graph completion ( Bordes et al , 2013 ) and question answering ( Dong et al , 2015 ) . RE aims to extract structured relational facts , i.e. , triples such as ( Bill Gates , founder_of , Microsoft ) from plain texts . Recently , various models ( Zeng et al , 2018 ; Takanobu et al , 2019 ; Fu et al , 2019 ; have been proposed to identify the relational facts and achieved state - of - theart ( SOTA ) performance , among which the latest method CasRel achieves notable 91.8 % F1 score on WebNLG ( Gardent et al , 2017 ) and 89.6 % on NYT ( Riedel et al , 2010 ) . However , can these seemingly fantastic results prove that the current RE models are powerful enough to perform well in practical applications . To answer the question , we employ CasRel on 300 randomly selected samples of WebNLG and the same number of data from practical DuIE 1 . The F1 scores under these scenarios drop significantly from 89.3 % to 62.8 % . As illustrated in Figure 1 , CasRel extracts correct triples ( Elliot See , place_of_birth , Dallas ) and ( Elliot See , place_of_death , St. Louis ) in WebNLG where keywords such as born and died explicitly express the relation information . In contrast , Cas - Rel fails to extract triples such as ( Yang Jima , graduate_from , Communication University of China ) where no keywords like graduate are mentioned . The most significant reason why CasRel performs well on WebNLG but struggles on practical data is that more challenging instances which we refer to as hard cases exist in the practical applications . Moreover , according to the statistics of entity description documents in CN - DBpedia , at least 40.1 % relational facts can only be extracted from hard cases . Therefore , relation extraction from hard cases can not be neglected and demands more attention . Although many datasets ( Li et al , 2016 ; Yao et al , 2019 ) have been proposed for RE , they rarely analyze the performance gap and focus on the hard cases . In order to make models robust on hard cases and more fit practical scenarios , in this paper , we aim to build a RE dataset with sufficient hard cases . To this end , we propose a case - oriented construction framework based on the challenging instances and build a Hard Case Relation Extraction Dataset ( HacRED ) . Specifically , we first obtain general , massive , and various contexts as well as relational facts from CN - DBpedia to construct a distantly supervised dataset . The crucial part is to distinguish hard cases from abundant data . Therefore , we formulate nine indicators through systematic analysis of hard cases to quantify them . Then , we conduct feature engineering based on the valid indicators . Afterwards , a classifier is trained for distinguishing the desired hard cases . Finally , we develop a crowdsourcing platform with a novel three - stage annotation strategy and effective aggregation method CrowdTruth2.0 ( Dumitrache et al , 2018 ) to guarantee the data size and quality . In total , HacRED consists of 9 , 231 instances with 26 predefined relations and 9 types of entities . To the best of our knowledge , it is one of the largest document - level RE benchmark . Moreover , HacRED contains sufficient and diverse hard cases in line with practice . We conduct extensive experiments and systematic error analysis of SOTA models on HacRED . A sharp performance drop on HacRED compared to the existing benchmarks proves that RE in practical applications remains an open problem and still requires further research . To recap , our main contributions are three - fold : We first analyze the performance gap between popular datasets and practical applications , and therefore construct one of the largest Chinese document - level RE dataset which contains sufficient and diverse hard cases to improve the evaluation for complex RE tasks . We propose a case - oriented construction framework to build RE dataset toward spe - cial cases . Meanwhile , we design a novel three - stage annotation method applicable for crowdsourcing of complex RE . We systematically evaluate the current mainstream RE models on HacRED and justify its effectiveness in depth . 2 Related Work", "entities": [[0, 2, "TaskName", "Relation extraction"], [19, 22, "TaskName", "knowledge graph completion"], [30, 32, "TaskName", "question answering"], [118, 120, "MetricName", "F1 score"], [121, 122, "DatasetName", "WebNLG"], [179, 180, "DatasetName", "WebNLG"], [192, 193, "MetricName", "F1"], [235, 236, "DatasetName", "WebNLG"], [289, 290, "DatasetName", "WebNLG"], [326, 327, "DatasetName", "DBpedia"], [344, 346, "TaskName", "relation extraction"], [446, 448, "TaskName", "Relation Extraction"], [473, 474, "DatasetName", "DBpedia"], [513, 515, "TaskName", "feature engineering"]]}
{"text": "Recently , many exciting works have been proposed to solve the RE tasks . ( 1 ) Joint Model : NovelTagging ( Zheng et al , 2017 ) first formulates the task as a sequence labeling problem and presents a novel tagging schema to jointly extract entities and relations . CopyRE ( Zeng et al , 2018 ) extracts triples based on a sequence - to - sequence structure and integrates the copy mechanism for entity generation . GraphRel ( Fu et al , 2019 ) uses graph convolutional network ( GCN ) to capture features of words and text . CasRel is different from the past and is able to extract more triples by learning relationspecific entity taggers . ( 2 ) Pipeline Model : PURE ( Zhong and Chen , 2020 ) is a simple pipelined approach which learns an entity model and a relation model independently . DGCNN - BERT is a powerful pipeline method that first identifies multiple relations and then labels the head and tail entities given a relation . It achieves 89.3 F1 scores and has won the champion in the Competition of DuIE held by Baidu Inc. ( 3 ) Document - level Relation Classification Models : LSR ( Nan et al , 2020 ) is a model that empowers the relational reasoning across sentences by automatically inducing the latent document - level graph . GAIN ( Zeng et al , 2020 ) introduces a path reasoning mechanism based on a heterogeneous mention - level graph and an entity - level graph . ATLOP proposes two techniques , adaptive thresholding and localized context pooling . SSAN ( Xu et al , 2021 ) designs several transformations to incorporate mention structural dependencies for document - level relation classification ( DocRC ) .", "entities": [[87, 90, "MethodName", "graph convolutional network"], [91, 92, "MethodName", "GCN"], [150, 151, "MethodName", "DGCNN"], [152, 153, "MethodName", "BERT"], [178, 179, "MetricName", "F1"], [200, 202, "TaskName", "Relation Classification"], [218, 220, "TaskName", "relational reasoning"], [292, 294, "TaskName", "relation classification"]]}
{"text": "To build a dataset toward practical hard cases , we systematically formulate the nine indicators of hard cases ( refer to Section 3 ) and introduce measurements to quantify them . For example , we calculate the Argument Distance as the number of tokens between the head and tail entity mentions in the text . More details of feature engineering are described in Appendix A. After hard case oriented feature engineering , we discard the instances in D ds without any indicator of hard cases . The remaining part forms a hard case candidate dataset D with about 108 thousand instances . We randomly sample 3 , 500 instances from D and ask experts to select the hard cases given the context and features . Specifically , if an instance with multiple hard case indicators or with only one indicator but selected by all three experts based on their expertise , it is regarded as a hard case . To further evaluate the quality of selected hard cases , we utilize DGCNN - BERT to test the selected and unselected data . If the F1 score drops \u03b4=10 % on the hard cases , we reserve the data to constitute the high quality seeds of hard case D p . The remaining data is easy case D n . In total , we obtain 1 , 431 seeds of hard cases .", "entities": [[58, 60, "TaskName", "feature engineering"], [69, 71, "TaskName", "feature engineering"], [171, 172, "MethodName", "DGCNN"], [173, 174, "MethodName", "BERT"], [184, 186, "MetricName", "F1 score"], [203, 204, "DatasetName", "seeds"], [227, 228, "DatasetName", "seeds"]]}
{"text": "In this section , we analyze various aspects of common RE datasets and HacRED . Data Size . As shown in Table 3 , HacRED has a greater average number of words , entities , and triples in each text than all of the sentencelevel datasets . Thus we regard HacRED as a document - level RE dataset . Compared with the document - level datasets , DocRED aims at common document - level RE but not consider performance gaps and various hard cases in practical scenarios . BC5CDR is specially designed for biomedical domain . By contrast , we are the first to analyze the performance gap between popular datasets and practical applications , and propose HacRED which focuses on different kinds of hard cases in general domain . Besides , HacRED is larger in scale and contains much more various relational facts than BC5CDR and DocRED but with lower duplicated triples ratio . Data Distribution . We calculate three global statistic metrics about data distribution of common datasets and HacRED . and the relation while low - frequency mentions are neglected . All these three aspects reveal the unreasonable data distribution of common datasets . In comparison , we observe a more reasonable data distribution in HacRED from Table 4 and Table 5 . HacRED has a low ratio of duplicate triples and contains various relational facts , which addresses semantic bias . No biased relation existing in HacRED reduces the risk of selection bias . The proportion of top 20 % relations promotes the alleviation of relation bias on HacRED . The more comparison of overall data distribution can be found in Appendix E. Data Quality . We evaluate the quality of HacRED through both automatic metrics and human evaluation . Specifically , we first compute the average unit quality score ( UQS ) , annotation quality score ( AQS ) , and worker quality score ( WQS ) of the whole 9 , 231 instances . UQS , AQS and WQS are proposed by CrowdTruth2.0 ( Appendix F provides more calculation details ) . The closer these scores are to 1 , the higher quality of the crowdsourcing is . Meanwhile , we randomly sample 400 instances from HacRED and compute the precision , recall , and F1 score with annotations based on the revision of humans . The evaluation scores are reported in Table 7 . From this table , our Ha - cRED achieves a considerable annotation quality . As a comparison , NYT contains about 31 % noise instances ( Riedel et al , 2010 ) and TACRED has poor annotation quality ( Alt et al , 2020 ) . Hard Case Types . We group the randomly sampled 400 instances into nine categories as shown in Table 6 . The proportions of different kinds of instances reflect that HacRED contains a various range of hard cases , which evaluates models comprehensively for practical applications .", "entities": [[67, 68, "DatasetName", "DocRED"], [88, 89, "DatasetName", "BC5CDR"], [145, 146, "DatasetName", "BC5CDR"], [147, 148, "DatasetName", "DocRED"], [245, 247, "TaskName", "selection bias"], [381, 383, "MetricName", "F1 score"], [434, 435, "DatasetName", "TACRED"]]}
{"text": "As DGCNN - BERT has been used in the main process of construction , we evaluate other strong RE models including joint RE models , pipeline RE models , and DocRC models on HacRED . First , we limit the relation set within 20 types both in Ha - cRED and DuIE , and then separate a part of instances in DuIE to form the contrastive easy case dataset D ec . We carry out the equivalent substitution of hard cases in HacRED for easy ones in D ec in different proportions . Figure 3 shows the F1 curve of the performances w.r . in performance . The SOTA model CasRel still outperforms other joint models and achieves great F1 on 100 % D ec . However , the performance drops on data with more complex instances . We notice that F1 value of easy cases is generally greater than that of hard cases in different substitution ratio settings , which illustrates that RE models indeed struggle when tackling hard cases . Note that by combining HacRED with easy cases in existing datasets , it is easy to simulate diverse practical scenarios . In addition , we split HacRED into train , dev , and test sets with 6231 , 1500 , 1500 instances respectively . The precision , recall , and F1 score of the three major categories of models are shown in Table 8 . The joint and pipeline learning strategies do not contribute to a great F1 on triple extraction . For the NER task , PURE has a separate entity model but results in a 30.61 % F1 when all entities in a document are considered , including entities with no positive relation labels . This also reflects the challenge to obtain complete entity information in practical scenarios . On the other hand , the relation classification performances of DocRC models are far from satisfactory . The results suggest that existing models have remarkably poor performance on HacRED compared with humans ( Table 9 ) , which indicates that RE applicable for practical hard cases still requires further research .", "entities": [[1, 2, "MethodName", "DGCNN"], [3, 4, "MethodName", "BERT"], [97, 98, "MetricName", "F1"], [119, 120, "MetricName", "F1"], [141, 142, "MetricName", "F1"], [222, 224, "MetricName", "F1 score"], [249, 250, "MetricName", "F1"], [256, 257, "TaskName", "NER"], [271, 272, "MetricName", "F1"], [309, 311, "TaskName", "relation classification"]]}
{"text": "In this section , we give insight into the abilities of current mainstream joint models when tackling different kinds of hard cases and propose some research indications as well . As it is hard to obtain complete entity information in practical scenarios , we do not consider DocRC models in this section that entity information is provided as input . Multiple Triples . Table 11 shows the F1 score of existing models when extracting from texts with different number of triples . The performance of NovelTagging and CopyRE decreases as the number of triples increases , which indicates that the novel tagging schema and multiple decoder mechanism are not able to address the challenge of Multiple Triples . Since GraphRel predicts relations for all word pairs and CasRel learns separate entity tagger for different relations , these two models alleviate this problem . An interesting point is that the performance of GraphRel and CasRel rises as the number of triples increases when the triples number is less than 16 , indicating that these two models work well in texts with number of triples nearing the average . However , all models get F1 score below average when text mentions have more than 16 triples . BiLSTM - based neural models like NovelTagging and CopyRE . The performance improvement on CasRel suggests the powerfulness of BERT encoder in the long - distance context . Homogeneous Entities and Similar Relations . Since the text mentions multiple homogeneous entities and semantically similar relations , models are required to distinguish the fine - grained difference of the context to extract the correct triples . The first two columns in Table 10 have similar results , which indicates that the contexts with homogeneous entities and similar relations are as challenging as the long - distance contexts . Long - tail Relations . We observe a dramatic decrease on the instances with long - tail relational triples . As long - tail relations are common in realworld scenarios , a more efficient learning method is required to make RE models applicable for practical applications . Overlapping Triples . CasRel achieves a better performance on extracting overlapping triples . This proves the effectiveness of cascade binary tagging strategy by first identifying the head mention and then extract the corresponding tail mention given a relation . Specifically , the F1 scores of overlapping head and tail mentions are 66.38 % and 47.44 % respectively . Similarly , results of the two above metrics in CopyRE are 13.31 % and 3.57 % . The relative higher performance on overlapping head mentions than tail mentions also suggests that the order of extracting arguments could have effect on the results . Distractor and Reasoning . We manually select instances with Distractor and Reasoning indicators in HacRED because they cooccur frequently in corpus . As illustrated in Table 10 , we observe a drop of the F1 . This suggests that models are vulnerable to this kind of instances . However , there are lots of texts with distractions or implicit expression , which needs reasoning , and even common sense . The model design should take the reasoning mechanism into consideration in the future work .", "entities": [[67, 69, "MetricName", "F1 score"], [192, 194, "MetricName", "F1 score"], [205, 206, "MethodName", "BiLSTM"], [224, 225, "MethodName", "BERT"], [391, 392, "MetricName", "F1"], [484, 485, "MetricName", "F1"]]}
{"text": "A decision tree is learned by the auxilliary features calculated in stage 2 . For deep models , we concatenate multiple embeddings and auxilliary features to make up the input . We add special tokens to mark the border of each entity and generate the representation vector as recommended in Baldini Soares et al ( 2019 ) . We assign a label 1 to each instance in D p and \u22121 in D n . The deep models output the probability of the instance belonging to hard cases and are optimized with the binary cross entropy loss objective . To start PU learning , we sample from D to form a unlabeled dataset D u and set the hyperparameter \u03c0 p = 0.41 estimated by the proportion of hard cases selected by experts . We implement nnPU ( Kiryo et al , 2017 ) which is efficient for massive data and deep learning and use J nnpu as the optimized objective , J nnpu = \u03c0 p E p ( x | y=1 ) [ l ( g ( x ) ) ] + max { 0 , E p ( x ) [ l ( \u2212g ( x ) ) ] \u2212 \u03c0 p E p ( x | y=1 ) [ l ( \u2212g ( x ) ) ] } ( 1 ) where \u03c0 p = p ( y = 1 ) , g is decision function , l is surrogate loss function . We choose the double hinge loss l = max ( \u2212z , max ( 0 , 1 2 \u2212 1 2 z ) ) proposed by ( du Plessis et al , 2015 ) .", "entities": [[96, 97, "MetricName", "loss"], [186, 187, "DatasetName", "0"], [244, 245, "MetricName", "loss"], [252, 253, "MetricName", "loss"], [261, 262, "DatasetName", "0"]]}
{"text": "Broadly speaking , the dialog belief tracking algorithms can be divided into three families : 1 ) hand - crafted rules 2 ) generative models , and 3 ) maximum - entropy model ( Metallinou et al , 2013 ) . Later on , many deep learning based discriminative models have surged to replace the traditional strategies ( Henderson et al , 2014a ; Williams et al , 2016 ) and achieved state - of - the - art results on various datasets . Though the discriminative models are reported to achieve fairly high accuracy , their applications are heavily restricted by the domain , ontology , and language . Recently , a pointer network based algorithm ( Xu and Hu , 2018 ) and another multi - domain algorithm ( Rastogi et al , 2017 ) have been proposed to break the ontology and domain boundary . Besides , ( Mrk\u0161i\u0107 et al , 2017 ) has proposed an algorithm to train a unified framework to deal with multiple languages with annotated datasets . In contrast , our paper focuses on breaking the language boundary and transfer DST knowledge from one language into other zeroannotation languages .", "entities": [[94, 95, "MetricName", "accuracy"], [105, 106, "MethodName", "ontology"], [113, 115, "MethodName", "pointer network"], [143, 144, "MethodName", "ontology"]]}
{"text": "We design our cross - lingual DST on top of the state - of - the - art Neural Belief Tracker ( NBT ) , which demonstrates many advantages ( no hand - crafted lexicons , no linguistic knowledge required , etc ) . These nice properties are essential for our cross - lingual DST design because we are pursuing a general and simple framework regardless of the language properties . In short , NBT consists of a neural network that computes the matching score for every candidate slot - value pair ( c s , c v ) based on the following three inputs : ( i ) the system dialog acts a t = ( t q , t s , t v ) , 2 ( ii ) the user utterance u t , and ( iii ) the candidate slot - value pair . And it identifies the user intents by evaluating the scores for all the slot - value pairs ( see Figure 3 ) . With a slight abuse of notation , we still use c s , c v , t s , t v , t q R H to denote the vector representations of themselves , where H is the embedding dimension . We will use pre - trained embedding vectors in our cross - lingual NBT , just like the original NBT and they will be fixed during training . To enable cross - lingual transfer learning , we first re - interpret the architecture of the original NBT by decomposing it into three components : Utterance Encoding The first component is an utterance encoder , which maps the utterance u t = { w 1 , w 2 , , w N } of a particular language into a semantic representation vector r ( u t ) R H , where w i R H is the word vector for the i - th token and N is the length of the utterance . Note that the dimension of the semantic vector r ( u t ) is the same as that of the word vector . We implement . the encoder using the same convolutional neural network ( CNN ) as the original NBT , with a slight modification of adding a top batch normalization layer . We will explain this change in section 5 .", "entities": [[209, 211, "HyperparameterName", "embedding dimension"], [242, 246, "TaskName", "cross - lingual transfer"], [385, 387, "MethodName", "batch normalization"]]}
{"text": "We are given a well - trained NBT for a source language e , and we want to learn an NBT for a target language f without any annotated training data . Therefore , we can not learn the target - side NBT from standard supervised learning . Instead , we use a teacher - student framework to distill the knowledge from the source - side NBT ( teacher network ) into the target - side NBT ( student network ) ( see Figure 4 ) . Let x e ( c e s , c e v , u e t , a e t ) be the input to the teacher network and let x f ( c f s , c f v , u f t , a f t ) be the associated input to the student network . The standard teacher - student framework trains the student network by minimizin\u011d ) . However , the target - side inputs ( c f s , c f v , u f t , a f t ) parallel to ( c e s , c e v , u e t , a e t ) are usually not available in crosslingual DST , and , even worse , the target - side utterance u e t is not available . We may have to generate synthetic input data for the student network or leverage external data sources . It is relatively easy to use the mapping M ( ) to generate ( c f s , c f v , a f t ) ) ( i.e. , the inputs of the target - side context gate ) from the ( c e s , c e v , a e t ) . But it is more challenging to obtain the parallel utterance data u f t from u e t ) . Therefore , we have to leverage external bilingual data sources to alleviate the problem . However , the external bilingual data are usually not in the same domain as the utterance , and hence they are not aligned with the slot - value pair and system acts ( i.e. , ( c e s , c e v , a e t ) or ( c f s , c f v , a f t ) ) . For this reason , we can not perform the knowledge transfer by minimizing the cost ( 5 ) . Instead , we need to develop a new cost function where the utterance is not required to be aligned with the slot - value pair and the system acts . To this end , let g e = g e ( c e s , c e v , a e t ) and g J1 = xe , x f | | y ( c e s , c e v , u e t , a e t ) \u2212 y ( c f s , c f v , u f t , a f t ) | | 2 ( f = g f ( c f s , c f v , a f t ) . And we substitute ( 3 ) into ( 5 ) and get : J1 \u2264 | | Wy | | 2 c f v , c e v | | re g e \u2212 r f g f | | 2 = | | Wy | | 2 c f v , c e v | | g e ( re \u2212 r f ) + r f ( g e \u2212 g f ) | | 2 \u2264 | | Wy | | 2 c f v , c e v | | g e | | 2 | | re \u2212 r f | | 2 + | | r f | | 2 | | g e \u2212 g f | | 2 where r e = r e ( u e t ) and r f = r f ( u f t ) . As we mentioned earlier , the weight W y in the slotvalue decoder is shared between the student and the teacher networks and will not be updated . The teacher - student optimization only adjusts the weights related to the language - specific parts in Figure 3 ( i.e. , utterance encoding and context gating ) . Therefore , the shared weight | | W y | | is seen as a constant . Furthermore , c f v , c e v | | g e | | 2 can be seen as a constant since the teacher gate is fixed . Since we use batch normalization layer to normalize the encoder output ( described in Figure 3 ) , | | r f ( u f t ) | | 2 can also be treated as a constant C 2 . Therefore , we formally write the upper bound of J 1 as our surrogate cost function J : J = C1 | | re ( u e t ) \u2212 r f ( u f t ) | | 2 + C2 c f v , c e v | | g e \u2212 g f | | 2 ( 6 ) The surrogate cost has successfully decoupled utterance encoder with context gate , and we use J r and J g to measure the encoder matching cost and the gate matching cost , respectively . Jr = | | re ( u e t ) \u2212 r f ( u f t ) | | 2 Jg = c f v , c e v | | g e \u2212 g f | | 2 ( 7 ) The encoder cost J r is optimized to distill the knowledge from the teacher encoder to student encoder while gate cost J g is optimized to distill the knowledge from teacher gate to student gate . This objective function successfully decouples the optimization of encoder and gate , thus we are able to optimize J r and J g separately from different data sources . Recall that we can easily simulate the target - side system acts , slot - value pairs ( c f s , c f v , a f ) by using the ontology mapping M . Therefore , optimizing J g is relatively easy . Formally , we write the gate matching cost as follows : However , exact optimization of J r is difficult and we have to approximate it using external parallel data . We consider two kinds of external resources ( bilingual corpus and bilingual dictionary ) in the sections 5.2 - 5.3 ( see Figure 5 for the main idea ) . Jg =", "entities": [[795, 797, "MethodName", "batch normalization"], [1036, 1037, "MetricName", "Recall"], [1068, 1069, "MethodName", "ontology"]]}
{"text": "In our first scenario , we assume there exists a parallel corpus D p consisting of sentence pairs from the source language and the target language . In this case , the cost function ( 6 ) is approximated by J = E ( me , m f ) Dp | | re ( me ) \u2212 r f ( m f ) | | 2 + \u03b1Jg ( 9 ) where \u03b1 is the balancing factor and J g is defined in ( 6 ) . The cost function ( 9 ) is minimized by stochastic gradient descent . At test time , we switch the encoder to receive target language inputs .", "entities": [[72, 73, "HyperparameterName", "\u03b1"], [96, 99, "MethodName", "stochastic gradient descent"]]}
{"text": "In the second scenario , we assume there exists no parallel corpus but a bilingual dictionary D B that defines the correspondence between source words and target words ( a one - to - many mapping { w : M D ( w ) } ) . Likewise , it is infeasible to optimize the exact encoder cost J r due to the lack of target - side utterances . We propose a word replacement strategy ( to be described later ) to generate synthetic parallel sentence\u00fb f t of \" mixed \" language . Then , we use the generated target parallel sentences to approximate the cost ( 6 ) by Jr = E u t D | | re ( u e t ) \u2212 r f ( \u00fb f t ) | | 2 + \u03b1Jg ( 10 ) where \u03b1 is the balancing factor . For word replacement , we first decide the number of words N w to be replaced , then we draw N w positions randomly from the source utterance and substitute the corresponding word w i with their target word synonym from M D ( w ) based on the context as follows : jp ( Nw = i ) = exp ( \u2212i / \u03c4 ) i < N exp ( \u2212i /\u03c4 ) p ( \u0175 ) = \u0175 h\u0175 w M ( w i ) w h\u0175 ( 11 ) where h\u0175 = 2 k=\u22122 : k = 0 w i+k represents the context vector and N denotes the utterance length . The context similarity of context and the targetside synonym can better help us in choosing the most appropriate candidate from the list . In our following experiments , we adjust the temperature of \u03c4 to control the aggressiveness of replacement .", "entities": [[143, 144, "HyperparameterName", "\u03b1"], [247, 249, "HyperparameterName", "k ="], [249, 250, "DatasetName", "0"]]}
{"text": "Here we highlight the baselines we use to compare with our cross - lingual algorithm as follows : ( 1 ) Supervised : this baseline algorithm assumes the existence of annotated dialog belief tracking datasets , and it determines the upper bound of the DST model . ( 2 ) w/o Transfer : this algorithm trains an English NBT , and then directly feeds target language into the embedding level as inputs during test time to evaluate the performance . ( 3 ) Ontology - match : this algorithm directly uses exact string matching against the utterance to discover the perceived slot - value pairs , it directly assigns a high score to the appearing candidates . ( 4 ) Translation - based : this system pre - trains a translator on the external bilingual corpus and then translates the English dialog and ontology into target language as \" annotated \" data , which is used to train the NBT in the target language domain ( more details about the implementation , performance and examples are listed in the appendix ) . ( 5 ) Word - By - Word ( WBW ) : this system transforms the English dialog corpus into target language word by word using the bilingual dictionary , which is used to train the NBT in target side . We demonstrate the results for our proposed algorithms and other competing algorithms in Table 2 , from which we can easily conclude that that ( i ) our Decoupled NBT does not affect the performance , and ( ii ) our cross - lingual NBT framework is able to achieve significantly better accuracy for both languages in both parallel - resource scenarios . Compare with Translator / WBW . With bilingual corpus , XL - NBT - C with pre - trained bilingual embedding can significantly outperform our Translator baseline ( Klein et al , 2017 ) . This is intuitive because the translation model requires both source - side encoding and target - side wordby - word decoding , while our XL - NBT only needs a bilingual source - encoding to align two vector space , which averts the compounded decoding errors . With the bilingual dictionary , the word - byword translator is very weak and leading to many broken target sentences , which poses challenges for DST training . In comparison , our XL - NBT - D can control the replacement by adjusting its temperature to maintain the stability of utterance representation . Furthermore , for both cases , our teacher - student framework can make use of the knowledge learned in source - side NBT to assist its decision making , while translator - based methods learn from scratch .", "entities": [[21, 23, "DatasetName", "Supervised :"], [83, 84, "MethodName", "Ontology"], [120, 121, "TaskName", "Translation"], [143, 144, "MethodName", "ontology"], [275, 276, "MetricName", "accuracy"], [447, 449, "TaskName", "decision making"]]}
{"text": "From the table , we can easily observe that bilingual corpus is obviously a more informative parallel resource to perform cross - lingual transfer learning . The accuracy of XL - NBT - D is lower than XL - NBT - C. We conjecture that our replace - ment strategy to generate \" mixed \" language utterance can sometimes break the semantic coherence and cause additional noises during the transfer process , which remarkably degrades the DST performance . Monolingual vs. Bilingual embedding . From the table , we can observe that the bilingual embedding and monolingual embedding does not make much difference in supervised training . However , the gap in the bilingual corpus case is quite obvious . Monolingual embedding even causes the transfer to fail in a bilingual dictionary case . We conjecture that the bilingual word embedding already contain many alignment information between two languages , which largely eases the training of encoder matching objective . German vs. Italian As can be seen , the transfer learning results for Italian are remarkably higher than German , especially for the \" Goal \" accuracy . We conjecture that it is due to German declension , which can produce many word forms . The very diverse word forms present great challenges for DST to understand its intention behind . Especially for the bilingual dictionary , German tends to have much longer replacement candidate lists than Italian , which introduces more noises to the replacement procedure . Error Analysis Here we showcase the most frequent error types in subsection 6.1 . From our observation , these three types of errors distribute evenly in the test dialogs . The error mainly comes from the unaligned utterance space , which leads to failure in understanding the intention of human utterance in the target language . This can lead the system to fail in modifying the dialog state or maintaining the previous dialog states .", "entities": [[20, 24, "TaskName", "cross - lingual transfer"], [27, 28, "MetricName", "accuracy"], [169, 171, "TaskName", "transfer learning"], [186, 187, "MetricName", "accuracy"], [248, 249, "MetricName", "Error"]]}
{"text": "Here we want to further highlight the comparison between our transfer learning algorithm with the MT - based approach . Though our approach outperforms the standard Translator trained on IWSLT - 2014 , it does not necessarily claim that our transfer algorithm outperforms any translation methods on any parallel corpus . In our further ablation studies , we found that using Google Translator 6 can actually achieve a better score than our transfer algorithm , which is understandable considering the complexity of Google Translator and the much larger parallel corpus it leverages . By leveraging more close - to - domain corpus and comprehensive entity recognition / replacement strategy , the translator model is able to achieve a higher score . Apparently , we need to trade off the efficiency for the accuracy . For DST problem , it is an overkill to introduce a more complex translation algorithm , what we pursue is a simple yet efficient algorithm to achieve promising scores . It is also worth mentioning that our XL - NBT algorithm only takes several hours to achieve the reported score , while the translator model takes much more time and memory to train depending on the complexity . Thus , the simplicity and efficiency makes our model a better fit for rarelanguage and limited - budget scenarios .", "entities": [[10, 12, "TaskName", "transfer learning"], [61, 62, "DatasetName", "Google"], [82, 83, "DatasetName", "Google"], [132, 133, "MetricName", "accuracy"]]}
{"text": "Here we investigate the effect ' of hyper - parameter \u03b1 , \u03c4 on the evaluation results . The \u03b1 is used to balance the optimization of encoder constraint and gate constraint , where larger \u03b1 means more optimization on gate constraint . The temperature \u03c4 is used to control the aggressiveness of the replacement XL - NBT - D , where smaller \u03c4 means more source words are replaced by target synonyms . From the table experimental results are not very sensitive to \u03b1 , a dramatic change of \u03b1 will not harm the final results too much , we simply choose \u03b1 = 1 as the hyper - parameter . In contrast , the system is more sensitive to temperature . Too conservative replacement will lead to weak transfer , while too aggressive replacement will destroy the utterance representation . Therefore , we choose the a moderate temperature of \u03c4 = 0.1 throughout our experiments . We also draw the learning curve ( Precision vs. Iteration ) in the Appendix for both XL - NBT - C and XL - NBT - D. The learning curves show that our algorithm is stable and converges quickly , and the reported results are highly reproducible .", "entities": [[10, 11, "HyperparameterName", "\u03b1"], [19, 20, "HyperparameterName", "\u03b1"], [35, 36, "HyperparameterName", "\u03b1"], [84, 85, "HyperparameterName", "\u03b1"], [90, 91, "HyperparameterName", "\u03b1"], [103, 104, "HyperparameterName", "\u03b1"], [165, 166, "MetricName", "Precision"]]}
{"text": "Rapid progress has been made in the field of reading comprehension and question answering , where several systems have achieved human parity in some simplified settings . However , the performance of these models degrades significantly when they are applied to more realistic scenarios , where answers are involved with various types , multiple text strings are correct answers , or discrete reasoning abilities are required . In this paper , we introduce the Multi - Type Multi - Span Network ( MTMSN ) , a neural reading comprehension model that combines a multi - type answer predictor designed to support various answer types ( e.g. , span , count , negation , and arithmetic expression ) with a multi - span extraction method for dynamically producing one or multiple text spans . In addition , an arithmetic expression reranking mechanism is proposed to rank expression candidates for further confirming the prediction . Experiments show that our model achieves 79.9 F1 on the DROP hidden test set , creating new state - of - the - art results . Source code 1 is released to facilitate future work .", "entities": [[9, 11, "TaskName", "reading comprehension"], [12, 14, "TaskName", "question answering"], [87, 89, "TaskName", "reading comprehension"], [160, 161, "MetricName", "F1"], [163, 164, "DatasetName", "DROP"]]}
{"text": "This paper considers the reading comprehension task in which some discrete - reasoning abilities are needed to correctly answer questions . Specifically , we focus on a new reading comprehension dataset called DROP ( Dua et al , 2019 ) , which requires Discrete Reasoning Over the content of Paragraphs to obtain the final answer . Unlike previous benchmarks such as CNN / DM ( Hermann et al , 2015 ) and SQuAD ( Rajpurkar et al , 2016 ) that have been well solved Devlin et al , 2019 ) , DROP is substantially more challenging in three ways . First , the answers to the questions involve a wide range of types such as numbers , dates , or text strings . Therefore , various kinds of prediction strategies are required to successfully find the answers . Second , rather than restricting the answer to be a span of text , DROP loosens the constraint so that answers may be a set of multiple text strings . Third , for questions that require discrete reasoning , a system must have a more comprehensive understanding of the context and be able to perform numerical operations such as addition , counting , or sorting . Existing approaches , when applied to this more realistic scenario , have three problems . First , to produce various answer types , Dua et al ( 2019 ) extend previous one - type answer prediction ( Seo et al , 2017 ) to multi - type prediction that supports span extraction , counting , and addition / subtraction . However , they have not fully considered all potential types . Take the question \" What percent are not non - families ? \" and the passage snippet \" 39.9 % were non - families \" as an example , a negation operation is required to infer the answer . Second , previous reading comprehension models ( Wang et al , 2017 ; Yu et al , 2018 ; Hu et al , 2018 ) are designed to produce one single span as the answer . But for some questions such as \" Which ancestral groups are smaller than 11 % ? \" , there may exist several spans as correct answers ( e.g. , \" Italian \" , \" English \" , and \" Polish \" ) , which can not be well handled by these works . Third , to support numerical reasoning , prior work ( Dua et al , 2019 ) learns to predict signed numbers for obtaining an arithmetic expression that can be executed by a symbolic system . Nevertheless , the prediction of each signed number is isolated , and the expression 's context information has not been considered . As a result , obviously - wrong expressions , such as all predicted signs are either minus or zero , are likely produced . To address the above issues , we introduce the Multi - Type Multi - Span Network ( MTMSN ) , a neural reading comprehension model for predicting various types of answers as well as dynamically extracting one or multiple spans . MTMSN utilizes a series of pre - trained Transformer blocks ( Devlin et al , 2019 ) to obtain a deep bidirectional context representation . On top of it , a multi - type answer predictor is proposed to not only support previous prediction strategies such as span , count number , and arithmetic expression , but also add a new type of logical negation . This results in a wider range of coverage of answer types , which turns out to be crucial to performance . Besides , rather than always producing one single span , we present a multi - span extraction method to produce multiple answers . The model first predicts the number of answers , and then extracts non - overlapped spans to the specific amount . In this way , the model can learn to dynamically extract one or multiple spans , thus being beneficial for multi - answer cases . In addition , we propose an arithmetic expression reranking mechanism to rank expression candidates that are decoded by beam search , so that their context information can be considered during reranking to further confirm the prediction . Our MTMSN model outperforms all existing approaches on the DROP hidden test set by achieving 79.9 F1 score , a 32.9 % absolute gain over prior best work at the time of submission . To make a fair comparison , we also construct a baseline that uses the same BERT - based encoder . Again , MTMSN surpasses it by obtaining a 13.2 F1 increase on the development set . We also provide an in - depth ablation study to show the effectiveness of our proposed methods , analyze performance breakdown by different answer types , and give some qualitative examples as well as error analysis .", "entities": [[4, 6, "TaskName", "reading comprehension"], [28, 30, "TaskName", "reading comprehension"], [32, 33, "DatasetName", "DROP"], [72, 73, "DatasetName", "SQuAD"], [92, 93, "DatasetName", "DROP"], [153, 154, "DatasetName", "DROP"], [251, 253, "TaskName", "type prediction"], [318, 320, "TaskName", "reading comprehension"], [507, 509, "TaskName", "reading comprehension"], [534, 535, "MethodName", "Transformer"], [728, 729, "DatasetName", "DROP"], [735, 737, "MetricName", "F1 score"], [768, 769, "MethodName", "BERT"], [782, 783, "MetricName", "F1"]]}
{"text": "Although existing reading comprehension tasks focus exclusively on finding one span of text as the final answer , DROP loosens the restriction so that the answer to the question may be several text spans . Therefore , specific adaption should be made to extend previous single - span extraction to multi - span scenario . To do this , we propose directly predicting the number of spans and model it as a classification problem . This is achieved by computing a probability distribution on span amount as p span = softmax ( FFN ( [ h Q 2 ; h P 2 ; h CLS ] ) ) To extract non - overlapped spans to the specific amount , we adopt the non - maximum suppression ( NMS ) algorithm ( Rosenfeld and Thurston , 1971 ) that is widely used in computer vision for pruning redundant bounding boxes , as shown in Algorithm 1 . Concretely , the model first proposes a set of top - K spans S according to the descending order of the span score , which is computed as p start k p end l for the span ( k , l ) . It also predicts the amount of extracted spans t from p span , and initializes a new setS. Next , we add the span s i that possesses the maximum span score to the setS , and remove it from S. We also delete any remaining span s j that overlaps with s i , where the degree of overlap is measured using the text - level F1 function . This process is repeated for remaining spans in S , until S is empty or the size ofS reaches t.", "entities": [[2, 4, "TaskName", "reading comprehension"], [18, 19, "DatasetName", "DROP"], [90, 91, "MethodName", "softmax"], [266, 267, "MetricName", "F1"]]}
{"text": "Since DROP does not indicate the answer type but only provides the answer string , we therefore adopt the weakly supervised annotation scheme , as suggested in Berant et al ( 2013 ) ; Dua et al ( 2019 ) . We find all possible annotations that point to the gold answer , including matching spans , arithmetic expressions , correct count numbers , negation operations , and the number of spans . We use simple rules to search over all mentioned numbers to find potential negations . That is , if 100 minus a number is equal to the answer , then a negation occurs on this number . Besides , we only search the addition / subtraction of three numbers at most due to the exponential search space . To train our model , we propose using a twostep training method composed of an inference step and a training step . In the first step , we use the model to predict the probabilities of sign assignments for numbers . If there exists any annotation of arithmetic expressions , we run beam search to produce expression candidates and label them as either correct or wrong , which are later used for supervising the reranking component . In the second step , we adopt the marginal likelihood objective function ( Clark and Gardner , 2018 ) , which sums over the probabilities of all possible annotations including the above labeled expressions . Notice that there are two objective functions for the multi - span component : one is a distantly - supervised loss that maximizes the probabilities of all matching spans , and the other is a classification loss that maximizes the probability on span amount . At test time , the model first chooses the answer type and then performs specific prediction strategies . For the span type , we use Algorithm 1 for decoding . If the type is addition / subtraction , arithmetic expression candidates will be proposed and further reranked . The expression with the maximum product of cumulative sign probability and reranking probability is chosen . As for the counting type , we choose the number that has the maximum counting probability . Finally , if the type is negation , we find the number that possesses the largest negation probability , and then output the answer as 100 minus this number .", "entities": [[1, 2, "DatasetName", "DROP"], [263, 264, "MetricName", "loss"], [279, 280, "MetricName", "loss"]]}
{"text": "Dataset We consider the reading comprehension benchmark that requires Discrete Reasoning Over Paragraphs ( DROP ) ( Dua et al , 2019 ) prehensive understanding of the context as well as the ability of numerical reasoning are required . Model settings We build our model upon two publicly available uncased versions of BERT : BERT BASE and BERT LARGE 2 , and refer readers to Devlin et al ( 2019 ) for details on model sizes . We use Adam optimizer with a learning rate of 3e - 5 and warmup over the first 5 % steps to train . The maximum number of epochs is set to 10 for base models and 5 for large models , while the batch size is 12 or 24 respectively . A dropout probability of 0.1 is used unless stated otherwise . The number of counting class is set to 10 , and the maximum number of spans is 8 . The beam size is 3 by default , while the maximum amount of signed numbers M is set to 4 . All texts are tokenized using Word - Piece vocabulary ( Wu et al , 2016 ) , and truncated to sequences no longer than 512 tokens . Baselines Following the implementation of Augmented QANet ( NAQANet ) ( Dua et al , 2019 ) , we introduce a similar baseline called Augmented BERT ( NABERT ) . The main difference is that we replace the encoder of QANet ( Yu et al , 2018 ) with the pre - trained Transformer blocks ( Devlin et al , 2019 ) . Moreover , it also supports the prediction of various answer types such as span , arithmetic expression , and count number .", "entities": [[4, 6, "TaskName", "reading comprehension"], [14, 15, "DatasetName", "DROP"], [52, 53, "MethodName", "BERT"], [54, 55, "MethodName", "BERT"], [55, 56, "MethodName", "BASE"], [57, 58, "MethodName", "BERT"], [79, 80, "MethodName", "Adam"], [80, 81, "HyperparameterName", "optimizer"], [83, 85, "HyperparameterName", "learning rate"], [102, 105, "HyperparameterName", "number of epochs"], [120, 122, "HyperparameterName", "batch size"], [231, 232, "MethodName", "BERT"], [259, 260, "MethodName", "Transformer"]]}
{"text": "Two metrics , namely Exact Match ( EM ) and F1 score , are utilized to evaluate models . We use the official script to compute these scores . Since the test set is hidden , we only submit the best single model to obtain test results . Table 1 shows the performance of our model and other competitive approaches on the develop -", "entities": [[4, 6, "MetricName", "Exact Match"], [7, 8, "MetricName", "EM"], [10, 12, "MetricName", "F1 score"]]}
{"text": "Component ablation To analyze the effect of the proposed components , we conduct ablation studies on the development set . As illustrated in Table 2 , the use of addition and subtraction is extremely crucial : the EM / F1 performance of both the base and large models drop drastically by more than 20 points if it is removed . Predicting count numbers is also an important component that contributes nearly 5 % gain on both metrics . Moreover , enhancing the model with the negation type significantly increases the F1 by roughly 9 percent on both models . In brief , the above results show that multi - type answer prediction is vitally important for handling different forms of answers , especially in cases where discrete reasoning abilities are required . We also report the performance after removing the multi - span extraction method . The results reveal that it has a more negative impact on the F1 score . We interpret this phenomenon as follows : producing multiple spans that are partially matched with ground - truth answers is much easier than generating an exactly - matched set of multiple answers . Hence for multi - span scenarios , the gain of our method on F1 is relatively easier to obtain than the one on EM . Finally , to ablate arithmetic expression reranking , we simply use the arithmetic expression that has the maximum cumulative sign probability instead . We find that our reranking mechanism gives 1.8 % gain on both metrics for the large model . This confirms that validating expression candidates with their context information is beneficial for filtering out highly - confident but wrong predictions . Architecture ablation We further conduct a detailed ablation in Table 3 to evaluate our architecture designs . First , we investigate the effects of some \" global vectors \" used in our model . Specifically , we find that removing the question and passage vectors from all involved computation leads to 1.3 % drop on F1 . Ablating the representation of [ CLS ] token leads to even worse results . We also try to use the last hidden representation ( denoted as M 3 ) to calculate question and passage vectors , but find that does not work . Next , we remove the gating mechanism used during span prediction , and observe a nearly 0.8 % decline on both metrics . Finally , we share parameters between the arithmetic expression component and the negation component , and find the performance drops by 1.1 % on F1 .", "entities": [[37, 38, "MetricName", "EM"], [39, 40, "MetricName", "F1"], [90, 91, "MetricName", "F1"], [158, 160, "MetricName", "F1 score"], [207, 208, "MetricName", "F1"], [217, 218, "MetricName", "EM"], [337, 338, "MetricName", "F1"], [429, 430, "MetricName", "F1"]]}
{"text": "Performance breakdown We now provide a quantitative analysis by showing performance breakdown on the development set . Table 4 shows that our gains mainly come from the most frequent number type , which requires various types of symbolic , discrete reasoning operations . Moreover , significant improvements are also obtained in the multi - span category , where the F1 score increases by more than 40 points . This result further proves the validity of our multi - span extraction method . We also give the performance statistics that are categorized according to the predicted answer types in Table 5 . As shown in the Table , the main improvements are due to the addition / subtraction and negation types . We conjecture that there are two reasons for these improvements . First , our proposed expression reranking mechanism helps validate candidate expressions . Second , a new inductive bias that enables the model to perform logical negation has been introduced . The impressive performance on the negation type confirms our judgement , and suggests that the model is able to find most of negation operations . In addition , we also observe promising gains brought by the span and count types . We think the gains are mainly due to the multi - span extraction method as well as architecture designs . Effect of maximum number of spans To investigate the effect of maximum number of spans on multi - span extraction , we conduct an experiment on the dev set and show the curves in Figure 3 . We vary the value from 2 to 12 , increased by 2 , and also include the extreme value 1 . According to the Figure , the best results are obtained at 8 . A higher value could potentially increase the answer recall but damage the precision by making more predictions , and a smaller value may force the model to produce limited number of answers , resulting in high precision but low recall . Therefore , a value of 8 turns out to be a good trade - off between recall and precision . Moreover , when the value decreases to 1 , the multi - span extraction degrades to previous single - span scenario , and the performance drops significantly . Effect of beam size and M We further investigate the effect of beam size and maximum amount of signed numbers in Figure 4 . As we can see , a beam size of 3 leads to the best performance , likely because a larger beam size might confuse the model as too many candidates are ranked , on the other hand , a small size could be not sufficient to cover the correct expression . In addition , we find that the performance constantly decreases as the maximum threshold M increases , suggesting that most of expressions only contain two or three signed numbers , and setting a larger threshold could bring in additional distractions .", "entities": [[59, 61, "MetricName", "F1 score"]]}
{"text": "We list the annotation statistics on the DROP train set in Table 6 . As we can see , only annotating matching spans results in a labeled ratio of 56.4 % , indicating that DROP includes various answer types beyond text spans . By further considering the arithmetic expression , the ratio increase sharply to 91.7 % , suggesting more than 35 % answers need to be inferred with numeral reasoning . Continuing adding counting leads to a percentage of 94.4 % , and a final 97.9 % coverage is achieved by additionally taking negation into account . More importantly , the F1 score constantly increases as more answer types are considered . This result is consistent with our observations in ablation study . Error analysis Finally , to better understand the remaining challenges , we randomly sample 100 incorrectly predicted examples based on EM and categorize them into 7 classes . 38 % of errors are incorrect arithmetic computations , 18 % require sorting over multiple entities , 13 % are due to mistakes on multi - span extraction , 10 % are singlespan extraction problems , 8 % involve miscounting , another 8 % are wrong predictions on span number , the rest ( 5 % ) are due to various reasons such as incorrect preprocessing , negation error , and so on . See Appendix for some examples of the above error cases .", "entities": [[7, 8, "DatasetName", "DROP"], [34, 35, "DatasetName", "DROP"], [102, 104, "MetricName", "F1 score"], [124, 125, "MetricName", "Error"], [144, 145, "MetricName", "EM"]]}
{"text": "We introduce MTMSN , a multi - type multi - span network for reading comprehension that requires discrete reasoning over the content of paragraphs . We enhance a multi - type answer predictor to support logical negation , propose a multi - span extraction method for producing multiple answers , and design an arithmetic expression reranking mechanism to further confirm the prediction . Our model achieves 79.9 F1 on the DROP hidden test set , creating new state - of - the - art results . As future work , we would like to consider handling additional types such as sorting or multiplication / division . We also plan to explore more advanced methods for performing complex numerical reasoning .", "entities": [[13, 15, "TaskName", "reading comprehension"], [67, 68, "MetricName", "F1"], [70, 71, "DatasetName", "DROP"]]}
{"text": "Text summarization is considered as a challenging task in the NLP community . The availability of datasets for the task of multilingual text summarization is rare , and such datasets are difficult to construct . In this work , we build an abstract text summarizer for the German language text using the state - of - the - art \" Transformer \" model . We propose an iterative data augmentation approach which uses synthetic data along with the real summarization data for the German language . To generate synthetic data , the Common Crawl ( German ) dataset is exploited , which covers different domains . The synthetic data is effective for the low resource condition and is particularly helpful for our multilingual scenario where availability of summarizing data is still a challenging issue . The data are also useful in deep learning scenarios where the neural models require a large amount of training data for utilization of its capacity . The obtained summarization performance is measured in terms of ROUGE and BLEU score . We achieve an absolute improvement of +1.5 and +16.0 in ROUGE1 F1 ( R1 F1 ) on the development and test sets , respectively , compared to the system which does not rely on data augmentation .", "entities": [[0, 2, "TaskName", "Text summarization"], [22, 24, "TaskName", "text summarization"], [60, 61, "MethodName", "Transformer"], [68, 70, "TaskName", "data augmentation"], [79, 80, "TaskName", "summarization"], [92, 94, "DatasetName", "Common Crawl"], [163, 164, "TaskName", "summarization"], [172, 174, "MetricName", "BLEU score"], [186, 187, "MetricName", "F1"], [189, 190, "MetricName", "F1"], [209, 211, "TaskName", "data augmentation"]]}
{"text": "The Transformer model is implemented in OpenNMT - py . To train the model , we use a single GPU . To fit the model to the GPU cluster , a batch size equal to 4 , 096 is selected for training . The validation batch size is set to 8 . We use an initial learning rate of 2 , drop out of 0.2 and 8 , 000 warm - up steps . Decoding uses a beam size of 10 and we did not set any minimum length of output summary .", "entities": [[1, 2, "MethodName", "Transformer"], [31, 33, "HyperparameterName", "batch size"], [45, 47, "HyperparameterName", "batch size"], [56, 58, "HyperparameterName", "learning rate"]]}
{"text": "We evaluate the results for every 10 , 000 iterations on the dev and test set . The automatic evaluation results based on the dev and test set are shown in Table 2 with sample summaries in Table 3 . To evaluate the proposed algorithms , we use ROUGE ( Recall - Oriented Understudy for Gisting Evaluation ) score , which is a popular metric for text summarization task , and has several variants like ROUGE - N , and ROUGE - L , which measure the overlap of n - grams between the system and reference summary ( LIN , 2004 ) . We use ROUGE 1 F1 ( R1 F1 ) , ROUGE 2 F1 ( R2 F1 ) , and ROUGE L F1 ( RL F1 ) for scoring the generated summary . In addition , we also use the SacreBLEU 4 evaluation metric ( Post , 2018 ) . Figure 3 presents the learning curves for the models ( S1 and S2 ) on the development set . It can be seen that there is a variance ( e.g. word selection , summary length ) for model S2 generated summary as compared with model S1 . During manual verification , we found that the summaries generated without a minimum length constraint appear better compared to summaries with minimum length constraint . Although we do not explicitly specify a minimum length parameter for generating summaries for the models , the average length of words generated by model S2 ( e.g. 41.42 words ) is longer than the model S1 ( e.g. 39.81 words ) . Some data ( e.g. name , year ) were found inconsistent during a comparison of the generated summary with respect to the reference . There is a variance in summaries generated by model S3 as compared to S2 and S1 . In terms of Rouge score model S3 outperforms model S1 but perform worse than model S2 ( see Table 2 ) .", "entities": [[50, 51, "MetricName", "Recall"], [66, 68, "TaskName", "text summarization"], [80, 83, "MetricName", "ROUGE - L"], [108, 109, "MetricName", "F1"], [111, 112, "MetricName", "F1"], [116, 117, "MetricName", "F1"], [119, 120, "MetricName", "F1"], [125, 126, "MetricName", "F1"], [128, 129, "MetricName", "F1"], [143, 144, "MetricName", "SacreBLEU"]]}
{"text": "Visual Question Answering ( VQA ) ( Antol et al , 2015 ) , the task of answering questions about visual content , was proposed to facilitate the development of models with human - like visual and linguistic understanding . However , existing VQA models often exploit superficial statistical biases to produce responses , instead of producing the right answers for the right reasons ( Kafle et al , 2019 ) . The VQA - CP dataset showcases this phenomenon by incorporating different question type / answer distributions in the train and test sets . Since the linguistic priors in the train and test sets differ , models that exploit these priors fail on the test set . To tackle this issue , recent works have endeavored to enforce proper visual grounding , where the goal is to make models produce answers by looking at relevant visual regions ( Gan et al , 2017 ; Selvaraju et al , Figure 1 : We find that existing visual sensitivity enhancement methods improve performance on VQA - CPv2 through regularization as opposed to proper visual grounding . 2019 ; Wu and Mooney , 2019 ) , instead of exploiting linguistic priors . These approaches rely on additional annotations / cues such as human - based attention maps ( Das et al , 2017 ) , textual explanations ( Huk Park et al , 2018 ) and object label predictions ( Ren et al , 2015 ) to identify relevant regions , and train the model to base its predictions on those regions , showing large improvements ( 8 - 10 % accuracy ) on the VQA - CPv2 dataset . Here , we study these methods . We find that their improved accuracy does not actually emerge from proper visual grounding , but from regularization effects , where the model forgets the linguistic priors in the train set , thereby performing better on the test set . To support these claims , we first show that it is possible to achieve such gains even when the model is trained to look at : a ) irrelevant visual regions , and b ) random visual regions . Second , we show that differences in the predictions from the variants trained with relevant , irrelevant and random visual regions are not statistically significant . Third , we show that these methods degrade performance when the priors remain intact and instead work on VQA - CPv2 by hurting its train accuracy . Based on these observations , we hypothesize that controlled degradation on the train set al ows models to forget the training priors to improve test accuracy . To test this hypothesis , we introduce a simple regularization scheme that zeros out the ground truth answers , thereby always penalizing the model , whether the predictions are correct or incorrect . We find that this approach also achieves near state - of - the - art performance ( 48.9 % on VQA - CPv2 ) , providing further support for our claims . While we agree that visual grounding is a useful direction to pursue , our experiments show that the community requires better ways to test if systems are actually visually grounded . We make some recommendations in the discussion section .", "entities": [[0, 3, "DatasetName", "Visual Question Answering"], [4, 5, "TaskName", "VQA"], [43, 44, "TaskName", "VQA"], [73, 76, "DatasetName", "VQA - CP"], [130, 132, "TaskName", "visual grounding"], [173, 174, "TaskName", "VQA"], [182, 184, "TaskName", "visual grounding"], [269, 270, "MetricName", "accuracy"], [273, 274, "TaskName", "VQA"], [290, 291, "MetricName", "accuracy"], [297, 299, "TaskName", "visual grounding"], [408, 409, "TaskName", "VQA"], [415, 416, "MetricName", "accuracy"], [442, 443, "MetricName", "accuracy"], [497, 498, "TaskName", "VQA"], [513, 515, "TaskName", "visual grounding"]]}
{"text": "Some recent approaches employ a question - only branch as a control model to discover the questions most affected by linguistic correlations . The question - only model is either used to perform adversarial regularization ( Grand and Belinkov , 2019 ; Ramakrishnan et al , 2018 ) or to re - scale the loss based on the difficulty of the question ( Cadene et al , 2019 ) . However , when these ideas are applied to the UpDn model ( Anderson et al , 2018 ) , which attempts to learn correct visual grounding , these approaches achieve 4 - 7 % lower accuracy compared to the state - of - the - art methods .", "entities": [[54, 55, "MetricName", "loss"], [94, 96, "TaskName", "visual grounding"], [105, 106, "MetricName", "accuracy"]]}
{"text": "Both Human Importance Aware Network Tuning ( HINT ) ( Selvaraju et al , 2019 ) and Self Critical Reasoning ( SCR ) ( Wu and Mooney , 2019 ) , train the network to be more sensitive towards salient image regions by improving the alignment between visual cues and gradient - based sensitivity scores . HINT proposes a ranking loss between humanbased importance scores ( Das et al , 2016 ) and the gradient - based sensitivities . In contrast , SCR does not require exact saliency ranks . Instead , it penalizes the model if correct answers are more sensitive towards non - important regions as compared to important regions , and if incorrect answers are more sensitive to important regions than correct answers .", "entities": [[60, 61, "MetricName", "loss"]]}
{"text": "To reduce the reliance on linguistic priors , visual sensitivity enhancement methods attempt to train the model to be more sensitive to relevant visual regions when answering questions . Following ( Wu and Mooney , 2019 ) , we define the sensitivity of an answer a with respect to a visual region v i as : S ( a , v i ) : = ( \u2207 v i P ( a | I , Q ) ) T 1 . ( 2 ) Existing methods propose the following training objectives to improve grounding using S : HINT uses a ranking loss , which penalizes the model if the pair - wise rankings of the sensitivities of visual regions towards ground truth answers a gt are different from the ranks computed from the human - based attention maps . SCR divides the region proposals into influential and non - influential regions and penalizes the model if : 1 ) S ( a gt ) of a non - influential region is higher than an influential region , and 2 ) the region most influential for the correct answer has even higher sensitivity for incorrect answers . Both methods improve baseline accuracy by 8 - 10 % . Is this actually due to better visual grounding ? 4 Why Did the Performance Improve ? We probe the reasons behind the performance improvements of HINT and SCR . We first analyze if the results improve even when the visual cues are irrelevant ( Sec . 4.2 ) or random ( Sec . 4.3 ) and examine if their differences are statistically significant ( Sec . 4.4 ) . Then , we analyze the regularization effects by evaluating the performance on VQA - CPv2 's train split ( Sec . 4.5 ) and the behavior on a dataset without changing priors ( Sec . 4.6 ) . We present a new metric to assess visual grounding in Sec . 4.7 and describe our regularization method in Sec . 5 .", "entities": [[101, 102, "MetricName", "loss"], [200, 201, "MetricName", "accuracy"], [213, 215, "TaskName", "visual grounding"], [288, 289, "TaskName", "VQA"], [321, 323, "TaskName", "visual grounding"]]}
{"text": "To test if the changes in results were statistically significant , we performed Welch 's t - tests ( Welch , 1938 ) on the predictions of the variants trained on relevant , irrelevant and random cues . We pick Welch 's t - test over the Student 's t - test , because the latter assumes equal variances for predictions from different variants . To perform the tests , we first randomly sample 5000 subsets of non - overlapping test instances . We then average the accuracy of each subset across 5 runs , obtaining 5000 values . Next , we run the t - tests for HINT and SCR separately on the subset accuracies . As shown in Table 2 , the p - values across the variants of HINT and SCR are greater than or equal to 0.3 . Using a confidence level of 95 % ( \u03b1 = 0.05 ) , we fail to reject the null hypothesis that the mean difference between the paired values is 0 , showing that the variants are not statistically significantly different from each other . We also compare the predictions of HINT / SCR against baseline , and find that p - values are all zeros , showing that the differences have statistical significance . Percentage of Overlaps : To further check if the variants trained on irrelevant or random regions gain performance in a manner similar to the models trained on relevant regions , we compute the overlap between their predictions on VQA - CPv2 's test set . The percentage of overlap is defined as : % Overlap = n same n total \u00d7 100 % , where , n same denotes the number of instances where either both variants were correct or both were incorrect and n total denotes the total number of test instances . As shown in Table 2 , we compare % Overlap between different variants of HINT / SCR with baseline and against each other . We find 89.7 \u2212 91.9 % and 89.5 \u2212 92.0 % overlaps for different variants of HINT and SCR respectively . These high overlaps suggest that the variants are not working in fundamentally different manners .", "entities": [[87, 88, "MetricName", "accuracy"], [150, 151, "HyperparameterName", "\u03b1"], [171, 172, "DatasetName", "0"], [254, 255, "TaskName", "VQA"]]}
{"text": "We compare the training accuracies to analyze the regularization effects . As shown in Table 1 , the baseline method has the highest training results , while the other methods cause 6.0 \u2212 14.0 % and 3.3\u221210.5 % drops in the training accuracy on VQA - CPv2 and VQAv2 , respectively . We hypothesize that degrading performance on the train set helps forget linguistic biases , which in turn helps accuracy on VQA - CPv2 's test set but hurts accuracy on VQAv2 's val set .", "entities": [[42, 43, "MetricName", "accuracy"], [44, 45, "TaskName", "VQA"], [70, 71, "MetricName", "accuracy"], [72, 73, "TaskName", "VQA"], [80, 81, "MetricName", "accuracy"]]}
{"text": "The usage of visual cues and sensitivities in existing methods is superfluous because the results indicate that performance improves through degradation of training accuracy . We hypothesize that simple regularization that does not rely on cues or sensitivities can also achieve large performance gains for VQA - CP . To test this hypothesis , we devise a simple loss function which continuously degrades the training accuracy by training the network to always predict a score of zero for all possible answers i.e. produce a zero vector ( 0 ) . The overall loss function can be written as : L : = BCE ( P ( A ) , A gt ) + \u03bbBCE ( P ( A ) , 0 ) , where , BCE refers to the binary cross entropy loss and P ( A ) is a vector consisting of predicted scores for all possible answers . The first term is the binary cross entropy loss between model predictions and ground truth answer vector ( A gt ) , and the second term is our regularizer with a coefficient of \u03bb = 1 . Note that this regularizer continually penalizes the model during the course of the training , whether its predictions are correct or incorrect . As shown in Table 1 , we present results when this loss is used on : a ) Fixed subset covering 1 % of the dataset , b ) Varying subset covering 1 % of the dataset , where a new random subset is sampled every epoch and c ) 100 % of the dataset . Confirming our hypothesis , all variants of our model achieve near state - of - the - art results , solidifying our claim that the performance gains for recent methods come from regularization effects . It is also interesting to note that the drop in training accuracy is lower with this regularization scheme as compared to the state - of - the - art methods . Of course , if any model was actually visually grounded , then we would expect it to improve performances on both train and test sets . We do not observe such behavior in any of the methods , indicating that they are not producing right answers for the right reasons .", "entities": [[23, 24, "MetricName", "accuracy"], [45, 48, "DatasetName", "VQA - CP"], [58, 59, "MetricName", "loss"], [65, 66, "MetricName", "accuracy"], [87, 88, "DatasetName", "0"], [92, 93, "MetricName", "loss"], [120, 121, "DatasetName", "0"], [132, 133, "MetricName", "loss"], [158, 159, "MetricName", "loss"], [221, 222, "MetricName", "loss"], [312, 313, "MetricName", "accuracy"]]}
{"text": "While our results indicate that current visual grounding based bias mitigation approaches do not suffice , we believe this is still a good research direction . However , future methods must seek to verify that performance gains are not stemming from spurious sources by using an experimental setup similar to that presented in this paper . We recommend that both train and test accuracy be reported , because a model truly capable of visual grounding would not cause drastic drops in training accuracy to do well on the test sets . Finally , we advocate for creating a dataset with ground truth grounding available for 100 % of the instances using synthetically generated datasets Kafle et al , 2018 ; Acharya et al , 2019b ; Hudson and Manning , 2019 ; Johnson et al , 2017 ) , enabling the community to evaluate if their methods are able to focus on relevant information . Another alternative is to use tasks that explicitly test grounding , e.g. , in visual query detection an agent must output boxes around any regions of a scene that match the natural language query ( Acharya et al , 2019a ) .", "entities": [[6, 8, "TaskName", "visual grounding"], [63, 64, "MetricName", "accuracy"], [73, 75, "TaskName", "visual grounding"], [82, 83, "MetricName", "accuracy"], [173, 174, "DatasetName", "agent"]]}
{"text": "We compare four different variants of HINT and SCR to study the causes behind the improvements including the models that are fine - tuned on : 1 ) relevant regions ( state - of - the - art methods ) 2 ) irrelevant regions 3 ) fixed random regions and 4 ) variable random regions . For all variants , we fine - tune a pretrained UpDn , which was trained on either VQA - CPv2 or VQAv2 for 40 epochs with a learning rate of 10 \u22123 . When fine - tuning with HINT , SCR or our method , we also use the main binary cross entropy VQA loss , whose weight is set to 1 . The batch size is set to 384 for all of the experiments .", "entities": [[73, 74, "TaskName", "VQA"], [83, 85, "HyperparameterName", "learning rate"], [109, 110, "TaskName", "VQA"], [110, 111, "MetricName", "loss"], [120, 122, "HyperparameterName", "batch size"]]}
{"text": "Following ( Selvaraju et al , 2019 ) , we train HINT on the subset with human - based attention maps ( Das et al , 2017 ) , which are available for 9 % of the VQA - CPv2 train and test sets . The same subset is used for VQAv2 too . The learning rate is set to 2 \u00d7 10 \u22125 and the weight for the HINT loss is set to 2 .", "entities": [[37, 38, "TaskName", "VQA"], [55, 57, "HyperparameterName", "learning rate"], [70, 71, "MetricName", "loss"]]}
{"text": "Since ( Wu and Mooney , 2019 ) reported that humanbased textual explanations ( Huk Park et al , 2018 ) gave better results than human - based attention maps for SCR , we train all of the SCR variants on the subset containing textual explanation - based cues . SCR is trained in two phases . For the first phase , which strengthens the influential objects , we use a learning rate of 5 \u00d7 10 \u22125 , loss weight of 3 and train the model to a maximum of 12 epochs . Then , following ( Wu and Mooney , 2019 ) , for the second phase , we use the best performing model from the first phase to train the second phase , which criticizes incorrect dominant answers . For the second phase , we use a learning rate of 10 \u22124 and weight of 1000 , which is applied alongside the loss term used in the first phase . The specified hyperparameters worked better for us than the values provided in the original paper . Our Zero - Out Regularizer Our regularization method , which is a binary cross entropy loss between the model predictions and a zero vector , does not use additional cues or sensitivities and yet achieves near state - of - the - art performance on VQA - CPv2 . We set the learning rate to : 2\u00d710 \u22126 r , where r is the ratio of the training instances used for fine - tuning . The weight for the loss is set to 2 . We report the performance obtained at the 8 th epoch .", "entities": [[71, 73, "HyperparameterName", "learning rate"], [79, 80, "MetricName", "loss"], [140, 142, "HyperparameterName", "learning rate"], [155, 156, "MetricName", "loss"], [194, 195, "MetricName", "loss"], [224, 225, "TaskName", "VQA"], [231, 233, "HyperparameterName", "learning rate"], [258, 259, "MetricName", "loss"]]}
{"text": "We test our regularization method on random subsets of varying sizes . Fig . A6 shows the results when we apply our loss to 1 \u2212 100 % of the training instances . Clearly , the ability to regularize the model does not vary much with respect to the size of the train subset , with the best performance occurring when our loss is applied to 1 % of the training instances . These results support our claims that it is possible to improve performance without actually performing visual grounding . Q : Is this food sweet ? A : yes Remarks : The most sensitive regions for irrelevant / random variants do not contain food , yet their answers are correct .", "entities": [[22, 23, "MetricName", "loss"], [62, 63, "MetricName", "loss"], [88, 90, "TaskName", "visual grounding"]]}
{"text": "HINT trained on relevant cues HINT trained on irrelevant cues HINT trained on random cues Q : Has the boy worn out his jeans ? A : yes Remarks : All of the variants look at both relevant and irrelevant regions to produce correct answer . Q : Is the sport being played tennis or volleyball ? A : tennis Remarks : None of the variants look at relevant regions , and yet produce correct answer . Q : What is the swimmer doing ? A : surfing Remarks : Models trained on irrelevant / random cues do not look at the swimmer at all , yet produce correct answer . We pick samples where all variants produce correct response to the question . The first column shows ground truth regions and columns 2 - 4 show visualizations from HINT trained on relevant , irrelevant and fixed random regions respectively . Figure A5 : % CPIG for baseline and different variants of SCR and our method , computed using ground truth relevant regions taken from textual explanations ( txt ) . Train Test Figure A6 : The regularization effect of our loss is invariant with respect to the dataset size .", "entities": [[191, 192, "MetricName", "loss"]]}
{"text": "Following the first treebanking efforts , in English ( Marcus et al , 1993 ) , Chinese ( Xue et al , 2005 ) , and Arabic ( Maamouri and Bies , 2004 ) , and with the surge of interest in developing statistical , broad - coverage , parsing models , Sima'an et al ( 2001 ) introduced a pilot treebanking study and a Hebrew treebank ( HTB ) , which included 500 sentences from the Hebrew newspaper ha'aretz , morphologically segmented and morpho - syntactically annotated with part - of - speech tags , morphological features , and labeled phrase - structure trees . Following the annotation practices at the time , much of the tagging and labeling scheme was adopted almost as is from the UPenn Treebank ( Marcus et al , 1993 ) . However , due to its rich morphology and Semitic phenomena , several annotation decisions in the HTB diverged from these practices . Firstly , the basic units that appear as leaves of the trees are not space - delimited tokens , but segmented units that we call morphemes . 1 Various prefixes that mark independent function words , including 2 B ( in ) , L ( to ) , M ( from ) , F ( that ) , KF ( when ) and H ( definite article ) are segmented away from their host . In addition , pronominal suffixes that appear on top of function words are also segmented away . So , the tokens FLW ( of him ) , LK ( to you ) , and AITM ( with them ) , are segmented into FL ( of ) + HWA ( he ) , L ( to ) + ATH ( you ) , EM ( with ) + HM ( them ) respectively . 3 The POS tags labeling scheme in the HTB includes quite a few changes from PTB , including the addition of special tags lexicalizing important functional elements in Hebrew : AT ( for the accusative marker ) , H ( the definite article ) , POSS ( the possesive marker ) , and HAM ( the yes / no question marker ) . In addition , the HTB introduces the NNT , JJT , CDT labels , marking the constructstate variants of NN , JJ , CD in the PTB , and a specific tag MOD that tags modifier words which is neither an adjective nor an adverb . On top of that , all open class POS tags as well as auxiliaries have been marked for their inflectional features ( gender , number , person , time ) , yielding in total hundreds of possible fine - grained POS categories . The syntactic labels in the phrase structure trees of the HTB were adopted from the Penn Treebank ( PTB ) almost as is , with the addition of a PREDP label for marking verbless predicates . The syntactic trees themselves looked superficially like the PTB but they differ in several aspects . Due to word - order freedom at the clause level , S - level categories present a flat structure , where the positions of the arguments do not entail anything about their grammatical function . The HTB provided 3 types of manually verified function tags to indicate such functions : SUBJect , OBJect , and COMplement , the latter marking obligatory arguments of the verb . Finally the HTB defined three types of null elements : * T * marking phonologically empty anaphors , * PRO * for pro - drop subjects , and * NONE * for elliptical elements . The work of Guthmann et al ( 2008 ) extended the HTB to 6501 sentences , in a manuallyvalidated automatic process . 4 During this process they further added a systematic marking of mother - daughter dependencies . That is - due to feature - spreading in Hebrew , morphological features of phrases may be contributed by different daughters , and not necessarily via a single head . So they marked each daughter with the role it plays in determining its mothers ' features ( gender , number , tense , etc . ) . Using these feature - based dependencies , they performed feature - percolation from daughter to mother , so that phrasal nodes are also marked with their morphological signatures . 5 Still , the phrase - structure trees yielded by HTBtrained parsers were not useful for downstream applications in Hebrew . This is because Hebrew is a relatively - free word order language , where the position of a constituent does not entail its grammatical function or semantic role . This in particular precludes the use of well known ' head tables ' for selecting a single head and deriving labeled and unlabeled dependencies . To overcome this , Tsarfaty ( 2010 ) devised a set of rules based on the daughter - dependencies , function tags and empty elements , to automatically derive the relationalrealizational ( RR ) version of the HTB . In the RR HTB , each node is marked with its relational network ( an unordered set of grammatical functions ) mapped to the ordered syntactic constituents . The RR HTB retained the morphological conventions and core non - core distinction of the original HTB . In a parallel effort , and with the surge of interest in dependency parsing ( Buchholz and Marsi , 2006 ; Nivre et al , 2007 ) , 6 Goldberg ( 2011 ) automatically converted the HTB into its first , unlabeled , dependency version . The automatic conversion procedure assumed that heads are functional rather than lexical . As a result , the coordination marker would head coordination structures , the accusative marker would head direct object phrases , and so on . On top of that , in order to remain compatible with the wide - coverage lexicon of Itai and Wintner ( 2008 ) , this version of the HTB adopted the POS tags scheme of Adler ( 2007 ) , rather than the POS tags of Sima'an et al ( 2001 ) Based on this version , Goldberg and Elhadad ( 2009 ) presented the first Hebrew dependency parsing results , only unlabeled attachment scores ( UAS ) at this point . Here too , as with the phrasestructure trees , it was impossible to devise an external procedure that would infer dependency labels for the unlabeled arcs - and there were no labeled dependencies to train such a labeler on . At that point , where the need for Hebrew labeled dependencies had become pressing , Tsarfaty ( 2013 ) presented the Unified - Stanford Dependencies ( Unified - SD ) version of the HTB , extending the Stanford dependencies ( SD ) scheme to cover both morphological and syntactic phenomena . Similar to SD , U - SD assumed a labeling hierarchy , with several changes : the hierarchy now included branches for head - types ( hd ) , dependency types ( dep ) , and functional types ( func ) . In particular , dependencies in the func branch mark syntactic functions that are in fact interchangeable with morphology , when considering these functions from a typological perspective . Tsarfaty used the U - SD labels to edit three versions of the HTB : ( i ) to mark the original phrasestructure trees in the HTB with the labels as dashfeatures , ( ii ) to relabel the relational networks in RR trees with U - SD labels , and ( iii ) to derive a labeled dependencies version of the HTB . As with the unlabeled dependencies of Goldberg , the U - SD HTB assumed functional heads across the board , and the POS tags layer was again changed to comply with the wide - coverage lexicon ( HEBLEX ) of Itai and Wintner ( 2008 ) . The labeled dependencies treebank of U - SD then provided the Hebrew section of the SPMRL shared tasks ( Seddah et al , 2013 ( Seddah et al , , 2014 . 3 The Hebrew UD Treebank", "entities": [[89, 92, "DatasetName", "part - of"], [298, 299, "MetricName", "EM"], [324, 325, "DatasetName", "PTB"], [362, 363, "DatasetName", "HAM"], [398, 399, "DatasetName", "PTB"], [404, 405, "DatasetName", "MOD"], [476, 478, "DatasetName", "Penn Treebank"], [479, 480, "DatasetName", "PTB"], [505, 506, "DatasetName", "PTB"], [845, 846, "DatasetName", "RR"], [854, 855, "DatasetName", "RR"], [881, 882, "DatasetName", "RR"], [910, 912, "TaskName", "dependency parsing"], [1049, 1051, "TaskName", "dependency parsing"], [1267, 1268, "DatasetName", "RR"], [1371, 1372, "DatasetName", "UD"]]}
{"text": "The present version of UDv2 thus results from a sequence of automatic and semi - automatic conversions on the trees of Guthmann et al ( 2008 ) . In order to validate the current UDv2 trees , we reviewed the list of UD POS tags , relation labels and features , and for each of these items we identified the dependency structures in the HTB dev set that contain them . At this point , for each item , a linguist characterized the role such item actually fulfills in the Hebrew grammatical structures , ( as opposed to the role it was designed to fulfill in the UD scheme ) . During this process the linguist documented errors and inconsistencies that were found , either between the realistic use of a function in the UDv2 HTB and the UDv2 guidelines , or simply attesting insufficient or incorrect coverage of the linguistic structure that this particular label , tag or feature is supposed to describe . This validation process 8 was conducted on the entire HTB UDv2 dev set 9 and it was followed by a sequence of discussions in which our research team , consisting of two linguists , two NLP specialists , and a senior NLP researcher , discussed possible solutions for each error . The discussions were focused on explicitly assessing the merits of each solution alternative according to the six criteria of the Mannings Law . That is : linguistically adequate , typologically adequate , suitable for rapid , consistent annotation , suitable for parsing with high accuracy , easily comprehended by non - linguists , and provides good support for downstream NLP tasks . 10 After narrowing down the list of adequate solutions , the final decision about which revisions to make leaned on their importance and feasibility . For example , a very important , yet easily executable revision was to simply replace all instances of prepositional iobj with obl . Just as important , but far more complex , was to switch between a head and a dependent in the case of structures containing auxiliaries ( e.g. , modals , as we illustrate shortly ) . All revisions were made with the python Pandas package , and they were applied to all , dev , train and test , sets . Revisions were made with respect to linguistic patterns that refer to existing labels , tags or features , with no consideration of any particular ( lexicalized ) Hebrew words . Furthermore , we refrained from manual changes of specific errors , considering that their source might be a vaster problem , to be dealt with in the future . As an example for simple edits , consider adding a label compound : affix . For this , all rows containing the feature ' Prefix = Yes ' had to be retrieved , and the label was changed to compound : affix . As a more complex case , consider the case involving modality mentioned above . Here , all rows with the xcomp label were retrieved . For each row , if the head had a morphological feature ' Verb - Type = Mod ' , the head 's label was relabeled ' aux ' , the row itself was relabeled with the original label of the head , and the numbers were changed respectively in the ' HEAD ' column ( see Table 1 ) .", "entities": [[42, 43, "DatasetName", "UD"], [107, 108, "DatasetName", "UD"], [260, 261, "MetricName", "accuracy"]]}
{"text": "Eliminating acl : inf to acl . The automatic conversion to UD has kept fine - grained labels as subrelations , resulting with the language - specific label acl : inf . Since the UD guidelines permit infinitive structures in acl , it is unnecessary to mark infinity as a sub - relation . Moreover , all cases of acl : inf bear the feature ' VerbForm = Inf ' . So eliminating the morphological feature inf from the subrelation acl : inf does not lead to any information loss . Adding compound : affix This new relation is dedicated to non - standalone words , which function semantically like affixes , but syntactically surface as separate words , at times separated by a hyphen and in others by white - space . A subset of these words are loan words ( mainly from English , like ' non ' , ' multi ' etc . ) where originally they surface syntactically as affixes . In UDv2 these items were marked by the feature Prefix = Yes . However , since they mark a certain type of Hebrew compounds , we used sub - typing to indicate it . 11 In \" KLL - EWLMIT \" for example , KLL ' uni - ' is semantically a prefix to EWLMIT ' worldly ' , but in Hebrew the two are separate words .", "entities": [[11, 12, "DatasetName", "UD"], [34, 35, "DatasetName", "UD"], [89, 90, "MetricName", "loss"]]}
{"text": "The Words - as - Classifiers ( WAC ) approach to grounded semantics is quite simple : train a binary classifier for each word in a corpus where the features to that classifier are derived from images ( Kennington and Schlangen , 2015 ) . Each classifier is given positive and negative examples of visual denotations of each word by the images and learns a \" fitness \" score by the classifier . For example , the red classifier is given images of objects that are referred to as red in a corpus , and randomly assigned negative examples of things that are not referred to as red , as depicted in Figure 1 . We follow Kiros et al ( 2018 ) and use Google Image Search to find images using the BERT vocabulary , resulting in 27 , 152 words and corresponding images ( some words did not result in images , and we did not download images for filler words ) . For each word , we perform an image search and download the top 100 images . We then follow Schlangen et al ( 2016 ) and process each image by passing them through the recent CLIP ( Jia et al , 2021 ) convolutional neural network ( trained on ImageNet , using CLIP 's ViT - B/32 model ) , yielding a vector of size 512 for each image . We use the 100 images as positive examples for each term in our vocabulary and randomly select three negative examples for each positive example . We then use a logistic regression classifier ( C=0.25 , max iterations=1000 ) , one for each word , trained on the images for each word . After training , we then follow Moro et al ( 2019 ) and extract the coefficients to arrive at a vector of size 513 ( all coefficients plus the bias term ) which we use in our evaluations below . We call these the WAC vectors . The WAC model is useful because , as explained in Kennington and Schlangen ( 2015 ) , the classifiers can actually identify objects ( something that language models can not do on their own ) , the coefficients represent a computed word intension , new words in a vocabulary can easily be added without retraining all other classifiers including adjectives like red which are often missing from pre - trained object classifiers , and the classifiers are effectively learned with only a few examples , making it effective for fast learning of concrete , grounded concepts . However , the WAC model suffers from two assumptions : first , that all words have concrete , visual denotations even though many abstract words like utopia clearly do not , and that all words are independent of each other in terms of linguistic context . We hypothesize in both experiments below that these coefficients used as vectorized embeddings will be useful to a text - only language model because they add necessary visual information ; the language model complements WAC by using linguistic context ( i.e. , text ) for training , overcoming WAC 's assumptions . 4 Experiment 1 : Tying embedding weights and pre - training ELECTRA , fine - tuning on GLUE In this experiment , and crucially for our ongoing work that aligns with child - inspired language acquisition , we use ELECTRA ( Clark et al , 2020 ) as a language model because it has been shown to be trainable with smaller amounts of data than other language models , yet yield respectable results and can be trained using a single GPU . Task & Procedure Wang et al ( 2018 ) introduced the GLUE benchmark which consists of nine English sentence understanding tasks covering several domains ( e.g. , movie reviews and online question answering ) . We opt for this benchmark because of its coverage over several domains and to show that adding multimodal knowledge improves tasks that are based on text . 2 Our aim is to achieve improved results over the text - only baseline with a specified number of training steps using the openwebtext data for training . 3 We report results on the development set , as done in Wu et al ( 2021 ) . We only report the results for the MRPC ( a paraphrase task that uses accuracy and f1 metrics ) , COLA ( a grammatical acceptibility task ; uses Matthew 's Correlation ) , and WNLI ( ambiguity resolution ; uses an accuracy metric ) tasks because they are sufficient to illustrate the utility of our method when applied to ELECTRA . To give ELECTRA knowledge about additional modalities from the Lancaster and WAC vectors , we tie the vectors to the the weights of the generator and discriminator of ELECTRA depicted in Figure 2 , and vary whether the embeddings are frozen or not during pre - training , then train for 100 , 000 steps . 4 We then fine - tune the resulting ELECTRA model on the GLUE tasks using the multimodal vectors following standard fine - tuning protocols ; that is , we add a linear layer with a softmax to the pre - trained model and use the ADAM solver with a learning rate of 2e - 5 for 3 epochs . As the WAC vectors were larger than ELECTRA 's expected embedding size of 128 , we applied UMAP to reduce the dimensionality to 128 ; similarly for the WAC and Lancaster concatenated embeddings . For cases where there was no vector for a word ( e.g. , the [ unmapped ] words or words outside of the vocabulary of the Lancaster vectors ) , we simply used zero vectors . For Lancaster vectors , we set the ELECTRA embedding size to 39 . We explored freezing the embeddings ; our hypothesis is that not freezing the embeddings will lead to better results because the training regime can overpower the embeddings , but retain the multimodal knowledge . For a broader comparison , we also compared to GloVE ( Pennington et al , 2014 ) and several ablations where we concatenate multimodal vectors with the GloVe vectors ( we used the evaluation script for GloVE provided by Wang et al ( 2018 ) ) . We also use the same training and evaluation regime for the WAC and Lancaster vectors , and a concatenation of the two , on their own treating them as word - level embeddings similar to GloVe . Results Table 1 shows the results on the GLUE benchmark . The word - level embeddings of GloVe , WAC , and Lancaster are shown in the top 5 rows of the table . As expected , these word - level embeddings are not state - of - the - art , but we notice that both Lancaster and WAC vectors perform comparably against the GloVE vectors despite only being trained on images ( WAC ) or derived from the Lancaster norms . Of note is a significant advantage of using the Lancaster vectors alone compared to using any other embedding or combination for the WNLI task which is co - reference and natural language inference for fiction books . This suggests that inference on fiction is helped by knowing which modalities affect each word . Interestingly , the best performing model for COLA was GloVE and Lancaster word - level embeddings ; COLA is a grammaticality test , which is important in language understanding , but arguably less critical in early - stage child language acquisition . All other rows show the ELECTRA baseline and ELECTRA that uses some variation of WAC , Lancaster , or both as embeddings ( denoted with the EL - prefix ) . The bottom part of the table compares ELECTRA with a variant of ELECTRA that uses WAC embeddings ( both with and without freezing the embedding weights ) , ELECTRA with lancaster embeddings and ELECTRA with WAC embeddings concatenated with the Lancaster embeddings ( where the length of the WAC embeddings plus the size of ELECTRA is 128 ) . Contrary to our hypothesis , we observe that when ELECTRA uses WAC with frozen weights , the performance on the benchmark performs better than all others , including the ELECTRA baseline . This could suggest that ELECTRA can make effective use of the visual and Lancaster embeddings by adjusting weights in the other layers of the model . The EL - lan - wac variant performed well above the ELECTRA baseline , substantiating the hypothesis that enriching the model with multimodal knowledge can improve results . Taken together , we find the results encouraging because the relatively short training regime still yielded respectable results , suggesting that ELECTRA with a visual or other multimodal embedding can be useful with less training as is the case when children learn language .", "entities": [[125, 126, "DatasetName", "Google"], [133, 134, "MethodName", "BERT"], [200, 201, "MethodName", "CLIP"], [214, 215, "DatasetName", "ImageNet"], [217, 218, "MethodName", "CLIP"], [264, 266, "MethodName", "logistic regression"], [540, 541, "MethodName", "ELECTRA"], [546, 547, "DatasetName", "GLUE"], [563, 565, "TaskName", "language acquisition"], [568, 569, "MethodName", "ELECTRA"], [622, 623, "DatasetName", "GLUE"], [642, 644, "TaskName", "question answering"], [696, 697, "DatasetName", "openwebtext"], [727, 728, "DatasetName", "MRPC"], [734, 735, "MetricName", "accuracy"], [740, 741, "MethodName", "COLA"], [754, 755, "DatasetName", "WNLI"], [761, 762, "MetricName", "accuracy"], [779, 780, "MethodName", "ELECTRA"], [783, 784, "MethodName", "ELECTRA"], [809, 810, "MethodName", "ELECTRA"], [845, 846, "MethodName", "ELECTRA"], [849, 850, "DatasetName", "GLUE"], [868, 870, "MethodName", "linear layer"], [872, 873, "MethodName", "softmax"], [882, 883, "DatasetName", "ADAM"], [886, 888, "HyperparameterName", "learning rate"], [903, 904, "MethodName", "ELECTRA"], [973, 974, "MethodName", "ELECTRA"], [1040, 1041, "MethodName", "GloVe"], [1095, 1096, "MethodName", "GloVe"], [1105, 1106, "DatasetName", "GLUE"], [1114, 1115, "MethodName", "GloVe"], [1202, 1203, "DatasetName", "WNLI"], [1210, 1213, "TaskName", "natural language inference"], [1240, 1241, "MethodName", "COLA"], [1250, 1251, "MethodName", "COLA"], [1272, 1274, "TaskName", "language acquisition"], [1280, 1281, "MethodName", "ELECTRA"], [1283, 1284, "MethodName", "ELECTRA"], [1313, 1314, "MethodName", "ELECTRA"], [1318, 1319, "MethodName", "ELECTRA"], [1334, 1335, "MethodName", "ELECTRA"], [1339, 1340, "MethodName", "ELECTRA"], [1360, 1361, "MethodName", "ELECTRA"], [1374, 1375, "MethodName", "ELECTRA"], [1384, 1386, "DatasetName", "the benchmark"], [1394, 1395, "MethodName", "ELECTRA"], [1401, 1402, "MethodName", "ELECTRA"], [1434, 1435, "MethodName", "ELECTRA"], [1472, 1473, "MethodName", "ELECTRA"]]}
{"text": "The evaluation in Experiment 1 was made up of text - based tasks . In this experiment , we use an evaluation that requires knowledge of the visual world by evaluating the Lancaster and WAC vectors on the Visual Dialog task , termed visdial . Moreover , Experiment 1 used pre - training on a subset of the data for only 100 , 000 steps . In this experiment , we evaluate using a fully pre - trained RoBERTa model by replacing its embeddings with the WAC and Lancaster vectors . Task Following Murahari et al ( 2019 ) , given an image , dialogue history consisting of questionanswer pairs , and a follow - up question about the image , the task of visdial is to predict a free - form natural language answer to the question . The visdial dataset introduced in Das et al ( 2019 ) also includes evaluation metrics and human - annotated answers to the natural language queries about the image . Five human annotators identified which responses out of 100 candidates could be considered correct . This allows multiple answers to be correct ( e.g. , yes and yeah are semantically identical ) . Metrics We report the following metrics : R@1 Rate of times the top - ranked answer is a correct one ; i.e. , accuracy . R@5 Rate of times correct answers are in the top - five ranked answers . MRR Mean Reciprocal Rank is the multiplicative inverse of the rank of the first correct answer . NDCG Normalized Discount Accumulative Gain is a measure of ranking quality that takes the top K ranked options , where K is the number of answers marked as correct by a least one annotator ; in this measure , the fraction of annotators that marked a particular answer as correct is taken into account . Baseline and Procedure We report the values for the model described in Murahari et al ( 2019 ) for our baseline - work which builds on VilBERT ( Lu et al , 2019 ) , a parallel model of vision and language used for the visual dialogue task - and leverage their model with our custom , multimodal embeddings . Their model uses two transformers , one for the language modality and one for the visual modality . As explained in Lu et al ( 2019 ) , the interaction between the two transformers is mediated by two co - attention layers where attention in one modality is conditioned on inputs from the other modality . Murahari et al ( 2019 ) adapted the VilBERT model for the visdial task by using a pre - trained language model trained on English Wikipedia and the BooksCorpus ( Zhu et al , 2015 ) using masked language modeling and next sentence prediction losses . They then frame the task as a next - sentence prediction task ( whereas the original VilBERT was modeled to generate descriptions of images ) . They then use the Conceptual Captions ( Sharma et al , 2018 ) and Visual Question Answering ( VQA ) ( Antol et al , 2015 ) datasets ( using masked image region , masked language modeling , and next sentence prediction losses ) to train the two transformers . They then fine - tune on the visdial task ( also using masked image region , masked language modeling , and next sentence prediction losses ) . The underlying architecture uses a pre - trained BERT language model ( i.e. , bert - base - uncased ) as a starting point before training on the Wikipedia , BooksCorpus , Conceptual Captions , and VQA datasets . This constitutes our baseline . We do n't consider the dense representations from Murahari et al ( 2019 ) due to hardware limitations . We altered their architecture by replacing the RoBERTa pre - trained embedding layer with the Lancaster and WAC vectors , as depicted in Figure 3 . We then fine - tuned on the visdial task using their training regime . 5 We explain how we made vectors compatible with their architecture . Vocabulary : RoBERTa & AoA Abstract words do not have concrete , visual denotations , such as utopia or justice , so it does not make theoretical sense to include a WAC embedding for words that are clearly abstract because whatever set of images represents those concepts may not have useful semantic information . Moreover , children learning their first language learn concrete concepts before they learn more abstract concepts ( Borghi et al , 2019 ; Ponari et al , 2018 ) . To explore if RoBERTa could make use of a WAC embedding that uses words that are more aimed at a child vocabulary , we report results of filtering out words not in the the Age - of - Acquisition ( AoA ) list ( Kuperman et al , 2012b ) . AoA a list of 30 , 000 English words rated for the average age when children first speak those words ( avg 11 years ; std 3.0 , most common words are for ages 2 - 14 ) . This resulted in 9 , 627 remaining words in the vocabulary ; all other words were set to an embedding of zeros . Lancaster vectors Similar to AoA , the Lancaster Norms has a predefined vocabulary , which , when compared to the RoBERTa vocabulary results in 11 , 402 words in both . For each word in the RoBERTa vocabulary that was also in the Lancaster norms , we replaced the RoBERTa embedding with the Lancaster vector for that word ; otherwise words retained the original RoBERTa embedding . As their model expects vectors of size 768 ( the embedding size for RoBERTa ) , but the Lancaster vectors are only size 39 , we padded the rest of the vector with zeros . WAC vectors We use the vocabulary from the RoBERTa tokenizer as with the Lancaster Vectors , which results in a a 27 , 152 - word overlap with the WAC vectors . As the WAC vectors have a dimensionality of 513 , smaller than the required size of RoBERTa 's 768 , we padded zeros after each vector . All vectors that did not exist in the WAC set were zero vectors of size 768 . We followed a training regime for two settings : no - freeze The embedding layer was not frozen so as to allow weight changes during training . 2 - freeze The embedding layer was frozen for two epochs , then the weights were unfrozen for the rest of training ; prior work has shown that freezing layers after a certain number of epochs can improve results ( Liu et al , 2021 ) ; we opt for two because it still allows the finetuning to overpower the exiting embeddings if needed and preliminary results showed that freezing the weights for all epochs resulted in poor model performance . We trained each model for 20 epochs in total , which is the default training setting for this task . We used the settings that were used to train the baseline model ( i.e. , learning rate of 2e - 5 ) . We report the results of the baseline model and the variants of our above changes . 6 Compared to Experiment 1 with the GLUE benchmark , the approach taken in this section fundamentally changes how the Lancaster and WAC embeddings are applied to RoBERTa ; here the Lancaster and WAC embeddings are used on a pre - trained model . We hy - 6 Note that the baseline we are comparing to here is lower than what is reported on the leaderboard https://eval.ai/web/challenges/ challenge - page/518 / leaderboard . This is partially due to the fact that our training regime was altered due to hardware limitations ( i.e. , we could only use a batch size of 8 on a single 12 GB GPU ) . no freeze MRR R@1 R@5 NDCG pothesize that RoBERTa will improve with WAC embeddings , as well as the Lancaster concatenated to WAC ( denoted lanwac ) , though the Lancaster embedding on its own may be too small to make a difference . As words that are learned earlier in a child 's life are generally more concrete , we hypothesize that RoBERTa will improve when WAC only uses words from the AoA data as more abstract terms are represented by zero vectors . Results Table 2 shows the results for the visdial task . Though it is clear that RoBERTa is doing the heavy lifting , when added to RoBERTa , the Lancaster and WAC vectors show improvements over the RoBERTa baseline for some metrics . As noted in Murahari et al ( 2019 ) , the NDCG metric is actually counter to MRR , but is important because it takes multiple dialogue response annotations into account . For cases where the Lancaster and WAC models yield better performance , these results suggest that a pre - trained language model can make use of adding multimodal knowledge in the form of vectors derived from multimodal knowledge ( Lancaster ) and visual ( WAC ) for the visdial task . RoBERTA that uses the WAC embedding especially shows respectable results in the visdial task , particularly when the embedding uses the AoA vocabulary ( we only considered AoA for WAC because WAC peformed better than lanwac in this experiment ) . The WAC vectors were trained on very noisy data , yet despite the noise and the parsimonious model , there is some information about the visual world that enriches the baseline model . The variant trained with frozen weights for 2 epochs , then unfrozen for the remaining 18 epochs had respectable performance across all metrics . 7 7 As a sanity check , we also evaluated using randomly generated embeddings which performed only slightly worse", "entities": [[38, 40, "TaskName", "Visual Dialog"], [43, 44, "DatasetName", "visdial"], [78, 79, "MethodName", "RoBERTa"], [124, 125, "DatasetName", "visdial"], [140, 141, "DatasetName", "visdial"], [208, 209, "MetricName", "R@1"], [224, 225, "MetricName", "accuracy"], [226, 227, "MetricName", "R@5"], [241, 242, "MetricName", "MRR"], [339, 340, "MethodName", "VilBERT"], [413, 415, "HyperparameterName", "attention layers"], [437, 438, "MethodName", "VilBERT"], [441, 442, "DatasetName", "visdial"], [466, 469, "TaskName", "masked language modeling"], [491, 492, "MethodName", "VilBERT"], [505, 507, "DatasetName", "Conceptual Captions"], [515, 518, "DatasetName", "Visual Question Answering"], [519, 520, "TaskName", "VQA"], [535, 538, "TaskName", "masked language modeling"], [558, 559, "DatasetName", "visdial"], [567, 570, "TaskName", "masked language modeling"], [586, 587, "MethodName", "BERT"], [610, 612, "DatasetName", "Conceptual Captions"], [614, 615, "TaskName", "VQA"], [648, 649, "MethodName", "RoBERTa"], [674, 675, "DatasetName", "visdial"], [695, 696, "MethodName", "RoBERTa"], [780, 781, "MethodName", "RoBERTa"], [910, 911, "MethodName", "RoBERTa"], [926, 927, "MethodName", "RoBERTa"], [939, 940, "MethodName", "RoBERTa"], [954, 955, "MethodName", "RoBERTa"], [970, 971, "MethodName", "RoBERTa"], [1000, 1001, "MethodName", "RoBERTa"], [1040, 1041, "MethodName", "RoBERTa"], [1128, 1131, "HyperparameterName", "number of epochs"], [1211, 1213, "HyperparameterName", "learning rate"], [1242, 1243, "DatasetName", "GLUE"], [1262, 1263, "MethodName", "RoBERTa"], [1333, 1335, "HyperparameterName", "batch size"], [1347, 1348, "MetricName", "MRR"], [1348, 1349, "MetricName", "R@1"], [1349, 1350, "MetricName", "R@5"], [1353, 1354, "MethodName", "RoBERTa"], [1408, 1409, "MethodName", "RoBERTa"], [1438, 1439, "DatasetName", "visdial"], [1446, 1447, "MethodName", "RoBERTa"], [1456, 1457, "MethodName", "RoBERTa"], [1467, 1468, "MethodName", "RoBERTa"], [1490, 1491, "MetricName", "MRR"], [1553, 1554, "DatasetName", "visdial"], [1568, 1569, "DatasetName", "visdial"]]}
{"text": "In order to evaluate the adaptability hypothesis where the prediction of upcoming verb is driven by local nominal arguments , we implement an ngram language model using the data discussed in Section 3.2 . Such models are typically used to compute the surprisal metric ( Hale , 2001 ; Levy , 2008 ) given local context ( e.g. , Levy et al , 2012 ) . Recall that we have at most 3 NPs as the preverbal context , and therefore , we use a 4 - gram model so that the model has access to the complete context in a given condition to make a verbal prediction . Unlike the models discussed in Section 5 , the preverbal context in this model is free of noise .", "entities": [[66, 67, "MetricName", "Recall"]]}
{"text": "| \u03b5 D contains all balanced strings of parentheses and square brackets . Since D is often viewed as a canonical example of a context - free language ( Chomsky and Sch\u00fctzenberger , 1959 ) , several recent studies , including Sennhauser and Berwick ( 2018 ) , Bernardy ( 2018 ) , Skachkova et al ( 2018 ) , and Yu et al ( 2019 ) , have used D to evaluate whether LSTMs can learn hierarchical dependencies implemented by pushdown automata . Here , we consider the bracket prediction task proposed by Sennhauser and Berwick ( 2018 ) . Task 5 ( Bracket Prediction Task ) . Given a prefix p of some string in D , identify the next valid closing bracket for p. In heatmaps for the bracket prediction task , we expect the last unclosed bracket to receive the highest - magnitude relevance score .", "entities": [[1, 2, "HyperparameterName", "\u03b5"]]}
{"text": "We consider two types of automata - based networks : one that implements a finite - state automaton ( FSA ) for the SP task , and one that implements a pushdown automaton ( PDA ) for the bracket prediction task . Our FSA construction is similar to Korsky and Berwick 's ( 2019 ) FSA construction for simple recurrent networks . Consider a deterministic FSA A with states Q and alphabet \u03a3. To simulate A using an LSTM , we use | Q | | \u03a3 | hidden units , with the following interpretation . Suppose that A transitions to state q after reading input x ( 1 ) , x ( 2 ) , . . . , x ( t ) . The hidden state h ( t ) is a onehot representation of the pair \u27e8 q , x ( t ) \u27e9 , which encodes both the current state of A and the most recent input symbol . Since the FSA undergoes a state transition with each input symbol , the forget gate always clears c ( t ) , so that information written to the cell state does not persist beyond a single time step . The output layer simply detects whether or not the FSA is in an accepting state . Details are provided in Appendix A.3 . Next , we describe how to implement a PDA for the bracket prediction task . We use a stack containing all unclosed brackets observed in the input string , and make predictions based on the top item of the stack . We represent a bounded stack of size k using 2k + 1 hidden units . The first k \u2212 1 positions contain all stack items except the top item , with ( represented by the value 1 , [ represented by \u22121 , and empty positions represented by 0 . The kth position contains the top item of the stack . The next k positions contain the height of the stack in unary notation , and the last position contains a bit indicating whether or not the stack is empty . For example , after reading the input ( [ ( ( ) with a stack of size 4 , the stack contents ( [ ( are represented by c ( 5 ) = [ 1 \u22121 0 1 1 1 1 0 0 ] \u22a4 . The 1 in position 4 indicates that the top item of the stack is ( , and the 1 , \u22121 , and 0 in positions 1 - 3 indicate that the remainder of the stack is ( [ . The three 1s in positions 5 - 8 indicate that the stack height is 3 , and the 0 in position 9 indicates that the stack is not empty . When : k\u22121 , pushing the opening bracket to the stack . The empty stack bit is then set to 0 , marking the stack as non - empty . When the current input symbol is a closing bracket , the highest item of positions 1 through k \u2212 1 is deleted and copied to position k , popping the top item from the stack . Because the PDA network is quite complex , we focus here on describing how the top stack item in position k is determined , and leave other details for Appendix A.4 . Let \u03b1 ( t ) be 1 if x ( t ) is ( or [ , it is copied to c ( t ) Name Formula Saliency R ( c ) t , i ( X ) = \u2202\u0177c \u2202x ( t ) i x ( t ) i = X t , i G \u00d7 I R ( c ) t , i ( X ) = Xt , i \u2202\u0177c \u2202x ( t ) i x ( t ) i = X t , i IG R ( c ) t , i ( X ) = Xt , i \u222b 1 0 \u2202\u0177c \u2202x ( t ) i x ( t ) i = \u03b1X t , i d\u03b1 x ( t ) = ( , \u22121 if x ( t ) = [ , and 0 otherwise . At each time step , g ( t ) k = tanh ( m u ( t ) ) , where m \u226b 0 and u ( t ) = 2 k \u03b1 ( t ) + k\u22121 \u2211 j=1 2 j\u22121 h ( t\u22121 ) j . ( 1 ) Observe that m u ( t ) \u226b 0 when \u03b1 ( t ) = 1 , and m u ( t ) \u226a 0 when \u03b1 ( t ) = \u22121 . Thus , g ( t ) k contains the stack encoding of the current input symbol if it is an opening bracket . If the current input symbol is a closing bracket , then \u03b1 ( t ) = 0 , so the sign of u ( t ) is determined by the highest item of h ( t\u22121 ) : k\u22121 .", "entities": [[78, 79, "MethodName", "LSTM"], [314, 315, "DatasetName", "0"], [317, 318, "DatasetName", "kth"], [393, 394, "DatasetName", "0"], [398, 399, "DatasetName", "0"], [399, 400, "DatasetName", "0"], [426, 427, "DatasetName", "0"], [461, 462, "DatasetName", "0"], [493, 494, "DatasetName", "0"], [572, 573, "HyperparameterName", "\u03b1"], [676, 677, "DatasetName", "0"], [711, 712, "DatasetName", "0"], [723, 725, "HyperparameterName", "k ="], [737, 738, "DatasetName", "0"], [746, 747, "HyperparameterName", "\u03b1"], [773, 774, "DatasetName", "0"], [775, 776, "HyperparameterName", "\u03b1"], [789, 790, "DatasetName", "0"], [791, 792, "HyperparameterName", "\u03b1"], [832, 833, "HyperparameterName", "\u03b1"], [837, 838, "DatasetName", "0"]]}
{"text": "Decomposition - based methods are methods that satisfy the relation y c = R ( c ) bias + \u2211 t , i R ( c ) t , i ( X ) , ( 2 ) where R ( c ) bias is a relevance score assigned to the bias units of the network . The interpretation of equation ( 2 ) is that the logit score\u0177 c is \" distributed \" among the input features and the bias units , so that the relevance scores form a \" decomposition \" of\u0177 c . The one decomposition - based method we consider is LRP , which computes scores using a backpropagation algorithm that distributes scores layer by layer . The scores of the output layer are initialized to r ( c , output ) i = { \u0177 i , i = c 0 , otherwise . For each layer l with activation z ( l ) , activation function f ( l ) , and output a ( l ) = f ( l ) ( z ( l ) ) , the relevance r ( c , l ) of a ( l ) is determined by the following propagation rule : r ( c , l ) i = \u2211 l \u2032 \u2211 j r ( c , l \u2032 ) j W ( l \u2032 l ) j , i a ( l ) i z ( l \u2032 ) j + sign ( z ( l \u2032 ) j ) \u03b5 , where l \u2032 ranges over all layers to which l has a forward connection via W ( l \u2032 l ) and \u03b5 > 0 is a stabilizing constant . 3 For the LSTM gate interactions , we follow Arras et al ( 2017b ) in treating multiplicative connections of the form a ( l 1 ) a ( l 2 ) as activation functions of the form a ( l 1 ) f ( l 2 ) ( ) , where a ( l 1 ) is f ( t ) , i ( t ) , or o ( t ) . The final attribution scores are given by the values propagated to the input layer : R ( c ) t , i ( X ) = r ( c , input t ) i .", "entities": [[144, 145, "DatasetName", "0"], [159, 161, "HyperparameterName", "activation function"], [256, 257, "HyperparameterName", "\u03b5"], [280, 281, "HyperparameterName", "\u03b5"], [282, 283, "DatasetName", "0"], [291, 292, "MethodName", "LSTM"]]}
{"text": "As mentioned in the previous section , network saturation causes gradients to be approximately 0 when using sigmoid or tanh activation functions . 1.000 \u22126.68 \u00d7 10 \u22125 100.0 69.9 10 1.000 \u22122.46 \u00d7 10 \u22125 100.0 92.3 11 1.000 \u22129.05 \u00d7 10 \u22126 100.0 98.7 12 1.000 \u22123.33 \u00d7 10 \u22126 100.0 99.8 by saturation , Table 4 shows heatmaps for the input accb generated by gradient - based methods for different instantiations of the counter - based SP network with varying degrees of saturation . Recall from Section 4 that counter values for this network are expressed in multiples of the scaling factor v. We control the saturation of the network via the parameter u = tanh \u22121 ( v ) . For all three gradient - based methods , scores for a decrease and scores for b increase as u increases . Additionally , saliency scores for the first c decrease when u increases . When u = 8 , v is almost completely saturated , causing G \u00d7 I to produce all - zero heatmaps . On the other hand , IG is still able to produce nonzero heatmaps even at u = 64 . Thus , IG is much more resistant to the effects of saturation than G \u00d7 I. According to Sundararajan et al ( 2017 ) , gradient - based methods satisfy the axiom of implementation invariance : they produce the same heatmaps for any two networks that compute the same function . This formal property is seemingly at odds with the diverse array of heatmaps appearing in Table 4 , which are produced for networks that all yield identical classifiers . In particular , the networks with u = 8 , 16 , and 64 yield qualitatively different heatmaps , despite the fact that the three networks are distinguished only by differences in v of less than 0.001 . Because the three functions are technically not equal , implementation invariance is not violated in theory ; but the fact that IG produces different heatmaps for three nearly identical networks shows that the intuition described by implementation invariance is not borne out in practice . Besides the gradient - based methods , LRP is also susceptible to problems arising from saturation . Recall from heatmaps # 3 and # 4 of Table 3 that for the counting task network , LRP assigns scores of 0 to prefixes with equal numbers of as and bs . We hypothesize that this phenomenon is related to the fact c ( t ) = 0 after reading such prefixes , since the counter has been incremented and decremented in equal amounts . Accordingly , we test whether this phenomenon can be mitigated by desaturating the gates so that c ( t ) does not exactly reach 0 . Recall that the white - box LSTM gates approximate 1 \u2248 \u03c3 ( m ) using a constant m \u226b 0 . We construct networks with varying values of m and compute LRP scores on a randomly generated testing set of 1000 strings , each of which contains at least one prefix with equal numbers of as and bs . In Table 5 we report the percentage of examples for which such prefixes receive LRP scores of 0 , along with the network 's accuracy on this testing set and the average value of c ( t ) when the counter reaches 0 . Indeed , the percentage of prefixes receiving scores of 0 increases as the approximation c ( t ) \u2248 0 becomes more exact .", "entities": [[14, 15, "DatasetName", "0"], [19, 21, "MethodName", "tanh activation"], [87, 88, "MetricName", "Recall"], [379, 380, "MetricName", "Recall"], [401, 402, "DatasetName", "0"], [427, 428, "DatasetName", "0"], [469, 470, "DatasetName", "0"], [471, 472, "MetricName", "Recall"], [477, 478, "MethodName", "LSTM"], [491, 492, "DatasetName", "0"], [548, 549, "DatasetName", "0"], [555, 556, "MetricName", "accuracy"], [573, 574, "DatasetName", "0"], [584, 585, "DatasetName", "0"], [594, 595, "DatasetName", "0"]]}
{"text": "As described in Subsection 4.1 , the network for the counting task simply sets g ( t ) to v = tanh ( u ) when x ( t ) = a and \u2212v when x ( t ) = b. All gates are fixed to 1 . The output layer uses h ( t ) = tanh ( c ( t ) ) as the score for the True class and v/2 as the score for the False class . g ( t ) = tanh ( u [ 1 \u22121 ] x ( t ) ) f ( t ) = \u03c3 ( m ) i ( t ) = \u03c3 ( m ) o ( t ) = \u03c3 ( m ) y ( t ) = [ 1 0 ] h ( t ) + [ 0 v/2 ] A.2 SP Task Network ( Counter - Based ) The seven counters for the SP task are implemented as follows . First , we compute g ( t ) under the assumption that one of the first four counters is always incremented , and one of the last three counters is always incremented as long as x ( t ) \u0338 = a. g ( t ) = tanh \uf8eb \uf8ec \uf8ec \uf8ed u \uf8ee \uf8ef \uf8ef \uf8f0 I 4 0 1 0 0 0 0 1 0 0 0 0 1 \uf8f9 \uf8fa \uf8fa \uf8fb x ( t ) \uf8f6 \uf8f7 \uf8f7 \uf8f8 Then , we use the input gate to condition the last three counters on the value of the first four counters . For example , if h ( t\u22121 ) 1 = 0 , then no as have been encountered in the input string before time t. In that case , the input gate for counter # 5 , which represents subsequences ending with b , is set to i ( t ) 5 = \u03c3 ( \u2212m ) \u2248 0 . This is because a b encountered at time t would not form part of a subsequence if no as have been encountered so far , so counter # 5 should not be incremented . i ( t ) = \u03c3 \uf8eb \uf8ec \uf8ec \uf8ed 2 m \uf8ee \uf8ef \uf8ef \uf8f0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 \uf8f9 \uf8fa \uf8fa \uf8fb h ( t\u22121 ) + m [ 1 1 1 1 \u22121 \u22121 \u22121 ] \u22a4 ) All other gates are fixed to 1 . The output layer sets the score of the True class to h ( t ) 5 + h ( t ) 6 + h ( t ) 7 and the score of the False class to v/2 . f ( t ) = \u03c3 ( m1 ) o ( t ) = \u03c3 ( m1 ) y ( t ) = [ 0 1 1 1 0 0 0 0 ] h ( t ) + [ 0 v/2 ] A.3 FSA Network Here we describe a general construction of an LSTM simulating an FSA with states Q , accepting states Q F \u2286 Q , alphabet \u03a3 , and transition function \u03b4 : Q \u00d7 \u03a3 Q. Recall that h ( t ) contains a one - hot representation of pairs in Q \u00d7 \u03a3 encoding the current state of the FSA and the most recent input symbol . The initial state h ( 0 ) = 0 represents the starting configuration of the FSA . At a high level , the state transition system works as follows . First , g ( t ) first marks all the positions corresponding to the current input x ( t ) . 6 g ( t ) \u27e8q , x\u27e9 = { v , x = x ( t ) 0 , otherwise The input gate then filters out any positions that do not represent valid transitions from the previous state q \u2032 , which is recovered from h ( t\u22121 ) . i ( t ) \u27e8q , x\u27e9 = { 1 , \u03b4 ( q \u2032 , x ) = q 0 , otherwise Now , we describe how this behavior is implemented in our LSTM . The cell state update is straightforwardly implemented as follows : g ( t ) = tanh ( uW ( c , x ) x ( t ) ) , where W ( c , x ) \u27e8q , x\u27e9 , j = { 1 , j is the index for x 0 , otherwise . Observe that the matrix W ( c , x ) essentially contains a copy of I 4 for each state , such that each copy is distributed across the different cell state units designated for that state . The input gate is more complex . First , the bias term handles the case where the current case is the starting state q 0 . This is necessary because the initial configuration of the network is represented by h ( 0 ) = 0 . b ( i ) \u27e8q , x\u27e9 = { m , \u03b4 ( q 0 , x ) = q \u2212m , otherwise The bias vector sets i ( t ) \u27e8q , x\u27e9 to be 1 if the FSA transitions from q 0 to q after reading x , and 0 otherwise . We replicate this behavior for other values 6 We use v = tanh ( 1 ) \u2248 0.762 . of h ( t\u22121 ) by using the weight matrix W ( i , h ) , taking the bias vector into account : i ( t ) = \u03c3 ( W ( i , h ) h ( t\u22121 ) + b ( i ) ) , where W ( i ) \u27e8q , x\u27e9 , \u27e8q \u2032 , x \u2032 \u27e9 = { m \u2212 b ( i ) \u27e8q , x\u27e9 , \u03b4 ( q \u2032 , x ) = q \u2212m \u2212 b ( i ) \u27e8q , x\u27e9 , otherwise . The forget gate is fixed to \u22121 , since the state needs to be updated at every time step . The output gate is fixed to 1 . f ( t ) = \u03c3 ( \u2212m1 ) o ( t ) = \u03c3 ( m1 ) The output layer simply selects hidden units that represent accepting and rejecting states : y ( t ) = W h ( t ) , where W c , \u27e8q , x\u27e9 = \uf8f4 \uf8f2 \uf8f4 \uf8f3 1 , c = True and q Q F 1 , c = False and q / Q F 0 , otherwise .", "entities": [[132, 133, "DatasetName", "0"], [140, 141, "DatasetName", "0"], [223, 224, "DatasetName", "0"], [225, 226, "DatasetName", "0"], [226, 227, "DatasetName", "0"], [227, 228, "DatasetName", "0"], [228, 229, "DatasetName", "0"], [230, 231, "DatasetName", "0"], [231, 232, "DatasetName", "0"], [232, 233, "DatasetName", "0"], [233, 234, "DatasetName", "0"], [279, 280, "DatasetName", "0"], [327, 328, "DatasetName", "0"], [379, 380, "DatasetName", "0"], [380, 381, "DatasetName", "0"], [382, 383, "DatasetName", "0"], [383, 384, "DatasetName", "0"], [384, 385, "DatasetName", "0"], [385, 386, "DatasetName", "0"], [387, 388, "DatasetName", "0"], [389, 390, "DatasetName", "0"], [390, 391, "DatasetName", "0"], [392, 393, "DatasetName", "0"], [393, 394, "DatasetName", "0"], [485, 486, "DatasetName", "0"], [489, 490, "DatasetName", "0"], [490, 491, "DatasetName", "0"], [491, 492, "DatasetName", "0"], [492, 493, "DatasetName", "0"], [500, 501, "DatasetName", "0"], [514, 515, "MethodName", "LSTM"], [535, 536, "HyperparameterName", "\u03b4"], [541, 542, "MetricName", "Recall"], [578, 579, "DatasetName", "0"], [581, 582, "DatasetName", "0"], [642, 643, "DatasetName", "0"], [686, 687, "HyperparameterName", "\u03b4"], [695, 696, "DatasetName", "0"], [709, 710, "MethodName", "LSTM"], [762, 763, "DatasetName", "0"], [828, 829, "DatasetName", "0"], [845, 846, "DatasetName", "0"], [848, 849, "DatasetName", "0"], [861, 862, "HyperparameterName", "\u03b4"], [864, 865, "DatasetName", "0"], [893, 894, "DatasetName", "0"], [901, 902, "DatasetName", "0"], [998, 999, "HyperparameterName", "\u03b4"], [1121, 1122, "DatasetName", "0"]]}
{"text": "Finally , we describe how the PDA network for the bracket prediction task is implemented . Of the four networks , this one is the most intricate . Recall from Subsection 4.2 that we implement a bounded stack of size k using 2k + 1 hidden units , with the following interpretation : c 2k+1 is a bit , which is set to be positive if the stack is empty and nonpositive otherwise . We represent the brackets ( , [ , ) , and ] in onehot encoding with the indices 1 , 2 , 3 , and 4 , respectively . The opening brackets ( and [ are represented on the stack by 1 and \u22121 , respectively . T We begin by describing g ( t ) . Due to the complexity of the network , we describe the weights and biases individually , which are combined as follows . g ( t ) = tanh ( m ( z ( g , t ) ) ) , where z ( g , t ) = W ( c , x ) x ( t ) + W ( c , h ) h ( t\u22121 ) + b ( c ) First , the bias vector sets c ( t ) 2k+1 to be 1 , indicating that the stack is empty . This ensures that the initial hidden state h ( t ) = 0 is treated as an empty stack . b ( c ) = [ 0 2 ] W ( c , x ) serves three functions when x ( t ) is an open bracket , and does nothing when x ( t ) is a closing bracket . First , it pushes x ( t ) to the top of the stack , represented by c k+1:2k to 1 in order to increment the unary counter for the height of the stack . Later , we will see that the input gate filters out all positions except for the top of the stack . Finally , W ( c , x ) sets the empty stack indicator to \u22121 , indicating that the stack is not empty . W ( c , x ) ( c , h ) performs two functions . First , it completes equation ( 1 ) for c ( t ) k , setting it to be the secondhighest stack item from the previous time step . Second , it copies the top of the stack to the first k \u2212 1 positions , with the input gate filtering out all but the highest position . = \uf8ee \uf8ef \uf8ef \uf8f0 0 0 0 0 2 k \u22122 k 0 0 1 1 0 0 \u22122 \u22122 0 0 \uf8f9 \uf8fa \uf8fa \uf8fb W W ( c , h ) = \uf8ee \uf8ef \uf8ef \uf8f0 0 1 0 0 2 4 2 k\u22121 0 0 0 0 0 0 0 0 0 \u22121 0 \uf8f9 \uf8fa \uf8fa \uf8fb Finally , the \u22121s serve to decrease the empty stack indicator by an amount proportional to the stack height at time t \u2212 1 . Observe that if x ( t ) is a closing bracket and h ( t\u22121 ) represents a stack with only one item , then = \u22121 + 2 = 1 , so the empty stack indicator is set to 1 , indicating that the stack is empty . Otherwise , W ( c , x ) 2k+1 , : x ( t ) + W ( c , h ) 2k+1 , : h ( t\u22121 ) \u2264 \u22122 , so the empty stack indicator is nonpositive . Now , we describe the input gate , given by the following . W ( c , x ) i ( t ) = \u03c3 ( m ( z ( i , t ) ) ) z ( i , t ) = W ( i , x ) x ( t ) + W ( i , h ) h ( t\u22121 ) + b ( i ) W ( i , x ) sets the input gate for the first k \u2212 1 positions to 0 when x ( t ) is a closing bracket . In that case , an item needs to be popped from the stack , so nothing can be copied to these hidden units . When x ( t ) is an opening bracket , W ( i , x ) sets i ( t ) k = 1 , so that the bracket can be copied to the top of the stack . ( i , h ) uses a matrix T n R n\u00d7n , defined below . Suppose v represents the number s in unary notation : v j is 1 if j \u2264 s and 0 otherwise . T n has the special property that T n v is a one - hot vector for s. W ( i , x ) = 2 \uf8ee \uf8f0 0 0 \u22121 \u22121 1 1 0 0 0 \uf8f9 \uf8fb W Based on this , W ( i , h ) is defined as follows . W ( i , h ) = 2 \uf8ee \uf8ef \uf8ef \uf8f0 0 ( T k ) : k\u22121 , : 0 ( T k ) : k\u22121 , : 0 0 \uf8f9 \uf8fa \uf8fa \uf8fb W ( i , h ) : k\u22121 , k+1:2k contains T k , with the last row truncated . This portion of the matrix converts h ( t\u22121 ) k+1:2k , which contains a unary encoding of the stack height , to a one - hot vector marking the position of the top of the stack . This ensures that , when pushing to the stack , the top stack item from time t \u2212 1 is only copied to the appropriate position of h ( t ) : k\u22121 . The other copy of T k , again with the last row omitted , occurs in W ( i , h ) k+2:2k , k+1:2k . This copy of T k ensures that when the unary counter for the stack height is incremented , only the appropriate position is updated . Finally , the bias vector ensures that the top stack item and the empty stack indicator are always updated . b ( i ) = \uf8ee \uf8ef \uf8ef \uf8f0 \u22121 1 \u22121 1 \uf8f9 \uf8fa \uf8fa \uf8fb The forget gate is responsible for deleting portions of memory when stack items are popped . f ( t ) = \u03c3 ( m ( z ( f , t ) ) ) z ( f , t ) = W ( f , x ) x ( t ) + W ( f , h ) h ( t\u22121 ) + b ( f ) W ( f , x ) first ensures that no stack items are deleted when an item is pushed to the stack . W ( f , x ) = 2 \uf8ee \uf8ef \uf8ef \uf8f0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 \uf8f9 \uf8fa \uf8fa \uf8fb Next , W ( f , h ) marks the second highest stack position and the top of the unary counter for deletion , in case an item needs to be popped . W ( f , h ) = 2 \uf8ee \uf8ef \uf8ef \uf8f0 0 \u2212 ( T k ) 2 : , : 0 \u2212T k 0 0 \uf8f9 \uf8fa \uf8fa \uf8fb Finally , the bias term ensures that the top stack item and empty stack indicator are always cleared . b ( i ) = \uf8ee \uf8ef \uf8ef \uf8f0 1 \u22121 1 \u22121 \uf8f9 \uf8fa \uf8fa \uf8fb To complete the construction , we fix the output gate to 1 , and have the output layer read the top stack position : o ( t ) = \u03c3 ( m1 ) y ( t ) = W h ( t ) , where W c , j = \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 1 , c = ) and j = k \u22121 , c = ] and j = k 1 , c = None and j = 2k + 1 0 , otherwise .", "entities": [[28, 29, "MetricName", "Recall"], [239, 240, "DatasetName", "0"], [253, 254, "DatasetName", "0"], [446, 447, "DatasetName", "0"], [447, 448, "DatasetName", "0"], [448, 449, "DatasetName", "0"], [449, 450, "DatasetName", "0"], [454, 455, "DatasetName", "0"], [455, 456, "DatasetName", "0"], [458, 459, "DatasetName", "0"], [459, 460, "DatasetName", "0"], [462, 463, "DatasetName", "0"], [463, 464, "DatasetName", "0"], [480, 481, "DatasetName", "0"], [482, 483, "DatasetName", "0"], [483, 484, "DatasetName", "0"], [488, 489, "DatasetName", "0"], [489, 490, "DatasetName", "0"], [490, 491, "DatasetName", "0"], [491, 492, "DatasetName", "0"], [492, 493, "DatasetName", "0"], [493, 494, "DatasetName", "0"], [494, 495, "DatasetName", "0"], [495, 496, "DatasetName", "0"], [496, 497, "DatasetName", "0"], [498, 499, "DatasetName", "0"], [704, 705, "DatasetName", "0"], [760, 762, "HyperparameterName", "k ="], [813, 814, "DatasetName", "0"], [844, 845, "DatasetName", "0"], [845, 846, "DatasetName", "0"], [850, 851, "DatasetName", "0"], [851, 852, "DatasetName", "0"], [852, 853, "DatasetName", "0"], [883, 884, "DatasetName", "0"], [892, 893, "DatasetName", "0"], [901, 902, "DatasetName", "0"], [902, 903, "DatasetName", "0"], [1190, 1191, "DatasetName", "0"], [1191, 1192, "DatasetName", "0"], [1192, 1193, "DatasetName", "0"], [1193, 1194, "DatasetName", "0"], [1194, 1195, "DatasetName", "0"], [1195, 1196, "DatasetName", "0"], [1198, 1199, "DatasetName", "0"], [1199, 1200, "DatasetName", "0"], [1200, 1201, "DatasetName", "0"], [1201, 1202, "DatasetName", "0"], [1202, 1203, "DatasetName", "0"], [1203, 1204, "DatasetName", "0"], [1253, 1254, "DatasetName", "0"], [1263, 1264, "DatasetName", "0"], [1266, 1267, "DatasetName", "0"], [1267, 1268, "DatasetName", "0"], [1397, 1398, "DatasetName", "0"]]}
{"text": "Before detailing the annotation process , we give a formal introduction to the \" topic \" mentioned in this paper . In topic modeling , a topic is usually viewed as a probability distribution over a fixed vocabulary ( Liu et al , 2016 ) . In addition , previous studies on argument mining usually manually define some coarse - grain topic categories for either topic - dependent argument classification or clustering ( Reimers et al , 2019 ) . Different from previous work , topics in this study refer to fine - grained topic categories that fit the context . For example , given the sentence \" House prices are expected to be fragile . \" , the coarse - grained topic label of it could be \" economics \" and the fine - grained label is \" house price \" . Comparing the two kinds of labels , the first one seems more like the theme of an article which is useful in text - clustering or text - tilling , and the second one gives us more detailed description on the topic itself which is more practical in discourselevel topic chaining . For better understanding of our annotation , we present some preliminary definitions as following : Discourse Topic Unit ( DTU ) refers to the elementary topic unit in our annotated DTC structure . In the literature , Xi and Zhou ( 2017 ) hold the view that each sentence is composed of multiple DTUs with different sub - topics which they refer to as elementary discourse topic unit ( EDTU ) . Different from them , we study macro DTC structures in this work where each sentence is taken as an independent DTU 1 . It is worth mentioning that not all the 1 Although we built the corpus based on RST - DT , it remains DTUs are topic - bearing , there are also some sentences with no topic meaning , e.g. , the sentence \" Oops ! \" . Topic Object ( TO ) could be subject , object , or other noun or noun phrase in the DTU which can provide a certain basis for topic chain parsing . Usually , each TO is closely related to the topic of its DTU , and each DTU maintains an independent TO set . Notably , the \" TO \" mentioned here is not directly equivalent to the \" entity \" in co - reference resolution , the judgment of TO requires a comprehensive consideration of document context . For example , given the DTU \" Drexel Burnham Lambert Inc. is the adviser on the transaction . \" , if the surrounding context of the DTU is mainly about the company , then we choose \" Drexel Burnham Lambert Inc. \" as a TO ; if the context is mainly about the transaction , then we choose \" transaction \" as a TO , and we can also select both of them if necessary . It is worth mentioning that the TOs could also be implicit ones which require human judgments . Topic Event ( TE ) refers to the main phrase which most clearly expresses an event occurrence or a description of the TOs in the DTU . For the DTU u4 in Figure 1 , we select \" develop vaccines against the virus \" as the topic event of the DTU . With the above - mentioned definitions in mind , we argue that each DTU is composed of a set of TOs and a core TE . Based on this concept , we give the following four annotation suggestions : Given two adjacent DTUs in a topic chain , their TO sets should have an intersection in the topic space . For the two DTUs u3 and u4 in Figure 1 , although the two corresponding TO sets , { WHO , risky to directly take each elementary discourse unit ( EDU ) as a DTU since there are many competing hypotheses about what constitutes an EDU but without \" topic \" . Previous work on topic - dependent argument mining usually take each independent sentence as an elementary unit , and this work is inspired by these researches . global expert networks } and { researchers in various countries , vaccines } , have no vocabulary intersection , they are highly related in the topic space on \" international response \" . In a sense , the relationship between TO sets is similar to that between mentions in co - reference resolution or tokens in lexical chains . The difference is that DTC parsing requires not only the correlation between TO sets but also the topic transitivity between DTUs . Therefore , for any two adjacent DTUs on a topic chain , the TE in the second DTU should evolve from the TEs in the established chain where the first DTU is located . Sometimes , a DTU may have topic relevance to multiple subsequent DTUs , we only opt for the closest and most relevant one for annotation . To achieve this , we follow two principles to build each arc in a topic chain : ( 1 ) For each DTU , we search its topic - related DTU from near to far ; ( 2 ) We label topic links for DTUs in order and the annotated DTC structure is dynamically optimized during the human annotation process . For example , when comparing the current DTU ( U - j ) with previous ones , we directly replace the previously annotated arc ( U - i , U - k ) with ( U - i , U - j ) if the topic relevancy between U - i and U - j obviously surpasses that between U - i and U - k. In other words , we do not require all topic chains to be labeled , but we try to ensure the accuracy of the annotated chains as much as possible . This labeling strategy can enhance the value of this small - scale corpus to some extent . In news articles , many DTUs are organized in an overview - example format where similarities among the examples do exist but the evolution of topics is unseen . In this study , we do not consider simple juxtapositions like this . Taking wsj_2349 for example , \" u1 : The following issues were recently filed with the Securities and Exchange Commission : u2 : American Cyanamid Co. , offering of 1 , 250 , 000 common shares , via Merrill Lynch Capital Markets . u3 : Limited Inc. , offering of up to $ 300 million of debt securities . ... u8 : Trans World Airlines Inc. , offering of ... \" . There is a certain textual structure in between the DTUs from u2 to u8 ( e.g. , they share the multinuclear relation List in the RST theory ( Mann and Thompson , 1988 ) ) , but the topic transitivity is weak . Therefore , we do not mark any topic chains among the DTUs . Due to the principle of saving words and avoiding repetitions , ellipsis and co - reference occur frequently . Under this condition , we need to manually fill in the ellipsis and clarify the co - reference for better annotation . Here we take the example in Figure 1 to illustrate the annotation process . Simply put , the annotation process is also the process of comparing the TO and TEs of the current DTU with that of the previous ones . According to the annotation instructions , we do the comparison from near to far aiming to obtain the closest path for two adjacent DTUs on the chain . For the DTU u1 , its TO set contains two topic objects , i.e. , \" coronavirus \" and \" COVID - 19 \" , and its core topic event can be sketched as \" coronavirus outbreak in Wuhan \" . Correspondingly , the TO set of u2 contains a pronoun object \" it \" , referring to \" coronavirus \" , and its core TE is manually detected as \" there is still no idea how to beat it \" . Obviously , the two TO sets have an intersection ( i.e. , \" coronavirus \" ) and the TE in u2 does evolve from that in u1 . Consequently , we mark a topic link between the two DTUs . For u3 , both the TO set and TE do not meet our annotation requirements , so we neither link it to u1 nor u2 . For u4 , the TO set is relevant to that of u3 as international institutions and the two TEs are also interrelated , we therefore build a link between them . In this way , the overall vein of topic chains will be built after several rounds of comparison . Notably , from the resulting graph we find that the topic chain with u1 , u2 , u5 , and u - k on it does provide rich and low - noise information about the evolution of COVID - 19 , which reflects the practical value of our annotated DTCs .", "entities": [[52, 54, "TaskName", "argument mining"], [214, 215, "DatasetName", "DTU"], [287, 288, "DatasetName", "DTU"], [306, 309, "DatasetName", "RST - DT"], [356, 357, "DatasetName", "DTU"], [380, 381, "DatasetName", "DTU"], [384, 385, "DatasetName", "DTU"], [431, 432, "DatasetName", "DTU"], [452, 453, "DatasetName", "DTU"], [544, 545, "DatasetName", "DTU"], [548, 549, "DatasetName", "DTU"], [569, 570, "DatasetName", "DTU"], [584, 585, "DatasetName", "DTU"], [665, 666, "DatasetName", "DTU"], [689, 691, "TaskName", "argument mining"], [808, 809, "DatasetName", "DTU"], [821, 822, "DatasetName", "DTU"], [828, 829, "DatasetName", "DTU"], [873, 874, "DatasetName", "DTU"], [881, 882, "DatasetName", "DTU"], [919, 920, "DatasetName", "DTU"], [999, 1000, "MetricName", "accuracy"], [1270, 1271, "DatasetName", "DTU"], [1308, 1309, "DatasetName", "DTU"]]}
{"text": "Recent years have witnessed the great effects of pre - trained language models ( Devlin et al , Cui et al , 2020 ) on natural language understanding . Following previous work , we introduce a Bert - based ( Devlin et al , 2019 ) method in our baseline system . Given a discourse with k - 1 DTUs , we use the pretrained Bert 3 model to encode the entire discourse where each DTU is surrounded by the [ CLS ] and [ SEP ] tokens . And we take the Bert output corresponding to [ CLS ] as our DTU representation . Following previous work , we also fine - tuned the pre - trained language model parameters during the training process . For the convenience of calculation , a zero - initialized vector u z is added at the end of the DTU sequence for the tail DTUs of the topic chains or the isolated DTUs to point to , obtaining U = ( u 1 , . . . , u k\u22121 , u z ) . For dependency parsing , we simply build a bi - linear function between U and its duplicate to achieve it , as following : U \u03b1 = W \u03b1 U + b \u03b1 U \u03b2 = W \u03b2 U + b \u03b2 s = U T \u03b1 WU \u03b2 where U \u03b1 and U \u03b2 are ( D\u00d7k ) matrices representing U and its duplicate , W R D\u00d7D denotes the parameters of the bilinear term , and s R k\u00d7k refers to the scores for each DTU upon its candidate successor DTUs . The detailed system configuration is presented in the Appendix . We measure the micro - averaged F1 scores of both topic links and chains for performance , and we do not take those isolated DTUs into consideration to avoid the overestimation of performance . For human performance , we asked 5 other researchers majoring in human language analysis to manually annotate the test set and took the averaged F1 scores as human performance . Experimental results in Table 3 show that fine - tuning the contextualized Bert model can achieve a great performance close to human level . By observing the model outputs ( sampled in Appendix ) , we find that the automatically parsed chain structures are highly consistent with the manual annotations , which indicates the high reliability of our corpus . Notably , the obtained system has good generalization and robustness , and can be easily migrated to other NLP tasks for DTC structure incorporation .", "entities": [[25, 28, "TaskName", "natural language understanding"], [75, 76, "DatasetName", "DTU"], [102, 103, "DatasetName", "DTU"], [146, 147, "DatasetName", "DTU"], [183, 185, "TaskName", "dependency parsing"], [207, 208, "HyperparameterName", "\u03b1"], [210, 211, "HyperparameterName", "\u03b1"], [214, 215, "HyperparameterName", "\u03b1"], [216, 217, "HyperparameterName", "\u03b2"], [219, 220, "HyperparameterName", "\u03b2"], [223, 224, "HyperparameterName", "\u03b2"], [228, 229, "HyperparameterName", "\u03b1"], [230, 231, "HyperparameterName", "\u03b2"], [233, 234, "HyperparameterName", "\u03b1"], [236, 237, "HyperparameterName", "\u03b2"], [269, 270, "DatasetName", "DTU"], [292, 293, "MetricName", "F1"], [344, 345, "MetricName", "F1"]]}
{"text": "We used the 768D Bert - base and 1024D Bert - large model for DTU representation . In order to prevent memory overflow , we segment each article according to the maximum length of 64 , and encode the segmented text fragments in turn . We manually set the dropout rate , learning rate , L2 regularization value by 0.2 , 1e - 5 , and 1e - 5 , respectively , according to their contributions to F1 - score , and the number of hyper - parameter search trials was around 15 . We trained the models iteratively on the training corpus for 20 rounds with the batch size set to 1 ( document ) , and we got the best model around the 18 - th round . We implemented the codes based on the PyTorch framework , and all the experiments were conducted on the NVIDIA Tesla P40 GPUs with the random seed set to 2 . The number of parameters in each model and the runtime time of each system are shown in the table below .", "entities": [[14, 15, "DatasetName", "DTU"], [52, 54, "HyperparameterName", "learning rate"], [55, 57, "HyperparameterName", "L2 regularization"], [77, 80, "MetricName", "F1 - score"], [108, 110, "HyperparameterName", "batch size"], [161, 164, "HyperparameterName", "number of parameters"]]}
{"text": "The decoder network is similar to GNMT ( Wu et al , 2016 ) . At each time - step t , let y 0 t\u22121 R e denotes the word embedding of y t\u22121 and y 1 t\u22121 R h denotes the output of bottom LSTM from previous time - step . The attention network calculates the context vector a t as the weighted sum of source annotation vectors : a t = S i=1 \u03b1 t , i x i ( 6 ) Different from GNMT ( Wu et al , 2016 ) , we use the concatenation of y 0 t\u22121 and y 1 t\u22121 as the query vector for attention network , as described follows : h t = [ y 0 t\u22121 T ; y 1 t\u22121 T ] T ( 7 ) e t , i = v T a tanh ( W a h t + U a x i ) ( 8 ) \u03b1 t , i = exp ( e t , i ) S j=1 exp ( e t , j ) This approach is also used in ( Wang et al , 2017 ) . The context vector a t is then fed to all decoder LSTMs . The probability of the next word y t is simply modeled using a softmax layer on the output of top LSTM : p ( y t | x , y < t ) = softmax ( y t , y L dec t ) ( 10 ) We set L dec to 8 in all our experiments . 3 Experimental Features", "entities": [[24, 25, "DatasetName", "0"], [46, 47, "MethodName", "LSTM"], [76, 77, "HyperparameterName", "\u03b1"], [102, 103, "DatasetName", "0"], [125, 126, "DatasetName", "0"], [161, 162, "HyperparameterName", "\u03b1"], [222, 223, "MethodName", "softmax"], [229, 230, "MethodName", "LSTM"], [243, 244, "MethodName", "softmax"]]}
{"text": "For all our models , we adopt Adam ( Kingma and Ba , 2015 ) ( \u03b2 1 = 0.9 , \u03b2 2 = 0.999 and = 1\u00d7 10 \u22128 ) as the optimizer . The learning rate is set to 5 \u00d7 10 \u22124 . We gradually halve the learning rate during the training process . As a common way to train RNNs , we clip the norm of gradient to a predefined value 5.0 . The batch size is 128 . We use dropout ( Srivastava et al , 2014 ) to avoid overfitting with a keep probability of 0.8 .", "entities": [[7, 8, "MethodName", "Adam"], [16, 17, "HyperparameterName", "\u03b2"], [21, 22, "HyperparameterName", "\u03b2"], [33, 34, "HyperparameterName", "optimizer"], [36, 38, "HyperparameterName", "learning rate"], [50, 52, "HyperparameterName", "learning rate"], [78, 80, "HyperparameterName", "batch size"]]}
{"text": "Test ( BLEU ) Baseline 25.7 + Synthetic 26.1 + Ensemble 26.7 Table 1 : English - German translation results on newstest2017 . Table 1 show the results of English - German Translation . The baseline system is trained on preprocessed parallel data 4 . For synthetic data , we randomly sample 10 M German sentences from NewsCrawl2016 and translate them back to English using an German - English model . However , we found random sampling do not work well . As a result , for Chinese - English translation , we select monolingual data according to development set . We first train one baseline model and continue to train 4 models on synthetic data with different shuffles . Next we ensemble 4 models and get the final results . We found this approach do not lead to substantial improvements .", "entities": [[2, 3, "MetricName", "BLEU"], [32, 33, "TaskName", "Translation"]]}
{"text": "Test ( BLEU ) We use all training data ( CWMT Corpus , UN Parallel Corpus and News Commentary ) to train a baseline system . The Chinese sentences are segmented using Stanford Segmenter 5 . For English sentences , we use the moses tokenizer 6 . We filter bad sentences according to the alignment score obtained by fast - align toolkit 7 and remove duplications in the training data . The preprocessed training data consists of 19 M bilingual pairs . As noted earlier , the monolingual data is selected using newsdev2017 . We first train 4 L2R models and one R2L model on training data , then we finetune our model on a mixture of 2.5 M synthetic bilingual pairs and 2.5 M bilingual pairs sampled from CWMT corpus . As shown in Table 3 show the results of English - Chinese Translation . We use our reimplementation of DL4MT to train English - Chinese models on CWMT and UN parallel corpus . The preprocessing steps , including word segmentation , tokenization , and sentence filtering , are almost the same as Section 4.2 , except that we limited the vocabulary size to 50 K and split all target side OOVs into characters . For synthetic parallel data , we use SRILM 8 to train a 5 - gram KN language model on XinhuaNet2011 and select 2.5 M sentences from XinhuaNet2011 according to their perplexities . We obtained +3.9 BLEU score when tuning the single best model on a mixture of 2.5 M synthetic bilingual pairs and 2.5 M bilingual pairs selected from CWMT parallel data randomly . We further gain +1.5 BLEU score when ensembling 4 models .", "entities": [[2, 3, "MetricName", "BLEU"], [144, 145, "TaskName", "Translation"], [241, 243, "MetricName", "BLEU score"], [274, 276, "MetricName", "BLEU score"]]}
{"text": "Figure 1 : Examples of text with rigid formats . In lyrics , the syllables of the lyric words must align with the tones of the notation . In SongCi and Sonnet , there are strict rhyming schemes and the rhyming words are labeled in red color and italic font . 2014 ; Gehring et al , 2017 ) , Transformer and its variants ( Vaswani et al , 2017 ; , pre - trained auto - regressive language models such as XLNet and GPT2 ( Radford et al , 2019 ) , etc . Performance has been improved significantly in lots of tasks such as machine translation Vaswani et al , 2017 ) , dialogue systems ( Vinyals and Le , 2015 ; Shang et al , 2015 ; Li , 2020 ) , text summarization ( Rush et al , 2015 ; Li et al , 2017 ; See et al , 2017 ) , story telling ( Fan et al , 2018 ; See et al , 2019 ) , poetry writing ( Zhang and Lapata , 2014 ; Lau et al , 2018 ; Liao et al , 2019 ) , etc . Generally , most of the above mentioned tasks can be regarded as free text generation , which means that no constraints on the format and structure , say the number of words and rhyming rules . Note that tasks of dialogue generation and story telling are almost in an open - ending generation style as long as the generated content is relevant with the conditional input text . Although there are formats constraints on the poetry text , the proposed models just treat the formats as kind of latent information and let the model capture this feature implicitly during training ( Liao et al , 2019 ) . The model trained on the five - character quatrain corpus can not generate seven - character verses . Moreover , it is impossible to trigger these models to generate satisfying results according to arbitrary new defined formats . In practice we will confront some special text paradigms such as Lyrics ( assume the music score is given ) , Sonnet ( say Shakespeare 's Sonnets ( Shakespeare , 2000 ) ) , SongCi ( a kind of Ci . Ci is a type of lyric poetry in the tradition of Classical Chinese poetry . 2 , SongCi is the Ci created during Song dynasty ) , etc . , and some examples are illustrated in Figure 1 . The typical characteristics of these text can be categorized into three folds : ( 1 ) The assembling of text must comply fully with the predefined rigid formats . Assume that the music score is composed , then the lyricist must fill the lyric content strictly tally with the schemes lie in the notation . Take partial of song \" Edelweiss \" as shown in the first row of Figure 1 as example , the syllables of the lyric words must align with the tones of the notation . The second row of Figure 1 depicts the content of a SongCi created based on the CiPai of \" Bu Suan Zi \" . Given the CiPai , the number of characters and the syntactical structure of the content are also defined ( e.g. , the number of characters of each clause : 5 , 5 . 7 , 5 . 5 , 5 . 7 , 5 . ) . ( 2 ) The arrangement of the content must obey the defined rhyming schemes . For example , all the final words ( words in red color and italic font ) of the SongCi content in Figure1 are rhyming ( the spelling of each word is : \" zhu \" , \" yu \" , \" du \" , and \" gu \" . ) . The example in the third row of Figure 1 comes from Shakespeare 's \" Sonnet 116 \" ( Shakespeare , 2000 ) , the first four sentences . Usually , the rhyming schemes of Shakespeare 's Sonnets is \" ABAB CDCD EFEF GG \" 3 . In the example , the rhyming words in scheme \" ABAB \" are \" minds \" , \" love \" , \" finds \" , and \" remove \" . ( 3 ) Even though the format is rigid , the sentence integrity must always be guaranteed . Incomplete sentence such as \" love is not the \" is inappropriate . To the best of our knowledge , text generation based on the predefined rigid formats constraints has not been well investigated yet . In this work , we propose a simple and elegant framework named SongNet to address this challenging problem . The backbone of the framework is a Transformer - based auto - regressive language model . Considering the three folds characteristics mentioned above , we introduce sets of tailor - designed indicating symbols to improve the modeling performance , especially for the robustness of the format , rhyme , as well as sentence integrity . We improve the attention mechanism to impel the model to capture the future information on the format to further enhance sentence integrity . Inspired by BERT ( Devlin et al , 2019 ) and GPT ( Radford et al , 2018 ( Radford et al , , 2019 , a pretraining and fine - tuning framework is designed to further improve the generation quality . To verify the performance of our framework , we collect two corpora , SongCi and Sonnet , in Chinese and English respectively . Extensive experiments on the collected datasets demonstrate that our proposed framework can generate satisfying results in terms of both the tailor - designed automatic metrics including format accuracy , rhyming accuracy , sentence integrity , as well as the human evaluation results on relevance , fluency , and style . In summary , our contributions are as follows : We propose to tackle a new challenging task : rigid formats controlled text generation . A pre - training and fine - tuning framework named SongNet is designed to address the problem . Sets of symbols are tailor - designed to improve the modeling performance . We improve the attention mechanism to impel the model to capture the future information to further enhance the sentence integrity . To verify the performance of our framework SongNet , we collect two corpora , SongCi and Sonnet , in Chinese and English respectively . We design several automatic evaluation metrics and human evaluation metrics to conduct the performance evaluation . Extensive experiments conducted on two collected corpora demonstrate that our proposed framework generates significantly better results given arbitrary formats , including the cold - start formats or even the formats newly defined by ourselves .", "entities": [[60, 61, "MethodName", "Transformer"], [82, 83, "MethodName", "XLNet"], [106, 108, "TaskName", "machine translation"], [135, 137, "TaskName", "text summarization"], [210, 212, "TaskName", "text generation"], [237, 239, "TaskName", "dialogue generation"], [763, 765, "TaskName", "text generation"], [791, 792, "MethodName", "SongNet"], [805, 806, "MethodName", "Transformer"], [876, 877, "DatasetName", "Inspired"], [878, 879, "MethodName", "BERT"], [887, 888, "MethodName", "GPT"], [968, 969, "MetricName", "accuracy"], [971, 972, "MetricName", "accuracy"], [1012, 1014, "TaskName", "text generation"], [1025, 1026, "MethodName", "SongNet"], [1074, 1075, "MethodName", "SongNet"]]}
{"text": "The task of rigid formats controlled text generation is defined as follows : Input : a rigid format C C : C = { c 0 c 1 c 2 c 3 , c 0 c 1 c 2 c 3 c 4 c 5 . } ( 1 ) where C is the set of all possible formats . Note that we can define arbitrary new formats not restricted to the ones pre - defined in the corpus , thus | C | . Format token c i denotes a place - holder symbol of C which need to be translated into a real word token . Format C contains 10 words plus two extra punctuation characters \" , \" and \" . \" Output : a natural language sentence Y Y which tally with the defined format C : Y = love is not love , bends with the remover to remove . where the example sentences are extracted from the Shakespeare 's Sonnets ( Shakespeare , 2000 ) . From the result Y we can observe that the count of words is 10 which is consistent with the format C. The punctuation characters \" , \" and \" . \" are also correct . Thus , we claim that it is a 100 % format accuracy result . Also , since the two clause sentences are complete , we can get a good sentence integrity score . If C is defined on the literary genres of SongCi or Sonnet which have rhyming constraints , the rhyming performance should be evaluated as well . Recall that C can be arbitrary and flexible , thus we can rebuild a new format C based on the generated result Y by masking partial content , say C = { c 0 c 1 c 2 love , c 0 c 1 c 2 c 3 c 4 remove . } , then we may obtain better results by re - generating based on C . We name this operation as polishing . Finally , the target of this problem is to find a mapping function G to conduct the rigid formats controlled text generation : Y = G ( C ) ( 2 ) 3 Framework Description", "entities": [[6, 8, "TaskName", "text generation"], [25, 26, "DatasetName", "0"], [34, 35, "DatasetName", "0"], [218, 219, "MetricName", "accuracy"], [266, 267, "MetricName", "Recall"], [299, 300, "DatasetName", "0"], [307, 308, "DatasetName", "0"], [361, 363, "TaskName", "text generation"]]}
{"text": "As shown in Figure 2 , the backbone of our framework is a Transformer - based auto - regressive language model . The input can be the whole token sequences of samples from SongCi or Sonnet . We tailor - design several sets of indicating symbols to enhance the performance in terms of accuracy on format , rhyme , and sentence integrity . Specifically , symbols C = { c i } are introduced for format and rhyming modeling ; Intra - position symbols P = { p i } are designed to represent the local positions of the tokens within each sentence aiming to improve the rhyming performance and the sentence integrity . Segment symbols S = { s i } are employed to identify the sentence border to further improve the sentence quality . Attention mechanism is improved to impel the model to capture the future format information such as the sentence ending markers . Similar to BERT ( Devlin et al , 2019 ) and GPT ( Radford et al , 2018 ( Radford et al , , 2019 , pre - training and fine - tuning paradigm is utilized to boost the performance of the original models .", "entities": [[13, 14, "MethodName", "Transformer"], [53, 54, "MetricName", "accuracy"], [159, 160, "MethodName", "BERT"], [168, 169, "MethodName", "GPT"]]}
{"text": "We use two sentences ( as shown in Figure 1 ) \" love is not love , ... , bends with the remover to remove \" extracted from the Shakespeare 's Sonnets ( Shakespeare , 2000 ) as examples to describe the details of our framework SongNet . Since our basic model is a Transformer - based auto - regressive language model , during training , the input is \" bos love is not love , /s ... , bends with the remover to remove . /s \" , and the corresponding output is a left - shifting version of the input ( tokenized , and we ignore \" ... \" for convenience and clarity ) : love is not love , /s bends with the remover to remove . /s eos where /s denotes the clause or sentence separator , and eos is the ending marker of the whole sequence . The target of our framework is to conduct the formats controlled text generation . Therefore , the indicating symbols for format and rhyme as well as the sentence integrity are designed based on the target output sequence . Format and Rhyme Symbols : C = { c 0 , c 0 , c 0 , c 2 , c 1 , /s c 0 , c 0 , c 0 , c 0 , c 0 , c 2 , c 1 , /s , eos } ( 3 ) where we use { c 0 } to represent the general tokens ; { c 1 } depict the punctuation characters ; { c 2 } represent the rhyming tokens \" love \" and \" remove \" . /s and eos are kept . Intra - Position Symbols : P = { p 4 , p 3 , p 2 , p 1 , p 0 , /s p 6 , p 5 , p 4 , p 3 , p 2 , p 1 , p 0 , /s , eos } ( 4 ) { p i } denote the local positions of tokens within the same clause or sentence . Note that we align the position symbol indices in a descending order . The aim is to improve the sentence integrity by impelling the symbols capture the sentence dynamic information , precisely , the sense to end a sequence . For example , { p 0 } usually denote punctuation characters , thus { p 1 } should be the ending words of sentences . Segment Symbols : S = { s 0 , s 0 , s 0 , s 0 , s 0 , /s s 1 , s 1 , s 1 , s 1 , s 1 , s 1 , s 1 , /s , eos } ( 5 ) where s i is the symbol index for sentence i. The purpose is to enhance the interactions between different sentences in different positions by defining the sentence index features . During training , all the symbols as well as the input tokens are fed into the transformer - based language model . Contrast to Transformer ( Vaswani et al , 2017 ) , BERT ( Devlin et al , 2019 ) , and GPT2 ( Radford et al , 2019 ) , we modify the traditional attention strategies slightly to fit our problem . Specifically , for the input , we first obtain the representations by summing all the embeddings of the input tokens and symbols , as shown in the red solid box of Figure 2 : H 0 t = E wt + E ct + E pt + E st + E gt ( 6 ) where 0 is the layer index and t is the state index . E * is the embedding vector for input * . w t is the real token at position t. c , p , and s are three pre - defined symbols . g is the global position index same as position symbols used in Transformer ( Vaswani et al , 2017 ) . Moreover , the state at time t need to know some future information to grasp the global sequence dynamic information . For example , the model may want to know if it should close the decoding progress by generating the last word and a punctuation character to end the sentence . To represent the global dynamic information , we introduce another variable F 0 by only summing the pre - defined symbols as shown in the blue dash box of Figure 2 : F 0 t = E ct + E pt + E st ( 7 ) After processing the input , two blocks of attention mechanisms are introduced to conduct the feature learning procedure . The first block is a masking multi - head self - attention component , and the second block is named global multi - head attention . Masking Multi - Head Self - Attention : C 1 t = LN FFN ( C 1 t ) + C 1 t C 1 t = LN SLF - ATT ( Q 0 t , K 0 \u2264t , V 0 \u2264t ) + H 0 t Q 0 = H 0 W Q K 0 , V 0 = H 0 W K , H 0 W V ( 8 ) where SLF - ATT ( ) , LN ( ) , and FFN ( ) represent self - attention mechanism , layer normalization , and feed - forward network respectively . Note that we only use the states whose indices \u2264 t as the attention context . After obtaining C 1 t from Equation ( 8 ) , we feed it into the second attention block to capture the global dynamic information from F 0 . Global Multi - Head Attention : H 1 t = LN FFN ( H 1 t ) + H 1 t H 1 t = LN GLOBAL - ATT ( Q 1 t , K 1 , V 1 ) + C 1 t Q 1 = C 1 W Q K 1 , V 1 = F 0 W K , F 0 W V ( 9 ) We can observe that all the context information from F 0 are considered . This is the reason why we name it as \" global attention \" and why the input real token information E wt is NOT considered . Then the calculation of the unified first model layer is finished . We can iteratively apply these two attention blocks on the whole L model layers until obtain the final representations H L . Note that H is renewed layerly , however the global variable F 0 is fixed . Finally , the training objective is to minimize the negative log - likelihood over the whole sequence : L nll = \u2212 n t=1 log P ( y t | y < t ) ( 10 )", "entities": [[46, 47, "MethodName", "SongNet"], [54, 55, "MethodName", "Transformer"], [163, 165, "TaskName", "text generation"], [199, 200, "DatasetName", "0"], [202, 203, "DatasetName", "0"], [205, 206, "DatasetName", "0"], [215, 216, "DatasetName", "0"], [218, 219, "DatasetName", "0"], [221, 222, "DatasetName", "0"], [224, 225, "DatasetName", "0"], [227, 228, "DatasetName", "0"], [247, 248, "DatasetName", "0"], [307, 308, "DatasetName", "0"], [329, 330, "DatasetName", "0"], [400, 401, "DatasetName", "0"], [427, 428, "DatasetName", "0"], [430, 431, "DatasetName", "0"], [433, 434, "DatasetName", "0"], [436, 437, "DatasetName", "0"], [439, 440, "DatasetName", "0"], [524, 525, "MethodName", "Transformer"], [533, 534, "MethodName", "BERT"], [599, 600, "DatasetName", "0"], [620, 621, "DatasetName", "0"], [676, 677, "MethodName", "Transformer"], [748, 749, "DatasetName", "0"], [769, 770, "DatasetName", "0"], [823, 827, "MethodName", "multi - head attention"], [861, 862, "DatasetName", "0"], [865, 866, "DatasetName", "0"], [869, 870, "DatasetName", "0"], [874, 875, "DatasetName", "0"], [877, 878, "DatasetName", "0"], [880, 881, "DatasetName", "0"], [884, 885, "DatasetName", "0"], [887, 888, "DatasetName", "0"], [890, 891, "DatasetName", "0"], [895, 896, "DatasetName", "0"], [922, 924, "MethodName", "layer normalization"], [975, 976, "DatasetName", "0"], [978, 982, "MethodName", "Multi - Head Attention"], [1035, 1036, "DatasetName", "0"], [1040, 1041, "DatasetName", "0"], [1056, 1057, "DatasetName", "0"], [1132, 1133, "DatasetName", "0"], [1146, 1149, "MetricName", "log - likelihood"], [1155, 1156, "MetricName", "nll"]]}
{"text": "Although our framework can be trained purely on the training dataset of the target corpus , usually the scale of the corpus is limited . For example , there are only about 150 samples in the corpus of Shakespeare 's Sonnets ( Shakespeare , 2000 ) . Therefore , we also design a pre - training and fine - tuning framework to further improve the generation quality . Recall that in the task definition in Section 2 , we claim that our model owns the ability of refining and polishing . To achieve this goal , we adjust the masking strategy used in BERT ( Devlin et al , 2019 ) to our framework according to our definitions . Specifically , we randomly ( say 20 % ) select partial of the original content and keep them not changed when building the format symbols C. For example , we will get a new symbol set C for the example sentences : C = { c0 , c0 , c0 , love , c1 , /s bends , c0 , c0 , c0 , c0 , remove , c1 , /s , eos } where \" love \" , \" bends \" and \" remove \" are kept in the format C . After the pre - training stage , we can conduct the fine - tuning procedure directly on the target corpus without adjusting any model structure .", "entities": [[68, 69, "MetricName", "Recall"], [103, 104, "MethodName", "BERT"]]}
{"text": "The parameter size of our model are fixed in both the pre - training stage and the fine - tuning stage . The number of layers L = 12 , and hidden size is 768 . We employ 12 heads in both the masking multihead self - attention block and the global attention block . Adam ( Kingma and Ba , 2014 ) optimization method with Noam learning - rate decay strategy and 10 , 000 warmup steps is employed to conduct the pre - training .", "entities": [[23, 26, "HyperparameterName", "number of layers"], [55, 56, "MethodName", "Adam"]]}
{"text": "Besides PPL and Distinct ( Li et al , 2016 ) , we also tailor - design several metrics for our task to conduct the evaluation for format , rhyme , and sentence integrity . Format Assume that there are m sentences defined in the format C = { C s 1 , C s 2 , ... , C s m } , and the generated results Y contains n sentences Y = { Y s 1 , Y s 2 , ... , Y s n } . Without loss of generality , we align C and Y from the beginning , and calculate the format quality according to the following rules : ( 1 ) the length difference | | C s i | \u2212 | Y s i | | \u2264 \u03b4 ; ( 2 ) the punctuation characters must be same . For SongCi , we let \u03b4 = 0 and rule ( 2 ) must be conforming . For Sonnet , we relax the condition where we let \u03b4 = 1 and ignore rule ( 2 ) . Assume that the number of format - correct sentences is n , then we can obtain Precision p = n /n , Recall r = n /m , and F1 - measure . We report both the Macro - F1 and Micro - F1 in the results tables . Model PPL\u2193 Diversity ( Distinct ) \u2191 VAL TEST MA - D - 1 MI - D - 1 MA - D - 2 MI - D - 2 GPT2 w/ Rhyme For SongCi , usually , there is only one group of rhyming words in one sample . As the example shown in Table 1 , the pronunciation of the red rhyming words are \" zhu \" , \" y\u00fc \" , \" du \" , and \" gu \" respectively , and the rhyming phoneme is \" u \" . For the generated samples , we first use the tool pinyin 4 to get the pronunciations ( PinYin ) of the words in the rhyming positions , and then conduct the evaluation . For Shakespeare 's Sonnets corpus , the rhyming rule is clear \" ABAB CDCD EFEF GG \" and there are 7 groups of rhyming tokens . For the generated samples , we employ the CMU Pronouncing Dictionary 5 ( Speech@CMU , 1998 ) to obtain the phonemes of the words in the rhyming positions . For example , the phonemes for word \" asleep \" and \" steep \" are [ ' AH0 ' , 'S ' , ' L ' , ' IY1 ' , ' P ' ] and [ 'S ' , ' T ' , ' IY1 ' , ' P ' ] respectively . And then we can conduct the evaluation by counting the overlapping units from both the original words and the extracted phonemes group by group . We report the Macro - F1 and Micro - F1 numbers in the results tables as well . Integrity Since the format in our task is strict and rigid , thus the number of words to be predicted is also pre - defined . Our model must organize the language using the limited positions , thus sentence integrity may become a serious issue . For example , the integrity of \" love is not love . /s \" is much better than\"love is not the . /s \" . To conduct the evaluation of sentence integrity , we design a straightforward method by calculating the prediction probability of the punctuation characters before /s given the prefix tokens : Model PPL\u2193 Diversity ( Distinct ) \u2191 VAL TEST MA - D - 1 MI - D - 1 MA - D - 2 MI - D - 2 Integrity = 2 \u2212 1 | Y | | Y | i=1 log ( P ( y i punc | y i 0 , y i 1 , ... , y i < punc ) ) ( 11 ) where Y is the generated sequence of sentences . Smaller integrity metric value indicates higher sentence quality . To achieve this goal , we conduct pre - trainings for two GPT2 ( Radford et al , 2019 ) models on the large scale Chinese corpus and English corpus respectively . Then we utilize the GPT2 models to conduct the evaluation for sentence integrity . Human Evaluations For SongCi , we sampled 50 samples for 25 CiPais . For Sonnet , the whole 27 samples in the test set are selected for human evaluation . We recruit three helpers to score the Relevance , Fluency , and Style . The rating criteria are as follows : Relevance : +2 : all the sentences are relevant to the same topic ; +1 : partial sentences are relevant ; 0 : not relevant at all . Fluency : +2 : fluent ; +1 : readable but with some grammar mistakes ; 0 : unreadable . Style : +2 : match with SongCi or Sonnet genres ; +1 : partially match ; 0 : mismatch .", "entities": [[91, 92, "MetricName", "loss"], [135, 136, "HyperparameterName", "\u03b4"], [152, 153, "HyperparameterName", "\u03b4"], [154, 155, "DatasetName", "0"], [174, 175, "HyperparameterName", "\u03b4"], [200, 201, "MetricName", "Precision"], [206, 207, "MetricName", "Recall"], [213, 214, "MetricName", "F1"], [221, 224, "MetricName", "Macro - F1"], [225, 228, "MetricName", "Micro - F1"], [495, 498, "MetricName", "Macro - F1"], [499, 502, "MetricName", "Micro - F1"], [661, 662, "DatasetName", "0"], [814, 815, "DatasetName", "0"], [836, 837, "DatasetName", "0"], [856, 857, "DatasetName", "0"]]}
{"text": "Please note that we mainly employ top - k sampling method ( Fan et al , 2018 ; Radford et al , 2019 ) to conduct the generation , and we let k = 32 here . The parameter tuning of k is described in Section 5.3 . on SongCi . The main reason is that Sonnet only contains 100 samples in the training set as shown in Table 3 . Therefore , the model can not capture sufficient useful features especially for the rhyming issue .", "entities": [[32, 34, "HyperparameterName", "k ="]]}
{"text": "Since we employ top - k sampling as our main decoding strategy , thus we design several experiments to conduct the parameter tuning on k. We let k to be 1 , 5 , 10 , 20 , 50 , 500 respectively . We also provide the beam - search ( beam=5 ) results for comparing and reference . The parameter tuning results are depicted in Figure 3 . From the results we can observe that large k can increase the diversity of the results significantly . But the Rhyme accuracy and the sentence integrity will drop simultaneously . Therefore , in the experiments we let k = 32 to obtain a trade - off between the diversity and the general quality .", "entities": [[90, 91, "MetricName", "accuracy"], [106, 108, "HyperparameterName", "k ="]]}
{"text": "In this phase , instances for evaluation are released . 3 Each participating system generated predictions for the test instances , for up to N models . 4 Predictions are submitted to CodaLab 5 and evaluated automatically against the goldstandard labels . Submissions were anonymized . The only statistics displayed were the highest score of all systems per day . The total allowable number of system submissions per day was limited to 5 per team per track . The metric used for evaluation is the F1 score ( least frequent class / label , which is \" metaphor \" ) with Precision and Recall also available via the detailed results link in CodaLab . The shared task started on January 12 , 2020 when the training data was made available to registered participants . On February 14 , 2020 , the testing data was released . Submissions were accepted until April 17 , 2020 . Table 3 shows the submission statistics for systems with a system paper . Generally , there were more participants in the VUA tracks than in TOEFL tracks , and in All POS tracks than in Verbs tracks . In total , 13 system papers were submitted describing methods for generating metaphor / non - metaphor predictions .", "entities": [[85, 87, "MetricName", "F1 score"], [101, 102, "MetricName", "Precision"], [103, 104, "MetricName", "Recall"]]}
{"text": "The clearest trend in the 2020 submissions is the use of deep learning architectures based on BERT ( Devlin et al , 2018 ) - more than half of the participating systems used BERT or its variant . The usefulness of BERT for metaphor detection has been shown by Mao et al ( 2019 ) , where a BERT - based system posted F1 = 0.717 on VUA AllPOS , hence our use of a BERT - based system as Baseline 3 . Beyond explorations of neural architectures , we also observe usage of new lexical , grammatical , and morphological information , such as finegrained POS , spell - corrected variants of words ( for TOEFL data ) , sub - word level information ( e.g. , character embeddings ) , idioms , sensorimotor and embodiment - related information .", "entities": [[16, 17, "MethodName", "BERT"], [33, 34, "MethodName", "BERT"], [41, 42, "MethodName", "BERT"], [58, 59, "MethodName", "BERT"], [63, 64, "MetricName", "F1"], [75, 76, "MethodName", "BERT"]]}
{"text": "Since the same VUA dataset was used in 2020 shared task as in the 2018 shared task , we can directly compare the performance of the best systems to observe the extent of the improvement . The best system in 2018 performed at F1 = 0.651 ; the best performance in 2020 is more than 10 points better - F1 = 0.769 . Indeed , the 2018 best performing system would have earned the rank of 11 in the 2020 All POS track , suggesting that the field has generally moved to more effective models than those proposed for the 2018 competitions . The best results posted for the 2020 shared task are the new state - of - the - art for both VUA 7 and TOEFL corpora . 7 While a number of recent systems were evaluted on VUA data ( Le et al , 2020 ; Dankers et al , 2019 ; Mao et al , 2019 ; Gao et al , 2018 ) , their results are not directly comparable to the shared task , since they evaluated on all parts of speech , including function words . See Dankers et al ( 2020 ) for a discussion .", "entities": [[43, 44, "MetricName", "F1"], [59, 60, "MetricName", "F1"]]}
{"text": "Table 6 shows performance by genre for the VUA data All POS track . The patterns are highly consistent across systems , and replicate those observed for the 2018 shared task - Academic and News genres are substantially easier to handle than Fiction and Conversation . The gap between the best and worst performance across genres for the same system remains wide - between 11.4 F1 points and 24.3 F1 points . Somewhat encouragingly , the gap is narrower for the better performing systems - the top 6 systems show the smallest gaps between best and worst genres ( 11.4 - 14.0 ) .", "entities": [[65, 66, "MetricName", "F1"], [69, 70, "MetricName", "F1"]]}
{"text": "Table 7 shows performance and ranks of the best systems for teams that participated in both VUA and TOEFL AllPOS tracks , along with baselines . Overall , the relative performance rankings are consistent - F1 scores are correlated at r = .92 and team ranks are correlated at r = 0.95 across the two datasets . All teams posted better performance on the VUA data than on the TOEFL data ; the difference ( see column 4 in Table 6 : VUA Dataset : Performance ( F1 - score ) of the best systems submitted to All - POS track by genre subsets of the test data . In parentheses , we show the rank of the given genre within all genres for the system . The last column shows the overall drop in performance from best genre ( ranked 1 ) to worst ( ranked 4 ) . The top three performances for a given genre are boldfaced . F1 point ( zhengchang ) to 5 F1 points ( DeepMet , umd bilstm , Zenith ) . The BERT baseline posted a relatively large difference of 9 F1 points ; this could be because BNC data is more similar to the data on which BERT has been pre - trained than TOEFL data . We note , however , that participating systems that used BERT showed a smaller performance gap between VUA and TOEFL data ; in zhengchang the gap is all but eliminated . This suggests that a BERT - based system with parameters optimized for performance on TOEFL data can close this gap . Considering TOEFL data as an additional genre , along with the four genres represented in VUA , we observe that it is generally harder than Academic and News , and is commensurate with Fiction in terms of performance , for the three systems with best VUA All POS performance ( Deep - Met : 0.72 both , Go Figure ! : 0.69 both , illiniMet : 0.69 for VUA Fiction , .70 for TOEFL ) ; a caveat to this observation is that the difference between VUA and TOEFL is not only in genre but in the metaphor annotation guidelines as well .", "entities": [[35, 36, "MetricName", "F1"], [87, 90, "MetricName", "F1 - score"], [161, 162, "MetricName", "F1"], [168, 169, "MetricName", "F1"], [174, 175, "MethodName", "bilstm"], [180, 181, "MethodName", "BERT"], [189, 190, "MetricName", "F1"], [206, 207, "MethodName", "BERT"], [226, 227, "MethodName", "BERT"], [251, 252, "MethodName", "BERT"], [320, 321, "DatasetName", "Met"]]}
{"text": "Table 8 shows performance for All POS track on the TOEFL data by the writer 's proficiency level - high or medium . We note that the quality of the human annotations does not appear to differ substantially by proficiency : The average inter - annotator agreement for the high proficiency essays was \u03ba = 0.619 , while it was \u03ba = 0.613 for the medium proficiency essays . We observe that generally systems tend to perform better on the higher proficiency essays , although two of the 12 systems posted better performance on the medium proficiency data . However , even though the medium proficiency essays might have deficiencies in grammar , spelling , coherence and other properties of the essay that could interfere with metaphor detection , we generally observe relatively small differences in performance by proficiency - up to 3.5 F1 points , with a few ex - ceptions ( zhengchang , Go Figure ! ) . Interestingly , automatic correction of spelling errors does not seem to guarantee a smaller gap in performance ( see , Go Figure ! ) . In parentheses , we show the rank of the given proficiency level within all levels for the system . The last column shows the overall drop in performance from best proficiency level ( ranked 1 ) to worst ( ranked 4 ) . The top three performances for a given genre are boldfaced .", "entities": [[143, 144, "MetricName", "F1"]]}
{"text": "Table 9 shows the performance of the systems submitted to the All POS tracks for VUA and TOEFL data broken down by part of speech ( Verbs , Nouns , Adjectives , Adverbs ) . As can be observed both from the All POS vs Verbs tracks ( Tables 4 and 5 ) and from Table 9 , performance on Verbs is generally better than on All POS . 8 For VUA data , all but one systems perform best on Verbs , followed by Adjectives and Nouns , with the worst performance generally observed for Adverbs . These results replicate the findings from the 2018 shared task and follow the proportions of metaphors in the respective parts of speech , led by Verbs ( 30 % ) , Adjectives ( 18 % ) , Nouns ( 13 % ) , Adverbs ( 8 % ) . The average gap between best and worst POS performance has also stayed similar - 11 F1 points ( it was 9 % in 2018 ) . For the TOEFL data , the situation is quite different . Adjectives lead the scoreboard for all but 3 systems , with Adverbs and Verbs coming next , while Nouns proved to be the most challenging category for all participating systems . Furthermore , the gap between best and worst POS performance is large - 17 F1 points on average , ranging between 11 and 22 points . The best performance on Nouns is only F1 = 0.641 ; it would have ranked 10th out of 12 on Adjectives . The proportions of metaphorically used Verbs ( 13 % ) , Adjectives ( 8 % ) , Nouns ( 4 % ) , and Adverbs ( 3 % ) ( based on training data ) perhaps offer some explanation of the difficulty with nouns , since nominal metaphors seem to be quite rare . Stemle and Onysko ( 2020 ) observed that metaphors occur more frequently in responses to some essay prompts that to others among the 8 prompts covered in the TOEFL dataset ; moreover , for some prompts , a metaphor is suggested in the prompt itself and occurs frequently in responses ( e.g. whether broad knowledge is better than specialized knowledge ) . It is possible that prompt - based patterns interact with POS patterns in ways that affect relative ease or difficulty of POS for metaphor identification . 9 : VUA and TOEFL Datasets by POS : Performance ( F1 - score ) of the best systems submitted to All - POS track by POS subsets of the test data . In parentheses , we show the rank of the given POS within all POS for the system . The last column shows the overall drop in performance from best POS ( ranked 1 ) to worst ( ranked 4 ) .", "entities": [[162, 163, "MetricName", "F1"], [229, 230, "MetricName", "F1"], [248, 249, "MetricName", "F1"], [416, 419, "MetricName", "F1 - score"]]}
{"text": "Our query - assisted summarization model , M Summ , is autoregressive , outputting the requested number of summary sentences one - by - one . At time step t , a sentence e out t is output according to the current query and an encoding of the summary - so - far E t = { e in 1 , ... , e in k , e out 1 , ... , e out t\u22121 } to prevent information repetition . At inference time , M Summ outputs the summary sentences with the given query and history ( possibly empty ) . At train time , we emulate a session by invoking M Summ with a sequence of differing queries , Q = { q 1 , q 2 , ... , q m } , for which to generate the corresponding sequence of output sentences . I.e. , output sentence e out t is biased on query q t and the summary - so - far E t at time step t. We next describe the architecture 3 of M Summ , also illustrated in Figure 2 . Sentence encoding . The first step of the model is hierarchically encoding the sentences of the document set D to obtain contextualized representation c j for sentence s j \u2200j . A CNN ( Kim , 2014 ) encodes s j on the sentence level and then a bi - LSTM ( Huang et al , 2015 ) forms representation c j on the document level , given the CNN encodings . Query encoding . Additionally , at each time step t we prepare sentence+query representations c t j = c j CNN ( q t ) , i.e. , obtained by concatenating a sentence representation and the CNN - encoding of the current query . This sentence+query represen - Contextual sentence embeddings are concatenated to the current query embedding . The sentence+query representation is softly attended with a transformed query - focused MMR score , and a sentence selection distribution is obtained with a two - hop attention mechanism , considering a summary - so - far representation . A dual - reward mechanism , using the reference summaries and query , optimizes a policy to train the model for summary content quality and sentence - to - query resemblance . At inference time , an initial summary is generated with empty E in and q t - s , while for an expansion they are not empty . tation influences the relevance of a sentence with respect to the current input query . Query - MMR score weighting . MMR has been shown to be effective in MDS , where information repeats across documents . It aims to select a salient sentence for a summary , that is non - redundant to previous summary sentences . We extend standard MMR so that the importance of the sentence is in regards to both the document set and the query . Formally , the query - focused MMR function defines a score m t j for each s j at time step t as follows : m t j = \u03bb BISIM ( s j , D , q t ) \u2212 ( 1 \u2212 \u03bb ) max e Et SIM ( s j , e ) ( 1 ) BISIM ( s j , D , q t ) = \u03b2 SIM ( s j , D ) + ( 1 \u2212 \u03b2 ) SIM ( s j , q t ) ( 2 ) where \u03bb [ 0 , 1 ] balances salience and redundancy and \u03b2 [ 0 , 1 ] balances a sentence 's salience within its document set and its resemblance to the current query . SIM ( x , y ) measures the similarity of texts x and y , and D is a fully concatenated version of document set D. Following findings of , SIM computes cosine similarity between the two compared texts ' TF - IDF vectors . Redundancy to previous sentences is computed as the highest similarity - score against any of the previous sentences . We set \u03bb = 0.6 ( following Lebanoff et al , 2018 ) and \u03b2 = 0.5 ( see Appendix B.3 ) . The query - focused MMR scores are incorporated into M Summ by softly attending on the sentence representations with their respective translated query - focused MMR scores : \u00b5 t = softmax ( MLP ( m t ) ) ( 3 ) c t j = \u00b5 t j c t j ( 4 ) State representation . At time t , a representation z t of the summary - so - far is computed by applying an LSTM encoder on { c idx ( e in 1 ) , ... , c idx ( e in k ) , c idx ( e out 1 ) , ... , c idx ( e out t\u22121 ) } , i.e. , on the plain sentence representations of E t , where idx ( e ) is the index of sentence e. Then , a state representation g t considers z t and all sentence representations with the glimpse operation ( Vinyals et al , 2016 ) : a t j = v 1 tanh ( W 1\u0109 t j + W 2 z t ) ( 5 ) \u03b1 t = softmax ( a t ) ( 6 ) g t = j \u03b1 t j W 1\u0109 t j ( 7 ) where v 1 , W 1 and W 2 are model parameters , and a t represents the vector composed of a t j . Finally , a sentence s j at time t is assigned a selection probability softmax ( p t ) j such that : p t j = v2 tanh ( W 3\u0109 t j + W 4 g t ) if sj / Et \u2212 otherwise ( 8 ) where v 2 , W 3 and W 4 are model parameters . Reinforcement learning . As M Summ 's goal is to incrementally generate a query - assisted summary , it should strive to optimize ( 1 ) nonredundant salient - sentence extraction and ( 2 ) queryto - sentence similarity , that can be appraised with ROUGE ( Lin , 2004 ) and text - similarity metrics , respectively . A policy gradient - based RL approach ( Williams , 1992 ) allows optimizing on such nondifferentiable metrics . Specifically , we adopt the Advantage Actor Critic method ( Mnih et al , 2016 ) for policy learning , and a dual - reward procedure ( Pasunuru and Bansal , 2018 ) to alternate between the summary and query - similarity rewards . At time step t , for selected sentence e out t ( based on softmax ( p t ) ) , reward r t is computed and weighted into M Summ 's loss function . The reward function alternates , from one train batch to the next , between ROUGE \u2206 ( e out t , E t , R ) and QSIM ( e out t , q t ) . The former computes the ROUGE difference before adding e t to E t and after : ROUGE\u2206 ( e out t , Et , R ) = ROUGE ( ( Et \u222a e out t ) , R ) \u2212 ROUGE ( E t , R ) ( 9 ) A larger ROUGE \u2206 value implies that e t concisely adds more information onto E t , with respect to topic reference summaries R. We use ROUGE - 1 F 1 as the ROUGE function here . The querysimilarity reward function QSIM ( e out t , q t ) = avg ( SEMSIM ( e out t , q t ) , LEXSIM ( e out t , q t ) ) ( 10 ) computes an average of semantic and lexical similarities between the selected sentence and corresponding query . SEMSIM computes the cosine similarity between the average of word embeddings ( spaCy : Honnibal and Montani , 2021 ) of e out t and that of q t . For lexical similarity , LEXSIM ( e out t , q t ) = avg ( R p 1 ( e out t , q t ) , R p 2 ( e out t , q t ) , R p L ( e out t , q t ) ) ( 11 ) is the average of ROUGE - 1 , 2 and L precision scores between sentence and query . By alternating between the two rewards , we train a sentenceselection policy in M Summ to balance summary informativeness and adherence to queries . Overall system . Our M Summ model adopts its base architecture from ( for generic MDS ) . Chiefly , we modify their model for handling an input query - sequence and a sentence history , and employ a different summarization reward function . The query is incorporated in the sentence representation , in the new query - focused MMR function and in the dual - reward mechanism .", "entities": [[4, 5, "TaskName", "summarization"], [240, 241, "MethodName", "LSTM"], [311, 313, "TaskName", "sentence embeddings"], [571, 572, "HyperparameterName", "\u03b2"], [583, 584, "HyperparameterName", "\u03b2"], [599, 600, "DatasetName", "0"], [608, 609, "HyperparameterName", "\u03b2"], [610, 611, "DatasetName", "0"], [709, 710, "HyperparameterName", "\u03b2"], [749, 750, "MethodName", "softmax"], [751, 752, "DatasetName", "MLP"], [796, 797, "MethodName", "LSTM"], [906, 907, "HyperparameterName", "\u03b1"], [909, 910, "MethodName", "softmax"], [921, 922, "HyperparameterName", "\u03b1"], [970, 971, "MethodName", "softmax"], [1154, 1155, "MethodName", "softmax"], [1172, 1173, "MetricName", "loss"], [1363, 1365, "TaskName", "word embeddings"], [1521, 1522, "TaskName", "summarization"]]}
{"text": "We adopt and adjust the architecture in 3.2 for this subtask . Similar to M Summ , M Sugg selects input units one - by - one considering a history , with the main difference being the absence of query injection . Additionally , inputs and outputs are processed on the phrase - rather than the sentence level . Phrase and state representation . For the given document set , all noun phrases are extracted using a standard part - of - speech regular expression method ( Mihalcea and Tarau , 2004 ; Wan and Xiao , 2008 ) . We obtain document - level contextual phrase embeddings , c j for phrase \u03c1 j , with the CNN and bi - LSTM networks , and softly attend the embeddings with a standard MMR score : m t j = \u03bb SIM ( \u03c1 j , D ) \u2212 ( 1 \u2212 \u03bb ) max e Et SIM ( \u03c1 j , e ) ( 12 ) The MMR - based phrase representations then pass through the glimpse attention procedure , which culminates in the phrase probability distribution for selecting the next output phrase . Reinforcement learning . The policy in M Sugg is trained with a single reward function that measures how prominent the selected phrase is within the reference summaries , and how different it is from previously seen phrases . Formally , at time step t , the reward r t of selected phrase e out t is : r t = PF ( e out t , R ) \u2212 \u03b3 1 PFMAX ( e out t , E in , R ) \u2212 \u03b3 2 PFMAX ( e out t , E t \\ E in , R ) ( 13 ) PF ( e out t , R ) = avg r R ( avg w e out t TF ( w , r ) ) ( 14 ) PFMAX ( e out t , L , R ) = max e L PF ( e out t \u2229e , R ) ( 15 ) where TF ( w , r ) is the relative frequency of word w in reference summary r. Namely , PF computes the average term frequency of a phrase over its words and across the reference summaries , as an estimate of the phrase importance within the topic . PFMAX computes the highest PF against a list of phrases , which is used to lower the reward of a phrase that is redundant to phrases used earlier . Different weights are given to the PFMAX against the input history ( \u03b3 1 ) and that of the phrases output so far ( \u03b3 2 ) .", "entities": [[78, 81, "DatasetName", "part - of"], [122, 123, "MethodName", "LSTM"], [264, 265, "HyperparameterName", "\u03b3"], [278, 279, "HyperparameterName", "\u03b3"], [441, 442, "HyperparameterName", "\u03b3"], [453, 454, "HyperparameterName", "\u03b3"]]}
{"text": "Similarly to M Summ , we first pre - train the base model to get a warm start on embedding formation and salience detection . The reduced architecture of M Summ and M Sugg for pre - training are identical . We use the same DUC 2007 training data , with document sets and reference summaries , and additionally prepare three \" histories \" per topic : one empty and two non - empty . An empty history mimics generating a session 's initial list of suggested queries , while a non - empty history trains the model to consider previously known information . Training with two non - empty histories per topic prepares a model for varying informational states . These are curated from a generic summary ( from a trained M Summ model ) that is truncated at two random sentence - lengths between 1 and 12 . Overall , the model is trained on three versions of each topic , each time with a different history . Similarly to M Summ , validation is guided by the average normalized area under the recall curve . Here , the accumulating r t scores from Equation 13are used as the recall of the expanding suggested queries list . I.e. , a higher reward means better suggested queries are output earlier . The AUC is normalized with the total token - length of all suggested queries to mitigate for lengthy phrase extractions .", "entities": [[45, 47, "DatasetName", "DUC 2007"], [223, 224, "MetricName", "AUC"]]}
{"text": "We collect real user sessions via controlled crowdsourcing ( which provides high quality work , see Appendix D ) with the use of an INTSUMM web application 5 running either our M Summ + M Sugg models or the S 2 baseline algorithms , enabling a comparative assessment of the two systems . Notably , our algorithms have the low latency required for the interactive setting ( Attig et al , 2017 ) , i.e. , responding almost immediately . 6 Using the DUC 2006 INTSUMM test set , we prepared two complementing user sets of 20 topics , each with 10 of the topics to be run on our system and the other 10 on the baseline . We apply the evaluation metrics of Shapira et al ( 2021b ) area under the sessions ' ROUGE recall curves , in a common word - length interval across all sessions and topics , which demonstrates how fast salient information is exposed in sessions . ( 2 ) ROUGE F 1 at the initial summary and at 250 tokens , that indicate how effectively the interactive system can generate summaries at pre - specified , comparable lengths . ( 3 ) Manually assigned query - responsiveness score ( 1 to 5 scale ) , which expresses how well users think the system responded to their requests . And ( 4 ) manual UMUX - Lite ( Lewis et al , 2013 ) score for system usability ( effectiveness and ease of use ) , where 68 is considered \" acceptable \" and 80.3 is considered \" excellent \" . We also measure automatic query - responsiveness with QSIM . 7 We conducted two such comparative collection and assessment experiments , either employing M Summ configuration v or i , namely the best of the two configuration sets . In both cases , the M Sugg model used was set with \u03b3 1 = 0.5 and \u03b3 2 = 0.9 after some hyperparameter tuning ( Appendix B.4 ) . The first experiment ( with configuration v ) is described here , and the other in Appendix E.1 . We hired 6 qualified workers using the controlled crowdsourcing procedure , and collected 2 - 3 sessions per topic per system ( 111 total sessions ) . In the sessions , users explore their given topic by submitting queries with a common generic informational goal in mind ( Appendix D ) . Overall system assessment . Table 2 , presenting average scores over the collected sessions , shows that our system is significantly more effective for exposing salient information , as depicted in the first three rows . Users indicate a slight degradation in query - responsiveness of our system , consistent with QSIM scores ( row 4 - 5 ) . Note that the observed difference in QSIM scores , between simulations and user sessions , partly stems from the fact that they were computed over different sets of queries . The varying queries issued by the users in user sessions form a less stable query responsiveness comparison than the one in Table 1 , where QSIM scores are computed using consistent queries for all systems . Despite the gap in QSIM scores between our system and S 2 in Table 2 , the overall usability scores are slightly better ( last row ) . This may suggest that users appreciate the informativeness of the produced summary even when they are aware that the summary is less biased on their queries ; thus our system improves informativeness while still providing a favorable user experience .", "entities": [[83, 85, "DatasetName", "DUC 2006"], [319, 320, "HyperparameterName", "\u03b3"], [324, 325, "HyperparameterName", "\u03b3"]]}
{"text": "Datasets . The DUC 2006 and 2007 datasets were obtained according to the DUC website ( duc . nist.gov ) requirements . It was not possible for others to reconstruct the document sets and reference summaries of the dataset from the crowdsourcing tasks . The datasets are composed of new articles mainly from the late 1990s from large news outlets , compiled by NIST . All data exposed by our systems are directly extracted from those articles . For extraction , we do not intentionally add in any rules for ignoring or boosting certain information due to an opinion . Crowdsourcing . Due to the need for English speaking workers , a location filter was set on the Amazon Mechanical Turk ( https://www . mturk.com ) tasks for the US , UK and Australia . All tasks paid according to a $ 10 per hour wage , according to the estimated required time of each task . The payment was either paid per assignment , or as a combination with a bonus . Compute resources . Our M Summ and M Sugg models required between 2 and 20 hours of training ( usually around 4 hours ) , depending on the configuration . We trained on one NVIDIA GeForce GTX 1080 Ti GPU with 11 GB memory . The pretrained base model was trained once and reused in all subsequent training . Outputting at inference time is computationally cheap : M Summ runs upto about 1 second , but mostly in a few hundred milliseconds , and M Sugg runs upto about 7 seconds , but mostly in under 4 seconds . Training with a batch size of 8 used about 3 GB GPU memory for M Summ , and about 9 GB memory for M Sugg ( since there are many more input units per document set , i.e. , all noun phrases versus sentences ) .", "entities": [[3, 5, "DatasetName", "DUC 2006"], [275, 277, "HyperparameterName", "batch size"]]}
{"text": "To provide a warm start for training M Summ and M Sugg , a reduced version of the models , which is the same for both , is first pre - trained for generic extractive single - document summarization using the CNN / Daily Mail corpus ( Hermann et al , 2015 ) with about 287k samples , as proposed by Chen and Bansal ( 2018 ) . In this reduced model , \u0109 t j is replaced by c j in Equations 5 , 7 and 8 . Further - more , there is a single reward function for learning the policy , computed per selected sentence e out t as ROUGE - L F 1 w.r.t . the ( single ) reference summary 's sentence at index t. The reduced model pre - trains the full model for contextual sentence representation and for salient - sentence selection in the single - document generic setting . This allows training M Summ and M Sugg with a relatively small dataset for their final purposes .", "entities": [[37, 39, "TaskName", "document summarization"], [41, 45, "DatasetName", "CNN / Daily Mail"], [112, 115, "MetricName", "ROUGE - L"]]}
{"text": "Following , the pre - trained base model is the rnn - ext + RL model from Chen and Bansal ( 2018 ) , and is trained like in Lebanoff et al ( 2018 ) . Both M Summ and M Sugg are further trained on our adjusted DUC 2007 data using an Adam optimizer with a learning rate of 5e - 4 and no weight decay . A discount factor of 0.99 is used for the reinforcement learning rewards . The batch size was 8 . Training was halted once 30 consecutive epochs did not improve the validation score . The MMR function within our models uses TF - IDF vector cosine similarity for all SIM instances ( in Equations 1 and 12 ) . The TF - IDF vectorizer is initialized with the document set on which the MMR score is computed . As is commonly practiced , selection of an output sentence / phrase e out t is done by sampling probability distribution p t ( in Equation 8 ) at train time , and by extracting the maximum scoring sentence / phrase at inference time . The MLP in Equation 3 transforms the MMR score with a feed - forward network with one - hidden layer of dimension 80 following .", "entities": [[48, 50, "DatasetName", "DUC 2007"], [53, 54, "MethodName", "Adam"], [54, 55, "HyperparameterName", "optimizer"], [57, 59, "HyperparameterName", "learning rate"], [65, 67, "MethodName", "weight decay"], [82, 84, "HyperparameterName", "batch size"], [191, 192, "DatasetName", "MLP"]]}
{"text": "Model configurations . The architecture of the M Summ model and its training allowed for much creativity in the configuration process . Other than the combinations mentioned in the paper in Table 1 , we also experimented with other components . We list here many of the experiments , without formal results . Anecdotes are taken by looking at validation scores and some eyeballing . ( 1 ) The \u03b2 value in the query - focused MMR function in Equation 2 , that impacts the weight of the query on a sentence versus the document set on the sentence . We tried out a few \u03b2 values and mainly noticed that a value of 0.5 kept validation results more stable across configurations , or kept training time shorter . In our experiments , to cancel out this component ( both at training and inference time ) , we simply set \u03b2 = 1 so that the query is not considered . ( 2 ) Different summary reward functions . ROUGE \u2206 recall ( instead of F 1 ) was also a good alternative , but gave somewhat less stable results across configurations . ROUGE ( not as \u2206 ) was also less stable with recall and F 1 , and gave too short and irrelevant sentences with precision . We also tried sentence level ROUGE - L , like in , eventually outputting sentences that were much less compliant to queries . ( 3 ) Using only the query similarity reward instead of the dual reward mechanism worked surprisingly well . This may be due to the queries on which the model was trained on . These queries were very relevant to the gold reference summaries , hence possibly implicitly providing a strong signal to salient sentences within the document set . Still , this was less productive than our final choice of reward . ( 4 ) Adding training data ( additional DUC MDS datasets ) did not impact the results . Importantly , since DUC 2007 is most similar to the test DUC 2006 set , it seems to be more beneficial to include DUC 2007 in the training set . ( 5 ) We also tried representing the query in the input by concatenating it 's raw text to each input sentence before get the sentence representations . ( 6 ) To represent the sentences , we also tried using average w2v vectors ( Honnibal and Montani , 2021 ) and Sentence - BERT ( Reimers and Gurevych , 2019 ) instead of the CNN network . These did not show any apparent improvements , and were notably expensive in terms of execution time . ( 7 ) For the sentence similarity in the query - MMR component , we tried w2v and Sentence - BERT representations instead of TF - IDF vectors . Similarly to ( 6 ) , they did not show improvements over using TF - IDF , and were very time - costly . ( 8 ) Instead of the dual - reward mechanism that alternates between the two rewards from batch to batch , we also considered using a weighted average of the two rewards , consistently over all batches . Further experimentation is required on this technique for a more conclusive judgment . Queries used for training . The queries used for training the M Summ model can affect the way it learns to respond to a query . Seemingly , the most natural approach would be to train the model as close as possible to the model 's use at inference time . This would mean training M Summ with queries from real sessions . However , a session 's queries are dependent on outputs previously produced by the used system . It is therefore not certain that the sequence of queries from a different system 's usage would necessarily benefit the training process when compared to a synthesized sequence of queries . I.e. , it 's not actually possible to train with \" real sessions \" in a conventional way . Also , as stated in 3.3 , the synthetic queries we eventually used direct the model to select salient sentences , which can support our dual - objectives : to get a sentence that is both globally salient to the topic , as well as responsive to the query . We tried training on other query types , synthesized with various keyphrase extraction techniques , and found that our final choice of queries more consistently gave good results overall . Sentence length . We segmented the sentences in the document sets with the NLTK 8 sentence tokenizer , and removed sentences that contain quotes in them or do not end with a period . During training we did not constrain the input sentences in any way . Some of the configuration experiments described above were done to check how the configuration might influence the length of the selected sentences . The best configurations , including the one we eventually used in our tests , tended to output somewhat longer sentences . Very long sentences are usually tedious for human readers , and we hence limited the sentences to 30 tokens at inference time . We found that this length constraint caused a slight degradation in simulation score results of our models , however still gave superior informativeness results compared to the baseline system . Initial summary length . Sentences are accumulated until surpassing 75 tokens . Therefore summaries are not shorter than 75 tokens , but mostly not much longer than that .", "entities": [[69, 70, "HyperparameterName", "\u03b2"], [105, 106, "HyperparameterName", "\u03b2"], [150, 151, "HyperparameterName", "\u03b2"], [224, 227, "MetricName", "ROUGE - L"], [336, 338, "DatasetName", "DUC 2007"], [344, 346, "DatasetName", "DUC 2006"], [356, 358, "DatasetName", "DUC 2007"], [416, 417, "MethodName", "BERT"], [468, 469, "MethodName", "BERT"], [743, 745, "TaskName", "keyphrase extraction"]]}
{"text": "Model configurations . We experimented with different configurations and hyper - parameter finetuning in the M Sugg model as well . Tuning was performed in accordance to the validation scores and generic keyphrase extraction scores on the MK - DUC - 01 multi - document keyphrase extraction dataset of Shapira et al ( 2021a ) . ( 1 ) In the reward function in Equation 13 , we set \u03b3 1 = 0.5 and \u03b3 2 = 0.9 , i.e. , the preceding output phrases are more strongly accounted for than the phrases in the session history . We tested several values between 0 and 1 for both hyper - parameters . ( 2 ) We implemented altered versions of the reward function in Equation 13 . Instead of phrase unigram - level frequency , we tried computing the full phrase frequency and computing partial phrase frequency , i.e. , a maximal phrase template match within a reference summary . All functions tested were adequate overall , though our final choice of reward function was closest to the keyphrase extraction task unigram overlap metric , and gave best results overall . ( 3 ) We also attempted noun phrase extraction with the spaCy 9 noun chunker and named entity recognizer . This combined approach misses some noun phrases within the text , but mainly is also more computationally heavy than the simple POS regex search that we use .", "entities": [[32, 34, "TaskName", "keyphrase extraction"], [45, 47, "TaskName", "keyphrase extraction"], [69, 70, "HyperparameterName", "\u03b3"], [74, 75, "HyperparameterName", "\u03b3"], [103, 104, "DatasetName", "0"], [178, 180, "TaskName", "keyphrase extraction"]]}
{"text": "Controlled crowdsourcing protocol . We followed the controlled crowdsourcing protocol of Shapira et al ( 2021b ) , which includes three steps : ( 1 ) a trap task for finding qualified workers ; ( 2 ) practice tasks for explaining the interface and the purpose , as well as reiterating the generic information goal ( see below ) during exploration ; ( 3 ) the session collection tasks . We used the Amazon Mechanical Turk HITs prepared by Shapira et al ( 2021b ) . Process cost . We paid $ 0.40 for a trap task assignment , with 400 assignments released , and $ 0.90 for a practice task assignment , with 28 assignments completed . The session collection assignment paid $ 0.70 , and a bonus mainly according to the length of interaction and additional comments provided . The bonus was between $ 0.15 and $ 0.35 . A total of 111 sessions were recorded from 6 high quality workers . The full process cost about $ 385 in total ( including the Mechanical Turk fees ) for the experiment including configuration v in Table 1 . The second round of experiments done on another variant of our system ( configuration i ) also included 28 practice tasks and compiled 10 final workers for a total of 180 collected sessions . Bonuses ranged from $ 0.10 and $ 0.40 on the session collection task . The full process cost of the second experiment was about $ 475 in total ( including the Mechanical Turk fees ) . Session collection data preparation . We used the same 20 test topics as Shapira et al ( 2021b ) , and created 2 batches of tasks . For the first batch , in alternating order of topics , 10 topics were paired with our system , and the other 10 were paired with the S 2 baseline . The other batch consisted of the complementing topic - system pairings . The workers were assigned a batch to work on such that half of the workers would work on each batch . User informational goal . Since all sessions on a topic are evaluated against the same reference summaries , it is important that users aim to explore similar information . Following Shapira et al ( 2021b ) , during practice tasks all users received a common informational goal to follow , so that the sessions are comparable . The emphasized description was : \" produce an informative summary draft text which a journalist could use to best produce an overview of the topic \" . Sessions filtering . In the first experiment , we filtered out 7 sessions that accumulated less than 250 tokens ( from 2 different workers ) . In the second experiment , 9 of the 10 workers completed at least 19 of the 20 topics One worker completed only 3 tasks and we disregarded those sessions . We also threw away 9 sessions that accumulated less than 250 tokens . INTSUMM user interface . We used the same user interface developed by Shapira et al ( 2021b ) with a small change to enable suggested query list updates after each interaction ( the interface was designed for the baselines , where the suggestedquery list is static ) . To refrain from any possible user experience bias , we made the UI change as least apparent as possible . System response time . M Summ is able to generate summaries mostly in under a second , and M Sugg prepares the list in a few seconds . The summary expansion is hence presented to the user almost immediately after query submission , and the suggested queries list is shown shortly afterwords , before the user finishes reading the expansion . The small delay in suggested query updating is hence almost unnoticed . The baseline summarizer responds similarly fast to M Summ , making response - time difference unperceivable between the systems . User feedback . Many of the users provided feedback about the session collection tasks after finishing their assignment batch . The overall impression was that there was no strong preference for either system . For example , one user wrote : \" I did not discern a consistent difference between the two systems that would result in having a clear preference . \" This kind of comment was repeated by several users . Generally , there were no explicit comments about the difference in quality of the summary outputs , and topics were mostly scored or commented on similarly between the two systems since the complexity of the topic influenced the ability of the systems to comply to the user . A comment in favor of updating suggested queries during interaction said : \" It was nice to have a new list as you progressed through the task , it helped me think of where to go next if I got stuck ... \" This specific comment was written by a user that explored topics quite deeply . On the other hand , a user that explored more shallow liked that used suggested queries in the static list were marked : \" I did notice ... the red font color on the used queries . That was helpful . \" It therefore seems that updating suggested queries are more useful for lengthy exploration , but for quick navigation , the static list might naturally be enough . We conducted two comparative session collection and analysis experiments , one using M Summ model configuration v ( from Table 1 ) , as presented in 5.3 and Table 2 , and another with M Summ model configuration i. As explained in 5.2 , these two configurations performed best , on simulations , out of their respective configuration sets . We show here results of the second experiment , where we used M Summ model configuration i , with the same M Sugg model as in the first experiment . The S 2 baseline was similarly used for comparison . We also kept the same AUC length limits ( 106 to 250 tokens ) for easy comparability to Table 2 . Table 3 shows the results . Here too , while less substantially , informativeness is improved with our system without significantly harming the user experience . Overall , it seems that users were somewhat more satisfied with the INTSUMM system that uses M Summ configuration v than configuration i. Interestingly , it seems the users may have appreciated the slightly better informativeness of configuration v even if the query - responsiveness was not as good as in configuration i , as shown through the QSIM score . In addition , we see that absolute manual scores in Table 3 are lower than in Table 2 , but trends are generally similar . It is common that scaling of manually supplied scores can fluctuate ( e.g. Gillick and Liu , 2010 ) . Figures 3 and 4 show the averaged ( per topic and then over all topics ) recall curves of the collected sessions in the experiment described in 5.3 and above , respectively . The x - axis is the accumulating token - length of the session , and the y - axis is the ROUGE - 1 recall . The points on the curve are the average interpolated values from all the sessions . The vertical dashed lines are the intersecting bounds of the sessions , from 106 tokens to 250 . The area under the curve ( AUC ) is computed for each of the curves , and reported in the first row of Tables 2 and 3 . The higher AUC scores obtained from the recall curves of our models , compared to those of the S 2 baseline , highlight the ability to expose more salient information earlier in the session .", "entities": [[1017, 1018, "MetricName", "AUC"], [1263, 1264, "MetricName", "AUC"], [1287, 1288, "MetricName", "AUC"]]}
{"text": "The normalized AUC score for the validation metric ( explained in 3.3 ) is computed over the recall curve produced from the accumulating summary expansions . Each point on the curve marks an accumulating token - length ( x - axis ) and an accumulating recall score ( y - axis ) of an interactive state , as depicted in Figures 3 and 4 ( although these figures show the averaged session recall curves with bounds , whereas during validation the curve is for a single session and there are no bounds set ) . By computing the area under the full curve , and dividing by the full length , the normalized AUC score is obtained . The normalization gives an approximate absolute value that can be compared at different lengths ( although at large length differences this is not comparable due to the decaying slope of the curve ) . The manual query - responsiveness score , reported in Tables 2 and 3 , is obtained by asking users , at the end of a session , \" During the interactive stage , how well did the responses respond to your queries ? \" , for which they rate on a 1 - to - 5 scale . The scores are averaged over the topic and then over all topics . This follows the evaluation defined in Shapira et al ( 2021b ) . The UMUX - Lite score ( Lewis et al , 2013 ) , reported in Tables 2 and 3 , is obtained by asking users to rate ( 1 - to - 5 ) two statements at the end of a session : ( 1 ) \" The system 's capabilities meet the need to efficiently collect useful information for a journalistic overview \" and ( 2 ) \" The system is easy to use \" . The first question refers to the users ' informational goal that they received , in order to follow a consistent objective goal during their exploration . The final score is a function of these two scores , and is used as a replacement for the popular SUS metric ( Brooke , 1996 ) ( with a much longer questionnaire ) , to which it shows very high correlation , thus offering a cheaper alternative . This also follows the evaluation defined in Shapira et al ( 2021b ) . All confidence intervals in Tables 1 , 2 and 3 are computed as margins - of - error , on the topiclevel , over the standard error of the mean with 95 % confidence . 10 The token - length values in", "entities": [[2, 3, "MetricName", "AUC"], [113, 114, "MetricName", "AUC"]]}
{"text": "A policy gradient - based reinforcement learning approach ( Williams , 1992 ) allows optimizing on nondifferentiable metrics , and eliminates the exposure bias that occurs with traditional training methods , like cross - entropy , on generation tasks ( Ranzato et al , 2016 ) . Specifically , we use the Advantage Actor Critic ( A2C ) policy gradient training method . See technical explanations in the appendix of ( Chen and Bansal , 2018 ) . At a high level , an output reward ( subtracted by a baseline reward - computed on a version of the model without MMR attention ) is used to weight the output selection in the loss function . In so , outputs with higher rewards increase the likelihood of those outputs and lower rewards decrease the likelihood . Since the reward function is not differentiable , it is used as a weight on the probability of the selected output , which is then given to the loss function .", "entities": [[56, 57, "MethodName", "A2C"], [113, 114, "MetricName", "loss"], [164, 165, "MetricName", "loss"]]}
{"text": "As a pilot study , we consider the task of binary text classification . The training set is denoted as D train = { ( x 1 , l 1 ) , ... , ( x n , l n ) } , where x i is the i - th example and l i { 0 , 1 } is the corresponding label . We fit a model f \u2236 ( x ; \u03b8 ) \u21a6 { 0 , 1 } with parameters \u03b8 on the training data . A textual perturbation is a transformation g \u2236 ( x ; \u03b2 ) x * that injects a specific type of noise into an example x with parameters \u03b2 and the resulting perturbed example is x * . We design several experiment settings ( Table 1 ) to answer our research questions . Experiment 0 in Table 1 is the standard learning setup , where we train and evaluate a model on the original dataset . Below we detail other experiment settings .", "entities": [[11, 13, "TaskName", "text classification"], [56, 57, "DatasetName", "0"], [74, 75, "HyperparameterName", "\u03b8"], [78, 79, "DatasetName", "0"], [84, 85, "HyperparameterName", "\u03b8"], [101, 102, "HyperparameterName", "\u03b2"], [118, 119, "HyperparameterName", "\u03b2"], [144, 145, "DatasetName", "0"]]}
{"text": "Robustness . We apply the perturbations to test examples and measure the robustness of model to said perturbations as the decrease in accuracy . In Table 1 , Experiment 1 is related to robustness measurement , where we train a model on unperturbed dataset and test it on perturbed examples . We denote the test accuracy of a model f ( \u22c5 ) on examples perturbed by g ( \u22c5 ) in Experiment 1 as A 1 ( f , g , D * test ) . Similarly , the test accuracy in Experiment 0 is A 0 ( f , D test ) . Consequently , the robustness is calculated as the difference of test accuracies : robustness ( f , g , D ) = A 1 ( f , g , D * test ) \u2212A 0 ( f , D test ) . ( 1 ) Models usually suffer a performance drop when encountering perturbations , therefore the robustness is usually negative , where lower values indicate decreased robustness . Improvement by Data Augmentation ( Post Augmentation \u2206 ) . To improve robust accuracy ( Tu et al , 2020 ) ( i.e. , accuracy on the perturbed test set ) , it is a common practice to leverage data augmentation ( Li and Specia , 2019 ; Min et al , 2020 ; Tan and Joty , 2021 ) . We simulate the data augmentation process by appending perturbed data to the training set ( Experiment 2 of Table 1 ) . We calculate the improvement on performance after data augmentation as the difference of test accuracies : \u2206 post_aug ( f , g , D ) = A 2 ( f , g , D * test ) \u2212A 1 ( f , g , D * test ) . ( ) 2 where A 2 ( f , g , D * test ) denotes the test accuracy of Experiment 2 . \u2206 post_aug is the higher the better . Learnability . We want to compare perturbations in terms of how well the model learns to identify them with a small amount of evidence . We cast learnability estimation as a perturbation classification task , where a model is trained to identify the perturbation in an example . We define that the learnability estimation consists of three steps , namely \u2460 assigning random labels , \u2461 perturbing with probabilities , and \u2462 estimating model performance . Below we introduce the procedure and intuition for each step . This estimation framework is further grounded in concepts from the causality literature in Section 3 , which justifies our motivations . We summarize our estimation approach formally in Algorithm 1 ( Appendix A ) . \u2460 Assigning Random Labels . We randomly assign pseudo labels to each training example regardless of its original label . Each data point has equal probability of being assigned to positive ( l \u2032 = 1 ) or negative ( l \u2032 = 0 ) pseudo label . This results in a randomly labeled dataset D \u2032 train = { ( x 1 ; l \u2032 1 ) , ... , ( x n , l \u2032 n ) } , where L \u2032 \u223c Bernoulli ( 1 , 0.5 ) . In this way , we ensure that there is no difference between the two pseudo groups since the data are randomly split . \u2461 Perturbing with Probabilities . We apply the perturbation g ( \u22c5 ) to each training example in one of the pseudo groups ( e.g. , l \u2032 = 1 in Algorithm 1 ) 1 . In this way , we create a correlation between the existence of perturbation and label ( i.e. , the perturbation occurrence is predictive of the label ) . We control the perturbation probability p [ 0 , 1 ] , i.e. , an example has a specific probability p of being perturbed . This results in a perturbed training set D \u2032 * train = { ( x * 1 , l \u2032 1 ) , ... , ( x * n , l \u2032 n ) } , where the perturbed example x * i is : Z \u223c U ( 0 , 1 ) , \u2200i { 1 , 2 , ... , n } x * i = \u23a7 \u23aa \u23aa \u23a8 \u23aa \u23aa \u23a9 g ( x i ) l \u2032 i = 1 z < p , x i otherwise . ( ) 3 Here Z is a random variable drawn from a uniform distribution U ( 0 , 1 ) . Due to randomization in the formal step , now the only difference between the two pseudo groups is the occurrence of perturbation . \u2462 Estimating Model Performance . We train a model on the randomly labeled dataset with per - 1 Because the training data is randomly split into two pseudo groups , applying perturbations to any one of the groups should yield same result . We assume that we always perturb into the first group ( l \u2032 = 1 ) hereafter . turbed examples . Since the only difference between the two pseudo groups is the existence of the perturbation , the model is trained to identify the perturbation . The original test examples D test are also assigned random labels and become D \u2032 test . We perturb all of the test examples in one pseudo group ( e.g. , l \u2032 = 1 , as in step 2.1 ) to produce a perturbed test set D \u2032 * test . Finally , the perturbation learnability is calculated as the difference of accuracies on D \u2032 * test and D \u2032 test , which indicates how much the model learns from the perturbation 's co - occurrence with pseudo label : learnability ( f , g , p , D ) = A 3 ( f , g , p , D \u2032 * test ) \u2212A 4 ( f , g , p , D \u2032 test ) . ( 4 ) A 4 ( f , g , p , D \u2032 * test ) and A 3 ( f , g , p , D \u2032 test ) are accuracies measured by Experiment 4 and 3 of Table 1 , respectively . We observe that the learnability depends on perturbation probability p. For each modelperturbation pair , we obtain multiple learnability estimates by varying the perturbation probability ( Figure 3 ) . However , we expect that learnability of the perturbation ( as a concept ) should be independent of perturbation probability . To this end , we use the log AU C ( area under the curve in log scale ) of the p \u2212 learnability curve ( Figure 3 ) , termed as \" average learnability \" , which summarizes the overall learnability across different perturbation probabilities p 1 , ... , p t : avg_learnability ( f , g , D ) \u2236= log AU C ( { ( p i , learnability ( f , g , p i , D ) ) | i { 1 , 2 , ... , t } } ) . ( 5 ) We use log AU C rather than AU C because we empirically find that the learnability varies substantially between perturbations when p is small , and a log scale can better capture this nuance . We also introduce learnability at a specific perturbation probability ( Learnability @ p ) as an alternate summary metric and provide a comparison of this metric against log AU C in Appendix D.", "entities": [[22, 23, "MetricName", "accuracy"], [55, 56, "MetricName", "accuracy"], [91, 92, "MetricName", "accuracy"], [94, 95, "DatasetName", "0"], [97, 98, "DatasetName", "0"], [139, 140, "DatasetName", "0"], [176, 178, "TaskName", "Data Augmentation"], [187, 188, "MetricName", "accuracy"], [198, 199, "MetricName", "accuracy"], [213, 215, "TaskName", "data augmentation"], [238, 240, "TaskName", "data augmentation"], [264, 266, "TaskName", "data augmentation"], [324, 325, "MetricName", "accuracy"], [502, 503, "DatasetName", "0"], [645, 646, "DatasetName", "0"], [712, 713, "DatasetName", "0"], [772, 773, "DatasetName", "0"]]}
{"text": "We identify learnability as a causal estimand . In causality , the term \" identification \" refers to the process of moving from a causal estimand ( Average Treatment Effect , ATE ) to an equivalent statistical estimand . We show that the difference of accuracies on D \u2032 * test and D \u2032 test is actually a causal estimand . We define the outcome Y of a test example x i as the correctness of the predicted label : Y i ( 0 ) \u2236= 1 { f ( x i ) = l \u2032 i } . ( 6 ) where 1 { \u22c5 } is the indicator function . Similarly , the outcome Y of a perturbed test example x * i is : Y i ( 1 ) \u2236= 1 { f ( x * i ) = l \u2032 i } . ( 7 ) According to the definition of Individual Treatment Effect ( ITE , see Equation 9of Appendix B ) , we have IT E i = 1 { f ( x * i ) = l \u2032 i } \u22121 { f ( x i ) = l \u2032 i } . We then take the average over all the perturbed test examples ( half of the test set ) 3 . This is our Average Treatment Effect ( ATE ) : AT E = E [ Y ( 1 ) ] \u2212 E [ Y ( 0 ) ] = E [ 1 { f ( x * ) = l \u2032 } ] \u2212 E [ 1 { f ( x ) = l \u2032 } ] = P ( f ( x * ) = l \u2032 ) \u2212 P ( f ( x ) = l \u2032 ) = A ( f , g , p , D \u2032 * test ) \u2212 A ( f , g , p , D \u2032 test ) . ( 8 ) Perturbation Example Sentence None His quiet and straightforward demeanor was rare then and would be today . duplicate_punctuations His quiet and straightforward demeanor was rare then and would be today .. butter_fingers_perturbation His quiet and straightforward demeanor was rarw then and would be today . shuffle_word quiet would and was be and straightforward then demeanor His today . rare random_upper_transformation His quiEt and straightForwARd Demeanor was rare TheN and would be today . insert_abbreviation His quiet and straightforward demeanor wuz rare then and would b today . whitespace_perturbation His quiet and straightforward demean or wa s rare thenand would be today . visual_attack_letters Hi\u1e69 q\u1ee7i\u1ebdt \u1ea7\u057cd str\u1e01igh\u1e6d\u1e1forw\u1eb3r\u0221 d\u0511meano\u0155 w\u0203\u1e63 r\u0227re t\u1e2ben and wou\u1d85d \u03f8\u04d9 t\u0ead\u1e0f\u1ea7\u0233 . leet_letters His qui3 t and strai9htfor3ard d3m3an0r 3as rar3 t43n and 30uld 63 t0da4 . where A ( f , g , p , D ) is the accuracy of model f ( \u22c5 ) trained with perturbation g ( \u22c5 ) at perturbation probability p on test set D. Therefore , we show that ATE is exactly the difference of accuracy on the perturbed and unperturbed test sets with random labels . And the difference is learnability according to Equation 4 . We discuss another means of identification of ATE in Appendix C , based on the prediction probability . We compare between the probability - based and accuracy - based metrics there . We find that our accuracy - based metric yields better resolution , so we report this metric in the main text of this paper .", "entities": [[83, 84, "DatasetName", "0"], [245, 246, "DatasetName", "0"], [475, 476, "MetricName", "accuracy"], [508, 509, "MetricName", "accuracy"], [556, 557, "MetricName", "accuracy"], [566, 567, "MetricName", "accuracy"]]}
{"text": "Potential Impacts . Our findings seem intuitive but are non - trivial . The NLP models were not trained on perturbed examples when measuring robustness , but still they display a strong correlation with perturbation learnability . Understanding these findings are important for a more principled evaluation of and control over NLP models . Specifically , the learnability metric complements to the evaluation of newly designed perturbations by revealing model weaknesses in a clean setup . Reducing perturbation learnability is promising for improving robustness of models . Contrastive learning ( Gao et al , 2021 ; Yan et al , 2021 ) that pulls the representations of the original and perturbed text together , makes it difficult for the model to identify the perturbation ( reducing learnability ) and thus may help improve robustness . Perturbation can also be viewed as injecting spurious feature into the examples , so the learnability metric also helps to interpret robustness to spurious correlation ( Sagawa et al , 2020 ) . Moreover , learnability may facilitate the development of model architectures with explicit inductive biases ( Warstadt and Bowman , 2020 ; to avoid sensitivity to noisy perturbations . Grounding the learnability within the causality framework inspires future researchers to incorporate the causal perspective into model design ( Zhang et al , 2020 ) , and make the model robust to different types of perturbations . Limitations . In this work , we focus on the robust accuracy ( Section 2.1 ) , which is accuracy on the perturbed test set . We do not assume that the test accuracy of the original test set , a.k.a in - distribution accuracy , is invariant invariant against training with augmentation or not . It would be interesting to investigate the trade - off between robust accuracy and in - distribution accuracy in the future . We also note that this work has not established that the relationship between learnability and robustness is causal . This could be explored with other approaches in causal inference for deconfounding besides simulation on randomized control trial , such as working with real data but stratifying it ( Frangakis and Rubin , 2002 ) , to bring the learnability experiment closer to more naturalistic settings . Although we restrict to balanced , binary classification for simplicity in this pilot study , our framework can also be extended to imbalanced , multi - class classification . We are aware that computing average learnability is expensive for large models and datasets , which is further discussed in Section 8 . We provide a greener solution in Appendix D. We could further verify our assumptions for perturbations with a user study ( Moradi and Samwald , 2021 ) which investigates how understandable the perturbed texts are to humans .", "entities": [[87, 89, "MethodName", "Contrastive learning"], [96, 99, "DatasetName", "Yan et al"], [244, 245, "MetricName", "accuracy"], [252, 253, "MetricName", "accuracy"], [266, 267, "MetricName", "accuracy"], [277, 278, "MetricName", "accuracy"], [301, 302, "MetricName", "accuracy"], [306, 307, "MetricName", "accuracy"], [338, 340, "MethodName", "causal inference"], [401, 405, "TaskName", "multi - class classification"]]}
{"text": "Algorithm 1 Learnability Estimation Input : training set D train = { ( x 1 , l 1 ) , ... , ( x n , l n ) } , test set D test = { ( x n+1 , l n+1 ) , ... , ( x n+m , l n+m ) } , D = D train \u222a D test , model f \u2236 ( x ; \u03b8 ) \u21a6 { 0 , 1 } , perturbation g \u2236 ( x ; \u03b2 ) x * , perturbation probability p Output : learnability ( f , g , p , D ) 1 : // \u2460 assigning random labels 2 : Initialize an empty dataset D \u2032 3 : for i in { 1 , 2 , ... , n + m } do 4 : l \u2032 i randint [ 0 , 1 ] 5 : D \u2032 D \u2032 \u222a { ( x i , l \u2032 i ) } 6 : end for 7 : // \u2461 perturbing with probabilities 8 : Initialize an empty dataset D \u2032 * 9 : for i in { 1 , 2 , ... , n + m } do 10 : z rand ( 0 , 1 ) 11 : x * i x i 12 : if l \u2032 i = 1 z < p then 13 : x * i g ( x i ) 14 : end if 15 : D \u2032 * D \u2032 * \u222a { ( x * i , l \u2032 i ) D \u2032 train , D \u2032 test D \u2032 [ 1 \u2236 n ] , D \u2032 [ n + 1 \u2236 n + m ] 19 : D \u2032 * train , D \u2032 * test D \u2032 * [ 1 \u2236 n ] , D \u2032 * [ n+1 \u2236 n+m ] 20 : fit the model f ( \u22c5 ) on D \u2032 * train 21 : A ( f , g , p , D \u2032 * test ) f ( \u22c5 ) accuracy on D \u2032 * test 22 : A ( f , g , p , D \u2032 test ) f ( \u22c5 ) accuracy on D \u2032 test 23 : return A ( f , g , p , D \u2032 * test ) \u2212 A ( f , g , p , D \u2032 test )", "entities": [[70, 71, "HyperparameterName", "\u03b8"], [74, 75, "DatasetName", "0"], [85, 86, "HyperparameterName", "\u03b2"], [144, 145, "DatasetName", "0"], [207, 208, "DatasetName", "0"], [350, 351, "MetricName", "accuracy"], [374, 375, "MetricName", "accuracy"]]}
{"text": "In Section 3.2 , we propose an accuracy - based identification of ATE . Now we discuss another probability - based identification and compare between them . We can also define the outcome Y of a test example x i as the predicted probability of ( pseudo ) true label given by the trained model f ( \u22c5 ) : Y i ( 0 ) \u2236= P f ( L \u2032 = l \u2032 i | X = x i ) ( 0 , 1 ) . ( 11 ) Similarly , the performance outcome Y of a perturbed test data point x * i is : Y i ( 1 ) \u2236= P f ( L \u2032 = l \u2032 i | X = x * i ) ( 0 , 1 ) . ( 12 ) For example , for a test example ( x i , l \u2032 i ) which receives treatment ( l \u2032 i = 1 ) , the trained model f ( \u22c5 ) predicts its label as 1 with only a small probability 0.1 before treatment ( it has not been perturbed yet ) , and 0.9 after treatment . So the Individual Treatment Effect ( ITE , see Equation 9 ) of this example is calculated as IT E i = Y i ( 1 ) \u2212 Y i ( 0 ) = 0.9 \u2212 0.1 = 0.8 . We then take an average over all the perturbed test examples ( half of the test set ) as Average Treatment Effect ( ATE , see Equation 10 ) , which is exactly the learnability of a perturbation for a model . To clarify , the two operands in Equation 10 are defined as follows : E [ Y ( 1 ) ] \u2236= P ( f , g , p , D \u2032 * test ) . ( 13 ) It means the average predicted probability of ( pseudo ) true label given by the trained model f ( \u22c5 ) on the perturbed test set D \u2032 * test . E [ Y ( 0 ) ] \u2236= P ( f , g , p , D \u2032 test ) . ( 14 ) Similarly , this is the average predicted probability on the randomly labeled test set D \u2032 test . Notice that the accuracy - based definition of outcome Y ( Equation 6 ) can also be written in a similar form to the probability - based one ( Equation 11 ) : Y i ( 0 ) \u2236= 1 { f ( x i ) = l \u2032 i } = 1 { P f ( L \u2032 = l \u2032 i | X = x i ) > 0.5 } { 0 , 1 } . ( 15 ) because the correctness of the prediction is equal to whether the predicted probability of true ( pseudo ) label exceeds a certain threshold ( i.e. , 0.5 ) . The major difference is that , accuracy - based IT E is a discrete variable falling in { \u22121 , 0 , 1 } , while probability - based IT E is a continuous one ranging from - 1 to 1 . For example , if a model learns to identify a perturbation and thus changes its prediction from wrong ( before perturbation ) to correct ( after perturbation ) , accuracy - based IT E will be 1 \u2212 0 = 1 while probability - based IT E will be less than 1 . That is to say , accuracy - based AT E tends to vary more drastically than probability - based if inconsistent predictions occur more often , and thus can better capture the nuance of perturbation learnability . Empirically , we find that accuracy - based average learnability varies greatly ( \u03c3 = 0.375 , Table 4 ) and thus can better distinguish between different model - perturbation pairs than probabilitybased one ( \u03c3 = 0.288 , Table 4 ) . As a result , we choose accuracy - based ATE as the primary measurement of learnability in this paper .", "entities": [[7, 8, "MetricName", "accuracy"], [63, 64, "DatasetName", "0"], [82, 83, "DatasetName", "0"], [130, 131, "DatasetName", "0"], [229, 230, "DatasetName", "0"], [354, 355, "DatasetName", "0"], [395, 396, "MetricName", "accuracy"], [428, 429, "DatasetName", "0"], [465, 466, "DatasetName", "0"], [508, 509, "MetricName", "accuracy"], [522, 523, "DatasetName", "0"], [573, 574, "MetricName", "accuracy"], [582, 583, "DatasetName", "0"], [602, 603, "MetricName", "accuracy"], [639, 640, "MetricName", "accuracy"], [683, 684, "MetricName", "accuracy"]]}
{"text": "Inspired by Precision @ K in Information Retrieval ( IR ) , we propose a similar metric dubbed Learnability @ p , which is the learnability of a perturbation for a model at a specific perturbation probability p. We are primarily interested in whether a selected p can represent the learnability over different perturbation probabilities and correlates well with robustness and post data augmentation \u2206. We calculate the standard deviation ( \u03c3 ) of Learnability @ p and average learnability ( log AU C ) over all model - perturbation pairs to measure how well it can distinguish between different models and perturbations . Table 4 shows that average learnability is more diversified than all Learnability @ p and diversity ( \u03c3 ) peaks at p = 0.01 for accuracybased / probability - based measurement . Accuracybased Learnability @ p is generally more diversified across models and perturbations than its counterpart . To investigate the strength of the correlations , we also calculate Spearman \u03c1 between accuracy - based / probability - based learnability @ p vs. average learnability / robustness / post data augmentation \u2206 over all model - perturbation pairs . Table 4 shows that generally average learnability has stronger correlation than Learnability @ p. Correlations with both robustness and post data augmentation \u2206 peak at p = 0.02 for accuracybased / probability - based measurements , and the correlations with average learnability ( 0.816*/0.886 * ) are also strong at these perturbation probabilities . Overall , Learnability @ p with higher standard deviation correlates better with average learnability , robustness and post data augmentation \u2206. Our analysis shows that if p is carefully selected by \u03c3 , Learnability @ p is also a promising metric , though not as accurate as average learnability . One advantage of Learnability @ p over average learnability is that it costs less time to obtain learnability at a single perturbation probability . 3 ) of each model - perturbation pair on QQP dataset . Rows are sorted by average values over all models . The perturbation for which a model is most learnable is highlighted in bold while the following one is underlined .", "entities": [[0, 1, "DatasetName", "Inspired"], [2, 3, "MetricName", "Precision"], [6, 8, "TaskName", "Information Retrieval"], [62, 64, "TaskName", "data augmentation"], [166, 167, "MetricName", "accuracy"], [183, 185, "TaskName", "data augmentation"], [213, 215, "TaskName", "data augmentation"], [265, 267, "TaskName", "data augmentation"], [330, 331, "DatasetName", "QQP"]]}
{"text": "Pre - trained sequence - to - sequence language models have led to widespread success in many natural language generation tasks . However , there has been relatively less work on analyzing their ability to generate structured outputs such as graphs . Unlike natural language , graphs have distinct structural and semantic properties in the context of a downstream NLP task , e.g. , generating a graph that is connected and acyclic can be attributed to its structural constraints , while the semantics of a graph can refer to how meaningfully an edge represents the relation between two node concepts . In this work , we study pre - trained language models that generate explanation graphs in an end - to - end manner and analyze their ability to learn the structural constraints and semantics of such graphs . We first show that with limited supervision , pre - trained language models often generate graphs that either violate these constraints or are semantically incoherent . Since curating large amount of humanannotated graphs is expensive and tedious , we propose simple yet effective ways of graph perturbations via node and edge edit operations that lead to structurally and semantically positive and negative graphs . Next , we leverage these graphs in different contrastive learning models with Max - Margin and InfoNCE losses . Our methods lead to significant improvements in both structural and semantic accuracy of explanation graphs and also generalize to other similar graph generation tasks . Lastly , we show that human errors are the best negatives for contrastive learning and also that automatically generating more such human - like negative graphs can lead to further improvements . 1", "entities": [[211, 213, "MethodName", "contrastive learning"], [219, 220, "MethodName", "InfoNCE"], [233, 234, "MetricName", "accuracy"], [243, 245, "TaskName", "graph generation"], [259, 261, "MethodName", "contrastive learning"]]}
{"text": "Pre - trained sequence - to - sequence language models ( PLMs ) like BART ( Lewis et al , 2020 ) and ( Saha et al , 2021b ) showing the belief , argument , stance , gold explanation graph , and T5 - generated explanation graph . The dashed nodes represent commonsense nodes and the dashed edges are incorrect edges . The first generated graph is structurally incorrect and the second graph is semantically incorrect . T5 ( Raffel et al , 2020 ) have led to significant advances in many natural language generation tasks like text summarization and machine translation . The models are pre - trained on massive amounts of text data with self - supervision , thus enabling them to construct coherent natural language sentences for downstream tasks . This then raises the question whether pre - trained language models , trained on free - form natural language data , can also adapt themselves to generate structured outputs like graphs . Graphs are common in NLP tasks that involve representing structured knowledge in the form of knowledge bases ( Guarino and Giaretta , 1995 ) , constructing event chains from documents ( Chambers and Jurafsky , 2009 ) , or more recent work on encoding reasoning chains , explanations , or deductive proofs ( Saha et al , 2020 ; . Graphs differ from free - form natural language . In the context of NLP , natural language graphs ( consisting of textual nodes and edges ) can have distinct structural and semantic properties . For example , consider a recently proposed commonsense explanation graph generation task shown in Fig . 1 ( Saha et al , 2021b ) . Each example shows a belief , an argument and an explanation graph explaining how the argument supports or refutes the belief . These explanation graphs encode structured knowledge ( augmented with commonsense ) and consist of concepts as nodes and relations from ConceptNet ( Liu and Singh , 2004 ) as edges . For example , the second graph encodes the knowledge that \" both salads and fast food are part of mcdonalds and hence mcdonalds is not greasy and fattening \" , thus explicitly refuting the belief . From prior work , the structural constraints enforce the graphs to be connected directed acyclic and the nodes to contain at least two concepts from the belief and two from the argument . The semantic aspect deals with commonsense and evaluates whether each edge expresses coherent relational knowledge and if the whole graph explains the stance . Following Saha et al ( 2021b ) , we represent graphs as strings composed of concatenated edges and fine - tune T5 to generate graphs in an autoregressive manner . We observe that while moderate amount of supervision enables the model to learn valid graph encodings , the graphs frequently violate task - specific structural constraints ( like connectivity ) . For instance , the first example in Fig . 1 shows a graph generated by T5 that is disconnected and hence structurally incorrect . Moreover , for the fraction of graphs that are structurally correct , the model also makes commonsense mistakes , a type of semantic error , by inferring wrong or incoherent relations between concepts . Both T5 - generated graphs shown in Fig . 1 contain incoherent or noncommonsensical edges ( marked by dashed arrows ) like \" fast food ; has context ; salads \" . Based on these observations , we study PLMs that generate explanation graphs in an end - to - end manner and analyze their ability to learn the structural constraints as well as the semantics of such graphs . While a general recipe towards improving the structural and semantic aspects of graph generation can be via large - scale training with more humanannotated graphs , it is prohibitive under most practical scenarios because of the cognitive load associated with a complex data creation task like graph annotation Saha et al , 2021b ) . Hence , we propose simple yet effective methods of graph perturbations that perform various kinds of node and edge addition , deletion , and replacement operations to construct structurally and semantically positive ( correct ) and negative ( incorrect ) graphs . Overall , we leverage three types of negative graphs ( synthetic structural , synthetic semantic , and human - created semantic ) and develop multiple contrastive learning models ( Hjelm et al , 2018 ; Chen et al , 2020a ; Khosla et al , 2020 ; Gunel et al , 2020 ) for effectively distinguishing between correct and incorrect graphs . Our first method is a Generate - and - Refine model that first generates an initial graph and further refines it using another T5 model . Next , we propose two improved modelsone that uses the negative graphs in a max - margin formulation and another that uses both positive and negative graphs with a InfoNCE ( van den Oord et al , 2018 ) contrastive loss . On two real - world tasks of explanation graph generation and temporal graph generation , with varied node and edge semantics , we observe that our proposed methods and graph perturbation techniques generalize well and lead to improvements in both structural and semantic accuracy of graphs . Further analysis of different types of negative graphs reveal that the human - error graphs are the hardest , most diverse , and hence the best type of negatives to learn from in contrastive learning . Hence , we also develop methods to automatically generate more such human - like semantic negative graphs , which leads to further improvements . We summarize our contributions as follows . We present a detailed analysis of graph structure and semantics for end - to - end explanation graph generation via pre - trained language models . We propose simple yet effective graph perturbation techniques for constructing positive and negative graphs and use them in different graph contrastive learning models . Our methods lead to significant improvements in both structural and semantic accuracy of explanation graphs and also generalize to other similar graph generation tasks .", "entities": [[14, 15, "MethodName", "BART"], [43, 44, "MethodName", "T5"], [78, 79, "MethodName", "T5"], [98, 100, "TaskName", "text summarization"], [101, 103, "TaskName", "machine translation"], [269, 271, "TaskName", "graph generation"], [327, 328, "DatasetName", "ConceptNet"], [452, 453, "MethodName", "T5"], [507, 508, "MethodName", "T5"], [551, 552, "MethodName", "T5"], [632, 634, "TaskName", "graph generation"], [742, 744, "MethodName", "contrastive learning"], [802, 803, "MethodName", "T5"], [834, 835, "MethodName", "InfoNCE"], [845, 846, "MetricName", "loss"], [855, 857, "TaskName", "graph generation"], [859, 861, "TaskName", "graph generation"], [890, 891, "MetricName", "accuracy"], [927, 929, "MethodName", "contrastive learning"], [978, 980, "TaskName", "graph generation"], [1007, 1009, "MethodName", "contrastive learning"], [1022, 1023, "MetricName", "accuracy"], [1032, 1034, "TaskName", "graph generation"]]}
{"text": "Our primary task of interest is a recently proposed commonsense explanation graph generation task called ExplaGraphs ( Saha et al , 2021b ) . In Sec . 6.4 , we also experiment with another related task of temporal graph generation ( Madaan et al , 2020 ) . In both these tasks , the structural aspect deals with satisfying certain task - specific constraints on the graph ( like connectivity ) and the semantic aspect deals with the construction of meaningful edges ( that adhere to commonsense ) . Below we discuss ExplaGraphs briefly and analyze pre - trained language models for their ability to generate explanation graphs . ExplaGraphs ( Saha et al , 2021b ) . In this task , given a belief and an argument , an agent has to perform two sub - tasks - predict the stance ( support / counter ) and also generate an explanation graph explaining the stance . Explanation graphs are structured explanations that capture explicit reasoning chains between the belief and the argument , thereby making models more interpretable . Formally , an explanation graph is a connected DAG with nodes as concepts and edges as commonsense relations between two concepts ( See Fig . 1 ) . The concepts are either part of the belief or the argument ( represented with solid boxes ) or any external commonsense phrase ( represented with dashed boxes ) . Each edge in the graph forms a coherent sentence and the graph , when read as a whole , forms reasoning structures explaining why the argument supports or refutes the belief . Saha et al ( 2021b ) evaluate explanation graphs by defining two accuracy metrics - ( 1 ) Structural Correctness Accuracy ( StCA ) : Fraction of graphs that satisfy all structural constraints , and ( 2 ) Semantic Correctness Accuracy ( SeCA ) : Fraction of graphs that are both structurally and semantically correct . A graph is considered structurally correct if it satisfies the following constraints : ( 1 ) it is connected , ( 2 ) it is a DAG , ( 3 ) the edge relations belong to a pre - defined list , ( 4 ) there are at least two concepts from the belief and two from the argument . If all these constraints are satisfied , the graph is next evaluated for semantic correctness by a model - based metric ( Saha et al , 2021b ) . It works on the principle that an explanation graph is semantically correct if the stance inferred from the belief and the graph matches the gold stance . Refer to Appendix A for a detailed description of all evaluation metrics . Baseline T5 Model . Following prior work ( Saha et al , 2021b ) , we generate explanation graphs as post - hoc explanations by conditioning on the belief , argument and the predicted stance . 2 The stance prediction model is a fine - tuned RoBERTa model ( Liu et al , 2019 ) which we keep unaltered from prior work and focus on the graph generation sub - task . We generate graphs as linearized strings in an endto - end manner by leveraging an encoder - decoder pre - trained language model , T5 ( Raffel et al , 2020 ) . The input to the model is the concatenated belief , argument and the stance along with a prefix \" Generate an Explanation Graph for \" . The graphs are encoded as concatenated bracketed edges , in which the edges are ordered according to the Depth First Search ( DFS ) order of the nodes . While we choose T5 because of its superior performance ( Saha et al , 2021b ) , we do not make any model - specific assumptions and graphs can be generated via any encoder - decoder style pre - trained language model ( e.g. , see Appendix E for results with BART ) . Analysis of T5 Baseline . We analyze the quality of the explanation graphs generated by T5 in Table 1 . We vary the amount of training data from 500 to 2368 samples ( all ) and report StCA and SeCA along with other metrics like Graph - BertScore ( G - BS ) introduced in prior work ( Saha et al , 2021b ) . While the structural accuracy improves with increase in training data , the gain saturates quickly and even after training on the entire data , we find a significant fraction of graphs to violate the structural constraints . We note that a high 91 % of T5 's generations are valid graph encodings i.e. , the generated strings can be parsed into graphical structures ( without any post - processing ) , suggesting that T5 is able to learn the graph encoding from a fairly small amount of supervision . However , it fails to satisfy the various structural constraints - ( 1 ) 20 % of the graphs are disconnected , ( 2 ) 6 % of the graphs contain cycles , and ( 3 ) 14 % of the graphs have less than two concepts from the belief or from the argument . Note that these constraints are not encoded in the model , thus making them fairly hard to learn from limited supervision . On the fraction of structurally correct graphs , the model makes further semantic errors and a lower SeCA of 35 % demonstrates that . In Fig . 1 , we show examples of structurally incorrect and semantically incorrect graphs generated by T5 . Overall , these results indicate that there is a significant scope for improvement both on graph structure and semantics , thus motivating us to develop methods with design choices aimed at improving both aspects .", "entities": [[11, 13, "TaskName", "graph generation"], [38, 40, "TaskName", "graph generation"], [130, 131, "DatasetName", "agent"], [281, 282, "MetricName", "accuracy"], [289, 290, "MetricName", "Accuracy"], [309, 310, "MetricName", "Accuracy"], [455, 456, "MethodName", "T5"], [500, 501, "MethodName", "RoBERTa"], [520, 522, "TaskName", "graph generation"], [550, 551, "MethodName", "T5"], [617, 618, "MethodName", "T5"], [665, 666, "MethodName", "BART"], [670, 671, "MethodName", "T5"], [683, 684, "MethodName", "T5"], [736, 737, "MetricName", "accuracy"], [778, 779, "MethodName", "T5"], [806, 807, "MethodName", "T5"], [940, 941, "MethodName", "T5"]]}
{"text": "One simple method to augment existing training data is to create synthetic positive graphs . These graphs should be created such that all the taskspecific constraints continue to hold upon perturbations . E.g. , removing a node that makes the graph disconnected is a prohibitive action . Hence , we choose nodes ( concepts ) that are not part of the belief or the argument ( also termed as commonsense nodes ) and replace them with phrases that are synonymous to the original phrases . To do so , we select words from the concept with POS tags of Adjective , Noun , Adverb , or Verb and replace them with that synonym from Wordnet ( Miller , 1995 ) for which the cosine similarity of their word2vec representations ( Mikolov et al , 2013 ) is the highest . 3 Fig . 2 shows an example of a positive graph perturbation where the node \" loss of jobs \" is replaced with \" going of business \" . Note that our node replacement operations will always lead to structurally similar graphs . Automatically constructing structurally diverse positive graphs is a challenging problem and we leave that for future work .", "entities": [[156, 157, "MetricName", "loss"]]}
{"text": "We also construct semantically incorrect negative explanation graphs . While the previous category of negative graphs ( SySt ) captures structural constraints , SySe captures the relational knowledge in graphs . Semantic incorrectness typically arises from inappropriate relations that do not adhere to human commonsense ( \" loss of jobs ; is a ; humane \" ) . We create such negative graphs by selecting a random number of edges and then replacing the relations with some other relations . Fig . 2 shows a semantic negative graph in which the relations marked with dashed lines are perturbed . Human - created & Semantic Negative Graphs ( HuSe ) . The space of semantically incorrect graphs is fairly large and in order to augment our synthetic negative graphs with harder structurallydiverse negatives , we make use of human - created incorrect graphs from prior work ( Saha et al , 2021b ) . 4 Humans make subtle errors , thus making them ideal negative candidates for contrastive learning . ExplaGraphs was constructed via an iterative framework in which the graphs are iteratively refined ( up to two times ) until they are verified as correct . We treat these refined graphs as negatives . Specifically , in two rounds , if an initial graph G 1 is refined into graphs G 2 and G 3 successively , then G 1 and G 2 are considered as negative graphs . Unlike SySe which only perturb the relations , these negatives are structurally diverse ( see Fig . 2 ) and capture semantics not just at the level of each edge but for the graph as a whole ( e.g. , a graph might be refined because it does not explain the stance ) . Note that human - created graphs can only be semantically incorrect , since their structural correctness is already ensured during construction .", "entities": [[47, 48, "MetricName", "loss"], [166, 168, "MethodName", "contrastive learning"]]}
{"text": "Our next model leverages the negatively perturbed graphs in a max - margin formulation . During training , given a ( belief , argument , stance ) context x , a ground truth graph G ( g ) and a negative graph G ( n ) , linearized into a sequence of words { y ( g ) i } k i=1 and { y ( n ) i } l i=1 respectively , we define the loss function L as a linear combination of the standard crossentropy loss L CE and a max - margin loss L MM , defined between a word y ( g ) i of the positive graph and a word y ( n ) i of the negative graph . L CE = i \u2212logP \u03b8 ( y ( g ) i | y ( g ) < i , x ) L MM = i max ( 0 , logP \u03b8 ( y ( g ) i | y ( g ) < i , x ) \u2212 log P \u03b8 ( y ( n ) i | y ( n ) < i , x ) + \u03b2 ) L = L CE + \u03b1L MM where \u03b1 and \u03b2 ( margin ) are hyperparameters . As noted earlier , the baseline model often makes commonsense mistakes in distinguishing between positive and negative relations ( \" causes \" vs \" not causes \" ) and our relation perturbing negative graphs and the max - margin loss component facilitate learning a better boundary between them .", "entities": [[77, 78, "MetricName", "loss"], [88, 89, "MetricName", "loss"], [96, 97, "MetricName", "loss"], [131, 132, "HyperparameterName", "\u03b8"], [154, 155, "DatasetName", "0"], [157, 158, "HyperparameterName", "\u03b8"], [177, 178, "HyperparameterName", "\u03b8"], [195, 196, "HyperparameterName", "\u03b2"], [205, 206, "HyperparameterName", "\u03b1"], [207, 208, "HyperparameterName", "\u03b2"], [253, 254, "MetricName", "loss"]]}
{"text": "Our Contrastive Graph Generation Model ( Fig . 2 ) also leverages both positive and negative graphs but instead of doing so in a 2 - stage Generate & Refine model , uses a contrastive learning framework ( Khosla et al , 2020 ; Gunel et al , 2020 ) . Given a ground - truth graph G ( g ) , a positive graph G ( p ) and a set of negative graphs { G ( n ) i } M i=1 , contrastive learning aims to learn the graph representations such that the gold graph 's representation is close to that of the synthetic positive graph while being distant from those of the negative graphs . Similar to Cao and Wang ( 2021 ) , we use the last layer of the decoder in T5 as the representation of each token in the graph and obtain the graph representation by averaging over the constituent token representations . Let the graph representations be denoted by h ( g ) , h ( p ) and { h ( n ) i } M i=1 . Given H ( g ) = { h ( p ) } { h ( n ) i } M i=1 , our overall loss combines the cross - entropy loss L CE and the InfoNCE contrastive loss ( van den Oord et al , 2018 ) L CL as shown below . ( Efron and Tibshirani , 1994 ) ) with p < 0.005 . L CL = \u2212 log exp ( sim ( h ( g ) , h ( p ) ) /\u03c4 ) h i H ( g ) exp ( sim ( h ( g ) , h i ) /\u03c4 ) L = L CE + where \u03b1 and the temperature \u03c4 are the hyperparameters and sim ( ) denotes the cosine similarity function between the graph representations . 6 Experiments", "entities": [[2, 4, "TaskName", "Graph Generation"], [34, 36, "MethodName", "contrastive learning"], [85, 87, "MethodName", "contrastive learning"], [137, 138, "MethodName", "T5"], [211, 212, "MetricName", "loss"], [217, 218, "MetricName", "loss"], [222, 223, "MethodName", "InfoNCE"], [224, 225, "MetricName", "loss"], [300, 301, "HyperparameterName", "\u03b1"]]}
{"text": "In Table 2 , we compare the various modeling techniques described in Sec . 5 and their effect on the structural and semantic correctness of the generated graphs . While our primary metrics of interest are Graph Structural Accuracy ( StCA ) and Semantic Accuracy ( SeCA ) , following prior work ( Saha et al , 2021b ) , we also report Stance Accuracy ( SA ) , Graph - BertScore ( G - BS ) , Graph Edit Distance ( GED ) and Edge Accuracy ( EA ) . We observe that using a larger T5 model improves StCA by 12 % and SeCA by 16 % . This finding is in line with other commonsense reasoning tasks ( Lourie et al , 2021 ; Elazar et al , 2021 ) which also show that fine - tuning a larger language model typically leads to better performance . Together with the results reported in Table 1 , we conclude that much of the improvement in explanation graph generation comes from increasing the training data and using a larger model . Given its superior performance , we build our proposed models on T5 - large .", "entities": [[38, 39, "MetricName", "Accuracy"], [44, 45, "MetricName", "Accuracy"], [64, 65, "MetricName", "Accuracy"], [82, 83, "DatasetName", "GED"], [86, 87, "MetricName", "Accuracy"], [97, 98, "MethodName", "T5"], [168, 170, "TaskName", "graph generation"], [193, 194, "MethodName", "T5"]]}
{"text": "We test the generalizability of constructing structurally and semantically perturbed graphs for contrastive learning by also experimenting on a temporal graph generation task ( Madaan and Yang , 2021 ) that requires constructing a temporal graph from a document . The nodes in the graph are events from the document and the edges are temporal relations between events ( \" before \" , \" after \" , etc ) . Following our overall goal of improving graph generation with limited data , we randomly sample 1.3 % of the overall corpus ( \u223c9.5k samples ) as the training data such that all graphs are connected DAGs . Similar to ExplaGraphs , we create structurally negative graphs with disconnected and cyclic graphs and semantic negative graphs by perturbating the temporal relations . E.g. , if an edge relation is \" before \" , we replace it with \" after \" . We construct positive graphs by replacing edges like \" A before B \" with \" B after A \" ( more details in Appendix C ) . In 6.5 Analysis of Generated Graphs Fig . 3 shows an example of the graphs generated by different models ( more examples in Appendix F ) . Unlike T5 , our models ' graphs are both structurally and semantically correct with diverse commonsense nodes ( \" Groupthink \" , \" Good Thing \" ) . While our models generate more correct graphs , they lack in structural diversity - the Contrastive model generates 77 % of linear graphs ( i.e. , the nodes are in a linear chain ) which is comparable to 75 % in the T5 model . This can be attributed to our structurally similar positive graphs as the model does not obtain enough supervision to generate diverse graphs . Structural diversity is not a measure of graph correctness ; however , like diverse text generation ( Vijayakumar et al , 2018 ) , generating diverse graphs is an interesting direction for future work . 6.6 Generating Human - like Semantic Negatives ( HuSe - Gen ) In ExplaGraphs , human - created negatives account for 38 % of the samples for which the initially constructed graph was incorrect and was refined . Moreover , we see in the previous section that humanerror graphs are the best negative candidates for contrastive learning ( which is intuitive since tricky and subtle errors made by expert human annotators would make for some of the hardest negatives / distractors for a contrastive learning model to learn from ) . Hence , in this final section , we further explore whether it is also possible to automatically imitate and generate more of such harder humanlike incorrect graphs for the remaining samples as well . Our method consists of the following steps . Human - like Negative Edge Generation . We first fine - tune a T5 model that conditions on the belief , argument and the stance to generate a set of incorrect edges ( which is the set of edges that are present in the incorrect graph and not in the refined graph ) . Human - like Negative Graph Construction . This generated set of incorrect edges is then added to the correct graph to construct the incorrect graph , such that it is structurally correct and hence representative of human - like erroneous graphs . Filtering High - quality Negative Graphs . Con - trastive models will only benefit from these negatives if the negative edge generation model is accurate and generates edges that are actually incorrect . Hence , we control the quality of the generated incorrect graphs by the following two techniques - ( a ) Thresholding via fraction of Acceptable Edges ( AE ) : We say that a generated incorrect edge is acceptable if it is not part of the correct graph and can be added to the correct graph without violating any structural constraints . We compute the fraction of acceptable edges for every generated negative graph and choose only those graphs with AE above a certain threshold \u03b4 . Intuitively , this ensures that a high fraction of the generated edges are actually incorrect and hence when added to the correct graph , will lead to a sufficiently different ( human - like ) incorrect graph . ( b ) Thresholding via Incorrect Probability of a graph ( IP ) : We use our SeCA metric model ( that classifies a graph into support , counter , or incorrect class ) to compute the probability of the generated graph being incorrect and choose those graphs that are above a certain threshold \u03b3 of incorrect probability . We set \u03b4 = 0.4 and \u03b3 = 0.5 ( tuned on the dev set ) and train the Max - margin model using these additionally generated human - like negative graphs . As shown in Table 5 both thresholding approaches lead to further improvements over using just the human - created negative graphs . These initial promising results for emulating hard / tricky human errors as strong negatives for contrastive learning will hopefully lead to further future work in this interesting direction .", "entities": [[12, 14, "MethodName", "contrastive learning"], [20, 22, "TaskName", "graph generation"], [76, 78, "TaskName", "graph generation"], [205, 206, "MethodName", "T5"], [274, 275, "MethodName", "T5"], [314, 316, "TaskName", "text generation"], [390, 392, "MethodName", "contrastive learning"], [418, 420, "MethodName", "contrastive learning"], [481, 482, "MethodName", "T5"], [526, 528, "TaskName", "Graph Construction"], [624, 625, "MethodName", "AE"], [677, 678, "MethodName", "AE"], [682, 683, "HyperparameterName", "\u03b4"], [776, 777, "HyperparameterName", "\u03b3"], [783, 784, "HyperparameterName", "\u03b4"], [787, 788, "HyperparameterName", "\u03b3"], [851, 853, "MethodName", "contrastive learning"]]}
{"text": "From an ethics standpoint , we provide a brief overview and show samples from the datasets that our models are trained on throughout the paper and also in the Appendix . Explanation graph generation improves the interpretability of neural commonsense reasoning systems and could prove to be effective in understanding and debugging such models . Hence we do not foresee any major risks or negative societal impact of our work . However , like any other ML model , the graphs generated by our models may not always be completely accurate and hence should be used with caution for real - world applications . et al , 2019 ) classifier that given a belief and a generated explanation graph , infers whether the graph supports the belief , counters the belief or is incorrect ( because of incoherent edges ) . If it predicts support or counter and this stance matches the gold stance , then the graph is considered semantically correct . In essense , SeCA works on the principle that an explanation graph is semantically correct if a stance can be unambiguously inferred from it ( by a model in this case or a human ) and that stance is the same as the gold stance . Note that SeCA is a reference - free metric ( does not use the groundtruth graph ) and hence is invariant to structural variations in explanation graphs . Graph - BertScore ( G - BS ) . Graph - BertScore is an extension of BertScore for computing the degree of match between the predicted graphs and the ground - truth graphs . It treats a graph as a set of edges and computes the best match between the gold edges and the predicted edges , where the matching score between a pair of edges is given by the BertScore F1 . Graph Edit Distance ( GED ) . GED is the standard Graph Edit Distance for graphs , measuring the number of edit operations ( addition , deletion , and replacement of nodes and edges ) to transform one graph to the other and further normalized by an appropriate normalizing constant . Edge Accuracy ( EA ) . The final metric , Edge Accuracy ( EA ) measures the fraction of edges in the graph that are important . An edge is considered important if removing it from the graph leads to a drop in the gold stance prediction confidence .", "entities": [[32, 34, "TaskName", "graph generation"], [308, 309, "MetricName", "F1"], [314, 315, "DatasetName", "GED"], [317, 318, "DatasetName", "GED"], [362, 363, "MetricName", "Accuracy"], [372, 373, "MetricName", "Accuracy"]]}
{"text": "Table 7 shows the number of train , validation and test samples of the two datasets we experiment with . We build our models on top of the Hugging Face transformers library ( Wolf et al , 2020 ) . 6 All models for the ExplaGraphs dataset 7 ( Saha et al , 2021b ) are trained with a batch size of 8 and an initial learning rate of 3 * 10 \u22125 for a maximum of 15 epochs . The maximum input and output sequence lengths are both set to 150 . For the max - margin graph generation model , we set both the hyperparameters \u03b1 ( mixing ratio ) and \u03b2 ( margin ) to 1.0 while for the contrastive graph generation model , we set \u03b1 to 0.1 . For the temporal graph generation task 8 ( Madaan and Yang , 2021 ) , we train all models with a batch size of 4 and an initial learning rate of 3 * 10 \u22125 for a maximum of 10 epochs . The maximum input and output sequence lengths are set to 512 and 256 respectively . On this task , the hyperparameters \u03b1 and \u03b2 for the max - margin model are again set to 1.0 while for the contrastive graph generation model , we set \u03b1 to 0.2 . Across all models and tasks , graphs are generated using beam search decoding with a beam size of 4 . The batch size and learning rate are manually tuned in the range { 4 , 8 , 16 } and { 10 \u22125 , 2 * 10 \u22125 , 3 * 10 \u22125 } respectively and the best models are chosen based on the respective validation set performance . Similarly , the mixing ratio hyperparameter \u03b1 is manually tuned in the range", "entities": [[59, 61, "HyperparameterName", "batch size"], [66, 68, "HyperparameterName", "learning rate"], [98, 100, "TaskName", "graph generation"], [107, 108, "HyperparameterName", "\u03b1"], [113, 114, "HyperparameterName", "\u03b2"], [123, 125, "TaskName", "graph generation"], [129, 130, "HyperparameterName", "\u03b1"], [136, 138, "TaskName", "graph generation"], [154, 156, "HyperparameterName", "batch size"], [161, 163, "HyperparameterName", "learning rate"], [196, 197, "HyperparameterName", "\u03b1"], [198, 199, "HyperparameterName", "\u03b2"], [214, 216, "TaskName", "graph generation"], [220, 221, "HyperparameterName", "\u03b1"], [245, 247, "HyperparameterName", "batch size"], [248, 250, "HyperparameterName", "learning rate"], [299, 300, "HyperparameterName", "\u03b1"]]}
{"text": "Below we provide brief descriptions of the evaluation metrics used for the ExplaGraphs task . For further details , we refer readers to prior work ( Saha et al , 2021b ) . Structural Correctness Accuracy of Graphs ( StCA ) . It computes the fraction of graphs where all the structural constraints are satisfied . ( SeCA ) . SeCA is a model - based metric that computes the fraction of graphs that are both structurally and semantically correct . For computing SeCA , prior work trains a 3 - way RoBERTa ( Liu Figure 6 : Example of explanation graphs generated by different models . The baseline T5 - generated graph is semantically incorrect ( incoherent relations marked in dashed red ) while our proposed models generate both structurally and semantically correct graphs .", "entities": [[35, 36, "MetricName", "Accuracy"], [92, 93, "MethodName", "RoBERTa"], [109, 110, "MethodName", "T5"]]}
{"text": "During the time of writing this paper the dataset included 25 , 512 , 320 abstracts from PubMed and 1 , 350 , 119 full articles from PMCOA , resulting in 155 , 356 , 970 and 232 , 838 , 618 sentences respectively . These numbers are not identical to the ones reported by NCBI for couple of reasons . Firstly , at the moment , we do not process the deletion updates nor do we remove the old versions of PM - COA articles if they are revised , i.e. our dataset may include articles , which have been retracted and an article may be included multiple times if The main processing steps of the pipeline . First , the articles are downloaded from the source and filtered to prevent reprocessing old documents . The documents are then converted to plain text format . This text data is split to independent sentences , tokenized and tagged with POS labels and syntactic dependencies . In addition , named entity recognition for several entity types is carried out . the content has been modified . We plan to take the deletions into account in near future . Secondly , the external tools in our pipeline may occasionally fail , in which case some of the articles are not processed . Since the pipeline processes the input data in batches , a critical error may lead to a whole batch not being processed . We are currently improving the pipeline to automatically reprocess the failed batches with the problematic articles excluded to minimize the loss of data . Running the parsing pipeline , including tokenization , POS tagging and conversion to the collapsed Stanford scheme , is the most time consuming part of the whole pipeline . Execution of this step has taken 84 , 552 CPU hours ( 9.6 CPU years ) for the currently available data . Unfortunately we do not have exact processing time statistics for named entity recognition and thus estimate its computational requirements by extrapolating from a smaller test run . Based on this experiment NER has demanded 4 , 100 CPU hours thus far . The text preprocessing and sentence splitting steps are negligible and thus the overall processing time required is approximately 10 CPU years . In total , our processing pipeline has detected 526 , 175 , 528 named entities . GGPs are the most common entities , covering 36.2 % of all entity mentions , whereas the cell lines are the most infrequent , forming only 1.3 % of the data . The entity type specific statistics along with the most common entity spans are listed in Table 2 .", "entities": [[168, 171, "TaskName", "named entity recognition"], [263, 264, "MetricName", "loss"], [328, 331, "TaskName", "named entity recognition"], [349, 350, "TaskName", "NER"]]}
{"text": "The normalization of Microorganism entities component of our system is based on exact matching against the names and synonyms of the concepts in the NCBI taxonomy . Error analysis on the training and developments data sets revealed that applying some rules may improve the results . For instance , \" Escherichia coli \" has an exact match that can be successfully normalized to the referent concept with an ID \" 562 \" in the NCBI taxonomy . In the following parts of the document , although the \" E. coli \" mention indicates a clear reference to the same concept , it can not be normalized to the \" Escherichia coli \" concept with an exact matching approach . In this kind of cases , if an exact match does not exist , the previously mentioned similar entities in the text are searched . If a match is found , the same concept is assigned as the normalized concept for the corresponding mention \" E. coli \" . If there does not exist a match with the previously normalized concepts , the root concept with an ID \" 2 \" is assigned . 4 Relation Extraction", "entities": [[27, 28, "MetricName", "Error"], [55, 57, "MetricName", "exact match"], [127, 129, "MetricName", "exact match"], [194, 196, "TaskName", "Relation Extraction"]]}
{"text": "In the BioNLP Shared Task 2019 Bacteria Biotopes normalization sub - task , entities are given with their boundaries in the text and the participants are required to predict the normalization of the entities . In the official evaluation , for each normalized Habitat / Phenotype entity , Wang similarity W ( Wang et al , 2007 ) is calculated to measure the similarity between the reference concept and the predicted concept for the normalization . The performances of the submitted systems are evaluated with their Precision values , which are calculated as : Precision = S p / N ( 2 ) where S p indicates the total Wang similarity W for all predictions ( Deleger et al , 2016 ) , and N is the number of predicted entities . In the BioNLP Shared Task 2019 Bacteria Biotopes relation extraction sub - task , entities are given with their boundaries in the text and the participants are asked to predict the relations between the entities . The performances of the submitted systems are evaluated with their F1 ( F - measure ) , recall and precision values .", "entities": [[86, 87, "MetricName", "Precision"], [94, 95, "MetricName", "Precision"], [140, 142, "TaskName", "relation extraction"], [178, 179, "MetricName", "F1"], [180, 183, "MetricName", "F - measure"]]}
{"text": "The official results obtained by our system and the other participants for the BB - norm sub - task are shown in Table 3 . Our system ( BOUN - ISIK - 2 ) achieved the best performance with 67.9 % Precision in the BB - norm sub - task ( Entity Normalization ) .", "entities": [[41, 42, "MetricName", "Precision"]]}
{"text": "We use area under the ROC curve ( AUC ) as the primary evaluation metric for SLA modeling ( Fawcett , 2006 ) . AUC is a common measure of ranking quality in classification tasks , and can be interpreted as the probability that the system will rank a randomly - chosen error above a randomlychosen non - error . We argue that this notion of ranking quality is particularly useful for evaluating systems that might be used for personalized learning , e.g. , if we wish to prioritize words or exercises for an individual learner 's review based on how likely they are to have forgotten or make errors at a given point in time . We also report F1 score - the harmonic mean of precision and recall - as a secondary metric , since it is more common in similar skewed - class labeling tasks ( e.g. , Ng et al , 2013 ) . Note , however , that F1 can be significantly improved simply by tuning the classification threshold ( fixed at 0.5 for our evaluations ) without affecting AUC .", "entities": [[8, 9, "MetricName", "AUC"], [24, 25, "MetricName", "AUC"], [120, 122, "MetricName", "F1 score"], [163, 164, "MetricName", "F1"], [172, 174, "HyperparameterName", "classification threshold"], [184, 185, "MetricName", "AUC"]]}
{"text": "A total of 15 teams participated in the task , of which 13 responded to a brief survey about their approach , and 11 submitted system description papers . All but two of these teams submitted predictions for all three language tracks . Official shared task results are reported in Table 2 . System ranks are determined by sorting teams according to AUC , and using DeLong 's test ( DeLong et al , 1988 ) to identify statistical ties . For the remainder of this section , we provide a summary of each team 's approach , ordered by the team 's average rank across all three tracks . Certain teams are marked with modeling choice indicators ( \u2662 , \u2663 , \u2021 ) , which we discuss further in 5 . SanaLabs ( Nilsson et al , 2018 ) used a combination of recurrent neural network ( RNN ) predictions with those of a Gradient Boosted Decision Tree ( GBDT ) ensemble , trained independently for each track . This was motivated by the observation that RNNs work well for sequence data , while GBDTs are often the best - performing non - neural model for shared tasks using tabular data . They also engineered several token context features , and learner / token history features such as number of times seen , time since last practice , etc . singsound ( Xu et al , 2018 ) used an RNN architecture using four types of encoders , representing different types of features : token context , linguistic information , user data , and exercise format . The RNN decoder integrated information from all four encoders . Ablation experiments revealed the context encoder ( representing the token ) contributed the most to model performance , while the linguistic encoder ( representing grammatical information ) contributed the least . NYU ( Rich et al , 2018 ) used an ensemble of GBDTs with features engineered based on psychological theories of cognition . Predictions for each track were averaged between a track - specific model and a unified model ( trained on data from all three tracks ) . In addition to the word , user , and exercise features provided , the authors included word lemmas , corpus frequency , L1 - L2 cognates , and features indicating user motivation and diligence ( derived from usage patterns ) , and others . Ablation studies indicated that most of the performance was due to the user and token features . TMU ( Kaneko et al , 2018 ) used a combination of two bidirectional RNNs - the first to predict potential user errors at a given token , and a second to track the history of previous answers by each user . These networks were jointly trained through a unified objective function . The authors did not engineer any additional features , but did train a single model for all three tracks ( using a track ID feature to distinguish among them ) . CECL ( Bestgen , 2018 ) used a logistic regression approach . The base feature set was expanded to include many feature conjunctions , including word n - grams crossed with the token , user , format , and session features provided with the data set . Cambridge ( Yuan , 2018 ) trained two RNNsa sequence labeler , and a sequence - to - sequence model taking into account previous answers - and found that averaging their predictions yielded the best results . They focused on the English track , experimenting with additional features derived from other English learner corpora . Hyper - parameters were tuned for English and used as - is for other tracks , with comparable results . UCSD ( Tomoschuk and Lovelett , 2018 ) used a random forest classifier with a set of engineered features motivated by previous research in memory and linguistic effects in SLA , including \" word neighborhoods , \" corpus frequency , cognates , and repetition / experience with a given word . The system also included features specific to each user , such as mean and variance of error rates . LambdaLab used GBDT models independently for each track , deriving their features from confirmatory analysis of psychologically - motivated hypotheses on the TRAIN set . These include proxies for student engagement , spacing effect , response time , etc . nihalnayak ( Nayak and Rao , 2018 ) used a logistic regression model similar to the baseline , but added features inspired by research in codemixed language - learning where context plays an important role . In particular , they included word , part of speech , and metaphone features for previous : current and current : next token pairs . Grotoco ( Klerke et al , 2018 ) also used logistic regression , including word lemmas , frequency , cognates , and user - specific features such as word error rate . Interestingly , the authors found that ignoring each user 's first day of exercise data improved their predictions , suggesting that learners first needed to familiarize themselves with app before their data were reliable for modeling . jilljenn ( Vie , 2018 ) used a deep factorization machine ( DeepFM ) , a neural architecture developed for click - through rate prediction in recommender systems . This model allows learning from both lower - order and higher - order induced features and their interactions . The DeepFM outperformed a simple logistic regression baseline without much additional feature engineering . Other teams did not submit system description papers . However , according to a task organizer survey ymatusevych used a linear model with multilingual word embeddings , corpus frequency , and several L1 - L2 features such as cognates . Additionally , simplelinear used an ensemble of some sort ( for the French track only ) . renhk and zlb241 provided no details about their systems . SLAM_baseline is the baseline system provided by the task organizers . It is a simple logistic regression using data set features , trained separately for each track using stochastic gradient descent on the TRAIN set only .", "entities": [[62, 63, "MetricName", "AUC"], [512, 514, "MethodName", "logistic regression"], [551, 552, "DatasetName", "Cambridge"], [746, 748, "MethodName", "logistic regression"], [807, 809, "MethodName", "logistic regression"], [886, 891, "TaskName", "click - through rate prediction"], [919, 921, "MethodName", "logistic regression"], [925, 927, "TaskName", "feature engineering"], [952, 954, "TaskName", "word embeddings"], [1010, 1012, "MethodName", "logistic regression"], [1023, 1026, "MethodName", "stochastic gradient descent"]]}
{"text": "Here we attempt to answer the question of whether particular machine learning algorithms have a significant impact on task performance . For example , the results in Table 2 suggest that the algorithmic choices indicated by ( \u2662 , \u2663 , \u2021 ) are particularly effective . Is this actually the case ? To answer this question , we partitioned the TEST set into 6.4k subsets ( one for each learner ) , and computed per - user AUC scores for each team 's predictions ( 83.9k observations total ) . We also coded each team with indicator variables to describe their algorithmic approach , and used a regression analysis to determine if these algorithmic variations had any significant effects on learnerspecific AUC scores . To analyze this properly , however , we need to determine whether the differences among modeling choices are actually meaningful , or can simply be explained by sampling error due to random variations among users , teams , or tracks . To do this , we use a linear mixed - effects model ( cf . , Baayen , 2008 , Ch . 7 ) . In addition to modeling the fixed effects of the various learning algorithms , we can also model the random effects represented by the user ID ( learners may vary by ability ) , the team ID ( teams may differ in other aspects not captured by our schema , e.g. , the hardware used ) , and the track ID ( tracks may vary inherently in difficulty ) . Table 3 presents a mixed - effects analysis for the algorithm variations used by at least 3 teams . The intercept can be interpreted as the \" average \" AUC of .786 . Controlling for the random effects of user ( which exhibits a wide standard deviation of \u00b1.086 AUC ) , team ( \u00b1.013 ) , and track ( \u00b1.011 ) , three of the algorithmic choices are at least marginally significant ( p < .1 ) . For example , we might expect a system that uses RNNs to model learner mastery over time would add + .028 to learner - specific AUC ( all else being equal ) . Note that most teams ' systems that were not based on RNNs or tree ensembles used logistic regression , hence the \" linear model \" effect is negligible ( effectively treated as a control condition in the analysis ) . These results suggest two key insights for SLA modeling . First , non - linear algorithms are particularly desirable 4 , and second , multitask learning approaches that share information across tracks ( i.e. , languages ) are also effective .", "entities": [[78, 79, "MetricName", "AUC"], [122, 123, "MetricName", "AUC"], [289, 290, "MetricName", "AUC"], [309, 310, "MetricName", "AUC"], [364, 365, "MetricName", "AUC"], [388, 390, "MethodName", "logistic regression"]]}
{"text": "Another interesting research question is : what is the upper - bound for this task ? This can be estimated by treating each team 's best submission as an independent system , and combining the results using ensemble methods in a variety of ways . Such analyses have been previously applied to other shared task challenges and meta - analyses ( e.g. , Malmasi et al , 2017 ) . The oracle system is meant to be an upperbound : for each token in the TEST set , the oracle outputs the team prediction with the lowest error for that particular token . We also experiment with stacking ( Wolpert , 1992 ) by training a logistic regression classifier using each team 's prediction as an input feature 6 . Finally , we also pool system predictions together by taking their average ( mean ) . Table 5 reports AUC for various ensemble methods as well as some of the top performing team systems for all three tracks . Interestingly , the oracle is exceptionally accurate ( > .993 AUC and > .884 F1 , not shown ) . This indicates that the potential upper limit of performance on this task is quite high , since there exists a near - perfect ranking of tokens in the TEST set based only on predictions from these 15 diverse participating teams . The stacking classifier produces significantly better rankings than any of the constituent systems alone , while the average ( over all teams ) ranked between the 3rd and 4th best system in all three tracks . Inspection of stacking model weights revealed that it largely learned to trust the topperforming systems , so we also tried simply averaging the top 3 systems for each track , and this method was statistically tied with stacking for the English and French tracks ( p = 0.002 for Spanish ) . Interestingly , the highest - weighted team in each track 's stacking model was singsound ( +2.417 on average across the three models ) , followed teams and learning algorithms . It would be interesting to revisit these ideas using a more diverse and longitudinal data set in the future . To support ongoing research in SLA modeling , current and future releases of our data set will be publicly maintained online at : https://doi.org/10.7910/DVN/8SWHNO .", "entities": [[116, 118, "MethodName", "logistic regression"], [149, 150, "MetricName", "AUC"], [179, 180, "MetricName", "AUC"], [183, 184, "MetricName", "F1"]]}
{"text": "Examining the parallels between human and machine learning is a natural way for us to better understand the former and track our progress in the latter . The \" black box \" aspect of neural networks has recently inspired a large body of work related to interpretability , i.e. understanding of representations that such models learn . In NLP , this push has been largely motivated by linguistic questions , such as : what linguistic properties are captured by neural networks ? and to what extent do decisions made by neural models reflect established linguistic theories ? Given the relative recency of such questions , much work in the domain so far has been focused on the context of models in isolation ( e.g. what does model X learn about linguistic phenomenon Y ? ) In order to more broadly understand models ' representational tendencies , however , it is vital that such questions be formed not only with other models in mind , but also other rep - resentational methods and modalities ( e.g. behavioral data , fMRI measurements , etc . ) . In context of the latter concern , the present - day interpretability toolkit has not yet been able to afford a practical way of reconciling this . In this work , we employ Representational Similarity Analysis ( RSA ) as a simple method of interpreting neural models ' representational spaces as they relate to other models and modalities . In particular , we conduct an experiment wherein we investigate the correspondence between human processing difficulty ( as reflected by gaze fixation measurements ) and the representations induced by popular pretrained language models . In our experiments , we hypothesize that there exists an overlap between the sentences which are difficult for humans to process and those for which per - layer encoder representations are least correlated . Our intuition is that such sentences may exhibit factors such as low - frequency vocabulary , lexical ambiguity , and syntactic complexity ( e.g. multiple embedded clauses ) , etc . that are uncommon in both standard language and , relatedly , the corpora employed in training large - scale language models . In the case of a human reader , encountering such a sentence may result in a number of processing delays , e.g. longer aggregate gaze duration . In the case of a sentence encoder , an uncommon sentence may lead to a degradation of representations in the encoder 's layers , wherein a lower layer might learn to encode vastly different information than a higher one . Similarly , different models ' representations may emphasize different aspects of these more complex sentences and therefore diverge from each other . With this in mind , our hypothesis is that sentences which are difficult for humans to process are likely to have divergent representations within models ' internal layers and between different models ' layers . Understanding and analysing language encoders In recent years , some prominent efforts towards interpreting neural networks for NLP have included : developing suites that evaluate network representations through performance on downstream tasks ( Conneau et al , 2017a ; Wang et al , 2018 ; McCann et al , 2018 ) ; analyzing network predictions on carefully curated datasets ( Linzen et al , 2016 ; Marvin and Linzen , 2018 ; Gulordava et al , 2018 ; Loula et al , 2018 ; Dasgupta et al , 2018 ; Tenney et al , 2018 ) ; and employing diagnostic classifiers to assess whether certain classes of information are encoded in a model 's ( intermediate ) representations ( Adi et al , 2016 ; Chrupa\u0142a et al , 2017 ; Hupkes et al , 2017 ; Belinkov et al , 2017 ) . While these approaches provide valuable insights into how neural networks process a large variety of phenomena , they rely on decoding accuracy as a probe for encoded linguistic information . If properly biased , this means that they can detect whether information is encoded in a representation or not . However , they do not allow for a direct comparison of representational structure between models . Consider a toy dataset of five sentences of interest and three encodings derived from quite different processing models ; a hidden state of a trained neural language model , a tf - idf weighted bag - of - words representation , and measurements of fixation duration from an eyetracking device . Probing methods do not allow us to quantify or visualise , for each of these encoding strategies , how the encoder 's responses to the five sentences relate to each other . Moreover , probing methods would not directly reveal whether the fixations from the eye - tracking device aligned more closely with the tf - idf representation or the states of the neural language model . In short , while probing classifier methods can establish if phenomena are separable based on the provided representations , they do not tell us about the overall geometry of the representational spaces . RSA , on the other hand , provides a basis for higher - order comparisons between spaces of representations , and a way to visualise and quantify the extent to which they are isomorphic . Indeed , RSA has seen a modest introduction within interpretable NLP in recent years . For example , Chrupa\u0142a et al ( 2017 ) employed RSA as a means of correlating encoder representations of speech , text , and images in a post - hoc analysis of a multi - task neural pipeline . Similarly , Bouchacourt and Baroni ( 2018 ) used the framework to measure the similarity between input image embeddings and the representations of the same image by an agent in an language game setting . More recently , Chrupa\u0142a and Alishahi ( 2019 ) correlated activation patterns of sentence encoders with symbolic representations , such as syntax trees . Lastly , similar to our work here , Abnar et al ( 2019 ) proposed an extension to RSA that enables the comparison of a single model in the face of isolated , changing parameters , and employed this metric along with RSA to correlate NLP models ' and human brains ' respective representations of language . We hope to position our work among this brief survey and further demonstrate the flexibility of RSA across several levels of abstraction .", "entities": [[274, 277, "TaskName", "pretrained language models"], [654, 655, "MetricName", "accuracy"], [967, 968, "DatasetName", "agent"]]}
{"text": "RSA was proposed by Kriegeskorte et al ( 2008 ) as a method of relating the different representational modalities employed in neuroscientific studies . Due to the lack of correspondence between the activity patterns of disparate measurement modalities ( e.g. brain activity via fMRI , behavioural responses ) , RSA aims to abstract away from the activity patterns themselves and instead compute representational dissimilarity matrices ( RDMs ) , which characterize the information carried by a given representation method through dissimilarity structure . Given a set of representational methods ( e.g. , pretrained encoders ) M and a set of experimental conditions ( sentences ) N , we can construct RDMs for each method in M . Each cell in an RDM corresponds to the dissimilarity between the activity patterns associated with pairs of experimental conditions n i , n j N , say , a pair of sentences . When n i = n j , the dissimilarity between an experimental condition and itself is intuitively 0 , thus making the N \u00d7 N RDM symmetric along a diagonal of zeros ( Kriegeskorte et al , 2008 ) . The RDMs of the different representational methods in M can then be directly compared in a Representational Similarity Matrix ( RSM ) . This comparison of RDMs is known as second - order analysis , which is broadly based on the idea of a second - order isomorphism ( Shepard and Chipman , 1970 ) . In such an analysis , the principal point of comparison is the match between the dissimilarity structure of the different representa - tional methods . Intuitively , this can be expressed through the notion of distance between distances , and is thus related to Earth Mover 's Distance ( Rubner et al , 2000 ) . 1 Figure 1 shows an illustration of the first and second order analyses for pretrained language encoders . Note that RSA is meaningfully different from , and complementary to , methods that employ saturating functions of representation distances ( e.g. decoding accuracy , mutual information ) , which suffer from ( a ) a ceiling effect : being able to distinguish experimental phenomenon A from B with with an accuracy of 100 % and experimental phenomenon C from D with an accuracy of 100 % does not mean that the distance between A and B is the same as that between C and D ; and ( b ) discretization ( Nili et al , 2014 ) . We follow Kriegeskorte et al ( 2008 ) in using the correlation distance of experimental condition pairs n i , n j N as a dissimilarity measure , wheren i is the mean of n i 's elements , is the dot product , and is the l 2 norm : corr ( x ) = 1 \u2212 ( n i \u2212n i ) ( n j \u2212n j ) ( n i \u2212n i 2 ( n j \u2212n j 2 . Compared to other measures , correlation distance is preferable as it normalizes both the mean and variance of activity patterns over experimental conditions . Other popular measures include the Euclidean distance and the Malahanobis distance ( Kriegeskorte et al , 2006 ) .", "entities": [[167, 168, "DatasetName", "0"], [343, 344, "MetricName", "accuracy"], [371, 372, "MetricName", "accuracy"], [383, 384, "MetricName", "accuracy"]]}
{"text": "We use two product categories , namely , Home&Kitchen and Sports&Outdoors for our analysis from the dataset mentioned in Section 2.1 after combining the question - answer and review dataset with the Product IDs . We will denote the two categories as Home and Sports , respectively . We use the same data split from OAAG 1 to retrain the models . Since there is no validation dataset , we take the 10 % of the train data as validation data . Table A.1 in the Appendix shows the details of training , validation , and test split . We keep all the hyper - parameters the same as the OAAG and CHIME . We train all the OAAG models for 20 epochs and CHIME models for 3 epochs , and the model that performs the best on the validation set is used to evaluate the test set . We evaluate the model with ROUGE metric and report the F1 scores for ROUGE - 1 ( R1 ) and ROUGE - L ( RL ) , which measure the word overlap and the longest common sequence between the reference answer and the generated answer , respectively . We obtain the ROUGE scores using rouge - score 2 package . Both the models use the BM25 algorithm to retrieve relevant reviews using the questions in the test dataset . We refer to this test setting as BM25Q.", "entities": [[66, 68, "DatasetName", "validation dataset"], [159, 160, "MetricName", "F1"], [169, 172, "MetricName", "ROUGE - L"]]}
{"text": "In this paper , we extensively analyze the generative approach of question - answering in e - commerce using a state - of - the - art OAAG model ( Deng et al , 2020 ) and CHIME model ( Lu et al , 2020 ) . We find many shortcomings which need to be addressed for a reliable PQA system . We try to address four re - search questions related to the generative approach for PQA , such as how the models utilize the reviews , how it performs on diverse question types , whether it is biased toward frequent phrases in training data , and the correctness of the generated response . We hope that our analysis will lead to more rigorous PQA research . A.2 Generated Answers with High R1 Score R1 score . In Table A.2 , in the first and the second example , the generated answers are exactly the opposite of the reference answers . In the third example , the question was about sweating of the bottle and straw cover , but the answer does not address any of these . In the fourth example , the answer is ambivalent . The last example contains a frequently occurring phrase \" I do n't know \" with a very high R1 score .", "entities": [[30, 33, "DatasetName", "Deng et al"], [134, 135, "MetricName", "Score"]]}
{"text": "Total Retained A comparison with the organizer - provided parallel training data used in our WMT18 system ( which is largely the same as the provided parallel data for WMT19 in the Russian - English language pair ) on baseline Marian transformer systems with identical training conditions show that aggressive language ID based filtering yields an approximate +1 BLEU point improvement as measured by SacreBLEU ( Post , 2018 ) . These results are shown in Table 2 .", "entities": [[58, 59, "MetricName", "BLEU"], [64, 65, "MetricName", "SacreBLEU"]]}
{"text": "As with last year 's efforts , we train multiple Marian ( Junczys - Dowmunt et al , 2018 ) models with both University of Edinburgh 's \" bi - deep \" and Google 's transformer ( Vaswani et al , 2017 ) architectures . Network hyperparameters are the same as detailed in Gwinnup et al ( 2018 ) . We again use newstest2014 as the validation set during training . Utilizing the best - performing BPE parameters from Section 2.2 , we first trained a baseline system in each of the two network architectures , noting the Transformer system 's better performance of +0.82 BLEU on average across decoded test sets . An additional six distinct transformer models were then independently 3 trained for use in ensemble decoding . We then ensemble decoded test sets with all eight models . Marian typically assigns each model used in ensemble decoding a feature weight of 1.0 ; thus each model contributes equally to the decoding process . Borrowing from our Moses training approach , we utilize a multi - iteration decode and optimize feature weights using the \" Expected Corpus BLEU \" ( ECB ) metric with the Drem optimizer ( Erdmann and Gwinnup , 2015 ) . We experimented using newstest2014 and newstest2017 as tuning sets - 2017 did not help performance , but using 2014 did improve performance by up to +0.9 BLEU 4 over the non - tuned ensemble . Scores for all the above - mentioned systems are shown in Table 4 . The best - performing ensemble ( ensemble tune14 ) was used in system combination .", "entities": [[33, 34, "DatasetName", "Google"], [76, 77, "MethodName", "BPE"], [98, 99, "MethodName", "Transformer"], [105, 106, "MetricName", "BLEU"], [189, 190, "MetricName", "BLEU"], [198, 199, "HyperparameterName", "optimizer"], [233, 234, "MetricName", "BLEU"]]}
{"text": "As in previous years , we trained a phrase - based Moses ( Koehn et al , 2007 ) system with the same data as the Marian system outlined in Section 3.1 in order to provide diversity for system combination . This system employed a hierarchical reordering model ( Galley and Manning , 2008 ) and 5 - gram operation sequence model ( Durrani et al , 2011 ) . The 5 - gram English language model was trained with KenLM on all permissable monolingual English news - crawl data . The BPE model used was applied to both the parallel training data and the language modeling corpus . System weights were tuned with the Drem ( Erdmann and Gwinnup , 2015 ) optimizer using the \" Expected Corpus BLEU \" ( ECB ) metric .", "entities": [[92, 93, "MethodName", "BPE"], [123, 124, "HyperparameterName", "optimizer"], [129, 130, "MetricName", "BLEU"]]}
{"text": "Jane system combination ( Freitag et al , 2014 ) was employed to combine outputs from the best systems from each approach outlined above . Individual component system and final combination scores are shown in Table 6 for cased , detokenized BLEU and BEER 2.0 ( Stanojevi\u0107 and Sima'an , 2014 ) .", "entities": [[41, 42, "MetricName", "BLEU"]]}
{"text": "The dataset comprises randomly chosen comments from the Globe and Mail news site ( sampled from the SFU Opinion and Comment Corpus dataset ) ( Kolhatkar et al , 2019 ) , of 250 characters or less . Comment scores were crowdsourced using Figure Eight ( now Appen ) . The annotation job consisted of 588 crowdworkers ( annotators ) providing 244468 judgements on 44355 comments . 2 Each annotator was asked to identify for each comment whether it was healthy and if any of the attributes were present , in the form of a standard questionnaire ( see Appendix A ) . Annotators were not given any wider context or additional information about where a comment was posted or how it was engaged with by other users . To both accommodate and attempt to resolve meaningful disagreement , we applied a dynamic judgement method which requests additional annotations for those comments on which there was insufficient consensus ( either yes or no with a confidence of less than 75 % ) . All comments were annotated at least three times , and more annotators were added , up to a limit of five annotators per comment until sufficient consensus was reached . Annotation Job Refinement . The inherent subtlety , subjectivity , and frequent ambiguity of the attributes covered in this dataset make crowdsourcing quality attribute labels an unavoidably difficult process . Typically the goal in an annotation task would simply be to maximise agreement between the multiple annotators of each comment . However , when the annotation task is inherently subjective and meaningful difference of opinion is itself valuable data , the goal becomes instead to maximise common understanding of the task across annotators . This entails tailoring the phrasing of the questions put to annotators , so as to create as common an understanding as possible of what each question is really asking . This way , disagreement between annotators reflected in the dataset will represent different reasonable readings of the same comment which are themselves important to capture . In research on irony and sarcasm , for example , Filatova noted the difficulty even among expert researchers in formally defining these terms ( Filatova , 2012 ) . For the other attributes included in this dataset which are as ( if not more ) ambiguous and subtle than sarcasm , we expect this to hold true as well . The exact wording of each question on the questionnaire went through multiple iterations , tested by smaller scale experiments to evaluate effectiveness . The quality of the resulting data was evaluated manually by our team , calculating the proportion of perceived mistaken annotations and their ' severity ' : to what extent a judgement was ' obviously wrong ' , as opposed to an understandable alternative reading of a comment . We found that providing annotators with precise and more comprehensive definitions of each attribute was not more likely to produce interannotator agreement or better quality data . Neither , however , were best results produced by asking simple , ' yes or no ' questions such as ' Is this comment dismissive ? ' for all attributes . The best results were achieved by relying primarily on annotators implicit understandings of and intuitions about the attributes , aided by brief inline explanations . We added explanations to avoid mistakes for those attributes which are more ambiguous , and for which our smaller tests had indicated required further guidance . These can be seen in the questionnaire included as Appendix A. To ensure that disagreement reflects reasonable difference of opinion , rather than inattention or misunderstanding of the task , it is necessary to apply a method of quality control . The attempt to create a labeled dataset is premised on the assumption of some ' ground truth ' ; that it is possible for comments to have labels and confidence scores accurately representing the presence of one or more attributes to some extent . However , the extent to which a comment displays one or more attribute is subjective , and the scores would be unhelpful if they did not capture what a wider and more diverse audience than our team of authors would understand the comments to mean . Our process of quality control therefore aimed to reduce the number of ' bad ' annotators , those who either do not understand or appropriately engage with the task , while still allowing for differences of opinion . Our primary quality control mechanism was to collate a set of ' test comments ' , for which we had manually established the correct answers . Annotators encountered one test comment per batch of seven comments they reviewed , without knowing which of the seven was the test comment , and their running accuracy on these test comments was defined as their ' trustworthiness score ' . The task required that annotators maintain a trustworthiness score of more than 78 % . If an annotator dropped below this level , they were removed from the annotator pool for this task , and all of their prior annotations were discarded 3 . The removed ' bad ' annotator judgements were replaced by newly collected trusted judgements as necessary . We restricted our test comments to what were ( in our view ) clear and definitive examples of the attributes , such that one would fail on the test comments only if one has an incorrect understanding of what is meant by a particular attribute . In the course of our preliminary small - scale refining iterations of the questionnaire , analysis of responses revealed some recurring misunderstandings or mistakes . For example , a common error was to label all non - sarcastic humour as sarcasm , or to conflate polite disagreement with dismissiveness . As a result , we identified and included specific test comments , drawn from real examples , aimed at reducing these common errors . We included very few test comments for the higher level question on whether a comment belongs in a healthy conversation . Any test questions on this topic were very extreme examples , such as highly abusive explicit comments , to ensure that annotators were not randomly answering that question . We had two reasons for minimising the use of test comments for this question . Firstly , since this was in our view the most open - ended question , it is difficult to establish tests on the basis of which to exclude annotators . Secondly , allowing greater annotator discretion on this question provides insight on whether there is a correlation between the six attributes and being labelled as unhealthy . 4", "entities": [[759, 760, "DatasetName", "collate"], [805, 806, "MetricName", "accuracy"]]}
{"text": "The dataset comprises a total of 44355 comments labelled ' yes ' or ' no ' for each attribute , along with a confidence score for each label . The labels and corresponding confidence scores for each attribute are based on an aggregation of the answers given by different annotators , weighted by their respective ' trustworthiness ' scores . As an example to demonstrate this process , consider a comment annotated by 5 annotators with trustworthiness scores 0.78 , 0.85 , 0.9 , 1.0 , and 0.95 , who judge a comment for a particular attribute with judgements ' yes ' , ' yes ' , ' yes ' , ' no ' , ' yes ' respectively . Let T be the sum of their trustworthiness scores , and T y , T n the sum of the trustworthiness scores of those who answered ' yes ' and ' no ' respectively . The label is then determined by which of T y or T n is larger , in this case it is T y , and the confidence score is T y /T , in this case 0.78 . The proportion of comments that contain each attribute is shown in Table 1 As the comments were sampled from the SFU Opinion and Comment Corpus dataset , the prevalence for each attribute is inevitably low . Despite the label imbalance , the dataset represents an important contribution to identification of this wider variety of subtle attributes , with thousands of positive examples for each . Our manual analysis during initial iterations of the annotation job indicated that 4 There remains a clear methodological issue with using this data for comparing the set of comments classed as ' unhealthy ' with those classed as one or more of the other attributes : having been asked all questions as part of the same questionnaire , annotators may have been primed to associate the attributes with ' unhealthiness ' , even if they would not have done so otherwise . ( Rosenblatt et al , 1956 ) of confidence scores for each attribute . Figure 2a shows confidence scores for those comments labelled as ' no ' for each unhealthy attribute , while Figure 2b represents those of comments labelled ' yes ' . these final proportions are roughly representative of the prevalence of these attributes in similar live contexts , such as North American online newspaper comment sections . There are specific attributes , notably sarcasm , for which it can be possible to collate a corpus of self - labelled data , for example by scraping tweets with ' # sarcastic ' from Twitter , or comments followed by ' /s ' on Reddit ( Khodak et al , 2018 ) . In these specific circumstances , the avoidance of the need to crowdsource and pay for annotations can permit much larger and more balanced datasets . However , for all other attributes we consider , and in fora like the comment sections of news sites , relying on self - labelled data is not possible . For these attributes , crowdsourcing is the only feasible way to obtain high quality data , and as such we would expect proportions reflecting those observed in similar contexts . Inspection of random subsets of the new UCC dataset reveals that the data is generally of a high quality , and captures important nuances , accurately identifying these subtle attributes , both when they overlap ( as is common ) , and also when they do not ( see Figure 3 for examples ) . Figure 4 shows the correlations between attributes , calculated based on the pool of comments which are labelled as one or more of the six unhealthy attributes . The figure highlights two important facts . First , the relatively low correlation between most attributes indicates that the dataset succeeds in differentiating between these different types of subtle unhealthy attributes . As expected , there is significant correlation between antagonistic and hostile comments . There is some correlation between the often more subtle attributes like dismissiveness / condescension and antagonism , while these are less correlated with hostility . We also include correlations with the ' toxicity ' scores produced by Jigsaw 's Perspective API ( perspectiveapi.com ) , which again confirms that our attributes , in particular those other than antagonistic and hostile , capture something distinct from overt toxicity . A notable feature of Figure 4 is the slightly negative correlations between sarcasm and other attributes , indicating that annotators generally did not associate sarcasm with other unhealthy attributes . Secondly , ' unhealthy ' correlates significantly with antagonism and hostility , but very little with the other attributes , indicating a fairly broad general notion of healthy conversation on the part of the annotators , which mostly includes dismissive , condescending , sarcastic and generalising comments . Despite its generally high quality , the nature of the task and the annotation method entails some level of noise in the dataset . This noise is particularly difficult to quantify given the need to distinguish between different but reasonable interpretations of a comment , and simply incorrect annotations caused by a lack of understanding or care on the part of an annotator ( for example , one comment reading \" You are an ignorant * sshole \" was judged not to be needlessly hostile , an obvious error ) . This highlights the difficulties of using traditional reliability metrics like Krippendorff 's \u03b1 for crowdsourced annotations on subjective tasks ( D'Arcey et al , 2019 ) . Krippendorff 's \u03b1 is a number between 0 and 1 intended to indicate the extent to which annotators agree compared with what would have happened if they guessed randomly . The base assumption then is that all disagreement between annotators decreases reliability , which is not necessarily the case for subjective attributes ( Salminen et al , 2018b ; Swanson et al , 2014 ) . Despite the above caveat , we conduct analysis using Krippendorff 's \u03b1 ( K - \u03b1 ) for two reasons . Firstly , to allow for comparison with other literature in the field , we report the K - \u03b1 for judgements on each attribute in Table 2 . They range from 0.31 - 0.39 , which is comparable with other datasets labelling ' similar ' phenomenon , such as sarcasm ( 0.24 - 0.38 ) ( Swanson et al , 2014 ; Justo et al , 2018 ; D'Arcey et al , 2019 ) , and hate speech with sub - attributes from Figure Eight annotators ( 0.21 ) ( Lazaridou et al , 2020 ) . The one exception is the set of judgements on whether a comment has a place in a healthy conversation , with a lower K - \u03b1 of 0.26 . Given that this is a more open - ended question , this is not necessarily surprising . Secondly , to the extent that K - \u03b1 is an important reliability metric for this form of data , it supports our use of ' trustworthiness ' scores when aggregating judgements on a given comment to decide labels and confidence scores . Specifically , as shown in Figure 5 , we see that as we increase the trustworthiness threshold for annotators whose judgements are included , the resulting K - \u03b1 steadily increase . This provides some indication that our trustworthiness scores do capture the reliability of our annotators , and thus that their judgements ought to be weighted more highly in the final confidence in a comment 's labels .", "entities": [[425, 426, "DatasetName", "collate"], [455, 456, "DatasetName", "Reddit"], [556, 557, "DatasetName", "UCC"], [714, 715, "MethodName", "Jigsaw"], [926, 927, "HyperparameterName", "\u03b1"], [943, 944, "HyperparameterName", "\u03b1"], [948, 949, "DatasetName", "0"], [1018, 1019, "HyperparameterName", "\u03b1"], [1022, 1023, "HyperparameterName", "\u03b1"], [1046, 1047, "HyperparameterName", "\u03b1"], [1104, 1106, "DatasetName", "hate speech"], [1150, 1151, "HyperparameterName", "\u03b1"], [1179, 1180, "HyperparameterName", "\u03b1"], [1242, 1243, "HyperparameterName", "\u03b1"]]}
{"text": "Use of a pre - trained BERT model ( Devlin et al , 2019 ) and fine - tuning on this dataset produces classifiers with modest performance ( Figure 6 ) , compared to the state of the art for sequence classification . The best performing attributes , ' hostile ' and ' antagonistic ' are also those most similar to the types of attributes typically annotated in comment classification work . The other attributes seem to cluster together , with the ' sarcastic ' label particularly noteworthy for its low performance . To give context to the model performance , we follow ( Wulczyn et al , 2017 ) and compare our performance with human workers . For each comment , we hold out one annotator to act as our ' human model ' and use the aggregated score of the other annotators as the ground truth to compute the ROC AUC . To stabilize our results , this procedure is repeated five times and the average reported . We use the same test sets to compute the ROC AUC of the trained BERT model and average those scores as well . As we can see , for all attributes other than ' sarcastic ' the BERT model outperforms a randomly selected human annotator , indicating that it has sufficiently captured the semantic and syntactic structures for these attributes . For ' sarcastic ' , the gap between the BERT model and human annotators indicates a rich area for studying whether model performance can be improved .", "entities": [[6, 7, "MethodName", "BERT"], [151, 153, "MetricName", "ROC AUC"], [179, 181, "MetricName", "ROC AUC"], [184, 185, "MethodName", "BERT"], [207, 208, "MethodName", "BERT"], [240, 241, "MethodName", "BERT"]]}
{"text": "We found that CNN - based systems obtained the best results for some error types , likely due to some characteristics derived from CNN . We trained four CNN - based ensemble systems , using the model architecture in ( Chollampatt and Ng , 2018 ) , but without reranking . Four best combinations to build the ensemble systems were selected . Unlike ( Chollampatt and Ng , 2018 ) , we did not use fastText ( Bojanowski et al , 2017 ) to initialize word embeddings because we found no improvement on the development set by doing that . We tuned parameters for the system , such as batch size , word embedding dimension , etc .", "entities": [[75, 76, "MethodName", "fastText"], [85, 87, "TaskName", "word embeddings"], [109, 111, "HyperparameterName", "batch size"], [112, 115, "HyperparameterName", "word embedding dimension"]]}
{"text": "We expect to combine these models trained above into a more powerful system through effective ensemble methods . Our ensemble work mainly focuses on rule - based solutions . We will introduce two main modules first . Confidence Table We can obtain the precision and F 0.5 metric on each error type through sentence alignment and error type classification by Errant ( Bryant et al , 2017 ) . Errant provides performance statistics based on 55 error types and is also the tool used to evaluate this GEC shared task , thus we use the result of operation and error type span - level ( Bryant et al , 2017 ) for a model or system as the confidence table . Conflict Solver We often encounter GEC error conflicts when combining multiple models or systems . For example , We love played soccer . One system corrects played to playing , while another system may correct played to to play . When two different corrections occur in the same place , we need to consider which one to choose . We solve this problem in a unified pipeline , which can also be seen as an ensemble way : ( 1 ) We sort each group of conflicting corrections proposed by all the systems in a reverse order of location index and confidence . ( 2 ) We apply three sub - strategies : When combining outcomes from different systems , we treat the precision in a confidence table as the confidence . Each correction has its confidence obtained by looking up the precision of the corresponding type of the correction in the table . If two conflicting corrections are the same , we merge them and add \u03b1 to the confidence of the correction ; otherwise , the correction with a lower confidence will be discarded . After combining outcomes , if the confidence of a correction is lower than \u03b2 , the correction is discarded . \u03b3 is used to distinguish when it is more important to focus on the precision or F 0.5 of a correction . When we move to the final ensemble with confidence tables of existing systems , if the confidence is larger than \u03b3 , we select the correction proposed by the system that has the best F 0.5 on the type of this correction . Otherwise , the correction by a system with the best precision is selected . In Figure 1 , \u2297 means the outcome is obtained by combining two systems represented by intersecting lines of two different colours . If there are multiple \u2297 on a line , it means the ensemble is over all of these \u2297 on this line . Figure 1 displays three types of ensemble methods based on all of the CNN - based and Transformer - based translation models . Combine each CNN - based ensemble model with each of the selected five of the Transformer - based models . This is noted as ' ensemble - by - 2 ' . Perform ensemble over all of the ensemble models relating to either CNN ensemble 1 or CNN ensemble 2 , noted as EoE ( Ensemble over Ensemble ) 1 and 2 . Ensemble each CNN ensemble model with some selected combinations of Transformer - based models to produce 16 strong ensemble system outcomes , represented as ' Hybrid Ensemble ' in Figure 1 . It is where multiple lines of the same color are merged into one line in Figure 1 . After getting all of the ensemble outcomes , we will do the final ensemble step : select the best confidence for each type from each single or ensemble system to form the strongest final outcome . In this ensemble step , we use the last aforementioned sub - strategy , and discard the error types with very low confidence to boost the final performance .", "entities": [[288, 289, "HyperparameterName", "\u03b1"], [321, 322, "HyperparameterName", "\u03b2"], [328, 329, "HyperparameterName", "\u03b3"], [370, 371, "HyperparameterName", "\u03b3"], [470, 471, "MethodName", "Transformer"], [491, 492, "MethodName", "Transformer"], [549, 550, "MethodName", "Transformer"]]}
{"text": "We trained eight Transformer - based translation models in different combinations of error adaptation , domain adaptation , and GPU set . In Table 4 , we notice that a smaller error weight yields higher precision and a slight decrease in recall . We set the copy number as 8 , 10 and 15 , and find that domain adaptation has no significant effect on the results . 4 GPU is obviously better than 2 GPU sets , which is probably because of the larger batch size accumulation for gradient calculation .", "entities": [[3, 4, "MethodName", "Transformer"], [15, 17, "TaskName", "domain adaptation"], [58, 60, "TaskName", "domain adaptation"], [85, 87, "HyperparameterName", "batch size"]]}
{"text": "As described in Section 2.1.3 , we need to ensemble all of the CNN - based and Transformer - based translation models . We have already introduced the configuration of the single models in Section 3.2.1 and Section 3.2.2 . Next we will describe the configuration of the ensemble system . For the three ensemble types : Ensemble - by - 2 , EoE and Hybrid Ensemble , as shown in Figure 1 , we used different parameters in the conflict solver . We did a small - scale grid search for the parameters in Table 5 . When combining two models that are not strong , we expect a higher recall so \u03b2 was not high . For EoE and hybrid ensemble , we expect a higher precision so that they can provide high quality single type performance . Corrections proposed by multiple models are given higher weights ( controlled by \u03b1 ) . If the confidence of a correction finally reaches \u03b2 , the correction will be adopted . In the final ensemble , we select the best performance on each type from each single system or ensemble system and discard the corrections with low precision ( controlled by \u03b2 ) . To get higher F 0.5 , in the case where the precision is greater than a predefined threshold ( controlled by \u03b3 ) , we will choose the model with the highest F 0.5 for the corresponding error type . The final outcome of the data set is then fed through the translation models and ensemble systems again to do a second pass correction .", "entities": [[17, 18, "MethodName", "Transformer"], [113, 114, "HyperparameterName", "\u03b2"], [152, 153, "HyperparameterName", "\u03b1"], [163, 164, "HyperparameterName", "\u03b2"], [201, 202, "HyperparameterName", "\u03b2"], [225, 226, "HyperparameterName", "\u03b3"]]}
{"text": "Almost all argument mining frameworks proposed so far employ a pipeline of stages , each of which is addressing a sub - task of the argument mining problem ( Lippi and Torroni , 2015a ) . The segmentation of text into argumentative units is typically the first sub - task encountered in such an argument mining pipeline , aiming to segment texts into argumentative and non - argumentative text units ( i.e. segments that do contain or do not contain argument components , such as claims or premises ) . The granularity of argument components is text - dependant . For example , in Wikipedia articles studied in ( Rinott et al , 2015 ) , argument components spanned from less than a sentence to more than a paragraph , although 90 % of the cases was up to 3 sentences , with 95 % of components being comprised of whole sentences . Several approaches address the identification of argumentative units at the sentence level , a subtask known as \" argumentative sentence detection \" , which typically models the task as a binary classification problem . Employing machine learning and a set of features representing sentences , the goal is to discard sentences that are not part ( or do not contain a component ) of an argument . As reported also by Lippi and Torroni ( 2015a ) , the vast majority of existing approaches employ \" classic , off - the - self \" classifiers , while most of the effort is devoted to highly engineered features . A plethora of learning algorithms have been applied on the task , including Naive Bayes ( Moens et al , 2007 ; Park and Cardie , 2014 ) , Support Vector Machines ( SVM ) ( Mochales and Moens , 2011 ; Rooney et al , 2012 ; Park and Cardie , 2014 ; Stab and Gurevych , 2014b ; Lippi and Torroni , 2015b ) , Maximum Entropy ( Mochales and Moens , 2011 ) , Logistic Regression ( Goudas et al , 2014 ( Goudas et al , , 2015Levy et al , 2014 ) , Decision Trees and Random Forests ( Goudas et al , 2014 ( Goudas et al , , 2015Stab and Gurevych , 2014b ) . However , approaches addressing this task in a semi - supervised or unsupervised manner are still scarce . In ( Petasis and Karkaletsis , 2016 ) an unsupervised approach is presented , which addresses the sub - task of identifying the main claim in a document by exploiting evidence from an extractive summarization algorithm , TextRank ( Mihalcea and Tarau , 2004 ) . In an attempt to study the overlap between graph - based approaches and approaches targeting extractive summarization with argument mining , evaluation results suggest a positive effect on the sub - task , achieving an accuracy of 50 % on the corpus compiled by Hasan and Ng ( 2014 ) from online debate forums and on a corpus of persuasive essays ( Stab and Gurevych , 2014a ) . Regarding semi - supervised approaches , Habernal and Gurevych ( 2015 ) propose new unsupervised features that exploit clustering of unlabeled argumentative data from debate portals based on word embeddings , outperforming several baselines . This work employs also topic modeling as one of its features , by including as features the distributions of sentences from LDA ( Blei et al , 2003 ) . Topic modeling has been mainly exploited for identification of argumentative relations and for extraction of argument and domain lexicons . In Lawrence et al ( 2014 ) , LDA is used to decide whether a proposition can be attached to its previous proposition in order to identify non directional relations among propositions detected through classifiers based on words and part - ofspeech tags . LDA has been also used to mine lexicons of argument ( words that are topic independent ) and domain words ( Nguyen and Litman , 2015 ) , by post - processing document topics generated by LDA . These lexicons have been used as features for supervised approaches for argument mining ( Nguyen and Litman , 2016a , b ) . However , to the best of our knowledge , no prior approach has applied topic modeling to argumentative sentence detection in an unsupervised setting , which is the featuring aspect of the proposed A2 T approach presented in the following .", "entities": [[2, 4, "TaskName", "argument mining"], [25, 27, "TaskName", "argument mining"], [54, 56, "TaskName", "argument mining"], [294, 295, "MethodName", "SVM"], [338, 340, "MethodName", "Logistic Regression"], [434, 436, "TaskName", "extractive summarization"], [462, 464, "TaskName", "extractive summarization"], [465, 467, "TaskName", "argument mining"], [482, 483, "MetricName", "accuracy"], [544, 546, "TaskName", "word embeddings"], [572, 573, "MethodName", "LDA"], [609, 610, "MethodName", "LDA"], [645, 646, "MethodName", "LDA"], [681, 682, "MethodName", "LDA"], [694, 696, "TaskName", "argument mining"]]}
{"text": "Sentence Index ( S ) Figure 1 : Schema of the A2 T approach s i is a sentence containing an argumentative unit , c is the text containing s , and l is the argumentative role expressed by the unit ( e.g. , major claim , claim , premise ) . The A2 T approach is articulated in the following activities : Sentence extraction . A2 T approach is characterized by the use of topic modeling at sentencelevel granularity . For this reason , a pre - processing step of the corpus C is enforced based on conventional techniques for sentence tokenization , words tokenization , normalization , and indexing ( Manning et al , 2008 ) . The result is a sentence set S = { \u2212 s 1 , c , pos 1 , . . . , \u2212 s m , c , pos m } , where \u2212 s i is the vector representation of the sentence s i and c , pos are text and position in the text where the sentence appears , respectively . The sentence set is stored in a sentence index for efficient access of S elements . Topic modeling . The set of extracted sentences S is used as the document corpus on which topic modeling is applied . The result of this activity is twofold . First , topic modeling returns a set of topics T = { t 0 , . . . , t k } representing the latent variables that are most representative for the sentences S. Second , topic modeling returns a distribution of sentences over topics \u03b8 = { \u03b8 s 1 , . . . , \u03b8 sm } . In particular , \u03b8 s i = [ p ( t 0 | s i ) , . . . , p ( t k | s i ) ] is the probability distribution of the sentence s i over the set of topics T , where p ( t j | s i ) represents the probability of the topic t j given the sentence s i ( i.e. , the so - called assignment value of s i to t j ) . Attraction evaluation . The notion of attraction is introduced to measure the degree of focus that characterizes sentences with respect to the emerged topics . To this end , the distribution of sentences over topics \u03b8 is exploited with the aim at determining the best topic assignment for each sentence of S. The result is an attraction set A = { s 1 , a 1 , . . . , s m , a m } where s i is a sentence of S and a i is its corresponding attraction value . Sentence labeling . By exploiting the attraction set A , labeling has the goal to determine the sentences of S that are more focused on a specific topic , according to the hypothesis that those sentences are the argumentative units . In a basic scenario , labeling consists in distinguishing between sentences that are argumentative units ( l = au ) and sentences that are not argumentative units ( l = au ) . In a more articulated scenario , labeling consists in assigning a role to sentences that are recognized as argumentative units . For instance , it is possible to distinguish argumentative - unit sentences that are claims ( l = cl ) , major claims ( l = mc ) , or premises ( l = pr ) . A sentence s recognized as argumentative unit is inserted in the final set U with the assigned label and it is returned as a result of A2 T .", "entities": [[241, 242, "DatasetName", "0"], [273, 274, "HyperparameterName", "\u03b8"], [276, 277, "HyperparameterName", "\u03b8"], [284, 285, "HyperparameterName", "\u03b8"], [291, 292, "HyperparameterName", "\u03b8"], [299, 300, "DatasetName", "0"], [408, 409, "HyperparameterName", "\u03b8"]]}
{"text": "In A2 T , the sentence extraction step is enforced by relying on standard techniques for representing documents in terms of feature vectors and bag of words ( using tf - idf as weighting scheme ) ( Castano et al , 2017 ) . Probabilistic topic modeling is exploited to enforce the subsequent topic modeling step . Probabilistic topic models are a suite of algorithms whose aim is to discover the hidden thematic structure in large archives of documents , namely sentences in A2 T . The idea is that documents are represented as random mixtures over latent topics , where each topic is characterized by a distribution over words ( Blei et al , 2003 ) . Probabilistic topic modeling algorithms infer the distribution \u03b8 of documents over topics and the distribution \u03c6 of words over topics , by sampling from the bag of words of each document . In our approach , we choose to exploit the Hierarchical Dirichlet Process ( HDP ) . With respect to other algorithms ( such as LDA ) , HDP has the advantage to provide the optimal number of topics instead of requiring to set such a number as input ( Teh et al , 2006 ) . Attraction evaluation . The notion of attraction is introduced in A2 T to capture the intuition that argumentative units are related to the distribution of sentences over topics . Consider a set of sentences S and the distribution \u03b8 of sentences over the set of topics T . The more the distribution \u03b8 s i of a sentence s i over the topics is unequal , the more s i is focused on a topic , thus suggesting s i as a possible argumentative unit . A further feature that attraction aims to capture is that argumentative units often appear either at the beginning or at the end of texts . The attraction a i of a sentence s i is calculated as follows : a i = K\u03d5 s i + ( 1 \u2212 K ) \u03c1 s i s j c \u03c1 s j , \u03d5 s i = max ( \u03b8 s i ) is a measure of how much s i is focused on a topic and \u03c1 s i = \u03b1f ( pos i ) 2 + \u03b2f ( pos i ) + \u03b3 is a parabolic function over the position of the sentence in c. In particular , given L ( c ) as the number of sentences in c , f ( pos i ) = L ( c ) 2 \u2212 pos i such that f ( pos i ) is higher when s i appears either at the beginning or at the end of c. The parameters \u03b1 , \u03b2 , \u03b3 determine the shape of \u03c1 s i . K [ 0 , 1 ] is a constant value used to balance the role of focus and position in calculating the attraction . The attraction a i can be interpreted as the probability of a sentence s i to contain an argumentative unit . According to this interpretation , given s i , also the contiguous sentences s i\u22121 and s i+1 have a chance to be argumentative units . As a result , given the calculated attraction set A , we update the attraction values a i through an interpolation mechanism based on the Savitzky - Golay smoothing filter ( SGF ) ( Savitzky and Golay , 1964 ) , so that A : = SGF ( A ) . In Figure 2 , an example of attraction evaluation is provided by showing the values of \u03d5 , \u03c1 , attraction , and interpolated attraction for all the sentences within one considered student essays included in the corpus from ( Stab and Gurevych , 2014a ) ( see Section 4 ) . Sentence labeling . Sentence labeling has the goal to turn attraction values into labeled categories . Consider a set of possible labels L = { l 1 , . . . , l g } , each one denoting a possible argumentative role that can be assigned to a sentence . Given a set of attraction values A , a thresholdbased mechanism is enforced to assign labels to sentences according to the following scheme : a i < \u03c4 1 : s i l 1 \u03c4 1 \u2264 a i < \u03c4 2 : s i l 2 . . . . . . . . . a i \u2265 \u03c4 g\u22121 : s i l g where \u03c4 1 < \u03c4 2 < ... < \u03c4 g\u22121 ( \u03c4 1 , . . . \u03c4 g\u22121 ( 0 , 1 ] ) are prefixed threshold values . The result of sentence labeling is a partition of S into g categories with associated labels . In the experiments , we discuss two different strategies for sentence labeling . The first one is a two - class labeling strategy where the possible labels for a sentence are argumentative unit ( au ) and non - argumentative unit ( au . The second strategy is a multi - class labeling in which the possible labels of a sentence are non - argumentative unit au , premise ( pr ) , claim ( cl ) , and major claim ( mc ) .", "entities": [[58, 60, "TaskName", "topic models"], [125, 126, "HyperparameterName", "\u03b8"], [174, 175, "MethodName", "LDA"], [244, 245, "HyperparameterName", "\u03b8"], [258, 259, "HyperparameterName", "\u03b8"], [359, 360, "HyperparameterName", "\u03b8"], [394, 395, "HyperparameterName", "\u03b3"], [462, 463, "HyperparameterName", "\u03b1"], [464, 465, "HyperparameterName", "\u03b2"], [466, 467, "HyperparameterName", "\u03b3"], [477, 478, "DatasetName", "0"], [787, 788, "DatasetName", "0"]]}
{"text": "For evaluation of the proposed A2 T approach , we have used two English corpora . The first corpus ( C1 in the following ) is a collection of 90 student persuasive essays ( Stab and Gurevych , 2014a ) which has been manually annotated with major claims ( one per essay ) , claims and premises at the clause level . In addition , the corpus contains manual annotations of argumentative relations , where the claims and premises are linked , while claims are linked to the major claim either with a support or an attack relation . Interannotation agreement has been measured to unitized alpha ( Krippendorff , 2004 ) \u03b1 U = 0.724 . These 90 essays consist of a total of 1 , 675 sentences ( from which 19.3 % contain no argument components ) , with an average length of 18.61 \u00b1 7 sentences per essay , while the 5.4 % of sentences contain a major claim , 26.4 % contain a claim , and 61.1 % contain a premise . The second corpus ( C2 in the following ) has been compiled and manually annotated as described in ( Habernal and Gurevych , 2017 ) . This corpus focuses on user generated content , including user comments , forum posts , blogs , and newspaper articles , covering several thematic domains from educational controversies , such as homeschooling , private vs. public schools , or singlesex education . Containing in total 340 documents , the corpus has been manually annotated with an argument scheme based on extended Toulmin 's model , involving claims , premises , and backing , rebuttal , refutation argument units . The corpus contains documents of various sizes , with a mean size of 11.44 \u00b1 11.70 sentences per document , while the inter - annotator agreement was measured as \u03b1 U = 0.48 . The corpus consists of 3 , 899 sentences , from which 2 , 214 sentences ( 57 % ) contain no argument components . Both corpora have been preprocessed with NLTK ( Loper and Bird , 2002 ) in order to identify tokens and sentences . Then , each sentence was annotated as argumentative or non - argumentative , depending on whether it contained an argument unit ( i.e. a text fragment annotated as major claim , claim , or premise ) . In addition , each argumentative sentence was further annotated with one of major claim , claim , and premise , based on the type of the contained argumentative unit . For the second corpus , which utilizes a richer argument scheme , we have considered backing , rebuttal and refutation units as premises . This second corpus does not contain units annotated as major claims . The following three tasks have been executed : Task 1 : Argumentative sentence identification - given a sentence , classify whether or not it contains an argument component . Task 2 : Major claim identification - given a argumentative sentence , classify whether or not it contains a major claim . Task 3 : Argumentative sentence classification - given a sentence , classify the sentence as major claim , claim , premise , or nonargumentative . Baseline . As a baseline for comparison against our approach , we created a probabilistic classifier of sentences which evaluates the probability p ( l = au | s i ) as follows . Given the text c containing L ( c ) sentences s i , let be \u03b6 c \u223c Dir ( \u03b1 ) the probability distribution of the sentences in c , such that \u03b6 s i c \u223c p ( l = au | s i ) . The L ( c ) parameters \u03b1 used to generate \u03b6 c are defined such that \u03b1 i = L ( c ) 2 \u2212 pos i . The rationale of this procedure is to bias the random assignment of a sentence to the au label in favor of sentences appearing either in the beginning or in the end of a text . This bias attempts to model empirical evi - dence that in several types of documents , the density of argumentative units in various sections of documents depends on the structure of documents . The beginning and end of a document are expected to contain argumentative units in structured documents like news , scientific publications , or argumentative essays ( Stab and Gurevych , 2017 ) , where major claims and supporting premises are frequently found in the beginning of documents , with documents frequently ending with repeating the major claims and supporting evidence .", "entities": [[106, 107, "HyperparameterName", "alpha"], [112, 113, "HyperparameterName", "\u03b1"], [310, 311, "HyperparameterName", "\u03b1"], [519, 521, "TaskName", "sentence classification"], [594, 595, "HyperparameterName", "\u03b1"], [628, 629, "HyperparameterName", "\u03b1"], [638, 639, "HyperparameterName", "\u03b1"]]}
{"text": "The goal of Task 1 is to associate each sentence of the corpora to a label in L = { au , au } by following a two - class labeling strategy ( see Section 3 ) . As a first experiment , we performed sentence labeling with different threshold ranging from 0 to 1 with step 0.05 . In Figure 3 , we report the precision , recall , and F1 - measure for A2 T and for the baseline . In addition , we report also the results of applying sentence labeling based on \u03d5 and \u03c1 ( the components of attraction ) separately . The parameter K for attraction calculation has been set to 0.5 . Since A2 T is an unsupervised method , there is no easy way to define the threshold parameter \u03c4 , which has been empirically defined to \u03c4 = 0.3 . The different behavior of A2 T with respect to the baseline is shown in the confusion matrices reported in Figures 4 and 5 . From Figure 3 , we can see that A2 T is significantly better than the baseline , especially for the C1 corpus . A characteristic of this corpus is that argumentative units are frequently located in the introduction or the conclusion of an essay , which is also reflected by the baseline that achieved an F1 - measure of 0.35 for a threshold of \u03c4 = 0.05 ( with the baseline being particularly precise , suggesting that argumentative units are very frequently at the beginning and end of essays ) . Both components of attraction ( \u03d5 and \u03c1 ) perform well , with the topic component \u03d5 being slightly better than position information \u03c1 , both in precision and recall . The results are similar for corpus C2 , with A2 T surpassing the baseline , although A2 T advantage in precision is smaller . As shown in the confusion matrix of Figure 5 , the main source of error is the large number of false positives for the au class , proposing more argumentative units than what have been manu - ally identified in corpus C2 . This can be attributed to the sparseness of argumentative units in the C2 corpus , with almost 60 % of the sentences being non - argumentative .", "entities": [[52, 53, "DatasetName", "0"], [71, 72, "MetricName", "F1"], [228, 229, "MetricName", "F1"]]}
{"text": "The goal of Task 3 is to associate each sentence of the corpora to a label in L = { au , pr , cl , mc } by following a multi - class labeling strategy ( see Section 3 ) . In particular , we adopted the thresholds [ 0.1 , 0.3 , 0.5 ] . This task is challenging since it is required to distinguish the different role played in argumentation by sentences that are often very similar from the terminological point of view . The confusion matrix for corpus C1 is shown in Figure 6 , while Figure 7 shows the confusion matrix for corpus C2 . Both A2 T and the baseline achieve low results , but the accuracy of A2 T is 0.3 against the 0.1 of the baseline . From Figure 6 we see that A2 T achieved good results for premises , and quite good results for claims , although distinguishing between claims and premises is challenging for the A2 T approach . In particular , the role it may be useful to work also on semantic relations holding among sentences . This is actually one of the future tasks in our research work . Another specific challenge emerges when we consider the corpus C2 . Indeed , C2 contains a limited number of argumentative sentences with respect to the corpus size . In this case , since we analyze all the sentences according to their bag of words , we tend to overestimate the number of argumentative units , collecting a relatively high number of false positives .", "entities": [[122, 123, "MetricName", "accuracy"]]}
{"text": "Monolingual training of a dependency parser has been successful when relatively large treebanks are available ( Kiperwasser and Goldberg , 2016 ; Dozat and Manning , 2017 ) . However , for many languages , treebanks are either too small or unavailable . Therefore , multilingual models leveraging Universal Dependency annotations ( Nivre et al , 2018 ) have drawn serious attention ( Zhang and Barzilay , 2015 ; Ammar et al , 2016 ; Kondratyuk and Straka , 2019 ) . Multilingual approaches learn generalizations across languages and share information between them , making it possible to parse a target language without supervision in that language . Moreover , multilingual models can be faster to train and easier to maintain than a large set of monolingual models . However , scaling a multilingual model over a high number of languages can lead to sub - optimal results , especially if the training languages are typologically diverse . Often , multilingual neural models have been found to outperform their monolingual counterparts on low - and zero - resource languages due to positive transfer effects , but underperform for high - resource languages ( Johnson et al , 2017 ; Arivazhagan et al , 2019 ; Conneau et al , 2020 ) , a problem also known as \" the curse of multilinguality \" . Generally speaking , a multilingual model without language - specific supervision is likely to suffer from over - generalization and perform poorly on high - resource languages due to limited capacity compared to the monolingual baselines , as verified by our experiments on parsing . In this paper , we strike a good balance between maximum sharing and language - specific capacity in multilingual dependency parsing . Inspired by recently introduced parameter sharing techniques ( Platanios et al , 2018 ; Houlsby et al , 2019 ) , we propose a new multilingual parser , UDapter , that learns to modify its language - specific parameters including the adapter modules , as a function of language embeddings . This allows the model to share parameters across languages , ensuring generalization and transfer ability , but also enables language - specific parameterization in a single multilingual model . Furthermore , we propose not to learn language embeddings from scratch , but to leverage a mix of linguistically curated and predicted typological features as obtained from the URIEL language typology database which supports 3718 languages including all languages represented in UD . While the importance of typological features for cross - lingual parsing is known for both non - neural ( Naseem et al , 2012 ; Zhang and Barzilay , 2015 ) and neural approaches ( Ammar et al , 2016 ; Scholivet et al , 2019 ) , we are the first to use them effectively as direct input to a neural parser , without manual selection , over a large number of languages in the context of zero - shot parsing where gold POS labels are not given at test time . In our model , typological features are crucial , leading to a substantial LAS increase on zero - shot languages and no loss on high - resource languages when compared to the language embeddings learned from scratch . We train and test our model on the 13 syntactically diverse high - resource languages that were used by Kulmizev et al ( 2019 ) , and also evaluate it on 30 genuinely low - resource languages . Results show that UDapter significantly outperforms stateof - the - art monolingual ( Straka , 2018 ) and multilingual ( Kondratyuk and Straka , 2019 ) parsers on most high - resource languages and achieves overall promising improvements on zero - shot languages . Contributions We conduct several experiments on a large set of languages and perform thorough analyses of our model . Accordingly , we make the following contributions : 1 ) We apply the idea of adapter tuning ( Rebuffi et al , 2018 ; Houlsby et al , 2019 ) to the task of universal dependency parsing . 2 ) We combine adapters with the idea of contextual parameter generation ( Platanios et al , 2018 ) , leading to a novel language adaptation approach with state - of - the art UD parsing results . 3 ) We provide a simple but effective method for conditioning the language adaptation on existing typological language features , which we show is crucial for zero - shot performance .", "entities": [[288, 290, "TaskName", "dependency parsing"], [291, 292, "DatasetName", "Inspired"], [412, 413, "DatasetName", "UD"], [529, 530, "MetricName", "loss"], [681, 683, "TaskName", "dependency parsing"], [718, 719, "DatasetName", "UD"]]}
{"text": "This section presents the background of our approach . Multilingual Neural Networks Early models in multilingual neural machine translation ( NMT ) designed dedicated architectures ( Dong et al , 2015 ; Firat et al , 2016 ) whilst subsequent models , from Johnson et al ( 2017 ) onward , added a simple language identifier to the models with the same architecture as their monolingual counterparts . More recently , multilingual NMT models have focused on maximizing transfer accuracy for low - resource language pairs , while preserving high - resource language accuracy ( Platanios et al , 2018 ; Neubig and Hu , 2018 ; Aharoni et al , 2019 ; Arivazhagan et al , 2019 ) , known as the ( positive ) transfer - ( negative ) interference trade - off . Another line of work builds massively multilingual pre - trained language mod - els to produce contextual representation to be used in downstream tasks ( Devlin et al , 2019 ; Conneau et al , 2020 ) . As the leading model , multilingual BERT ( mBERT ) 2 ( Devlin et al , 2019 ) which is a deep self - attention network , was trained without language - specific signals on the 104 languages with the largest Wikipedias . It uses a shared vocabulary of 110 K WordPieces ( Wu et al , 2016 ) , and has been shown to facilitate cross - lingual transfer in several applications ( Pires et al , 2019 ; Wu and Dredze , 2019 ) . Concurrently to our work , Pfeiffer et al ( 2020 ) have proposed to combine language and task adapters , small bottleneck layers ( Rebuffi et al , 2018 ; Houlsby et al , 2019 ) , to address the capacity issue which limits multilingual pre - trained models for cross - lingual transfer . Cross - Lingual Dependency Parsing The availability of consistent dependency treebanks in many languages Nivre et al , 2018 ) has provided an opportunity for the study of cross - lingual parsing . Early studies trained a delexicalized parser ( Zeman and Resnik , 2008 ; on one or more source languages by using either gold or predicted POS labels ( Tiedemann , 2015 ) and applied it to target languages . Building on this , later work used additional features such as typological language properties ( Naseem et al , 2012 ) , syntactic embeddings ( Duong et al , 2015 ) , and cross - lingual word clusters ( T\u00e4ckstr\u00f6m et al , 2012 ) . Among lexicalized approaches , Vilares et al ( 2016 ) learns a bilingual parser on a corpora obtained by merging harmonized treebanks . Ammar et al ( 2016 ) trains a multilingual parser using multilingual word embeddings , token - level language information , language typology features and fine - grained POS tags . More recently , based on mBERT ( Devlin et al , 2019 ) , zero - shot transfer in dependency parsing was investigated ( Wu and Dredze , 2019 ; Tran and Bisazza , 2019 ) . Finally Kondratyuk and Straka ( 2019 ) trained a multilingual parser on the concatenation of all available UD treebanks . Language Embeddings and Typology Conditioning a multilingual model on the input language is studied in NMT ( Ha et al , 2016 ; Johnson et al , 2017 ) , syntactic parsing ( Ammar et al , 2016 ) and language modeling ( \u00d6stling and Tiedemann , 2017 ) . The goal is to embed language information in real - valued vectors in order to enrich internal representations with input language for multilingual models . In dependency parsing , several previous studies ( Naseem et al , 2012 ; Zhang and Barzilay , 2015 ; Ammar et al , 2016 ; Scholivet et al , 2019 ) have suggested that typological features are useful for the selective sharing of transfer information . Results , however , are mixed and often limited to a handful of manually selected features ( Fisch et al , 2019 ; Ponti et al , 2019 ) . As the most similar work to ours , Ammar et al ( 2016 ) uses typological features to learn language embeddings as part of training , by augmenting each input token and parsing action representation . Unfortunately though , this technique is found to underperform the simple use of randomly initialized language embeddings ( ' language IDs ' ) . Authors also reported that language embeddings hurt the performance of the parser in zero - shot experiments ( Ammar et al , 2016 , footnote 30 ) . Our work instead demonstrates that typological features can be very effective if used with the right adaptation strategy in both supervised and zero - shot settings . Finally , Lin et al ( 2019 ) use typological features , along with properties of the training data , to choose optimal transfer languages for various tasks , including UD parsing , in a hard manner . By contrast , we focus on a soft parameter sharing approach to maximize generalizations within a single universal model .", "entities": [[17, 19, "TaskName", "machine translation"], [79, 80, "MetricName", "accuracy"], [93, 94, "MetricName", "accuracy"], [180, 181, "MethodName", "BERT"], [182, 183, "MethodName", "mBERT"], [196, 200, "MethodName", "self - attention network"], [240, 244, "TaskName", "cross - lingual transfer"], [311, 315, "TaskName", "cross - lingual transfer"], [319, 321, "TaskName", "Dependency Parsing"], [469, 471, "TaskName", "word embeddings"], [493, 494, "MethodName", "mBERT"], [507, 509, "TaskName", "dependency parsing"], [542, 543, "DatasetName", "UD"], [621, 623, "TaskName", "dependency parsing"], [842, 843, "DatasetName", "UD"]]}
{"text": "To obtain contextualized word representations , UDapter uses mBERT . For a token i in sentence S , BERT builds an input representation w i composed by summing a WordPiece embedding x i ( Wu et al , 2016 ) and a position embedding f i . Each w i S is then passed to a stacked self - attention layers ( SA ) to generate the final encoder representation r i : w i = x i + f i ( 4 ) r i = SA ( w i ; \u0398 ( ad ) ) ( 5 ) where \u0398 ( ad ) denotes the adapter modules . During training , instead of fine - tuning the whole encoder network together with the task - specific top layer , we use adapter modules ( Rebuffi et al , 2018 ; Stickland and Murray , 2019 ; Houlsby et al , 2019 ) , or simply adapters , to capture both task - specific and language - specific information . Adapters are small modules added between layers of a pre - trained network . In adapter tuning , the weights of the original network are kept frozen , whilst the adapters are trained for a downstream task . Tuning with adapters was mainly suggested for parameter efficiency but they also act as an information module for the task or the language to be adapted ( Pfeiffer et al , 2020 ) . In this way , the original network serves as a memory for the language ( s ) . In UDapter , following Houlsby et al ( 2019 ) , two bottleneck adapters with two feedforward projections and a GELU nonlinearity ( Hendrycks and Gimpel , 2016 ) are inserted into each transformer layer as shown in Figure 1 . We apply adapter tuning for two reasons : 1 ) Each adapter module consists of only few parameters and allows to use contextual parameter generation ( CPG ; see 3.3 ) with a reasonable number of trainable parameters . 3 2 ) Adapters enable taskspecific as well as language - specific adaptation via CPG since it keeps backbone multilingual representations as memory for all languages in pre - training , which is important for multilingual transfer . F 1 1 0 0 1 0 0 1 1 0 1 0 1 1 Biaffine Attention ( for", "entities": [[8, 9, "MethodName", "mBERT"], [18, 19, "MethodName", "BERT"], [29, 30, "MethodName", "WordPiece"], [59, 61, "HyperparameterName", "attention layers"], [92, 93, "HyperparameterName", "\u0398"], [101, 102, "HyperparameterName", "\u0398"], [281, 282, "MethodName", "GELU"], [382, 383, "DatasetName", "0"], [383, 384, "DatasetName", "0"], [385, 386, "DatasetName", "0"], [386, 387, "DatasetName", "0"], [389, 390, "DatasetName", "0"], [391, 392, "DatasetName", "0"]]}
{"text": "To control the amount of sharing across languages , we generate trainable parameters of the model using a contextual parameter generator ( CPG ) function inspired by Platanios et al ( 2018 ) . CPG enables UDapter to retain high multilingual quality without losing performance on a single language , during multi - language training . We define CPG as a function of language embeddings . Since we only train adapters and the biaffine attention ( i.e. adapter tuning ) , the parameter generator is formalized as { \u03b8 ( ad ) , \u03b8 ( bf ) } g ( m ) ( l e ) where g ( m ) denotes the parameter generator with language embedding l e , and \u03b8 ( ad ) and \u03b8 ( bf ) denote the parameters of adapters and biaffine attention respectively . We implement CPG as a simple linear transform of a language embedding , similar to Platanios et al ( 2018 ) , so that weights of adapters in the encoder and biaffine attention are generated by the dot product of language embeddings : g ( m ) ( l e ) = ( W ( ad ) , W ( bf ) ) l e ( 6 ) 3 Due to CPG , the number of adapter parameters is multiplied by language embedding size , resulting in a larger model compared to the baseline ( more details in Appendix A.1 ) . where l e R M , W ( ad ) R P ( ad ) \u00d7M , W ( bf ) R P ( bf ) \u00d7M , M is the language embedding size , P ( ad ) and P ( bf ) are the number of parameters for adapters and biaffine attention respectively . 4 An important advantage of CPG is the easy integration of existing task or language features .", "entities": [[88, 89, "HyperparameterName", "\u03b8"], [93, 94, "HyperparameterName", "\u03b8"], [122, 123, "HyperparameterName", "\u03b8"], [127, 128, "HyperparameterName", "\u03b8"], [289, 292, "HyperparameterName", "number of parameters"]]}
{"text": "Soft sharing via CPG enables our model to modify its parsing decisions depending on a language embedding . While this allows UDapter to perform well on the languages in training , even if they are typologically diverse , information sharing is still a problem for languages not seen during training ( zero - shot learning ) as a language embedding is not available . Inspired by Naseem et al ( 2012 ) and Ammar et al ( 2016 ) , we address this problem by defining language embeddings as a function of a large set of language typological features , including syntactic and phonological features . We use a multi - layer perceptron MLP ( lang ) with two feedforward layers and a ReLU nonlinear activation to compute a language embedding l e : l e = MLP ( lang ) ( l t ) ( 7 ) where l t is a typological feature vector for a language consisting of all 103 syntactic , 28 phonological and 158 phonetic inventory features from the URIEL language typology database ( Lewis et al , 2015 ) and Glottolog ( Hammarstr\u00f6m et al , 2020 ) . As many feature values are not available for each language , we use the values predicted by using a k - nearest neighbors approach based on average of genetic , geographical and feature distances between languages . For the encoder , we use BERT - multilingualcased together with its WordPiece tokenizer . Since dependency annotations are between words , we pass the BERT output corresponding to the first wordpiece per word to the biaffine parser . We apply the same hyper - parameter settings as Kondratyuk and Straka ( 2019 ) . Additionally , we use 256 and 32 for adapter size and language embedding size respectively . In our approach , pre - trained BERT weights are frozen , and only adapters and biaffine attention are trained , thus we use the same learning rate for the whole network by applying an inverse square root learning rate decay with linear warmup ( Howard and Ruder , 2018 ) . Appendix A.1 gives the hyper - parameter details . Baselines We compare UDapter to the current state of the art in UD parsing : [ 1 ] UUparser+BERT ( Kulmizev et al , 2019 ) , a graph - based BLSTM parser ( de Lhoneux et al , 2017 ; Smith et al , 2018 ) using mBERT embeddings as additional features . [ 2 ] UDpipe ( Straka , 2018 ) , a monolingually trained multi - task parser that uses pretrained word embeddings and character representations . [ 3 ] UDify ( Kondratyuk and Straka , 2019 ) , the mBERTbased multi - task UD parser on which our UDapter is based , but originally trained on all language treebanks from UD . UDPipe scores are taken from Kondratyuk and Straka ( 2019 ) . To enable a direct comparison , we also re - train UDify on our set of 13 high - resource languages both monolingually ( one treebank at a time ; monoudify ) and multilingually ( on the concatenation of languages ; multi - udify ) . Finally , we evaluate two variants of our model : 1 ) Adapter - only has only task - specific adapter modules and no languagespecific adaptation , i.e. no contextual parameter generator ; and 2 ) UDapter - proxy is trained without typology features : a separate language embedding is learnt from scratch for each in - training language , and for low - resource languages we use one from the same language family , if available , as proxy representation . Importantly , all baselines are either trained for a single language , or multilingually without any language - specific adaptation . By comparing UDapter to these parsers , we highlight its unique character that enables language specific parameterization by typological features within a multilingual framework for both supervised and zero - shot learning setup .", "entities": [[51, 55, "TaskName", "zero - shot learning"], [64, 65, "DatasetName", "Inspired"], [113, 114, "DatasetName", "MLP"], [123, 124, "MethodName", "ReLU"], [137, 138, "DatasetName", "MLP"], [214, 218, "MethodName", "k - nearest neighbors"], [238, 239, "MethodName", "BERT"], [244, 245, "MethodName", "WordPiece"], [257, 258, "MethodName", "BERT"], [263, 264, "MethodName", "wordpiece"], [310, 311, "MethodName", "BERT"], [329, 331, "HyperparameterName", "learning rate"], [341, 343, "HyperparameterName", "learning rate"], [345, 347, "MethodName", "linear warmup"], [376, 377, "DatasetName", "UD"], [412, 413, "MethodName", "mBERT"], [438, 440, "TaskName", "word embeddings"], [461, 462, "DatasetName", "UD"], [478, 479, "DatasetName", "UD"], [550, 551, "MethodName", "Adapter"], [669, 673, "TaskName", "zero - shot learning"]]}
{"text": "We start by analyzing the projection weights assigned to different typological features by the first layer of the language embedding network ( see eq . 7 ) . Figure 4b shows the averages of normalized syntactic , phonological and phonetic inventory feature weights . Although dependency parsing is a syntactic task , the network does not only utilize syntactic features , as also observed by Lin et al ( 2019 ) , but exploits all available typological features to learn its representations . Next , we plot the language representations learned in UDapter by using t - SNE ( van der Maaten and Hinton , 2008 ) , which is similar to the analysis carried out by Ponti et al ( 2019 , figure 8 ) using the language vectors learned by Malaviya et al ( 2017 ) . Figure 5 illustrates 2D vector spaces generated for the typological feature vectors l t ( A ) and the language embeddings l e learned by UDapter with or without typological features ( B and C respectively ) . The benefits of using typological features can be understood by comparing A and B : During training , UDapter learns to project URIEL features to language embeddings in a way that is optimal for in - training language parsing quality . This leads to a different placement of the high - resource languages ( red points ) in the space , where many linguistic similarities are preserved ( e.g. Hebrew and Arabic ; European languages except Basque ) but others are overruled ( Japanese drifting away from Korean ) . Looking at the low - resource languages ( blue points ) we find that typologically similar languages tend to have similar embeddings to the closest highresource language in both A and B. In fact , most groupings of genetically related languages , such as the Indian languages ( hi - cluster ) or the Uralic ones ( fi - cluster ) are largely preserved across these two spaces . Comparing B and C where language embeddings are learned from scratch , the absence of typological features leads to a seemingly random space with no linguistic similarities ( e.g. Arabic far away from Hebrew , Korean closer to English than to Japanese , etc . ) and , therefore , no principled way to represent additional languages . Taken together with the parsing results of 4.1 , these plots suggest that UDapter embeddings strike a good balance between a linguistically motivated representation space and one solely optimized for in - training language accuracy .", "entities": [[45, 47, "TaskName", "dependency parsing"], [428, 429, "MetricName", "accuracy"]]}
{"text": "In section 4.1 we observed that adapter tuning alone ( that is , without CPG ) improved the multilingual baseline in the high - resource languages , but worsened it considerably in the zero - shot setup . By contrast , the addition of CPG with typological features led to the best results over all languages . But could we have obtained similar results by simply increasing the adapter size ? For instance , in multilingual MT , increasing overall model capacity of an already very large and deep architecture can be a powerful alternative to more sophisticated parameter sharing approaches ( Arivazhagan et al , 2019 ) . To answer this question we train another adapteronly model with doubled size ( 2048 instead of the 1024 used in the main experiments ) . As seen in 3a , increase in model size brings a slight gain to the high - resource languages , but actually leads to a small loss in the zero - shot setup . This shows that adapters enlarge the per - language capacity for in - training languages , but at the same time they hurt generalization and zero - shot transfer . By contrast , UDapter including CPG which increases the model size by language embeddings ( see Appendix A.1 for details ) , outperforms both adapter - only models , confirming once more the importance of this component . For our last analysis ( Fig . 3b ) , we study soft parameter sharing via CPG on different portions of the network , namely : only on the adapter modules ' cpg ( adapters ) ' versus on both adapters and biaffine attention ' cpg ( adap.+biaf . ) ' corresponding to the full UDapter . Results show that most of the gain in the high - resource languages is obtained by only applying CPG on the multilingual encoder . On the other hand , for the low - resource languages , typological feature based parameter sharing is most important in the biaffine attention layer . We leave further investigation of this result to future work .", "entities": [[122, 123, "DatasetName", "2048"], [160, 161, "MetricName", "loss"]]}
{"text": "We have presented UDapter , a multilingual dependency parsing model that learns to adapt languagespecific parameters on the basis of adapter modules ( Rebuffi et al , 2018 ; Houlsby et al , 2019 ) and the contextual parameter generation ( CPG ) method ( Platanios et al , 2018 ) which is in principle applicable to a range of multilingual NLP tasks . While adapters provide a more general tasklevel adaptation , CPG enables language - specific adaptation , defined as a function of language embeddings projected from linguistically curated typological features . In this way , the model retains high per - language performance in the training data and achieves better zero - shot transfer . UDapter , trained on a concatenation of typologically diverse languages ( Kulmizev et al , 2019 ) , outperforms strong monolingual and multilingual baselines on the majority of both high - resource and low - resource ( zero - shot ) languages , which reflects its strong balance between per - language capacity and maximum sharing . Finally , the analyses we performed on the underlying characteristics of our model show that typological features are crucial for zero - shot languages . ( Kondratyuk and Straka , 2019 ) for all low - resource languages . ' * ' shows languages not present in mBERT training data . Additionally , ( \u2020 ) indicates languages where no significant difference between UDapter and multi - udify by significance testing . For udapter - proxy , chosen proxy language is given between brackets . CTR means centroid language embedding . models are trained separately so the total number of parameters for 13 languages is 2.5B ( 13x191 M ) .", "entities": [[7, 9, "TaskName", "dependency parsing"], [60, 62, "TaskName", "multilingual NLP"], [222, 223, "MethodName", "mBERT"], [273, 276, "HyperparameterName", "number of parameters"]]}
{"text": "We train the model to minimize the negative log - likelihood loss using back - propagation with stochastic gradient descent and a mini - batch size of 16 . To monitor model performance , we use the train / validation split provided by the organizers . For optimization , we use the AdamW optimizer ( Loshchilov and Hutter , 2019 ) with gradient clipping ( Pascanu et al , 2013 ) and a linear scheduler with no warm - up . We use FP - 16 mixed precision ( Micikevicius et al , 2018 ) training ( and inference ) in order to afford a larger batch size and increased training speed . To optimize GPU use by minimizing the amount of memory allocated for padding tokens , we use dynamic padding and length - based batching in the sense of ( Skinner , 2018 ) . Finally , we employ label smoothing ( Szegedy et al , 2016 ) with a smoothing factor of 0.1 .", "entities": [[8, 11, "MetricName", "log - likelihood"], [11, 12, "MetricName", "loss"], [17, 20, "MethodName", "stochastic gradient descent"], [22, 26, "HyperparameterName", "mini - batch size"], [52, 53, "MethodName", "AdamW"], [53, 54, "HyperparameterName", "optimizer"], [62, 64, "MethodName", "gradient clipping"], [106, 108, "HyperparameterName", "batch size"], [151, 153, "MethodName", "label smoothing"]]}
{"text": "As mentioned in Section [ 1 ] , we do not experiment with hyperparameter tuning but rather keep the default parameters of the Trainer API in the Hugging Face transformers library . More specifically , we use \u03b2 1 = 0.9 , \u03b2 2 = 0.999 and \u03f5 = 10 \u22128 for the AdamW optimizer parameter values and a learning rate of 0.00005 . We train the models for a maximum of 25 epochs with an early stopping patience level set to 0.001 for 3 epochs . Finally , we set a maximum sequence length of 128 since input sentences are generally short and we would like to avoid consuming GPU memory for padding tokens .", "entities": [[37, 38, "HyperparameterName", "\u03b2"], [42, 43, "HyperparameterName", "\u03b2"], [53, 54, "MethodName", "AdamW"], [54, 55, "HyperparameterName", "optimizer"], [59, 61, "HyperparameterName", "learning rate"], [76, 78, "MethodName", "early stopping"]]}
{"text": "Table [ 3 ] summarizes the performance of our approach in the validation set for each sub - task . Note that in this set of experiments , the validation set was used both during training ( e.g. for early stopping or selection of batch size ) as well as for the reporting of the systems ' performance . Table [ 4 ] summarizes the performance of our approach in the evaluation set for each sub - task . The organizers chose to disclose to each team only their respective score along with the average score of all submitted systems . Our system performed considerably better than the average in sub - task 1a and surpassed the existing state - of - the - art F1 - score of 0.63 reported in ( Magge et al , 2021 ) . Performance was considerably poorer for sub - tasks 3 and 8 . As mentioned in 4 : Results on the evaluation set for sub - tasks 1a , 3 and 8 . Average score of all participating systems in parentheses . Metric is F1 - score for class 1 . Section [ 1 ] , our main efforts were dedicated to subtask 1a and the system developed did not transfer well to the remaining sub - tasks .", "entities": [[39, 41, "MethodName", "early stopping"], [44, 46, "HyperparameterName", "batch size"], [125, 128, "MetricName", "F1 - score"], [183, 186, "MetricName", "F1 - score"]]}
{"text": "We demonstrated that a RoBERTa model ( Liu et al , 2019 ) pretrained on approximately 128 million tweets performs very competitively when finetuned on English tweet classification for ADRs . Using only a standard finetuning approach , our model obtained competitive results , outperforming the average of all submissions for sub - task 1 by a 9 % absolute difference in F1 - score . This constitutes yet another testament of the fact that large pre - trained language models have rightfully become the default approach in virtually all NLP tasks . With respect to potential future work , there is a large collection of available options . Text classification and , more generally , binary classification is one of the oldest and most widely researched topics in NLP . Most approaches aiming to improve performance of classification models can be broadly categorized into three groups , depending on the segment of the machine learning workflow that they are targeting . Data augmentation methods typically target the initial part of the workflow , the data , aiming to increase the quantity , quality and diversity of the training dataset to ensure that model performance is robust to small syntactic or semantic perturbations in the inputs . Transformations acting directly on strings , such random token insertions or deletions , synonym / antonym replacements and related techniques ( Wei and Zou , 2019 ; Karimi et al , 2021 , inter alia ) have shown significant performance improvements , especially in lowresource scenarios much like the one in this shared task . A second approach , evidently a natural extension of the previous technique , would be to target vector encodings of the tokens and/or documents that are produced by the various layers of the neural networks . We can distinguish two different approaches here : ( i ) improve the language model backbone during the pretraining phase , or ( ii ) improve the weights of the language model backbone during finetuning . The research community has devoted intensive efforts in the former approach , as can be observed by the ever - increasing list of transformerbased pretrained language models ( Devlin et al , 2019 ; Joshi et al , 2020 ; Kitaev et al , 2020 ; Raffel et al , 2020 ; Brown et al , 2020 , inter multi alia ) released . Model size , in terms of total number of trainable parameters , has been consistently shown to correlate strongly with downstream performance , so opting for a larger pretrained model would be a reasonable first steps towards more transferable vector representations ( and hence improved performance ) in the downstream task . The latter approach would include domain adaptation techniques , such as continued self - supervised pretraining followed by supervised finetuning , which has been shown ( Gururangan et al , 2020 ) to consistently lead to superior results relative to direct finetuning . Finally , one could aim to improve performance by modifying aspects of the objective function . ( Hui and Belkin , 2021 ) , in an extensive series of experiments , show that the established practice of using a cross - entropy loss for classification is not well - founded and show through a variety of diverse experiments that a square loss can , in many cases , significantly improve performance .", "entities": [[4, 5, "MethodName", "RoBERTa"], [62, 65, "MetricName", "F1 - score"], [109, 111, "TaskName", "Text classification"], [162, 164, "TaskName", "Data augmentation"], [358, 361, "TaskName", "pretrained language models"], [455, 457, "TaskName", "domain adaptation"], [535, 536, "MetricName", "loss"], [554, 555, "MetricName", "loss"]]}
{"text": "Most deep learning approaches for text - to - SQL generation are limited to the WikiSQL dataset , which only supports very simple queries over a single table . We focus on the Spider dataset , a complex and crossdomain text - to - SQL task , which includes complex queries over multiple tables . In this paper , we propose a SQL clause - wise decoding neural architecture with a self - attention based database schema encoder to address the Spider task . Each of the clause - specific decoders consists of a set of sub - modules , which is defined by the syntax of each clause . Additionally , our model works recursively to support nested queries . When evaluated on the Spider dataset , our approach achieves 4.6 % and 9.8 % accuracy gain in the test and dev sets , respectively . In addition , we show that our model is significantly more effective at predicting complex and nested queries than previous work .", "entities": [[5, 10, "TaskName", "text - to - SQL"], [15, 16, "DatasetName", "WikiSQL"], [40, 45, "TaskName", "text - to - SQL"], [136, 137, "MetricName", "accuracy"]]}
{"text": "Text - to - SQL generation is the task of translating a natural language question into the corresponding SQL . Recently , various deep learning approaches have been proposed based on the WikiSQL dataset ( Zhong et al , 2017 ) . However , because WikiSQL contains only very simple queries over just a single table , these approaches ( Xu et al , 2017 ; Huang et al , 2018 ; Yu et al , 2018a ; Dong and Lapata , 2018 ) can not be applied directly to generate complex queries containing elements such as JOIN , GROUP BY , and nested queries . To overcome this limitation , Yu et al ( 2018c ) introduced Spider , a new complex and crossdomain text - to - SQL dataset . It contains a large number of complex queries over different databases with multiple tables . It also requires a model to generalize to unseen database schema as different databases are used for training and testing . Therefore , a model should understand not only the natural language question but also the schema of the corresponding database to predict the correct SQL query . In this paper , we propose a novel SQL - specific clause - wise decoding neural network model to address the Spider task . We first predict a sketch for each SQL clause ( e.g. , SELECT , WHERE ) with text classification modules . Then , clause - specific decoders find the columns and corresponding operators based on the sketches . Our contributions are summarized as follows . We decompose the clause - wise SQL decoding process . We also modularize each of the clause - specific decoders into sub - modules based on the syntax of each clause . Our architecture enables the model to learn clausedependent context and also ensures the syntactic correctness of the predicted SQL . Our model works recursively so that it can predict nested queries . We also introduce a self - attention based database schema encoder that enables our model to generalize to unseen databases . In the experiment on the Spider dataset , we achieve 24.3 % and 28.8 % exact SQL matching accuracy on the test and dev set respectively , which outperforms the previous state - of - the - art approach ( Yu et al , 2018b ) by 4.6 % and 9.8 % . In addition , we show that our approach is significantly more effective compared to previous work at predicting not only simple SQL queries , but also complex and nested queries .", "entities": [[0, 5, "TaskName", "Text - to - SQL"], [32, 33, "DatasetName", "WikiSQL"], [45, 46, "DatasetName", "WikiSQL"], [125, 130, "TaskName", "text - to - SQL"], [236, 238, "TaskName", "text classification"], [367, 368, "MetricName", "accuracy"]]}
{"text": "We encode a natural language question with a bidirectional LSTM . We denote H Q R d\u00d7 | X | as the question encoding , where d is the number of LSTM units and | X | is the number of tokens in the question . To encode a database schema , we consider each column in its tables as a concatenated sequence of words from the table name and column name with a separation token . ( e.g. , [ student , [ SEP ] , first , name ] ) . First , we apply bi - directional LSTM over this sequence for each column . Then , we apply the self - attention mechanism ( Lin et al , 2017 ) over the LSTM outputs to form a summarized fixedsize vector for each column . For the ith column , its encoding h ( i ) col R d is computed by a weighted sum of the LSTM output o ( i ) col R d\u00d7 | L | as follows : \u03b1 = softmax ( w T tanh ( o ( i ) col ) ) ( 1 ) h ( i ) col = o ( i ) col \u03b1 T ( 2 ) where | L | is the number of tokens in the column and w R d is a trainable parameter . We denote H col = [ h ( 1 ) col , ... h ( | C | ) col ] as columns encoding where | C | is the number of columns in the database .", "entities": [[8, 10, "MethodName", "bidirectional LSTM"], [31, 32, "MethodName", "LSTM"], [100, 101, "MethodName", "LSTM"], [126, 127, "MethodName", "LSTM"], [160, 161, "MethodName", "LSTM"], [175, 176, "HyperparameterName", "\u03b1"], [177, 178, "MethodName", "softmax"], [204, 205, "HyperparameterName", "\u03b1"]]}
{"text": "We predict the clause - wise sketch via 8 different text classification modules that include the number of SQL expressions in each clause , the presence of LIMIT clause , and the presence of INTERSECT / UNION / EXCEPT as described in Figure 1 . All of them share the same model architecture but are trained separately . For the classification , we applied attention - based bi - directional LSTM following Zhou et al ( 2016 ) . First , we compute sentence representation r s R d by a weighted sum of question encoding H Q R d\u00d7 | X | . Then we apply the softmax classifier to choose the sketch as follows : \u03b1 s = softmax ( w T s tanh ( H Q ) ) ( 3 ) r s = H Q \u03b1 T s ( 4 ) P sketch = softmax ( W s r s + b s ) ( 5 ) where w s R d , W s R ns\u00d7d , b s R ns are trainable parameters and n s is the number of possible sketches .", "entities": [[10, 12, "TaskName", "text classification"], [70, 71, "MethodName", "LSTM"], [108, 109, "MethodName", "softmax"], [117, 118, "HyperparameterName", "\u03b1"], [120, 121, "MethodName", "softmax"], [139, 140, "HyperparameterName", "\u03b1"], [148, 149, "MethodName", "softmax"]]}
{"text": "To predict columns and operators , we use the LSTM decoder with the attention mechanism ( Luong et al , 2015 ) such that the number of decoding steps are decided by the sketch prediction module . We train 5 different column prediction modules separately for each SQL clause , but they share the same architecture . In the column prediction module , the hidden state of the decoder at the t - th decoding step is computed as d ( t ) col ( R d ) = LSTM ( d ( t\u22121 ) col , h ( t\u22121 ) col ) , where h ( t\u22121 ) col R d is an encoding of the predicted column in the previous decoding step . The context vector r ( t ) is computed by a weighted sum of question encodings H Q R d\u00d7 | X | based on attention weight as follows : \u03b1 ( t ) = softmax ( d ( t ) col T H Q ) ( 6 ) r ( t ) = H Q \u03b1 ( t ) T ( 7 ) Then , the attentional output of the t - th decoding step a ( t ) col is computed as a linear combination of d ( t ) col R d and r ( t ) R d followed by tanh activation . a ( t ) col = tanh ( W 1 d ( t ) col + W 2 r ( t ) ) ( 8 ) where W 1 , W 2 R d\u00d7d are trainable parameters . Finally , the probability for each column at the tth decoding step is computed as a dot product between a ( t ) col R d and the encoding of each column in H col R d\u00d7 | C | followed by softmax . P ( t ) col = softmax ( a ( t ) col T H col ) ( 9 ) To predict corresponding operators for each predicted column , we use a decoder of the same architecture as in the column prediction module . The only difference is that a decoder input at the t - th decoding step is an encoding of the t - th predicted column from the column prediction module . d ( t ) op = LSTM ( d ( t\u22121 ) op , h ( t ) col ) ( 10 ) Attentional output a ( t ) op R d is computed identically to Eq . ( 8 ) . Then , the probability for operators corresponding to the t - th predicted column is computed by the softmax classifier as follows : P ( t ) op = softmax ( W o a ( t ) op + b o ) ( 11 ) where W o R no\u00d7d and b o R no are trainable parameters and n o is the number of possible operators .", "entities": [[9, 10, "MethodName", "LSTM"], [89, 90, "MethodName", "LSTM"], [155, 156, "HyperparameterName", "\u03b1"], [160, 161, "MethodName", "softmax"], [181, 182, "HyperparameterName", "\u03b1"], [229, 231, "MethodName", "tanh activation"], [312, 313, "MethodName", "softmax"], [320, 321, "MethodName", "softmax"], [395, 396, "MethodName", "LSTM"], [449, 450, "MethodName", "softmax"], [460, 461, "MethodName", "softmax"]]}
{"text": "We evaluate our model with Spider ( Yu et al , 2018c ) , a large - scale , complex and cross - domain text - to - SQL dataset . We follow the same database split as Yu et al ( 2018c ) , which ensures that any database schema that appears in the training set does not appear in the dev or test set . Through this split , we examine how well our model can be generalized to unseen databases . Because the test set is not opened to the public , we use the dev set for the ablation analysis . For the evaluation metrics , we use 1 ) accuracy of exact SQL matching and 2 ) F1 score of SQL component matching , proposed by ( Yu et al , 2018c ) . We also follow their query hardness criteria to understand the model performance on different levels of queries . Our model and all the baseline models are trained based on only the Spider dataset without data augmentation .", "entities": [[24, 29, "TaskName", "text - to - SQL"], [114, 115, "MetricName", "accuracy"], [122, 124, "MetricName", "F1 score"], [173, 175, "TaskName", "data augmentation"]]}
{"text": "We use the same hyperparameters for every module . For the word embedding , we apply deep contextualized word representations ( ELMO ) from Peters et al ( 2018 ) and allow them to be fine - tuned during the training . For the question and column encoders , we use a 1 - layer 512 - unit bi - directional LSTM . For the decoders in the columns and operators prediction modules , we use a 1 - layer 1024unit uni - directional LSTM . For the training , we use Adam optimizer ( Kingma and Ba , 2014 ) with a learning rate of 1e - 4 and use early stopping with 50 epochs . Additionally , we use dropout ( Hinton et al , 2012 ) with a rate of 0.2 for the regularization .", "entities": [[21, 22, "MethodName", "ELMO"], [61, 62, "MethodName", "LSTM"], [84, 85, "MethodName", "LSTM"], [92, 93, "MethodName", "Adam"], [93, 94, "HyperparameterName", "optimizer"], [103, 105, "HyperparameterName", "learning rate"], [111, 113, "MethodName", "early stopping"]]}
{"text": "Table 1 shows the exact SQL matching accuracy of our model and previous models . We achieve 24.3 % and 28.8 % on the test and dev sets respectively , which outperforms the previous best model SyntaxSQLNet ( Yu et al , 2018b ) by 4.6 % and 9.8 % . Moreover , our model outperforms previous models on all different query hardness levels . To examine how each technique contributes to the performance , we conduct an ablation analysis of three aspects : 1 ) without recursion , 2 ) without self - attention for database schema encoding , and 3 ) without sketch prediction modules that decide the number of decoding steps . Without recursive sub - query generation , the accuracy drops by 5.7 % and 3.6 % for hard and extra hard queries , respectively . This result shows that the recursion we use enables the model to predict nested queries . When using the final LSTM hidden state as in Yu et al ( 2018b ) instead of using self - attention for schema encoding , the accuracy drops by 4.0 % on all queries . Finally , when using only an encoder - decoder architecture without sketch generation for columns prediction , the accuracy drops by 4.7 % . For the component matching result for each SQL clause , our model outperforms previous approaches for all of the SQL components by a significant margin , as shown in Table 2 . Examples of predicted SQL from different models are shown in Appendix A.", "entities": [[7, 8, "MetricName", "accuracy"], [123, 124, "MetricName", "accuracy"], [160, 161, "MethodName", "LSTM"], [182, 183, "MetricName", "accuracy"], [209, 210, "MetricName", "accuracy"]]}
{"text": "Classification Few studies to date have addressed sentence - level readability for English . Napoles & Dredze ( 2010 ) built their own corpus with documents from English and Simple English Wikipedia to train both document - and sentence - level classifiers . Using bag - of - words features , unigram and bigram part - of - speech features , type - token ratio , the proportion of words appearing on a list of easier words , and parse features similar to Petersen 's , their binary classifier achieved an accuracy of 80.8 % on this task . The structure of this task , however , is not suited to text simplification applications , because the sentences are not controlled for meaning . Classifying a sentence in isolation as more likely to be from Simple Wikipedia or English Wikipedia is not as useful as a model trained to differentiate sentences carrying the same meaning . This work is not directly comparable to that of Vajjala & Meurers ( 2012 ; or subsequent work on ranking sentences by their complexity due to the differences in choice of corpora and task structure . In the medical domain , Kauchak et al ( 2014 ) also looked at sentence - level classification , identifying sentences as being either simple or difficult . Their features included word length , sentences length , part - of - speech counts , average unigram frequencies and standard deviation , and the proportion of words not on a list of the five thousand most frequent words as well as three domainspecific features based on an ontology of medical terminology . Ranking Vajjala & Meurers ( Vajjala and Meurers , 2014 ; Vajjala , 2015 ) were the first to look at ranking sentences rather than classifying them , having observed that the distributions of predicted reading levels across the two subcorpora of the Parallel Wikipedia corpus ( Zhu et al , 2010 , PWKP ) were different . While the Simple English portion of the corpus was clearly skewed toward the lower grade levels , it appears that the English portion of the corpus was evenly distributed across all grade levels , making binary - classification difficult . This led Vajjala & Meurers to develop a ranking model using the predicted reading levels from a multiclass classifier trained on whole documents . For each sentence pair , they assumed that the English Wikipedia sentence should be classified at a higher level than the Simple English Wikipedia sentence . Using a hard cut - off ( i.e. rank ( sent english ) > rank ( sent simple ) ) , their model achieved about 59 % accuracy , although this improved to 70 % by relaxing the inequality constraint to include equality . Based on the finding that 30 % of sentence pairs from the PWKP corpus are incorrectly ranked despite lying within one reading level of each other , we hypothesize that finer - grained distinctions may be necessary to tease apart the differences in related pairs of sentences . Offline Psycholinguistic Features While Vajjala & Meurers ( 2012 ; 2014 ) do use some psycholinguistically - motivated features , their features are primarily lexical in nature and therefore complementary to ours , which depend on the sentence processing context . They drew psycholinguistic features from the MRC psycholinguistic database ( Wilson , 1988 ) , including word familiarity , concreteness , imageability , meaningfulness , and age of acquisition . These features were coupled with a second age of acquisition database and values related to the average number of senses per word . Towards online considerations More recently , Ambati et al ( 2016 ) used an incremental parser to extend Vajjala & Meurers work . Since human processing is incremental , they reasoned , features from an incremental parser might be more informative than features extracted from a nonincremental parser . To this end , they used the incremental derivations from a combinatory categorial grammar ( CCG ) parser . Ambati et al ran several models on the English and Simple English Wikipedia data set ( Hwang et al , 2015 , ESEW ) : one using only the syntactic features from ( Vajjala and Meurers , 2014 ) ; another ( INCCCG ) using only features from the incremental parser ; and INCCCG+ , incorporating morpho - syntactic and psycholinguistic features from ( Vajjala and Meurers , 2014 ) . At the sentence level , they include sentence length , number of CCG constituents in the final parse , and the depth of the CCG derivation . They also use count features for the number of times each CCG derivation rule is applied ( e.g. forward application , type - raising ) . Finally , they include counts of different CCG syntactic categories as well as the average ' complexity ' of the syntactic categories . While the parser they use is inspired by human behavior , in that it is an incremental parser , these features do not re - late to any specific linguistic theory of sentence processing . The work presented here is most comparable to that of Vajjala & Meurers and Ambati et al , as we all address the problem of ranking sentences according to their linguistic complexity . Our study is the only one of the three to examine features based on theories of online sentence processing . Ambati et al ( 2016 ) provide accuracy information for their own features as well as Vajjala & Meurers ' ( 2014 ) features on the English and Simple English Wikipedia corpus ( ESEW ) which we use , but used a 60 - 20 - 20 training - dev - test split where we used 10 - fold cross - validation , making the results not directly comparable .", "entities": [[0, 1, "TaskName", "Classification"], [54, 57, "DatasetName", "part - of"], [91, 92, "MetricName", "accuracy"], [111, 113, "TaskName", "text simplification"], [194, 196, "DatasetName", "medical domain"], [229, 232, "DatasetName", "part - of"], [268, 269, "MethodName", "ontology"], [448, 449, "MetricName", "accuracy"], [917, 918, "MetricName", "accuracy"]]}
{"text": "It is a work - hard , play - hard ethic that many of the world 's billionaires might subscribe to but it would be a huge change for most workers and their employers . 2 It is a ' work - hard , play - hard ' way of thinking that many of the world 's billionaires might agree with but it would be a huge change for most workers and their employers . 1 Many of the world 's billionaires might agree with this way of thinking but it would be a very big change for most workers and their employers . difficult - to - integrate point in the sentence . These features comprise the INTEGRATIONCOST model . Finally , we use a modified version of the IDD3 library from Andre Cunha ( Cunha et al , 2015 ) to extract idea density decomposed across three types of propositional idea : predications , modifications , and connections . 10 Here we use only averaged features , as the crucial measure is the idea density rather than the raw number of ideas being expressed . These features comprise the IDEAD - ENSITY model . As a point of comparison for these models , we created a BASELINE which used only sentence length and the average word length as features . We also created models based on features grouped by the parser used to extract them : SUR - PRISAL+EMBED for the ModelBlocks parser and IDEA+INTEGRATION for the Stanford parser . While ModelBlocks achieves competitive accuracies , it is much slower than other state - of - theart parsers available today . Therefore we wanted to provide a point of comparison regarding the relative utility of these parsers : grouping features by parser allows us to assess the trade - off between model accuracy and the time necessary for feature extraction . Finally , we considered combinations of the parser - grouped features with the baseline ( BASE+SURPRISAL+EMBED and BASE+IDEA+INTEGRATION ) and a 10 Code available at : https://github.com/ dmhowcroft / idd3 . FULLMODEL using the baseline features and all of the psycholinguistic features . Replication The scripts required for replication are available at https://github.com/ dmhowcroft / eacl2017 - replication . This includes pointers to the corpora , preprocessing scripts and settings for the parsers , as well as scripts for feature extraction and running the averaged perceptron model .", "entities": [[304, 305, "MetricName", "accuracy"]]}
{"text": "The feature sets for individual psycholinguistic theories only achieve accuracies between 55 % and 65 % ( see the first 4 columns of Fig . 1 ) . Combining all of these features into the PSYCHOLIN - GUISTIC model improves performance to nearly 70 % ( column 5 ) . Looking at the feature sets grouped by parser ( columns 6 and 7 ) , we see that the combination of surprisal and embedding depth ( from the ModelBlocks parser ) significantly outperforms the combination of integration cost and idea density ( from the Stanford Parser ) . However , the strength of the features derived from ModelBlocks seems to be primarily driven by the EMBEDDING features , while the strength of the dependency - parse - derived features appears to stem from INTEGRATIONCOST . Moving to Figure 2 , we see that our BASE - LINE features achieved an accuracy of 71.24 % , despite using only average word length and sentence length . This is 1.48 percentage points higher than the 69.76 % accuracy of the PSYCHOLINGUIS - TIC model , which includes surprisal , embedding depth , integration cost , and idea density . However , the FULL model ( column 4 ) outperforms the BASELINE by a statistically significant 11 1.98 percentage points ( p 0.01 ) . This confirms our primary hypothesis : psycholinguistic features based on online sentence processing can improve models of sentence complexity beyond a simple baseline . To address the secondary hypothesis , we turn to the OSE data in Figure 3 . The best model for this corpus uses the baseline features combined with embedding depth and surprisal features extracted from ModelBlocks . In both OSE f ar and OSE near we gain about 3 points over the baseline when adding these features ( 3.18 and 3.25 points , respectively ) , which is similar to the gains for the FULL model over the baseline . The fact that the increase in performance between the BASELINE model and the best performing model does not differ between the OSE near and the OSE f ar datasets suggests a lack of support for our secondary hypothesis that these features are espe - cially helpful for distinguishing items of similar difficulty levels . These results warrant a full comparison to the work of Ambati et al ( 2016 ) , despite the differences in our evaluation sets . Ambati et al found that their features based on incremental CCG derivations achieved an accuracy of 72.12 % , while the offline psycholinguistic features of Vajjala & Meurers came in at 74.58 % , 1.36 percentage points better than our 73.22 % . Finally , a model combining all of Vajjala & Meurers featurs with the incremental CCG features achieved a performance of 78.87 % . Since the features examined in our study are complementary to those proposed by these two previous studies , a model combining all of these features should further improve in accuracy .", "entities": [[144, 145, "MethodName", "BASE"], [146, 147, "MethodName", "LINE"], [150, 151, "MetricName", "accuracy"], [175, 176, "MetricName", "accuracy"], [419, 420, "MetricName", "accuracy"], [500, 501, "MetricName", "accuracy"]]}
{"text": "It is obvious that all proposed procedure candidates are co - related to each other . In order to encode this interaction , we follow the method in ( Zhou et al , 2018a ) which uses an LSTM model to predict a sequence from the K \u00d7 T generated procedure proposal . The input of the recurrent prediction model for each time step consists of three parts : frame features , the position embedding , the plausibility score feature . Frame Features For a generated procedure proposal , the corresponding feature vectors F ( k , t ) are calculated as follows : F ( k , t ) = f C ( k , t ) \u2212L ( k , t ) , , f C ( k , t ) + L ( k , t ) ( 1 ) C ( k , t ) = t + k ( k ) \u00d7 M ( k , t ) m ( 2 ) L ( k , t ) = k ( k ) \u00d7 M ( k , t ) l 2 ( 3 ) where k = { k 1 , , k K } is a list of different ker - nel sizes . The M ( k , t ) m and M ( k , t ) l represent the midpoint and length offset of the span for k - th kernel and t - th frame respectively and k ( k ) is the length of the k - th kernel .", "entities": [[38, 39, "MethodName", "LSTM"], [191, 193, "HyperparameterName", "k ="]]}
{"text": "We treat all possible positions as a list of tokens and use an embedding layer to get a continuous representation . The [ BOS ] and [ EOS ] , i.e. the begin of sentence and the end of sentence , are also added into the vocabulary for sequence prediction . Score Feature The score feature is a flatten of matrix M s , i.e. s R K T \u00d71 . The input embedding of each time step is the concatenation of : 1 . The averaged features of the proposal predicted in the previous step t : F ( k , t ) = 1 2L ( k , t ) L ( k , t ) t = \u2212L ( k , t ) f C ( k , t ) + t ( 4 ) 2 . The position embedding of the proposal . 3 . The score feature s. Specifically , for the first step , the input frame feature is the averaged frame features of the entire video . F = 1 T T t=1 f t and the position embedding is the encoding of [ BOS ] . The procedure extraction finishes when [ EOS ] is predicted , and the output of this module is a sequence of indexes of frames : P = { p 1 p L } where L is the maximum count of the predicted proposals .", "entities": [[51, 52, "MetricName", "Score"]]}
{"text": "The target of the model is to extract procedures and generate captions . The loss function consists of four parts : ( 1 ) L s : a binary cross - entropy loss of each generated positive and negative procedure ; ( 2 ) L r : the regression loss with a smooth l1 - loss ( Ren et al , 2015 ) of a time span between the extracted and the ground - truth procedure . ( 3 ) L p : the cross - entropy loss of each proposed procedure in the predicted sequence of proposals . ( 4 ) L c : the cross - entropy loss of each token in the generated procedure captions . Here are the formulations : L = \u03b1 s L s + \u03b1 r L r + \u03b1 p L p + \u03b1 c L c ( 5 ) L s = \u2212 1 C P C P i=1 log ( M P s ) \u2212 1 C N C N i=1 log ( 1 \u2212 M N s ) ( 6 ) L r = 1 C P C P i=1 | | B pred i \u2212 B gt i | | s\u2212l1 ( 7 ) L p = \u2212 1 L L l=1 log ( p l 1 ( gt l ) l ) ( 8 ) L c = \u2212 1 L L l=1 1 | W l | w W l log ( w1 ( gt w ) ) ( 9 ) where M P s and M N s are the scoring matrix of positive and negative samples in a video , and C P and C N represent the count separately . Here we regard a sample as positive if its IoU ( Intersection of Union ) with any ground - truth procedure is more than 0.8 . If the IoU is less than 0.2 , we treat it as negative . The loss L s aims to enlarge the score of all positive samples and decrease the score otherwise . The B pred i and B gt i represent the boundary ( calculated by the offset of midpoint and length ) of the positive sample and ground - truth procedure separately . We only take positive samples into account and conduct the regression with L r to shorten the distance between all positive samples and the ground - truth procedures . The p l is the classification result of the procedure extraction module and the value of 1 will be 1 if the predicted class of extracted procedure proposal is identical to the class of the groundtruth proposal with the maximal IoU and 0 otherwise . The cross - entropy loss L p aims to exploit the model to correctly select the most similar proposal of each ground - truth procedure from many positive samples . Finally , W stores all decoded captions of procedures of a video . The L c is designed for the captioning module based on the extracted procedures .", "entities": [[14, 15, "MetricName", "loss"], [32, 33, "MetricName", "loss"], [49, 50, "MetricName", "loss"], [55, 56, "MetricName", "loss"], [87, 88, "MetricName", "loss"], [109, 110, "MetricName", "loss"], [126, 127, "HyperparameterName", "\u03b1"], [131, 132, "HyperparameterName", "\u03b1"], [136, 137, "HyperparameterName", "\u03b1"], [141, 142, "HyperparameterName", "\u03b1"], [296, 297, "MetricName", "IoU"], [315, 316, "MetricName", "IoU"], [328, 329, "MetricName", "loss"], [447, 448, "MetricName", "IoU"], [449, 450, "DatasetName", "0"], [456, 457, "MetricName", "loss"]]}
{"text": "We separately evaluate the procedure extraction and captioning module . For procedure extraction , we adopt the widely used mJacc ( mean of Jaccard ) ( Bojanowski et al , 2014 ) and mIoU ( mean of IoU ) metrics for evaluating the procedure proposition . The Jaccard calculates the intersection of the predicted and ground - truth procedure proposals over the length of the latter . The IoU replaces the denominator part with the union of predicted and ground - truth procedures . For procedure captioning , we adopt BLEU - 4 ( Papineni et al , 2002 ) and METEOR ( Banerjee and Lavie , 2005 ) as the metrics to evaluate the performance on the result of captioning based on both extracted and ground - truth procedures .", "entities": [[33, 34, "MetricName", "mIoU"], [37, 38, "MetricName", "IoU"], [68, 69, "MetricName", "IoU"], [90, 91, "MetricName", "BLEU"], [101, 102, "DatasetName", "METEOR"]]}
{"text": "For the procedure extraction module , we follow the method in ( Zhou et al , 2018a ) to use 16 different kernel sizes for the temporal convolutional layer , i.e. from 3 to 123 with the interval step of 8 , which can cover the different lengths . We also used a max - pooling layer with a kernel of [ 8 , 5 ] after the convolutional layer . We extract at most 16 procedures for each video , and the maximum caption length of each extracted procedure is 50 . The hidden size of all recurrent model ( LSTM ) is 512 and we conduct a dropout for each layer with a probability of 0.5 . We use two transformer models with 2048 inner hidden sizes , 8 heads , and 6 layers to encode context - aware transcripts and video frame features separately . We adopt an Adam optimizer ( Kingma and Ba , 2015 ) with a starting learning rate of 0.000025 and \u03b1 = 0.8 and \u03b2 = 0.999 to train the model . The batch size of training is 4 for each GPU and we use 4 GPUs to train our model so the overall batch size is 16 . We demonstrate the result of the procedure extraction model by Table 1 . We compare our model with several baseline methods : ( 1 ) SCNN - prop ( Shou et al , 2016 ) ( 2 ) vsLSTM is an LSTM based video summarization model ; ( 3 ) Proc - Nets ( Zhou et al , 2018a ) which is the previous SOTA method .", "entities": [[101, 102, "MethodName", "LSTM"], [125, 126, "DatasetName", "2048"], [151, 152, "MethodName", "Adam"], [152, 153, "HyperparameterName", "optimizer"], [163, 165, "HyperparameterName", "learning rate"], [168, 169, "HyperparameterName", "\u03b1"], [172, 173, "HyperparameterName", "\u03b2"], [181, 183, "HyperparameterName", "batch size"], [202, 204, "HyperparameterName", "batch size"], [248, 249, "MethodName", "LSTM"], [250, 252, "TaskName", "video summarization"]]}
{"text": "For evaluating procedure captioning , we consider two baseline models : ( 1 ) Bi - LSTM with temporal attention ( Yao et al , 2015 ) ( 2 ) an end - to - end transformer based video dense captioning model proposed in ( Zhou et al , 2018b ) . We evaluate the performance of captioning on two different procedures : ( 1 ) the ground - truth procedure ; ( 2 ) the procedure extracted by models . In Table 2 , we demonstrate that using ground - truth procedures can generate better captions . Additionally , our model achieves the SOTA result on BLEU - 4 and METEOR metrics when using the ground - truth procedures as well as the extracted procedures .", "entities": [[16, 17, "MethodName", "LSTM"], [18, 20, "MethodName", "temporal attention"], [107, 108, "MetricName", "BLEU"], [111, 112, "DatasetName", "METEOR"]]}
{"text": "We study several captioning results and find that the Caption by Video Model tends to generate general descriptions such as \" add ... \" for all steps . Nonetheless , our model tends to generate various fine - grained captions . Motivated by this , we conduct another experiment to use cherry picked sentence like add the chicken ( or beef , carrot , onion , etc . ) to the pan and stir or add pepper and salt to the bowl as the captions for all procedures and can still achieve a good result on BLEU ( 4.0 + ) and METEOR ( 16.0 + ) . We find that the distribution of captions in this dataset is biased because there are many similar procedure descriptions even in different recipes .", "entities": [[96, 97, "MetricName", "BLEU"], [102, 103, "DatasetName", "METEOR"]]}
{"text": "This paper proposes a new method of instance - based domain adaptation for sentiment analysis . Sentiment analysis involves judging a polarity , positive or negative , of a review such as a movie review . This is one of the document classification tasks and supervised learning can be used to solve it . However , if the domain of the test data is different from the domain for the learning data ( for example , book reviews ) , the accuracy of the classifier obtained through standard supervised learning reduced . This is the problem with domain shift . The solution to this problem is domain adaptation . Domain adaptation can be roughly divided into two categories : feature - based and instance - based ( Pan and Yang , 2010 ) . In summary , both are weightedlearning methods , but feature - based gives weights to features and instance - based gives weight to instance . Here , we present a new instance - based method . Generally , the instance - based method assumes a covariate shift , and gives the weight based on the probability density ratio between target domain and source domain . However , the computational cost for the instance - based method is too high . The method presented here is simple and its effect is better than methods using a typical probability density ratio . Our method first defines l w , the likelihood of the keyword of the word w using the IDF in the target domain . Using l w , the weight of a review x in the source domain is set as the keyword content rate w x . After that , weighted - learning is performed by giving w x to each document in the source domain x to overcome the domain shift . In the experiment , we used Amazon dataset ( Blitzer et al , 2007 ) , and compared our proposed method with two typical instance - based methods : unconstrained least squares importance fitting ( uL - SIF ) ( Yamada et al , 2011 ) using the probability density ratio and the method defining weight through Naive Bayes model ( Shinnou and Sasaki , 2014 ) , to demonstrate the effectiveness of the proposed method .", "entities": [[10, 12, "TaskName", "domain adaptation"], [13, 15, "TaskName", "sentiment analysis"], [16, 18, "TaskName", "Sentiment analysis"], [41, 43, "TaskName", "document classification"], [81, 82, "MetricName", "accuracy"], [106, 108, "TaskName", "domain adaptation"], [109, 111, "TaskName", "Domain adaptation"]]}
{"text": "NONE in Table1 is compared with the case weighting method ( uLSIF , NB , and our method ) . It is clear that NONE has a high positive solution rate . For theae data only , the instance - based method has no effect on domain adaptation . However , feature - based and instance - based methods are easy to combine . Here , the four domains applied in paper are adapted to B E , D B , E K and K D , and SCL conversion training is used first . The prime vector of the data , then experiment with the weighted - learning in this transformed vector is performed using the proposed method . The results are shown in Table 3 . The CORAL in Table 3 is taken from . From Table 3 , it can be seen that SCL has no effect . After SCL is combined with our method , the accuracy is not high enough . However , when SCL is combined with the proposed method , the precision for SCL alone is improved . The positive effect of the combined the instance - based and feature - based methods can be confirmed . There are many ways in useing the feature - based method , in addition to SCL , so it can be improved by combining these techniques with the proposed method . In addition , although the weighted - learning SVM is used in this paper , the loss value of the loss function in the neural network is multiplied by the weight , and it is easier to realize weighted - learning as the loss value . There are many options for using the domain adaptation method on deep learning , and these solutions combined with instance - based are easier to compute . In a simple example , we used the AutoEncoder ( AE ) as the feature - based method . Using AE , the dimension of the data in the source and target domain was reduced , that is , encoded . In learning and testing , we used the connected data of the orig - inal data and the encoded data instead of the original data . In learning , as described above , the value of the loss function was multiplied by the weight obtained by our method and was taken as the loss value FIG . 1 . Only the experiment of B E is performed , and the results in Table 4 FIG . 2 were obtained . In addition , in this experiment , neural network learning was ended in 50 epochs , and the correct rate was the result from evaluating the model obtained by learning data after 50 epochs . Moreover , the dimension was reduced to 200 . Every method used the same multi - layer perceptron , which has three layers .", "entities": [[46, 48, "TaskName", "domain adaptation"], [160, 161, "MetricName", "accuracy"], [243, 244, "MethodName", "SVM"], [251, 252, "MetricName", "loss"], [255, 256, "MetricName", "loss"], [278, 279, "MetricName", "loss"], [288, 290, "TaskName", "domain adaptation"], [316, 317, "MethodName", "AutoEncoder"], [318, 319, "MethodName", "AE"], [328, 329, "MethodName", "AE"], [386, 387, "MetricName", "loss"], [402, 403, "MetricName", "loss"]]}
{"text": "This paper describes the system of team LeisureX in the CoNLL 2018 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies . Our system predicts the part - of - speech tag and dependency tree jointly . For the basic tasks , including tokenization , lemmatization and morphology prediction , we employ the official baseline model ( UDPipe ) . To train the low - resource languages , we adopt a sampling method based on other richresource languages . Our system achieves a macro - average of 68.31 % LAS F1 score , with an improvement of 2.51 % compared with the UDPipe .", "entities": [[21, 23, "DatasetName", "Universal Dependencies"], [28, 31, "DatasetName", "part - of"], [47, 48, "TaskName", "lemmatization"], [92, 94, "MetricName", "F1 score"]]}
{"text": "The goal of Universal Dependencies ( UD ) ( Nivre et al , 2016 ; Zeman et al , 2017 ) is to develop multilingual treebank , whose annotations of morphology and syntax are cross - linguistically consistent . In this paper , we describe our system for the CoNLL 2018 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies ( Zeman et al , 2018 ) , and we focus only on the subtasks of part - of - speech ( POS ) tagging and dependency parsing . For the intermediate steps , including tokenization , lemmatization and morphology prediction , we tackle them by the official baseline model ( UDPipe ) 1 . Dependency parsing that aims to predict the existence and type of linguistic dependency relations between words , is a fundamental part in natural language processing ( NLP ) tasks ( Li et al , 2018c ; . Many referential natural language processing studies ( Zhang et al , 2018 ; Cai et al , 2018 ; Li et al , 2018b ; Wang et al , 2018 ; Qin et al , 2017 ) can also contribute to the universal dependency parsing system . Universal dependency parsing focuses on learning syntactic dependency structure over many typologically different languages , even low - resource languages in a real - world setting . Within the dependency parsing literature , there are two dominant techniques , graph - based ( McDonald et al , 2005 ; Ma and Zhao , 2012 ; Kiperwasser and Goldberg , 2016 ; and transition - based parsing ( Nivre , 2003 ; Dyer et al , 2015 ; . Graph - based dependency parsers enjoy the advantage of the global search which learns the scoring functions for all possible parsing trees to find the globally highest scoring one while transition - based dependency parsers build dependency trees from left to right incrementally , which makes the series of multiple choice decisions locally . In our system , we adopt the transition - based dependency parsing in view of its relatively lower time complexity . Our system implements universal dependency parsing based on the stack - pointer networks ( STACKPTR ) parser introduced by ( Ma et al , 2018 ) . Furthermore , previous work ( Straka et al , 2016 ; Nguyen et al , 2017 ) showed that POS tags are helpful to dependency parsing . In particular , ( Nguyen et al , 2017 ) pointed out that parsing performance could be improved by the merit of accurate POS tags and the context of syntactic parse tree could help resolve POS ambiguities . Therefore , we seek to jointly learn POS tagging and dependency parsing . As Long short - term memory ( LSTM ) networks ( Hochreiter and Schmidhuber , 1997 ) have shown significant representational effectiveness to a wide range of NLP tasks , we leverage bidirectional LSTMs ( BiLSTM ) to learn shared representations for both POS tagging and dependency parsing . In addition , to train the low - resource languages , we adopt a sampling method based on other richresource languages . In terms of all the above model improvement , compared to the UDPipe baseline , our system achieves a macro - average of 68.31 % LAS F1 score , with an improvement of 2.51 % in this task .", "entities": [[3, 5, "DatasetName", "Universal Dependencies"], [6, 7, "DatasetName", "UD"], [60, 62, "DatasetName", "Universal Dependencies"], [78, 81, "DatasetName", "part - of"], [88, 90, "TaskName", "dependency parsing"], [99, 100, "TaskName", "lemmatization"], [117, 119, "TaskName", "Dependency parsing"], [197, 199, "TaskName", "dependency parsing"], [202, 204, "TaskName", "dependency parsing"], [230, 232, "TaskName", "dependency parsing"], [340, 345, "TaskName", "transition - based dependency parsing"], [358, 360, "TaskName", "dependency parsing"], [405, 407, "TaskName", "dependency parsing"], [456, 458, "TaskName", "dependency parsing"], [460, 465, "MethodName", "Long short - term memory"], [466, 467, "MethodName", "LSTM"], [494, 495, "MethodName", "BiLSTM"], [505, 507, "TaskName", "dependency parsing"], [556, 558, "MetricName", "F1 score"]]}
{"text": "The universal dependency parsing component of our system is built on the current state - of - the - art approach STACKPTR , which combines pointer networks ( Vinyals et al , 2015 ) with an internal stack for tracking the status of depth - first search . It benefits from the global information of the sentence and all previously derived subtree structures , and removes the left - to - right restriction in classical transition - based parsers . The STACKPTR parser mainly consists of two parts : encoder and decoder . The encoder based on BiLSTM - CNNs architecture takes the sequence of tokens and their POS tags as input , then encodes it into encoder hidden state s i . The internal stack \u03c3 is initialized with dummy ROOT . For decoder ( a uni - directional RNN ) , it receives the input from last step and outputs decoder hidden state h t . The pointer neural network takes the top element w h in the stack \u03c3 at each timestep t as current head to select a specific child w c with biaffine attention mechanism for attention score function in all possible head - dependent pairs . Then the child w c will be pushed onto the stack \u03c3 for next step when c = h , otherwise it indicates that all children of the current head h have been selected , therefore the head w h will be popped out of the stack \u03c3 . The attention scoring function used is given as follows and the pointer neural network uses a t as pointer to select the child element : e t i = h T t Ws i + U T h t + V T s i + b a t = sof tmax ( e t ) More specifically , the decoder maintains a list of available words in test phase . For each head h at each decoding step , the selected child will be removed from the list to make sure that it can not be selected as a child of other head words . Given a dependency tree , there may be multiple children for a specific head . This results in more than one valid selection for each time step , which might confuse the decoder . To address this problem , the parser introduces an inside - outside order to utilize second - order sibling information , which has been proven to be an important feature for parsing process ( McDonald and Pereira , 2006 ; Koo and Collins , 2010 ) . To utilize the secondorder information , the parser replaces the input of decoder from s i as follows : \u03b2 i = s s s h s i where s and h indicate the sibling and head index of node i , is the element - wise sum operation to ensure no additional model parameters .", "entities": [[2, 4, "TaskName", "dependency parsing"], [97, 98, "MethodName", "BiLSTM"], [300, 301, "DatasetName", "sof"], [456, 457, "HyperparameterName", "\u03b2"]]}
{"text": "Our system focuses on three targets : the UPOS tag , dependency arc and dependency relation . Therefore , we rely on the UDPipe model ( Straka Treebank Sampling Breton KEB English , Irish Czech PUD Czech PDT English PUD English EWT Faroese OFT Norwegian , English , Danish , Swedish , German , Dutch Finnish PUD Finnish TDT Japanese Modern Japanese GSD Naija NSC English Swedish PUD Swedish Talbanken Thai PUD English , Chinese , Hindi , Vietnamese For treebanks with non - empty training dataset ( including treebanks whose training set is very small ) , we utilize the baseline model UDPipe trained on corresponding treebank , which has been provided by the organizer . For the remaining nine treebanks without training data , we construct the train dataset by sampling from the other training datasets according to the language similarity inspired by ( Zhao et al , 2009 ( Zhao et al , , 2010Wang et al , , 2016 , as detailed in Table 1 . Our system adopts the hyper - parameter configuration in ( Ma et al , 2018 ) , with a few exceptions . We initialize word vectors with 50 - dimensional pretrained word embeddings , 100 - dimensional tag embeddings and 512 - dimensional recurrent states ( in each direction ) . Our system drops embeddings and hidden states independently with 33 % probability . We optimize with Adam ( Kingma and Ba , 2015 ) , setting the learning rate to 1e \u22123 and \u03b2 1 = \u03b2 2 = 0.9 . Moreover , we train models for up to 100 epochs with batch size 32 on 3 NVIDIA GeForce GTX 1080Ti GPUs with 200 to 500 sentences per second and occupying 2 to 3 GB graphic memory each model . A full run over the test datasets on the TIRA virtual machine ( Potthast et al , 2014 ) takes about 12 hours . with absolute gains ( 1.28 - 3.08 % ) on average LAS , UAS , MLAS and CLAS . These results show that our joint model could improve the performance of universal dependency parsing . Surprisingly , in the case of POS tagging , our joint model obtains lower averaged accuracy in both UPOS and XPOS . The possible reason for performance degradation may be that we select all hyper - parameters based on English and do not tune them individually . Furthermore , we also compare the performance of our system with the baseline and the best scorer on big treebanks ( Table 3 ) , PUD treebanks ( Table 4 ) , low - resource languages ( Table 5 ) , respectively .", "entities": [[201, 203, "TaskName", "word embeddings"], [237, 238, "MethodName", "Adam"], [248, 250, "HyperparameterName", "learning rate"], [254, 255, "HyperparameterName", "\u03b2"], [257, 258, "HyperparameterName", "\u03b2"], [273, 275, "HyperparameterName", "batch size"], [357, 359, "TaskName", "dependency parsing"], [375, 376, "MetricName", "accuracy"]]}
{"text": "Since our model applies the baseline model for tokenization and segmentation , we show all results of focused metrics on each treebank in Table 6 . In addition , we compare our model with the best and the average results of top ten models on each treebank , using LAS F1 for the evaluation metric , as shown in Figure 2 .", "entities": [[50, 51, "MetricName", "F1"]]}
{"text": "Summarization is the task of condensing a piece of text to a shorter version that contains the main information from the original . There are two broad approaches to summarization : extractive and abstractive . Extractive methods assemble summaries exclusively from passages ( usually whole sentences ) taken directly from the source text , while abstractive methods may generate novel words and phrases not featured in the source text - as a human - written abstract usually does . The extractive approach is easier , because copying large Original Text ( truncated ) : lagos , nigeria ( cnn ) a day after winning nigeria 's presidency , muhammadu buhari told cnn 's christiane amanpour that he plans to aggressively fight corruption that has long plagued nigeria and go after the root of the nation 's unrest . buhari said he 'll \" rapidly give attention \" to curbing violence in the northeast part of nigeria , where the terrorist group boko haram operates . by cooperating with neighboring nations chad , cameroon and niger , he said his administration is confident it will be able to thwart criminals and others contributing to nigeria 's instability . for the first time in nigeria 's history , the opposition defeated the ruling party in democratic elections . buhari defeated incumbent goodluck jonathan by about 2 million votes , according to nigeria 's independent national electoral commission . the win comes after a long history of military rule , coups and botched attempts at democracy in africa 's most populous nation . Baseline Seq2Seq + Attention : UNK UNK says his administration is confident it will be able to destabilize nigeria 's economy . UNK says his administration is confident it will be able to thwart criminals and other nigerians . he says the country has long nigeria and nigeria 's economy . Pointer - Gen : muhammadu buhari says he plans to aggressively fight corruption in the northeast part of nigeria . he says he 'll \" rapidly give attention \" to curbing violence in the northeast part of nigeria . he says his administration is confident it will be able to thwart criminals . Pointer - Gen + Coverage : muhammadu buhari says he plans to aggressively fight corruption that has long plagued nigeria . he says his administration is confident it will be able to thwart criminals . the win comes after a long history of military rule , coups and botched attempts at democracy in africa 's most populous nation . Figure 1 : Comparison of output of 3 abstractive summarization models on a news article . The baseline model makes factual errors , a nonsensical sentence and struggles with OOV words muhammadu buhari . The pointer - generator model is accurate but repeats itself . Coverage eliminates repetition . The final summary is composed from several fragments . chunks of text from the source document ensures baseline levels of grammaticality and accuracy . On the other hand , sophisticated abilities that are crucial to high - quality summarization , such as paraphrasing , generalization , or the incorporation of real - world knowledge , are possible only in an abstractive framework ( see Figure 5 ) . Due to the difficulty of abstractive summarization , the great majority of past work has been extractive ( Kupiec et al , 1995 ; Paice , 1990 ; Saggion and Poibeau , 2013 ) . However , the recent success of sequence - to - sequence models ( Sutskever ... et al , 2014 ) , in which recurrent neural networks ( RNNs ) both read and freely generate text , has made abstractive summarization viable Rush et al , 2015 ; Zeng et al , 2016 ) . Though these systems are promising , they exhibit undesirable behavior such as inaccurately reproducing factual details , an inability to deal with out - of - vocabulary ( OOV ) words , and repeating themselves ( see Figure 1 ) . In this paper we present an architecture that addresses these three issues in the context of multi - sentence summaries . While most recent abstractive work has focused on headline generation tasks ( reducing one or two sentences to a single headline ) , we believe that longer - text summarization is both more challenging ( requiring higher levels of abstraction while avoiding repetition ) and ultimately more useful . Therefore we apply our model to the recently - introduced CNN/ Daily Mail dataset ( Hermann et al , 2015 ; , which contains news articles ( 39 sentences on average ) paired with multi - sentence summaries , and show that we outperform the stateof - the - art abstractive system by at least 2 ROUGE points . Our hybrid pointer - generator network facilitates copying words from the source text via pointing ( Vinyals et al , 2015 ) , which improves accuracy and handling of OOV words , while retaining the ability to generate new words . The network , which can be viewed as a balance between extractive and abstractive approaches , is similar to Gu et al 's ( 2016 ) CopyNet and Miao and Blunsom 's ( 2016 ) Forced - Attention Sentence Compression , that were applied to short - text summarization . We propose a novel variant of the coverage vector ( Tu et al , 2016 ) from Neural Machine Translation , which we use to track and control coverage of the source document . We show that coverage is remarkably effective for eliminating repetition .", "entities": [[0, 1, "TaskName", "Summarization"], [29, 30, "TaskName", "summarization"], [261, 262, "MethodName", "Seq2Seq"], [432, 433, "TaskName", "summarization"], [494, 495, "MetricName", "accuracy"], [510, 511, "TaskName", "summarization"], [546, 547, "TaskName", "summarization"], [614, 615, "TaskName", "summarization"], [719, 721, "TaskName", "text summarization"], [824, 825, "MetricName", "accuracy"], [878, 880, "DatasetName", "Sentence Compression"], [887, 889, "TaskName", "text summarization"], [908, 910, "TaskName", "Machine Translation"]]}
{"text": "Our baseline model is similar to that of , and is depicted in Figure 2 . The tokens of the article w i are fed one - by - one into the encoder ( a single - layer bidirectional LSTM ) , producing a sequence of encoder hidden states h i . On each step t , the decoder ( a single - layer unidirectional LSTM ) receives the word embedding of the previous word ( while training , this is the previous word of the reference summary ; at test time it is the previous word emitted by the decoder ) , and has decoder state s t . The attention distribution a t is calculated as in Bahdanau et al ( 2015 ) : e t i = v T tanh ( W h h i + W s s t + b attn ) ( 1 ) a t = softmax ( e t ) ( 2 ) where v , W h , W s and b attn are learnable parameters . The attention distribution can be viewed as For each decoder timestep a generation probability p gen [ 0 , 1 ] is calculated , which weights the probability of generating words from the vocabulary , versus copying words from the source text . The vocabulary distribution and the attention distribution are weighted and summed to obtain the final distribution , from which we make our prediction . Note that out - of - vocabulary article words such as 2 - 0 are included in the final distribution . Best viewed in color . a probability distribution over the source words , that tells the decoder where to look to produce the next word . Next , the attention distribution is used to produce a weighted sum of the encoder hidden states , known as the context vector h * t : h * t = \u2211 i a t i h i ( 3 ) The context vector , which can be seen as a fixedsize representation of what has been read from the source for this step , is concatenated with the decoder state s t and fed through two linear layers to produce the vocabulary distribution P vocab : P vocab = softmax ( V ( V [ s t , h * t ] + b ) + b ) ( 4 ) where V , V , b and b are learnable parameters . P vocab is a probability distribution over all words in the vocabulary , and provides us with our final distribution from which to predict words w : P ( w ) = P vocab ( w ) ( 5 ) During training , the loss for timestep t is the negative log likelihood of the target word w * t for that timestep : loss t = \u2212 log P ( w * t ) ( 6 ) and the overall loss for the whole sequence is : loss = 1 T \u2211 T t=0 loss t ( 7 )", "entities": [[38, 40, "MethodName", "bidirectional LSTM"], [65, 66, "MethodName", "LSTM"], [153, 154, "MethodName", "softmax"], [193, 194, "DatasetName", "0"], [256, 257, "DatasetName", "0"], [380, 381, "MethodName", "softmax"], [458, 459, "MetricName", "loss"], [478, 479, "MetricName", "loss"], [495, 496, "MetricName", "loss"], [502, 503, "MetricName", "loss"], [509, 510, "MetricName", "loss"]]}
{"text": "Our pointer - generator network is a hybrid between our baseline and a pointer network ( Vinyals et al , 2015 ) , as it allows both copying words via pointing , and generating words from a fixed vocabulary . In the pointer - generator model ( depicted in Figure 3 ) the attention distribution a t and context vector h * t are calculated as in section 2.1 . In addition , the generation probability p gen [ 0 , 1 ] for timestep t is calculated from the context vector h * t , the decoder state s t and the decoder input x t : p gen = \u03c3 ( w T h * h * t + w T s s t + w T x x t + b ptr ) ( 8 ) where vectors w h * , w s , w x and scalar b ptr are learnable parameters and \u03c3 is the sigmoid function . Next , p gen is used as a soft switch to choose between generating a word from the vocabulary by sampling from P vocab , or copying a word from the input sequence by sampling from the attention distribution a t . For each document let the extended vocabulary denote the union of the vocabulary , and all words appearing in the source document . We obtain the following probability distribution over the extended vocabulary : P ( w ) = p gen P vocab ( w ) + ( 1 \u2212 p gen ) \u2211 i : w i = w a t i ( 9 ) Note that if w is an out - of - vocabulary ( OOV ) word , then P vocab ( w ) is zero ; similarly if w does not appear in the source document , then \u2211 i : w i = w a t i is zero . The ability to produce OOV words is one of the primary advantages of pointer - generator models ; by contrast models such as our baseline are restricted to their pre - set vocabulary . The loss function is as described in equations ( 6 ) and ( 7 ) , but with respect to our modified probability distribution P ( w ) given in equation ( 9 ) .", "entities": [[13, 15, "MethodName", "pointer network"], [79, 80, "DatasetName", "0"], [134, 135, "DatasetName", "ptr"], [153, 154, "DatasetName", "ptr"], [357, 358, "MetricName", "loss"]]}
{"text": "Repetition is a common problem for sequenceto - sequence models ( Tu et al , 2016 ; , and is especially pronounced when generating multi - sentence text ( see Figure 1 ) . We adapt the coverage model of Tu et al ( 2016 ) to solve the problem . In our coverage model , we maintain a coverage vector c t , which is the sum of attention distributions over all previous decoder timesteps : c t = \u2211 t\u22121 t = 0 a t ( 10 ) Intuitively , c t is a ( unnormalized ) distribution over the source document words that represents the degree of coverage that those words have received from the attention mechanism so far . Note that c 0 is a zero vector , because on the first timestep , none of the source document has been covered . The coverage vector is used as extra input to the attention mechanism , changing equation ( 1 ) to : e t i = v T tanh ( W h h i + W s s t + w c c t i + b attn ) ( 11 ) where w c is a learnable parameter vector of same length as v. This ensures that the attention mechanism 's current decision ( choosing where to attend next ) is informed by a reminder of its previous decisions ( summarized in c t ) . This should make it easier for the attention mechanism to avoid repeatedly attending to the same locations , and thus avoid generating repetitive text . We find it necessary ( see section 5 ) to additionally define a coverage loss to penalize repeatedly attending to the same locations : covloss t = \u2211 i min ( a t i , c t i ) ( 12 ) Note that the coverage loss is bounded ; in particular covloss t \u2264 \u2211 i a t i = 1 . Equation ( 12 ) differs from the coverage loss used in Machine Translation . In MT , we assume that there should be a roughly oneto - one translation ratio ; accordingly the final coverage vector is penalized if it is more or less than 1 . Our loss function is more flexible : because summarization should not require uniform coverage , we only penalize the overlap between each attention distribution and the coverage so far - preventing repeated attention . Finally , the coverage loss , reweighted by some hyperparameter \u03bb , is added to the primary loss function to yield a new composite loss function : loss t = \u2212 log P ( w * t ) + \u03bb \u2211 i min ( a t i , c t i ) ( 13 ) 3 Related Work Neural abstractive summarization . Rush et al ( 2015 ) were the first to apply modern neural networks to abstractive text summarization , achieving state - of - the - art performance on DUC - 2004 and Gigaword , two sentence - level summarization datasets . Their approach , which is centered on the attention mechanism , has been augmented with recurrent decoders , Abstract Meaning Representations ( Takase et al , 2016 ) , hierarchical networks , variational autoencoders ( Miao and Blunsom , 2016 ) , and direct optimization of the performance metric ( Ranzato et al , 2016 ) , further improving performance on those datasets . However , large - scale datasets for summarization of longer text are rare . adapted the DeepMind question - answering dataset ( Hermann et al , 2015 ) for summarization , resulting in the CNN / Daily Mail dataset , and provided the first abstractive baselines . The same authors then published a neural extractive approach ( Nallapati et al , 2017 ) , which uses hierarchical RNNs to select sentences , and found that it significantly outperformed their abstractive result with respect to the ROUGE metric . To our knowledge , these are the only two published results on the full dataset . Prior to modern neural methods , abstractive summarization received less attention than extractive summarization , but Jing ( 2000 ) explored cutting unimportant parts of sentences to create summaries , and Cheung and Penn ( 2014 ) explore sentence fusion using dependency trees . Pointer - generator networks . The pointer network ( Vinyals et al , 2015 ) is a sequence - tosequence model that uses the soft attention distribution of Bahdanau et al ( 2015 ) to produce an output sequence consisting of elements from the input sequence . The pointer network has been used to create hybrid approaches for NMT ( Gulcehre et al , 2016 ) , language modeling ( Merity et al , 2016 ) , and summarization ( Gu et al , 2016 ; Gulcehre et al , 2016 ; Miao and Blunsom , 2016 ; Zeng et al , 2016 ) . Our approach is close to the Forced - Attention Sentence Compression model of Miao and Blunsom ( 2016 ) and the CopyNet model of Gu et al ( 2016 ) , with some small differences : ( i ) We calculate an explicit switch probability p gen , whereas Gu et al induce competition through a shared softmax function . ( ii ) We recycle the attention distribution to serve as the copy distribution , but Gu et al use two separate distributions . ( iii ) When a word appears multiple times in the source text , we sum probability mass from all corresponding parts of the attention distribution , whereas Miao and Blunsom do not . Our reasoning is that ( i ) calculating an explicit p gen usefully enables us to raise or lower the probability of all generated words or all copy words at once , rather than individually , ( ii ) the two distributions serve such similar purposes that we find our simpler approach suffices , and ( iii ) we observe that the pointer mechanism often copies a word while attending to multiple occurrences of it in the source text . Our approach is considerably different from that of Gulcehre et al ( 2016 ) and . Those works train their pointer components to activate only for out - of - vocabulary words or named entities ( whereas we allow our model to freely learn when to use the pointer ) , and they do not mix the probabilities from the copy distribution and the vocabulary distribution . We believe the mixture approach described here is better for abstractive summarization - in section 6 we show that the copy mechanism is vital for accurately reproducing rare but in - vocabulary words , and in section 7.2 we observe that the mixture model enables the language model and copy mechanism to work together to perform abstractive copying . Coverage . Originating from Statistical Machine Translation ( Koehn , 2009 ) , coverage was adapted for NMT by Tu et al ( 2016 ) and , who both use a GRU to update the coverage vector each step . We find that a simpler approach - summing the attention distributions to obtain the coverage vector - suffices . In this respect our approach is similar to Xu et al ( 2015 ) , who apply a coverage - like method to image cap - tioning , and Chen et al ( 2016 ) , who also incorporate a coverage mechanism ( which they call ' distraction ' ) as described in equation ( 11 ) into neural summarization of longer text . Temporal attention is a related technique that has been applied to NMT ( Sankaran et al , 2016 ) and summarization . In this approach , each attention distribution is divided by the sum of the previous , which effectively dampens repeated attention . We tried this method but found it too destructive , distorting the signal from the attention mechanism and reducing performance . We hypothesize that an early intervention method such as coverage is preferable to a post hoc method such as temporal attention - it is better to inform the attention mechanism to help it make better decisions , than to override its decisions altogether . This theory is supported by the large boost that coverage gives our ROUGE scores ( see Table 1 ) , compared to the smaller boost given by temporal attention for the same task .", "entities": [[84, 85, "DatasetName", "0"], [126, 127, "DatasetName", "0"], [281, 282, "MetricName", "loss"], [313, 314, "MetricName", "loss"], [338, 339, "MetricName", "loss"], [341, 343, "TaskName", "Machine Translation"], [378, 379, "MetricName", "loss"], [385, 386, "TaskName", "summarization"], [415, 416, "MetricName", "loss"], [428, 429, "MetricName", "loss"], [435, 436, "MetricName", "loss"], [438, 439, "MetricName", "loss"], [471, 472, "TaskName", "summarization"], [488, 491, "TaskName", "abstractive text summarization"], [512, 513, "TaskName", "summarization"], [548, 549, "MethodName", "autoencoders"], [586, 587, "TaskName", "summarization"], [608, 609, "TaskName", "summarization"], [613, 617, "DatasetName", "CNN / Daily Mail"], [690, 691, "TaskName", "summarization"], [695, 697, "TaskName", "extractive summarization"], [721, 723, "TaskName", "sentence fusion"], [733, 735, "MethodName", "pointer network"], [775, 777, "MethodName", "pointer network"], [805, 806, "TaskName", "summarization"], [841, 843, "DatasetName", "Sentence Compression"], [889, 890, "MethodName", "softmax"], [1108, 1109, "TaskName", "summarization"], [1161, 1163, "TaskName", "Machine Translation"], [1187, 1188, "MethodName", "GRU"], [1274, 1275, "TaskName", "summarization"], [1279, 1281, "MethodName", "Temporal attention"], [1299, 1300, "TaskName", "summarization"], [1363, 1365, "MethodName", "temporal attention"], [1415, 1417, "MethodName", "temporal attention"]]}
{"text": "For all experiments , our model has 256dimensional hidden states and 128 - dimensional word embeddings . For the pointer - generator models , we use a vocabulary of 50k words for both source and target - note that due to the pointer network 's ability to handle OOV words , we can use For the baseline model , we also try a larger vocabulary size of 150k . Note that the pointer and the coverage mechanism introduce very few additional parameters to the network : for the models with vocabulary size 50k , the baseline model has 21 , 499 , 600 parameters , the pointer - generator adds 1153 extra parameters ( w h * , w s , w x and b ptr in equation 8 ) , and coverage adds 512 extra parameters ( w c in equation 11 ) . Unlike , we do not pretrain the word embeddings - they are learned from scratch during training . We train using Adagrad ( Duchi et al , 2011 ) with learning rate 0.15 and an initial accumulator value of 0.1 . ( This was found to work best of Stochastic Gradient Descent , Adadelta , Momentum , Adam and RM - SProp ) . We use gradient clipping with a maximum gradient norm of 2 , but do not use any form of regularization . We use loss on the validation set to implement early stopping . During training and at test time we truncate the article to 400 tokens and limit the length of the summary to 100 tokens for training and 120 tokens at test time . 3 This is done to expedite training and testing , but we also found that truncating the article can raise the performance of the model ( see section 7.1 for more details ) . For training , we found it efficient to start with highly - truncated sequences , then raise the maximum length once converged . We train on a single Tesla K40 m GPU with a batch size of 16 . At test time our summaries are produced using beam search with beam size 4 . We trained both our baseline models for about 600 , 000 iterations ( 33 epochs ) - this is similar to the 35 epochs required by Nallapati et al 's ( 2016 ) best model . Training took 4 days and 14 hours for the 50k vocabulary model , and 8 days 21 hours for the 150k vocabulary model . We found the pointer - generator model quicker to train , requiring less than 230 , 000 training iterations ( 12.8 epochs ) ; a total of 3 days and 4 hours . In particular , the pointer - generator model makes much quicker progress in the early phases of training . To obtain our final coverage model , we added the coverage mechanism with coverage loss weighted to \u03bb = 1 ( as described in equation 13 ) , and trained for a further 3000 iterations ( about 2 hours ) . In this time the coverage loss converged to about 0.2 , down from an initial value of about 0.5 . We also tried a more aggressive value of \u03bb = 2 ; this reduced coverage loss but increased the primary loss function , thus we did not use it . We tried training the coverage model without the loss function , hoping that the attention mechanism may learn by itself not to attend repeatedly to the same locations , but we found this to be ineffective , with no discernible reduction in repetition . We also tried training with coverage from the first iteration rather than as a separate training phase , but found that in the early phase of training , the coverage objective interfered with the main objective , reducing overall performance .", "entities": [[14, 16, "TaskName", "word embeddings"], [42, 44, "MethodName", "pointer network"], [125, 126, "DatasetName", "ptr"], [152, 154, "TaskName", "word embeddings"], [166, 167, "MethodName", "Adagrad"], [175, 177, "HyperparameterName", "learning rate"], [194, 197, "MethodName", "Stochastic Gradient Descent"], [198, 199, "MethodName", "Adadelta"], [202, 203, "MethodName", "Adam"], [211, 213, "MethodName", "gradient clipping"], [232, 233, "MetricName", "loss"], [239, 241, "MethodName", "early stopping"], [342, 344, "HyperparameterName", "batch size"], [488, 489, "MetricName", "loss"], [520, 521, "MetricName", "loss"], [550, 551, "MetricName", "loss"], [555, 556, "MetricName", "loss"], [573, 574, "MetricName", "loss"]]}
{"text": "Our results are given in Table 1 . We evaluate our models with the standard ROUGE metric ( Lin , 2004b ) , reporting the F 1 scores for ROUGE - 1 , ROUGE - 2 and ROUGE - L ( which respectively measure the word - overlap , bigram - overlap , and longest common sequence between the reference summary and the summary to be evaluated ) . We obtain our ROUGE scores using the pyrouge package . 4 We also evaluate with the METEOR metric ( Denkowski and Lavie , 2014 ) , both in exact match mode ( rewarding only exact matches between words ) and full mode ( which additionally rewards matching stems , synonyms and paraphrases ) . 5 In addition to our own models , we also report the lead - 3 baseline ( which uses the first three sentences of the article as a summary ) , and compare to the only existing abstractive and extractive ( Nallapati et al , 2017 ) models on the full dataset . The output of our models is available online . 6 Given that we generate plain - text summaries but 2017 ) generate anonymized summaries ( see Section 4 ) , our ROUGE scores are not strictly comparable . There is evidence to suggest that the original - text dataset may result in higher ROUGE scores in general than the anonymized dataset - the lead - 3 baseline is higher on the former than the latter . One possible explanation is that multi - word named entities lead to a higher rate of n - gram overlap . Unfortunately , ROUGE is the only available means of comparison with Nallapati et al 's work . Nevertheless , given that the disparity in the lead - 3 scores is ( +1.1 ROUGE - 1 , +2.0 ROUGE - 2 , +1.1 ROUGE - L ) points respectively , and our best model scores exceed by ( +4.07 ROUGE - 1 , +3.98 ROUGE - 2 , +3.73 ROUGE - L ) points , we may estimate that we outperform the only previous abstractive system by at least 2 ROUGE points allround .", "entities": [[37, 40, "MetricName", "ROUGE - L"], [85, 86, "DatasetName", "METEOR"], [97, 99, "MetricName", "exact match"], [315, 318, "MetricName", "ROUGE - L"], [341, 344, "MetricName", "ROUGE - L"]]}
{"text": "We have shown that our pointer mechanism makes our abstractive system more reliable , copying factual details correctly more often . But does the ease of copying make our system any less abstractive ? Figure 6 shows that our final model 's summaries contain a much lower rate of novel n - grams ( i.e. , those that do n't appear in the article ) than the reference summaries , indicating a lower degree of abstraction . Note that the baseline model produces novel n - grams more frequently - however , this statistic includes all the incorrectly copied words , UNK tokens and fabrications alongside the good instances of abstraction . Figure 6 : Although our best model is abstractive , it does not produce novel n - grams ( i.e. , n - grams that do n't appear in the source text ) as often as the reference summaries . The baseline model produces more novel n - grams , but many of these are erroneous ( see section 7.2 ) . Article : andy murray ( ... ) is into the semi - finals of the miami open , but not before getting a scare from 21 year - old austrian dominic thiem , who pushed him to 4 - 4 in the second set before going down 3 - 6 6 - 4 , 6 - 1 in an hour and three quarters . ( ... ) Summary : andy murray defeated dominic thiem 3 - 6 6 - 4 , 6 - 1 in an hour and three quarters . Article : ( ... ) wayne rooney smashes home during manchester united 's 3 - 1 win over aston villa on saturday . ( ... ) Summary : manchester united beat aston villa 3 - 1 at old trafford on saturday . Figure 7 : Examples of abstractive summaries produced by our model ( bold denotes novel words ) . In particular , Figure 6 shows that our final model copies whole article sentences 35 % of the time ; by comparison the reference summaries do so only 1.3 % of the time . This is a main area for improvement , as we would like our model to move beyond simple sentence extraction . However , we observe that the other 65 % encompasses a range of abstractive techniques . Article sentences are truncated to form grammatically - correct shorter versions , and new sentences are composed by stitching together fragments . Unnecessary interjections , clauses and parenthesized phrases are sometimes omitted from copied passages . Some of these abilities are demonstrated in Figure 1 , and the supplementary material contains more examples . Figure 7 shows two examples of more impressive abstraction - both with similar structure . The dataset contains many sports stories whose summaries follow the X beat Y score on day tem - plate , which may explain why our model is most confidently abstractive on these examples . In general however , our model does not routinely produce summaries like those in Figure 7 , and is not close to producing summaries like in Figure 5 . The value of the generation probability p gen also gives a measure of the abstractiveness of our model . During training , p gen starts with a value of about 0.30 then increases , converging to about 0.53 by the end of training . This indicates that the model first learns to mostly copy , then learns to generate about half the time . However at test time , p gen is heavily skewed towards copying , with a mean value of 0.17 . The disparity is likely due to the fact that during training , the model receives word - by - word supervision in the form of the reference summary , but at test time it does not . Nonetheless , the generator module is useful even when the model is copying . We find that p gen is highest at times of uncertainty such as the beginning of sentences , the join between stitched - together fragments , and when producing periods that truncate a copied sentence . Our mixture model allows the network to copy while simultaneously consulting the language model - enabling operations like stitching and truncation to be performed with grammaticality . In any case , encouraging the pointer - generator model to write more abstractively , while retaining the accuracy advantages of the pointer module , is an exciting direction for future work .", "entities": [[444, 446, "DatasetName", "supplementary material"], [744, 745, "MetricName", "accuracy"]]}
{"text": "In text summarization , manual evaluation , as exemplified by the Pyramid method ( Nenkova and Passonneau , 2004 ) , is the gold - standard in evaluation . However , due to time required and relatively high cost of annotation , the great majority of research papers on summarization use exclusively automatic evaluation metrics , such as ROUGE ( Lin , 2004 ) , JS - 2 ( Louis and Nenkova , 2013 ) , S3 ( Peyrard et al , 2017 ) , BERTScore ( Zhang et al , 2020 ) , Mover - Score ( Zhao et al , 2019 ) etc . Among these metrics , ROUGE is by far the most popular , and there is relatively little discussion of how ROUGE may deviate from human judgment and the potential for this deviation to change conclusions drawn regarding relative merit of baseline and proposed methods . To characterize the relative goodness of evaluation metrics , it is necessary to perform metaevaluation ( Graham , 2015 ; Lin and Och , 2004 ) , where a dataset annotated with human judgments ( e.g. TAC 1 2008 ( Dang and Owczarzak , 2008 ) ) is used to test the degree to which automatic metrics correlate therewith . However , the classic TAC meta - evaluation datasets are now 6 - 12 years old 2 and it is not clear whether conclusions found there will hold with modern systems and summarization tasks . Two earlier works exemplify this disconnect : ( 1 ) Peyrard ( 2019 ) observed that the human - annotated summaries in the TAC dataset are mostly of lower quality than those produced by modern systems and that various automated evaluation metrics strongly disagree in the higher - scoring range in which current systems now operate . ( 2 ) Rankel et al ( 2013 ) observed that the correlation between ROUGE and human judgments in the TAC dataset decreases when looking at the best systems only , even for systems from eight years ago , which are far from today 's state - of - the - art . Constrained by few existing human judgment datasets , it remains unknown how existing metrics behave on current top - scoring summarization systems . In this paper , we ask the question : does the rapid progress of model development in summarization models require us to re - evaluate the evaluation process used for text summarization ? To this end , we create and release a large benchmark for meta - evaluating summarization metrics including : Outputs from 25 top - scoring extractive and abstractive summarization systems on the CNN / DailyMail dataset . Automatic evaluations from several evaluation metrics including traditional metrics ( e.g. ROUGE ) and modern semantic matching metrics ( e.g. BERTScore , MoverScore ) . Ability of metrics to Observations on existing human judgments ( TAC ) Observations on new human judgments ( CNNDM ) Exp - I : evaluate all systems ? ( Sec . 4.1 ) MoverScore and JS - 2 outperform all other metrics . ROUGE - 2 outperforms all other metrics . MoverScore and JS - 2 performs worse both in extractive ( only achieved nearly 0.1 Pearson correlation ) and abstractive summaries . Exp - II : evaluate top - k systems ? ( Sec . 4.2 ) As k becomes smaller , ROUGE - 2 de - correlates with humans . For extractive and abstractive systems , ROUGE - 2 highly correlates with humans . For evaluating a mix of extractive and abstractive systems , all metrics de - correlate . Exp - III : compare 2 systems ? ( Sec . 4.3 ) MoverScore and JS - 2 outperform all other metrics . ROUGE - 2 is the most reliable for abstractive systems while ROUGE - 1 is most reliable for extractive systems . Exp - IV : evaluate summaries ? ( Sec . 4.4 ) ( 1 ) MoverScore and JS - 2 outperform all other metrics . ( 2 ) Metrics have much lower correlations when evaluating summaries than systems . ( 1 ) ROUGE metrics outperform all other metrics . ( 2 ) For extractive summaries , most metrics are better at evaluating summaries than systems . For abstractive summaries , some metrics are better at summary level , others are better at system level . Manual evaluations using the lightweight pyramids method ( Shapira et al , 2019 ) , which we use as a gold - standard to evaluate summarization systems as well as automated metrics . Using this benchmark , we perform an extensive analysis , which indicates the need to re - examine our assumptions about the evaluation of automatic summarization systems . Specifically , we conduct four experiments analyzing the correspondence between various metrics and human evaluation . Somewhat surprisingly , we find that many of the previously attested properties of metrics found on the TAC dataset demonstrate different trends on our newly collected CNNDM dataset , as shown in Tab . 1 . For example , MoverScore is the best performing metric for evaluating summaries on dataset TAC , but it is significantly worse than ROUGE - 2 on our collected CNNDM set . Additionally , many previous works ( Novikova et al , 2017 ; Peyrard et al , 2017 ; Chaganty et al , 2018 ) show that metrics have much lower correlations at comparing summaries than systems . For extractive summaries on CNNDM , however , most metrics are better at comparing summaries than systems . Calls for Future Research These observations demonstrate the limitations of our current bestperforming metrics , highlighting ( 1 ) the need for future meta - evaluation to ( i ) be across multiple datasets and ( ii ) evaluate metrics on different application scenarios , e.g. summary level vs. system level ( 2 ) the need for more systematic metaevaluation of summarization metrics that updates with our ever - evolving systems and datasets , and ( 3 ) the potential benefit to the summarization community of a shared task similar to the WMT 3 Metrics Task in Machine Translation , where systems and metrics co - evolve . 3 http://www.statmt.org/wmt20/", "entities": [[1, 3, "TaskName", "text summarization"], [49, 50, "TaskName", "summarization"], [96, 97, "MetricName", "Score"], [243, 244, "TaskName", "summarization"], [376, 377, "TaskName", "summarization"], [396, 397, "TaskName", "summarization"], [409, 411, "TaskName", "text summarization"], [427, 428, "TaskName", "summarization"], [440, 441, "TaskName", "summarization"], [540, 542, "MetricName", "Pearson correlation"], [760, 761, "TaskName", "summarization"], [793, 794, "TaskName", "summarization"], [995, 996, "TaskName", "summarization"], [1017, 1018, "TaskName", "summarization"], [1031, 1033, "TaskName", "Machine Translation"]]}
{"text": "We examine eight metrics that measure the agreement between two texts , in our case , between the system summary and reference summary . BERTScore ( BScore ) measures soft overlap between contextual BERT embeddings of tokens between the two texts 4 ( Zhang et al , 2020 ) . MoverScore ( MScore ) applies a distance measure to contextualized BERT and ELMo word embeddings 5 ( Zhao et al , 2019 ) . Sentence Mover Similarity ( SMS ) applies minimum distance matching between text based on sentence embeddings ( Clark et al , 2019 ) . Word Mover Similarity ( WMS ) measures similarity using minimum distance matching between texts which are represented as a bag of word embeddings 6 ( Kusner et al , 2015 ) . JS divergence ( JS - 2 ) measures Jensen - Shannon divergence between the two text 's bigram distributions 7 ( Lin et al , 2006 ) . ROUGE - 1 and ROUGE - 2 measure overlap of unigrams and bigrams respectively 8 ( Lin , 2004 ) . ROUGE - L measures overlap of the longest common subsequence between two texts ( Lin , 2004 ) . We use the recall variant of all metrics ( since the Pyramid method of human evaluations is inherently recall based ) except MScore which has no specific recall variant .", "entities": [[33, 34, "MethodName", "BERT"], [60, 61, "MethodName", "BERT"], [62, 63, "MethodName", "ELMo"], [63, 65, "TaskName", "word embeddings"], [88, 90, "TaskName", "sentence embeddings"], [119, 121, "TaskName", "word embeddings"], [179, 182, "MetricName", "ROUGE - L"]]}
{"text": "Pearson Correlation is a measure of linear correlation between two variables and is popular in metaevaluating metrics at the system level ( Lee Rodgers , 1988 ) . We use the implementation given by Virtanen et al ( 2020 ) . William 's Significance Test is a means of calculating the statistical significance of differences in correlations for dependent variables ( Williams , 1959 ; Graham and Baldwin , 2014 ) . This is useful for us since metrics evaluated on the same dataset are not independent of each other .", "entities": [[0, 2, "MetricName", "Pearson Correlation"]]}
{"text": "In text summarization , a \" good \" summary should represent as much relevant content from the input document as possible , within the acceptable length limits . Many human evaluation methods have been proposed to capture this desideratum ( Nenkova and Passonneau , 2004 ; Chaganty et al , 2018 ; Fan et al , 2018 ; Shapira et al , 2019 ) . Among these , Pyramid ( Nenkova and Passonneau , 2004 ) is a reliable and widely used method , that evaluates content selection by ( 1 ) exhaustively obtaining Semantic Content Units ( SCUs ) from reference summaries , ( 2 ) weighting them based on the number of times they are mentioned and ( 3 ) scoring a system summary based on which SCUs can be inferred . Recently , Shapira et al ( 2019 ) extended Pyramid to a lightweight , crowdsourcable method - LitePyramids , which uses Amazon Mechanical Turk 10 ( AMT ) for gathering human annotations . LitePyramids simplifies Pyramid by ( 1 ) allowing crowd workers to extract a subset of all possible SCUs and ( 2 ) eliminating the difficult task of merging duplicate SCUs from different reference summaries , instead using SCU sampling to simulate frequency - based weighting . Both Pyramid and LitePyramid rely on the presence of multiple references per document to assign importance weights to SCUs . However in the CNNDM dataset there is only one reference summary per document . We therefore adapt the LitePyramid method for the single - reference setting as follows . SCU Extraction The LitePyramids annotation instructions define a Semantic Content Unit ( SCU ) as a sentence containing a single fact written as briefly and clearly as possible . Instead , we focus on shorter , more fine - grained SCUs that contain at most 2 - 3 entities . This allows for partial content overlap between a generated and reference summary , and also makes the task easy for workers . Tab . 2 gives an example . We exhaustively extract ( up to 16 ) SCUs 11 from each reference summary . Requiring the set of SCUs to be exhaustive increases the complexity of the SCU generation task , and hence instead of relying on crowd - workers , we create SCUs from reference summaries ourselves . In the end , we obtained nearly 10.5 SCUs on average from each reference summary . System Evaluation During system evaluation the full set of SCUs is presented to crowd workers . Workers are paid similar to Shapira et al ( 2019 ) , scaling the rates for fewer SCUs and shorter summary texts . For abstractive systems , we pay $ 0.20 per summary and for extractive systems , we pay $ 0.15 per summary since extractive summaries are more readable and might precisely overlap with SCUs . We post - process system output summaries before presenting them to annotators by true - casing the text using Stanford CoreNLP ( Manning et al , 2014 ) and replacing \" unknown \" tokens with a special symbol \" 2 \" ( Chaganty et al , 2018 ) . Tab . 2 depicts an example reference summary , system summary , SCUs extracted from the reference summary , and annotations obtained in evaluating the system summary . Annotation Scoring For robustness ( Shapira et al , 2019 ) , each system summary is evaluated by 4 crowd workers . Each worker annotates up to 16 SCUs by marking an SCU \" present \" if it can be ( a ) Reference Summary : Bayern Munich beat Porto 6 - 1 in the Champions League on Tuesday . Pep Guardiola 's side progressed 7 - 4 on aggregate to reach semi - finals . Thomas Muller scored 27th Champions League goal to pass Mario Gomez . Muller is now the leading German scorer in the competition . After game Muller led the celebrations with supporters using a megaphone . ( b ) System Summary ( BART , Lewis et al ( 2019 ) ) : Bayern Munich beat Porto 6 - 1 at the Allianz Arena on Tuesday night . Thomas Muller scored his 27th Champions League goal . The 25 - year - old became the highest - scoring German since the tournament took its current shape in 1992 . Bayern players remained on the pitch for some time as they celebrated with supporters . ( c ) SCUs with corresponding evaluations : Bayern Munich beat Porto . Bayern Munich won 6 - 1 . Bayern Munich won in Champions League . inferred from the system summary or \" not present \" otherwise . We obtain a total of 10 , 000 human annotations ( 100 documents \u00d7 25 systems \u00d7 4 workers ) . For each document , we identify a \" noisy \" worker as one who disagrees with the majority ( i.e. marks an SCU as \" present \" when majority thinks \" not present \" or vice - versa ) , on the largest number of SCUs . We remove the annotations of noisy workers and retain 7 , 742 annotations of the 10 , 000 . After this filtering , we obtain an average inter - annotator agreement ( Krippendorff 's alpha ( Krippendorff , 2011 ) ) of 0.66 . 12 Finally , we use the majority vote to mark the presence of an SCU in a system summary , breaking ties by the class , \" not present \" .", "entities": [[1, 3, "TaskName", "text summarization"], [674, 675, "MethodName", "BART"], [886, 887, "HyperparameterName", "alpha"]]}
{"text": "Automatic metrics are widely used to determine where a new system may rank against existing state - of - the - art systems . Thus , in meta - evaluation studies , calculating correlation of automatic metrics with human judgments at the system level is a commonly - used setting ( Novikova et al , 2017 ; Bojar et al , 2016 ; Graham , 2015 ) . We follow this setting and specifically , ask two questions : Can metrics reliably compare different systems ? To answer this we observe the Pearson correlation between different metrics and human judgments in Fig . 2 , finding that : ( 1 ) MoverScore and JS - 2 , which were the best performing metrics on TAC , have poor correlations with humans in comparing CNNDM Ext systems . ( 2 ) Most metrics have high correlations on the TAC - 2008 dataset but many suffer on TAC - 2009 , especially ROUGE based metrics . However , ROUGE metrics consistently perform well on the collected CNNDM datasets . Are some metrics significantly better than others in comparing systems ? Since automated metrics calculated on the same data are not independent , we must perform the William 's test ( Williams , 1959 ) to establish if the difference in correlations between metrics is statistically significant ( Graham and Baldwin , 2014 ) . In Fig . 1 we report the pvalues of William 's test . We find that Figure 1 : p - value of William 's Significance Test for the hypothesis \" Is the system on left ( y - axis ) significantly better than system on top ( x - axis ) \" . ' BScore ' refers to BERTScore and ' MScore ' refers to MoverScore . A dark green value in cell ( i , j ) denotes metric m i has a significantly higher Pearson correlation with human scores compared to metric m j ( p - value < 0.05 ) . 13 ' - ' in cell ( i , j ) refers to the case when Pearson correlation of m i with human scores is less that of m j ( Sec . 4.1 ) . ( 1 ) MoverScore and JS - 2 are significantly better than other metrics in correlating with human judgments on the TAC datasets . ( 2 ) However , on CNNDM Abs and CNNDM Mix , R - 2 significantly outperforms all others whereas on CNNDM Ext none of the metrics show significant improvements over others . Takeaway : These results suggest that metrics run the risk of overfitting to some datasets , highlighting the need to meta - evaluate metrics for modern datasets and systems . Additionally , there is no one - size - fits - all metric that can outperform others on all datasets . This suggests the utility of using different metrics for different datasets to evaluate systems e.g. MoverScore on TAC - 2008 , JS - 2 on TAC - 2009 and R - 2 on CNNDM datasets .", "entities": [[92, 94, "MetricName", "Pearson correlation"], [319, 321, "MetricName", "Pearson correlation"], [353, 355, "MetricName", "Pearson correlation"]]}
{"text": "Instead of comparing many systems ( Sec . 4.1 , 4.2 ) ranking two systems aims to test the discriminative power of a metric , i.e. , the degree to which the metric can capture statistically significant differences between two summarization systems . We analyze the reliability of metrics along a useful dimension : can metrics reliably say if one system is significantly better than another ? Since we only have 100 annotated summaries to compare any two systems , sys 1 and sys 2 , we use paired bootstrap resampling , to test with statistical sig - nificance if sys 1 is better than sys 2 according to metric m ( Koehn , 2004 ; Dror et al , 2018 ) . We take all J 2 pairs of systems and compare their mean human score ( Eqn . 3 ) using paired bootstrap resampling . We assign a label y true = 1 if sys 1 is better than sys 2 with 95 % confidence , y true = 2 for viceversa and y true = 0 if the confidence is below 95 % . We treat this as the ground truth label of the pair ( sys 1 , sys 2 ) . This process is then repeated for all metrics , to get a \" prediction \" , y m pred from each metric m for the same J 2 pairs . If m is a good proxy for human judgments , the F1 score ( Goutte and Gaussier , 2005 ) between y m pred and y true should be high . We calculate the weighted macro F1 score for all metrics and view them in Fig . 4 . We find that ROUGE based metrics perform moderately well in this task . R - 2 performs the best on CNNDM datasets . While on the TAC 2009 dataset , JS - 2 achieves the highest F1 score , its performance is low on CNNDM Ext . Takeaway : Different metrics are better suited for different datasets . For example , on the CNNDM datasets , we recommend using R - 2 while , on the TAC datasets , we recommend using JS - 2 .", "entities": [[40, 41, "TaskName", "summarization"], [96, 97, "MethodName", "sig"], [178, 179, "DatasetName", "0"], [247, 249, "MetricName", "F1 score"], [271, 273, "MetricName", "macro F1"], [321, 323, "MetricName", "F1 score"]]}
{"text": "In addition to comparing systems , real - world application scenarios also require metrics to reliably compare multiple summaries of a document . For example , top - scoring reinforcement learning based summarization systems ( B\u00f6hm et al , 2019 ) and the current state - of - the - art extractive system ( Zhong et al , 2020 ) heavily rely on summary - level reward scores to guide the optimization process . In this experiment , we ask the question : how well do different metrics perform at the summary level , i.e. in comparing system summaries generated from the same document ? We use Eq . 1 to calculate Pearson correlation between different metrics and human judgments for different datasets and collected system outputs . Our observations are summarized in Fig . 5 . We find that : ( 1 ) As compared to semantic matching metrics , R - 1 , R - 2 and R - L have lower correlations on the TAC datasets but are strong indicators of good summaries especially for extractive summaries on the CNNDM dataset . ( 2 ) Notably , BERTScore , WMS , R - 1 and R - L have negative correlations on TAC - 2009 but perform moderately well on other datasets including CNNDM . ( 3 ) Previous meta - evaluation studies ( Novikova et al , 2017 ; Peyrard et al , 2017 ; Chaganty et al , 2018 ) conclude that automatic metrics tend to correlate well with humans at the system level but have poor correlations at the instance ( here summary ) level . We find this observation only holds on TAC - 2008 . Some metrics ' summary - level correlations can outperform system - level on the CNNDM dataset as shown in Fig . 7b ( bins below y = 0 ) . Notably , MoverScore has a correlation of only 0.05 on CNNDM Ext at the system level but 0.74 at the summary level . Takeaway : Meta - evaluations of metrics on the old TAC datasets show significantly different trends than meta - evaluation on modern systems and datasets . Even though some metrics might be good at comparing summaries , they may point in the wrong direction when comparing systems . Moreover , some metrics show poor generalization ability to different datasets ( e.g. BERTScore on TAC - 2009 vs other datasets ) . This highlights the need for empirically testing the efficacy of different automatic metrics in evaluating summaries on multiple datasets .", "entities": [[32, 33, "TaskName", "summarization"], [112, 114, "MetricName", "Pearson correlation"], [310, 311, "DatasetName", "0"]]}
{"text": "The effectiveness of different automatic metrics - ROUGE - 2 ( Lin , 2004 ) , ROUGE - L ( Lin , 2004 ) , ROUGE - WE ( Ng and Abrecht , 2015 ) , JS - 2 ( Louis and Nenkova , 2013 ) and S3 ( Peyrard et al , 2017 ) is commonly evaluated based on their correlation with human judgments ( e.g. , on the TAC - 2008 ( Dang and Owczarzak , 2008 ) and TAC - 2009 ( Dang andOwczarzak , 2009 ) datasets ) . As an important supplementary technique to metaevaluation , Graham ( 2015 ) advocate for the use of a significance test , William 's test ( Williams , 1959 ) , to measure the improved correlations of a metric with human scores and show that the popular variant of ROUGE ( mean ROUGE - 2 score ) is sub - optimal . Unlike these works , instead of proposing a new metric , in this paper , we upgrade the meta - evaluation environment by introducing a sizeable human judgment dataset evaluating current top - scoring systems and mainstream datasets . And then , we re - evaluate diverse metrics at both systemlevel and summary - level settings . ( Novikova et al , 2017 ) also analyzes existing metrics , but they only focus on dialog generation .", "entities": [[16, 19, "MetricName", "ROUGE - L"]]}
{"text": "The identification of PHI is a type of named entity recognition task where sensitive named entities specifically are identified . The first study with CRF - based de - identification for Swedish was on the gold standard Stockholm EPR PHI Corpus . The distribution of PHIs is shown in Table 1 . In this instance , manual annotation with expert consensus was used to create the gold standard ( Dalianis and Velupillai , 2010 ) . De - identification tasks based on the CRF machine learning algorithm has been carried out on this data set previously with precision scores ranging between 85 % and 95 % , recalls ranging between 71 % and 87 % and F1 - scores between 0.76 and 0.91 ( Dalianis and Velupillai , 2010 ; Berg and Dalianis , 2019 ) . One approach previously used for concealing the training set 's sensitive data was carried out by , using the Stockholm EPR PHI Corpus . In the study , the textual part of the data were used to create 14 different features and part of speech tags . The textual part was then removed , and only the features and part of speech tags were used for training a Random Forest model . Fairly high precision of 89.1 % was obtained , but with a recall of 54.3 % and F1 - score of 64.8 . In contrast to using only the sensitive EHR data for training , McMurry et al ( 2013 ) integrated both publicly available scientific , medical publications and private sensitive clinical notes to develop a de - identification system . While considering the term frequencies and part of speech tags between the two data sources , they used both rule lists and decision trees for their system . This was an interesting approach since it raised the prospect of using non - sensitive data in building useful deidentification models . However , it is not clear whether medical journals have significant advantages over any other public text , like news corpora , for detecting PHI . A study similar to Mc - Murry et al ( 2013 ) , by Berg and Dalianis ( 2019 ) , showed few benefits of combining non - medical public text and sensitive clinical notes to build a de - identification system for medical records . More recently , deep learning approaches using recurrent neural networks seem to yield significant improvements over traditional rules - based methods or statistical machine learning ( Dernoncourt et al , 2017 ) . Still , recent studies indicate that combining several approaches will yield the best results . For instance , the best system in a recent de - identification shared task was a combination of bidirectional LSTM , CRF and a rule - based subsystem ( Liu et al , 2017 ) . Significant domain variation , such as a different language , is an important factor that was not considered in the discussed shared task . Domain differences were cited as the reason for poor performance on psychiatric notes de - identification ( Stubbs et al , 2017 ) , compared with the previous de - identification task on general clinical narratives ( Stubbs et al , 2015 ) . Within the same language and similar clinical settings , the change of domain is likely not substantial . While in future research it may be worth considering domain adaption techniques to work towards a system meant to be used between hospitals , they were not considered in this study , beyond the use of non - sensitive dictionaries for names and location .", "entities": [[8, 11, "TaskName", "named entity recognition"], [24, 25, "MethodName", "CRF"], [27, 30, "TaskName", "de - identification"], [76, 79, "TaskName", "De - identification"], [83, 84, "MethodName", "CRF"], [116, 117, "MetricName", "F1"], [226, 229, "MetricName", "F1 - score"], [266, 269, "TaskName", "de - identification"], [385, 388, "TaskName", "de - identification"], [450, 453, "TaskName", "de - identification"], [459, 461, "MethodName", "bidirectional LSTM"], [462, 463, "MethodName", "CRF"], [514, 517, "TaskName", "de - identification"], [529, 532, "TaskName", "de - identification"]]}
{"text": "The results of the experimental work are summarised in Figure 2 . As can be observed in the figure , the CRF algorithm seems to generally outperform the LSTM algorithm on all metrics ; precision , recall and F1 measure . This result is not consistent with repeated reports in the literature , where deep learning apsklearn - crfsuite.readthedocs.io/en/ latest/ 5 word2vec , https://github.com/tmikolov/ word2vec proaches such as LSTM have been shown to out - perform most other methods , including CRF . Since deep learning approaches normally require very large amounts of data , one explanation for this result could be that the word embeddings used in this study did not contain sufficient context variations required for more robust performance or an insufficient training set of annotated data . The ability to identify date part and age entities are similar when training on pseudonymised data and real data for the CRF . In contrast , Location , Health Care Unit and Full Date were negatively affected when using pseudonymised training data regardless of using a CRF or LSTM model .", "entities": [[21, 22, "MethodName", "CRF"], [28, 29, "MethodName", "LSTM"], [38, 39, "MetricName", "F1"], [68, 69, "MethodName", "LSTM"], [81, 82, "MethodName", "CRF"], [104, 106, "TaskName", "word embeddings"], [151, 152, "MethodName", "CRF"], [176, 177, "MethodName", "CRF"], [178, 179, "MethodName", "LSTM"]]}
{"text": "Experimental results of the CRF algorithm are shown in Table 3 . Not presented in the table is the combination of training on real data and evaluation of pseudo data ( Real - Pseudo ) , but the results of this combination gave a precision of 86.37 and recall of 77.80 % and an F1 - score of 81.86 .", "entities": [[4, 5, "MethodName", "CRF"], [54, 57, "MetricName", "F1 - score"]]}
{"text": "The experimental results of the LSTM algorithm are shown in Table 4 and again , not presented in the table is the combination of training on real data and evaluation of pseudo data ( Real - Pseudo ) . The result of this combination is a precision of 65.83 % and recall of 74.79 % and F1 - score of 70.03 .", "entities": [[5, 6, "MethodName", "LSTM"], [56, 59, "MetricName", "F1 - score"]]}
{"text": "In a world abounding in constant protests resulting from events like a global pandemic , climate change , religious or political conflicts , there has always been a need to detect events / protests before getting amplified by news media or social media . This paper demonstrates our work on the sentence classification subtask of multilingual protest detection in CASE@ACL - IJCNLP 2021 . We approached this task by employing various multilingual pre - trained transformer models to classify if any sentence contains information about an event that has transpired or not . Furthermore , we performed soft voting over the models , achieving the best results among the models , accomplishing a macro F1 - Score of 0.8291 , 0.7578 , and 0.7951 in English , Spanish , and Portuguese , respectively . The source codes for our systems are published 1 .", "entities": [[51, 53, "TaskName", "sentence classification"], [114, 117, "MetricName", "F1 - Score"]]}
{"text": "Bidirectional Encoder Representations from Transformers ( BERT ) ( Devlin et al , 2019 ) is a pretrained language model which was created with the objective that fine - tuning a pretrained model yields better performance . BERT 's pretraining phase includes two tasks . Firstly , Masked Language Modeling ( MLM ) is where certain words are randomly masked in a sequence . About 15 % of the words in a sequence is masked . The model then attempts to predict the masked words . Secondly , Next Sentence Prediction ( NSP ) , where the model has an additional loss function , NSP loss , indicates if the second sequence follows the first one . Around 50 % of the inputs are a pair , and they randomly chose the other 50 . Here , we use a bert - base - multilingual - cased ( Pires et al , 2019 ) trained on top of 104 languages in the largest Wikipedia corpus . This model has 12 layers , 12 Attention heads with over 179 million parameters .", "entities": [[6, 7, "MethodName", "BERT"], [37, 38, "MethodName", "BERT"], [47, 50, "TaskName", "Masked Language Modeling"], [51, 52, "DatasetName", "MLM"], [101, 102, "MetricName", "loss"], [105, 106, "MetricName", "loss"]]}
{"text": "DistilBERT ( Sanh et al , 2019 ) is the distilled version of BERT . DistilBERT employs a triple loss language modelling , where it integrates cosine distance loss with knowledge distillation . DistilBERT has 40 % fewer parameters than BERT but still promises 97 % of the latter 's performance . It is also 60 % faster than BERT . In this system , we used a cased multilingual DistilBERT model as they are three different languages . For our cause , we finetune distilbert - base - multilingual - cased , which is distilled from the mBERT checkpoint . The model has 6 layers , 768 dimensions , and 12 Attention heads , totalizing about 134 million parameters .", "entities": [[0, 1, "MethodName", "DistilBERT"], [13, 14, "MethodName", "BERT"], [15, 16, "MethodName", "DistilBERT"], [19, 20, "MetricName", "loss"], [20, 22, "TaskName", "language modelling"], [28, 29, "MetricName", "loss"], [30, 32, "MethodName", "knowledge distillation"], [33, 34, "MethodName", "DistilBERT"], [40, 41, "MethodName", "BERT"], [59, 60, "MethodName", "BERT"], [70, 71, "MethodName", "DistilBERT"], [85, 86, "MethodName", "distilbert"], [98, 99, "MethodName", "mBERT"]]}
{"text": "For our system , we fine - tune the pretrained models discussed in Section 4.1 , 4.2 , and 4.3 . We combine the three datasets as the number of samples for Spanish and Portuguese are quite low . After combining the models , we split the validation set accordingly , maintaining the split 's ratio and tabulating the results on the concatenated dataset in Table3 . The embeddings are extracted from these models to be fed as input to the LSTM layer , ( Hochreiter and Schmidhuber , 1997 ) as shown in Figure1 . The resulting output is fed into a global average pooling layer ( Lin et al , 2014 ) and then passed into fully connected layers , followed by a sigmoid activation function to obtain the resulting probability score for the input sentences . The same parameters are used for all three models . A dropout layer ( Srivastava et al , 2014 ) is also added in between the fully connected layers for regularization . Refer Table 4 for the parameters used in the model . English ( 22 , 825 ) . We also believe that our approach of combining datasets could have influenced the performance of the low support datasets .", "entities": [[28, 31, "HyperparameterName", "number of samples"], [81, 82, "MethodName", "LSTM"], [103, 106, "MethodName", "global average pooling"], [125, 127, "MethodName", "sigmoid activation"]]}
{"text": "The need to develop automated systems to detect any event is an active protest has constantly been increasing because of the escalation of social media users and several platforms to support them . In this paper , we have explored several multilingual language models to classify if a given sentence talks about an event that has happened ( Event ) or not ( Not - event ) in three languages . Our work primarily focuses on fine - tuning language models and feeding them to an architecture we created . We also observe that the problem of class imbalance has had a significant impact on the performance of the models . The soft voting approach has achieved macro F1 - Scores of 0.8291 , 0.7578 , and 0.7951 for English , Spanish , and Portuguese , respectively . For future work , we intend to explore class weighting techniques and semi - supervised approaches to improve our performance .", "entities": [[117, 119, "MetricName", "macro F1"]]}
{"text": "Accurate detection of emotion from natural language has applications ranging from building emotional chatbots to better understanding individuals and their lives . However , progress on emotion detection has been hampered by the absence of large labeled datasets . In this work , we build a very large dataset for fine - grained emotions and develop deep learning models on it . We achieve a new state - of - the - art on 24 fine - grained types of emotions ( with an average accuracy of 87.58 % ) . We also extend the task beyond emotion types to model Robert Plutchik 's 8 primary emotion dimensions , acquiring a superior accuracy of 95.68 % .", "entities": [[3, 4, "DatasetName", "emotion"], [26, 27, "DatasetName", "emotion"], [84, 86, "MetricName", "average accuracy"], [97, 98, "DatasetName", "emotion"], [106, 107, "DatasetName", "emotion"], [112, 113, "MetricName", "accuracy"]]}
{"text": "The SemEval - 2007 Affective Text task ( Strapparava and Mihalcea , 2007 ) [ SEM07 ] focused on classification of emotion and valence ( i.e. , positive and negative texts ) in news headlines . A total of 1 , 250 headlines were manually labeled with the 6 basic emotions of Ekman ( Ekman , 1972 ) and made available to participants . Similarly , ( Aman and Szpakowicz , 2007 ) describe an emotion annotation task of identifying emotion category , emotion intensity and the words / phrases that indicate emotion in blog post data of 4 , 090 sentences and a system exploiting the data . Our work differs from both that of SEM07 ( Strapparava and Mihalcea , 2007 ) and ( Aman and Szpakowicz , 2007 ) in that we focus on a different genre ( i.e. , Twitter ) and investigate distant supervision as a way to acquire a significantly larger labeled dataset . Our work is similar to ( Mohammad , 2012 ; Mohammad and Kiritchenko , 2015 ) , ( Wang et al , 2012 ) , and ( Volkova and Bachrach , 2016 ) who use distant supervision to acquire Twitter data with emotion hashtags and report analyses and experiments to validate the utility of this approach . For example , ( Mohammad , 2012 ) shows that by using a simple domain adaptation method to train a classifier on their data they are able to improve both precision and recall on the SemEval - 2007 ( Strapparava andMihalcea , 2007 ) dataset . As the author points out , this is another premise that the selflabeled hashtags acquired from Twitter are consistent , to some degree , with the emotion labels given by the trained human judges who labeled the SemEval - 2007 data . As pointed out earlier , ( Wang et al , 2012 ) randomly sample a set of 400 tweets from their data and human - label as relevant / irrelevant , as a way to verify the distant supervision approach with the quality assurance heuristics they employ . The authors found that the precision on a test set is 93.16 % , thus confirming the utility of the heuristics . ( Wang et al , 2012 ) provide a number of important observations , as conclusions based on their work . These include that since they are provided by the tweets ' writers , the emotion hashtags are more natural and reliable than the emotion labels traditionally assigned by annotators to data by a few annotators . This is the case since in the lab - condition method annotators need to infer the writers emotions from text , which may not be accurate . Additionally , ( Volkova and Bachrach , 2016 ) follow the same distant supervision approach and find correlations of users ' emotional tone and the perceived demographics of these users ' social networks exploiting the emotion hashtag - labeled data . Our dataset is more than an order of magnitude larger than ( Mohammad , 2012 ) and ( Volkova and Bachrach , 2016 ) and the range of emotions we target is much more fine grained than ( Mohammad , 2012 ; Wang et al , 2012 ; Volkova and Bachrach , 2016 ) since we model 24 emotion types , rather than focus on \u2264 7 basic emotions . ( Yan et al , 2016 ; Yan and Turtle , 2016a , b ) develop a dataset of 15 , 553 tweets labeled with 28 emotion types and so target a fine - grained range as we do . The authors instruct human annotators under lab conditions to assign any emotion they feel is expressed in the data , allowing them to assign more than one emotion to a given tweet . A set of 28 chosen emotions was then decided upon and further annotations were performed using Amazon Mechanical Turk ( AMT ) . The authors cite an agreement of 0.50 Krippendorff 's alpha ( \u03b1 ) between the lab / expert annotators , and an ( \u03b1 ) of 0.28 between experts and AMT workers . EmoTweet - 28 is a useful resource . However , the agreement between annotators is not high and the set of assigned labels do not adhere to a specific theory of emotion . We use a much larger dataset and report an accuracy of the hashtag approach at 90 % based on human judgement as reported in Section 4 .", "entities": [[4, 6, "DatasetName", "Affective Text"], [21, 22, "DatasetName", "emotion"], [75, 76, "DatasetName", "emotion"], [80, 81, "DatasetName", "emotion"], [83, 84, "DatasetName", "emotion"], [92, 93, "DatasetName", "emotion"], [202, 203, "DatasetName", "emotion"], [231, 233, "TaskName", "domain adaptation"], [289, 290, "DatasetName", "emotion"], [410, 411, "DatasetName", "emotion"], [419, 420, "DatasetName", "emotion"], [494, 495, "DatasetName", "emotion"], [558, 559, "DatasetName", "emotion"], [571, 574, "DatasetName", "Yan et al"], [596, 597, "DatasetName", "emotion"], [621, 622, "DatasetName", "emotion"], [637, 638, "DatasetName", "emotion"], [675, 676, "HyperparameterName", "alpha"], [677, 678, "HyperparameterName", "\u03b1"], [689, 690, "HyperparameterName", "\u03b1"], [730, 731, "DatasetName", "emotion"], [741, 742, "MetricName", "accuracy"]]}
{"text": "In their work , ( Wang et al , 2012 ) manually label a random sample of 400 tweets extracted with hash - tags in a similar way as we acquire our data and find that human annotators agree 93 % of the time with the hashtag emotion type if the hashtag occurs as the last word in the tweet . We wanted to validate our use of hashtags in a similar fashion and on a bigger random sample . We had human annotators label a random sample of 5 , 600 tweets that satisfy our preprocessing pipeline . Manual inspection during annotation resulted in further removing a negligible 16 tweets that were found to have problems . For each of the remaining 5 , 584 tweets , the annotators assign a binary tag from the set { relevant , irrelevant } to indicate whether a tweet carries an emotion category as assigned using our distant supervision method or not . Annotators assigned 61.37 % ( n = 3 , 427 ) \" relevant \" tags and 38.63 % ( n = 2 , 157 ) \" irrelevant \" tags . Our analysis of this manually labeled dataset al o supports the findings of ( Wang et al , 2012 ) : When we limit position of the emotion hashtag to the end of a tweet , we acquire 90.57 % relevant data . We also find that if we relax the constraint on the hashtag position such that we allow the hashtag to occur in the last quarter of a tweet ( based on a total tweet character count ) , we acquire 85.43 % relevant tweets . We also find that only 23.20 % ( n = 795 out of 3 , 427 ) of the emotion carrying tweets have the emotion hashtags occurring in final position , whereas 31.75 % ( n = 1 , 088 out of 3 , 427 ) of the tweets have the emotion hashtags in the last quarter of the tweet string . This shows how enforcing a final hashtag location results in loss of a considerable number of emotion tweets . As shown in Table 2 , only 1 , 608 , 233 tweets out of a total of 6 , 851 , 955 tweets ( % = 23 , 47 ) in our bigger dataset have emotion hashtags occurring in final position . Overall , we agree with ( Mohammad , 2012 ; Wang et al , 2012 ) that the accuracy acquired by enforcing a strict pipeline and limiting to emotion hashtags to final position is a reasonable measure for warranting good - quality data for training supervised systems , an assumption we have also validated with our empirical findings here . One advantage of using distant supervision under these conditions for labeling emotion data , as ( Wang et al , 2012 ) also notes , is that the label is assigned by the writer of the tweet himself / herself rather than an annotator who could wrongly decide what category a tweet is . After all , emotion is a fuzzy concept and > 90 % agreement as we report here is higher than the human agreement usually acquired on many NLP tasks . Another advantage of this method is obviously that it enables us to acquire a sufficiently large training set to use deep learning . We now turn to describing our deep learning methods .", "entities": [[47, 48, "DatasetName", "emotion"], [149, 150, "DatasetName", "emotion"], [218, 219, "DatasetName", "emotion"], [298, 299, "DatasetName", "emotion"], [303, 304, "DatasetName", "emotion"], [330, 331, "DatasetName", "emotion"], [351, 352, "MetricName", "loss"], [357, 358, "DatasetName", "emotion"], [396, 397, "DatasetName", "emotion"], [421, 422, "MetricName", "accuracy"], [431, 432, "DatasetName", "emotion"], [474, 475, "DatasetName", "emotion"], [520, 521, "DatasetName", "emotion"]]}
{"text": "As explained earlier , Plutchik organizes the 24 emotion types in the 3 main circles that we will refer to as plutchik - 1 , plutchik - 2 , and plutchik - 3 . Emotion Qadir ( 2013 ) Roberts ( 2012 ) MD ( 2015 We model the set of emotions belonging to each of the 3 circles independently , thus casting each as an 8 - way classification task . Inspired by observations from the literature and our own annotation study , we limit our data to tweets of at least 5 words with an emotional hashtag occurring at the end . We then split the data representing each of the 3 circles into 80 % training ( TRAIN ) , 10 % development ( DEV ) , and 10 % testing ( TEST ) . As mentioned above , we run experiments with a range of online , out - of - core classifiers as well as the GRNNs . To train the GRNNs , we optimize the hyper - parameters of the network on a development set as we describe below , choosing a vocabulary size of 80 K words ( a vocabulary size we also use for the out - of - core classifiers ) , a word embedding vector of size 300 dimensions learnt directly from the training data , an input maximum length of 30 words , 7 epochs , and the Adam ( Kingma and Ba , 2014 ) optimizer with a learning rate of 0.001 . We use 3 dense layers each with 1 , 000 units . We use dropout ( Hinton et al , 2012 ) for regularization , with a dropout rate of 0.5 . For our loss function , we use categorical cross - entropy . We use a minibatch ( Cotter et al , 2011 ) size of 128 . We found this architecture to work best with almost all the settings and so we fix it across the board for all experiments with GRNNs . Results with Traditional Classifiers Results with the online classifiers are presented in terms of F - score in Table 3 . As the table shows , among this group of classifiers , the Passive Agressive classifier ( PAC ) acquires the best performance . PAC achieves an overall F - score of 64.86 % on plutchik - 1 , 53.30 % on plutchik - 2 , and 68.14 % on plutchik - 3 , two of which are higher than an arbitrary baseline 3 of 60 % . Results with GRNNs Table 4 presents results with GRNNs , compared with the best results using the traditional classifiers as acquired with PAC . As the table shows , the GRNN models are very successful across all the 3 classification tasks . With GRNNs , we acquire an overall F - scores of : 91.21 % on plutchik - 1 , 82.32 % on plutchik - 2 , and 87.47 % on plutchik - 3 . These results are 26.35 % , 29.02 % , and 25.37 % higher than PAC , respectively . Negative Results We experiment with aug - menting training data reported here in two ways : 1 ) For each emotion type , we concatenate the training data with training data of tweets that are more ( or less ) intense from the same sector / dimension in the wheel , and 2 ) for each emotion type , we add tweets where emotion hashtags occur in the last quarter of a tweet ( which were originally filtered out from TRAIN ) . However , we gain no improvements based on either of these methods , thus reflecting the importance of using high - quality training data and the utility of our strict pipeline .", "entities": [[8, 9, "DatasetName", "emotion"], [72, 73, "DatasetName", "Inspired"], [239, 240, "MethodName", "Adam"], [247, 248, "HyperparameterName", "optimizer"], [250, 252, "HyperparameterName", "learning rate"], [289, 290, "MetricName", "loss"], [542, 543, "DatasetName", "emotion"], [578, 579, "DatasetName", "emotion"], [585, 586, "DatasetName", "emotion"]]}
{"text": "We now investigate the task of predicting each of the 8 primary emotion dimensions represented by the sectors of the wheel ( where the three degrees of intensity of a given emotion are reduced to a single emotion dimension [ e.g. , { ecstasy , joy , serenity } are reduced to the joy dimension ] ( Volkova and Bachrach , 2016 ) 's model . ( TRAIN - ALL ) , the 10 % DEV to form DEV - ALL , and the 10 % TEST to form TEST - ALL . We test a number of hyper - parameters on DEV and find the ones we have identified on the fine - grained prediction to work best and so we adopt them as is with the exception of limiting to only 2 epochs . We believe that with a wider exploration of hyperparameters , improvements could be possible . As Table 5 shows , we are able to model the 8 dimensions with an overall superior accuracy of 95.68 % . As far as we know , this is the first work on modeling these dimensions .", "entities": [[12, 13, "DatasetName", "emotion"], [31, 32, "DatasetName", "emotion"], [37, 38, "DatasetName", "emotion"], [168, 169, "MetricName", "accuracy"]]}
{"text": "We compare our results on the 8 basic emotions to the published literature . As Table 6 shows , on this subset of emotions , our system is 4.53 % ( acc ) higher than the best published results ( Volkova and Bachrach , 2016 ) , facilitated by the fact that we have an order of magnitude more training data . As shown in Table 7 , we also apply ( Volkova and Bachrach , 2016 ) 's pre - trained model on our test set of the 6 emotions they predict ( which belong to plutchik - 2 ) , and acquire an overall accuracy of 26.95 % , which is significantly lower than our accuracy .", "entities": [[31, 32, "MetricName", "acc"], [105, 107, "MetricName", "overall accuracy"], [117, 118, "MetricName", "accuracy"]]}
{"text": "The size of the resulting lexicons ( a complete list is provided in Table 8 in the Appendix ) ranges from roughly 100k to more than 2 M entries mainly depending on the vocabulary of the respective embeddings . We want to point out that not every single entry should be considered meaningful because of noise in the embedding vocabulary caused by typos and tokenization errors . However , choosing the \" best \" size for an emotion lexicon necessarily translates into a quality - coverage trade - off for which there is no general solution . Instead , we release the full - size lexicons and leave it to prospective users to apply any sort of filtering they deem appropriate . Silver Evaluation . Figure 2 displays the results of our silver evaluation . Languages ( x - axis ) are sorted by their average performance over all variables ( not shown in the plot ; tabular data given in the Appendix ) . As can be seen , the evaluation results for English are markedly better than for any other language . This is not surprising since no ( potentially error - prone ) machine translation was performed . Apart from that , performance remains relatively stable across most of the languages and starts degrading more quickly only for the last third of them . In particular , for Valence - typically the easiest variable to predict - we achieve a strong performance of r > .7 for 56 languages . On the other hand , for Arousal - typically , the most difficult one to predict - we achieve a solid performance of r > .5 for 55 languages . Dominance and the discrete emotion variables show performance trajectories swinging between these two extremes . We assume that the main factors for explaining performance differences between languages are the quality of the translation and embedding models which , in turn , both depend on the amount of available text data ( parallel or monolingual , respectively ) . Comparing MTLFFN and ridge baseline , we find that the neural network reliably outperforms the linear model . On average over all languages and variables , the MTL models achieve 6.7 % - points higher Pearson correlation . Conversely , ridge regression outperforms MTLFFN in only 15 of the total 728 cases ( 91 languages \u00d7 8 variables ) . Gold Evaluation . Results for VAD variables on gold data are given in Table 3 . As can be seen , our lexicons show a good correlation with human judgment and do so robustly , even for less - resourced languages , such as Indonesian ( i d ) , Turkish ( tr ) , or Croatian ( hr ) , and across affective variables . Perhaps the strongest negative outliers are the Arousal results for the two Chinese datasets ( zh ) , which are likely to result from the low reliability of the gold ratings ( see below ) . Buechel and Hahn ( 2018b ) . Shared words between TargetGold and TargetPred - test ; ( % ) : percentage relative to TargetGold ; Mn ( all ) : mean over all datasets ; Mn ( vs. monolingual ) : mean over datasets with comparative results . We compare these results against those from Buechel and Hahn ( 2018b ) which were acquired on the respective TargetGold dataset in a monolingual fashion using 10 - fold cross - validation ( 10 - CV ) . We admit that those results are not fully comparable to those presented here because we use fixed splits rather than 10 - CV . Nevertheless , we find that the results of our cross - lingual set - up are more than competitive , outperforming the monolingual results from Buechel and Hahn ( 2018b ) in 17 out of 30 cases ( mainly for Valence and Dominance , less often for Arousal ) . This is surprising since we use an otherwise identical model and training procedure . We conjecture that the large size of the English Source lexicon , compared to most TargetGold lexicons , more than compensates for error - prone machine translation . Table 4 shows the results for BE5 datasets which are in line with the VAD results . Regarding the ordering of the emotional variables , again , we find Valence to be the easiest one to predict , Arousal the hardest , whereas basic emotions and Dominance take a middle ground . Comparison against Human Reliability . We base this analysis on inter - study reliability ( ISR ) , a rather strong criterion for human performance . ISR is computed , per variable , as the correlation between the ratings from two distinct annotation studies ( Warriner et al , 2013 ) . Hence , this analysis is restricted to languages where more than one gold lexicon exists per emotion format . We intersect the entries from both gold standards as well as the respective TargetPred - test set and compute the correlation between all three pairs of lexicons . If our lexicon agrees more with one of the gold standards than the two gold standards agree with each other , we consider this as an indicator for superhuman reliability ( Buechel and Hahn , 2018b ) . As shown in Table 5 , our lexicons are often competitive with human reliability for Valence ( especially for English and Chinese ) , but outperform human reliability in 4 out of 6 cases for Arousal , and in the single test case for Dominance . There are no cases of overlapping gold standards for BE5 . G o l d 1 G o l d 2 S h a r e d E m o G 1 v s G 2 G 1 v s P r G 2 v", "entities": [[77, 78, "DatasetName", "emotion"], [196, 198, "TaskName", "machine translation"], [287, 288, "DatasetName", "emotion"], [376, 378, "MetricName", "Pearson correlation"], [702, 704, "TaskName", "machine translation"], [825, 826, "DatasetName", "emotion"]]}
{"text": "This section investigates patterns in prediction quality across languages , validating design decisions of our methodology . Translation vs. Prediction . Is it beneficial to predict new ratings for the words in TargetMT rather than using them as final lexicon entries straight away ? For each TargetGold lexicon ( cf . Table 2 ) , we intersect its word material with that in TargetMT and TargetPred . Then , we compute the correlation between TargetPred and TargetMT with the gold standard . This analysis was done on the respective train sets because using TargetMT rather than TargetPred is only an option for entries known at training time . Table 6 depicts the results of this comparison averaged over all gold lexicons . As hypothesized , the TargetPred lexicons agree , on average , more with human judgment than the TargetMT lexicons , suggesting that the word emotion model acts as a value - adding post - processor , partly mitigating rating inconsistencies introduced by mere translation of the lexicons . The observation holds for each individual emotion variable with particularly large benefits for Arousal , where the postprocessed TargetPred lexicons are on average 14 % - points better compared to the translation - only TargetMT lexicons . This seems to indicate that lexical Arousal is less consistent between translational equivalents compared to other emotional meaning components like Valence and Sadness , which appear to be more robust against translation . V a l A r o D o m J o y A n g S a d F e a D i Gold vs. Silver Evaluation . How meaningful is silver evaluation without gold data ? We compute the Pearson correlation between gold and silver evaluation results across languages per emotion variable . For languages where we consider multiple datasets during gold evaluation , we first average the gold evaluation results for each emotion variable . As can be seen from Table 7 , the correlation values range between r = .91 for Joy and r = .27 for Disgust . This relatively large dispersion is not surprising when we take into account that we correlate very small data series ( for Valence and Arousal there are just 12 languages for which both gold and silver evaluation results are available ; for BE5 there are only 5 such languages ) . However , the mean over all correlation values in Table 7 is .64 , indicating that there is a relatively strong correlation between both types of evaluation . This suggests that the silver evaluation may be used as a rather reliable proxy of lexicon quality even in the absence of language - specific gold data . Table 7 : Agreement between gold and silver evaluation across languages in Pearson 's r relative to the number of applicable languages ( \" # Lg \" ) .", "entities": [[17, 18, "TaskName", "Translation"], [146, 147, "DatasetName", "emotion"], [176, 177, "DatasetName", "emotion"], [280, 282, "MetricName", "Pearson correlation"], [291, 292, "DatasetName", "emotion"], [314, 315, "DatasetName", "emotion"]]}
{"text": "Training of the MTLFFN model closely followed the procedure specified by Buechel and Hahn ( 2018b ) : For each language , the model was trained for roughly 15k iterations ( exactly 168 epochs ) with a batch size of 128 using the Adam optimizer ( Kingma and Ba , 2015 ) with learning rate 10 \u22123 , and .5 dropout on the hidden layers and .2 on the input layer . As nonlinear activation function we used leaky ReLU with \" leakage \" of 0.01 . Embedding vectors are the only model input . They have 300 dimensions for every language , independent of their respective training data size ( Grave et al , 2018 ) . Since the automatic translation of Source is not guaranteed to result in single - word translations , we use the following workaround to derive embedding vectors for multi - token translations : If the translation as a whole can not be found in the embedding model , the multi - token term gets split up into its constituent parts , using spaces , apostrophes or hyphens as separators . Each substring is looked up in the embedding model , the averaged vector is taken as input . If no substring is recognized , we use the zero vector instead . We also use the zero vector for single - token entries in TargetMT that are missing in the embeddings . Since Buechel and Hahn ( 2018b ) considered only VAD but not BE5 datasets , we conducted a development experiment on the TargetMT - dev sets for all 91 languages where we assessed whether MTL is advantageous for BE5 variables as well , or for a combination of VAD and BE5 variables . We found that MTL improved performance when applied separately among all VAD and BE5 variables . Yet , when jointly learning all eight emotion variables , the results were somewhat inconclusive . Performance increased for BE5 , but decreased for VAD . Hence , for lexicon creation , we took a cautious approach and trained two separate models per language , one for VAD , the other for BE5 . An analysis of MTL across VAD and BE5 is left for future work . The MTLFFN model is implemented in PY - TORCH , adapting part of the TENSORFLOW code from Buechel and Hahn ( 2018b ) . The ridge regression baseline model is implemented with SCIKIT - LEARN ( Pedregosa et al , 2011 )", "entities": [[37, 39, "HyperparameterName", "batch size"], [43, 44, "MethodName", "Adam"], [44, 45, "HyperparameterName", "optimizer"], [53, 55, "HyperparameterName", "learning rate"], [74, 76, "HyperparameterName", "activation function"], [78, 80, "MethodName", "leaky ReLU"], [314, 315, "DatasetName", "emotion"]]}
{"text": "Two general types of evaluation metrics are commonly used to evaluate paraphrase generation : automatic evaluation and human evaluation . Automatic Evaluation Several automatic evaluation metrics are used for the evaluation of paraphrase generation . The widely - used metrics include ( 1 ) BLEU ( Papineni et al , 2002 ) , which was originally developed to evaluate machine translation systems ; ( 2 ) METEOR ( Denkowski and Lavie , 2014 ) , which aims to address BLEU 's weakness of being unable to measure semantic equivalents when applied to low - resource languages and has a better correlation with human judgment at the sentence / segment level than BLEU ; ( 3 ) ROUGE ( Lin , 2004 ) , a recall - based evaluation metric originally developed for text summarization , has also been used to evaluate paraphrase generation . Its versions , ROUGE - N ( computing the n - gram recall ) and ROUGE - L ( focusing on the longest common subsequence ) are mostly used . ( 4 ) TER ( Snover et al , 2006 ) , which was also developed to evaluate machine translation . It measures the number of edits that a human translator would have to perform to change a translation so it exactly matches a reference translation . A TER score is a value in the range of 0 - 1 , but is frequently presented as a percentage , where lower is better . Human Evaluation Due to the fact that automatic evaluation metrics mainly focus on the ngram overlaps instead of meaning , human evaluation is used to provide a more accurate and qualitative evaluation of the generated output . In human evaluation , human annotators are asked to score generated paraphrases along multiple dimensions of quality such as similarity , clarity , and fluency . Owing to the manual annotation efforts , human evaluation is naturally more costly compared to automatic evaluation , but more representative of the quality of the generated output .", "entities": [[11, 13, "TaskName", "paraphrase generation"], [32, 34, "TaskName", "paraphrase generation"], [44, 45, "MetricName", "BLEU"], [59, 61, "TaskName", "machine translation"], [66, 67, "DatasetName", "METEOR"], [79, 80, "MetricName", "BLEU"], [111, 112, "MetricName", "BLEU"], [132, 134, "TaskName", "text summarization"], [141, 143, "TaskName", "paraphrase generation"], [159, 162, "MetricName", "ROUGE - L"], [192, 194, "TaskName", "machine translation"], [231, 232, "DatasetName", "0"]]}
{"text": "Model - focused improvements only aim to utilize various mechanisms to enhance the encoder or the decoder without paying special attention to the attributes of the generated paraphrases ( e.g. , granularity level such as word - level , phrase - level and sentence - level ) . Attention The Attention mechanism ( Bahdanau et al , 2015 ) enables the decoder to focus on some words / phrases that are of high relevance when generating a word . First , a weight for each token in the source sequence in each timestep is computed to indicate the importance , emphasizing the important information from the input and de - emphasizing the unimportant information . Given the weight distribution over all the tokens in the source sequence , this extra input vector , the context vector , is provided to the decoder .. Copy To counter the effect of rare and outof - vocabulary words in neural sequence models , ( Vinyals et al , 2015 ) proposed a pointer network . A pointer network copies an element from the input sequence directly into the output . Similarly , copy mechanism copies a span of elements from the input sequence decided by attention mechanism directly into the output . With copy mechanism , the decoder is able to determine whether a generate mode or a copy mode should be used at each timestep . First introduced by Gu et al ( 2016 ) for abstractive summarization , Cao et al ( 2017 ) haev also applied the copy mechanism to paraphrase generation . Despite the advantage of generating well - formed paraphrases by using the copy mechanism , it leads to the undesirable consequence of making a paraphrase contain many of the phrases in the original sentence and limits diversity . This calls for a controlled use of the copy mechanism during paraphrase generation . Gupta et al ( 2017 ) implemented the encoder and decoder with LSTMs , whereas the transformer is utilized by Roy and Grangier ( 2019 ) . Reinforcement Learning As pointed out by ( Ranzato et al , 2015 ) , a well - known problem of the encoder - decoder architecture is exposure bias : the decoding of current word is conditioned on the gold references during training but on the generated output from the last timestep during testing . Therefore , the error might be accumulated and propagated when testing . Another problem lies in the mismatch between the training goal and the evaluation metrics . While the generated paraphrases are finally evaluated automatically using the previously mentioned metrics , the network is trained to maximize the probability of generating the reference paraphrases . Therefore , minimizing the training loss might not correspond to optimizing the evaluation metric . To address this limitation , reinforcement learning ( RL ) is leveraged . RL aims to train an agent to interact with the environment with the goal of maximizing its reward . Toward finding an optimal policy , RL can be used to maximize the reward indicated as a desired evaluation metric or a combination of multiple desired metrics . Rather than minimizing loss ( the conventional approach ) , first utilized RL to maximize the reward given by an evaluator which outputs a real value to represent the matching degree between two sentences as paraphrases of each other . Other reward functions have been explored by researchers , including ROUGE score , perplexity score and language fluency ( Siddique et al , 2020 ; . Generative adversarial networks ( GAN ) Proposed by Goodfellow et al ( 2014 ) , GANs consist of generators and discriminators , where generators try to generate realistic outputs that match the real distribution and discriminators try to distinguish between the samples generated by generators and the samples that are real . GAN is originally trained by minimax optimization proposed in ( Goodfellow et al , 2014 ) . However , when GAN is applied in text generation , the traditional training method can not be used because generating discrete words is non - differentiable . Therefore , the idea of policy gradient ( Sutton et al , 1999 ) is leveraged to solve this problem ( Yu et al , 2017 ) . With policy gradient applied , discriminators act like the reward function in RL . Moreover , different discriminators can provide different desired rewards and thus equip the model with the capacity to generating text with different conditions . Here , a model is usually trained in an adversarial way : generators and discriminators are first pretrained , then generators are trained to maximize the loss of the fixed discriminators , then generators are fixed and discriminators are again trained to minimize the loss by provided the real samples and the samples generated by the fixed generators . For the task of paraphrase generation , different discriminators are designed to distinguish between generated samples and real samples , paraphrases and non - paraphrases ( Yang et al , 2019 ; Vizcarra and Ochoa - Luna , 2020 ) .", "entities": [[169, 171, "MethodName", "pointer network"], [173, 175, "MethodName", "pointer network"], [245, 246, "TaskName", "summarization"], [260, 262, "TaskName", "paraphrase generation"], [312, 314, "TaskName", "paraphrase generation"], [456, 457, "MetricName", "loss"], [484, 485, "DatasetName", "agent"], [529, 530, "MetricName", "loss"], [579, 580, "MetricName", "perplexity"], [596, 597, "MethodName", "GAN"], [644, 645, "MethodName", "GAN"], [664, 665, "MethodName", "GAN"], [668, 670, "TaskName", "text generation"], [780, 781, "MetricName", "loss"], [798, 799, "MetricName", "loss"], [817, 819, "TaskName", "paraphrase generation"]]}
{"text": "For attribute - focused improvements , their purpose is to improve the quality of generated paraphrases in some specific aspects such as diversity and also provide control over some attributes of generated paraphrases such as syntax and granularity level . These attribute - focused works usually use the previously mentioned models as their backbone models . Based on the backbone models , different mechanisms are applied for different focuses . Diversity Attempts focusing on diversity aim to generate multiple diverse paraphrases for a given sentence . Some works control diversity by provid - ing control signals to the decoder . Random pattern embeddings are used by . ( Kumar et al , 2019 ) utilized a submodular mechanism to maximize submodular functions measuring fidelity and diversity . , and ( Cao and Wan , 2020 ) all generate diverse paraphrases by providing the decoder with different latent patterns as control signal . Furthermore , ( Cao and Wan , 2020 ) also incorporated their model with a diversity loss to control diversity . use RL with multiple reward functions to generate diverse paraphrases . One of the reward functions computes ROUGE score between a generated sentence and original sentence , which can focus on the word variations and diversity . Acting like a reward function in RL , discriminators naturally can be used to provide control over some desired attributes . ( Qian et al , 2019 ) utilized multiple generators in GAN to generate multiple diverse paraphrases . A generator discriminator is used to distinguish sentences generated by different generators and guarantee the generated paraphrases are diverse enough . Word - Level Works on word - level paraphrasing mainly focus on generating paraphrases by replacing original words in the source texts with synonyms . Some works leveraged external linguistic knowledge ( Cao et al , 2017 ; Lin et al , 2020 ) . ( Cao et al , 2017 ) utilized an alignment table capturing many synonym mappings based on the IBM Model ( Chahuneau et al , 2013 ) . ( Lin et al , 2020 ) utilized WordNet ( Miller , 1995 to retrieve synonyms . Other works instead proposed special mechanisms to learn a mapping of synonyms ( Ma et al , 2018 ; Fu et al , 2019 ) . For example , ( Ma et al , 2018 ) utilized retrieved - based method to learn such a mapping . ( Fu et al , 2019 ) incorporates a novel latent bag - of - word mechanism into seq2seq model for content planning , which mainly provides candidate synonyms for words in the source texts . However , generating paraphrases only on a word - level makes the quality and diversity of generated paraphrases limited . Therefore , paraphrasing has also been studied on other granularity level , e.g. syntax level . Syntax Works in this category explore methods to provide control over the syntax of generated paraphrases . Basically , all the methods used by previous works can be split into two classes : 1 . Explicit Control and 2 . Implicit Control . Methods in the first class first encode the syntax tree of an exemplar sentence into a list of vector representations and then feed them into decoder at each timestep when decoding ( Iyyer et al , 2018 ; Chen et al , 2019 ; Goyal and Durrett , 2020 ; Kumar et al , 2020 ) . These methods can provide explicit control over the syntax of generated paraphrases and thus has better interpretability . The second class of methods will first learn a distribution over syntax information by VAE . Then a latent syntax variable sampled from the learned distribution will be fed into decoder at each decoding step . Although the control provided by this method is implicit , it does not require exemplar sentences and also can group multiple related syntax under the same latent assignment . Multi - Level Focusing on a single granularity level of paraphrasing still makes generated paraphrases limited . Therefore , researchers also explore methods to combine multiple granularity levels together . Such attempts equip their model with the capacity of generating synonyms , substituting phrases and also rearrange sentential structures ( Li et al , 2019 ; Huang et al , 2019 ; Kazemnejad et al , 2020 ) . By using multiple encoders , ( Li et al , 2019 ) and ( Kazemnejad et al , 2020 ) both enable their models to capture paraphrasing patterns on different granularity levels . ( Huang et al , 2019 ) instead utilized the help of external linguistic knowledge from the paraphrase database ( Ganitkevitch et al , 2013 ) to retrieve and learn word - level and phraselevel paraphrases . With different methods , both of them successfully combine multiple granularity levels together when generating paraphrases . 6 State - of - the - Art Performance Table 3 shows the ROUGE and BLEU scores of state - of - the - art performance on some most frequently used evaluation corpus in recent years : MSCOCO and Quora . Due to the facts that different metrics are used in different works , different datasets are used in different works and many of them did not release their codes , Table 3 is not fully filled . However , with most of the table filled , we can still have some observations worth mentioning . First , the use of attention mechanism achieves a close performance on Quora but has a better performance on MSCOCO ( row 1 and 2 ) . Similarly , the simple application of VAE also achieves a close performance on Quora but further improves the performance on MSCOCO ( row4 ) . With the copy mechanism , the Seq2Seq model is able to retain some words and thus yields a much better results ( row 3 ) . Transformer ( row 5 ) outperforms all the Seq2Seq - based models without copy mechanism ( row 1 , 2 , 4 , 6 ) , which shows the advantages of Transformer and meanwhile also proves the effectiveness of copy mechanism . Second , a model that employs RL ( row 7 ) has a great advantage for generating better paraphrases because of the reward provided . Therefore , a well designed optimization goal plays an important role in the task of paraphrase generation . Third , a novel decoding algorithm based on large pretrained language models helps to generate better paraphrases at the word level ( row 8 ) because of the strength of large pretrained language models and the synonyms learned by decoding algorithm . Fourth , the attempts to improve paraphrase generation with a special focus on combining multiple granularity levels also yield good performance ( row 9 , 10 ) . When learning to generate paraphrase in word level , phrase level and sentence level at the same time , their models improve the performance on multiple metrics compared with their backbone Transformer model ( row 5 ) . Finally , incorporating syntax control into paraphrase generation will also yield better results at word level and sentence level ( row 11 , 12 ) . Compared with implicit control ( row 11 ) , explicit control has a much better performance ( row 12 ) based on Quora . It should be noted that most of the works utilize two datasets for experiments ( as shown in Table 4 ) with one of them focusing on question paraphrases and the other focusing on general sentence paraphrases . Quora is the most popular dataset for question paraphrases . However , for corpus focusing on general sentnece paraphrases , different works have different choices among MSCOCO , Twitter URL and ParaNMT . MSCOCO is more preferred for less noise compared with Twitter URL and ParaNMT . Therefore , a combination of MSCOCO and Quora is more reasonable . For evaluation metrics , BLEU is the most frequently used one . However , as proposed by ( Niu et al , 2020 ) ( Niu et al , 2020 ) . because of \" curse of BLEU on paraphrase evaluation \" . As shown in Table 5 , examples with low BLEU scores might include both relatively good and bad paraphrasing because BLEU scores only measure the overlap between outputs and references . However , a generated paraphrase might still be a good paraphrase even it is not same with the reference . Therefore , for evaluation , it is better to combine automatic evaluation metrics and human evaluation together for a more comprehensive evaluation .", "entities": [[108, 109, "DatasetName", "Kumar"], [168, 169, "MetricName", "loss"], [242, 243, "MethodName", "GAN"], [425, 426, "MethodName", "seq2seq"], [572, 573, "DatasetName", "Kumar"], [611, 612, "MethodName", "VAE"], [833, 834, "MetricName", "BLEU"], [855, 856, "DatasetName", "MSCOCO"], [933, 934, "DatasetName", "MSCOCO"], [947, 948, "MethodName", "VAE"], [961, 962, "DatasetName", "MSCOCO"], [972, 973, "MethodName", "Seq2Seq"], [992, 993, "MethodName", "Transformer"], [1000, 1001, "MethodName", "Seq2Seq"], [1023, 1024, "MethodName", "Transformer"], [1074, 1076, "TaskName", "paraphrase generation"], [1086, 1089, "TaskName", "pretrained language models"], [1108, 1111, "TaskName", "pretrained language models"], [1125, 1127, "TaskName", "paraphrase generation"], [1178, 1179, "MethodName", "Transformer"], [1191, 1193, "TaskName", "paraphrase generation"], [1299, 1300, "DatasetName", "MSCOCO"], [1306, 1307, "DatasetName", "MSCOCO"], [1325, 1326, "DatasetName", "MSCOCO"], [1336, 1337, "MetricName", "BLEU"], [1369, 1370, "MetricName", "BLEU"], [1384, 1385, "MetricName", "BLEU"], [1395, 1396, "MetricName", "BLEU"]]}
{"text": "Most recent works on multi - level paraphrase generation only focus on word - level paraphrasing and phrase - level paraphrasing . However , more granularity levels can be incorporated . We believe it is worthwhile to study the combination of various levels , including word - level , phrase - level , syntaxlevel and sentence - level . Transfer learning With the goal of generating different surfaces of given sentences while preserving the meaning , text summarization , text simplification and paraphrase generation are essentially similar . Therefore , one could utilize transfer learning of these three tasks to improve the performance . Stylistic paraphrase generation Currently , word - and phrase - substitution in paraphrase gener - ation can not be carefully controlled . Therefore , it is hard to control the style of generated paraphrases . We believe it is worthwhile to explore methods of incorporating specific styles into generated paraphrases . For instance , by controlling the types of words and phrases , we can incorporate metaphor and idiomatic expressions into paraphrases ( Zhou et al , 2021b , a ) , which could also help to enhance creativity and diversity of generated paraphrases . Evaluation metrics As stated in Section 6 , BLEU scores and other automatic evaluation metrics based on similar principle are not good enough to evaluate paraphrase generation . Thus there is a need for better automatic evaluation methods . One possible method is to utilize paraphrase identification in the automatic evaluation metrics to explicitly provide an evaluation of if the generated sentence and input sentence are paraphrases .", "entities": [[7, 9, "TaskName", "paraphrase generation"], [59, 61, "TaskName", "Transfer learning"], [76, 78, "TaskName", "text summarization"], [79, 81, "TaskName", "text simplification"], [82, 84, "TaskName", "paraphrase generation"], [93, 95, "TaskName", "transfer learning"], [105, 107, "TaskName", "paraphrase generation"], [207, 208, "MetricName", "BLEU"], [224, 226, "TaskName", "paraphrase generation"], [244, 246, "TaskName", "paraphrase identification"]]}
{"text": "The vast majority of modern Hebrew texts are written in a letter - only version of the Hebrew script , one which omits the diacritics present in the full diacritized , or dotted variant . 1 Since most vowels are encoded via diacritics , the pronunciation of words in the text is left underspecified , and a considerable mass of tokens becomes ambiguous . This ambiguity forces readers and learners to infer the intended reading using syntactic and semantic context , as well as common sense ( Bentin and Frost , 1987 ; Abu - Rabia , 2001 ) . In NLP systems , recovering such signals is difficult , and indeed their performance on Hebrew tasks is adversely affected by the presence of undotted text ( Shacham and Wintner , 2007 ; Goldberg and Elhadad , 2010 ; Tsarfaty et al , 2019 ) . As an example , the sentence in Table 1 ( a ) will be resolved by a typical reader as ( b ) in most reasonable contexts , knowing that the word \" softly \" may characterize landings . In contrast , an automatic system processing Hebrew text may not be as sensitive to this kind of grammatical knowledge and instead interpret the undotted token as the more ' The plane landed congratulations ' Table 1 : An example of an undotted Hebrew text ( a ) ( written right to left ) which can be interpreted in at least two different ways ( b , c ) , dotted and pronounced differently , but only ( b ) makes grammatical sense . frequent word in ( c ) , harming downstream performance . One possible way to overcome this problem is by adding diacritics to undotted text , or dotting , implemented using data - driven algorithms trained on dotted text . Obtaining such data is not trivial , even given correct pronunciation : the standard Tiberian diacritic system contains several sets of identicallyvocalized forms , so while most Hebrew speakers easily read dotted text , they are unable to produce it . Moreover , the process of manually adding diacritics in either handwritten script or through digital input devices is mechanically cumbersome . Thus , the overwhelming majority of modern Hebrew text is undotted , and manually dotting it requires expertise . The resulting scarcity of available dotted text in modern Hebrew contrasts with Biblical and Rabbinical texts which , while dotted , manifest a very different language register . This state of affairs allows individuals and companies to offer dotting as paid services , either by experts or automatically , e.g. the Morfix engine by Melingo . 2 Such usage practices also force a disconnect in the NLP pipeline , requiring an API call into an external service whose parameters can not be updated . Existing computational approaches to dotting are manifested as complex , multi - resourced systems which perform morphological analysis on the undotted text and look undotted words up in handcrafted dictionaries as part of the dotting process . Dicta 's Nakdan ( Shmidman et al , 2020 ) , the current state - of - the - art , applies such methods in addition to applying multiple neural networks over different levels of the text , requiring manual annotation not only for dotting but also for morphology . Among the resources it uses are a diacritized corpus of 3 M tokens and a POS - tagged corpus of 300 K tokens . Training the model takes several weeks . 3 In this work , we set out to simplify the dotting task as much as possible to standard modules . We introduce a large corpus of semi - automatically dotted Hebrew , collected from various sources , and use it to train an RNN - based model . Our system , NAKDIMON , accepts the undotted character sequence as its input , consults no external resources or lexical components , and produces diacritics for each character , resulting in dotted text whose quality is comparable to that of the commercial Morfix , on both character - level and word - level accuracy . Our model is easy to integrate within larger systems that perform end - to - end Hebrew processing tasks , as opposed to the existing proprietary dotters . To our knowledge , this is the first attempt at a \" light \" model for Hebrew dotting since early HMM - based systems ( Kontorovich , 2001 ; Gal , 2002 ) . We introduce a novel test set for Modern Hebrew dotting , derived from larger and more diverse sources than existing datasets . In experiments over our dataset , we show that our system is particularly useful in the main use case of modern dotting , which is to convey the desired pronunciation to a reader , and that the errors it makes should be more easily detectable by non - professionals than Dicta 's . 4 2 Task and Datasets", "entities": [[490, 492, "TaskName", "morphological analysis"], [694, 695, "MetricName", "accuracy"]]}
{"text": "Dotted modern Hebrew text is scarce , since speakers usually read and write undotted text , with the occasional diacritic added for disambiguation when context does not suffice . As we are unaware of legally - obtainable dotted modern corpora , we use a combination of dotted pre - modern texts as well as automatically and semi - automatically dotted modern sources to train NAKDIMON : The PRE - MODERN portion is obtained from two main sources : A combination of late pre - modern text from Project Ben - Yehuda , mostly texts from the late 19th century and the early 20th century ; 5 rabbinical texts from the medieval period , the most important of which is Mishneh Torah ( obtained from Project Mamre ) ; 6 and 23 short stories from the short story project . 7 This portion contains roughly 1.81 M Hebrew tokens , most of which are dotted , with a varying level of accuracy , varying dotting styles , and varying degree of similarity to Modern Hebrew . The AUTOMATIC portion contains 547 short stories taken from the short story project . The stories are dotted using Dicta without manual validation . The corpus contains roughly 1.27 M Hebrew tokens . Lastly , the MODERN portion contains manually collected text in Modern Hebrew , mostly from undotted sources , which we dot using Dicta and follow up by manually fixing errors , either using Dicta 's API or via automated scripts which catch common mistakes . We made an effort to collect a diverse set of sources : news , opinion columns , paragraphs from books , short stories , Wikipedia articles , governmental publications , blog posts and forums expressing various domains and voices , and more . Our MODERN corpus contains roughly 326 K Hebrew tokens , and is much more consistent and similar to the expectation of a native Hebrew speaker than the PRE - MODERN or the AUTOMATIC corpora , and more accurately dotted than the AU - TOMATIC corpus . The sources and statistics of this dataset are presented in Table 2 .", "entities": [[69, 70, "MethodName", "MODERN"], [160, 161, "MetricName", "accuracy"], [211, 212, "MethodName", "MODERN"], [297, 298, "MethodName", "MODERN"], [325, 326, "MethodName", "MODERN"]]}
{"text": "Shmidman et al ( 2020 ) provide a benchmark dataset for dotting modern Hebrew documents . However , it is relatively small and non - diverse : all 22 documents in the dataset originate in a single source , namely Hebrew Wikipedia articles . Therefore , we created a new test set 8 from a larger variety of texts , including high - quality Wikipedia articles and edited news stories , as well as user - generated blog posts . This set consists of ten documents from each of eleven sources ( 5x Dicta 's test set ) , and totals 20 , 474 Hebrew tokens , roughly 3.5x Dicta 's . We use the same technique and style for dotting this corpus as we do for the MODERN corpus ( 2.2 ) , but the documents were 3 Nakdimon NAKDIMON embeds the input characters and passes them through a two - layer Bi - LSTM ( Hochreiter and Schmidhuber , 1997 ) . The LSTM output is fed into a single linear layer , which then feeds three linear layers , one for each diacritic category ( see 2 ) . Each character then receives a prediction for each category independently and all predicted marks are added to it as output . Decoding is performed greedily , with no validation of readability or any other dependence between character - level decisions . The input is pre - processed by removing all but Hebrew characters , spaces and punctuation ; digits are converted to a dedicated symbol , as are Latin characters . All existing diacritic marks are stripped , and each document is split into chunks bounded at whitespace , ignoring sentence boundaries . We train NAKDIMON first over PRE - MODERN , then over the AUTOMATIC corpus , and then by over the MODERN corpus . During training , the loss is the sum of the cross - entropy loss from all three categories . Trivial decisions , such as the label for the shin / sin diacritic for any non - \u202b\u05e9\u202c letter , are masked . Tuning experiments are detailed in Appendix B ; an evaluation of a preliminary version of NAKDI - MON over the Dicta test set is in Appendix C , and Hyperparameters are detailed in Appendix D.", "entities": [[128, 129, "MethodName", "MODERN"], [155, 156, "MethodName", "LSTM"], [165, 166, "MethodName", "LSTM"], [172, 174, "MethodName", "linear layer"], [292, 293, "MethodName", "MODERN"], [305, 306, "MethodName", "MODERN"], [312, 313, "MetricName", "loss"], [321, 322, "MetricName", "loss"]]}
{"text": "We compare the performance of NAKDIMON on our new test set ( 2.3 ) against Dicta , 9 Snopi , 10 and Morfix ( Kamir et al , 2002 ) . as well as a MA - JORITY baseline which returns the most common dotting for each word seen in our full training set . Metrics We report four metrics : decision accuracy ( DEC ) is computed over the entire set of individual possible decisions : dagesh / mappiq for letters that allow it , sin / shin dot for the letter \u202b , \u05e9\u202c and all other diacritics for letters that allow them ; character accuracy ( CHA ) is the portion of characters in the text that end up in their intended final form ( which may combine two or three decisions , e.g. dagesh + vowel ) ; word accuracy ( WOR ) is the portion of words with no mistakes ; and vocalization", "entities": [[62, 63, "MetricName", "accuracy"], [107, 108, "MetricName", "accuracy"], [143, 144, "MetricName", "accuracy"]]}
{"text": "We provide document - level macro - averaged accuracy percentage results for a single run over our test set in Table 3 . All systems , except Snopi , substantially outperform the majority - dotting baseline on all metrics . NAKDIMON outperforms Morfix on character - level metrics but not on word - level metrics , mostly since Morfix ignores certain words altogether , incurring errors on multiple characters . We note the substantial improvement our model achieves on the VOC metric compared to the WOR metric : 18.43 % of word - level errors are attributable to vocalization - agnostic dotting , compared to 13.80 % for Dicta and 10.41 % for Snopi ( but 20.91 % for Morfix ) . Considering that the central use case for dotting modern Hebrew text is to facilitate pronunciation to learners and for reading , and that undotted homograph ambiguity typically comes with pronunciation differences , we believe this measure to be no less important than WOR . Results on Dicta 's test set ( Shmidman et al , 2020 ) are presented in Appendix C.", "entities": [[8, 9, "MetricName", "accuracy"]]}
{"text": "Existing work on diacritizing Hebrew is not common , and all efforts build on word - level features . Kontorovich ( 2001 ) trains an HMM on a vocalized and morphologically - tagged portion of the Hebrew Bible containing 30 , 743 words , and evaluates the result on a test set containing 2 , 852 words , achieving 81 % WOR accuracy . Note that Biblical Hebrew is very different from Modern Hebrew in both vocabulary , grammatical structure , and diacritization , and also has many words with unique diacritization . In our system , we exclude the Bible altogether from the training set , as its inclusion actively hurts performance on the validation set , which consists of Modern Hebrew . Tomer ( 2012 ) designs a diacritization system for Hebrew verbs consisting of a combination of a verb inflection system , a syllable boundary detector , and an SVM model for classifying verb inflection paradigms . The focus on verbs in a type - level setup makes this work incomparable to ours or to others in this survey . In Arabic , diacritization serves a comparable purpose to that in Hebrew , but not exclusively : most diacritic marks differentiate consonantal phonemes from each other , e.g. /b/ vs. /t/ ( which only the sin / shin dot does in Hebrew ) , whereas vocalization marks are in a one - to - one relationship with their phonetic realizations , e.g. only the fatha as in /ba/ encodes the /a/ vowel . Dictionary - less Arabic diacritization has been attempted using a 3 - layer Bi - LSTM ( Belinkov and Glass , 2015 ) . Abandah et al ( 2015 ) use a Bi - LSTM where characters are assigned either one or more diacritic symbols . Our system differs from theirs by virtue of separating the diacritization categories . Mubarak et al ( 2019 ) tackled Arabic diacritization as a sequence - to - sequence problem , tasking the model with reproducing the characters as well as the marks . Zalmout and Habash ( 2017 ) have made the case against RNN - only systems , arguing for the importance of morphological analyzers in Arabic NLP systems . We concede that well - curated systems may perform better than uncurated ones , particularly on low - resource languages such as Hebrew , but we note that they are difficult to train for individual use cases and are burdensome to incorporate within larger systems . Diacritics restoration in Latin - based scripts , applicable mostly to European languages , forms a substantially different problem from the one in Hebrew given the highly lexicalized nature of diacritic usage in these languages and the very low rate of characters requiring diacritics . The state - of - theart systems in such languages employ transformer models in a sequence - to - sequence setup ( N\u00e1plava et al , 2021 ; Stankevi\u010dius et al , 2022 ) , supplanting character - RNN sequence prediction architectures reminiscent of ours ( N\u00e1plava et al , 2018 ) . Indeed , the authors of this latter work note the only non - European in their dataset , Vietnamese , as a special outlier .", "entities": [[62, 63, "MetricName", "accuracy"], [152, 153, "MethodName", "SVM"], [271, 272, "MethodName", "LSTM"], [290, 291, "MethodName", "LSTM"]]}
{"text": "We tried to further improve NAKDIMON by initializing its parameters from a language model trained to predict masked characters in a large undotted Wikipedia corpus ( 440 MB , 30 % mask rate ) , but were only able to achieve an improvement of 0.07 % . Attempted architectural modifications , including substituting a Transformer ( Vaswani et al , 2017 ) for the LSTM ; adding a CRF layer to the decoding process ; and adding a residual connection between the character LSTM layers , yielded no substantial benefits in these experiments . Similarly , varying the number of LSTM layers between 2 and 5 ( keeping the total number of parameters roughly constant , close to the 5 , 313 , 223 parameters of our final model ) had little to no impact on the accuracy on the validation set .", "entities": [[54, 55, "MethodName", "Transformer"], [64, 65, "MethodName", "LSTM"], [68, 69, "MethodName", "CRF"], [78, 80, "MethodName", "residual connection"], [83, 84, "MethodName", "LSTM"], [100, 101, "MethodName", "LSTM"], [110, 113, "HyperparameterName", "number of parameters"], [137, 138, "MetricName", "accuracy"]]}
{"text": "Speech translation ( ST ) involves translating the acoustic speech signals in the source language into the words in the target language . We use it to evaluate the semantic capability of SSL models , and how they benefit the translation task . We use the CoVoST2 En\u00d1De ( Wang et al , 2020 ) dataset ( CC0 Licensed ) with their official train , validation , and test splits while removing all the samples containing \" REMOVE \" , resulting in 425.8 , 25.9 and 24.5 hours respectively . For text , we keep original case , normalize punctuation , and build character vocabulary with 100 % train - set coverage . We report case - sensitive de - tokenized BLEU using sacreBLEU ( Post , 2018 ) . Our downstream model has an encoder - decoder architecture with 3 layers of Transformers ( Vaswani et al , 2017 ) each with hidden dimension of 512 . A convolutional subsampler is used to reduce the sequence length of the input before feeding it to the encoder . We train our model with label - smoothing using a probability of 0.1 . A beam size of 20 is used for inference .", "entities": [[32, 33, "DatasetName", "SSL"], [121, 122, "MetricName", "BLEU"], [123, 124, "MetricName", "sacreBLEU"]]}
{"text": "Although an ASR is included in SUPERB , it only examines SSL models on read English corpus Lib - riSpeech ( Panayotov et al , 2015 ) . Therefore , we introduce out - of - domain ASR ( OOD - ASR ) , which aims to evaluate the models ' capabilities across languages , and out - of - domain scenarios . The OOD - ASR tasks are categorized into cross - lingual and spontaneous speech tasks . For the cross - lingual tasks , we choose the Mexican Spanish ( es ) , Mandarin ( zh ) , and Arabic ( ar ) subsets from Common Voice 7.0 ( Ardila et al , 2020 ) ( CC0 Licensed ) containing 21.5 , 31.2 , and 30.7 hours of training data respectively . The validation set sizes are 1.2 hours , 14.4 hours and 12.24 hours , and the test set sizes are 0.6 hour , 15.3 hours and 12.5 hours for es , zh and ar respectively . For the spontaneous speech task ( spon ) , we use the Santa Barbara Corpus of Spoken American English ( SBCSAE ) ( Du Bois et al , 2000 - 2005 ( CC BY - ND 3.0 Licensed ) , consisting of 60 conversations over different topics spanning 16.7 hours of data . The validation and test set sizes are 1.6 hours and 2.2 hours respectively . For evaluation , we use word error rate ( WER ) as the metric except for Mandarin which character error rate ( CER ) is used . The error rates are averaged across all sub - tasks to offer an overall score . The ASR model is a 2 - layer BLSTM ( Hochreiter and Schmidhuber , 1997 ) with hidden states of 1024 dimension . The training objective is to minimize the Connectionist Temporal Classification ( CTC ) loss ( Graves et al , 2006 ) . During inference , we use CTC greedy decoding without language model re - scoring to simplify the process and to highlight the impact of the learned acoustic representations .", "entities": [[11, 12, "DatasetName", "SSL"], [107, 110, "DatasetName", "Common Voice 7.0"], [242, 248, "MetricName", "word error rate ( WER )"], [312, 313, "TaskName", "Classification"], [314, 315, "DatasetName", "CTC"], [316, 317, "MetricName", "loss"], [330, 331, "DatasetName", "CTC"]]}
{"text": "For voice conversion ( VC ) , we consider the intralingual VC task in VCC2020 ( Zhao et al , 2020 ) For the pretraining methods , we abbreviate \" vector quantization \" as VQ , \" future \" as F , \" masked \" as M , \" generation \" as G , \" contrastive discrimination \" as C , and \" token prediction / classification \" as P. Parameters for both pretraining and inference are counted . ( ODbL Licensed ) under the any - to - one ( A2O ) setting . A2O VC aims to convert speech from any arbitrary speaker into that of a predefined target speaker . We use the task to evaluate the speaker transferability as well as the generalizability of the SSL models . We use 60 utterances from the target speaker that spans 5 minutes for training , and 25 utterances for testing that span 2 minutes . No validation set was used . We use the commonly used mel - cepstrum distortion ( MCD ) , word error rate ( WER ) and automatic speaker verification ( ASV ) accept rate from off - the - shelf ASR and ASV models as evaluation metrics . The downstream model is trained to reconstruct the acoustic feature from the upstream representations in a target - speaker - dependent manner . In the conversion phase , given the representations extracted by the upstream , the model generates the converted acoustic features , which are then sent to a neural vocoder to synthesize the converted waveform . We adopted Tacotron2 ( Shen et al , 2018 ) as the downstream model , which is an autoregressive network consisting of convolutional and LSTM layers . For the neural vocoder , we used the Hifi - GAN ( Kong et al , 2020 ) . We follow an implementation described in ( Huang et al , 2021b ) .", "entities": [[1, 3, "TaskName", "voice conversion"], [31, 32, "TaskName", "quantization"], [129, 130, "DatasetName", "SSL"], [176, 182, "MetricName", "word error rate ( WER )"], [184, 186, "TaskName", "speaker verification"], [265, 266, "MethodName", "Tacotron2"], [287, 288, "MethodName", "LSTM"], [298, 301, "MethodName", "Hifi - GAN"]]}
{"text": "For each task , 2 additional downstream architectures are created by modifying the number of layers and the hidden dimensions compared to our default setting . We create small and large models that are roughly the half and twice of default in terms of the number of trainable parameters . A detailed comparison of the downstream architectures is shown in Table 5 . The results are shown in Table 6 . We show that the ranking of the upstream models is almost fixed when the model sizes are varied . As expected , the small architecture has worse perfor - mance than default , while large has better . Moreover , the scores causing the change in ranking are negligible , e.g. , TERA / CPC in SS and wav2vec 2.0 Base / HuBERT Base in OOD - ASR with large . The results show that the relative performance achieved by different upstream models is agnostic to the downstream architecture , confirming the robustness of the framework used in SUPERB - SG .", "entities": [[13, 16, "HyperparameterName", "number of layers"]]}
{"text": "Yes , we cited those artifacts properly in Section 3 . B.4.2 Did you discuss the license or terms for use and/or distribution of any artifacts ? Yes , the licenses of the artifacts are clearly indicated in Section 3 . B.4.3 Did you discuss if your use of existing artifact ( s ) was consistent with their intended use , provided that it was specified ? For the artifacts you create , do you specify intended use and whether that is compatible with the original access conditions ( in particular , derivatives of data accessed for research purposes should not be used outside of research contexts ) ? Yes , we use the official implementations of the upstream models in Table 1 and followed their public API to access the models . For the datasets , we also follow their licenses . B.4.4 Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content , and the steps taken to protect / anonymize it ? No , there were no data collection involved in this work . We used the widely - used public datasets and followed the common data preprocessing steps . B.4.5 Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and linguistic phenomena , demographic groups represented , etc . ? Yes , the properties of the artifacts were indicated in Section 3 . Yes . B.5.1 Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ? We reported the number of the parameters in Section 5.3.1 . The computational budget and computing infrastructures are reported in Table 12 . B.5.2 Did you discuss the experimental setup , including hyperparameter search and best - found hyperparameter values ? No , we did n't do the hyperparameter searching in a unified way . Some hyperparameters came from the official implementation or related works and some were searched by ourselves . However , the hyperparameters we used are public available 1 . B.5.3 Did you report descriptive statistics about your results ( e.g. , error bars around results , summary statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean , etc . or just a single run ? Yes , we indicated that in Section 4 . B.5.4 If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE , etc . ) ? Yes , we reported them in Section 3 . B.6 Did you use human annotators ( e.g. , crowdworkers ) or research with human subjects ? No .", "entities": [[260, 263, "HyperparameterName", "number of parameters"]]}
{"text": "The input consists of dialogue context of t\u22121 turns , each including a pair of user utterance U and system response R , ( U 1 , R 1 ) , ... , ( U t\u22121 , R t\u22121 ) , and the user utterance at current turn U t . A taskoriented dialogue system aims to generate the next response R t . The information for responses is typically queried from a database based on the user 's provided information i.e. inform slots tracked by a DST . We assume access to a database of all domains with each column corresponding to a specific slot being tracked . We denote the intermediate output , including the dialogue state of current turn B t and dialogue act as A t . We denote the list of all domains D = ( d 1 , d 2 , ... ) , all slots S = ( s 1 , s 2 , ... ) , and all acts A = ( a 1 , a 2 , ... ) . We also denote the list of all ( domain , slot ) pairs as DS = ( ds 1 , ds 2 , ... ) . Note that DS \u2264 D \u00d7 S as some slots might not be applicable in all domains . Given the current dialogue turn t , we represent each text input as a sequence of tokens , each of which is a unique token index from a vocabulary set V : dialogue context X ctx , current user utterance X utt , and target system response X res . Similarly , we also represent the list of domains as X D and the list of slots as X S . In DST , we consider the raw text form of dialogue state of the previous turn B t\u22121 , similarly as ( Lei et al , 2018 ; Budzianowski and Vuli\u0107 , 2019 ) . In the context - to - text setting , we assume access to the ground - truth dialogue states of current turn B t . The dialogue state of the previous and current turn can then be represented as a sequence of tokens X prev st and X curr st respectively . For a fair comparison with current approaches , during inference , we use the model predicted dialogue statesX prev st and do not use X curr st in DST and end - to - end tasks . Following ( Wen et al , 2015 ; , we consider the delexicalized target response X dl res by replacing tokens of slot values by their corresponding generic tokens to allow learning valueindependent parameters . Our model consists of 3 major components ( See Figure 1 ) . First , Encoders encode all text input into continuous representations . To make it consistent , we encode all input with the same embedding dimension . Secondly , our Bi - level State Tracker ( BDST ) is used to detect contextual dependencies to generate dialogue states . The DST includes 2 modules for slot - level and domain - level representation learning . Each module comprises attention layers to project domain or slot representations and incorporate important information from dialogue context , dialogue state of the previous turn , and current user utterance . The outputs are combined as a context - aware vector to decode the corresponding inform or request slots in each domain . Lastly , our Joint Dialogue Act and Response Generator ( DARG ) projects the target system response representations and enhances them with information from various dialogue components . Our response generator can also learn a latent representation to generate dialogue acts , which condition all target tokens during each generation step .", "entities": [[489, 491, "HyperparameterName", "embedding dimension"], [527, 529, "TaskName", "representation learning"], [533, 535, "HyperparameterName", "attention layers"]]}
{"text": "An encoder encodes a text sequence X to a sequence of continuous representation Z R L X \u00d7d . L X is the length of sequence X and d is the embedding dimension . Each encoder includes a token - level embedding layer . The embedding layer is a trainable embedding matrix E R V \u00d7d . Each row represents a token in the vocabulary set V as a d - dimensional vector . We denote E ( X ) as the embedding function that transform the sequence X by looking up the respective token index : Z emb = E ( X ) R L X \u00d7d . We inject the positional attribute of each token as similarly adopted in ( Vaswani et al , 2017 ) . The positional encoding is denoted as P E. The final embedding is the element - wise summation between token - embedded representations and positional encoded representations with layer normalization ( Ba et al , 2016 ) : Z = LayerNorm ( Z emb + P E ( X ) ) R L X \u00d7d . The encoder outputs include representations of dialogue context Z ctx , current user utterance Z utt , and target response Z dl res . We also encode the dialogue states of the previous turn and current turn and obtain Z prev st and Z curr st respectively . We encode X S and X D using only token - level embedding layer : Z S = LayerNorm ( E ( X S ) ) and Z D = LayerNorm ( E ( X D ) ) . During training , we shift the target response by one position to the left side to allow auto - regressive prediction in each generation step . We share the embedding matrix E to encode all text tokens except for tokens of target responses as the delexicalized outputs contain different semantics from natural language inputs .", "entities": [[31, 33, "HyperparameterName", "embedding dimension"], [156, 158, "MethodName", "layer normalization"]]}
{"text": "The representations Z dst are used as context - aware representations to decode individual dialogue states . Given a domain index i and slot index j , the feature vector Z dst [ i , j , : ] R d is used to generate value of the corresponding ( domain , slot ) pair . The vector is used as an initial hidden state for an RNN decoder to decode an inform slot value . Given the k - th ( domain , slot ) pair and decoding step l , the output hidden state in each recurrent step h kl is passed through a linear transformation with softmax to obtain output distribution over vocabulary set V : P inf kl = Softmax ( h kl W inf ) R V where W inf dst R drnn\u00d7 V . For request slot of k - th ( domain , slot ) pair , we pass the corresponding vector Z dst vector through a linear layer with sigmoid activation to predict a value of 0 or 1 . P req k = Sigmoid ( Z dst k W req ) . Optimization . The DST is optimized by the crossentropy loss functions of inform and request slots : L dst = L inf + L req = DS k=1 Y k l=1 \u2212 log ( P inf kl ( y kl ) ) + DS k=1 \u2212y k log ( P req k ) \u2212 ( 1 \u2212 y k ) ( 1 \u2212 log ( P req k ) ) 3.3 Joint Dialogue Act and Response Generator ( DARG ) Database Representations . Following , we create a one - hot vector for each domain d : x d db { 0 , 1 } 6 and 6 i x d db , i = 1 . Each position of the vector indicates a number or a range of entities . The vectors of all domains are concatenated to create a multi - domain vector X db R 6\u00d7 D . We embed this vector as described in Section 3.1 . Response Generation . We adopt a stackedattention architecture that sequentially learns dependencies between each token in target responses with each dialogue component representation . First , we obtain Z gen res = Att ( Z res , Z res ) R Lres\u00d7d . This attention layer can learn semantics within the target response to construct a more semantically structured sequence . We then use attention to capture dependencies in background information contained in dialogue context and user utterance . The outputs are Z gen ctx = Att ( Z ctx , Z gen res ) R Lres\u00d7d and Z gen utt = Att ( Z utt , Z gen ctx ) R Lres\u00d7d sequentially . To incorporate information of dialogue states and DB results , we apply attention steps to capture dependencies between each response token representation and state or DB representation . Specifically , we first obtain Z gen dst = Att ( Z dst , Z gen utt ) R Lres\u00d7d . In the context - to - text setting , as we directly use the ground - truth dialogue states , we simply replace Z dst with Z curr st . Then we obtain Z gen db = Att ( Z db , Z gen dst ) R Lres\u00d7d . These attention layers capture the information needed to generate tokens that are towards task completion and supplement the contextual cues obtained in previous attention layers . We let the models to progressively capture these dependencies for N gen times and denote the final output as Z gen . The final output is passed to a linear layer with softmax activation to decode system responses auto - regressively : P res = Softmax ( Z gen W gen ) R Lres\u00d7 Vres Dialogue Act Modeling . We couple response generation with dialogue act modeling by learning a latent variable Z act R d . We place the vector in the first position of Z res , resulting in Z res+act R ( Lres+1 ) \u00d7d . We then pass this tensor to the same stacked attention layers as above . By adding the latent variable in the first position , we allow our model to semantically condition all downstream tokens from second position , i.e. all tokens in the target response , on this latent variable . The output representation of the latent vector i.e. by domain first row in Z gen , incorporates contextual signals accumulated from all attention layers and is used to predict dialogue acts . We denote this representation as Z gen act and pass it through a linear layer to obtain a multi - hot encoded tensor . We apply Sigmoid on this tensor to classify each dialogue act as 0 or 1 : P act = Sigmoid ( Z gen act W act ) R A . Optimization . The response generator is jointly trained by the cross - entropy loss functions of generated responses and dialogue acts : L gen = L res + L act = Yres l=1 \u2212 log ( P res l ( y l ) ) + A a=1 \u2212y a log ( P act a ) \u2212 ( 1 \u2212 y a ) ( 1 \u2212 log ( P act a ) ) 4 Experiments", "entities": [[109, 110, "MethodName", "softmax"], [123, 124, "MethodName", "Softmax"], [164, 166, "MethodName", "linear layer"], [167, 169, "MethodName", "sigmoid activation"], [174, 175, "DatasetName", "0"], [180, 182, "HyperparameterName", "k ="], [200, 201, "MetricName", "loss"], [292, 293, "DatasetName", "0"], [352, 354, "TaskName", "Response Generation"], [566, 568, "HyperparameterName", "attention layers"], [588, 590, "HyperparameterName", "attention layers"], [620, 622, "MethodName", "linear layer"], [623, 624, "MethodName", "softmax"], [636, 637, "MethodName", "Softmax"], [652, 654, "TaskName", "response generation"], [699, 701, "HyperparameterName", "attention layers"], [763, 765, "HyperparameterName", "attention layers"], [786, 788, "MethodName", "linear layer"], [809, 810, "DatasetName", "0"], [840, 841, "MetricName", "loss"]]}
{"text": "Joint Acc . HJST ( Eric et al , 2019 ) 35.55 % DST Reader 36.40 % TSCP ( Lei et al , 2018 ) 37.12 % FJST ( Eric et al , 2019 ) 38.00 % HyST 38.10 % TRADE ( Wu et al , 2019a ) 45.60 % NADST ( Le et al , 2020 ) 49.04 % DSTQA ( Zhou and Small , 2019 ) 51.17 % SOM - DST ( Kim et al , 2020 ) 53.01 % BDST ( Ours ) 49.55 % 71.29 % 60.96 % 18.80 TokenMoE ( Pei et al , 2019 ) 75.30 % 59.70 % 16.81 HDSA 82.90 % 68.90 % 23.60 Structured Fusion ( Mehri et al , 2019 ) 82.70 % 72.10 % 16.34 LaRL ( Zhao et al , 2019 ) 82.78 % 79.20 % 12.80 GPT2 ( Budzianowski and Vuli\u0107 , 2019 ) We adopt a teacher - forcing training strategy by simply using the ground - truth inputs of dialogue state of the previous turn and the gold DB representations . During inference in DST and end - to - end tasks , we decode system responses sequentially turn by turn , using the previously decoded state as input in the current turn , and at each turn , using the new predicted state to query DBs . We train all networks with Adam optimizer ( Kingma and Ba , 2015 ) and a decaying learning rate schedule . All models are trained up to 30 epochs and the best models are selected based on validation loss . We used a greedy approach to decode all slots and a beam search with beam size 5 . To evaluate the models , we use the following metrics : Joint Accuracy and Slot Accuracy ( Henderson et al , 2014b ) , Inform and Success ( Wen et al , 2017 ) , and BLEU score ( Papineni et al , 2002 ) . As suggested by Liu et al ( 2016 ) , human evaluation , even though popular in dialogue research , might not be necessary in tasks with domain constraints such as MultiWOZ . We implemented all models using Pytorch and will release our code on github 1 .", "entities": [[1, 2, "MetricName", "Acc"], [70, 71, "MethodName", "SOM"], [228, 229, "MethodName", "Adam"], [229, 230, "HyperparameterName", "optimizer"], [240, 242, "HyperparameterName", "learning rate"], [261, 262, "MetricName", "loss"], [293, 294, "MetricName", "Accuracy"], [296, 297, "MetricName", "Accuracy"], [317, 319, "MetricName", "BLEU score"], [358, 359, "DatasetName", "MultiWOZ"]]}
{"text": "DST . We test our state tracker ( i.e. using only L dst ) and compare the performance with the baseline models in ( Peng et al , 2019 ) , our single model generates better responses in all domains without relying on multiple domainspecific teacher models . We also noted that the performance of DST improves in contrast to the previous DST task . This can be explained as additional supervision from system responses not only contributes to learn a natural response but also positively impact the DST component . Other baseline models such as Wu et al , 2019b ) present challenges in the MultiWOZ benchmark as the models could not fully optimize due to the large scale entity memory . For example , following GLMP ( Wu et al , 2019b ) , the restaurant domain alone has over 1 , 000 memory tuples of ( Subject , Relation , Object ) . Ablation . We conduct a comprehensive ablation analysis with several model variants in Table 6 and have the following observations : The model variant with a single - level DST ( by considering S = DS and N dst D = 0 ) ( Row A2 ) performs worse than the Bi - level DST ( Row A1 ) . In addition , using the dual architecture also improves the latency in each attention layers as typically D + S DS . The performance gap also indicates the potential of separating global and local dialogue state dependencies by domain and slot level . Using B t\u22121 and only the last user utterance as the dialogue context ( Row A1 and B1 ) performs as well as using B t\u22121 and a full - length dialogue history ( Row A5 and B3 ) . This demonstrates that the information from the last dialogue state is sufficient to represent the dialogue history up to the last user utterance . One benefit from not using the full dialogue history is that it reduces the memory cost as the number of tokens in a full - length dialogue history is much larger than that of a dialogue state ( particularly as the conversation evolves over many turns ) . We note that removing the loss function to learn the dialogue act latent variable ( Row B2 ) can hurt the generation performance , especially by the task completion metrics Inform and Success . This is interesting as we expect dialogue acts affect the general semantics of output sentences , indicated by BLEU score , rather than the model ability to retrieve correct entities . This reveals the benefit of our approach . By enforcing a semantic condition on each token of the target response , the model can facility the dialogue flow towards successful task completion . In both state tracker and response generator modules , we note that learning feature representations through deeper attention networks can improve the quality of predicted states and system responses . This is consistent with our DST performance as compared to baseline models of shallow networks . Lastly , in the end - to - end task , our model achieves better performance as the number of attention heads increases , by learning more high - resolution dependencies .", "entities": [[106, 107, "DatasetName", "MultiWOZ"], [197, 198, "DatasetName", "0"], [229, 231, "HyperparameterName", "attention layers"], [376, 377, "MetricName", "loss"], [423, 425, "MetricName", "BLEU score"]]}
{"text": "Baseline . provides a baseline for this setting by following the sequenceto - sequence model . The source sequence is all past dialogue turns and the target sequence is the system response . The initial hidden state of the RNN decoder is incorporated with additional signals from the dialogue states and database representations . TokenMoE ( Pei et al , 2019 ) . TokenMoE refers to Token - level Mixture - of - Expert model . The model follows a modularized approach by separating different components known as expert bots for different dialogue scenarios . A dialogue scenario can be dependent on a domain , a type of dialogue act , etc . A chair bot is responsible for controlling expert bots to dynamically generate dialogue responses . HDSA . This is the current stateof - the - art in terms of Inform and BLEU score in the context - to - text generation setting in MultiWOZ2.0 . HDSA leverages the structure of dialogue acts to build a multi - layer hierarchical graph . The graph is incorporated as an inductive bias in a self - attention network to improve the semantic quality of generated dialogue responses . Structured Fusion ( Mehri et al , 2019 ) . This approach follows a traditional modularized dialogue system architecture , including separate components for NLU , DM , and NLG . These compo - nents are pre - trained and combined into an end - toend system . Each component output is used as a structured input to other components . LaRL ( Zhao et al , 2019 ) . This model uses a latent dialogue action framework instead of handcrafted dialogue acts . The latent variables are learned using unsupervised learning with stochastic variational inference . The model is trained in a reinforcement learning framework whereby the parameters are trained to yield a better Success rate . The model is the current state - of - the - art in terms of Success metric . GPT2 ( Budzianowski and Vuli\u0107 , 2019 ) . Unsupervised pre - training language models have significantly improved machine learning performance in many NLP tasks . This baseline model leverages the power of a pre - trained model ( Radford et al , 2019 ) and adapts to the context - to - text generation setting in task - oriented dialogues . All input components , including dialogue state and database state , are transformed into raw text format and concatenated as a single sequence . The sequence is used as input to a pre - trained GPT - 2 model which is then fine - tuned with MultiWOZ data . DAMD . This is the current state - of - the - art model for context - to - text generation task in MultiWOZ 2.1 . This approach augments training data with multiple responses of similar context . Each dialogue state is mapped to multiple valid dialogue acts to create additional state - act pairs . B.3 End - to - End TSCP ( Lei et al , 2018 ) . In addition to the DST task , we evaluate TSCP as an end - to - end dialogue system that can do both DST and NLG . We adapt the models to the multi - domain DST setting as described in Section B.1 and keep the original response decoder . Similar to the DST component , the response generator of TSCP also adopts a pointer network to generate tokens of the target system responses by copying tokens from source sequences . In this setting , we test TSCP with two settings of the maximum length of the output dialogue state sequence : L = 8 and L = 20 . HRED - TS ( Peng et al , 2019 ) . This model adopts a teacher - student framework to address multidomain task - oriented dialogues . Multiple teacher networks are trained for different domains and intermediate representations of dialogue acts and output responses are used to guide a universal student network . The student network uses these representations to directly generate responses from dialogue context without predicting dialogue states .", "entities": [[144, 146, "MetricName", "BLEU score"], [152, 154, "TaskName", "text generation"], [184, 188, "MethodName", "self - attention network"], [292, 294, "MethodName", "variational inference"], [343, 347, "TaskName", "Unsupervised pre - training"], [387, 389, "TaskName", "text generation"], [431, 432, "MethodName", "GPT"], [442, 443, "DatasetName", "MultiWOZ"], [464, 466, "TaskName", "text generation"], [468, 470, "DatasetName", "MultiWOZ 2.1"], [580, 582, "MethodName", "pointer network"], [628, 629, "MethodName", "TS"]]}
{"text": "We examine an example of dialogue in the test data and compare our predicted outputs with the baseline TSCP ( L = 20 ) ( Lei et al , 2018 ) and the ground truth . From Figure 4 , we observe that both our predicted dialogue state and system response are more correct than the baseline . Specifically , our dialogue state can detect the correct type slot in the attraction domain . As our dialogue state is correctly predicted , the queried results from DB is also more correct , resulting in better response with the right information ( i.e. ' no attraction available ' ) . In Figure 5 , we show the visualization of domain - level and slot - level attention on the user utterance . We notice important tokens of the text sequences , i.e. ' entertainment ' and ' close to ' , are attended with higher attention scores . Besides , at domain - level attention , we find a potential additional signal from the token ' restaurant ' , which is also the domain from the previous dialogue turn . We also observe that attention is more refined throughout the neural network layers . For example , in the domain - level processing , compared to the 2 nd layer , the 4 th layer attention is more clustered around specific tokens of the user utterance . In Table 10 and 11 , we reported the complete output of this example dialogue . Overall , our dialogue agent can carry a proper dialogue with the user throughout the dialogue steps . Specifically , we observed that our model can detect new domains at dialogue steps where the domains are introduced e.g. attraction domain at the 5 th turn and taxi domain at the 8 th turn . The dialogue agent can also detect some of the co - references among the domains . For example , at the 5 th turn , the dialogue agent can infer the slot area for the new domain attraction as the user mentioned ' close the restaurant ' . We noticed that that at later dialogue steps such as the 6 th turn , our decoded dialogue state is not correct possibly due to the incorrect decoded dialogue state in the previous turn , i.e. 5 th turn . In Figure 2 and 3 , we plotted the Joint Goal Accuracy and BLEU metrics of our model by dialogue turn . As we expected , the Joint Accuracy metric tends to decrease as the dialogue history extends over time . The dialogue agent achieves the highest accuracy in state tracking at the 1 st turn and gradually reduces to zero accuracy at later dialogue steps , i.e. 15 th to 18 th turns . For response generation performance , the trend of BLEU score is less obvious . The dialogue agent obtains the highest BLEU scores at the 3 rd turn and fluctuates between the 2 nd and 13 th turn . 10 . Each turn includes the input of past system response S t\u22121 and current user utterance U t , and the predicted dialogue dialogue state BS t and system response S t . The dialogue consists of 11 turns in total and extends across 3 domains sequentially : restaurant , attraction , and taxi .", "entities": [[256, 257, "DatasetName", "agent"], [308, 309, "DatasetName", "agent"], [333, 334, "DatasetName", "agent"], [405, 406, "MetricName", "Accuracy"], [407, 408, "MetricName", "BLEU"], [422, 423, "MetricName", "Accuracy"], [437, 438, "DatasetName", "agent"], [441, 442, "MetricName", "accuracy"], [455, 456, "MetricName", "accuracy"], [470, 472, "TaskName", "response generation"], [477, 479, "MetricName", "BLEU score"], [485, 486, "DatasetName", "agent"], [489, 490, "MetricName", "BLEU"]]}
{"text": "Our model is conducted in a multi - task learning manner which consists of a shared encoder , a predictor , and a pair of counterfactual decoders . The predictor and the decoders take the output of the encoder as input . Our model looks like SHAPED ( Zhang et al , 2018 ) ( several decoders with a classifier ) , but the motivations and mechanisms behind the model are different . Claim - aware Encoder Intuitively , the plaintiff 's claim c and the fact description f are sequences of words . Therefore , the encoder firstly transforms the words to embeddings . Then the embedding sequences are fed to the Bi - LSTM , producing two sequences of hidden states h c , h f corresponding to the plaintiff 's claim and the fact description respectively . After that , we use a claim - aware attention mechanism to fuse h c and h f . For each hidden state h f i in h f , e i k is its attention weight on h c k , and the attention distribution q i is calculated as follow : e i k = v T tanh ( W c h c k + W f h f i + b attn ) ( 4 ) q i = sof tmax ( e i ) ( 5 ) where v , W c , W f , b attn are learnable parameters . The attention distribution can be regarded as the importance of each word in the plaintiff 's claim for a word in fact description . Next , the new representation of fact description is produced as follows : h f * i = h f i + k q i k h c k ( 6 ) After feeding to another Bi - LSTM layer , we get the claim - aware representation of fact h. Judgment Predictor Given the claim - aware representation of fact h , the judgment predictor produces the probability of support P sup through a fully connected layer and a sigmoid operation . The prediction result j is obtained as follow : j = 1 P sup > 0.5 0 P sup < = 0.5 ( 7 ) where 1 means support , and 0 means non - support . Counterfactual Decoder To eliminate the effect of data bias , here we use a pair of counterfactual decoders , which contains two decoders , one is for supported cases , and the other is for non - supported cases . The two decoders have the same structure but aim to generate the court 's view with different judgments . We name them as counterfactual decoders because every time there is only one of the two generated court 's views correct . Still , we apply the attention - mechanism . At each step t , given the encoder 's output h , and the decode state s t , the attention distribution a t is calculated the same way as q i in Eq . 5 , but with different parameters . The context vector h * t is then a weighted sum of h : h * t = i a t i h i ( 8 ) The context vector h * t , which can be regarded as a representation of the input for this step , is concatenated with the decode state s t and fed to linear layers to produce the vocabulary distribution p vocab : p vocab = sof tmax ( V ( V [ s t , h * t ] ) + b ) + b ) ( 9 ) where V , V , b , b are all learnable parameters . Then we add a generation probability ( See et al , 2017 ) to solve the OOV problem . Given the context h * t , the decode state s t and the decoder 's input ( the word embedding of the previous word ) x t , the generation probability p gen can be calculated : P gen = \u03c3 ( w T h * h * t + w T s s t + w T x x t + b ptr ) ( 10 ) where w h * , w s , w x and b ptr are learnable , and \u03c3 is the sigmoid function . The final probability for a word w in time step is obtained : Training For predictor , we use cross - entropy as the loss function : P ( w ) = P gen * p vocab ( w ) + ( 1 \u2212 P gen ) i : w i = w a t i ( L pred = \u2212\u0135log ( P sup ) \u2212 ( 1 \u2212\u0135 ) log ( 1 \u2212 P sup ) ( 12 ) where\u0135 is the real judgment . For decoders , the previous word in training is the word in real court 's view , and the loss for timestep t is the negative log - likelihood of the target word w * t : L t = \u2212logP ( w * t ) ( 13 ) and the overall generation loss is : L gen = 1 T T t=0 L t ( 14 ) where T is the length of real court 's view . Since we aim to make the two decoders generate two different court 's views , we take a mask operation when calculating the loss of each decoder . The exact loss for the support decoder is : L sup = L gen\u0135 = 1 0\u0135 = 0 ( 15 ) the loss for the non - support decoder L nsup is obtained by the opposite way . Thus , the total loss is : L total = L sup + L nsup + \u03bbL pred ( 16 ) where we set \u03bb to 0.1 in our model . Inference In inference , the counterfactual decoders apply beam search to generate two court 's views , and one of them will be selected as the final output , depending on the result of the predictor j.", "entities": [[6, 10, "TaskName", "multi - task learning"], [115, 116, "MethodName", "LSTM"], [195, 197, "HyperparameterName", "k ="], [222, 223, "DatasetName", "sof"], [308, 309, "MethodName", "LSTM"], [369, 370, "DatasetName", "0"], [384, 385, "DatasetName", "0"], [594, 595, "DatasetName", "sof"], [714, 715, "DatasetName", "ptr"], [731, 732, "DatasetName", "ptr"], [766, 767, "MetricName", "loss"], [847, 848, "MetricName", "loss"], [854, 857, "MetricName", "log - likelihood"], [881, 882, "MetricName", "loss"], [930, 931, "MetricName", "loss"], [937, 938, "MetricName", "loss"], [953, 954, "DatasetName", "0"], [958, 959, "MetricName", "loss"], [978, 979, "MetricName", "loss"]]}
{"text": "Since there is no publicly available court 's view generation dataset in civil cases , we build a dataset based on raw civil legal documents 3 . Specifically , we choose private lending , which is the most frequent category in civil cases , to construct the dataset . We process the legal documents as following steps : 1 ) Split legal documents into three parts : plaintiff 's claim , facts description , and court 's view , which can be objectively split by keywords ( subtitles ) . 2 ) Human annotation . We employ experts with legal backgrounds to annotate the judgment ( defined in Sec . 3 ) on the court 's view . 3 ) Annotation verification . We use random sampling test to ensure that the annotation accuracy is over 95 % . After that , we get the dataset as shown in Tab . 1 . We randomly separate the dataset into a training set , a validation set , and a test set according to a ratio of 8 : 1 : 1 , the ratio of supported cases is about 76 % in each set .", "entities": [[133, 134, "MetricName", "accuracy"]]}
{"text": "ROUGE 4 is a set of metrics used in the NLP task . We keep the results of ROUGE - 1 , ROUGE - 2 , and ROUGE - L. ROUGE - 1 and ROUGE - 2 refer to the overlap of unigram and bigram between the generated and reference documents , respectively . ROUGE - L is a Longest Common Subsequence ( LCS ) based statistics . BLEU 5 ( Papineni et al , 2002 ) is a method of au - tomatic text - generation evaluation that highly correlates with human evaluation . We use BLEU - 1 , BLEU - 2 to evaluate from the perspectives of unigram , bigram . BLEU - N is an average of BLEU - 1 , BLEU2 , BLEU - 3 , BLEU - 4 . BERT SCORE 6 ( Zhang et al , 2019 ) computes a similarity score by using contextual embedding of the tokens . We calculate the precision ( p ) , recall ( r ) and f 1 - score to evaluate the information matching degree . Accuracy of judgment prediction To evaluate the performance of the predictor , we calculate the precision ( p ) , recall ( r ) and , f 1 - score of supported and non - supported cases , respectively .", "entities": [[54, 57, "MetricName", "ROUGE - L"], [68, 69, "MetricName", "BLEU"], [97, 98, "MetricName", "BLEU"], [101, 102, "MetricName", "BLEU"], [114, 115, "MetricName", "BLEU"], [121, 122, "MetricName", "BLEU"], [127, 128, "MetricName", "BLEU"], [131, 132, "MetricName", "BLEU"], [135, 136, "MethodName", "BERT"], [181, 182, "MetricName", "Accuracy"]]}
{"text": "Tab . 2 demonstrates the results of court 's view generation with ROUGE , BLEU , and BERT SCORE . Also , we report the results on the judgment prediction of our predictor component with precision", "entities": [[14, 15, "MetricName", "BLEU"], [17, 18, "MethodName", "BERT"]]}
{"text": "The court concluded that : The subject of the private lending relationship between Plaintiff A and Defendant B was qualified , the content was legal , and the meaning was true . It should be deemed valid . Defendant should repay the plaintiff 's loan within a reasonable period after the plaintiff urged . Therefore , Defendant B should bear the civil liability of returning the plaintiff 's loan of $ 495 , 000 and paying overdue interest Acceptance . The court did not support the plaintiff 's claim requesting the defendant C to return the loan together because the evidence was insufficient ( p ) , recall ( r ) , and f 1 - score ( f1 ) in Tab . 3 . To demonstrate that our method is de - biased on judgment generation , we report the result of human evaluation in Tab . 4 . Results of court 's view generation : From Tab . 2 , we can conclude that : ( 1 ) S2S tends to repeat words , which makes it get high BLEU but low BERT SCORE . ( 2 ) Oversampling strategy does n't benefit the models , hence , it can not address the confounding bias . ( 3 ) With claim - aware encoder and backdoor - inspired counterfactual decoders , our AC - NLG achieves better performance on court 's view generation compared with baselines . ( 4 ) The performance gap between AC - NLGw / oCA and AC - NLG demonstrates the effectiveness of our proposed claim - aware encoder , and the gap between AC - NLGw / oBA and AC - NLG illustrates the superiority of our counterfactual decoders . Results of judgment prediction : From Tab . 3 , we have the following observations : ( 1 ) The counterfactual decoders in our model can significantly eliminate the confounding bias , hence , achieve remarkable improvement on the non - supported cases , for example boosting f 1 from 49.8 % to 76.9 % . ( 2 ) The proposed claim - aware encoder has a limited effect on judgment prediction since it 's designed for improving the quality of generation as shown in Tab . 2 . ( 3 ) Still , oversampling brings no improvement to the model . Results of human evaluation : From Tab . 4 , we have the following observations : ( 1 ) due to the confounding bias in data , the performance of judgment generation in PGN is poor for non - supported cases , and its performance gap between supported and non - supported cases is huge ( 1.56 ) . ( 2 ) By debiasing with backdoor - inspired counterfactual decoders , our AC - NLG significantly improves the performance of judgment generation , especially for non - supported cases , and achieves a smaller performance gap ( only 0.28 ) between the supported and non - supported cases . ( 3 ) With a claim - aware encoder , our AC - NLG also achieves better performance on the generation of rational and generated court 's view fluency . ( 4 ) Kappa coefficient \u03ba is more than 0.8 between any two judges , which proves the validation of human evaluation . Overall , thanks to the proposed claim - aware encoder , counterfactual decoders , and a synergistic judgment predictor , our model achieves better performance than single - task baselines on the task of judgment prediction , judgment generation in court 's view and court 's view generation .", "entities": [[181, 182, "MetricName", "BLEU"], [184, 185, "MethodName", "BERT"]]}
{"text": "While AI is gaining adoption in legal justice ( Lin et al , 2012 ; Zhong et al , 2018 ; Hu et al , 2018 ; Jiang et al , 2018 ; Chalkidis et al , 2019 ) , any subtle statistical miscalculation may trigger serious consequences . From a fairness perspective , prior studies suggested that global ( statistical ) optimization = individual ( demographic ) fairness ( Zemel et al , 2013 ) , and this ethical concern should be further investigated . In this section , we explore the following ethical issues . Target User : According to the report of statistics , a typical active trial judge closed around 250 cases in a year . Trial judges suffering from ' daunting workload ' is becoming an critical issue ( Duan et al , 2019 ) . The proposed algorithm is designed for generating the court 's view draft for assisting the trial judges for decision making . This work is an algorithmic investigation , but such algorithm should never ' replace ' human judges . Human knowledge / judgment should be the final safeguard to protect social justice and individual fairness . Potential Error : The potential error would be as follows : a ) generating a wrong judgment and b ) generating a wrong rationale . The goal of our algorithm is to generate a draft of court 's view for trail judge as a reference , and judges need to proofread the content generated from algorithm . Demographic Bias : In this paper , we focus on addressing the bias problem from the data generation by treating the variable of data generation as confounder in back - door adjustment . The model adoption can face potential demographic bias / unfairness challenges , such as gender and race bias in the training data . To further ensure the model fairness , in the future , algorithm adoption should be empowered with de - biased legal content pretraining , which could avoid potential demographic bias . For instance , in order to remove gender / race bias , system could use ( Bolukbasi et al , 2016 ) algorithm to debias the sensitive gender / race information , e.g. , replace ' he / she ' and ' asian / hispanic ' with gender / race neutral words for pretraining , which can be vital for legal domain .", "entities": [[159, 161, "TaskName", "decision making"], [198, 199, "MetricName", "Error"]]}
{"text": "In this paper , we propose a novel Attentional and Counterfactual based Natural Language Genera - tion ( AC - NLG ) method to solve the task of court 's view generation in civil cases and ensure the fairness of the judgment . We design a claim - aware encoder to represent the fact description which emphasizes on the plaintiff 's claim , as well as a pair of backdoor - inspired counterfactual decoders to generate judgment - discriminative court 's views ( both supportive and non - supportive views ) and to eliminate the bias that arose from the data generation mechanism by connecting with a synergistic judgment predictive model . The experimental results show the effectiveness of our method . Based on the AC - NLG method , in the future , we can explore the following directions : ( 1 ) Improve the accuracy of judgment on a claim - level . ( 2 ) Add external knowledge ( e.g. a logic graph ) to the predictor for the interpretability of the model .", "entities": [[146, 147, "MetricName", "accuracy"]]}
{"text": "1 . The defendants B and C jointly repaid the loan principal of $ 20 , 000 and the interest loss ( calculated from the bank 's loan interest rate at the same period from the date of prosecution to the date when the judgment is confirmed ) . 2.The litigation costs in this case are paid by the two defendants .", "entities": [[20, 21, "MetricName", "loss"]]}
{"text": "The court concluded that : The private lending relationship between the plaintiff and the defendant is established and effective , and shall be protected according to law . The defendant should repay the loan after receiving it , but now he did not repay , it is obviously a breach of contract . Therefore , this court supports the claim of the plaintiff that the defendant should return the loan of $ 20 , 000 and the corresponding loss of interest Acceptance . The plaintiff claimed that the defendant should pay interest , but did not provide evidence to prove that both of them clearly agreed on the interest , so the court does not support the plaintiff 's claim for interest Rejection . The plaintiff withdrew some of the claims in the court hearing , and this court permitted it .", "entities": [[78, 79, "MetricName", "loss"]]}
{"text": "To start with , the input vector of each word is generated by utilizing a word embedding lookup table L w R r\u00d7dw and a positional embedding lookup table L p R n\u00d7dp , where d w is the dimension of word embeddings , r is the vocabulary size , and d p is the dimension of positional embeddings . These embedding lookup tables will map s = { w 1 , ... , w n } to { e 1 w , ... , e n w } and { e 1 p , ... , e n p } , respectively . For our base models ( not using a pre - trained language model ) , e i w will be projected to a low dimensional vector e i low which is calculated as follows : e i low = \u03c3 ( W e e i w ) , where W e R d low \u00d7dw ( d low < d w ) denotes the matrix of projection and \u03c3 ( ) is the activation function . In this case , t i in the input T = { t 1 , ... , t n } is rep - resented by [ e i low ; e i p ] and d model = d low + d p . For a pre - trained language model like BERT ( Devlin et al , 2019 ) , t i equals the sum of e i w , e i p , and e i s , where e s = { e 1 s , ... , e n s } ( the dimension of e i s is d p ) represents segment embeddings , and d model = d p = d w . Then , the input vector T is passed to multihead self - attention modules , where a feed - forward network and an add - norm network are combined in sequence to generate the context representation of each layer H = { H 1 , ... , H l } , where l is the number of multi - head attention layers and H i = { H 1 i , ... , H n i } . H i can be calculated as follows : O i = M H ( H i\u22121 , h ) , ( 4 ) F F N i = max ( 0 , O i W i 1 + b i 1 ) W i 2 + b i 2 , ( 5 ) H i = LN ( H i\u22121 + F F N i ) , ( 6 ) where h is the number of attention heads , H 0 = T , the matrices W i 1 R d model \u00d7d f f and W i 2 R d f f \u00d7d model represent mappings from d model to d f f and back to d model . LN ( ) is a layer normalization method applying to sequential data ( Ba et al , 2016 ) . Finally , the output of the encoder is H l , i.e. , the last layer of H.", "entities": [[41, 43, "TaskName", "word embeddings"], [177, 179, "HyperparameterName", "activation function"], [232, 233, "MethodName", "BERT"], [357, 361, "MethodName", "multi - head attention"], [408, 409, "DatasetName", "0"], [458, 459, "DatasetName", "0"], [503, 505, "MethodName", "layer normalization"]]}
{"text": "The single - task version of our approaches is TSMSA . Given a predicted label sequence Y and a sequential representation H l , the score function S ( H l , Y ) can be defined as follows : S ( H l , Y ) = n i=1 Q y i\u22121 , y i + n i=1 P i , y i , ( 7 ) P = H l W p + b p , ( 8 ) where the matrix Q R k\u00d7k captures the relation of adjacent labels , the matrix P R n\u00d7k learns the relation of tokens and labels , and the matrices W p R d model \u00d7k and b p R n\u00d7k indicate a projection operation from dimension d model to dimension k. In the above , k means the dimension of the label space . Then , the linear - chain CRF is exploited to calculate the conditional probability of the predicted sequence Y as follows : p ( Y | H l ) = exp ( S ( H l , Y ) ) \u1ef8 Y all exp ( S ( H l , \u1ef8 ) ) , ( 9 ) where Y all denotes the set of all possible sequential labels . So the loss of a sentence can be calculated by the negative log likelihood as follows : L ( s ) = \u2212 log p ( Y | H l ) . ( 10 )", "entities": [[151, 152, "MethodName", "CRF"], [216, 217, "MetricName", "loss"]]}
{"text": "By integrating aspect and opinion term extraction ( task 0 ) and TOWE ( task 1 ) into a multi - task architecture , we propose a MT - TSMSA method for AOPE . MT - TSMSA can be defined as using a sentence H l and a task i d { 0 , 1 } to calculate the conditional probability p ( Y | H l , i d ) . When the task i d equals 0 , it means aspect and opinion term extraction . For TOWE , the task i d is 1 . Some examples are shown in Figure 1 ( d ) . Aiming at handling different tasks , different score functions S 0 ( H l , Y 0 ) and S 1 ( H l , Y 1 ) are defined , where S 0 ( ) and S 1 ( ) have different parameter matrices , Y 0 ( Y 0 i { B - ASP , I - ASP , B - OP , I - OP O } ) and Y 1 ( Y 1 i { B , I , O , [ SEP ] } ) represent the sequential labels of aspect and opinion term extraction , and TOWE , respectively . So the conditional probabilities of the predicted sequences Y 0 and Y 1 can be calculated as follows : p ( Y 0 | H l , i d = 0 ) = exp ( S 0 ( H l , Y 0 ) ) \u1ef8 Y 0 all exp ( S 0 ( H l , \u1ef8 ) ) , ( 11 ) p ( Y 1 | H l , i d = 1 ) = exp ( S 1 ( H l , Y 1 ) ) \u1ef8 Y 1 all exp ( S 1 ( H l , \u1ef8 ) ) , ( 12 ) where Y 0 all denotes the set of all possible sequential labels of task 0 and Y 1 all represents the set of all possible sequential labels of task 1 . The loss of a sentence is also calculated by the negative log likelihood as follows : L ( s , i d ) = \u2212 log p ( Y | H l , i d ) . ( 13 ) Given M sentences S = { s 1 , s 2 , ... , s M } with i d = { i d 1 , ... , i d M } , we can minimize the loss for training : J ( \u03b8 ) = M k=1 ( ( 1 \u2212 i d k ) \u03bb + i d k ) L ( s k , i d k ) , ( 14 ) where \u03bb is the hyper - parameter used to balance these two tasks .", "entities": [[5, 7, "TaskName", "term extraction"], [9, 10, "DatasetName", "0"], [52, 53, "DatasetName", "0"], [78, 79, "DatasetName", "0"], [85, 87, "TaskName", "term extraction"], [119, 120, "DatasetName", "0"], [125, 126, "DatasetName", "0"], [142, 143, "DatasetName", "0"], [156, 157, "DatasetName", "0"], [159, 160, "DatasetName", "0"], [207, 209, "TaskName", "term extraction"], [224, 225, "DatasetName", "0"], [237, 238, "DatasetName", "0"], [245, 246, "DatasetName", "0"], [251, 252, "DatasetName", "0"], [257, 258, "DatasetName", "0"], [262, 263, "DatasetName", "0"], [267, 268, "DatasetName", "0"], [326, 327, "DatasetName", "0"], [338, 339, "DatasetName", "0"], [356, 357, "MetricName", "loss"], [432, 433, "MetricName", "loss"], [438, 439, "HyperparameterName", "\u03b8"]]}
{"text": "For the TOWE task , Fan et al ( 2019 ) utilize 300dimension GloVe ( Pennington et al , 2014 ) vectors which are pre - trained on unlabeled data of 840 billion tokens to initialize word embedding vectors in IOG . The word embeddings are fixed at the stage of training . For fair comparison , we use the same fixed word embeddings in TSMSA ( Base ) . We randomly select 20 % of the training set as the development set for adjusting all hyper - parameters . The value of d model is 128 , and the numbers of attention heads and layers are 4 and 6 , respectively . In addition , the dropout rate , learning rate , and maximal sequence length are set to 0.5 , 0.001 , and 100 , respectively . Adam optimizer ( Kingma and Ba , 2015 ) is adopted to optimize our model . Pretrained language models like BERT ( Devlin et al , 2019 ) can be applied to our methods , and we adopt BERT - base 3 model , where d model is 768 and the number of attention heads and layers are both 12 . Other hyper - parameters include the learning rate of BERT and CRF , the maximal sequence length , and the number of epochs . Based on the development set , these hyper - parameters are set to 5e - 5 , 2e - 4 , 100 , and 8 , respectively . Unless otherwise mentioned , \u03bb is set to 1 . To be consistent with various baselines ( Fan et al , 2019 ; Chen et al , 2020 ; , the term - level F1 score is used as the evaluation metric for both TOWE and AOPE tasks . Term - level means that the boundaries of the span are the same as the ground - truth . For the AOPE task , the consistency of a predicted aspect - opinion pair with the labeled pair indicates the correctness of prediction . based methods are poor because the rules only cover a small number of cases . By utilizing BiL - STM or BERT as the encoder to extract opinion terms , the BiLSTM / BERT + Distance - rule perform much better than other rule - based methods . However , these methods can not deal with the one - to - many case . Secondly , TC - BiLSTM and TF - BERT extract static word embeddings for aspects and then incorporate them into sentence representation by concatenation or addition . Nevertheless , the results of TC - BiLSTM and TF - BERT are still over 10 % lower than IOG / TSMSA ( Base ) and SDRN / TSMSA ( BERT ) , respectively . It reveals that the static word embedding is not a good representation of the aspect and the concatenation / addition operation is not good enough to represent the specific aspect . Finally , IOG is a state - ofthe - art baseline method for TOWE and the performance of TSMSA ( Base ) trained by the same word embedding is similar to IOG , which indicates the effectiveness in capturing the representation of a specific aspect with the symbol \" [ SEP ] \" . Furthermore , the pre - trained language model BERT can be applied to our basic method . The F1 score of TSMSA ( BERT ) is in average 8 % higher than TSMSA ( Base ) and IOG . SDRN , which also exploits BERT as the encoder , passes the information of the aspect through a synchronization unit and utilizes supervised self - attention to capture this information . Nevertheless , it represents the specific aspect implicitly , which might have an negative impact on capturing the information of targets . In average , the performance of SDRN is 2 % lower than TSMSA ( BERT ) . The overall results reveal that our proposed method achieves state - of - the - art performance on TOWE .", "entities": [[13, 14, "MethodName", "GloVe"], [43, 45, "TaskName", "word embeddings"], [62, 64, "TaskName", "word embeddings"], [120, 122, "HyperparameterName", "learning rate"], [139, 140, "MethodName", "Adam"], [140, 141, "HyperparameterName", "optimizer"], [155, 158, "TaskName", "Pretrained language models"], [159, 160, "MethodName", "BERT"], [177, 178, "MethodName", "BERT"], [206, 208, "HyperparameterName", "learning rate"], [209, 210, "MethodName", "BERT"], [211, 212, "MethodName", "CRF"], [220, 223, "HyperparameterName", "number of epochs"], [286, 288, "MetricName", "F1 score"], [365, 366, "MethodName", "BERT"], [375, 376, "MethodName", "BiLSTM"], [377, 378, "MethodName", "BERT"], [412, 413, "MethodName", "BiLSTM"], [416, 417, "MethodName", "BERT"], [419, 421, "TaskName", "word embeddings"], [442, 443, "MethodName", "BiLSTM"], [446, 447, "MethodName", "BERT"], [465, 466, "MethodName", "BERT"], [563, 564, "MethodName", "BERT"], [573, 575, "MetricName", "F1 score"], [578, 579, "MethodName", "BERT"], [599, 600, "MethodName", "BERT"], [661, 662, "MethodName", "BERT"]]}
{"text": "The results of convergence and sensitivity studies are shown in Figure 2 . Figure 2 ( a ) reveals that our model gradually converges as the number of epochs increases . Although the dropout rate is set to 0.5 , it also converges smoothly . Figure 2 ( b ) shows the effect of the number of attention heads . When the number of attention heads is 4 , TSMSA ( Base ) achieves stable and good performance , and as the value increased , the performance might be better . Figure 2 ( c ) shows that the best performance is achieved when the number of multi - head self - attention layers is 6 , and as the number increased , the model might be confronted with overfitting . Figure 2 ( d ) indicates the impact of \u03bb on our model which influences the learning of different tasks . Stable and good results can be obtained when \u03bb = 1 , and better performance can be achieved when the value is set to 0.5 or 2 . Compared with other hyper - parameters , the results also indicate that \u03bb has a relatively small impact on the model performance .", "entities": [[26, 29, "HyperparameterName", "number of epochs"], [112, 114, "HyperparameterName", "attention layers"]]}
{"text": "To further compare our MT - TSMSA ( BERT ) with the best - performing baseline of SDRN , we here conduct a case study by following ( Chen et al , 2020 ) . As shown in Table 6 , both SDRN and MT - TSMSA ( BERT ) perform well in extracting aspectopinion pairs from complicated relations . But in some cases like Case 4 , SDRN misses the pair of ( watching videos , hot ) . The reason may be that the massive hyper - parameters in SDRN have a great impact on the effect . For example , the threshold \u03b2 in the relation synchronization mechanism of SDRN will largely affect the results of the model . On the other hand , our method can extract all the pairs because it introduces fewer hyper - parameters , which leads to stable results . However , in Case 5 , our method can not extract the pair . The rea - son is that task 0 of MT - TSMSA ( BERT ) fails to extract the aspect term \" log into the system \" . Moreover , the in - depth reason is that for the aspect term extraction task , the performance of SDRN ( i.e. , 83.67 % , 89.49 % , and 74.05 % ) is better than that of MT - TSMSA ( BERT ) , i.e. , 83.11 % , 84.85 % , and 72.69 % on the datasets from ( Chen et al , 2020 ) .", "entities": [[8, 9, "MethodName", "BERT"], [48, 49, "MethodName", "BERT"], [105, 106, "HyperparameterName", "\u03b2"], [169, 170, "DatasetName", "0"], [175, 176, "MethodName", "BERT"], [202, 204, "TaskName", "term extraction"], [232, 233, "MethodName", "BERT"]]}
{"text": "We formulate frame detection as a multilabel classification problem for each of the three typologies , using our dataset to train supervised models . Experimental Setup Our proposed model is a RoBERTa model ( Liu et al , 2019b ) trained using binary cross - entropy on the CLS token . We consider both ( i ) a model trained using the roberta - base parameters and ( ii ) a second model that has first been fine - tuned on our full set of immigration tweets using masked - language modeling . Fine tuning was performed for 60 epochs . In both models , early stopping is used to avoid overfitting . Models are compared with two baselines : random prediction , and logistic regression with unigram and bigram features . Each model was trained five times with different random seeds and we report bootstrapped mean performance . Results The fine - tuned RoBERTa model significantly outperforms all baselines ( Table 2 ) . RoBERTa has the most substantial gains over logistic regression for low - frequency frames ( Supplementary Material C , Figure 8 ) . These gains for rare frames are essential for analyzing immigra - tion discourse on social media in order to capture diverse perspectives and arguments . Table 3 shows several evaluation metrics separated by frame type . Precision , recall , and F1 are calculated as unweighted averages over all frames belonging to each category . Overall , issue - generic policy and narrative frames can be detected more effectively than issue - specific frames . This difference reflects that issue - specific frames were sparser in the training data , but also that detecting these frames is inherently more challenging because it requires jointly reasoning about immigration - related topics and how these topics affect immigrants . For example , tweets about immigrants committing crimes and tweets about hate crimes committed against immigrants have distinct issue - specific frames ( threat : public order and victim : discrimination ) , even though these texts can be linguistically quite similar . Given some thematic similarities between typologies , we tested an additional model that jointly predicted frames from all three typologies using the fine - tuned RoBERTa model ; however , the resulting model offered worse performance than any single - typology model , suggesting minimal benefits of cross - typology learning . Supplementary Section C contains additional model performance analyses by frame and region . Hegemonic Framing Conservative media 's framing of political issues is known to be more consistent , coordinated , and hegemonic than mainstream media , which has been vital to the success of the American conservative movement ( Hemmer , 2016 ; Speakman and Funk , 2020 ) . If the same pattern holds for social media , we would expect automated", "entities": [[31, 32, "MethodName", "RoBERTa"], [105, 107, "MethodName", "early stopping"], [124, 126, "MethodName", "logistic regression"], [141, 142, "DatasetName", "seeds"], [154, 155, "MethodName", "RoBERTa"], [165, 166, "MethodName", "RoBERTa"], [172, 174, "MethodName", "logistic regression"], [180, 182, "DatasetName", "Supplementary Material"], [224, 225, "MetricName", "Precision"], [229, 230, "MetricName", "F1"], [373, 374, "MethodName", "RoBERTa"]]}
{"text": "Overgeneralizing highly - correlated features Many words and phrases do not directly cue frames , but are highly - correlated . The model makes erroneous predictions when such features are used in different contexts ( e.g. violence against immigrants , rather than immigrants being violent ) Lunaria 's figures from 2018 recorded 12 shootings , two murders and 33 physical assaults against migrants in the first two months since Salvini entered government . Model missed Victim : Humanitarian frame Pronoun ambiguity Coreference resolution is often not possible and annotators avoided making assumptions to resolve ambiguities . For example , \" you \" can be used to discuss individuals ' experiences ( episodic ) but its impersonal sense can be in broad generalizations ( thematic ) . It 's worse when you have immigrant parents who do n't speak the language cause you have to deal with all the paperwork , be the translator for them whenever they go ( ... ) its tiring but someone has to Model predicted Episodic but referent is unclear frame detection to achieve higher performance on conservative tweets due to more linguistic regularities across messages . Indeed , we find that issuegeneric and issue - specific classifiers achieve higher F1 scores on tweets written by conservative authors compared to liberal authors ( Figure 1 ) , even though there are fewer conservative tweets in the training data ( 334 conservative vs 385 liberal tweets ) . Higher model performance on conservative tweets suggests that , like political and media elites , conservatives on social media are more consistent than liberals in their linguistic framing of immigration . Error Analysis We identify classification errors by qualitatively analyzing a random sample of 200 tweets that misclassified at least one frame . Table 4 shows the most common categories of errors .", "entities": [[81, 83, "TaskName", "Coreference resolution"], [204, 205, "MetricName", "F1"], [272, 273, "MetricName", "Error"]]}
{"text": "We are interested in the successful classification of aggressive posts only and therefore , rather than reporting precision , recall and F - measures , we re - port accuracy as in equation ( 1 ) : Accuracy = true positives true positives + false negatives ( 1 )", "entities": [[29, 30, "MetricName", "accuracy"], [37, 38, "MetricName", "Accuracy"]]}
{"text": "All test and training texts were lower - cased and transformed into document - term matrices using the text2vec package for R ( Selivanov and Wang , 2017 ) . For each value of threshold t from 1 to 10 , the training texts were assigned true and false labels according to their attack score s where aggression is true if s \u2265 t. We trained an extreme gradient boosting ( XG - Boost ) classifier with the R package xgboost ( Chen et al , 2018 ) . Boosting is an additive technique whereby new models are added to correct the errors made by existing models thus far : models are added sequentially until no further improvements can be made . In gradient boosting , new models predict the residuals or errors of prior models using a gradient descent algorithm . XG - Boost is known to work well with sparse matrices , which is the kind of input associated with textual data , and in NLP terms has been shown to perform competitively in sentiment analysis shared tasks ( Nasim , 2017 ; Jabreel and Moreno , 2018 ) . To avoid over - fitting we set parameters fairly conservatively , with a maximum tree depth of 6 , the number of rounds at 10 and early stopping set to 5 , gamma at 1 , and the learning rate at 0.3 . We report classifier accuracy according to equation ( 1 ) on gold aggression : true labels in our CrimeBB test corpus . Recall that we do not compare XGBoost with other classifiers , as our focus is on the training data rather than performance . In future work we can investigate other models including neural networks , though logistic regression has in some cases out - performed neural nets in the detection of abusive language ( Park and Fung , 2017 ) . As the value of t increases the size of the aggression : true dataset decreases , as seen in Table 1 . To ensure any change in accuracy is not due to the decrease in aggression : true training instances , we run a second experiment in which for all values of t both label subsets ( aggression : true and aggression : false ) are randomly reduced to 3223 instances - the size of the smallest attack score subcorpus ( per the cumulative n.posts column in Table 1 ) . For this latter experiment we report accuracies averaged over one hundred runs to smooth variation in the random sampling process ( identified as ' Acc . Control ' in Table 3 ) .", "entities": [[176, 178, "TaskName", "sentiment analysis"], [218, 220, "MethodName", "early stopping"], [224, 225, "HyperparameterName", "gamma"], [230, 232, "HyperparameterName", "learning rate"], [238, 239, "MetricName", "accuracy"], [257, 258, "MetricName", "Recall"], [293, 295, "MethodName", "logistic regression"], [308, 310, "TaskName", "abusive language"], [345, 346, "MetricName", "accuracy"], [433, 434, "MetricName", "Acc"]]}
{"text": "Classification accuracies are shown in Table 3 5 . It is apparent that in both training data settings - controlled and non - controlled ( ' all ' ) - the accuracy of aggression identification reduces as the true / false cut - off threshold t increases . In the case of the controlled training data setting there is at first a small increase in accuracy as t rises from 1 to 3 . This result suggests that the levels in the Wiki - Comments Corpus most closely matching the aggressive posts on HackForums are those in the attack score range 1 to 5 , and that the optimal value of t is between 2 and 3 . To illustrate the rise and fall in classification accuracy as t increases , we plot accuracies as boxplots for the 100 runs in the controlled training data setting ( Figure 4.3 ) . The boxplots show medians ( the thick horizontal bars ) , first and third quar - tiles ( Q1 , Q3 , shown by the hinges ) , and whiskers extending as far as 1.5 * IQR where IQR is the inter - quartile range between Q1 and Q3 . Datapoints beyond the whiskers are outliers and are plotted individually .", "entities": [[0, 1, "TaskName", "Classification"], [31, 32, "MetricName", "accuracy"], [33, 35, "TaskName", "aggression identification"], [65, 66, "MetricName", "accuracy"], [126, 127, "MetricName", "accuracy"], [187, 188, "DatasetName", "IQR"], [189, 190, "DatasetName", "IQR"]]}
{"text": "We have shown that abusive language in an online hacking forum is relatively mild compared to that found in Wikipedia page edit comments . We propose that the tendency of forum users to on the whole engage in constructive and informative discourse results in positive behaviour and non - toxic language . WikiComments , on the other hand , is made up of debates about the rights and wrongs of page edits , and perhaps inevitably this adversarial set up allows more aggressive behaviours to manifest themselves in writing . In future work we evidently need to annotate more data so that we have more than 100 examples of abusive language from CrimeBB . Due to the low hit rate for abusive language in CrimeBB texts ( 100 in 4123 , for instance ) we can investigate automatic annotation of further chunks of the data , along with supervised sampling from those new annotations to check their quality . These labelled data on a larger scale will allow us to analyse more general patterns of behaviour such as individual and community - wide trends over time , how aggression surfaces and is dealt with by moderators , and linguistic facets of aggressive behaviour such as homophobia , racism , misogyny and so on . We can also investigate other Internet domains such as social media , other forums and potentially the Dark Web , but also other sections of CrimeBB , such as the reputation voting area within HackForums in which we might expect to find more vitriolic interactions given that votes can be both positive and negative and accompanied by review - like texts . Finally , we are also interested in applications of our research , including the questions of desired accuracy of any deployed system , the appropriate actions to take , and the ethics of data collection , analysis and intervention ( Kennedy et al , 2017 ; . One option could be to create an alert system for forum moderators , thereby offering real - world impact for our work while allowing the appropriate authorities to take action when necessary ( Kumar et al , 2018 ) .", "entities": [[4, 6, "TaskName", "abusive language"], [109, 111, "TaskName", "abusive language"], [121, 123, "TaskName", "abusive language"], [293, 294, "MetricName", "accuracy"], [356, 357, "DatasetName", "Kumar"]]}
{"text": "Knowledge Bases ( KBs ) can be created in many different ways depending on the availability of external resources and specific application needs . Recently , much work in Natural Language Processing focused on Knowledge Base Completion ( Nickel et al , 2016a , KBC ) , the task of enriching and refining existing KBs . Many different methods have been explored for KBC , including exploitation of resources such as text corpora ( Snow et al , 2006 ; Mintz et al , 2009 ; Aprosio et al , 2013 ) or other KBs Bryl and Bizer , 2014 ) for acquiring additional knowledge . Alternative approaches , in contrast , primarily rely on existing information from the KB itself ( Socher et al , 2013 ; Nickel et al , 2016b ) used as ground - truth to simultaneously learn continuous representations of KB concepts and relations , which are used to infer additional KB relations . Finally , Open Information Extraction methods looked at ways to extract large amounts of facts from Web - scale corpora in order to acquire open - domain KBs Faruqui and Kumar , 2015 , inter alia ) ; In this paper , we focus on a different , yet complementary task , which is a necessary step when inducing novel KBs from scratch , namely extracting clean taxonomies from noisy knowl - edge graphs . State - of - the - art algorithms differ by the amount of human supervision required and their ability to respect some topological properties while pruning . Approaches like those of Kozareva and Hovy ( 2010 ) , Velardi et al ( 2013 ) and Kapanipathi et al ( 2014 ) , for instance , apply different topological pruning strategies that require to specify the root and leaf concept nodes of the KB in advance - i.e. , a predefined set of abstract toplevel concepts and lower terminological nodes , respectively . The approach of avoids such supervision on the basis of an iterative method that uses an efficient variant of topological sorting ( Tarjan , 1972 ) for cycle pruning . Such lack of supervision , however , comes at the cost of not being able to preserve the original connectivity between the top ( abstract ) and the bottom ( instance ) concepts . Random edge removal , in fact , can lead to disconnected components , a problem shared with the OntoLearn Reloaded approach ( Velardi et al , 2013 ) , which can not ensure such property when used to approximate the solution for a large noisy graph . Our work goes one step beyond the previous contributions by presenting a new efficient algorithm that is able to extract a clean taxonomy from a noisy knowledge graph without needing to know in advance - that is , having to manually specifythe top - level and leaf concepts of the taxonomy , while preserving the overall connectivity of the graph . We achieve this by projecting the information from a reference KB such as , for instance , WordNet ( Fellbaum , 1998 ) , onto the input noisy KB on the basis of pre - existing KB links - which in turn can be automatically generated with high precision using any of the existing solutions for KB mapping ( Navigli and Ponzetto , 2012 ; Faralli et al , 2016 , inter alia ) or by relying on ground truth information from the Linguistic Linked Open Data cloud ( Chiarcos et al , 2012 ) . Some aspects of the proposed approachnamely , the propagation of the nodes ' weights through the graph , which we metaphorically represent as the flow of a contrast medium across nodes ( Section 3.3 ) - are somewhat similar in spirit to spreading activation ( Collins and Loftus , 1975 ) and random walks on graphs ( Lov\u00e1sz , 1993 ) approaches . However , in contrast to spreading activation approaches we leverage the graph directionality in order to reach all the possible nodes within the same connected components . More - over , in contrast to random walks on graphs our method is deterministic in nature . Here , we argue for the choice of a deterministic approach , like ours , that does not require tuning of parameters : its termination is guaranteed by the number of iterations , which we bind by the maximal diameter | E | for a graph G = ( V , E ) . Generally , random walk algorithms would provide an approximation that may lead to a less precise estimation of the order induced by the contrast medium level . 3 The ContrastMedium Algorithm", "entities": [[34, 37, "TaskName", "Knowledge Base Completion"], [161, 164, "TaskName", "Open Information Extraction"], [189, 190, "DatasetName", "Kumar"], [731, 734, "HyperparameterName", "number of iterations"]]}
{"text": "We benchmark ContrastMedium using a variety of metrics that are meant to capture structural properties of the output taxonomies ( to describe the impact of pruning on the original NKGs ) , as well as an estimation of their overall quality . Edge compression : the ratio of the number of pruned edges over the total number of edges : C E G , G = | E G | \u2212 | E G | | E G | where E G and E G represent the number of edges found within the input ( G ) and pruned ( G ) taxonomy , respectively . Pruning accuracy : the performance on a 3 - way classification task to automatically detect the level of granularity of a concept as a proxy to quantify the overall quality of the output taxonomies . Pruning accuracy is estimated using gold - standard annotations that are created from a random sample of 1 , 000 nodes for each NKG . Two annotators with previous experience in knowledge acquisition and engineering were asked to provide for each concept whether it can be classified as : i ) a root , top - level abstract concept - i.e. , any of entity , object , etc . and more in general nodes that correspond to abstract concepts that we can expect to be part of a core ontology such as , for instance , DOLCE ( Gangemi et al , 2002 ) ; ii ) a leaf terminological node ( i.e. , instances such as Lady Gaga or Porsche 911 ) ; iii ) or a middle - level concept ( e.g. , celebrity or cars , concepts not fitting into any of the previous classes ) . An adjudication procedure was used to resolve any discrepancy between the two annotators : the inter - annotator agreement after adjudication is \u03ba = 0.657 ( Fleiss , 1971 ) , with most disagreement occurring on the identification of abstract , core ontology concepts . A local 3 - way classification task provides a rather crude way to estimate the performance on inducing hierarchical structures like taxonomies . Here , we use it primarily to benchmark how well ContrastMedium compares against other , structure - agnostic algorithms used within state - ofthe - art solutions such as , for instance , Tarjan 's topological sorting ( Section 2 ) , which only break cycles in a random fashion . Given ground - truth concept granularity judgements , we compute standard accuracy for each of the three classes . That is , we compare the system outputs against the gold standards and obtain three accuracy measures : one for the root nodes ( A R ) , one for the nodes ' in the middle ' ( A M ) and finally one for the leaf nodes ( A L ) . For example a true positive root node is a node annotated as a root node in the gold standard and having no incoming edges in the pruned graph . Error Reduction ( ER ) : finally , we compute the relative error reduction of ContrastMedium against other , baseline approaches as : Baseline | V G | | E G | C E G , G | V G | | E G | C E G , G A R A M A L A R A M A L news - p1 .6 170k 1.536k 0.15 % 170k 1.535k 0.18 % 98.9 98.3 99.3 93.3 94.6 95.3 0.62 news - p2.3 225k 1.867k 0.19 % 225k 1.866k 0.23 % 98.7 98.7 99.9 95.7 94.7 95.6 0.50 wiki - p1.8 183k 1.165k 0.18 % 183k 1.164k 0.22 % 97.6 94.7 97.3 93.1 87.3 94.1 0.41 wiki - p6.0 394k 1.897k 0.18 % 394k 1.896k 0.21 % 95.9 94.3 98.3 89.5 90.1 92.8 0.50 ( 2015 ) based on Tarjan 's topological sorting ( Section 2 ) , which iteratively searches for a cycle ( until no cycle can be found ) and randomly removes an edge from it . To the best of our knowledge , this is the only algorithm that we can fairly compare with , since alternative solutions all need to know the sets of root and leaf nodes in advance .", "entities": [[107, 108, "MetricName", "accuracy"], [142, 143, "MetricName", "accuracy"], [231, 232, "MethodName", "ontology"], [334, 335, "MethodName", "ontology"], [422, 423, "MetricName", "accuracy"], [445, 446, "MetricName", "accuracy"], [512, 513, "MetricName", "Error"]]}
{"text": "Table 3 summarizes the performance of Con - trastMedium on the four automatically acquired NKGs . The results show that the pruning impact of our approach is lower than that of the baseline ( an average of 1 K edges of difference , cf . columns 3 and 6 ) , which also determines higher edge compression C E G , G values for the baseline method . Despite being less aggressive in terms of the number of edges pruned , ContrastMedium outperforms the Tarjan - based algorithm on all datasets in terms of accuracy . Thanks to our method , in fact , we are able to achieve , even despite the baseline already reaching very high performance levels ( well above 90 % accuracy ) , improvements of up to 6 points , with an overall error reduction between around 40 % and 60 % . To provide an intuition of why ContrastMedium clearly outperforms the baseline approach , we provide in Figure 2 an exemplified depiction of a typical case on which the baseline fails ( based on a manually inspected random sample ) . In our example , the Tarjan baseline first detects the cycle C 1 = ( lion animal feline lion ) and randomly decides to break it by removing the edge ( animal feline ) . Next , it detects the cycle C 2 = ( animal great apes animal ) and randomly decides to break it by removing the edge ( animal great apes ) . ContrastMedium , instead , after the shaking of the graph can leverage the partial ordering of the nodes ( based on the concept granularity of the corresponding concepts ) to select the edges ( animal , feline ) , ( feline , lion ) and ( animal , great apes ) , while removing all remaining wrong and redundant edges .", "entities": [[94, 95, "MetricName", "accuracy"], [125, 126, "MetricName", "accuracy"]]}
{"text": "Our experiments are designed in leave - one - topicout cross - validation setting : From the 10 topicstance pairs in the dataset , we use nine for training and the last as the test fold , and we repeat this once for each possible fold . This way , no topic - specific knowledge can be used in the synthesis process . For each given basic rhetorical strategy ( logosoriented and pathos - oriented ) , we train one model each for the selection , the arrangement , and the \" phrasing \" of argumentative discourse units ( ADUs ) on the nine training folds . The arguments synthesized by their combination are then evaluated against the human - generated arguments in the test folds . The evaluation covers all three models as well as the final generated argument for each strategy . We report the average accuracy across all ten folds for each of the models .", "entities": [[147, 149, "MetricName", "average accuracy"]]}
{"text": "In each training / test experiment for one of the two strategies , we first abstract all ADUs across all strategy - specific topic - stance pairs by extracting the LIWC , NRC , and MPQA features , as described in Section 4.1 . Then , we cluster the given training set using standard k - means ( Ostrovsky et al , 2012 ) . After some initial experiments , we decide to set k to 6 , because this best balanced the distribution of arguments over clusters , and showed clear strategyspecific differences . 3 Using the resulting clustering model , we predicted the type A - F of each ADU in the test set ( the tenth topic ) . Given the ADU types , we next converted the human - generated training and test arguments from a sequence of ADUs to a sequence of ADU types . After that , we trained one 2 - gram and one 3 - gram selection language . 4 In Table 3 , we report the mean perplexity of the models for both strategies . As shown , the 2 - gram perplexity is lower than the 3 - gram perplexity in both cases . We assume that the reason lies in the limited size of the dataset and the narrow setting : Only 117 sentences ( ADUs ) are given per strategy for training , with a vocabulary size of 6 ( number of ADU types ) . Based on the results , we decided to use the 2 - gram selection language model to generate candidate arguments .", "entities": [[35, 36, "DatasetName", "MPQA"], [176, 177, "MetricName", "perplexity"], [191, 192, "MetricName", "perplexity"], [199, 200, "MetricName", "perplexity"]]}
{"text": "To train arrangement as described in Section 4.2 , we took all arguments of the nine training topics in each experiment . We converted each argument from a sequence of ADUs to a sequence of ADU roles ( thesis , pro , and con ) . After that , we trained a 2 - gram and 3 - gram language model for each strategy . Table 4 lists the mean perplexity values over the 10 folds . Here , the perplexity is lower for 3 - grams than for 2 - grams , which can be expected to yield better performance . Therefore , we used the 3 - gram language model to filter the set of candidate arguments .", "entities": [[70, 71, "MetricName", "perplexity"], [80, 81, "MetricName", "perplexity"]]}
{"text": "For phrasing ( in terms of choosing the best ADU sequence ) , we first extracted features from each candidate , as described in Section 4.3 . Then , we calculated the semantic similarities between each pair of adjacent ADUs as follows : 1 . We obtained a 300 - dimensional word embedding for each word in an ADU using the pre - trained GloVe common - crawl model ( Pennington et al , 2014 ) . 5 2 . We averaged the embeddings of all words in an ADU , resulted in one vector representing the ADU . 3 . For each adjacent pair of ADUs , we computed the cosine similarity of their vectors . Figure 4 shows a histogram of the distribution of the cosine similarities of each adjacent pair of 5 The used model can be found here : http://nlp . stanford.edu/data/glove.42B.300d.zip . ADUs ( i.e. , each ADU 2 - gram ) in logos - oriented arguments and in pathos - oriented arguments . We observe a generally high similarity between neighboring ADUs for both strategies , with logos - oriented 2 - grams being slightly more similar on average . Given the ADU 2 - grams , we train a linear regression model that predicts the sum of ADU 2 - gram probabilities in each argument . In case of the logos strategy , the model has a mean squared error ( MSE ) of 0.05 . In case of pathos the MSE is 0.03 .", "entities": [[64, 65, "MethodName", "GloVe"], [206, 208, "MethodName", "linear regression"], [238, 239, "MetricName", "MSE"], [248, 249, "MetricName", "MSE"]]}
{"text": "Up to this point , we trained all selection , arrangement , and phrasing models 10 times . Combining the three models for each strategy , we finally generated one argument per strategy for the topic - stance pair left out in each experiments . Hence , we ended up with 10 computationally synthesized arguments per strategy in total . We evaluate each of these arguments by checking whether it matches any of the 13 humangenerated ground - truth arguments given per topicstance pair . The matching is quantified in terms of n - gram overlap with n = { 1 , . . . , 5 } . For comparison , we consider a baseline that randomly generates arguments for each topic - stance pair as follows : 1 . Select a random thesis unit from t 1 to t 4 . 2 . Select a random con unit from c 1 to c 4 . 3 . Select three random pro units from p 1 to p 12 . 4 . Randomly arrange the selected units . Table 5 : Accuracy of n - gram overlaps between the human - generated arguments for each strategy and the arguments computationally synthesized by our model and the baseline . In the sequential case , the ordering is considered , in the non - sequential case , it is ignored . The better result in each experiment is marked bold , if any . Strategy ID Argument Computationally Synthesized from Five Argumentative Discourse Units", "entities": [[182, 183, "MetricName", "Accuracy"]]}
{"text": "Natural language inference is the task of predicting , given two fragments of text , whether the meaning of one ( premise ) entails the other ( hypothesis ) ( Dagan et al , 2006 ) . The task is formulated as a 3 - way classification problem , in which the premise and hypothesis pairs are labeled as entailment , contradiction , or neutral , if their relationship could not be directly inferred ( Bowman et al , 2015b ) . NLI has been widely used as an evaluation task for language understanding , and there have been a large number of challenging datasets , which have been used to further our understanding of the capabilities of language models ( Wang et al , 2018 . Paired data for figurative language is relatively sparse , and there is a gap in the diagnostic datasets used for NLI in this area . Previous work includes the literal / metaphoric paraphrases of Mohammad et al ( 2016 ) and Bizzoni and Lappin ( 2018 ) , although both contain only hundreds of samples , insufficient for proper model training and evaluation . With regard to NLI , early work proposed the task of textual entailment as a way of understanding metaphor processing capabilities Agerri , 2008 ) . Poliak et al ( 2018 ) build a dataset for diverse NLI , which includes some creative language such as puns , albeit making no claims with regard to figurativeness . Zhou et al ( 2021 ) build a dataset consisting of paired idiomatic and literal expressions . They begin with a set of 823 idiomatic expressions yielding 5 , 170 sentences , and had annotators manually rewrite sentences containing these idioms as literal expressions . We expand on this methodology by having annotators only correct definitions for the idioms themselves and use these definitions to automatically generate the literal interpretations of the idioms by replacing them into appropriate contexts : this allows us to scale up to over 24k silver sentences . We also expand beyond paraphrasing by incorporating both entailment and non - entailment Their dataset consists of figurative / literal pairs recast from previously developed simile and metaphor datasets , along with a parallel dataset between ironic and non - ironic rephrasing . This sets the groundwork for figurative NLI , but the dataset is relatively small outside of the irony domain , and the non - entailments are generated purely by replacing words with their antonyms , restricting the novelty of the hypotheses . Their dataset is relatively easy for NLI models ; here we show that figurative language can be challenging , particularly with regard to non - entailments . Zhou et al ( 2021 ) and Chakrabarty et al ( 2021a ) provide invaluable resources for figurative NLI ; our works aims to covers gaps in a number of areas . First , we generate a large number of both entailment and non - entailment pairs , allowing for better evaluation of adversarial non - entailing examples . Second , our silver methods allow for rapid development of larger scale data , allowing for model training and evaluation . We show that while entailment pairs are relatively easy ( accuracy scores ranging from .86 to .89 ) , the non - entailment pairs are exceedingly challenging , with the roberta - large model achieving accuracy scores ranging from .311 to .539 .", "entities": [[0, 3, "TaskName", "Natural language inference"], [542, 543, "MetricName", "accuracy"], [567, 568, "MetricName", "accuracy"]]}
{"text": "We obtain baseline NLI models by fine - tuning roberta - base and roberta - large models on the MNLI dataset ( Williams et al , 2018 ) , with entailments as the positive class and all others as the negative and evaluate them on their original test sets as well as IMPLI . 5 Due to variance in neural model performance ( Reimers and Gurevych , 2017 ) , we take the mean score over 5 runs using different seeds . We report results in Table 5 . We observe that idiomatic entailments are relatively easy to classify , with accuracy scores over .84 . Non - entailments were much more challenging . Silver pairs generated through adversarial definitions were especially difficult : the pairs contain high lexical overlap , and in many cases the premise and hypotheses are semantically similar . The replacement into literal samples were easier , as the idiomatic definition clashes more starkly with the original premise , making non - entailment predictions more likely . Consistent with Chakrabarty et al ( 2021a ) 's work in metaphors , non - entailment through antonym replacement is easiest for idioms : the antonymic relationship can be a marker for non - entailment , despite the high word overlap . With regard to metaphors , silver entailment pairs are relatively easy . Manual pairs are more challenging but are still much easier than idioms . This is supported by the fact that metaphors are common in everyday language : these models have likely seen the same ( or similar ) metaphors in training . Our findings show that in fact metaphoricity may not be particularly challenging for deep pre - trained models , as they are able to effectively capture the metaphoric entailment relations . The roberta - large model performs better for metaphoric expressions than roberta - base , but the difference on other partitions is relatively small . 6 We also find that lexical overlap plays a significant role here as noted by previous work ( McCoy et al , 2019 ) : sentences with high overlap tend to be classified as entailments regardless of the true label ( for more , see Appendix B ) . We note that the manual pairs tend to be more difficult for both idioms and metaphors : these pairs can be more flexible and creative , whereas the silver pairs are restricted to more regular patterns .", "entities": [[19, 20, "DatasetName", "MNLI"], [80, 81, "DatasetName", "seeds"], [101, 102, "MetricName", "accuracy"]]}
{"text": "We use a fixed set of hyperparameters for all NLI fine - tuning experiments : learning rate of 1e \u22125 , batch size 32 , and maximum input length of 128 tokens . The models are trained for 3 epochs . We used the HuggingFace implementation of the models ( Wolf et al , 2020 ) .", "entities": [[15, 17, "HyperparameterName", "learning rate"], [21, 23, "HyperparameterName", "batch size"]]}
{"text": "This paper presents our system details and results of participation in the RDoC Tasks of BioNLP - OST 2019 . Research Domain Criteria ( RDoC ) construct is a multi - dimensional and broad framework to describe mental health disorders by combining knowledge from genomics to behaviour . Non - availability of RDoC labelled dataset and tedious labelling process hinders the use of RDoC framework to reach its full potential in Biomedical research community and Healthcare industry . Therefore , Task - 1 aims at retrieval and ranking of PubMed abstracts relevant to a given RDoC construct and Task - 2 aims at extraction of the most relevant sentence from a given PubMed abstract . We investigate ( 1 ) attention based supervised neural topic model and SVM for retrieval and ranking of PubMed abstracts and , further utilize BM25 and other relevance measures for re - ranking , ( 2 ) supervised and unsupervised sentence ranking models utilizing multi - view representations comprising of query - aware attention - based sentence representation ( QAR ) , bag - of - words ( BoW ) and TF - IDF . Our best systems achieved 1st rank and scored 0.86 mAP and 0.58 macro average accuracy in Task - 1 and Task - 2 respectively .", "entities": [[17, 18, "DatasetName", "OST"], [127, 128, "MethodName", "SVM"], [199, 200, "MetricName", "mAP"], [203, 205, "MetricName", "average accuracy"]]}
{"text": "RDoc - IR Task - 1 : The task aims at retrieving and ranking the PubMed abstracts ( within each of the eight clusters ) that are relevant for the RDoC construct ( i.e , a query ) related to the cluster in the abstract appears . The training data consists of abstracts ( title + sentences ) each annotated with one or more RDoC constructs . Test data consists of abstracts without annotation and the goal is to submit a ranked lists of relevant articles for each medical domain RDoC construct . RDoc - IE Task - 2 The task aims at extracting the most relevant sentence from each PubMed abstract for the corresponding RDoC construct . The input consists of an abstract ( title t and sentences s ) for an RDoC construct q. The training data consists of abstracts each annotated with one RDoC construct and the most relevant sentence . Test data contains abstracts relevant for RDoC constructs and the goal is to submit a list of predicted most relevant sentence for each abstract . Our Contributions : Following are our multifold contributions in this paper : ( 1 ) RDoC - IR Task - 1 : We perform document ( or abstract ) ranking in two steps , first using supervised neural topic model and SVM . Moreover , we have introduced attentions in supervised neural topic model , along with pre - trained word embeddings from several sources . Then , we re - rank documents using BM25 and similarity scores between query and query - aware attention - based document representation . Comparing with other participating systems in the shared task , our submission is ranked 1 st with a mAP score of 0.86 . ( 2 ) RDoC - IE Task - 2 : We have addressed the sentence ranking task by introducing unsupervised and supervised sentence ranking schemes . Moreover , we have employed multi - view representations consisting of bag - of - words , TF - IDF and query - aware attention - based sentence representation via enhanced query - sentence interactions . We have also investigated relevance of title with the sentences and coined ways to incorporate both query - sentence and title - sentence relevance scores in ranking sentences with an abstract . Comparing with other participating systems in the shared task , our submission is ranked 1 st with a macro average accuracy of 0.58 . Our code is available at https://github.com/ YatinChaudhary / RDoC_Task .", "entities": [[88, 90, "DatasetName", "medical domain"], [220, 221, "MethodName", "SVM"], [239, 241, "TaskName", "word embeddings"], [287, 288, "MetricName", "mAP"], [405, 407, "MetricName", "average accuracy"]]}
{"text": "In this paper , we deal with texts of different lengths in form of query , sentence and document . In this section , we describe the way we represent the different texts . Bag - of - words ( BoW ) and Term frequencyinverse document frequency ( TF - IDF ) : We use two the local representation schemes : BoW and TF - IDF ( Manning et al , 2008 ) to compute sentence / document vectors . \ua788 v 2 v 1 v 2 v i \ua788 v D h ( v ) ... h e ( v ) h 2 ( v < 2 ) h i ( v < i ) U v ... \u03b1 D \u03b1 2 \u03b1 1 V i = p ( V i | v < i ) \ua788 . Embedding Sum Representation ( ESR ) : Word embeddings ( Mikolov et al , 2013 ; Pennington et al , 2014 ) have been successfully used in computing distributed representation of text snippets ( short or long ) . In ESR scheme , we employ the pre - trained word embeddings from FastText ( Bojanowski et al , 2017 ) and word2vec ( Mikolov et al , 2013 ) . To represent a text ( query , sentence or document ) , we compute the sum of ( pre - trained ) word vectors of each word in the text . E.g. , ESR for a document d with D words can be computed as : ESR ( d ) = d = D i=1 e ( d i ) where , e R E is the pre - trained embedding vector of dimension E for the word d i . Query - aware Attention - based Representation ( QAR ) for Documents and Sentences : Unlike ESR , we reward the maximum matches between a query and document by computing density of matches between them , similar to McDonald et al ( 2018 ) . In doing so , we introduce a weighted sum of word vectors from pre - trained embeddings and therefore , incorporate importance / attention of certain words in document ( or sentence ) that appear in the query text . For an enhanced query - aware attention based document ( or sentence ) representation , we first compute an histogram a i ( d ) R D of attention weights for each word k in the document d ( or sentence s ) relative to the ith query word q i , using cosine similarity : a i ( d ) = [ a i , k ] D k=1 where , a i , k = e ( q i ) T e ( d k ) | | e ( q i ) | | | | e ( d k ) | | for each kth word in the document d. Here , e ( w ) refers to an embedding vector of the word w. We then compute an query - aware attentionbased representation \u03a6 i ( d ) of document d from the viewpoint of ith query word by summing the word vectors of the document , weighted by their attention scores a i ( d ) : \u03a6 i ( d ) = D k=1 a i , k ( d ) e ( d k ) = a i ( d ) [ e ( d k ) ] D k=1 where is an element - wise multiplication operator . Next , we compute density of matches between several words in query and the document by summing each of the attention histograms a i for all the query terms i. Therefore , the query - aware document representation for a document ( or sentence ) relative to all query words in q is given by : QAR ( d ) = \u03a6 q ( d ) = | q | i \u03a6 i ( d ) ( 1 ) Similarly , a query - aware sentence representation \u03a6 q ( s ) and query - aware title representation \u03a6 q ( t ) can be computed for the sentence s and document title t , respectively . For query representation , we use ESR scheme as q = | q | i=1 e ( w i ) . Figure 2 illustrates the computation of queryaware attention - based sentence representation .", "entities": [[119, 120, "HyperparameterName", "\u03b1"], [121, 122, "HyperparameterName", "\u03b1"], [123, 124, "HyperparameterName", "\u03b1"], [146, 148, "TaskName", "Word embeddings"], [188, 190, "TaskName", "word embeddings"], [191, 192, "MethodName", "FastText"], [450, 452, "HyperparameterName", "k ="], [483, 484, "DatasetName", "kth"]]}
{"text": "Topic models ( TMs ) ( Blei et al , 2003 ) have shown to capture thematic structures , i.e. , topics appearing within the document collection . Beyond interpretability , topic models can extract latent document representation that is used to perform document retrieval . Recently , Gupta et al ( 2019a ) and Gupta et al ( 2019b ) have shown that the neural network - based topic models ( NTM ) outperform LDA - based topic models ( Blei et al , 2003 ; Srivastava and Sutton , 2017 ) in terms of generalization , interpretability and document retrieval . In order to perform document classification and retrieval , we have employed supervised version of neural topic model with extra features and further introduced word - level attention in a neural topic model , i.e. in DocNADE ( Larochelle and Lauly , 2012 ; Gupta et al , 2019a ) . Supervised NTM ( SupDocNADE ) : Document Neural Autoregressive Distribution Estimator ( DocNADE ) is a neural network based topic model that works on bag - of - words ( BoW ) representation to model a document collection in a language modeling fashion . Consider a document d , represented as v = [ v 1 , ... , v i , ... , v D ] of size D , where v i { 1 , ... , Z } is the index of ith word in the vocabulary and Z is the vocabulary size . DocNADE models the joint distribution p ( v ) of document v by decomposing p ( v ) into autoregressive conditional of each word v i in the document , i.e. , p ( v ) = D i=1 p ( v i | v < i ) , where v < i { v 1 , ... , v i\u22121 } . As shown in Figure 1 ( left ) , DocNADE computes each autoregressive conditional p ( v i | v < i ) using a feed forward neural network for i { 1 , ... , D } as , p ( v i = w | v < i ) = exp ( b w + U w , : h ( v < i ) ) w exp ( b w + U w , : h ( v < i ) ) h i ( v < i ) = f ( c + j < i W : , v j ) where , f ( ) is a non - linear activation function , W R H\u00d7Z and U R Z\u00d7H are encoding and decoding matrices , c R H and b R Z are encoding and decoding biases , H is the number of units in latent representation h i ( v < i ) . Here , h i ( v < i ) contains information of words preceding the word v i . For a document v , the log - likelihood L ( v ) and latent representation h ( v ) are given as , L unsup ( v ) = D i=1 log p ( v i | v < i ) ( 2 ) h ( v ) = f ( c + D i=1 W : , v i ) ( 3 ) Here , L ( v ) is used to optimize the topic model in unsupervised fashion and h ( v ) encodes the topic proportion . See Gupta et al ( 2019a ) for further details on training unsupervised DocNADE . Here , we extend the unsupervised version to DocNADE with a hybrid cost L hybrid ( v ) , consisting of a ( supervised ) discriminative training cost p ( y = q | v ) along with an unsupervised generative cost p ( v ) for a given query q and associated document v : L hybrid ( v ) = L sup ( v ) + \u03bb L unsup ( v ) ( 4 ) where \u03bb [ 0 , 1 ] . The supervised cost is given by : L sup ( v ) = p ( y = q | v ) = softmax ( d + S h ( v ) ) Here , S R L\u00d7H and d R L are output matrix and bias , L is the total number of unique RDoC constructs ( i.e. , unique query labels ) . Supervised Attention - based NTM ( a - SupDocNADE ) : Observe in equation 3 that the DocNADE computes document representation h ( v ) via aggregation of word embedding vectors without considering attention over certain words . However , certain content words own high important , especially in classification task . Therefore , we have introduced attention - based embedding aggregation in supDocNADE ( Figure 1 , left ) : h ( v ) = f ( c + D i=1 \u03b1 i W : , v i ) ( 5 ) Here , \u03b1 i is an attention score of each word i in the document v , learned via supervised training . Additionally , we incorporate extra word features , such as pre - trained word embeddings from several sources : FastText ( E f ast ) ( Bojanowski et al , 2017 ) and word2vec ( E word2vec ) ( Mikolov et al , 2013 ) . We introduce these features by concatenating h e ( v ) with h ( v ) in the supervised portion of the a - supDocNADE model , as Therefore , the classification portion of a - supDocNADE with additional features is given by : h e ( v ) = f c + D i=1 \u03b1 i ( E f ast : , v i + E word2vec : , v i ) ( 6 p ( q | v ) = softmax ( d + S concat ( h ( v ) , h e ( v ) ) ) where , S R H \u00d7L and H = H + E f ast + E word2vec .", "entities": [[0, 2, "TaskName", "Topic models"], [31, 33, "TaskName", "topic models"], [69, 71, "TaskName", "topic models"], [75, 76, "MethodName", "LDA"], [78, 80, "TaskName", "topic models"], [107, 109, "TaskName", "document classification"], [430, 432, "HyperparameterName", "activation function"], [462, 465, "HyperparameterName", "number of units"], [501, 504, "MetricName", "log - likelihood"], [681, 682, "DatasetName", "0"], [708, 709, "MethodName", "softmax"], [832, 833, "HyperparameterName", "\u03b1"], [845, 846, "HyperparameterName", "\u03b1"], [878, 880, "TaskName", "word embeddings"], [884, 885, "MethodName", "FastText"], [966, 967, "HyperparameterName", "\u03b1"], [993, 994, "MethodName", "softmax"]]}
{"text": "BM25 : A ranking function proposed by Robertson and Zaragoza ( 2009 ) is used to estimate the relevance of a document for a given query . BM25 - Extra : The relevance score of BM - 25 is combined with four extra features : ( 1 ) percentage of query words with exact match in the document , ( 2 ) percentage of query words bigrams matched in the document , ( 3 ) IDF weighted document vector for feature # 1 , and ( 4 ) IDF weighted document vector for feature # 2 . Therefore , BM25 - Extra returns a vector of 5 scores .", "entities": [[53, 55, "MetricName", "exact match"]]}
{"text": "RDoC Task - 1 aims at retrieving and ranking of PubMed abstracts ( title and content ) that are relevant for 8 RDoC constructs . Participants are provided with 8 clusters , each with a RDoC construct label and required to rank abstracts within each cluster based on their relevance to the corresponding cluster label . Each cluster contains abstracts relevant to its RDoC construct , while some ( or most ) of the abstracts are noisy in the sense that they belong to a different RDoC construct . Ideally , the participants are required to rank abstracts in each of the clusters by determining their relevance with the RDoC construct of the cluster in which they appear . To address the RDoc Task - 1 , we learn a mapping function between latent representation h ( v ) of a document ( i.e .. , abstract ) v and its RDoC construct , i.e. , query words q in a supervised fashion . In doing so , we have employed supervised classifiers , especially supervised neural topic model a - supDocNADE ( section 3.2 ) for document ranking . We treat q as label and maximize p ( q | v ) leading to maximize L hybrid ( v ) in a - supDocNADE model . As demonstrated in Figure 1 ( right ) , we perform document ranking in two steps : ( 1 ) Document Relevance Ranking : We build a supervised classifier using all the training documents and their corresponding labels ( RDoC constructs ) , provided with the training set . At the test time , we compute prediction probability score p ( CID = q | v test ( CID ) ) ) of the label = CID for each test document v test ( CID ) in the cluster , CID . This prediction probability ( or confidence score ) is treated as a relevance score of the document for the RDoC construct of the cluster . Figure 1 ( right ) shows that we perform document ranking using the probability scores ( col - 2 ) of the RDoC construct ( e.g. loss ) within the cluster C1 . Observe that a test document with least confidence for a cluster are ranked lower within the cluster and thus , improving mean average precision ( mAP ) . Additionally , we also show the predicted RDoC construct in col - 1 by the supervised classifier . ( 2 ) Document Relevance Re - ranking : Secondly , we re - ranked each document v ( ti - tle+abstract ) within each cluster ( with label q ) using unsupervised ranking , where the relevance scores are computed as : ( a ) reRank ( BM25 - Extra ) : sum each of the 5 relevance scores to get the final relevance , and ( b ) reRank ( QAR ) : cosine - similarity ( QAR ( v ) , q ) .", "entities": [[187, 189, "TaskName", "document ranking"], [228, 230, "TaskName", "document ranking"], [278, 279, "DatasetName", "CID"], [285, 286, "DatasetName", "CID"], [293, 294, "DatasetName", "CID"], [301, 302, "DatasetName", "CID"], [307, 308, "DatasetName", "CID"], [343, 345, "TaskName", "document ranking"], [360, 361, "MetricName", "loss"], [389, 391, "MetricName", "average precision"], [392, 393, "MetricName", "mAP"]]}
{"text": "Beyond unsupervised ranking , we further investigate sentence ranking in supervised paradigm by introducing a distance metric between the query ( or title ) and sentence vectors . Figure 2 describes the computation of relevance score for a sentence s j using a supervised sentence ranker scheme . Like the unsupervised ranker ( section 3.5.1 ) , the supervised ranker also employs vector representations : \u03a6 q ( s j ) , t and q. Using the projection matrix G , we then apply a projection to each of the representation to obtain \u03a6 p q ( s j ) , t p and q p . Here , the operator \u2297 performs concatenation of the projected vector with its input via residual connection . Next , we apply a Manhattan distance metric to compute similarity ( or relevance ) scores , following : r sup = exp \u2212 | | ( \u03a6 p q ( s j ) , q p ) + \u03b2 ( \u03a6 p q ( s j ) , t p ) | | 2 where \u03b2 [ 0 , 1 ] controls the relevance of title , determined by cross - validation . A final relevance score r sup f [ 0 , 1 ] is computed by feeding a vector [ r sup , r sup siamese , BM25 - extra ] into a supervised linear regression , which is trained end - to - end by minimizing mean squared error between the r sup f and { 0 , 1 } , i.e. , 1 when the sentence s j is relevant to query q. Here , r sup siamese refers to a relevance Best mAP score for each model is marked in bold . ( reRank # 1 : \" reRank ( BM25 - Extra ) \" ; reRank # 2 : \" reRank ( QAR ) \" ; reRank # 3 : \" reRank ( BM25 - Extra ) + reRank ( QAR ) \" ) score computed between q and s j via Siamese - LSTM . To perform sentence ranking within an abstract for a given RDoC construct q , the relevance score r sup f ( or r unsup f ) is computed for all the sentences and a sentence with the highest score is extracted .", "entities": [[15, 17, "HyperparameterName", "distance metric"], [122, 124, "MethodName", "residual connection"], [131, 133, "HyperparameterName", "distance metric"], [164, 165, "HyperparameterName", "\u03b2"], [181, 182, "HyperparameterName", "\u03b2"], [183, 184, "DatasetName", "0"], [207, 208, "DatasetName", "0"], [232, 234, "MethodName", "linear regression"], [255, 256, "DatasetName", "0"], [283, 284, "MetricName", "mAP"], [346, 347, "MethodName", "LSTM"]]}
{"text": "Dataset Description : Dataset for RDoC Tasks contains a total of 266 PubMed abstracts labelled with 8 RDoC constructs in a single label fashion . Number of abstracts for each RDoC construct is described in Table 2 , where first row describes the statistics for all abstracts and second & third row shows the split of those abstracts into training and development sets maintaining a 80 - 20 ratio for each RDoC construct . For Task - 1 , each PubMed abstract contains its associated title , PubMed ID ( PMID ) and label ( RDoC construct ) . In addition for Task - 2 , each PubMed abstract also contains a list of most relevant sentences from that abstract . Final evaluation test data for Task - 1 & Task - 2 contains 999 & 244 abstracts respectively . We use \" RegexpTokenizer \" from scikit - learn to tokenize abstracts and lower - cased all tokens . After this , we remove those tokens which occur in less than 3 abstracts and also remove stopwords ( using nltk ) . For computing BM25 - Extra relevance score , we use unprocessed raw text of sentences and titles . Experimental Setup : As the training dataset labelled with RDoC constructs is very small , we use an external source of semantical knowledge by incorporating pretrained distributional word embeddings ( Zhang et al , 2019 ) from FastText model ( Bojanowski et al , 2017 ) trained on the entire corpus of PubMed and MIMIC III Clinical notes ( Johnson et al , 2016 ) . Similarly , we also use pretrained word embeddings ( Moen and Ananiadou , 2013 ) from word2vec model ( Mikolov et al , 2013 ) trained on PubMed and PMC abstracts . We create 3 folds * of train / dev splits for cross - validation . RDoC Task - 1 : For DocNADE topic model , we use latent representation of size 50 . We use pretrained FastText embeddings of size 300 and pretrained word2vec embeddings of size 200 . For SVM , we use Bag - of - words ( BoW ) representation of abstracts with radial basis kernel function . PubMed abstracts are provided in eight different clusters , one for each RDoC construct , for final test set evaluation . RDoC Task - 2 : We use pretrained FastText embeddings to compute query - aware sentence representation of a sentence ( \u03a6 q ( s j ) ) , title ( t ) and query ( q ) representations . We also train Replicated - Siamese - LSTM model with input as sentence and query pair i.e. , ( s j , q ) and label as 1 if s j is relevant otherwise 0 . We use \u03b2 { 0 , 1 } .", "entities": [[227, 229, "TaskName", "word embeddings"], [237, 238, "MethodName", "FastText"], [272, 274, "TaskName", "word embeddings"], [334, 335, "MethodName", "FastText"], [348, 349, "MethodName", "SVM"], [398, 399, "MethodName", "FastText"], [437, 438, "MethodName", "LSTM"], [464, 465, "DatasetName", "0"], [468, 469, "HyperparameterName", "\u03b2"], [470, 471, "DatasetName", "0"]]}
{"text": "Table 3 shows the performance of supervised Document Ranker models i.e , a - supDocNADE and SVM , for Task - 1 . SVM achieves a classification accuracy of 0.947 and mean average precision . It shows that an intruder / noisy abstract ( Gold Label : Loss ) is assigned higher probability than the abstracts with same Gold Label as the cluster . But , using re - ranking with BM25 - Extra ( reRank ( BM25 - Extra ) ) relevance score assigns lowest relevance to the intruder abstract . ( mAP ) of 0.992 by ranking the abstracts in their respective clusters using the supervised prediction probabilities ( p ( q | v ) ) . After that , we use three different relevance scores : ( 1 ) reRank ( BM25 - Extra ) , ( 2 ) reRank ( QAR ) and ( 3 ) reRank ( BM25 - Extra ) + reRank ( QAR ) , for re - ranking of the abstracts in their respective clusters . It is to be noted that the ranking mAP of the clusters using prediction probabilities is already the best possible i.e. , the intruder abstracts ( abstracts with different label ( RDoC construct ) than the cluster label ) are at the bottom of the ranked clusters . Therefore , re - ranking of these clusters would not achieve a better score . Similarly , we train a - supDocNADE model with three different settings : ( 1 ) random weight initialization , ( 2 ) incorporating FastText embeddings ( h e ( v ) ) and ( 3 ) incorporating Fast - Text and word2vec embeddings ( h e ( v ) ) . By using the pretrained embeddings , the classification accuracy increases from 0.912 to 0.965 , this shows that distributional pretrained embeddings carry significant semantic knowledge . Furthermore , re - ranking using reRank ( BM25 - Extra ) and reRank ( QAR ) further results in the improvement of mAP score ( 0.994 vs 0.983 ) by shifting the intruder documents at the bottom of each impure cluster .", "entities": [[16, 17, "MethodName", "SVM"], [23, 24, "MethodName", "SVM"], [27, 28, "MetricName", "accuracy"], [32, 34, "MetricName", "average precision"], [93, 94, "MetricName", "mAP"], [182, 183, "MetricName", "mAP"], [261, 262, "MethodName", "FastText"], [297, 298, "MetricName", "accuracy"], [338, 339, "MetricName", "mAP"]]}
{"text": "Table 4 shows an impure \" Potential Threat Anxiety \" cluster of abstracts containing an intruder abstract with label ( RDoC construct ) \" Loss \" . When this cluster is ranked on the basis of predic - tion probabilities ( p ( q | v ) ) , then \" Loss \" abstract is ranked third from the bottom and it degrades the mAP score of the retrieval system . But after re - ranking this cluster using reRank ( BM25 - Extra ) relevance score , the \" Loss \" abstract is ranked at the bottom , thus maximizing the mAP score . Therefore , re - ranking with BM25 - Extra on top of ranking with p ( q | v ) is , evidently , a robust abstract / document ranking technique .", "entities": [[64, 65, "MetricName", "mAP"], [102, 103, "MetricName", "mAP"], [133, 135, "TaskName", "document ranking"]]}
{"text": "Table 5 shows results for Task - 2 using three unsupervised and two supervised sentence ranker models . For unsupervised model , using reRank ( BM25 - Extra ) relevance score between a query ( q ) , label ( RDoC construct ) of an abstract , and all the sentences ( s j ) in an abstract , we get an macroaverage accuracy ( MAA ) of 0.631 . However , using version1 and version2 models ( see Fig 2 ) , we achieve a MAA score of 0.701 and 0.526 respectively . Higher accuracy of version1 model suggests that title ( t ) of an abstract also contains the essential information regarding the most relevant sentence . For supervised model , we get an MAA score of 0.772 and 0.737 by setting \u03b2 = 0 & 1 in supervised relevance score ( r sup f ) equation in section 3.5.2 . Hence , for supervised sentence ranker model , title ( t ) is playing a negative influence in correctly identifying the relevance ( r sup f ) of different sentences . Furthermore , we combine the knowledge of unsupervised and supervised sentence rankers by creating multiple ensembles ( majority voting ) of the predictions from different models . We achieve the highest MAA score of 0.789 by combining the predictions of ( 1 ) reRank ( BM25 - Extra ) , ( 2 ) version1 , and ( 3 ) r sup f with \u03b2 = 0 . Notice that all the proposed supervised and unsupervised sentence ranking mod - - - We found that nurses experience a grieving process similar to those directly suffering from perinatal loss .", "entities": [[63, 64, "MetricName", "accuracy"], [95, 96, "MetricName", "accuracy"], [134, 135, "HyperparameterName", "\u03b2"], [136, 137, "DatasetName", "0"], [247, 248, "HyperparameterName", "\u03b2"], [249, 250, "DatasetName", "0"], [280, 281, "MetricName", "loss"]]}
{"text": "Table 6 shows the final evaluation scores of different competing systems for both the RDoC Task - 1 & Task - 2 on final test set . Observe that our submission ( MIC - CIS ) scored a mAP score of 0.86 and MAA of 0.58 in Task - 1 and Task - 2 , respectively . Notice that we outperform the second best system by 20.83 % ( 0.58 vs 0.48 ) margin in Task2 .", "entities": [[38, 39, "MetricName", "mAP"]]}
{"text": "In conclusion , both supervised neural topic model and SVM can effectively perform ranking of PubMed abstracts in a given cluster based on the prediction probabilities . However , a further reranking using BM25 - Extra or query - aware sentence representation ( QAR ) has proven to maximize the mAP score by correctly assigning the lowest relevance score to the intruder abstracts . Also , unsupervised and supervised sentence ranker models using query - title - sentence interactions outperform the traditional BM25 - Extra based ranking model by a significant margin . In future , we would like to introduce complementary feature representation via hidden vectors of LSTM jointly with topic models and would like to further investigate the interpretability ( Gupta et al , 2015 ; of the proposed neural ranking models in the sense that one can extract salient patterns determining relationship between query and text . Another promising direction would be introduce abstract information , such as part - of - speech and named entity tags ( Lample et al , 2016 ; Gupta et al , 2016 ) to augment information retrieval ( IR ) .", "entities": [[9, 10, "MethodName", "SVM"], [50, 51, "MetricName", "mAP"], [108, 109, "MethodName", "LSTM"], [111, 113, "TaskName", "topic models"], [161, 164, "DatasetName", "part - of"], [185, 187, "TaskName", "information retrieval"]]}
{"text": "In this section we compare HCNs to existing approaches on the public \" bAbI dialog \" dataset ( Bordes and Weston , 2016 ) . This dataset includes two end - to - end dialog learning tasks , in the restaurant domain , called task5 and task6 . 2 Task5 consists of synthetic , simulated dialog data , with highly regular user behavior and constrained vocabulary . Dialogs include a database access action which retrieves relevant restaurants from a database , with results included in the dialog transcript . We test on the \" OOV \" variant of Task5 , which includes entity values not observed in the training set . Task6 draws on human - computer dialog data from the second dialog state tracking challenge ( DSTC2 ) , where usability subjects ( crowd - workers ) interacted with several variants of a spoken dialog system ( Henderson et al , 2014a ) . Since the database from DSTC2 was not provided , database calls have been inferred from the data and inserted into the dialog transcript . Example dialogs are provided in the Appendix Sections A.2 and A.3 . To apply HCNs , we wrote simple domain - specific software , as follows . First , for entity extraction ( step 4 in Figure 1 ) , we used a simple string match , with a pre - defined list of entity names - i.e. , the list of restaurants available in the database . Second , in the context update ( step 5 ) , we wrote simple logic for tracking entities : when an entity is recognized in the user input , it is retained by the software , over - writing any previously stored value . For example , if the price \" cheap \" is recognized in the first turn , it is retained as price = cheap . If \" expensive \" is then recognized in the third turn , it over - writes \" cheap \" so the code now holds price = expensive . Third , system actions were templatized : for example , system actions of the form \" prezzo is a nice restaurant in the west of town in the moderate price range \" all map to the template \" < name > is a nice restaurant in the < location > of town in the < price > price range \" . This results in 16 templates for Task5 and 58 for Task6 . 3 Fourth , when database results are received into the entity state , they are sorted by rating . Finally , an action mask was created which encoded common - sense dependencies . These are implemented as simple if - then rules based on the presence of entity values : for example , only allow an API call if pre - conditions are met ; only offer a restaurant if database results have already been received ; do not ask for an entity if it is already known ; etc . For Task6 , we noticed that the system can say that no restaurants match the current query without consulting the database ( for an example dialog , see Section A.3 in the Appendix ) . In a practical system this information would be retrieved from the database and not encoded in the RNN . So , we mined the training data and built a table of search queries known to yield no results . We also added context features that indicated the state of the database - for example , whether there were any restaurants matching the current query . The complete set of context features is given in Appendix Section A.4 . Altogether this code consisted of about 250 lines of Python . We then trained an HCN on the training set , employing the domain - specific software described above . We selected an LSTM for the recurrent layer ( Hochreiter and Schmidhuber , 1997 ) , with the AdaDelta optimizer ( Zeiler , 2012 ) . We used the development set to tune the number of hid - den units ( 128 ) , and the number of epochs ( 12 ) . Utterance embeddings were formed by averaging word embeddings , using a publicly available 300dimensional word embedding model trained using word2vec on web data ( Mikolov et al , 2013 ) . 4 The word embeddings were static and not updated during LSTM training . In training , each dialog formed one minibatch , and updates were done on full rollouts ( i.e. , non - truncated back propagation through time ) . The training loss was categorical cross - entropy . Further low - level implementation details are in the Appendix Section A.1 . We ran experiments with four variants of our model : with and without the utterance embeddings , and with and without the action mask ( Figure 1 , steps 3 and 6 respectively ) . Following past work , we report average turn accuracy - i.e. , for each turn in each dialog , present the ( true ) history of user and system actions to the network and obtain the network 's prediction as a string of characters . The turn is correct if the string matches the reference exactly , and incorrect if not . We also report dialog accuracy , which indicates if all turns in a dialog are correct . We compare to four past end - to - end approaches ( Bordes and Weston , 2016 ; Liu and Perez , 2016 ; Eric and Manning , 2017 ; Seo et al , 2016 ) . We emphasize that past approaches have applied purely sequence - to - sequence models , or ( as a baseline ) purely programmed rules ( Bordes and Weston , 2016 ) . By contrast , Hybrid Code Networks are a hybrid of hand - coded rules and learned models . Results are shown in Table 1 . Since Task5 is synthetic data generated using rules , it is possible to obtain perfect accuracy using rules ( line 1 ) . The addition of domain knowledge greatly simplifies the learning task and enables HCNs to also attain perfect accuracy . On Task6 , rules alone fare poorly , whereas HCNs outperform past learned models . We next examined learning curves , training with increasing numbers of dialogs . To guard against bias in the ordering of the training set , we averaged over 5 runs , randomly permuting the order of the training dialogs in each run . Results are in Figure 2 . In Task5 , the action mask and utterance embeddings substantially reduce the number of training dialogs required ( note the horizontal axis scale is logarithmic ) . For Task6 , the bene - ( Bordes and Weston , 2016 ) . Results for \" Rules \" taken from Bordes and Weston ( 2016 ) . Note that , unlike cited past work , HCNs make use of domainspecific procedural knowledge . Figure 2 : Training dialog count vs. turn accuracy for bAbI dialog Task5 - OOV and Task6 . \" embed \" indicates whether utterance embeddings were included ; \" mask \" indicates whether the action masking code was active . fits of the utterance embeddings are less clear . An error analysis showed that there are several systematic differences between the training and testing sets . Indeed , DSTC2 intentionally used different dialog policies for the training and test sets , whereas our goal is to mimic the policy in the training set . Nonetheless , these tasks are the best public benchmark we are aware of , and HCNs exceed performance of existing sequence - to - sequence models . In addition , they match performance of past models using an order of magnitude less data ( 200 vs. 1618 dialogs ) , which is crucial in practical settings where collecting realistic dialogs for a new domain can be expensive .", "entities": [[34, 36, "TaskName", "dialog learning"], [653, 654, "MethodName", "LSTM"], [668, 669, "MethodName", "AdaDelta"], [669, 670, "HyperparameterName", "optimizer"], [696, 699, "HyperparameterName", "number of epochs"], [709, 711, "TaskName", "word embeddings"], [736, 738, "TaskName", "word embeddings"], [744, 745, "MethodName", "LSTM"], [777, 778, "MetricName", "loss"], [840, 841, "MetricName", "accuracy"], [898, 899, "MetricName", "accuracy"], [1020, 1021, "MetricName", "accuracy"], [1045, 1046, "MetricName", "accuracy"], [1190, 1191, "MetricName", "accuracy"]]}
{"text": "We now turn to comparing with purely handcrafted approaches . To do this , we obtained logs from our company 's text - based customer support dialog system , which uses a sophisticated rulebased dialog manager . Data from this system is attractive for evaluation because it is used by real customers - not usability subjects - and because its rule - based dialog manager was developed by customer support professionals at our company , and not the authors . This data is not publicly available , but we are unaware of suitable humancomputer dialog data in the public domain which uses rules . Customers start using the dialog system by entering a brief description of their problem , such as \" I need to update my operating system \" . They are then routed to one of several hundred domains , where each domain attempts to resolve a particular problem . In this study , we collected humancomputer transcripts for the high - traffic domains \" reset password \" and \" can not access account \" . We labeled the dialog data as follows . First , we enumerated unique system actions observed in the data . Then , for each dialog , starting from the beginning , we examined each system action , and determined whether it was \" correct \" . Here , correct means that it was the most appropriate action among the set of existing system actions , given the history of that dialog . If multiple actions were arguably appropriate , we broke ties in favor of the existing rule - based dialog manager . Example dialogs are provided in the Appendix Sections A.5 and A.6 . If a system action was labeled as correct , we left it as - is and continued to the next system action . If the system action was not correct , we replaced it with the correct system action , and discarded the rest of the dialog , since we do not know how the user would have replied to this new system action . The resulting dataset contained a mixture of complete and partial dialogs , containing only correct system actions . We partitioned this set into training and test dialogs . Basic statistics of the data are shown in Table 2 . In this domain , no entities were relevant to the control flow , and there was no obvious mask logic since any question could follow any question . Therefore , we wrote no domain - specific software for this instance of the HCN , and relied purely on the recurrent neural network to drive the conversation . The architecture and training of the RNN was the same as in Section 4 , except that here we did not have enough data for a validation set , so we instead trained until we either achieved 100 % accuracy on the training set or reached 200 epochs . To evaluate , we observe that conventional measures like average dialog accuracy unfairly penalize the system used to collect the dialogs - in our case , the rule - based system . If the system used for collection makes an error at turn t , the labeled dialog only includes the sub - dialog up to turn t , and the system being evaluated off - line is only evaluated on that sub - dialog . In other words , in our case , reporting dialog accuracy would favor the HCN because it would be evaluated on fewer turns than the rule - based system . We therefore , where C ( HCN - win ) is the number of test dialogs where the rule - based approach output a wrong action before the HCN ; C ( rule - win ) is the number of test dialogs where the HCN output a wrong action before the rulebased approach ; and C ( all ) is the number of dialogs in the test set . When \u2206P > 0 , there are more dialogs in which HCNs produce longer continuous sequences of correct actions starting from the beginning of the dialog . We run all experiments 5 times , each time shuffling the order of the training set . Results are in Figure 3 . HCNs exceed performance of the existing rule - based system after about 30 dialogs . In these domains , we have a further source of knowledge : the rule - based dialog managers themselves can be used to generate example \" sunnyday \" dialogs , where the user provides purely expected inputs . From each rule - based controller , synthetic dialogs were sampled to cover each expected user response at least once , and added to the set of labeled real dialogs . This resulted in 75 dialogs for the \" Forgot password \" domain , and 325 for the \" Ca n't access account \" domain . Training was repeated as described above . Results are also included in Figure 3 , with the suffix \" sampled \" . In the \" Ca n't access account \" domain , the sampled dialogs yield a large improvement , probably because the flow chart for this domain is large , so the sampled dialogs increase coverage . The gain in the \" forgot password \" domain is present but smaller . In summary , HCNs can out - perform Figure 3 : Training dialogs vs. \u2206P , where \u2206P is the fraction of test dialogs where HCNs produced longer initial correct sequences of system actions than the rules , minus the fraction where rules produced longer initial correct sequences than the HCNs . \" embed \" indicates whether utterance embeddings were included ; \" sampled \" indicates whether dialogs sampled from the rule - based controller were included in the training set . production - grade rule - based systems with a reasonable number of labeled dialogs , and adding synthetic \" sunny - day \" dialogs improves performance further . Moreover , unlike existing pipelined approaches to dialog management that rely on an explicit state tracker , this HCN used no explicit state tracker , highlighting an advantage of the model .", "entities": [[483, 484, "MetricName", "accuracy"], [504, 505, "MetricName", "accuracy"], [579, 580, "MetricName", "accuracy"], [671, 672, "DatasetName", "0"]]}
{"text": "In the previous sections , supervised learning ( SL ) was applied to train the LSTM to mimic dialogs provided by the system developer . Once a system operates at scale , interacting with a large number of users , it is desirable for the system to continue to learn autonomously using reinforcement learning ( RL ) . With RL , each turn receives a measurement of goodness called a reward ; the agent explores different sequences of actions in different situations , and makes adjustments so as to maximize the expected discounted sum of rewards , which is called the return , denoted G. For optimization , we selected a policy gradient approach ( Williams , 1992 ) , which has been successfully applied to dialog systems ( Jur\u010d\u00ed\u010dek et al , 2011 ) , robotics ( Kohl and Stone , 2004 ) , and the board game Go ( Silver et al , 2016 ) . In policy gradient - based RL , a model \u03c0 is parameterized by w and outputs a distribution from which actions are sampled at each timestep . At the end of a trajectory - in our case , dialog - the return G for that trajectory is computed , and the gradients of the probabilities of the actions taken with respect to the model weights are computed . The weights are then adjusted by taking a gradient step proportional to the return : w w+\u03b1 ( t w log \u03c0 ( a t | h t ; w ) ) ( G\u2212b ) ( 1 ) where \u03b1 is a learning rate ; a t is the action taken at timestep t ; h t is the dialog history at time t ; G is the return of the dialog ; x F denotes the Jacobian of F with respect to x ; b is a baseline described below ; and \u03c0 ( a | h ; w ) is the LSTM - i.e. , a stochastic policy which outputs a distribution over a given a dialog history h , parameterized by weights w. The baseline b is an estimate of the average return of the current policy , estimated on the last 100 dialogs using weighted importance sampling . 5 Intuitively , \" better \" dialogs receive a positive gradient step , making the actions selected more likely ; and \" worse \" dialogs receive a negative gradient step , making the actions selected less likely . SL and RL correspond to different methods of updating weights , so both can be applied to the same network . However , there is no guarantee that the optimal RL policy will agree with the SL training set ; therefore , after each RL gradient step , we check whether the updated policy reconstructs the training set . If not , we re - run SL gradient steps on the training set until the model reproduces the training set . Note that this approach allows new training dialogs to be added at any time during RL optimization . We illustrate RL optimization on a simulated dialog task in the name dialing domain . In this system , a contact 's name may have synonyms ( \" Michael \" may also be called \" Mike \" ) , and a contact may have more than one phone number , such as \" work \" or \" mobile \" , which may in turn have synonyms like \" cell \" for \" mobile \" . This domain has a database of names and phone numbers taken from the Microsoft personnel directory , 5 entity typesfirstname , nickname , lastname , phonenumber , and phonetype - and 14 actions , including 2 API call actions . Simple entity logic was coded , which retains the most recent copy of recognized entities . A simple action mask suppresses impossible actions , such as placing a phonecall before a phone number has been retrieved from the database . Example dialogs are provided in Appendix Section A.7 . To perform optimization , we created a simulated user . At the start of a dialog , the simulated user randomly selected a name and phone type , including names and phone types not covered by the dialog system . When speaking , the simulated user can use the canonical name or a nickname ; usually answers questions but can ignore the system ; can provide additional information not requested ; and can give up . The simulated user was parameterized by around 10 probabilities , set by hand . We defined the reward as being 1 for successfully completing the task , and 0 otherwise . A discount of 0.95 was used to incentivize the system to complete dialogs faster rather than slower , yielding return 0 for failed dialogs , and G = 0.95 T \u22121 for successful dialogs , where T is the number of system turns in the dialog . Finally , we created a set of 21 labeled dialogs , which will be used for supervised learning . For the RNN in the HCN , we again used an LSTM with AdaDelta , this time with 32 hidden units . RL policy updates are made after each dialog . Since a simulated user was employed , we did not have real user utterances , and instead relied on context features , omitting bag - of - words and utterance embedding features . We first evaluate RL by randomly initializing an LSTM , and begin RL optimization . After 10 RL updates , we freeze the policy , and run 500 dialogs with the user simulation to measure task completion . We repeat all of this for 100 runs , and report average performance . In addition , we also report results by initializing the LSTM using supervised learning on the training set , consisting of 1 , 2 , 5 , or 10 dialogs sampled randomly from the training set , then running RL as described above . Results are in Figure 4 . Although RL alone can find a good policy , pre - training with just a handful of labeled dialogs improves learning speed dramatically . Additional experiments , not shown for space , found that ablating the action mask slowed training , agreeing with Williams ( 2008 ) . Finally , we conduct a further experiment where we sample 10 training dialogs , then add one to the training set just before RL dialog 0 , 100 , 200 , ... , 900 . Results are shown in Figure 4 . This shows that SL dialogs can be introduced as RL is in progress - i.e. , that it is possible to interleave RL and SL . This is an attractive property for practical systems : if a dialog error is spotted by a developer while RL is in progress , it is natural to add a training dialog to the training set .", "entities": [[15, 16, "MethodName", "LSTM"], [73, 74, "DatasetName", "agent"], [265, 266, "HyperparameterName", "\u03b1"], [268, 270, "HyperparameterName", "learning rate"], [329, 330, "MethodName", "LSTM"], [360, 362, "MetricName", "average return"], [783, 784, "DatasetName", "0"], [806, 807, "DatasetName", "0"], [863, 864, "MethodName", "LSTM"], [865, 866, "MethodName", "AdaDelta"], [924, 925, "MethodName", "LSTM"], [978, 979, "MethodName", "LSTM"], [1091, 1092, "DatasetName", "0"]]}
{"text": "Discourse elements in argumentative essays are sensitive to their positions . For example , introduction mostly comes before thesis or main ideas and main ideas may occur more often at the beginnings or endings of paragraphs . Figure 2 shows an essay with 7 sentences and 4 paragraphs as an example . We consider three types of sentence positions for positional encoding . Global position : The index of a sentence is used to describe its position where we assume sentences in an essay form a sequence . Paragraph position : An essay has multiple paragraphs . The position of the paragraph that contains the sentence is also important . Local position : The position of the sentence in its paragraph is informative as well . We adopt a relative positional encoding approach . We compute the relative positions for the above three position types . For example , the relative global position of the i - th ( i \u2265 1 ) sentence in an essay E is pos global ( i ) = i | E | , ( 4 ) where | E | is the number of sentences . To integrate with sentence representations , we expand pos global ( i ) to a vector of the same dimension d of the distributed sentence representations by duplicating its value to every dimension , noted as pos global ( i ) R d . The relative paragraph position representation pos para ( i ) and relative local position representation pos local ( i ) are computed in the same way . The final position representation pos ( i ) is formulated as a liner combination of the three relative position representations , i.e. , pos ( i ) = t { global , local , para } \u03b2 t pos t ( i ) , ( 5 ) where { \u03b2 t } are parameters to be learnt in training . The element representation of the i - th sentence is e i = tanh ( BiLSTM ( C i + pos ( i ) ) ) . ( 6 )", "entities": [[300, 301, "HyperparameterName", "\u03b2"], [313, 314, "HyperparameterName", "\u03b2"], [339, 340, "MethodName", "BiLSTM"]]}
{"text": "Self - Attention relates elements at different positions by computing attention between every pair of elements . An attention function is to map a query and a set of key - value pairs to an output . The queries Q , keys K and values V are vectors . We define Q , K R d k \u00d7n and d k is the dimension . The attention is computed as \u03b1 = Attn ( Q , K ) = softmax ( QK T \u221a d k ) . ( 7 ) The output is computed as a weighted sum of the values , i.e. , \u03b1V. Here , we are interested in the attention vectors rather than the weighted output , because an attention vector reflects the relatedness of a given sentence to every other sentence . We propose the inter - sentence attention ( ISA ) by applying self - attention to sentence semantic representations C and discourse element representations E = { e i } . Element Self - Attention ( ElemSA ) : ElemSA models relations among discourse elements . We use E to get Q and K , Q = EW Q , K = EW K , where W Q , W K R d\u00d7d k . We do not use the normalized attention vectors as shown in Equation 7to capture relative relatedness . Instead , we use \u03b1 e = tanh ( QK T \u221a d k ) as attention vectors . Content Self - Attention ( ContSA ) : ContSA explores content relatedness to model sentence interactions . Similarly to ElemSA , we use the sentence semantic representations C to compute the ContSA vector \u03b1 c . The parameters are independent from ElemSA . Adaptive Maxpooling Different essays have different number of sentences . To have a fixed - length attention vector , we borrow the idea of spatial pyramid pooling from image processing ( He et al , 2015 ) . It can maintain relatedness information by maxpooling \u03b1 e and \u03b1 c in local bins . These bins have sizes proportional to the number of an essay 's sentences so that the number of bins is fixed regardless of the essay length . We set the number of bins to 1 , 2 , 4 and 8 , respectively . The resulted representations can be seen as descriptions of the relatedness of a sentence to different zones of its essay . These representations are concatenated so that the dimension of the pooled attention vectors \u03b1 c , \u03b1 e is 1 + 2 + 4 + 8=15 . Finally , the prediction is made according to Y = softmax linear ( [ \u03b1 e ; \u03b1 c ; E ] ) , ( 8 ) where \u03b1 c , \u03b1 e and E are concatenated .", "entities": [[18, 20, "HyperparameterName", "attention function"], [70, 71, "HyperparameterName", "\u03b1"], [79, 80, "MethodName", "softmax"], [197, 199, "HyperparameterName", "K ="], [233, 234, "HyperparameterName", "\u03b1"], [281, 282, "HyperparameterName", "\u03b1"], [315, 318, "MethodName", "spatial pyramid pooling"], [336, 337, "HyperparameterName", "\u03b1"], [339, 340, "HyperparameterName", "\u03b1"], [423, 424, "HyperparameterName", "\u03b1"], [426, 427, "HyperparameterName", "\u03b1"], [447, 448, "MethodName", "softmax"], [451, 452, "HyperparameterName", "\u03b1"], [454, 455, "HyperparameterName", "\u03b1"], [465, 466, "HyperparameterName", "\u03b1"], [468, 469, "HyperparameterName", "\u03b1"]]}
{"text": "The thesis express the central claim of an author with respect to the essay 's topic . Main Idea The ideas establish foundational ideas or aspects that are related to the thesis . Evidence The evidence elements provide examples or other evidence that are used to support main ideas and thesis . Elaboration The elaboration elements further explain main ideas or provide reasons , but contain no examples or other evidence . Conclusion The conclusion sentence is the extension of the central argument , summarizes the full text , and echos the thesis of the essay . Other Other elements refer to the ones that do not match the above classes . The dataset has 1 , 230 argumentative essays written by high school students , covering diverse topics . These essays were collected from a website LeleKetang . 1 We asked two annotators from the literal arts college to assign discourse elements to sentences from these essays according to a manual . The annotators discussed to reach a consensus and refined the manual for several rounds . We use one annotator 's annotation as the gold answer , and the other 's annotation as the prediction , and compute the F1 scores to measure the agreement , which is shown in Figure 3 . Table 1 shows the basic statistics of the dataset . The distribution of discourse elements is imbalanced . Elaboration and evidence sentences are [ To conclude , art could play an active role in improving the quality of people 's lives , ] s 1 [ but I think that governments should attach heavier weight to other social issues such as education and housing needs ] s 2 [ because those are the most essential ways enable to make people a decent life . ] s 3 . many more than thesis and main idea sentences . The type of other sentence accounts for a very small percentage of the dataset . The test dataset is 10 % of the whole dataset .", "entities": [[201, 202, "MetricName", "F1"]]}
{"text": "The max length of sentences is set to 40 words . Sentences are padded or truncated according to this length . The Tencent pre - trained word embeddings ( Song et al , 2018 ) were used for experiments on the Chinese dataset . The dimension of word embeddings is 200 . The Bert tokenizer and embeddings were used for experiments on the English dataset . The dimension of all the BiLSTM hidden layers is 256 on Chinese dataset , and 128 on English dataset . So is the dimension of d k . The dimension of the attention vectors is 15 . The optimizer is stochastic gradient descent ( SGD ) with a learning rate 0.1 . The best models were selected for all settings based on the results on the validation data , which is 10 % of the training data . We use accuracy ( Acc . ) and macro - F1 as evaluation metrics .", "entities": [[26, 28, "TaskName", "word embeddings"], [47, 49, "TaskName", "word embeddings"], [71, 72, "MethodName", "BiLSTM"], [104, 105, "HyperparameterName", "optimizer"], [106, 109, "MethodName", "stochastic gradient descent"], [110, 111, "MethodName", "SGD"], [114, 116, "HyperparameterName", "learning rate"], [146, 147, "MetricName", "accuracy"], [148, 149, "MetricName", "Acc"], [152, 155, "MetricName", "macro - F1"]]}
{"text": "Table 4 shows the performance of the baselines and DiSA . We can see that HBiLSTM performs even worse than feature - based approach . HBiL - STM has a low macro - F1 score , indicating that it has difficulties in identifying particular discourse elements . The two end - to - end models do not consider position information and interactions among sentences . The performance of BERT is worse than HBiLSTM . This verifies that sequence modeling is more proper than single sentence classification for this task . DiSA achieves the best performance on all metrics , with a large improvement compared with the baselines . Figure 3 further illustrates system performance on identifying specific discourse elements . The human performance is also measured by considering one annotator 's annotation as the answer , and the other one 's as the prediction . The discourse elements that HBiLSTM is unable to accurately identify are thesis and main idea . Despite their importance for understanding a text , their scale is obviously smaller than other discourse elements , which may bring in obstacles for datadriven approaches . Feature - based method performs better than HBiLSTM on identifying thesis and main idea . But it heavily relies on feature - engineering such as manually collected discourse markers and cue words . It does not perform well on identifying evidence due to the difficulties in designing related features . DiSA is also an end - to - end model the same as HBiLSTM but performs much better . We will discuss the impacts of positional encoding and intersentence attention in Section 6.3.2 and 6.3.3 . Compared with the feature - based method , DiSA has comparable performance on identifying thesis but has superior results on identifying main idea ( 9 % higher in F1 score ) and evidence ( 21 % higher in F1 score ) .", "entities": [[31, 34, "MetricName", "macro - F1"], [68, 69, "MethodName", "BERT"], [84, 86, "TaskName", "sentence classification"], [302, 304, "MetricName", "F1 score"], [312, 314, "MetricName", "F1 score"]]}
{"text": "This part investigates the effect of sentence positional encodings . We compare our relative sentence positional encoding ( relativeSPE ) with two other encoding strategies which are previously used for word sequences . Sinusoidal indicates the sinusoidal positional encoding which is used in Transformer ( Vaswani et al , 2017 ) . PosEmbedding uses a distributed vector to represent an absolute position . The position embeddings are learned during training . Each of the above three strategies is applied for modeling global position , local position and paragraph position , which are then combined according to Equation 5 . Table 5 lists the results of using different SPEs and modeling different positions . RelativeSPE performs best with improvements of 2 - 3 % macro - F1 score compared with Sinusoidal and PosEmbedding . Without SPE , the metrics drop at least 6.2 % compared with using any SPE strategy , and 8.6 % compared with relativeSPE . If we explicitly add only pos global , the results even decrease . Perhaps recurrent neural networks such as LSTM naturally capture sequential positional information . However , encoding paragraph position ( pos para ) and local position ( pos local ) largely improves the performance . This indicates that proper structural positional encodings can exploit richer discourse structures than sequential structures .", "entities": [[43, 44, "MethodName", "Transformer"], [123, 126, "MetricName", "macro - F1"], [176, 177, "MethodName", "LSTM"]]}
{"text": "Table 6 shows the effects of removing intersentence attention ( ISA ) components from DiSA . We can see that both ElemSA and ContSA can make contributions , and ElemSA seems to have a larger effect on macro - F1 score . Removing ISA , the accuracy and the macro - F1 score decreases 1.8 % and 2.2 % . Remind that ISA uses attention vectors as representations rather than the final output \u03b1V in the self - attention module . The result is not good . This indicates that semantic relation among sentences is more important for DEI than the specific meaning of sentences . We further analyze ISA 's impact on specific discourse elements . As shown in Table 7 , ISA affects the identification of the minority discourse element thesis most . It also benefits identifying evidence which is not a minority discourse element . Thesis sentences often relate to other sentences from different essay zones , while evidence sentences mainly provide facts or examples so they often relate to local context in content . ISA helps capture such patterns . The performance on other types also increases with different degrees . Anyway , ISA provides a way to build useful representations by exploiting relations between sentences in the same text without any extra burden .", "entities": [[37, 40, "MetricName", "macro - F1"], [46, 47, "MetricName", "accuracy"], [49, 52, "MetricName", "macro - F1"]]}
{"text": "Table 8 and Table 9 show main experimental results on the English dataset . The second column of Table 8 shows the results on distinguishing four component types ( i.e. , major claim , claim , premise , other ) . DiSA outperforms the baselines with a large margin on both accuracy and macro - F1 . Again , removing SPE leads to a large performance decrease . , where Joint - Best incorporates relation identification as an auxiliary task . The third column of Table 8 shows the comparison to the best results from . DiSA does not perform competitively based on the distributed representation only , because the baseline uses some strong hand - crafted features , especially the component position features , which rely on the correct argument component information . Thus we build a feature vector by incorporating the indicator features and a component position feature : number of preceding and following components in paragraph , out of 8 categories of features introduced in . The vector is concatenated with the distributed representation . This combination obtains improvements , outperforms Single - Best results , and achieves close performance compared with Joint - Best , which considers argumentative relation identification as an auxiliary task . We also attempt to apply the same strategy for the Chinese task . But the improvement is negligible . The reason may be that the indicator phrases used in Chinese essays is much less than in English essays . The English dataset heavily relies on phrases signaling beliefs or argumentative discourse connectors . Table 9 shows the macro - F1 scores of DiSA on identifying specific argument components . Without the ISA module , the identification of major claims and claims would decline by 3 % and 1.4 % absolute F1 score , respectively . This is consistent with the experimental results on the Chinese dataset . As a result , the effectiveness of the SPE and ISA can be verified on both the Chinese and the English datasets .", "entities": [[51, 52, "MetricName", "accuracy"], [53, 56, "MetricName", "macro - F1"], [266, 269, "MetricName", "macro - F1"], [299, 301, "MetricName", "F1 score"]]}
{"text": "Text classification assumes a dataset D = { x i , y i } N i=1 which associates an input text x i to its corresponding class label y i . We will omit the index i when dealing with a single input sample . Let the input sequence of word features ( e.g. , embeddings ) be denoted as x = { w t } T t=1 , where T is the length of the sequence . The sequence of hidden states produced by an encoding function f \u03c6 with learnable parameters \u03c6 is then h = { h t } T t=1 . Formally , f \u03c6 : x ( h , \u03b1 ) , where attention weights\u03b1 = { \u03b1 t } T t=1 indicate a probability distribution over the hidden states ( Zou et al , 2018 ; Yang et al , 2016 ) . Finally , the hidden representations are fed into a function g \u03b8 : ( h , \u03b1 ) \u0177 with learnable parameters \u03b8 and a softmax layer that predicts the probabilities\u0177 over classes : y = Softmax ( W h + b ) , \u03b8 = { W , b } ( 1 ) whereh = h t h\u03b1t h t and Softmax ( z i ) = e z i / j e z j . The parameters \u03c6 and \u03b8 are trained to minimize the cross - entropy loss L task ( \u0177 , y ) between the predicted label\u0177 and the ground - truth label y.", "entities": [[0, 2, "TaskName", "Text classification"], [114, 115, "HyperparameterName", "\u03b1"], [122, 123, "HyperparameterName", "\u03b1"], [160, 161, "HyperparameterName", "\u03b8"], [165, 166, "HyperparameterName", "\u03b1"], [171, 172, "HyperparameterName", "\u03b8"], [174, 175, "MethodName", "softmax"], [185, 186, "MethodName", "Softmax"], [193, 194, "HyperparameterName", "\u03b8"], [211, 212, "MethodName", "Softmax"], [230, 231, "HyperparameterName", "\u03b8"], [239, 240, "MetricName", "loss"]]}
{"text": "Attention can be treated as output variables , so that humans can supervise . Given an input sample x , let \u03b1 and\u03b1 be the attention labels ( provided by human annotators ) and the trained attention weights . Then , the loss for attention supervision is defined as the cross - entropy loss L att ( \u03b1 , \u03b1 ) between\u03b1 and \u03b1 . Finally , the parameters of the text classification network with attention supervision are trained to minimize both loss terms together as follows : L = L task ( \u0177 , y ) + \u00b5 L att ( \u03b1 , \u03b1 ) ( 2 ) where \u00b5 is a preference weight . Requiring humans to explicitly annotate soft labels \u03b1 has been considered unrealistic ( Barrett et al , 2018 ) , and often delegated to implicit signals such as eye gaze . As an alternative to asking humans to annotate , important words for the given sample and class label have been typically annotated as rationale ( Bao et al , 2018 ; Zhao et al , 2018 ) . Formally , given an input sample x and its class label y , let A { 0 , 1 } T be a binary vector of selecting words in x , i.e. , \u2200w t x : A ( w t ) { 0 , 1 } . Then , we convert the attention annotation A into a soft distribution of target attention labels \u03b1 using softmax : \u03b1 t = exp ( \u03bb A ( w t ) ) T t = 1 exp ( \u03bb A ( w t ) ) ( 3 ) where \u03bb is a positive hyper - parameter that controls the variance of scores : when \u03bb increases , the distribution of \u03b1 becomes more skewed , guiding to attend a few of more important words . To illustrate a rationale , when given the aforementioned review sample in Sec . 1 , possible annotations for the negative label are either \" this place is small and crowded but the service is quick \" or \" this place is small and crowded but the service is quick \" , where the underlines indicate the hard selection by human . Then , we can translate them into the sample - level anno - tation A = [ 1 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 ] or A = [ 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 ] .", "entities": [[21, 22, "HyperparameterName", "\u03b1"], [42, 43, "MetricName", "loss"], [53, 54, "MetricName", "loss"], [57, 58, "HyperparameterName", "\u03b1"], [59, 60, "HyperparameterName", "\u03b1"], [63, 64, "HyperparameterName", "\u03b1"], [71, 73, "TaskName", "text classification"], [82, 83, "MetricName", "loss"], [102, 103, "HyperparameterName", "\u03b1"], [104, 105, "HyperparameterName", "\u03b1"], [123, 124, "HyperparameterName", "\u03b1"], [201, 202, "DatasetName", "0"], [228, 229, "DatasetName", "0"], [249, 250, "HyperparameterName", "\u03b1"], [251, 252, "MethodName", "softmax"], [253, 254, "HyperparameterName", "\u03b1"], [303, 304, "HyperparameterName", "\u03b1"], [409, 410, "DatasetName", "0"], [411, 412, "DatasetName", "0"], [413, 414, "DatasetName", "0"], [415, 416, "DatasetName", "0"], [417, 418, "DatasetName", "0"], [423, 424, "DatasetName", "0"], [425, 426, "DatasetName", "0"], [427, 428, "DatasetName", "0"], [429, 430, "DatasetName", "0"], [431, 432, "DatasetName", "0"], [435, 436, "DatasetName", "0"], [437, 438, "DatasetName", "0"], [439, 440, "DatasetName", "0"], [441, 442, "DatasetName", "0"], [443, 444, "DatasetName", "0"]]}
{"text": "Our key idea is to leverage causal signals ( Johansson et al , 2016 ) from human annotation A ( or attention labels \u03b1 ) of an input sample x to its corresponding model prediction\u0177 . More specifically , we test whether two different attentions ( one is original and the other is counterfactual ) on the same input sample x lead to different prediction result\u015d y. If high ( original ) and low ( counterfactual ) attention weights for an word w t yield the same ( or very similar ) prediction , it provides evidence to edit the importance of word w t in A into a lower value . Formally , let\u03b1 and\u1fb1 be the original and counterfactual attention weights , respectively , and let y and\u0233 t be the original prediction and its counterfactual prediction with attention change ( i.e. , from \u03b1 t to\u1fb1 t ) on w t x , respectively . Then , knowing the quantity | \u0177 \u2212\u0233 t | , measured as the individualized treatment effect ( ITE ) , enables measuring how Algorithm 1 SANA Input : Training dataset D , Task - level annotation A Output : Model parameters { \u03c6 , \u03b8 } Initialize attention labels \u03b1 from A Using Eq ( 3 ) { \u03c6 , \u03b8 } argmin \u03c6 , \u03b8 L ( D , \u03b1 ; \u03c6 , \u03b8 ) Using Eq ( 2 ) for z = 1 to z max do for each ( x , y ) D do h , \u03b1 f \u03c6 ( x ) y g \u03b8 ( h , \u03b1 ) for each w t x do if A ( w t ) > 0 then \u03b1 Counterfactuals ( \u03b1 , w t ) y t g \u03b8 ( h , \u1fb1 ) if T V D ( \u0177 , \u0233 t ) < then A ( w t ) \u03b3 A ( w t ) end end end end \u03bb \u03b3 \u22121 \u03bb In Eq ( 3 ) Update attention labels \u03b1 from A Using Eq ( 3 ) { \u03c6 , \u03b8 } argmin \u03c6 , \u03b8 L ( D , \u03b1 ; \u03c6 , \u03b8 ) Using Eq ( 2 ) end return { \u03c6 , \u03b8 } much the word w t contributes to the original prediction via attention mechanism . For this measurement , we adopt the Total Variance Distance ( Jain and Wallace , 2019 ) between the two predictions , which is defined as follows : T V D ( \u0177 , \u0233 t ) = 1 2 C c=1 | \u0177 c \u2212\u0233 c t | ( 4 ) where c is the class index . If T V D value is too low , we can give a penalty by decaying the human annotation A ( w t ) with a factor of \u03b3 , which we empirically set as 0.5 , to update the attention labels .", "entities": [[23, 24, "HyperparameterName", "\u03b1"], [146, 147, "HyperparameterName", "\u03b1"], [203, 204, "HyperparameterName", "\u03b8"], [208, 209, "HyperparameterName", "\u03b1"], [219, 220, "HyperparameterName", "\u03b8"], [224, 225, "HyperparameterName", "\u03b8"], [229, 230, "HyperparameterName", "\u03b1"], [233, 234, "HyperparameterName", "\u03b8"], [259, 260, "HyperparameterName", "\u03b1"], [267, 268, "HyperparameterName", "\u03b8"], [271, 272, "HyperparameterName", "\u03b1"], [286, 287, "DatasetName", "0"], [288, 289, "HyperparameterName", "\u03b1"], [291, 292, "HyperparameterName", "\u03b1"], [299, 300, "HyperparameterName", "\u03b8"], [322, 323, "HyperparameterName", "\u03b3"], [333, 334, "HyperparameterName", "\u03b3"], [344, 345, "HyperparameterName", "\u03b1"], [355, 356, "HyperparameterName", "\u03b8"], [360, 361, "HyperparameterName", "\u03b8"], [365, 366, "HyperparameterName", "\u03b1"], [369, 370, "HyperparameterName", "\u03b8"], [381, 382, "HyperparameterName", "\u03b8"], [484, 485, "HyperparameterName", "\u03b3"]]}
{"text": "Based on TVD , we propose a simple yet effective approach , Sample - level AttentioN Adaptation ( SANA ) , to derive the sample - level machine attention from the task - level human annotation . As described in Alg . 1 , SANA starts with the classification model trained with the initial attention labels \u03b1 . Based on \u03c6 and \u03b8 , we run the classification inference several times for an input sample : one for obtaining the original attention weights\u03b1 and the others for counterfactual attention weights \u03b1 . More specifically , we first store the hidden representations h and the attention weights\u03b1 from f \u03c6 , and the original prediction\u0177 . Then , for each word w t , Counterfactuals returns the counterfactual attention weights\u1fb1 , by 1 ) copying\u03b1 but 2 ) assigning zero to the t - th dimension and 3 ) renormalizing as probability distribution , and we obtain its corresponding prediction result\u0233 t by re - using h. Note that , since the hidden representation at time step t contextualizes a word w t with surrounding words , we adopt perturbing only single words in SANA , not multiple words at the same time , also enjoying the computational advantage . Finally , based on\u0177 and\u0233 t , as defined in Eq ( 4 ) , we compute T V D and update the human annotation A by threshold and decay ratio \u03b3 . Once an iteration 1 is completed over the whole training corpus , we re - train the network with the updated attention annotation and labels . For the stable update , we observe that increasing the coefficient \u03bb in Eq ( 3 ) is crucial , as T V D is not an optimal metric , preventing \u03b1 from being flattened .", "entities": [[56, 57, "HyperparameterName", "\u03b1"], [62, 63, "HyperparameterName", "\u03b8"], [90, 91, "HyperparameterName", "\u03b1"], [240, 241, "HyperparameterName", "\u03b3"], [299, 300, "HyperparameterName", "\u03b1"]]}
{"text": "In an extreme scenario without any human annotator and public resources , inspired by self knowledge distillation ( Furlanello et al , 2018 ) , we report results for using the attention weights of the unsupervised model as a supervision . Note , however , this is highly unlikely in practice , but reported as a lower bound accuracy , when unsupervised attention noise is propagated through distillation supervision . Using SANA is even more critical in this noisy annotation scenario , to denoise attention supervision from counterfactual reasoning , which we empirically analyze this in the subsequent section . 1 O ( | D | T ) , where T is the maximum sequence length 4 Experiment Setup", "entities": [[15, 17, "MethodName", "knowledge distillation"], [58, 59, "MetricName", "accuracy"]]}
{"text": "For all datasets , we use skip - gram ( Mikolov et al , 2013 ) ( official GoogleNews - vectors - negative300 ) word embeddings with 300 dimensions . We use 1layered GRU for each direction with hidden size of 150 for both SST2 and IMDB , and 300 for 20NG dataset , with g \u03b8 of 300 dimension with 0.5 dropout rate . For attention mechanism , the size of trainable context vector is set to 100 for SST2 and 300 for IMDB and 20NG . For attention supervision , we use the balancing coefficient \u00b5 = 1.0 for SST2 and IMDB , and \u00b5 = 2.0 for 20NG . Contrary to Zou et al ( 2018 ) , we observe a larger \u00b5 is more effective for the smaller dataset . We set the contrasting coefficient \u03bb = 3 except \u03bb = 5 for 20NG dataset . In Alg . 1 , we use decay ratio \u03b3 = 2.0 and TVD threshold = 0.3 . In our experiments , the decay ratio is not significantly correlated with the final accuracy , but correlated more with the convergence period . Setting \u03b3 = 2.0 leads to the reported performance within z max = 5 . For BERT , we train BERT - base architecture with a batch size of 4 over 3 epochs . We used Adam with a learning rate of 6.25e - 5 and PiecewiseLinear scheduler . All parameters are optimized until convergence , using Adam optimizer of learning rate 0.001 . The learning parameters were chosen by the best performance on the validation set . In Alg . 1 , the models are additionally fine - tuned over 10 epochs for each iteration . Note that learning time longer than our setting does not contribute to improving the model accuracy .", "entities": [[24, 26, "TaskName", "word embeddings"], [33, 34, "MethodName", "GRU"], [44, 45, "DatasetName", "SST2"], [46, 47, "DatasetName", "IMDB"], [56, 57, "HyperparameterName", "\u03b8"], [80, 81, "DatasetName", "SST2"], [84, 85, "DatasetName", "IMDB"], [101, 102, "DatasetName", "SST2"], [103, 104, "DatasetName", "IMDB"], [159, 160, "HyperparameterName", "\u03b3"], [182, 183, "MetricName", "accuracy"], [193, 194, "HyperparameterName", "\u03b3"], [208, 209, "MethodName", "BERT"], [212, 213, "MethodName", "BERT"], [218, 220, "HyperparameterName", "batch size"], [228, 229, "MethodName", "Adam"], [231, 233, "HyperparameterName", "learning rate"], [249, 250, "MethodName", "Adam"], [250, 251, "HyperparameterName", "optimizer"], [252, 254, "HyperparameterName", "learning rate"], [304, 305, "MetricName", "accuracy"]]}
{"text": "We now proceed to empirically validate the effectiveness of SANA , compared to unsupervised attention , and attention supervision approaches using either task - level or sample - level annotations as baselines ( shortly , unsupervised , task - level , and sample ) . For task - level annotations ( e.g. , in SANA ) , we adopt pre - annotated task - level annotations without any additional human efforts : for the two sentiment tasks , we use SentiWordNet ( Esuli and Sebastiani , 2006 ) , and for 20NG task , we use entities recognized by AllenNLP NER ( Peters et al , 2017 ) . We thus present the empirical findings for the following four research questions : RQ1 : Does SANA improve model accuracy ? RQ2 : Does SANA improve model robustness ? RQ3 : Is SANA effective for data - scarce cases ? RQ4 : Does SANA improve attention explainability ?", "entities": [[100, 101, "TaskName", "NER"], [128, 129, "MetricName", "accuracy"]]}
{"text": "The main objective of this work is to improve attention supervisions for the purpose of better text classification . Thus , we evaluate the three attention methods by their contribution to the classification performance . Tab . 2 shows the classification accuracy for three classification datasets . In the table , we can observe the proposed approach , SANA with task - level annotation , outperforms all baselines in all the datasets . Among the results , SANA achieves the largest improvement over in 20NG dataset , which has the smallest training data . This suggests that SANA can also provide effective attention supervisions in data - scarce environments . To discuss this issue further , we will repeat this comparison over the varying size of training data for RQ3 . Our study also confirms two additional observations to our advantage - counterfactual 1 ) is effective even in model distillation setting and 2 ) meaningfully contributes to performance gains . More specifically , 1 ) SANA achieves 84.35 % in SST2 dataset which is higher than the distillation only model , but lower than task - level supervised model . 2 ) this model gets 88.23 % in 20NG dataset , which outperforms even task - level supervised model with 1.04 point gains . This also suggests the limitation of model distillation as supervision signals and supervision by public resources can provide better initial point for SANA than model distillation . Our key contribution is to show zero - cost attention supervision can improve a simple model closer to a highly sophisticated model , such as BERT ( Devlin et al , 2019 ) requiring more layers and data . This motivates us to supervise attention for BERT , though understanding of BERT internals , such as ( Rogers et al , 2020 ) , is mostly observational at this stage - Intervening with attention would be an interesting future work . Our experimental results show that SANA works well in diverse scenarios , but we observe that the effectiveness is reduced when the length of target text increases ( Figure 2 ) or token identifiability decreases ( e.g. , complex architecture ) : SANA more effectively works when the token identifiability is improved ( by adding residual connection between two recurrent layers ) , achieving 0.83 point gain from 89.14 % , which is larger gap than 0.47 point gain without residual connection .", "entities": [[16, 18, "TaskName", "text classification"], [41, 42, "MetricName", "accuracy"], [171, 172, "DatasetName", "SST2"], [267, 268, "MethodName", "BERT"], [288, 289, "MethodName", "BERT"], [293, 294, "MethodName", "BERT"], [378, 380, "MethodName", "residual connection"], [403, 405, "MethodName", "residual connection"]]}
{"text": "Having tested for the overall performance with the original datasets , we evaluate the robustness of SANA with the the adversarial datasets . Recently , adversarial examples ( Zhang et al , 2019 ) have been employed as an evaluation tool for model robustness : while the adversarial example conveys very similar semantics of its original sample , but with small and intentional feature perturbations to cause classification models to make false predictions . For robustness analysis , we thus test whether the attention models can keep the original predictions from adversarial examples . This experiment consists of the following steps : First , based on the original training data , we set a basic BiGRU model ( without attention mechanism ) as threat model , which an adversarial attack method aims to deceive . Second , based on the original test data , we generate paraphrase texts by using the state - of - the - art attack method ( Alzantot et al , 2018 ) with word - level perturbations . Third , we randomly select almost 500 paraphrase texts , which succeed in changing the prediction of threat model , i.e. , adversarial examples . Finally , we report the accuracy of the three attention models over both adversarial examples and their corresponding original samples , respectively . Tab . 3 presents the results of adversarial attacks . 3 In the table , we can find that SANA is more robust , showing the smallest gap of the classification accuracy between the original and adversarial samples . It demonstrates that , when the network is attending to the words having causal signals to the model prediction , the network becomes more robust against adversarial attacks , which is consistent with the experimental results in Lai et al ( 2019 ) . In addition to that , we observe similar results against the white - box adversarial examples ( Tsai et al , 2019 ) , where SANA improves 3.20 and 1.80 point gains from both unsupervised and supervised attentions .", "entities": [[115, 116, "MethodName", "BiGRU"], [128, 130, "TaskName", "adversarial attack"], [203, 204, "MetricName", "accuracy"], [252, 253, "MetricName", "accuracy"]]}
{"text": "This section compares models over the varying amount of training samples in IMDB dataset , as a stress test for data - scarce scenarios . For this experiment , we collect the samplespecific annotations from human workers . First , we randomly select 500 training samples from IMDB dataset , and ask the worker to underline the apparent rationales for the sentiment class , guided by the definition of rationale in Zhang et al ( 2016 ) . The data collection is conducted using an open annotation tool ( Yang et al , 2018 ) . Then , we build an additional method , named sample , which is trained with the collected sample - specific annotations . The results are presented in Fig . 1 . We notice that SANA and sample show much stronger performance when the training data is scarce , where similar results are reported in ( Bao et al , 2018 ) . As we expected , the attention supervision using the sample - specific annotations gets a higher accuracy than that using the task - level annotations , but can not be scaled - up above 500 training samples , which is represented by the red reference line . In contrast , SANA improves accuracy with \u2265 1000 samples and its scalability . This result demonstrates that our counterfactual inferences successfully augment one annotation into multiple ( counterfactual ) attention supervisions , better regularizing from limited samples .", "entities": [[12, 13, "DatasetName", "IMDB"], [47, 48, "DatasetName", "IMDB"], [174, 175, "MetricName", "accuracy"], [210, 211, "MetricName", "accuracy"]]}
{"text": "As for human consuming attention as explanation , there has been criticism that unsupervised attention weights are too poorly correlated with the contribution of each word for machine decision ( or , unfaithful ) ( Jain and Wallace , 2019 ; Serrano and Smith , 2019 ; Pruthi et al , 2019 ) . Meanwhile , ( Wiegreffe and Pinter , 2019 ) develops diagnostics to decide when attention is good enough as explanation . As for improving human consumption , one direction focuses on better aligning models to human , another on improving annotation quality . First , identifiability ( Brunner et al , 2020 ) explains human - machine discrepancy , where tokenlevel information is lost in model hidden states . For better alignment , ( Tutek and\u0160najder , 2020 ) utilizes masked language model ( MLM ) loss and ( Mohankumar et al , 2020 ) invents orthogonal LSTM representations . Second , toward the direction of improving annotation , ( Barrett et al , 2018 ; Zhong et al , 2019 ; Bao et al , 2018 ) adopts sample - specific human annotations . In addition to rationales , ( Zhao et al , 2018 ) uses event trigger words and ( Kim and Kim , 2018 ) leverages user authenticated domains to narrow down the scope of attentions . ( Strubell et al , 2018 ) injects word dependency relations to recognize the semantic roles in text . Such annotation overhead can be replaced by existing pre - annotated resources : ( Zou et al , 2018 ) considers sentiment lexicon dictionary for a related task . We pursue the second direction , but without incurring additional human annotation , by exploring the counterfactual augmentation , originated from self - supervision signals , contributing towards both accuracy and robustness of the model .", "entities": [[138, 139, "DatasetName", "MLM"], [140, 141, "MetricName", "loss"], [151, 152, "MethodName", "LSTM"], [302, 303, "MetricName", "accuracy"]]}
{"text": "Machine consuming attention for higher accuracy is the most classical target scenario . ( Yang et al , 2016 ) proposes hierarchical attention for document classification , ( Chen et al , 2016 ) personalizes classification to user and product attributes . ( Margatina et al , 2019 ) incorporates knowledge information to the self - attention module , i.e. , lexicon features . Alternatively , machine may mine or augment attention supervision : ( Tang et al , 2019 ) automatically mines attention supervision by masking - out highly attentive words in a progressive manner . ( Choi et al , 2019 ) augments counterfactual observations to debias human attention supervision via instance similarity . Our work is of combining the strength of the two works : we automatically improve attention supervision via self - supervision signals , but we build it with free task - level resources .", "entities": [[5, 6, "MetricName", "accuracy"], [24, 26, "TaskName", "document classification"]]}
{"text": "We studied the problem of attention supervision , and showed that requiring sample - level human supervision is often less effective than task - level alternative with lower ( and often zero - ) overhead . Specifically , we proposed a counterfactual signal for self - supervision , to augment task - level human annotation , into sample - level machine attention supervision , to increase both the accuracy and robustness of the model . We hope future research to explore scenarios where human intuition is not working as well as text classification , such as graph attention ( Veli\u010dkovi\u0107 et al , 2017 ) .", "entities": [[68, 69, "MetricName", "accuracy"], [91, 93, "TaskName", "text classification"]]}
{"text": "Tables 3 - 5 show the results . Target F - measure is calculated using the subset metric ( similar to metrics used by Yang and Cardie ( 2013 ) , Irsoy and Cardie ( 2014 ) ) ; if either the predicted or gold target tokens are a subset of the other , the match is counted when computing F - measure . Overlapping matches that are not subsets do not count ( e.g Egypt 's position and Israel 's position do not match . ) . For this task , in the case of multiple mentions of the same entity in the post , any mention will be considered correct if the subset matches 4 ( e.g if Palestine is a gold target , and state of Palestine is predicted at a different position in the post , it is still correct ) . This evaluation is driven from the sentiment summarization perspective : we want to predict the overall opinion in the post towards an entity . F - pos , F - neg , and Acc - sent show the performance of the sentiment model on only the correctly predicted targets 5 . Since the target and sentiment models are trained separately , this is meant to give an idea of how the sentiment model would perform in standalone mode , if targets were already provided . F - all shows the overall F - measure showing the performance of correctly predicted targets with correct sentiment compared to the total number of polar targets . This evaluates the end - to - end scenario of both important target and sentiment prediction . Best results are shown in bold . Significance thresholds are calculated for the best performing systems ( Tables 4 - 5 ) using the approximate randomization test ( Yeh , 2000 ) for target recall , precision , F - measure , Acc - sent and F - all . Significance over the method in the previous row is indicated by * ( p < 0.05 ) , * * ( p < 0.005 ) , * * ( p < 0.0005 ) . A confidence interval of almost four F - measure points is required to obtain p < 0.05 . Our dataset is small ; nonetheless we get significant results . Comparing Sentiment Lexicons Table 3 shows the results comparing the different baselines . All targets are retrieved using all - NP ; sentiment is determined using the lexical baselines . As expected , the all - NP baseline shows near perfect recall and low precision in predicting important targets . We observe that the gloss - translated MPQA lexicon outperforms the two other Arabic lexicons among the sentiment baselines . We believe that the hit rate of MPQA is higher than that of the smaller , manually - labeled SIFAAT , and it is more precise than the automatically generated WordNet - based lexicon ArSenL. The performance of MPQA is , however , reliant on the availability of high - quality English glosses . We found MPQA to consistently outperform in the model results , so in our best - linguistic models , we only show results using the MPQA lexicon .", "entities": [[9, 12, "MetricName", "F - measure"], [60, 63, "MetricName", "F - measure"], [153, 154, "TaskName", "summarization"], [179, 180, "MetricName", "Acc"], [237, 240, "MetricName", "F - measure"], [314, 317, "MetricName", "F - measure"], [318, 319, "MetricName", "Acc"], [366, 369, "MetricName", "F - measure"], [446, 447, "DatasetName", "MPQA"], [466, 467, "DatasetName", "MPQA"], [497, 498, "DatasetName", "MPQA"], [515, 516, "DatasetName", "MPQA"], [538, 539, "DatasetName", "MPQA"]]}
{"text": "Looking at table 4 , we can see that using the lemma representation easily outperforms the sparser surface word , and that adding tokenized clitics as separate tokens outperforms representations which only use the word form . Moreover , upon using the D3 decliticization method , we observe a significant increase in recall of targets over the ATB representation . This shows that the presence of the Arabic definite article Al+ is an important indicator of a target entity ; thus , even if an entity is not named , Al+ indicates that it is a known entity and is likely more salient . The more tokens are split off , the more targets are recalled , although this comes at the cost of a decrease in sentiment performance , where the lemma representation has the highest sentiment score and the D3 representation has the lowest af - ter surface word . We believe the addition of extra tokens in the sequence ( which are function words and have not much bearing on semantics ) generates noise with respect to the sentiment model . All models significantly improve the baselines on Fmeasure ; for Acc - sent , the surface word CRF does not significantly outperform the MPQA baseline . Effect of Word Clusters Figures 2 - 5 show the performance of different morphological representations when varying the number of word vector clusters k. ( Higher k means more clusters and fewer entities per semantic cluster . ) Adding cluster features tends to further boost the recall of important targets for all morphological schemes , while more or less maintaining precision . The difference in different schemes is consistent with the results of Table 4 ; the D3 representation maintains the highest recall of targets , while the opposite is true for identifying sentiment towards the targets . The ATB representation shows the best overall Fmeasure , peaking at 41.5 using k=250 ( compare with 38.2 using no clusters ) ; however , it recalls much fewer targets than the D3 representation . The effect of clusters on sentiment is less clear ; it seems to benefit the D3 and ATB schemes more than lemma ( significant boosts in sentiment accuracy ) . The improvements in F - measure and F - all observed by using the best value of k is statistically significant for all schemes ( k=10 for lemma , k=250 for lemma+ATB , k=500 for lemma+D3 , with F - all values of 40.7 , 41.5 , and 39.1 respectively ) . In general , the cluster performances tend to peak at a certain value of k which balances the reduced sparsity of the model ( fewer clusters ) with the semantic closeness of entities within a cluster ( more clusters ) . Performance of Best Linguistic Model Table 5 shows the performance of our best - linguistic model , which in addition to the word form and part of speech , contains named entity and base phrase chunks , the syntactic dependency features , and the sentiment lexicon features . The best linguistic model is run using both ATB and D3 tokenization schemes , and then using a combined ATB+D3 scheme where we use D3 for the target model and remove the extra clitics before piping in the output to the sentiment model . This combined scheme results in the best results overall : F - score of 61.4 for targets , accuracy of 75.4 for sentiment and overall F - measure of 43.1 . Adding the richer linguistic resources results in both improved target precision , recall , and sentiment scores , with F - measure for positive targets reaching 67.7 for positive targets and 80 for negative targets . Performance exceeds that of the simpler models which use only POS and word clusters , but it is worth noting that using only the basic model with the word clusters can achieve significant boosts in recall and F - measure bringing it closer to the rich linguistic model . The last row shows the best linguistic model D3+ATB combined with the clusters ( best result for k=8000 , or about 30 words per cluster ) . Adding the clusters improves target and Fmeasure scores , although this result is not statistically significant . We observe that it becomes more difficult to improve on the rich linguistic model using word clusters , which are more beneficial for low resource scenarios . Our results are comparable to published work for most similar tasks in English : e.g Yang and Cardie ( 2013 ) who reported target subset Fmeasure of~65 , Pontiki et al ( 2014 ) where best Figure 6 : Overall F - score vs clusters . performing SemEval systems reported 70 - 80 % for sentiment given defined aspects , and ( Mitchell et al , 2013 ; Deng and Wiebe , 2015 ) for overall Fmeasure ; we note that our tasks differ as described in section 2 . Results on blind test Table 6 shows the results on unseen test data for best - linguistic using D3 , D3+ATB and with clusters using k=8000 . The results are similar to what was observed in the development data .", "entities": [[11, 12, "DatasetName", "lemma"], [42, 43, "DatasetName", "D3"], [132, 133, "DatasetName", "lemma"], [141, 142, "DatasetName", "D3"], [194, 195, "MetricName", "Acc"], [201, 202, "MethodName", "CRF"], [207, 208, "DatasetName", "MPQA"], [287, 288, "DatasetName", "D3"], [340, 341, "DatasetName", "D3"], [358, 359, "DatasetName", "D3"], [364, 365, "DatasetName", "lemma"], [370, 371, "MetricName", "accuracy"], [376, 379, "MetricName", "F - measure"], [400, 401, "DatasetName", "lemma"], [524, 525, "DatasetName", "D3"], [538, 539, "DatasetName", "D3"], [576, 577, "MetricName", "accuracy"], [583, 586, "MetricName", "F - measure"], [608, 611, "MetricName", "F - measure"], [662, 665, "MetricName", "F - measure"], [853, 854, "DatasetName", "D3"]]}
{"text": "The core of our system is based upon computing semantic similarity of sentence chunks . More precisely , we are looking for the best estimation of sim ( CH a i , CH b j ) . The sim score should describe semantic similarity of a given chunk pairthe higher score the more easily both chunks can be replaced with each other without chaining the meaning of both sentences . The similarity score ranges from 0 to 5 , where 0 is the lowest similarity and 5 is the highest similarity . Eg . the sim ( \" a new laptop \" , \" a new notebook \" ) = 5 and sim ( \" a new laptop \" , \" an old rock \" ) = 0 . We use the chunk similarity as a feature in our machine learning approach ( Section 3 ) and as a metric in our unsupervised approach ( Section 4 ) . Our attempts to estimate the sim function are based upon estimating semantic similarity of individual words and compiling them into one number for a given chunk pair . We experiment with Word2Vec ( Mikolov et al , 2013 ) and GloVe ( Pennington et al , 2014 ) for estimating similarity of words . We compile all the word similarities in one number that reflects semantic similarity of whole chunks via the following methods : 1 ) the vector composition method and 2 ) an adapted method for constructing vectors called lexical semantic vectors . Vector composition requires that the semantics of words is described by vectors . E.g. we have vectors for all words m i : \u2200w i CH a j and n i : \u2200w i CH b k in two given chunks CH a j and CH b k . The vectors for words in each chunk are summed ( or averaged ) to obtain one vector for each chunk : m = i ( m i ) and n = j ( n j ) . The vectors are then compared with cosine distance : sim ( CH a j , CH b k ) = cos ( \u03b8 ) = m n m | n | . Lexical semantic vectors were originally introduced in ( Li et al , 2006 ) . We have made two modifications . We do not weight words with their information content and we use methods for distributional semantics ( Word2Vec and GloVe ) rather than semantic networks . The modified method is explained here . First of all , we create a combined vocabulary of all unique words from chunks CH a k and CH b l : L = unique ( CH a k \u222a CH b l ) . Then we take all words from vocabulary L : w i L and look for maximal similarities with words from chunks a and b , respectively . This way we get vectors m and n containing maximal similarities of chunk words and words from the combined vocabulary : m i = max j:1\u2264j\u2264 | CH a k | sim ( w i , w j ) : \u2200w i L n i = max j:1\u2264j\u2264 | CH b l | sim ( w i , w j ) : \u2200w i L ( 1 ) where m i and n i are elements of vectors m and n. In order to obtain similarity of a chunk pair we compare their respective vectors with the cosine similarity similarly to the previous approach . The principle of the method is illustrated by the example in figure 1 . iDF weighting . We assume that some words are more important than others . In order to reflect this assumption , we try to weight the vectors with iDF weighting . We compute the iDF weights on the articles from English wikipedia text data ( Wikipedia dump from March 7 , 2015 ) .", "entities": [[9, 11, "TaskName", "semantic similarity"], [42, 44, "TaskName", "semantic similarity"], [75, 76, "DatasetName", "0"], [80, 81, "DatasetName", "0"], [127, 128, "DatasetName", "0"], [170, 172, "TaskName", "semantic similarity"], [199, 200, "MethodName", "GloVe"], [224, 226, "TaskName", "semantic similarity"], [362, 363, "HyperparameterName", "\u03b8"], [412, 413, "MethodName", "GloVe"]]}
{"text": "The main effort of our team was focused on the machine learning approach to the task . We divided the task into to three classification / regression tasks : Alignment binary classification - we decide whether two given chunks should be aligned with each other . Score classification / regression - we experiment with both classification and regression of the chunks similarity score . Type classification - we classify all aligned pairs of chunks into a predefined set of types - see Section 1.1 .", "entities": [[46, 47, "MetricName", "Score"]]}
{"text": "Machine learning approach We employ the following classifiers and classification frameworks : Alignment binary classification - Voted perceptron ( Weka ) . Score classification - Maximum entropy ( Brainy ) . Type classification - Support vector machines ( Brainy ) . These classifiers perform best on the evaluation datasets . We achieved the best results for estimating chunk similarity with Word2Vec and the modified lexical semantic vectors - see Section 2.2 . We experimented with reduced feature set ( word overlap , word positions difference , POS tags difference , semantic similarity , global semantic similarity , paraphrase database ) - run 1 and with all featuresrun 3 . The run 1 contains the optimal combination of features . Since this combination is established on evaluation datasets it does not need to be optimal for the test datasets . To increase our chances in the completion , we also run the system with all features - run 3 . We use the provided annotated evaluation dataset ( Images , Headlines , Answer students ) for training the models . We train three models , each for one dataset . For development , we use the 10 - fold crossvalidation . For final test runs , we train the three models on evaluation datasets and run the system on the corresponding test datasets ( e.g. Images evaluation dataset based model is used to annotate Images test data ) . We do not neither modify the original datasets nor annotate any additional data . Rule - based approach There are little options in this approach . Again , we have achieved the best results for estimating chunk similarity with Word2Vec and the modified lexical semantic vectors - see Section 2.2 . We set the threshold for the similarity score to 2.5 . All lower values are set to 0 . This is the run 2 . Individual setting for different dataset We restrained from setting individual configurations for different datasets . The setup is completely identical for all datasets .", "entities": [[22, 23, "MetricName", "Score"], [90, 92, "TaskName", "semantic similarity"], [94, 96, "TaskName", "semantic similarity"], [306, 307, "DatasetName", "0"]]}
{"text": "In this section , we summarize the official results for the SemEval 2016 competition - see table 1 . The results are calculated for the following dataset : Headlines , Images and Answer students . The results show F1 scores for chunk alignment ( Ali ) , determination of the relation type ( Type ) , chunk similarity score ( Score ) and combination of relation type and score similarity ( T+S ) . The bold numbers are the overall best scores . We participated only in the gold standard chunk scenario . The results clearly show that the unsupervised run 2 perform much worse than the supervised runs 1 and 3 . We expected that . However , it is worth of noticing that the unsupervised alignment algorithm inspired by machine translation alignment placed quite well . In fact , it is newer looses more than 3 % from the best alignment score in all datasets . The overall rank of the run 2 places in the top half among all system with exception of the answer student dataset . The poor performance of the run 2 on this dataset is most likely caused by the fact that the hand - crafted rules were prepared for the images and headlines datasets and they are clearly not applicable on the answer student which is substantially different . The runs 1 and 3 perform very similarly . The optimized feature set of the run 1 helps especially in the answer student dataset . However , the differences between these runs are too small and they can be caused by chance . It is worth of noticing that the run 1 is not the best one in any of the datasets and it still wins in the overall results table . The reason is that it provides the most consistent results among all other runs of all systems in the competition . In order to provide additional information about the features effectiveness , we have evaluated them on the final test datasets . In many cases , the obtained results are not conclusive . On some datasets , the features help slightly on others they even decrease the performance . However , the following three features have significant influence on the final results : modified lexical semantic vectors ( +3 % of the mean of T+S F1 scores ) , shared words ( +2 % ) , POS tags difference ( +2 % ) . The modified lexical semantic vectors method performed better than vector composition by 1 % for the machine learning approach and by 2 % for the rule - based approach in average . By optimizing the feature set , we were able to increase the mean score to 0.6484 of T+S F1 measure .", "entities": [[38, 39, "MetricName", "F1"], [60, 61, "MetricName", "Score"], [131, 133, "TaskName", "machine translation"], [394, 395, "MetricName", "F1"], [463, 464, "MetricName", "F1"]]}
{"text": "Given an event phrase , our models aim to generate three entity - specific pragmatic inferences : Per - sonX 's intent , PersonX 's reaction , and others ' reactions . The general outline of our model architecture is illustrated in Figure 3 . The input to our model is an event pattern described through free - form text with typed variables such as PersonX gives PersonY as a gift . For notation purposes , we describe each event pattern E as a sequence of word embeddings he 1 , e 2 , . . . , e n i 2 R n \u21e5 D . This input is encoded as a vector h E 2 R H that will be used for predicting output . The output of the model is its hypotheses about PersonX 's intent , PersonX 's reaction , and others ' reactions ( v i , v x , and v o , respectively ) . We experiment with representing the From an encoded event , our model predicts intents and reactions in a multitask setting . output in two decoding set - ups : three vectors interpretable as discrete distributions over words and phrases ( n - gram reranking ) or three sequences of words ( sequence decoding ) . Encoding events The input event phrase E is compressed into an H - dimensional embedding h E via an encoding function f : R n \u21e5 D ! R H : h E = f ( e 1 , . . . , e n ) We experiment with several ways for defining f , inspired by standard techniques in sentence and phrase classification ( Kim , 2014 ) . First , we experiment with max - pooling and mean - pooling over the word vectors { e i } n i=1 . We also consider a convolutional neural network ( ConvNet ; LeCun et al , 1998 ) taking the last layer of the network as the encoded version of the event . Lastly , we encode the event phrase with a bi - directional RNN ( specifically , a GRU ; Cho et al , 2014 ) , concatenating the final hidden states of the forward and backward cells as the encoding : h E = [ ! h n ; h 1 ] . For hyperparameters and other details , we refer the reader to Appendix B. Though the event sequences are typically rather short ( 4.6 tokens on average ) , our model still benefits from the ConvNet and BiRNN 's ability to compose words . Pragmatic inference decoding We use three decoding modules that take the event phrase embedding h E and output distributions of possible PersonX 's intent ( v i ) , PersonX 's reactions ( v x ) , and others ' reactions ( v o ) . We experiment with two different decoder set - ups . First , we experiment with n - gram re - ranking , considering the | V | most frequent { 1 , 2 , 3 } grams in our annotations . Each decoder projects the event phrase embedding h E into a | V | dimensional vector , which is then passed through a softmax function . For instance , the distribution over descriptions of PersonX 's intent is given by : v i = softmax ( W i h E + b i ) Second , we experiment with sequence generation , using RNN decoders to generate the textual description . The event phrase embedding h E is set as the initial state h dec of three decoder RNNs ( using GRU cells ) , which then output the intent / reactions one word at a time ( using beam - search at test time ) . For example , an event 's intent sequence ( v i = v ( 0 ) i v ( 1 ) i . . . ) is computed as follows : v ( t+1 ) i = softmax ( W i RNN ( v ( t ) i , h ( t ) i , dec ) + b i ) Training objective We minimize the crossentropy between the predicted distribution over words and phrases , against the one actually observed in our dataset . Further , we employ multitask learning , simultaneously minimizing the loss for all three decoders at each iteration . Training details We fix our input embeddings , using 300 - dimensional skip - gram word embeddings trained on Google News ( Mikolov et al , 2013 ) . For decoding , we consider a vocabulary of size | V | = 14 , 034 in the n - gram re - ranking setup . For the sequence decoding setup , we only consider the unigrams in V , yielding an output space of 7 , 110 at each time step . We randomly divided our set of 24 , 716 unique events ( 57 , 094 annotations ) into a training / dev./test set using an 80/10/10 % split . Some annotations have multiple responses ( i.e. , a crowdworker gave multiple possible intents and reactions ) , in which case we take each of the combinations of their responses as a separate training example .", "entities": [[86, 88, "TaskName", "word embeddings"], [358, 359, "MethodName", "GRU"], [547, 548, "MethodName", "softmax"], [568, 569, "MethodName", "softmax"], [615, 616, "MethodName", "GRU"], [655, 656, "DatasetName", "0"], [678, 679, "MethodName", "softmax"], [736, 737, "MetricName", "loss"], [760, 762, "TaskName", "word embeddings"], [764, 765, "DatasetName", "Google"]]}
{"text": "Neural machine translation ( NMT ) is one of the core topics in natural language processing , which aims to generate sequences of words in the target language conditioned on the source inputs ( Sutskever et al , 2014 ; Cho et al , 2014 ; Vaswani et al , 2017 ) . In the common supervised setting , the training objective is to learn a transformation from the source space to the target space X Y : f ( y | x ; \u0398 ) with the usage of parallel data . In this way , NMT models are expected to 1 The core codes are contained in Appendix E. be capable of generalizing to unseen instances with the help of large scale training data , which poses a big challenge for scenarios with limited resources . To address this problem , various methods have been developed to leverage abundant unlabeled data for augmenting limited labeled data ( Sennrich et al , 2016a ; Cheng et al , 2016 ; Hoang et al , 2018 ; Song et al , 2019 ) . For example , backtranslation ( BT ) ( Sennrich et al , 2016a ) makes use of the monolingual data on the target side to synthesize large scale pseudo parallel data , which is further combined with the real parallel corpus in machine translation task . Another line of research is to introduce adversarial inputs to improve the generalization of NMT models towards small perturbations ( Iyyer et al , 2015 ; Fadaee et al , 2017 ; Wang et al , 2018 ; Cheng et al , 2018 ; Gao et al , 2019 ) . While these methods lead to significant boosts in translation quality , we argue that augmenting the observed training data in the discrete space inherently has two major limitations . First , augmented training instances in discrete space are lack diversity . We still take BT as an example , it typically uses beam search ( Sennrich et al , 2016a ) or greedy search ( Lample et al , 2018a , c ) to generate synthetic source sentences for each target monolingual sentence . The above two search strategies are approximate algorithms to identify the maximum a - posteriori ( MAP ) output , and thus favor the most frequent one in case of ambiguity . proposed a sampling strategy from the output distribution to alleviate this issue , but this method typically yields synthesized data with low quality . While some extensions ( Wang et al , 2018 ; Imamura et al , 2018 ; Khayrallah et al , 2020 ; Nguyen et al , 2020 ) augment each training instance with multiple literal forms , they still fail to cover adequate variants under the same meaning . Second , it is difficult for augmented texts in dis - crete space to preserve their original meanings . In the context of natural language processing , discrete manipulations such as adds , drops , reorders , and/or replaces words in the original sentences often result in significant changes in semantics . To address this issue , Gao et al ( 2019 ) and Cheng et al ( 2020 ) instead replace words with other words that are predicted using language model under the same context , by interpolating their embeddings . Although being effective , these techniques are limited to word - level manipulation and are unable to perform the whole sentence transformation , such as producing another sentence by rephrasing the original one so that they have the same meaning . In this paper , we propose Continuous Semantic Augmentation ( CSANMT ) , a novel data augmentation paradigm for NMT , to alleviate both limitations mentioned above . The principle of CSANMT is to produce diverse training data from a semantically - preserved continuous space . Specifically , ( 1 ) we first train a semantic encoder via a tangential contrast , which encourages each training instance to support an adjacency semantic region in continuous space and treats the tangent points of the region as the critical states of semantic equivalence . This is motivated by the intriguing observation made by recent work showing that the vectors in continuous space can easily cover adequate variants under the same meaning ( Wei et al , 2020a ) . ( 2 ) We then introduce a Mixed Gaussian Recurrent Chain ( MGRC ) algorithm to sample a cluster of vectors from the adjacency semantic region . ( 3 ) Each of the sampled vectors is finally incorporated into the decoder by developing a broadcasting integration network , which is agnostic to model architectures . As a consequence , transforming discrete sentences into the continuous space can effectively augment the training data space and thus improve the generalization capability of NMT models . We evaluate our framework on a variety of machine translation tasks , including WMT14 English - German / French , NIST Chinese - English and multiple IWSLT tasks . Specifically , CSANMT sets the new state of the art among existing augmentation techniques on the WMT14 English - German task with 30.94 BLEU score . In addition , our approach could achieve comparable performance with the baseline model with the usage of only 25 % of training data . This reveals that CSANMT has great potential to achieve good results with very few data . Furthermore , CSANMT demonstrates consistent improvements over strong baselines in low resource scenarios , such as IWSLT14 English - German and IWSLT17 English - French .", "entities": [[1, 3, "TaskName", "machine translation"], [84, 85, "HyperparameterName", "\u0398"], [226, 228, "TaskName", "machine translation"], [381, 382, "DatasetName", "MAP"], [618, 620, "TaskName", "data augmentation"], [821, 823, "TaskName", "machine translation"], [826, 827, "DatasetName", "WMT14"], [858, 859, "DatasetName", "WMT14"], [865, 867, "MetricName", "BLEU score"]]}
{"text": "Problem Definition Supposing X and Y are two data spaces that cover all possible sequences of words in source and target languages , respectively . We denote ( x , y ) ( X , Y ) as a pair of two sentences with the same meaning , where x = { x 1 , x 2 , ... , x T } is the source sentence with T tokens , and y = { y 1 , y 2 , ... , y T \u2032 } is the target sentence with T \u2032 tokens . A sequence - tosequence model is usually applied to neural machine translation , which aims to learn a transformation from the source space to the target space X Y : f ( y | x ; \u0398 ) with the usage of parallel data . Formally , given a set of observed sentence pairs C = { ( x ( n ) , y ( n ) ) } N n=1 , the training objective is to maximize the log - likelihood : J mle ( \u0398 ) = E ( x , y ) \u223cC log P ( y | x ; \u0398 ) . ( 1 ) The log - probability is typically decomposed as : log P ( y | x ; \u0398 ) = T \u2032 t=1 log P ( y t | y < t , x ; \u0398 ) , where \u0398 is a set of trainable parameters and y < t is a partial sequence before time - step t. However , there is a major problem in the common supervised setting for neural machine translation , that is the number of training instances is very limited because of the cost in acquiring parallel data . This makes it difficult to learn an NMT model generalized well to unseen instances . Traditional data augmentation methods generate more training samples by applying discrete manipulations to unlabeled ( or labeled ) data , such as back - translation or randomly replacing a word with another one , which usually suffer from the problems of semantic deviation and the lack of diversity .", "entities": [[106, 108, "TaskName", "machine translation"], [132, 133, "HyperparameterName", "\u0398"], [175, 178, "MetricName", "log - likelihood"], [182, 183, "HyperparameterName", "\u0398"], [199, 200, "HyperparameterName", "\u0398"], [221, 222, "HyperparameterName", "\u0398"], [239, 240, "HyperparameterName", "\u0398"], [243, 244, "HyperparameterName", "\u0398"], [277, 279, "TaskName", "machine translation"], [315, 317, "TaskName", "data augmentation"]]}
{"text": "We propose a novel data augmentation paradigm for neural machine translation , termed continuous semantic augmentation ( CSANMT ) , to better generalize the model 's capability to unseen instances . We adopt the Transformer ( Vaswani et al , 2017 ) model as a backbone , and the framework is shown in Figure 1 . In this architecture , an extra semantic encoder translates the source x and the target sentence y to real - value vectors r x = \u03c8 ( x ; \u0398 \u2032 ) and r y = \u03c8 ( y ; \u0398 \u2032 ) respectively , where \u03c8 ( ; \u0398 \u2032 ) is the forward function of the semantic encoder parameterized by \u0398 \u2032 ( parameters other than \u0398 ) . \u2200 ( x , y ) ( X , Y ) : r x = r y . Besides , an adjacency semantic region \u03bd ( r x , r y ) in the semantic space describes adequate variants of literal expression centered around each observed sentence pair ( x , y ) . In our scenario , we first sample a series of vectors ( denoted by R ) from the adjacency semantic region to augment the current training instance , that is R = { r ( 1 ) , r ( 2 ) , ... , r ( K ) } , wherer ( k ) \u223c \u03bd ( r x , r y ) . K is the hyperparameter that determines the number of sampled vectors . Each sampler ( k ) is then integrated into the generation process through a broadcasting integration network : ot = W1r ( k ) + W2ot + b , ( 2 ) where o t is the output of the self - attention module at position t. Finally , the training objective in Eq . ( 1 ) can be improved as J mle ( \u0398 ) = E ( x , y ) \u223cC , r ( k ) R log P ( y | x , r ( k ) ; \u0398 ) . ( 3 ) By augmenting the training instance ( x , y ) with diverse samples from the adjacency semantic region , the model is expected to generalize to more unseen instances . To this end , we must consider such two problems : ( 1 ) How to optimize the semantic encoder so that it produces a meaningful adjacency semantic region for each observed training pair . ( i ) , y ( i ) ) . ( 2 ) How to obtain samples from the adjacency semantic region in an efficient and effective way . In the rest part of this section , we introduce the resolutions of these two problems , respectively . Tangential Contrastive Learning We start from analyzing the geometric interpretation of adjacency semantic regions . The schematic diagram is illustrated in Figure 2 . Let ( x ( i ) , y ( i ) ) and ( x ( j ) , y ( j ) ) are two instances randomly sampled from the training corpora . For ( x ( i ) , y ( i ) ) , the adjacency semantic region \u03bd ( r x ( i ) , r y ( i ) ) is defined as the union of two closed balls that are centered by r x ( i ) and r y ( i ) , respectively . The radius of both balls is d = \u2225 r x ( i ) \u2212 r y ( i ) \u2225 2 , which is also considered as a slack variable for determining semantic equivalence . The underlying interpretation is that vectors whose distances from r x ( i ) ( or r y ( i ) ) do not exceed d , are semantically - equivalent to both r x ( i ) and r y ( i ) . To make \u03bd ( r x ( i ) , r y ( i ) ) conform to the interpretation , we employ a similar method as in ( Zheng et al , 2019 ; to optimize the semantic encoder with the tangential contrast . Specifically , we construct negative samples by applying the convex interpolation between the current instance and other ones in the same training batch for instance comparison . And the tangent points ( i.e. , the points on the boundary ) are considered as the critical states of semantic equivalence . The training objective is formulated as : J ctl ( \u0398 \u2032 ) = E ( x ( i ) , y ( i ) ) \u223cB log e s r x ( i ) , r y ( i ) e s r x ( i ) , r y ( i ) + \u03be , \u03be = | B | j&j\u0338 = i e s r y ( i ) , r y \u2032 ( j ) + e s r x ( i ) , r x \u2032 ( j ) , ( 4 ) where B indicates a batch of sentence pairs randomly selected from the training corpora C , and s ( ) is the score function that computes the cosine similarity between two vectors . The negative samples r x \u2032 ( j ) and r y \u2032 ( j ) are designed as the following interpolation : r x \u2032 ( j ) = r x ( i ) + \u03bbx ( r x ( j ) \u2212 r x ( i ) ) , \u03bbx ( d d \u2032 x , 1 ] , r y \u2032 ( j ) = r y ( i ) + \u03bby ( r y ( j ) \u2212 r y ( i ) ) , \u03bby ( d d \u2032 y , 1 ] , ( 5 ) where d \u2032 x = \u2225 r x ( i ) \u2212 r x ( j ) \u2225 and d \u2032 y = \u2225 r y ( i ) \u2212 r y ( j ) \u2225. The two equations in Eq . ( 5 ) set up when d \u2032 x and d \u2032 y are larger than d respectively , or else r x \u2032 ( j ) = r x ( j ) and r y \u2032 ( j ) = r y ( j ) . According to this design , an adjacency semantic region for the i - th training instance can be fully established by interpolating various instances in the same training batch . We follow to adaptively adjust the value of \u03bb x ( or \u03bb y ) during the training process , and refer to the original paper for details . MGRC Sampling To obtain augmented data from the adjacency semantic region for the training instance ( x , y ) , we introduce a Mixed Gaussian Recurrent Chain ( denoted by MGRC ) algorithm to design an efficient and effective sampling strategy . As illustrated in Figure 3 , we first transform the bias vectorr = r y \u2212 r x according to a predefined scale vector \u03c9 , that is \u03c9 r , where is the element - wise product operation . Then , we construct a novel sampler = r + \u03c9 r for augmenting the current instance , in which r is either r x or r y . As a consequence , the goal of the sampling strategy turns into find a set of scale vectors , i.e. { \u03c9 ( 1 ) , \u03c9 ( 2 ) , ... , \u03c9 ( K ) } . Intuitively , we can assume that \u03c9 follows a distribution with universal or Gaussian forms , despite the latter demonstrates better results in our experience . Formally , we design a", "entities": [[4, 6, "TaskName", "data augmentation"], [9, 11, "TaskName", "machine translation"], [34, 35, "MethodName", "Transformer"], [85, 86, "HyperparameterName", "\u0398"], [96, 97, "HyperparameterName", "\u0398"], [105, 106, "HyperparameterName", "\u0398"], [118, 119, "HyperparameterName", "\u0398"], [124, 125, "HyperparameterName", "\u0398"], [323, 324, "HyperparameterName", "\u0398"], [351, 352, "HyperparameterName", "\u0398"], [471, 473, "MethodName", "Contrastive Learning"], [772, 773, "HyperparameterName", "\u0398"]]}
{"text": "Input : The representations of the training instance ( x , y ) , i.e. rx and ry . Output : A set of augmented samples R = { r ( 1 ) , r ( 2 ) , ... , r ( K ) } 1 : Normalizing the importance of each element inr = ry \u2212 rx : Wr = | r | \u2212min ( | r | ) max ( | r | ) \u2212min ( | r | ) 2 : Set k = 1 , \u03c9 ( 1 ) \u223c N ( 0 , diag ( W 2 r ) ) , r ( 1 ) = r + \u03c9 ( 1 ) ( ry \u2212 rx ) 3 : Initialize the set of samples as R = { r ( 1 ) } . 4 : while k \u2264 ( K \u2212 1 ) do 5 : k k + 1 6 : Calculate the current scale vector : \u03c9 ( k ) \u223c p ( \u03c9 | \u03c9 ( 1 ) , \u03c9 ( 2 ) , ... , \u03c9 ( k\u22121 ) according to Eq . ( 6 ) . 7 : Calculate the current sample : r ( k ) = r + \u03c9 ( k ) ( ry \u2212 rx ) .", "entities": [[86, 88, "HyperparameterName", "k ="], [97, 98, "DatasetName", "0"]]}
{"text": "R R { r ( k ) } . 9 : end while mixed Gaussian distribution as follow : \u03c9 ( k ) \u223c p ( \u03c9 | \u03c9 ( 1 ) , \u03c9 ( 2 ) , ... , \u03c9 ( k\u22121 ) ) , p = \u03b7N 0 , diag ( W 2 r ) + ( 1.0 \u2212 \u03b7 ) N 1 k \u2212 1 k\u22121 i=1 \u03c9 ( i ) , 1 . ( 6 ) This framework unifies the recurrent chain and the rejection sampling mechanism . Concretely , we first normalize the importance of each dimension inr as W r = | r | \u2212min ( | r | ) max ( | r | ) \u2212min ( | r | ) , the operation | | takes the absolute value of each element in the vector , which means the larger the value of an element is the more informative it is . Thus N ( 0 , diag ( W 2 r ) ) limits the range of sampling to a subspace of the adjacency semantic region , and rejects to conduct sampling from the uninformative dimensions . Moreover , N ( 1 k\u22121 k\u22121 i=1 \u03c9 ( i ) , 1 ) simulates a recurrent chain that generates a sequence of reasonable vectors where the current one is dependent on the prior vectors . The reason for this design is that we expect that p in Eq . ( 6 ) can become a stationary distribution with the increase of the number of samples , which describes the fact that the diversity of each training instance is not infinite . \u03b7 is a hyperparameter to balance the importance of the above two Gaussian forms . For a clearer presentation , Algorithm 1 summarizes the sampling process .", "entities": [[49, 50, "DatasetName", "0"], [163, 164, "DatasetName", "0"], [260, 263, "HyperparameterName", "number of samples"]]}
{"text": "The training objective in our approach is a combination of J mle ( \u0398 ) in Eq . ( 3 ) and J ctl ( \u0398 \u2032 ) in Eq . ( 4 ) . In practice , we introduce a two - phase training procedure with mini - batch losses . Firstly , we train the semantic encoder from scratch using the task - specific data , i.e. \u0398 \u2032 * = argmax \u0398 \u2032 J ctl ( \u0398 \u2032 ) . Secondly , we optimize the encoder - decoder model by maximizing the log - likelihood , i.e. \u0398 * = argmax \u0398 J mle ( \u0398 ) , and fine - tune the semantic encoder with a small learning rate at the same time . During inference , the sequence of target words is generated auto - regressively , which is almost the same as the vanilla Transformer ( Vaswani et al , 2017 ) . A major difference is that our method involves the semantic vector of the input sequence for generation : y * t = argmax yt P ( | y < t , x , r x ; \u0398 ) , where r x = \u03c8 ( x ; \u0398 \u2032 ) . This module is plug - in - use as well as is agnostic to model architectures .", "entities": [[13, 14, "HyperparameterName", "\u0398"], [25, 26, "HyperparameterName", "\u0398"], [69, 70, "HyperparameterName", "\u0398"], [74, 75, "HyperparameterName", "\u0398"], [79, 80, "HyperparameterName", "\u0398"], [95, 98, "MetricName", "log - likelihood"], [100, 101, "HyperparameterName", "\u0398"], [104, 105, "HyperparameterName", "\u0398"], [108, 109, "HyperparameterName", "\u0398"], [121, 123, "HyperparameterName", "learning rate"], [150, 151, "MethodName", "Transformer"], [195, 196, "HyperparameterName", "\u0398"], [206, 207, "HyperparameterName", "\u0398"]]}
{"text": "We first apply CSANMT to NIST Chinese - English ( Zh En ) , WMT14 English - German ( En De ) and English - French ( En Fr ) tasks , and conduct extensive analyses for better understanding the proposed method . And then we generalize the capability of our method to low - resource IWSLT tasks . Training Details . We implement our approach on top of the Transformer ( Vaswani et al , 2017 ) . The semantic encoder is a 4 - layer transformer encoder with the same hidden size as the backbone model . Following sentence - bert ( Reimers and Gurevych , 2019 ) , we average the outputs of all positions as the sequence - level representation . The learning rate for finetuning the semantic encoder at the second training stage is set as 1e \u2212 5 . All experiments are performed on 8 V100 GPUs . We accumulate the gradient of 8 iterations and update the models with a batch of about 65 K tokens . The hyperparameters K and \u03b7 in MGRC sampling are tuned on the validation set with the range of K { 10 , 20 , 40 , 80 } and \u03b7 { 0.15 , 0.30 , 0.45 , 0.6 , 0.75 , 0.90 } . We use the default setup of K = 40 for all three tasks , \u03b7 = 0.6 for both Zh En and En De while \u03b7 = 0.45 for En Fr . For evaluation , the beam size and length penalty are set to 4 and 0.6 for the En De as well as En Fr , while 5 and 1.0 for the Zh En task .", "entities": [[14, 15, "DatasetName", "WMT14"], [70, 71, "MethodName", "Transformer"], [126, 128, "HyperparameterName", "learning rate"], [224, 226, "HyperparameterName", "K ="]]}
{"text": "Results of Zh En . Table 1 shows the results on the Chinese - to - English translation task . From the results , we can conclude that our approach outperforms existing augmentation strategies such as back - translation ( Sennrich et al , 2016a ; Wei et al , 2020a ) and switchout ( Wang et al , 2018 ) by a large margin ( up to 3.63 BLEU ) , which verifies that augmentation in continuous space is more effective than methods with discrete manipulations . Compared to the approaches that replace words in the embedding space ( Cheng et al , 2020 ) , our approach also demonstrates superior performance , which reveals that sentence - level augmentation with continuous semantics works better on generalizing to unseen instances . Moreover , compared to the vanilla Transformer , our approach consistently achieves promising improvements on five test sets . Results of En De and En Fr . From Table 2 , our approach consistently performs better than existing methods ( Sennrich et al , 2016a ; Wang et al , 2018 ; Wei et al , 2020a ; Cheng et al , 2020 ) , yielding significant gains ( 0.65\u223c1.76 BLEU ) on the En De and En Fr tasks . An exception is that Nguyen et al ( 2020 ) achieved comparable results with ours via multiple forward and backward NMT models , thus data diversification intuitively demonstrates lower training efficiency . Moreover , we observe that CSANMT gives 30.16 BLEU on the En De task with the base setting , significantly outperforming the vanilla Transformer by 2.49 BLEU points . Our approach yields a further improvement of 0.68 BLEU by equipped with the wider architecture , demonstrating superiority over the standard Transformer by 2.15 BLEU . Similar observations can be drawn for the En Fr task .", "entities": [[69, 70, "MetricName", "BLEU"], [138, 139, "MethodName", "Transformer"], [202, 203, "MetricName", "BLEU"], [253, 254, "MetricName", "BLEU"], [268, 269, "MethodName", "Transformer"], [271, 272, "MetricName", "BLEU"], [282, 283, "MetricName", "BLEU"], [295, 296, "MethodName", "Transformer"], [298, 299, "MetricName", "BLEU"]]}
{"text": "Effects of K and \u03b7 . Figure 4 illustrates how the hyper - parameters K and \u03b7 in MGRC sampling affect the translation quality . From Figures 4 ( a ) - 4 ( c ) , we can observe that gradually increasing the number of samples significantly improves BLEU scores , which demonstrates large gaps between K = 10 and K = 40 . However , assigning larger values ( e.g. , 80 ) to K does not result in further improvements among all three tasks . We conjecture that the reasons are two folds : ( 1 ) it is fact that the diversity of each training instance is not infinite and thus MGRC gets saturated is inevitable with K increasing . ( 2 ) MGRC sampling with a scaled item ( i.e. , W r ) may degenerate to traverse in the same place . This prompts us to design more sophisticated algorithms in future work . In our experiments , we default set K = 40 to achieve a balance between the training efficiency and translation quality . Figure 4 ( d ) shows the effect of \u03b7 on validation sets , which balances the importance of two Gaussian forms during the sampling process . The setting of \u03b7 = 0.6 achieves the best results on both the Zh En and En De tasks , and \u03b7 = 0.45 consistently outperforms other values on the En Fr task .", "entities": [[44, 47, "HyperparameterName", "number of samples"], [49, 50, "MetricName", "BLEU"], [57, 59, "HyperparameterName", "K ="], [61, 63, "HyperparameterName", "K ="], [167, 169, "HyperparameterName", "K ="]]}
{"text": "We demonstrate both the lexical diversity ( measured by TTR= num . of types num . of tokens ) of various trans - lations and the semantic faithfulness of machine translated ones ( measured by BLEURT with considering human translations as the references ) in Table 4 . It is clear that CSANMT substantially bridge the gap of the lexical diversity between translations produced by human and machine . Meanwhile , CSANMT shows a better capability on preserving the semantics of the generated translations than Transformer . We intuitively attribute the significantly increases of BLEU scores on all datasets to these two factors . We also have studied the robustness of CSANMT towards noisy inputs and the translationese effect , see Appendix D for details . Effect of the semantic encoder . We introduce two variants of the semantic encoder to investigate its performance on En De validation set . Specifically , ( 1 ) we remove the extra semantic encoder and construct the sentence - level representations by averaging the sequence of outputs of the vanilla sentence encoder . ( 2 ) We replace the default 4 - layer semantic encoder with a large pre - trained model ( PTM ) ( i.e. , XLM - R ( Conneau et al , 2020 ) ) . The results are reported in Table 3 . Comparing line 2 with line 3 , we can conclude that an extra semantic encoder is necessary for constructing the universal continuous space among different languages . Moreover , when the large PTM is incorporated , our approach yields further improvements , but it causes massive computational overhead . Comparison between discrete and continuous augmentations . To conduct detailed compar - isons between different augmentation methods , we asymptotically increase the training data to analyze the performance of them on the En De translation . As in Figure 5 , our approach significantly outperforms the back - translation method on each subset , whether or not extra monolingual data ( Sennrich et al , 2016a ) is introduced . These results demonstrate the stronger ability of our approach than discrete augmentation methods on generalizing to unseen instances with the same set of observed data points . Encouragingly , our approach achieves comparable performance with the baseline model with only 25 % of training data , which indicates that our approach has great potential to achieve good results with very few data . Effect of MGRC sampling and tangential contrastive learning . To better understand the effectiveness of the MGRC sampling and the tangential contrastive learning , we conduct detailed ablation studies in Table 5 . The details of four variants with different objectives or sampling strategies are shown in Appendix C. From the results , we can observe that both removing the recurrent dependence and replacing the Gaussian forms with uniform distributions make the translation quality decline , but the former demonstrates more drops . We also have tried the training objectives with other forms , such as variational inference and cosine similarity , to optimize the semantic encoder . However , the BLEU score drops significantly . Training Cost and Convergence . shows the evolution of BLEU scores during training . It is obvious that our method performs consistently better than both the vanilla Transformer and the back - translation method at each iteration ( except for the first 10 K warm - up iterations , where the former one has access to less unique training data than the latter two due to the K times over - sampling ) . For the vanilla Transformer , the BLEU score reaches its peak at about 52 K iterations . In comparison , both CSANMT and the back - translation method require 75 K updates for convergence . In other words , CSANMT spends 44 % more training costs than the vanilla Transformer , due to the longer time to make the NMT model converge with augmented training instances . This is the same as the back - translation method . Word prediction accuracy . Figure 7 illustrates the prediction accuracy of both frequent and rare words . As expected , CSANMT generalizes to rare words better than the vanilla Transformer , and the gap of word prediction accuracy is as large as 16 % . This indicates that the NMT model alleviates the probability under - estimation of rare words via continuous semantic augmentation . Baselines . In contrast to the vanilla Transformer , CSANMT involves with approximate 20 % additional parameters . In this section , we further compare against the baselines with increased amounts of parameters , and investigate the performance of CSANMT equipped with much stronger baselines ( e.g. deep and scale Transformers Wei et al , 2020b ) ) . From the results on WMT14 testsets in Table 6 , we can observe that CSANMT still outperforms the vanilla Transformer ( by more than 1.2 BLEU ) under the same amount of parameters , which shows that the additional parameters are not the key to the improvement . Moreover , CSANMT yields at least 0.9 BLEU gains equipped with much stronger baselines . For example , the scale Transformer , which originally gives 29.3 BLEU in the En De task , now gives 31.37 BLEU with our continuous semantic augmentation strategy . It is important to mention that our method can help models to achieve further improvement , even if they are strong enough .", "entities": [[85, 86, "MethodName", "Transformer"], [94, 95, "MetricName", "BLEU"], [205, 206, "MethodName", "XLM"], [413, 415, "MethodName", "contrastive learning"], [428, 430, "MethodName", "contrastive learning"], [503, 505, "MethodName", "variational inference"], [518, 520, "MetricName", "BLEU score"], [532, 533, "MetricName", "BLEU"], [550, 551, "MethodName", "Transformer"], [600, 601, "MethodName", "Transformer"], [603, 605, "MetricName", "BLEU score"], [646, 647, "MethodName", "Transformer"], [677, 678, "MetricName", "accuracy"], [684, 685, "MetricName", "accuracy"], [704, 705, "MethodName", "Transformer"], [712, 713, "MetricName", "accuracy"], [747, 748, "MethodName", "Transformer"], [803, 804, "DatasetName", "WMT14"], [818, 819, "MethodName", "Transformer"], [824, 825, "MetricName", "BLEU"], [854, 855, "MetricName", "BLEU"], [867, 868, "MethodName", "Transformer"], [873, 874, "MetricName", "BLEU"], [883, 884, "MetricName", "BLEU"]]}
{"text": "We further generalize the capability of the proposed CSANMT to various low - resource machine translation tasks , including IWSLT14 English - German and IWSLT17 English - French . The details of the datasets and model configurations can be found in Appendix B. Table 7 shows the results of different models . Compared to the vanilla Transformer , the proposed CSANMT improve the BLEU scores of the two tasks by 2.7 and 2.9 points , respectively . This result indicates that the claiming of the continuous semantic augmentation enriching the training corpora with very limited observed instances .", "entities": [[14, 16, "TaskName", "machine translation"], [56, 57, "MethodName", "Transformer"], [63, 64, "MetricName", "BLEU"]]}
{"text": "We propose a novel data augmentation paradigm CSANMT , which involves with an adjacency semantic region as the vicinity manifold for each training instance . This method is expected to make more unseen instances under generalization with very limited training data . The main components of CSANMT consists of the tangential contrastive learning and the Mixed Gaussian Recurrent Chain ( MGRC ) sampling . Experiments on both rich - and low - resource machine translation tasks demonstrate the effectiveness of our method . In the future work , we would like to further study the vicinal risk minimization with the combination of multi - lingual aligned scenarios and large - scale monolingual data , and development it as a pure data augmentator merged into the vanilla Transformer . We use the Stanford segmenter ( Tseng et al , 2005 ) for Chinese word segmentation and apply the script tokenizer.pl of Moses ( Koehn et al , 2007 ) for English , German and French tokenization . We measure the performance with the 4gram BLEU score ( Papineni et al , 2002 ) . Both the case - sensitive tokenized BLEU ( compued by multi - bleu.pl ) and the detokenized sacrebleu 3 ( Post , 2018 ) are reported on the En De and En Fr tasks . The case - insensitive BLEU is reported on the Zh En task .", "entities": [[4, 6, "TaskName", "data augmentation"], [51, 53, "MethodName", "contrastive learning"], [73, 75, "TaskName", "machine translation"], [126, 127, "MethodName", "Transformer"], [141, 144, "TaskName", "Chinese word segmentation"], [173, 175, "MetricName", "BLEU score"], [189, 190, "MetricName", "BLEU"], [200, 201, "MetricName", "sacrebleu"], [222, 223, "MetricName", "BLEU"]]}
{"text": "For the low - resource scenario , we choose the IWSLT14 English - German ( En De ) and IWSLT17 English - French ( En Fr ) tasks . Datasets . For IWSLT14 En De , there are 160k sentence pairs for training and 7584 sentence pairs for validation . As in previous work ( Ranzato et al , 2016 ; Zhu et al , 2020 ) , the concatenation of dev2010 , dev2012 , test2010 , test2011 and test2012 is used as the test set . For IWSLT17 En Fr , there are 236k sentence pairs for training and 10263 for validation . The concatenation of test2010 , test2011 , test2012 , test2013 , test2014 and test2015 is used as the test set . We use a joint source and target vocabulary with 10k byte - pair - encoding ( BPE ) types ( Sennrich et al , 2016b ) for above two tasks . Model Settings . The model configuration is transformer_iwslt , representing a 6 - layer model with embedding size 512 and FFN layer dimension 1024 . We train all models using the Adam optimizer with adaptive learning rate schedule ( warm - up step with 4 K ) as in ( Vaswani et al , 2017 ) . During inference , we use beam search with a beam size of 5 and length penalty of 1.0 .", "entities": [[141, 142, "MethodName", "BPE"], [187, 188, "MethodName", "Adam"], [188, 189, "HyperparameterName", "optimizer"], [191, 193, "HyperparameterName", "learning rate"]]}
{"text": "Dialogue systems that aim to acquire user models through interactions with users need to have interviewing functionality . In this study , we propose a method to generate interview dialogues to build a dialogue system that acquires user preferences for food . First , we collected 118 text - based dialogues between the interviewer and customer and annotated the communicative function and semantic content of the utterances . Next , using the corpus as training data , we created a classification model for the communicative function of the interviewer 's next utterance and a generative model that predicts the semantic content of the utterance based on the dialogue history . By representing semantic content as a sequence of tokens , we evaluated the semantic content prediction model using BLEU . The results demonstrated that the semantic content produced by the proposed method was closer to the ground truth than the semantic content transformed from the output text generated by the retrieval model and GPT - 2 . Further , we present some examples of dialogue generation by applying model outputs to template - based sentence generation .", "entities": [[128, 129, "MetricName", "BLEU"], [163, 164, "MethodName", "GPT"], [174, 176, "TaskName", "dialogue generation"]]}
{"text": "As part of the interviewing system , we created a Semantic Content Generation ( SCG ) model that generates the semantic content of the interviewer 's next sentence . The model takes the history of messages of both the interviewer and customer as input and predicts the semantic content of the last sentence in the next interviewer 's message 1 . The representation of semantic content follows the annotation scheme described in Section 3.2 . To train the SCG model , we used a pre - trained Japanese language model 2 of the Transformerbased GPT - 2 model ( Radford et al , 2019 ) , which is commonly used for conversation generation and fine - tuned it using our own small dataset described in Section 3.1 . Figure 3 illustrates GTP - 2 fine tuning to create the SCG model . Each sample of the training data is a pair of dialogue context and semantic content of the interviewer 's next sentence . As the dialogue context , messages preceding the prediction target sentence are concatenated . The end of each context message is indicated by [ SEP ] special token . The maximum number of context messages is five . This 1 When the next interviewer message consists of multiple sentences , the semantic content of the last sentence is used as the prediction target . This is because the main assertion of the message is often made in the last sentence . 2 japanese - gpt2 - small : https://huggingface.co/rinna/japanese - gpt2 - small sequence is concatenated with the semantic content of the prediction target ( the interviewer 's sentence ) and fed to GPT - 2 . The semantic content is represented as a sequence of tokens : verb , object - features , and evaluation description if necessary . Example - 1 in Figure 3 shows an example of object - features consisting of ObjectAttribute and AttributeValue , in which the semantic content of the interviewer 's next sentence is \" [ like , [ ( Dish , pasta , type - of , ? ) ] ] \" ( original sentence : \" What kind of pasta do you like ? \" ) . The verb , ObjectType , ObjectName , Objec - tAttribute , and AttributeValue are concatenated into a sequence . Each of these is separated by a [ SEP ] . Additionally , the < s > and < /s > tokens indicate the beginning and end of each sample , respectively . In Example - 2 , the semantic content contains the evaluation part : \" [ think , [ ( Dish , steak ) ] , [ Evaluation , good ] ] \" ( original sentence : \" Steak is good . \" ) , where the second argument [ Evaluation , good ] is added . Each input sequence is tokenized by the tokenizer , and GPT - 2 optimizes the model weights by minimizing the negative log - likelihood for the next - token prediction .", "entities": [[94, 95, "MethodName", "GPT"], [277, 278, "MethodName", "GPT"], [488, 489, "MethodName", "GPT"], [499, 502, "MetricName", "log - likelihood"]]}
{"text": "We compared the proposed models with two baseline models : the retrieval model and text generation model . Retrieval Model : We simply applied a technique used in information retrieval to a response selection , as proposed in ( Ritter et al , 2011 ; Sordoni et al , 2015b ) . The customer 's message and the interviewer 's response to it were paired as an input - response pair . In the response selection process , among all pairs , the one whose input sentence had the highest similarity to the customer 's input was selected , and the response part of this pair was used as the system 's ( interviewer 's ) response . The sentence vector was a hidden representation of the [ CLS ] token obtained from BERT , and cosine similarity was used to calculate the sentence similarity . Text Generation Model : A GPT - 2 language model was trained using pairs of dialogue context and the next interviewer 's sentence . The difference from the SCG model is that the dialogue context was paired with the text ( not the semantic content ) of the interviewer 's response . Therefore , this model generated an interviewer 's response text rather than were excluded from the dataset . Table 3 : Average BLEU - 4 scores . Numbers in parentheses indicate the length of the dialogue history in the best model using the validation dataset . In the retrieval model , the length of the dialogue history was set to one .", "entities": [[14, 16, "TaskName", "text generation"], [28, 30, "TaskName", "information retrieval"], [133, 134, "MethodName", "BERT"], [146, 148, "TaskName", "Text Generation"], [151, 152, "MethodName", "GPT"], [220, 221, "MetricName", "BLEU"], [241, 243, "DatasetName", "validation dataset"]]}
{"text": "BLEU - 4 score ( standard deviation ) Retrieval 11.5 ( 20.6 ) Text Generation ( N=4 ) 13.0 ( 22.3 ) SCG ( Proposed ) ( N=3 ) 17.3 ( 24.7 ) the semantic content of the sentence .", "entities": [[0, 1, "MetricName", "BLEU"], [13, 15, "TaskName", "Text Generation"]]}
{"text": "To evaluate the output produced by the models , we conducted an automated evaluation using the BLEU with respect to the semantic content . For this purpose , we treated the semantic content of the target interviewer 's sentence as a sequence of words ( e.g. , \" like [ SEP ] Dish [ SEP ] pasta [ SEP ] typeof [ SEP ] ? \" ) and used it as the ground truth . For the SCG model , the BLEU score was calculated by comparing the generated semantic content with the ground truth . For the retrieval model , the semantic content annotation for the response part was compared to the ground truth . For the text generation model , the semantic content was assigned by annotating the generated message and comparing it with the ground truth to calculate the BLEU score . As an evaluation of semantic content consisting of a combination of the verb and object - features , we show the average of BLEU scores using 4grams in the test set in Table 3 . The proposed model achieved the highest BLEU score . We changed the dialogue context length from 1 to 5 and found that a model with a dialogue context length of three achieved the best performance in the validation dataset . These results suggest that the proposed SCG model performed the best in reproducing the semantic content of the interviewer 's message .", "entities": [[16, 17, "MetricName", "BLEU"], [81, 83, "MetricName", "BLEU score"], [118, 120, "TaskName", "text generation"], [142, 144, "MetricName", "BLEU score"], [168, 169, "MetricName", "BLEU"], [186, 188, "MetricName", "BLEU score"], [193, 195, "HyperparameterName", "context length"], [207, 209, "HyperparameterName", "context length"], [217, 219, "DatasetName", "validation dataset"]]}
{"text": "We evaluated the performance of the CFP model by setting the length of the context to three as this setting performed best in the SCG model . The results showed that the model performance for the seven - classes classification was 0.39 in accuracy and 0.30 in weighted average of the F1 score .", "entities": [[43, 44, "MetricName", "accuracy"], [51, 53, "MetricName", "F1 score"]]}
{"text": "In this study , we created a dialogue model to interview the food preferences of users . Text - based dialogues between an interviewer and customer were collected , and the communicative function and semantic content of the interviewer 's utterances were annotated . Using this dataset , we created models to predict the communicative function of the interviewer 's utterances and generate semantic content . The outputs of these two models were then applied to template - based response generation to produce a response . In the model evaluation for generating semantic content , the proposed model outperformed the two baseline models , retrieval and generative , in the automatic evaluation using BLEU - 4 . As future work , we will improve the response generation mechanism to generate a variety of expressions because the current template - based response generation may not be sufficient in its expressiveness . For example , one of the ideas would be presenting candidates such as Japanese , Chinese , and Italian when asking about preferences for a genre and asking the user to select one . It would also be useful to predict the user 's preference based on the dialog history and user information and generate questions such as \" Do you prefer Chinese to Italian ? Thus , by using question content ( e.g. , genre ) and related vocabulary and knowledge ( Chinese and Italian as examples of genre ) , the question variation can be increased . Another possibility is to automatically extract or determine the response templates through machine learning , but this is a challenging task . Further , a user study should be conducted , as it is known that automatic evaluation using BLEU does not always correlate with human evaluation ( Liu et al , 2016 ) . In the user study , users interact with the system , and then they evaluate the quality of the responses generated from the system , and judge whether the system effectively elicits information from the user .", "entities": [[79, 81, "TaskName", "response generation"], [113, 114, "MetricName", "BLEU"], [125, 127, "TaskName", "response generation"], [140, 142, "TaskName", "response generation"], [288, 289, "MetricName", "BLEU"]]}
{"text": "We treat the task of categorizing narratives related to sexual abuse disclosure - Stance , Hate Speech , 3 Implementation used for BERTweet is available here Sarcasm and Dialogue Acts , independently . Each STL model is given an input representation e ( Equation 1 ) . Within the proposed tasks for classifying sexual abuse disclosure narrative for the tweets related to the # MeToo movement ( Section 3 ) , we use sigmoid activation for Sarcasm detection ( whose classification outputs are binary ) and softmax activation for all other tasks for the final output layer . Model Optimization To account for the imbalance present among the labels , we use classbalanced focal loss as the optimization loss function ( Cui et al , 2019 ) , as formulated in Equation 5 . Given a sample class i containing n i samples in total , it adds a weighting factor of ( 1\u2212\u03b2 ) ( 1\u2212\u03b2 n i ) with parameters \u03b2 [ 0 , 1 ) , where n y is the number of samples in the ground truth class y. The proposed class - balanced term is model agnostic . p represents predicted class probabilities and L represents the choice of the loss function ( binary cross entropy for Sarcasm and categorical cross entropy for others ) . CB ( p , y ) = 1 \u2212 \u03b2 1 \u2212 \u03b2 ny L ( p , y ) ( 5 ) As for the multilabel emotion classification task , the unnormalized output ( assuming one or more of 11 different emotions ) is subjected to a Sigmoid activation , and the network is optimized using binary cross - entropy ( BCE ) as : LBCE = \u2212 1 N N i=1 yi.log ( p ( yi ) ) + ( 1 \u2212 yi ) .log ( 1 \u2212 p ( yi ) ) ( 6 ) where N is the number of training samples , y and p ( y ) denotes true and predicted labels respectively .", "entities": [[15, 17, "DatasetName", "Hate Speech"], [73, 75, "MethodName", "sigmoid activation"], [76, 78, "TaskName", "Sarcasm detection"], [86, 87, "MethodName", "softmax"], [113, 115, "MethodName", "focal loss"], [118, 119, "MetricName", "loss"], [162, 163, "HyperparameterName", "\u03b2"], [164, 165, "DatasetName", "0"], [174, 177, "HyperparameterName", "number of samples"], [205, 206, "MetricName", "loss"], [230, 231, "HyperparameterName", "\u03b2"], [233, 234, "HyperparameterName", "\u03b2"], [248, 250, "TaskName", "emotion classification"], [269, 271, "MethodName", "Sigmoid activation"]]}
{"text": "For our MTL approach , we use two optimization objectives : one for the primary task , which can be any of the proposed tasks for classifying tweets related to # MeToo movement ( Section 3 ) , and other for the auxiliary task , which can be either a task related to classifying sexual abuse disclosure for # MeToo movement ( Homogeneous MTL ) or emotion classification task ( Heterogeneous MTL ) . The two objectives are weighted by a parameter \u03b3 , which controls the importance placed on the auxiliary task ( 1 \u2212 \u03b3 for the primary task ) . Multitask learning frameworks are generally built using either of these two approaches : hard parameter sharing or soft parameter sharing . In a hard parameter sharing model ( Caruana , 1997 ) , both the primary and auxiliary tasks have a shared encoder followed by separate task - specific network branches , and the shared encoder is updated by both the tasks alternately . On the other hand , in the soft parameter sharing approach , tasks have different encoders with independent parameters , and the distance between their parameters is regularized using a regularization constraint ( Duong et al , 2015 ; Yang and Hospedales , 2016 ) , to encourage the parameters to be similar . Flexible Cross - Stitched Parameter Sharing Architecture : We design our model so that the task - agnostic textual feature representations benefit from hard sharing while the regularization of the task - specific features can be learned according to task pair settings . We call our approach flexible cross - stitched parameter sharing , presented in Figure 2 . Specifically , we train two separate models ( one for each task ) in tandem while also having a shared encoder that is updated by both of them and weighted joint learning of primary task decoder parameters that are tuned specifically for the task . This allows both the models to have their own set of parameters while also encouraging knowledge transfer via the shared encoder weights . For each training pass of the primary task , the input representation e ( p ) is passed through ( a ) stacked BiLSTM encoder and ( b ) stacked shared BiLSTM encoder . This results in two contextualized word representations ( h ( p ) 1 , h ( p ) 2 , ... h ( p ) n ) and ( h ( s ) 1 , h ( s ) 2 , ... h ( s ) n ) , where superscript ( p ) is used to denote the representations resulting from encoder in the primary task model and superscript ( s ) is used to denote the ones from shared encoder . We calculate the weighted summation of these two representations - h ( p ) , using two learnable parameters , \u03b1 ( p ) and \u03b1 ( s ) ( where \u03b1 ( p ) + \u03b1 ( s ) = 1 ) , as formulated in Equation 7to regulate the information resulting from the two encoders ( Figure 2 ) . h ( p ) = \u03b1 ( p ) h ( p ) + \u03b1 ( s ) h ( s ) ( 7 ) Such an approach to aggregate information flow from two encoders has facilitated success in prior Multitask learning settings as well ( Rajamanickam et al , 2020 ; Dankers et al , 2019 ) . As for our auxiliary task , we pass the embeddings e ( a ) through only the shared encoder ( h ( a ) = h ( s ) ) , followed by a dropout layer . We use this architecture for Heterogeneous MTL experiments . For Homogeneous MTL ones , we employ hard parameter sharing model due to statistical out - performance in this scenario . This technique consists of a single stacked encoder that is shared and updated by both tasks related to identifying narratives related to sexual abuse disclosures within # MeToo movement , followed by task - specific branches . The shared representations from the encoder are passed through the dropout layer . These output representations ( in the case of both Homogeneous and Heterogeneous experiments ) are passed through respective BiLSTM decoders and dropout layers to get the final representation m ( p ) and m ( a ) , respectively for both the tasks . The auxiliary network branch is optimized using either Equation 5 ( Class Balanced Focal Loss ) or Equation 6 ( Binary Cross Entropy ) , depending upon whether the auxiliary task is associated with identifying sexual abuse disclosure narratives or emotions . These output representations m ( p ) and m ( a ) are passed through a linear output layer to get unnormalized outputs o ( p ) and o ( a ) respectively . Sigmoid activation function is used for Sarcasm detection and the emotion classification task , and Softmax activation for others .", "entities": [[66, 68, "TaskName", "emotion classification"], [82, 83, "HyperparameterName", "\u03b3"], [96, 97, "HyperparameterName", "\u03b3"], [371, 372, "MethodName", "BiLSTM"], [379, 380, "MethodName", "BiLSTM"], [485, 486, "HyperparameterName", "\u03b1"], [490, 491, "HyperparameterName", "\u03b1"], [496, 497, "HyperparameterName", "\u03b1"], [501, 502, "HyperparameterName", "\u03b1"], [532, 533, "HyperparameterName", "\u03b1"], [541, 542, "HyperparameterName", "\u03b1"], [721, 722, "MethodName", "BiLSTM"], [760, 762, "MethodName", "Focal Loss"], [823, 825, "MethodName", "Sigmoid activation"], [829, 831, "TaskName", "Sarcasm detection"], [833, 835, "TaskName", "emotion classification"], [838, 839, "MethodName", "Softmax"]]}
{"text": "MTL framework traditionally improves generalization by leveraging the domain - specific information due to the relatedness of the tasks present in the training signals ( Caruana , 1997 ) ; hence we use two publicly available datasets mined from Twitter : n ) identify BERTweet word - level embeddings for the primary and auxiliary task respectively . The different arrows are used to indicate the alternate passes of the primary task ( solid arrows ) and auxiliary task ( dotted arrows ) . Two controllable parameters \u03b1 ( p ) and \u03b1 ( s ) are used to control information flow from task - specific and shared encoder respectively , for the primary task . Sexual Abuse Disclosures - # MeTooMA This dataset 4 has 9 , 973 tweets and covers different mutually non - exclusive linguistic annotations related to the # MeToo movement . The distribution and statistics about various labels are present in Table 1 and Section 3 . We present an instance associated with each of the proposed tasks in Table 1 . For our experiments , we focus only on tweets that are annotated as relevant to the # MeToo movement . Emotions - SemEval18 This dataset 5 has been taken from SemEval - 2018 Task - 1 ( Mohammad et al , 2018 ) and covers emotion - specific labels representing the mental state of the authors of the tweets . It consists of 10 , 986 tweets distributed across 11 emotion labels - ( anger , disgust , anticipation , fear , joy , love , optimism , pessimism , sadness , surprise and trust ) , each being a binary label to indicate the presence of a particular emotion .", "entities": [[86, 87, "HyperparameterName", "\u03b1"], [91, 92, "HyperparameterName", "\u03b1"], [119, 121, "DatasetName", "# MeTooMA"], [221, 222, "DatasetName", "emotion"], [246, 247, "DatasetName", "emotion"], [285, 286, "DatasetName", "emotion"]]}
{"text": "Preprocessing We pre - process tweet text by ( i ) normalizing user mentions and URLs , and ( ii ) translating the emoticon into text ( Hutto and Gilbert , 2014 ) . For tokenization , we use Tweet Tokenizer from NLTK . 6 Hyperparameters For our model 7 hyperparameters were tuned on the validation set to find the best configurations . We use a pre - trained BERTweet model to extract 768 - dimensional token - level embeddings . Grid search was performed to find the optimal value of hyperparameters and their range is summarized as : size of BiLSTM and dense layers { 128 , 256 , 512 } , embedding size d { 100 , 200 , 300 } , dropout \u03b4 { 0.1 , 0.2 , 0.3 , 0.4 , 0.5.0.6 } , learning rate \u03bb { 10 \u22125 , 10 \u22124 , 10 \u22123 , 10 \u22122 , 10 \u22121 } , weight decay \u03c9 { 10 \u22126 , 10 \u22125 , 10 \u22124 , 10 \u22123 } , optimizer { Adam , Adadelta } , batch size b { 32 , 64 , 128 } and epochs ( < 100 ) . For the MTL experiments , we tune the weightage of the auxiliary task ( \u03b3 [ 0.1 , 0.9 ] with intervals of 0.1 ) for each task pair . For each task associated with identifying narratives pertaining to the # MeToo movement in the MTL setup , its value is considered as the one where the model performance improved the most and for both the tasks . For instance , we find the optimal value of \u03b3 for hate speech ( as the auxiliary task ) to be 0.4 in all Homogeneous task cases and of emotion detection to be 0.2 for the Heterogeneous tasks . For the MTL experiments , \u03b1 p and \u03b1 s are learnable and tuned on the validation loss . The encoders consist of two stacked BiLSTM 's with hidden size = 128 . BiLSTM classifier has hidden size = 256 , and the number of units in the penultimate dense layer is 128 . Dropout is set to 0.3 . For all our experiments , we use Adam optimizer ( Kingma and Ba , 2014 ) and initialize model weights using Xavier initialization ( Glorot and Bengio , 2010 ) . We set the batch size to 128 and the learning rate to 1e \u2212 3 . Training All models were trained until convergence for both primary and auxiliary tasks . For our MTL experiments , the training process involves alternating between primary and auxiliary task steps , with each task having its own loss function . All experiments are run using stratified 5 - fold crossvalidation . We report the average macro F1 scores across the 5 folds to account for imbalance , as previously used in multi - label settings ( Zhang and Zhou , 2013 6 Results and Discussion", "entities": [[101, 102, "MethodName", "BiLSTM"], [125, 126, "HyperparameterName", "\u03b4"], [138, 140, "HyperparameterName", "learning rate"], [158, 160, "MethodName", "weight decay"], [175, 176, "HyperparameterName", "optimizer"], [177, 178, "MethodName", "Adam"], [179, 180, "MethodName", "Adadelta"], [182, 184, "HyperparameterName", "batch size"], [213, 214, "HyperparameterName", "\u03b3"], [276, 277, "HyperparameterName", "\u03b3"], [278, 280, "DatasetName", "hate speech"], [296, 297, "DatasetName", "emotion"], [311, 312, "HyperparameterName", "\u03b1"], [314, 315, "HyperparameterName", "\u03b1"], [323, 324, "MetricName", "loss"], [331, 332, "MethodName", "BiLSTM"], [339, 340, "MethodName", "BiLSTM"], [349, 352, "HyperparameterName", "number of units"], [360, 361, "MethodName", "Dropout"], [373, 374, "MethodName", "Adam"], [374, 375, "HyperparameterName", "optimizer"], [387, 389, "MethodName", "Xavier initialization"], [400, 402, "HyperparameterName", "batch size"], [406, 408, "HyperparameterName", "learning rate"], [450, 451, "MetricName", "loss"], [468, 470, "MetricName", "macro F1"]]}
{"text": "Learning the affective states in the # MeTooMA dataset is challenging due to the inherently subjective nature of the tweets coupled with limitations on the data 's size . Multitask learning achieves significant performance gains in terms of macro F1 score , as shown in Homogeneous MTL with row identifying primary task and columns denoting auxiliary task . The higher performance of Homogeneous MTL can be inferred to be indicative of better generalization when pairs of tasks are jointly modeled . Interestingly , these tasks show their best performance with the selective counterparts in the Homogeneous MTL setup . Stance detection is strongly coupled with Sarcasm labeling , and the same is seen to be true for Hate Speech classification and Stance identification . This selective out - performance of specific pairs of tasks can be attributed to the high correlation between the tasks themselves ( Frenda , 2018 ; . For instance , the offensive text is often strongly coupled with sarcasm , as wit is a common linguistic denominator for understanding the intended meaning of phrases related to anger ( Badlani et al , 2019 ) . We further detail this through examples in Section 6.4 .", "entities": [[6, 8, "DatasetName", "# MeTooMA"], [38, 40, "MetricName", "macro F1"], [99, 101, "TaskName", "Stance detection"], [117, 119, "DatasetName", "Hate Speech"]]}
{"text": "Overnight The Overnight semantic parsing dataset ( Wang et al , 2015 ) consists of 13 , 682 natural utterance , canonical form , meaning representation triples split across eight domains . To simulate low - resource splits of this dataset , we follow Shin et al and create randomly subsampled splits of 200 training examples for each domain , using 20 % of the remaining data for validation . We measure and report denotation accuracy by evaluating all predicted queries using the SEMPRE toolkit ( Berant et al , 2013 ) . We repeat each experiment on Overnight with five different random splits . TOPv2 Chen et al ( 2020 ) introduce the TOPv2 dataset , a task - oriented semantic parsing dataset with eight domains , two of which come with predefined low - resource splits . The authors propose a principled way of constructing low - resource training sets , samples per intent and slot ( SPIS ) , intended to ensure equal exposure to ontology labels across domains of varying complexity . We experiment with the weather and reminder domains at the 10 , 25 , and 500 SPIS resource splits , performing five runs on each model varying the random seed . The reminder domain is the most challenging with 19 intent labels , 32 slot labels , and with 21 % of the programs having a depth greater than 2 . Weather in comparison has 7 intent labels , 11 slot labels , and no programs with depth greater than 2 .", "entities": [[3, 5, "TaskName", "semantic parsing"], [75, 76, "MetricName", "accuracy"], [105, 106, "DatasetName", "TOPv2"], [114, 115, "DatasetName", "TOPv2"], [121, 123, "TaskName", "semantic parsing"], [168, 169, "MethodName", "ontology"]]}
{"text": "Prompt tuning , as proposed by Lester et al ( 2021 ) , prepends a sequence of continuous embeddings p = ( p 1 , . . . , p K ) to the sequence input embeddings e ( u ) = ( e ( u 1 ) , . . . , e ( u N ) ) before feeding it to a language model with parameters \u03b8 . During prompt tuning we optimize the prompt embeddings ( p 1 , . . . , p K ) exclusively , keeping the language model parameters \u03b8 and the pretrained vocabulary embeddings fixed . Note that this process still requires backpropagating gradients through the full language model . Like fine - tuning models , we maximize the likelihood of generating the output sequence z.", "entities": [[68, 69, "HyperparameterName", "\u03b8"], [96, 97, "HyperparameterName", "\u03b8"]]}
{"text": "We find that prompt tuning improves over finetuning for all large model configurations and target representations . On Overnight , prompt tuned denotation accuracy exceeds fine - tuned counterparts by up to 5 points with T5 - large and T5 - xl . For T5 - small and T5 - base , prompt tuning remains competitive ( within 1 % average accuracy ) with fine - tuning when predicting canonical forms . On TOPv2 , prompt tuning achieves an absolute improvement of 15 % mean accuracy over fine - tuning on the lowest SPIS split . This performance disparity lessens when training data increases ; however , prompt tuned T5 - large continues to beat its finetuned counterpart by 5 points at 500 SPIS and the BART - CopyPtr model by 1.4 points . Our prompt tuning models outperform previously reported results on these datasets . On Overnight , our best model - T5 - xl PT with canonical representations and constrained decodingoutperforms the BART FT model of Shin et al ( 2021 ) by 5 accuracy points , and GPT - 3 by more than 2 points . On the 25 SPIS split of TOPv2 , we see an average improvement of more than 5 points compared to the BART - CopyPTR of Chen et al ( 2020 ) .", "entities": [[23, 24, "MetricName", "accuracy"], [35, 36, "MethodName", "T5"], [39, 40, "MethodName", "T5"], [44, 45, "MethodName", "T5"], [48, 49, "MethodName", "T5"], [60, 62, "MetricName", "average accuracy"], [73, 74, "DatasetName", "TOPv2"], [85, 86, "MetricName", "accuracy"], [109, 110, "MethodName", "T5"], [126, 127, "MethodName", "BART"], [153, 154, "MethodName", "T5"], [164, 165, "MethodName", "BART"], [176, 177, "MetricName", "accuracy"], [180, 181, "MethodName", "GPT"], [195, 196, "DatasetName", "TOPv2"], [210, 211, "MethodName", "BART"]]}
{"text": "Here we provide all model details and hyperparameters to reproduce our results . We experiment with BART and T5 ( Lewis et al , 2020 ; Raffel et al , 2020 ) , two large pre - trained encoder - decoder language models . BART is trained on the same 160 GB text dataset used to train RoBERTa ( Lewis et al , 2020 ) ( Paszke et al , 2019 ; Wolf et al , 2020 ) . Fine - tuning baseline We compare against baselines that fine - tune all parameters of BART and T5 . We train the T5 models with AdaFactor ( Shazeer and Stern , 2018 ) and BART with Adam ( Lewis et al , 2020 ; Kingma and Ba , 2015 ) . On TOPv2 , we use a learning rate of 10 \u22124 and batch size of 128 . On Overnight , we use a learning rate of 10 \u22123 and a batch size of 64 across all sizes of T5 . On both datasets , we train for 5000 epochs and perform model selection by early stopping on the validation set . Prompt tuning We follow the prompt tuning procedure proposed by Lester et al for T5 . We use 150 prompt tokens for all model sizes with a learning rate of 0.3 optimized with AdaFactor . We train for 5000 epochs on most domains , although it sometimes took as many as 20000 epochs to converge on the low - resource splits . Like the fine - tuned baseline , we perform model selection with best exact match accuracy on the validation set . We apply the same method to BART and found that it did not converge under a number of hyperparameter configurations . We therefore exclude prompt tuned BART models from our results 1 . Constrained Decoding We implement grammarconstrained decoding by building a prefix tree containing all canonical or meaning representations in the dataset as in Shin et al ( 2021 ) . When doing constrained decoding we perform a beam search with 10 beams and use the prefix tree to look up valid single token continuations of the decoded sequence .", "entities": [[16, 17, "MethodName", "BART"], [18, 19, "MethodName", "T5"], [44, 45, "MethodName", "BART"], [57, 58, "MethodName", "RoBERTa"], [94, 95, "MethodName", "BART"], [96, 97, "MethodName", "T5"], [101, 102, "MethodName", "T5"], [104, 105, "MethodName", "AdaFactor"], [113, 114, "MethodName", "BART"], [115, 116, "MethodName", "Adam"], [131, 132, "DatasetName", "TOPv2"], [136, 138, "HyperparameterName", "learning rate"], [142, 144, "HyperparameterName", "batch size"], [153, 155, "HyperparameterName", "learning rate"], [160, 162, "HyperparameterName", "batch size"], [168, 169, "MethodName", "T5"], [181, 183, "TaskName", "model selection"], [184, 186, "MethodName", "early stopping"], [205, 206, "MethodName", "T5"], [218, 220, "HyperparameterName", "learning rate"], [224, 225, "MethodName", "AdaFactor"], [262, 264, "TaskName", "model selection"], [266, 268, "MetricName", "exact match"], [268, 269, "MetricName", "accuracy"], [280, 281, "MethodName", "BART"], [300, 301, "MethodName", "BART"]]}
{"text": "Our objective naturally rises from the model we described in the text . Furthermore , as our experiments show , it is easy to generalize this objective , to a \" semi - supervised \" setting , in which the learner has access to only a few fully labeled examples and additional partially labeled examples . E.g. , if only segmentation is annotated but the type information is missing . The loss function is a linear combination of the negative log probability of each sub - tasks , together with the decision module : J = \u2212 N i log P ( y i | x i ) + \u03b1 log P ( y seg ( i ) | x ( i ) ) + \u03b2 log P ( y typ ( i ) | x ( i ) ) , ( 1 ) where N is the number of examples in the training set , y seg and y typ are the decomposed segmentation and type tags corresponding to the two sub - task modules , and \u03b1 and \u03b2 are the hyperparameters controlling the importance of the two modules contributions respectively . If the training example is fully labeled with both segmentation and type annotated , training is straightforward ; if the training example is partially labeled , e.g. , only with segmentation but without type , we can set the log probability of the type module and the decision module 0 and only train the segmentation module . This formulation provides extra flexibility of using partially annotated corpus together with fully annotated corpus to improve the overall performance .", "entities": [[71, 72, "MetricName", "loss"], [109, 110, "HyperparameterName", "\u03b1"], [125, 126, "HyperparameterName", "\u03b2"], [178, 179, "HyperparameterName", "\u03b1"], [180, 181, "HyperparameterName", "\u03b2"], [243, 244, "DatasetName", "0"]]}
{"text": "We evaluated our models over three different sentiment analysis tasks adapted for sequence prediction . We included additional results for multilingual NER in the Appendix for reference . Target Sentiment Datasets We evaluated our models on the targeted sentiment dataset released by Mitchell et al ( 2013 ) , which consists of entity and sentiment annotations on both English and Spanish tweets . Similar to previous studies ( Mitchell et al , 2013 ; Zhang et al , 2015 ; Li and Lu , 2017 ) , our task focuses on people and organizations ( collapsed into volitional named entities tags ) and the sentiment associated with their description in tweets . After this processing , the labels of each tweets are composed of both segmentation ( entity spans ) and types ( sentiment tags ) . We used the original 10 - fold cross validation splits to calculate averaged F1 score , using 10 % of the training set for development . We used the same metrics in Zhang et al ( 2015 ) and Li and Lu ( 2017 ) for a fair comparison .", "entities": [[7, 9, "TaskName", "sentiment analysis"], [21, 22, "TaskName", "NER"], [150, 152, "MetricName", "F1 score"]]}
{"text": "Following previous studies ( Ma and Hovy , 2016 ; Liu et al , 2018 ) showing that the word embedding choice can significantly influence performance , we used the pre - trained GloVe 100 dimension Twitter embeddings only for all tasks in the main text . All the words not contained in these embeddings ( OOV , out - of - vocabulary words ) are treated as an \" unknown \" word . Our models were deployed with minimal hyper parameters tuning , and can be briefly summarized as : the character embeddings has dimension 30 , the hidden layer dimension of the character level LSTM is 25 , and the hidden layer of the word level LSTM has dimension 300 . Similar to Liu et al ( 2018 ) , we also applied highway networks ( Srivastava et al , 2015 ) from the character level LSTM to the word level LSTM . In our pilot study , we shrank the number of parameters in our modular architectures to around one third such that the total number of parameter is similar as that in the LSTM - CRF model , but we did not observe a significant performance change so we kept them as denoted . The values of \u03b1 and \u03b2 in the objective function were always set to 1.0 .", "entities": [[33, 34, "MethodName", "GloVe"], [106, 107, "MethodName", "LSTM"], [118, 119, "MethodName", "LSTM"], [135, 137, "MethodName", "highway networks"], [148, 149, "MethodName", "LSTM"], [153, 154, "MethodName", "LSTM"], [163, 166, "HyperparameterName", "number of parameters"], [187, 188, "MethodName", "LSTM"], [189, 190, "MethodName", "CRF"], [211, 212, "HyperparameterName", "\u03b1"], [213, 214, "HyperparameterName", "\u03b2"]]}
{"text": "We used BIOES tagging scheme but only during the training and convert them back to BIO2 for evaluation for all tasks 3 . Our model was implemented using pytorch ( Paszke et al , 2017 ) . To help improve performance we parallelized the for - ward algorithm and the Viterbi algorithm on the GPU . All the experiments were run on NVIDIA GPUs . We used the Stochastic Gradient Descent ( SGD ) optimization of batch size 10 , with a momentum 0.9 to update the model parameters , with the learning rate 0.01 , the decay rate 0.05 ; The learning rate decays over epochs by \u03b7/ ( 1 + e * \u03c1 ) , where \u03b7 is the learning rate , e is the epoch number , and \u03c1 is the decay rate . We used gradient clip to force the absolute value of the gradient to be less than 5.0 . We used early - stop to prevent over - fitting , with a patience of 30 and at least 120 epochs . In addition to dropout , we used Adversarial Training ( AT ) ( Goodfellow et al , 2014 ) , to regularize our model as the parameter numbers increase with modules . AT improves robustness to small worst - case perturbations by computing the gradients of a loss function w.r.t . the input . In this study , \u03b1 and \u03b2 in Eq . 1 are both set to 1.0 , and we leave other tuning choices for future investigation .", "entities": [[68, 71, "MethodName", "Stochastic Gradient Descent"], [72, 73, "MethodName", "SGD"], [76, 78, "HyperparameterName", "batch size"], [92, 94, "HyperparameterName", "learning rate"], [97, 99, "HyperparameterName", "decay rate"], [102, 104, "HyperparameterName", "learning rate"], [121, 123, "HyperparameterName", "learning rate"], [127, 129, "HyperparameterName", "epoch number"], [134, 136, "HyperparameterName", "decay rate"], [224, 225, "MetricName", "loss"], [235, 236, "HyperparameterName", "\u03b1"], [237, 238, "HyperparameterName", "\u03b2"]]}
{"text": "The complete results of our experiments on the target sentiment task are summarized in Tab . 4 . Our LSTM - CRF - TI ( g ) model outperforms all the other competing models in Precision , Recall and the F1 score .", "entities": [[19, 20, "MethodName", "LSTM"], [21, 22, "MethodName", "CRF"], [35, 36, "MetricName", "Precision"], [37, 38, "MetricName", "Recall"], [40, 42, "MetricName", "F1 score"]]}
{"text": "The proposed twofold modular infusion model ( with guided gating as an option ) breaks the complex learning problem into several sub - problems and then integrate them using joint training . The process defined by this formulation has more parameters and requires learning multiple objectives jointly . Our convergence analysis intends to evaluate whether the added complexity leads to a harder learning problem ( i.e. , slower to converge ) or whether the tasks constrain each other and as a result can be efficiently learned . We compare between our LSTM - CRF - TI ( g ) model and recent published top models on the English NER dataset in Figure 10 and on the subjec - tive polarity disambiguation datasets in Figure 11 . The curve compares convergence speed in terms of learning epochs . Our LSTM - CRF - TI ( g ) model has a much faster convergence rate compared to the other models . Figure 11 : Comparing convergence over the development set on the subjective polarity disambiguation datasets . The x - axis is number of epochs and the y - axis is the F1 - score .", "entities": [[91, 92, "MethodName", "LSTM"], [93, 94, "MethodName", "CRF"], [108, 109, "TaskName", "NER"], [138, 139, "MethodName", "LSTM"], [140, 141, "MethodName", "CRF"], [180, 183, "HyperparameterName", "number of epochs"], [190, 193, "MetricName", "F1 - score"]]}
{"text": "Aggression and related activities like trolling , hate speech etc . involve toxic comments in various forms . These are common scenarios in today 's time and websites react by shutting down their comment sections . To tackle this , an algorithmic solution is preferred to human moderation which is slow and expensive . In this paper , we propose a single model capsule network with focal loss to achieve this task which is suitable for production environment . Our model achieves competitive results over other strong baseline methods , which show its effectiveness and that focal loss exhibits significant improvement in such cases where class imbalance is a regular issue . Additionally , we show that the problem of extensive data preprocessing , data augmentation can be tackled by capsule networks implicitly . We achieve an overall ROC AUC of 98.46 on Kaggletoxic comment dataset and show that it beats other architectures by a good margin . As comments tend to be written in more than one language , and transliteration is a common problem , we further show that our model handles this effectively by applying our model on TRAC shared task dataset which contains comments in code - mixed Hindi - English .", "entities": [[7, 9, "DatasetName", "hate speech"], [63, 65, "MethodName", "capsule network"], [66, 68, "MethodName", "focal loss"], [96, 98, "MethodName", "focal loss"], [124, 126, "TaskName", "data augmentation"], [138, 140, "MetricName", "ROC AUC"]]}
{"text": "In today 's time , with an ever increasing penetration of social media , news portals , blogs , QnA forums , and other websites that allow user interaction , users often end up inviting comments that are nasty , harrasing , insulting , toxic etc . This can have adverse effects on users , who then become victims of cyberbullying or online harrasment . An online survey carried out by the Pew Research Centre in 2017 states that 4 in 10 Americans have personally experienced online harrasment . Strikingly , 1 in 5 Americans have witnessed severe form of online harrasment like physical threats , stalking , sexual harrasment etc . There are several challenges associated with solving this kind of problem . First being the problem of class imbalance found in the dataset . Since such type of comments are sparse in nature , they introduce skewness in the dataset . There are several ways to handle this problem , however , we choose a more recent technique which modifies the standard cross entropy loss function known as Focal Loss ( Lin et al , 2017 ) . We will briefly describe how it helps in improving classifier performance . The next problem we want to address is that of data preprocessing . This is the most time consuming task and requires a good understanding of the data . However , we wish to minimise this process so as to have a good model with minimal preprocessing of the data . Another frequently observed challenge is transliteration , which is often observed , especially , when we are working with text data from social networking websites . Users tend to speak in more than one language in the same statement . This leads to several out of vocabulary or OOV words for which the model would not have any word embedding . We use randomly initialised word embeddings in such a case and show how they can be trained during model training procedure such that it results in clusters of OOV words which have similar meaning in Hindi . We propose to tackle all the above described challenges using a single model as opposed to ensemble of several other models , which is a common practice in such competitive challenges . We also show that our proposed model can converge really quickly , hence the model can be trained in lesser time . This is essential when the model has to be deployed in a production environment where it requires retraining periodically .", "entities": [[176, 177, "MetricName", "loss"], [180, 182, "MethodName", "Focal Loss"], [318, 320, "TaskName", "word embeddings"]]}
{"text": "Proposed Model : The model proposed in ( Zhao et al , 2018 ) has been used for the experimentation with an inclusion of Focal Loss ( Lin et al , 2017 ) as a loss function to address the class imbalance problem . In our experiments we have compared performances of CNNs and RNNs as feature extractors and found that sentence representation obtained from RNNs performs better than representations obtained after applying convolution operation , although CNNs tends to perform better on short texts . The model consists of four layers : ( i ) Word Embedding Layer : We represent every comment x i , as a sequence of one - hot encoding of its words , x i = ( w 1 , w 2 , ... w n ) of length n max , which is the maximum length of the comment , with zero padding . Such a sequence becomes the input to the embedding layer . To represent word tokens several ideas like sparse representation or dense representation ( Collobert and Weston , 2008 ; Bengio et al , 2003 ) have been proposed . ( ii ) Feature Extraction Layer : This layer has been used to extract either n - grams feature at different position of a sentence through different filters ( CNNs ) or long term temporal dependencies within the sentence ( RNNs ) . We use RNNs as feature extractors in our final model . ( iii ) Capsule Layer : The Capsule layer is primarily composed of two sub - layers Primary Capsule Layer and Convolutional Capsule Layer . The primary capsule layer is supposed to capture the instantiated parameters of the inputs , for example , in case of texts local order of words and their semantic representation . Suppose we have\u00ea number of feature extractors , then the input to the Primary capsule layer will be Z R n\u00d7\u00ea ( where n is the number of timesteps in RNNs ) . The primary capsules transform a scalar - output feature detector to vector - valued capsules to capture the instantiated features . Let d be the dimension of each capsule , then for each capsule p i R d , where p denotes instantiated parameters set of a capsule ( Sabour et al , 2017 ) , we have p i = g ( W Z i + b ) , where Z i is captured by RNNs in the feature extractor layer . Here , g is the nonlinear squash function which shrinks the small vectors to around 0 and large vectors around 1 . ( iv ) The Convolutional Capsule : The Conv layers capsules output a local grid of vectors to capsules in earlier layers using different transformation matrices for each capsule and grids member ( Sabour et al , 2017 ) .Capsule networks are trained using a dynamic routing algorithm that overlooks words that are not important or unrelated in the text , like stopwords and name mentions . F L ( p t ) = \u2212\u03b1 t ( 1 \u2212 p t ) \u03b3 log ( p t ) , where p t = { p if y = 1 1 \u2212 p else \u03b3 is", "entities": [[24, 26, "MethodName", "Focal Loss"], [35, 36, "MetricName", "loss"], [73, 74, "MethodName", "convolution"], [327, 330, "HyperparameterName", "number of timesteps"], [432, 433, "DatasetName", "0"], [521, 522, "HyperparameterName", "\u03b3"], [542, 543, "HyperparameterName", "\u03b3"]]}
{"text": "For all our experiments we have used pre - trained embeddings for each word token obtained from ( Joulin et al , 2016 ) . We have also exploited ( Pennington et al , 2014 ) , ( Mikolov et al , 2013 ) , random and manually trained embeddings for initialization . After experimentation , fasttext embeddings with dimension of 300 were found to perform better than rest of the initialization process . In our experiments we observed that RMSProp ( Tieleman and Hinton , 2012 ) and Adam ( Kingma and Ba , 2014 ) as an optimizer works well for training RNNs and CNNs respectively and used this throughout . The learning rate was kept between [ .1 and .001 ] . For CNNs , number of Kernels was chosen from the range [ 128 , 256 , 512 ] and the LSTM units were selected from the range [ 128 , 256 ] . In all of our experiments with the proposed model only a single layer for feature extraction was used . Number of capsules was varied from [ 8 , 10 , 16 ] , the vector length of 8 for each capsule was found to be the best , and the dropout values for RNNs were taken as per suggestions from ( Zaremba et al , 2014 ) . The \u03b1 and \u03b3 values in focal loss were experimented for [ 1.5 , 2 , 2.5 , 3 , 3.5 ] and [ .2 , .25 , .3 ] and finally \u03b1 = 2 and \u03b3=0.25 were taken .", "entities": [[56, 57, "MethodName", "fasttext"], [80, 81, "MethodName", "RMSProp"], [89, 90, "MethodName", "Adam"], [99, 100, "HyperparameterName", "optimizer"], [114, 116, "HyperparameterName", "learning rate"], [145, 146, "MethodName", "LSTM"], [227, 228, "HyperparameterName", "\u03b1"], [229, 230, "HyperparameterName", "\u03b3"], [232, 234, "MethodName", "focal loss"], [258, 259, "HyperparameterName", "\u03b1"]]}
{"text": "The proposed CapsNet architecture was able to beat other strong baseline algorithms with reasonable difference in accuracies with minimal preprocessing . We demonstrated that using focal loss along with CapsNet gave us .25 raise in the ROC - AUC for Kaggle 's toxic comment classification and 1.39 and .80 gain in F1 scores on TRAC shared task dataset in English , from Facebook and Twitter comments respectively . All of our experiments were performed on NVIDIA Quadro M1200 4096 MB GPU system with 32 GB RAM and Intel i7 processor . The model took almost 33 minutes for an epoch to train which was faster in comparison with other models , with exception to the models using CNNs as feature extractors . For example , the second best performing model , which uses Pre - trained LSTM embeddings takes more than a day for the autoencoder to train and further 39 + minutes for each epoch . Hence , we can say that our model is viable for production environment . We have tested the capability of the architecture to handle the OOV words or misspelled words . For this we used TRAC shared dataset , initialised the word embeddings randomly and trained the model for classification process . Next , we enabled the embeddings to be changed during training process which is mentioned as dynamic channel in ( Kim , 2014 ) to let the model learn new embeddings . After training , we took the weights of embedding layer and plotted these embedings using Tensorboard ( Abadi et al , 2015 ) . From figure 2 we can see that the model is able to minimise the distance between the misspelled word and is able to capture the relationship between transliterated words as in Table : 2 . We found that total of 3 clusters were formed after the experiment as shown in Fig : 2b . We investigated these clusters and found that some of the highly used words in the comments belonged to certain classes . For example , one of the cluster contained more of neutral words , another cluster contained highly aggressive and abusive words , and the third cluster contained some toxic words along with place and country names related to one 's origin which were used in some foul comments . We show the capability of our model to tackle the problem of overfitting , as observed during training we see the model to have comparatively lower difference in training and validation loss than other models . Same can be seen from Fig : 2a the loss margin difference does n't change . We have shown that , not only our model has performed well on the classification task , it also has ability to generalise well and can learn good representation for word tokens .", "entities": [[2, 3, "MethodName", "CapsNet"], [25, 27, "MethodName", "focal loss"], [29, 30, "MethodName", "CapsNet"], [36, 39, "MetricName", "ROC - AUC"], [42, 45, "TaskName", "toxic comment classification"], [51, 52, "MetricName", "F1"], [85, 86, "MethodName", "RAM"], [136, 137, "MethodName", "LSTM"], [145, 146, "MethodName", "autoencoder"], [198, 200, "TaskName", "word embeddings"], [420, 421, "MetricName", "loss"], [434, 435, "MetricName", "loss"]]}
{"text": "BERT is one of the key innovations in the recent progress of contextualized representation learning ( Peters et al , 2018 ; Howard and Ruder , 2018 ; Radford et al , 2018a ; Devlin et al , 2018 ) . The idea behind the progress is that even though the word embedding ( Mikolov et al , 2013 ; Pennington et al , 2014 ) layer ( in a typical neural network for NLP ) is trained from large - scale corpora , training a wide variety of neural architectures that encode contextual representations only from the limited supervised data on end tasks is insufficient . Unlike ELMo ( Peters et al , 2018 ) and ULMFiT Figure 1 : Overview of BERT settings for review reading comprehension ( RRC ) , aspect extraction ( AE ) and aspect sentiment classification ( ASC ) . ( Howard and Ruder , 2018 ) that are intended to provide additional features for a particular architecture that bears human 's understanding of the end task , BERT adopts a fine - tuning approach that requires almost no specific architecture for each end task . This is desired as an intelligent agent should minimize the use of prior human knowledge in the model design . Instead , it should learn such knowledge from data . BERT has two parameter intensive settings : BERT BASE : 12 layers , 768 hidden dimensions and 12 attention heads ( in transformer ) with the total number of parameters , 110 M ; BERT LARGE : 24 layers , 1024 hidden dimensions and 16 attention heads ( in transformer ) with the total number of parameters , 340M. We only extend BERT with one extra taskspecific layer and fine - tune BERT on each end task . We focus on three ( 3 ) review - based tasks : review reading comprehension ( RRC ) , aspect extraction ( AE ) and aspect sentiment classification ( ASC ) . The inputs / outputs settings are depicted in Figure 1 and detailed in the following subsections .", "entities": [[0, 1, "MethodName", "BERT"], [13, 15, "TaskName", "representation learning"], [108, 109, "MethodName", "ELMo"], [117, 118, "MethodName", "ULMFiT"], [123, 124, "MethodName", "BERT"], [127, 129, "TaskName", "reading comprehension"], [133, 135, "TaskName", "aspect extraction"], [136, 137, "MethodName", "AE"], [174, 175, "MethodName", "BERT"], [198, 199, "DatasetName", "agent"], [222, 223, "MethodName", "BERT"], [229, 230, "MethodName", "BERT"], [230, 231, "MethodName", "BASE"], [249, 252, "HyperparameterName", "number of parameters"], [256, 257, "MethodName", "BERT"], [276, 279, "HyperparameterName", "number of parameters"], [284, 285, "MethodName", "BERT"], [294, 295, "MethodName", "BERT"], [313, 315, "TaskName", "reading comprehension"], [319, 321, "TaskName", "aspect extraction"], [322, 323, "MethodName", "AE"]]}
{"text": "Following the success of SQuAD ( Rajpurkar et al , 2016 ) and BERT 's SQuAD implementation , we design review reading comprehension as follows . Given a question q = ( q 1 , . . . , q m ) asking for an answer from a review d = ( d 1 , . . . , d n ) , we formulate the input as a sequence x = ( [ CLS ] , q 1 , . . . , q m , [ SEP ] , d 1 , . . . , d n , [ SEP ] ) , where [ CLS ] is a dummy token not used for RRC and [ SEP ] is intended to separate q and d. Let BERT ( ) be the pre - trained ( or posttrained as in the next section ) BERT model . We first obtain the hidden representation as h = BERT ( x ) R r h * | x | , where | x | is the length of the input sequence and r h is the size of the hidden dimension . Then the hidden representation is passed to two separate dense layers followed by softmax functions : Training the RRC model involves minimizing the loss that is designed as the averaged cross entropy on the two pointers : l 1 = softmax ( W 1 h + b 1 ) and l 2 = softmax ( W 2 h + b 2 ) , where W 1 , W 2 R r h and b 1 , b 2 R. L RRC = \u2212 log l 1 I ( s ) + log l 2 I ( e ) 2 , where I ( s ) and I ( e ) are one - hot vectors representing the ground truths of pointers . RRC may suffer from the prohibitive cost of annotating large - scale training data covering a wide range of domains . And BERT severely lacks two kinds of prior knowledge : ( 1 ) large - scale domain knowledge ( e.g. , about a specific product category ) , and ( 2 ) task - awareness knowledge ( MRC / RRC in this case ) . We detail the technique of jointly incorporating these two types of knowledge in Sec . 4 .", "entities": [[4, 5, "DatasetName", "SQuAD"], [13, 14, "MethodName", "BERT"], [15, 16, "DatasetName", "SQuAD"], [21, 23, "TaskName", "reading comprehension"], [130, 131, "MethodName", "BERT"], [147, 148, "MethodName", "BERT"], [159, 160, "MethodName", "BERT"], [206, 207, "MethodName", "softmax"], [216, 217, "MetricName", "loss"], [233, 234, "MethodName", "softmax"], [246, 247, "MethodName", "softmax"], [337, 338, "MethodName", "BERT"]]}
{"text": "As a core task in ABSA , aspect extraction ( AE ) aims to find aspects that reviewers have expressed opinions on ( Hu and Liu , 2004 ) . In supervised settings , it is typically modeled as a sequence labeling task , where each token from a sentence is labeled as one of { Begin , Inside , Outside } . A continuous chunk of tokens that are labeled as one B and followed by zero or more Is forms an aspect . The input sentence with m words is constructed as x = ( [ CLS ] , x 1 , . . . , x m , [ SEP ] ) . After h = BERT ( x ) , we apply a dense layer and a softmax for each position of the sequence : l 3 = softmax ( W 3 h+b 3 ) , where W 3 R 3 * r h and b 3 R 3 ( 3 is the total number of labels ( BIO ) ) . Softmax is applied along the dimension of labels for each position and l 3 [ 0 , 1 ] 3 * | x | . The labels are predicted as taking argmax function at each position of l 3 and the loss function is the averaged cross entropy across all positions of a sequence . AE is a task that requires intensive domain knowledge ( e.g. , knowing that \" screen \" is a part of a laptop ) . Previous study ( Xu et al , 2018a ) has shown that incorporating domain word embeddings greatly improve the performance . Adapting BERT 's general language models to domain reviews is crucial for AE , as shown in Sec . 5 .", "entities": [[7, 9, "TaskName", "aspect extraction"], [10, 11, "MethodName", "AE"], [119, 120, "MethodName", "BERT"], [131, 132, "MethodName", "softmax"], [142, 143, "MethodName", "softmax"], [176, 177, "MethodName", "Softmax"], [191, 192, "DatasetName", "0"], [217, 218, "MetricName", "loss"], [231, 232, "MethodName", "AE"], [270, 272, "TaskName", "word embeddings"], [278, 279, "MethodName", "BERT"], [289, 290, "MethodName", "AE"]]}
{"text": "As discussed in the introduction , fine - tuning BERT directly on the end task that has limited tuning data faces both domain challenges and taskawareness challenge . To enhance the performance of RRC ( and also AE and ASC ) , we may need to reduce the bias introduced by non - review knowledge ( e.g. , from Wikipedia corpora ) and fuse domain knowledge ( DK ) ( from unsupervised domain data ) and task knowledge ( from supervised MRC task but out - of - domain data ) . Given MRC is a general task with answers of questions covering almost all document contents , a large - scale MRC supervised corpus may also benefit AE and ASC . Eventually , we aim to have a general - purpose post - training strategy that can exploit the above two kinds of knowledge for end tasks . To post - train on domain knowledge , we leverage the two novel pre - training objectives from BERT : masked language model ( MLM ) and next sentence 7 prediction ( NSP ) . The former predicts randomly masked words and the latter detects whether two sides of the input are from the same document or not . A training example is formulated as ( [ CLS ] , x 1 : j , [ SEP ] , x j+1 : n , [ SEP ] ) , where x 1 : n is a document ( with randomly masked words ) split into two sides x 1 : j and x j+1 : n and [ SEP ] separates those two . MLM is crucial for injecting review domain knowledge and for alleviating the bias of the knowledge from Wikipedia . For example , in the Wikipedia domain , BERT may learn to guess the [ MASK ] in \" The [ MASK ] is bright \" as \" sun \" . But in a laptop domain , it could be \" screen \" . Further , if the [ MASK ] ed word is an opinion word in \" The touch screen is [ MASK ] \" , this objective challenges BERT to learn the representations for fine - grained opinion words like \" great \" or \" terrible \" for [ MASK ] . The objective of NSP further encourages BERT to learn contextual representation beyond word - level . In the context of reviews , NSP formulates a task of \" artificial review prediction \" , where a negative example is an original review but a positive example is a synthesized fake review by combining two different reviews . This task exploits the rich relationships between two sides in the input , such as whether two sides of texts have the same rating or not ( when two reviews with different ratings are combined as a positive example ) , or whether two sides are targeting the same product or not ( when two reviews from different products are merged as a positive example ) . In summary , these two objectives encourage to learn a myriad of fine - grained features for potential end tasks . We let the loss function of MLM be L MLM and the loss function of next text piece prediction be L NSP , the total loss of the domain knowledge posttraining is L DK = L MLM + L NSP . To post - train BERT on task - aware knowledge , we use SQuAD ( 1.1 ) , which is a popular largescale MRC dataset . Although BERT gains great success on SQuAD , this success is based on the huge amount of training examples of SQuAD ( 100 , 000 + ) . This amount is large enough to ameliorate the flaws of BERT that has almost no questions on the left side and no textual span predictions based on both the question and the document on the right side . However , a small amount of finetuning examples is not sufficient to turn BERT to be more task - aware , as shown in Sec . 5 . We let the loss on SQuAD be L MRC , which is in a similar setting as the loss L RRC for RRC . As a result , the joint loss of post - training is defined as L = L DK + L MRC . One major issue of post - training on such a loss is the prohibitive cost of GPU memory usage . Instead of updating parameters over a batch , we divide a batch into multiple sub - batches and accumulate gradients on those sub - batches before parameter updates . This allows for a smaller subbatch to be consumed in each iteration . u : number of sub - batches . 1 \u2207 \u0398 L 0 2 { D DK , 1 , . . . , D DK , u } Split ( D DK , u ) 3 { D MRC , 1 , . . . , D MRC , u } Split ( D MRC , u ) 4 for i { 1 , . . . , u } do 5 L partial L DK ( D DK , i ) + L MRC ( D MRC , i ) u 6 \u2207 \u0398 L \u2207 \u0398 L + BackProp ( L partial ) 7 end 8 \u0398 ParameterUpdates ( \u2207 \u0398 L ) to be able to fit into GPU memory . In line 5 , it computes the partial joint loss L partial of two subbatches D DK , i and D MRC , i from the i - th iteration through forward pass . Note that the summation of two sub - batches ' losses is divided by u , which compensate the scale change introduced by gradient accumulation in line 6 . Line 6 accumulates the gradients produced by backpropagation from the partial joint loss . To this end , accumulating the gradients u times is equivalent to computing the gradients on the whole batch once . But the subbatches and their intermediate hidden representations during the i - th forward pass can be discarded to save memory space . Only the gradients \u2207 \u0398 are kept throughout all iterations and used to update parameters ( based on the chosen optimizer ) in line 8 . We detail the hyper - parameter settings of this algorithm in Sec . 5.3 .", "entities": [[9, 10, "MethodName", "BERT"], [37, 38, "MethodName", "AE"], [118, 119, "MethodName", "AE"], [167, 168, "MethodName", "BERT"], [173, 174, "DatasetName", "MLM"], [273, 274, "DatasetName", "MLM"], [300, 301, "MethodName", "BERT"], [363, 364, "MethodName", "BERT"], [393, 394, "MethodName", "BERT"], [534, 535, "MetricName", "loss"], [537, 538, "DatasetName", "MLM"], [540, 541, "DatasetName", "MLM"], [543, 544, "MetricName", "loss"], [556, 557, "MetricName", "loss"], [567, 568, "DatasetName", "MLM"], [576, 577, "MethodName", "BERT"], [585, 586, "DatasetName", "SQuAD"], [599, 600, "MethodName", "BERT"], [604, 605, "DatasetName", "SQuAD"], [618, 619, "DatasetName", "SQuAD"], [636, 637, "MethodName", "BERT"], [677, 678, "MethodName", "BERT"], [695, 696, "MetricName", "loss"], [697, 698, "DatasetName", "SQuAD"], [710, 711, "MetricName", "loss"], [722, 723, "MetricName", "loss"], [748, 749, "MetricName", "loss"], [810, 811, "HyperparameterName", "\u0398"], [812, 813, "DatasetName", "0"], [895, 896, "HyperparameterName", "\u0398"], [898, 899, "HyperparameterName", "\u0398"], [909, 910, "HyperparameterName", "\u0398"], [913, 914, "HyperparameterName", "\u0398"], [934, 935, "MetricName", "loss"], [1000, 1001, "MetricName", "loss"], [1050, 1051, "HyperparameterName", "\u0398"], [1066, 1067, "HyperparameterName", "optimizer"]]}
{"text": "We adopt BERT BASE ( uncased ) as the basis for all experiments 10 . Since post - training may take a large footprint on GPU memory ( as BERT pretraining ) , we leverage FP16 computation 11 to reduce the size of both the model and hidden representations of data . We set a static loss scale of 2 in FP16 , which can avoid any over / under - flow of floating point computation . The maximum length of post - training is set to 320 with a batch size of 16 for each type of knowledge . The number of subbatch u is set to 2 , which is good enough to store each sub - batch iteration into a GPU memory of 11G. We use Adam optimizer and set the learning rate to be 3e - 5 . We train 70 , 000 steps for the laptop domain and 140 , 000 steps for the restaurant domain , which roughly have one pass over the preprocessed data on the respective domain .", "entities": [[2, 3, "MethodName", "BERT"], [3, 4, "MethodName", "BASE"], [29, 30, "MethodName", "BERT"], [56, 57, "MetricName", "loss"], [90, 92, "HyperparameterName", "batch size"], [129, 130, "MethodName", "Adam"], [130, 131, "HyperparameterName", "optimizer"], [134, 136, "HyperparameterName", "learning rate"]]}
{"text": "As BERT outperforms existing open source MRC baselines by a large margin , we do not intend to exhaust existing implementations but focus on variants of BERT introduced in this paper . DrQA is a baseline from the document reader 12 of DrQA ( Chen et al , 2017 ) . We adopt this baseline because of its simple implementation for reproducibility . We run the document reader with random initialization and train it directly on ReviewRC . We use all default hyper - parameter settings for this baseline except the number of epochs , which is set as 60 for better convergence . DrQA+MRC is derived from the above baseline with official pre - trained weights on SQuAD . We fine - tune document reader with ReviewRC . We expand the vocabulary of the embedding layer from the pre - trained model on ReviewRC since reviews may have words that are rare in Wikipedia and keep other hyper - parameters as their defaults . For AE and ASC , we summarize the scores of the state - of - the - arts on SemEval ( based the best of our knowledge ) for brevity . DE - CNN ( Xu et al , 2018a ) reaches the state - ofthe - arts for AE by leveraging domain embeddings . MGAN ( Li et al , 2018 ) reaches the state - of - theart ASC on SemEval 2014 task 4 . Lastly , to answer RQ1 , RQ2 , and RQ3 , we have the following BERT variants . BERT leverages the vanilla BERT pre - trained weights and fine - tunes on all 3 end tasks . We use this baseline to answer RQ2 and show that BERT 's pre - trained weights alone have limited performance gains on review - based tasks . BERT - DK post - trains BERT 's weights only on domain knowledge ( reviews ) and fine - tunes on the 3 end tasks . We use BERT - DK and the following BERT - MRC to answer RQ3 . BERT - MRC post - trains BERT 's weights on SQuAD 1.1 and then fine - tunes on the 3 end tasks . BERT - PT ( proposed method ) post - trains BERT 's weights using the joint post - training algorithm in Section 4 and then fine - tunes on the 3 end tasks .", "entities": [[1, 2, "MethodName", "BERT"], [26, 27, "MethodName", "BERT"], [91, 94, "HyperparameterName", "number of epochs"], [118, 119, "DatasetName", "SQuAD"], [166, 167, "MethodName", "AE"], [214, 215, "MethodName", "AE"], [257, 258, "MethodName", "BERT"], [260, 261, "MethodName", "BERT"], [264, 265, "MethodName", "BERT"], [289, 290, "MethodName", "BERT"], [306, 307, "MethodName", "BERT"], [312, 313, "MethodName", "BERT"], [334, 335, "MethodName", "BERT"], [340, 341, "MethodName", "BERT"], [347, 348, "MethodName", "BERT"], [353, 354, "MethodName", "BERT"], [357, 358, "DatasetName", "SQuAD"], [370, 371, "MethodName", "BERT"], [380, 381, "MethodName", "BERT"]]}
{"text": "To be consistent with existing research on MRC , we use the same evaluation script from SQuAD 1.1 ( Rajpurkar et al , 2016 ) for RRC , which reports Exact Match ( EM ) and F1 scores . EM requires the answers to have exact string match with human annotated answer spans . F1 score is the averaged F1 scores of individual answers , which is typically higher than EM and is the major metric . Each individual F1 score is the harmonic mean of individual precision and recall computed based on the number of overlapped words between the predicted answer and human annotated answers . For AE , we use the standard evaluation scripts come with the SemEval datasets and report the F1 score . For ASC , we compute both accuracy and Macro - F1 over 3 classes of polarities , where Macro - F1 is the major metric as the imbalanced classes introduce biases on accuracy . To be consistent with existing research ( Tang et al , 2016 ) , examples belonging to the conflict polarity are dropped due to a very small number of examples . We set the maximum number of epochs to 4 for BERT variants , though most runs converge just within 2 epochs . Results are reported as averages of 9 runs ( 9 different random seeds for random batch generation ) . 13", "entities": [[16, 17, "DatasetName", "SQuAD"], [30, 32, "MetricName", "Exact Match"], [33, 34, "MetricName", "EM"], [36, 37, "MetricName", "F1"], [39, 40, "MetricName", "EM"], [54, 56, "MetricName", "F1 score"], [59, 60, "MetricName", "F1"], [70, 71, "MetricName", "EM"], [79, 81, "MetricName", "F1 score"], [108, 109, "MethodName", "AE"], [124, 126, "MetricName", "F1 score"], [133, 134, "MetricName", "accuracy"], [135, 138, "MetricName", "Macro - F1"], [145, 148, "MetricName", "Macro - F1"], [159, 160, "MetricName", "accuracy"], [196, 199, "HyperparameterName", "number of epochs"], [202, 203, "MethodName", "BERT"], [226, 227, "DatasetName", "seeds"]]}
{"text": "The neutralization module is designed to filter sentiment information out from the input text . For example , the input hate when my bus is late should be filtered to produce a neutral statement like bus is late . Neutralization is performed by a neural sentiment classification module . Each word in the input sentence of length N ( x = { x 1 , x 2 , ... , x N } ) ( in one - hot representation , padded wherever necessary ) is transformed into K - dimensional embedding . The embeddings are then passed through a layer of recurrent units such as Long Short Term Memory ( LSTM ) ( Hochreiter and Schmidhuber , 1997 ) . The output of the LSTM layers are then sent to a self - attention layer before passing through a softmax based classifier . The classifier is trained with the supervision from sentiment positive / negative labels using corpora P and N . During testing , for a given input of length N , the self attention vector \u03b1 = \u03b1 1 , \u03b1 2 , ... , \u03b1 N is first extracted ( details skipped for brevity , refer Xu et al ( 2018 ) ) . We then inverse and discretize the self attention vector as follows : \u03b1 i = 0 , if \u03b1 i > \u00b5 + 2\u03c3 1 , otherwise ( 1 ) where \u03b1 i is the attention weight for the i th word , \u00b5 and \u03c3 are the mean and standard deviation for the attention vector . For each word in the input , if the discretized attention weight is 0 , it is filtered out . The motivation behind such a design is that if the classifier is trained well , the attention weights will represent the contribution of each word to the overall sentiment decision ( Xu et al , 2018 ) . Sentiment bearing words will receive higher attention whereas neutral words will have lower attention weights . It is worth noting that neutralization can be done in several other ways such as filtering based on a sentiment lexicon . However , such operations would require additional resources such as sentiment dictionary , sense disambiguation tools , whereas the neural classification based filtering can only work with binary sentiment labeled data . Also , for computing word - level sentiment contributions , recent techniques ( such as gradient - based methods ( Sundararajan et al , 2017 ) ) can be used . For simplicity , we use attention based filtering .", "entities": [[111, 112, "MethodName", "LSTM"], [125, 126, "MethodName", "LSTM"], [140, 141, "MethodName", "softmax"], [178, 179, "HyperparameterName", "\u03b1"], [180, 181, "HyperparameterName", "\u03b1"], [183, 184, "HyperparameterName", "\u03b1"], [188, 189, "HyperparameterName", "\u03b1"], [220, 221, "HyperparameterName", "\u03b1"], [223, 224, "DatasetName", "0"], [226, 227, "HyperparameterName", "\u03b1"], [239, 240, "HyperparameterName", "\u03b1"], [278, 279, "DatasetName", "0"]]}
{"text": "The sarcasm synthesis module is a sequence - tosequence network that expects a set of keywords related to positive sentiment and negative situation phrases . For training this module , the sarcasm corpus S is used . To prepare the input , we implement a keyword extraction technique based on POS tagging . Sentences in S are POS - tagged and then stopwords are removed , and then based on the POS tags noun , verb , adjective and adverbs are retained . This way , the input keywords to the system would somewhat be similar to the keywords expected in real time scenario . The module follows an encode - attend - decode style architecture like the positive sentiment induction module , but with a different learning objective . Keywords in the input ( in one - hot representation , padded wherever necessary ) are transformed into a sequence of embeddings and then encoded by layers of LSTMs , which produces a hidden representations for each input word . The decoder , consisting of a single layer of LSTMs stacked on the top of a decoder embedding layer , attend over the encoded hidden representations and generate target words . In general , for T training instances of keywords and sarcastic texts , { x i , y i } T i=1 , the training objective is to maximize the likelihood of a target y i given x i , which is similar to minimizing the cross entropy between the target distribution and the predicted output distribution . For training the neural network , the crossentropy loss is back propagated . In other words , the gradient of the negative cross - entropy loss is considered to update the model parameters . The gradient is given by : \u2207 \u03b8 L ( \u03b8 ) = \u2207 \u03b8 T i=1 y i log P M \u03b8 ( \u0177 i | x i ) ( 3 ) where L is the loss function and M \u03b8 is the translation system with parameter \u03b8.\u0177 i is the predicted sentence . In our setting , where the input is not a proper sentence , the problem with the above objective is that it does not strongly enforce the decoder to learn and produce sarcastic output . We speculate that minimizing the token - level cross entropy loss in Eq . 3 may help produce an output that is grammatically correct but not sarcastic enough . For instance , the decoder may incur insignificant cross - entropy loss after generating a sentence like absolutely loved it , as this sentence has considerable overlap with the reference sarcastic text that provides supervision . One possible idea to tackle such problems is to employ a sarcasm scorer that can determine the sarcasm content in the generated output , and use the scores given by the sarcasm scorer for better training of the generator . However , the sarcasm scorer may be external to the sequence - tosequence pipeline , and the scoring function may not be differentiable with respect to the model M \u03b8 . For this , we apply reinforcement learning which considers sarcasm content score as a form of reward and use it to fine - tune the learning process . For learning , the policy gradient theorem ( Williams , 1992 ) is used . The system is trained under a modified learning objective i.e. , to maximize the expected reward score for a set of produced candidate sentences . The generator , operating with a policy of P M \u03b8 ( \u0177 i | x i ) , producing an output\u0177 i with an expected reward score computed using a scorer , will thus have the following gradient : \u2207 \u03b8 RL ( \u03b8 ) = \u2207 \u03b8 E\u0177 i \u223cP M \u03b8 ( \u0177 i | x i ) [ R ( \u0177 i ) ] = E [ R ( \u0177 i ) \u2207 \u03b8 log ( P M \u03b8 ( \u0177 i | x i ) ) ] ( 4 ) where RL is the modified learning objective which has to be maximized and R is a reward function that is computed using an external scorer . In practice , the expected reward is computed by ( a ) sampling candidate outputs from the policy P M \u03b8 , ( b ) computing the reward score for each candidate and ( c ) averaging the rewards so obtained . In typical RL settings , the learner is typically initialized to a random policy distribution . However , in our case , since some supervision is already available in the form of target sarcastic sentences , we pretrain the model with the loss minimization objective given in Eq . 3 and then fine - tune the model based on the policy gradient scheme following Eq . 4 . Thus , the learner gets initialized with a better policy distribution . For reward calculation , we consider the confidence score of a sarcasm classifier ( probability of being sarcastic ) trained using S as positive examples and P and N taken as negative examples . For our setting , the classifier is analogous to the classifier used for neutralization . The classifier is based on embedding , LSTM and softmax layers . Since the input to the system is a list of words , it may seem that the sarcasm synthesis module may not require sequence to sequence learning , and a much simpler approach like bag - of - words to sequence generation could have been used . However , note that the input to the generator is obtained after dropping words during neutralization and later appending the negative situation phrase . The sequentiality is , thus , not completely lost . This makes sequence to sequence model an intuitive choice . We now explain our experimental setup .", "entities": [[45, 47, "TaskName", "keyword extraction"], [267, 268, "MetricName", "loss"], [284, 285, "MetricName", "loss"], [300, 301, "HyperparameterName", "\u03b8"], [303, 304, "HyperparameterName", "\u03b8"], [307, 308, "HyperparameterName", "\u03b8"], [315, 316, "HyperparameterName", "\u03b8"], [330, 331, "MetricName", "loss"], [334, 335, "HyperparameterName", "\u03b8"], [393, 394, "MetricName", "loss"], [423, 424, "MetricName", "loss"], [517, 518, "HyperparameterName", "\u03b8"], [597, 598, "HyperparameterName", "\u03b8"], [628, 629, "HyperparameterName", "\u03b8"], [631, 632, "HyperparameterName", "\u03b8"], [635, 636, "HyperparameterName", "\u03b8"], [640, 641, "HyperparameterName", "\u03b8"], [664, 665, "HyperparameterName", "\u03b8"], [669, 670, "HyperparameterName", "\u03b8"], [728, 729, "HyperparameterName", "\u03b8"], [792, 793, "MetricName", "loss"], [886, 887, "MethodName", "LSTM"], [888, 889, "MethodName", "softmax"], [914, 917, "MethodName", "sequence to sequence"], [974, 977, "MethodName", "sequence to sequence"]]}
{"text": "For the neutralization module , the embedding dimension size is set to 128 , two layers of LSTMs of hidden dimension of 200 are used . The classifier trains for 10 epochs with a batch size of 32 , and achieves a validation accuracy of 96 % and training accuracy of 98 % . For positive sentiment induction module , the embedding dimensions for both encoder and decoder are set to 500 . Both the encoder and decoder have only one layer of LSTM , with a hidden dimension of 500 . The module is built on top of the OpenNMT ( Klein et al , 2017 ) framework . Training happens in 100 , 000 iterations and the batch size is set to 64 . The positive sentiment induction module , at the end of the training , produces a bigram BLEU ( Papineni et al , 2002 ) score of 62.25 % . For bootstrapping negative situations and other purposes , the POS tagger from Spacy 5 is used . The Lucene - IR framework is set up to retrieve negative situations . The model configuration and training parameters for the sarcasm synthesizer is the same as the positive sentiment induction module . For the RL scheme , for each instance , the expected reward is computed over 100 candidate samples . At the end of the training , the bigram BLEU score on the validation set turns out to be 59.3 % . For reward computation , we use a classifier similar to the one used for neutralization . The embedding size for this classifier is 300 and it uses two layers of unidirectional LSTMs with a hidden dimension of 300 . It trains with a batch size of 64 and produces a validation accuracy of 78.3 % . The probability estimates given by the classifier for any input text are taken as reward scores . For optimization , cross entropy loss criterion is used .", "entities": [[6, 8, "HyperparameterName", "embedding dimension"], [34, 36, "HyperparameterName", "batch size"], [43, 44, "MetricName", "accuracy"], [49, 50, "MetricName", "accuracy"], [83, 84, "MethodName", "LSTM"], [119, 121, "HyperparameterName", "batch size"], [142, 143, "MetricName", "BLEU"], [233, 235, "MetricName", "BLEU score"], [289, 291, "HyperparameterName", "batch size"], [297, 298, "MetricName", "accuracy"], [324, 325, "MetricName", "loss"]]}
{"text": "Absence of automatic evaluation metrics capable of capturing subtleties of sarcasm makes it difficult to evaluate sarcasm generators . For evaluation , we still use the popular translation and summarization evaluation metrics METEOR ( Banerjee and Lavie , 2005 ) and ROUGE ( Lin , 2004 ) . Additionally , to check the semantic relatedness between the input and output , we use Skip - thought sentence similarity metric 6 . Note that using BLEU ( Papineni et al , 2002 ) will be futile here as direct n - gram overlaps between the predicted and goldstandard sentences are not expected to be significantly higher for such a task . We still include it as an evaluation metric for completion . We employ an additional metric to judge the percentage of length increment ( abbreviated as WL ) to see the if the length of the output is generally more than that of the input ( for the reference text it is 67 % ) . The notion behind this metric is that sarcasm typically requires more context than its literal version , requiring to have more words present at the target side .", "entities": [[29, 30, "TaskName", "summarization"], [32, 33, "DatasetName", "METEOR"], [74, 75, "MetricName", "BLEU"]]}
{"text": "For this task , we used 4 deep learning models . The architecture of the first 3 models were relatively similar , differing in the embedding layer . The first model involves character embedding with dimension equal to the total number of unique characters in training set including emojis . The output of this layer is fed to a series of 6 convolutional neural network layers ( CNNs ) with ReLU activation . Each CNN used 256 filters , with a filter size of 7 for the first two layers and 3 for the rest . Max pooling with size 3 was used for the first two and last CNNs . The CNNs ' output was fed into a bidirectional LSTM ( Bi - LSTM ) with 2 * 200 units , whose output was flattened to feed into two dense layers . We used two fully connected layers with 1024 units each , ReLU activation , and dropout of 0.5 . Finally , we used a dense layer with size two and softmax activation . We used Adam as the optimizer and binary cross - entropy as the loss function . The model was trained with 10 epochs and batch size of 128 . The second architecture was identical to the first , except the first layer was a word embedding using GloVe 2 pre - trained on Twitter data with embedding dimension of 100 . The third model was a concatenation of word and character embeddings . We combined the Bi - LSTM output of the first and second models and then applied dense layers as before . After building the above models , we tried to improve the outcomes by adding layers and features . We used a multi - head self - attention with an attention width of 15 and ReLU activation . We also explored the effect of sentiment features . Since the data classes were imbalanced , we tried to make class sizes equal by downsampling and upsampling . In downsampling , samples from the majority class ( tweets without ADR mentions ) were randomly sampled without replacement . In upsampling we did the opposite , adding samples from the minority class with replacement . None of these strategies substantially altered our baseline results . In our final model , we used ELMo ( Peters et al , 2018 ) ( Embeddings from Language Models ) with 1024 dimensions . In contrast to traditional word embeddings such as GloVe and word2vec , ELMo assigns each word to a vector as a function of the entire sentence containing that word . Therefore , the same word can have different embeddings depending on its context . Since ELMo already captures character - level information under the hood , we decided to encircle the complexity inside the embedding layer and used only two additional dense layers with 256 and 2 units , using ReLU and softmax activations , respectively .", "entities": [[70, 71, "MethodName", "ReLU"], [96, 98, "MethodName", "Max pooling"], [119, 121, "MethodName", "bidirectional LSTM"], [124, 125, "MethodName", "LSTM"], [154, 155, "MethodName", "ReLU"], [173, 174, "MethodName", "softmax"], [178, 179, "MethodName", "Adam"], [181, 182, "HyperparameterName", "optimizer"], [189, 190, "MetricName", "loss"], [200, 202, "HyperparameterName", "batch size"], [223, 224, "MethodName", "GloVe"], [232, 234, "HyperparameterName", "embedding dimension"], [254, 255, "MethodName", "LSTM"], [304, 305, "MethodName", "ReLU"], [388, 389, "MethodName", "ELMo"], [410, 412, "TaskName", "word embeddings"], [414, 415, "MethodName", "GloVe"], [418, 419, "MethodName", "ELMo"], [451, 452, "MethodName", "ELMo"], [486, 487, "MethodName", "ReLU"], [488, 489, "MethodName", "softmax"]]}
{"text": "Among all architectures , the best results came from ELMo embedding ( F1 = 0.64 ) . Therefore , we only submitted ELMo results with 5 , 10 , and 15 epochs . The model performed less well for the validation set ( F1 = 0.41 ) , below the average F1 score of 0.50 among all teams , which might result from overfitting . Using more sophisticated architecture after the embedding layer might improve performance . Since task 2 's performance depends strongly on task 1 , we also scored lower on this task compared to the team average ( 0.40 vs. 0.54 ) . Since ADR phrases and tweets do not always lexically match , approaches such as named entity recognition ( NER ) might perform better . Other approaches to improve performance : Task 1 : Try other embeddings such as BERT I would also like to show my gratitude to Peter Leimbigler for comments that greatly improved the manuscript . Finally , special thanks go to Alfred Whitehead for supporting me to participate in this challenge .", "entities": [[9, 10, "MethodName", "ELMo"], [12, 13, "MetricName", "F1"], [22, 23, "MethodName", "ELMo"], [43, 44, "MetricName", "F1"], [50, 52, "MetricName", "average F1"], [120, 123, "TaskName", "named entity recognition"], [124, 125, "TaskName", "NER"], [144, 145, "MethodName", "BERT"]]}
{"text": "For ACSA , we manually create templates containing one slot for the given_category and another slot for the polarity_type label . We set a category word set A = { a 1 , . . . , a | C | } , | C | is the category type size ( e.g. , a i = \" price \" ) and polarity type word set P = { p 1 , . . . , p | L | } , | L | is the polarity type size ( e.g. , p k = \" positive \" ) , and use words to define templates T a i , p k ( e.g. \" The sentiment polarity of price is positive \" ) . The template T is \" The sentiment polarity of a i is p k \" . For a given category a i , we can obtain a list of templates T a i = [ T a i , p 1 , . . . , T a i , p | L | ] . For ACD , we use a i to create a sentiment template T + a i for an existing aspect category , and a none - category template T \u2212 a i . T + is \" The a i category is discussed \" and T \u2212 is \" The a i category is not discussed \" .", "entities": [[94, 96, "HyperparameterName", "k ="]]}
{"text": "For ACSA , we first enumerate all possible polarities for the given category of the sentence X and fill them in the prepared templates , and then use the fine - tuned pre - trained generative language model to assign a score for each template T a i , p k = { t 1 , . . . , t m } , formulated as : f ( Ta i , p k ) = m c=1 log P ( tc | t1 : c\u22121 , X ) ( 1 ) We calculate a score f ( T a i , p k ) for each possible polarity by employing the pre - trained generative language model ( i.e. , BART ) to score the templates , and then choose the polarity of category a i with the largest score . For ACD , we first create templates T + a i and T \u2212 a i for all possible categories of the sentence X , and then use the fine - tuned pre - trained generative language model to assign a score for each template T a i = { t 1 , . . . , t m } , in a similar way as Equation 1 . Also , we decide whether the a i category is discussed or not in the input sentence according to the higher score between T + a i and T \u2212 a i .", "entities": [[50, 52, "HyperparameterName", "k ="], [121, 122, "MethodName", "BART"]]}
{"text": "For ACSA , suppose that the polarity type of a i is p k . We fill the given category a i and the polarity type p k into template T to create a gold target output T a i , p k . Similarly for ACD , if the category of a i is discussed , the gold target T + a i is obtained by filling a i into T + , and otherwise is T \u2212 a i . For ACSA , we use all gold polarities in the training set to construct ( X , T ) pairs . For ACD , we use all gold categories in the training set to construct ( X , T + ) pairs , and additionally create negative samples ( X , T \u2212 ) by sampling all none existing categories in the input . Finally , we obtain { ( X , T ) } = { ( X , T + ) \u222a ( X , T \u2212 ) } Given a sequence pair ( X , T ) , we feed the input X = x 1 : n to the BART encoder , obtaining hidden representations of the sentence : h enc = ENCODER ( x1 : n ) ( 2 ) At the c th step of the decoder , h enc and previous output tokens t 1 : c\u22121 are then as inputs , yielding a representation using attention ( Vaswani et al , 2017 ) h dec c = DECODER ( h enc , t1 : c\u22121 ) ( 3 ) The conditional probability of the word t c is defined as : P ( tc | t1 : c\u22121 , X ) = SOFTMAX ( h dec c W lm + b lm ) , ( 4 ) where W lm R d h \u00d7 | V | and b lm R | V | , | V | represents the vocab size of pre - trained BART . The cross - entropy between the decoder 's output and the original template is used as the loss function : L = \u2212 m c=1 log P ( tc | t1 , c\u22121 , X ) ( 5 ) 4 Experiments We choose the SemEval - 2014 restaurant review ( Rest14 ) ( Pontiki et al , 2014a ) , a variant of Rest14 ( Rest14 - hard ) ( Xue and Li , 2018 ) and the multiaspect multi - sentiment ( MAMS ) ( Jiang et al , 2019 ) datasets for sentence - level sentiment , the Tri - pAdvisor ( Wang et al , 2010 ) and BeerAdvocate ( McAuley et al , 2012 ; Lei et al , 2016 ) datasets for document - level sentiment . Standard splits of training / development / testing sets are adopted following previous work Tay et al ( 2018 ) , the details of which are shown in Appendix A. We use the pre - trained BERT - base 1 and BARTbase 2 models for task fine - tuning . We select the fine - tuning learning rate from { 4e - 5 , 2e - 5 , and 1e - 5 } and batch size from { 8 , 16 , 24 } for different models . The dropout probability is 0.1 . The best model configuration is selected according to the highest performance on the development set . The details of settings are shown in Appendix A.", "entities": [[195, 196, "MethodName", "BART"], [292, 293, "MethodName", "SOFTMAX"], [336, 337, "MethodName", "BART"], [355, 356, "MetricName", "loss"], [421, 422, "DatasetName", "MAMS"], [449, 450, "DatasetName", "BeerAdvocate"], [506, 507, "MethodName", "BERT"], [526, 528, "HyperparameterName", "learning rate"], [544, 546, "HyperparameterName", "batch size"]]}
{"text": "Different templates can be used for expressing the same meaning . For instance , \" The sentiment polarity of given_category is positive \" can also be expressed by \" The sentiment is positive for given_category \" . For ACSA , we investigate the impact of manual templates using the MAMS development set . Table 1 shows the impact of different choice of templates . For instance , \" The given_category category has a polarity_type label \" and \" The sentiment polarity of given_category is polarity_type \" give 82.31 % and 83.78 % accuracy , respectively , indicating that the template has influence on the final performance . This is consistent with finds of Gao et al ( 2020 ) for the fewshot task . Based on the development results , we use the top performing template \" The sentiment polarity of given_category is polarity_type \" in our ACSA experiments . For ACD , we investigate the impact of templates using the Rest14 development set . Table 2 shows the performance impact of different templates . We use the top performing template \" The category_type category is discussed \" as template T + and \" The category_type category is not discussed \" as template T \u2212 in our ACD experiments .", "entities": [[49, 50, "DatasetName", "MAMS"], [92, 93, "MetricName", "accuracy"]]}
{"text": "The results of sentence - level ACSA are shown in Table 3 . We can see that , first , the performance of BERT MLM and BART MLM is better than BERT classification and BART classification , respectively . In particular , BERT MLM gives a strong baseline , outperforming all non - BERT and BERT classification baselines . This shows that making use of pre - training at the task level can achieve better results than that at the representation level . Also , the BART MLM and classification models perform better than the corresponding BERT models . Second , BART generation outperforms all baselines on all three datasets , which indicates that our model can better detect multiple sentiment polarities in one sentence toward different aspect categories . Third , BART generation performs significantly better than BART MLM , giving absolutely 3.89 % stronger accuracy on MAMS , demonstrating the effectiveness of the generation method . This shows the strength of BART pre - training for generating semantically related content , which was also reflected by the strong performance of BART on abstractive sum - We use the results reported in XRCE ( Brun et al , 2014 ) , NRC - Canada ( Kiritchenko et al , 2014 ) , BERT - pair - NLI - B and CNE - net ( Dai et al , 2020 ) . marization ( Lewis et al , 2020 ) . In contrast , the MLM method concatenates the input and output into one sequence , and thus fails to model their correlation in encoder - decoder pre - trainng . The performances of our model on documentlevel ACSA are shown in Table 4 . Compared with LSTM , HAN and MR , BERT classification and BART classification outperform all baselines , which shows the effectiveness of pre - training . BERT MLM and BART MLM surpass BERT classification and BART classification , respectively . Our BART generation model achieves improvements of 1.15 % and 0.70 % over BART MLM on TripAdvisor and BeerAdvocate , respectively , demonstrating that the generation method can more effectively make use of BART for ACSA .", "entities": [[23, 24, "MethodName", "BERT"], [24, 25, "DatasetName", "MLM"], [26, 27, "MethodName", "BART"], [27, 28, "DatasetName", "MLM"], [31, 32, "MethodName", "BERT"], [34, 35, "MethodName", "BART"], [42, 43, "MethodName", "BERT"], [43, 44, "DatasetName", "MLM"], [53, 54, "MethodName", "BERT"], [55, 56, "MethodName", "BERT"], [86, 87, "MethodName", "BART"], [87, 88, "DatasetName", "MLM"], [96, 97, "MethodName", "BERT"], [101, 102, "MethodName", "BART"], [132, 133, "MethodName", "BART"], [138, 139, "MethodName", "BART"], [139, 140, "DatasetName", "MLM"], [146, 147, "MetricName", "accuracy"], [148, 149, "DatasetName", "MAMS"], [163, 164, "MethodName", "BART"], [182, 183, "MethodName", "BART"], [213, 214, "MethodName", "BERT"], [245, 246, "DatasetName", "MLM"], [287, 288, "MethodName", "LSTM"], [291, 292, "DatasetName", "MR"], [293, 294, "MethodName", "BERT"], [296, 297, "MethodName", "BART"], [311, 312, "MethodName", "BERT"], [312, 313, "DatasetName", "MLM"], [314, 315, "MethodName", "BART"], [315, 316, "DatasetName", "MLM"], [317, 318, "MethodName", "BERT"], [320, 321, "MethodName", "BART"], [326, 327, "MethodName", "BART"], [338, 339, "MethodName", "BART"], [339, 340, "DatasetName", "MLM"], [343, 344, "DatasetName", "BeerAdvocate"], [358, 359, "MethodName", "BART"]]}
{"text": "Results on the Rest14 ACD subtask are presented in Table 5 . Following Pontiki et al ( 2014b ) , we use Micro - F1 for evaluating . Again BART generation achieves better results than BART classification and BART MLM . Our model outperforms all baselines on precision and F - 1 score . In particular , a more than 95 % precision score is achieved , which shows that our model can effectively exclude the aspect categories not mentioned in the input . We also investigate the performance on the MAMS dataset , which consists of at least two unique aspect categories with different sentiment polarities in each input sentence . Table 7 shows that BART generation outperforms all baselines , indicating better ability of our model to detect multiple aspect categories in one sentence .", "entities": [[22, 25, "MetricName", "Micro - F1"], [29, 30, "MethodName", "BART"], [35, 36, "MethodName", "BART"], [38, 39, "MethodName", "BART"], [39, 40, "DatasetName", "MLM"], [91, 92, "DatasetName", "MAMS"], [116, 117, "MethodName", "BART"]]}
{"text": "The generation method allows us to build a straightforward joint model by extending the first template in Table 1 , using \" The sentiment polarity of < given_category > is none \" as a template for nonexisting aspect categories . The results on Rest - 14 and MAMS are presented in Table 6 . We find that joint BART generation achieves better results on this task with improvements over pipeline BART generation . Joint BART generation outperforms all baselines on precision , recall and F - 1 score , which shows the advantage of joint learning . ( 10 , 20 , 50 , 100 , 200 , 500 instances per category type for Rest14 and MAMS ) . The results are shown in Figure 4 , where the methods of BERT classification , BART classification and BART MLM are also compared . It can be seen that on all the datasets , our model outperforms BERT classification , BART classification and BART MLM , especially when the number of training instances is small . For example , when there are only 10 training instances , our model gives accuracy scores of 82.01 % on Rest14 , as compared to 38.57 % by BERT classification and 50.16 % by BART classification . When the number of instances grows as large as 500 , our model gives 2.24 % and 2.65 % better accuracies than BART MLM on Rest14 and MAMS , respectively . One possible reason is that our method makes more use of direct sentiment knowledge in the pre - trained language model by directly adopting the original structure of BART mentioned earlier . In contrast , classification methods can not achieve this due to transferring the sentiment bias indirectly . The results of our zero - shot learning experiments are in Table 8 . In all cases , our method outperforms all the baselines . In particular , the model trained on MAMS has a better performance on Rest14 than the reverse zero - shot setting , which proves that the MAMS dataset has a higher challenge .", "entities": [[47, 48, "DatasetName", "MAMS"], [58, 59, "MethodName", "BART"], [70, 71, "MethodName", "BART"], [74, 75, "MethodName", "BART"], [116, 117, "DatasetName", "MAMS"], [131, 132, "MethodName", "BERT"], [134, 135, "MethodName", "BART"], [137, 138, "MethodName", "BART"], [138, 139, "DatasetName", "MLM"], [156, 157, "MethodName", "BERT"], [159, 160, "MethodName", "BART"], [162, 163, "MethodName", "BART"], [163, 164, "DatasetName", "MLM"], [189, 190, "MetricName", "accuracy"], [203, 204, "MethodName", "BERT"], [209, 210, "MethodName", "BART"], [234, 235, "MethodName", "BART"], [235, 236, "DatasetName", "MLM"], [239, 240, "DatasetName", "MAMS"], [271, 272, "MethodName", "BART"], [296, 300, "TaskName", "zero - shot learning"], [324, 325, "DatasetName", "MAMS"], [343, 344, "DatasetName", "MAMS"]]}
{"text": "Aspect categories can be implicit and do not necessarily occur as terms in the given sentence . To explore the correlation between ACSA accuracy and the occurrence frequency of a given category , we split the eight categories in the MAMS test set into four subsets based on the occurrence frequency . The category ( i.e. , miscellaneous ) that never occurs in the given sentence is put into the zero frequency subset , the 15 % least frequent ( i.e. , ambience , staff ) are put into low frequency subset , the 30 % most frequent ( i.e. , menu , service ) are put into high frequency subset , and the remaining ( i.e. , price , food , place ) are put into mid frequency subset . Figure 5 shows the accuracy of BART classification and our model against the frequency . As the category occurrence frequency decreases , the relative gap of accuracy between the two models increases . In the zero frequency , our method gives absolutely 8.03 % stronger accuracy than BART classification . This demonstrates that our method is more robust in summarizing the sentiment polarity of abstract or rare categories . Even if there are no explicit category terms in the sentence , the generation method can give the implicit category opinion of the whole sentence according to the context .", "entities": [[23, 24, "MetricName", "accuracy"], [40, 41, "DatasetName", "MAMS"], [135, 136, "MetricName", "accuracy"], [137, 138, "MethodName", "BART"], [157, 158, "MetricName", "accuracy"], [176, 177, "MetricName", "accuracy"], [178, 179, "MethodName", "BART"]]}
{"text": "Deep learning ( DL ) is being used extensively for text classification . However , researchers have demonstrated the vulnerability of such classifiers to adversarial attacks . Attackers modify the text in a way which misleads the classifier while keeping the original meaning close to intact . State - of - the - art ( SOTA ) attack algorithms follow the general principle of making minimal changes to the text so as to not jeopardize semantics . Taking advantage of this we propose a novel and intuitive defense strategy called Sample Shielding . It is attacker and classifier agnostic , does not require any reconfiguration of the classifier or external resources and is simple to implement . Essentially , we sample subsets of the input text , classify them and summarize these into a final decision . We shield three popular DL text classifiers with Sample Shielding , test their resilience against four SOTA attackers across three datasets in a realistic threat setting . Even when given the advantage of knowing about our shielding strategy the adversary 's attack success rate is < = 10 % with only one exception and often < 5 % . Additionally , Sample Shielding maintains near original accuracy when applied to original texts . Crucially , we show that the ' make minimal changes ' approach of SOTA attackers leads to critical vulnerabilities that can be defended against with an intuitive sampling strategy . 1", "entities": [[10, 12, "TaskName", "text classification"], [203, 204, "MetricName", "accuracy"]]}
{"text": "Text classifiers have become ubiquitous . Unfortunately , they are subject to attacks from adversaries , typically executed using machine learning methods . Attackers work by making small modifications to the text that mislead the classifier . Adversarial attackers are now a growing part of the ecosystem . Like classifiers , attack algorithms have achieved strong success due to advances in machine learning / deep learning . Current text attackers , like 1 Our code and data are available at : https://github.com/JonRusert/SampleShielding TextFooler and Bert - Attack ( Li et al , 2020 ) , are able to reduce near perfect classification accuracy down to 5 % . Additionally , these attackers achieve this while perturbing ( changing ) only a small amount of the original text . This helps preserve the original meaning so that humans are able to understand the original message even though classifiers are duped . As a counter , classifier shielding techniques are being explored . One such approach is adversarial training where the classifier , assumed to have access to the attacker , uses it to generate perturbed texts - these are added to the classifier 's training data . While this leads to model resilience against that attacker it leaves the classifier open to attacks by new attackers . Other defenses involve modifying classifier structure to reduce the information an attacker can glean from it ( Goel et al , 2020 ) . However , this type of reconfiguration will not be possible if a third party classifier ( e.g. Google Perspective ) is leveraged . Even other approaches involve modifying the input text during classification time , but are currently limited to classifiers built from specific masked language models ( Zeng et al , 2021 ) or rely on external synonym datasets ( Wang et al , 2021a ) . We propose a shielding technique which is attacker - agnostic , does not require additional training / reconfiguration to the classifier , can shield any classifier , does not require an external data source , and can be used in a more realistic threat setting . We refer to this as Sample Shielding . Sample Shielding takes advantage of current constraints in SOTA attacks . Mainly , to preserve original meaning , these make the minimal changes needed to deceive the classifier . For example , BERT - Attack ( Li et al , 2020 ) only perturbs up to 16 % of text , and often far less ( e.g. 1.1 % ) for some datasets . Thus , if we would look at the 84 % to 99 % of text that is untouched our model would be more likely to classify correctly . Hence , in Sample Shielding we take many samples of the input 2 . We assess Sample Shielding under a realistic threat model where the attacker can not query a website 's classifier hundreds of times since that pattern is easily detectable by the website . We run experiments under two conditions , when the attacker has knowledge of Sample Shielding and when it does not . In both cases the attacker uses a local copy of the websites ' classifier . This is an optimistic assumption favouring the attacker and thus provides a lower bound to our results . 3 . We test against 4 SOTA text attack algorithms , 3 text datasets and 3 classifiers . When the attacker does not have knowledge of Sample Shielding , our defense reduces attack success rate from near total decimation 90 - 100 % down to 13 - 36 % , while still maintaining accuracy on original texts . When the attacker has knowledge of Sample Shielding , our defense performs even better , reducing attacks down to 1 - 10 % success rate . This is partially due to Sample Shielding 's random nature providing unreliable feedback to attackers . Our success with Sample Shielding is good news for classifiers - and it raises the bar significantly for the next generation attackers . We share code and our perturbed text collections for future research .", "entities": [[102, 103, "MetricName", "accuracy"], [257, 258, "DatasetName", "Google"], [394, 395, "MethodName", "BERT"], [607, 608, "MetricName", "accuracy"]]}
{"text": "We run experiments on the combination of the three victim classification models , three datasets , and four attack algorithms . These combinations are run on both threat model conditions ( attacker is aware/ not aware of SampleShielding ) . This leads to 72 shielding experiments . For all attacks , we leverage TextAttack framework 7 which provides classification algorithms and adversarial text generation algorithms implemented as specified in respective papers ( Morris et al , 2020 ) . In all experiments where the attacker does not use Sample Shielding 4 We share the original and perturbed texts for replicability . We note that replicability of previous defenses are limited because the identity of their randomly sampled test instances are not provided . 5 We calibrated classifier accuracies against previous research ( Li et al , 2020 ; 6 huggingface.co/textattack 7 textattack.readthedocs.io/en/latest/index.html we set k = 100 and p = 0.3 . While better performance was achieved with other values in preliminary experiments , we chose to go with a single combination of p and k for simplicity . In experiments where the attacker uses Sample Shielding pre - processing we reduce k to 30 for efficiency . Except where otherwise noted , majority voting is used to generate results . Additionally , shifting sampling ( Section 2.2.1 ) shielding typically achieved 10 - 20 points lower accuracy compared to random , thus we do not include it in the results .", "entities": [[61, 63, "TaskName", "adversarial text"], [144, 146, "HyperparameterName", "k ="], [227, 228, "MetricName", "accuracy"]]}
{"text": "We examine accuracy and Attack Success Rate : accuracy = # examples_classif ied_correctly # total_examples ( 1 ) ASR = Original Acc . \u2212 Attacked Acc . Original Acc . ( 2 ) 4 Results We first present results for the condition where the attacker is not aware of Sample Shielding based pre - processing and then the results for when the attacker also employs Sample Shielding .", "entities": [[2, 3, "MetricName", "accuracy"], [8, 9, "MetricName", "accuracy"], [21, 22, "MetricName", "Acc"], [25, 26, "MetricName", "Acc"], [28, 29, "MetricName", "Acc"]]}
{"text": "Results are in Table 1 . BERT is the strongest classifier achieving 91 - 100 % accuracy on the original datasets . Attacks are highly successful against unshielded texts . TextFooler and Bert - Attack are the most successful , dropping accuracies to 0 - 5 % generally . Attacks were able to achieve strong drops with minimal amount of text perturbed ( about 10 % ) . Figure 3 shows that the average percent of words perturbed across datasets for each attack are about equal in the mid regions of the plots . For AG News , attacks are less successful against BERT ; accuracy drops to 19 % in the strongest attack ( TextFooler ) , and only to 49 % in the weakest ( TextBugger", "entities": [[6, 7, "MethodName", "BERT"], [16, 17, "MetricName", "accuracy"], [43, 44, "DatasetName", "0"], [95, 97, "DatasetName", "AG News"], [103, 104, "MethodName", "BERT"], [105, 106, "MetricName", "accuracy"]]}
{"text": "Increasing p raises the risk of samples containing increased amounts of perturbed text . Decreasing k raises the risk of not covering enough of the unperturbed portions of the original text . While our settings of p = 0.3 and k = 100 for our main results are reasonable values ( Table 1 , Table 2 ) they are not necessarily optimal . Optimal p. Figure 4 shows the results for all com - binations of attacks against LSTM on IMDB with word shielding as the defense , k fixed at 100 . As we increase p , we see a continued drop in accuracy which is consistent with the idea that a higher p is more likely to capture perturbed text . The optimal value range appears to be in 0.2 - 0.4 range , although we do not see large drops until 0.6 onward . We also examined the same combination on AG News ( Figure 5 ) since it 's texts are considerably shorter and found consistent results . Optimal k. Figure 6 shows results for all attacks against LSTM on IMDB with word sampling as the defense , p fixed at 0.3 . The optimal k is not as clear as p. We see clear increases after 30 samples , but then the optimal k varies depending on attack . However , we see a leveling off around 90 samples , which gives some credence to our chosen k of 100 . We also found similar results when examining the same combination on AG News ( Figure 7 ) , however , k stabilized lower ( about 50 ) .", "entities": [[40, 42, "HyperparameterName", "k ="], [78, 79, "MethodName", "LSTM"], [80, 81, "DatasetName", "IMDB"], [104, 105, "MetricName", "accuracy"], [154, 156, "DatasetName", "AG News"], [182, 183, "MethodName", "LSTM"], [184, 185, "DatasetName", "IMDB"], [257, 259, "DatasetName", "AG News"]]}
{"text": "Sample Shielding , an intuitively designed defense which is attacker and classifier agnostic , protects effectively ; reducing ASR from 90 - 100 % down to 14 - 34 % with minimal accuracy loss ( 3 % ) in original texts . The randomness ( through sampling ) provides unreliable feedback for attackers , thus it even thwarts attackers who have query access to classifiers protected with Sample Shielding . Attack strategies will need to increase the amount of perturbation to make sure a majority of samples fail at classification . However , this will risk semantic integrity . Thus , we expect Sample Shielding to cause ripples in future adversarial attack strategies while providing text classifiers with a definite advantage .", "entities": [[32, 33, "MetricName", "accuracy"], [33, 34, "MetricName", "loss"], [110, 112, "TaskName", "adversarial attack"]]}
{"text": "In this paper we introduce the systems IIE submitted for the WMT20 shared task on German\u2194French news translation . Our systems are based on the Transformer architecture with some effective improvements . Multiscale collaborative deep architecture , data selection , back translation , knowledge distillation , domain adaptation , model ensemble and re - ranking are employed and proven effective in our experiments . Our German French system achieved 35.0 BLEU and ranked the second among all anonymous submissions , and our French German system achieved 36.6 BLEU and ranked the fourth in all anonymous submissions .", "entities": [[25, 26, "MethodName", "Transformer"], [43, 45, "MethodName", "knowledge distillation"], [46, 48, "TaskName", "domain adaptation"], [70, 71, "MetricName", "BLEU"], [87, 88, "MetricName", "BLEU"]]}
{"text": "The structure of NMT models has evolved quickly , such as RNN - based ( Wu et al , 2016 ) , CNN - based ( Gehring et al , 2017 ) and attentionbased ( Vaswani et al , 2017 ) systems . Deep neural networks have revolutionized the state - of - the - art in various communities , from computer vision to natural language processing . We adopt the deep transformer model proposed by our work ( Wei et al , 2020b ) . Instead of relying on the whole encoder stack to directly learn a desired representation , we let each encoder block learn a fine - grained representation and enhance it by encoding spatial dependencies using a bottom - up network . For coordination , we attend each block of the decoder to both the corresponding representation of the encoder and the contextual representation with spatial dependencies . This not only shortens the path of error propagation , but also helps to prevent the lower level information from being forgotten or diluted . In this section we describe the details ( as illustrated in figure 1 ) of our deep architectures as below : Block - Scale Collaboration . An intuitive extension of naive stacking of layers is to group few stacked layers into a block . We suppose that the encoder and decoder of our model have the same number of blocks ( i.e. , N ) . Each block of the encoder has M n ( n { 1 , 2 , ... , N } ) identical layers , while each decoder block contains one layer . Thus , we can adjust the value of each M n flexibly to increase the depth of the encoder . Formally , for the n - th block of the encoder : B n e = BLOCK e ( B n\u22121 e ) , ( 1 ) where BLOCK e ( ) is the block function , in which the layer function F ( ) is iterated M n times , i.e. where l { 1 , 2 , ... , M n } , H n , l e and \u0398 n , l e are the representation and parameters of the l - th layer in the n - th block , respectively . The decoder works in a similar way but the layer function G ( ) is iterated only once in each block , B n d = BLOCK d ( B n\u22121 d , B n e ) = G ( B n\u22121 d , B n e ; \u0398 n d ) + B n\u22121 d . ( 3 ) Each block of the decoder attends to the corresponding encoder block . Contextual Collaboration . To model long - term spatial dependencies and reuse global representations , we define a GRU cell Q ( c , x ) , which maps a hidden state c and an additional inputx into a new hidden state : C n = Q ( C n\u22121 , B n e ) , n [ 1 , N ] C 0 = E e , ( 4 ) where E e is the embedding matrix of the source input x. The new state C n can be fused with each layer of the subsequent blocks in both the encoder and the decoder . Formally , B n e in Eq . ( 1 ) can be re - calculated in the following way : B n e = H n , Mn e , H n , l e = F ( H n , l\u22121 e , C n\u22121 ; \u0398 n , l e ) + H n , l\u22121 e , H n , 0 e = B n\u22121 e . ( 5 ) Similarly , for decoder , we have B n d = BLOCK d ( B n\u22121 d , B n e ) = G ( B n\u22121 d , B n e , C n ; \u0398 n d ) + B n\u22121 d . ( 6 )", "entities": [[365, 366, "HyperparameterName", "\u0398"], [438, 439, "HyperparameterName", "\u0398"], [480, 481, "MethodName", "GRU"], [525, 526, "DatasetName", "0"], [616, 617, "HyperparameterName", "\u0398"], [632, 633, "DatasetName", "0"], [678, 679, "HyperparameterName", "\u0398"]]}
{"text": "We use the PyTorch implementation of Transformer 2 . We choose the Transformer base setting , in which the encoder and decoder are of 48 and 6 layers , respectively . The dropout rate is fixed as 0.1 . We set the batch size as 4096 and the parameter - - update - freq as 16 .", "entities": [[6, 7, "MethodName", "Transformer"], [12, 13, "MethodName", "Transformer"], [42, 44, "HyperparameterName", "batch size"]]}
{"text": "Results and ablations for De Fr Fr De are shown in Table 1 and 2 , respectively . We report case - sensitive SacreBLEU scores using Sacre - BLEU ( Post , 2018 ) 3 , using international tokenization for German\u2194French . German French For De Fr , iterative BT improves our baseline performance on newstest 2019 by about 2.5 BLEU . The addition of KD and model ensemble improves single model performance by 0.8 BLEU , but combining this with fine - tuning and reranking gives us a total of 2 BLEU . Our final submission for WMT20 achieves 35.0 BLEU points for German French translation ( ranked in the second place ) . French German For Fr De , we see similar improvements with iterative BT by about 2.3 BLEU . KD , ensembling , and fine - tuning add an additional 1.4 BLEU , with reranking contributing 0.9 BLEU . Our final submission for WMT20 achieves 36.6 BLEU points for French German translation ( ranked in the fourth among anonymous submissions ) .", "entities": [[23, 24, "MetricName", "SacreBLEU"], [28, 29, "MetricName", "BLEU"], [60, 61, "MetricName", "BLEU"], [75, 76, "MetricName", "BLEU"], [92, 93, "MetricName", "BLEU"], [101, 102, "MetricName", "BLEU"], [131, 132, "MetricName", "BLEU"], [145, 146, "MetricName", "BLEU"], [151, 152, "MetricName", "BLEU"], [160, 161, "MetricName", "BLEU"]]}
{"text": "This paper describes CAS IIE 's submission to the WMT20 German\u2194French news translation task . We investigate extremely deep models ( with 48 layers ) and exploit effective strategies to better utilize parallel data as well as monolingual data . Finally , our German French system achieved 35.0 BLEU and ranked the second among all anonymous submissions , and our French German system achieved 36.6 BLEU and ranked the fourth in all anonymous submissions .", "entities": [[48, 49, "MetricName", "BLEU"], [65, 66, "MetricName", "BLEU"]]}
{"text": "Following Chau et al ( 2020 ) , we consider how to apply the pretrained multilingual BERT model ( MBERT ; Devlin et al , 2019 ) to a target lowresource language , for which both labeled and unlabeled data is scarce . This model has produced strong CWRs for many languages ( Kondratyuk and Straka , 2019 , inter alia ) and has been the starting model for many studies on low - resource languages ( Muller et al , 2021 ; Pfeiffer et al , 2020 ; . MBERT covers the languages with the 104 largest Wikipedias , and it uses this data to con - struct a wordpiece vocabulary ( Wu et al , 2016 ) and train its transformer - based architecture ( Vaswani et al , 2017 ) . Although low - resource languages are slightly oversampled , high - resource languages still dominate both the final pretraining data and the vocabulary ( \u00c1cs , 2019 ; Devlin et al , 2019 ) . Chau et al ( 2020 ) note that target low - resource languages fall into three categories with respect to MBERT 's pretraining data : the lowest - resource languages in the data ( Type 1 ) , completely unseen low - resource languages ( Type 2 ) , and low - resouce languages with more representation ( Type 0 ) . 4 Due to their poor representation in the vocabulary , Type 1 and Type 2 languages achieve suboptimal tokenization and higher rates of the \" unknown \" wordpiece 5 when using MBERT out of the box . This hinders the model 's ability to capture meaningful patterns in the data , resulting in reduced data efficiency and degraded performance . We note that this challenge is exacerbated when modeling languages written in non - Latin scripts . MBERT 's vocabulary is heavily Latin - centric ( \u00c1cs , 2019 ; Muller et al , 2021 ) , resulting in a significantly larger portion of non - Latin scripts being represented with \" unknown \" tokens ( Pfeiffer et al , 2021b ) and further limiting the model 's ability to generalize . In effect , MBERT 's low initial performance on such languages can be attributed to its inability to represent the script itself . To alleviate the problem of poor tokenization , Chau et al ( 2020 ) propose to specialize MBERT using Vocabulary Augmentation ( VA ) . Given unlabeled data in the target language , they train a new wordpiece vocabulary on the data , then select the 99 most common wordpieces in the new vocabulary that replace \" unknown \" tokens under the original vocabulary . They then add these 99 wordpieces to the original vocabulary and continue pretraining MBERT on the unlabeled data for additional steps . They further describe a tiered variant ( TVA ) , in which a larger learning rate is used for the embeddings of these 99 new wordpieces . VA yields strong gains over unadapted multilingual language models on dependency parsing in four low - resource languages with Latin scripts . How - ever , no evaluation has been performed on other tasks or on languages with non - Latin scripts , which raises our first research question : RQ1 : Do the conclusions of Chau et al ( 2020 ) hold for other tasks and for languages with non - Latin scripts ? We can view VA and TVA as an instantation of a more general framework of vocabulary augmentation , shared by other approaches to using MBERT in low - resource settings . Given a new vocabulary V , number of wordpieces n , and learning rate multiplier a , the n most common wordpieces in V are added to the original vocabulary . Additional pretraining is then performed , with the embeddings of the n wordpieces taking on a learning rate a times greater than the overall learning rate . For VA , we set n = 99 and a = 1 , while we treat a as a hyperparameter for TVA . The related E - MBERT method of sets n = | V | and a = 1 . Investigating various other instantiations of this framework is an interesting research direction , though it is out of the scope of this work .", "entities": [[16, 17, "MethodName", "BERT"], [19, 20, "MethodName", "MBERT"], [90, 91, "MethodName", "MBERT"], [110, 111, "MethodName", "wordpiece"], [189, 190, "MethodName", "MBERT"], [228, 229, "DatasetName", "0"], [258, 259, "MethodName", "wordpiece"], [262, 263, "MethodName", "MBERT"], [308, 309, "MethodName", "MBERT"], [366, 367, "MethodName", "MBERT"], [403, 404, "MethodName", "MBERT"], [423, 424, "MethodName", "wordpiece"], [464, 465, "MethodName", "MBERT"], [487, 489, "HyperparameterName", "learning rate"], [510, 512, "TaskName", "dependency parsing"], [599, 600, "MethodName", "MBERT"], [618, 620, "HyperparameterName", "learning rate"], [653, 655, "HyperparameterName", "learning rate"], [661, 663, "HyperparameterName", "learning rate"], [691, 692, "MethodName", "MBERT"]]}
{"text": "To pretrain LAPT and VA models , we use the code of Chau et al ( 2020 ) , who modify the pretraining code of Devlin et al ( 2019 ) to only use the masked language modeling ( MLM ) loss . To generate VA vocabularies , we train a new vocabulary of size 5000 and select the 99 wordpieces that replace the most unknown tokens . We train with a fixed linear warmup of 1000 steps . To pretrain BERT models , we use the HuggingFace Transformers library ( Wolf et al , 2020 ) . Following Muller et al ( 2021 ) , we train a half - sized RoBERTa model with six layers and 12 attention heads . We use a byte - pair vocabulary of size 52000 and a linear warmup of 1 epoch . For LAPT , VA , and BERT , we train for up to 20 epochs total , selecting the highest - performing epoch based on validation masked language modeling loss . FASTT models are trained with the skipgram model for five epochs , with the default hyperparameters of Bojanowski et al ( 2017 ) . Training of downstream parsers and taggers follows Chau et al ( 2020 ) and Kondratyuk and Straka ( 2019 ) , with an inverse square - root learning rate decay and linear warmup , and layer - wise gradual unfreezing and discriminative finetuning . Models are trained with AllenNLP , version 2.1.0 , for up to 200 epochs with early stopping based on validation performance . We choose batch sizes to be the maximum that allows for successful training on one GPU .", "entities": [[35, 38, "TaskName", "masked language modeling"], [39, 40, "DatasetName", "MLM"], [41, 42, "MetricName", "loss"], [73, 75, "MethodName", "linear warmup"], [81, 82, "MethodName", "BERT"], [112, 113, "MethodName", "RoBERTa"], [134, 136, "MethodName", "linear warmup"], [146, 147, "MethodName", "BERT"], [166, 169, "TaskName", "masked language modeling"], [169, 170, "MetricName", "loss"], [222, 224, "HyperparameterName", "learning rate"], [226, 228, "MethodName", "linear warmup"], [254, 256, "MethodName", "early stopping"]]}
{"text": "We perform further analysis to investigate VA 's patterns of success . Concretely , we hypothesize that VA significantly improves the tokenizer 's coverage of target languages where it is most successful . Inspired by \u00c1cs ( 2019 ) , Chau et al ( 2020 ) , and Rust et al ( 2021 ) , we quantify tokenizer coverage using the percentage of tokens in the raw text that yield unknown wordpieces when tokenized with a given vocabulary ( \" UNK token percentage \" ) . These are tokens whose representations contain at least partial ambiguity due to the inclusion of the unknown wordpiece . Tab . 3 presents the UNK token percentage for each dataset using the MBERT vocabulary , averaged over each script and language type . This vocabulary is used in LAPT and represents the baseline level of vocabulary coverage . We also include the change in the UNK token percentage between the MBERT and VA vocabularies , which quantifies the coverage improvement . Both sets of values are juxtaposed against the average change in task - specific performance from LAPT to VA , representing the effect of augmenting the vocabulary on task - specific performance . We observe that off - the - shelf MBERT already at - tains relatively high vocabulary coverage for Type 0 and 1 languages , as well as languages written in Latin and Cyrillic scripts . On the other hand , up to one - fifth of the tokens in Arabic languages and one - sixth of those in Type 2 languages yield an unknown wordpiece . For these languages , there is great room for increasing tokenizer coverage , and VA indeed addresses this more tangible need . This aligns with the task - specific performance improvements for each group and helps to explain our results in 2.3 . It is notable that VA does not always eliminate the issue of unknown wordpieces , even in languages for which MBERT attains high vocabulary coverage . This suggests that the remaining unknown wordpieces in these languages are more sparsely distributed ( i.e. , they represent low frequency sequences ) , while the unknown wordpieces in languages with lower vocabulary coverage represent sequences that occur more commonly . As a result , augmenting the vocabulary in such languages quickly improves coverage while associating these commonly occurring sequences with each other , which benefits the overall tokenization quality . We further explore the association between the improvements in vocabulary coverage and taskspecific performance in Fig . 1 . Although we do not find that languages from the same types or scripts form clear clusters , we nonetheless observe a loose correlation between the two factors in question and see that VA delivers greater performance gains on Type 2 and Arabic - script languages compared to their Type 0/1 and Latin - script counterparts , respectively . To quantify the strength of this association , we also compute the language - level Spearman correlation between the change in UNK token percentage on the unlabeled dataset 7 from the MBERT to VA vocabulary and the task - specific performance improvements from LAPT to VA . The resulting \u03c1 - values - 0.29 for NER , 0.56 for POS tagging , and 0.81 for UD parsing - suggest that this set of factors is meaningful for some tasks , though additional and more fine - grained analysis in future work should give a more complete explanation . 3 Mix - in Specialization : VA and Transliteration We now expand on the observation made in 2.3 regarding the difficulties that MBERT encounters when faced with unseen low - resource languages in non - Latin scripts because of its inability to model the script . Having observed that VA is beneficial in such cases , we now investigate the interaction between this method and another specialization approach that targets this problem . Specifically , we consider the transliteration methods of Muller et al ( 2021 ) , in which unseen low - resource languages in non - Latin scripts are transliterated into the Latin script , often using transliteration schemes inspired by the Latin orthographies of languages related to the target language . They hypothesize that the increased similarity in the languages ' writing systems , combined with MBERT 's overall Latin - centricity , provides increased opportunity for crosslingual transfer . We can view transliteration as a inverted form of vocabulary augmentation : instead of adapting the model to the needs of the data , the data is adjusted to meet the assumptions of the model . Furthermore , the transliteration step is performed prior to pretraining MBERT on additional unlabeled data in the target language , the same stage at which VA is performed . In both cases , the ultimate goal is identical : improving tokenization and more effectively using available data . We can thus view transliteration and VA as two instantiations of a more general mix - in paradigm for model specialization , whereby various transformations ( mix - ins ) are applied to the data and/or model prior to performing additional pretraining . These mix - ins target different components of the experimental pipeline , which naturally raises our second research question : RQ2 : How do the VA and transliteration mix - ins for MBERT compare and interact ?", "entities": [[33, 34, "DatasetName", "Inspired"], [103, 104, "MethodName", "wordpiece"], [118, 119, "MethodName", "MBERT"], [156, 157, "MethodName", "MBERT"], [208, 209, "MethodName", "MBERT"], [219, 220, "DatasetName", "0"], [264, 265, "MethodName", "wordpiece"], [329, 330, "MethodName", "MBERT"], [498, 500, "MetricName", "Spearman correlation"], [514, 515, "MethodName", "MBERT"], [538, 539, "TaskName", "NER"], [548, 549, "DatasetName", "UD"], [589, 590, "TaskName", "Transliteration"], [603, 604, "MethodName", "MBERT"], [720, 721, "MethodName", "MBERT"], [780, 781, "MethodName", "MBERT"], [893, 894, "MethodName", "MBERT"]]}
{"text": "Predictions trained with Cross Entropy Each phrase is a sequence of word embeddings that is passed through an LSTM to produce a 512d vector representation for the premise and the hypothesis . Both vectors are used to compute the predicted conditional probability and calculate the loss . can not express P ( X ) = 0 exactly , but can get arbitrarily close in order to represent the probability of a phrase that is extremely unlikely . 6 Our model for P ( x ) and P ( x , y ) We train a neural network model to predict P ( x ) , P ( y ) , and P ( x | y ) for phrases x and y. This model consists of an LSTM that outputs a 512d vector which is passed through an additional 512d layer . We use 300d GloVe vectors ( Pennington et al , 2014 ) trained on 840B tokens as the word embedding input to the LSTM . We use the same model to represent both x and y regardless of which phrase is the premise or the hypothesis . Thus , we pass the sequence of word embeddings for phrase x through the model to get x , and we do the same for phrase y to get y. As previously described , we sum the elements of x and y to get the predicted denotational probabilities P ( x ) and P ( y ) . From x and y , we find the joint vector z , which we use to compute the predicted denotational conditional probability P ( x | y ) according to the equation in Section 5 . Figure 3 illustrates the structure of our model . Our training data consists of ordered phrase pairs x , y . We train our model to predict the denotational probabilities of each phrase ( P ( x ) and P ( y ) ) as well as the conditional probability P ( x | y ) . Typically the pair y , x will also appear in the training data . Our per - example loss is the sum of the cross entropy losses for P ( x ) , P ( y ) , and P ( x | y ) : L = \u2212 P ( x ) log Q ( x ) + ( 1\u2212P ( x ) ) log 1\u2212Q ( x ) \u2212 P ( y ) log Q ( y ) + ( 1\u2212P ( y ) ) log 1\u2212Q ( y ) \u2212 P ( x | y ) log Q ( x | y ) + ( 1\u2212P ( x | y ) ) log 1\u2212Q ( x | y ) We use the Adam optimizer with a learning rate of 0.001 , and a dropout rate of 0.5 . These parameters were tuned on the development data . Numerical issues In Section 5 , we described the probability vectors x as being in the positive orthant . However , in our implementation , we use unnormalized log probabilities . This puts all of our vectors in the negative orthant instead , but it prevents the gradients from becoming too small during training . To ensure that the vectors are in R N \u2212 , we clip the values of the elements of x so that x i \u2264 0 . To compute log P ( x ) , we sum the elements of x and clip the sum to the range ( log ( 10 \u221210 ) , \u22120.0001 ) in order to avoid errors caused by passing log ( 0 ) values to the loss function . The conditional log probability is simply log P ( x | y ) = log P ( x , y ) \u2212 log P ( y ) , where log P ( x , y ) is now the element - wise minimum : log P ( x , y ) = i min ( x i , y i ) This element - wise minimum is a standard pooling operation ( we take the minimum instead of the more common max pooling ) . Note that if x i > y i , neither element x i nor y i is updated with respect to the P ( x | y ) loss . Both x i and y i will always be updated with respect to the P ( x ) and P ( y ) components of the loss .", "entities": [[11, 13, "TaskName", "word embeddings"], [18, 19, "MethodName", "LSTM"], [45, 46, "MetricName", "loss"], [55, 56, "DatasetName", "0"], [127, 128, "MethodName", "LSTM"], [145, 146, "MethodName", "GloVe"], [165, 166, "MethodName", "LSTM"], [196, 198, "TaskName", "word embeddings"], [358, 359, "MetricName", "loss"], [465, 466, "MethodName", "Adam"], [466, 467, "HyperparameterName", "optimizer"], [469, 471, "HyperparameterName", "learning rate"], [570, 571, "DatasetName", "0"], [612, 613, "DatasetName", "0"], [617, 618, "MetricName", "loss"], [701, 703, "MethodName", "max pooling"], [733, 734, "MetricName", "loss"], [761, 762, "MetricName", "loss"]]}
{"text": "To train our model , we use phrase pairs x , y from the denotation graph generated on the training split of the FLICKR30 K corpus ( Young et al , 2014 ) . We consider all 271 , 062 phrases that occur with at least 10 images in the training split of the graph , to ensure that the phrases are frequent enough that their computed denotational probabilities are reliable . Since the FLICKR30 K captions are lemmatized in order to construct the denotation graph , all the phrases in the dataset described in this section are lemmatized as well . We include all phrase pairs where the two phrases have at least one image in common . These constitute 45 million phrase pairs x , y with P ( x | y ) > 0 . To train our model to predict P ( x | y ) = 0 , we include phrase pairs x , y that have no images in common if N \u00d7P ( x ) P ( y ) \u2265 N \u22121 ( N is the total number of images ) , meaning that x and y occur frequently enough that we would expect them to co - occur at least once in the data . This yields 2 million pairs where P ( x | y ) = 0 . For additional examples of P ( x | y ) = 1 , we include phrase pairs that have an ancestordescendant relationship in the denotation graph . We include all ancestor - descendant pairs where each phrase occurs with at least 2 images , for an additional 3 million phrase pairs . For evaluation purposes , we first assign 5 % of the phrases to the development pool and 5 % to the test pool . The actual test data then consists of all phrase pairs where at least one of the two phrases comes from the test pool . The resulting test data contains 10.6 % unseen phrases by type and 51.2 % unseen phrases by token . All phrase pairs in the test data contain at least one phrase that was unseen in the training or development data . The development data was created the same way . This dataset is available to download at http://nlp.cs.illinois.edu/ HockenmaierGroup / data.html . We train our model on the training data ( 42 million phrase pairs ) with batch size 512 for 10 epochs , and use the mean KL divergence on the conditional probabilities in the development data to select the best model . Since P ( x | y ) is a Bernoulli distribution , we compute the KL divergence for each phrase pair x , y as D KL ( P | | Q ) = P ( x | y ) log P ( x | y ) Q ( x | y ) + 1 \u2212 P ( x | y ) log 1 \u2212 P ( x | y ) 1 \u2212 Q ( x | y ) where Q ( x | y ) is the conditional probability predicted by our model .", "entities": [[136, 137, "DatasetName", "0"], [151, 152, "DatasetName", "0"], [226, 227, "DatasetName", "0"], [405, 407, "HyperparameterName", "batch size"]]}
{"text": "We evaluate our model using 1 ) the KL divergences D KL ( P | | Q ) of the gold individual and conditional probabilities P ( x ) and P ( x | y ) against the corresponding predicted probabilities Q , and 2 ) the Pearson correlation r , which expresses the correlation between two variables ( the per - item gold and predicted probabilities ) as a value between \u22121 ( total negative correlation ) and 1 ( total positive correlation ) . As described above , we compute the KL divergence on a per - item basis , and report the mean over all items in the test set . Table 1 shows the performance of our trained model on unseen test data . The full test data consists of 4.6 million phrase pairs , all of which contain at least one phrase that was not observed in either the training or development data . Our model does reasonably well at predicting these conditional probabilities , reaching a correlation of r = 0.949 with P ( x | y ) on the complete test data . On the subset of 123 , 000 test phrase pairs where both phrases are previously unseen , the model 's predictions are almost as good at r = 0.920 . On the subset of 3 , 100 test phrase pairs where at We also analyze our model 's accuracy on phrase pairs where the gold P ( x | y ) is either 0 or 1 . The latter case reflects an important property of the denotation graph , since P ( x | y ) = 1 when x is an ancestor of y. More generally , we can interpret P ( h | p ) = 1 as a confident prediction of entailment , and P ( h | p ) = 0 as a confident prediction of contradiction . Figure 4 shows the distribution of predicted conditional probabilities for phrase pairs where gold P ( h | p ) = 0 ( top ) and gold P ( h | p ) = 1 ( bottom ) . Our model 's predictions on unseen phrase pairs ( gray bars ) are nearly as accurate as its predictions on the full test data ( black bars ) .", "entities": [[47, 49, "MetricName", "Pearson correlation"], [238, 239, "MetricName", "accuracy"], [253, 254, "DatasetName", "0"], [314, 315, "DatasetName", "0"], [343, 344, "DatasetName", "0"]]}
{"text": "In Section 7.2 , we trained our probability model on both short phrase pairs for which we had gold probabilities and longer SNLI sentence pairs for which we estimated probabilities . We now evaluate the effectiveness of this model for textual entailment , and demonstrate that these predicted probabilities are informative features for predicting entailment on both SICK and SNLI . Model We first train an LSTM similar to the 100d LSTM that achieved the best accuracy of the neural models in Bowman et al ( 2015 ) . It takes GloVe word vectors as input and produces 100d sentence vectors for the premise and hypothesis . 200d tanh layers and a softmax layer for 3 - class entailment classification . We train the LSTM on the SNLI training data with batch size 512 for 10 epochs . We use the Adam optimizer with a learning rate of 0.001 and a dropout rate of 0.85 , and use the development data to select the best model . Next , we take the output vector produced by the LSTM for each sentence pair and append our predicted P ( h | p ) value ( the probability of the hypothesis given the premise ) . We train another classifier that passes this 201d vector through two tanh layers with a dropout rate of 0.5 and a final 3 - class softmax classification layer . Holding the parameters of the LSTM fixed , we train this model for 10 epochs on the SNLI training data with batch size 512 . Results Table 2 contains our results on SNLI . Our baseline LSTM achieves the same 77.2 % accuracy reported by Bowman et al ( 2015 ) , whereas a classifier that combines the output of this LSTM with only a single feature from the output of our probability model improves to 78.2 % accuracy . We use the same approach to evaluate the effectiveness of our predictions on SICK ( Table 3 ) . SICK does not have enough data to train an LSTM , so we combine the SICK and SNLI training data to train both the LSTM and the final model . When we add the predicted conditional probability as a single feature for each SICK sentence pair , performance increases from 81.5 % to 82.7 % accuracy . This approach outperforms the transfer learning approach of Bowman et al ( 2015 ) , which was also trained on both SICK and SNLI .", "entities": [[22, 23, "DatasetName", "SNLI"], [57, 58, "DatasetName", "SICK"], [59, 60, "DatasetName", "SNLI"], [66, 67, "MethodName", "LSTM"], [71, 72, "MethodName", "LSTM"], [76, 77, "MetricName", "accuracy"], [91, 92, "MethodName", "GloVe"], [112, 113, "MethodName", "softmax"], [124, 125, "MethodName", "LSTM"], [127, 128, "DatasetName", "SNLI"], [131, 133, "HyperparameterName", "batch size"], [141, 142, "MethodName", "Adam"], [142, 143, "HyperparameterName", "optimizer"], [145, 147, "HyperparameterName", "learning rate"], [177, 178, "MethodName", "LSTM"], [229, 230, "MethodName", "softmax"], [238, 239, "MethodName", "LSTM"], [250, 251, "DatasetName", "SNLI"], [254, 256, "HyperparameterName", "batch size"], [265, 266, "DatasetName", "SNLI"], [269, 270, "MethodName", "LSTM"], [275, 276, "MetricName", "accuracy"], [294, 295, "MethodName", "LSTM"], [311, 312, "MetricName", "accuracy"], [326, 327, "DatasetName", "SICK"], [332, 333, "DatasetName", "SICK"], [341, 342, "MethodName", "LSTM"], [347, 348, "DatasetName", "SICK"], [349, 350, "DatasetName", "SNLI"], [356, 357, "MethodName", "LSTM"], [375, 376, "DatasetName", "SICK"], [387, 388, "MetricName", "accuracy"], [393, 395, "TaskName", "transfer learning"], [410, 411, "DatasetName", "SICK"], [412, 413, "DatasetName", "SNLI"]]}
{"text": "The knowledge learned through model - based RL is contributed to a knowledge base that can be used for many tasks . So our KRR - RL framework enables a robot to dynamically generate partial world models for tasks under settings that were never experienced . For example , an agent does not know the current time is morning or noon , there are two possible values for variable \" time \" . Consider that our agent has learned world dynamics under the times of morning and noon . Our KRR - RL framework enables the robot to reason about the two transition systems under the two settings and generate a new transition system for this \" morning - or - noon \" setting . Without our framework , an agent would have to randomly select one between the \" morning \" and \" noon \" policies . To evaluate our policies dynamically constructed via KRR , we let an agent learn three controllers under three different environment settings - the navigation actions have decreasing success rates under the settings . In this experiment , the robot does not know which setting it is in ( out of two that are randomly selected ) . The baseline does not have the KRR capability of merging knowledge learned from different settings , and can only randomly select a policy from the two ( each corresponding to a setting ) . Experimental results show that the baseline agent achieved an average of 26.8 % success rate in navigation tasks , whereas our KRR - RL agent achieved 83.8 % success rate on average . Figure 7 shows the costs in a box plot ( including min - max , 25 % , and 75 % values ) . Thus , KRR - RL enables a robot to effectively apply the learned knowledge to tasks under new settings . Let us take a closer look at the \" time \" variable T . If T is the domain of T , the RL - only baseline has to compute a total of 2 | T | world models to account for all possible information about the value of T , where 2 | T | is the number of subsets of T . If there are N such variables , the number of world models grows exponentially to 2 | T | N . In comparison , the KRR - RL agent needs to compute only | T | N world models , which dramatically reduces the number of parameters that must be learned through RL while retaining policy quality .", "entities": [[50, 51, "DatasetName", "agent"], [76, 77, "DatasetName", "agent"], [130, 131, "DatasetName", "agent"], [160, 161, "DatasetName", "agent"], [245, 246, "DatasetName", "agent"], [263, 264, "DatasetName", "agent"], [408, 409, "DatasetName", "agent"], [424, 427, "HyperparameterName", "number of parameters"]]}
{"text": "Reinforcement learning ( RL ) , or specifically , policy gradient methods ( Williams , 1992 ) , have been frequently adopted to both task - oriented dialogue agents ( Roman Roman et al , 2020 ; Deng et al , 2021 ) or open - domain chitchat agents ( Li et al , 2016c ; Saleh et al , 2020 ) . It can either propagate non - differentiable loss ( Cai et al , 2019a ) or optimize an expert reward such as ease of answering ( Li et al , 2016c ) . It also adopts a scenario where a user simulator and a dialogue agent interact , and an Figure 1 : An example of the inference flow that shows the generated partner personas and the incorporation of partner personas generation into response generation . Figure 2 : The illustrated reinforcement learning strategy that directly backpropagates the response - related rewards from the critic network to the partner personas generator and the dialogue response generator . expert reward function can be defined to assign the goodness to each response generated ( Roman Roman et al , 2020 ) .", "entities": [[9, 12, "TaskName", "policy gradient methods"], [37, 40, "DatasetName", "Deng et al"], [70, 71, "MetricName", "loss"], [108, 109, "DatasetName", "agent"], [136, 138, "TaskName", "response generation"]]}
{"text": "We employ a critic network to compute the reinforcement learning rewards for our generators . We use a binary classifier as critic by extracting training instances ( s , r , L = 1 ) , 3 ( s A , r A , L = 1 ) and ( s B , r B , L = 1 ) . Then we can derive two negative samples as : ( s A , r B , L = 0 ) and ( s B , r A , L = 0 ) . Thereafter , we fine - tune on a binary classifier to be used as our critic in RL on the training partition by minimizing the binary cross - entropy loss : \u2212Llog ( P ( L | s , r ) ) \u2212 ( 1\u2212L ) log ( 1 \u2212 P ( L | s , r ) ) , where the binary label L indicates whether the response is relevant to the personas . We then use this classifier acting as a critic network that outputsL , conditioned on the generated partner personasp and generated responser . The predicted binary labelL is then converted to a reward R. R is a positive reward whenL = 1 , and R is a negative reward whenL = 0 . We empirically set the reward R for RL to { 1 , - 1 } for both PPG and DRG . We then update our RL agents with the following gradients : \u2206\u03b8 PPG = \u2212R \u25bd \u03b8 PPG log P ( p | s , c ) for the partner personas generator ( PPG ) , and for the dialogue response generator ( DRG ) : \u2206\u03b8 DRG = \u2212R \u25bd \u03b8 DRG log P ( r | s , p , c ) By formulating a reward that measures the relevance between generated partner personas and generated dialogue response , we are motivated by the following objectives : Further fine - tune the partner personas generator to generate personas that benefits the downstream dialogue response generation . Further fine - tune the dialogue response generator trained with ground - truth partner personas to adapt to noisy partner personas generated by the partner personas generator . As mentioned in Section 3.1 , the first motivation is that we are generating the complete personas profile . However , some of them can be irrelevant and unhelpful for the next - turn dialogue response generation . It could be challenging for the partner personas generator alone to identify which personas could be helpful . Therefore , we design such a reward to train the personas generator to learn to generate a set of personas that is more helpful for the downstream dialogue response generation . Our second motivation is that the dialogue response generator has not been exposed to the generated partner personas . We would like to fine - tune the response generator to mitigate the potential traininginference discrepancy . Experimental results indicate that our design empirically works well . The previous work from Cai et al ( 2019a ) employed critic network for RL loss backpropagation . The major difference is that their critic is trained in an adversarial manner ( Li et al , 2018 ) to pick up the gold response among other negative candidates . Also , their critic network conditions only on the dialogue response but not on the generated skeleton . In contrast , we aim for improved response generation with a classifier conditioning on both the generated personas and the generated response .", "entities": [[79, 80, "DatasetName", "0"], [91, 92, "DatasetName", "0"], [123, 124, "MetricName", "loss"], [220, 221, "DatasetName", "0"], [259, 260, "HyperparameterName", "\u03b8"], [294, 295, "HyperparameterName", "\u03b8"], [348, 350, "TaskName", "response generation"], [414, 416, "TaskName", "response generation"], [463, 465, "TaskName", "response generation"], [527, 528, "MetricName", "loss"], [586, 588, "TaskName", "response generation"]]}
{"text": "For both PPG and DRG , perplexity ( PPL ) is reported to measure the intrinsic performance with the ground truth output ( Roller et al , 2021 ) . We adopt well - known sequence evaluation metrics weighted BLEU ( Papineni et al , 2002 ) and Fmeasure for ROUGE - L ( Lin , 2004 ) as the extrinsic evaluations . For PPG , we also report Distinct - N with N= { 1 , 2 } to measure the response diversity ( Li et al , 2016a ; Cai et al , 2019b ; Gao et al , 2019 ) with the ratio of distinct unigrams / bigrams against total number of unigrams / bigrams generated .", "entities": [[6, 7, "MetricName", "perplexity"], [39, 40, "MetricName", "BLEU"], [50, 53, "MetricName", "ROUGE - L"]]}
{"text": "We conduct experiments on the PERSONACHAT , the most well - known multiturn dialogue dataset conditioned on personas . We follow the train / valid / test split from the PARLAI platform ( Miller et al , 2017 ) that contains about 65 , 000/7 , 800/7 , 500 instances respectively . Each instance contains about 8 utterances on average and about 4 traits for each of the self and partner personas . We denote the dataset with this original personas as PERSONACHAT - ORI . Later the original personas have been manually scrutinized by rephrasing , generalizing or specializing , which we denote as PERSONACHAT - REV . We apply the same preprocessing operation to both datasets . To train the critic for RL , we collected about 130 , 000 instances from the train split with equally distributed positive and negative samples . During early experiments , we found that feeding all traits yields lower performance . Retrieving Top - 3 relevant partner personas using BM25 ( Robertson and Walker , 1994 ) yields the best performance on the original personas . GPT - 2 This is a comparison model fine - tuned on GPT - 2 ( Radford et al , 2019 ) . We build the same three E2E systems described above , and the best model is selected , the third one . TRANSFERTRANSFO A comparison model built with a Transformer - based model pre - trained on gen - eral domain corpus , which is then fine - tuned on PERSONACHAT . PERCVAE This is a comparison model that employs a memory - augmented architecture incorporated with conditional variational autoencoder that exploits persona information . PAML This is a comparison model that leverages several dialogues collected from the same speaker to enhance response personality via metalearning ( Madotto et al , 2019 ) . As the authors did not conduct experiments on the PERSONACHAT - REV and no preprocessing scripts are provided for the revised personas , we only report the results of their model on the PERSONACHAT - ORI only . MTL w/ Personas Reconstruction This is a multi - task learning ( MTL ) comparison model ( Lee et al , 2021 ) trained to maximise the objective : \u03b1L PPG + ( 1 \u2212 \u03b1 ) L DRG , where L PPG represents the auxiliary PPG likelihood , and L DRG represents the DRG likelihood . \u03b1 is weight tuned over the validation set , and both tasks condition on dialogue context and self personas and share the same model parameters .", "entities": [[184, 185, "MethodName", "GPT"], [196, 197, "MethodName", "GPT"], [212, 213, "DatasetName", "E2E"], [235, 236, "MethodName", "Transformer"], [274, 276, "MethodName", "variational autoencoder"], [351, 352, "TaskName", "Reconstruction"], [355, 359, "TaskName", "multi - task learning"], [383, 384, "HyperparameterName", "\u03b1"], [405, 406, "HyperparameterName", "\u03b1"]]}
{"text": "For supervised phase , we set Adam ( Kingma and Ba , 2015 ) as our optimizer , with hyperparameters \u03b7 = 5e\u22124 , \u03b2 1 = 0.9 , \u03b2 2 = 0.999 , \u03f5 = 1e\u22128 . The models are fine - tuned for 2 epochs . For RL phase , we set Adam as our optimizer , with \u03b7 = 5e\u22126 , \u03b2 1 = 0.9 , \u03b2 2 = 0.999 , \u03f5 = 1e\u22128 . We update the model parameters every 20 training instances and validate the model performance every 50 updates . DistilBERT is used to initialize the model parameters for the critic network . We set Adam as our optimizer , with hyperparameters \u03b7 = 5e\u22126 , \u03b2 1 = 0.9 , \u03b2 2 = 0.999 , \u03f5 = 1e\u22128 . We fine - tune the critic for 1 epoch , and we freeze it empirically during RL . All the experiments are conducted based on the TRANSFORMERS library from HUGGINGFACE ( Wolf et al , 2020 ) .", "entities": [[6, 7, "MethodName", "Adam"], [16, 17, "HyperparameterName", "optimizer"], [24, 25, "HyperparameterName", "\u03b2"], [29, 30, "HyperparameterName", "\u03b2"], [54, 55, "MethodName", "Adam"], [57, 58, "HyperparameterName", "optimizer"], [64, 65, "HyperparameterName", "\u03b2"], [69, 70, "HyperparameterName", "\u03b2"], [96, 97, "MethodName", "DistilBERT"], [111, 112, "MethodName", "Adam"], [114, 115, "HyperparameterName", "optimizer"], [122, 123, "HyperparameterName", "\u03b2"], [127, 128, "HyperparameterName", "\u03b2"]]}
{"text": "We present the progressive change of the testing perplexity for DRG and PPG on PERSONACHAT - ORI in Figure 4 . 4 We observe that they improve D Human Evaluation Criteria ( Appropriateness ) : \" Who is more appropriate given the previous dialogue context ? \" ( Informativeness ) : \" Who is more diverse instead of null answers such as I do not know ? \" ( Engagingness ) : \" Who would you prefer to talk with for a long conversation ? \" ( Human - likeness ) : \" Which speaker do you think sounds more like a real person ? \" ( Coherence ) : \" Which persona contains traits that are more coherent to each other ? \" ( Interestingness ) : \" Which persona is more interesting and diverse ? \" The first four are from the existing work Zou et al , 2021 ) and we propose the last two for evaluating PPG . We report the first four for DRG , and we report the last four for PPG .", "entities": [[8, 9, "MetricName", "perplexity"], [125, 126, "DatasetName", "Interestingness"]]}
{"text": "We run all our experiments on a single NVIDIA TI - TAN RTX with 24 GB GPU memory . Fine - tuning the generators for 2 epochs as we have done on our preprocessed PERSONACHAT train split consumes about 3 - 4 hours . Fine - tuning our critic classifier for 1 epoch consumes about 1 hour . Our RL phase consumes about 15 hours to achieve the best validation loss before being early stopped . We report averaged results from 3 runs for our dialogue response generation and partner personas generation results reported in Table 1 , Table 4 and Table 7 .", "entities": [[70, 71, "MetricName", "loss"], [86, 88, "TaskName", "response generation"]]}
{"text": "The Variational Autoencoder is a generative model first introduced by Kingma and Welling ( 2013 ) . Like other autoencoders , VAEs learn a mapping q \u03b8 ( z | x ) from high dimensional input x to a low dimensional latent variable z. Instead of doing this in a deterministic way , the encoder learns the parameters of e.g. a normal distribution . The desired effect is that each area in the latent space has a semantic meaning and thus samples from p ( z ) can be decoded in a meaningful way . The decoder p \u03b8 ( x | z ) , also referred to as dec ( z ) , is trained to reconstruct the input x based on the latent variable z. In order to approximate \u03b8 via gradient descent the reparametrization trick ( Kingma and Welling , 2013 ) was introduced . This trick allows the gradient to flow through non - deterministic z by separating the discrete sampling operation . Let \u00b5 and \u03c3 be deterministic outputs of the encoder q \u03b8 ( \u00b5 , \u03c3 | x ) : z = \u00b5 + \u03c3 where \u223c N ( 0 , I ) and is the element - wise product . To prevent the model from pushing \u03c3 close to 0 and thus falling back to a deterministic autoencoder , the objective is extended by the Kullback - Leibler ( KL ) diver - gence between prior p ( z ) and q ( z | x ) : L ( \u03b8 ; x ) = \u2212KL ( q \u03b8 ( z | x ) | | p ( z ) ) + E q \u03b8 ( z | x ) [ logp \u03b8 ( x | z ) ] . ( 2 ) Bowman et al ( 2016 ) apply this idea for sentence generation using an RNN as encoder and decoder . They observe that a strong auto - regressive language modeling ability in the decoder reduces the information stored in the latent variable , right up to a complete collapse of the KL term . They explore different techniques to weaken the decoder , like word dropout or KL term weight annealing , as possible solutions . This guarantees a semantically rich latent variable and good sentence generation ability . Below , we describe how to combine both techniques in order to generate meaningful queries for Membership Query Synthesis .", "entities": [[1, 3, "MethodName", "Variational Autoencoder"], [19, 20, "MethodName", "autoencoders"], [26, 27, "HyperparameterName", "\u03b8"], [98, 99, "HyperparameterName", "\u03b8"], [131, 132, "HyperparameterName", "\u03b8"], [178, 179, "HyperparameterName", "\u03b8"], [196, 197, "DatasetName", "0"], [217, 218, "DatasetName", "0"], [225, 226, "MethodName", "autoencoder"], [258, 259, "HyperparameterName", "\u03b8"], [266, 267, "HyperparameterName", "\u03b8"], [282, 283, "HyperparameterName", "\u03b8"], [290, 291, "HyperparameterName", "\u03b8"]]}
{"text": "The data used in our experiments comes from two sources , ( i ) the SST2 ( Socher et al , 2013 ) and ( ii ) SAR14 ( Nguyen et al , 2014 ) . We limit sentence length to a maximum of 15 words . This is motivated by lower training times and the tendency of vanilla VAEs not to perform well on longer sentences ( Shen et al , 2019 ) . ( Hochreiter and Schmidhuber , 1997 ) with size 512 . As additional regularization we set weight dropout to 0.3 ( Srivastava et al , 2014 ) . Input embeddings are also of size 512 , which allows us to share the embed - ding weights with the softmax weights of the output layer ( Press and Wolf , 2016 ) . To prevent posterior collapse we use logistic annealing of the KL term weight and weaken the decoder by applying word dropout with probability 0.5 ( Bowman et al , 2016 ) . The model is trained using the Adam optimizer ( Kingma and Ba , 2014 ) with an initial learning rate of 0.005 . Once the KL term weight is close to 1 , the learning weight is linearly decreased to 0 . The training stops after 20 epochs and the latent variable z has k = 50 dimensions . The trained VAE achieves a reconstruction loss of 45.3 and KL divergence of 13.2 on the SST2 training set . Learner The Learner is an SVM 1 with linear kernel . Each instance is represented as the latent variable z learned by the autoencoder . The latent variable is a vector with 50 dimensions and the SVM is trained on this representation . We calculate classification performance on the reduced SST2 test set and report F1 - scores . Generator The generator is the decoder of the VAE described above . Once a point z in feature space is selected , it is used as the input of the decoder x = dec ( z ) which generates the human readable sentence x in an autoregressive way .", "entities": [[15, 16, "DatasetName", "SST2"], [123, 124, "MethodName", "softmax"], [175, 176, "MethodName", "Adam"], [176, 177, "HyperparameterName", "optimizer"], [187, 189, "HyperparameterName", "learning rate"], [209, 210, "DatasetName", "0"], [223, 225, "HyperparameterName", "k ="], [230, 231, "MethodName", "VAE"], [234, 235, "MetricName", "loss"], [244, 245, "DatasetName", "SST2"], [253, 254, "MethodName", "SVM"], [271, 272, "MethodName", "autoencoder"], [284, 285, "MethodName", "SVM"], [298, 299, "DatasetName", "SST2"], [303, 304, "MetricName", "F1"], [315, 316, "MethodName", "VAE"]]}
{"text": "The instances selected or generated by any model or baseline are annotated manually by one human coder . 2 Although the pool data has labels on the review level , we do not use these labels in our experiments . Positive reviews can include negative sentences and vice versa . This means that using document - level labels would introduce noise and might impair the baselines . During each of the three experimental runs , all models and baselines are annotated simultaneously by the same person . The annotator is presented with one instance at a time and has no information which of the models has produced each particular instance . Once a label is selected , it is transmitted to the corresponding model and triggers the selection / generation of the next instance . Thus , at any given time there is one unlabeled instance for each model or baseline . From this set of unlabeled instances , one instance is chosen randomly and presented to the annotator . This procedure is repeated until 500 instances are labeled for each model or baseline . Hiding the instance source from the annotator is intended to prevent any bias during the annotation process . 6 Results and Analysis 6.1 Classification Performance F - scores as a function of annotated instances Figure 3 shows learning curves for the different AL strategies and baselines as a function of the number of annotation instances added to the training data . The random and least conf baselines perform reasonably well . Least conf struggles in the beginning , likely attributed to the minimal seed set . Once enough instances are labeled it catches up . Gen uniform has a strong start but , after around 200 instances , is outperformed by the nearest neighbor approaches which yield the highest F1 - scores . Among the nearest neighbor approaches , the uniform schedule ranks better than wang . The same behaviour is observed for the generation methods , although gen wang produces the worst results overall . Overall , gen uniform is competitive with respect to F1 - scores and shows that sentences generated from points in the feature space are informative and useful for training a text classifier . F - scores as a function of annotation time AL simulations have often been criticized for reporting unrealistic results , based merely on the number of annotated instances ( see , e.g. , Settles ( 2009 ) , pp . 37 ff . ) . It is well known , however , that the number of annotated instances is often not a good predictor for the real annotation costs . AL strategies tend to select the hard nuts for human annotators and it is not unreasonable to assume that the annotation of N instances in an AL setup might take longer and thus might be more expensive than annotating the same number of randomly selected instances . Therefore , we also show learning curves as a function of annotation time ( Figure 4 ) . The results show a clear advantage for the generation models . The reduction in annotation time is due to shorter query length and less neutral or noisy instances , as shown in Table 2 . This speeds up the annotation by a significant margin while providing the Learner with informative instances , despite their short length . Figure 5 shows that the length of generated instances increase over time and further exploration also hints that the generated length is correlated with the length of the sentences in the seed set . As listed in Table 2 , the random baseline reveals that 36.8 percent of sentences in the pool are neutral / artifacts and positive sentences outweigh negative ones by a factor of 2.6 . This means that random sampling results in unbalanced datasets with far more positive examples . Our generation method does not show this disadvantage . In contrast , the generated instances maintain a more balanced distribution of class labels and are less likely to be skipped . These are indicators that the selected points are close to the hyperplane and the VAE is able to generate coherent and highly informative sentences from them .", "entities": [[208, 209, "TaskName", "Classification"], [304, 305, "MetricName", "F1"], [350, 351, "MetricName", "F1"], [694, 695, "MethodName", "VAE"]]}
{"text": "NER ( Sundheim , 1995 ) is a fundamental task in natural language processing . The task has a lot of applications in various domains such as social media ( Derczynski et al , 2017 ) , news ( Tjong Kim Sang , 2002 ; Tjong Kim Sang and De Meulder , 2003 ) , Ecommerce ( Fetahu et al , 2021 ; Wang et al , 2021b ) , and medical domains ( Dogan et al , 2014 ; Li et al , 2016 ) . Recently , pretrained contextual embeddings such as BERT ( Devlin et al , 2019 ) , XLM - R and LUKE ( Yamada et al , 2020 ) have significantly improved the NER performance . The embeddings are trained on large - scale unlabeled data such as Wikipedia , which can significantly improve the contextual representations of named entities . Recent efforts ( Peters et al , 2018 ; Akbik et al , 2018 ; Strakov\u00e1 et al , 2019 ) concatenate different kinds of pretrained embeddings to form stronger token representations . Moreover , the embeddings are trained over long documents , which allows the model to easily model long - range dependencies to disambiguate complex named entities in the sentence . Recently , a lot of work shows that utilizing the document - level contexts in the CoNLL NER datasets can significantly improve token representations and achieves state - of - the - art performance ( Yu et al , 2020 ; Luoma and Pyysalo , 2020 ; Yamada et al , 2020 ; Wang et al , 2021a ) . However , the lack of context in the MultiCoNER datasets means the embeddings can not take advantage of long - range dependencies for entity disambiguation . Recently , Wang et al ( 2021b ) use Google search to retrieve external contexts of the input sentence and successfully achieve state - of - the - art performance across multiple domains . We adopt this idea so that the embeddings can utilize the related knowledge by taking the advantage of long - range dependencies to form stronger token representations . Comparing with Wang et al ( 2021b ) , we build the local KB based on Wikipedia because the KB matches the indomain data of the shared task and is fast enough to meet the time requirement in the test phase 2 . Fine - tuning pretrained contextual embeddings is a useful and effective approach to many NLP tasks . Recently , some of the research efforts propose to further train the fine - tuned embeddings with specific training data or in a larger model architecture to improve model performance . Shi and Lee ( 2021 ) proposed two - stage fine - tuning , which first trains a general multilingual Enhanced Universal Dependency ( Bouma et al , 2021 ) parser and then finetunes on each specific language separately . Wang et al ( 2021a ) proposed to train models through concatenating fine - tuned embeddings . We extend these ideas as multi - stage fine - tuning , which improves the accuracy of monolingual models that use finetuned multilingual embeddings as initialization in training . Moreover , multi - stage fine - tuning can accelerate the training process in system building .", "entities": [[0, 1, "TaskName", "NER"], [30, 31, "DatasetName", "Derczynski"], [94, 95, "MethodName", "BERT"], [103, 104, "MethodName", "XLM"], [119, 120, "TaskName", "NER"], [227, 228, "TaskName", "NER"], [278, 279, "DatasetName", "MultiCoNER"], [293, 295, "TaskName", "entity disambiguation"], [305, 306, "DatasetName", "Google"], [521, 522, "MetricName", "accuracy"]]}
{"text": "In our system , we use XLM - R large as the embedding for all the tracks . It is a multilingual model and is applicable to all tracks . Given the input sentence x and the retrieved contexts { x 1 , , x k } , we add the separator token ( i.e. , \" < /s > \" in XLM - R ) between them and concatenated them together to form the inputx of the NER module . We chunk retrieved texts to avoid the amount of subtoken in the sequence exceeding the maximum subtoken length in XLM - R ( i.e. , 512 in XLM - R ) . Our system regards the NER task as a sequence labeling problem . The embedding layer in the NER module encode the concatenated sequenc\u1ebd x and output the corresponding token representa - tions { v 1 , , v n , } . The module then feeds the token representations { v 1 , , v n } of the input sentence into a linear - chain CRF layer to obtain the conditional probability p \u03b8 ( y | x ) : \u03c8 ( y \u2032 , y , v i ) = exp ( W T y v i + b y \u2032 , y ) ( 1 ) p \u03b8 ( y | x ) = n i=1 \u03c8 ( y i\u22121 , y i , v i ) y \u2032 Y ( x ) n i=1 \u03c8 ( y \u2032 i\u22121 , y \u2032 i , v i ) where \u03b8 represents the model parameters and Y ( x ) denotes the set of all possible label sequences given x. In the potential function \u03c8 ( y \u2032 , y , v i ) , W T y v i is the emission score and b y \u2032 , y is the transition score , where W T R t\u00d7d and b R t\u00d7t are parameters and the subscripts y \u2032 and y are the indices of the matrices . During training , the negative log - likelihood loss L NLL ( \u03b8 ) = \u2212 log p \u03b8 ( y * | x ) for the concatenated input sequence with gold labels y * is used . During inference , the model prediction\u0177 \u03b8 is given by Viterbi decoding .", "entities": [[6, 7, "MethodName", "XLM"], [62, 63, "MethodName", "XLM"], [78, 79, "TaskName", "NER"], [100, 101, "MethodName", "XLM"], [108, 109, "MethodName", "XLM"], [117, 118, "TaskName", "NER"], [130, 131, "TaskName", "NER"], [179, 180, "MethodName", "CRF"], [187, 188, "HyperparameterName", "\u03b8"], [223, 224, "HyperparameterName", "\u03b8"], [265, 266, "HyperparameterName", "\u03b8"], [350, 353, "MetricName", "log - likelihood"], [353, 354, "MetricName", "loss"], [355, 356, "MetricName", "NLL"], [357, 358, "HyperparameterName", "\u03b8"], [363, 364, "HyperparameterName", "\u03b8"], [389, 390, "HyperparameterName", "\u03b8"]]}
{"text": "Given predictions { \u0177 \u03b8 1 , , \u0177 \u03b8m } from m models with different random seeds , we use majority voting to generate the final prediction\u0177 . We convert the label sequences into entity spans to perform majority voting . Following Yamada et al ( 2020 ) , the module ranks all spans in the predictions by the number of votes in descending order and selects the spans with more than 50 % votes into the final prediction . The spans with more votes are kept if the selected spans have overlaps and the longer spans are kept if the spans have the same votes . ( Nguyen et al , 2016 ) containing a lot of natural language questions ; ORCAS ( Search Query NER ) contains user queries from Microsoft Bing ( Craswell et al , 2020 ) . The MSQ and ORCAS samples are taken as out - ofdomain data in the shared task . The training and development sets only contain a small collection of samples of these two domains and mainly contain data from the LOWNER domain . The test set , however , contains much more MSQ and ORCAS samples to assess the out - of - domain performance . The results of the shared task are evaluated with the entity - level macro F1 scores , which treat all the labels equally . In comparison , most of the publicly available NER datasets ( e.g. , CoNLL 2002CoNLL , 2003 are evaluated with the entity - level micro F1 scores , which emphasize common labels .", "entities": [[4, 5, "HyperparameterName", "\u03b8"], [17, 18, "DatasetName", "seeds"], [123, 124, "DatasetName", "ORCAS"], [127, 128, "TaskName", "NER"], [146, 147, "DatasetName", "ORCAS"], [196, 197, "DatasetName", "ORCAS"], [221, 223, "MetricName", "macro F1"], [240, 241, "TaskName", "NER"], [256, 258, "MetricName", "micro F1"]]}
{"text": "NER Model Training Before building the final system , we compare a lot of variants of the system . We train these variant models on the training set for 3 times each with different random seeds and compare the averaged performance of the models . According to the dataset sizes , we train the models for 5 epochs , 10 epochs and 100 epochs for multilingual , monolingual and code - mixed models respectively . Our final NER models are trained on the combined dataset including both the training and development sets on each track to fully utilize the labeled data . For models trained on the training set , we use the best macro F1 on the development set during training to select the best model checkpoint . For models trained on the combined dataset , Continue Pretraining To make XLM - R learn the data distribution of the shared task , we combine the training and development sets on the monolingual tracks to build a corpus to continue pretrain XLM - R. Specifically , we collocate all sentences according to their languages , then cut the text into chunks of fixed length , and train the model on these text chunks using the Masked Language Modeling objective . We continue pretrain XLM - R for 5 epochs . We use the continue pretrained XLM - R model as the initialization of the multilingual 7 Please refer to Appendix A for detailed settings . models during training .", "entities": [[0, 1, "TaskName", "NER"], [35, 36, "DatasetName", "seeds"], [77, 78, "TaskName", "NER"], [114, 116, "MetricName", "macro F1"], [141, 142, "MethodName", "XLM"], [171, 172, "MethodName", "XLM"], [205, 208, "TaskName", "Masked Language Modeling"], [213, 214, "MethodName", "XLM"], [225, 226, "MethodName", "XLM"]]}
{"text": "There are 55 teams that participated in the shared task . Due to limited space , we only compare our system with the systems from teams USTC - NELSLIP , RACAI and Sliced 10 . In the postevaluation phase , we evaluate a baseline system without using the knowledge retrieval module to further show the effectiveness of our knowledgebased system . The official results and the results of our baseline system are shown in Table 1 . Our system performs the best on 10 out of 13 tracks and is competitive on the other 3 tracks . Moreover , our system outperforms our baseline by 14.39 F1 on average , which shows the knowledge retrieval module is extremely helpful for disambiguating complex entities leading to significant improvement on model performance .", "entities": [[106, 107, "MetricName", "F1"]]}
{"text": "To further show the effectiveness of our knowledgebased system , we show the relative improvements of our system over our baseline system on each domain in Table 2 . We observe that in most of the cases , the two out - of - domain test sets have more relative improvements than the in - domain test set . This observation shows that the knowledge from Wikipedia can not only improve the performance of the LOWNER domain which is the same domain as the KB , but also has very strong cross - domain Table 2 : Per - domain macro F1 score on the test set of our system and our baseline system for each language . \u2206 represents the relative improvements of our system over the baseline system . transferability to other domains such as web questions and user queries . According to the baseline performance over the three domains , the ORCAS domain has the lowest score , which shows the challenges in recognizing named entities in user queries . However , our retrieved documents in KB can significantly ease the challenges in this domain and results in the highest improvement out of the three domains .", "entities": [[100, 102, "MetricName", "macro F1"], [154, 155, "DatasetName", "ORCAS"]]}
{"text": "To evaluate the relevance of the retrieval results to the query , we define a character - level relevance metric , which calculates the Intersectionover - Union ( IoU ) between the characters of query and result . Assuming that the character sets 11 of query and retrieval result are A and B respectively , then the character - level IoU is A\u2229B A\u222aB . We calculate the character - level IoU of the sentence and its top - 1 retrieval result on all tracks , and plot its distribution on the training , development and test set in Figure 3 . We have the following observations : 1 ) the IoU values are concentrated around 1.0 on the training and development sets of EN , ES , NL , RU , TR , KO , FA , which indicates that most of the samples were derived from Wikipedia . Therefore , by retrieving , we can obtain the original documents for these samples . 2 ) the distribution of data on the test set is consistent with the training and development sets for most languages , except for TR . On TR , the character - level IoU values of the samples and query results cluster at around 0.5 . We hypothesize that this is because the source of the test set for TR is different from the training set . However , the model still performs strongly on this language , suggesting that the model can mitigate the difficulties caused 11 The sets take repeat characters as different characters . by inconsistent data distribution by retrieving the context from Wikipedia .", "entities": [[28, 29, "MetricName", "IoU"], [60, 61, "MetricName", "IoU"], [71, 72, "MetricName", "IoU"], [111, 112, "MetricName", "IoU"], [136, 137, "MethodName", "FA"], [198, 199, "MetricName", "IoU"]]}
{"text": "As we mentioned in Section 3.1 , there are three context processing options , which are : 1 ) use the matched paragraph ; 2 ) use the matched sentence ; 3 ) use the matched sentence but remove the wiki anchors . We denote the three options as PARA , SENT and SENT - LINK respectively . Entity Retrieval with Gold Entities We use gold entities on the development set to see whether the model performance can be improved . This can be seen as the most ideal scenario for iterative retrieval . We denote this process as ITER G and use PARA for the context type . In Table 3 , we can observe that : 1 ) For the three context options , PARA is the best option for EN , ES , NL , RU , TR , KO , FA , MIX and MULTI . SENT - LINK is the best option for HI and BN . For DE and ZH , SENT and SENT - LINK are competitive . As a result , we choose SENT for the two languages since we believe the wiki anchors from the Wikipedia can help model performance ; 2 ) Comparing with the baseline , the knowledge from Google Search can improve model performance . Based on the best context option of each track , the knowledge from Wikipedia is better than the online search engine ; 3 ) For ITER G , we can find that the context can further Iterative Entity Retrieval with Predicted Entities Based on the results in Table 3 , we further analyze how the predicted entity mentions can improve the retrieval quality . We denote the iterative entity retrieval with predicted mentions as ITER P . In the experiment , we set T = 2 . 12 We extract the predicted mentions of the development sets from the models based on the best context option for each track . We conduct the experiments over HI , BN and MIX which have significant improvement with ITER G . In Table 4 , we also list the performance of ITER G for reference , which can be seen as using the predicted mentions with 100 % accuracy . From the results , we observe that only MIX can be improved . Since iterative entity retrieval uses predicted mentions as a part of retrieval query , the performance of mention detection directly affects the retrieval quality . To further analyze the observation in Table 4 , we evaluate the mention F1 score of the NER models with sentence retrieval . For comparison with mention detection performance of NER models , we additionally train mention detection models by discarding the entity labels during training . From the results in Table 5 , we suspect the low mention F1 introduces noises in the knowledge retrieval module for BN and HI , which lead to the decline of performance as shown in Table 4 . Moreover , the mention F1 of mention detection models ( second row of Table 5 ) only outperform that of the NER models ( first row of Table 5 ) in a moderate scale . Therefore , we train the ITER models only for the code - mixed track and use the NER models with sentence retrieval to predict mentions .", "entities": [[58, 60, "TaskName", "Entity Retrieval"], [144, 145, "MethodName", "FA"], [210, 211, "DatasetName", "Google"], [254, 256, "TaskName", "Entity Retrieval"], [285, 287, "TaskName", "entity retrieval"], [372, 373, "MetricName", "accuracy"], [389, 391, "TaskName", "entity retrieval"], [425, 427, "MetricName", "F1 score"], [429, 430, "TaskName", "NER"], [442, 443, "TaskName", "NER"], [471, 472, "MetricName", "F1"], [501, 502, "MetricName", "F1"], [518, 519, "TaskName", "NER"], [549, 550, "TaskName", "NER"]]}
{"text": "For the knowledge retrieval module , we retrieve top - 10 related results from the KB . For iterative entity retrieval , we set T = 2 . In masked language model pretraining , we use a learning rate of 5 \u00d7 10 \u22125 . For the NER module , we use a learning rate of 5 \u00d7 10 \u22126 for fine - tuning the XLM - R embeddings and use a learning rate of 0.05 to update the parameters in the CRF layer following Wang et al ( 2021b ) . Each NER model built by our system can be trained and evaluated on a single Tesla V100 GPU with 16 GB memory . For the ensemble module , we train about 10 models for each track . A. ( Akbik et al , 2018 ) , ELMo embeddings ( Peters et al , 2018 ; Che et al , 2018 ) , XLM - R embeddings fine - tuned on the whole training data and XLM - R embeddings fine - tuned on the language data by multi - stage finetuning . We only feed the knowledge - based input into XLM - R embeddings and feed the original input into other embeddings because it is hard for the other embeddings ( especially for LSTM - based embeddings such as Flair and ELMo ) to encode such a long input . We use Bi - LSTM encoder to encode the concatenated embeddings with a hidden state of 1 , 000 and then feed the output token representations into the CRF layer . Following most of the previous efforts , we use SGD optimizer with a learning rate of 0.01 . For ACE , we search the embedding concatenation for 30 episodes .", "entities": [[19, 21, "TaskName", "entity retrieval"], [37, 39, "HyperparameterName", "learning rate"], [47, 48, "TaskName", "NER"], [53, 55, "HyperparameterName", "learning rate"], [65, 66, "MethodName", "XLM"], [72, 74, "HyperparameterName", "learning rate"], [82, 83, "MethodName", "CRF"], [93, 94, "TaskName", "NER"], [138, 139, "MethodName", "ELMo"], [154, 155, "MethodName", "XLM"], [167, 168, "MethodName", "XLM"], [193, 194, "MethodName", "XLM"], [216, 217, "MethodName", "LSTM"], [224, 225, "MethodName", "ELMo"], [237, 238, "MethodName", "LSTM"], [261, 262, "MethodName", "CRF"], [273, 274, "MethodName", "SGD"], [274, 275, "HyperparameterName", "optimizer"], [277, 279, "HyperparameterName", "learning rate"]]}
{"text": "Our EN - PT input lexicon has 931 , 568 manually validated translations ( words and phrases ) . This lexicon has been compiled in a long term effort started in the context of project ISTRION 2 . The translations were extracted automatically from several corpora , including Europarl ( Koehn and Monz , 2005 ) , JRC - Acquis ( Steinberger et al , 2006 ) , OPUS EMEA ( Tiedemann , 2009 ) and others , using a combination of complementary alignment and extraction methods : GIZA ( Och and Ney , 2003 ) , Anymalign ( Lardilleux and Lepage , 2009 ) , spelling similarity measure SpSim ( Gomes and Lopes , 2011 ) combined with co - occurrence Dice measure , and others . The automatically extracted word and phrasal translations were automatically classified , prior to human validation , using an SVM classifier trained on previously validated translations as described by Mahesh et al ( 2015 ) . The automatic classification speeds up human validation because very few translations ( less than 5 % ) are incorrectly classified , and only those need to be manually labeled as correct or incorrect . We did not perform any extraction or validation of new translations from the corpus provided for this shared task . We did , however , complement our lexicon with cognate and homograph alignments using the SpSim ( Gomes and Lopes , 2011 ) spelling similarity measure .", "entities": [[69, 70, "MethodName", "EMEA"], [122, 123, "MetricName", "Dice"], [146, 147, "MethodName", "SVM"]]}
{"text": "Since no development data was supplied , we took the initiative to prepare some development sets in order to have an idea of the most promising set of parameters to be used in our system over the provided data to produce the intended translations . As such , several documents were removed from the original training data , composed by the medlinepubmed , biological and health sets , applying the training methods on the remaining documents and using the selected ones to translate and compare the translations against their originals by determining their BLEU ( Papineni et al , 2002 ) scores . However , in order to get a clearer picture of the type of results that could be expected , some additional tests were carried out including the selected set of documents in the training data . Our translation model supports : a conservative extraction approach , which is more restrictive , allowing fewer translation equivalents , having a lower recall but a higher precision ; and a flexible extraction approach , which is more permissive , allowing a larger number of equivalents but at the cost of an increase of incorrect ones . We were interested in evaluating the impact of both approaches on results . Table 4 shows the average results on both translation directions of those preliminary tests , consisting of the average BLEU scores for the conservative ( cons . ) and flexible ( flex . ) approaches , as well as the average times taken to translate the documents on either extraction approaches . Those results concern the following configurations : full : the documents used for testing were not removed from the training set ( medlinepubmed , biological and health ) ; dev : the documents used for testing were removed form the training set ; dev - europarl : the same as dev , but including the europarl corpus ; and dev - europarl - low : the same as dev - europarl , but assigned a lower relevance to the europarl corpus . These preliminary tests have shown that the flexible extraction approach produced on average better translation results when the reference documents were not included in the test set , which is the normal testing situation , so we used the flexible approach . The Europarl corpus 4 , which is significantly larger ( 54 , 543 , 044 words in English and 60 , 375 , 477 words in Portuguese ) , was tested as a source of additional term coverage , which allowed a translation quality improvement lower than 1 BLEU point . However , given its significant increase in processing time because of its large size , a time increase around 14 times larger , we had to drop it from the submission tests due to deadline constraints . Additionally , these results show that assigning a lower relevance to a corpus from a totally different domain may have some positive impact on average results . Once we have decided , from this initial testing preparation , which would be the most promising and interesting features to use in the final runs , we ran the training processes again to include the documents that have been left out , this way using the full data provided by the organizers for the runs to be submitted .", "entities": [[93, 94, "MetricName", "BLEU"], [228, 229, "MetricName", "BLEU"], [433, 434, "MetricName", "BLEU"]]}
{"text": "This last run shares the same features as the previous run ( assigning higher relevances to corresponding corpora ) but this time our bilingual lexicon and named entities database was included for term coverage improvement , and an alignment based on cognates ( Gomes and Lopes , 2011 ) is used . About our bilingual lexicon , considering that it was built mainly from the European legislation , it was given a lower relevance because past experiences have shown us that , when the domain is not shared with the texts to be translated , it should not have the same relevance in order to reduce the probability of using inadequate terms for the intended translation domain or subject . Again , this is a situation that has also been confirmed and noted in Table 4 between dev - europarl and deveuroparl - low : reducing the relevance of europarl contributed to a slight score increase compared to when the relevance is the same . As a side note , translating the tests took nearly 14 hours for each run 6 . Had we included europarl , judging by Table 4 , we would have taken nearly 200 hours , which is more than a week , expecting to simply gain 0.75 BLEU points , on average , so we had no other option than leaving it out . Such increase in translation time is due to the substantial increase of translation equivalents available for decoding from such a large corpus . The decision to carry out the alignment based on cognates was taken because after a first run of tests we realized that many of the untranslated terms referred to medical terms and diseases , which shared many letters between both languages and therefore had a high level of cognaticity . All these changes allowed a significant reduction of the unique untranslated terms to a total of 4700 , and for all the reasons in this subsection , we have considered this run as being our best .", "entities": [[212, 213, "MetricName", "BLEU"]]}
{"text": "Recent years have seen a surge of interests in fine - grained entity typing ( FET ) as it serves as an important cornerstone of several nature language processing tasks including relation extraction ( Mintz et al , 2009 ) , entity linking ( Raiman and Raiman , 2018 ) , and knowledge base completion ( Dong et al , 2014 ) . To reduce manual efforts in labelling training data , distant supervision ( Mintz et al , 2009 ) has been widely adopted by recent FET systems . With the help of an external knowledge base ( KB ) , an entity mention is first Figure 1 : T - SNE visualization of the mention embeddings generated by NFETC ( left ) and CLSC ( right ) on the BBN dataset . Our model ( CLSC ) clearly groups mentions of the same type into a compact cluster . linked to an existing entity in KB , and then labeled with all possible types of the KB entity as supervision . However , despite its efficiency , distant supervision also brings the challenge of outof - context noise , as it assigns labels in a context agnostic manner . Early works usually ignore such noise in supervision ( Ling and Weld , 2012 ; Shimaoka et al , 2016 ) , which dampens the performance of distantly supervised models . Towards overcoming out - of - context noise , two lines of work have been proposed to distantly supervised FET . The first kind of work try to filter out noisy labels using heuristic rules ( Gillick et al , 2014 ) . However , such heuristic pruning significantly reduces the amount of training data , and thus can not make full use of distantly annotated data . In contrast , the other thread of works try to incorporate such imperfect annotation by partiallabel loss ( PLL ) . The basic assumption is that , for a noisy mention , the maximum score associated with its candidate types should be greater than the scores associated with any other non - candidate types ( Ren et al , 2016a ; Abhishek et al , 2017 ; Xu and Barbosa , 2018 ) . Despite their success , PLLbased models still suffer from Confirmation Bias by taking its own prediction as optimization objective in the next step . Specifically , given an entity mention , if the typing system selected a wrong type with the maximum score among all candidates , it will try to further maximize the score of the wrong type in following optimization epoches ( in order to minimize PLL ) , thus amplifying the confirmation bias . Such bias starts from the early stage of training , when the typing model is still very suboptimal , and can accumulate in training process . Related discussion can be also found in the setting of semi - supervised learning ( Lee et al , 2006 ; Laine and Aila , 2017 ; Tarvainen and Valpola , 2017 ) . In this paper , we propose a new method for distantly supervised fine - grained entity typing . Enlightened by ( Kamnitsas et al , 2018 ) , we propose to effectively utilize imperfect annotation as model regularization via Compact Latent Space Clustering ( CLSC ) . More specifically , our model encourages the feature extractor to group mentions of the same type as a compact cluster ( dense region ) in the representation space , which leads to better classification performance . For training data with noisy labels , instead of generating pseudo supervision by the typing model itself , we dynamically construct a similarity - weighted graph between clean and noisy mentions , and apply label propagation on the graph to help the formation of compact clusters . Figure 1 demonstrates the effectiveness of our method in clustering mentions of different types into dense regions . In contrast to PLL - based models , we do not force the model to fit pseudo supervision generated by itself , but only use noisy data as part of regularization for our feature extractor layer , thus avoiding bias accumulation . Extensive experiments on standard benchmarks show that our method consistently outperforms state - of - the - art models . Further study reveals that , the advantage of our model over the competitors gets even more significant as the portion of noisy data rises .", "entities": [[12, 14, "TaskName", "entity typing"], [31, 33, "TaskName", "relation extraction"], [41, 43, "TaskName", "entity linking"], [52, 55, "TaskName", "knowledge base completion"], [316, 317, "MetricName", "loss"], [526, 528, "TaskName", "entity typing"]]}
{"text": "Overview . The basic assumptions of our idea are : ( 1 ) all mentions belong to the same type should be close to each other in the representation space because they should have similar context , ( 2 ) similar contexts lead to the same type . For clean data , we compact the representation space of the same type to comply ( 1 ) . For noisy data , given assumption ( 2 ) , we infer the their type distributions via label propagation and candidate types constrain . Figure 2 shows the overall framework of the proposed method . Clean data is used to train classifier and feature extractor end - to - endly , while noisy data is only used in CLSC regularization . Formally , given a batch of samples { ( m i , c i , Y t i ) } B i=1 , we first convert each sample ( m i , c i ) into a real - valued vector z i via a feature extractor z ( ( m i , c i ) ; \u03b8 z ) parameterized by \u03b8 z . Then a type classifier g ( z i ; \u03b8 g ) parameterized by \u03b8 g gives the posterior P ( y | z i ; \u03b8 g ) . By incorporating CLSC regularization in the objective function , we encourage the feature extractor z to group mentions of the same type into a compact cluster , which facilitates classification as is shown in Figure 1 . Noisy data enhances the formation of compact clusters with the help of label propagation .", "entities": [[185, 186, "HyperparameterName", "\u03b8"], [190, 191, "HyperparameterName", "\u03b8"], [202, 203, "HyperparameterName", "\u03b8"], [207, 208, "HyperparameterName", "\u03b8"], [219, 220, "HyperparameterName", "\u03b8"]]}
{"text": "Figure 3 illustrates our feature extractor . For fair comparison , we adopt the same feature extraction pipeline as used in ( Xu and Barbosa , 2018 ) . The feature extractor is composed of an embedding layer and two encoders which encode mentions and contexts respectively . Embedding Layer : The output of this layer is a concatenation of word embedding and word position embedding . We use the popular 300dimensional word embedding supplied by ( Pennington et al , 2014 ) to capture the semantic information and random initialized position embedding ( Zeng et al , 2014 ) to acquire information about the relation between words and the mentions . Formally , Given a word embedding matrix W word of shape d w \u00d7 | V | , where V is the vocabulary and d w is the size of word embedding , each column of W word represents a specific word w in V . We map each word w j in ( m i , c i ) to a word embedding w d j R dw . Analogously , we get the word position embedding w p j R dp of each word according to the relative distance between the word and the mention , we only use a fixed length context here . The final embedding of the j - th word is w E j = [ w d j , w p j ] . Mention Encoder : To capture lexical level information of mentions , an averaging mention encoder and a LSTM mention encoder ( Hochreiter and Schmidhuber , 1997 ) is applied to encode mentions . Given m i = ( w s , w s+1 , , w e ) , the aver - aging mention representation r a i R dw is : r a i = 1 e \u2212 s + 1 e j = s w d j ( 1 ) By applying a LSTM over an extended mention ( w s\u22121 , w s , w s+1 , , w e , w e+1 ) , we get a sequence ( h s\u22121 , h s , h s+1 , , h e , h e+1 ) . We use h e+1 as LSTM mention representation r l i R d l . The final mention representation is r m i = [ r a i , r l i ] R dw+d l . Context Encoder : A bidirectional LSTM with d l hidden units is employed to encode embedding se - quence ( w E s\u2212W , w E s\u2212W+1 , , w E e+W ) : \u2212 h j = LST M ( \u2212\u2212 h j\u22121 , w E j\u22121 ) \u2212 h j = LST M ( \u2212\u2212 h j\u22121 , w E j\u22121 ) h j = [ \u2212 h j \u2212 h j ] ( 2 ) where denotes element - wise plus . Then , the word - level attention mechanism computes a score \u03b2 i , j over different word j in the context c i to get the final context representation r c i : \u03b1 j = w T tanh ( h j ) \u03b2 i , j = exp ( \u03b1 j ) k exp ( \u03b1 k ) r c i = j \u03b2 i , j h i , j ( 3 ) We use r i = [ r m i , r c i ] R dz = R dw+d l + d l as the feature representation of ( m i , c i ) and use a Neural Networks q over r i to get the feature vector z i . q has n layers with h n hidden units and use ReLu activation .", "entities": [[260, 261, "MethodName", "LSTM"], [328, 329, "MethodName", "LSTM"], [378, 379, "MethodName", "LSTM"], [414, 416, "MethodName", "bidirectional LSTM"], [506, 507, "HyperparameterName", "\u03b2"], [529, 530, "HyperparameterName", "\u03b1"], [539, 540, "HyperparameterName", "\u03b2"], [546, 547, "HyperparameterName", "\u03b1"], [552, 553, "HyperparameterName", "\u03b1"], [560, 561, "HyperparameterName", "\u03b2"], [634, 635, "MethodName", "ReLu"]]}
{"text": "The overview of CLSC regularization is exhibited in Figure 4 , which includes three steps : dynamic graph construction ( Figure 4c ) , label propagation ( Figure 4d , e ) and Markov chains ( Figure 4 g ) . The idea of compact clustering for semisupervised learning is first proposed by ( Kamnitsas et al , 2018 ) . The basic idea is to encourage mentions of the same type to be clustered into a dense region in the embedding space . We introduce more details of CLSC for distantly supervised FET in following sections . Dynamic Graph Construction : We start by creating a fully connected graph G over the batch of samples Z = { z i } B i=1 , as shown in Figure 4c 1 . Each node of G is a feature representation z i , while the distance between nodes is defined by a scaled dot - product distance function ( Vaswani et al , 2017 ) : A ij = exp ( z T i z j \u221a d z ) , \u2200z i , z j Z A = exp ( Z T Z \u221a d z ) ( 4 ) Each entry A ij measures the similarity between z i and z j , A R B\u00d7B can be viewed as the weighted adjacency matrix of G. Label Propagation : The end goal of CLSC is to cluster mentions of the same type to a dense region . For mentions which have more than one labeled types , we apply label propagation ( LP ) on G to estimate their type distribution . Formally , we denote \u03a6 R B\u00d7K as the label propagation posterior of a training batch . The original label propagation proposed by ( Zhu and Ghahramani , 2002 ) uses a transition matrix H to model the probability of a node i propagating its type posterior \u03c6 i = P ( y i | x i ) R K to the other nodes . Each entry of the transition matrix H R B\u00d7B is defined as : H ij = A ij / b A ib ( 5 ) The original label propagation algorithm is defined as : 1 . Propagate the label by transition matrix H , \u03a6 ( t+1 ) = H\u03a6 ( t ) 2 . Clamp the labeled data to their true labels . Repeat from step 1 until \u03a6 converges In this work \u03a6 ( 0 ) is randomly initialized 2 . Unlike unlabeled data in semi - supervised learning , distantly labeled mentions in FET have a limited set of candidate types . Based on this observation , We assume that ( m i , c i ) can only transmit and receive probability of types in Y t i no matter it is noisy data or clean data . Formally , define a B \u00d7 K indicator matrix M R B\u00d7K , where M ij = 1 if type j in Y t i otherwise 0 , where B is the batch size and K is the number of types . Our clamping step relies on M as is shown in Figure 4d : \u03a6 ( t+1 ) ij \u03a6 ( t+1 ) ij M ij / k \u03a6 ( t+1 ) ik M ik ( 6 ) For convenience , we iterate through these two steps S lp times , S lp is a hyperparameter . Compact Clustering : The LP posterior \u03a6 = \u03a6 ( S lp +1 ) is used to judge the label agreement between samples . In the desired optimal state , transition probabilities between samples should be uniform inside the same class , while be zero between different classes . Based on this assumption , the desirable transition matrix T R B\u00d7B is defined as : T ij = K k=1 \u03a6 ik \u03a6 jk m k , m k = B b=1 \u03a6 bk ( 7 ) m k is a normalization term for class k. Transition matrix H derived from z ( ( m i , c i ) ; \u03b8 z ) should be in keeping with T . Thus we minimize the cross entropy between T and H : L 1\u2212step = \u2212 1 B 2 B i=1 B j=1 T ij log ( H ij ) ( 8 ) For instance , if T ij is close to 1 , H ij needs to be bigger , which results in the growth of A ij and finally optimize \u03b8 z ( Eq.4 ) . The loss L 1\u2212step has largely described the regularization we use in z ( ( m i , c i ) ; \u03b8 z ) for compression clustering . In order to keep the structure of existing clusters , ( Kamnitsas et al , 2018 ) proposed an extension of L 1\u2212step to the case of Markov chains with multiple transitions between samples , which should remain within a single class . The extension maximizes probability of paths that only traverse among samples belong to one class . Define E R B\u00d7B as : E = \u03a6 T \u03a6 ( 9 ) E ij measures the label similarities between z i and z j , which is used to mask the transition between different clusters . The extension is given by : H ( 1 ) = H H ( s ) = ( H E ) ( s\u22121 ) H = ( H E ) H ( s\u22121 ) , ( 10 ) where is Hadamard Product , and H ( s ) ij is the probability of a Markov process to transit from node i to node j after s \u2212 1 steps within the same class . The extended loss function models paths of different length s between samples on the graph : L clsc = \u2212 1 S m 1 B 2 Sm s=1 B i=1 B j=1 T ij log ( H ( s ) ij ) . ( 11 ) For S m = 1 , L clsc = L 1\u2212step . By minimizing the cross entropy between T and H ( s ) ( Eq.11 ) , L clsc compact paths of different length between samples within the same class . Here , S m is a hyper - parameter to control the maximum length of Markov chain . L clsc is added to the final objective function as regularization to encourage compact cluttering .", "entities": [[17, 19, "TaskName", "graph construction"], [99, 101, "TaskName", "Graph Construction"], [415, 416, "DatasetName", "0"], [507, 508, "DatasetName", "0"], [513, 515, "HyperparameterName", "batch size"], [657, 659, "HyperparameterName", "k ="], [690, 691, "HyperparameterName", "\u03b8"], [761, 762, "HyperparameterName", "\u03b8"], [768, 769, "MetricName", "loss"], [789, 790, "HyperparameterName", "\u03b8"], [969, 970, "MetricName", "loss"]]}
{"text": "Given the representation of a mention , the type posterior is given by a standard softmax classifier parameterized by \u03b8 g : P ( \u0177 i | z i ; \u03b8 g ) = sof tmax ( W c z i + b c ) , ( 12 ) where W c R K\u00d7dz is a parameter matrix , b R K is the bias vector , where K is the number of types . The predicted type is then given byt i = argmax y i P ( \u0177 i | z i ; \u03b8 g ) . Our loss function consists of two parts . L sup is supervision loss defined by KL divergence : L sup = \u2212 1 B c Bc i=1 K k=1 y ik log ( P ( y i | z i ; \u03b8 g ) ) k ( 13 ) Here B c is the number of clean data in a training batch , K is the number of target types . The regularization term is given by L clsc . Hence , the overall loss function is : L f inal = L sup + \u03bb clsc \u00d7 L clsc ( 14 ) \u03bb clsc is a hyper parameter to control the influence of CLSC .", "entities": [[15, 16, "MethodName", "softmax"], [19, 20, "HyperparameterName", "\u03b8"], [30, 31, "HyperparameterName", "\u03b8"], [34, 35, "DatasetName", "sof"], [95, 96, "HyperparameterName", "\u03b8"], [100, 101, "MetricName", "loss"], [111, 112, "MetricName", "loss"], [140, 141, "HyperparameterName", "\u03b8"], [183, 184, "MetricName", "loss"]]}
{"text": "We compare the proposed method with several state - of - the - art FET systems 3 : Attentive ( Shimaoka et al , 2016 ) uses an attention based feature extractor and does n't distinguish clean from noisy data ; AFET ( Ren et al , 2016a ) trains label embedding with partial label loss ; AAA ( Abhishek et al , 2017 ) learns joint representation of mentions and type labels ; PLE+HYENA / FIGER ( Ren et al , 2016b ) proposes heterogeneous partial - label embedding for label noise reduction to boost typing systems . We compare two PLE models with HYENA ( Yogatama et al , 2015 ) and FIGER ( Ling and Weld , 2012 ) as the base typing system respectively ; NFETC ( Xu and Barbosa , 2018 ) trains neural fine - grained typing system with hierarchy - aware loss . We compare the performance of the NFETC model with two different loss functions : partial - label loss and PLL+hierarchical loss . We denote the two variants as NFETC and NFETC hier respectively ; NFETC - CLSC is the proposed model in this work . We use the NFETC model as our base model , based on which we apply Compact Latent Space Clustering Regularization as described in Section 3.2 ; Similarly , we report results produced by using both KLdivergense - based loss ( NFETC - CLSC ) and KL+hierarchical loss ( NFETC - CLSC hier ) .", "entities": [[55, 56, "MetricName", "loss"], [76, 77, "DatasetName", "FIGER"], [114, 115, "DatasetName", "FIGER"], [148, 149, "MetricName", "loss"], [161, 162, "MetricName", "loss"], [167, 168, "MetricName", "loss"], [170, 171, "MetricName", "loss"], [233, 234, "MetricName", "loss"], [241, 242, "MetricName", "loss"]]}
{"text": "For evaluation metrics , we adopt strict accuracy , loose macro , and loose micro F - scores widely used in the FET task ( Ling and Weld , 2012 ) . To fine tuning the hyper - parameters , we randomly sampled 10 % of the test set as a development set for both datasets . With the fine - tuned hyperparameter as mentioned in 4.4 , we run the model five times and report the average strict accuracy , macro F1 and micro F1 on the test set .", "entities": [[7, 8, "MetricName", "accuracy"], [79, 80, "MetricName", "accuracy"], [81, 83, "MetricName", "macro F1"], [84, 86, "MetricName", "micro F1"]]}
{"text": "We search the hyper parameter of Ontonotes and BBN respectively via Hyperopt proposed by ( Bergstra et al , 2013 ) . Hyper parameters are shown in Appendix A. We optimize the model via Adam Optimizer . The full hyper parameters includes the learning rate lr , the dimension d p of word position embedding , the dimension d l of the mention encoder 's output ( equal to the dimension of the context encoder 's ourput ) , the input dropout keep probability p i and output dropout keep probability p o for LSTM layers ( in context encoder and LSTM mention encoder ) , the L2 regularization parameter \u03bb , the factor of hierarchical loss normalization \u03b1 ( \u03b1 > 0 means use the normalization ) , BN ( whether using Batch normalization ) , the max step S lp of the label propagation , the max length S m of Markov chain , the influence parameter \u03bb clsc of CLSC , the batch size B , the number n of hidden layers in q and the number h n of hidden units of the hidden layers . We implement all models using Tensorflow 4 .", "entities": [[6, 7, "DatasetName", "Ontonotes"], [34, 35, "MethodName", "Adam"], [35, 36, "HyperparameterName", "Optimizer"], [43, 45, "HyperparameterName", "learning rate"], [94, 95, "MethodName", "LSTM"], [101, 102, "MethodName", "LSTM"], [107, 109, "HyperparameterName", "L2 regularization"], [116, 117, "MetricName", "loss"], [118, 119, "HyperparameterName", "\u03b1"], [120, 121, "HyperparameterName", "\u03b1"], [122, 123, "DatasetName", "0"], [133, 135, "MethodName", "Batch normalization"], [165, 167, "HyperparameterName", "batch size"]]}
{"text": "Table 1 shows performance comparison between the proposed CLSC model and state - of - the - art FET systems . On both benchmarks , the CLSC model achieves the best performance in all three metrics . When focusing on the comparison between NFETC and CLSC , we have following observation : Compact Latent Space Clustering shows its effectiveness on both clean data and noisy data . By applying CLSC regularization on the basic NFETC model , we observe consistent and significant performance boost ; Hierarchical - aware loss shows significant advantage on the OntoNotes dataset , while showing insignificant performance boost on the BBN dataset . This is due to different distribution of labels on the test set . The proportion of terminal types of the test set is 69 % for the BBN dataset , while is only 33 % on the OntoNotes dataset . Thus , applying hierarchical - aware loss on the BBN dataset brings little improvement ; Both algorithms are able to utilize noisy data to improve performance , so we would like to further study their performance in different noisy scenarios in following discussions . By principle , with sufficient amount of clean training data , most typing systems can achieve satisfying performance . To further study the robustness of the methods to label noise , we compare their performance with the presence of 25 % , 20 % , 15 % , 10 % and 5 % clean training data and all noisy training data . Figure 5 shows the performance curves as the proportion of clean data drops . As it reveals , the CLSC model consistently wins in the comparison . The advantage is especially clear on the BBN dataset , which offers less amount of training data . Note that , with only 27.9 % of training data ( when only leaving 5 % clean data ) on the BBN dataset , the CLSC model yield a comparable result with the NFETC model trained on full data . This comparison clearly shows the superiority of our approach in the effectiveness of utilizing noisy data .", "entities": [[88, 89, "MetricName", "loss"], [94, 95, "DatasetName", "OntoNotes"], [144, 145, "DatasetName", "OntoNotes"], [153, 154, "MetricName", "loss"]]}
{"text": "Named entity Recognition ( NER ) has been excavated for a long time ( Collins and Singer , 1999 ; , which classifies coarsegrained types ( e.g. person , location ) . Recently , ( Nagesh and Surdeanu , 2018a , b ) applied ladder network ( Rasmus et al , 2015 ) to coarse - grained entity classification in a semi - supervised learning fashion . ( Ling and Weld , 2012 ) proposed Fine - Grained Entity Recognition ( FET ) . They used distant supervision to get training corpus for FET . Embedding techniques was applied to learn feature representations since ( Yogatama et al , 2015 ; Dong et al , 2015 ) . ( Shimaoka et al , 2016 ) introduced attention mechanism for FET to capture informative words . ( Xin et al , 2018a ) used the TransE entity embeddings ( Bordes et al , 2013 ) as the query vector of attention . Early works ignore the out - of - context noise , ( Gillick et al , 2014 ) proposed context dependent FET and use three heuristics to clean the noisy labels with the side effect of losing training data . To utilize noisy data , ( Ren et al , 2016a ) distinguished the loss function of noisy data from clean data via partial label loss ( PLL ) . ( Abhishek et al , 2017 ; Xu and Barbosa , 2018 ) proposed variants of PLL , which still suffer from confirmation bias . ( Xu and Barbosa , 2018 ) proposed hierarchical loss to handle over - specific noise . On top of AFET , ( Ren et al , 2016b ) proposed a method PLE to reduce the label noise , which lead to a great success in FET . Because label noise reduction is separated from the learning of FET , there might be error propagation problem . Recently , ( Xin et al , 2018b ) proposed utilizing a pretrained language model measures the compatibility between context and type names , and use it to repel the interference of noisy labels . However , the compatibility got by language model may not be right and type information is defined by corpus and annotation guidelines rather than type names as is mentioned in ( Azad et al , 2018 ) . In addition , there are some work about entity - level typing which aim to figure out the types of entities in KB ( Yaghoobzadeh and Sch\u00fctze , 2015 ; Jin et al , 2018 ) .", "entities": [[0, 3, "TaskName", "Named entity Recognition"], [4, 5, "TaskName", "NER"], [144, 145, "MethodName", "TransE"], [145, 147, "TaskName", "entity embeddings"], [215, 216, "MetricName", "loss"], [226, 227, "MetricName", "loss"], [265, 266, "MetricName", "loss"]]}
{"text": "In recent years , significant progress has been made in the field of open - domain question answering ( Chen et al , 2017 ; Wang et al , 2017Wang et al , , 2018Min et al , 2018 ; Asai et al , 2019 ) . Very recently , some works turn to deal with a more challenging task of asking complex questions ( Welbl et al , 2018 ; Yang et al , 2018 ) from the open - domain text corpus . In the open - domain scenario , one critical challenge raised by complex questions is that each question may require multiple pieces of evidence to get the right answer , while the evidence usually scatters in different passages . Examples in Figure 1 shows two types of questions that require evidence from multiple passages . To deal with the challenging multi - evidence questions , an open - domain QA system should be able to ( 1 ) efficiently retrieve a small number of passages that cover the full evidence ; and ( 2 ) accurately extract the answer by jointly considering the candidate evidence passages . While there have been several prior works in the latter direction ( Wang et al , 2017 ; Figure 1 : Examples of complex questions involving two facts of a person . Different facts are color - coded . P # are all relevant passages , while only the ones with solid - line boxes are the true supporting passages . Lin et al , 2018 ) , the solutions to the first problem still rely on traditional or neural information retrieval ( IR ) approaches , which solely measure the relevance between the question and each individual paragraph , and will highly possibly put the wrong evidence to the top . 1 For example in Figure 1 ( top ) , P1 and P2 are two candidate evidence passages that are closely related to the question but only cover the same unilateral fact required by the question , therefore leading us to the wrong answer Newton . This paper formulates a new problem of complementary evidence identification for answering complex questions . The key idea is to consider the problem as measuring the properties of the selected passages , more than the individual relevance . Specifically , we hope the selected passages can serve as a set of spanning bases that supports the question . The selected passage set thus should satisfy the properties of ( 1 ) relevancy , i.e. , they should be closely related to the question ; ( 2 ) diversity , i.e. , they should cover diverse information given the coverage property is satisfied ; ( 3 ) compactness , i.e. , the number of passages to satisfy the above properties should be minimal . With these three defined properties , we hope to both improve the selective accuracy and encourage the interpretability of the evidence identification . Note that complementary evidence identification in QA is different from Search Result Diversification ( SRD ) in IR on their requirement of compactness . The size of the selected set is constrained in QA tasks by the capability of downstream reasoning models and practically needs to be a small value , whereas it is not the case in SRD . To achieve the above goals , a straightforward approach is to train a model that evaluates each subset of the candidate passages , e.g. , by concatenating passages in any subsets . However , this approach is highly inefficient since it requires to encode O ( K L ) passage subsets , where K is the total number of candidates and L is the maximum size of subsets . Thus , a practical complementary evidence identification method needs to be computationally efficient . This is especially critical when we use heavy models like ELMo ( Peters et al , 2018 ) and BERT ( Devlin et al , 2018 ) , where passage encoding is time and memory consuming . To this end , we propose an efficient method to select a set of spanning passages that is sufficient and diverse . The core idea is to represent questions and passages in a vector space and define the measures of our criterion in the vector space . For example , in the vector space , sufficiency can be defined as a similarity between the question vector and the sum of selected passage vectors , measured by a cosine function with a higher score indicating a closer similarity ; and diversity can be defined as 1 distance between each pair of passages . By properly training the passage encoder with a loss function derived by the above terms , we expect the resulted vector space satisfies the property that the complementary evidence passages lead to large scores . In addition , our method only encodes each passage in the candidate set once , which is more efficient than the naive solution mentioned above . To evaluate the proposed method , we use the multi - hop QA dataset HotpotQA ( the full wiki setting ) since the ground - truth of evidence passages are provided . Experiments show that our method significantly improves the accuracy of complementary evidence selection .", "entities": [[13, 18, "TaskName", "open - domain question answering"], [272, 274, "TaskName", "information retrieval"], [486, 487, "MetricName", "accuracy"], [510, 511, "DatasetName", "SRD"], [554, 555, "DatasetName", "SRD"], [649, 650, "MethodName", "ELMo"], [658, 659, "MethodName", "BERT"], [786, 787, "MetricName", "loss"], [853, 854, "DatasetName", "HotpotQA"], [879, 880, "MetricName", "accuracy"]]}
{"text": "We propose a new supervised training objective to learn the BERT encoder for QA that optimizes the previous conditions . Note that in this work we assume a set of labeled training examples are available , i.e. , the ground truth annotations contain complementary supporting paragraphs . Recently there was a growing in such datasets ( Yang et al , 2018 ; Yao et al , 2019 ) , due to the increasing interest in model explainability . Also , such supervision signals can also be obtained with distant supervision . For each training instance ( q , P ) , we define { p i } + = { p i } , \u2200i { i | p i P + } ( 1 ) { p i } \u2212 = { p i } , \u2200i { i | p i P \u2212 } ( 2 ) { p i } = { p i } + \u222a { p i } \u2212 ( 3 ) Denoting y p i = 1 if p i P + and y p i = 0 if p i P \u2212 , we have the following training objective function : L ( { pi } ; q ; y ) = Lsup ( { pi } ; q ; y ) + \u03b1L d ( { pi } + ) + \u03b2Lc ( { pi } ; q ; y ) ( 4 ) where Lsup ( { pi } ; q ; y ) = \u2212 i yp i log ( f ( pi ) ) , ( 5 ) L d ( { pi } + ) = p i , p j , i = j ( 1 \u2212 1 ( pi , p j ) ) . ( 6 ) Lc ( { pi } ; q ; y ) = \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 1 \u2212 cos ( q , i pi ) , if \u03a0p i yp i = 1 max ( 0 , cos ( q , i pi ) \u2212 \u03b3 ) , if \u03a0p i yp i = 0 ( 7 ) where \u03b1 and \u03b2 are the hyperparameter weights and 1 ( , ) denotes L1 loss between two input vectors . Eq 5 is the cross - entropy loss corresponding to relevance condition ; Eq 6 regularizes the diversity condition ; Eq 7 is the cosine - embedding loss 2 for the compactness condition and \u03b3 > 0 is the margin to encourage data samples with better question coverage . 2 Refer to CosineEmbeddingLoss in PyTorch .", "entities": [[10, 11, "MethodName", "BERT"], [184, 185, "DatasetName", "0"], [341, 342, "DatasetName", "0"], [351, 352, "HyperparameterName", "\u03b3"], [360, 361, "DatasetName", "0"], [365, 366, "HyperparameterName", "\u03b1"], [367, 368, "HyperparameterName", "\u03b2"], [379, 380, "MetricName", "loss"], [392, 393, "MetricName", "loss"], [412, 413, "MetricName", "loss"], [419, 420, "HyperparameterName", "\u03b3"], [421, 422, "DatasetName", "0"]]}
{"text": "Score Function During inference , we use the following score function to find the best paragraph combination : g ( P sel ; q ; { pi } ) = p i P ( pi | q ) + \u03b1 cos ( p i pi , q ) + \u03b2 p i , p j , i = j 1 ( pi , pj ) ( 8 ) where \u03b1 and \u03b2 are hyperparameters similar to Eq 4 . Note that our approach requires to encode each passage in P only once for each question , resulting in an O ( K ) time complexity of encoding ( K = | P | ) ; and the subset selection is performed in the vector space , which is much more efficient than selecting subsets before encoding . Beam Search In a real - world application , there is usually a large candidate set of P , e.g. , retrieved passages for q via a traditional IR system . Our algorithm requires O ( K ) time encoding , and O ( K L ) time scoring in vector space when ranking all the combinations in L candidates . Thus when K becomes large , it is still inefficient even when L = 2 . We resort to beam search to deal with scenarios with large Ks . The details can be found in Appendix A. 3 Experiments", "entities": [[0, 1, "MetricName", "Score"], [39, 40, "HyperparameterName", "\u03b1"], [49, 50, "HyperparameterName", "\u03b2"], [69, 70, "HyperparameterName", "\u03b1"], [71, 72, "HyperparameterName", "\u03b2"], [108, 110, "HyperparameterName", "K ="]]}
{"text": "Baseline We compare with the BERT passage ranker ( Nie et al , 2019 ) that is commonly used on open - domain QA including HotpotQA . The baseline uses the same BERT architecture as our approach described in Section 2.2 , but is trained with only the relevancy loss ( Eq 5 ) and therefore only consider the relevancy when selecting evidence . We also compare the DRN model from ( Harel et al , 2019 ) which is designed for the SRD task . Their ensemble system first finds the most relevant evidence to the given question , and then select the second diverse evidence using their score function . The major differences from our method are that ( 1 ) they train two separate models for evidence selection ; ( 2 ) they do not consider the compactness among the evidences . It is worth mentioning that we replace their LSTM encoder with BERT encoder for fair comparison . Metric During the evaluation we make each method output its top 2 ranked results 3 ( i.e. the top 1 ranked pair from our method ) as the prediction . The final performance is evaluated by exact match ( EM ) , i.e. , whether both true evidence passages are covered , and the F1 score on the test sets .", "entities": [[5, 6, "MethodName", "BERT"], [25, 26, "DatasetName", "HotpotQA"], [32, 33, "MethodName", "BERT"], [49, 50, "MetricName", "loss"], [83, 84, "DatasetName", "SRD"], [153, 154, "MethodName", "LSTM"], [156, 157, "MethodName", "BERT"], [198, 200, "MetricName", "exact match"], [201, 202, "MetricName", "EM"], [216, 218, "MetricName", "F1 score"]]}
{"text": "In the experiments , we have M = 3 , N = 4 for MNLI - 12 and M = 4 , N = 5 for HotpotQA - 50 with our method . The values are selected according to development performance . We follow the settings and hyperparameters used in ( Harel et al , 2019 ) for the DRN model . many pieces of true evidences enclosed by the complete set of candidate passages where our proposed ranker selects from . For HotpotQA dataset , we use a bi - gram BM25 ranker to collect top 50 relevant passages and build the basis for the experiments 4 , which inevitably leads some of the true evidences to be filtered out and makes its upper - bound less than 100 % . For the artificial MNLI - 12 dataset , all the true evidences are guaranteed to be included . Table 1 shows that our method achieves significant improvements on both datasets . On HotpotQA - 50 , all systems have low EM scores , because of the relatively low recall of the BM25 retrieval . Only 35.49 % of the samples in the test set contain both ground - truth evidence passages . On MNLI - 12 , the EM score is around 50 % . This is mainly because the segments are usually much shorter than a paragraph , with an average length of 7 words . Therefore it is more challenging in matching the q with the p i s. Specifically , both our method and the BERT baseline surpass the DRN model on all datasets and metrics , which results from our question - conditioned passage encoding approach . Our defined vector space proves beneficial to model the complementation among the evidence with respect to a given question . The ablation study of our loss function further illustrates that the diversity and the compactness terms efficiently bring additional 20%/30 % increase in EM score on two datasets and consequently raise the F1 score by about 8/6 absolute points . Figure 2 gives examples about how our model improves over the baseline . Our method can successfully select complementary passages while the baselines only select passages that look similar to the question . A more interesting example is given at the bottom where the top - 50 only covers one supporting passage . The BERT baseline selects two incorrect passages that cover identical part of facts required by the question and similarly the DRN baseline select a relevant evidence and an irrelevant evidence , while our method scores lower the second passage that does not bring new information , and reaches a supporting selection . A similar situation contributes to the majority of improvement on one - supporting - evidence data sample in HotpotQA - 50 . Inference Speed Our beam search with score function brings slight overheads to the running time . On HotpotQA - 50 , it takes 1 , 990 milliseconds ( ms ) on average to obtain the embeddings of all passages for one data sample whereas our vector - based complementary selection only adds an extra 2 ms which can be negligible compared to the encoding time .", "entities": [[14, 15, "DatasetName", "MNLI"], [26, 27, "DatasetName", "HotpotQA"], [83, 84, "DatasetName", "HotpotQA"], [135, 136, "DatasetName", "MNLI"], [164, 165, "DatasetName", "HotpotQA"], [172, 173, "MetricName", "EM"], [205, 206, "DatasetName", "MNLI"], [210, 211, "MetricName", "EM"], [260, 261, "MethodName", "BERT"], [308, 309, "MetricName", "loss"], [326, 327, "MetricName", "EM"], [335, 337, "MetricName", "F1 score"], [397, 398, "MethodName", "BERT"], [466, 467, "DatasetName", "HotpotQA"], [487, 488, "DatasetName", "HotpotQA"]]}
{"text": "The latest dense retrieval methods ( Lee et al , 2019 ; Karpukhin et al , 2020 ; Guu et al , 2020 ) show promising results on efficient inference on the full set of Wikipedia articles , which allows to skip the initial standard BM25 retrieval and avoid the significant loss during the pre - processing step . Our proposed approach is able to directly cooperate with these methods as we all work in the vector space . Therefore , the extension to dense retrieval can be naturally the next step of our work .", "entities": [[51, 52, "MetricName", "loss"]]}
{"text": "In the paper , we propose a new problem of complementary evidence identification and define the criterion of complementary evidence in vector space . We further design an algorithm and a loss function to support efficient training and inference for complementary evidence selection . Compared to the baseline , our approach improves more than 20 % and remains to scale well to the computationally complex cases . In both examples , the DRN baseline first finds the most relevant evidence to the question ( left ) and then select a diverse one ( right ) ; the BERT baseline model selected the top - 2 most relevant passages ( P1 , P2 ) to the question regardless of their complementation ; whereas our model made the selection ( P1 , P3 ) with consideration of both relevance and evidence sufficiency . Note that , in the bottom example , one of the ground - truth supporting passages and the answer were excluded when building the dataset .", "entities": [[31, 32, "MetricName", "loss"], [97, 98, "MethodName", "BERT"], [130, 131, "DatasetName", "P3"]]}
{"text": "Advances in machine learning methods and the release of annotated datasets of clinical texts ( Uzuner et al , 2011 ; Styler IV et al , 2014 ) in the past decade has led to an increase of available clinical NLP systems for interesting tasks . Recent advances in pre - trained models ( Devlin et al , 2019 ; Liu et al , 2019 ) have made ever more accurate clinical NLP systems possible . Unsupervised domain adaptation algorithms ( e.g. , Ziser and Reichart ( 2019 ) ) have made it possible to reduce performance degradation when applying trained models to new domains . The great promise of these developments is that these methods can be combined into pipelines that allow for sophisticated information extraction capabilities for downstream clinical use cases . Rather than building one - off datasets for each complex downstream task that arises , standard NLP components could potentially be used as \" Lego \" - style building blocks that allow for flexibly approaching new tasks as they arise . However , the existence of the building blocks alone does not solve this problem . Combining individual components into NLP pipelines can lead to cascading errors ( Finkel et al , 2006 ) . The true error rate for structured extraction tasks is potentially as high as the sum of the component tasks ' errors . For example , if the goal is to extract normalized concepts with assertion status , the concept error can come from normalization error , negation detection error , uncertainty detection error , etc , and the errors may not be correlated . These problems are exacerbated in the common case where individual components are trained on data from different domains , and tested on data from yet another domain . In this work , we quantitatively examine the issues described above in the context of extracting drug temporality signatures , with the goal of understanding drug start and stop events . We approach this task with the combination of three sub - tasks : 1 ) the temporal relation of these mentions to the document creation time ( DocTimeRel ) , 2 ) negation status of the mention , and 3 ) aspectual link relations of the mention ( e.g. , is it being described as starting or stopping ) . Figure 1 shows an example sentence with a drug mention , that demonstrates how the three tasks work together to establish the status of that drug in that patient . Successfully solving this task is beneficial for understanding patient treatment course , and enabling more causal understanding in important tasks such as adverse drug event detection or relating medication courses to outcomes . We first set state - of - the - art benchmarks for three tasks on the THYME corpus by fine - tuning large pre - trained transformer models ( Devlin et al , 2019 ; Liu et al , 2019 ) . We then examine how the performance of individual systems degrades when moving from the training data to our target data ( a pediatric cardiology cohort ) , and how the overall system performs when combining multiple systems Additionally , as the patient has preserved ejection fraction , no prior history of embolic phenomena , and no significant valvular disease , it would be acceptable for him to remain off e Coumadin /e for the interim . Figure 1 : An example sentence with highlighted drug name Coumadin to be classified for all three tasks . The gold standard has this drug mention classified as negated , with DocTimeRel = OVERLAP , and ALINK = CONTINUES . These three facts can be used to understand that the patient is not on the drug now or going forward , and was likely not on the drug prior to the note as well . with imperfect performance . Despite strong individual results , we find that performance suffers immensely due to both out - of - domain performance losses and the basic combinatorial math of integrating outputs from multiple systems . This is the case even though we use a metric , accuracy , that is forgiving to the worst - performing individual model .", "entities": [[76, 79, "TaskName", "Unsupervised domain adaptation"], [255, 257, "TaskName", "negation detection"], [446, 448, "TaskName", "event detection"], [696, 697, "MetricName", "accuracy"]]}
{"text": "It is both formally and empirically understood that classifiers can suffer performance loss when the test data is drawn from a different distribution than the training data ( sometimes called domain shift ) . This presents a difficult challenge in clinical NLP because data - sharing limitations make it difficult to create large and diverse training corpora . As a result , domain adaptation approaches have been applied to multiple tasks in clinical NLP ( Miller et al , 2017 ; Liu et al , 2018 ; Hur et al , 2020 ) . Recent work in the general domain has made use of transfer learning , which can attack the problem of domain shift , but by a different mechanism than domain adaptation ; by training on massive corpora , large pre - trained models both learn general features , and are able to learn from smaller new datasets without overfitting . The most prominent of these models are based on the transformer architecture ( Vaswani et al , 2017 ) . BERT ( Devlin et al , 2019 ) uses a transformer encoder , and has shown that pre - training with massive amounts of text on a language modeling task , then fine - tuning on a supervised task of interest , achieves large performance gains in multiple NLP tasks . 1 During fine - tuning for sentence classification tasks , a classification head with randomly initialized weights is attached to a special sentenceinitial token . Fine - tuning then proceeds in a standard supervised learning paradigm , with the goal of learning the weights of the classification head , but where the weights of all of the transformer encoder layers can also be updated . We use RoBERTabase , a 12 - layer transformer encoder that provides excellent performance but manageable memory utilization for our hardware ( Liu et al , 2019 ) . The bigger vision of our current work is extracting temporally - aware medication mentions from electronic health records data . This would enable important downstream tasks including automatically extracting drug timelines to correlate with treatments , or extracting better causal information about drugs and potential adverse events . Some other recent work has also examined this topic ( Ramirez et al , 2019 ) , but focused on a single drug class ( proton pump inhibitors ) , was limited to the problem list section , and made the assumption that missing drug implied drug stoppage .", "entities": [[12, 13, "MetricName", "loss"], [62, 64, "TaskName", "domain adaptation"], [104, 106, "TaskName", "transfer learning"], [122, 124, "TaskName", "domain adaptation"], [173, 174, "MethodName", "BERT"], [230, 232, "TaskName", "sentence classification"]]}
{"text": "This is the task of finding whether a given event is being negated ( e.g. , statins is negated in not currently on statins ) . We model this as a spanin - context classification - given a sentence in a document with a marked event span , classify that span as being negated or not negated . We experiment with two different machine learning models . The first is a classical feature - based support vector machine that is the default model of Apache cTAKES ( Savova et al , 2010 ) . Features include bag of words and part of speech tags in and around the event , negation cue words from lists and their relation to the event , and dependency parse features that relate negation cue words to events . Details of this system were presented by Wu et al ( 2014 ) . For comparison we train a RoBERTa - based system , where the input representation is the sentence with special tokens indicating the event to be classified . We put a binary sigmoid layer as the output , with the \" [ CLS ] \" token representation from the final layer as the classifier input , and fine - tune the entire model . Hyperparameters such as learning rate and number of training epochs are optimized on the THYME colon development set . Our implementation uses the Huggingface Transformers library ( Wolf et al , 2019 ) .", "entities": [[75, 78, "MethodName", "support vector machine"], [153, 154, "MethodName", "RoBERTa"], [214, 216, "HyperparameterName", "learning rate"]]}
{"text": "The tasks described above are trained on a single source dataset , and must be combined into a pipeline that will run on data from a different target distribution . To adapt to the target domain , we use unsupervised domain adaptation methods , where we have access to only unlabeled target examples . Since large pre - trained transformer models have arrived , they have been shown to be quite robust to out - of - distribution examples ( Hendrycks et al , 2020 ) , including on clinical tasks ( Lin et al , 2020 ) , where it was shown that adding domain adaptation layers on top of BERT was no better than BERT itself for negation detection . One of the few effective methods for improving the out - of - distribution performance of pre - trained transformer models has been to continue to pre - train the language modeling objective on the target domain data , before any fine - tuning is done on the source data ( Han and Eisenstein , 2019 ; Gururangan et al , 2020 ) . In this work , we focus on that method , since this is currently the most promising direction for adapting large pre - trained transformers . Specifically , to use this method , we run additional masked language model training steps on the target training data from the RoBERTa - base checkpoint , before fine - tuning on the labeled colon cancer data , and then testing on target test data . We tune the learning rate for the language model pre - training on target development set data , optimizing for perplexity .", "entities": [[39, 42, "TaskName", "unsupervised domain adaptation"], [105, 107, "TaskName", "domain adaptation"], [111, 112, "MethodName", "BERT"], [116, 117, "MethodName", "BERT"], [119, 121, "TaskName", "negation detection"], [234, 235, "MethodName", "RoBERTa"], [261, 263, "HyperparameterName", "learning rate"], [278, 279, "MetricName", "perplexity"]]}
{"text": "For the three tasks of interest , we evaluate indomain ( THYME colon cancer corpus ) , as well as one closely related out - of - domain corpus ( THYME brain cancer corpus ) . We also use a second out - ofdomain corpus , an internal data set we annotated for all three tasks ( pulmonary hypertension [ PH ] notes ) . This annotation was performed by an experienced annotator who has worked on clinical annotation projects in the past . We measure performance on negation with F1score , on DocTimeRel with accuracy ( because the classes are relatively balanced ) , and on ALINK extraction with the average F1 score of all categories , macro - F1 ( because the high frequency NONE label makes accuracy uninformative ) . In addition to system - level performance , we report an evaluation of mention - level accuracy : an event is counted as correct if all three systems made the correct prediction , and we report the percentage of events that were correct . This setting estimates how usable the entire pipeline is , given different system settings . The \" Colon \" columns of Table 1 show results on the THYME colon cancer data ( in - domain ) . RoBERTa performance is stronger than the SVM on all three tasks . Negation performance is particularly strong , though we are not aware of any reported results on this dataset to compare against . DocTimeRel performance is 3 points better than the best result of Clinical TempEval 2016 . ALINK scores are lower than the other tasks , though again there are no published comparisons . It is likely this is a more difficult task , in particular because the RE - INITIATES category has relatively few examples and whose low performance skews the averaging of the macro - F1 . The \" Brain \" and \" PH \" columns of Table 1 show out - of - domain performance of the same systems on the THYME brain cancer and our internal pulmonary hypertension data , respectively . On THYME brain cancer data , RoBERTa again out - performs SVM substantially on all sub - tasks , but surprisingly the SVM performs better on PH data for negation and DocTimeRel . Adapting the RoBERTa model ( RoBERTa+LM ) by performing additional language modeling in the target domain before fine - tuning on colon cancer data leads to gains only on DocTimeRel for the PH data and on ALINK for both corpora . However , the improvement to DocTimeRel from adapting RoBERTa still leaves it worse off than the SVM . Mention level accuracy ( \" All \" column ) is good for the in - domain data ( THYME colon cancer ) , but drops off substantially even for the THYME brain cancer corpus from the same institution , created with the same guidelines and using the same annotators . The mention level accuracy for our internal PH data is unusable at an accuracy of 0.506 with RoBERTa+LM . This accuracy means that roughly one of every two drug mentions will have at least one of its attributes classified incorrectly .", "entities": [[95, 96, "MetricName", "accuracy"], [111, 113, "MetricName", "average F1"], [118, 121, "MetricName", "macro - F1"], [129, 130, "MetricName", "accuracy"], [149, 150, "MetricName", "accuracy"], [214, 215, "MethodName", "RoBERTa"], [220, 221, "MethodName", "SVM"], [311, 314, "MetricName", "macro - F1"], [358, 359, "MethodName", "RoBERTa"], [363, 364, "MethodName", "SVM"], [374, 375, "MethodName", "SVM"], [387, 388, "MethodName", "RoBERTa"], [434, 435, "MethodName", "RoBERTa"], [442, 443, "MethodName", "SVM"], [446, 447, "MetricName", "accuracy"], [497, 498, "MetricName", "accuracy"], [507, 508, "MetricName", "accuracy"], [514, 515, "MetricName", "accuracy"]]}
{"text": "NER We select a subset of the following two NER tasks , CoNLL - 2002NER ( Sang , 2002 and CoNLL - 2003 NER ( Sang andDe Meulder , 2003 ) , to form this cross - lingual NER dataset . It covers 4 languages , including English , German , Spanish and Dutch , and 4 types of named entities , including Person , Location , Organization and Miscellaneous entities that do not belong to the previous three types . F1 score is used as the metric . POS Tagging ( POS ) Following ( Kim et al , 2017 ) , we select a subset of Universal Dependencies ( UD ) Treebanks ( v2.5 ) ( Zeman et al , 2019 ) , which covers 18 languages . Accuracy ( ACC ) of the predicted POS tags is used as the metric . News Classification ( NC ) This task aims to predict the category given a news article . It covers 5 languages , including English , Spanish , French , German and Russian . Each labeled instance is a 3 - tuple : < news title , news body , category > . The category number is 10 . We crawl this dataset from Microsoft News ( MSN ) . Accuracy ( ACC ) of the multi - class classification is used as the metric .", "entities": [[0, 1, "TaskName", "NER"], [9, 10, "TaskName", "NER"], [23, 24, "TaskName", "NER"], [35, 39, "TaskName", "cross - lingual NER"], [69, 70, "TaskName", "Miscellaneous"], [81, 83, "MetricName", "F1 score"], [108, 110, "DatasetName", "Universal Dependencies"], [111, 112, "DatasetName", "UD"], [130, 131, "MetricName", "Accuracy"], [132, 133, "MetricName", "ACC"], [145, 147, "TaskName", "News Classification"], [214, 215, "MetricName", "Accuracy"], [216, 217, "MetricName", "ACC"], [220, 224, "TaskName", "multi - class classification"]]}
{"text": "MLQA The MLQA ( Lewis et al , 2019b ) is a multilingual machine reading comprehension task , which contains QA annotations labeled in 7 languages , including English , Arabic , German , Spanish , Hindi , Vietnamese and Chinese . F1 score of the predicted answers is used as the metric . XNLI We reuse the original XNLI dataset ( Conneau et al , 2018 ) in XGLUE . PAWS - X The PAWS - X ( Yang et al , 2019a ) is a paraphrase identification dataset , which extends the Wikipedia portion of the PAWS ( Zhang et al , 2019 ) evaluation to more languages . We select 4 languages , including English , Spanish , French and German , from the original dataset and use them in XGLUE . Accuracy ( ACC ) of the binary classification is used as the metric . Query - Ad Matching ( QADSM ) This task aims to predict whether an advertisement ( ad ) is relevant to an input query . It covers 3 languages , including English , French and German . Each labeled instance is a 4 - tuple : < query , ad title , ad description , label > . The label indicates whether the ad is relevant to the query ( Good ) , or not ( Bad ) . We con - struct this dataset based on Bing . Accuracy ( ACC ) of the binary classification is used as the metric . Web Page Ranking ( WPR ) This task aims to predict whether a web page is relevant to an input query . It covers 7 languages , including English , German , French , Spanish , Italian , Portuguese and Chinese . Each labeled instance is a 4 - tuple : < query , web page title , web page snippet , label > . The relevance label contains 5 ratings : Perfect ( 4 ) , Excellent ( 3 ) , Good ( 2 ) , Fair ( 1 ) and Bad ( 0 ) . We construct this dataset based on Bing . Normalize Discounted Cumulative Gain ( nDCG ) is used as the metric . QA Matching ( QAM ) This task aims to predict whether a < question , passage > pair is a QA pair . It covers 3 languages , including English , French and German . Each labeled instance is a 3 - tuple : < question , passage , label > . The label indicates whether the passage is the answer of the question ( 1 ) , or not ( 0 ) . We construct this dataset based on Bing . Accuracy ( ACC ) of the binary classification is used as the metric .", "entities": [[0, 1, "DatasetName", "MLQA"], [2, 3, "DatasetName", "MLQA"], [13, 16, "TaskName", "machine reading comprehension"], [42, 44, "MetricName", "F1 score"], [54, 55, "DatasetName", "XNLI"], [59, 60, "DatasetName", "XNLI"], [69, 70, "DatasetName", "XGLUE"], [71, 74, "DatasetName", "PAWS - X"], [75, 78, "DatasetName", "PAWS - X"], [87, 89, "TaskName", "paraphrase identification"], [98, 99, "DatasetName", "PAWS"], [133, 134, "DatasetName", "XGLUE"], [135, 136, "MetricName", "Accuracy"], [137, 138, "MetricName", "ACC"], [238, 239, "MetricName", "Accuracy"], [240, 241, "MetricName", "ACC"], [346, 347, "DatasetName", "0"], [441, 442, "DatasetName", "0"], [452, 453, "MetricName", "Accuracy"], [454, 455, "MetricName", "ACC"]]}
{"text": "Question Generation ( QG ) This task aims to generate a question for a given passage . We collect < passage , question > pairs from Bing . It covers 6 languages , including English , French , German , Spanish , Italian and Portuguese . BLEU - 4 score is used as the metric . News Title Generation ( NTG ) This task aims to generate a proper title for a given news body . We collect < news body , news title > pairs from Microsoft News ( MSN ) . It covers 5 languages , including German , English , French , Spanish and Russian . BLEU - 4 score is used as the metric . 3 Pre - train Unicoder for Cross - lingual Understanding Tasks We select Unicoder as the backbone model . Section 3 introduces a simplified version of Unicoder using two pre - training tasks ( MLN and TLM ) for cross - lingual understanding tasks . Section 4 describes how to extend Unicoder to cover cross - lingual generation tasks . The original Unicoder includes more pre - training tasks besides MLM and TLM . But to keep the baseline pre - trained model simple and to reduce the experimental cost , we just use MLM and TLM in this paper . It means for understanding tasks , Unicoder is almost equal to XLM , except some hyper - parameter differences .", "entities": [[0, 2, "TaskName", "Question Generation"], [46, 47, "MetricName", "BLEU"], [109, 110, "MetricName", "BLEU"], [189, 190, "DatasetName", "MLM"], [213, 214, "DatasetName", "MLM"], [231, 232, "MethodName", "XLM"]]}
{"text": "Following Devlin et al ( 2019 ) , this task extends the masked language model task to multiple languages . At each iteration , a batch is composed of sentences sampled from different languages . The sampling probability of a language l i is defined as \u03bb l i = p \u03b1 l i / l i p \u03b1 l i , where p l i is the percentage of the language l i in the entire corpus , the smoothing factor \u03b1 is set to 0.3 . For each batch , we randomly sample 15 % of the words and replace them with ( i ) a special symbol [ MASK ] , ( ii ) a random token or ( iii ) keep them unchanged with probability 80 % , 10 % and 10 % , respectively . For each token , we only use its token embedding and position embedding , and discard segment embedding and language embedding .", "entities": [[51, 52, "HyperparameterName", "\u03b1"], [58, 59, "HyperparameterName", "\u03b1"], [82, 83, "HyperparameterName", "\u03b1"]]}
{"text": "Following Conneau and Lample ( 2019 ) , this task extends the MLM task to bilingual corpus . Given a bilingual sentence pair , TLM first concatenates them into a single sentence , and then masks words using the same strategy of MLM . The pre - trained model learns to recover each masked word based on the bilingual context . We follow MLM to sample language pairs in each batch with \u03b1 = 0.3 .", "entities": [[12, 13, "DatasetName", "MLM"], [42, 43, "DatasetName", "MLM"], [63, 64, "DatasetName", "MLM"], [72, 73, "HyperparameterName", "\u03b1"]]}
{"text": "Motivated by BART ( Lewis et al , 2019a ) , xDAE aims to predict the original text X = ( x 1 , x 2 , ... , x | X | ) l i from a language l i based on its corrupted form c ( X ) , where c ( X ) is a noising function that corrupts an input text X as its output . Four different text noising strategies for c ( ) are explored in this paper . ( 1 ) Shuffle the input text X by adding a noise \u03b1 \u223c U ( 0 , 3 ) to the input indices and then re - ordering X based on the rank of the noised indices . ( 2 ) Drop words with a probability of 0.1 . ( 3 ) Replace 10 % of the input words in X with the [ MASK ] symbol . ( 4 ) Sample a number of token spans from X with span lengths drawn from a Poisson distribution ( \u03bb = 3 ) , and then replace each token span with a single [ MASK ] token . Here , 0 - length spans correspond to the insertion of [ MASK ] tokens . Based on the performance of different noising strategies ( Table 10 ) , we select ( 4 ) and use it in pre - training . We leave finding better text noising strategies for future work . We train Unicoder using this task by maximizing the following loss function L xDAE : L xDAE = l i L X l i | X | t=1 log p ( x t | x < t , c ( X ) ) where L = l 1 , ... , l N denotes N languages , X is an instance in the i th language l i , p ( x t | x < t , c ( X ) ) denotes the probability of generating a single token x t at time step t given c ( X ) and x < t .", "entities": [[2, 3, "MethodName", "BART"], [97, 98, "HyperparameterName", "\u03b1"], [101, 102, "DatasetName", "0"], [195, 196, "DatasetName", "0"], [256, 257, "MetricName", "loss"]]}
{"text": "Motivated by ProphetNet ( Yan et al , 2020 ) , xFNP introduces a future n - gram prediction mechanism to natural language generation . It encourages the model to plan for the future tokens explicitly and prevents over - fitting on strong local correlations . Given an input text X = ( x 1 , x 2 , ... , x | X | ) l i from a language l i , we randomly mask k token spans of X to generate the masked text X as the input , and concatenate all masked token spans into Y as the output . Details of this mask strategy are described in Section 6.1 . After this , xFNP first encodes X to H enc with the encoder : H enc = Encoder ( X ) Then , instead of predicting the next token only at each time step , xFNP generates n future tokens simultaneously at time step t with the decoder : p ( y t | y < t , X ) , ... , p ( y t+n\u22121 | y < t , X ) = Decoder ( y < t , H enc ) Following Yan et al ( 2020 ) , we set n = 2 . We train Unicoder using this task by maximizing the following loss function L xF N P : L xF N P = l i L X l i { \u03b1 0 | Y | t=1 log p ( y t | y < t , X ) + \u03b1 1 | Y | \u22121 t=1 log p ( y t+1 | y < t , X ) } where X and Y are generated from X based on the method mentioned above . Following Yan et al ( 2020 ) , we set \u03b1 0 = \u03b1 1 = 1 .", "entities": [[2, 3, "MethodName", "ProphetNet"], [4, 7, "DatasetName", "Yan et al"], [200, 203, "DatasetName", "Yan et al"], [223, 224, "MetricName", "loss"], [242, 243, "HyperparameterName", "\u03b1"], [243, 244, "DatasetName", "0"], [261, 262, "HyperparameterName", "\u03b1"], [297, 300, "DatasetName", "Yan et al"], [306, 307, "HyperparameterName", "\u03b1"], [307, 308, "DatasetName", "0"], [309, 310, "HyperparameterName", "\u03b1"]]}
{"text": "Understanding Tasks The hyper - parameters are set as follows : 768 hidden units , 12 heads , GELU activation , a dropout rate of 0.1 , 512 max input length , 12 layers in encoder . In the pre - training stage , we first initialize Unicoder LC with XLM - R base , and then run continue pre - training with the accumulated 8 , 192 batch size with gradients accumulation . We use Adam Optimizer with a linear warm - up and set the learning rate to 3e - 5 . We select different understanding tasks randomly in different batches . This costed 12 days on 16 V100 . In the fine - tuning stage , the batch size is set to 32 . We use Adam Optimizer ( Kingma and Ba , 2014 ) with warm - up and set the learning rate to 5e - 6 . For all sentence classification tasks , we finetune 10 epochs . For POS Tagging and NER , we fine - tune 20 epochs . And for POS Tagging , we set the learning rate to 2e - 5 . For MLQA , we set the learning rate to 3e - 5 , batch size to 12 and train 2 epochs following BERT for SQuAD . After each epoch , we test the fine - tuned model on the dev sets of all languages . We select the model with the best average result on the dev sets of all languages . , the hyper - parameters are set as follows : 768 hidden units , 12 heads , GELU activation , a dropout rate of 0.1 , 512 max input length , 12 layers in encoder , 12 layers in decoder .", "entities": [[18, 19, "MethodName", "GELU"], [50, 51, "MethodName", "XLM"], [68, 70, "HyperparameterName", "batch size"], [76, 77, "MethodName", "Adam"], [77, 78, "HyperparameterName", "Optimizer"], [87, 89, "HyperparameterName", "learning rate"], [120, 122, "HyperparameterName", "batch size"], [129, 130, "MethodName", "Adam"], [130, 131, "HyperparameterName", "Optimizer"], [145, 147, "HyperparameterName", "learning rate"], [154, 156, "TaskName", "sentence classification"], [167, 168, "TaskName", "NER"], [184, 186, "HyperparameterName", "learning rate"], [192, 193, "DatasetName", "MLQA"], [197, 199, "HyperparameterName", "learning rate"], [204, 206, "HyperparameterName", "batch size"], [213, 214, "MethodName", "BERT"], [215, 216, "DatasetName", "SQuAD"], [270, 271, "MethodName", "GELU"]]}
{"text": "In the pre - training stage , we first initialize encoder and decoder with XLM - R , and then run continue pre - training with 1 , 024 batch size . We use Adam optimizer with warm - up and set the learning rate to 2e - 4 . This costed 10 days on 16 V100 . In the fine - tuning stage , the batch size is 1024 . We use Adam Optimizer with learning rate 1e - 5 and warm - up steps 2000 . For Unicoder xF N P SC , the hyper - parameters are set as follows : 1 , 024 hidden size , 12 layers in encoder , 12 layers in decoder , 512 max input length . In the pre - training stage , we pre - train the model from scratch and follow ProphetNet ( Yan et al , 2020 ) to randomly mask a continuous span ( with a fixed length 9 ) in every 64 tokens . About 15 % of the tokens in original sequence are masked in this step . We use a special symbol [ MASK ] to replace 80 % of the masked tokens , keep 10 % unchanged , and random replace 10 % of the masked tokens . We set the batch size to 1 , 024 , training steps to 350 , 000 . The learning rate is set to 1e - 4 . We set the number of future tokens n to 2 . In the fine - tuning stage , we use Adam Optimizer and set the learning rate to 1e - 4 . We set the batch size to 64 and the warm - up steps to 1 , 000 . ( Devlin et al , 2019 ) , XLM ( Conneau and Lample , 2019 ) and XLM - R base All models are ( 12 - layer ) based ones . Given a task , each pre - trained model is fine - tuned using its English training set only , and then applied to all test sets in different languages . AVG 2 U and AVG 2 G denote the average score of the average scores on 9 understanding tasks and 2 generation tasks , respectively . 12 - layer Unicoder xF N P SC trained on Wikipedia corpus for 100 languages . Given a downstream task , each pre - trained model is fine - tuned using its English training set and then applied to all test sets in different languages . Note that , all results are reproduced by this paper , except the XLM \u2020 results on XNLI are from Conneau and Lample ( 2019 ) .", "entities": [[14, 15, "MethodName", "XLM"], [29, 31, "HyperparameterName", "batch size"], [34, 35, "MethodName", "Adam"], [35, 36, "HyperparameterName", "optimizer"], [43, 45, "HyperparameterName", "learning rate"], [66, 68, "HyperparameterName", "batch size"], [73, 74, "MethodName", "Adam"], [74, 75, "HyperparameterName", "Optimizer"], [76, 78, "HyperparameterName", "learning rate"], [142, 143, "MethodName", "ProphetNet"], [144, 147, "DatasetName", "Yan et al"], [218, 220, "HyperparameterName", "batch size"], [233, 235, "HyperparameterName", "learning rate"], [262, 263, "MethodName", "Adam"], [263, 264, "HyperparameterName", "Optimizer"], [267, 269, "HyperparameterName", "learning rate"], [277, 279, "HyperparameterName", "batch size"], [300, 301, "MethodName", "XLM"], [309, 310, "MethodName", "XLM"], [440, 441, "MethodName", "XLM"], [444, 445, "DatasetName", "XNLI"]]}
{"text": "We reimplement the Transformer - XL model in Jax ( Bradbury et al , 2018 ) . In our experiments , we employ the base model in ( Dai et al , 2019 ) , except that we increase the tail shrinkage factor used for the adaptive softmax and input representations from 1 to 4 , which saves 63 % of the parameters without compromising the performance . On the full Wikitext - 103 dataset , our implementation has a test perplexity of 24.2 ( published result for this base model was 24.0 ) . We train our models using the standard likelihood objective for language models with a total batch size of 64 on 8 V100 GPUs . Adam optimizer is used with an initial learning rate of 2.5 \u00d7 10 \u22124 , which decays up to 200k steps following a cosine curve . During training , we use text segments of 150 steps and a memory of equal size . When evaluating the model , we use a sequence length of 64 and memory size 640 . Unless further noted , in our experiments we use an embedding size of 256 for BoW - conditioned models . For other models , we project each node or edge represented by BoW to an embedding space of size 128 . The default GNN we use has a single linear message passing layer of 256 hidden units .", "entities": [[3, 6, "MethodName", "Transformer - XL"], [46, 48, "MethodName", "adaptive softmax"], [81, 82, "MetricName", "perplexity"], [110, 112, "HyperparameterName", "batch size"], [119, 120, "MethodName", "Adam"], [120, 121, "HyperparameterName", "optimizer"], [126, 128, "HyperparameterName", "learning rate"]]}
{"text": "Our first task is text generation conditioned on the graph . We evaluate model performance by ( 1 ) computing model perplexity on held - out text and ( 2 ) drawing samples from the model and comparing that to the ground truth text article . We use BLEU score ( Papineni et al , 2002 ) to measure the similarity of our generated samples to the ground truth . Table 3 : The perplexity and the generated text reverse - BLEU score of different types of graph - conditioned models . We show the reverse - BLEU score with or without prompting the original title at the start of the text generation . Unlike previous use cases for BLEU score where there are many references for one generated sample , here we have only one ground truth reference but we can generate multiple samples . We therefore simply swapped the reference with the samples when computing the score , which we term as the reverse - BLEU ( rBLEU ) . We have also tried other ways of computing the BLEU score and find that they do n't change how models compare against each other . Unless explicitly stated , we let the model sample with a memory size of 640 , and condition on the graphs in the test set to generate text for up to 512 tokens per sample for a total of 20 samples per graph . The rBLEU score is computed based on these samples and corresponding ground - truth texts are truncated to the same length . We sample the texts from the distribution with a temperature of 0.8 . For each case , we report the average rBLEU score of 3 sampling runs . We find the variances are insignificant which do not affect the comparison results . In Appendix A.3 we also report results for generating longer samples for up to 4096 tokens .", "entities": [[4, 6, "TaskName", "text generation"], [21, 22, "MetricName", "perplexity"], [48, 50, "MetricName", "BLEU score"], [74, 75, "MetricName", "perplexity"], [81, 83, "MetricName", "BLEU score"], [97, 99, "MetricName", "BLEU score"], [111, 113, "TaskName", "text generation"], [119, 121, "MetricName", "BLEU score"], [167, 168, "MetricName", "BLEU"], [181, 183, "MetricName", "BLEU score"]]}
{"text": "In Table 3 , we show the perplexity and the rBLEU score of the unconditional , BoW , nodes - only , and GNN conditioned models . As a reference , a standard Transformer - XL model trained on the full Wikitext - 103 training set reaches 25.08 perplexity on our test set , which contains 71.7 % of the original test articles . We can see that the unconditional , i.e. text only , model trained on our dataset gets a very similar performance as trained on the full set . This is strong evidence that our dataset can be a good benchmark for state - of - the - art text generative models . We also see that conditioned on the graphs , model perplexity did n't improve , but the relevance of the samples measured by the BLEU scores did improve significantly . This indicates that the graph conditioned models can indeed steer the language model towards more relevant topics , but this so far can not yet improve likelihood metrics . To make the evaluation more fair to the text - only model , we also tried to prompt the generation with the title of the article , such that the text - only model also has some context . In this setting the graph models are still better , showing the importance of modeling the structure . Lastly , among all the 3 graph model variants , we observe that using a set of embeddings from the nodes model is better than using a single embedding from the BoW model , and fully utilizing the graph structure through the GNN model is consistently better than ignoring the edges as in the nodes model . However the differences among the methods are relatively small . For visualizations of a few graphs in our dataset and the corresponding samples generated based on them please refer to Appendix A.", "entities": [[7, 8, "MetricName", "perplexity"], [33, 36, "MethodName", "Transformer - XL"], [48, 49, "MetricName", "perplexity"], [126, 127, "MetricName", "perplexity"], [140, 141, "MetricName", "BLEU"]]}
{"text": "In this task , we evaluate the possibility of retrieving relevant text for a given query graph . We pair all articles with all graphs in the test set , resulting in 43\u00d743=1849 pairs . Then the trained graphconditioned language models are used to produce the per - token likelihood of each pair , and we use these likelihood scores to rank the text articles for each graph . We expect the learned models can rank the correct pairs higher than wrong ones . To measure the results we use standard ranking metrics including recall@K , which computes the fraction of times the correct pair is included in the top K predictions , as well as mean average precision ( mAP ) . In Table 5 , it is observed that graph - conditioned models can indeed retrieve more relevant texts from the graph than the unconditional model , among which the GNN - based model performs the best , and the unconditional model performs close to a random guess .", "entities": [[117, 119, "MetricName", "average precision"], [120, 121, "MetricName", "mAP"]]}
{"text": "A pattern which is not used in the construction of the Hearst Corpus is used here : Hyponym Noun Phrase is ( a | an | the ) Hypernym Noun Phrase . Here the original input paragraph is searched against this pattern and all the possible matches are returned in the form of hyponym : hypernym . a fennel is a plant is a match for this pattern with noun phrases a fennel and plant . UMBC Embedding : A word embedding matrix is created over the Normalized Corpus using word2vec ( Mikolov et al , 2013 ) . The specifications of the model are as follows : ( a ) Model : Continuous Bag of Words ( CBOW ) - a term 's embedding value is determined by its context words . The order of the words in the window size does not matter . ( b ) Window Size : 10 . The context window size for a term which determines its vector value . ( c ) Minimum Frequency Count : 5 . If the frequency of a word is less than this value , the word does not exist in the embedding . ( d ) Embedding Dimension Size : 300 . The number of dimensions for the embedding matrix .", "entities": [[200, 202, "HyperparameterName", "Embedding Dimension"]]}
{"text": "Output candidate hypernym lists are evaluated against gold hypernym lists using the following evaluation criteria : Mean Reciprocal Rank ( MRR ) , Mean Average Precision ( MAP ) and Precision At k ( P@k ) , where k is 1 , 3 , 5 and 15 . We ran our model against two sets of data training data and test data with 1500 input terms each . These results are shown in Tables 1 and 2 , where it can be clearly observed that our system performs much better for concepts . However , the IS - A module seemed to fetch good candidates for both entity and concept data . The gold data provided with the task does not always consider all possible word senses or domains of an input term . As a result , we observed numerous candidate hypernyms that seem to be plausible solutions that are not considered correct when compared to the gold data . For example , the input concept navigator has gold standard hypernyms of [ military branch , ex - plorer , military machine , travel , adventurer , seaman ] . Our system finds candidate hypernyms [ browser , web browser , website , application ] . We also noticed that due to our normalization decisions ( i.e. , using all lower - case characters ) and the contents of the corpus , Babbage performs poorly in some cases . For example , the gold hypernyms for input entity Hurricane are [ video game , software program , computer program ] but our system produced [ storm , windstorm , typhoon , tornado , cyclone ] . Clearly , our system did not differentiate between the named entity Hurricane and the common noun hurricane while training the wordembedding models . On the positive side , our system produced promising results in some cases . Hyponym liberalism produced [ theory , philosophy , economic policy ] which is very similar to the gold data [ economic theory , theory ] . It also correctly generated the hyponym person for hypernyms such as collector , moderator , director , senior , and reporter . For input reporter it produced [ writer , person ] which matches the gold hypernym set .", "entities": [[20, 21, "MetricName", "MRR"], [24, 26, "MetricName", "Average Precision"], [27, 28, "DatasetName", "MAP"], [30, 31, "MetricName", "Precision"], [96, 99, "DatasetName", "IS - A"]]}
{"text": "Word forms are ambiguous , and derive meaning from the context in which they appear . For example , the form \" bass \" can refer to a musical instrument , a low - frequency sound , a type of voice , or a kind of fish . The correct reference is determined by the surrounding linguistic context . Traditionally , this kind of ambiguity was dealt via word sense disambiguation ( WSD ) , a task that disambiguates word forms in context between symbolic sense - ids from a sense inventory such as WordNet ( Miller , 1992 ) or , more recently , BabelNet ( Navigli and Ponzetto , 2010 ) . Such sense inventories rely heavily on manual curation , are labor intensive to produce , are not available in specialized domains and inherently unsuitable for words with emerging senses . 1 This can be remedied by word sense induction ( WSI ) , a task where the input is a given word - type and a corpus , and the output is a derived sense inventory for that word . Then , sense disambiguation can be performed over the WSI - derived senses . The introduction of large - scale pre - trained LMs and Masked LMs ( MLM ) seemingly made WSI / WSD tasks obsolete : instead of representing tokens with symbols that encode sense information , each token is associated with a contextualized vector embeddings that captures various aspects of its in - context semantics , including the word - sense . These contextualized vectors proved to be very effective as features for downstream NLP tasks . However , contextualized embeddings also have some major shortcomings : most notably for our case , they are expensive to store ( e.g. BERT embeddings are 768 or 1024 floating point numbers for each token ) , and are hard to index and query at scale . Even if we do manage to store and query them , they are not interpretable , making it impossible for a user to query for a particular sense of a word without providing a full disambiguating context for that word . For example , consider a user wishing to query a dataset for sentences discussing Oracle in the mythology - prophet sense , rather than the tech company sense . It is not clear how to formulate such a query to an index of contextualized word vectors . However , it is trivial to do for an index that annotates each token with its derived sense - i d ( in terms of UI , after a user issues a query such as \" Oracle \" , the system may show a prompt such as \" did you mean Oracle related to IBM ; Sun ; Microsoft , or to Prophet ; Temple ; Queen \" , allowing to narrow the search in the right direction ) . Goldberg ( 2018 , 2019 ) show how contextualized embeddings can be used for achieving state - of - the - art WSI results . The core idea of their WSI algorithm is based on the intuition , first proposed by Ba\u015fkaya et al ( 2013 ) , that occurrences of a word that share a sense , also share in - context substitutes . An MLM is then used to derive top - k word substitutes for each word , and these substitutevectors are clustered to derive word senses . Our main contribution in this work is proposing a method that scales up Amrami and Goldberg ( 2018 ) 's work to efficiently annotate all tokens in a large corpus ( e.g. Wikipedia ) with automatically derived word - senses . This combines the high - accuracy of the MLM - based approach , with the symbolic representation provided by discrete sense annotations . The discrete annotations are interpretable ( each sense is represented as a set of words ) , editable , indexable and searchable using standard IR techniques . We show two applications of the discrete annotations , the first one is senseaware information retrieval ( 7 ) , and the second is high - quality senseful static word embeddings we can derive by training a static embeddings model on the large sense annotated corpus ( 8 ) . We first show how the method proposed by Amrami and Goldberg ( 2018 ) can be adapted from deriving senses of individual lemmas to efficiently and cheaply annotating all the corpus occurrences of all the words in a large vocabulary ( 3 ) . Deriving word - sense clusters for all of English Wikipedia words that appear as single - token words in BERT - LARGE 's ( Devlin et al , 2019 ) vocabulary , and assigning a sense to each occurrence in the corpus , required 100 hours of cheap P100 GPUs ( 5 hours of wall - clock time on 20 single GPU machines ) followed by roughly 4 hours on a single 96 - cores CPU machines . The whole process requires less than 50 GB of disk space , and costs less than 150 $ on Google Cloud platform . After describing the clustering algorithm ( 4 ) , we evaluate the quality of our system and of the automatic sense tagging using SemEval datasets and a new manually annotated dataset we created ( 5 ) . We show that with the produced annotated corpora it is easy to serve sense - aware information retrieval applications ( 7 ) . Another immediate application is feeding the sense - annotated corpora to a static embedding algorithm such as word2vec ( Mikolov et al , 2013 ) , for deriving sense - aware static embeddings ( 8 ) . This results in state - of - theart sense - aware embeddings , which we evaluate both on an existing WiC benchmark ( Pilehvar and Camacho - Collados , 2019 ) and on a new challenging benchmark which we create ( 9 ) . In contrast to WSD which relies on curated sense inventories , our method is data - driven , therefore resulting senses are corpus dependent . The method can be applied to any domain for which a BERTlike model is available , as we demonstrate by applying it to the PubMed Abstracts of scientific papers , using SCIBERT ( Beltagy et al , 2019 ) . The resulting senses cover scientific terms which are not typically found in standard sense inventories ( 6 ) . Figure 1 shows examples of induced senses for selected words from the English Wikipedia corpus . For each sense we list 5 communitybased representatives ( 3 ) , as well as the 5 closest neighbours in the sense - aware embedding space ( 8 ) . Additional examples are available in Appendix A. Code and resources are available in github.com/allenai/WSIatScale .", "entities": [[68, 71, "TaskName", "word sense disambiguation"], [150, 153, "TaskName", "word sense induction"], [212, 213, "DatasetName", "MLM"], [297, 298, "MethodName", "BERT"], [555, 556, "DatasetName", "MLM"], [626, 627, "MetricName", "accuracy"], [629, 630, "DatasetName", "MLM"], [685, 687, "TaskName", "information retrieval"], [700, 702, "TaskName", "word embeddings"], [784, 785, "MethodName", "BERT"], [862, 863, "DatasetName", "Google"], [919, 921, "TaskName", "information retrieval"], [983, 984, "DatasetName", "WiC"]]}
{"text": "For each word w in the vocabulary , we construct a graph G w = ( V w , E w ) where each vertex v V w is a substitute - word predicted by the MLM for w , and an edge ( u , v ) E w connects substitutes that are predicted for the same instance . The edge is weighted by the number of instances in which both u and v were predicted . More formally , let X = { x i w } n i=1 bet the set of all top - k substitutes for n instances of word w , and x i w = { w j x i w } k j=1 represents the k top substitutes for the ith instance of word w. The graph G w is defined as follows : V w = { u : i u x i w } E w = { ( u , v ) : i u x i w v x i w } W ( u , v ) = | { i : ( u , v ) x i w } | Community detection A community in a subgraph corresponds to a set of tokens that tend to co - occur in top - k substitutes of many instances , and not co - occur with top - k substitutes of other instances . This corresponds well to senses and we take community 's nodes as sense 's representatives . We identify communities using the fast \" Louvain \" method ( Blondel et al , 2008 ) . Briefly , Louvain searches for an assignment of nodes to clusters such that the modularity score Q - which measures the density of edges inside communities compared to edges between communities - is maximized : Q = 1 2 m u v W ( u , v ) \u2212 k u k v 2 m \u03b4 ( c u , c v ) m is the sum of all edge weights in the graph , k u = v W ( u , v ) is the sum of the weights of the edges attached to node u , c u is the community to which u is assigned , and \u03b4 is Kronecker delta function . This objective is optimized using an iterative heuristic process . For details , see Blondel et al ( 2008 ) .", "entities": [[36, 37, "DatasetName", "MLM"], [195, 197, "TaskName", "Community detection"], [326, 327, "HyperparameterName", "\u03b4"], [382, 383, "HyperparameterName", "\u03b4"]]}
{"text": "We start by intrinsically evaluating the WSI clustering method on : ( a ) SemEval 2010 and SemEval 2013 ; and ( b ) a new test set we develop for largescale WSI . In section 9 , we additionally extrinsically evaluate the accuracy of static embeddings derived from a sense - induced Wikipedia dataset . When collecting word - substitutes , we lemmatize the top - k list , join equivalent lemmas , remove stopwords and the target word from the list , and keep the top - 5 remaining lemmas .", "entities": [[17, 19, "DatasetName", "SemEval 2013"], [43, 44, "MetricName", "accuracy"]]}
{"text": "We evaluate the community - based WSI algorithm on two WSI datasets : SemEval 2010 Task 14 and SemEval 2013 Task 13 ( Jurgens and Klapaftis , 2013 ) . Table 2 compares our method to Goldberg ( 2018 , 2019 ) and AutoSense ( Amplayo et al , 2019 ) , which is the second - best available WSI method . Bert - noDP / DP are taken from Amrami and Goldberg ( 2019 ) . Bert - DP uses \" dynamic patterns \" which precludes widescale application . We follow previous work Komninos and Manandhar , 2016 ; Amrami and Goldberg , 2019 ) and evaluate SemEval 2010 using F - Score and V - Measure and SemEval 2013 using Fuzzy Normalized Mutual Information ( FNMI ) and Fuzzy B - Cubed ( FBC ) as well as their geometric mean ( AVG ) . Our method performs best on SemEval 2010 and comparable to state - of - the - art results on SemEval 2013 . The algorithm performs on - par with the Bert - noDP method , and does not fall far behind the Bert - DP method . We now turn to assess the end - to - end induction and tagging over Wikipedia .", "entities": [[18, 20, "DatasetName", "SemEval 2013"], [113, 114, "MetricName", "Score"], [119, 121, "DatasetName", "SemEval 2013"], [166, 168, "DatasetName", "SemEval 2013"]]}
{"text": "We used a list of 20 ambiguous words from CoarseWSD - 20 ( Loureiro et al , 2021 ) . The full list and per - word results can be found in Appendix C. For each word we sampled 100 passages from English Wikipedia with the target word , including inflected forms ( case insensitive ) . Unlike CoarseWSD - 20 , we sampled examples without any respect to a predefined set of senses . For example , the only two senses that appear in CoarseWSD - 20 for the target word arm are arm ( anatomy ) , and arm ( computing ) , leaving out instances matching senses reflecting weapons , subdivisions , mechanical arms etc . With the notion that word sense induction systems should be robust to different annotations schemes , we gave two fluent English speakers 100 sentences for each of the 20 ambiguous words from CoarseWSD - 20 . Annotators were not given a sense inventory . Each annotator was asked to label each instance with the matching sense according to their judgment . For example , for the target word apple in the sentence \" The iPhone was announced by Apple CEO . \" , annotators can label the target sense with Apple Inc. , Apple The Company etc . Annotation Guidelines are available in Appendix B. On average annotators labeled 6.65 senses per word ( 5.85 and 7.45 average clusters per word for the two annotators ) . This is more than the 2.65 average senses according to CoarseWSD - 20 and less than WordNet 's 9.85 . Results We report our system 's performance alongside two additional methods : A strong baseline of the most frequent sense ( MFS ) , and Babelfy ( Moro et al , 2014 ) - the sense disambiguation system used in BabelNet ( Tested using Babelfy live version April 2021 ) . Differently from the latter , our system does not disambiguates but induces senses , therefore , clusters are not labeled with a sense tag from a sense inventory . Instead , we represent senses to annotators using a list of common substitute words and a few examples . Thus , after annotating the Wikipedia passages , we additionally asked annotators to name the system 's clusters with the same naming convention as in their annotations . Given a similar naming convention between systems and annotators , we report F1 scores of systems ' tagging accuracy with respect to the manual annotations . We report F1 averaged over words in Table 3 . Our system outperforms both baselines , despite Babelfy having access to a list of predefined word senses . A full by - word table and comprehensive results analysis are in Appendix C. While a 1 - to - 1 mapping between system clusters and manual senses is optimal , our system sometimes splits senses into smaller clusters , thus annotators will name two system clusters with the same label . Therefore it is also important to report the number of clusters produced by the system comparing to the number of senses after the annotators merged similar clusters . Our system produced 7.25 clusters with 2.25 clusters on average merged by the annotators . 7 Additionally , in rare cases our system encapsulates a few senses in a single cluster : this happened 3 and 5 times for both annotators across all the dataset .", "entities": [[123, 126, "TaskName", "word sense induction"], [405, 406, "MetricName", "F1"], [411, 412, "MetricName", "accuracy"], [421, 422, "MetricName", "F1"]]}
{"text": "Acc . JBT ( Pelevina et al , 2016 ) 53.6 Sense - aware Embeddings ( this work ) 58.3 SW2V * ( Mancini et al , 2017 ) 58.1 DeConf * ( Pilehvar and Collier , 2016 ) 58.7 LessLex * ( Colla et al , 2020 ) 59.2 Our method is the following : Given the senseaware embeddings , a target word w and two contexts , we calculate the context vector as the average of the context words . The matching sense vector is the closest out of all w embeddings . We then classify the contexts as corresponding to the same meaning if the cosine distance of the found sense embedding is more than threshold apart . We do not use the train set . The threshold is optimized over the development set and fixed to 0.68 . This task has a few tracks , we compare our embeddings systems to the best performing methods from the Sense Representations track . Of these , JBT ( Pelevina et al , 2016 ) , a lexical embedding method , is the only one that does not use an external lexical resource ( induction ) . The results in Table 4 show accuracy on this task . We outperform the induction method , and are on - par with the lexicon - based methods , despite not using any external lexical resource .", "entities": [[0, 1, "MetricName", "Acc"], [204, 205, "MetricName", "accuracy"]]}
{"text": "Another setup for evaluating word embeddings is that of outlier detection : given a set of words , identify which one does not belong to the set ( Blair et al , 2016 ) . Outlier detection instances are composed of in - group elements and a set of outliers from a related semantic space . In each evaluation round , one outlier is added to the in - group items , and the algorithm is tasked with finding the outlier . Existing outlier detection datasets either did not explicitly target sense - ambiguous words ( 8 - 8 - 8 ( Camacho - Collados and Navigli , 2016 ) , WikiSem500 ( Blair et al , 2016 ) ) or explicitly removed ambiguous words altogether ( 25 - 8 - 8 - sem ( Brink Andersen et al , 2020 ) ) . Ambiguity - driven Outlier Detection . We construct a challenge set for outlier detection that specifically targets ambiguous cases . In order to account for sense ambiguity , we add a distractor to each of the in - group sets : the distractor is an item which has multiple senses , where the most salient sense does not belong to the group , while another sense does belong to the group . For example : In - group : zeus , hades , poseidon , aphrodite , ares , athena , artemis Outliers : mercury , odysseus , jesus , sparta , delphi , rome , wrath , atlanta Distractor : nike Here , a model which does not explicitly represent the greek - god sense of nike is likely to place it far away from the in - group instances , causing it to be mistakenly marked as the outlier . The starting point for our dataset is 25 - 8 - 8 - Sem ( Brink Andersen et al , 2020 ) . This dataset contains 25 test groups , each with 8 in - group elements and 8 outliers , resulting in 200 unique test cases . The outliers are sorted in a decreasing degree of relatedness to the in - group elements . In our dataset we replace one of the in - group elements with an ambiguous distractor . For example , in the Greek - gods case above , we replaced the original 8 th item ( \" hera \" ) with the ambiguous distractor nike . 12 The dataset consists of 25 groups of 7 non ambiguous group elements , 1 distractor and 8 outliers ( 25 - 7 - 1 - 8 ) , similarly resulting 200 unique test cases . Method Following Camacho - Collados and Navigli ( 2016 ) , we rank each word likelihood of being the outlier by the average of all pair - wise semantic similarities of the words in W \\ { w } . Therefore if w is an outlier , this score should be low . See Appendix D for additional details . Metrics Camacho - Collados and Navigli ( 2016 ) proposed evaluating outlier detection using the accuracy ( The fraction of correctly classified outliers among the total cases ) and Outlier Position Percentage ( OPP ) metric . OPP indicates how close outliers are to being classified correctly : OP P = W D OP ( W ) | W | \u22121 | D | \u00d7 100 where OP ( W ) is the position of the outlier according to the algorithm . Results In Table 5 we report performance of on the 25 - 7 - 1 - 8 set . Word2vec and GloVe accuracy scores are low while having high OPP scores . This is the expected behaviour for embeddings without sense awareness . These will position the distractor and the outlier furthest away from the group items while not designed to make the hard decision required for high Accuracy . Our sense - aware embeddings strongly outperform GloVe and word2vec which do not include senses . Our embeddings also outperform the word embeddings proposed in De - Conf ( Pilehvar and Collier , 2016 ) , which are the best performing sense embeddings on WiC which are also publicly available .", "entities": [[4, 6, "TaskName", "word embeddings"], [9, 11, "TaskName", "outlier detection"], [35, 37, "TaskName", "Outlier detection"], [83, 85, "TaskName", "outlier detection"], [111, 112, "DatasetName", "WikiSem500"], [147, 149, "TaskName", "Outlier Detection"], [156, 158, "TaskName", "outlier detection"], [235, 236, "DatasetName", "artemis"], [240, 241, "DatasetName", "odysseus"], [514, 516, "TaskName", "outlier detection"], [518, 519, "MetricName", "accuracy"], [606, 607, "MethodName", "GloVe"], [607, 608, "MetricName", "accuracy"], [653, 654, "MetricName", "Accuracy"], [662, 663, "MethodName", "GloVe"], [676, 678, "TaskName", "word embeddings"], [699, 700, "DatasetName", "WiC"]]}
{"text": "In table 6 we report a by - word analysis of our manual evaluation results . For each word we detail F1 scores of the most frequent sense ( MFS ) , Babelfy , and our proposed system . Similarly to Loureiro et al ( 2021 ) , we report the ratio of the first sense with respect to the rest ( F2R ) and normalized entropy 17 to reflect sense balance . All of which are reported per annotator . Analysis Analysis of our system 's error shows that for some words the system could not create a matching cluster for specific senses ( to name a few examples , \" yard \" as a ship identifier and \" impound / enclosure \" sense for the word \" pound \" ) . It appears that a matching cluster was not created due to the low tally of these senses in the English Wikipedia , and indeed the two senses appeared only two and three times respectively in the 100 for all words 17 Computed as \u2212 k i=1 c i n log c i n log ( k ) , where k is the number of annotated senses , each of size ci and n is the size of annotated examples per word , in our case n = 100 . passages sample . Additionally , annotator 2 annotated in a more fine - grained manner that does not correspond to our system tendency to merge capitalized instances of the target word into a sense that corresponds to \" part of named entity \" . As described above , in rare cases our system merged two senses into a single cluster . For example , the same cluster of the word \" trunk \" contained occurrences which annotator 1 tagged either \" human torso \" or \" tube - like organs \" ( like the pulmonary trunk ) . While such annotation was uncommon ( 3 out of 117 senses for annotator 1 and 5 out of 149 senses for annotator 2 ) , it does affect our system 's micro F1 score for the better . In case we do not allow such annotation our overall score drops from 87.52 to 86.65 . A comparison between Babelfy and our gold annotation shows a common mistake in its labeling where Babelfy attributes the vast majority of sentences to the same non - salient sense . For example , Babelfy attributes 77 out of 100 instances of hood to \" An aggressive and violent young criminal \" - a sense that was not found even once in the manual annotation . While in a number of cases Babelfy used finer - grained sysnset groups than in our annotations we took into account any senses that are a subset of our annotated senses . For examples , Babelfy 's \" United States writer who lived in Europe ; strongly influenced the development of modern literature ( 1885 - 1972 ) \" synset was attribute any instances from the senses surname that refer to the writer Ezra Pound .", "entities": [[21, 22, "MetricName", "F1"], [351, 353, "MetricName", "micro F1"]]}
{"text": "For well over half a century , solutions to the ad hoc retrieval problem - where the system 's task is return a list of top k texts from an arbitrarily large corpus D that maximizes some metric of quality such as average precision or NDCG - has been dominated by sparse vector representations , for example , bag - of - words BM25 . Even in modern multi - stage ranking architectures , which take advantage of large pretrained transformers such as BERT ( Devlin et al , 2019 ) , the models are deployed as rerankers over initial candidates retrieved based on sparse vector representations ; this is sometimes called \" first - stage retrieval \" . One well - known example of this design is the BERT - based reranker of Nogueira and Cho ( 2019 ) ; see Lin et al ( 2020 ) for a recent survey . multi - vector bi - encoders or cross - encoders . Hence , improving the effectiveness of single - vector biencoders represents an important problem . One approach to improving the effectiveness of single - vector bi - encoders is hard negative mining , by training with carefully selected negative examples that emphasize discrimination between relevant and non - relevant texts . There are several approaches to accomplish this . Karpukhin et al ( 2020 ) and Qu et al ( 2020 ) leverage large in - batch negatives to enrich training signals . Guu et al ( 2020 ) and propose to mine hard negatives using the trained bi - encoder itself . By searching for global negative samples from an asynchronously updated ANN index , the bi - encoder can learn information not present in the training data produced by sparse representations . However , both large in - batch negative sampling and asynchronous ANN index updates are computationally demanding . The later is especially impractical for large corpora since it requires periodic inference over all texts in the corpus to ensure that the best negative examples are retrieved . There is also work that explores knowledge distillation ( KD ) ( Hinton et al , 2015 ) to enhance retrieval effectiveness and efficiency . Most related to our study is Hofst\u00e4tter et al ( 2020 ) , who demonstrate that KD using a cross - encoder teacher significantly improves the effectiveness of bi - encoders for dense retrieval . Similarly , Barkan et al ( 2020 ) investigate the effectiveness of distilling a trained cross - encoder into a bi - encoder for sentence similarity tasks . Gao et al ( 2020a ) explore KD combinations of different objectives such as language modeling and ranking . However , the above papers use computationally expensive cross - encoder teacher models ; thus , combining them for KD with more advanced negative sampling techniques can be impractical . In light of existing work on hard negative mining and knowledge distillation , we propose to improve the effectiveness of single - vector bi - encoders with a more efficient KD approach : in - batch KD using a bi - encoder teacher . The advantage of our design is that , during distillation , it enables the efficient exploitation of all possible query - passage pairs within a minibatch , which we call tight coupling ( illustrated in Figure 1 ) . This is a key difference between our KD approach and previous methods for dense retrieval , where only the scores of given query - passage triplets ( not all combinations ) are", "entities": [[42, 44, "MetricName", "average precision"], [83, 84, "MethodName", "BERT"], [129, 130, "MethodName", "BERT"], [351, 353, "MethodName", "knowledge distillation"], [492, 494, "MethodName", "knowledge distillation"]]}
{"text": "The bi - encoder design has been widely adopted for dense retrieval Guu et al , 2020 ; Karpukhin et al , 2020 ; Luan et al , 2021 ; Qu et al , 2020 ; , where queries and passages are encoded in a low - dimensional space . It aims to learn lowdimensional representations that pull queries and relevant passages together and push queries and non - relevant passages apart . Following the work of Mnih and Kavukcuoglu ( 2013 ) , we formulate a common objective for dense representation learning for passage retrieval . Given a query q and a parameterized scoring function \u03c6 \u03b8 that computes the relevance between a query and a candidate passage p , we define a probability distribution over documents in a corpus D with respect to relevance , as follows : P q \u03b8 ( p , D ) = exp ( \u03c6 \u03b8 ( q , p ) ) p D exp ( \u03c6 \u03b8 ( q , p ) ) = exp ( h q h p ) p D exp ( h q h p ) , ( 1 ) where h q ( h p ) R d denotes the query ( passage ) representation produced by the bi - encoder . A typical bi - encoder uses a simple scoring function for \u03c6 \u03b8 , for example , the inner product of two vectors , as shown above . The main challenge of evaluating and computing gradients of Eq . ( 1 ) is the prohibitively expensive computation cost given the number of passages in the corpus D , typically millions ( or even more ) . This is already setting aside the cost of using pretrained transformers such as BERT as the encoder to compute h q and h p . Thus , previous work approximates Eq . ( 1 ) by NCE , which samples p D + from training data and p D = { D + \u222a D \u2212 } , where D \u2212 is from a noisy distribution such as candidates retrieved by BM25 ( Nogueira and Cho , 2019 ) , filtered by finetuned transformers ( Qu et al , 2020 ) , or retrieved by an asynchronously updated bi - encoder model itself . Another simple yet effective approach is in - batch negative sampling , as used by Karpukhin et al ( 2020 ) , which takes p and p of other queries within a minibatch as negative examples in NCE .", "entities": [[91, 93, "TaskName", "representation learning"], [94, 96, "TaskName", "passage retrieval"], [107, 108, "HyperparameterName", "\u03b8"], [142, 143, "HyperparameterName", "\u03b8"], [152, 153, "HyperparameterName", "\u03b8"], [164, 165, "HyperparameterName", "\u03b8"], [227, 228, "HyperparameterName", "\u03b8"], [294, 295, "MethodName", "BERT"]]}
{"text": "Other than designing sophisticated sampling methods for p , training bi - encoder models using knowledge distillation ( KD ) with effective teacher models is another promising approach ( Hofst\u00e4tter et al , 2020 ) . In this case , we aim to make the bi - encoder model mimic the teacher model 's probability distribution as follows : P q \u03b8 ; student ( p , D ) = exp ( h q h p ) p D exp ( h q h p ) \u2248 exp ( \u03c6\u03b8 ( q , p ) /\u03c4 ) p D exp ( \u03c6\u03b8 ( q , p ) /\u03c4 ) = P q \u03b8 ; teacher ( p , D ) , ( 2 ) where \u03c6\u03b8 denotes the relevance score estimated by a pretrained model parameterized by\u03b8 and \u03c4 , the temperature hyperparameter used in the KD framework . To improve retrieval effectiveness , one can leverage pre - computed scores from pretrained models such as cross - encoders , e.g. , BERT , bi - encoders , e.g. , ColBERT , or ensembled scores from multiple models \u03c6\u03b8 = j \u03c6\u03b8 ; j . 3 Our Approach arg min \u03b8 q Q B p D B L \u03c6 \u03b8 , \u03c6\u03b8 , ( 3 ) where L \u03c6 \u03b8 , \u03c6\u03b8 is : P q \u03b8 ; teacher ( p , D B ) log P q \u03b8 ; teacher ( p , D B ) P q \u03b8 ; student ( p , D B ) . ( 4 ) Note that here we consider all pairwise relationship between queries and passages within a minibatch that contains a query set Q B and a passage set D B .", "entities": [[15, 17, "MethodName", "knowledge distillation"], [61, 62, "HyperparameterName", "\u03b8"], [112, 113, "HyperparameterName", "\u03b8"], [172, 173, "MethodName", "BERT"], [200, 201, "HyperparameterName", "\u03b8"], [209, 210, "HyperparameterName", "\u03b8"], [219, 220, "HyperparameterName", "\u03b8"], [226, 227, "HyperparameterName", "\u03b8"], [238, 239, "HyperparameterName", "\u03b8"], [249, 250, "HyperparameterName", "\u03b8"]]}
{"text": "In this section , we conduct experiments on the MS MARCO passage and document corpora . For passage ranking , we first train models on BM25 negatives as warm - up and compare different KD methods . We then further train models on the hard negatives retrieved by the BM25 warmed - up checkpoint . For document ranking , following previous work Zhan et al , 2020 ; Lu et al , 2021 ) , we start with our BM25 warmed - up checkpoint for passage ranking and conduct additional hard negative training . To evaluate output quality , we report MRR@10 ( NDCG@10 ) for MARCO Dev ( TREC - DL ' 19 ) and Recall@1 K , denoted as R@1K. To compare with current state - of - the - art models , we evaluate our design , TCT - ColBERT , under two approaches for negative sampling : ( 1 ) BM25 and ( 2 ) hard negatives retrieved by the bi - encoder itself .", "entities": [[9, 11, "DatasetName", "MS MARCO"], [17, 19, "TaskName", "passage ranking"], [56, 58, "TaskName", "document ranking"], [85, 87, "TaskName", "passage ranking"], [103, 104, "MetricName", "NDCG@10"], [109, 110, "DatasetName", "TREC"], [116, 117, "MetricName", "Recall@1"]]}
{"text": "In this setting , models are trained using the official public data triples.train.small , where negative samples are produced by BM25 . We compare different bi - encoder models using BERT - base as the backbone , which uses single 768 - dim vectors to represent each query and passage : 1 . Baseline : a single - vector bi - encoder trained with in - batch negatives , as discussed in Section 2.1 , which is similar to Karpukhin et al ( 2020 ) but with a smaller batch size . 2 . Pairwise KD : the approach of Hofst\u00e4tter et al ( 2020 ) , who improve ranking effectiveness using cross - encoders with pairwise KD . We also compare against two models , KD - T1 and KD - T2 , which use BERT - base bi - encoders as student models . In the former , the student is distilled from a BERT - base cross - encoder , while the latter is distilled from ensembled cross - encoders comprising BERT - base , BERT - large , and ALBERTlarge . These figures reported in Table 2 are copied from Hofst\u00e4tter et al ( 2020 ) . For a fair comparison with our models based on KL - divergence KD , we also implement our KD - T2 using the precomputed pairwise softmax probabilities provided by Hofst\u00e4tter et al ( 2020 ) ( who use MSE margin loss for KD ) . In addition , we adopt pairwise softmax probabilities from fine - tuned ColBERT to train KD - ColBERT for comparison . All our models are fine - tuned with batch size 96 and learning rate 7 \u00d7 10 \u22126 for 500 K steps on a single TPU - V2 . For TCT - ColBERT , there are two steps in our training procedure : ( 1 ) finetune \u03c6\u03b8 ; MaxSim as our teacher model , ( 2 ) freeze \u03c6\u03b8 ; MaxSim and distill knowledge into our student model \u03c6 \u03b8 . We keep all the hyperparameter settings the same but adjust temperature \u03c4 = 0.25 for KD at the second step . For all our models , including the baseline , we initialize the student model using the fine - tuned weights of the teacher model in the first step . We limit the input tokens to 32 ( 150 ) for queries ( passages ) . To evaluate effectiveness , we encode all passages in the corpus and conduct brute force search over the vector representations . Our main results , including paired t - test for significance testing , are shown in Table 2 . In addition to the effectiveness of the student models , we also show the effectiveness of the teacher models for the KD methods . 1 First , we see that pairwise KD methods show significant improvements over the baseline , indicat - 0 1 2 3 4 5 6 7 8 9 Index Size ( 10 ing that information from BM25 negatives can not be fully exploited without teacher models . Second , although KD - T2 improves the bi - encoder 's effectiveness over KD - T1 , it is not consistently better than KD - ColBERT in terms of students ' effectiveness . We suspect that they have comparable capabilities to discriminate most paired passages ( BM25 negative vs. positive samples ) , i.e. , Col - BERT is good enough to guide bi - encoder student models to discriminate them . On the other hand , our TCT - ColBERT model , which uses only one teacher model and adds only 33 % more training time over the baseline , yields the best effectiveness , demonstrating the advantages of our proposed inbatch KD - exhaustive exploitation of all querydocument combinations in a minibatch . To understand why TCT - ColBERT yields better results , we study the models ' retrieval effectiveness against carefully selected distractors . We start with a small synthetic corpus composed of the relevant passages and the top - 1000 BM25 candidates of the 6980 ( 43 ) queries from MARCO Dev ( TREC - DL ' 19 ) . To increase the corpus size , we gradually add passages uniformly sampled from the corpus without replacement . From Figure 2 , we see that the three KD models exhibit nearly the same effectiveness when the corpus only contains BM25 candidates . This shows that the bi - encoders learn to discriminate relevant passages from the BM25 negative samples well . However , as the index size increases , TCT - ColBERT demonstrates better ranking effectiveness than the other pairwise KD methods , indicating that the learned representations are more robust . We attribute this robustness against \" distractors \" to the enriched information from inbatch KD , where we are able to exploit all in - batch query - document combinations .", "entities": [[30, 31, "MethodName", "BERT"], [89, 91, "HyperparameterName", "batch size"], [136, 137, "MethodName", "BERT"], [156, 157, "MethodName", "BERT"], [174, 175, "MethodName", "BERT"], [178, 179, "MethodName", "BERT"], [226, 227, "MethodName", "softmax"], [239, 240, "MetricName", "MSE"], [241, 242, "MetricName", "loss"], [252, 253, "MethodName", "softmax"], [275, 277, "HyperparameterName", "batch size"], [279, 281, "HyperparameterName", "learning rate"], [337, 338, "HyperparameterName", "\u03b8"], [487, 488, "DatasetName", "0"], [574, 575, "MethodName", "BERT"], [694, 695, "DatasetName", "TREC"]]}
{"text": "In this subsection , we evaluate TCT - ColBERT when training with hard negatives ( HNs ) . We compare our model to four competitive approaches : 1 . ANCE is the most representative work , which proposes asynchronous index refreshes to mine hard negatives . The model is trained for 600 K steps with index refreshes every 10 K steps . ANCE uses RoBERTa - base as its backbone . 2 . LTRe ( Zhan et al , 2020 ) further improves from an ANCE checkpoint by adding more training steps with the same hard negative mining approach ; thus , the computation cost of index refreshes from ANCE can not be neglected . LTRe also use RoBERTa - base as its backbone . 3 . SEED - Encoder ( Lu et al , 2021 ) leverages a pretraining strategy to enhance the capability of the bi - encoder , which is further fine - tuned with HNs using asynchronous index refreshes . 4 . RocketQA ( Qu et al , 2020 ) trains a bi - encoder model using hard negatives denoised by a crossencoder , ERNIE - 2.0 - Large ( Sun et al , 2019 ) . It further demonstrates that training bi - encoders with many in - batch negatives ( batch size up to 4096 ) significantly improves ranking effectiveness ; however , this approach is computationally expensive ( the authors report using 8\u00d7V100 GPUs for training ) . To the best of our knowledge , RocketQA represents the state of the art in single - vector bi - encoders for dense retrieval . For a more fair comparison , we also report the ranking effectiveness of their model trained with a smaller batch size of 128 . For all the approaches above , we directly copy the reported effectiveness from the original papers . For our TCT - ColBERT model , following the settings of the above approaches , we first use our TCT - ColBERT model trained on BM25 negatives as a warm - up starting point and index all 8.8 M MARCO passages . Using the warmed - up index , we retrieve top - 200 passages for each training query and randomly sample ( with replacement ) hard negatives from the 200 candidates to form our training data . Note that due to resource limitations we do not conduct experiments with asynchronous index refreshes since multiple V100 GPUs are required for such a model training scheme . 2 In this experiment , all the hyperparameter settings are the same as the ones in the BM25 negative training , except for training steps , which is set to 100 K for both student and teacher training . Table 3 reports the results of our experiments with hard negative training . First , we observe that our TCT - ColBERT model trained with BM25 negatives marginally outperforms the other models trained with HNs , except for RocketQA . Comparing the different training strategies discussed in Section 3.3 ( second main block of the table ) , we see that the ranking effectiveness of TCT - ColBERT ( HN ) degrades when training on hard negatives without the guide of a teacher . This is consistent with the findings of Qu et al ( 2020 ) that hard negatives contain noisy information ( i.e. , some hard negatives may actually be relevant ) . Also , show that training bi - encoders with hard negatives can be unstable : hard negatives benefit ranking effectiveness only under certain hyperparameter settings . In contrast , hard negative training using Col - BERT 's in - batch KD further boosts ranking effectiveness , especially when our teacher ( ColBERT ) 2 Re - encoding the entire corpus takes \u223c10 hours on one GPU . is trained with the same hard negative samples beforehand . It is also worth noting that our TCT - ColBERT ( w/ TCT HN+ ) with batch size 96 yields competitive ranking effectiveness compared to RocketQA ( the current state of the art ) , which uses batch size 4096 . These results demonstrate the advantages of our TCT design : our approach effectively exploits hard negatives in a computationally efficient manner ( i.e. , without the need for large batch sizes or periodic index refreshes ) .", "entities": [[64, 65, "MethodName", "RoBERTa"], [118, 119, "MethodName", "RoBERTa"], [127, 128, "DatasetName", "SEED"], [216, 218, "HyperparameterName", "batch size"], [289, 291, "HyperparameterName", "batch size"], [606, 607, "MethodName", "BERT"], [664, 666, "HyperparameterName", "batch size"], [685, 687, "HyperparameterName", "batch size"]]}
{"text": "To validate the effectiveness and generality of our training strategy , we conduct further experiments on document retrieval using the MS MARCO document ranking dataset . This dataset contains 3.2 M web pages gathered from passages in the MS MARCO passage ranking dataset . Similar to the passage condition , we evaluate model effectiveness on two test sets of queries : Per official guidelines , we report different metrics for the two query sets : MRR@100 for MARCO Dev and NDCG@10 for TREC - DL ' 19 . Following the FirstP setting for document retrieval described in , we feed the first 512 tokens of each document for encoding , and start with the warmed - up checkpoint for our encoder 's parameters trained for passage retrieval ( using BM25 negatives , as described in Section 4.1.1 ) . The settings for fine - tuning our warmed - up encoder Table 4 : Document retrieval results using the FirstP approach . All our implemented models are labeled with a number and superscripts represent significant improvements over the labeled model ( paired t - test , p < 0.05 ) .", "entities": [[20, 22, "DatasetName", "MS MARCO"], [22, 24, "TaskName", "document ranking"], [38, 40, "DatasetName", "MS MARCO"], [40, 42, "TaskName", "passage ranking"], [80, 81, "MetricName", "NDCG@10"], [82, 83, "DatasetName", "TREC"], [125, 127, "TaskName", "passage retrieval"]]}
{"text": "MARCO Dev TREC - DL ' 19 MRR@100 NDCG@10 ANCE .368 .614 LTRe ( Zhan et al , 2020 ) - .634 SEED - Encoder ( Lu et al , 2021 Ranking effectiveness is reported in Table 4 . First , we observe that TCT - ColBERT ( our warmedup checkpoint ) performs far worse than other approaches to document retrieval using the FirstP method . This may be due to the fact that FirstP document retrieval is very different from passage retrieval , making zero - shot transfer ineffective . After applying HN training on both teacher and student models ( condition 2 ) , the ranking effectiveness increases significantly . In addition , we find that another iteration of training with an index refresh ( condition 3 ) further improves ranking effectiveness . To sum up , in the document ranking task , TCT - ColBERT yields competitive effectiveness with a one - time index refresh and outperforms other computationally expensive methods with one additional index refresh .", "entities": [[2, 3, "DatasetName", "TREC"], [8, 9, "MetricName", "NDCG@10"], [22, 23, "DatasetName", "SEED"], [81, 83, "TaskName", "passage retrieval"], [141, 143, "TaskName", "document ranking"]]}
{"text": "In our final set of experiments , we show that dense retrieval with single - vector representations can be integrated with results from sparse retrieval to further increase effectiveness . We illustrate the endto - end tradeoffs in terms of quality , time , and space of different dense - sparse hybrid combinations on the passage retrieval tasks . Many papers ( Luan et al , 2021 ; Gao et al , 2020b ; have demonstrated that sparse retrieval can complement dense retrieval via a simple linear combination of their scores . In our implementation , for each query q , we use sparse and dense techniques to retrieve the top - 1000 passages , D sp and D ds , with their relevance scores , \u03c6 sp ( q , p D sp ) and \u03c6 ds ( q , p D ds ) , respectively . Then , we compute the final relevance score for each retrieved passage \u03c6 ( q , p ) , where p D sp \u222a D ds , as follows : \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \u03b1 \u03c6 sp ( q , p ) + min p D ds \u03c6 ds ( q , p ) , if p / D ds \u03b1 min p Dsp \u03c6 sp ( q , p ) + \u03c6 ds ( q , p ) , if p / D sp \u03b1 \u03c6 sp ( q , p ) + \u03c6 ds ( q , p ) , otherwise . This technique is an approximation of a linear combination of sparse and dense retrieval scores . Specifically , if p / D sp ( or D ds ) , we instead use the minimum score of \u03c6 sp ( q , p D sp ) , or \u03c6 ds ( q , p D ds ) as a substitute . For the sparse and dense retrieval combinations , we tune the hyperparameter \u03b1 on 6000 randomly sampled queries from the MS MARCO training set . We conduct dense - sparse hybrid experiments with sparse retrieval ( BM25 ranking ) on the original passages ( denoted BM25 ) and on passages with docTTTTTquery document expansion ( Nogueira and Lin , 2019 ) ( denoted doc2query - T5 ) . To characterize end - to - end effectiveness and efficiency , we perform sparse retrieval with the Pyserini toolkit and dense retrieval with Faiss ( Johnson et al , 2017 ) , but implement the score combination in separate custom code . Table 5 shows passage retrieval results in terms of ranking effectiveness , query latency , and storage requirements ( i.e. , index size ) for each model and Table 6 reports the component latencies of our TCT - ColBERT dense - sparse hybrid . 3 The crossencoder reranker of Nogueira and Cho ( 2019 ) provides a point of reference for multi - stage reranking designs , which is effective but slow . Generally , dense retrieval methods ( whether single - vector or multi - vector ) are more effective but slower than sparse retrieval methods , which rely on bag - of - words querying using inverted indexes . Single - vector dense models also require more space than sparse retrieval methods . Moving ( Dai and Callan , 2020 ) .243 .551 55 4 doc2query - T5 ( Nogueira and Lin , 2019 ) .277 .551 64 14 Dense retrieval : single - vector TAS - B ( Hofst\u00e4tter et al , 2021 ) .343 .722 64 13 RocketQA ( Qu et al , 2020 ) .370 - 107 b 13 a TCT - ColBERT .344 .685 107 13 TCT - ColBERT ( w/ TCT HN+ ) .359 .719 107 13 Dense retrieval : multi - vector ME - BERT ( Luan et al , 2021 ) .334 .687 - 96 ColBERT ( Khattab and Zaharia , 2020 ) .360 - 458 154 Hybrid dense + sparse CLEAR ( Gao et al , 2020b ) .338 .699 - 17 a ME - HYBRID - E ( Luan et al , 2021 ) .343 .706 - 100 TAS - B + doc2query - T5 ( Hofst\u00e4tter et al , 2021 ) . ( Hofst\u00e4tter et al , 2021 ) .421 .759 12800 27 a RocketQA with reranking ( Qu et al , 2020 ) .439 - - 13 a a We estimate dense index size using 16 - bit floats ; for hybrid , we add the sizes of sparse and dense indexes . b We assume latency comparable to our settings . from single - vector to multi - vector dense models , we see that ColBERT exhibits higher effectiveness but is slower and requires much more storage . Finally , when integrated with sparse retrieval methods , TCT - ColBERT is able to beat a basic multi - stage reranking design ( BM25 + BERTlarge ) , but with much lower query latency , although at the cost of increased storage . Hybrid TCT - ColBERT ( w/ TCT HN+ ) + doc2query - T5 compares favorably with a recent advanced model , TAS - B + doc2query - T5 ( Hofst\u00e4tter et al , 2021 ) , which introduces topic - aware sampling and dual teachers , incorporating part of our TCT - ColBERT work . Nevertheless , even the best hybrid variant of TCT - ColBERT alone , without further reranking , remains quite some distance from RocketQA , the current state of the art ( with reranking using cross - encoders ) . This suggests that there remain relevance signals that require full attention interactions to exploit .", "entities": [[55, 57, "TaskName", "passage retrieval"], [186, 187, "HyperparameterName", "\u03b1"], [212, 213, "HyperparameterName", "\u03b1"], [237, 238, "HyperparameterName", "\u03b1"], [328, 329, "HyperparameterName", "\u03b1"], [336, 338, "DatasetName", "MS MARCO"], [381, 382, "MethodName", "T5"], [429, 431, "TaskName", "passage retrieval"], [565, 566, "MethodName", "T5"], [638, 639, "MethodName", "BERT"], [666, 667, "DatasetName", "CLEAR"], [701, 702, "MethodName", "T5"], [854, 855, "MethodName", "T5"], [869, 870, "MethodName", "T5"]]}
{"text": "A string language L over alphabet \u03a3 is strictly local ( SL ) iff there is some k N such that L is generated by a strictly k - local grammar G. Such a grammar consists of a finite set of k - grams , each one of which describes an illicit substring . More precisely , given a string w \u03a3 * , let\u0175 k : = k w k ( where , / \u03a3 ) and k - grams ( w ) : = { s | s is a substring of\u0175 k\u22121 of length k } . Then G generates string w iff k - grams ( w ) \u2229 G = . That is to say , G generates every string over \u03a3 that does not contain an illicit substring . Most phonological dependencies can be described in strictly local terms - see Heinz ( 2015 ) for numerous examples . Consider for instance the well - known process of word - final obstruent devoicing that forces voiced obstruents at the end of the word to be realized as voiceless : moroz [ maros ] ' frost ' in Russian , Bad [ bat ] ' bath ' in German ) . If one considers phonotactics rather than mappings from underlying representations to surface forms , this is tantamount to a ban against word - final voiced obstruents . Said ban , in turn , is captured by a strictly 2 - local grammar G that contains all bigrams of the form v , with v a voiced obstruent . The specification of SL grammars can be simplified by applying mappings . In the case at hand , one could define a function f that replaces every voiced obstruent by the designated symbol \u2666 so that the grammar G can be reduced to the single bigram \u2666 . One has to be careful , though . The SL languages are not closed under relabelings , in fact , every regular language is the image of a strictly 2local language under some relabeling . However , the directionality of the \u2666 - relabeling above is the opposite : first the relabeling is applied , and then the grammar filters out strings in the image of that relabeling . As long as the relabeling is a manyto - one map between alphabets ( and thus does not introduce distinctions that were n't already part of the original alphabet ) , this provably does not increase weak generative capacity for any of the formalisms discussed in this paper . We make use of such relabelings in the following sections in order to convert natural language patterns into more easily described formal languages . For morphotactics , though , this raises the issue how the size of the atomic units should be chosen . One could posit that morphology , just like phonology , treats every phonological segment as a symbol . In that case , stems and morphemes are strings of symbols . Alternatively , one may treat each morpheme , including stems , as an atomic symbol . This is an important decision when it comes to modeling the interactions of morphology and phonology such as phonologically conditioned allomorphy . Fortunately our results are independent of this choice , due to the productive nature of compounding . To better understand why different representations could in principle affect subregular complexity , note first that whether a stem is represented as a single , atomic symbol or as a sequence of phonological segments seems to determine if prefixes and suffixes might be separated by an unbounded amount of symbols . Consider a circumfix u - - v , where neither part of the affix may occur without the other . A concrete example is the nominalization circumfix ke - - an in Indonesian ( Mahdi , 2012 ; Sneddon , 1996 ) : ( 1 ) a. tingii high b. ke - NMN - tinggi high - an - NMN ' altitude ' If a stem is a single symbol x , then x and uxv are well - formed whereas ux and xv are not due to u - - v being a circumfix whose subparts can not occur in isolation . This generalization is easily captured by the strictly 3 - local grammar { xv , ux } . However , if stems are sequences of symbols , then the well - formed patterns are of the form x + or ux + v ( since the length of stems is in principle unbounded ) . The illict strings , on the other hand , are of the form x + v and ux + . But no strictly local grammar can generate the former patterns without also generating the latter . That is due to the strictly local languages being closed under suffix substitution closure . Suffix Substitution Closure Language L is SL iff there exists a k N such that for all strings u 1 , v 1 , u 2 , v 2 and any string x of length k \u2212 1 , if u 1 xv 1 , u 2 xv 2 L , then u 1 xv 2 L. If there is no upper bound on the length of stems , then we can infer from x k L and ux k v L that both x k v L and ux k L. It seems , then , that circumfixes are strictly local only if each stem is an atomic symbol . But this line of reasoning erroneously assumes that the circumfix can only apply to individual stems , which ignores the availability of compounding . Returning to Indonesian , we see that its nominalization marker is not restricted to single stems and can also apply to compounds . ( 2 ) a. maha big siswa pupil ' student ' b. ke - NMN - maha big siswa pupil - an - NMN ' student affairs ' Compounding is an unbounded process , so even if each stem is mapped to a single symbol x , one ends up with the same patterns as with the segmental mapping approach : x + and ux + v are well - formed , while ux + and x + v are ill - formed . Since the choice of representation does not affect the subregular complexity results , we opt for the segmental mapping , which does not require us to use compounding in all our natural language data points . The details of the segmental mapping are as follows : within a stem , all segments are replaced by some distinguished symbol . We choose x for this purpose . All morphemes , on the other hand , are replaced by single symbols . Symbols are chosen to maximize clarity of exposition , so that we sometimes assign each morpheme a unique symbol and sometimes map irrelevant morphemes to a randomly chosen filler symbol . For some linguistic phenomena we follow linguistic convention in assuming that the underlying representations contain additional distinguished symbols to mark the edges of the stem - this will be mentioned explicitly for all relevant cases . The preceding discussion yielded as a nice sideresult that circumfixation is not SL unless each part of the circumfix can also occur on its own . Few circumfixes display that kind of freedom , wherefore not all aspects of morphotactics are SL . However , large swaths of morphology still are , with a couple of examples from English given below : ( 3 ) a. una - do xx b. break xxxxx - able - b ( 4 ) a. punch xxxxx - ed - c b. put xxx - \u03b5 - c Any kind of affix that only consists of one part and whose distribution is determined within a locally bounded domain is part of strictly local morphotactics . Although we did not carry out any rigorous quantitative comparisons , we believe the majority of morphological dependencies to belong to this class .", "entities": [[1299, 1300, "HyperparameterName", "\u03b5"]]}
{"text": "As pointed out in the previous section , the Swahili pattern is n't too dissimilar from the phonological requirement that no word has more than one primary stress . However , the distribution of primary stress is more specific than that : every phonological word has exactly one primary stress . Ensuring the presence of at least one primary stress is beyond the capabilities of SP grammars - once again this holds because every subsequence of an ill - formed word without primary stress is also a subsequence of the well - formed counterpart with exactly one primary stress . A better model for primary stress assignment is furnished by tier - based strictly local ( TSL ; Heinz et al ( 2011 ) ) grammars , which also happen to be powerful enough for circumfixation . A TSL grammar is an SL grammar that operates over a tier , a specific substructure of the string . Given a tier - alphabet T \u2286 \u03a3 , let E T be a mapping that erases all symbols in a string that do not belong to T . First , E T ( \u03b5 ) = \u03b5 . Then for a \u03a3 and w \u03a3 * , E T ( aw ) : = a E T ( w ) if a T E T ( w ) otherwise The T - tier of a string w is its image under E T . A tier - based strictly k - local grammar G is a pair K , T where K is a strictly k - local grammar over tier - alphabet T . The grammar generates the language L ( G ) : = { w | E T ( w ) L ( K ) } . Note that every SL language is a TSL language with T = \u03a3. The distribution of primary stress is tier - based strictly 2 - local . Assuming that primary stress is indicated as some diacritic on symbols , the tieralphabet T contains all symbols with this diacritic . This is tantamount to projecting a tier that only contains segments with primary stress . The grammar then contains the bigram to block words with an empty primary stress tier , i.e. words that contain no primary stress . In addition , every bigram uv for u , v T is added to rule out words with more than one primary stress . The requirement of exactly one primary stress per word thus boils down to having exactly one segment on the primary stress tier , which is a strictly local dependency over that tier . The Swahili pattern from the previous section can also be analyzed as tier - based strictly local , and the same is true for circumfixation . For Swahili we project a tier that contains only the affix vyo , and we do not allow more than one segment on this tier . As a result , vyo occurs at most once per word . To ensure that vyo is a prefix whenever si is present , we furthermore postulate a marker # that indicates the edges of the stem . The projected tier then includes all instances of vyo , si and the marker # ( of which there are exactly two ) . On this tier , the 4 - gram si##vyo correctly excludes all ill - formed cases of vyo as a suffix , whereas vyo # # prevents vyo from occurring as a prefix in the absence of si . Adapting the ban against two instances of vyo to this slightly expanded tier is left as an exercise to the reader . In order to regulate the distribution of circumfixes such as u - - v , we have to project a tier that contains only those subparts u and v. If the affixes can never occur by themselves , then we block v and u . Removing one of those two bigrams creates asymmetric cases where one of the affixesbut not the other - is sometimes allowed to be present by itself . We also add uu and vv to block strings where the prefix parts outnumber or are outnumbered by the suffix parts of the circumfix . Note that this has the side effect of also prohibiting unbounded circumfixation , a point we return to in Sec . 3.2 . At this point , we can safely say that natural language morphotactics is at least TSL ( barring the discovery of any intermediate classes between SL and TSL , or SP and TSL ) . SL is sufficiently powerful for large parts of morphology , but any kind of dependency that involves both prefixes and suffixes is likely not SL . Some patterns that are not SL are SP , but these also turn out to be TSL . To the best of our knowledge , there are no morphological dependencies that are SP but not TSL ( even though the two language classes are incomparable ) . We thus put forward the following conjecture : Subregular Morphotactics All morphotactic dependencies are tier - based strictly local . As any universal claim about the real world , our conjecture can not be proved conclusively - the fact that no counterexamples have been encountered does not guarantee that counterexamples will never be encountered . But there are additional reasons to consider TSL a better fit for morphotactics than one of the more powerful classes . Moving beyond TSL in the subregular hierarchy would take us into the class of star - free languages , which are equivalent to the string sets definable in first - order logic with the transitive closure of the successor relation . As mentioned before , every language that is generated by a tier - based strictly k - local grammar can be identified in the limited from positive text , provided the learner knows the value of k. The class of star - free languages , on the other hand , is not learnable in the limit from positive text . It also makes largely incorrect typological predictions : Unlike TSL , the class of starfree languages is closed under union and relative complement . But the union or relative complement of two morphotactic systems attested in natural languages rarely yields linguistically plausible morphotactics . Similarly , it is trivial to write firstorder formulas for highly unnatural patterns , e.g. that every word containing two instances of a but less than three bs must contain no substring of the form cd + c. These points show that moving from TSL to star - free means losing essential properties of natural language morphotactics . Future work may of course identify more adequate classes in the vicinity of TSL . Given our current , more limited knowledge of the subregular hierarchy , however , the strongest empirically defensible stance is that tier - based strict locality is both sufficient and necessary for natural language morphotactics .", "entities": [[191, 192, "HyperparameterName", "\u03b5"], [194, 195, "HyperparameterName", "\u03b5"]]}
{"text": "We introduce a new language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers . Unlike recent language representation models ( Peters et al , 2018a ; Radford et al , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . As a result , the pre - trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models for a wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications . BERT is conceptually simple and empirically powerful . It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQuAD v1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQuAD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) .", "entities": [[8, 9, "MethodName", "BERT"], [38, 39, "MethodName", "BERT"], [70, 71, "MethodName", "BERT"], [100, 102, "TaskName", "question answering"], [112, 113, "MethodName", "BERT"], [141, 142, "DatasetName", "GLUE"], [154, 155, "DatasetName", "MultiNLI"], [155, 156, "MetricName", "accuracy"], [166, 167, "DatasetName", "SQuAD"], [168, 170, "TaskName", "question answering"], [171, 172, "MetricName", "F1"], [181, 182, "DatasetName", "SQuAD"], [184, 185, "MetricName", "F1"]]}
{"text": "Language model pre - training has been shown to be effective for improving many natural language processing tasks ( Dai and Le , 2015 ; Peters et al , 2018a ; Radford et al , 2018 ; Howard and Ruder , 2018 ) . These include sentence - level tasks such as natural language inference ( Bowman et al , 2015 ; Williams et al , 2018 ) and paraphrasing ( Dolan and Brockett , 2005 ) , which aim to predict the relationships between sentences by analyzing them holistically , as well as token - level tasks such as named entity recognition and question answering , where models are required to produce fine - grained output at the token level ( Tjong Kim Sang and De Meulder , 2003 ; Rajpurkar et al , 2016 ) . There are two existing strategies for applying pre - trained language representations to downstream tasks : feature - based and fine - tuning . The feature - based approach , such as ELMo ( Peters et al , 2018a ) , uses task - specific architectures that include the pre - trained representations as additional features . The fine - tuning approach , such as the Generative Pre - trained Transformer ( OpenAI GPT ) ( Radford et al , 2018 ) , introduces minimal task - specific parameters , and is trained on the downstream tasks by simply fine - tuning all pretrained parameters . The two approaches share the same objective function during pre - training , where they use unidirectional language models to learn general language representations . We argue that current techniques restrict the power of the pre - trained representations , especially for the fine - tuning approaches . The major limitation is that standard language models are unidirectional , and this limits the choice of architectures that can be used during pre - training . For example , in OpenAI GPT , the authors use a left - toright architecture , where every token can only attend to previous tokens in the self - attention layers of the Transformer ( Vaswani et al , 2017 ) . Such restrictions are sub - optimal for sentence - level tasks , and could be very harmful when applying finetuning based approaches to token - level tasks such as question answering , where it is crucial to incorporate context from both directions . In this paper , we improve the fine - tuning based approaches by proposing BERT : Bidirectional Encoder Representations from Transformers . BERT alleviates the previously mentioned unidirectionality constraint by using a \" masked language model \" ( MLM ) pre - training objective , inspired by the Cloze task ( Taylor , 1953 ) . The masked language model randomly masks some of the tokens from the input , and the objective is to predict the original vocabulary i d of the masked word based only on its context . Unlike left - toright language model pre - training , the MLM objective enables the representation to fuse the left and the right context , which allows us to pretrain a deep bidirectional Transformer . In addition to the masked language model , we also use a \" next sentence prediction \" task that jointly pretrains text - pair representations . The contributions of our paper are as follows : We demonstrate the importance of bidirectional pre - training for language representations . Unlike Radford et al ( 2018 ) , which uses unidirectional language models for pre - training , BERT uses masked language models to enable pretrained deep bidirectional representations . This is also in contrast to Peters et al ( 2018a ) , which uses a shallow concatenation of independently trained left - to - right and right - to - left LMs . We show that pre - trained representations reduce the need for many heavily - engineered taskspecific architectures . BERT is the first finetuning based representation model that achieves state - of - the - art performance on a large suite of sentence - level and token - level tasks , outperforming many task - specific architectures . BERT advances the state of the art for eleven NLP tasks . The code and pre - trained models are available at https://github.com/ google - research / bert .", "entities": [[52, 55, "TaskName", "natural language inference"], [100, 103, "TaskName", "named entity recognition"], [104, 106, "TaskName", "question answering"], [170, 171, "MethodName", "ELMo"], [208, 209, "MethodName", "Transformer"], [211, 212, "MethodName", "GPT"], [324, 325, "MethodName", "GPT"], [348, 350, "HyperparameterName", "attention layers"], [352, 353, "MethodName", "Transformer"], [390, 392, "TaskName", "question answering"], [418, 419, "MethodName", "BERT"], [426, 427, "MethodName", "BERT"], [442, 443, "DatasetName", "MLM"], [506, 507, "DatasetName", "MLM"], [528, 529, "MethodName", "Transformer"], [596, 597, "MethodName", "BERT"], [660, 661, "MethodName", "BERT"], [699, 700, "MethodName", "BERT"]]}
{"text": "We introduce BERT and its detailed implementation in this section . There are two steps in our framework : pre - training and fine - tuning . During pre - training , the model is trained on unlabeled data over different pre - training tasks . For finetuning , the BERT model is first initialized with the pre - trained parameters , and all of the parameters are fine - tuned using labeled data from the downstream tasks . Each downstream task has separate fine - tuned models , even though they are initialized with the same pre - trained parameters . The question - answering example in Figure 1 will serve as a running example for this section . A distinctive feature of BERT is its unified architecture across different tasks . There is mini - mal difference between the pre - trained architecture and the final downstream architecture . Model Architecture BERT 's model architecture is a multi - layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al ( 2017 ) and released in the tensor2tensor library . 1 Because the use of Transformers has become common and our implementation is almost identical to the original , we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al ( 2017 ) as well as excellent guides such as \" The Annotated Transformer . \" 2 In this work , we denote the number of layers ( i.e. , Transformer blocks ) as L , the hidden size as H , and the number of self - attention heads as A. 3 We primarily report results on two model sizes : BERT BASE ( L=12 , H=768 , A=12 , Total Param - eters=110 M ) and BERT LARGE ( L=24 , H=1024 , A=16 , Total Parameters=340 M ) . BERT BASE was chosen to have the same model size as OpenAI GPT for comparison purposes . Critically , however , the BERT Transformer uses bidirectional self - attention , while the GPT Transformer uses constrained self - attention where every token can only attend to context to its left . 4 Input / Output Representations To make BERT handle a variety of down - stream tasks , our input representation is able to unambiguously represent both a single sentence and a pair of sentences ( e.g. , Question , Answer ) in one token sequence . Throughout this work , a \" sentence \" can be an arbitrary span of contiguous text , rather than an actual linguistic sentence . A \" sequence \" refers to the input token sequence to BERT , which may be a single sentence or two sentences packed together . We use WordPiece embeddings ( Wu et al , 2016 ) with a 30 , 000 token vocabulary . The first token of every sequence is always a special classification token ( [ CLS ] ) . The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks . Sentence pairs are packed together into a single sequence . We differentiate the sentences in two ways . First , we separate them with a special token ( [ SEP ] ) . Second , we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1 , we denote input embedding as E , the final hidden vector of the special [ CLS ] token as C R H , and the final hidden vector for the i th input token as T i R H . For a given token , its input representation is constructed by summing the corresponding token , segment , and position embeddings . A visualization of this construction can be seen in Figure 2 .", "entities": [[2, 3, "MethodName", "BERT"], [50, 51, "MethodName", "BERT"], [124, 125, "MethodName", "BERT"], [153, 154, "MethodName", "BERT"], [163, 164, "MethodName", "Transformer"], [235, 236, "MethodName", "Transformer"], [246, 249, "HyperparameterName", "number of layers"], [252, 253, "MethodName", "Transformer"], [284, 285, "MethodName", "BERT"], [285, 286, "MethodName", "BASE"], [300, 301, "MethodName", "BERT"], [314, 315, "MethodName", "BERT"], [315, 316, "MethodName", "BASE"], [326, 327, "MethodName", "GPT"], [336, 337, "MethodName", "BERT"], [337, 338, "MethodName", "Transformer"], [346, 347, "MethodName", "GPT"], [347, 348, "MethodName", "Transformer"], [372, 373, "MethodName", "BERT"], [446, 447, "MethodName", "BERT"], [462, 463, "MethodName", "WordPiece"]]}
{"text": "Unlike Peters et al ( 2018a ) and Radford et al ( 2018 ) , we do not use traditional left - to - right or right - to - left language models to pre - train BERT . Instead , we pre - train BERT using two unsupervised tasks , described in this section . This step is presented in the left part of Figure 1 . Task # 1 : Masked LM Intuitively , it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left - to - right model or the shallow concatenation of a left - toright and a right - to - left model . Unfortunately , standard conditional language models can only be trained left - to - right or right - to - left , since bidirectional conditioning would allow each word to indirectly \" see itself \" , and the model could trivially predict the target word in a multi - layered context . former is often referred to as a \" Transformer encoder \" while the left - context - only version is referred to as a \" Transformer decoder \" since it can be used for text generation . In order to train a deep bidirectional representation , we simply mask some percentage of the input tokens at random , and then predict those masked tokens . We refer to this procedure as a \" masked LM \" ( MLM ) , although it is often referred to as a Cloze task in the literature ( Taylor , 1953 ) . In this case , the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary , as in a standard LM . In all of our experiments , we mask 15 % of all WordPiece tokens in each sequence at random . In contrast to denoising auto - encoders ( Vincent et al , 2008 ) , we only predict the masked words rather than reconstructing the entire input . Although this allows us to obtain a bidirectional pre - trained model , a downside is that we are creating a mismatch between pre - training and fine - tuning , since the [ MASK ] token does not appear during fine - tuning . To mitigate this , we do not always replace \" masked \" words with the actual [ MASK ] token . The training data generator chooses 15 % of the token positions at random for prediction . If the i - th token is chosen , we replace the i - th token with ( 1 ) the [ MASK ] token 80 % of the time ( 2 ) a random token 10 % of the time ( 3 ) the unchanged i - th token 10 % of the time . Then , T i will be used to predict the original token with cross entropy loss . We compare variations of this procedure in Appendix C.2 . Task # 2 : Next Sentence Prediction ( NSP ) Many important downstream tasks such as Question Answering ( QA ) and Natural Language Inference ( NLI ) are based on understanding the relationship between two sentences , which is not directly captured by language modeling . In order to train a model that understands sentence relationships , we pre - train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus . Specifically , when choosing the sentences A and B for each pretraining example , 50 % of the time B is the actual next sentence that follows A ( labeled as IsNext ) , and 50 % of the time it is a random sentence from the corpus ( labeled as NotNext ) . As we show in Figure 1 , C is used for next sentence prediction ( NSP ) . 5 Despite its simplicity , we demonstrate in Section 5.1 that pre - training towards this task is very beneficial to both QA and NLI . 6 The NSP task is closely related to representationlearning objectives used in Jernite et al ( 2017 ) and Logeswaran and Lee ( 2018 ) . However , in prior work , only sentence embeddings are transferred to down - stream tasks , where BERT transfers all parameters to initialize end - task model parameters .", "entities": [[37, 38, "MethodName", "BERT"], [45, 46, "MethodName", "BERT"], [177, 178, "MethodName", "Transformer"], [194, 196, "MethodName", "Transformer decoder"], [203, 205, "TaskName", "text generation"], [246, 247, "DatasetName", "MLM"], [286, 287, "MethodName", "softmax"], [309, 310, "MethodName", "WordPiece"], [320, 321, "TaskName", "denoising"], [498, 499, "MetricName", "loss"], [526, 528, "TaskName", "Question Answering"], [532, 535, "TaskName", "Natural Language Inference"], [720, 722, "TaskName", "sentence embeddings"], [731, 732, "MethodName", "BERT"]]}
{"text": "The General Language Understanding Evaluation ( GLUE ) benchmark ( Wang et al , 2018a ) is a collection of diverse natural language understanding tasks . Detailed descriptions of GLUE datasets are included in Appendix B.1 . To fine - tune on GLUE , we represent the input sequence ( for single sentence or sentence pairs ) as described in Section 3 , and use the final hidden vector C R H corresponding to the first input token ( [ CLS ] ) as the aggregate representation . The only new parameters introduced during fine - tuning are classification layer weights W R K\u00d7H , where K is the number of labels . We compute a standard classification loss with C and W , i.e. , log ( softmax ( CW T ) ) . We use a batch size of 32 and fine - tune for 3 epochs over the data for all GLUE tasks . For each task , we selected the best fine - tuning learning rate ( among 5e - 5 , 4e - 5 , 3e - 5 , and 2e - 5 ) on the Dev set . Additionally , for BERT LARGE we found that finetuning was sometimes unstable on small datasets , so we ran several random restarts and selected the best model on the Dev set . With random restarts , we use the same pre - trained checkpoint but perform different fine - tuning data shuffling and classifier layer initialization . 9 Results are presented in Table 1 . Both BERT BASE and BERT LARGE outperform all systems on all tasks by a substantial margin , obtaining 4.5 % and 7.0 % respective average accuracy improvement over the prior state of the art . Note that BERT BASE and OpenAI GPT are nearly identical in terms of model architecture apart from the attention masking . For the largest and most widely reported GLUE task , MNLI , BERT obtains a 4.6 % absolute accuracy improvement . On the official GLUE leaderboard 10 , BERT LARGE obtains a score of 80.5 , compared to OpenAI GPT , which obtains 72.8 as of the date of writing . We find that BERT LARGE significantly outperforms BERT BASE across all tasks , especially those with very little training data . The effect of model size is explored more thoroughly in Section 5.2 .", "entities": [[1, 2, "DatasetName", "General"], [6, 7, "DatasetName", "GLUE"], [21, 24, "TaskName", "natural language understanding"], [29, 30, "DatasetName", "GLUE"], [42, 43, "DatasetName", "GLUE"], [118, 119, "MetricName", "loss"], [128, 129, "MethodName", "softmax"], [138, 140, "HyperparameterName", "batch size"], [154, 155, "DatasetName", "GLUE"], [168, 170, "HyperparameterName", "learning rate"], [197, 198, "MethodName", "BERT"], [260, 261, "MethodName", "BERT"], [261, 262, "MethodName", "BASE"], [263, 264, "MethodName", "BERT"], [283, 285, "MetricName", "average accuracy"], [296, 297, "MethodName", "BERT"], [297, 298, "MethodName", "BASE"], [300, 301, "MethodName", "GPT"], [322, 323, "DatasetName", "GLUE"], [325, 326, "DatasetName", "MNLI"], [327, 328, "MethodName", "BERT"], [333, 334, "MetricName", "accuracy"], [339, 340, "DatasetName", "GLUE"], [343, 344, "MethodName", "BERT"], [354, 355, "MethodName", "GPT"], [369, 370, "MethodName", "BERT"], [373, 374, "MethodName", "BERT"], [374, 375, "MethodName", "BASE"]]}
{"text": "The Stanford Question Answering Dataset ( SQuAD v1.1 ) is a collection of 100k crowdsourced question / answer pairs ( Rajpurkar et al , 2016 ) . Given a question and a passage from 9 The GLUE data set distribution does not include the Test labels , and we only made a single GLUE evaluation server submission for each of BERTBASE and BERTLARGE . 10 https://gluebenchmark.com/leaderboard Wikipedia containing the answer , the task is to predict the answer text span in the passage . As shown in Figure 1 , in the question answering task , we represent the input question and passage as a single packed sequence , with the question using the A embedding and the passage using the B embedding . We only introduce a start vector S R H and an end vector E R H during fine - tuning . The probability of word i being the start of the answer span is computed as a dot product between T i and S followed by a softmax over all of the words in the paragraph : P i = e S T i j e S T j . The analogous formula is used for the end of the answer span . The score of a candidate span from position i to position j is defined as S T i + E T j , and the maximum scoring span where j \u2265 i is used as a prediction . The training objective is the sum of the log - likelihoods of the correct start and end positions . We fine - tune for 3 epochs with a learning rate of 5e - 5 and a batch size of 32 . Table 2 shows top leaderboard entries as well as results from top published systems ( Seo et al , 2017 ; Clark and Gardner , 2018 ; Peters et al , 2018a ; Hu et al , 2018 ) . The top results from the SQuAD leaderboard do not have up - to - date public system descriptions available , 11 and are allowed to use any public data when training their systems . We therefore use modest data augmentation in our system by first fine - tuning on TriviaQA ( Joshi et al , 2017 ) befor fine - tuning on SQuAD . Our best performing system outperforms the top leaderboard system by +1.5 F1 in ensembling and +1.3 F1 as a single system . In fact , our single BERT model outperforms the top ensemble system in terms of F1 score . Without TriviaQA fine - tuning data , we only lose 0.1 - 0.4 F1 , still outperforming all existing systems by a wide margin . 12", "entities": [[0, 5, "DatasetName", "The Stanford Question Answering Dataset"], [6, 7, "DatasetName", "SQuAD"], [36, 37, "DatasetName", "GLUE"], [53, 54, "DatasetName", "GLUE"], [92, 94, "TaskName", "question answering"], [171, 172, "MethodName", "softmax"], [273, 275, "HyperparameterName", "learning rate"], [281, 283, "HyperparameterName", "batch size"], [331, 332, "DatasetName", "SQuAD"], [364, 366, "TaskName", "data augmentation"], [375, 376, "DatasetName", "TriviaQA"], [388, 389, "DatasetName", "SQuAD"], [401, 402, "MetricName", "F1"], [406, 407, "MetricName", "F1"], [417, 418, "MethodName", "BERT"], [427, 429, "MetricName", "F1 score"], [431, 432, "DatasetName", "TriviaQA"], [443, 444, "MetricName", "F1"]]}
{"text": "The SQuAD 2.0 task extends the SQuAD 1.1 problem definition by allowing for the possibility that no short answer exists in the provided paragraph , making the problem more realistic . We use a simple approach to extend the SQuAD v1.1 BERT model for this task . We treat questions that do not have an answer as having an answer span with start and end at the [ CLS ] token . The probability space for the start and end answer span positions is extended to include the position of the [ CLS ] token . For prediction , we compare the score of the no - answer span : s null = S C + E C to the score of the best non - null span 12 The TriviaQA data we used consists of paragraphs from TriviaQA - Wiki formed of the first 400 tokens in documents , that contain at least one of the provided possible answers . s i , j = max j\u2265i S T i + E T j . We predict a non - null answer when\u015d i , j > s null + \u03c4 , where the threshold \u03c4 is selected on the dev set to maximize F1 . We did not use TriviaQA data for this model . We fine - tuned for 2 epochs with a learning rate of 5e - 5 and a batch size of 48 . The results compared to prior leaderboard entries and top published work ( Sun et al , 2018 ; Wang et al , 2018b ) are shown in Table 3 , excluding systems that use BERT as one of their components . We observe a +5.1 F1 improvement over the previous best system .", "entities": [[1, 2, "DatasetName", "SQuAD"], [6, 7, "DatasetName", "SQuAD"], [39, 40, "DatasetName", "SQuAD"], [41, 42, "MethodName", "BERT"], [130, 131, "DatasetName", "TriviaQA"], [138, 139, "DatasetName", "TriviaQA"], [205, 206, "MetricName", "F1"], [211, 212, "DatasetName", "TriviaQA"], [226, 228, "HyperparameterName", "learning rate"], [234, 236, "HyperparameterName", "batch size"], [273, 274, "MethodName", "BERT"], [284, 285, "MetricName", "F1"]]}
{"text": "The Situations With Adversarial Generations ( SWAG ) dataset contains 113k sentence - pair completion examples that evaluate grounded commonsense inference ( Zellers et al , 2018 ) . Given a sentence , the task is to choose the most plausible continuation among four choices . When fine - tuning on the SWAG dataset , we construct four input sequences , each containing the concatenation of the given sentence ( sentence A ) and a possible continuation ( sentence B ) . The only task - specific parameters introduced is a vector whose dot product with the [ CLS ] token representation C denotes a score for each choice which is normalized with a softmax layer . We fine - tune the model for 3 epochs with a learning rate of 2e - 5 and a batch size of 16 . Results are presented in Table 4 . BERT LARGE outperforms the authors ' baseline ESIM+ELMo system by +27.1 % and OpenAI GPT by 8.3 % .", "entities": [[6, 7, "DatasetName", "SWAG"], [52, 53, "DatasetName", "SWAG"], [114, 115, "MethodName", "softmax"], [128, 130, "HyperparameterName", "learning rate"], [136, 138, "HyperparameterName", "batch size"], [148, 149, "MethodName", "BERT"], [162, 163, "MethodName", "GPT"]]}
{"text": "In this section , we explore the effect of model size on fine - tuning task accuracy . We trained a number of BERT models with a differing number of layers , hidden units , and attention heads , while otherwise using the same hyperparameters and training procedure as described previously . Results on selected GLUE tasks are shown in Table 6 . In this table , we report the average Dev Set accuracy from 5 random restarts of fine - tuning . We can see that larger models lead to a strict accuracy improvement across all four datasets , even for MRPC which only has 3 , 600 labeled training examples , and is substantially different from the pre - training tasks . It is also perhaps surprising that we are able to achieve such significant improvements on top of models which are already quite large relative to the existing literature . For example , the largest Transformer explored in Vaswani et al ( 2017 ) is ( L=6 , H=1024 , A=16 ) with 100 M parameters for the encoder , and the largest Transformer we have found in the literature is ( L=64 , H=512 , A=2 ) with 235 M parameters ( Al - Rfou et al , 2018 ) . By contrast , BERT BASE contains 110 M parameters and BERT LARGE contains 340 M parameters . It has long been known that increasing the model size will lead to continual improvements on large - scale tasks such as machine translation and language modeling , which is demonstrated by the LM perplexity of held - out training data shown in Table 6 . However , we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks , provided that the model has been sufficiently pre - trained . Peters et al ( 2018b ) presented mixed results on the downstream task impact of increasing the pre - trained bi - LM size from two to four layers and Melamud et al ( 2016 ) mentioned in passing that increasing hidden dimension size from 200 to 600 helped , but increasing further to 1 , 000 did not bring further improvements . Both of these prior works used a featurebased approach - we hypothesize that when the model is fine - tuned directly on the downstream tasks and uses only a very small number of randomly initialized additional parameters , the taskspecific models can benefit from the larger , more expressive pre - trained representations even when downstream task data is very small .", "entities": [[16, 17, "MetricName", "accuracy"], [23, 24, "MethodName", "BERT"], [28, 31, "HyperparameterName", "number of layers"], [55, 56, "DatasetName", "GLUE"], [73, 74, "MetricName", "accuracy"], [93, 94, "MetricName", "accuracy"], [102, 103, "DatasetName", "MRPC"], [158, 159, "MethodName", "Transformer"], [186, 187, "MethodName", "Transformer"], [218, 219, "MethodName", "BERT"], [219, 220, "MethodName", "BASE"], [225, 226, "MethodName", "BERT"], [254, 256, "TaskName", "machine translation"], [266, 267, "MetricName", "perplexity"], [360, 363, "HyperparameterName", "hidden dimension size"]]}
{"text": "All of the BERT results presented so far have used the fine - tuning approach , where a simple classification layer is added to the pre - trained model , and all parameters are jointly fine - tuned on a downstream task . However , the feature - based approach , where fixed features are extracted from the pretrained model , has certain advantages . First , not all tasks can be easily represented by a Transformer encoder architecture , and therefore require a task - specific model architecture to be added . Second , there are major computational benefits to pre - compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation . In this section , we compare the two approaches by applying BERT to the CoNLL - 2003 Named Entity Recognition ( NER ) task ( Tjong Kim Sang and De Meulder , 2003 ) . In the input to BERT , we use a case - preserving WordPiece model , and we include the maximal document context provided by the data . Following standard practice , we formulate this as a tagging task but do not use a CRF layer in the output . We use the representation of the first sub - token as the input to the token - level classifier over the NER label set . To ablate the fine - tuning approach , we apply the feature - based approach by extracting the activations from one or more layers without fine - tuning any parameters of BERT . These contextual embeddings are used as input to a randomly initialized two - layer 768 - dimensional BiLSTM before the classification layer . Results are presented in Table 7 . BERT LARGE performs competitively with state - of - the - art methods . The best performing method concatenates the token representations from the top four hidden layers of the pre - trained Transformer , which is only 0.3 F1 behind fine - tuning the entire model . This demonstrates that BERT is effective for both finetuning and feature - based approaches .", "entities": [[3, 4, "MethodName", "BERT"], [76, 77, "MethodName", "Transformer"], [137, 138, "MethodName", "BERT"], [143, 146, "TaskName", "Named Entity Recognition"], [147, 148, "TaskName", "NER"], [165, 166, "MethodName", "BERT"], [173, 174, "MethodName", "WordPiece"], [204, 205, "MethodName", "CRF"], [231, 232, "TaskName", "NER"], [266, 267, "MethodName", "BERT"], [285, 286, "MethodName", "BiLSTM"], [298, 299, "MethodName", "BERT"], [331, 332, "MethodName", "Transformer"], [337, 338, "MetricName", "F1"], [349, 350, "MethodName", "BERT"]]}
{"text": "Recent empirical improvements due to transfer learning with language models have demonstrated that rich , unsupervised pre - training is an integral part of many language understanding systems . In particular , these results enable even low - resource tasks to benefit from deep unidirectional architectures . Our major contribution is further generalizing these findings to deep bidirectional architectures , allowing the same pre - trained model to successfully tackle a broad set of NLP tasks . To generate each training input sequence , we sample two spans of text from the corpus , which we refer to as \" sentences \" even though they are typically much longer than single sentences ( but can be shorter also ) . The first sentence receives the A embedding and the second receives the B embedding . 50 % of the time B is the actual next sentence that follows A and 50 % of the time it is a random sentence , which is done for the \" next sentence prediction \" task . They are sampled such that the combined length is \u2264 512 tokens . The LM masking is applied after WordPiece tokenization with a uniform masking rate of 15 % , and no special consideration given to partial word pieces . We train with batch size of 256 sequences ( 256 sequences * 512 tokens = 128 , 000 tokens / batch ) for 1 , 000 , 000 steps , which is approximately 40 epochs over the 3.3 billion word corpus . We use Adam with learning rate of 1e - 4 , \u03b2 1 = 0.9 , \u03b2 2 = 0.999 , L2 weight decay of 0.01 , learning rate warmup over the first 10 , 000 steps , and linear decay of the learning rate . We use a dropout probability of 0.1 on all layers . We use a gelu activation ( Hendrycks and Gimpel , 2016 ) rather than the standard relu , following OpenAI GPT . The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood . Training of BERT BASE was performed on 4 Cloud TPUs in Pod configuration ( 16 TPU chips total ) . 13 Training of BERT LARGE was performed on 16 Cloud TPUs ( 64 TPU chips total ) . Each pretraining took 4 days to complete . Longer sequences are disproportionately expensive because attention is quadratic to the sequence length . To speed up pretraing in our experiments , we pre - train the model with sequence length of 128 for 90 % of the steps . Then , we train the rest 10 % of the steps of sequence of 512 to learn the positional embeddings .", "entities": [[5, 7, "TaskName", "transfer learning"], [15, 19, "TaskName", "unsupervised pre - training"], [192, 193, "MethodName", "WordPiece"], [216, 218, "HyperparameterName", "batch size"], [257, 258, "MethodName", "Adam"], [259, 261, "HyperparameterName", "learning rate"], [266, 267, "HyperparameterName", "\u03b2"], [271, 272, "HyperparameterName", "\u03b2"], [277, 279, "MethodName", "weight decay"], [282, 284, "HyperparameterName", "learning rate"], [298, 300, "HyperparameterName", "learning rate"], [315, 316, "MethodName", "gelu"], [328, 329, "MethodName", "relu"], [332, 333, "MethodName", "GPT"], [336, 337, "MetricName", "loss"], [356, 357, "MethodName", "BERT"], [357, 358, "MethodName", "BASE"], [377, 378, "MethodName", "BERT"]]}
{"text": "For fine - tuning , most model hyperparameters are the same as in pre - training , with the exception of the batch size , learning rate , and number of training epochs . The dropout probability was always kept at 0.1 . The optimal hyperparameter values are task - specific , but we found the following range of possible values to work well across all tasks : Batch size : 16 , 32 We also observed that large data sets ( e.g. , 100k+ labeled training examples ) were far less sensitive to hyperparameter choice than small data sets . Fine - tuning is typically very fast , so it is reasonable to simply run an exhaustive search over the above parameters and choose the model that performs best on the development set . A.4 Comparison of BERT , ELMo , and OpenAI GPT Here we studies the differences in recent popular representation learning models including ELMo , OpenAI GPT and BERT . The comparisons between the model architectures are shown visually in Figure 3 . Note that in addition to the architecture differences , BERT and OpenAI GPT are finetuning approaches , while ELMo is a feature - based approach . The most comparable existing pre - training method to BERT is OpenAI GPT , which trains a left - to - right Transformer LM on a large text corpus . In fact , many of the design decisions in BERT were intentionally made to make it as close to GPT as possible so that the two methods could be minimally compared . The core argument of this work is that the bi - directionality and the two pretraining tasks presented in Section 3.1 account for the majority of the empirical improvements , but we do note that there are several other differences between how BERT and GPT were trained : GPT is trained on the BooksCorpus ( 800 M words ) ; BERT is trained on the BooksCorpus ( 800 M words ) and Wikipedia ( 2 , 500 M words ) . GPT uses a sentence separator ( GPT was trained for 1 M steps with a batch size of 32 , 000 words ; BERT was trained for 1 M steps with a batch size of 128 , 000 words . GPT used the same learning rate of 5e - 5 for all fine - tuning experiments ; BERT chooses a task - specific fine - tuning learning rate which performs the best on the development set . To isolate the effect of these differences , we perform ablation experiments in Section 5.1 which demonstrate that the majority of the improvements are in fact coming from the two pre - training tasks and the bidirectionality they enable .", "entities": [[22, 24, "HyperparameterName", "batch size"], [25, 27, "HyperparameterName", "learning rate"], [68, 70, "HyperparameterName", "Batch size"], [138, 139, "MethodName", "BERT"], [140, 141, "MethodName", "ELMo"], [144, 145, "MethodName", "GPT"], [153, 155, "TaskName", "representation learning"], [157, 158, "MethodName", "ELMo"], [160, 161, "MethodName", "GPT"], [162, 163, "MethodName", "BERT"], [186, 187, "MethodName", "BERT"], [189, 190, "MethodName", "GPT"], [195, 196, "MethodName", "ELMo"], [212, 213, "MethodName", "BERT"], [215, 216, "MethodName", "GPT"], [225, 226, "MethodName", "Transformer"], [242, 243, "MethodName", "BERT"], [252, 253, "MethodName", "GPT"], [307, 308, "MethodName", "BERT"], [309, 310, "MethodName", "GPT"], [313, 314, "MethodName", "GPT"], [325, 326, "MethodName", "BERT"], [346, 347, "MethodName", "GPT"], [352, 353, "MethodName", "GPT"], [361, 363, "HyperparameterName", "batch size"], [369, 370, "MethodName", "BERT"], [378, 380, "HyperparameterName", "batch size"], [386, 387, "MethodName", "GPT"], [390, 392, "HyperparameterName", "learning rate"], [403, 404, "MethodName", "BERT"], [412, 414, "HyperparameterName", "learning rate"]]}
{"text": "The illustration of fine - tuning BERT on different tasks can be seen in Figure 4 . Our task - specific models are formed by incorporating BERT with one additional output layer , so a minimal number of parameters need to be learned from scratch . Among the tasks , The GLUE benchmark includes the following datasets , the descriptions of which were originally summarized in Wang et al ( 2018a ) : MNLI Multi - Genre Natural Language Inference is a large - scale , crowdsourced entailment classification task ( Williams et al , 2018 ) . Given a pair of sentences , the goal is to predict whether the second sentence is an entailment , contradiction , or neutral with respect to the first one . QQP Quora Question Pairs is a binary classification task where the goal is to determine if two questions asked on Quora are semantically equivalent . QNLI Question Natural Language Inference is a version of the Stanford Question Answering Dataset ( Rajpurkar et al , 2016 ) which has been converted to a binary classification task ( Wang et al , 2018a ... ... with human annotations of their sentiment ( Socher et al , 2013 ) . CoLA The Corpus of Linguistic Acceptability is a binary single - sentence classification task , where the goal is to predict whether an English sentence is linguistically \" acceptable \" or not ( Warstadt et al , 2018 ) .", "entities": [[6, 7, "MethodName", "BERT"], [26, 27, "MethodName", "BERT"], [36, 39, "HyperparameterName", "number of parameters"], [51, 52, "DatasetName", "GLUE"], [73, 74, "DatasetName", "MNLI"], [77, 80, "TaskName", "Natural Language Inference"], [128, 129, "DatasetName", "QQP"], [129, 132, "DatasetName", "Quora Question Pairs"], [153, 154, "DatasetName", "QNLI"], [155, 158, "TaskName", "Natural Language Inference"], [162, 167, "DatasetName", "the Stanford Question Answering Dataset"], [205, 206, "DatasetName", "CoLA"], [209, 211, "TaskName", "Linguistic Acceptability"], [216, 218, "TaskName", "sentence classification"]]}
{"text": "The Semantic Textual Similarity Benchmark is a collection of sentence pairs drawn from news headlines and other sources ( Cer et al , 2017 ) . They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning . MRPC Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources , with human annotations for whether the sentences in the pair are semantically equivalent ( Dolan and Brockett , 2005 ) . RTE Recognizing Textual Entailment is a binary entailment task similar to MNLI , but with much less training data ( Bentivogli et al , 2009 ) . 14 WNLI Winograd NLI is a small natural language inference dataset ( Levesque et al , 2011 ) . The GLUE webpage notes that there are issues with the construction of this dataset , 15 and every trained system that 's been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class . We therefore exclude this set to be fair to OpenAI GPT . For our GLUE submission , we always predicted the majority class .", "entities": [[1, 4, "TaskName", "Semantic Textual Similarity"], [49, 50, "DatasetName", "MRPC"], [86, 87, "DatasetName", "RTE"], [97, 98, "DatasetName", "MNLI"], [114, 115, "DatasetName", "WNLI"], [120, 123, "TaskName", "natural language inference"], [133, 134, "DatasetName", "GLUE"], [157, 158, "DatasetName", "GLUE"], [165, 166, "MetricName", "accuracy"], [182, 183, "MethodName", "GPT"], [186, 187, "DatasetName", "GLUE"]]}
{"text": "Our evaluation results are shown in Table 2 for all methods . This includes the two baseline methods , and the machine learning methods with the different feature sets . We evaluate the machine learning methods using each dataset as a test set , and using each of the other two and their combination as the training set . The First Mentioned baseline is very weak . The Most Mentioned baseline is much stronger . In almost all cases machine learning methods outperform both baselines . The results of the machine learning method on the ASOIAF and SOC are very strong . The results for WOT are weaker , though they are still accurate enough to be useful when combined with manual checking . It is surprising that using the combination of two training sets does not always out - perform each on their own . For many methods training on just one dataset resulted in better results . We believe that the difference between the top result for a method and the result using the combined training sets is too small to be meaningful . It can , perhaps , be attributed to a coincidental small similarity in writing style of one of the training books to the testing book . To maximise the generalisability of the NovelPerspective prototype ( see Section 5 ) , we deploy models trained on all three datasets combined . Almost all the machine learning models resulted in similarly high accuracy . The exception to this is word embedding features based model trained on SOC , which for both ASOIAF and WOT test sets performed much worse . We attribute the poor performance of these models to the small amount of training data . SOC has only 91 chapters to generate its training cases from , and the word embedding feature set has 600 dimensions . It is thus very easily to over - fit which causes these poor results . Table 3 shows the training set accuracy of each machine learning model . This is a rough upper bound for the possible performance of these models on each test set , as imposed by the classifier and the feature set . The WOT bound is much lower than the other two texts . This likely relates to WOT being written in a style that closer to the line between third - person omniscient , than the more clear third - person limited POV of the other texts . We believe longer range features are required to improve the results for WOT . However , as this achieves such high accuracy for the other texts , further features would not improve accuracy significantly , without additional more difficult training data ( and may cause over - fitting ) . The results do not show a clear advantage to either machine learning feature set . Both the classical features and the word embeddings work well . Though , it seems that the classical feature are more robust ; both with smaller training sets ( like SOC ) , and with more difficult test sets ( like WOT ) .", "entities": [[97, 98, "DatasetName", "SOC"], [246, 247, "MetricName", "accuracy"], [260, 261, "DatasetName", "SOC"], [290, 291, "DatasetName", "SOC"], [333, 334, "MetricName", "accuracy"], [436, 437, "MetricName", "accuracy"], [447, 448, "MetricName", "accuracy"], [486, 488, "TaskName", "word embeddings"], [510, 511, "DatasetName", "SOC"]]}
{"text": "We evaluated the tools for the task of text similarity . Therefore , we calculated the similarity between the input and candidate documents , either based on the whole text or on selected rhetorical elements as provided by the tools . When utilizing the output from the various tools , we built a pseudo - document based either on the sentences or entities that we obtained . For the zoning tools , we concatenated the sentences to form a single text , while we printed the entities ( one per line ) for the entity - based predictions . Similarly , when evaluating combination of various labels , we concatenated the text from various labels into a single file . We performed text similarity using the TextFlow tool ( Mrabet et al , 2017 ) and utilized these similarity scores to rank the candidate documents . Subsequently , we evaluated the ranked list with regard the metrics of precision , recall and f - score at rank 10 , i.e. P@10 , R@10 and F@10 . P@10 is the rate of correct positive candidate documents in the top 10 highest ranked documents , i.e. P @10 = T P @10 10 . The R@10 corresponds to the rate of positives candidate documents in the top 10 over the total of all positive instances , i.e. R@10 = T P @10 N um . P ositive . Finally , the F@10 is the harmonic average of the P@10 and R@10 above , i.e. F @10 = 2 * P @10 * R@10 P @10+R@10 . We considered as positive examples all those publications manually classified by our expert as \" very similar \" or \" similar \" . Given the few of these instances in our datasets , we decided to make no distinction between both categories . As a result , the number of positive examples for the input documents in Figure 1 are 4 , 10 , 16 , 11 , 8 , 23 and 6 , respectively . We evaluated at rank 10 due to the reason that only two datasets have more than 20 positive instances , while only two of them over 10 positive instances . For datasets which contain more than 10 positive examples , we considered the number of positive instances to be equal to 10 in the equation of R@10 . For the final comparison between the various tools and baselines , we per - formed an average of the metrics over the seven datasets . We defined two baselines for comparison : ( i ) the original order of the candidate documents as returned by PubMed 's \" similar articles \" functionality ; and ( ii ) string similarity based on the whole text ( title and abstract ) without any pre - processing on the text . For the first baseline , we searched in PubMed for each of the seven PMIDs and downloaded the list of the top 100 similar articles ( stand of March 13th , 2019 ) . Given that the current list of similar articles might include citations not present at the time when our corpus was annotated , we dismissed any document not included in our dataset when calculating the above metrics , i.e. , we did not consider them as false positives .", "entities": [[8, 10, "TaskName", "text similarity"], [122, 124, "TaskName", "text similarity"], [172, 173, "MetricName", "R@10"], [203, 204, "MetricName", "R@10"], [225, 226, "MetricName", "R@10"], [248, 249, "MetricName", "R@10"], [260, 261, "MetricName", "R@10"], [396, 397, "MetricName", "R@10"]]}
{"text": "We compared the tools based on the metrics of P@10 , R@10 and F@10 that assess the performance of the various tools for the ranking task . We performed a total of 38 experiments which includes the four tools and baselines , as well as some combinations of selected labels from the tools . The combination of labels were decided based on the performance of the single labels and on our understanding of which labels are more relevant for our use case . Table 2 presents the results for our two baselines and the best results for each tool . In the following we specify the labels that obtained the best results : Achakulvisut et al the combination of all labels , i.e. \" Background - Conclusions - Methods - Objective - Results \" ArguminSci : two combinations of labels were equally good : \" Background - Challenge - Table 2 : Summary of the results from the two baselines ( two first rows ) and when using the selected tools . The maximum scores represent the maximum value of P@10 , R@10 and F@10 that could have been obtained by any of the approaches . The minimum scores are the ones obtained when randomly selecting 10 candidates in each dataset , averaged over 1 , 000 experiments . Outcome \" and \" Background - Challenge - Outcome - FutureWork \" . MAZEA : the combination \" Method - Result \" . Prasad and Kan : the combination \" Process - Material \" . For our datasets , all approaches using rhetorical tools obtained a better performance than the baseline from PubMed . Further , three tools scored higher than our strong baseline that uses TextFlow over the whole text ( titles and abstracts ) . Two of the tools ( Achakulvisut et al and ArguminSci ) address zoning elements while one of them ( Prasad and Kan ) returns entity - level annotations . However , none of the tools scored close the maximum possible scores . Given that we do not have at least 10 positive instances ( \" very similar \" or \" similar \" ) for some of our input documents , our maximum P@10 is of 0.83 instead of 1.0 . The three zoning tools rely on labels that can be mapped to one another , as shown by the order of their labels in Table 3 . When examining the performance of single labels , only the \" Outcome \" label from ArguminSci tool could perform close our strong baseline . The labels that we expected to be more relevant , i.e. the ones more related to the background and outcome sections and less with the methods section , did not always perform better in the ranking task . For instance , the F@10 obtained by the label \" Approach \" from ArguminSci performed slightly better ( 0.28 ) than the \" Background \" ( 0.24 ) and \" Challenge \" ( 0.24 ) labels . Similarly , the label \" Method \" from MAZEA performed better ( 0.32 ) than \" Background \" ( 0.25 ) and \" Purpose \" ( 0.25 ) sections . We wonder whether the good performance of methods - related labels were actually due to mistakes in the classification performed by the tools . Our experiments showed that a combination of labels always performed better than the single ones , while some combinations of labels performed better than others ( cf . Figure 3 ) . We could not find any difference in the text similarity scores ( as computed by TextFlow ) when considering different order of the same labels in the concatenation of the text .", "entities": [[11, 12, "MetricName", "R@10"], [182, 183, "MetricName", "R@10"], [596, 598, "TaskName", "text similarity"]]}
{"text": "The noise and complexity of the training data is a source of predictive uncertainty in itself , referred to as data or aleatoric uncertainty ( Kiureghian and Ditlevsen , 2009 ) . This uncertainty is often reflected in the disagreement between human annotations for the same sourcehypothesis segment ( Cohn and Specia , 2013 ; Fornaciari et al , 2021 ) . We hypothesize that the direct assessments can be better modelled as normally distributed scores rather than a single score , and that a model trained to predict this distribution ( mean and standard deviation ) could provide better quality estimates 2 . We formalise this as a KL divergence objective , using the closed form solution to estimate the KL divergence between the target distribution p ( x ) = N ( \u00b5 1 , \u03c3 1 ) and the predicted distribution q ( x ) = N ( \u00b5 2 , \u03c3 2 ) , as shown in Eq . 1 . KL ( p | | q ) = log \u03c3 2 \u03c3 1 + \u03c3 2 1 + ( \u00b5 1 \u2212 \u00b5 2 ) 2 2\u03c3 2 2 \u2212 1 2 ( 1 ) where we take the mean and standard deviation ( std ) of the direct assessment z_scores as the target ( ground truth proxy ) values p. This way , we account for the annotator disagreement ( reflected in the std value ) during learning . QE epistemic uncertainty We use MC dropout ( Gal and Ghahramani , 2016 ) to account for the uncertainty of the QE model . Specifically , we enable dropout during inference and run multiple forward runs over each test instance . Thus we obtain a distribution of quality predictions for each instance instead of a single point estimate . We use the estimated mean of the distribution as our predicted quality estimate . MC dropout has been shown to improve predictive accuracy and perform on par or even better compared to deep ensembles for MT evaluation tasks ( Glushkova et al , 2021 ) . It thus allows us to simulate ensembling in a cheap and effective way , without the need to train multiple checkpoints .", "entities": [[326, 327, "MetricName", "accuracy"], [336, 338, "MethodName", "deep ensembles"]]}
{"text": "The QE data is relatively limited , making it harder to train multilingual models with a large number of parameters without over - fitting . Thus , as explained in 3.1 we aimed to investigate whether we could obtain models that generalise better and are more robust to noise and out - of - distribution data by training the XLM - RoBERTa model first on a larger - yet noisier and out - of - domain dataset . To that end we leverage the data provided for the past Metrics shared tasks , which covers the language pairs used in this year 's QE task , including the blind tests for which we had no in - domain data available . Altogether , it encompasses 30 language pairs from the news domain ( versus 7 in the QE dataset ) . We provide more detailed statistics for each language pair of the Metrics data in Appendix C. We refer to experiments using the model initially trained on the Metrics data as M1 M - . We also show that using the trained XLM - RoBERTa encoder from the M1 M model can prove beneficial for the predictions on post - edited data of Task 2 ( see Table 3 ) .", "entities": [[17, 20, "HyperparameterName", "number of parameters"], [59, 60, "MethodName", "XLM"], [61, 62, "MethodName", "RoBERTa"], [182, 183, "MethodName", "XLM"], [184, 185, "MethodName", "RoBERTa"]]}
{"text": "The results can be seen in Tables 1 and 2 . In line with the shared task guidelines we treat Pearson r as the primary performance metric and select the submitted models accordingly . We can observe , that while on average the M1 model and its variations outperform the M2 model , their performance is comparable , and M2 - KL - G - MCD can even outperform M1 M - ADAPT for specific language pairs , hence it made sense to combine them in the final ensemble . We can also see that fine - tuning the M1 model on the Metrics data , results in performance gains for the majority of the language pairs . Specifically , even applying the M1 M directly , without further fine - tuning on QE data , achieves competitive performance for most pairs , which further improves upon fine - tuning . It helps in increasing the performance on the blind sets ( denoted as zeroshot in the Appendix B assessments for each segment . Thus , the difference in target score range and distribution could affect the magnitude of predicted scores and the distance to the ground truth values , which is reflected in the MAE and RMSE metrics . These findings , further supported by the results on Task 2 , is a first step in exploring the underlying connection and bridging the gap between the Metrics and Quality Estimation shared tasks .", "entities": [[205, 206, "MetricName", "MAE"], [207, 208, "MetricName", "RMSE"]]}
{"text": "The results can be seen in Table 3 . Similarly to Task 1 , the primary evaluation metric for the sentence level sub - task of Task 2 is the Pearson r coefficient , 2 : Results for Task 1 with the M2 predictorestimator ( mBART ) and different uncertainty handling additions . \" KL \" signifies the incorporation of KL loss , \" G\"the incorporation of glass - box features and MCD the addition of MC dropout . ML stands for MULTILIN - GUAL , showing the performance averaged over all language pairs . Underlined numbers indicate the best result for each language pair and evaluation metric . Bold systems were selected for the final ensemble . while the word level sub - task is evaluated using the Matthews correlation coefficient ( MCC , ( Matthews , 1975 ) ) as the primary performance indicator . We can see that while HTER scores do not always correlate highly with DAs ( see Table 4 ) , the use of the M1 M model encoder that was trained on large data with direct assessments can still prove useful . Indeed , when fine - tuning on the Task2 data , the model using the M1 M encoder ( M1 M - ADAPT in the table 3 ) provides a performance boost for the Pearson correlation in most language pairs , and competitive performance for the rest . Based on these results , we deem it worthwhile to include checkpoints trained with this configuration in the ensemble estimating that they will contribute in higher performance , especially on the blind test sets . This can be further confirmed when", "entities": [[45, 46, "MethodName", "mBART"], [61, 62, "MetricName", "loss"], [223, 225, "MetricName", "Pearson correlation"]]}
{"text": "This section describes how to construct the discourse relation graphs and action graphs . Formally , for a given conversation C = { u 0 , ... , u m } with m utterances , we construct discourse relation graph G D = ( V D , E D ) , where V D is the set of nodes representing Elementary Discourse Units ( EDUs ) , and E D is the adjacent matrix that describes the relations between EDUs , and action graph G A = ( V A , E A ) , where V A is the set of nodes representing \" WHO \" , \" DOING \" and \" WHAT \" arguments , and E A is the adjacent matrix to link \" WHO - DOING - WHAT \" triples . Discourse Relation Graph Utterances from different speakers do not occur in isolation ; instead , they are related within the context of discourse ( Murray et al , 2006 ; Qin et al , 2017 ) , which has been shown effective for dialogue understanding like identifying the decisions in multi - party dialogues ( Bui et al , 2009 ) and detecting salient content in email conversations ( McKeown et al , 2007 ) . Although current attention - based neural models are supposed to , or might implicitly , learn certain relations between utterances , they often struggle to focus on many informative utterances ( Chen and Yang , 2020 ; Song et al , 2020 ) and fail to address long - range dependencies ( Xu et al , 2020 ) , especially when there are frequent interruptions . As a result , explicitly incorporating the discourse relations will help neural summarization models better encode the unstructured conversations and concentrate on the most salient utterances to generate more informative and less redundant summaries . To do so , we view each utterance as an EDU and use the discourse relation types defined in Asher et al ( 2016 ) . We first pre - train a discourse parsing model ( Shi and Huang , 2019 ) on a humanannotated multiparty dialogue corpus ( Asher et al , 2016 ) , with 0.775 F1 score on link predictions and 0.557 F1 score on relation classifications , which are comparable to the state - of - the - art results ( Shi and Huang , 2019 ) . We then utilize this pre - trained parser to predict the discourse relations within conversations in our SAMSum corpus ( Gliwa et al , 2019 ) . After predictions , there are 138 , 554 edges identified in total and 8.48 edges per conversation . The distribution of these predicted discourse relation types is : Comment ( 19.3 % ) , Clarification Question ( 15.2 % ) , Elaboration ( 2.3 % ) , Acknowledgement ( 8.4 % ) , Continuation ( 10.1 % ) , Explanation ( 2.8 % ) , Conditional ( 0.2 % ) , Question Answer Pair ( 21.5 % ) , Alternation ( 0.3 % ) , Q - Elab ( 2.5 % ) , Result ( 5.5 % ) , Background ( 0.4 % ) , Narration ( 0.4 % ) , Correction ( 0.4 % ) , Parallel ( 0.9 % ) , and Contrast ( 1.0 % ) . Then for each conversation , we construct a discourse relation graph G D = ( V D , E D ) , where V D [ k ] represents the k - th utterance . E D [ i ] [ j ] = r if there is a link from the i - th utterance to the j - th one with discourse relation r.", "entities": [[24, 25, "DatasetName", "0"], [178, 180, "TaskName", "dialogue understanding"], [289, 290, "TaskName", "summarization"], [344, 346, "TaskName", "discourse parsing"], [370, 372, "MetricName", "F1 score"], [377, 379, "MetricName", "F1 score"], [421, 423, "DatasetName", "SAMSum corpus"]]}
{"text": "Node Initialization For discourse relation graph , we employ the output embeddings of the special tokens x i , 0 from the utterance encoder , i.e. , h U i , 0 , to initialize the i - th node v D i in G D . We use a one - hot embedding layer to encode the relations E D [ i ] [ j ] = e D i , j between utterance i and j. For action graph , we first utilize F U ( . ) to encode each token in nodes v A i and then average their output embeddings as their initial representations . Structured Graph Attention Network Based on Graph Attention Network ( Veli\u010dkovi\u0107 et al , 2018 ) , we utilize these relations between nodes to encode each node W , W e and a are trainable parameters . [ . . ] denotes the concatenation of two vectors . \u03c3 is the activation function , N i is the set containing nodei 's neighbours in G. v D i in G D or v A i in G A through : \u03b1 ij = exp \u03c3 a T [ Wv i Wv j W e e i , j ] k N i exp ( \u03c3 ( a T [ Wv i Wv k W e e i , k ] ) ) h i = \u03c3 ( j N i \u03b1 ij Wv j ) Dataset Split # Through two graph encoders F D ( . , . ) and F A ( . , . ) , we then obtain the hidden representations of these nodes as : { h D 0 , ... , h D m } = F D ( { v D 0 , ... , v D m } , E D ) ( 2 ) { h A 0 , ... , h A n } = F A ( { x A 0 , ... , x A n } , E A ) ( 3 )", "entities": [[19, 20, "DatasetName", "0"], [31, 32, "DatasetName", "0"], [111, 114, "MethodName", "Graph Attention Network"], [116, 119, "MethodName", "Graph Attention Network"], [161, 163, "HyperparameterName", "activation function"], [190, 191, "HyperparameterName", "\u03b1"], [240, 241, "HyperparameterName", "\u03b1"], [282, 283, "DatasetName", "0"], [297, 298, "DatasetName", "0"], [315, 316, "DatasetName", "0"], [330, 331, "DatasetName", "0"]]}
{"text": "Different levels of encoded representations are then aggregated via our multi - granularity decoder to generate summaries as shown in Figure 2 ( b ) . With s \u2212 1 previously generated tokens y 1 , ... , y s\u22121 , our decoder G ( . ) predicts the l - th token via : y = G ( y 1 : s\u22121 , F U ( C ) , F D ( G D ) , F A ( G A ) ) ( 4 ) P ( \u1ef9 s | y < s , C , G D , G A ) = Softmax ( W p\u0177 ) ( 5 ) To better incorporate the information in constructed graphs , different from the traditional pretrained BART model ( Lewis et al , 2020 ) , we improve the BART transformer decoder with two extra cross attentions ( Discourse Attention and Action Attention ) added to each decoder layer , which attends to the encoded node representations in discourse relation graphs and action graphs . In each decoder layer , after performing the original cross attentions over every token in utterances { h U i , 0 : l } and getting the utterance - attended representation x U , multi - granularity decoder then conducts cross attentions over nodes { h D 0 : m } and { h A 0 : n } that are encoded from graph encoders in parallel , to obtain the discourse - attended representation x D and action - attended representation x A . These two attended vectors are then combined into a structureaware representation x S , through a feed - forward network for further forward passing in the decoder . To alleviate the negative impact of randomly initialized graph encoders and cross attentions over graphs on pre - trained BART decoders at early stages and accelerate the learning of newlyintroduced modules during training , we apply ReZero ( Bachlechner et al , 2020 ) to the residual connection after attending to graphs in each decoder layer : x S = x U + \u03b1x S ( 6 ) where \u03b1 is one trainable parameter instead of a fixed value 1 , which modulates updates from cross attentions over graphs . Training During training , we seek to minimize the cross entropy and use the teacher - forcing strategy ( Bengio et al , 2015 ) : L = \u2212 log P ( \u1ef9 l | y < l , C , G D , G A ) ( 7 ) 4 Experiments", "entities": [[105, 106, "MethodName", "Softmax"], [127, 128, "MethodName", "BART"], [140, 141, "MethodName", "BART"], [141, 143, "MethodName", "transformer decoder"], [197, 198, "DatasetName", "0"], [224, 225, "DatasetName", "0"], [232, 233, "DatasetName", "0"], [309, 310, "MethodName", "BART"], [326, 327, "MethodName", "ReZero"], [336, 338, "MethodName", "residual connection"], [359, 360, "HyperparameterName", "\u03b1"]]}
{"text": "We used the BART - base model to initialize our sequence - to - sequence model for training in all experiments . For parameters in the original BART encoder / decoder , we followed the default settings and set the learning rate 3e - 5 with 120 warm - up steps . For graph encoders , we set the number of hidden dimensions as 768 , the number of attention heads as 2 , the number of layers as 2 , and the dropout rate as 0.2 . For graph cross attentions added to BART decoder layers , we set the number of attention heads as 2 . The weights \u03b1 in ReZero residual connections were initialized with 1 . The learning rate for parameters in newly added modules was 3e - 4 with 60 warm - up steps . All experiments were performed on GeForce RTX 2080Ti ( 11 GB memory ) .", "entities": [[3, 4, "MethodName", "BART"], [27, 28, "MethodName", "BART"], [40, 42, "HyperparameterName", "learning rate"], [75, 78, "HyperparameterName", "number of layers"], [94, 95, "MethodName", "BART"], [110, 111, "HyperparameterName", "\u03b1"], [112, 113, "MethodName", "ReZero"], [121, 123, "HyperparameterName", "learning rate"]]}
{"text": "Automatic Evaluation We evaluated all the models with the widely used automatic metric , All model variants of S - BART received significantly higher ratings than BART ( student t - test , p < 0.05 ) . ROUGE scores ( Lin and Och , 2004 ) 3 , and reported ROUGE - 1 , ROUGE - 2 , and ROUGE - L in Table 2 . We found that , compared to simple sequence - to - sequence models ( Pointer Generator and Transformer ) , incorporating extra information such as commonsense knowledge from ConceptNet ( D - HGN ) increased the ROUGE metrics . When equipped with pre - trained models and simple conversation structures such as topics and conversation stages , Multi - View Seq2Seq boosted ROUGE scores . Incorporating discourse relation graphs or action graphs helped the performances of summarization , suggesting the effectiveness of explicitly modeling relations between utterances and the associations between speakers and actions within utterances . Combining two different structured graphs produced better ROUGE scores compared to previous state - of - the - art methods and our base models , with an increase of 2.0 % on ROUGE - 1 , 4.3 % on ROUGE - 2 , and 1.2 % on ROUGE - L compared to our base model , BART . This indicates that , our structure - aware models with discourse and action graphs could help abstractive conversation summarization , and these two graphs complemented each other in generating better summaries .", "entities": [[20, 21, "MethodName", "BART"], [26, 27, "MethodName", "BART"], [60, 63, "MetricName", "ROUGE - L"], [84, 85, "MethodName", "Transformer"], [95, 96, "DatasetName", "ConceptNet"], [127, 128, "MethodName", "Seq2Seq"], [143, 144, "TaskName", "summarization"], [211, 214, "MetricName", "ROUGE - L"], [220, 221, "MethodName", "BART"], [240, 241, "TaskName", "summarization"]]}
{"text": "To investigate the generalizability of our structureaware models , we then tested the S - BART model trained on SAMSum corpus directly on the debate summarization domain ( ADSC Corpus ( Misra et al , 2015 ) ) in a zero - shot setting . Besides the differences in topics , utterances in debate conversations were generally longer and include more action triples ( 37.20 vs 6.81 as shown in Table 1 ) and fewer participants . The distribution of discourse relation types also differed a lot across different domains 4 ( e.g. , more Contrast in debates ( 19.5 % ) than in daily conversations ( 1.0 % ) ) . As shown in Table 3 , our single graph models S - BART w. Discourse and S - BART w. Action boosted ROUGE scores compared to BART , suggesting that utilizing structures can also increase the generalizability of conversation summarization methods . However , contrary to in - domain results in Table 2 , action graphs led to much more gains than discourse graphs . This indicated that when domain shifts , action triples were most robust in terms of zero - shot setups ; differences in discourse relation distributions could limit such generalization . Consistent with in - domain scenarios , our S - BART w. Discourse&Action achieved better results , with an increase of 66.2 % on ROUGE - 1 , 373.4 % on ROUGE - 2 , and 82.2 % on ROUGE - L over BART .", "entities": [[15, 16, "MethodName", "BART"], [19, 21, "DatasetName", "SAMSum corpus"], [25, 26, "TaskName", "summarization"], [124, 125, "MethodName", "BART"], [130, 131, "MethodName", "BART"], [138, 139, "MethodName", "BART"], [151, 152, "TaskName", "summarization"], [217, 218, "MethodName", "BART"], [246, 249, "MetricName", "ROUGE - L"], [250, 251, "MethodName", "BART"]]}
{"text": "This part conducted ablation studies to show the effectiveness of structured graphs in our S - BART . The Quality of Discourse Relation Graphs We showed how the quality of discourse relation graphs affected the performances of conversation summarization in Table 5 . Specifically , we compared the ROUGE scores of S - BART using our constructed discourse relation graphs ( S - BART w. Discourse Graph ) and S - BART using randomly generated discourse relation graphs S - BART w. Random Graph where both connections between nodes and relation types were randomized . The number of edges in two graphs was kept the same . We found that S - BART with our discourse graphs outperformed 6 . Here , parallel strategy performed cross attentions on different graphs separately and then combined the attended results with feed - forward networks as discussed in Section 3.3 ; sequential strategy performed cross attentions on two graphs in a specific order ( from discourse relation graphs to actions graphs , or vice versa ) . We found that the parallel strategy showed better performances and the sequential ones did not introduce gains compared to S - BART with single graphs . This demonstrates that discourse relation graphs and action graphs were both important and provided different signals for abstractive conversation summarization . Visualizing ReZero Weights We further tested our structure - aware BART with two ReZero settings : ( i ) initializing \u03b1 from 0 , ( ii ) initializing \u03b1 from 1 , and found initializing \u03b1 from 1 would bring in more performance gains ( see Appendix ) . We then visualized the average \u03b1 over different decoder layers after training in Figure 3 , and observed that ( i ) when \u03b1 was initialized with 1 , the final \u03b1 was much larger than the setting where \u03b1 was initialized with 0 , which might because randomly initialized modules barely received supervisions at early stages and therefore contributes less to BART . ( ii ) Compared to discourse graphs , action graphs received higher \u03b1 weights after training in both initializing settings , suggesting that the information from structured action graphs might be harder for the end - to - end BART models to capture . ( iii ) Utilizing both graphs spontaneously led to higher ReZero weights , further validating the effectiveness of combining discourse relation graphs and action graphs and their complementary properties .", "entities": [[16, 17, "MethodName", "BART"], [38, 39, "TaskName", "summarization"], [53, 54, "MethodName", "BART"], [63, 64, "MethodName", "BART"], [71, 72, "MethodName", "BART"], [80, 81, "MethodName", "BART"], [112, 113, "MethodName", "BART"], [195, 196, "MethodName", "BART"], [219, 220, "TaskName", "summarization"], [222, 223, "MethodName", "ReZero"], [231, 232, "MethodName", "BART"], [234, 235, "MethodName", "ReZero"], [241, 242, "HyperparameterName", "\u03b1"], [243, 244, "DatasetName", "0"], [249, 250, "HyperparameterName", "\u03b1"], [256, 257, "HyperparameterName", "\u03b1"], [275, 276, "HyperparameterName", "\u03b1"], [293, 294, "HyperparameterName", "\u03b1"], [301, 302, "HyperparameterName", "\u03b1"], [309, 310, "HyperparameterName", "\u03b1"], [313, 314, "DatasetName", "0"], [332, 333, "MethodName", "BART"], [346, 347, "HyperparameterName", "\u03b1"], [373, 374, "MethodName", "BART"], [388, 389, "MethodName", "ReZero"]]}
{"text": "To inspect when our summarization models could help the conversations summarization , we visualized the average number of discourse edges and the average number of action triples in three sets of conversations in examples where both S - BART and BART showed low ROUGE scores ( ROUGE - 1 < 20.0 , ROUGE - 2 < 10.0 , ROUGE - L < 10.0 ) . When the structures in conversations were simpler ( fewer discourse edges and fewer action triples than the average ) , BART showed similar performance as S - BART . As the structures of conversations become more complex with more discourse relations and more action mentions , S - BART outperformed BART as it explicitly incorporated these structured graphs . However , both BART and S - BART struggled when there were much more interactions beyond certain thresholds , calling for better mechanisms to model structures in conversations for generating better summaries .", "entities": [[4, 5, "TaskName", "summarization"], [10, 11, "TaskName", "summarization"], [38, 39, "MethodName", "BART"], [40, 41, "MethodName", "BART"], [58, 61, "MetricName", "ROUGE - L"], [85, 86, "MethodName", "BART"], [92, 93, "MethodName", "BART"], [113, 114, "MethodName", "BART"], [115, 116, "MethodName", "BART"], [127, 128, "MethodName", "BART"], [131, 132, "MethodName", "BART"]]}
{"text": "We tested our structure - aware BART ( S - BART w. Discourse / Action ) within two ReZero settings : ( i ) initializing \u03b1 from 0 , ( ii ) initializing \u03b1 from 1 . And the results were shown in Table 8 . S - BART with 1 as the initialized ReZero weight outperformed", "entities": [[6, 7, "MethodName", "BART"], [10, 11, "MethodName", "BART"], [18, 19, "MethodName", "ReZero"], [25, 26, "HyperparameterName", "\u03b1"], [27, 28, "DatasetName", "0"], [33, 34, "HyperparameterName", "\u03b1"], [48, 49, "MethodName", "BART"], [54, 55, "MethodName", "ReZero"]]}
{"text": "Many NLP tasks can be formulated as sequence labeling problems , such as part - of - speech ( POS ) tagging ( Zheng et al , 2013 ) , named entity recognition ( NER ) ( Lample et al , 2016 ) , and event extraction ( Yang et al , 2019 ) . Recently , neural sequential models ( Lample et al , 2016 ; Akbik et al , 2018 ; Vaswani et al , 2017 ) have shown strong performance for various sequence labeling task . However , these deep neural models are label hungrythey require large amounts of annotated sequences to achieve strong performance . Obtaining large amounts of annotated data can be too expensive for practical sequence labeling tasks , due to tokenlevel annotation efforts . Active learning is an important technique for sequence labeling in low - resource settings . Active sequence labeling is an iterative process . In each iteration , a fixed number of unlabeled sequences are selected by a query policy for annotation and then model updating , in hope of maximally improving model performance . For example , Tomanek et al ( 2007 ) ; Shen et al ( 2017 ) select query samples based on data uncertainties ; Hazra et al ( 2019 ) compute model - aware similarity to eliminate redundant examples and improve the diversity of query samples ; and Fang et al ( 2017 ) ; Liu et al ( 2018 ) use reinforcement learning to learn query policies . However , existing methods for active sequence labeling all use the queried samples alone in each iteration . We argue that the queried samples provide limited data diversity , and using them alone for model updating is inefficient in terms of leveraging human annotation efforts . We study the problem of enhancing active sequence labeling via data augmentation . We aim to generate augmented labeled sequences for the queried samples in each iteration , thereby introducing more data diversity and improve model generalization . However , data augmentation for active sequence labeling is challenging , because we need to generate sentences and token - level labels jointly . Prevailing generative models ( Zhang et al , 2016 ; Bowman et al , 2016 ) are inapplicable because they can only generate word sequences without labels . It is also infeasible to apply heuristic data augmentation methods such as context - based words substitution ( Kobayashi , 2018 ) , synonym replacement , random insertion , swap , and deletion ( Wei and Zou , 2019 ) , paraphrasing ( Cho et al , 2019 ) or back translation , because label composition is complex for sequence labeling . Directly using these techniques to manipulate tokens may inject incorrectly labeled sequences into training data and harm model performance . We propose SeqMix , a data augmentation method for generating sub - sequences along with their labels based on mixup ( Zhang et al , 2018 ) . Under the active sequence labeling framework , Se - qMix is capable of generating plausible pseudo labeled sequences for the queried samples in each iteration . This is enabled by two key techniques in SeqMix : ( 1 ) First , in each iteration , it searches for pairs of eligible sequences and mixes them both in the feature space and the label space . ( 2 ) Second , it has a discriminator to judge if the generated sequence is plausible or not . The discriminator is designed to compute the perplexity scores for all the generated candidate sequences and select the low - perplexity sequences as plausible ones . We show that SeqMix consistently outperforms standard active sequence labeling baselines under different data usage percentiles with experiments on Named Entity Recognition and Event Detection tasks . On average , it achieves 2.95 % , 2.27 % , 3.75 % F 1 improvements on the CoNLL - 2003 , ACE05 and WebPage datasets . The advantage of SeqMix is especially prominent in low - resource scenarios , achieving 12.06 % , 8.86 % , 16.49 % F 1 improvements to the original active learning approach on the above three datasets . Our results also verify the proposed mixup strategies and the discriminator are vital to the performance of SeqMix .", "entities": [[13, 16, "DatasetName", "part - of"], [30, 33, "TaskName", "named entity recognition"], [34, 35, "TaskName", "NER"], [45, 47, "TaskName", "event extraction"], [131, 133, "TaskName", "Active learning"], [310, 312, "TaskName", "data augmentation"], [340, 342, "TaskName", "data augmentation"], [397, 399, "TaskName", "data augmentation"], [477, 479, "TaskName", "data augmentation"], [491, 492, "MethodName", "mixup"], [592, 593, "MetricName", "perplexity"], [605, 606, "MetricName", "perplexity"], [630, 633, "TaskName", "Named Entity Recognition"], [634, 636, "TaskName", "Event Detection"], [693, 695, "TaskName", "active learning"], [708, 709, "MethodName", "mixup"]]}
{"text": "Many NLP problems can be formulated as sequence labeling problems . Given an input sequence , the task is to annotate it with token - level labels . The labels often consist of a position prefix provided by a labeling schema and a type indicator provided by the specific task . For example , in the named entity recognition task , we can adopt the BIO ( Beginning , Inside , Outside ) tagging scheme ( M\u00e0rquez et al , 2005 ) to assign labels for each token : the first token of an entity mention with type X is labeled as B - X , the tokens inside that mention are labeled as I - X and the non - entity tokens are labeled as O. Consider a large unlabeled corpus U , traditional active learning starts from a small annotated seed set L , and utilizes a query function \u03c8 ( U , K , \u03b3 ( ) ) to obtain K most informative unlabeled samples X = { x 1 , . . . , x K } along with their labels Y = { y 1 , , y K } , where \u03b3 ( ) is the query policy . Then , we remove X from the unlabeled data U and repeat the above procedure until the satisfactory performance achieved or the annotation capacity reached . In SeqMix , we aim to further exploit the annotated set X , Y to generate augmented data X * , Y * . Then the labeled dataset is expanded as L = L \u222a X , Y \u222a X * , Y * . Formally , we define our task as : ( 1 ) construct a generator \u03c6 ( ) to implement sequence and label generation based on the actively sampled data X and its label Y , ( 2 ) set a discriminator d ( ) to yield the filtered generation , then ( 3 ) augment the labeled set as L = L \u222a X , Y \u222a d ( \u03c6 ( X , Y ) ) .", "entities": [[56, 59, "TaskName", "named entity recognition"], [135, 137, "TaskName", "active learning"], [157, 158, "HyperparameterName", "\u03b3"], [197, 198, "HyperparameterName", "\u03b3"]]}
{"text": "Active sequence labeling selects K most informative instances \u03c8 ( , K , \u03b3 ( ) ) in each iteration , with the hope of maximally improving model performance with a fixed labeled budget . With the input sequence x of length T , we denote the model output as f ( | x ; \u03b8 ) . Our method is generic to any query policies \u03b3 ( ) . Below , we introduce several representative policies . Least Confidence ( LC ) Culotta and McCallum ( 2005 ) measure the uncertainty of sequence models by the most likely predicted sequence . For a CRF model ( Lafferty et al , 2001 ) , we calculate \u03b3 with the predicted sequential label y * as \u03b3 LC ( x ) = 1 \u2212 max y * ( P ( y * | x ; \u03b8 ) , ( 1 ) where y * is the Viterbi parse . For BERT ( Devlin et al , 2019 ) with a token classification head , we adopt a variant of the least confidence measure : \u03b3 LC ' ( x ) = T t=1 ( 1 \u2212 max yt P ( y t | x ; \u03b8 ) ) , ( 2 ) where P ( y t | x ; \u03b8 ) = softmax ( f ( y t | x ; \u03b8 ) ) . Normalized Token Entropy ( NTE ) Another uncertainty measure for the query policy is normalized entropy ( Settles and Craven , 2008 ) , defined as : Disagreement Sampling Query - by - committee ( QBC ) ( Seung et al , 1992 ) , is another approach for specifying the policy , where the unlabeled data can be sampled by the disagreement of the base models . The disagreement can be defined in several ways , here we take the vote entropy proposed by ( Dagan and Engelson , 1995 ) . Given a committee consist of C models , the vote entropy for input x is : \u03b3 TE ( x ) = \u2212 1 T T t=1 M m=1 P m ( y t | x , \u03b8 ) log P m ( y t | x , \u03b8 ) , ( 3 ) \u03b3 VE ( x ) = \u2212 1 T T t=1 M m=1 V m ( y t ) C log V m ( y t ) C , ( 4 ) where V m ( y t ) is the number of models that predict the t - th token x t as the label m. 3 The SeqMix Method", "entities": [[13, 14, "HyperparameterName", "\u03b3"], [55, 56, "HyperparameterName", "\u03b8"], [66, 67, "HyperparameterName", "\u03b3"], [104, 105, "MethodName", "CRF"], [116, 117, "HyperparameterName", "\u03b3"], [125, 126, "HyperparameterName", "\u03b3"], [144, 145, "HyperparameterName", "\u03b8"], [159, 160, "MethodName", "BERT"], [169, 171, "TaskName", "token classification"], [183, 184, "HyperparameterName", "\u03b3"], [204, 205, "HyperparameterName", "\u03b8"], [219, 220, "HyperparameterName", "\u03b8"], [222, 223, "MethodName", "softmax"], [231, 232, "HyperparameterName", "\u03b8"], [344, 345, "HyperparameterName", "\u03b3"], [365, 366, "HyperparameterName", "\u03b8"], [376, 377, "HyperparameterName", "\u03b8"], [382, 383, "HyperparameterName", "\u03b3"]]}
{"text": "Given a corpus for sequence labeling , we assume the dataset contains a small labeled set L and a large unlabeled set U initially . We start from augmenting the seed set L with SeqMix . First , we adopt a pairing function \u03b6 ( ) to find paired samples by traversing L. Next , we generate mixed - labeled sequences via latent space linear interpolation with one of the approaches mentioned in Section 3.2 . To ensure the semantic quality of the generated sequences , we use a discriminator After that , the iterative active learning procedure begins . In each iteration , we actively select instances from U with a query policy \u03b3 ( ) ( Section 2.2 ) to obtain the top K samples X = \u03c8 ( U , K , \u03b3 ( ) ) . The newly selected samples will be labeled with Y , and the batch of samples X , Y will be used for SeqMix . Again , we generate L * = SeqMix ( X , Y , \u03b1 , \u03b6 ( ) , d ( ) ) and expand the training set as L = L \u222a L * . Then we train the model \u03b8 on the newly augmented set L. The iterative active learning procedure terminates when a fixed number of iterations are reached . We summarize the above procedure in Algorithm 1 .", "entities": [[95, 97, "TaskName", "active learning"], [114, 115, "HyperparameterName", "\u03b3"], [135, 136, "HyperparameterName", "\u03b3"], [177, 178, "HyperparameterName", "\u03b1"], [205, 206, "HyperparameterName", "\u03b8"], [214, 216, "TaskName", "active learning"], [221, 224, "HyperparameterName", "number of iterations"]]}
{"text": "Mixup ( Zhang et al , 2018 ) is a data augmentation method that implements linear interpolation in the input space . Given two input samples x i , x j along // active learning iterations with augmentation for round in active learning rounds do X = \u03c8 ( U , K , \u03b3 ( ) ) U = U \u2212 X Annotate X to get X , Y L * = SeqMix ( X , Y , \u03b1 , \u03b6 ( ) , d ( ) ) L = L \u222a X , Y \u222a L * \u03b8 = train ( \u03b8 , L ) end Output : The sequence model trained with active data augmentation : \u03b8 with the labels y i , y j , the mixing process is : x = \u03bbx i + ( 1 \u2212 \u03bb ) x j , ( 5 ) y = \u03bby i + ( 1 \u2212 \u03bb ) y j , ( 6 ) where \u03bb \u223c Beta ( \u03b1 , \u03b1 ) is the mixing coefficient . Through linear combinations on the input level of paired examples and their labels , Mixup regularizes the model to present linear behavior among the training data . Mixup is not directly applicable to generate interpolated samples for text data , because the input space is discrete . To overcome this , SeqMix performs token - level interpolation in the embedding space and selects a token closest to the interpolated embedding . Specifically , SeqMix constructs a table of tokens W and their corresponding contextual embeddings E 1 . Given two sequences x i = { w 1 i , , w T i } and x j = { w 1 j , , w T j } with their embedding representations e x i = { e 1 i , , e T i } and e x j = { e 1 j , , e T j } , the t - th mixed token is the token whose embedding e t is closest to the mixed embedding : e t = arg min e E e \u2212 ( \u03bbe t i + ( 1 \u2212 \u03bb ) e t j ) 2 . ( 7 ) To get the corresponding w t , we can query the table { W , E } using e t . The label generation is straightforward . For two label sequences y i = { y 1 i , , y T i } and y j = { y 1 j , , y T j } , we get the t - th mixed label as : y t = \u03bby t i + ( 1 \u2212 \u03bb ) y t j , ( 8 ) where y t i and y t j are one - hot encoded labels . Along with the above sequence mixup procedure , we also introduce a pairing strategy that selects sequences for mixup . The reason is that , in many sequence labeling tasks , the labels of interest are scarce . For example , in the NER and event detection tasks , the \" O \" label is dominant in the corpus , which do not refer to any entities or events of interest . We thus define the labels of interest as valid labels , e.g. , the non - \" O \" labels in NER and event detection , and design a sequence pairing function to select more informative parent sequences for mixup . Specifically , the sequence pairing function \u03b6 ( ) is designed according to valid label density . For a sequence , its valid label density is defined as \u03b7 = n s , where n is the number of valid labels and s is the length of the sub - sequence . We set a threshold \u03b7 0 for \u03b6 ( ) , and the sequence will be considered as an eligible candidate for mixup only when \u03b7 \u2265 \u03b7 0 . Based on the above token - level mixup procedure and the sequence pairing function , we propose three different strategies for generating interpolated labeled sequences . These strategies are shown in Figure 1 and described below : Whole - sequence mixup As the name suggests , whole - sequence mixup ( Figure 1 ( a ) ) performs sequence mixing at the whole - sequence level . Given two sequences x i , y i , x j , y j L , they must share the same length without counting padding words . Besides , the paring function \u03b6 ( ) requires that both the two sequences satisfy \u03b7 \u2265 \u03b7 0 . Then we perform mixup at all token positions , by employing Equation 7to generate mixed tokens and Equation 8 to generate mixed labels ( note that the mixed labels are soft labels ) . Sub - sequence mixup One drawback of the whole - sequence mixup is that it indiscriminately mixes over all tokens , which may include incompatible subsequences and generate implausible sequences . To tackle this , we consider sub - sequence mixup ( Figure 1 x i , y i , x j , y j , ( i = j ) in L do if \u03b6 ( x i , y i , x j , y j ) then \u03bb \u223c Beta ( \u03b1 , \u03b1 ) // mixup the target sub - sequences for t = 1 , , T do Calculate e t by Eq . ( 7 ) ; Get corresponding token w t for e t ; Calculate y t by Eq . ( 8 ) . end x sub = { w 1 , , w T } y sub = { y 1 , , y T } // replace the original sequences for k in { i , j } d\u00f5 x k = x k \u2212 x ksub + x sub y k = y k \u2212 y ksub + \u1ef9 sub if d ( x k ) then L * = L * \u222a x k , \u1ef9 k end if | L * | \u2265 N x i , y i , x j , y j as X isub = x 1 isub , . . . , x s isub , X jsub = x 1 jsub , . . . , x s jsub . If x isub X isub , x jsub X jsub , such that their \u03b7 \u2265 \u03b7 0 , we have \u03b6 ( x i , y i , x j , y j ) = True . Then the subsequences x isub and x jsub are mixed as Figure 1 ( b ) . The mixed sub - sequence and labels will replace the original parts of the parent samples , and the other parts of the parent samples remain unchanged . In this way , sub - sequence mixup is expected to keep the syntax structure of the original sequence , while providing data diversity . Label - constrained sub - sequence mixup can be considered as a special case of sub - sequence mixup , where the constraints inherit sub - sequence mixup , and further require that the sub - sequence labels are consistent . As Figure 1 ( c ) shows , after mixing such paired samples , the generation will just update the tokens of the sub - sequences while keeping the labels the same as before . Hence , this Generated Sequence Generated Sequence 4 5 4 5 1 2 3 1 2 3 1 2 3 1 2 3", "entities": [[0, 1, "MethodName", "Mixup"], [10, 12, "TaskName", "data augmentation"], [33, 35, "TaskName", "active learning"], [41, 43, "TaskName", "active learning"], [53, 54, "HyperparameterName", "\u03b3"], [78, 79, "HyperparameterName", "\u03b1"], [98, 99, "HyperparameterName", "\u03b8"], [102, 103, "HyperparameterName", "\u03b8"], [115, 117, "TaskName", "data augmentation"], [118, 119, "HyperparameterName", "\u03b8"], [170, 171, "HyperparameterName", "\u03b1"], [172, 173, "HyperparameterName", "\u03b1"], [193, 194, "MethodName", "Mixup"], [206, 207, "MethodName", "Mixup"], [487, 488, "MethodName", "mixup"], [500, 501, "MethodName", "mixup"], [525, 526, "TaskName", "NER"], [527, 529, "TaskName", "event detection"], [575, 576, "TaskName", "NER"], [577, 579, "TaskName", "event detection"], [593, 594, "MethodName", "mixup"], [652, 653, "DatasetName", "0"], [669, 670, "MethodName", "mixup"], [675, 676, "DatasetName", "0"], [684, 685, "MethodName", "mixup"], [717, 718, "MethodName", "mixup"], [726, 727, "MethodName", "mixup"], [789, 790, "DatasetName", "0"], [794, 795, "MethodName", "mixup"], [828, 829, "MethodName", "mixup"], [836, 837, "MethodName", "mixup"], [865, 866, "MethodName", "mixup"], [909, 910, "HyperparameterName", "\u03b1"], [911, 912, "HyperparameterName", "\u03b1"], [914, 915, "MethodName", "mixup"], [995, 997, "HyperparameterName", "k ="], [1006, 1008, "HyperparameterName", "k ="], [1101, 1102, "DatasetName", "0"], [1174, 1175, "MethodName", "mixup"], [1198, 1199, "MethodName", "mixup"], [1210, 1211, "MethodName", "mixup"], [1219, 1220, "MethodName", "mixup"]]}
{"text": "During sequence mixup , the mixing coefficient \u03bb determines the strength of interpolation . When \u03bb approximates 0 or 1 , the generated sequence will be similar to one of the parent sequences , while the \u03bb around 0.5 produces relatively diverse generation . However , generating diverse sequences means lowquality sequences can be generated , which can provide noisy contextual information and hurt model performance . To maintain the quality of mixed sequences , we set a discriminator to score the perplexity of the sequences . The final generated sequences will consist of only the sequences that pass the sequence quality screening . For the screening , we utilize a language model GPT - 2 ( Radford et al , 2019 ) to score sequence x by computing its perplexity : Perplexity ( x ) = 2 \u2212 1 T T i=1 log p ( w i ) , ( 9 ) where T is the number of tokens before padding , w i is the i - th token of sequence x. Based on the perplexity and a score range [ s 1 , s 2 ] , the discriminator can give judgment for sequence x : d ( x ) = 1 { s 1 \u2264 Perplexity ( x ) \u2264 s 2 } . ( 10 ) The lower the perplexity score , the more natural the sequence . However , the discriminator should also consider the regularization effectiveness and the generation capacity . Hence , a blind low perplexity setting is undesirable . The overall sequence mixup and selection procedure is illustrated in Algorithm 2 .", "entities": [[2, 3, "MethodName", "mixup"], [17, 18, "DatasetName", "0"], [82, 83, "MetricName", "perplexity"], [113, 114, "MethodName", "GPT"], [130, 131, "MetricName", "perplexity"], [132, 133, "MetricName", "Perplexity"], [177, 178, "MetricName", "perplexity"], [209, 210, "MetricName", "Perplexity"], [224, 225, "MetricName", "perplexity"], [253, 254, "MetricName", "perplexity"], [261, 262, "MethodName", "mixup"]]}
{"text": "Datasets . We conduct experiments on three sequence labeling datasets for the named entity recognition ( NER ) and event detection tasks . ( 1 ) CoNLL - 03 ( Tjong Kim Sang and De Meulder , 2003 ) is a corpus for NER task . It provides four named entity types : persons , locations , organizations , and miscellaneous . 2 ( 2 ) ACE05 is a corpus for event detection . It provides 8 event types and 33 subtypes . We study the event trigger detection problem , which aims to identify trigger tokens in a sentence . ( 3 ) Webpage ( Ratinov and Roth , 2009 ) is a NER corpus with 20 webpages related to computer science conference and academic websites . It inherits the entity types from CoNLL - 03 . Data Split . To investigate low - resource sequence labeling , we randomly take 700 labeled sentences from the original CoNLL - 03 dataset as the training set . For ACE05 and WebPage dataset , the annotation is sparse , so we conduct experiments on their original dataset without further slicing . We set 6 data usage percentiles for the training set in each corpus . The sequence model is initialed on a small seed set , then it performs five iterates of active learning . For the query policy , we use random sampling and the three active learning policies mentioned in Section 2.2 . The machine learning performance is evaluated by F 1 score for each data usage percentile . Parameters . We use BERT - base - cased for the NER task as the underlying model , and BERT - basemultilingual - cased for the event trigger detection task . We set the max length as 128 to pad the varying - length sequences . The learning rate of the underlying model is 5e - 5 , and the batch size is 32 . We train them for 10 epochs at each data usage percentile . For the parameters of SeqMix , we set \u03b1 = 8 to sample \u03bb from Beta ( \u03b1 , \u03b1 ) . We use the sub - sequence window length s = { 5 , 5 , 4 } , the valid label density \u03b7 0 = { 0.6 , 0.2 , 0.5 } for CoNLL - 03 , ACE05 and Webpage , respectively . The augment rate is set as 0.2 , and the discriminator score range is set as ( 0 , 500 ) . We also perform a detailed parameter study in Section 4.4 .", "entities": [[12, 15, "TaskName", "named entity recognition"], [16, 17, "TaskName", "NER"], [19, 21, "TaskName", "event detection"], [43, 44, "TaskName", "NER"], [71, 73, "TaskName", "event detection"], [114, 115, "TaskName", "NER"], [221, 223, "TaskName", "active learning"], [236, 238, "TaskName", "active learning"], [264, 265, "MethodName", "BERT"], [271, 272, "TaskName", "NER"], [279, 280, "MethodName", "BERT"], [307, 309, "HyperparameterName", "learning rate"], [320, 322, "HyperparameterName", "batch size"], [345, 346, "HyperparameterName", "\u03b1"], [354, 355, "HyperparameterName", "\u03b1"], [356, 357, "HyperparameterName", "\u03b1"], [382, 383, "DatasetName", "0"], [419, 420, "DatasetName", "0"]]}
{"text": "The main results are presented in Figure 2 , where we use NTE sampling as the default active learning policy . From the result , it is clear that our method achieves the best performance consistently at each data usage percentile for all three datasets . The best SeqMix method ( sub - sequence mixup with NTE sampling ) outperforms the strongest active learning baselines by 2.95 % on CoNLL - 03 , 2.27 % on ACE05 and 3.75 % on WebPage in terms of F 1 score on average . Moreover , the augmentation advantage is especially prominent for the seed set initialization stage where we only have a very limited number of labeled data . Through the augmentation , we improve the model performance from 68.65 % to 80.71 % , where the seed set is 200 labeled sequences and the augmentation provides extra 40 data points for CoNLL - 03 . The improvement is also significant on ACE05 ( 40.65 % to 49.51 % ) , and WebPage ( 55.18 % to 71.67 % ) , which indicates that our SeqMix can largely resolve the label scarcity issue in low - resource scenarios . We also perform statistical significance tests for the above results . We use Wilcoxon Signed Rank Test ( Wilcoxon , 1992 ) , a non - parametric alternative to the paired t - test . This significance test fits our task as F - score is generally assumed to be not normally distributed ( Dror et al , 2018 ) , and nonparametric significance tests should be used in such a case . The results show that sub - sequence mixup and label - constrained sub - sequence mixup can provide a statistical significance ( the confidence level \u03b1 = 0.05 and the number of data points N = 6 ) for all the comparisons with active learning baselines on used datasets . The whole - sequence mixup passes the statistical significance test with \u03b1 = 0.1 and N = 6 on CoNLL - 03 and WebPage , but fails on ACE05 . Among all the three SeqMix variants , subsequence mixup gives the overall best performance ( label - constrained sub - sequence mixup achieves very close performance with sub - sequence mixup on ACE05 dataset ) , but whole - sequence mixup does not yield a consistent improvement to the original active learning method . This is because the whole - sequence mixup may generate semantically poor new sequences . Instead , the sub - sequencelevel process reserves the original context information between the sub - sequence and the other parts of the whole sequence . Meanwhile , the updated sub - sequences inherit the original local informativeness , and introduce linguistic diversity to enhance the model 's generalization ability . To justify that SeqMix can provide improvement to the active learning framework with various query policies , we employ different query policies with SeqMix augmentation under the same experiment setting as Figure 2 ( a ) . From Figure 3 , we find that there is a consistent performance improvement when employing SeqMix with different query policies . As SeqMix achieves { 2.46 % , 2.85 % , 2.94 % } performance gain for random sampling , LC sampling and NTE sampling respectively .", "entities": [[17, 19, "TaskName", "active learning"], [54, 55, "MethodName", "mixup"], [62, 64, "TaskName", "active learning"], [277, 278, "MethodName", "mixup"], [285, 286, "MethodName", "mixup"], [295, 296, "HyperparameterName", "\u03b1"], [313, 315, "TaskName", "active learning"], [324, 325, "MethodName", "mixup"], [331, 332, "HyperparameterName", "\u03b1"], [358, 359, "MethodName", "mixup"], [371, 372, "MethodName", "mixup"], [380, 381, "MethodName", "mixup"], [390, 391, "MethodName", "mixup"], [400, 402, "TaskName", "active learning"], [411, 412, "MethodName", "mixup"], [479, 481, "TaskName", "active learning"]]}
{"text": "To verify the effectiveness of the discriminator , we conduct the ablation study on a subset of CoNLL - 03 with 700 labeled sequences . We use subsequence mixup with NTE sampling as the backbone and change the perplexity score range of the discriminator . We start from the seed set with 200 labeled data , then actively query 100 data in each learning round and repeat 5 rounds in total . The result in Table 1 demonstrates the discriminator provides a stable improvement for the last four data usage percentiles , and the discriminator with score range ( 0 , 500 ) can boost the model by 1.07 % F 1 score , averaged by all the data usage percentiles . The comparison between 3 different score thresholds demonstrates the lower the perplexity , the better the generation quality . As a result , the final F 1 score becomes higher with the better generated tokens . Actually , we can further narrow down the score range to get more performance improvement in return , but the too strict constraints will slow down the generation in practice and reduce the number of generated samples .", "entities": [[28, 29, "MethodName", "mixup"], [38, 39, "MetricName", "perplexity"], [99, 100, "DatasetName", "0"], [133, 134, "MetricName", "perplexity"]]}
{"text": "In this subsection , we study the effect of several key parameters . Augment rate r. We vary the augment rate r = | L * | | \u03c8 ( U , K , \u03b3 ( ) ) | in { 0 .2 , 0.4 , 0.6 , 0.8 , 1.0 } and keep the number of initial data usage same to investigate the effect of augment rate for data augmentation . Table 2 shows that r \u2264 0.6 can provide better F 1 improvement . The model with r = 0.2 surpasses the model with r = 1.0 by 0.73 % , evaluated by the average F 1 score for all the data usage percentiles . This result indicates that the model appreciates moderate augmentation more . However , the performance variance based on the augment rate is not prominent compared to the improvement provided by SeqMix to the active learning framework . Valid tag density \u03b7 0 . We search the valid tag density \u03b7 0 as Section 3.2 defined by varying the sub - sequence window length s and the required number of valid tag n within the window . The results in Figure 4 ( a ) illustrate the combination ( s = 5 , n = 3 ) outperforms other settings . When s is too small , the window usually truncates the continuous clause , thus cutting off the local syntax or semantic information . When s is too large , sub - sequence mixup tends to behave like wholesequence mixup , where the too long sub - sequence generation can hardly maintain the rationality of syntax and semantics as before . The high \u03b7 0 with long window length may result in an insufficient amount of eligible parent sequences . Actually , even with a moderate augment rate \u03b1 = 0.2 , the combination ( s = 6 , n = 5 ) has been unable to provide enough generation . Mixing parameter \u03b1 . We show the performance with different \u03b1 in Figure 4 ( b ) . The parameter \u03b1 decides the distribution \u03bb \u223c Beta ( \u03b1 , \u03b1 ) , and the coefficient \u03bb directly involved the mixing of tokens and labels . Among the values { 0.5 , 1 , 2 , 4 , 8 , 16 } , we observed \u03b1 = 8 presents the best performance . It outperforms the second - best parameter setting 0.49 % by average . From the perspective of Beta distribution , larger \u03b1 will make the sampled \u03bb more concentrated around 0.5 , which assigns more balance weights to the parent samples to be mixed . In this way , the interpolation produces encoded token with further distance to both the parent samples , thus introduces a more diverse generation .", "entities": [[34, 35, "HyperparameterName", "\u03b3"], [41, 42, "DatasetName", "0"], [69, 71, "TaskName", "data augmentation"], [150, 152, "TaskName", "active learning"], [158, 159, "DatasetName", "0"], [167, 168, "DatasetName", "0"], [250, 251, "MethodName", "mixup"], [256, 257, "MethodName", "mixup"], [281, 282, "DatasetName", "0"], [305, 306, "HyperparameterName", "\u03b1"], [330, 331, "HyperparameterName", "\u03b1"], [338, 339, "HyperparameterName", "\u03b1"], [348, 349, "HyperparameterName", "\u03b1"], [356, 357, "HyperparameterName", "\u03b1"], [358, 359, "HyperparameterName", "\u03b1"], [393, 394, "HyperparameterName", "\u03b1"], [422, 423, "HyperparameterName", "\u03b1"]]}
{"text": "Figure 5 presents a generation example via subsequence mixup . For the convenience of presentation , we set the length of sub - sequence s = 3 and the valid label density threshold \u03b7 0 = 2 3 . The two input sequences get paired for their eligible sub - sequences \" COLORADO 10 St \" and \" Slovenia , Kwasniewski \" . The subsequences are mixed by \u03bb = 0.39 in this case , which is sampled from Beta ( \u03b1 , \u03b1 ) . Then the generated sub - sequence \" Ohio ( novelist \" replaces the original parts in the two input sequences . Among the generated tokens , \" Ohio \" inherits the label B - ORG from \" COLORADO \" and the label B - LOC from \" Slovenia \" , and the distribution Beta ( \u03b1 , \u03b1 ) assigns the two labels with weights \u03bb = 0.39 and ( 1 \u2212 \u03bb ) = 0.61 . The open parenthesis is produced by the mixing of a digit and a punctuation mark , and keeps the label O shared by its parents . Similarly , the token \" novelist \" generated by \" St \" and \" Kwasniewski \" gets a mixed label from B - ORG and B - PER . The discriminator then evaluates the two generated sequences . The generated sequence i is not reasonable enough intuitively , and its perplexity score 877 exceeds the threshold , so it is not added into the training set . The generated sequence j retains the original syntax and semantic structure much better . Although the open parenthesis seems strange , it plays a role as the comma in the original sequence to separate two clauses . This generation behaves closely to a normal sequence and earns 332 perplexity score , which permits its incorporation into the training set .", "entities": [[8, 9, "MethodName", "mixup"], [34, 35, "DatasetName", "0"], [81, 82, "HyperparameterName", "\u03b1"], [83, 84, "HyperparameterName", "\u03b1"], [141, 142, "HyperparameterName", "\u03b1"], [143, 144, "HyperparameterName", "\u03b1"], [239, 240, "MetricName", "perplexity"], [304, 305, "MetricName", "perplexity"]]}
{"text": "The key parameters setting in our framework are stated here : ( 1 ) The number of active learning round is 5 for all the three datasets , but the size of seed set and the number of samples in each round differs from the dataset . We list the specific numbers as Table 3 . ( 2 ) The sub - sequence window length s and the valid label density threshold \u03b7 0 vary from the datasets . For CoNLL - 03 , s = 5 , \u03b7 0 = 0.6 ; for ACE05 , s = 5 , \u03b7 0 = 0.2 ; for Web - Page , s = 4 , \u03b7 0 = 0.5 . ( 3 ) We set \u03b1 = 8 for the Beta distribution . ( 4 ) The discriminator score range is set as ( 0 , 500 ) for all the datasets . ( 5 ) For BERT configuration , we choose 5e - 5 for learning rate , 128 for padding length , 32 for batch size , 0.1 for dropout rate , 1e - 8 for in Adam . At each data usage point , we train the model for 10 Epochs . ( 6 ) We set C = 3 for the QBC query policy .", "entities": [[17, 19, "TaskName", "active learning"], [36, 39, "HyperparameterName", "number of samples"], [73, 74, "DatasetName", "0"], [89, 90, "DatasetName", "0"], [101, 102, "DatasetName", "0"], [115, 116, "DatasetName", "0"], [124, 125, "HyperparameterName", "\u03b1"], [143, 144, "DatasetName", "0"], [156, 157, "MethodName", "BERT"], [165, 167, "HyperparameterName", "learning rate"], [175, 177, "HyperparameterName", "batch size"], [188, 189, "MethodName", "Adam"]]}
{"text": "We take following criteria to evaluate the sequence labeling task . A named entity is correct only if it is an exact match of the corresponding entity in the data file . An event trigger is correct only if the span and type match with golden labels . Based on the above metric , we evaluate F 1 score in our experiments .", "entities": [[21, 23, "MetricName", "exact match"]]}
{"text": "refers to the number of labeled data , excluding the augmentation data . Sub - sequence mixup is trained with ( 1+\u03b1 ) times data , where the \u03b1 denotes the augment rate . Note that WebPage is a very limited dataset , there is a big difference between the performance on the validation set and the test set . We average each experiment by 5 times .", "entities": [[16, 17, "MethodName", "mixup"], [28, 29, "HyperparameterName", "\u03b1"]]}
{"text": "For the discriminator score range , we first examine the perplexity score distribution of the CoNLL training set . Then determine an approximate score range ( 0 , 2000 ) first . We linearly split score ranges below 2000 to conduct parameter study and report the representative ranges in Section 4.3 . Given the consideration to the generation speed and the augment rate setting , we finally choose 500 as the upper limit rather than a too narrow score range setting . For the mixing coefficient \u03bb , we follow ( Zhang et al , 2018 ) to sample it from Beta ( \u03b1 , \u03b1 ) and explore \u03b1 ranging from [ 0.5 , 16 ] . We present this parameter study in Section 4.4 . The result shows different \u03b1 did not influence the augmentation performance much . For the augment rate and the valid tag density , we also have introduced the parameter study in Section 4.4 .", "entities": [[10, 11, "MetricName", "perplexity"], [26, 27, "DatasetName", "0"], [103, 104, "HyperparameterName", "\u03b1"], [105, 106, "HyperparameterName", "\u03b1"], [109, 110, "HyperparameterName", "\u03b1"], [131, 132, "HyperparameterName", "\u03b1"]]}
{"text": "[ name ] Is that for when people ca n't travel due to your staff having to strike to keep everyone safe ? Or perhaps short formed trains that you ca nt get on . sure that the lexicon in T r is different from that of C. Namely , we calculated the cosine similarity between the two datasets in the tf - idf space . The cosine similarity at a value of 0.028 was statistically significant with a Pearson correlation coefficient value 0.012 ( p - value 0.0034 ) ( Schober et al , 2018 ) .", "entities": [[79, 81, "MetricName", "Pearson correlation"]]}
{"text": "We trained a logistic regression model for complaint detection using each one of the features described in section 4.1 . Table 3 summarizes the results in terms of accuracy and macro averaged F1 - score . The best performing model is based on unigrams , with an accuracy of 75.3 % . There is not a significant difference in the performance of different sentiment models . It is also interesting to observe that simple features like the counts of different pronoun types and counts of intensifiers have strong predictive ability . Overall , we observe that most of the features studied here have some ability to predict complaints .", "entities": [[3, 5, "MethodName", "logistic regression"], [28, 29, "MetricName", "accuracy"], [32, 35, "MetricName", "F1 - score"], [47, 48, "MetricName", "accuracy"]]}
{"text": "This paper presents our submission for the SemEval shared task 6 , sub - task A on the identification of offensive language . Our proposed model , C - BiGRU , combines a Convolutional Neural Network ( CNN ) with a bidirectional Recurrent Neural Network ( RNN ) . We utilize word2vec to capture the semantic similarities between words . This composition allows us to extract long term dependencies in tweets and distinguish between offensive and non - offensive tweets . In addition , we evaluate our approach on a different dataset and show that our model is capable of detecting online aggressiveness in both English and German tweets . Our model achieved a macro F1 - score of 79.40 % on the SemEval dataset .", "entities": [[29, 30, "MethodName", "BiGRU"], [115, 118, "MetricName", "F1 - score"]]}
{"text": "In addition to Twitter data provided by the organizers of the SemEval shared task , we further evaluate our approach on German tweets from the GermEval ( 2018 ) shared task . The OLID dataset contains 13 , 240 tweets , with 4 , 400 offensive and 8 , 840 non - offensive tweets ( 66.77 % offensive , 33.23 % non - offensive ) . Similarly , the GermEval dataset contains 5 , 009 tweets , divided into 1 , 688 offensive and 3 , 321 non - offensive tweets ( 66.30 % offensive , 33.70 % non - offensive ) . To compensate for the imbalanced class distributions and weigh each class equally , we choose the macro averaged F1 - score of both classes as our main evaluation metric . From both data sets we use 10 % of our tweets as test set . The remaining tweets are split into 90 % training set and 10 % validation set . We conduct a stratified 10 - fold cross - validation on the training and validation set to prevent overfitting and to validate our model . The pretrained w2v model , which is used to initialize the weights of our embedding layer , resulted from the work of Godin et al ( 2015 ) . The w2v model for the GermEval dataset originates from our previous work ( 2018 ) . For comparison to our proposed model , a token bag - of - n - gram model composed of unigrams , bigrams , and trigrams weighted by their TF - IDF is used as baseline approach . We subsequently analyze the performance of different classifiers on the resulting feature space . We have used the packages keras , scikit - learn , gensim , and nltk for preprocessing and the implementation of our models .", "entities": [[33, 34, "DatasetName", "OLID"], [121, 124, "MetricName", "F1 - score"]]}
{"text": "After the preprocessing step , we construct a dictionary which maps all unique tokens to their number of occurrences in the respective corpus . Tokens which appear only once in a corpus are disregarded and treated as unknown token . As a next step , we construct the weighting matrix W m\u00d7dim for our embedding layer , where dim is the dimension of the used w2v model and m the number of unique tokens t i , i { 1 , ... , m } . The word vector of t i is stored in W if the token is represented in the w2v model . If t i has no pretrained word vector , we generate a random vector drawn from the uniform distribution within \u2212 6 dim , 6 dim as suggested by He et al ( 2015 ) . We fix the maximum length of a sentence to 150 tokens , longer sequences are clipped at the end and shorter sequences are padded with a masking token . The convolutional layer of our classifier consists of ( k \u00d7 128 ) 1 - dimensional filters , where k is the number of different window sizes . These window sizes range from 2 to 5 and allow the extraction of n - gram features . The padding of the input is kept constant , resulting in the same output sequence length as the input . We further choose ReLu as activation function . The resulting feature maps are concatenated and passed towards the recurrent layer . Gated Recurrent Units ( GRU ) as initially proposed by are used in RNNs to capture long - term dependencies of input sequences . Similar to Long Short - Term Memory ( LSTM ) units ( Hochreiter and Schmidhuber , 1997 ) GRU are able to overcome the vanishing gradient problem by using a gating mechanism . GRU have shown to achieve comparable results to LSTM in sequence modeling tasks and are able to outperform the latter on smaller data sets ( Chung et al , 2014 ) . The recurrent layer in our model consists of a bidirectional GRU , where the concatenated feature maps , which resulted from the convolutional layer , are used as input for the GRU layer . Simultaneously , the reversed copy of the input sequence is used for the second GRU layer . Both GRU layers return a hidden state for each processed feature map . The output of both layers is then concatenated . We set the length of the returned hidden states to 64 for both layers , resulting in an output space of ( 150 \u00d7 128 ) neurons . Afterwards , a global max pooling layer reduces the output space to ( 1 \u00d7 128 ) nodes . The following fully - connected layer consists of 32 neurons , which connect to a single output neuron . The output neuron utilizes the sigmoid activation function . To additionally prevent overfitting , we include two dropout layers with a dropout rate of 0.2 ; one after the embedding layer and another one after the fully - connected layer . Furthermore , we adopt early stopping and use 10 % of the training data as validation split . We use cross entropy as error function for our model and the optimizer ' adam ' to update our network weights ( Kingma and Ba , 2014 ) . The batch size for the gradient update is set to 32 . A schema of our proposed model is illustrated in Figure 1 .", "entities": [[240, 241, "MethodName", "ReLu"], [242, 244, "HyperparameterName", "activation function"], [262, 263, "MethodName", "GRU"], [284, 289, "MethodName", "Long Short - Term Memory"], [290, 291, "MethodName", "LSTM"], [300, 301, "MethodName", "GRU"], [315, 316, "MethodName", "GRU"], [323, 324, "MethodName", "LSTM"], [356, 358, "MethodName", "bidirectional GRU"], [378, 379, "MethodName", "GRU"], [395, 396, "MethodName", "GRU"], [399, 400, "MethodName", "GRU"], [452, 454, "MethodName", "max pooling"], [491, 493, "MethodName", "sigmoid activation"], [531, 533, "MethodName", "early stopping"], [557, 558, "HyperparameterName", "optimizer"], [575, 577, "HyperparameterName", "batch size"]]}
{"text": "For the comparison model , the SVM performs best on the OLID dataset with an F1 - score of 70.22 % averaged over a 10 - fold cross - validation . The SVM also shows the best results on the GermEval dataset with an F1 - score of 66.61 % . The evaluation on the test set results in 66.78 % F1 - score for the GermEval gold test set . The evaluation of the baseline model for the OLID gold test set is not possible at the time of writing , since the gold test data have not yet been released . The C - BiGRU achieved a 76.28 % F1 - score on the OLID and a 71.13 % F1 - score on the GermEval dataset on average over a 10 - fold cross - validation . On the OLID gold test set , our model achieved an F1 - score of 79.40 % . The evaluation on the GermEval gold test data resulted in a 72.41 % F1score . An overview of all results can be found in Table 1 . Figure 2 shows the confusion matrix of our submitted predictions for the SemEval shared task .", "entities": [[6, 7, "MethodName", "SVM"], [11, 12, "DatasetName", "OLID"], [15, 18, "MetricName", "F1 - score"], [32, 33, "MethodName", "SVM"], [44, 47, "MetricName", "F1 - score"], [61, 64, "MetricName", "F1 - score"], [79, 80, "DatasetName", "OLID"], [106, 107, "MethodName", "BiGRU"], [111, 114, "MetricName", "F1 - score"], [116, 117, "DatasetName", "OLID"], [121, 124, "MetricName", "F1 - score"], [141, 142, "DatasetName", "OLID"], [150, 153, "MetricName", "F1 - score"]]}
{"text": "The presented model continues our work on the identification of offensive German tweets ( 2018 ) . We were able to improve our proposed model by adjusting the architecture of the recurrent layer in our neural network . By using a bidirectional GRU instead of a unidirectional LSTM , we are able to capture past and future information about the input sequence and exploit the better performance of GRU networks on smaller datasets . Furthermore , we return the hidden states for each feature map instead of returning only the last hidden state . This allows us to extract higher - level sequentially dependent features from each concatenated feature map . Our experiments show that our suggested model outperforms the baseline model on both datasets . The difference between the F1 - scores for the English and German dataset might be attributed to the smaller size of the German training set , which contains only about 5 , 000 tweets . The discrepancy between the results of our cross - validation and achieved score on the OLID test set might be explained by the small amount of test tweets , which may lead to imprecise results for the submitted runs . By utilizing w2v as features , we are able to limit extensive and language specific preprocessing . \" @USER Lolol God he is such an a**hole . \" In this example , the vector representation of \" a**hole \" has a high cosine similarity ( 0.63 ) to the vector representation of \" asshole \" , which allows our model to classify this tweet as offensive . On the contrary , our approach falls short when confronted with indirect insults . \" @USER @USER I m sure the air that he is breathing is also bad . \" Our model wrongly predicts a non - offensive tweet in this instance . The detection of offensive , hateful , racist , and/or sexist user behavior in social media still proves to be a challenge . Even for humans , it can be problematic to identify offensive microposts , since these posts can be ambiguous and dependant on the personal mindset of a reader . Ross et al ( 2017 ) show that it can be difficult to measure the agreement of annotators about hate speech in the light of the European refugee crisis . They conclude that instead of a classification problem , a regression model with an average offensiveness score of multiple annotators might be more suitable for this task . Furthermore , it can be difficult to grasp the full context of an arbitrary tweet . With only excerpts of a conversation , the context and true intention of the author may be difficult to determine .", "entities": [[41, 43, "MethodName", "bidirectional GRU"], [47, 48, "MethodName", "LSTM"], [68, 69, "MethodName", "GRU"], [130, 131, "MetricName", "F1"], [176, 177, "DatasetName", "OLID"], [383, 385, "DatasetName", "hate speech"]]}
{"text": "In this paper , we describe our submitted model for the SemEval shared task 6 and evaluation methods for the identification of online aggressiveness in social media microposts . Our model achieves good results in the two evaluated datasets . For the OLID dataset which contains English tweets , a macro F1 - score of 79.40 % is reached , while our network resulted in an F1 - score of 72.41 % on the GermEval dataset , which consists of German tweets . We plan to evaluate our approach on more datasets to further investigate the potential of our model for different languages . One such set is the TRAC dataset , which contains aggressionannotated Facebook posts and comments in Hindi . Furthermore , we want to examine whether additional features such as character - level embeddings or POS tagging will improve our results . Inclusion of figurative language detection has proved to enhance many NLP tasks , such as argument mining and so - called hidden hate speech ( Mitrovi\u0107 et al , 2017 ) , which is also one of our future directions .", "entities": [[42, 43, "DatasetName", "OLID"], [51, 54, "MetricName", "F1 - score"], [66, 69, "MetricName", "F1 - score"], [160, 162, "TaskName", "argument mining"], [167, 169, "DatasetName", "hate speech"]]}
{"text": "We begin by creating a lexicon of words that are commonly used to refer to COVID - 19 . This allows us to determine the extent to which users in each subreddit are discussing COVID - 19 , and also gives us a clearer idea of when COVID - 19 began to directly affect discussion in the mental health subreddits . We based the lexicon on a set of twitter search keywords from Huang et al ( 2020 ) , and added six additional words that we believed would be indicative of discussion about COVID - 19 ( see the full lexicon in Appendix A ) . To study changes in the number of users seeking mental health support in subreddits , we record the author usernames for each post in our dataset . Since individuals can create multiple accounts under different usernames , the number of unique usernames associated with posts is likely not equal to the true number of unique users ; however , it is a reasonable proxy . To study changes in content that occur during the pandemic , we use the LIWC lexicon ( Pennebaker et al , 2015 ) and Latent Dirichlet Allocation ( LDA ) topic modeling ( Blei et al , 2003 ) . The LIWC lexicon consists of seventy - three hierarchical psycholinguistic word categories , encapsulating properties including linguistic categories ( e.g. , 1st person plural pronouns , verbs ) , emotions ( e.g. , anxiety , sadness ) , time ( e.g. , present , future ) , and personal concerns ( e.g. , work , money , death ) . To capture the discussion topics that are common in the r / Anxiety , r / depression , and r / SuicideWatch subreddits specifically , we train a topic model on posts from these subreddits . We ensure that discussions from each of the subreddits are equally represented in our training dataset by randomly downsampling the posts from the subreddits with more data . We use the implementation of LDA topic modeling provided in the MALLET toolkit ( McCallum , 2002 ) and train models with k = 5 , 10 , .. , 40 topics . We select a single model to use in our analysis by examining their coherence scores , a measure of the semantic similarity of high probability words within each topic ( Mimno et al , 2011 ) . As coherence scores tend to increase with increasing k , we select k as the first local maxima of coherence scores , which we found to be k = 25 . In Appendix B , we show the 25 topics obtained from our topic model , along with the highest probability words associated with each topic . We also provide labels that summarize the essence of each topic , which we created by examining their representative words . Common themes of discussion include : daily life concerns ( e.g. , school , work , sleep and routine ) , personal relationships ( e.g. , friends , family , relationships ) , and mental health struggles ( e.g. , anxiety , suicide , medical treatment ) . When using text from posts , we remove special characters and sequences , such as newlines , quotes , emails , and tables . To represent the text of a post , we concatenate the title with the text content , as was done in prior work ( Chakravorti et al , 2018 ) . We apply additional pre - processing steps for our topic modeling analysis : ( 1 ) we remove a set of common stopwords that do not appear in the LIWC lexicon ( we kept those in LIWC as they have been found to have psychological meaning ) , ( 2 ) we form bigrams from pairs of words that commonly appear together , and ( 3 ) we lemmatize each word .", "entities": [[200, 201, "MethodName", "LDA"], [341, 342, "MethodName", "LDA"], [358, 360, "HyperparameterName", "k ="], [389, 391, "TaskName", "semantic similarity"], [433, 435, "HyperparameterName", "k ="]]}
{"text": "We report the ten topics with the highest proportion of outliers for each subreddit during the COVID - 19 period ( March to May 2020 ) in Table 3 . One notable trend is an increase in the amount of discussion related to family ; we find that the FAM - ILY AND HOME topic increased significantly in all three subreddits and the FAMILY AND CHIL - DREN topic increased significantly in r / depression . Figure 4b shows how the usage of the FAMILY AND HOME topic has changed since January 2019 within each subreddit . While there are noticeable increases in all three subreddits , we a see particularly large spike in r / Anxiety starting around mid - March . Within all subreddits , we see a significant decrease in the TRANSPORT AND DAILY LIFE topic ( see Figure 4c ) , which is associated with words such \" drive , \" \" car , \" \" time , \" and \" day \" . Mirroring the reduction of WORK - related language we observed in Section 5.3.1 , we also find that there has been a significant decrease in discussion of the SCHOOL and WORK topics within the r / Anxiety and r / depression subreddits . We observe significant changes in topics that are explicitly related to mental health . One of the most prominent trends is a significant increase in discussions of ANXIETY and its symptoms ( keywords include : \" panic , \" \" heart , \" and \" chest \" ) . As seen in Figure 4a , we see a spike in ANXIETY in mid - March in all three subreddits ; however , whereas we see a return to a typical level in both r / depression and r / SuicideWatch , within the r / Anxiety , ANX - IETY discussion rates have remained abnormally high all the way through the end of May . We find that both INFORMATION SHARING ( keywords include : \" post , \" \" read , \" \" share , \" \" find , \" and \" hope \" ) and COMMUNICATION ( keywords include : \" talk , \" \" call , \" and \" message \" ) have become more frequent topics of discussion . Discussion Several of the results in Table 3 seem to reflect the disruption to normal daily life caused by COVID - 19 and the resulting quarantine measures . This includes the increase in the FAMILY AND CHILDREN topic ( Figure 4b ) , which is largely expected , as quarantine policies implemented to help contain COVID - 19 have resulted in many people spending more time at home and with family than they previously had . While there are noticeable increases in all three subreddits , we see a particularly large spike in r / Anxiety starting around mid - March . Prior studies on disease outbreaks have found that uncertainty regarding the wellbeing of loved ones is a common source of anxiety during epidemics , which may help to explain this finding ( Chew et al , 2020 ) . Another contributing factor may be the emergence of new family responsibilities , such as childcare and home - schooling , that many people have had to take on in the face of closures caused by the pandemic . The decrease in the TRANSPORT AND DAILY LIFE topic ( Figure 4c ) is intuitive ; quarantine practices following COVID - 19 have led to a large reduction in driving and other forms of transportation ( Domonoske and Adeline , 2020 ) and , more generally , to a disruption in daily lifestyles . To the extent that these results indicate an abandonment of routine , they are somewhat concerning , as evidence from prior outbreaks suggests that getting back into normal routines helps to reduce loneliness and anxiety during quarantines ( Huremovi\u0107 , 2019 ) . The decreases in discussion of the SCHOOL and WORK topics may indicate that these previously common sources of stress have now become secondary concerns compared to the more immediate concerns associated with COVID - 19 . The increases in the ANXITETY topic , especially on r / Anxiety , are aligned with existing research that has found that anxiety and the somatic symptoms associated with it are common psychological responses to epidemics ( Chew et al , 2020 ) . Further , studies of prior epidemics have found that feelings of anxiety and fear can persist even after the disease itself has been contained ( Usher et al , 2020 ) . The increase in the INFORMATION SHARING and COMMUNICATION topics may be tied to the effects of social distancing measures , which have limited in - person interactions and led people to increasingly turn to digital methods of communication . These observations may also reflect a desire to seek out information related to COVID - 19 ; individuals who experience health anxiety are more likely to exhibit online health information seeking behavior ( McMullan et al , 2019 ) . The increase in mentions of words related to social media ( e.g. , \" post , \" \" share \" ) is somewhat worrisome ; studies of disaster events have found that both more frequent social media use and exposure to conflicting information online ( a widely acknowledged issue with COVID - Table 3 : Ten topics with the most outliers for r / Anxiety , r / depression , and r / SuicideWatch . Arrows mark the direction in which the mean of the outliers shifted from the predicted mean . Topics marked with * have a statistically significant percentage of outliers ( with Bonferroni correction ; \u03b1 = 0.05 before correction ) . 19 ( Kouzy et al , 2020 ) ) lead to higher stress levels ( Torales et al , 2020 ) . However , the rise of the INFORMATION SHARING topic , especially in it 's relation to words like \" share , \" \" hope , \" and \" story , \" could also be indicative of a collective coping process , in which individuals come together for social support . As noted in Section 5.3.1 , this type of coping strategy has frequently been observed during past disease outbreaks ( Chew et al , 2020 ) and may also be reflected by the increase in the usage of WE we saw for discussions in r / Anxiety .", "entities": [[959, 960, "HyperparameterName", "\u03b1"]]}
{"text": "We apply a one - sample proportion test to assess whether the proportion of observations outside of the prediction interval in the post - COVID period is significantly greater than 5 % . This test assumes that the observations are independent ; however , we find that there is order - 1 autocorrelation in our data . We therefore apply a correction for order - 1 autocorrelation ( Zwiers and Von Storch , 1995 ) when computing the z - test statistic . The corrected test statistic is : z = p \u2212 p 0 p 0 ( 1 \u2212 p 0 ) ( 1 + r ) /n ( 1 \u2212 r ) ( 2 ) wherep is the proportion of observations outside of the prediction interval in the post - COVID period , p 0 = 0.05 , n is the number of observations in the post - COVID period , and r is the lag - 1 correlation coefficient of the pre - COVID data . We use a Bonferroni correction when determining statistical significance for our discussion content metrics , as we ran almost 300 tests . M = 294 , which is the number of LIWC categories and topics , multiplied by the number of subreddits . Our corrected \u03b1 = 0.05/294 = 1.7 \u00d7 10 \u22125 .", "entities": [[94, 95, "DatasetName", "0"], [96, 97, "DatasetName", "0"], [101, 102, "DatasetName", "0"], [136, 137, "DatasetName", "0"], [214, 215, "HyperparameterName", "\u03b1"]]}
{"text": "We are grateful to the Michigan AI lab for discussions that led to this project , and to the statistical consultants at CSCAR who helped with developing our process for hypothesis testing , especially Kerby Shedden and Thomas Fiore . This material is based in part on work supported by the Precision Health initiative at the University of Michigan , the NSF ( grant # 1815291 ) , and the John Templeton Foundation ( grant # 61156 ) . Any opinions , findings , conclusions , or recommendations in this material are those of the authors and do not necessarily reflect the views of the Precision Health initiative , the NSF , or the John Templeton Foundation .", "entities": [[51, 52, "MetricName", "Precision"], [105, 106, "MetricName", "Precision"]]}
{"text": "We measure inter - annotator agreement with Krippendorff 's alpha ( Krippendorff , 2004 ) and find that , over all labels , there are substantial levels of agreement within groups of annotators : \u03b1 = 0.79 for trained annotators and \u03b1 = 0.71 and 72 for untrained annotators on the Yahoo and IAC threads , respectively . However , there is lower agreement on thread labels than comment labels ( Table 2 ) . The agreement of thread type is 25 % higher for the Yahoo threads than the IAC ( 0.62 - 0.64 compared to 0.48 ) . The less subjective comment labels ( i.e. , agreement , audience , and topic ) have higher agreement than persuasiveness , sentiment , and tone . While some of the labels have only moderate agreement ( 0.5 < \u03b1 < 0.6 ) , we find these results satisfactory as the agreement levels are higher than those reported for similarly subjective discourse annotation tasks ( e.g. , Walker et al ( 2012 ) ) . To evaluate the untrained annotators , we compare the thread - level annotations made on 300 Yahoo threads by both trained and untrained coders , by taking the majority label per item from each group of annotators and calculating the percent of exact matches ( Table 3 ) . When classifying the thread type , multiple labels are allowed for each thread , so we convert each option into a boolean and analyze them separately . Only 8 % of the threads have no majority constructive label in the trained and/or untrained annotations , and 20 % have no majority agreement label . Within both annotation groups , there are majority labels on all of the thread type labels . The category with the lowest agreement is constructive class with only 61 % of the majority labels matching , followed closely by agreement ( only 62 % matching ) . A very high percent of the thread type labels ( 81 % ) . The strong agreement levels between trained and untrained annotators suggest that crowdsourcing is reliable for coding thread - level characteristics .", "entities": [[9, 10, "HyperparameterName", "alpha"], [34, 35, "HyperparameterName", "\u03b1"], [41, 42, "HyperparameterName", "\u03b1"], [138, 139, "HyperparameterName", "\u03b1"], [159, 161, "DatasetName", "subjective discourse"]]}
{"text": "There has recently been huge interest in wellbeing , with a recent review arguing that psychological well - being plays a causal role in promoting job success , physical health , and long - term relationships ( Lyubomirsky et al , 2005 ; Kahneman , 1999 ) . In this paper we analyze a corpus of private micro - blogs from a well - being application called ECHO , with the aim to detect , understand , and fur - ECHO initiates user - written reactions to daily events , called RECORDINGS , as well as subsequent REFLECTIONS on those events at points in the future ( Isaacs et al , 2013 ) . 1 Each reaction is labelled at the time of recording or reflection by the user , the first - person experiencer , with a happiness rating from 1 and 9 . Note that all users ' posts and ratings are private , distinguishing this corpus from public sources like LiveJournal , where the content of posts might be influenced by considerations of self - presentation . Figure 1 shows a RECORDING and REFLECTION from two users , after binning the happiness ratings into positive and negative . Our goal is to ground the linguistic descriptions of events that users experience , such as those in Figure 1 , in theories of well - being and happiness . Without such a grounding , it is difficult for the ECHO system to make recommendations to users to improve their well - being , or to explain the relationships between different event types and well - being , or to develop a policy that can do a good job of selecting events for targeted reflection ( Konrad et al , 2015 ; Isaacs et al , 2013 ) . That is , for ECHO 's purposes , we need techniques that not only reliably categorize a user 's scalar happiness level , but are explanatory with respect to the sources of that happiness level . There are two principal challenges to this goal . First , different theories posit different sources for feelings of well - being and happiness . Second , the relevant computational resources for sentiment or mood are primarily lexically based , while many of the events can only be characterized well via their compositional semantics ( Reschke and Anand , 2011 ) . Other research also shares our motivation of understanding the relationship between what people say and their levels of happiness and related moods . Mishne ( 2005 ) used a corpus of 340 , 000 posts from Livejournal that were self - annotated with the 40 most common moods . Lexical features alone improved classification accuracy by 6 to 15 % over a balanced baseline . These results were then improved considerably ( Keshtkar and Inkpen , 2009 ) . Mihalcea and Liu ( 2006 ) experimented with the subset of happy / sad posts , and used conditional probability to explore the \" happiness factor \" of various terms , and the relationship of these terms to well - being categories such as human - centeredness and socialness . Schwartz et al ( 2016 ) extract 5 , 100 public status updates on Facebook and have Turkers annotate them using Seligman 's dimensions for well - being : Positive Emotions , Engagement , Relationships , Meaning , and Accomplish ( Seligman et al , 2006 ; Forgeard et al , 2011 ) . They then predict each dimension with lexical and LDA topic features . A related line of work builds lexico - semantic resources for sentiment analysis with a focus on how the participants of an event are affected by it . Goyal and Riloff ( 2013 ) bootstrap a set of patientpolarity verbs from narratives and Ding and Riloff ( 2016 ) extract event - triples from blogs that reliably indicate positive or negative affect on one of the event participants . Reed et al ( 2017 ) take a similar approach . Deng et al ( 2013 ) annotate how participants of an event are affected , and Deng & Wiebe ( 2014 ) show that this assists inference about the author 's sentiment towards entities or events . Balahur et al ( 2012 ) use the narratives produced by the ISEAR questionnaire ( Scherer et al , 1986 ) for first - person examples of particular emotions ( \" I felt angry when X and then Y happened \" ) and extract sequences of subject - verbobject triples , which they then annotate for seven basic emotions . Choi & Wiebe ( 2014 ) use Word - Net to try to learn similar patterns , and Rupenhofer & Brandes ( 2015 ) annotate synsets in Ger - maNet based on an event decomposition framework . Russo et al ( 2015 ) proposed a shared task for recognition of a set of pleasant and unpleasant events from a clinical framework for well - being ( MacPhillamy and Lewinsohn , 1982 ) . Work on AFINN , SentiWordNet and the Connotation Lexicon also aim to refine existing sentiment resources to capture more subtle notions of sentiment ( Feng et al , 2013 ; Kang et al , 2014 ; Baccianella et al , 2010 ; Nielsen , 2011 ) . Here we report an exploratory study where we synthesize theoretical constructs associated with well - being and happiness from different sources . We then develop several methods for characterizing events in terms of these theories . We examine the extent to which different theoretical accounts can explain the variance in the happiness scores in ECHO . We show that each theory explains a part of the variance , but that our event characterizations need to be more fine - grained . We show that several recurrent event types which affect people 's feelings of well - being , such as OBLIGATION and INCOMPETENCE , are not captured in current lexical or semantic resources .", "entities": [[452, 453, "MetricName", "accuracy"], [589, 590, "MethodName", "LDA"], [604, 606, "TaskName", "sentiment analysis"], [673, 676, "DatasetName", "Deng et al"], [722, 723, "DatasetName", "ISEAR"]]}
{"text": "ECHO is designed to encourage users to react to daily events as well as to periodically reflect on past events ( Isaacs et al , 2013 ) . Figure 2 depicts the user interface , showing a RECORDING from today , as well as prompts to reflect on events from the past . ECHO has been deployed with 134 users , in three different experiments on well - being ( Konrad et al , 2016b , a ) . The total corpus consists of 10354 posts , where 7573 are RECORDINGS and 2781 are REFLECTIONS . While the corpus could be considered relatively small , these posts provide a window onto users ' private thoughts as opposed to what users are willing to make public on social media . In addtion , the annotations for happiness are provided by the user , the first - person experiencer , and not by a third party . Our aim is to explain users ' emotional reactions to different categories of events mentioned in ECHO posts , linking the user reactions directly to theories of well - being as exemplified in Table 1 . Influential accounts such as Appraisal Theory ( Scherer et al , 2001 ( Scherer et al , , 1986Ortony et al , 1990 ) of successfully achieving goals . Appraisal theory posits that goal achievement promotes positive affect , which then serves to reinforce the relevant behavior . Row 2 provides an example of failing to achieve an important personal goal , which is posited to promote negative affect , motivating people to modify current behaviors to change that negative outcome . There are significant critiques of the adaptive goal - based account espoused in Appraisal theory . Appraisal theory focuses on short - term personal goals , but Eudaimonic psychologists instead focus on what determines long - term happiness . Eudaimonic theorists suggest that certain fundamental psychological needs have to be satisfied for people to experience sustained positive long - term emotions . Self - determination theory argues that there are 3 basic psychological needs : AUTONOMY , COMPETENCE and CONNECTION ( Deci and Ryan , 2010 ; Ryan and Deci , 2000 ; Bandura , 1977 ) . We add these to our inventory in Table 1 in Rows 3 to 8 . According to self - determination theory , satisfaction of these basic needs results in positive emotions . Row 3 describes a good day at work . Row 5 describes feeling competent because hard work led to an achievement , and Row 7 describes feeling connected with family . On the other hand , if these basic needs are not satisfied , then negative emotions will regularly arise . For example , obligations to do things one does not feel like doing ( Row 4 ) , or a job that does not engage personal decision making or involvement ( lack of autonomy ) can make one feel unhappy . Similarly , people may feel unhappy due to an experience where the demands of the situation outstrip one 's basic abilities , such as doing poorly on a test ( lack of competence ) , as in Row 6 . In addition , bad things happening to friends ( Row 8 ) as well as separation from family or friends often reduces happiness ( lack of connection ) . In addition , there is strong evidence from SAVOURING theory ( Jose et al , 2012 ; Bryant et al , 2011 ) arguing that people often experience highly positive or negative emotions arising from situations that are n't directly goal - related , and that relate more directly to basic drives ( Maslow , 1943 ; Elson , 2012 ) . For example , experiences such as eating , experiencing nature , sex and physical exercise tend to engender positive emotions , whereas pain , discomfort and inactivity have the opposite effects , and these are documented in results from happiness surveys ( Kahneman et al , 2004 ; Seligman et al , 2006 ) . Thus while experiences such as eating may serve the survival goal of preventing starvation , avoiding starvation is unlikely to be a direct personal goal every time we eat , suggesting that such experiences are not explained by Appraisal theory . Similar arguments have been made by Lewinsohn and colleagues who have shown that encouraging people to engage in certain simple activities ( shopping , mowing the lawn , driving , personal hygiene ) have quite predictable effects on mood without engaging significant personal goals ( MacPhillamy and Lewinsohn , 1982 ; Lewinsohn et al , 1985 ; Lewinsohn and Amenson , 1978 ) . We start with the 10354 posts from the ECHO corpus and map happiness scores between [ 1 , 4 ] to negative , and scores between [ 6 , 9 ] to positive . For posts labelled 5 by the experiencer , we categorize it as negative if its REFLECTION score decreases to lower than 5 , and positive if its REFLECTION score increases . We label the rest of the 5s as neutral , and leave them aside . We then have 5997 positive posts and 3573 negative posts . We randomly sample 2868 posts as training data , and 478 as test data . We keep the rest of the 6224 posts untouched for future work . Then we split the posts into sentences . Table 2 shows the splits for each class . We first test the separability of the positive and negative sentences with an SVM classifier from Weka 3.8 , using as baselines only unigrams and LIWC ( Pennebaker et al , 2001 ) illustrating that the positive and negative classes can be separated with F1 above .70 , and that both unigrams and LIWC perform worse on the negative class . However , as discussed above , the word level representations of the features in the baselines do not help us with our goal to understand how linguistic descriptions of events that affect wellbeing map onto theoretical constructs . and LIWC categories . We can not recommend to an ECHO user that they should for example , try to use the word why less ( Row 7 ) because it is correlated with negative feelings , or try to use less negation ( Rows 9 and 10 ) . It is difficult to associate these features with well - being classes . Even in cases where the words seem to be strongly related to a well - being category , a single word typically fails to provide enough information , e.g. , \" it was fun talking to him \" and \" worked on a fun project \" belong to different well - being classes . Moreover , the mapping of LIWC categories to words are many - to - many , e.g. the \" discrep \" category contains words related to both Goals and Autonomy . We posit that we need compositional semantic features to ground our a Well - Being classification of events . We thus explore two different methods for mapping these well - being event categories into lexical descriptions , one of which is top - down and the other which is bottom - up . Our top - down method is based on mapping general event types from FrameNet to the theoretical categories enumerated in Table 1 . We take frame specific features for each theoretical category from the lexical units for each frame . For example , GOALS are often dis - cussed in terms of specific frames from the Desiring and the Intentionally act classes , as shown in the first two rows of Table 6 . We show that FrameNet features do provide an interesting level of generalization but much of the compositional semantics of events is still missing from this characterization ( Section 4 ) . Thus , our bottom - up method applies the AutoSlog linguistic - pattern learner to induce lexically - grounded predicate patterns from the ECHO data ( Section 5 ) . We show how many light verbs acquire a specific semantics with their arguments , and how common events like \" Talking \" are separated into positive and negative events depending on whether they are \" Talking about \" or \" Talking with \" .", "entities": [[480, 482, "TaskName", "decision making"], [935, 936, "MethodName", "SVM"], [966, 967, "MetricName", "F1"], [1235, 1236, "DatasetName", "FrameNet"], [1299, 1300, "DatasetName", "FrameNet"]]}
{"text": "Table 6 provides our posited mapping from frame categories to the appraisal category of GOALS as well as to the eudaimonic categories of AUTON - OMY , COMPETENCE and CONNECTION , and to the hedonic category of SAVOURING . To develop features related to these frame categories , we apply SEMAFOR ( Das et al , 2013 ) to label the ECHO posts with their corresponding frames using FrameNet 1.5 ( Baker et al , 2015 ; Baker , 2014 ) . We partition frame features into subsets corresponding to the different theoretical constructs as defined in Table 6 . We acknowledge that our mapping may not be perfect , and that some frames could conceivably be categorized as both goal related and eudaimonic . We train an SVM with each feature subset , and evaluate the models on our test set , with results in Table 7 . The general ALL FRAME feature is also listed for comparison . The .67 F1 of FRAME is slightly lower than LIWC in Table 3 , but in our view , more interpretable . In addition , the average count of FRAME features per sentence is an order of magnitude less than LIWC features ( hence , much less than unigram features ) , suggesting the targeted power of these features . See Table 8 . We posit that FRAMES are thus more discriminative than LIWC for well - being classes , and that FRAME features are more naturally categorized into wellbeing categories at a semantic level . The Goals section of Table 7 shows that Appraisal theory does well at predicting positive events , but performs poorly for negative events , primarily due to low recall . All features achieve good F1 for the positive class , but not the negative class . This is consistent with the results in Table 3 . The EUDAIMONIC features include Autonomy & Obligation , Competence and Connection . The SVM trained with just eudaimonic features produces the highest F1 score for the negative class , highlighting the role of eudaimonic related events in negative well - being . See Table 7 . The results for an breaking eudaimonic into its constituent categories is in Table 9 . The results show that most of our autonomy categories are related to negative autonomy , to obligations that cause feelings of negative well - being . On the other hand , the results indicate that competence and connection play a large role in positive well - being . The top 25 most informative frame features are illustrated in Table 10 ( out of 639 instantiated in ECHO ) . These illustrate general events for well - being , but compositional differences , such as \" spending my nights by the side of my textbook \" and \" spending my nights with friends \" are not captured . The first \" spend ( time ) \" evokes the theoretical construct of obligation , while \" spend ( time with ) \" is related to connection .", "entities": [[68, 69, "DatasetName", "FrameNet"], [128, 129, "MethodName", "SVM"], [162, 163, "MetricName", "F1"], [290, 291, "MetricName", "F1"], [325, 326, "MethodName", "SVM"], [334, 336, "MetricName", "F1 score"]]}
{"text": "We also apply Autoslog - TS , a weakly supervised linguistic - pattern learner as a way of learning some compositional patterns . Autoslog only requires training documents labeled broadly into our two classes of POSITIVE or NEGATIVE . The learner uses a set of syntactic templates to define different types of linguistic expressions . In general , this method tends to produce high precision ( and potentially low recall ) markers of the particular classes that can seed further hypothesizing . The left - hand side of a specific lexico - syntactic pattern ( in bold ) that represents an instantiation of each general pattern template for learning well - being patterns in our data . 2 In order to enable selection of particular patterns , AutoSlog - TS computes statistics on the strength of association of each pattern with each class , i.e. P ( POSITIVE | p ) and P ( NEGATIVE | p ) , along with the pattern 's overall frequency . We define two tuning parameters for each class : \u03b8 f , the frequency with which a pattern occurs , \u03b8 p , the probability with which a pattern is associated with the given class . AutoSlog lets us systematically explore tradeoffs with precision and recall . Here we select \u03b8 f and \u03b8 p to optimize F1 on our test set . For more detail , see ( Riloff , 1996 ; Oraby et al , 2015 ) . Our primary interest here is Autoslog 's ability to learn compositional patterns . Autoslog can , in principle , provide three kinds of information : i ) it can provide supplement the lexical units for a given frame ; ii ) it can supplement the frames in a well - being category ; and iii ) it can reveal reliable markers of mood that well - being categories do not capture . Because our interest in frames is ultimately as a way of relating well - being categories with linguistic signals , we will not distinguish ( i ) and ( ii ) here . Here we discuss all patterns with a \u03b8 p > .7 Several lexicosyntactic patterns fit within our wellbeing categories but are not captured by frames , while as expected there are overlaps between FrameNet and Autoslog as well . Examples are listed in Table 12 . One large class includes straightforward lexical patterns : FINISHED , FIN - ISH , and FINALLY which we associate with feelings of comptence . Verbal patterns with EAT and ATE indicate savouring , with NOT EAT reliably marking negative sentences . The frames also show many specific types of food ( cake ) , and we use a comprehensive list from DBpedia ( Lehmann et al , 2014 ) to collapse all these to the general type FOOD , allowing us to develop patterns such as MADE FOOD . Autoslog also discovers many patterns syntactically linking content ( nouns and verbs ) and function words ( e.g. , prepositions and light verbs ) . It thus furnishes a ready source for multi - word , partially compositional expressions of positivity or negativity . In what follows , we provide some examples ( note that in the patterns below , expressions in brackets are used to indicate expressions not part of the pattern that correlate with it in the data ) . There are 262 positive patterns of the form Verb / Noun + \" with \" , e.g. There are 36 patterns with the string ' go ' , 12 positive ( 16 items ) and 24 negative ( 40 items ) . There are 34 patterns involving the past tense form \" went \" , which reverses the polarity to 25 positive patterns ( 273 items ) and 9 negative ( 9 items ) . Across the two versions of the lemma , the positive patterns provide several expressions for savouring ( WENT / GO ON / FOR [ a walk , a hike , a ride ] , WENT / GO SHOPPING / SWIMMING , WENT / GO TO [ the mall , a movie ] ) . For the negative , the predominance of ' go ' comes from the fact that they are largely negated ( NOT GO TO [ the movies ] ) or in infinitive contexts that suggest obligation ( [ HAVE TO ] GO TO [ class ] , [ HAVE TO ] GO WORK ) . Similarly , the positive class contains 9 patterns with ' bought ' and 1 with ' buy ' ( ENTICED [ TO ] BUY ) and the negative class has 6 patterns with ' bought ' and 16 with ' buy ' , all emphasizing buying necessities ( BUY GROCERIES / TICKET , NEED / WANT BUY , NOT BUY ) Thus , even though these expressions all involve the same verbs and prepositions , the surrounding environments , as reflected in the form of the verb , split between positive and negative sentence classes . There are 73 bigram patterns of the form NEW X , 56 positive ( 83 items ) and 17 negative ( 21 items ) . In general , the positive ones describe new objects - SHIRT , SHEETS , COMPUTER , CLOTHES , TEA - and acquaintances ( NEW FRIEND ) , thus encompassing both Connection and possibly Savouring . In contrast , the negative patterns describe changes to routines - HABITS , school QUARTER , PROFESSOR , LIVING [ conditions ] , or SCHEDULE - which are likely to engender a sense of instability , and hence be Eudaimonically negative . Thus , these patterns illustrate that Autoslog can serve as a high - precision method of building additional patterns - especially compositional onesfor a given well - being category .", "entities": [[5, 6, "MethodName", "TS"], [129, 130, "MethodName", "TS"], [176, 177, "HyperparameterName", "\u03b8"], [187, 188, "HyperparameterName", "\u03b8"], [217, 218, "HyperparameterName", "\u03b8"], [220, 221, "HyperparameterName", "\u03b8"], [224, 225, "MetricName", "F1"], [359, 360, "HyperparameterName", "\u03b8"], [385, 386, "DatasetName", "FrameNet"], [408, 409, "DatasetName", "FIN"], [459, 460, "DatasetName", "DBpedia"], [650, 651, "DatasetName", "lemma"]]}
{"text": "The intuition of weighted clustering is based on the formulation of classical LDA which models the probability of the word type t belonging to a topic i as N t , i + \u03b2t t N t i + \u03b2 t , where N t , i refers to the number of times word type t has been assigned to topic i , and \u03b2 is a parameter of the Dirichlet prior on the pertopic word distribution . In our case , illustrated by the schematic in Fig . 1 , weighting is a natural way to account for the frequency effects of vocabulary terms during clustering .", "entities": [[12, 13, "MethodName", "LDA"], [39, 40, "HyperparameterName", "\u03b2"], [64, 65, "HyperparameterName", "\u03b2"]]}
{"text": "We apply PCA to the word embeddings before clustering to investigate the amount of redundancy in the dimensions of large embeddings , which impact clustering complexity ( 4 ) . With reranking , the dimensions of all embeddings can be reduced by more than 80 % ( Fig . 2 ) . We observe that KM w r can consistently reduce the number of dimensions across different embedding types without loss of performance . Although GMM w does not require reranking for good performance , it 's cubic complexity indicates that KM w r might be preferred in practical settings .", "entities": [[2, 3, "MethodName", "PCA"], [5, 7, "TaskName", "word embeddings"], [70, 71, "MetricName", "loss"]]}
{"text": "We present the results for using different reranking schemes for KM ( Table 5 ) and Weighted KM for Frequency ( Table 6 ) . We can see that compared to the TF results in the main paper , other schemes for reranking such as aggregated TF - IDF and TF - DF improve over the original hard clustering , but fare worse in comparison with reranking with TF . of topics due to the greater diversity of words over all the topics . Top 10 Word for Each Topic NPMI dollar rate rates exchange currency market dealers central interest point 0.369 year growth rise government economic economy expected domestic inflation report 0.355 gold reserves year tons company production exploration ounces feet mine 0.290 billion year rose dlrs fell marks earlier figures surplus rise - 0.005 year tonnes crop production week grain sugar estimated expected area 0.239 dlrs company sale agreement unit acquisition assets agreed subsidiary sell - 0.043 bank billion banks money interest market funds credit debt loans 0.239 tonnes wheat export sugar tonne exports sources shipment sales week 0.218 plan bill industry farm proposed government administration told proposal change 0.212 prices production price crude output barrels barrel increase demand industry 0.339 group company investment stake firm told companies capital chairman president 0.191 trade countries foreign officials told official world government imports agreement 0.298 offer company shares share dlrs merger board stock tender shareholders 0.074 shares stock share common dividend company split shareholders record outstanding 0.277 dlrs year quarter earnings company share sales reported expects results - 0.037 market analysts time added long analyst term noted high back 0.316 coffee meeting stock producers prices export buffer quotas market price 0.170 loss dlrs profit shrs includes year gain share mths excludes - 0.427 spokesman today government strike union state yesterday workers officials told 0.201 program corn dlrs prior futures price loan contract contracts cents - 0.287 Top 10 Word for Each Topic NPMI rise increase growth fall change decline drop gains cuts rising 0.238 president chairman minister house baker administration secretary executive chief washington 0.111 make continue result include reduce open support work raise remain 0.101 january march february april december june september october july friday 0.043 year quarter week month earlier months years time period term 0.146 rose fell compared reported increased estimated revised adjusted unchanged raised 0.196 today major made announced recent full previously strong final additional 0.125 share stock shares dividend common cash stake shareholders outstanding preferred 0.281 dlrs billion tonnes marks francs barrels cents tonne barrel tons - 0.364 sales earnings business operations companies products markets assets industries operating 0.115 sale acquisition merger sell split sold owned purchase acquire held 0.003 board meeting report general commission annual bill committee association council 0.106 loss profit revs record note oper prior shrs gain includes 0.221 company corp group unit firm management subsidiary trust pacific holdings 0.058 prices price current total lower higher surplus system high average 0.198 offer agreement agreed talks tender plan terms program proposed issue 0.138 bank trade market rate exchange dollar foreign interest rates banks 0.327 told official added department analysts officials spokesman sources statement reuters 0.181 production export exports industry wheat sugar imports output crude domestic 0.262 japan government international world countries american japanese national states united 0.251", "entities": [[281, 282, "MetricName", "loss"], [289, 290, "DatasetName", "mths"], [456, 457, "MetricName", "loss"]]}
{"text": "PUBMEDBERT MEL addresses this problem by framing it as a task of mapping entity mentions to unified concepts in a medical knowledge graph . 3 The main bottleneck of MEL is the quality of the entity representations ( Basaldella et al , 2020 ) . Prior works in this domain have adopted very sophisticated text pre - processing heuristics ( D'Souza and Ng , 2015 ; Kim et al , 2019 ; Ji et al , 2020 ; Sung et al , 2020 ) which can hardly cover all the variations of biomedical names . In parallel , self - supervised learning has shown tremendous success in NLP via leveraging the masked language modelling ( MLM ) objective to learn semantics from distributional representations Liu et al , 2019 ) . Domain - specific pretraining on biomedical corpora ( e.g. BIOBERT , Lee et al 2020 and BIOMEGA - TRON , Shin et al 2020 ) have made much progress in biomedical text mining tasks . Nonetheless , representing medical entities with the existing SOTA pretrained MLMs ( e.g. PUBMEDBERT , Gu et al 2020 ) as suggested in Fig . 1 ( left ) does not lead to a well - separated representation space . To address the aforementioned issue , we propose to pretrain a Transformer - based language model on the biomedical knowledge graph of UMLS ( Bodenreider , 2004 ) , the largest interlingua of biomedical ontologies . UMLS contains a comprehensive collection of biomedical synonyms in various forms ( UMLS 2020AA has 4M+ concepts and 10M+ synonyms which stem from over 150 controlled vocabularies including MeSH , SNOMED CT , RxNorm , Gene Ontology and OMIM ) . 4 We design a selfalignment objective that clusters synonyms of the same concept . To cope with the immense size of UMLS , we sample hard training pairs from the knowledge base and use a scalable metric learning loss . We name our model as Self - aligning pretrained BERT ( SAPBERT ) . Being both simple and powerful , SAPBERT obtains new SOTA performances across all six MEL benchmark datasets . In contrast with the current systems which adopt complex pipelines and hybrid components ( Xu et al , 2020 ; Ji et al , 2020 ; Sung et al , 2020 ) , SAPBERT applies a much simpler training procedure without requiring any pre - or post - processing steps . At test time , a simple nearest neighbour 's search is sufficient for making a prediction . When compared with other domain - specific pretrained language models ( e.g. BIOBERT and SCIBERT ) , SAPBERT also brings substantial improvement by up to 20 % on accuracy across all tasks . The effectiveness of the pretraining in SAP - BERT is especially highlighted in the scientific language domain where SAPBERT outperforms previous SOTA even without fine - tuning on any MEL datasets . We also provide insights on pretraining 's impact across domains and explore pretraining with fewer model parameters by using a recently introduced ADAPTER module in our training scheme . Figure 2 : The distribution of similarity scores for all sampled PUBMEDBERT representations in a minibatch . The left graph shows the distribution of + andpairs which are easy and already well - separated . The right graph illustrates larger overlap between the two groups generated by the online mining step , making them harder and more informative for learning .", "entities": [[98, 102, "TaskName", "self - supervised learning"], [112, 114, "TaskName", "language modelling"], [115, 116, "DatasetName", "MLM"], [217, 218, "MethodName", "Transformer"], [228, 229, "DatasetName", "UMLS"], [242, 243, "DatasetName", "UMLS"], [254, 255, "DatasetName", "UMLS"], [278, 279, "MethodName", "Ontology"], [304, 305, "DatasetName", "UMLS"], [319, 321, "TaskName", "metric learning"], [321, 322, "MetricName", "loss"], [332, 333, "MethodName", "BERT"], [430, 433, "TaskName", "pretrained language models"], [451, 452, "MetricName", "accuracy"], [464, 465, "MethodName", "BERT"]]}
{"text": "We design a metric learning framework that learns to self - align synonymous biomedical entities . The framework can be used as both pretraining on UMLS , and fine - tuning on task - specific datasets . We use an existing BERT model as our starting point . In the following , we introduce the key components of our framework . Formal Definition . Let ( x , y ) X \u00d7 Y denote a tuple of a name and its categorical label . For the self - alignment pretraining step , X \u00d7 Y is the set of all ( name , CUI 5 ) pairs in UMLS , e.g. ( Remdesivir , C4726677 ) ; while for the finetuning step , it is formed as an entity mention and its corresponding mapping from the ontology , e.g. ( scratchy throat , 102618009 ) . Given any pair of tuples ( x i , y i ) , ( x j , y j ) X \u00d7 Y , the goal of the self - alignment is to learn a function f ( ; \u03b8 ) : X R d parameterised by \u03b8 . Then , the similarity f ( x i ) , f ( x j ) ( in this work we use cosine similarity ) can be used to estimate the resemblance of x i and x j ( i.e. , high if x i , x j are synonyms and low otherwise ) . We model f by a BERT model with its output [ CLS ] token regarded as the representation of the input . 6 During the learning , a sampling procedure selects the informative pairs of training samples and uses them in the pairwise metric learning loss function ( introduced shortly ) . Online Hard Pairs Mining . We use an online hard triplet mining condition to find the most informative training examples ( i.e. hard positive / negative pairs ) within a mini - batch for efficient training , Fig . 2 . For biomedical entities , this step can be particularly useful as most examples can be easily classified while a small set of very hard ones cause the most challenge to representation learning . 7 We start from constructing all possible triplets for all names within the mini - batch where each triplet is in the form of ( x a , x p , x n ) . Here x a is called anchor , an arbitrary name in the minibatch ; x p a positive match of x a ( i.e. y a = y p ) and x n a negative match of x a ( i.e. y a = y n ) . Among the constructed triplets , we select out all triplets that violate the following condition : f ( x a ) \u2212 f ( x p ) 2 < f ( x a ) \u2212 f ( x n ) 2 + \u03bb , ( 1 ) where \u03bb is a pre - set margin . In other words , we only consider triplets with the negative sample closer to the positive sample by a margin of \u03bb . These are the hard triplets as their original representations were very far from correct . Every hard triplet contributes one hard positive pair ( x a , x p ) and one hard negative pair ( x a , x n ) . We collect all such positive & negative pairs and denote them as P , N . A similar but not identical triplet mining condition was used by Schroff et al ( 2015 ) for face recognition to select hard negative samples . Switching - off this mining process , causes a drastic performance drop ( see Tab . 2 ) . Loss Function . We compute the pairwise cosine similarity of all the BERT - produced name representations and obtain a similarity matrix S R | X b | \u00d7 | X b | where each entry S ij corresponds to the cosine similarity between the i - th and j - th names in the mini - batch b. We adapted the Multi - Similarity loss ( MS loss , Wang et al 2019 ) , a SOTA metric learning objective on visual recognition , for learning from the positive and negative pairs : L = 1 | X b | | X b | i=1 1 \u03b1 log 1 + n N i e \u03b1 ( S in \u2212 ) + 1 \u03b2 log 1 + p P i e \u2212\u03b2 ( S ip \u2212 ) , ( 2 ) where \u03b1 , \u03b2 are temperature scales ; is an offset applied on the similarity matrix ; P i , N i are indices of positive and negative samples of the anchor i. 8 While the first term in Eq . 2 pushes negative pairs away from each other , the second term pulls positive pairs together . This dynamic allows for a re - calibration of the alignment space using the semantic biases of synonymy relations . The MS loss leverages similarities among and between positive and negative pairs to re - weight the importance of the samples . The most informative pairs will receive more gradient signals during training and thus can better use the information stored in data .", "entities": [[3, 5, "TaskName", "metric learning"], [25, 26, "DatasetName", "UMLS"], [41, 42, "MethodName", "BERT"], [108, 109, "DatasetName", "UMLS"], [136, 137, "MethodName", "ontology"], [185, 186, "HyperparameterName", "\u03b8"], [193, 194, "HyperparameterName", "\u03b8"], [254, 255, "MethodName", "BERT"], [292, 294, "TaskName", "metric learning"], [294, 295, "MetricName", "loss"], [372, 374, "TaskName", "representation learning"], [614, 616, "TaskName", "face recognition"], [653, 654, "MethodName", "BERT"], [706, 707, "MetricName", "loss"], [709, 710, "MetricName", "loss"], [719, 721, "TaskName", "metric learning"], [748, 749, "HyperparameterName", "\u03b1"], [756, 757, "HyperparameterName", "\u03b1"], [764, 765, "HyperparameterName", "\u03b2"], [783, 784, "HyperparameterName", "\u03b1"], [785, 786, "HyperparameterName", "\u03b2"], [862, 863, "MetricName", "loss"]]}
{"text": "During training , we use AdamW ( Loshchilov and Hutter , 2018 ) with a learning rate of 2e - 5 and weight decay rate of 1e - 2 . Models are trained on the prepared pairwise UMLS data for 1 epoch ( approximately 50k iterations ) with a batch size of 512 ( i.e. , 256 pairs per mini - batch ) . We train with Automatic Mixed Precision ( AMP ) 10 provided in PyTorch 1.7.0 . This takes approximately 5 hours on our machine ( con - 4231 scientific language social media language model NCBI BC5CDR - d BC5CDR - c MedMentions AskAPatient COMETA @1 @5 @1 @5 @1 @5 @1 @5 @1 @5 @1 @5 the improvement comparing to the base model ( the deeper the more ) . Bottom : SAPBERT vs. SOTA results . Blue and red denote unsupervised and supervised models . Bold and underline denote the best and second best results in the column . \" \u2020 \" denotes statistically significant better than supervised SOTA ( T - test , \u03c1 < 0.05 ) . On COMETA , the results inside the parentheses added the supervised SOTA 's dictionary back - off technique ( Basaldella et al , 2020 ) . \" - \" : not reported in the SOTA paper . \" OOM \" : out - of - memory ( 192GB+ ) . figurations specified in App . B.4 ) . For other hyperparameters used , please view App . C.2 . Evaluation Data and Protocol . We experiment on 6 different English MEL datasets : 4 in the scientific domain ( NCBI , Dogan et al 2014 ; BC5CDR - c and BC5CDR - d , Li et al 2016 ; MedMentions , Mohan and Li 2018 ) and 2 in the social media domain ( COMETA , Basaldella et al 2020 andAskAPatient , Limsopatham andCollier 2016 ) . Descriptions of the datasets and their statistics are provided in App . A. We report Acc @1 and Acc @5 ( denoted as @1 and @5 ) for evaluating performance . In all experiments , SAPBERT denotes further pretraining with our self - alignment method on UMLS . At the test phase , for all SAPBERT models we use nearest neighbour search without further fine - tuning on task data ( unless stated otherwise ) . Except for numbers reported in previous papers , all results are the average of five runs with different random seeds . Fine - Tuning on Task Data . The red rows in Tab . 1 are results of models ( further ) fine - tuned on the training sets of the six MEL datasets . Similar to pretraining , a positive pair list is generated through traversing the combinations of mention and all ground truth synonyms where mentions are from the training set and ground truth synonyms are from the reference ontology . We use the same optimiser and learning rates but train with a batch size of 256 ( to accommodate the memory of 1 GPU ) . On scientific language datasets , we train for 3 epochs while on AskAPatient and COMETA we train for 15 and 10 epochs respectively . For BIOSYN on social media language datasets , we empirically found that 10 epochs work the best . Other configurations are the same as the original BIOSYN paper .", "entities": [[5, 6, "MethodName", "AdamW"], [15, 17, "HyperparameterName", "learning rate"], [22, 25, "HyperparameterName", "weight decay rate"], [37, 38, "DatasetName", "UMLS"], [49, 51, "HyperparameterName", "batch size"], [69, 70, "MetricName", "Precision"], [71, 72, "MethodName", "AMP"], [98, 99, "DatasetName", "BC5CDR"], [101, 102, "DatasetName", "BC5CDR"], [104, 105, "DatasetName", "MedMentions"], [106, 107, "DatasetName", "COMETA"], [184, 185, "DatasetName", "COMETA"], [279, 280, "DatasetName", "BC5CDR"], [283, 284, "DatasetName", "BC5CDR"], [292, 293, "DatasetName", "MedMentions"], [307, 308, "DatasetName", "COMETA"], [335, 336, "MetricName", "Acc"], [338, 339, "MetricName", "Acc"], [366, 367, "DatasetName", "UMLS"], [415, 416, "DatasetName", "seeds"], [487, 488, "MethodName", "ontology"], [501, 503, "HyperparameterName", "batch size"], [529, 530, "DatasetName", "COMETA"]]}
{"text": "* BERT + SAPBERT ( Tab . 1 , top ) . We illustrate the impact of SAPBERT pretraining over 7 existing BERT - based models ( * BERT = { BIOBERT , PUBMEDBERT , ... } ) . SAPBERT obtains consistent improvement over all * BERT models across all datasets , with larger gains ( by up to 31.0 % absolute Acc @1 increase ) observed in the social media domain . While SCIBERT is the leading model before applying SAPBERT , PUBMEDBERT+SAPBERT performs the best afterwards . SAPBERT vs. SOTA ( Tab . 1 , bottom ) . We take PUBMEDBERT+SAPBERT ( w / wo fine - tuning ) and compare against various published SOTA results ( see App . C.1 for a full listing of 10 baselines ) which all require task supervision . For the scientific language domain , the SOTA is BIOSYN ( Sung et al , 2020 ) . For the social media domain , the SOTA are Basaldella et al ( 2020 ) and GEN - RANK ( Xu et al , 2020 ) on COMETA and AskAPatient respectively . All these SOTA methods combine BERT with heuristic modules such as tf - idf , string matching and information retrieval system ( i.e. Apache Lucene ) in a multi - stage manner . Measured by Acc @1 , SAPBERT achieves new SOTA with statistical significance on 5 of the 6 datasets and for the dataset ( BC5CDR - c ) where SAPBERT is not significantly better , it performs on par with SOTA ( 96.5 vs. 96.6 ) . Interestingly , on scientific language datasets , SAPBERT outperforms SOTA without any task supervision ( fine - tuning mostly leads to overfitting and performance drops ) . On social media language datasets , unsupervised SAPBERT lags behind supervised SOTA by large margins , highlighting the well - documented complex nature of social media language ( Baldwin et al , 2013 ; Collier , 2015 , 2016 ; Basaldella et al , 2020 ; Tutubalina et al , 2020 ) . However , after fine - tuning on the social media datasets ( using the MS loss introduced earlier ) , SAPBERT outperforms SOTA significantly , indicating that knowledge acquired during the selfaligning pretraining can be adapted to a shifted domain without much effort . The ADAPTER Variant . As an option for parameter efficient pretraining , we explore a variant of SAPBERT using a recently introduced training module named ADAPTER ( Houlsby et al , 2019 ) . While maintaining the same pretraining scheme with the same SAPBERT online mining + MS loss , instead of training from the full model of PUBMEDBERT , we insert new ADAPTER layers between Transformer layers of the fixed PUBMEDBERT , and only train the weights of these ADAPTER layers . In our experiments , we use the enhanced ADAPTER configuration by Pfeiffer et al ( 2020 ) . We include two variants where trained parameters are 13.22 % and 1.09 % of the full SAPBERT variant . The ADAPTER variant of SAPBERT achieves comparable performance to full - model - tuning in scientific datasets but lags behind in social media datasets , Tab . 1 . The results indicate that more parameters are needed in pretraining for knowledge transfer to a shifted domain , in our case , the social media datasets . The Impact of Online Mining ( Eq . ( 1 ) ) . As suggested in Tab . 2 , switching off the online hard pairs mining procedure causes a large performance drop in @1 and a smaller but still significant drop in @5 . This is due to the presence of many easy and already well - separated samples in the mini - batches . These uninformative training examples dominated the gradients and harmed the learning process . Integrating SAPBERT in Existing Systems . SAPBERT can be easily inserted into existing BERT - based MEL systems by initialising the systems with SAPBERT pretrained weights . We use the SOTA scientific language system , BIOSYN ( originally initialised with BIOBERT weights ) , as an example and show the performance is boosted across all datasets ( last two rows , Tab . 1 ) .", "entities": [[1, 2, "MethodName", "BERT"], [22, 23, "MethodName", "BERT"], [28, 29, "MethodName", "BERT"], [46, 47, "MethodName", "BERT"], [62, 63, "MetricName", "Acc"], [182, 183, "DatasetName", "COMETA"], [192, 193, "MethodName", "BERT"], [205, 207, "TaskName", "information retrieval"], [222, 223, "MetricName", "Acc"], [243, 244, "DatasetName", "BC5CDR"], [361, 362, "MetricName", "loss"], [438, 439, "MetricName", "loss"], [456, 457, "MethodName", "Transformer"], [658, 659, "MethodName", "BERT"]]}
{"text": "We use COMETA ( zeroshot general ) as a benchmark for selecting learning objectives . Note that this split of COMETA is different from the stratified - general split used in Tab . 4 . It is very challenging ( so easy to see the difference of the performance ) and also does not directly affect the model 's performance on other datasets . The results are listed in Tab . 6 . Note that online mining is switched on for all models here . ( Basaldella et al , 2020 ) 64.6 74.6 NCA loss ( Goldberger et al , 2005 ) 65.2 77.0 Lifted - Structure loss ( Oh Song et al , 2016 ) 62.0 72.1 InfoNCE ( Oord et al , 2018 ; He et al , 2020 ) 63.3 74.2 Circle loss ( Sun et al , 2020 ) 66.7 78.7 Multi - Similarity loss ( Wang et al , 2019 ) 67.2 80.3 Schumacher et al ( 2020 ) for clinical concept linking . InfoNCE has been very popular in selfsupervised learning and contrastive learning ( Oord et al , 2018 ; He et al , 2020 ) . Lifted - Structure loss ( Oh Song et al , 2016 ) and NCA loss ( Goldberger et al , 2005 ) are two very classic metric learning objectives . Multi - Similarity loss ( Wang et al , 2019 ) and Circle loss ( Sun et al , 2020 ) are two recently proposed metric learning objectives and have been considered as SOTA on large - scale visual recognition benchmarks .", "entities": [[2, 3, "DatasetName", "COMETA"], [20, 21, "DatasetName", "COMETA"], [95, 96, "MetricName", "loss"], [108, 109, "MetricName", "loss"], [119, 120, "MethodName", "InfoNCE"], [136, 137, "MetricName", "loss"], [149, 150, "MetricName", "loss"], [170, 171, "MethodName", "InfoNCE"], [179, 181, "MethodName", "contrastive learning"], [198, 199, "MetricName", "loss"], [209, 210, "MetricName", "loss"], [221, 223, "TaskName", "metric learning"], [228, 229, "MetricName", "loss"], [238, 239, "MetricName", "loss"], [250, 252, "TaskName", "metric learning"]]}
{"text": "In Tab . 7 we list number of parameters trained in the three ADAPTER variants along with full - modeltuning for easy comparison . BIOBERT ( Lee et al , 2020 ) https://huggingface.co/dmis - lab / biobert - v1.1 BLUEBERT ( Peng et al , 2019 ) https://huggingface.co/bionlp/bluebert_pubmed_mimic_uncased_L - 12_H - 768_A - 12 CLINICALBERT ( Alsentzer et al , 2019 ) https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT SCIBERT ( Beltagy et al , 2019 ) https://huggingface.co/allenai/scibert_scivocab_uncased UMLSBERT ( Michalopoulos et al , 2020 ) https://www.dropbox.com/s/qaoq5gfen69xdcc/umlsbert.tar.xz?dl=0 PUBMEDBERT ( Gu et al , 2020 ) https://huggingface.co/microsoft/BiomedNLP - PubMedBERT - base - uncased - abstract - fulltext", "entities": [[6, 9, "HyperparameterName", "number of parameters"]]}
{"text": "Here , we first examine the relevance of our proposal to reinstitute summarization evaluation over multiple summary lengths . Then , we investigate our research question of whether using reference summaries of a single length suffices for evaluating system summaries of multiple lengths . We turn to the DUC 2001 and 2002 multi - document summarization datasets , which , to the best of our knowledge , are the only available datasets that provide the necessary requirements for this analysis ( see Table 1 ) . The importance of evaluating and comparing systems at several lengths is demonstrated with the observation that system rankings can change quite significantly at different summary lengths . In 2001 , the Spearman correlation between the available human rankings of systems at the 50word and 400 - word lengths is 0.61 . For example , the system ranked first at length 50 ranks sixth at lengths 200 and 400 . Even for the human system ranking at the 100 - word length , which deviates the least from human rankings at the other lengths , the correlation with system ranking at the 400 length is only 0.73 . Generally , the larger the difference between a pair of summary lengths , the greater the fluctuation in system rankings . Similar trends were observed for DUC 2002 , and when comparing system rankings by automatic ROUGE scoring ( both rankings are elaborated below ) . Obviously , such performance differences are overlooked when evaluating systems over summaries of a single length . Next , we turn to investigate our research question . In this paper , we examine it with respect to automatic summary evaluation , which has become most common for system development and evaluation , thanks to its speed and low cost . Specifically , we use several variants of the ROUGE metric ( Lin , 2004 ) , which is almost exclusively utilized as an automatic evaluation metric class for summarization . ROUGE variants are based on word sequence overlap between a system summary and a reference summary , where each variant measures a different aspect of text comparison . Despite its pitfalls , ROUGE has shown reasonable correlation of its system scores to those obtained by manual evaluation methods ( Lin , 2004 ; Over and James , 2004 ; Over et al , 2007 ; Nenkova et al , 2007 ; Louis and Nenkova , 2013 ; Peyrard et al , 2017 ) , such as SEE ( Lin , 2001 ) , responsiveness ( NIST , 2006 ) and Pyramid ( Nenkova et al , 2007 ) . We follow the same methodology of assessing the reliability of automatic evaluation scores by measuring their correlation to human evaluation scores . In our case , DUC 2001 and 2002 applied the SEE manual evaluation method . NIST assessors compared systems ' summaries to reference summaries , which were all decomposed into a list of elementary discourse units ( EDUs ) . Each reference EDU was marked throughout the system EDUs and was scored for how well it was expressed . The final manually evaluated scores , called the human mean content coverage scores , are provided in the DUC datasets . We can then correlate the human - based system ranking , attained from these provided scores , to the system ranking attained from the automatic scores that we calculate using our proposed methodology . As a baseline , we consider the ROUGE Recall scores obtained by the standard reference summary configuration ( Standard , first row in Table 2 ) , that is , when system summaries of each length ( table columns ) are evaluated against reference summaries of the same length . This is the same configuration used by Lin ( 2004 ) when introducing and assessing ROUGE . Then , looking into our research question , we consider reference summary configurations in which system summaries of all lengths are evaluated against reference summaries of a single chosen length ( OnlyNNN , subsequent rows of Table 2 ) . In each configuration ( each row ) , we repeat the evaluation twice : once using the complete set of available reference sum - 2 ) for different ROUGE variants ( column pairs ) and reference summary configurations ( rows ) , when using 1 reference or multiple . The first row presents absolute correlations , with relative differences in subsequent rows . maries of the utilized reference length , and once with just one randomly chosen reference summary from that set ( the 3refs and 1ref sub - columns ) . For each reference summary configuration , we compute ROUGE Recall system scores 1 for the three common ROUGE variants R - 1 , R - 2 and R - L , which compare unigrams , bigrams and the longest common subsequence , respectively . System scores , per summary length , are obtained by averaging across all summarized texts . We then calculate their Pearson correlation 2 with the available human mean content coverage scores for the systems . The first row of Table 2 shows these correlations , considering the R - 1 scores for the DUC 2001 systems , per summary length . The subsequent rows show the corresponding figures for the single - reference - length configurations . For readability , we present in these rows the relative differences to the Standard baseline row . Hence , positive values indicate a configuration that is at least as good as the standard configuration . Table 3 presents correlations averaged over all summary lengths , for the three ROUGE variants over both datasets . We see in the tables that evaluating system summaries of all lengths against references of a single length often performs on par with the standard configuration . In particular , the single fixed set of 50 - word reference summaries performs overall as well as the standard approach , and , although not substantially , is the most effective configuration within the data analyzed . In other words , in this dataset , the 50 - word reference summaries provide a \" test sample \" for evaluating the longer system summaries , which is as effective as the same length references used by the standard method . We note that even when a single reference summary is available , reasonable correlations with human scores are obtained for the 50 word reference . This suggests that it may be possible to compare system summaries of multiple lengths even against a single reference summary , of a relatively short length . This observation seems to deserve further assessment over recent large scale datasets , such as CNN / DailyMail , which provide a single relatively short reference for each summarized text . In addition to correlation to human assessment , we computed the correlations between system rankings calculated by Standard and those calcu - ( Gillick et al , 2008 ) . lated by Only50 , at each system summary length . We find very high correlations ( above 0.95 for all system summary lengths , in both datasets ) when using multiple references and slightly lower ( 0.85 to 0.9 ) with one reference summary . These figures show that the Only50 configuration ranks systems very similarly to Standard . To further verify our results , we computed correlations in two additional settings . First , we conducted the same analysis , excluding 2 - 3 of the worst systems , which might artificially boost the correlation ( Rankel et al , 2013 ) . Second , we computed score differences between all pairs of systems , for both human and ROUGE scores , and computed the correlation between these two sets of differences ( Rankel et al , 2011 ) . In both cases we observed rather consistent results , assessing that a single set of short reference summaries evaluates system summaries of different lengths just as well as the standard configuration .", "entities": [[12, 13, "TaskName", "summarization"], [52, 56, "TaskName", "multi - document summarization"], [117, 119, "MetricName", "Spearman correlation"], [327, 328, "TaskName", "summarization"], [582, 583, "MetricName", "Recall"], [782, 783, "MetricName", "Recall"], [837, 839, "MetricName", "Pearson correlation"]]}
{"text": "This section illustrates how system performances can be measured and compared when evaluating them on outputs of varying lengths against a single reference point . Figure 1 presents the ROUGE scores of the Only50 configuration for three DUC - 01 submitted systems , and for ICSISumm ( Gillick et al , 2008 ) , a later competitive system . As expected when measuring ROUGE Recall against a fixed reference length , longer system summaries typically cover more of the reference summaries content than shorter ones , yielding higher scores . Yet , it can be noted , for example , that the value of the 400 - word summary of system R in the figure is lower than that of the 200 - word summaries of the other systems . Such a compar - ison is impossible in the standard setup , as each system length is evaluated against different reference summaries . We note that similar comparisons are embedded in the evaluations of Steinberger and Jezek ( 2004 ) and Kikuchi et al ( 2016 ) , who also evaluated multiple summary lengths . Further , one can define the marginal value of longer summaries of a given system as the ROUGE score increase per number of additional words , namely the graph slope . This denotation allows measuring the effectiveness of producing longer summaries . For example , deploying system R , we might decide to output only summaries no longer than 200 words , since the marginal value of longer summaries becomes too small . The other systems , on the other hand , seem marginally effective also in 400 word summaries .", "entities": [[64, 65, "MetricName", "Recall"]]}
{"text": "The dataset construction consist of ( 1 ) crowdsourcing the dialogues , ( 2 ) annotating dialog acts and entities and ( 3 ) linking utterances into grounded knowledge . We explain these three steps in order and present the dataset statistics in the end . Dialogue Crowd - sourcing We obtain the dialogue dataset through a two - phase Wizard - of - Ozstyle collection ( Kelley , 1984 ; Dahlb\u00e4ck et al , 1993 ) . In the first phase , we run small - scale pilot studies and examine the quality of collected conversations . Based on the examination , we created tutorials and qualification tests . They are used to train and qualify crowd - workers for the second phase . During this second phase , we consistently monitor the collected dialogue datasets and perform periodic quality check on samples from every individual work pairs . If more than 5 % from one pair are considered invalid , their collections will be removed . Before a conversation started , two workers are paired and a movie is chosen agreed by both 4 . We constrain at least one of them to have watched the movie to make sure the conversation is contentful 5 . The annotators are especially instructed to ( 1 ) behave naturally as in daily life , ( 2 ) avoid dirty words and ( 3 ) talk differently in each conversation . Duplicate conversations will be removed if more than 70 % of their contents are overlapped . To encourage diverse movies , we further set an upper limit to forbid one movie from being talked about for more than 100 times . The whole collecting process lasts two months . In the end , 245 participants are involved with 66 , 424 movies being talked about in total . Dialogue Act and Entity Annotation Following prior work , we base our annotation schema on the ISO 24617 - 2 standard ( Bunt et al , 2010 ( Bunt et al , , 2012 . Table 1 shows our annotation schema , counts , descriptions , and brief examples . The dialogue acts ( DAs ) are organized in a hierarchical structure . The first layer makes distinctions on three concepts : objective facts , recommendations and subjective feelings . Each concept can either be either requested or informed during the conversation . We further define an \" Other \" class to include actions that do not belong to any of the three concepts , like some general non - contentful greetings or echos . The second layer includes 15 finer - grained aspects covering most popular topics being discussed . Every first - layer DA ( except Other ) will be further group it into one of these 15 aspects , e.g. , the de - tailed DA of the first example in Table 1 will be request fact director . If one utterance contains multiple dialogue acts , we order the dialogue acts based on their turn of appearance in the utterance . As for the named entity recognition , we labeled 5 kinds of entities : movie names , director , actor , type and role ( first 5 aspects ) . To speed up the annotation process , we first define a set of handcrafted regular expressions , which covers most frequent patterns at each class , to train a DA and NER classifier . The annotators are instructed to post - correct the auto - labeled dialogues instead of doing everything from scratch . The classifiers are trained with online learning ( Sahoo et al , 2018 ) to keep improving the accuracy and lower down the frequency of post - correction in consequence . As we observe , this semi - automated way significantly speeds up the labeling process . All the dataset is finished labeling within three weeks with 188 annotators involved . Knowledge Linkage We extract fact knowledge from the structured table in Douban Movie 6 , a popular Chinese platform for movies . The knowledge is organized in the form of key - value pairs , where the key corresponds to the 15 aspects defined by us . Some aspects , like lines or music , are not directly available from the structured table . We extract these missing information from other sources and combine it into our knowledge base . For utterances labeld as inform / request fact , we link them to the key - value pairs from the same aspect . Apart from the objective knowledge , we also crawl movie comments from Douban Movie to support the generation of responses expressing subjective feelings . These comments can be a good supplementary to provide knowledge that can be hardly organized in the structured form ( Moghe et al , 2018 ) . For utterances labeled as inform / request feeling , we compare them with Douban comments from the same movie and compute the similarity score based on weighted average of edit distance , Jaccard distance , tf - idf , sentence vector cosine similarity , common words and entities . Each utterance is linked to the most similar comment with a threshold cutoff . In the end , 51.7 % of the utterances about feelings have grounded comments . For utterances about recommendations , we simply ground them to the men - tioned movie entities 7 , and no grounded knowledge is linked for utterances labeled as Other . An example of our annotation is presented in Table 1 .", "entities": [[516, 519, "TaskName", "named entity recognition"], [574, 575, "TaskName", "NER"], [602, 604, "TaskName", "online learning"], [615, 616, "MetricName", "accuracy"], [669, 670, "DatasetName", "Douban"], [773, 774, "DatasetName", "Douban"], [825, 826, "DatasetName", "Douban"]]}
{"text": "The intent prediction is also cast as a sequence prediction task . Compared with the traditional way of multi - label classification , casting it as sequence prediction is better at addressing the coexistence of multiple DAs and capturing the sequential dependencies among the hierarchy ( Raffel et al , 2019 ; Vedula et al , 2020 ) . For example , to predict the DAs of the 4th utterance in Figure 1 , the sequence fed to the language model will be \" [ context ] dialogue context [ intent ] inform , feeling , plot , request , fact , plot \" . By this means , before predicting a DA , the model can condition on both the dialogue context and its previous DAs to improve the accuracy .", "entities": [[18, 22, "TaskName", "multi - label classification"], [130, 131, "MetricName", "accuracy"]]}
{"text": "Automatic Evaluation In Table 5 , we report the perplexity , BLEU scores and distinct uni / bigrams for three model sizes . To investigate the effects of incorporating annotations and pretraining , we start from a basic model which trains from scratch on our movie corpus . At each time , we add one more condition to see its influence . The results show a clear tendency of gradual improvement as more conditions are added to the training . Adding knowledge especially boosts the performance , which is understandable considering movie - domain chats usually contain many movie - specific rare names . Without knowledge grounding , it can hardly predict the correct tokens . Pretraining on general - domain conversations can improve both the overlap with ground truth . The distinct uni / bigrams also consistently increase , implying the model can learn useful patterns in the pretrained corpus to enrich its generations in the movie domain . In unseen testset , the performance generally drops for all , especially for models without knowledge grounding as they have to make up facts and comments for totally unseen movies in the training set . Table 6 measures the accuracy of predicting dialogue act ( DA ) , aspect and movie tracker of our model . Our models are all pretrained with general - domain corpus beforehand . Apart from being trained only to predict the individual tasks , we include the results where all subtasks are cotrained end - to - end in the last line . We compare our models with the Chinese RoBERTa ( Liu et Table 7 measures the performance of retrieving fact knowledge , movie comments and recommen - dation respectively . We report the hit@1 and hit@5 scores for them ( Zhang et al , 2018 ) . We compare our model with a random baseline , bag - ofword ( BOW ) and the Bert ( Devlin et al , 2019 ) model ( we pass sentences through Bert and derive a fixed - sized vector by averaging the outputs from the second - to - last layer ( May et al , 2019 ) ) . The BOW and Bert model are finetuned with our knowledge linkage annotations . We find that our unified model again outperforms all baseline approaches . Adding the DA as a condition further helps . Fact retrieval has the highest hit rate as it is well structured and easy to match . Recommendation , on the other hand , is very hard to predict . As an accurate recommendation system is clearly beyond the scope of this paper , it is understandable that our simple way fails to provide satisfying recommendations . Human Evaluation Automatically evaluating dialogue systems are known to be extremely hard ( Liu et al , 2016 ; . We further conduct a set of static and interactive human evaluations . We focus on evaluate the machine - generated response from four perspectives . Apart from the oft - used metrics ( 1 ) Sensibleness ( Sens ) and ( 2 ) Engagement ( Enga ) for open - domain chatbots , we further evaluate on ( 3 ) Factuality ( Fact ) and ( 4 ) Informativeness ( Info ) to see if models can actively provide informative responses based on movie facts . Details are in Appendix B. As evaluating factuality requires specific movie knowledge , this metric is only evaluated by the same person who produced the dialogue . The other metrics are evaluated by 3 workers each . Table 8 shows the agreement scores . The agreement is reasonable considering the evaluations are subjective . The results are the majority votes of the binary scores . In the static evaluation , we sample 300 responses for each model from the test set ( mixing seen and unseen ) . The responses can come from any turn in a conversation . We show the results in Figure 2 . Our largest model with 762 M is clearly preferred by human evaluators on almost all metrics and approaches human performance . By training a larger model and increasing the training size , the gap might be further closed . In the interactive evaluation , humans can chat with any topic but restricted in the movie domain . We conduct an online Turing test where one side is always a human participant not aware whom he is talking with . The other side could be either Mitsuku , XiaoIce 13 , our model ( 762 M with pretraining ) or a real human . Mitsuku interacts in English , so we hire only English native speakers for the experiment . We collect 100 conversations for all models . Humans can stop interacting once they ( 1 ) find the other side is a machine or ( 2 ) reaches the maximum turn of 20 . Responses from all models are later passed to the third party to judge the scores . The results are shown on the right of Figure 2 . Our model outperforms Mitsuku and XiaoIce by a large margin . As Mitsuku and XiaoIce are designed to be open - domain chatbots , restricting to be on the movie domain will give our model some natural advantage . We can also notice that Mitsuku and XiaoIce almost never produce fake facts . The cost is the extremely low ratio of informative responses since they tend to behave over - safely and will only answer it when they are 100 % sure . Our model is closer to humans in that sense . It will converse actively at some risk of containing fact errors . Figure 3 : Change of SEA and FIA as the turn proceeds . 13 We use its chat service through Weibo . It will sometimes generate responses containing keywords like \" XiaoIce \" . We manually replace it to prevent disclosing its identity . Distance from Human Performance In the interactive evaluation , compared with human performance , our model loses a bit on sensibleness and factuality but wins on the other two . To investigate where our model fails , figure 3 visualizes the change of SSA ( Sensibleness - Engagement average ) and FIA ( Factuality - Informativeness average ) when the conversational turn proceeds . A good chatbot should balance well these skills ( Adiwardana et al , 2020 ) . SEA can reflect how it behaves as a general chatbot while FIA can better test its capability at incorporating domain knowledge . We can see a clear trend of decrease for all models . As for human performance , however , the score is quite consistent across turn rounds , implying a large improvement space for current models to deal with multi - turn context . In figure 4 , we further show the \" dying distribution \" of our model , namely , in which DA our model fails to pass the Turing test and thereby \" dies \" . Unsurprisingly , we can see the system fails mostly when informing facts or feelings . Only a small portion are from non - grounded chitchats ( other ) . This suggests the most crucial bottleneck lies in the interaction with movie - specific knowledge and seamlessly incorporating it into the response generation . We show some snippets of interactions with our model in Table 9 . The first two are failing cases labeled by humans as not factual and sensible . We can see the model struggles at replying to too specific facts . This is understandable since our knowledge base only provide short introductions and can not cover all what happened in the movie . The second case shows its shortcoming at handing long - range consistency . It still recommends the current movie when the user asks about \" which other movie \" . Employing larger knowledge bases and explicitly tracking the states by a checklist ( Kiddon et al , 2016 ) might potentially alleviate both issue . We also provide examples for controllable generations where the DA and aspect are manually assigned . As observed , the model shows decent performance at fitting both the dialogue con - text and specified conditions . This can be helpful when finer - grained control is needed .", "entities": [[9, 10, "MetricName", "perplexity"], [11, 12, "MetricName", "BLEU"], [199, 200, "MetricName", "accuracy"], [265, 266, "MethodName", "RoBERTa"], [944, 945, "DatasetName", "converse"], [974, 975, "DatasetName", "Weibo"], [1064, 1065, "TaskName", "chatbot"], [1087, 1088, "TaskName", "chatbot"], [1229, 1231, "TaskName", "response generation"]]}
{"text": "Before training each classifier , we employ the best performing top features from the Figure 2 , where every classifier has its most fitting top features for each subtask . Next , we construct a LOO crossvalidation framework for within - dataset evaluations . 1 It is important to note that , in each step of the LOO , we choose new user ids for evaluation and completely exclude all of their tweets from the training sets to evade ML methods potentially learning the way a person drafts tweets . That means the within - dataset LOO results of a subtask are reported for all users of the labeled set . Moreover , the labeled datasets have more users than the unlabeled test sets per subtask ( e.g. 57 vs. 11 suicidal users in subtask1 ) . Ergo , we expect a high magnitudinal difference between the within - dataset and the test results . The within - dataset evaluation results of the selected methods are in Table 1 . For subtask 1 , we obtain the best LOO cross - validation score from the wEns method that combines the results of four ML methods ( LR , MNB , GNB , lSVM ) in a way that improves the results obtained from each of them . Meanwhile , GRU - Bert and MNB return the lowest false positive rates ( FPR ) for this subtask , which might be a critical rate to consider in real - life applications in social media domains . LOO results of subtask 2 in Table 1 show that wEns returns the best scores for the longer - spanning dataset as well , where LR returns the best FPR , and GBN returns the highest true positives rate ( TPR ) . Based on the LOO results , we select three different methods we were allowed to submit for the evaluation of the test set : LR , wEns , and GRU - Bert . We choose LR and wEns for their high performance on LOO experiments , while we select GRU - Bert for measuring how a DL method would generalize over the test sets . The baseline classifier provided by the organizers is also a logistic regression . However , it performs the classification over merged tweets of users - therefore is different from our implementation of LR . In Table 2 , wEns appears to provide the best F1 , F2 , and TPR scores over the test set of subtask 1 , while our LR outperforms the AUC of the baseline method . While these methods show the success of generalizability on the 30 - days test set , the results are not that successful for subtask 2 . The wEns method performs the same as the baseline in terms of TPR , but the rest of the scores are lower than the baseline results .", "entities": [[219, 220, "MethodName", "GRU"], [327, 328, "MethodName", "GRU"], [347, 348, "MethodName", "GRU"], [373, 375, "MethodName", "logistic regression"], [407, 408, "MetricName", "F1"], [427, 428, "MetricName", "AUC"]]}
{"text": "Pretrained Language Models ( PLM ) are predominant in tackling current Natural Language Processing ( NLP ) tasks . Most PLMs based on the Transformer architecture ( Vaswani et al , 2017 ) are first trained on massive text corpora with the selfsupervised objective to learn word representations ( Devlin et al , 2019 ; Liu et al , 2019 ) , and then are fine - tuned for a specific target task . The pretraining and fine - tuning of PLMs achieves state - ofthe - art ( SOTA ) performance in many NLP tasks . Inspired by the benefits of pretraining , there have been studies demonstrate the effects of continued pretraining on the domain of a target task or the target task dataset ( Mitra et al , 2020 ; Han and Eisenstein , 2019 ; Gururangan et al , 2020 ) . Gururangan et al , 2020 adapt PLMs on the target task by further pretraining RoBERTa ( Liu et al , 2019 ) on the target text corpus before it is fine - tuned for the corresponding task and showed that this task adaptation consistently improves the performance for text classification tasks . However , this full process of pretraining and then fine - tuning can be parameter inefficient for recent PLMs that have millions or billions of parameters ( Devlin et al , 2019 ; Radford et al , 2018 ) . This parameter inefficiency becomes even worse when one continues pre - training all the parameters of PLMs on the task - specific corpus . Furthermore , recent PLMs need more than 100s of MB to store all the weights ( Liu et al , 2019 ; Radford et al , 2018 ) , making it difficult to download and share the pre - trained models on the fly . Recently , adapters have been proposed as an alternative approach to decrease the substantial number of parameters of PLMs in the fine - tuning stage ( Houlsby et al , 2019 ) . Finetuning with adapters mostly matches the performance of those with the full fine - tuning strategy on many NLP tasks including GLUE benchmark ( Wang et al , 2018 ) and reduces the size of the model from 100s of MB to the order of MB ( Pfeiffer et al , 2020b ) . As such , a natural question arises from the successes of the adapter approach : can the adapter alone adapt PLMs to the target task when it is used in the second phase of the pretraining stage and thus lead to the improvement of the performance on the corresponding task ? In this paper , we explore task - adaptive pretraining , termed TAPT ( Gururangan et al , 2020 ) , with adapters to address this question and overcome the limitations of the conventional full pretraining and fine - tuning . We only train the adapter modules in the second phase of pretraining as well as the fine - tuning stage to achieve both parameter efficiency and the benefits of continual pretraining and compare those with the adapter - based model without pretraining . Surprisingly , we find that directly fine - tuning adapters performs mostly on par with the pre - trained adapter model and outperforms the full TAPT , contradicting the previously proposed benefits of continual pretraining in the full pretraining fine - tuning scheme . As directly fine - tuning adapters skips the second phase of pretraining and the training steps of adapters are faster than those of the full model , it substantially reduces the training time . We further investigate different hyperparameter settings that affect the effectiveness of pretraining .", "entities": [[0, 3, "TaskName", "Pretrained Language Models"], [24, 25, "MethodName", "Transformer"], [97, 98, "DatasetName", "Inspired"], [160, 161, "MethodName", "RoBERTa"], [194, 196, "TaskName", "text classification"], [321, 324, "HyperparameterName", "number of parameters"], [361, 362, "DatasetName", "GLUE"], [515, 517, "TaskName", "continual pretraining"], [562, 564, "TaskName", "continual pretraining"]]}
{"text": "Our implementation is based on HuggingFace since we found AllenNLP ( Gardner et al , 2018 ) used in Gururangan et al , 2020 is incompatible with adapter - transformer ( Pfeiffer et al , 2020b ) . We follow the hyperparameters setting in Gururangan et al , 2020 , and each model in the pretraining and fine - tuning stage is trained on a single GPU ( NVIDIA RTX 3090 ) . Details of hyperparameters are described in Appendix A. Note that for the pretraining step , we use a batch size of 8 and accumulate the gradient for every 32 steps to be consistent with the hyperparameter setting in Gururangan et al , 2020 . We perform pretraining with the self - supervised objectives , which are randomly masked tokens , with a probability of 15 % for each epoch and we do not apply validation to pretraining and save the model at the end of the training from a single seed . For TAPT , we train the entire parameters of the RoBERTa via masked language modeling ( MLM ) on the target dataset , whereas for the adapter - based model , we embed the language adapters in each transformer layer and add invertible adapters after the embedding layers to perform MLM while freezing the original parameters of RoBERTa , following Pfeiffer et al , 2020c . Fine - tuning step is straightforward . We perform fine - tuning parameters that are pretrained via MLM for both TAPT and the adapter model . Validation is performed after each epoch and the best checkpoint is loaded at the end of the training to evaluate the performance on the test set .", "entities": [[91, 93, "HyperparameterName", "batch size"], [175, 176, "MethodName", "RoBERTa"], [177, 180, "TaskName", "masked language modeling"], [181, 182, "DatasetName", "MLM"], [215, 216, "DatasetName", "MLM"], [222, 223, "MethodName", "RoBERTa"], [248, 249, "DatasetName", "MLM"]]}
{"text": "Experiments cover four different models . First , we reproduce the performance of RoBERTa and TAPT in Gururangan et al , 2020 as presented in Appendix C. Then we proceed to the adapter - based approach . To investigate the benefits of task - adaptive pretraining with adapters , we compare the performance of the pre - trained adapter model with the model without pretraining , i.e. , directly fine - tuning adapters in RoBERTa on the target task . For the adapter - based approach , we compare the adapter - based model with the second phase of pretraining and the model without the pretraining . Since the weights of the adapters are randomly initialized , we empirically found that a larger learning rate worked well compared to the full fine - tuning experiments . We sweep the learning rates in { 2e - 5 , 1e - 4 , 3e - 4 , 6e - 4 } and the number of epochs in { 10 , 20 } on the validation set and report the test score that performs the best on the validation set .", "entities": [[13, 14, "MethodName", "RoBERTa"], [74, 75, "MethodName", "RoBERTa"], [123, 125, "HyperparameterName", "learning rate"], [161, 164, "HyperparameterName", "number of epochs"]]}
{"text": "The results are summarized in Table 2 . Surprisingly , for the average F 1 score , the adapter - based model without task - adaptive pretraining performs best , followed by the other adapter with the pretraining model , TAPT , and the baseline RoBERTa . Except for Hyperpartisan news , the adapter model without pretraining performs mostly on par with the counterpart adapter model that involves pretraining on target text corpus , suggesting that the benefits of additional task - adaptive pretraining diminish when we use the adapter - based approach . Furthermore , directly fine - tuned adapter model only trains 1.42 % of the entire parameters which leads to the 30 % faster - training step than the full model and skips the pretraining stage that typically expensive to train than the fine - tuning , substantially reducing Figure 2 : F 1 score as a function of learning rate on test set with log scale on x - axis . F 1 score is averaged over 5 random seeds for low - resource tasks ( CHEMPROT , ACL - ARC , SCIERC , HYPER ) due to the high variance . For high - resource tasks ( RCT , AGNEWS , HELPFULNESS , IMDB ) , we report the F 1 score from a single random seed for each task . For RoBERTa and TAPT , we follow the hyper - parameter settings in Gururangan et al , 2020 except for the learning rate . the training time while the relative speed for the inference only decreases by 2 % to the full model .", "entities": [[45, 46, "MethodName", "RoBERTa"], [152, 154, "HyperparameterName", "learning rate"], [173, 174, "DatasetName", "seeds"], [180, 181, "DatasetName", "CHEMPROT"], [182, 185, "DatasetName", "ACL - ARC"], [186, 187, "DatasetName", "SCIERC"], [208, 209, "DatasetName", "IMDB"], [227, 228, "MethodName", "RoBERTa"], [247, 249, "HyperparameterName", "learning rate"]]}
{"text": "We analyze how the adapter alone can surpass or perform on par with both the full model and adapter model with task - adaptive pretraining . Since we sweep the learning rates and the number of epochs in the range that includes larger figures compared to those in the full model when fine - tuning adapters and kept the other hyper - parameters the same as in Gururangan et al , 2020 , we the larger learning rate zeroes out the benefits of pretraining . Figure 2 . shows the average F 1 score across all tasks as a function of learning rate . The adapter model without a second phase of pretraining consistently outperforms or performs on par with the adapter model with pretraining from 1e - 4 to 6e - 4 , demonstrating that the additional pretraining turns out to be ineffective . In contrast , TAPT outperforms baseline RoBERTa from 2e - 5 , where both TAPT and baseline RoBERTa perform best . The results show that different learning rates used in the fine - tuning stage can affect the effectiveness of pretraining and demonstrate that directly fine - tuning a fraction of parameters can provide comparable performance to the full - model as well as the adapter model with pretraining while substantially reducing the training time . Inspired by the results of the adapter models , we perform the same experiments for the full model ( baseline RoBERTa and TAPT ) on our implementation by sweeping the learning rates and the number of epochs . We hypothesize that proper hyperparameter settings such as a larger learning rate or increasing the number of training steps in the fine - tuning stage can improve the performance of baseline RoBERTa , making pretraining on the unlabeled target task less effective . We sweep the learning rates in { 1e - 5 , 2e - 5 , 3e - 5 } and the number of epochs in { 10 , 20 } on the validation set and report the test score that performs the best on the validation set . Table 3 shows the best performance of the full models for each task among different hyper - parameter settings . The average F 1 score of baseline RoBERTa greatly increases and surprisingly , it surpasses the performance of TAPT in some tasks . The results ensure that although pretraining PLMs on the target task results in better performance , one can achieve comparable performance by simply using a larger learning rate or increasing training steps in the fine - tuning stage while skipping the pretraining step that is computationally demanding compared to the fine - tuning .", "entities": [[34, 37, "HyperparameterName", "number of epochs"], [76, 78, "HyperparameterName", "learning rate"], [101, 103, "HyperparameterName", "learning rate"], [151, 152, "MethodName", "RoBERTa"], [162, 163, "MethodName", "RoBERTa"], [221, 222, "DatasetName", "Inspired"], [241, 242, "MethodName", "RoBERTa"], [255, 258, "HyperparameterName", "number of epochs"], [269, 271, "HyperparameterName", "learning rate"], [290, 291, "MethodName", "RoBERTa"], [323, 326, "HyperparameterName", "number of epochs"], [377, 378, "MethodName", "RoBERTa"], [419, 421, "HyperparameterName", "learning rate"]]}
{"text": "We provide replication results of Gururangan et al , 2020 in Table 9 . Table 7 : Validation performance of adapter experiments . Each score is averaged over 5 random seeds . Evaluation metric is macro - F 1 scores for each task except for CHMEPROT and RCT which use micro - F 1 . Figure 3 : F 1 score as a function of learning rate on development setwith log scale on x - axis . F 1 score is averaged over 5 random seeds for low - resource tasks ( CHEMPROT , ACL - ARC , SCIERC , HYPER ) due to the high variance . For high - resource tasks ( RCT , AGNEWS , HELPFULNESS , IMDB ) , we report the F 1 score from a single random seed for each task . Here we sweep the learning rates in { 1e - 4 , 3e - 4 , 6e - 4 } , the number of epochs in { 10 , 20 } , and the patience factor in { 3 , 5 } .", "entities": [[30, 31, "DatasetName", "seeds"], [65, 67, "HyperparameterName", "learning rate"], [85, 86, "DatasetName", "seeds"], [92, 93, "DatasetName", "CHEMPROT"], [94, 97, "DatasetName", "ACL - ARC"], [98, 99, "DatasetName", "SCIERC"], [120, 121, "DatasetName", "IMDB"], [160, 163, "HyperparameterName", "number of epochs"]]}
{"text": "The JANES system ( the name of the system comes from the Slovene national project JANES inside which the system was developed 1 ) is based on conditional random fields ( CRFs ) ( Lafferty et al , 2001 ) , exploiting the following handcrafted features : lowercased focus token ( token for which features are being extracted ) lowercased tokens in a window of { \u22123 , \u22122 , \u22121 , 1 , 2 , 3 } form the focus token focus token suffixes of length { 1 , 2 , 3 , 4 } features encoding whether the focus token starts with http ( link ) , # ( hashtag ) or @ ( mention ) Brown cluster binary paths for the focus token , with the path length of { 2 , 4 , 6 , 8 } These features were proven to yield optimal results in our previous work on tagging non - standard Slovene ( Ljube\u0161i\u0107 et al , 2017a ) . The Brown clusters , the output of a method for context - dependent hierarchical word clustering ( Brown et al , 1992 ) , were calculated from the web data that were made available through the shared task , namely the slWaC web corpus of Slovene and the hrWaC and srWaC corpora of Croatian and Serbian ( Ljube\u0161i\u0107 and Klubi\u010dka , 2014 ) . We have used default parameters for calculating Brown clusters , except for the minimum occurrence parameter which was set to 5 . The web text was previously lowercased and punctuations and newlines were removed from it . For training the tagger , we exploited ( 1 ) the proximity of the Croatian and Serbian language , and ( 2 ) the fact that we have much more standard training data and much less Twitter training data . We sampled our final training data for each language in the following manner : for Slovene : we added to the Slovene standard training data ten times the available non - standard data , thereby reaching a similar amount of standard and non - standard data in our training set ; from previous work we know that for CRFs oversampling in - domain data is the simplest and most effective method in merging out - domain and in - domain training data ( Horsmann and Zesch , 2015 ; Ljube\u0161i\u0107 et al , 2017a ) for Croatian : we merged the Croatian and the Serbian standard language training datasets , added to it ten copies of the Croatian Twitter training dataset and two copies of the Serbian training dataset , thereby putting emphasis on the Croatian training data , which is expected to be closer to the Croatian test data for Serbian : we merged the Croatian standard training data , two copies of the Serbian standard training data ( as these are more than five times smaller than the Croatian ones ) , ten copies of nonstandard Croatian training data , and four copies of non - standard Serbian training data , with the rationale that most non - standard elements in Croatian are present in non - standard Serbian as well , but with lower frequency ; by oversampling non - standard Croatian in the Serbian dataset we emphasize the non - standard elements in the Croatian non - standard training data as the Serbian non - standard data is much closer to the standard language ( Mili\u010devi\u0107 and Ljube\u0161i\u0107 , 2016 ) The system was implemented in CRFSuite ( Okazaki , 2007 ) , using the passive aggressive optimizer and 10 epochs , a setting which proved to yield best results in previous experiments ( Ljube\u0161i\u0107 and Erjavec , 2016 ) .", "entities": [[599, 600, "HyperparameterName", "optimizer"]]}
{"text": "The first group of results considers different ways of pretraining word embeddings . The word embeddings were always pretrained on the web data available for each language . We considered only two tools for pretraining word embeddings : word2vec ( Mikolov et al , 2013 ) and fasttext ( Bojanowski et al , 2017 ) , and two architectures , CBOW and Skipgram . The results ( word2vec cbow vs. word2vec skipgram ) show for Skipgram to be significantly better suited for this task , which is in line with previous results ( Reimers and Gurevych , 2017 ) . Comparing word2vec and fasttext ( word2vec skipgram vs. fasttext skipgram ) , fasttext shows a slightly better performance , but the difference gets more obvious ( almost half a point in token accuracy ) once fasttext is used to generate representations for the words not present in the pretrained word embeddings ( fasttext skipgram generated ) . 3 setup token accuracy with stdev word2vec cbow 0.8407 \u00b1 0.0025 word2vec skipgram 0.8550 \u00b1 0.0041 fasttext skipgram 0.8578 \u00b1 0.0041 fasttext skipgram generated 0.8596 \u00b1 0.0031 added character - level encoding 0.8780 \u00b1 0.0030 added bidirectional encoding 0.8790 \u00b1 0.0032 additionally tuned on in - domain data 0.8836 \u00b1 0.0026 pretrained character - level encoder on web data 0.8855 \u00b1 0.0015 Table 2 : Initial experiments on the JSI system , performed on the Slovene dataset . The standard deviation is calculated from ten evaluations performed during the last epoch .", "entities": [[10, 12, "TaskName", "word embeddings"], [14, 16, "TaskName", "word embeddings"], [35, 37, "TaskName", "word embeddings"], [47, 48, "MethodName", "fasttext"], [103, 104, "MethodName", "fasttext"], [108, 109, "MethodName", "fasttext"], [112, 113, "MethodName", "fasttext"], [132, 133, "MetricName", "accuracy"], [135, 136, "MethodName", "fasttext"], [149, 151, "TaskName", "word embeddings"], [152, 153, "MethodName", "fasttext"], [160, 161, "MetricName", "accuracy"], [173, 174, "MethodName", "fasttext"], [178, 179, "MethodName", "fasttext"]]}
{"text": "The second group of experiments considers the impact of adding character - level representations of each token to the word representation via a dedicated character - level BiLSTM . Adding the character - level representation has shown the biggest impact among all the experiments , with \u223c 2 accuracy points increase , and a minor difference between encoding the character sequence with a single - direction or a bi - directional LSTM .", "entities": [[27, 28, "MethodName", "BiLSTM"], [48, 49, "MetricName", "accuracy"], [71, 72, "MethodName", "LSTM"]]}
{"text": "Finally , in the last set of experiments we investigated whether there is positive impact if the characterlevel encoder was pretrained on a inflectional - lexicon - like resource . In this shared task the web data were automatically tagged with a CRF tagger relying on a lexicon Ljube\u0161i\u0107 and Erjavec , 2016 ) , therefore we transformed the automatically - tagged web data into a lexicon by ( 1 ) picking only token - tag pairs occurring at least 100 times in the web data and ( 2 ) selecting only the most frequent token - tag pair per token . With the second criterion we lost some information on homonymous words , but also got rid of a lot of wrong automatic annotations of frequent words . The results on pretraining the character - level encoder show that the improvement lies below half an accuracy point , but this improvement showed to be consistent across all the three languages . 4", "entities": [[42, 43, "MethodName", "CRF"], [146, 147, "MetricName", "accuracy"]]}
{"text": "In this section we report the results of the final setups of the JANES and the JSI system and compare it to the HunPos baseline ( Hal\u00e1csy et al , 2007 ) defined by the shared task organizers . Additionally , we report the results of the JANES system using an inflectional lexicon for the specific language , namely Sloleks for Slovene ( Dobrovoljc et al , 2015 ) , hrLex for Croatian ( Ljube\u0161i\u0107 et al , 2016a ) and srLex for Serbian ( Ljube\u0161i\u0107 et al , 2016b ) . We call this system JANES - lex . We compare to this system as it is very straightforward to add information from an inflectional lexicon as additional features to a CRF - based system . Table 3 : Results of the two systems , their two adaptations and the baseline on the test data . Reported metric is token - level accuracy . We also report a modification of the JSI system that we implemented after the shared task was already concluded . Namely , we removed the fully connected layer between the main BiLSTM and the softmax layer , which is actually the most frequent setup for sequence labeling . The removed layer in the JSI system is a residue from the tagger we based our implementation on 5 . We call the simplified tagger JSI - simpler . The results of the two taggers and the two variants are given in Table 3 . The reported results are those obtained on the test data . We can first observe that ( 1 ) all the systems outperform the HunPos baseline by a wide margin and that ( 2 ) the results of the four remaining systems are rather close . The largest difference that can be observed between the four systems are 2 accuracy points on Slovene between the basic CRF implementation ( JANES ) and the simplified BiLSTM implementation ( JSIsimpler ) . The same difference is not to be observed on the other two languages , with the same systems having a difference of 0.5 points on Croatian and 0.3 points on Serbian . The reason for the larger difference on Slovene data lies in the fact that the Slovene data is least standard ( 17 % tokens being nonstandard ) , followed by Croatian ( 13 % non - standard tokens ) , with Serbian data deviating the least from the norm ( 10 % non - standard tokens ) ( Mili\u010devi\u0107 et al , 2017 ) as more complex modeling techniques pay off more as the language deviates stronger from the norm . Adding lexicon information to the JANES system ( JANES vs. JANES - lex ) improves the results on all three languages , but just slightly , between 0.1 % and 0.6 % . Previous work on the problem ( Ljube\u0161i\u0107 et al , 2017a ) has shown that Brown clusters already provide to a large extent the information that was traditionally obtained through inflectional lexicons . Comparing the JANES and JSI results by using the McNemar 's statistical test ( McNemar , 1947 ) , the difference on Slovene is statistically significant at the p < 0.001 level , with an absolute difference in 1.2 points and an relative error reduction of 9.3 % . The differences on the remaining two languages are not statistically significant . When comparing the JSI and JSI - simpler results , it becomes obvious that the additional layer in the JSI system actually deteriorates the results . On all the three languages , the differences are statistically significant , on Slovene and Croatian on the p < 0.001 level , while on Serbian it is on the p < 0.05 level . The level of significance of difference between the JANES and JSI - simpler systems is identical to that of between JSI and JSI - simpler . The most interesting observation from the final evaluation of the submitted and modified systems is that the difference between the traditional CRFs and the ( probably over - hyped ? ) BiLSTMs is actually quite small , with relative error reductions being 15 % on Slovene , 5 % on Croatian and only 3 % on Serbian . These results , as well as some preliminary results on standard test sets , suggest that there would be no significant difference in the results between CRFs and BiLSTMs on standard training and test data .", "entities": [[122, 123, "MethodName", "CRF"], [153, 154, "MetricName", "accuracy"], [186, 187, "MethodName", "BiLSTM"], [189, 190, "MethodName", "softmax"], [307, 308, "MetricName", "accuracy"], [314, 315, "MethodName", "CRF"], [322, 323, "MethodName", "BiLSTM"]]}
{"text": "In this paper we have compared two popular sequence labeling techniques : conditional random fields ( CRFs ) and bidirectional long short - term memories ( BiLSTMs ) on the task of morphosyntactic annotation of tweets written in three closely related South Slavic languages : Slovene , Croatian and Serbian . We have shown that CRFs with well defined features come very close to the performance of the stronger BiLSTM models , the difference between those two being bigger as the data are more nonstandard . The relative error reduction between those two systems lies between 15 % for Slovene , for which the Twitter variety deviates the most from the standard , and 3 % for Serbian , for which the Twitter variety deviates the least . For the CRF system , we have shown that using contextual , suffixal and distributional features gives very good results . The latter make an inflectional lexicon mostly obsolete , with just minor improvements in accuracy if features from large inflectional lexicons are added . For the BiLSTM system , we have shown that encoding a character - level representation of a word is the single most useful intervention , with minor improvements obtained through proper word embedding pretraining , fine - tuning on in - domain data and pretraining the character - level encoder on pairs of words and MSD tags from a large automatically tagged web corpus . With an error analysis we have shown that the types of error performed by each of the systems are actually very similar , most of them still being typical tagger errors for languages with a rich inflectional morphology . However , there is evidence that BiLSTMs resolve long - range dependencies much better , such as discriminating between masculinum nouns in nominative and accusative singular , but yielding slightly more mistakes in the close - range dependencies such as the case of prepositions .", "entities": [[69, 70, "MethodName", "BiLSTM"], [130, 131, "MethodName", "CRF"], [163, 164, "MetricName", "accuracy"], [175, 176, "MethodName", "BiLSTM"], [228, 229, "DatasetName", "MSD"]]}
{"text": "Alignment Methods . Given a bitext B src = ( s 1 , ... , s j , ... , s N ) and B trg = ( t 1 , ... , t i , ... , t M ) where B src is a sentence in the source language and B trg is its translation in the target language , an alignment A is a mapping of words between B src and B trg ( Tiedemann , 2011 ) , formally defined as A \u2286 { ( j , i ) : j = 1 , ... , N ; i = 1 , ... , M } ( 1 ) We study three different settings : ( a ) standard word alignment between corresponding words , ( b ) alignments between all target words and the language label in the input string , and ( c ) the union between the former two . Figure 1 shows an example of those approaches . To produce word alignments between parallel sentences , i.e. , Figure 1 ( a ) , we use the awesome - align tool ( Dou and Neubig , 2021 ) , a recent work that leverages multilingual BERT ( Devlin et al , 2019 ) to extract the links . 1 Models . To train Many - to - Many MNMT models , we use a 6 - layer Transformer architecture ( Vaswani et al , 2017 ) , prepending a language label in the input to indicate the target language ( Johnson et al , 2017 ) . Following Garg et al ( 2019 ) , given an alignment matrix AM M , N and an attention matrix computed by a cross attention head AH M , N , for each target word i , we use the following cross - entropy loss L a to minimize the Kullback - Leibler divergence between AH and AM : L a ( AH , AM ) = \u2212 1 M M i=1 N j=1 AM i , j log ( AH i , j ) ( 2 ) The overall loss L is : L = L t + \u03b3L a ( AH , AM ) ( 3 ) where L t is the standard NLL translation loss , and \u03b3 is a hyperparameter . We use \u03b3 = 0.05 , supervising only one cross attention head at the third last layer . 2 Given the sparse nature of the alignments , we replace the softmax operator in the cross attention head with the \u03b1 - entmax function ( Peters et al , 2019 ; Correia et al , 2019 ) . Entmax allows sparse attention weights for any \u03b1 > 1 . Following Peters et al ( 2019 ) , we use \u03b1=1.5 .", "entities": [[123, 125, "TaskName", "word alignment"], [203, 204, "MethodName", "BERT"], [235, 236, "MethodName", "Transformer"], [309, 310, "MetricName", "loss"], [355, 356, "MetricName", "loss"], [380, 381, "MetricName", "NLL"], [382, 383, "MetricName", "loss"], [385, 386, "HyperparameterName", "\u03b3"], [392, 393, "HyperparameterName", "\u03b3"], [420, 421, "MethodName", "softmax"], [429, 430, "HyperparameterName", "\u03b1"], [454, 455, "HyperparameterName", "\u03b1"]]}
{"text": "We use three highly multilingual MT benchmarks : TED Talks ( Qi et al , 2018 ) . An Englishcentric parallel corpus with 10 M training sentences across 116 translation directions . Following Aharoni et al ( 2019 ) , we evaluate on a total of 16 language directions , while as zeroshot test we evaluate on 4 language pairs . WMT - 2018 ( Bojar et al , 2018 ) . 3 A parallel dataset provided by the WMT - 2018 shared task on news translation . We use all available language pairs , i.e. 14 , up to 5 M training sentences for each language pair . We evaluate the models on the test sets of the shared task , i.e. newstest2018 . As there are no zero - shot test sets provided by the competition , we use the test portion from the Tatoeba - challenge ( Tiedemann , 2020 ) , 4 in all possible language pair combinations included in the challenge . OPUS - 100 ) . An Englishcentric multi - domain benchmark , built upon the OPUS parallel text collection ( Tiedemann , 2012 ) Aharoni et al ( 2019 ) , and 2 its variant with a 1.5 - entmax function on the cross attention heads as in Correia et al ( 2019 ) . The labels ( a ) , ( b ) , ( c ) denote the use of different alignment supervision ( see Section 2 ) . \" # Param . \" : trainable parameter number . \" EN - > X ( 16 ) \" and \" X - > EN ( 16 ) \" : average BLEU scores for English to Non - English languages and for Non - English languages to English on 16 language pairs respectively . \" BLEU zero ( 4 ) \" and \" ACC zero ( 4 ) \" : average BLEU scores and target language identification accuracy over 4 zero - shot language directions . We report average BLEU and accuracy scores , plus the standard deviation over 3 training runs with different random seeds . language pair . It provides supervised translation test data for 188 language pairs , and zero - shot evaluation data for 30 pairs . Following related work ( Aharoni et al , 2019 ; , we apply joint Byte - Pair Encoding ( BPE ) segmentation ( Sennrich et al , 2016 ; Kudo and Richardson , 2018 ) , with a shared vocabulary size of 32 K symbols for TED Talks and 64 K for WMT - 2018 and OPUS - 100 . As evaluation measure , we use tokenized BLEU ( Papineni et al , 2002 ) to be comparable with Aharoni et al ( 2019 ) for the TED Talks benchmark , and SACREBLEU 5 ( Post , 2018 ) for WMT - 2018 and OPUS - 100 . 6 As an additional evaluation , we report the target language identification accuracy score for the zeroshot cases , called ACC zero . We use fasttext as a language identification tool ( Joulin et al , 2017 ) , counting how many times the translation language matches the reference target language . The Transformer models follow the base setting of Vaswani et al ( 2017 ) , with three different random seeds in each run . All of them are trained on the Many - to - Many English - centric scenario , i.e. , on the concatenation of the training data having English either as the source or target language . Details about data and model settings in the Appendix . 5 Signature : BLEU+case.mixed+numrefs.1+smooth.exp+ tok . { 13a , ja - mecab - 0.996 - IPA , zh } + version.1.5.0 6 We report average BLEU over all test sets . Scores for each language pair are available in the supplementary material .", "entities": [[146, 147, "DatasetName", "Tatoeba"], [279, 280, "MetricName", "BLEU"], [303, 304, "MetricName", "BLEU"], [311, 312, "MetricName", "ACC"], [319, 320, "MetricName", "BLEU"], [323, 325, "TaskName", "language identification"], [325, 326, "MetricName", "accuracy"], [337, 338, "MetricName", "BLEU"], [339, 340, "MetricName", "accuracy"], [353, 354, "DatasetName", "seeds"], [398, 399, "MethodName", "BPE"], [446, 447, "MetricName", "BLEU"], [471, 472, "MetricName", "SACREBLEU"], [497, 499, "TaskName", "language identification"], [499, 500, "MetricName", "accuracy"], [507, 508, "MetricName", "ACC"], [512, 513, "MethodName", "fasttext"], [515, 517, "TaskName", "language identification"], [540, 541, "MethodName", "Transformer"], [558, 559, "DatasetName", "seeds"], [634, 635, "MetricName", "BLEU"], [649, 651, "DatasetName", "supplementary material"]]}
{"text": "Throughout this section we refer to our baseline MNMT models by the labels 1 and 2 , while 3 , 4 , and 5 mark the models trained with the auxiliary alignment supervision task , ( a ) , ( b ) , ( c ) from Figure 1 respectively ( see Section 2 ) . TED Talks . Table 1 shows the results on the TED Talks benchmark . Regarding translation quality on the language pairs seen during training ( EN X and X EN columns ) , average BLEU scores from all models end up in the same ballpark . In contrast , zero - shot results vary across the board , with 5 attaining the best performance , with almost 2 BLEU points better than its baseline 2 . Moreover , 5 considerably improves target language identification accuracy ( ACC zero ) , with more stable results , i.e. lower standard deviation , than counterparts . Surprisingly , the addition of alignment supervision ( a ) and ( b ) as an auxiliary task has an overall detrimental effect on the zero - shot performance , even though model 4 results in more stable results than 2 . WMT - 2018 . Table 2 reports the results on the WMT - 2018 benchmark . As expected , in a highresource scenario bilingual baselines are hard to beat . Among multilingual models , the overall performance follows a similar trend as before . Enriching the model with alignment supervision ( c ) results in the best system overall , with an improvement of more than 3 BLEU points in the zero - shot ( 2020 ) . MATT denotes the use of merged attention ( Zhang et al , 2019 ) . LALN and LALT indicate the use of language - aware components . Average BLEU , target language identification accuracy and standard deviation of 3 training runs . testbed compared to baseline 2 , and with stable results across three training runs ( standard deviations of 0.12 and 0.82 ) .", "entities": [[90, 91, "MetricName", "BLEU"], [124, 125, "MetricName", "BLEU"], [138, 140, "TaskName", "language identification"], [140, 141, "MetricName", "accuracy"], [142, 143, "MetricName", "ACC"], [268, 269, "MetricName", "BLEU"], [307, 308, "MetricName", "BLEU"], [310, 312, "TaskName", "language identification"], [312, 313, "MetricName", "accuracy"]]}
{"text": "As one can see from Table 3 , we confirm the positive effect of adding the alignment strategy ( c ) both as translation quality and as a mechanism to produce stable results even in a highly multilingual setup , i.e. , training on 198 language directions . The average score over 30 zeroshot language pairs is low but the individual results range from 0.3 to 17.5 BLEU showing the potentials of multilingual models in this challenging data set as well . 7 Even though the results from our best model still lag behind models with languagespecific components , i.e. MATT+LALN+LALT from , we note that our results demonstrate the positive effect of alignment on zero - shot translation . 8 Overall , our experiments show consistent results across different benchmarks , providing quantitative evidence on the utility of guided alignment in highly multilingual MT scenarios . Supervising a single cross attention head with the alignment method ( c ) substantially reduces the instability between training runs , mitigating the off - target translation issue in the zero - shot evaluation . Zero - shot improvements , i.e. BLEU zero and ACC zero , are large in two benchmarks out of three , i.e. Ted Talks and WMT - 2018 , and with a similar trend in OPUS - 100 . We also note that performance differences may be related to the different data sizes ( see Appendix A ) . TED Talks is a rather small and imbalanced multilingual dataset with 116 language directions with a total of 10 M training sentences , while WMT - 2018 and OPUS - 100 comprise 14 language pairs for a total of 47.8 M training sentences , and 110 M training sentences for 198 language pairs , respectively . We plan on investigating the impact of the training size and the resulting alignments on the zero - shot test sets further in future work . Limitations Finally , we highlight that we have focused on a quantitative evaluation on Englishcentric MNMT benchmarks only , therefore we lack a comprehensive evaluation on complete MNMT benchmarks including training data without English as source and target language ( Freitag and Firat , 2020 ; Rios et al , 2020 ; Tiedemann , 2020 ; Goyal et al , 2021 ) .", "entities": [[67, 68, "MetricName", "BLEU"], [188, 189, "MetricName", "BLEU"], [191, 192, "MetricName", "ACC"]]}
{"text": "We use the OpenNMT - py framework ( Klein et al , 2017 ) , and the Transformer base model setting ( Vaswani et al , 2017 ) . Specifically , we use 6 layers for the encoder and the decoder , 512 as model dimension , and 2048 as hidden dimension . We applied 0.1 as dropout for both residual layers and attention weights , using the Adam optimizer ( Kingma and Ba , 2015 ) with \u03b21 = 0.9 , and \u03b22 = 0.998 , with learning rate set at 3 and 40 K warmup steps as in Aharoni et al ( 2019 ) . We train the models with three random seeds each , for 200 K training steps for the TED Talks and WMT - 2018 benchmarks , while for 500 K training steps for the OPUS - 100 . To speed up training , we use halfprecision , i.e. , FP16 .", "entities": [[17, 18, "MethodName", "Transformer"], [48, 49, "DatasetName", "2048"], [68, 69, "MethodName", "Adam"], [69, 70, "HyperparameterName", "optimizer"], [88, 90, "HyperparameterName", "learning rate"], [114, 115, "DatasetName", "seeds"]]}
{"text": "In this paper , we discuss our submission for DialDoc subtask 1 . The subtask requires systems to extract knowledge from FAQ - type documents vital to reply to a user 's query in a conversational setting . We experiment with pretraining a BERT - based question - answering model on different QA datasets from MRQA , as well as conversational QA datasets like CoQA and QuAC . Our results show that models pretrained on CoQA and QuAC perform better than their counterparts that are pretrained on MRQA datasets . Our results also indicate that adding more pretraining data does not necessarily result in improved performance . Our final model , which is an ensemble of AlBERT - XL pretrained on CoQA and QuAC independently , with the chosen answer having the highest average probability score , achieves an F1 - Score of 70.9 % on the official test - set .", "entities": [[43, 44, "MethodName", "BERT"], [55, 56, "DatasetName", "MRQA"], [64, 65, "DatasetName", "CoQA"], [66, 67, "DatasetName", "QuAC"], [75, 76, "DatasetName", "CoQA"], [77, 78, "DatasetName", "QuAC"], [87, 88, "DatasetName", "MRQA"], [121, 122, "DatasetName", "CoQA"], [123, 124, "DatasetName", "QuAC"], [139, 142, "MetricName", "F1 - Score"]]}
{"text": "We pass the pre - processed training data through a QA model that leverages a transformer encoder to contextually represent the question ( dialogue history ) along with the context ( document ) . Since the grounding document is often longer than the maximum input sequence length for transformers , we follow ( Feng et al , 2020 ) and truncate the documents in sliding windows with a stride . The document trunk and the dialogue history are passed through the transformer encoder to create contextual representations for each token in the input . To extract the beginning and the ending positions of the answer span within the document , the encoded embeddings are sent to a linear layer to output two logits that correspond to the probability of the position being the start and end position of the answer span . The training loss is computed using the Cross - Entropy loss function . We use the huggingface transformers toolkit in all of our experiments .", "entities": [[117, 119, "MethodName", "linear layer"], [144, 145, "MetricName", "loss"], [152, 153, "MetricName", "loss"]]}
{"text": "Firstly , we briefly describe the different datasets used for the continual pretraining of our transformer - based QA models . MRQA - 19 Shared task ( Fisch et al , 2019 ) CoQA ( Reddy et al , 2019 ) . 2 For both datasets , we filter out samples which do not adhere to SQuADlike extractive QA setup ( e.g. yes / no questions ) or have a context length of more than 5000 characters . Table 1 presents the size of the different pretraining datasets after the removal of non - extractive QA samples .", "entities": [[11, 13, "TaskName", "continual pretraining"], [21, 22, "DatasetName", "MRQA"], [33, 34, "DatasetName", "CoQA"], [70, 72, "HyperparameterName", "context length"]]}
{"text": "The shared - task relies on Exact Match ( EM ) and F1 metrics to evaluate the systems on subtask 1 . To compute these scores , we use the metrics for SQuAD from huggingface . 3", "entities": [[6, 8, "MetricName", "Exact Match"], [9, 10, "MetricName", "EM"], [12, 13, "MetricName", "F1"], [32, 33, "DatasetName", "SQuAD"]]}
{"text": "We only submitted our best performing models on the official test set due to a constraint on the number of submissions . Contrary to the trends for testdev phase , albert - xl models trained on conversational QA datasets perform the best . albert - xl + QuAC is the best - performing single model according to the EM metric ( EM = 52.60 ) , whereas albert - xl + CoQA performs the best on F1 metric ( F 1 = 69.48 ) on the test set .", "entities": [[47, 48, "DatasetName", "QuAC"], [58, 59, "MetricName", "EM"], [61, 62, "MetricName", "EM"], [71, 72, "DatasetName", "CoQA"], [76, 77, "MetricName", "F1"]]}
{"text": "We perform ensembling over the outputs of the model variants to obtain a single unified ranked list . For a given question Q , we produce 20 candidate spans , along with a corresponding probability score ps . We compute rank - scores rs for the answer - spans at rank r as rs = 1 log 2 ( r+1 ) . We then aggregate the information of the answer spans for the model variants using the following techniques . Frequent : We chose the answer span which was the most frequent across the model variants . Rank Score : We chose the answer span which was the highest average rank score . Probability Score : We chose the answer span which was the highest average probability score . We observe empirically that ensembling using the probability score performs the best and hence we report the results of ensembling using the probability score ( E ) in Table 3 . We observe the highest gains after ensembling the outputs of all the 5 model variants on the validation test and test - dev set . However , the best performance on the test set was achieved by ensembling over the albert - xl models pre - trained independently on CoQA and QuAC ( EM = 53.5 , F 1 = 70.9 ) . This was the final submission for our team .", "entities": [[98, 99, "MetricName", "Score"], [114, 115, "MetricName", "Score"], [209, 210, "DatasetName", "CoQA"], [211, 212, "DatasetName", "QuAC"], [213, 214, "MetricName", "EM"]]}
{"text": "We investigate the disparate impact of pretraining on different MRQA - 19 datasets on the Doc2Dial shared task . Specifically , we explored factors such as answer length , relative position of the answer in the context , question length , and context length in Table 4 . We observe that the SQuAD , NewsQA , and NaturalQuestions ( NQ ) has compartaively longer answers than the other datasets . However , we do not observe a noticeable difference in terms of question length , context length or relative position of the answer in the context , with respect to the other datasets . We also use the dataset of Li and Roth ( 2002 ) to train a BERT classifier to predict answer type of a question with 97 % accuracy . The coarse - answer types are DESC ( Description ) , NUM ( Numerical ) , ENT ( Entity ) , HUM ( Person ) , LOC ( Location ) and ABBR ( Abbreviation ) . We use the classifier to gauge the distribution of answer types on MRQA datasets and Doc2Dial . We observe from Figure 2 that a majority of questions in Doc2Dial require a descriptive answer . These DESC type questions are more prevelant in SQuAD , NewsQA , and NQ , which might explain their efficacy . To ascertain the benefit of intelligent sampling , we pretrain on a much smaller subset of the SQuAD , NewsQA , and NaturalQuestions dataset , which we obtain via intelligent sampling . We select questions which satisfy one of the following criteria , ( i ) the answer length of the question is \u2265 50 , ( ii ) the question includes ' how ' or ' why ' question word or ( iii ) the answer type of the question is ' DESC ' . Overall , the size of the selected sample is only 20 % of the original dataset , yet achieves a higher EM score than the combined dataset as seen in Table 2 . Yet , surprisingly , the performance is lower than each of the individual dataset .", "entities": [[9, 10, "DatasetName", "MRQA"], [15, 16, "DatasetName", "Doc2Dial"], [42, 44, "HyperparameterName", "context length"], [52, 53, "DatasetName", "SQuAD"], [54, 55, "DatasetName", "NewsQA"], [59, 60, "DatasetName", "NQ"], [85, 87, "HyperparameterName", "context length"], [119, 120, "MethodName", "BERT"], [131, 132, "MetricName", "accuracy"], [181, 182, "DatasetName", "MRQA"], [184, 185, "DatasetName", "Doc2Dial"], [197, 198, "DatasetName", "Doc2Dial"], [211, 212, "DatasetName", "SQuAD"], [213, 214, "DatasetName", "NewsQA"], [216, 217, "DatasetName", "NQ"], [241, 242, "DatasetName", "SQuAD"], [243, 244, "DatasetName", "NewsQA"], [331, 332, "MetricName", "EM"]]}
{"text": "Our submission to the DialDoc subtask 1 performs continual pretraining of a transformer - based encoder on out - of - domain QA datasets . Experiments with different QA datasets suggest that conversational QA datasets like CoQA and QuAC are highly beneficial as their setup is substantially similar to Doc2Dial , the downstream dataset of interest . Our final submission ensembles two AlBERT - XL models independently pretrained on CoQA and QuAC and achieves an F1 - Score of 70.9 % and EM - Score of 53.5 % on the competition test - set .", "entities": [[8, 10, "TaskName", "continual pretraining"], [36, 37, "DatasetName", "CoQA"], [38, 39, "DatasetName", "QuAC"], [49, 50, "DatasetName", "Doc2Dial"], [69, 70, "DatasetName", "CoQA"], [71, 72, "DatasetName", "QuAC"], [75, 78, "MetricName", "F1 - Score"], [82, 83, "MetricName", "EM"], [84, 85, "MetricName", "Score"]]}
{"text": "Stance detection determines whether the author of a text is in favor of , against or neutral to a specific target and provides valuable insights into important events such as presidential election . However , progress on stance detection has been hampered by the absence of large annotated datasets . In this paper , we present P - STANCE , a large stance detection dataset in the political domain , which contains 21 , 574 labeled tweets . We provide a detailed description of the newly created dataset and develop deep learning models on it . Our best model achieves a macro - average F1 - score of 80.53 % , which we improve further by using semi - supervised learning . Moreover , our P - STANCE dataset can facilitate research in the fields of cross - domain stance detection such as cross - target stance detection where a classifier is adapted from a different but related target . We publicly release our dataset and code . 1", "entities": [[0, 2, "TaskName", "Stance detection"], [37, 39, "TaskName", "stance detection"], [62, 64, "TaskName", "stance detection"], [104, 107, "MetricName", "F1 - score"], [139, 141, "TaskName", "stance detection"], [146, 148, "TaskName", "stance detection"]]}
{"text": "Nowadays , people often express their stances toward specific targets ( e.g. , political events or figures , religion , or abortion ) on social media . These opinions can provide valuable insights into important events , e.g. , presidential election . The goal of the stance detection task is to determine whether the author of a piece of text is in favor of , against , or neutral toward a specific target ( Mohammad et al , 2016b ; K\u00fc\u00e7\u00fck and Can , 2020 ; ALDayel and Magdy , 2021 ) . Twitter as a social platform has produced a large quantity of user - generated content , which has become a rich source for mining useful information about various topics such as presidential election . Political figures , who usually receive considerable attention and involve themselves in a large number of political events , are great targets to study stance detection . Therefore , detecting the 1 https://github.com/chuchun8/PStance stance expressed toward political figures on Twitter has drawn a lot of attention in the NLP community ( Mohammad et al , 2016a ; Darwish et al , 2017 ) . Even though stance detection has received a lot of attention , the annotated data are usually limited , which poses strong challenges to supervised models . Moreover , a limitation of existing datasets is that explicit mentions of targets and surface - level lexical cues that may expose the stance can be widely observed in the data ( Mohammad et al , 2016a ; Swami et al , 2018 ; Darwish et al , 2018 ; Conforti et al , 2020b ; Lai et al , 2020 ) , which means a model can detect the stance without extracting effective representations for the meanings of sentences ( i.e. , their lexical and compositional semantics ) . Another limitation of existing datasets , especially the datasets built on social media , is that the average length of tweets is short , which indicates that the data in these previous datasets are less informative and thus the stance can be detected more easily . In an effort to minimize these drawbacks , we present P - STANCE , a dataset for stance detection whose primary goal is to bridge these gaps by making it possible to run large - scale evaluations that require a deeper semantic understanding . This large annotated dataset is composed of 21 , 574 English tweets in the political domain and each tweet is annotated with a stance toward one of three different targets : \" Donald Trump , \" \" Joe Biden , \" and \" Bernie Sanders . \" Examples from our dataset and their stance labels are shown in Table 1 . The main motivation of building this dataset is to provide a new benchmark for in - target stance detection where a classifier is trained and validated on the same target . However , we show additional interest in constructing a large corpus to facilitate research on cross - target stance detection where a classifier is adapted from different but related target . More interestingly , P - Stance enables a new task in stance detection , which is cross - topic stance detection where a classifier is adapted from the same target but with different topics in the past . These tasks , which use labeled training data of a source target and aim to train a model that generalizes well to a destination target with a shifted distribution , hold great practical value . Our contributions include the following : 1 ) We present P - STANCE , a large dataset for stance detection composed of 21 , 574 tweets sampled from over 2.8 million tweets collected from Twitter . P - STANCE is more than three times larger than the previous benchmark ( Mohammad et al , 2016a ) and brings additional challenges such as linguistic complexities . We provide a detailed description and a comprehensive analysis of this dataset ; 2 ) We conduct experiments on the proposed P - STANCE dataset and establish a strong baseline based on BERTweet ( Nguyen et al , 2020 ) . BERTweet achieves a macro - average F1 - score of 80.53 % , which we improve further by using semisupervised learning ; 3 ) The union of P - STANCE and previous benchmark datasets provides more opportunities for studying other stance detection tasks , e.g. , cross - target stance detection and crosstopic stance detection .", "entities": [[46, 48, "TaskName", "stance detection"], [151, 153, "TaskName", "stance detection"], [193, 195, "TaskName", "stance detection"], [370, 372, "TaskName", "stance detection"], [475, 477, "TaskName", "stance detection"], [507, 509, "TaskName", "stance detection"], [531, 533, "TaskName", "stance detection"], [539, 541, "TaskName", "stance detection"], [611, 613, "TaskName", "stance detection"], [705, 708, "MetricName", "F1 - score"], [739, 741, "TaskName", "stance detection"], [748, 750, "TaskName", "stance detection"], [752, 754, "TaskName", "stance detection"]]}
{"text": "We gathered stance annotations of three targets through the Amazon Mechanical Turk ( AMT ) crowdsourcing platform . The AMT workers were asked to annotate each tweet with \" Favor , \" \" Against , \" \" None , \" or \" I do n't know . \" To ensure the annotation quality , we employed strict requirements for the annotators : 1 ) Many completed tasks ( > 500 ) ; 2 ) To reside in the USA ; 3 ) A high acceptance rate ( > 95 % ) . Moreover , we ran the annotation process in several batches of 1000 examples . In each batch , we include 100 internally annotated examples to measure the quality of the annotators . If an annotator mislabels more than 25 % of these examples , we discard the annotations of the worker completely , and relabel them . Interestingly , this process led to a considerable number of reannotations , amounting for more than 20 % of the data . Each tweet was labeled by three random annotators , and disagreements in the labels were decided by the majority voting among the three annotators . After obtaining the annotation results , we computed Krippendorff 's alpha ( Krippendorff , 2011 ) as the measure of inter - annotator agreement , as shown in Table 5 . Tweets that were annotated with label \" I do n't know \" after the majority voting were removed from the dataset . We observed that annotators had difficulties in reaching an agreement on tweets with label \" None \" and the average of Krippendorff 's alpha values increases from 0.60 to 0.81 when we consider two classes : \" Favor \" and \" Against \" . Similar to prior work ( Vamvas and Sennrich , 2020 ) , we removed the label \" None \" from the dataset in our experiments .", "entities": [[206, 207, "HyperparameterName", "alpha"], [272, 273, "HyperparameterName", "alpha"]]}
{"text": "Similar to Mohammad et al ( 2017 ) and , F avg and macro - average of F1 - score ( F macro ) are adopted to evaluate the performance of our baseline models . First , the F1 - score of label \" Favor \" and \" Against \" is calculated as follows : F f avor = 2P f avor R f avor P f avor + R f avor ( 1 ) F against = 2P against R against P against + R against ( 2 ) where P and R are precision and recall , respectively . After that , the F avg is calculated as : F avg = F f avor + F against 2 ( 3 ) We compute the F avg for each target . F macro is calculated by averaging the F avg across all targets .", "entities": [[17, 20, "MetricName", "F1 - score"], [38, 41, "MetricName", "F1 - score"]]}
{"text": "We run experiments with the following baselines . BiLSTM ( Schuster and Paliwal , 1997 ) : A BiL - STM model that takes tweets as inputs without considering the target information . CNN ( Kim , 2014 ) : Similar to BiLSTM , the vanilla CNN only takes tweets as inputs and does not consider the target information . TAN ( Du et al , 2017 ) : TAN is an attention - based LSTM model that extracts target specific features . BiCE ( Augenstein et al , 2016 ) : A BiLSTM that uses conditional encoding for stance detection . The target information is first encoded by a BiLSTM , whose hidden representations are then used to initialize another BiLSTM with tweets as inputs . BiCE is also a strong baseline for crosstarget stance detection . CrossNet ( Xu et al , 2018 ) : CrossNet is another model for cross - target stance detection . It encodes the target and the tweet by using the same approach with BiCE and add an aspect attention layer to signal the core part of a stance - bearing input . Cross - Net improves BiCE in many cross - target settings . GCAE ( Xue and Li , 2018 ) : A CNN model that utilizes a gating mechanism to block targetunrelated information . GCAE is a strong baseline for aspect - based sentiment analysis and we apply it to our stance detection task . PGCNN ( Huang and Carley , 2018 ) : Similar to GCAE , PGCNN is based on gated convolutional networks and encodes target information by generating target - sensitive filters . BERT ( Devlin et al , 2019 ) : A pre - trained language model that predicts the stance by appending a linear classification layer to the hidden representation of [ CLS ] token . We fine - tune the BERT - base on the stance detection task . BERTweet ( Nguyen et al , 2020 ) : BERTweet is another pre - trained language model following the training procedure of RoBERTa ( Liu et al , 2019 ) . Similar to BERT , we fine - tune the pretrained BERTweet to predict the stance by appending a linear classification layer to the hidden representation of the [ CLS ] token . The pre - trained BERTweet model is fine - tuned under the PyTorch framework . The maximum sequence length is set to 128 and the batch size is 32 . We use AdamW optimizer ( Loshchilov and Hutter , 2019 ) and the learning rate is 2e - 5 .", "entities": [[8, 9, "MethodName", "BiLSTM"], [42, 43, "MethodName", "BiLSTM"], [75, 76, "MethodName", "LSTM"], [93, 94, "MethodName", "BiLSTM"], [99, 101, "TaskName", "stance detection"], [110, 111, "MethodName", "BiLSTM"], [121, 122, "MethodName", "BiLSTM"], [135, 137, "TaskName", "stance detection"], [155, 157, "TaskName", "stance detection"], [230, 235, "TaskName", "aspect - based sentiment analysis"], [241, 243, "TaskName", "stance detection"], [276, 277, "MethodName", "BERT"], [316, 317, "MethodName", "BERT"], [321, 323, "TaskName", "stance detection"], [347, 348, "MethodName", "RoBERTa"], [358, 359, "MethodName", "BERT"], [413, 415, "HyperparameterName", "batch size"], [420, 421, "MethodName", "AdamW"], [421, 422, "HyperparameterName", "optimizer"], [431, 433, "HyperparameterName", "learning rate"]]}
{"text": "During elections , there is a considerable amount of data generated by users expressing their opinions about candidates , out of which only a small amount can be annotated and used for supervised stance detection . We explore the potential of the abundant unlabeled tweets and show that we can leverage them to improve the performance of our models . To this end , we turn to semi - supervised learning , and leverage techniques such as Uncertainty - aware Self - Training ( UST ) . UST ( Mukherjee and Awadallah , 2020 ) is a semi - supervised approach which uses the standard teacher - student self - training framework , but adds a few powerful changes . Concretely , UST designs different techniques which leverage the uncertainty of the teacher model to select the unlabeled set of examples in each self - training iteration . First , we train our teacher model on the labeled examples . Next , we compute uncertainty estimates of our teacher model on the set of unlabeled examples by performing a few forward passes with dropout enabled . Finally , we incorporate the uncertainty estimates into our framework as follows : 1 ) We use these estimates to select the examples for which the teacher is most or least confident about . 2 ) We incorporate the teacher confidence in the student loss by penalizing the student 's misclassified examples in which the teacher has high confidence . We use the BERTweet model as teacher and student . We perform various experiments to show the benefits of using a large amount of unlabeled data from P - STANCE - EXT alongside our UST model . We carry out three barely supervised experiments with various number of examples in the training set . Specifically , we experiment with 30 , 50 , and 100 training examples . Moreover , we also consider an experiment using the whole training set to investigate the effect of the unlabeled examples when all the training data are available . We run experiments with different training sets , and report the F1 - scores obtained on the entire testing set . We show the results of our semi - supervised ex -", "entities": [[33, 35, "TaskName", "stance detection"], [230, 231, "MetricName", "loss"], [353, 354, "MetricName", "F1"]]}
{"text": "In this section , we formally define the GEC task discussed in this paper . Let D be the GEC training data that comprise pairs of an ungrammatical source sentence X and grammatical target sentence Y , i.e. , D = { ( X n , Y n ) } n . Here , | D | denotes the number of sentence pairs in the dataset D. Let \u0398 represent all trainable parameters of the model . Our objective is to find the optimal parameter set \u0398 that minimizes the following objective function L ( D , \u0398 ) for the given training data D : L ( D , \u0398 ) = \u2212 1 | D | ( X , Y ) D log ( p ( Y | X , \u0398 ) ) , ( 1 ) where p ( Y | X , \u0398 ) denotes the conditional probability of Y given X. In the standard supervised learning setting , the parallel data D comprise only \" genuine \" parallel data D g ( i.e. , D = D g ) . However , in GEC , incorporating pseudo data D p that are generated from grammatical sentences Y T , where T represents seed corpus ( i.e. , a set of grammatical sentences ) , is common ( Xie et al , 2018 ; Zhao et al , 2019 ; Grundkiewicz et al , 2019 ) . Our interest lies in the following three nontrivial aspects of Equation 1 . Aspect ( i ) : multiple methods for generating pseudo data D p are available ( Section 3 ) . Aspect ( ii ) : options for the seed corpus T are numerous . To the best of our knowledge , how the seed corpus domain affects the model performance is yet to be shown . We compare three corpora , namely , Wikipedia , Simple Wikipedia ( SimpleWiki ) and English Gigaword , as a first trial . Wikipedia and SimpleWiki have similar domains , but different grammatical complexities . Therefore , we can investigate how grammatical complexity affects model performance by comparing these two corpora . We assume that Gigaword contains the smallest amount of noise among the three corpora . We can therefore use Gigaword to investigate whether clean text improves model performance . Aspect ( iii ) : at least two major settings for incorporating D p into the optimization of Equation 1 are available . One is to use the two datasets jointly by concatenating them as D = D g \u222a D p , which hereinafter we refer to as JOINT . The other is to use D p for pretraining , namely , minimizing L ( D p , \u0398 ) to acquire \u0398 , and then fine - tuning the model by minimizing L ( D g , \u0398 ) ; hereinafter , we refer to this setting as PRETRAIN . We investigate these aspects through our extensive experiments ( Section 4 ) .", "entities": [[68, 69, "HyperparameterName", "\u0398"], [86, 87, "HyperparameterName", "\u0398"], [97, 98, "HyperparameterName", "\u0398"], [110, 111, "HyperparameterName", "\u0398"], [132, 133, "HyperparameterName", "\u0398"], [146, 147, "HyperparameterName", "\u0398"], [460, 461, "HyperparameterName", "\u0398"], [464, 465, "HyperparameterName", "\u0398"], [480, 481, "HyperparameterName", "\u0398"]]}
{"text": "Backtranslation for the EncDec model was proposed originally by Sennrich et al ( 2016b ) . In backtranslation , a reverse model , which generates an ungrammatical sentence from a given grammatical sentence , is trained . The output of the reverse model is paired with the input and then used as pseudo data . BACKTRANS ( NOISY ) is a variant of backtranslation that was proposed by Xie et al ( 2018 ) 2 . This method adds r\u03b2 random to the score of each hypothesis in the beam for every time step . Here , noise r is sampled uniformly from the interval [ 0 , 1 ] , and \u03b2 random R \u22650 is a hyper - parameter that controls the noise scale . If we set \u03b2 random = 0 , then BACK - TRANS ( NOISY ) is identical to standard backtranslation . BACKTRANS ( SAMPLE ) is another variant of backtranslation , which was proposed by Edunov et al ( 2018 ) for MT . In BACKTRANS ( SAMPLE ) , sentences are decoded by sampling from the distribution of the reverse model . DIRECTNOISE Whereas BACKTRANS ( NOISY ) and BACKTRANS ( SAMPLE ) generate ungrammatical sentences with a reverse model , DIRECT - NOISE injects noise \" directly \" into grammatical sentences ( Edunov et al , 2018 ; Zhao et al , 2019 ) . Specifically , for each token in the given sentence , this method probabilistically chooses one of the following operations : ( i ) masking with a placeholder token mask , ( ii ) deletion , ( iii ) insertion of a random token , and ( iv ) keeping the original 3 . For each token , the choice is made based on the categorical distribution ( \u00b5 mask , \u00b5 deletion , \u00b5 insertion , \u00b5 keep ) .", "entities": [[106, 107, "DatasetName", "0"], [112, 113, "HyperparameterName", "\u03b2"], [130, 131, "HyperparameterName", "\u03b2"], [133, 134, "DatasetName", "0"]]}
{"text": "We compare the effectiveness of the BACK - TRANS ( NOISY ) , BACKTRANS ( SAMPLE ) , and DIRECTNOISE methods for generating pseudo data . In DIRECTNOISE , we set ( \u00b5 mask , \u00b5 deletion , \u00b5 insertion , \u00b5 keep ) = ( 0.5 , 0.15 , 0.15 , 0.2 ) 11 . We use \u03b2 random = 6 for BACKTRANS ( NOISY ) 12 . In addition , we use ( i ) the JOINT setting and ( ii ) all of SimpleWiki as the seed corpus T throughout this section . The results are summarized in Table 2 . BACK - TRANS ( NOISY ) and BACKTRANS ( SAMPLE ) show competitive values of F 0.5 . Given this result , we exclusively use BACKTRANS ( NOISY ) and discard BACKTRANS ( SAMPLE ) for the rest of the experiments . The advantage of BACKTRANS ( NOISY ) is that its effectiveness in GEC has already been demonstrated by Xie et al ( 2018 ) . In addition , in our preliminary experiment , BACKTRANS ( NOISY ) decoded ungrammatical sentence 1.2 times faster than BACKTRANS ( SAMPLE ) did . We also use DI - RECTNOISE because it achieved the best value of F 0.5 among all the methods .", "entities": [[58, 59, "HyperparameterName", "\u03b2"]]}
{"text": "The present experimental results show that the following configurations are effective for improving the model performance : ( i ) the combination of JOINT and Gigaword ( Section 4.3 ) , ( ii ) the amount of pseudo data D p not being too large in JOINT ( Section 4.4 ( a ) ) , and ( iii ) PRETRAIN with BACK - TRANS ( NOISY ) using large pseudo data D p ( Section 4.4 ( b ) ) . We summarize these findings and attempt to combine PRETRAIN and JOINT . Specifically , we pretrain the model using 70 M pseudo data of BACKTRANS ( NOISY ) . We then fine - tune the model by combining BEA - train and relatively small DIRECTNOISE pseudo data generated from Gigaword ( we set | D p | = 250 K ) . However , the performance does not improve on BEA - valid . Therefore , the best approach available is simply to pretrain the model with large ( 70 M ) BACKTRANS ( NOISY ) pseudo data and then fine - tune using BEAtrain , which hereinafter we refer to as PRETLARGE . We use Gigaword for the seed corpus T because it has the best performance in Table 3 . We evaluate the performance of PRETLARGE on test sets and compare the scores with the current top models . our PRETLARGE achieves F 0.5 = 61.3 on CoNLL - 2014 . This result outperforms not only all previous single - model results but also all ensemble results except for that by Grundkiewicz et al ( 2019 ) . To further improve the performance , we incorporate the following techniques that are widely used in shared tasks such as BEA - 2019 and WMT 13 : Synthetic Spelling Error ( SSE ) Lichtarge et al ( 2019 ) proposed the method of probabilistically injecting character - level noise into the source sentence of pseudo data D p . Specifically , one of the following operations is applied randomly at a rate of 0.003 per character : deletion , insertion , replacement , or transposition of adjacent characters . Right - to - left Re - ranking ( R2L ) Following Sennrich et al ( 2016aSennrich et al ( , 2017 ; Grundkiewicz et al ( 2019 ) , we train four right - to - left models . The ensemble of four left - to - right models generate n - best candidates and their corresponding scores ( i.e. , conditional probabilities ) . We then pass each candidate to the ensemble of the four right - to - left models and compute the score . Finally , we re - rank the n - best candidates based on the sum of the two scores . Sentence - level Error Detection ( SED ) SED classifies whether a given sentence contains a grammatical error . Asano et al ( 2019 ) proposed incorporating SED into the evaluation pipeline and reported improved precision . Here , the GEC model is applied only if SED detects a grammatical error in the given source sentence . The motivation is that SED could potentially reduce the number of false - positive errors of the GEC model . We use the re - implementation of the BERT - based SED model ( Asano et al , 2019 ) . Table 5 presents the results of applying SSE , 13 http://www.statmt.org/wmt19/ R2L , and SED . It is noteworthy that PRET - LARGE+SSE+R2L achieves state - of - the - art performance on both CoNLL - 2014 ( F 0.5 = 65.0 ) and BEA - test ( F 0.5 = 69.8 ) , which are better than those of the best system of the BEA - 2019 shared task ( Grundkiewicz et al , 2019 ) . In addition , PRET - LARGE+SSE+R2L+SED can further improve the performance on BEA - test ( F 0.5 = 70.2 ) . However , unfortunately , incorporating SED decreased the performance on CoNLL - 2014 and JFLEG . This fact implies that SED is sensitive to the domain of the test set since the SED model is fine - tuned with the official validation split of BEA dataset . We leave this sensitivity issue as our future work .", "entities": [[300, 301, "MetricName", "Error"], [302, 303, "MethodName", "SSE"], [471, 472, "MetricName", "Error"], [553, 554, "MethodName", "BERT"], [573, 574, "MethodName", "SSE"], [680, 681, "DatasetName", "JFLEG"]]}
{"text": "In this paper , we describe our systems submitted to the very low resource supervised translation task at WMT20 . We participate in both translation directions for Upper Sorbian - German language pair . Our primary submission is a subword - level Transformer - based neural machine translation model trained on original training bitext . We also conduct several experiments with backtranslation using limited monolingual data in our postsubmission work and include our results for the same . In one such experiment , we observe jumps of up to 2.6 BLEU points over the primary system by pretraining on a synthetic , backtranslated corpus followed by fine - tuning on the original parallel training data .", "entities": [[42, 43, "MethodName", "Transformer"], [46, 48, "TaskName", "machine translation"], [90, 91, "MetricName", "BLEU"]]}
{"text": "The Transformer model is the dominant architecture within current NMT models due to its superior performance on several language pairs . While still a sequence - to - sequence ( Sutskever et al , 2014 ) model composed of an encoder and a decoder , Transformer models are highly parallelizable thanks to being composed purely of feedforward and self - attention layers rather than recurrent layers ( Hochreiter and Schmidhuber , 1997 ; Cho et al , 2014b ) . The reader is encouraged to read the original paper ( Vaswani et al , 2017 ) to gain a deeper understanding of the model . We adopt the Transformer base architecture available under the fairseq 2 ( Ott et al , 2019 ) library for all our models . However , NMT models are known to be datahungry ( Koehn and Knowles , 2017 ) ; their performance improves sharply with the availability of more parallel training data . Except for a few language pairs ( e.g. English - German ) , most have little to no such data available . On the other hand , a far greater number of languages have a decent amount of monolingual data available online ( e.g. Wikipedia ) . To address this issue of lack of parallel data , Sennrich et al ( 2016a ) introduced the concept of backtranslation . It involves creating a synthetic parallel corpus by translating sentences from the target - side monolingual data to the source language and making corresponding pairs . A baseline target source model ( PBSMT or NMT ) , trained with limited data , is generally used for this purpose . It enables the use of large corpora of monolingual data for several languages , the size of which is typically orders of magnitude larger than any corresponding bitext available . What is notable is that only the sourceside data is synthetic in such a scenario and the target - side still corresponds to original monolingual data . Some studies ( Poncelas et al , 2018 ; Popel , 2018 ) have investigated the effects of varying the amount of backtranslated data as a proportion of the total training corpus , including training only on the synthetic dataset as a standalone corpus . We follow some of the related experiments conducted by Kocmi and Bojar ( 2019 ) on Gujarati - English ( another low - resource pair ) with a few exceptions . Besides , we also report performance when pretraining solely on the synthetic corpus following by finetuning on either original or mixed data . While not quite the same , one could think of this approach as having some similarities with transfer learning ( Zoph et al , 2016 ) as well as domain adaptation ( Luong and Manning , 2015 ; Freitag and Al - Onaizan , 2016 ) for machine translation . There has also been work on using sampling ( Edunov et al , 2018 ) for generating backtranslations , but we stick to using beam search in this work .", "entities": [[1, 2, "MethodName", "Transformer"], [45, 46, "MethodName", "Transformer"], [60, 62, "HyperparameterName", "attention layers"], [108, 109, "MethodName", "Transformer"], [450, 452, "TaskName", "transfer learning"], [462, 464, "TaskName", "domain adaptation"], [480, 482, "TaskName", "machine translation"]]}
{"text": "Our primary system is a Transformer base model , trained on the parallel training corpus for both translation directions till 60 epochs . We keep most of the hyperparameters to their default values in fairseq . More precisely , we chose Adam ( Kingma and Ba , 2015 ) as the optimizer and Adam betas were set to 0.9 and 0.98 , respectively . The maximum number of tokens in each batch was set to 4096 . Learning rate was set to 0.0005 , with an inverse squared root decay schedule and 4000 steps of warmup updates . Label smoothing was set to 0.1 and dropout to 0.3 . Label - smoothed cross - entropy was used as the training criterion . We trained all our models for a fixed number of epochs , determined separately for each system , and chose the last checkpoint for reporting BLEU ( Papineni et al , 2002 ) scores on the test sets . All training was done using a single NVIDIA P100 GPU . Due to the small amount of parallel training data , each epoch of training took about 90 seconds on average for the primary system .", "entities": [[5, 6, "MethodName", "Transformer"], [41, 42, "MethodName", "Adam"], [51, 52, "HyperparameterName", "optimizer"], [53, 54, "MethodName", "Adam"], [77, 79, "HyperparameterName", "Learning rate"], [98, 100, "MethodName", "Label smoothing"], [130, 133, "HyperparameterName", "number of epochs"], [147, 148, "MetricName", "BLEU"]]}
{"text": "In this section , we report our post - submission work on using monolingual data for backtranslation . We took the raw monolingual data that we describe in Section 3.1 and backtranslated with our primary submission models for the respective translation directions , i.e. , hsb de for Upper Sorbian data and de hsb for German data . We used fairseq - generate function with a beam size of 5 for this purpose . Once again , we limited the number of subwords in each sentence to 250 . Finally , we took all sentence pairs for backtranslated Upper Sorbian corpus and the first two million sentence pairs for the German corpus . Table 1 indicates the size of the backtranslated corpora by original language . For further experiments , we name the datasets as follows : auth : Processed original training data . synth : Backtranslated de hsb and hsb de corpora . mixed : Augmented training data obtained by mixing auth with a portion of synth in 1:1 ratio , providing a total of 116 , 778 sentence pairs . We define the following systems for making use of the backtranslated data . Note that the first system only differs from the primary system in the number of training epochs completed . auth - from - scratch : This system has the same settings as the primary system . It was trained on the auth corpus till 80 epochs ( as opposed to 60 for primary ) . mixed - from - scratch : We trained models on mixed data from scratch for 40 epochs . 5 synth - from - scratch : Models were trained only on the synth datasets . To adjust for the difference in the size of the respective backtranslated corpora , we trained hsb de system for 10 epochs and de hsb system for 30 epochs . synth - auth - finetune : We took the models trained via the previous system and fine - tuned them on auth data for 20 epochs in each translation direction . synth - mixed - finetune : Same as the last model , except that fine - tuning was done on mixed data . Fine - tuning was carried out by loading pretrained checkpoints and adding extra training flags in reset - optimizer and reset - lr - scheduler .", "entities": [[386, 387, "HyperparameterName", "optimizer"]]}
{"text": "The systems were evaluated on the blind test set ( newstest2020 ) using automated metrics ; no human evaluation was done . Table 2 shows cased BLEU scores for various systems . Our primary systems achieved a BLEU score of 47.6 for Upper Sorbian German and 45.2 for German Upper Sorbian translation . We achieved an improvement of 0.3 and 0.4 BLEU points , respectively , by training further till 80 epochs in each direction . We also evaluated a third system , synth - auth - finetune , as described in Section 4 , which provided a jump of 2.6 points in BLEU score over the primary system for Upper Sorbian German and 2.5 for German Upper Sorbian . In addition to evaluating on blind test sets , we also report BLEU scores on the development test set in the same table . Two outcomes are worth highlighting : Model trained only on synth data for German Upper Sorbian translation matched the performance of a similar model trained on the authentic bitext . Best results were obtained by fine - tuning a model trained on synth data with either auth or mixed . The second result is notable since the regime of pretraining followed by fine - tuning improves the BLEU scores by up to 4 points on this test set when compared to training only on the original bitext . Moreover , while the model trained on synth was not able to match the performance of that trained on auth for Upper Sorbian German , it still provides the same benefits as German Upper Sorbian model when fine - tuned further . Looking at the small improvements achieved by using only the mixed corpus for training , increasing its size by combining upsampled auth data with more synth data might lead to even further jumps in the BLEU scores .", "entities": [[26, 27, "MetricName", "BLEU"], [37, 39, "MetricName", "BLEU score"], [61, 62, "MetricName", "BLEU"], [103, 105, "MetricName", "BLEU score"], [132, 133, "MetricName", "BLEU"], [211, 212, "MetricName", "BLEU"], [309, 310, "MetricName", "BLEU"]]}
{"text": "In this paper , we described our Transformer model for supervised machine translation for Upper Sorbian - German language pair . We take note of relatively high BLEU scores achieved by our primary systems ( and those of other participants ) on this low - resource language pair , which could relate to the high quality of the training corpus . We also report results and takeaways from several experiments with backtranslated data completed post the shared task . A key result is matching the performance of a system trained on the original bitext with one trained on a limited amount of synthetic , backtranslated data . Domain mismatch and a difference in the quality of monolingual corpus might have prevented the system from achieving a similar result in the other direction . We notice big improvements in performance over the primary systems by following a \" pretraining then fine - tuning \" regime . An interesting future work would be to measure the applicability of this approach to other lowresource language pairs . Additional systems could be added as well . For instance , models trained on mixed data and fine - tuned on auth data might provide a meaningful comparison . Prior work ( Ding et al , 2019 ) has shown that the number of BPE merge operations has a significant effect on the performance of NMT systems . This work was pointed out during the review process and should be an avenue for further improvement of the model performance .", "entities": [[7, 8, "MethodName", "Transformer"], [11, 13, "TaskName", "machine translation"], [27, 28, "MetricName", "BLEU"], [218, 219, "MethodName", "BPE"]]}
{"text": "Recently , neural twitter sentiment classification has become one of state - of - thearts , which requires less feature engineering work compared with traditional methods . In this paper , we propose a simple and effective ensemble method to further boost the performances of neural models . We collect several word embedding sets which are publicly released ( often are learned on different corpus ) or constructed by running Skip - gram on released largescale corpus . We make an assumption that different word embeddings cover different words and encode different semantic knowledge , thus using them together can improve the generalizations and performances of neural models . In the SemEval 2017 , our method ranks 1st in Accuracy , 5th in AverageR. Meanwhile , the additional comparisons demonstrate the superiority of our model over these ones based on only one word embedding set . We release our code 1 for the method replicability .", "entities": [[19, 21, "TaskName", "feature engineering"], [84, 86, "TaskName", "word embeddings"], [119, 120, "MetricName", "Accuracy"]]}
{"text": "We use 4 embedding sets which are described in Table 1 . Meanwhile , we crawl and merge all annotated datasets of previous SemEvals , and split them into training , development , and testing sets with ratio 8:1:1 , which are shown in Table 2 together with testing set of SemEval 2017 . From the table , we can see that testing set of SemEval 2017 has big differences on the category ratio ( negative : neutral : positive ) , compared with the previous SemEval datasets . For the model settings , all RCNN models have same configurations but different word embedding sets . We set dimensions of hidden vectors to 250 and depths d to 2 . To avoid model over - fitting , we use dropout and regularization as follows : ( 1 ) the regularization parameter is set to 1e - 5 ; ( 2 ) the dropout rate is set to 0.3 , which is applied in the final text representation . All parameters are learned by Adam optimizer ( Kingma and Ba , 2014 ) with the learning rate 0.001 . Note that , all word embedding sets are fixed when training . All models are tuned by the development set in Training .", "entities": [[173, 174, "MethodName", "Adam"], [174, 175, "HyperparameterName", "optimizer"], [184, 186, "HyperparameterName", "learning rate"]]}
{"text": "In this section , we first report the results on datasets of previous SemEvals , which are described in Table 3 . Then , we report the performances of our method on SemEval 2017 in Table 4 . From the Table 3 , we observe that gloveT performs worst though it is trained on in - domain twitter dataset and the word2vecY performs best though it is derived from yelp reviews . As far as we known , Yelp data is constructed by carefully filtering and is high - quality . Thus , we can include that the quality of corpus is also important as the size of corpus and domain in twitter sentiment classification . Additionally , we can infer that word2vecGN outperforms others in recall of negative category , word2vecY performs best in recall of neutral category , and gloveT is best in recall of positive category . Different embedding sets propose different characteristics . Additionally , the ensemble method obtains a significant improvement of 4 % . In the Table 4 , we compare our method with best and median systems in SemEval 2017 , and report the results of individual embedding sets . Our method outperforms other systems in accuracy , but performs worse in R Average , especially in R Negative . Compared with the median system , our method has improvements of about 5 % in both accuracy and R Average . Different from the results in worse among these embedding sets , while the gloveT obtains best performances . Additionally , we can observe that gloveT performs best both in R Negative and R Positive , and word2vecY performs best in R Neutral . Compared with the embedding baselines , our ensemble method obtains improvements of 2.7 % and 1.5 % in accuracy and R Average respectively , which demonstrates the effectiveness of the proposed method .", "entities": [[203, 204, "MetricName", "accuracy"], [233, 234, "MetricName", "accuracy"], [299, 300, "MetricName", "accuracy"]]}
{"text": "To understand the meaning of a sentence or a text , it is essential to analyze relations between a predicate and its arguments . Such analysis is called semantic role labeling ( SRL ) or predicate - argument structure ( PAS ) analysis . For English , the accuracy of SRL has reached approximately 80 % - 90 % ( Ouchi et al , 2018 ; Strubell et al , 2018 ; Tan et al , 2018 ) . However , there are many omissions of arguments in Japanese , and the accuracy of Japanese PAS analysis on omitted arguments is still around 50 % - 60 % ( Shibata et al , 2016 ; Shibata and Kurohashi , 2018 ; Kurita et al , 2018 ; Ouchi et al , 2017 ) . A reason for such low accuracy is the shortage of gold datasets and knowledge about PAS analysis , which require a prohibitive cost of creation ( Iida et al , 2007 ; Kawahara et al , 2002 ) . From the viewpoint of text understanding , machine comprehension ( MC ) has been actively studied in recent years . In MC studies , QA datasets consisting of triplets of a document , a question and * The current affiliation is Yahoo Japan Corporation . its answer are constructed , and an MC model is trained using these datasets ( e.g. , Rajpurkar et al ( 2016 ) and Trischler et al ( 2017 ) ) . MC has made remarkable progress in the last couple of years , and MC models have even exceeded human accuracy in some datasets ( Devlin et al , 2019 ) . However , MC accuracy is not necessarily high for documents that contain anaphoric phenomena and those that need external knowledge or inference ( Mihaylov et al , 2018 ; . In this paper , we propose a Japanese PAS analysis method based on the MC framework for a specific domain . In particular , we focus on a challenging task of finding an antecedent of a zero pronoun within PAS analysis . We construct a widecoverage QA dataset for PAS analysis ( PAS - QA ) in the domain and feed it to an MC model to perform PAS analysis . We also construct a QA dataset for reading comprehension ( RC - QA ) in the same domain and jointly use the two datasets in the MC model to improve PAS analysis . We consider the domain of blogs on driving because of the following two reasons . Firstly , we can construct high - quality QA datasets in a short time using crowdsourcing . Crowdworkers can interpret driving blog articles based on the traffic commonsense shared by the society . Secondly , if computers can understand driving situations correctly by extracting driving behavior from blogs , it is possible to predict danger and warn drivers to achieve safer transportation . Our contributions are summarized as follows . FitzGerald et al ( 2018 ) and constructed QA - SRL Bank 2.0 and QAMRs using crowdsourcing , respectively . They asked crowdworkers to generate question - answer pairs that represent a PAS . These datasets are similar to our PAS - QA dataset , but different in that we focus on omitted arguments and automatically generate questions ( see Section 3.1 ) . Many RC - QA datasets have been constructed in recent years . For example , Rajpurkar et al ( 2016 ) constructed SQuAD 1.1 , which contains 100 K crowdsourced questions and answer spans in a Wikipedia article . Rajpurkar et al ( 2018 ) updated SQuAD 1.1 to 2.0 by adding unanswerable questions . Some RC - QA datasets have been built in a specific domain ( Welbl et al , 2017 ; Suster and Daelemans , 2018 ; Pampari et al , 2018 ) .", "entities": [[28, 31, "TaskName", "semantic role labeling"], [48, 49, "MetricName", "accuracy"], [92, 93, "MetricName", "accuracy"], [139, 140, "MetricName", "accuracy"], [269, 270, "MetricName", "accuracy"], [284, 285, "MetricName", "accuracy"], [389, 391, "TaskName", "reading comprehension"], [508, 513, "DatasetName", "QA - SRL Bank 2.0"], [586, 587, "DatasetName", "SQuAD"], [610, 611, "DatasetName", "SQuAD"]]}
{"text": "Many MC models based on neural networks have been proposed to solve RC - QA datasets . For example , Devlin et al ( 2019 ) proposed an MC model using a language representation model , BERT , which achieved a high - ranked accuracy on the SQuAD 1.1 leaderboard as of September 30 , 2019 . As a previous study of transfer learning of MC models to other tasks , Pan et al ( 2018 ) pre - trained an MC model using an RC - QA dataset and transfered the pre - trained knowledge to sequence - tosequence models . They used SQuAD 1.1 as the RC - QA dataset and experimented on translation and summarization . While they used different models for pre - training and fine - tuning , we use the same MC model by constructing PAS - QA and RC - QA datasets in the same QA form .", "entities": [[36, 37, "MethodName", "BERT"], [44, 45, "MetricName", "accuracy"], [47, 48, "DatasetName", "SQuAD"], [62, 64, "TaskName", "transfer learning"], [104, 105, "DatasetName", "SQuAD"], [117, 118, "TaskName", "summarization"]]}
{"text": "We conduct PAS analysis experiments of our MCsingle / merged / stepwise methods using the PAS - QA and RC - QA datasets . We also compare our methods with the neural network - based PAS analysis model ( Shibata and Kurohashi , 2018 ) ( hereafter , NN - PAS ) , which achieved the state - of - theart accuracy on Japanese PAS analysis .", "entities": [[61, 62, "MetricName", "accuracy"]]}
{"text": "We adopt BERT ( Devlin et al , 2019 ) as an MC model . We split the triplets in the PAS - QA dataset as shown in Table 4 . All sentences in these datasets are preprocessed using the Japanese morphological analyzer , JUMAN++ 3 . We trained a Japanese pre - trained BERT model using Japanese Wikipedia , which consists of approximately 18 million sentences . The input sentences were segmented into words by JUMAN++ , and words were broken into subwords by applying BPE ( Sennrich et al , 2016 ) . The parameters of BERT are the same as English BERT BASE . The number of epochs for the pre - training was 30 . The state - of - the - art baseline PAS analyzer , NN - PAS , was trained using the existing PAS dataset , KWDLC 4 ( Kyoto University Web Document Leads Corpus ) , as described in Shibata and Kurohashi ( 2018 ) . We also trained an NN - PAS model using the PAS - QA dataset in addition to KWDLC ( hereafter , NN - PAS \u2032 ) . For this training , the PAS - QA dataset was converted to the same format as KWDLC , where questions are deleted , and only answers are used . The PAS - QA test data is used to compare the baseline methods with the proposed methods . As an evaluation measure , EM ( Exact Match ) is used for all the MC models . EM is defined as ( the number of questions in which the system answer matches the gold answer in the dataset ) / ( the number of questions in the entire dataset ) . For each experimental condition , training and testing were conducted five times , and the average scores were calculated . 3 and 4 , only the outputs of MC - stepwise were correct . We found some cases that MC - stepwise successfully captured knowledge in the driving domain . In the example shown in Figure 4 , the correspondence between \" \" ( climb up the slope ) and \" \" ( going up the slope ) can be recognized . MC - merged 's answer \" \" ( the hill road ) , which has a coreference relation with \"", "entities": [[2, 3, "MethodName", "BERT"], [54, 55, "MethodName", "BERT"], [86, 87, "MethodName", "BPE"], [98, 99, "MethodName", "BERT"], [104, 105, "MethodName", "BERT"], [105, 106, "MethodName", "BASE"], [108, 111, "HyperparameterName", "number of epochs"], [243, 244, "MetricName", "EM"], [245, 247, "MetricName", "Exact Match"], [256, 257, "MetricName", "EM"]]}
{"text": "Multimodal pre - training has propelled great advancement in vision - and - language research . These large - scale pre - trained models , although successful , fatefully suffer from slow inference speed due to enormous computation cost mainly from cross - modal attention in Transformer architecture . When applied to reallife applications , such latency and computation demand severely deter the practical use of pre - trained models . In this paper , we study Image - text retrieval ( ITR ) , the most mature scenario of V+L application , which has been widely studied even prior to the emergence of recent pre - trained models . We propose a simple yet highly effective approach , LightningDOT that accelerates the inference time of ITR by thousands of times , without sacrificing accuracy . LightningDOT removes the time - consuming cross - modal attention by pre - training on three novel learning objectives , extracting feature indexes offline , and employing instant dot - product matching with further re - ranking , which significantly speeds up retrieval process . In fact , Light - ningDOT achieves new state of the art across multiple ITR benchmarks such as Flickr30k , COCO and Multi30 K , outperforming existing pre - trained models that consume 1000\u00d7 magnitude of computational hours . 1", "entities": [[46, 47, "MethodName", "Transformer"], [134, 135, "MetricName", "accuracy"], [199, 200, "DatasetName", "Flickr30k"], [201, 202, "DatasetName", "COCO"]]}
{"text": "V+L Pre - training Inspired by the success of Transformer - based ( Vaswani et al , 2017 ) language model pre - training ( Devlin et al , 2019 ; Yang et al , 2019 ; Raffel et al , 2020 ; Lan et al , 2020 ; Clark et al , 2020 ) , vision - andlanguage pre - training ( Huang et al , 2020b ; Su et al , 2020 ; Li et al , 2020bLi et al , , 2019a has become the prevailing paradigm in learning multimodal representations , with strong results on tasks such as image - text retrieval ( Kiros et al , 2014 ) , visual question answering ( Antol et al , 2015 ) and referring expression comprehension ( Yu et al , 2016 ) . Exemplary works include two - stream ( Tan and Bansal , 2019 ; and single - stream models Li et al , 2020a ; . Multi - task learning and adversarial training are also explored . This family of pre - training methods aims for general - purpose V+L without computation cost consideration . To the best of our knowledge , our work is the first known effort on pre - training visualsemantic embedding that enables low - latency realtime cross - modal retrieval . Ours is concurrent work with CLIP ( Radford et al , 2021 ) . Image - Text Retrieval Early cross - modal embedding works ( Kiros et al , 2014 ; Faghri et al , 2017 ) focus on using a twostream model to learn a unified visual - semantic embedding , with progressive improvement on two popular benchmarks : Flickr30 K ( Plummer et al , 2015 ) and COCO ( Chen et al , 2015 ) . Later methods with cross - attention ( Lee et al , 2018Wang et al , 2019 ; become more popular , with significant performance gain . Pre - trained V+L models also fall into this category . By exploiting large - scale image - text datasets , pretrained V+L models further push the performance on Flickr30 K and COCO . Although achieving high recall , cross - attention requires excessive computation cost during inference that can not be overlooked . 2 In this work , inspired by dense retrieval in text retrieval domain ( Guu et al , 2020 ; Karpukhin et al , 2020 ; Xiong et al , 2020 ; Mao et al , 2020 ; Lewis et al , 2020 ) , we propose a more efficient attention - less framework . With pre - training , our model achieves better performance while being significantly faster than cross - modal attention methods . Note that the proposed approach is orthogonal to model compression techniques that reduce the number of layers / parameters ( Sun et al , 2019 ; Jiao et al , 2020 ) , since we do not reduce the number of parameters from the UNITER baseline . These two approaches can be combined to further boost the speed , which is an interesting future work direction .", "entities": [[4, 5, "DatasetName", "Inspired"], [9, 10, "MethodName", "Transformer"], [114, 117, "DatasetName", "visual question answering"], [125, 128, "TaskName", "referring expression comprehension"], [161, 165, "TaskName", "Multi - task learning"], [216, 220, "TaskName", "cross - modal retrieval"], [226, 227, "MethodName", "CLIP"], [291, 292, "DatasetName", "COCO"], [358, 359, "DatasetName", "COCO"], [464, 466, "TaskName", "model compression"], [470, 473, "HyperparameterName", "number of layers"], [495, 498, "HyperparameterName", "number of parameters"], [500, 501, "MethodName", "UNITER"]]}
{"text": "We denote the Transformer - based ( Vaswani et al , 2017 ) image encoder and language encoder by f \u03b8 V and f \u03b8 L , respectively ( \u03b8 V , \u03b8 L are learnable parameters ) . Given a dataset of paired image and text { ( i , t ) } , we first extract region features v = { v 0 , v 1 , . . . , v N } ( v j R dv , N is the number of regions ) for image i , along with bounding box positions of regions via a pre - trained Faster - RCNN ( Ren et al , 2015 ; Anderson et al , 2018 ) . 3 The image encoder f \u03b8 V encodes this sequence of image regions into a d - dimensional space f \u03b8 V ( v ) = h = { h 0 , . . . , h N } ( h j R d ) . The corresponding text t is tokenized into sub - word units and projected into high - dimensional feature vectors w = { w 0 , w 1 , ... , w T } ( w j R dw , T is the number of tokens ) following Devlin et al ( 2019 ) . 4 Similarly , the text encoding process can be written as f \u03b8 L ( w ) = z = { z 0 , . . . , z T } ( z j R d ) . We regard the output [ CLS ] embedding h 0 as global image representation , and z 0 as global text representation . Following sections discuss how to jointly train these two encoders to learn strong visual - semantic embeddings , through three pre - training objectives .", "entities": [[3, 4, "MethodName", "Transformer"], [20, 21, "HyperparameterName", "\u03b8"], [24, 25, "HyperparameterName", "\u03b8"], [29, 30, "HyperparameterName", "\u03b8"], [32, 33, "HyperparameterName", "\u03b8"], [64, 65, "DatasetName", "0"], [127, 128, "HyperparameterName", "\u03b8"], [142, 143, "HyperparameterName", "\u03b8"], [152, 153, "DatasetName", "0"], [191, 192, "DatasetName", "0"], [234, 235, "HyperparameterName", "\u03b8"], [244, 245, "DatasetName", "0"], [269, 270, "DatasetName", "0"], [277, 278, "DatasetName", "0"]]}
{"text": "L MLM ( t ) = \u2212 log P \u03b8 L ( w m | w \\m ) = \u2212 1 M M k=1 log P \u03b8 mlm ( w m k | z m k ) , ( 1 ) where \u03b8 mlm is the additional parameters introduced to map hidden states z to word probabilities . Under the V+L setting , the textual input is usually highly correlated with the image . To leverage this cross - modal relation , we propose visualembedding fused MLM ( VMLM ) , in which the paired image i is considered as additional input when training the model to reconstruct masked tokens in sentence t. The loss function of VMLM can be formulated as : L VMLM ( t , i ) = \u2212 log P \u03b8 ( w m | w \\m , i ) = \u2212 1 M M k=1 log P \u03b8 mlm ( w m k | z m k + h 0 ) , ( 2 ) where \u03b8 = { \u03b8 V , \u03b8 L } and the word probabilities P \u03b8 are conditioned on the corresponding image i via the global image representation h 0 . Although VMLM takes a similar mathematical form to the MLM task proposed in UNITER , they differ in two main aspects : 1 ) LightningDOT uses two separate encoders ( h 0 is computed by f \u03b8 V ) ; and 2 ) visual dependency is explicitly injected to text representations ( z m k + h 0 ) , instead of implicitly learned through cross - modal attention . Semantic - embedding Fused Masked Region Modeling ( SMRM ) Recent works on V+L pretraining Tan and Bansal , 2019 ) have shown that mask - then - reconstruct pre - training on image regions also helps image+text embedding learning . Similar to MLM , Masked Region Modeling ( MRM ) is supervised by : L MRM ( i ) = D \u03b8mrm ( v m , f \u03b8 V ( v \\m ) ) = 1 M M k=1 D \u03b8mrm ( v m k , h m k ) , ( 3 ) where D can be any differentiable distance function . Among the variants of MRM , we consider Masked Region Feature Regression ( MRFR ) with L2 distance and Masked Region Classification with KL - Divergence ( MRC - kl ) , due to their proven success in learning V+L representations . 6 In MRFR , the L 2 distance between two feature vectors x and y is defined as : D \u03b8 fr ( x , y ) = k x k \u2212 g \u03b8 fr ( y k ) 2 2 , where 2 denotes L 2 - norm , and g \u03b8 fr ( ) is a learnable Multi - layer Perceptron ( MLP ) with parameters \u03b8 fr . The KL - divergence D KL in MRC - kl measures distance between two probability distributions : D \u03b8mrc ( x , y ) = k D KL ( c ( x k ) | | g \u03b8mrc ( y k ) ) , where \u03b8 mrc is the parameters of a trainable MLP that maps feature vector x k to the object class distribution c ( x k ) predicted by Faster R - CNN . To incorporate language information encoded in the paired text , we extend MRM to Semanticembedding fused MRM ( SMRM ) , where the global text representation z 0 is exploited when reconstructing masked regions . L SMRM ( i , t ) = D \u03b8mrm ( v m , f \u03b8 V ( v \\m ) , t ) = 1 M M k=1 D \u03b8mrm ( v m k , h m k + z 0 ) . ( 4 ) The specific variants SMRFR and SMRC - kl can be derived using the corresponding distance function , which is omitted for simplicity . Note that both the cross - modal fusion introduced in Eqn . ( 2 ) and Eqn . ( 4 ) uses simple addition without introducing extra parameters from their uni - modal counterpart . Moreover , the extra parameters \u03b8 mlm and \u03b8 mrm is not needed at downstream inference so will not slow down the retrieval . Cross - modal Retrieval Objective ( CMR ) Beyond image or text focused reconstructive objectives , we also propose a new pre - training task , Cross - modal Retrieval ( CMR ) , to leverage the paired information between image and text . With this learning objective , the model is optimized to promote high similarity score for a matched imagesentence pair ( i , t ) and vice versa . The similarity score between query t and image i is defined as : S ( t , i ) = z 0 , h 0 , ( 5 ) where , denotes the inner product between two vectors , and h 0 and z 0 are the output [ CLS ] embeddings from image encoder f \u03b8 V and language encoder f \u03b8 L , respectively . In order to capture both image - retrieval and textretrieval supervision signals in a single forwardbackward pass , we propose a bi - directional variant of contrastive loss . Given any matched image - text pair ( i , t ) , we treat text t as the query , sample n \u2212 1 negative images { i 2 , i 3 , . . . , i n } , and then compute the objective function as : L ( t ) IR = \u2212 log e S ( t , i 1 ) n k=1 e S ( t , i k ) , where t 1 : = t. Similarly , we take image i as query ( i 1 : = i ) , sample n \u2212 1 negative text , and compute : L ( i ) TR = \u2212 log e S ( i , t 1 ) n k=1 e S ( i , t k ) to optimize for text retrieval . Following Henderson et al ( 2017 ) ; Gillick et al ( 2019 ) ; Karpukhin et al ( 2020 ) , we use in - batch negatives to avoid the actual sampling of a negative image or text : given a batch of n positive image - text pairs B = { ( i 1 , t 1 ) , . . . , ( i n , t n ) } , we use all other images from within the batch as negatives ( { i j } , where j { 1 , 2 , . . . , n } and j = k ) for every positive pair ( i k , t k ) , and vice versa for negative text . The final CMR loss for batch B is : L CMR ( B ) = 1 2n n k=1 L ( i k ) TR + L ( t k ) IR . ( 6 ) An illustration of L CMR is presented in Figure 3 . 7 Through joint pre - training with CMR , VMLM and SMRM , the visual - semantic embeddings learned from image encoder and language encoder can be readily applied to downstream tasks . During finetuning stage , we directly adopt CMR loss to supervise the training process . 7 The whole similarity matrix can be computed efficiently with one batched matrix multiplication call . This operation can take advantage of GPU hardware with Tensor Cores for faster training .", "entities": [[1, 2, "DatasetName", "MLM"], [9, 10, "HyperparameterName", "\u03b8"], [26, 27, "HyperparameterName", "\u03b8"], [27, 28, "DatasetName", "mlm"], [42, 43, "HyperparameterName", "\u03b8"], [43, 44, "DatasetName", "mlm"], [86, 87, "DatasetName", "MLM"], [114, 115, "MetricName", "loss"], [134, 135, "HyperparameterName", "\u03b8"], [152, 153, "HyperparameterName", "\u03b8"], [153, 154, "DatasetName", "mlm"], [164, 165, "DatasetName", "0"], [171, 172, "HyperparameterName", "\u03b8"], [174, 175, "HyperparameterName", "\u03b8"], [177, 178, "HyperparameterName", "\u03b8"], [185, 186, "HyperparameterName", "\u03b8"], [199, 200, "DatasetName", "0"], [210, 211, "DatasetName", "MLM"], [214, 215, "MethodName", "UNITER"], [232, 233, "DatasetName", "0"], [237, 238, "HyperparameterName", "\u03b8"], [258, 259, "DatasetName", "0"], [314, 315, "DatasetName", "MLM"], [339, 340, "HyperparameterName", "\u03b8"], [396, 397, "TaskName", "Classification"], [437, 438, "HyperparameterName", "\u03b8"], [450, 451, "HyperparameterName", "\u03b8"], [469, 470, "HyperparameterName", "\u03b8"], [481, 482, "DatasetName", "MLP"], [485, 486, "HyperparameterName", "\u03b8"], [533, 534, "HyperparameterName", "\u03b8"], [541, 542, "DatasetName", "MLP"], [560, 564, "MethodName", "Faster R - CNN"], [592, 593, "DatasetName", "0"], [615, 616, "HyperparameterName", "\u03b8"], [641, 642, "DatasetName", "0"], [710, 711, "HyperparameterName", "\u03b8"], [711, 712, "DatasetName", "mlm"], [713, 714, "HyperparameterName", "\u03b8"], [729, 733, "TaskName", "Cross - modal Retrieval"], [755, 759, "TaskName", "Cross - modal Retrieval"], [822, 823, "DatasetName", "0"], [825, 826, "DatasetName", "0"], [842, 843, "DatasetName", "0"], [845, 846, "DatasetName", "0"], [857, 858, "HyperparameterName", "\u03b8"], [863, 864, "HyperparameterName", "\u03b8"], [895, 896, "MetricName", "loss"], [1169, 1170, "MetricName", "loss"], [1254, 1255, "MetricName", "loss"]]}
{"text": "For simplicity , we take text - to - image retrieval as an example to introduce the real - time inference pipeline ( Figure 2 Offline Feature Extraction Image retrieval task requires the model to rank every image i in an image database I based on its similarity to a text query t. In LightningDOT , we first apply the image encoder f \u03b8 V to all images in I , and cache the resulting global image representations { h ( Johnson et al , 2019 ) in memory for later use . Note that the entire image - to - index process , including Faster - RCNN feature extraction and Transformer encoding , can all be conducted offline . Therefore , for every new query t at real time , the cached index can be reused for maximum inference time saving . ( i ) 0 R d | i I } into an index Online Retrieval During inference , given a text query t , we encode it with the language encoder \u03b8 L , and then compute its similarity score to the embedding of every image in I ( stored in memory index ) via Eqn ( 5 ) . Finally , the images will be ranked by their similarity scores , from the highest to lowest . In practice , people are more interested in top - K retrieval , with a list of K images I t satisfying : I t : = { i m k } K k=1 , where S ( t , i m 1 ) \u2265 S ( t , i m 2 ) \u2265 \u2265 S ( t , i m K ) and S ( t , i m K ) \u2265 S ( t , i ) \u2200i ( I \\ I t ) . ( 7 ) This optimization problem has been well studied , and we use FAISS ( Johnson et al , 2019 ) to solve it in our implementation . It is worth noting that in order to apply fast search , the similarity function has to be decomposable . Therefore , we choose the simple dot product as S instead of a more complicated neural network function . Similarly , for text retrieval , the same architecture can be applied by simply pre - computing the embedding for all sentences and using an image as query instead . Re - ranking To further improve retrieval accuracy , we propose a two - stage approach by adopting an optional re - ranking model . In the first stage , we use LightningDOT to retrieve top - M images ( or texts ) , where M is an integer much smaller R@1 R@5 R@10 R@1 R@5 R@10 AR R@1 R@5 R@10 R@1 R@5 R@10 AR", "entities": [[5, 11, "TaskName", "text - to - image retrieval"], [28, 30, "TaskName", "Image retrieval"], [63, 64, "HyperparameterName", "\u03b8"], [111, 112, "MethodName", "Transformer"], [146, 147, "DatasetName", "0"], [174, 175, "HyperparameterName", "\u03b8"], [413, 414, "MetricName", "accuracy"], [457, 458, "MetricName", "R@1"], [458, 459, "MetricName", "R@5"], [459, 460, "MetricName", "R@10"], [460, 461, "MetricName", "R@1"], [461, 462, "MetricName", "R@5"], [462, 463, "MetricName", "R@10"], [464, 465, "MetricName", "R@1"], [465, 466, "MetricName", "R@5"], [466, 467, "MetricName", "R@10"], [467, 468, "MetricName", "R@1"], [468, 469, "MetricName", "R@5"], [469, 470, "MetricName", "R@10"]]}
{"text": "We compare the proposed approach with state - ofthe - art methods ( with and without pre - training ) and report the results in Table 1 . Without crossattention , our method outperforms non - pre - training approaches by large margins on all metrics . Specifically , our model improves over CAAN ( SOTA method with cross - attention ) by 3.3 % ( 73.5 vs. 70.2 ) on COCO and 9.5 % ( 89.3 vs. 79.8 ) on Flickr30 K in terms of AR . When compared with methods without cross - attention ( VSE++ ( Faghri et al , 2017 ) and SCO ( Huang et al , 2018 ) ) , LightningDOT achieves nearly Model COCO Full ( 123 K Images ) Flickr30 K Full ( 31 K Images ) Text Retrieval Image Retrieval Text Retrieval Image Retrieval R@5 R@10 R@20 R@5 R@10 R@20 AR R@5 R@10 R@20 R@5 R@10 R@20 AR ( Lee et al , 2018 ) . LightningDOT with / without UNITER - base re - ranker is significantly faster . 20 - point gain on AR . Although LightningDOT achieves slightly lower AR than UNITER ( pretraining method with cross - attention ) , with 3.5/1.1 points drop on Flickr30K / COCO , it is 600/1900 \u00d7 faster than UNITER during inference time . We further apply second - stage re - ranking , and use UNITER to score top - M retrieved image - text pairs from LightningDOT to obtain the final top - K ranked lists . With re - ranking , LightningDOT achieves an instant performance lift , surpassing UNITER on both benchmarks , while still 46 - 95 times faster than UNITER . With an even stronger re - ranker OSCAR , LightningDOT achieves similar results to the state - of - the - art performance on COCO .", "entities": [[71, 72, "DatasetName", "COCO"], [120, 121, "DatasetName", "COCO"], [137, 139, "TaskName", "Image Retrieval"], [141, 143, "TaskName", "Image Retrieval"], [143, 144, "MetricName", "R@5"], [144, 145, "MetricName", "R@10"], [146, 147, "MetricName", "R@5"], [147, 148, "MetricName", "R@10"], [150, 151, "MetricName", "R@5"], [151, 152, "MetricName", "R@10"], [153, 154, "MetricName", "R@5"], [154, 155, "MetricName", "R@10"], [169, 170, "MethodName", "UNITER"], [193, 194, "MethodName", "UNITER"], [211, 212, "DatasetName", "COCO"], [219, 220, "MethodName", "UNITER"], [236, 237, "MethodName", "UNITER"], [272, 273, "MethodName", "UNITER"], [285, 286, "MethodName", "UNITER"], [294, 295, "MethodName", "OSCAR"], [311, 312, "DatasetName", "COCO"]]}
{"text": "To demonstrate the efficiency of LightningDOT , we use UNITER - base as baseline to compare inference speed . We also compare with a more lightweight cross - attention method SCAN ( Lee et al , 2018 ) , which uses GRU ( Chung et al , 2014 ) instead of a 12 - layer Transformer . All methods are tested on a single TITAN RTX GPU , with batch size of 400 . As shown in Table 3 , SCAN is \u223c1.9\u00d7 faster than UNITER - base across both benchmarks , as the computational cost of GRU is much cheaper than that of Transformer ( performance drop is significant though ) . However , the speedup from SCAN is limited , as it computes cross - attention between each query and all images . On the other hand , LightningDOT is 639\u00d7 faster than UNITER on Flickr30K. When tested with 5 times more i m - ages in COCO , the speedup from LightningDOT is 1927\u00d7. Even with re - ranking , LightningDOT is still much more efficient than UNITER - base ( 46\u00d7 faster on Flickr30 K and 95\u00d7 faster on COCO ) . To mimic a real - life scenario for image retrieval , where the candidate pool contains hundreds of thousands of images , we combine all images from training , validation and test set to form a larger candidate pool . Note that models are still trained on the training set . Although the number of text queries remain the same , the number of candidate images scales up by > 20\u00d7 , where cross - attention methods immediately become impractical . We refer this setting on both benchmarks as Flickr30k - full ( 31k ) and COCO - full ( 123k ) . Our algorithm is 6 , 591\u00d7 faster on Flickr30k - full and 23 , 869\u00d7 faster on COCO - full , which clearly shows the advantage of LightningDOT and its potential in real - world applications . With re - ranking , LightningDOT is still more than 1 , 000\u00d7 and 2 , 000\u00d7 faster on Flickr30kfull and COCO - full , respectively . In general , for other re - rankers such as OSCAR , our algorithm can approximately speed up inference by N images /M times , where N images is the number of candidate images , and M is number of re - ranked images from top - M retrieved results by LightningDOT . Similarly , we construct a full setting for text retrieval by combining all text queries from training , validation and test set . Results are summarized in Table 2 . Considering the size of candidate pool has become more than 20\u00d7 larger , we adopt recall at top 5 , 10 , 50 as evaluation metrics . Our method achieves reasonably good performance , with AR of 44.4 on COCO and 70.2 on Flickr30K. Re - ranking further lifts AR to 56.4 and 76.2 . Results from UNITER or SCAN are not included as the computation of pairwise scores is extremely expensive , given the excessive amount of retrieval candidates . While LightningDOT only takes minutes to evaluate , UNITER - base is estimated to take about 28 days 9 to evaluate under the full setting for both Text Retrieval Image Retrieval Method R@1 R@5 R@10 R@1 R@5 R@10 AR", "entities": [[9, 10, "MethodName", "UNITER"], [30, 31, "DatasetName", "SCAN"], [41, 42, "MethodName", "GRU"], [55, 56, "MethodName", "Transformer"], [64, 65, "DatasetName", "TITAN"], [69, 71, "HyperparameterName", "batch size"], [80, 81, "DatasetName", "SCAN"], [85, 86, "MethodName", "UNITER"], [97, 98, "MethodName", "GRU"], [104, 105, "MethodName", "Transformer"], [118, 119, "DatasetName", "SCAN"], [145, 146, "MethodName", "UNITER"], [159, 160, "DatasetName", "COCO"], [180, 181, "MethodName", "UNITER"], [193, 194, "DatasetName", "COCO"], [204, 206, "TaskName", "image retrieval"], [285, 286, "DatasetName", "Flickr30k"], [292, 293, "DatasetName", "COCO"], [307, 308, "DatasetName", "Flickr30k"], [316, 317, "DatasetName", "COCO"], [357, 358, "DatasetName", "COCO"], [373, 374, "MethodName", "OSCAR"], [485, 486, "DatasetName", "COCO"], [503, 504, "MethodName", "UNITER"], [505, 506, "DatasetName", "SCAN"], [535, 536, "MethodName", "UNITER"], [556, 558, "TaskName", "Image Retrieval"], [559, 560, "MetricName", "R@1"], [560, 561, "MetricName", "R@5"], [561, 562, "MetricName", "R@10"], [562, 563, "MetricName", "R@1"], [563, 564, "MetricName", "R@5"], [564, 565, "MetricName", "R@10"]]}
{"text": "Image Retrieval LightningDOT R@1 R@5 R@10 R@1 R@5 R@10 AR image retrieval and text retrieval . In addition , We compare all models with the same setting : cache as much as possible for fastest speed , where our model outperforms others in both speed and space on image retrieval . The proposed algorithm maps each image to a 768 - dimensional vector , which only consumes about 300Mb storage space for the whole COCO dataset . For crossattention models such as SCAN , UNITER or OS - CAR , they also need to cache image features , which typically requires to save a 36 x 2048 dimensional vector per image , and it consumes about 28 GB storage space for COCO dataset .", "entities": [[0, 2, "TaskName", "Image Retrieval"], [3, 4, "MetricName", "R@1"], [4, 5, "MetricName", "R@5"], [5, 6, "MetricName", "R@10"], [6, 7, "MetricName", "R@1"], [7, 8, "MetricName", "R@5"], [8, 9, "MetricName", "R@10"], [10, 12, "TaskName", "image retrieval"], [48, 50, "TaskName", "image retrieval"], [74, 75, "DatasetName", "COCO"], [82, 83, "DatasetName", "SCAN"], [84, 85, "MethodName", "UNITER"], [88, 89, "DatasetName", "CAR"], [106, 107, "DatasetName", "2048"], [121, 122, "DatasetName", "COCO"]]}
{"text": "We further report results on multilingual image - text retrieval tasks . Specially , we evaluate Lightning - DOT under the translate - test setting , which is to translate the test captions in other languages to English by leveraging Machine Translation ( MT ) tool . 10 Note that our method is only trained on English captions , without exploiting the original or translated captions from multilingual benchmarks . We consider two benchmarks : Multi30 K ( Elliott et al , 2016 ( Elliott et al , , 2017Barrault et al , 2018 ) with captions in German , French and Czech ; and COCO Japanese ( Yoshikawa et al , 2017 ) and Chinese ( Li et al , 2019b ) . Average Recall ( AR ) is used as the evaluation metric . Meta - Ave , the average of AR over different languages across two benchmarks , is used as a global metric . More details on multilingual ITR benchmarks are included in Appendix . We compare LightningDOT against 3 task - specific methods : S - LIWE ( Wehrmann et al , 2019 ) , MULE and SMALR ( Burns et al , 2020 ) , which all exploit captions in different languages to learn multilingual or language - agnostic word embeddings . We also compare with a pre - trained model M 3 P ( Huang et al , 2020a ) , which is alternatively pre - trained with image - caption pairs labeled in English and cross - lingual corpus in 100 different languages . Note that all methods discussed above are trained / finetuned on captions in different languages . For fair comparison , we report performance of UNITER under the same translate - test setting , which is finetuned with English captions only and tested on translated captions . Table 6 shows similar trends of performance improvements as on English benchmarks . Compared to both state - of - the - art task - specific methods and pre - trained models , LightningDOT under translatetest setting achieves new state of the art on most languages and establishes a strong baseline for future study on these multilingual benchmarks .", "entities": [[40, 42, "TaskName", "Machine Translation"], [105, 106, "DatasetName", "COCO"], [125, 126, "MetricName", "Recall"], [215, 217, "TaskName", "word embeddings"], [286, 287, "MethodName", "UNITER"]]}
{"text": "To further facilitate the reproductivity of our proposed method , we include more details about the choice of model size and hyper - parameters for both pre - training and fine - tuning . The model dimensions are set to ( L=12 , H=768 , A=12 ) for both image encoder and language encoder , where L is the number of stacked Transformer blocks ; H stands for hidden activation dimension , and A is the number of attention heads . The total number of parameters in LightningDOT is 220M. Pre - training and finetuning learn the parameters of both encoders . During inference , with offline representation caching , only the forwarding pass with one encoder from the query modality will be performed online . For both pre - training and finetuning , AdamW ( Loshchilov and Hutter , 2019 ) is used to optimize the model training , with \u03b2 1 = 0.9 , \u03b2 2 = 0.98 . We adopt a learning rate warmup strategy , where the learning rate is linearly increased during the first 10 % of training steps , followed by a linear decay to 0 . We set the L2 weight decay to be 0.01 . During pre - training , we follow UNITER to randomly sample 1 task per minibatch update . 11 Our best model is pre - trained on VMLM+SMRM+CRM for 300 , 000 optimization steps . We set the batch size to 10240 per GPU ( batch size is specified by # tokens + # regions , as in UNITER ) . Pre - training experiments are conducted on 8\u00d7 V100 GPUs with 6 - step gradient accumulation , and the learning rate is set to be 5e - 5 . For ablation studies presented in Table 5 , the ablated instances of our model are pre - trained for 30k steps on COCO dataset ( Lin et al , 2014 ) only , and the same choice of learning rate and batch size are applied as in the best pre - training setting . For finetuning , we set batch size n to 96 ( n is in examples , instead of the sequence length of tokens and regions ) , and search learning rate from { 1e - 5 , 2e - 5 , 5e - 5 } . We select models based on their AR on the validation set . The best learning rate is 5e - 5 for COCO and 1e - 5 for Flickr30K. Our models are trained for 15 epochs on Flickr30k , and 20 epochs on COCO . For re - ranking , we choose k from { 20 , 50 } .", "entities": [[62, 63, "MethodName", "Transformer"], [83, 86, "HyperparameterName", "number of parameters"], [134, 135, "MethodName", "AdamW"], [151, 152, "HyperparameterName", "\u03b2"], [156, 157, "HyperparameterName", "\u03b2"], [164, 166, "HyperparameterName", "learning rate"], [171, 173, "HyperparameterName", "learning rate"], [191, 192, "DatasetName", "0"], [197, 199, "MethodName", "weight decay"], [210, 211, "MethodName", "UNITER"], [240, 242, "HyperparameterName", "batch size"], [247, 249, "HyperparameterName", "batch size"], [260, 261, "MethodName", "UNITER"], [282, 284, "HyperparameterName", "learning rate"], [314, 315, "DatasetName", "COCO"], [330, 332, "HyperparameterName", "learning rate"], [333, 335, "HyperparameterName", "batch size"], [351, 353, "HyperparameterName", "batch size"], [375, 377, "HyperparameterName", "learning rate"], [406, 408, "HyperparameterName", "learning rate"], [413, 414, "DatasetName", "COCO"], [428, 429, "DatasetName", "Flickr30k"], [434, 435, "DatasetName", "COCO"]]}
{"text": "Although the room size is awful , the hotel does have some nice touches . Another benefit of the hotel is that ... Table 1 : Review ordering from a functional approach . Just with the first words of the sentences some characteristic features can be appreciated ( Verb tenses , person - thirdfirst - , ... ) Therefore , our hypothesis is that it is possible to characterise subparts of a discourse ( related to a genre ) according to their functionality and , at the same time , learn about its ( flexible ) ordering . Due to the lack of annotated corpora with discourse information about that communicative purposes , we propose to work with unsupervised techniques to achieve that goal . We expect to obtain the necessary knowledge to produce appropriate document plans . Taking into account several genres that normally exhibit a pre - defined or known - in - advance structure , such as the case of news , Wikipedia pages , or scientific article , we would be able to validate our suggested approach in other textual genres that lacks such well - defined structure a priori . Our methodology relies on pattern detection techniques . Until now , we have tried clustering that does not require previous knowledge of the number of clusters . Over an annotated corpus we apply an Expectation - Maximisation ( EM ) algorithm , having included within the features linguistic information related to its placement . The remainder of this paper is organised as follows . Section 2 summarises the related work concerning text classification efforts and genre studies related to communication objectives . Section 3 describes the kind of linguistic lexical features that we have been using in our experiments until now . After that , section 4 describes some resources coming from the Semantic Web environment that could complement and enrich those features . Finally , section 5 describes the experiments already performed and outlines future research opportunities .", "entities": [[233, 234, "MetricName", "EM"], [266, 268, "TaskName", "text classification"]]}
{"text": "Probing LMs for Relational Knowledge Since the introduction of transformer - based LMs , a large number of works have focused on analysing the capabilities of such models , covering the extent to which they capture syntax ( Goldberg , 2019 ; Saphra and Lopez , 2019 ; Hewitt and Manning , 2019 ; van Schijndel et al , 2019 ; Jawahar et al , 2019 ; Tenney et al , 2019 ) , lexical semantics ( Ethayarajh , 2019 ; Bommasani et al , 2020 ; Vulic et al , 2020 ) , and various forms of factual and commonsense knowledge ( Petroni et al , 2019 ; Forbes et al , 2019 ; Davison et al , 2019 ; Zhou et al , 2020 ; Talmor et al , 2020 ; Roberts et al , 2020 ) , among others . The idea of extracting relational knowledge from LMs , in particular , has also been studied . For instance , Petroni et al ( 2019 ) use BERT for link prediction . To this end , they use a manually defined prompt for each relation type , in which the tail entity is replaced by a < mask > token . To complete a knowledge graph triple such as ( Dante , born - in , ? ) they create the input \" Dante was born in < mask > \" and then look at the predictions of BERT for the masked token to retrieve the correct answer . It is notable that BERT is thus used for extracting relational knowledge without any fine - tuning . This clearly shows that a substantial amount of factual knowledge is encoded in the parameters of pre - trained LMs . Some works have also looked at how such knowledge is stored . Geva et al ( 2020 ) argue that the feed - forward layers of transformer - based LMs act as neural memories , which would suggest that e.g. \" the place where Dante is born \" is stored as a property of Florence . Dai et al ( 2021 ) present further evidence of this view . What is less clear , then , is whether relations themselves have an explicit representation , or whether transformer models essentially store a propositionalised knowledge graph . The results we present in this paper suggest that common lexical relations ( e.g. hypernymy , meronymy , has - attribute ) , at least , must have some kind of explicit representation , although it remains unclear how they are encoded . Another notable work focusing on link prediction is ( Bosselut et al , 2019 ) , where GPT is fine - tuned to complete triples from commonsense knowledge graphs , in particular ConceptNet ( Speer et al , 2017 ) and ATOMIC . While their model was able to generate new knowledge graph triples , it is unclear to what extent this is achieved by extracting commonsense knowledge that was already captured by the pre - trained GPT model , or whether this rather comes from the ability to generalise from the training triples . For the ConceptNet dataset , for instance , Jastrz\u0119bski et al ( 2018 ) found that most test triples are in fact minor variations of training triples . In this paper , we also rely on fine - tuning , which makes it harder to determine to what extent the pre - trained LM already captures relational knowledge . We address this concern by including relation types in our evaluation which are different from the ones that have been used for fine - tuning . Unsupervised Relation Discovery Modelling how different words are related is a long - standing challenge in NLP . An early approach is DIRT ( Lin and Pantel , 2001 ) , which encodes the relation between two nouns as the dependency path connecting them . Their view is that two such dependency paths are similar if the sets of word pairs with which they co - occur are similar . Hasegawa et al ( 2004 ) cluster named entity pairs based on the bag - of - words representations of the contexts in which they appear . Along the same lines , Yao et al ( 2011 ) proposed a generative probabilistic model , inspired by LDA ( Blei et al , 2003 ) , in which relations are viewed as latent variables ( similar to topics in LDA ) . Turney ( 2005 ) proposed a method called Latent Relational Analysis ( LRA ) , which uses matrix factorization to learn relation embeddings based on co - occurrences of word pairs and dependency paths . Matrix factorization is also used in the Universal Schema approach from Riedel et al ( Riedel et al , 2013 ) , which jointly models the contexts in which words appear in a corpus with a given set of relational facts . The aforementioned works essentially represent the relation between two words by summarising the contexts in which these words co - occur . In recent years , a number of strategies based on distributional models have been explored that rely on similar intuitions but go beyond simple vector operations of word embeddings . 2 For instance , Jameel et al ( 2018 ) introduced a variant of the GloVe word embedding model , in which relation vectors are jointly learned with word vectors . In SeVeN ( Espinosa - Anke and Schockaert , 2018 ) and RELATIVE ( Camacho - Collados et al , 2019 ) , relation vectors are computed by averaging the embeddings of context words , while pair2vec uses an LSTM to summarise the contexts in which two given words occur , and Washio and Kato ( 2018 ) learn embeddings of dependency paths to encode word pairs . Another line of work is based on the idea that relation embeddings should facilitate link prediction , i.e. given the first word and a relation vector , we should be able to predict the second word ( Marcheggiani and Titov , 2016 ; Simon et al , 2019 ) . This idea also lies at the basis of the approach from Soares et al ( 2019 ) , who train a relation encoder by fine - tuning BERT ( Devlin et al , 2019 ) with a link prediction loss . However , it should be noted that they focus on learning relation vectors from individual sentences , as a pre - training task for applications such as few - shot relation extraction . In contrast , our focus in this paper is on characterising the overall relationship between two words .", "entities": [[170, 171, "MethodName", "BERT"], [172, 174, "TaskName", "link prediction"], [241, 242, "MethodName", "BERT"], [256, 257, "MethodName", "BERT"], [345, 346, "MethodName", "Florence"], [435, 437, "TaskName", "link prediction"], [447, 448, "MethodName", "GPT"], [457, 459, "TaskName", "knowledge graphs"], [462, 463, "DatasetName", "ConceptNet"], [471, 472, "DatasetName", "ATOMIC"], [507, 508, "MethodName", "GPT"], [527, 528, "DatasetName", "ConceptNet"], [726, 727, "MethodName", "LDA"], [748, 749, "MethodName", "LDA"], [763, 764, "DatasetName", "LRA"], [877, 879, "TaskName", "word embeddings"], [895, 896, "MethodName", "GloVe"], [950, 951, "MethodName", "LSTM"], [993, 995, "TaskName", "link prediction"], [1056, 1057, "MethodName", "BERT"], [1066, 1068, "TaskName", "link prediction"], [1068, 1069, "MetricName", "loss"], [1100, 1102, "TaskName", "relation extraction"]]}
{"text": "Manual Prompts A basic prompt generation strategy is to rely on manually created templates , 2 Interestingly , Roller and Erk ( 2016 ) showed that the direct concatenation of distributional word vectors in isolation can effectively identify Hearst Patterns ( Hearst , 1992 ) . which has proven effective in factual knowledge probing ( Petroni et al , 2019 ) and text classification ( Schick and Sch\u00fctze , 2021 ; Tam et al , 2021 ; Le Scao and Rush , 2021 ) , among many others . To test whether manually generated templates can be effective for learning relation embeddings , we will consider the following five templates : ( Bouraoui et al , 2020 ; Jiang et al , 2020 ) . Learned Prompts The choice of prompt can have a significant impact on an LM 's performance . Since it is difficult to generate manual prompts in a systematic way , several strategies for automated generation of task - specific prompts have been proposed , e.g. based on mining patterns from a corpus ( Bouraoui et al , 2020 ) , paraphrasing ( Jiang et al , 2020 ) , training an additional LM for template generation ( Haviv et al , 2021 ; Gao et al , 2020 ) , and prompt optimization ( Shin et al , 2020 ; Liu et al , 2021 ) . In our work , we focus on the latter strategy , given its conceptual simplicity and its strong reported performance on various benchmarks . Specifically , we consider AutoPrompt ( Shin et al , 2020 ) and P - tuning ( Liu et al , 2021 ) . Note that both methods rely on training data . We will use the same training data and loss function that we use for fine - tuning the LM ; see Section 3.2 . AutoPrompt initializes the prompt as a fixedlength template : T = ( z 1 , . . . , z \u03c0 , [ h ] , z \u03c0+1 , . . . , z \u03c0+\u03c4 , [ t ] , z \u03c0+\u03c4 +1 , . . . , z \u03c0+\u03c4 + \u03b3 ) ( 1 ) where \u03c0 , \u03c4 , \u03b3 are hyper - parameters which determine the length of the template . The tokens of the form z i are called trigger tokens . These tokens are initialized as < mask > . The method then iteratively finds the best token to replace each mask , based on the gradient of the task - specific loss function . 3 P - tuning employs the same template initialization as AutoPrompt but its trigger tokens are newly introduced special tokens with trainable embeddings\u00ea 1 : \u03c0+\u03c4 + \u03b3 , which are learned using a taskspecific loss function while the LM 's weights are frozen .", "entities": [[62, 64, "TaskName", "text classification"], [297, 298, "MetricName", "loss"], [364, 365, "HyperparameterName", "\u03b3"], [374, 375, "HyperparameterName", "\u03b3"], [430, 431, "MetricName", "loss"], [460, 461, "HyperparameterName", "\u03b3"], [468, 469, "MetricName", "loss"]]}
{"text": "To fine - tune the LM , we need training data and a loss function . As training data , we assume that , for a number of different relation types r , we have access to examples of word pairs ( h , t ) that are instances of that relation type . The loss function is based on the following intuition : the embeddings of word pairs that belong to the same relation type should be closer together than the embeddings of pairs that belong to different relations . In particular , we use the triplet loss from Schroff et al ( 2015 ) and the classification loss from Reimers and Gurevych ( 2019 ) , both of which are based on this intuition . Triplet Loss We draw a triplet from the relation dataset by selecting an anchor pair a = ( h a , t a ) , a positive example p = ( h p , t p ) and a negative example n = ( h n , t n ) , i.e. we select word pairs a , p , n such that a and p belong to the same relation type while n belongs to a different relation type . Let us write x a , x p , x n for the corresponding relation embeddings . Each relation embedding is produced by the same LM , which is trained to make the distance between x a and x p smaller than the distance between x a and x n . Formally , this is accomplished using the following triplet loss function : L t = max 0 , x a \u2212 x p \u2212 x a \u2212 x n + \u03b5 3 We note that in most implementations of AutoPrompt the vocabulary to sample trigger tokens is restricted to that of the training data . However , given the nature of our training data ( i.e. , pairs of words and not sentences ) , we consider the full pre - trained LM 's vocabulary . where \u03b5 > 0 is the margin and is the l 2 norm . Classification Loss Following SBERT ( Reimers and Gurevych , 2019 ) , we use a classifier to predict whether two word pairs belong to the same relation . The classifier is jointly trained with the LM using the negative log likelihood loss function : L c = \u2212 log ( g ( x a , x p ) ) \u2212 log ( 1 \u2212 g ( x a , x n ) ) where g ( u , v ) = sigmoid ( W [ u v | v \u2212 u | ] T ) with W R 3\u00d7d , u , v R d , | | the element - wise absolute difference , and concatenation .", "entities": [[13, 14, "MetricName", "loss"], [55, 56, "MetricName", "loss"], [97, 99, "MethodName", "triplet loss"], [109, 110, "MetricName", "loss"], [127, 129, "MethodName", "Triplet Loss"], [267, 269, "MethodName", "triplet loss"], [275, 276, "DatasetName", "0"], [289, 290, "HyperparameterName", "\u03b5"], [346, 347, "HyperparameterName", "\u03b5"], [348, 349, "DatasetName", "0"], [359, 360, "TaskName", "Classification"], [400, 401, "MetricName", "loss"]]}
{"text": "In this section , we explain our experimental setting to train and evaluate RelBERT . triples are from different relations ) . Figure 2 illustrates this idea . Note how the effective batch size thus increases quadratically , while the number of vectors that needs to be encoded by the LM remains unchanged . In our setting , this leads to an additional 13500 triples per relation . Similar in - batch negative sampling has been shown to be effective in information retrieval ( Karpukhin et al , 2020 ; Gillick et al , 2019 ) . Third , we also construct training triples by considering the 10 high - level categories as relation types . In this case , we choose two positive examples from different relations that belong to the same category , along with a positive example from a relation from a different category . We add 5040 triples of this kind for each of the 10 categories . Training RelBERT training consists of two phases : prompt optimization ( unless a manually defined prompt is used ) and language model finetuning . First we optimize the prompt over the training set with the triplet loss L t while the parameters of the LM are frozen . Subsequently , we fine - tune the LM with the resulting prompt , using the sum of the triplet loss L t and the classification loss L c over the same training set . We do not use the classification loss during the prompt optimisation , as that would involve training the classifier while optimizing the prompt . We select the best hyper - parameters of the prompting methods based on the final loss over the validation set . In particular , when manual prompts are used , we choose the best template among the five candidates described in Section 3.1 . For AutoPrompt and Ptuning , we consider all combinations of \u03c0 { 8 , 9 } , \u03c4 { 1 , 2 } , \u03b3 { 1 , 2 } . We use RoBERTa ( Liu et al , 2019 ) as our main LM , where the initial weights were taken from the roberta - large model checkpoint shared by the Huggingface transformers model hub ( Wolf et al , 2020 ) . We use the Adam optimizer ( Kingma and Ba , 2014 ) with learn - ing rate 0.00002 , batch size 64 and we fine - tune the model for 1 epoch . For AutoPrompt , the top - 50 tokens are considered and the number of iterations is set to 50 . In each iteration , one of the input tokens is re - sampled and the loss is re - computed across the entire training set . 4 For P - tuning , we train the weights that define the trigger embeddings ( i.e. the weights of the input vectors and the parameters of the LSTM ) for 2 epochs . Note that we do not tune RelBERT on any task - specific training or validation set . We thus use the same relation embeddings across all the considered evaluation tasks .", "entities": [[32, 34, "HyperparameterName", "batch size"], [81, 83, "TaskName", "information retrieval"], [197, 199, "MethodName", "triplet loss"], [228, 230, "MethodName", "triplet loss"], [235, 236, "MetricName", "loss"], [250, 251, "MetricName", "loss"], [283, 284, "MetricName", "loss"], [336, 337, "HyperparameterName", "\u03b3"], [345, 346, "MethodName", "RoBERTa"], [389, 390, "MethodName", "Adam"], [390, 391, "HyperparameterName", "optimizer"], [405, 407, "HyperparameterName", "batch size"], [431, 434, "HyperparameterName", "number of iterations"], [454, 455, "MetricName", "loss"], [493, 494, "MethodName", "LSTM"]]}
{"text": "We evaluate RelBERT on two relation - centric tasks : solving analogy questions ( unsupervised ) and lexical relation classification ( supervised ) . Analogy Questions We consider the task of solving word analogy questions . Given a query word pair , the model is required to select the relationally most similar word pair from a list of candidates . To solve this task , we simply choose the candidate whose RelBERT embedding has the highest cosine similarity with the RelBERT embedding of the query pair . Note that this task is completely unsupervised , without the need for any training or tuning . We use the five analogy datasets that were considered by Ushio et al ( 2021 ) : the SAT analogies dataset ( Turney et al , 2003 ) , the U2 and U4 analogy datasets , which were collected from an educational website 5 , and datasets that were derived 6 from BATS ( Gladkova et al , 2016 ) and the Google analogy dataset ( Mikolov et al , 2013b ) . These five datasets consist of tuning and testing fragments . In particular , they contain 37/337 ( SAT ) , 24/228 ( U2 ) , 48/432 ( U4 ) , 50/500 ( Google ) , and 199/1799 ( BATS ) questions for validation / testing . As there is no need to tune RelBERT on task - specific data , we only use the test fragments . For SAT , we will also report results on the full dataset ( i.e. the testing fragment and tuning fragment combined ) , as this allows us to compare the performance with published results . We will refer to this full version of the SAT dataset as SAT \u2020. Lexical Relation Classification We consider the task of predicting which relation a given word pair belongs to . To solve this task , we train a multi - layer perceptron ( MLP ) which takes the ( frozen ) RelBERT embedding of the word pair as input . We consider the following widelyused multi - class relation classification benchmarks : K&H+N ( Nec\u015fulescu et al , 2015 ) , BLESS ( Baroni and Lenci , 2011 ) , ROOT09 ( Santus et al , 2016b ) , EVALution ( Santus et al , 2015 ) , and CogALex - V Subtask 2 ( Santus et al , 2016a ) . Table 1 shows the size of the training , validation and test sets for each of the relation classification dataset . The hyperparameters of the MLP classifier are tuned on the validation set of each dataset . Concretely , we tune the learning rate from [ 0.001 , 0.0001 , 0.00001 ] and the hidden layer size from [ 100 , 150 , 200 ] . CogALex - V only has testing fragments so for this dataset we employ the default configuration of Scikit - Learn ( Pedregosa et al , 2011 ) , which uses a 100 - dimensional hidden layer and is optimized using Adam with a learning rate of 0.001 . These datasets focus on the following lexical relations : co - hyponymy ( cohyp ) , hypernymy ( hyp ) , meronymy ( mero ) , possession ( poss ) , synonymy ( syn ) , antonymy ( ant ) , attribute ( attr ) , event , and random ( rand ) .", "entities": [[18, 20, "TaskName", "relation classification"], [166, 167, "DatasetName", "Google"], [209, 210, "DatasetName", "Google"], [294, 296, "TaskName", "Relation Classification"], [324, 325, "DatasetName", "MLP"], [349, 351, "TaskName", "relation classification"], [380, 381, "DatasetName", "EVALution"], [420, 422, "TaskName", "relation classification"], [428, 429, "DatasetName", "MLP"], [445, 447, "HyperparameterName", "learning rate"], [457, 460, "HyperparameterName", "hidden layer size"], [509, 510, "MethodName", "Adam"], [512, 514, "HyperparameterName", "learning rate"]]}
{"text": "As baselines , we consider two standard word embedding models : GloVe ( Pennington et al , 2014 ) and FastText ( Bojanowski et al , 2017 ) , where word pairs are represented by the vector difference of their word embeddings ( diff ) . 7 For the classification experiments , we also consider the concatena - 7 Vector difference is the most common method for encoding relations , and has been shown to be the most reliable in the context of word analogies ( Hakami and Bollegala , 2017 ) . tion of the two word embeddings ( cat ) and their element - wise multiplication 8 ( dot ) . We furthermore experiment with two pre - trained word pair embedding models : pair2vec ( pair ) and RELATIVE ( Camacho - Collados et al , 2019 ) ( rel ) . For these word pair embeddings , as well as for RelBERT , we concatenate the embeddings from both directions , i.e. ( h , t ) and ( t , h ) . For the analogy questions , two simple statistical baselines are included : the expected random performance and a strategy based on point - wise mutual information ( PMI ) Church and Hanks ( 1990 ) . In particular , the PMI score of a word pair is computed using the English Wikipedia , with a fixed window size of 10 . We then choose the candidate pair with the highest PMI as the prediction . Note that this PMI - based method completely ignores the query pair . We also compare with the published results from Ushio et al ( 2021 ) , where a strategy is proposed to solve analogy questions by using LMs to compute an analogical proportion score . In particular , a four - word tuple ( a , b , c , d ) is encoded using a custom prompt and perplexity based scoring strategies are used to determine whether the word pair ( a , b ) has the same relation as the word pair ( c , d ) . Finally , for the SAT \u2020 dataset , we compare with the published results from GPT - 3 ( Brown et al , 2020 ) , LRA ( Turney , 2005 ) and SuperSim ( Turney , 2013 ) ; for relation classification we report the published results of the LexNet ( Shwartz et al , 2016 ) and SphereRE relation classification models , taking the results from the latter publication . We did not reproduce these latter methods in similar conditions as our work , and hence they are not fully comparable . More - over , these approaches are a different nature , as the aim of our work is to provide universal relation embeddings instead of task - specific models .", "entities": [[11, 12, "MethodName", "GloVe"], [20, 21, "MethodName", "FastText"], [40, 42, "TaskName", "word embeddings"], [97, 99, "TaskName", "word embeddings"], [324, 325, "MetricName", "perplexity"], [370, 371, "MethodName", "GPT"], [381, 382, "DatasetName", "LRA"], [396, 398, "TaskName", "relation classification"], [415, 417, "TaskName", "relation classification"]]}
{"text": "Table 2 shows the accuracy on the analogy benchmarks . The RelBERT models substantially outperform the baselines on all datasets , except for the Google analogy dataset . 9 Comparing the different prompt generation approaches , we can see that , surprisingly , the manual prompt consistently outperforms the automatically - learned prompt strategies . It can furthermore be noted that the other two relation embedding methods ( i.e. pair2vec and REL - ATIVE ) perform poorly in this unsupervised task . The analogical proportion score from Ushio et al ( 2021 ) also underperforms RelBERT , even when tuned on dataset - specific tuning data .", "entities": [[4, 5, "MetricName", "accuracy"], [24, 25, "DatasetName", "Google"]]}
{"text": "Table 3 summarizes the results of the lexical relation classification experiments , in terms of macro and micro averaged F1 score . The RelBERT models achieve the best results on all datasets except for BLESS and K&H+N , where the performance of all models is rather close . We observe a particularly large improvement over the word embedding and SotA models on the EVALution dataset . When comparing the different prompting strategies , we again find that the manual prompts perform surprisingly well , although the best results are now obtained with learned prompts in a few cases . 9 The Google analogy dataset has been shown to be biased toward word similarity and therefore to be well suited to word embeddings ( Linzen , 2016 ; Rogers et al , 2017", "entities": [[8, 10, "TaskName", "relation classification"], [19, 21, "MetricName", "F1 score"], [63, 64, "DatasetName", "EVALution"], [101, 102, "DatasetName", "Google"], [111, 113, "TaskName", "word similarity"], [120, 122, "TaskName", "word embeddings"]]}
{"text": "We show comparisons of versions of RelBERT with optimized prompt with / without finetuning . Figure 4 shows the absolute accuracy drop from RelBERT ( i.e. the model with fine - tuning ) to the vanilla RoBERTa model ( i.e. without fine - tuning ) with the same prompt . In all cases , the accuracy drop for the models without fine - tuning is substantial .", "entities": [[20, 21, "MetricName", "accuracy"], [36, 37, "MethodName", "RoBERTa"], [55, 56, "MetricName", "accuracy"]]}
{"text": "We use RoBERTa in our main experiments and here we train RelBERT with ALBERT and BERT instead , and evaluate them on both of the analogy and relation classification tasks . Table 7 shows the accuracy on the analogy questions , while Table 8 shows the accuracy on the relation classification task . In both tasks , we can confirm that RoBERTa achieves the best performance within the LMs , by a relatively large margin in most cases .", "entities": [[2, 3, "MethodName", "RoBERTa"], [13, 14, "MethodName", "ALBERT"], [15, 16, "MethodName", "BERT"], [27, 29, "TaskName", "relation classification"], [35, 36, "MetricName", "accuracy"], [46, 47, "MetricName", "accuracy"], [49, 51, "TaskName", "relation classification"], [61, 62, "MethodName", "RoBERTa"]]}
{"text": "Table 10 shows the best prompt configuration based on the validation loss for the SemEval 2012 Task 2 dataset in our main experiments using RoBERTa .", "entities": [[11, 12, "MetricName", "loss"], [24, 25, "MethodName", "RoBERTa"]]}
{"text": "All the trigger tokens are initialized by mask tokens and updated based on the gradient of a loss function L t . Concretely , let us denote the loss value with template T as L t ( T ) . The candidate set for the j th trigger is derived b\u1ef9 W j = top - k w W e T w \u2207 j L t ( T ) ( 2 ) where the gradient is taken with respect to j th trigger token and e w is the input embedding for the word w. Then we evaluate each token based on the loss function as z j = argmin w W j L t ( rep ( T , j , w ) ) ( 3 ) where rep ( T , j , w ) replaces the j th token in T by w and j is randomly chosen . We ignore any candidates that do not improve current loss value to further enhance the prompt quality .", "entities": [[17, 18, "MetricName", "loss"], [28, 29, "MetricName", "loss"], [103, 104, "MetricName", "loss"], [161, 162, "MetricName", "loss"]]}
{"text": "Text classification aims at mapping documents into a set of predefined categories . Supervised machine learning models have shown great success in this area but they require a large number of labeled documents to reach adequate accuracy . This is particularly true when the number of target categories is in the tens or the hundreds . In this work , we explore an unsupervised approach to classify documents into categories simply described by a label . The proposed method is inspired by the way a human proceeds in this situation : It draws on textual similarity between the most relevant words in each document and a dictionary of keywords for each category reflecting its semantics and lexical field . The novelty of our method hinges on the enrichment of the category labels through a combination of human expertise and language models , both generic and domain specific . Our experiments on 5 standard corpora show that the proposed method increases F1 - score over relying solely on human expertise and can also be on par with simple supervised approaches . It thus provides a practical alternative to situations where lowcost text categorization is needed , as we illustrate with our application to operational risk incidents classification .", "entities": [[0, 2, "TaskName", "Text classification"], [36, 37, "MetricName", "accuracy"], [160, 163, "MetricName", "F1 - score"], [190, 192, "TaskName", "text categorization"]]}
{"text": "In this review of relevant work , we focus predominantly on techniques that have been proposed to overcome the requirement of having a large number of annotated data for standard text classification techniques . Overall , the majority of approaches focus on generating labeled examples without full manual annotation . Those include semi - supervised techniques that seek to leverage a small set of labeled documents to derive labels for the remainder of the corpus . For instance , Nigam et al ( 2000 ) propose to follow the Expectation - Maximization ( EM ) algorithm by iteratively using the set of labeled data to obtain probabilistically - weighted class labels for each unlabeled document and then training a classifier on the complete corpus based on those annotations . This process is repeated until convergence of the log likelihood of the parameters given observed data . Other approaches attempt to automatically derive labels without any starting set of annotations . For instance , Turney ( 2002 ) classifies a review as recommended or not recommended by computing the pointwise mutual information of the words in the review with a positive reference word ( excellent ) and with a negative reference word ( poor ) using search engine results as a proxy for a reference corpus . Another example is Ko and Seo ( 2000 ) who leverage an initial set of manually provided keywords for each target category to derive labels . Based on those key - words , they look for representative sentences in the corpus to support label assignment . Finally , Yang et al ( 2013 ) make use of wikipedia as background knowledge to assemble representative set of words for each category label via topic modeling and use them to annotate the unlabeled documents . In a similar way , Miller et al ( 2016 ) represent each target category as a TF - IDF ( termfrequency / inverse document frequency ) vector obtained from Wikipedia and then use this category representation as an informed prior to Latent Dirichlet Allocation ( LDA ) , an unsupervised algorithm that finds the topics that best satisfy the data given the priors . The occurrence of these topics in a document can be used as a noisy label for that document . Our approach differs in spirit in the sense that our objective is not to construct surrogate labels so that we can apply a machine learning classifier to our unlabeled data . By contrast , we opted for a fully unsupervised method which hinges on computing a similarity metric between documents and target categories . To that end , a richer representation of category labels is derived . The method that were proposed by Yang et al ( 2013 ) ; Miller et al ( 2016 ) could be adapted to align with our perspective ( by removing the classification step ) . Other examples of unsupervised approach include Rao et al ( 2006 ) which defined the label of documents based on a k - means word clustering . They select a set of representative words from each cluster as a label and derive a set of candidate labels . An input document vector is then assigned to the label vector that maximizes the norm of the dotproduct . While this approach performs well when there are no categories specified as input , e.g. , social listening , trend monitoring , topic modeling , it is less likely to do so with a set of predefined target categories where it is difficult to steer word clusters to categories of interest and , critically , to ensure the full coverage of target categories ( new internal taxonomy of risk in our practical case ) . Finally , our method makes use of word embeddings as a mean to enrich category label via semantic expansion . As far as we know , word embeddings have been used to improve text classification performance through their application as a document representation technique . In Liu et al ( 2018 ) , the authors show that task oriented embeddings , which penalise outputs where the representative words of a category are close to the representative words of another category , outperform general domain embeddings . As we do not have any labeled data , this approach is not directly relevant to our problem setting .", "entities": [[30, 32, "TaskName", "text classification"], [93, 94, "MetricName", "EM"], [345, 346, "MethodName", "LDA"], [634, 636, "TaskName", "word embeddings"], [653, 655, "TaskName", "word embeddings"], [660, 662, "TaskName", "text classification"]]}
{"text": "In our method , an offline process is used to extract initial keywords from category labels . For the purpose of testing our approach , we had to emulate human experts ourselves . For each category , one team member added a few keywords based only on label description . Then , we randomly selected 2 or 3 documents for each label that were read by two team members who used them to identify 5 to 10 salient words to be added to each dictionary . In average , we manually added 9 words per label for 20NewsGroup , 17 words for AGs Corpus and Google - Snippets , 11 words for Yahoo - Answers and 14 words for 5AbstractsGroup . We present in Table 2 , the output of that process for the AGs Corpus dataset . Once we identify initial keywords , we make the series of enrichment steps described in section 3.2 . For every word in the set of initial keywords , we add all its synonym sets from WordNet as well as the 10 most similar words from Glove , CBOW and Skip - Gram . The average length of label dictionaries obtained from the full enrichment pipeline ( which we refer to as all keywords ) is 428 words . We use the word2vec python implementation provided by gensim ( Rehurek and Sojka , 2010 ) . For Skip - gram and CBOW , a 10 - word window size is used to provide the same amount of raw information . Also words appearing 3 times or fewer are filtered out , 10 workers were used and train - ing was performed in 100 epochs . We chose 300 for the size of all word embeddings , it has been reported to perform well in classification tasks ( Mikolov et al , 2013a ) . Filtering word dictionaries with the Functionaware Component ( FAC ) allowed to keep in average 37 % of all keywords per label . As described previously , once different versions of label dictionaries have been obtained , we calculate their similarity with input documents using LSA and Cosine distance . The optimal dimension ( k ) of the latent space depends on the dataset . Optimal k values are typically in the range of 100 - 300 dimensions ( Harman , 1993 ; Letsche and Berry , 1997 ) . In this work , for each dataset , we set a range of 100 - 300 values , and we determine the optimal k by maximizing the topic coherence score ( R\u00f6der et al , 2015 ) . The multi - class classification performance was evaluated in terms of precision ( Prec . ) , recall ( Rec . ) and F1 - score ( F1 ) . All measures are computed based on a weighted average of each class using the number of true instances to determine the weights .", "entities": [[105, 106, "DatasetName", "Google"], [290, 292, "TaskName", "word embeddings"], [440, 444, "TaskName", "multi - class classification"], [462, 465, "MetricName", "F1 - score"], [466, 467, "MetricName", "F1"]]}
{"text": "Table 3 summarizes the performance of each of the methods tested on the five corpora that we considered . Overall , the various configurations of our method , all leveraging embeddings for semantic expansion , outperform the simple unsupervised baselines , leading to a doubling of the F1 - score for all corpora , the least affected being the 5Abstracts - Group where F1 goes from 38.1 to 68.3 percent , comparing with the all keywords variant of our method . When focusing on our various configurations , first without the FAC consolidation , we observe that domain specific embeddings alone lead to better performance than generic embeddings alone and this across all corpora and all metrics , except for the Yahoo - Answers dataset . The difference in performance however is not very large , with the exception of 20NewsGroup where F1 - score increases from 52.6 with generic embeddings to 61 with domain specific ones . We notice also that combining all enrichments ( All keywords ) provides a modest increase in performance over embeddings only as shown by the results for Yahoo - Answers , 5AbstractsGroup and Google - Snippets . Finally the use of the consolidation step further improves performance except for 20NewsGroup where precision increases from 64.7 to 71.1 but recall decreases from 57.8 to 35.6 . Comparing now our best unsupervised performance with the supervised baseline , we observe that the ratio of the best F1 - score performance over the supervised baseline performance varies from 0.71 to 1.11 with two datasets yielding ratios above 1 . Such results demonstrate the validity of the unsupervised approach as a practical alternative to investing to a cognitively and timely costly annotation effort .", "entities": [[47, 50, "MetricName", "F1 - score"], [63, 64, "MetricName", "F1"], [142, 145, "MetricName", "F1 - score"], [190, 191, "DatasetName", "Google"], [241, 244, "MetricName", "F1 - score"]]}
{"text": "In our application , we were asked to map both internal incidents and external incidents to the new taxonomy . In this paper , we focus on the external incidents for confidentiality reasons . More precisely , our task was to assign a unique category to each one of the 25 , 000 incidents that was obtained from ORX news . The Operational Risk Exchange ( ORX ) is a consortium of financial institutions focused on operational risk information sharing . The ORX news service provides publicly reported operational risk loss data to its institutional members . An incident record is mostly composed of an incident description along with associated metainformation such as geographical indicators , time information and institution affected . We only make use of the incident descriptions . Their average length is 2150 words , with a standard deviation of 1181 words and ranging from 10 words to more than 14000 words . The target taxonomy is composed of three levels . The first one contains 16 labels and indicates at a very high level the domain of the incidents such as IT , legal , regulatory . The second and third levels contain respectively 69 and 264 levels to add increasing granularity to the incident classification . Figure 2 presents an extract of the taxonomy focused on ICT risk , which is public as it draws upon Article 107 ( 3 ) of Directive 2013/36 / EU2 which aim to ensure the convergence of supervisory practices in the assessment of the information and communication technology ( ICT ) risk . Before discussing the results , we thought it would be meaningful to point out some of the characteristics of this application . One natural challenge in real world cases is the lack of unequivocal ground truth . Experts can often identify categories that do not correspond to the input but in the end , they can not ascertain whether one category should prevail over another unless there is some clear guidelines or convention at the level of the organization . That difficulty is further compounded in our case as most documents are very dense in term of information and become ambiguous . For instance , \" In Japan , a building destruction resulting from a massive earthquake has caused power outage making AMD - based servers unbootable \" , could be classified as Natural Disaster , Dysfunctional ICT data processing or handling or Destruction / loss of physical assets among others .", "entities": [[90, 91, "MetricName", "loss"], [398, 399, "DatasetName", "Disaster"], [409, 410, "MetricName", "loss"]]}
{"text": "Idioms and collocations are special types of phrases in many languages . An idiom is a phrase whose meaning can not be obtained compositionally , i.e. , by combining the meanings of the words that compose it . Collocations are phrases in which there is a semantic association between the component words and some restrictions on which words can be replaced and which can not . In short , collocations are arbitrarily restricted lexeme combinations such as look into and fully aware . Many scientists from diverse fields have worked on the challenging tasks of automated collocation and idiom extraction , e.g. , see ( Garg and Goyal , 2014 ; Seretan , 2013 ; Verma and Vuppuluri , 2015 ; Verma et al , 2016 ) and the references contained therein , yet there is no multi - purpose , ready - to - use , and flexible system for extracting these phrases . Collocation and its special forms , such as idioms , can be useful in many important tasks , e.g. , summarization ( Barrera and Verma , 2012 ) , question - answering ( Barrera et al , 2011 ) , language translation , topic segmentation , authorial style , and so on . As a result , a tool for these tasks would be very handy . To tackle this void , we introduce a feature - rich system called ICE ( short for Idiom and Collocation Extractor ) , which has two versions : one is flexible and pipelined seamlessly for research purposes as a component of a larger system such as a question answering system , and the second as a web - based tool for educational purposes . ICE has a modular architecture and also includes a POS tagger , which can be used alone or as part of collocation or idiom extraction . An experiment with the CoNLL dataset shows that ICE 's POS tagger is competitive against the Stanford POS tagger . For ease of use in research , we provide ICE as a python package . For collocation extraction , ICE uses the IR models and techniques introduced by ( Verma et al , 2016 ) . These methods include : dictionary search , online dictionaries , a substitution method that compares the Bing hit counts of a phrase against the Bing hit counts of new phrases obtained by substituting the component words of the phrase one at a time to determine the \" adherence factor \" of the component words in a collocation , and two methods that try to measure the probability of association of the component words again using hit counts . In ( Verma et al , 2016 ) , the authors created a gold - standard dataset of collocations by taking 100 sentences at random from the Wiki50 dataset and manually annotating them for collocations ( including idioms ) using eight volunteers , who used the Oxford Dictionary of Collocations and Oxford Dictionary of Idioms . Each sentence was given to two annotators , who were given 25 sentences each for annotation , and their work was checked and corrected afterwards by two other people . In creating this dataset , even with the assistance of dictionaries , human performance varied from an F1 - score of about 39 % to 70 % for the collocation task . A comparison showed that their combination schemes outperformed existing techniques , such as MWEToolkit ( Ramisch et al , 2010 ) and Text - NSP ( Banerjee and Pedersen , 2003 ) , with the best method achieving an F1score of around 40 % on the gold - standard dataset , which is within the range of human performance . For idiom extraction , ICE uses the semanticsbased methods introduced by ( Verma and Vuppuluri , 2015 ) . The salient feature of these methods is that they use Bing search for the definition of a given phrase and then check the compositionality of the phrase definition against combinations of the words obtained when a define word query is issued , where the word belongs to the phrase . If there is a difference in the meaning , that phrase is considered an idiom . In ( Verma and Vuppuluri , 2015 ) , authors showed that their method outperforms AMALGr ( Schneider et al , 2014 ) . Their best method achieved an F1 - score of about 95 % on the VNC tokens dataset . Thus , ICE includes extraction methods for idioms and collocations that are state - of - the - art . Other tools exist for collocation extraction , e.g. , see ( Anagnostou and Weir , 2006 ) , in which four methods including Text - NSP are compared .", "entities": [[175, 176, "TaskName", "summarization"], [269, 271, "TaskName", "question answering"], [549, 552, "MetricName", "F1 - score"], [738, 741, "MetricName", "F1 - score"]]}
{"text": "As ICE 's algorithms are based on Bing search , users must provide a valid user i d for the Bing API . ICE receives a list of sentences as an input and outputs a list of all collocations and idioms . It first splits the input sentences using NLTK sentence tokenizer , then generates n - grams and part of speech tags . ICE 's n - gram generator takes care of punctuation marks and has been shown to be better than NSP 's n - gram generator . Finally , the output n - grams are given to the collocation and idiom detection algorithms . Collocation and idiom extraction has been done by the algorithm given by ( Verma et al , 2016 ) 1 and ( Verma and Vuppuluri , 2015 ) . For part of speech tagging we combined NLTK 's regex tagger with NLTK 's N - Gram Tagger to have a better performance on POS tagging . We compared our tagger with Stanford POS tagger ( Manning et al , 2014 ) on the CoNLL dataset . 2 The accuracy of our tagger is 92.11 % , which is Collocation / Idiom Extractor . The collocation extraction technique combines different methods in a pipeline in order to increase precision . Figures 1 and 2 show the idiom and collocation extraction system architectures separately . As shown in the diagrams , there are two methods for identifying idioms ( called And and Or ) and four different methods for identifying collocations including : offline dictionary search , online dictionary search , web search and substitution , and web search and independence . For collocations , ICE pipelines the first and second methods , then pipelines them with the third or the fourth method ( both options are available in the code ) . These methods are connected sequentially . This means that if something is considered as a collocation in one component , it will be added to the list of collocations and will not be given to the next component ( yes / no arrows in the diagram ) . Table 1 shows the description of each component in collocation extractor diagram . The Ngram Extractor receives all sentences and generates n - grams ranging from bigrams up to 8grams . It uses NLTK sentence and word tokenizers for generating tokens . Then , it combines the generated tokens together taking care of punctuation to generate the n - grams . Dictionary Check uses WordNet ( Miller , 1995 ) as a dictionary and looks up each n - gram to see if it exists in WordNet or not ( a collocation should exist in the dictionary ) . All n - grams that are considered as non - collocations are given to the next component as input . The next component is Online Dictionary . It searches online dictionaries to see if the n - gram exists in any of them or not . It uses Bing Search API 3 to search for n - grams in the web . Web Search and Substitution is the next component in the pipeline . This method uses Bing Search API to obtain hit counts for a phrase query . Then each word in the n - gram will be replaced by 5 random words ( one at the time ) , and the hit counts are obtained . At the end , we will have a list of hit counts . These values will be used to differentiate between collocations and non - collocations . The last component in the pipeline of collocation extraction is Web Search and Independence . The idea of this method is to check whether the probability of a phrase exceeds the probability that we would expect if the words are independent . It uses hit counts in order to estimate the probabilities . These probabilities are used to differentiate between collocations and non - collocations . When running the collocation extraction function , one of the components should be selected out of the third and fourth ones . The Idiom Extractor diagram is relatively simpler . Given the input n - gram , it creates n + 1 sets . The first contains ( stemmed ) words in the meaning of the phrase . The next n sets contain stemmed word in the meaning of each word in the n - gram . Then it applies the set difference operator to n pairs containing the first set and each of the n sets . The Or subsystem considers a phrase as an idiom if at least one word survives one of the subtractions ( union of difference sets should be non - empty ) . For the And , at least one word has to exist that survived every subtraction ( intersection of difference sets should be non - empty ) Performance . ICE outperforms both Text - NSP and MWEToolkit . On the gold - standard dataset , ICE 's F1 - score was 40.40 % , MWE - Toolkit 's F1 - score was 18.31 % , and Text - NSP had 18 % . We also compared our idiom extraction with AMALGr method ( Schneider et al , 2014 ) on their dataset and the highest F1 - score achieved by ICE was 95 % compared to 67.42 % for AMALGr . For detailed comparison of ICE 's collocation and idiom extraction algorithm with existing tools , please refer to ( Verma et al , 2016 ) and ( Verma and Vuppuluri , 2015 ) . Sample Code . Below is the sample code for using ICE 's collocation extraction as part of a bigger system . For idiom extraction you can use IdiomExtractor class instead of collocationExtractor . > > input= [ \" he and Chazz duel with all keys on the line . \" ] > > from ICE import CollocationExtractor > > extractor = CollocationExtractor . with_collocation_pipeline ( \" T1 \" , bing_key = \" Temp \" , pos_check = False ) > > print ( extractor . get_collocations_of_length ( input , length = 3 ) ) > > [ \" on the line \" ] Educational Uses . ICE also has a web - based interface for demonstration and educational purposes . A user can type in a sentence into an input field and get a list of the idioms or collocations in the sentence . A screen - shot of the web - based interface is shown in Figure 3 . 4", "entities": [[185, 186, "MetricName", "accuracy"], [841, 844, "MetricName", "F1 - score"], [852, 855, "MetricName", "F1 - score"], [889, 892, "MetricName", "F1 - score"]]}
{"text": "We train our models on a single NVIDIA K80 GPU . We tune hyperparameter values for our model using the validation sets provided by our evaluation datasets ; we achieve the best validation performance using 8 attention blocks per Transformer , each with 5 attention heads , and a hidden size was set to 40 . The dropout rate was set to 0.15 ; the best learning rate for IEMOCAP was 0.02 , while for CMU - MOSI and CMU - MOSEI it was 0.01 , with batch sizes of 32 , 128 , and 40 , respectively .", "entities": [[39, 40, "MethodName", "Transformer"], [66, 68, "HyperparameterName", "learning rate"], [69, 70, "DatasetName", "IEMOCAP"], [77, 78, "DatasetName", "MOSI"], [79, 82, "DatasetName", "CMU - MOSEI"]]}
{"text": "IEMOCAP [ Busso et al , 2008 ] consists of video recordings of 151 conversation sessions ( dialogues ) , totaling around 6k verbal interactions . This dataset is intended for multilabel emotion classification ; we evaluate on the four labeled emotions ( Happy , Sad , Angry , and Neutral ) used in previous work [ Wang et al , 2019 ] ; also following previous work , we report binary accuracy and F1 score as the evaluation metrics on this dataset . CMU - MOSI [ Zadeh et al , 2016 ] is a sentiment analysis dataset of 2199 short monologues labeled in the range [ \u22123 , 3 ] , with \u22123 being strongly negative and +3 being strongly positive . Following previous work , we report seven - class and binary accuracy , F1 score , mean absolute error , and correlation with human judgments . CMU - MOSEI [ Zadeh et al , 2018b ] is a sentiment and emotion analysis dataset of 23 K movie reviews from YouTube . As with CMU - MOSI , it is labeled in the range of [ \u22123 , 3 ] , and its evaluation metrics are the same as in CMU - MOSI .", "entities": [[0, 1, "DatasetName", "IEMOCAP"], [32, 34, "TaskName", "emotion classification"], [72, 73, "MetricName", "accuracy"], [74, 76, "MetricName", "F1 score"], [86, 87, "DatasetName", "MOSI"], [96, 98, "TaskName", "sentiment analysis"], [135, 136, "MetricName", "accuracy"], [137, 139, "MetricName", "F1 score"], [150, 153, "DatasetName", "CMU - MOSEI"], [164, 165, "DatasetName", "emotion"], [179, 180, "DatasetName", "MOSI"], [205, 206, "DatasetName", "MOSI"]]}
{"text": "We present the results of our model compared to the reported results of our baseline models in Tables 1 , 2 , and 3 . The best - performing MuLT and FMT models are extremely dense , with around 15 and 77 million parameters , respectively . In contrast , our models have between 7 - 9 million trainable parameters , depending on the architecture ; despite using about half as many parameters as MuLT , we see that our models produce comparable results . We perform fairly well on IEMOCAP , which has around 2717 training samples ; we achieve scores around 1 - 2 % below the best - performing model , FMT . Late Fusion models give state of the art results on seven - way and binary accuracy , respectively . The CMU - MOSEI dataset is much larger than IEMOCAP and CMU - MOSI , with close to 16265 training samples . Our models perform the weakest on this dataset , falling short of the state of the art models by around 2 - 3 % , suggesting that our models may be too small to learn the entire distribution . Neither MARN [ Zadeh et al , 2018c ] nor FMT reports results on CMU - MOSEI , so they are omitted from Table 3 . We also experiment with the open source code available for MuLT and FMT ( denoted by * ) . Using the hyperparameter settings provided 2 , we were nevertheless unable to match those systems ' reported performance , possibly due to differences 2 Batch size for FMT * is not given ; we use 20 , the default . resulting from random initialization . In training MuLT * and FMT * , we observe that the models are overfitting , with a mean difference of 15 - 20 % between the train and test accuracy ; in contrast , the largest train - test accuracy difference among our three models is only about 10 % . The smaller number of parameters in our model reduces the risk of overfitting on smaller datasets , while still achieving good performance on larger datasets .", "entities": [[90, 91, "DatasetName", "IEMOCAP"], [131, 132, "MetricName", "accuracy"], [136, 139, "DatasetName", "CMU - MOSEI"], [144, 145, "DatasetName", "IEMOCAP"], [148, 149, "DatasetName", "MOSI"], [210, 213, "DatasetName", "CMU - MOSEI"], [265, 267, "HyperparameterName", "Batch size"], [316, 317, "MetricName", "accuracy"], [326, 327, "MetricName", "accuracy"], [340, 343, "HyperparameterName", "number of parameters"]]}
{"text": "We compare the training time and memory footprint of our models with MuLT * and FMT * in Table 4 on CMU - MOSEI ( the number of epochs needed for MuLT to converge , as reported by Tsai et al [ 2019 ] ) . On the smallest dataset , CMU - MOSI , training MuLT * took just over seven minutes , while FMT * took 2.5 hours . Our models train in under three minutes and outperform both MuLT * and FMT * , and this difference in training speed holds for CMU - MOSI and CMU - MOSEI as well . Thus our model , available in the supplementary materials 4 , is the fastest and best - performing multimodal sentiment system currently available for public use . We also conduct experiments on a substantially reduced IEMOCAP training subset of 1284 samples , matching the size of CMU - MOSI , which we create by randomly sampling from the full IEMO - CAP training set . Table 5 shows the results of our models , as well as MuLT * and FMT * , retrained on this smaller IEMOCAP training set , and evaluated on the full IEMOCAP test set . We see that our models , with their smaller numbers of parameters , are better able to learn from limited training data than are state - of - the - art models with double or more the number of trainable parameters .", "entities": [[21, 24, "DatasetName", "CMU - MOSEI"], [26, 29, "HyperparameterName", "number of epochs"], [53, 54, "DatasetName", "MOSI"], [97, 98, "DatasetName", "MOSI"], [99, 102, "DatasetName", "CMU - MOSEI"], [140, 141, "DatasetName", "IEMOCAP"], [153, 154, "DatasetName", "MOSI"], [166, 167, "DatasetName", "CAP"], [192, 193, "DatasetName", "IEMOCAP"], [201, 202, "DatasetName", "IEMOCAP"]]}
{"text": "The first component of our framework MIN is composed of A - LSTM and O - LSTM . Both LSTMs have extended memories for task - level memory interactions . A - LSTM involves a large aspect memory H A t R nm\u00d7dim A h and an opinion summary vector m O t R dim O h where H A t contains n m pieces of aspect hidden states of dimension dim A h and m O t is distilled from H O t . As for O - LSTM , similarly , an opinion memory H O t R nm\u00d7dim O h and an aspect - specific summary vector m A t R dim A h are included . We use the aspect term annotations in the training data for training A - LSTM . As there is no ground truth available for opinion words in the training data , sentiment lexicon and highprecision dependency rules are introduced to find potential opinion words . Commonly used opinion words can be found in some general sentiment lexicons . To find opinion words , not in sentiment lexicons , in a sentence , we build a small rule set R composed of dependency relations with high confidence , e.g. , amod , nsubj , and determine if w t directly depends on the gold aspect word through the dependencies in R. If so , w t will be treated as a potential opinion word . Then such opinion words are used as training data for O - LSTM . In the memory - enhanced A - LSTM and O - LSTM , we manually design three kinds of operations : ( 1 ) READ to select n m pieces of aspect ( opinion ) hidden states from the past memories and build H A t ( H O t ) ; ( 2 ) DIGEST to distill an aspect ( opinion ) - specific summary m A t ( m O t ) from H A t ( H O t ) where influences of opinion terms and relative positions of inputs are considered ; ( 3 ) INTERACT to perform interaction between A - LSTM and O - LSTM using the task specific summaries ( i.e. , m A t and m O t ) . Consider the work flow of A - LSTM for aspect term extraction . Since opinion words and aspect terms should co - occur , the goal of A - LSTM participating in memory interactions is to acquire opinion summaries from O - LSTM ( i.e. , m O t ) for better aspect prediction . First of all , MIN will READ n m pieces of opinion memories which are most related to w t from O - LSTM . Syntax structure could be used but syntactic parsers are not effective for processing short informal review sentences . Therefore , MIN selects memory segments temporally related to w t . Precisely , the opinion memory at the time step t is H O t = [ h O t\u22121 ; ... ; h O t\u2212nm ] where h O t\u2212i is the ( t \u2212 i ) - th hidden state from O - LSTM . Since the linear context contains most of the parent nodes and the child nodes of w t on the dependency parse tree , treating the corresponding memory segments as relevant segments to w t is reasonable . Then MIN will DIGEST the collected opinion memories H O t in the A - LSTM . As different memory segments are not of equal importance for the current decision and the same segment in different memories ( i.e. , different H O t ) also makes a difference , MIN leverages two kind of weights to summarize the collected content . The first weight is the indicator score of being opinion terms denoted as v I R nm , which is used to measure how much opinion information the word w t\u2212i ( i = 1 , .. , n m ) holds . We adopt Euclidean distance between distributed representations of w t\u2212i and opinion words . It is obvious that computing the distance between x t\u2212i and each opinion word is expensive . Thus , we run an off - the - shelf clustering algorithm over opinion words in the training set and then use the produced n c centroids to estimate the indicator score v I i of w t\u2212i being an opinion word : v I i = nc j=1 1 | | x t\u2212i \u2212 c j | | 2 ( 1 ) where x t\u2212i is the distributed representation of w t\u2212i and c j is the centroid vector representation of j - th cluster . This weighting scheme ensures that w t\u2212i is assigned a high score as long as x t\u2212i is close to a particular centroid . The aspect decision of w t is also affected by relative position between w t\u2212i and w t . Thus , MIN employs the second weight v P to explicitly model their positional relevance and the initial weight for the i - th segment v P i is calculated as below : v P i = n m \u2212 i + 1 nm k=1 k ( 2 ) where n m is the number of hidden state in H O t . This position - aware weight enables that the closer the word w t\u2212i is to the current input , the more the corresponding memory segment will contribute to the current decision . To better capture the local positional relevance , we make the initialized v P as learnable parameters . Combining the above two weights helps to utilize each active memory segment according to the importance for prediction and m O t , the summary of H O t is generated : m O t = ( H O t ) ( v I v P | | v I | | 2 ) ( 3 ) where denotes element - wise multiplication and | | * | | 2 is Euclidean norm of vectors . From Equation 3 , m O t is dominated by the associated memory segment of w t\u2212i that obtains the high combined weights . In the last operation INTERACT , A - LSTM communicates with O - LSTM by acquiring m O t from O - LSTM and incorporating the summary into the memory update . The update process is as follows : i A t = \u03c3 ( W A i x t + U A i [ H A t [ 1 ] : m O t ] ) + b A i ) f A t = \u03c3 ( W A f x t + U A f [ H A t [ 1 ] : m O t ] ) + b A f ) c A t = tanh ( W A c x t + U A c [ H A t [ 1 ] : m O t ] ) + b A c ) o A t = \u03c3 ( W A o x t + U A o [ H A t [ 1 ] : m O t ] ) + b A o ) c A t = i A t \u0109 A t + c A t\u22121 f A t h A t = tanh ( c A t ) o A t ( 4 ) where W A * , U A * and b A * are weight parameters of the A - LSTM and \u03c3 is the sigmoid activation function . [ : ] denotes vector concatenation operation . m O t can be seen as the summary of the opinion indicator in the left context of w t and H A t [ 1 ] is the most immediate hidden memory of A - LSTM . MIN blends the opinion summary from O - LSTM with the memory from A - LSTM . The co - occurrence relation between aspects and opinion words is modeled by such \" memory fusion \" strategy . Since opinion words can appear on both sides of w t , memory segments corresponding to the right context ( i.e. , \" future \" memory ) should be included . Hence , we conduct bi - directional training for A - LSTM . The work flow of memory interaction and the update process of the internal memories in O - LSTM are kept same with those in A - LSTM except the DIGEST operation . Specifically , we set m A t , the task - specific summary of A - LSTM , as h A t . The second component of MIN is a generic LSTM called S - LSTM for discriminating sentimental sentences and non - sentimental sentences . The design and the process of the memory update in this component are similar to that in Jozefowicz et al ( 2015 ) . In sentences not conveying any sentimental meanings , some words like food , service tend to be misclassified as aspect terms since they are commonly used in user reviews . To avoid this kind of error , we add a constraint that an aspect term should come from sentimental sentence . Specifically , S - LSTM learns the sentimental representation h S T of the sentence and then feeds it in aspect prediction as a soft constraint : On the whole , our proposed MIN framework has three LSTMs and each of them is differentiable . Thus , our MIN framework can be efficiently trained with gradient descent . For A - LSTM and O - LSTM , we use the token - level cross - entropy error between the predicted distribution P ( y T t | x t ) and the gold standard distribution P ( y T , g t | x t ) as the loss function ( T { A , O } ) : Loss ( T ) = \u2212 1 N * T N i=1 T t=1 P ( Y T , g i , t | X i , t ) log [ P ( Y T i , t | X i , t ) ] ( 6 ) For S - LSTM , sentence - level cross entropy error are employed to calculate the corresponding loss : 3 Experiment Loss ( S ) = \u2212 1 N N i=1 P ( Y S , g i | X i ) log [ P ( Y S i | X i ) ] ( 7", "entities": [[12, 13, "MethodName", "LSTM"], [16, 17, "MethodName", "LSTM"], [32, 33, "MethodName", "LSTM"], [89, 90, "MethodName", "LSTM"], [134, 135, "MethodName", "LSTM"], [256, 257, "MethodName", "LSTM"], [265, 266, "MethodName", "LSTM"], [269, 270, "MethodName", "LSTM"], [364, 365, "MethodName", "LSTM"], [368, 369, "MethodName", "LSTM"], [393, 394, "MethodName", "LSTM"], [396, 398, "TaskName", "term extraction"], [415, 416, "MethodName", "LSTM"], [428, 429, "MethodName", "LSTM"], [464, 465, "MethodName", "LSTM"], [540, 541, "MethodName", "LSTM"], [594, 595, "MethodName", "LSTM"], [1066, 1067, "MethodName", "LSTM"], [1071, 1072, "MethodName", "LSTM"], [1080, 1081, "MethodName", "LSTM"], [1281, 1282, "MethodName", "LSTM"], [1286, 1288, "MethodName", "sigmoid activation"], [1334, 1335, "MethodName", "LSTM"], [1344, 1345, "MethodName", "LSTM"], [1351, 1352, "MethodName", "LSTM"], [1415, 1416, "MethodName", "LSTM"], [1434, 1435, "MethodName", "LSTM"], [1443, 1444, "MethodName", "LSTM"], [1465, 1466, "MethodName", "LSTM"], [1480, 1481, "MethodName", "LSTM"], [1484, 1485, "MethodName", "LSTM"], [1574, 1575, "MethodName", "LSTM"], [1631, 1632, "MethodName", "LSTM"], [1635, 1636, "MethodName", "LSTM"], [1678, 1679, "MetricName", "loss"], [1740, 1741, "MethodName", "LSTM"], [1754, 1755, "MetricName", "loss"]]}
{"text": "To evaluate the proposed MIN framework , we perform comparison with the following two groups of methods : ( 1 ) CRF based methods : CRF : Conditional Random Fields with basic feature templates 2 and word embeddings . Semi - CRF : First - order semi - Markov conditional random fields ( Sarawagi et al , 2004 ) and the feature template in Cuong et al ( 2014 ) is adopted . IHS RD ( Chernyshevich , 2014 ) , NLANGP ( Toh and Su , 2016 ) : Best systems in ATE subtask in SemEval ABSA challenges ( Pontiki et al , 2014 ( Pontiki et al , , 2016 . DLIREC ( Toh and Wang , 2014 ) , AUEB ( Xenos et al , 2016 ) : Top - ranked CRF - based systems in ATE subtask in SemEval ABSA challenges ( Pontiki et al , 2014 ( Pontiki et al , , 2016 . WDEmb ( Yin et al , 2016 ) : Enhanced CRF with word embeddings , linear context embeddings and dependency path embeddings . ( 2 ) Neural Network based methods LSTM : Vanilla bi - directional LSTM with pre - trained word embeddings 3 . RNCRF ( Wang et al , 2016 ) : Dependency Tree based Recursive Neural Network with CRF extractor 4 . For datasets in the restaurant domain , we train word embeddings of dimension 200 with word2vec ( Mikolov et al , 2013 ) on Yelp reviews 5 . For those in laptop domain , we use pre - trained glove.840B.300d 6 . 2 http://sklearn - crfsuite.readthedocs.io/en/latest/ 3 As we use our own implementation of LSTM , the reported results are different from those in ( Liu et al , 2015 ) 4 Specifically , we list the result of RNCRF over D1 without opinion annotations for fair comparison . As no result is provided for RNCRF - no - opinion over D2 , we report the corresponding performance of the full model . See their following works ( Wang et al , 2017a , b ) . Also , CMLA ( Wang et al , 2017a ) The hyper - parameters are selected via ten - fold cross validation . The dimension of hidden representations are 100 , 20 , 40 for A - LSTM , O - LSTM and S - LSTM respectively . The dropout rate for O - LSTM and S - LSTM is 0.4 . The size of the aspect ( opinion ) memory n m is 4 . The batch size is set to 32 . As for initialization of network parameters , we adopt the strategy that the initial weights are sampled from the uniform distribution ( Glorot and Bengio , 2010 ) . We employ ADAM ( Kingma and Ba , 2014 ) as optimizer and the default settings of ADAM are used . To better reveal the capability of the proposed MIN , we train 5 models with the same group of hyper - parameters and report the average F 1 score over the testing set .", "entities": [[21, 22, "MethodName", "CRF"], [25, 26, "MethodName", "CRF"], [36, 38, "TaskName", "word embeddings"], [41, 42, "MethodName", "CRF"], [134, 135, "MethodName", "CRF"], [169, 170, "MethodName", "CRF"], [171, 173, "TaskName", "word embeddings"], [189, 190, "MethodName", "LSTM"], [195, 196, "MethodName", "LSTM"], [200, 202, "TaskName", "word embeddings"], [220, 221, "MethodName", "CRF"], [233, 235, "TaskName", "word embeddings"], [278, 279, "MethodName", "LSTM"], [388, 389, "MethodName", "LSTM"], [392, 393, "MethodName", "LSTM"], [396, 397, "MethodName", "LSTM"], [405, 406, "MethodName", "LSTM"], [409, 410, "MethodName", "LSTM"], [428, 430, "HyperparameterName", "batch size"], [466, 467, "DatasetName", "ADAM"], [475, 476, "HyperparameterName", "optimizer"], [481, 482, "DatasetName", "ADAM"]]}
{"text": "The first set of experiments consists in evaluating which is the best method for combining word - level embeddings into sentence representations in order to understand what kind of implicit linguistic properties are encoded within both contextual and noncontextual representations using different combining methods . To do so , we firstly extracted from each sentence in the UD dataset the corresponding word embeddings using the output of the internal representations of Word2vec and BERT layers 1 As suggested in Jawahar et al ( 2019 ) ( from input layer - 12 to output layer - 1 ) . Secondly , we computed the sentence - representations according to the different combining strategies defined in 3.3 . We then performed our set of 68 probing tasks using the LinearSVR model for each sentence representation . Since the majority of our probing features is correlated to sentence length , we compared probing results with the ones obtained with a baseline computed by measuring the \u03c1 coefficient between the length of the UD sentences and each of the 68 probing features . Evaluation was performed with a 5 - cross fold validation and using Spearman correlation score ( \u03c1 ) between predicted and gold labels as evaluation metric . Table 2 report average \u03c1 scores aggregating all probing results ( All features ) and according to raw text ( Raw text ) , morphosyntactic ( Morphosyntax ) and syntactic ( Syntax ) levels of annotations . Scores are computed by averaging Max - , Min - pooling , Mean and Sum results . As a general remark , we notice that the scores obtained by Word2vec and BERT 's internal representations outperforms the ones obtained with the correlation baseline , thus showing that both models are capable of implicitly encoding a wide spectrum of linguistic phenomena . Interestingly , we can notice that Word2vec sentence representations outperform BERT ones when considering all the probing features in average . We report in Table 3 and Figure 1 the probing scores obtained by the two models . For what concerns Word2vec representations , we notice that the Sum method prove to be the best one for encoding raw text and syntactic features , while mo - rophosyntactic properties are better represented averaging all the word embeddings ( Mean ) . In general , best results are obtained with probing tasks related to morphosyntactic and syntactic features , like the distribution of POS ( e.g. upos dist PRON , upos dist VERB ) or the maximum depth of the syntactic tree ( parse depth ) . If we look instead at the average \u03c1 scores obtained with BERT layerwise representations ( Figure 1 ) , we observe that , differently from Word2vec , best results are the ones related to raw - text features , such as sentence length or Type / Token Ratio . The Mean method prove to be the best one for almost all the probing tasks , achieving highest scores in the first five layers . The only exceptions mainly concern some of the linguistic features related to syntactic properties , e.g. the average length of dependency links ( avg links len ) or the maximum depth of the syntactic tree ( parse depth ) , for which best scores across layers are obtained with the Sum strategy . The Maxand Min - pooling methods , instead , show a similar trend for almost all the probing features . Interestingly , the representations corresponding to the [ CLS ] token , although considered as a summarization of the entire input sequence , achieve results comparable to those obtained with Maxand Minpooling methods . Moreover , it can be noticed that , unlike Maxand Min - pooling , the representations computed with Mean and Sum methods tend to lose their average precision in encoding our set of linguistic properties across the 12 layers . In order to investigate more in depth how the linguistic knowledge encoded by BERT across its layers differs from that learned by Word2vec , we report in Table 4 average \u03c1 differences between the two models according to the four combining strategies . As a general remark , we can notice that , regardless of the aggregation strategy taken into account , BERT and Word2vec sentence representations achieve quite similar results on average . Hence , although BERT is capable of understanding the full context of each word in an input sequence , the amount of linguistic knowledge implicitly encoded in its aggregated sentence representations is still comparable to that which can be achieved with a non - contextual language model . In Figure 2 we report instead the differences between BERT and Word2vec scores for all the 68 probing features ( ordered by correlation with sentence length ) . For the comparison , we used the representations obtained with the Mean combining method . As a first remark , we notice that there is a clear distinction in terms of \u03c1 scores between features better predicted by BERT and Word2vec . In fact , features most related to syntactic properties ( left heatmap ) are those for which BERT results are generally higher with respect to those obtained with Word2vec . This result demonstrates that BERT , unlike a non - contextual language model as Word2vec , is able to encode information within its representa - tions that involves the entire input sequence , thus making more simple to solve probing tasks that refer to syntatic characteristics . Focusing instead on the right heatmap , we observe that Word2vec non - contextual representations are still capable of encoding a wide spectrum of linguistic properties with higher \u03c1 values compared to BERT ones , especially if we consider scores closer to BERT 's output layers ( from - 4 to - 1 ) . This is particularly evident for morphosyntactic features related to the distribution of POS categories ( xpos dist * , upos dist * ) , most likely because non - contextual representations tend to encode properties related to single tokens rather than syntactic relations between them .", "entities": [[57, 58, "DatasetName", "UD"], [61, 63, "TaskName", "word embeddings"], [73, 74, "MethodName", "BERT"], [169, 170, "DatasetName", "UD"], [191, 193, "MetricName", "Spearman correlation"], [274, 275, "MethodName", "BERT"], [314, 315, "MethodName", "BERT"], [379, 381, "TaskName", "word embeddings"], [419, 421, "HyperparameterName", "maximum depth"], [441, 442, "MethodName", "BERT"], [533, 535, "HyperparameterName", "maximum depth"], [593, 594, "TaskName", "summarization"], [637, 639, "MetricName", "average precision"], [664, 665, "MethodName", "BERT"], [713, 714, "MethodName", "BERT"], [728, 729, "MethodName", "BERT"], [782, 783, "MethodName", "BERT"], [839, 840, "MethodName", "BERT"], [854, 855, "MethodName", "heatmap"], [860, 861, "MethodName", "BERT"], [877, 878, "MethodName", "BERT"], [925, 926, "MethodName", "heatmap"], [952, 953, "MethodName", "BERT"], [962, 963, "MethodName", "BERT"]]}
{"text": "With the growth of the internet , social media becomes a crucial part of everyone 's life . As every coin has two side positive and negative , social media also comes with a number of problems . The challenge of identifying misogyny ( Srivastava et al , 2017 ) in different social media specially in forms of meme which contains both image and text is very complicated . Misogyny meme highly affected the life of women 's as its spread hate and prejudice behaviour against women 's . Social media like twitter , Instagram , etc have handled by their own ways . However , detecting such memes is highly challenging . Due to this challenge , it attracts the researcher 's attention . According to one social media Instagram , more than 1 million users shared memes daily . So , with this huge amount of data in social medias and internet it is impossible to detect every misogyny meme by man power . So , we need machine learning , deep learning and artificial intelligence techniques to detect automatically misogyny memes in social media . In this paper , we have explored various Machine Learning ( ML ) and Deep Learning ( DL ) algorithms for misogyny identification in shared task MAMI ( Fersini et al , 2022 ) challenge and my team 's name is IIT DHANBAD CODECHAMPS . As per requirement of MAMI , I have submitted 4 runs for Subtask - A. My best run in Subtask - A has achieved Macro - F1 score of 0.656 .", "entities": [[257, 260, "MetricName", "Macro - F1"]]}
{"text": "For Subtask - A , we have submitted 4 runs based on four different algorithms , namely - Logistic Regression ( Sammut and Webb , 2010 ) , SVM ( Noble , 2006 ) , LSTM ( Hochreiter and Schmidhuber , 1997 ) , Bert ( Devlin et al , 2018 ) with different parameters like batch size , epochs , number of perceptron etc . We have used the scikit - learn library for logistic regression based models and SVM ( support vector machines ) models . Keras is used for LSTM and BERT . We scored maximum F1 score 0.656 using BERT . We have used the following value of parameters : - 1.For TfidfVectorizer , we have used mindf=20 , maxfeatures=2000 and maxdf=0.6 . 2 . For LSTM and BERT , we have used batch size = 2 , epochs = 3 and number of layers = 2 .", "entities": [[18, 20, "MethodName", "Logistic Regression"], [28, 29, "MethodName", "SVM"], [35, 36, "MethodName", "LSTM"], [56, 58, "HyperparameterName", "batch size"], [75, 77, "MethodName", "logistic regression"], [80, 81, "MethodName", "SVM"], [92, 93, "MethodName", "LSTM"], [94, 95, "MethodName", "BERT"], [99, 101, "MetricName", "F1 score"], [103, 104, "MethodName", "BERT"], [130, 131, "MethodName", "LSTM"], [132, 133, "MethodName", "BERT"], [137, 139, "HyperparameterName", "batch size"], [146, 149, "HyperparameterName", "number of layers"]]}
{"text": "The results of Subtask - A are represented in terms of Macro - F1 ( shown in Table 2 ) . The best score as Macro - F1 for Subtask - A we get is 0.656 . Table 2 shows the score of our submissions based on different algorithms on MAMI challenge official ranking . For Subtask - A BERT performs better than all other models with the parameters batch size = 2 , epochs = 3 , number of hidden layers = 2 and number of perceptron 's is 128 in first layer and 64 in second layer .", "entities": [[11, 14, "MetricName", "Macro - F1"], [25, 28, "MetricName", "Macro - F1"], [59, 60, "MethodName", "BERT"], [69, 71, "HyperparameterName", "batch size"]]}
{"text": "In previous entailment graph research ( Hosseini et al , 2018 ) a representation vector is computed for each typed predicate in the graph . These are compared via the DIH to establish entailment edges between predicates . The features of each vector are typically based on the argument pairs seen with that predicate . Specifically , for a typed predicate p with corresponding vector v , v consists of features f i which are the pointwise mutual information ( PMI ) of p and the argument pair a i { ( e m , e n ) | e m E t 1 , e n E t 2 } . Here t 1 , t 2 T , and E t is the subset of entities of type t. For example , the predicate BUILD ( : company , : thing ) might have some feature f 37 , the PMI of \" build \" with argument pair ( Apple , iPhone ) . A Balanced Inclusion ( BInc ) score is calculated for the directed entailment from one predicate to another ( Szpektor and Dagan , 2008 ) . BInc is the geometric mean of two subscores : a directional score , Weeds Precision ( Weeds and Weir , 2003 ) , measuring how much one vector 's features \" cover \" the other 's ; and a symmetrical score , Lin Similarity ( Lin , 1998 ) , which downweights infrequent predicates that cause spurious false positives . In this work we compute local binary graphs following Hosseini et al ( 2018 ) and leverage the new MDIH to compute additional entailments for unaries and between valencies . To do this we compute a vector for each argument slot respecting its position in the predicate . For a predicate p , a slot vector v ( s ) for s { 1 , 2 } consists of features f i E t . Slot vectors are computed for the slot in unary relations and both slots in binaries . Each slot vector for p has size | v ( s ) | = | E t | , the number of entities in the data with the same type t. Continuing the example , we calculate two vectors for BUILD ( : company , : thing ) : v ( 1 ) R | E : company | which contains a feature for Apple , and v ( 2 ) R | E : thing | which contains a feature for iPhone . Slot vectors are comparable if they represent the same entity type . Edges are learned by comparing corresponding slot vectors between predicates . For instance , DEFEAT ( : person1 , : person2 ) BE.WINNER ( : person1 ) 4 is learned by comparing the slot 1 vector of DEFEAT with the slot 1 vector of BE.WINNER . If the entities who have defeated someone are usually found amongst the entities who are winners then we get a high BInc score , indicating defeat entails that its subject is a winner . Figure 2 Because a unary has only one type t i it may be entailed by binaries in up to 2 * | T | \u2212 1 subgraphs with types { ( t i , t j ) | j T } , i.e. all bivalent graphs containing type t i . We learn entailments from unaries ( UU ) in separate 1 - type univalent graphs . This efficiently learns one set of entailments for each unary , but allows them to be freely entailed by higher - valency predicates , e.g. binaries . Bivalent graphs point transitively into univalent graphs . In Figure 2 , DEFEAT ( : person1 , : person2 ) BE.WINNER ( : person1 ) in the person - person graph . E.g. further entailments of BE.WINNER ( : person ) are in the person univalent graph . Figure 2 : Bivalent graphs model entailments from binary predicates to equal - and lower - valency predicates ( binary and unary ) . Univalent graphs model entailments from unaries to equal - valency unary predicates .", "entities": [[206, 207, "MetricName", "Precision"]]}
{"text": "The models produce a gradation of judgement scores between 0 ( false ) and 1 ( true ) . As in earlier work , we slide a classification threshold over the score range to produce a precision - recall curve for each model . Results are in Figure 3 ( left ) . Multivalent graph performance is shown incrementally . The BB model can answer a portion of binary questions ; the UU model can answer more unary questions ; adding the BU model can answer still more unary questions using binary evidence . We observe successful inference of our kill / die example and others . \" Obama was elected to office \" affirms the question \" Was Obama a candidate ? \" and \" Zach Randolph returned \" affirms \" Did Zach Randolph arrive ? \" Our test set is from multiple sources over the same time period . The exact - match baseline shows the limitations of answering questions simply by collecting more data ; most questions require inference to answer . The complete MGraph achieves 3x this recall by drawing inferences . Our model achieves higher precision than BERT and RoBERTa similarity models in the low recall range . The similarity models perform well , achieving full recall by generalizing for rarer predicates . We note that RoBERTa bests BERT due to extensive in - domain pretraining . The BB model appears to struggle . In fact 90.5 % of unary questions have a vertex in the graph , but only 64.1 % of binaries do . The BB model frequently can not answer questions because the question predicate was n't seen in training . This difference is because binary predicates are more diverse so suffer more from sparsity : they are often multiword expressions and have a second , typed argument . Indeed , most binary predicate research ( in symbolic methods ) focuses on only the top 50 % of recall in several datasets ( Berant et al , 2010 ( Berant et al , , 2015Levy and Dagan , 2016 ; Hosseini et al , 2018 ) . For an even comparison we create a filtered question set . From all questions we remove those without a vertex in the MGraph , then balance them as in 5 , resulting in 20 , 519 questions ( 10 , 273 unary and 10 , 246 binary ) . This filtered test directly compares the models , since both the entailment graphs and the similarity models have a chance to answer all the questions . Results are shown in Figure 3 ( right ) , with a very different outcome . Head - to - head , the MGraph offers substantially better precision across all recall levels . At 50 % recall , the MGraph has 76 % precision with RoBERTa at 65 % . Notably , on both tests , more unary questions are answered using both unary and binary predicate evidence than just using unary evidence alone . On the filtered test , the BU model increases max recall from 54 % to 70 % . Finally , we note PPDB 's poor performance ( highest recall shown ) , only 1 % higher recall than the exact - match baseline despite having entries for 88 % of questions . Though PPDB features many directional entailments , this sparsity of edges useful for the task is likely because bilingual pivoting excels at detecting near - paraphrases , not relations between distinct eventualities , e.g. it ca n't learn \" getting elected \" entails \" being a candidate . \" Advantageously , our method learns this open - domain knowledge by tracking entities across all the events they participate in . We show a breakdown of the filtered test results in Table 3 . Models do n't answer all the questions , so following Lewis and Steedman ( 2013 ) who design a similar QA task , we evaluate models on the accuracy of their K most confident predictions .", "entities": [[9, 10, "DatasetName", "0"], [27, 29, "HyperparameterName", "classification threshold"], [192, 193, "MethodName", "BERT"], [194, 195, "MethodName", "RoBERTa"], [221, 222, "MethodName", "RoBERTa"], [223, 224, "MethodName", "BERT"], [475, 476, "MethodName", "RoBERTa"], [668, 669, "MetricName", "accuracy"]]}
{"text": "In general , each option should have the same correct rate for multi - choice questions , but in fact , the order in which the correct options appear is not completely random , and the more the number of options , the lower the degree of randomization ( Poundstone , 2014 ) . Given the complex nature of multi - choice tasks , we employ three control methods to ensure a fair comparison among various open - domain QA models . Random A \u2032 = Random ( O ) . For each question , an option is randomly chosen as the answer from five candidate options . We perform this experiment five times and average the results as the baseline of the Random method . Constant A \u2032 = Constant j ( O ) , where j { A , B , C , D , E } . For each question , the j th option is always chosen as the answer to obtain the accuracy distribution of five candidate options . Mixed A \u2032 = M ixed ( O ) . Incorporating the previous experiences of NMLEC and multi - choice task work ( Vilares and G\u00f3mez - Rodr\u00edguez , 2019 ) , the Mixed method simulates how humans solving uncertain questions , and consists of the following three strategies : ( 1 ) the correct rate of choosing \" All of the options above is correct / incorrect \" is much higher than the other options . ( 2 ) Supposing the length of options is roughly equal , only one option is obviously longer with more detailed and specific descriptions , or is obviously shorter than the other options , then choose this option . ( 3 ) The correct option tends to appear in the middle of candidate options . The three strategies are applied in turn . If any strategy matches , then the option that matches the strategy is chosen as the answer .", "entities": [[167, 168, "MetricName", "accuracy"]]}
{"text": "We conduct detailed experiments and analyses to investigate the performance of control methods and open - domain QA methods on MLEC - QA . As shown in Figure 2 , we implement a two - stage retriever - reader framework : ( 1 ) a retriever first retrieves question relevant documents from Chinese Wikipedia using ElasticSearch , ( 2 ) and then a reader employs machine reading comprehension models to generate answers in given documents retrieved by the retriever . For the reader , all machine reading comprehension models are trained with 12 epochs , an initial learning rate of 2e - 6 , a maximum sequence length of 512 , a batch size of 5 . The parameters are selected based on the best performance on the development set , and we keep the default values for the other hyper - parameters ( Devlin et al , 2019 ) . We use accuracy as the metric to evaluate different methods , and provide baseline results , as well as human pass mark ( 60 % ) instead of human performance due to the wide variations exist in human performance , from almost full marks to can not even pass the exam .", "entities": [[65, 68, "TaskName", "machine reading comprehension"], [85, 88, "TaskName", "machine reading comprehension"], [97, 99, "HyperparameterName", "learning rate"], [112, 114, "HyperparameterName", "batch size"], [153, 154, "MetricName", "accuracy"]]}
{"text": "The main drawbacks of the Chinese Wikipedia database in biomedicine are that it is not comprehensive and thorough , that is , it may not provide complete coverage of all subjects . To evaluate whether retrieved documents can cover enough evidence to answer questions , we sampled 5 % ( 681 ) questions from the development sets of five categories using stratified random sampling , and manually annotate each question by five medical experts with 3 labels : ( 1 ) Exactly Match ( EM ) : the retrieved documents exactly match the question . ( 2 ) Partial Match ( PM ) : the retrieved documents partially match the question , can be confused with the correct options or are incomplete . ( 3 ) Mismatch ( MM ) : the retrieved documents do not match the question at all . Table 7 lists the performance of the retrieval strategy as well as the results of the annotation for KQ and CQ questions on five subsets . From the table , we make the following observations . First , most retrieved documents indicate PM with the questions , while the matching rates of EM and MM achieve maximums of 20.83 % ( CWM ) and 50 % ( PH ) , respectively . Second , the matching rate of CQ is higher than KQ in most subsets as CQ are usually related to simpler concepts , and use more words to describe questions , which leads to easier retrieval . By contrast , KQ usually involve more complex concepts that may not be included in the Chinese Wikipedia database . Therefore , the mismatching rate of KQ is significantly higher than that of CQ . Third , among different subsets , the performance in the subset Cli achieves the best as clinical medicine is more \" general \" to retrieve compare with other specialties . Whereas the performance in the subset PH achieves the worst as the Public Health is usually related to \" confusing concepts \" , which leads to poor retrieval performance .", "entities": [[84, 85, "MetricName", "EM"], [194, 195, "MetricName", "EM"]]}
{"text": "Sources Both books and Wikipedia have been used as the information sources in previous research . One of our subsets , Clinic , has been studied by MEDQA ( Jin et al , 2020 ) as a subset ( MCMLE ) for cross - lingual research . MEDQA uses 33 medical textbooks as their information sources and the evaluation result shows that their collected text materials can provide enough information to answer all the questions in MCMLE . We compare the best model ( RoBERTa - wwm - ext - large ) performance on both datasets as shown in Table 9 . Notably , questions in MCMLE have four candidate options due to one of the wrong options being deleted . Therefore , the random accuracy on MCMLE is higher than ours . From the results we can see that even with 100 % covered materials , the best model can only achieve 16.88 % higher accuracy on the test set than ours , which indicates that using Wikipedia as information sources is not that terrible compared with medical books , and the main reason for baseline performance may come from machine reading comprehension models that lack sophisticated reasoning ability . Retriever - Reader We also perform an experiment that sampled 5 % ( 92 ) questions from the development set of Public Health , and manually annotate each question by a medical expert to determine whether that can exactly or partially match with the top K retrieved documents , as shown in Table 10 . Notably , the actual number of retrieved documents is 5\u00d7K as we define Q i O ij = Q i + O ij as a search query and is repeated for all options . From the results , we can see that more documents even bring more noise instead , as the best match documents have already been fetched in the top 1 documents . It indicates that the poor performance of machine reading comprehension models is coming from the insufficiency of reasoning ability rather than the number of retrieved documents . In order to benefit researchers on improving the open - domain QA models , and also make advances for Biomedical Question Answering ( BQA ) systems , we present MLEC - QA , the largest - scale Chinese multi - choice BQA dataset to date .", "entities": [[84, 85, "MethodName", "RoBERTa"], [125, 126, "MetricName", "accuracy"], [156, 157, "MetricName", "accuracy"], [191, 194, "TaskName", "machine reading comprehension"], [328, 331, "TaskName", "machine reading comprehension"], [368, 370, "TaskName", "Question Answering"]]}
{"text": "We use the following notation . If n is an integer , [ n ] denotes the set { 1 , . . . , n } . Let \u0393 be an alphabet , i.e. , a finite set . Then s \u0393 * denotes that s is a sequence of arbitrary length , each element of which is in \u0393. We denote by | s | the length of s. A ranked alphabet is an alphabet \u0393 paired with an arity mapping ( i.e. , a total function ) rank : \u0393 N. Definition 1 . A hypergraph ( or simply graph ) over a ranked alphabet \u0393 is a tuple G = ( V G , E G , att G , lab G , ext G ) where V G is a finite set of nodes ; E G is a finite set of edges ( distinct from V G ) ; att G : E G V * G maps each edge to a sequence of nodes ; lab G : E G \u0393 maps each edge to a label such that | att G ( e ) | = rank ( lab G ( e ) ) ; and ext G is an ordered subset of V G called the external nodes of G. We assume that the elements of ext G are pairwise distinct , and the elements of att G ( e ) for each edge e are also pairwise distinct . An edge e is attached to its nodes by tentacles , each labeled by an integer indicating the node 's position in att G ( e ) = ( v 1 , . . . , v k ) . The tentacle from e to v i will have label i , so the tentacle labels lie in the set [ k ] where k = rank ( e ) . To express that a node v is attached to the ith tentacle of an edge e , we say vert ( e , i ) = v. Likewise , the nodes in ext G are labeled by their position in ext G . We refer to the ith external node of G by ext G ( i ) and in figures this will be labeled ( i ) . The rank of an edge e is k if att ( e ) = ( v 1 , . . . , v k ) ( or equivalently , rank ( lab ( e ) ) = k ) . The rank of a hypergraph G , denoted by rank ( G ) is the size of ext G . Example 1 . Hypergraph G in Figure 2 has four nodes ( shown as black dots ) and three hyperedges labeled a , b , and X ( shown boxed ) . The bracketed numbers ( 1 ) and ( 2 ) denote its external nodes and the numbers between edges and the nodes are tentacle labels . Call the top node v 1 and , proceeding clockwise , call the other nodes v 2 , v 3 , and v 4 . Call its edges e 1 , e 2 and e 3 . Its definition would state att G ( e 1 ) = ( v 1 , v 2 ) , att G ( e 2 ) = ( v 2 , v 3 ) , att G ( e 3 ) = ( v 1 , v 4 , v 3 ) , lab G ( e 1 ) = a , lab G ( e 2 ) = b , lab G ( e 3 ) = X , and ext G = ( v 4 , v 2 ) . Definition 2 . Let G be a hypergraph containing an edge e with att G ( e ) = ( v 1 , . . . , v k ) and let H be a hypergraph of rank k with node and edge sets disjoint from those of G. The replacement of e by H is the graph G = G [ e / H ] . Its node set V G is V \u222a V H where V = V G \u2212 { v 1 , . . . , v k } . Its edge set is E G = ( E G \u2212 { e } ) \u222a E H . We define att G = att \u222a att H where for every e ( E G \u2212 { e } ) , att ( e ) is obtained from att G ( e ) by replacing v i by the ith external node of H. Let lab G = lab \u222a lab H where lab is the restriction of lab G to edges in E G \u2212 { e } . Finally , let ext G = ext G . Example 2 . A replacement is shown in Figure 2 .", "entities": [[29, 30, "HyperparameterName", "\u0393"], [42, 43, "HyperparameterName", "\u0393"], [77, 78, "HyperparameterName", "\u0393"], [92, 93, "HyperparameterName", "\u0393"], [108, 109, "HyperparameterName", "\u0393"], [177, 178, "HyperparameterName", "\u0393"], [313, 315, "HyperparameterName", "k ="]]}
{"text": "A regular graph grammar ( RGG ; Courcelle 1991 ) is a restricted form of HRG . To explain the restrictions , we first require some definitions . Definition 4 . Given a graph G , a path in G from a node v to a node v is a sequence ( v 0 , i 1 , e 1 , j 1 , v 1 ) ( v 1 , i 2 , e 2 , j 2 , v 2 ) . . . ( v k\u22121 , i k , e k , j k , v k ) ( 1 ) such that v 0 = v , v k = v , and for each r [ k ] , vert ( e r , i r ) = v r\u22121 and vert ( e r , j r ) = v r . The length of this path is k. A path is terminal if every edge in the path has a terminal label . A path is internal if each v i is internal for 1 \u2264 i \u2264 k \u2212 1 . Note that the endpoints v 0 and v k of an internal path can be external . Definition 5 . A HRG G is a Regular Graph Grammar ( or simply RGG ) if each nonterminal in N G has rank at least one and for each p P G the following hold : ( C1 ) R ( p ) has at least one edge . Either it is a single terminal edge , all nodes of which are external , or each of its edges has at least one internal node . ( C2 ) Every pair of nodes in R ( p ) is connected by a terminal and internal path . Example 4 . The grammar in Table 1 is an RGG . Although HRGs can produce context - free languages ( and beyond ) as shown in Figure 4 , the only string languages RGGs can produce are the regular string languages . See Figure 5 for an example of a string generating RGG . Similarly , RGGs can produce regular tree languages , but not context - free tree languages . Figure 6 shows a tree generating RGG that generates binary trees the internal nodes of which are represented by a - labeled edges , and the leaves of which are represented by b - labeled edges . Note that these two results of regularity of the string - and tree - languages generated by RGG follow from the fact that graph languages produced by RGG are MSO - definable ( Courcelle , 1991 ) , and the well - known facts that the regular string and graph languages are MSO - definable . X ( 1 ) a Y ( 1 ) b 1 1 Figure 5 : A RGG for a regular string language .", "entities": [[53, 54, "DatasetName", "0"], [108, 109, "DatasetName", "0"], [113, 115, "HyperparameterName", "k ="], [195, 196, "DatasetName", "0"]]}
{"text": "Just as the algorithm of Chiang et al ( 2013 ) generalizes CKY to HRG , our algorithm generalizes Earley 's algorithm ( Earley , 1970 ) . Both algorithms operate by recognizing incrementally larger subgraphs of the input graph , using a succinct representation for subgraphs that depends on an arbitrarily chosen marker node m of the input graph . For each production p of the grammar , we impose a fixed order on the edges of R ( p ) , as in Drewes et al ( 2015 ) . We discuss this order in detail in 3.2 . As in Earley 's algorithm , we use dotted rules to represent partial recognition of productions : X \u0113 1 . . .\u0113 i\u22121 \u0113 i . . .\u0113 n means that we have identified the edges\u0113 1 to\u0113 i\u22121 and that we must next recognize edge\u0113 i . We write\u0113 and v for edges and nodes in productions and e and v for edges and nodes in a derived graph . When the identity of the sequence is immaterial we abbreviate it as \u03b1 , for example writing X \u03b1 . We present our recognizer as a deductive proof system ( Shieber et al , 1995 ) . The items of the recognizer are of the form Name Rule Conditions PREDICT [ b ( I ) , p : X \u01131 . . . \u0113 i . . .\u0113n , \u03c6p ] [ q : Y \u03b1 ] [ \u03c6p ( \u0113i ) , q : Y \u03b1 , \u03c6 0 q [ ext R ( q ) = \u03c6p ( \u0113i ) ] ] lab ( \u0113i ) = Y SCAN [ b ( I ) , X \u01131 . . . \u0113 i . . .\u0113n , \u03c6p ] [ e = edg lab ( \u0113 i ) ( v1 , . . . , vm ) ] [ b ( I \u222a { e } ) , X \u01131 . . . \u0113 i+1 . . .\u0113n , \u03c6p [ att ( \u0113i ) = ( v1 , . . . , vm ) ] ] \u03c6p ( \u0113i ) ( j ) VG \u21d2 \u03c6p ( \u0113i ) ( j ) = vert ( e , j ) COMPLETE [ b ( I ) , p : X \u01131 . . . \u0113 i . . .\u0113n , \u03c6p ] [ b ( J ) , q : Y \u03b1 , \u03c6q ] [ b ( I \u222a J ) , X \u01131 . . . \u0113 i+1 . . .\u0113n , \u03c6p [ att ( \u0113i ) = \u03c6p ( ext R ( q ) ) ] ] \u03c6p ( \u0113i ) ( j ) VG \u21d2 \u03c6p ( \u0113i ) ( j ) = \u03c6q ( ext R ( q ) ) ( j ) , lab ( \u0113i ) = Y , EI \u2229 EJ = [ b ( I ) , p : X \u0113 1 . . . \u0113 i . . .\u0113 n , \u03c6 p ] where I is a subgraph that has been recognized as matching\u0113 1 , . . . , \u0113 i\u22121 ; p : X \u0113 1 , . . . , \u0113 n is a production in the grammar with the edges in order ; and \u03c6 p : E R ( p ) V * G maps the endpoints of edges in R ( p ) to nodes in G. For each production p , we number the nodes in some arbitrary but fixed order . Using this , we construct the function \u03c6 0 p : E R ( p ) V * R ( p ) such that for\u0113 E R ( p ) if att ( \u0113 ) = ( v 1 , v 2 ) then \u03c6 0 p ( \u0113 ) = ( v 1 , v 2 ) . As we match edges in the graph with edges in p , we assign the nodesv to nodes in the graph . For example , if we have an edge\u0113 in a production p such that att ( \u0113 ) = ( v 1 , v 2 ) and we find an edge e which matches\u0113 , then we update \u03c6 p to record this fact , written \u03c6 p [ att ( \u0113 ) = att ( e ) ] . We also use \u03c6 p to record assignments of external nodes . If we assign the ith external node to v , we write \u03c6 p [ ext p ( i ) = v ] . We write \u03c6 0 p to represent a mapping with no grounded nodes . Since our algorithm makes top - down predictions based on known external nodes , our boundary representation must cover the case where a subgraph is empty except for these nodes . If at some point we know that our subgraph has external nodes \u03c6 ( \u0113 ) , then we use the shorthand \u03c6 ( \u0113 ) rather than the full boundary representation \u03c6 ( \u0113 ) , , m \u03c6 ( \u0113 ) . To keep notation uniform , we use dummy nonterminal S * N G that derives S G via the production p 0 . For graph G , our system includes the axiom : [ ext G , p 0 : S * S G , \u03c6 0 p 0 [ ext R ( p 0 ) = ext G ] ] . Our goal is to prove : [ b ( G ) , p S : S * S G , \u03c6 p S ] where \u03c6 p S has a single edge\u0113 in its domain which has label S G in R ( p S ) and \u03c6 p S ( \u0113 ) = ext G . As in Earley 's algorithm , we have three inference rules : PREDICT , SCAN and COMPLETE ( Table 2 ) . PREDICT is applied when the edge after the dot is nonterminal , assigning any external nodes that have been identified . SCAN is applied when the edge after the dot is terminal . Using \u03c6 p , we may already know where some of the endpoints of the edge should be , so it requires the endpoints of the scanned edge to match . COMPLETE requires that each of the nodes of\u0113 i in R ( p ) have been identified , these nodes match up with the corresponding external nodes of the subgraph J , and that the subgraphs I and J are edge - disjoint . We provide a high - level proof that the recognizer is sound and complete . Proposition 1 . Let G be a HRG and G a graph .", "entities": [[185, 186, "HyperparameterName", "\u03b1"], [191, 192, "HyperparameterName", "\u03b1"], [248, 249, "HyperparameterName", "\u03b1"], [259, 260, "HyperparameterName", "\u03b1"], [262, 263, "DatasetName", "0"], [283, 284, "DatasetName", "SCAN"], [415, 416, "HyperparameterName", "\u03b1"], [613, 614, "DatasetName", "0"], [650, 651, "DatasetName", "0"], [785, 786, "DatasetName", "0"], [892, 893, "DatasetName", "0"], [909, 910, "DatasetName", "0"], [917, 918, "DatasetName", "0"], [919, 920, "DatasetName", "0"], [925, 926, "DatasetName", "0"], [1004, 1005, "DatasetName", "SCAN"], [1033, 1034, "DatasetName", "SCAN"]]}
{"text": "The Transformer architecture has become the state - of - the - art in Machine Translation . This model , which relies on attention - based mechanisms , has outperformed previous neural machine translation architectures in several tasks . In this system description paper , we report details of training neural machine translation with multi - source Romance languages with the Transformer model and in the evaluation frame of the biomedical WMT 2018 task . Using multi - source languages from the same family allows improvements of over 6 BLEU points .", "entities": [[1, 2, "MethodName", "Transformer"], [14, 16, "TaskName", "Machine Translation"], [32, 34, "TaskName", "machine translation"], [51, 53, "TaskName", "machine translation"], [61, 62, "MethodName", "Transformer"], [71, 73, "DatasetName", "WMT 2018"], [89, 90, "MetricName", "BLEU"]]}
{"text": "Neural Machine Translation ( NMT ) ( Bahdanau et al , 2015 ) proved to be competitive with the encoder - decoder architecture based on recurrent neural networks and attention . After this architecture , new proposals based on convolutional neural networks ( Gehring et al , 2017 ) or only attention - based mechanisms appeared . The latter architecture has achieved great success in Machine Translation ( MT ) and it has already been extended to other tasks such as Parsing , Speech Recognition 1 , Speech Translation ( Cros et al , 2018 ) , Chatbots among others . However , training with low resources is still a big drawback for neural architectures and NMT is not an exception ( Koehn and Knowles , 2017 ) . To face low resource scenarios , several techniques have been proposed , like using multi - source ( Zoph and Knight , 2016 ) , multiple languages ( Johnson et al , 2017 ) or unsupervised techniques ( Lample et al , 2018 ; Artetxe et al , 2018 ) , among many others . In this paper , we use the Transformer enhanced with the multi - source technique to participate in the Biomedical WMT 2018 task , which can be somehow considered a low - resourced task , given the large quantity of data that it is required for NMT . Our multi - source enhancement is done only with Romance languages . The fact of using similar languages in a multi - source system may be a factor towards improving the final system which ends up with over 6 BLEU points of improvement over the single source system .", "entities": [[1, 3, "TaskName", "Machine Translation"], [65, 67, "TaskName", "Machine Translation"], [83, 85, "TaskName", "Speech Recognition"], [88, 89, "TaskName", "Translation"], [191, 192, "MethodName", "Transformer"], [204, 206, "DatasetName", "WMT 2018"], [271, 272, "MetricName", "BLEU"]]}
{"text": "The results of the regression models showed an association between the estimated correction factors and the proposed adequacy indicators . We trained three single - language systems , one for each language pair . We required 14 epochs for the Spanish - to - English system ( 7 hours of training ) , 16 epochs for the French - to - English system ( 9 hours of training ) , and 17 epochs for the Portuguese - to - English system ( 7 hours of training ) . For the multi - source system , which concatenated the three parallel corpus together , we required 11 epochs ( 23 hours of training ) . We stopped training when the validation accuracy did not increase in two consecutive epochs .", "entities": [[120, 121, "MetricName", "accuracy"]]}
{"text": "Best ranking systems from WMT17 and WMT18 are shown in Table 1 , except for French - to - English WMT17 since the references for this set are not available . For this pair , we used 1000 sentences from the Khresmoi development data . Table 1 shows BLEU results for the baseline systems , the single - language and multi - source approaches . The Transformer architecture outperforms WMT17 best system . Results become even better with the system is trained with the common corpus of Romance languages , what we call the multi - source approach . The latter is consistent with the universal truth that more data equals better results , even if the source language is not the same . Finally , Table 2 shows some examples of the output translations .", "entities": [[48, 49, "MetricName", "BLEU"], [66, 67, "MethodName", "Transformer"]]}
{"text": "The main conclusions of our experiments are that the multi - source inputs of the same family applied to the Transformer architecture can improve the single input . Best improvements achieve an increase of 6 BLEU points in translation quality . part by the Spanish Ministerio de Econom\u00eda y Competitividad , the European Regional Development Fund and the Agencia Estatal de Investigaci\u00f3n , through the postdoctoral senior grant Ram\u00f3n y Cajal , the contract TEC2015 - 69266 - P ( MINECO / FEDER , EU ) and the contract PCIN - 2017 - 079 ( AEI / MINECO ) .", "entities": [[20, 21, "MethodName", "Transformer"], [35, 36, "MetricName", "BLEU"]]}
{"text": "We study relation extraction for knowledge base ( KB ) enrichment . Specifically , we aim to extract entities and their relationships from sentences in the form of triples and map the elements of the extracted triples to an existing KB in an end - to - end manner . Previous studies focus on the extraction itself and rely on Named Entity Disambiguation ( NED ) to map triples into the KB space . This way , NED errors may cause extraction errors that affect the overall precision and recall . To address this problem , we propose an end - to - end relation extraction model for KB enrichment based on a neural encoder - decoder model . We collect high - quality training data by distant supervision with co - reference resolution and paraphrase detection . We propose an n - gram based attention model that captures multi - word entity names in a sentence . Our model employs jointly learned word and entity embeddings to support named entity disambiguation . Finally , our model uses a modified beam search and a triple classifier to help generate high - quality triples . Our model outperforms state - of - theart baselines by 15.51 % and 8.38 % in terms of F1 score on two real - world datasets .", "entities": [[2, 4, "TaskName", "relation extraction"], [61, 63, "TaskName", "Entity Disambiguation"], [104, 106, "TaskName", "relation extraction"], [165, 167, "TaskName", "entity embeddings"], [170, 172, "TaskName", "entity disambiguation"], [212, 214, "MetricName", "F1 score"]]}
{"text": "Knowledge bases ( KBs ) , often in the form of knowledge graphs ( KGs ) , have become essential resources in many tasks including Q&A systems , recommender system , and natural language generation . Large KBs such as DBpedia ( Auer et al , 2007 ) , Wikidata ( Vrandecic and Kr\u00f6tzsch , 2014 ) and Yago ( Suchanek et al , 2007 ) contain millions of facts about entities , which are represented in the form of subject - predicate - object triples . However , these KBs are far from complete and mandate continuous enrichment and curation . Previous studies work on embedding - based model ( Nguyen et al , 2018 ; and entity alignment model ( Chen et al , 2017 ; Trisedya et al , 2019 ) to enrich a knowledge base . Following the success of the sequence - to - sequence architecture ( Bahdanau et al , 2015 ) for generating sentences from structured data ( Marcheggiani and Perez - Beltrachini , 2018 ; Trisedya et al , 2018 ) , we employ this architecture to do the opposite , which is extracting triples from a sentence . In this paper , we study how to enrich a KB by relation exaction from textual sources . Specifically , we aim to extract triples in the form of h , r , t , where h is a head entity , t is a tail entity , and r is a relationship between the entities . Importantly , as KBs typically have much better coverage on entities than on relationships , we assume that h and t are existing entities in a KB , r is a predicate that falls in a predefined set of predicates we are interested in , but the relationship h , r , t does not exist in the KB yet . We aim to find more relationships between h and t and add them to the KB . For example , from the first extracted triples in Table 1 we may recognize two entities \" NYU \" ( abbreviation of New York University ) and \" Private University \" , which already exist in the KB ; also the predicate \" instance of \" is in the set of predefined predicates we are interested in , but the relationship of NYU , instance of , Private University does not exist in the KB . We aim to add this relationship to our KB . This is the typical situation for KB enrichment ( as opposed to constructing a KB from scratch or performing relation extraction for other purposes , such as Q&A or summarization ) . KB enrichment mandates that the entities and relationships of the extracted triples are canonicalized by mapping them to their proper entity and predicate IDs in a KB . Table 1 illustrates an example of triples extracted from a sentence . The entities and predicate of the first extracted triple , including NYU , instance of , and Private University , are mapped to their unique IDs Q49210 , P31 , and Q902104 , respectively , to comply with the semantic space of the KB . Previous studies on relation extraction have employed both unsupervised and supervised approaches . Unsupervised approaches typically start with a small set of manually defined extraction patterns to detect entity names and phrases about relationships in an input text . This paradigm is known as Open Information Extraction ( Open IE ) ( Banko et al , 2007 ; Corro and Gemulla , 2013 ; Gashteovski et al , 2017 ) . In this line of approaches , both entities and predicates are captured in their surface forms without canonicalization . Supervised approaches train statistical and neural models for inferring the relationship between two known entities in a sentence ( Mintz et al , 2009 ; Riedel et al , 2010Riedel et al , , 2013Zeng et al , 2015 ; Lin et al , 2016 ) . Most of these studies employ a preprocessing step to recognize the entities . Only few studies have fully integrated the mapping of extracted triples onto uniquely identified KB entities by using logical reasoning on the existing KB to disambiguate the extracted entities ( e.g. , ( Suchanek et al , 2009 ; Sa et al , 2017 ) ) . Most existing methods thus entail the need for Named Entity Disambiguation ( NED ) ( cf . the survey by Shen et al ( 2015 ) ) as a separate processing step . In addition , the mapping of relationship phrases onto KB predicates necessitates another mapping step , typically aided by paraphrase dictionaries . This two - stage architecture is inherently prone to error propagation across its two stages : NED errors may cause extraction errors ( and vice versa ) that lead to inaccurate relationships being added to the KB . We aim to integrate the extraction and the canonicalization tasks by proposing an endto - end neural learning model to jointly extract triples from sentences and map them into an existing KB . Our method is based on the encoder - decoder framework ( Cho et al , 2014 ) by treating the task as a translation of a sentence into a sequence of elements of triples . For the example in Table 1 , our model aims to translate \" New York University is a private university in Manhattan \" into a sequence of IDs \" Q49210 P31 Q902104 Q49210 P131 Q11299 \" , from which we can derive two triples to be added to the KB . A standard encoder - decoder model with attention ( Bahdanau et al , 2015 ) is , however , unable to capture the multi - word entity names and verbal or noun phrases that denote predicates . To address this problem , we propose a novel form of n - gram based attention that computes the ngram combination of attention weight to capture the verbal or noun phrase context that complements the word level attention of the standard attention model . Our model thus can better capture the multi - word context of entities and relationships . Our model harnesses pre - trained word and entity embeddings that are jointly learned with skip gram ( Mikolov et al , 2013 ) andTransE ( Bordes et al , 2013 ) . The advantages of our jointly learned embeddings are twofold . First , the embeddings capture the relationship between words and entities , which is essential for named entity disambiguation . Second , the entity embeddings preserve the relationships between entities , which help to build a highly accurate classifier to filter the invalid extracted triples . To cope with the lack of fully labeled training data , we adapt distant supervision to generate aligned pairs of sentence and triple as the training data . We augment the process with co - reference resolution ( Clark and Manning , 2016 ) and dictionary - based paraphrase detection ( Ganitkevitch et al , 2013 ; Grycner and Weikum , 2016 ) . The co - reference resolution helps extract sentences with implicit entity names , which enlarges the set of candidate sentences to be aligned with existing triples in a KB . The paraphrase detection helps filter sentences that do not express any relationships between entities . The main contributions of this paper are : We propose an end - to - end model for extract - ( Lin et al , 2016 ) coupled with state - of - the - art NED models ( Hoffart et al , 2011 ; Kolitsas et al , 2018 ) . 2 Related Work 2.1 Open Information Extraction Banko et al ( 2007 ) introduced the paradigm of Open Information Extraction ( Open IE ) and proposed a pipeline that consists of three stages : learner , extractor , and assessor . The learner uses dependency - parsing information to learn patterns for extraction , in an unsupervised way . The extractor generates candidate triples by identifying noun phrases as arguments and connecting phrases as predicates . The assessor assigns a probability to each candidate triple based on statistical evidence . This approach was prone to extracting incorrect , verbose and uninformative triples . Various followup studies ( Fader et al , 2011 ; Mausam et al , 2012 ; Angeli et al , 2015 ; Mausam , 2016 ) improved the accuracy of Open IE , by adding handcrafted patterns or by using distant supervision . Corro and Gemulla ( 2013 ) developed ClausIE , a method that analyzes the clauses in a sentence and derives triples from this structure . Gashteovski et al ( 2017 ) developed MinIE to advance ClausIE by making the resulting triples more concise . Stanovsky et al ( 2018 ) proposed a supervised learner for Open IE by casting relation extraction into sequence tagging . A bi - LSTM model is trained to predict the label ( entity , predicate , or other ) of each token of the input . The work most related to ours is Neural Open IE ( Cui et al , 2018 ) , which proposed an encoder - decoder with attention model to extract triples . However , this work is not geared for extracting relations of canonicalized entities . Another line of studies use neural learning for semantic role labeling ( He et al , 2018 ) , but the goal here is to recognize the predicate - argument structure of a single input sentence - as opposed to extracting relations from a corpus . All of these methods generate triples where the head and tail entities and the predicate stay in their surface forms . Therefore , different names and phrases for the same entities result in multiple triples , which would pollute the KG if added this way . The only means to map triples to uniquely identified entities in a KG is by post - processing via entity linking ( NED ) methods ( Shen et al , 2015 ) or by clustering with subsequent mapping ( Gal\u00e1rraga et al , 2014 ) .", "entities": [[11, 13, "TaskName", "knowledge graphs"], [40, 41, "DatasetName", "DBpedia"], [58, 59, "DatasetName", "Yago"], [118, 120, "TaskName", "entity alignment"], [437, 439, "TaskName", "relation extraction"], [447, 448, "TaskName", "summarization"], [538, 540, "TaskName", "relation extraction"], [579, 582, "TaskName", "Open Information Extraction"], [741, 743, "TaskName", "Entity Disambiguation"], [1049, 1051, "TaskName", "entity embeddings"], [1101, 1103, "TaskName", "entity disambiguation"], [1107, 1109, "TaskName", "entity embeddings"], [1295, 1298, "TaskName", "Open Information Extraction"], [1308, 1311, "TaskName", "Open Information Extraction"], [1395, 1396, "DatasetName", "followup"], [1422, 1423, "MetricName", "accuracy"], [1496, 1498, "TaskName", "relation extraction"], [1505, 1506, "MethodName", "LSTM"], [1581, 1584, "TaskName", "semantic role labeling"], [1684, 1686, "TaskName", "entity linking"]]}
{"text": "Our relation extraction model is based on the encoder - decoder framework which has been widely used in Neural Machine Translation to translate text from one language to another . In our setup , we aim to translate a sentence into triples , and hence the vocabulary of the source input is a set of English words while the vocabulary of the target output is a set of entity and predicate IDs in an existing KG . To compute the embeddings of the source and target vocabularies , we propose a joint learning of word and entity embeddings that is effective to capture the similarity between words and entities for named entity disambiguation ( Yamada et al , 2016 ) . Note that our method differs from that of Yamada et al ( 2016 ) . We use joint learning by combining skip - gram ( Mikolov et al , 2013 ) to compute the word embeddings and TransE ( Bordes et al , 2013 ) to compute the entity embeddings ( including the relationship embeddings ) , while Yamada et al ( 2016 ) use Wikipedia Link - based Measure ( WLM ) ( Milne and Witten , 2008 ) that does not consider the relationship embeddings . Our model learns the entity embeddings by minimizing a margin - based objective function J E : JE = tr Tr t r T r max 0 , \u03b3 + f ( tr ) \u2212 f ( t r ) ( 1 ) Tr = { h , r , t | h , r , t G } ( 2 ) Tr = h , r , t | h E \u222a h , r , t | t E ( 3 ) f ( tr ) = h + r \u2212 t ( 4 ) Here , x is the L1 - Norm of vector x , \u03b3 is a margin hyperparameter , T r is the set of valid relationship triples from a KG G , and T r is the set of corrupted relationship triples ( recall that E is the set of entities in G ) . The corrupted triples are used as negative samples , which are created by replacing the head or tail entity of a valid triple in T r with a random entity . We use all triples in Wikidata except those which belong to the testing data to compute the entity embeddings . To establish the interaction between the entity and word embeddings , we follow the Anchor Context Model proposed by Yamada et al ( 2016 ) . First , we generate a text corpus by combining the original text and the modified anchor text of Wikipedia . This is done by replacing the entity names in a sentence with the related entity or predicate IDs . For example , the sentence \" New York University is a private university in Manhattan \" is modified into \" Q49210 is a Q902104 in Q11299 \" . Then , we use the skip - gram method to compute the word embeddings from the generated corpus ( the entity IDs in the modified anchor text are treated as words in the skip - gram model ) . Given a sequence of n words [ w 1 , w 2 , ... , w n ] , The model learns the word embeddings , by minimizing the following objective function J W : J W = 1 T n t=1 \u2212c\u2264j\u2264c , j = 0 log P ( w t+j | w t ) ( 5 ) P ( w t+j | w t ) = exp ( v w t+j v wt ) W i=1 ( v i v wt ) ( 6 ) where c is the size of the context window , w t denotes the target word , and w t+j is the context word ; v w and v w are the input and output vector representations of word w , and W is the vocabulary size . The overall objective function of the joint learning of word and entity embeddings is : J = J E + J W ( 7 )", "entities": [[1, 3, "TaskName", "relation extraction"], [19, 21, "TaskName", "Machine Translation"], [96, 98, "TaskName", "entity embeddings"], [111, 113, "TaskName", "entity disambiguation"], [155, 157, "TaskName", "word embeddings"], [158, 159, "MethodName", "TransE"], [169, 171, "TaskName", "entity embeddings"], [213, 215, "TaskName", "entity embeddings"], [235, 236, "DatasetName", "0"], [237, 238, "HyperparameterName", "\u03b3"], [318, 319, "HyperparameterName", "\u03b3"], [409, 411, "TaskName", "entity embeddings"], [420, 422, "TaskName", "word embeddings"], [517, 519, "TaskName", "word embeddings"], [567, 569, "TaskName", "word embeddings"], [590, 591, "DatasetName", "0"], [690, 692, "TaskName", "entity embeddings"]]}
{"text": "Our proposed relation extraction model integrates the extraction and canonicalization tasks for KB enrichment in an end - to - end manner . To build such a model , we employ an encoder - decoder model ( Cho et al , 2014 ) to translate a sentence into a sequence of triples . The encoder encodes a sentence into a vector that is used by the decoder as a context to generate a sequence of triples . Because we treat the input and output as a sequence , We use the LSTM networks ( Hochreiter and Schmidhuber , 1997 ) in the encoder and the decoder . The encoder - decoder with attention model ( Bahdanau et al , 2015 ) has been used in machine translation . However , in the relation extraction task , the attention model can not capture the multiword entity names . In our preliminary investigation , we found that the attention model yields misalignment between the word and the entity . The above problem is due to the same words in the names of different entities ( e.g. , the word University in different university names such as New York University , Washington University , etc . ) . During training , the model pays more attention to the word University to differentiate different types of entities of a similar name , e.g. , New York University , New York Times Building , or New York Life Building , but not the same types of entities of different names ( e.g. , New York University and Washington University ) . This may cause errors in entity alignment , especially when predicting the ID of an entity that is not in the training data . Even though we add Entity - name , Entity - ID pairs as training data ( see the Training section ) , the misalignments still take place . We address the above problem by proposing an n - gram based attention model . This model computes the attention of all possible n - grams of the sentence input . The attention weights are computed over the n - gram combinations of the word embeddings , and hence the context vector for the decoder is computed as follows . c d t = \uf8ee \uf8f0 h e ; | N | n=1 W n \uf8eb \uf8ed | X n | i=1 \u03b1 n i x n i \uf8f6 \uf8f8 \uf8f9 \uf8fb ( 8 ) \u03b1 n i = exp ( h e V n x n i ) | X n | j=1 exp ( h e V n x n j ) ( 9 ) Here , c d t is the context vector of the decoder at timestep t , h e is the last hidden state of the encoder , the superscript n indicates the n - gram combination , x is the word embeddings of input sentence , | X n | is the total number of n - gram token combination , N indicates the maximum value of n used in the n - gram combinations ( N = 3 in our experiments ) , W and V are learned parameter matrices , and \u03b1 is the attention weight .", "entities": [[2, 4, "TaskName", "relation extraction"], [91, 92, "MethodName", "LSTM"], [125, 127, "TaskName", "machine translation"], [132, 134, "TaskName", "relation extraction"], [271, 273, "TaskName", "entity alignment"], [362, 364, "TaskName", "word embeddings"], [400, 401, "HyperparameterName", "\u03b1"], [413, 414, "HyperparameterName", "\u03b1"], [485, 487, "TaskName", "word embeddings"], [538, 539, "HyperparameterName", "\u03b1"]]}
{"text": "The output of the encoder - decoder model is a sequence of the entity and predicate IDs where every three tokens indicate a triple . Therefore , to extract a triple , we simply group every three tokens of the generated output . However , the greedy approach ( i.e. , picking the entity with the highest probability of the last softmax layer of the decoder ) may lead the model to extract incorrect entities due to the similarity between entity embeddings ( e.g. , the embeddings of New York City and Chicago may be similar because both are cities in USA ) . To address this problem , we propose two strategies : re - ranking the predicted entities using a modified beam search and filtering invalid triples using a triple classifier . The modified beam search re - ranks top - k ( k = 10 in our experiments ) entity IDs that are predicted by the decoder by computing the edit distance between the entity names ( obtained from the KB ) and every n - gram token of the input sentence . The intuition is that the entity name should be mentioned in the sentence so that the entity with the highest similarity will be chosen as the output . Our triple classifier is trained with entity embeddings from the joint learning ( see Section 3.3 ) . Triple classification is one of the metrics to evaluate the quality of entity embeddings ( Socher et al , 2013 ) . We build a classifier to determine the validity of a triple h , r , t . We train a binary classifier based on the plausibility score ( h + r \u2212 t ) ( the score to compute the entity embeddings ) . We create negative samples by corrupting the valid triples ( i.e. , replacing the head or tail entity by a random entity ) . The triple classifier is effective to filter invalid triple such as New York University , capital of , Manhattan .", "entities": [[61, 62, "MethodName", "softmax"], [80, 82, "TaskName", "entity embeddings"], [145, 147, "HyperparameterName", "k ="], [220, 222, "TaskName", "entity embeddings"], [232, 234, "TaskName", "Triple classification"], [244, 246, "TaskName", "entity embeddings"], [294, 296, "TaskName", "entity embeddings"]]}
{"text": "We evaluate our model on two real datasets including WIKI and GEO test datasets ( see Section 3.2 ) . We use precision , recall , and F1 score as the evaluation metrics .", "entities": [[27, 29, "MetricName", "F1 score"]]}
{"text": "We use grid search to find the best hyperparameters for the networks . We use 512 hidden units for both the encoder and the decoder . We use 64 dimensions of pre - trained word and entity embeddings ( see Section 3.3 ) . We use a 0.5 dropout rate for regularization on both the encoder and the decoder . We use Adam ( Kingma and Ba , 2015 ) with a learning rate of 0.0002 .", "entities": [[36, 38, "TaskName", "entity embeddings"], [62, 63, "MethodName", "Adam"], [72, 74, "HyperparameterName", "learning rate"]]}
{"text": "Table 3 shows that the end - to - end models outperform the existing model . In particular , our proposed n - gram attention model achieves the best results in terms of precision , recall , and F1 score . Our proposed model outperforms the best existing model ( MinIE ) by 33.39 % and 34.78 % in terms of F1 score on the WIKI and GEO test dataset respectively . These results are expected since the existing models are affected by the error propagation of the NED . As expected , the combination of the existing models with AIDA achieves higher F1 scores than the combination with NeuralEL as AIDA achieves a higher precision than NeuralEL . To further show the effect of error propagation , we set up an experiment without the canonicalization task ( i.e. , the objective is predicting a relationship between known entities ) . We remove the NED pre - processing step by allowing the CNN model to access the correct entities . Meanwhile , we provide the correct entities to the decoder of our proposed model . In this setup , our proposed model achieves 86.34 % and 79.11 % , while CNN achieves 81.92 % and 75.82 % in precision over the WIKI and GEO test datasets , respectively . Our proposed n - gram attention model outperforms the end - to - end models by 15.51 % and 8.38 % in terms of F1 score on the WIKI and GEO test datasets , respectively . The Transformer model also only yields similar performance to that of the Single Attention model , which is worse than ours . These results indicate that our model captures multi - word entity name ( in both datasets , 82.9 % of the entities have multi - word entity name ) in the input sentence better than the other models . Table 3 also shows that the pre - trained embeddings improve the performance of the model in all measures . Moreover , the pre - trained embeddings help the model to converge faster . In our experiments , the models that use the pre - trained embeddings converge in 20 epochs on average , while the models that do not use the pre - trained embeddings converge in 30 \u2212 40 epochs . Our triple classifier combined with the modified beam search boost the performance of the model . The modified beam search provides a high recall by extracting the correct entities based on the surface form in the input sentence while the triple classifier provides a high precision by filtering the invalid triples .", "entities": [[38, 40, "MetricName", "F1 score"], [61, 63, "MetricName", "F1 score"], [103, 104, "MetricName", "F1"], [243, 245, "MetricName", "F1 score"], [256, 257, "MethodName", "Transformer"]]}
{"text": "We proposed an end - to - end relation extraction model for KB enrichment that integrates the extraction and canonicalization tasks . Our model thus reduces the error propagation between relation extraction and NED that existing approaches are prone to . To obtain high - quality training data , we adapt distant supervision and augment it with co - reference resolution and paraphrase detection . We propose an n - gram based attention model that better captures the multi - word entity names in a sentence . Moreover , we propose a modified beam search and a triple classification that helps the model to generate high - quality triples . Experimental results show that our proposed model outperforms the existing models by 33.39 % and 34.78 % in terms of F1 score on the WIKI and GEO test dataset respectively . These results confirm that our model reduces the error propagation between NED and relation extraction . Our proposed n - gram attention model outperforms the other encoder - decoder models by 15.51 % and 8.38 % in terms of F1 score on the two real - world datasets . These results confirm that our model better captures the multi - word entity names in a sentence . In the future , we plan to explore contextbased similarity to complement the lexical similarity to improve the overall performance .", "entities": [[8, 10, "TaskName", "relation extraction"], [30, 32, "TaskName", "relation extraction"], [97, 99, "TaskName", "triple classification"], [130, 132, "MetricName", "F1 score"], [154, 156, "TaskName", "relation extraction"], [180, 182, "MetricName", "F1 score"]]}
{"text": "NELL - 995 ( Xiong et al , 2017 ) was taken from the Never Ending Language Learner ( NELL ) sys - tem ( Mitchell et al , 2018 ) , which continuously reads the web to obtain and update its knowledge . NELL - 995 , a subset of the 995th iteration of NELL , contains 75 , 492 entities , 200 relations , and 154 , 213 triples . While NELL - 995 is general and covers many domains , its mean average precision was less than 50 % around its 1000th iteration ( Mitchell et al , 2018 ) . A cursory inspection reveals that many of the triples in NELL - 995 are nonsensical or overly generic , suggesting that NELL - 995 is not a meaningful dataset for KGC evaluation . 1 YAGO3 - 10 ( Dettmers et al , 2018 ) is a subset of YAGO3 ( Mahdisoltani et al , 2014 ) , which covers portions of Wikipedia , Wikidata , and Word - Net . YAGO3 - 10 has 123 , 182 entities , 37 relations , and 1 , 089 , 040 triples mostly limited to facts about people and locations . While YAGO3 - 10 is a highprecision dataset , it was recently shown to be too easy for link prediction because it contains a large proportion of duplicate relations ( Akrami et al , 2020 ; Pezeshkpour et al , 2020 ) .", "entities": [[0, 1, "DatasetName", "NELL"], [19, 20, "DatasetName", "NELL"], [44, 45, "DatasetName", "NELL"], [55, 56, "DatasetName", "NELL"], [73, 74, "DatasetName", "NELL"], [85, 87, "MetricName", "average precision"], [114, 115, "DatasetName", "NELL"], [125, 126, "DatasetName", "NELL"], [138, 141, "DatasetName", "YAGO3 - 10"], [174, 177, "DatasetName", "YAGO3 - 10"], [203, 206, "DatasetName", "YAGO3 - 10"], [220, 222, "TaskName", "link prediction"]]}
{"text": "To create smaller data snapshots , we filtered the initial 1.15 million triples to k - cores , which are maximal subgraphs G of a given graph G such that every node in G has a degree of at least k ( Batagelj and Zaver\u0161nik , 2011 ) . 2 We constructed three CODEX datasets ( Table 2 ) : CODEX - S ( k = 15 ) , which has 36k triples . Because of its smaller size , we recommend that CODEX - S be used for model testing and debugging , as well as evaluation of methods that are less computationally efficient ( e.g. , symbolic search - based approaches ) . CODEX - M ( k = 10 ) , which has 206k triples . CODEX - M is all - purpose , being comparable in size to FB15 K - 237 ( 2.1 ) , one of the most popular benchmarks for KGC evaluation . CODEX - L ( k = 5 ) , which has 612k triples . CODEX - L is comparable in size to FB15 K ( 2.1 ) , and can be used for both general evaluation and \" few - shot \" evaluation . We also release the raw dump that we collected via snowball sampling , but focus on CODEX - S through L for the remainder of this paper . To minimize train / test leakage , we removed inverse relations from each dataset . We computed ( head , tail ) and ( tail , head ) overlap between all pairs of relations , and removed each relation whose entity pair set overlapped with that of another relation more than 50 % of the time . Finally , we split each dataset into 90/5/5 train / validation / test triples such that the validation and test sets contained only entities and relations seen in the respective training sets .", "entities": [[64, 66, "HyperparameterName", "k ="], [119, 121, "HyperparameterName", "k ="], [164, 166, "HyperparameterName", "k ="]]}
{"text": "Link prediction The link prediction task is conducted as follows : Given a test triple ( h , r , t ) , we construct queries ( ? , r , t ) and ( h , r , ? ) . For each query , a model scores candidate head ( tail ) entitie\u015d h ( t ) according to its belief that\u0125 ( t ) completes the triple ( i.e. , answers the query ) . The goal is of link prediction is to rank true triples ( \u0125 , r , t ) or ( h , r , t ) higher than false and unseen triples . Link prediction performance is evaluated with mean reciprocal rank ( MRR ) and hits@k . MRR is the average reciprocal of each ground - truth entity 's rank over all ( ? , r , t ) and ( h , r , ? ) test triples . Hits@k measures the proportion of test triples for which the ground - truth entity is ranked in the top - k predicted entities . In computing these metrics , we exclude the predicted entities for which ( \u0125 , r , t ) G or ( h , r , t ) G so that known positive triples do not artificially lower ranking scores . This is called \" filtering \" ( Bordes et al , 2013 ) . Triple classification Given a triple ( h , r , t ) , the goal of triple classification is to predict a corresponding label y { \u22121 , 1 } . Since knowledge graph embedding models output real - valued scores for triples , we convert these scores into labels by selecting a decision threshold per relation on the validation set such that validation accuracy is maximized for the model in question . A similar approach was used by Socher et al ( 2013 ) . We compare results on three sets of evaluation negatives : ( 1 ) We generate one negative per positive by replacing the positive triple 's tail entity by a tail entity t sampled uniformly at random ; ( 2 ) We generate negatives by sampling tail entities according to their relative frequency in the tail slot of all triples ; and ( 3 ) We use the CODEX hard negatives . We measure accuracy and F1 score .", "entities": [[0, 2, "TaskName", "Link prediction"], [3, 5, "TaskName", "link prediction"], [82, 84, "TaskName", "link prediction"], [111, 113, "TaskName", "Link prediction"], [121, 122, "MetricName", "MRR"], [126, 127, "MetricName", "MRR"], [238, 240, "TaskName", "Triple classification"], [254, 256, "TaskName", "triple classification"], [270, 273, "TaskName", "knowledge graph embedding"], [302, 303, "MetricName", "accuracy"], [397, 398, "MetricName", "accuracy"], [399, 401, "MetricName", "F1 score"]]}
{"text": "As recent studies have observed that training strategies are equally , if not more , important than architecture for link prediction ( Kadlec et al , 2017 ; Lacroix et al , 2018 ; Ruffinelli et al , 2020 ) , we search across a large range of hyperparameters to ensure a truly fair comparison . To this end we use the PyTorch - based LibKGE framework for training and selecting knowledge graph embeddings . 4 In the remainder of this section we outline the most important parameters of our model selection process . Training negatives Given a set of positive training triples { ( h , r , t ) } , we compare three types of negative sampling strategy implemented by LibKGE : ( a ) NegSamp , or randomly corrupting head entities h or tail entities t to create negatives ; ( b ) 1vsAll , or treating all possible head / tail corruptions of ( h , r , t ) as negatives , including the corruptions that are actually positives ; and ( c ) KvsAll , or treating batches of head / tail corruptions not seen in the knowledge graph as negatives . Loss functions We consider the following loss functions : ( i ) MR or margin ranking , which aims to maximize a margin between positive and negative triples ; ( ii ) BCE or binary cross - entropy , which is computed by applying the logistic sigmoid to triple scores ; and ( iii ) CE or cross - entropy between the softmax over the entire distribution of triple scores and the label distribution over all triples , normalized to sum to one . Search strategies We select models using the Ax platform , which supports hyperparameter search using both quasi - random sequences of generated configurations and Bayesian optimization ( BO ) with Gaussian processes . 5 At a high level , for each dataset and model , we generate both quasirandom and BO trials per negative sampling and loss function combination , ensuring that we search over a wide range of hyperparameters for different types of training strategy . Appendix F provides specific details on the search strategy for each dataset , which was determined according to resource constraints and observed performance patterns .", "entities": [[19, 21, "TaskName", "link prediction"], [71, 74, "TaskName", "knowledge graph embeddings"], [90, 92, "TaskName", "model selection"], [205, 206, "MetricName", "loss"], [211, 212, "DatasetName", "MR"], [261, 262, "MethodName", "softmax"], [313, 315, "TaskName", "Gaussian processes"], [339, 340, "MetricName", "loss"]]}
{"text": "Table 5 gives link prediction results . We find that ComplEx is the best at modeling symmetry and antisymmetry , and indeed it was designed specifically to improve upon bilinear models that do not capture symmetry , like DistMult ( Trouillon et al , 2016 ) . As such , it performs the best on CODEX - S , which has the highest proportion of symmetric relations . For example , on the most frequent symmetric relation ( diplomatic relation ) , ComplEx achieves 0.859 MRR , compared to 0.793 for ConvE , 0.490 for RESCAL , and 0.281 for TransE. By contrast , TuckER is strongest at modeling compositional relations , so it performs best on CODEX - L , which has a high degree of compositionality . For example , on the most frequent compositional relation in CODEX - L ( languages spoken , written , or signed ) , TuckER achieves 0.465 MRR , compared to 0.464 for RESCAL , 0.463 for ConvE , 0.456 for ComplEx , and 0.385 for TransE. By contrast , since CODEX - M is mostly asymmetric and non - compositional , ComplEx performs best because of its ability to model asymmetry . Effect of hyperparameters As shown by Figure 1 , hyperparameters have a strong impact on link prediction performance : Validation MRR for all models varies by over 30 percentage points depending on the training strategy and input configuration . This finding is consistent with previous observations in the literature ( Kadlec et al , 2017 ; Ruffinelli et al , 2020 ) . Appendix F provides the best configurations for each model . Overall , we find that the choice of loss function in particular significantly impacts model performance . Each model consistently achieved its respective peak performance with cross - entropy ( CE ) loss , a finding which is corroborated by several other KGC comparison papers ( Kadlec et al , 2017 ; Ruffinelli et al , 2020 ; Jain et al , 2020 ) . As far as negative sampling techniques , we do not find that a single strategy is dominant , suggesting that the choice of loss function is more important .", "entities": [[3, 5, "TaskName", "link prediction"], [85, 86, "MetricName", "MRR"], [95, 96, "MethodName", "RESCAL"], [104, 105, "MethodName", "TuckER"], [152, 153, "MethodName", "TuckER"], [155, 156, "MetricName", "MRR"], [161, 162, "MethodName", "RESCAL"], [216, 218, "TaskName", "link prediction"], [221, 222, "MetricName", "MRR"], [282, 283, "MetricName", "loss"], [306, 307, "MetricName", "loss"], [362, 363, "MetricName", "loss"]]}
{"text": "Table 6 gives triple classification results . Evidently , triple classification on randomly generated negatives is a nearly - solved task . On negatives generated uniformly at random , performance scores are nearly identical at almost 100 % accuracy . Even with a negative sampling strategy \" smarter \" than uniform random , all models perform well . Hard negatives Classification performance degenerates considerably on our hard negatives , around 8 to 11 percentage points from relative frequency - based sampling and 13 to 19 percentage points from uniformly random sampling . Relative performance also varies : In contrast to our link prediction task in which ComplEx and TuckER were by far the strongest models , RESCAL is slightly stronger on the CODEX - S hard negatives , whereas ConvE performs best on the CODEX - M hard negatives . These results indicate that triple classification is indeed a distinct task that requires different architectures and , in many cases , different training strategies ( Appendix F ) . We believe that few recent works use triple classification as an evaluation task because of the lack of true hard negatives in existing benchmarks . Early works reported high triple classification accuracy on sampled negatives ( Socher et al , 2013 ; Wang et al , 2014 ) , perhaps leading the community to believe that the task was nearly solved . However , our results demonstrate that the task is far from solved when the negatives are plausible but truly false .", "entities": [[3, 5, "TaskName", "triple classification"], [9, 11, "TaskName", "triple classification"], [38, 39, "MetricName", "accuracy"], [60, 61, "TaskName", "Classification"], [101, 103, "TaskName", "link prediction"], [108, 109, "MethodName", "TuckER"], [116, 117, "MethodName", "RESCAL"], [144, 146, "TaskName", "triple classification"], [176, 178, "TaskName", "triple classification"], [198, 200, "TaskName", "triple classification"], [200, 201, "MetricName", "accuracy"]]}
{"text": "Next , we compare the datasets in a link prediction task to show that CODEX - M is more difficult . Baseline We devise a \" non - learning \" link prediction baseline . Let ( h , r , ? ) be a test query . Our baseline scores candidate tail entities by their relative frequency in the tail slot of all training triples mentioning r , filtering out tail entities t for which ( h , r , t ) is already observed in the training set . If all tail entities t are filtered out , we score entities by frequency before filtering . The logic of our approach works in reverse for ( ? , r , t ) queries . In evaluating our baseline , we follow LibKGE 's protocol for breaking ties in ranking ( i.e. , for entities that appear with equal frequency ) by taking the mean rank of all entities with the same score . Setup We compare our baseline to the best pretrained embedding model per dataset : RESCAL for FB15 K - 237 , which was released by Ruffinelli et al ( 2020 ) , and ComplEx for CODEX - M. We evaluate performance with MRR and Hits@10 . Beyond overall performance , we also compute per - relation improvement of the respective embedding over our baseline in terms of percentage points MRR . This measures the amount of learning beyond frequency statistics necessary for each relation . Results and discussion Table 7 compares the overall performance of our baseline versus the best embedding per dataset , and Figure 3 shows the improvement of the respective embedding over our baseline per relation type on each dataset . The improvement of the embedding is much smaller on FB15 K - 237 than CODEX - M , and in fact our baseline performs on par with or even outperforms the embedding on FB15 K - 237 for some relation types . To further explore these cases , Figure 4 gives the empirical cumulative distribution function of improvement , which shows the percentage of test triples for which the level of improvement is less than or equal to a given value on each dataset . Surprisingly , the improvement for both MRR and Hits@10 is less than five percentage points for nearly 40 % of FB15 K - 237 's test set , and is zero or negative 15 % of the time . By contrast , our baseline is significantly weaker than the strongest embedding method on CODEX - M. The disparity in improvement is due to two relation patterns prevalent in FB15 K - 237 : Skewed relations FB15 K - 237 contains many relations that are skewed toward a single head or tail entity . For example , our baseline achieves perfect performance over all ( h , r , ? ) We conclude that while FB15 K - 237 is a valuable dataset , CODEX is more appropriately difficult for link prediction . Additionally , we note that in FB15 K - 237 , all validation and test triples containing entity pairs directly linked in the training set were deleted , meaning that symmetry can not be tested for in FB15 K - 237 . Given that CODEX datasets contain both symmetry and compositionality , CODEX is more suitable for assessing how well models can learn relation patterns that go beyond frequency . queries", "entities": [[8, 10, "TaskName", "link prediction"], [30, 32, "TaskName", "link prediction"], [178, 179, "MethodName", "RESCAL"], [206, 207, "MetricName", "MRR"], [208, 209, "MetricName", "Hits@10"], [233, 234, "MetricName", "MRR"], [379, 380, "MetricName", "MRR"], [381, 382, "MetricName", "Hits@10"], [502, 504, "TaskName", "link prediction"]]}
{"text": "Table 10 gives our hyperparameter search space . Tables 11 , 12 , and 13 report the best hyperparameter configurations for link prediction on CODEX - S , CODEX - M , and CODEX - L , respectively . Tables 14 and 15 report the best hyperparameter configurations for triple classification on the hard negatives in CODEX - S and CODEX - M , respectively . Terminology For embedding initialization , Xv refers to Xavier initialization ( Glorot and Bengio , 2010 ) . The reciprocal relations model refers to learning separate relation embeddings for queries in the direction of ( h , r , ? ) versus ( ? , r , t ) ( Kazemi and Poole , 2018 ) . The frequency weighting regularization technique refers to regularizing embeddings by the relative frequency of the corresponding entity or relation in the training data . Search strategies Recall that we select models using Ax , which supports hyperparameter search using both quasi - random sequences of generated configurations and Bayesian optimization ( BO ) . The search strategy for each CODEX dataset is as follows : CODEX - S : Per negative sampling type / loss combination , we generate 30 quasi - random trials followed by 10 BO trials . We select the best - performing model by validation MRR over all such combinations . In each trial , the model is trained for a maximum of 400 epochs with an early stopping patience of 5 . We also terminate a trial after 50 epochs if the model does not reach \u2265 0.05 MRR . CODEX - M : Per negative sampling type / loss combination , we generate 20 quasi - random trials . The maximum number of epochs and early stopping criteria are the same as for CODEX - S. CODEX - L : Per negative sampling type / loss combination , we generate 10 quasi - random trials of 20 training epochs instead of 400 . We reduce the number of epochs to limit resource usage . In most cases , MRR plateaus after 20 - 30 epochs , an observation which is consistent with ( Ruffinelli et al , 2020 ) . Then , we take the best - performing model by validation MRR over all such combinations , and retrain that model for a maximum of 400 epochs . Note that we search using MRR as our metric , but the triple classification task measures 0/1 accuracy , not ranking performance . For triple classification , we choose the model with the highest validation accuracy among the pre - trained models across all negative sampling type / loss function combinations . We release all pretrained LibKGE models and accompanying configuration files in the centralized CODEX repository .", "entities": [[21, 23, "TaskName", "link prediction"], [49, 51, "TaskName", "triple classification"], [74, 76, "MethodName", "Xavier initialization"], [149, 150, "MetricName", "Recall"], [197, 198, "MetricName", "loss"], [222, 223, "MetricName", "MRR"], [244, 246, "MethodName", "early stopping"], [266, 267, "MetricName", "MRR"], [277, 278, "MetricName", "loss"], [290, 293, "HyperparameterName", "number of epochs"], [294, 296, "MethodName", "early stopping"], [314, 315, "MetricName", "loss"], [335, 338, "HyperparameterName", "number of epochs"], [347, 348, "MetricName", "MRR"], [380, 381, "MetricName", "MRR"], [402, 403, "MetricName", "MRR"], [409, 411, "TaskName", "triple classification"], [414, 415, "MetricName", "accuracy"], [421, 423, "TaskName", "triple classification"], [432, 433, "MetricName", "accuracy"], [445, 446, "MetricName", "loss"]]}
