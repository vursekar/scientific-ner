
All	O
models	O
are	O
trained	O
with	O
the	O
following	O
hyperparameters	O
:	O
both	O
encoder	O
and	O
decoder	O
are	O
set	O
to	O
one	O
layer	O
with	O
GRU	O
cells	O
,	O
where	O
the	O
hidden	B-HyperparameterName
state	I-HyperparameterName
size	I-HyperparameterName
of	O
GRU	O
is	O
256	B-HyperparameterValue
;	O
the	O
utterance	B-HyperparameterName
length	I-HyperparameterName
is	O
limited	O
to	O
50	B-HyperparameterValue
;	O
the	O
vocabulary	B-HyperparameterName
size	I-HyperparameterName
is	O
50	B-HyperparameterValue
,	I-HyperparameterValue
000	I-HyperparameterValue
and	O
the	O
word	B-HyperparameterName
embedding	I-HyperparameterName
dimension	I-HyperparameterName
is	O
256	B-HyperparameterValue
;	O
the	O
word	O
embeddings	O
are	O
shared	O
by	O
the	O
encoder	O
and	O
decoder	O
;	O
all	O
trainable	O
parameters	O
are	O
initialized	O
from	O
a	B-HyperparameterValue
uniform	I-HyperparameterValue
distribution	I-HyperparameterValue
[	O
-	O
0.08	O
,	O
0.08	O
]	O
;	O
we	O
employ	O
the	O
Adam	B-HyperparameterValue
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
for	O
optimization	O
with	O
a	O
mini	B-HyperparameterName
-	I-HyperparameterName
batch	I-HyperparameterName
size	I-HyperparameterName
128	B-HyperparameterValue
and	O
initialized	O
learning	B-HyperparameterName
rate	I-HyperparameterName
0.001	B-HyperparameterValue
;	O
the	O
gradient	O
clipping	O
strategy	O
is	O
utilized	O
to	O
avoid	O
gradient	O
explosion	O
,	O
where	O
the	B-HyperparameterName
gradient	I-HyperparameterName
clipping	I-HyperparameterName
value	I-HyperparameterName
is	O
set	O
to	O
be	O
5	B-HyperparameterValue
.	O
For	O
the	O
latent	O
variable	O
,	O
we	O
adopt	O
dimensional	B-HyperparameterName
size	I-HyperparameterName
256	B-HyperparameterValue
and	O
the	O
component	B-HyperparameterName
number	I-HyperparameterName
of	O
the	O
mixture	O
Gaussian	O
for	O
prior	O
networks	O
in	O
WAE	B-MethodName
is	O
set	O
to	O
5	B-HyperparameterValue
.	O
As	O
to	O
the	O
discriminator	O
,	O
we	O
set	O
the	O
initialized	O
learning	B-HyperparameterName
rate	I-HyperparameterName
as	O
0.0002	B-HyperparameterValue
and	O
use	O
128	B-HyperparameterValue
different	O
kernels	O
for	O
each	B-HyperparameterName
kernel	I-HyperparameterName
size	I-HyperparameterName
in	O
{	O
2	B-HyperparameterValue
,	I-HyperparameterValue
3	I-HyperparameterValue
,	I-HyperparameterValue
4	I-HyperparameterValue
}	O
.	O
The	O
size	B-HyperparameterName
of	I-HyperparameterName
the	I-HyperparameterName
response	I-HyperparameterName
bag	I-HyperparameterName
is	O
limited	O
to	O
10	B-HyperparameterValue
where	O
the	O
instances	O
inside	O
are	O
randomly	O
sampled	O
for	O
each	O
mini	O
-	O
batch	O
.	O
All	O
the	O
models	O
are	O
implemented	O
with	O
Pytorch	O
0.4.1	O
4	O
.	O

To	O
comprehensively	O
evaluate	O
the	O
quality	O
of	O
generated	O
response	O
utterances	O
,	O
we	O
adopt	O
both	O
automatic	O
and	O
human	O
evaluation	O
metrics	O
:	O
BLEU	B-MetricName
:	O
In	O
dialogue	O
generation	O
,	O
BLEU	B-MetricName
is	O
widely	O
used	O
in	O
previous	O
studies	O
(	O
Yao	O
et	O
al	O
,	O
2017	O
;	O
Shang	O
et	O
al	O
,	O
2018	O
)	O
.	O
Since	O
multiple	O
valid	O
responses	O
exist	O
in	O
this	O
paper	O
,	O
we	O
adopt	O
multi	O
-	O
reference	O
BLEU	B-MetricName
where	O
the	O
evaluated	O
utterance	O
is	O
compared	O
to	O
provided	O
multiple	O
references	O
simultaneously	O
.	O
Distinctness	O
:	O
To	O
distinguish	O
safe	O
and	O
commonplace	O
responses	O
,	O
the	O
distinctness	O
score	O
(	O
Li	O
et	O
al	O
,	O
2016a	O
)	O
is	O
designed	O
to	O
measure	O
word	O
-	O
level	O
diversity	O
by	O
counting	O
the	O
ratio	O
of	O
distinctive	O
[	O
1	O
,	O
2	O
]	O
-	O
grams	O
.	O
In	O
our	O
experiments	O
,	O
we	O
adopt	O
both	O
Intra	O
-	O
Dist	O
:	O
the	O
distinctness	O
scores	O
of	O
multiple	O
responses	O
for	O
a	O
given	O
query	O
and	O
Inter	O
-	O
Dist	O
:	O
the	O
distinctness	O
scores	O
of	O
generated	O
responses	O
of	O
the	O
whole	O
testing	O
set	O
.	O
Embedding	O
Similarity	O
:	O
Embedding	O
-	O
based	O
metrics	O
compute	O
the	O
cosine	O
similarity	O
between	O
the	O
sentence	O
embedding	O
of	O
a	O
ground	O
-	O
truth	O
response	O
and	O
that	O
of	O
the	O
generated	O
one	O
.	O
There	O
are	O
various	O
ways	O
to	O
obtain	O
the	O
sentence	O
-	O
level	O
embedding	O
from	O
the	O
constituent	O
word	O
embeddings	O
.	O
In	O
our	O
experiments	O
,	O
we	O
apply	O
three	O
most	O
commonly	O
used	O
strategies	O
:	O
Greedy	O
matches	O
each	O
word	O
of	O
the	O
reference	O
with	O
the	O
most	O
similar	O
word	O
in	O
the	O
evaluated	O
sentence	O
;	O
Average	O
uses	O
the	O
average	O
of	O
word	O
embed	O
-	O
Input	O
火山喷发瞬间的一些壮观景象	O
。	O
再过十分钟就进入win8时代，我是系统升级控	O
。	O
Query	O
These	O
are	O
some	O
magnificent	O
sights	O
at	O
the	O
moment	O
of	O
the	O
volcanic	O
eruption	O
.	O
There	O
remain	O
ten	O
minutes	O
before	O
we	O
entering	O
the	O
era	O
of	O
win8	O
.	O
I	O
am	O
a	O
geek	O
of	O
system	O
updating	O
.	O
What	O
application	O
is	O
this	O
.	O
如此这般这般淼小	O
。	O
我觉得这样的界面更像windows8	O
。	O
It	O
is	O
so	O
so	O
imperceptible	O
.	O
I	O
think	O
interface	O
like	O
this	O
looks	O
more	O
like	O
windows8	O
.	O
Table	O
3	O
:	O
Case	O
study	O
for	O
the	O
generated	O
responses	O
from	O
the	O
testing	O
set	O
of	O
Weibo	O
,	O
where	O
the	O
Chinese	O
utterances	O
are	O
translated	O
into	O
English	O
for	O
the	O
sake	O
of	O
readability	O
.	O
For	O
each	O
input	O
query	O
,	O
we	O
show	O
four	O
responses	O
generated	O
by	O
each	O
method	O
and	O
an	O
additional	O
intermediate	O
utterance	O
(	O
marked	O
with	O
underline	O
)	O
for	O
our	O
model	O
.	O
dings	O
;	O
and	O
Extreme	O
takes	O
the	O
most	O
extreme	O
value	O
among	O
all	O
words	O
for	O
each	O
dimension	O
of	O
word	O
embeddings	O
in	O
a	O
sentence	O
.	O
Since	O
multiple	O
references	O
exist	O
,	O
for	O
each	O
utterance	O
to	O
be	O
evaluated	O
,	O
we	O
compute	O
its	O
score	O
with	O
the	O
most	O
similar	O
reference	O
.	O
Human	O
Evaluation	O
with	O
Case	O
Analysis	O
:	O
As	O
automatic	O
evaluation	O
metrics	O
lose	O
sight	O
of	O
the	O
overall	O
quality	O
of	O
a	O
response	O
(	O
Tao	O
et	O
al	O
,	O
2018	O
)	O
,	O
we	O
also	O
adopt	O
human	O
evaluation	O
on	O
100	O
random	O
samples	O
to	O
assess	O
the	O
generation	O
quality	O
with	O
three	O
independent	O
aspects	O
considered	O
:	O
relevance	O
(	O
whether	O
the	O
reply	O
is	O
relevant	O
to	O
the	O
query	O
)	O
,	O
diversity	O
(	O
whether	O
the	O
reply	O
narrates	O
with	O
diverse	O
words	O
)	O
and	O
readability	O
(	O
whether	O
the	O
utterance	O
is	O
grammatically	O
formed	O
)	O
.	O
Each	O
property	O
is	O
assessed	O
with	O
a	O
score	O
from	O
1	O
(	O
worst	O
)	O
to	O
5	O
(	O
best	O
)	O
by	O
three	O
annotators	O
.	O
The	O
evaluation	O
is	O
conducted	O
in	O
a	O
blind	O
process	O
with	O
the	O
utterance	O
belonging	O
unknown	O
to	O
the	O
reviewers	O
.	O

We	O
compare	O
our	O
model	O
with	O
representative	O
dialogue	B-TaskName
generation	I-TaskName
approaches	O
as	O
listed	O
below	O
:	O
S2S	O
:	O
the	O
vanilla	O
sequence	O
-	O
to	O
-	O
sequence	O
model	O
with	O
attention	O
mechanism	O
where	O
standard	O
beam	O
search	O
is	O
applied	O
in	O
testing	O
to	O
generate	O
multiple	O
different	O
responses	O
.	O
Method	O
Multi	B-MetricName
-	I-MetricName
BLEU	I-MetricName
EMBEDDING	I-MetricName
Intra	I-MetricName
-	I-MetricName
Dist	I-MetricName
Inter	I-MetricName
-	O
Dist	B-MetricName
BLEU	I-MetricName
-	O
1	O
BLEU	O
-	O
2	O
G	B-MetricName
A	I-MetricName
E	I-MetricName
Dist	I-MetricName
-	O
1	O
Dist	O
-	O
2	O
S2S+DB	B-MetricName
:	O
the	O
vanilla	O
sequence	O
-	O
to	O
-	O
sequence	O
model	O
with	O
the	O
modified	O
diversity	O
-	O
promoting	O
beam	B-TaskName
search	I-TaskName
method	I-TaskName
(	O
Li	O
et	O
al	O
,	O
2016b	O
)	O
where	O
a	O
fixed	O
diversity	B-HyperparameterName
rate	I-HyperparameterName
0.5	B-HyperparameterValue
is	O
used	O
.	O
MMS	O
:	O
the	O
modified	O
multiple	O
responding	O
mechanisms	O
enhanced	O
dialogue	O
model	O
proposed	O
by	O
Zhou	O
et	O
al	O
(	O
2018a	O
)	O
which	O
introduces	O
responding	O
mechanism	O
embeddings	O
for	O
diverse	O
response	O
generation	O
.	O
CVAE	B-MethodName
:	O
the	O
vanilla	O
CVAE	B-MethodName
model	O
with	O
and	O
without	O
BOW	B-MetricName
(	I-MetricName
bag	I-MetricName
-	I-MetricName
of	I-MetricName
-	I-MetricName
word	I-MetricName
)	I-MetricName
loss	I-MetricName
(	I-MetricName
CVAE+BOW	B-MethodName
and	O
CVAE	B-MethodName
)	O
.	O
WAE	B-MethodName
:	O
the	O
conditional	B-MethodName
Wasserstein	I-MethodName
autoencoder	I-MethodName
model	O
for	O
dialogue	B-TaskName
generation	I-TaskName
(	O
Gu	O
et	O
al	O
,	O
2018	O
)	O
which	O
models	O
the	O
distribution	O
of	O
data	O
by	O
training	O
a	O
GAN	B-MethodName
within	O
the	O
latent	O
variable	O
space	O
.	O
Ours	O
:	O
we	O
explore	O
our	O
model	O
Ours	O
and	O
conduct	O
various	O
ablation	O
studies	O
:	O
the	O
model	O
with	O
only	O
the	O
second	O
stage	O
generation	O
(	O
Ours	O
-	O
First	O
)	O
,	O
the	O
model	O
without	O
the	O
discriminator	O
(	O
Ours	O
-	O
Disc	O
)	O
and	O
multireference	B-MetricName
BOW	I-MetricName
loss	I-MetricName
(	O
Ours	O
-	O
MBOW	O
)	O
,	O
and	O
the	O
model	O
with	O
GMM	O
prior	O
networks	O
(	O
Ours+GMP	O
)	O
.	O

Our	O
whole	O
model	O
can	O
be	O
trained	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
fashion	O
.	O
To	O
train	O
the	O
model	O
,	O
we	O
first	O
pre	O
-	O
train	O
the	O
word	O
embedding	O
using	O
Glove	O
(	O
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
)	O
1	O
.	O
Then	O
modules	O
of	O
the	O
model	O
are	O
jointly	O
trained	O
by	O
optimizing	O
the	O
losses	O
L	O
f	O
irst	O
and	O
L	O
second	O
of	O
the	O
two	O
generation	O
phases	O
respectively	O
.	O
To	O
overcome	O
the	O
vanishing	O
latent	O
variable	O
problem	O
(	O
Wang	O
et	O
al	O
,	O
2017	O
)	O
of	O
CVAE	B-MethodName
,	O
we	O
adopt	O
the	O
KL	B-MethodName
annealing	I-MethodName
strategy	I-MethodName
(	O
Bowman	O
et	O
al	O
,	O
2016	O
)	O
,	O
where	O
the	O
weight	O
of	O
the	O
KL	O
term	O
is	O
gradually	O
increased	O
during	O
training	O
.	O
The	O
other	O
technique	O
employed	O
is	O
the	O
MBOW	B-MetricName
loss	O
which	O
is	O
able	O
to	O
sharpen	O
the	O
distribution	O
of	O
latent	O
variable	O
z	O
for	O
each	O
specific	O
response	O
and	O
alleviate	O
the	O
vanishing	O
problem	O
at	O
the	O
same	O
time	O
.	O
During	O
testing	O
,	O
diverse	O
responses	O
can	O
be	O
obtained	O
by	O
the	O
two	O
generation	O
phases	O
described	O
above	O
,	O
where	O
the	O
distinctive	O
latent	O
variable	O
z	O
corresponding	O
to	O
each	O
specific	O
response	O
is	O
sampled	O
from	O
the	O
prior	O
probability	O
network	O
.	O
This	O
process	O
is	O
illustrated	O
in	O
Figure	O
2	O
.	O
Capable	O
of	O
capturing	O
the	O
common	O
feature	O
of	O
the	O
response	O
bag	O
,	O
the	O
variable	O
c	O
is	O
obtained	O
from	O
the	O
mapping	O
network	O
and	O
no	O
intermediate	O
utterance	O
is	O
required	O
,	O
which	O
facilitates	O
reducing	O
the	O
complexity	O
of	O
decoding	O
.	O

The	O
second	O
generation	O
phase	O
aims	O
to	O
model	O
each	O
specific	O
response	O
in	O
a	O
response	O
bag	O
respectively	O
.	O
In	O
practice	O
,	O
we	O
adopt	O
the	O
CVAE	B-MethodName
architecture	O
,	O
while	O
two	O
prominent	O
modifications	O
remain	O
.	O
Firstly	O
,	O
rather	O
than	O
modeling	O
each	O
response	O
with	O
the	O
latent	O
variable	O
z	O
from	O
scratch	O
,	O
our	O
model	O
approximates	O
each	O
response	O
based	O
on	O
the	O
bag	O
representation	O
c	O
with	O
only	O
the	O
distinctive	O
feature	O
of	O
each	O
specific	O
response	O
remaining	O
to	O
be	O
captured	O
.	O
Secondly	O
,	O
the	O
prior	O
common	O
feature	O
c	O
can	O
provide	O
extra	O
information	O
for	O
the	O
sampling	O
network	O
which	O
is	O
supposed	O
to	O
decrease	O
the	O
latent	O
searching	O
space	O
.	O
Specifically	O
,	O
similar	O
to	O
the	O
CVAE	B-MethodName
architecture	O
,	O
the	O
overall	O
objective	O
for	O
our	O
model	O
in	O
the	O
second	O
generation	O
phase	O
is	O
as	O
below	O
:	O
L	O
cvae	O
=	O
E	O
q	O
φ	O
(	O
z	O
|	O
x	O
,	O
y	O
,	O
c	O
)	O
p	O
θ	O
(	O
c	O
|	O
x	O
)	O
[	O
log	O
p	O
ψ	O
(	O
y	O
|	O
c	O
,	O
z	O
)	O
]	O
−	O
D	O
[	O
q	O
φ	O
(	O
z	O
|	O
x	O
,	O
y	O
,	O
c	O
)	O
|	O
|	O
p	O
ϕ	O
(	O
z	O
|	O
x	O
,	O
c	O
)	O
]	O
(	O
5	O
)	O
where	O
q	O
φ	O
represents	O
the	O
recognition	O
network	O
and	O
p	O
ϕ	O
is	O
the	O
prior	O
network	O
with	O
φ	O
and	O
ϕ	O
as	O
the	O
trainable	O
parameters	O
;	O
D	O
(	O
|	O
|	O
)	O
is	O
the	O
regularization	O
term	O
which	O
measures	O
the	O
distance	O
between	O
the	O
two	O
distributions	O
.	O
In	O
practice	O
,	O
the	O
recognition	O
networks	O
are	O
implemented	O
with	O
a	O
feed	O
-	O
forward	O
network	O
that	O
µ	O
log	O
σ	O
2	O
=	O
W	O
q	O
	O
	O
h	O
x	O
h	O
y	O
c	O
	O
	O
+	O
b	O
q	O
(	O
6	O
)	O
where	O
h	O
x	O
and	O
h	O
y	O
are	O
the	O
utterance	O
representations	O
of	O
query	O
and	O
response	O
got	O
by	O
GRU	O
respectively	O
,	O
and	O
the	O
latent	O
variable	O
z	O
∼	O
N	O
(	O
µ	O
,	O
σ	O
2	O
I	O
)	O
.	O
For	O
the	O
prior	O
networks	O
,	O
we	O
consider	O
two	O
kinds	O
of	O
implements	O
.	O
One	O
is	O
the	O
vanilla	O
CVAE	B-MethodName
model	O
where	O
the	O
prior	O
p	O
ϕ	O
(	O
z	O
|	O
x	O
,	O
c	O
)	O
is	O
modeled	O
by	O
a	O
another	O
feed	O
-	O
forward	O
network	O
conditioned	O
on	O
the	O
representations	O
h	O
x	O
and	O
c	O
as	O
follows	O
,	O
µ	O
log	O
σ	O
2	O
=	O
W	O
p	O
h	O
x	O
c	O
+	O
b	O
p	O
(	O
7	O
)	O
and	O
the	O
distance	O
D	O
(	O
|	O
|	O
)	O
here	O
is	O
measured	O
by	O
the	O
KL	B-MetricName
divergence	I-MetricName
.	O
For	O
the	O
other	O
,	O
we	O
adopt	O
the	O
WAE	B-MethodName
model	O
(	O
Gu	O
et	O
al	O
,	O
2018	O
)	O
in	O
which	O
the	O
prior	O
p	O
ϕ	O
(	O
z	O
|	O
x	O
,	O
c	O
)	O
is	O
modeled	O
by	O
a	O
mixture	O
of	O
Gaussian	O
distributions	O
GMM	O
(	O
π	O
k	O
,	O
µ	O
k	O
,	O
σ	O
k	O
2	O
I	O
)	O
K	O
k=1	O
,	O
where	O
K	B-HyperparameterName
is	O
the	O
number	O
of	O
Gaussian	O
distributions	O
and	O
π	O
k	O
is	O
the	O
mixture	O
coefficient	O
of	O
the	O
k	O
-	O
th	O
component	O
of	O
the	O
GMM	O
module	O
as	O
computed	O
:	O
π	O
k	O
=	O
exp	O
(	O
e	O
k	O
)	O
K	O
i=1	O
exp	O
(	O
e	O
i	O
)	O
(	O
8	O
)	O
and	O
	O
	O
e	O
k	O
µ	O
k	O
log	O
σ	O
2	O
k	O
	O
	O
=	O
W	O
p	O
,	O
k	O
h	O
x	O
c	O
+	O
b	O
p	O
,	O
k	O
(	O
9	O
)	O
To	O
sample	O
an	O
instance	O
,	O
Gumble	O
-	O
Softmax	O
reparametrization	O
trick	O
(	O
Kusner	O
and	O
Hernández	O
-	O
Lobato	O
,	O
2016	O
)	O
is	O
utilized	O
to	O
normalize	O
the	O
coefficients	O
.	O
The	O
distance	O
here	O
is	O
measured	O
by	O
the	O
Wasserstein	O
distance	O
which	O
is	O
implemented	O
with	O
an	O
adversarial	O
discriminator	O
.	O
Recap	O
that	O
in	O
the	O
second	O
generation	O
phase	O
the	O
latent	O
variable	O
z	O
is	O
considered	O
to	O
only	O
capture	O
the	O
distinctive	O
feature	O
of	O
each	O
specific	O
response	O
.	O
Hence	O
to	O
distinguish	O
the	O
latent	O
variable	O
z	O
for	O
each	O
separate	O
response	O
,	O
we	O
further	O
introduce	O
a	O
multireference	B-MetricName
bag	I-MetricName
-	I-MetricName
of	I-MetricName
-	I-MetricName
word	I-MetricName
loss	I-MetricName
(	I-MetricName
MBOW	I-MetricName
)	I-MetricName
which	O
requires	O
the	O
network	O
to	O
predict	O
the	O
current	O
response	O
y	O
against	O
the	O
response	O
bag	O
:	O
L	O
mbow	O
=	O
E	O
q	O
φ	O
(	O
z	O
|	O
x	O
,	O
y	O
,	O
c	O
)	O
[	O
log	O
p	O
(	O
y	O
bow	O
|	O
x	O
,	O
z	O
)	O
+	O
λ	O
log	O
(	O
1	O
−	O
p	O
(	O
{	O
ȳ	O
}	O
bow	O
|	O
x	O
,	O
z	O
)	O
)	O
]	O
(	O
10	O
)	O
where	O
the	O
probability	O
is	O
computed	O
by	O
a	O
feedforward	O
network	O
f	O
as	O
the	O
vanilla	O
bag	O
-	O
of	O
-	O
word	O
loss	O
does	O
;	O
{	O
ȳ	O
}	O
is	O
the	O
complementary	O
response	O
bag	O
of	O
y	O
and	O
its	O
probability	O
is	O
computed	O
as	O
the	O
average	O
probability	O
of	O
responses	O
in	O
the	O
bag	O
;	O
and	O
λ	O
is	O
a	O
scaling	O
factor	O
accounting	O
for	O
the	O
difference	O
in	O
magnitude	O
.	O
As	O
it	O
shows	O
,	O
the	O
MBOW	O
loss	O
penalizes	O
the	O
recognition	O
networks	O
if	O
other	O
complementary	O
responses	O
can	O
be	O
predicted	O
from	O
the	O
distinctive	O
variable	O
z.	O
Besides	O
,	O
since	O
the	O
probability	O
of	O
the	O
complementary	O
term	O
may	O
approach	O
zero	O
which	O
makes	O
it	O
difficult	O
to	O
optimize	O
,	O
we	O
actually	O
adopt	O
its	O
lower	O
bound	O
in	O
practice	O
:	O
log	O
(	O
1	O
−	O
p	O
(	O
y	O
bow	O
|	O
x	O
,	O
z	O
)	O
)	O
=	O
log	O
(	O
1	O
−	O
|	O
y	O
|	O
t=1	O
e	O
fy	O
t	O
|	O
V	O
|	O
j	O
e	O
f	O
j	O
)	O
≥	O
log	O
(	O
|	O
y	O
|	O
t=1	O
(	O
1	O
−	O
e	O
fy	O
t	O
|	O
V	O
|	O
j	O
e	O
f	O
j	O
)	O
)	O
(	O
11	O
)	O
where	O
|	O
V	O
|	O
is	O
vocabulary	O
size	O
.	O
Totally	O
,	O
the	O
whole	O
loss	O
for	O
the	O
step	O
-	O
two	O
generation	O
is	O
then	O
:	O
L	O
second	O
=	O
L	O
cvae	O
+	O
L	O
mbow	O
(	O
12	O
)	O
which	O
can	O
be	O
optimized	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
way	O
.	O

In	O
the	O
first	O
generation	O
step	O
,	O
we	O
aim	O
to	O
map	O
from	O
the	O
input	O
query	O
x	O
to	O
the	O
common	O
feature	O
c	O
of	O
the	O
response	O
bag	O
{	O
y	O
}	O
.	O
Inspired	O
by	B-TaskName
multi	I-TaskName
-	I-TaskName
instance	I-TaskName
learning	I-TaskName
(	O
Zhou	O
,	O
2004	O
)	O
,	O
we	O
start	O
from	O
the	O
simple	O
intuition	O
that	O
it	O
is	O
much	O
easier	O
for	O
the	O
model	O
to	O
fit	O
multiple	O
instances	O
from	O
their	O
mid	O
-	O
point	O
than	O
a	O
random	O
start	O
-	O
point	O
,	O
as	O
illustrated	O
in	O
Figure	O
1	O
.	O
To	O
obtain	O
this	O
,	O
we	O
model	O
the	O
common	O
feature	O
of	O
the	O
response	O
bag	O
as	O
the	O
mid	O
-	O
point	O
of	O
embeddings	O
of	O
multiple	O
responses	O
.	O
In	O
practice	O
,	O
we	O
first	O
encode	O
the	O
input	O
x	O
with	O
a	O
bidirectional	O
gated	O
recurrent	O
units	O
(	O
GRU	O
)	O
to	O
obtain	O
an	O
input	O
representation	O
h	O
x	O
.	O
Then	O
,	O
the	O
common	O
feature	O
c	O
is	O
computed	O
by	O
a	O
mapping	O
network	O
which	O
is	O
implemented	O
by	O
a	O
feed	O
-	O
forward	O
neural	O
network	O
whose	O
trainable	O
parameter	O
is	O
denoted	O
as	O
θ	O
.	O
The	O
feature	O
c	O
is	O
then	O
fed	O
into	O
the	O
response	O
decoder	O
to	O
obtain	O
the	O
intermediate	O
response	O
y	O
c	O
which	O
is	O
considered	O
to	O
approximate	O
all	O
valid	O
responses	O
.	O
Mathematically	O
,	O
the	O
objective	B-HyperparameterName
function	I-HyperparameterName
is	O
defined	O
as	O
:	O
L	O
avg	O
=	O
1	O
|	O
{	O
y	O
}	O
|	O
y	O
{	O
y	O
}	O
log	O
p	O
ψ	O
(	O
y	O
|	O
c	O
)	O
(	O
1	O
)	O
where	O
|	O
{	O
y	O
}	O
|	O
is	O
the	O
cardinality	O
of	O
the	O
response	O
bag	O
{	O
y	O
}	O
and	O
p	O
ψ	O
represents	O
the	O
response	O
decoder	O
.	O
Besides	O
,	O
to	O
measure	O
how	O
well	O
the	O
intermediate	O
response	O
y	O
c	O
approximates	O
the	O
mid	O
-	O
point	O
response	O
,	O
we	O
set	O
up	O
an	O
individual	O
discriminator	O
and	O
derive	O
the	O
mapping	O
function	O
to	O
produce	O
better	O
results	O
.	O
As	O
to	O
the	O
discriminator	O
,	O
we	O
first	O
project	O
each	O
utterance	O
to	O
an	O
embedding	O
space	O
with	O
fixed	O
dimensionality	O
via	O
convolutional	O
neural	O
networks	O
(	O
CNNs	O
)	O
with	O
different	O
kernels	O
as	O
the	O
process	O
shown	O
in	O
Figure	O
3	O
.	O
Then	O
,	O
the	O
cosine	B-MetricName
similarity	I-MetricName
of	O
the	O
query	O
and	O
response	O
embeddings	O
is	O
computed	O
,	O
denoted	O
as	O
D	O
θ	O
(	O
x	O
,	O
y	O
)	O
,	O
where	O
θ	O
represents	O
trainable	O
parameter	O
in	O
the	O
discriminator	O
.	O
For	O
the	O
response	O
bag	O
{	O
y	O
}	O
,	O
the	O
average	O
response	O
embedding	O
is	O
used	O
to	O
compute	O
the	O
matching	B-MetricName
score	I-MetricName
.	O
The	O
objective	O
of	O
intermediate	O
response	O
y	O
c	O
is	O
then	O
to	O
minimize	O
the	O
difference	O
between	O
D	O
θ	O
(	O
x	O
,	O
y	O
c	O
)	O
and	O
D	O
θ	O
(	O
x	O
,	O
{	O
y	O
}	O
)	O
:	O
L	O
disc	O
=	O
E	O
x	O
,	O
{	O
y	O
}	O
,	O
y	O
c	O
[	O
D	O
θ	O
(	O
x	O
,	O
y	O
c	O
)	O
−	O
D	O
θ	O
(	O
x	O
,	O
{	O
y	O
}	O
)	O
]	O
(	O
2	O
)	O
where	O
y	O
c	O
denotes	O
the	O
utterance	O
produced	O
by	O
the	O
decoder	O
conditioned	O
on	O
the	O
variable	O
c.	O
To	O
overcome	O
the	O
discrete	O
and	O
non	O
-	O
differentiable	O
problem	O
,	O
which	O
breaks	O
down	O
gradient	O
propagation	O
from	O
the	O
discriminator	O
,	O
we	O
adopt	O
a	O
"	O
soft	O
"	O
continuous	O
approximation	O
(	O
Hu	O
et	O
al	O
,	O
2017	O
)	O
:	O
y	O
ct	O
∼	O
softmax	O
(	O
o	O
t	O
/τ	O
)	O
(	O
3	O
)	O
where	O
o	O
t	O
is	O
the	O
logit	O
vector	O
as	O
the	O
inputs	O
to	O
the	O
softmax	O
function	O
at	O
time	O
-	O
step	O
t	O
and	O
the	O
temperature	O
τ	O
is	O
set	O
to	O
τ	O
0	O
as	O
training	O
proceeds	O
for	O
increasingly	O
peaked	O
distributions	O
.	O
The	O
whole	O
loss	O
for	O
the	O
step	O
-	O
one	O
generation	O
is	O
then	O
L	O
f	O
irst	O
=	O
L	O
avg	O
+	O
L	O
disc	O
(	O
4	O
)	O
which	O
is	O
optimized	O
by	O
a	O
minimax	O
game	O
with	O
adversarial	B-TaskName
training	I-TaskName
(	O
Goodfellow	O
et	O
al	O
,	O
2014	O
)	O
.	O

Along	O
with	O
the	O
flourishing	O
development	O
of	O
neural	O
networks	O
,	O
the	O
sequence	O
-	O
to	O
-	O
sequence	O
framework	O
has	O
been	O
widely	O
used	O
for	O
conversation	B-TaskName
response	I-TaskName
generation	I-TaskName
(	O
Shang	O
et	O
al	O
,	O
2015	O
;	O
Sordoni	O
et	O
al	O
,	O
2015	O
)	O
where	O
the	O
mapping	O
from	O
a	O
query	O
x	O
to	O
a	O
reply	O
y	O
is	O
learned	O
with	O
the	O
negative	O
log	O
likelihood	O
.	O
However	O
,	O
these	O
models	O
suffer	O
from	O
the	O
"	O
safe	O
"	O
response	O
problem	O
.	O
To	O
address	O
this	O
problem	O
,	O
various	O
methods	O
have	O
been	O
proposed	O
.	O
Li	O
et	O
al	O
(	O
2016a	O
)	O
propose	O
a	O
diversity	O
-	O
promoting	O
objective	O
function	O
to	O
encourage	O
diverse	O
responses	O
during	O
decoding	O
.	O
Zhou	O
et	O
al	O
(	O
,	O
2018a	O
introduce	O
a	O
responding	O
mechanism	O
between	O
the	O
encoder	O
and	O
decoder	O
to	O
generate	O
various	O
responses	O
.	O
incorporate	O
topic	O
information	O
to	O
generate	O
informative	O
responses	O
.	O
However	O
,	O
these	O
models	O
suffer	O
from	O
the	O
deterministic	O
structure	O
when	O
generating	O
multiple	O
diverse	O
responses	O
.	O
Besides	O
,	O
during	O
the	O
training	O
of	O
these	O
models	O
,	O
response	O
utterances	O
are	O
only	O
used	O
in	O
the	O
loss	O
function	O
and	O
ignored	O
when	O
forward	O
computing	O
,	O
which	O
can	O
confuse	O
the	O
model	O
for	O
pursuing	O
multiple	O
objectives	O
simultaneously	O
.	O
A	O
few	O
works	O
explore	O
to	O
change	O
the	O
deterministic	O
structure	O
of	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
by	O
introducing	O
stochastic	O
latent	O
variables	O
.	O
VAE	B-MethodName
is	O
one	O
of	O
the	O
most	O
popular	O
methods	O
(	O
Bowman	O
et	O
al	O
,	O
2016	O
;	O
Serban	O
et	O
al	O
,	O
2017	O
;	O
Cao	O
and	O
Clark	O
,	O
2017	O
)	O
,	O
where	O
the	O
discourse	O
-	O
level	O
diversity	O
is	O
modeled	O
by	O
a	O
Gaussian	O
distribution	O
.	O
However	O
,	O
it	O
is	O
observed	O
that	O
in	O
the	O
CVAE	B-MethodName
with	O
a	O
fixed	O
Gaussian	O
prior	O
,	O
the	O
learned	O
conditional	O
posteriors	O
tend	O
to	O
collapse	O
to	O
a	O
single	O
mode	O
,	O
resulting	O
in	O
a	O
relatively	O
simple	O
scope	O
(	O
Wang	O
et	O
al	O
,	O
2017	O
)	O
.	O
To	O
tackle	O
this	O
,	O
WAE	B-MethodName
(	O
Gu	O
et	O
al	O
,	O
2018	O
)	O
which	O
adopts	O
a	O
Gaussian	O
mixture	O
prior	O
network	O
with	O
Wasserstein	B-MetricName
distance	I-MetricName
and	O
VAD	B-MetricName
(	O
Du	O
et	O
al	O
,	O
2018	O
)	O
which	O
sequentially	O
introduces	O
a	O
series	O
of	O
latent	O
variables	O
to	O
condition	O
each	O
word	O
in	O
the	O
response	O
sequence	O
are	O
proposed	O
.	O
Although	O
these	O
models	O
overcome	O
the	O
deterministic	O
structure	O
of	O
sequence	O
-	O
to	O
-	O
sequence	O
model	O
,	O
they	O
still	O
ignore	O
the	O
correlation	O
of	O
multiple	O
valid	O
responses	O
and	O
each	O
case	O
is	O
trained	O
separately	O
.	O
To	O
consider	O
the	O
multiple	O
responses	O
jointly	O
,	O
the	O
maximum	O
likelihood	O
strategy	O
is	O
explored	O
.	O
Zhang	O
et	O
al	O
(	O
2018a	O
)	O
propose	O
the	O
maximum	O
generated	O
likelihood	O
criteria	O
which	O
model	O
a	O
query	O
with	O
its	O
multiple	O
responses	O
as	O
a	O
bag	O
of	O
instances	O
and	O
proposes	O
to	O
optimize	O
the	O
model	O
towards	O
the	O
most	O
likely	O
answer	O
rather	O
than	O
all	O
possible	O
responses	O
.	O
Similarly	O
,	O
Rajendran	O
et	O
al	O
(	O
2018	O
)	O
propose	O
to	O
reward	O
the	O
dialogue	O
system	O
if	O
any	O
valid	O
answer	O
is	O
produced	O
in	O
the	O
reinforcement	O
learning	O
phase	O
.	O
Though	O
considering	O
multiple	O
responses	O
jointly	O
,	O
the	O
maximum	O
likelihood	O
strategy	O
fails	O
to	O
utilize	O
all	O
the	O
references	O
during	O
training	O
with	O
some	O
cases	O
ig	O
-	O
Figure	O
2	O
:	O
The	O
overall	O
architecture	O
of	O
our	O
proposed	O
dialogue	O
system	O
where	O
the	O
two	O
generation	O
steps	O
and	O
testing	O
process	O
are	O
illustrated	O
.	O
Given	O
an	O
input	O
query	O
x	O
,	O
the	O
model	O
aims	O
to	O
approximate	O
the	O
multiple	O
responses	O
in	O
a	O
bag	O
{	O
y	O
}	O
simultaneously	O
with	O
the	O
continuous	O
common	O
and	O
distinctive	O
features	O
,	O
i.e.	O
,	O
the	O
latent	O
variables	O
c	O
and	O
z	O
obtained	O
from	O
the	O
two	O
generation	O
phases	O
respectively	O
.	O
nored	O
.	O
In	O
our	O
approach	O
,	O
we	O
consider	O
multiple	O
responses	O
jointly	O
and	O
model	O
each	O
specific	O
response	O
separately	O
by	O
a	O
two	O
-	O
step	O
generation	O
architecture	O
.	O

We	O
tested	O
our	O
framework	O
on	O
another	O
task	O
,	O
the	O
distantly	O
supervised	O
open	O
-	O
domain	B-TaskName
question	I-TaskName
answering	I-TaskName
(	O
DS	B-TaskName
-	I-TaskName
QA	I-TaskName
)	O
task	O
(	O
Chen	O
et	O
al	O
,	O
2017	O
)	O
,	O
to	O
check	O
its	O
generalizability	O
.	O
Table	O
4	O
shows	O
the	O
statistics	O
for	O
the	O
datasets	O
used	O
in	O
this	O
experiment	O
.	O
The	O
first	O
three	O
,	O
Quasar	B-DatasetName
-	I-DatasetName
T	I-DatasetName
,	O
SearchQA	B-DatasetName
,	O
and	O
TriviaQA	B-DatasetName
provided	O
by	O
Lin	O
et	O
al	O
(	O
2018	O
)	O
,	O
were	O
used	O
for	O
training	O
and	O
evaluating	O
DS	B-TaskName
-	I-TaskName
QA	I-TaskName
methods	O
.	O
The	O
training	O
data	O
of	B-DatasetName
SQuAD	I-DatasetName
v1.1	I-DatasetName
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
was	O
used	O
for	O
training	O
our	O
AGR	O
.	O
The	B-DatasetName
SQuAD	I-DatasetName
dataset	I-DatasetName
consisted	O
of	O
the	O
triples	O
of	O
a	O
question	O
,	O
an	O
answer	O
,	O
and	O
a	O
paragraph	O
that	O
includes	O
the	O
answer	O
.	O
We	O
assume	O
that	O
the	O
answers	O
are	O
our	O
compact	O
answers	O
,	O
although	O
the	O
answers	O
in	O
the	O
dataset	O
are	O
consecutive	O
short	O
word	O
sequences	O
(	O
2.8	O
words	O
on	O
average	O
)	O
,	O
whose	O
majority	O
are	O
noun	O
phrases	O
,	O
unlike	O
the	O
compact	O
answers	O
for	O
our	O
why	O
-	O
QA	O
experiment	O
,	O
i.e.	O
,	O
sentences	O
or	O
phrases	O
(	O
8.3	O
words	O
on	O
average	O
)	O
.	O
We	O
trained	O
our	O
AGR	O
with	O
all	O
the	O
triples	O
of	O
a	O
question	O
,	O
an	O
answer	O
,	O
and	O
a	O
paragraph	O
in	O
the	O
training	O
data	O
of	O
SQuAD	B-DatasetName
-	I-DatasetName
v1.1	I-DatasetName
under	O
the	O
same	O
settings	O
for	O
the	O
AGR	O
's	O
hyperparameters	O
as	O
in	O
our	O
why	O
-	O
QA	B-TaskName
experiment	O
except	O
that	O
we	O
use	O
neither	O
causal	O
word	O
embeddings	O
nor	O
causality	O
-	O
attention	O
.	O
In	O
this	O
experiment	O
,	O
we	O
used	O
the	O
AGR	O
training	O
schemes	O
for	O
Ours	O
(	O
OP	O
)	O
and	O
Ours	O
(	O
RV	O
)	O
.	O
We	O
used	O
the	O
300	O
-	O
dimensional	O
GloVe	O
word	O
embeddings	O
learned	O
from	O
840	O
billion	O
tokens	O
in	O
the	O
web	O
crawl	O
data	O
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
,	O
as	O
general	O
word	O
embeddings	O
.	O
Then	O
we	O
combined	O
the	O
resulting	O
fake	O
-	O
representation	O
generator	O
F	O
in	O
the	O
AGR	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
DS	B-TaskName
-	I-TaskName
QA	I-TaskName
method	I-TaskName
,	O
OpenQA	B-TaskName
(	O
Lin	O
et	O
al	O
,	O
2018	O
)	O
7	O
.	O
We	O
also	O
used	O
the	O
hyperparameters	O
presented	O
in	O
Lin	O
et	O
al	O
(	O
2018	O
)	O
.	O
OpenQA	B-TaskName
is	O
composed	O
of	O
two	O
components	O
:	O
a	O
paragraph	O
selector	O
to	O
choose	O
relevant	O
paragraphs	O
(	O
or	O
answer	O
passages	O
in	O
our	O
terms	O
)	O
from	O
a	O
set	O
of	O
paragraphs	O
and	O
a	O
paragraph	O
reader	O
to	O
extract	O
answers	O
from	O
the	O
selected	O
paragraphs	O
.	O
For	O
identifying	O
answer	O
a	O
to	O
given	O
question	O
q	O
from	O
set	O
of	O
paragraphs	O
P	O
=	O
{	O
p	O
i	O
}	O
,	O
the	O
paragraph	O
selector	O
and	O
the	O
paragraph	O
reader	O
respectively	O
compute	O
probabilities	O
P	O
r	O
(	O
p	O
i	O
|	O
q	O
,	O
P	O
)	O
and	O
P	O
r	O
(	O
a	O
|	O
q	O
,	O
p	O
i	O
)	O
,	O
and	O
final	O
output	O
P	O
r	O
(	O
a	O
|	O
q	O
,	O
P	O
)	O
is	O
obtained	O
by	O
combining	O
the	O
probabilities	O
.	O
We	O
introduced	O
c	O
i	O
,	O
which	O
is	O
a	O
compact	O
-	O
answer	O
representation	O
generated	O
by	O
fakerepresentation	O
generator	O
F	O
with	O
question	O
q	O
and	O
paragraph	O
p	O
i	O
as	O
its	O
input	O
,	O
to	O
the	O
computation	O
of	O
the	O
probabilities	O
as	O
follows	O
:	O
P	O
r	O
(	O
a	O
|	O
q	O
,	O
P	O
,	O
C	O
)	O
=	O
i	O
P	O
r	O
(	O
a	O
|	O
q	O
,	O
p	O
i	O
,	O
c	O
i	O
)	O
P	O
r	O
(	O
p	O
i	O
|	O
q	O
,	O
P	O
,	O
c	O
i	O
)	O
In	O
the	O
original	O
OpenQA	O
,	O
the	O
paragraph	O
selector	O
and	O
the	O
reader	O
use	O
bidirectional	O
stacked	O
RNNs	O
for	O
encoding	O
paragraphs	O
,	O
where	O
word	O
embeddings	O
p	O
i	O
of	O
a	O
paragraph	O
is	O
used	O
as	O
the	O
input	O
.	O
In	O
our	O
implementation	O
,	O
we	O
computed	O
attention	O
-	O
weighted	O
embeddingp	O
i	O
of	O
a	O
paragraph	O
by	O
using	O
compactanswer	O
representation	O
c	O
i	O
.	O
Given	O
word	O
embedding	O
p	O
j	O
i	O
for	O
the	O
j	O
-	O
th	O
word	O
in	O
paragraph	O
p	O
i	O
,	O
its	O
attentionweighted	O
embeddingp	O
j	O
i	O
was	O
computed	O
by	O
using	O
a	O
bilinear	O
function	O
(	O
Sutskever	O
et	O
al	O
,	O
2009	O
)	O
:	O
p	O
j	O
i	O
=	O
softmax	O
j	O
(	O
p	O
T	O
i	O
Mc	O
i	O
)	O
p	O
j	O
i	O
,	O
where	O
M	O
R	O
d×d	O
is	O
a	O
trainable	O
matrix	O
,	O
softmax	O
j	O
(	O
x	O
)	O
denotes	O
the	O
j	O
-	O
th	O
element	O
of	O
the	O
softmaxed	O
vector	O
of	O
x	O
,	O
and	O
d	O
=	O
300	O
.	O
We	O
gave	O
[	O
p	O
j	O
i	O
;	O
p	O
j	O
i	O
]	O
,	O
a	O
concatenation	O
of	O
p	O
j	O
i	O
andp	O
j	O
i	O
,	O
as	O
the	O
word	O
embedding	O
of	O
the	O
j	O
-	O
th	O
word	O
in	O
paragraph	O
p	O
i	O
to	O
the	O
bidirectional	O
stacked	O
RNNs	O
.	O
Table	O
5	O
shows	O
the	O
performances	O
of	O
the	O
four	O
DS	O
-	O
QA	O
methods	O
:	O
R	B-MethodName
3	I-MethodName
(	O
Wang	O
et	O
al	O
,	O
2018	O
)	O
,	O
OpenQA	B-MethodName
(	O
Lin	O
et	O
al	O
,	O
2018	O
)	O
,	O
Ours	O
(	O
OP	B-MethodName
)	O
,	O
and	O
Ours	O
(	O
RV	B-MethodName
)	O
evaluated	O
against	O
the	O
Quasar	B-DatasetName
-	I-DatasetName
T	I-DatasetName
,	O
SearchQA	B-DatasetName
and	O
TriviaQA	B-DatasetName
datasets	O
.	O
All	O
the	O
methods	O
were	O
evaluated	O
with	O
EM	B-MetricName
and	O
F1	B-MetricName
scores	O
,	O
following	O
Lin	O
et	O
al	O
(	O
2018	O
)	O
.	O
EM	B-MetricName
measures	O
the	O
percentage	O
of	O
predictions	O
that	O
exactly	O
match	O
one	O
of	O
the	O
ground	O
-	O
truth	O
answers	O
and	O
F1	B-MetricName
is	O
a	O
metric	O
that	O
loosely	O
measures	O
the	O
average	O
overlap	O
between	O
the	O
prediction	O
and	O
ground	O
-	O
truth	O
answer	O
.	O
Note	O
that	O
both	O
Ours	O
(	O
OP	B-MethodName
)	O
and	O
Ours	O
(	O
RV	B-MethodName
)	O
outperformed	O
both	O
previous	O
methods	O
,	O
R	B-MethodName
3	I-MethodName
and	O
OpenQA	B-MethodName
,	O
except	O
for	O
the	O
F1	B-MetricName
score	O
for	O
the	O
TriviaQA	B-DatasetName
dataset	O
.	O
Some	O
of	O
the	O
improvements	O
over	O
the	O
previous	O
state	O
-	O
ofthe	O
-	O
art	O
method	O
,	O
OpenQA	B-MethodName
,	O
were	O
statistically	O
significant	O
.	O
These	O
findings	O
suggest	O
that	O
our	O
framework	O
can	O
be	O
effective	O
for	O
tasks	O
other	O
than	O
the	O
original	O
why	O
-	O
QA	O
and	O
the	O
other	O
datasets	O
.	O

Another	O
interesting	O
point	O
is	O
that	O
Ours	O
(	O
RV	B-MethodName
)	O
,	O
in	O
which	O
fake	O
-	O
representation	O
generator	O
F	O
RV	O
was	O
trained	O
using	O
random	O
vectors	O
,	O
achieved	O
almost	O
the	O
same	O
performance	O
as	O
that	O
of	O
Ours	O
(	O
OP	O
)	O
.	O
This	O
result	O
was	O
puzzling	O
,	O
so	O
we	O
first	O
checked	O
whether	O
F	O
RV	B-MethodName
's	O
output	O
was	O
not	O
just	O
random	O
noise	O
(	O
which	O
could	O
prevent	O
the	O
why	O
-	O
QA	B-TaskName
model	O
from	O
overfitting	O
)	O
by	O
replacing	O
in	O
Ours	O
(	O
RV	B-MethodName
)	O
the	O
output	O
of	O
F	O
RV	B-MethodName
by	O
random	O
vectors	O
.	O
Although	O
we	O
sampled	O
the	O
random	O
vectors	O
from	O
different	O
distribution	O
types	O
with	O
various	O
ranges	O
,	O
we	O
obtained	O
at	O
best	O
similar	O
performance	O
to	O
that	O
of	O
BASE	B-MethodName
:	O
51.6	B-MetricValue
in	O
P@1	B-MetricName
.	O
This	O
result	O
confirms	O
that	O
it	O
is	O
not	O
trivial	O
to	O
mimic	O
F	O
RV	B-MethodName
using	O
random	O
vectors	O
at	O
least	O
.	O
We	O
investigated	O
the	O
F	O
RV	B-MethodName
's	O
output	O
to	O
check	O
whether	O
it	O
actually	O
focused	O
on	O
the	O
compact	O
answer	O
in	O
a	O
given	O
passage	O
.	O
We	O
computed	O
the	O
following	O
three	O
representation	O
sets	O
from	O
a	O
gold	O
set	O
of	O
3	O
,	O
608	O
triples	O
of	O
why	O
-	O
questions	O
,	O
answer	O
passages	O
and	O
manually	O
created	O
compact	O
-	O
answers	O
that	O
do	O
not	O
overlap	O
with	O
CmpAns	O
:	O
{	O
r	O
org	O
i	O
}	O
:	O
F	B-MethodName
RV	I-MethodName
's	O
output	O
with	O
the	O
pairs	O
of	O
a	O
whyquestion	O
and	O
an	O
answer	O
passage	O
in	O
the	O
gold	O
set	O
as	O
its	O
input	O
;	O
{	O
r	O
in	O
i	O
}	O
:	O
F	O
RV	B-MethodName
's	O
output	O
for	O
the	O
same	O
input	O
as	O
{	O
r	O
org	O
i	O
}	O
,	O
where	O
we	O
replaced	O
the	O
word	O
embeddings	O
of	O
all	O
the	O
content	O
words	O
in	O
the	O
answer	O
passages	O
that	O
also	O
appeared	O
in	O
the	O
associated	O
gold	O
compact	O
-	O
answers	O
with	O
random	O
vectors	O
;	O
{	O
r	O
out	O
i	O
}	O
:	O
F	O
RV	O
's	O
output	O
for	O
the	O
same	O
input	O
as	O
{	O
r	O
org	O
i	O
}	O
,	O
where	O
we	O
replaced	O
the	O
word	O
embeddings	O
of	O
all	O
the	O
content	O
words	O
in	O
the	O
answer	O
passages	O
that	O
did	O
not	O
appear	O
in	O
the	O
associated	O
gold	O
compact	O
-	O
answers	O
with	O
random	O
vectors	O
6	O
.	O
If	O
F	B-MethodName
RV	I-MethodName
perfectly	O
focuses	O
on	O
the	O
gold	O
standard	O
compact	O
-	O
answers	O
,	O
for	O
each	O
question	O
-	O
passage	O
pair	O
,	O
6	O
For	O
both	O
r	O
in	O
i	O
and	O
r	O
out	O
i	O
,	O
we	O
never	O
replaced	O
the	O
word	O
embeddings	O
for	O
the	O
words	O
that	O
also	O
appeared	O
in	O
the	O
question	O
.	O
r	O
out	O
i	O
should	O
be	O
the	O
same	O
as	O
r	O
org	O
i	O
and	O
r	O
in	O
i	O
should	O
significantly	O
differ	O
from	O
r	O
org	O
i	O
.	O
Next	O
we	O
computed	O
the	O
average	O
Euclidian	O
distance	O
among	O
{	O
r	O
org	O
i	O
}	O
,	O
{	O
r	O
in	O
i	O
}	O
and	O
{	O
r	O
out	O
i	O
}	O
.	O
The	O
average	O
distance	O
(	O
2.67	O
)	O
between	O
{	O
r	O
org	O
i	O
}	O
and	O
{	O
r	O
out	O
i	O
}	O
was	O
much	O
smaller	O
than	O
the	O
average	O
distance	O
(	O
13.3	O
)	O
between	O
{	O
r	O
org	O
i	O
}	O
and	O
{	O
r	O
in	O
i	O
}	O
.	O
Note	O
that	O
we	O
replaced	O
the	O
word	O
embeddings	O
for	O
much	O
more	O
words	O
with	O
random	O
vectors	O
in	O
the	O
computation	O
of	O
{	O
r	O
out	O
i	O
}	O
than	O
those	O
in	O
the	O
computation	O
of	O
{	O
r	O
in	O
i	O
}	O
(	O
38.1	O
words	O
vs.	O
5.6	O
words	O
)	O
.	O
This	O
implies	O
that	O
the	O
distance	O
between	O
{	O
r	O
org	O
i	O
}	O
and	O
{	O
r	O
out	O
i	O
}	O
might	O
be	O
much	O
larger	O
than	O
that	O
between	O
{	O
r	O
org	O
i	O
}	O
and	O
{	O
r	O
in	O
i	O
}	O
if	O
F	O
RV	B-MethodName
focused	O
equally	O
on	O
every	O
answer	O
passage	O
word	O
.	O
However	O
,	O
the	O
actual	O
results	O
suggest	O
that	O
this	O
is	O
not	O
the	O
case	O
.	O
Although	O
we	O
can	O
not	O
draw	O
decisive	O
conclusions	O
due	O
to	O
the	O
complex	O
nature	O
of	O
neural	O
networks	O
,	O
we	O
believe	O
from	O
the	O
results	O
that	O
F	B-MethodName
RV	I-MethodName
does	O
actually	O
focus	O
more	O
on	O
words	O
that	O
are	O
a	O
part	O
of	O
a	O
compact	O
answer	O
than	O
on	O
other	O
words	O
.	O
We	O
also	O
computed	O
{	O
r	O
org	O
i	O
}	O
,	O
{	O
r	O
in	O
i	O
}	O
,	O
and	O
{	O
r	O
out	O
i	O
}	O
with	O
fakerepresentation	O
generator	O
F	O
OP	O
in	O
the	O
same	O
way	O
and	O
observed	O
the	O
same	O
tendency	O
.	O

Table	O
3	O
shows	O
the	O
performances	O
of	O
all	O
the	O
methods	O
in	O
the	O
Precision	B-MetricValue
of	O
the	O
top	O
answer	O
(	O
P@1	O
)	O
and	O
the	O
Mean	B-MetricValue
Average	I-MetricValue
Precision	I-MetricValue
(	O
MAP	B-MetricValue
)	O
(	O
Oh	O
et	O
al	O
,	O
2013	O
)	O
.	O
Note	O
that	O
the	O
Oracle	O
method	O
indicates	O
the	O
performance	O
of	O
a	O
fictional	O
method	O
that	O
ranks	O
the	O
answer	O
passages	O
perfectly	O
,	O
i.e.	O
,	O
it	O
locates	O
all	O
the	O
m	O
correct	O
answers	O
to	O
a	O
question	O
in	O
the	O
top	O
-	O
m	O
ranks	O
,	O
based	O
on	O
the	O
gold	O
-	O
standard	O
labels	O
.	O
This	O
performance	O
is	O
the	O
upper	O
bound	O
of	O
those	O
of	O
all	O
the	O
implementable	O
methods	O
.	O
Our	O
proposed	O
method	O
,	O
Ours	O
(	O
OP	O
)	O
,	O
outperformed	O
all	O
the	O
other	O
methods	O
.	O
Our	O
starting	O
point	O
,	O
i.e.	O
,	O
BASE	B-MethodName
,	O
was	O
already	O
superior	O
to	O
the	O
methods	O
in	O
the	O
previous	O
works	O
.	O
Compared	O
with	O
BASE	B-MethodName
and	O
BASE+AddTr	B-MethodName
,	O
neither	O
of	O
which	O
used	O
compactanswer	O
representations	O
or	O
fake	O
-	O
representation	O
generator	O
F	O
,	O
Ours	O
(	O
OP	O
)	O
gave	O
3.4	B-MetricValue
%	I-MetricValue
and	B-MetricValue
2.8	I-MetricValue
%	I-MetricValue
improvement	O
in	B-MetricName
P@1	I-MetricName
,	O
respectively	O
.	O
It	O
also	O
outperformed	O
BASE+CAns	B-MethodName
and	O
BASE+CEnc	B-MethodName
,	O
which	O
generated	O
compact	O
-	O
answer	O
representations	O
in	O
a	O
way	O
different	O
from	O
the	O
proposed	O
method	O
,	O
and	O
BASE+Enc	O
,	O
which	O
trained	O
the	O
fake	O
-	O
representation	O
generator	O
without	O
adversarial	O
learning	O
.	O
These	O
performance	O
differences	O
were	O
statistically	O
significant	O
(	O
p	O
<	O
0.01	O
by	O
the	O
McNemar	O
's	O
test	O
)	O
.	O
Ours	O
(	O
OP	O
)	O
also	O
outperformed	O
all	O
the	O
BERTbased	B-MethodName
models	O
but	O
an	O
interesting	O
point	O
is	O
that	O
fakerepresentation	O
generator	O
F	O
boosted	O
the	O
performance	O
of	O
the	O
BERT	B-MethodName
-	O
based	O
models	O
(	O
statistically	O
significant	O
with	O
p	O
<	O
0.01	O
by	O
the	O
McNemar	O
's	O
test	O
)	O
.	O
These	O
results	O
suggest	O
that	O
AGR	B-MethodName
is	O
effective	O
in	O
both	O
our	O
why	O
-	O
QA	B-TaskName
model	O
and	O
our	O
BERT	B-MethodName
-	O
based	O
model	O
.	O

Same	O
as	O
BERT+FOP	B-MethodName
except	O
that	O
it	O
used	O
FRV	B-MethodName
instead	O
of	O
FOP	B-MethodName
for	O
producing	O
compact	O
-	O
answer	O
representation	O
.	O
To	O
pre	O
-	O
train	O
the	O
BERT	B-MethodName
-	O
based	O
models	O
,	O
we	O
used	O
a	O
combination	O
of	O
sentences	O
extracted	O
from	O
Japanese	O
Wikipedia	O
articles	O
(	O
August	O
2018	O
version	O
)	O
and	O
causality	O
expressions	O
automatically	O
recognized	O
from	O
a	O
causality	O
recognizer	O
(	O
Oh	O
et	O
al	O
,	O
2013	O
)	O
.	O
This	O
data	O
mix	O
consists	O
of	O
75	O
%	O
of	O
sentences	O
extracted	O
from	O
Wikipedia	O
(	O
14	O
,	O
675	O
,	O
535	O
sentences	O
taken	O
out	O
of	O
784	O
,	O
869	O
articles	O
randomly	O
sampled	O
)	O
and	O
25	O
%	O
of	O
cause	O
and	O
effect	O
phrases	O
taken	O
from	O
causality	O
expressions	O
(	O
4	O
,	O
891	O
,	O
846	O
phrases	O
from	O
2	O
,	O
445	O
,	O
923	O
causal	O
relations	O
)	O
.	O
This	O
ratio	O
was	O
determined	O
through	O
preliminary	O
experiments	O
using	O
the	O
development	O
data	O
.	O
For	O
the	O
pre	O
-	O
training	O
parameters	O
,	O
we	O
followed	O
the	O
settings	O
of	O
BERT	B-MethodName
BASE	O
in	O
Devlin	O
et	O
al	O
(	O
2019	O
)	O
3	O
except	O
for	O
the	O
batch	O
size	O
of	O
50	O
.	O
We	O
ran	O
3	O
epochs	O
with	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
1e	B-HyperparameterValue
-	I-HyperparameterValue
5	I-HyperparameterValue
for	O
finetuning	O
the	O
BERT	B-MethodName
-	O
based	O
models	O
4	O
.	O
+	O
+	O
+	O
+	O
+	O
+	O
E	O
[	O
CLS	O
]	O
E	O
'	O
E	O
′	O
)	O
E	O
Q	O
E	O
Q	O
E	O
P	O
E	O
0	O
E	O
1	O
E	O
N+M+1	O
E	O
[	O
A	O
BERT	B-MethodName
-	O
based	O
model	O
,	O
BERT	B-MethodName
,	O
takes	O
a	O
questionpassage	O
pair	O
as	O
input	O
and	O
computes	O
the	O
input	O
representation	O
using	O
token	O
,	O
segment	O
,	O
position	O
,	O
and	O
attention	O
feature	O
embeddings	O
(	O
Fig	O
.	O
3	O
)	O
.	O
For	O
the	O
input	O
representation	O
computation	O
,	O
the	O
original	O
BERT	O
only	O
used	O
the	O
token	O
,	O
segment	O
,	O
and	O
position	O
embeddings	O
,	O
while	O
BERT	O
additionally	O
used	O
the	O
attention	O
feature	O
embeddings	O
5	O
to	O
exploit	O
the	O
same	O
similarity	O
-	O
attention	O
and	O
causalityattention	O
features	O
used	O
in	O
our	O
proposed	O
method	O
.	O
We	O
used	O
the	O
attention	O
feature	O
embeddings	O
during	O
the	O
fine	O
-	O
tuning	O
and	O
testing	O
,	O
but	O
not	O
during	O
the	O
pretraining	O
of	O
the	O
BERT	O
-	O
based	O
model	O
.	O
The	O
attention	O
feature	O
embeddings	O
for	O
answer	O
passages	O
(	O
i.e.	O
,	O
E	O
sim	O
w	O
1	O
,	O
,	O
E	O
sim	O
w	O
M	O
,	O
and	O
E	O
caus	O
w	O
1	O
,	O
,	O
E	O
caus	O
w	O
M	O
)	O
were	O
computed	O
from	O
the	O
same	O
attention	O
feature	O
vectors	O
,	O
a	O
s	O
and	O
a	O
c	O
,	O
as	O
those	O
in	O
our	O
proposed	O
methods	O
;	O
those	O
for	O
the	O
other	O
parts	O
(	O
i.e.	O
,	O
questions	O
,	O
[	O
CLS	O
]	O
,	O
and	O
[	O
SEP	O
]	O
)	O
were	O
computed	O
from	O
a	O
zero	O
vector	O
(	O
indicating	O
no	O
attention	O
feature	O
)	O
.	O
The	O
transformer	O
encoder	O
processed	O
the	O
input	O
representation	O
to	O
gen	O
-	O
3	O
12	O
-	O
layers	O
,	O
768	B-HyperparameterValue
hidden	B-HyperparameterName
states	I-HyperparameterName
,	O
12	B-HyperparameterValue
heads	B-HyperparameterName
and	O
training	O
for	O
1	B-HyperparameterValue
-	I-HyperparameterValue
million	I-HyperparameterValue
steps	B-HyperparameterName
with	O
the	O
warmup	B-HyperparameterName
rate	I-HyperparameterName
of	O
1	B-HyperparameterValue
%	I-HyperparameterValue
using	O
Adam	B-HyperparameterValue
optimizer	B-HyperparameterName
with	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
1e	B-HyperparameterValue
-	I-HyperparameterValue
4	I-HyperparameterValue
.	O
4	O
We	O
tested	O
all	O
the	O
combinations	O
of	O
epochs	B-HyperparameterName
{	O
1	B-HyperparameterValue
,	I-HyperparameterValue
2	I-HyperparameterValue
,	I-HyperparameterValue
3	I-HyperparameterValue
,	I-HyperparameterValue
4	I-HyperparameterValue
,	I-HyperparameterValue
5	I-HyperparameterValue
}	O
and	O
learning	B-HyperparameterName
rates	I-HyperparameterName
of	O
{	O
1e	B-HyperparameterValue
-	I-HyperparameterValue
5	I-HyperparameterValue
,	I-HyperparameterValue
2e	I-HyperparameterValue
-	I-HyperparameterValue
5	I-HyperparameterValue
,	I-HyperparameterValue
3e	I-HyperparameterValue
-	I-HyperparameterValue
5	I-HyperparameterValue
}	O
and	O
chose	O
the	O
one	O
that	O
maximized	O
the	O
performance	O
on	O
the	O
development	O
data	O
in	O
W	O
hySet	O
.	O
5	O
We	O
also	O
evaluated	O
a	O
BERT	B-MethodName
-	O
based	O
model	O
that	O
did	O
not	O
use	O
the	O
attention	O
feature	O
embeddings	O
,	O
but	O
its	O
P@1	B-MetricName
(	O
41.4	B-MetricValue
)	O
was	O
much	O
lower	O
than	O
that	O
of	O
BERT	B-MethodName
(	O
51.2	B-MetricValue
)	O
.	O
P@1	B-MetricName
MAP	I-MetricName
Oh	O
et	O
al	O
(	O
2013	O
)	O
41.8	O
41.0	O
Sharp	O
et	O
al	O
(	O
2016	O
)	O

In	O
our	O
proposed	O
methods	O
and	O
their	O
variants	O
,	O
all	O
the	O
weights	O
in	O
the	O
CNNs	O
were	O
initialized	B-HyperparameterName
using	O
He	B-HyperparameterValue
's	I-HyperparameterValue
method	I-HyperparameterValue
(	O
He	O
et	O
al	O
,	O
2015	O
)	O
,	O
and	O
the	O
other	O
weights	O
in	O
our	O
why	O
-	O
QA	O
model	O
were	O
initialized	B-HyperparameterName
randomly	O
with	O
a	O
uniform	B-HyperparameterValue
distribution	I-HyperparameterValue
in	I-HyperparameterValue
the	I-HyperparameterValue
range	I-HyperparameterValue
of	I-HyperparameterValue
(	I-HyperparameterValue
-	I-HyperparameterValue
0.01	I-HyperparameterValue
,	I-HyperparameterValue
0.01	I-HyperparameterValue
)	I-HyperparameterValue
.	O
For	O
the	O
CNN	O
-	O
based	O
components	O
,	O
we	O
set	O
the	O
window	B-HyperparameterName
size	I-HyperparameterName
of	O
the	O
filters	O
to	O
"	O
1	B-HyperparameterValue
,	O
2	B-HyperparameterValue
,	O
3	B-HyperparameterValue
"	O
with	O
100	O
filters	O
each	O
2	O
.	O
We	O
used	O
dropout	B-HyperparameterName
(	O
Srivastava	O
et	O
al	O
,	O
2014	O
)	O
with	O
probability	O
0.5	B-HyperparameterValue
on	O
the	O
final	O
logistic	O
regression	O
layer	O
.	O
All	O
of	O
these	O
hyper	O
-	O
parameters	O
were	O
chosen	O
with	O
our	O
development	O
data	O
.	O
We	O
optimized	O
the	O
learned	O
parameters	O
with	O
the	O
Adam	B-HyperparameterValue
stochastic	I-HyperparameterValue
gradient	I-HyperparameterValue
descent	I-HyperparameterValue
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
.	O
The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
was	O
set	O
to	O
0.001	B-HyperparameterValue
,	O
and	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
for	O
each	O
iteration	O
was	O
set	O
to	O
20	B-HyperparameterValue
.	O

att	O
is	O
given	O
to	O
CNNs	O
to	O
generate	O
final	O
representation	O
r	O
t	O
of	O
a	O
given	O
passage	O
/	O
compact	O
-	O
answer	O
t.	O
The	O
CNNs	O
resembles	O
those	O
in	O
Kim	O
(	O
2014	O
)	O
.	O
Convolutions	O
are	O
performed	O
over	O
the	O
word	O
embeddings	O
using	O
both	O
multiple	O
filters	O
and	O
multiple	O
filter	O
windows	O
(	O
e.g.	O
,	O
sliding	O
over	O
1	O
,	O
2	O
,	O
or	O
3	O
word	O
windows	O
at	O
a	O
time	O
and	O
100	O
filters	O
for	O
each	O
window	O
)	O
.	O
An	O
average	O
pooling	O
operation	O
is	O
applied	O
to	O
the	O
convolution	O
results	O
to	O
generate	O
representation	O
r	O
t	O
,	O
which	O
is	O
the	O
output	O
/	O
value	O
of	O
Encoder	O
(	O
t	O
;	O
θ	O
,	O
q	O
)	O
;	O
r	O
t	O
=	O
Encoder	O
(	O
t	O
;	O
θ	O
,	O
q	O
)	O
.	O
In	O
our	O
experiments	O
,	O
we	O
set	O
the	O
dimension	O
of	O
representation	O
r	O
t	O
to	O
300	O
.	O
4	O
Why	O
-	O
QA	B-TaskName
Experiments	O

The	O
pre	O
-	O
trained	O
word	O
embeddings	O
used	O
in	O
Encoder	O
(	O
t	O
;	O
θ	O
,	O
q	O
)	O
were	O
obtained	O
by	O
concatenating	O
two	O
types	O
of	O
d	O
-	O
dimensional	O
word	O
embeddings	O
(	O
d	B-HyperparameterName
=	O
300	B-HyperparameterValue
in	O
this	O
work	O
)	O
:	O
general	O
word	O
embeddings	O
and	O
causal	O
word	O
embeddings	O
.	O
General	O
word	O
embeddings	O
are	O
widely	O
used	O
embedding	O
vectors	O
(	O
300	B-HyperparameterValue
dimensions	B-HyperparameterName
)	O
that	O
were	O
pretrained	O
for	O
about	O
1.65	O
million	O
words	O
by	O
applying	O
word2vec	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
to	O
about	O
35	O
million	O
sentences	O
from	O
Japanese	B-TaskName
Wikipedia	I-TaskName
(	O
January	O
2015	O
version	O
)	O
.	O
Causal	O
word	O
embeddings	O
(	O
Sharp	O
et	O
al	O
,	O
2016	O
)	O
were	O
proposed	O
for	O
representing	B-MetricValue
the	I-MetricValue
causal	I-MetricValue
associations	I-MetricValue
between	I-MetricValue
words	I-MetricValue
.	O
Sharp	O
et	O
al	O
(	O
2016	O
)	O
cre	O
-	O
ated	O
a	O
set	O
of	O
cause	O
-	O
effect	O
word	O
pairs	O
by	O
paring	O
each	O
content	O
word	O
in	O
a	O
cause	O
part	O
with	O
each	O
content	O
word	O
in	O
an	O
effect	O
part	O
of	O
the	O
same	O
causality	O
expression	O
,	O
such	O
as	O
"	O
Volcanoes	O
erupt	O
because	O
magma	O
pushes	O
through	O
vents	O
and	O
fissures	O
.	O
"	O
In	O
this	O
work	O
,	O
we	O
extracted	O
100	O
million	O
causality	O
expressions	O
from	O
4	O
-	O
billion	O
Japanese	O
web	O
pages	O
using	O
the	O
causality	O
recognizer	O
of	O
Oh	O
et	O
al	O
(	O
2013	O
)	O
.	O
Then	O
,	O
following	O
Sharp	O
et	O
al	O
(	O
2016	O
)	O
,	O
we	O
trained	O
300dimensional	O
causal	O
word	O
embeddings	O
for	O
about	O
1.85	O
million	O
words	O
by	O
applying	O
the	O
generalized	O
skip	B-MethodName
-	I-MethodName
gram	I-MethodName
embedding	I-MethodName
model	I-MethodName
of	O
Levy	O
and	O
Goldberg	O
(	O
2014	O
)	O
to	O
the	O
causality	O
expressions	O
.	O

Figure	O
2	O
illustrates	O
the	O
architecture	O
shared	O
by	O
our	O
fake	O
-	O
representation	O
generator	O
F	O
and	O
real	O
-	O
representation	O
generator	O
R	O
,	O
namely	O
,	O
Encoder	O
(	O
t	O
;	O
θ	O
,	O
q	O
)	O
,	O
where	O
θ	O
is	O
a	O
set	O
of	O
parameters	O
,	O
q	O
is	O
a	O
why	O
-	O
question	O
,	O
and	O
t	O
is	O
either	O
an	O
answer	O
passage	O
or	O
a	O
manually	O
created	O
compact	O
-	O
answer	O
.	O
Encoder	O
(	O
t	O
;	O
θ	O
,	O
q	O
)	O
first	O
represents	O
question	O
q	O
and	O
passage	O
/	O
compact	O
-	O
answer	O
t	O
with	O
pre	O
-	O
trained	O
word	O
embeddings	O
,	O
which	O
are	O
supplemented	O
with	O
attention	O
mechanisms	O
.	O
The	O
resulting	O
attention	O
-	O
weighted	O
word	O
embeddings	O
are	O
given	O
to	O
convolutional	O
neural	O
networks	O
(	O
CNNs	O
)	O
that	O
generate	O
a	O
single	O
feature	O
vector	O
,	O
which	O
is	O
an	O
output	O
/	O
value	O
of	O
Encoder	O
(	O
t	O
;	O
θ	O
,	O
q	O
)	O
.	O
In	O
the	O
following	O
,	O
we	O
give	O
an	O
overview	O
of	O
the	O
word	O
embeddings	O
,	O
the	O
attention	O
mechanisms	O
,	O
and	O
the	O
CNNs	O
used	O
in	O
Encoder	O
(	O
t	O
;	O
θ	O
,	O
q	O
)	O
.	O
All	O
of	O
these	O
techniques	O
were	O
proposed	O
by	O
previous	O
works	O
.	O
Further	O
details	O
are	O
given	O
in	O
Section	O
B	O
of	O
the	O
supplementary	O
materials	O
.	O

In	O
our	O
implementation	O
,	O
both	O
F	O
and	O
R	O
are	O
networks	O
with	O
identical	O
structure	O
called	O
Encoder	O
.	O
They	O
are	O
defined	O
as	O
follows	O
,	O
where	O
p	O
,	O
c	O
,	O
and	O
q	O
are	O
respectively	O
an	O
answer	O
passage	O
,	O
a	O
manually	O
created	O
compact	O
-	O
answer	O
,	O
and	O
a	O
why	O
-	O
question	O
:	O
F	O
(	O
p	O
|	O
q	O
)	O
=	O
Encoder	O
(	O
p	O
;	O
θ	O
F	O
,	O
q	O
)	O
R	O
(	O
c	O
|	O
q	O
)	O
=	O
Encoder	O
(	O
c	O
;	O
θ	O
R	O
,	O
q	O
)	O
Here	O
θ	O
F	O
and	O
θ	O
R	O
represent	O
the	O
parameters	O
of	O
networks	O
F	O
and	O
R.	O
The	O
details	O
of	O
Encoder	O
are	O
described	O
below	O
.	O
Discriminator	O
D	O
(	O
r	O
)	O
takes	O
as	O
input	O
r	O
,	O
either	O
the	O
output	O
of	O
F	O
(	O
p	O
|	O
q	O
)	O
or	O
that	O
of	O
R	O
(	O
c	O
|	O
q	O
)	O
,	O
and	O
computes	O
the	O
probability	O
that	O
given	O
representation	O
r	O
comes	O
from	O
a	O
real	O
compact	O
-	O
answer	O
using	O
a	O
feedforward	O
network	O
with	O
two	B-HyperparameterValue
hidden	B-HyperparameterName
layers	I-HyperparameterName
(	O
100	O
nodes	O
in	O
the	O
first	O
layer	O
and	O
50	O
in	O
the	O
second	O
layer	O
)	O
and	O
a	O
logistic	O
regression	O
layer	O
on	O
top	O
of	O
the	O
hidden	O
layers	O
.	O
We	O
used	O
sigmoid	O
outputs	O
by	O
the	O
logistic	O
regression	O
layer	O
as	O
the	O
output	O
probability	O
.	O

Why	O
-	O
question	B-TaskName
answering	I-TaskName
(	O
why	O
-	O
QA	B-TaskName
)	O
tasks	O
retrieve	O
from	O
a	O
text	O
archive	O
answers	O
to	O
such	O
why	O
-	O
questions	O
as	O
"	O
Why	O
does	O
honey	O
last	O
such	O
a	O
long	O
time	O
?	O
"	O
Previous	O
why	O
-	O
QA	B-TaskName
methods	O
retrieve	O
from	O
a	O
text	O
archive	O
answer	O
passages	O
,	O
each	O
of	O
which	O
consists	O
of	O
several	O
sentences	O
,	O
like	O
A	O
in	O
Table	O
1	O
(	O
Girju	O
,	O
2003	O
;	O
Higashinaka	O
and	O
Isozaki	O
,	O
2008	O
;	O
Oh	O
et	O
al	O
,	O
,	O
2013Sharp	O
et	O
al	O
,	O
2016	O
;	O
Verberne	O
et	O
al	O
,	O
2011	O
)	O
,	O
and	O
then	O
determine	O
whether	O
the	O
passages	O
answer	O
the	O
question	O
.	O
A	O
proper	O
answer	O
passage	O
must	O
contain	O
(	O
1	O
)	O
a	O
paraphrase	O
of	O
the	O
why	O
-	O
question	O
(	O
e.g.	O
,	O
the	O
underlined	O
texts	O
in	O
Table	O
1	O
)	O
and	O
(	O
2	O
)	O
the	O
reasons	O
or	O
the	O
causes	O
(	O
e.g.	O
,	O
the	O
bold	O
texts	O
in	O
Table	O
1	O
)	O
of	O
Q	O
Why	O
does	O
honey	O
last	O
a	O
long	O
time	O
?	O
A	O
While	O
excavating	O
Egypt	O
's	O
pyramids	O
,	O
archaeologists	O
have	O
found	O
pots	O
of	O
honey	O
in	O
an	O
ancient	O
tomb	O
:	O
thousands	O
of	O
years	O
old	O
and	O
still	O
preserved	O
.	O
Honey	O
can	O
last	O
a	O
long	O
time	O
due	O
to	O
three	O
special	O
properties	O
.	O
Its	O
average	O
pH	O
is	O
3.9	O
,	O
which	O
is	O
quite	O
acidic	O
.	O
Such	O
high	O
level	O
of	O
acidity	O
is	O
certainly	O
hostile	O
and	O
hinders	O
the	O
growth	O
of	O
many	O
microbes	O
.	O
Though	O
honey	O
contains	O
around	O
17	O
-	O
18	O
%	O
water	O
,	O
its	O
water	O
activity	O
is	O
too	O
low	O
to	O
support	O
the	O
growth	O
of	O
microbes	O
.	O
Moreover	O
honey	O
contains	O
hydrogen	O
peroxide	O
,	O
which	O
is	O
thought	O
to	O
help	O
prevent	O
the	O
growth	O
of	O
microbes	O
in	O
honey	O
.	O
Despite	O
these	O
properties	O
,	O
honey	O
can	O
be	O
contaminated	O
under	O
certain	O
circumstances	O
.	O
C	O
Because	O
its	O
acidity	O
,	O
low	O
water	O
activity	O
,	O
and	O
hydrogen	O
peroxide	O
together	O
hinder	O
the	O
growth	O
of	O
microbes	O
.	O
Table	O
1	O
:	O
Answer	O
passage	O
A	O
to	O
why	O
-	O
question	O
Q	O
and	O
its	O
compact	O
answer	O
C	O
the	O
events	O
described	O
in	O
the	O
why	O
-	O
question	O
,	O
both	O
of	O
which	O
are	O
often	O
written	O
in	O
multiple	O
non	O
-	O
adjacent	O
sentences	O
.	O
This	O
multi	O
-	O
sentenceness	O
implies	O
that	O
the	O
answer	O
passages	O
often	O
contain	O
redundant	O
parts	O
that	O
are	O
not	O
directly	O
related	O
to	O
a	O
why	O
-	O
question	O
or	O
its	O
reason	O
/	O
cause	O
and	O
whose	O
presence	O
complicates	O
the	O
why	O
-	O
QA	B-TaskName
task	O
.	O
Highly	O
accurate	O
why	O
-	O
QA	B-TaskName
methods	O
should	O
be	O
able	O
to	O
find	O
the	O
exact	O
reason	O
sought	O
by	O
a	O
why	O
-	O
question	O
in	O
an	O
answer	O
passage	O
without	O
being	O
distracted	O
by	O
redundancy	O
.	O
In	O
this	O
paper	O
,	O
we	O
train	O
a	O
neural	O
network	O
(	O
NN	O
)	O
to	O
generate	O
,	O
from	O
an	O
answer	O
passage	O
,	O
a	O
vector	O
representation	O
of	O
the	O
non	O
-	O
redundant	O
reason	O
asked	O
by	O
a	O
why	O
-	O
question	O
,	O
and	O
exploit	O
the	O
generated	O
vector	O
representation	O
as	O
evidence	O
for	O
judging	O
whether	O
the	O
passage	O
answers	O
the	O
why	O
-	O
question	O
.	O
This	O
idea	O
was	O
inspired	O
by	O
Ishida	O
et	O
al	O
(	O
2018	O
)	O
,	O
who	O
used	O
a	O
seq2seq	O
model	O
to	O
automatically	O
generate	O
such	O
compact	O
answers	O
as	O
C	O
in	O
Table	O
1	O
from	O
the	O
answer	O
passages	O
retrieved	O
by	O
a	O
why	O
-	O
QA	B-TaskName
method	O
.	O
Compact	O
answers	O
are	O
sentences	O
or	O
phrases	O
that	O
express	O
the	O
reasons	O
for	O
a	O
given	O
why	O
-	O
question	O
without	O
redundancy	O
.	O
If	O
we	O
can	O
use	O
such	O
automatically	O
generated	O
compact	O
-	O
answers	O
to	O
support	O
a	O
why	O
-	O
QA	B-TaskName
method	O
in	O
finding	O
the	O
exact	O
reason	O
of	O
a	O
whyquestion	O
in	O
these	O
passages	O
,	O
why	O
-	O
QA	O
accuracy	O
may	O
be	O
improved	O
.	O
We	O
actually	O
tried	O
this	O
idea	O
in	O
a	O
preliminary	O
study	O
in	O
which	O
we	O
generated	O
a	O
compact	O
answer	O
from	O
a	O
given	O
question	O
-	O
passage	O
pair	O
by	O
using	O
the	O
compact	O
-	O
answer	O
generation	O
method	O
of	O
Iida	O
et	O
al	O
(	O
2019	O
)	O
and	O
used	O
the	O
generated	O
compactanswer	O
along	O
with	O
the	O
given	O
question	O
-	O
passage	O
pair	O
to	O
find	O
proper	O
answer	O
passages	O
.	O
However	O
,	O
we	O
were	O
disappointed	O
by	O
the	O
small	O
performance	O
improvement	O
,	O
as	O
shown	O
in	O
our	O
experimental	O
results	O
.	O
We	O
chose	O
an	O
alternative	O
approach	O
.	O
Instead	O
of	O
generating	O
a	O
compact	O
answer	O
of	O
an	O
answer	O
passage	O
as	O
word	O
sequences	O
,	O
we	O
devised	O
a	O
model	O
to	O
generate	O
a	O
compact	O
-	O
answer	O
representation	O
,	O
which	O
is	O
a	O
vector	O
representation	O
for	O
a	O
compact	O
answer	O
,	O
from	O
an	O
answer	O
passage	O
.	O
Inspired	O
by	O
the	O
generative	B-MethodName
adversarial	I-MethodName
network	I-MethodName
(	I-MethodName
GAN	I-MethodName
)	I-MethodName
approach	O
(	O
Goodfellow	O
et	O
al	O
,	O
2014	O
)	O
,	O
we	O
developed	O
an	O
adversarial	O
network	O
called	O
the	O
Adversarial	O
networks	O
for	O
Generating	O
compact	O
-	O
answer	O
Representation	O
(	O
AGR	O
)	O
.	O
Like	O
the	O
original	O
GAN	B-MethodName
,	O
an	O
AGR	B-MethodName
is	O
composed	O
of	O
a	O
generator	O
and	O
a	O
discriminator	O
:	O
the	O
generator	O
network	O
is	O
trained	O
for	O
generating	O
(	O
from	O
answer	O
passages	O
)	O
fake	O
representations	O
to	O
make	O
it	O
hard	O
for	O
the	O
discriminator	O
network	O
to	O
distinguish	O
these	O
fake	O
representations	O
from	O
the	O
true	O
representations	O
derived	O
from	O
manually	O
created	O
compact	O
-	O
answers	O
.	O
We	O
combined	O
the	O
generator	O
network	O
in	O
the	O
AGR	B-MethodName
with	O
an	O
extension	O
of	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
why	O
-	O
QA	O
method	O
.	O
Our	O
evaluation	O
against	O
a	O
Japanese	O
open	O
-	O
domain	O
why	O
-	O
QA	B-TaskName
dataset	O
,	O
which	O
was	O
created	O
using	O
general	O
web	O
texts	O
as	O
a	O
source	O
of	O
answer	O
passages	O
,	O
revealed	O
that	O
the	O
generator	O
network	O
significantly	O
improved	O
the	O
accuracy	O
of	O
the	O
top	O
-	O
ranked	O
answer	O
passages	O
and	O
that	O
the	O
combination	O
significantly	O
outperformed	O
several	O
strong	O
baselines	O
,	O
including	O
a	O
combination	O
of	O
a	O
generator	O
network	O
and	O
a	O
BERT	B-MethodName
model	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
.	O
This	O
combination	O
also	O
outperformed	O
a	O
vanilla	O
BERT	O
model	O
,	O
suggesting	O
that	O
the	O
generator	O
network	O
in	O
our	O
AGR	B-MethodName
may	O
be	O
effective	O
even	O
if	O
it	O
is	O
combined	O
with	O
many	O
types	O
of	O
NN	O
architectures	O
.	O
Another	O
interesting	O
point	O
is	O
that	O
the	O
performance	O
improved	O
even	O
when	O
we	O
replaced	O
,	O
as	O
the	O
inputs	O
to	O
AGR	B-MethodName
,	O
the	O
word	O
embedding	O
vectors	O
that	O
represent	O
an	O
answer	O
passage	O
,	O
with	O
a	O
random	O
vector	O
.	O
This	O
observation	O
warrants	O
further	O
exploration	O
in	O
our	O
future	O
work	O
.	O
Finally	O
,	O
we	O
applied	O
our	O
AGR	O
to	O
a	O
distantly	O
su	O
-	O
(	O
Chen	O
et	O
al	O
,	O
2017	O
)	O
,	O
which	O
is	O
an	O
extension	O
of	O
a	O
machinereading	O
task	O
,	O
to	O
check	O
whether	O
it	O
is	O
applicable	O
to	O
other	O
datasets	O
.	O
We	O
combined	O
our	O
generator	O
network	O
with	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
DS	B-TaskName
-	I-TaskName
QA	I-TaskName
method	O
,	O
OpenQA	O
(	O
Lin	O
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
used	O
a	O
generated	O
compact	O
-	O
answer	O
representation	O
from	O
a	O
given	O
passage	O
as	O
evidence	O
to	O
1	O
)	O
select	O
relevant	O
passages	O
from	O
the	O
retrieved	O
ones	O
and	O
2	O
)	O
find	O
an	O
answer	O
from	O
the	O
selected	O
passages	O
.	O
Although	O
the	O
task	O
was	O
not	O
our	O
initial	O
target	O
(	O
why	O
-	O
QA	O
)	O
and	O
the	O
answers	O
in	O
the	O
DS	B-TaskName
-	I-TaskName
QA	I-TaskName
task	O
were	O
considerably	O
shorter	O
than	O
those	O
in	O
the	O
why	O
-	O
QA	B-TaskName
,	O
experiments	O
using	O
three	O
publicly	O
available	O
datasets	O
(	O
Quasar	B-DatasetName
-	I-DatasetName
T	I-DatasetName
(	O
Dhingra	O
et	O
al	O
,	O
2017	O
)	O
,	O
SearchQA	B-DatasetName
(	O
Dunn	O
et	O
al	O
,	O
2017	O
)	O
,	O
and	O
Triv	B-DatasetName
-	I-DatasetName
iaQA	I-DatasetName
(	O
Joshi	O
et	O
al	O
,	O
2017	O
)	O
)	O
revealed	O
that	O
the	O
generator	O
network	O
improved	O
the	O
performance	O
in	O
most	O
cases	O
.	O
This	O
suggests	O
that	O
AGR	B-MethodName
may	O
be	O
applicable	O
to	O
many	O
QA	B-TaskName
-	O
like	O
tasks	O
.	O
2	O
Why	O
-	O
QA	B-TaskName
Model	O
Figure	O
1	O
illustrates	O
the	O
architecture	O
of	O
our	O
why	O
-	O
QA	O
model	O
and	O
the	O
AGR	O
.	O
Our	O
why	O
-	O
QA	O
model	O
computes	O
the	O
probability	O
that	O
a	O
given	O
answer	O
passage	O
describes	O
a	O
proper	O
answer	O
to	O
a	O
given	O
why	O
-	O
question	O
using	O
the	O
representations	O
of	O
a	O
question	O
,	O
an	O
answer	O
passage	O
,	O
and	O
a	O
compact	O
answer	O
.	O
The	O
probability	O
(	O
the	O
why	O
-	O
QA	B-TaskName
model	O
's	O
final	O
output	O
)	O
is	O
computed	O
from	O
these	O
representations	O
by	O
our	O
answer	O
selection	O
module	O
,	O
which	O
is	O
a	O
logistic	O
regression	O
layer	O
with	O
dropout	O
and	O
softmax	O
output	O
.	O
The	O
representations	O
of	O
why	O
-	O
questions	O
and	O
answer	O
passages	O
are	O
generated	O
by	O
Convolutional	O
Neural	O
Networks	O
(	O
CNNs	O
)	O
(	O
Collobert	O
et	O
al	O
,	O
2011	O
;	O
LeCun	O
et	O
al	O
,	O
1998	O
)	O
that	O
(	O
1	O
)	O
are	O
augmented	O
by	O
two	O
types	O
of	O
attention	O
mechanisms	O
,	O
similarityattention	O
and	O
causality	O
-	O
attention	O
,	O
and	O
(	O
2	O
)	O
are	O
given	O
two	O
types	O
of	O
word	O
embeddings	O
,	O
general	O
word	O
embeddings	O
computed	O
by	O
word2vec	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
using	O
Wikipedia	O
and	O
causal	O
word	O
embeddings	O
(	O
Sharp	O
et	O
al	O
,	O
2016	O
)	O
.	O
Note	O
that	O
in	O
computing	O
a	O
question	O
's	O
representation	O
,	O
the	O
answer	O
passage	O
is	O
given	O
to	O
the	O
question	O
encoder	O
to	O
guide	O
the	O
computation	O
.	O
Likewise	O
the	O
passage	O
encoder	O
is	O
given	O
the	O
question	O
and	O
the	O
representation	O
of	O
the	O
compact	O
answer	O
.	O
We	O
represent	O
these	O
information	O
flows	O
with	O
dotted	O
arrows	O
in	O
Fig	O
.	O
1	O
(	O
a	O
)	O
.	O
The	O
representations	O
of	O
compact	O
answers	O
are	O
created	O
by	O
a	O
generator	O
network	O
called	O
a	O
fakerepresentation	O
generator	O
(	O
F	O
in	O
Fig	O
.	O
1	O
(	O
a	O
)	O
)	O
,	O
which	O
is	O
pre	O
-	O
trained	O
in	O
an	O
adversarial	O
learning	O
manner	O
(	O
Fig	O
.	O
1	O
(	O
b	O
)	O
)	O
.	O
During	O
the	O
training	O
of	O
the	O
whole	O
why	O
-	O
QA	B-TaskName
model	O
,	O
the	O
generator	O
's	O
parameters	O
are	O
fixed	O
and	O
no	O
further	O
fine	O
-	O
tuning	O
is	O
conducted	O
.	O
In	O
the	O
next	O
section	O
,	O
we	O
describe	O
our	O
main	O
contribution	O
:	O
the	O
AGR	O
and	O
the	O
fake	O
-	O
representation	O
generator	O
.	O
The	O
entire	O
why	O
-	O
QA	B-TaskName
model	O
can	O
be	O
seen	O
as	O
an	O
extension	O
of	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
why	O
-	O
QA	B-TaskName
method	O
.	O
Its	O
details	O
are	O
described	O
in	O
Section	O
A	O
of	O
the	O
supplementary	O
materials	O
.	O

We	O
analyzed	O
examples	O
for	O
which	O
ASR	O
is	O
correct	O
and	O
PR	O
is	O
not	O
.	O
Tab	O
.	O
7	O
shows	O
that	O
,	O
given	O
q	B-HyperparameterName
and	O
k	B-HyperparameterName
=	O
3	B-HyperparameterValue
candidates	O
,	O
PR	B-MethodName
chooses	O
c	O
1	O
,	O
a	O
suitable	O
but	O
wrong	O
answer	O
.	O
This	O
probably	O
happens	O
since	O
the	O
answer	O
best	O
matches	O
the	O
syntactic	O
/	O
semantic	O
pattern	O
of	O
the	O
question	O
,	O
which	O
asks	O
for	O
a	O
type	O
of	O
color	O
,	O
indeed	O
,	O
the	O
answer	O
offers	O
such	O
type	O
,	O
primary	O
colors	O
.	O
PR	O
does	O
not	O
rely	O
on	O
any	O
background	O
information	O
that	O
can	O
support	O
the	O
set	O
of	O
colors	O
in	O
the	O
answer	O
.	O
In	O
contrast	O
,	O
ASR	B-MethodName
selects	O
c	O
2	O
as	O
it	O
can	O
rely	O
on	O
the	O
support	O
of	O
other	O
answers	O
.	O
Its	O
ASC	O
provides	O
an	O
average	O
score	O
for	O
the	O
category	O
0	O
(	O
both	O
members	O
are	O
correct	O
)	O
of	O
c	O
2	O
,	O
i.e.	O
,	O
1	O
k	O
i	O
=	O
2	O
ASC	O
(	O
c	O
2	O
,	O
c	O
i	O
)	O
=	O
0.653	O
,	O
while	O
for	O
c	O
1	O
the	O
average	B-MetricName
score	I-MetricName
is	O
significant	O
lower	O
,	O
i.e.	O
,	O
0.522	B-MetricValue
.	O
This	O
provides	O
higher	O
support	O
for	O
c	O
2	O
,	O
which	O
is	O
used	O
by	O
ASR	B-MethodName
to	O
rerank	O
the	O
output	O
of	O
PR	O
.	O
Tab	O
.	O
8	O
shows	O
an	O
interesting	O
case	O
where	O
all	O
the	O
sentences	O
contain	O
the	O
required	O
information	O
,	O
i.e.	O
,	O
February	O
.	O
However	O
,	O
PR	B-MethodName
and	O
ASR	B-MethodName
both	O
choose	O
answer	O
c	O
0	O
,	O
which	O
is	O
correct	O
but	O
not	O
natural	O
,	O
as	O
it	O
provides	O
the	O
requested	O
information	O
indirectly	O
.	O
Also	O
,	O
it	O
contains	O
a	O
lot	O
of	O
ancillary	O
information	O
.	O
In	O
contrast	O
,	O
MASR	B-MetricName
is	O
able	O
to	O
rerank	O
the	O
best	O
answer	O
,	O
c	O
1	O
,	O
in	O
the	O
top	O
position	O
.	O

As	O
the	O
state	O
of	O
the	O
art	O
for	O
AS2	O
is	O
obtained	O
using	O
RoBERTa	B-MethodName
Large	O
,	O
we	O
trained	O
KGAT	B-MethodName
and	O
ASR	O
using	O
this	O
pre	O
-	O
trained	O
language	O
model	O
.	O
Table	O
5	O
also	O
reports	O
the	O
comparison	O
with	O
PR	O
,	O
which	O
is	O
the	O
official	O
state	O
of	O
the	O
art	O
.	O
Again	O
,	O
our	O
PR	O
replicates	O
the	O
results	O
of	O
Garg	O
et	O
al	O
(	O
2020	O
)	O
,	O
obtaining	O
slightly	O
lower	O
performance	O
on	O
WikiQA	B-DatasetName
but	O
higher	O
on	O
TREC	B-DatasetName
-	I-DatasetName
QA	I-DatasetName
.	O
KGAT	B-MethodName
performs	O
lower	O
than	O
PR	O
on	O
both	O
datasets	O
.	O
ASR	O
establishes	O
the	O
new	O
state	O
of	O
the	O
art	O
on	O
WikiQA	B-DatasetName
with	O
an	O
MAP	B-MetricName
of	O
92.80	O
vs.	O
92.00	O
.	O
The	O
P@1	O
also	O
significantly	O
improves	O
by	O
2	O
%	O
,	O
i.e.	O
,	O
achieving	O
89.71	O
,	O
which	O
is	O
impressively	O
high	O
.	O
Also	O
,	O
on	O
TREC	B-DatasetName
-	I-DatasetName
QA	I-DatasetName
,	O
ASR	O
outperforms	O
all	O
models	O
,	O
being	O
on	O
par	O
with	O
PR	O
regarding	B-MetricName
P@1	I-MetricName
.	O
The	O
latter	O
is	O
97.06	O
,	O
which	O
corresponds	O
to	O
mistaking	O
the	O
answers	O
of	O
only	O
two	O
questions	O
.	O
We	O
manually	O
checked	O
these	O
and	O
found	O
out	O
that	O
these	O
were	O
two	O
annotation	O
errors	O
:	O
ASR	O
achieves	O
perfect	O
accuracy	B-MetricName
while	O
PR	O
only	O
mistakes	O
one	O
answer	O
.	O
Of	O
course	O
,	O
this	O
just	O
provides	O
evidence	O
that	O
PR	O
based	O
on	O
RoBERTa	B-MethodName
-	O
Large	O
solves	O
the	O
task	O
of	B-TaskName
selecting	I-TaskName
the	I-TaskName
best	I-TaskName
answers	I-TaskName
(	O
i.e.	O
,	O
measuring	O
P@1	B-MetricName
on	O
this	O
dataset	O
is	O
not	O
meaningful	O
anymore	O
)	O
.	O
Sec	O
.	O
4.1	O
)	O
.	O
ACC	O
is	O
the	O
overall	B-MetricName
accuracy	I-MetricName
while	O
F1	B-MetricName
refers	O
to	O
the	O
category	O
0	O
.	O
We	O
note	O
that	O
ASC	O
in	O
MASR	B-MetricName
-	I-MetricName
FP	I-MetricName
achieves	O
the	O
highest	O
accuracy	B-MetricName
with	O
respect	O
to	O
the	O
average	O
over	O
all	O
datasets	O
.	O
This	O
happens	O
since	O
we	O
pre	O
-	O
fine	O
-	O
tuned	O
it	O
with	O
the	O
FEVER	B-DatasetName
data	O
.	O

Table	O
4	O
reports	O
the	O
P@1	O
,	O
MAP	B-MetricName
and	O
MRR	B-MetricName
of	O
the	O
rerankers	O
,	O
and	O
different	O
answer	O
supporting	O
models	O
on	O
WikiQA	B-DatasetName
,	O
TREC	B-DatasetName
-	I-DatasetName
QA	I-DatasetName
and	O
WQA	B-DatasetName
datasets	O
.	O
As	O
WQA	B-DatasetName
is	O
an	O
internal	O
dataset	O
,	O
we	O
only	O
report	O
the	O
improvement	O
over	O
PR	O
in	O
the	O
tables	O
.	O
All	O
models	O
use	O
RoBERTa	B-MethodName
-	O
Base	O
pre	O
-	O
trained	O
checkpoint	O
and	O
start	O
from	O
the	O
same	O
set	O
of	O
k	O
candidates	O
reranked	O
by	O
PR	O
(	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
model	O
)	O
.	O
The	O
table	O
shows	O
that	O
:	O
PR	O
replicates	O
the	O
MAP	B-MetricName
and	O
MRR	B-MetricName
of	O
the	O
stateof	O
-	O
the	O
-	O
art	O
reranker	O
by	O
Garg	O
et	O
al	O
(	O
2020	O
)	O
on	O
WikiQA	B-DatasetName
.	O
Joint	O
Model	O
Multi	O
-	O
classifier	O
performs	O
lower	O
than	O
PR	O
for	O
all	O
measures	O
and	O
all	O
datasets	O
.	O
This	O
is	O
in	O
line	O
with	O
the	O
findings	O
of	O
Bonadiman	O
and	O
Moschitti	O
(	O
2020	O
)	O
,	O
who	O
also	O
did	O
not	O
obtain	O
improvement	O
when	O
jointly	O
used	O
all	O
the	O
candidates	O
altogether	O
in	O
a	O
representation	O
.	O
Joint	O
Model	O
Pairwise	O
differs	O
from	O
ASR	O
as	O
it	O
concatenates	O
the	O
embeddings	O
of	O
the	O
(	O
q	O
,	O
c	O
i	O
)	O
,	O
instead	O
of	O
using	O
max	O
-	O
pooling	O
,	O
and	O
does	O
not	O
use	O
any	O
Answer	O
Support	O
Classifier	O
(	O
ASC	O
)	O
.	O
Still	O
,	O
it	O
exploits	O
the	O
idea	O
of	O
aggregating	O
the	O
information	O
of	O
all	O
pairs	O
(	O
q	O
,	O
c	O
i	O
)	O
with	O
respect	O
to	O
a	O
target	O
answer	O
t	O
,	O
which	O
proves	O
to	O
be	O
effective	O
,	O
as	O
the	O
model	O
improves	O
on	O
PR	O
over	O
all	O
measures	O
and	O
datasets	O
.	O
Our	O
KGAT	B-MethodName
version	O
for	O
AS2	O
also	O
improves	O
PR	O
over	O
all	O
datasets	O
and	O
almost	O
all	O
measures	O
,	O
confirming	O
that	O
the	O
idea	O
of	O
using	O
candidates	O
as	O
support	O
of	O
the	O
target	O
answer	O
is	O
generally	O
valid	O
.	O
However	O
,	O
it	O
is	O
not	O
superior	O
to	O
Joint	O
Model	O
Pairwise	O
.	O
ASR	O
achieves	O
the	O
highest	O
performance	O
among	O
all	O
models	O
(	O
but	O
MASR	B-MethodName
-	I-MethodName
FP	I-MethodName
on	O
WQA	B-DatasetName
)	O
,	O
all	O
datasets	O
,	O
and	O
all	O
measures	O
.	O
For	O
example	O
,	O
it	O
outperforms	O
PR	O
by	O
almost	O
3	O
absolute	O
percent	O
points	O
in	O
P@1	O
on	O
WikiQA	B-DatasetName
,	O
and	O
by	O
almost	O
6	O
points	O
on	O
TREC	B-DatasetName
from	O
91.18	O
%	O
to	O
97.06	O
%	O
,	O
which	O
corresponds	O
to	O
an	O
error	B-MetricName
reduction	I-MetricName
of	B-MetricValue
60	I-MetricValue
%	O
.	O
We	O
perform	O
randomization	O
test	O
(	O
Yeh	O
,	O
2000	O
)	O
to	O
verify	O
if	O
the	O
models	O
significantly	O
differ	O
in	O
terms	O
of	O
prediction	O
outcome	O
.	O
We	O
use	O
100	O
,	O
000	O
trials	O
for	O
each	O
calculation	O
.	O
The	O
results	O
confirm	O
the	O
statistically	O
significant	O
difference	O
between	O
ASR	O
and	O
all	O
the	O
baselines	O
,	O
with	O
p	O
<	O
0.05	O
for	O
WikiQA	O
,	O
and	O
between	O
ASR	O
and	O
all	O
models	O
(	O
i.e.	O
,	O
including	O
also	O
KGAT	B-MethodName
)	O
on	O
WQA	B-DatasetName
.	O

Metrics	O
The	O
performance	O
of	O
QA	O
systems	O
is	O
typically	O
measured	O
with	O
Accuracy	B-MetricName
in	O
providing	O
correct	O
answers	O
,	O
i.e.	O
,	O
the	O
percentage	B-MetricName
of	I-MetricName
correct	I-MetricName
responses	I-MetricName
.	O
This	O
is	O
also	O
referred	O
to	O
Precision	B-MetricName
-	O
at	O
-	O
1	O
(	O
P@1	O
)	O
in	O
the	O
context	O
of	O
reranking	O
,	O
while	O
standard	O
Precision	B-MetricName
and	O
Recall	B-MetricName
are	O
not	O
essential	O
in	O
our	O
case	O
as	O
we	O
assume	O
the	O
system	O
does	O
not	O
abstain	O
from	O
providing	O
answers	O
.	O
We	O
also	O
use	O
Mean	B-MetricName
Average	I-MetricName
Precision	I-MetricName
(	I-MetricName
MAP	I-MetricName
)	I-MetricName
and	O
Mean	B-MetricName
Reciprocal	I-MetricName
Recall	I-MetricName
(	I-MetricName
MRR	I-MetricName
)	I-MetricName
evaluated	O
on	O
the	O
test	O
set	O
,	O
using	O
the	O
entire	O
set	O
of	O
candidates	O
for	O
each	O
Table	O
4	O
:	O
Results	O
on	O
WikiQA	B-DatasetName
,	O
TREC	B-DatasetName
-	I-DatasetName
QA	I-DatasetName
and	O
WQA	B-DatasetName
,	O
using	O
RoBERTa	B-MethodName
base	O
Transformer	O
.	O
†	O
is	O
used	O
to	O
indicate	O
that	O
the	O
difference	O
in	O
P@1	O
between	O
ASR	O
and	O
the	O
other	O
marked	O
systems	O
is	O
statistically	O
significant	O
at	O
95	O
%	O
.	O
JOINT	O
-	O
MULTICLASSIFER	O
JOINT	O
-	O
PAIR	O
KGAT	O
ASR	O
1	O
2	O
3	O
4	O
5	O
−3	O
−2	O
−1	O
0	O
1	O
2	O
3	O
4	O
5	O
k	O
Improvement	O
(	O
%	O
)	O
Figure	O
2	O
:	O
Impact	O
of	O
k	O
on	O
the	O
WQA	B-DatasetName
dev	O
.	O
set	O
question	O
(	O
this	O
varies	O
according	O
to	O
the	O
dataset	O
)	O
,	O
to	O
have	O
a	O
direct	O
comparison	O
with	O
the	O
state	O
of	O
the	O
art	O
.	O
Models	O
We	O
use	O
the	O
pre	O
-	O
trained	O
RoBERTa	B-MethodName
-	O
Base	O
(	O
12	O
layer	O
)	O
and	O
RoBERTa	O
-	O
Large	O
-	O
MNLI	O
(	O
24	O
layer	O
)	O
models	O
,	O
which	O
were	O
released	O
as	O
checkpoints	O
for	O
use	O
in	O
downstream	O
tasks	O
4	O
.	O
Reranker	O
training	O
We	O
adopt	O
Adam	B-HyperparameterValue
optimizer	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2e	B-HyperparameterValue
-	I-HyperparameterValue
5	I-HyperparameterValue
for	O
the	O
transfer	O
step	O
on	O
the	O
ASNQ	B-DatasetName
dataset	O
(	O
Garg	O
et	O
al	O
,	O
2020	O
)	O
,	O
and	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
1e	B-HyperparameterValue
-	I-HyperparameterValue
6	I-HyperparameterValue
for	O
the	O
adapt	O
step	O
on	O
the	O
target	O
dataset	O
.	O
We	O
apply	O
early	B-HyperparameterName
stopping	I-HyperparameterName
on	O
the	O
development	O
set	O
of	O
the	O
target	O
corpus	O
for	O
both	O
fine	O
-	O
tuning	O
steps	O
based	O
on	O
the	O
highest	O
MAP	B-MetricName
score	O
.	O
We	O
set	O
the	O
max	B-HyperparameterName
number	I-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
equal	O
to	O
3	B-HyperparameterValue
and	O
9	B-HyperparameterValue
for	O
the	O
adapt	O
and	O
transfer	O
steps	O
,	O
respectively	O
.	O
We	O
set	O
the	O
maximum	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
for	O
RoBERTa	B-MethodName
to	O
128	B-HyperparameterValue
tokens	O
.	O
KGAT	B-MethodName
and	O
ASR	B-MethodName
training	O
Again	O
,	O
we	O
use	O
the	O
Adam	B-HyperparameterValue
optimizer	B-HyperparameterName
with	O
a	B-HyperparameterName
learning	I-HyperparameterName
rate	I-HyperparameterName
of	O
2e	B-HyperparameterValue
-	I-HyperparameterValue
6	I-HyperparameterValue
for	O
training	O
the	O
ASR	O
model	O
on	O
the	O
target	O
dataset	O
.	O
We	O
utilize	O
1	O
Tesla	O
V100	O
GPU	O
with	O
32	O
GB	O
memory	O
and	O
a	O
train	O
batch	O
size	O
of	O
eight	O
.	O
We	O
set	O
the	O
maximum	O
sequence	O
length	O
for	O
RoBERTa	B-MethodName
Base	O
/	O
Large	O
to	O
130	O
tokens	O
and	O
the	O
number	O
of	O
training	B-HyperparameterName
epochs	I-HyperparameterName
to	O
20	B-HyperparameterValue
.	O
The	O
other	O
training	O
configurations	O
are	O
the	O
same	O
of	O
the	O
original	O
KGAT	B-MethodName
model	O
from	O
(	O
Liu	O
et	O
al	O
,	O
2020	O
)	O
.	O
We	O
use	O
two	O
transformer	O
models	O
for	O
ASR	O
:	O
a	O
RoBERTa	B-MethodName
4	O
https://github.com/pytorch/fairseq	O
Base	O
/	O
Large	O
for	O
PR	O
,	O
and	O
one	O
for	O
ASC	O
.	O
We	O
set	O
the	B-HyperparameterName
maximum	I-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
for	O
RoBERTa	B-MethodName
to	O
128	B-HyperparameterValue
tokens	O
and	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
to	O
20	B-HyperparameterValue
.	O

The	O
goal	O
of	O
MASR	B-MethodName
is	O
to	O
measure	O
the	O
relation	O
between	O
k	O
+	O
1	O
target	O
answers	O
,	O
t	O
0	O
,	O
..	O
,	O
t	O
k	O
.	O
The	O
representation	O
of	O
each	O
target	O
answer	O
is	O
the	O
embedding	O
V	O
R	O
2d	O
from	O
Equation	O
1	O
in	O
ASR	O
.	O
Then	O
,	O
we	O
concatenate	O
the	O
hidden	O
vectors	O
of	O
k	O
+	O
1	O
target	O
answers	O
to	O
form	O
a	O
matrix	O
V	O
(	O
q	O
,	O
k+1	O
)	O
R	O
(	O
k+1	O
)	O
×2d	O
.	O
We	O
use	O
this	O
matrix	O
and	O
a	O
classification	O
layer	O
weights	O
W	O
R	O
2d	O
,	O
and	O
compute	O
a	O
standard	O
multi	B-MetricName
-	I-MetricName
class	I-MetricName
classification	I-MetricName
loss	I-MetricName
:	O
L	O
M	O
ASR	O
=	O
y	O
*	O
log	O
(	O
sof	O
tmax	O
(	O
V	O
(	O
q	O
,	O
k+1	O
)	O
W	O
T	O
)	O
,	O
(	O
2	O
)	O
where	O
y	O
is	O
a	O
one	O
-	O
hot	O
-	O
vector	O
,	O
and	O
|	O
y	O
|	O
=	O
|	O
k	O
+	O
1	O
|	O
.	O

We	O
developed	O
ASR	O
architecture	O
described	O
in	O
Figure	O
2	O
.	O
To	O
reduce	O
the	O
noise	O
that	O
may	O
be	O
introduced	O
by	O
irrelevant	O
c	O
i	O
,	O
we	O
use	O
the	O
Answer	B-MethodName
Support	I-MethodName
Classifier	I-MethodName
(	I-MethodName
ASC	I-MethodName
)	I-MethodName
,	O
which	O
classifies	O
each	O
(	O
t	O
,	O
c	O
i	O
)	O
in	O
one	O
of	O
the	O
following	O
four	O
classes	O
:	O
0	O
:	O
t	O
and	O
c	O
i	O
are	O
both	O
correct	O
,	O
1	O
:	O
t	O
is	O
correct	O
while	O
c	O
i	O
is	O
not	O
,	O
2	O
:	O
vice	O
versa	O
,	O
and	O
3	O
:	O
both	O
incorrect	O
.	O
This	O
multi	O
-	O
classifier	O
,	O
described	O
in	O
Figure	O
1b	O
,	O
is	O
built	O
on	O
top	O
a	O
RoBERTa	B-MethodName
Transformer	O
,	O
which	O
produced	O
a	O
PairWise	O
Representation	O
(	O
PWR	O
)	O
.	O
ASC	O
is	O
trained	O
end	O
-	O
to	O
-	O
end	O
with	O
the	O
rest	O
of	O
the	O
network	O
in	O
a	O
multi	O
-	O
task	O
learning	O
fashion	O
,	O
using	O
its	O
specific	O
cross	B-MetricName
-	I-MetricName
entropy	I-MetricName
loss	I-MetricName
,	O
computed	O
with	O
the	O
labels	O
above	O
.	O
3	O
.	O
The	O
ASR	O
(	O
see	O
Figure	O
1c	O
)	O
uses	O
the	O
joint	O
representation	O
of	O
(	O
q	O
,	O
t	O
)	O
with	O
(	O
t	O
,	O
c	O
i	O
)	O
,	O
i	O
=	O
1	O
,	O
..	O
,	O
k	O
,	O
where	O
t	O
and	O
c	O
i	O
are	O
the	O
top	O
-	O
candidates	O
reranked	O
by	O
PR	O
.	O
The	O
k	O
representations	O
are	O
summarized	O
by	O
applying	O
a	O
max	O
-	O
pooling	O
operation	O
,	O
which	O
will	O
aggregate	O
all	O
the	O
supporting	O
or	O
not	O
supporting	O
properties	O
of	O
the	O
candidates	O
with	O
respect	O
to	O
the	O
target	O
answer	O
.	O
The	O
concatenation	O
of	O
the	O
PR	O
embedding	O
with	O
the	O
max	O
-	O
pooling	O
embedding	O
is	O
given	O
as	O
input	O
to	O
the	O
final	O
classification	O
layer	O
,	O
which	O
scores	O
t	O
with	O
respect	O
to	O
q	O
,	O
also	O
using	O
the	O
information	O
from	O
the	O
other	O
candidates	O
.	O
For	O
training	O
and	O
testing	O
,	O
we	O
select	O
a	O
t	O
from	O
the	O
k	O
+	O
1	O
candidates	O
of	O
q	O
at	O
a	O
time	O
,	O
and	O
compute	O
its	O
score	O
.	O
This	O
way	O
,	O
we	O
can	O
rerank	O
all	O
the	O
k	O
+	O
1	O
candidates	O
with	O
their	O
scores	O
.	O
Implementation	O
details	O
:	O
ASR	O
is	O
a	O
PR	O
that	O
also	O
exploits	O
the	O
relation	O
between	O
t	O
and	O
A	O
\	O
{	O
t	O
}	O
.	O
We	O
use	O
RoBERTa	B-MethodName
to	O
generate	O
the	O
[	O
CLS	O
]	O
R	O
d	O
embedding	O
of	O
(	O
q	O
,	O
t	O
)	O
=	O
E	O
t	O
.	O
We	O
denote	O
withÊ	O
j	O
the	O
[	O
CLS	O
]	O
output	O
by	O
another	O
RoBERTa	B-MethodName
Transformer	O
applied	O
to	O
answer	O
pairs	O
,	O
i.e.	O
,	O
(	O
t	O
,	O
c	O
j	O
)	O
.	O
Then	O
,	O
we	O
concatenate	O
E	O
t	O
to	O
the	O
max	O
-	O
pooling	O
tensor	O
fromÊ	O
1	O
,	O
..	O
,	O
Ê	O
k	O
:	O
V	O
=	O
[	O
E	O
t	O
:	O
Maxpool	O
(	O
[	O
Ê	O
1	O
,	O
..	O
,	O
Ê	O
k	O
]	O
)	O
]	O
,	O
(	O
1	O
)	O
where	O
V	O
R	O
2d	O
is	O
the	O
final	O
representation	O
of	O
the	O
target	O
answer	O
t.	O
Then	O
,	O
we	O
use	O
a	O
standard	O
feedforward	O
network	O
to	O
implement	O
a	O
binary	O
classification	O
layer	O
:	O
p	O
(	O
y	O
i	O
|	O
q	O
,	O
t	O
,	O
C	O
k	O
)	O
=	O
sof	O
tmax	O
(	O
V	O
W	O
T	O
+	O
B	O
)	O
,	O
where	O
W	O
R	O
2×2d	O
and	O
B	O
are	O
parameters	O
to	O
transform	O
the	O
representation	O
of	O
the	O
target	O
answer	O
t	O
from	O
dimension	O
2d	O
to	O
dimension	O
2	O
,	O
which	O
represents	O
correct	O
or	O
incorrect	O
labels	O
.	O
ASC	B-MethodName
labels	O
There	O
can	O
be	O
different	O
interpretations	O
when	O
attempting	O
to	O
define	O
labels	O
for	O
answer	O
pairs	O
.	O
An	O
alternative	O
to	O
the	O
definition	O
illustrated	O
above	O
is	O
to	O
use	O
the	O
following	O
FEVER	B-MethodName
compatible	O
encoding	O
:	O
0	O
:	O
t	O
is	O
correct	O
,	O
while	O
c	O
i	O
can	O
be	O
any	O
value	O
,	O
as	O
also	O
an	O
incorrect	O
c	O
i	O
may	O
provide	O
important	O
context	O
(	O
corresponding	O
to	O
FEVER	B-MethodName
Support	O
label	O
)	O
;	O
1	O
:	O
t	O
is	O
incorrect	O
,	O
c	O
i	O
correct	O
,	O
since	O
c	O
i	O
can	O
provide	O
evidence	O
that	O
t	O
is	O
not	O
similar	O
to	O
a	O
correct	O
answer	O
(	O
corresponding	O
to	O
FEVER	B-MethodName
Refutal	O
label	O
)	O
;	O
and	O
2	O
:	O
both	O
are	O
incorrect	O
,	O
in	O
this	O
case	O
,	O
nothing	O
can	O
be	O
told	O
(	O
corresponding	O
to	O
FEVER	B-MethodName
Neutral	O
label	O
)	O
.	O

The	O
first	O
baseline	O
is	O
also	O
a	O
Transformer	O
-	O
based	O
architecture	O
:	O
we	O
concatenate	O
the	O
question	O
with	O
the	O
top	O
k	O
+	O
1	O
answer	O
can	O
-	O
didates	O
,	O
i.e.	O
,	O
(	O
q	O
[	O
SEP	O
]	O
c	O
1	O
[	O
SEP	O
]	O
c	O
2	O
.	O
.	O
.	O
[	O
SEP	O
]	O
c	O
k+1	O
)	O
,	O
and	O
provide	O
this	O
input	O
to	O
the	O
same	O
Transformer	O
model	O
used	O
for	O
pointwise	O
reranking	O
.	O
We	O
use	O
the	O
final	O
hidden	O
vector	O
E	O
corresponding	O
to	O
the	O
first	O
input	O
token	O
[	O
CLS	O
]	O
generated	O
by	O
the	O
Transformer	O
,	O
and	O
a	O
classification	O
layer	O
with	O
weights	O
W	O
R	O
(	O
k+1	O
)	O
×	O
|	O
E	O
|	O
,	O
and	O
train	O
the	O
model	O
using	O
a	O
standard	O
cross	B-MetricName
-	I-MetricName
entropy	I-MetricName
classification	I-MetricName
loss	I-MetricName
:	O
y	O
×	O
log	O
(	O
sof	O
tmax	O
(	O
EW	O
T	O
)	O
)	O
,	O
where	O
y	O
is	O
a	O
one	O
-	O
hot	O
vector	O
representing	O
labels	O
for	O
the	O
k	O
+	O
1	O
candidates	O
,	O
i.e.	O
,	O
|	O
y	O
|	O
=	O
k	O
+	O
1	O
.	O
We	O
use	O
a	O
transformer	O
model	O
fine	O
-	O
tuned	O
with	O
the	O
TANDA	B-MethodName
-	I-MethodName
RoBERTa	I-MethodName
-	O
base	O
or	O
large	O
models	O
,	O
i.e.	O
,	O
RoBERTa	B-MethodName
models	O
fine	O
-	O
tuned	O
on	O
ASNQ	B-TaskName
(	O
Garg	O
et	O
al	O
,	O
2020	O
)	O
.	O
The	O
scores	O
for	O
the	O
candidate	O
answers	O
are	O
calculated	O
as	O
p	O
(	O
c	O
1	O
)	O
,	O
..	O
,	O
p	O
(	O
c	O
k+1	O
)	O
=	O
sof	O
tmax	O
(	O
EW	O
T	O
)	O
.	O
Then	O
,	O
we	O
rerank	O
c	O
i	O
according	O
their	O
probability	O
.	O
Joint	O
Model	O
Pairwise	O
Our	O
second	O
baseline	O
is	O
similar	O
to	O
the	O
first	O
.	O
We	O
concatenate	O
the	O
question	O
with	O
each	O
c	O
i	O
to	O
constitute	O
the	O
(	O
q	O
,	O
c	O
i	O
)	O
pairs	O
,	O
which	O
are	O
input	O
to	O
the	O
Transformer	O
,	O
and	O
we	O
use	O
the	O
first	O
input	O
token	O
[	O
CLS	O
]	O
as	O
the	O
representation	O
of	O
each	O
(	O
q	O
,	O
c	O
i	O
)	O
pair	O
.	O
Then	O
,	O
we	O
concatenate	O
the	O
embedding	O
of	O
the	O
pair	O
containing	O
the	O
target	O
candidate	O
,	O
(	O
q	O
,	O
t	O
)	O
with	O
the	O
embedding	O
of	O
all	O
the	O
other	O
candidates	O
'	O
[	O
CLS	O
]	O
.	O
(	O
q	O
,	O
t	O
)	O
is	O
always	O
in	O
the	O
first	O
position	O
.	O
We	O
train	O
the	O
model	O
using	O
a	O
standard	O
classification	O
loss	O
.	O
At	O
classification	O
time	O
,	O
we	O
select	O
one	O
target	O
candidate	O
at	O
a	O
time	O
,	O
and	O
set	O
it	O
in	O
the	O
first	O
position	O
,	O
followed	O
by	O
all	O
the	O
others	O
.	O
We	O
classify	O
all	O
k	O
+	O
1	O
candidates	O
and	O
use	O
their	O
score	O
for	O
reranking	O
them	O
.	O
It	O
should	O
be	O
noted	O
that	O
to	O
qualify	O
for	O
a	O
pairwise	O
approach	O
,	O
Joint	O
Model	O
Pairwise	O
should	O
use	O
a	O
ranking	O
loss	O
.	O
However	O
,	O
we	O
always	O
use	O
standard	O
cross	O
-	O
entropy	O
loss	O
as	O
it	O
is	O
more	O
efficient	O
and	O
the	O
different	O
is	O
performance	O
is	O
negligible	O
.	O
Joint	O
Model	O
with	O
KGAT	B-MethodName
Liu	O
et	O
al	O
(	O
2020	O
)	O
presented	O
an	O
interesting	O
model	O
,	O
Kernel	B-MethodName
Graph	I-MethodName
Attention	I-MethodName
Network	I-MethodName
(	I-MethodName
KGAT	I-MethodName
)	I-MethodName
,	O
for	O
fact	B-TaskName
verification	I-TaskName
:	O
given	O
a	O
claimed	O
fact	O
f	O
,	O
and	O
a	O
set	O
of	O
evidences	O
Ev	O
=	O
{	O
ev	O
1	O
,	O
ev	O
2	O
,	O
.	O
.	O
.	O
,	O
ev	O
m	O
}	O
,	O
their	O
model	O
carries	O
out	O
joint	O
reasoning	O
over	O
Ev	O
,	O
e.g.	O
,	O
aggregating	O
information	O
to	O
estimate	O
the	O
probability	O
of	O
f	O
to	O
be	O
true	O
or	O
false	O
,	O
p	O
(	O
y	O
|	O
f	O
,	O
Ev	O
)	O
,	O
where	O
y	O
{	O
true	O
,	O
false	O
}	O
.	O
The	O
approach	O
is	O
based	O
on	O
a	O
fully	O
connected	O
graph	O
,	O
G	O
,	O
whose	O
nodes	O
are	O
the	O
n	O
i	O
=	O
(	O
f	O
,	O
ev	O
i	O
)	O
pairs	O
,	O
and	O
p	O
(	O
y	O
|	O
f	O
,	O
Ev	O
)	O
=	O
p	O
(	O
y	O
|	O
f	O
,	O
ev	O
i	O
,	O
Ev	O
)	O
p	O
(	O
ev	O
i	O
|	O
f	O
,	O
Ev	O
)	O
,	O
where	O
p	O
(	O
y	O
|	O
f	O
,	O
ev	O
i	O
,	O
Ev	O
)	O
=	O
p	O
(	O
y	O
|	O
n	O
i	O
,	O
G	O
)	O
is	O
the	O
label	O
probability	O
in	O
each	O
node	O
i	O
conditioned	O
on	O
the	O
whole	O
graph	O
,	O
and	O
p	O
(	O
ev	O
i	O
|	O
f	O
,	O
Ev	O
)	O
=	O
p	O
(	O
n	O
i	O
|	O
G	O
)	O
is	O
the	O
probability	O
of	O
selecting	O
the	O
most	O
informative	O
evidence	O
.	O
KGAT	B-MethodName
uses	O
an	O
edge	O
kernel	O
to	O
perform	O
a	O
hierarchi	O
-	O
cal	O
attention	O
mechanism	O
,	O
which	O
propagates	O
information	O
between	O
nodes	O
and	O
aggregate	O
evidences	O
.	O
We	O
built	O
a	O
KGAT	B-MethodName
model	O
for	O
AS2	O
as	O
follows	O
:	O
we	O
replace	O
(	O
i	O
)	O
ev	O
i	O
with	O
the	O
set	O
of	O
candidate	O
answers	O
c	O
i	O
,	O
and	O
(	O
ii	O
)	O
the	O
claim	O
f	O
with	O
the	O
question	O
and	O
a	O
target	O
answer	O
pair	O
,	O
(	O
q	O
,	O
t	O
)	O
.	O
KGAT	B-MethodName
constructs	O
the	O
evidence	O
graph	O
G	O
by	O
using	O
each	O
claim	O
-	O
evidence	O
pair	O
as	O
a	O
node	O
,	O
which	O
,	O
in	O
our	O
case	O
,	O
is	O
(	O
(	O
q	O
,	O
t	O
)	O
,	O
c	O
i	O
)	O
,	O
and	O
connects	O
all	O
node	O
pairs	O
with	O
edges	O
,	O
making	O
it	O
a	O
fully	O
-	O
connected	O
evidence	O
graph	O
.	O
This	O
way	O
,	O
sentence	O
and	O
token	O
attention	O
operate	O
over	O
the	O
triplets	O
,	O
(	O
q	O
,	O
t	O
,	O
c	O
i	O
)	O
,	O
establishing	O
semantic	O
links	O
,	O
which	O
can	O
help	O
to	O
support	O
or	O
undermine	O
the	O
correctness	O
of	O
t.	O
The	O
original	O
KGAT	B-MethodName
aggregates	O
all	O
the	O
pieces	O
of	O
information	O
we	O
built	O
,	O
based	O
on	O
their	O
relevance	O
,	O
to	O
determine	O
the	O
probability	O
of	O
t.	O
As	O
we	O
use	O
AS2	O
data	O
,	O
the	O
probability	O
will	O
be	O
about	O
the	O
correctness	O
of	O
t.	O
More	O
in	O
detail	O
,	O
we	O
initialize	O
the	O
node	O
representation	O
using	O
the	O
contextual	O
embeddings	O
obtained	O
with	O
two	O
TANDA	B-MethodName
-	I-MethodName
RoBERTa	I-MethodName
-	O
base	O
models	O
1	O
:	O
the	O
first	O
produces	O
the	O
embedding	O
of	O
(	O
q	O
,	O
t	O
)	O
,	O
while	O
the	O
second	O
outputs	O
the	O
embedding	O
of	O
(	O
q	O
,	O
c	O
i	O
)	O
.	O
Then	O
,	O
we	O
apply	O
a	O
max	O
-	O
pooling	O
operation	O
on	O
these	O
two	O
to	O
get	O
the	O
final	O
node	O
representation	O
.	O
The	O
rest	O
of	O
the	O
architecture	O
is	O
identical	O
to	O
the	O
original	O
KGAT	B-MethodName
.	O
Finally	O
,	O
at	O
test	O
time	O
,	O
we	O
select	O
one	O
c	O
i	O
at	O
a	O
time	O
,	O
as	O
the	O
target	O
t	O
,	O
and	O
compute	O
its	O
probability	O
,	O
which	O
ranks	O
c	O
i	O
.	O

One	O
simple	O
and	O
effective	O
method	O
to	O
build	O
an	O
answer	O
selector	O
is	O
to	O
use	O
a	O
pre	O
-	O
trained	O
Transformer	O
model	O
,	O
adding	O
a	O
simple	O
classification	O
layer	O
to	O
it	O
,	O
and	O
fine	O
-	O
tuning	O
the	O
model	O
on	O
the	O
AS2	B-TaskName
task	I-TaskName
.	O
Specifically	O
,	O
q	O
=	O
Tok	O
q	O
1	O
,	O
...	O
,	O
Tok	O
,	O
inserted	O
at	O
the	O
beginning	O
,	O
as	O
separator	O
,	O
and	O
at	O
the	O
end	O
,	O
respectively	O
.	O
This	O
input	O
is	O
encoded	O
as	O
three	O
embeddings	O
based	O
on	O
tokens	O
,	O
segments	O
and	O
their	O
positions	O
,	O
which	O
are	O
fed	O
as	O
input	O
to	O
several	O
layers	O
(	O
up	O
to	O
24	O
)	O
.	O
Each	O
of	O
them	O
contains	O
sublayers	O
for	O
multi	O
-	O
head	O
attention	O
,	O
normalization	O
and	O
feed	O
forward	O
processing	O
.	O
The	O
result	O
of	O
this	O
transformation	O
is	O
an	O
embedding	O
,	O
E	O
,	O
representing	O
(	O
q	O
,	O
c	O
)	O
,	O
which	O
models	O
the	O
dependencies	O
between	O
words	O
and	O
segments	O
of	O
the	O
two	O
sentences	O
.	O
For	O
the	O
downstream	O
task	O
,	O
E	O
is	O
fed	O
(	O
after	O
applying	O
a	O
non	O
-	O
linearity	O
function	O
)	O
to	O
a	O
fully	O
connected	O
layer	O
having	O
weights	O
:	O
W	O
and	O
B.	O
The	O
output	O
layer	O
can	O
be	O
used	O
to	O
implement	O
the	O
task	O
function	O
.	O
For	O
example	O
,	O
a	O
softmax	O
can	O
be	O
used	O
to	O
model	O
the	O
probability	O
of	O
the	O
question	O
/	O
candidate	O
pair	O
classification	O
,	O
as	O
:	O
p	O
(	O
q	O
,	O
c	O
)	O
=	O
sof	O
tmax	O
(	O
W	O
×	O
tanh	O
(	O
E	O
(	O
q	O
,	O
c	O
)	O
)	O
+	O
B	O
)	O
.	O
We	O
can	O
train	O
this	O
model	O
with	B-MetricName
log	I-MetricName
cross	I-MetricName
-	I-MetricName
entropy	I-MetricName
loss	I-MetricName
:	I-MetricName
L	I-MetricName
=	I-MetricName
−	I-MetricName
l	I-MetricName
{	I-MetricName
0	I-MetricName
,	I-MetricName
1	I-MetricName
}	I-MetricName
y	I-MetricName
l	I-MetricName
×	I-MetricName
log	I-MetricName
(	I-MetricName
ŷ	I-MetricName
l	I-MetricName
)	I-MetricName
on	O
pairs	O
of	O
texts	O
,	O
where	O
y	O
l	O
is	O
the	O
correct	O
and	O
incorrect	O
answer	O
label	O
,	O
ŷ	O
1	O
=	O
p	O
(	O
q	O
,	O
c	O
)	O
,	O
andŷ	O
0	O
=	O
1	O
−	O
p	O
(	O
q	O
,	O
c	O
)	O
.	O
Training	O
the	O
Transformer	O
from	O
scratch	O
requires	O
a	O
large	O
amount	O
of	O
labeled	O
data	O
,	O
but	O
it	O
can	O
be	O
pre	O
-	O
trained	O
using	O
a	O
masked	O
language	O
model	O
,	O
and	O
the	O
next	B-TaskName
sentence	I-TaskName
prediction	I-TaskName
tasks	I-TaskName
,	O
for	O
which	O
labels	O
can	O
be	O
automatically	O
generated	O
.	O
Several	O
methods	O
for	O
pretraining	O
Transformer	O
-	O
based	O
language	O
models	O
have	O
been	O
proposed	O
,	O
e.g.	O
,	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
,	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
,	O
XLNet	B-MethodName
(	O
Yang	O
et	O
al	O
,	O
2019	O
)	O
,	O
AlBERT	B-MethodName
(	O
Lan	O
et	O
al	O
,	O
2020	O
)	O
.	O

The	O
task	O
of	O
reranking	B-TaskName
answer	I-TaskName
-	I-TaskName
sentence	I-TaskName
candidates	I-TaskName
provided	O
by	O
a	O
retrieval	O
engine	O
can	O
be	O
modeled	O
with	O
a	O
classifier	O
scoring	O
the	O
candidates	O
.	O
Let	O
q	O
be	O
an	O
element	O
of	O
the	O
question	O
set	O
,	O
Q	O
,	O
and	O
A	O
=	O
{	O
c	O
1	O
,	O
.	O
.	O
.	O
,	O
c	O
n	O
}	O
be	O
a	O
set	O
of	O
candidates	O
for	O
q	O
,	O
a	O
reranker	O
can	O
be	O
defined	O
as	O
R	O
:	O
Q	O
×	O
Π	O
(	O
A	O
)	O
Π	O
(	O
A	O
)	O
,	O
where	O
Π	O
(	O
A	O
)	O
is	O
the	O
set	O
of	O
all	O
permutations	O
of	O
A.	O
Previous	O
work	O
targeting	O
ranking	O
problems	O
in	O
the	O
text	O
domain	O
has	O
classified	O
reranking	O
functions	O
into	O
three	O
buckets	O
:	O
pointwise	O
,	O
pairwise	O
,	O
and	O
listwise	O
methods	O
.	O
Pointwise	B-TaskName
reranking	I-TaskName
:	O
This	O
approach	O
learns	O
p	O
(	O
q	O
,	O
c	O
i	O
)	O
,	O
which	O
is	O
the	O
probability	O
of	O
c	O
i	O
correctly	O
answering	O
q	O
,	O
using	O
a	O
standard	O
binary	O
classification	O
setting	O
.	O
The	O
final	O
rank	O
is	O
simply	O
obtained	O
sorting	O
c	O
i	O
,	O
based	O
on	O
p	O
(	O
q	O
,	O
c	O
i	O
)	O
.	O
Previous	O
work	O
estimates	O
p	O
(	O
q	O
,	O
c	O
i	O
)	O
with	O
neural	O
models	O
(	O
Severyn	O
and	O
Moschitti	O
,	O
2015	O
)	O
,	O
also	O
using	O
attention	O
mechanisms	O
,	O
e.g.	O
,	O
Compare	O
-	O
Aggregate	O
(	O
Yoon	O
et	O
al	O
,	O
2019	O
)	O
,	O
inter	O
-	O
weighted	O
alignment	O
networks	O
(	O
Shen	O
et	O
al	O
,	O
2017	O
)	O
,	O
and	O
pre	O
-	O
trained	O
Transformer	O
models	O
,	O
which	O
are	O
the	O
state	O
of	O
the	O
art	O
.	O
Garg	O
et	O
al	O
(	O
2020	O
)	O
proposed	O
TANDA	B-MethodName
,	O
which	O
is	O
the	O
current	O
most	O
accurate	O
model	O
on	O
WikiQA	B-DatasetName
and	O
TREC	B-DatasetName
-	I-DatasetName
QA	I-DatasetName
.	O
Pairwise	O
reranking	O
:	O
The	O
method	O
considers	O
binary	O
classifiers	O
of	O
the	O
form	O
χ	O
(	O
q	O
,	O
c	O
i	O
,	O
c	O
j	O
)	O
for	O
determining	O
the	O
partial	O
rank	O
between	O
c	O
i	O
and	O
c	O
j	O
,	O
then	O
the	O
scoring	O
function	O
p	O
(	O
q	O
,	O
c	O
i	O
)	O
is	O
obtained	O
by	O
summing	O
up	O
all	O
the	O
contributions	O
with	O
respect	O
to	O
the	O
target	O
candidate	O
t	O
=	O
c	O
i	O
,	O
e.g.	O
,	O
p	O
(	O
q	O
,	O
c	O
i	O
)	O
=	O
j	O
χ	O
(	O
q	O
,	O
c	O
i	O
,	O
c	O
j	O
)	O
.	O
There	O
has	O
been	O
a	O
large	O
body	O
of	O
work	O
preceding	O
Transformer	O
models	O
,	O
e.g.	O
,	O
(	O
Laskar	O
et	O
al	O
,	O
2020	O
;	O
Tayyar	O
Madabushi	O
et	O
al	O
,	O
2018	O
;	O
Rao	O
et	O
al	O
,	O
2016	O
)	O
.	O
However	O
,	O
these	O
methods	O
are	O
largely	O
outperformed	O
by	O
the	O
pointwise	O
TANDA	B-MethodName
model	O
.	O
Listwise	O
reranking	O
:	O
This	O
approach	O
,	O
e.g.	O
,	O
(	O
Bian	O
et	O
al	O
,	O
2017	O
;	O
Cao	O
et	O
al	O
,	O
2007	O
;	O
Ai	O
et	O
al	O
,	O
2018	O
)	O
,	O
aims	O
at	O
learning	O
p	O
(	O
q	O
,	O
π	O
)	O
,	O
π	O
Π	O
(	O
A	O
)	O
,	O
using	O
the	O
information	O
on	O
the	O
entire	O
set	O
of	O
candidates	O
.	O
The	O
loss	B-HyperparameterName
function	I-HyperparameterName
for	O
training	O
such	O
networks	O
is	O
constituted	O
by	O
the	O
contribution	O
of	O
all	O
elements	O
of	O
its	O
ranked	O
items	O
.	O
The	O
closest	O
work	O
to	O
our	O
research	O
is	O
by	O
Bonadiman	O
and	O
Moschitti	O
(	O
2020	O
)	O
,	O
who	O
designed	O
several	O
joint	O
models	O
.	O
These	O
improved	O
early	O
neural	O
networks	O
based	O
on	O
CNN	O
and	O
LSTM	O
for	O
AS2	O
,	O
but	O
failed	O
to	O
improve	O
the	O
state	O
of	O
the	O
art	O
using	O
pre	O
-	O
trained	O
Transformer	O
models	O
.	O

A	O
more	O
structured	O
approach	O
to	O
building	B-TaskName
joint	I-TaskName
models	I-TaskName
over	O
sentences	O
can	O
instead	O
be	O
observed	O
in	O
Fact	O
Verification	O
Systems	O
,	O
e.g.	O
,	O
the	O
methods	O
developed	O
in	O
the	O
FEVER	B-DatasetName
challenge	O
(	O
Thorne	O
et	O
al	O
,	O
2018a	O
)	O
.	O
Such	O
systems	O
take	O
a	O
claim	O
,	O
e.g.	O
,	O
Joe	O
Walsh	O
was	O
inducted	O
in	O
2001	O
,	O
as	O
input	O
(	O
see	O
Tab	O
.	O
1	O
)	O
,	O
and	O
verify	O
if	O
it	O
is	O
valid	O
,	O
using	O
related	O
sentences	O
called	O
evidences	O
(	O
typically	O
retrieved	O
by	O
a	O
search	O
engine	O
)	O
.	O
For	O
example	O
,	O
Ev	O
1	O
,	O
As	O
a	O
member	O
of	O
the	O
Eagles	O
,	O
Walsh	O
was	O
inducted	O
into	O
the	O
Rock	O
and	O
Roll	O
Hall	O
of	O
Fame	O
in	O
1998	O
,	O
and	O
into	O
the	O
Vocal	O
Group	O
Hall	O
of	O
Fame	O
in	O
2001	O
,	O
and	O
Ev	O
3	O
,	O
Walsh	O
was	O
awarded	O
with	O
the	O
Vocal	O
Group	O
Hall	O
of	O
Fame	O
in	O
2001	O
,	O
support	O
the	O
veracity	O
of	O
the	O
claim	O
.	O
In	O
contrast	O
,	O
Ev	O
2	O
is	O
neutral	O
as	O
it	O
describes	O
who	O
Joe	O
Walsh	O
is	O
but	O
does	O
not	O
contribute	O
to	O
establish	O
the	O
induction	O
.	O
We	O
conjecture	O
that	O
supporting	O
evidence	O
for	O
answer	O
correctness	O
in	O
AS2	O
task	O
can	O
be	O
modeled	O
with	O
a	O
similar	O
rationale	O
.	O
In	O
this	O
paper	O
,	O
we	O
design	O
joint	O
models	O
for	O
AS2	O
based	O
on	O
the	O
assumption	O
that	O
,	O
given	O
q	O
and	O
a	O
target	O
answer	O
candidate	O
t	O
,	O
the	O
other	O
answer	O
candidates	O
,	O
(	O
c	O
1	O
,	O
..	O
c	O
k	O
)	O
can	O
provide	O
positive	O
,	O
negative	O
,	O
or	O
neutral	O
support	O
to	O
decide	O
the	O
correctness	O
of	O
t.	O
Our	O
first	O
approach	O
exploits	O
Fact	B-TaskName
Checking	I-TaskName
research	I-TaskName
:	O
we	O
adapted	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
FEVER	B-DatasetName
system	O
,	O
KGAT	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2020	O
)	O
,	O
for	O
AS2	O
.	O
We	O
defined	O
a	O
claim	O
as	O
a	O
pair	O
constituted	O
of	O
the	O
question	O
and	O
one	O
target	O
answer	O
,	O
while	O
considering	O
all	O
the	O
other	O
answers	O
as	O
evidences	O
.	O
We	O
re	O
-	O
trained	O
and	O
rebuilt	O
all	O
its	O
embeddings	O
for	O
the	O
AS2	O
task	O
.	O
Our	O
second	O
method	O
,	O
Answer	B-MethodName
Support	I-MethodName
-	I-MethodName
based	I-MethodName
Reranker	I-MethodName
(	I-MethodName
ASR	I-MethodName
)	I-MethodName
,	O
is	O
completely	O
new	O
,	O
it	O
is	O
based	O
on	O
the	O
representation	O
of	O
the	O
pair	O
,	O
(	O
q	O
,	O
t	O
)	O
,	O
generated	O
by	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
AS2	O
models	O
,	O
concatenated	O
with	O
the	O
representation	O
of	O
all	O
the	O
pairs	O
(	O
t	O
,	O
c	O
i	O
)	O
.	O
The	O
latter	O
summarizes	O
the	O
contribution	O
of	O
each	O
c	O
i	O
to	O
t	O
using	O
a	O
maxpooling	O
operation	O
.	O
c	O
i	O
can	O
be	O
unrelated	O
to	O
(	O
q	O
,	O
t	O
)	O
since	O
the	O
candidates	O
are	O
automatically	O
retrieved	O
,	O
thus	O
it	O
may	O
introduce	O
just	O
noise	O
.	O
To	O
mitigate	O
this	O
problem	O
,	O
we	O
use	O
an	B-MethodName
Answer	I-MethodName
Support	I-MethodName
Classifier	I-MethodName
(	I-MethodName
ASC	I-MethodName
)	I-MethodName
to	O
learn	O
the	O
relatedness	O
between	O
t	O
and	O
c	O
i	O
by	O
classifying	O
their	O
embedding	O
,	O
which	O
we	O
obtain	O
by	O
applying	O
a	O
transformer	O
network	O
to	O
their	O
concatenated	O
text	O
.	O
ASC	B-MethodName
tunes	O
the	O
(	O
t	O
,	O
c	O
i	O
)	O
embedding	O
parameters	O
according	O
to	O
the	O
evidence	O
that	O
c	O
i	O
provides	O
to	O
t.	O
Our	O
Answer	O
Support	O
-	O
based	O
Reranker	O
(	O
ASR	O
)	O
significantly	O
improves	O
the	O
state	O
of	O
the	O
art	O
,	O
and	O
is	O
also	O
simpler	O
than	O
our	O
approach	O
based	O
on	O
KGAT	B-MethodName
.	O
Our	O
third	O
method	O
is	O
an	O
extension	O
of	O
ASR	B-MethodName
.	O
It	O
should	O
be	O
noted	O
that	O
,	O
although	O
ASR	B-MethodName
exploits	O
the	O
information	O
from	O
the	O
k	O
candidates	O
,	O
it	O
still	O
produces	O
a	O
score	O
for	O
a	O
target	O
t	O
without	O
knowing	O
the	O
scores	O
produced	O
for	O
the	O
other	O
target	O
answers	O
.	O
Thus	O
,	O
we	O
jointly	O
model	O
the	O
representation	O
obtained	O
for	O
each	O
target	O
in	O
a	O
multi	O
-	O
ASR	O
(	O
MASR	O
)	O
architecture	O
,	O
which	O
can	O
then	O
carry	O
out	O
a	O
complete	O
global	O
reasoning	O
over	O
all	O
target	O
answers	O
.	O
We	O
experimented	O
with	O
our	O
models	O
over	O
three	O
datasets	O
,	O
WikiQA	B-DatasetName
,	O
TREC	B-DatasetName
-	I-DatasetName
QA	I-DatasetName
and	O
WQA	B-DatasetName
,	O
where	O
the	O
latter	O
is	O
an	O
internal	O
dataset	O
built	O
on	O
anonymized	O
customer	O
questions	O
.	O
The	O
results	O
show	O
that	O
:	O
ASR	B-MethodName
improves	O
the	O
best	O
current	O
model	O
for	O
AS2	O
,	O
i.e.	O
,	O
TANDA	O
by	O
∼3	O
%	O
,	O
corresponding	O
to	O
an	O
error	B-MetricName
reduction	I-MetricName
of	O
10	B-MetricValue
%	I-MetricValue
in	O
Accuracy	B-MetricName
,	O
on	O
both	O
Wik	B-DatasetName
-	I-DatasetName
iQA	I-DatasetName
and	O
TREC	B-DatasetName
-	I-DatasetName
QA	I-DatasetName
.	O
We	O
also	O
obtain	O
a	O
relative	O
improvement	O
of	O
∼3	O
%	O
over	O
TANDA	B-MethodName
on	O
WQA	B-DatasetName
,	O
confirming	O
that	O
ASR	B-MethodName
is	O
a	O
general	O
solution	O
to	O
design	B-TaskName
accurate	I-TaskName
QA	I-TaskName
systems	I-TaskName
.	O
Most	O
interestingly	O
,	O
MASR	B-MethodName
improves	O
ASR	B-MethodName
by	O
additional	O
2	O
%	O
,	O
confirming	O
the	O
benefit	O
of	O
joint	O
modeling	O
.	O
Finally	O
,	O
it	O
is	O
interesting	O
to	O
mention	O
that	O
MASR	O
improvement	O
is	O
also	O
due	O
to	O
the	O
use	O
of	O
FEVER	B-DatasetName
data	O
for	O
pre	O
-	O
fine	O
-	O
tuning	O
ASC	O
,	O
suggesting	O
that	O
the	O
fact	B-TaskName
verification	I-TaskName
inference	I-TaskName
and	O
the	O
answer	B-TaskName
support	I-TaskName
inference	I-TaskName
are	O
similar	O
.	O

Recall	O
that	O
,	O
compared	O
with	O
GBMT	B-MethodName
,	O
GBMT	B-MethodName
ctx	O
contains	O
three	O
types	O
of	O
rules	O
:	O
basic	O
rules	O
,	O
segmenting	O
rules	O
,	O
and	O
selecting	O
rules	O
.	O
While	O
basic	O
rules	O
exist	O
in	O
both	O
systems	O
,	O
segmenting	O
and	O
selecting	O
rules	O
make	O
GBMT	B-MethodName
ctx	O
context	O
-	O
aware	O
.	O
Table	O
2	O
shows	O
the	O
number	O
of	O
rules	O
in	O
GBMT	B-MethodName
ctx	O
according	O
to	O
their	O
types	O
.	O
We	O
found	O
that	O
on	O
both	O
language	O
pairs	O
35	O
%	O
-	O
36	O
%	O
of	O
rules	O
are	O
basic	O
rules	O
.	O
While	O
the	O
proportion	O
of	O
segmenting	O
rules	O
is	O
∼53	O
%	O
,	O
selecting	O
rules	O
only	O
account	O
for	O
11	O
%	O
-	O
12	O
%	O
.	O
This	O
is	O
because	O
segmenting	O
rules	O
contain	O
richer	O
contextual	O
information	O
than	O
selecting	O
rules	O
.	O
Table	O
3	O
shows	O
BLEU	B-MetricName
scores	O
of	O
GBMT	B-MethodName
ctx	O
when	O
different	O
types	O
of	O
rules	O
are	O
used	O
.	O
Note	O
that	O
when	O
only	O
basic	O
rules	O
are	O
allowed	O
,	O
our	O
system	O
degrades	O
to	O
the	O
conventional	O
GBMT	B-MethodName
system	O
.	O
The	O
results	O
in	O
Table	O
3	O
suggest	O
that	O
both	O
segmenting	O
and	O
selecting	O
rules	O
consistently	O
improve	O
GBMT	O
on	O
both	O
language	O
pairs	O
.	O
However	O
,	O
segmenting	O
rules	O
are	O
more	O
useful	O
than	O
selecting	O
rules	O
.	O
This	O
is	O
reasonable	O
since	O
(	O
hong	O
kong	O
macao	O
taiwan	O
)	O
hong	O
kong	O
spring	O
festival	O
retail	O
business	O
rise	O
10	O
%	O
(	O
Gang	O
Ao	O
Tai	O
)	O
XiangGang	O
XinChun	O
LingShou	O
ShengYi	O
ShangSheng	O
YiCheng	O
Ref	O
:	O
GBMT	B-MethodName
:	O
GBMTctx	B-MethodName
:	O
(	O
hong	O
kong	O
,	O
macao	O
and	O
taiwan	O
)	O
hong	O
kong	O
's	O
retail	O
sales	O
up	O
10	O
%	O
during	O
spring	O
festival	O
(	O
the	O
spring	O
festival	O
)	O
hong	O
kong	O
retail	O
business	O
in	O
hong	O
kong	O
,	O
macao	O
and	O
taiwan	O
rose	O
by	O
10	O
%	O
(	O
hong	O
kong	O
,	O
macao	O
and	O
taiwan	O
)	O
hong	O
kong	O
spring	O
retail	O
business	O
will	O
increase	O
by	O
10	O
%	O
(	O
a	O
)	O
subgraph	O
selection	O
we	O
also	O
dedicate	O
protect	O
and	O
improve	O
living	O
emvironment	O
.	O

The	O
1	O
:	O
BLEU	B-MetricName
scores	O
of	O
all	O
systems	O
.	O
Bold	O
figures	O
mean	O
GBMT	B-MethodName
ctx	O
is	O
significantly	O
better	O
than	O
GBMT	B-MethodName
at	O
p	O
≤	O
0.01	O
.	O
*	O
means	O
a	O
system	O
is	O
significantly	O
better	O
than	O
PBMT	B-MethodName
at	O
p	O
≤	O
0.01	O
.	O
+	O
means	O
a	O
system	O
is	O
significantly	O
better	O
than	O
TBMT	B-MethodName
at	O
p	O
≤	O
0.01	O
.	O
Following	O
Li	O
et	O
al	O
(	O
2016	O
)	O
,	O
Chinese	O
and	O
German	O
sentences	O
are	O
parsed	O
into	O
projective	O
dependency	O
trees	O
which	O
are	O
then	O
converted	O
to	O
graphs	O
by	O
adding	O
bigram	O
edges	O
.	O
Word	O
alignment	O
is	O
performed	O
by	O
GIZA++	O
(	O
Och	O
and	O
Ney	O
,	O
2003	O
)	O
with	O
the	O
heuristic	O
function	O
grow	O
-	O
diag	O
-	O
final	O
-	O
and	O
.	O
We	O
use	O
SRILM	B-MethodName
(	O
Stolcke	O
,	O
2002	O
)	O
to	O
train	O
a	O
5	O
-	O
gram	O
language	O
model	O
on	O
the	O
Xinhua	O
portion	O
of	O
the	O
English	O
Gigaword	O
corpus	O
5th	O
edition	O
with	O
modified	O
Kneser	O
-	O
Ney	O
discounting	O
(	O
Chen	O
and	O
Goodman	O
,	O
1996	O
)	O
.	O
Batch	O
MIRA	B-MethodName
(	O
Cherry	O
and	O
Foster	O
,	O
2012	O
)	O
is	O
used	O
to	O
tune	O
feature	O
weights	O
.	O
We	O
report	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
scores	O
averaged	O
on	O
three	O
runs	O
of	O
MIRA	B-DatasetName
(	O
Clark	O
et	O
al	O
,	O
2011	O
)	O
.	O
We	O
compare	O
our	O
system	O
GBMT	B-MethodName
ctx	O
with	O
several	O
other	O
systems	O
.	O
A	O
system	O
PBMT	B-MethodName
is	O
built	O
using	O
the	O
phrase	O
-	O
based	O
model	O
in	O
Moses	O
(	O
Koehn	O
et	O
al	O
,	O
2007	O
)	O
.	O
GBMT	O
is	O
the	O
graph	O
-	O
based	O
translation	O
system	O
described	O
in	O
Li	O
et	O
al	O
(	O
2016	O
)	O
.	O
To	O
examine	O
the	O
influence	O
of	O
bigram	O
links	O
,	O
GBMT	O
is	O
also	O
used	O
to	O
translate	B-TaskName
dependency	I-TaskName
trees	I-TaskName
where	O
treelets	O
Xiong	O
et	O
al	O
,	O
2007	O
)	O
are	O
the	O
basic	O
translation	O
units	O
.	O
Accordingly	O
,	O
we	O
name	O
the	O
system	O
TBMT	B-MethodName
.	O
All	O
systems	O
are	O
implemented	O
in	O
Moses	O
.	O

