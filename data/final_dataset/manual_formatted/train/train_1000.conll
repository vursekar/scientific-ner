
We	O
trained	O
a	O
logistic	O
regression	O
model	O
for	O
complaint	O
detection	O
using	O
each	O
one	O
of	O
the	O
features	O
described	O
in	O
section	O
4.1	O
.	O
Table	O
3	O
summarizes	O
the	O
results	O
in	O
terms	O
of	O
accuracy	B-MetricName
and	O
macro	B-MetricName
averaged	I-MetricName
F1	I-MetricName
-	I-MetricName
score	I-MetricName
.	O
The	O
best	O
performing	O
model	O
is	O
based	O
on	O
unigrams	O
,	O
with	O
an	O
accuracy	B-MetricName
of	O
75.3	B-MetricValue
%	I-MetricValue
.	O
There	O
is	O
not	O
a	O
significant	O
difference	O
in	O
the	O
performance	O
of	O
different	O
sentiment	O
models	O
.	O
It	O
is	O
also	O
interesting	O
to	O
observe	O
that	O
simple	O
features	O
like	O
the	O
counts	O
of	O
different	O
pronoun	O
types	O
and	O
counts	O
of	O
intensifiers	O
have	O
strong	O
predictive	O
ability	O
.	O
Overall	O
,	O
we	O
observe	O
that	O
most	O
of	O
the	O
features	O
studied	O
here	O
have	O
some	O
ability	O
to	O
predict	O
complaints	O
.	O

The	O
Semantic	B-DatasetName
Textual	I-DatasetName
Similarity	I-DatasetName
Benchmark	I-DatasetName
is	O
a	O
collection	O
of	O
sentence	O
pairs	O
drawn	O
from	O
news	O
headlines	O
and	O
other	O
sources	O
(	O
Cer	O
et	O
al	O
,	O
2017	O
)	O
.	O
They	O
were	O
annotated	O
with	O
a	O
score	O
from	O
1	O
to	O
5	O
denoting	O
how	O
similar	O
the	O
two	O
sentences	O
are	O
in	O
terms	O
of	O
semantic	O
meaning	O
.	O
MRPC	B-DatasetName
Microsoft	I-DatasetName
Research	I-DatasetName
Paraphrase	I-DatasetName
Corpus	I-DatasetName
consists	O
of	O
sentence	O
pairs	O
automatically	O
extracted	O
from	O
online	O
news	O
sources	O
,	O
with	O
human	O
annotations	O
for	O
whether	O
the	O
sentences	O
in	O
the	O
pair	O
are	O
semantically	O
equivalent	O
(	O
Dolan	O
and	O
Brockett	O
,	O
2005	O
)	O
.	O
RTE	B-DatasetName
Recognizing	I-DatasetName
Textual	I-DatasetName
Entailment	I-DatasetName
is	O
a	O
binary	O
entailment	O
task	O
similar	O
to	O
MNLI	O
,	O
but	O
with	O
much	O
less	O
training	O
data	O
(	O
Bentivogli	O
et	O
al	O
,	O
2009	O
)	O
.	O
14	O
WNLI	B-DatasetName
Winograd	I-DatasetName
NLI	I-DatasetName
is	O
a	O
small	O
natural	O
language	O
inference	O
dataset	O
(	O
Levesque	O
et	O
al	O
,	O
2011	O
)	O
.	O
The	O
GLUE	O
webpage	O
notes	O
that	O
there	O
are	O
issues	O
with	O
the	O
construction	O
of	O
this	O
dataset	O
,	O
15	O
and	O
every	O
trained	O
system	O
that	O
's	O
been	O
submitted	O
to	O
GLUE	O
has	O
performed	O
worse	O
than	O
the	O
65.1	B-MetricValue
baseline	O
accuracy	B-MetricName
of	O
predicting	O
the	O
majority	O
class	O
.	O
We	O
therefore	O
exclude	O
this	O
set	O
to	O
be	O
fair	O
to	O
OpenAI	B-MethodName
GPT	I-MethodName
.	O
For	O
our	O
GLUE	O
submission	O
,	O
we	O
always	O
predicted	O
the	O
majority	O
class	O
.	O

The	O
SQuAD	B-DatasetName
2.0	I-DatasetName
task	O
extends	O
the	O
SQuAD	B-DatasetName
1.1	I-DatasetName
problem	O
definition	O
by	O
allowing	O
for	O
the	O
possibility	O
that	O
no	O
short	O
answer	O
exists	O
in	O
the	O
provided	O
paragraph	O
,	O
making	O
the	O
problem	O
more	O
realistic	O
.	O
We	O
use	O
a	O
simple	O
approach	O
to	O
extend	O
the	O
SQuAD	B-MethodName
v1.1	I-MethodName
BERT	I-MethodName
model	O
for	O
this	O
task	O
.	O
We	O
treat	O
questions	O
that	O
do	O
not	O
have	O
an	O
answer	O
as	O
having	O
an	O
answer	O
span	O
with	O
start	O
and	O
end	O
at	O
the	O
[	O
CLS	O
]	O
token	O
.	O
The	O
probability	O
space	O
for	O
the	O
start	O
and	O
end	O
answer	O
span	O
positions	O
is	O
extended	O
to	O
include	O
the	O
position	O
of	O
the	O
[	O
CLS	O
]	O
token	O
.	O
For	O
prediction	O
,	O
we	O
compare	O
the	O
score	O
of	O
the	O
no	O
-	O
answer	O
span	O
:	O
s	O
null	O
=	O
S	O
C	O
+	O
E	O
C	O
to	O
the	O
score	O
of	O
the	O
best	O
non	O
-	O
null	O
span	O
12	O
The	O
TriviaQA	B-DatasetName
data	O
we	O
used	O
consists	O
of	O
paragraphs	O
from	O
TriviaQA	B-DatasetName
-	I-DatasetName
Wiki	I-DatasetName
formed	O
of	O
the	O
first	O
400	O
tokens	O
in	O
documents	O
,	O
that	O
contain	O
at	O
least	O
one	O
of	O
the	O
provided	O
possible	O
answers	O
.	O
s	O
i	O
,	O
j	O
=	O
max	O
j≥i	O
S	O
T	O
i	O
+	O
E	O
T	O
j	O
.	O
We	O
predict	O
a	O
non	O
-	O
null	O
answer	O
whenŝ	O
i	O
,	O
j	O
>	O
s	O
null	O
+	O
τ	O
,	O
where	O
the	O
threshold	O
τ	O
is	O
selected	O
on	O
the	O
dev	O
set	O
to	O
maximize	O
F1	B-MetricName
.	O
We	O
did	O
not	O
use	O
TriviaQA	B-DatasetName
data	O
for	O
this	O
model	O
.	O
We	O
fine	O
-	O
tuned	O
for	O
2	B-HyperparameterValue
epochs	B-HyperparameterName
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
5e	B-HyperparameterValue
-	I-HyperparameterValue
5	I-HyperparameterValue
and	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
48	B-HyperparameterValue
.	O
The	O
results	O
compared	O
to	O
prior	O
leaderboard	O
entries	O
and	O
top	O
published	O
work	O
(	O
Sun	O
et	O
al	O
,	O
2018	O
;	O
Wang	O
et	O
al	O
,	O
2018b	O
)	O
are	O
shown	O
in	O
Table	O
3	O
,	O
excluding	O
systems	O
that	O
use	O
BERT	O
as	O
one	O
of	O
their	O
components	O
.	O
We	O
observe	O
a	O
+5.1	B-HyperparameterValue
F1	B-MetricName
improvement	O
over	O
the	O
previous	O
best	O
system	O
.	O

Understanding	O
Tasks	O
The	O
hyper	O
-	O
parameters	O
are	O
set	O
as	O
follows	O
:	O
768	B-HyperparameterValue
hidden	B-HyperparameterName
units	I-HyperparameterName
,	O
12	B-HyperparameterValue
heads	B-HyperparameterName
,	O
GELU	B-HyperparameterValue
activation	B-HyperparameterName
,	O
a	O
dropout	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.1	B-HyperparameterValue
,	O
512	B-HyperparameterValue
max	B-HyperparameterName
input	I-HyperparameterName
length	I-HyperparameterName
,	O
12	B-HyperparameterValue
layers	B-HyperparameterName
in	O
encoder	O
.	O
In	O
the	O
pre	O
-	O
training	O
stage	O
,	O
we	O
first	O
initialize	O
Unicoder	B-MethodName
LC	I-MethodName
with	O
XLM	O
-	O
R	O
base	O
,	O
and	O
then	O
run	O
continue	O
pre	O
-	O
training	O
with	O
the	O
accumulated	O
8	O
,	O
192	B-HyperparameterValue
batch	B-HyperparameterName
size	I-HyperparameterName
with	O
gradients	O
accumulation	O
.	O
We	O
use	O
Adam	O
Optimizer	O
with	O
a	O
linear	O
warm	O
-	O
up	O
and	O
set	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
to	O
3e	B-HyperparameterValue
-	I-HyperparameterValue
5	I-HyperparameterValue
.	O
We	O
select	O
different	O
understanding	O
tasks	O
randomly	O
in	O
different	O
batches	O
.	O
This	O
costed	O
12	O
days	O
on	O
16	O
V100	O
.	O
In	O
the	O
fine	O
-	O
tuning	O
stage	O
,	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
is	O
set	O
to	O
32	B-HyperparameterValue
.	O
We	O
use	O
Adam	O
Optimizer	O
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
with	O
warm	O
-	O
up	O
and	O
set	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
to	O
5e	B-HyperparameterValue
-	I-HyperparameterValue
6	I-HyperparameterValue
.	O
For	O
all	O
sentence	O
classification	O
tasks	O
,	O
we	O
finetune	O
10	B-HyperparameterValue
epochs	B-HyperparameterName
.	O
For	O
POS	B-TaskName
Tagging	I-TaskName
and	O
NER	B-TaskName
,	O
we	O
fine	O
-	O
tune	O
20	B-HyperparameterValue
epochs	B-HyperparameterName
.	O
And	O
for	O
POS	B-TaskName
Tagging	I-TaskName
,	O
we	O
set	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
to	O
2e	B-HyperparameterValue
-	I-HyperparameterValue
5	I-HyperparameterValue
.	O
For	O
MLQA	B-TaskName
,	O
we	O
set	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
to	O
3e	B-HyperparameterValue
-	I-HyperparameterValue
5	I-HyperparameterValue
,	O
batch	B-HyperparameterName
size	I-HyperparameterName
to	O
12	B-HyperparameterValue
and	O
train	O
2	B-HyperparameterValue
epochs	B-HyperparameterName
following	O
BERT	O
for	O
SQuAD	O
.	O
After	O
each	O
epoch	O
,	O
we	O
test	O
the	O
fine	O
-	O
tuned	O
model	O
on	O
the	O
dev	O
sets	O
of	O
all	O
languages	O
.	O
We	O
select	O
the	O
model	O
with	O
the	O
best	O
average	O
result	O
on	O
the	O
dev	O
sets	O
of	O
all	O
languages	O
.	O
,	O
the	O
hyper	O
-	O
parameters	O
are	O
set	O
as	O
follows	O
:	O
768	B-HyperparameterValue
hidden	B-HyperparameterName
units	I-HyperparameterName
,	O
12	B-HyperparameterValue
heads	B-HyperparameterName
,	O
GELU	B-HyperparameterValue
activation	B-HyperparameterName
,	O
a	O
dropout	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.1	B-HyperparameterValue
,	O
512	B-HyperparameterValue
max	B-HyperparameterName
input	I-HyperparameterName
length	I-HyperparameterName
,	O
12	B-HyperparameterValue
layers	B-HyperparameterName
in	O
encoder	O
,	O
12	B-HyperparameterValue
layers	B-HyperparameterName
in	O
decoder	O
.	O

Baseline	O
We	O
compare	O
with	O
the	O
BERT	B-MethodName
passage	I-MethodName
ranker	I-MethodName
(	O
Nie	O
et	O
al	O
,	O
2019	O
)	O
that	O
is	O
commonly	O
used	O
on	O
open	B-TaskName
-	I-TaskName
domain	I-TaskName
QA	I-TaskName
including	O
HotpotQA	B-DatasetName
.	O
The	O
baseline	O
uses	O
the	O
same	O
BERT	O
architecture	O
as	O
our	O
approach	O
described	O
in	O
Section	O
2.2	O
,	O
but	O
is	O
trained	O
with	O
only	O
the	O
relevancy	B-MetricName
loss	I-MetricName
(	O
Eq	O
5	O
)	O
and	O
therefore	O
only	O
consider	O
the	O
relevancy	O
when	O
selecting	O
evidence	O
.	O
We	O
also	O
compare	O
the	O
DRN	B-MethodName
model	O
from	O
(	O
Harel	O
et	O
al	O
,	O
2019	O
)	O
which	O
is	O
designed	O
for	O
the	O
SRD	O
task	O
.	O
Their	O
ensemble	O
system	O
first	O
finds	O
the	O
most	O
relevant	O
evidence	O
to	O
the	O
given	O
question	O
,	O
and	O
then	O
select	O
the	O
second	O
diverse	O
evidence	O
using	O
their	O
score	O
function	O
.	O
The	O
major	O
differences	O
from	O
our	O
method	O
are	O
that	O
(	O
1	O
)	O
they	O
train	O
two	O
separate	O
models	O
for	O
evidence	O
selection	O
;	O
(	O
2	O
)	O
they	O
do	O
not	O
consider	O
the	O
compactness	O
among	O
the	O
evidences	O
.	O
It	O
is	O
worth	O
mentioning	O
that	O
we	O
replace	O
their	O
LSTM	O
encoder	O
with	O
BERT	O
encoder	O
for	O
fair	O
comparison	O
.	O
Metric	O
During	O
the	O
evaluation	O
we	O
make	O
each	O
method	O
output	O
its	O
top	O
2	O
ranked	O
results	O
3	O
(	O
i.e.	O
the	O
top	O
1	O
ranked	O
pair	O
from	O
our	O
method	O
)	O
as	O
the	O
prediction	O
.	O
The	O
final	O
performance	O
is	O
evaluated	O
by	O
exact	B-MetricName
match	I-MetricName
(	O
EM	B-MetricName
)	O
,	O
i.e.	O
,	O
whether	O
both	O
true	O
evidence	O
passages	O
are	O
covered	O
,	O
and	O
the	O
F1	B-MetricName
score	I-MetricName
on	O
the	O
test	O
sets	O
.	O

Score	O
Function	O
During	O
inference	O
,	O
we	O
use	O
the	O
following	O
score	O
function	O
to	O
find	O
the	O
best	O
paragraph	O
combination	O
:	O
g	O
(	O
P	O
sel	O
;	O
q	O
;	O
{	O
pi	O
}	O
)	O
=	O
p	O
i	O
P	O
(	O
pi	O
|	O
q	O
)	O
+	O
α	B-HyperparameterName
cos	O
(	O
p	O
i	O
pi	O
,	O
q	O
)	O
+	O
β	B-HyperparameterName
p	O
i	O
,	O
p	O
j	O
,	O
i	O
=	O
j	O
1	O
(	O
pi	O
,	O
pj	O
)	O
(	O
8	O
)	O
where	O
α	B-HyperparameterName
and	O
β	B-HyperparameterName
are	O
hyperparameters	O
similar	O
to	O
Eq	O
4	O
.	O
Note	O
that	O
our	O
approach	O
requires	O
to	O
encode	O
each	O
passage	O
in	O
P	O
only	O
once	O
for	O
each	O
question	O
,	O
resulting	O
in	O
an	O
O	O
(	O
K	O
)	O
time	O
complexity	O
of	O
encoding	O
(	O
K	O
=	O
|	O
P	O
|	O
)	O
;	O
and	O
the	O
subset	O
selection	O
is	O
performed	O
in	O
the	O
vector	O
space	O
,	O
which	O
is	O
much	O
more	O
efficient	O
than	O
selecting	O
subsets	O
before	O
encoding	O
.	O
Beam	O
Search	O
In	O
a	O
real	O
-	O
world	O
application	O
,	O
there	O
is	O
usually	O
a	O
large	O
candidate	O
set	O
of	O
P	O
,	O
e.g.	O
,	O
retrieved	O
passages	O
for	O
q	O
via	O
a	O
traditional	O
IR	O
system	O
.	O
Our	O
algorithm	O
requires	O
O	O
(	O
K	O
)	O
time	O
encoding	O
,	O
and	O
O	O
(	O
K	O
L	O
)	O
time	O
scoring	O
in	O
vector	O
space	O
when	O
ranking	O
all	O
the	O
combinations	O
in	O
L	O
candidates	O
.	O
Thus	O
when	O
K	O
becomes	O
large	O
,	O
it	O
is	O
still	O
inefficient	O
even	O
when	O
L	O
=	O
2	O
.	O
We	O
resort	O
to	O
beam	O
search	O
to	O
deal	O
with	O
scenarios	O
with	O
large	O
Ks	O
.	O
The	O
details	O
can	O
be	O
found	O
in	O
Appendix	O
A.	O
3	O
Experiments	O

We	O
propose	O
a	O
new	O
supervised	O
training	O
objective	O
to	O
learn	O
the	O
BERT	O
encoder	O
for	O
QA	B-TaskName
that	O
optimizes	O
the	O
previous	O
conditions	O
.	O
Note	O
that	O
in	O
this	O
work	O
we	O
assume	O
a	O
set	O
of	O
labeled	O
training	O
examples	O
are	O
available	O
,	O
i.e.	O
,	O
the	O
ground	O
truth	O
annotations	O
contain	O
complementary	O
supporting	O
paragraphs	O
.	O
Recently	O
there	O
was	O
a	O
growing	O
in	O
such	O
datasets	O
(	O
Yang	O
et	O
al	O
,	O
2018	O
;	O
Yao	O
et	O
al	O
,	O
2019	O
)	O
,	O
due	O
to	O
the	O
increasing	O
interest	O
in	O
model	O
explainability	O
.	O
Also	O
,	O
such	O
supervision	O
signals	O
can	O
also	O
be	O
obtained	O
with	O
distant	O
supervision	O
.	O
For	O
each	O
training	O
instance	O
(	O
q	O
,	O
P	O
)	O
,	O
we	O
define	O
{	O
p	O
i	O
}	O
+	O
=	O
{	O
p	O
i	O
}	O
,	O
∀i	O
{	O
i	O
|	O
p	O
i	O
P	O
+	O
}	O
(	O
1	O
)	O
{	O
p	O
i	O
}	O
−	O
=	O
{	O
p	O
i	O
}	O
,	O
∀i	O
{	O
i	O
|	O
p	O
i	O
P	O
−	O
}	O
(	O
2	O
)	O
{	O
p	O
i	O
}	O
=	O
{	O
p	O
i	O
}	O
+	O
∪	O
{	O
p	O
i	O
}	O
−	O
(	O
3	O
)	O
Denoting	O
y	O
p	O
i	O
=	O
1	O
if	O
p	O
i	O
P	O
+	O
and	O
y	O
p	O
i	O
=	O
0	O
if	O
p	O
i	O
P	O
−	O
,	O
we	O
have	O
the	O
following	O
training	O
objective	O
function	O
:	O
L	O
(	O
{	O
pi	O
}	O
;	O
q	O
;	O
y	O
)	O
=	O
Lsup	O
(	O
{	O
pi	O
}	O
;	O
q	O
;	O
y	O
)	O
+	O
αL	B-HyperparameterName
d	O
(	O
{	O
pi	O
}	O
+	O
)	O
+	O
βLc	B-HyperparameterName
(	O
{	O
pi	O
}	O
;	O
q	O
;	O
y	O
)	O
(	O
4	O
)	O
where	O
Lsup	O
(	O
{	O
pi	O
}	O
;	O
q	O
;	O
y	O
)	O
=	O
−	O
i	O
yp	O
i	O
log	O
(	O
f	O
(	O
pi	O
)	O
)	O
,	O
(	O
5	O
)	O
L	O
d	O
(	O
{	O
pi	O
}	O
+	O
)	O
=	O
p	O
i	O
,	O
p	O
j	O
,	O
i	O
=	O
j	O
(	O
1	O
−	O
1	O
(	O
pi	O
,	O
p	O
j	O
)	O
)	O
.	O
(	O
6	O
)	O
Lc	O
(	O
{	O
pi	O
}	O
;	O
q	O
;	O
y	O
)	O
=	O
	O
	O
	O
	O
	O
	O
	O
	O
1	O
−	O
cos	O
(	O
q	O
,	O
i	O
pi	O
)	O
,	O
if	O
Πp	O
i	O
yp	O
i	O
=	O
1	O
max	O
(	O
0	O
,	O
cos	O
(	O
q	O
,	O
i	O
pi	O
)	O
−	O
γ	O
)	O
,	O
if	O
Πp	O
i	O
yp	O
i	O
=	O
0	O
(	O
7	O
)	O
where	O
α	B-HyperparameterName
and	O
β	B-HyperparameterName
are	O
the	O
hyperparameter	O
weights	O
and	O
1	O
(	O
,	O
)	O
denotes	O
L1	O
loss	O
between	O
two	O
input	O
vectors	O
.	O
Eq	O
5	O
is	O
the	O
cross	B-MetricName
-	I-MetricName
entropy	I-MetricName
loss	I-MetricName
corresponding	O
to	O
relevance	O
condition	O
;	O
Eq	O
6	O
regularizes	O
the	O
diversity	O
condition	O
;	O
Eq	O
7	O
is	O
the	O
cosine	B-MetricName
-	I-MetricName
embedding	I-MetricName
loss	I-MetricName
2	O
for	O
the	O
compactness	O
condition	O
and	O
γ	O
>	O
0	O
is	O
the	O
margin	O
to	O
encourage	O
data	O
samples	O
with	O
better	O
question	O
coverage	O
.	O
2	O
Refer	O
to	O
CosineEmbeddingLoss	O
in	O
PyTorch	O
.	O

Named	B-TaskName
entity	I-TaskName
Recognition	I-TaskName
(	O
NER	B-TaskName
)	O
has	O
been	O
excavated	O
for	O
a	O
long	O
time	O
(	O
Collins	O
and	O
Singer	O
,	O
1999	O
;	O
,	O
which	O
classifies	O
coarsegrained	O
types	O
(	O
e.g.	O
person	O
,	O
location	O
)	O
.	O
Recently	O
,	O
(	O
Nagesh	O
and	O
Surdeanu	O
,	O
2018a	O
,	O
b	O
)	O
applied	O
ladder	O
network	O
(	O
Rasmus	O
et	O
al	O
,	O
2015	O
)	O
to	O
coarse	O
-	O
grained	O
entity	O
classification	O
in	O
a	O
semi	O
-	O
supervised	O
learning	O
fashion	O
.	O
(	O
Ling	O
and	O
Weld	O
,	O
2012	O
)	O
proposed	O
Fine	B-MethodName
-	I-MethodName
Grained	I-MethodName
Entity	I-MethodName
Recognition	I-MethodName
(	O
FET	B-MethodName
)	O
.	O
They	O
used	O
distant	O
supervision	O
to	O
get	O
training	O
corpus	O
for	O
FET	B-MethodName
.	O
Embedding	O
techniques	O
was	O
applied	O
to	O
learn	O
feature	O
representations	O
since	O
(	O
Yogatama	O
et	O
al	O
,	O
2015	O
;	O
Dong	O
et	O
al	O
,	O
2015	O
)	O
.	O
(	O
Shimaoka	O
et	O
al	O
,	O
2016	O
)	O
introduced	O
attention	O
mechanism	O
for	O
FET	B-MethodName
to	O
capture	O
informative	O
words	O
.	O
(	O
Xin	O
et	O
al	O
,	O
2018a	O
)	O
used	O
the	O
TransE	O
entity	O
embeddings	O
(	O
Bordes	O
et	O
al	O
,	O
2013	O
)	O
as	O
the	O
query	O
vector	O
of	O
attention	O
.	O
Early	O
works	O
ignore	O
the	O
out	O
-	O
of	O
-	O
context	O
noise	O
,	O
(	O
Gillick	O
et	O
al	O
,	O
2014	O
)	O
proposed	O
context	O
dependent	O
FET	O
and	O
use	O
three	O
heuristics	O
to	O
clean	O
the	O
noisy	O
labels	O
with	O
the	O
side	O
effect	O
of	O
losing	O
training	O
data	O
.	O
To	O
utilize	O
noisy	O
data	O
,	O
(	O
Ren	O
et	O
al	O
,	O
2016a	O
)	O
distinguished	O
the	O
loss	O
function	O
of	O
noisy	O
data	O
from	O
clean	O
data	O
via	O
partial	B-MetricName
label	I-MetricName
loss	I-MetricName
(	O
PLL	B-MetricName
)	O
.	O
(	O
Abhishek	O
et	O
al	O
,	O
2017	O
;	O
Xu	O
and	O
Barbosa	O
,	O
2018	O
)	O
proposed	O
variants	O
of	O
PLL	B-MetricName
,	O
which	O
still	O
suffer	O
from	O
confirmation	O
bias	O
.	O
(	O
Xu	O
and	O
Barbosa	O
,	O
2018	O
)	O
proposed	O
hierarchical	O
loss	O
to	O
handle	O
over	O
-	O
specific	O
noise	O
.	O
On	O
top	O
of	O
AFET	O
,	O
(	O
Ren	O
et	O
al	O
,	O
2016b	O
)	O
proposed	O
a	O
method	O
PLE	O
to	O
reduce	O
the	O
label	O
noise	O
,	O
which	O
lead	O
to	O
a	O
great	O
success	O
in	O
FET	B-MethodName
.	O
Because	O
label	O
noise	O
reduction	O
is	O
separated	O
from	O
the	O
learning	O
of	O
FET	B-MethodName
,	O
there	O
might	O
be	O
error	O
propagation	O
problem	O
.	O
Recently	O
,	O
(	O
Xin	O
et	O
al	O
,	O
2018b	O
)	O
proposed	O
utilizing	O
a	O
pretrained	O
language	O
model	O
measures	O
the	O
compatibility	O
between	O
context	O
and	O
type	O
names	O
,	O
and	O
use	O
it	O
to	O
repel	O
the	O
interference	O
of	O
noisy	O
labels	O
.	O
However	O
,	O
the	O
compatibility	O
got	O
by	O
language	O
model	O
may	O
not	O
be	O
right	O
and	O
type	O
information	O
is	O
defined	O
by	O
corpus	O
and	O
annotation	O
guidelines	O
rather	O
than	O
type	O
names	O
as	O
is	O
mentioned	O
in	O
(	O
Azad	O
et	O
al	O
,	O
2018	O
)	O
.	O
In	O
addition	O
,	O
there	O
are	O
some	O
work	O
about	O
entity	O
-	O
level	O
typing	O
which	O
aim	O
to	O
figure	O
out	O
the	O
types	O
of	O
entities	O
in	O
KB	O
(	O
Yaghoobzadeh	O
and	O
Schütze	O
,	O
2015	O
;	O
Jin	O
et	O
al	O
,	O
2018	O
)	O
.	O

Table	O
1	O
shows	O
performance	O
comparison	O
between	O
the	O
proposed	O
CLSC	B-MethodName
model	O
and	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
FET	B-TaskName
systems	O
.	O
On	O
both	O
benchmarks	O
,	O
the	O
CLSC	B-MethodName
model	O
achieves	O
the	O
best	O
performance	O
in	O
all	O
three	O
metrics	O
.	O
When	O
focusing	O
on	O
the	O
comparison	O
between	O
NFETC	B-MethodName
and	O
CLSC	B-MethodName
,	O
we	O
have	O
following	O
observation	O
:	O
Compact	O
Latent	O
Space	O
Clustering	O
shows	O
its	O
effectiveness	O
on	O
both	O
clean	O
data	O
and	O
noisy	O
data	O
.	O
By	O
applying	O
CLSC	B-MethodName
regularization	O
on	O
the	O
basic	O
NFETC	B-MethodName
model	O
,	O
we	O
observe	O
consistent	O
and	O
significant	O
performance	O
boost	O
;	O
Hierarchical	O
-	O
aware	O
loss	O
shows	O
significant	O
advantage	O
on	O
the	O
OntoNotes	B-DatasetName
dataset	O
,	O
while	O
showing	O
insignificant	O
performance	O
boost	O
on	O
the	O
BBN	B-DatasetName
dataset	O
.	O
This	O
is	O
due	O
to	O
different	O
distribution	O
of	O
labels	O
on	O
the	O
test	O
set	O
.	O
The	O
proportion	O
of	O
terminal	O
types	O
of	O
the	O
test	O
set	O
is	O
69	O
%	O
for	O
the	O
BBN	B-DatasetName
dataset	O
,	O
while	O
is	O
only	O
33	O
%	O
on	O
the	O
OntoNotes	B-DatasetName
dataset	O
.	O
Thus	O
,	O
applying	O
hierarchical	B-MetricName
-	I-MetricName
aware	I-MetricName
loss	I-MetricName
on	O
the	O
BBN	B-DatasetName
dataset	O
brings	O
little	O
improvement	O
;	O
Both	O
algorithms	O
are	O
able	O
to	O
utilize	O
noisy	O
data	O
to	O
improve	O
performance	O
,	O
so	O
we	O
would	O
like	O
to	O
further	O
study	O
their	O
performance	O
in	O
different	O
noisy	O
scenarios	O
in	O
following	O
discussions	O
.	O
By	O
principle	O
,	O
with	O
sufficient	O
amount	O
of	O
clean	O
training	O
data	O
,	O
most	O
typing	O
systems	O
can	O
achieve	O
satisfying	O
performance	O
.	O
To	O
further	O
study	O
the	O
robustness	O
of	O
the	O
methods	O
to	O
label	O
noise	O
,	O
we	O
compare	O
their	O
performance	O
with	O
the	O
presence	O
of	O
25	B-HyperparameterValue
%	I-HyperparameterValue
,	O
20	B-HyperparameterValue
%	I-HyperparameterValue
,	O
15	B-HyperparameterValue
%	I-HyperparameterValue
,	O
10	B-HyperparameterValue
%	I-HyperparameterValue
and	O
5	B-HyperparameterValue
%	I-HyperparameterValue
clean	B-HyperparameterName
training	I-HyperparameterName
data	I-HyperparameterName
and	O
all	O
noisy	O
training	O
data	O
.	O
Figure	O
5	O
shows	O
the	O
performance	O
curves	O
as	O
the	O
proportion	O
of	O
clean	O
data	O
drops	O
.	O
As	O
it	O
reveals	O
,	O
the	O
CLSC	B-MethodName
model	O
consistently	O
wins	O
in	O
the	O
comparison	O
.	O
The	O
advantage	O
is	O
especially	O
clear	O
on	O
the	O
BBN	B-DatasetName
dataset	O
,	O
which	O
offers	O
less	O
amount	O
of	O
training	O
data	O
.	O
Note	O
that	O
,	O
with	O
only	O
27.9	B-HyperparameterValue
%	I-HyperparameterValue
of	O
training	B-HyperparameterName
data	I-HyperparameterName
(	O
when	O
only	O
leaving	O
5	O
%	O
clean	O
data	O
)	O
on	O
the	O
BBN	B-DatasetName
dataset	O
,	O
the	O
CLSC	B-MethodName
model	O
yield	O
a	O
comparable	O
result	O
with	O
the	O
NFETC	B-MethodName
model	O
trained	O
on	O
full	O
data	O
.	O
This	O
comparison	O
clearly	O
shows	O
the	O
superiority	O
of	O
our	O
approach	O
in	O
the	O
effectiveness	O
of	O
utilizing	O
noisy	O
data	O
.	O

We	O
search	O
the	O
hyper	O
parameter	O
of	O
Ontonotes	B-DatasetName
and	O
BBN	B-DatasetName
respectively	O
via	O
Hyperopt	O
proposed	O
by	O
(	O
Bergstra	O
et	O
al	O
,	O
2013	O
)	O
.	O
Hyper	O
parameters	O
are	O
shown	O
in	O
Appendix	O
A.	O
We	O
optimize	O
the	O
model	O
via	O
Adam	O
Optimizer	O
.	O
The	O
full	O
hyper	O
parameters	O
includes	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
lr	B-HyperparameterName
,	O
the	O
dimension	O
d	B-HyperparameterName
p	I-HyperparameterName
of	O
word	O
position	O
embedding	O
,	O
the	O
dimension	O
d	B-HyperparameterName
l	I-HyperparameterName
of	O
the	O
mention	O
encoder	O
's	O
output	O
(	O
equal	O
to	O
the	O
dimension	O
of	O
the	O
context	O
encoder	O
's	O
ourput	O
)	O
,	O
the	O
input	B-HyperparameterName
dropout	I-HyperparameterName
keep	I-HyperparameterName
probability	I-HyperparameterName
p	B-HyperparameterName
i	I-HyperparameterName
and	O
output	B-HyperparameterName
dropout	I-HyperparameterName
keep	I-HyperparameterName
probability	I-HyperparameterName
p	B-HyperparameterName
o	I-HyperparameterName
for	O
LSTM	O
layers	O
(	O
in	O
context	O
encoder	O
and	O
LSTM	O
mention	O
encoder	O
)	O
,	O
the	O
L2	B-HyperparameterName
regularization	I-HyperparameterName
parameter	I-HyperparameterName
λ	B-HyperparameterName
,	O
the	O
factor	B-HyperparameterName
of	I-HyperparameterName
hierarchical	I-HyperparameterName
loss	I-HyperparameterName
normalization	I-HyperparameterName
α	B-HyperparameterName
(	O
α	B-HyperparameterName
>	O
0	O
means	O
use	O
the	O
normalization	O
)	O
,	O
BN	B-HyperparameterName
(	O
whether	O
using	O
Batch	O
normalization	O
)	O
,	O
the	O
max	B-HyperparameterName
step	I-HyperparameterName
S	B-HyperparameterName
lp	I-HyperparameterName
of	O
the	O
label	O
propagation	O
,	O
the	O
max	B-HyperparameterName
length	I-HyperparameterName
S	B-HyperparameterName
m	I-HyperparameterName
of	O
Markov	O
chain	O
,	O
the	O
influence	B-HyperparameterName
parameter	I-HyperparameterName
λ	B-HyperparameterName
clsc	I-HyperparameterName
of	O
CLSC	B-MethodName
,	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
B	B-HyperparameterName
,	O
the	O
number	B-HyperparameterName
n	I-HyperparameterName
of	I-HyperparameterName
hidden	I-HyperparameterName
layers	I-HyperparameterName
in	O
q	O
and	O
the	O
number	B-HyperparameterName
h	I-HyperparameterName
n	I-HyperparameterName
of	I-HyperparameterName
hidden	I-HyperparameterName
units	I-HyperparameterName
of	O
the	O
hidden	O
layers	O
.	O
We	O
implement	O
all	O
models	O
using	O
Tensorflow	O
4	O
.	O

For	O
evaluation	O
metrics	O
,	O
we	O
adopt	O
strict	O
accuracy	B-MetricName
,	O
loose	B-MetricName
macro	I-MetricName
,	O
and	O
loose	B-MetricName
micro	I-MetricName
F	I-MetricName
-	I-MetricName
scores	I-MetricName
widely	O
used	O
in	O
the	O
FET	B-TaskName
task	O
(	O
Ling	O
and	O
Weld	O
,	O
2012	O
)	O
.	O
To	O
fine	O
tuning	O
the	O
hyper	O
-	O
parameters	O
,	O
we	O
randomly	O
sampled	O
10	O
%	O
of	O
the	O
test	O
set	O
as	O
a	O
development	O
set	O
for	O
both	O
datasets	O
.	O
With	O
the	O
fine	O
-	O
tuned	O
hyperparameter	O
as	O
mentioned	O
in	O
4.4	O
,	O
we	O
run	O
the	O
model	O
five	O
times	O
and	O
report	O
the	O
average	O
strict	O
accuracy	B-MetricName
,	O
macro	B-MetricName
F1	I-MetricName
and	O
micro	B-MetricName
F1	I-MetricName
on	O
the	O
test	O
set	O
.	O

We	O
compare	O
the	O
proposed	O
method	O
with	O
several	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
FET	B-TaskName
systems	O
3	O
:	O
Attentive	B-MethodName
(	O
Shimaoka	O
et	O
al	O
,	O
2016	O
)	O
uses	O
an	O
attention	O
based	O
feature	O
extractor	O
and	O
does	O
n't	O
distinguish	O
clean	O
from	O
noisy	O
data	O
;	O
AFET	B-MethodName
(	O
Ren	O
et	O
al	O
,	O
2016a	O
)	O
trains	O
label	O
embedding	O
with	O
partial	O
label	O
loss	O
;	O
AAA	B-MethodName
(	O
Abhishek	O
et	O
al	O
,	O
2017	O
)	O
learns	O
joint	O
representation	O
of	O
mentions	O
and	O
type	O
labels	O
;	O
PLE+HYENA	B-MethodName
/	I-MethodName
FIGER	I-MethodName
(	O
Ren	O
et	O
al	O
,	O
2016b	O
)	O
proposes	O
heterogeneous	O
partial	O
-	O
label	O
embedding	O
for	O
label	O
noise	O
reduction	O
to	O
boost	O
typing	O
systems	O
.	O
We	O
compare	O
two	O
PLE	O
models	O
with	O
HYENA	B-MethodName
(	O
Yogatama	O
et	O
al	O
,	O
2015	O
)	O
and	O
FIGER	B-MethodName
(	O
Ling	O
and	O
Weld	O
,	O
2012	O
)	O
as	O
the	O
base	O
typing	O
system	O
respectively	O
;	O
NFETC	B-MethodName
(	O
Xu	O
and	O
Barbosa	O
,	O
2018	O
)	O
trains	O
neural	O
fine	O
-	O
grained	O
typing	O
system	O
with	O
hierarchy	O
-	O
aware	O
loss	O
.	O
We	O
compare	O
the	O
performance	O
of	O
the	O
NFETC	B-MethodName
model	O
with	O
two	O
different	O
loss	O
functions	O
:	O
partial	B-MetricName
-	I-MetricName
label	I-MetricName
loss	I-MetricName
and	O
PLL+hierarchical	B-MetricName
loss	I-MetricName
.	O
We	O
denote	O
the	O
two	O
variants	O
as	O
NFETC	B-MethodName
and	O
NFETC	B-MethodName
hier	I-MethodName
respectively	O
;	O
NFETC	B-MethodName
-	I-MethodName
CLSC	I-MethodName
is	O
the	O
proposed	O
model	O
in	O
this	O
work	O
.	O
We	O
use	O
the	O
NFETC	B-MethodName
model	O
as	O
our	O
base	O
model	O
,	O
based	O
on	O
which	O
we	O
apply	O
Compact	O
Latent	O
Space	O
Clustering	O
Regularization	O
as	O
described	O
in	O
Section	O
3.2	O
;	O
Similarly	O
,	O
we	O
report	O
results	O
produced	O
by	O
using	O
both	O
KLdivergense	B-MetricName
-	O
based	O
loss	O
(	O
NFETC	B-MethodName
-	I-MethodName
CLSC	I-MethodName
)	O
and	O
KL+hierarchical	B-MetricName
loss	I-MetricName
(	O
NFETC	B-MethodName
-	I-MethodName
CLSC	I-MethodName
hier	I-MethodName
)	O
.	O

Given	O
the	O
representation	O
of	O
a	O
mention	O
,	O
the	O
type	O
posterior	O
is	O
given	O
by	O
a	O
standard	O
softmax	O
classifier	O
parameterized	O
by	O
θ	O
g	O
:	O
P	O
(	O
ŷ	O
i	O
|	O
z	O
i	O
;	O
θ	O
g	O
)	O
=	O
sof	O
tmax	O
(	O
W	O
c	O
z	O
i	O
+	O
b	O
c	O
)	O
,	O
(	O
12	O
)	O
where	O
W	O
c	O
R	O
K×dz	O
is	O
a	O
parameter	O
matrix	O
,	O
b	O
R	O
K	O
is	O
the	O
bias	O
vector	O
,	O
where	O
K	O
is	O
the	O
number	O
of	O
types	O
.	O
The	O
predicted	O
type	O
is	O
then	O
given	O
byt	O
i	O
=	O
argmax	O
y	O
i	O
P	O
(	O
ŷ	O
i	O
|	O
z	O
i	O
;	O
θ	O
g	O
)	O
.	O
Our	O
loss	O
function	O
consists	O
of	O
two	O
parts	O
.	O
L	O
sup	O
is	O
supervision	O
loss	O
defined	O
by	O
KL	B-MetricName
divergence	I-MetricName
:	O
L	O
sup	O
=	O
−	O
1	O
B	O
c	O
Bc	O
i=1	O
K	O
k=1	O
y	O
ik	O
log	O
(	O
P	O
(	O
y	O
i	O
|	O
z	O
i	O
;	O
θ	O
g	O
)	O
)	O
k	O
(	O
13	O
)	O
Here	O
B	O
c	O
is	O
the	O
number	O
of	O
clean	O
data	O
in	O
a	O
training	O
batch	O
,	O
K	O
is	O
the	O
number	O
of	O
target	O
types	O
.	O
The	O
regularization	O
term	O
is	O
given	O
by	O
L	O
clsc	O
.	O
Hence	O
,	O
the	O
overall	O
loss	O
function	O
is	O
:	O
L	O
f	O
inal	O
=	O
L	O
sup	O
+	O
λ	B-HyperparameterName
clsc	I-HyperparameterName
×	O
L	O
clsc	O
(	O
14	O
)	O
λ	B-HyperparameterName
clsc	I-HyperparameterName
is	O
a	O
hyper	O
parameter	O
to	O
control	O
the	O
influence	O
of	O
CLSC	O
.	O

Figure	O
3	O
illustrates	O
our	O
feature	O
extractor	O
.	O
For	O
fair	O
comparison	O
,	O
we	O
adopt	O
the	O
same	O
feature	O
extraction	O
pipeline	O
as	O
used	O
in	O
(	O
Xu	O
and	O
Barbosa	O
,	O
2018	O
)	O
.	O
The	O
feature	O
extractor	O
is	O
composed	O
of	O
an	O
embedding	O
layer	O
and	O
two	O
encoders	O
which	O
encode	O
mentions	O
and	O
contexts	O
respectively	O
.	O
Embedding	O
Layer	O
:	O
The	O
output	O
of	O
this	O
layer	O
is	O
a	O
concatenation	O
of	O
word	O
embedding	O
and	O
word	O
position	O
embedding	O
.	O
We	O
use	O
the	O
popular	O
300dimensional	B-HyperparameterValue
word	O
embedding	O
supplied	O
by	O
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
to	O
capture	O
the	O
semantic	O
information	O
and	O
random	O
initialized	O
position	O
embedding	O
(	O
Zeng	O
et	O
al	O
,	O
2014	O
)	O
to	O
acquire	O
information	O
about	O
the	O
relation	O
between	O
words	O
and	O
the	O
mentions	O
.	O
Formally	O
,	O
Given	O
a	O
word	O
embedding	O
matrix	O
W	O
word	O
of	O
shape	O
d	O
w	O
×	O
|	O
V	O
|	O
,	O
where	O
V	O
is	O
the	O
vocabulary	O
and	O
d	O
w	O
is	O
the	O
size	B-HyperparameterName
of	I-HyperparameterName
word	I-HyperparameterName
embedding	I-HyperparameterName
,	O
each	O
column	O
of	O
W	O
word	O
represents	O
a	O
specific	O
word	O
w	O
in	O
V	O
.	O
We	O
map	O
each	O
word	O
w	O
j	O
in	O
(	O
m	O
i	O
,	O
c	O
i	O
)	O
to	O
a	O
word	O
embedding	O
w	O
d	O
j	O
R	O
dw	O
.	O
Analogously	O
,	O
we	O
get	O
the	O
word	O
position	O
embedding	O
w	O
p	O
j	O
R	O
dp	O
of	O
each	O
word	O
according	O
to	O
the	O
relative	O
distance	O
between	O
the	O
word	O
and	O
the	O
mention	O
,	O
we	O
only	O
use	O
a	O
fixed	O
length	O
context	O
here	O
.	O
The	O
final	O
embedding	O
of	O
the	O
j	O
-	O
th	O
word	O
is	O
w	O
E	O
j	O
=	O
[	O
w	O
d	O
j	O
,	O
w	O
p	O
j	O
]	O
.	O
Mention	O
Encoder	O
:	O
To	O
capture	O
lexical	O
level	O
information	O
of	O
mentions	O
,	O
an	O
averaging	O
mention	O
encoder	O
and	O
a	O
LSTM	O
mention	O
encoder	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
is	O
applied	O
to	O
encode	O
mentions	O
.	O
Given	O
m	O
i	O
=	O
(	O
w	O
s	O
,	O
w	O
s+1	O
,	O
,	O
w	O
e	O
)	O
,	O
the	O
aver	O
-	O
aging	O
mention	O
representation	O
r	O
a	O
i	O
R	O
dw	O
is	O
:	O
r	O
a	O
i	O
=	O
1	O
e	O
−	O
s	O
+	O
1	O
e	O
j	O
=	O
s	O
w	O
d	O
j	O
(	O
1	O
)	O
By	O
applying	O
a	O
LSTM	O
over	O
an	O
extended	O
mention	O
(	O
w	O
s−1	O
,	O
w	O
s	O
,	O
w	O
s+1	O
,	O
,	O
w	O
e	O
,	O
w	O
e+1	O
)	O
,	O
we	O
get	O
a	O
sequence	O
(	O
h	O
s−1	O
,	O
h	O
s	O
,	O
h	O
s+1	O
,	O
,	O
h	O
e	O
,	O
h	O
e+1	O
)	O
.	O
We	O
use	O
h	O
e+1	O
as	O
LSTM	O
mention	O
representation	O
r	O
l	O
i	O
R	O
d	O
l	O
.	O
The	O
final	O
mention	O
representation	O
is	O
r	O
m	O
i	O
=	O
[	O
r	O
a	O
i	O
,	O
r	O
l	O
i	O
]	O
R	O
dw+d	O
l	O
.	O
Context	O
Encoder	O
:	O
A	O
bidirectional	O
LSTM	O
with	O
d	O
l	O
hidden	O
units	O
is	O
employed	O
to	O
encode	O
embedding	O
se	O
-	O
quence	O
(	O
w	O
E	O
s−W	O
,	O
w	O
E	O
s−W+1	O
,	O
,	O
w	O
E	O
e+W	O
)	O
:	O
−	O
h	O
j	O
=	O
LST	O
M	O
(	O
−−	O
h	O
j−1	O
,	O
w	O
E	O
j−1	O
)	O
−	O
h	O
j	O
=	O
LST	O
M	O
(	O
−−	O
h	O
j−1	O
,	O
w	O
E	O
j−1	O
)	O
h	O
j	O
=	O
[	O
−	O
h	O
j	O
−	O
h	O
j	O
]	O
(	O
2	O
)	O
where	O
denotes	O
element	O
-	O
wise	O
plus	O
.	O
Then	O
,	O
the	O
word	O
-	O
level	O
attention	O
mechanism	O
computes	O
a	O
score	O
β	O
i	O
,	O
j	O
over	O
different	O
word	O
j	O
in	O
the	O
context	O
c	O
i	O
to	O
get	O
the	O
final	O
context	O
representation	O
r	O
c	O
i	O
:	O
α	O
j	O
=	O
w	O
T	O
tanh	O
(	O
h	O
j	O
)	O
β	O
i	O
,	O
j	O
=	O
exp	O
(	O
α	O
j	O
)	O
k	O
exp	O
(	O
α	O
k	O
)	O
r	O
c	O
i	O
=	O
j	O
β	O
i	O
,	O
j	O
h	O
i	O
,	O
j	O
(	O
3	O
)	O
We	O
use	O
r	O
i	O
=	O
[	O
r	O
m	O
i	O
,	O
r	O
c	O
i	O
]	O
R	O
dz	O
=	O
R	O
dw+d	O
l	O
+	O
d	O
l	O
as	O
the	O
feature	O
representation	O
of	O
(	O
m	O
i	O
,	O
c	O
i	O
)	O
and	O
use	O
a	O
Neural	O
Networks	O
q	O
over	O
r	O
i	O
to	O
get	O
the	O
feature	O
vector	O
z	O
i	O
.	O
q	O
has	O
n	B-HyperparameterValue
layers	B-HyperparameterName
with	O
h	B-HyperparameterValue
n	I-HyperparameterValue
hidden	B-HyperparameterName
units	I-HyperparameterName
and	O
use	O
ReLu	B-HyperparameterName
activation	I-HyperparameterName
.	O

We	O
employ	O
a	O
critic	O
network	O
to	O
compute	O
the	O
reinforcement	O
learning	O
rewards	O
for	O
our	O
generators	O
.	O
We	O
use	O
a	O
binary	O
classifier	O
as	O
critic	O
by	O
extracting	O
training	O
instances	O
(	O
s	O
,	O
r	O
,	O
L	O
=	O
1	O
)	O
,	O
3	O
(	O
s	O
A	O
,	O
r	O
A	O
,	O
L	O
=	O
1	O
)	O
and	O
(	O
s	O
B	O
,	O
r	O
B	O
,	O
L	O
=	O
1	O
)	O
.	O
Then	O
we	O
can	O
derive	O
two	O
negative	O
samples	O
as	O
:	O
(	O
s	O
A	O
,	O
r	O
B	O
,	O
L	O
=	O
0	O
)	O
and	O
(	O
s	O
B	O
,	O
r	O
A	O
,	O
L	O
=	O
0	O
)	O
.	O
Thereafter	O
,	O
we	O
fine	O
-	O
tune	O
on	O
a	O
binary	O
classifier	O
to	O
be	O
used	O
as	O
our	O
critic	O
in	O
RL	O
on	O
the	O
training	O
partition	O
by	O
minimizing	O
the	O
binary	B-MetricName
cross	I-MetricName
-	I-MetricName
entropy	I-MetricName
loss	I-MetricName
:	O
−Llog	O
(	O
P	O
(	O
L	O
|	O
s	O
,	O
r	O
)	O
)	O
−	O
(	O
1−L	O
)	O
log	O
(	O
1	O
−	O
P	O
(	O
L	O
|	O
s	O
,	O
r	O
)	O
)	O
,	O
where	O
the	O
binary	O
label	O
L	O
indicates	O
whether	O
the	O
response	O
is	O
relevant	O
to	O
the	O
personas	O
.	O
We	O
then	O
use	O
this	O
classifier	O
acting	O
as	O
a	O
critic	O
network	O
that	O
outputsL	O
,	O
conditioned	O
on	O
the	O
generated	O
partner	O
personasp	O
and	O
generated	O
responser	O
.	O
The	O
predicted	O
binary	O
labelL	O
is	O
then	O
converted	O
to	O
a	O
reward	O
R.	O
R	O
is	O
a	O
positive	O
reward	O
whenL	O
=	O
1	O
,	O
and	O
R	O
is	O
a	O
negative	O
reward	O
whenL	O
=	O
0	O
.	O
We	O
empirically	O
set	O
the	O
reward	O
R	B-HyperparameterName
for	O
RL	O
to	O
{	O
1	B-HyperparameterValue
,	O
-	B-HyperparameterValue
1	I-HyperparameterValue
}	O
for	O
both	O
PPG	B-MethodName
and	O
DRG	B-MethodName
.	O
We	O
then	O
update	O
our	O
RL	O
agents	O
with	O
the	O
following	O
gradients	O
:	O
∆θ	O
PPG	O
=	O
−R	O
▽	O
θ	O
PPG	O
log	O
P	O
(	O
p	O
|	O
s	O
,	O
c	O
)	O
for	O
the	O
partner	B-MethodName
personas	I-MethodName
generator	I-MethodName
(	O
PPG	B-MethodName
)	O
,	O
and	O
for	O
the	O
dialogue	B-MethodName
response	I-MethodName
generator	I-MethodName
(	O
DRG	B-MethodName
)	O
:	O
∆θ	O
DRG	O
=	O
−R	O
▽	O
θ	O
DRG	O
log	O
P	O
(	O
r	O
|	O
s	O
,	O
p	O
,	O
c	O
)	O
By	O
formulating	O
a	O
reward	O
that	O
measures	O
the	O
relevance	O
between	O
generated	O
partner	O
personas	O
and	O
generated	O
dialogue	O
response	O
,	O
we	O
are	O
motivated	O
by	O
the	O
following	O
objectives	O
:	O
Further	O
fine	O
-	O
tune	O
the	O
partner	O
personas	O
generator	O
to	O
generate	O
personas	O
that	O
benefits	O
the	O
downstream	O
dialogue	O
response	O
generation	O
.	O
Further	O
fine	O
-	O
tune	O
the	O
dialogue	O
response	O
generator	O
trained	O
with	O
ground	O
-	O
truth	O
partner	O
personas	O
to	O
adapt	O
to	O
noisy	O
partner	O
personas	O
generated	O
by	O
the	O
partner	O
personas	O
generator	O
.	O
As	O
mentioned	O
in	O
Section	O
3.1	O
,	O
the	O
first	O
motivation	O
is	O
that	O
we	O
are	O
generating	O
the	O
complete	O
personas	O
profile	O
.	O
However	O
,	O
some	O
of	O
them	O
can	O
be	O
irrelevant	O
and	O
unhelpful	O
for	O
the	O
next	O
-	O
turn	O
dialogue	O
response	O
generation	O
.	O
It	O
could	O
be	O
challenging	O
for	O
the	O
partner	O
personas	O
generator	O
alone	O
to	O
identify	O
which	O
personas	O
could	O
be	O
helpful	O
.	O
Therefore	O
,	O
we	O
design	O
such	O
a	O
reward	O
to	O
train	O
the	O
personas	O
generator	O
to	O
learn	O
to	O
generate	O
a	O
set	O
of	O
personas	O
that	O
is	O
more	O
helpful	O
for	O
the	O
downstream	O
dialogue	O
response	O
generation	O
.	O
Our	O
second	O
motivation	O
is	O
that	O
the	O
dialogue	O
response	O
generator	O
has	O
not	O
been	O
exposed	O
to	O
the	O
generated	O
partner	O
personas	O
.	O
We	O
would	O
like	O
to	O
fine	O
-	O
tune	O
the	O
response	O
generator	O
to	O
mitigate	O
the	O
potential	O
traininginference	O
discrepancy	O
.	O
Experimental	O
results	O
indicate	O
that	O
our	O
design	O
empirically	O
works	O
well	O
.	O
The	O
previous	O
work	O
from	O
Cai	O
et	O
al	O
(	O
2019a	O
)	O
employed	O
critic	O
network	O
for	O
RL	O
loss	O
backpropagation	O
.	O
The	O
major	O
difference	O
is	O
that	O
their	O
critic	O
is	O
trained	O
in	O
an	O
adversarial	O
manner	O
(	O
Li	O
et	O
al	O
,	O
2018	O
)	O
to	O
pick	O
up	O
the	O
gold	O
response	O
among	O
other	O
negative	O
candidates	O
.	O
Also	O
,	O
their	O
critic	O
network	O
conditions	O
only	O
on	O
the	O
dialogue	O
response	O
but	O
not	O
on	O
the	O
generated	O
skeleton	O
.	O
In	O
contrast	O
,	O
we	O
aim	O
for	O
improved	O
response	O
generation	O
with	O
a	O
classifier	O
conditioning	O
on	O
both	O
the	O
generated	O
personas	O
and	O
the	O
generated	O
response	O
.	O

Overview	O
.	O
The	O
basic	O
assumptions	O
of	O
our	O
idea	O
are	O
:	O
(	O
1	O
)	O
all	O
mentions	O
belong	O
to	O
the	O
same	O
type	O
should	O
be	O
close	O
to	O
each	O
other	O
in	O
the	O
representation	O
space	O
because	O
they	O
should	O
have	O
similar	O
context	O
,	O
(	O
2	O
)	O
similar	O
contexts	O
lead	O
to	O
the	O
same	O
type	O
.	O
For	O
clean	O
data	O
,	O
we	O
compact	O
the	O
representation	O
space	O
of	O
the	O
same	O
type	O
to	O
comply	O
(	O
1	O
)	O
.	O
For	O
noisy	O
data	O
,	O
given	O
assumption	O
(	O
2	O
)	O
,	O
we	O
infer	O
the	O
their	O
type	O
distributions	O
via	O
label	O
propagation	O
and	O
candidate	O
types	O
constrain	O
.	O
Figure	O
2	O
shows	O
the	O
overall	O
framework	O
of	O
the	O
proposed	O
method	O
.	O
Clean	O
data	O
is	O
used	O
to	O
train	O
classifier	O
and	O
feature	O
extractor	O
end	O
-	O
to	O
-	O
endly	O
,	O
while	O
noisy	O
data	O
is	O
only	O
used	O
in	O
CLSC	O
regularization	O
.	O
Formally	O
,	O
given	O
a	O
batch	O
of	O
samples	O
{	O
(	O
m	O
i	O
,	O
c	O
i	O
,	O
Y	O
t	O
i	O
)	O
}	O
B	O
i=1	O
,	O
we	O
first	O
convert	O
each	O
sample	O
(	O
m	O
i	O
,	O
c	O
i	O
)	O
into	O
a	O
real	O
-	O
valued	O
vector	O
z	O
i	O
via	O
a	O
feature	O
extractor	O
z	O
(	O
(	O
m	O
i	O
,	O
c	O
i	O
)	O
;	O
θ	O
z	O
)	O
parameterized	O
by	O
θ	O
z	O
.	O
Then	O
a	O
type	O
classifier	O
g	O
(	O
z	O
i	O
;	O
θ	O
g	O
)	O
parameterized	O
by	O
θ	O
g	O
gives	O
the	O
posterior	O
P	O
(	O
y	O
|	O
z	O
i	O
;	O
θ	O
g	O
)	O
.	O
By	O
incorporating	O
CLSC	O
regularization	O
in	O
the	O
objective	O
function	O
,	O
we	O
encourage	O
the	O
feature	O
extractor	O
z	O
to	O
group	O
mentions	O
of	O
the	O
same	O
type	O
into	O
a	O
compact	O
cluster	O
,	O
which	O
facilitates	O
classification	O
as	O
is	O
shown	O
in	O
Figure	O
1	O
.	O
Noisy	O
data	O
enhances	O
the	O
formation	O
of	O
compact	O
clusters	O
with	O
the	O
help	O
of	O
label	O
propagation	O
.	O

Recent	O
years	O
have	O
seen	O
a	O
surge	O
of	O
interests	O
in	O
fine	B-TaskName
-	I-TaskName
grained	I-TaskName
entity	I-TaskName
typing	I-TaskName
(	O
FET	B-TaskName
)	O
as	O
it	O
serves	O
as	O
an	O
important	O
cornerstone	O
of	O
several	O
nature	O
language	O
processing	O
tasks	O
including	O
relation	O
extraction	O
(	O
Mintz	O
et	O
al	O
,	O
2009	O
)	O
,	O
entity	O
linking	O
(	O
Raiman	O
and	O
Raiman	O
,	O
2018	O
)	O
,	O
and	O
knowledge	O
base	O
completion	O
(	O
Dong	O
et	O
al	O
,	O
2014	O
)	O
.	O
To	O
reduce	O
manual	O
efforts	O
in	O
labelling	O
training	O
data	O
,	O
distant	O
supervision	O
(	O
Mintz	O
et	O
al	O
,	O
2009	O
)	O
has	O
been	O
widely	O
adopted	O
by	O
recent	O
FET	O
systems	O
.	O
With	O
the	O
help	O
of	O
an	O
external	O
knowledge	O
base	O
(	O
KB	O
)	O
,	O
an	O
entity	O
mention	O
is	O
first	O
Figure	O
1	O
:	O
T	O
-	O
SNE	O
visualization	O
of	O
the	O
mention	O
embeddings	O
generated	O
by	O
NFETC	O
(	O
left	O
)	O
and	O
CLSC	O
(	O
right	O
)	O
on	O
the	O
BBN	O
dataset	O
.	O
Our	O
model	O
(	O
CLSC	B-MethodName
)	O
clearly	O
groups	O
mentions	O
of	O
the	O
same	O
type	O
into	O
a	O
compact	O
cluster	O
.	O
linked	O
to	O
an	O
existing	O
entity	O
in	O
KB	O
,	O
and	O
then	O
labeled	O
with	O
all	O
possible	O
types	O
of	O
the	O
KB	O
entity	O
as	O
supervision	O
.	O
However	O
,	O
despite	O
its	O
efficiency	O
,	O
distant	O
supervision	O
also	O
brings	O
the	O
challenge	O
of	O
outof	O
-	O
context	O
noise	O
,	O
as	O
it	O
assigns	O
labels	O
in	O
a	O
context	O
agnostic	O
manner	O
.	O
Early	O
works	O
usually	O
ignore	O
such	O
noise	O
in	O
supervision	O
(	O
Ling	O
and	O
Weld	O
,	O
2012	O
;	O
Shimaoka	O
et	O
al	O
,	O
2016	O
)	O
,	O
which	O
dampens	O
the	O
performance	O
of	O
distantly	O
supervised	O
models	O
.	O
Towards	O
overcoming	O
out	O
-	O
of	O
-	O
context	O
noise	O
,	O
two	O
lines	O
of	O
work	O
have	O
been	O
proposed	O
to	O
distantly	O
supervised	O
FET	B-TaskName
.	O
The	O
first	O
kind	O
of	O
work	O
try	O
to	O
filter	O
out	O
noisy	O
labels	O
using	O
heuristic	O
rules	O
(	O
Gillick	O
et	O
al	O
,	O
2014	O
)	O
.	O
However	O
,	O
such	O
heuristic	O
pruning	O
significantly	O
reduces	O
the	O
amount	O
of	O
training	O
data	O
,	O
and	O
thus	O
can	O
not	O
make	O
full	O
use	O
of	O
distantly	O
annotated	O
data	O
.	O
In	O
contrast	O
,	O
the	O
other	O
thread	O
of	O
works	O
try	O
to	O
incorporate	O
such	O
imperfect	O
annotation	O
by	O
partiallabel	B-MetricName
loss	I-MetricName
(	O
PLL	B-MetricName
)	O
.	O
The	O
basic	O
assumption	O
is	O
that	O
,	O
for	O
a	O
noisy	O
mention	O
,	O
the	O
maximum	O
score	O
associated	O
with	O
its	O
candidate	O
types	O
should	O
be	O
greater	O
than	O
the	O
scores	O
associated	O
with	O
any	O
other	O
non	O
-	O
candidate	O
types	O
(	O
Ren	O
et	O
al	O
,	O
2016a	O
;	O
Abhishek	O
et	O
al	O
,	O
2017	O
;	O
Xu	O
and	O
Barbosa	O
,	O
2018	O
)	O
.	O
Despite	O
their	O
success	O
,	O
PLLbased	O
models	O
still	O
suffer	O
from	O
Confirmation	O
Bias	O
by	O
taking	O
its	O
own	O
prediction	O
as	O
optimization	O
objective	O
in	O
the	O
next	O
step	O
.	O
Specifically	O
,	O
given	O
an	O
entity	O
mention	O
,	O
if	O
the	O
typing	O
system	O
selected	O
a	O
wrong	O
type	O
with	O
the	O
maximum	O
score	O
among	O
all	O
candidates	O
,	O
it	O
will	O
try	O
to	O
further	O
maximize	O
the	O
score	O
of	O
the	O
wrong	O
type	O
in	O
following	O
optimization	O
epoches	O
(	O
in	O
order	O
to	O
minimize	O
PLL	B-MetricName
)	O
,	O
thus	O
amplifying	O
the	O
confirmation	O
bias	O
.	O
Such	O
bias	O
starts	O
from	O
the	O
early	O
stage	O
of	O
training	O
,	O
when	O
the	O
typing	O
model	O
is	O
still	O
very	O
suboptimal	O
,	O
and	O
can	O
accumulate	O
in	O
training	O
process	O
.	O
Related	O
discussion	O
can	O
be	O
also	O
found	O
in	O
the	O
setting	O
of	O
semi	O
-	O
supervised	O
learning	O
(	O
Lee	O
et	O
al	O
,	O
2006	O
;	O
Laine	O
and	O
Aila	O
,	O
2017	O
;	O
Tarvainen	O
and	O
Valpola	O
,	O
2017	O
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
new	O
method	O
for	O
distantly	O
supervised	O
fine	O
-	O
grained	O
entity	O
typing	O
.	O
Enlightened	O
by	O
(	O
Kamnitsas	O
et	O
al	O
,	O
2018	O
)	O
,	O
we	O
propose	O
to	O
effectively	O
utilize	O
imperfect	O
annotation	O
as	O
model	O
regularization	O
via	O
Compact	O
Latent	O
Space	O
Clustering	O
(	O
CLSC	O
)	O
.	O
More	O
specifically	O
,	O
our	O
model	O
encourages	O
the	O
feature	O
extractor	O
to	O
group	O
mentions	O
of	O
the	O
same	O
type	O
as	O
a	O
compact	O
cluster	O
(	O
dense	O
region	O
)	O
in	O
the	O
representation	O
space	O
,	O
which	O
leads	O
to	O
better	O
classification	O
performance	O
.	O
For	O
training	O
data	O
with	O
noisy	O
labels	O
,	O
instead	O
of	O
generating	O
pseudo	O
supervision	O
by	O
the	O
typing	O
model	O
itself	O
,	O
we	O
dynamically	O
construct	O
a	O
similarity	O
-	O
weighted	O
graph	O
between	O
clean	O
and	O
noisy	O
mentions	O
,	O
and	O
apply	O
label	O
propagation	O
on	O
the	O
graph	O
to	O
help	O
the	O
formation	O
of	O
compact	O
clusters	O
.	O
Figure	O
1	O
demonstrates	O
the	O
effectiveness	O
of	O
our	O
method	O
in	O
clustering	O
mentions	O
of	O
different	O
types	O
into	O
dense	O
regions	O
.	O
In	O
contrast	O
to	O
PLL	O
-	O
based	O
models	O
,	O
we	O
do	O
not	O
force	O
the	O
model	O
to	O
fit	O
pseudo	O
supervision	O
generated	O
by	O
itself	O
,	O
but	O
only	O
use	O
noisy	O
data	O
as	O
part	O
of	O
regularization	O
for	O
our	O
feature	O
extractor	O
layer	O
,	O
thus	O
avoiding	O
bias	O
accumulation	O
.	O
Extensive	O
experiments	O
on	O
standard	O
benchmarks	O
show	O
that	O
our	O
method	O
consistently	O
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
.	O
Further	O
study	O
reveals	O
that	O
,	O
the	O
advantage	O
of	O
our	O
model	O
over	O
the	O
competitors	O
gets	O
even	O
more	O
significant	O
as	O
the	O
portion	O
of	O
noisy	O
data	O
rises	O
.	O

This	O
last	O
run	O
shares	O
the	O
same	O
features	O
as	O
the	O
previous	O
run	O
(	O
assigning	O
higher	O
relevances	O
to	O
corresponding	O
corpora	O
)	O
but	O
this	O
time	O
our	O
bilingual	O
lexicon	O
and	O
named	O
entities	O
database	O
was	O
included	O
for	O
term	O
coverage	O
improvement	O
,	O
and	O
an	O
alignment	O
based	O
on	O
cognates	O
(	O
Gomes	O
and	O
Lopes	O
,	O
2011	O
)	O
is	O
used	O
.	O
About	O
our	O
bilingual	O
lexicon	O
,	O
considering	O
that	O
it	O
was	O
built	O
mainly	O
from	O
the	O
European	O
legislation	O
,	O
it	O
was	O
given	O
a	O
lower	O
relevance	O
because	O
past	O
experiences	O
have	O
shown	O
us	O
that	O
,	O
when	O
the	O
domain	O
is	O
not	O
shared	O
with	O
the	O
texts	O
to	O
be	O
translated	O
,	O
it	O
should	O
not	O
have	O
the	O
same	O
relevance	O
in	O
order	O
to	O
reduce	O
the	O
probability	O
of	O
using	O
inadequate	O
terms	O
for	O
the	O
intended	O
translation	O
domain	O
or	O
subject	O
.	O
Again	O
,	O
this	O
is	O
a	O
situation	O
that	O
has	O
also	O
been	O
confirmed	O
and	O
noted	O
in	O
Table	O
4	O
between	O
dev	B-DatasetName
-	I-DatasetName
europarl	I-DatasetName
and	O
deveuroparl	B-DatasetName
-	I-DatasetName
low	I-DatasetName
:	O
reducing	O
the	O
relevance	O
of	O
europarl	B-DatasetName
contributed	O
to	O
a	O
slight	O
score	O
increase	O
compared	O
to	O
when	O
the	O
relevance	O
is	O
the	O
same	O
.	O
As	O
a	O
side	O
note	O
,	O
translating	O
the	O
tests	O
took	O
nearly	O
14	O
hours	O
for	O
each	O
run	O
6	O
.	O
Had	O
we	O
included	O
europarl	B-DatasetName
,	O
judging	O
by	O
Table	O
4	O
,	O
we	O
would	O
have	O
taken	O
nearly	O
200	O
hours	O
,	O
which	O
is	O
more	O
than	O
a	O
week	O
,	O
expecting	O
to	O
simply	O
gain	O
0.75	B-MetricValue
BLEU	B-MetricName
points	O
,	O
on	O
average	O
,	O
so	O
we	O
had	O
no	O
other	O
option	O
than	O
leaving	O
it	O
out	O
.	O
Such	O
increase	O
in	O
translation	O
time	O
is	O
due	O
to	O
the	O
substantial	O
increase	O
of	O
translation	O
equivalents	O
available	O
for	O
decoding	O
from	O
such	O
a	O
large	O
corpus	O
.	O
The	O
decision	O
to	O
carry	O
out	O
the	O
alignment	O
based	O
on	O
cognates	O
was	O
taken	O
because	O
after	O
a	O
first	O
run	O
of	O
tests	O
we	O
realized	O
that	O
many	O
of	O
the	O
untranslated	O
terms	O
referred	O
to	O
medical	O
terms	O
and	O
diseases	O
,	O
which	O
shared	O
many	O
letters	O
between	O
both	O
languages	O
and	O
therefore	O
had	O
a	O
high	O
level	O
of	O
cognaticity	O
.	O
All	O
these	O
changes	O
allowed	O
a	O
significant	O
reduction	O
of	O
the	O
unique	O
untranslated	O
terms	O
to	O
a	O
total	O
of	O
4700	O
,	O
and	O
for	O
all	O
the	O
reasons	O
in	O
this	O
subsection	O
,	O
we	O
have	O
considered	O
this	O
run	O
as	O
being	O
our	O
best	O
.	O

Since	O
no	O
development	O
data	O
was	O
supplied	O
,	O
we	O
took	O
the	O
initiative	O
to	O
prepare	O
some	O
development	O
sets	O
in	O
order	O
to	O
have	O
an	O
idea	O
of	O
the	O
most	O
promising	O
set	O
of	O
parameters	O
to	O
be	O
used	O
in	O
our	O
system	O
over	O
the	O
provided	O
data	O
to	O
produce	O
the	O
intended	O
translations	O
.	O
As	O
such	O
,	O
several	O
documents	O
were	O
removed	O
from	O
the	O
original	O
training	O
data	O
,	O
composed	O
by	O
the	O
medlinepubmed	O
,	O
biological	O
and	O
health	O
sets	O
,	O
applying	O
the	O
training	O
methods	O
on	O
the	O
remaining	O
documents	O
and	O
using	O
the	O
selected	O
ones	O
to	O
translate	O
and	O
compare	O
the	O
translations	O
against	O
their	O
originals	O
by	O
determining	O
their	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
scores	O
.	O
However	O
,	O
in	O
order	O
to	O
get	O
a	O
clearer	O
picture	O
of	O
the	O
type	O
of	O
results	O
that	O
could	O
be	O
expected	O
,	O
some	O
additional	O
tests	O
were	O
carried	O
out	O
including	O
the	O
selected	O
set	O
of	O
documents	O
in	O
the	O
training	O
data	O
.	O
Our	O
translation	O
model	O
supports	O
:	O
a	O
conservative	O
extraction	O
approach	O
,	O
which	O
is	O
more	O
restrictive	O
,	O
allowing	O
fewer	O
translation	O
equivalents	O
,	O
having	O
a	O
lower	O
recall	B-MetricName
but	O
a	O
higher	O
precision	B-MetricName
;	O
and	O
a	O
flexible	O
extraction	O
approach	O
,	O
which	O
is	O
more	O
permissive	O
,	O
allowing	O
a	O
larger	O
number	O
of	O
equivalents	O
but	O
at	O
the	O
cost	O
of	O
an	O
increase	O
of	O
incorrect	O
ones	O
.	O
We	O
were	O
interested	O
in	O
evaluating	O
the	O
impact	O
of	O
both	O
approaches	O
on	O
results	O
.	O
Table	O
4	O
shows	O
the	O
average	O
results	O
on	O
both	O
translation	O
directions	O
of	O
those	O
preliminary	O
tests	O
,	O
consisting	O
of	O
the	O
average	O
BLEU	B-MetricName
scores	O
for	O
the	O
conservative	O
(	O
cons	O
.	O
)	O
and	O
flexible	O
(	O
flex	O
.	O
)	O
approaches	O
,	O
as	O
well	O
as	O
the	O
average	O
times	O
taken	O
to	O
translate	O
the	O
documents	O
on	O
either	O
extraction	O
approaches	O
.	O
Those	O
results	O
concern	O
the	O
following	O
configurations	O
:	O
full	O
:	O
the	O
documents	O
used	O
for	O
testing	O
were	O
not	O
removed	O
from	O
the	O
training	O
set	O
(	O
medlinepubmed	O
,	O
biological	O
and	O
health	O
)	O
;	O
dev	O
:	O
the	O
documents	O
used	O
for	O
testing	O
were	O
removed	O
form	O
the	O
training	O
set	O
;	O
dev	B-DatasetName
-	I-DatasetName
europarl	I-DatasetName
:	O
the	O
same	O
as	O
dev	O
,	O
but	O
including	O
the	O
europarl	B-DatasetName
corpus	O
;	O
and	O
dev	B-DatasetName
-	I-DatasetName
europarl	I-DatasetName
-	I-DatasetName
low	I-DatasetName
:	O
the	O
same	O
as	O
dev	B-DatasetName
-	I-DatasetName
europarl	I-DatasetName
,	O
but	O
assigned	O
a	O
lower	O
relevance	O
to	O
the	O
europarl	B-DatasetName
corpus	O
.	O
These	O
preliminary	O
tests	O
have	O
shown	O
that	O
the	O
flexible	O
extraction	O
approach	O
produced	O
on	O
average	O
better	O
translation	O
results	O
when	O
the	O
reference	O
documents	O
were	O
not	O
included	O
in	O
the	O
test	O
set	O
,	O
which	O
is	O
the	O
normal	O
testing	O
situation	O
,	O
so	O
we	O
used	O
the	O
flexible	O
approach	O
.	O
The	O
Europarl	B-DatasetName
corpus	O
4	O
,	O
which	O
is	O
significantly	O
larger	O
(	O
54	O
,	O
543	O
,	O
044	O
words	O
in	O
English	O
and	O
60	O
,	O
375	O
,	O
477	O
words	O
in	O
Portuguese	O
)	O
,	O
was	O
tested	O
as	O
a	O
source	O
of	O
additional	O
term	O
coverage	O
,	O
which	O
allowed	O
a	O
translation	O
quality	O
improvement	O
lower	O
than	O
1	B-MetricValue
BLEU	B-MetricName
point	O
.	O
However	O
,	O
given	O
its	O
significant	O
increase	O
in	O
processing	O
time	O
because	O
of	O
its	O
large	O
size	O
,	O
a	O
time	O
increase	O
around	O
14	O
times	O
larger	O
,	O
we	O
had	O
to	O
drop	O
it	O
from	O
the	O
submission	O
tests	O
due	O
to	O
deadline	O
constraints	O
.	O
Additionally	O
,	O
these	O
results	O
show	O
that	O
assigning	O
a	O
lower	O
relevance	O
to	O
a	O
corpus	O
from	O
a	O
totally	O
different	O
domain	O
may	O
have	O
some	O
positive	O
impact	O
on	O
average	O
results	O
.	O
Once	O
we	O
have	O
decided	O
,	O
from	O
this	O
initial	O
testing	O
preparation	O
,	O
which	O
would	O
be	O
the	O
most	O
promising	O
and	O
interesting	O
features	O
to	O
use	O
in	O
the	O
final	O
runs	O
,	O
we	O
ran	O
the	O
training	O
processes	O
again	O
to	O
include	O
the	O
documents	O
that	O
have	O
been	O
left	O
out	O
,	O
this	O
way	O
using	O
the	O
full	O
data	O
provided	O
by	O
the	O
organizers	O
for	O
the	O
runs	O
to	O
be	O
submitted	O
.	O

Our	O
EN	O
-	O
PT	O
input	O
lexicon	O
has	O
931	O
,	O
568	O
manually	O
validated	O
translations	O
(	O
words	O
and	O
phrases	O
)	O
.	O
This	O
lexicon	O
has	O
been	O
compiled	O
in	O
a	O
long	O
term	O
effort	O
started	O
in	O
the	O
context	O
of	O
project	O
ISTRION	B-DatasetName
2	O
.	O
The	O
translations	O
were	O
extracted	O
automatically	O
from	O
several	O
corpora	O
,	O
including	O
Europarl	B-DatasetName
(	O
Koehn	O
and	O
Monz	O
,	O
2005	O
)	O
,	O
JRC	B-DatasetName
-	I-DatasetName
Acquis	I-DatasetName
(	O
Steinberger	O
et	O
al	O
,	O
2006	O
)	O
,	O
OPUS	B-DatasetName
EMEA	I-DatasetName
(	O
Tiedemann	O
,	O
2009	O
)	O
and	O
others	O
,	O
using	O
a	O
combination	O
of	O
complementary	O
alignment	O
and	O
extraction	O
methods	O
:	O
GIZA	O
(	O
Och	O
and	O
Ney	O
,	O
2003	O
)	O
,	O
Anymalign	O
(	O
Lardilleux	O
and	O
Lepage	O
,	O
2009	O
)	O
,	O
spelling	O
similarity	O
measure	O
SpSim	O
(	O
Gomes	O
and	O
Lopes	O
,	O
2011	O
)	O
combined	O
with	O
co	O
-	O
occurrence	O
Dice	O
measure	O
,	O
and	O
others	O
.	O
The	O
automatically	O
extracted	O
word	O
and	O
phrasal	O
translations	O
were	O
automatically	O
classified	O
,	O
prior	O
to	O
human	O
validation	O
,	O
using	O
an	O
SVM	O
classifier	O
trained	O
on	O
previously	O
validated	O
translations	O
as	O
described	O
by	O
Mahesh	O
et	O
al	O
(	O
2015	O
)	O
.	O
The	O
automatic	O
classification	O
speeds	O
up	O
human	O
validation	O
because	O
very	O
few	O
translations	O
(	O
less	O
than	O
5	O
%	O
)	O
are	O
incorrectly	O
classified	O
,	O
and	O
only	O
those	O
need	O
to	O
be	O
manually	O
labeled	O
as	O
correct	O
or	O
incorrect	O
.	O
We	O
did	O
not	O
perform	O
any	O
extraction	O
or	O
validation	O
of	O
new	O
translations	O
from	O
the	O
corpus	O
provided	O
for	O
this	O
shared	O
task	O
.	O
We	O
did	O
,	O
however	O
,	O
complement	O
our	O
lexicon	O
with	O
cognate	O
and	O
homograph	O
alignments	O
using	O
the	O
SpSim	B-MetricName
(	O
Gomes	O
and	O
Lopes	O
,	O
2011	O
)	O
spelling	O
similarity	O
measure	O
.	O

For	O
the	O
knowledge	O
retrieval	O
module	O
,	O
we	O
retrieve	O
top	O
-	O
10	O
related	O
results	O
from	O
the	O
KB	O
.	O
For	O
iterative	B-TaskName
entity	I-TaskName
retrieval	I-TaskName
,	O
we	O
set	O
T	B-MetricName
=	O
2	B-MetricValue
.	O
In	O
masked	O
language	O
model	O
pretraining	O
,	O
we	O
use	O
a	O
learning	O
rate	O
of	O
5	O
×	O
10	O
−5	O
.	O
For	O
the	O
NER	O
module	O
,	O
we	O
use	O
a	O
learning	O
rate	O
of	O
5	O
×	O
10	O
−6	O
for	O
fine	O
-	O
tuning	O
the	O
XLM	O
-	O
R	O
embeddings	O
and	O
use	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.05	B-HyperparameterValue
to	O
update	O
the	O
parameters	O
in	O
the	O
CRF	O
layer	O
following	O
Wang	O
et	O
al	O
(	O
2021b	O
)	O
.	O
Each	O
NER	B-TaskName
model	O
built	O
by	O
our	O
system	O
can	O
be	O
trained	O
and	O
evaluated	O
on	O
a	O
single	O
Tesla	O
V100	O
GPU	O
with	O
16	O
GB	O
memory	O
.	O
For	O
the	O
ensemble	O
module	O
,	O
we	O
train	O
about	O
10	O
models	O
for	O
each	O
track	O
.	O
A.	O
(	O
Akbik	O
et	O
al	O
,	O
2018	O
)	O
,	O
ELMo	O
embeddings	O
(	O
Peters	O
et	O
al	O
,	O
2018	O
;	O
Che	O
et	O
al	O
,	O
2018	O
)	O
,	O
XLM	O
-	O
R	O
embeddings	O
fine	O
-	O
tuned	O
on	O
the	O
whole	O
training	O
data	O
and	O
XLM	O
-	O
R	O
embeddings	O
fine	O
-	O
tuned	O
on	O
the	O
language	O
data	O
by	O
multi	B-MethodName
-	I-MethodName
stage	I-MethodName
finetuning	I-MethodName
.	O
We	O
only	O
feed	O
the	O
knowledge	O
-	O
based	O
input	O
into	O
XLM	O
-	O
R	O
embeddings	O
and	O
feed	O
the	O
original	O
input	O
into	O
other	O
embeddings	O
because	O
it	O
is	O
hard	O
for	O
the	O
other	O
embeddings	O
(	O
especially	O
for	O
LSTM	O
-	O
based	O
embeddings	O
such	O
as	O
Flair	O
and	O
ELMo	O
)	O
to	O
encode	O
such	O
a	O
long	O
input	O
.	O
We	O
use	O
Bi	O
-	O
LSTM	O
encoder	O
to	O
encode	O
the	O
concatenated	O
embeddings	O
with	O
a	O
hidden	O
state	O
of	O
1	O
,	O
000	O
and	O
then	O
feed	O
the	O
output	O
token	O
representations	O
into	O
the	O
CRF	O
layer	O
.	O
Following	O
most	O
of	O
the	O
previous	O
efforts	O
,	O
we	O
use	O
SGD	O
optimizer	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.01	B-HyperparameterValue
.	O
For	O
ACE	O
,	O
we	O
search	O
the	O
embedding	O
concatenation	O
for	O
30	O
episodes	O
.	O

As	O
we	O
mentioned	O
in	O
Section	O
3.1	O
,	O
there	O
are	O
three	O
context	O
processing	O
options	O
,	O
which	O
are	O
:	O
1	O
)	O
use	O
the	O
matched	O
paragraph	O
;	O
2	O
)	O
use	O
the	O
matched	O
sentence	O
;	O
3	O
)	O
use	O
the	O
matched	O
sentence	O
but	O
remove	O
the	O
wiki	O
anchors	O
.	O
We	O
denote	O
the	O
three	O
options	O
as	O
PARA	O
,	O
SENT	O
and	O
SENT	O
-	O
LINK	O
respectively	O
.	O
Entity	B-TaskName
Retrieval	I-TaskName
with	O
Gold	O
Entities	O
We	O
use	O
gold	O
entities	O
on	O
the	O
development	O
set	O
to	O
see	O
whether	O
the	O
model	O
performance	O
can	O
be	O
improved	O
.	O
This	O
can	O
be	O
seen	O
as	O
the	O
most	O
ideal	O
scenario	O
for	O
iterative	O
retrieval	O
.	O
We	O
denote	O
this	O
process	O
as	O
ITER	B-MethodName
G	I-MethodName
and	O
use	O
PARA	B-MethodName
for	O
the	O
context	O
type	O
.	O
In	O
Table	O
3	O
,	O
we	O
can	O
observe	O
that	O
:	O
1	O
)	O
For	O
the	O
three	O
context	O
options	O
,	O
PARA	B-MethodName
is	O
the	O
best	O
option	O
for	O
EN	O
,	O
ES	O
,	O
NL	O
,	O
RU	O
,	O
TR	O
,	O
KO	O
,	O
FA	O
,	O
MIX	O
and	O
MULTI	O
.	O
SENT	B-MethodName
-	I-MethodName
LINK	I-MethodName
is	O
the	O
best	O
option	O
for	O
HI	O
and	O
BN	O
.	O
For	O
DE	O
and	O
ZH	O
,	O
SENT	B-MethodName
and	O
SENT	B-MethodName
-	I-MethodName
LINK	I-MethodName
are	O
competitive	O
.	O
As	O
a	O
result	O
,	O
we	O
choose	O
SENT	B-MethodName
for	O
the	O
two	O
languages	O
since	O
we	O
believe	O
the	O
wiki	O
anchors	O
from	O
the	O
Wikipedia	O
can	O
help	O
model	O
performance	O
;	O
2	O
)	O
Comparing	O
with	O
the	O
baseline	O
,	O
the	O
knowledge	O
from	O
Google	O
Search	O
can	O
improve	O
model	O
performance	O
.	O
Based	O
on	O
the	O
best	O
context	O
option	O
of	O
each	O
track	O
,	O
the	O
knowledge	O
from	O
Wikipedia	O
is	O
better	O
than	O
the	O
online	O
search	O
engine	O
;	O
3	O
)	O
For	O
ITER	B-MethodName
G	I-MethodName
,	O
we	O
can	O
find	O
that	O
the	O
context	O
can	O
further	O
Iterative	B-TaskName
Entity	I-TaskName
Retrieval	I-TaskName
with	O
Predicted	O
Entities	O
Based	O
on	O
the	O
results	O
in	O
Table	O
3	O
,	O
we	O
further	O
analyze	O
how	O
the	O
predicted	O
entity	O
mentions	O
can	O
improve	O
the	O
retrieval	O
quality	O
.	O
We	O
denote	O
the	O
iterative	O
entity	O
retrieval	O
with	O
predicted	O
mentions	O
as	O
ITER	B-MethodName
P	I-MethodName
.	O
In	O
the	O
experiment	O
,	O
we	O
set	O
T	B-HyperparameterName
=	O
2	B-HyperparameterValue
.	O
12	O
We	O
extract	O
the	O
predicted	O
mentions	O
of	O
the	O
development	O
sets	O
from	O
the	O
models	O
based	O
on	O
the	O
best	O
context	O
option	O
for	O
each	O
track	O
.	O
We	O
conduct	O
the	O
experiments	O
over	O
HI	O
,	O
BN	O
and	O
MIX	O
which	O
have	O
significant	O
improvement	O
with	O
ITER	B-MethodName
G	I-MethodName
.	O
In	O
Table	O
4	O
,	O
we	O
also	O
list	O
the	O
performance	O
of	O
ITER	B-MethodName
G	I-MethodName
for	O
reference	O
,	O
which	O
can	O
be	O
seen	O
as	O
using	O
the	O
predicted	O
mentions	O
with	O
100	O
%	O
accuracy	O
.	O
From	O
the	O
results	O
,	O
we	O
observe	O
that	O
only	O
MIX	B-MethodName
can	O
be	O
improved	O
.	O
Since	O
iterative	O
entity	O
retrieval	O
uses	O
predicted	O
mentions	O
as	O
a	O
part	O
of	O
retrieval	O
query	O
,	O
the	O
performance	O
of	O
mention	O
detection	O
directly	O
affects	O
the	O
retrieval	O
quality	O
.	O
To	O
further	O
analyze	O
the	O
observation	O
in	O
Table	O
4	O
,	O
we	O
evaluate	O
the	O
mention	O
F1	B-MetricName
score	I-MetricName
of	O
the	O
NER	B-TaskName
models	O
with	O
sentence	O
retrieval	O
.	O
For	O
comparison	O
with	O
mention	O
detection	O
performance	O
of	O
NER	B-TaskName
models	O
,	O
we	O
additionally	O
train	O
mention	O
detection	O
models	O
by	O
discarding	O
the	O
entity	O
labels	O
during	O
training	O
.	O
From	O
the	O
results	O
in	O
Table	O
5	O
,	O
we	O
suspect	O
the	O
low	O
mention	O
F1	B-MetricName
introduces	O
noises	O
in	O
the	O
knowledge	O
retrieval	O
module	O
for	O
BN	O
and	O
HI	O
,	O
which	O
lead	O
to	O
the	O
decline	O
of	O
performance	O
as	O
shown	O
in	O
Table	O
4	O
.	O
Moreover	O
,	O
the	O
mention	O
F1	B-MetricName
of	O
mention	O
detection	O
models	O
(	O
second	O
row	O
of	O
Table	O
5	O
)	O
only	O
outperform	O
that	O
of	O
the	O
NER	B-TaskName
models	O
(	O
first	O
row	O
of	O
Table	O
5	O
)	O
in	O
a	O
moderate	O
scale	O
.	O
Therefore	O
,	O
we	O
train	O
the	O
ITER	B-MethodName
models	O
only	O
for	O
the	O
code	O
-	O
mixed	O
track	O
and	O
use	O
the	O
NER	B-TaskName
models	O
with	O
sentence	O
retrieval	O
to	O
predict	O
mentions	O
.	O

To	O
evaluate	O
the	O
relevance	O
of	O
the	O
retrieval	O
results	O
to	O
the	O
query	O
,	O
we	O
define	O
a	O
character	B-MetricName
-	I-MetricName
level	I-MetricName
relevance	I-MetricName
metric	O
,	O
which	O
calculates	O
the	O
Intersectionover	O
-	O
Union	O
(	O
IoU	O
)	O
between	O
the	O
characters	O
of	O
query	O
and	O
result	O
.	O
Assuming	O
that	O
the	O
character	O
sets	O
11	O
of	O
query	O
and	O
retrieval	O
result	O
are	O
A	O
and	O
B	O
respectively	O
,	O
then	O
the	O
character	O
-	O
level	O
IoU	O
is	O
A∩B	O
A∪B	O
.	O
We	O
calculate	O
the	O
character	B-MetricName
-	I-MetricName
level	I-MetricName
IoU	I-MetricName
of	O
the	O
sentence	O
and	O
its	O
top	O
-	O
1	O
retrieval	O
result	O
on	O
all	O
tracks	O
,	O
and	O
plot	O
its	O
distribution	O
on	O
the	O
training	O
,	O
development	O
and	O
test	O
set	O
in	O
Figure	O
3	O
.	O
We	O
have	O
the	O
following	O
observations	O
:	O
1	O
)	O
the	O
IoU	B-MetricName
values	O
are	O
concentrated	O
around	O
1.0	B-MetricValue
on	O
the	O
training	O
and	O
development	O
sets	O
of	O
EN	O
,	O
ES	O
,	O
NL	O
,	O
RU	O
,	O
TR	O
,	O
KO	O
,	O
FA	O
,	O
which	O
indicates	O
that	O
most	O
of	O
the	O
samples	O
were	O
derived	O
from	O
Wikipedia	O
.	O
Therefore	O
,	O
by	O
retrieving	O
,	O
we	O
can	O
obtain	O
the	O
original	O
documents	O
for	O
these	O
samples	O
.	O
2	O
)	O
the	O
distribution	O
of	O
data	O
on	O
the	O
test	O
set	O
is	O
consistent	O
with	O
the	O
training	O
and	O
development	O
sets	O
for	O
most	O
languages	O
,	O
except	O
for	O
TR	O
.	O
On	O
TR	O
,	O
the	O
character	O
-	O
level	O
IoU	O
values	O
of	O
the	O
samples	O
and	O
query	O
results	O
cluster	O
at	O
around	O
0.5	B-MetricValue
.	O
We	O
hypothesize	O
that	O
this	O
is	O
because	O
the	O
source	O
of	O
the	O
test	O
set	O
for	O
TR	O
is	O
different	O
from	O
the	O
training	O
set	O
.	O
However	O
,	O
the	O
model	O
still	O
performs	O
strongly	O
on	O
this	O
language	O
,	O
suggesting	O
that	O
the	O
model	O
can	O
mitigate	O
the	O
difficulties	O
caused	O
11	O
The	O
sets	O
take	O
repeat	O
characters	O
as	O
different	O
characters	O
.	O
by	O
inconsistent	O
data	O
distribution	O
by	O
retrieving	O
the	O
context	O
from	O
Wikipedia	O
.	O

To	O
further	O
show	O
the	O
effectiveness	O
of	O
our	O
knowledgebased	O
system	O
,	O
we	O
show	O
the	O
relative	O
improvements	O
of	O
our	O
system	O
over	O
our	O
baseline	O
system	O
on	O
each	O
domain	O
in	O
Table	O
2	O
.	O
We	O
observe	O
that	O
in	O
most	O
of	O
the	O
cases	O
,	O
the	O
two	O
out	O
-	O
of	O
-	O
domain	O
test	O
sets	O
have	O
more	O
relative	O
improvements	O
than	O
the	O
in	O
-	O
domain	O
test	O
set	O
.	O
This	O
observation	O
shows	O
that	O
the	O
knowledge	O
from	O
Wikipedia	O
can	O
not	O
only	O
improve	O
the	O
performance	O
of	O
the	O
LOWNER	B-DatasetName
domain	O
which	O
is	O
the	O
same	O
domain	O
as	O
the	O
KB	O
,	O
but	O
also	O
has	O
very	O
strong	O
cross	O
-	O
domain	O
Table	O
2	O
:	O
Per	O
-	O
domain	O
macro	B-MetricName
F1	I-MetricName
score	I-MetricName
on	O
the	O
test	O
set	O
of	O
our	O
system	O
and	O
our	O
baseline	O
system	O
for	O
each	O
language	O
.	O
∆	B-MetricName
represents	O
the	O
relative	O
improvements	O
of	O
our	O
system	O
over	O
the	O
baseline	O
system	O
.	O
transferability	O
to	O
other	O
domains	O
such	O
as	O
web	O
questions	O
and	O
user	O
queries	O
.	O
According	O
to	O
the	O
baseline	O
performance	O
over	O
the	O
three	O
domains	O
,	O
the	O
ORCAS	B-DatasetName
domain	O
has	O
the	O
lowest	O
score	O
,	O
which	O
shows	O
the	O
challenges	O
in	O
recognizing	O
named	O
entities	O
in	O
user	O
queries	O
.	O
However	O
,	O
our	O
retrieved	O
documents	O
in	O
KB	O
can	O
significantly	O
ease	O
the	O
challenges	O
in	O
this	O
domain	O
and	O
results	O
in	O
the	O
highest	O
improvement	O
out	O
of	O
the	O
three	O
domains	O
.	O

There	O
are	O
55	O
teams	O
that	O
participated	O
in	O
the	O
shared	O
task	O
.	O
Due	O
to	O
limited	O
space	O
,	O
we	O
only	O
compare	O
our	O
system	O
with	O
the	O
systems	O
from	O
teams	O
USTC	O
-	O
NELSLIP	O
,	O
RACAI	O
and	O
Sliced	O
10	O
.	O
In	O
the	O
postevaluation	O
phase	O
,	O
we	O
evaluate	O
a	O
baseline	O
system	O
without	O
using	O
the	O
knowledge	B-TaskName
retrieval	I-TaskName
module	O
to	O
further	O
show	O
the	O
effectiveness	O
of	O
our	O
knowledgebased	O
system	O
.	O
The	O
official	O
results	O
and	O
the	O
results	O
of	O
our	O
baseline	O
system	O
are	O
shown	O
in	O
Table	O
1	O
.	O
Our	O
system	O
performs	O
the	O
best	O
on	O
10	O
out	O
of	O
13	O
tracks	O
and	O
is	O
competitive	O
on	O
the	O
other	O
3	O
tracks	O
.	O
Moreover	O
,	O
our	O
system	O
outperforms	O
our	O
baseline	O
by	O
14.39	B-HyperparameterValue
F1	B-MetricName
on	O
average	O
,	O
which	O
shows	O
the	O
knowledge	O
retrieval	O
module	O
is	O
extremely	O
helpful	O
for	O
disambiguating	O
complex	O
entities	O
leading	O
to	O
significant	O
improvement	O
on	O
model	O
performance	O
.	O

NER	B-TaskName
Model	O
Training	O
Before	O
building	O
the	O
final	O
system	O
,	O
we	O
compare	O
a	O
lot	O
of	O
variants	O
of	O
the	O
system	O
.	O
We	O
train	O
these	O
variant	O
models	O
on	O
the	O
training	O
set	O
for	O
3	O
times	O
each	O
with	O
different	O
random	O
seeds	O
and	O
compare	O
the	O
averaged	O
performance	O
of	O
the	O
models	O
.	O
According	O
to	O
the	O
dataset	O
sizes	O
,	O
we	O
train	O
the	O
models	O
for	O
5	B-HyperparameterValue
epochs	B-HyperparameterName
,	O
10	B-HyperparameterValue
epochs	B-HyperparameterName
and	O
100	B-HyperparameterValue
epochs	B-HyperparameterName
for	O
multilingual	O
,	O
monolingual	O
and	O
code	O
-	O
mixed	O
models	O
respectively	O
.	O
Our	O
final	O
NER	B-TaskName
models	O
are	O
trained	O
on	O
the	O
combined	O
dataset	O
including	O
both	O
the	O
training	O
and	O
development	O
sets	O
on	O
each	O
track	O
to	O
fully	O
utilize	O
the	O
labeled	O
data	O
.	O
For	O
models	O
trained	O
on	O
the	O
training	O
set	O
,	O
we	O
use	O
the	O
best	O
macro	O
F1	B-MetricName
on	O
the	O
development	O
set	O
during	O
training	O
to	O
select	O
the	O
best	O
model	O
checkpoint	O
.	O
For	O
models	O
trained	O
on	O
the	O
combined	O
dataset	O
,	O
Continue	O
Pretraining	O
To	O
make	O
XLM	B-MethodName
-	I-MethodName
R	I-MethodName
learn	O
the	O
data	O
distribution	O
of	O
the	O
shared	O
task	O
,	O
we	O
combine	O
the	O
training	O
and	O
development	O
sets	O
on	O
the	O
monolingual	O
tracks	O
to	O
build	O
a	O
corpus	O
to	O
continue	O
pretrain	O
XLM	B-MethodName
-	I-MethodName
R.	I-MethodName
Specifically	O
,	O
we	O
collocate	O
all	O
sentences	O
according	O
to	O
their	O
languages	O
,	O
then	O
cut	O
the	O
text	O
into	O
chunks	O
of	O
fixed	O
length	O
,	O
and	O
train	O
the	O
model	O
on	O
these	O
text	O
chunks	O
using	O
the	O
Masked	B-MetricName
Language	I-MetricName
Modeling	I-MetricName
objective	I-MetricName
.	O
We	O
continue	O
pretrain	O
XLM	B-MethodName
-	I-MethodName
R	I-MethodName
for	O
5	B-HyperparameterValue
epochs	B-HyperparameterName
.	O
We	O
use	O
the	O
continue	O
pretrained	O
XLM	B-MethodName
-	I-MethodName
R	I-MethodName
model	O
as	O
the	O
initialization	O
of	O
the	O
multilingual	O
7	O
Please	O
refer	O
to	O
Appendix	O
A	O
for	O
detailed	O
settings	O
.	O
models	O
during	O
training	O
.	O

Given	O
predictions	O
{	O
ŷ	O
θ	O
1	O
,	O
,	O
ŷ	O
θm	O
}	O
from	O
m	O
models	O
with	O
different	O
random	O
seeds	O
,	O
we	O
use	O
majority	O
voting	O
to	O
generate	O
the	O
final	O
predictionŷ	O
.	O
We	O
convert	O
the	O
label	O
sequences	O
into	O
entity	O
spans	O
to	O
perform	O
majority	O
voting	O
.	O
Following	O
Yamada	O
et	O
al	O
(	O
2020	O
)	O
,	O
the	O
module	O
ranks	O
all	O
spans	O
in	O
the	O
predictions	O
by	O
the	O
number	O
of	O
votes	O
in	O
descending	O
order	O
and	O
selects	O
the	O
spans	O
with	O
more	O
than	O
50	O
%	O
votes	O
into	O
the	O
final	O
prediction	O
.	O
The	O
spans	O
with	O
more	O
votes	O
are	O
kept	O
if	O
the	O
selected	O
spans	O
have	O
overlaps	O
and	O
the	O
longer	O
spans	O
are	O
kept	O
if	O
the	O
spans	O
have	O
the	O
same	O
votes	O
.	O
(	O
Nguyen	O
et	O
al	O
,	O
2016	O
)	O
containing	O
a	O
lot	O
of	O
natural	O
language	O
questions	O
;	O
ORCAS	B-DatasetName
(	O
Search	B-TaskName
Query	I-TaskName
NER	I-TaskName
)	O
contains	O
user	O
queries	O
from	O
Microsoft	O
Bing	O
(	O
Craswell	O
et	O
al	O
,	O
2020	O
)	O
.	O
The	O
MSQ	B-DatasetName
and	O
ORCAS	B-DatasetName
samples	O
are	O
taken	O
as	O
out	O
-	O
ofdomain	O
data	O
in	O
the	O
shared	O
task	O
.	O
The	O
training	O
and	O
development	O
sets	O
only	O
contain	O
a	O
small	O
collection	O
of	O
samples	O
of	O
these	O
two	O
domains	O
and	O
mainly	O
contain	O
data	O
from	O
the	O
LOWNER	B-DatasetName
domain	O
.	O
The	O
test	O
set	O
,	O
however	O
,	O
contains	O
much	O
more	O
MSQ	B-DatasetName
and	O
ORCAS	B-DatasetName
samples	O
to	O
assess	O
the	O
out	O
-	O
of	O
-	O
domain	O
performance	O
.	O
The	O
results	O
of	O
the	O
shared	O
task	O
are	O
evaluated	O
with	O
the	O
entity	O
-	O
level	O
macro	O
F1	B-MetricName
scores	I-MetricName
,	O
which	O
treat	O
all	O
the	O
labels	O
equally	O
.	O
In	O
comparison	O
,	O
most	O
of	O
the	O
publicly	O
available	O
NER	B-TaskName
datasets	O
(	O
e.g.	O
,	O
CoNLL	B-DatasetName
2002CoNLL	B-DatasetName
,	O
2003	O
are	O
evaluated	O
with	O
the	O
entity	O
-	O
level	O
micro	O
F1	B-MetricName
scores	I-MetricName
,	O
which	O
emphasize	O
common	O
labels	O
.	O

In	O
our	O
system	O
,	O
we	O
use	O
XLM	B-MethodName
-	I-MethodName
R	I-MethodName
large	I-MethodName
as	O
the	O
embedding	O
for	O
all	O
the	O
tracks	O
.	O
It	O
is	O
a	O
multilingual	O
model	O
and	O
is	O
applicable	O
to	O
all	O
tracks	O
.	O
Given	O
the	O
input	O
sentence	O
x	O
and	O
the	O
retrieved	O
contexts	O
{	O
x	O
1	O
,	O
,	O
x	O
k	O
}	O
,	O
we	O
add	O
the	O
separator	O
token	O
(	O
i.e.	O
,	O
"	O
<	O
/s	O
>	O
"	O
in	O
XLM	O
-	O
R	O
)	O
between	O
them	O
and	O
concatenated	O
them	O
together	O
to	O
form	O
the	O
inputx	O
of	O
the	O
NER	B-TaskName
module	O
.	O
We	O
chunk	O
retrieved	O
texts	O
to	O
avoid	O
the	O
amount	O
of	O
subtoken	O
in	O
the	O
sequence	O
exceeding	O
the	O
maximum	B-HyperparameterName
subtoken	I-HyperparameterName
length	I-HyperparameterName
in	O
XLM	B-MethodName
-	I-MethodName
R	I-MethodName
(	O
i.e.	O
,	O
512	B-HyperparameterValue
in	O
XLM	B-MethodName
-	I-MethodName
R	I-MethodName
)	O
.	O
Our	O
system	O
regards	O
the	O
NER	B-TaskName
task	O
as	O
a	O
sequence	O
labeling	O
problem	O
.	O
The	O
embedding	O
layer	O
in	O
the	O
NER	B-TaskName
module	O
encode	O
the	O
concatenated	O
sequencẽ	O
x	O
and	O
output	O
the	O
corresponding	O
token	O
representa	O
-	O
tions	O
{	O
v	O
1	O
,	O
,	O
v	O
n	O
,	O
}	O
.	O
The	O
module	O
then	O
feeds	O
the	O
token	O
representations	O
{	O
v	O
1	O
,	O
,	O
v	O
n	O
}	O
of	O
the	O
input	O
sentence	O
into	O
a	O
linear	B-MethodName
-	I-MethodName
chain	I-MethodName
CRF	I-MethodName
layer	O
to	O
obtain	O
the	O
conditional	O
probability	O
p	O
θ	O
(	O
y	O
|	O
x	O
)	O
:	O
ψ	O
(	O
y	O
′	O
,	O
y	O
,	O
v	O
i	O
)	O
=	O
exp	O
(	O
W	O
T	O
y	O
v	O
i	O
+	O
b	O
y	O
′	O
,	O
y	O
)	O
(	O
1	O
)	O
p	O
θ	O
(	O
y	O
|	O
x	O
)	O
=	O
n	O
i=1	O
ψ	O
(	O
y	O
i−1	O
,	O
y	O
i	O
,	O
v	O
i	O
)	O
y	O
′	O
Y	O
(	O
x	O
)	O
n	O
i=1	O
ψ	O
(	O
y	O
′	O
i−1	O
,	O
y	O
′	O
i	O
,	O
v	O
i	O
)	O
where	O
θ	O
represents	O
the	O
model	O
parameters	O
and	O
Y	O
(	O
x	O
)	O
denotes	O
the	O
set	O
of	O
all	O
possible	O
label	O
sequences	O
given	O
x.	O
In	O
the	O
potential	O
function	O
ψ	O
(	O
y	O
′	O
,	O
y	O
,	O
v	O
i	O
)	O
,	O
W	O
T	O
y	O
v	O
i	O
is	O
the	O
emission	O
score	O
and	O
b	O
y	O
′	O
,	O
y	O
is	O
the	O
transition	O
score	O
,	O
where	O
W	O
T	O
R	O
t×d	O
and	O
b	O
R	O
t×t	O
are	O
parameters	O
and	O
the	O
subscripts	O
y	O
′	O
and	O
y	O
are	O
the	O
indices	O
of	O
the	O
matrices	O
.	O
During	O
training	O
,	O
the	O
negative	B-MetricName
log	I-MetricName
-	I-MetricName
likelihood	I-MetricName
loss	I-MetricName
L	O
NLL	B-MetricName
(	O
θ	O
)	O
=	O
−	O
log	O
p	O
θ	O
(	O
y	O
*	O
|	O
x	O
)	O
for	O
the	O
concatenated	O
input	O
sequence	O
with	O
gold	O
labels	O
y	O
*	O
is	O
used	O
.	O
During	O
inference	O
,	O
the	O
model	O
predictionŷ	O
θ	O
is	O
given	O
by	O
Viterbi	O
decoding	O
.	O

NER	B-TaskName
(	O
Sundheim	O
,	O
1995	O
)	O
is	O
a	O
fundamental	O
task	O
in	O
natural	O
language	O
processing	O
.	O
The	O
task	O
has	O
a	O
lot	O
of	O
applications	O
in	O
various	O
domains	O
such	O
as	O
social	O
media	O
(	O
Derczynski	O
et	O
al	O
,	O
2017	O
)	O
,	O
news	O
(	O
Tjong	O
Kim	O
Sang	O
,	O
2002	O
;	O
Tjong	O
Kim	O
Sang	O
and	O
De	O
Meulder	O
,	O
2003	O
)	O
,	O
Ecommerce	O
(	O
Fetahu	O
et	O
al	O
,	O
2021	O
;	O
Wang	O
et	O
al	O
,	O
2021b	O
)	O
,	O
and	O
medical	O
domains	O
(	O
Dogan	O
et	O
al	O
,	O
2014	O
;	O
Li	O
et	O
al	O
,	O
2016	O
)	O
.	O
Recently	O
,	O
pretrained	O
contextual	O
embeddings	O
such	O
as	O
BERT	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
XLM	O
-	O
R	O
and	O
LUKE	O
(	O
Yamada	O
et	O
al	O
,	O
2020	O
)	O
have	O
significantly	O
improved	O
the	O
NER	O
performance	O
.	O
The	O
embeddings	O
are	O
trained	O
on	O
large	O
-	O
scale	O
unlabeled	O
data	O
such	O
as	O
Wikipedia	O
,	O
which	O
can	O
significantly	O
improve	O
the	O
contextual	O
representations	O
of	O
named	O
entities	O
.	O
Recent	O
efforts	O
(	O
Peters	O
et	O
al	O
,	O
2018	O
;	O
Akbik	O
et	O
al	O
,	O
2018	O
;	O
Straková	O
et	O
al	O
,	O
2019	O
)	O
concatenate	O
different	O
kinds	O
of	O
pretrained	O
embeddings	O
to	O
form	O
stronger	O
token	O
representations	O
.	O
Moreover	O
,	O
the	O
embeddings	O
are	O
trained	O
over	O
long	O
documents	O
,	O
which	O
allows	O
the	O
model	O
to	O
easily	O
model	O
long	O
-	O
range	O
dependencies	O
to	O
disambiguate	O
complex	O
named	O
entities	O
in	O
the	O
sentence	O
.	O
Recently	O
,	O
a	O
lot	O
of	O
work	O
shows	O
that	O
utilizing	O
the	O
document	O
-	O
level	O
contexts	O
in	O
the	O
CoNLL	B-DatasetName
NER	B-TaskName
datasets	O
can	O
significantly	O
improve	O
token	O
representations	O
and	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
(	O
Yu	O
et	O
al	O
,	O
2020	O
;	O
Luoma	O
and	O
Pyysalo	O
,	O
2020	O
;	O
Yamada	O
et	O
al	O
,	O
2020	O
;	O
Wang	O
et	O
al	O
,	O
2021a	O
)	O
.	O
However	O
,	O
the	O
lack	O
of	O
context	O
in	O
the	O
MultiCoNER	B-DatasetName
datasets	O
means	O
the	O
embeddings	O
can	O
not	O
take	O
advantage	O
of	O
long	O
-	O
range	O
dependencies	O
for	O
entity	O
disambiguation	O
.	O
Recently	O
,	O
Wang	O
et	O
al	O
(	O
2021b	O
)	O
use	O
Google	O
search	O
to	O
retrieve	O
external	O
contexts	O
of	O
the	O
input	O
sentence	O
and	O
successfully	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
across	O
multiple	O
domains	O
.	O
We	O
adopt	O
this	O
idea	O
so	O
that	O
the	O
embeddings	O
can	O
utilize	O
the	O
related	O
knowledge	O
by	O
taking	O
the	O
advantage	O
of	O
long	O
-	O
range	O
dependencies	O
to	O
form	O
stronger	O
token	O
representations	O
.	O
Comparing	O
with	O
Wang	O
et	O
al	O
(	O
2021b	O
)	O
,	O
we	O
build	O
the	O
local	O
KB	O
based	O
on	O
Wikipedia	O
because	O
the	O
KB	O
matches	O
the	O
indomain	O
data	O
of	O
the	O
shared	O
task	O
and	O
is	O
fast	O
enough	O
to	O
meet	O
the	O
time	O
requirement	O
in	O
the	O
test	O
phase	O
2	O
.	O
Fine	O
-	O
tuning	O
pretrained	O
contextual	O
embeddings	O
is	O
a	O
useful	O
and	O
effective	O
approach	O
to	O
many	O
NLP	O
tasks	O
.	O
Recently	O
,	O
some	O
of	O
the	O
research	O
efforts	O
propose	O
to	O
further	O
train	O
the	O
fine	O
-	O
tuned	O
embeddings	O
with	O
specific	O
training	O
data	O
or	O
in	O
a	O
larger	O
model	O
architecture	O
to	O
improve	O
model	O
performance	O
.	O
Shi	O
and	O
Lee	O
(	O
2021	O
)	O
proposed	O
two	O
-	O
stage	O
fine	O
-	O
tuning	O
,	O
which	O
first	O
trains	O
a	O
general	O
multilingual	O
Enhanced	O
Universal	O
Dependency	O
(	O
Bouma	O
et	O
al	O
,	O
2021	O
)	O
parser	O
and	O
then	O
finetunes	O
on	O
each	O
specific	O
language	O
separately	O
.	O
Wang	O
et	O
al	O
(	O
2021a	O
)	O
proposed	O
to	O
train	O
models	O
through	O
concatenating	O
fine	O
-	O
tuned	O
embeddings	O
.	O
We	O
extend	O
these	O
ideas	O
as	O
multi	B-MethodName
-	I-MethodName
stage	I-MethodName
fine	I-MethodName
-	I-MethodName
tuning	I-MethodName
,	O
which	O
improves	O
the	O
accuracy	O
of	O
monolingual	O
models	O
that	O
use	O
finetuned	O
multilingual	O
embeddings	O
as	O
initialization	O
in	O
training	O
.	O
Moreover	O
,	O
multi	B-MethodName
-	I-MethodName
stage	I-MethodName
fine	I-MethodName
-	I-MethodName
tuning	I-MethodName
can	O
accelerate	O
the	O
training	O
process	O
in	O
system	O
building	O
.	O

The	O
instances	O
selected	O
or	O
generated	O
by	O
any	O
model	O
or	O
baseline	O
are	O
annotated	O
manually	O
by	O
one	O
human	O
coder	O
.	O
2	O
Although	O
the	O
pool	O
data	O
has	O
labels	O
on	O
the	O
review	O
level	O
,	O
we	O
do	O
not	O
use	O
these	O
labels	O
in	O
our	O
experiments	O
.	O
Positive	O
reviews	O
can	O
include	O
negative	O
sentences	O
and	O
vice	O
versa	O
.	O
This	O
means	O
that	O
using	O
document	O
-	O
level	O
labels	O
would	O
introduce	O
noise	O
and	O
might	O
impair	O
the	O
baselines	O
.	O
During	O
each	O
of	O
the	O
three	O
experimental	O
runs	O
,	O
all	O
models	O
and	O
baselines	O
are	O
annotated	O
simultaneously	O
by	O
the	O
same	O
person	O
.	O
The	O
annotator	O
is	O
presented	O
with	O
one	O
instance	O
at	O
a	O
time	O
and	O
has	O
no	O
information	O
which	O
of	O
the	O
models	O
has	O
produced	O
each	O
particular	O
instance	O
.	O
Once	O
a	O
label	O
is	O
selected	O
,	O
it	O
is	O
transmitted	O
to	O
the	O
corresponding	O
model	O
and	O
triggers	O
the	O
selection	O
/	O
generation	O
of	O
the	O
next	O
instance	O
.	O
Thus	O
,	O
at	O
any	O
given	O
time	O
there	O
is	O
one	O
unlabeled	O
instance	O
for	O
each	O
model	O
or	O
baseline	O
.	O
From	O
this	O
set	O
of	O
unlabeled	O
instances	O
,	O
one	O
instance	O
is	O
chosen	O
randomly	O
and	O
presented	O
to	O
the	O
annotator	O
.	O
This	O
procedure	O
is	O
repeated	O
until	O
500	O
instances	O
are	O
labeled	O
for	O
each	O
model	O
or	O
baseline	O
.	O
Hiding	O
the	O
instance	O
source	O
from	O
the	O
annotator	O
is	O
intended	O
to	O
prevent	O
any	O
bias	O
during	O
the	O
annotation	O
process	O
.	O
6	O
Results	O
and	O
Analysis	O
6.1	O
Classification	O
Performance	O
F	B-MetricName
-	I-MetricName
scores	I-MetricName
as	O
a	O
function	O
of	O
annotated	O
instances	O
Figure	O
3	O
shows	O
learning	O
curves	O
for	O
the	O
different	O
AL	B-TaskName
strategies	O
and	O
baselines	O
as	O
a	O
function	O
of	O
the	O
number	O
of	O
annotation	O
instances	O
added	O
to	O
the	O
training	O
data	O
.	O
The	O
random	B-MethodName
and	O
least	B-MethodName
conf	I-MethodName
baselines	O
perform	O
reasonably	O
well	O
.	O
Least	B-MethodName
conf	I-MethodName
struggles	O
in	O
the	O
beginning	O
,	O
likely	O
attributed	O
to	O
the	O
minimal	O
seed	O
set	O
.	O
Once	O
enough	O
instances	O
are	O
labeled	O
it	O
catches	O
up	O
.	O
Gen	O
uniform	O
has	O
a	O
strong	O
start	O
but	O
,	O
after	O
around	O
200	O
instances	O
,	O
is	O
outperformed	O
by	O
the	O
nearest	O
neighbor	O
approaches	O
which	O
yield	O
the	O
highest	O
F1	B-MetricName
-	I-MetricName
scores	I-MetricName
.	O
Among	O
the	O
nearest	O
neighbor	O
approaches	O
,	O
the	O
uniform	O
schedule	O
ranks	O
better	O
than	O
wang	B-MethodName
.	O
The	O
same	O
behaviour	O
is	O
observed	O
for	O
the	O
generation	O
methods	O
,	O
although	O
gen	B-MethodName
wang	I-MethodName
produces	O
the	O
worst	O
results	O
overall	O
.	O
Overall	O
,	O
gen	B-MethodName
uniform	I-MethodName
is	O
competitive	O
with	O
respect	O
to	O
F1	B-MetricName
-	I-MetricName
scores	I-MetricName
and	O
shows	O
that	O
sentences	O
generated	O
from	O
points	O
in	O
the	O
feature	O
space	O
are	O
informative	O
and	O
useful	O
for	O
training	O
a	O
text	O
classifier	O
.	O
F	B-MetricName
-	I-MetricName
scores	I-MetricName
as	O
a	O
function	O
of	O
annotation	O
time	O
AL	B-TaskName
simulations	O
have	O
often	O
been	O
criticized	O
for	O
reporting	O
unrealistic	O
results	O
,	O
based	O
merely	O
on	O
the	O
number	O
of	O
annotated	O
instances	O
(	O
see	O
,	O
e.g.	O
,	O
Settles	O
(	O
2009	O
)	O
,	O
pp	O
.	O
37	O
ff	O
.	O
)	O
.	O
It	O
is	O
well	O
known	O
,	O
however	O
,	O
that	O
the	O
number	O
of	O
annotated	O
instances	O
is	O
often	O
not	O
a	O
good	O
predictor	O
for	O
the	O
real	O
annotation	O
costs	O
.	O
AL	B-TaskName
strategies	O
tend	O
to	O
select	O
the	O
hard	O
nuts	O
for	O
human	O
annotators	O
and	O
it	O
is	O
not	O
unreasonable	O
to	O
assume	O
that	O
the	O
annotation	O
of	O
N	O
instances	O
in	O
an	O
AL	B-TaskName
setup	O
might	O
take	O
longer	O
and	O
thus	O
might	O
be	O
more	O
expensive	O
than	O
annotating	O
the	O
same	O
number	O
of	O
randomly	O
selected	O
instances	O
.	O
Therefore	O
,	O
we	O
also	O
show	O
learning	O
curves	O
as	O
a	O
function	O
of	O
annotation	O
time	O
(	O
Figure	O
4	O
)	O
.	O
The	O
results	O
show	O
a	O
clear	O
advantage	O
for	O
the	O
generation	O
models	O
.	O
The	O
reduction	O
in	O
annotation	O
time	O
is	O
due	O
to	O
shorter	O
query	O
length	O
and	O
less	O
neutral	O
or	O
noisy	O
instances	O
,	O
as	O
shown	O
in	O
Table	O
2	O
.	O
This	O
speeds	O
up	O
the	O
annotation	O
by	O
a	O
significant	O
margin	O
while	O
providing	O
the	O
Learner	O
with	O
informative	O
instances	O
,	O
despite	O
their	O
short	O
length	O
.	O
Figure	O
5	O
shows	O
that	O
the	O
length	O
of	O
generated	O
instances	O
increase	O
over	O
time	O
and	O
further	O
exploration	O
also	O
hints	O
that	O
the	O
generated	O
length	O
is	O
correlated	O
with	O
the	O
length	O
of	O
the	O
sentences	O
in	O
the	O
seed	O
set	O
.	O
As	O
listed	O
in	O
Table	O
2	O
,	O
the	O
random	O
baseline	O
reveals	O
that	O
36.8	O
percent	O
of	O
sentences	O
in	O
the	O
pool	O
are	O
neutral	O
/	O
artifacts	O
and	O
positive	O
sentences	O
outweigh	O
negative	O
ones	O
by	O
a	O
factor	O
of	O
2.6	O
.	O
This	O
means	O
that	O
random	O
sampling	O
results	O
in	O
unbalanced	O
datasets	O
with	O
far	O
more	O
positive	O
examples	O
.	O
Our	O
generation	O
method	O
does	O
not	O
show	O
this	O
disadvantage	O
.	O
In	O
contrast	O
,	O
the	O
generated	O
instances	O
maintain	O
a	O
more	O
balanced	O
distribution	O
of	O
class	O
labels	O
and	O
are	O
less	O
likely	O
to	O
be	O
skipped	O
.	O
These	O
are	O
indicators	O
that	O
the	O
selected	O
points	O
are	O
close	O
to	O
the	O
hyperplane	O
and	O
the	O
VAE	O
is	O
able	O
to	O
generate	O
coherent	O
and	O
highly	O
informative	O
sentences	O
from	O
them	O
.	O

The	O
data	O
used	O
in	O
our	O
experiments	O
comes	O
from	O
two	O
sources	O
,	O
(	O
i	O
)	O
the	O
SST2	B-DatasetName
(	O
Socher	O
et	O
al	O
,	O
2013	O
)	O
and	O
(	O
ii	O
)	O
SAR14	B-DatasetName
(	O
Nguyen	O
et	O
al	O
,	O
2014	O
)	O
.	O
We	O
limit	O
sentence	B-HyperparameterName
length	I-HyperparameterName
to	O
a	O
maximum	O
of	O
15	B-HyperparameterValue
words	O
.	O
This	O
is	O
motivated	O
by	O
lower	O
training	O
times	O
and	O
the	O
tendency	O
of	O
vanilla	O
VAEs	O
not	O
to	O
perform	O
well	O
on	O
longer	O
sentences	O
(	O
Shen	O
et	O
al	O
,	O
2019	O
)	O
.	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
with	O
size	O
512	O
.	O
As	O
additional	O
regularization	O
we	O
set	O
weight	B-HyperparameterName
dropout	I-HyperparameterName
to	O
0.3	B-HyperparameterValue
(	O
Srivastava	O
et	O
al	O
,	O
2014	O
)	O
.	O
Input	B-HyperparameterName
embeddings	I-HyperparameterName
are	O
also	O
of	O
size	O
512	B-HyperparameterValue
,	O
which	O
allows	O
us	O
to	O
share	O
the	O
embed	O
-	O
ding	O
weights	O
with	O
the	O
softmax	O
weights	O
of	O
the	O
output	O
layer	O
(	O
Press	O
and	O
Wolf	O
,	O
2016	O
)	O
.	O
To	O
prevent	O
posterior	O
collapse	O
we	O
use	O
logistic	O
annealing	O
of	O
the	O
KL	O
term	O
weight	O
and	O
weaken	O
the	O
decoder	O
by	O
applying	O
word	O
dropout	B-HyperparameterName
with	O
probability	O
0.5	B-HyperparameterValue
(	O
Bowman	O
et	O
al	O
,	O
2016	O
)	O
.	O
The	O
model	O
is	O
trained	O
using	O
the	O
Adam	O
optimizer	O
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
with	O
an	O
initial	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.005	B-HyperparameterValue
.	O
Once	O
the	O
KL	O
term	O
weight	O
is	O
close	O
to	O
1	O
,	O
the	O
learning	O
weight	O
is	O
linearly	O
decreased	O
to	O
0	O
.	O
The	O
training	O
stops	O
after	O
20	B-HyperparameterValue
epochs	B-HyperparameterName
and	O
the	O
latent	O
variable	O
z	O
has	O
k	B-HyperparameterName
=	O
50	B-HyperparameterValue
dimensions	O
.	O
The	O
trained	O
VAE	O
achieves	O
a	O
reconstruction	B-MetricName
loss	I-MetricName
of	O
45.3	B-MetricValue
and	O
KL	B-MetricName
divergence	I-MetricName
of	O
13.2	B-MetricValue
on	O
the	O
SST2	B-DatasetName
training	O
set	O
.	O
Learner	O
The	O
Learner	O
is	O
an	O
SVM	O
1	O
with	O
linear	O
kernel	O
.	O
Each	O
instance	O
is	O
represented	O
as	O
the	O
latent	O
variable	O
z	O
learned	O
by	O
the	O
autoencoder	O
.	O
The	O
latent	O
variable	O
is	O
a	O
vector	O
with	O
50	O
dimensions	O
and	O
the	O
SVM	O
is	O
trained	O
on	O
this	O
representation	O
.	O
We	O
calculate	O
classification	O
performance	O
on	O
the	O
reduced	O
SST2	B-DatasetName
test	O
set	O
and	O
report	B-MetricName
F1	I-MetricName
-	I-MetricName
scores	I-MetricName
.	O
Generator	O
The	O
generator	O
is	O
the	O
decoder	O
of	O
the	O
VAE	O
described	O
above	O
.	O
Once	O
a	O
point	O
z	O
in	O
feature	O
space	O
is	O
selected	O
,	O
it	O
is	O
used	O
as	O
the	O
input	O
of	O
the	O
decoder	O
x	O
=	O
dec	O
(	O
z	O
)	O
which	O
generates	O
the	O
human	O
readable	O
sentence	O
x	O
in	O
an	O
autoregressive	O
way	O
.	O

The	O
Variational	B-MethodName
Autoencoder	I-MethodName
is	O
a	O
generative	O
model	O
first	O
introduced	O
by	O
Kingma	O
and	O
Welling	O
(	O
2013	O
)	O
.	O
Like	O
other	O
autoencoders	O
,	O
VAEs	O
learn	O
a	O
mapping	O
q	O
θ	O
(	O
z	O
|	O
x	O
)	O
from	O
high	O
dimensional	O
input	O
x	O
to	O
a	O
low	O
dimensional	O
latent	O
variable	O
z.	O
Instead	O
of	O
doing	O
this	O
in	O
a	O
deterministic	O
way	O
,	O
the	O
encoder	O
learns	O
the	O
parameters	O
of	O
e.g.	O
a	O
normal	O
distribution	O
.	O
The	O
desired	O
effect	O
is	O
that	O
each	O
area	O
in	O
the	O
latent	O
space	O
has	O
a	O
semantic	O
meaning	O
and	O
thus	O
samples	O
from	O
p	O
(	O
z	O
)	O
can	O
be	O
decoded	O
in	O
a	O
meaningful	O
way	O
.	O
The	O
decoder	O
p	O
θ	O
(	O
x	O
|	O
z	O
)	O
,	O
also	O
referred	O
to	O
as	O
dec	O
(	O
z	O
)	O
,	O
is	O
trained	O
to	O
reconstruct	O
the	O
input	O
x	O
based	O
on	O
the	O
latent	O
variable	O
z.	O
In	O
order	O
to	O
approximate	O
θ	O
via	O
gradient	O
descent	O
the	O
reparametrization	O
trick	O
(	O
Kingma	O
and	O
Welling	O
,	O
2013	O
)	O
was	O
introduced	O
.	O
This	O
trick	O
allows	O
the	O
gradient	O
to	O
flow	O
through	O
non	O
-	O
deterministic	O
z	O
by	O
separating	O
the	O
discrete	O
sampling	O
operation	O
.	O
Let	O
µ	O
and	O
σ	O
be	O
deterministic	O
outputs	O
of	O
the	O
encoder	O
q	O
θ	O
(	O
µ	O
,	O
σ	O
|	O
x	O
)	O
:	O
z	O
=	O
µ	O
+	O
σ	O
where	O
∼	O
N	O
(	O
0	O
,	O
I	O
)	O
and	O
is	O
the	O
element	O
-	O
wise	O
product	O
.	O
To	O
prevent	O
the	O
model	O
from	O
pushing	O
σ	O
close	O
to	O
0	O
and	O
thus	O
falling	O
back	O
to	O
a	O
deterministic	O
autoencoder	O
,	O
the	O
objective	O
is	O
extended	O
by	O
the	O
Kullback	B-MetricName
-	I-MetricName
Leibler	I-MetricName
(	I-MetricName
KL	I-MetricName
)	I-MetricName
diver	I-MetricName
-	I-MetricName
gence	I-MetricName
between	O
prior	O
p	O
(	O
z	O
)	O
and	O
q	O
(	O
z	O
|	O
x	O
)	O
:	O
L	O
(	O
θ	O
;	O
x	O
)	O
=	O
−KL	O
(	O
q	O
θ	O
(	O
z	O
|	O
x	O
)	O
|	O
|	O
p	O
(	O
z	O
)	O
)	O
+	O
E	O
q	O
θ	O
(	O
z	O
|	O
x	O
)	O
[	O
logp	O
θ	O
(	O
x	O
|	O
z	O
)	O
]	O
.	O
(	O
2	O
)	O
Bowman	O
et	O
al	O
(	O
2016	O
)	O
apply	O
this	O
idea	O
for	O
sentence	O
generation	O
using	O
an	O
RNN	O
as	O
encoder	O
and	O
decoder	O
.	O
They	O
observe	O
that	O
a	O
strong	O
auto	O
-	O
regressive	O
language	O
modeling	O
ability	O
in	O
the	O
decoder	O
reduces	O
the	O
information	O
stored	O
in	O
the	O
latent	O
variable	O
,	O
right	O
up	O
to	O
a	O
complete	O
collapse	O
of	O
the	O
KL	O
term	O
.	O
They	O
explore	O
different	O
techniques	O
to	O
weaken	O
the	O
decoder	O
,	O
like	O
word	O
dropout	O
or	O
KL	O
term	O
weight	O
annealing	O
,	O
as	O
possible	O
solutions	O
.	O
This	O
guarantees	O
a	O
semantically	O
rich	O
latent	O
variable	O
and	O
good	O
sentence	O
generation	O
ability	O
.	O
Below	O
,	O
we	O
describe	O
how	O
to	O
combine	O
both	O
techniques	O
in	O
order	O
to	O
generate	O
meaningful	O
queries	O
for	O
Membership	B-TaskName
Query	I-TaskName
Synthesis	I-TaskName
.	O

We	O
run	O
all	O
our	O
experiments	O
on	O
a	O
single	O
NVIDIA	O
TI	O
-	O
TAN	O
RTX	O
with	O
24	O
GB	O
GPU	O
memory	O
.	O
Fine	O
-	O
tuning	O
the	O
generators	O
for	O
2	B-HyperparameterValue
epochs	B-HyperparameterName
as	O
we	O
have	O
done	O
on	O
our	O
preprocessed	O
PERSONACHAT	B-DatasetName
train	O
split	O
consumes	O
about	O
3	O
-	O
4	O
hours	O
.	O
Fine	O
-	O
tuning	O
our	O
critic	O
classifier	O
for	O
1	B-HyperparameterValue
epoch	B-HyperparameterName
consumes	O
about	O
1	O
hour	O
.	O
Our	O
RL	O
phase	O
consumes	O
about	O
15	O
hours	O
to	O
achieve	O
the	O
best	O
validation	O
loss	O
before	O
being	O
early	O
stopped	O
.	O
We	O
report	O
averaged	O
results	O
from	O
3	O
runs	O
for	O
our	O
dialogue	O
response	O
generation	O
and	O
partner	O
personas	O
generation	O
results	O
reported	O
in	O
Table	O
1	O
,	O
Table	O
4	O
and	O
Table	O
7	O
.	O

We	O
present	O
the	O
progressive	O
change	O
of	O
the	O
testing	O
perplexity	O
for	O
DRG	B-MethodName
and	O
PPG	B-MethodName
on	O
PERSONACHAT	B-DatasetName
-	I-DatasetName
ORI	I-DatasetName
in	O
Figure	O
4	O
.	O
4	O
We	O
observe	O
that	O
they	O
improve	O
D	O
Human	B-MetricName
Evaluation	I-MetricName
Criteria	I-MetricName
(	O
Appropriateness	O
)	O
:	O
"	O
Who	O
is	O
more	O
appropriate	O
given	O
the	O
previous	O
dialogue	O
context	O
?	O
"	O
(	O
Informativeness	O
)	O
:	O
"	O
Who	O
is	O
more	O
diverse	O
instead	O
of	O
null	O
answers	O
such	O
as	O
I	O
do	O
not	O
know	O
?	O
"	O
(	O
Engagingness	O
)	O
:	O
"	O
Who	O
would	O
you	O
prefer	O
to	O
talk	O
with	O
for	O
a	O
long	O
conversation	O
?	O
"	O
(	O
Human	O
-	O
likeness	O
)	O
:	O
"	O
Which	O
speaker	O
do	O
you	O
think	O
sounds	O
more	O
like	O
a	O
real	O
person	O
?	O
"	O
(	O
Coherence	O
)	O
:	O
"	O
Which	O
persona	O
contains	O
traits	O
that	O
are	O
more	O
coherent	O
to	O
each	O
other	O
?	O
"	O
(	O
Interestingness	O
)	O
:	O
"	O
Which	O
persona	O
is	O
more	O
interesting	O
and	O
diverse	O
?	O
"	O
The	O
first	O
four	O
are	O
from	O
the	O
existing	O
work	O
Zou	O
et	O
al	O
,	O
2021	O
)	O
and	O
we	O
propose	O
the	O
last	O
two	O
for	O
evaluating	O
PPG	B-MethodName
.	O
We	O
report	O
the	O
first	O
four	O
for	O
DRG	B-MethodName
,	O
and	O
we	O
report	O
the	O
last	O
four	O
for	O
PPG	B-MethodName
.	O

For	O
supervised	O
phase	O
,	O
we	O
set	O
Adam	O
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
as	O
our	O
optimizer	O
,	O
with	O
hyperparameters	O
η	B-HyperparameterName
=	O
5e−4	B-HyperparameterValue
,	O
β	B-HyperparameterName
1	I-HyperparameterName
=	O
0.9	B-HyperparameterValue
,	O
β	B-HyperparameterName
2	I-HyperparameterName
=	O
0.999	B-HyperparameterValue
,	O
ϵ	B-HyperparameterName
=	O
1e−8	B-HyperparameterValue
.	O
The	O
models	O
are	O
fine	O
-	O
tuned	O
for	O
2	B-HyperparameterValue
epochs	B-HyperparameterName
.	O
For	O
RL	O
phase	O
,	O
we	O
set	O
Adam	O
as	O
our	O
optimizer	O
,	O
with	O
η	B-HyperparameterName
=	O
5e−6	B-HyperparameterValue
,	O
β	B-HyperparameterName
1	I-HyperparameterName
=	O
0.9	B-HyperparameterValue
,	O
β	B-HyperparameterName
2	I-HyperparameterName
=	O
0.999	B-HyperparameterValue
,	O
ϵ	O
=	O
1e−8	B-HyperparameterValue
.	O
We	O
update	O
the	O
model	O
parameters	O
every	O
20	O
training	O
instances	O
and	O
validate	O
the	O
model	O
performance	O
every	O
50	O
updates	O
.	O
DistilBERT	B-MethodName
is	O
used	O
to	O
initialize	O
the	O
model	O
parameters	O
for	O
the	O
critic	O
network	O
.	O
We	O
set	O
Adam	O
as	O
our	O
optimizer	O
,	O
with	O
hyperparameters	O
η	B-HyperparameterName
=	O
5e−6	B-HyperparameterValue
,	O
β	B-HyperparameterName
1	I-HyperparameterName
=	O
0.9	B-HyperparameterValue
,	O
β	B-HyperparameterName
2	I-HyperparameterName
=	O
0.999	B-HyperparameterValue
,	O
ϵ	B-HyperparameterName
=	O
1e−8	B-HyperparameterValue
.	O
We	O
fine	O
-	O
tune	O
the	O
critic	O
for	O
1	O
epoch	B-HyperparameterName
,	O
and	O
we	O
freeze	O
it	O
empirically	O
during	O
RL	O
.	O
All	O
the	O
experiments	O
are	O
conducted	O
based	O
on	O
the	O
TRANSFORMERS	O
library	O
from	O
HUGGINGFACE	O
(	O
Wolf	O
et	O
al	O
,	O
2020	O
)	O
.	O

We	O
conduct	O
experiments	O
on	O
the	O
PERSONACHAT	B-DatasetName
,	O
the	O
most	O
well	O
-	O
known	O
multiturn	B-TaskName
dialogue	I-TaskName
dataset	O
conditioned	O
on	O
personas	O
.	O
We	O
follow	O
the	O
train	B-HyperparameterName
/	I-HyperparameterName
valid	I-HyperparameterName
/	I-HyperparameterName
test	I-HyperparameterName
split	I-HyperparameterName
from	O
the	O
PARLAI	B-MethodName
platform	O
(	O
Miller	O
et	O
al	O
,	O
2017	O
)	O
that	O
contains	O
about	O
65	B-HyperparameterValue
,	I-HyperparameterValue
000/7	I-HyperparameterValue
,	I-HyperparameterValue
800/7	I-HyperparameterValue
,	I-HyperparameterValue
500	I-HyperparameterValue
instances	O
respectively	O
.	O
Each	O
instance	O
contains	O
about	O
8	O
utterances	O
on	O
average	O
and	O
about	O
4	O
traits	O
for	O
each	O
of	O
the	O
self	O
and	O
partner	O
personas	O
.	O
We	O
denote	O
the	O
dataset	O
with	O
this	O
original	O
personas	O
as	O
PERSONACHAT	B-DatasetName
-	I-DatasetName
ORI	I-DatasetName
.	O
Later	O
the	O
original	O
personas	O
have	O
been	O
manually	O
scrutinized	O
by	O
rephrasing	O
,	O
generalizing	O
or	O
specializing	O
,	O
which	O
we	O
denote	O
as	O
PERSONACHAT	B-DatasetName
-	I-DatasetName
REV	I-DatasetName
.	O
We	O
apply	O
the	O
same	O
preprocessing	O
operation	O
to	O
both	O
datasets	O
.	O
To	O
train	O
the	O
critic	O
for	O
RL	O
,	O
we	O
collected	O
about	O
130	O
,	O
000	O
instances	O
from	O
the	O
train	O
split	O
with	O
equally	O
distributed	O
positive	O
and	O
negative	O
samples	O
.	O
During	O
early	O
experiments	O
,	O
we	O
found	O
that	O
feeding	O
all	O
traits	O
yields	O
lower	O
performance	O
.	O
Retrieving	O
Top	O
-	O
3	O
relevant	O
partner	O
personas	O
using	O
BM25	B-MethodName
(	O
Robertson	O
and	O
Walker	O
,	O
1994	O
)	O
yields	O
the	O
best	O
performance	O
on	O
the	O
original	O
personas	O
.	O
GPT	B-MethodName
-	I-MethodName
2	I-MethodName
This	O
is	O
a	O
comparison	O
model	O
fine	O
-	O
tuned	O
on	O
GPT	B-MethodName
-	I-MethodName
2	I-MethodName
(	O
Radford	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
build	O
the	O
same	O
three	O
E2E	O
systems	O
described	O
above	O
,	O
and	O
the	O
best	O
model	O
is	O
selected	O
,	O
the	O
third	O
one	O
.	O
TRANSFERTRANSFO	B-MethodName
A	O
comparison	O
model	O
built	O
with	O
a	O
Transformer	O
-	O
based	O
model	O
pre	O
-	O
trained	O
on	O
gen	O
-	O
eral	O
domain	O
corpus	O
,	O
which	O
is	O
then	O
fine	O
-	O
tuned	O
on	O
PERSONACHAT	B-DatasetName
.	O
PERCVAE	B-MethodName
This	O
is	O
a	O
comparison	O
model	O
that	O
employs	O
a	O
memory	O
-	O
augmented	O
architecture	O
incorporated	O
with	O
conditional	O
variational	O
autoencoder	O
that	O
exploits	O
persona	O
information	O
.	O
PAML	B-MethodName
This	O
is	O
a	O
comparison	O
model	O
that	O
leverages	O
several	O
dialogues	O
collected	O
from	O
the	O
same	O
speaker	O
to	O
enhance	O
response	O
personality	O
via	O
metalearning	O
(	O
Madotto	O
et	O
al	O
,	O
2019	O
)	O
.	O
As	O
the	O
authors	O
did	O
not	O
conduct	O
experiments	O
on	O
the	O
PERSONACHAT	B-DatasetName
-	I-DatasetName
REV	I-DatasetName
and	O
no	O
preprocessing	O
scripts	O
are	O
provided	O
for	O
the	O
revised	O
personas	O
,	O
we	O
only	O
report	O
the	O
results	O
of	O
their	O
model	O
on	O
the	O
PERSONACHAT	B-DatasetName
-	I-DatasetName
ORI	I-DatasetName
only	O
.	O
MTL	B-MethodName
w/	I-MethodName
Personas	I-MethodName
Reconstruction	I-MethodName
This	O
is	O
a	O
multi	O
-	O
task	O
learning	O
(	O
MTL	O
)	O
comparison	O
model	O
(	O
Lee	O
et	O
al	O
,	O
2021	O
)	O
trained	O
to	O
maximise	O
the	O
objective	O
:	O
αL	O
PPG	O
+	O
(	O
1	O
−	O
α	O
)	O
L	O
DRG	O
,	O
where	O
L	O
PPG	O
represents	O
the	O
auxiliary	B-MetricName
PPG	I-MetricName
likelihood	I-MetricName
,	O
and	O
L	O
DRG	O
represents	O
the	O
DRG	B-MetricName
likelihood	I-MetricName
.	O
α	B-HyperparameterName
is	O
weight	B-HyperparameterValue
tuned	O
over	O
the	O
validation	O
set	O
,	O
and	O
both	O
tasks	O
condition	O
on	O
dialogue	O
context	O
and	O
self	O
personas	O
and	O
share	O
the	O
same	O
model	O
parameters	O
.	O

For	O
both	O
PPG	B-MethodName
and	O
DRG	B-MethodName
,	O
perplexity	B-MetricName
(	O
PPL	B-MetricName
)	O
is	O
reported	O
to	O
measure	O
the	O
intrinsic	O
performance	O
with	O
the	O
ground	O
truth	O
output	O
(	O
Roller	O
et	O
al	O
,	O
2021	O
)	O
.	O
We	O
adopt	O
well	O
-	O
known	O
sequence	O
evaluation	O
metrics	O
weighted	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
and	O
Fmeasure	B-MetricName
for	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
(	O
Lin	O
,	O
2004	O
)	O
as	O
the	O
extrinsic	O
evaluations	O
.	O
For	O
PPG	B-MethodName
,	O
we	O
also	O
report	O
Distinct	B-MetricName
-	I-MetricName
N	I-MetricName
with	O
N=	B-HyperparameterName
{	O
1	B-HyperparameterValue
,	O
2	B-HyperparameterValue
}	O
to	O
measure	O
the	O
response	O
diversity	O
(	O
Li	O
et	O
al	O
,	O
2016a	O
;	O
Cai	O
et	O
al	O
,	O
2019b	O
;	O
Gao	O
et	O
al	O
,	O
2019	O
)	O
with	O
the	O
ratio	O
of	O
distinct	O
unigrams	O
/	O
bigrams	O
against	O
total	O
number	O
of	O
unigrams	O
/	O
bigrams	O
generated	O
.	O

Reinforcement	O
learning	O
(	O
RL	O
)	O
,	O
or	O
specifically	O
,	O
policy	O
gradient	O
methods	O
(	O
Williams	O
,	O
1992	O
)	O
,	O
have	O
been	O
frequently	O
adopted	O
to	O
both	B-TaskName
task	I-TaskName
-	I-TaskName
oriented	I-TaskName
dialogue	I-TaskName
agents	O
(	O
Roman	O
Roman	O
et	O
al	O
,	O
2020	O
;	O
Deng	O
et	O
al	O
,	O
2021	O
)	O
or	O
open	B-TaskName
-	I-TaskName
domain	I-TaskName
chitchat	I-TaskName
agents	O
(	O
Li	O
et	O
al	O
,	O
2016c	O
;	O
Saleh	O
et	O
al	O
,	O
2020	O
)	O
.	O
It	O
can	O
either	O
propagate	O
non	O
-	O
differentiable	O
loss	O
(	O
Cai	O
et	O
al	O
,	O
2019a	O
)	O
or	O
optimize	O
an	O
expert	O
reward	O
such	O
as	O
ease	O
of	O
answering	O
(	O
Li	O
et	O
al	O
,	O
2016c	O
)	O
.	O
It	O
also	O
adopts	O
a	O
scenario	O
where	O
a	O
user	O
simulator	O
and	O
a	O
dialogue	O
agent	O
interact	O
,	O
and	O
an	O
Figure	O
1	O
:	O
An	O
example	O
of	O
the	O
inference	O
flow	O
that	O
shows	O
the	O
generated	O
partner	O
personas	O
and	O
the	O
incorporation	O
of	O
partner	O
personas	O
generation	O
into	O
response	O
generation	O
.	O
Figure	O
2	O
:	O
The	O
illustrated	O
reinforcement	O
learning	O
strategy	O
that	O
directly	O
backpropagates	O
the	O
response	O
-	O
related	O
rewards	O
from	O
the	O
critic	O
network	O
to	O
the	O
partner	O
personas	O
generator	O
and	O
the	O
dialogue	O
response	O
generator	O
.	O
expert	O
reward	O
function	O
can	O
be	O
defined	O
to	O
assign	O
the	O
goodness	O
to	O
each	O
response	O
generated	O
(	O
Roman	O
Roman	O
et	O
al	O
,	O
2020	O
)	O
.	O

The	O
knowledge	O
learned	O
through	O
model	B-MethodName
-	I-MethodName
based	I-MethodName
RL	I-MethodName
is	O
contributed	O
to	O
a	O
knowledge	O
base	O
that	O
can	O
be	O
used	O
for	O
many	O
tasks	O
.	O
So	O
our	O
KRR	B-MethodName
-	I-MethodName
RL	I-MethodName
framework	O
enables	O
a	O
robot	O
to	O
dynamically	O
generate	O
partial	O
world	O
models	O
for	O
tasks	O
under	O
settings	O
that	O
were	O
never	O
experienced	O
.	O
For	O
example	O
,	O
an	O
agent	O
does	O
not	O
know	O
the	O
current	O
time	O
is	O
morning	O
or	O
noon	O
,	O
there	O
are	O
two	O
possible	O
values	O
for	O
variable	O
"	O
time	O
"	O
.	O
Consider	O
that	O
our	O
agent	O
has	O
learned	O
world	O
dynamics	O
under	O
the	O
times	O
of	O
morning	O
and	O
noon	O
.	O
Our	O
KRR	B-MethodName
-	I-MethodName
RL	I-MethodName
framework	O
enables	O
the	O
robot	O
to	O
reason	O
about	O
the	O
two	O
transition	O
systems	O
under	O
the	O
two	O
settings	O
and	O
generate	O
a	O
new	O
transition	O
system	O
for	O
this	O
"	O
morning	O
-	O
or	O
-	O
noon	O
"	O
setting	O
.	O
Without	O
our	O
framework	O
,	O
an	O
agent	O
would	O
have	O
to	O
randomly	O
select	O
one	O
between	O
the	O
"	O
morning	O
"	O
and	O
"	O
noon	O
"	O
policies	O
.	O
To	O
evaluate	O
our	O
policies	O
dynamically	O
constructed	O
via	O
KRR	B-MethodName
,	O
we	O
let	O
an	O
agent	O
learn	O
three	O
controllers	O
under	O
three	O
different	O
environment	O
settings	O
-	O
the	O
navigation	O
actions	O
have	O
decreasing	O
success	O
rates	O
under	O
the	O
settings	O
.	O
In	O
this	O
experiment	O
,	O
the	O
robot	O
does	O
not	O
know	O
which	O
setting	O
it	O
is	O
in	O
(	O
out	O
of	O
two	O
that	O
are	O
randomly	O
selected	O
)	O
.	O
The	O
baseline	O
does	O
not	O
have	O
the	O
KRR	B-MethodName
capability	O
of	O
merging	O
knowledge	O
learned	O
from	O
different	O
settings	O
,	O
and	O
can	O
only	O
randomly	O
select	O
a	O
policy	O
from	O
the	O
two	O
(	O
each	O
corresponding	O
to	O
a	O
setting	O
)	O
.	O
Experimental	O
results	O
show	O
that	O
the	O
baseline	O
agent	O
achieved	O
an	O
average	O
of	O
26.8	B-MetricValue
%	I-MetricValue
success	B-MetricName
rate	I-MetricName
in	O
navigation	O
tasks	O
,	O
whereas	O
our	O
KRR	B-MethodName
-	I-MethodName
RL	I-MethodName
agent	O
achieved	B-MetricValue
83.8	I-MetricValue
%	I-MetricValue
success	B-MetricName
rate	I-MetricName
on	O
average	O
.	O
Figure	O
7	O
shows	O
the	O
costs	B-MetricName
in	O
a	O
box	O
plot	O
(	O
including	O
min	O
-	O
max	O
,	O
25	O
%	O
,	O
and	O
75	O
%	O
values	O
)	O
.	O
Thus	O
,	O
KRR	B-MethodName
-	I-MethodName
RL	I-MethodName
enables	O
a	O
robot	O
to	O
effectively	O
apply	O
the	O
learned	O
knowledge	O
to	O
tasks	O
under	O
new	O
settings	O
.	O
Let	O
us	O
take	O
a	O
closer	O
look	O
at	O
the	O
"	O
time	O
"	O
variable	O
T	O
.	O
If	O
T	O
is	O
the	O
domain	O
of	O
T	O
,	O
the	O
RL	O
-	O
only	O
baseline	O
has	O
to	O
compute	O
a	O
total	O
of	O
2	O
|	O
T	O
|	O
world	O
models	O
to	O
account	O
for	O
all	O
possible	O
information	O
about	O
the	O
value	O
of	O
T	O
,	O
where	O
2	O
|	O
T	O
|	O
is	O
the	O
number	O
of	O
subsets	O
of	O
T	O
.	O
If	O
there	O
are	O
N	O
such	O
variables	O
,	O
the	O
number	O
of	O
world	O
models	O
grows	O
exponentially	O
to	O
2	O
|	O
T	O
|	O
N	O
.	O
In	O
comparison	O
,	O
the	O
KRR	B-MethodName
-	I-MethodName
RL	I-MethodName
agent	O
needs	O
to	O
compute	O
only	O
|	O
T	O
|	O
N	O
world	O
models	O
,	O
which	O
dramatically	O
reduces	O
the	O
number	O
of	O
parameters	O
that	O
must	O
be	O
learned	O
through	O
RL	O
while	O
retaining	O
policy	O
quality	O
.	O

In	O
Section	O
7.2	O
,	O
we	O
trained	O
our	O
probability	O
model	O
on	O
both	O
short	O
phrase	O
pairs	O
for	O
which	O
we	O
had	O
gold	O
probabilities	O
and	O
longer	O
SNLI	B-MethodName
sentence	O
pairs	O
for	O
which	O
we	O
estimated	O
probabilities	O
.	O
We	O
now	O
evaluate	O
the	O
effectiveness	O
of	O
this	O
model	O
for	O
textual	B-TaskName
entailment	I-TaskName
,	O
and	O
demonstrate	O
that	O
these	O
predicted	O
probabilities	O
are	O
informative	O
features	O
for	O
predicting	O
entailment	O
on	O
both	O
SICK	B-DatasetName
and	O
SNLI	B-DatasetName
.	O
Model	O
We	O
first	O
train	O
an	O
LSTM	B-MethodName
similar	O
to	O
the	O
100d	O
LSTM	B-MethodName
that	O
achieved	O
the	O
best	O
accuracy	O
of	O
the	O
neural	O
models	O
in	O
Bowman	O
et	O
al	O
(	O
2015	O
)	O
.	O
It	O
takes	O
GloVe	O
word	O
vectors	O
as	O
input	O
and	O
produces	O
100d	O
sentence	O
vectors	O
for	O
the	O
premise	O
and	O
hypothesis	O
.	O
200d	B-HyperparameterValue
tanh	B-HyperparameterName
layers	I-HyperparameterName
and	O
a	O
softmax	O
layer	O
for	O
3	O
-	O
class	O
entailment	O
classification	O
.	O
We	O
train	O
the	O
LSTM	B-MethodName
on	O
the	O
SNLI	B-DatasetName
training	O
data	O
with	O
batch	B-HyperparameterName
size	I-HyperparameterName
512	B-HyperparameterValue
for	O
10	B-HyperparameterValue
epochs	B-HyperparameterName
.	O
We	O
use	O
the	O
Adam	O
optimizer	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.001	B-HyperparameterValue
and	O
a	O
dropout	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.85	B-HyperparameterValue
,	O
and	O
use	O
the	O
development	O
data	O
to	O
select	O
the	O
best	O
model	O
.	O
Next	O
,	O
we	O
take	O
the	O
output	O
vector	O
produced	O
by	O
the	O
LSTM	B-MethodName
for	O
each	O
sentence	O
pair	O
and	O
append	O
our	O
predicted	O
P	O
(	O
h	O
|	O
p	O
)	O
value	O
(	O
the	O
probability	O
of	O
the	O
hypothesis	O
given	O
the	O
premise	O
)	O
.	O
We	O
train	O
another	O
classifier	O
that	O
passes	O
this	O
201d	O
vector	O
through	O
two	O
tanh	O
layers	O
with	O
a	B-HyperparameterName
dropout	I-HyperparameterName
rate	I-HyperparameterName
of	B-HyperparameterValue
0.5	I-HyperparameterValue
and	O
a	O
final	O
3	O
-	O
class	O
softmax	O
classification	O
layer	O
.	O
Holding	O
the	O
parameters	O
of	O
the	O
LSTM	B-MethodName
fixed	O
,	O
we	O
train	O
this	O
model	O
for	O
10	B-HyperparameterValue
epochs	B-HyperparameterName
on	O
the	O
SNLI	B-DatasetName
training	O
data	O
with	O
batch	B-HyperparameterName
size	I-HyperparameterName
512	B-HyperparameterValue
.	O
Results	O
Table	O
2	O
contains	O
our	O
results	O
on	O
SNLI	B-DatasetName
.	O
Our	O
baseline	O
LSTM	B-MethodName
achieves	O
the	O
same	O
77.2	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
reported	O
by	O
Bowman	O
et	O
al	O
(	O
2015	O
)	O
,	O
whereas	O
a	O
classifier	O
that	O
combines	O
the	O
output	O
of	O
this	O
LSTM	B-MethodName
with	O
only	O
a	O
single	O
feature	O
from	O
the	O
output	O
of	O
our	O
probability	O
model	O
improves	O
to	O
78.2	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
.	O
We	O
use	O
the	O
same	O
approach	O
to	O
evaluate	O
the	O
effectiveness	O
of	O
our	O
predictions	O
on	O
SICK	B-DatasetName
(	O
Table	O
3	O
)	O
.	O
SICK	B-DatasetName
does	O
not	O
have	O
enough	O
data	O
to	O
train	O
an	O
LSTM	O
,	O
so	O
we	O
combine	O
the	O
SICK	B-DatasetName
and	O
SNLI	B-DatasetName
training	O
data	O
to	O
train	O
both	O
the	O
LSTM	B-MethodName
and	O
the	O
final	O
model	O
.	O
When	O
we	O
add	O
the	O
predicted	O
conditional	O
probability	O
as	O
a	O
single	O
feature	O
for	O
each	O
SICK	B-DatasetName
sentence	O
pair	O
,	O
performance	O
increases	O
from	O
81.5	B-MetricValue
%	I-MetricValue
to	O
82.7	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
.	O
This	O
approach	O
outperforms	O
the	O
transfer	O
learning	O
approach	O
of	O
Bowman	O
et	O
al	O
(	O
2015	O
)	O
,	O
which	O
was	O
also	O
trained	O
on	O
both	O
SICK	B-DatasetName
and	O
SNLI	B-DatasetName
.	O

We	O
evaluate	O
our	O
model	O
using	O
1	O
)	O
the	O
KL	B-MetricName
divergences	I-MetricName
D	O
KL	O
(	O
P	O
|	O
|	O
Q	O
)	O
of	O
the	O
gold	O
individual	O
and	O
conditional	O
probabilities	O
P	O
(	O
x	O
)	O
and	O
P	O
(	O
x	O
|	O
y	O
)	O
against	O
the	O
corresponding	O
predicted	O
probabilities	O
Q	O
,	O
and	O
2	O
)	O
the	O
Pearson	B-MetricName
correlation	I-MetricName
r	O
,	O
which	O
expresses	O
the	O
correlation	O
between	O
two	O
variables	O
(	O
the	O
per	O
-	O
item	O
gold	O
and	O
predicted	O
probabilities	O
)	O
as	O
a	O
value	O
between	O
−1	O
(	O
total	O
negative	O
correlation	O
)	O
and	O
1	O
(	O
total	O
positive	O
correlation	O
)	O
.	O
As	O
described	O
above	O
,	O
we	O
compute	O
the	O
KL	B-MetricName
divergence	I-MetricName
on	O
a	O
per	O
-	O
item	O
basis	O
,	O
and	O
report	O
the	O
mean	O
over	O
all	O
items	O
in	O
the	O
test	O
set	O
.	O
Table	O
1	O
shows	O
the	O
performance	O
of	O
our	O
trained	O
model	O
on	O
unseen	O
test	O
data	O
.	O
The	O
full	O
test	O
data	O
consists	O
of	O
4.6	O
million	O
phrase	O
pairs	O
,	O
all	O
of	O
which	O
contain	O
at	O
least	O
one	O
phrase	O
that	O
was	O
not	O
observed	O
in	O
either	O
the	O
training	O
or	O
development	O
data	O
.	O
Our	O
model	O
does	O
reasonably	O
well	O
at	O
predicting	O
these	O
conditional	O
probabilities	O
,	O
reaching	O
a	O
correlation	B-MetricName
of	O
r	B-MetricName
=	O
0.949	B-MetricValue
with	O
P	O
(	O
x	O
|	O
y	O
)	O
on	O
the	O
complete	O
test	O
data	O
.	O
On	O
the	O
subset	O
of	O
123	O
,	O
000	O
test	O
phrase	O
pairs	O
where	O
both	O
phrases	O
are	O
previously	O
unseen	O
,	O
the	O
model	O
's	O
predictions	O
are	O
almost	O
as	O
good	O
at	O
r	B-MetricName
=	O
0.920	B-MetricValue
.	O
On	O
the	O
subset	O
of	O
3	O
,	O
100	O
test	O
phrase	O
pairs	O
where	O
at	O
We	O
also	O
analyze	O
our	O
model	O
's	O
accuracy	B-MetricName
on	O
phrase	O
pairs	O
where	O
the	O
gold	O
P	O
(	O
x	O
|	O
y	O
)	O
is	O
either	O
0	B-MetricValue
or	O
1	B-MetricValue
.	O
The	O
latter	O
case	O
reflects	O
an	O
important	O
property	O
of	O
the	O
denotation	O
graph	O
,	O
since	O
P	O
(	O
x	O
|	O
y	O
)	O
=	O
1	O
when	O
x	O
is	O
an	O
ancestor	O
of	O
y.	O
More	O
generally	O
,	O
we	O
can	O
interpret	O
P	O
(	O
h	O
|	O
p	O
)	O
=	O
1	O
as	O
a	O
confident	O
prediction	O
of	O
entailment	O
,	O
and	O
P	O
(	O
h	O
|	O
p	O
)	O
=	O
0	O
as	O
a	O
confident	O
prediction	O
of	O
contradiction	O
.	O
Figure	O
4	O
shows	O
the	O
distribution	O
of	O
predicted	O
conditional	O
probabilities	O
for	O
phrase	O
pairs	O
where	O
gold	O
P	O
(	O
h	O
|	O
p	O
)	O
=	O
0	O
(	O
top	O
)	O
and	O
gold	O
P	O
(	O
h	O
|	O
p	O
)	O
=	O
1	O
(	O
bottom	O
)	O
.	O
Our	O
model	O
's	O
predictions	O
on	O
unseen	O
phrase	O
pairs	O
(	O
gray	O
bars	O
)	O
are	O
nearly	O
as	O
accurate	O
as	O
its	O
predictions	O
on	O
the	O
full	O
test	O
data	O
(	O
black	O
bars	O
)	O
.	O

To	O
train	O
our	O
model	O
,	O
we	O
use	O
phrase	O
pairs	O
x	O
,	O
y	O
from	O
the	O
denotation	O
graph	O
generated	O
on	O
the	O
training	O
split	O
of	O
the	O
FLICKR30	B-DatasetName
K	I-DatasetName
corpus	O
(	O
Young	O
et	O
al	O
,	O
2014	O
)	O
.	O
We	O
consider	O
all	O
271	O
,	O
062	O
phrases	O
that	O
occur	O
with	O
at	O
least	O
10	O
images	O
in	O
the	O
training	O
split	O
of	O
the	O
graph	O
,	O
to	O
ensure	O
that	O
the	O
phrases	O
are	O
frequent	O
enough	O
that	O
their	O
computed	O
denotational	O
probabilities	O
are	O
reliable	O
.	O
Since	O
the	O
FLICKR30	B-DatasetName
K	I-DatasetName
captions	O
are	O
lemmatized	O
in	O
order	O
to	O
construct	O
the	O
denotation	O
graph	O
,	O
all	O
the	O
phrases	O
in	O
the	O
dataset	O
described	O
in	O
this	O
section	O
are	O
lemmatized	O
as	O
well	O
.	O
We	O
include	O
all	O
phrase	O
pairs	O
where	O
the	O
two	O
phrases	O
have	O
at	O
least	O
one	O
image	O
in	O
common	O
.	O
These	O
constitute	O
45	O
million	O
phrase	O
pairs	O
x	O
,	O
y	O
with	O
P	O
(	O
x	O
|	O
y	O
)	O
>	O
0	O
.	O
To	O
train	O
our	O
model	O
to	O
predict	O
P	O
(	O
x	O
|	O
y	O
)	O
=	O
0	O
,	O
we	O
include	O
phrase	O
pairs	O
x	O
,	O
y	O
that	O
have	O
no	O
images	O
in	O
common	O
if	O
N	O
×P	O
(	O
x	O
)	O
P	O
(	O
y	O
)	O
≥	O
N	O
−1	O
(	O
N	O
is	O
the	O
total	O
number	O
of	O
images	O
)	O
,	O
meaning	O
that	O
x	O
and	O
y	O
occur	O
frequently	O
enough	O
that	O
we	O
would	O
expect	O
them	O
to	O
co	O
-	O
occur	O
at	O
least	O
once	O
in	O
the	O
data	O
.	O
This	O
yields	O
2	O
million	O
pairs	O
where	O
P	O
(	O
x	O
|	O
y	O
)	O
=	O
0	O
.	O
For	O
additional	O
examples	O
of	O
P	O
(	O
x	O
|	O
y	O
)	O
=	O
1	O
,	O
we	O
include	O
phrase	O
pairs	O
that	O
have	O
an	O
ancestordescendant	O
relationship	O
in	O
the	O
denotation	O
graph	O
.	O
We	O
include	O
all	O
ancestor	O
-	O
descendant	O
pairs	O
where	O
each	O
phrase	O
occurs	O
with	O
at	O
least	O
2	O
images	O
,	O
for	O
an	O
additional	O
3	O
million	O
phrase	O
pairs	O
.	O
For	O
evaluation	O
purposes	O
,	O
we	O
first	O
assign	O
5	B-HyperparameterValue
%	I-HyperparameterValue
of	O
the	O
phrases	O
to	O
the	O
development	O
pool	O
and	O
5	B-HyperparameterValue
%	I-HyperparameterValue
to	O
the	O
test	O
pool	O
.	O
The	O
actual	O
test	O
data	O
then	O
consists	O
of	O
all	O
phrase	O
pairs	O
where	O
at	O
least	O
one	O
of	O
the	O
two	O
phrases	O
comes	O
from	O
the	O
test	O
pool	O
.	O
The	O
resulting	O
test	O
data	O
contains	O
10.6	O
%	O
unseen	O
phrases	O
by	O
type	O
and	O
51.2	O
%	O
unseen	O
phrases	O
by	O
token	O
.	O
All	O
phrase	O
pairs	O
in	O
the	O
test	O
data	O
contain	O
at	O
least	O
one	O
phrase	O
that	O
was	O
unseen	O
in	O
the	O
training	O
or	O
development	O
data	O
.	O
The	O
development	O
data	O
was	O
created	O
the	O
same	O
way	O
.	O
This	O
dataset	O
is	O
available	O
to	O
download	O
at	O
http://nlp.cs.illinois.edu/	O
HockenmaierGroup	O
/	O
data.html	O
.	O
We	O
train	O
our	O
model	O
on	O
the	O
training	O
data	O
(	O
42	O
million	O
phrase	O
pairs	O
)	O
with	O
batch	B-HyperparameterName
size	I-HyperparameterName
512	B-HyperparameterValue
for	O
10	B-HyperparameterValue
epochs	B-HyperparameterName
,	O
and	O
use	O
the	O
mean	B-MetricName
KL	I-MetricName
divergence	I-MetricName
on	O
the	O
conditional	O
probabilities	O
in	O
the	O
development	O
data	O
to	O
select	O
the	O
best	O
model	O
.	O
Since	O
P	O
(	O
x	O
|	O
y	O
)	O
is	O
a	O
Bernoulli	O
distribution	O
,	O
we	O
compute	O
the	O
KL	B-MetricName
divergence	I-MetricName
for	O
each	O
phrase	O
pair	O
x	O
,	O
y	O
as	O
D	O
KL	O
(	O
P	O
|	O
|	O
Q	O
)	O
=	O
P	O
(	O
x	O
|	O
y	O
)	O
log	O
P	O
(	O
x	O
|	O
y	O
)	O
Q	O
(	O
x	O
|	O
y	O
)	O
+	O
1	O
−	O
P	O
(	O
x	O
|	O
y	O
)	O
log	O
1	O
−	O
P	O
(	O
x	O
|	O
y	O
)	O
1	O
−	O
Q	O
(	O
x	O
|	O
y	O
)	O
where	O
Q	O
(	O
x	O
|	O
y	O
)	O
is	O
the	O
conditional	O
probability	O
predicted	O
by	O
our	O
model	O
.	O

Predictions	O
trained	O
with	O
Cross	B-MetricName
Entropy	I-MetricName
Each	O
phrase	O
is	O
a	O
sequence	O
of	O
word	O
embeddings	O
that	O
is	O
passed	O
through	O
an	O
LSTM	B-MethodName
to	O
produce	O
a	O
512d	O
vector	O
representation	O
for	O
the	O
premise	O
and	O
the	O
hypothesis	O
.	O
Both	O
vectors	O
are	O
used	O
to	O
compute	O
the	O
predicted	O
conditional	O
probability	O
and	O
calculate	O
the	O
loss	B-MetricName
.	O
can	O
not	O
express	O
P	O
(	O
X	O
)	O
=	O
0	O
exactly	O
,	O
but	O
can	O
get	O
arbitrarily	O
close	O
in	O
order	O
to	O
represent	O
the	O
probability	O
of	O
a	O
phrase	O
that	O
is	O
extremely	O
unlikely	O
.	O
6	O
Our	O
model	O
for	O
P	O
(	O
x	O
)	O
and	O
P	O
(	O
x	O
,	O
y	O
)	O
We	O
train	O
a	O
neural	O
network	O
model	O
to	O
predict	O
P	O
(	O
x	O
)	O
,	O
P	O
(	O
y	O
)	O
,	O
and	O
P	O
(	O
x	O
|	O
y	O
)	O
for	O
phrases	O
x	O
and	O
y.	O
This	O
model	O
consists	O
of	O
an	O
LSTM	B-MethodName
that	O
outputs	O
a	O
512d	B-HyperparameterValue
vector	O
which	O
is	O
passed	O
through	O
an	O
additional	O
512d	B-HyperparameterValue
layer	O
.	O
We	O
use	O
300d	B-HyperparameterValue
GloVe	O
vectors	O
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
trained	O
on	O
840B	O
tokens	O
as	O
the	O
word	O
embedding	O
input	O
to	O
the	O
LSTM	B-MethodName
.	O
We	O
use	O
the	O
same	O
model	O
to	O
represent	O
both	O
x	O
and	O
y	O
regardless	O
of	O
which	O
phrase	O
is	O
the	O
premise	O
or	O
the	O
hypothesis	O
.	O
Thus	O
,	O
we	O
pass	O
the	O
sequence	O
of	O
word	O
embeddings	O
for	O
phrase	O
x	O
through	O
the	O
model	O
to	O
get	O
x	O
,	O
and	O
we	O
do	O
the	O
same	O
for	O
phrase	O
y	O
to	O
get	O
y.	O
As	O
previously	O
described	O
,	O
we	O
sum	O
the	O
elements	O
of	O
x	O
and	O
y	O
to	O
get	O
the	O
predicted	O
denotational	O
probabilities	O
P	O
(	O
x	O
)	O
and	O
P	O
(	O
y	O
)	O
.	O
From	O
x	O
and	O
y	O
,	O
we	O
find	O
the	O
joint	O
vector	O
z	O
,	O
which	O
we	O
use	O
to	O
compute	O
the	O
predicted	O
denotational	O
conditional	O
probability	O
P	O
(	O
x	O
|	O
y	O
)	O
according	O
to	O
the	O
equation	O
in	O
Section	O
5	O
.	O
Figure	O
3	O
illustrates	O
the	O
structure	O
of	O
our	O
model	O
.	O
Our	O
training	O
data	O
consists	O
of	O
ordered	O
phrase	O
pairs	O
x	O
,	O
y	O
.	O
We	O
train	O
our	O
model	O
to	O
predict	O
the	O
denotational	O
probabilities	O
of	O
each	O
phrase	O
(	O
P	O
(	O
x	O
)	O
and	O
P	O
(	O
y	O
)	O
)	O
as	O
well	O
as	O
the	O
conditional	O
probability	O
P	O
(	O
x	O
|	O
y	O
)	O
.	O
Typically	O
the	O
pair	O
y	O
,	O
x	O
will	O
also	O
appear	O
in	O
the	O
training	O
data	O
.	O
Our	O
per	O
-	O
example	O
loss	O
is	O
the	O
sum	O
of	O
the	O
cross	O
entropy	O
losses	O
for	O
P	O
(	O
x	O
)	O
,	O
P	O
(	O
y	O
)	O
,	O
and	O
P	O
(	O
x	O
|	O
y	O
)	O
:	O
L	O
=	O
−	O
P	O
(	O
x	O
)	O
log	O
Q	O
(	O
x	O
)	O
+	O
(	O
1−P	O
(	O
x	O
)	O
)	O
log	O
1−Q	O
(	O
x	O
)	O
−	O
P	O
(	O
y	O
)	O
log	O
Q	O
(	O
y	O
)	O
+	O
(	O
1−P	O
(	O
y	O
)	O
)	O
log	O
1−Q	O
(	O
y	O
)	O
−	O
P	O
(	O
x	O
|	O
y	O
)	O
log	O
Q	O
(	O
x	O
|	O
y	O
)	O
+	O
(	O
1−P	O
(	O
x	O
|	O
y	O
)	O
)	O
log	O
1−Q	O
(	O
x	O
|	O
y	O
)	O
We	O
use	O
the	O
Adam	O
optimizer	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.001	B-HyperparameterValue
,	O
and	O
a	O
dropout	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.5	B-HyperparameterValue
.	O
These	O
parameters	O
were	O
tuned	O
on	O
the	O
development	O
data	O
.	O
Numerical	O
issues	O
In	O
Section	O
5	O
,	O
we	O
described	O
the	O
probability	O
vectors	O
x	O
as	O
being	O
in	O
the	O
positive	O
orthant	O
.	O
However	O
,	O
in	O
our	O
implementation	O
,	O
we	O
use	O
unnormalized	O
log	O
probabilities	O
.	O
This	O
puts	O
all	O
of	O
our	O
vectors	O
in	O
the	O
negative	O
orthant	O
instead	O
,	O
but	O
it	O
prevents	O
the	O
gradients	O
from	O
becoming	O
too	O
small	O
during	O
training	O
.	O
To	O
ensure	O
that	O
the	O
vectors	O
are	O
in	O
R	O
N	O
−	O
,	O
we	O
clip	O
the	O
values	O
of	O
the	O
elements	O
of	O
x	O
so	O
that	O
x	O
i	O
≤	O
0	O
.	O
To	O
compute	O
log	O
P	O
(	O
x	O
)	O
,	O
we	O
sum	O
the	O
elements	O
of	O
x	O
and	O
clip	O
the	O
sum	O
to	O
the	O
range	O
(	O
log	O
(	O
10	O
−10	O
)	O
,	O
−0.0001	O
)	O
in	O
order	O
to	O
avoid	O
errors	O
caused	O
by	O
passing	O
log	O
(	O
0	O
)	O
values	O
to	O
the	O
loss	O
function	O
.	O
The	O
conditional	O
log	O
probability	O
is	O
simply	O
log	O
P	O
(	O
x	O
|	O
y	O
)	O
=	O
log	O
P	O
(	O
x	O
,	O
y	O
)	O
−	O
log	O
P	O
(	O
y	O
)	O
,	O
where	O
log	O
P	O
(	O
x	O
,	O
y	O
)	O
is	O
now	O
the	O
element	O
-	O
wise	O
minimum	O
:	O
log	O
P	O
(	O
x	O
,	O
y	O
)	O
=	O
i	O
min	O
(	O
x	O
i	O
,	O
y	O
i	O
)	O
This	O
element	O
-	O
wise	O
minimum	O
is	O
a	O
standard	O
pooling	O
operation	O
(	O
we	O
take	O
the	O
minimum	O
instead	O
of	O
the	O
more	O
common	O
max	O
pooling	O
)	O
.	O
Note	O
that	O
if	O
x	O
i	O
>	O
y	O
i	O
,	O
neither	O
element	O
x	O
i	O
nor	O
y	O
i	O
is	O
updated	O
with	O
respect	O
to	O
the	O
P	O
(	O
x	O
|	O
y	O
)	O
loss	O
.	O
Both	O
x	O
i	O
and	O
y	O
i	O
will	O
always	O
be	O
updated	O
with	O
respect	O
to	O
the	O
P	O
(	O
x	O
)	O
and	O
P	O
(	O
y	O
)	O
components	O
of	O
the	O
loss	O
.	O

Link	B-TaskName
prediction	I-TaskName
The	O
link	B-TaskName
prediction	I-TaskName
task	O
is	O
conducted	O
as	O
follows	O
:	O
Given	O
a	O
test	O
triple	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
,	O
we	O
construct	O
queries	O
(	O
?	O
,	O
r	O
,	O
t	O
)	O
and	O
(	O
h	O
,	O
r	O
,	O
?	O
)	O
.	O
For	O
each	O
query	O
,	O
a	O
model	O
scores	O
candidate	O
head	O
(	O
tail	O
)	O
entitieŝ	O
h	O
(	O
t	O
)	O
according	O
to	O
its	O
belief	O
thatĥ	O
(	O
t	O
)	O
completes	O
the	O
triple	O
(	O
i.e.	O
,	O
answers	O
the	O
query	O
)	O
.	O
The	O
goal	O
is	O
of	O
link	B-TaskName
prediction	I-TaskName
is	O
to	O
rank	O
true	O
triples	O
(	O
ĥ	O
,	O
r	O
,	O
t	O
)	O
or	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
higher	O
than	O
false	O
and	O
unseen	O
triples	O
.	O
Link	B-TaskName
prediction	I-TaskName
performance	O
is	O
evaluated	O
with	O
mean	B-MetricName
reciprocal	I-MetricName
rank	I-MetricName
(	O
MRR	B-MetricName
)	O
and	O
hits@k	B-MetricName
.	O
MRR	B-MetricName
is	O
the	O
average	O
reciprocal	O
of	O
each	O
ground	O
-	O
truth	O
entity	O
's	O
rank	O
over	O
all	O
(	O
?	O
,	O
r	O
,	O
t	O
)	O
and	O
(	O
h	O
,	O
r	O
,	O
?	O
)	O
test	O
triples	O
.	O
Hits@k	B-MetricName
measures	O
the	O
proportion	O
of	O
test	O
triples	O
for	O
which	O
the	O
ground	O
-	O
truth	O
entity	O
is	O
ranked	O
in	O
the	O
top	O
-	O
k	O
predicted	O
entities	O
.	O
In	O
computing	O
these	O
metrics	O
,	O
we	O
exclude	O
the	O
predicted	O
entities	O
for	O
which	O
(	O
ĥ	O
,	O
r	O
,	O
t	O
)	O
G	O
or	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
G	O
so	O
that	O
known	O
positive	O
triples	O
do	O
not	O
artificially	O
lower	O
ranking	O
scores	O
.	O
This	O
is	O
called	O
"	O
filtering	O
"	O
(	O
Bordes	O
et	O
al	O
,	O
2013	O
)	O
.	O
Triple	B-TaskName
classification	I-TaskName
Given	O
a	O
triple	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
,	O
the	O
goal	O
of	O
triple	O
classification	O
is	O
to	O
predict	O
a	O
corresponding	O
label	O
y	O
{	O
−1	O
,	O
1	O
}	O
.	O
Since	O
knowledge	O
graph	O
embedding	O
models	O
output	O
real	O
-	O
valued	O
scores	O
for	O
triples	O
,	O
we	O
convert	O
these	O
scores	O
into	O
labels	O
by	O
selecting	O
a	O
decision	O
threshold	O
per	O
relation	O
on	O
the	O
validation	O
set	O
such	O
that	O
validation	O
accuracy	O
is	O
maximized	O
for	O
the	O
model	O
in	O
question	O
.	O
A	O
similar	O
approach	O
was	O
used	O
by	O
Socher	O
et	O
al	O
(	O
2013	O
)	O
.	O
We	O
compare	O
results	O
on	O
three	O
sets	O
of	O
evaluation	O
negatives	O
:	O
(	O
1	O
)	O
We	O
generate	O
one	O
negative	O
per	O
positive	O
by	O
replacing	O
the	O
positive	O
triple	O
's	O
tail	O
entity	O
by	O
a	O
tail	O
entity	O
t	O
sampled	O
uniformly	O
at	O
random	O
;	O
(	O
2	O
)	O
We	O
generate	O
negatives	O
by	O
sampling	O
tail	O
entities	O
according	O
to	O
their	O
relative	O
frequency	O
in	O
the	O
tail	O
slot	O
of	O
all	O
triples	O
;	O
and	O
(	O
3	O
)	O
We	O
use	O
the	O
CODEX	O
hard	O
negatives	O
.	O
We	O
measure	O
accuracy	B-MetricName
and	O
F1	B-MetricName
score	I-MetricName
.	O

To	O
pretrain	O
LAPT	B-MethodName
and	O
VA	B-MethodName
models	O
,	O
we	O
use	O
the	O
code	O
of	O
Chau	O
et	O
al	O
(	O
2020	O
)	O
,	O
who	O
modify	O
the	O
pretraining	O
code	O
of	O
Devlin	O
et	O
al	O
(	O
2019	O
)	O
to	O
only	O
use	O
the	O
masked	B-MetricName
language	I-MetricName
modeling	I-MetricName
(	I-MetricName
MLM	I-MetricName
)	I-MetricName
loss	I-MetricName
.	O
To	O
generate	O
VA	B-MethodName
vocabularies	O
,	O
we	O
train	O
a	O
new	O
vocabulary	O
of	O
size	B-HyperparameterName
5000	B-HyperparameterValue
and	O
select	O
the	O
99	O
wordpieces	O
that	O
replace	O
the	O
most	O
unknown	O
tokens	O
.	O
We	O
train	O
with	O
a	O
fixed	O
linear	O
warmup	B-HyperparameterName
of	O
1000	B-HyperparameterValue
steps	O
.	O
To	O
pretrain	O
BERT	B-MethodName
models	O
,	O
we	O
use	O
the	O
HuggingFace	O
Transformers	O
library	O
(	O
Wolf	O
et	O
al	O
,	O
2020	O
)	O
.	O
Following	O
Muller	O
et	O
al	O
(	O
2021	O
)	O
,	O
we	O
train	O
a	O
half	O
-	O
sized	O
RoBERTa	B-MethodName
model	O
with	O
six	B-HyperparameterValue
layers	B-HyperparameterName
and	O
12	B-HyperparameterValue
attention	B-HyperparameterName
heads	I-HyperparameterName
.	O
We	O
use	O
a	O
byte	O
-	O
pair	O
vocabulary	O
of	O
size	B-HyperparameterName
52000	B-HyperparameterValue
and	O
a	O
linear	O
warmup	B-HyperparameterName
of	O
1	B-HyperparameterValue
epoch	O
.	O
For	O
LAPT	B-MethodName
,	O
VA	B-MethodName
,	O
and	O
BERT	B-MethodName
,	O
we	O
train	O
for	O
up	O
to	O
20	B-HyperparameterValue
epochs	B-HyperparameterName
total	O
,	O
selecting	O
the	O
highest	O
-	O
performing	O
epoch	O
based	O
on	O
validation	O
masked	B-MetricName
language	I-MetricName
modeling	I-MetricName
loss	I-MetricName
.	O
FASTT	B-MethodName
models	O
are	O
trained	O
with	O
the	O
skipgram	O
model	O
for	O
five	B-HyperparameterValue
epochs	B-HyperparameterName
,	O
with	O
the	O
default	O
hyperparameters	O
of	O
Bojanowski	O
et	O
al	O
(	O
2017	O
)	O
.	O
Training	O
of	O
downstream	O
parsers	O
and	O
taggers	O
follows	O
Chau	O
et	O
al	O
(	O
2020	O
)	O
and	O
Kondratyuk	O
and	O
Straka	O
(	O
2019	O
)	O
,	O
with	O
an	O
inverse	O
square	O
-	O
root	O
learning	O
rate	O
decay	O
and	O
linear	O
warmup	O
,	O
and	O
layer	O
-	O
wise	O
gradual	O
unfreezing	O
and	O
discriminative	O
finetuning	O
.	O
Models	O
are	O
trained	O
with	O
AllenNLP	O
,	O
version	O
2.1.0	O
,	O
for	O
up	O
to	O
200	B-HyperparameterValue
epochs	B-HyperparameterName
with	O
early	O
stopping	O
based	O
on	O
validation	O
performance	O
.	O
We	O
choose	O
batch	B-HyperparameterName
sizes	I-HyperparameterName
to	O
be	O
the	O
maximum	O
that	O
allows	O
for	O
successful	O
training	O
on	O
one	O
GPU	O
.	O

Following	O
Chau	O
et	O
al	O
(	O
2020	O
)	O
,	O
we	O
consider	O
how	O
to	O
apply	O
the	O
pretrained	B-MethodName
multilingual	I-MethodName
BERT	I-MethodName
model	O
(	O
MBERT	B-MethodName
;	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
to	O
a	O
target	O
lowresource	O
language	O
,	O
for	O
which	O
both	O
labeled	O
and	O
unlabeled	O
data	O
is	O
scarce	O
.	O
This	O
model	O
has	O
produced	O
strong	O
CWRs	O
for	O
many	O
languages	O
(	O
Kondratyuk	O
and	O
Straka	O
,	O
2019	O
,	O
inter	O
alia	O
)	O
and	O
has	O
been	O
the	O
starting	O
model	O
for	O
many	O
studies	O
on	O
low	O
-	O
resource	O
languages	O
(	O
Muller	O
et	O
al	O
,	O
2021	O
;	O
Pfeiffer	O
et	O
al	O
,	O
2020	O
;	O
.	O
MBERT	B-MethodName
covers	O
the	O
languages	O
with	O
the	O
104	O
largest	O
Wikipedias	O
,	O
and	O
it	O
uses	O
this	O
data	O
to	O
con	O
-	O
struct	O
a	O
wordpiece	O
vocabulary	O
(	O
Wu	O
et	O
al	O
,	O
2016	O
)	O
and	O
train	O
its	O
transformer	O
-	O
based	O
architecture	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
Although	O
low	O
-	O
resource	O
languages	O
are	O
slightly	O
oversampled	O
,	O
high	O
-	O
resource	O
languages	O
still	O
dominate	O
both	O
the	O
final	O
pretraining	O
data	O
and	O
the	O
vocabulary	O
(	O
Ács	O
,	O
2019	O
;	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
.	O
Chau	O
et	O
al	O
(	O
2020	O
)	O
note	O
that	O
target	O
low	O
-	O
resource	O
languages	O
fall	O
into	O
three	O
categories	O
with	O
respect	O
to	O
MBERT	B-MethodName
's	O
pretraining	O
data	O
:	O
the	O
lowest	O
-	O
resource	O
languages	O
in	O
the	O
data	O
(	O
Type	O
1	O
)	O
,	O
completely	O
unseen	O
low	O
-	O
resource	O
languages	O
(	O
Type	O
2	O
)	O
,	O
and	O
low	O
-	O
resouce	O
languages	O
with	O
more	O
representation	O
(	O
Type	O
0	O
)	O
.	O
4	O
Due	O
to	O
their	O
poor	O
representation	O
in	O
the	O
vocabulary	O
,	O
Type	O
1	O
and	O
Type	O
2	O
languages	O
achieve	O
suboptimal	O
tokenization	O
and	O
higher	O
rates	O
of	O
the	O
"	O
unknown	O
"	O
wordpiece	O
5	O
when	O
using	O
MBERT	B-MethodName
out	O
of	O
the	O
box	O
.	O
This	O
hinders	O
the	O
model	O
's	O
ability	O
to	O
capture	O
meaningful	O
patterns	O
in	O
the	O
data	O
,	O
resulting	O
in	O
reduced	O
data	O
efficiency	O
and	O
degraded	O
performance	O
.	O
We	O
note	O
that	O
this	O
challenge	O
is	O
exacerbated	O
when	O
modeling	O
languages	O
written	O
in	O
non	O
-	O
Latin	O
scripts	O
.	O
MBERT	B-MethodName
's	O
vocabulary	O
is	O
heavily	O
Latin	O
-	O
centric	O
(	O
Ács	O
,	O
2019	O
;	O
Muller	O
et	O
al	O
,	O
2021	O
)	O
,	O
resulting	O
in	O
a	O
significantly	O
larger	O
portion	O
of	O
non	O
-	O
Latin	O
scripts	O
being	O
represented	O
with	O
"	O
unknown	O
"	O
tokens	O
(	O
Pfeiffer	O
et	O
al	O
,	O
2021b	O
)	O
and	O
further	O
limiting	O
the	O
model	O
's	O
ability	O
to	O
generalize	O
.	O
In	O
effect	O
,	O
MBERT	B-MethodName
's	O
low	O
initial	O
performance	O
on	O
such	O
languages	O
can	O
be	O
attributed	O
to	O
its	O
inability	O
to	O
represent	O
the	O
script	O
itself	O
.	O
To	O
alleviate	O
the	O
problem	O
of	O
poor	O
tokenization	O
,	O
Chau	O
et	O
al	O
(	O
2020	O
)	O
propose	O
to	O
specialize	O
MBERT	B-MethodName
using	O
Vocabulary	O
Augmentation	O
(	O
VA	O
)	O
.	O
Given	O
unlabeled	O
data	O
in	O
the	O
target	O
language	O
,	O
they	O
train	O
a	O
new	O
wordpiece	O
vocabulary	O
on	O
the	O
data	O
,	O
then	O
select	O
the	O
99	O
most	O
common	O
wordpieces	O
in	O
the	O
new	O
vocabulary	O
that	O
replace	O
"	O
unknown	O
"	O
tokens	O
under	O
the	O
original	O
vocabulary	O
.	O
They	O
then	O
add	O
these	O
99	O
wordpieces	O
to	O
the	O
original	O
vocabulary	O
and	O
continue	O
pretraining	O
MBERT	B-MethodName
on	O
the	O
unlabeled	O
data	O
for	O
additional	O
steps	O
.	O
They	O
further	O
describe	O
a	O
tiered	O
variant	O
(	O
TVA	O
)	O
,	O
in	O
which	O
a	O
larger	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
used	O
for	O
the	O
embeddings	O
of	O
these	O
99	O
new	O
wordpieces	O
.	O
VA	O
yields	O
strong	O
gains	O
over	O
unadapted	O
multilingual	O
language	O
models	O
on	O
dependency	O
parsing	O
in	O
four	O
low	O
-	O
resource	O
languages	O
with	O
Latin	O
scripts	O
.	O
How	O
-	O
ever	O
,	O
no	O
evaluation	O
has	O
been	O
performed	O
on	O
other	O
tasks	O
or	O
on	O
languages	O
with	O
non	O
-	O
Latin	O
scripts	O
,	O
which	O
raises	O
our	O
first	O
research	O
question	O
:	O
RQ1	O
:	O
Do	O
the	O
conclusions	O
of	O
Chau	O
et	O
al	O
(	O
2020	O
)	O
hold	O
for	O
other	O
tasks	O
and	O
for	O
languages	O
with	O
non	O
-	O
Latin	O
scripts	O
?	O
We	O
can	O
view	O
VA	O
and	O
TVA	O
as	O
an	O
instantation	O
of	O
a	O
more	O
general	O
framework	O
of	O
vocabulary	O
augmentation	O
,	O
shared	O
by	O
other	O
approaches	O
to	O
using	O
MBERT	B-MethodName
in	O
low	O
-	O
resource	O
settings	O
.	O
Given	O
a	O
new	O
vocabulary	O
V	O
,	O
number	O
of	O
wordpieces	O
n	O
,	O
and	O
learning	O
rate	O
multiplier	O
a	O
,	O
the	O
n	O
most	O
common	O
wordpieces	O
in	O
V	O
are	O
added	O
to	O
the	O
original	O
vocabulary	O
.	O
Additional	O
pretraining	O
is	O
then	O
performed	O
,	O
with	O
the	O
embeddings	O
of	O
the	O
n	O
wordpieces	O
taking	O
on	O
a	O
learning	O
rate	O
a	O
times	O
greater	O
than	O
the	O
overall	O
learning	B-HyperparameterValue
rate	I-HyperparameterValue
.	O
For	O
VA	O
,	O
we	O
set	O
n	B-HyperparameterName
=	O
99	B-HyperparameterValue
and	O
a	B-HyperparameterName
=	O
1	B-HyperparameterValue
,	O
while	O
we	O
treat	O
a	B-HyperparameterName
as	O
a	O
hyperparameter	O
for	O
TVA	O
.	O
The	O
related	O
E	B-MethodName
-	I-MethodName
MBERT	I-MethodName
method	O
of	O
sets	O
n	B-HyperparameterName
=	O
|	B-HyperparameterValue
V	I-HyperparameterValue
|	I-HyperparameterValue
and	O
a	B-HyperparameterName
=	O
1	B-HyperparameterValue
.	O
Investigating	O
various	O
other	O
instantiations	O
of	O
this	O
framework	O
is	O
an	O
interesting	O
research	O
direction	O
,	O
though	O
it	O
is	O
out	O
of	O
the	O
scope	O
of	O
this	O
work	O
.	O

This	O
paper	O
describes	O
CAS	O
IIE	O
's	O
submission	O
to	O
the	O
WMT20	B-DatasetName
German↔French	B-TaskName
news	I-TaskName
translation	I-TaskName
task	I-TaskName
.	O
We	O
investigate	O
extremely	O
deep	O
models	O
(	O
with	O
48	B-HyperparameterValue
layers	B-HyperparameterName
)	O
and	O
exploit	O
effective	O
strategies	O
to	O
better	O
utilize	O
parallel	O
data	O
as	O
well	O
as	O
monolingual	O
data	O
.	O
Finally	O
,	O
our	O
German	O
French	O
system	O
achieved	O
35.0	B-MetricValue
BLEU	B-MetricName
and	O
ranked	O
the	O
second	O
among	O
all	O
anonymous	O
submissions	O
,	O
and	O
our	O
French	O
German	O
system	O
achieved	O
36.6	B-MetricValue
BLEU	B-MetricName
and	O
ranked	O
the	O
fourth	O
in	O
all	O
anonymous	O
submissions	O
.	O

Results	O
and	O
ablations	O
for	O
De	O
Fr	O
Fr	O
De	O
are	O
shown	O
in	O
Table	O
1	O
and	O
2	O
,	O
respectively	O
.	O
We	O
report	O
case	O
-	O
sensitive	O
SacreBLEU	B-MetricName
scores	O
using	O
Sacre	O
-	O
BLEU	O
(	O
Post	O
,	O
2018	O
)	O
3	O
,	O
using	O
international	O
tokenization	O
for	O
German↔French	O
.	O
German	O
French	O
For	O
De	O
Fr	O
,	O
iterative	B-MethodName
BT	I-MethodName
improves	O
our	O
baseline	O
performance	O
on	O
newstest	B-DatasetName
2019	O
by	O
about	O
2.5	B-MetricValue
BLEU	B-MetricName
.	O
The	O
addition	O
of	O
KD	O
and	O
model	O
ensemble	O
improves	O
single	O
model	O
performance	O
by	O
0.8	B-MetricValue
BLEU	B-MetricName
,	O
but	O
combining	O
this	O
with	O
fine	O
-	O
tuning	O
and	O
reranking	O
gives	O
us	O
a	O
total	O
of	O
2	B-MetricValue
BLEU	B-MetricName
.	O
Our	O
final	O
submission	O
for	O
WMT20	B-DatasetName
achieves	O
35.0	B-MetricValue
BLEU	B-MetricName
points	O
for	O
German	B-TaskName
French	I-TaskName
translation	I-TaskName
(	O
ranked	O
in	O
the	O
second	O
place	O
)	O
.	O
French	O
German	O
For	O
Fr	O
De	O
,	O
we	O
see	O
similar	O
improvements	O
with	O
iterative	B-MethodName
BT	I-MethodName
by	O
about	O
2.3	B-MetricValue
BLEU	B-MetricName
.	O
KD	O
,	O
ensembling	O
,	O
and	O
fine	O
-	O
tuning	O
add	O
an	O
additional	O
1.4	B-MetricValue
BLEU	B-MetricName
,	O
with	O
reranking	O
contributing	O
0.9	B-MetricValue
BLEU	B-MetricName
.	O
Our	O
final	O
submission	O
for	O
WMT20	B-DatasetName
achieves	O
36.6	O
BLEU	B-MetricName
points	O
for	O
French	B-TaskName
German	I-TaskName
translation	I-TaskName
(	O
ranked	O
in	O
the	O
fourth	O
among	O
anonymous	O
submissions	O
)	O
.	O

We	O
use	O
the	O
PyTorch	O
implementation	O
of	O
Transformer	B-MethodName
2	O
.	O
We	O
choose	O
the	O
Transformer	B-MethodName
base	O
setting	O
,	O
in	O
which	O
the	O
encoder	O
and	O
decoder	O
are	O
of	O
48	B-HyperparameterValue
and	O
6	B-HyperparameterValue
layers	B-HyperparameterName
,	O
respectively	O
.	O
The	O
dropout	B-HyperparameterName
rate	I-HyperparameterName
is	O
fixed	O
as	O
0.1	B-HyperparameterValue
.	O
We	O
set	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
as	O
4096	B-HyperparameterValue
and	O
the	O
parameter	B-HyperparameterName
-	I-HyperparameterName
-	I-HyperparameterName
update	I-HyperparameterName
-	I-HyperparameterName
freq	I-HyperparameterName
as	O
16	B-HyperparameterValue
.	O

The	O
structure	O
of	O
NMT	B-TaskName
models	O
has	O
evolved	O
quickly	O
,	O
such	O
as	O
RNN	O
-	O
based	O
(	O
Wu	O
et	O
al	O
,	O
2016	O
)	O
,	O
CNN	O
-	O
based	O
(	O
Gehring	O
et	O
al	O
,	O
2017	O
)	O
and	O
attentionbased	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
systems	O
.	O
Deep	O
neural	O
networks	O
have	O
revolutionized	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
various	O
communities	O
,	O
from	O
computer	O
vision	O
to	O
natural	O
language	O
processing	O
.	O
We	O
adopt	O
the	O
deep	B-MethodName
transformer	I-MethodName
model	O
proposed	O
by	O
our	O
work	O
(	O
Wei	O
et	O
al	O
,	O
2020b	O
)	O
.	O
Instead	O
of	O
relying	O
on	O
the	O
whole	O
encoder	O
stack	O
to	O
directly	O
learn	O
a	O
desired	O
representation	O
,	O
we	O
let	O
each	O
encoder	O
block	O
learn	O
a	O
fine	O
-	O
grained	O
representation	O
and	O
enhance	O
it	O
by	O
encoding	O
spatial	O
dependencies	O
using	O
a	O
bottom	O
-	O
up	O
network	O
.	O
For	O
coordination	O
,	O
we	O
attend	O
each	O
block	O
of	O
the	O
decoder	O
to	O
both	O
the	O
corresponding	O
representation	O
of	O
the	O
encoder	O
and	O
the	O
contextual	O
representation	O
with	O
spatial	O
dependencies	O
.	O
This	O
not	O
only	O
shortens	O
the	O
path	O
of	O
error	O
propagation	O
,	O
but	O
also	O
helps	O
to	O
prevent	O
the	O
lower	O
level	O
information	O
from	O
being	O
forgotten	O
or	O
diluted	O
.	O
In	O
this	O
section	O
we	O
describe	O
the	O
details	O
(	O
as	O
illustrated	O
in	O
figure	O
1	O
)	O
of	O
our	O
deep	O
architectures	O
as	O
below	O
:	O
Block	B-MethodName
-	I-MethodName
Scale	I-MethodName
Collaboration	I-MethodName
.	O
An	O
intuitive	O
extension	O
of	O
naive	O
stacking	O
of	O
layers	O
is	O
to	O
group	O
few	O
stacked	O
layers	O
into	O
a	O
block	O
.	O
We	O
suppose	O
that	O
the	O
encoder	O
and	O
decoder	O
of	O
our	O
model	O
have	O
the	O
same	O
number	O
of	O
blocks	O
(	O
i.e.	O
,	O
N	O
)	O
.	O
Each	O
block	O
of	O
the	O
encoder	O
has	O
M	O
n	O
(	O
n	O
{	O
1	O
,	O
2	O
,	O
...	O
,	O
N	O
}	O
)	O
identical	O
layers	O
,	O
while	O
each	O
decoder	O
block	O
contains	O
one	O
layer	O
.	O
Thus	O
,	O
we	O
can	O
adjust	O
the	O
value	O
of	O
each	O
M	O
n	O
flexibly	O
to	O
increase	O
the	O
depth	O
of	O
the	O
encoder	O
.	O
Formally	O
,	O
for	O
the	O
n	O
-	O
th	O
block	O
of	O
the	O
encoder	O
:	O
B	O
n	O
e	O
=	O
BLOCK	O
e	O
(	O
B	O
n−1	O
e	O
)	O
,	O
(	O
1	O
)	O
where	O
BLOCK	O
e	O
(	O
)	O
is	O
the	O
block	O
function	O
,	O
in	O
which	O
the	O
layer	O
function	O
F	O
(	O
)	O
is	O
iterated	O
M	O
n	O
times	O
,	O
i.e.	O
where	O
l	O
{	O
1	O
,	O
2	O
,	O
...	O
,	O
M	O
n	O
}	O
,	O
H	O
n	O
,	O
l	O
e	O
and	O
Θ	O
n	O
,	O
l	O
e	O
are	O
the	O
representation	O
and	O
parameters	O
of	O
the	O
l	O
-	O
th	O
layer	O
in	O
the	O
n	O
-	O
th	O
block	O
,	O
respectively	O
.	O
The	O
decoder	O
works	O
in	O
a	O
similar	O
way	O
but	O
the	O
layer	O
function	O
G	O
(	O
)	O
is	O
iterated	O
only	O
once	O
in	O
each	O
block	O
,	O
B	O
n	O
d	O
=	O
BLOCK	O
d	O
(	O
B	O
n−1	O
d	O
,	O
B	O
n	O
e	O
)	O
=	O
G	O
(	O
B	O
n−1	O
d	O
,	O
B	O
n	O
e	O
;	O
Θ	O
n	O
d	O
)	O
+	O
B	O
n−1	O
d	O
.	O
(	O
3	O
)	O
Each	O
block	O
of	O
the	O
decoder	O
attends	O
to	O
the	O
corresponding	O
encoder	O
block	O
.	O
Contextual	O
Collaboration	O
.	O
To	O
model	O
long	O
-	O
term	O
spatial	O
dependencies	O
and	O
reuse	O
global	O
representations	O
,	O
we	O
define	O
a	O
GRU	O
cell	O
Q	O
(	O
c	O
,	O
x	O
)	O
,	O
which	O
maps	O
a	O
hidden	O
state	O
c	O
and	O
an	O
additional	O
inputx	O
into	O
a	O
new	O
hidden	O
state	O
:	O
C	O
n	O
=	O
Q	O
(	O
C	O
n−1	O
,	O
B	O
n	O
e	O
)	O
,	O
n	O
[	O
1	O
,	O
N	O
]	O
C	O
0	O
=	O
E	O
e	O
,	O
(	O
4	O
)	O
where	O
E	O
e	O
is	O
the	O
embedding	O
matrix	O
of	O
the	O
source	O
input	O
x.	O
The	O
new	O
state	O
C	O
n	O
can	O
be	O
fused	O
with	O
each	O
layer	O
of	O
the	O
subsequent	O
blocks	O
in	O
both	O
the	O
encoder	O
and	O
the	O
decoder	O
.	O
Formally	O
,	O
B	O
n	O
e	O
in	O
Eq	O
.	O
(	O
1	O
)	O
can	O
be	O
re	O
-	O
calculated	O
in	O
the	O
following	O
way	O
:	O
B	O
n	O
e	O
=	O
H	O
n	O
,	O
Mn	O
e	O
,	O
H	O
n	O
,	O
l	O
e	O
=	O
F	O
(	O
H	O
n	O
,	O
l−1	O
e	O
,	O
C	O
n−1	O
;	O
Θ	O
n	O
,	O
l	O
e	O
)	O
+	O
H	O
n	O
,	O
l−1	O
e	O
,	O
H	O
n	O
,	O
0	O
e	O
=	O
B	O
n−1	O
e	O
.	O
(	O
5	O
)	O
Similarly	O
,	O
for	O
decoder	O
,	O
we	O
have	O
B	O
n	O
d	O
=	O
BLOCK	O
d	O
(	O
B	O
n−1	O
d	O
,	O
B	O
n	O
e	O
)	O
=	O
G	O
(	O
B	O
n−1	O
d	O
,	O
B	O
n	O
e	O
,	O
C	O
n	O
;	O
Θ	O
n	O
d	O
)	O
+	O
B	O
n−1	O
d	O
.	O
(	O
6	O
)	O

In	O
this	O
paper	O
we	O
introduce	O
the	O
systems	O
IIE	O
submitted	O
for	O
the	O
WMT20	B-DatasetName
shared	O
task	O
on	O
German↔French	O
news	B-TaskName
translation	I-TaskName
.	O
Our	O
systems	O
are	O
based	O
on	O
the	O
Transformer	O
architecture	O
with	O
some	O
effective	O
improvements	O
.	O
Multiscale	O
collaborative	O
deep	O
architecture	O
,	O
data	B-TaskName
selection	I-TaskName
,	O
back	B-TaskName
translation	I-TaskName
,	O
knowledge	B-TaskName
distillation	I-TaskName
,	O
domain	B-TaskName
adaptation	I-TaskName
,	O
model	O
ensemble	O
and	O
re	B-TaskName
-	I-TaskName
ranking	I-TaskName
are	O
employed	O
and	O
proven	O
effective	O
in	O
our	O
experiments	O
.	O
Our	O
German	O
French	O
system	O
achieved	O
35.0	B-MetricValue
BLEU	B-MetricName
and	O
ranked	O
the	O
second	O
among	O
all	O
anonymous	O
submissions	O
,	O
and	O
our	O
French	O
German	O
system	O
achieved	O
36.6	B-MetricValue
BLEU	B-MetricName
and	O
ranked	O
the	O
fourth	O
in	O
all	O
anonymous	O
submissions	O
.	O

Sample	B-MethodName
Shielding	I-MethodName
,	O
an	O
intuitively	O
designed	O
defense	O
which	O
is	O
attacker	O
and	O
classifier	O
agnostic	O
,	O
protects	O
effectively	O
;	O
reducing	O
ASR	B-MetricName
from	B-MetricValue
90	I-MetricValue
-	I-MetricValue
100	I-MetricValue
%	I-MetricValue
down	O
to	O
14	B-MetricName
-	I-MetricName
34	I-MetricName
%	I-MetricName
with	O
minimal	O
accuracy	B-MetricName
loss	O
(	O
3	B-MetricValue
%	I-MetricValue
)	O
in	O
original	O
texts	O
.	O
The	O
randomness	O
(	O
through	O
sampling	O
)	O
provides	O
unreliable	O
feedback	O
for	O
attackers	O
,	O
thus	O
it	O
even	O
thwarts	O
attackers	O
who	O
have	O
query	O
access	O
to	O
classifiers	O
protected	O
with	O
Sample	B-MethodName
Shielding	I-MethodName
.	O
Attack	O
strategies	O
will	O
need	O
to	O
increase	O
the	O
amount	O
of	O
perturbation	O
to	O
make	O
sure	O
a	O
majority	O
of	O
samples	O
fail	O
at	O
classification	B-TaskName
.	O
However	O
,	O
this	O
will	O
risk	O
semantic	O
integrity	O
.	O
Thus	O
,	O
we	O
expect	O
Sample	B-MethodName
Shielding	I-MethodName
to	O
cause	O
ripples	O
in	O
future	O
adversarial	O
attack	O
strategies	O
while	O
providing	O
text	O
classifiers	O
with	O
a	O
definite	O
advantage	O
.	O

Results	O
are	O
in	O
Table	O
1	O
.	O
BERT	B-MethodName
is	O
the	O
strongest	O
classifier	O
achieving	O
91	B-MetricValue
-	I-MetricValue
100	I-MetricValue
%	I-MetricValue
accuracy	B-MetricName
on	O
the	O
original	O
datasets	O
.	O
Attacks	O
are	O
highly	O
successful	O
against	O
unshielded	O
texts	O
.	O
TextFooler	B-MethodName
and	O
Bert	B-MethodName
-	I-MethodName
Attack	I-MethodName
are	O
the	O
most	O
successful	O
,	O
dropping	O
accuracies	B-MetricName
to	O
0	B-MetricValue
-	I-MetricValue
5	I-MetricValue
%	I-MetricValue
generally	O
.	O
Attacks	O
were	O
able	O
to	O
achieve	O
strong	O
drops	O
with	O
minimal	O
amount	O
of	O
text	O
perturbed	O
(	O
about	O
10	O
%	O
)	O
.	O
Figure	O
3	O
shows	O
that	O
the	O
average	O
percent	O
of	O
words	O
perturbed	O
across	O
datasets	O
for	O
each	O
attack	O
are	O
about	O
equal	O
in	O
the	O
mid	O
regions	O
of	O
the	O
plots	O
.	O
For	O
AG	B-DatasetName
News	I-DatasetName
,	O
attacks	O
are	O
less	O
successful	O
against	O
BERT	B-MethodName
;	O
accuracy	B-MetricName
drops	O
to	O
19	B-MetricValue
%	I-MetricValue
in	O
the	O
strongest	O
attack	O
(	O
TextFooler	B-MethodName
)	O
,	O
and	O
only	O
to	O
49	B-MetricValue
%	I-MetricValue
in	O
the	O
weakest	O
(	O
TextBugger	B-MethodName

Increasing	O
p	B-HyperparameterName
raises	O
the	O
risk	O
of	O
samples	O
containing	O
increased	O
amounts	O
of	O
perturbed	O
text	O
.	O
Decreasing	O
k	B-HyperparameterName
raises	O
the	O
risk	O
of	O
not	O
covering	O
enough	O
of	O
the	O
unperturbed	O
portions	O
of	O
the	O
original	O
text	O
.	O
While	O
our	O
settings	O
of	O
p	B-HyperparameterName
=	O
0.3	B-HyperparameterValue
and	O
k	B-HyperparameterName
=	O
100	B-HyperparameterValue
for	O
our	O
main	O
results	O
are	O
reasonable	O
values	O
(	O
Table	O
1	O
,	O
Table	O
2	O
)	O
they	O
are	O
not	O
necessarily	O
optimal	O
.	O
Optimal	O
p.	B-HyperparameterName
Figure	O
4	O
shows	O
the	O
results	O
for	O
all	O
com	O
-	O
binations	O
of	O
attacks	O
against	O
LSTM	B-MethodName
on	O
IMDB	B-DatasetName
with	O
word	O
shielding	O
as	O
the	O
defense	O
,	O
k	B-HyperparameterName
fixed	O
at	O
100	B-HyperparameterValue
.	O
As	O
we	O
increase	O
p	B-HyperparameterName
,	O
we	O
see	O
a	O
continued	O
drop	O
in	O
accuracy	B-MetricName
which	O
is	O
consistent	O
with	O
the	O
idea	O
that	O
a	O
higher	O
p	B-HyperparameterName
is	O
more	O
likely	O
to	O
capture	O
perturbed	O
text	O
.	O
The	O
optimal	O
value	O
range	O
appears	O
to	O
be	O
in	O
0.2	B-HyperparameterValue
-	I-HyperparameterValue
0.4	I-HyperparameterValue
range	O
,	O
although	O
we	O
do	O
not	O
see	O
large	O
drops	O
until	O
0.6	B-HyperparameterValue
onward	O
.	O
We	O
also	O
examined	O
the	O
same	O
combination	O
on	O
AG	B-DatasetName
News	I-DatasetName
(	O
Figure	O
5	O
)	O
since	O
it	O
's	O
texts	O
are	O
considerably	O
shorter	O
and	O
found	O
consistent	O
results	O
.	O
Optimal	O
k.	B-HyperparameterName
Figure	O
6	O
shows	O
results	O
for	O
all	O
attacks	O
against	O
LSTM	B-MethodName
on	O
IMDB	B-DatasetName
with	O
word	O
sampling	O
as	O
the	O
defense	O
,	O
p	B-HyperparameterName
fixed	O
at	O
0.3	B-HyperparameterValue
.	O
The	O
optimal	O
k	B-HyperparameterName
is	O
not	O
as	O
clear	O
as	O
p.	B-HyperparameterName
We	O
see	O
clear	O
increases	O
after	O
30	O
samples	O
,	O
but	O
then	O
the	O
optimal	O
k	B-HyperparameterName
varies	O
depending	O
on	O
attack	O
.	O
However	O
,	O
we	O
see	O
a	O
leveling	O
off	O
around	O
90	O
samples	O
,	O
which	O
gives	O
some	O
credence	O
to	O
our	O
chosen	O
k	B-HyperparameterName
of	O
100	B-HyperparameterValue
.	O
We	O
also	O
found	O
similar	O
results	O
when	O
examining	O
the	O
same	O
combination	O
on	O
AG	B-DatasetName
News	I-DatasetName
(	O
Figure	O
7	O
)	O
,	O
however	O
,	O
k	B-HyperparameterName
stabilized	O
lower	O
(	O
about	O
50	B-HyperparameterValue
)	O
.	O

