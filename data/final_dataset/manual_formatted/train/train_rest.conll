
Text	B-TaskName
Simplification	I-TaskName
improves	O
the	O
readability	O
of	O
sentences	O
through	O
several	O
rewriting	O
transformations	O
,	O
such	O
as	O
lexical	O
paraphrasing	O
,	O
deletion	O
,	O
and	O
splitting	O
.	O
Current	O
simplification	O
systems	O
are	O
predominantly	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
that	O
are	O
trained	O
end	O
-	O
to	O
-	O
end	O
to	O
perform	O
all	O
these	O
operations	O
simultaneously	O
.	O
However	O
,	O
such	O
systems	O
limit	O
themselves	O
to	O
mostly	O
deleting	O
words	O
and	O
can	O
not	O
easily	O
adapt	O
to	O
the	O
requirements	O
of	O
different	O
target	O
audiences	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
hybrid	O
approach	O
that	O
leverages	O
linguistically	O
-	O
motivated	O
rules	O
for	O
splitting	O
and	O
deletion	O
,	O
and	O
couples	O
them	O
with	O
a	O
neural	O
paraphrasing	O
model	O
to	O
produce	O
varied	O
rewriting	O
styles	O
.	O
We	O
introduce	O
a	O
new	O
data	O
augmentation	O
method	O
to	O
improve	O
the	O
paraphrasing	O
capability	O
of	O
our	O
model	O
.	O
Through	O
automatic	O
and	O
manual	O
evaluations	O
,	O
we	O
show	O
that	O
our	O
proposed	O
model	O
establishes	O
a	O
new	O
state	O
-	O
ofthe	O
art	O
for	O
the	O
task	O
,	O
paraphrasing	O
more	O
often	O
than	O
the	O
existing	O
systems	O
,	O
and	O
can	O
control	O
the	O
degree	O
of	O
each	O
simplification	O
operation	O
applied	O
to	O
the	O
input	O
texts	O
.	O
1	O

We	O
presented	O
an	O
empirical	O
method	O
to	O
explore	O
the	O
relationship	O
between	O
label	O
consistency	O
and	O
NER	B-TaskName
model	O
performance	O
.	O
It	O
identified	O
the	O
label	O
inconsistency	O
of	O
test	O
data	O
in	O
SCIERC	B-DatasetName
and	O
CoNLL03	B-DatasetName
datasets	O
(	O
with	O
26.7	O
%	O
and	O
5.4	O
%	O
label	O
mistakes	O
)	O
.	O
It	O
validated	O
the	O
label	O
consistency	O
in	O
multiple	O
sets	O
of	O
NER	B-TaskName
data	O
annotation	O
on	O
two	O
benchmarks	O
,	O
CoNLL03	B-DatasetName
and	O
SCIERC	B-DatasetName
.	O

SeqMix	B-MethodName
:	O
Augmenting	O
Active	O
Sequence	O
Labeling	O
via	O
Sequence	O
Mixup	O

We	O
propose	O
a	O
simple	O
data	O
augmentation	O
method	O
SeqMix	B-MethodName
to	O
enhance	O
active	B-TaskName
sequence	I-TaskName
labeling	I-TaskName
.	O
By	O
performing	O
sequence	O
mixup	O
in	O
the	O
latent	O
space	O
,	O
Se	B-MethodName
-	I-MethodName
qMix	I-MethodName
improves	O
data	O
diversity	O
during	O
active	O
learning	O
,	O
while	O
being	O
able	O
to	O
generate	O
plausible	O
augmented	O
sequences	O
.	O
This	O
method	O
is	O
generic	O
to	O
different	O
active	O
learning	O
policies	O
and	O
various	O
sequence	O
labeling	O
tasks	O
.	O
Our	O
experiments	O
demonstrate	O
that	O
SeqMix	B-MethodName
can	O
improve	O
active	O
learning	O
baselines	O
consistently	O
for	O
NER	B-TaskName
and	O
event	B-TaskName
detection	I-TaskName
tasks	O
;	O
and	O
its	O
benefits	O
are	O
especially	O
prominent	O
in	O
low	O
-	O
data	O
regimes	O
.	O
For	O
future	O
research	O
,	O
it	O
is	O
interesting	O
to	O
enhance	O
SeqMix	B-MethodName
with	O
language	O
models	O
during	O
the	O
mixup	O
process	O
,	O
and	O
harness	O
external	O
knowledge	O
for	O
further	O
improving	O
diversity	O
and	O
plausibility	O
.	O

Here	O
we	O
list	O
the	O
link	O
to	O
datasets	O
used	O
in	O
our	O
experiments	O
.	O
CoNLL	B-DatasetName
-	O
03	O
:	O
https://github.com/	O
synalp	O
/	O
NER	O
/	O
tree	O
/	O
master	O
/	O
corpus/	O
CoNLL	O
-	O
2003	O
.	O
ACE05	B-DatasetName
:	O
We	O
are	O
unable	O
to	O
provide	O
the	O
downloadable	O
version	O
due	O
to	O
it	O
is	O
not	O
public	O
.	O
This	O
corpus	O
can	O
be	O
applied	O
through	O
the	O
website	O
of	O
LDC	O
:	O
https://www.ldc.upenn.edu/	O
collaborations	O
/	O
past	O
-	O
projects/	O
ace	O
.	O
Webpage	O
:	O
Please	O
refer	O
the	O
link	O
in	O
the	O
paper	O
(	O
Ratinov	O
and	O
Roth	O
,	O
2009	O
)	O
.	O

We	O
implement	O
bert	B-MethodName
-	I-MethodName
base	I-MethodName
-	I-MethodName
cased	I-MethodName
as	O
the	O
underlying	O
model	O
for	O
the	O
NER	B-TaskName
task	O
and	O
bert	B-MethodName
-	I-MethodName
base	I-MethodName
-	I-MethodName
multilingualcased	I-MethodName
as	O
the	O
underlying	O
model	O
for	O
the	O
event	B-TaskName
detection	I-TaskName
task	O
.	O
We	O
use	O
the	O
model	O
from	O
Huggingface	O
Transformer	O
codebase	O
3	O
,	O
and	O
the	O
repository	O
4	O
to	O
finetune	O
our	O
model	O
for	O
sequence	O
labeling	O
task	O
.	O

In	O
Section	O
3.2	O
,	O
we	O
construct	O
a	O
table	O
of	O
tokens	O
W	O
and	O
their	O
corresponding	O
contextual	O
embedding	O
E.	O
For	O
our	O
underlying	O
BERT	B-MethodName
model	O
,	O
we	O
use	O
the	O
vocabulary	O
provided	O
by	O
the	O
tokenizer	O
to	O
build	O
up	O
W	O
,	O
and	O
the	O
embedding	O
initialized	O
on	O
the	O
training	O
set	O
as	O
E.	O
We	O
also	O
need	O
to	O
construct	O
a	O
special	O
token	O
collection	O
to	O
exclude	O
some	O
generation	O
in	O
the	O
process	O
of	O
sequence	O
mixing	O
.	O
For	O
example	O
,	O
BERT	B-MethodName
places	O
token	O
[	O
CLS	O
]	O
and	O
[	O
SEP	O
]	O
at	O
the	O
starting	O
position	O
and	O
the	O
ending	O
position	O
for	O
sentence	O
,	O
and	O
pad	O
the	O
inputs	O
with	O
[	O
PAD	O
]	O
.	O
We	O
exclude	O
these	O
disturbing	O
tokens	O
and	O
the	O
parent	O
tokens	O
.	O

For	O
the	O
5	O
-	O
round	O
active	O
learning	O
with	O
SeqMix	O
augmentation	O
,	O
our	O
program	O
runs	O
about	O
500	O
seconds	O
for	O
WebPage	B-DatasetName
dataset	O
,	O
1700	O
seconds	O
for	O
the	O
CoNLL	B-DatasetName
slicing	O
dataset	O
,	O
and	O
3.5	O
hours	O
for	O
ACE	B-DatasetName
2005	I-DatasetName
.	O
If	O
the	O
QBC	O
query	O
policy	O
used	O
,	O
all	O
the	O
runtime	O
will	O
be	O
multiplied	O
about	O
3	O
times	O
.	O

We	O
also	O
wanted	O
to	O
understand	O
the	O
predictive	O
power	O
of	O
different	O
types	O
of	O
linguistic	O
features	O
towards	O
the	O
detection	O
of	O
complaints	O
.	O
These	O
features	O
can	O
be	O
broadly	O
broken	O
down	O
into	O
four	O
groups	O
.	O
(	O
i	O
)	O
The	O
first	O
group	O
of	O
features	O
are	O
based	O
on	O
simple	O
semantic	O
properties	O
such	O
as	O
n	O
-	O
grams	O
,	O
word	O
embeddings	O
,	O
and	O
part	O
of	O
speech	O
tags	O
.	O
(	O
ii	O
)	O
The	O
second	O
group	O
of	O
features	O
are	O
based	O
on	O
pre	O
-	O
trained	O
sentiment	O
models	O
or	O
lexicons	O
.	O
(	O
iii	O
)	O
The	O
third	O
group	O
of	O
features	O
use	O
orthographic	O
information	O
such	O
as	O
hashtags	O
,	O
user	O
mentions	O
,	O
and	O
intensifiers	O
.	O
(	O
iv	O
)	O
The	O
last	O
group	O
of	O
features	O
again	O
use	O
pre	O
-	O
trained	O
models	O
or	O
lexicons	O
associated	O
with	O
request	O
,	O
which	O
is	O
a	O
closely	O
related	O
speech	O
act	O
(	O
Švárová	O
,	O
2008	O
)	O
.	O

We	O
expect	O
sentiment	O
to	O
contribute	O
strongly	O
towards	O
the	O
prediction	O
of	O
complaints	O
.	O
We	O
experiment	O
with	O
two	O
pre	O
-	O
trained	O
models	O
:	O
Stanford	B-MethodName
Sentiment	I-MethodName
(	O
Socher	O
et	O
al	O
,	O
2013	O
)	O
and	O
VADER	B-MethodName
(	O
Hutto	O
and	O
Gilbert	O
,	O
2014	O
)	O
.	O
Namely	O
,	O
we	O
use	O
the	O
scores	O
predicted	O
by	O
these	O
models	O
as	O
representations	O
of	O
tweets	O
.	O
Likewise	O
,	O
we	O
also	O
experiment	O
with	O
two	O
sentiment	O
lexicons	O
:	O
MPQA	B-MethodName
(	O
Wilson	O
et	O
al	O
,	O
2005	O
)	O
,	O
NRC	B-MethodName
(	O
Mohammad	O
et	O
al	O
,	O
2013	O
)	O
for	O
assigning	O
sentiment	O
scores	O
to	O
tweets	O
.	O

Our	O
goal	O
is	O
to	O
identify	O
how	O
mental	O
health	O
subreddit	O
activity	O
has	O
changed	O
during	O
the	O
pandemic	O
.	O
We	O
first	O
create	O
time	O
series	O
for	O
a	O
number	O
of	O
metrics	O
that	O
could	O
be	O
affected	O
by	O
the	O
pandemic	O
,	O
encompassing	O
activity	O
levels	O
and	O
text	O
content	O
(	O
Section	O
4.1	O
)	O
.	O
We	O
then	O
use	O
a	O
time	O
series	O
intervention	O
analysis	O
technique	O
to	O
determine	O
whether	O
there	O
are	O
significant	O
changes	O
in	O
our	O
metrics	O
during	O
the	O
pandemic	O
(	O
Section	O
4.2	O
)	O
.	O

In	O
traditional	O
topic	B-TaskName
modeling	I-TaskName
(	O
LDA	B-MethodName
)	O
,	O
the	O
top	O
J	O
words	O
are	O
those	O
with	O
highest	O
probability	O
under	O
each	O
topic	O
-	O
word	O
distribution	O
.	O
For	O
centroid	O
based	O
clustering	O
algorithms	O
,	O
the	O
top	O
words	O
of	O
some	O
cluster	O
i	O
are	O
naturally	O
those	O
closest	O
to	O
the	O
cluster	O
center	O
c	O
(	O
i	O
)	O
,	O
or	O
with	O
highest	O
probability	O
under	O
the	O
cluster	O
parameters	O
.	O
Formally	O
,	O
this	O
means	O
choosing	O
the	O
set	O
of	O
types	O
J	O
as	O
argmin	O
J	O
:	O
|	O
J	O
|	O
=	O
10	O
j	O
J	O
	O
	O
	O
	O
c	O
(	O
i	O
)	O
−	O
x	O
j	O
2	O
2	O
for	O
KM	O
/	O
KD	O
,	O
cos	O
(	O
c	O
(	O
i	O
)	O
,	O
x	O
j	O
)	O
for	O
SK	O
,	O
f	O
(	O
x	O
j	O
|	O
c	O
(	O
i	O
)	O
,	O
Σ	O
i	O
)	O
for	O
GMM	B-MethodName
/	O
VMFM	B-MethodName
.	O
Our	O
results	O
in	O
6	O
focus	O
on	O
KM	B-MethodName
and	O
GMM	B-MethodName
,	O
as	O
we	O
observe	O
that	O
k	O
-	O
medoids	O
,	O
spherical	O
KM	O
and	O
von	O
Mises	O
-	O
Fisher	O
tend	O
to	O
perform	O
worse	O
than	O
KM	B-MethodName
and	O
GMM	O
(	O
see	O
App	O
.	O
A	O
,	O
App	O
.	O
B	O
)	O
.	O
Note	O
that	O
it	O
is	O
possible	O
to	O
extend	O
this	O
approach	O
to	O
obtain	O
the	O
top	O
topics	O
given	O
a	O
document	O
:	O
compute	O
similarity	O
scores	O
between	O
learned	O
topic	O
cluster	O
centers	O
and	O
all	O
word	O
embeddings	O
from	O
that	O
particular	O
document	O
,	O
and	O
normalize	O
them	O
using	O
softmax	O
to	O
obtain	O
a	O
(	O
non	O
-	O
calibrated	O
)	O
probability	O
distribution	O
.	O
Crucial	O
to	O
our	O
method	O
is	O
the	O
incorporation	O
of	O
corpus	O
statistics	O
on	O
top	O
of	O
vanilla	O
clustering	O
algorithms	O
,	O
which	O
we	O
will	O
describe	O
in	O
the	O
remainder	O
of	O
this	O
section	O
.	O

The	O
complexity	O
of	O
KM	B-MethodName
is	O
O	O
(	O
tknm	O
)	O
,	O
and	O
of	O
GMM	B-MethodName
is	O
O	O
(	O
tknm	O
3	O
)	O
,	O
for	O
t	O
iterations	O
,	O
3	O
k	O
clusters	O
(	O
topics	O
)	O
,	O
n	O
word	O
types	O
(	O
unique	O
vocabulary	O
)	O
,	O
and	O
m	O
embedding	O
dimensions	O
.	O
Weighted	O
variants	O
have	O
a	O
oneoff	O
cost	O
of	O
weight	O
initialization	O
,	O
and	O
contribute	O
a	O
constant	O
factor	O
when	O
recalulculating	O
the	O
centroid	O
during	O
clustering	O
.	O
Reranking	O
has	O
an	O
additional	O
O	O
(	O
n	O
log	O
(	O
n	O
k	O
)	O
)	O
factor	O
,	O
where	O
n	O
k	O
is	O
the	O
average	O
number	O
of	O
elements	O
in	O
a	O
cluster	O
.	O
In	O
contrast	O
,	O
LDA	B-MethodName
via	O
collapsed	O
Gibbs	O
sampling	O
has	O
a	O
complexity	O
of	O
O	O
(	O
tkN	O
)	O
,	O
where	O
N	O
is	O
the	O
number	O
of	O
all	O
tokens	O
,	O
so	O
when	O
N	O
n	O
,	O
clustering	O
methods	O
can	O
potentially	O
achieve	O
better	O
performance	O
-	O
complexity	O
tradeoffs	O
.	O
Note	O
that	O
running	O
ELMo	B-MethodName
and	O
BERT	B-MethodName
over	O
documents	O
also	O
requires	O
iterating	O
over	O
all	O
tokens	O
,	O
but	O
only	O
once	O
,	O
and	O
not	O
for	O
every	O
topic	O
and	O
iteration	O
.	O

For	O
readily	O
available	O
pretrained	O
word	O
embeddings	O
such	O
as	O
word2vec	B-MethodName
,	O
FastText	B-MethodName
,	O
GloVe	B-MethodName
and	O
Spherical	B-MethodName
,	O
the	O
embeddings	O
can	O
be	O
considered	O
as	O
'	O
given	O
'	O
as	O
the	O
practioner	O
does	O
not	O
need	O
to	O
generate	O
these	O
embeddings	O
from	O
scratch	O
.	O
However	O
for	O
contextual	O
embeddings	O
such	O
as	O
ELMo	B-MethodName
and	O
BERT	B-MethodName
,	O
there	O
is	O
additional	O
computational	O
cost	O
in	O
obtaining	O
these	O
embeddings	O
before	O
clustering	O
,	O
which	O
requires	O
passing	O
through	O
RNN	O
and	O
transformer	O
layers	O
respectively	O
.	O
This	O
can	O
be	O
trivially	O
parallelised	O
by	O
batching	O
the	O
context	O
window	O
(	O
usually	O
a	O
sentence	O
)	O
.	O
We	O
use	O
standard	O
pretrained	O
ELMo	B-MethodName
and	O
BERT	B-MethodName
models	O
in	O
our	O
experiments	O
and	O
therefore	O
do	O
not	O
consider	O
the	O
runtime	O
of	O
training	O
these	O
models	O
from	O
scratch	O
.	O

We	O
lowercase	O
tokens	O
,	O
remove	O
stopwords	O
,	O
punctuation	O
and	O
digits	O
,	O
and	O
exclude	O
words	O
that	O
appear	O
in	O
less	O
than	O
5	O
documents	O
and	O
appear	O
in	O
long	O
sentences	O
of	O
more	O
than	O
50	O
words	O
,	O
removing	O
email	O
artifacts	O
and	O
noisy	O
token	O
sequences	O
which	O
are	O
not	O
valid	O
sentences	O
.	O
An	O
analysis	O
on	O
the	O
effect	O
of	O
rare	O
word	O
removal	O
can	O
be	O
found	O
in	O
6.2	O
.	O
For	O
contextualized	O
word	O
embeddings	O
(	O
BERT	B-MethodName
and	O
ELMo	B-MethodName
)	O
,	O
sentences	O
served	O
as	O
the	O
context	O
window	O
to	O
obtain	O
the	O
token	O
representations	O
.	O
Subword	O
representations	O
were	O
averaged	O
for	O
BERT	B-MethodName
,	O
which	O
performs	O
better	O
than	O
just	O
using	O
the	O
first	O
subword	O
.	O

Running	O
LDA	B-MethodName
with	O
MALLET	O
(	O
McCallum	O
,	O
2002	O
)	O
takes	O
a	O
minute	O
,	O
but	O
performs	O
no	O
better	O
than	O
KM	B-MethodName
w	I-MethodName
r	I-MethodName
,	O
which	O
takes	O
little	O
more	O
than	O
10	O
seconds	O
on	O
CPU	O
using	O
sklearn	O
(	O
Pedregosa	O
et	O
al	O
,	O
2011	O
)	O
,	O
and	O
3	O
-	O
4	O
seconds	O
using	O
a	O
simple	O
implementation	O
using	O
JAX	O
(	O
Bradbury	O
et	O
al	O
,	O
2018	O
)	O
on	O
GPU	O
.	O

From	O
Table	O
1	O
,	O
we	O
see	O
that	O
reranking	O
and	O
weighting	O
greatly	O
improves	O
clustering	O
performance	O
across	O
different	O
embeddings	O
.	O
As	O
a	O
first	O
step	O
to	O
uncover	O
why	O
,	O
we	O
investigate	O
how	O
sensitive	O
our	O
methods	O
are	O
to	O
restricting	O
the	O
clustering	O
to	O
only	O
frequently	O
appearing	O
word	O
types	O
.	O
Visualized	O
in	O
Fig	O
.	O
3	O
,	O
we	O
find	O
that	O
as	O
we	O
vary	O
the	O
cutoff	O
term	O
frequency	O
,	O
thus	O
changing	O
the	O
vocabulary	O
size	O
and	O
allowing	O
more	O
rare	O
words	O
on	O
the	O
x	O
-	O
axis	O
,	O
NPMI	B-MetricName
is	O
more	O
affected	O
for	O
the	O
models	O
without	O
reweighting	O
.	O
This	O
suggests	O
that	O
reweighting	O
using	O
term	O
frequency	O
is	O
effective	O
for	O
clustering	O
without	O
the	O
need	O
for	O
ad	O
-	O
hoc	O
restriction	O
of	O
infrequent	O
terms	O
-	O
without	O
it	O
,	O
all	O
combinations	O
perform	O
poorly	O
compared	O
to	O
LDA	O
.	O
In	O
general	O
,	O
GMM	B-MethodName
outperforms	O
KM	B-MethodName
for	O
both	O
weighted	O
and	O
unweighted	O
variants	O
averaged	O
across	O
all	O
embedding	O
methods	O
(	O
p	O
<	O
0.05	O
)	O
.	O
7	O

For	O
KM	B-MethodName
,	O
extracted	O
topics	O
before	O
reranking	O
results	O
in	O
reasonable	O
looking	O
themes	O
,	O
but	O
scores	O
poorly	O
on	O
NPMI	B-MetricName
.	O
Reranking	O
strongly	O
improves	O
KM	B-MethodName
on	O
average	O
(	O
p	O
<	O
0.02	O
)	O
for	O
both	O
Reuters	B-DatasetName
and	O
20NG	B-DatasetName
.	O
Examples	O
before	O
and	O
after	O
reranking	O
are	O
provided	O
in	O
Table	O
2	O
.	O
This	O
indicates	O
that	O
while	O
cluster	O
centers	O
are	O
centered	O
around	O
valid	O
themes	O
,	O
they	O
are	O
surrounded	O
by	O
low	O
frequency	O
word	O
types	O
.	O
We	O
observe	O
that	O
when	O
applying	O
reranking	O
to	O
GMM	B-MethodName
w	I-MethodName
the	O
gains	O
are	O
much	O
less	O
pronounced	O
than	O
KM	B-MethodName
w	I-MethodName
.	O
The	O
top	O
topic	O
words	O
before	O
and	O
after	O
reranking	O
for	O
BERT	B-MethodName
-	I-MethodName
GMM	I-MethodName
w	I-MethodName
have	O
an	O
average	B-MetricName
Jaccard	I-MetricName
similarity	I-MetricName
score	O
of	O
0.910	B-MetricValue
,	O
indicating	O
that	O
the	O
cluster	O
centers	O
learned	O
by	O
weighted	O
GMMs	B-MethodName
are	O
already	O
centered	O
at	O
word	O
types	O
of	O
high	O
frequency	O
in	O
the	O
training	O
corpus	O
.	O

Spherical	B-MethodName
embeddings	I-MethodName
and	O
BERT	B-MethodName
perform	O
consistently	O
well	O
across	O
both	O
datasets	O
.	O
For	O
20NG	B-DatasetName
,	O
KM	B-MethodName
w	I-MethodName
r	I-MethodName
Spherical	I-MethodName
and	O
LDA	B-MethodName
both	O
achieve	O
0.26	B-MetricValue
NPMI	B-MetricName
.	O
For	O
Reuters	B-DatasetName
,	O
GMM	B-MethodName
w	I-MethodName
r	I-MethodName
BERT	I-MethodName
achieves	O
the	O
top	O
NPMI	B-MetricName
score	O
of	O
0.15	B-MetricValue
compared	O
to	O
0.12	B-MetricValue
of	O
LDA	B-MethodName
.	O
Word2vec	B-MethodName
and	O
ELMo	B-MethodName
(	O
using	O
only	O
the	O
last	O
layer	O
8	O
)	O
perform	O
poorly	O
compared	O
to	O
the	O
other	O
embeddings	O
.	O
Fast	B-MethodName
-	I-MethodName
Text	I-MethodName
and	O
GloVe	B-MethodName
can	O
achieve	O
similar	O
performance	O
to	O
BERT	B-MethodName
on	O
20NG	B-DatasetName
but	O
are	O
slightly	O
inferior	O
on	O
Reuters	B-DatasetName
.	O
Training	O
or	O
fine	O
-	O
tuning	O
embeddings	O
on	O
the	O
given	O
data	O
prior	O
to	O
clustering	O
could	O
potentially	O
achieve	O
better	O
performance	O
,	O
but	O
we	O
leave	O
this	O
to	O
future	O
work	O
.	O

We	O
find	O
that	O
our	O
approach	O
yields	O
a	O
greater	O
diversity	O
within	O
topics	O
as	O
compared	O
to	O
LDA	B-MethodName
while	O
achieving	O
comparable	O
coherence	B-MetricName
scores	I-MetricName
(	O
App	O
.	O
D	O
)	O
.	O
Such	O
topics	O
are	O
arguably	O
more	O
valuable	O
for	O
exploratory	O
analysis	O
.	O

We	O
outlined	O
a	O
methodology	O
for	O
clustering	O
word	O
embeddings	O
for	O
unsupervised	B-TaskName
document	I-TaskName
analysis	I-TaskName
,	O
and	O
presented	O
a	O
systematic	O
comparison	O
of	O
various	O
influential	O
embedding	O
methods	O
and	O
clustering	O
algorithms	O
.	O
Our	O
experiments	O
suggest	O
that	O
pretrained	B-MethodName
word	I-MethodName
embeddings	I-MethodName
(	O
both	O
contextualized	O
and	O
non	O
-	O
contextualized	O
)	O
,	O
combined	O
with	O
tf	O
-	O
weighted	O
k	O
-	O
means	O
and	O
tf	O
-	O
based	O
reranking	O
,	O
provide	O
a	O
viable	O
alternative	O
to	O
traditional	O
topic	O
modeling	O
at	O
lower	O
complexity	O
and	O
runtime	O
.	O

We	O
thank	O
Aaron	O
Mueller	O
,	O
Pamela	O
Shapiro	O
,	O
Li	O
Ke	O
,	O
Adam	O
Poliak	O
,	O
Kevin	O
Duh	O
and	O
the	O
anonymous	O
reviewers	O
for	O
their	O
feedback	O
.	O

We	O
present	O
SAPBERT	B-MethodName
,	O
a	O
self	O
-	O
alignment	O
pretraining	O
scheme	O
for	O
learning	O
biomedical	O
entity	O
representations	O
.	O
We	O
highlight	O
the	O
consistent	O
performance	O
boost	O
achieved	O
by	O
SAPBERT	B-MethodName
,	O
obtaining	O
new	O
SOTA	O
in	O
all	O
six	O
widely	O
used	O
MEL	B-TaskName
benchmarking	O
datasets	O
.	O
Strikingly	O
,	O
without	O
any	O
fine	O
-	O
tuning	O
on	O
task	O
-	O
specific	O
labelled	O
data	O
,	O
SAPBERT	B-MethodName
already	O
outperforms	O
the	O
previous	O
supervised	O
SOTA	O
(	O
sophisticated	O
hybrid	O
entity	O
linking	O
systems	O
)	O
on	O
multiple	O
datasets	O
in	O
the	O
scientific	O
language	O
domain	O
.	O
Our	O
work	O
opens	O
new	O
avenues	O
to	O
explore	O
for	O
general	O
domain	O
self	O
-	O
alignment	O
(	O
e.g.	O
by	O
leveraging	O
knowledge	O
graphs	O
such	O
as	O
DBpedia	O
)	O
.	O
We	O
plan	O
to	O
incorporate	O
other	O
types	O
of	O
relations	O
(	O
i.e.	O
,	O
hypernymy	O
and	O
hyponymy	O
)	O
and	O
extend	O
our	O
model	O
to	O
sentence	O
-	O
level	O
representation	O
learning	O
.	O
In	O
particular	O
,	O
our	O
ongoing	O
work	O
using	O
a	O
combination	O
of	O
SAPBERT	B-MethodName
and	O
ADAPTER	O
is	O
a	O
promising	O
direction	O
for	O
tackling	O
sentence	O
-	O
level	O
tasks	O
.	O

NCBI	B-DatasetName
disease	I-DatasetName
(	O
Dogan	O
et	O
al	O
,	O
2014	O
)	O
is	O
a	O
corpus	O
containing	O
793	O
fully	O
annotated	O
PubMed	O
abstracts	O
and	O
6	O
,	O
881	O
mentions	O
.	O
The	O
mentions	O
are	O
mapped	O
into	O
the	O
MEDIC	B-DatasetName
dictionary	O
(	O
Davis	O
et	O
al	O
,	O
2012	O
)	O
.	O
We	O
denote	O
this	O
dataset	O
as	O
"	O
NCBI	O
"	O
in	O
our	O
experiments	O
.	O
BC5CDR	B-DatasetName
(	O
Li	O
et	O
al	O
,	O
2016	O
)	O
consists	O
of	O
1	O
,	O
500	O
PubMed	O
articles	O
with	O
4	O
,	O
409	O
annotated	O
chemicals	O
,	O
5	O
,	O
818	O
diseases	O
and	O
3	O
,	O
116	O
chemical	O
-	O
disease	O
interactions	O
.	O
The	O
disease	O
mentions	O
are	O
mapped	O
into	O
the	O
MEDIC	B-DatasetName
dictionary	O
like	O
the	O
NCBI	O
disease	O
corpus	O
.	O
The	O
chemical	O
mentions	O
are	O
mapped	O
into	O
the	O
Comparative	B-DatasetName
Toxicogenomics	I-DatasetName
Database	I-DatasetName
(	O
CTD	O
)	O
(	O
Davis	O
et	O
al	O
,	O
2019	O
)	O
chemical	O
dictionary	O
.	O
We	O
denote	O
the	O
disease	O
and	O
chemical	O
mention	O
sets	O
as	O
"	O
BC5CDRd	B-DatasetName
"	O
and	O
"	O
BC5CDR	B-DatasetName
-	I-DatasetName
c	I-DatasetName
"	O
respectively	O
.	O
For	O
NCBI	B-DatasetName
and	O
BC5CDR	B-DatasetName
we	O
use	O
the	O
same	O
data	O
and	O
evaluation	O
protocol	O
by	O
Sung	O
et	O
al	O
(	O
2020	O
)	O
.	O
11	O
MedMentions	B-DatasetName
(	O
Mohan	O
and	O
Li	O
,	O
2018	O
)	O
is	O
a	O
verylarge	O
-	O
scale	O
entity	O
linking	O
dataset	O
containing	O
over	O
4	O
,	O
000	O
abstracts	O
and	O
over	O
350	O
,	O
000	O
mentions	O
linked	O
to	O
UMLS	B-DatasetName
2017AA	I-DatasetName
.	O
According	O
to	O
Mohan	O
and	O
Li	O
(	O
2018	O
)	O
,	O
training	O
TAGGERONE	B-MethodName
,	O
a	O
very	O
popular	O
MEL	B-TaskName
system	O
,	O
on	O
a	O
subset	O
of	O
MedMentions	B-DatasetName
require	O
>	O
900	O
GB	O
of	O
RAM	O
.	O
Its	O
massive	O
number	O
of	O
mentions	O
and	O
more	O
importantly	O
the	O
used	O
reference	O
ontology	O
(	O
UMLS	O
2017AA	O
has	O
3M+	O
concepts	O
)	O
make	O
the	O
application	O
of	O
most	O
MEL	B-TaskName
systems	O
infeasible	O
.	O
However	O
,	O
through	O
our	O
metric	O
learning	O
formulation	O
,	O
SAPBERT	B-MethodName
can	O
be	O
applied	O
on	O
MedMentions	B-DatasetName
with	O
minimal	O
effort	O
.	O

AskAPatient	B-DatasetName
(	O
Limsopatham	O
and	O
Collier	O
,	O
2016	O
)	O
includes	O
17	O
,	O
324	O
adverse	O
drug	O
reaction	O
(	O
ADR	O
)	O
annotations	O
collected	O
from	O
askapatient.com	O
blog	O
posts	O
.	O
The	O
mentions	O
are	O
mapped	O
to	O
1	O
,	O
036	O
medical	O
concepts	O
grounded	O
onto	O
SNOMED	B-DatasetName
-	I-DatasetName
CT	I-DatasetName
(	O
Donnelly	O
,	O
2006	O
)	O
and	O
AMT	B-DatasetName
(	O
the	O
Australian	O
Medicines	O
Terminology	O
)	O
.	O
For	O
this	O
dataset	O
,	O
we	O
follow	O
the	O
10	O
-	O
fold	O
evaluation	O
protocol	O
stated	O
in	O
the	O
original	O
paper	O
.	O
12	O
COMETA	B-DatasetName
(	O
Basaldella	O
et	O
al	O
,	O
2020	O
)	O
is	O
a	O
recently	O
released	O
large	O
-	O
scale	O
MEL	O
dataset	O
that	O
specifically	O
focuses	O
on	O
MEL	O
in	O
the	O
social	O
media	O
domain	O
,	O
containing	O
around	O
20k	O
medical	O
mentions	O
extracted	O
from	O
health	O
-	O
related	O
discussions	O
on	O
reddit.com	O
.	O
Mentions	O
are	O
mapped	O
to	O
SNOMED	B-DatasetName
-	I-DatasetName
CT	I-DatasetName
.	O
We	O
use	O
the	O
"	O
stratified	O
(	O
general	O
)	O
"	O
split	O
and	O
follow	O
the	O
evaluation	O
protocol	O
of	O
the	O
original	O
paper	O
.	O
13	O

We	O
list	O
all	O
the	O
versions	O
of	O
BERT	B-MethodName
models	O
used	O
in	O
this	O
study	O
,	O
linking	O
to	O
the	O
specific	O
versions	O
in	O
Tab	O
.	O
5	O
.	O
Note	O
that	O
we	O
exhaustively	O
tried	O
all	O
official	O
variants	O
of	O
the	O
selected	O
models	O
and	O
the	O
best	O
performing	O
ones	O
are	O
chosen	O
.	O
All	O
BERT	B-MethodName
models	O
refer	O
to	O
the	O
BERT	B-MethodName
Base	I-MethodName
architecture	O
in	O
this	O
paper	O
.	O
S	O
denotes	O
the	O
set	O
of	O
all	O
surface	O
forms	O
/	O
synonyms	O
of	O
all	O
concepts	O
in	O
C	O
;	O
M	O
denotes	O
the	O
set	O
of	O
mentions	O
/	O
queries	O
.	O
COMETA	O
(	O
s.g	O
.	O
)	O
and	O
(	O
z.g	O
.	O
)	O
are	O
the	O
stratified	O
(	O
general	O
)	O
and	O
zeroshot	O
(	O
general	O
)	O
split	O
respectively	O
.	O
model	O
NCBI	O
BC5CDR	O
-	O
d	O
BC5CDR	O
-	O
c	O
MedMentions	O
AskAPatient	O
COMETA	O
@1	O
@5	O
@1	O
@5	O
@1	O
@5	O
@1	O
@5	O
@1	O
@5	O
@1	O
@5	O
SIEVE	B-MethodName
-	I-MethodName
BASED	I-MethodName
(	O
D'Souza	O
and	O
Ng	O
,	O
2015	O
)	O
84.7	O
-	O
84.1	O
-	O
90.7	O
-	O
-	O
-	O
WORDCNN	B-MethodName
(	O
Limsopatham	O
and	O
Collier	O
,	O
2016	O
)	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
81.4	O
-	O
-	O
-	O
WORDGRU+TF	B-MethodName
-	I-MethodName
IDF	I-MethodName
(	O
Tutubalina	O
et	O
al	O
,	O
2018	O
)	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
85.7	O
-	O
-	O
-	O
TAGGERONE	B-MethodName
87.7	O
-	O
88.9	O
-	O
94.1	O
-	O
OOM	O
OOM	O
-	O
-	O
-	O
-	O
NORMCO	B-MethodName
(	O
Wright	O
et	O
al	O
,	O
2019	O
)	O
87.8	O
-	O
88.0	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
BNE	B-MethodName
(	O
Phan	O
et	O
al	O
,	O
2019	O
)	O
87.7	O
-	O
90.6	O
-	O
95.8	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
BERTRANK	B-MethodName
(	O
Ji	O
et	O
al	O
,	O
2020	O
)	O
89	O
.	O

We	O
thank	O
the	O
three	O
reviewers	O
and	O
the	O
Area	O
Chair	O
for	O
their	O
insightful	O
comments	O
and	O
suggestions	O
.	O
FL	O
is	O
supported	O
by	O
Grace	O
&	O
Thomas	O
C.H.	O
Chan	O
Cambridge	O
Scholarship	O
.	O
NC	O
and	O
MB	O
would	O
like	O
to	O

Being	O
able	O
to	O
perform	O
in	O
-	O
depth	O
chat	O
with	O
humans	O
in	O
a	O
closed	O
domain	O
is	O
a	O
precondition	O
before	O
an	O
open	O
-	O
domain	O
chatbot	O
can	O
ever	O
be	O
claimed	O
.	O
In	O
this	O
work	O
,	O
we	O
take	O
a	O
close	O
look	O
at	O
the	O
movie	O
domain	O
and	O
present	O
a	O
large	O
-	O
scale	O
high	O
-	O
quality	O
corpus	O
with	O
fine	O
-	O
grained	O
annotations	O
in	O
hope	O
of	O
pushing	O
the	O
limit	O
of	O
moviedomain	O
chatbots	O
.	O
We	O
propose	O
a	O
unified	O
,	O
readily	O
scalable	O
neural	O
approach	O
which	O
reconciles	O
all	O
subtasks	O
like	O
intent	B-TaskName
prediction	I-TaskName
and	O
knowledge	B-TaskName
retrieval	I-TaskName
.	O
The	O
model	O
is	O
first	O
pretrained	O
on	O
the	O
huge	O
general	O
-	O
domain	O
data	O
,	O
then	O
finetuned	O
on	O
our	O
corpus	O
.	O
We	O
show	O
this	O
simple	O
neural	O
approach	O
trained	O
on	O
high	O
-	O
quality	O
data	O
is	O
able	O
to	O
outperform	O
commercial	O
systems	O
replying	O
on	O
complex	O
rules	O
.	O
On	O
both	O
the	O
static	O
and	O
interactive	O
tests	O
,	O
we	O
find	O
responses	O
generated	O
by	O
our	O
system	O
exhibits	O
remarkably	O
good	O
engagement	O
and	O
sensibleness	O
close	O
to	O
human	O
-	O
written	O
ones	O
.	O
We	O
further	O
analyze	O
the	O
limits	O
of	O
our	O
work	O
and	O
point	O
out	O
potential	O
directions	O
for	O
future	O
work	O
1	O
.	O

Language	O
models	O
have	O
demonstrated	O
impressive	O
performance	O
as	O
a	O
universal	O
learner	O
across	O
NLP	O
tasks	O
(	O
Shen	O
et	O
al	O
,	O
2017	O
;	O
Peters	O
et	O
al	O
,	O
2018	O
;	O
Radford	O
et	O
al	O
,	O
2019	O
;	O
Brown	O
et	O
al	O
,	O
2020	O
)	O
.	O
Inspired	O
by	O
this	O
,	O
our	O
dialogue	B-TaskName
generation	I-TaskName
model	O
is	O
implemented	O
as	O
a	O
Transformer	B-MethodName
-	O
based	O
language	O
model	O
like	O
GPT2	O
(	O
Radford	O
et	O
al	O
,	O
2019	O
;	O
.	O
It	O
contains	O
a	O
pipeline	O
process	O
of	O
movie	O
tracker	O
,	O
intent	O
prediction	O
,	O
knowledge	O
retrieval	O
and	O
text	O
gener	O
-	O
7	O
We	O
only	O
consider	O
recommending	O
movies	O
as	O
for	O
the	O
DA	O
about	O
recommendation	O
.	O
Recommending	O
other	O
aspects	O
require	O
assembling	O
recommendation	O
systems	O
of	O
different	O
domains	O
,	O
which	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
paper	O
.	O

The	O
knowledge	O
retrieval	O
component	O
is	O
similar	O
to	O
the	O
classical	O
DSSM	B-MethodName
model	O
(	O
Huang	O
et	O
al	O
,	O
2013	O
)	O
.	O
We	O
replace	O
the	O
MLP	O
with	O
our	O
language	O
model	O
encoder	O
to	O
get	O
the	O
embedding	O
for	O
knowledge	O
.	O
Note	O
that	O
we	O
only	O
select	O
knowledge	O
from	O
the	O
current	O
movie	O
,	O
which	O
can	O
be	O
obtained	O
from	O
the	O
movie	O
tracker	O
,	O
so	O
it	O
is	O
possible	O
to	O
"	O
will	O
be	O
fed	O
to	O
the	O
language	O
model	O
to	O
generate	O
the	O
response	O
.	O
To	O
make	O
it	O
consistent	O
with	O
the	O
pretrained	O
general	O
-	O
domain	O
dialogue	O
,	O
the	O
position	O
embedding	O
of	O
the	O
decoded	O
response	O
will	O
skip	O
the	O
concatenated	O
intent	O
and	O
knowledge	O
and	O
directly	O
follow	O
the	O
dialogue	O
context	O
.	O
We	O
find	O
this	O
beneficial	O
when	O
combined	O
with	O
pretrained	O
models	O
.	O
The	O
objective	O
also	O
follows	O
the	O
pretrained	O
model	O
mixing	O
maximum	O
lilkelihood	O
and	O
unlikelihood	O
training	O
.	O

We	O
present	O
MovieChats	B-MethodName
:	O
a	O
movie	O
-	O
domain	O
chatbot	O
built	O
upon	O
a	O
large	O
-	O
scale	O
,	O
high	O
-	O
quality	O
conversational	O
corpus	O
with	O
fine	O
-	O
grained	O
annotations	O
.	O
The	O
model	O
can	O
be	O
trained	O
end	O
-	O
to	O
-	O
end	O
with	O
a	O
simple	O
unified	O
language	O
model	O
architecture	O
.	O
We	O
show	O
that	O
our	O
model	O
,	O
powered	O
by	O
well	O
-	O
defined	O
knowledge	O
grounding	O
,	O
is	O
able	O
to	O
approach	O
human	O
performance	O
in	O
some	O
perspective	O
,	O
though	O
still	O
lagged	O
behind	O
when	O
it	O
comes	O
to	O
dealing	O
with	O
detailed	O
knowledge	O
or	O
long	O
-	O
turn	O
consistency	O
.	O

Preface	O
:	O
General	O
Chair	O

Mirella	O
Lapata	O
is	O
professor	O
of	O
natural	O
language	O
processing	O
in	O
the	O
School	O
of	O
Informatics	O
at	O
the	O
University	O
of	O
Edinburgh	O
.	O
Her	O
research	O
focuses	O
on	O
getting	O
computers	O
to	O
understand	O
,	O
reason	O
with	O
,	O
and	O
generate	O
.	O
She	O
is	O
as	O
an	O
associate	O
editor	O
of	O
the	O
Journal	O
of	O
Artificial	O
Intelligence	O
Research	O
and	O
has	O
served	O
on	O
the	O
editorial	O
boards	O
of	O
Transactions	O
of	O
the	O
ACL	O
and	O
Computational	O
Linguistics	O
.	O
She	O
was	O
the	O
first	O
recipient	O
of	O
the	O
Karen	O
Sparck	O
Jones	O
award	O
of	O
the	O
British	O
Computer	O
Society	O
,	O
recognizing	O
key	O
contributions	O
to	O
NLP	O
and	O
information	O
retrieval	O
.	O
She	O
received	O
two	O
EMNLP	O
best	O
paper	O
awards	O
and	O
currently	O
holds	O
a	O
prestigious	O
Consolidator	O
Grant	O
from	O
the	O
European	O
Research	O
Council	O
.	O
xxiii	O

Pre	O
-	O
trained	O
language	O
model	O
We	O
use	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
,	O
a	O
Transformer	O
-	O
based	O
language	O
model	O
that	O
is	O
pre	O
-	O
trained	O
on	O
a	O
massive	O
text	O
corpus	O
,	O
following	O
Gururangan	O
et	O
al	O
,	O
2020	O
.	O
RoBERTa	B-MethodName
is	O
an	O
extension	O
of	O
BERT	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
with	O
optimized	O
hyperparameters	O
and	O
a	O
modification	O
of	O
the	O
pretraining	O
objective	O
,	O
which	O
excludes	O
next	O
sentence	O
prediction	O
and	O
only	O
uses	O
the	O
randomly	O
masked	O
tokens	O
in	O
the	O
input	O
sentence	O
.	O
To	O
evaluate	O
the	O
performance	O
of	O
RoBERTa	B-MethodName
on	O
a	O
certain	O
task	O
,	O
a	O
classification	O
layer	O
is	O
appended	O
on	O
top	O
of	O
the	O
language	O
model	O
after	O
the	O
pretraining	O
and	O
all	O
the	O
parameters	O
in	O
RoBERTa	B-MethodName
are	O
trained	O
in	O
a	O
supervised	O
way	O
using	O
the	O
label	O
of	O
the	O
dataset	O
.	O
In	O
this	O
paper	O
,	O
training	O
word	O
representations	O
using	O
RoBERTa	B-MethodName
on	O
a	O
masked	O
language	O
modeling	O
task	O
will	O
be	O
referred	O
to	O
as	O
pretraining	O
.	O
Further	O
,	O
taking	O
this	O
pretrained	O
model	O
and	O
adding	O
a	O
classification	O
layer	O
with	O
additional	O
updates	O
to	O
the	O
language	O
model	O
parameters	O
will	O
be	O
referred	O
to	O
as	O
fine	O
-	O
tuning	O
.	O
Task	O
-	O
adaptive	O
pretraining	O
(	O
TAPT	O
)	O
Although	O
RoBERTa	B-MethodName
achieves	O
strong	O
performance	O
by	O
simply	O
fine	O
-	O
tuning	O
the	O
PLMs	O
on	O
a	O
target	O
task	O
,	O
there	O
can	O
be	O
a	O
distributional	O
mismatch	O
between	O
the	O
pretraining	O
and	O
target	O
corpora	O
.	O
To	O
address	O
this	O
issue	O
,	O
pretraining	O
on	O
the	O
target	O
task	O
or	O
the	O
domain	O
of	O
the	O
target	O
task	O
can	O
be	O
usefully	O
employed	O
to	O
adapt	O
the	O
language	O
models	O
to	O
the	O
target	O
task	O
and	O
it	O
further	O
improves	O
the	O
performance	O
of	O
the	O
PLMs	O
.	O
Such	O
methods	O
can	O
be	O
referred	O
to	O
as	O
Domain	O
-	O
Adaptive	O
Pretraining	O
(	O
DAPT	O
)	O
or	O
Task	O
Adaptive	O
-	O
Pretraining	O
(	O
TAPT	O
)	O
(	O
Gururangan	O
et	O
al	O
,	O
2020	O
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
limit	O
the	O
scope	O
of	O
our	O
works	O
to	O
TAPT	O
as	O
domain	O
text	O
corpus	O
is	O
not	O
always	O
available	O
for	O
each	O
task	O
,	O
whereas	O
TAPT	O
can	O
be	O
easily	O
applied	O
by	O
directly	O
using	O
the	O
dataset	O
of	O
the	O
target	O
task	O
while	O
its	O
performance	O
often	O
matches	O
with	O
DAPT	O
(	O
Gururangan	O
et	O
al	O
,	O
2020	O
)	O
.	O
In	O
TAPT	O
,	O
the	O
second	O
phase	O
of	O
pretraining	O
is	O
per	O
-	O
Figure	O
1	O
:	O
The	O
adapter	O
achitecture	O
in	O
the	O
Transformer	O
layer	O
(	O
Pfeiffer	O
et	O
al	O
,	O
2020a	O
)	O
formed	O
with	O
RoBERTa	B-MethodName
using	O
the	O
unlabeled	O
text	O
corpus	O
of	O
the	O
target	O
task	O
,	O
and	O
then	O
it	O
is	O
fine	O
-	O
tuned	O
on	O
the	O
target	O
task	O
.	O
Adapter	O
Adapter	O
modules	O
have	O
been	O
employed	O
as	O
a	O
feature	O
extractor	O
in	O
computer	O
vision	O
(	O
Rebuffi	O
et	O
al	O
,	O
2017	O
)	O
and	O
have	O
been	O
recently	O
adopted	O
in	O
the	O
NLP	O
literature	O
as	O
an	O
alternative	O
approach	O
to	O
fully	O
fine	O
-	O
tuning	O
PLMs	O
.	O
Adapters	O
are	O
sets	O
of	O
new	O
weights	O
that	O
are	O
typically	O
embedded	O
in	O
each	O
transformer	O
layer	O
of	O
PLMs	O
and	O
consist	O
of	O
feed	O
-	O
forward	O
layers	O
with	O
normalizations	O
,	O
residual	O
connections	O
,	O
and	O
projection	O
layers	O
.	O
The	O
architectures	O
of	O
adapters	O
vary	O
with	O
respect	O
to	O
the	O
different	O
configuration	O
settings	O
.	O
We	O
use	O
the	O
configuration	O
proposed	O
by	O
Pfeiffer	O
et	O
al	O
,	O
2020a	O
in	O
Figure	O
1	O
,	O
which	O
turned	O
out	O
to	O
be	O
effective	O
on	O
diverse	O
NLP	O
tasks	O
,	O
and	O
add	O
the	O
adapter	O
layer	O
to	O
each	O
transformer	O
layer	O
.	O
Pfeiffer	O
et	O
al	O
,	O
2020c	O
use	O
two	O
types	O
of	O
adapter	O
:	O
language	O
-	O
specific	O
adapters	O
and	O
taskspecific	O
adapters	O
for	O
cross	O
-	O
lingual	O
transfer	O
.	O
These	O
two	O
types	O
of	O
adapter	O
modules	O
have	O
similar	O
architecture	O
as	O
in	O
Figure	O
1	O
.	O
However	O
,	O
the	O
language	O
adapters	O
involve	O
invertible	O
adapters	O
after	O
the	O
embedding	O
layer	O
to	O
capture	O
token	O
-	O
level	O
language	O
representation	O
when	O
those	O
are	O
trained	O
via	O
masked	O
language	O
modeling	O
in	O
the	O
pretraining	O
stage	O
,	O
whereas	O
the	O
task	O
adapters	O
are	O
simply	O
embedded	O
in	O
each	O
transformer	O
layer	O
and	O
trained	O
in	O
the	O
fine	O
-	O
tuning	O
stage	O
to	O
learn	O
the	O
task	O
representation	O
.	O
Following	O
Pfeiffer	O
et	O
al	O
,	O
2020c	O
,	O
we	O
employ	O
language	O
adapter	O
modules	O
with	O
invertible	O
adapter	O
layers	O
to	O
perform	O
pretraining	O
adapters	O
on	O
the	O
unlabeled	O
target	O
dataset	O
.	O
However	O
,	O
we	O
perform	O
fine	O
-	O
tuning	O
pre	O
-	O
trained	O
parameters	O
of	O
the	O
language	O
adapter	O
modules	O
for	O
evaluation	O
to	O
align	O
with	O
(	O
Maas	O
et	O
al	O
,	O
2011	O
)	O
)	O
and	O
low	O
-	O
resource	O
(	O
CHEMPROT	O
(	O
Kringelum	O
et	O
al	O
,	O
2016	O
)	O
,	O
ACL	O
-	O
ARC	O
(	O
Jurgens	O
et	O
al	O
,	O
2018	O
)	O
,	O
SCIERC	O
(	O
Luan	O
et	O
al	O
,	O
2018	O
)	O
,	O
HYPERPARTISAN	O
(	O
Kiesel	O
et	O
al	O
,	O
2019	O
)	O
settings	O
.	O
TAPT	O
,	O
whereas	O
Pfeiffer	O
et	O
al	O
,	O
2020c	O
employ	O
both	O
the	O
language	O
and	O
the	O
task	O
adapters	O
by	O
stacking	O
task	O
adapters	O
on	O
top	O
of	O
the	O
language	O
adapters	O
.	O

Comparing	O
CRF	B-MethodName
and	O
LSTM	B-MethodName
performance	O
on	O
the	O
task	O
of	O
morphosyntactic	B-TaskName
tagging	I-TaskName
of	O
non	O
-	O
standard	O
varieties	O
of	O
South	O
Slavic	O
languages	O

This	O
paper	O
presents	O
two	O
systems	O
taking	O
part	O
in	O
the	O
Morphosyntactic	B-TaskName
Tagging	I-TaskName
of	I-TaskName
Tweets	I-TaskName
shared	O
task	O
on	O
Slovene	O
,	O
Croatian	O
and	O
Serbian	O
data	O
,	O
organized	O
inside	O
the	O
VarDial	O
Evaluation	O
Campaign	O
.	O
While	O
one	O
system	O
relies	O
on	O
the	O
traditional	O
method	O
for	O
sequence	O
labeling	O
(	O
conditional	B-MethodName
random	I-MethodName
fields	I-MethodName
)	O
,	O
the	O
other	O
relies	O
on	O
its	O
neural	O
alternative	O
(	O
bidirectional	B-MethodName
long	I-MethodName
short	I-MethodName
-	I-MethodName
term	I-MethodName
memory	I-MethodName
)	O
.	O
We	O
investigate	O
the	O
similarities	O
and	O
differences	O
of	O
these	O
two	O
approaches	O
,	O
showing	O
that	O
both	O
methods	O
yield	O
very	O
good	O
and	O
quite	O
similar	O
results	O
,	O
with	O
the	O
neural	O
model	O
outperforming	O
the	O
traditional	O
one	O
more	O
as	O
the	O
level	O
of	O
non	O
-	O
standardness	O
of	O
the	O
text	O
increases	O
.	O
Through	O
an	O
error	O
analysis	O
we	O
show	O
that	O
the	O
neural	O
system	O
is	O
better	O
at	O
long	O
-	O
range	O
dependencies	O
,	O
while	O
the	O
traditional	O
system	O
excels	O
and	O
slightly	O
outperforms	O
the	O
neural	O
system	O
at	O
the	O
local	O
ones	O
.	O
We	O
present	O
in	O
the	O
paper	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
in	O
morphosyntactic	B-TaskName
annotation	I-TaskName
of	O
non	O
-	O
standard	O
text	O
for	O
Slovene	O
,	O
Croatian	O
and	O
Serbian	O
.	O

We	O
collected	O
tweets	O
using	O
the	O
Twitter	O
streaming	O
API	O
.	O
Similar	O
to	O
prior	O
works	O
(	O
Mohammad	O
et	O
al	O
,	O
2016a	O
;	O
)	O
that	O
target	O
presidential	O
candidates	O
,	O
we	O
focus	O
our	O
attention	O
on	O
three	O
political	O
figures	O
2	O
in	O
the	O
presidential	O
race	O
of	O
2020	O
:	O
"	O
Donald	O
Trump	O
,	O
"	O
"	O
Joe	O
Biden	O
,	O
"	O
and	O
"	O
Bernie	O
Sanders	O
.	O
"	O
We	O
used	O
a	O
set	O
of	O
query	O
hashtags	O
as	O
seeds	O
to	O
collect	O
target	O
-	O
related	O
tweets	O
,	O
which	O
can	O
be	O
categorized	O
as	O
favor	O
hashtags	O
,	O
against	O
hashtags	O
and	O
neutral	O
hashtags	O
(	O
Mohammad	O
et	O
al	O
,	O
2016a	O
)	O
.	O
We	O
show	O
examples	O
of	O
these	O
query	O
hashtags	O
in	O
Table	O
3	O
.	O
In	O
total	O
,	O
we	O
gathered	O
around	O
2.8	O
million	O
tweets	O
for	O
all	O
three	O
targets	O
combined	O
.	O

An	O
Empirical	O
Study	O
of	O
Incorporating	O
Pseudo	O
Data	O
into	O
Grammatical	B-TaskName
Error	I-TaskName
Correction	I-TaskName

In	O
this	O
section	O
,	O
we	O
present	O
our	O
main	O
experimental	O
results	O
,	O
testing	O
the	O
relation	O
embeddings	O
learned	O
by	O
RelBERT	B-MethodName
on	O
analogy	B-TaskName
questions	I-TaskName
(	O
Section	O
5.1	O
)	O
and	O
relation	B-TaskName
classification	I-TaskName
(	O
Section	O
5.2	O
)	O
.	O

In	O
our	O
main	O
experiments	O
,	O
RelBERT	B-MethodName
is	O
trained	O
using	O
the	O
SemEval	B-DatasetName
2012	I-DatasetName
Task	O
2	O
dataset	O
.	O
This	O
dataset	O
contains	O
a	O
broad	O
range	O
of	O
semantic	O
relations	O
,	O
including	O
hypernymy	O
and	O
meronymy	O
relations	O
.	O
This	O
raises	O
an	O
important	O
question	O
:	O
Does	O
RelBERT	B-MethodName
provide	O
us	O
with	O
a	O
way	O
to	O
extract	O
relational	O
knowledge	O
from	O
the	O
parameters	O
of	O
the	O
As	O
a	O
further	O
analysis	O
,	O
Table	O
5	O
shows	O
a	O
breakdown	O
of	O
the	O
Google	O
and	O
BATS	O
analogy	O
results	O
,	O
showing	O
the	O
average	O
performance	O
on	O
each	O
of	O
the	O
top	O
-	O
level	O
categories	O
from	O
these	O
datasets	O
.	O
10	O
While	O
RelBERT	B-MethodName
is	O
outperformed	O
by	O
FastText	B-MethodName
on	O
the	O
morphological	O
relations	O
,	O
it	O
should	O
be	O
noted	O
that	O
the	O
differences	O
are	O
small	O
,	O
while	O
such	O
relations	O
are	O
of	O
a	O
very	O
different	O
nature	O
than	O
those	O
from	O
the	O
SemEval	O
dataset	O
.	O
This	O
confirms	O
that	O
RelBERT	B-MethodName
is	O
able	O
to	O
model	O
a	O
broad	O
range	O
of	O
relations	O
,	O
although	O
it	O
can	O
be	O
expected	O
that	O
better	O
results	O
would	O
be	O
possible	O
by	O
including	O
task	O
-	O
specific	O
training	O
data	O
into	O
the	O
fine	O
-	O
tuning	O
process	O
(	O
e.g.	O
including	O
morphological	O
relations	O
for	O
tasks	O
where	O
such	O
relations	O
matter	O
)	O
.	O

Nearest	O
Neighbors	O
barista	O
:	O
coffee	O
baker	O
:	O
bread	O
,	O
brewer	O
:	O
beer	O
,	O
bartender	O
:	O
cocktail	O
,	O
winemaker	O
:	O
wine	O
,	O
bartender	O
:	O
drink	O
,	O
baker	O
:	O
cake	O
bag	O
:	O
plastic	O
bottle	O
:	O
plastic	O
,	O
bag	O
:	O
leather	O
,	O
container	O
:	O
plastic	O
,	O
box	O
:	O
plastic	O
,	O
jug	O
:	O
glass	O
,	O
bottle	O
:	O
glass	O
duck	O
:	O
duckling	O
chicken	O
:	O
chick	O
,	O
pig	O
:	O
piglet	O
,	O
cat	O
:	O
kitten	O
,	O
ox	O
:	O
calf	O
,	O
butterfly	O
:	O
larvae	O
,	O
bear	O
:	O
cub	O
cooked	O
:	O
raw	O
raw	O
:	O
cooked	O
,	O
regulated	O
:	O
unregulated	O
,	O
sober	O
:	O
drunk	O
,	O
loaded	O
:	O
unloaded	O
,	O
armed	O
:	O
unarmed	O
,	O
published	O
:	O
unpublished	O
chihuahua	O
:	O
dog	O
dachshund	O
:	O
dog	O
,	O
poodle	O
:	O
dog	O
,	O
terrier	O
:	O
dog	O
,	O
chinchilla	O
:	O
rodent	O
,	O
macaque	O
:	O
monkey	O
,	O
dalmatian	O
:	O
dog	O
dog	O
:	O
dogs	O
cat	O
:	O
cats	O
,	O
horse	O
:	O
horses	O
,	O
pig	O
:	O
pigs	O
,	O
rat	O
:	O
rats	O
,	O
wolf	O
:	O
wolves	O
,	O
monkey	O
:	O
monkeys	O
spy	O
:	O
espionage	O
pirate	O
:	O
piracy	O
,	O
robber	O
:	O
robbery	O
,	O
lobbyist	O
:	O
lobbying	O
,	O
scout	O
:	O
scouting	O
,	O
terrorist	O
:	O
terrorism	O
,	O
witch	O
:	O
witchcraft	O

Figure	O
3	O
compares	O
the	O
performance	O
of	O
RelBERT	B-MethodName
with	O
that	O
of	O
the	O
vanilla	O
pre	O
-	O
trained	O
RoBERTa	B-MethodName
model	O
(	O
i.e.	O
when	O
only	O
the	O
prompt	O
is	O
optimized	O
)	O
.	O
As	O
can	O
be	O
seen	O
,	O
the	O
fine	O
-	O
tuning	O
process	O
is	O
critical	O
for	O
achieving	O
good	O
results	O
.	O
In	O
Figure	O
3	O
,	O
we	O
also	O
compare	O
the	O
performance	O
of	O
our	O
main	O
RelBERT	B-MethodName
model	O
,	O
which	O
is	O
based	O
on	O
RoBERTa	B-MethodName
,	O
with	O
versions	O
that	O
were	O
instead	O
initialized	O
with	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
and	O
ALBERT	B-MethodName
(	O
Lan	O
et	O
al	O
,	O
2019	O
)	O
.	O
11	O
RoBERTa	B-MethodName
clearly	O
outperforms	O
the	O
other	O
two	O
LMs	O
,	O
which	O
is	O
in	O
accordance	O
with	O
findings	O
from	O
the	O
literature	O
suggesting	O
that	O
RoBERTa	B-MethodName
captures	O
more	O
semantic	O
knowledge	O
Warstadt	O
et	O
al	O
,	O
2020	O
)	O
.	O

The	O
Transformer	O
network	O
[	O
Vaswani	O
et	O
al	O
,	O
2017	O
]	O
has	O
been	O
used	O
widely	O
in	O
neural	O
machine	O
translation	O
[	O
Tubay	O
and	O
Costa	O
-	O
jussà	O
,	O
2018	O
,	O
Edunov	O
et	O
al	O
,	O
2018	O
,	O
Xia	O
et	O
al	O
,	O
2019	O
,	O
Devlin	O
et	O
al	O
,	O
2019	O
and	O
has	O
proven	O
effective	O
for	O
sentiment	O
analysis	O
and	O
emotion	O
recognition	O
.	O
However	O
,	O
existing	O
architectures	O
are	O
very	O
dense	O
compared	O
to	O
our	O
three	O
lightweight	O
models	O
.	O
The	O
Multimodal	B-MethodName
Transformer	I-MethodName
(	O
MuLT	B-MethodName
)	O
of	O
Tsai	O
et	O
al	O
[	O
2019	O
]	O
modifies	O
the	O
Transformer	O
block	O
to	O
compute	O
cross	O
-	O
modal	O
attention	O
for	O
two	O
modalities	O
at	O
a	O
time	O
.	O
It	O
combines	O
modalities	O
in	O
directed	O
pairs	O
,	O
using	O
a	O
total	O
of	O
six	O
Transformers	O
,	O
whose	O
outputs	O
are	O
then	O
merged	O
into	O
a	O
single	O
multimodal	O
representation	O
.	O
Unlike	O
other	O
works	O
,	O
MuLT	B-MethodName
is	O
able	O
to	O
handle	O
cases	O
where	O
the	O
three	O
modalities	O
are	O
not	O
aligned	O
at	O
the	O
word	O
level	O
;	O
it	O
learns	O
soft	O
alignments	O
via	O
the	O
cross	O
-	O
modal	O
attention	O
weights	O
for	O
each	O
pair	O
of	O
modalities	O
.	O
The	O
model	O
works	O
well	O
in	O
the	O
unaligned	O
case	O
,	O
and	O
in	O
the	O
aligned	O
case	O
,	O
it	O
gives	O
state	O
of	O
the	O
art	O
performance	O
the	O
Happy	O
emotion	O
in	O
IEMO	B-DatasetName
-	I-DatasetName
CAP	I-DatasetName
.	O
The	O
Factorized	B-MethodName
Multimodal	I-MethodName
Transformer	I-MethodName
(	O
FMT	B-MethodName
)	O
of	O
introduces	O
Factorized	O
Multimodal	O
Self	O
-	O
Attention	O
(	O
FSM	O
)	O
modules	O
,	O
which	O
compute	O
self	O
-	O
attention	O
over	O
unimodal	O
,	O
bimodal	O
,	O
and	O
trimodal	O
inputs	O
in	O
parallel	O
.	O
FMT	B-MethodName
gives	O
state	O
of	O
the	O
art	O
performance	O
in	O
the	O
word	O
-	O
aligned	O
case	O
on	O
CMU	B-DatasetName
-	I-DatasetName
MOSI	I-DatasetName
and	O
on	O
the	O
Sad	O
,	O
Angry	O
,	O
and	O
Neutral	O
emotions	O
in	O
IEMOCAP	B-DatasetName
.	O
We	O
use	O
FMT	B-MethodName
,	O
along	O
with	O
the	O
word	O
-	O
aligned	O
version	O
of	O
MuLT	B-MethodName
,	O
as	O
baselines	O
for	O
comparison	O
in	O
our	O
experiments	O
.	O

We	O
do	O
not	O
give	O
an	O
exhaustive	O
list	O
of	O
prior	O
work	O
in	O
multimodal	B-TaskName
sentiment	I-TaskName
analysis	I-TaskName
,	O
but	O
focus	O
on	O
recent	O
neural	O
approaches	O
that	O
achieved	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
at	O
their	O
times	O
of	O
publication	O
.	O

We	O
relied	O
on	O
a	O
pre	O
-	O
trained	O
English	O
version	O
of	O
BERT	B-MethodName
(	O
BERT	B-MethodName
-	I-MethodName
base	I-MethodName
uncased	O
,	O
12	B-HyperparameterValue
layers	B-HyperparameterName
)	O
for	O
the	O
extraction	O
of	O
the	O
contextual	O
word	O
embeddings	O
.	O
To	O
obtain	O
the	O
representations	O
for	O
our	O
sentence	O
-	O
level	O
tasks	O
we	O
experimented	O
the	O
activation	O
of	O
the	O
first	O
input	O
token	O
(	O
[	O
CLS	O
]	O
)	O
1	O
and	O
four	O
different	O
combining	O
methods	O
:	O
Max	O
-	O
pooling	O
,	O
Min	O
-	O
pooling	O
,	O
Mean	O
and	O
Sum	O
.	O
Each	O
of	O
this	O
four	O
combining	O
methods	O
returns	O
a	O
single	O
s	O
vector	O
,	O
such	O
that	O
each	O
s	O
n	O
is	O
obtained	O
by	O
combining	O
the	O
n	O
th	O
components	O
w	O
1n	O
,	O
w	O
2n	O
,	O
...	O
,	O
w	O
mn	O
of	O
the	O
embedding	O
of	O
each	O
word	O
in	O
the	O
input	O
sentence	O
.	O
In	O
order	O
to	O
conduct	O
a	O
comparison	O
of	O
contextbased	O
and	O
word	O
-	O
based	O
representations	O
when	O
solving	O
our	O
set	O
of	O
probing	O
tasks	O
,	O
we	O
performed	O
all	O
the	O
probing	O
experiments	O
using	O
also	O
the	O
embeddings	O
extracted	O
from	O
a	O
pre	O
-	O
trained	O
version	O
of	O
Word2vec	O
.	O
In	O
particular	O
,	O
we	O
trained	O
the	O
model	O
on	O
the	O
English	B-DatasetName
Wikipedia	I-DatasetName
dataset	O
(	O
dump	O
of	O
March	O
2020	O
)	O
,	O
resulting	O
in	O
300	O
-	O
dimensional	O
vectors	O
.	O
In	O
the	O
same	O
manner	O
as	O
BERT	O
's	O
contextual	O
representations	O
,	O
we	O
experimented	O
four	O
combining	O
methods	O
:	O
Max	O
-	O
pooling	O
,	O
Min	O
-	O
pooling	O
,	O
Mean	O
and	O
Sum	O
.	O
We	O
used	O
a	O
linear	O
Support	B-MethodName
Vector	I-MethodName
Regression	I-MethodName
model	I-MethodName
(	O
LinearSVR	O
)	O
as	O
probing	O
model	O
.	O

First	O
,	O
we	O
removed	O
all	O
the	O
punctuations	O
,	O
numbers	O
,	O
links	O
and	O
stop	O
words	O
.	O
We	O
have	O
used	O
lemmatization	O
for	O
grouping	O
together	O
the	O
different	O
forms	O
of	O
a	O
word	O
into	O
a	O
single	O
word	O
.	O
NLTK	O
wordnet	O
(	O
Loper	O
and	O
Bird	O
,	O
2002	O
)	O
is	O
used	O
for	O
lemmatization	O
.	O

Multivalent	B-MethodName
Entailment	I-MethodName
Graphs	I-MethodName
for	O
Question	B-TaskName
Answering	I-TaskName

The	O
task	O
of	O
recognizing	B-TaskName
textual	I-TaskName
entailment	I-TaskName
(	O
Dagan	O
et	O
al	O
,	O
2006	O
)	O
requires	O
models	O
to	O
predict	O
a	O
relation	O
between	O
a	O
text	O
T	O
and	O
hypothesis	O
H	O
;	O
"	O
T	O
entails	O
H	O
if	O
,	O
typically	O
,	O
a	O
human	O
reading	O
T	O
would	O
infer	O
that	O
H	O
is	O
most	O
likely	O
true	O
.	O
"	O
From	O
here	O
,	O
research	O
has	O
moved	O
in	O
several	O
directions	O
.	O
We	O
study	O
predicates	O
,	O
including	O
verbs	O
and	O
phrases	O
that	O
apply	O
to	O
arguments	O
.	O
Research	O
in	O
predicate	O
entailment	O
graphs	O
has	O
evolved	O
from	O
"	O
local	O
"	O
learning	O
of	O
entailment	O
rules	O
(	O
Geffet	O
and	O
Dagan	O
,	O
2005	O
;	O
Szpektor	O
and	O
Dagan	O
,	O
2008	O
)	O
to	O
later	O
work	O
on	O
joint	O
learning	O
of	O
"	O
globalized	O
"	O
rules	O
,	O
overcoming	O
sparsity	O
in	O
local	O
graphs	O
(	O
Berant	O
et	O
al	O
,	O
2010	O
;	O
Hosseini	O
et	O
al	O
,	O
2018	O
)	O
.	O
These	O
graphs	O
frequently	O
rely	O
on	O
the	O
DIH	O
for	O
the	O
local	O
learning	O
step	O
to	O
learn	O
initial	O
predicate	O
entailments	O
.	O
The	O
DIH	O
states	O
that	O
for	O
some	O
predicates	O
p	O
and	O
q	O
,	O
if	O
the	O
contextual	O
features	O
of	O
p	O
are	O
included	O
in	O
those	O
of	O
q	O
,	O
then	O
p	O
entails	O
q	O
(	O
Geffet	O
and	O
Dagan	O
,	O
2005	O
)	O
.	O
In	O
previous	O
work	O
predicate	O
arguments	O
are	O
successfully	O
used	O
as	O
these	O
contextual	O
features	O
,	O
but	O
only	O
predicates	O
of	O
the	O
same	O
valency	O
are	O
considered	O
(	O
e.g.	O
binary	O
predicates	O
entail	O
binary	O
;	O
unary	O
entail	O
unary	O
)	O
,	O
and	O
further	O
research	O
computes	O
additional	O
edges	O
in	O
these	O
same	O
-	O
valency	O
graphs	O
such	O
as	O
with	O
link	O
prediction	O
(	O
Hosseini	O
et	O
al	O
,	O
2019	O
)	O
.	O
However	O
,	O
this	O
leaves	O
out	O
crucial	O
inferences	O
that	O
cross	O
valencies	O
such	O
as	O
the	O
kill	O
/	O
die	O
example	O
,	O
which	O
are	O
easy	O
for	O
humans	O
.	O
We	O
generalize	O
the	O
DIH	O
to	O
learn	O
entailments	O
within	O
and	O
across	O
valencies	O
.	O
Typing	O
is	O
very	O
helpful	O
for	O
entailment	O
graph	O
learning	O
(	O
Berant	O
et	O
al	O
,	O
2010	O
;	O
Lewis	O
and	O
Steedman	O
,	O
2013	O
;	O
Hosseini	O
et	O
al	O
,	O
2018	O
)	O
.	O
Inducing	O
a	O
type	O
for	O
each	O
entity	O
such	O
as	O
"	O
person	O
,	O
"	O
"	O
location	O
,	O
"	O
etc	O
.	O
enables	O
generalized	O
learning	O
across	O
instances	O
and	O
disambiguates	O
word	O
sense	O
,	O
e.g.	O
"	O
running	O
a	O
company	O
"	O
has	O
different	O
entailments	O
than	O
"	O
running	O
code	O
.	O
"	O
We	O
compare	O
our	O
model	O
to	O
several	O
baselines	O
,	O
including	O
strong	O
pretrained	O
language	O
models	O
in	O
an	O
unsupervised	O
setting	O
using	O
similarity	O
.	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
generates	O
impressive	O
word	O
representations	O
,	O
even	O
unsupervised	O
(	O
Petroni	O
et	O
al	O
,	O
2019	O
)	O
,	O
which	O
we	O
compare	O
with	O
on	O
a	O
task	O
of	O
predicate	O
inference	O
.	O
We	O
further	O
test	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
to	O
show	O
the	O
impact	O
of	O
robust	O
in	O
-	O
domain	O
pretraining	O
on	O
the	O
same	O
architecture	O
.	O
These	O
non	O
-	O
directional	O
similarity	O
models	O
provide	O
a	O
strong	O
baseline	O
for	O
evaluating	O
directional	O
entailment	O
graphs	O
.	O

We	O
sample	O
300	O
false	O
positives	O
(	O
100	O
for	O
each	O
model	O
)	O
and	O
report	O
analyses	O
in	O
Table	O
4	O
.	O
In	O
all	O
models	O
spurious	O
entailments	O
are	O
the	O
largest	O
issue	O
,	O
and	O
may	O
occur	O
due	O
to	O
normalization	O
of	O
predicates	O
during	O
learning	O
,	O
or	O
incidental	O
correlations	O
in	O
the	O
data	O
.	O
The	O
UU	B-MethodName
and	O
BU	B-MethodName
models	O
also	O
suffer	O
during	O
relation	O
extraction	O
(	O
parsing	O
)	O
.	O
When	O
we	O
fail	O
to	O
parse	O
a	O
second	O
argument	O
for	O
a	O
predicate	O
we	O
assume	O
it	O
only	O
has	O
one	O
and	O
extract	O
a	O
malformed	O
unary	O
,	O
which	O
can	O
interfere	O
with	O
question	B-TaskName
answering	I-TaskName
(	O
e.g.	O
reporting	O
verbs	O
"	O
explain	O
,	O
"	O
"	O
announce	O
,	O
"	O
etc	O
.	O
which	O
fail	O
to	O
parse	O
with	O
their	O
quote	O
)	O
.	O
We	O
also	O
find	O
relatively	O
few	O
poorly	O
generated	O
negatives	O
,	O
which	O
are	O
actually	O
true	O
given	O
the	O
text	O
.	O
In	O
these	O
cases	O
the	O
model	O
finds	O
an	O
entailment	O
which	O
the	O
authors	O
judge	O
to	O
be	O
correct	O
.	O

MLEC	B-DatasetName
-	I-DatasetName
QA	I-DatasetName
:	O
A	O
Chinese	O
Multi	O
-	O
Choice	O
Biomedical	B-TaskName
Question	I-TaskName
Answering	I-TaskName
Dataset	O

This	O
work	O
was	O
supported	O
in	O
part	O
by	O
ERC	O
H2020	O
Advanced	O
Fellowship	O
GA	O
742137	O
SEMANTAX	O
,	O

Multi	B-TaskName
-	I-TaskName
source	I-TaskName
translation	I-TaskName
consists	O
in	O
exploiting	O
multiple	O
text	O
inputs	O
to	O
improve	O
NMT	B-TaskName
(	O
Zoph	O
and	O
Knight	O
,	O
2016	O
)	O
.	O
In	O
our	O
case	O
,	O
we	O
are	O
using	O
this	O
approach	O
in	O
the	O
Transformer	B-MethodName
architecture	O
described	O
above	O
and	O
using	O
only	O
inputs	O
from	O
the	O
same	O
language	O
family	O
.	O

The	O
experimental	O
framework	O
is	O
the	O
Biomedical	B-TaskName
Translation	I-TaskName
Task	I-TaskName
(	O
WMT18	B-DatasetName
)	O
2	O
.	O
The	O
corpus	O
used	O
to	O
train	O
the	O
model	O
are	O
the	O
one	O
provided	O
for	O
the	O
task	O
for	O
the	O
selected	O
languages	O
pairs	O
:	O
Spanishto	O
-	O
English	O
(	O
es2en	O
)	O
,	O
French	O
-	O
to	O
-	O
English	O
(	O
fr2en	O
)	O
and	O
Portuguese	O
-	O
to	O
-	O
English	O
(	O
pt2en	O
)	O
.	O
Sources	O
are	O
mainly	O
from	O
Scielo	O
and	O
Medline	O
and	O
detailed	O
in	O
Table	O
3	O
.	O
Validation	O
sets	O
were	O
taken	O
from	O
Khresmoi	O
development	O
data	O
3	O
,	O
as	O
recommended	O
in	O
the	O
task	O
description	O
.	O
Each	O
validation	O
dataset	O
contains	O
500	O
sentence	O
pairs	O
.	O
Test	O
sets	O
were	O
the	O
ones	O
provides	O
by	O
the	O
task	O
for	O
the	O
previous	O
year	O
competition	O
(	O
WMT17	B-DatasetName
4	O
)	O
.	O
Preprocessing	O
relied	O
on	O
three	O
basic	O
steps	O
:	O
tokenization	O
,	O
truecasing	O
and	O
limiting	O
sentence	O
length	O
to	O
80	O
words	O
.	O
Words	O
were	O
segmented	O
by	O
means	O
of	O
Byte	O
-	O
Pair	O
Encoding	O
(	O
BPE	O
)	O
(	O
Sennrich	O
et	O
al	O
,	O
2015	O
)	O
.	O

CODEX	B-DatasetName
:	O
A	O
Comprehensive	O
Knowledge	B-TaskName
Graph	I-TaskName
Completion	I-TaskName
Benchmark	O

We	O
first	O
compare	O
the	O
content	O
in	O
CODEX	B-DatasetName
-	I-DatasetName
M	I-DatasetName
,	O
which	O
is	O
extracted	O
from	O
Wikidata	O
,	O
with	O
that	O
of	O
FB15	B-DatasetName
K	I-DatasetName
-	I-DatasetName
237	I-DatasetName
,	O
which	O
is	O
extracted	O
from	O
Freebase	O
.	O
For	O
brevity	O
,	O
Figure	O
2	O
compares	O
the	O
top	O
-	O
15	O
relations	O
by	O
mention	O
count	O
in	O
the	O
two	O
datasets	O
.	O
Appendix	O
E	O
provides	O
more	O
content	O
comparisons	O
.	O
Diversity	O
The	O
most	O
common	O
relation	O
in	O
CODEX	B-DatasetName
-	I-DatasetName
M	I-DatasetName
is	O
occupation	O
,	O
which	O
is	O
because	O
most	O
people	O
on	O
Wikidata	O
have	O
multiple	O
occupations	O
listed	O
.	O
By	O
contrast	O
,	O
the	O
frequent	O
relations	O
in	O
FB15	B-DatasetName
K	I-DatasetName
-	I-DatasetName
237	I-DatasetName
are	O
mostly	O
related	O
to	O
awards	O
and	O
film	O
.	O
In	O
fact	O
,	O
over	O
25	O
%	O
of	O
all	O
triples	O
in	O
FB15	B-DatasetName
K	I-DatasetName
-	I-DatasetName
237	I-DatasetName
belong	O
to	O
the	O
/award	O
relation	O
domain	O
,	O
suggesting	O
that	O
CODEX	B-DatasetName
covers	O
a	O
more	O
diverse	O
selection	O
of	O
content	O
.	O
Interpretability	O
The	O
Freebase	B-DatasetName
-	O
style	O
relations	O
are	O
also	O
arguably	O
less	O
interpretable	O
than	O
those	O
in	O
Wikidata	B-DatasetName
.	O
Whereas	O
Wikidata	B-DatasetName
relations	O
have	O
concise	O
natural	O
language	O
labels	O
,	O
the	O
Freebase	B-DatasetName
relation	O
labels	O
are	O
hierarchical	O
,	O
often	O
at	O
five	O
or	O
six	O
levels	O
of	O
hierarchy	O
(	O
Figure	O
2	O
)	O
.	O
Moreover	O
,	O
all	O
relations	O
in	O
Wikidata	B-DatasetName
are	O
binary	O
,	O
whereas	O
some	O
Freebase	B-DatasetName
relations	O
are	O
n	O
-	O
nary	O
(	O
Tanon	O
et	O
al	O
,	O
2016	O
)	O
,	O
meaning	O
that	O
they	O
connect	O
more	O
than	O
two	O
entities	O
.	O
The	O
relations	O
containing	O
a	O
dot	O
(	O
"	O
.	O
"	O
)	O
are	O
such	O
n	O
-	O
nary	O
relations	O
,	O
and	O
are	O
difficult	O
to	O
reason	O
about	O
without	O
understanding	O
the	O
structure	O
of	O
Freebase	B-DatasetName
,	O
which	O
has	O
been	O
deprecated	O
.	O
We	O
further	O
discuss	O
the	O
impact	O
of	O
such	O
n	O
-	O
nary	O
relations	O
for	O
link	O
prediction	O
in	O
the	O
following	O
section	O
.	O

We	O
present	O
CODEX	B-DatasetName
,	O
a	O
set	O
of	O
knowledge	O
graph	O
COmpletion	O
Datasets	O
EXtracted	O
from	O
Wikidata	O
and	O
Wikipedia	O
,	O
and	O
show	O
that	O
CODEX	O
is	O
suitable	O
for	O
multiple	O
KGC	B-TaskName
tasks	O
.	O
We	O
release	O
data	O
,	O
code	O
,	O
and	O
pretrained	O
models	O
for	O
use	O
by	O
the	O
community	O
at	O
https://bit.ly/2EPbrJs	O
.	O
Some	O
promising	O
future	O
directions	O
on	O
CODEX	B-DatasetName
include	O
:	O
Better	O
model	O
understanding	O
CODEX	B-DatasetName
can	O
be	O
used	O
to	O
analyze	O
the	O
impact	O
of	O
hyperparameters	O
,	O
training	O
strategies	O
,	O
and	O
model	O
architectures	O
in	O
KGC	B-TaskName
tasks	O
.	O
Revival	O
of	O
triple	O
classification	O
We	O
encourage	O
the	O
use	O
of	O
triple	B-TaskName
classification	I-TaskName
on	O
CODEX	B-DatasetName
in	O
addition	O
to	O
link	O
prediction	O
because	O
it	O
directly	O
tests	O
discriminative	O
power	O
.	O
Fusing	O
text	O
and	O
structure	O
Including	O
text	O
in	O
both	O
the	O
link	B-TaskName
prediction	I-TaskName
and	O
triple	B-TaskName
classification	I-TaskName
tasks	O
should	O
substantially	O
improve	O
performance	O
.	O
Furthermore	O
,	O
text	O
can	O
be	O
used	O
for	O
few	O
-	O
shot	O
link	O
prediction	O
,	O
an	O
emerging	O
research	O
direction	O
(	O
Xiong	O
et	O
al	O
,	O
2017	O
;	O
Shi	O
and	O
Weninger	O
,	O
2017	O
)	O
.	O
Overall	O
,	O
we	O
hope	O
that	O
CODEX	B-DatasetName
will	O
provide	O
a	O
boost	O
to	O
research	O
in	O
KGC	B-TaskName
,	O
which	O
will	O
in	O
turn	O
impact	O
many	O
other	O
fields	O
of	O
artificial	O
intelligence	O
.	O

In	O
this	O
work	O
,	O
we	O
explored	O
the	O
potential	O
of	O
BERT	B-MethodName
in	O
various	O
text	O
generation	O
tasks	O
under	O
the	O
NAG	B-TaskName
framework	O
.	O
To	O
address	O
problems	O
from	O
NAG	B-TaskName
models	O
previously	O
having	O
a	O
prefixed	O
output	O
length	O
,	O
we	O
devised	O
a	O
decoding	O
mechanism	O
which	O
enables	O
the	O
model	O
to	O
determine	O
the	O
output	O
length	O
dynamically	O
.	O
To	O
reduce	O
errors	O
stemming	O
from	O
the	O
assumption	O
of	O
conditional	O
independence	O
of	O
output	O
tokens	O
,	O
we	O
proposed	O
a	O
context	O
-	O
aware	O
objective	O
as	O
well	O
as	O
using	O
a	O
CRF	O
decoding	O
.	O
Furthermore	O
,	O
to	O
maximize	O
the	O
inference	O
speed	O
advantage	O
of	O
our	O
model	O
,	O
we	O
introduced	O
a	O
ratio	O
-	O
first	O
decoding	O
strategy	O
.	O
We	O
evaluated	O
our	O
model	O
on	O
three	O
benchmark	O
datasets	O
and	O
the	O
results	O
show	O
that	O
our	O
model	O
significantly	O
outperforms	O
many	O
strong	O
NAG	B-TaskName
baselines	O
and	O
performs	O
comparably	O
to	O
many	O
strong	O
AG	O
models	O
.	O

We	O
train	O
our	O
models	O
using	O
the	O
open	O
data	O
sets	O
from	O
CoNLL	B-DatasetName
,	O
Twitter	B-DatasetName
and	O
OntoNotes	B-DatasetName
.	O
The	O
training	O
,	O
development	O
and	O
test	O
splits	O
of	O
CoNLL	B-DatasetName
and	O
OntoNotes	B-DatasetName
follows	O
the	O
standard	O
splits	O
.	O
Similarly	O
,	O
we	O
randomly	O
split	O
the	O
Twitter	B-DatasetName
data	O
set	O
randomly	O
into	O
70	O
%	O
for	O
training	O
,	O
10	O
%	O
for	O
development	O
and	O
20	O
%	O
for	O
testing	O
.	O
The	O
final	O
train	O
,	O
dev	O
and	O
test	O
sets	O
are	O
obtained	O
by	O
joining	O
all	O
the	O
respective	O
splits	O
across	O
the	O
individual	O
data	O
sets	O
.	O

For	O
subtask	O
3	O
,	O
we	O
modeled	O
the	O
problem	O
as	O
a	O
multilabel	B-TaskName
classification	I-TaskName
task	I-TaskName
of	O
the	O
meme	O
text	O
and	O
image	O
content	O
.	O
We	O
used	O
a	O
parallel	O
channel	O
model	O
of	O
text	O
and	O
image	O
channels	O
,	O
and	O
then	O
concatenated	O
the	O
text	O
and	O
image	O
features	O
extracted	O
by	O
the	O
two	O
parallel	O
channels	O
to	O
apply	O
multi	O
-	O
label	O
meme	O
classification	O
.	O
The	O
architecture	O
of	O
the	O
proposed	O
model	O
is	O
shown	O
in	O
Figure	O
4	O
.	O
Text	O
Channel	O
.	O
In	O
the	O
text	O
channel	O
,	O
we	O
used	O
the	O
ALBERT	B-MethodName
-	I-MethodName
Text	I-MethodName
-	I-MethodName
CNN	I-MethodName
model	O
used	O
in	O
subtask	O
1	O
,	O
taking	O
the	O
text	O
part	O
of	O
the	O
meme	O
content	O
as	O
an	O
input	O
to	O
obtain	O
a	O
768	O
-	O
dimensional	O
text	O
feature	O
vector	O
as	O
the	O
output	O
.	O
Image	O
Channel	O
.	O
In	O
the	O
image	O
channel	O
,	O
we	O
used	O
ResNet	B-MethodName
and	O
VGGNet	B-MethodName
,	O
taking	O
the	O
image	O
part	O
of	O
the	O
meme	O
content	O
as	O
input	O
to	O
obtain	O
a	O
512	O
-	O
dimensional	O
image	O
feature	O
vector	O
as	O
the	O
output	O
.	O
The	O
ResNet	B-MethodName
model	O
(	O
He	O
et	O
al	O
,	O
2016	O
)	O
is	O
a	O
deep	O
residual	O
learning	O
model	O
for	O
image	O
recognition	O
,	O
and	O
presents	O
the	O
interlayer	O
residual	O
jump	O
connection	O
and	O
solves	O
the	O
deep	O
vanishing	O
gradient	O
problem	O
.	O
VGGNet	B-MethodName
(	O
Simonyan	O
and	O
Zisserman	O
,	O
2015	O
)	O
is	O
a	O
deep	O
convolutional	O
neural	O
network	O
with	O
small	O
-	O
sized	O
convolutional	O
kernels	O
and	O
a	O
regular	O
network	O
structure	O
,	O
in	O
which	O
the	O
size	B-HyperparameterName
of	I-HyperparameterName
the	I-HyperparameterName
convolution	I-HyperparameterName
kernels	I-HyperparameterName
used	O
in	O
VGG16	B-MethodName
in	O
our	O
experiment	O
is	O
3	B-HyperparameterValue
×	I-HyperparameterValue
3	I-HyperparameterValue
,	O
and	O
the	O
pooling	O
kernels	O
is	O
2	B-HyperparameterValue
×	I-HyperparameterValue
2	I-HyperparameterValue
.	O
Furthermore	O
,	O
only	O
the	O
structures	O
of	O
the	O
ResNet	B-MethodName
and	O
VGGNet	B-MethodName
were	O
used	O
in	O
our	O
experiment	O
,	O
and	O
the	O
pre	O
-	O
training	O
weights	O
were	O
not	O
applied	O
.	O

Subtask	O
2	O
was	O
a	O
multi	B-TaskName
-	I-TaskName
label	I-TaskName
sequence	I-TaskName
-	I-TaskName
labeling	I-TaskName
task	O
.	O
We	O
built	O
the	O
model	O
by	O
converting	O
the	O
problem	O
to	O
detect	O
the	O
coverage	O
of	O
each	O
propagation	O
technique	O
separately	O
for	O
the	O
input	O
sequence	O
,	O
and	O
built	O
a	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
sequence	I-TaskName
labeling	I-TaskName
model	O
based	O
on	O
a	O
fine	O
-	O
tuning	O
of	O
BERT	B-MethodName
.	O
As	O
illustrated	O
in	O
Figure	O
3	O
,	O
the	O
input	O
sequence	O
was	O
first	O
obtained	O
using	O
the	O
pre	O
-	O
trained	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
model	O
with	O
a	O
hidden	O
representation	O
matrix	O
with	O
dimensions	B-HyperparameterName
of	O
512	B-HyperparameterValue
×	O
768	B-HyperparameterValue
.	O
Subsequently	O
,	O
20	B-HyperparameterValue
parallel	O
fully	B-HyperparameterName
connected	I-HyperparameterName
layers	I-HyperparameterName
were	O
input	O
separately	O
for	O
the	O
detection	O
of	O
each	O
propaganda	O
technique	O
coverage	O
span	O
(	O
For	O
each	O
propagation	O
technique	O
,	O
the	O
sequence	O
labeling	O
task	O
is	O
performed	O
separately	O
for	O
the	O
input	O
text	O
)	O
.	O
For	O
each	O
technique	O
,	O
the	O
intermediate	O
result	O
of	O
each	O
parallel	O
channel	O
output	O
is	O
a	O
512	O
×	O
41	O
matrix	O
,	O
and	O
the	O
ensemble	O
layer	O
represents	O
the	O
stacking	O
of	O
20	O
matrices	O
from	O
20	O
parallel	O
channels	O
,	O
the	O
dimensions	O
of	O
the	O
final	O
output	O
were	O
20	O
×	O
512	O
×	O
41	O
,	O
which	O
denote	O
the	O
propaganda	O
technique	O
category	O
,	O
maximum	O
sentence	O
length	O
,	O
and	O
code	O
corresponding	O
to	O
each	O
technique	O
,	O
respectively	O
.	O

In	O
recent	O
years	O
,	O
memes	O
combining	O
image	O
and	O
text	O
have	O
been	O
widely	O
used	O
in	O
social	O
media	O
,	O
and	O
memes	O
are	O
one	O
of	O
the	O
most	O
popular	O
types	O
of	O
content	O
used	O
in	O
online	O
disinformation	O
campaigns	O
.	O
In	O
this	O
paper	O
,	O
our	O
study	O
on	O
the	O
detection	O
of	O
persuasion	O
techniques	O
in	O
texts	O
and	O
images	O
in	O
SemEval	O
-	O
2021	O
Task	O
6	O
is	O
summarized	O
.	O
For	O
propaganda	O
technology	O
detection	O
in	O
text	O
,	O
we	O
propose	O
a	O
combination	O
model	O
of	O
both	O
AL	B-MethodName
-	I-MethodName
BERT	I-MethodName
and	O
Text	B-MethodName
-	I-MethodName
CNN	I-MethodName
for	O
text	B-TaskName
classification	I-TaskName
,	O
as	O
well	O
as	O
a	O
BERT	B-MethodName
-	O
based	O
multi	O
-	O
task	O
sequence	O
labeling	O
model	O
for	O
propaganda	O
technology	O
coverage	O
span	B-TaskName
detection	I-TaskName
.	O
For	O
the	O
meme	B-TaskName
classification	I-TaskName
task	O
involved	O
in	O
text	O
understanding	O
and	O
visual	O
feature	O
extraction	O
,	O
we	O
designed	O
a	O
parallel	O
channel	O
model	O
divided	O
into	O
text	O
and	O
image	O
channels	O
.	O
Our	O
method	O
1	O
achieved	O
a	O
good	O
performance	O
on	O
subtasks	O
1	O
and	O
3	O
.	O
The	O
micro	B-MetricName
F	I-MetricName
1scores	I-MetricName
of	O
0.492	B-MetricValue
,	O
0.091	B-MetricValue
,	O
and	O
0.446	B-MetricValue
achieved	O
on	O
the	O
test	O
sets	O
of	O
the	O
three	O
subtasks	O
ranked	O
12th	O
,	O
7th	O
,	O
and	O
11th	O
,	O
respectively	O
,	O
and	O
all	O
are	O
higher	O
than	O
the	O
baseline	O
model	O
.	O

YNU	O
-	O
HPCC	O
at	O
SemEval	O
-	O
2021	O
Task	O
6	O
:	O
Combining	O
ALBERT	B-MethodName
and	O
Text	B-MethodName
-	I-MethodName
CNN	I-MethodName
for	O
Persuasion	B-TaskName
Detection	I-TaskName
in	O
Texts	O
and	O
Images	O

Current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
machine	B-TaskName
translation	I-TaskName
systems	O
are	O
based	O
on	O
encoder	O
-	O
decoder	O
architectures	O
,	O
that	O
first	O
encode	O
the	O
input	O
sequence	O
,	O
and	O
then	O
generate	O
an	O
output	O
sequence	O
based	O
on	O
the	O
input	O
encoding	O
.	O
Both	O
are	O
interfaced	O
with	O
an	O
attention	O
mechanism	O
that	O
recombines	O
a	O
fixed	O
encoding	O
of	O
the	O
source	O
tokens	O
based	O
on	O
the	O
decoder	O
state	O
.	O
We	O
propose	O
an	O
alternative	O
approach	O
which	O
instead	O
relies	O
on	O
a	O
single	O
2D	B-MethodName
convolutional	I-MethodName
neural	I-MethodName
network	I-MethodName
across	O
both	O
sequences	O
.	O
Each	O
layer	O
of	O
our	O
network	O
recodes	O
source	O
tokens	O
on	O
the	O
basis	O
of	O
the	O
output	O
sequence	O
produced	O
so	O
far	O
.	O
Attention	O
-	O
like	O
properties	O
are	O
therefore	O
pervasive	O
throughout	O
the	O
network	O
.	O
Our	O
model	O
yields	O
excellent	O
results	O
,	O
outperforming	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
encoderdecoder	O
systems	O
,	O
while	O
being	O
conceptually	O
simpler	O
and	O
having	O
fewer	O
parameters	O
.	O

To	O
understand	O
why	O
our	O
proposed	O
VAT	B-MethodName
model	O
is	O
more	O
effective	O
than	O
the	O
standard	O
attention	O
-	O
based	O
model	O
,	O
we	O
visualize	O
two	O
examples	O
of	O
LSTM	B-MethodName
-	O
based	O
models	O
using	O
attention	O
heatmaps	O
(	O
Figure	O
7	O
)	O
.	O
First	O
,	O
the	O
standard	O
attention	O
-	O
based	O
LSTM	B-MethodName
model	O
focuses	O
on	O
the	O
wrong	O
words	O
(	O
e.g.	O
,	O
"	O
this	O
"	O
,	O
"	O
work	O
"	O
)	O
even	O
though	O
it	O
predicts	O
the	O
right	O
sentiment	O
while	O
our	O
VAT	B-MethodName
model	O
finds	O
the	O
correct	O
words	O
(	O
e.g.	O
,	O
"	O
admired	O
"	O
,	O
"	O
lot	O
"	O
)	O
.	O
It	O
indicates	O
integrating	O
IB	O
into	O
attention	O
can	O
help	O
it	O
focus	O
on	O
the	O
key	O
words	O
and	O
reduce	O
the	O
noisy	O
information	O
.	O
Second	O
,	O
our	O
proposed	O
model	O
can	O
also	O
improve	O
the	O
attention	O
's	O
performance	O
by	O
capturing	O
the	O
critical	O
words	O
accurately	O
.	O
For	O
example	O
,	O
in	O
the	O
sentence	O
"	O
That	O
sucks	O
if	O
you	O
have	O
to	O
take	O
the	O
sats	O
tomorrow	O
.	O
"	O
,	O
our	O
model	O
predicts	O
the	O
right	O
class	O
label	O
by	O
attending	O
the	O
words	O
"	O
sucks	O
"	O
and	O
"	O
have	O
to	O
.	O
"	O

For	O
LSTM	B-MethodName
-	O
based	O
models	O
,	O
we	O
use	O
GloVe	O
embedding	O
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
with	O
300	O
-	O
dimension	O
to	O
initialize	O
the	O
word	O
embedding	O
and	O
fine	O
-	O
tune	O
it	O
during	O
the	O
training	O
.	O
We	O
randomly	O
initialize	O
all	O
outof	O
-	O
vocabulary	O
words	O
and	O
weights	O
with	O
the	O
uniform	O
distribution	O
U	O
p´0.1	O
,	O
0.1q	O
.	O
For	O
the	O
BERT	B-MethodName
-	O
based	O
models	O
,	O
we	O
fine	O
-	O
tune	O
pre	O
-	O
trained	O
BERT	B-MethodName
-	O
base	O
model	O
.	O

In	O
this	O
paper	O
,	O
we	O
describe	O
a	O
series	O
of	O
experiments	O
that	O
contribute	O
to	O
our	O
submission	O
to	O
the	O
WMT	O
2021	O
shared	O
task	O
of	O
Very	B-TaskName
Low	I-TaskName
Resource	I-TaskName
Supervised	I-TaskName
Machine	I-TaskName
Translation	I-TaskName
.	O
These	O
experiments	O
,	O
as	O
well	O
as	O
the	O
good	O
results	O
of	O
the	O
final	O
submission	O
,	O
show	O
that	O
dual	O
transfer	O
can	O
work	O
in	O
synergy	O
with	O
several	O
widely	O
used	O
techniques	O
in	O
realistic	O
scenarios	O
.	O

Towards	O
Generative	B-TaskName
Aspect	I-TaskName
-	I-TaskName
Based	I-TaskName
Sentiment	I-TaskName
Analysis	I-TaskName
*	O

The	O
BERT	B-MethodName
QA	O
model	O
concatenates	O
question	O
and	O
document	O
pairs	O
into	O
a	O
single	O
sequence	O
and	O
predicts	O
the	O
answer	O
span	O
by	O
a	O
dot	O
product	O
between	O
the	O
final	O
hidden	O
vectors	O
,	O
a	O
start	O
vector	O
and	O
an	O
end	O
vector	O
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
.	O
Due	O
to	O
the	O
memory	O
and	O
computational	O
requirements	O
,	O
BERT	B-MethodName
can	O
encode	O
sequences	O
with	O
a	O
maximum	B-HyperparameterName
length	I-HyperparameterName
of	O
512	B-HyperparameterValue
tokens	O
that	O
is	O
less	O
than	O
the	O
average	O
sample	O
length	O
in	O
NLQuAD	B-DatasetName
.	O
Therefore	O
,	O
we	O
adopt	O
a	O
sliding	O
window	O
approach	O
.	O
We	O
split	O
the	O
samples	O
into	O
segments	O
using	O
a	O
sliding	B-HyperparameterName
window	I-HyperparameterName
of	O
512	B-HyperparameterValue
tokens	O
and	O
a	O
stride	O
of	O
128	B-HyperparameterValue
tokens	O
.	O
Each	O
segment	O
is	O
augmented	O
with	O
its	O
corresponding	O
question	O
.	O
The	O
segments	O
can	O
include	O
no	O
answer	O
,	O
a	O
portion	O
of	O
the	O
answer	O
,	O
or	O
the	O
entire	O
answer	O
.	O
We	O
train	O
BERT	B-MethodName
on	O
the	O
segments	O
independently	O
.	O
Finally	O
,	O
the	O
predicted	O
spans	O
corresponding	O
to	O
a	O
single	O
sample	O
are	O
aggregated	O
to	O
predict	O
the	O
final	O
span	O
that	O
is	O
the	O
span	O
between	O
the	O
earliest	O
start	O
position	O
and	O
the	O
latest	O
end	O
position	O
.	O
The	O
output	O
is	O
considered	O
empty	O
when	O
all	O
segments	O
have	O
empty	O
spans	O
.	O
RoBERTa	B-MethodName
has	O
the	O
same	O
model	O
architecture	O
and	O
input	O
length	O
limitation	O
as	O
BERT	B-MethodName
but	O
with	O
a	O
robustly	O
optimized	O
pre	O
-	O
training	O
scheme	O
allowing	O
it	O
to	O
generalize	O
better	O
to	O
downstream	O
tasks	O
such	O
as	O
QA	O
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
apply	O
the	O
same	O
sliding	O
window	O
approach	O
for	O
RoBERTa	B-MethodName
.	O

The	O
left	O
null	O
space	O
of	O
a	O
m	O
p	O
×	O
n	O
p	O
matrix	O
P	O
can	O
be	O
defined	O
as	O
the	O
set	O
of	O
vectors	O
v	O
-	O
LN	O
P	O
=	O
{	O
v	O
T	O
R	O
1×mp	O
|	O
v	O
T	O
P	O
=	O
0	O
}	O
(	O
10	O
)	O
If	O
the	O
rows	O
of	O
P	O
are	O
linearly	O
independent	O
(	O
P	O
is	O
full	O
-	O
row	O
rank	O
)	O
the	O
left	O
null	O
space	O
of	O
P	O
is	O
zero	O
dimensional	O
.	O
The	O
only	O
solution	O
to	O
the	O
system	O
of	O
equations	O
v	O
P	O
=	O
0	O
is	O
trivial	O
,	O
i.e.	O
,	O
v=0	O
.	O
The	O
dimensions	O
of	O
the	O
null	O
space	O
,	O
known	O
as	O
nullity	O
,	O
of	O
P	O
can	O
be	O
calculated	O
as	O
dim	O
LN	O
(	O
P	O
)	O
=	O
m	O
p	O
−	O
rank	O
(	O
P	O
)	O
.	O
The	O
nullity	O
of	O
P	O
sets	O
the	O
dimensions	O
of	O
the	O
space	O
v	O
lies	O
in	O
.	O
In	O
3	O
,	O
we	O
utilize	O
our	O
knowledge	O
of	O
appendix	O
A.2	O
and	O
appendix	O
A.3	O
to	O
analyse	O
identifiability	O
in	O
a	O
Transformer	B-MethodName
.	O

This	O
work	O
probed	O
Transformer	B-MethodName
for	O
identifiability	O
of	O
self	O
-	O
attention	O
,	O
i.e.	O
,	O
the	O
attention	O
weights	O
can	O
be	O
uniquely	O
identified	O
from	O
the	O
head	O
's	O
output	O
.	O
With	O
theoretical	O
analysis	O
and	O
supporting	O
empirical	O
evidence	O
,	O
we	O
were	O
able	O
to	O
identify	O
the	O
limitations	O
of	O
the	O
existing	O
study	O
by	O
Brunner	O
et	O
al	O
(	O
2019	O
)	O
.	O
We	O
found	O
the	O
study	O
largely	O
ignored	O
the	O
constraint	O
coming	O
from	O
the	O
first	O
phase	O
of	O
self	O
-	O
attention	O
in	O
the	O
encoder	O
,	O
i.e.	O
,	O
the	O
size	B-HyperparameterName
of	I-HyperparameterName
the	I-HyperparameterName
key	I-HyperparameterName
vector	I-HyperparameterName
.	O
Later	O
,	O
we	O
proved	O
how	O
we	O
can	O
utilize	O
d	B-HyperparameterName
k	I-HyperparameterName
to	O
make	O
the	O
attention	O
weights	O
more	O
identifiable	O
.	O
To	O
give	O
a	O
more	O
concrete	O
solution	O
,	O
we	O
propose	O
encoder	O
variants	O
that	O
are	O
more	O
identifiable	O
,	O
theoretically	O
as	O
well	O
as	O
experimentally	O
,	O
for	O
a	O
large	O
range	O
of	O
input	O
sequence	O
lengths	O
.	O
The	O
identifiable	O
variants	O
do	O
not	O
show	O
any	O
performance	O
drop	O
when	O
experiments	O
are	O
done	O
on	O
varied	O
text	B-TaskName
classification	I-TaskName
tasks	O
.	O
Future	O
works	O
may	O
analyse	O
the	O
critical	O
impact	O
of	O
identifiability	O
on	O
the	O
explainability	O
and	O
interpretability	O
of	O
the	O
Transformer	B-MethodName
.	O

IMDB	B-DatasetName
(	O
Maas	O
et	O
al	O
,	O
2011	O
)	O
.	O
The	O
dataset	O
for	O
the	O
task	O
of	O
sentiment	B-TaskName
classification	I-TaskName
consist	O
of	O
IMDB	O
movie	O
reviews	O
with	O
their	O
sentiment	O
as	O
positive	O
or	O
negative	O
.	O
Each	O
of	O
the	O
train	O
and	O
test	O
sets	O
contain	O
25	O
,	O
000	O
data	O
samples	O
equally	O
distributed	O
in	O
both	O
the	O
sentiment	O
polarities	O
.	O
TREC	B-DatasetName
(	O
Voorhees	O
and	O
Tice	O
,	O
2000	O
)	O
.	O
We	O
use	O
the	O
6	O
-	O
class	O
version	O
of	O
the	O
dataset	O
for	O
the	O
task	O
of	O
question	B-TaskName
classification	I-TaskName
consisting	O
of	O
open	O
-	O
domain	O
,	O
facet	O
-	O
based	O
questions	O
.	O
There	O
are	O
5	O
,	O
452	O
and	O
500	O
samples	O
for	O
training	O
and	O
testing	O
,	O
respectively	O
.	O
SST	B-DatasetName
(	O
Socher	O
et	O
al	O
,	O
2013	O
)	O
.	O
Stanford	O
sentiment	O
analysis	O
dataset	O
consist	O
of	O
11	O
,	O
855	O
sentences	O
obtained	O
from	O
movie	O
reviews	O
.	O
We	O
use	O
the	O
3	O
-	O
class	O
version	O
of	O
the	O
dataset	O
for	O
the	O
task	O
of	O
sentiment	B-TaskName
classification	I-TaskName
.	O
Each	O
review	O
is	O
labeled	O
as	O
positive	O
,	O
neutral	O
,	O
or	O
negative	O
.	O
The	O
provided	O
train	B-HyperparameterName
/	I-HyperparameterName
test	I-HyperparameterName
/	I-HyperparameterName
valid	I-HyperparameterName
split	I-HyperparameterName
is	O
8	O
,	O
544/2	B-HyperparameterValue
,	I-HyperparameterValue
210/1	I-HyperparameterValue
,	I-HyperparameterValue
101	I-HyperparameterValue
.	O
8	O
ds	O
-	O
max	O
<	O
de	O
as	O
in	O
the	O
regular	O
Transformer	O
setting	O
.	O

SNLI	B-DatasetName
(	O
Bowman	O
et	O
al	O
,	O
2015	O
)	O
.	O
The	O
dataset	O
contain	O
549	O
,	O
367	O
samples	O
in	O
the	O
training	O
set	O
,	O
9	O
,	O
842	O
samples	O
in	O
the	O
validation	O
set	O
,	O
and	O
9	O
,	O
824	O
samples	O
in	O
the	O
test	O
set	O
.	O
For	O
the	O
task	O
of	O
recognizing	B-TaskName
textual	I-TaskName
entailment	I-TaskName
,	O
each	O
sample	O
consists	O
of	O
a	O
premisehypothesis	O
sentence	O
pair	O
and	O
a	O
label	O
indicating	O
whether	O
the	O
hypothesis	O
entails	O
the	O
premise	O
,	O
contradicts	O
it	O
,	O
or	O
neutral	O
.	O
Please	O
refer	O
to	O
Zhang	O
et	O
al	O
(	O
2015	O
)	O
for	O
more	O
details	O
about	O
the	O
following	O
datasets	O
:	O
Yelp	B-DatasetName
.	O
We	O
use	O
the	O
large	O
-	O
scale	O
Yelp	O
review	O
dataset	O
for	O
the	O
task	O
of	O
binary	B-TaskName
sentiment	I-TaskName
classification	I-TaskName
.	O
There	O
are	O
560	O
,	O
000	O
samples	O
for	O
training	O
and	O
38	O
,	O
000	O
samples	O
for	O
testing	O
,	O
equally	O
split	O
into	O
positive	O
and	O
negative	O
polarities	O
.	O
DBPedia	B-DatasetName
.	O
The	O
Ontology	O
dataset	O
for	O
topic	B-TaskName
classification	I-TaskName
consist	O
of	O
14	O
non	O
-	O
overlapping	O
classes	O
each	O
with	O
40	O
,	O
000	O
samples	O
for	O
training	O
and	O
5	O
,	O
000	O
samples	O
for	O
testing	O
.	O
Sogou	B-DatasetName
News	I-DatasetName
.	O
The	O
dataset	O
for	O
news	B-TaskName
article	I-TaskName
classification	I-TaskName
consist	O
of	O
450	O
,	O
000	O
samples	O
for	O
training	O
and	O
60	O
,	O
000	O
for	O
testing	O
.	O
Each	O
article	O
is	O
labeled	O
in	O
one	O
of	O
the	O
5	O
news	O
categories	O
.	O
The	O
dataset	O
is	O
perfectly	O
balanced	O
.	O
AG	B-DatasetName
News	I-DatasetName
.	O
The	O
dataset	O
for	O
the	O
news	O
articles	O
classification	O
partitioned	O
into	O
four	O
categories	O
.	O
The	O
balanced	O
train	O
and	O
test	O
set	O
consist	O
of	O
120	O
,	O
000	O
and	O
7	O
,	O
600	O
samples	O
,	O
respectively	O
.	O
Yahoo	B-DatasetName
!	I-DatasetName
Answers	I-DatasetName
.	O
The	O
balanced	O
dataset	O
for	O
10class	O
topic	B-TaskName
classification	I-TaskName
contain	O
1	O
,	O
400	O
,	O
000	O
samples	O
for	O
training	O
and	O
50	O
,	O
000	O
samples	O
for	O
testing	O
.	O
Amazon	B-DatasetName
Reviews	I-DatasetName
.	O
For	O
the	O
task	O
of	O
sentiment	B-TaskName
classification	I-TaskName
,	O
the	O
dataset	O
contain	O
3	O
,	O
600	O
,	O
000	O
samples	O
for	O
training	O
and	O
400	O
,	O
000	O
samples	O
for	O
testing	O
.	O
The	O
samples	O
are	O
equally	O
divided	O
into	O
positive	O
and	O
negative	O
sentiment	O
labels	O
.	O
Except	O
for	O
the	O
SST	B-DatasetName
and	O
SNLI	B-DatasetName
,	O
where	O
the	O
validation	O
split	O
is	O
already	O
provided	O
,	O
we	O
flag	O
30	O
%	O
of	O
the	O
train	O
set	O
as	O
part	O
of	O
the	O
validation	O
set	O
and	O
the	O
rest	O
70	O
%	O
were	O
used	O
for	O
model	O
parameter	O
learning	O
.	O

Hyperbolic	B-MethodName
Capsule	I-MethodName
Networks	I-MethodName
for	O
Multi	B-TaskName
-	I-TaskName
Label	I-TaskName
Classification	I-TaskName

We	O
report	O
experimental	O
results	O
with	O
the	O
addition	O
of	O
a	O
teacher	O
distillation	O
step	O
as	O
previous	O
work	O
showed	O
this	O
boosts	O
movement	O
pruning	O
at	O
little	O
cost	O
.	O
In	O
this	O
section	O
,	O
we	O
conduct	O
an	O
ablation	O
study	O
to	O
evaluate	O
the	O
impact	O
of	O
distillation	O
using	O
a	O
BERT	B-MethodName
-	I-MethodName
base	I-MethodName
teacher	O
.	O

We	O
are	O
using	O
a	O
minimal	O
set	O
of	O
hyperparameters	O
.	O
The	O
ratio	O
of	O
λ	B-HyperparameterName
att	I-HyperparameterName
and	O
λ	B-HyperparameterName
ffn	I-HyperparameterName
is	O
fixed	O
by	O
the	O
relative	O
sizes	O
.	O
We	O
performed	O
a	O
few	O
experiments	O
with	O
differ	O
-	O
ent	O
values	O
fixed	O
manually	O
for	O
these	O
parameters	O
,	O
but	O
their	O
influence	O
is	O
minor	O
.	O
The	O
main	O
hyperparameter	O
is	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
training	I-HyperparameterName
epochs	I-HyperparameterName
.	O
For	O
SQuAD	B-DatasetName
v1.1	I-DatasetName
,	O
we	O
are	O
using	O
20	B-HyperparameterValue
epochs	B-HyperparameterName
instead	O
of	O
typically	O
2	B-HyperparameterValue
for	O
BERT	B-MethodName
models	O
.	O
This	O
means	O
a	O
fine	O
-	O
tuning	O
is	O
taking	O
about	O
12h	O
with	O
our	O
method	O
instead	O
of	O
45mn	O
with	O
a	O
standard	O
finetuning	O
setup	O
.	O
This	O
number	O
has	O
to	O
be	O
large	O
enough	O
to	O
let	O
pruning	O
happen	O
slowly	O
enough	O
for	O
a	O
given	O
task	O
.	O
A	O
warming	O
up	O
phase	O
and	O
a	O
post	O
-	O
pruning	O
cooldown	O
phase	O
are	O
helpful	O
,	O
but	O
their	O
exact	O
length	O
has	O
not	O
a	O
large	O
impact	O
on	O
final	O
performance	O
.	O
We	O
believe	O
the	O
training	O
time	O
is	O
less	O
important	O
than	O
the	O
inference	O
time	O
for	O
energy	O
consideration	O
,	O
as	O
inference	O
is	O
performed	O
repeatedly	O
.	O
Our	O
method	O
is	O
optimizing	O
inference	O
by	O
a	O
large	O
factor	O
:	O
the	O
training	O
energy	O
is	O
potentially	O
recouped	O
by	O
a	O
large	O
margin	O
with	O
inference	O
savings	O
.	O
Finally	O
,	O
the	O
checkpoints	O
created	O
during	O
the	O
experiments	O
are	O
available	O
on	O
an	O
AWS	O
S3	O
bucket	O
,	O
with	O
their	O
metadata	O
and	O
training	O
parameters	O
,	O
totaling	O
3	O
TB	O
of	O
data	O
,	O
to	O
facilitate	O
reproduction	O
of	O
our	O
results	O
and	O
to	O
make	O
it	O
possible	O
to	O
study	O
further	O
the	O
behavior	O
of	O
those	O
models	O
.	O
Code	O
for	O
experiments	O
,	O
analysis	O
,	O
and	O
tools	O
to	O
prepare	O
the	O
present	O
paper	O
are	O
available	O
on	O
GitHub	O
(	O
see	O
Appendix	O
A	O
)	O
.	O

Table	O
5	O
is	O
a	O
summary	O
of	O
the	O
information	O
about	O
the	O
version	O
of	O
all	O
Transformer	O
-	O
based	O
models	O
used	O
and	O
their	O
pretraining	O
methods	O
.	O
D	O
Detailed	O
metrics	O
of	O
all	O
the	O
models	O
(	O
Weissenbacher	O
et	O
al	O
,	O
2019	O
)	O
and	O
take	O
into	O
account	O
"	O
partial"matches	O
,	O
in	O
which	O
it	O
is	O
sufficient	O
for	O
a	O
system	O
prediction	O
to	O
partially	O
overlap	O
with	O
the	O
gold	O
annotation	O
to	O
be	O
considered	O
as	O
a	O
true	O
match	O
.	O

Pretrained	O
transformer	O
-	O
based	O
models	O
,	O
such	O
as	O
BERT	B-MethodName
and	O
its	O
variants	O
,	O
have	O
become	O
a	O
common	O
choice	O
to	O
obtain	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performances	O
in	O
NLP	O
tasks	O
.	O
In	O
the	O
identification	B-TaskName
of	I-TaskName
Adverse	I-TaskName
Drug	I-TaskName
Events	I-TaskName
(	O
ADE	O
)	O
from	O
social	O
media	O
texts	O
,	O
for	O
example	O
,	O
BERT	B-MethodName
architectures	O
rank	O
first	O
in	O
the	O
leaderboard	O
.	O
However	O
,	O
a	O
systematic	O
comparison	O
between	O
these	O
models	O
has	O
not	O
yet	O
been	O
done	O
.	O
In	O
this	O
paper	O
,	O
we	O
aim	O
at	O
shedding	O
light	O
on	O
the	O
differences	O
between	O
their	O
performance	O
analyzing	O
the	O
results	O
of	O
12	O
models	O
,	O
tested	O
on	O
two	O
standard	O
benchmarks	O
.	O
SpanBERT	B-MethodName
and	O
PubMedBERT	B-MethodName
emerged	O
as	O
the	O
best	O
models	O
in	O
our	O
evaluation	O
:	O
this	O
result	O
clearly	O
shows	O
that	O
span	O
-	O
based	O
pretraining	O
gives	O
a	O
decisive	O
advantage	O
in	O
the	O
precise	O
recognition	O
of	O
ADEs	O
,	O
and	O
that	O
in	O
-	O
domain	O
language	O
pretraining	O
is	O
particularly	O
useful	O
when	O
the	O
transformer	O
model	O
is	O
trained	O
just	O
on	O
biomedical	O
text	O
from	O
scratch	O
.	O

BERT	O
Prescriptions	O
to	O
Avoid	O
Unwanted	O
Headaches	O
:	O
A	O
Comparison	O
of	O
Transformer	O
Architectures	O
for	O
Adverse	B-TaskName
Drug	I-TaskName
Event	I-TaskName
Detection	I-TaskName

In	O
this	O
paper	O
,	O
we	O
proposed	O
an	O
ensemble	O
learning	O
model	O
for	O
the	O
geolocation	B-TaskName
of	I-TaskName
Swiss	I-TaskName
German	I-TaskName
social	I-TaskName
media	I-TaskName
posts	I-TaskName
.	O
The	O
ensemble	O
is	O
based	O
on	O
an	O
XGBoost	B-MethodName
meta	I-MethodName
-	I-MethodName
learner	I-MethodName
applied	O
on	O
top	O
of	O
three	O
individual	O
models	O
:	O
a	O
hybrid	B-MethodName
CNN	I-MethodName
,	O
an	O
approach	O
based	O
on	O
string	O
kernels	O
and	O
a	O
fine	O
-	O
tuned	O
German	B-MethodName
BERT	I-MethodName
model	O
.	O
Given	O
the	O
final	O
results	O
obtained	O
in	O
the	O
SMG	O
-	O
CH	O
subtask	O
,	O
we	O
conclude	O
that	O
predicting	O
the	O
location	O
of	O
Swiss	O
German	O
social	O
media	O
posts	O
is	O
a	O
challenging	O
task	O
,	O
the	O
median	B-MetricName
distance	I-MetricName
being	O
higher	O
than	O
20	B-MetricValue
km	I-MetricValue
.	O
Using	O
external	O
data	O
sources	O
to	O
build	O
a	O
language	O
model	O
seems	O
to	O
be	O
a	O
more	O
promising	O
path	O
towards	O
success	O
,	O
as	O
shown	O
by	O
the	O
final	O
standings	O
of	O
the	O
VarDial	O
2020	O
(	O
Gȃman	O
et	O
al	O
,	O
2020	O
)	O
and	O
2021	O
(	O
Chakravarthi	O
et	O
al	O
,	O
2021	O
SMG	O
shared	O
tasks	O
.	O
In	O
future	O
work	O
,	O
we	O
aim	O
to	O
study	O
the	O
applicability	O
of	O
our	O
ensemble	O
on	O
other	O
geolocation	O
tasks	O
,	O
perhaps	O
taking	O
into	O
consideration	O
future	O
VarDial	O
challenges	O
.	O

We	O
train	O
three	O
different	O
models	O
which	O
rely	O
on	O
different	O
learning	O
methods	O
and	O
types	O
of	O
features	O
to	O
perform	O
the	O
required	O
double	O
regression	O
.	O
Thus	O
,	O
we	O
have	O
the	O
hybrid	B-MethodName
CNN	I-MethodName
relying	O
on	O
both	O
words	O
and	O
characters	O
as	O
features	O
,	O
the	O
shallow	B-MethodName
ν	I-MethodName
-	I-MethodName
SVR	I-MethodName
based	O
on	O
string	O
kernels	O
and	O
three	O
fine	O
-	O
tuned	O
German	B-MethodName
BERT	I-MethodName
models	O
looking	O
at	O
higher	O
-	O
level	O
features	O
and	O
understanding	O
dependencies	O
in	O
a	O
bidirectional	O
manner	O
.	O
Table	O
1	O
shows	O
the	O
preliminary	O
results	O
obtained	O
on	O
the	O
development	O
set	O
by	O
each	O
individual	O
model	O
as	O
well	O
as	O
the	O
results	O
of	O
the	O
submitted	O
XGBoost	B-MethodName
ensemble	I-MethodName
.	O
The	O
individual	O
models	O
provide	O
quite	O
similar	O
results	O
in	O
terms	O
of	O
the	O
median	O
distance	O
between	O
the	O
predicted	O
and	O
ground	O
-	O
truth	O
locations	O
.	O
These	O
results	O
stay	O
around	O
a	O
value	O
of	O
30	B-MetricValue
km	I-MetricValue
for	O
the	O
median	B-MetricName
distance	I-MetricName
and	O
35	B-MetricValue
km	I-MetricValue
for	O
the	O
mean	B-MetricName
distance	I-MetricName
.	O
Among	O
the	O
independent	O
models	O
,	O
the	O
hybrid	B-MethodName
CNN	I-MethodName
obtains	O
slightly	O
better	O
results	O
in	O
terms	O
of	O
the	O
median	B-MetricName
distance	I-MetricName
(	O
30.05	B-MetricValue
km	I-MetricValue
)	O
,	O
whereas	O
the	O
second	O
attempt	O
at	O
fine	O
-	O
tuning	O
BERT	B-MethodName
gives	O
the	O
worst	O
distances	O
,	O
namely	O
33.86	B-MetricValue
km	I-MetricValue
for	O
the	O
median	B-MetricName
distance	I-MetricName
and	O
38.85	O
km	O
for	O
the	O
mean	B-MetricName
distance	I-MetricName
.	O
ν	B-MethodName
-	I-MethodName
SVR	I-MethodName
surpasses	O
all	O
the	O
other	O
models	O
,	O
by	O
a	O
small	O
margin	O
,	O
in	O
terms	O
of	O
the	O
mean	B-MetricName
distance	I-MetricName
(	O
34.82	B-MetricValue
km	I-MetricValue
)	O
.	O
The	O
results	O
of	O
the	O
submitted	O
XGBoost	B-MethodName
ensemble	I-MethodName
model	O
stand	O
proof	O
that	O
our	O
intuition	O
was	O
correct	O
,	O
namely	O
that	O
all	O
these	O
individual	O
models	O
have	O
the	O
potential	O
to	O
complement	O
each	O
other	O
if	O
put	O
together	O
in	O
an	O
ensemble	O
.	O
Indeed	O
,	O
the	O
submitted	O
system	O
clearly	O
surpasses	O
the	O
best	O
individual	O
model	O
by	O
approximately	O
5	B-MetricValue
km	I-MetricValue
in	O
terms	O
of	O
both	O
the	O
median	O
and	O
the	O
mean	O
distance	O
metrics	O
.	O

Transformers	B-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
represent	O
an	O
important	O
advance	O
in	O
NLP	O
,	O
with	O
many	O
benefits	O
over	O
the	O
traditional	O
sequential	O
neural	O
architectures	O
.	O
Based	O
on	O
an	O
encoder	O
-	O
decoder	O
architecture	O
with	O
attention	O
,	O
transformers	B-MethodName
proved	O
to	O
be	O
better	O
at	O
modeling	O
long	O
-	O
term	O
dependencies	O
in	O
sequences	O
,	O
while	O
being	O
effectively	O
trained	O
as	O
the	O
sequential	O
dependency	O
of	O
previous	O
tokens	O
is	O
removed	O
.	O
Unlike	O
other	O
contemporary	O
attempts	O
at	O
using	O
transformers	O
in	O
language	O
modeling	O
(	O
Radford	O
et	O
al	O
,	O
2018	O
)	O
,	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
builds	O
deep	O
language	O
representations	O
in	O
a	O
self	O
-	O
supervised	O
fashion	O
and	O
incorporates	O
context	O
from	O
both	O
directions	O
.	O
The	O
masked	O
language	O
modeling	O
technique	O
enables	O
BERT	B-MethodName
to	O
pretrain	O
these	O
deep	O
bidirectional	O
representations	O
,	O
that	O
can	O
be	O
further	O
fine	O
-	O
tuned	O
and	O
adapted	O
for	O
a	O
variety	O
of	O
downstream	O
tasks	O
,	O
without	O
significant	O
architectural	O
updates	O
.	O
We	O
also	O
make	O
use	O
of	O
this	O
property	O
in	O
the	O
current	O
work	O
,	O
employing	O
the	O
Hugging	O
Face	O
(	O
Wolf	O
et	O
al	O
,	O
2020	O
)	O
version	O
of	O
the	O
cased	O
German	O
BERT	B-MethodName
model	O
1	O
.	O
The	O
model	O
was	O
initially	O
trained	O
on	O
the	O
latest	O
German	B-DatasetName
Wikipedia	I-DatasetName
dump	O
,	O
the	O
OpenLe	B-DatasetName
-	I-DatasetName
galData	I-DatasetName
dump	O
and	O
a	O
collection	O
of	O
news	O
articles	O
,	O
summing	O
up	O
to	O
a	O
total	O
of	O
12	O
GB	O
of	O
text	O
files	O
.	O
We	O
fine	O
-	O
tune	O
this	O
pre	O
-	O
trained	O
German	B-MethodName
BERT	I-MethodName
model	O
for	O
the	O
geolocation	B-TaskName
of	I-TaskName
Swiss	I-TaskName
German	I-TaskName
short	I-TaskName
texts	I-TaskName
,	O
in	O
a	O
regression	O
setup	O
.	O
The	O
choice	O
of	O
hyperparameters	O
is	O
,	O
in	O
part	O
,	O
inspired	O
by	O
the	O
winning	O
system	O
in	O
the	O
last	O
year	O
's	O
SMG	O
-	O
CH	O
subtask	O
at	O
VarDial	O
(	O
Scherrer	O
and	O
Ljubešić	O
,	O
2020	O
)	O
.	O

This	O
research	O
was	O
supported	O
by	O
the	O
DARPA	O
Communicating	O
with	O
Computers	O
program	O
,	O
under	O
ARO	O
contract	O
W911NF	O
-	O
15	O
-	O
1	O
-	O
0542	O
.	O

The	O
statistics	O
of	O
the	O
training	O
,	O
validation	O
,	O
and	O
test	O
datasets	O
on	O
Turkish	O
-	O
English	O
and	O
Uyghur	O
-	O
Chinese	O
machine	O
translation	O
tasks	O
are	O
shown	O
in	O
Table	O
1	O
.	O
For	O
the	O
Turkish	O
-	O
English	O
machine	O
translation	O
,	O
following	O
(	O
Sennrich	O
et	O
al	O
,	O
2015a	O
)	O
,	O
we	O
use	O
the	O
WIT	B-DatasetName
corpus	O
(	O
Cettolo	O
et	O
al	O
,	O
2012	O
)	O
and	O
the	O
SETimes	B-DatasetName
corpus	O
(	O
Tyers	O
and	O
Alperen	O
,	O
2010	O
)	O
as	O
the	O
training	O
dataset	O
,	O
merge	O
the	O
dev2010	O
and	O
tst2010	O
as	O
the	O
validation	O
dataset	O
,	O
and	O
use	O
tst2011	O
,	O
tst2012	O
,	O
tst2013	O
,	O
tst2014	O
from	O
the	O
IWSLT	O
as	O
the	O
test	O
datasets	O
.	O
We	O
also	O
use	O
the	O
talks	O
data	O
from	O
the	O
IWSLT	O
evaluation	O
campaign	O
1	O
in	O
2018	O
and	O
the	O
news	O
data	O
from	O
News	B-DatasetName
Crawl	I-DatasetName
corpora	O
2	O
in	O
2017	O
as	O
external	O
monolingual	O
data	O
for	O
the	O
stemming	O
task	O
on	O
Turkish	O
sentences	O
.	O
For	O
the	O
Uyghur	O
-	O
Chinese	O
machine	O
translation	O
,	O
we	O
use	O
the	O
news	O
data	O
from	O
the	O
China	O
Workshop	O
on	O
Machine	O
Translation	O
in	O
2017	O
(	O
CWMT2017	O
)	O
as	O
the	O
training	O
dataset	O
and	O
validation	O
dataset	O
,	O
use	O
the	O
news	O
data	O
from	O
CWMT2015	B-DatasetName
as	O
the	O
test	O
dataset	O
.	O
Each	O
Uyghur	O
sentence	O
has	O
four	O
Chinese	O
reference	O
sentences	O
.	O
Moreover	O
,	O
we	O
use	O
the	O
news	O
data	O
from	O
the	O
Tianshan	O
website	O
3	O
as	O
external	O
monolingual	O
data	O
for	O
the	O
stemming	O
task	O
on	O
Uyghur	O
sentences	O
.	O

We	O
propose	O
a	O
multi	B-MethodName
-	I-MethodName
task	I-MethodName
neural	I-MethodName
model	I-MethodName
for	O
machine	B-TaskName
translation	I-TaskName
from	O
and	O
into	O
a	O
low	O
-	O
resource	O
and	O
morphologically	O
-	O
rich	O
agglutinative	O
language	O
.	O
We	O
train	O
the	O
model	O
to	O
jointly	O
learn	O
to	O
perform	O
both	O
the	O
bi	O
-	O
directional	O
translation	O
task	O
and	O
the	O
stemming	O
task	O
on	O
an	O
agglutinative	O
language	O
by	O
using	O
the	O
standard	O
NMT	B-TaskName
framework	O
.	O
Moreover	O
,	O
we	O
add	O
an	O
artificial	O
token	O
before	O
each	O
source	O
sentence	O
to	O
specify	O
the	O
desired	O
target	O
outputs	O
for	O
different	O
tasks	O
.	O
The	O
architecture	O
of	O
the	O
proposed	O
model	O
is	O
shown	O
in	O
Figure	O
1	O
.	O
We	O
take	O
the	O
Turkish	O
-	O
English	O
translation	O
task	O
as	O
example	O
.	O
The	O
"	O
<	O
MT	O
>	O
"	O
token	O
denotes	O
the	O
bilingual	O
translation	O
task	O
and	O
the	O
"	O
<	O
ST	O
>	O
"	O
token	O
denotes	O
the	O
stemming	O
task	O
on	O
Turkish	O
sentence	O
.	O

Neural	B-TaskName
machine	I-TaskName
translation	I-TaskName
(	O
NMT	B-TaskName
)	O
has	O
achieved	O
impressive	O
performance	O
recently	O
by	O
using	O
large	O
-	O
scale	O
parallel	O
corpora	O
.	O
However	O
,	O
it	O
struggles	O
in	O
the	O
low	O
-	O
resource	O
and	O
morphologically	O
-	O
rich	O
scenarios	O
of	O
agglutinative	O
language	O
translation	O
task	O
.	O
Inspired	O
by	O
the	O
finding	O
that	O
monolingual	O
data	O
can	O
greatly	O
improve	O
the	O
NMT	B-TaskName
performance	O
,	O
we	O
propose	O
a	O
multi	B-MethodName
-	I-MethodName
task	I-MethodName
neural	I-MethodName
model	I-MethodName
that	O
jointly	O
learns	O
to	O
perform	O
bi	O
-	O
directional	O
translation	O
and	O
agglutinative	O
language	O
stemming	O
.	O
Our	O
approach	O
employs	O
the	O
shared	O
encoder	O
and	O
decoder	O
to	O
train	O
a	O
single	O
model	O
without	O
changing	O
the	O
standard	O
NMT	B-TaskName
architecture	O
but	O
instead	O
adding	O
a	O
token	O
before	O
each	O
source	O
-	O
side	O
sentence	O
to	O
specify	O
the	O
desired	O
target	O
outputs	O
of	O
the	O
two	O
different	O
tasks	O
.	O
Experimental	O
results	O
on	O
Turkish	O
-	O
English	O
and	O
Uyghur	O
-	O
Chinese	O
show	O
that	O
our	O
proposed	O
approach	O
can	O
significantly	O
improve	O
the	O
translation	O
performance	O
on	O
agglutinative	O
languages	O
by	O
using	O
a	O
small	O
amount	O
of	O
monolingual	O
data	O
.	O

We	O
conducted	O
two	O
rounds	O
of	O
human	B-MetricName
evaluations	I-MetricName
,	O
each	O
time	O
across	O
200	O
examples	O
from	O
our	O
test	O
set	O
.	O
Annotators	O
were	O
crowd	O
sourced	O
,	O
and	O
each	O
example	O
was	O
rated	O
by	O
seven	O
judges	O
for	O
a	O
total	O
of	O
1400	O
judgements	O
.	O
8	O
Command	O
and	O
Grounding	O
In	O
our	O
first	O
round	O
of	O
human	B-MetricName
evaluations	I-MetricName
we	O
compared	O
our	O
model	O
's	O
top	O
output	O
from	O
beam	O
search	O
to	O
the	O
reference	O
edit	O
.	O
There	O
were	O
two	O
tasks	O
.	O
In	O
the	O
first	O
task	O
,	O
we	O
asked	O
judges	O
to	O
choose	O
which	O
system	O
better	O
accomplished	O
the	O
command	O
q.	O
In	O
the	O
second	O
,	O
we	O
asked	O
which	O
system	O
was	O
more	O
faithful	O
to	O
the	O
grounding	O
G.	O
8	O
:	O
Human	B-MetricName
Evaluation	I-MetricName
:	O
comparisons	O
between	O
absolute	O
evaluations	O
of	O
different	O
settings	O
.	O
Raters	O
were	O
asked	O
whether	O
edits	O
were	O
satisfactory	O
.	O
0	O
corresponds	O
to	O
strong	O
disagreement	O
,	O
and	O
5	O
to	O
strong	O
agreement	O
.	O
Systems	O
are	O
given	O
by	O
model	O
(	O
full	O
or	O
with	O
the	O
comment	O
ablated	O
)	O
,	O
and	O
whether	O
the	O
command	O
was	O
shown	O
to	O
the	O
raters	O
(	O
+	O
or	O
-	O
)	O
.	O
Bolded	O
numbers	O
indicate	O
significant	O
difference	O
with	O
p	O
<	O
0.0125	O
.	O
than	O
the	O
reference	O
.	O
9	O
In	O
the	O
grounding	O
task	O
,	O
Interactive	O
Editor	O
demonstrates	O
good	O
correspondence	O
with	O
the	O
background	O
material	O
.	O
10	O
Judges	O
were	O
further	O
asked	O
whether	O
the	O
retrieved	O
grounding	O
was	O
relevant	O
to	O
the	O
context	O
D	O
:	O
92.86	O
%	O
of	O
judgments	O
recorded	O
the	O
grounding	O
as	O
either	O
"	O
Somewhat	O
relevant	O
"	O
or	O
"	O
Very	O
relevant	O
"	O
.	O

We	O
begin	O
with	O
various	O
definitions	O
and	O
results	O
.	O
We	O
define	O
simulation	O
of	O
Turing	O
machines	O
by	O
RNNs	O
and	O
state	O
the	O
Turing	O
-	O
completeness	O
result	O
for	O
RNNs	O
.	O
We	O
define	O
vanilla	O
and	O
directional	O
Transformers	O
and	O
what	O
it	O
means	O
for	O
Transformers	O
to	O
simulate	O
RNNs	O
.	O
Many	O
of	O
the	O
definitions	O
from	O
the	O
main	O
paper	O
are	O
reproduced	O
here	O
,	O
but	O
in	O
more	O
detail	O
.	O
In	O
Sec	O
.	O
C.1	O
we	O
discuss	O
the	O
effect	O
of	O
removing	O
a	O
residual	O
connection	O
on	O
computational	O
power	O
of	O
Transformers	O
.	O
Sec	O
.	O
C.2	O
contains	O
the	O
proof	O
of	O
Turing	O
completeness	O
of	O
vanilla	O
Transformers	O
and	O
Sec	O
.	O
D	O
the	O
corresponding	O
proof	O
for	O
directional	O
Transformers	O
.	O
Finally	O
,	O
Sec	O
.	O
5	O
has	O
further	O
details	O
of	O
experiments	O
.	O

The	O
goal	O
of	O
this	O
task	O
is	O
to	O
evaluate	O
the	O
performance	O
of	O
automatic	B-TaskName
event	I-TaskName
detection	I-TaskName
systems	O
on	O
modeling	O
the	O
spatial	O
and	O
temporal	O
pattern	O
of	O
a	O
social	O
protest	O
movement	O
.	O
We	O
evaluate	O
the	O
capability	O
of	O
participant	O
systems	O
to	O
reproduce	O
a	O
manually	O
curated	O
BLM	O
-	O
related	O
protest	B-DatasetName
event	I-DatasetName
data	I-DatasetName
set	I-DatasetName
,	O
by	O
detecting	O
BLM	O
event	O
reports	O
,	O
enriched	O
with	O
location	O
and	O
date	O
attributes	O
,	O
from	O
a	O
news	O
corpus	O
collection	O
,	O
a	O
Twitter	O
collection	O
,	O
and	O
from	O
the	O
union	O
of	O
the	O
two	O
.	O

Lemma	O
D.2	O
.	O
There	O
exists	O
a	O
function	O
O	O
(	O
1	O
)	O
(	O
.	O
)	O
defined	O
by	O
feed	O
-	O
forward	O
network	O
such	O
that	O
,	O
Proof	O
.	O
We	O
define	O
the	O
feed	O
-	O
forward	O
network	O
O	O
(	O
1	O
)	O
(	O
.	O
)	O
such	O
that	O
We	O
define	O
the	O
feed	O
-	O
forward	O
network	O
O	O
(	O
a	O
t	O
)	O
as	O
follows	O
,	O

We	O
recruit	O
3	O
annotators	O
that	O
work	O
as	O
academic	O
researchers	O
in	O
the	O
areas	O
of	O
NLP	O
and	O
social	O
science	O
.	O
For	O
each	O
one	O
of	O
the	O
30	O
topics	O
,	O
the	O
annotators	O
are	O
provided	O
with	O
the	O
top	O
-	O
10	O
topic	O
keywords	O
and	O
the	O
summaries	O
of	O
top	O
-	O
10	O
most	O
relevant	O
documents	O
from	O
each	O
news	O
corpus	O
(	O
as	O
a	O
total	O
of	O
60	O
documents	O
)	O
.	O
First	O
,	O
the	O
annotators	O
select	O
15	O
topics	O
on	O
which	O
they	O
feel	O
it	O
is	O
straightforward	O
to	O
find	O
two	O
polarized	O
political	O
stances	O
by	O
reading	O
the	O
relevant	O
documents	O
.	O
For	O
example	O
,	O
on	O
topic	O
12	O
about	O
Democratic	O
primaries	O
,	O
it	O
is	O
intuitive	O
to	O
perceive	O
the	O
two	O
political	O
stances	O
are	O
"	O
endorsing	O
Biden	O
"	O
and	O
"	O
endorsing	O
Sanders	O
"	O
after	O
reading	O
relevant	O
articles	O
,	O
and	O
then	O
this	O
topic	O
is	O
likely	O
to	O
be	O
selected	O
.	O
We	O
take	O
the	O
overlap	O
of	O
the	O
15	O
selected	O
topics	O
from	O
3	O
annotators	O
and	O
obtain	O
10	O
topics	O
:	O
T	O
labeled	O
=	O
{	O
t	O
1	O
,	O
t	O
2	O
,	O
t	O
8	O
,	O
t	O
9	O
,	O
t	O
10	O
,	O
t	O
11	O
,	O
t	O
12	O
,	O
t	O
27	O
,	O
t	O
30	O
,	O
t	O
33	O
}	O
with	O
defined	O
polarized	O
political	O
stances	O
.	O
In	O
other	O
words	O
,	O
the	O
annotators	O
reach	O
an	O
agreement	O
that	O
it	O
is	O
more	O
clear	O
on	O
these	O
10	O
topics	O
that	O
there	O
are	O
two	O
political	O
stances	O
.	O
We	O
find	O
that	O
on	O
each	O
of	O
these	O
10	O
topics	O
,	O
the	O
two	O
stances	O
defined	O
by	O
3	O
annotators	O
reach	O
a	O
complete	O
agreement	O
.	O
We	O
do	O
not	O
annotate	O
all	O
topics	O
because	O
1	O
)	O
it	O
is	O
difficult	O
for	O
humans	O
to	O
discern	O
the	O
two	O
political	O
stances	O
on	O
some	O
topics	O
,	O
especially	O
when	O
such	O
two	O
stances	O
do	O
not	O
exist	O
at	O
all	O
;	O
2	O
)	O
we	O
use	O
the	O
vanilla	O
LDA	O
topic	O
modeling	O
which	O
is	O
not	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
,	O
so	O
the	O
modeled	O
topics	O
will	O
change	O
using	O
different	O
topic	O
models	O
,	O
in	O
which	O
case	O
the	O
annotating	O
step	O
should	O
be	O
repeated	O
.	O
Nevertheless	O
,	O
we	O
argue	O
that	O
annotating	O
10	O
topics	O
is	O
sufficient	O
to	O
quantitatively	O
evaluate	O
the	O
effectiveness	O
of	O
PaCTE	O
.	O
Given	O
a	O
topic	O
t	O
from	O
T	O
labeled	O
,	O
the	O
defined	O
two	O
stances	O
,	O
and	O
its	O
60	O
most	O
relevant	O
documents	O
(	O
10	O
from	O
each	O
of	O
the	O
six	O
news	O
sources	O
)	O
,	O
for	O
each	O
document	O
,	O
we	O
ask	O
the	O
annotators	O
to	O
label	O
which	O
stance	O
it	O
belongs	O
to	O
and	O
label	O
it	O
as	O
0	O
or	O
1	O
;	O
if	O
the	O
annotator	O
is	O
not	O
able	O
to	O
perceive	O
a	O
clear	O
political	O
stance	O
,	O
then	O
the	O
annotator	O
will	O
label	O
it	O
as	O
-	O
1	O
.	O
For	O
each	O
document	O
,	O
the	O
majority	O
vote	O
of	O
the	O
three	O
labels	O
with	O
be	O
used	O
as	O
the	O
final	O
annotation	O
.	O
If	O
no	O
majority	O
vote	O
is	O
achieved	O
,	O
in	O
other	O
words	O
,	O
the	O
three	O
annotators	O
give	O
three	O
different	O
labels	O
to	O
a	O
document	O
,	O
then	O
a	O
fourth	O
annotator	O
will	O
read	O
the	O
document	O
again	O
and	O
decide	O
the	O
final	O
label	O
.	O
For	O
a	O
complete	O
list	O
of	O
all	O
document	O
labels	O
on	O
the	O
10	O
selected	O
topics	O
,	O
please	O
refer	O
to	O
our	O
public	O
repository	O
.	O

We	O
use	O
MALLET	B-DatasetName
5	O
topic	O
modeling	O
.	O
The	O
top	O
-	O
10	O
keywords	O
of	O
all	O
39	O
topics	O
are	O
shown	O
in	O
Table	O
6	O
.	O
Among	O
them	O
topic	O
0	O
,	O
3	O
,	O
4	O
,	O
14	O
,	O
16	O
,	O
26	O
,	O
35	O
,	O
36	O
,	O
37	O
are	O
not	O
used	O
in	O
further	O
analysis	O
because	O
after	O
reading	O
relevant	O
articles	O
we	O
find	O
that	O
they	O
are	O
more	O
about	O
advertisements	O
,	O
sport	O
events	O
,	O
gossip	O
news	O
and	O
recipes	O
and	O
etc	O
.	O
,	O
which	O
are	O
more	O
factual	O
and	O
convey	O
limited	O
media	O
ideologies	O
.	O
30	O
topics	O
are	O
left	O
after	O
removing	O
the	O
9	O
topics	O
.	O
Table	O
6	O
lists	O
the	O
top	O
-	O
10	O
keywords	O
of	O
the	O
30	O
topics	O
.	O

The	O
Strength	O
of	O
the	O
Weakest	O
Supervision	O
:	O
Topic	B-TaskName
Classification	I-TaskName
Using	O
Class	O
Labels	O

This	O
paper	O
proposes	O
a	O
VAT	B-MethodName
-	O
based	O
framework	O
to	O
improve	O
the	O
performance	O
and	O
interpretability	O
of	O
attentions	O
via	O
both	O
fine	O
-	O
tuning	O
and	O
compressing	O
.	O
The	O
experimental	O
results	O
on	O
eight	O
benchmark	O
datasets	O
for	O
text	B-TaskName
classification	I-TaskName
verify	O
the	O
effectiveness	O
of	O
our	O
models	O
within	O
this	O
framework	O
.	O
In	O
addition	O
,	O
we	O
apply	O
the	O
framework	O
for	O
sentiment	O
detection	O
,	O
which	O
further	O
demonstrates	O
the	O
superiority	O
in	O
terms	O
of	O
interpretability	O
.	O
It	O
is	O
also	O
interesting	O
to	O
find	O
that	O
training	O
the	O
models	O
by	O
fine	O
-	O
tuning	O
and	O
compressing	O
iteratively	O
is	O
effective	O
to	O
improve	O
the	O
text	O
representations	O
.	O
In	O
the	O
future	O
,	O
we	O
will	O
investigate	O
the	O
effectiveness	O
of	O
our	O
proposed	O
attention	O
framework	O
for	O
other	O
tasks	O
and	O
areas	O
,	O
such	O
as	O
machine	O
translation	O
and	O
visual	O
question	O
answering	O
.	O

In	O
this	O
section	O
we	O
describe	O
several	O
strategies	O
for	O
improving	O
the	O
role	O
-	O
playing	O
accuracy	O
of	O
dialogue	O
agents	O
,	O
specifically	O
ways	O
to	O
improve	O
our	O
transformer	B-MethodName
baselines	O
.	O

Word	O
embeddings	O
create	O
a	O
vector	O
-	O
space	O
representation	O
in	O
which	O
words	O
with	O
a	O
similar	O
meaning	O
are	O
in	O
close	O
proximity	O
.	O
Existing	O
approaches	O
to	O
make	O
embeddings	O
interpretable	O
,	O
e.g.	O
,	O
via	O
contextual	O
(	O
Subramanian	O
et	O
al	O
,	O
2018	O
)	O
,	O
sparse	O
embeddings	O
(	O
Panigrahi	O
et	O
al	O
,	O
2019	O
)	O
,	O
or	O
learned	O
(	O
Senel	O
et	O
al	O
,	O
2018	O
)	O
transformations	O
(	O
Mathew	O
et	O
al	O
,	O
2020	O
)	O
-	O
all	O
focus	O
on	O
text	O
only	O
.	O
Yet	O
,	O
emoji	O
are	O
widely	O
used	O
in	O
casual	O
communication	O
,	O
e.g.	O
,	O
Online	O
Social	O
Networks	O
(	O
OSN	O
)	O
,	O
and	O
are	O
known	O
to	O
extend	O
textual	O
expressiveness	O
,	O
demonstrated	O
to	O
benefit	O
,	O
e.g.	O
,	O
sentiment	O
analysis	O
(	O
Novak	O
et	O
al	O
,	O
2015	O
;	O
Hu	O
et	O
al	O
,	O
2017	O
)	O
.	O

We	O
study	O
the	O
extent	O
to	O
which	O
emoji	O
can	O
be	O
used	O
to	O
add	O
interpretability	O
to	O
embeddings	O
of	O
text	O
and	O
emoji	O
.	O
To	O
do	O
so	O
,	O
we	O
extend	O
the	O
POLAR	B-MethodName
-	O
framework	O
that	O
transforms	O
word	O
embeddings	O
to	O
interpretable	O
counterparts	O
and	O
apply	O
it	O
to	O
word	O
-	O
emoji	O
embeddings	O
trained	O
on	O
four	O
years	O
of	O
messaging	O
data	O
from	O
the	O
Jodel	O
social	O
network	O
.	O
We	O
devise	O
a	O
crowdsourced	O
human	O
judgement	O
experiment	O
to	O
study	O
six	O
usecases	O
,	O
evaluating	O
against	O
words	O
only	O
,	O
what	O
role	O
emoji	O
can	O
play	O
in	O
adding	O
interpretability	O
to	O
word	O
embeddings	O
.	O
That	O
is	O
,	O
we	O
use	O
a	O
revised	O
POLAR	B-MethodName
approach	O
interpreting	O
words	O
and	O
emoji	O
with	O
words	O
,	O
emoji	O
or	O
both	O
according	O
to	O
human	O
judgement	O
.	O
We	O
find	O
statistically	O
significant	O
trends	O
demonstrating	O
that	O
emoji	O
can	O
be	O
used	O
to	O
interpret	O
other	O
emoji	O
very	O
well	O
.	O

In	O
order	O
to	O
process	O
the	O
question	O
and	O
entire	O
documents	O
at	O
the	O
same	O
time	O
,	O
we	O
use	O
the	O
Longformer	B-MethodName
model	O
.	O
It	O
employs	O
an	O
attention	O
mechanism	O
scaling	O
linearly	O
with	O
the	O
sequence	O
length	O
which	O
enables	O
Longformer	B-MethodName
to	O
process	O
up	O
to	O
4	O
,	O
096	O
tokens	O
.	O
It	O
uses	O
multiple	O
attention	O
heads	O
with	O
different	O
dilation	O
configurations	O
to	O
attend	O
to	O
the	O
entire	O
sequence	O
and	O
includes	O
global	O
attention	O
to	O
question	O
tokens	O
in	O
the	O
sequence	O
.	O
Question	O
and	O
document	O
pairs	O
are	O
packed	O
together	O
into	O
a	O
single	O
sequence	O
without	O
having	O
to	O
use	O
sliding	O
windows	O
and	O
the	O
answer	O
span	O
is	O
calculated	O
by	O
a	O
dot	O
product	O
(	O
Beltagy	O
et	O
al	O
,	O
2020	O
)	O
.	O

SQuAD	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
is	O
a	O
factoid	O
span	O
detection	O
data	O
set	O
with	O
short	O
answers	O
.	O
Crowdworkers	O
generated	O
the	O
questions	O
given	O
a	O
set	O
of	O
articles	O
.	O
DROP	B-DatasetName
(	O
Dua	O
et	O
al	O
,	O
2019	O
)	O
makes	O
the	O
problem	O
more	O
challenging	O
by	O
adversarially	O
-	O
created	O
questions	O
requiring	O
discrete	O
reasoning	O
over	O
the	O
text	O
.	O
SQuAD	B-DatasetName
and	O
DROP	B-DatasetName
use	O
Wikipedia	O
pages	O
as	O
context	O
passages	O
whereas	O
SearchQA	B-DatasetName
(	O
Dunn	O
et	O
al	O
,	O
2017	O
)	O
uses	O
IR	O
approaches	O
to	O
collect	O
context	O
passages	O
.	O
Answer	O
generation	O
based	O
on	O
a	O
set	O
of	O
passages	O
is	O
another	O
approach	O
to	O
address	O
this	O
task	O
.	O
MS	B-DatasetName
MARCO	I-DatasetName
(	O
Bajaj	O
et	O
al	O
,	O
2016	O
)	O
consists	O
of	O
real	O
-	O
world	O
search	O
queries	O
and	O
retrieved	O
documents	O
corresponding	O
to	O
the	O
queries	O
.	O
There	O
are	O
also	O
different	O
types	O
of	O
QA	B-TaskName
data	O
sets	O
such	O
as	O
Antique	O
(	O
Hashemi	O
et	O
al	O
,	O
2020	O
)	O
,	O
which	O
is	O
a	O
data	O
set	O
for	O
answer	O
retrieval	O
for	O
non	O
-	O
factoid	O
ques	O
-	O
tions	O
.	O
There	O
is	O
also	O
a	O
range	O
of	O
multiple	O
-	O
choice	O
QA	B-TaskName
tasks	O
such	O
as	O
RACE	B-DatasetName
(	O
Lai	O
et	O
al	O
,	O
2017	O
)	O
,	O
ARC	B-DatasetName
(	O
Clark	O
et	O
al	O
,	O
2018	O
)	O
,	O
SWAQ	B-DatasetName
(	O
Zellers	O
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
COS	B-DatasetName
-	I-DatasetName
MOS	I-DatasetName
QA	I-DatasetName
(	O
Huang	O
et	O
al	O
,	O
2019	O
)	O
that	O
are	O
clustered	O
together	O
with	O
the	O
short	O
-	O
context	O
QA	B-TaskName
data	O
sets	O
.	O

NLQuAD	B-DatasetName
:	O
A	O
Non	O
-	O
Factoid	O
Long	O
Question	O
Answering	O
Data	O
Set	O

Fully	B-MethodName
Quantized	I-MethodName
Transformer	I-MethodName
for	O
Machine	B-TaskName
Translation	I-TaskName

Transformer	O
based	O
Natural	O
Language	O
Generation	O
for	O
Question	B-TaskName
-	I-TaskName
Answering	I-TaskName

In	O
this	O
section	O
,	O
we	O
describe	O
the	O
datasets	O
using	O
in	O
our	O
experiments	O
on	O
many	O
-	O
to	O
-	O
many	O
and	O
one	O
-	O
to	O
-	O
many	O
multilingual	O
translation	O
scenarios	O
.	O
Many	O
-	O
to	O
-	O
Many	O
For	O
this	O
translation	O
scenario	O
,	O
we	O
test	O
our	O
approach	O
on	O
IWSLT	B-DatasetName
-	O
17	O
1	O
translation	O
datasets	O
,	O
including	O
English	O
,	O
Italian	O
,	O
Romanian	O
,	O
Dutch	O
(	O
briefly	O
,	O
En	O
,	O
It	O
,	O
Ro	O
,	O
Nl	O
)	O
.	O
We	O
experimented	O
in	O
eight	O
directions	O
,	O
including	O
It↔En	O
,	O
Ro↔En	O
,	O
Nl↔En	O
,	O
and	O
It↔Ro	O
,	O
with	O
231.6k	O
,	O
220.5k	O
,	O
237.2k	O
,	O
and	O
217.5k	O
data	O
for	O
each	O
language	O
pair	O
.	O
We	O
choose	O
test2016	O
and	O
test2017	O
as	O
our	O
development	O
and	O
test	O
set	O
,	O
respectively	O
.	O
Sentences	O
of	O
all	O
languages	O
were	O
tokenized	O
by	O
the	O
Moses	O
scripts	O
2	O
and	O
further	O
segmented	O
into	O
subword	O
symbols	O
using	O
Byte	O
-	O
Pair	O
Encoding	O
(	O
BPE	O
)	O
rules	O
(	O
Sennrich	O
et	O
al	O
,	O
2016	O
)	O
with	O
40	O
K	O
merge	O
operations	O
for	O
all	O
languages	O
jointly	O
.	O
One	O
-	O
to	O
-	O
Many	O
We	O
evaluate	O
the	O
quality	O
of	O
our	O
multilingual	O
translation	O
models	O
using	O
training	O
data	O
from	O
the	O
Europarl	B-DatasetName
Corpus	O
3	O
,	O
Release	O
V7	O
.	O
Our	O
experiments	O
focus	O
on	O
English	O
to	O
twelve	O
primary	O
languages	O
:	O
Czech	O
,	O
Finnish	O
,	O
Greek	O
,	O
Hungarian	O
,	O
Lithuanian	O
,	O
Latvian	O
,	O
Polish	O
,	O
Portuguese	O
,	O
Slovak	O
,	O
Slovene	O
,	O
Swedish	O
,	O
Spanish	O
(	O
briefly	O
,	O
Cs	O
,	O
Fi	O
,	O
El	O
,	O
Hu	O
,	O
Lt	O
,	O
Lv	O
,	O
Pl	O
,	O
Pt	O
,	O
Sk	O
,	O
Sl	O
,	O
Sv	O
,	O
Es	O
)	O
.	O
For	O
each	O
language	O
pair	O
,	O
we	O
randomly	O
sampled	O
0.6	O
M	O
parallel	O
sentences	O
as	O
training	O
corpus	O
(	O
7.2	O
M	O
in	O
all	O
)	O
.	O
The	O
Europarl	B-DatasetName
evaluation	O
data	O
set	O
dev2006	O
is	O
used	O
as	O
our	O
validation	O
set	O
,	O
while	O
devtest2006	O
is	O
our	O
test	O
set	O
.	O
For	O
language	O
pairs	O
without	O
available	O
development	O
and	O
test	O
set	O
,	O
we	O
randomly	O
split	O
1	O
K	O
unseen	O
sentence	O
pairs	O
from	O
the	O
corresponding	O
training	O
set	O
as	O
the	O
development	O
and	O
test	O
data	O
respectively	O
.	O
We	O
tokenize	O
and	O
truecase	O
the	O
sentences	O
with	O
Moses	O
scripts	O
and	O
apply	O
a	O
jointly	O
-	O
learned	O
set	O
of	O
90k	O
BPE	O
obtained	O
from	O
the	O
merged	O
source	O
and	O
target	O
sides	O
of	O
the	O
training	O
data	O
for	O
all	O
twelve	O
language	O
pairs	O
.	O

Multilingual	B-TaskName
neural	I-TaskName
machine	I-TaskName
translation	I-TaskName
with	O
a	O
single	O
model	O
has	O
drawn	O
much	O
attention	O
due	O
to	O
its	O
capability	O
to	O
deal	O
with	O
multiple	O
languages	O
.	O
However	O
,	O
the	O
current	O
multilingual	O
translation	O
paradigm	O
often	O
makes	O
the	O
model	O
tend	O
to	O
preserve	O
the	O
general	O
knowledge	O
,	O
but	O
ignore	O
the	O
language	O
-	O
specific	O
knowledge	O
.	O
Some	O
previous	O
works	O
try	O
to	O
solve	O
this	O
problem	O
by	O
adding	O
various	O
kinds	O
of	O
language	O
-	O
specific	O
modules	O
to	O
the	O
model	O
,	O
but	O
they	O
suffer	O
from	O
the	O
parameter	O
explosion	O
problem	O
and	O
require	O
specialized	O
manual	O
design	O
.	O
To	O
solve	O
these	O
problems	O
,	O
we	O
propose	O
to	O
divide	O
the	O
model	O
neurons	O
into	O
general	O
and	O
language	O
-	O
specific	O
parts	O
based	O
on	O
their	O
importance	O
across	O
languages	O
.	O
The	O
general	O
part	O
is	O
responsible	O
for	O
preserving	O
the	O
general	O
knowledge	O
and	O
participating	O
in	O
the	O
translation	O
of	O
all	O
the	O
languages	O
,	O
while	O
the	O
language	O
-	O
specific	O
part	O
is	O
responsible	O
for	O
preserving	O
the	O
languagespecific	O
knowledge	O
and	O
participating	O
in	O
the	O
translation	O
of	O
some	O
specific	O
languages	O
.	O
Experimental	O
results	O
on	O
several	O
language	O
pairs	O
,	O
covering	O
IWSLT	B-DatasetName
and	O
Europarl	B-DatasetName
corpus	O
datasets	O
,	O
demonstrate	O
the	O
effectiveness	O
and	O
universality	O
of	O
the	O
proposed	O
method	O
.	O

Multilingual	B-MethodName
Code	I-MethodName
-	I-MethodName
Switching	I-MethodName
for	O
Zero	B-TaskName
-	I-TaskName
Shot	I-TaskName
Cross	I-TaskName
-	I-TaskName
Lingual	I-TaskName
Intent	I-TaskName
Prediction	I-TaskName
and	O
Slot	B-TaskName
Filling	I-TaskName

This	O
preliminary	O
work	O
demonstrates	O
that	O
a	O
single	O
NLU	O
model	O
can	O
perform	O
simultaneous	O
slot	B-TaskName
filling	I-TaskName
,	O
translation	B-TaskName
,	O
intent	B-TaskName
classification	I-TaskName
,	O
and	O
language	B-TaskName
identification	I-TaskName
across	O
7	O
languages	O
using	O
MultiATIS++	B-DatasetName
.	O
Such	O
an	O
NLU	O
model	O
would	O
negate	O
the	O
need	O
for	O
multiple	O
-	O
language	O
support	O
in	O
some	O
portion	O
of	O
downstream	O
system	O
components	O
.	O
Performance	O
is	O
not	O
irreconcilably	O
worse	O
than	O
traditional	O
slot	B-TaskName
-	I-TaskName
filling	I-TaskName
models	O
,	O
and	O
performance	O
is	O
statistically	O
equivalent	O
with	O
a	O
small	O
amount	O
of	O
additional	O
training	O
data	O
.	O
Looking	O
forward	O
,	O
a	O
more	O
challenging	O
dataset	O
is	O
needed	O
to	O
further	O
develop	O
the	O
translation	O
compo	O
-	O
nent	O
of	O
the	O
STIL	O
task	O
.	O
The	O
English	O
MultiATIS++	B-DatasetName
test	O
set	O
only	O
contains	O
455	O
unique	O
entity	O
-	O
slot	O
pairs	O
.	O
An	O
ideal	O
future	O
dataset	O
would	O
include	O
freeform	O
and	O
varied	O
content	O
,	O
such	O
as	O
text	O
messages	O
,	O
song	O
titles	O
,	O
or	O
open	O
-	O
domain	O
questions	O
.	O
Until	O
then	O
,	O
work	O
remains	O
to	O
achieve	O
parity	O
with	O
English	O
-	O
only	O
ATIS	B-DatasetName
models	O
.	O

The	O
Airline	B-DatasetName
Travel	I-DatasetName
Information	I-DatasetName
System	I-DatasetName
(	O
ATIS	B-DatasetName
)	O
dataset	O
is	O
a	O
classic	O
benchmark	O
for	O
goal	O
-	O
oriented	O
NLU	B-TaskName
(	O
Price	O
,	O
1990	O
;	O
Tur	O
et	O
al	O
,	O
2010	O
)	O
.	O
It	O
contains	O
utterances	O
focused	O
on	O
airline	O
travel	O
,	O
such	O
as	O
how	O
much	O
is	O
the	O
cheapest	O
flight	O
from	O
Boston	O
to	O
New	O
York	O
tomorrow	O
morning	O
?	O
The	O
dataset	O
is	O
annotated	O
with	O
17	O
intents	O
,	O
though	O
the	O
distribution	O
is	O
skewed	O
,	O
with	O
70	O
%	O
of	O
intents	O
being	O
the	O
flight	O
intent	O
.	O
Slots	O
are	O
labeled	O
using	O
the	O
Beginning	O
Inside	O
Outside	O
(	O
BIO	O
)	O
format	O
.	O
ATIS	B-DatasetName
was	O
localized	O
to	O
Turkish	O
and	O
Hindi	O
in	O
2018	O
,	O
forming	O
MultiATIS	B-DatasetName
(	O
Upadhyay	O
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
then	O
to	O
Spanish	O
,	O
Portuguese	O
,	O
German	O
,	O
French	O
,	O
Chinese	O
,	O
and	O
Japanese	O
in	O
2020	O
,	O
forming	O
Multi	B-DatasetName
-	I-DatasetName
ATIS++	I-DatasetName
(	O
Xu	O
et	O
al	O
,	O
2020	O
)	O
.	O
In	O
this	O
work	O
,	O
Portuguese	O
was	O
excluded	O
due	O
to	O
a	O
lack	O
of	O
Portuguese	O
pretraining	O
in	O
the	O
publicly	O
available	O
mBART	O
model	O
,	O
and	O
Japanese	O
was	O
excluded	O
due	O
to	O
a	O
current	O
lack	O
of	O
alignment	O
between	O
Japanese	O
and	O
English	O
samples	O
in	O
MultiATIS++	B-DatasetName
.	O
Hindi	O
and	O
Turkish	O
data	O
were	O
taken	O
from	O
Multi	B-DatasetName
-	I-DatasetName
ATIS	I-DatasetName
,	O
and	O
the	O
training	O
data	O
were	O
upsampled	O
by	O
3x	O
for	O
Hindi	O
and	O
7x	O
for	O
Turkish	O
.	O
Prior	O
to	O
any	O
upsampling	O
,	O
there	O
were	O
4	O
,	O
488	O
training	O
samples	O
for	O
English	O
,	O
Spanish	O
,	O
German	O
,	O
French	O
,	O
and	O
Chinese	O
.	O
The	O
test	O
sets	O
contained	O
893	O
samples	O
for	O
all	O
languages	O
except	O
Turkish	O
,	O
which	O
had	O
715	O
samples	O
.	O
For	O
English	O
,	O
Spanish	O
,	O
German	O
,	O
French	O
,	O
and	O
Chinese	O
,	O
validation	O
sets	O
of	O
490	O
samples	O
were	O
used	O
in	O
all	O
cases	O
.	O
Given	O
the	O
smaller	O
data	O
quantities	O
for	O
Hindi	O
and	O
Turkish	O
,	O
two	O
training	O
and	O
validation	O
set	O
configurations	O
were	O
considered	O
.	O
The	O
first	O
configuration	O

Multilingual	B-TaskName
Natural	I-TaskName
Language	I-TaskName
Understanding	I-TaskName
(	O
NLU	B-TaskName
)	O
,	O
also	O
called	O
cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
NLU	I-TaskName
,	O
is	O
a	O
technique	O
by	O
which	O
an	O
NLU	B-TaskName
-	O
based	O
system	O
can	O
scale	O
to	O
multiple	O
languages	O
.	O
A	O
single	O
model	O
is	O
trained	O
on	O
more	O
than	O
one	O
language	O
,	O
and	O
it	O
can	O
accept	O
input	O
from	O
more	O
than	O
one	O
language	O
during	O
inference	O
.	O
In	O
most	O
recent	O
high	O
-	O
performing	O
systems	O
,	O
a	O
model	O
is	O
first	O
pre	O
-	O
trained	O
using	O
unlabeled	O
data	O
for	O
all	O
supported	O
languages	O
and	O
then	O
fine	O
tuned	O
for	O
a	O
specific	O
task	O
using	O
a	O
small	O
set	O
of	O
labeled	O
data	O
(	O
Conneau	O
and	O
Lample	O
,	O
2019	O
;	O
Pires	O
et	O
al	O
,	O
2019	O
)	O
.	O
Two	O
typical	O
tasks	O
for	O
goal	O
-	O
based	O
systems	O
,	O
such	O
as	O
virtual	O
assistants	O
and	O
chatbots	O
,	O
are	O
intent	B-TaskName
classification	I-TaskName
and	O
slot	B-TaskName
filling	I-TaskName
(	O
Gupta	O
et	O
al	O
,	O
2006	O
)	O
.	O
Though	O
intent	B-TaskName
classification	I-TaskName
creates	O
a	O
language	O
agnostic	O
output	O
(	O
the	O
intent	O
of	O
the	O
user	O
)	O
,	O
slot	B-TaskName
filling	I-TaskName
does	O
not	O
.	O
Instead	O
,	O
a	O
slot	O
-	O
filling	O
model	O
outputs	O
the	O
labels	O
for	O
each	O
of	O
input	O
tokens	O
from	O
the	O
user	O
.	O
Suppose	O
the	O
slot	O
-	O
filling	O
model	O
can	O
handle	O
L	O
languages	O
.	O
Downstream	O
components	O
must	O
therefore	O
handle	O
all	O
L	O
languages	O
for	O
the	O
full	O
system	O
to	O
be	O
multilingual	O
across	O
L	O
languages	O
.	O
Machine	O
translation	O
could	O
be	O
performed	O
before	O
the	O
slot	O
filling	O
model	O
at	O
system	O
runtime	O
,	O
though	O
the	O
latency	O
would	O
be	O
fully	O
additive	O
,	O
and	O
some	O
amount	O
of	O
information	O
useful	O
to	O
the	O
slotfilling	O
model	O
may	O
be	O
lost	O
.	O
Similarly	O
,	O
translation	O
could	O
occur	O
after	O
the	O
slot	O
-	O
filling	O
model	O
at	O
runtime	O
,	O
but	O
slot	O
alignment	O
between	O
the	O
source	O
and	O
target	O
language	O
is	O
a	O
non	O
-	O
trivial	O
task	O
(	O
Jain	O
et	O
al	O
,	O
2019	O
;	O
Xu	O
et	O
al	O
,	O
2020	O
)	O
.	O
Instead	O
,	O
the	O
goal	O
of	O
this	O
work	O
was	O
to	O
build	O
a	O
single	O
model	O
that	O
can	O
simultaneously	O
translate	O
the	O
input	O
,	O
output	O
slotted	O
text	O
in	O
a	O
single	O
language	O
(	O
English	O
)	O
,	O
classify	O
the	O
intent	O
,	O
and	O
classify	O
the	O
input	O
language	O
(	O
See	O
Table	O
1	O
)	O
.	O
The	O
STIL	B-TaskName
task	O
is	O
defined	O
such	O
that	O
the	O
input	O
language	O
tag	O
is	O
not	O
given	O
to	O
the	O
model	O
as	O
input	O
.	O
Thus	O
,	O
language	O
identification	O
is	O
necessary	O
so	O
that	O
the	O
system	O
can	O
communicate	O
back	O
to	O
the	O
user	O
in	O
the	O
correct	O
language	O
.	O
In	O
all	O
STIL	B-TaskName
cases	O
,	O
the	O
output	O
is	O
in	O
English	O
.	O
Each	O
token	O
is	O
followed	O
by	O
its	O
BIO	O
-	O
tagged	O
slot	O
label	O
.	O
The	O
sequence	O
of	O
tokens	O
and	O
slots	O
are	O
followed	O
by	O
the	O
intent	O
and	O
then	O
the	O
language	O
.	O
sification	O
,	O
and	O
Language	O
identification	O
(	O
STIL	B-TaskName
)	O
;	O
(	O
2	O
)	O
both	O
non	O
-	O
translated	O
and	O
STIL	B-TaskName
results	O
using	O
the	O
mBART	B-MethodName
model	O
(	O
Liu	O
et	O
al	O
,	O
2020	O
)	O
trained	O
using	O
a	O
fully	O
text	O
-	O
to	O
-	O
text	O
data	O
format	O
;	O
and	O
(	O
3	O
)	O
public	O
release	O
of	O
source	O
code	O
used	O
in	O
this	O
study	O
,	O
with	O
a	O
goal	O
toward	O
reproducibility	O
and	O
future	O
work	O
on	O
the	O
STIL	B-TaskName
task	O
1	O
.	O

STIL	B-TaskName
-	O
Simultaneous	B-TaskName
Slot	I-TaskName
Filling	I-TaskName
,	I-TaskName
Translation	I-TaskName
,	I-TaskName
Intent	I-TaskName
Classification	I-TaskName
,	I-TaskName
and	I-TaskName
Language	I-TaskName
Identification	I-TaskName
:	O
Initial	O
Results	O
using	O
mBART	B-MethodName
on	O
MultiATIS++	B-DatasetName

Previous	O
approaches	O
for	O
intent	B-TaskName
classification	I-TaskName
and	O
slot	B-TaskName
filling	I-TaskName
have	O
used	O
either	O
(	O
1	O
)	O
separate	O
models	O
for	O
slot	O
filling	O
,	O
including	O
support	O
vector	O
machines	O
(	O
Moschitti	O
et	O
al	O
,	O
2007	O
)	O
,	O
conditional	O
random	O
fields	O
(	O
Xu	O
and	O
Sarikaya	O
,	O
2014	O
)	O
,	O
and	O
recurrent	O
neural	O
networks	O
of	O
various	O
types	O
(	O
Kurata	O
et	O
al	O
,	O
2016	O
)	O
or	O
(	O
2	O
)	O
joint	O
models	O
that	O
diverge	O
into	O
separate	O
decoders	O
or	O
layers	O
for	O
intent	B-TaskName
classification	I-TaskName
and	O
slot	B-TaskName
filling	I-TaskName
(	O
Xu	O
and	O
Sarikaya	O
,	O
2013	O
;	O
Guo	O
et	O
al	O
,	O
2014	O
;	O
Liu	O
and	O
Lane	O
,	O
2016	O
;	O
Hakkani	O
-	O
Tür	O
et	O
al	O
,	O
2016	O
)	O
or	O
that	O
share	O
hidden	O
states	O
(	O
Wang	O
et	O
al	O
,	O
2018	O
)	O
.	O
In	O
this	O
work	O
,	O
a	O
fully	O
text	O
-	O
to	O
-	O
text	O
approach	O
similar	O
to	O
that	O
of	O
the	O
T5	O
model	O
was	O
used	O
,	O
such	O
that	O
the	O
model	O
would	O
have	O
maximum	O
information	O
sharing	O
across	O
the	O
four	O
STIL	O
sub	O
-	O
tasks	O
.	O
Encoder	O
-	O
decoder	O
models	O
,	O
first	O
introduced	O
in	O
2014	O
(	O
Sutskever	O
et	O
al	O
,	O
2014	O
)	O
,	O
are	O
a	O
mainstay	O
of	O
neural	O
machine	O
translation	O
.	O
The	O
original	O
transformer	O
model	O
included	O
both	O
an	O
encoder	O
and	O
a	O
decoder	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
Since	O
then	O
,	O
much	O
of	O
the	O
work	O
on	O
transformers	O
focuses	O
on	O
models	O
with	O
only	O
an	O
encoder	O
pretrained	O
with	O
autoencoding	O
techniques	O
(	O
e.g.	O
BERT	O
by	O
Devlin	O
et	O
al	O
(	O
2018	O
)	O
)	O
or	O
auto	O
-	O
regressive	O
models	O
with	O
only	O
a	O
decoder	O
(	O
e.g.	O
GPT	O
by	O
Radford	O
(	O
2018	O
)	O
)	O
.	O
In	O
this	O
work	O
,	O
it	O
was	O
assumed	O
that	O
encoder	O
-	O
decoder	O
models	O
,	O
such	O
as	O
BART	O
(	O
Lewis	O
et	O
al	O
,	O
2019	O
)	O
and	O
T5	O
(	O
Raffel	O
et	O
al	O
,	O
2019	O
)	O
,	O
are	O
the	O
best	O
architectural	O
candidates	O
given	O
the	O
translation	O
component	O
of	O
the	O
STIL	B-TaskName
task	O
,	O
as	O
well	O
as	O
past	O
state	O
of	O
the	O
art	O
advancement	O
by	O
encoder	O
-	O
decoder	O
models	O
on	O
ATIS	O
,	O
cited	O
above	O
.	O
Rigorous	O
architectural	O
comparisons	O
are	O
left	O
to	O
future	O
work	O
.	O

We	O
studied	O
the	O
problem	O
of	O
training	O
topic	O
classifiers	O
using	O
only	O
class	O
labels	O
.	O
Experiments	O
on	O
six	O
data	O
sets	O
show	O
that	O
class	O
labels	O
can	O
save	O
a	O
significant	O
amount	O
of	O
labeled	O
examples	O
in	O
the	O
beginning	O
.	O
Retrieval	O
-	O
based	O
and	O
semi	O
-	O
supervised	O
methods	O
tend	O
to	O
perform	O
better	O
on	O
long	O
documents	O
,	O
while	O
the	O
proposed	O
method	O
performs	O
better	O
on	O
short	O
documents	O
.	O
This	O
study	O
opens	O
up	O
many	O
interesting	O
avenues	O
for	O
future	O
work	O
.	O
First	O
,	O
we	O
introduce	O
a	O
new	O
perspective	O
on	O
text	O
classification	O
:	O
can	O
we	O
build	O
a	O
text	O
classifier	O
by	O
just	O
providing	O
a	O
short	O
description	O
of	O
each	O
class	O
?	O
This	O
is	O
a	O
more	O
challenging	O
(	O
but	O
more	O
user	O
-	O
friendly	O
)	O
setup	O
than	O
standard	O
supervised	O
classification	O
.	O
Second	O
,	O
future	O
work	O
can	O
investigate	O
tasks	O
such	O
as	O
sentiment	O
and	O
emotion	O
classification	O
,	O
which	O
are	O
more	O
challenging	O
than	O
topic	O
classification	O
tasks	O
.	O
Third	O
,	O
the	O
two	O
approaches	O
-	O
leveraging	O
unlabeled	O
data	O
(	O
retrievalbased	O
and	O
semi	O
-	O
supervised	O
methods	O
)	O
and	O
leveraging	O
pretrained	O
models	O
(	O
the	O
proposed	O
method	O
)	O
could	O
be	O
combined	O
to	O
give	O
robust	O
performance	O
on	O
both	O
short	O
and	O
long	O
documents	O
.	O
Finally	O
,	O
we	O
can	O
invite	O
users	O
into	O
the	O
training	O
loop	O
:	O
in	O
addition	O
to	O
labeling	O
documents	O
,	O
users	O
can	O
also	O
revise	O
the	O
class	O
definitions	O
to	O
improve	O
the	O
classifier	O
.	O

We	O
consider	O
six	O
topic	O
classification	O
data	O
sets	O
with	O
different	O
document	O
lengths	O
and	O
application	O
domains	O
.	O
Table	O
1	O
summarizes	O
basic	O
statistics	O
of	O
these	O
data	O
sets	O
.	O
Table	O
4	O
and	O
Three	O
short	O
text	O
data	O
sets	O
are	O
(	O
1	O
)	O
Wiki	B-DatasetName
Titles	I-DatasetName
:	O
Wikipedia	O
article	O
titles	O
sampled	O
from	O
15	O
main	O
categories	O
(	O
Wikipedia	O
Main	O
Topic	O
)	O
.	O
(	O
2	O
)	O
News	B-DatasetName
Titles	I-DatasetName
:	O
The	O
UCI	B-DatasetName
news	I-DatasetName
title	I-DatasetName
data	I-DatasetName
set	I-DatasetName
(	O
Lichman	O
,	O
2013	O
)	O
.	O
(	O
3	O
)	O
Y	B-DatasetName
Questions	I-DatasetName
:	O
User	O
-	O
posted	O
questions	O
in	O
Yahoo	O
Answers	O
(	O
Yahoo	O
Language	O
Data	O
,	O
2007	O
)	O
.	O
Three	O
long	O
text	O
data	O
sets	O
are	O
(	O
1	O
)	O
20	B-DatasetName
News	I-DatasetName
:	O
The	O
well	O
-	O
known	O
20	B-DatasetName
newsgroup	I-DatasetName
data	I-DatasetName
set	I-DatasetName
.	O
(	O
2	O
)	O
Reuters	B-DatasetName
.	O
The	O
Reuters	B-DatasetName
-	I-DatasetName
21578	I-DatasetName
data	I-DatasetName
set	I-DatasetName
(	O
Lewis	O
)	O
.	O
We	O
take	O
the	O
articles	O
from	O
the	O
10	O
largest	O
topics	O
.	O
(	O
3	O
)	O
Med	B-DatasetName
WSD	I-DatasetName
:	O
The	O
MeSH	B-DatasetName
word	O
sense	O
disambiguation	O
(	O
WSD	O
)	O
data	O
set	O
(	O
Jimeno	O
-	O
Yepes	O
et	O
al	O
,	O
2011	O
)	O
.	O
Each	O
WSD	O
task	O
aims	O
to	O
tell	O
the	O
sense	O
(	O
meaning	O
)	O
of	O
an	O
ambiguous	O
term	O
in	O
a	O
MEDLINE	O
abstract	O
.	O
For	O
instance	O
,	O
the	O
term	O
"	O
cold	O
"	O
may	O
refer	O
to	O
Low	O
Temperature	O
,	O
Common	O
Cold	O
,	O
or	O
Chronic	O
Obstructive	O
Lung	O
Disease	O
,	O
depending	O
on	O
its	O
context	O
.	O
These	O
senses	O
are	O
used	O
as	O
the	O
class	O
labels	O
.	O
We	O
use	O
198	O
ambiguous	O
words	O
with	O
at	O
least	O
100	O
labeled	O
abstracts	O
in	O
the	O
data	O
set	O
,	O
and	O
report	O
the	O
average	O
statistics	O
over	O
198	O
independent	O
classification	O
tasks	O
.	O
Although	O
no	O
true	O
labels	O
are	O
used	O
for	O
training	O
,	O
some	O
methods	O
require	O
unlabeled	O
data	O
for	O
retrieval	O
,	O
pseudo	O
-	O
labeling	O
,	O
and	O
re	O
-	O
training	O
.	O
We	O
split	O
unlabeled	O
data	O
into	O
5	O
folds	O
,	O
using	O
4	O
folds	O
to	O
"	O
train	O
"	O
a	O
classifier	O
and	O
1	O
fold	O
for	O
test	O
.	O
We	O
use	O
macroaveraged	B-MetricName
F	I-MetricName
1	I-MetricName
as	O
the	O
performance	O
metric	O
because	O
not	O
all	O
data	O
sets	O
have	O
a	O
balanced	O
class	O
distribution	O
.	O

We	O
compare	O
a	O
variety	O
of	O
methods	O
on	O
six	O
topic	O
classification	O
data	O
sets	O
.	O
The	O
goals	O
are	O
(	O
1	O
)	O
to	O
study	O
the	O
best	O
classification	O
performance	O
achievable	O
using	O
class	O
labels	O
only	O
,	O
and	O
(	O
2	O
)	O
to	O
estimate	O
the	O
equivalent	O
amount	O
of	O
true	O
labels	O
needed	O
to	O
achieve	O
the	O
same	O
warm	O
-	O
start	O
performance	O
.	O

We	O
explored	O
statistical	O
and	O
neural	O
models	O
for	O
noun	B-TaskName
ellipsis	I-TaskName
detection	I-TaskName
and	I-TaskName
resolution	I-TaskName
,	O
presenting	O
a	O
strong	O
results	O
for	O
this	O
task	O
.	O
As	O
expected	O
,	O
neural	O
classifiers	O
perform	O
significantly	O
better	O
than	O
the	O
statistical	O
with	O
the	O
same	O
input	O
representation	O
.	O
As	O
with	O
several	O
other	O
NLP	O
tasks	O
,	O
the	O
contextual	O
nature	O
of	O
BERT	B-MethodName
is	O
useful	O
for	O
noun	B-TaskName
ellipsis	I-TaskName
resolution	I-TaskName
too	O
,	O
making	O
robust	O
predictions	O
with	O
simple	O
neural	O
classifiers	O
.	O
Finally	O
,	O
addition	O
of	O
manual	O
features	O
boosts	O
the	O
performance	O
of	O
all	O
classifiers	O
including	O
those	O
that	O
use	O
BERT	B-MethodName
,	O
highlighting	O
that	O
ellipsis	O
is	O
a	O
syntactically	O
constrained	O
phenomenon	O
.	O

We	O
explore	O
utilizing	O
an	O
unlikelihood	B-MetricName
(	O
UL	B-MetricName
)	O
loss	O
While	O
training	O
on	O
the	O
LIGHT	B-DatasetName
dataset	O
with	O
standard	O
NLL	B-MetricName
loss	O
,	O
with	O
some	O
fixed	O
probability	O
we	O
consider	O
a	O
candidate	O
model	O
generation	O
for	O
UL	B-MetricName
loss	O
.	O
The	O
full	O
generation	O
is	O
sent	O
to	O
the	O
RPA	O
classifier	O
;	O
if	O
the	O
generation	O
is	O
classified	O
as	O
coming	O
from	O
the	O
incorrect	O
character	O
,	O
we	O
examine	O
each	O
partial	O
generated	O
sequence	O
of	O
the	O
output	O
,	O
and	O
send	O
these	O
sequences	O
to	O
the	O
LTR	O
RPA	O
classifier	O
to	O
determine	O
whether	O
the	O
candidate	O
partial	O
sequences	O
match	O
the	O
model	O
's	O
character	O
.	O
We	O
apply	O
UL	O
loss	O
to	O
tokens	O
that	O
yield	O
the	O
wrong	O
character	O
classification	O
.	O

We	O
tackle	O
various	O
ABSA	B-TaskName
tasks	O
in	O
a	O
novel	O
generative	O
framework	O
in	O
this	O
paper	O
.	O
By	O
formulating	O
the	O
target	O
sentences	O
with	O
our	O
proposed	O
annotation	O
-	O
style	O
and	O
extraction	O
-	O
style	O
paradigms	O
,	O
we	O
solve	O
multiple	O
sentiment	O
pair	O
or	O
triplet	O
extraction	O
tasks	O
with	O
a	O
unified	O
generation	O
model	O
.	O
Extensive	O
experiments	O
on	O
multiple	O
benchmarks	O
across	O
four	O
ABSA	B-TaskName
tasks	O
show	O
the	O
effectiveness	O
of	O
our	O
proposed	O
method	O
.	O
Our	O
work	O
is	O
an	O
initial	O
attempt	O
on	O
transforming	O
ABSA	B-TaskName
tasks	O
,	O
which	O
are	O
typically	O
treated	O
as	O
classification	O
problems	O
,	O
into	O
text	O
generation	O
problems	O
.	O
Experimental	O
results	O
indicate	O
that	O
such	O
transformation	O
is	O
an	O
effective	O
solution	O
to	O
tackle	O
various	O
ABSA	B-TaskName
tasks	O
.	O
Following	O
this	O
direction	O
,	O
designing	O
more	O
effective	O
generation	O
paradigms	O
and	O
extending	O
such	O
ideas	O
to	O
other	O
tasks	O
can	O
be	O
interesting	O
research	O
problems	O
for	O
future	O
work	O
.	O

We	O
adopt	O
F1	B-MetricName
scores	I-MetricName
as	O
the	O
main	O
evaluation	O
metrics	O
for	O
all	O
tasks	O
.	O
A	O
prediction	O
is	O
correct	O
if	O
and	O
only	O
if	O
all	O
its	O
predicted	O
sentiment	O
elements	O
in	O
the	O
pair	O
or	O
triplet	O
are	O
correct	O
.	O

Aspect	B-TaskName
-	I-TaskName
based	I-TaskName
sentiment	I-TaskName
analysis	I-TaskName
(	O
ABSA	B-TaskName
)	O
has	O
received	O
increasing	O
attention	O
recently	O
.	O
Most	O
existing	O
work	O
tackles	O
ABSA	B-TaskName
in	O
a	O
discriminative	O
manner	O
,	O
designing	O
various	O
task	O
-	O
specific	O
classification	O
networks	O
for	O
the	O
prediction	O
.	O
Despite	O
their	O
effectiveness	O
,	O
these	O
methods	O
ignore	O
the	O
rich	O
label	O
semantics	O
in	O
ABSA	B-TaskName
problems	O
and	O
require	O
extensive	O
task	O
-	O
specific	O
designs	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
to	O
tackle	O
various	O
ABSA	B-TaskName
tasks	O
in	O
a	O
unified	O
generative	O
framework	O
.	O
Two	O
types	O
of	O
paradigms	O
,	O
namely	O
annotation	O
-	O
style	O
and	O
extraction	O
-	O
style	O
modeling	O
,	O
are	O
designed	O
to	O
enable	O
the	O
training	O
process	O
by	O
formulating	O
each	O
ABSA	B-TaskName
task	O
as	O
a	O
text	O
generation	O
problem	O
.	O
We	O
conduct	O
experiments	O
on	O
four	O
ABSA	B-TaskName
tasks	O
across	O
multiple	O
benchmark	O
datasets	O
where	O
our	O
proposed	O
generative	O
approach	O
achieves	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
in	O
almost	O
all	O
cases	O
.	O
This	O
also	O
validates	O
the	O
strong	O
generality	O
of	O
the	O
proposed	O
framework	O
which	O
can	O
be	O
easily	O
adapted	O
to	O
arbitrary	O
ABSA	B-TaskName
task	O
without	O
additional	O
taskspecific	O
model	O
design	O
.	O
1	O

The	O
original	O
paper	O
(	O
Zhang	O
et	O
al	O
,	O
2021	O
)	O
evaluated	O
dual	O
transfer	O
only	O
with	O
Transformer	O
base	O
.	O
In	O
this	O
shared	O
task	O
,	O
we	O
scale	O
up	O
to	O
Transformer	O
big	O
.	O
We	O
also	O
face	O
a	O
more	O
realistic	O
setting	O
where	O
the	O
monolingual	O
data	O
for	O
the	O
low	O
resource	O
languages	O
(	O
chv	O
and	O
hsb	O
)	O
are	O
quite	O
scarce	O
.	O
Therefore	O
it	O
is	O
worth	O
testing	O
the	O
effect	O
of	O
scaling	O
up	O
.	O
Results	O
in	O
Table	O
3	O
show	O
that	O
Transformer	O
big	O
brings	O
consistent	O
improvements	O
.	O
We	O
also	O
report	O
the	O
runtime	O
of	O
each	O
step	O
in	O
dual	O
transfer	O
for	O
NMT	B-TaskName
chv	O
ru	O
with	O
Transformer	O
big	O
in	O
Table	O
4	O
for	O
reference	O
,	O
but	O
the	O
numbers	O
can	O
vary	O
depending	O
on	O
implementation	O
and	O
data	O
size	O
.	O
In	O
the	O
following	O
experiments	O
and	O
our	O
final	O
submission	O
,	O
we	O
use	O
Transformer	O
big	O
models	O
.	O

We	O
reproduced	O
the	O
illustration	O
of	O
dual	O
transfer	O
from	O
the	O
original	O
paper	O
(	O
Zhang	O
et	O
al	O
,	O
2021	O
)	O
,	O
as	O
shown	O
in	O
Figure	O
1	O
.	O
This	O
illustration	O
shows	O
the	O
case	O
of	O
general	O
transfer	O
,	O
where	O
the	O
high	O
resource	O
translation	O
direction	O
is	O
A	O
B	O
,	O
and	O
the	O
low	O
resource	O
translation	O
direction	O
is	O
P	O
Q.	O
As	O
discussed	O
in	O
the	O
original	O
paper	O
,	O
in	O
many	O
cases	O
,	O
it	O
is	O
possible	O
to	O
use	O
shared	O
target	O
transfer	O
(	O
B	O
=	O
Q	O
)	O
or	O
shared	O
source	O
transfer	O
(	O
A	O
=	O
P	O
)	O
.	O
Taking	O
chv	O
ru	O
as	O
an	O
example	O
,	O
we	O
can	O
choose	O
en	O
ru	O
as	O
the	O
high	O
resource	O
translation	O
direction	O
,	O
resulting	O
in	O
an	O
instance	O
of	O
shared	O
target	O
transfer	O
.	O
In	O
this	O
shared	O
task	O
,	O
when	O
training	O
the	O
high	O
resource	O
translation	O
model	O
,	O
we	O
always	O
initialize	O
the	O
shared	O
language	O
side	O
with	O
the	O
pretrained	O
language	O
model	O
BERT	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
.	O

NoahNMT	B-MethodName
at	O
WMT	O
2021	O
:	O
Dual	O
Transfer	O
for	O
Very	O
Low	O
Resource	O
Supervised	B-TaskName
Machine	I-TaskName
Translation	I-TaskName

We	O
next	O
propose	O
an	O
approach	O
to	O
improve	O
the	O
interpretability	O
of	O
word	O
embeddings	O
by	O
adding	O
emoji	O
.	O
It	O
uses	O
our	O
extended	O
version	O
POLAR	B-MethodName
ρ	I-MethodName
and	O
adds	O
emoji	O
to	O
the	O
POLAR	B-MethodName
space	O
by	O
creating	O
word	O
embeddings	O
that	O
include	O
emoji	O
.	O

Style	B-TaskName
transfer	I-TaskName
has	O
been	O
widely	O
explored	O
in	O
natural	O
language	O
generation	O
with	O
non	O
-	O
parallel	O
corpus	O
by	O
directly	O
or	O
indirectly	O
extracting	O
a	O
notion	O
of	O
style	O
from	O
source	O
and	O
target	O
domain	O
corpus	O
.	O
A	O
common	O
shortcoming	O
of	O
existing	O
approaches	O
is	O
the	O
prerequisite	O
of	O
joint	O
annotations	O
across	O
all	O
the	O
stylistic	O
dimensions	O
under	O
consideration	O
.	O
Availability	O
of	O
such	O
dataset	O
across	O
a	O
combination	O
of	O
styles	O
limits	O
the	O
extension	O
of	O
these	O
setups	O
to	O
multiple	O
style	O
dimensions	O
.	O
While	O
cascading	O
single	O
-	O
dimensional	O
models	O
across	O
multiple	O
styles	O
is	O
a	O
possibility	O
,	O
it	O
suffers	O
from	O
content	O
loss	O
,	O
especially	O
when	O
the	O
style	O
dimensions	O
are	O
not	O
completely	O
independent	O
of	O
each	O
other	O
.	O
In	O
our	O
work	O
,	O
we	O
relax	O
this	O
requirement	O
of	O
jointly	O
annotated	O
data	O
across	O
multiple	O
styles	O
by	O
using	O
independently	O
acquired	O
data	O
across	O
different	O
style	O
dimensions	O
without	O
any	O
additional	O
annotations	O
.	O
We	O
initialize	O
an	O
encoder	O
-	O
decoder	O
setup	O
with	O
transformerbased	O
language	O
model	O
pre	O
-	O
trained	O
on	O
a	O
generic	O
corpus	O
and	O
enhance	O
its	O
re	O
-	O
writing	O
capability	O
to	O
multiple	O
target	O
style	O
dimensions	O
by	O
employing	O
multiple	O
style	O
-	O
aware	O
language	O
models	O
as	O
discriminators	O
.	O
Through	O
quantitative	O
and	O
qualitative	O
evaluation	O
,	O
we	O
show	O
the	O
ability	O
of	O
our	O
model	O
to	O
control	O
styles	O
across	O
multiple	O
style	O
dimensions	O
while	O
preserving	O
content	O
of	O
the	O
input	O
text	O
.	O
We	O
compare	O
it	O
against	O
baselines	O
involving	O
cascaded	B-MethodName
state	I-MethodName
-	I-MethodName
of	I-MethodName
-	I-MethodName
the	I-MethodName
-	I-MethodName
art	I-MethodName
uni	I-MethodName
-	I-MethodName
dimensional	I-MethodName
style	I-MethodName
transfer	I-MethodName
models	I-MethodName
.	O

We	O
test	O
the	O
proposed	O
framework	O
on	O
unconditional	O
and	O
conditional	O
text	B-TaskName
generation	I-TaskName
tasks	O
,	O
and	O
analyze	O
the	O
results	O
to	O
understand	O
the	O
performance	O
gained	O
by	O
the	O
guider	O
network	O
.	O
We	O
also	O
perform	O
an	O
ablation	O
investigation	O
on	O
the	O
improvements	O
brought	O
by	O
each	O
part	O
of	O
our	O
proposed	O
method	O
,	O
and	O
consider	O
non	B-TaskName
-	I-TaskName
parallel	I-TaskName
style	I-TaskName
transfer	I-TaskName
.	O
All	O
experiments	O
are	O
conducted	O
on	O
a	O
single	O
Tesla	O
P100	O
GPU	O
and	O
implemented	O
with	O
TensorFlow	O
and	O
Theano	O
.	O
Details	O
of	O
the	O
datasets	O
,	O
the	O
experimental	O
setup	O
and	O
model	O
architectures	O
are	O
provided	O
in	O
the	O
Appendix	O
.	O

Acknowledgement	O
The	O
authors	O
would	O
like	O
to	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
insightful	O
comments	O
.	O
The	O
research	O
was	O
supported	O
in	O
part	O
by	O
DARPA	O
,	O
DOE	O
,	O
NIH	O
,	O
NSF	O
and	O
ONR	O
.	O

The	O
LSTM	O
state	O
of	O
dimension	O
for	O
the	O
generator	O
is	O
300	O
,	O
and	O
the	O
LSTM	O
state	O
of	O
dimension	O
for	O
the	O
guider	O
is	O
300	O
.	O
The	O
dimension	B-HyperparameterName
of	I-HyperparameterName
word	I-HyperparameterName
-	I-HyperparameterName
embedding	I-HyperparameterName
is	O
300	B-HyperparameterValue
.	O

More	O
Generated	O
Samples	O
of	O
Text	B-TaskName
Generation	I-TaskName
Table	O
13	O
lists	O
more	O
generated	O
samples	O
on	O
the	O
proposed	O
GMGAN	O
and	O
its	O
baselines	O
.	O
From	O
the	O
experiments	O
,	O
we	O
can	O
see	O
,	O
(	O
i	O
)	O
SeqGAN	B-MethodName
tends	O
to	O
generate	O
shorter	O
sentences	O
,	O
and	O
the	O
readability	O
and	O
fluency	O
is	O
very	O
poor	O
.	O
(	O
ii	O
)	O
LeakGAN	B-MethodName
tends	O
to	O
generate	O
very	O
long	O
sentences	O
,	O
and	O
usually	O
longer	O
than	O
the	O
original	O
sentences	O
.	O
However	O
,	O
even	O
with	O
good	O
locality	O
fluency	O
,	O
its	O
sentences	O
usually	O
are	O
not	O
semantically	O
consistent	O
.	O
By	O
contrast	O
,	O
our	O
proposed	O
GMGAN	B-MethodName
can	O
generate	O
sentences	O
with	O
similar	O
length	O
to	O
the	O
original	O
sentences	O
,	O
and	O
has	O
good	O
readability	O
and	O
fluency	O
.	O
This	O
is	O
also	O
validated	O
in	O
the	O
Human	B-MethodName
evaluation	I-MethodName
experiment	O
.	O

We	O
have	O
proposed	O
a	O
model	B-MethodName
-	I-MethodName
based	I-MethodName
imitationlearning	I-MethodName
framework	O
for	O
adversarial	B-TaskName
text	I-TaskName
generation	I-TaskName
,	O
by	O
introducing	O
a	O
guider	O
network	O
to	O
model	O
the	O
generation	O
environment	O
.	O
The	O
guider	O
network	O
provides	O
a	O
plan	O
-	O
ahead	O
mechanism	O
for	O
next	O
-	O
word	O
selection	O
.	O
Furthermore	O
,	O
this	O
framework	O
can	O
alleviate	O
the	O
sparse	O
-	O
reward	O
issue	O
,	O
as	O
the	O
intermediate	O
rewards	O
are	O
used	O
to	O
optimize	O
the	O
generator	O
.	O
Our	O
proposed	O
models	O
are	O
validated	O
on	O
both	O
unconditional	O
and	O
conditional	O
text	O
generation	O
,	O
including	O
adversarial	B-TaskName
text	I-TaskName
generation	I-TaskName
and	O
non	B-TaskName
-	I-TaskName
parallel	I-TaskName
style	I-TaskName
transfer	I-TaskName
.	O
We	O
achieve	O
improved	O
performance	O
in	O
terms	O
of	O
generation	B-MetricName
quality	I-MetricName
and	O
diversity	B-MetricName
for	O
unconditional	O
and	O
conditional	O
generation	O
tasks	O
.	O

The	O
model	O
is	O
illustrated	O
in	O
Figure	O
1	O
,	O
with	O
an	O
autoeocoder	O
(	O
AE	O
)	O
structure	O
for	O
sentence	O
feature	O
extraction	O
and	O
generation	O
.	O
The	O
encoder	O
is	O
shared	O
for	O
sentences	O
from	O
both	O
training	O
data	O
and	O
generated	O
data	O
,	O
as	O
explained	O
in	O
detail	O
below	O
.	O
Overall	O
,	O
text	B-TaskName
generation	I-TaskName
can	O
be	O
formulated	O
as	O
an	O
imitationlearning	O
problem	O
.	O
At	O
each	O
timestep	O
t	O
,	O
the	O
agent	O
,	O
also	O
called	O
a	O
generator	O
(	O
which	O
corresponds	O
to	O
the	O
LSTM	O
decoder	O
)	O
,	O
takes	O
the	O
current	O
LSTM	O
state	O
as	O
input	O
,	O
denoted	O
as	O
s	O
t	O
.	O
The	O
policy	O
π	O
φ	O
(	O
|	O
s	O
t	O
)	O
parameterized	O
by	O
φ	O
is	O
a	O
conditional	O
generator	O
,	O
to	O
generate	O
the	O
next	O
token	O
(	O
action	O
)	O
given	O
s	O
t	O
,	O
the	O
observation	O
representing	O
the	O
current	O
generated	O
sentence	O
.	O
The	O
objective	O
of	O
text	B-TaskName
generation	I-TaskName
is	O
to	O
maximize	O
the	O
total	B-MetricName
reward	I-MetricName
as	O
in	O
(	O
4	O
)	O
.	O
We	O
detail	O
the	O
components	O
for	O
our	O
proposed	O
model	O
in	O
the	O
following	O
subsections	O
.	O

The	O
main	O
objective	O
of	O
this	O
work	O
is	O
to	O
develop	O
a	O
multi	O
-	O
domain	O
dialog	O
system	O
toolkit	O
that	O
allows	O
for	O
multi	B-TaskName
-	I-TaskName
modal	I-TaskName
information	I-TaskName
processing	I-TaskName
and	O
that	O
provides	O
different	O
modules	O
for	O
extracting	O
social	O
signals	O
such	O
as	O
emotional	O
states	O
and	O
for	O
integrating	O
them	O
into	O
the	O
decision	O
making	O
process	O
.	O
The	O
toolkit	O
should	O
be	O
easy	O
to	O
use	O
and	O
extend	O
for	O
users	O
of	O
all	O
levels	O
of	O
technical	O
experience	O
,	O
providing	O
a	O
flexible	O
collaborative	O
research	O
platform	O
.	O

We	O
present	O
ADVISER	B-MethodName
1	I-MethodName
-	O
an	O
open	O
-	O
source	O
,	O
multi	O
-	O
domain	O
dialog	O
system	O
toolkit	O
that	O
enables	O
the	O
development	O
of	O
multi	O
-	O
modal	O
(	O
incorporating	O
speech	O
,	O
text	O
and	O
vision	O
)	O
,	O
sociallyengaged	O
(	O
e.g.	O
emotion	O
recognition	O
,	O
engagement	O
level	O
prediction	O
and	O
backchanneling	O
)	O
conversational	O
agents	O
.	O
The	O
final	O
Python	O
-	O
based	O
implementation	O
of	O
our	O
toolkit	O
is	O
flexible	O
,	O
easy	O
to	O
use	O
,	O
and	O
easy	O
to	O
extend	O
not	O
only	O
for	O
technically	O
experienced	O
users	O
,	O
such	O
as	O
machine	O
learning	O
researchers	O
,	O
but	O
also	O
for	O
less	O
technically	O
experienced	O
users	O
,	O
such	O
as	O
linguists	O
or	O
cognitive	O
scientists	O
,	O
thereby	O
providing	O
a	O
flexible	O
platform	O
for	O
collaborative	O
research	O
.	O

Given	O
the	O
fact	O
that	O
including	O
more	O
data	O
in	O
a	O
reading	O
comprehension	O
system	O
is	O
important	O
for	O
gen	O
-	O
eralization	O
(	O
Chung	O
et	O
al	O
,	O
2018	O
;	O
Talmor	O
and	O
Berant	O
,	O
2019	O
)	O
,	O
and	O
given	O
that	O
our	O
created	O
dataset	O
has	O
the	O
SBRCS	O
which	O
are	O
missed	O
in	O
previous	O
datasets	O
,	O
we	O
propose	O
a	O
two	O
-	O
steps	O
method	O
to	O
generate	O
skillrelated	O
questions	O
from	O
a	O
given	O
story	O
:	O
HTA	B-MethodName
followed	O
by	O
WTA	B-MethodName
.	O
HTA	B-MethodName
teaches	O
the	O
model	O
the	O
typical	O
format	O
for	O
comprehension	O
questions	O
using	O
large	O
previously	O
released	O
datasets	O
.	O
We	O
use	O
two	O
well	O
-	O
known	O
datasets	O
,	O
SQuAD	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
and	O
Cos	B-DatasetName
-	I-DatasetName
mosQA	I-DatasetName
(	O
Huang	O
et	O
al	O
,	O
2019	O
)	O
.	O
In	O
Appendix	O
A.3	O
,	O
we	O
add	O
more	O
details	O
on	O
both	O
of	O
these	O
datasets	O
.	O
These	O
previous	O
datasets	O
are	O
not	O
annotated	O
with	O
the	O
question	O
types	O
outlined	O
in	O
Section	O
3.1	O
,	O
so	O
the	O
HTA	B-MethodName
phase	O
allows	O
us	O
to	O
take	O
advantage	O
of	O
those	O
datasets	O
.	O
WTA	B-MethodName
guides	O
the	O
model	O
to	O
generate	O
questions	O
to	O
test	O
the	O
specific	O
comprehension	O
skills	O
enumerated	O
in	O
Section	O
3.1	O
.	O
Thus	O
,	O
in	O
HTA	B-MethodName
,	O
we	O
train	O
(	O
fine	O
-	O
tune	O
)	O
a	O
model	O
on	O
large	O
QG	B-TaskName
datasets	O
,	O
and	O
then	O
,	O
we	O
further	O
train	O
the	O
model	O
to	O
teach	O
the	O
model	O
what	O
to	O
ask	O
(	O
WTA	B-MethodName
)	O
.	O
For	O
the	O
generation	O
model	O
,	O
we	O
use	O
the	O
pre	O
-	O
trained	O
Text	O
-	O
to	O
-	O
Text	O
Transfer	O
Transformer	O
T5	O
(	O
Raffel	O
et	O
al	O
,	O
2020	O
)	O
,	O
which	O
closely	O
follows	O
the	O
encoder	O
-	O
decoder	O
architecture	O
of	O
the	O
transformer	O
model	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
T5	O
is	O
a	O
SOTA	O
model	O
on	O
multiple	O
tasks	O
,	O
including	O
QA	O
.	O

Our	O
stories	O
(	O
passages	O
)	O
are	O
multi	O
-	O
genre	O
,	O
selfcontained	O
narratives	O
.	O
This	O
content	O
variety	O
leads	O
annotators	O
towards	O
asking	O
non	O
-	O
localized	O
questions	O
that	O
test	O
for	O
more	O
advanced	O
reading	O
comprehension	O
skills	O
.	O
The	O
stories	O
are	O
generated	O
using	O
several	O
resources	O
:	O
1	O
.	O
acquired	O
from	O
free	O
public	O
domain	O
content	O
(	O
Gutenberg	O
Project	O
2	O
)	O
,	O
2	O
.	O
partnerships	O
with	O
a	O
publishing	O
house	O
(	O
Blue	O
Moon	O
Publishers	O
3	O
)	O
and	O
an	O
educational	O
curriculum	O
development	O
foundation	O
(	O
The	O
Reimagined	O
Classroom	O
4	O
)	O
,	O
and	O
3	O
.	O
authored	O
by	O
two	O
professional	O
writers	O
,	O
(	O
the	O
majority	O
of	O
the	O
stories	O
are	O
from	O
this	O
last	O
category	O
)	O
.	O
To	O
provide	O
good	O
lexical	O
coverage	O
and	O
diverse	O
stories	O
,	O
we	O
choose	O
to	O
write	O
and	O
collect	O
stories	O
that	O
come	O
from	O
a	O
varied	O
set	O
of	O
genres	O
(	O
e.g.	O
science	O
,	O
social	O
studies	O
,	O
fantasy	O
,	O
fairy	O
tale	O
,	O
historical	O
fiction	O
,	O
horror	O
,	O
mystery	O
,	O
adventure	O
,	O
etc	O
.	O
)	O
.	O
In	O
total	O
,	O
we	O
collect	O
726	O
multi	O
-	O
domain	O
stories	O
.	O
The	O
stories	O
'	O
lengths	O
range	O
from	O
a	O
single	O
sentence	O
to	O
113	O
sentences	O
.	O

QG	B-TaskName
has	O
progressed	O
rapidly	O
due	O
to	O
new	O
datasets	O
and	O
model	O
improvements	O
.	O
Many	O
different	O
QG	B-TaskName
models	O
have	O
been	O
proposed	O
,	O
starting	O
for	O
simple	O
vanilla	O
Sequence	O
to	O
Sequence	O
Neural	O
Networks	O
models	O
(	O
seq2seq	O
)	O
Yuan	O
et	O
al	O
,	O
2017	O
)	O
to	O
the	O
more	O
recent	O
transformer	O
-	O
based	O
models	O
(	O
Dong	O
et	O
al	O
,	O
2019	O
;	O
Chan	O
and	O
Fan	O
,	O
2019	O
;	O
Varanasi	O
et	O
al	O
,	O
2020	O
;	O
Narayan	O
et	O
al	O
,	O
2020	O
;	O
Bao	O
et	O
al	O
,	O
2020	O
)	O
.	O
Some	O
QG	B-TaskName
systems	O
use	O
manual	O
linguistic	O
features	O
in	O
their	O
models	O
(	O
Harrison	O
and	O
Walker	O
,	O
2018	O
;	O
Khullar	O
et	O
al	O
,	O
2018	O
;	O
Liu	O
et	O
al	O
,	O
2019a	O
;	O
Dhole	O
and	O
Manning	O
,	O
2020	O
)	O
,	O
some	O
consider	O
how	O
to	O
select	O
question	O
-	O
worthy	O
content	O
Li	O
et	O
al	O
,	O
2019	O
;	O
Scialom	O
et	O
al	O
,	O
2019	O
;	O
,	O
and	O
some	O
systems	O
explicitly	O
model	O
question	O
types	O
(	O
Duan	O
et	O
al	O
,	O
2017	O
;	O
Sun	O
et	O
al	O
,	O
2018	O
;	O
Kang	O
et	O
al	O
,	O
2019	O
;	O
.	O
The	O
last	O
group	O
focused	O
only	O
on	O
generating	O
questions	O
that	O
start	O
with	O
specific	O
interrogative	O
words	O
(	O
what	O
,	O
how	O
,	O
etc	O
.	O
)	O
.	O
QG	B-TaskName
has	O
been	O
used	O
to	O
solve	O
many	O
real	O
-	O
life	O
problems	O
.	O
For	O
example	O
,	O
QG	B-TaskName
in	O
conversational	O
dialogue	O
(	O
Gu	O
et	O
al	O
,	O
2021	O
;	O
Shen	O
et	O
al	O
,	O
2021	O
;	O
Liu	O
et	O
al	O
,	O
2021b	O
)	O
where	O
models	O
were	O
taught	O
to	O
ask	O
a	O
series	O
of	O
coherent	O
questions	O
grounded	O
in	O
a	O
QA	O
style	O
,	O
QG	B-TaskName
based	O
on	O
visual	O
input	O
(	O
Mostafazadeh	O
et	O
al	O
,	O
2016	O
;	O
Shin	O
et	O
al	O
,	O
2018	O
;	O
Shukla	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
QG	B-TaskName
for	O
deep	O
questions	O
such	O
as	O
mathematical	O
,	O
curiosity	O
-	O
driven	O
,	O
clinical	O
,	O
and	O
examinationtype	O
questions	O
(	O
Liyanage	O
and	O
Ranathunga	O
,	O
2019	O
;	O
Yue	O
et	O
al	O
,	O
2020	O
;	O
Jia	O
et	O
al	O
,	O
2021	O
)	O
.	O

Reading	O
is	O
integral	O
to	O
everyday	O
life	O
,	O
and	O
yet	O
learning	O
to	O
read	O
is	O
a	O
struggle	O
for	O
many	O
young	O
learners	O
.	O
During	O
lessons	O
,	O
teachers	O
can	O
use	O
comprehension	O
questions	O
to	O
increase	O
engagement	O
,	O
test	O
reading	O
skills	O
,	O
and	O
improve	O
retention	O
.	O
Historically	O
such	O
questions	O
were	O
written	O
by	O
skilled	O
teachers	O
,	O
but	O
recently	O
language	O
models	O
have	O
been	O
used	O
to	O
generate	O
comprehension	O
questions	O
.	O
However	O
,	O
many	O
existing	O
Question	B-TaskName
Generation	I-TaskName
(	O
QG	B-TaskName
)	O
systems	O
focus	O
on	O
generating	O
literal	O
questions	O
from	O
the	O
text	O
,	O
and	O
have	O
no	O
way	O
to	O
control	O
the	O
type	O
of	O
the	O
generated	O
question	O
.	O
In	O
this	O
paper	O
,	O
we	O
study	O
QG	B-TaskName
for	O
reading	O
comprehension	O
where	O
inferential	O
questions	O
are	O
critical	O
and	O
extractive	O
techniques	O
can	O
not	O
be	O
used	O
.	O
We	O
propose	O
a	O
two	O
-	O
step	O
model	O
(	O
HTA	B-MethodName
-	I-MethodName
WTA	I-MethodName
)	O
that	O
takes	O
advantage	O
of	O
previous	O
datasets	O
,	O
and	O
can	O
generate	O
questions	O
for	O
a	O
specific	O
targeted	O
comprehension	O
skill	O
.	O
We	O
propose	O
a	O
new	O
reading	O
comprehension	O
dataset	O
that	O
contains	O
questions	O
annotated	O
with	O
story	O
-	O
based	O
reading	O
comprehension	O
skills	O
(	O
SBRCS	B-DatasetName
)	O
,	O
allowing	O
for	O
a	O
more	O
complete	O
reader	O
assessment	O
.	O
Across	O
several	O
experiments	O
,	O
our	O
results	O
show	O
that	O
HTA	B-MethodName
-	I-MethodName
WTA	I-MethodName
outperforms	O
multiple	O
strong	O
baselines	O
on	O
this	O
new	O
dataset	O
.	O
We	O
show	O
that	O
the	O
HTA	B-MethodName
-	I-MethodName
WTA	I-MethodName
model	O
tests	O
for	O
strong	O
SCRS	B-DatasetName
by	O
asking	O
deep	O
inferential	O
questions	O
.	O

BERT	O
is	O
a	O
large	O
Transformer	O
encoder	O
;	O
for	O
background	O
,	O
we	O
refer	O
readers	O
to	O
Vaswani	O
et	O
al	O
(	O
2017	O
)	O
or	O
one	O
of	O
these	O
excellent	O
tutorials	O
(	O
Alammar	O
,	O
2018	O
;	O
Klein	O
et	O
al	O
,	O
2017	O
)	O
.	O

This	O
paper	O
presented	O
a	O
system	O
which	O
uses	O
sophisticated	O
learning	O
to	O
rank	O
method	O
with	O
semantic	O
features	O
to	O
obtain	O
promising	O
results	O
on	O
ranking	O
similar	O
questions	O
.	O
The	O
paper	O
shows	O
that	O
semantic	O
features	O
and	O
pairwise	O
learning	O
are	O
essential	O
components	O
to	O
the	O
system	O
by	O
ablation	O
tests	O
.	O
In	O
future	O
,	O
we	O
would	O
like	O
to	O
extend	O
our	O
neural	O
architecture	O
to	O
attention	O
based	O
models	O
which	O
have	O
shown	O
success	O
in	O
recent	O
times	O
.	O
We	O
also	O
plan	O
to	O
use	O
Triplet	O
loss	O
(	O
Hoffer	O
and	O
Ailon	O
,	O
2015	O
)	O
which	O
captures	O
ranking	O
task	O
in	O
better	O
way	O
.	O
Another	O
direction	O
is	O
to	O
use	O
state	O
-	O
of	O
-	O
art	O
listwise	O
learning	O
to	O
rank	O
methods	O
that	O
can	O
directly	O
optimize	O
MAP	B-MetricName
.	O

We	O
use	O
pairwise	O
learning	O
to	O
rank	O
for	O
ranking	O
task	O
which	O
poses	O
the	O
ranking	O
problem	O
as	O
classification	O
problem	O
to	O
minimize	O
the	O
average	O
number	O
of	O
inversions	O
in	O
ranking	O
.	O
This	O
formulation	O
is	O
more	O
closer	O
to	O
ranking	O
task	O
than	O
predicting	O
relevance	O
as	O
regression	O
and	O
also	O
has	O
theoretical	O
guarantees	O
of	O
maximizing	O
the	O
MAP	B-MetricName
in	O
ranking	O
(	O
Chen	O
et	O
al	O
,	O
2009	O
)	O
.	O
First	O
,	O
we	O
create	O
these	O
pairs	O
by	O
taking	O
original	O
question	O
Q	O
o	O
and	O
two	O
candidate	O
questions	O
of	O
which	O
one	O
was	O
relevant	O
and	O
other	O
one	O
not	O
,	O
Q	O
c1	O
and	O
Q	O
c2	O
.	O
Then	O
we	O
generate	O
above	O
mentioned	O
feature	O
vectors	O
f	O
(	O
Q	O
o	O
,	O
Q	O
c1	O
)	O
,	O
f	O
(	O
Q	O
o	O
,	O
Q	O
c2	O
)	O
and	O
use	O
feature	O
dif	O
-	O
ference	O
f	O
(	O
Q	O
o	O
,	O
Q	O
c1	O
)	O
−	O
f	O
(	O
Q	O
o	O
,	O
Q	O
c2	O
)	O

Topic	O
modeling	O
is	O
used	O
to	O
generate	O
the	O
salient	O
topics	O
in	O
the	O
text	O
.	O
We	O
use	O
Latent	O
Dirichlet	O
al	O
ocation	O
(	O
LDA	O
)	O
(	O
Blei	O
et	O
al	O
,	O
2003	O
)	O
to	O
compute	O
topic	O
similarity	O
between	O
texts	O
.	O
We	O
train	O
LDA	O
topic	O
model	O
using	O
the	O
whole	O
text	O
(	O
body	O
and	O
subject	O
)	O
as	O
corpus	O
.	O
Then	O
a	O
topic	O
distribution	O
over	O
the	O
50	O
topics	O
is	O
computed	O
for	O
both	O
the	O
text	O
and	O
cosine	O
similarity	O
is	O
used	O
as	O
a	O
feature	O
in	O
the	O
system	O
.	O

We	O
use	O
rank	O
given	O
by	O
the	O
search	O
engine	O
as	O
a	O
feature	O
in	O
our	O
system	O
.	O
This	O
gives	O
the	O
system	O
the	O
baseline	O
accuracy	B-MetricName
of	O
the	O
search	O
engine	O
.	O

Since	O
the	O
task	O
is	O
a	O
ranking	O
task	O
,	O
our	O
system	O
uses	O
learning	O
to	O
rank	O
(	O
Trotman	O
,	O
2005	O
)	O
to	O
model	O
the	O
ranking	O
of	O
questions	O
.	O
Learning	O
to	O
rank	O
refers	O
to	O
various	O
machine	O
learning	O
techniques	O
used	O
in	O
ranking	O
tasks	O
.	O
These	O
have	O
been	O
studied	O
in	O
information	O
retrieval	O
literature	O
and	O
they	O
power	O
many	O
of	O
the	O
industrial	O
search	O
engines	O
.	O
These	O
systems	O
mainly	O
fall	O
into	O
3	O
categories	O
:	O
pointwise	O
,	O
pairwise	O
and	O
listwise	O
as	O
described	O
in	O
.	O
We	O
use	O
pairwise	O
methods	O
for	O
our	O
system	O
with	O
rich	O
feature	O
set	O
.	O
Our	O
feature	O
set	O
is	O
combination	O
of	O
various	O
hand	O
generated	O
features	O
and	O
semantic	O
features	O
learned	O
by	O
neural	O
network	O
.	O
In	O
the	O
following	O
section	O
we	O
first	O
describe	O
these	O
features	O
and	O
then	O
the	O
learning	O
to	O
rank	O
method	O
used	O
.	O

We	O
primarily	O
use	O
the	O
annotated	O
training	O
,	O
development	O
and	O
testing	O
dataset	O
provided	O
by	O
the	O
SemEval	B-DatasetName
-	I-DatasetName
2017	I-DatasetName
task	O
3	O
organizers	O
.	O
The	O
dataset	O
is	O
collected	O
by	O
organizers	O
from	O
Qatar	O
living	O
forum	O
.	O
It	O
's	O
in	O
the	O
form	O
of	O
an	O
original	O
question	O
and	O
set	O
of	O
related	O
questions	O
.	O
Each	O
related	O
question	O
in	O
training	O
and	O
development	O
dataset	O
is	O
annotated	O
with	O
one	O
of	O
the	O
3	O
possible	O
tags	O
,	O
PerfectMatch	O
,	O
Relevant	O
or	O
Irrelevant	O
.	O
A	O
ranking	O
task	O
is	O
required	O
to	O
rank	O
both	O
Per	O
-	O
fectMatch	O
and	O
Relevant	O
above	O
Irrelevant	O
questions	O
without	O
any	O
distinction	O
between	O
the	O
first	O
two	O
.	O
The	O
train	O
dataset	O
for	O
subtask	O
B	O
consists	O
of	O
317	O
original	O
questions	O
and	O
3169	O
retrieved	O
questions	O
by	O
search	O
engine	O
roughly	O
10	O
related	O
questions	O
per	O
original	O
question	O
.	O
The	O
organizers	O
have	O
also	O
provided	O
annotated	O
test	O
dataset	O
from	O
SemEval	B-DatasetName
-	I-DatasetName
2016	I-DatasetName
challenge	O
.	O
Along	O
with	O
these	O
we	O
also	O
used	O
Glove	O
embeddings	O
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
which	O
were	O
pretrained	O
using	O
6	O
billion	O
tokens	O
from	O
Wikipedia	O
-	O
2014	O
and	O
Gigaword	O
dataset	O
.	O

This	O
paper	O
describes	O
our	O
official	O
entry	O
LearningToQuestion	B-MethodName
for	O
SemEval	B-DatasetName
2017	I-DatasetName
task	O
3	O
community	O
question	O
answer	O
,	O
subtask	O
B.	O
The	O
objective	O
is	O
to	O
rerank	O
questions	O
obtained	O
in	O
web	O
forum	O
as	O
per	O
their	O
similarity	O
to	O
original	O
question	O
.	O
Our	O
system	O
uses	O
pairwise	O
learning	O
to	O
rank	O
methods	O
on	O
rich	O
set	O
of	O
hand	O
designed	O
and	O
representation	O
learning	O
features	O
.	O
We	O
use	O
various	O
semantic	O
features	O
that	O
help	O
our	O
system	O
to	O
achieve	O
promising	O
results	O
on	O
the	O
task	O
.	O
The	O
system	O
achieved	O
second	O
highest	O
results	O
on	O
official	O
metrics	O
MAP	B-MetricName
and	O
good	O
results	O
on	O
other	O
search	O
metrics	O
.	O

In	O
online	O
forums	O
question	B-TaskName
answering	I-TaskName
is	O
one	O
of	O
the	O
most	O
popular	O
way	O
for	O
users	O
to	O
share	O
information	O
between	O
each	O
other	O
.	O
Due	O
to	O
the	O
unstructured	O
nature	O
of	O
these	O
forums	O
,	O
it	O
's	O
a	O
problem	O
to	O
find	O
relevant	O
information	O
from	O
the	O
already	O
existing	O
information	O
for	O
users	O
.	O
One	O
way	O
to	O
solve	O
this	O
problem	O
is	O
to	O
design	O
systems	O
to	O
automatically	O
find	O
similar	O
content	O
(	O
question	O
,	O
answer	O
,	O
comment	O
)	O
to	O
the	O
user	O
's	O
posted	O
question	O
.	O
SemEval	B-DatasetName
-	I-DatasetName
2017	I-DatasetName
task	O
3	O
(	O
Nakov	O
et	O
al	O
,	O
2017	O
)	O
focuses	O
on	O
solving	O
this	O
problem	O
in	O
community	O
question	O
answer	O
by	O
various	O
subtasks	O
of	O
ranking	O
relevant	O
information	O
in	O
Qatar	O
living	O
forums	O
data	O
.	O
The	O
system	O
presented	O
in	O
this	O
paper	O
focuses	O
on	O
subtask	O
B	O
,	O
to	O
re	O
-	O
rank	O
given	O
set	O
of	O
questions	O
retrieved	O
by	O
search	O
engine	O
,	O
in	O
their	O
similarity	O
to	O
original	O
question	O
.	O
The	O
system	O
is	O
mainly	O
designed	O
by	O
employing	O
learning	O
to	O
rank	O
methods	O
on	O
the	O
rich	O
feature	O
set	O
obtained	O
by	O
text	O
processing	O
of	O
the	O
question	O
text	O
.	O

We	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
valuable	O
feedback	O
.	O
We	O
thank	O
Newsela	O
for	O
sharing	O
the	O
data	O
and	O
NVIDIA	O
for	O
providing	O
GPU	O
computing	O
resources	O
.	O
This	O
research	O
is	O
supported	O
in	O
part	O
by	O
the	O
NSF	O
award	O
IIS	O
-	O
1822754	O
,	O
ODNI	O
and	O
IARPA	O
via	O
the	O
BETTER	O
program	O
contract	O
19051600004	O
.	O
The	O
views	O
and	O
conclusions	O
contained	O
herein	O
are	O
those	O
of	O
the	O
authors	O
and	O
should	O
not	O
be	O
interpreted	O
as	O
necessarily	O
representing	O
the	O
official	O
policies	O
,	O
either	O
expressed	O
or	O
implied	O
,	O
of	O
NSF	O
,	O
ODNI	O
,	O
IARPA	O
,	O
or	O
the	O
U.S.	O
Government	O
.	O
The	O
U.S.	O
Government	O
is	O
authorized	O
to	O
reproduce	O
and	O
distribute	O
reprints	O
for	O
governmental	O
purposes	O
notwithstanding	O
any	O
copyright	O
annotation	O
therein	O
.	O

As	O
she	O
spoke	O
,	O
the	O
building	O
echoed	O
with	O
music	O
and	O
the	O
beat	O
of	O
drums	O
.	O
Hybrid	B-MethodName
-	I-MethodName
NG	I-MethodName
echoed	O
the	O
room	O
.	O
LSTM	B-MethodName
the	O
room	O
echoed	O
with	O
the	O
sounds	O
of	O
song	O
,	O
the	O
voices	O
of	O
young	O
men	O
.	O
Transformer	B-MethodName
bert	I-MethodName
the	O
room	O
echoed	O
with	O
the	O
sound	O
of	O
song	O
,	O
the	O
beat	O
of	O
drums	O
,	O
the	O
voices	O
of	O
young	O
men	O
.	O

Validating	O
Label	O
Consistency	O
in	O
NER	B-TaskName
Data	O
Annotation	O

Sentiment	B-TaskName
Analysis	I-TaskName
(	O
ABSA	O
)	O
This	O
subtask	O
contains	O
different	O
slots	O
,	O
having	O
participated	O
in	O
three	O
of	O
them	O
,	O
which	O
are	O
slot	O
1	O
,	O
slot	O
2	O
and	O
slot	O
3	O
.	O
The	O
system	O
for	O
Spanish	O
and	O
English	O
language	O
is	O
exactly	O
the	O
same	O
for	O
both	O
slots	O
1	O
and	O
2	O
.	O
1	O
Taken	O
from	O
the	O
lists	O
available	O
at	O
https://es.speaklanguages.com/inglés/vocabulario/comidas	O

Transformers	O
are	O
being	O
used	O
extensively	O
across	O
several	O
sequence	O
modeling	O
tasks	O
.	O
Significant	O
research	O
effort	O
has	O
been	O
devoted	O
to	O
experimentally	O
probe	O
the	O
inner	O
workings	O
of	O
Transformers	O
.	O
However	O
,	O
our	O
conceptual	O
and	O
theoretical	O
understanding	O
of	O
their	O
power	O
and	O
inherent	O
limitations	O
is	O
still	O
nascent	O
.	O
In	O
particular	O
,	O
the	O
roles	O
of	O
various	O
components	O
in	O
Transformers	O
such	O
as	O
positional	O
encodings	O
,	O
attention	O
heads	O
,	O
residual	O
connections	O
,	O
and	O
feedforward	O
networks	O
,	O
are	O
not	O
clear	O
.	O
In	O
this	O
paper	O
,	O
we	O
take	O
a	O
step	O
towards	O
answering	O
these	O
questions	O
.	O
We	O
analyze	O
the	O
computational	O
power	O
as	O
captured	O
by	O
Turing	O
-	O
completeness	O
.	O
We	O
first	O
provide	O
an	O
alternate	O
and	O
simpler	O
proof	O
to	O
show	O
that	O
vanilla	O
Transformers	O
are	O
Turing	O
-	O
complete	O
and	O
then	O
we	O
prove	O
that	O
Transformers	O
with	O
only	O
positional	O
masking	O
and	O
without	O
any	O
positional	O
encoding	O
are	O
also	O
Turing	O
-	O
complete	O
.	O
We	O
further	O
analyze	O
the	O
necessity	O
of	O
each	O
component	O
for	O
the	O
Turing	O
-	O
completeness	O
of	O
the	O
network	O
;	O
interestingly	O
,	O
we	O
find	O
that	O
a	O
particular	O
type	O
of	O
residual	O
connection	O
is	O
necessary	O
.	O
We	O
demonstrate	O
the	O
practical	O
implications	O
of	O
our	O
results	O
via	O
experiments	O
on	O
machine	B-TaskName
translation	I-TaskName
and	O
synthetic	O
tasks	O
.	O

We	O
propose	O
task	B-MethodName
-	I-MethodName
oriented	I-MethodName
dialogue	I-MethodName
BERT	I-MethodName
(	O
TOD	B-MethodName
-	I-MethodName
BERT	I-MethodName
)	O
trained	O
on	O
nine	O
human	O
-	O
human	O
and	O
multiturn	O
task	O
-	O
oriented	O
datasets	O
across	O
over	O
60	O
domains	O
.	O
TOD	B-MethodName
-	I-MethodName
BERT	I-MethodName
outperforms	O
BERT	O
on	O
four	O
dialogue	O
downstream	O
tasks	O
,	O
including	O
intention	B-TaskName
classification	I-TaskName
,	O
dialogue	B-TaskName
state	I-TaskName
tracking	I-TaskName
,	O
dialogue	B-TaskName
act	I-TaskName
prediction	I-TaskName
,	O
and	O
response	B-TaskName
selection	I-TaskName
.	O
It	O
also	O
has	O
a	O
clear	O
advantage	O
in	O
the	O
few	O
-	O
shot	O
experiments	O
when	O
only	O
limited	O
labeled	O
data	O
is	O
available	O
.	O
TOD	B-MethodName
-	I-MethodName
BERT	I-MethodName
is	O
easy	O
-	O
to	O
-	O
deploy	O
and	O
will	O
be	O
open	O
-	O
sourced	O
,	O
allowing	O
the	O
NLP	O
research	O
community	O
to	O
apply	O
or	O
fine	O
-	O
tune	O
any	O
task	O
-	O
oriented	O
conversational	O
problem	O
.	O

This	O
submission	O
uses	O
a	O
jumble	O
of	O
features	O
and	O
classifiers	O
,	O
most	O
from	O
the	O
sklearn	O
module	O
(	O
Buitinck	O
et	O
al	O
,	O
2013	O
)	O
.	O
The	O
final	O
classifier	O
is	O
a	O
hard	O
voting	O
classifier	O
with	O
three	O
input	O
streams	O
:	O
1	O
.	O
Soft	O
voting	O
classifier	O
on	O
:	O
on	O
language	O
-	O
model	O
-	O
scores	O
for	O
character	O
and	O
language	O
models	O
on	O
the	O
corpus	O
-	O
6	O
language	O
models	O
and	O
character	O
language	O
models	O
for	O
the	O
corpus	O
-	O
26	O
language	O
models	O
.	O
2	O
.	O
Support	O
vector	O
machine	O
,	O
svm	O
.	O
SVC	O
(	O
gamma='scale	O
'	O
,	O
kernel	O
=	O
'	O
poly	O
'	O
,	O
degree	O
=	O
2	O
)	O
with	O
the	O
same	O
features	O
as	O
item	O
1e	O
.	O

Pretrained	O
language	O
models	O
like	O
BERT	O
have	O
achieved	O
good	O
results	O
on	O
NLP	O
tasks	O
,	O
but	O
are	O
impractical	O
on	O
resource	O
-	O
limited	O
devices	O
due	O
to	O
memory	O
footprint	O
.	O
A	O
large	O
fraction	O
of	O
this	O
footprint	O
comes	O
from	O
the	O
input	O
embeddings	O
with	O
large	O
input	O
vocabulary	O
and	O
embedding	O
dimensions	O
.	O
Existing	O
knowledge	B-TaskName
distillation	I-TaskName
methods	O
used	O
for	O
model	O
compression	O
can	O
not	O
be	O
directly	O
applied	O
to	O
train	O
student	O
models	O
with	O
reduced	O
vocabulary	O
sizes	O
.	O
To	O
this	O
end	O
,	O
we	O
propose	O
a	O
distillation	O
method	O
to	O
align	O
the	O
teacher	O
and	O
student	O
embeddings	O
via	O
mixed	O
-	O
vocabulary	O
training	O
.	O
Our	O
method	O
compresses	O
BERT	O
LARGE	O
to	O
a	O
task	O
-	O
agnostic	O
model	O
with	O
smaller	O
vocabulary	O
and	O
hidden	O
dimensions	O
,	O
which	O
is	O
an	O
order	O
of	O
magnitude	O
smaller	O
than	O
other	O
distilled	O
BERT	O
models	O
and	O
offers	O
a	O
better	O
size	O
-	O
accuracy	O
trade	O
-	O
off	O
on	O
language	O
understanding	O
benchmarks	O
as	O
well	O
as	O
a	O
practical	O
dialogue	O
task	O
.	O

The	O
results	O
are	O
shown	O
in	O
Tab	O
2	O
.	O
For	O
CNNDM	B-DatasetName
and	O
NYT	B-DatasetName
we	O
use	O
BART	O
as	O
the	O
backbone	O
model	O
while	O
for	O
XSum	B-DatasetName
we	O
use	O
the	O
pre	O
-	O
trained	O
PEGASUS	O
model	O
as	O
our	O
base	O
model	O
since	O
it	O
achieves	O
better	O
performance	O
than	O
BART	O
.	O
We	O
have	O
the	O
following	O
observations	O
:	O
(	O
1	O
)	O
BRIO	B-MethodName
-	I-MethodName
Ctr	I-MethodName
outperforms	O
SimCLS	B-MethodName
,	O
its	O
counterpart	O
as	O
an	O
evaluation	O
model	O
in	O
a	O
two	O
-	O
stage	O
summarization	O
framework	O
.	O
Specifically	O
,	O
both	O
BRIO	B-MethodName
-	I-MethodName
Ctr	I-MethodName
and	O
SimCLS	B-MethodName
are	O
used	O
to	O
score	O
the	O
candidate	O
summaries	O
generated	O
by	O
a	O
Seq2Seq	O
abstractive	O
model	O
(	O
BART	O
)	O
.	O
The	O
final	O
outputs	O
are	O
selected	O
based	O
on	O
those	O
scores	O
.	O
We	O
attribute	O
BRIO	B-MethodName
-	I-MethodName
Ctr	I-MethodName
's	O
superior	O
performance	O
to	O
its	O
use	O
of	O
the	O
same	O
model	O
architecture	O
(	O
BART	O
)	O
for	O
both	O
candidate	O
generation	O
and	O
scoring	O
,	O
while	O
SimCLS	B-MethodName
uses	O
RoBERTa	O
as	O
the	O
evaluation	O
model	O
.	O
As	O
a	O
result	O
,	O
BRIO	B-MethodName
-	I-MethodName
Ctr	I-MethodName
maximizes	O
the	O
parameter	O
sharing	O
between	O
the	O
two	O
stages	O
,	O
and	O
preserves	O
the	O
power	O
of	O
the	O
Seq2Seq	O
model	O
pre	O
-	O
trained	O
on	O
the	O
same	O
dataset	O
.	O
(	O
2	O
)	O
BRIO	B-MethodName
-	I-MethodName
Mul	I-MethodName
is	O
able	O
to	O
establish	O
the	O
new	O
stare	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
CNNDM	B-DatasetName
.	O
Notably	O
,	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
model	O
,	O
GSum	B-MethodName
,	O
takes	O
additional	O
guidance	O
as	O
input	O
and	O
needs	O
a	O
separate	O
encoder	O
to	O
encode	O
the	O
guidance	O
information	O
,	O
while	O
BRIO	B-MethodName
-	I-MethodName
Mul	I-MethodName
uses	O
the	O
same	O
parameterization	O
of	O
BART	O
.	O
Compared	O
to	O
other	O
methods	O
(	O
ConSum	B-MethodName
,	O
SeqCo	B-MethodName
,	O
GOLD	B-MethodName
)	O
that	O
aim	O
to	O
improve	O
upon	O
BART	O
,	O
BRIO	B-MethodName
-	I-MethodName
Mul	I-MethodName
performs	O
much	O
better	O
,	O
showing	O
the	O
effectiveness	O
of	O
our	O
training	O
method	O
.	O
(	O
3	O
)	O
Since	O
on	O
XSum	B-DatasetName
we	O
use	O
PEGASUS	O
instead	O
of	O
BART	O
as	O
the	O
base	O
model	O
,	O
the	O
result	O
shows	O
that	O
our	O
method	O
is	O
not	O
restricted	O
to	O
the	O
specific	O
choice	O
of	O
the	O
base	O
model	O
.	O

Abstractive	O
summarization	O
models	O
are	O
commonly	O
trained	O
using	O
maximum	O
likelihood	O
estimation	O
,	O
which	O
assumes	O
a	O
deterministic	O
(	O
onepoint	O
)	O
target	O
distribution	O
in	O
which	O
an	O
ideal	O
model	O
will	O
assign	O
all	O
the	O
probability	O
mass	O
to	O
the	O
reference	O
summary	O
.	O
This	O
assumption	O
may	O
lead	O
to	O
performance	O
degradation	O
during	O
inference	O
,	O
where	O
the	O
model	O
needs	O
to	O
compare	O
several	O
system	O
-	O
generated	O
(	O
candidate	O
)	O
summaries	O
that	O
have	O
deviated	O
from	O
the	O
reference	O
summary	O
.	O
To	O
address	O
this	O
problem	O
,	O
we	O
propose	O
a	O
novel	O
training	O
paradigm	O
which	O
assumes	O
a	O
non	O
-	O
deterministic	O
distribution	O
so	O
that	O
different	O
candidate	O
summaries	O
are	O
assigned	O
probability	O
mass	O
according	O
to	O
their	O
quality	O
.	O
Our	O
method	O
achieves	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
result	O
on	O
the	O
CNN	B-DatasetName
/	I-DatasetName
DailyMail	I-DatasetName
(	O
47.78	B-MetricValue
ROUGE	B-MetricName
-	I-MetricName
1	I-MetricName
)	O
and	O
XSum	B-DatasetName
(	O
49.07	B-MetricValue
ROUGE	B-MetricName
-	I-MetricName
1	I-MetricName
)	O
datasets	O
.	O
Further	O
analysis	O
also	O
shows	O
that	O
our	O
model	O
can	O
estimate	O
probabilities	O
of	O
candidate	O
summaries	O
that	O
are	O
more	O
correlated	O
with	O
their	O
level	O
of	O
quality	O
.	O
1	O

For	O
embedding	O
and	O
annotation	O
projections	O
,	O
a	B-MethodName
bidirectional	I-MethodName
LSTM	I-MethodName
-	I-MethodName
based	I-MethodName
NER	I-MethodName
tagger	I-MethodName
using	O
a	O
CRF	O
decoder	O
is	O
adapted	O
to	O
build	O
our	O
NER	B-TaskName
models	O
(	O
Lample	O
et	O
al	O
,	O
2016	O
)	O
.	O
Details	O
of	O
the	O
hyperparameters	O
are	O
described	O
in	O
Appendix	O
A.3	O

We	O
further	O
study	O
the	O
domains	O
that	O
are	O
selected	O
by	O
the	O
methods	O
above	O
by	O
creating	O
confusion	O
matrices	O
between	O
the	O
domain	O
predictions	O
of	O
three	O
setups	O
:	O
domain	B-TaskName
classification	I-TaskName
,	O
domain	B-TaskName
prediction	I-TaskName
in	O
the	O
proposed	O
MultDomain	B-MethodName
-	I-MethodName
SP	I-MethodName
-	I-MethodName
Aux	I-MethodName
model	I-MethodName
and	O
the	O
oracle	O
in	O
-	O
domain	O
choice	O
on	O
gold	O
data	O
.	O
Figure	O
2	O
shows	O
that	O
the	O
Oracle	B-MethodName
model	I-MethodName
relies	O
on	O
the	O
corresponding	O
InDomain	O
model	O
to	O
only	O
a	O
limited	O
extent	O
for	O
each	O
model	O
.	O
In	O
uniformly	O
many	O
cases	O
,	O
predictions	O
from	O
other	O
in	O
-	O
domain	O
models	O
are	O
better	O
than	O
the	O
existing	O
in	O
-	O
domain	O
one	O
,	O
showing	O
the	O
variability	O
of	O
the	O
NER	B-TaskName
models	O
.	O
The	O
domain	O
classifier	O
predictions	O
align	O
closer	O
to	O
the	O
actual	O
domains	O
.	O
The	O
MultDomain	B-MethodName
-	I-MethodName
SP	I-MethodName
-	I-MethodName
Aux	I-MethodName
model	I-MethodName
also	O
tends	O
to	O
predict	O
the	O
domain	O
correctly	O
,	O
but	O
we	O
see	O
that	O
it	O
better	O
learns	O
the	O
NW	O
,	O
WB	O
and	O
BN	O
domains	O
.	O
Note	O
noting	O
that	O
the	O
MultDomain	B-MethodName
-	I-MethodName
SP	I-MethodName
-	I-MethodName
Aux	I-MethodName
model	I-MethodName
does	O
not	O
use	O
these	O
domain	O
predictions	O
in	O
inference	O
and	O
the	O
model	O
uses	O
the	O
shared	O
components	O
for	O
unknown	O
domains	O
or	O

We	O
use	O
a	O
collection	O
of	O
data	O
sets	O
spanning	O
eight	O
genres	O
to	O
evaluate	O
our	O
methods	O
.	O
In	O
addition	O
,	O
in	O
order	O
to	O
test	O
the	O
feasibility	O
of	O
NER	B-TaskName
tagging	O
in	O
a	O
zero	O
-	O
shot	O
domain	O
setup	O
,	O
we	O
present	O
additional	O
data	O
covering	O
four	O
other	O
genres	O
.	O
Each	O
genre	O
of	O
documents	O
is	O
considered	O
a	O
domain	O
in	O
modelling	O
.	O

The	O
basic	O
component	O
of	O
our	O
NER	B-TaskName
models	O
is	O
an	O
architecture	O
which	O
has	O
reached	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
several	O
times	O
over	O
the	O
last	O
few	O
years	O
(	O
Lample	O
et	O
al	O
,	O
2016	O
;	O
Peters	O
et	O
al	O
,	O
2018	O
;	O
Akbik	O
et	O
al	O
,	O
2018	O
)	O
.	O
Named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
task	O
is	O
a	O
structured	O
prediction	O
task	O
and	O
earlier	O
statistical	O
approaches	O
are	O
based	O
models	O
like	O
Conditional	O
Random	O
Fields	O
(	O
Lafferty	O
et	O
al	O
,	O
2001	O
)	O
,	O
which	O
rely	O
on	O
features	O
often	O
designed	O
based	O
on	O
domain	O
-	O
specific	O
knowledge	O
(	O
Luo	O
et	O
al	O
,	O
2015	O
)	O
.	O
The	O
current	O
dominant	O
approach	O
to	O
the	O
NER	B-TaskName
task	O
consists	O
of	O
neural	O
architectures	O
based	O
on	O
recurrent	O
neural	O
networks	O
with	O
different	O
choices	O
of	O
input	O
representations	O
Ma	O
and	O
Hovy	O
,	O
2016	O
;	O
Lample	O
et	O
al	O
,	O
2016	O
;	O
Peters	O
et	O
al	O
,	O
2018	O
;	O
Akbik	O
et	O
al	O
,	O
2018Akbik	O
et	O
al	O
,	O
,	O
2019	O
.	O
The	O
input	O
consists	O
of	O
a	O
concatenation	O
of	O
pretrained	O
word	O
embeddings	O
and	O
character	O
embeddings	O
.	O
Character	O
embeddings	O
are	O
trained	O
using	O
an	O
LSTM	O
from	O
randomly	O
initialized	O
vectors	O
as	O
in	O
(	O
Lample	O
et	O
al	O
,	O
2016	O
)	O
.	O
Word	O
embeddings	O
are	O
derived	O
from	O
a	O
combination	O
GloVe	O
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
and	O
FastText	O
(	O
Bojanowski	O
et	O
al	O
,	O
2017	O
)	O
pre	O
-	O
trained	O
word	O
embeddings	O
,	O
as	O
used	O
in	O
(	O
Ma	O
and	O
Hovy	O
,	O
2016	O
)	O
.	O
The	O
choice	O
of	O
embeddings	O
is	O
orthogonal	O
to	O
the	O
architecture	O
and	O
thus	O
,	O
we	O
hold	O
these	O
constant	O
in	O
all	O
experiments	O
.	O
This	O
representation	O
is	O
passed	O
through	O
two	O
LSTM	O
layers	O
that	O
process	O
the	O
input	O
sequence	O
in	O
differ	O
-	O
.	O
The	O
outputs	O
of	O
these	O
layers	O
are	O
concatenated	O
and	O
,	O
in	O
order	O
to	O
map	O
the	O
word	O
representation	O
obtained	O
from	O
the	O
LSTM	O
module	O
into	O
the	O
label	O
distribution	O
,	O
passed	O
to	O
a	O
one	O
-	O
layer	O
feed	O
-	O
forward	O
network	O
.	O
A	O
Conditional	O
Random	O
Field	O
is	O
applied	O
to	O
the	O
class	O
predictions	O
to	O
jointly	O
assign	O
the	O
sequence	O
tags	O
using	O
a	O
transition	O
matrix	O
.	O
This	O
CRF	O
layer	O
improves	O
performance	O
of	O
the	O
model	O
(	O
Lample	O
et	O
al	O
,	O
2016	O
)	O
as	O
it	O
ensures	O
the	O
output	O
sequence	O
takes	O
into	O
account	O
dependencies	O
between	O
the	O
tags	O
and	O
also	O
models	O
the	O
constraints	O
the	O
output	O
sequence	O
adheres	O
to	O
(	O
e.g.	O
I	O
-	O
PER	O
can	O
not	O
follow	O
B	O
-	O
LOC	O
)	O
.	O

This	O
section	O
describes	O
the	O
proposed	O
NER	B-TaskName
architecture	O
tailored	O
the	O
architecture	O
to	O
our	O
multi	O
-	O
domain	O
experimental	O
setups	O
,	O
which	O
is	O
independent	O
of	O
input	O
embedding	O
representation	O
.	O

In	O
this	O
paper	O
,	O
we	O
proposed	O
to	O
construct	O
sentiment	O
lexicons	O
based	O
on	O
a	O
sentiment	O
-	O
aware	O
word	O
representation	O
learning	O
approach	O
.	O
In	O
contrast	O
to	O
traditional	O
methods	O
normally	O
learned	O
based	O
on	O
only	O
the	O
document	O
-	O
level	O
sentiment	O
supervision	O
.	O
We	O
proposed	O
word	B-TaskName
representation	I-TaskName
learning	I-TaskName
via	O
hierarchical	B-MethodName
sentiment	I-MethodName
supervision	I-MethodName
,	O
i.e.	O
,	O
under	O
the	O
supervi	O
-	O
sion	O
at	O
both	O
word	O
and	O
document	O
levels	O
.	O
The	O
wordlevel	O
supervision	O
can	O
be	O
provided	O
based	O
on	O
either	O
predefined	O
sentiment	O
lexicons	O
or	O
the	O
learned	O
PMI	O
-	O
SO	O
based	O
sentiment	O
annotation	O
of	O
words	O
.	O
A	O
wide	O
range	O
of	O
experiments	O
were	O
conducted	O
on	O
several	O
benchmark	O
sentiment	O
classification	O
datasets	O
.	O
The	O
results	O
indicate	O
that	O
our	O
method	O
is	O
quite	O
effective	O
for	O
sentiment	O
-	O
aware	O
word	O
representation	O
,	O
and	O
the	O
sentiment	O
lexicon	O
generated	O
by	O
our	O
approach	O
beats	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
sentiment	O
lexicon	O
construction	O
approaches	O
.	O

Multi	B-TaskName
-	I-TaskName
Domain	I-TaskName
Named	I-TaskName
Entity	I-TaskName
Recognition	I-TaskName
with	O
Genre	O
-	O
Aware	O
and	O
Agnostic	O
Inference	O

Table	O
4	O
reports	O
the	O
results	O
on	O
ELI5	B-DatasetName
test	O
set	O
.	O
11	O
All	O
models	O
outperform	O
the	O
majority	O
and	O
summarylead	O
baselines	O
.	O
The	O
sequential	O
prediction	O
model	O
(	O
T5	B-MethodName
)	O
significantly	O
outperform	O
classification	O
model	O
(	O
RoBERTa	B-MethodName
)	O
which	O
makes	O
a	O
prediction	O
per	O
sentence	O
.	O
The	O
roles	O
with	O
lower	O
human	O
agreement	O
(	O
auxiliary	O
,	O
organizational	O
sentence	O
,	O
answer	O
)	O
also	O
exhibit	O
low	O
model	O
performances	O
,	O
reflecting	O
the	O
subjectivity	O
and	O
ambiguity	O
of	O
roles	O
for	O
some	O
sentences	O
.	O
Overall	O
,	O
with	O
a	O
moderate	O
amount	O
of	O
in	O
-	O
domain	O
annotated	O
data	O
,	O
our	O
best	O
model	O
(	O
T5	B-MethodName
-	I-MethodName
large	I-MethodName
)	O
can	O
reliably	O
classify	O
functional	O
roles	O
of	O
sentences	O
in	O
the	O
long	O
-	O
form	O
answers	O
,	O
showing	O
comparable	O
performances	O
to	O
human	O
lower	O
bound	O
.	O
Table	O
5	O
reports	O
the	O
results	O
on	O
the	O
three	O
out	O
-	O
ofdomain	O
datasets	O
,	O
WebGPT	B-DatasetName
,	O
NQ	B-DatasetName
and	O
ELI5	B-DatasetName
-	O
model	O
(	O
model	O
-	O
generated	O
answers	O
)	O
.	O
Human	B-MetricName
agreement	I-MetricName
numbers	I-MetricName
are	O
comparable	O
across	O
all	O
datasets	O
(	O
0.53	B-MetricValue
-	I-MetricValue
0.59	I-MetricValue
for	O
lower	O
bound	O
,	O
0.73	B-MetricValue
-	I-MetricValue
0.78	I-MetricValue
for	O
upper	O
bound	O
)	O
.	O
While	O
T5	B-MethodName
-	I-MethodName
large	I-MethodName
still	O
exhibits	O
the	O
best	O
overall	O
performance	O
,	O
all	O
learned	O
models	O
perform	O
worse	O
,	O
partially	O
as	O
the	O
role	O
distribution	O
has	O
changed	O
.	O
Despite	O
trained	O
on	O
the	O
ELI5	B-DatasetName
dataset	O
,	O
role	O
classification	O
model	O
also	O
perform	O
worse	O
on	O
model	O
-	O
generated	O
answers	O
(	O
ELI5model	B-MethodName
)	O
,	O
echoing	O
our	O
observation	O
that	O
human	O
annotators	O
find	O
it	O
challenging	O
to	O
process	O
the	O
discourse	O
structure	O
of	O
model	O
-	O
generated	O
answers	O
.	O
Our	O
pilot	O
showed	O
that	O
training	O
with	O
in	O
-	O
domain	O
data	O
improved	O
the	O
performances	O
consistently	O
,	O
but	O
the	O
evaluation	O
is	O
on	O
a	O
small	O
subset	O
(	O
after	O
setting	O
apart	O
some	O
for	O
training	O
)	O
,	O
so	O
we	O
do	O
not	O
report	O
it	O
here	O
.	O
We	O
anticipate	O
that	O
automatic	O
role	O
classification	O
is	O
feasible	O
given	O
moderate	O
amount	O
of	O
annotation	O
for	O
all	O
three	O
humanwritten	O
long	O
-	O
form	O
answer	O
datasets	O
we	O
study	O
.	O

Question	B-TaskName
Generation	I-TaskName
for	I-TaskName
Reading	I-TaskName
Comprehension	I-TaskName
Assessment	O
by	O
Modeling	O
How	O
and	O
What	O
to	O
Ask	O

To	O
understand	O
the	O
errors	O
generated	O
by	O
our	O
model	O
,	O
we	O
manually	O
classified	O
200	O
simplifications	O
from	O
the	O
NEWSELA	B-DatasetName
-	I-DatasetName
AUTO	I-DatasetName
test	O
set	O
into	O
the	O
following	O
categories	O
:	O
(	O
a	O
)	O
Good	O
,	O
where	O
the	O
model	O
generated	O
meaningful	O
simplifications	O
,	O
(	O
b	O
)	O
Hallucinations	O
,	O
where	O
the	O
model	O
introduced	O
information	O
not	O
in	O
the	O
input	O
,	O
(	O
c	O
)	O
Fluency	O
Errors	O
,	O
where	O
the	O
model	O
generated	O
ungrammatical	O
output	O
,	O
(	O
d	O
)	O
Anaphora	O
Resolution	O
,	O
where	O
it	O
was	O
difficult	O
to	O
resolve	O
pronouns	O
in	O
the	O
output	O
.	O
(	O
e	O
)	O
Bad	O
substitution	O
,	O
where	O
the	O
model	O
inserted	O
an	O
incorrect	O
simpler	O
phrase	O
,	O
and	O
(	O
e	O
)	O
Human	O
Reference	O
Errors	O
,	O
where	O
the	O
reference	O
does	O
not	O
reflect	O
the	O
source	O
sentence	O
.	O
Note	O
that	O
a	O
simplification	O
can	O
belong	O
to	O
multiple	O
error	O
categories	O
.	O
Table	O
7	O
shows	O
the	O
examples	O
of	O
each	O
category	O
.	O

In	O
this	O
paper	O
,	O
I	O
present	O
my	O
result	O
on	O
Offensive	B-TaskName
Language	I-TaskName
Identification	I-TaskName
in	O
Dravidian	O
Languages	O
-	O
EACL	O
2021	O
which	O
includes	O
three	O
tasks	O
of	O
different	O
languages	O
.	O
For	O
this	O
task	O
,	O
I	O
regard	O
it	O
as	O
a	O
multiple	O
classification	O
task	O
,	O
I	O
use	O
the	O
BiGRU	B-MethodName
-	I-MethodName
Attention	I-MethodName
based	O
on	O
the	O
ALBERT	B-MethodName
model	O
to	O
complete	O
,	O
and	O
my	O
model	O
works	O
very	O
well	O
.	O
I	O
also	O
summarized	O
the	O
possible	O
reasons	O
for	O
classifying	O
only	O
three	O
types	O
of	O
labels	O
.	O
At	O
the	O
same	O
time	O
,	O
I	O
also	O
use	O
some	O
other	O
neural	O
networks	O
for	O
comparative	O
experiments	O
to	O
prove	O
that	O
my	O
model	O
can	O
obtain	O
excellent	O
performance	O
.	O
The	O
result	O
shows	O
that	O
my	O
model	O
ranks	O
5th	O
in	O
the	O
Malayalam	O
task	O
.	O
Due	O
to	O
the	O
continuous	O
development	O
of	O
the	O
definition	O
of	O
offensive	O
information	O
on	O
the	O
Internet	O
,	O
it	O
is	O
difficult	O
to	O
accurately	O
describe	O
the	O
nature	O
of	O
this	O
information	O
only	O
from	O
the	O
perspective	O
of	O
data	O
mining	O
,	O
which	O
makes	O
it	O
impossible	O
to	O
model	O
this	O
information	O
effectively	O
.	O
In	O
the	O
future	O
,	O
I	O
will	O
use	O
methods	O
based	O
on	O
multidisciplinary	O
discovery	O
to	O
guide	O
model	O
learning	O
.	O
These	O
models	O
are	O
more	O
likely	O
to	O
use	O
limited	O
data	O
to	O
learn	O
more	O
effective	O
models	O
.	O
At	O
the	O
same	O
time	O
,	O
I	O
will	O
also	O
consider	O
whether	O
I	O
can	O
use	O
other	O
transfer	O
learning	O
models	O
to	O
perform	O
better	O
on	O
multi	O
-	O
classification	O
tasks	O
.	O

The	O
BiGRU	B-MethodName
-	I-MethodName
Attention	I-MethodName
model	I-MethodName
(	O
Cover	O
and	O
Hart	O
,	O
1967	O
)	O
is	O
divided	O
into	O
three	O
parts	O
:	O
text	O
vector	O
input	O
layer	O
,	O
hidden	O
layer	O
,	O
and	O
output	O
layer	O
.	O
Among	O
them	O
,	O
the	O
hidden	O
layer	O
consists	O
of	O
three	O
layers	O
:	O
the	O
BiGRU	O
layer	O
,	O
the	O
attention	O
layer	O
,	O
and	O
the	O
Dense	O
layer	O
(	O
fully	O
connected	O
layer	O
)	O
.	O
I	O
set	O
the	O
output	O
of	O
the	O
ALBERT	B-MethodName
model	O
as	O
the	O
input	O
.	O
After	O
receiving	O
the	O
input	O
,	O
it	O
uses	O
the	O
BiGRU	O
neural	O
network	O
layer	O
to	O
extract	O
features	O
of	O
the	O
deep	O
-	O
level	O
information	O
of	O
the	O
text	O
firstly	O
.	O
Secondly	O
,	O
it	O
uses	O
the	O
attention	O
layer	O
to	O
assign	O
corresponding	O
weights	O
to	O
the	O
deep	O
-	O
level	O
information	O
of	O
the	O
extracted	O
text	O
.	O
Finally	O
,	O
the	O
text	O
feature	O
information	O
with	O
different	O
weights	O
is	O
put	O
into	O
the	O
softmax	O
function	O
layer	O
for	O
classification	O
.	O
The	O
structure	O
of	O
the	O
BiGRU	B-MethodName
-	I-MethodName
Attention	I-MethodName
model	I-MethodName
is	O
shown	O
in	O
Figure	O
3	O
.	O

An	O
overall	O
framework	O
and	O
processing	O
pipeline	O
of	O
my	O
solution	O
are	O
shown	O
in	O
Figure	O
1	O
.	O
In	O
my	O
job	O
,	O
I	O
use	O
the	O
ALBERT	B-MethodName
model	O
as	O
my	O
base	O
model	O
and	O
take	O
BiGRU	O
-	O
Attention	O
behind	O
it	O
.	O
My	O
model	O
is	O
shown	O
in	O
Figure	O
2	O
.	O

