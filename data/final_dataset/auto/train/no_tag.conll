Total	O
number	O
train	O
16010	O
development	O
1999	O
test	O
2001	O
tion	O
of	O
computational	O
complexity	O
,	O
shallow	O
learning	O
models	O
generally	O
show	O
better	O
performance	O
than	O
deep	O
learning	O
models	O
.	O
Therefore	O
,	O
some	O
researchers	O
have	O
studied	O
the	O
design	O
of	O
shallow	O
models	O
in	O
specific	O
areas	O
of	O
data	O
replacement	O
.	O
Deep	O
learning	O
consists	O
of	O
multiple	O
hidden	O
layers	O
in	O
a	O
neural	O
network	O
(	O
Aroyehun	O
and	O
Gelbukh	O
,	O
2018	O
)	O
,	O
has	O
higher	O
complexity	O
,	O
and	O
can	O
be	O
trained	O
on	O
unstructured	O
data	O
.	O
The	O
deep	O
learning	O
architecture	O
can	O
directly	O
learn	O
feature	O
representations	O
from	O
the	O
input	O
without	O
excessive	O
manual	O
intervention	O
and	O
prior	O
knowledge	O
.	O
However	O
,	O
deep	O
learning	O
technology	O
is	O
a	O
data	O
-	O
driven	O
method	O
that	O
usually	O
requires	O
a	O
lot	O
of	O
data	O
to	O
achieve	O
high	O
performance	O
.	O
And	O
the	O
self	O
-	O
attention	O
-	O
based	O
model	O
can	O
bring	O
some	O
interword	O
interpretability	O
to	O
DNN	O
,	O
but	O
the	O
comparison	O
with	O
the	O
shallow	O
model	O
does	O
not	O
explain	O
why	O
and	O
how	O
it	O
works	O
.	O

This	O
is	O
a	O
comment	O
/	O
post	O
level	O
classification	O
task	O
.	O
Given	O
a	O
Youtube	O
comment	O
(	O
Chakravarthi	O
et	O
al	O
,	O
2020bChakravarthi	O
and	O
Muralidaran	O
,	O
2021	O
)	O
,	O
the	O
system	O
has	O
to	O
classify	O
it	O
into	O
one	O
of	O
the	O
five	O
categories	O
mentioned	O
in	O
the	O
Abstract	O
section	O
.	O
For	O
this	O
task	O
,	O
the	O
available	O
sentences	O
including	O
16010	O
training	O
sentences	O
,	O
1999	O
development	O
sentences	O
,	O
and	O
2001	O
testing	O
sentences	O
.	O
The	O
label	O
distribution	O
is	O
very	O
uneven	O
(	O
Not	O
-	O
offensive	O
label	O
accounts	O
88.4	O
%	O
.	O
The	O
label	O
with	O
the	O
second	O
largest	O
number	O
is	O
not	O
-	O
malayalam	O
,	O
which	O
accounts	O
for	O
only	O
0.08	O
%	O
of	O
the	O
total	O
.	O
And	O
there	O
are	O
relatively	O
fewer	O
labels	O
in	O
other	O
categories	O
.	O
)	O
The	O
number	O
of	O
sentences	O
for	O
each	O
domain	O
is	O
listed	O
in	O
Table	O
1	O
.	O

The	O
output	O
of	O
the	O
classification	O
result	O
is	O
shown	O
in	O
is	O
zero	O
.	O
N	O
ot	O
−	O
Of	O
f	O
ensive	O
labels	O
account	O
for	O
the	O
majority	O
,	O
accounting	O
for	O
91.15	O
%	O
of	O
the	O
total	O
number	O
of	O
labels	O
.	O
The	O
N	O
ot	O
−	O
M	O
alayalam	O
labels	O
account	O
for	O
the	O
second	O
most	O
significant	O
7.5	O
%	O
of	O
the	O
total	O
.	O
Offensive	O
-	O
Untargeted	O
labels	O
are	O
the	O
least	O
,	O
only	O
about	O
1	O
%	O
.	O
This	O
may	O
be	O
due	O
to	O
data	O
imbalance	O
(	O
N	O
ot	O
−	O
Of	O
f	O
ensive	O
labels	O
in	O
the	O
training	O
set	O
account	O
for	O
about	O
88	O
%	O
of	O
the	O
total	O
)	O
resulting	O
in	O
only	O
three	O
categories	O
being	O
identified	O
.	O

Fast	O
and	O
Accurate	O
Entity	O
Recognition	O
with	O
Iterated	O
Dilated	O
Convolutions	O

We	O
present	O
iterated	O
dilated	O
convolutional	O
neural	O
networks	O
,	O
fast	O
token	O
encoders	O
that	O
efficiently	O
aggregate	O
broad	O
context	O
without	O
losing	O
resolution	O
.	O
These	O
provide	O
impressive	O
speed	O
improvements	O
for	O
sequence	O
labeling	O
,	O
particularly	O
when	O
processing	O
entire	O
documents	O
at	O
a	O
time	O
.	O
In	O
the	O
future	O
we	O
hope	O
to	O
extend	O
this	O
work	O
to	O
NLP	O
tasks	O
with	O
richer	O
structured	O
output	O
,	O
such	O
as	O
parsing	O
.	O

Corrected	O
Examples	O
Starting	O
from	O
a	O
DP	O
-	O
based	O
solution	O
to	O
the	O
[	O
traveling	O
salesman	O
problem	O
]	O
Method	O
,	O
we	O
present	O
a	O
novel	O
technique	O
...	O

Based	O
on	O
the	O
correction	O
contributed	O
by	O
(	O
Wang	O
et	O
al	O
,	O
2019	O
)	O
,	O
we	O
use	O
the	O
proposed	O
method	O
to	O
justify	O
label	O
inconsistency	O
though	O
the	O
label	O
mistakes	O
take	O
"	O
only	O
"	O
5.38	O
%	O
.	O
It	O
also	O
validates	O
the	O
label	O
consistency	O
after	O
recovery	O
.	O
Figure	O
3	O
(	O
a	O
)	O
shows	O
that	O
starting	O
with	O
the	O
wrong	O
labels	O
in	O
the	O
original	O
test	O
set	O
makes	O
the	O
performance	O
worse	O
than	O
starting	O
with	O
the	O
training	O
set	O
or	O
the	O
good	O
test	O
subset	O
.	O
After	O
label	O
correction	O
,	O
this	O
issue	O
is	O
fixed	O
in	O
Figure	O
3	O
(	O
b	O
)	O
.	O

This	O
work	O
is	O
supported	O
by	O
National	O
Science	O
Foundation	O
IIS	O
-	O
1849816	O
and	O
CCF	O
-	O
1901059	O
.	O

In	O
this	O
section	O
,	O
we	O
compare	O
our	O
approach	O
to	O
various	O
sentence	O
simplification	O
models	O
using	O
both	O
automatic	O
and	O
manual	O
evaluations	O
.	O
We	O
show	O
that	O
our	O
model	O
achieves	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
and	O
can	O
adapt	O
easily	O
to	O
different	O
simplification	O
styles	O
,	O
such	O
as	O
paraphrasing	O
and	O
splitting	O
without	O
deletion	O
.	O

In	O
this	O
section	O
,	O
we	O
analyze	O
the	O
contribution	O
of	O
each	O
model	O
component	O
and	O
examine	O
the	O
system	O
errors	O
.	O

The	O
Seattle	O
kids	O
petitioned	O
Washington	O
state	O
last	O
year	O
to	O
adopt	O
stricter	O
science	O
-	O
based	O
regulations	O
to	O
protect	O
them	O
against	O
climate	O
change	O
.	O

The	O
eight	O
kids	O
from	O
Seattle	O
who	O
petitioned	O
Washington	O
state	O
last	O
year	O
are	O
fighting	O
for	O
the	O
state	O
to	O
adopt	O
stricter	O
laws	O
to	O
protect	O
their	O
future	O
against	O
climate	O
change	O
.	O

The	O
Seattle	O
kids	O
asked	O
the	O
Washington	O
state	O
last	O
year	O
to	O
take	O
tougher	O
rules	O
against	O
climate	O
change	O
.	O
Hallucinations	O
(	O
13	O
%	O
)	O
Complex	O
"	O
It	O
's	O
more	O
of	O
a	O
family	O
than	O
living	O
outside	O
,	O
"	O
said	O
Jessica	O
Konczal	O
,	O
33	O
,	O
whose	O
husband	O
is	O
Sgt	O
.	O
Matthew	O
Konczal	O
.	O

Jessica	O
Konczal	O
,	O
age	O
33	O
,	O
says	O
the	O
base	O
is	O
its	O
own	O
type	O
of	O
family	O
.	O
Our	O
Model	O
"	O
It	O
's	O
more	O
of	O
a	O
family	O
than	O
living	O
outside	O
,	O
"	O
said	O
Jessica	O
Konczal	O
,	O
one	O
of	O
the	O
protesters	O
.	O

Parental	O
feedback	O
on	O
the	O
menu	O
additions	O
so	O
far	O
,	O
from	O
some	O
of	O
the	O
early	O
adopter	O
markets	O
,	O
has	O
been	O
"	O
phenomenal	O
,	O
"	O
Leverton	O
said	O
.	O

Leverton	O
said	O
parents	O
love	O
the	O
new	O
food	O
.	O

So	O
far	O
,	O
parents	O
parents	O
have	O
feedback	O
on	O
the	O
menu	O
changes	O
has	O
been	O
a	O
great	O
deal	O
,	O
he	O
added	O
.	O

One	O
of	O
the	O
device	O
's	O
inventors	O
explained	O
to	O
the	O
president	O
that	O
the	O
machine	O
was	O
a	O
prototype	O
.	O

One	O
of	O
the	O
girls	O
explained	O
it	O
was	O
just	O
a	O
prototype	O
.	O

One	O
of	O
the	O
inventors	O
told	O
the	O
president	O
that	O
the	O
first	O
design	O
was	O
a	O
design	O
.	O
Anaphora	O
Resolution	O
/	O
Entailment	O
(	O
12	O
%	O
)	O
Complex	O
Sea	O
slugs	O
dubbed	O
sacoglossans	O
are	O
some	O
of	O
the	O
most	O
remarkable	O
biological	O
burglars	O
on	O
the	O
planet	O
.	O

The	O
sea	O
slugs	O
known	O
as	O
sacoglossans	O
are	O
some	O
of	O
the	O
most	O
interesting	O
burglars	O
on	O
the	O
planet	O
.	O

Scientists	O
say	O
these	O
are	O
some	O
of	O
the	O
most	O
interesting	O
creatures	O
on	O
the	O
planet	O
.	O
Human	O
Reference	O
Errors	O
(	O
4	O
%	O
)	O
Complex	O
They	O
float	O
in	O
and	O
out	O
of	O
formations	O
that	O
combine	O
the	O
underwater	O
world	O
with	O
the	O
stage	O
.	O

In	O
the	O
dance	O
on	O
a	O
round	O
stage	O
on	O
the	O
seabed	O
,	O
the	O
dancers	O
swim	O
together	O
and	O
apart	O
.	O

Experts	O
say	O
China	O
's	O
air	O
pollution	O
exacts	O
a	O
tremendous	O
toll	O
on	O
human	O
health	O
.	O

This	O
year	O
,	O
the	O
FAA	O
has	O
approved	O
dozens	O
of	O
permits	O
for	O
agricultural	O
drone	O
businesses	O
.	O

this	O
year	O
,	O
the	O
government	O
has	O
approved	O
dozens	O
of	O
permits	O
for	O
drone	O
businesses	O
for	O
no	O
permission	O
.	O
Our	O
Model	O
(	O
cp	O
=	O
0.6	O
)	O
this	O
year	O
,	O
the	O
faa	O
has	O
allowed	O
many	O
businesses	O
to	O
use	O
drones	O
.	O
Our	O
Model	O
(	O
cp	O
=	O
0.7	O
,	O
0.8	O
)	O
this	O
year	O
,	O
the	O
faa	O
has	O
approved	O
dozens	O
of	O
permits	O
for	O
drones	O
.	O

The	O
room	O
echoed	O
with	O
the	O
sounds	O
of	O
song	O
,	O
the	O
beat	O
of	O
drums	O
,	O
the	O
voices	O
of	O
young	O
men	O
.	O

LearningToQuestion	O
at	O
SemEval	O
2017	O
Task	O
3	O
:	O
Ranking	O
Similar	O
Questions	O
by	O
Learning	O
to	O
Rank	O
Using	O
Rich	O
Features	O

These	O
set	O
of	O
features	O
represent	O
the	O
lexical	O
similarity	O
between	O
question	O
texts	O
.	O
The	O
lexical	O
used	O
are	O
the	O
common	O
n	O
-	O
gram	O
(	O
n	O
=	O
1	O
,	O
2	O
,	O
3	O
)	O
counts	O
between	O
the	O
original	O
question	O
and	O
a	O
candidate	O
question	O
.	O
Apart	O
from	O
these	O
features	O
,	O
we	O
compute	O
a	O
count	O
vector	O
and	O
a	O
tfidf	O
vector	O
for	O
n	O
-	O
grams	O
(	O
n	O
=	O
1	O
,	O
2	O
,	O
3	O
)	O
for	O
both	O
the	O
question	O
and	O
candidate	O
question	O
and	O
compute	O
the	O
cosine	O
similarity	O
between	O
them	O
.	O

These	O
features	O
represent	O
the	O
syntactical	O
similarity	O
between	O
the	O
texts	O
of	O
questions	O
.	O
This	O
is	O
represented	O
by	O
cosine	O
similarity	O
of	O
POS	O
count	O
vector	O
for	O
ngram	O
(	O
n	O
=	O
1	O
,	O
2	O
,	O
3	O
)	O
.	O

We	O
compute	O
the	O
heuristic	O
based	O
on	O
length	O
in	O
tokens	O
of	O
both	O
the	O
texts	O
as	O
f	O
(	O
l	O
1	O
,	O
l	O
2	O
)	O
=	O
abs	O
(	O
l	O
1	O
−l	O
2	O
)	O
l	O
1	O
+	O
l	O
2	O
.	O

We	O
use	O
count	O
of	O
10	O
selected	O
common	O
question	O
words	O
also	O
as	O
a	O
feature	O
in	O
the	O
system	O
.	O
All	O
of	O
the	O
above	O
features	O
are	O
calculated	O
for	O
both	O
the	O
question	O
subject	O
and	O
body	O
separately	O
.	O

A	O
Appendix	O

Previous	O
comprehension	O
question	O
datasets	O
focused	O
on	O
either	O
inferential	O
or	O
literal	O
questions	O
.	O
Although	O
these	O
questions	O
assess	O
comprehension	O
skills	O
,	O
they	O
do	O
not	O
provide	O
fine	O
-	O
grained	O
evaluation	O
of	O
the	O
reader	O
comprehension	O
.	O
Thus	O
,	O
to	O
build	O
a	O
more	O
comprehensive	O
list	O
of	O
question	O
types	O
,	O
we	O
started	O
by	O
reviewing	O
curriculum	O
documents	O
available	O
from	O
Columbia	O
University	O
Teacher	O
's	O
College	O
Readers	O
5	O
and	O
Writers	O
Workshop	O
Program	O
6	O
.	O
Then	O
,	O
we	O
compiled	O
a	O
list	O
of	O
SBRCS	O
,	O
which	O
we	O
then	O
expanded	O
to	O
include	O
additional	O
skills	O
based	O
on	O
school	O
teachers	O
'	O
recommendations	O
.	O
In	O
Section	O
A.1	O
,	O
we	O
present	O
further	O
details	O
for	O
each	O
skill	O
type	O
.	O
Also	O
,	O
in	O
Appendix	O
A.2	O
,	O
we	O
give	O
further	O
details	O
on	O
the	O
skills	O
list	O
and	O
on	O
the	O
educational	O
theory	O
behind	O
the	O
skills	O
taxonomy	O
.	O
Our	O
final	O
list	O
contains	O
the	O
following	O
skills	O
:	O
1	O
.	O
Basic	O
Story	O
Elements	O
(	O
BSE	O
)	O
:	O
Can	O
the	O
reader	O
identify	O
the	O
story	O
's	O
main	O
characters	O
and	O
setting	O
?	O
From	O
the	O
details	O
in	O
this	O
passage	O
,	O
how	O
many	O
individuals	O
were	O
part	O
of	O
this	O
investigation	O
?	O
2	O
.	O
Character	O
Traits	O
(	O
CT	O
)	O
:	O
Can	O
the	O
reader	O
identify	O
the	O
traits	O
attributable	O
to	O
certain	O
characters	O
in	O
the	O
story	O
(	O
e.g.	O
character	O
feelings	O
,	O
physical	O
attributes	O
)	O
?	O
How	O
did	O
the	O
Rabbit	O
feel	O
in	O
this	O
passage	O
?	O
Which	O
word	O
in	O
the	O
passage	O
is	O
a	O
synonym	O
for	O
"	O
stubborn	O
"	O
?	O
With	O
our	O
list	O
of	O
SBRCS	O
as	O
a	O
guide	O
,	O
we	O
wrote	O
question	O
-	O
answer	O
pairs	O
for	O
each	O
story	O
.	O
Given	O
the	O
difficulty	O
of	O
the	O
task	O
,	O
we	O
needed	O
a	O
large	O
number	O
of	O
trained	O
content	O
writers	O
to	O
build	O
the	O
required	O
questions	O
.	O
Each	O
written	O
question	O
should	O
fall	O
into	O
one	O
of	O
the	O
mentioned	O
skills	O
.	O
For	O
that	O
,	O
a	O
total	O
of	O
25	O
professionals	O
contributed	O
to	O
the	O
writing	O
process	O
(	O
18	O
teachers	O
,	O
7	O
graduate	O
students	O
)	O
.	O
Each	O
annotator	O
was	O
asked	O
to	O
write	O
a	O
question	O
per	O
skill	O
for	O
a	O
given	O
story	O
.	O
Not	O
every	O
skill	O
is	O
applicable	O
to	O
every	O
story	O
,	O
so	O
some	O
skills	O
were	O
discarded	O
for	O
some	O
stories	O
.	O
We	O
chose	O
not	O
to	O
use	O
crowdworkers	O
(	O
e.g.	O
Amazon	O
Mechanical	O
Turk	O
)	O
to	O
ensure	O
high	O
-	O
quality	O
and	O
educationally	O
-	O
appropriate	O
questions	O
.	O
To	O
verify	O
the	O
quality	O
of	O
the	O
generated	O
content	O
,	O
a	O
second	O
team	O
member	O
reviews	O
each	O
question	O
-	O
answer	O
pair	O
before	O
adding	O
them	O
to	O
the	O
dataset	O
.	O
If	O
the	O
second	O
team	O
member	O
found	O
issues	O
,	O
a	O
discussion	O
took	O
place	O
.	O
In	O
the	O
cases	O
that	O
the	O
team	O
members	O
could	O
not	O
reach	O
an	O
agreement	O
,	O
a	O
third	O
team	O
member	O
is	O
brought	O
in	O
to	O
resolve	O
the	O
disagreement	O
.	O
In	O
addition	O
to	O
annotating	O
questions	O
with	O
a	O
skills	O
label	O
,	O
our	O
content	O
writers	O
annotate	O
each	O
question	O
as	O
either	O
Literal	O
or	O
Inferential	O
question	O
types	O
.	O
This	O
information	O
is	O
important	O
to	O
measure	O
the	O
comprehension	O
performance	O
of	O
the	O
reader	O
on	O
each	O
question	O
type	O
.	O
Overall	O
,	O
we	O
generate	O
4	O
K	O
question	O
-	O
answer	O
pairs	O
,	O
with	O
an	O
average	O
of	O
5.5	O
pairs	O
per	O
story	O
.	O
Note	O
that	O
we	O
did	O
not	O
ask	O
multiple	O
annotators	O
to	O
write	O
questions	O
per	O
story	O
in	O
order	O
to	O
measure	O
the	O
annotators	O
'	O
agreement	O
.	O
Different	O
annotators	O
often	O
write	O
the	O
same	O
question	O
in	O
different	O
ways	O
,	O
or	O
may	O
choose	O
a	O
different	O
question	O
topic	O
for	O
a	O
given	O
skill	O
,	O
or	O
even	O
select	O
a	O
different	O
skill	O
.	O
Thus	O
,	O
measuring	O
inter	O
-	O
annotator	O
agreement	O
is	O
not	O
meaningful	O
.	O
Instead	O
,	O
we	O
chose	O
to	O
ask	O
one	O
annotator	O
to	O
write	O
questions	O
and	O
another	O
to	O
validate	O
the	O
questions	O
grammatically	O
and	O
to	O
check	O
whether	O
the	O
question	O
is	O
correctly	O
related	O
to	O
the	O
chosen	O
skill	O
.	O

Decoding	O
strategies	O
are	O
crucial	O
and	O
directly	O
impact	O
output	O
quality	O
.	O
In	O
general	O
,	O
Beam	O
Search	O
(	O
Reddy	O
,	O
1977	O
)	O
is	O
the	O
most	O
common	O
algorithm	O
,	O
in	O
addition	O
to	O
some	O
other	O
sampling	O
techniques	O
such	O
as	O
Nucleus	O
sampling	O
(	O
Top	O
-	O
p	O
)	O
(	O
Holtzman	O
et	O
al	O
,	O
2019	O
)	O
.	O
In	O
Beam	O
Search	O
,	O
the	O
output	O
of	O
a	O
model	O
is	O
found	O
by	O
maximizing	O
the	O
model	O
probability	O
.	O
On	O
the	O
other	O
hand	O
,	O
Nucleus	O
sampling	O
selects	O
the	O
smallest	O
possible	O
set	O
of	O
tokens	O
whose	O
cumulative	O
probability	O
exceeds	O
the	O
probability	O
p.	O
Experimentally	O
,	O
we	O
found	O
that	O
using	O
the	O
top	O
-	O
p	O
(	O
p=0.9	O
)	O
algorithm	O
yields	O
the	O
best	O
results	O
in	O
terms	O
of	O
the	O
used	O
scoring	O
metrics	O
,	O
thus	O
we	O
use	O
it	O
in	O
all	O
of	O
our	O
experiments	O
.	O

Determining	O
what	O
are	O
the	O
main	O
story	O
elements	O
is	O
one	O
of	O
the	O
comprehension	O
skills	O
to	O
assess	O
the	O
reader	O
understanding	O
.	O
Using	O
this	O
skill	O
,	O
we	O
can	O
understand	O
whether	O
the	O
reader	O
is	O
able	O
to	O
identify	O
the	O
main	O
characters	O
and	O
environment	O
settings	O
of	O
the	O
stories	O
.	O
2	O
.	O
Character	O
Traits	O
(	O
CT	O
)	O
:	O
Identifying	O
permanent	O
traits	O
that	O
can	O
be	O
assigned	O
to	O
characters	O
or	O
describe	O
character	O
development	O
.	O
For	O
instance	O
,	O
knowing	O
what	O
most	O
likely	O
X	O
character	O
felt	O
during	O
the	O
story	O
,	O
recognizing	O
facts	O
about	O
X	O
,	O
identifying	O
main	O
adjectives	O
that	O
X	O
has	O
,	O
etc	O
.	O

Figurative	O
language	O
is	O
common	O
in	O
stories	O
as	O
it	O
makes	O
ideas	O
and	O
concepts	O
easier	O
to	O
visualize	O
by	O
the	O
reader	O
.	O
Also	O
,	O
it	O
is	O
an	O
effective	O
way	O
of	O
conveying	O
an	O
idea	O
that	O
is	O
not	O
easily	O
understood	O
.	O
With	O
this	O
skill	O
,	O
we	O
examine	O
the	O
reader	O
ability	O
of	O
recognizing	O
the	O
implicated	O
meaning	O
of	O
a	O
sentence	O
or	O
a	O
type	O
of	O
figurative	O
language	O
.	O
5	O
.	O
Inferring	O
(	O
I	O
)	O
:	O
Writers	O
sometimes	O
jump	O
into	O
the	O
action	O
or	O
skip	O
forward	O
in	O
their	O
stories	O
.	O
Good	O
readers	O
must	O
infer	O
what	O
happened	O
in	O
between	O
scenes	O
if	O
the	O
time	O
in	O
-	O
between	O
is	O
not	O
explicitly	O
detailed	O
.	O
In	O
addition	O
,	O
readers	O
must	O
infer	O
their	O
characters	O
'	O
emotions	O
if	O
their	O
characters	O
do	O
not	O
share	O
those	O
aloud	O
.	O

Consolidating	O
a	O
text	O
into	O
a	O
precise	O
synopsis	O
of	O
only	O
the	O
most	O
key	O
information	O
.	O
Summarizing	O
skill	O
contains	O
the	O
main	O
literary	O
elements	O
of	O
the	O
characters	O
,	O
the	O
problem	O
,	O
and	O
the	O
solutions	O
.	O
Key	O
events	O
from	O
the	O
beginning	O
,	O
middle	O
,	O
and	O
end	O
are	O
included	O
in	O
a	O
summary	O
.	O
8	O
.	O
Visualizing	O
(	O
V	O
)	O
:	O
This	O
skill	O
requires	O
readers	O
to	O
visualize	O
scenes	O
in	O
their	O
heads	O
to	O
fully	O
comprehend	O
the	O
story	O
.	O
It	O
can	O
assess	O
readers	O
ability	O
of	O
imagining	O
specific	O
events	O
or	O
elements	O
in	O
the	O
stories	O
.	O

Identifying	O
the	O
meaning	O
of	O
unfamiliar	O
words	O
in	O
the	O
text	O
is	O
a	O
key	O
skill	O
for	O
readers	O
to	O
fully	O
comprehend	O
the	O
story	O
.	O
In	O
this	O
skill	O
,	O
the	O
reader	O
should	O
identify	O
the	O
right	O
meaning	O
of	O
a	O
word	O
within	O
a	O
context	O
when	O
the	O
word	O
has	O
multiple	O
possible	O
definitions	O
.	O
Additionally	O
,	O
the	O
reader	O
should	O
be	O
able	O
to	O
identify	O
vocabulary	O
based	O
questions	O
related	O
to	O
identifying	O
synonyms	O
,	O
antonyms	O
,	O
homophones	O
,	O
compound	O
words	O
,	O
and	O
word	O
types	O
(	O
e.g.	O
noun	O
,	O
verb	O
,	O
etc	O
.	O
)	O
.	O

In	O
Table	O
6	O
,	O
we	O
show	O
the	O
fined	O
-	O
grained	O
results	O
per	O
skill	O
name	O
after	O
the	O
manual	O
labeling	O
experiment	O
for	O
the	O
generated	O
questions	O
from	O
both	O
One	O
-	O
Step	O
and	O
HTA	O
-	O
WTA	O
models	O
.	O

In	O
this	O
section	O
,	O
we	O
list	O
some	O
random	O
examples	O
from	O
HTA	O
-	O
WTA	O
model	O
for	O
inferential	O
questions	O
:	O
Story	O
:	O
"	O
The	O
Line	O
1	O
Toronto	O
train	O
was	O
a	O
subway	O
like	O
many	O
others	O
you	O
've	O
seen	O
.	O
He	O
rocketed	O
down	O
Yonge	O
Street	O
,	O
around	O
the	O
Union	O
loop	O
,	O
and	O
rattled	O
off	O
towards	O
Vaughn	O
.	O
At	O
Vaughn	O
he	O
'd	O
let	O
out	O
a	O
loud	O
,	O
hissing	O
sigh	O
and	O
a	O
clanking	O
sort	O
of	O
grunt	O
,	O
then	O
reverse	O
and	O
do	O
the	O
whole	O
thing	O
backwards	O
all	O
over	O
again	O
.	O
He	O
liked	O
his	O
transit	O
union	O
job	O
well	O
enough	O
,	O
but	O
he	O
could	O
n't	O
help	O
thinking	O
about	O
the	O
lights	O
at	O
the	O
end	O
of	O
his	O
tunnels	O
.	O
No	O
matter	O
how	O
long	O
he	O
'd	O
been	O
running	O
,	O
or	O
how	O
much	O
he	O
wished	O
for	O
anything	O
else	O
,	O
that	O
little	O
hopeful	O
point	O
of	O
light	O
always	O
turned	O
out	O
to	O
be	O
just	O
one	O
more	O
dirty	O
subway	O
platform	O
.	O
"	O
Generated	O
Figurative	O
Language	O
question	O
:	O
"	O
Reread	O
this	O
sentence	O
:	O
"	O
He	O
rocketed	O
down	O
Yonge	O
Street	O
,	O
around	O
the	O
Union	O
loop	O
,	O
and	O
rattled	O
off	O
towards	O
Vaughn	O
.	O
"	O
Which	O
figurative	O
language	O
technique	O
is	O
being	O
used	O
here	O
?	O
"	O
Generated	O
answer	O
:	O
"	O
Alliteration	O
"	O
.	O
Story	O
:	O
"	O
"	O
The	O
map	O
says	O
left	O
"	O
,	O
said	O
Bri	O
.	O
"	O
But	O
my	O
heart	O
says	O
right	O
!	O
"	O
cried	O
Rob	O
.	O
"	O
Is	O
your	O
heart	O
full	O
of	O
hidden	O
treasure	O
?	O
"	O
asked	O
Bri	O
.	O
"	O
Yes	O
.	O
"	O
Rob	O
replied	O
.	O
"	O
At	O
least	O
,	O
that	O
's	O
what	O
my	O
mom	O
says	O
.	O
"	O
"	O
Generated	O
Inferring	O
question	O
:	O
"	O
Why	O
do	O
you	O
think	O
Bri	O
's	O
heart	O
says	O
"	O
But	O
my	O
heart	O
says	O
right	O
!	O
"	O
?	O
"	O
Generated	O
answer	O
:	O
"	O
Because	O
she	O
thinks	O
she	O
has	O
found	O
something	O
"	O
.	O
Story	O
:	O
"	O
Mary	O
looked	O
at	O
it	O
,	O
not	O
really	O
knowing	O
why	O
the	O
hole	O
was	O
there	O
,	O
and	O
as	O
she	O
looked	O
she	O
saw	O
something	O
almost	O
buried	O
in	O
the	O
newlyturned	O
soil	O
.	O
It	O
was	O
something	O
like	O
a	O
ring	O
of	O
rusty	O
iron	O
or	O
brass	O
and	O
when	O
the	O
robin	O
flew	O
up	O
into	O
a	O
tree	O
nearby	O
she	O
put	O
out	O
her	O
hand	O
and	O
picked	O
the	O
ring	O
up	O
.	O
It	O
was	O
more	O
than	O
a	O
ring	O
,	O
however	O
;	O
it	O
was	O
an	O
old	O
key	O
which	O
looked	O
as	O
if	O
it	O
had	O
been	O
buried	O
a	O
long	O
time	O
.	O
Mistress	O
Mary	O
stood	O
up	O
and	O
looked	O
at	O
it	O
with	O
an	O
almost	O
frightened	O
face	O
as	O
it	O
hung	O
from	O
her	O
finger	O
.	O
"	O
Perhaps	O
it	O
has	O
been	O
buried	O
for	O
ten	O
years	O
,	O
"	O
she	O
said	O
in	O
a	O
whisper	O
.	O
"	O
Perhaps	O
it	O
is	O
the	O
key	O
to	O
the	O
garden	O
!	O
"	O
"	O
Generated	O
Vocabulary	O
question	O
:	O
"	O
Reread	O
this	O
sentence	O
:	O
"	O
Perhaps	O
it	O
has	O
been	O
buried	O
for	O
ten	O
years	O
"	O
What	O
is	O
the	O
correct	O
definition	O
of	O
the	O
word	O
"	O
frightened	O
"	O
as	O
it	O
is	O
used	O
here	O
?	O
"	O
Generated	O
answer	O
:	O
"	O
Scared	O
"	O
.	O

ADVISER	O
:	O
A	O
Toolkit	O
for	O
Developing	O
Multi	O
-	O
modal	O
,	O
Multi	O
-	O
domain	O
and	O
Socially	O
-	O
engaged	O
Conversational	O
Agents	O

We	O
extend	O
and	O
substantially	O
modify	O
our	O
previous	O
,	O
text	O
-	O
based	O
dialog	O
system	O
toolkit	O
(	O
Ortega	O
et	O
al	O
,	O
2019	O
)	O
while	O
following	O
the	O
same	O
design	O
choices	O
.	O
This	O
means	O
that	O
our	O
toolkit	O
is	O
meant	O
to	O
optimize	O
the	O
following	O
four	O
criteria	O
:	O
Modularity	O
,	O
Flexibility	O
,	O
Transparency	O
and	O
User	O
-	O
friendliness	O
at	O
different	O
levels	O
.	O
This	O
is	O
accomplished	O
by	O
decomposing	O
the	O
dialog	O
system	O
into	O
independent	O
modules	O
(	O
services	O
)	O
,	O
which	O
in	O
turn	O
are	O
either	O
rule	O
-	O
based	O
,	O
machine	O
learning	O
-	O
based	O
or	O
both	O
.	O
These	O
services	O
can	O
easily	O
be	O
combined	O
in	O
different	O
orders	O
/	O
architectures	O
,	O
providing	O
users	O
with	O
flexible	O
options	O
to	O
design	O
new	O
dialog	O
architectures	O
.	O

ADVISER	O
supports	O
three	O
options	O
to	O
access	O
information	O
from	O
external	O
information	O
sources	O
.	O
In	O
addition	O
to	O
being	O
able	O
to	O
query	O
information	O
from	O
SQL	O
-	O
based	O
databases	O
,	O
we	O
add	O
two	O
new	O
options	O
that	O
includes	O
querying	O
information	O
via	O
APIs	O
and	O
from	O
knowledge	O
bases	O
(	O
e.g.	O
Wikidata	O
(	O
Vrandečić	O
and	O
Krötzsch	O
,	O
2014	O
)	O
)	O
.	O
For	O
example	O
,	O
when	O
a	O
user	O
asks	O
a	O
simple	O
question	O
-	O
Where	O
was	O
Dirk	O
Nowitzki	O
born	O
?	O
,	O
our	O
pretrained	O
neural	O
network	O
predicts	O
the	O
topic	O
entity	O
-	O
Dirk	O
Nowitzki	O
-	O
and	O
the	O
relation	O
-	O
place	O
of	O
birth	O
.	O
Then	O
,	O
the	O
answer	O
is	O
automatically	O
looked	O
up	O
using	O
Wikidata	O
's	O
SPARQL	O
endpoint	O
.	O

To	O
allow	O
for	O
maximum	O
flexibility	O
in	O
combining	O
and	O
reusing	O
components	O
,	O
we	O
consider	O
a	O
dialog	O
system	O
as	O
a	O
group	O
of	O
services	O
which	O
communicate	O
asynchronously	O
by	O
publishing	O
/	O
subscribing	O
to	O
certain	O
topics	O
.	O
A	O
service	O
is	O
called	O
as	O
soon	O
as	O
at	O
least	O
one	O
message	O
for	O
all	O
its	O
subscribed	O
topics	O
is	O
received	O
and	O
may	O
additionally	O
publish	O
to	O
one	O
or	O
more	O
topics	O
.	O
Services	O
can	O
elect	O
to	O
receive	O
the	O
most	O
recent	O
message	O
for	O
a	O
topic	O
(	O
e.g.	O
up	O
-	O
to	O
-	O
date	O
belief	O
state	O
)	O
or	O
a	O
list	O
of	O
all	O
messages	O
for	O
that	O
topic	O
since	O
the	O
last	O
service	O
call	O
(	O
e.g.	O
a	O
list	O
of	O
video	O
frames	O
)	O
.	O
Constructing	O
a	O
dialog	O
system	O
in	O
this	O
way	O
allows	O
us	O
to	O
break	O
free	O
from	O
a	O
pipeline	O
architecture	O
.	O
Each	O
step	O
in	O
the	O
dialog	O
process	O
is	O
represented	O
by	O
one	O
or	O
more	O
services	O
which	O
can	O
operate	O
in	O
parallel	O
or	O
sequentially	O
.	O
For	O
example	O
,	O
tasks	O
like	O
video	O
and	O
speech	O
capture	O
may	O
be	O
performed	O
and	O
processed	O
in	O
parallel	O
before	O
being	O
synchronized	O
by	O
a	O
user	O
state	O
tracking	O
module	O
subscribing	O
to	O
input	O
from	O
both	O
sources	O
.	O
Figure	O
2	O
illustrates	O
the	O
system	O
architecture	O
.	O
For	O
debugging	O
purposes	O
,	O
we	O
provide	O
a	O
utility	O
to	O
draw	O
the	O
dialog	O
graph	O
,	O
showing	O
the	O
information	O
flow	O
between	O
services	O
,	O
including	O
remote	O
services	O
,	O
and	O
any	O
inconsistencies	O
in	O
publish	O
/	O
subscribe	O
connections	O
.	O

Services	O
are	O
location	O
-	O
transparent	O
and	O
may	O
thus	O
be	O
distributed	O
across	O
multiple	O
machines	O
.	O
A	O
central	O
dialog	O
system	O
discovers	O
local	O
and	O
remote	O
services	O
and	O
provides	O
synchronization	O
guarantees	O
for	O
dialog	O
initialization	O
and	O
termination	O
.	O
Distribution	O
of	O
services	O
enables	O
,	O
for	O
instance	O
,	O
a	O
more	O
powerful	O
computer	O
to	O
handle	O
tasks	O
such	O
as	O
real	O
-	O
time	O
text	O
-	O
to	O
-	O
speech	O
generation	O
(	O
see	O
Figure	O
2	O
)	O
.	O
This	O
is	O
particularly	O
helpful	O
when	O
multiple	O
resource	O
-	O
heavy	O
tasks	O
are	O
combined	O
into	O
a	O
single	O
dialog	O
system	O
.	O

In	O
addition	O
to	O
providing	O
multi	O
-	O
modal	O
support	O
,	O
the	O
publish	O
/	O
subscribe	O
framework	O
also	O
allows	O
for	O
multidomain	O
support	O
by	O
providing	O
a	O
structure	O
which	O
enables	O
arbitrary	O
branching	O
and	O
rejoining	O
of	O
graph	O
structures	O
.	O
When	O
a	O
service	O
is	O
created	O
,	O
users	O
simply	O
specify	O
which	O
domain	O
(	O
s	O
)	O
it	O
should	O
publish	O
/	O
subscribe	O
to	O
.	O
This	O
,	O
in	O
combination	O
with	O
a	O
domain	O
tracking	O
service	O
,	O
allows	O
for	O
seamless	O
integration	O
of	O
domain	O
-	O
agnostic	O
services	O
(	O
such	O
as	O
speech	O
input	O
/	O
output	O
)	O
and	O
domain	O
-	O
specific	O
services	O
(	O
such	O
as	O
NLU	O
/	O
NLG	O
for	O
the	O
lecturers	O
domain	O
)	O
.	O

We	O
provide	O
several	O
example	O
domains	O
to	O
demonstrate	O
ADVISER	O
's	O
functionalities	O
.	O
Databases	O
for	O
lecturers	O
and	O
courses	O
at	O
the	O
Institute	O
for	O
Natural	O
Language	O
Processing	O
(	O
IMS	O
)	O
,	O
which	O
we	O
used	O
in	O
the	O
previous	O
version	O
of	O
ADVISER	O
,	O
were	O
adapted	O
to	O
the	O
new	O
system	O
architecture	O
.	O
As	O
example	O
APIs	O
,	O
we	O
implemented	O
a	O
weather	O
domain	O
that	O
makes	O
calls	O
to	O
the	O
OpenWeatherMap	O
API	O
3	O
and	O
a	O
mensa	O
domain	O
for	O
gathering	O
information	O
from	O
the	O
dining	O
hall	O
at	O
the	O
university	O
of	O
Stuttgart	O
.	O
Note	O
that	O
affective	O
templates	O
were	O
only	O
added	O
to	O
the	O
lecturers	O
and	O
mensa	O
domain	O
.	O
All	O
domains	O
can	O
be	O
used	O
within	O
the	O
same	O
dialog	O
,	O
simply	O
by	O
switching	O
the	O
topic	O
.	O

We	O
introduce	O
ADVISER	O
-	O
an	O
open	O
-	O
source	O
,	O
multidomain	O
dialog	O
system	O
toolkit	O
that	O
allows	O
users	O
to	O
easily	O
develop	O
multi	O
-	O
modal	O
and	O
socially	O
-	O
engaged	O
conversational	O
agents	O
.	O
We	O
provide	O
a	O
large	O
variety	O
of	O
functionalities	O
,	O
ranging	O
from	O
speech	O
processing	O
to	O
core	O
dialog	O
system	O
capabilities	O
and	O
social	O
signal	O
processing	O
.	O
With	O
this	O
toolkit	O
,	O
we	O
hope	O
to	O
provide	O
a	O
flexible	O
platform	O
for	O
collaborative	O
research	O
in	O
multi	O
-	O
domain	O
,	O
multi	O
-	O
modal	O
,	O
socially	O
-	O
engaged	O
conversational	O
agents	O
.	O

Linked	O
Open	O
Treebanks	O
.	O
Interlinking	O
Syntactically	O
Annotated	O
Corpora	O
in	O
the	O
LiLa	O
Knowledge	O
Base	O
of	O
Linguistic	O
Resources	O
for	O
Latin	O

In	O
spite	O
of	O
the	O
current	O
availability	O
of	O
large	O
collections	O
of	O
treebanks	O
that	O
can	O
be	O
used	O
and	O
queried	O
from	O
one	O
common	O
place	O
on	O
the	O
web	O
,	O
we	O
are	O
still	O
far	O
from	O
achieving	O
a	O
real	O
interconnection	O
,	O
both	O
between	O
treebanks	O
themselves	O
and	O
with	O
other	O
(	O
kinds	O
of	O
)	O
linguistic	O
resources	O
.	O
However	O
,	O
making	O
resources	O
interoperable	O
is	O
a	O
crucial	O
requirement	O
to	O
maximize	O
the	O
contribution	O
of	O
each	O
single	O
resource	O
,	O
as	O
well	O
as	O
to	O
account	O
for	O
the	O
linguistic	O
complexity	O
of	O
the	O
texts	O
provided	O
by	O
(	O
annotated	O
)	O
corpora	O
and	O
particularly	O
by	O
treebanks	O
.	O
This	O
paper	O
describes	O
how	O
dependency	O
treebanks	O
are	O
interlinked	O
in	O
a	O
Knowledge	O
Base	O
of	O
linguistic	O
resources	O
for	O
Latin	O
based	O
on	O
Linked	O
Open	O
Data	O
practices	O
and	O
standards	O
.	O
The	O
Knowledge	O
base	O
is	O
built	O
to	O
make	O
linguistic	O
resources	O
interact	O
by	O
integrating	O
all	O
types	O
of	O
annotation	O
applied	O
to	O
a	O
particular	O
word	O
/	O
text	O
into	O
a	O
common	O
representation	O
.	O

In	O
this	O
section	O
,	O
we	O
discuss	O
how	O
we	O
integrated	O
the	O
Latin	O
treebanks	O
into	O
the	O
LiLa	O
Knowledge	O
Base	O
and	O
how	O
the	O
linked	O
data	O
obtained	O
by	O
connecting	O
the	O
treebank	O
tokens	O
to	O
the	O
other	O
resources	O
support	O
complex	O
queries	O
crossing	O
through	O
different	O
linguistic	O
resources	O
.	O

In	O
this	O
paper	O
,	O
we	O
have	O
described	O
how	O
we	O
interlinked	O
three	O
dependency	O
treebanks	O
for	O
Latin	O
(	O
one	O
available	O
in	O
two	O
versions	O
)	O
into	O
a	O
Knowledge	O
Base	O
of	O
linguistic	O
resources	O
based	O
on	O
Linked	O
Open	O
Data	O
practices	O
and	O
standards	O
.	O
Linking	O
resources	O
of	O
different	O
kind	O
(	O
such	O
as	O
corpora	O
and	O
lexica	O
)	O
makes	O
it	O
possible	O
to	O
exploit	O
their	O
potential	O
to	O
the	O
best	O
.	O
Indeed	O
,	O
single	O
resources	O
tend	O
to	O
focus	O
on	O
a	O
limited	O
set	O
of	O
linguistic	O
features	O
(	O
e.g.	O
morphology	O
and	O
syntax	O
for	O
treebanks	O
)	O
,	O
which	O
are	O
in	O
most	O
cases	O
insufficient	O
to	O
provide	O
a	O
full	O
analysis	O
of	O
the	O
textual	O
or	O
lexical	O
data	O
.	O
Making	O
interoperable	O
the	O
still	O
scattered	O
and	O
unconnected	O
resources	O
that	O
are	O
currently	O
available	O
for	O
Latin	O
(	O
as	O
well	O
as	O
for	O
many	O
other	O
languages	O
)	O
is	O
a	O
way	O
to	O
approach	O
the	O
data	O
from	O
the	O
various	O
layers	O
of	O
annotation	O
that	O
such	O
resources	O
provide	O
.	O
Our	O
work	O
of	O
interlinking	O
the	O
linguistic	O
resources	O
for	O
Latin	O
has	O
just	O
begun	O
.	O
In	O
the	O
near	O
future	O
,	O
we	O
plan	O
to	O
integrate	O
into	O
the	O
LiLa	O
Knowledge	O
Base	O
two	O
other	O
lexical	O
resources	O
,	O
namely	O
an	O
etymological	O
dictionary	O
(	O
de	O
Vaan	O
,	O
2008	O
)	O
and	O
the	O
Latin	O
WordNet	O
.	O
Interlinking	O
these	O
resources	O
with	O
the	O
textual	O
occurrences	O
of	O
their	O
lemmas	O
(	O
enriched	O
with	O
syntactic	O
annotation	O
in	O
treebanks	O
)	O
will	O
enable	O
the	O
users	O
of	O
LiLa	O
to	O
run	O
complex	O
queries	O
crossing	O
different	O
kinds	O
of	O
linguistic	O
features	O
.	O
Given	O
that	O
the	O
set	O
of	O
interlinked	O
resources	O
will	O
grow	O
in	O
the	O
coming	O
years	O
,	O
the	O
chain	O
of	O
connection	O
can	O
be	O
continued	O
indefinitely	O
;	O
as	O
long	O
as	O
new	O
lexical	O
resources	O
are	O
connected	O
to	O
the	O
Knowledge	O
Base	O
,	O
all	O
the	O
connections	O
from	O
any	O
corpus	O
token	O
to	O
their	O
nodes	O
will	O
become	O
explorable	O
in	O
the	O
network	O
.	O

This	O
project	O
has	O
received	O
funding	O
from	O
the	O
European	O
Research	O
Council	O
(	O
ERC	O
)	O
under	O
the	O
European	O
Union	O
's	O
Horizon	O
2020	O
research	O
and	O
innovation	O
programme	O
-	O
Grant	O
Agreement	O
No	O
769994	O
.	O

Original	O
:	O
all	O
the	O
employees	O
are	O
friendly	O
and	O
helpful	O
.	O
Transferred	O
:	O
all	O
the	O
employees	O
are	O
rude	O
and	O
unfriendly	O
.	O
Original	O
:	O
i	O
'	O
m	O
so	O
lucky	O
to	O
have	O
found	O
this	O
place	O
!	O
Transferred	O
:	O
i	O
'	O
m	O
so	O
embarrassed	O
that	O
i	O
picked	O
this	O
place	O
.	O

Compute	O
Q	O
t	O
via	O
(	O
5	O
)	O
,	O
and	O
update	O
π	O
φ	O
with	O
policy	O
gradient	O
via	O
(	O
8	O
)	O
.	O

We	O
use	O
the	O
same	O
initial	O
state	O
for	O
both	O
the	O
generator	O
and	O
the	O
guider	O
networks	O
.	O
For	O
conditional	O
generation	O
,	O
the	O
initial	O
state	O
is	O
the	O
encoded	O
latent	O
code	O
of	O
the	O
conditional	O
information	O
for	O
both	O
training	O
and	O
testing	O
.	O
For	O
unconditional	O
generation	O
,	O
the	O
initial	O
state	O
is	O
the	O
encoded	O
latent	O
code	O
of	O
a	O
target	O
sentence	O
in	O
training	O
and	O
random	O
noise	O
in	O
testing	O
.	O

Compute	O
evaluation	O
scores	O
based	O
on	O
references	O
.	O

Compute	O
Q	O
s	O
t	O
via	O
(	O
6	O
)	O
,	O
and	O
update	O
π	O
φ	O
with	O
policy	O
gradient	O
via	O
(	O
8	O
)	O
.	O
7	O
:	O
until	O
GMST	O
converges	O

Generated	O
Examples	O
Real	O
Data	O
What	O
this	O
group	O
does	O
is	O
to	O
take	O
down	O
various	O
different	O
websites	O
it	O
believes	O
to	O
be	O
criminal	O
and	O
leading	O
to	O
terrorist	O
acts	O
.	O
Over	O
1	O
,	O
600	O
a	O
day	O
have	O
reached	O
Greece	O
this	O
month	O
,	O
a	O
higher	O
rate	O
than	O
last	O
July	O
when	O
the	O
crisis	O
was	O
already	O
in	O
full	O
swing	O
.	O
"	O
We	O
'	O
re	O
working	O
through	O
a	O
legacy	O
period	O
,	O
with	O
legacy	O
products	O
that	O
are	O
10	O
or	O
20	O
years	O
old	O
,	O
"	O
he	O
says	O
.	O
'	O
The	O
first	O
time	O
anyone	O
says	O
you	O
need	O
help	O
,	O
I	O
'	O
m	O
on	O
the	O
defensive	O
,	O
but	O
that	O
'	O
s	O
all	O
that	O
I	O
know	O
.	O
Out	O
of	O
those	O
who	O
came	O
last	O
year	O
,	O
69	O
per	O
cent	O
were	O
men	O
,	O
18	O
per	O
cent	O
were	O
children	O
and	O
just	O
13	O
per	O
cent	O
were	O
women	O
.	O
He	O
has	O
not	O
played	O
for	O
Tottenham	O
'	O
s	O
first	O
team	O
since	O
and	O
it	O
is	O
now	O
nearly	O
two	O
years	O
since	O
he	O
completed	O
a	O
full	O
Premier	O
League	O
match	O
for	O
the	O
club	O
.	O
So	O
you	O
have	O
this	O
man	O
who	O
seems	O
to	O
represent	O
this	O
way	O
to	O
live	O
and	O
how	O
to	O
be	O
a	O
good	O
citizen	O
of	O
the	O
world	O
.	O
CNN	O
:	O
You	O
made	O
that	O
promise	O
,	O
but	O
it	O
wasn	O
'	O
t	O
until	O
45	O
years	O
later	O
that	O
you	O
acted	O
on	O
it	O
.	O
This	O
is	O
a	O
part	O
of	O
the	O
population	O
that	O
is	O
notorious	O
for	O
its	O
lack	O
of	O
interest	O
in	O
actually	O
showing	O
up	O
when	O
the	O
political	O
process	O
takes	O
place	O
.	O
They	O
picked	O
him	O
off	O
three	O
times	O
and	O
kept	O
him	O
out	O
of	O
the	O
end	O
zone	O
in	O
a	O
22	O
-	O
6	O
victory	O
at	O
Arizona	O
in	O
2013	O
.	O
The	O
treatment	O
was	O
going	O
to	O
cost	O
£	O
12	O
,	O
000	O
,	O
but	O
it	O
was	O
worth	O
it	O
for	O
the	O
chance	O
to	O
be	O
a	O
mum	O
.	O
But	O
if	O
black	O
political	O
power	O
is	O
so	O
important	O
,	O
why	O
hasn	O
'	O
t	O
it	O
made	O
more	O
of	O
a	O
difference	O
in	O
the	O
lives	O
of	O
poor	O
black	O
people	O
in	O
Baltimore	O
such	O
as	O
Gray	O
?	O
Local	O
media	O
reported	O
the	O
group	O
were	O
not	O
looking	O
to	O
hurt	O
anybody	O
,	O
but	O
they	O
would	O
not	O
rule	O
out	O
violence	O
if	O
police	O
tried	O
to	O
remove	O
them	O
.	O
The	O
idea	O
was	O
that	O
couples	O
got	O
six	O
months	O
'	O
leave	O
per	O
child	O
with	O
each	O
parent	O
entitled	O
to	O
half	O
the	O
days	O
each	O
.	O
The	O
55	O
to	O
43	O
vote	O
was	O
largely	O
split	O
down	O
party	O
lines	O
and	O
fell	O
short	O
of	O
the	O
60	O
votes	O
needed	O
for	O
the	O
bill	O
to	O
advance	O
.	O
Taiwan	O
'	O
s	O
Defence	O
Ministry	O
said	O
it	O
was	O
"	O
aware	O
of	O
the	O
information	O
,	O
"	O
and	O
declined	O
further	O
immediate	O
comment	O
,	O
Reuters	O
reported	O
.	O
I	O
'	O
m	O
racing	O
against	O
a	O
guy	O
who	O
I	O
lost	O
a	O
medal	O
to	O
-	O
but	O
am	O
I	O
ever	O
going	O
to	O
get	O
that	O
medal	O
back	O
?	O
Others	O
pushed	O
back	O
their	O
trips	O
,	O
meaning	O
flights	O
early	O
this	O
week	O
are	O
likely	O
to	O
be	O
even	O
more	O
packed	O
than	O
usual	O
.	O
"	O
In	O
theory	O
there	O
'	O
s	O
a	O
lot	O
to	O
like	O
,	O
"	O
Clinton	O
said	O
,	O
"	O
but	O
'	O
in	O
theory	O
'	O
isn	O
'	O
t	O
enough	O
.	O
If	O
he	O
makes	O
it	O
to	O
the	O
next	O
election	O
he	O
'	O
ll	O
lose	O
,	O
but	O
the	O
other	O
three	O
would	O
have	O
lost	O
just	O
as	O
much	O
.	O

A	O
general	O
trend	O
in	O
machine	O
learning	O
research	O
is	O
to	O
mathematically	O
model	O
the	O
input	O
-	O
output	O
relationship	O
from	O
a	O
dataset	O
.	O
This	O
is	O
carried	O
out	O
by	O
quantitatively	O
estimating	O
the	O
set	O
of	O
model	O
parameters	O
that	O
best	O
fit	O
the	O
data	O
.	O
The	O
approach	O
warrants	O
prior	O
(	O
to	O
fitting	O
)	O
examination	O
of	O
the	O
following	O
aspects	O
:	O
The	O
sufficiency	O
of	O
the	O
informative	O
data	O
to	O
the	O
estimate	O
model	O
parameters	O
,	O
i.e.	O
,	O
practical	O
identifiability	O
.	O
Thus	O
,	O
the	O
limitation	O
comes	O
from	O
the	O
dataset	O
quality	O
or	O
quantity	O
and	O
may	O
lead	O
to	O
ambiguous	O
data	O
interpretations	O
(	O
Raue	O
et	O
al	O
,	O
2009	O
)	O
.	O
The	O
possibility	O
that	O
the	O
structure	O
of	O
the	O
model	O
allows	O
its	O
parameters	O
to	O
be	O
uniquely	O
estimated	O
,	O
irrespective	O
of	O
the	O
quality	O
or	O
quantity	O
of	O
the	O
available	O
data	O
.	O
This	O
aspect	O
is	O
called	O
structural	O
identifiability	O
.	O
A	O
model	O
is	O
said	O
to	O
be	O
structurally	O
unidentifiable	O
if	O
a	O
different	O
set	O
of	O
parameters	O
yield	O
the	O
same	O
outcome	O
.	O
In	O
this	O
work	O
,	O
we	O
focus	O
on	O
the	O
structural	O
identifiability	O
(	O
Bellman	O
andÅström	O
,	O
1970	O
)	O
.	O
It	O
is	O
noteworthy	O
that	O
the	O
goodness	O
of	O
the	O
fit	O
of	O
a	O
model	O
on	O
the	O
data	O
does	O
not	O
dictate	O
its	O
structural	O
identifiability	O
.	O
Similar	O
to	O
Brunner	O
et	O
al	O
(	O
2019	O
)	O
,	O
we	O
focus	O
our	O
analysis	O
on	O
the	O
identifiability	O
of	O
attention	O
weights	O
,	O
which	O
are	O
not	O
model	O
parameters	O
,	O
yet	O
demands	O
meaningful	O
interpretations	O
and	O
are	O
crucial	O
to	O
the	O
stability	O
of	O
representations	O
learned	O
by	O
the	O
model	O
.	O

A.1	O
Span	O
,	O
Column	O
space	O
and	O
Row	O
space	O
Given	O
a	O
set	O
of	O
vectors	O
V	O
:	O
=	O
{	O
v	O
1	O
,	O
v	O
2	O
,	O
.	O
.	O
.	O
,	O
v	O
n	O
}	O
,	O
the	O
span	O
of	O
V	O
,	O
span	O
(	O
V	O
)	O
,	O
is	O
defined	O
as	O
the	O
set	O
obtained	O
from	O
all	O
the	O
possible	O
linear	O
combination	O
of	O
vectors	O
in	O
V	O
,	O
i.e.	O
,	O
span	O
(	O
V	O
)	O
:	O
=	O
{	O
n	O
i=1	O
λ	O
i	O
v	O
i	O
|	O
λ	O
i	O
R	O
,	O
i	O
{	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
}	O
.	O
The	O
span	O
(	O
V	O
)	O
can	O
also	O
be	O
seen	O
as	O
the	O
smallest	O
vector	O
space	O
that	O
contains	O
the	O
set	O
V.	O
Given	O
a	O
matrix	O
A	O
R	O
m×n	O
,	O
the	O
column	O
space	O
of	O
A	O
,	O
Cs	O
(	O
A	O
)	O
,	O
is	O
defined	O
as	O
space	O
spanned	O
by	O
its	O
column	O
vectors	O
.	O
Similarly	O
,	O
the	O
row	O
space	O
of	O
A	O
,	O
Rs	O
(	O
A	O
)	O
,	O
is	O
the	O
space	O
spanned	O
by	O
the	O
row	O
vectors	O
of	O
A.	O
Cs	O
(	O
A	O
)	O
and	O
Rs	O
(	O
A	O
)	O
are	O
the	O
subspaces	O
of	O
the	O
real	O
spaces	O
R	O
m	O
and	O
R	O
n	O
,	O
respectively	O
.	O
If	O
the	O
row	O
vectors	O
of	O
A	O
are	O
linearly	O
independent	O
,	O
the	O
Rs	O
(	O
A	O
)	O
will	O
span	O
R	O
m	O
.	O
A	O
similar	O
argument	O
holds	O
between	O
Cs	O
(	O
A	O
)	O
and	O
R	O
n	O
.	O

The	O
rank	O
of	O
a	O
matrix	O
P	O
(	O
denoted	O
as	O
rank	O
(	O
P	O
)	O
)	O
tells	O
about	O
the	O
dimensions	O
of	O
the	O
space	O
spanned	O
by	O
the	O
row	O
vectors	O
or	O
column	O
vectors	O
.	O
It	O
can	O
also	O
be	O
seen	O
as	O
the	O
number	O
of	O
linearly	O
independent	O
rows	O
or	O
columns	O
.	O
The	O
following	O
properties	O
hold	O
rank	O
P	O
≤	O
min	O
m	O
p	O
,	O
n	O
p	O
rank	O
P	O
Q	O
≤	O
min	O
rank	O
(	O
P	O
)	O
,	O
rank	O
(	O
Q	O
)	O
.	O
Where	O
,	O
P	O
and	O
Q	O
are	O
m	O
p	O
×	O
n	O
p	O
and	O
m	O
q	O
×	O
n	O
q	O
dimensional	O
matrices	O
,	O
respectively	O
.	O

This	O
research	O
is	O
supported	O
by	O
A*STAR	O
under	O
its	O
RIE	O
2020	O
Advanced	O
Manufacturing	O
and	O
Engineering	O
programmatic	O
grant	O
,	O
Award	O
No	O
.	O
-	O
A19E2b0098	O
.	O

Existing	O
large	O
-	O
scale	O
QA	O
data	O
sets	O
can	O
be	O
categorized	O
based	O
on	O
their	O
context	O
passage	O
length	O
in	O
two	O
groups	O
:	O
short	O
-	O
context	O
QA	O
,	O
i.e.	O
,	O
data	O
sets	O
with	O
paragraph	O
-	O
level	O
context	O
,	O
and	O
long	O
-	O
context	O
QA	O
,	O
i.e.	O
,	O
data	O
sets	O
with	O
multiple	O
-	O
paragraph	O
or	O
documentlevel	O
context	O
.	O
Long	O
-	O
context	O
QA	O
can	O
potentially	O
include	O
questions	O
demanding	O
long	O
answers	O
.	O
In	O
this	O
section	O
,	O
we	O
only	O
review	O
QA	O
datasets	O
.	O
However	O
,	O
it	O
is	O
worth	O
noting	O
that	O
very	O
recently	O
,	O
(	O
Tay	O
et	O
al	O
,	O
2020a	O
)	O
introduced	O
a	O
unified	O
benchmark	O
using	O
different	O
tasks	O
for	O
evaluating	O
model	O
quality	O
under	O
long	O
-	O
context	O
scenarios	O
.	O

NLQuAD	O
consists	O
of	O
news	O
articles	O
as	O
context	O
documents	O
,	O
interrogative	O
sub	O
-	O
headings	O
in	O
the	O
articles	O
as	O
questions	O
,	O
and	O
body	O
paragraphs	O
corresponding	O
to	O
the	O
sub	O
-	O
headings	O
as	O
contiguous	O
answers	O
to	O
the	O
questions	O
.	O
We	O
automatically	O
extract	O
target	O
answers	O
because	O
annotating	O
for	O
non	O
-	O
factoid	O
long	O
QA	O
is	O
rather	O
challenging	O
and	O
costly	O
.	O
To	O
ensure	O
the	O
qual	O
-	O
ity	O
of	O
answers	O
in	O
addition	O
to	O
the	O
initial	O
investigations	O
,	O
we	O
perform	O
human	O
evaluations	O
(	O
Section	O
5.3	O
)	O
.	O
We	O
choose	O
the	O
BBC	O
news	O
website	O
as	O
the	O
resource	O
of	O
our	O
documents	O
and	O
the	O
question	O
-	O
answer	O
pairs	O
,	O
mainly	O
because	O
its	O
articles	O
contain	O
a	O
considerable	O
amount	O
of	O
high	O
-	O
quality	O
question	O
-	O
like	O
sub	O
-	O
headings	O
which	O
are	O
suitable	O
for	O
the	O
QA	O
task	O
.	O
NLQuAD	O
's	O
characteristics	O
make	O
it	O
an	O
appealing	O
and	O
challenging	O
data	O
set	O
for	O
the	O
non	O
-	O
factoid	O
long	O
QA	O
task	O
:	O
Its	O
context	O
documents	O
are	O
long	O
,	O
and	O
its	O
questions	O
are	O
non	O
-	O
factoid	O
in	O
a	O
way	O
that	O
can	O
not	O
be	O
answered	O
by	O
single	O
or	O
multiple	O
entities	O
.	O
The	O
questions	O
are	O
addressed	O
by	O
more	O
than	O
seven	O
sentences	O
on	O
average	O
.	O
Meanwhile	O
,	O
it	O
covers	O
a	O
wide	O
range	O
of	O
topics	O
,	O
making	O
it	O
an	O
open	O
-	O
domain	O
QA	O
data	O
set	O
.	O
The	O
BBC	O
news	O
articles	O
typically	O
follow	O
a	O
specific	O
template	O
.	O
They	O
begin	O
with	O
an	O
introductory	O
section	O
consisting	O
of	O
news	O
summaries	O
(	O
Narayan	O
et	O
al	O
,	O
2018	O
)	O
and	O
one	O
or	O
more	O
sections	O
accompanied	O
by	O
sub	O
-	O
headings	O
.	O
Each	O
section	O
contains	O
multiple	O
short	O
to	O
medium	O
-	O
length	O
paragraphs	O
.	O
We	O
remove	O
the	O
template	O
and	O
section	O
break	O
-	O
lines	O
to	O
prevent	O
revealing	O
possible	O
answer	O
boundaries	O
.	O

We	O
exploit	O
Wayback	O
Machine	O
,	O
2	O
a	O
digital	O
archive	O
of	O
the	O
Web	O
,	O
and	O
Wayback	O
Machine	O
Scraper	O
3	O
to	O
scrape	O
the	O
article	O
archives	O
.	O
Links	O
in	O
the	O
scraped	O
pages	O
are	O
used	O
to	O
collect	O
additional	O
pages	O
from	O
the	O
original	O
website	O
.	O
We	O
scraped	O
the	O
English	O
BBC	O
news	O
website	O
from	O
2016	O
to	O
2020	O
as	O
a	O
limited	O
number	O
of	O
questions	O
can	O
be	O
found	O
in	O
articles	O
before	O
2016	O
.	O
Only	O
textual	O
information	O
is	O
kept	O
and	O
we	O
strip	O
away	O
multimedia	O
objects	O
and	O
hyperlinks	O
outside	O
of	O
the	O
body	O
of	O
the	O
articles	O
.	O
Duplicate	O
documents	O
are	O
removed	O
and	O
questions	O
with	O
bullet	O
list	O
answer	O
types	O
are	O
discarded	O
.	O
We	O
detect	O
interrogative	O
sub	O
-	O
headings	O
by	O
checking	O
if	O
they	O
end	O
with	O
a	O
question	O
mark	O
.	O

NLQuAD	O
contains	O
31k	O
non	O
-	O
factoid	O
questions	O
based	O
on	O
13k	O
supporting	O
documents	O
from	O
news	O
articles	O
.	O
Table	O
2	O
shows	O
the	O
data	O
set	O
statistics	O
.	O
We	O
randomly	O
partition	O
the	O
data	O
set	O
into	O
training	O
(	O
80	O
%	O
)	O
,	O
development	O
(	O
10	O
%	O
)	O
,	O
and	O
evaluation	O
(	O
10	O
%	O
)	O
sets	O
.	O
While	O
NLQuAD	O
has	O
long	O
documents	O
and	O
longanswer	O
QA	O
pairs	O
,	O
the	O
histograms	O
in	O
Figure	O
2	O
indicate	O
the	O
wide	O
range	O
of	O
samples	O
.	O
Figure	O
3	O
We	O
manually	O
investigated	O
100	O
randomly	O
sampled	O
question	O
-	O
answer	O
pairs	O
from	O
the	O
NLQuAD	O
training	O
set	O
and	O
find	O
that	O
87	O
%	O
of	O
the	O
questions	O
are	O
not	O
self	O
-	O
contained	O
and	O
require	O
additional	O
contextual	O
information	O
to	O
be	O
understood	O
or	O
disambiguated	O
.	O
Most	O
of	O
the	O
answers	O
consist	O
of	O
explanations	O
,	O
descriptions	O
,	O
or	O
opinions	O
,	O
and	O
only	O
2	O
%	O
of	O
the	O
questions	O
can	O
be	O
answered	O
by	O
a	O
short	O
span	O
of	O
text	O
.	O

This	O
research	O
was	O
partly	O
supported	O
by	O
VIVAT	O
.	O
We	O
thank	O
the	O
BBC	O
for	O
giving	O
permission	O
to	O
publish	O
our	O
extracted	O
data	O
for	O
non	O
-	O
commercial	O
,	O
research	O
purposes	O
.	O
We	O
also	O
thank	O
our	O
volunteers	O
for	O
providing	O
human	O
assessments	O
.	O

Interpreting	O
Emoji	O
with	O
Emoji	O
:	O
⇒	O

We	O
explain	O
next	O
our	O
deployed	O
tool	O
for	O
creating	O
interpretable	O
word	O
-	O
emoji	O
embeddings	O
:	O
PO	O
-	O
LAR	O
(	O
Mathew	O
et	O
al	O
,	O
2020	O
)	O
;	O
and	O
provide	O
detail	O
on	O
a	O
revised	O
POLAR	O
extension	O
via	O
projection	O
.	O

−	O
E	O
v	O
=	O
(	O
dir	O
T	O
)	O
−1	O
−	O
W	O
a	O
v	O
yielding	O
an	O
interpretable	O
subspace	O
along	O
the	O
differentials	O
−	O
−	O
dir	O
i	O
that	O
carries	O
over	O
specific	O
geometric	O
semantics	O
from	O
the	O
input	O
embedding	O
.	O
I.e.	O
,	O
for	O
each	O
word	O
v	O
V	O
within	O
the	O
resulting	O
interpretable	O
embedding	O
E	O
,	O
its	O
embedding	O
vector	O
−	O
E	O
v	O
now	O
carries	O
a	O
measure	O
along	O
each	O
polar	O
dimension	O
's	O
semantics	O
.	O
Limitations	O
.	O
Polar	O
opposites	O
being	O
very	O
close	O
in	O
the	O
original	O
embedding	O
space	O
might	O
tear	O
apart	O
.	O
From	O
a	O
technical	O
perspective	O
,	O
the	O
used	O
pseudo	O
inverse	O
for	O
the	O
base	O
change	O
becomes	O
numerically	O
ill	O
-	O
conditioned	O
if	O
d	O
≈	O
N	O
(	O
Mathew	O
et	O
al	O
,	O
2020	O
)	O
.	O

While	O
the	O
base	O
change	O
approach	O
seems	O
natural	O
,	O
its	O
given	O
limitations	O
lead	O
us	O
to	O
propose	O
a	O
variant	O
that	O
comes	O
with	O
several	O
benefits	O
.	O
Instead	O
of	O
creating	O
a	O
new	O
interpretable	O
vector	O
space	O
,	O
we	O
take	O
measurements	O
on	O
the	O
differentials	O
dir	O
defined	O
as	O
before	O
(	O
Fig	O
.	O
2a	O
,	O
red	O
dashed	O
vectors	O
)	O
.	O
However	O
,	O
we	O
now	O
project	O
each	O
embedding	O
vector	O
−	O
W	O
v	O
for	O
v	O
orthogonally	O
onto	O
the	O
differentials	O
as	O
shown	O
in	O
Fig	O
.	O
2b	O
(	O
blue	O
dotted	O
vectors	O
)	O
.	O
This	O
leads	O
to	O
a	O
smallest	O
distance	O
between	O
both	O
lines	O
w.r.t	O
.	O
the	O
differential	O
,	O
yet	O
simultaneously	O
allows	O
for	O
a	O
direct	O
scale	O
measure	O
on	O
the	O
differential	O
vector	O
as	O
shown	O
in	O
Fig	O
.	O
2b	O
&	O
Fig	O
.	O
2c	O
(	O
green	O
vectors	O
)	O
.	O
Thereby	O
,	O
we	O
also	O
decouple	O
the	O
transformation	O
matrix	O
,	O
which	O
eases	O
later	O
add	O
-	O
ins	O
to	O
the	O
interpretable	O
embedding	O
.	O
Orthogonal	O
projection	O
(	O
blue	O
dotted	O
vectors	O
)	O
of	O
each	O
input	O
embedding	O
vector	O
−	O
W	O
a	O
v	O
onto	O
a	O
differential	O
i	O
provides	O
us	O
the	O
adjacent	O
leg	O
vector	O
as	O
follows	O
:	O
oproj	O
dir	O
i	O
(	O
−	O
W	O
a	O
v	O
)	O
=	O
−	O
W	O
a	O
v	O
−	O
−	O
dir	O
i	O
|	O
−	O
−	O
dir	O
i	O
|	O
scalar	O
−	O
−	O
dir	O
i	O
|	O
−	O
−	O
dir	O
i	O
|	O
direction	O
As	O
this	O
adjacent	O
leg	O
(	O
green	O
vectors	O
)	O
's	O
direction	O
naturally	O
equals	O
the	O
differential	O
,	O
we	O
focus	O
only	O
on	O
the	O
scalar	O
part	O
representing	O
a	O
direct	O
scale	O
measure	O
.	O
By	O
normalizing	O
the	O
differential	O
vector	O
lengthŝ	O
dir	O
=	O
dir	O
|	O
dir	O
|	O
−1	O
,	O
the	O
projected	O
scale	O
value	O
conveniently	O
results	O
in	O
:	O
oproj	O
scalar	O
dir	O
i	O
(	O
−	O
W	O
a	O
v	O
)	O
=	O
−	O
W	O
a	O
v	O
−	O
−	O
dir	O
i	O
.	O
This	O
transformation	O
allows	O
to	O
create	O
a	O
new	O
interpretable	O
embedding	O
E	O
R	O
|	O
V	O
|	O
×N	O
for	O
each	O
embedding	O
vector	O
−	O
W	O
a	O
v	O
(	O
exemplified	O
in	O
Fig	O
.	O
1	O
)	O
as	O
follows	O
:	O
−	O
E	O
v	O
=	O
oproj	O
scalar	O
dir	O
(	O
−	O
W	O
a	O
v	O
)	O
=	O
d	O
ir	O
T	O
−	O
W	O
a	O
v	O
R	O
N	O
Computationally	O
it	O
requires	O
an	O
inital	O
matrix	O
multiplication	O
for	O
each	O
embedded	O
term	O
;	O
Dimension	O
increments	O
require	O
a	O
dot	O
product	O
on	O
each	O
term	O
.	O
Downstream	O
Tasks	O
.	O
Other	O
experiments	O
indicate	O
POLAR	O
ρ	O
downstream	O
task	O
performance	O
being	O
on	O
par	O
with	O
the	O
input	O
embedding	O
,	O
and	O
an	O
edge	O
over	O
base	O
change	O
POLAR	O
if	O
d	O
≈	O
N	O
(	O
not	O
shown	O
)	O
.	O

POLAR	O
ρ	O
can	O
create	O
interpretable	O
embeddings	O
w.r.t	O
.	O
a	O
-	O
priori	O
provided	O
opposites	O
.	O
We	O
next	O
describe	O
how	O
we	O
select	O
these	O
opposites	O
to	O
make	O
POLAR	O
ρ	O
applicable	O
to	O
our	O
data	O
.	O
Most	O
importantly	O
,	O
the	O
approach	O
requires	O
being	O
part	O
of	O
or	O
locating	O
desired	O
opposites	O
within	O
the	O
original	O
embedding	O
space	O
.	O
Words	O
.	O
As	O
we	O
extend	O
the	O
word	O
embedding	O
space	O
with	O
emoji	O
,	O
we	O
still	O
want	O
to	O
use	O
words	O
.	O
We	O
find	O
common	O
sources	O
of	O
polar	O
opposites	O
in	O
antonym	O
wordlists	O
(	O
Shwartz	O
et	O
al	O
,	O
2017	O
)	O
as	O
used	O
in	O
the	O
original	O
POLAR	O
work	O
.	O
To	O
fit	O
our	O
German	O
dataset	O
,	O
we	O
translated	O
and	O
manually	O
checked	O
all	O
pairs	O
keeping	O
1275	O
items	O
.	O
From	O
GermaNet	O
(	O
Hamp	O
and	O
Feldweg	O
,	O
1997	O
)	O
,	O
we	O
extracted	O
1732	O
word	O
pairs	O
via	O
antonym	O
relations	O
leading	O
to	O
|	O
P	O
words	O
|	O
=	O
1832	O
word	O
pairs	O
.	O
Emoji	O
.	O
Being	O
not	O
ideal	O
,	O
but	O
due	O
to	O
lack	O
of	O
better	O
alternatives	O
,	O
we	O
ended	O
up	O
heuristically	O
creating	O
semantic	O
opposites	O
from	O
emoji	O
through	O
qualitative	O
surveys	O
across	O
friends	O
and	O
colleagues	O
resulting	O
in	O
|	O
P	O
emoji	O
|	O
=	O
44	O
emoji	O
pairs	O
,	O
cf	O
.	O
Tab	O
.	O
3	O
.	O
While	O
we	O
could	O
use	O
far	O
more	O
opposites	O
especially	O
of	O
facial	O
emoji	O
,	O
due	O
to	O
emoji	O
clustering	O
in	O
the	O
input	O
embedding	O
,	O
spanned	O
expressive	O
space	O
would	O
arguably	O
become	O
redundant	O
at	O
similar	O
EWSO	O
scores	O
for	O
many	O
directions	O
.	O
Effectively	O
it	O
may	O
bias	O
interpretability	O
over	O
proportionally	O
towards	O
facial	O
emoji	O
.	O

While	O
we	O
have	O
now	O
created	O
a	O
supposedly	O
interpretable	O
embedding	O
,	O
it	O
remains	O
to	O
be	O
seen	O
how	O
well	O
it	O
is	O
perceived	O
by	O
humans	O
.	O
That	O
is	O
,	O
we	O
next	O
evaluate	O
our	O
two	O
key	O
RQs	O
,	O
discuss	O
significance	O
,	O
and	O
provide	O
further	O
details	O
:	O
RQ1	O
)	O
How	O
well	O
does	O
POLAR	O
ρ	O
with	O
EWSO	O
perform	O
in	O
selecting	O
most	O
interpretable	O
dimensions	O
at	O
varying	O
expressiveness	O
of	O
words	O
and	O
emoji	O
?	O
RQ2	O
)	O
How	O
well	O
do	O
POLAR	O
ρ	O
scalar	O
values	O
reflect	O
directions	O
on	O
the	O
differential	O
scales	O
?	O
i	O
)	O
Do	O
humans	O
prefer	O
emoji	O
to	O
words	O
?	O
ii	O
)	O
How	O
well	O
do	O
human	O
raters	O
align	O
w.r.t	O
.	O
interpretability	O
?	O
iii	O
)	O
What	O
impact	O
do	O
demographic	O
factors	O
play	O
in	O
interpretability	O
with	O
or	O
without	O
emoji	O
?	O

To	O
gather	O
human	O
judgement	O
,	O
we	O
employ	O
crowdsourcing	O
on	O
the	O
Microworkers	O
platform	O
.	O

Our	O
evaluation	O
of	O
the	O
POLAR	O
ρ	O
approach	O
including	O
emoji	O
to	O
the	O
differentials	O
bases	O
on	O
two	O
main	O
questions	O
next	O
to	O
demographics	O
.	O
Selection	O
test	O
.	O
Analogous	O
to	O
the	O
original	O
work	O
,	O
we	O
want	O
to	O
find	O
out	O
whether	O
humans	O
agree	O
on	O
best	O
interpretability	O
of	O
POLAR	O
ρ	O
selected	O
differentials	O
with	O
a	O
word	O
intrusion	O
task	O
.	O
The	O
question	O
asks	O
our	O
coders	O
to	O
select	O
five	O
out	O
of	O
ten	O
differentials	O
that	O
describe	O
a	O
given	O
word	O
best	O
as	O
shown	O
in	O
Fig	O
.	O
3b	O
.	O
We	O
select	O
half	O
of	O
these	O
dimensions	O
according	O
to	O
the	O
highest	O
absolute	O
projection	O
scale	O
values	O
(	O
most	O
extreme	O
)	O
.	O
The	O
other	O
half	O
consists	O
of	O
a	O
random	O
selection	O
from	O
the	O
bottom	O
half	O
of	O
available	O
differentials	O
.	O
I.e.	O
,	O
if	O
the	O
projection	O
approach	O
determines	O
interpretable	O
dimensions	O
well	O
,	O
humans	O
would	O
choose	O
all	O
five	O
out	O
of	O
five	O
POLAR	O
ρ	O
chosen	O
differentials	O
.	O
As	O
any	O
user	O
might	O
choose	O
differently	O
,	O
we	O
count	O
how	O
often	O
coders	O
choose	O
certain	O
differentials	O
.	O
The	O
resulting	O
frequencies	O
immediately	O
translate	O
in	O
a	O
ranking	O
that	O
we	O
leverage	O
for	O
calculating	O
the	O
fraction	O
of	O
Top	O
1	O
..	O
5	O
being	O
POLAR	O
ρ	O
chosen	O
differentials	O
.	O
Preference	O
test	O
.	O
Additionally	O
,	O
we	O
introduce	O
the	O
preference	O
test	O
evaluating	O
whether	O
the	O
direction	O
on	O
a	O
given	O
differential	O
scale	O
is	O
in	O
line	O
with	O
human	O
judgement	O
.	O
That	O
is	O
,	O
for	O
the	O
same	O
words	O
from	O
the	O
selection	O
test	O
,	O
we	O
display	O
the	O
same	O
ten	O
dimensions	O
(	O
5	O
top	O
-	O
POLAR	O
ρ	O
,	O
5	O
random	O
bottom	O
)	O
where	O
coders	O
select	O
their	O
interpretation	O
of	O
the	O
given	O
word	O
on	O
scales	O
as	O
shown	O
in	O
Fig	O
.	O
3c	O
.	O
Typical	O
for	O
semantic	O
differential	O
scales	O
(	O
Tullis	O
and	O
Albert	O
,	O
2008	O
;	O
Osgood	O
et	O
al	O
,	O
1957	O
)	O
,	O
we	O
deliberately	O
use	O
a	O
seven	O
point	O
scale	O
representing	O
-	O
3	O
to	O
3	O
,	O
allowing	O
more	O
freedom	O
than	O
3	O
or	O
5	O
points	O
(	O
Simms	O
et	O
al	O
,	O
2019	O
)	O
.	O
Further	O
,	O
we	O
specifically	O
allow	O
a	O
center	O
point	O
-	O
being	O
equal	O
-	O
as	O
it	O
might	O
indicate	O
both	O
being	O
equally	O
well	O
or	O
not	O
good	O
at	O
all	O
.	O
Due	O
to	O
scale	O
usage	O
heterogeneity	O
(	O
Rossi	O
et	O
al	O
,	O
2001	O
)	O
,	O
we	O
normalize	O
coder	O
chosen	O
directions	O
(	O
shift+scale	O
according	O
to	O
mean	O
)	O
prohibiting	O
disproportional	O
influence	O
of	O
single	O
coders	O
.	O
We	O
evaluate	O
the	O
coder	O
agreement	O
by	O
counting	O
direction	O
(	O
sign	O
)	O
non	O
-	O
/alignment	O
with	O
the	O
POLAR	O
ρ	O
projection	O
scale	O
.	O
Demographics	O
.	O
There	O
is	O
a	O
multitude	O
of	O
other	O
external	O
factors	O
that	O
might	O
have	O
impact	O
on	O
coders	O
'	O
choices	O
.	O
To	O
better	O
understand	O
participant	O
back	O
-	O
ground	O
,	O
we	O
ask	O
for	O
their	O
education	O
,	O
emoji	O
usage	O
(	O
familiarity	O
)	O
,	O
smartphone	O
platform	O
(	O
different	O
emoji	O
pictograms	O
)	O
,	O
and	O
if	O
they	O
had	O
used	O
Jodel	O
before	O
.	O

Crowdworker	O
Campaigns	O
We	O
run	O
a	O
campaign	O
for	O
each	O
of	O
the	O
cross	O
product	O
between	O
words	O
only	O
,	O
emoji	O
only	O
,	O
and	O
mixed	O
Tab	O
.	O
3a	O
and	O
Fig	O
.	O
2	O
.	O
(	O
W	O
/	O
W	O
)	O
word	O
/	O
word	O
sets	O
a	O
baseline	O
comparison	O
to	O
results	O
from	O
the	O
original	O
POLAR	O
work	O
,	O
albeit	O
now	O
using	O
the	O
projection	O
approach	O
.	O
(	O
W	O
/	O
M	O
)	O
:	O
word	O
/	O
mixed	O
uses	O
not	O
only	O
words	O
,	O
but	O
includes	O
emoji	O
to	O
the	O
POLAR	O
subspace	O
.	O
(	O
W	O
/	O
E	O
)	O
:	O
word	O
/	O
emoji	O
uses	O
only	O
emoji	O
to	O
describe	O
words	O
.	O
(	O
E	O
/	O
W	O
)	O
:	O
emoji	O
/	O
word	O
provides	O
another	O
baseline	O
as	O
to	O
how	O
well	O
emoji	O
may	O
be	O
interpreted	O
with	O
words	O
only	O
.	O
(	O
E	O
/	O
M	O
)	O
:	O
emoji	O
/	O
mixed	O
uses	O
both	O
,	O
emoji	O
and	O
words	O
to	O
interpret	O
emoji	O
.	O
(	O
E	O
/	O
E	O
)	O
:	O
emoji	O
/	O
emoji	O
may	O
be	O
the	O
most	O
interesting	O
as	O
we	O
only	O
use	O
the	O
expressiveness	O
of	O
emoji	O
to	O
describe	O
emoji	O
.	O
For	O
mixed	O
cases	O
(	O
emoji	O
and	O
words	O
within	O
the	O
PO	O
-	O
LAR	O
subspace	O
)	O
,	O
we	O
create	O
rankings	O
from	O
absolute	O
scale	O
values	O
on	O
both	O
types	O
(	O
words	O
/	O
emoji	O
)	O
separately	O
and	O
then	O
select	O
them	O
equally	O
often	O
to	O
achieve	O
similar	O
amounts	O
of	O
word	O
and	O
emoji	O
differentials	O
.	O
Used	O
Words	O
&	O
Emoji	O
.	O
We	O
selected	O
50	O
words	O
and	O
emoji	O
to	O
be	O
described	O
in	O
each	O
campaign	O
.	O
To	O
ensure	O
that	O
i	O
)	O
we	O
only	O
use	O
common	O
words	O
that	O
are	O
very	O
likely	O
known	O
to	O
our	O
coders	O
,	O
and	O
ii	O
)	O
these	O
words	O
are	O
captured	O
well	O
within	O
the	O
underlying	O
embedding	O
,	O
we	O
pick	O
them	O
out	O
of	O
the	O
upper	O
25	O
%	O
quantile	O
by	O
occurrences	O
in	O
the	O
corpus	O
(	O
n	O
≥1.6k	O
)	O
.	O
I.e.	O
,	O
we	O
chose	O
emoji	O
and	O
words	O
that	O
appear	O
frequently	O
and	O
should	O
therefore	O
be	O
well	O
-	O
known	O
.	O
For	O
words	O
,	O
we	O
ensured	O
that	O
they	O
are	O
part	O
of	O
the	O
German	O
dictionary	O
Duden	O
.	O
Tasks	O
Setup	O
.	O
Within	O
our	O
six	O
campagins	O
,	O
we	O
now	O
have	O
each	O
50	O
emoji	O
or	O
50	O
words	O
to	O
be	O
interpreted	O
.	O
We	O
bundled	O
this	O
into	O
5	O
tasks	O
each	O
consisting	O
of	O
10	O
emoji	O
/	O
words	O
-	O
resulting	O
in	O
30	O
different	O
tasks	O
.	O
Each	O
of	O
these	O
tasks	O
contains	O
the	O
Selection	O
test	O
,	O
Preference	O
test	O
,	O
and	O
demographics	O
.	O
Subjects	O
.	O
Human	O
judgement	O
and	O
crowdsourced	O
evaluations	O
are	O
noisy	O
by	O
nature	O
.	O
While	O
it	O
is	O
usually	O
sufficient	O
to	O
employ	O
few	O
trusted	O
expert	O
coders	O
,	O
it	O
is	O
suggested	O
to	O
use	O
more	O
in	O
the	O
non	O
-	O
expert	O
case	O
(	O
Snow	O
et	O
al	O
,	O
2008	O
)	O
.	O
Thus	O
,	O
we	O
assign	O
5	O
different	O
annotators	O
to	O
each	O
of	O
the	O
30	O
tasks	O
.	O
At	O
estimated	O
10	O
-	O
15min	O
duration	O
,	O
we	O
provide	O
3	O
$	O
compensation	O
for	O
answering	O
a	O
single	O
task	O
,	O
above	O
minimum	O
wage	O
in	O
our	O
country	O
.	O
Quality	O
Assurance	O
.	O
Any	O
crowdsourcing	O
task	O
offers	O
an	O
incentive	O
to	O
rush	O
tasks	O
for	O
the	O
money	O
,	O
which	O
requires	O
us	O
to	O
employ	O
means	O
of	O
quality	O
assurance	O
(	O
QA	O
)	O
.	O
As	O
we	O
have	O
an	O
uncontrolled	O
environment	O
and	O
thus	O
untrusted	O
coders	O
,	O
we	O
handcraft	O
test	O
questions	O
for	O
the	O
selection	O
and	O
preference	O
test	O
.	O
This	O
task	O
is	O
non	O
-	O
trivial	O
as	O
we	O
require	O
unambiguity	O
in	O
correct	O
answers	O
(	O
we	O
ensured	O
this	O
with	O
multiple	O
qualitative	O
tests	O
among	O
friends	O
and	O
colleagues	O
)	O
,	O
while	O
simultaneously	O
not	O
being	O
too	O
obvious	O
.	O
We	O
place	O
one	O
test	O
question	O
for	O
selection	O
and	O
one	O
for	O
preference	O
randomly	O
into	O
each	O
task	O
(	O
ending	O
up	O
in	O
11	O
words	O
or	O
emoji	O
per	O
task	O
)	O
.	O
This	O
also	O
means	O
that	O
each	O
coder	O
can	O
only	O
participate	O
in	O
up	O
to	O
5	O
different	O
tasks	O
within	O
a	O
single	O
campaign	O
before	O
re	O
-	O
seeing	O
a	O
test	O
question	O
.	O
We	O
define	O
acceptance	O
thresholds	O
of	O
four	O
out	O
of	O
five	O
correct	O
answers	O
for	O
both	O
,	O
the	O
selection	O
test	O
and	O
the	O
correct	O
direction	O
for	O
the	O
preference	O
test	O
.	O

Within	O
the	O
crowdsourcing	O
process	O
,	O
we	O
rejected	O
about	O
10	O
%	O
of	O
all	O
tasks	O
according	O
to	O
our	O
QA	O
measures	O
,	O
which	O
then	O
had	O
to	O
be	O
re	O
-	O
taken	O
.	O
We	O
ended	O
up	O
with	O
6	O
campaigns	O
each	O
having	O
50	O
words	O
/	O
emoji	O
answered	O
by	O
5	O
coders	O
;	O
summing	O
up	O
to	O
completed	O
150	O
tasks	O
.	O
In	O
total	O
,	O
16	O
different	O
coders	O
accomplished	O
this	O
series	O
of	O
which	O
4	O
completed	O
Σ	O
≥	O
100	O
tasks	O
.	O

First	O
we	O
focus	O
on	O
the	O
describing	O
emoji	O
campaigns	O
(	O
E/	O
*	O
)	O
.	O
We	O
present	O
our	O
main	O
evaluation	O
results	O
in	O
Tab	O
.	O
1	O
.	O
Within	O
columns	O
,	O
we	O
show	O
results	O
for	O
random	O
,	O
original	O
POLAR	O
,	O
and	O
our	O
six	O
campaigns	O
.	O
We	O
split	O
the	O
rows	O
into	O
results	O
from	O
the	O
selection	O
test	O
across	O
Top1	O
..	O
5	O
entries	O
and	O
the	O
preference	O
test	O
.	O
Selection	O
Test	O
.	O
We	O
find	O
very	O
good	O
results	O
along	O
all	O
emoji	O
campaigns	O
(	O
E/	O
*	O
)	O
being	O
consistently	O
better	O
than	O
any	O
campaign	O
describing	O
words	O
(	O
W/	O
*	O
)	O
.	O
The	O
best	O
performance	O
was	O
achieved	O
for	O
explaining	O
emoji	O
with	O
emoji	O
(	O
E	O
/	O
E	O
)	O
;	O
others	O
are	O
on	O
par	O
.	O
We	O
want	O
to	O
note	O
however	O
,	O
that	O
the	O
small	O
size	O
of	O
used	O
emoji	O
-	O
differential	O
set	O
may	O
ease	O
selection	O
.	O
E.g.	O
,	O
facial	O
expression	O
emoji	O
regularly	O
achieve	O
higher	O
embedding	O
scores	O
than	O
others	O
,	O
which	O
thus	O
may	O
bias	O
the	O
bottom	O
control	O
half	O
(	O
4.1.1	O
)	O
.	O
However	O
,	O
interpreting	O
emoji	O
or	O
words	O
with	O
words	O
only	O
,	O
(	O
E	O
/	O
W	O
)	O
and	O
(	O
W	O
/	O
W	O
)	O
,	O
achieve	O
comparable	O
performance	O
.	O
Preference	O
Test	O
.	O
Here	O
,	O
we	O
make	O
the	O
same	O
observation	O
;	O
The	O
projected	O
scales	O
on	O
the	O
differentials	O
are	O
mostly	O
well	O
in	O
line	O
with	O
human	O
judgement	O
.	O

Again	O
,	O
we	O
refer	O
to	O
Tab	O
.	O
1	O
,	O
but	O
now	O
change	O
our	O
focus	O
to	O
describing	O
words	O
,	O
campaigns	O
(	O
W/	O
*	O
)	O
.	O
Selection	O
Test	O
.	O
Albeit	O
not	O
being	O
directly	O
comparable	O
,	O
using	O
POLAR	O
ρ	O
in	O
compaings	O
:	O
describing	O
words	O
with	O
words	O
(	O
W	O
/	O
W	O
)	O
,	O
or	O
describing	O
words	O
with	O
words	O
and	O
emoji	O
(	O
W	O
/	O
M	O
)	O
achieved	O
performance	O
well	O
on	O
par	O
with	O
POLAR	O
.	O
Noteworthy	O
,	O
describing	O
words	O
with	O
emoji	O
(	O
W	O
/	O
E	O
)	O
yielded	O
the	O
worst	O
results	O
.	O
The	O
projection	O
scale	O
values	O
for	O
the	O
emoji	O
dimensions	O
were	O
mostly	O
lower	O
compared	O
to	O
words	O
.	O
I.e.	O
,	O
according	O
to	O
POLAR	O
ρ	O
,	O
for	O
words	O
only	O
few	O
emoji	O
differentials	O
would	O
be	O
among	O
the	O
top	O
5	O
opposites	O
.	O
Preference	O
Test	O
.	O
As	O
for	O
the	O
preference	O
test	O
,	O
describing	O
words	O
yield	O
the	O
best	O
results	O
using	O
word	O
opposites	O
only	O
(	O
W	O
/	O
W	O
)	O
.	O
Explaining	O
words	O
with	O
emoji	O
(	O
W	O
/	O
E	O
)	O
performs	O
particularly	O
worse	O
.	O
Task	O
Random	O
POLAR	O
(	O
W	O
/	O
W	O
)	O
(	O
W	O
/	O
M	O
)	O
(	O
W	O
/	O
E	O
)	O
(	O
E	O
/	O
W	O
)	O
(	O
E	O
/	O
M	O
)	O
(	O
E	O
/	O
E	O
)	O

Significance	O
.	O
To	O
test	O
for	O
differences	O
within	O
the	O
coder	O
alignment	O
with	O
POLAR	O
ρ	O
,	O
we	O
model	O
both	O
,	O
the	O
selection	O
and	O
preference	O
test	O
.	O
With	O
our	O
primary	O
goal	O
to	O
understand	O
the	O
impact	O
of	O
including	O
emoji	O
to	O
a	O
POLAR	O
ρ	O
interpretable	O
word	O
embedding	O
,	O
we	O
anchor	O
to	O
the	O
(	O
W	O
/	O
W	O
)	O
campaign	O
as	O
a	O
baseline	O
.	O
For	O
the	O
selection	O
test	O
,	O
we	O
count	O
if	O
coders	O
aligned	O
with	O
POLAR	O
ρ	O
or	O
chose	O
any	O
of	O
the	O
random	O
alternatives	O
across	O
the	O
Top	O
1	O
..	O
5	O
selection	O
.	O
For	O
the	O
preference	O
test	O
,	O
we	O
count	O
whether	O
coders	O
aligned	O
with	O
POLAR	O
ρ	O
's	O
scale	O
direction	O
.	O
We	O
apply	O
double	O
-	O
sided	O
chi	O
-	O
square	O
tests	O
χ	O
2	O
with	O
p	O
<	O
0.05	O
between	O
the	O
interpreting	O
words	O
with	O
words	O
(	O
W	O
/	O
W	O
)	O
baseline	O
and	O
the	O
remaining	O
five	O
campaigns	O
.	O
We	O
identify	O
significant	O
differences	O
in	O
coder	O
-	O
POLAR	O
ρ	O
alignment	O
to	O
the	O
(	O
W	O
/	O
W	O
)	O
baseline	O
when	O
describing	O
words	O
with	O
emoji	O
(	O
W	O
/	O
E	O
)	O
over	O
Top1	O
..	O
5	O
selection	O
and	O
preference	O
.	O
Counts	O
from	O
explaining	O
emoji	O
with	O
emoji	O
(	O
E	O
/	O
E	O
)	O
signal	O
significance	O
for	O
preference	O
and	O
selection	O
Top3	O
..	O
5	O
.	O
Coder	O
-	O
POLAR	O
ρ	O
alignment	O
in	O
preferences	O
is	O
also	O
significant	O
for	O
describing	O
emoji	O
with	O
emoji	O
and	O
words	O
(	O
E	O
/	O
M	O
)	O
.	O

Though	O
we	O
are	O
confident	O
in	O
applied	O
QA	O
measures	O
,	O
none	O
of	O
the	O
demographics	O
can	O
be	O
confirmed	O
.	O
The	O
annotator	O
sample	O
-	O
size	O
is	O
small	O
and	O
thus	O
most	O
likely	O
not	O
representative	O
.	O
Further	O
,	O
we	O
find	O
most	O
workers	O
providing	O
contrasting	O
answers	O
across	O
multiple	O
tasks	O
they	O
participated	O
in	O
,	O
rendering	O
collected	O
demographic	O
information	O
unusable	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
the	O
techniques	O
used	O
in	O
our	O
system	O
.	O
Interested	O
readers	O
are	O
encouraged	O
to	O
check	O
out	O
the	O
original	O
papers	O
for	O
further	O
details	O
.	O

Iterative	O
back	O
-	O
translation	O
(	O
Hoang	O
et	O
al	O
,	O
2018	O
)	O
is	O
an	O
extension	O
of	O
back	O
-	O
translation	O
(	O
Sennrich	O
et	O
al	O
,	O
2016a	O
)	O
.	O
It	O
can	O
exploit	O
both	O
sides	O
of	O
monolingual	O
data	O
of	O
a	O
language	O
pair	O
,	O
and	O
produces	O
translation	O
models	O
for	O
both	O
directions	O
,	O
which	O
is	O
suitable	O
for	O
this	O
shared	O
task	O
.	O
The	O
initial	O
models	O
for	O
generating	O
synthetic	O
parallel	O
data	O
are	O
produced	O
by	O
using	O
dual	O
transfer	O
with	O
low	O
resource	O
authentic	O
parallel	O
data	O
.	O
In	O
each	O
iteration	O
of	O
iterative	O
back	O
-	O
translation	O
,	O
we	O
use	O
the	O
latest	O
model	O
to	O
greedily	O
decode	O
a	O
disjoint	O
subset	O
of	O
4	O
m	O
monolingual	O
sentences	O
1	O
to	O
generate	O
synthetic	O
parallel	O
data	O
.	O
Then	O
a	O
new	O
model	O
is	O
trained	O
on	O
a	O
mixture	O
of	O
authentic	O
and	O
synthetic	O
parallel	O
data	O
.	O
With	O
the	O
use	O
of	O
dual	O
transfer	O
,	O
model	O
training	O
can	O
start	O
from	O
[	O
A	O
]	O
PLM	O
emb	O
.	O
[	O
A	O
]	O
PLM	O
body	O
A	O
and	O
B	O
mono	O
.	O
(	O
1	O
)	O
[	O
P	O
]	O
PLM	O
emb	O
.	O
[	O
A	O
]	O
PLM	O
body	O
P	O
and	O
Q	O
mono	O
.	O
(	O
2	O
)	O
[	O
A	O
]	O
NMT	O
encoder	O
emb	O
.	O
[	O
A	O
]	O
NMT	O
encoder	O
body	O
[	O
B	O
]	O
NMT	O
decoder	O
emb	O
.	O

(	O
3	O
)	O
[	O
P	O
]	O
NMT	O
encoder	O
emb	O
.	O

We	O
only	O
use	O
selected	O
finetuning	O
for	O
the	O
chv	O
-	O
ru	O
pair	O
because	O
parallel	O
data	O
for	O
hsb	O
-	O
de	O
is	O
scarce	O
.	O
In	O
order	O
to	O
test	O
the	O
effect	O
of	O
selected	O
finetuning	O
,	O
we	O
start	O
from	O
the	O
models	O
of	O
Iteration	O
2	O
in	O
Table	O
5	O
.	O
Results	O
in	O
Table	O
6	O
indicate	O
that	O
selected	O
finetuning	O
gives	O
modest	O
improvements	O
.	O

Ideally	O
,	O
the	O
generated	O
element	O
e	O
s	O
after	O
decoding	O
is	O
supposed	O
to	O
exactly	O
belong	O
to	O
the	O
vocabulary	O
set	O
it	O
is	O
meant	O
to	O
be	O
.	O
For	O
example	O
,	O
the	O
predicted	O
aspect	O
term	O
should	O
explicitly	O
appear	O
in	O
the	O
input	O
sentence	O
.	O
However	O
,	O
this	O
might	O
not	O
always	O
hold	O
since	O
each	O
element	O
is	O
generated	O
from	O
the	O
vocabulary	O
set	O
containing	O
all	O
tokens	O
instead	O
of	O
its	O
specific	O
vocabulary	O
set	O
.	O
Thus	O
,	O
the	O
predictions	O
of	O
a	O
generation	O
model	O
may	O
exhibit	O
morphology	O
shift	O
from	O
the	O
ground	O
-	O
truths	O
,	O
e.g.	O
,	O
from	O
single	O
to	O
plural	O
nouns	O
.	O
We	O
propose	O
a	O
prediction	O
normalization	O
strategy	O
to	O
refine	O
the	O
incorrect	O
predictions	O
resulting	O
from	O
such	O
issue	O
.	O
For	O
each	O
sentiment	O
type	O
c	O
denoting	O
the	O
type	O
of	O
the	O
element	O
e	O
such	O
as	O
the	O
aspect	O
term	O
or	O
sentiment	O
polarity	O
,	O
we	O
first	O
construct	O
its	O
corresponding	O
vocabulary	O
set	O
V	O
c	O
.	O
For	O
aspect	O
term	O
and	O
opinion	O
term	O
,	O
V	O
c	O
contains	O
all	O
words	O
in	O
the	O
current	O
input	O
sentence	O
x	O
;	O
for	O
aspect	O
category	O
,	O
V	O
c	O
is	O
a	O
collection	O
of	O
all	O
categories	O
in	O
the	O
dataset	O
;	O
for	O
sentiment	O
polarity	O
,	O
V	O
c	O
contains	O
all	O
possible	O
polarities	O
.	O
Then	O
for	O
a	O
predicted	O
element	O
e	O
of	O
the	O
sentiment	O
type	O
c	O
,	O
if	O
it	O
does	O
not	O
belong	O
to	O
the	O
corresponding	O
vocabulary	O
set	O
V	O
c	O
,	O
we	O
useē	O
V	O
c	O
,	O
which	O
has	O
the	O
smallest	O
Levenshtein	O
distance	O
(	O
Levenshtein	O
,	O
1966	O
)	O
with	O
e	O
,	O
to	O
replace	O
e.	O

Datasets	O
We	O
evaluate	O
the	O
proposed	O
GAS	O
framework	O
on	O
four	O
popular	O
benchmark	O
datasets	O
including	O
Laptop14	O
,	O
Rest14	O
,	O
Rest15	O
,	O
and	O
Rest16	O
,	O
originally	O
provided	O
by	O
the	O
SemEval	O
shared	O
challenges	O
(	O
Pontiki	O
et	O
al	O
,	O
2014	O
(	O
Pontiki	O
et	O
al	O
,	O
,	O
2015	O
(	O
Pontiki	O
et	O
al	O
,	O
,	O
2016	O
.	O
For	O
each	O
ABSA	O
task	O
,	O
we	O
use	O
the	O
public	O
datasets	O
derived	O
from	O
them	O
with	O
more	O
sentiment	O
annotations	O
.	O
Specifically	O
,	O
we	O
adopt	O
the	O
dataset	O
provided	O
by	O
Fan	O
et	O
al	O
(	O
2019	O
)	O
,	O
Li	O
et	O
al	O
(	O
2019a	O
)	O
,	O
,	O
Wan	O
et	O
al	O
(	O
2020	O
)	O
for	O
the	O
AOPE	O
,	O
UABSA	O
,	O
ASTE	O
,	O
TASD	O
task	O
respectively	O
.	O
For	O
a	O
fair	O
comparison	O
,	O
we	O
use	O
the	O
same	O
data	O
split	O
as	O
previous	O
works	O
.	O

Annotation	O
-	O
style	O
&	O
Extraction	O
-	O
style	O
As	O
shown	O
in	O
result	O
tables	O
,	O
the	O
annotation	O
-	O
style	O
method	O
generally	O
performs	O
better	O
than	O
the	O
extraction	O
-	O
style	O
method	O
on	O
the	O
AOPE	O
and	O
UASA	O
task	O
.	O
However	O
,	O
the	O
former	O
one	O
becomes	O
inferior	O
to	O
the	O
latter	O
on	O
the	O
more	O
complex	O
ASTE	O
and	O
TASD	O
tasks	O
.	O
One	O
possible	O
reason	O
is	O
that	O
,	O
on	O
the	O
ASTE	O
and	O
TASD	O
tasks	O
,	O
the	O
annotation	O
-	O
style	O
method	O
introduces	O
too	O
much	O
content	O
,	O
such	O
as	O
the	O
aspect	O
category	O
and	O
sentiment	O
polarity	O
,	O
into	O
the	O
target	O
sentence	O
,	O
which	O
increases	O
the	O
difficulty	O
of	O
sequence	O
-	O
to	O
-	O
sequence	O
learning	O
.	O

Program	O
Committee	O

Am	O
I	O
Me	O
or	O
You	O
?	O
State	O
-	O
of	O
-	O
the	O
-	O
Art	O
Dialogue	O
Models	O
Can	O
not	O
Maintain	O
an	O
Identity	O

We	O
also	O
consider	O
combining	O
expanded	O
attention	O
with	O
re	O
-	O
ranking	O
methods	O
,	O
or	O
with	O
multi	O
-	O
objective	O
training	O
,	O
to	O
see	O
if	O
the	O
combination	O
can	O
improve	O
results	O
.	O
For	O
the	O
latter	O
we	O
use	O
the	O
automated	O
grounding	O
trainable	O
mask	O
method	O
.	O
4	O
Experimental	O
Results	O

Profile	O
Grounding	O
Expanding	O
the	O
decoder	O
attention	O
yields	O
significant	O
gains	O
across	O
all	O
automated	O
metrics	O
,	O
as	O
seen	O
in	O
Table	O
6	O
for	O
a	O
1024	O
-	O
truncate	O
model	O
(	O
and	O
in	O
Table	O
15	O
in	O
Appendix	O
F	O
for	O
a	O
128truncate	O
model	O
)	O
.	O
As	O
a	O
baseline	O
we	O
explore	O
simply	O
re	O
-	O
attending	O
to	O
the	O
full	O
context	O
again	O
;	O
this	O
indeed	O
improves	O
metrics	O
across	O
the	O
board	O
for	O
the	O
shortcontext	O
model	O
,	O
but	O
the	O
long	O
-	O
context	O
model	O
actually	O
suffers	O
.	O
However	O
,	O
both	O
models	O
improve	O
substantially	O
over	O
the	O
baseline	O
when	O
including	O
the	O
full	O
LIGHT	O
context	O
without	O
the	O
dialogue	O
history	O
,	O
and	O
attention	O
over	O
sub	O
-	O
components	O
of	O
the	O
LIGHT	O
context	O
still	O
yields	O
strong	O
improvements	O
.	O
To	O
see	O
how	O
much	O
this	O
expanded	O
attention	O
matters	O
,	O
we	O
explored	O
varying	O
the	O
number	O
of	O
rounds	O
r	O
{	O
1	O
,	O
2	O
,	O
3	O
}	O
of	O
expanded	O
attention	O
,	O
i.e.	O
,	O
how	O
many	O
times	O
the	O
model	O
attends	O
to	O
this	O
additional	O
context	O
.	O
In	O
Table	O
6	O
,	O
we	O
also	O
see	O
that	O
a	O
second	O
expanded	O
attention	O
round	O
yields	O
even	O
better	O
results	O
,	O
but	O
performance	O
drops	O
off	O
after	O
applying	O
a	O
third	O
round	O
.	O

We	O
performed	O
human	O
evaluation	O
on	O
our	O
models	O
.	O
For	O
each	O
model	O
we	O
collected	O
100	O
human	O
-	O
model	O
conversations	O
,	O
set	O
up	O
similarly	O
to	O
the	O
original	O
LIGHT	O
dataset	O
conversations	O
.	O
During	O
the	O
conversation	O
,	O
crowdworkers	O
were	O
asked	O
to	O
annotate	O
the	O
model	O
's	O
response	O
for	O
the	O
following	O
attributes	O
:	O
1	O
)	O
Mistaken	O
Identity	O
:	O
your	O
partner	O
says	O
something	O
that	O
would	O
imply	O
they	O
believe	O
they	O
're	O
someone	O
other	O
than	O
who	O
they	O
're	O
noted	O
to	O
be	O
;	O
2	O
)	O
Contradiction	O
:	O
your	O
partner	O
says	O
something	O
that	O
contradicts	O
something	O
they	O
've	O
said	O
before	O
;	O
3	O
)	O
Wrong	O
Location	O
:	O
your	O
partner	O
says	O
something	O
that	O
would	O
imply	O
they	O
believe	O
they	O
are	O
in	O
a	O
different	O
location	O
than	O
the	O
provided	O
one	O
;	O
4	O
)	O
Unrelated	O
:	O
your	O
partner	O
says	O
something	O
that	O
does	O
n't	O
follow	O
the	O
previous	O
turns	O
;	O
and	O
5	O
)	O
Repetitive	O
:	O
your	O
partner	O
says	O
something	O
they	O
've	O
already	O
said	O
,	O
or	O
are	O
driving	O
the	O
conversation	O
in	O
circles	O
.	O
Utterances	O
that	O
do	O
not	O
contain	O
any	O
of	O
the	O
negative	O
attributes	O
are	O
denoted	O
"	O
all	O
good	O
"	O
.	O
Finally	O
,	O
we	O
collect	O
an	O
engagingness	O
score	O
on	O
a	O
scale	O
of	O
1	O
-	O
5	O
at	O
the	O
end	O
of	O
the	O
conversation	O
.	O
More	O
details	O
in	O
Appendix	O
I.	O
Results	O
are	O
given	O
in	O
Correlations	O
between	O
automatic	O
metrics	O
and	O
human	O
evaluations	O
are	O
measured	O
in	O
Appendix	O
K	O
,	O
where	O
we	O
find	O
that	O
RPA	O
and	O
mistaken	O
identity	O
are	O
indeed	O
strongly	O
correlated	O
.	O
5	O
Qualitative	O
Analysis	O

We	O
note	O
that	O
the	O
human	O
dialogue	O
data	O
is	O
classified	O
as	O
being	O
"	O
in	O
character	O
"	O
only	O
92.8	O
%	O
of	O
the	O
time	O
on	O
the	O
validation	O
set	O
by	O
the	O
LTR	O
RPA	O
classifier	O
.	O
We	O
examine	O
the	O
scenarios	O
in	O
which	O
the	O
classifier	O
is	O
incorrect	O
,	O
with	O
example	O
input	O
/	O
output	O
pairs	O
in	O
Table	O
21	O
in	O
the	O
Appendix	O
.	O
First	O
,	O
there	O
are	O
instances	O
where	O
either	O
character	O
could	O
have	O
said	O
the	O
output	O
response	O
(	O
row	O
1	O
)	O
.	O
Second	O
,	O
there	O
are	O
instances	O
where	O
there	O
are	O
not	O
enough	O
clues	O
in	O
the	O
context	O
to	O
provide	O
an	O
estimation	O
of	O
who	O
said	O
the	O
response	O
,	O
for	O
example	O
at	O
the	O
beginning	O
of	O
the	O
conversation	O
(	O
row	O
2	O
)	O
.	O
And	O
,	O
there	O
are	O
still	O
some	O
small	O
amount	O
of	O
instances	O
that	O
the	O
classifier	O
simply	O
fails	O
(	O
row	O
3	O
)	O
.	O

We	O
analyze	O
the	O
results	O
of	O
turn	O
annotation	O
to	O
understand	O
what	O
failure	O
modes	O
contribute	O
to	O
mistaken	O
identity	O
.	O
A	O
full	O
list	O
of	O
such	O
modes	O
is	O
in	O
Table	O
16	O
;	O
the	O
baseline	O
model	O
most	O
often	O
mistakes	O
its	O
partner	O
for	O
itself	O
(	O
i.e.	O
,	O
the	O
model	O
thinks	O
it	O
is	O
talking	O
to	O
itself	O
)	O
.	O
Other	O
common	O
failures	O
include	O
the	O
model	O
thinking	O
that	O
it	O
is	O
its	O
partner	O
's	O
character	O
,	O
or	O
emulating	O
irrelevant	O
characteristics	O
.	O

We	O
consider	O
the	O
RPA	O
of	O
various	O
models	O
when	O
evaluated	O
across	O
the	O
turns	O
of	O
conversation	O
.	O
Intuitively	O
,	O
baseline	O
models	O
would	O
suffer	O
as	O
the	O
conversation	O
goes	O
on	O
for	O
a	O
variety	O
of	O
reasons	O
(	O
character	O
roles	O
are	O
truncated	O
out	O
of	O
context	O
,	O
more	O
input	O
yields	O
noisier	O
outputs	O
,	O
etc	O
.	O
)	O
.	O
Appendix	O
Figure	O
1	O
shows	O
the	O
perturn	O
results	O
for	O
a	O
set	O
of	O
representative	O
models	O
.	O
The	O
human	O
outputs	O
are	O
most	O
often	O
correct	O
on	O
the	O
first	O
turn	O
,	O
with	O
gradual	O
RPA	O
decay	O
throughout	O
the	O
conversation	O
.	O
The	O
128	O
-	O
truncate	O
baseline	O
,	O
as	O
expected	O
,	O
suffers	O
a	O
dramatic	O
performance	O
drop	O
after	O
the	O
first	O
couple	O
of	O
turns	O
.	O
Meanwhile	O
,	O
with	O
the	O
profile	O
expanded	O
attention	O
,	O
we	O
see	O
near	O
-	O
human	O
performance	O
,	O
with	O
better	O
RPA	O
in	O
later	O
turns	O
.	O
Including	O
RPA	O
reranking	O
improves	O
dramatically	O
over	O
all	O
turns	O
.	O

To	O
gain	O
some	O
insight	O
into	O
what	O
is	O
happening	O
with	O
the	O
expanded	O
attention	O
,	O
we	O
mapped	O
out	O
the	O
attention	O
between	O
context	O
and	O
response	O
tokens	O
for	O
both	O
a	O
baseline	O
model	O
with	O
no	O
expanded	O
attention	O
,	O
and	O
a	O
model	O
with	O
profile	O
expanded	O
attention	O
.	O
Figures	O
4	O
and	O
5	O
in	O
the	O
Appendix	O
display	O
the	O
heat	O
maps	O
for	O
an	O
example	O
context	O
and	O
response	O
,	O
with	O
details	O
on	O
heat	O
map	O
construction	O
given	O
in	O
Appendix	O
M.	O
We	O
find	O
that	O
the	O
baseline	O
model	O
spreads	O
its	O
attention	O
out	O
across	O
both	O
the	O
LIGHT	O
context	O
and	O
the	O
dialogue	O
history	O
,	O
with	O
the	O
majority	O
of	O
the	O
attention	O
looking	O
at	O
overlapping	O
words	O
in	O
the	O
context	O
and	O
the	O
response	O
and	O
almost	O
no	O
attention	O
on	O
the	O
character	O
names	O
.	O
The	O
expanded	O
attention	O
model	O
concentrates	O
on	O
the	O
recent	O
dialogue	O
history	O
heavily	O
in	O
the	O
first	O
level	O
of	O
attention	O
,	O
and	O
then	O
concentrates	O
on	O
pertinent	O
words	O
in	O
the	O
context	O
related	O
to	O
the	O
character	O
information	O
(	O
i.e.	O
,	O
the	O
character	O
names	O
)	O
in	O
the	O
second	O
round	O
of	O
attention	O
.	O

In	O
this	O
work	O
we	O
explored	O
the	O
problem	O
of	O
maintaining	O
one	O
's	O
character	O
in	O
open	O
dialogue	O
,	O
and	O
showed	O
that	O
state	O
-	O
art	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
have	O
a	O
fundamental	O
weakness	O
in	O
this	O
regard	O
.	O
We	O
provided	O
a	O
clear	O
framing	O
of	O
the	O
problem	O
and	O
showed	O
one	O
can	O
build	O
automatic	O
metrics	O
(	O
RPA	O
)	O
that	O
evaluate	O
models	O
using	O
a	O
classifier	O
.	O
We	O
then	O
explored	O
a	O
variety	O
of	O
methods	O
throughout	O
this	O
paper	O
.	O
While	O
a	O
wide	O
variety	O
of	O
well	O
-	O
known	O
techniques	O
,	O
such	O
as	O
multi	O
-	O
objective	O
or	O
unlikelihood	O
training	O
,	O
have	O
little	O
impact	O
,	O
we	O
found	O
that	O
expanded	O
attention	O
and	O
re	O
-	O
ranking	O
are	O
two	O
approaches	O
that	O
can	O
help	O
to	O
a	O
degree	O
,	O
and	O
their	O
combination	O
also	O
improves	O
results	O
.	O
Our	O
introduced	O
method	O
PACER	O
performs	O
well	O
and	O
may	O
be	O
suitable	O
for	O
other	O
tasks	O
beyond	O
the	O
focus	O
of	O
this	O
paper	O
.	O
Nevertheless	O
,	O
our	O
best	O
methods	O
still	O
lag	O
behind	O
human	O
(	O
crowdworker	O
)	O
performance	O
in	O
several	O
regards	O
,	O
e.g.	O
1.34	O
%	O
vs.	O
2.23	O
%	O
in	O
terms	O
of	O
mistaken	O
identity	O
per	O
turn	O
,	O
or	O
5	O
%	O
vs.	O
14.7	O
%	O
per	O
conversation	O
.	O
Therefore	O
considerable	O
progress	O
still	O
has	O
to	O
be	O
made	O
on	O
this	O
challenging	O
problem	O
.	O

Limitations	O
We	O
note	O
in	O
the	O
conclusion	O
that	O
the	O
problem	O
is	O
not	O
solved	O
;	O
our	O
best	O
models	O
still	O
lag	O
behind	O
human	O
performance	O
in	O
maintaining	O
character	O
identity	O
.	O
All	O
results	O
are	O
tested	O
in	O
the	O
LIGHT	O
environment	O
,	O
comprising	O
open	O
-	O
domain	O
dialogue	O
within	O
constrained	O
settings	O
with	O
assigned	O
characters	O
.	O
The	O
application	O
of	O
these	O
methods	O
to	O
other	O
role	O
-	O
playing	O
(	O
or	O
otherwise	O
)	O
settings	O
is	O
left	O
for	O
future	O
work	O
,	O
though	O
we	O
believe	O
that	O
such	O
methods	O
could	O
be	O
beneficial	O
outside	O
of	O
LIGHT	O
.	O

We	O
provide	O
methods	O
for	O
mitigating	O
mistaken	O
identity	O
in	O
dialogue	O
models	O
.	O
It	O
follows	O
that	O
such	O
methods	O
yield	O
models	O
that	O
are	O
more	O
convincingly	O
role	O
-	O
playing	O
as	O
a	O
given	O
character	O
.	O
With	O
more	O
convincingly	O
in	O
-	O
character	O
models	O
,	O
someone	O
with	O
bad	O
intentions	O
could	O
have	O
a	O
model	O
imitate	O
realworld	O
people	O
without	O
consent	O
,	O
or	O
worse	O
,	O
can	O
say	O
negative	O
/	O
harmful	O
things	O
while	O
impersonating	O
someone	O
else	O
.	O
We	O
note	O
that	O
our	O
methods	O
are	O
orthogonal	O
to	O
improvements	O
in	O
dialogue	O
safety	O
(	O
Xu	O
et	O
al	O
,	O
2020	O
;	O
Dinan	O
et	O
al	O
,	O
2019a	O
)	O
,	O
and	O
so	O
can	O
be	O
used	O
in	O
tandem	O
to	O
mitigate	O
these	O
potential	O
risks	O
.	O

We	O
make	O
use	O
of	O
LIGHT	O
in	O
this	O
work	O
(	O
Urbanek	O
et	O
al	O
,	O
2019	O
)	O
(	O
released	O
under	O
CC	O
-	O
BY	O
license	O
)	O
,	O
an	O
English	O
-	O
language	O
crowdsourced	O
dataset	O
.	O
We	O
also	O
plan	O
to	O
release	O
the	O
code	O
and	O
models	O
(	O
will	O
be	O
released	O
under	O
MIT	O
license	O
)	O
,	O
with	O
the	O
intended	O
use	O
being	O
for	O
others	O
(	O
and	O
ourselves	O
)	O
to	O
reproduce	O
and	O
build	O
upon	O
the	O
research	O
discussed	O
in	O
this	O
paper	O
.	O

We	O
build	O
the	O
training	O
data	O
for	O
the	O
RPA	O
classifiers	O
from	O
the	O
LIGHT	O
dataset	O
.	O
The	O
input	O
is	O
a	O
concate	O
-	O
2	O
https://parl.ai	O

In	O
Table	O
10	O
,	O
we	O
see	O
how	O
each	O
RPA	O
classifier	O
performs	O
on	O
the	O
various	O
datasplits	O
,	O
varying	O
the	O
number	O
of	O
prior	O
utterances	O
used	O
during	O
training	O
and	O
evaluation	O
.	O
Each	O
model	O
performs	O
best	O
on	O
the	O
split	O
on	O
which	O
it	O
was	O
trained	O
(	O
the	O
highlighted	O
numbers	O
)	O
.	O

We	O
find	O
that	O
the	O
left	O
-	O
to	O
-	O
right	O
RPA	O
classifiers	O
are	O
correctly	O
sensitive	O
to	O
per	O
-	O
token	O
perturbations	O
in	O
the	O
input	O
,	O
and	O
can	O
accurately	O
predict	O
the	O
speaker	O
at	O
the	O
token	O
level	O
.	O
In	O
Table	O
11	O
,	O
we	O
give	O
an	O
example	O
where	O
the	O
classifier	O
changes	O
its	O
character	O
prediction	O
,	O
depending	O
on	O
the	O
candidate	O
utterance	O
.	O

In	O
Table	O
14	O
,	O
we	O
see	O
how	O
,	O
when	O
using	O
either	O
the	O
encoder+decoder	O
or	O
just	O
the	O
decoder	O
outputs	O
,	O
we	O
do	O
not	O
require	O
additional	O
multi	O
-	O
objective	O
layers	O
(	O
as	O
we	O
did	O
in	O
the	O
non	O
-	O
automated	O
-	O
grounding	O
case	O
)	O
.	O

We	O
provide	O
results	O
for	O
both	O
the	O
128	O
-	O
truncate	O
and	O
1024	O
-	O
truncate	O
models	O
with	O
profile	O
grounding	O
in	O
Table	O
15	O
.	O
Trends	O
remain	O
the	O
same	O
for	O
both	O
models	O
.	O

Table	O
17	O
includes	O
results	O
on	O
the	O
LIGHT	O
validation	O
set	O
for	O
models	O
in	O
Table	O
4	O
.	O

We	O
evaluated	O
a	O
Poly	O
-	O
encoder	O
baseline	O
model	O
with	O
an	O
RPA	O
re	O
-	O
ranker	O
as	O
well	O
.	O
The	O
Poly	O
-	O
encoder	O
scores	O
utterances	O
from	O
the	O
full	O
training	O
set	O
as	O
candidates	O
,	O
and	O
the	O
candidates	O
for	O
re	O
-	O
ranking	O
are	O
the	O
top	O
-	O
k	O
ranked	O
utterances	O
;	O
results	O
are	O
in	O
Table	O
18	O
.	O
Retrieval	O
models	O
benefit	O
dramatically	O
from	O
the	O
re	O
-	O
ranking	O
,	O
improving	O
to	O
almost	O
99	O
%	O
RPA	O
as	O
measured	O
by	O
the	O
LTR	O
classifier	O
.	O
As	O
the	O
candidate	O
responses	O
for	O
retrieval	O
models	O
come	O
from	O
the	O
set	O
of	O
all	O
training	O
utterances	O
,	O
and	O
due	O
to	O
overlap	O
between	O
the	O
set	O
of	O
characters	O
appearing	O
in	O
the	O
train	O
and	O
valid	O
sets	O
,	O
we	O
can	O
examine	O
how	O
often	O
the	O
model	O
output	O
was	O
originally	O
spoken	O
by	O
its	O
partner	O
's	O
character	O
;	O
this	O
can	O
be	O
seen	O
as	O
a	O
proxy	O
for	O
mistaken	O
identity	O
.	O
We	O
find	O
that	O
the	O
re	O
-	O
ranker	O
reduces	O
the	O
amount	O
of	O
time	O
that	O
the	O
model	O
returns	O
a	O
message	O
its	O
partner	O
said	O
,	O
indicating	O
some	O
viable	O
and	O
promising	O
results	O
.	O

In	O
Table	O
19	O
,	O
we	O
display	O
the	O
full	O
results	O
of	O
human	O
evaluations	O
across	O
all	O
dimensions	O
.	O
We	O
note	O
that	O
the	O
Poly	O
-	O
encoder	O
model	O
is	O
best	O
at	O
not	O
mistaking	O
location	O
or	O
being	O
repetitive	O
,	O
but	O
this	O
is	O
expected	O
given	O
its	O
retrieving	O
over	O
human	O
-	O
written	O
utterances	O
.	O
In	O
Setting	O
:	O
Turquoise	O
Shore	O
,	O
Shore	O
A	O
beautiful	O
turquoise	O
color	O
water	O
by	O
the	O
shore	O
.	O
It	O
is	O
filled	O
with	O
many	O
gems	O
and	O
gold	O
.	O
Figure	O
2	O
,	O
we	O
show	O
a	O
screenshot	O
of	O
the	O
instructions	O
for	O
the	O
evaluation	O
task	O
provided	O
to	O
crowdworkers	O
on	O
Amazon	O
Mechanical	O
Turk	O
.	O

We	O
provide	O
qualitative	O
analysis	O
of	O
the	O
various	O
generation	O
methods	O
below	O
.	O
No	O
Re	O
-	O
ranking	O
When	O
examining	O
the	O
baseline	O
with	O
no	O
re	O
-	O
ranking	O
,	O
we	O
found	O
that	O
nucleus	O
sampling	O
can	O
help	O
when	O
beam	O
search	O
does	O
not	O
work	O
;	O
however	O
,	O
both	O
can	O
go	O
out	O
of	O
character	O
the	O
farther	O
one	O
goes	O
in	O
conversation	O
.	O
Beam	O
Search	O
Re	O
-	O
rankers	O
The	O
beam	O
outputs	O
in	O
standard	O
beam	O
search	O
are	O
at	O
times	O
too	O
similar	O
,	O
in	O
which	O
case	O
re	O
-	O
ranking	O
does	O
next	O
to	O
nothing	O
,	O
unless	O
a	O
viable	O
response	O
is	O
available	O
.	O
Nucleus	O
Sampling	O
Re	O
-	O
rankers	O
Using	O
nucleus	O
setting	O
in	O
a	O
re	O
-	O
ranking	O
setup	O
yields	O
more	O
diverse	O
choices	O
to	O
choose	O
from	O
;	O
however	O
,	O
sometimes	O
the	O
model	O
simply	O
does	O
not	O
address	O
*	O
any	O
*	O
character	O
within	O
the	O
conversation	O
.	O
Delayed	O
Beam	O
Search	O
Re	O
-	O
rankers	O
This	O
strikes	O
a	O
nice	O
balance	O
between	O
sensible	O
outputs	O
from	O
beam	O
search	O
and	O
diversity	O
from	O
nucleus	O
sampling	O
.	O
Mixed	O
-	O
Decoding	O
Re	O
-	O
ranker	O
Using	O
mixed	O
decoding	O
(	O
re	O
-	O
ranking	O
several	O
decoding	O
schemes	O
)	O
can	O
work	O
quite	O
well	O
,	O
as	O
it	O
is	O
a	O
nice	O
blend	O
of	O
different	O
generation	O
methods	O
.	O

Qualitative	O
analysis	O
of	O
the	O
turn	O
annotation	O
results	O
are	O
in	O
Table	O
16	O
.	O
We	O
generally	O
found	O
that	O
beam	O
search	O
fails	O
the	O
vast	O
majority	O
of	O
the	O
time	O
when	O
the	O
model	O
thinks	O
that	O
it	O
is	O
talking	O
to	O
itself	O
;	O
i.e.	O
,	O
it	O
confuses	O
its	O
partner	O
for	O
its	O
own	O
character	O
.	O
The	O
rerankers	O
can	O
help	O
shift	O
the	O
hallucination	O
away	O
from	O
this	O
regime	O
.	O

seems	O
to	O
be	O
most	O
helpful	O
in	O
later	O
turns	O
,	O
where	O
other	O
models	O
generally	O
drop	O
off	O
in	O
efficacy	O
.	O

To	O
build	O
the	O
heat	O
maps	O
in	O
Figures	O
4	O
and	O
5	O
,	O
we	O
look	O
at	O
the	O
maximum	O
attention	O
applied	O
per	O
-	O
head	O
,	O
and	O
the	O
maximum	O
weight	O
applied	O
across	O
the	O
model	O
decoder	O
layers	O
;	O
other	O
combinations	O
were	O
considered	O
(	O
mean	O
per	O
-	O
head	O
,	O
mean	O
over	O
layers	O
or	O
last	O
layer	O
)	O
and	O
yielded	O
similar	O
findings	O
.	O
The	O
speaker	O
is	O
the	O
mermaid	O
,	O
whose	O
partner	O
is	O
a	O
sea	O
-	O
witch	O
.	O
The	O
last	O
utterance	O
from	O
the	O
sea	O
-	O
witch	O
is	O
,	O
"	O
What	O
are	O
you	O
doing	O
on	O
the	O
turquoise	O
shore	O
?	O
"	O
.	O
The	O
mermaid	O
responds	O
,	O
"	O
I	O
've	O
been	O
catching	O
waves	O
with	O
the	O
dolphins	O
all	O
morning	O
.	O
What	O
kind	O
of	O
victims	O
do	O
you	O
expect	O
to	O
find	O
in	O
a	O
tranquil	O
place	O
like	O
this	O
?	O
"	O

Exploring	O
Statistical	O
and	O
Neural	O
Models	O
for	O
Noun	O
Ellipsis	O
Detection	O
and	O
Resolution	O
in	O
English	O

Noun	O
ellipsis	O
is	O
a	O
linguistic	O
phenomenon	O
where	O
the	O
head	O
noun	O
of	O
a	O
noun	O
phrase	O
gets	O
deleted	O
,	O
without	O
making	O
the	O
sentence	O
ungrammatical	O
.	O
For	O
example	O
in	O
the	O
sentence	O
in	O
(	O
1	O
)	O
from	O
(	O
Lobeck	O
,	O
1995	O
)	O
,	O
the	O
noun	O
presentation	O
is	O
elided	O
at	O
"	O
[	O
e	O
]	O
"	O
.	O
1	O
.	O
John	O
's	O
presentation	O
on	O
urban	O
development	O
was	O
virtually	O
ignored	O
because	O
[	O
NP	O
Mary	O
's	O
[	O
e	O
]	O
]	O
was	O
so	O
much	O
more	O
interesting	O
.	O
The	O
elided	O
information	O
can	O
be	O
retrieved	O
from	O
the	O
previous	O
context	O
as	O
in	O
(	O
1	O
)	O
or	O
with	O
the	O
knowledge	O
of	O
idiomatic	O
usage	O
of	O
language	O
as	O
in	O
I	O
will	O
be	O
back	O
in	O
two	O
[	O
e	O
]	O
.	O
where	O
two	O
means	O
two	O
minutes	O
.	O
It	O
is	O
also	O
possible	O
that	O
the	O
reference	O
of	O
the	O
elided	O
information	O
comes	O
from	O
extra	O
-	O
linguistic	O
,	O
situational	O
context	O
.	O
For	O
example	O
,	O
consider	O
a	O
speaker	O
pointing	O
towards	O
the	O
roses	O
in	O
a	O
shop	O
and	O
saying	O
an	O
utterance	O
as	O
in	O
I	O
will	O
take	O
two	O
[	O
e	O
]	O
.	O
While	O
human	O
interlocutors	O
resolve	O
any	O
such	O
elided	O
information	O
by	O
disambiguating	O
from	O
context	O
,	O
cognitive	O
commonsense	O
extension	O
and	O
reasoning	O
(	O
Chen	O
,	O
2016	O
)	O
,	O
ellipsis	O
resolution	O
can	O
be	O
a	O
hard	O
task	O
for	O
Natural	O
Language	O
Processing	O
(	O
NLP	O
)	O
systems	O
(	O
Hardt	O
,	O
1999	O
)	O
.	O
Resolution	O
of	O
ellipsis	O
comprises	O
two	O
tasks	O
-	O
detection	O
of	O
the	O
elided	O
material	O
and	O
antecedent	O
selection	O
(	O
Liu	O
et	O
al	O
,	O
2016b	O
;	O
Nielsen	O
,	O
2003	O
)	O
.	O
Ellipses	O
occur	O
in	O
the	O
environment	O
of	O
certain	O
syntactical	O
structures	O
or	O
trigger	O
words	O
,	O
also	O
known	O
as	O
licensors	O
or	O
triggers	O
of	O
ellipses	O
.	O
They	O
are	O
useful	O
syntactic	O
cues	O
for	O
the	O
detection	O
of	O
noun	O
ellipsis	O
.	O
See	O
Figure	O
1	O
for	O
an	O
example	O
of	O
the	O
noun	O
ellipsis	O
resolution	O
process	O
.	O

We	O
use	O
POS	O
tags	O
of	O
the	O
licensor	O
and	O
modifier	O
of	O
the	O
antecedent	O
as	O
our	O
syntactic	O
features	O
and	O
cosine	O
similarity	O
between	O
their	O
POS	O
tags	O
as	O
our	O
semantic	O
features	O
,	O
following	O
(	O
Khullar	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
concatenate	O
these	O
features	O
to	O
the	O
embeddings	O
to	O
explore	O
the	O
efficacy	O
of	O
adding	O
manual	O
features	O
on	O
resolution	O
.	O

Attending	O
via	O
both	O
Fine	O
-	O
tuning	O
and	O
Compressing	O

Though	O
being	O
a	O
primary	O
trend	O
for	O
enhancing	O
interpretability	O
of	O
neural	O
networks	O
,	O
attention	O
mechanism	O
's	O
reliability	O
and	O
validity	O
are	O
still	O
under	O
debate	O
.	O
In	O
this	O
paper	O
,	O
we	O
try	O
to	O
purify	O
attention	O
scores	O
to	O
obtain	O
a	O
more	O
faithful	O
explanation	O
of	O
downstream	O
models	O
.	O
Specifically	O
,	O
we	O
propose	O
a	O
framework	O
consisting	O
of	O
a	O
learner	O
and	O
a	O
compressor	O
,	O
which	O
performs	O
finetuning	O
and	O
compressing	O
iteratively	O
to	O
enhance	O
the	O
performance	O
and	O
interpretability	O
of	O
the	O
attention	O
mechanism	O
.	O
The	O
learner	O
focuses	O
on	O
learning	O
better	O
text	O
representations	O
to	O
achieve	O
good	O
decisions	O
by	O
fine	O
-	O
tuning	O
,	O
while	O
the	O
compressor	O
aims	O
to	O
perform	O
compressions	O
over	O
the	O
representations	O
to	O
retain	O
the	O
most	O
useful	O
clues	O
for	O
explanations	O
with	O
a	O
Variational	O
information	O
bottleneck	O
ATtention	O
(	O
VAT	O
)	O
mechanism	O
.	O
Extensive	O
experiments	O
on	O
eight	O
benchmark	O
datasets	O
show	O
the	O
great	O
advantages	O
of	O
our	O
proposed	O
approach	O
in	O
terms	O
of	O
both	O
performance	O
and	O
interpretability	O
.	O

In	O
this	O
section	O
,	O
we	O
introduce	O
our	O
framework	O
consisting	O
of	O
a	O
learner	O
and	O
a	O
compressor	O
with	O
a	O
Variational	O
information	O
bottleneck	O
ATtenttion	O
(	O
VAT	O
)	O
mechanism	O
.	O
Given	O
an	O
attention	O
-	O
based	O
neural	O
network	O
model	O
,	O
we	O
formulate	O
our	O
idea	O
within	O
the	O
framework	O
of	O
variational	O
information	O
bottleneck	O
(	O
VIB	O
)	O
(	O
Tishby	O
et	O
al	O
,	O
1999	O
)	O
.	O
Our	O
framework	O
aims	O
to	O
improve	O
the	O
attention	O
's	O
interpretalility	O
with	O
better	O
performance	O
by	O
restricting	O
the	O
attention	O
to	O
capture	O
the	O
crucial	O
words	O
while	O
filter	O
the	O
useless	O
information	O
.	O

First	O
,	O
we	O
perform	O
our	O
models	O
and	O
baselines	O
on	O
eight	O
benchmark	O
datasets	O
and	O
visualize	O
the	O
text	O
representation	O
to	O
verify	O
the	O
effectiveness	O
of	O
VAT	O
(	O
Section	O
5.1	O
)	O
.	O
Second	O
,	O
to	O
further	O
investigate	O
our	O
VAT	O
model	O
,	O
we	O
adopt	O
two	O
popular	O
explanation	O
metrics	O
for	O
quantitative	O
evaluation	O
(	O
Section	O
5.2	O
)	O
.	O
Third	O
,	O
we	O
apply	O
our	O
models	O
to	O
semi	O
-	O
supervision	O
sentiment	O
detection	O
task	O
to	O
evaluate	O
the	O
explanation	O
of	O
our	O
model	O
(	O
Section	O
5.3	O
)	O
.	O
Fourth	O
,	O
we	O
explore	O
the	O
influence	O
of	O
our	O
iteration	O
strategy	O
in	O
Section	O
5.4	O
and	O
provide	O
case	O
studies	O
in	O
Section	O
5.5	O
.	O
For	O
the	O
limitation	O
of	O
the	O
space	O
,	O
we	O
may	O
only	O
list	O
the	O
results	O
on	O
parts	O
of	O
the	O
datasets	O
in	O
some	O
cases	O
since	O
the	O
conclusions	O
are	O
similar	O
for	O
other	O
datasets	O
.	O
The	O
complete	O
results	O
are	O
presented	O
in	O
the	O
supplementary	O
materials	O
.	O

The	O
authors	O
wish	O
to	O
thank	O
the	O
reviewers	O
for	O
their	O
helpful	O
comments	O
and	O
suggestions	O
.	O
This	O
research	O
is	O
(	O
partially	O
)	O
supported	O
by	O
NSFC	O
(	O
62076097	O
)	O
,	O
STCSM	O
(	O
18ZR1411500	O
)	O
,	O
the	O
Fundamental	O
Research	O
Funds	O
for	O
the	O
Central	O
Universities	O
.	O
This	O
research	O
is	O
also	O
funded	O
by	O
the	O
Science	O
and	O
Technology	O
Commission	O
of	O
Shanghai	O
Municipality	O
(	O
19511120200	O
&	O
20511105102	O
)	O
.	O
The	O
computation	O
is	O
performed	O
in	O
ECNU	O
Multifunctional	O
Platform	O
for	O
Innovation	O
(	O
001	O
)	O
.	O
The	O
corresponding	O
authors	O
are	O
Yuanbin	O
Wu	O
and	O
Liang	O
He	O
.	O

Pervasive	O
Attention	O
:	O
2D	O
Convolutional	O
Neural	O
Networks	O
for	O
Sequence	O
-	O
to	O
-	O
Sequence	O
Prediction	O

In	O
this	O
section	O
,	O
we	O
present	O
our	O
experimental	O
setup	O
,	O
followed	O
by	O
quantitative	O
results	O
,	O
qualitative	O
examples	O
of	O
implicit	O
sentence	O
alignments	O
from	O
our	O
model	O
,	O
and	O
a	O
comparison	O
to	O
the	O
state	O
of	O
the	O
art	O
.	O

The	O
JPC2	O
patent	O
task	O
consisted	O
of	O
translation	O
in	O
the	O
patent	O
domain	O
between	O
English	O
and	O
Japanese	O
,	O
Korean	O
and	O
Japanese	O
,	O
and	O
Chinese	O
and	O
Japanese	O
.	O
The	O
training	O
data	O
consisted	O
of	O
parallel	O
corpora	O
provided	O
by	O
the	O
Japan	O
Patent	O
Office	O
(	O
JPO	O
)	O
,	O
with	O
training	O
sets	O
containing	O
one	O
million	O
sentence	O
pairs	O
for	O
each	O
language	O
pair	O
.	O
The	O
data	O
are	O
drawn	O
from	O
four	O
domains	O
,	O
chemistry	O
,	O
electricity	O
,	O
mechanical	O
engineering	O
,	O
and	O
physics	O
.	O
1	O

The	O
overall	O
architecture	O
of	O
our	O
universal	O
dependency	O
parser	O
is	O
shown	O
in	O
Figure	O
1	O
.	O
The	O
whole	O
system	O
can	O
be	O
divided	O
into	O
two	O
parts	O
:	O
Known	O
Language	O
Parser	O
and	O
Surprise	O
Language	O
Parser	O
.	O
The	O
former	O
deals	O
with	O
known	O
languages	O
,	O
including	O
rich	O
resource	O
treebanks	O
and	O
low	O
resource	O
treebanks	O
,	O
whose	O
annotations	O
as	O
the	O
training	O
data	O
are	O
accessible	O
,	O
while	O
the	O
latter	O
disposes	O
of	O
the	O
ones	O
without	O
dependency	O
annotations	O
.	O
When	O
the	O
text	O
to	O
be	O
processed	O
by	O
the	O
system	O
is	O
inputed	O
,	O
it	O
is	O
first	O
discriminated	O
as	O
rich	O
-	O
resource	O
or	O
low	O
-	O
resource	O
and	O
then	O
dispatched	O
to	O
the	O
corresponding	O
sub	O
-	O
systems	O
,	O
which	O
will	O
be	O
described	O
as	O
follows	O
.	O
For	O
the	O
Known	O
Language	O
Parser	O
,	O
the	O
related	O
pipeline	O
contains	O
three	O
steps	O
as	O
follow	O
.	O
(	O
1	O
)	O
Tokenizer	O
The	O
raw	O
texts	O
are	O
split	O
into	O
basic	O
units	O
for	O
the	O
latter	O
processing	O
of	O
dependency	O
anal	O
-	O
ysis	O
,	O
which	O
is	O
the	O
main	O
task	O
of	O
the	O
tokenizer	O
.	O
For	O
all	O
rich	O
resource	O
languages	O
,	O
we	O
train	O
tokenziers	O
using	O
provided	O
training	O
data	O
,	O
including	O
the	O
languages	O
which	O
can	O
be	O
easily	O
tokenized	O
by	O
specific	O
delimiters	O
.	O
(	O
2	O
)	O
Tagger	O
The	O
tokenized	O
texts	O
are	O
labeled	O
by	O
taggers	O
,	O
which	O
provides	O
them	O
with	O
the	O
tags	O
which	O
will	O
be	O
utilized	O
in	O
the	O
later	O
dependency	O
analysis	O
,	O
such	O
as	O
POS	O
and	O
morphological	O
features	O
.	O
Like	O
the	O
previous	O
step	O
,	O
we	O
train	O
taggers	O
for	O
all	O
the	O
rich	O
resource	O
languages	O
.	O
(	O
3	O
)	O
Dependency	O
Parser	O
Tokens	O
and	O
linguistic	O
features	O
generated	O
by	O
taggers	O
are	O
put	O
into	O
the	O
dependency	O
parser	O
to	O
generate	O
the	O
final	O
dependency	O
structures	O
.	O
For	O
Surprise	O
Language	O
Parser	O
,	O
only	O
Dependency	O
Parser	O
is	O
needed	O
.	O
We	O
directly	O
take	O
the	O
provided	O
CoNLL	O
-	O
U	O
files	O
which	O
already	O
include	O
the	O
tokens	O
and	O
features	O
as	O
inputs	O
and	O
predicts	O
the	O
results	O
.	O
Without	O
annotated	O
training	O
data	O
,	O
we	O
could	O
not	O
train	O
the	O
tokenizers	O
and	O
taggers	O
for	O
these	O
languages	O
;	O
Meanwhile	O
for	O
the	O
parsing	O
,	O
we	O
adopt	O
a	O
delexicalized	O
and	O
cross	O
-	O
lingual	O
strategy	O
,	O
which	O
will	O
be	O
described	O
later	O
in	O
Section	O
3.3	O
.	O

In	O
the	O
final	O
testing	O
phase	O
of	O
the	O
shared	O
task	O
,	O
there	O
are	O
mainly	O
three	O
types	O
of	O
test	O
data	O
(	O
Nivre	O
et	O
al	O
,	O
2017a	O
)	O

In	O
the	O
Known	O
Language	O
Parser	O
,	O
the	O
first	O
step	O
is	O
to	O
tokenize	O
the	O
input	O
raw	O
text	O
,	O
generating	O
the	O
basic	O
units	O
for	O
later	O
processing	O
.	O
We	O
train	O
tokenizers	O
for	O
all	O
the	O
languages	O
using	O
UDPipe	O
,	O
including	O
those	O
ones	O
which	O
are	O
quite	O
easy	O
to	O
separate	O
using	O
simple	O
rules	O
,	O
like	O
identifying	O
the	O
blank	O
spaces	O
in	O
English	O
.	O
Considering	O
there	O
are	O
some	O
languages	O
that	O
could	O
not	O
be	O
simply	O
tokenized	O
by	O
blank	O
spaces	O
,	O
we	O
adopt	O
this	O
unified	O
treatment	O
for	O
this	O
step	O
.	O
The	O
tokenizers	O
are	O
trained	O
mainly	O
using	O
the	O
SpaceAfter	O
features	O
provided	O
in	O
the	O
CoNLL	O
-	O
U	O
files	O
and	O
the	O
parameters	O
of	O
UDPipe	O
Tokenizer	O
are	O
shown	O
in	O
Table	O
1	O
.	O

Morphological	O
features	O
from	O
the	O
universal	O
feature	O
inventory	O
or	O
from	O
a	O
defined	O
language	O
-	O
specific	O
extension	O
.	O
These	O
features	O
will	O
be	O
used	O
as	O
inputs	O
in	O
the	O
final	O
parsing	O
step	O
for	O
Rich	O
Resource	O
Languages	O
.	O

Evaluation	O
process	O
of	O
this	O
shared	O
task	O
is	O
deployed	O
in	O
TIRA	O
6	O
(	O
Potthast	O
et	O
al	O
,	O
2014	O
)	O
.	O
LAS	O
is	O
the	O
main	O
scoring	O
metric	O
and	O
we	O
show	O
performances	O
of	O
our	O
system	O
in	O
several	O
types	O
of	O
treebanks	O
in	O
Table	O
5	O
using	O
the	O
same	O
groups	O
as	O
the	O
official	O
results	O
.	O
What	O
's	O
more	O
,	O
LAS	O
of	O
our	O
system	O
in	O
Surprise	O
Languages	O
are	O
shown	O
in	O
Table	O
6	O
.	O
We	O
show	O
several	O
official	O
evaluation	O
results	O
such	O
as	O
LAS	O
,	O
UAS	O
and	O
other	O
results	O
and	O
compared	O
with	O
best	O
results	O
in	O
Table	O
7	O
.	O

Let	O
a	O
test	O
document	O
x	O
be	O
a	O
sequence	O
of	O
words	O
(	O
w	O
1	O
,	O
,	O
w	O
j	O
,	O
)	O
,	O
and	O
a	O
class	O
topic	O
description	O
y	O
be	O
a	O
sequence	O
of	O
words	O
d	O
y	O
=	O
(	O
w	O
1	O
,	O
,	O
w	O
y	O
,	O
)	O
.	O
All	O
words	O
are	O
in	O
vocabulary	O
V	O
.	O
We	O
propose	O
a	O
generative	O
approach	O
,	O
where	O
the	O
predictive	O
probabil	O
-	O
ity	O
p	O
(	O
y	O
|	O
x	O
)	O
∝	O
p	O
(	O
x	O
|	O
y	O
)	O
p	O
(	O
y	O
)	O
.	O
Generative	O
approaches	O
tends	O
to	O
perform	O
well	O
when	O
training	O
data	O
is	O
scarce	O
,	O
which	O
is	O
the	O
case	O
in	O
our	O
setting	O
.	O
We	O
assume	O
there	O
exists	O
weak	O
prior	O
knowledge	O
on	O
which	O
classes	O
are	O
popular	O
and	O
which	O
are	O
rare	O
.	O
We	O
can	O
then	O
construct	O
rough	O
estimatesp	O
(	O
y	O
)	O
using	O
simple	O
heuristics	O
as	O
described	O
in	O
(	O
Schapire	O
et	O
al	O
,	O
2002	O
)	O
.	O
It	O
distributes	O
probability	O
mass	O
q	O
evenly	O
among	O
majority	O
classes	O
,	O
and	O
1	O
−	O
q	O
evenly	O
among	O
minority	O
classes	O
.	O
We	O
treat	O
the	O
most	O
frequent	O
class	O
as	O
the	O
majority	O
class	O
,	O
the	O
rest	O
as	O
minority	O
classes	O
,	O
and	O
q	O
=	O
0.7	O
in	O
our	O
experiments	O
.	O
By	O
interpreting	O
class	O
topic	O
description	O
as	O
words	O
,	O
we	O
obtainp	O
(	O
x	O
|	O
y	O
)	O
=	O
p	O
(	O
x	O
|	O
d	O
y	O
)	O
.	O
We	O
assume	O
that	O
the	O
d	O
y	O
expresses	O
a	O
noisy	O
-	O
OR	O
relation	O
of	O
the	O
words	O
it	O
contains	O
(	O
Oniśko	O
et	O
al	O
,	O
2001	O
)	O
.	O
Up	O
to	O
first	O
-	O
order	O
approximation	O
:	O
p	O
(	O
x	O
|	O
d	O
y	O
)	O
=	O
1	O
−	O
wy	O
dy	O
(	O
1	O
−	O
p	O
(	O
x	O
|	O
w	O
y	O
)	O
)	O
≈	O
wy	O
dy	O
p	O
(	O
x	O
|	O
w	O
y	O
)	O
,	O
(	O
1	O
)	O
where	O
each	O
w	O
y	O
is	O
a	O
word	O
in	O
the	O
class	O
topic	O
description	O
d	O
y	O
.	O
Further	O
,	O
we	O
assume	O
that	O
words	O
in	O
document	O
x	O
are	O
conditionally	O
independent	O
given	O
a	O
label	O
word	O
w	O
y	O
(	O
naïve	O
Bayes	O
assumption	O
)	O
:	O
p	O
(	O
x	O
|	O
w	O
y	O
)	O
=	O
w	O
j	O
x	O
p	O
(	O
w	O
j	O
|	O
w	O
y	O
)	O
.	O
(	O
2	O
)	O
Combining	O
(	O
1	O
)	O
and	O
(	O
2	O
)	O
,	O
the	O
document	O
likelihood	O
iŝ	O
p	O
(	O
x	O
|	O
y	O
)	O
=	O
wy	O
dy	O
w	O
j	O
x	O
p	O
(	O
w	O
j	O
|	O
w	O
y	O
)	O
.	O
To	O
this	O
end	O
,	O
we	O
need	O
a	O
word	O
association	O
model	O
p	O
(	O
w	O
1	O
|	O
w	O
2	O
)	O
,	O
∀w	O
1	O
,	O
w	O
2	O
V	O
.	O
It	O
can	O
be	O
efficiently	O
learned	O
by	O
word	O
embedding	O
algorithms	O
.	O
The	O
skipgram	O
algorithm	O
learns	O
vector	O
representations	O
of	O
words	O
,	O
such	O
that	O
for	O
words	O
w	O
1	O
,	O
w	O
2	O
,	O
their	O
vectors	O
u	O
w	O
1	O
,	O
v	O
w	O
2	O
approximate	O
the	O
conditional	O
probability	O
1	O
p	O
(	O
w	O
1	O
|	O
w	O
2	O
)	O
=	O
exp	O
u	O
w	O
1	O
v	O
w	O
2	O
w	O
V	O
exp	O
(	O
u	O
w	O
v	O
w	O
2	O
)	O
.	O
(	O
4	O
)	O
Combining	O
(	O
3	O
)	O
with	O
(	O
4	O
)	O
,	O
the	O
document	O
likelihood	O
becomeŝ	O
p	O
(	O
x	O
|	O
y	O
)	O
=	O
wy	O
dy	O
exp	O
	O
	O
w	O
j	O
x	O
u	O
w	O
j	O
v	O
wy	O
−	O
C	O
wy	O
	O
	O
,	O
where	O
C	O
wy	O
=	O
log	O
w	O
V	O
exp	O
u	O
w	O
v	O
wy	O
is	O
independent	O
of	O
document	O
x	O
and	O
only	O
related	O
to	O
label	O
word	O
w	O
y	O
,	O
therefore	O
can	O
be	O
precomputed	O
and	O
stored	O
to	O
save	O
computation	O
.	O
Finally	O
,	O
we	O
construct	O
an	O
generative	O
classifier	O
aŝ	O
p	O
(	O
y	O
|	O
x	O
)	O
∝p	O
(	O
x	O
|	O
y	O
)	O
p	O
(	O
y	O
)	O
.	O
We	O
call	O
this	O
method	O
word	O
embedding	O
naïve	O
Bayes	O
(	O
WENB	O
)	O
.	O

We	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
helpful	O
comments	O
.	O
This	O
work	O
was	O
in	O
part	O
supported	O
by	O
the	O
National	O
Library	O
of	O
Medicine	O
under	O
grant	O
number	O
2R01LM010681	O
-	O
05	O
.	O
Qiaozhu	O
Mei	O
's	O
work	O
was	O
supported	O
in	O
part	O
by	O
the	O
National	O
Science	O
Foundation	O
under	O
grant	O
numbers	O
1633370	O
and	O
1620319	O
.	O
Yue	O
Wang	O
would	O
like	O
to	O
thank	O
the	O
support	O
of	O
the	O
Eleanor	O
M.	O
and	O
Frederick	O
G.	O
Kilgour	O
Research	O
Grant	O
Award	O
by	O
the	O
UNC	O
-	O
CH	O
School	O
of	O
Information	O
and	O
Library	O
Science	O
.	O

The	O
author	O
would	O
like	O
to	O
thank	O
Saleh	O
Soltan	O
,	O
Gokhan	O
Tur	O
,	O
Saab	O
Mansour	O
,	O
and	O
Batool	O
Haider	O
for	O
reviewing	O
this	O
work	O
and	O
providing	O
valuable	O
feedback	O
.	O

Automatic	O
Compositor	O
Attribution	O
in	O
the	O
First	O
Folio	O
of	O
Shakespeare	O

Within	O
literary	O
studies	O
,	O
the	O
field	O
of	O
bibliography	O
has	O
an	O
unusually	O
long	O
tradition	O
of	O
quantitative	O
analysis	O
.	O
One	O
particularly	O
relevant	O
area	O
is	O
that	O
of	O
compositor	O
attribution	O
-	O
the	O
clustering	O
of	O
pages	O
in	O
a	O
historical	O
printed	O
document	O
by	O
the	O
individual	O
(	O
the	O
compositor	O
)	O
who	O
set	O
the	O
type	O
.	O
Like	O
stylometry	O
,	O
a	O
long	O
-	O
standing	O
area	O
of	O
NLP	O
that	O
has	O
largely	O
focused	O
on	O
attributing	O
the	O
authorship	O
of	O
text	O
(	O
Holmes	O
,	O
1994	O
;	O
Hope	O
,	O
1994	O
;	O
Juola	O
,	O
2006	O
;	O
Koppel	O
et	O
al	O
,	O
2009	O
;	O
Jockers	O
and	O
Witten	O
,	O
2010	O
)	O
,	O
the	O
analysis	O
of	O
orthographic	O
patterns	O
is	O
fundamental	O
to	O
compositor	O
attribution	O
.	O
Additionally	O
,	O
compositor	O
attribution	O
often	O
makes	O
use	O
of	O
visual	O
features	O
,	O
such	O
as	O
whitespace	O
layout	O
,	O
introducing	O
new	O
challenges	O
.	O
These	O
analyses	O
have	O
traditionally	O
been	O
done	O
by	O
hand	O
,	O
but	O
efforts	O
are	O
painstaking	O
due	O
to	O
the	O
difficulty	O
of	O
manually	O
recording	O
these	O
features	O
.	O
In	O
this	O
paper	O
,	O
we	O
present	O
an	O
unsupervised	O
model	O
specifically	O
designed	O
for	O
compositor	O
attribution	O
that	O
incorporates	O
both	O
textual	O
and	O
visual	O
sources	O
of	O
evidence	O
traditionally	O
used	O
by	O
bibliographers	O
(	O
Hinman	O
,	O
1963	O
;	O
Taylor	O
,	O
1981	O
;	O
Blayney	O
,	O
1991	O
)	O
.	O
spelling	O
variation	O
spacing	O
variation	O
medial	O
comma	O
Figure	O
1	O
:	O
The	O
compositor	O
of	O
the	O
left	O
page	O
tended	O
to	O
use	O
the	O
spellings	O
doe	O
and	O
deere	O
,	O
while	O
the	O
compositor	O
for	O
the	O
right	O
page	O
used	O
spellings	O
do	O
and	O
deare	O
,	O
indicating	O
these	O
pages	O
were	O
likely	O
set	O
by	O
different	O
people	O
.	O
The	O
varying	O
width	O
of	O
the	O
medial	O
comma	O
whitespace	O
also	O
distinguishes	O
the	O
typesetters	O
.	O
Our	O
model	O
jointly	O
describes	O
the	O
patterns	O
of	O
variation	O
both	O
in	O
orthography	O
and	O
in	O
the	O
whitespace	O
between	O
glyphs	O
,	O
allowing	O
us	O
to	O
cluster	O
pages	O
by	O
discovering	O
patterns	O
of	O
similarity	O
and	O
difference	O
.	O
When	O
applied	O
to	O
digital	O
scans	O
of	O
historical	O
printed	O
documents	O
,	O
our	O
approach	O
learns	O
orthographic	O
and	O
whitespace	O
preferences	O
of	O
individual	O
compositors	O
and	O
predicts	O
groupings	O
of	O
pages	O
set	O
by	O
the	O
same	O
compositor	O
.	O
1	O
This	O
is	O
,	O
to	O
our	O
knowledge	O
,	O
the	O
first	O
attempt	O
to	O
perform	O
compositor	O
attribution	O
automatically	O
.	O
Prior	O
work	O
has	O
proposed	O
automatic	O
approaches	O
to	O
authorship	O
attributionwhich	O
is	O
typically	O
viewed	O
as	O
the	O
supervised	O
problem	O
of	O
identifying	O
a	O
particular	O
author	O
given	O
samples	O
of	O
their	O
writing	O
.	O
In	O
contrast	O
,	O
compositor	O
attribution	O
lacks	O
supervision	O
because	O
compositors	O
are	O
unknown	O
and	O
,	O
in	O
addition	O
,	O
focuses	O
on	O
different	O
linguistic	O
patterns	O
.	O
We	O
explain	O
spellings	O
of	O
words	O
conditioned	O
on	O
word	O
choice	O
,	O
not	O
the	O
word	O
choice	O
itself	O
.	O
c	O
i	O
s	O
ik	O
m	O
ij	O
d	O
ij	O
dear	O
deere	O
K	O
i	O
J	O
i	O
I	O
Compositor	O
C	O
!	O
!	O
!	O
!	O
!	O
!	O
Edit	O
operation	O
weights	O
:	O

Word	O
variant	O
weights	O
:	O
Figure	O
2	O
:	O
In	O
our	O
model	O
,	O
a	O
compositor	O
ci	O
is	O
generated	O
for	O
page	O
i	O
from	O
a	O
multinomial	O
prior	O
.	O
Then	O
,	O
each	O
diplomatic	O
word	O
,	O
dij	O
,	O
is	O
generated	O
conditioned	O
on	O
ci	O
and	O
the	O
corresponding	O
modern	O
word	O
,	O
mij	O
,	O
from	O
a	O
distribution	O
parameterized	O
by	O
weight	O
vector	O
wc	O
.	O
Finally	O
,	O
each	O
medial	O
comma	O
spacing	O
width	O
(	O
measured	O
in	O
pixels	O
)	O
,	O
s	O
ik	O
,	O
is	O
generated	O
conditioned	O
on	O
ci	O
from	O
a	O
distribution	O
parameterized	O
by	O
θc	O
.	O
To	O
evaluate	O
our	O
approach	O
,	O
we	O
fit	O
our	O
model	O
to	O
digital	O
scans	O
of	O
Shakespeare	O
's	O
First	O
Folio	O
(	O
1623	O
)	O
a	O
document	O
with	O
well	O
established	O
manual	O
judgements	O
of	O
compositor	O
attribution	O
.	O
We	O
find	O
that	O
even	O
when	O
relying	O
on	O
noisy	O
OCR	O
transcriptions	O
of	O
textual	O
content	O
,	O
our	O
model	O
predicts	O
compositor	O
attributions	O
that	O
agree	O
with	O
manual	O
annotations	O
87	O
%	O
of	O
the	O
time	O
,	O
outperforming	O
several	O
simpler	O
baselines	O
.	O
Our	O
approach	O
opens	O
new	O
possibilities	O
for	O
considering	O
patterns	O
across	O
a	O
larger	O
vocabulary	O
of	O
words	O
and	O
at	O
a	O
higher	O
visual	O
resolution	O
than	O
has	O
been	O
possible	O
historically	O
.	O
Such	O
a	O
tool	O
may	O
enable	O
scalable	O
first	O
-	O
pass	O
analysis	O
in	O
understudied	O
domains	O
as	O
a	O
complement	O
to	O
humanistic	O
studies	O
of	O
composition	O
.	O

In	O
this	O
paper	O
we	O
focus	O
on	O
modeling	O
the	O
same	O
types	O
of	O
observations	O
made	O
by	O
scholars	O
and	O
demonstrate	O
agreement	O
with	O
authoritative	O
attributions	O
.	O
We	O
use	O
compositor	O
studies	O
of	O
Shakespeare	O
's	O
First	O
Folio	O
to	O
inform	O
our	O
approach	O
,	O
drawing	O
on	O
the	O
methods	O
proposed	O
by	O
Hinman	O
(	O
1963	O
)	O
,	O
Howard	O
-	O
Hill	O
(	O
1973	O
)	O
,	O
andTaylor	O
(	O
1981	O
)	O
.	O
Hinman	O
's	O
landmark	O
1963	O
study	O
clustered	O
the	O
pages	O
of	O
the	O
First	O
Folio	O
according	O
to	O
five	O
different	O
compositors	O
based	O
on	O
variations	O
in	O
spelling	O
among	O
three	O
common	O
words	O
.	O
Figure	O
1	O
,	O
for	O
example	O
,	O
shows	O
portions	O
of	O
two	O
pages	O
of	O
the	O
First	O
Folio	O
with	O
different	O
spelling	O
variants	O
for	O
the	O
words	O
dear	O
and	O
do	O
:	O
one	O
compositor	O
used	O
deere	O
and	O
doe	O
,	O
while	O
the	O
other	O
used	O
deare	O
and	O
do	O
.	O
Hinman	O
relied	O
on	O
the	O
assumption	O
that	O
each	O
compositor	O
was	O
consistent	O
in	O
their	O
preferences	O
for	O
the	O
sake	O
of	O
convenience	O
in	O
the	O
typesetting	O
process	O
(	O
Blayney	O
,	O
1991	O
)	O
.	O
Subsequent	O
studies	O
looked	O
at	O
larger	O
sets	O
of	O
words	O
and	O
more	O
general	O
orthographic	O
preferences	O
(	O
e.g.	O
the	O
preference	O
to	O
terminate	O
words	O
with	O
-	O
ie	O
instead	O
of	O
-	O
y	O
)	O
,	O
leading	O
to	O
modifications	O
of	O
Hinman	O
's	O
original	O
analysis	O
(	O
Howard	O
-	O
Hill	O
,	O
1973	O
;	O
Taylor	O
,	O
1981	O
)	O
.	O
In	O
this	O
paper	O
we	O
propose	O
a	O
probabilistic	O
model	O
designed	O
to	O
capture	O
both	O
word	O
-	O
specific	O
preferences	O
and	O
general	O
orthographic	O
patterns	O
.	O
To	O
separate	O
the	O
effect	O
of	O
the	O
compositor	O
from	O
the	O
choices	O
made	O
by	O
the	O
author	O
or	O
editor	O
,	O
we	O
condition	O
on	O
a	O
modernized	O
(	O
collated	O
)	O
version	O
of	O
Shakespeare	O
's	O
text	O
as	O
was	O
done	O
by	O
scholars	O
.	O
Visual	O
features	O
,	O
including	O
typeface	O
usage	O
and	O
whitespace	O
layout	O
,	O
also	O
inform	O
compositor	O
attribution	O
.	O
For	O
example	O
,	O
the	O
highlighted	O
spacing	O
in	O
Figure	O
1	O
shows	O
different	O
choices	O
after	O
medial	O
commas	O
(	O
commas	O
that	O
occur	O
before	O
the	O
end	O
of	O
the	O
line	O
)	O
.	O
Bibliographers	O
produced	O
new	O
hypotheses	O
about	O
how	O
many	O
compositors	O
were	O
involved	O
in	O
production	O
based	O
on	O
the	O
analysis	O
of	O
the	O
use	O
of	O
spaces	O
before	O
and	O
after	O
punctuation	O
(	O
Howard	O
-	O
Hill	O
,	O
1973	O
;	O
Taylor	O
,	O
1981	O
)	O
.	O
We	O
additionally	O
incorporate	O
this	O
source	O
of	O
evidence	O
into	O
to	O
our	O
automatic	O
approach	O
by	O
modeling	O
pixel	O
-	O
level	O
whitespace	O
distances	O
.	O
Bibliographers	O
also	O
use	O
contextual	O
information	O
to	O
inform	O
their	O
analyses	O
,	O
including	O
copy	O
text	O
orthography	O
,	O
printing	O
house	O
records	O
,	O
collation	O
,	O
type	O
case	O
usage	O
,	O
and	O
the	O
use	O
of	O
type	O
with	O
cast	O
-	O
on	O
spaces	O
.	O
In	O
our	O
model	O
,	O
we	O
restrict	O
our	O
analysis	O
to	O
only	O
those	O
features	O
that	O
can	O
be	O
derived	O
from	O
the	O
OCR	O
output	O
and	O
simple	O
visual	O
analysis	O
.	O

The	O
results	O
on	O
OCR	O
(	O
character	O
error	O
rate	O
for	O
most	O
plays	O
≈	O
10	O
−	O
15	O
%	O
)	O
transcripts	O
are	O
only	O
marginally	O
worse	O
than	O
those	O
on	O
manual	O
transcripts	O
,	O
which	O
shows	O
that	O
our	O
approach	O
can	O
be	O
generalized	O
for	O
the	O
common	O
case	O
where	O
manual	O
diplomatic	O
transcriptions	O
are	O
not	O
available	O
.	O
For	O
our	O
experiments	O
,	O
we	O
also	O
chose	O
a	O
common	O
modern	O
edition	O
of	O
Shakespeare	O
instead	O
of	O
more	O
carefully	O
produced	O
modernized	O
transcription	O
of	O
the	O
facsimile	O
-	O
our	O
goal	O
being	O
to	O
again	O
show	O
that	O
this	O
approach	O
can	O
be	O
generalized	O
,	O
perhaps	O
to	O
documents	O
where	O
careful	O
modernizations	O
of	O
the	O
facsimile	O
are	O
not	O
available	O
.	O
Together	O
,	O
these	O
results	O
suggest	O
that	O
our	O
model	O
may	O
be	O
sufficiently	O
robust	O
to	O
aid	O
bibliographers	O
in	O
their	O
analysis	O
of	O
less	O
studied	O
texts	O
.	O
Figure	O
3	O
shows	O
an	O
example	O
of	O
the	O
feature	O
weights	O
and	O
spacing	O
parameters	O
learned	O
by	O
the	O
FEAT	O
w/	O
ALL	O
model	O
.	O
Our	O
statistical	O
approach	O
is	O
able	O
to	O
successfully	O
explain	O
some	O
of	O
the	O
observations	O
scholars	O
made	O
.	O
For	O
example	O
,	O
Taylor	O
(	O
1981	O
)	O
notices	O
that	O
compositors	O
C	O
and	O
D	O
prefer	O
to	O
omit	O
u	O
in	O
young	O
but	O
A	O
does	O
not	O
.	O
Our	O
model	O
reflects	O
this	O
by	O
giving	O
u	O
DEL	O
high	O
weight	O
for	O
D	O
and	O
low	O
weight	O
for	O
A.	O
However	O
,	O
the	O
weight	O
of	O
a	O
single	O
feature	O
is	O
difficult	O
to	O
interpret	O
in	O
isolation	O
.	O
This	O
might	O
be	O
the	O
reason	O
why	O
our	O
model	O
only	O
moderately	O
agrees	O
in	O
case	O
of	O
compositor	O
C.	O
Another	O
example	O
can	O
be	O
seen	O
in	O
spacing	O
patterns	O
:	O
according	O
to	O
Taylor	O
(	O
1981	O
)	O
,	O
compositor	O
C	O
uses	O
spaced	O
medial	O
commas	O
unlike	O
A	O
and	O
D.	O
Our	O
model	O
learns	O
the	O
same	O
behavior	O
.	O

Our	O
primary	O
goal	O
is	O
to	O
scale	O
the	O
methods	O
of	O
compositor	O
attribution	O
,	O
including	O
both	O
textual	O
and	O
visual	O
modes	O
of	O
evidence	O
,	O
for	O
use	O
across	O
books	O
and	O
corpora	O
.	O
By	O
using	O
principled	O
statistical	O
techniques	O
and	O
considering	O
evidence	O
at	O
a	O
larger	O
scale	O
,	O
we	O
offer	O
a	O
more	O
robust	O
approach	O
to	O
compositor	O
identification	O
than	O
has	O
previously	O
been	O
possible	O
.	O
The	O
fact	O
that	O
our	O
system	O
works	O
well	O
on	O
OCR	O
texts	O
means	O
that	O
we	O
are	O
not	O
restricted	O
to	O
only	O
those	O
documents	O
for	O
which	O
we	O
have	O
manually	O
produced	O
transcriptions	O
,	O
opening	O
up	O
the	O
possibility	O
for	O
bibliographic	O
study	O
on	O
a	O
much	O
larger	O
class	O
of	O
texts	O
.	O
Though	O
we	O
are	O
unable	O
to	O
incorporate	O
the	O
kinds	O
of	O
world	O
knowledge	O
used	O
by	O
bibliographers	O
,	O
our	O
ability	O
to	O
include	O
more	O
information	O
and	O
more	O
finegrained	O
information	O
allows	O
us	O
to	O
recreate	O
their	O
results	O
.	O
Having	O
validated	O
these	O
techniques	O
on	O
the	O
First	O
Folio	O
,	O
where	O
historical	O
claims	O
are	O
well	O
established	O
,	O
we	O
hope	O
future	O
work	O
can	O
extend	O
these	O
methods	O
and	O
their	O
application	O
.	O

We	O
thank	O
the	O
three	O
anonymous	O
reviewers	O
for	O
their	O
valuable	O
feedback	O
.	O
This	O
project	O
is	O
funded	O
in	O
part	O
by	O
the	O
NSF	O
under	O
grant	O
1618044	O
.	O

Detecting	O
Polarized	O
Topics	O
Using	O
Partisanship	O
-	O
aware	O
Contextualized	O
Topic	O
Embeddings	O

Growing	O
polarization	O
of	O
the	O
news	O
media	O
has	O
been	O
blamed	O
for	O
fanning	O
disagreement	O
,	O
controversy	O
and	O
even	O
violence	O
.	O
Early	O
identification	O
of	O
polarized	O
topics	O
is	O
thus	O
an	O
urgent	O
matter	O
that	O
can	O
help	O
mitigate	O
conflict	O
.	O
However	O
,	O
accurate	O
measurement	O
of	O
topic	O
-	O
wise	O
polarization	O
is	O
still	O
an	O
open	O
research	O
challenge	O
.	O
To	O
address	O
this	O
gap	O
,	O
we	O
propose	O
Partisanship	O
-	O
aware	O
Contextualized	O
Topic	O
Embeddings	O
(	O
PaCTE	O
)	O
,	O
a	O
method	O
to	O
automatically	O
detect	O
polarized	O
topics	O
from	O
partisan	O
news	O
sources	O
.	O
Specifically	O
,	O
utilizing	O
a	O
language	O
model	O
that	O
has	O
been	O
finetuned	O
on	O
recognizing	O
partisanship	O
of	O
the	O
news	O
articles	O
,	O
we	O
represent	O
the	O
ideology	O
of	O
a	O
news	O
corpus	O
on	O
a	O
topic	O
by	O
corpus	O
-	O
contextualized	O
topic	O
embedding	O
and	O
measure	O
the	O
polarization	O
using	O
cosine	O
distance	O
.	O
We	O
apply	O
our	O
method	O
to	O
a	O
dataset	O
of	O
news	O
articles	O
about	O
the	O
COVID	O
-	O
19	O
pandemic	O
.	O
Extensive	O
experiments	O
on	O
different	O
news	O
sources	O
and	O
topics	O
demonstrate	O
the	O
efficacy	O
of	O
our	O
method	O
to	O
capture	O
topical	O
polarization	O
,	O
as	O
indicated	O
by	O
its	O
effectiveness	O
of	O
retrieving	O
the	O
most	O
polarized	O
topics	O
.	O
1	O

We	O
use	O
the	O
AYLIEN	O
COVID	O
-	O
19	O
dataset	O
3	O
consisting	O
of~1.5	O
M	O
news	O
articles	O
related	O
to	O
the	O
pandemic	O
spanning	O
from	O
Nov	O
2019	O
to	O
July	O
2020	O
that	O
are	O
from	O
440	O
global	O
sources	O
.	O
To	O
discover	O
the	O
polarization	O
between	O
politically	O
divided	O
news	O
media	O
,	O
we	O
select	O
six	O
well	O
-	O
known	O
US	O
publishers	O
evenly	O
split	O
between	O
partisan	O
leanings	O
:	O
CNN	O
,	O
Huffington	O
Post	O
(	O
Huff	O
)	O
,	O
New	O
York	O
Times	O
(	O
NYT	O
)	O
as	O
liberal	O
sources	O
vs.	O
Fox	O
,	O
Breitbart	O
(	O
Breit	O
)	O
and	O
New	O
York	O
Post	O
(	O
NYP	O
)	O
as	O
conservative	O
sources	O
.	O
After	O
filtering	O
the	O
publishers	O
and	O
remove	O
duplicate	O
articles	O
,	O
66	O
,	O
368	O
articles	O
are	O
left	O
spanning	O
from	O
Jan	O
2020	O
to	O
July	O
2020	O
.	O
The	O
statistics	O
of	O
news	O
articles	O
are	O
shown	O
in	O
Appendix	O
A.	O

To	O
quantitatively	O
evaluate	O
the	O
effectiveness	O
of	O
PaCTE	O
and	O
the	O
baselines	O
in	O
capturing	O
topic	O
polarization	O
,	O
we	O
use	O
the	O
10	O
manually	O
labeled	O
topics	O
to	O
create	O
a	O
ground	O
truth	O
ranking	O
of	O
polarized	O
topics	O
and	O
score	O
models	O
on	O
their	O
ability	O
to	O
retrieve	O
the	O
most	O
polarized	O
topics	O
on	O
this	O
ranked	O
list	O
.	O
Evaluation	O
protocol	O
.	O
Given	O
a	O
liberal	O
news	O
corpus	O
D	O
L	O
,	O
a	O
conservative	O
news	O
corpus	O
D	O
R	O
,	O
and	O
a	O
list	O
of	O
10	O
topics	O
ranked	O
by	O
ground	O
-	O
truth	O
polarization	O
scores	O
,	O
l	O
gt	O
(	O
D	O
L	O
,	O
D	O
R	O
,	O
T	O
labeled	O
)	O
,	O
as	O
described	O
in	O
Section	O
4.3	O
,	O
we	O
define	O
the	O
top	O
-	O
3	O
topics	O
in	O
the	O
list	O
as	O
the	O
target	O
polarized	O
topics	O
that	O
deserve	O
more	O
attention	O
and	O
that	O
should	O
be	O
addressed	O
when	O
trying	O
to	O
prevent	O
polarization	O
from	O
escalating	O
.	O
The	O
target	O
polarized	O
topics	O
between	O
different	O
pairs	O
of	O
news	O
sources	O
are	O
shown	O
in	O
Table	O
2	O
.	O
Then	O
,	O
given	O
a	O
ranked	O
list	O
of	O
topics	O
f	O
pred	O
(	O
D	O
L	O
,	O
D	O
R	O
,	O
T	O
labeled	O
)	O
predicted	O
by	O
a	O
model	O
,	O
we	O
evaluate	O
how	O
effectively	O
the	O
3	O
target	O
polarized	O
topics	O
are	O
retrieved	O
in	O
this	O
model	O
predicted	O
list	O
using	O
recall@3	O
.	O
In	O
other	O
words	O
,	O
we	O
check	O
how	O
much	O
the	O
overlap	O
is	O
between	O
the	O
top	O
-	O
3	O
topics	O
in	O
the	O
ground	O
-	O
truth	O
ranking	O
and	O
the	O
top	O
-	O
3	O
topics	O
in	O
the	O
predicted	O
ranking	O
,	O
of	O
the	O
10	O
labeled	O
topics	O
.	O
We	O
call	O
this	O
task	O
polarized	O
topics	O
retrieval	O
.	O

In	O
Section	O
4.5	O
we	O
quantitatively	O
demonstrate	O
the	O
effectiveness	O
of	O
PaCTE	O
in	O
retrieving	O
polarized	O
topics	O
when	O
evaluating	O
with	O
the	O
10	O
labeled	O
topics	O
.	O
We	O
believe	O
that	O
such	O
success	O
generalizes	O
to	O
the	O
case	O
where	O
the	O
input	O
to	O
the	O
model	O
is	O
the	O
complete	O
topic	O
list	O
T	O
containing	O
30	O
topics	O
.	O
In	O
this	O
section	O
,	O
we	O
conduct	O
a	O
case	O
study	O
and	O
retrieve	O
the	O
top	O
-	O
3	O
most	O
polarized	O
topics	O
from	O
T	O
in	O
CNN	O
vs.	O
Fox	O
,	O
Huff	O
vs.	O
Breit	O
and	O
NYT	O
vs.	O
NYP	O
,	O
by	O
PaCTE	O
.	O
Since	O
we	O
do	O
not	O
have	O
the	O
ground	O
-	O
truth	O
target	O
polarized	O
topics	O
from	O
T	O
,	O
for	O
the	O
retrieved	O
topics	O
,	O
we	O
conduct	O
manual	O
inspections	O
on	O
relevant	O
documents	O
and	O
give	O
explanations	O
about	O
the	O
polarization	O
.	O
For	O
the	O
topics	O
in	O
T	O
labeled	O
,	O
the	O
polarization	O
is	O
formed	O
due	O
to	O
the	O
two	O
political	O
stances	O
.	O
Therefore	O
in	O
this	O
section	O
we	O
only	O
focus	O
on	O
the	O
retrieved	O
topics	O
not	O
in	O
T	O
labeled	O
.	O
CNN	O
vs.	O
Fox	O
.	O
The	O
retrieved	O
top	O
-	O
3	O
topics	O
are	O
topic	O
28	O
,	O
6	O
,	O
10	O
,	O
where	O
topic	O
10	O
is	O
in	O
T	O
labeled	O
.	O
The	O
first	O
retrieved	O
topic	O
is	O
topic	O
28	O
,	O
where	O
CNN	O
suggests	O
the	O
surge	O
of	O
new	O
COVID	O
cases	O
every	O
day	O
but	O
Fox	O
suggests	O
that	O
the	O
state	O
should	O
reopen	O
.	O
On	O
topic	O
6	O
CNN	O
reports	O
the	O
serious	O
situation	O
of	O
coronavirus	O
in	O
the	O
US	O
,	O
including	O
the	O
high	O
number	O
of	O
cases	O
and	O
collapse	O
of	O
quarantine	O
hotels	O
,	O
but	O
Fox	O
focuses	O
more	O
on	O
worldwide	O
coronavirus	O
situation	O
and	O
suggests	O
the	O
high	O
number	O
of	O
cases	O
in	O
Michigan	O
is	O
misleading	O
.	O
Huff	O
vs.	O
Breit	O
.	O
The	O
retrieved	O
top	O
-	O
3	O
topics	O
are	O
topic	O
29	O
,	O
9	O
,	O
31	O
,	O
where	O
topic	O
9	O
is	O
in	O
T	O
labeled	O
.	O
On	O
topic	O
29	O
,	O
Huff	O
advocates	O
Pelosi	O
's	O
coronavirus	O
bills	O
while	O
Breit	O
criticizes	O
them	O
.	O
On	O
topic	O
31	O
,	O
the	O
articles	O
talk	O
about	O
different	O
court	O
cases	O
;	O
however	O
,	O
no	O
clear	O
polarization	O
is	O
discerned	O
between	O
the	O
pair	O
of	O
news	O
sources	O
by	O
manual	O
inspections	O
.	O
We	O
regard	O
it	O
as	O
a	O
failure	O
case	O
of	O
PaCTE	O
.	O
Although	O
the	O
relevant	O
articles	O
are	O
regarding	O
the	O
same	O
topic	O
,	O
they	O
have	O
different	O
subjects	O
or	O
events	O
,	O
and	O
thus	O
misleading	O
PaCTE	O
to	O
perceive	O
polarization	O
between	O
them	O
.	O
NYT	O
vs.	O
NYP	O
.	O
The	O
retrieved	O
top	O
-	O
3	O
topics	O
are	O
topic	O
28	O
,	O
12	O
,	O
10	O
,	O
where	O
topic	O
12	O
and	O
10	O
are	O
in	O
T	O
labeled	O
.	O
On	O
topic	O
28	O
,	O
just	O
as	O
in	O
CNN	O
vs.	O
Fox	O
,	O
NYT	O
takes	O
the	O
pandemic	O
more	O
seriously	O
and	O
NYP	O
suggests	O
reopening	O
.	O
As	O
a	O
result	O
,	O
despite	O
a	O
minor	O
error	O
,	O
PaCTE	O
manages	O
to	O
retrieve	O
polarized	O
topics	O
from	O
T	O
on	O
the	O
three	O
pairs	O
of	O
news	O
sources	O
.	O
Although	O
we	O
are	O
not	O
able	O
to	O
verify	O
if	O
the	O
retrieved	O
topics	O
are	O
indeed	O
the	O
groundtruth	O
top	O
-	O
3	O
most	O
polarized	O
topics	O
,	O
we	O
argue	O
that	O
if	O
given	O
the	O
ground	O
-	O
truth	O
ranking	O
on	O
T	O
,	O
PaCTE	O
will	O
retain	O
its	O
satisfactory	O
quantitative	O
performance	O
in	O
retrieving	O
polarized	O
topics	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
method	O
to	O
automatically	O
discover	O
topic	O
-	O
level	O
polarization	O
between	O
partisan	O
news	O
sources	O
by	O
contextualized	O
topic	O
embeddings	O
.	O
For	O
evaluation	O
,	O
we	O
create	O
annotations	O
on	O
topic	O
polarization	O
scores	O
in	O
different	O
partisan	O
news	O
source	O
pairs	O
on	O
a	O
variety	O
of	O
topics	O
.	O
Compared	O
to	O
the	O
leaveout	O
estimator	O
(	O
Demszky	O
et	O
al	O
,	O
2019	O
)	O
that	O
is	O
purely	O
based	O
on	O
statistical	O
features	O
,	O
our	O
method	O
can	O
more	O
precisely	O
and	O
meaningfully	O
capture	O
topical	O
polarization	O
as	O
indicated	O
by	O
the	O
performance	O
on	O
polarized	O
topics	O
retrieval	O
.	O
We	O
hope	O
that	O
more	O
NLP	O
and	O
researchers	O
and	O
practitioners	O
can	O
contribute	O
to	O
this	O
research	O
area	O
that	O
is	O
promising	O
but	O
receiving	O
insufficient	O
attention	O
.	O
Because	O
detecting	O
polarized	O
topics	O
between	O
partisan	O
news	O
sources	O
is	O
a	O
less	O
established	O
task	O
in	O
the	O
research	O
community	O
,	O
we	O
articulate	O
the	O
data	O
annotation	O
and	O
the	O
model	O
evaluation	O
in	O
great	O
detail	O
and	O
make	O
the	O
method	O
seemingly	O
"	O
complicate	O
"	O
.	O
However	O
,	O
we	O
believe	O
that	O
for	O
public	O
media	O
watchdogs	O
and	O
social	O
media	O
platforms	O
to	O
flag	O
the	O
highly	O
polarized	O
topics	O
,	O
our	O
method	O
is	O
simple	O
to	O
implement	O
,	O
because	O
each	O
of	O
the	O
five	O
steps	O
described	O
in	O
Section	O
3	O
is	O
based	O
on	O
robust	O
methods	O
in	O
NLP	O
.	O
For	O
future	O
work	O
,	O
we	O
plan	O
to	O
perform	O
our	O
method	O
on	O
more	O
datasets	O
,	O
such	O
as	O
the	O
tweets	O
with	O
noisy	O
texts	O
(	O
Demszky	O
et	O
al	O
,	O
2019	O
)	O
.	O
In	O
addition	O
,	O
we	O
will	O
study	O
how	O
to	O
finetune	O
the	O
language	O
model	O
when	O
when	O
partisanship	O
labels	O
are	O
not	O
available	O
.	O

The	O
statistics	O
of	O
the	O
dataset	O
is	O
in	O
Table	O
5	O
.	O
We	O
use	O
the	O
summary	O
of	O
each	O
news	O
article	O
to	O
perform	O
the	O
textual	O
analysis	O
,	O
because	O
the	O
summary	O
contains	O
sufficient	O
information	O
to	O
understand	O
the	O
political	O
stance	O
of	O
the	O
article	O
and	O
the	O
whole	O
text	O
is	O
lengthy	O
for	O
the	O
pretrained	O
language	O
model	O
to	O
handle	O
.	O
For	O
a	O
complete	O
list	O
of	O
all	O
documents	O
,	O
please	O
check	O
our	O
public	O
repository	O
4	O
.	O

On	O
topic	O
10	O
,	O
we	O
show	O
six	O
examples	O
of	O
news	O
articles	O
,	O
one	O
from	O
each	O
news	O
source	O
.	O
For	O
a	O
complete	O
list	O
of	O
news	O
articles	O
,	O
please	O
refer	O
to	O
our	O
public	O
repository	O
.	O
CNN	O
:	O
There	O
has	O
been	O
a	O
concerted	O
effort	O
among	O
aides	O
and	O
allies	O
to	O
get	O
President	O
Donald	O
Trump	O
to	O
stop	O
conducting	O
the	O
daily	O
coronavirus	O
briefings	O
,	O
multiple	O
sources	O
tell	O
CNN	O
.	O
The	O
briefing	O
came	O
a	O
day	O
after	O
Trump	O
had	O
given	O
a	O
lengthy	O
briefing	O
to	O
the	O
media	O
,	O
at	O
one	O
point	O
suggesting	O
it	O
might	O
be	O
possible	O
to	O
treat	O
coronavirus	O
by	O
injecting	O
people	O
with	O
sunlight	O
or	O
disinfectants	O
.	O
Trump	O
asked	O
White	O
House	O
coronavirus	O
task	O
force	O
coordinator	O
Dr.	O
Deborah	O
Birx	O
during	O
Thursday	O
's	O
briefing	O
.	O
A	O
source	O
close	O
to	O
the	O
coronavirus	O
task	O
force	O
said	O
Trump	O
was	O
upset	O
about	O
the	O
"	O
flack	O
"	O
he	O
was	O
taking	O
after	O
those	O
comments	O
and	O
that	O
appears	O
to	O
be	O
part	O
of	O
the	O
reason	O
why	O
the	O
President	O
cut	O
Friday	O
's	O
briefing	O
short	O
.	O
During	O
the	O
earlier	O
questioning	O
from	O
reporters	O
on	O
Friday	O
,	O
Trump	O
said	O
he	O
was	O
being	O
"	O
sarcastic	O
"	O
with	O
his	O
suggestion	O
that	O
people	O
inject	O
themselves	O
with	O
disinfectant	O
,	O
even	O
though	O
he	O
was	O
clearly	O
being	O
serious	O
during	O
Thursday	O
's	O
briefing	O
.	O
Fox	O
:	O
White	O
House	O
press	O
secretary	O
Kayleigh	O
McEnany	O
,	O
during	O
her	O
first	O
official	O
briefing	O
,	O
promised	O
that	O
she	O
'	O
will	O
never	O
lie	O
'	O
to	O
the	O
press	O
in	O
her	O
new	O
role	O
.	O
White	O
House	O
press	O
secretary	O
Kayleigh	O
McEnany	O
,	O
during	O
her	O
first	O
official	O
briefing	O
,	O
promised	O
that	O
she	O
"	O
will	O
never	O
lie	O
"	O
to	O
the	O
press	O
in	O
her	O
new	O
role	O
.	O
McEnany	O
took	O
the	O
podium	O
for	O
the	O
first	O
time	O
Friday	O
,	O
after	O
being	O
tapped	O
as	O
White	O
House	O
press	O
secretary	O
from	O
her	O
post	O
as	O
national	O
spokeswoman	O
for	O
President	O
Trump	O
's	O
re	O
-	O
election	O
campaign	O
earlier	O
this	O
month	O
.	O
TRUMP	O
NAMES	O
KAYLEIGH	O
MCENANY	O
AS	O
NEW	O
WHITE	O
HOUSE	O
PRESS	O
SECRETARY	O
"	O
I	O
will	O
never	O
lie	O
to	O
you	O
,	O
"	O
McEnany	O
told	O
reporters	O
.	O
McEnany	O
seemed	O
to	O
signal	O
that	O
the	O
White	O
House	O
would	O
scale	O
back	O
on	O
their	O
daily	O
coronavirus	O
task	O
force	O
briefings	O
,	O
which	O
were	O
regularly	O
led	O
by	O
the	O
president	O
himself	O
,	O
and	O
Vice	O
President	O
Pence	O
,	O
with	O
appearances	O
from	O
Dr.	O
Deborah	O
Birx	O
and	O
Dr.	O
Anthony	O
Fauci	O
to	O
provide	O
public	O
health	O
information	O
.	O
Huffington	O
Post	O
:	O
President	O
Donald	O
Trump	O
on	O
Sunday	O
tore	O
into	O
former	O
President	O
Barack	O
Obama	O
,	O
calling	O
him	O
"	O
an	O
incompetent	O
president	O
"	O
after	O
Obama	O
appeared	O
to	O
criticize	O
his	O
response	O
to	O
the	O
coronavirus	O
crisis	O
during	O
two	O
commencement	O
speeches	O
a	O
day	O
earlier	O
.	O
Asked	O
about	O
Obama	O
's	O
remarks	O
,	O
Trump	O
told	O
reporters	O
on	O
the	O
White	O
House	O
lawn	O
that	O
he	O
"	O
did	O
n't	O
hear	O
it	O
"	O
before	O
proceeding	O
to	O
bash	O
his	O
predecessor	O
as	O
"	O
grossly	O
incompetent	O
.	O
"	O
President	O
Trump	O
:	O
"	O
[	O
President	O
Obama	O
]	O
was	O
an	O
incompetent	O
president	O
.	O
But	O
earlier	O
this	O
month	O
,	O
Obama	O
reportedly	O
bashed	O
the	O
Trump	O
administration	O
's	O
response	O
to	O
the	O
pandemic	O
as	O
"	O
an	O
absolute	O
chaotic	O
disaster	O
"	O
during	O
a	O
phone	O
call	O
with	O
some	O
of	O
his	O
former	O
White	O
House	O
aides	O
.	O
When	O
a	O
Washington	O
Post	O
reporter	O
last	O
week	O
asked	O
Trump	O
to	O
explain	O
"	O
Obamagate	O
,	O
"	O
the	O
president	O
refused	O
.	O
Breibart	O
:	O
New	O
York	O
magazine	O
Washington	O
correspondent	O
Olivia	O
Nuzzi	O
responded	O
angrily	O
to	O
criticism	O
from	O
former	O
White	O
House	O
press	O
secretary	O
Ari	O
Fleischer	O
on	O
Monday	O
evening	O
,	O
tweeting	O
at	O
him	O
:	O
"	O
Oh	O
shut	O
the	O
f*ck	O
up	O
.	O
"	O
Fleisher	O
,	O
who	O
served	O
under	O
President	O
George	O
W.	O
Bush	O
,	O
criticized	O
Nuzzi	O
after	O
a	O
Rose	O
Garden	O
press	O
briefing	O
on	O
the	O
coronavirus	O
pandemic	O
in	O
which	O
she	O
asked	O
President	O
Donald	O
Trump	O
:	O
"	O
If	O
an	O
American	O
president	O
loses	O
more	O
Americans	O
over	O
the	O
course	O
of	O
six	O
weeks	O
than	O
died	O
in	O
the	O
entirety	O
of	O
the	O
Vietnam	O
War	O
,	O
does	O
he	O
deserve	O
to	O
be	O
re	O
-	O
elected	O
?	O
"	O
One	O
example	O
is	O
a	O
"	O
fake	O
news	O
"	O
viral	O
photograph	O
of	O
President	O
Lyndon	O
B.	O
Johnson	O
,	O
which	O
was	O
presented	O
by	O
many	O
Trump	O
critics	O
as	O
if	O
Johnson	O
had	O
been	O
expressing	O
grief	O
over	O
the	O
deaths	O
in	O
Vietnam	O
.	O
President	O
Trump	O
is	O
said	O
to	O
be	O
reconsidering	O
post	O
,	O
twitter	O
,	O
video	O
,	O
facebook	O
,	O
tweet	O
,	O
social_media	O
,	O
share	O
,	O
write	O
,	O
call	O
,	O
make	O
10	O
trump	O
,	O
president	O
,	O
white_house	O
,	O
donald	O
,	O
administration	O
,	O
fauci	O
,	O
coronavirus	O
,	O
vice	O
,	O
briefing	O
,	O
task_force	O
11	O
covid	O
,	O
dr	O
,	O
coronavirus	O
,	O
health	O
,	O
disease	O
,	O
drug	O
,	O
expert	O
,	O
risk	O
,	O
treatment	O
,	O
director	O
12	O
mr	O
,	O
biden	O
,	O
campaign	O
,	O
election	O
,	O
party	O
,	O
democratic	O
,	O
voter	O
,	O
joe_biden	O
,	O
republican	O
,	O
primary	O
13	O
school	O
,	O
child	O
,	O
student	O
,	O
university	O
,	O
parent	O
,	O
high	O
,	O
kid	O
,	O
year	O
,	O
family	O
,	O
class	O
15	O
american	O
,	O
pandemic	O
,	O
crisis	O
,	O
america	O
,	O
nation	O
,	O
make	O
,	O
policy	O
,	O
job	O
,	O
people	O
,	O
economy	O
17	O
time	O
,	O
world	O
,	O
space	O
,	O
launch	O
,	O
turn	O
,	O
center	O
,	O
long	O
,	O
life	O
,	O
leave	O
,	O
moment	O
18	O
coronavirus	O
,	O
report	O
,	O
outbreak	O
,	O
accord	O
,	O
ship	O
,	O
official	O
,	O
quarantine	O
,	O
military	O
,	O
force	O
,	O
iran	O
19	O
city	O
,	O
york	O
,	O
de_blasio	O
,	O
mayor	O
,	O
resident	O
,	O
area	O
,	O
yorker	O
,	O
coronavirus	O
,	O
people	O
,	O
tuesday	O
20	O
mask	O
,	O
people	O
,	O
wear	O
,	O
face	O
,	O
service	O
,	O
social_distance	O
,	O
church	O
,	O
sunday	O
,	O
coronavirus	O
,	O
stay	O
21	O
people	O
,	O
time	O
,	O
thing	O
,	O
good	O
,	O
work	O
,	O
make	O
,	O
lot	O
,	O
add	O
,	O
give	O
,	O
feel	O
22	O
department	O
,	O
official	O
,	O
national	O
,	O
security	O
,	O
fire	O
,	O
investigation	O
,	O
report	O
,	O
threat	O
,	O
call	O
,	O
director	O
23	O
employee	O
,	O
worker	O
,	O
company	O
,	O
restaurant	O
,	O
food	O
,	O
store	O
,	O
work	O
,	O
customer	O
,	O
business	O
,	O
amazon	O
24	O
china	O
,	O
chinese	O
,	O
world	O
,	O
outbreak	O
,	O
virus	O
,	O
wuhan	O
,	O
organization	O
,	O
coronavirus	O
,	O
global	O
,	O
government	O
25	O
time	O
,	O
series	O
,	O
film	O
,	O
show	O
,	O
year	O
,	O
make	O
,	O
movie	O
,	O
live	O
,	O
race	O
,	O
set	O
27	O
year	O
,	O
company	O
,	O
market	O
,	O
stock	O
,	O
price	O
,	O
drop	O
,	O
month	O
,	O
business	O
,	O
global	O
,	O
sale	O
28	O
state	O
,	O
coronavirus	O
,	O
cuomo	O
,	O
florida	O
,	O
texas	O
,	O
york	O
,	O
governor	O
,	O
tuesday	O
,	O
week	O
,	O
monday	O
29	O
house	O
,	O
coronavirus	O
,	O
republican	O
,	O
member	O
,	O
bill	O
,	O
senate	O
,	O
democrat	O
,	O
wednesday	O
,	O
washington	O
,	O
thursday	O
30	O
country	O
,	O
lockdown	O
,	O
government	O
,	O
coronavirus	O
,	O
measure	O
,	O
people	O
,	O
italy	O
,	O
restriction	O
,	O
travel	O
,	O
border	O
31	O
claim	O
,	O
court	O
,	O
judge	O
,	O
law	O
,	O
federal	O
,	O
district	O
,	O
rule	O
,	O
chicago	O
,	O
legal	O
,	O
decision	O
32	O
health	O
,	O
public	O
,	O
people	O
,	O
work	O
,	O
community	O
,	O
include	O
,	O
protect	O
,	O
provide	O
,	O
group	O
,	O
pandemic	O
33	O
hospital	O
,	O
care	O
,	O
health	O
,	O
patient	O
,	O
medical	O
,	O
covid	O
,	O
center	O
,	O
facility	O
,	O
home	O
,	O
doctor	O
34	O
program	O
,	O
pay	O
,	O
money	O
,	O
fund	O
,	O
economic	O
,	O
job	O
,	O
business	O
,	O
relief	O
,	O
federal	O
,	O
receive	O
38	O
coronavirus	O
,	O
office	O
,	O
letter	O
,	O
pandemic	O
,	O
call	O
,	O
send	O
,	O
statement	O
,	O
issue	O
,	O
write	O
,	O
act	O
his	O
daily	O
press	O
briefings	O
because	O
journalists	O
use	O
them	O
to	O
grandstand	O
and	O
to	O
score	O
political	O
points	O
,	O
rather	O
than	O
to	O
pursue	O
information	O
.	O
The	O
contrast	O
with	O
press	O
briefings	O
for	O
governors	O
and	O
mayors	O
is	O
stark	O
:	O
there	O
,	O
journalists	O
tend	O
to	O
be	O
more	O
deferential	O
and	O
to	O
ask	O
questions	O
aimed	O
at	O
eliciting	O
information	O
rather	O
than	O
assigning	O
political	O
fault	O
.	O
New	O
York	O
Times	O
:	O
WASHINGTON	O
-	O
After	O
several	O
days	O
spent	O
weathering	O
attacks	O
from	O
White	O
House	O
officials	O
,	O
Dr.	O
Anthony	O
S.	O
Fauci	O
hit	O
back	O
on	O
Wednesday	O
,	O
calling	O
recent	O
efforts	O
to	O
discredit	O
him	O
"	O
bizarre	O
"	O
and	O
a	O
hindrance	O
to	O
the	O
government	O
's	O
ability	O
to	O
communicate	O
information	O
about	O
the	O
coronavirus	O
pandemic	O
.	O
On	O
Wednesday	O
,	O
Peter	O
Navarro	O
,	O
Mr.	O
Trump	O
's	O
top	O
trade	O
adviser	O
,	O
published	O
a	O
brazen	O
op	O
-	O
ed	O
article	O
in	O
USA	O
Today	O
describing	O
Dr.	O
Fauci	O
as	O
"	O
wrong	O
about	O
everything	O
.	O
"	O
All	O
the	O
while	O
,	O
White	O
House	O
officials	O
-	O
including	O
the	O
president	O
and	O
the	O
press	O
secretary	O
-	O
assert	O
in	O
the	O
face	O
of	O
the	O
evidence	O
that	O
there	O
is	O
no	O
concerted	O
effort	O
to	O
attack	O
Dr.	O
Fauci	O
.	O
given	O
"	O
opposition	O
research	O
"	O
to	O
discredit	O
Fauci	O
,	O
including	O
his	O
past	O
remarks	O
early	O
on	O
in	O
the	O
pandemic	O
that	O
the	O
public	O
did	O
n't	O
need	O
to	O
wear	O
masks	O
.	O
"	O
We	O
were	O
asked	O
a	O
very	O
specific	O
question	O
by	O
the	O
Washington	O
Post	O
,	O
and	O
that	O
question	O
was	O
President	O
Trump	O
noted	O
that	O
Dr.	O
Fauci	O
had	O
made	O
some	O
mistakes	O
,	O
and	O
we	O
provided	O
a	O
direct	O
answer	O
to	O
what	O
was	O
a	O
direct	O
question	O
.	O
"	O

We	O
show	O
the	O
topic	O
-	O
10	O
most	O
relevant	O
document	O
indices	O
on	O
all	O
30	O
topics	O
on	O
each	O
source	O
.	O
On	O
some	O
topics	O
there	O
are	O
less	O
than	O
10	O
relevant	O
documents	O
on	O
some	O
sources	O
.	O
Note	O
that	O
such	O
topics	O
are	O
not	O
in	O
the	O
10	O
labeled	O
topics	O
and	O
are	O
only	O
used	O
for	O
qualitative	O
analysis	O
;	O
in	O
other	O
words	O
,	O
for	O
quantitative	O
analysis	O
,	O
we	O
ensure	O
that	O
on	O
all	O
the	O
10	O
labeled	O
topics	O
,	O
there	O
are	O
10	O
relevant	O
documents	O
on	O
each	O
source	O
.	O
Topic	O
1	O
.	O
CNN	O
:	O
22873	O
,	O
62724	O
,	O
62635	O
,	O
63979	O
,	O

The	O
famous	O
"	O
laurel	O
/	O
yanny	O
"	O
phenomenon	O
references	O
an	O
audio	O
clip	O
that	O
elicits	O
dramatically	O
different	O
responses	O
from	O
different	O
listeners	O
.	O
For	O
the	O
original	O
clip	O
,	O
roughly	O
half	O
the	O
population	O
hears	O
the	O
word	O
"	O
laurel	O
,	O
"	O
while	O
the	O
other	O
half	O
hears	O
"	O
yanny	O
.	O
"	O
How	O
common	O
are	O
such	O
"	O
polyperceivable	O
"	O
audio	O
clips	O
?	O
In	O
this	O
paper	O
we	O
apply	O
ML	O
techniques	O
to	O
study	O
the	O
prevalence	O
of	O
polyperceivability	O
in	O
spoken	O
language	O
.	O
We	O
devise	O
a	O
metric	O
that	O
correlates	O
with	O
polyperceivability	O
of	O
audio	O
clips	O
,	O
use	O
it	O
to	O
efficiently	O
find	O
new	O
"	O
laurel	O
/	O
yanny	O
"	O
-	O
type	O
examples	O
,	O
and	O
validate	O
these	O
results	O
with	O
human	O
experiments	O
.	O
Our	O
results	O
suggest	O
that	O
polyperceivable	O
examples	O
are	O
surprisingly	O
prevalent	O
,	O
existing	O
for	O
>	O
2	O
%	O
of	O
English	O
words	O
.	O
1	O

To	O
investigate	O
polyperceivability	O
in	O
everyday	O
auditory	O
input	O
,	O
we	O
searched	O
for	O
audio	O
clips	O
of	O
single	O
spoken	O
words	O
that	O
exhibit	O
the	O
desired	O
effect	O
.	O
Our	O
method	O
consisted	O
of	O
two	O
phases	O
:	O
(	O
1	O
)	O
sample	O
a	O
large	O
number	O
of	O
audio	O
clips	O
that	O
are	O
likely	O
to	O
be	O
polyperceivable	O
,	O
and	O
(	O
2	O
)	O
collect	O
human	O
perception	O
data	O
on	O
those	O
clips	O
using	O
Amazon	O
Mechanical	O
Turk	O
to	O
identify	O
perceptual	O
modes	O
and	O
confirm	O
polyperceivability	O
.	O

Each	O
Mechanical	O
Turk	O
worker	O
was	O
randomly	O
assigned	O
25	O
clips	O
from	O
our	O
importance	O
-	O
sampled	O
set	O
of	O
200	O
.	O
Each	O
clip	O
was	O
slowed	O
to	O
either	O
0.9x	O
,	O
0.75x	O
,	O
or	O
0.6x	O
the	O
original	O
rate	O
.	O
Workers	O
responded	O
with	O
a	O
perceived	O
word	O
and	O
a	O
confidence	O
score	O
for	O
each	O
clip	O
.	O
We	O
collected	O
responses	O
from	O
574	O
workers	O
,	O
all	O
of	O
whom	O
self	O
-	O
identified	O
as	O
US	O
-	O
based	O
native	O
English	O
speakers	O
.	O
This	O
yielded	O
14	O
,	O
370	O
responses	O
(	O
≈	O
72	O
responses	O
per	O
clip	O
)	O
.	O
Next	O
,	O
we	O
manually	O
reviewed	O
these	O
responses	O
and	O
selected	O
the	O
most	O
promising	O
clips	O
for	O
a	O
second	O
round	O
with	O
only	O
11	O
of	O
the	O
200	O
clips	O
.	O
Note	O
that	O
because	O
these	O
selections	O
were	O
made	O
by	O
manual	O
review	O
(	O
i.e.	O
listening	O
to	O
clips	O
ourselves	O
)	O
,	O
there	O
is	O
a	O
chance	O
we	O
passed	O
over	O
some	O
polyperceivable	O
clips	O
-	O
this	O
means	O
that	O
our	O
computations	O
in	O
Section	O
3	O
are	O
only	O
a	O
conservative	O
lower	O
bound	O
.	O
For	O
this	O
round	O
,	O
we	O
also	O
included	O
clips	O
of	O
the	O
5	O
words	O
identified	O
by	O
Guan	O
and	O
Valiant	O
(	O
2019	O
)	O
,	O
12	O
potentially	O
-	O
polyperceivable	O
words	O
we	O
had	O
found	O
in	O
earlier	O
experiments	O
,	O
and	O
"	O
laurel	O
"	O
as	O
controls	O
.	O
We	O
collected	O
an	O
additional	O
3	O
,	O
950	O
responses	O
among	O
these	O
29	O
clips	O
(	O
≈	O
136	O
responses	O
per	O
clip	O
)	O
to	O
validate	O
that	O
they	O
were	O
indeed	O
polyperceivable	O
.	O
Finally	O
,	O
we	O
took	O
the	O
words	O
associated	O
with	O
these	O
29	O
clips	O
and	O
produced	O
a	O
new	O
set	O
of	O
clips	O
using	O
each	O
of	O
the	O
16	O
voices	O
,	O
for	O
a	O
total	O
of	O
464	O
clips	O
.	O
We	O
collected	O
4	O
,	O
125	O
responses	O
for	O
this	O
last	O
set	O
(	O
≈	O
3	O
responses	O
for	O
each	O
word	O
/	O
voice	O
/	O
rate	O
combination	O
)	O
.	O

An	O
enormous	O
body	O
of	O
work	O
from	O
cognitive	O
sciences	O
communities	O
explores	O
the	O
quirks	O
of	O
human	O
/	O
animal	O
sensory	O
systems	O
(	O
Fahle	O
et	O
al	O
,	O
2002	O
)	O
.	O
These	O
works	O
often	O
have	O
the	O
explicit	O
goal	O
of	O
exploring	O
isolated	O
"	O
illusions	O
"	O
that	O
provide	O
insights	O
into	O
our	O
perceptual	O
systems	O
(	O
Davis	O
and	O
Johnsrude	O
,	O
2007	O
;	O
Fritz	O
et	O
al	O
,	O
2005	O
)	O
.	O
However	O
,	O
there	O
are	O
few	O
efforts	O
to	O
quantify	O
the	O
extent	O
to	O
which	O
"	O
typical	O
"	O
instances	O
are	O
polyperceivable	O
or	O
lie	O
close	O
to	O
decision	O
boundaries	O
.	O
Miller	O
(	O
1981	O
)	O
studies	O
the	O
effect	O
of	O
speaking	O
rate	O
on	O
how	O
listeners	O
perceive	O
phonemes	O
.	O
The	O
perceptual	O
shifts	O
studied	O
therein	O
are	O
between	O
phonetically	O
adjacent	O
perceptions	O
(	O
e.g.	O
"	O
pip	O
"	O
vs.	O
"	O
peep	O
"	O
)	O
rather	O
than	O
dramatically	O
different	O
perceptions	O
(	O
e.g.	O
"	O
laurel	O
"	O
vs.	O
"	O
yanny	O
"	O
)	O
.	O
The	O
"	O
perturbation	O
"	O
of	O
increasing	O
human	O
speaking	O
rate	O
is	O
much	O
more	O
complex	O
than	O
simply	O
linearly	O
scaling	O
the	O
playback	O
rate	O
of	O
an	O
audio	O
clip	O
.	O
Speaking	O
-	O
rate	O
induced	O
shifts	O
also	O
seem	O
to	O
hold	O
more	O
universally	O
across	O
voices	O
,	O
as	O
opposed	O
to	O
the	O
polyperceivable	O
instances	O
we	O
examine	O
.	O

We	O
would	O
like	O
to	O
thank	O
Melody	O
Guan	O
for	O
early	O
discussions	O
on	O
this	O
project	O
,	O
and	O
the	O
anonymous	O
reviewers	O
for	O
their	O
thoughtful	O
suggestions	O
.	O
This	O
research	O
was	O
supported	O
by	O
a	O
seed	O
grant	O
from	O
Stanford	O
's	O
HAI	O
Institute	O
,	O
NSF	O
award	O
AF	O
-	O
1813049	O
and	O
ONR	O
Young	O
Investigator	O
Award	O
N00014	O
-	O
18	O
-	O
1	O
-	O
2295	O
.	O

SPDB	O
Innovation	O
Lab	O
at	O
SemEval	O
-	O
2022	O
Task	O
3	O
:	O
Recognize	O
Appropriate	O
Taxonomic	O
Relations	O
Between	O
Two	O
Nominal	O
Arguments	O
with	O
ERNIE	O
-	O
M	O
Model	O

Synonyms	O
and	O
antonym	O
practices	O
are	O
the	O
most	O
common	O
practices	O
in	O
our	O
early	O
childhood	O
.	O
It	O
correlated	O
our	O
known	O
words	O
to	O
a	O
better	O
place	O
deep	O
in	O
our	O
intuition	O
.	O
At	O
the	O
beginning	O
of	O
life	O
for	O
a	O
machine	O
,	O
we	O
would	O
like	O
to	O
treat	O
the	O
machine	O
as	O
a	O
baby	O
and	O
build	O
a	O
similar	O
training	O
for	O
it	O
as	O
well	O
to	O
present	O
a	O
qualified	O
performance	O
.	O
In	O
this	O
paper	O
,	O
we	O
present	O
an	O
ensemble	O
model	O
for	O
sentence	O
logistics	O
classification	O
,	O
which	O
outperforms	O
the	O
state	O
-	O
of	O
-	O
art	O
methods	O
.	O
Our	O
approach	O
essentially	O
builds	O
on	O
two	O
models	O
including	O
ERNIE	O
-	O
M	O
and	O
DeBERTaV3	O
.	O
With	O
crossvalidation	O
and	O
random	O
seed	O
tuning	O
,	O
we	O
select	O
the	O
top	O
performance	O
models	O
for	O
the	O
last	O
soft	O
ensemble	O
and	O
make	O
them	O
vote	O
for	O
the	O
final	O
answer	O
,	O
achieving	O
the	O
top	O
6	O
performance	O
.	O

In	O
this	O
section	O
,	O
we	O
first	O
describe	O
the	O
dataset	O
and	O
our	O
data	O
preprocessing	O
steps	O
,	O
and	O
then	O
we	O
present	O
the	O
details	O
of	O
the	O
experimental	O
setup	O
for	O
subtask1	O
.	O

This	O
section	O
describes	O
our	O
problem	O
definition	O
,	O
codeswitching	O
algorithm	O
,	O
language	O
families	O
,	O
and	O
the	O
training	O
methodology	O
.	O

Given	O
a	O
source	O
(	O
S	O
)	O
and	O
a	O
set	O
of	O
target	O
(	O
T	O
)	O
languages	O
,	O
the	O
goal	O
is	O
to	O
train	O
a	O
classifier	O
using	O
data	O
only	O
in	O
the	O
source	O
language	O
and	O
predict	O
examples	O
from	O
the	O
completely	O
unseen	O
target	O
languages	O
.	O
We	O
assume	O
the	O
target	O
language	O
is	O
unknown	O
during	O
training	O
(	O
fine	O
-	O
tuning	O
)	O
time	O
,	O
which	O
makes	O
direct	O
translation	O
to	O
target	O
infeasible	O
.	O
In	O
this	O
context	O
,	O
we	O
use	O
code	O
-	O
switching	O
(	O
cs	O
)	O
to	O
augment	O
the	O
monolingual	O
source	O
data	O
.	O
Thus	O
,	O
the	O
input	O
,	O
augmented	O
input	O
,	O
and	O
output	O
of	O
our	O
problem	O
can	O
be	O
defined	O
as	O
:	O
lset	O
=	O
googletrans.languages	O
−	O
lT	O
for	O
i	O
1	O
..	O
k	O
do	O
for	O
j	O
1	O
..	O
len	O
(	O
X	O
en	O
ut	O
)	O
do	O
G	O
cs	O
,	O
L	O
cs	O
chunks	O
=	O
slot_chunks	O
(	O
X	O
en	O
ut	O
[	O
j	O
]	O
,	O
y	O
en	O
sl	O
[	O
j	O
]	O
)	O
for	O
c	O
chunks	O
do	O
l	O
random.choice	O
(	O
lset	O
)	O
t	O
translate	O
(	O
c	O
,	O
l	O
)	O
G	O
cs	O
G	O
cs	O
∪	O
t	O
L	O
cs	O
L	O
cs	O
∪	O
align_label	O
(	O
c	O
,	O
t	O
)	O
end	O
X	O
cs	O
ut	O
X	O
cs	O
ut	O
∪	O
G	O
cs	O
y	O
cs	O
y	O
cs	O
∪	O
y	O
cs	O
[	O
j	O
]	O
y	O
cs	O
sl	O
y	O
cs	O
sl	O
∪	O
L	O
cs	O
end	O
end	O
Input	O
:	O
X	O
S	O
ut	O
,	O
y	O
S	O
,	O
y	O
S	O
sl	O
,	O
l	O
T	O
Code	O
-	O
Switched	O
Input	O
:	O
X	O
cs	O
ut	O
,	O
y	O
cs	O
,	O
y	O
cs	O
sl	O
Output	O
:	O
y	O
T	O
,	O
y	O
T	O
sl	O
predict	O
(	O
X	O
T	O
ut	O
)	O
where	O
X	O
ut	O
represents	O
sentences	O
,	O
y	O
their	O
ground	O
truth	O
intent	O
classes	O
,	O
y	O
sl	O
the	O
slot	O
labels	O
for	O
the	O
words	O
in	O
those	O
sentences	O
,	O
and	O
l	O
T	O
the	O
set	O
of	O
target	O
languages	O
.	O
An	O
example	O
sentence	O
,	O
its	O
intent	O
class	O
,	O
and	O
slot	O
labels	O
are	O
shown	O
in	O
Figure	O
2	O
.	O

A	O
language	O
family	O
is	O
defined	O
as	O
a	O
group	O
of	O
related	O
languages	O
that	O
likely	O
share	O
a	O
common	O
ancestor	O
.	O
For	O
example	O
,	O
Portuguese	O
,	O
Spanish	O
,	O
French	O
,	O
Italian	O
,	O
and	O
Romanian	O
are	O
all	O
derived	O
from	O
Latin	O
(	O
Rowe	O
and	O
Levine	O
,	O
2017	O
)	O
.	O
We	O
use	O
language	O
families	O
to	O
study	O
their	O
impact	O
on	O
the	O
target	O
languages	O
.	O
We	O
augment	O
the	O
source	O
language	O
with	O
code	O
-	O
switching	O
to	O
a	O
particular	O
language	O
family	O
.	O
For	O
instance	O
,	O
codeswitching	O
the	O
English	O
dataset	O
with	O
Turkic	O
language	O
family	O
and	O
testing	O
on	O
Japanese	O
can	O
reveal	O
how	O
closely	O
the	O
two	O
are	O
aligned	O
in	O
the	O
vector	O
space	O
of	O
a	O
pre	O
-	O
trained	O
multilingual	O
model	O
.	O
We	O
work	O
with	O
6	O
language	O
groups	O
:	O
Afro	O
-	O
Asiatic	O
(	O
Voegelin	O
and	O
Voegelin	O
,	O
1976	O
)	O
,	O
Germanic	O
(	O
Harbert	O
,	O
2006	O
)	O
,	O
Indo	O
-	O
Aryan	O
(	O
Masica	O
,	O
1993	O
)	O
,	O
Romance	O
(	O
Elcock	O
and	O
Green	O
,	O
1960	O
)	O
,	O
and	O
Turkic	O
(	O
Johanson	O
and	O
Johanson	O
,	O
2015	O
)	O
,	O
also	O
grouping	O
Sino	O
-	O
Tibetan	O
,	O
Koreanic	O
and	O
Japonic	O
(	O
Shafer	O
,	O
1955	O
;	O
Miller	O
,	O
1967	O
)	O
.	O
2	O
Germanic	O
,	O
Romance	O
,	O
and	O
Indo	O
-	O
Aryan	O
are	O
genera	O
of	O
the	O
Indo	O
-	O
European	O
family	O
.	O
Language	O
groups	O
and	O
corresponding	O
languages	O
are	O
shown	O
in	O
Table	O
1	O
.	O
Each	O
group	O
is	O
selected	O
based	O
on	O
a	O
target	O
language	O
in	O
the	O
dataset	O
,	O
and	O
the	O
Afro	O
-	O
Asiatic	O
family	O
is	O
added	O
as	O
an	O
extra	O
group	O
.	O
In	O
experiments	O
,	O
lset	O
in	O
Algorithm	O
1	O
will	O
be	O
assigned	O
languages	O
from	O
a	O
specific	O
family	O
.	O

The	O
tweet	O
dataset	O
that	O
we	O
constructed	O
for	O
disaster	O
NLU	O
was	O
originally	O
released	O
by	O
Appen	O
6	O
,	O
and	O
we	O
use	O
it	O
to	O
construct	O
slot	O
labels	O
in	O
two	O
languages	O
:	O
English	O
(	O
en	O
)	O
and	O
Haitian	O
Creole	O
(	O
ht	O
)	O
.	O
Data	O
statement	O
that	O
includes	O
annotator	O
guidelines	O
for	O
the	O
labeling	O
jobs	O
and	O
other	O
dataset	O
information	O
will	O
be	O
provided	O
with	O
the	O
implementation	O
.	O
From	O
a	O
broader	O
impact	O
perspective	O
,	O
our	O
code	O
and	O
developed	O
models	O
are	O
open	O
-	O
source	O
and	O
allows	O
NLP	O
technology	O
to	O
be	O
accessible	O
to	O
information	O
systems	O
for	O
emergency	O
services	O
and	O
social	O
scientists	O
in	O
quickly	O
deploying	O
model	O
during	O
disaster	O
events	O
.	O

This	O
work	O
was	O
partially	O
supported	O
by	O
U.S.	O
National	O
Science	O
Foundation	O
grants	O
IIS	O
-	O
1815459	O
,	O
IIS	O
-	O
1657379	O
,	O
and	O
2040926	O
.	O
This	O
work	O
was	O
also	O
supported	O
in	O
part	O
by	O
the	O
grant	O
H	O
-	O
4Q21	O
-	O
009	O
from	O
the	O
Commonwealth	O
Cyber	O
Initiative	O
,	O
an	O
investment	O
in	O
the	O
advancement	O
of	O
cyber	O
R&D	O
,	O
innovation	O
,	O
and	O
workforce	O
development	O
(	O
for	O
more	O
information	O
about	O
CCI	O
,	O
visit	O
www.cyberinitiative	O
.	O
org	O
)	O
.	O
The	O
authors	O
are	O
thankful	O
to	O
Ming	O
Sun	O
and	O
Alexis	O
Conneau	O
for	O
giving	O
valuable	O
insights	O
on	O
multilingual	O
model	O
training	O
,	O
as	O
well	O
as	O
to	O
the	O
anonymous	O
reviewers	O
for	O
their	O
constructive	O
feedback	O
.	O
We	O
also	O
acknowledge	O
ARGO	O
,	O
a	O
research	O
computing	O
cluster	O
provided	O
by	O
the	O
Office	O
of	O
Research	O
Computing	O
at	O
George	O
Mason	O
University	O
,	O
were	O
most	O
experiments	O
were	O
conducted	O
.	O

The	O
Agriculture	O
ministers	O
of	O
El	O
Salvador	O
and	O
Honduras	O
...	O
to	O
control	O
the	O
spread	O
of	O
disease	O
affecting	O
poultry	O
,	O
like	O
the	O
virus	O
Newcastle	O
[	O
Disease	O
]	O
.	O
Urrutia	O
...	O
to	O
study	O
the	O
Newcastle	O
outbreak	O
.	O
The	O
disease	O
has	O
killed	O
close	O
to	O
half	O
a	O
million	O
Honduran	O
chickens	O
[	O
Victims	O
]	O
in	O
recent	O
weeks	O
.	O
Honduras	O
[	O
Country	O
]	O
said	O
this	O
week	O
it	O
would	O
halt	O
the	O
importation	O
of	O
chickens	O
and	O
eggs	O
from	O
Guatemala	O
[	O
Country	O
]	O
,	O
where	O
the	O
disease	O
has	O
been	O
detected	O
earlier	O
,	O
and	O
.....	O

Similar	O
to	O
the	O
work	O
of	O
Kummerfeld	O
and	O
Klein	O
(	O
2013	O
)	O
,	O
our	O
error	O
analysis	O
approach	O
is	O
systemagnostic	O
,	O
i.e.	O
it	O
only	O
uses	O
system	O
output	O
and	O
does	O
not	O
consider	O
intermediate	O
system	O
decisions	O
.	O
This	O
allows	O
for	O
error	O
analysis	O
and	O
comparison	O
across	O
different	O
kinds	O
of	O
systems	O
-	O
end	O
-	O
to	O
-	O
end	O
or	O
pipeline	O
;	O
neural	O
or	O
pattern	O
-	O
based	O
.	O
Given	O
inputs	O
consisting	O
of	O
the	O
system	O
-	O
predicted	O
templates	O
and	O
gold	O
standard	O
templates	O
(	O
i.e.	O
desired	O
output	O
)	O
for	O
every	O
document	O
in	O
the	O
target	O
dataset	O
,	O
our	O
error	O
analysis	O
tool	O
operates	O
in	O
three	O
steps	O
.	O
For	O
each	O
document	O
,	O
1	O
.	O
Perform	O
an	O
optimized	O
mapping	O
of	O
the	O
associated	O
predicted	O
templates	O
and	O
gold	O
templates	O
.	O
2	O
.	O
Apply	O
a	O
pre	O
-	O
defined	O
set	O
of	O
transformations	O
to	O
convert	O
each	O
system	O
-	O
predicted	O
template	O
into	O
the	O
desired	O
gold	O
template	O
,	O
keeping	O
track	O
of	O
the	O
transformations	O
applied	O
.	O
3	O
.	O
Map	O
the	O
changes	O
made	O
in	O
the	O
conversion	O
process	O
to	O
an	O
IE	O
-	O
based	O
set	O
of	O
error	O
types	O
.	O
We	O
describe	O
each	O
step	O
in	O
detail	O
in	O
the	O
subsections	O
below	O
.	O

The	O
transformations	O
in	O
Section	O
4.2	O
are	O
mapped	O
onto	O
a	O
set	O
of	O
IE	O
-	O
specific	O
error	O
types	O
as	O
shown	O
in	O
Figure	O
3	O
.	O
In	O
some	O
cases	O
,	O
a	O
single	O
transformation	O
maps	O
onto	O
a	O
single	O
error	O
,	O
while	O
in	O
others	O
a	O
sequence	O
of	O
transformations	O
is	O
associated	O
with	O
a	O
single	O
error	O
.	O
Full	O
details	O
are	O
in	O
Appendix	O
A.	O

As	O
new	O
models	O
for	O
information	O
extraction	O
continue	O
to	O
be	O
developed	O
,	O
we	O
find	O
that	O
their	O
predicted	O
error	O
types	O
contain	O
insights	O
regarding	O
their	O
shortcomings	O
.	O
Analyzing	O
error	O
patterns	O
within	O
model	O
predictions	O
in	O
a	O
more	O
fine	O
-	O
grained	O
manner	O
beyond	O
scores	O
provided	O
by	O
commonly	O
used	O
metrics	O
is	O
important	O
for	O
the	O
progress	O
of	O
the	O
field	O
.	O
We	O
introduce	O
a	O
framework	O
for	O
the	O
automatic	O
categorization	O
of	O
model	O
prediction	O
errors	O
for	O
document	O
-	O
level	O
IE	O
tasks	O
.	O
We	O
used	O
the	O
tool	O
to	O
analyze	O
the	O
errors	O
of	O
two	O
state	O
-	O
of	O
-	O
theart	O
models	O
on	O
three	O
datasets	O
from	O
varying	O
domains	O
and	O
compared	O
the	O
error	O
profiles	O
of	O
these	O
models	O
to	O
four	O
of	O
the	O
earliest	O
systems	O
in	O
the	O
field	O
on	O
a	O
dataset	O
from	O
that	O
era	O
.	O
We	O
find	O
that	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
,	O
when	O
compared	O
to	O
the	O
earlier	O
manual	O
feature	O
-	O
based	O
models	O
,	O
perform	O
better	O
at	O
span	O
extraction	O
but	O
worse	O
at	O
template	O
detection	O
and	O
role	O
assignment	O
.	O
With	O
a	O
better	O
balance	O
between	O
precision	O
and	O
recall	O
,	O
the	O
best	O
early	O
model	O
outperforms	O
the	O
relatively	O
highprecision	O
,	O
low	O
-	O
recall	O
modern	O
models	O
.	O
Missing	O
role	O
fillers	O
remain	O
the	O
main	O
source	O
of	O
errors	O
,	O
and	O
scientific	O
corpora	O
are	O
the	O
most	O
difficult	O
for	O
all	O
systems	O
,	O
suggesting	O
that	O
improvements	O
in	O
these	O
areas	O
should	O
be	O
a	O
priority	O
for	O
future	O
system	O
development	O
.	O

We	O
also	O
provide	O
example	O
error	O
types	O
with	O
the	O
ProMED	O
dataset	O
.	O

We	O
did	O
not	O
run	O
the	O
DyGIE++	O
model	O
on	O
the	O
MUC	O
-	O
4	O
dataset	O
as	O
the	O
model	O
output	O
was	O
made	O
available	O
to	O
us	O
by	O
Xinya	O
Du	O
.	O

All	O
code	O
is	O
made	O
publicly	O
available	O
.	O
13	O
Exhaustive	O
reproducibility	O
details	O
,	O
including	O
how	O
to	O
access	O
all	O
datasets	O
,	O
are	O
provided	O
in	O
Appendix	O
B.	O
We	O
fully	O
adhere	O
to	O
the	O
EMNLP	O
2020	O
Reproducibility	O
guidelines	O
,	O
addressing	O
all	O
relevant	O
checklist	O
items	O
.	O

For	O
compression	O
,	O
we	O
found	O
sentence	O
-	O
level	O
compression	O
to	O
be	O
a	O
naturally	O
motivated	O
metric	O
given	O
that	O
many	O
extractive	O
systems	O
are	O
constrained	O
to	O
extract	O
sentence	O
-	O
length	O
sequence	O
.	O
We	O
also	O
considered	O
byte	O
-	O
level	O
compression	O
as	O
an	O
alternative	O
to	O
word	O
-	O
level	O
compression	O
(	O
as	O
computational	O
length	O
constraints	O
have	O
sometimes	O
been	O
used	O
in	O
evaluation	O
instead	O
of	O
word	O
length	O
constraints	O
)	O
.	O
We	O
found	O
the	O
results	O
to	O
be	O
highly	O
correlated	O
with	O
word	O
-	O
level	O
compression	O
and	O
to	O
not	O
be	O
further	O
revealing	O
(	O
and	O
bytes	O
may	O
be	O
inherently	O
less	O
interpretable	O
for	O
NLP	O
when	O
compared	O
with	O
words	O
)	O
.	O
We	O
also	O
considered	O
only	O
considering	O
content	O
words	O
,	O
motivated	O
by	O
literature	O
in	O
topic	O
modelling	O
(	O
Schofield	O
et	O
al	O
,	O
2017	O
)	O
that	O
has	O
considered	O
removing	O
stopwords	O
and	O
other	O
such	O
lexical	O
categories	O
.	O
These	O
results	O
were	O
also	O
highly	O
correlated	O
with	O
the	O
original	O
word	O
-	O
level	O
compression	O
results	O
and	O
we	O
did	O
not	O
find	O
any	O
discerning	O
trends	O
in	O
looking	O
at	O
individual	O
examples	O
.	O

Our	O
general	O
framework	O
for	O
quantifying	O
abstractivity	O
is	O
derived	O
from	O
Grusky	O
et	O
al	O
(	O
2018	O
)	O
.	O
We	O
considered	O
p	O
{	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
}	O
initially	O
and	O
found	O
p	O
=	O
1	O
to	O
be	O
the	O
most	O
informative	O
regarding	O
abstractivity	O
.	O
In	O
particular	O
,	O
we	O
find	O
that	O
for	O
increasing	O
p	O
,	O
useful	O
conclusions	O
about	O
abstractivity	O
are	O
inherently	O
masked	O
by	O
the	O
dominance	O
of	O
the	O
|	O
S	O
i	O
|	O
p	O
denominator	O
in	O
the	O
definition	O
.	O
We	O
report	O
the	O
scores	O
for	O
ABS	O
2	O
in	O
Table	O
6	O
.	O
We	O
also	O
considered	O
the	O
natural	O
extensions	O
to	O
ABS	O
3	O
and	O
ABS	O
4	O
but	O
we	O
found	O
that	O
the	O
normalization	O
dominates	O
any	O
deviation	O
in	O
the	O
scores	O
and	O
all	O
datasets	O
essentially	O
receive	O
a	O
score	O
of	O
1	O
.	O
We	O
also	O
considered	O
other	O
forms	O
of	O
normalization	O
(	O
i.e.	O
normalizing	O
ABS	O
2	O
in	O
the	O
style	O
of	O
the	O
L	O
2	O
norm	O
/	O
the	O
style	O
of	O
generalized	O
p	O
-	O
norms	O
)	O
in	O
initial	O
experiments	O
but	O
found	O
no	O
substantial	O
differences	O
.	O

We	O
provide	O
precise	O
and	O
comprehensive	O
details	O
discussing	O
all	O
data	O
,	O
preprocessing	O
and	O
modelling	O
decisions	O
.	O
All	O
code	O
will	O
be	O
made	O
publicly	O
available	O
as	O
noted	O
in	O
the	O
main	O
paper	O
.	O

Fragments	O
(	O
Grusky	O
et	O
al	O
,	O
2018	O
)	O
were	O
computed	O
using	O
the	O
scripts	O
released	O
in	O
that	O
work	O
for	O
the	O
purposes	O
of	O
estimating	O
abstractivity	O
.	O
In	O
the	O
case	O
of	O
the	O
NWS	O
dataset	O
,	O
the	O
authors	O
already	O
provide	O
fragment	O
-	O
related	O
scores	O
which	O
we	O
use	O
without	O
recomputing	O
these	O
values	O
.	O

In	O
the	O
main	O
paper	O
,	O
we	O
briefly	O
discuss	O
how	O
we	O
discovered	O
that	O
several	O
of	O
our	O
metrics	O
can	O
serve	O
the	O
dual	O
purpose	O
of	O
detecting	O
generally	O
low	O
quality	O
examples	O
for	O
example	O
that	O
achieve	O
extreme	O
scores	O
.	O
Figures	O
1	O
through	O
9	O
are	O
several	O
examples	O
we	O
found	O
to	O
be	O
representative	O
of	O
the	O
general	O
structure	O
of	O
low	O
quality	O
examples	O
for	O
a	O
given	O
metric	O
.	O
In	O
some	O
cases	O
,	O
the	O
trends	O
are	O
highly	O
dataset	O
-	O
specific	O
whereas	O
in	O
others	O
they	O
are	O
more	O
general	O
.	O
To	O
facilitate	O
reproducibility	O
efforts	O
,	O
we	O
provide	O
all	O
examples	O
IDs	O
we	O
studied	O
for	O
each	O
(	O
dataset	O
,	O
metric	O
)	O
in	O
Table	O
8	O
.	O
Original	O
Text	O
(	O
truncated	O
)	O
:	O
Let	O
us	O
,	O
in	O
the	O
beginning	O
,	O
give	O
a	O
word	O
of	O
cordial	O
praise	O
to	O
the	O
American	O
publishers	O
of	O
these	O
splendid	O
volumes	O
.	O
The	O
undertaking	O
,	O
in	O
the	O
first	O
place	O
,	O
was	O
an	O
intellectual	O
compliment	O
to	O
the	O
country	O
.	O
It	O
was	O
based	O
on	O
the	O
faith	O
that	O
there	O
is	O
in	O
this	O
country	O
enough	O
of	O
philosophy	O
and	O
scholarship	O
to	O
justify	O
a	O
new	O
and	O
complete	O
edition	O
of	O
.	O
.	O
.	O
Summary	O
:	O
Let	O
us	O
,	O
in	O
the	O
beginning	O
,	O
give	O
a	O
word	O
of	O
cordial	O
praise	O
to	O
the	O
American	O
publishers	O
of	O
these	O
splendid	O
volumes	O
.	O
The	O
undertaking	O
,	O
in	O
the	O
first	O
place	O
,	O
was	O
an	O
intellectual	O
compliment	O
to	O
the	O
country	O
.	O

Figure	O
1	O
:	O
Dataset	O
:	O
NWS	O
.	O
This	O
summary	O
simply	O
is	O
the	O
lede	O
and	O
we	O
do	O
not	O
find	O
it	O
to	O
be	O
a	O
useful	O
summary	O
for	O
readers	O
not	O
familiar	O
with	O
the	O
full	O
context	O
of	O
the	O
article	O
.	O
We	O
hypothesize	O
that	O
such	O
a	O
summary	O
may	O
have	O
been	O
useful	O
for	O
members	O
of	O
a	O
newsroom	O
communicating	O
information	O
about	O
the	O
article	O
to	O
the	O
other	O
(	O
given	O
their	O
intimate	O
familiarity	O
with	O
the	O
article	O
)	O
but	O
this	O
likely	O
is	O
inappropriate	O
as	O
a	O
summary	O
in	O
most	O
settings	O
.	O

The	O
entropy	O
of	O
a	O
random	O
variable	O
X	O
is	O
defined	O
as	O
:	O
H	O
(	O
X	O
)	O
−	O
x	O
p	O
(	O
x	O
)	O
log	O
2	O
p	O
(	O
x	O
)	O
Original	O
Text	O
(	O
truncated	O
)	O
:	O
A	O
FULL	O
-	O
SERVICE	O
hotel	O
and	O
conference	O
center	O
is	O
to	O
go	O
up	O
in	O
the	O
Lafayette	O
Yard	O
area	O
of	O
Trenton	O
,	O
giving	O
the	O
city	O
a	O
hotel	O
for	O
the	O
first	O
time	O
since	O
the	O
1980	O
's	O
and	O
bringing	O
to	O
an	O
end	O
its	O
unenviable	O
distinction	O
as	O
the	O
only	O
state	O
capital	O
without	O
lodging	O
for	O
visitors	O
.	O
.	O
.	O

Detector	O
:	O
Extremely	O
Low	O
Abstraction	O
Figure	O
2	O
:	O
Dataset	O
:	O
NYT	O
.	O
This	O
summary	O
simply	O
conveys	O
no	O
useful	O
information	O
to	O
someone	O
who	O
has	O
not	O
also	O
read	O
the	O
reference	O
document	O
and	O
simply	O
is	O
a	O
word	O
copied	O
from	O
the	O
source	O
document	O
.	O
It	O
appears	O
to	O
be	O
a	O
label	O
rather	O
than	O
a	O
summary	O
.	O
Original	O
Text	O
(	O
truncated	O
)	O
:	O
a	O
lógicaé	O
o	O
estudo	O
dos	O
princípios	O
e	O
critéiros	O
de	O
inferências	O
e	O
demonstrações	O
válidas	O
.	O
um	O
sistema	O
lógicoé	O
composto	O
por	O
três	O
partes	O
:	O
a	O
sintaxe	O
(	O
ou	O
notação	O
)	O
,	O
.	O
.	O
.	O

:	O
logic	O
is	O
the	O
science	O
of	O
correct	O
inferences	O
and	O
a	O
logical	O
system	O
is	O
a	O
tool	O
to	O
prove	O
assertions	O
in	O
a	O
certain	O
logic	O
in	O
a	O
correct	O
way	O
.	O
.	O
.	O

Detector	O
:	O
Extremely	O
High	O
Abstraction	O
NYT	O
.	O
This	O
summary	O
is	O
unlikely	O
to	O
be	O
informative	O
to	O
someone	O
who	O
has	O
not	O
read	O
the	O
reference	O
document	O
and	O
is	O
more	O
of	O
a	O
categorization	O
/	O
label	O
than	O
a	O
summary	O
.	O
This	O
is	O
similar	O
to	O
the	O
previous	O
NYT	O
example	O
given	O
.	O
The	O
conditional	O
entropy	O
of	O
X	O
given	O
Y	O
is	O
defined	O
as	O
:	O
Original	O
Text	O
(	O
truncated	O
)	O
:	O
Brodie	O
(	O
the	O
dog	O
)	O
was	O
neglected	O
,	O
and	O
ended	O
up	O
with	O
serious	O
anger	O
and	O
health	O
issues	O
concerning	O
his	O
skin	O
and	O
allergies	O
.	O
My	O
boyfriend	O
adopted	O
him	O
.	O
.	O
.	O
Summary	O
:	O
Onions	O
.	O
H	O
(	O
X	O
|	O
Y	O
)	O
y	O
p	O
(	O

In	O
the	O
main	O
paper	O
,	O
we	O
report	O
the	O
average	O
score	O
for	O
each	O
metric	O
on	O
each	O
dataset	O
.	O
To	O
complement	O
reporting	O
the	O
mean	O
,	O
we	O
report	O
the	O
standard	O
deviation	O
for	O
each	O
metric	O
on	O
each	O
dataset	O
in	O
Table	O
9	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
the	O
onlinization	O
of	O
the	O
offline	O
model	O
and	O
propose	O
two	O
ways	O
to	O
control	O
the	O
quality	O
-	O
latency	O
trade	O
-	O
off	O
.	O

Depending	O
on	O
the	O
language	O
pair	O
,	O
translation	O
tasks	O
may	O
require	O
reordering	O
or	O
a	O
piece	O
of	O
information	O
that	O
might	O
not	O
be	O
apparent	O
until	O
the	O
source	O
utterance	O
ends	O
.	O
In	O
the	O
offline	O
setting	O
,	O
the	O
model	O
processes	O
the	O
whole	O
utterance	O
at	O
once	O
,	O
rendering	O
the	O
strategy	O
most	O
optimal	O
in	O
terms	O
of	O
quality	O
.	O
If	O
applied	O
in	O
online	O
mode	O
,	O
this	O
ultimately	O
leads	O
to	O
a	O
large	O
latency	O
.	O
One	O
approach	O
to	O
reducing	O
the	O
latency	O
is	O
to	O
break	O
the	O
source	O
utterance	O
into	O
chunks	O
and	O
perform	O
the	O
translation	O
on	O
each	O
chunk	O
.	O
In	O
this	O
paper	O
,	O
we	O
follow	O
the	O
incremental	O
decoding	O
framework	O
described	O
by	O
Liu	O
et	O
al	O
(	O
2020a	O
)	O
.	O
We	O
break	O
the	O
input	O
utterance	O
into	O
small	O
fixed	O
-	O
size	O
chunks	O
and	O
decode	O
each	O
time	O
after	O
we	O
receive	O
a	O
new	O
chunk	O
.	O
After	O
each	O
decoding	O
step	O
,	O
we	O
identify	O
a	O
stable	O
part	O
of	O
the	O
hypothesis	O
using	O
stable	O
hypothesis	O
detection	O
.	O
The	O
stable	O
part	O
is	O
sent	O
to	O
the	O
user	O
(	O
"	O
committed	O
"	O
in	O
the	O
following	O
)	O
and	O
is	O
no	O
longer	O
changed	O
afterward	O
(	O
i.e.	O
,	O
no	O
retranslation	O
)	O
.	O
2	O
Our	O
current	O
implementation	O
assumes	O
that	O
the	O
whole	O
speech	O
input	O
fits	O
into	O
memory	O
,	O
in	O
other	O
words	O
,	O
we	O
are	O
only	O
adding	O
new	O
chunks	O
as	O
they	O
are	O
arriving	O
.	O
This	O
simplification	O
is	O
possible	O
because	O
the	O
evaluation	O
of	O
the	O
shared	O
task	O
is	O
performed	O
on	O
segmented	O
input	O
,	O
on	O
individual	O
utterances	O
.	O
With	O
each	O
newly	O
arrived	O
input	O
chunk	O
,	O
the	O
decoding	O
starts	O
with	O
forced	O
decoding	O
of	O
the	O
already	O
committed	O
tokens	O
and	O
continues	O
with	O
beam	O
search	O
decoding	O
.	O

The	O
limited	O
context	O
of	O
the	O
early	O
chunks	O
might	O
result	O
in	O
an	O
unstable	O
hypothesis	O
and	O
an	O
emission	O
of	O
erroneous	O
tokens	O
.	O
The	O
autoregressive	O
nature	O
of	O
the	O
model	O
might	O
cause	O
further	O
performance	O
degradation	O
in	O
later	O
chunks	O
.	O
One	O
possible	O
solution	O
is	O
to	O
use	O
longer	O
chunks	O
,	O
but	O
it	O
inevitably	O
leads	O
to	O
a	O
higher	O
latency	O
throughout	O
the	O
whole	O
utterance	O
.	O
To	O
mitigate	O
this	O
issue	O
,	O
we	O
explore	O
a	O
lengthening	O
of	O
the	O
first	O
chunk	O
.	O
We	O
call	O
this	O
strategy	O
an	O
initial	O
wait	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
the	O
onlinization	O
experiments	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
the	O
experiments	O
and	O
discuss	O
the	O
results	O
.	O

We	O
experiment	O
with	O
chunk	O
sizes	O
of	O
250	O
ms	O
,	O
500	O
ms	O
,	O
1s	O
,	O
and	O
2	O
s.	O
We	O
combine	O
the	O
sizes	O
of	O
the	O
chunks	O
with	O
different	O
partial	O
hypothesis	O
selection	O
strategies	O
.	O
The	O
results	O
are	O
shown	O
in	O
Figure	O
1	O
.	O
The	O
results	O
document	O
that	O
the	O
chunk	O
size	O
parameter	O
has	O
a	O
stronger	O
influence	O
on	O
the	O
trade	O
-	O
off	O
than	O
different	O
prefix	O
strategies	O
.	O
Additionally	O
,	O
this	O
enables	O
constant	O
trade	O
-	O
off	O
strategies	O
(	O
e.g.	O
,	O
LA	O
-	O
2	O
)	O
to	O
become	O
flexible	O
.	O

We	O
experiment	O
with	O
three	O
strategies	O
:	O
hold	O
-	O
n	O
(	O
withholds	O
last	O
n	O
tokens	O
)	O
,	O
shared	O
prefix	O
(	O
SP	O
-	O
n	O
;	O
finds	O
the	O
longest	O
common	O
prefix	O
of	O
all	O
beams	O
in	O
n	O
consecutive	O
chunks	O
)	O
and	O
local	O
agreement	O
(	O
LA	O
-	O
n	O
;	O
finds	O
the	O
longest	O
common	O
prefix	O
of	O
the	O
best	O
hypothesis	O
in	O
n	O
consecutive	O
chunks	O
)	O
.	O
For	O
hold	O
-	O
n	O
,	O
we	O
select	O
n	O
=	O
3	O
,	O
6	O
,	O
12	O
;	O
for	O
SP	O
-	O
n	O
,	O
we	O
select	O
n	O
=	O
1	O
,	O
2	O
(	O
n	O
=	O
1	O
corresponds	O
to	O
the	O
strategy	O
by	O
Nguyen	O
et	O
al	O
(	O
2021	O
)	O
)	O
;	O
for	O
LA	O
-	O
n	O
we	O
select	O
n	O
=	O
2	O
,	O
3	O
,	O
4	O
(	O
n	O
=	O
2	O
corresponds	O
to	O
the	O
strategy	O
by	O
Liu	O
et	O
al	O
(	O
2020a	O
)	O
)	O
.	O
The	O
results	O
are	O
in	O
Figures	O
2	O
and	O
3	O
.	O
Hold	O
-	O
n	O
The	O
results	O
suggest	O
(	O
see	O
Figure	O
2	O
)	O
that	O
the	O
hold	O
-	O
n	O
strategy	O
can	O
use	O
either	O
n	O
or	O
chunk	O
size	O
to	O
control	O
the	O
quality	O
-	O
latency	O
trade	O
-	O
off	O
with	O
equal	O
effect	O
.	O
The	O
only	O
exception	O
seems	O
to	O
be	O
too	O
low	O
n	O
<	O
=	O
3	O
,	O
which	O
slightly	O
underperforms	O
the	O
options	O
with	O
higher	O
n	O
and	O
shorter	O
chunk	O
size	O
.	O
Local	O
agreement	O
(	O
LA	O
-	O
n	O
)	O
The	O
local	O
agreement	O
seems	O
to	O
outperform	O
all	O
other	O
strategies	O
(	O
see	O
Figure	O
3	O
)	O
.	O
LA	O
-	O
n	O
for	O
all	O
n	O
follows	O
the	O
same	O
qualitylatency	O
trade	O
-	O
off	O
line	O
.	O
The	O
advantage	O
of	O
LA	O
-	O
2	O
is	O
in	O
reduced	O
computational	O
complexity	O
compared	O
to	O
the	O
other	O
LA	O
-	O
n	O
strategies	O
with	O
n	O
>	O
2	O
.	O
Shared	O
prefix	O
(	O
SP	O
-	O
n	O
)	O
SP	O
-	O
1	O
strongly	O
underperforms	O
other	O
strategies	O
in	O
quality	O
(	O
see	O
Figure	O
3	O
)	O
.	O
While	O
the	O
SP	O
-	O
1	O
strategy	O
performs	O
well	O
in	O
the	O
ASR	O
task	O
(	O
Nguyen	O
et	O
al	O
,	O
2021	O
)	O
,	O
it	O
is	O
probably	O
too	O
lax	O
for	O
the	O
speech	O
translation	O
task	O
.	O
The	O
generalized	O
and	O
more	O
conservative	O
SP	O
-	O
2	O
performs	O
much	O
better	O
.	O
Although	O
,	O
the	O
more	O
relaxed	O
LA	O
-	O
2	O
,	O
which	O
considers	O
only	O
the	O
best	O
item	O
in	O
the	O
beam	O
,	O
has	O
a	O
better	O
qualitylatency	O
trade	O
-	O
off	O
curve	O
than	O
the	O
more	O
conservative	O
SP	O
-	O
2	O
.	O

In	O
this	O
paper	O
,	O
we	O
reviewed	O
onlinization	O
strategies	O
for	O
end	O
-	O
to	O
-	O
end	O
speech	O
translation	O
models	O
.	O
We	O
identified	O
the	O
optimal	O
stable	O
hypothesis	O
detection	O
strategy	O
and	O
proposed	O
two	O
separate	O
ways	O
of	O
the	O
qualitylatency	O
trade	O
-	O
off	O
parametrization	O
.	O
We	O
showed	O
that	O
the	O
onlinization	O
of	O
the	O
offline	O
models	O
is	O
easy	O
and	O
performs	O
almost	O
on	O
par	O
with	O
the	O
offline	O
run	O
.	O
We	O
demonstrated	O
that	O
an	O
improvement	O
in	O
the	O
offline	O
model	O
leads	O
to	O
improved	O
online	O
performance	O
.	O
We	O
also	O
showed	O
that	O
our	O
method	O
outperforms	O
a	O
dedicated	O
simultaneous	O
system	O
.	O
Finally	O
,	O
we	O
proposed	O
an	O
improvement	O
in	O
the	O
average	O
latency	O
metric	O
.	O

Ensembling	O
of	O
Distilled	O
Models	O
from	O
Multi	O
-	O
task	O
Teachers	O
for	O
Constrained	O
Resource	O
Language	O
Pairs	O

Following	O
the	O
constrained	O
track	O
,	O
we	O
use	O
bitext	O
data	O
provided	O
in	O
WMT21	O
for	O
the	O
following	O
pairs	O
:	O
Bengali	O
↔	O
Hindi	O
,	O
English	O
↔	O
Hausa	O
,	O
Xhosa	O
↔	O
Zulu	O
and	O
English	O
↔	O
German	O
.	O
Statistics	O
of	O
the	O
parallel	O
data	O
used	O
for	O
the	O
three	O
pairs	O
in	O
addition	O
to	O
the	O
German	O
helper	O
are	O
shown	O
in	O
Table	O
1	O
.	O
We	O
also	O
use	O
monolingual	O
data	O
for	O
all	O
previously	O
mentioned	O
languages	O
provided	O
in	O
WMT21	O
for	O
techniques	O
such	O
as	O
multi	O
-	O
task	O
training	O
and	O
back	O
-	O
translation	O
.	O
Statistics	O
of	O
the	O
monolingual	O
data	O
used	O
for	O
the	O
6	O
languages	O
in	O
addition	O
to	O
the	O
German	O
helper	O
are	O
shown	O
in	O
Table	O
2	O
.	O
For	O
very	O
low	O
resource	O
languages	O
,	O
Hausa	O
,	O
Xhosa	O
and	O
Zulu	O
,	O
we	O
use	O
all	O
the	O
available	O
monolingual	O
data	O
,	O
e.g.	O
NewsCrawl	O
+	O
CommonCrawl	O
+	O
Extended	O
CommonCrawl	O
for	O
Hausa	O
,	O
and	O
Extended	O
Common	O
-	O
Crawl	O
for	O
both	O
Xhosa	O
and	O
Zulu	O
.	O
For	O
relatively	O
high	O
resource	O
languages	O
,	O
Bengali	O
,	O
Hindi	O
,	O
English	O
and	O
German	O
,	O
we	O
only	O
use	O
a	O
subset	O
of	O
the	O
provided	O
data	O
mostly	O
from	O
NewsCrawl	O
due	O
to	O
its	O
high	O
-	O
quality	O
.	O
In	O
addition	O
to	O
the	O
NewsCrawl	O
monolingual	O
subset	O
,	O
we	O
add	O
a	O
sampled	O
subset	O
from	O
CommonCrawl	O
to	O

The	O
final	O
MT	O
system	O
in	O
each	O
direction	O
is	O
an	O
ensemble	O
of	O
two	O
NMT	O
models	O
comprising	O
a	O
bilingual	O
model	O
(	O
one	O
for	O
each	O
of	O
the	O
six	O
primary	O
directions	O
)	O
and	O
a	O
multilingual	O
model	O
trained	O
to	O
provide	O
translations	O
for	O
8	O
directions	O
(	O
the	O
six	O
primary	O
directions	O
plus	O
English	O
↔	O
German	O
)	O
.	O
The	O
multilingual	O
system	O
uses	O
a	O
recently	O
proposed	O
multitask	O
framework	O
for	O
training	O
(	O
Wang	O
et	O
al	O
,	O
2020	O
)	O
.	O
We	O
describe	O
the	O
individual	O
systems	O
in	O
Subsection	O
3.1	O
.	O
This	O
is	O
followed	O
by	O
presenting	O
our	O
system	O
combination	O
techniques	O
in	O
Subsection	O
3.2	O
.	O
Finally	O
we	O
present	O
the	O
architecture	O
of	O
the	O
submitted	O
system	O
highlighting	O
our	O
design	O
decisions	O
in	O
Subsection	O
3.3	O
.	O

Our	O
overall	O
system	O
is	O
depicted	O
in	O
Figure	O
2	O
1	O
following	O
the	O
temperature	O
-	O
based	O
strategy	O
in	O
(	O
Arivazhagan	O
et	O
al	O
,	O
2019	O
)	O
to	O
balance	O
the	O
training	O
data	O
in	O
different	O
resource	O
languages	O
using	O
T	O
=	O
5	O
.	O
We	O
pick	O
the	O
best	O
system	O
and	O
use	O
it	O
to	O
back	O
translate	O
the	O
selected	O
monolingual	O
data	O
.	O
For	O
most	O
pairs	O
,	O
as	O
detailed	O
in	O
Section	O
4	O
,	O
we	O
find	O
that	O
M	O
T	O
+	O
DAE	O
and	O
M	O
T	O
+	O
M	O
LM	O
+	O
DAE	O
are	O
quite	O
close	O
.	O
Therefore	O
,	O
we	O
use	O
the	O
M	O
T	O
+	O
DAE	O
to	O
do	O
back	O
translation	O
for	O
all	O
submitted	O
6	O
pairs	O
.	O
We	O
use	O
beam	O
search	O
with	O
beam	O
size	O
=	O
5	O
when	O
generating	O
the	O
synthetic	O
back	O
-	O
translated	O
data	O
.	O
Once	O
we	O
get	O
the	O
back	O
-	O
translated	O
data	O
(	O
called	O
BT	O
1	O
)	O
we	O
add	O
it	O
to	O
our	O
parallel	O
and	O
monolingual	O
data	O
and	O
build	O
a	O
new	O
multilingual	O
model	O
called	O
M	O
T	O
+	O
DAE	O
+	O
BT	O
1	O
.	O
We	O
tag	O
the	O
back	O
-	O
translated	O
data	O
with	O
<	O
BT	O
>	O
tag	O
at	O
beginning	O
of	O
each	O
source	O
sentence	O
so	O
the	O
model	O
can	O
differentiate	O
between	O
the	O
genuine	O
parallel	O
and	O
backtranslated	O
data	O
quality	O
.	O
The	O
resulting	O
model	O
is	O
used	O
to	O
regenerate	O
the	O
back	O
-	O
translated	O
data	O
(	O
called	O
BT	O
2	O
)	O
and	O
to	O
knowledge	O
distill	O
the	O
bitext	O
(	O
called	O
KD	O
)	O
.	O
The	O
latter	O
two	O
data	O
sets	O
are	O
augmented	O
and	O
used	O
to	O
build	O
a	O
bilingual	O
system	O
(	O
called	O
M	O
T	O
+	O
KD	O
+	O
BT	O
2	O
)	O
.	O
We	O
upsample	O
the	O
KD	O
data	O
set	O
and	O
the	O
upsampling	O
ratio	O
is	O
selected	O
based	O
on	O
parameter	O
sweeping	O
and	O
validating	O
the	O
resulting	O
improvement	O
on	O
the	O
validation	O
set	O
.	O
Finally	O
,	O
the	O
latter	O
bilingual	O
model	O
is	O
combined	O
with	O
our	O
final	O
multilingual	O
model	O
using	O
the	O
method	O
in	O
Section	O
3.2	O
to	O
create	O
our	O
submission	O
.	O

Understanding	O
and	O
Improving	O
the	O
Exemplar	O
-	O
based	O
Generation	O
for	O
Open	O
-	O
domain	O
Conversation	O

Exemplar	O
-	O
based	O
generative	O
models	O
for	O
opendomain	O
conversation	O
produce	O
responses	O
based	O
on	O
the	O
exemplars	O
provided	O
by	O
the	O
retriever	O
,	O
taking	O
advantage	O
of	O
generative	O
models	O
and	O
retrieval	O
models	O
.	O
However	O
,	O
due	O
to	O
the	O
oneto	O
-	O
many	O
problem	O
of	O
the	O
open	O
-	O
domain	O
conversation	O
,	O
they	O
often	O
ignore	O
the	O
retrieved	O
exemplars	O
while	O
generating	O
responses	O
or	O
produce	O
responses	O
over	O
-	O
fitted	O
to	O
the	O
retrieved	O
exemplars	O
.	O
To	O
address	O
these	O
advantages	O
,	O
we	O
introduce	O
a	O
training	O
method	O
selecting	O
exemplars	O
that	O
are	O
semantically	O
relevant	O
to	O
the	O
gold	O
response	O
but	O
lexically	O
distanced	O
from	O
the	O
gold	O
response	O
.	O
In	O
the	O
training	O
phase	O
,	O
our	O
training	O
method	O
first	O
uses	O
the	O
gold	O
response	O
instead	O
of	O
dialogue	O
context	O
as	O
a	O
query	O
to	O
select	O
exemplars	O
that	O
are	O
semantically	O
relevant	O
to	O
the	O
gold	O
response	O
.	O
And	O
then	O
,	O
it	O
eliminates	O
the	O
exemplars	O
that	O
lexically	O
resemble	O
the	O
gold	O
responses	O
to	O
alleviate	O
the	O
dependency	O
of	O
the	O
generative	O
models	O
on	O
that	O
exemplars	O
.	O
The	O
remaining	O
exemplars	O
could	O
be	O
irrelevant	O
to	O
the	O
given	O
context	O
since	O
they	O
are	O
searched	O
depending	O
on	O
the	O
gold	O
response	O
.	O
Thus	O
,	O
our	O
training	O
method	O
further	O
utilizes	O
the	O
relevance	O
scores	O
between	O
the	O
given	O
context	O
and	O
the	O
exemplars	O
to	O
penalize	O
the	O
irrelevant	O
exemplars	O
.	O
Extensive	O
experiments	O
demonstrate	O
that	O
our	O
proposed	O
training	O
method	O
alleviates	O
the	O
drawbacks	O
of	O
the	O
existing	O
exemplar	O
-	O
based	O
generative	O
models	O
and	O
significantly	O
improves	O
the	O
performance	O
in	O
terms	O
of	O
appropriateness	O
and	O
informativeness	O
.	O
†	O
Equal	O
contribution	O
*	O
Corresponding	O
author	O
Given	O
context	O
A	O
:	O
Do	O
you	O
ever	O
feel	O
like	O
time	O
is	O
just	O
going	O
by	O
way	O
too	O
fast	O
?	O
Retrieved	O
exemplar	O
B	O
:	O
It	O
's	O
hard	O
to	O
get	O
anything	O
done	O
with	O
coworkers	O
around	O
.	O
Generated	O
responses	O
B	O
:	O
Do	O
you	O
have	O
any	O
hobbies	O
that	O
you	O
like	O
to	O
do	O
while	O
you	O
're	O
at	O
work	O
?	O
like	O
gardening	O
?	O
(	O
a	O
)	O
RetNRef	O
(	O
Weston	O
et	O
al	O
2018	O
)	O
B	O
:	O
It	O
's	O
hard	O
to	O
get	O
things	O
done	O
with	O
coworkers	O
when	O
you	O
're	O
busy	O
all	O
the	O
time	O
.	O
(	O
c	O
)	O
RetNRef	O
w/	O
CORGE	O
(	O
Ours	O
)	O
B	O
:	O
OMG	O
!	O
especially	O
recently	O
.	O
a	O
week	O
seems	O
like	O
one	O
day	O
.	O
A	O
:	O
Yes	O
!	O
Time	O
especially	O
goes	O
by	O
fast	O
when	O
I	O
'm	O
working	O
at	O
my	O
job	O
.	O
I	O
'm	O
constantly	O
busy	O
.	O
B	O
:	O
It	O
's	O
hard	O
to	O
get	O
around	O
.	O
anything	O
done	O
with	O
coworkers	O
gets	O
to	O
get	O
done	O
with	O
anything	O
.	O

(	O
1	O
)	O
We	O
analyze	O
the	O
shortcomings	O
of	O
existing	O
exemplar	O
-	O
based	O
generative	O
models	O
derived	O
from	O
the	O
nature	O
of	O
the	O
opendomain	O
conversation	O
,	O
the	O
one	O
-	O
to	O
-	O
many	O
problem	O
.	O
(	O
2	O
)	O
We	O
introduce	O
a	O
training	O
method	O
(	O
CORGE	O
)	O
to	O
improve	O
the	O
quality	O
of	O
generated	O
responses	O
by	O
selecting	O
useful	O
exemplars	O
and	O
weighting	O
the	O
exemplars	O
by	O
relevance	O
scores	O
assessed	O
by	O
the	O
retriever	O
.	O
(	O
3	O
)	O
Through	O
the	O
human	O
evaluation	O
,	O
we	O
demonstrate	O
that	O
CORGE	O
significantly	O
improves	O
the	O
performance	O
of	O
exemplar	O
-	O
based	O
generative	O
models	O
in	O
terms	O
of	O
appropriateness	O
and	O
informativeness	O
.	O

While	O
generative	O
models	O
have	O
shown	O
remarkable	O
performance	O
on	O
the	O
open	O
-	O
domain	O
conversation	O
,	O
it	O
is	O
well	O
-	O
known	O
that	O
generative	O
models	O
tend	O
to	O
yield	O
uninformative	O
and	O
bland	O
responses	O
(	O
Li	O
et	O
al	O
,	O
2016	O
;	O
Liu	O
et	O
al	O
,	O
2016	O
;	O
Serban	O
et	O
al	O
,	O
2017	O
;	O
Li	O
et	O
al	O
,	O
2020	O
;	O
Holtzman	O
et	O
al	O
,	O
2019	O
;	O
Welleck	O
et	O
al	O
,	O
2019	O
)	O
.	O
Exemplar	O
-	O
based	O
generative	O
models	O
are	O
introduced	O
to	O
overcome	O
the	O
aforementioned	O
problem	O
generative	O
models	O
suffer	O
.	O
Wu	O
et	O
al	O
(	O
2019	O
)	O
introduce	O
an	O
exemplar	O
-	O
based	O
generative	O
model	O
for	O
open	O
-	O
domain	O
conversation	O
,	O
which	O
retrieves	O
a	O
context	O
-	O
exemplar	O
pair	O
conditioned	O
by	O
the	O
input	O
context	O
and	O
encodes	O
the	O
lexical	O
difference	O
between	O
the	O
input	O
context	O
and	O
the	O
retrieved	O
context	O
to	O
the	O
edit	O
vector	O
.	O
The	O
response	O
is	O
produced	O
by	O
feeding	O
the	O
exemplar	O
and	O
the	O
edit	O
vector	O
to	O
the	O
generator	O
.	O
;	O
Roller	O
et	O
al	O
(	O
2021	O
)	O
also	O
retrieve	O
the	O
exemplar	O
using	O
the	O
given	O
context	O
as	O
a	O
query	O
and	O
concatenate	O
the	O
exemplar	O
with	O
the	O
context	O
,	O
then	O
feed	O
the	O
concatenated	O
exemplar	O
into	O
the	O
generator	O
to	O
produce	O
the	O
final	O
response	O
for	O
the	O
open	O
-	O
domain	O
conversation	O
.	O
Cai	O
et	O
al	O
(	O
2019a	O
,	O
b	O
)	O
propose	O
a	O
method	O
that	O
removes	O
the	O
irrelevant	O
information	O
from	O
the	O
exemplar	O
,	O
then	O
uses	O
the	O
masked	O
exemplar	O
to	O
inform	O
the	O
generator	O
to	O
produce	O
the	O
response	O
.	O
Gupta	O
et	O
al	O
(	O
2021	O
)	O
condition	O
the	O
generator	O
with	O
the	O
retrieved	O
exemplars	O
and	O
the	O
extracted	O
semantic	O
frames	O
of	O
the	O
exemplars	O
,	O
which	O
improves	O
the	O
coherence	O
of	O
generated	O
responses	O
.	O
We	O
do	O
not	O
consider	O
this	O
model	O
as	O
a	O
baseline	O
because	O
their	O
model	O
requires	O
an	O
additional	O
semantic	O
frame	O
extractor	O
,	O
and	O
it	O
can	O
be	O
mutually	O
complemented	O
with	O
our	O
proposed	O
training	O
method	O
.	O

Let	O
D	O
=	O
{	O
(	O
c	O
i	O
,	O
r	O
i	O
)	O
|	O
1	O
≤	O
i	O
≤	O
n	O
}	O
denote	O
the	O
dialogue	O
dataset	O
,	O
which	O
consists	O
of	O
n	O
pairs	O
of	O
context	O
c	O
and	O
response	O
r.	O
Exemplar	O
-	O
based	O
generative	O
models	O
are	O
composed	O
of	O
two	O
components	O
:	O
a	O
retriever	O
R	O
and	O
a	O
generator	O
G.	O
For	O
a	O
given	O
context	O
c	O
i	O
,	O
the	O
retriever	O
finds	O
the	O
top	O
-	O
scoring	O
exemplar	O
based	O
on	O
the	O
relevance	O
score	O
S	O
R	O
(	O
z	O
,	O
c	O
i	O
)	O
of	O
the	O
exemplar	O
z	O
R	O
,	O
where	O
R	O
is	O
a	O
pre	O
-	O
defined	O
response	O
set	O
.	O
The	O
generator	O
computes	O
the	O
probability	O
of	O
the	O
response	O
for	O
the	O
context	O
c	O
i	O
while	O
utilizing	O
the	O
exemplar	O
z	O
as	O
P	O
G	O
(	O
r	O
|	O
c	O
i	O
,	O
z	O
)	O
.	O

We	O
hypothesize	O
that	O
selecting	O
semantically	O
relevant	O
but	O
lexically	O
distanced	O
exemplars	O
from	O
the	O
gold	O
response	O
could	O
solve	O
the	O
drawbacks	O
above	O
.	O
To	O
validate	O
this	O
hypothesis	O
,	O
we	O
introduce	O
a	O
training	O
method	O
of	O
exemplar	O
-	O
based	O
generative	O
models	O
,	O
called	O
CORGE	O
.	O
Our	O
proposed	O
training	O
method	O
is	O
illustrated	O
in	O
Figure	O
3	O
,	O
and	O
the	O
illustrative	O
examples	O
about	O
the	O
exemplars	O
selected	O
by	O
CORGE	O
are	O
described	O
in	O
Table	O
1	O
.	O

To	O
verify	O
the	O
effectiveness	O
of	O
each	O
component	O
in	O
CORGE	O
,	O
we	O
conduct	O
the	O
ablation	O
study	O
.	O
In	O
Table	O
5	O
A	O
Implementation	O
Details	O

We	O
prepare	O
dialogue	O
cases	O
that	O
have	O
three	O
-	O
turn	O
input	O
contexts	O
and	O
the	O
gold	O
response	O
from	O
the	O
BST	O
and	O
evaluate	O
them	O
by	O
human	O
pair	O
-	O
wise	O
comparison	O
and	O
automatic	O
evaluation	O
.	O
There	O
are	O
980	O
test	O
cases	O
,	O
and	O
we	O
randomly	O
choose	O
100	O
test	O
cases	O
for	O
the	O
human	O
evaluation	O
.	O

As	O
we	O
described	O
in	O
Section	O
5.3	O
,	O
we	O
use	O
Amazon	O
Mechanical	O
Turk	O
to	O
collect	O
the	O
annotations	O
.	O
Each	O
test	O
case	O
is	O
rated	O
by	O
three	O
annotators	O
to	O
improve	O
the	O
robustness	O
of	O
the	O
evaluation	O
result	O
.	O
We	O
set	O
a	O
maximum	O
number	O
of	O
annotations	O
per	O
worker	O
in	O
order	O
to	O
reduce	O
the	O
potential	O
bias	O
.	O
To	O
control	O
the	O
quality	O
of	O
the	O
annotations	O
,	O
we	O
only	O
allowed	O
annotators	O
who	O
satisfy	O
the	O
following	O
requirements	O
to	O
evaluate	O
our	O
results	O
:	O
(	O
1	O
)	O
HITs	O
approval	O
rate	O
greater	O
than	O
95	O
%	O
,	O
(	O
2	O
)	O
Location	O
is	O
one	O
of	O
Australia	O
,	O
Canada	O
,	O
New	O
Zealand	O
,	O
United	O
Kingdom	O
,	O
and	O
the	O
United	O
States	O
,	O
(	O
3	O
)	O
Lifetime	O
number	O
of	O
HITs	O
approved	O
greater	O
than	O
1000	O
,	O
following	O
Li	O
et	O
al	O
(	O
2018	O
)	O
.	O
Figure	O
5	O
shows	O
the	O
instructions	O
and	O
the	O
interface	O
for	O
the	O
human	O
evaluation	O
.	O
To	O
mitigate	O
the	O
bias	O
from	O
the	O
annotator	O
,	O
we	O
randomly	O
shuffle	O
the	O
order	O
of	O
the	O
model	O
and	O
the	O
corresponding	O
response	O
.	O

We	O
provide	O
additional	O
samples	O
for	O
the	O
retrieved	O
exemplar	O
and	O
the	O
model	O
response	O
from	O
the	O
baselines	O
and	O
our	O
models	O
in	O
Table	O
6	O
.	O

A	O
:	O
I	O
crash	O
around	O
230	O
to	O
300	O
.	O
I	O
do	O
n't	O
have	O
much	O
of	O
a	O
social	O
life	O
.	O
B	O
:	O
Neither	O
do	O
I	O
,	O
you	O
are	O
not	O
alone	O
A	O
:	O
I	O
am	O
alone	O
but	O
I	O
'm	O
not	O
lonely	O
.	O
I	O
have	O
a	O
dog	O
who	O
's	O
great	O
company	O
.	O

No	O
we	O
went	O
to	O
a	O
haunted	O
house	O
and	O
I	O
got	O
chased	O
often	O
.	O

No	O
we	O
never	O
got	O
to	O
go	O
to	O
the	O
haunted	O
house	O
we	O
went	O
to	O
once	O
and	O
got	O
chased	O
by	O
a	O
ghost	O
.	O
MatToGen	O
+	O
CORGE	O
No	O
,	O
I	O
don	O
'	O
t	O
like	O
to	O
go	O
to	O
haunted	O
houses	O
.	O
I	O
'	O
m	O
scared	O
of	O
them	O
.	O

Yes	O
,	O
I	O
went	O
to	O
a	O
haunted	O
house	O
and	O
watched	O
all	O
the	O
horror	O
movies	O
.	O
It	O
was	O
so	O
scary	O
!	O

A	O
:	O
Oh	O
God	O
no	O
!	O
I	O
ca	O
n't	O
stay	O
tied	O
down	O
to	O
one	O
woman	O
.	O
Why	O
would	O
you	O
want	O
to	O
do	O
that	O
?	O
B	O
:	O
I	O
know	O
right	O
?	O
Most	O
people	O
consider	O
marriage	O
to	O
be	O
involving	O
2	O
people	O
but	O
in	O
certain	O
parts	O
of	O
the	O
world	O
that	O
varies	O
between	O
cultures	O
and	O
religions	O
,	O
so	O
it	O
does	O
n't	O
seem	O
so	O
bad	O
A	O
:	O
Marriage	O
is	O
very	O
good	O
.	O

Benchmarking	O
Hierarchical	O
Script	O
Knowledge	O

Our	O
approach	O
situates	O
script	O
learning	O
as	O
a	O
case	O
of	O
grounding	O
.	O
For	O
simplicity	O
of	O
exposition	O
,	O
let	O
us	O
assume	O
there	O
are	O
three	O
levels	O
of	O
abstraction	O
to	O
grounding	O
:	O
abstract	O
concrete	O
motor	O
control	O
.	O
Most	O
prior	O
work	O
in	O
grounding	O
treats	O
language	O
monolithically	O
1	O
and	O
ignores	O
the	O
issue	O
of	O
audience	O
.	O
In	O
practice	O
,	O
this	O
means	O
the	O
task	O
formulation	O
or	O
exposed	O
API	O
may	O
implicitly	O
bias	O
the	O
language	O
to	O
be	O
more	O
concrete	O
.	O
By	O
viewing	O
the	O
task	O
as	O
purely	O
linguistic	O
,	O
we	O
have	O
no	O
API	O
or	O
robot	O
that	O
constrains	O
our	O
language	O
;	O
instead	O
,	O
we	O
define	O
our	O
audience	O
as	O
children	O
.	O
By	O
eliciting	O
child	O
-	O
directed	O
instructions	O
,	O
we	O
collect	O
concrete	O
language	O
capturing	O
otherwise	O
implicit	O
world	O
knowledge	O
that	O
a	O
child	O
would	O
not	O
know	O
.	O
Because	O
annotators	O
assume	O
a	O
smart	O
and	O
capable	O
but	O
uninformed	O
listener	O
,	O
we	O
posit	O
this	O
language	O
corresponds	O
closely	O
to	O
the	O
most	O
"	O
concrete	O
"	O
form	O
in	O
which	O
language	O
naturally	O
occurs	O
.	O

To	O
investigate	O
what	O
new	O
knowledge	O
is	O
being	O
introduced	O
and	O
whether	O
a	O
model	O
has	O
captured	O
it	O
,	O
we	O
construct	O
a	O
cloze	O
-	O
style	O
slot	O
-	O
filling	O
task	O
(	O
Chambers	O
,	O
2017	O
;	O
Hermann	O
et	O
al	O
,	O
2015	O
)	O
.	O
We	O
drop	O
key	O
content	O
words	O
from	O
the	O
concrete	O
realization	O
of	O
an	O
abstract	O
instruction	O
and	O
ask	O
the	O
model	O
to	O
predict	O
them	O
.	O
Several	O
examples	O
from	O
the	O
validation	O
set	O
are	O
shown	O
in	O
Table	O
2	O
.	O
Correctly	O
predicting	O
the	O
missing	O
words	O
requires	O
knowledge	O
of	O
the	O
manner	O
of	O
executing	O
a	O
task	O
and	O
the	O
tools	O
required	O
.	O
To	O
choose	O
candidate	O
words	O
to	O
drop	O
,	O
we	O
only	O
allow	O
words	O
that	O
occur	O
primarily	O
in	O
the	O
concrete	O
instructions	O
.	O
Additionally	O
,	O
we	O
do	O
not	O
drop	O
stop	O
words	O
,	O
numbers	O
,	O
or	O
words	O
occurring	O
fewer	O
than	O
five	O
times	O
.	O
We	O
do	O
,	O
however	O
,	O
drop	O
units	O
of	O
measure	O
(	O
cup	O
,	O
minute	O
,	O
etc	O
.	O
)	O
.	O
This	O
ensures	O
we	O
create	O
blanks	O
whose	O
answers	O
are	O
previously	O
omitted	O
concrete	O
details	O
.	O
Relatedly	O
,	O
under	O
this	O
filter	O
the	O
answer	O
to	O
a	O
blank	O
is	O
very	O
rarely	O
an	O
ingredient	O
,	O
as	O
our	O
goal	O
is	O
not	O
to	O
memorize	O
recipes	O
,	O
but	O
to	O
infer	O
the	O
tool	O
knowledge	O
necessary	O
to	O
execute	O
them	O
.	O
In	O
total	O
,	O
we	O
whitelist	O
∼1	O
,	O
000	O
words	O
that	O
can	O
be	O
dropped	O
to	O
create	O
blanks	O
.	O
We	O
prefer	O
longer	O
blanks	O
when	O
available	O
to	O
give	O
preference	O
to	O
compound	O
nouns	O
(	O
e.g.	O
wire	O
whisk	O
)	O
.	O
Finally	O
,	O
we	O
do	O
not	O
drop	O
any	O
words	O
ABS	O
chop	O
garlic	O
into	O
small	O
pieces	O
.	O
CON	O
put	O
garlic	O
on	O
cutting	O
board	O
.	O
press	O
on	O
back	O
of	O
knife	O
with	O
hand	O
,	O
cutting	O
into	O
small	O
pieces	O
.	O
ABS	O
add	O
some	O
parmesan	O
cheese	O
into	O
the	O
bowl	O
and	O
mix	O
them	O
well	O
.	O
CON	O
use	O
a	O
grater	O
to	O
grate	O
some	O
parmesan	O
cheese	O
into	O
the	O
bowl	O
.	O
use	O
a	O
wire	O
whisk	O
to	O
stir	O
the	O
cheese	O
in	O
.	O
ABS	O
add	O
the	O
tofu	O
to	O
the	O
wok	O
.	O
CON	O
drain	O
the	O
water	O
from	O
the	O
tofu	O
using	O
a	O
strainer	O
.	O
add	O
the	O
tofu	O
into	O
the	O
pan	O
.	O
use	O
a	O
spoon	O
to	O
stir	O
the	O
tofu	O
in	O
the	O
mixture	O
.	O
from	O
the	O
concrete	O
sentence	O
if	O
they	O
occur	O
in	O
the	O
abstract	O
description	O
.	O
This	O
restriction	O
eliminates	O
any	O
benefits	O
that	O
might	O
have	O
been	O
achieved	O
via	O
models	O
with	O
copy	O
mechanisms	O
.	O
Examples	O
that	O
do	O
not	O
meet	O
our	O
criteria	O
are	O
removed	O
from	O
the	O
corpus	O
.	O

We	O
visualize	O
alignments	O
of	O
our	O
transduction	O
model	O
over	O
two	O
partial	O
sequences	O
in	O
Fig	O
.	O
2	O
.	O
This	O
shows	O
which	O
hidden	O
vector	O
of	O
the	O
abstract	O
sentence	O
aligned	O
to	O
every	O
region	O
of	O
the	O
concrete	O
sequence	O
.	O
Specifically	O
,	O
we	O
see	O
how	O
tools	O
like	O
the	O
big	O
bowl	O
,	O
spoon	O
,	O
and	O
tongs	O
are	O
introduced	O
to	O
facilitate	O
the	O
actions	O
.	O
There	O
are	O
also	O
implications	O
,	O
e.g.	O
that	O
high	O
indicates	O
grill	O
.	O
For	O
further	O
analysis	O
we	O
extract	O
alignments	O
over	O
the	O
training	O
corpus	O
,	O
linking	O
each	O
decoded	O
phrase	O
with	O
the	O
word	O
from	O
the	O
encoding	O
it	O
used	O
during	O
generation	O
.	O
We	O
then	O
aggregate	O
these	O
tuples	O
into	O
a	O
table	O
which	O
we	O
can	O
filter	O
(	O
based	O
on	O
our	O
whitelist	O
)	O
and	O
sort	O
(	O
with	O
PMI	O
)	O
.	O
This	O
process	O
is	O
imprecise	O
as	O
it	O
discards	O
the	O
context	O
in	O
which	O
the	O
alignment	O
occurs	O
,	O
but	O
it	O
nonetheless	O
extracts	O
many	O
Abs	O
shape	O
each	O
dough	O
ball	O
into	O
a	O
circle	O
and	O
add	O
tomato	O
sauce	O
.	O
Pred	O
flatten	O
out	O
your	O
dough	O
into	O
a	O
flat	O
circle	O
using	O
your	O
hands	O
.	O
take	O
a	O
knife	O
to	O
add	O
tomato	O
sauce	O
to	O
the	O
center	O
of	O
your	O
dough	O
.	O
use	O
the	O
back	O
side	O
of	O
the	O
knife	O
to	O
cut	O
the	O
sauce	O
out	O
.	O
make	O
sure	O
you	O
keep	O
the	O
sauce	O
about	O
an	O
inch	O
from	O
the	O
edges	O
.	O
Gold	O
flatten	O
out	O
your	O
dough	O
into	O
a	O
flat	O
circle	O
using	O
your	O
hands	O
.	O
take	O
a	O
spoon	O
to	O
add	O
tomato	O
sauce	O
to	O
the	O
center	O
of	O
your	O
dough	O
.	O
use	O
the	O
back	O
side	O
of	O
the	O
spoon	O
to	O
spread	O
the	O
sauce	O
out	O
.	O
make	O
sure	O
you	O
keep	O
the	O
sauce	O
about	O
an	O
inch	O
from	O
the	O
edges	O
.	O
Abs	O
place	O
the	O
kale	O
cucumber	O
bell	O
peppers	O
carrots	O
and	O
radishes	O
on	O
the	O
wrapper	O
.	O
Pred	O
put	O
the	O
cut	O
on	O
a	O
cutting	O
.	O
put	O
a	O
cutting	O
amount	O
of	O
kale	O
on	O
the	O
cutting	O
.	O
add	O
a	O
cut	O
amount	O
of	O
cucumber	O
...	O
Gold	O
put	O
the	O
wrap	O
on	O
a	O
plate	O
.	O
put	O
a	O
small	O
amount	O
of	O
kale	O
on	O
the	O
wrap	O
.	O
add	O
a	O
small	O
amount	O
of	O
cucumber	O
...	O

We	O
introduce	O
a	O
new	O
hierarchical	O
script	O
learning	O
dataset	O
and	O
cloze	O
task	O
in	O
which	O
models	O
must	O
learn	O
commonsense	O
world	O
knowledge	O
about	O
tools	O
,	O
procedures	O
and	O
even	O
basic	O
physics	O
to	O
perform	O
well	O
.	O
Our	O
aim	O
is	O
to	O
begin	O
a	O
conversation	O
about	O
abstraction	O
in	O
language	O
,	O
how	O
it	O
is	O
modeled	O
,	O
and	O
what	O
is	O
implicitly	O
hidden	O
.	O
Our	O
abstract	O
and	O
concrete	O
instructions	O
are	O
grounded	O
in	O
the	O
same	O
videos	O
yet	O
differ	O
dramatically	O
due	O
to	O
their	O
assumed	O
audiences	O
.	O
We	O
show	O
that	O
a	O
neural	O
transduction	O
model	O
produces	O
interpretable	O
alignments	O
for	O
analyzing	O
these	O
otherwise	O
latent	O
correlations	O
and	O
phenomena	O
.	O

We	O
briefly	O
describe	O
the	O
model	O
of	O
Yu	O
,	O
Buys	O
,	O
and	O
Blunsom	O
(	O
2016	O
)	O
and	O
our	O
minor	O
modifications	O
thereto	O
.	O

We	O
would	O
like	O
to	O
thank	O
the	O
reviewers	O
for	O
insightful	O
comments	O
.	O
This	O
research	O
was	O
supported	O
in	O
part	O
by	O
an	O
Amazon	O
Research	O
Award	O
to	O
K.	O
Gimpel	O
.	O

The	O
organizer	O
provided	O
a	O
dataset	O
containing	O
687	O
memes	O
for	O
the	O
training	O
set	O
,	O
63	O
memes	O
for	O
the	O
development	O
set	O
,	O
and	O
200	O
memes	O
for	O
the	O
test	O
set	O
.	O
The	O
dataset	O
of	O
subtask	O
1	O
provides	O
the	O
ID	O
,	O
text	O
of	O
the	O
meme	O
,	O
and	O
the	O
corresponding	O
propaganda	O
techniques	O
used	O
,	O
and	O
the	O
dataset	O
of	O
subtask	O
3	O
also	O
contains	O
the	O
corresponding	O
meme	O
image	O
.	O
The	O
dataset	O
of	O
subtask	O
2	O
provides	O
the	O
ID	O
,	O
text	O
of	O
the	O
meme	O
,	O
and	O
the	O
corresponding	O
propaganda	O
techniques	O
used	O
in	O
a	O
certain	O
text	O
fragment	O
,	O
in	O
which	O
the	O
scope	O
covered	O
by	O
the	O
propaganda	O
technology	O
in	O
the	O
text	O
is	O
marked	O
as	O
"	O
start	O
,	O
"	O
"	O
end	O
,	O
"	O
and	O
"	O
text	O
fragment	O
,	O
"	O
respectively	O
.	O
The	O
datasets	O
were	O
preprocessed	O
using	O
the	O
following	O
procedures	O
before	O
model	O
training	O
:	O
In	O
subtasks	O
1	O
and	O
3	O
,	O
we	O
first	O
used	O
one	O
-	O
hot	O
encoding	O
to	O
encode	O
the	O
label	O
into	O
a	O
vector	O
whose	O
length	O
is	O
the	O
total	O
number	O
of	O
technology	O
categories	O
.	O
In	O
subtask	O
2	O
,	O
we	O
labeled	O
each	O
token	O
in	O
the	O
text	O
as	O
"	O
I	O
-	O
technique	O
"	O
and	O
"	O
O	O
-	O
technique	O
"	O
based	O
on	O
the	O
20	O
propaganda	O
technology	O
terms	O
.	O
"	O
Itechnique	O
"	O
indicates	O
that	O
the	O
publicity	O
technique	O
was	O
used	O
and	O
"	O
O	O
-	O
technique	O
"	O
indicates	O
that	O
it	O
was	O
not	O
,	O
e.g.	O
,	O
O	O
-	O
Smears	O
and	O
I	O
-	O
Smears	O
.	O
For	O
20	O
different	O
propaganda	O
techniques	O
there	O
are	O
40	O
different	O
codes	O
,	O
and	O
then	O
add	O
another	O
padding	O
code	O
,	O
so	O
the	O
label	O
code	O
length	O
is	O
41	O
.	O
In	O
subtask	O
3	O
,	O
we	O
normalized	O
the	O
meme	O
image	O
size	O
to	O
224	O
×	O
224	O
×	O
3	O
.	O

The	O
official	O
evaluation	O
measure	O
for	O
all	O
subtasks	O
is	O
the	O
micro	O
F	O
1	O
-	O
score	O
,	O
which	O
is	O
defined	O
as	O
follows	O
:	O
F	O
1	O
−	O
score	O
=	O
2	O
*	O
P	O
rec	O
*	O
Rec	O
P	O
rec	O
+	O
Rec	O
(	O
1	O
)	O
where	O
Prec	O
and	O
Rec	O
denote	O
the	O
precision	O
and	O
recall	O
scores	O
of	O
all	O
samples	O
,	O
respectively	O
.	O
For	O
subtask	O
2	O
,	O
the	O
standard	O
micro	O
F	O
1	O
-	O
score	O
was	O
slightly	O
modified	O
to	O
account	O
for	O
partial	O
matching	O
between	O
spans	O
(	O
Dimitrov	O
et	O
al	O
,	O
2021	O
)	O
.	O
In	O
addition	O
,	O
the	O
macro	O
F	O
1	O
-	O
score	O
was	O
also	O
reported	O
for	O
each	O
type	O
of	O
propaganda	O
.	O

Learning	O

This	O
work	O
was	O
supported	O
by	O
the	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
(	O
NSFC	O
)	O
under	O
Grants	O
Nos	O
.	O
61702443	O
,	O
61966038	O
and	O
61762091	O
.	O

Deep	O
Learning	O
and	O
Sociophonetics	O
:	O
Automatic	O
Coding	O
of	O
Rhoticity	O
Using	O
Neural	O
Networks	O

Automated	O
extraction	O
methods	O
are	O
widely	O
available	O
for	O
vowels	O
(	O
Rosenfelder	O
et	O
al	O
,	O
2014	O
)	O
,	O
but	O
automated	O
methods	O
for	O
coding	O
rhoticity	O
have	O
lagged	O
far	O
behind	O
.	O
R	O
-	O
fulness	O
versus	O
rlessness	O
(	O
in	O
words	O
like	O
park	O
,	O
store	O
,	O
etc	O
.	O
)	O
is	O
a	O
classic	O
and	O
frequently	O
cited	O
variable	O
(	O
Labov	O
,	O
1966	O
)	O
,	O
but	O
it	O
is	O
still	O
commonly	O
coded	O
by	O
human	O
analysts	O
rather	O
than	O
automated	O
methods	O
.	O
Human	O
-	O
coding	O
requires	O
extensive	O
resources	O
and	O
lacks	O
replicability	O
,	O
making	O
it	O
difficult	O
to	O
compare	O
large	O
datasets	O
across	O
research	O
groups	O
(	O
Yaeger	O
-	O
Dror	O
et	O
al	O
,	O
2008	O
;	O
Heselwood	O
et	O
al	O
,	O
2008	O
)	O
.	O
Can	O
reliable	O
automated	O
methods	O
be	O
developed	O
to	O
aid	O
in	O
coding	O
rhoticity	O
?	O
In	O
this	O
study	O
,	O
we	O
use	O
Neural	O
Networks	O
/	O
Deep	O
Learning	O
,	O
training	O
our	O
model	O
on	O
208	O
Boston	O
-	O
area	O
speakers	O
.	O

Despite	O
advances	O
in	O
automation	O
for	O
phonetic	O
alignment	O
and	O
extraction	O
of	O
vowel	O
formants	O
,	O
there	O
is	O
still	O
no	O
reliable	O
automated	O
method	O
for	O
classifying	O
r	O
-	O
dropping	O
,	O
that	O
is	O
,	O
whether	O
a	O
given	O
word	O
is	O
pronounced	O
with	O
an	O
/r/	O
in	O
words	O
like	O
park	O
(	O
pahk	O
)	O
,	O
start	O
(	O
staht	O
)	O
,	O
and	O
so	O
on	O
.	O
R	O
-	O
dropping	O
,	O
also	O
known	O
as	O
non	O
-	O
rhotic	O
speech	O
,	O
is	O
an	O
important	O
sociolinguistic	O
variable	O
in	O
modern	O
dialect	O
research	O
.	O
But	O
unfortunately	O
most	O
researchers	O
continue	O
to	O
depend	O
on	O
human	O
judgments	O
(	O
Nagy	O
and	O
Irwin	O
,	O
2010	O
;	O
Becker	O
,	O
2009	O
;	O
Nagy	O
and	O
Roberts	O
,	O
2004	O
)	O
,	O
which	O
is	O
an	O
inconsistent	O
and	O
time	O
-	O
consuming	O
method	O
that	O
lacks	O
replicability	O
.	O
Turning	O
to	O
the	O
field	O
of	O
machine	O
learning	O
,	O
our	O
deep	O
learning	O
approach	O
investigates	O
a	O
new	O
way	O
to	O
distinguish	O
rhotic	O
versus	O
non	O
-	O
rhotic	O
pronunciations	O
in	O
recorded	O
data	O
.	O
This	O
is	O
the	O
first	O
study	O
to	O
use	O
neural	O
networks	O
to	O
classify	O
rhotic	O
versus	O
non	O
-	O
rhotic	O
speech	O
.	O
Although	O
human	O
-	O
coding	O
requires	O
extensive	O
resources	O
and	O
lacks	O
consistency	O
and	O
replicability	O
(	O
Yaeger	O
-	O
Dror	O
et	O
al	O
,	O
2008	O
;	O
Heselwood	O
et	O
al	O
,	O
2008	O
)	O
,	O
making	O
it	O
difficult	O
to	O
compare	O
large	O
datasets	O
across	O
different	O
research	O
groups	O
,	O
it	O
is	O
the	O
only	O
method	O
we	O
have	O
right	O
now	O
.	O
How	O
soon	O
will	O
computers	O
be	O
able	O
to	O
quickly	O
and	O
reliably	O
code	O
rhoticity	O
up	O
to	O
this	O
standard	O
?	O
In	O
terms	O
of	O
other	O
machine	O
learning	O
approaches	O
,	O
McLarty	O
,	O
Jones	O
,	O
and	O
Hall	O
work	O
on	O
this	O
challenge	O
using	O
Support	O
Vector	O
Machines	O
(	O
SVMs	O
)	O
(	O
Mclarty	O
et	O
al	O
,	O
2018	O
)	O
.	O
The	O
present	O
study	O
uses	O
Neural	O
Networks	O
/	O
Deep	O
Learning	O
,	O
one	O
of	O
the	O
most	O
effective	O
and	O
fastest	O
-	O
growing	O
approaches	O
in	O
machine	O
-	O
learning	O
.	O
To	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
attempt	O
to	O
use	O
neural	O
networks	O
for	O
automatic	O
coding	O
of	O
any	O
sociophonetic	O
variable	O
.	O
This	O
new	O
method	O
was	O
developed	O
using	O
audio	O
recordings	O
from	O
over	O
200	O
New	O
England	O
speakers	O
from	O
Boston	O
,	O
Maine	O
,	O
and	O
central	O
New	O
Hampshire	O
(	O
Stanford	O
,	O
forthcoming	O
)	O
,	O
and	O
is	O
here	O
compared	O
to	O
other	O
work	O
on	O
rhoticity	O
(	O
Heselwood	O
et	O
al	O
,	O
2008	O
;	O
Mclarty	O
et	O
al	O
,	O
2018	O
)	O
.	O
In	O
what	O
ways	O
can	O
neural	O
networks	O
be	O
effective	O
tools	O
in	O
assisting	O
the	O
coding	O
of	O
rhoticity	O
?	O
To	O
what	O
level	O
can	O
they	O
perform	O
compared	O
to	O
traditional	O
coding	O
methods	O
and	O
other	O
approaches	O
?	O

The	O
phoneme	O
/r/	O
has	O
been	O
particularly	O
difficult	O
to	O
pin	O
down	O
because	O
it	O
may	O
be	O
articulated	O
in	O
different	O
ways	O
,	O
yet	O
still	O
produce	O
the	O
same	O
acoustic	O
signal	O
.	O
As	O
most	O
phoneticians	O
have	O
come	O
to	O
agree	O
,	O
F3	O
is	O
one	O
of	O
the	O
primary	O
acoustic	O
correlates	O
of	O
rhoticity	O
(	O
Espy	O
-	O
Wilson	O
et	O
al	O
,	O
2000	O
;	O
Hagiwara	O
,	O
1995	O
;	O
Thomas	O
,	O
2011	O
)	O
.	O
The	O
general	O
consensus	O
is	O
that	O
the	O
F3	O
measurement	O
for	O
/r/	O
is	O
lower	O
than	O
that	O
of	O
other	O
non	O
-	O
rhotic	O
vowels	O
,	O
but	O
reliable	O
standards	O
for	O
coding	O
rhoticity	O
are	O
lacking	O
.	O
In	O
this	O
paper	O
,	O
rhoticity	O
will	O
refer	O
to	O
post	O
-	O
vocalic	O
realizations	O
of	O
the	O
phoneme	O
/r/	O
which	O
do	O
not	O
occur	O
before	O
other	O
vowels	O
.	O
For	O
example	O
,	O
rhotic	O
tokens	O
of	O
interest	O
would	O
include	O
park	O
and	O
father	O
but	O
not	O
marry	O
.	O
British	O
phonetician	O
John	O
Wells	O
used	O
the	O
term	O
"	O
rhotic	O
"	O
,	O
which	O
has	O
been	O
subsequently	O
considered	O
in	O
the	O
field	O
as	O
one	O
of	O
the	O
most	O
defining	O
traits	O
of	O
varieties	O
of	O
English	O
(	O
Wells	O
,	O
1982	O
)	O
.	O
Rhotic	O
and	O
non	O
-	O
rhotic	O
dialects	O
have	O
been	O
widely	O
studied	O
as	O
they	O
relate	O
to	O
sociolinguistic	O
features	O
of	O
location	O
,	O
age	O
,	O
gender	O
,	O
and	O
socioeconomic	O
status	O
.	O
However	O
,	O
we	O
are	O
still	O
reliant	O
on	O
human	O
analysts	O
to	O
make	O
judgements	O
of	O
rhotic	O
vs.	O
non	O
-	O
rhotic	O
speech	O
,	O
which	O
can	O
require	O
a	O
lot	O
of	O
time	O
and	O
money	O
.	O
Despite	O
advances	O
in	O
many	O
areas	O
of	O
computational	O
linguistics	O
,	O
there	O
is	O
still	O
not	O
an	O
accurate	O
way	O
to	O
determine	O
rhoticity	O
based	O
on	O
acoustic	O
components	O
alone	O
;	O
a	O
human	O
must	O
judge	O
for	O
themselves	O
whether	O
or	O
not	O
an	O
/r/	O
has	O
been	O
dropped	O
.	O
As	O
expected	O
,	O
this	O
is	O
not	O
highly	O
replicable	O
as	O
different	O
speakers	O
may	O
perceive	O
things	O
differently	O
especially	O
when	O
it	O
comes	O
to	O
dialects	O
that	O
are	O
not	O
so	O
clear	O
-	O
cut	O
(	O
Yaeger	O
-	O
Dror	O
et	O
al	O
,	O
2008	O
)	O
.	O
For	O
this	O
reason	O
,	O
an	O
automated	O
way	O
to	O
determine	O
rhotic	O
/	O
non	O
-	O
rhotic	O
tokens	O
would	O
be	O
especially	O
helpful	O
in	O
these	O
contexts	O
.	O
3	O
Other	O
work	O
3.1	O
Heselwood	O
,	O
Plug	O
,	O
and	O
Tickle	O
Heselwood	O
et	O
al	O
(	O
2008	O
)	O
extracted	O
formant	O
data	O
from	O
the	O
spectrograms	O
on	O
the	O
Bark	O
scale	O
-	O
usually	O
,	O
formant	O
data	O
F2	O
/	O
F3	O
is	O
reported	O
on	O
the	O
Hertz	O
scale	O
.	O
The	O
Bark	O
scale	O
more	O
closely	O
correlates	O
to	O
human	O
perception	O
of	O
sounds	O
,	O
that	O
is	O
,	O
on	O
a	O
logarithmic	O
scale	O
rather	O
than	O
absolute	O
.	O
After	O
conversion	O
,	O
F2	O
was	O
labeled	O
Z2	O
and	O
F3	O
was	O
labeled	O
Z3	O
,	O
and	O
a	O
series	O
of	O
perceptual	O
experiments	O
were	O
performed	O
to	O
ascertain	O
rhoticity	O
thresholds	O
.	O
Note	O
that	O
it	O
was	O
conducted	O
for	O
the	O
purposes	O
of	O
perceptual	O
research	O
rather	O
than	O
coding	O
applications	O
.	O

In	O
this	O
initial	O
study	O
,	O
we	O
used	O
Boston	O
-	O
area	O
field	O
recordings	O
of	O
208	O
speakers	O
,	O
100	O
tokens	O
per	O
speaker	O
(	O
107	O
women/101	O
men	O
,	O
born	O
1915	O
-	O
1997	O
)	O
.	O
These	O
on	O
-	O
the	O
-	O
street	O
interviews	O
(	O
15	O
-	O
20	O
minutes	O
each	O
)	O
are	O
typical	O
sociolinguistic	O
recordings	O
in	O
terms	O
of	O
speech	O
styles	O
(	O
word	O
-	O
list	O
,	O
sentences	O
,	O
reading	O
passage	O
,	O
free	O
speech	O
)	O
and	O
occasional	O
background	O
noise	O
.	O
We	O
chose	O
to	O
omit	O
free	O
speech	O
because	O
its	O
token	O
variability	O
between	O
speakers	O
would	O
present	O
another	O
challenging	O
factor	O
,	O
leaving	O
us	O
with	O
recordings	O
where	O
participants	O
were	O
reading	O
(	O
word	O
-	O
list	O
,	O
sentences	O
,	O
passage	O
)	O
.	O
Given	O
word	O
transcriptions	O
,	O
we	O
used	O
the	O
Montreal	O
Forced	O
Aligner	O
(	O
McAuliffe	O
et	O
al	O
,	O
2017	O
)	O
and	O
modified	O
Praat	O
scripts	O
(	O
DiCanio	O
,	O
2014	O
;	O
Koops	O
,	O
2013	O
)	O
to	O
align	O
and	O
extract	O
vowel+	O
(	O
r	O
)	O
sequences	O
,	O
e.g.	O
,	O
park	O
,	O
short	O
.	O
However	O
,	O
note	O
that	O
because	O
non	O
-	O
rhotic	O
dialects	O
are	O
less	O
common	O
,	O
and	O
some	O
of	O
our	O
recordings	O
had	O
background	O
noise	O
,	O
it	O
could	O
be	O
possible	O
that	O
alignments	O
were	O
not	O
perfect	O
for	O
all	O
of	O
our	O
tokens	O
.	O
Two	O
human	O
analysts	O
listened	O
to	O
recordings	O
and	O
judged	O
each	O
vowel+	O
(	O
r	O
)	O
token	O
as	O
r	O
-	O
ful	O
or	O
r	O
-	O
less	O
.	O
The	O
human	O
analysts	O
agreed	O
on	O
89.9	O
%	O
of	O
the	O
tokens	O
,	O
similar	O
to	O
human	O
agreement	O
elsewhere	O
(	O
Nagy	O
and	O
Irwin	O
,	O
2010	O
)	O
.	O
Like	O
other	O
studies	O
,	O
we	O
omitted	O
tokens	O
when	O
the	O
human	O
analysts	O
disagreed	O
(	O
10	O
%	O
)	O
.	O
So	O
overall	O
,	O
1700	O
tokens	O
were	O
discarded	O
because	O
of	O
speaker	O
disagreement	O
,	O
and	O
6500	O
rhotic	O
tokens	O
and	O
5300	O
non	O
-	O
rhotic	O
tokens	O
remained	O
for	O
analysis	O
.	O

We	O
are	O
grateful	O
for	O
the	O
anonymous	O
reviewers	O
of	O
EMNLP	O
who	O
gave	O
us	O
very	O
valuable	O
comments	O
and	O
suggestions	O
.	O

A	O
Yes	O
/	O
no	O
/	O
maybe	O
Answerability	O
Not	O
all	O
naturally	O
occuring	O
question	O
titles	O
from	O
PubMed	O
are	O
answerable	O
by	O
yes	O
/	O
no	O
/	O
maybe	O
.	O
The	O
first	O
step	O
of	O
annotating	O
PQA	O
-	O
L	O
(	O
as	O
shown	O
in	O
algorithm	O
1	O
)	O
from	O
pre	O
-	O
PQA	O
-	O
U	O
is	O
to	O
manually	O
identify	O
questions	O
that	O
can	O
be	O
answered	O
using	O
yes	O
/	O
no	O
/	O
maybe	O
.	O
We	O
labeled	O
1091	O
(	O
about	O
50.2	O
%	O
)	O
of	O
2173	O
question	O
titles	O
as	O
unanswerable	O
.	O
For	O
example	O
,	O
those	O
questions	O
can	O
not	O
be	O
answered	O
by	O
yes	O
/	O
no	O
/	O
maybe	O
:	O
"	O
Critical	O
Overview	O
of	O
HER2	O
Assessement	O
in	O
Bladder	O
Cancer	O
:	O
What	O
Is	O
Missing	O
for	O
a	O
Better	O
Therapeutic	O
Approach	O
?	O
"	O
(	O
wh	O
-	O
question	O
)	O
"	O
Otolaryngology	O
externships	O
and	O
the	O
match	O
:	O
Productive	O
or	O
futile	O
?	O
"	O
(	O
multiple	O
choices	O
)	O

Strictly	O
speaking	O
,	O
most	O
yes	O
/	O
no	O
/	O
maybe	O
research	O
questions	O
can	O
be	O
answered	O
by	O
"	O
maybe	O
"	O
since	O
there	O
will	O
always	O
be	O
some	O
conditions	O
where	O
one	O
statement	O
is	O
true	O
and	O
vice	O
versa	O
.	O
However	O
,	O
the	O
task	O
will	O
be	O
trivial	O
in	O
this	O
case	O
.	O
Instead	O
,	O
we	O
annotate	O
a	O
question	O
using	O
"	O
yes	O
"	O
if	O
the	O
experiments	O
and	O
results	O
in	O
the	O
paper	O
indicate	O
it	O
,	O
so	O
the	O
answer	O
is	O
not	O
universal	O
but	O
context	O
-	O
dependent	O
.	O
Given	O
a	O
question	O
like	O
"	O
Do	O
patients	O
benefit	O
from	O
drug	O
X	O
?	O
"	O
:	O
certainly	O
not	O
all	O
patients	O
will	O
benefit	O
from	O
it	O
,	O
but	O
if	O
there	O
is	O
a	O
significant	O
difference	O
in	O
an	O
outcome	O
between	O
the	O
experimental	O
and	O
control	O
group	O
,	O
the	O
answer	O
will	O
be	O
"	O
yes	O
"	O
.	O
If	O
there	O
is	O
not	O
,	O
the	O
answer	O
will	O
be	O
"	O
no	O
"	O
.	O
"	O
Maybe	O
"	O
is	O
annotated	O
when	O
(	O
1	O
)	O
the	O
paper	O
discusses	O
conditions	O
where	O
the	O
answer	O
is	O
True	O
and	O
conditions	O
where	O
the	O
answer	O
is	O
False	O
or	O
(	O
2	O
)	O
more	O
than	O
one	O
intervention	O
/	O
observation	O
/	O
etc	O
.	O
is	O
asked	O
,	O
and	O
the	O
answer	O
is	O
True	O
for	O
some	O
but	O
False	O
for	O
the	O
others	O
(	O
e.g.	O
:	O
"	O
Do	O
Disease	O
A	O
,	O
Disease	O
B	O
and/or	O
Disease	O
C	O
benefit	O
from	O
drug	O
X	O
?	O
"	O
)	O
.	O
To	O
model	O
uncertainty	O
of	O
the	O
answer	O
,	O
we	O
do	O
n't	O
strictly	O
follow	O
the	O
logic	O
calculations	O
where	O
such	O
questions	O
can	O
always	O
be	O
answered	O
by	O
either	O
"	O
yes	O
"	O
or	O
"	O
no	O
"	O
.	O

An	O
Expert	O
Annotated	O
Dataset	O
for	O
the	O
Detection	O
of	O
Online	O
Misogyny	O

We	O
developed	O
a	O
hierarchical	O
taxonomy	O
with	O
three	O
levels	O
.	O
First	O
,	O
we	O
make	O
a	O
binary	O
distinction	O
between	O
Misogynistic	O
content	O
and	O
Non	O
-	O
misogynistic	O
content	O
,	O
which	O
are	O
mutually	O
exclusive	O
.	O
Second	O
,	O
we	O
elaborated	O
subtypes	O
of	O
Misogynistic	O
and	O
Nonmisogynistic	O
content	O
.	O
For	O
Misogynistic	O
content	O
we	O
defined	O
four	O
categories	O
:	O
(	O
i	O
)	O
Misogynistic	O
Pejoratives	O
,	O
(	O
ii	O
)	O
descriptions	O
of	O
Misogynistic	O
Treatment	O
,	O
(	O
iii	O
)	O
acts	O
of	O
Misogynistic	O
Derogation	O
and	O
(	O
iv	O
)	O
Gendered	O
Personal	O
attacks	O
against	O
women	O
.	O
For	O
Nonmisogynistic	O
content	O
we	O
defined	O
three	O
categories	O
:	O
(	O
i	O
)	O
Counter	O
speech	O
against	O
misogyny	O
,	O
(	O
ii	O
)	O
Nonmisogynistic	O
personal	O
attacks	O
and	O
(	O
iii	O
)	O
None	O
of	O
the	O
categories	O
.	O
Third	O
,	O
we	O
included	O
additional	O
flags	O
for	O
some	O
of	O
the	O
second	O
level	O
categories	O
.	O
Within	O
both	O
Misogynistic	O
and	O
Non	O
-	O
misogynistic	O
content	O
,	O
the	O
second	O
level	O
categories	O
are	O
not	O
mutually	O
exclusive	O
,	O
thereby	O
allowing	O
for	O
multiple	O
labels	O
per	O
entry	O
.	O
For	O
instance	O
,	O
a	O
Misogynistic	O
entry	O
could	O
be	O
assigned	O
labels	O
for	O
both	O
a	O
Pejorative	O
and	O
Treatment	O
.	O
This	O
taxonomy	O
draws	O
on	O
the	O
typologies	O
of	O
abuse	O
presented	O
by	O
Waseem	O
et	O
al	O
(	O
2017	O
)	O
and	O
Vidgen	O
et	O
al	O
(	O
2019	O
)	O
as	O
well	O
as	O
theoretical	O
work	O
in	O
online	O
misogyny	O
research	O
(	O
Filipovic	O
,	O
2007	O
;	O
Mantilla	O
,	O
2013	O
;	O
Jane	O
,	O
2016	O
;	O
Ging	O
,	O
2017	O
;	O
Ging	O
and	O
Siapera	O
,	O
2019	O
;	O
Farrell	O
et	O
al	O
,	O
2019	O
)	O
.	O
It	O
was	O
developed	O
by	O
reviewing	O
existing	O
literature	O
on	O
online	O
misogyny	O
and	O
then	O
iterating	O
over	O
small	O
samples	O
of	O
the	O
dataset	O
.	O
This	O
deductive	O
-	O
inductive	O
process	O
allowed	O
us	O
to	O
ensure	O
that	O
conceptually	O
distinct	O
varieties	O
of	O
abuse	O
are	O
separated	O
and	O
that	O
different	O
types	O
of	O
misogyny	O
can	O
be	O
unpicked	O
.	O
This	O
is	O
important	O
given	O
that	O
they	O
can	O
have	O
very	O
different	O
impacts	O
on	O
victims	O
,	O
different	O
causes	O
,	O
and	O
reflect	O
different	O
outlooks	O
and	O
interests	O
on	O
the	O
part	O
of	O
the	O
speaker	O
.	O

Misogynistic	O
content	O
directs	O
abuse	O
at	O
women	O
or	O
a	O
closely	O
related	O
gendered	O
group	O
(	O
e.g.	O
feminists	O
)	O
.	O
This	O
content	O
can	O
fall	O
in	O
to	O
four	O
non	O
-	O
mutually	O
exclusive	O
categories	O
.	O

Misogynistic	O
pejoratives	O
are	O
terms	O
which	O
are	O
used	O
to	O
disparage	O
women	O
.	O
It	O
includes	O
terms	O
which	O
are	O
explicitly	O
insulting	O
and	O
derogatory	O
,	O
such	O
as	O
'	O
slut	O
'	O
or	O
'	O
whore	O
'	O
,	O
as	O
well	O
as	O
terms	O
which	O
implicitly	O
express	O
negativity	O
or	O
animosity	O
against	O
women	O
,	O
such	O
as	O
'	O
Stacy	O
'	O
or	O
'	O
Becky	O
'	O
.	O
For	O
example	O
,	O
'	O
Stacy	O
'	O
is	O
a	O
term	O
used	O
in	O
the	O
incel	O
community	O
to	O
describe	O
women	O
considered	O
attractive	O
and	O
unattainable	O
,	O
in	O
opposition	O
to	O
a	O
more	O
average	O
and	O
attainable	O
'	O
Becky	O
'	O
(	O
Jennings	O
,	O
2018	O
)	O
.	O

Misogynistic	O
treatment	O
is	O
content	O
that	O
discusses	O
,	O
advocates	O
,	O
incites	O
or	O
plans	O
negative	O
or	O
harmful	O
treatment	O
of	O
women	O
.	O
It	O
includes	O
expressing	O
intent	O
to	O
take	O
action	O
against	O
women	O
,	O
as	O
well	O
as	O
expressing	O
desires	O
about	O
how	O
they	O
should	O
be	O
treated	O
.	O
Misogynistic	O
treatment	O
contains	O
third	O
-	O
level	O
subcategories	O
:	O
Threatening	O
language	O
and	O
Disrespectful	O
actions	O
.	O
1	O
.	O
Threatening	O
language	O
:	O
Content	O
which	O
expresses	O
an	O
intent	O
/	O
desire	O
to	O
inflict	O
/	O
cause	O
women	O
to	O
suffer	O
harm	O
,	O
or	O
expresses	O
support	O
for	O
,	O
encourages	O
,	O
advocates	O
or	O
incites	O
such	O
harm	O
.	O
It	O
is	O
an	O
'	O
explicit	O
'	O
form	O
of	O
abuse	O
.	O
It	O
falls	O
in	O
to	O
three	O
thematic	O
groups	O
:	O
(	O
a	O
)	O
Physical	O
violence	O
:	O
non	O
-	O
sexual	O
physical	O
violence	O
such	O
as	O
killing	O
,	O
maiming	O
,	O
beating	O
,	O
etc	O
.	O
e.g.	O
'	O
Feminists	O
deserve	O
to	O
be	O
shot	O
'	O
.	O
(	O
b	O
)	O
Sexual	O
violence	O
:	O
explicit	O
sexual	O
violence	O
such	O
as	O
rape	O
,	O
penetration	O
,	O
molestation	O
,	O
etc	O
.	O
e.g.	O
'	O
Someone	O
should	O
rape	O
her	O
-	O
that	O
would	O
put	O
her	O
in	O
her	O
place	O
'	O
.	O
(	O
c	O
)	O
Privacy	O
:	O
an	O
invasion	O
of	O
privacy	O
such	O
as	O
the	O
disclosure	O
of	O
personal	O
information	O
(	O
i.e.	O
doxing	O
)	O
or	O
threats	O
to	O
visit	O
them	O
.	O
e.g.	O
'	O
I	O
know	O
where	O
you	O
live	O
,	O
bitch	O
'	O
.	O

Content	O
which	O
treats	O
/	O
portrays	O
women	O
as	O
either	O
lacking	O
or	O
not	O
deserving	O
independence	O
/	O
autonomy	O
.	O
This	O
includes	O
more	O
subtly	O
abusive	O
statements	O
about	O
how	O
women	O
should	O
be	O
treated	O
and	O
what	O
they	O
should	O
be	O
allowed	O
to	O
do	O
.	O
It	O
is	O
an	O
'	O
implicit	O
'	O
form	O
of	O
abuse	O
.	O
It	O
falls	O
in	O
to	O
four	O
thematic	O
groups	O
:	O
(	O
a	O
)	O
Controlling	O
:	O
suggesting	O
or	O
stating	O
that	O
women	O
should	O
be	O
controlled	O
in	O
some	O
way	O
,	O
especially	O
by	O
a	O
man	O
or	O
men	O
.	O
E.g.	O
'	O
I	O
would	O
never	O
let	O
my	O
girlfriend	O
do	O
that	O
'	O
.	O
(	O
b	O
)	O
Manipulation	O
:	O
using	O
or	O
advocating	O
the	O
use	O
of	O
tactics	O
such	O
as	O
lying	O
and	O
gaslighting	O
to	O
manipulate	O
what	O
women	O
do	O
or	O
think	O
.	O
E.g.	O
'	O
Told	O
my	O
last	O
girlfriend	O
she	O
was	O
hallucinating	O
when	O
she	O
saw	O
the	O
texts	O
from	O
my	O
side	O
piece	O
'	O
.	O
(	O
c	O
)	O
Seduction	O
and	O
conquest	O
:	O
discussing	O
woman	O
solely	O
as	O
sexual	O
conquests	O
or	O
describing	O
previous	O
incidences	O
of	O
when	O
they	O
have	O
been	O
treated	O
as	O
such	O
.	O
E.g.	O
'	O
Got	O
her	O
home	O
and	O
used	O
her	O
so	O
hard	O
'	O
.	O
(	O
d	O
)	O
Other	O
:	O
content	O
that	O
is	O
not	O
covered	O
by	O
the	O
other	O
subcategories	O
.	O

Misogynistic	O
derogation	O
is	O
content	O
that	O
demeans	O
or	O
belittles	O
women	O
.	O
This	O
content	O
can	O
be	O
explicitly	O
or	O
implicitly	O
abusive	O
.	O
It	O
is	O
separated	O
into	O
third	O
-	O
level	O
subcategories	O
:	O
1	O
.	O
Intellectual	O
inferiority	O
:	O
making	O
negative	O
judgements	O
of	O
women	O
's	O
intellectual	O
abilities	O
,	O
such	O
as	O
a	O
lack	O
of	O
critical	O
thinking	O
or	O
emotional	O
control	O
.	O
This	O
includes	O
content	O
which	O
infantilizes	O
women	O
.	O
An	O
implicit	O
example	O
would	O
be	O
'	O
My	O
gf	O
cries	O
at	O
the	O
stupidest	O
shit	O
-	O
lol	O
!	O
'	O
for	O
suggesting	O
irrational	O
emotional	O
responses	O
.	O
An	O
explicit	O
example	O
is	O
'	O
Typical	O
stupid	O
bitchtalking	O
about	O
things	O
she	O
does	O
n't	O
understand	O
'	O
.	O
2	O
.	O
Moral	O
inferiority	O
:	O
making	O
negative	O
judgements	O
of	O
women	O
's	O
moral	O
worth	O
,	O
such	O
as	O
suggesting	O
they	O
are	O
deficient	O
or	O
lesser	O
to	O
men	O
in	O
some	O
way	O
.	O
This	O
includes	O
subjects	O
such	O
as	O
superficiality	O
(	O
e.g.	O
only	O
liking	O
men	O
who	O
are	O
rich	O
or	O
attractive	O
)	O
,	O
promiscuity	O
,	O
and	O
untrustworthiness	O
.	O
An	O
implicit	O
example	O
is	O
'	O
Girls	O
love	O
your	O
money	O
more	O
than	O
you	O
'	O
.	O
An	O
explicit	O
example	O
is	O
'	O
My	O
ex	O
-	O
girlfriend	O
was	O
a	O
whore	O
,	O
she	O
slept	O
with	O
every	O
guy	O
she	O
saw	O
'	O
.	O
3	O
.	O
Sexual	O
and/or	O
physical	O
limitations	O
:	O
making	O
negative	O
judgements	O
of	O
women	O
's	O
physical	O
and/or	O
sexual	O
ability	O
.	O
This	O
includes	O
perceived	O
unattractiveness	O
(	O
i.e.	O
a	O
lack	O
of	O
sexual	O
desirability	O
)	O
,	O
ugliness	O
(	O
i.e.	O
a	O
lack	O
of	O
beauty	O
)	O
,	O
frigidness	O
(	O
i.e.	O
a	O
lack	O
of	O
sexual	O
willingness	O
)	O
,	O
as	O
well	O
as	O
belittling	O
statements	O
about	O
feminine	O
physical	O
weakness	O
.	O
An	O
implicit	O
example	O
is	O
'	O
I	O
gave	O
it	O
my	O
A	O
-	O
game	O
but	O
she	O
would	O
not	O
give	O
in	O
,	O
so	O
uptight	O
!	O
'	O
An	O
explicit	O
example	O
is	O
'	O
Yikes	O
,	O
Dianne	O
Abbott	O
looks	O
like	O
a	O
monkey	O
!	O
'	O
4	O
.	O
Other	O
:	O
content	O
that	O
is	O
not	O
covered	O
by	O
the	O
other	O
subcategories	O
but	O
is	O
derogatory	O
towards	O
women	O
.	O

Gender	O
personal	O
attacks	O
are	O
highly	O
gendered	O
attacks	O
and	O
insults	O
.	O
This	O
category	O
is	O
used	O
only	O
when	O
the	O
nature	O
of	O
the	O
abuse	O
is	O
misogynistic	O
,	O
e.g.	O
'	O
Hilary	O
Clinton	O
is	O
such	O
a	O
stupid	O
bitch	O
,	O
someone	O
should	O
give	O
her	O
a	O
good	O
fucking	O
and	O
put	O
her	O
in	O
her	O
place	O
'	O
.	O
The	O
category	O
has	O
a	O
level	O
three	O
flag	O
for	O
the	O
gender	O
of	O
the	O
recipient	O
of	O
the	O
abuse	O
.	O
We	O
include	O
this	O
flag	O
as	O
research	O
has	O
shown	O
that	O
men	O
can	O
also	O
be	O
targeted	O
by	O
misogynistic	O
attacks	O
(	O
Jane	O
,	O
2014	O
)	O
.	O
The	O
gender	O
can	O
either	O
be	O
a	O
woman	O
(	O
e.g.	O
'	O
That	O
chick	O
is	O
dumb	O
'	O
)	O
,	O
a	O
man	O
(	O
e.g.	O
'	O
This	O
dude	O
is	O
a	O
piece	O
of	O
shit	O
'	O
)	O
or	O
unknown	O
(	O
e.g.	O
'	O
You	O
're	O
are	O
an	O
idiot	O
,	O
fuck	O
off	O
'	O
)	O
.	O
If	O
the	O
content	O
was	O
replying	O
to	O
an	O
entry	O
which	O
reveals	O
the	O
recipient	O
's	O
gender	O
we	O
can	O
infer	O
it	O
from	O
this	O
context	O
.	O
For	O
example	O
if	O
'	O
You	O
're	O
an	O
idiot	O
,	O
fuck	O
off	O
'	O
was	O
a	O
response	O
to	O
'	O
I	O
'm	O
a	O
man	O
and	O
a	O
feminist	O
there	O
's	O
nothing	O
contradictory	O
about	O
that	O
'	O
we	O
know	O
the	O
abuse	O
is	O
targeted	O
at	O
a	O
man	O
.	O

Non	O
-	O
misogynistic	O
content	O
can	O
fall	O
in	O
to	O
three	O
nonmutually	O
exclusive	O
categories	O
,	O
all	O
of	O
which	O
are	O
relevant	O
for	O
misogyny	O
research	O
.	O

Interpersonal	O
abuse	O
which	O
is	O
not	O
misogynistic	O
.	O
We	O
include	O
this	O
category	O
to	O
allow	O
for	O
a	O
comparison	O
of	O
the	O
nature	O
of	O
abuse	O
directed	O
at	O
women	O
and	O
men	O
(	O
Duggan	O
,	O
2017	O
)	O
.	O
It	O
includes	O
content	O
which	O
personally	O
attacks	O
a	O
woman	O
but	O
is	O
not	O
misogynistic	O
in	O
nature	O
,	O
e.g.	O
'	O
Hilary	O
Clinton	O
has	O
no	O
clue	O
what	O
she	O
's	O
talking	O
about	O
,	O
idiot	O
!	O
'	O
.	O
It	O
uses	O
the	O
same	O
level	O
three	O
flag	O
for	O
the	O
gender	O
of	O
the	O
recipient	O
as	O
Misogynistic	O
personal	O
attack	O
.	O
This	O
allows	O
us	O
to	O
compare	O
the	O
rates	O
of	O
personal	O
attacks	O
against	O
women	O
and	O
men	O
.	O
Note	O
that	O
although	O
it	O
is	O
possible	O
for	O
an	O
entry	O
to	O
contain	O
both	O
Misogyny	O
and	O
a	O
Non	O
-	O
misogynistic	O
personal	O
attack	O
,	O
this	O
was	O
very	O
rare	O
.	O
In	O
such	O
cases	O
,	O
we	O
chose	O
to	O
not	O
annotate	O
the	O
Non	O
-	O
misogynistic	O
personal	O
attack	O
in	O
order	O
to	O
keep	O
the	O
first	O
level	O
as	O
a	O
binary	O
distinction	O
.	O

Counter	O
speech	O
is	O
content	O
which	O
challenges	O
,	O
refutes	O
,	O
and	O
puts	O
into	O
question	O
previous	O
misogynistic	O
abuse	O
in	O
a	O
thread	O
.	O
It	O
could	O
directly	O
criticise	O
previous	O
abuse	O
(	O
e.g.	O
'	O
What	O
you	O
said	O
is	O
unacceptable	O
'	O
)	O
,	O
specifically	O
accuse	O
it	O
of	O
prejudice	O
(	O
e.g.	O
'	O
That	O
's	O
incredibly	O
sexist	O
'	O
)	O
,	O
or	O
offer	O
a	O
different	O
perspective	O
which	O
challenges	O
the	O
misogyny	O
(	O
e.g	O
'	O
That	O
's	O
not	O
how	O
women	O
act	O
,	O
you	O
're	O
so	O
wrong	O
'	O
)	O
.	O

Fleiss	O
Our	O
taxonomy	O
has	O
seven	O
partially	O
overlapping	O
categories	O
,	O
and	O
as	O
such	O
annotation	O
is	O
considerably	O
more	O
difficult	O
compared	O
with	O
most	O
prior	O
work	O
,	O
which	O
tends	O
to	O
involve	O
only	O
binary	O
labelling	O
.	O
As	O
such	O
,	O
whilst	O
slightly	O
low	O
,	O
we	O
believe	O
that	O
our	O
agreement	O
scores	O
show	O
the	O
robustness	O
of	O
our	O
annotation	O
approach	O
.	O
Further	O
,	O
all	O
disagreements	O
were	O
then	O
discussed	O
with	O
an	O
expert	O
adjudicator	O
,	O
meaning	O
that	O
points	O
of	O
disagreement	O
were	O
addressed	O
before	O
the	O
final	O
labels	O
were	O
determined	O
.	O

Of	O
the	O
6	O
,	O
567	O
agreed	O
labels	O
in	O
the	O
final	O
dataset	O
10.6	O
%	O
are	O
Misogynistic	O
(	O
n=699	O
)	O
and	O
89.4	O
%	O
are	O
868	O
)	O
.	O
Tables	O
2	O
and	O
3	O
show	O
the	O
number	O
of	O
labels	O
in	O
the	O
final	O
dataset	O
for	O
each	O
of	O
the	O
Misogynistic	O
and	O
Non	O
-	O
misogynistic	O
categories	O
,	O
broken	O
down	O
by	O
the	O
level	O
two	O
categories	O
.	O
The	O
vast	O
majority	O
of	O
entries	O
fall	O
under	O
None	O
of	O
the	O
categories	O
(	O
88.6	O
%	O
of	O
all	O
labels	O
)	O
.	O
The	O
next	O
most	O
common	O
category	O
is	O
Misogynistic	O
Pejoratives	O
followed	O
by	O
Misogynistic	O
Derogation	O
pejoratives	O
(	O
4.2	O
%	O
)	O
.	O
There	O
are	O
relatively	O
few	O
labels	O
for	O
Personal	O
attacks	O
with	O
just	O
0.7	O
%	O
in	O
total	O
for	O
each	O
of	O
the	O
Misogynistic	O
and	O
Nonmisogynistic	O
categories	O
,	O
respectively	O
.	O
The	O
least	O
common	O
category	O
is	O
Counter	O
speech	O
against	O
misogyny	O
,	O
with	O
only	O
ten	O
cases	O
(	O
0.2	O
%	O
)	O
.	O

Number	O

Annotators	O
identified	O
at	O
least	O
one	O
misogynistic	O
pejorative	O
in	O
4.2	O
%	O
of	O
all	O
entries	O
.	O
The	O
most	O
common	O
misogynistic	O
term	O
in	O
the	O
labels	O
is	O
'	O
bitch	O
'	O
(	O
n=43	O
)	O
followed	O
by	O
'	O
stacy	O
'	O
(	O
24	O
)	O
and	O
'	O
stacies	O
'	O
(	O
21	O
)	O
.	O

There	O
are	O
103	O
labels	O
of	O
Treatment	O
.	O
Figure	O
1	O
shows	O
the	O
number	O
of	O
labels	O
for	O
each	O
level	O
three	O
subcategory	O
.	O
There	O
are	O
almost	O
five	O
times	O
as	O
many	O
labels	O
for	O
Disrespectful	O
actions	O
(	O
n=85	O
)	O
than	O
Threatening	O
language	O
(	O
n=18	O
)	O
.	O
Both	O
level	O
three	O
subcategories	O
were	O
broken	O
down	O
into	O
more	O
specific	O
misogynistic	O
themes	O
.	O
Within	O
Disrespectful	O
actions	O
,	O
Seduction	O
and	O
conquest	O
is	O
the	O
most	O
common	O
topic	O
,	O
with	O
twice	O
as	O
many	O
labels	O
as	O
the	O
second	O
most	O
common	O
,	O
Controlling	O
(	O
43	O
vs	O
17	O
)	O
.	O
And	O
,	O
within	O
Threatening	O
language	O
,	O
Physical	O
violence	O
was	O
the	O
most	O
common	O
theme	O
(	O
13	O
Sexual	O
violence	O
and	O
Invasion	O
of	O
privacy	O
only	O
have	O
a	O
couple	O
of	O
labels	O
each	O
(	O
three	O
and	O
two	O
,	O
respectively	O
)	O
.	O

The	O
are	O
286	O
Derogation	O
labels	O
.	O

Table	O
5	O
shows	O
the	O
breakdown	O
of	O
both	O
Misogynistic	O
and	O
Nonmisogynistic	O
personal	O
attacks	O
.	O
Slightly	O
more	O
than	O
half	O
(	O
55	O
%	O
)	O
of	O
interpersonal	O
abuse	O
was	O
not	O
misogynistic	O
.	O
Of	O
these	O
women	O
were	O
still	O
the	O
target	O
of	O
the	O
abuse	O
almost	O
four	O
times	O
as	O
often	O
as	O
men	O
(	O
n=32	O
vs	O
n=8	O
)	O
.	O
And	O
women	O
were	O
as	O
likely	O
to	O
receive	O
misogynistic	O
person	O
attacks	O
as	O
non	O
-	O
misogynistic	O
ones	O
(	O
n=32	O
)	O
.	O
The	O
gender	O
of	O
the	O
target	O
was	O
only	O
unknown	O
in	O
5	O
%	O
of	O
cases	O
,	O
one	O
misogynistic	O
and	O
three	O
not	O
.	O
There	O
were	O
two	O
cases	O
of	O
misogynistic	O
abuse	O
against	O
men	O
.	O
All	O
other	O
misogynistic	O
personal	O
attacks	O
were	O
towards	O
women	O
.	O

There	O
are	O
only	O
10	O
cases	O
of	O
Counter	O
speech	O
in	O
the	O
final	O
dataset	O
of	O
agreed	O
labels	O
.	O
Annotators	O
originally	O
identified	O
far	O
more	O
counter	O
speech	O
(	O
188	O
labels	O
for	O
149	O
unique	O
entries	O
were	O
initially	O
made	O
)	O
but	O
few	O
were	O
accepted	O
during	O
the	O
adjudication	O
meetings	O
.	O
In	O
Section	O
5.2	O
we	O
showed	O
that	O
the	O
category	O
has	O
one	O
of	O
the	O
lowest	O
Kappa	O
values	O
.	O
Notably	O
,	O
39	O
%	O
of	O
original	O
Counter	O
speech	O
labels	O
were	O
made	O
by	O
one	O
annotator	O
,	O
showing	O
that	O
the	O
annotators	O
had	O
different	O
understandings	O
of	O
the	O
threshold	O
for	O
Counter	O
speech	O
.	O
However	O
,	O
the	O
number	O
of	O
original	O
labels	O
for	O
Counter	O
speech	O
decreased	O
over	O
the	O
first	O
few	O
weeks	O
of	O
the	O
annotation	O
process	O
,	O
as	O
shown	O
in	O
Figure	O
2	O
.	O
This	O
reflects	O
the	O
complexity	O
of	O
the	O
category	O
;	O
it	O
took	O
annotators	O
time	O
to	O
differentiate	O
content	O
that	O
was	O
pro	O
-	O
women	O
from	O
that	O
which	O
actually	O
countered	O
previous	O
misogynistic	O
speech	O
.	O

Of	O
the	O
86	O
entries	O
misclassified	O
as	O
Misogynistic	O
,	O
three	O
are	O
labelled	O
as	O
Nonmisogynistic	O
personal	O
attacks	O
.	O
An	O
example	O
is	O
"	O
Male	O
feminist	O
reacts	O
to	O
vile	O
scumbag	O
who	O
murdered	O
his	O
kids	O
by	O
telling	O
ALL	O
men	O
to	O
,	O
you	O
know	O
,	O
stop	O
killing	O
women	O
and	O
kids	O
...	O
"	O
.	O
The	O
use	O
of	O
aggressive	O
language	O
combined	O
with	O
gendered	O
phrases	O
(	O
such	O
as	O
"	O
stop	O
killing	O
women	O
"	O
)	O
likely	O
led	O
to	O
its	O
misclassification	O
.	O
The	O
remaining	O
83	O
false	O
positives	O
fall	O
under	O
None	O
of	O
the	O
categories	O
and	O
all	O
contain	O
some	O
reference	O
to	O
women	O
.	O
Some	O
refer	O
to	O
topics	O
often	O
associated	O
with	O
misogyny	O
but	O
are	O
not	O
misogynistic	O
in	O
themselves	O
.	O
For	O
example	O
,	O
a	O
comment	O
in	O
r	O
/	O
seduction	O
stated	O
,	O
"	O
the	O
most	O
manly	O
thing	O
is	O
to	O
find	O
your	O
dream	O
woman	O
,	O
marry	O
her	O
,	O
and	O
live	O
happily	O
ever	O
after	O
.	O
The	O
constant	O
sex	O
with	O
women	O
is	O
so	O
overrated	O
anyways	O
"	O
.	O
This	O
entry	O
,	O
which	O
suggests	O
that	O
other	O
things	O
than	O
high	O
levels	O
of	O
sexual	O
activity	O
should	O
be	O
prioritised	O
,	O
is	O
thematically	O
similar	O
to	O
misogynistic	O
content	O
in	O
the	O
dataset	O
.	O
Other	O
false	O
positives	O
mention	O
women	O
indirectly	O
.	O
"	O
Because	O
they	O
are	O
n't	O
men	O
,	O
they	O
are	O
SIMPS	O
"	O
.	O
'	O
Simp	O
'	O
is	O
a	O
pejorative	O
term	O
used	O
in	O
the	O
manosphere	O
for	O
a	O
man	O
who	O
cares	O
too	O
much	O
about	O
a	O
woman	O
.	O
Under	O
our	O
taxonomy	O
it	O
did	O
not	O
count	O
as	O
a	O
misogynistic	O
pejorative	O
but	O
it	O
is	O
likely	O
that	O
the	O
term	O
appears	O
in	O
misogynistic	O
entries	O
in	O
the	O
dataset	O
.	O
Some	O
false	O
positives	O
are	O
critical	O
of	O
misogyny	O
,	O
though	O
not	O
actively	O
enough	O
to	O
count	O
as	O
Counter	O
speech	O
.	O
For	O
example	O
"	O
Does	O
this	O
moid	O
even	O
know	O
the	O
meaning	O
of	O
the	O
term	O
'	O
butterface	O
'	O
?	O
If	O
this	O
woman	O
is	O
ugly	O
,	O
there	O
is	O
no	O
hope	O
for	O
most	O
of	O
the	O
female	O
population	O
.	O
"	O
.	O
This	O
discussion	O
of	O
unrealistic	O
beauty	O
standards	O
of	O
women	O
references	O
misogyny	O
but	O
is	O
not	O
itself	O
misogynistic	O
.	O

Of	O
the	O
51	O
Misogynistic	O
entries	O
the	O
model	O
misses	O
,	O
almost	O
half	O
(	O
n=24	O
)	O
contain	O
Derogation	O
.	O
Implicit	O
and	O
explicit	O
derogation	O
are	O
missed	O
at	O
roughly	O
similar	O
rates	O
,	O
as	O
are	O
each	O
of	O
the	O
subcategories	O
.	O
Importantly	O
this	O
shows	O
that	O
the	O
different	O
forms	O
of	O
derogation	O
are	O
no	O
more	O
or	O
less	O
likely	O
to	O
be	O
missed	O
.	O

All	O
annotators	O
were	O
based	O
in	O
the	O
United	O
Kingdom	O
and	O
worked	O
remotely	O
.	O
They	O
were	O
paid	O
£	O
14	O
per	O
hour	O
for	O
all	O
work	O
including	O
training	O
.	O
Five	O
of	O
the	O
six	O
annotators	O
gave	O
permission	O
to	O
share	O
their	O
basic	O
demographic	O
information	O
.	O
All	O
were	O
between	O
18	O
and	O
29	O
years	O
old	O
.	O
Two	O
had	O
high	O
school	O
degrees	O
,	O
two	O
had	O
an	O
undergraduate	O
degree	O
,	O
and	O
one	O
had	O
a	O
postgraduate	O
taught	O
degree	O
or	O
equivalent	O
.	O
Four	O
identified	O
as	O
women	O
,	O
one	O
as	O
a	O
man	O
.	O
All	O
were	O
British	O
nationals	O
,	O
native	O
English	O
speakers	O
,	O
and	O
identified	O
as	O
ethnically	O
white	O
.	O
All	O
annotators	O
used	O
social	O
media	O
at	O
least	O
once	O
per	O
day	O
.	O
Two	O
had	O
never	O
been	O
personally	O
targeted	O
by	O
online	O
abuse	O
,	O
two	O
had	O
been	O
targeted	O
2	O
-	O
3	O
times	O
(	O
in	O
separate	O
instances	O
more	O
than	O
a	O
year	O
ago	O
)	O
,	O
and	O
one	O
had	O
been	O
personally	O
targeted	O
more	O
than	O
3	O
times	O
within	O
the	O
previous	O
month	O
.	O

Table	O
9	O
lists	O
the	O
subreddits	O
used	O
for	O
target	O
sampling	O
of	O
data	O
.	O
The	O
columns	O
Num	O
entries	O
and	O
Num	O
threads	O
state	O
how	O
many	O
individual	O
entries	O
and	O
threads	O
from	O
each	O
subreddit	O
are	O
in	O
the	O
datasets	O
.	O
The	O
column	O
Selection	O
shows	O
whether	O
the	O
subreddit	O
was	O
identified	O
from	O
existing	O
literature	O
,	O
which	O
is	O
cited	O
,	O
or	O
using	O
snowball	O
sampling	O
.	O

(	O
1	O
)	O

(	O
1.1	O
)	O

This	O
research	O
was	O
funded	O
by	O
Wave	O
1	O
of	O
The	O
UKRI	O
Strategic	O
Priorities	O
Fund	O
under	O
the	O
EPSRC	O
Grant	O
EP	O
/	O
T001569/1	O
at	O
The	O
Alan	O
Turing	O
Institute	O
,	O
particularly	O
the	O
"	O
Tools	O
,	O
Practices	O
and	O
System	O
"	O
and	O
"	O
Criminal	O
Justice	O
"	O
themes	O
.	O

Where	O
Are	O
We	O
in	O
Discourse	O
Relation	O
Recognition	O
?	O

We	O
would	O
like	O
to	O
thank	O
the	O
reviewers	O
,	O
Diane	O
Litman	O
and	O
Matthew	O
Stone	O
for	O
providing	O
helpful	O
feedback	O
for	O
this	O
work	O
.	O

In	O
the	O
standard	O
paradigm	O
of	O
MNMT	O
,	O
all	O
parameters	O
are	O
shared	O
across	O
languages	O
and	O
the	O
model	O
is	O
jointly	O
trained	O
on	O
multiple	O
language	O
pairs	O
.	O
We	O
follow	O
Johnson	O
et	O
al	O
(	O
2017	O
)	O
to	O
reuse	O
standard	O
bilingual	O
NMT	O
models	O
for	O
multilingual	O
translation	O
by	O
altering	O
the	O
source	O
input	O
with	O
a	O
language	O
token	O
lang	O
,	O
i.e.	O
changing	O
x	O
to	O
x	O
=	O
(	O
lang	O
,	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
J	O
)	O
.	O

Our	O
goal	O
is	O
to	O
build	O
a	O
unified	O
model	O
,	O
which	O
can	O
achieve	O
good	O
performance	O
on	O
all	O
language	O
pairs	O
.	O
The	O
main	O
idea	O
of	O
our	O
method	O
is	O
that	O
different	O
neurons	O
have	O
different	O
importance	O
to	O
the	O
translation	O
of	O
different	O
languages	O
.	O
Based	O
on	O
this	O
,	O
we	O
divide	O
them	O
into	O
general	O
and	O
language	O
-	O
specific	O
ones	O
and	O
make	O
general	O
neurons	O
participate	O
in	O
the	O
translation	O
of	O
all	O
the	O
languages	O
while	O
language	O
-	O
specific	O
neurons	O
focus	O
on	O
some	O
specific	O
languages	O
.	O
Specifically	O
,	O
the	O
proposed	O
approach	O
involves	O
the	O
following	O
steps	O
shown	O
in	O
Figure	O
1	O
.	O
First	O
,	O
we	O
pretrain	O
the	O
model	O
on	O
the	O
combined	O
data	O
of	O
all	O
the	O
language	O
pairs	O
following	O
the	O
normal	O
paradigm	O
in	O
Johnson	O
et	O
al	O
(	O
2017	O
)	O
.	O
Second	O
,	O
we	O
evaluate	O
the	O
importance	O
of	O
different	O
neurons	O
on	O
these	O
language	O
pairs	O
and	O
allocate	O
them	O
into	O
general	O
neurons	O
and	O
language	O
-	O
specific	O
neurons	O
.	O
Last	O
,	O
we	O
fine	O
-	O
tune	O
the	O
translation	O
model	O
on	O
the	O
combined	O
data	O
again	O
.	O
It	O
should	O
be	O
noted	O
that	O
for	O
a	O
specific	O
language	O
pair	O
only	O
the	O
general	O
neurons	O
and	O
the	O
language	O
-	O
specific	O
neurons	O
for	O
this	O
language	O
pair	O
will	O
participate	O
in	O
the	O
forward	O
and	O
backward	O
computation	O
when	O
the	O
model	O
is	O
trained	O
on	O
this	O
language	O
pair	O
.	O
Other	O
neurons	O
will	O
be	O
zeroed	O
out	O
during	O
both	O
training	O
and	O
inference	O
.	O

In	O
this	O
step	O
,	O
we	O
should	O
determine	O
which	O
neurons	O
are	O
shared	O
across	O
all	O
the	O
language	O
pairs	O
and	O
which	O
neurons	O
are	O
shared	O
only	O
for	O
some	O
specific	O
language	O
pairs	O
.	O

Except	O
for	O
the	O
general	O
neurons	O
shared	O
by	O
all	O
the	O
language	O
pairs	O
,	O
our	O
method	O
allocates	O
other	O
neurons	O
to	O
different	O
language	O
pairs	O
based	O
on	O
their	O
importance	O
.	O
These	O
language	O
-	O
specific	O
neurons	O
are	O
important	O
for	O
preserving	O
the	O
language	O
-	O
specific	O
knowledge	O
.	O
To	O
better	O
understand	O
the	O
effectiveness	O
of	O
our	O
method	O
,	O
we	O
will	O
show	O
how	O
these	O
specific	O
neurons	O
are	O
distributed	O
in	O
the	O
model	O
.	O
To	O
evaluate	O
the	O
proportion	O
of	O
language	O
-	O
specific	O
neurons	O
for	O
different	O
language	O
pairs	O
at	O
each	O
layer	O
,	O
we	O
introduce	O
a	O
new	O
metric	O
,	O
LScore	O
,	O
formulated	O
as	O
:	O
LScore	O
(	O
l	O
,	O
m	O
)	O
=	O
Ĩ	O
m	O
l	O
I	O
l	O
,	O
m	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
M	O
}	O
(	O
10	O
)	O
whereĨ	O
m	O
l	O
denotes	O
the	O
number	O
of	O
neurons	O
allocated	O
to	O
language	O
pair	O
m	O
in	O
the	O
l	O
-	O
th	O
layer	O
,	O
andĨ	O
l	O
denotes	O
the	O
total	O
number	O
of	O
the	O
language	O
-	O
specific	O
neurons	O
in	O
the	O
l	O
-	O
th	O
layer	O
.	O
The	O
larger	O
the	O
LScore	O
,	O
the	O
more	O
neurons	O
allocated	O
to	O
the	O
language	O
pair	O
m.	O
We	O
also	O
introduce	O
a	O
metric	O
to	O
evaluate	O
the	O
average	O
proportion	O
of	O
language	O
-	O
specific	O
neurons	O
of	O
each	O
language	O
in	O
different	O
modules	O
,	O
which	O
formulated	O
as	O
:	O
MScore	O
(	O
l	O
,	O
f	O
)	O
=	O
1	O
M	O
M	O
m=0Ĩ	O
m	O
l	O
,	O
f	O
I	O
l	O
,	O
f	O
,	O
m	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
M	O
}	O
(	O
11	O
)	O
whereĨ	O
m	O
l	O
,	O
f	O
denotes	O
the	O
number	O
of	O
specific	O
neurons	O
for	O
language	O
pair	O
m	O
of	O
in	O
the	O
f	O
module	O
of	O
the	O
lth	O
layer	O
and	O
M	O
denotes	O
the	O
total	O
number	O
of	O
the	O
language	O
pair	O
.	O
The	O
larger	O
the	O
MScore	O
is	O
,	O
the	O
more	O
specific	O
neurons	O
are	O
allocated	O
to	O
different	O
language	O
pairs	O
in	O
this	O
module	O
.	O
As	O
shown	O
in	O
Figure	O
3	O
(	O
a	O
)	O
and	O
Figure	O
3	O
(	O
b	O
)	O
,	O
the	O
language	O
pairs	O
have	O
low	O
LScores	O
at	O
the	O
top	O
and	O
bottom	O
layers	O
and	O
high	O
LScores	O
at	O
the	O
middle	O
layers	O
of	O
both	O
the	O
encoder	O
and	O
decoder	O
.	O
The	O
highest	O
LScore	O
appears	O
at	O
the	O
third	O
or	O
fourth	O
layers	O
,	O
which	O
indicates	O
that	O
the	O
neuron	O
importance	O
of	O
different	O
language	O
pairs	O
is	O
similar	O
and	O
the	O
neurons	O
of	O
the	O
middle	O
layers	O
are	O
shared	O
by	O
more	O
languages	O
.	O
As	O
a	O
contrast	O
,	O
the	O
bottom	O
and	O
top	O
layers	O
will	O
be	O
more	O
specialized	O
for	O
different	O
language	O
pairs	O
.	O
Next	O
,	O
from	O
Figure	O
3	O
(	O
c	O
)	O
and	O
Figure	O
3	O
(	O
d	O
)	O
,	O
we	O
can	O
see	O
the	O
MScores	O
of	O
the	O
attention	O
modules	O
are	O
almost	O
near	O
1.0	O
,	O
which	O
means	O
neurons	O
in	O
self	O
attention	O
and	O
cross	O
attention	O
are	O
almost	O
shared	O
across	O
all	O
language	O
pairs	O
.	O
However	O
,	O
the	O
MScores	O
of	O
Feed	O
Forward	O
Network	O
(	O
FFN	O
)	O
gradually	O
decrease	O
as	O
layer	O
depth	O
increases	O
and	O
it	O
shows	O
that	O
the	O
higher	O
layers	O
in	O
FFN	O
are	O
more	O
essential	O
for	O
capturing	O
the	O
language	O
-	O
specific	O
knowledge	O
.	O

In	O
the	O
proposed	O
method	O
we	O
allocate	O
neurons	O
based	O
on	O
importance	O
of	O
language	O
pair	O
.	O
There	O
are	O
three	O
varieties	O
of	O
our	O
method	O
:	O
(	O
a	O
)	O
Source	O
-	O
Specific	O
,	O
share	O
all	O
neurons	O
according	O
to	O
the	O
source	O
language	O
only	O
;	O
(	O
b	O
)	O
Target	O
-	O
Specific	O
,	O
share	O
all	O
neurons	O
according	O
to	O
the	O
target	O
language	O
only	O
;	O
(	O
c	O
)	O
Separate	O
Enc	O
-	O
Dec	O
,	O
Encoder	O
neurons	O
are	O
shared	O
according	O
to	O
the	O
source	O
language	O
and	O
decoder	O
neurons	O
are	O
shared	O
according	O
to	O
the	O
target	O
language	O
.	O
Note	O
that	O
(	O
c	O
)	O
is	O
different	O
from	O
our	O
method	O
since	O
(	O
c	O
)	O
is	O
separate	O
neurons	O
to	O
two	O
parts	O
(	O
encoder	O
and	O
decoder	O
)	O
and	O
then	O
connect	O
specific	O
neurons	O
of	O
the	O
two	O
parts	O
to	O
form	O
a	O
whole	O
,	O
while	O
our	O
method	O
is	O
directly	O
based	O
on	O
language	O
pairs	O
.	O
As	O
shown	O
in	O
Figure	O
6	O
,	O
we	O
compare	O
our	O
Taylor	O
Expansion	O
method	O
with	O
the	O
other	O
three	O
varieties	O
.	O
Our	O
approach	O
outperforms	O
other	O
varieties	O
on	O
almost	O
all	O
language	O
pairs	O
,	O
and	O
the	O
performance	O
of	O
the	O
language	O
-	O
pair	O
based	O
approach	O
is	O
undoubtedly	O
the	O
best	O
.	O
The	O
second	O
is	O
based	O
on	O
the	O
target	O
language	O
and	O
the	O
source	O
language	O
.	O
Worst	O
of	O
all	O
are	O
the	O
separated	O
encoder	O
-	O
decoder	O
,	O
which	O
may	O
be	O
due	O
to	O
the	O
mismatch	O
between	O
the	O
neurons	O
of	O
the	O
encoder	O
and	O
decoder	O
when	O
they	O
are	O
reconnected	O
.	O

We	O
conducted	O
several	O
experiments	O
on	O
ρ	O
to	O
determine	O
the	O
optimal	O
hyper	O
-	O
parameter	O
,	O
so	O
as	O
to	O
determine	O
the	O
proportion	O
of	O
universal	O
neurons	O
.	O
As	O
shown	O
in	O
Table	O
3	O
,	O
when	O
ρ	O
=	O
90	O
%	O
the	O
model	O
gets	O
the	O
best	O
translation	O
result	O
and	O
reach	O
best	O
trade	O
-	O
off	O
between	O
general	O
and	O
language	O
-	O
specific	O
neurons	O
.	O

We	O
thank	O
all	O
the	O
anonymous	O
reviewers	O
for	O
their	O
insightful	O
and	O
valuable	O
comments	O
.	O
This	O
work	O
was	O
supported	O
by	O
National	O
Key	O
R&D	O
Program	O
of	O
China	O
(	O
NO	O
.	O
2017YFE0192900	O
)	O
.	O

Neural	O
Topic	O
Modeling	O
by	O
Incorporating	O
Document	O
Relationship	O
Graph	O

The	O
authors	O
would	O
like	O
to	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
insightful	O
comments	O
and	O
helpful	O
suggestions	O
.	O
This	O
work	O
was	O
funded	O
in	O
part	O
by	O
the	O
National	O
Key	O
Research	O
and	O
Development	O
Program	O
of	O
China	O
(	O
2016YFC1306704	O
)	O
and	O
the	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
(	O
61772132	O
)	O
.	O

NeuInfer	O
:	O
Knowledge	O
Inference	O
on	O
N	O
-	O
ary	O
Facts	O

Knowledge	O
inference	O
on	O
knowledge	O
graph	O
has	O
attracted	O
extensive	O
attention	O
,	O
which	O
aims	O
to	O
find	O
out	O
connotative	O
valid	O
facts	O
in	O
knowledge	O
graph	O
and	O
is	O
very	O
helpful	O
for	O
improving	O
the	O
performance	O
of	O
many	O
downstream	O
applications	O
.	O
However	O
,	O
researchers	O
have	O
mainly	O
poured	O
attention	O
to	O
knowledge	O
inference	O
on	O
binary	O
facts	O
.	O
The	O
studies	O
on	O
n	O
-	O
ary	O
facts	O
are	O
relatively	O
scarcer	O
,	O
although	O
they	O
are	O
also	O
ubiquitous	O
in	O
the	O
real	O
world	O
.	O
Therefore	O
,	O
this	O
paper	O
addresses	O
knowledge	O
inference	O
on	O
n	O
-	O
ary	O
facts	O
.	O
We	O
represent	O
each	O
n	O
-	O
ary	O
fact	O
as	O
a	O
primary	O
triple	O
coupled	O
with	O
a	O
set	O
of	O
its	O
auxiliary	O
descriptive	O
attribute	O
-	O
value	O
pair	O
(	O
s	O
)	O
.	O
We	O
further	O
propose	O
a	O
neural	O
network	O
model	O
,	O
NeuInfer	O
,	O
for	O
knowledge	O
inference	O
on	O
n	O
-	O
ary	O
facts	O
.	O
Besides	O
handling	O
the	O
common	O
task	O
to	O
infer	O
an	O
unknown	O
element	O
in	O
a	O
whole	O
fact	O
,	O
NeuInfer	O
can	O
cope	O
with	O
a	O
new	O
type	O
of	O
task	O
,	O
flexible	O
knowledge	O
inference	O
.	O
It	O
aims	O
to	O
infer	O
an	O
unknown	O
element	O
in	O
a	O
partial	O
fact	O
consisting	O
of	O
the	O
primary	O
triple	O
coupled	O
with	O
any	O
number	O
of	O
its	O
auxiliary	O
description	O
(	O
s	O
)	O
.	O
Experimental	O
results	O
demonstrate	O
the	O
remarkable	O
superiority	O
of	O
NeuInfer	O
.	O

Different	O
from	O
the	O
studies	O
that	O
define	O
n	O
-	O
ary	O
relations	O
first	O
and	O
then	O
represent	O
n	O
-	O
ary	O
facts	O
(	O
Wen	O
et	O
al	O
,	O
2016	O
;	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
,	O
we	O
represent	O
each	O
n	O
-	O
ary	O
fact	O
as	O
a	O
primary	O
triple	O
(	O
head	O
entity	O
,	O
relation	O
,	O
tail	O
entity	O
)	O
coupled	O
with	O
a	O
set	O
of	O
its	O
auxiliary	O
description	O
(	O
s	O
)	O
directly	O
.	O
Formally	O
,	O
given	O
an	O
n	O
-	O
ary	O
fact	O
F	O
ct	O
with	O
the	O
primary	O
triple	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
,	O
m	O
attributes	O
and	O
attribute	O
values	O
,	O
its	O
representation	O
is	O
:	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
,	O
{	O
|	O
−−	O
a1	O
:	O
v1	O
,	O
|	O
−−	O
a2	O
:	O
v2	O
,	O
|	O
−−	O
.	O
.	O
.	O
,	O
|	O
−−	O
am	O
:	O
vm	O
}	O
,	O
where	O
each	O
a	O
i	O
:	O
v	O
i	O
(	O
i	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
m	O
)	O
is	O
an	O
attributevalue	O
pair	O
,	O
also	O
called	O
an	O
auxiliary	O
description	O
to	O
the	O
primary	O
triple	O
.	O
An	O
element	O
of	O
F	O
ct	O
refers	O
to	O
h	O
/	O
r	O
/	O
t	O
/	O
a	O
i	O
/v	O
i	O
;	O
A	O
F	O
ct	O
=	O
{	O
a	O
1	O
,	O
a	O
2	O
,	O
.	O
.	O
.	O
,	O
a	O
m	O
}	O
is	O
F	O
ct	O
's	O
attribute	O
set	O
and	O
a	O
i	O
may	O
be	O
the	O
same	O
to	O
a	O
j	O
(	O
i	O
,	O
j	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
m	O
,	O
i	O
=	O
j	O
)	O
;	O
V	O
F	O
ct	O
=	O
{	O
v	O
1	O
,	O
v	O
2	O
,	O
.	O
.	O
.	O
,	O
v	O
m	O
}	O
is	O
F	O
ct	O
's	O
attribute	O
value	O
set	O
.	O
For	O
example	O
,	O
the	O
representation	O
of	O
the	O
5	O
-	O
ary	O
fact	O
,	O
mentioned	O
in	O
Section	O
1	O
,	O
is	O
:	O
Note	O
that	O
,	O
in	O
the	O
real	O
world	O
,	O
there	O
is	O
a	O
type	O
of	O
complicated	O
cases	O
,	O
say	O
,	O
where	O
more	O
than	O
two	O
entities	O
participate	O
in	O
the	O
same	O
n	O
-	O
ary	O
fact	O
with	O
the	O
same	O
primary	O
attribute	O
.	O
We	O
follow	O
Wikidata	O
(	O
Vrandečić	O
and	O
Krötzsch	O
,	O
2014	O
)	O
to	O
view	O
the	O
cases	O
from	O
different	O
aspects	O
of	O
different	O
entities	O
.	O
Take	O
the	O
case	O
that	O
John	O
Bardeen	O
,	O
W	O
alter	O
Houser	O
Brattain	O
,	O
and	O
W	O
illiam	O
Shockley	O
received	O
N	O
obel	O
P	O
rize	O
in	O
P	O
hysics	O
in	O
1956	O
for	O
example	O
,	O
besides	O
the	O
above	O
5	O
-	O
ary	O
fact	O
from	O
the	O
view	O
of	O
John	O
Bardeen	O
,	O
we	O
get	O
other	O
two	O
5	O
-	O
ary	O
facts	O
from	O
the	O
views	O
of	O
W	O
alter	O
Houser	O
Brattain	O
2	O
and	O
W	O
illiam	O
Shockley	O
3	O
,	O
respectively	O
:	O
(	O
W	O
alter	O
Houser	O
Brattain	O
,	O
award	O
-	O
received	O
,	O
N	O
obel	O

In	O
this	O
paper	O
,	O
we	O
handle	O
both	O
the	O
common	O
simple	O
knowledge	O
inference	O
and	O
the	O
newly	O
proposed	O
flexible	O
knowledge	O
inference	O
.	O
Before	O
giving	O
their	O
definitions	O
under	O
our	O
representation	O
form	O
of	O
n	O
-	O
ary	O
facts	O
,	O
let	O
us	O
define	O
whole	O
fact	O
and	O
partial	O
fact	O
first	O
.	O
Definition	O
1	O
(	O
Whole	O
fact	O
and	O
partial	O
fact	O
)	O
.	O
For	O
the	O
fact	O
F	O
ct	O
,	O
assume	O
its	O
set	O
of	O
auxiliary	O
description	O
(	O
s	O
)	O
as	O
S	O
d	O
=	O
{	O
a	O
i	O
:	O
v	O
i	O
|	O
i	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
m	O
}	O
.	O
Then	O
a	O
partial	O
fact	O
of	O
F	O
ct	O
is	O
:	O
F	O
ct	O
=	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
,	O
S	O
d	O
,	O
where	O
S	O
d	O
⊂	O
S	O
d	O
,	O
i.e.	O
,	O
S	O
d	O
is	O
a	O
subset	O
of	O
S	O
d	O
.	O
And	O
we	O
call	O
F	O
ct	O
the	O
whole	O
fact	O
to	O
differentiate	O
it	O
from	O
F	O
ct	O
.	O
Notably	O
,	O
whole	O
fact	O
and	O
partial	O
fact	O
are	O
relative	O
concepts	O
,	O
and	O
a	O
whole	O
fact	O
is	O
a	O
relatively	O
complete	O
fact	O
compared	O
to	O
its	O
partial	O
fact	O
.	O
In	O
this	O
paper	O
,	O
partial	O
facts	O
are	O
introduced	O
to	O
imitate	O
a	O
typical	O
openworld	O
setting	O
where	O
different	O
facts	O
of	O
the	O
same	O
type	O
may	O
have	O
different	O
numbers	O
of	O
attribute	O
-	O
value	O
pair	O
(	O
s	O
)	O
.	O
Definition	O
2	O
(	O
Simple	O
knowledge	O
inference	O
)	O
.	O
It	O
aims	O
to	O
infer	O
an	O
unknown	O
element	O
in	O
a	O
whole	O
fact	O
.	O
Definition	O
3	O
(	O
Flexible	O
knowledge	O
inference	O
)	O
.	O
It	O
aims	O
to	O
infer	O
an	O
unknown	O
element	O
in	O
a	O
partial	O
fact	O
.	O

To	O
conduct	O
knowledge	O
inference	O
on	O
n	O
-	O
ary	O
facts	O
,	O
NeuInfer	O
first	O
models	O
the	O
validity	O
of	O
the	O
n	O
-	O
ary	O
facts	O
and	O
then	O
casts	O
inference	O
as	O
a	O
classification	O
task	O
.	O
In	O
the	O
above	O
first	O
n	O
-	O
ary	O
fact	O
,	O
the	O
primary	O
triple	O
is	O
invalid	O
.	O
In	O
the	O
second	O
one	O
,	O
some	O
auxiliary	O
description	O
is	O
incompatible	O
with	O
the	O
primary	O
triple	O
.	O
Therefore	O
,	O
we	O
believe	O
that	O
a	O
valid	O
n	O
-	O
ary	O
fact	O
has	O
two	O
prerequisites	O
.	O
On	O
the	O
one	O
hand	O
,	O
its	O
primary	O
triple	O
should	O
be	O
valid	O
.	O
If	O
the	O
primary	O
triple	O
is	O
invalid	O
,	O
attaching	O
any	O
number	O
of	O
attribute	O
-	O
value	O
pairs	O
to	O
it	O
does	O
not	O
make	O
the	O
resulting	O
n	O
-	O
ary	O
fact	O
valid	O
;	O
on	O
the	O
other	O
hand	O
,	O
since	O
each	O
auxiliary	O
description	O
presents	O
a	O
qualifier	O
to	O
the	O
primary	O
triple	O
,	O
it	O
should	O
be	O
compatible	O
with	O
the	O
primary	O
triple	O
.	O
Even	O
if	O
the	O
primary	O
triple	O
is	O
basically	O
valid	O
,	O
any	O
incompatible	O
attribute	O
-	O
value	O
pair	O
makes	O
the	O
n	O
-	O
ary	O
fact	O
invalid	O
.	O
Therefore	O
,	O
NeuInfer	O
is	O
designed	O
to	O
characterize	O
these	O
two	O
aspects	O
and	O
thus	O
consists	O
of	O
two	O
components	O
corresponding	O
to	O
the	O
validity	O
evaluation	O
of	O
the	O
primary	O
triple	O
and	O
the	O
compatibility	O
evaluation	O
of	O
the	O
n	O
-	O
ary	O
fact	O
,	O
respectively	O
.	O

We	O
perform	O
an	O
ablation	O
study	O
to	O
look	O
deep	O
into	O
the	O
framework	O
of	O
NeuInfer	O
.	O
If	O
we	O
remove	O
the	O
compatibility	O
evaluation	O
component	O
,	O
NeuInfer	O
is	O
reduced	O
to	O
a	O
method	O
for	O
binary	O
but	O
not	O
n	O
-	O
ary	O
facts	O
.	O
Since	O
we	O
handle	O
knowledge	O
inference	O
on	O
n	O
-	O
ary	O
facts	O
,	O
it	O
is	O
inappropriate	O
to	O
remove	O
this	O
component	O
.	O
Thus	O
,	O
as	O
an	O
ablation	O
,	O
we	O
only	O
deactivate	O
the	O
validity	O
evaluation	O
component	O
,	O
denoted	O
as	O
NeuInfer	O
−	O
.	O
The	O
experimental	O
comparison	O
between	O
NeuInfer	O
and	O
NeuInfer	O
−	O
is	O
illustrated	O
in	O
Figure	O
2	O
.	O
It	O
can	O
be	O
observed	O
from	O
the	O
figure	O
that	O
NeuInfer	O
outperforms	O
NeuInfer	O
−	O
significantly	O
.	O
It	O
suggests	O
that	O
the	O
validity	O
evaluation	O
component	O
plays	O
a	O
pivotal	O
role	O
in	O
our	O
method	O
.	O
Thus	O
,	O
each	O
component	O
of	O
our	O
method	O
is	O
necessary	O
.	O

The	O
newly	O
proposed	O
flexible	O
knowledge	O
inference	O
focuses	O
on	O
n	O
-	O
ary	O
facts	O
of	O
arities	O
greater	O
than	O
2	O
.	O
It	O
includes	O
flexible	O
entity	O
inference	O
and	O
flexible	O
relation	O
inference	O
.	O
For	O
an	O
n	O
-	O
ary	O
fact	O
,	O
they	O
infer	O
one	O
of	O
the	O
entities	O
/	O
the	O
relation	O
in	O
the	O
primary	O
triple	O
given	O
any	O
number	O
of	O
its	O
auxiliary	O
description	O
(	O
s	O
)	O
or	O
infer	O
the	O
attribute	O
value	O
/	O
attribute	O
in	O
an	O
auxiliary	O
description	O
given	O
the	O
primary	O
triple	O
and	O
any	O
number	O
of	O
other	O
auxiliary	O
description	O
(	O
s	O
)	O
.	O
In	O
existing	O
knowledge	O
inference	O
methods	O
on	O
n	O
-	O
ary	O
facts	O
,	O
each	O
n	O
-	O
ary	O
fact	O
is	O
represented	O
as	O
a	O
group	O
of	O
peer	O
attributes	O
and	O
attribute	O
values	O
.	O
These	O
methods	O
have	O
not	O
poured	O
attention	O
to	O
the	O
above	O
flexible	O
knowledge	O
inference	O
.	O
Thus	O
,	O
we	O
conduct	O
this	O
new	O
type	O
of	O
task	O
only	O
on	O
NeuInfer	O
.	O
Before	O
elaborating	O
on	O
the	O
experimental	O
results	O
,	O
let	O
us	O
look	O
into	O
the	O
new	O
test	O
set	O
used	O
in	O
this	O
section	O
first	O
.	O

We	O
generate	O
the	O
new	O
test	O
set	O
as	O
follows	O
:	O
Collect	O
the	O
n	O
-	O
ary	O
facts	O
of	O
arities	O
greater	O
than	O
2	O
from	O
the	O
test	O
set	O
.	O
For	O
each	O
collected	O
n	O
-	O
ary	O
fact	O
,	O
compute	O
all	O
the	O
subsets	O
of	O
the	O
auxiliary	O
description	O
(	O
s	O
)	O
.	O
The	O
primary	O
triple	O
and	O
each	O
subset	O
form	O
a	O
new	O
n	O
-	O
ary	O
fact	O
,	O
which	O
is	O
added	O
to	O
the	O
candidate	O
set	O
.	O
Remove	O
the	O
n	O
-	O
ary	O
facts	O
that	O
also	O
exist	O
in	O
the	O
training	O
/	O
validation	O
set	O
from	O
the	O
candidate	O
set	O
and	O
then	O
remove	O
the	O
duplicate	O
n	O
-	O
ary	O
facts	O
.	O
The	O
remaining	O
n	O
-	O
ary	O
facts	O
form	O
the	O
new	O
test	O
set	O
.	O
The	O
size	O
of	O
the	O
resulting	O
new	O
test	O
set	O
on	O
JF17	O
K	O
is	O
34	O
,	O
784	O
,	O
and	O
that	O
on	O
WikiPeople	O
is	O
13	O
,	O
833	O
.	O

The	O
experimental	O
results	O
of	O
flexible	O
entity	O
and	O
relation	O
inference	O
on	O
these	O
new	O
test	O
sets	O
are	O
presented	O
in	O
Table	O
4	O
.	O
It	O
can	O
be	O
observed	O
that	O
NeuInfer	O
well	O
tackles	O
flexible	O
entity	O
and	O
relation	O
inference	O
on	O
partial	O
facts	O
,	O
and	O
achieves	O
excellent	O
performance	O
.	O
We	O
also	O
attribute	O
this	O
to	O
the	O
reasonable	O
modeling	O
of	O
n	O
-	O
ary	O
facts	O
.	O
For	O
each	O
n	O
-	O
ary	O
fact	O
,	O
NeuInfer	O
distinguishes	O
the	O
primary	O
triple	O
from	O
other	O
auxiliary	O
description	O
(	O
s	O
)	O
and	O
models	O
them	O
properly	O
.	O
Thus	O
,	O
NeuInfer	O
well	O
handles	O
various	O
types	O
of	O
entity	O
and	O
relation	O
inference	O
concerning	O
the	O
primary	O
triple	O
coupled	O
with	O
any	O
number	O
of	O
its	O
auxiliary	O
description	O
(	O
s	O
)	O
.	O

The	O
work	O
is	O
supported	O
by	O
the	O
National	O
Key	O
Research	O
and	O
Development	O
Program	O
of	O
China	O
under	O
grant	O
2016YFB1000902	O
,	O
the	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
under	O
grants	O
U1911401	O
,	O
61772501	O
,	O
U1836206	O
,	O
91646120	O
,	O
and	O
61722211	O
,	O
the	O
GFKJ	O
Innovation	O
Program	O
,	O
Beijing	O
Academy	O
of	O
Artificial	O
Intelligence	O
(	O
BAAI	O
)	O
under	O
grant	O
BAAI2019ZD0306	O
,	O
and	O
the	O
Lenovo	O
-	O
CAS	O
Joint	O
Lab	O
Youth	O
Scientist	O
Project	O
.	O

Question	O
-	O
Answering	O
systems	O
(	O
QAS	O
)	O
aim	O
at	O
analyzing	O
and	O
processing	O
user	O
questions	O
in	O
order	O
to	O
provide	O
relevant	O
answers	O
(	O
Hirschman	O
and	O
Gaizauskas	O
,	O
2001	O
)	O
.	O
The	O
recent	O
popularity	O
of	O
intelligent	O
assistants	O
has	O
increased	O
the	O
interest	O
in	O
QAS	O
which	O
have	O
become	O
a	O
key	O
component	O
of	O
"	O
Human	O
-	O
Machine	O
"	O
exchanges	O
since	O
they	O
allow	O
users	O
to	O
have	O
instant	O
answers	O
to	O
their	O
questions	O
in	O
natural	O
language	O
using	O
their	O
own	O
terminology	O
without	O
having	O
to	O
go	O
through	O
a	O
long	O
list	O
of	O
documents	O
to	O
find	O
the	O
appropriate	O
answers	O
.	O
Most	O
of	O
the	O
existing	O
research	O
work	O
focuses	O
on	O
the	O
major	O
complexity	O
of	O
these	O
systems	O
residing	O
in	O
the	O
processing	O
and	O
interpretation	O
of	O
the	O
question	O
that	O
expresses	O
the	O
user	O
's	O
need	O
for	O
information	O
,	O
without	O
considering	O
the	O
representation	O
of	O
the	O
answer	O
itself	O
.	O
Usually	O
,	O
the	O
answer	O
is	O
either	O
represented	O
by	O
a	O
short	O
set	O
of	O
terms	O
answering	O
exactly	O
the	O
question	O
(	O
case	O
of	O
QAS	O
which	O
extract	O
answers	O
from	O
structured	O
data	O
)	O
,	O
or	O
by	O
a	O
text	O
span	O
extracted	O
from	O
a	O
document	O
which	O
,	O
besides	O
the	O
exact	O
answer	O
,	O
can	O
integrate	O
other	O
unnecessary	O
information	O
that	O
are	O
not	O
relevant	O
to	O
the	O
context	O
of	O
the	O
question	O
asked	O
.	O
The	O
following	O
presents	O
two	O
answers	O
for	O
Who	O
is	O
the	O
thesis	O
supervisor	O
of	O
Albert	O
Einstein	O
?	O
possibly	O
generated	O
by	O
two	O
systems	O
:	O

We	O
have	O
put	O
forward	O
,	O
in	O
this	O
paper	O
,	O
an	O
approach	O
for	O
Natural	O
Language	O
Generation	O
within	O
the	O
framework	O
of	O
the	O
question	O
-	O
answering	O
task	O
that	O
considers	O
dependency	O
analysis	O
and	O
probability	O
distribution	O
of	O
words	O
sequences	O
.	O
This	O
approach	O
takes	O
part	O
of	O
a	O
question	O
/	O
answering	O
system	O
in	O
order	O
to	O
help	O
generate	O
a	O
user	O
-	O
friendly	O
answer	O
rather	O
than	O
a	O
short	O
one	O
.	O
The	O
results	O
obtained	O
through	O
a	O
human	O
evaluation	O
and	O
standard	O
metrics	O
tested	O
over	O
French	O
and	O
English	O
questions	O
are	O
very	O
promising	O
and	O
shows	O
a	O
good	O
correlation	O
with	O
human	O
judgement	O
.	O
However	O
,	O
we	O
intend	O
to	O
put	O
more	O
emphasis	O
on	O
the	O
Language	O
Model	O
choice	O
as	O
reported	O
by	O
the	O
human	O
study	O
and	O
consider	O
the	O
generation	O
of	O
more	O
than	O
one	O
missing	O
word	O
within	O
the	O
answer	O
.	O

How	O
Do	O
We	O
Answer	O
Complex	O
Questions	O
:	O
Discourse	O
Structure	O
of	O
Long	O
-	O
form	O
Answers	O

We	O
have	O
a	O
two	O
-	O
stage	O
annotation	O
process	O
:	O
annotators	O
first	O
determine	O
the	O
validity	O
of	O
the	O
QA	O
pair	O
,	O
and	O
proceed	O
to	O
discourse	O
annotation	O
only	O
if	O
they	O
consider	O
the	O
QA	O
pair	O
valid	O
.	O
We	O
define	O
the	O
QA	O
pair	O
as	O
valid	O
if	O
(	O
1	O
)	O
the	O
question	O
is	O
interpretable	O
,	O
(	O
2	O
)	O
the	O
question	O
does	O
not	O
have	O
presuppositions	O
rejected	O
by	O
the	O
answer	O
,	O
(	O
3	O
)	O
the	O
question	O
does	O
not	O
contain	O
more	O
than	O
one	O
sub	O
-	O
question	O
,	O
and	O
(	O
4	O
)	O
the	O
proposed	O
answer	O
properly	O
addresses	O
the	O
question	O
.	O
Examples	O
of	O
the	O
invalid	O
QA	O
pair	O
identified	O
are	O
in	O
A.1	O
.	O
6	O
We	O
collect	O
the	O
first	O
stage	O
annotation	O
from	O
USbased	O
crowdsource	O
workers	O
on	O
Amazon	O
Mechanical	O
Turk	O
and	O
second	O
stage	O
annotation	O
from	O
undergraduate	O
students	O
majoring	O
in	O
linguistics	O
,	O
who	O
are	O
native	O
speakers	O
in	O
English	O
.	O
7	O
A	O
total	O
of	O
29	O
crowdworker	O
participated	O
in	O
our	O
task	O
,	O
and	O
six	O
undergraduates	O
annotated	O
roles	O
for	O
a	O
subset	O
of	O
QA	O
pairs	O
annotated	O
as	O
valid	O
by	O
crowdworkers	O
.	O
We	O
first	O
qualified	O
and	O
then	O
provided	O
training	O
materials	O
to	O
both	O
groups	O
of	O
annotators	O
.	O
The	O
annotation	O
guideline	O
and	O
interface	O
can	O
be	O
found	O
in	O
A.4	O
.	O
We	O
paid	O
crowd	O
workers	O
$	O
0.5	O
per	O
example	O
,	O
and	O
our	O
undergraduate	O
annotators	O
$	O
13	O
/	O
hour	O
.	O
More	O
details	O
of	O
data	O
collection	O
can	O
be	O
found	O
in	O
our	O
datasheet	O
.	O
ples	O
and	O
role	O
annotations	O
for	O
about	O
half	O
of	O
them	O
.	O

As	O
our	O
tasks	O
are	O
complex	O
and	O
somewhat	O
subjective	O
,	O
we	O
collected	O
three	O
way	O
annotations	O
.	O
We	O
consider	O
a	O
QA	O
pair	O
valid	O
if	O
all	O
annotated	O
it	O
as	O
valid	O
,	O
and	O
invalid	O
if	O
more	O
than	O
two	O
annotated	O
it	O
as	O
invalid	O
.	O
If	O
two	O
annotators	O
considered	O
valid	O
,	O
we	O
collect	O
one	O
additional	O
annotation	O
and	O
consider	O
it	O
valid	O
if	O
and	O
only	O
if	O
the	O
additional	O
annotator	O
marked	O
it	O
as	O
valid	O
.	O
8	O
We	O
consider	O
the	O
majority	O
role	O
(	O
i.e.	O
chosen	O
by	O
two	O
or	O
more	O
than	O
two	O
annotators	O
)	O
as	O
the	O
gold	O
label	O
.	O
When	O
all	O
annotators	O
chose	O
different	O
roles	O
,	O
they	O
resolved	O
the	O
disagreement	O
through	O
adjudication	O
.	O
We	O
report	O
inter	O
-	O
annotator	O
agreement	O
before	O
the	O
adjudication	O
.	O
Inter	O
-	O
annotator	O
Agreement	O
We	O
find	O
modest	O
to	O
high	O
agreement	O
for	O
both	O
annotation	O
tasks	O
:	O
For	O
crowdworkers	O
,	O
Fleiss	O
Kappa	O
was	O
0.51	O
for	O
validity	O
annotation	O
.	O
For	O
student	O
annotators	O
,	O
Fleiss	O
Kappa	O
was	O
0.44	O
for	O
role	O
annotation	O
.	O
Figure	O
2	O
shows	O
the	O
confusion	O
matrix	O
between	O
pairs	O
of	O
annotations	O
,	O
with	O
the	O
numbers	O
normalized	O
by	O
row	O
and	O
averaged	O
across	O
pairs	O
of	O
annotators	O
.	O
We	O
observe	O
frequent	O
confusion	O
between	O
roles	O
denoting	O
different	O
levels	O
of	O
information	O
salience	O
-	O
Answer	O
vs.	O
Answer	O
-	O
Summary	O
,	O
and	O
Answer	O
vs.	O
Auxiliary	O
Information	O
,	O
reflecting	O
the	O
nuance	O
and	O
subjectivity	O
in	O
judging	O
what	O
information	O
is	O
necessary	O
to	O
answer	O
a	O
complicated	O
question	O
.	O
Examples	O
can	O
be	O
found	O
in	O
A.2	O
.	O
8	O
The	O
Fleiss	O
kappa	O
for	O
agreement	O
improves	O
to	O
0.70	O
after	O
this	O
re	O
-	O
annotation	O
process	O
.	O

WebGPT	O

Having	O
analyzed	O
discourse	O
roles	O
of	O
human	O
-	O
written	O
long	O
-	O
form	O
answers	O
,	O
we	O
investigate	O
the	O
discourse	O
structure	O
of	O
model	O
-	O
generated	O
answers	O
.	O
This	O
will	O
allow	O
us	O
to	O
quantitatively	O
study	O
the	O
difference	O
in	O
terms	O
of	O
discourse	O
structure	O
across	O
gold	O
and	O
generated	O
answers	O
,	O
which	O
we	O
hope	O
will	O
cast	O
insights	O
to	O
the	O
linguistic	O
quality	O
of	O
system	O
outputs	O
.	O

We	O
study	O
how	O
models	O
can	O
identify	O
the	O
discourse	O
role	O
for	O
each	O
sentence	O
in	O
long	O
-	O
form	O
answer	O
in	O
a	O
valid	O
QA	O
pair	O
.	O
10	O
Such	O
a	O
model	O
can	O
be	O
beneficial	O
for	O
large	O
-	O
scale	O
automatic	O
analysis	O
.	O

We	O
present	O
a	O
linguistically	O
motivated	O
study	O
of	O
longform	O
answers	O
.	O
We	O
find	O
humans	O
employ	O
various	O
strategies	O
-	O
introducing	O
sentences	O
laying	O
out	O
the	O
structure	O
of	O
the	O
answer	O
,	O
proposing	O
hypothetical	O
and	O
real	O
examples	O
,	O
and	O
summarizing	O
main	O
points	O
-	O
to	O
organize	O
information	O
.	O
Our	O
discourse	O
analysis	O
characterizes	O
three	O
types	O
of	O
long	O
-	O
form	O
answers	O
and	O
reveals	O
deficient	O
discourse	O
structures	O
of	O
modelgenerated	O
answers	O
.	O
Discourse	O
analysis	O
can	O
be	O
fruitful	O
direction	O
for	O
evaluating	O
long	O
-	O
form	O
answers	O
.	O
For	O
instance	O
,	O
highlighting	O
summary	O
sentence	O
(	O
s	O
)	O
or	O
sentence	O
-	O
level	O
discourse	O
role	O
could	O
be	O
helpful	O
for	O
human	O
evaluators	O
to	O
dissect	O
long	O
-	O
form	O
answers	O
,	O
whose	O
length	O
has	O
been	O
found	O
to	O
be	O
challenging	O
for	O
human	O
evaluation	O
(	O
Krishna	O
et	O
al	O
,	O
2021	O
)	O
.	O
Trained	O
role	O
classifier	O
can	O
also	O
evaluate	O
the	O
discourse	O
structure	O
of	O
model	O
-	O
generated	O
answers	O
.	O
Future	O
work	O
can	O
explore	O
using	O
sentences	O
belonging	O
to	O
the	O
summary	O
role	O
to	O
design	O
evaluation	O
metrics	O
that	O
focuses	O
on	O
the	O
core	O
parts	O
of	O
the	O
answer	O
(	O
Nenkova	O
and	O
Passonneau	O
,	O
2004	O
)	O
,	O
for	O
assessing	O
the	O
correctness	O
of	O
generated	O
the	O
answer	O
.	O
Exploring	O
controllable	O
generation	O
,	O
such	O
as	O
encouraging	O
models	O
to	O
provide	O
summaries	O
or	O
examples	O
,	O
would	O
be	O
another	O
exciting	O
avenue	O
for	O
future	O
work	O
.	O

If	O
you	O
are	O
n't	O
married	O
you	O
are	O
not	O
legally	O
a	O
part	O
of	O
that	O
person	O
's	O
life	O
,	O
so	O
any	O
legal	O
or	O
medical	O
decisions	O
would	O
be	O
up	O
to	O
the	O
parents	O
of	O
that	O
individual	O
.	O
Answer	O
Auxiliary	O
3	O
That	O
's	O
why	O
marriage	O
equality	O
was	O
important	O
a	O
few	O
years	O
ago	O
.	O
Auxiliary	O

If	O
someone	O
was	O
with	O
their	O
partner	O
for	O
15	O
years	O
and	O
then	O
suddenly	O
dropped	O
dead	O
,	O
their	O
partner	O
had	O
better	O
hope	O
their	O
in	O
-	O
laws	O
liked	O
them	O
or	O
even	O
supported	O
the	O
partnership	O
in	O
the	O
first	O
place	O
.	O
Example	O
Auxiliary	O

Figure	O
5	O
,	O
6	O
,	O
7	O
,	O
8	O
9	O
,	O
and	O
10	O
show	O
the	O
annotation	O
guideline	O
as	O
well	O
as	O
interface	O
presented	O
to	O
the	O
annotators	O
(	O
we	O
present	O
Step	O
1	O
for	O
crowdworkers	O
,	O
Step	O
2	O
and	O
Step	O
3	O
for	O
student	O
annotators	O
)	O
.	O
We	O
did	O
n't	O
capture	O
the	O
extended	O
example	O
section	O
as	O
well	O
as	O
FAQ	O
here	O
due	O
to	O
space	O
.	O
Figure	O
10	O
:	O
Screenshot	O
of	O
annotation	O
interface	O
for	O
sentence	O
-	O
level	O
role	O
,	O
as	O
well	O
as	O
summary	O
sentence	O
selection	O
.	O

This	O
work	O
was	O
partially	O
supported	O
by	O
NSF	O
grants	O
IIS	O
-	O
1850153	O
,	O
IIS	O
-	O
2107524	O
.	O
We	O
thank	O
Kalpesh	O
Krishna	O
and	O
Mohit	O
Iyyer	O
for	O
sharing	O
the	O
model	O
predictions	O
and	O
human	O
evaluation	O
results	O
.	O
We	O
would	O
like	O
to	O
thank	O
Tanya	O
Goyal	O
,	O
Jiacheng	O
Xu	O
,	O
Mohit	O
Iyyer	O
,	O
anonymous	O
reviewers	O
and	O
meta	O
reviewer	O
for	O
providing	O
constructive	O
feedback	O
to	O
improve	O
the	O
draft	O
.	O
Lastly	O
,	O
we	O
thank	O
Maanasa	O
V	O
Darisi	O
,	O
Meona	O
Khetrapal	O
,	O
Matthew	O
Micyk	O
,	O
Misty	O
Peng	O
,	O
Payton	O
Wages	O
,	O
Sydney	O
C	O
Willett	O
and	O
crowd	O
workers	O
for	O
their	O
help	O
with	O
the	O
complex	O
data	O
annotation	O
.	O

A.1	O
Invalid	O
QA	O
We	O
provide	O
definitions	O
,	O
as	O
well	O
as	O
examples	O
of	O
each	O
invalid	O
QA	O
type	O
.	O
No	O
valid	O
answer	O
The	O
answer	O
paragraph	O
does	O
n't	O
provide	O
a	O
valid	O
answer	O
to	O
the	O
question	O
.	O
[	O
Q	O
]	O
:	O
How	O
does	O
drinking	O
alcohol	O
affect	O
your	O
ability	O
to	O
lose	O
weight	O
?	O
[	O
A	O
]	O
:	O
Alcohol	O
itself	O
is	O
extremely	O
calorically	O
dense	O
.	O
Doesn't	O
really	O
matter	O
whether	O
you	O
're	O
drinking	O
a	O
light	O
beer	O
or	O
shots	O
,	O
alcohol	O
itself	O
has	O
plenty	O
of	O
calories	O
.	O
Just	O
think	O
of	O
every	O
three	O
shots	O
as	O
eating	O
a	O
mcdouble	O
,	O
with	O
even	O
less	O
nutritional	O
value	O
.	O

The	O
question	O
is	O
nonsensical	O
and	O
it	O
is	O
unclear	O
what	O
is	O
asked	O
.	O
[	O
Q	O
]	O
:	O
asia	O
vs	O
rest	O
of	O
the	O
world	O
cricket	O
match	O
Multiple	O
questions	O
asked	O
More	O
than	O
one	O
question	O
are	O
asked	O
in	O
the	O
question	O
sentence	O
.	O
[	O
Q	O
]	O
:	O
what	O
is	O
a	O
limpet	O
and	O
where	O
does	O
it	O
live	O
Assumptions	O
in	O
the	O
question	O
rejected	O
The	O
answer	O
focuses	O
on	O
rejecting	O
assumptions	O
in	O
the	O
question	O
,	O
without	O
answering	O
the	O
question	O
.	O
[	O
Q	O
]	O
:	O
Why	O
is	O
it	O
that	O
as	O
we	O
get	O
older	O
,	O
we	O
are	O
able	O
to	O
handle	O
eating	O
hotter	O
foods	O
[	O
A	O
]	O
:	O
I	O
'm	O
not	O
sure	O
I	O
accept	O
the	O
premise	O
.	O
Children	O
in	O
cultures	O
where	O
spicy	O
food	O
is	O
common	O
,	O
think	O
nothing	O
of	O
it	O
.	O
My	O
nephews	O
had	O
no	O
problem	O
eating	O
hot	O
peppers	O
when	O
they	O
were	O
very	O
young	O
because	O
it	O
was	O
just	O
a	O
normal	O
part	O
of	O
their	O
diet	O
.	O
[	O
...	O
]	O

We	O
include	O
example	O
role	O
annotations	O
in	O
Table	O
6	O
which	O
demonstrate	O
disagreement	O
between	O
Auxiliary	O
Information	O
and	O
the	O
Answer	O
role	O
.	O
Sentence	O
2	O
in	O
answer	O
(	O
a	O
)	O
was	O
annotated	O
as	O
answer	O
by	O
most	O
of	O
the	O
annotators	O
as	O
it	O
elaborates	O
on	O
becoming	O
a	O
legal	O
'	O
next	O
of	O
kin	O
'	O
by	O
providing	O
a	O
counterfactual	O
scenario	O
.	O
One	O
annotator	O
annotated	O
it	O
as	O
auxiliary	O
as	O
it	O
touches	O
upon	O
how	O
the	O
decisions	O
would	O
be	O
up	O
to	O
the	O
parents	O
,	O
which	O
goes	O
beyond	O
what	O
is	O
asked	O
in	O
the	O
question	O
.	O
For	O
answer	O
(	O
b	O
)	O
,	O
while	O
most	O
annotators	O
think	O
that	O
sentence	O
1	O
is	O
of	O
Answer	O
role	O
,	O
one	O
annotator	O
annotated	O
it	O
as	O
Auxiliary	O
Information	O
which	O
only	O
talks	O
about	O
the	O
property	O
of	O
purple	O
.	O

In	O
Section	O
3.1	O
,	O
we	O
introduce	O
two	O
kinds	O
of	O
wordlevel	O
sentiment	O
annotation	O
,	O
i.e.	O
,	O
soft	O
and	O
hard	O
sentiment	O
annotation	O
.	O
We	O
now	O
compare	O
two	O
methods	O
.	O
The	O
results	O
are	O
reported	O
in	O
Tables	O
5	O
and	O
6	O

In	O
order	O
to	O
gain	O
more	O
insight	O
of	O
our	O
model	O
and	O
observe	O
the	O
effectiveness	O
of	O
the	O
sentiment	O
lexicon	O
,	O
in	O

The	O
work	O
was	O
supported	O
by	O
the	O
Natural	O
Science	O
Foundation	O
of	O
China	O
(	O
No	O
.	O
61672288	O
)	O
,	O
and	O
the	O
Natural	O
Science	O
Foundation	O
of	O
Jiangsu	O
Province	O
for	O
Excellent	O
Young	O
Scholars	O
(	O
No	O
.	O
BK20160085	O
)	O
.	O

Data	O
set	O
statistics	O
are	O
presented	O
in	O
Table	O
1	O
.	O
This	O
shows	O
that	O
all	O
domains	O
are	O
represented	O
with	O
a	O
substantial	O
number	O
of	O
sentences	O
,	O
although	O
the	O
prevalence	O
of	O
named	O
entities	O
and	O
their	O
distribution	O
across	O
types	O
varies	O
,	O
as	O
expected	O
from	O
data	O
sets	O
collected	O
from	O
different	O
sources	O
and	O
genres	O
.	O
We	O
also	O
see	O
that	O
the	O
zero	O
-	O
shot	O
domains	O
are	O
significantly	O
different	O
in	O
entity	O
type	O
distribution	O
and	O
density	O
than	O
the	O
training	O
data	O
,	O
making	O
them	O
well	O
-	O
suited	O
for	O
this	O
setting	O
.	O

Runtime	O

A	O
couple	O
of	O
challenges	O
are	O
found	O
during	O
the	O
parallel	O
annotation	O
.	O
First	O
,	O
subjects	O
are	O
obligatory	O
in	O
English	O
for	O
most	O
sentence	O
forms	O
whereas	O
Korean	O
is	O
a	O
prodrop	O
language	O
so	O
that	O
entities	O
in	O
the	O
subject	O
position	O
can	O
be	O
missing	O
in	O
Korean	O
but	O
not	O
in	O
English	O
,	O
which	O
explains	O
the	O
greater	O
number	O
of	O
entities	O
in	O
English	O
.	O
Second	O
,	O
certain	O
inflectional	O
morphemes	O
in	O
Korean	O
can	O
be	O
dropped	O
without	O
violating	O
the	O
grammar	O
,	O
that	O
often	O
makes	O
the	O
labeling	O
ambiguous	O
.	O
For	O
instance	O
,	O
the	O
literal	O
translation	O
of	O
"	O
Korean	O
Church	O
"	O
would	O
be	O
"	O
한국	O
(	O
Korea	O
)	O
+	O
의	O
(	O
's	O
)	O
교회	O
(	O
Church	O
)	O
"	O
,	O
although	O
it	O
is	O
the	O
standard	O
practice	O
to	O
drop	O
"	O
의	O
(	O
's	O
)	O
"	O
in	O
this	O
case	O
such	O
that	O
it	O
becomes	O
"	O
한국	O
(	O
Korea	O
)	O
교회	O
(	O
Church	O
)	O
"	O
.	O
Given	O
this	O
translation	O
,	O
the	O
annotator	O
can	O
be	O
easily	O
confused	O
to	O
annotate	O
"	O
한국	O
(	O
Korea	O
)	O
"	O
as	O
a	O
geopolitical	O
entity	O
(	O
GPE	O
)	O
instead	O
of	O
a	O
nationality	O
(	O
NORP	O
)	O
,	O
which	O
may	O
lead	O
to	O
annotation	O
disagreement	O
.	O
Additional	O
analytics	O
by	O
news	O
sections	O
and	O
entity	O
types	O
are	O
described	O
in	O
Appendix	O
A.6	O
4	O
Zero	O
-	O
shot	O
Crosslingual	O
Learning	O

Since	O
the	O
number	O
of	O
named	O
entity	O
types	O
that	O
each	O
model	O
covers	O
are	O
different	O
,	O
named	O
entity	O
tags	O
are	O
mapped	O
based	O
on	O
the	O
definition	O
of	O
the	O
tags	O
.	O
Our	O
named	O
entities	O
are	O
more	O
fine	O
-	O
grained	O
,	O
which	O
makes	O
multiple	O
tags	O
(	O
Zero	O
-	O
shot	O
side	O
)	O
be	O
mapped	O
to	O
one	O
tag	O
(	O
Existing	O
side	O
)	O
.	O
Named	O
entity	O
tags	O
that	O
can	O
not	O
be	O
mapped	O
are	O
discarded	O
in	O
both	O
gold	O
labels	O
and	O
predicted	O
labels	O
,	O
thus	O
not	O
considered	O
in	O
the	O
evaluation	O
of	O
the	O
models	O
.	O

Another	O
virtual	O
workshop	O
.	O
.	O
.	O
Oh	O
well	O
:	O
maybe	O
next	O
year	O
we	O
will	O
go	O
back	O
to	O
the	O
(	O
new	O
)	O
normal	O
.	O
Looking	O
at	O
submission	O
numbers	O
,	O
this	O
year	O
's	O
edition	O
of	O
the	O
workshop	O
was	O
a	O
huge	O
success	O
.	O
We	O
have	O
received	O
really	O
unusually	O
many	O
submissions	O
(	O
thanks	O
,	O
everyone	O
!	O
)	O
.	O
Out	O
of	O
those	O
,	O
we	O
have	O
accepted	O
22	O
papers	O
for	O
a	O
38	O
%	O
acceptance	O
rate	O
.	O
A	O
round	O
of	O
applause	O
for	O
our	O
wonderful	O
program	O
committee	O
!	O
The	O
workshop	O
programme	O
consists	O
of	O
brief	O
eight	O
-	O
minute	O
Q&A	O
sessions	O
for	O
ten	O
oral	O
presentations	O
(	O
which	O
you	O
will	O
have	O
watched	O
by	O
then	O
!	O
)	O
,	O
and	O
a	O
twelve	O
-	O
poster	O
session	O
during	O
which	O
you	O
will	O
be	O
able	O
to	O
chat	O
with	O
any	O
author	O
you	O
like	O
.	O
Thematically	O
,	O
the	O
papers	O
cover	O
the	O
entire	O
range	O
of	O
"	O
Cultural	O
Heritage	O
,	O
Social	O
Sciences	O
,	O
Humanities	O
and	O
Literature	O
"	O
.	O
The	O
programme	O
shows	O
that	O
this	O
area	O
of	O
applied	O
language	O
technology	O
is	O
mature	O
and	O
active	O
.	O
Last	O
but	O
not	O
least	O
,	O
Sara	O
Tonelli	O
will	O
give	O
a	O
live	O
invited	O
talk	O
.	O
We	O
are	O
grateful	O
,	O
and	O
we	O
look	O
forward	O
to	O
it	O
.	O

Learning	O
CNF	O
Blocking	O
for	O
Large	O
-	O
scale	O
Author	O
Name	O
Disambiguation	O

Author	O
name	O
disambiguation	O
(	O
AND	O
)	O
algorithms	O
identify	O
a	O
unique	O
author	O
entity	O
record	O
from	O
all	O
similar	O
or	O
same	O
publication	O
records	O
in	O
scholarly	O
or	O
similar	O
databases	O
.	O
Typically	O
,	O
a	O
clustering	O
method	O
is	O
used	O
that	O
requires	O
calculation	O
of	O
similarities	O
between	O
each	O
possible	O
record	O
pair	O
.	O
However	O
,	O
the	O
total	O
number	O
of	O
pairs	O
grows	O
quadratically	O
with	O
the	O
size	O
of	O
the	O
author	O
database	O
making	O
such	O
clustering	O
difficult	O
for	O
millions	O
of	O
records	O
.	O
One	O
remedy	O
is	O
a	O
blocking	O
function	O
that	O
reduces	O
the	O
number	O
of	O
pairwise	O
similarity	O
calculations	O
.	O
Here	O
,	O
we	O
introduce	O
a	O
new	O
way	O
of	O
learning	O
blocking	O
schemes	O
by	O
using	O
a	O
conjunctive	O
normal	O
form	O
(	O
CNF	O
)	O
in	O
contrast	O
to	O
the	O
disjunctive	O
normal	O
form	O
(	O
DNF	O
)	O
.	O
We	O
demonstrate	O
on	O
PubMed	O
author	O
records	O
that	O
CNF	O
blocking	O
reduces	O
more	O
pairs	O
while	O
preserving	O
high	O
pairs	O
completeness	O
compared	O
to	O
the	O
previous	O
methods	O
that	O
use	O
a	O
DNF	O
and	O
that	O
the	O
computation	O
time	O
is	O
significantly	O
reduced	O
.	O
In	O
addition	O
,	O
we	O
also	O
show	O
how	O
to	O
ensure	O
that	O
the	O
method	O
produces	O
disjoint	O
blocks	O
so	O
that	O
much	O
of	O
the	O
AND	O
algorithm	O
can	O
be	O
efficiently	O
paralleled	O
.	O
Our	O
CNF	O
blocking	O
method	O
is	O
tested	O
on	O
the	O
entire	O
PubMed	O
database	O
of	O
80	O
million	O
author	O
mentions	O
and	O
efficiently	O
removes	O
82.17	O
%	O
of	O
all	O
author	O
record	O
pairs	O
in	O
10	O
minutes	O
.	O

Here	O
,	O
we	O
first	O
briefly	O
review	O
DNF	O
blocking	O
and	O
then	O
introduce	O
our	O
CNF	O
blocking	O
function	O
.	O
This	O
section	O
describes	O
the	O
gain	O
functions	O
that	O
select	O
an	O
optimal	O
predicate	O
term	O
for	O
each	O
step	O
in	O
the	O
CNF	O
learner	O
.	O
Finally	O
,	O
we	O
discuss	O
an	O
extension	O
that	O
ensures	O
the	O
production	O
of	O
disjunctive	O
blocks	O
.	O
Algorithm	O
1	O
DNF	O
Blocking	O
1	O
:	O
function	O
LEARNCONJTERMS	O
(	O
L	O
,	O
P	O
,	O
p	O
,	O
k	O
)	O

Let	O
P	O
os	O
be	O
set	O
of	O
positive	O
samples	O
in	O
L	O
3	O
:	O
Let	O
N	O
eg	O
be	O
set	O
of	O
negative	O
samples	O
in	O
L	O
4	O
:	O
T	O
erms	O
{	O
p	O
}	O
5	O
:	O
CurT	O
erm	O
p	O
6	O
:	O
i	O
1	O
7	O
:	O
while	O
i	O
<	O
k	O
do	O
8	O
:	O
Find	O
p	O
i	O
P	O
that	O
maximizes	O
gain	O
function	O
CALCGAIN	O
(	O
P	O
os	O
,	O
N	O
eg	O
,	O
CurT	O
erm	O
p	O
i	O
)	O
9	O
:	O
CurT	O
erm	O
CurT	O
erm	O
p	O
i	O
10	O
:	O
Add	O
CurT	O
erm	O
to	O
T	O
erms	O
11	O
:	O
Let	O
P	O
osCov	O
be	O
all	O
l	O
P	O
os	O
that	O
satisfies	O
T	O
i	O
i	O
+	O

Let	O
N	O
egCov	O
be	O
all	O
l	O
N	O
eg	O
that	O
satisfies	O
T	O
(	O
Bilenko	O
et	O
al	O
,	O
2006	O
;	O
Michelson	O
and	O
Knoblock	O
,	O
2006	O
)	O
.	O
Given	O
labeled	O
pairs	O
,	O
these	O
methods	O
attempt	O
to	O
learn	O
the	O
blocking	O
function	O
in	O
the	O
form	O
of	O
a	O
DNF	O
,	O
the	O
disjunction	O
(	O
logical	O
OR	O
)	O
of	O
conjunction	O
(	O
logical	O
AN	O
D	O
)	O
terms	O
.	O
Learning	O
DNFs	O
is	O
known	O
to	O
be	O
a	O
NP	O
-	O
hard	O
problem	O
(	O
Bilenko	O
et	O
al	O
,	O
2006	O
)	O
.	O
Thus	O
,	O
an	O
approximation	O
algorithm	O
was	O
used	O
to	O
learn	O
k	O
-	O
DNF	O
blocking	O
by	O
using	O
a	O
sequential	O
covering	O
algorithm	O
.	O
k	O
-	O
DNF	O
means	O
each	O
conjunction	O
term	O
has	O
,	O
at	O
most	O
,	O
k	O
predicates	O
.	O
Algorithm	O
1	O
shows	O
the	O
process	O
of	O
DNF	O
blocking	O
.	O
Function	O
LEARNDNF	O
in	O
lines	O
16	O
-	O
38	O
is	O
the	O
main	O
part	O
of	O
the	O
algorithm	O
.	O
It	O
has	O
3	O
inputs	O
which	O
are	O
the	O
L	O
labeled	O
sample	O
pairs	O
,	O
P	O
blocking	O
predicates	O
,	O
and	O
k	O
parameters	O
of	O
maximum	O
predicates	O
considered	O
for	O
each	O
conjunction	O
term	O
.	O
First	O
,	O
the	O
algorithm	O
selects	O
a	O
set	O
of	O
candidate	O
conjunction	O
terms	O
with	O
at	O
most	O
k	O
predicates	O
.	O
For	O
each	O
predicate	O
p	O
,	O
it	O
generates	O
k	O
candidate	O
conjunction	O
terms	O
with	O
the	O
highest	O
gain	O
function	O
.	O
Using	O
the	O
candidate	O
terms	O
,	O
the	O
algorithm	O
learns	O
the	O
blocking	O
function	O
by	O
using	O
a	O
sequential	O
covering	O
algorithm	O
.	O
It	O
sequentially	O
selects	O
a	O
conjunction	O
term	O
,	O
from	O
the	O
set	O
of	O
candidates	O
,	O
that	O
has	O
the	O
maximum	O
gain	O
value	O
on	O
the	O
remaining	O
samples	O
,	O
and	O
attaches	O
it	O
with	O
logical	O
OR	O
to	O
the	O
DNF	O
term	O
.	O
In	O
each	O
step	O
,	O
all	O
samples	O
covered	O
by	O
the	O
selected	O
conjunction	O
term	O
are	O
removed	O
.	O
This	O
process	O
repeats	O
until	O
it	O
covers	O
the	O
desired	O
minimum	O
amount	O
of	O
positive	O
samples	O
,	O
or	O
there	O
is	O
no	O
candidate	O
term	O
that	O
can	O
further	O
be	O
improved	O
.	O

The	O
gain	O
function	O
estimates	O
the	O
benefit	O
of	O
adding	O
a	O
specific	O
term	O
to	O
the	O
learned	O
formula	O
.	O
It	O
is	O
used	O
in	O
two	O
different	O
places	O
in	O
the	O
algorithm	O
-	O
when	O
choosing	O
the	O
conjunction	O
candidates	O
(	O
line	O
8	O
)	O
and	O
when	O
choosing	O
a	O
term	O
from	O
the	O
candidates	O
for	O
each	O
iteration	O
(	O
line	O
27	O
-	O
28	O
)	O
.	O
Previous	O
methods	O
have	O
proposed	O
different	O
gain	O
functions	O
.	O
Here	O
we	O
describe	O
each	O
and	O
compare	O
the	O
results	O
in	O
the	O
experiments	O
.	O
P	O
,	O
N	O
is	O
the	O
total	O
number	O
of	O
positive	O
and	O
negative	O
samples	O
,	O
and	O
p	O
,	O
n	O
is	O
the	O
number	O
of	O
remaining	O
positive	O
and	O
negative	O
samples	O
covered	O
by	O
the	O
term	O
.	O

Originally	O
from	O
Mooney	O
's	O
CNF	O
learner	O
(	O
1995	O
)	O
,	O
it	O
is	O
the	O
dual	O
of	O
the	O
information	O
gain	O
of	O
a	O
DNF	O
learner	O
gain	O
CN	O
F	O
=	O
n×	O
log	O
n	O
n	O
+	O
p	O
−log	O
N	O
N	O
+	O
P	O
.	O
(	O
Conj	O
LEARNCNF	O
(	O
L	O
,	O
P	O
disjoint	O
,	O
1	O
)	O

Let	O
L	O
be	O
set	O
of	O
l	O
L	O
satisfies	O
Conj	O

CN	O
F	O
LEARNCNF	O
(	O
L	O
remain	O
,	O
P	O
f	O
ull	O
,	O
k	O
)	O

Blocks	O
Apply	O
Conj	O
to	O
whole	O
data	O
6	O
:	O
for	O
Block	O
Blocks	O
do	O
7	O
:	O
Let	O
L	O
be	O
l	O
Block	O
that	O
satisfies	O
CN	O
F	O

We	O
use	O
the	O
PubMed	O
to	O
evaluate	O
these	O
methods	O
.	O
PubMed	O
is	O
a	O
public	O
large	O
-	O
scale	O
scholarly	O
database	O
maintained	O
by	O
the	O
National	O
Center	O
for	O
Biotechnology	O
Information	O
(	O
NCBI	O
)	O
at	O
the	O
National	O
Library	O
of	O
Medicine	O
(	O
NLM	O
)	O
.	O
We	O
use	O
NIH	O
principal	O
investigator	O
(	O
PI	O
)	O
data	O
for	O
evaluation	O
,	O
which	O
include	O
PI	O
IDs	O
and	O
corresponding	O
publications	O
.	O
We	O
randomly	O
picked	O
10	O
names	O
from	O
the	O
most	O
frequent	O
ones	O
in	O
the	O
dataset	O
and	O
manually	O
verified	O
that	O
all	O
publications	O
belong	O
to	O
each	O
PI	O
.	O
The	O
set	O
of	O
names	O
include	O
C	O
*	O
Lee	O
,	O
J	O
*	O
Chen	O
,	O
J	O
*	O
Smith	O
,	O
M	O
*	O
Johnson	O
,	O
M	O
*	O
Miller	O
,	O
R	O
*	O
Jones	O
,	O
S	O
*	O
Kim	O
,	O
X	O
*	O
Yang	O
,	O
Y	O
*	O
Li	O
,	O
Y	O
*	O
Wang	O
,	O
where	O
C	O
*	O
means	O
any	O
name	O
starts	O
with	O
C.	O
Table	O
2	O
shows	O
the	O
statistics	O
of	O
the	O
dataset	O
.	O
Experiments	O
are	O
done	O
with	O
5	O
-	O
fold	O
cross	O
validation	O
.	O

We	O
show	O
how	O
to	O
learn	O
an	O
efficient	O
blocking	O
function	O
with	O
a	O
conjunctive	O
normal	O
form	O
(	O
CNF	O
)	O
of	O
blocking	O
predicates	O
.	O
Using	O
CNF	O
as	O
a	O
negation	O
of	O
the	O
corresponding	O
disjunctive	O
normal	O
form	O
(	O
DNF	O
)	O
of	O
predicates	O
(	O
Mooney	O
,	O
1995	O
)	O
,	O
our	O
method	O
is	O
a	O
logical	O
dual	O
of	O
existing	O
DNF	O
blocking	O
methods	O
(	O
Bilenko	O
et	O
al	O
,	O
2006	O
;	O
Michelson	O
and	O
Knoblock	O
,	O
2006	O
)	O
.	O
We	O
find	O
that	O
our	O
method	O
reduces	O
more	O
pairs	O
for	O
a	O
large	O
number	O
of	O
target	O
pairs	O
completeness	O
and	O
has	O
a	O
faster	O
run	O
time	O
.	O
We	O
devise	O
an	O
extension	O
that	O
ensures	O
that	O
our	O
CNF	O
blocking	O
produces	O
disjoint	O
blocks	O
.	O
Thus	O
,	O
the	O
clustering	O
process	O
can	O
be	O
efficiently	O
parallelized	O
.	O
Future	O
work	O
could	O
use	O
multiple	O
levels	O
of	O
blocking	O
functions	O
for	O
processing	O
each	O
block	O
(	O
Das	O
Sarma	O
et	O
al	O
,	O
2012	O
)	O
and	O
using	O
linear	O
programming	O
to	O
find	O
an	O
optimal	O
CNF	O
(	O
Su	O
et	O
al	O
,	O
2016	O
)	O
.	O

We	O
gratefully	O
acknowledge	O
partial	O
support	O
from	O
the	O
National	O
Science	O
Foundation	O
and	O
the	O
National	O
Bureau	O
of	O
Economic	O
Research	O
and	O
useful	O
discussions	O
with	O
Bruce	O
Weinberg	O
.	O

The	O
W	O
-	O
NUT	O
2022	O
workshop	O
focuses	O
on	O
a	O
core	O
set	O
of	O
natural	O
language	O
processing	O
tasks	O
on	O
top	O
of	O
noisy	O
user	O
-	O
generated	O
text	O
,	O
such	O
as	O
that	O
found	O
on	O
social	O
media	O
,	O
web	O
forums	O
and	O
online	O
reviews	O
.	O
Recent	O
years	O
have	O
seen	O
a	O
significant	O
increase	O
of	O
interest	O
in	O
these	O
areas	O
.	O
The	O
internet	O
has	O
democratized	O
content	O
creation	O
leading	O
to	O
an	O
explosion	O
of	O
informal	O
user	O
-	O
generated	O
text	O
,	O
publicly	O
available	O
in	O
electronic	O
format	O
,	O
motivating	O
the	O
need	O
for	O
NLP	O
on	O
noisy	O
text	O
to	O
enable	O
new	O
data	O
analytics	O
applications	O
.	O
We	O
have	O
received	O
39	O
main	O
workshop	O
submissions	O
(	O
22	O
long	O
and	O
17	O
short	O
papers	O
)	O
.	O
The	O
workshop	O
will	O
be	O
held	O
in	O
hybrid	O
in	O
-	O
person	O
and	O
virtual	O
modes	O
.	O
We	O
have	O
two	O
invited	O
speakers	O
Yulia	O
Tsvetkov	O
(	O
University	O
of	O
Washington	O
)	O
and	O
David	O
Jurgens	O
(	O
University	O
of	O
Michigan	O
)	O
who	O
will	O
talk	O
about	O
their	O
work	O
.	O
We	O
're	O
very	O
thankful	O
to	O
have	O
them	O
in	O
our	O
workshop	O
.	O
We	O
have	O
the	O
best	O
paper	O
award	O
(	O
s	O
)	O
sponsored	O
by	O
Megagon	O
Labs	O
this	O
year	O
,	O
for	O
which	O
we	O
are	O
thankful	O
.	O
We	O
would	O
like	O
to	O
thank	O
the	O
Program	O
Committee	O
members	O
who	O
reviewed	O
the	O
papers	O
.	O
We	O
would	O
also	O
like	O
to	O
thank	O
the	O
workshop	O
participants	O
.	O

This	O
paper	O
describes	O
the	O
CMU	O
submission	O
to	O
shared	O
task	O
1	O
of	O
SIGMORPHON	O
2017	O
.	O
The	O
system	O
is	O
based	O
on	O
the	O
multi	O
-	O
space	O
variational	O
encoder	O
-	O
decoder	O
(	O
MSVED	O
)	O
method	O
of	O
Zhou	O
and	O
Neubig	O
(	O
2017	O
)	O
,	O
which	O
employs	O
both	O
continuous	O
and	O
discrete	O
latent	O
variables	O
for	O
the	O
variational	O
encoder	O
-	O
decoder	O
and	O
is	O
trained	O
in	O
a	O
semi	O
-	O
supervised	O
fashion	O
.	O
We	O
discuss	O
some	O
language	O
-	O
specific	O
errors	O
and	O
present	O
result	O
analysis	O
.	O

In	O
this	O
section	O
we	O
will	O
detail	O
the	O
multi	O
-	O
space	O
variational	O
encoder	O
-	O
decoder	O
model	O
.	O

In	O
morphological	O
reinflection	O
,	O
the	O
source	O
sequence	O
x	O
(	O
s	O
)	O
consists	O
of	O
the	O
characters	O
in	O
an	O
inflected	O
word	O
(	O
e.g.	O
,	O
"	O
played	O
"	O
)	O
,	O
while	O
the	O
associated	O
labels	O
y	O
(	O
t	O
)	O
describe	O
some	O
linguistic	O
features	O
(	O
e.g.	O
,	O
y	O
(	O
t	O
)	O
pos	O
=	O
Verb	O
,	O
y	O
(	O
t	O
)	O
tense	O
=	O
Past	O
)	O
that	O
we	O
hope	O
to	O
realize	O
in	O
the	O
target	O
.	O
The	O
target	O
sequence	O
x	O
(	O
t	O
)	O
is	O
therefore	O
the	O
characters	O
of	O
the	O
re	O
-	O
inflected	O
form	O
of	O
the	O
source	O
word	O
(	O
e.g.	O
,	O
"	O
played	O
"	O
)	O
that	O
satisfy	O
the	O
linguistic	O
features	O
specified	O
by	O
y	O
(	O
t	O
)	O
.	O
For	O
this	O
task	O
,	O
each	O
discrete	O
variable	O
y	O
(	O
t	O
)	O
k	O
has	O
a	O
set	O
of	O
possible	O
labels	O
(	O
e.g.	O
pos	O
=	O
V	O
,	O
pos	O
=	O
ADJ	O
,	O
etc	O
)	O
and	O
follows	O
a	O
multinomial	O
distribution	O
.	O

We	O
process	O
the	O
Wikipedia	O
corpus	O
provided	O
by	O
the	O
shared	O
task	O
organizer	O
as	O
our	O
unsupervised	O
training	O
data	O
together	O
with	O
words	O
in	O
the	O
training	O
data	O
.	O
For	O
each	O
language	O
,	O
we	O
first	O
get	O
the	O
character	O
vocabulary	O
of	O
the	O
corresponding	O
training	O
data	O
and	O
only	O
keep	O
words	O
in	O
the	O
Wiki	O
corpus	O
for	O
which	O
characters	O
are	O
all	O
in	O
the	O
character	O
set	O
we	O
obtained	O
.	O
All	O
words	O
that	O
occur	O
less	O
than	O
20	O
times	O
are	O
eliminated	O
.	O
We	O
also	O
limit	O
the	O
number	O
of	O
words	O
used	O
during	O
training	O
to	O
be	O
the	O
50000	O
most	O
frequent	O
words	O
.	O

In	O
this	O
work	O
,	O
we	O
further	O
examine	O
the	O
method	O
proposed	O
in	O
(	O
Zhou	O
and	O
Neubig	O
,	O
2017	O
)	O
for	O
the	O
shared	O
task	O
of	O
SIGMORPHON	O
2017	O
on	O
52	O
languages	O
and	O
demonstrate	O
the	O
effectiveness	O
of	O
this	O
approach	O
.	O
We	O
will	O
further	O
improve	O
our	O
model	O
's	O
sophistication	O
by	O
investigating	O
strategies	O
for	O
choosing	O
appropriate	O
semi	O
-	O
supervised	O
data	O
,	O
and	O
examining	O
the	O
model	O
's	O
performance	O
on	O
languages	O
with	O
a	O
high	O
inflection	O
level	O
.	O

This	O
work	O
has	O
been	O
supported	O
in	O
part	O
by	O
an	O
Amazon	O
Academic	O
Research	O
Award	O
.	O
We	O
thank	O
Matthew	O
Honnibal	O
for	O
pointing	O
out	O
that	O
the	O
data	O
distribution	O
of	O
Wikipedia	O
corpus	O
might	O
be	O
biased	O
.	O

Autoregressive	O
generation	O
(	O
AG	O
)	O
models	O
generate	O
sequences	O
based	O
on	O
a	O
left	O
-	O
to	O
-	O
right	O
factorization	O
.	O
As	O
shown	O
in	O
Figure	O
1	O
,	O
given	O
the	O
source	O
sequence	O
X	O
,	O
the	O
target	O
sequence	O
Y	O
with	O
length	O
T	O
is	O
generated	O
via	O
a	O
chain	O
of	O
conditional	O
probabilities	O
based	O
on	O
the	O
left	O
-	O
to	O
-	O
right	O
sequential	O
dependencies	O
as	O
:	O
p	O
(	O
Y	O
|	O
X	O
)	O
=	O
T	O
i=1	O
p	O
(	O
y	O
i	O
|	O
y	O
<	O
i	O
,	O
X	O
)	O
,	O
(	O
1	O
)	O
where	O
y	O
<	O
i	O
denotes	O
the	O
tokens	O
before	O
the	O
i	O
-	O
th	O
step	O
.	O
This	O
property	O
of	O
autoregressive	O
factorization	O
makes	O
the	O
generation	O
process	O
hard	O
to	O
be	O
parallelized	O
as	O
the	O
result	O
is	O
generated	O
token	O
by	O
token	O
.	O
Unlike	O
AG	O
models	O
,	O
non	O
-	O
autoregressive	O
(	O
NAG	O
)	O
models	O
generate	O
sequences	O
without	O
modelling	O
the	O
output	O
-	O
side	O
dependencies	O
.	O
As	O
shown	O
in	O
Figure	O
1	O
,	O
given	O
the	O
prespecified	O
output	O
length	O
T	O
,	O
the	O
probability	O
of	O
the	O
target	O
sequence	O
Y	O
is	O
then	O
modelled	O
as	O
:	O
p	O
(	O
Y	O
|	O
X	O
)	O
=	O
T	O
i=1	O
p	O
(	O
y	O
i	O
|	O
X	O
,	O
i	O
,	O
T	O
)	O
.	O
(	O
2	O
)	O
With	O
this	O
conditional	O
independence	O
assumption	O
,	O
NAG	O
models	O
can	O
fully	O
parallelize	O
their	O
generation	O
process	O
,	O
which	O
significantly	O
improves	O
the	O
inference	O
speed	O
.	O
However	O
,	O
it	O
has	O
been	O
shown	O
that	O
,	O
the	O
choice	O
of	O
the	O
prespecified	O
output	O
length	O
has	O
a	O
notable	O
impact	O
on	O
the	O
model	O
's	O
generation	O
quality	O
(	O
Gu	O
et	O
al	O
,	O
2018	O
)	O
.	O
In	O
addition	O
,	O
the	O
removal	O
of	O
output	O
-	O
side	O
sequential	O
dependency	O
also	O
causes	O
the	O
generation	O
quality	O
of	O
NAG	O
models	O
to	O
be	O
inferior	O
to	O
their	O
autoregressive	O
counterparts	O
(	O
Wang	O
et	O
al	O
,	O
2019b	O
)	O
.	O

The	O
authors	O
wish	O
to	O
thank	O
Jialu	O
Xu	O
,	O
Guanlin	O
Li	O
,	O
Xing	O
Wang	O
for	O
their	O
insightful	O
discussions	O
and	O
support	O
.	O
Many	O
thanks	O
to	O
our	O
anonymous	O
reviewers	O
for	O
their	O
suggestions	O
and	O
comments	O
.	O

Read	O
,	O
Revise	O
,	O
Repeat	O
:	O
A	O
System	O
Demonstration	O
for	O
Human	O
-	O
in	O
-	O
the	O
-	O
loop	O
Iterative	O
Text	O
Revision	O

Previous	O
works	O
on	O
modeling	O
text	O
revision	O
Botha	O
et	O
al	O
,	O
2018	O
;	O
Ito	O
et	O
al	O
,	O
2019	O
;	O
Faltings	O
et	O
al	O
,	O
2021	O
)	O
have	O
ignored	O
the	O
iterative	O
nature	O
of	O
the	O
task	O
,	O
and	O
simplified	O
it	O
into	O
a	O
one	O
-	O
shot	O
"	O
original	O
-	O
to	O
-	O
final	O
"	O
sentence	O
-	O
to	O
-	O
sentence	O
generation	O
task	O
.	O
However	O
,	O
in	O
practice	O
,	O
at	O
every	O
revision	O
step	O
,	O
multiple	O
edits	O
happen	O
at	O
the	O
document	O
-	O
level	O
which	O
also	O
play	O
an	O
important	O
role	O
in	O
text	O
revision	O
.	O
For	O
instance	O
,	O
reordering	O
and	O
deleting	O
sentences	O
to	O
improve	O
the	O
coherence	O
.	O
More	O
importantly	O
,	O
performing	O
multiple	O
highquality	O
edits	O
at	O
once	O
is	O
very	O
challenging	O
.	O
Continuing	O
the	O
previous	O
example	O
,	O
document	O
readability	O
can	O
degrade	O
after	O
reordering	O
sentences	O
,	O
and	O
further	O
adding	O
transitional	O
phrases	O
is	O
often	O
required	O
to	O
make	O
the	O
document	O
more	O
coherent	O
and	O
readable	O
.	O
Therefore	O
,	O
one	O
-	O
shot	O
sentence	O
-	O
to	O
-	O
sentence	O
text	O
revision	O
formulation	O
is	O
not	O
sufficient	O
to	O
deal	O
with	O
real	O
-	O
world	O
challenges	O
in	O
text	O
revision	O
tasks	O
.	O
While	O
some	O
prior	O
works	O
on	O
text	O
revision	O
(	O
Coenen	O
et	O
al	O
,	O
2021	O
;	O
Padmakumar	O
and	O
He	O
,	O
2021	O
;	O
Gero	O
et	O
al	O
,	O
2021	O
;	O
Lee	O
et	O
al	O
,	O
2022	O
)	O
have	O
proposed	O
humanmachine	O
collaborative	O
writing	O
interfaces	O
,	O
they	O
are	O
mostly	O
focused	O
on	O
collecting	O
human	O
-	O
machine	O
interaction	O
data	O
for	O
training	O
better	O
neural	O
models	O
,	O
rather	O
than	O
understanding	O
the	O
iterative	O
nature	O
of	O
the	O
text	O
revision	O
process	O
,	O
or	O
the	O
model	O
's	O
ability	O
to	O
adjust	O
editing	O
suggestions	O
according	O
to	O
human	O
feedback	O
.	O
Another	O
line	O
of	O
work	O
by	O
Sun	O
et	O
al	O
(	O
2021	O
)	O
;	O
Singh	O
et	O
al	O
(	O
2022	O
)	O
on	O
creative	O
writing	O
designed	O
humanmachine	O
interaction	O
interfaces	O
to	O
encourage	O
new	O
content	O
generation	O
.	O
However	O
,	O
text	O
revision	O
focuses	O
on	O
improving	O
the	O
quality	O
of	O
existing	O
writing	O
and	O
keeping	O
the	O
original	O
content	O
as	O
much	O
as	O
possible	O
.	O
In	O
this	O
work	O
,	O
we	O
provide	O
a	O
human	O
-	O
in	O
-	O
the	O
-	O
loop	O
text	O
revision	O
system	O
to	O
make	O
helpful	O
editing	O
suggestions	O
by	O
interacting	O
with	O
users	O
in	O
an	O
iterative	O
way	O
.	O

Figure	O
1	O
shows	O
the	O
general	O
pipeline	O
of	O
R3	O
humanin	O
-	O
the	O
-	O
loop	O
iterative	O
text	O
revision	O
system	O
.	O
In	O
this	O
section	O
,	O
we	O
will	O
describe	O
the	O
development	O
details	O
of	O
the	O
text	O
revision	O
models	O
and	O
demonstrate	O
our	O
user	O
interfaces	O
.	O
We	O
first	O
formulate	O
an	O
iterative	O
text	O
revision	O
process	O
:	O
given	O
a	O
source	O
document	O
1	O
D	O
t−1	O
,	O
at	O
each	O
revision	O
depth	O
t	O
,	O
a	O
text	O
revision	O
system	O
will	O
apply	O
a	O
set	O
of	O
edits	O
to	O
get	O
the	O
revised	O
document	O
D	O
t	O
.	O
The	O
system	O
will	O
continue	O
iterating	O
revision	O
until	O
the	O
revised	O
document	O
D	O
t	O
satisfies	O
a	O
set	O
of	O
predefined	O
stopping	O
criteria	O
,	O
such	O
as	O
reaching	O
a	O
predefined	O
maximum	O
revision	O
depth	O
t	O
max	O
,	O
or	O
making	O
no	O
edits	O
between	O
D	O
t−1	O
and	O
D	O
t	O
.	O

We	O
follow	O
the	O
prior	O
work	O
of	O
Du	O
et	O
al	O
(	O
2022	O
)	O
to	O
build	O
our	O
text	O
revision	O
system	O
.	O
The	O
system	O
is	O
composed	O
of	O
edit	O
intention	O
identification	O
models	O
and	O
a	O
text	O
revision	O
generation	O
model	O
.	O
We	O
follow	O
the	O
same	O
data	O
collection	O
procedure	O
in	O
Du	O
et	O
al	O
(	O
2022	O
)	O
to	O
collect	O
the	O
iterative	O
revision	O
data	O
.	O
2	O
Then	O
,	O
we	O
train	O
the	O
three	O
models	O
on	O
the	O
collected	O
revision	O
dataset	O
.	O
Edit	O
Intention	O
Identification	O
Models	O
.	O
Following	O
Du	O
et	O
al	O
(	O
2022	O
)	O
,	O
our	O
edit	O
intentions	O
have	O
four	O
categories	O
:	O
FLUENCY	O
,	O
COHERENCE	O
,	O
CLARITY	O
,	O
and	O
STYLE	O
.	O
We	O
build	O
our	O
edit	O
intention	O
identification	O
models	O
at	O
each	O
sentence	O
of	O
the	O
source	O
document	O
D	O
t−1	O
to	O
capture	O
the	O
more	O
fine	O
-	O
grained	O
edits	O
.	O
Specifically	O
,	O
given	O
a	O
source	O
sentence	O
,	O
the	O
system	O
will	O
make	O
two	O
-	O
step	O
predictions	O
:	O
(	O
1	O
)	O
whether	O
or	O
not	O
to	O
edit	O
,	O
and	O
(	O
2	O
)	O
which	O
edit	O
intention	O
to	O
apply	O
.	O
The	O
decision	O
whether	O
or	O
not	O
to	O
edit	O
is	O
taken	O
by	O
an	O
edit	O
-	O
prediction	O
classifier	O
that	O
predicts	O
a	O
binary	O
label	O
of	O
whether	O
to	O
edit	O
a	O
sentence	O
or	O
not	O
.	O
The	O
second	O
model	O
,	O
called	O
the	O
edit	O
-	O
intention	O
classifier	O
,	O
predicts	O
which	O
edit	O
intention	O
to	O
apply	O
to	O
the	O
sentence	O
.	O
If	O
the	O
edit	O
-	O
prediction	O
model	O
predicts	O
"	O
not	O
to	O
edit	O
"	O
in	O
the	O
first	O
step	O
,	O
the	O
source	O
sentence	O
will	O
be	O
kept	O
unchanged	O
at	O
the	O
current	O
revision	O
depth	O
.	O
Text	O
Revision	O
Generation	O
Model	O
.	O
We	O
fine	O
-	O
tune	O
a	O
large	O
pre	O
-	O
trained	O
language	O
model	O
like	O
PEGA	O
-	O
SUS	O
(	O
Zhang	O
et	O
al	O
,	O
2020	O
)	O
on	O
our	O
collected	O
revision	O
dataset	O
to	O
build	O
the	O
text	O
revision	O
generation	O
model	O
.	O
Given	O
a	O
source	O
sentence	O
and	O
its	O
predicted	O
edit	O
intention	O
,	O
the	O
model	O
will	O
generate	O
a	O
revised	O
sentence	O
,	O
conditioned	O
on	O
the	O
predicted	O
edit	O
intention	O
.	O
Then	O
,	O
we	O
concatenate	O
all	O
un	O
-	O
revised	O
and	O
revised	O
sentences	O
to	O
get	O
the	O
model	O
-	O
revised	O
document	O
D	O
t	O
,	O
and	O
extract	O
all	O
its	O
edits	O
using	O
latexdiff	O
3	O
and	O
difflib	O
.	O
4	O
In	O
summary	O
,	O
at	O
each	O
revision	O
depth	O
t	O
,	O
given	O
a	O
source	O
document	O
D	O
t−1	O
,	O
the	O
text	O
revision	O
system	O
first	O
predicts	O
the	O
need	O
for	O
revising	O
a	O
sentence	O
,	O
and	O
for	O
the	O
ones	O
that	O
need	O
revision	O
,	O
it	O
predicts	O
the	O
corresponding	O
fine	O
-	O
grained	O
edit	O
intentions	O
-	O
thus	O
,	O
generating	O
the	O
revised	O
document	O
D	O
t	O
based	O
on	O
the	O
source	O
document	O
and	O
the	O
predicted	O
edit	O
decisions	O
and	O
intentions	O
.	O

In	O
practice	O
,	O
not	O
all	O
model	O
-	O
generated	O
edits	O
are	O
equally	O
impactful	O
towards	O
improving	O
the	O
document	O
quality	O
(	O
Du	O
et	O
al	O
,	O
2022	O
)	O
.	O
Therefore	O
,	O
we	O
enable	O
user	O
interaction	O
in	O
the	O
iterative	O
text	O
revision	O
process	O
to	O
achieve	O
high	O
quality	O
of	O
text	O
revisions	O
along	O
with	O
a	O
productive	O
writing	O
experience	O
.	O
At	O
each	O
revision	O
depth	O
t	O
,	O
our	O
system	O
provides	O
the	O
user	O
with	O
suggested	O
edits	O
,	O
and	O
their	O
corresponding	O
edit	O
intentions	O
.	O
The	O
user	O
can	O
interact	O
with	O
the	O
system	O
by	O
choosing	O
to	O
accept	O
or	O
reject	O
the	O
suggested	O
edits	O
.	O
Figure	O
2	O
illustrates	O
the	O
details	O
of	O
R3	O
's	O
user	O
interface	O
.	O
First	O
,	O
a	O
user	O
enters	O
their	O
i	O
d	O
to	O
login	O
to	O
the	O
web	O
interface	O
as	O
shown	O
in	O
Figure	O
2a	O
.	O
Then	O
,	O
the	O
user	O
is	O
instructed	O
with	O
a	O
few	O
guidelines	O
on	O
how	O
to	O
operate	O
the	O
revision	O
as	O
demonstrated	O
in	O
Figure	O
2b	O
.	O
After	O
getting	O
familiar	O
with	O
the	O
interface	O
,	O
the	O
user	O
can	O
select	O
a	O
source	O
document	O
from	O
the	O
left	O
dropdown	O
menu	O
in	O
Figure	O
2c	O
.	O
By	O
clicking	O
the	O
source	O
document	O
,	O
all	O
the	O
edits	O
predicted	O
by	O
the	O
text	O
re	O
-	O
vision	O
model	O
,	O
as	O
well	O
as	O
their	O
corresponding	O
edit	O
intentions	O
will	O
show	O
up	O
in	O
the	O
main	O
page	O
as	O
illustrated	O
in	O
Figure	O
2d	O
(	O
left	O
panel	O
)	O
.	O
The	O
user	O
is	O
guided	O
to	O
go	O
through	O
each	O
suggested	O
edits	O
,	O
and	O
choose	O
to	O
accept	O
or	O
reject	O
the	O
current	O
edit	O
by	O
clicking	O
the	O
Confirm	O
button	O
in	O
Figure	O
2d	O
(	O
right	O
panel	O
)	O
.	O
After	O
going	O
through	O
all	O
the	O
suggested	O
edits	O
,	O
the	O
user	O
is	O
guided	O
to	O
click	O
the	O
Submit	O
button	O
to	O
save	O
their	O
decisions	O
on	O
the	O
edits	O
.	O
Then	O
,	O
the	O
user	O
is	O
guided	O
to	O
click	O
the	O
Next	O
Iteration	O
!	O
button	O
to	O
proceed	O
to	O
the	O
next	O
revision	O
depth	O
and	O
check	O
the	O
next	O
round	O
of	O
edits	O
suggested	O
by	O
the	O
system	O
.	O
This	O
interactive	O
process	O
continues	O
until	O
the	O
system	O
does	O
not	O
generate	O
further	O
edits	O
or	O
reaches	O
the	O
maximum	O
revision	O
depth	O
t	O
max	O
.	O

We	O
conduct	O
experiments	O
to	O
answer	O
the	O
following	O
research	O
questions	O
:	O
RQ1	O
How	O
likely	O
are	O
users	O
to	O
accept	O
the	O
editing	O
suggestions	O
predicted	O
by	O
our	O
text	O
revision	O
system	O
?	O
This	O
question	O
is	O
designed	O
to	O
evaluate	O
whether	O
our	O
text	O
revision	O
system	O
can	O
generate	O
high	O
quality	O
edits	O
.	O
RQ2	O
Which	O
types	O
of	O
edit	O
intentions	O
are	O
more	O
likely	O
to	O
be	O
accepted	O
by	O
users	O
?	O
This	O
question	O
is	O
aimed	O
to	O
identify	O
which	O
types	O
of	O
edits	O
are	O
more	O
favored	O
by	O
users	O
.	O
RQ3	O
Does	O
user	O
feedback	O
in	O
R3	O
help	O
produce	O
higher	O
quality	O
of	O
revised	O
documents	O
?	O
This	O
question	O
is	O
proposed	O
to	O
validate	O
the	O
effectiveness	O
of	O
human	O
-	O
in	O
-	O
the	O
-	O
loop	O
component	O
in	O
R3	O
.	O

Iterative	O
Revision	O
Systems	O
.	O
We	O
prepare	O
three	O
types	O
of	O
iterative	O
revision	O
systems	O
to	O
answer	O
the	O
above	O
questions	O
:	O
1	O
.	O
HUMAN	O
-	O
HUMAN	O
:	O
We	O
ask	O
users	O
to	O
accept	O
or	O
reject	O
text	O
revisions	O
made	O
by	O
human	O
writers	O
,	O
which	O
are	O
directly	O
sampled	O
from	O
our	O
collected	O
iterative	O
revision	O
dataset	O
.	O
This	O
serves	O
as	O
the	O
baseline	O
to	O
measure	O
the	O
gap	O
between	O
our	O
text	O
revision	O
system	O
and	O
human	O
writers	O
.	O
2	O
.	O
SYSTEM	O
-	O
HUMAN	O
:	O
We	O
ask	O
users	O
to	O
accept	O
or	O
reject	O
text	O
revisions	O
made	O
by	O
our	O
system	O
.	O
Then	O
,	O
we	O
incorporate	O
user	O
accepted	O
edits	O
to	O
the	O
system	O
to	O
generate	O
the	O
next	O
iteration	O
of	O
revision	O
.	O
This	O
is	O
the	O
standard	O
human	O
-	O
in	O
-	O
the	O
-	O
loop	O
process	O
of	O
R3	O
.	O

When	O
R3	O
generates	O
revisions	O
at	O
deeper	O
depths	O
,	O
we	O
observe	O
a	O
decrease	O
in	O
the	O
acceptance	O
ratio	O
by	O
human	O
users	O
.	O
It	O
is	O
crucial	O
to	O
create	O
a	O
text	O
revision	O
system	O
that	O
can	O
learn	O
different	O
revision	O
strategies	O
at	O
each	O
iteration	O
and	O
generate	O
high	O
quality	O
edits	O
at	O
deeper	O
revision	O
levels	O
.	O
Editing	O
suggestions	O
provided	O
by	O
our	O
text	O
revision	O
generation	O
models	O
could	O
be	O
improved	O
.	O
Particularly	O
,	O
FLUENCY	O
edits	O
show	O
a	O
huge	O
gap	O
between	O
human	O
and	O
system	O
revisions	O
(	O
45.05	O
%	O
and	O
82.02	O
%	O
)	O
.	O
Future	O
work	O
could	O
focus	O
on	O
developing	O
more	O
powerful	O
text	O
revision	O
generation	O
models	O
.	O
In	O
our	O
human	O
-	O
machine	O
interaction	O
,	O
we	O
restrict	O
the	O
users	O
'	O
role	O
to	O
accept	O
or	O
reject	O
the	O
model	O
's	O
predictions	O
.	O
Even	O
with	O
minimal	O
human	O
interaction	O
,	O
our	O
experiment	O
shows	O
comparable	O
or	O
even	O
better	O
revision	O
quality	O
as	O
compared	O
to	O
human	O
writers	O
at	O
early	O
revision	O
depths	O
.	O
A	O
potential	O
future	O
direction	O
for	O
human	O
-	O
machine	O
collaborative	O
text	O
revision	O
would	O
be	O
to	O
develop	O
advanced	O
human	O
-	O
machine	O
interaction	O
interfaces	O
,	O
such	O
as	O
asking	O
users	O
to	O
re	O
-	O
write	O
the	O
machine	O
-	O
revised	O
text	O
.	O
Also	O
,	O
a	O
larger	O
-	O
scale	O
user	O
study	O
could	O
be	O
carried	O
out	O
to	O
derive	O
more	O
meaningful	O
statistics	O
(	O
e.g.	O
optimal	O
number	O
of	O
revision	O
depths	O
and	O
edit	O
suggestions	O
)	O
and	O
investigate	O
if	O
there	O
is	O
any	O
intriguing	O
user	O
behavior	O
in	O
the	O
iterative	O
revision	O
process	O
.	O
For	O
example	O
,	O
as	O
mentioned	O
in	O
the	O
users	O
'	O
feedback	O
,	O
it	O
would	O
be	O
interesting	O
to	O
check	O
if	O
users	O
behave	O
differently	O
when	O
they	O
are	O
asked	O
to	O
accept	O
/	O
reject	O
edit	O
suggestions	O
provided	O
for	O
their	O
own	O
texts	O
as	O
opposed	O
to	O
the	O
texts	O
written	O
by	O
a	O
third	O
party	O
.	O

We	O
thank	O
all	O
linguistic	O
expert	O
annotators	O
at	O
Grammarly	O
for	O
participating	O
in	O
the	O
user	O
study	O
and	O
providing	O
us	O
with	O
valuable	O
feedback	O
during	O
the	O
process	O
.	O
We	O
also	O
thank	O
Karin	O
de	O
Langis	O
at	O
University	O
of	O
Minnesota	O
for	O
narrating	O
the	O
video	O
of	O
our	O
system	O
demonstration	O
.	O
We	O
would	O
like	O
to	O
extend	O
our	O
gratitude	O
to	O
the	O
anonymous	O
reviewers	O
for	O
their	O
helpful	O
comments	O
.	O

The	O
authors	O
gratefully	O
acknowledge	O
the	O
financial	O
support	O
of	O
the	O
research	O
reported	O
here	O
by	O
the	O
grant	O
Modellierung	O
lexikalisch	O
-	O
semantischer	O
Beziehungen	O
von	O
Kollokationen	O
awarded	O
by	O
the	O
Deutsche	O
Forschungsgemeinschaft	O
(	O
DFG	O
)	O
.	O
We	O
would	O
also	O
like	O
to	O
thank	O
three	O
anonymous	O
reviewers	O
for	O
their	O
constructive	O
remarks	O
on	O
an	O
earlier	O
version	O
of	O
this	O
paper	O
.	O

Hate	O
and	O
Toxic	O
Speech	O
Detection	O
in	O
the	O
Context	O
of	O
Covid	O
-	O
19	O
Pandemic	O
using	O
XAI	O
:	O
Ongoing	O
Applied	O
Research	O

Welcome	O
to	O
the	O
inaugural	O
Workshop	O
on	O
Knowledge	O
Extraction	O
and	O
Integration	O
for	O
Deep	O
Learning	O
Architectures	O
(	O
DeeLIO	O
)	O
!	O
The	O
DeeLIO	O
workshop	O
aims	O
to	O
bring	O
together	O
the	O
knowledge	O
interpretation	O
,	O
extraction	O
and	O
integration	O
lines	O
of	O
research	O
in	O
deep	O
learning	O
,	O
and	O
cover	O
the	O
area	O
in	O
between	O
.	O
We	O
hope	O
that	O
the	O
DeeLIO	O
workshop	O
will	O
become	O
a	O
regular	O
forum	O
for	O
the	O
exchange	O
of	O
ideas	O
and	O
will	O
contribute	O
to	O
foster	O
collaboration	O
within	O
these	O
research	O
fields	O
.	O
This	O
volume	O
includes	O
the	O
11	O
papers	O
presented	O
at	O
the	O
workshop	O
.	O
DeeLIO	O
was	O
co	O
-	O
located	O
with	O
the	O
2020	O
Conference	O
on	O
Empirical	O
Methods	O
in	O
Natural	O
Language	O
Processing	O
(	O
EMNLP	O
2020	O
)	O
and	O
was	O
held	O
on	O
November	O
19	O
,	O
2020	O
as	O
an	O
online	O
workshop	O
,	O
following	O
the	O
exceptional	O
"	O
new	O
normal	O
"	O
circumstances	O
of	O
2020	O
.	O
For	O
the	O
first	O
edition	O
of	O
the	O
workshop	O
,	O
we	O
received	O
21	O
paper	O
submissions	O
.	O
We	O
accepted	O
11	O
papers	O
(	O
acceptance	O
rate	O
52.4	O
%	O
)	O
which	O
were	O
presented	O
at	O
the	O
workshop	O
.	O
The	O
accepted	O
papers	O
cover	O
both	O
thematic	O
aspects	O
of	O
DeeLIO	O
:	O
the	O
extraction	O
of	O
linguistic	O
knowledge	O
from	O
deep	O
neural	O
models	O
and	O
the	O
integration	O
of	O
knowledge	O
from	O
external	O
resources	O
,	O
for	O
different	O
languages	O
and	O
applications	O
.	O
In	O
addition	O
to	O
the	O
regular	O
workshop	O
papers	O
,	O
the	O
first	O
edition	O
of	O
DeeLIO	O
also	O
included	O
presentations	O
of	O
several	O
papers	O
from	O
the	O
EMNLP	O
companion	O
volume	O
'	O
Findings	O
of	O
EMNLP	O
'	O
which	O
were	O
thematically	O
relevant	O
to	O
the	O
workshop	O
goals	O
.	O
There	O
was	O
no	O
distinction	O
between	O
oral	O
and	O
poster	O
presentations	O
this	O
year	O
,	O
and	O
all	O
presentations	O
involved	O
pre	O
-	O
recorded	O
talks	O
accompanied	O
with	O
individual	O
live	O
Q&A	O
sessions	O
.	O
We	O
take	O
this	O
opportunity	O
to	O
thank	O
the	O
DeeLIO	O
program	O
committee	O
for	O
their	O
thorough	O
reviews	O
.	O
We	O
also	O
thank	O
the	O
authors	O
who	O
presented	O
their	O
work	O
at	O
DeeLIO	O
,	O
and	O
the	O
workshop	O
participants	O
for	O
the	O
valuable	O
feedback	O
and	O
discussions	O
.	O
Finally	O
,	O
we	O
are	O
honored	O
to	O
have	O
two	O
excellent	O
invited	O
talks	O
from	O
our	O
invited	O
speakers	O
Ellie	O
Pavlick	O
and	O
Eduard	O
Hovy	O
.	O

Learning	O
Input	O
Strictly	O
Local	O
Functions	O
:	O
Comparing	O
Approaches	O
with	O
Catalan	O
Adjectives	O

The	O
FestCat	O
project	O
(	O
Bonafonte	O
et	O
al	O
,	O
2008	O
)	O
provides	O
broad	O
transcriptions	O
for	O
more	O
than	O
53	O
,	O
000	O
adjectival	O
surface	O
forms	O
in	O
two	O
major	O
dialects	O
of	O
Catalan	O
.	O
We	O
considered	O
the	O
Central	O
Catalan	O
forms	O
and	O
restricted	O
our	O
data	O
to	O
the	O
nearly	O
6	O
,	O
500	O
lemmas	O
that	O
are	O
also	O
attested	O
in	O
a	O
subtitle	O
lexicon	O
(	O
Boada	O
et	O
al	O
,	O
2020	O
)	O
.	O
While	O
our	O
main	O
focus	O
was	O
on	O
learning	O
,	O
we	O
also	O
developed	O
a	O
hand	O
-	O
written	O
ISL	O
transducer	O
for	O
the	O
mapping	O
to	O
masc.sg	O
.	O
forms	O
that	O
is	O
highly	O
accurate	O
(	O
>	O
98	O
%	O
correct	O
)	O
,	O
along	O
with	O
custom	O
code	O
to	O
derive	O
plausible	O
underlying	O
representations	O
from	O
masc.sg	O
.	O
∼	O
fem.sg	O
.	O
pairs	O
.	O

We	O
evaluated	O
all	O
four	O
models	O
on	O
the	O
same	O
training	O
/	O
validation	O
/	O
testing	O
data	O
,	O
as	O
summarized	O
in	O
Table	O
1	O
.	O
ISLFLA	O
and	O
OSTIA	O
were	O
unable	O
to	O
learn	O
accurate	O
mappings	O
except	O
when	O
the	O
fem.sg	O
.	O
and	O
masc.sg	O
.	O
forms	O
were	O
artificially	O
trimmed	O
to	O
their	O
final	O
VC	O
*	O
(	O
V	O
)	O
sequences	O
-	O
a	O
strong	O
,	O
languagespecific	O
bias	O
to	O
attend	O
to	O
changes	O
at	O
the	O
end	O
of	O
the	O
word	O
that	O
the	O
other	O
models	O
did	O
not	O
require	O
.	O
Results	O
for	O
larger	O
training	O
splits	O
,	O
and	O
for	O
mapping	O
from	O
URs	O
to	O
SRs	O
,	O
were	O
similar	O
.	O
The	O
errors	O
made	O
by	O
DNN	O
-	O
ISL	O
mostly	O
involved	O
underapplication	O
of	O
deletion	O
(	O
e.g.	O
,	O
*	O
[	O
blaNk	O
]	O
)	O
.	O

In	O
summary	O
,	O
we	O
evaluated	O
four	O
learning	O
models	O
on	O
an	O
ISL	O
phonological	O
mapping	O
(	O
with	O
a	O
small	O
number	O
of	O
exceptions	O
)	O
found	O
in	O
a	O
large	O
,	O
realistic	O
body	O
of	O
natural	O
language	O
data	O
.	O
The	O
models	O
that	O
have	O
proofs	O
of	O
learnability	O
and	O
efficiency	O
,	O
ISLFLA	O
and	O
OSTIA	O
,	O
performed	O
much	O
worse	O
than	O
models	O
that	O
currently	O
lack	O
such	O
theoretical	O
guarantees	O
but	O
share	O
the	O
inductive	O
bias	O
for	O
ISL	O
patterns	O
.	O
The	O
results	O
highlight	O
the	O
need	O
for	O
further	O
empirical	O
and	O
formal	O
study	O
of	O
highperforming	O
subsymbolic	O
models	O
such	O
as	O
DNN	O
-	O
ISL	O
,	O
and	O
extension	O
of	O
our	O
model	O
to	O
output	O
-	O
based	O
patterns	O
and	O
learning	O
of	O
underlying	O
representations	O
.	O
We	O
plan	O
to	O
release	O
our	O
processed	O
data	O
,	O
hand	O
-	O
written	O
ISL	O
transducer	O
,	O
and	O
model	O
implementations	O
.	O

Thanks	O
to	O
Coleman	O
Haley	O
and	O
Marina	O
Bedny	O
for	O
helpful	O
discussion	O
of	O
this	O
research	O
,	O
which	O
was	O
supported	O
by	O
NSF	O
grant	O
BCS	O
-	O
1941593	O
to	O
CW	O
.	O

In	O
this	O
section	O
,	O
we	O
explain	O
how	O
we	O
introduce	O
type	O
information	O
into	O
a	O
neural	O
CR	O
system	O
.	O

We	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
insightful	O
comments	O
.	O
We	O
are	O
also	O
grateful	O
to	O
the	O
members	O
of	O
the	O
TELEDIA	O
group	O
at	O
LTI	O
,	O
CMU	O
for	O
the	O
invaluable	O
feedback	O
.	O
This	O
work	O
was	O
funded	O
in	O
part	O
by	O
Dow	O
Chemical	O
,	O
and	O
Microsoft	O
.	O

To	O
have	O
an	O
intuitive	O
understanding	O
of	O
our	O
proposed	O
model	O
,	O
we	O
visualize	O
the	O
attention	O
weights	O
on	O
the	O
aspect	O
term	O
and	O
sentence	O
in	O
Figure	O
2	O
.	O
The	O
color	O
depth	O
indicates	O
the	O
importance	O
degree	O
of	O
the	O
weight	O
,	O
the	O
darker	O
the	O
more	O
important	O
.	O
In	O
Figure	O
2	O
,	O
the	O
sentence	O
is	O
"	O
This	O
is	O
one	O
great	O
place	O
to	O
eat	O
pizza	O
more	O
out	O
but	O
not	O
a	O
good	O
place	O
for	O
take	O
-	O
out	O
pizza	O
.	O
"	O
,	O
the	O
polarities	O
are	O
positive	O
and	O
negative	O
for	O
pizza	O
and	O
take	O
-	O
out	O
pizza	O
respectively	O
.	O
From	O
Figure	O
2	O
,	O
we	O
can	O
find	O
that	O
our	O
model	O
is	O
more	O
inclined	O
to	O
consider	O
the	O
neighboring	O
words	O
of	O
the	O
aspect	O
term	O
.	O
For	O
example	O
,	O
when	O
the	O
current	O
aspect	O
term	O
is	O
pizza	O
,	O
obviously	O
,	O
its	O
neighboring	O
words	O
such	O
as	O
"	O
great	O
"	O
,	O
"	O
place	O
"	O
and	O
"	O
more	O
"	O
get	O
more	O
attention	O
and	O
play	O
a	O
great	O
role	O
for	O
judging	O
sentiment	O
polarity	O
of	O
pizza	O
.	O
However	O
,	O
those	O
words	O
that	O
are	O
far	O
from	O
the	O
current	O
aspect	O
term	O
such	O
as	O
"	O
but	O
"	O
,	O
"	O
not	O
"	O
and	O
"	O
take	O
-	O
out	O
"	O
obtain	O
less	O
attention	O
,	O
which	O
demonstrates	O
the	O
effectiveness	O
of	O
the	O
position	O
information	O
.	O
For	O
aspect	O
term	O
take	O
-	O
out	O
pizza	O
,	O
it	O
is	O
obvious	O
that	O
the	O
word	O
"	O
take	O
-	O
out	O
"	O
is	O
more	O
important	O
to	O
express	O
the	O
aspect	O
term	O
than	O
the	O
word	O
"	O
pizza	O
"	O
.	O
From	O
Figure	O
2	O
,	O
it	O
is	O
worth	O
noting	O
that	O
some	O
words	O
such	O
as	O
"	O
good	O
"	O
and	O
"	O
place	O
"	O
get	O
less	O
attention	O
even	O
they	O
are	O
closer	O
to	O
the	O
current	O
aspect	O
term	O
than	O
"	O
but	O
"	O
and	O
"	O
not	O
"	O
.	O
This	O
is	O
because	O
different	O
words	O
in	O
aspect	O
term	O
have	O
different	O
effect	O
on	O
a	O
sentence	O
,	O
and	O
we	O
apply	O
the	O
bidirectional	O
attention	O
mechanism	O
to	O
choose	O
more	O
useful	O
words	O
.	O
For	O
instance	O
,	O
in	O
this	O
case	O
,	O
PBAN	O
should	O
pay	O
more	O
attention	O
on	O
the	O
word	O
"	O
take	O
-	O
out	O
"	O
.	O
Therefore	O
,	O
PBAN	O
is	O
capable	O
of	O
figuring	O
out	O
the	O
important	O
part	O
in	O
a	O
sentence	O
for	O
judging	O
the	O
sentiment	O
polarity	O
by	O
modeling	O
the	O
mutual	O
relation	O
between	O
sentence	O
and	O
different	O
words	O
in	O
aspect	O
term	O
.	O

Traditional	O
machine	O
learning	O
approaches	O
mainly	O
involve	O
text	O
representation	O
and	O
feature	O
extraction	O
,	O
such	O
as	O
bag	O
-	O
of	O
-	O
words	O
models	O
and	O
sentiment	O
lexicons	O
features	O
,	O
then	O
training	O
a	O
sentiment	O
classifier	O
(	O
Prez	O
-	O
Rosas	O
et	O
al	O
,	O
2012	O
)	O
.	O
Rao	O
et	O
al	O
(	O
2010	O
)	O
demonstrated	O
the	O
utility	O
of	O
graph	O
-	O
based	O
semi	O
-	O
supervised	O
learning	O
framework	O
for	O
building	O
sentiment	O
lexicons	O
.	O
Kaji	O
et	O
al	O
(	O
2007	O
)	O
explored	O
to	O
use	O
structural	O
clues	O
that	O
could	O
extract	O
polar	O
sentences	O
from	O
HTML	O
documents	O
,	O
and	O
built	O
lexicon	O
from	O
the	O
extracted	O
polar	O
sentences	O
.	O
However	O
,	O
these	O
methods	O
are	O
labor	O
-	O
intensive	O
,	O
and	O
usually	O
results	O
in	O
high	O
dimensional	O
and	O
high	O
sparse	O
phenomenon	O
for	O
the	O
text	O
representation	O
.	O

This	O
work	O
is	O
funded	O
in	O
part	O
by	O
the	O
national	O
key	O
research	O
and	O
development	O
program	O
of	O
China	O
(	O
2017YFE0111900	O
)	O
,	O
the	O
Key	O
Project	O
of	O
Tianjin	O
Natural	O
Science	O
Foundation	O
(	O
15JCZDJC31100	O
)	O
,	O
the	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
(	O
Key	O
Program	O
,	O
U1636203	O
)	O
,	O
the	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
(	O
U1736103	O
)	O
and	O
MSCA	O
-	O
ITN	O
-	O
ETN	O
-	O
European	O
Training	O
Networks	O
Project	O
(	O
QUARTZ	O
)	O
.	O

In	O
this	O
section	O
,	O
we	O
introduce	O
the	O
overall	O
framework	O
of	O
our	O
proposed	O
AMNRE	O
in	O
detail	O
.	O
As	O
shown	O
in	O
Figure	O
1	O
,	O
for	O
each	O
entity	O
pair	O
,	O
AMNRE	O
encodes	O
its	O
corresponding	O
sentences	O
in	O
different	O
languages	O
into	O
several	O
semantic	O
spaces	O
to	O
grasp	O
their	O
individual	O
language	O
patterns	O
.	O
Meanwhile	O
,	O
a	O
unified	O
space	O
is	O
also	O
set	O
up	O
to	O
encode	O
consistent	O
features	O
among	O
languages	O
.	O
By	O
explicitly	O
encoding	O
the	O
consistency	O
and	O
diversity	O
among	O
languages	O
,	O
AMNRE	O
can	O
achieve	O
better	O
extraction	O
results	O
in	O
the	O
multi	O
-	O
lingual	O
scenario	O
.	O
For	O
each	O
given	O
entity	O
pair	O
,	O
we	O
define	O
its	O
corresponding	O
sentences	O
in	O
n	O
different	O
languages	O
as	O
T	O
=	O
{	O
S	O
1	O
,	O
.	O
.	O
.	O
,	O
S	O
n	O
}	O
,	O
where	O
S	O
j	O
=	O
{	O
x	O
1	O
j	O
,	O
.	O
.	O
.	O
,	O
x	O
|	O
S	O
j	O
|	O
j	O
}	O
denotes	O
the	O
sentence	O
set	O
in	O
the	O
j	O
-	O
th	O
language	O
.	O
All	O
these	O
sentences	O
are	O
labeled	O
with	O
the	O
relation	O
r	O
R	O
by	O
heuristical	O
labeling	O
algorithms	O
in	O
distant	O
supervision	O
(	O
Mintz	O
et	O
al	O
,	O
2009	O
)	O
.	O
Our	O
model	O
aims	O
to	O
learn	O
a	O
relation	O
extractor	O
by	O
maximizing	O
the	O
conditional	O
probability	O
p	O
(	O
r	O
|	O
T	O
)	O
with	O
the	O
following	O
three	O
components	O
:	O
Sentence	O
Encoder	O
.	O
Given	O
a	O
sentence	O
and	O
its	O
target	O
entity	O
pair	O
,	O
we	O
employ	O
neural	O
networks	O
to	O
encode	O
the	O
sentence	O
into	O
a	O
embedding	O
.	O
In	O
this	O
paper	O
,	O
we	O
implement	O
the	O
sentence	O
encoder	O
with	O
both	O
convolutional	O
(	O
CNN	O
)	O
and	O
recurrent	O
(	O
RNN	O
)	O
architectures	O
.	O
Specifically	O
,	O
we	O
set	O
the	O
encoders	O
E	O
I	O
j	O
and	O
E	O
C	O
j	O
to	O
encode	O
each	O
sentence	O
in	O
the	O
j	O
-	O
th	O
language	O
into	O
its	O
individual	O
and	O
consistent	O
embeddings	O
respectively	O
,	O
and	O
expect	O
these	O
embeddings	O
to	O
capture	O
the	O
diversity	O
and	O
consistency	O
among	O
languages	O
.	O
Multi	O
-	O
lingual	O
Attention	O
.	O
Since	O
not	O
all	O
sentences	O
are	O
labeled	O
correctly	O
in	O
distant	O
supervision	O
,	O
we	O
adopt	O
multi	O
-	O
lingual	O
attention	O
mechanisms	O
to	O
capture	O
those	O
informative	O
sentences	O
.	O
In	O
practice	O
,	O
we	O
apply	O
language	O
-	O
individual	O
and	O
language	O
-	O
consistent	O
attentions	O
to	O
compute	O
local	O
and	O
global	O
textual	O
relation	O
representations	O
respectively	O
for	O
final	O
prediction	O
.	O
Adversarial	O
Training	O
.	O
Under	O
the	O
framework	O
of	O
AMNRE	O
,	O
we	O
encode	O
the	O
sentences	O
in	O
various	O
languages	O
into	O
a	O
unified	O
consistent	O
semantic	O
space	O
.	O
We	O
further	O
adopt	O
adversarial	O
training	O
to	O
ensure	O
these	O
sentences	O
are	O
well	O
fused	O
in	O
the	O
unified	O
space	O
after	O
encoding	O
so	O
that	O
our	O
model	O
can	O
effectively	O
extract	O
the	O
language	O
-	O
consistent	O
relation	O
patterns	O
.	O
We	O
will	O
introduce	O
the	O
three	O
components	O
in	O
detail	O
as	O
follows	O
.	O

Given	O
a	O
sentence	O
x	O
=	O
{	O
w	O
1	O
,	O
w	O
2	O
,	O
.	O
.	O
.	O
}	O
containing	O
two	O
entities	O
,	O
we	O
apply	O
neural	O
architectures	O
including	O
both	O
CNN	O
and	O
RNN	O
to	O
encode	O
the	O
sentence	O
into	O
a	O
continuous	O
low	O
-	O
dimensional	O
space	O
to	O
capture	O
its	O
implicit	O
semantics	O
.	O

For	O
each	O
given	O
entity	O
pair	O
,	O
AMNRE	O
adopts	O
multi	O
-	O
lingual	O
selective	O
attention	O
mechanisms	O
to	O
exploit	O
informative	O
sentences	O
in	O
T	O
.	O
We	O
explicitly	O
encode	O
languages	O
'	O
consistency	O
and	O
diversity	O
into	O
individual	O
and	O
consistent	O
representations	O
,	O
thus	O
our	O
attentions	O
are	O
more	O
simple	O
than	O
those	O
proposed	O
in	O
.	O

We	O
thank	O
Jiacheng	O
Zhang	O
for	O
his	O
help	O
.	O
This	O
work	O
is	O
supported	O
by	O
the	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
(	O
NSFC	O
No	O
.	O
61621136008	O
,	O
61772302	O
)	O
and	O
Tsinghua	O
University	O
Initiative	O
Scientific	O
Research	O
Program	O
(	O
20151080406	O
)	O
.	O
This	O
research	O
is	O
part	O
of	O
the	O
NExT++	O
project	O
,	O
supported	O
by	O
the	O
National	O
Research	O
Foundation	O
,	O
Prime	O
Minister	O
's	O
Office	O
,	O
Singapore	O
under	O
its	O
IRC@Singapore	O
Funding	O
Initiative	O
.	O

Interpreting	O
Verbal	O
Irony	O
:	O
Linguistic	O
Strategies	O
and	O
the	O
Connection	O
to	O
the	O
Type	O
of	O
Semantic	O
Incongruity	O

Human	O
communication	O
often	O
involves	O
the	O
use	O
of	O
verbal	O
irony	O
or	O
sarcasm	O
,	O
where	O
the	O
speakers	O
usually	O
mean	O
the	O
opposite	O
of	O
what	O
they	O
say	O
.	O
To	O
better	O
understand	O
how	O
verbal	O
irony	O
is	O
expressed	O
by	O
the	O
speaker	O
and	O
interpreted	O
by	O
the	O
hearer	O
we	O
conduct	O
a	O
crowdsourcing	O
task	O
:	O
given	O
an	O
utterance	O
expressing	O
verbal	O
irony	O
,	O
users	O
are	O
asked	O
to	O
verbalize	O
their	O
interpretation	O
of	O
the	O
speaker	O
's	O
ironic	O
message	O
.	O
We	O
propose	O
a	O
typology	O
of	O
linguistic	O
strategies	O
for	O
verbal	O
irony	O
interpretation	O
and	O
link	O
it	O
to	O
various	O
theoretical	O
linguistic	O
frameworks	O
.	O
We	O
design	O
computational	O
models	O
to	O
capture	O
these	O
strategies	O
and	O
present	O
empirical	O
studies	O
aimed	O
to	O
answer	O
three	O
questions	O
:	O
(	O
1	O
)	O
what	O
is	O
the	O
distribution	O
of	O
linguistic	O
strategies	O
used	O
by	O
hearers	O
to	O
interpret	O
ironic	O
messages	O
?	O
;	O
(	O
2	O
)	O
do	O
hearers	O
adopt	O
similar	O
strategies	O
for	O
interpreting	O
the	O
speaker	O
's	O
ironic	O
intent	O
?	O
;	O
and	O
(	O
3	O
)	O
does	O
the	O
type	O
of	O
semantic	O
incongruity	O
in	O
the	O
ironic	O
message	O
(	O
explicit	O
vs.	O
implicit	O
)	O
influence	O
the	O
choice	O
of	O
interpretation	O
strategies	O
by	O
the	O
hearers	O
?	O
⇤	O
Part	O
of	O
the	O
research	O
was	O
carried	O
out	O
while	O
Debanjan	O
was	O
a	O
Ph.D.	O
candidate	O
at	O
Rutgers	O
University	O
.	O

Messages	O
:	O
Explicit	O
vs.	O
Implicit	O
Attardo	O
(	O
2000	O
)	O
and	O
later	O
Burgers	O
(	O
2010	O
)	O
distinguish	O
between	O
two	O
theoretical	O
aspects	O
of	O
irony	O
:	O
irony	O
markers	O
and	O
irony	O
factors	O
.	O
Irony	O
markers	O
are	O
meta	O
-	O
communicative	O
signals	O
,	O
such	O
as	O
interjections	O
or	O
emoticons	O
that	O
alert	O
the	O
reader	O
that	O
an	O
utterance	O
might	O
be	O
ironic	O
.	O
In	O
contrast	O
,	O
irony	O
factors	O
can	O
not	O
be	O
removed	O
without	O
destroying	O
the	O
irony	O
,	O
such	O
as	O
the	O
incongruity	O
between	O
the	O
literal	O
evaluation	O
and	O
its	O
context	O
(	O
"	O
semantic	O
incongruity	O
"	O
)	O
.	O
Incongruity	O
expresses	O
the	O
contrast	O
between	O
the	O
conveyed	O
sentiment	O
(	O
usually	O
,	O
positive	O
)	O
and	O
the	O
targeted	O
situation	O
(	O
usually	O
,	O
negative	O
)	O
.	O
This	O
contrast	O
can	O
be	O
explicitly	O
or	O
implicitly	O
expressed	O
in	O
the	O
ironic	O
message	O
.	O
Following	O
Karoui	O
et	O
al	O
(	O
2017	O
)	O
,	O
we	O
consider	O
that	O
semantic	O
incongruity	O
is	O
explicit	O
,	O
when	O
it	O
is	O
lexicalized	O
in	O
the	O
utterance	O
itself	O
(	O
e.g.	O
,	O
both	O
the	O
positive	O
sentiment	O
word	O
(	O
s	O
)	O
and	O
the	O
negative	O
situation	O
are	O
available	O
to	O
the	O
reader	O
explicitly	O
)	O
.	O
On	O
Twitter	O
,	O
beside	O
sentiment	O
words	O
,	O
users	O
often	O
make	O
use	O
of	O
hashtags	O
(	O
e.g.	O
,	O
"	O
Studying	O
5	O
subjects	O
.	O
.	O
.	O
#	O
worstsaturdaynight	O
"	O
)	O
or	O
an	O
image	O
(	O
e.g.	O
,	O
"	O
Encouraging	O
how	O
Police	O
feel	O
they	O
're	O
above	O
the	O
law	O
.	O
URL	O
"	O
;	O
the	O
URL	O
shows	O
a	O
police	O
car	O
not	O
paying	O
parking	O
)	O
to	O
express	O
their	O
sentiment	O
.	O
We	O
consider	O
these	O
cases	O
as	O
explicit	O
,	O
since	O
the	O
incongruity	O
is	O
present	O
in	O
the	O
utterance	O
even	O
if	O
via	O
hashtags	O
or	O
other	O
media	O
.	O
For	O
implicit	O
incongruity	O
,	O
we	O
consider	O
cases	O
where	O
one	O
of	O
the	O
two	O
incongruent	O
terms	O
(	O
"	O
propositions	O
"	O
in	O
Karoui	O
et	O
al	O
(	O
2017	O
)	O
)	O
is	O
not	O
lexicalized	O
and	O
has	O
to	O
be	O
reconstructed	O
from	O
the	O
con	O
-	O
text	O
(	O
either	O
outside	O
word	O
knowledge	O
or	O
a	O
larger	O
conversational	O
context	O
)	O
.	O
For	O
example	O
"	O
You	O
are	O
such	O
a	O
nice	O
friend	O
!	O
!	O
!	O
"	O
,	O
or	O
"	O
Driving	O
in	O
Detroit	O
is	O
fun	O
;	O
)	O
"	O
are	O
cases	O
of	O
ironic	O
messages	O
where	O
the	O
semantic	O
incongruity	O
is	O
implicit	O
.	O
Based	O
on	O
these	O
definitions	O
of	O
explicit	O
and	O
implicit	O
incongruity	O
,	O
two	O
expert	O
annotators	O
annotated	O
the	O
S	O
i	O
m	O
-	O
H	O
int	O
dataset	O
(	O
1000	O
ironic	O
messages	O
)	O
as	O
containing	O
explicit	O
or	O
implicit	O
semantic	O
incongruity	O
.	O
The	O
inter	O
-	O
annotator	O
agreement	O
was	O
=0.7	O
,	O
which	O
denotes	O
good	O
agreement	O
similar	O
to	O
Karoui	O
et	O
al	O
(	O
2017	O
)	O
.	O
The	O
annotation	O
showed	O
that	O
38.7	O
%	O
of	O
the	O
ironic	O
messages	O
are	O
explicit	O
,	O
while	O
61.3	O
%	O
are	O
implicit	O
.	O
In	O
the	O
following	O
section	O
we	O
propose	O
a	O
typology	O
of	O
linguistic	O
strategies	O
used	O
in	O
hearers	O
'	O
interpretations	O
of	O
speakers	O
'	O
ironic	O
messages	O
and	O
in	O
section	O
6.2	O
we	O
discuss	O
the	O
correlation	O
of	O
linguistic	O
strategies	O
with	O
the	O
type	O
of	O
semantic	O
incongruity	O
.	O

Given	O
the	O
definition	O
of	O
verbal	O
irony	O
,	O
we	O
would	O
expect	O
that	O
Turkers	O
'	O
interpretation	O
of	O
speaker	O
's	O
ironic	O
message	O
will	O
contain	O
some	O
degree	O
of	O
opposite	O
meaning	O
with	O
respect	O
to	O
what	O
the	O
speaker	O
has	O
said	O
.	O
However	O
,	O
it	O
is	O
unclear	O
what	O
linguistic	O
strategies	O
the	O
Turkers	O
will	O
use	O
to	O
express	O
that	O
.	O
To	O
build	O
our	O
typology	O
,	O
from	O
the	O
total	O
set	O
of	O
S	O
i	O
m	O
-	O
H	O
int	O
pairs	O
obtained	O
through	O
crowdsourcing	O
(	O
i.e.	O
,	O
4	O
,	O
762	O
pairs	O
;	O
see	O
Section	O
2	O
)	O
we	O
selected	O
a	O
dev	O
set	O
of	O
500	O
S	O
i	O
m	O
-	O
H	O
int	O
pairs	O
.	O
Our	O
approach	O
does	O
not	O
assume	O
any	O
specific	O
theory	O
or	O
irony	O
,	O
but	O
it	O
is	O
data	O
-	O
driven	O
:	O
a	O
linguist	O
expert	O
in	O
semantics	O
and	O
pragmatics	O
analyzed	O
the	O
dev	O
set	O
to	O
formulate	O
the	O
lexical	O
and	O
pragmatic	O
phenomena	O
attested	O
in	O
the	O
data	O
.	O
The	O
assembled	O
typology	O
is	O
,	O
thus	O
,	O
the	O
result	O
of	O
a	O
bottom	O
-	O
up	O
procedure	O
.	O
A	O
S	O
i	O
m	O
-	O
H	O
int	O
pair	O
can	O
be	O
annotated	O
with	O
more	O
than	O
one	O
strategy	O
.	O
The	O
core	O
linguistic	O
strategies	O
are	O
explained	O
below	O
and	O
synthesized	O
in	O
Table	O
2	O
.	O

Lexical	O
and	O
phrasal	O
antonyms	O
:	O
This	O
category	O
contains	O
lexical	O
antonyms	O
(	O
e.g.	O
,	O
"	O
love	O
"	O
$	O
"	O
hate	O
"	O
,	O
"	O
great	O
"	O
$	O
"	O
terrible	O
"	O
)	O
as	O
well	O
as	O
indirect	O
antonyms	O
(	O
Fellbaum	O
,	O
1998	O
)	O
,	O
where	O
the	O
opposite	O
meaning	O
can	O
only	O
be	O
interpreted	O
in	O
context	O
(	O
e.g.	O
,	O
"	O
passionate	O
speaker	O
"	O
!	O
"	O
boring	O
speaker	O
"	O
;	O
Table	O
1	O
)	O
.	O
Although	O
the	O
typical	O
antonym	O
of	O
"	O
passionate	O
"	O
is	O
"	O
unpassionate	O
"	O
,	O
"	O
boring	O
"	O
works	O
in	O
this	O
context	O
as	O
a	O
lexical	O
opposite	O
since	O
a	O
speaker	O
who	O
is	O
passionate	O
entails	O
that	O
he	O
is	O
not	O
boring	O
.	O
Besides	O
lexical	O
antonyms	O
,	O
Turkers	O
sometimes	O
use	O
antonym	O
phrases	O
(	O
e.g.	O
,	O
"	O
I	O
ca	O
n't	O
wait	O
"	O
!	O
"	O
not	O
looking	O
forward	O
"	O
,	O
"	O
I	O
like	O
(	O
to	O
visit	O
ER	O
)	O
"	O
!	O
"	O
I	O
am	O
upset	O
(	O
to	O
visit	O
ER	O
)	O
"	O
)	O
.	O
Negation	O
:	O
Here	O
,	O
Turkers	O
negate	O
the	O
main	O
predicate	O
.	O
This	O
strategy	O
is	O
used	O
in	O
the	O
presence	O
of	O
copulative	O
constructions	O
where	O
the	O
predicative	O
expression	O
is	O
an	O
adjective	O
/	O
noun	O
expressing	O
sentiment	O
(	O
e.g.	O
,	O
"	O
is	O
great	O
"	O
!	O
"	O
is	O
not	O
great	O
"	O
)	O
and	O
of	O
verbs	O
expressing	O
sentiment	O
(	O
e.g.	O
,	O
"	O
love	O
"	O
!	O
"	O
do	O
not	O
love	O
"	O
)	O
or	O
propositional	O
attitudes	O
(	O
e.g.	O
,	O
"	O
I	O
wonder	O
"	O
!	O
"	O
I	O
do	O
n't	O
wonder	O
"	O
)	O
.	O
Weakening	O
the	O
intensity	O
of	O
sentiment	O
:	O
The	O
use	O
of	O
negation	O
and	O
antonyms	O
is	O
sometimes	O
accompanied	O
by	O
two	O
strategies	O
that	O
reflect	O
a	O
weakening	O
of	O
sentiment	O
intensity	O
.	O
First	O
,	O
when	O
S	O
i	O
m	O
contains	O
words	O
expressing	O
a	O
high	O
degree	O
of	O
positive	O
sentiment	O
,	O
the	O
hearer	O
's	O
interpretation	O
replaces	O
them	O
with	O
more	O
neutral	O
ones	O
(	O
e.g.	O
,	O
"	O
I	O
love	O
it	O
"	O
!	O
"	O
I	O
do	O
n't	O
like	O
it	O
"	O
)	O
.	O
Second	O
,	O
when	O
S	O
i	O
m	O
contains	O
an	O
intensifier	O
,	O
it	O
is	O
eliminated	O
in	O
the	O
Turkers	O
'	O
interpretation	O
.	O
Intensifiers	O
specify	O
the	O
degree	O
of	O
value	O
/	O
quality	O
expressed	O
by	O
the	O
words	O
they	O
modify	O
(	O
Méndez	O
-	O
Naya	O
,	O
2008	O
)	O
(	O
e.g.	O
,	O
"	O
cake	O
for	O
breakfast	O
.	O
so	O
healthy	O
"	O
!	O
"	O
cake	O
for	O
breakfast	O
.	O
not	O
healthy	O
"	O
)	O
.	O
Interrogative	O
to	O
Declarative	O
Transformation	O
(	O
+	O
Antonym	O
/	O
Negation	O
)	O
:	O
This	O
strategy	O
,	O
used	O
mostly	O
in	O
conjunction	O
with	O
the	O
negation	O
or	O
antonym	O
strategies	O
,	O
consists	O
in	O
replacing	O
the	O
interrogative	O
form	O
with	O
a	O
declarative	O
form	O
,	O
when	O
S	O
i	O
m	O
is	O
a	O
rhetorical	O
question	O
(	O
for	O
brevity	O
,	O
RQ	O
)	O
(	O
e.g.	O
,	O
"	O
do	O
n't	O
you	O
love	O
fighting	O
?	O
"	O
!	O
"	O
I	O
hate	O
fighting	O
"	O
)	O
.	O

When	O
the	O
ironic	O
utterance	O
expresses	O
a	O
positive	O
/	O
negative	O
sentiment	O
towards	O
a	O
past	O
event	O
(	O
e.g.	O
,	O
"	O
glad	O
you	O
relayed	O
this	O
news	O
"	O
)	O
or	O
an	O
expressive	O
speech	O
act	O
(	O
e.g.	O
,	O
"	O
thanks	O
X	O
that	O
picture	O
needed	O
more	O
copy	O
"	O
)	O
the	O
hearer	O
's	O
interpretation	O
of	O
intended	O
meaning	O
is	O
expressed	O
through	O
the	O
counterfactual	O
desiderative	O
constructions	O
I	O
wish	O
(	O
that	O
)	O
p	O
(	O
"	O
I	O
wish	O
you	O
had	O
n't	O
relayed	O
.	O
.	O
.	O
"	O
,	O
"	O
I	O
wish	O
X	O
did	O
n't	O
copy	O
.	O
.	O
.	O
"	O
)	O
.	O
Differently	O
from	O
antonymic	O
phrases	O
,	O
this	O
strategy	O
stresses	O
on	O
the	O
failure	O
of	O
the	O
speaker	O
's	O
expectation	O
more	O
than	O
on	O
their	O
commitment	O
to	O
the	O
opposite	O
meaning	O
.	O
Pragmatic	O
Inference	O
:	O
In	O
addition	O
to	O
the	O
above	O
strategies	O
,	O
there	O
are	O
cases	O
where	O
the	O
interpretation	O
calls	O
for	O
an	O
inferential	O
process	O
to	O
be	O
recognized	O
.	O
For	O
instance	O
,	O
"	O
made	O
174	O
this	O
month	O
.	O
.	O
.	O
I	O
'm	O
gon	O
na	O
buy	O
a	O
yacht	O
!	O
"	O
!	O
"	O
made	O
174	O
this	O
month	O
.	O
.	O
.	O
I	O
am	O
so	O
poor	O
"	O
.	O
The	O
distribution	O
of	O
the	O
strategies	O
on	O
the	O
dev	O
set	O
is	O
represented	O
in	O
Table	O
2	O
.	O

In	O
linguistic	O
literature	O
many	O
different	O
approaches	O
to	O
irony	O
have	O
been	O
provided	O
.	O
Here	O
we	O
focus	O
on	O
the	O
three	O
accounts	O
(	O
w.r.t	O
.	O
examples	O
from	O
S	O
i	O
m	O
-	O
H	O
int	O
corpus	O
)	O
that	O
bear	O
a	O
different	O
views	O
on	O
pragmatic	O
factors	O
.	O
According	O
to	O
Grice	O
(	O
1975	O
)	O
,	O
ironic	O
messages	O
are	O
uttered	O
to	O
convey	O
a	O
meaning	O
opposite	O
to	O
that	O
literally	O
expressed	O
,	O
flouting	O
the	O
conversational	O
maxim	O
of	O
quality	O
"	O
do	O
not	O
say	O
what	O
you	O
believe	O
to	O
be	O
false	O
"	O
.	O
In	O
verbal	O
irony	O
,	O
the	O
violation	O
of	O
the	O
maxim	O
is	O
frequently	O
signaled	O
by	O
"	O
the	O
opposite	O
"	O
of	O
what	O
is	O
said	O
literally	O
(	O
e.g.	O
,	O
intended	O
meaning	O
of	O
"	O
carcasses	O
are	O
flattering	O
"	O
is	O
they	O
are	O
gross	O
;	O
Table	O
1	O
)	O
.	O
The	O
linguistic	O
strategies	O
of	O
antonyms	O
(	O
e.g.	O
"	O
worst	O
day	O
of	O
my	O
life	O
"	O
)	O
and	O
simple	O
negation	O
(	O
"	O
yeap	O
we	O
totally	O
do	O
nt	O
drink	O
alcohol	O
every	O
single	O
day	O
"	O
[	O
...	O
]	O
)	O
cover	O
the	O
majority	O
of	O
the	O
S	O
i	O
m	O
-	O
H	O
int	O
corpus	O
and	O
seem	O
to	O
fit	O
the	O
Gricean	O
(	O
Grice	O
,	O
1975	O
)	O
account	O
of	O
irony	O
,	O
since	O
the	O
hearer	O
seems	O
to	O
have	O
primarily	O
recognized	O
the	O
presence	O
of	O
semantic	O
incongruity	O
.	O
However	O
,	O
as	O
touched	O
upon	O
by	O
Giora	O
(	O
1995	O
)	O
,	O
antonyms	O
and	O
direct	O
negation	O
are	O
not	O
always	O
semantically	O
equivalent	O
strategies	O
,	O
since	O
the	O
second	O
sometimes	O
allows	O
a	O
graded	O
interpretation	O
:	O
if	O
"	O
x	O
is	O
not	O
encouraging	O
"	O
,	O
it	O
is	O
not	O
nec	O
-	O
essarily	O
bad	O
,	O
but	O
simply	O
"	O
x	O
<	O
encouraging	O
"	O
.	O
Such	O
an	O
implicature	O
is	O
available	O
exclusively	O
with	O
items	O
allowing	O
mediated	O
contraries	O
,	O
such	O
as	O
sentiment	O
words	O
(	O
Horn	O
,	O
1989	O
)	O
.	O
Direct	O
negation	O
with	O
sentiment	O
words	O
implies	O
that	O
just	O
one	O
value	O
in	O
a	O
set	O
is	O
negated	O
,	O
while	O
the	O
others	O
are	O
potentially	O
affirmed	O
.	O
The	O
spectrum	O
of	O
interpretations	O
allowed	O
by	O
negation	O
as	O
a	O
rephrasing	O
strategy	O
indicates	O
that	O
hearers	O
recognize	O
that	O
the	O
relevance	O
of	O
the	O
ironic	O
utterance	O
in	O
itself	O
plays	O
a	O
role	O
next	O
to	O
what	O
the	O
utterances	O
refers	O
to	O
(	O
if	O
the	O
rephrased	O
utterance	O
is	O
intended	O
as	O
"	O
x	O
is	O
not	O
encouraging	O
at	O
all	O
"	O
,	O
the	O
perceived	O
irrelevance	O
of	O
the	O
corresponding	O
ironic	O
utterance	O
is	O
more	O
prominent	O
than	O
in	O
"	O
x	O
is	O
not	O
very	O
encouraging	O
"	O
)	O
.	O
The	O
fact	O
that	O
the	O
interpretation	O
of	O
irony	O
has	O
a	O
propositional	O
scope	O
is	O
even	O
clearer	O
when	O
the	O
ironic	O
sentence	O
in	O
interrogative	O
form	O
(	O
"	O
and	O
they	O
all	O
lived	O
happily	O
ever	O
after	O
?	O
"	O
)	O
is	O
rephrased	O
as	O
a	O
declarative	O
(	O
e.g.	O
"	O
I	O
doubt	O
they	O
all	O
lived	O
happily	O
ever	O
after	O
"	O
)	O
:	O
the	O
hearers	O
recognizes	O
that	O
the	O
question	O
has	O
a	O
rhetoric	O
value	O
since	O
otherwise	O
contextually	O
irrelevant	O
.	O
The	O
intentional	O
falsehood	O
of	O
Gricean	O
analysis	O
is	O
also	O
not	O
deemed	O
by	O
Sperber	O
and	O
Wilson	O
(	O
1986	O
)	O
;	O
Wilson	O
and	O
Sperber	O
(	O
2012	O
)	O
as	O
a	O
necessary	O
and	O
sufficient	O
condition	O
for	O
irony	O
.	O
According	O
to	O
their	O
theory	O
of	O
echoic	O
mentioning	O
,	O
irony	O
presupposes	O
the	O
mention	O
to	O
the	O
inappropriateness	O
of	O
the	O
entire	O
sentence	O
:	O
in	O
asserting	O
"	O
awesome	O
weather	O
in	O
Scotland	O
today	O
"	O
the	O
speaker	O
does	O
not	O
simply	O
want	O
to	O
express	O
that	O
the	O
weather	O
was	O
horrible	O
but	O
he	O
signals	O
that	O
assuming	O
that	O
the	O
weather	O
would	O
be	O
nice	O
was	O
irrelevant	O
and	O
,	O
thus	O
,	O
ridiculous	O
.	O
Kreuz	O
and	O
Glucksberg	O
(	O
1989	O
)	O
expand	O
the	O
Relevance	O
Theory	O
approach	O
talking	O
about	O
echoic	O
reminding	O
to	O
account	O
for	O
cases	O
such	O
as	O
"	O
could	O
you	O
be	O
just	O
a	O
little	O
louder	O
,	O
please	O
?	O
My	O
baby	O
is	O
n't	O
trying	O
to	O
sleep	O
"	O
where	O
the	O
extreme	O
politeness	O
reminds	O
the	O
hearer	O
that	O
the	O
question	O
is	O
indeed	O
a	O
request	O
and	O
that	O
the	O
mother	O
bears	O
a	O
certain	O
stance	O
and	O
has	O
certain	O
expectations	O
towards	O
the	O
addressee	O
.	O
Similarly	O
,	O
the	O
use	O
of	O
the	O
pragmatic	O
inference	O
strategy	O
can	O
not	O
be	O
fully	O
explained	O
in	O
Gricean	O
terms	O
:	O
the	O
rephrase	O
"	O
made	O
174	O
this	O
month	O
.	O
.	O
.	O
I	O
am	O
so	O
poor	O
"	O
for	O
"	O
made	O
174	O
this	O
month	O
.	O
.	O
.	O
I	O
am	O
gon	O
na	O
buy	O
a	O
yatch	O
"	O
more	O
than	O
pointing	O
to	O
the	O
presence	O
of	O
lexical	O
incongruity	O
,	O
show	O
that	O
the	O
hearers	O
knows	O
for	O
background	O
knowledge	O
that	O
the	O
assertion	O
of	O
"	O
buying	O
a	O
yatch	O
"	O
is	O
completely	O
irrelevant	O
in	O
the	O
context	O
of	O
a	O
low	O
salary	O
situation	O
.	O
Rephrasing	O
strategies	O
using	O
counterfactual	O
desiderative	O
constructions	O
(	O
e.g.	O
"	O
I	O
really	O
wish	O
my	O
friends	O
and	O
fam	O
-	O
ily	O
would	O
check	O
up	O
on	O
my	O
after	O
yesterday	O
's	O
near	O
death	O
experience	O
"	O
)	O
show	O
,	O
instead	O
,	O
that	O
the	O
interpretation	O
of	O
irony	O
involves	O
an	O
echoic	O
reminding	O
to	O
the	O
speaker	O
's	O
(	O
social	O
)	O
expectations	O
which	O
failed	O
to	O
be	O
fulfilled	O
.	O
Overall	O
,	O
using	O
the	O
results	O
of	O
our	O
crowdsourcing	O
experiment	O
with	O
main	O
existing	O
theories	O
of	O
irony	O
,	O
it	O
turns	O
out	O
that	O
the	O
theories	O
have	O
a	O
complementary	O
explanatory	O
power	O
.	O
In	O
Section	O
6.2	O
we	O
investigate	O
weather	O
this	O
situation	O
might	O
relate	O
to	O
the	O
presence	O
of	O
explicit	O
/	O
implicit	O
irony	O
.	O

Here	O
our	O
goal	O
is	O
to	O
perform	O
a	O
comparative	O
empirical	O
analysis	O
to	O
understand	O
how	O
hearers	O
interpret	O
verbal	O
irony	O
.	O
To	O
accomplish	O
this	O
,	O
we	O
propose	O
computational	O
models	O
to	O
automatically	O
detect	O
these	O
linguistic	O
strategies	O
in	O
two	O
datasets	O
:	O
(	O
1	O
)	O
S	O
i	O
m	O
-	O
H	O
int	O
dataset	O
and	O
(	O
2	O
)	O
the	O
SIGN	O
dataset	O
.	O
As	O
stated	O
in	O
Section	O
2	O
,	O
albeit	O
for	O
a	O
different	O
purpose	O
,	O
the	O
task	O
designed	O
in	O
Peled	O
and	O
Reichart	O
(	O
2017	O
)	O
is	O
identical	O
to	O
ours	O
:	O
they	O
used	O
a	O
set	O
of	O
3	O
,	O
000	O
sarcastic	O
tweets	O
and	O
collected	O
five	O
interpretation	O
verbalization	O
,	O
including	O
an	O
option	O
to	O
just	O
copy	O
the	O
original	O
message	O
if	O
it	O
was	O
not	O
deemed	O
ironic	O
.	O
They	O
used	O
workers	O
skilled	O
in	O
comedy	O
writing	O
and	O
literature	O
paraphrasing	O
.	O
SIGN	O
contains	O
14	O
,	O
970	O
pairs	O
.	O
To	O
evaluate	O
our	O
models	O
,	O
we	O
asked	O
two	O
annotators	O
to	O
annotate	O
two	O
test	O
sets	O
of	O
500	O
pairs	O
each	O
from	O
the	O
S	O
i	O
m	O
-	O
H	O
int	O
and	O
the	O
SIGN	O
dataset	O
(	O
i.e.	O
,	O
denoted	O
by	O
SIGN	O
test	O
)	O
,	O
respectively	O
.	O
Note	O
,	O
the	O
test	O
set	O
for	O
the	O
S	O
i	O
m	O
-	O
H	O
int	O
has	O
no	O
overlap	O
with	O
the	O
dev	O
set	O
of	O
500	O
S	O
i	O
m	O
-	O
H	O
int	O
pairs	O
used	O
to	O
identify	O
the	O
strategies	O
(	O
Section	O
4	O
)	O
.	O
Agreement	O
between	O
the	O
annotators	O
for	O
both	O
sets	O
is	O
high	O
with	O
	O
>	O
0.9	O
.	O
In	O
SIGN	O
test	O
,	O
79	O
instances	O
were	O
just	O
copies	O
of	O
the	O
original	O
message	O
,	O
which	O
we	O
eliminated	O
,	O
thus	O
the	O
SIGN	O
test	O
contains	O
only	O
421	O
instances	O
.	O

Identifying	O
phrasal	O
antonyms	O
and	O
pragmatic	O
inference	O
is	O
a	O
complex	O
task	O
,	O
and	O
thus	O
we	O
propose	O
a	O
method	O
of	O
phrase	O
matching	O
based	O
on	O
phrase	O
extraction	O
via	O
unsupervised	O
alignment	O
technique	O
in	O
SMT	O
.	O
We	O
use	O
IBM	O
Model	O
4	O
with	O
HMM	O
(	O
Giza++	O
;	O
(	O
Och	O
and	O
Ney	O
,	O
2000	O
)	O
)	O
,	O
phrase	O
extraction	O
via	O
Moses	O
(	O
Koehn	O
et	O
al	O
,	O
2007	O
)	O
and	O
the	O
IRST	O
tool	O
to	O
build	O
the	O
required	O
language	O
models	O
.	O
As	O
postprocessing	O
,	O
we	O
first	O
remove	O
phrase	O
pairs	O
obtained	O
from	O
the	O
S	O
i	O
m	O
-	O
H	O
int	O
bitext	O
that	O
are	O
also	O
present	O
in	O
the	O
set	O
of	O
extracted	O
phrases	O
from	O
the	O
H	O
int	O
-	O
H	O
int	O
bitext	O
.	O
This	O
increases	O
the	O
likelihood	O
of	O
retaining	O
semantically	O
opposite	O
phrases	O
,	O
since	O
phrases	O
extracted	O
from	O
the	O
H	O
int	O
-	O
H	O
int	O
bitext	O
are	O
more	O
likely	O
to	O
be	O
paraphrastic	O
.	O
Second	O
,	O
based	O
on	O
the	O
translation	O
probability	O
scores	O
,	O
for	O
phrase	O
e	O
if	O
we	O
have	O
a	O
set	O
of	O
aligned	O
phrases	O
f	O
set	O
we	O
reject	O
phrases	O
that	O
have	O
scores	O
less	O
than	O
1	O
size	O
(	O
fset	O
)	O
.	O
Finally	O
,	O
11	O
,	O
200	O
phrases	O
are	O
extracted	O
from	O
the	O
S	O
i	O
m	O
-	O
H	O
int	O
bitext	O
.	O
The	O
low	O
recall	O
for	O
this	O
strategy	O
is	O
expected	O
since	O
there	O
are	O
too	O
many	O
ways	O
that	O
users	O
can	O
employ	O
pragmatic	O
inference	O
or	O
rephrase	O
the	O
utterance	O
without	O
directly	O
using	O
any	O
antonym	O
or	O
negation	O
.	O
In	O
future	O
,	O
we	O
will	O
explore	O
neural	O
MT	O
(	O
Cho	O
et	O
al	O
,	O
2014	O
)	O
and	O
use	O
external	O
data	O
to	O
generate	O
more	O
phrases	O
.	O
Since	O
we	O
have	O
not	O
manually	O
evaluated	O
these	O
phrase	O
pairs	O
,	O
we	O
only	O
use	O
this	O
strategy	O
after	O
we	O
have	O
tried	O
all	O
the	O
remaining	O
strategies	O
(	O
AntPhrase+PragInf	O
in	O
Table	O
3	O
and	O
Table	O
4	O
)	O
.	O

We	O
investigate	O
how	O
hearers	O
adopt	O
strategies	O
for	O
interpreting	O
the	O
speaker	O
's	O
ironic	O
intent	O
.	O
To	O
implement	O
this	O
study	O
,	O
we	O
selected	O
three	O
Turkers	O
(	O
e.g.	O
,	O
H	O
1	O
,	O
H	O
2	O
,	O
and	O
H	O
3	O
;	O
In	O
Table	O
1	O
,	O
H	O
i	O
int	O
are	O
generated	O
by	O
the	O
correspondent	O
Turker	O
H	O
i	O
)	O
,	O
from	O
our	O
crowdsourced	O
data	O
,	O
who	O
were	O
able	O
to	O
rephrase	O
at	O
least	O
five	O
hundred	O
identical	O
S	O
i	O
m	O
messages	O
.	O
Note	O
,	O
we	O
can	O
not	O
carry	O
this	O
experiment	O
on	O
the	O
SIGN	O
dataset	O
(	O
Peled	O
and	O
Reichart	O
,	O
2017	O
)	O
because	O
the	O
annotators	O
'	O
information	O
is	O
absent	O
there	O
.	O
Although	O
the	O
three	O
Turkers	O
choose	O
lexical	O
antonym	O
and	O
simple	O
negation	O
as	O
two	O
top	O
choices	O
,	O
there	O
is	O
some	O
variation	O
among	O
them	O
.	O
H	O
1	O
and	O
H	O
2	O
choose	O
antonyms	O
more	O
frequently	O
than	O
negation	O
while	O
in	O
contrary	O
Turker	O
H	O
3	O
choose	O
negation	O
more	O
than	O
antonyms	O
,	O
sometime	O
combined	O
with	O
the	O
weakening	O
of	O
sentiment	O
strategy	O
.	O
As	O
we	O
mentioned	O
in	O
Section	O
4.2	O
,	O
antonyms	O
and	O
direct	O
negation	O
are	O
not	O
semantically	O
equivalent	O
strategies	O
since	O
the	O
latter	O
,	O
allows	O
a	O
graded	O
interpretation	O
:	O
if	O
"	O
x	O
is	O
not	O
inspiring	O
"	O
,	O
it	O
is	O
not	O
necessarily	O
bad	O
,	O
but	O
simply	O
"	O
x	O
<	O
inspiring	O
"	O
(	O
Giora	O
,	O
1995	O
)	O
.	O
In	O
Table	O
1	O
,	O
the	O
S	O
i	O
m	O
-	O
H	O
int	O
pair	O
"	O
passionate	O
"	O
!	O
"	O
boring	O
"	O
and	O
"	O
flattering	O
"	O
!	O
"	O
gross	O
"	O
(	O
interpretation	O
of	O
H	O
1	O
)	O
have	O
more	O
contrast	O
than	O
the	O
pair	O
"	O
passionate	O
"	O
!	O
"	O
not	O
passionate	O
"	O
and	O
"	O
so	O
flattering	O
"	O
!	O
"	O
not	O
flattering	O
"	O
(	O
interpretation	O
of	O
H	O
3	O
)	O
.	O
This	O
suggests	O
that	O
H	O
1	O
perceive	O
the	O
intensity	O
of	O
negative	O
sentiment	O
towards	O
the	O
target	O
of	O
irony	O
(	O
"	O
Ed	O
Davey	O
"	O
and	O
"	O
picture	O
of	O
dead	O
animals	O
"	O
,	O
respectively	O
)	O
higher	O
than	O
Turker	O
H	O
3	O
.	O
All	O
three	O
Turkers	O
have	O
chosen	O
the	O
remaining	O
strategies	O
with	O
similar	O
frequencies	O
.	O

Interpretation	O
Strategies	O
and	O
the	O
Type	O
of	O
Semantic	O
Incongruity	O
:	O
We	O
investigate	O
whether	O
the	O
type	O
of	O
semantic	O
incongruity	O
in	O
the	O
ironic	O
message	O
(	O
explicit	O
vs.	O
implicit	O
;	O
see	O
Section	O
3	O
)	O
influences	O
the	O
choice	O
of	O
interpretation	O
strategies	O
by	O
the	O
hearers	O
.	O
To	O
do	O
this	O
,	O
we	O
looked	O
at	O
S	O
i	O
m	O
-	O
level	O
distribution	O
of	O
interpretation	O
strategies	O
used	O
by	O
the	O
hearers	O
for	O
the	O
same	O
ironic	O
message	O
S	O
i	O
m	O
.	O
as	O
interpretation	O
strategy	O
more	O
when	O
the	O
semantic	O
incongruity	O
is	O
explicit	O
than	O
implicit	O
(	O
48.5	O
%	O
vs.	O
34.8	O
%	O
)	O
:	O
the	O
presence	O
of	O
explicit	O
sentiment	O
triggered	O
the	O
use	O
of	O
the	O
antonym	O
strategy	O
.	O
In	O
contrary	O
they	O
use	O
simple	O
negation	O
more	O
when	O
the	O
semantic	O
incongruity	O
is	O
implicit	O
than	O
explicit	O
.	O
We	O
also	O
analyze	O
the	O
interpretation	O
strategies	O
w.r.t	O
.	O
to	O
the	O
presence	O
(	O
+	O
)	O
or	O
absence	O
(	O
)	O
of	O
irony	O
markers	O
.	O
We	O
implement	O
various	O
morpho	O
-	O
syntactic	O
as	O
well	O
as	O
typographic	O
markers	O
(	O
similar	O
to	O
)	O
to	O
identify	O
the	O
presence	O
of	O
markers	O
.	O
We	O
observe	O
that	O
Lex	O
ant	O
strategy	O
is	O
used	O
more	O
in	O
cases	O
where	O
the	O
markers	O
are	O
absent	O
.	O
In	O
S	O
i	O
m	O
-	O
H	O
int	O
,	O
markers	O
are	O
present	O
twice	O
as	O
much	O
in	O
the	O
case	O
of	O
implicit	O
(	O
21	O
%	O
)	O
than	O
explicit	O
incongruity	O
(	O
10	O
%	O
)	O
.	O
This	O
finding	O
validates	O
(	O
Burgers	O
et	O
al	O
,	O
2012	O
)	O
who	O
argued	O
speakers	O
will	O
likely	O
use	O
markers	O
to	O
signal	O
their	O
ironic	O
intent	O
in	O
implicit	O
incongruity	O
.	O

In	O
Figure	O
1	O
,	O
the	O
vertical	O
columns	O
(	O
purple	O
:	O
S	O
i	O
m	O
-	O
H	O
int	O
and	O
grey	O
:	O
SIGN	O
)	O
depict	O
the	O
distribution	O
(	O
in	O
%	O
)	O
of	O
tweets	O
strategy	O
-	O
wise	O
.	O
In	O
S	O
i	O
m	O
-	O
H	O
int	O
dataset	O
,	O
for	O
17	O
%	O
of	O
messages	O
(	O
124	O
S	O
i	O
m	O
s	O
)	O
all	O
five	O
Turkers	O
use	O
the	O
same	O
strategy	O
to	O
interpret	O
the	O
S	O
i	O
m	O
s	O
(	O
labeled	O
as	O
5	O
on	O
the	O
X	O
-	O
axis	O
)	O
,	O
whereas	O
for	O
26	O
%	O
(	O
188	O
S	O
i	O
m	O
s	O
)	O
,	O
4	O
Turkers	O
used	O
same	O
strategy	O
(	O
labeled	O
as	O
4	O
,	O
1	O
on	O
Xaxis	O
)	O
and	O
so	O
on	O
.	O
We	O
observe	O
when	O
the	O
S	O
i	O
m	O
s	O
are	O
marked	O
by	O
strong	O
subjective	O
words	O
e.g.	O
,	O
"	O
great	O
"	O
,	O
"	O
best	O
"	O
,	O
etc	O
.	O
,	O
they	O
have	O
been	O
replaced	O
in	O
90	O
%	O
of	O
cases	O
as	O
lexical	O
antonyms	O
(	O
e.g.	O
,	O
"	O
great	O
"	O
!	O
"	O
terrible	O
"	O
)	O
.	O
In	O
addition	O
,	O
the	O
majority	O
of	O
adjectives	O
are	O
used	O
in	O
attributive	O
position	O
(	O
i.e.	O
,	O
"	O
lovely	O
neighbor	O
is	O
vacuuming	O
at	O
night	O
"	O
)	O
,	O
thus	O
blocking	O
paraphrases	O
involving	O
predicate	O
negation	O
.	O
However	O
,	O
not	O
all	O
strong	O
subjective	O
words	O
guarantee	O
the	O
use	O
of	O
direct	O
opposites	O
in	O
the	O
H	O
int	O
s	O
(	O
e.g.	O
,	O
"	O
flattering	O
"	O
!	O
"	O
not	O
flattering	O
"	O
;	O
See	O
Table	O
1	O
)	O
.	O
The	O
choice	O
of	O
strategies	O
may	O
also	O
depend	O
upon	O
the	O
target	O
of	O
ironic	O
situation	O
(	O
Ivanko	O
and	O
Pexman	O
,	O
2003	O
)	O
.	O
We	O
implement	O
the	O
bootstrapping	O
algorithm	O
from	O
Riloff	O
et	O
al	O
(	O
2013	O
)	O
to	O
identify	O
ironic	O
situations	O
in	O
S	O
i	O
m	O
s	O
that	O
are	O
rephrased	O
by	O
Lexical	O
antonym	O
strategy	O
.	O
We	O
find	O
utterances	O
containing	O
stereotypical	O
negative	O
situations	O
regarding	O
health	O
issues	O
(	O
e.g.	O
,	O
"	O
having	O
migraines	O
"	O
,	O
"	O
getting	O
killed	O
by	O
chemicals	O
"	O
)	O
and	O
other	O
undesirable	O
negative	O
states	O
such	O
as	O
"	O
oversleeping	O
"	O
,	O
"	O
luggage	O
lost	O
"	O
,	O
"	O
stress	O
in	O
life	O
"	O
are	O
almost	O
always	O
interpreted	O
via	O
lexical	O
antonym	O
strategy	O
.	O
Utterances	O
where	O
all	O
five	O
Turkers	O
used	O
simple	O
negation	O
,	O
if	O
negative	O
particles	O
are	O
positioned	O
in	O
the	O
ironic	O
message	O
with	O
a	O
sentential	O
scope	O
(	O
e.g.	O
,	O
"	O
not	O
a	O
biggie	O
"	O
,	O
"	O
not	O
awkward	O
"	O
)	O
then	O
they	O
are	O
simply	O
omitted	O
in	O
the	O
interpretations	O
.	O
This	O
trend	O
can	O
be	O
explained	O
according	O
to	O
the	O
inter	O
-	O
subjective	O
account	O
of	O
negation	O
types	O
(	O
Verhagen	O
,	O
2005	O
)	O
.	O
Sentential	O
negation	O
leads	O
the	O
addressee	O
to	O
open	O
up	O
an	O
alternative	O
mental	O
space	O
where	O
an	O
opposite	O
predication	O
is	O
at	O
stake	O
.	O

We	O
leveraged	O
a	O
crowdsourcing	O
task	O
to	O
obtain	O
a	O
dataset	O
of	O
ironic	O
utterances	O
paired	O
with	O
the	O
hearer	O
's	O
verbalization	O
of	O
their	O
interpretation	O
.	O
We	O
proposed	O
a	O
typology	O
of	O
linguistic	O
strategies	O
for	O
verbal	O
irony	O
interpretation	O
and	O
designed	O
computational	O
models	O
to	O
capture	O
these	O
strategies	O
with	O
good	O
performance	O
.	O
Our	O
study	O
shows	O
(	O
1	O
)	O
Turkers	O
mostly	O
adopt	O
lexical	O
antonym	O
and	O
negation	O
strategies	O
to	O
interpret	O
speaker	O
's	O
irony	O
,	O
(	O
2	O
)	O
interpretations	O
are	O
correlated	O
to	O
stereotype	O
ironic	O
situations	O
,	O
and	O
(	O
3	O
)	O
irony	O
expression	O
(	O
explicit	O
vs.	O
implicit	O
incongruity	O
and	O
absence	O
or	O
presence	O
of	O
markers	O
)	O
influences	O
the	O
choice	O
of	O
interpretation	O
strategies	O
and	O
match	O
with	O
different	O
explanatory	O
theories	O
(	O
the	O
Gricean	O
approach	O
links	O
up	O
better	O
with	O
explicit	O
incongruity	O
,	O
while	O
Relevance	O
Theory	O
with	O
the	O
implicit	O
one	O
)	O
.	O
The	O
latter	O
can	O
have	O
an	O
impact	O
on	O
irony	O
detection	O
by	O
bringing	O
out	O
more	O
discriminative	O
semantic	O
and	O
pragmatic	O
features	O
.	O

The	O
Future	O
is	O
not	O
One	O
-	O
dimensional	O
:	O
Complex	O
Event	O
Schema	O
Induction	O
by	O
Graph	O
Modeling	O
for	O
Event	O
Prediction	O

Subgraph	O
of	O
G	O
containing	O
events	O
before	O
ei	O
and	O
their	O
arguments	O

From	O
a	O
set	O
of	O
documents	O
describing	O
a	O
complex	O
event	O
,	O
we	O
construct	O
an	O
instance	O
graph	O
G	O
which	O
contains	O
event	O
nodes	O
E	O
and	O
entity	O
nodes	O
(	O
argument	O
nodes	O
)	O
V	O
.	O
There	O
are	O
three	O
types	O
of	O
edges	O
in	O
this	O
graph	O
:	O
(	O
1	O
)	O
event	O
-	O
event	O
edges	O
e	O
i	O
,	O
e	O
l	O
connecting	O
events	O
that	O
have	O
direct	O
temporal	O
relations	O
;	O
(	O
2	O
)	O
evententity	O
edges	O
e	O
i	O
,	O
a	O
,	O
v	O
j	O
connecting	O
arguments	O
to	O
the	O
event	O
;	O
and	O
(	O
3	O
)	O
entity	O
-	O
entity	O
edges	O
v	O
j	O
,	O
r	O
,	O
v	O
k	O
indicating	O
relations	O
between	O
entities	O
.	O
We	O
can	O
construct	O
instance	O
graphs	O
by	O
applying	O
Information	O
Extraction	O
(	O
IE	O
)	O
techniques	O
on	O
an	O
input	O
text	O
corpus	O
.	O
In	O
these	O
graphs	O
,	O
the	O
relation	O
edges	O
do	O
not	O
have	O
directions	O
but	O
temporal	O
edges	O
between	O
events	O
are	O
directional	O
,	O
going	O
from	O
the	O
event	O
before	O
to	O
the	O
event	O
after	O
.	O
For	O
each	O
complex	O
event	O
type	O
,	O
given	O
a	O
set	O
of	O
instance	O
graphs	O
G	O
,	O
the	O
goal	O
of	O
schema	O
induction	O
is	O
to	O
generate	O
a	O
schema	O
library	O
S.	O
In	O
each	O
schema	O
graph	O
S	O
,	O
the	O
nodes	O
are	O
abstracted	O
to	O
the	O
types	O
of	O
events	O
and	O
entities	O
.	O
Figure	O
1	O
is	O
an	O
example	O
of	O
schema	O
2	O
for	O
complex	O
event	O
type	O
car	O
-	O
bombing	O
.	O
Schema	O
graphs	O
can	O
be	O
regarded	O
as	O
a	O
summary	O
abstraction	O
of	O
instance	O
graphs	O
,	O
capturing	O
the	O
reoccurring	O
structures	O
.	O
3	O
Our	O
Approach	O

Baseline	O
1	O
:	O
Event	O
Language	O
Model	O
(	O
Rudinger	O
et	O
al	O
,	O
2015	O
;	O
Pichotta	O
and	O
Mooney	O
,	O
2016	O
)	O
is	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
event	O
schema	O
induction	O
method	O
.	O
It	O
learns	O
the	O
probability	O
of	O
temporal	O
event	O
sequences	O
,	O
and	O
the	O
event	O
sequences	O
generated	O
from	O
event	O
language	O
model	O
are	O
considered	O
as	O
schemas	O
.	O
Baseline	O
2	O
:	O
Sequential	O
Pattern	O
Mining	O
(	O
Pei	O
et	O
al	O
,	O
2001	O
)	O
is	O
a	O
classic	O
algorithm	O
for	O
discovering	O
common	O
sequences	O
.	O
We	O
also	O
attach	O
arguments	O
and	O
their	O
relations	O
as	O
extensions	O
to	O
the	O
pattern	O
.	O
Considering	O
that	O
the	O
event	O
language	O
model	O
baseline	O
can	O
not	O
handle	O
multiple	O
arguments	O
and	O
relations	O
,	O
we	O
add	O
sequential	O
pattern	O
mining	O
for	O
comparison	O
.	O
The	O
frequent	O
patterns	O
mined	O
are	O
considered	O
as	O
schemas	O
.	O
Reference	O
:	O
Human	O
Schema	O
is	O
added	O
as	O
a	O
baseline	O
in	O
the	O
extrinsic	O
task	O
of	O
event	O
prediction	O
.	O
Since	O
human	O
-	O
created	O
schemas	O
are	O
highly	O
accurate	O
but	O
not	O
probabilistic	O
,	O
we	O
want	O
to	O
evaluate	O
their	O
limits	O
at	O
predicting	O
events	O
in	O
the	O
extrinsic	O
task	O
.	O
We	O
match	O
schemas	O
to	O
instances	O
and	O
fill	O
in	O
the	O
matched	O
type	O
.	O
Ablation	O
Study	O
:	O
Event	O
Graph	O
Model	O
w/o	O
Argument	O
Generation	O
is	O
included	O
as	O
a	O
variant	O
of	O
our	O
model	O
in	O
which	O
we	O
remove	O
argument	O
generation	O
(	O
3.5	O
and	O
3.6	O
)	O
.	O
It	O
learns	O
to	O
generate	O
a	O
graph	O
containing	O
only	O
event	O
nodes	O
with	O
their	O
temporal	O
relations	O
,	O
aiming	O
to	O
verify	O
whether	O
incorporating	O
argument	O
information	O
helps	O
event	O
modeling	O
.	O

We	O
propose	O
a	O
new	O
task	O
to	O
induce	O
temporal	O
complex	O
event	O
schemas	O
,	O
which	O
are	O
capable	O
of	O
representing	O
multiple	O
temporal	O
dependencies	O
between	O
events	O
and	O
their	O
connected	O
arguments	O
.	O
We	O
induce	O
such	O
schemas	O
by	O
learning	O
an	O
event	O
graph	O
model	O
,	O
a	O
deep	O
auto	O
-	O
regressive	O
model	O
,	O
from	O
the	O
automatically	O
extracted	O
instance	O
graphs	O
.	O
Experiments	O
demonstrate	O
the	O
model	O
's	O
effectiveness	O
on	O
both	O
intrinsic	O
evaluation	O
and	O
the	O
downstream	O
task	O
of	O
schema	O
-	O
guided	O
event	O
prediction	O
.	O
These	O
schemas	O
can	O
guide	O
our	O
understanding	O
and	O
ability	O
to	O
make	O
predictions	O
with	O
respect	O
to	O
what	O
might	O
happen	O
next	O
,	O
along	O
with	O
background	O
knowledge	O
including	O
location	O
-	O
,	O
and	O
participant	O
-	O
specific	O
and	O
temporally	O
ordered	O
event	O
information	O
.	O
In	O
the	O
future	O
,	O
we	O
plan	O
to	O
extend	O
our	O
framework	O
to	O
hierarchical	O
event	O
schema	O
induction	O
,	O
as	O
well	O
as	O
event	O
and	O
argument	O
instance	O
prediction	O
.	O

Evaluating	O
the	O
Evaluation	O
of	O
Diversity	O
in	O
Natural	O
Language	O
Generation	O

Despite	O
growing	O
interest	O
in	O
natural	O
language	O
generation	O
(	O
NLG	O
)	O
models	O
that	O
produce	O
diverse	O
outputs	O
,	O
there	O
is	O
currently	O
no	O
principled	O
method	O
for	O
evaluating	O
the	O
diversity	O
of	O
an	O
NLG	O
system	O
.	O
In	O
this	O
work	O
,	O
we	O
propose	O
a	O
framework	O
for	O
evaluating	O
diversity	O
metrics	O
.	O
The	O
framework	O
measures	O
the	O
correlation	O
between	O
a	O
proposed	O
diversity	O
metric	O
and	O
a	O
diversity	O
parameter	O
,	O
a	O
single	O
parameter	O
that	O
controls	O
some	O
aspect	O
of	O
diversity	O
in	O
generated	O
text	O
.	O
For	O
example	O
,	O
a	O
diversity	O
parameter	O
might	O
be	O
a	O
binary	O
variable	O
used	O
to	O
instruct	O
crowdsourcing	O
workers	O
to	O
generate	O
text	O
with	O
either	O
low	O
or	O
high	O
content	O
diversity	O
.	O
We	O
demonstrate	O
the	O
utility	O
of	O
our	O
framework	O
by	O
:	O
(	O
a	O
)	O
establishing	O
best	O
practices	O
for	O
eliciting	O
diversity	O
judgments	O
from	O
humans	O
,	O
(	O
b	O
)	O
showing	O
that	O
humans	O
substantially	O
outperform	O
automatic	O
metrics	O
in	O
estimating	O
content	O
diversity	O
,	O
and	O
(	O
c	O
)	O
demonstrating	O
that	O
existing	O
methods	O
for	O
controlling	O
diversity	O
by	O
tuning	O
a	O
"	O
decoding	O
parameter	O
"	O
mostly	O
affect	O
form	O
but	O
not	O
meaning	O
.	O
Our	O
framework	O
can	O
advance	O
the	O
understanding	O
of	O
different	O
diversity	O
metrics	O
,	O
an	O
essential	O
step	O
on	O
the	O
road	O
towards	O
better	O
NLG	O
systems	O
.	O

To	O
conclude	O
,	O
increasing	O
interest	O
in	O
diversity	O
resulted	O
in	O
multiple	O
proposed	O
diversity	O
metrics	O
.	O
However	O
,	O
there	O
is	O
no	O
consensus	O
on	O
how	O
to	O
evaluate	O
diversity	O
and	O
what	O
each	O
metric	O
actually	O
measures	O
.	O

In	O
the	O
content	O
test	O
(	O
conTest	O
)	O
,	O
our	O
goal	O
is	O
to	O
evaluate	O
how	O
different	O
diversity	O
metrics	O
capture	O
the	O
notion	O
of	O
content	O
diversity	O
.	O
Measuring	O
content	O
diversity	O
requires	O
deep	O
understanding	O
of	O
the	O
semantics	O
of	O
responses	O
in	O
S	O
c	O
.	O
To	O
isolate	O
content	O
from	O
form	O
diversity	O
,	O
we	O
aim	O
to	O
generate	O
response	O
sets	O
with	O
a	O
similar	O
level	O
of	O
form	O
diversity	O
,	O
but	O
where	O
the	O
level	O
of	O
content	O
diversity	O
is	O
controlled	O
by	O
the	O
diversity	O
parameter	O
d.	O
In	O
6	O
,	O
we	O
will	O
focus	O
on	O
whether	O
automatic	O
diversity	O
metrics	O
can	O
perform	O
as	O
well	O
as	O
humans	O
on	O
the	O
task	O
of	O
estimating	O
content	O
diversity	O
.	O

One	O
of	O
the	O
core	O
questions	O
we	O
tackle	O
is	O
:	O
Can	O
humans	O
evaluate	O
diversity	O
reliably	O
?	O
Although	O
a	O
few	O
papers	O
(	O
Ghandeharioun	O
et	O
al	O
,	O
2019	O
;	O
Zhang	O
et	O
al	O
,	O
2019b	O
)	O
asked	O
humans	O
to	O
evaluate	O
diversity	O
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
no	O
work	O
thoroughly	O
investigated	O
this	O
question	O
.	O
The	O
importance	O
of	O
this	O
question	O
is	O
clear	O
when	O
comparing	O
to	O
quality	O
evaluation	O
.	O
There	O
,	O
human	O
judgment	O
is	O
the	O
gold	O
standard	O
,	O
and	O
automatic	O
quality	O
metrics	O
are	O
established	O
by	O
showing	O
high	O
correlation	O
with	O
human	O
score	O
.	O
Thus	O
,	O
understanding	O
if	O
humans	O
can	O
judge	O
diversity	O
is	O
important	O
for	O
improving	O
diversity	O
metrics	O
.	O
We	O
use	O
crowdsourcing	O
workers	O
2	O
to	O
compute	O
a	O
human	O
diversity	O
score	O
:	O
we	O
show	O
workers	O
a	O
context	O
followed	O
by	O
a	O
set	O
of	O
responses	O
,	O
and	O
ask	O
them	O
to	O
rate	O
the	O
diversity	O
of	O
the	O
set	O
.	O
To	O
establish	O
best	O
practices	O
,	O
we	O
experiment	O
with	O
multiple	O
variations	O
of	O
HDS	O
(	O
detailed	O
in	O
6.2	O
)	O
,	O
asking	O
humans	O
to	O
rate	O
the	O
diversity	O
of	O
a	O
response	O
set	O
,	O
and	O
evaluating	O
each	O
practice	O
with	O
our	O
framework	O
.	O
We	O
focus	O
on	O
the	O
following	O
questions	O
:	O
Should	O
humans	O
rate	O
diversity	O
of	O
a	O
set	O
or	O
similarity	O
between	O
pairs	O
in	O
the	O
set	O
,	O
from	O
which	O
diversity	O
can	O
be	O
inferred	O
?	O
(	O
tl	O
;	O
dr	O
:	O
diversity	O
)	O
Can	O
humans	O
evaluate	O
different	O
aspects	O
of	O
diversity	O
well	O
?	O
(	O
tl	O
;	O
dr	O
:	O
not	O
effectively	O
)	O
Should	O
humans	O
rate	O
the	O
absolute	O
diversity	O
score	O
of	O
a	O
set	O
of	O
sentences	O
or	O
rank	O
whether	O
one	O
set	O
is	O
more	O
diverse	O
than	O
another	O
?	O
Here	O
,	O
we	O
did	O
not	O
reach	O
a	O
conclusive	O
result	O
,	O
and	O
describe	O
this	O
experiment	O
in	O
the	O
Appendix	O
C.	O
As	O
a	O
preliminary	O
step	O
,	O
we	O
conducted	O
pilot	O
experiments	O
among	O
a	O
group	O
of	O
NLP	O
graduate	O
students	O
.	O
The	O
main	O
insights	O
were	O
:	O
(	O
a	O
)	O
humans	O
are	O
biased	O
by	O
quality	O
:	O
if	O
a	O
generated	O
set	O
has	O
high	O
diversity	O
but	O
low	O
quality	O
,	O
humans	O
will	O
rate	O
diversity	O
low	O
.	O
To	O
neutralize	O
this	O
,	O
we	O
explicitly	O
ask	O
workers	O
to	O
evaluate	O
the	O
quality	O
of	O
one	O
of	O
the	O
responses	O
in	O
the	O
set	O
S	O
c	O
,	O
and	O
then	O
instruct	O
them	O
to	O
ignore	O
quality	O
in	O
diversity	O
questions	O
;	O
(	O
b	O
)	O
To	O
make	O
sure	O
a	O
worker	O
reads	O
the	O
context	O
c	O
,	O
we	O
ask	O
them	O
to	O
generate	O
a	O
sentence	O
s	O
before	O
they	O
rate	O
diversity	O
;	O
(	O
c	O
)	O
It	O
is	O
difficult	O
for	O
workers	O
to	O
evaluate	O
the	O
diversity	O
of	O
a	O
set	O
with	O
more	O
than	O
10	O
responses	O
.	O
Our	O
crowdsourcing	O
tasks	O
are	O
provided	O
in	O
Appendix	O
A.	O

For	O
each	O
task	O
,	O
we	O
collected	O
200	O
sets	O
of	O
5	O
responses	O
each	O
(	O
100	O
sets	O
per	O
class	O
)	O
.	O
For	O
high	O
content	O
diversity	O
class	O
,	O
we	O
asked	O
workers	O
to	O
give	O
5	O
responses	O
per	O
context	O
,	O
with	O
as	O
different	O
content	O
and	O
structure	O
as	O
possible	O
.	O
Then	O
we	O
asked	O
the	O
same	O
workers	O
to	O
choose	O
a	O
single	O
response	O
they	O
wrote	O
,	O
and	O
rephrase	O
it	O
5	O
times	O
such	O
that	O
the	O
original	O
content	O
will	O
be	O
preserved	O
,	O
while	O
changing	O
the	O
form	O
-	O
this	O
set	O
is	O
used	O
for	O
the	O
low	O
content	O
diversity	O
class	O
.	O
A	O
sample	O
from	O
this	O
data	O
is	O
in	O
Figure	O
1	O
and	O
more	O
samples	O
in	O
Appendix	O
B.	O
For	O
each	O
HDS	O
metric	O
,	O
we	O
collected	O
10	O
ratings	O
from	O
crowdsourcing	O
workers	O
,	O
different	O
than	O
the	O
ones	O
who	O
composed	O
the	O
sets	O
.	O

In	O
this	O
work	O
,	O
we	O
focused	O
on	O
the	O
two	O
primary	O
aspects	O
of	O
diversity	O
:	O
content	O
diversity	O
(	O
What	O
to	O
say	O
?	O
)	O
and	O
form	O
diversity	O
(	O
How	O
to	O
say	O
it	O
?	O
)	O
.	O
In	O
Figure	O
1	O
,	O
Both	O
sets	O
are	O
diverse	O
,	O
but	O
Set	O
B	O
is	O
only	O
form	O
diverse	O
,	O
as	O
all	O
answers	O
deliver	O
the	O
same	O
massage	O
,	O
whereas	O
Set	O
A	O
is	O
diverse	O
in	O
both	O
form	O
and	O
content	O
.	O
Furthermore	O
,	O
we	O
can	O
observe	O
aspects	O
of	O
diversity	O
as	O
having	O
a	O
tree	O
-	O
like	O
structure	O
,	O
where	O
both	O
content	O
and	O
form	O
diversity	O
can	O
be	O
divided	O
to	O
subaspects	O
:	O
Content	O
diversity	O
(	O
e.g.	O
answering	O
the	O
question	O
"	O
How	O
are	O
you	O
today	O
?	O
"	O
)	O
can	O
be	O
expressed	O
by	O
using	O
different	O
sentiment	O
(	O
"	O
I	O
'm	O
doing	O
good	O
.	O
"	O
vs.	O
"	O
I	O
'm	O
so	O
glad	O
you	O
asked	O
!	O
I	O
'm	O
really	O
doing	O
good	O
.	O
"	O
)	O
,	O
different	O
relevance	O
(	O
"	O
I	O
'm	O
fine	O
"	O
vs.	O
"	O
Did	O
you	O
watch	O
the	O
game	O
last	O
night	O
?	O
"	O
)	O
,	O
and	O
more	O
.	O
Form	O
diversity	O
can	O
be	O
divided	O
into	O
sub	O
-	O
aspects	O
as	O
well	O
:	O
syntactic	O
diversity	O
(	O
"	O
Someone	O
took	O
it	O
from	O
me	O
.	O
"	O
vs.	O
"	O
It	O
was	O
taken	O
from	O
me	O
.	O
"	O
)	O
or	O
lexical	O
diversity	O
(	O
"	O
I	O
feel	O
fine	O
.	O
"	O
vs.	O
"	O
I	O
feel	O
very	O
well	O
.	O
"	O
)	O
.	O
Even	O
those	O
sub	O
-	O
aspects	O
can	O
be	O
further	O
divided	O
.	O
For	O
example	O
,	O
a	O
sub	O
-	O
aspect	O
of	O
lexical	O
diversity	O
is	O
register	O
diversity	O
(	O
"	O
How	O
are	O
you	O
?	O
"	O
vs.	O
"	O
Sup	O
bro	O
?	O
"	O
)	O
.	O
Another	O
observation	O
is	O
that	O
different	O
aspects	O
are	O
not	O
orthogonal	O
,	O
that	O
is	O
,	O
changing	O
one	O
aspect	O
may	O
lead	O
to	O
changes	O
in	O
other	O
aspects	O
.	O
Specifically	O
,	O
we	O
observe	O
that	O
while	O
it	O
is	O
relatively	O
easy	O
to	O
produce	O
high	O
form	O
diversity	O
with	O
low	O
content	O
diversity	O
(	O
Set	O
B	O
in	O
Figure	O
1	O
)	O
,	O
it	O
is	O
almost	O
impossible	O
to	O
diversify	O
content	O
without	O
changing	O
form	O
.	O
This	O
observation	O
was	O
important	O
during	O
the	O
design	O
of	O
conTest	O
.	O

This	O
work	O
presents	O
a	O
framework	O
for	O
evaluating	O
diversity	O
metrics	O
as	O
a	O
step	O
toward	O
standardized	O
evaluation	O
.	O
We	O
limit	O
the	O
scope	O
of	O
this	O
work	O
to	O
differ	O
-	O
ences	O
between	O
form	O
and	O
content	O
diversity	O
,	O
which	O
are	O
key	O
towards	O
understanding	O
different	O
aspects	O
of	O
diversity	O
.	O
Future	O
work	O
can	O
explore	O
other	O
aspects	O
of	O
diversity	O
,	O
e.g.	O
testing	O
sentiment	O
diversity	O
,	O
as	O
proposed	O
in	O
3	O
.	O
We	O
urge	O
researchers	O
to	O
use	O
this	O
framework	O
as	O
a	O
platform	O
for	O
developing	O
new	O
diversity	O
metrics	O
and	O
establishing	O
their	O
efficiency	O
.	O

All	O
Human	O
scores	O
for	O
HDS	O
metrics	O
were	O
collected	O
using	O
Amazon	O
Mechanical	O
Turk	O
(	O
AMT	O
)	O
crowdsourcing	O
platform	O
by	O
English	O
native	O
-	O
speaking	O
workers	O
that	O
were	O
specifically	O
qualified	O
for	O
this	O
task	O
.	O
Figure	O
7	O
presents	O
the	O
warm	O
-	O
up	O
part	O
,	O
common	O
for	O
all	O
HDS	O
questionnaires	O
.	O
Before	O
asking	O
workers	O
to	O
rate	O
the	O
diversity	O
of	O
each	O
set	O
,	O
we	O
first	O
asked	O
them	O
to	O
generate	O
a	O
response	O
for	O
the	O
context	O
themselves	O
,	O
to	O
make	O
sure	O
they	O
read	O
it	O
.	O
To	O
neutralize	O
the	O
effect	O
of	O
the	O
responses	O
'	O
quality	O
on	O
the	O
workers	O
,	O
we	O
also	O
asked	O
the	O
workers	O
to	O
rate	O
the	O
quality	O
of	O
the	O
first	O
response	O
in	O
the	O
set	O
,	O
then	O
explicitly	O
instructed	O
them	O
to	O
ignore	O
quality	O
when	O
rating	O
diversity	O
.	O
Figures	O
8	O
to	O
11	O
present	O
the	O
diversity	O
questions	O
of	O
absHDS	O
,	O
aspHDS	O
,	O
rnkHDS	O
and	O
simHDS	O
as	O
appeared	O
in	O
the	O
AMT	O
questionnaires	O
.	O
Costs	O
For	O
HDS	O
metrics	O
that	O
require	O
one	O
query	O
per	O
response	O
set	O
(	O
i.e.	O
absHDS	O
,	O
rnkHDS	O
,	O
aspDHS	O
)	O
,	O
the	O
cost	O
for	O
a	O
single	O
rating	O
was	O
0.18$.	O
We	O
collected	O
10	O
ratings	O
per	O
response	O
set	O
,	O
and	O
conduct	O
each	O
experiment	O
with	O
200	O
sets	O
,	O
hence	O
the	O
total	O
cost	O
for	O
an	O
experiment	O
was	O
360$.	O
In	O
the	O
case	O
of	O
simHDS	O
,	O
the	O
response	O
set	O
size	O
was	O
5	O
,	O
and	O
the	O
number	O
of	O
queries	O
needed	O
per	O
set	O
is	O
5	O
2	O
=	O
10	O
.	O
The	O
cost	O
of	O
a	O
single	O
rating	O
for	O
this	O
task	O
was	O
0.056	O
$	O
,	O
and	O
with	O
the	O
same	O
multipliers	O
,	O
the	O
total	O
cost	O
for	O
an	O
experiment	O
was	O
1120	O
$	O
,	O
three	O
times	O
more	O
expensive	O
.	O

Tables	O
20	O
to	O
22	O
present	O
data	O
samples	O
from	O
sto	O
-	O
ryGen	O
,	O
respGen	O
and	O
promptGen	O
with	O
the	O
human	O
testers	O
of	O
conTest	O
,	O
as	O
detailed	O
in	O
6	O
.	O
Each	O
table	O
presents	O
two	O
contexts	O
and	O
two	O
response	O
sets	O
per	O
context	O
-	O
one	O
for	O
the	O
low	O
content	O
diversity	O
class	O
and	O
one	O
for	O
the	O
high	O
content	O
diversity	O
class	O
.	O

Response	O
set	O
(	O
low	O
content	O
diversity	O
)	O
Suppose	O
there	O
's	O
an	O
escape	O
plan	O
we	O
have	O
n't	O
thought	O
of	O
yet	O
.	O
Suppose	O
there	O
's	O
an	O
omelet	O
that	O
is	O
the	O
most	O
amazing	O
ever	O
.	O
Suppose	O
there	O
's	O
an	O
airplane	O
ticket	O
that	O
's	O
even	O
cheaper	O
.	O
Suppose	O
there	O
's	O
an	O
actual	O
deadline	O
for	O
this	O
paper	O
.	O
Suppose	O
there	O
's	O
an	O
event	O
that	O
we	O
can	O
go	O
to	O
this	O
weekend	O
.	O
Suppose	O
there	O
's	O
an	O
airline	O
that	O
costs	O
less	O
.	O
Suppose	O
there	O
's	O
an	O
flight	O
that	O
is	O
n't	O
as	O
expensive	O
.	O
Suppose	O
there	O
's	O
an	O
air	O
travel	O
fare	O
,	O
but	O
does	O
n't	O
cost	O
as	O
much	O
.	O
Suppose	O
there	O
's	O
an	O
way	O
to	O
fly	O
there	O
that	O
is	O
low	O
cost	O
.	O
Suppose	O
there	O
's	O
an	O
flight	O
going	O
there	O
and	O
it	O
's	O
not	O
a	O
lot	O
of	O
money	O
Nothing	O
remotely	O
like	O
eating	O
a	O
big	O
breakfast	O
.	O
Nothing	O
remotely	O
like	O
dancing	O
with	O
your	O
wife	O
at	O
the	O
wedding	O
.	O
Nothing	O
remotely	O
like	O
singing	O
Justin	O
Bieber	O
's	O
greatest	O
hits	O
Nothing	O
remotely	O
like	O
falling	O
down	O
a	O
hill	O
Nothing	O
remotely	O
like	O
getting	O
yelled	O
at	O
Nothing	O
remotely	O
like	O
being	O
super	O
full	O
and	O
satisfied	O
.	O
Nothing	O
remotely	O
like	O
getting	O
to	O
taste	O
many	O
different	O
foods	O
.	O
Nothing	O
remotely	O
like	O
starting	O
the	O
day	O
off	O
right	O
.	O
Nothing	O
remotely	O
like	O
doing	O
exactly	O
what	O
I	O
want	O
to	O
do	O
.	O
Nothing	O
remotely	O
like	O
feeding	O
myself	O
with	O
great	O
food	O
.	O

We	O
thank	O
Aya	O
Meltzer	O
-	O
Asscher	O
for	O
linguistic	O
advice	O
,	O
and	O
Or	O
Nachmias	O
,	O
Ben	O
Bogin	O
,	O
Mor	O
Geva	O
,	O
Omer	O
Goldman	O
and	O
Ohad	O
Rubin	O
for	O
their	O
useful	O
suggestions	O
and	O
references	O
.	O
This	O
research	O
was	O
partially	O
supported	O
by	O
The	O
Israel	O
Science	O
Foundation	O
grant	O
942/16	O
,	O
The	O
Yandex	O
Initiative	O
for	O
Machine	O
Learning	O
and	O
the	O
European	O
Research	O
Council	O
(	O
ERC	O
)	O
under	O
the	O
European	O
Union	O
Horizons	O
2020	O
research	O
and	O
innovation	O
programme	O
(	O
grant	O
ERC	O
DELPHI	O
802800	O
)	O
.	O

Response	O
set	O
(	O
τ	O
=	O
0.25	O
)	O
Response	O
set	O
(	O
τ	O
=	O
0.8	O
)	O
Response	O
set	O
(	O
τ	O
=	O
1.1	O
)	O
Fire	O
next	O
door	O
.	O
John	O
woke	O
up	O
smelling	O
like	O
something	O
was	O
burning	O
.	O
He	O
went	O
outside	O
.	O
He	O
saw	O
the	O
fire	O
next	O
door	O
.	O
He	O
called	O
the	O
authorities	O
.	O
It	O
was	O
a	O
minor	O
fire	O
and	O
they	O
put	O
it	O
out	O
.	O
It	O
was	O
a	O
fire	O
.	O
It	O
was	O
a	O
fire	O
.	O
It	O
was	O
a	O
fire	O
.	O
It	O
was	O
a	O
fire	O
.	O
It	O
was	O
a	O
fire	O
.	O
It	O
was	O
a	O
minor	O
fire	O
.	O
It	O
was	O
a	O
fire	O
.	O
It	O
was	O
a	O
fire	O
.	O
It	O
was	O
a	O
minor	O
fire	O
and	O
they	O
put	O
it	O
out	O
.	O
They	O
arrived	O
and	O
put	O
out	O
the	O
fire	O
.	O
It	O
was	O
a	O
fire	O
.	O
It	O
was	O
a	O
fire	O
.	O
It	O
turned	O
out	O
to	O
be	O
a	O
fire	O
.	O
It	O
was	O
a	O
minor	O
fire	O
night	O
.	O
They	O
arrived	O
and	O
put	O
it	O
out	O
.	O
It	O
was	O
a	O
scary	O
but	O
beautiful	O
fire	O
next	O
time	O
he	O
went	O
in	O
.	O
It	O
was	O
a	O
fire	O
.	O
It	O
was	O
a	O
put	O
out	O
and	O
John	O
was	O
able	O
to	O
put	O
it	O
out	O
.	O
It	O
was	O
a	O
scary	O
place	O
to	O
live	O
.	O
It	O
turned	O
out	O
to	O
be	O
a	O
mechanic	O
.	O
Before	O
the	O
fire	O
was	O
put	O
out	O
it	O
was	O
a	O
fire	O
.	O
It	O
was	O
a	O
fire	O
.	O
They	O
co	O
-	O
worker	O
matter	O
how	O
bad	O
the	O
fire	O
was	O
.	O
Several	O
shells	O
,	O
the	O
fire	O
department	O
came	O
just	O
in	O
time	O
.	O
They	O
arrived	O
and	O
put	O
out	O
the	O
fire	O
.	O
It	O
turned	O
out	O
to	O
be	O
a	O
clear	O
,	O
fact	O
.	O
It	O
was	O
a	O
fire	O
.	O
It	O
was	O
a	O
scary	O
meal	O
for	O
wet	O
clothes	O
.	O
It	O
was	O
a	O
scary	O
stealing	O
from	O
him	O
.	O
Charles	O
Shaves	O
His	O
Chest	O
.	O
Charles	O
decides	O
he	O
no	O
longer	O
wants	O
chest	O
hair	O
.	O
He	O
shaves	O
his	O
chest	O
hair	O
with	O
a	O
razor	O
.	O
Although	O
his	O
skin	O
is	O
irritated	O
,	O
he	O
hair	O
is	O
now	O
gone	O
.	O
Charles	O
is	O
satisfied	O
.	O
He	O
is	O
glad	O
that	O
he	O
finally	O
shaved	O
.	O
He	O
is	O
glad	O
that	O
he	O
finally	O
shaved	O
.	O
He	O
is	O
very	O
happy	O
to	O
have	O
a	O
chest	O
hair	O
.	O
He	O
is	O
glad	O
that	O
he	O
finally	O
shaved	O
.	O
He	O
is	O
glad	O
that	O
he	O
finally	O
shaved	O
.	O
He	O
is	O
glad	O
that	O
he	O
finally	O
shaved	O
.	O
He	O
is	O
glad	O
that	O
he	O
finally	O
shaved	O
.	O
He	O
is	O
glad	O
that	O
he	O
finally	O
shaved	O
.	O
He	O
is	O
glad	O
that	O
he	O
finally	O
shaved	O
.	O
He	O
is	O
glad	O
that	O
he	O
finally	O
shaved	O
.	O
He	O
is	O
very	O
happy	O
to	O
have	O
a	O
chest	O
hair	O
.	O
He	O
is	O
glad	O
that	O
he	O
finally	O
shaved	O
.	O
He	O
is	O
very	O
happy	O
that	O
he	O
no	O
longer	O
has	O
chest	O
hair	O
.	O
He	O
is	O
glad	O
that	O
he	O
finally	O
shaved	O
.	O
He	O
is	O
glad	O
that	O
he	O
finally	O
shaved	O
.	O
He	O
is	O
glad	O
that	O
his	O
chest	O
hair	O
is	O
no	O
longer	O
shaved	O
.	O
He	O
is	O
glad	O
that	O
he	O
finally	O
shaved	O
He	O
is	O
glad	O
that	O
he	O
finally	O
shaved	O
.	O
He	O
is	O
glad	O
that	O
he	O
finally	O
of	O
shaving	O
.	O
He	O
is	O
glad	O
that	O
he	O
finally	O
shashock	O
his	O
chest	O
hair	O
.	O
He	O
is	O
glad	O
that	O
he	O
finally	O
peaceful	O
his	O
voice	O
again	O
.	O
He	O
is	O
glad	O
that	O
he	O
finally	O
shaavoided	O
his	O
chest	O
hair	O
.	O
He	O
is	O
glad	O
that	O
he	O
finally	O
downcome	O
.	O
He	O
is	O
ready	O
to	O
start	O
better	O
cooperation	O
his	O
fireplace	O
.	O
He	O
is	O
glad	O
that	O
he	O
finally	O
shaved	O
.	O
He	O
is	O
glad	O
that	O
he	O
no	O
longer	O
has	O
chest	O
hair	O
.	O
He	O
is	O
happy	O
to	O
finally	O
be	O
shaved	O
.	O
He	O
is	O
very	O
happy	O
to	O
have	O
ster	O
good	O
ride	O
of	O
his	O
life	O
.	O
He	O
is	O
glad	O
that	O
he	O
finally	O
shaved	O
.	O
He	O
is	O
glad	O
that	O
he	O
finally	O
has	O
chest	O
hair	O
.	O

Kate	O
was	O
wearing	O
big	O
over	O
the	O
ear	O
headphones	O
.	O
But	O
they	O
were	O
tight	O
and	O
squeezing	O
her	O
head	O
.	O
She	O
tried	O
to	O
adjust	O
them	O
to	O
relieve	O
the	O
tension	O
.	O
But	O
nothing	O
really	O
worked	O
.	O
Kate	O
decided	O
to	O
go	O
to	O
the	O
store	O
and	O
buy	O
some	O
ear	O
phones	O
.	O
She	O
decided	O
to	O
go	O
to	O
the	O
store	O
and	O
buy	O
some	O
headphones	O
instead	O
.	O
She	O
decided	O
to	O
go	O
to	O
the	O
store	O
and	O
buy	O
some	O
ear	O
phones	O
.	O
She	O
decided	O
to	O
go	O
buy	O
a	O
pair	O
of	O
headphones	O
instead	O
.	O
She	O
decided	O
to	O
go	O
to	O
the	O
store	O
and	O
buy	O
some	O
headphones	O
instead	O
.	O
She	O
decided	O
to	O
go	O
to	O
the	O
store	O
and	O
buy	O
some	O
headphones	O
instead	O
.	O
She	O
decided	O
to	O
go	O
to	O
the	O
doctor	O
and	O
have	O
some	O
rest	O
.	O
Kate	O
decided	O
to	O
go	O
to	O
the	O
store	O
and	O
buy	O
some	O
headphones	O
instead	O
.	O
She	O
decided	O
to	O
go	O
to	O
the	O
store	O
and	O
buy	O
some	O
ear	O
phones	O
.	O
She	O
decided	O
to	O
go	O
buy	O
some	O
ear	O
plugs	O
.	O
She	O
decided	O
to	O
go	O
buy	O
a	O
hat	O
so	O
she	O
could	O
enjoy	O
the	O
the	O
long	O
lin	O
Kate	O
decided	O
to	O
go	O
to	O
the	O
store	O
to	O
buy	O
some	O
candy	O
since	O
she	O
was	O
ti	O
Kate	O
decided	O
to	O
go	O
outside	O
to	O
rest	O
.	O
Kate	O
decided	O
to	O
go	O
to	O
the	O
store	O
and	O
buy	O
some	O
ran	O
phones	O
.	O
Kate	O
decided	O
to	O
go	O
to	O
the	O
store	O
and	O
buy	O
some	O
headphones	O
instead	O
.	O
Kate	O
decided	O
to	O
go	O
back	O
to	O
her	O
old	O
dishes	O
.	O
She	O
decided	O
to	O
go	O
buy	O
a	O
big	O
pair	O
of	O
headphones	O
instead	O
.	O
Kate	O
decided	O
to	O
go	O
to	O
the	O
store	O
and	O
wearing	O
some	O
headphones	O
.	O
She	O
was	O
forced	O
to	O
go	O
to	O
the	O
store	O
to	O
buy	O
some	O
cash	O
.	O
She	O
decided	O
to	O
go	O
to	O
the	O
store	O
and	O
buy	O
some	O
headphones	O
instead	O
.	O
She	O
decided	O
to	O
go	O
to	O
a	O
ess	O
instead	O
.	O
Kate	O
decided	O
to	O
go	O
to	O
the	O
store	O
and	O
buy	O
some	O
fake	O
headphones	O
.	O
She	O
decided	O
to	O
go	O
to	O
the	O
store	O
and	O
buy	O
some	O
ear	O
phones	O
.	O
She	O
decided	O
to	O
go	O
to	O
the	O
store	O
and	O
buy	O
some	O
headphones	O
instead	O
.	O
She	O
decided	O
to	O
go	O
to	O
the	O
store	O
and	O
buy	O
some	O
headphones	O
instead	O
.	O
She	O
decided	O
to	O
go	O
buy	O
some	O
ear	O
phones	O
.	O
Kate	O
decided	O
to	O
go	O
to	O
the	O
gym	O
and	O
use	O
some	O
saw	O
no	O
more	O
watching	O
T	O
Kate	O
decided	O
to	O
go	O
to	O
the	O
store	O
and	O
buy	O
some	O
cute	O
phones	O
.	O
She	O
decided	O
to	O
go	O
buy	O
a	O
fake	O
headphones	O
instead	O
.	O
She	O
decided	O
to	O
go	O
to	O
the	O
store	O
and	O
buy	O
some	O
concert	O
.	O

Response	O
set	O
(	O
high	O
content	O
diversity	O
)	O
Response	O
set	O
(	O
low	O
content	O
diversity	O
)	O
Sold	O
Out	O
Jane	O
wanted	O
to	O
watch	O
a	O
big	O
new	O
action	O
movie	O
.	O
She	O
had	O
been	O
waiting	O
a	O
long	O
time	O
for	O
it	O
to	O
come	O
out	O
.	O
When	O
tickets	O
became	O
available	O
she	O
was	O
too	O
busy	O
.	O
By	O
the	O
time	O
she	O
had	O
a	O
chance	O
to	O
buy	O
some	O
it	O
was	O
sold	O
out	O
.	O
Jane	O
cried	O
over	O
the	O
fact	O
that	O
she	O
could	O
n't	O
watch	O
it	O
and	O
just	O
gave	O
up	O
looking	O
for	O
a	O
ticket	O
.	O
Jane	O
decided	O
to	O
look	O
for	O
a	O
scalper	O
that	O
would	O
sell	O
her	O
the	O
ticket	O
for	O
the	O
movie	O
that	O
she	O
really	O
wanted	O
to	O
see	O
.	O
Jane	O
thought	O
it	O
was	O
okay	O
since	O
she	O
can	O
still	O
have	O
a	O
chance	O
to	O
watch	O
it	O
once	O
it	O
gets	O
uploaded	O
in	O
video	O
and	O
movie	O
streaming	O
applications	O
.	O
Jane	O
posted	O
a	O
status	O
on	O
her	O
social	O
media	O
accounts	O
asking	O
her	O
friends	O
for	O
any	O
spare	O
ticket	O
that	O
she	O
is	O
willing	O
to	O
buy	O
.	O
Jane	O
resorted	O
to	O
contacting	O
her	O
old	O
friend	O
who	O
is	O
working	O
at	O
a	O
huge	O
movie	O
theater	O
hoping	O
she	O
can	O
help	O
her	O
get	O
a	O
ticket	O
.	O
Jane	O
remembered	O
that	O
she	O
has	O
an	O
old	O
friend	O
who	O
is	O
a	O
manager	O
at	O
a	O
big	O
movie	O
theater	O
so	O
she	O
contacted	O
that	O
friend	O
in	O
the	O
hopes	O
that	O
she	O
can	O
buy	O
any	O
spare	O
ticket	O
.	O
Desperate	O
to	O
watch	O
the	O
movie	O
,	O
Jane	O
called	O
her	O
friend	O
,	O
who	O
works	O
at	O
a	O
movie	O
theater	O
,	O
asking	O
for	O
a	O
ticket	O
to	O
that	O
movie	O
.	O
Jane	O
recalled	O
that	O
her	O
friend	O
works	O
at	O
a	O
movie	O
theater	O
and	O
hoped	O
that	O
she	O
can	O
help	O
get	O
a	O
ticket	O
for	O
that	O
movie	O
.	O
Jane	O
decided	O
to	O
look	O
for	O
her	O
friend	O
who	O
could	O
possibly	O
have	O
access	O
to	O
tickets	O
for	O
that	O
movie	O
since	O
that	O
friend	O
currently	O
works	O
at	O
a	O
movie	O
theater	O
.	O
Jane	O
realized	O
that	O
her	O
friend	O
might	O
have	O
spare	O
tickets	O
since	O
she	O
is	O
a	O
manager	O
of	O
a	O
movie	O
theater	O
showing	O
that	O
film	O
.	O

My	O
friend	O
has	O
some	O
beavers	O
in	O
his	O
backyard	O
.	O
They	O
come	O
up	O
from	O
the	O
creek	O
by	O
his	O
house	O
.	O
He	O
invites	O
my	O
over	O
and	O
we	O
watch	O
them	O
.	O
We	O
take	O
pictures	O
of	O
them	O
and	O
send	O
them	O
to	O
our	O
friends	O
.	O
They	O
are	O
fascinating	O
animals	O
.	O
Our	O
friends	O
love	O
getting	O
the	O
pictures	O
.	O
Sometimes	O
his	O
dogs	O
chase	O
them	O
.	O
They	O
are	O
building	O
a	O
dam	O
on	O
the	O
creek	O
.	O
They	O
wo	O
n't	O
let	O
us	O
get	O
too	O
close	O
to	O
them	O
.	O
They	O
are	O
busy	O
gathering	O
sticks	O
to	O
make	O
a	O
dam	O
.	O
The	O
dam	O
they	O
are	O
building	O
is	O
almost	O
complete	O
.	O
It	O
's	O
fascinating	O
to	O
see	O
their	O
workmanship	O
building	O
a	O
dam	O
.	O
They	O
are	O
turning	O
the	O
creek	O
into	O
a	O
pond	O
by	O
building	O
a	O
dam	O
.	O
They	O
all	O
work	O
together	O
with	O
careful	O
engineering	O
to	O
build	O
a	O
dam	O
.	O
I	O
just	O
got	O
into	O
this	O
show	O
and	O
ca	O
n't	O
stop	O
watching	O
places	O
apple	O
slices	O
in	O
a	O
bowl	O
so	O
they	O
'll	O
stay	O
fresh	O
Oh	O
boy	O
,	O
I	O
love	O
apples	O
.	O
I	O
do	O
n't	O
need	O
you	O
telling	O
me	O
how	O
to	O
keep	O
things	O
fresh	O
,	O
take	O
a	O
hike	O
.	O
Girl	O
,	O
you	O
're	O
the	O
fresh	O
one	O
around	O
here	O
.	O
This	O
post	O
might	O
be	O
better	O
in	O
the	O
life	O
hacks	O
section	O
.	O
This	O
is	O
actually	O
a	O
useful	O
bit	O
of	O
advice	O
.	O
I	O
find	O
merit	O
in	O
this	O
input	O
.	O
That	O
information	O
will	O
serve	O
me	O
well	O
.	O
Thanks	O
,	O
that	O
's	O
really	O
good	O
to	O
know	O
!	O
Such	O
knowledge	O
is	O
certainly	O
beneficial	O
.	O
Wise	O
words	O
,	O
I	O
will	O
heed	O
them	O
.	O

For	O
each	O
question	O
type	O
,	O
we	O
select	O
an	O
object	O
in	O
the	O
image	O
scene	O
graph	O
,	O
and	O
update	O
the	O
question	O
by	O
substituting	O
the	O
reference	O
to	O
this	O
object	O
by	O
another	O
object	O
.	O
When	O
substituting	O
one	O
object	O
by	O
another	O
,	O
we	O
need	O
to	O
adjust	O
the	O
question	O
to	O
keep	O
it	O
fluent	O
.	O
Table	O
10	O
shows	O
the	O
specific	O
linguistic	O
rules	O
we	O
verify	O
when	O
performing	O
this	O
substitution	O
.	O
A.4	O
Annotation	O
Task	O
for	O
Verifying	O
Generated	O
Contrast	O
Sets	O
Fig	O
.	O
3	O
shows	O
the	O
annotation	O
task	O
that	O
is	O
shown	O
to	O
Turkers	O
to	O
validate	O
the	O
QA	O
pairs	O
generated	O
by	O
our	O
method	O
.	O

Singular	O
vs.	O
plural	O
If	O
the	O
noun	O
is	O
singular	O
and	O
countable	O
:	O
add	O
"	O
a	O
"	O
or	O
"	O
an	O
"	O
If	O
needed	O
,	O
replace	O
"	O
Are	O
"	O
and	O
"	O
Is	O
"	O
"	O
a	O
fence	O
"	O
,	O
"	O
men	O
"	O
"	O
a	O
boy	O
"	O
,	O
"	O
an	O
elephant	O
"	O
Definite	O
vs.	O
indefinite	O
Do	O
not	O
change	O
definite	O
articles	O
to	O
indefinite	O
articles	O
,	O
and	O
vice	O
versa	O
"	O
is	O
there	O
any	O
fence	O
near	O
the	O
boy	O
"	O
suggests	O
that	O
there	O
is	O
a	O
boy	O
in	O
the	O
scene	O
graph	O
,	O
which	O
is	O
not	O
always	O
correct	O

Meaning	O
can	O
be	O
changed	O
When	O
replacing	O
to	O
general	O
or	O
specific	O
terms	O
"	O
Cats	O
in	O
the	O
image	O
"	O
=	O
>	O
"	O
Animals	O
in	O
the	O
image	O
"	O
,	O
"	O
Animals	O
not	O
in	O
the	O
image	O
"	O
=	O
>	O
"	O
cats	O
not	O
in	O
the	O
image	O
"	O
,	O
The	O
opposite	O
directions	O
not	O
necessarily	O
holds	O
Countable	O
vs.	O
uncountable	O
If	O
the	O
noun	O
is	O
uncountable	O
,	O
do	O
not	O
add	O
"	O
a	O
"	O
or	O
"	O
an	O
"	O
"	O
A	O
cat	O
"	O
,	O
"	O
water	O
"	O
Table	O
10	O
:	O
Partial	O
linguistic	O
rules	O
to	O
notice	O
using	O
our	O
method	O
.	O

Designing	O
Algorithms	O
for	O
Referring	O
with	O
Proper	O
Names	O

Standard	O
algorithms	O
for	O
attribute	O
choice	O
in	O
the	O
generation	O
of	O
referring	O
expressions	O
have	O
little	O
to	O
say	O
about	O
the	O
role	O
of	O
Proper	O
Names	O
in	O
referring	O
expressions	O
.	O
We	O
discuss	O
the	O
implications	O
of	O
letting	O
these	O
algorithms	O
produce	O
Proper	O
Names	O
and	O
expressions	O
that	O
have	O
Proper	O
Names	O
as	O
parts	O
.	O

Reference	O
-	O
the	O
production	O
and	O
comprehension	O
of	O
referring	O
expressions	O
-	O
has	O
been	O
studied	O
intensively	O
throughout	O
the	O
cognitive	O
sciences	O
.	O
Computational	O
Linguists	O
are	O
no	O
exception	O
,	O
often	O
paying	O
particular	O
attention	O
to	O
the	O
generation	O
of	O
referring	O
expressions	O
(	O
REs	O
,	O
(	O
Krahmer	O
and	O
Van	O
Deemter	O
,	O
2012	O
)	O
for	O
a	O
survey	O
)	O
.	O
This	O
area	O
of	O
Natural	O
Language	O
Generation	O
is	O
known	O
as	O
Referring	O
Expressions	O
Generation	O
(	O
REG	O
)	O
.	O
An	O
important	O
strand	O
of	O
REG	O
focusses	O
on	O
"	O
one	O
-	O
shot	O
"	O
REs	O
,	O
which	O
do	O
not	O
rely	O
on	O
any	O
linguistic	O
context	O
(	O
precluding	O
anaphoric	O
and	O
other	O
attenuated	O
REs	O
)	O
;	O
these	O
are	O
also	O
the	O
primary	O
focus	O
of	O
this	O
paper	O
.	O
1	O
One	O
of	O
the	O
classic	O
algorithm	O
coming	O
out	O
or	O
REG	O
is	O
the	O
Incremental	O
Algorithm	O
(	O
IA	O
)	O
(	O
Dale	O
and	O
Reiter	O
,	O
1996	O
)	O
.	O
Simplifying	O
slightly	O
,	O
the	O
IA	O
starts	O
by	O
ordering	O
properties	O
in	O
a	O
sequence	O
known	O
as	O
the	O
Preference	O
Order	O
.	O
The	O
algorithm	O
starts	O
with	O
an	O
empty	O
RE	O
,	O
then	O
examines	O
the	O
first	O
property	O
from	O
the	O
Preference	O
Order	O
.	O
If	O
this	O
property	O
is	O
true	O
of	O
the	O
referent	O
r	O
and	O
rules	O
out	O
one	O
or	O
more	O
distractors	O
,	O
it	O
is	O
added	O
to	O
the	O
RE	O
;	O
otherwise	O
it	O
is	O
not	O
added	O
,	O
and	O
the	O
next	O
property	O
in	O
the	O
Preference	O
Order	O
is	O
examined	O
.	O
The	O
algorithm	O
terminates	O
when	O
properties	O
P	O
i	O
1	O
,	O
..	O
,	O
P	O
i	O
k	O
have	O
1	O
See	O
,	O
however	O
,	O
section	O
2.1	O
on	O
the	O
use	O
of	O
salience	O
.	O
been	O
selected	O
that	O
jointly	O
identify	O
the	O
referent	O
(	O
i.e.	O
,	O
[	O
[	O
P	O
i	O
1	O
]	O
]	O
∩	O
...	O
∩	O
[	O
[	O
P	O
i	O
k	O
]	O
]	O
=	O
{	O
r	O
}	O
)	O
.	O
Different	O
Preference	O
Orders	O
tend	O
to	O
generate	O
different	O
REs	O
,	O
so	O
finding	O
a	O
good	O
one	O
is	O
important	O
.	O
Proper	O
Names	O
(	O
PNs	O
)	O
are	O
among	O
the	O
most	O
widely	O
studied	O
REs	O
in	O
cognitive	O
science	O
(	O
see	O
e.g.	O
,	O
(	O
van	O
Langendonck	O
,	O
2007	O
)	O
,	O
passim	O
;	O
(	O
van	O
Deemter	O
,	O
2016	O
)	O
,	O
chapters	O
2	O
and	O
7	O
)	O
,	O
and	O
a	O
crucial	O
area	O
of	O
applied	O
work	O
in	O
Information	O
Extraction	O
(	O
e.g.	O
,	O
(	O
Jurafsky	O
and	O
Martin	O
,	O
2009	O
)	O
chapter	O
22	O
on	O
Named	O
Entities	O
)	O
.	O
Yet	O
REG	O
2	O
has	O
neglected	O
PNs	O
,	O
presumably	O
because	O
names	O
could	O
easily	O
trivialise	O
REG	O
:	O
suppose	O
the	O
KB	O
contained	O
a	O
set	O
of	O
people	O
.	O
If	O
only	O
one	O
of	O
the	O
people	O
in	O
the	O
KB	O
is	O
named	O
Obama	O
,	O
then	O
it	O
is	O
easy	O
to	O
identify	O
him	O
,	O
by	O
referring	O
to	O
him	O
by	O
his	O
name	O
.	O
Since	O
PNs	O
tend	O
to	O
make	O
excellent	O
REs	O
,	O
REG	O
would	O
become	O
trivial	O
-	O
so	O
the	O
presumed	O
argument	O
goes	O
.	O
We	O
argue	O
that	O
this	O
line	O
of	O
reasoning	O
misses	O
some	O
important	O
points	O
and	O
that	O
PNs	O
deserve	O
more	O
attention	O
from	O
researchers	O
in	O
REG	O
.	O

Observe	O
that	O
:	O
-	O
Name	O
are	O
often	O
ambiguous	O
.	O
"	O
Obama	O
"	O
,	O
for	O
instance	O
(	O
not	O
to	O
mention	O
"	O
Smith	O
"	O
)	O
could	O
refer	O
to	O
many	O
different	O
people	O
.	O
-	O
A	O
referent	O
can	O
have	O
many	O
names	O
(	O
"	O
Barack	O
"	O
,	O
"	O
Obama	O
"	O
,	O
"	O
Barack	O
Obama	O
"	O
,	O
etc	O
.	O
)	O
or	O
none	O
.	O
-	O
A	O
name	O
can	O
combine	O
with	O
other	O
properties	O
and	O
epithets	O
,	O
as	O
in	O
"	O
Mr	O
Barack	O
Obama	O
,	O
America	O
's	O
current	O
president	O
"	O
.	O
-	O
A	O
name	O
can	O
be	O
part	O
of	O
an	O
expression	O
that	O
refers	O
to	O
another	O
referent	O
.	O
The	O
process	O
is	O
recursive	O
,	O
e.g.	O
,	O
"	O
The	O
height	O
of	O
the	O
income	O
of	O
Obama	O
's	O
Secretary	O
of	O
State	O
"	O
.	O
So	O
how	O
might	O
PNs	O
be	O
given	O
a	O
place	O
in	O
REG	O
?	O

This	O
approach	O
works	O
,	O
but	O
it	O
puts	O
a	O
spotlight	O
on	O
some	O
difficult	O
issues	O
,	O
some	O
of	O
which	O
affect	O
the	O
generation	O
of	O
descriptive	O
REs	O
as	O
well	O
:	O
1	O
.	O
PNs	O
are	O
not	O
always	O
preferred	O
.	O
For	O
example	O
,	O
if	O
the	O
Director	O
of	O
Taxes	O
is	O
Mrs	O
X	O
,	O
this	O
does	O
not	O
mean	O
that	O
"	O
Contact	O
the	O
Director	O
of	O
Taxes	O
"	O
is	O
always	O
better	O
worded	O
as	O
"	O
Contact	O
Mrs	O
X	O
"	O
,	O
since	O
her	O
job	O
title	O
may	O
be	O
relevant	O
.	O
The	O
lack	O
of	O
a	O
computational	O
theory	O
of	O
relevance	O
affects	O
all	O
of	O
REG	O
but	O
becomes	O
very	O
noticeable	O
in	O
the	O
choice	O
between	O
PNs	O
and	O
descriptions	O
.	O
2	O
.	O
There	O
is	O
no	O
reason	O
for	O
limiting	O
reification	O
to	O
PNs	O
.	O
Colours	O
too	O
could	O
be	O
reified	O
,	O
for	O
example	O
,	O
to	O
generate	O
"	O
the	O
colour	O
of	O
grass	O
"	O
.	O
The	O
traditional	O
dichotomy	O
between	O
objects	O
and	O
properties	O
limits	O
the	O
range	O
of	O
REs	O
that	O
these	O
algorithms	O
can	O
generate	O
.	O
3	O
.	O
REG	O
algorithms	O
are	O
ignorant	O
about	O
social	O
relations	O
between	O
speaker	O
,	O
hearer	O
,	O
and	O
referent	O
.	O
Consider	O
a	O
couple	O
with	O
a	O
son	O
and	O
a	O
daughter	O
.	O
Speaking	O
to	O
his	O
mother	O
,	O
the	O
son	O
could	O
say	O
"	O
my	O
sister	O
"	O
,	O
"	O
your	O
daughter	O
"	O
,	O
etc	O
.	O
,	O
yet	O
in	O
most	O
situations	O
a	O
PN	O
would	O
be	O
better	O
.	O
Titles	O
and	O
epithets	O
like	O
"	O
Dr	O
"	O
and	O
"	O
Aunt	O
(	O
y	O
)	O
"	O
,	O
complicate	O
matters	O
further	O
.	O
4	O
.	O
As	O
elsewhere	O
in	O
REG	O
,	O
questions	O
about	O
overspecification	O
need	O
to	O
be	O
faced	O
.	O
When	O
,	O
for	O
example	O
,	O
is	O
it	O
useful	O
to	O
add	O
an	O
appositive	O
to	O
a	O
PN	O
,	O
as	O
in	O
"	O
Mr	O
Barack	O
Obama	O
,	O
America	O
's	O
current	O
president	O
"	O
?	O
Furthermore	O
,	O
Linguistic	O
Realisation	O
will	O
have	O
to	O
decide	O
about	O
the	O
surface	O
order	O
of	O
the	O
PN	O
and	O
the	O
appositive	O
,	O
perhaps	O
depending	O
on	O
whether	O
the	O
PN	O
and/or	O
the	O
appositive	O
(	O
by	O
itself	O
)	O
refers	O
uniquely	O
.	O
5	O
.	O
If	O
PNs	O
are	O
properties	O
of	O
the	O
referent	O
,	O
then	O
this	O
leaves	O
room	O
for	O
expressing	O
one	O
and	O
the	O
same	O
PN	O
with	O
a	O
different	O
string	O
.	O
(	O
For	O
example	O
,	O
"	O
Doctor	O
"	O
may	O
be	O
worded	O
as	O
"	O
Doctor	O
"	O
,	O
"	O
Dr.	O
"	O
,	O
or	O
"	O
Dr	O
"	O
.	O
)	O
The	O
desirability	O
of	O
this	O
use	O
of	O
Linguistic	O
Realisation	O
would	O
need	O
to	O
be	O
investigated	O
.	O
6	O
.	O
It	O
is	O
often	O
difficult	O
for	O
the	O
speaker	O
to	O
assess	O
whether	O
the	O
hearer	O
knows	O
who	O
a	O
given	O
PN	O
refers	O
to	O
.	O
The	O
hearer	O
may	O
never	O
have	O
heard	O
of	O
Joe	O
Klein	O
,	O
for	O
example	O
,	O
and	O
this	O
would	O
cause	O
the	O
RE	O
"	O
Joe	O
Klein	O
"	O
to	O
mis	O
-	O
fire	O
.	O
Lack	O
of	O
shared	O
knowledge	O
is	O
a	O
problem	O
for	O
descriptive	O
REs	O
as	O
well	O
,	O
but	O
it	O
is	O
exacerbated	O
in	O
the	O
case	O
of	O
PNs	O
,	O
because	O
names	O
are	O
highly	O
conventional	O
:	O
once	O
I	O
've	O
learned	O
what	O
"	O
red	O
"	O
means	O
,	O
I	O
can	O
apply	O
the	O
word	O
to	O
any	O
red	O
object	O
,	O
but	O
learning	O
your	O
name	O
does	O
not	O
teach	O
me	O
to	O
apply	O
this	O
name	O
to	O
anyone	O
else	O
.	O
The	O
last	O
point	O
has	O
important	O
implications	O
.	O
Imagine	O
a	O
programmer	O
wanting	O
to	O
implement	O
the	O
algorithm	O
of	O
section	O
2.1	O
,	O
aiming	O
to	O
mimic	O
human	O
language	O
use	O
.	O
If	O
she	O
decides	O
to	O
implement	O
an	O
Incremental	O
Algorithm	O
,	O
then	O
how	O
to	O
choose	O
its	O
free	O
pa	O
-	O
rameter	O
,	O
the	O
Preference	O
Order	O
?	O
She	O
could	O
learn	O
one	O
via	O
an	O
elicitation	O
experiment	O
,	O
but	O
how	O
does	O
she	O
find	O
a	O
generic	O
REG	O
algorithm	O
that	O
works	O
for	O
all	O
PNs	O
?	O
Consider	O
a	O
scene	O
from	O
an	O
experiment	O
where	O
speakers	O
referred	O
to	O
stimuli	O
on	O
a	O
screen	O
.	O
Participants	O
called	O
the	O
man	O
in	O
the	O
top	O
right	O
"	O
the	O
man	O
with	O
the	O
white	O
beard	O
"	O
,	O
etc	O
.	O
They	O
might	O
have	O
said	O
"	O
Samuel	O
Eilenberg	O
"	O
,	O
yet	O
noone	O
did	O
,	O
because	O
participants	O
did	O
n't	O
know	O
his	O
name	O
.	O
Participants	O
could	O
have	O
been	O
trained	O
to	O
be	O
familiar	O
with	O
every	O
individual	O
's	O
name	O
,	O
but	O
this	O
could	O
easily	O
have	O
primed	O
the	O
use	O
of	O
names	O
at	O
the	O
expense	O
of	O
descriptions	O
;	O
the	O
same	O
happens	O
when	O
names	O
are	O
visible	O
as	O
captions	O
,	O
as	O
was	O
done	O
in	O
(	O
de	O
Oliveira	O
et	O
al	O
,	O
2015	O
)	O
using	O
fictitious	O
names	O
of	O
geographical	O
areas	O
;	O
see	O
also	O
(	O
Anderson	O
et	O
al	O
,	O
1991	O
)	O
.	O
Such	O
an	O
approach	O
does	O
not	O
give	O
reliable	O
information	O
on	O
how	O
REG	O
algorithms	O
should	O
choose	O
between	O
PNs	O
and	O
descriptions	O
.	O
The	O
problem	O
is	O
not	O
just	O
that	O
PNs	O
are	O
conventional	O
,	O
but	O
that	O
their	O
conventional	O
meaning	O
can	O
be	O
entrenched	O
to	O
different	O
degrees	O
,	O
varying	O
from	O
shortlived	O
"	O
conceptual	O
pacts	O
"	O
(	O
Brennan	O
and	O
Clark	O
,	O
1996	O
)	O
to	O
names	O
that	O
are	O
very	O
widely	O
known	O
and	O
used	O
.	O

Suppose	O
someone	O
asks	O
"	O
Who	O
is	O
Joe	O
Klein	O
?	O
"	O
(	O
cf	O
.	O
,	O
section	O
2.2	O
,	O
point	O
6	O
)	O
.	O
Would	O
it	O
make	O
sense	O
to	O
respond	O
"	O
(	O
He	O
is	O
)	O
the	O
author	O
of	O
the	O
bestselling	O
political	O
novel	O
of	O
the	O
1990s	O
?	O
"	O
It	O
depends	O
on	O
the	O
importance	O
of	O
this	O
fact	O
and	O
how	O
widely	O
it	O
is	O
known	O
.	O
To	O
model	O
answers	O
to	O
"	O
Who	O
is	O
?	O
"	O
questions	O
(	O
see	O
(	O
Boër	O
and	O
Lycan	O
,	O
1986	O
)	O
for	O
a	O
theoretical	O
study	O
)	O
,	O
(	O
Kutlak	O
et	O
al	O
,	O
2013	O
)	O
designed	O
a	O
REG	O
algorithm	O
that	O
employs	O
the	O
following	O
Heuristic	O
:	O
Based	O
on	O
the	O
frequency	O
with	O
which	O
a	O
name	O
n	O
co	O
-	O
occurs	O
with	O
a	O
property	O
P	O
,	O
the	O
Heuristic	O
estimates	O
how	O
likely	O
the	O
proposition	O
P	O
(	O
n	O
)	O
is	O
to	O
be	O
known	O
by	O
an	O
arbitrarily	O
chosen	O
hearer	O
.	O
Evaluation	O
studies	O
suggest	O
that	O
this	O
Heuristic	O
goes	O
a	O
long	O
way	O
towards	O
estimating	O
how	O
many	O
people	O
know	O
a	O
fact	O
,	O
and	O
the	O
complete	O
REG	O
algorithm	O
(	O
which	O
involves	O
2	O
other	O
heuristics	O
)	O
outperforms	O
its	O
competitors	O
in	O
terms	O
of	O
its	O
ability	O
to	O
generate	O
descriptions	O
that	O
allow	O
hearers	O
to	O
guess	O
correctly	O
the	O
name	O
of	O
the	O
referent	O
.	O
Although	O
the	O
authors	O
focussed	O
on	O
the	O
WWW	O
,	O
the	O
approach	O
can	O
use	O
any	O
corpus	O
that	O
represents	O
the	O
ideas	O
of	O
a	O
community	O
(	O
e.g.	O
,	O
a	O
company	O
's	O
intranet	O
)	O
.	O
This	O
approach	O
suggests	O
a	O
promising	O
handle	O
on	O
the	O
conventionality	O
of	O
PNs	O
.	O
It	O
allows	O
us	O
to	O
estimate	O
,	O
for	O
example	O
,	O
the	O
likelihood	O
that	O
a	O
name	O
like	O
"	O
Joe	O
Klein	O
"	O
is	O
known	O
by	O
hearers	O
to	O
refer	O
to	O
the	O
commentator	O
and	O
novelist	O
of	O
that	O
name	O
,	O
and	O
this	O
would	O
allow	O
us	O
to	O
limit	O
the	O
KB	O
of	O
section	O
2	O
to	O
names	O
that	O
are	O
well	O
enough	O
known	O
.	O
We	O
hypothesise	O
that	O
PNs	O
have	O
a	O
higher	O
likelihood	O
of	O
being	O
uttered	O
as	O
part	O
of	O
REs	O
by	O
members	O
of	O
a	O
community	O
(	O
e.g.	O
,	O
users	O
of	O
the	O
WWW	O
)	O
the	O
more	O
frequently	O
these	O
PNs	O
occur	O
as	O
names	O
of	O
this	O
referent	O
in	O
documents	O
produced	O
by	O
that	O
community	O
.	O
Further	O
experiments	O
could	O
flesh	O
out	O
how	O
the	O
use	O
of	O
PNs	O
depends	O
on	O
a	O
number	O
of	O
factors	O
,	O
including	O
the	O
Knowledge	O
Heuristic	O
.	O
Essentially	O
,	O
PNs	O
would	O
be	O
treated	O
as	O
properties	O
of	O
a	O
referent	O
that	O
may	O
or	O
may	O
not	O
be	O
known	O
to	O
the	O
hearer	O
,	O
analogous	O
to	O
the	O
descriptive	O
properties	O
of	O
(	O
Kutlak	O
et	O
al	O
,	O
2013	O
)	O
.	O

We	O
have	O
shown	O
how	O
,	O
given	O
appropriate	O
semantic	O
representations	O
,	O
standard	O
attribute	O
algorithms	O
are	O
able	O
to	O
generate	O
REs	O
that	O
contain	O
PNs	O
,	O
thereby	O
solving	O
problems	O
with	O
the	O
standard	O
2	O
-	O
step	O
perspective	O
on	O
REG	O
that	O
separates	O
choosing	O
the	O
general	O
syntactic	O
type	O
of	O
RE	O
from	O
more	O
fine	O
-	O
grained	O
decisions	O
about	O
the	O
content	O
of	O
the	O
RE	O
.	O
However	O
,	O
our	O
approach	O
raises	O
difficult	O
questions	O
about	O
the	O
choices	O
that	O
a	O
REG	O
algorithm	O
needs	O
to	O
make	O
between	O
PNs	O
and	O
descriptive	O
REs	O
.	O
We	O
argue	O
that	O
some	O
of	O
the	O
trickiest	O
questions	O
in	O
this	O
area	O
may	O
be	O
solved	O
if	O
large	O
corpora	O
are	O
employed	O
as	O
a	O
source	O
of	O
insight	O
into	O
the	O
degree	O
to	O
which	O
a	O
PN	O
is	O
likely	O
to	O
be	O
known	O
by	O
the	O
recipient	O
of	O
the	O
RE	O
.	O

We	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
valuable	O
feedback	O
and	O
helpful	O
suggestions	O
.	O

Here	O
,	O
we	O
discuss	O
our	O
rationale	O
behind	O
reducing	O
the	O
student	O
vocabulary	O
size	O
and	O
its	O
challenges	O
,	O
followed	O
by	O
our	O
mixed	O
-	O
vocabulary	O
distillation	O
approach	O
.	O

We	O
propose	O
a	O
two	O
-	O
stage	O
approach	O
for	O
implicit	O
transfer	O
of	O
knowledge	O
to	O
the	O
student	O
via	O
the	O
student	O
embeddings	O
,	O
as	O
described	O
below	O
.	O

Dialogue	O
-	O
act	O
-	O
driven	O
Conversation	O
Model	O
:	O
An	O
Experimental	O
Study	O

In	O
this	O
section	O
,	O
we	O
provide	O
details	O
of	O
several	O
existing	O
models	O
that	O
we	O
will	O
use	O
to	O
validate	O
our	O
hypothesis	O
.	O
These	O
models	O
include	O
generative	O
models	O
(	O
such	O
as	O
encoder	O
-	O
decoder	O
model	O
and	O
its	O
hierarchical	O
version	O
i.e.	O
,	O
hierarchical	O
encoder	O
-	O
decoder	O
)	O
and	O
discriminative	O
model	O
(	O
Siamese	O
-	O
based	O
model	O
)	O
.	O
Next	O
,	O
we	O
provide	O
details	O
of	O
the	O
proposed	O
model	O
that	O
adds	O
the	O
hierarchical	O
structure	O
to	O
the	O
Siamese	O
model	O
along	O
with	O
the	O
dialogue	O
act	O
information	O
.	O
To	O
set	O
the	O
notations	O
,	O
we	O
are	O
given	O
a	O
set	O
D	O
of	O
N	O
conversations	O
,	O
i.e.	O
D	O
=	O
(	O
C	O
1	O
,	O
C	O
2	O
,	O
.	O
.	O
.	O
C	O
N	O
)	O
,	O
with	O
each	O
conversation	O
C	O
i	O
being	O
a	O
sequence	O
of	O
R	O
i	O
utterances	O
,	O
C	O
i	O
=	O
(	O
u	O
1	O
,	O
u	O
2	O
,	O
.	O
.	O
.	O
u	O
R	O
i	O
)	O
.	O
Each	O
utterance	O
u	O
j	O
in	O
turn	O
is	O
itself	O
a	O
sequence	O
of	O
S	O
j	O
words	O
,	O
i.e.	O
u	O
j	O
=	O
(	O
w	O
1	O
,	O
w	O
2	O
,	O
.	O
.	O
.	O
w	O
S	O
j	O
)	O
.	O

Generative	O
models	O
are	O
the	O
most	O
widely	O
used	O
models	O
for	O
conversation	O
modeling	O
.	O
These	O
models	O
include	O
encoder	O
-	O
decoder	O
model	O
and	O
hierarchical	O
encoder	O
-	O
decoder	O
model	O
.	O

A	O
decoder	O
in	O
the	O
encoder	O
-	O
decoder	O
model	O
generates	O
the	O
next	O
word	O
given	O
the	O
context	O
,	O
and	O
though	O
it	O
has	O
several	O
valid	O
and	O
reasonable	O
choices	O
,	O
it	O
is	O
burdened	O
with	O
the	O
task	O
of	O
generating	O
exactly	O
a	O
particular	O
choice	O
that	O
matches	O
the	O
ground	O
truth	O
.	O
For	O
example	O
,	O
for	O
a	O
context	O
I	O
am	O
enjoying	O
the	O
day	O
,	O
it	O
is	O
warm	O
and	O
sunny	O
,	O
if	O
decoder	O
generates	O
yes	O
,	O
it	O
is	O
.	O
and	O
the	O
ground	O
truth	O
dictates	O
yes	O
,	O
indeed	O
,	O
it	O
is	O
a	O
lovely	O
day	O
,	O
the	O
decoder	O
has	O
failed	O
,	O
though	O
it	O
is	O
a	O
valid	O
response	O
.	O
Due	O
to	O
these	O
challenges	O
with	O
generative	O
models	O
,	O
discriminative	O
models	O
are	O
trained	O
directly	O
to	O
discriminate	O
between	O
positive	O
and	O
negative	O
utterances	O
.	O
A	O
typical	O
discriminative	O
model	O
,	O
or	O
in	O
particular	O
Siamese	O
model	O
,	O
consists	O
of	O
two	O
encoders	O
,	O
one	O
encoder	O
encoding	O
the	O
context	O
,	O
while	O
another	O
encoding	O
the	O
candidate	O
utterance	O
,	O
i.e	O
utterance	O
K	O
+	O
1	O
.	O
These	O
two	O
representations	O
are	O
passed	O
to	O
a	O
final	O
layer	O
that	O
computes	O
the	O
probability	O
of	O
candidate	O
being	O
a	O
valid	O
response	O
given	O
the	O
context	O
.	O
Let	O
h	O
(	O
1	O
)	O
and	O
h	O
(	O
2	O
)	O
be	O
the	O
representations	O
obtained	O
from	O
the	O
first	O
encoder	O
and	O
second	O
encoder	O
,	O
respectively	O
,	O
then	O
the	O
probability	O
of	O
their	O
association	O
can	O
be	O
computed	O
using	O
the	O
following	O
expression	O
.	O
p	O
(	O
s	O
|	O
h	O
(	O
1	O
)	O
,	O
h	O
(	O
2	O
)	O
)	O
=	O
σ	O
h	O
(	O
1	O
)	O
T	O
Ah	O
(	O
2	O
)	O
+	O
b	O
(	O
3	O
)	O
where	O
,	O
the	O
bias	O
b	O
and	O
matrix	O
A	O
are	O
learned	O
model	O
parameters	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
the	O
details	O
of	O
the	O
experiments	O
,	O
i.e.	O
dataset	O
and	O
its	O
preparation	O
,	O
baseline	O
models	O
,	O
experimental	O
setup	O
,	O
and	O
analysis	O
of	O
results	O
.	O

Here	O
we	O
list	O
the	O
baseline	O
models	O
,	O
their	O
modified	O
version	O
enhanced	O
with	O
dialogue	O
act	O
information	O
,	O
and	O
the	O
proposed	O
model	O
.	O

Siamese	O
-	O
Also	O
known	O
as	O
Dual	O
-	O
Encoder	O
,	O
it	O
uses	O
two	O
encoders	O
(	O
both	O
utterance	O
encoders	O
)	O
with	O
shared	O
weights	O
,	O
to	O
produce	O
the	O
representation	O
for	O
the	O
K	O
utterances	O
and	O
the	O
(	O
K	O
+	O
1	O
)	O
utterance	O
.	O
HSiamese	O
-	O
A	O
Hierarchical	O
version	O
of	O
the	O
Siamese	O
model	O
that	O
uses	O
a	O
hierarchical	O
encoder	O
to	O
produce	O
a	O
representation	O
for	O
the	O
K	O
utterances	O
,	O
and	O
a	O
plain	O
encoder	O
(	O
utterance	O
encoder	O
)	O
to	O
produce	O
a	O
representation	O
for	O
the	O
(	O
K	O
+	O
1	O
)	O
utterance	O
.	O
Siamese	O
-	O
DA	O
-	O
An	O
extension	O
of	O
Siamese	O
model	O
that	O
uses	O
the	O
additional	O
dialogue	O
act	O
information	O
obtained	O
through	O
DA	O
-	O
encoder	O
.	O
The	O
representation	O
obtained	O
from	O
the	O
DA	O
-	O
encoder	O
is	O
linearly	O
combined	O
with	O
the	O
representation	O
of	O
the	O
K	O
utterances	O
obtained	O
from	O
an	O
utterance	O
encoder	O
.	O
HSiamese	O
-	O
DA	O
-	O
The	O
proposed	O
model	O
uses	O
a	O
Hierarchical	O
Encoder	O
and	O
a	O
DA	O
-	O
Encoder	O
.	O
The	O
representation	O
obtained	O
from	O
the	O
DA	O
-	O
Encoder	O
is	O
linearly	O
combined	O
with	O
the	O
representation	O
obtained	O
from	O
the	O
hierarchical	O
encoder	O
.	O

In	O
this	O
section	O
,	O
we	O
present	O
results	O
of	O
our	O
experimental	O
study	O
,	O
followed	O
by	O
its	O
analysis	O
.	O

ZCU	O
-	O
NLP	O
at	O
MADAR	O
2019	O
:	O
Recognizing	O
Arabic	O
Dialects	O

The	O
Madar	O
shared	O
tasks	O
(	O
Bouamor	O
et	O
al	O
,	O
2019	O
)	O
are	O
a	O
follow	O
-	O
up	O
to	O
Salameh	O
's	O
work	O
with	O
the	O
synthetic	O
corpus	O
of	O
Bouamor	O
(	O
Bouamor	O
et	O
al	O
,	O
2014	O
)	O
and	O
Salameh	O
's	O
work	O
with	O
tweets	O
based	O
on	O
the	O
corpus	O
.	O
Two	O
corpora	O
are	O
provided	O
,	O
a	O
six	O
-	O
city	O
corpus	O
of	O
travel	O
sentences	O
rendered	O
into	O
the	O
dialects	O
of	O
five	O
cities	O
and	O
MSA	O
1	O
,	O
and	O
a	O
25	O
-	O
city	O
+	O
MSA	O
corpus	O
using	O
a	O
smaller	O
number	O
of	O
sentences	O
.	O
In	O
the	O
first	O
task	O
,	O
test	O
data	O
is	O
classified	O
as	O
one	O
of	O
the	O
25	O
cities	O
or	O
MSA	O
.	O
For	O
the	O
second	O
task	O
,	O
the	O
organizers	O
chose	O
training	O
,	O
development	O
and	O
test	O
tweet	O
-	O
sets	O
for	O
download	O
from	O
Twitter	O
.	O
The	O
tweets	O
are	O
from	O
21	O
Arabic	O
countries	O
,	O
and	O
the	O
goal	O
is	O
to	O
determine	O
,	O
for	O
each	O
tweet	O
author	O
,	O
the	O
country	O
of	O
origin	O
.	O
For	O
S	O
-	O
1	O
we	O
did	O
not	O
use	O
any	O
external	O
data	O
,	O
only	O
data	O
provided	O
by	O
the	O
shared	O
task	O
organizers	O
.	O
1	O

The	O
organizers	O
provided	O
training	O
and	O
development	O
data	O
2	O
consisting	O
of	O
sentences	O
in	O
different	O
dialects	O
with	O
a	O
label	O
denoting	O
the	O
corresponding	O
dialect	O
.	O
The	O
training	O
data	O
contain	O
41	O
K	O
sentences	O
and	O
development	O
data	O
contain	O
5.2	O
K	O
sentences	O
.	O
Organizers	O
also	O
provided	O
additional	O
data	O
with	O
Arabic	O
sentences	O
in	O
seven	O
dialects	O
.	O
S	O
-	O
2	O
uses	O
a	O
corpus	O
of	O
tweets	O
.	O
Twitter	O
does	O
not	O
permit	O
the	O
organizers	O
to	O
distribute	O
tweets	O
,	O
only	O
the	O
user	O
ids	O
and	O
tweet	O
ids	O
.	O
Every	O
participant	O
must	O
arrange	O
with	O
Twitter	O
to	O
download	O
the	O
tweets	O
themselves	O
,	O
and	O
because	O
tweets	O
are	O
subject	O
to	O
deletion	O
over	O
time	O
,	O
it	O
is	O
possible	O
that	O
each	O
participant	O
's	O
version	O
of	O
the	O
corpus	O
and	O
test	O
is	O
unique	O
.	O

The	O
absence	O
of	O
audio	O
was	O
remedied	O
for	O
the	O
2017	O
and	O
2018	O
VarDial	O
workshops	O
,	O
(	O
Zampieri	O
et	O
al	O
,	O
2017	O
(	O
Zampieri	O
et	O
al	O
,	O
,	O
2018	O
However	O
,	O
the	O
five	O
dialects	O
plus	O
MSA	O
targeted	O
by	O
the	O
VarDial	O
shared	O
task	O
comprise	O
a	O
small	O
fraction	O
of	O
Arabic	O
's	O
dialectical	O
variation	O
.	O
Salameh	O
et	O
al	O
)	O
use	O
a	O
corpus	O
which	O
differentiates	O
between	O
twentyfive	O
different	O
cities	O
and	O
MSA	O
.	O
This	O
still	O
does	O
n't	O
address	O
urban	O
rural	O
divides	O
,	O
but	O
it	O
begins	O
to	O
reflect	O
more	O
realistic	O
diversity	O
.	O

In	O
this	O
section	O
we	O
describe	O
our	O
models	O
4	O
.	O
We	O
submitted	O
results	O
for	O
the	O
S	O
-	O
1	O
from	O
two	O
systems	O
-	O
Tortuous	O
Classifier	O
and	O
Neural	O
Network	O
Classifier	O
.	O

word	O
and	O
char	O
language	O
model	O
features	O
for	O
corpus	O
-	O
6	O
and	O
corpus	O
-	O
26	O
features	O
,	O
tfid	O
vectorized	O
word	O
1	O
-	O
2grams	O
,	O
and	O
tfid	O
vectorized	O
char	O
3	O
-	O
5grams	O
.	O
The	O
classifier	O
did	O
better	O
on	O
the	O
development	O
data	O
,	O
suggesting	O
that	O
it	O
is	O
over	O
-	O
fitted	O
,	O
but	O
the	O
language	O
model	O
features	O
,	O
which	O
are	O
the	O
most	O
predictive	O
,	O
also	O
did	O
better	O
on	O
the	O
development	O
data	O
.	O

For	O
the	O
Subtask	O
-	O
1	O
we	O
achieved	O
0.658	O
macro	O
F	O
1score	O
on	O
the	O
test	O
data	O
,	O
sixth	O
among	O
nineteen	O
submissions	O
with	O
the	O
Tortuous	O
Classifier	O
.	O
The	O
Neural	O
Network	O
Classifier	O
achieved	O
a	O
macro	O
F	O
1	O
-	O
score	O
of	O
0.648	O
on	O
the	O
test	O
data	O
.	O
For	O
the	O
Subtask	O
-	O
2	O
we	O
submitted	O
a	O
single	O
entry	O
.	O
It	O
ranked	O
7	O
th	O
among	O
9	O
submissions	O
with	O
0.475	O
macro	O
F	O
1	O
-	O
score	O
.	O
Figure	O
2	O
shows	O
that	O
many	O
of	O
the	O
errors	O
are	O
geographically	O
plausible	O
.	O
For	O
example	O
,	O
ASWan	O
ALXandria	O
and	O
CAIro	O
are	O
all	O
in	O
Egypt	O
,	O
and	O
each	O
has	O
a	O
sizeable	O
chunk	O
of	O
mistaken	O
identity	O
for	O
the	O
others	O
.	O
Similarly	O
,	O
DAMascus	O
,	O
ALEppo	O
,	O
AMMan	O
,	O
BEIrut	O
,	O
JERusalem	O
which	O
are	O
all	O
'	O
Levantine	O
'	O
and	O
only	O
a	O
few	O
hundred	O
miles	O
apart	O
.	O

This	O
paper	O
presents	O
an	O
automatic	O
approach	O
for	O
Arabic	O
dialect	O
detection	O
in	O
the	O
MADAR	O
Shared	O
Task	O
.	O
Our	O
proposed	O
systems	O
for	O
the	O
Subtask	O
-	O
1	O
use	O
language	O
model	O
features	O
.	O
Our	O
experiments	O
showed	O
that	O
simpler	O
machine	O
learning	O
algorithms	O
outperform	O
RNN	O
using	O
language	O
model	O
features	O
.	O
Subtask	O
-	O
2	O
turned	O
out	O
to	O
be	O
more	O
challenging	O
because	O
Tweets	O
,	O
which	O
are	O
real	O
-	O
world	O
wild	O
data	O
,	O
are	O
more	O
difficult	O
to	O
process	O
than	O
systematically	O
prepared	O
texts	O
.	O

This	O
work	O
has	O
been	O
partly	O
supported	O
by	O
Grant	O
No	O
.	O
SGS	O
-	O
2019	O
-	O
018	O
Processing	O
of	O
heterogeneous	O
data	O
and	O
its	O
specialized	O
applications	O
,	O
and	O
was	O
partly	O
supported	O
from	O
ERDF	O
"	O
Research	O
and	O
Development	O
of	O
Intelligent	O
Components	O
of	O
Advanced	O
Technologies	O
for	O
the	O
Pilsen	O
Metropolitan	O
Area	O
(	O
InteCom	O
)	O
"	O
.	O
Access	O
to	O
computing	O
and	O
storage	O
facilities	O
owned	O
by	O
parties	O
and	O
projects	O
contributing	O
to	O
the	O
National	O
Grid	O
Infrastructure	O
MetaCentrum	O
provided	O
under	O
the	O
programme	O
"	O
Projects	O
of	O
Large	O
Research	O
,	O
Development	O
,	O
and	O
Innovations	O
Infrastructures	O
"	O
(	O
CESNET	O
LM2015042	O
)	O
,	O
is	O
greatly	O
appreciated	O
.	O

A	O
FullyQT	O
Visual	O
Guide	O

Underspecification	O
and	O
interpretive	O
parallelism	O
in	O
Dependent	O
Type	O
Semantics	O

The	O
scope	O
parallelism	O
in	O
the	O
Geach	O
sentence	O
(	O
Every	O
boy	O
loves	O
,	O
and	O
every	O
girl	O
detests	O
,	O
some	O
saxophonist	O
)	O
and	O
the	O
related	O
parallel	O
interpretation	O
requirement	O
in	O
pronominal	O
binding	O
is	O
a	O
pervasive	O
phenomenon	O
found	O
across	O
different	O
types	O
of	O
coordination	O
and	O
ellipsis	O
phenomena	O
.	O
Previous	O
accounts	O
all	O
resort	O
to	O
additional	O
constraints	O
of	O
some	O
sort	O
that	O
restrict	O
the	O
otherwise	O
flexible	O
syntax	O
-	O
semantics	O
interface	O
to	O
avoid	O
overgeneration	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
approach	O
to	O
this	O
long	O
-	O
standing	O
problem	O
.	O
We	O
show	O
that	O
,	O
by	O
taking	O
a	O
proof	O
-	O
theoretic	O
perspective	O
on	O
natural	O
language	O
semantics	O
and	O
by	O
viewing	O
the	O
ambiguity	O
resolution	O
for	O
pronouns	O
and	O
indefinites	O
as	O
underspecification	O
resolution	O
that	O
invokes	O
extra	O
proof	O
search	O
,	O
a	O
conceptually	O
natural	O
solution	O
emerges	O
for	O
the	O
problem	O
of	O
interpretive	O
parallelism	O
.	O
The	O
analysis	O
is	O
cast	O
in	O
Dependent	O
Type	O
Semantics	O
,	O
with	O
Hybrid	O
Type	O
-	O
Logical	O
Categorial	O
Grammar	O
as	O
the	O
syntax	O
-	O
semantics	O
interface	O
backbone	O
.	O
For	O
empirical	O
illustration	O
,	O
we	O
show	O
how	O
the	O
proposed	O
approach	O
correctly	O
accounts	O
for	O
the	O
classical	O
Geach	O
paradigm	O
and	O
its	O
pronominal	O
variant	O
.	O

1	O
Introduction	O
:	O
interpretive	O
parallelism	O
in	O
coordination	O
and	O
ellipsis	O
One	O
of	O
the	O
long	O
-	O
standing	O
problems	O
in	O
the	O
analysis	O
of	O
coordination	O
and	O
ellipsis	O
is	O
the	O
strong	O
parallelism	O
requirement	O
imposed	O
on	O
the	O
interpretations	O
of	O
the	O
'	O
shared	O
'	O
linguistic	O
expression	O
.	O
For	O
example	O
,	O
Geach	O
(	O
1972	O
)	O
famously	O
noticed	O
that	O
,	O
in	O
the	O
following	O
type	O
of	O
examples	O
involving	O
right	O
-	O
node	O
raising	O
(	O
RNR	O
)	O
,	O
the	O
object	O
indefinite	O
that	O
is	O
shared	O
in	O
the	O
two	O
conjuncts	O
can	O
either	O
scope	O
below	O
the	O
subject	O
quantifier	O
in	O
each	O
conjunct	O
or	O
scope	O
over	O
the	O
entire	O
coordination	O
,	O
but	O
that	O
mixed	O
scope	O
readings	O
,	O
in	O
which	O
the	O
object	O
indefinite	O
scopes	O
above	O
the	O
subject	O
quantifier	O
in	O
one	O
conjunct	O
but	O
not	O
in	O
the	O
other	O
,	O
are	O
unavailable	O
.	O
(	O
1	O
)	O
Every	O
boy	O
loves	O
,	O
and	O
every	O
girl	O
detests	O
,	O
some	O
saxophonist	O
.	O
(	O
∀	O
>	O
∀	O
>	O
/	O
>	O
∀	O
>	O
∀	O
)	O
Jacobson	O
(	O
1999	O
)	O
notes	O
that	O
this	O
interpretive	O
parallelism	O
extends	O
to	O
the	O
pronominal	O
variable	O
binding	O
in	O
examples	O
such	O
as	O
the	O
following	O
(	O
on	O
reading	O
2	O
,	O
John	O
is	O
a	O
salient	O
male	O
individual	O
in	O
the	O
discourse	O
)	O
:	O
(	O
2	O
)	O
Every	O
Englishman	O
admires	O
,	O
and	O
every	O
American	O
loves	O
,	O
his	O
mother	O
.	O
reading	O
1	O
:	O
'	O
Every	O
Englishman	O
admires	O
his	O
own	O
mother	O
,	O
and	O
every	O
American	O
loves	O
his	O
own	O
mother	O
.	O
'	O
reading	O
2	O
:	O
'	O
Every	O
Englishman	O
admires	O
John	O
's	O
mother	O
,	O
and	O
every	O
American	O
loves	O
John	O
's	O
mother	O
.	O
'	O
In	O
general	O
,	O
pronouns	O
can	O
either	O
be	O
free	O
or	O
bound	O
by	O
a	O
quantifier	O
.	O
Thus	O
,	O
there	O
are	O
four	O
logically	O
possible	O
interpretations	O
for	O
(	O
2	O
)	O
(	O
bound	O
/	O
free	O
in	O
first	O
/	O
second	O
conjunct	O
)	O
.	O
And	O
all	O
these	O
interpretations	O
are	O
indeed	O
available	O
in	O
the	O
non	O
-	O
RNR	O
counterpart	O
of	O
(	O
2	O
)	O
(	O
Every	O
Englishman	O
admires	O
his	O
mother	O
and	O
every	O
American	O
loves	O
his	O
mother	O
)	O
.	O
However	O
,	O
only	O
two	O
of	O
these	O
readings	O
are	O
attested	O
for	O
(	O
2	O
)	O
,	O
as	O
indicated	O
above	O
.	O
The	O
parallel	O
interpretation	O
requirement	O
is	O
not	O
limited	O
to	O
coordination	O
but	O
extends	O
to	O
ellipsis	O
phenomena	O
.	O
For	O
example	O
,	O
Hirschbühler	O
(	O
1982	O
)	O
notes	O
that	O
VP	O
ellipsis	O
imposes	O
parallelism	O
requirement	O
for	O
the	O
scope	O
of	O
the	O
quantifier	O
inside	O
the	O
'	O
elided	O
'	O
material	O
,	O
in	O
a	O
way	O
essentially	O
parallel	O
to	O
the	O
RNR	O
sentences	O
:	O
(	O
3	O
)	O
An	O
American	O
flag	O
was	O
hanging	O
in	O
front	O
of	O
every	O
window	O
.	O
A	O
Canadian	O
flag	O
was	O
,	O
too	O
.	O
Like	O
the	O
RNR	O
example	O
in	O
(	O
1	O
)	O
,	O
there	O
are	O
only	O
the	O
∀	O
>	O
∀	O
>	O
and	O
>	O
∀	O
>	O
∀	O
parallel	O
scope	O
readings	O
for	O
this	O
sentence	O
.	O
Mixed	O
scope	O
readings	O
in	O
which	O
the	O
universal	O
scopes	O
over	O
the	O
indefinite	O
in	O
the	O
antecedent	O
but	O
not	O
in	O
the	O
ellipsis	O
site	O
(	O
or	O
vice	O
versa	O
)	O
are	O
unavailable	O
.	O
Just	O
as	O
the	O
RNR	O
parallelism	O
for	O
quantifier	O
scope	O
in	O
(	O
1	O
)	O
is	O
mirrored	O
in	O
the	O
anaphora	O
case	O
in	O
(	O
2	O
)	O
,	O
the	O
scopal	O
parallelism	O
in	O
the	O
VP	O
ellipsis	O
data	O
in	O
(	O
3	O
)	O
has	O
an	O
exact	O
analogue	O
in	O
the	O
anaphora	O
example	O
in	O
(	O
4	O
)	O
.	O
(	O
4	O
)	O
Every	O
Englishman	O
admires	O
his	O
mother	O
,	O
and	O
every	O
American	O
does	O
as	O
well	O
.	O
As	O
in	O
(	O
2	O
)	O
,	O
the	O
admiration	O
relation	O
holds	O
either	O
between	O
every	O
Englishman	O
and	O
every	O
American	O
male	O
and	O
his	O
own	O
respective	O
mother	O
or	O
between	O
every	O
Englishman	O
and	O
every	O
American	O
male	O
and	O
the	O
mother	O
of	O
some	O
specific	O
male	O
individual	O
in	O
the	O
antecedent	O
context	O
,	O
with	O
no	O
mixed	O
reading	O
possible	O
.	O
The	O
parallelism	O
patterns	O
in	O
(	O
1	O
)	O
and	O
(	O
2	O
)	O
recur	O
in	O
the	O
case	O
of	O
Stripping	O
(	O
see	O
Puthawala	O
(	O
2018	O
)	O
for	O
a	O
recent	O
formal	O
analysis	O
of	O
Stripping	O
,	O
as	O
well	O
as	O
a	O
discussion	O
of	O
important	O
properties	O
of	O
this	O
construction	O
)	O
.	O
(	O
5	O
)	O
a.	O
Every	O
boy	O
admires	O
a	O
saxophonist	O
,	O
and	O
every	O
girl	O
too	O
.	O
b.	O
Every	O
Englishman	O
admires	O
his	O
mother	O
,	O
and	O
every	O
American	O
as	O
well	O
.	O
These	O
examples	O
exhibit	O
only	O
the	O
parallel	O
interpretations	O
(	O
analogous	O
to	O
the	O
relevant	O
readings	O
for	O
the	O
RNR	O
and	O
VP	O
ellipsis	O
examples	O
given	O
above	O
)	O
for	O
the	O
quantifier	O
or	O
pronoun	O
contained	O
in	O
the	O
elided	O
material	O
.	O
The	O
interpretive	O
parallelism	O
in	O
the	O
data	O
surveyed	O
above	O
has	O
been	O
noted	O
by	O
many	O
authors	O
(	O
see	O
,	O
e.g.	O
,	O
Jacobson	O
1999	O
;	O
Fox	O
2000	O
;	O
Asudeh	O
and	O
Crouch	O
2002	O
;	O
Steedman	O
2012	O
)	O
,	O
but	O
no	O
uniform	O
analysis	O
currently	O
exists	O
which	O
treats	O
the	O
binding	O
and	O
quantifier	O
cases	O
in	O
a	O
principled	O
manner	O
and	O
which	O
covers	O
both	O
the	O
coordination	O
and	O
ellipsis	O
cases	O
.	O
The	O
present	O
paper	O
attempts	O
to	O
make	O
a	O
first	O
step	O
in	O
such	O
a	O
unified	O
analysis	O
by	O
focusing	O
on	O
the	O
binding	O
and	O
scope	O
data	O
in	O
RNR	O
(	O
i.e.	O
the	O
Geach	O
paradigm	O
)	O
.	O
The	O
key	O
claim	O
of	O
our	O
proposal	O
is	O
that	O
interpretive	O
parallelism	O
is	O
a	O
consequence	O
of	O
the	O
underspecification	O
involved	O
in	O
the	O
interpretation	O
of	O
pronouns	O
and	O
indefinites	O
(	O
in	O
this	O
sense	O
,	O
it	O
is	O
similar	O
in	O
spirit	O
to	O
Steedman	O
's	O
(	O
2012	O
)	O
approach	O
)	O
.	O
All	O
the	O
examples	O
above	O
have	O
one	O
property	O
in	O
common	O
:	O
the	O
shared	O
material	O
contains	O
an	O
expression	O
(	O
pronoun	O
/	O
quantifier	O
)	O
that	O
exhibits	O
interpretive	O
variability	O
.	O
Our	O
proposal	O
in	O
a	O
nutshell	O
is	O
that	O
interpretive	O
parallelism	O
falls	O
out	O
from	O
the	O
way	O
underspecification	O
resolution	O
happens	O
in	O
sentences	O
that	O
have	O
this	O
property	O
,	O
due	O
to	O
interactions	O
of	O
the	O
following	O
conceptually	O
natural	O
assumptions	O
:	O
(	O
i	O
)	O
interpretive	O
variability	O
is	O
resolved	O
by	O
underspecification	O
resolution	O
,	O
formalized	O
as	O
type	O
-	O
checking	O
(	O
ii	O
)	O
for	O
'	O
shared	O
'	O
material	O
,	O
the	O
syntax	O
-	O
semantics	O
mapping	O
requires	O
the	O
duplication	O
of	O
resource	O
at	O
some	O
point	O
in	O
the	O
mapping	O
from	O
the	O
surface	O
string	O
to	O
the	O
final	O
,	O
fully	O
resolved	O
translation	O
(	O
iii	O
)	O
the	O
formal	O
language	O
for	O
the	O
underspecified	O
semantic	O
representation	O
imposes	O
a	O
certain	O
restriction	O
on	O
the	O
way	O
multiple	O
copies	O
of	O
an	O
(	O
originally	O
)	O
underspecified	O
term	O
are	O
interpreted	O
The	O
third	O
condition	O
can	O
be	O
thought	O
to	O
arise	O
from	O
the	O
requirement	O
to	O
keep	O
the	O
mechanism	O
simple	O
for	O
ensuring	O
proper	O
identity	O
of	O
underspecified	O
terms	O
with	O
respect	O
to	O
their	O
interpretive	O
possibilities	O
.	O
We	O
show	O
below	O
that	O
these	O
simple	O
assumptions	O
suffice	O
to	O
ensure	O
the	O
right	O
range	O
of	O
interpretations	O
to	O
be	O
assigned	O
to	O
the	O
examples	O
discussed	O
above	O
,	O
by	O
taking	O
the	O
case	O
of	O
RNR	O
as	O
an	O
example	O
.	O
We	O
formulate	O
our	O
analysis	O
in	O
Dependent	O
Type	O
Semantics	O
(	O
DTS	O
;	O
Bekki	O
2014	O
;	O
Bekki	O
and	O
Mineshima	O
2017	O
)	O
,	O
by	O
proposing	O
a	O
novel	O
treatment	O
of	O
indefinites	O
involving	O
underspecification	O
.	O
The	O
proof	O
-	O
theoretic	O
perspective	O
of	O
DTS	O
provides	O
a	O
particularly	O
natural	O
setup	O
to	O
embody	O
the	O
assumptions	O
outlined	O
in	O
(	O
i	O
)	O
-	O
(	O
iii	O
)	O
above	O
.	O
For	O
the	O
sake	O
of	O
explicitness	O
,	O
we	O
adopt	O
Hybrid	O
Type	O
-	O
Logical	O
Categorial	O
Grammar	O
(	O
Hybrid	O
TLCG	O
;	O
Levine	O
2012	O
,	O
2015	O
)	O
for	O
the	O
syntax	O
-	O
semantics	O
interface	O
in	O
spelling	O
out	O
the	O
analyses	O
of	O
specific	O
linguistic	O
examples	O
.	O
The	O
choice	O
of	O
Hybrid	O
TLCG	O
for	O
syntax	O
is	O
not	O
essential	O
,	O
but	O
we	O
believe	O
that	O
it	O
helps	O
illuminate	O
the	O
general	O
nature	O
of	O
our	O
solution	O
,	O
which	O
is	O
compatible	O
with	O
any	O
suitably	O
general	O
theory	O
of	O
compositional	O
semantics	O
.	O
2	O
Anaphora	O
and	O
scope	O
via	O
underspecification	O
in	O
DTS	O
Dependent	O
Type	O
Theory	O
(	O
Martin	O
-	O
Löf	O
1984	O
)	O
is	O
an	O
extension	O
of	O
simply	O
typed	O
λ	O
-	O
calculus	O
.	O
Dependent	O
Type	O
Semantics	O
(	O
DTS	O
)	O
is	O
a	O
proof	O
-	O
theoretic	O
compositional	O
dynamic	O
semantics	O
based	O
on	O
Dependent	O
Type	O
Theory	O
.	O
This	O
framework	O
allows	O
us	O
to	O
use	O
types	O
depending	O
on	O
terms	O
and	O
to	O
represent	O
propositions	O
(	O
corresponding	O
to	O
semantic	O
representations	O
of	O
sentences	O
)	O
as	O
types	O
.	O
For	O
instance	O
,	O
run	O
(	O
x	O
)	O
is	O
a	O
type	O
depending	O
on	O
a	O
term	O
x.	O
Under	O
the	O
Curry	O
-	O
Howard	O
correspondence	O
(	O
propositions	O
-	O
as	O
-	O
types	O
principle	O
)	O
,	O
the	O
type	O
run	O
(	O
x	O
)	O
can	O
be	O
regarded	O
as	O
the	O
proposition	O
that	O
x	O
runs	O
.	O
If	O
a	O
term	O
u	O
has	O
this	O
type	O
,	O
we	O
write	O
u	O
:	O
run	O
(	O
x	O
)	O
,	O
expressing	O
that	O
u	O
is	O
a	O
proof	O
of	O
the	O
proposition	O
that	O
run	O
(	O
x	O
)	O
.	O
Such	O
a	O
term	O
u	O
is	O
called	O
a	O
proof	O
term	O
and	O
plays	O
a	O
key	O
role	O
in	O
representing	O
the	O
dynamic	O
notion	O
of	O
contexts	O
for	O
resolving	O
anaphora	O
in	O
DTS	O
.	O

In	O
the	O
following	O
analysis	O
,	O
we	O
mainly	O
use	O
two	O
constructions	O
,	O
Σ	O
-	O
types	O
and	O
Π	O
-	O
types	O
.	O
Σ	O
-	O
type	O
,	O
written	O
(	O
x	O
:	O
A	O
)	O
×	O
B	O
,	O
is	O
a	O
generalization	O
of	O
product	O
type	O
A	O
×	O
B.	O
A	O
term	O
of	O
type	O
(	O
x	O
:	O
A	O
)	O
×	O
B	O
is	O
a	O
pair	O
(	O
m	O
,	O
n	O
)	O
such	O
that	O
m	O
has	O
type	O
A	O
and	O
n	O
has	O
type	O
B	O
[	O
m	O
/	O
x	O
]	O
.	O
1	O
The	O
projection	O
functions	O
π	O
1	O
and	O
π	O
2	O
are	O
defined	O
so	O
that	O
π	O
1	O
(	O
m	O
,	O
n	O
)	O
=	O
m	O
and	O
π	O
2	O
(	O
m	O
,	O
n	O
)	O
=	O
n.	O
Σ	O
-	O
types	O
can	O
be	O
used	O
to	O
represent	O
existential	O
quantification	O
.	O
For	O
instance	O
,	O
A	O
man	O
entered	O
is	O
given	O
the	O
translation	O
(	O
6	O
)	O
in	O
DTS	O
.	O
2	O
(	O
6	O
)	O
(	O
u	O
:	O
(	O
x	O
:	O
e	O
)	O
×	O
man	O
(	O
x	O
)	O
)	O
×	O
enter	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
Here	O
u	O
is	O
a	O
proof	O
term	O
of	O
(	O
x	O
:	O
e	O
)	O
×	O
man	O
(	O
x	O
)	O
,	O
which	O
is	O
a	O
pair	O
of	O
x	O
having	O
type	O
e	O
(	O
entity	O
)	O
and	O
a	O
proof	O
that	O
x	O
is	O
a	O
man	O
.	O
Thus	O
,	O
its	O
first	O
component	O
(	O
the	O
entity	O
x	O
)	O
can	O
be	O
picked	O
up	O
by	O
the	O
projection	O
π	O
1	O
(	O
u	O
)	O
.	O
The	O
entire	O
translation	O
means	O
that	O
there	O
is	O
an	O
entity	O
x	O
such	O
that	O
x	O
is	O
a	O
man	O
and	O
x	O
enters	O
.	O
For	O
notational	O
simplicity	O
,	O
we	O
often	O
abbreviate	O
Σ	O
-	O
type	O
of	O
the	O
form	O
(	O
x	O
:	O
e	O
)	O
×A	O
(	O
x	O
)	O
as	O
A	O
*	O
,	O
thus	O
we	O
write	O
(	O
6	O
)	O
as	O
(	O
u	O
:	O
man	O
*	O
)	O
×enter	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
.	O
Π	O
-	O
type	O
,	O
(	O
x	O
:	O
A	O
)	O
B	O
in	O
our	O
notation	O
,	O
is	O
a	O
generalization	O
of	O
function	O
type	O
A	O
B.	O
A	O
term	O
of	O
type	O
(	O
x	O
:	O
A	O
)	O
B	O
is	O
a	O
function	O
f	O
such	O
that	O
for	O
any	O
term	O
m	O
of	O
type	O
A	O
,	O
f	O
(	O
m	O
)	O
is	O
of	O
type	O
B	O
[	O
m	O
/	O
x	O
]	O
.	O
Π	O
-	O
type	O
is	O
used	O
to	O
represent	O
universal	O
quantification	O
.	O
Thus	O
,	O
Every	O
man	O
entered	O
is	O
given	O
the	O
translation	O
in	O
(	O
7	O
)	O
.	O
(	O
7	O
)	O
(	O
u	O
:	O
man	O
*	O
)	O
enter	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
Note	O
that	O
when	O
the	O
variable	O
x	O
does	O
not	O
occur	O
free	O
in	O
B	O
,	O
(	O
x	O
:	O
A	O
)	O
×	O
B	O
and	O
(	O
x	O
:	O
A	O
)	O
B	O
can	O
be	O
written	O
A	O
×	O
B	O
and	O
A	O
B	O
,	O
respectively	O
.	O
We	O
illustrate	O
how	O
anaphora	O
resolution	O
works	O
in	O
DTS	O
by	O
the	O
example	O
A	O
man	O
entered	O
and	O
he	O
smiled	O
,	O
which	O
is	O
given	O
the	O
following	O
translation	O
as	O
an	O
initial	O
underspecified	O
representation	O
.	O
(	O
8	O
)	O
(	O
v	O
:	O
(	O
u	O
:	O
man	O
*	O
)	O
×	O
enter	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
)	O
×	O
smile	O
(	O
@	O
1	O
e	O
)	O
In	O
DTS	O
,	O
a	O
pronoun	O
is	O
analyzed	O
as	O
an	O
underspecified	O
term	O
@	O
,	O
possibly	O
annotated	O
with	O
its	O
type	O
A	O
,	O
which	O
we	O
write	O
@A.	O
We	O
assume	O
that	O
in	O
the	O
initial	O
underspecified	O
representation	O
,	O
each	O
occurrence	O
of	O
underspecified	O
term	O
@	O
is	O
assigned	O
a	O
mutually	O
distinct	O
index	O
.	O
In	O
the	O
above	O
example	O
,	O
the	O
pronoun	O
he	O
corresponds	O
to	O
@	O
1	O
e.	O
This	O
underspecified	O
term	O
searches	O
for	O
its	O
antecedent	O
of	O
type	O
e	O
in	O
the	O
context	O
represented	O
as	O
a	O
proof	O
term	O
.	O
The	O
initial	O
step	O
to	O
resolve	O
anaphora	O
is	O
type	O
checking	O
,	O
which	O
is	O
a	O
process	O
to	O
ensure	O
that	O
a	O
given	O
expression	O
is	O
a	O
type	O
(	O
i.e.	O
a	O
well	O
-	O
formed	O
proposition	O
)	O
.	O
This	O
amounts	O
to	O
proving	O
that	O
it	O
has	O
type	O
type	O
,	O
abbreviated	O
as	O
t.	O
The	O
formation	O
rules	O
(	O
see	O
Figure	O
1	O
)	O
tell	O
us	O
when	O
a	O
given	O
expression	O
has	O
type	O
t.	O
In	O
the	O
case	O
of	O
(	O
6	O
)	O
,	O
the	O
goal	O
is	O
to	O
prove	O
that	O
the	O
representation	O
in	O
(	O
6	O
)	O
has	O
type	O
t.	O
In	O
this	O
case	O
,	O
no	O
underspecified	O
term	O
appears	O
,	O
thus	O
using	O
the	O
inference	O
rules	O
in	O
Figure	O
1	O
,	O
we	O
have	O
the	O
following	O
closed	O
derivation	O
.	O
Here	O
we	O
assume	O
that	O
type	O
assignments	O
(	O
signatures	O
)	O
such	O
as	O
e	O
:	O
t	O
and	O
enter	O
:	O
e	O
t	O
are	O
in	O
the	O
initial	O
context	O
and	O
can	O
be	O
used	O
as	O
an	O
axiom	O
.	O
To	O
simplify	O
derivations	O
,	O
we	O
usually	O
omit	O
axioms	O
and	O
use	O
the	O
name	O
of	O
the	O
predicate	O
applied	O
(	O
possibly	O
with	O
its	O
type	O
)	O
as	O
a	O
rule	O
label	O
.	O
If	O
an	O
initial	O
representation	O
contains	O
an	O
underspecified	O
term	O
@	O
,	O
the	O
process	O
of	O
type	O
checking	O
tells	O
us	O
in	O
what	O
context	O
the	O
antecedent	O
of	O
the	O
@	O
-	O
term	O
can	O
be	O
found	O
.	O
For	O
this	O
purpose	O
,	O
we	O
use	O
the	O
following	O
rule	O
:	O
(	O
10	O
)	O
A	O
:	O
t	O
A	O
true	O
@	O
i	O
A	O
:	O
A	O
@	O
We	O
use	O
a	O
judgement	O
of	O
the	O
form	O
A	O
true	O
to	O
mean	O
that	O
there	O
exists	O
a	O
term	O
of	O
type	O
A	O
;	O
in	O
other	O
words	O
,	O
type	O
A	O
is	O
inhabited	O
.	O
Using	O
this	O
rule	O
,	O
the	O
type	O
checking	O
for	O
(	O
8	O
)	O
gives	O
an	O
open	O
derivation	O
as	O
follows	O
.	O
.	O
.	O
.	O
.	O
(	O
9	O
)	O
(	O
u	O
:	O
man	O
*	O
)	O
×	O
enter	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
:	O
t	O
e	O
:	O
t	O
Ax	O
e	O
true	O
@	O
1	O
e	O
:	O
e	O
@	O
smile	O
(	O
@	O
1	O
e	O
)	O
:	O
t	O
smile	O
(	O
v	O
:	O
(	O
u	O
:	O
man	O
*	O
)	O
×	O
enter	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
)	O
×	O
smile	O
(	O
@	O
1	O
e	O
)	O
:	O
t	O
ΣF	O
,	O
1	O
Here	O
the	O
derivation	O
starts	O
from	O
the	O
open	O
premise	O
e	O
true	O
.	O
Once	O
we	O
prove	O
e	O
true	O
and	O
find	O
a	O
witness	O
for	O
@	O
i	O
,	O
it	O
becomes	O
a	O
closed	O
derivation	O
.	O
To	O
formalize	O
this	O
idea	O
,	O
we	O
use	O
the	O
following	O
rule	O
for	O
@	O
-	O
elimination	O
.	O
(	O
12	O
)	O
@	O
-	O
elimination	O
:	O
Let	O
A	O
be	O
a	O
term	O
in	O
which	O
no	O
@	O
-	O
term	O
occurs	O
.	O
Then	O
the	O
derivation	O
on	O
the	O
left	O
can	O
be	O
transformed	O
into	O
the	O
derivation	O
on	O
the	O
right	O
:	O
.	O
.	O
.	O
.	O
A	O
:	O
t	O
.	O
.	O
.	O
.	O
D	O
2	O
u	O
:	O
A	O
A	O
true	O
@	O
i	O
A	O
:	O
A	O
@	O
.	O
.	O
.	O
.	O
D	O
1	O
.	O
.	O
.	O
.	O
D	O
2	O
u	O
:	O
A	O
.	O
.	O
.	O
.	O
D	O
1	O
[	O
u/@	O
i	O
A	O
]	O
This	O
rule	O
allows	O
us	O
to	O
replace	O
the	O
underspecified	O
term	O
@	O
i	O
A	O
with	O
its	O
witness	O
u	O
in	O
the	O
entire	O
derivation	O
.	O
To	O
find	O
a	O
witness	O
for	O
an	O
underspecified	O
term	O
@	O
,	O
we	O
need	O
to	O
do	O
proof	O
search	O
in	O
a	O
given	O
local	O
context	O
.	O
In	O
the	O
case	O
of	O
(	O
11	O
)	O
,	O
the	O
application	O
of	O
ΣF	O
rule	O
at	O
the	O
final	O
step	O
allows	O
us	O
to	O
use	O
a	O
proof	O
term	O
for	O
the	O
left	O
-	O
side	O
proposition	O
,	O
(	O
u	O
:	O
man	O
*	O
)	O
×	O
enter	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
,	O
to	O
find	O
a	O
witness	O
for	O
@	O
1	O
e.	O
It	O
can	O
be	O
easily	O
seen	O
that	O
one	O
such	O
witness	O
is	O
π	O
1	O
(	O
π	O
1	O
(	O
v	O
)	O
)	O
;	O
in	O
this	O
case	O
,	O
we	O
say	O
@	O
1	O
e	O
is	O
bound	O
to	O
π	O
1	O
(	O
π	O
1	O
(	O
v	O
)	O
)	O
.	O
3	O
Thus	O
we	O
have	O
a	O
closed	O
derivation	O
on	O
the	O
left	O
below	O
and	O
it	O
can	O
be	O
transformed	O
to	O
the	O
derivation	O
on	O
the	O
right	O
by	O
@	O
-	O
elimination	O
.	O
.	O
.	O
.	O
.	O
(	O
9	O
)	O
(	O
u	O
:	O
man	O
*	O
)	O
×	O
enter	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
:	O
t	O
e	O
:	O
t	O
Ax	O
[	O
v	O
:	O
(	O
u	O
:	O
man	O
*	O
)	O
×	O
enter	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
]	O
1	O
π	O
1	O
(	O
v	O
)	O
:	O
man	O
*	O
ΣE	O
π	O
1	O
(	O
π	O
1	O
(	O
v	O
)	O
)	O
:	O
e	O
ΣE	O
e	O
true	O
@	O
1	O
e	O
:	O
e	O
@	O
smile	O
(	O
@	O
1	O
e	O
)	O
:	O
t	O
smile	O
(	O
v	O
:	O
(	O
u	O
:	O
man	O
*	O
)	O
×	O
enter	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
)	O
×	O
smile	O
(	O
@	O
1	O
e	O
)	O
:	O
t	O
ΣF	O
,	O
1	O
.	O
.	O
.	O
.	O
(	O
9	O
)	O
(	O
u	O
:	O
man	O
*	O
)	O
×	O
enter	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
:	O
t	O
[	O
v	O
:	O
(	O
u	O
:	O
man	O
*	O
)	O
×	O
enter	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
]	O
1	O
π	O
1	O
(	O
v	O
)	O
:	O
man	O
*	O
π	O
1	O
(	O
π	O
1	O
(	O
v	O
)	O
)	O
:	O
e	O
ΣE	O
smile	O
(	O
π	O
1	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
)	O
:	O
t	O
smile	O
(	O
v	O
:	O
(	O
u	O
:	O
man	O
*	O
)	O
×	O
enter	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
)	O
×	O
smile	O
(	O
π	O
1	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
)	O
:	O
t	O
ΣF	O
,	O
1	O
The	O
final	O
representation	O
can	O
be	O
read	O
off	O
from	O
the	O
bottom	O
line	O
of	O
the	O
derivation	O
on	O
the	O
right	O
.	O
(	O
13	O
)	O
(	O
v	O
:	O
(	O
u	O
:	O
man	O
*	O
)	O
×	O
enter	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
)	O
×	O
smile	O
(	O
π	O
1	O
(	O
π	O
1	O
(	O
v	O
)	O
)	O
)	O
This	O
is	O
equivalent	O
to	O
saying	O
that	O
there	O
is	O
an	O
entity	O
x	O
such	O
that	O
it	O
satisfies	O
man	O
(	O
x	O
)	O
,	O
enter	O
(	O
x	O
)	O
,	O
and	O
smile	O
(	O
x	O
)	O
.	O
Our	O
analysis	O
naturally	O
accounts	O
for	O
the	O
bound	O
reading	O
of	O
(	O
14a	O
)	O
,	O
whose	O
translation	O
is	O
given	O
in	O
(	O
14b	O
)	O
.	O
(	O
14	O
)	O
a.	O
Every	O
Englishman	O
thinks	O
he	O
is	O
a	O
genius	O
.	O
b.	O
(	O
u	O
:	O
eng	O
*	O
)	O
think	O
(	O
genius	O
(	O
@	O
1	O
e	O
)	O
)	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
The	O
derivation	O
on	O
the	O
left	O
shows	O
the	O
type	O
checking	O
with	O
proof	O
search	O
to	O
find	O
a	O
witness	O
for	O
@	O
1	O
e	O
in	O
(	O
14b	O
)	O
.	O
(	O
15	O
)	O
[	O
u	O
:	O
eng	O
*	O
]	O
1	O
π	O
1	O
(	O
u	O
)	O
:	O
e	O
e	O
true	O
@	O
1	O
e	O
:	O
e	O
@	O
genius	O
(	O
@	O
1	O
e	O
)	O
:	O
t	O
genius	O
[	O
u	O
:	O
eng	O
*	O
]	O
1	O
π	O
1	O
(	O
u	O
)	O
:	O
e	O
think	O
(	O
genius	O
(	O
@	O
1	O
e	O
)	O
)	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
:	O
t	O
think	O
:	O
t	O
e	O
t	O
(	O
u	O
:	O
eng	O
*	O
)	O
think	O
(	O
genius	O
(	O
@	O
1	O
e	O
)	O
)	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
:	O
t	O
ΣF	O
,	O
1	O
[	O
u	O
:	O
eng	O
*	O
]	O
1	O
π	O
1	O
(	O
u	O
)	O
:	O
e	O
genius	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
:	O
t	O
genius	O
[	O
u	O
:	O
eng	O
*	O
]	O
1	O
π	O
1	O
(	O
u	O
)	O
:	O
e	O
think	O
(	O
genius	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
)	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
:	O
t	O
think	O
:	O
t	O
e	O
t	O
(	O
u	O
:	O
eng	O
*	O
)	O
think	O
(	O
genius	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
)	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
:	O
t	O
ΣF	O
,	O
1	O
Here	O
the	O
premise	O
e	O
true	O
follows	O
from	O
the	O
hypothesis	O
u	O
:	O
eng	O
*	O
licensed	O
by	O
the	O
application	O
of	O
ΣF	O
.	O
Thus	O
@	O
1	O
e	O
is	O
bound	O
to	O
π	O
1	O
(	O
u	O
)	O
and	O
the	O
@	O
-	O
term	O
can	O
be	O
eliminated	O
as	O
shown	O
in	O
the	O
derivation	O
on	O
the	O
right	O
.	O
This	O
yields	O
the	O
bound	O
reading	O
(	O
u	O
:	O
eng	O
*	O
)	O
think	O
(	O
genius	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
)	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
for	O
(	O
14a	O
)	O
,	O
as	O
desired	O
.	O
An	O
alternative	O
treatment	O
of	O
indefinites	O
In	O
the	O
classical	O
version	O
of	O
DTS	O
,	O
pronouns	O
and	O
definites	O
are	O
translated	O
as	O
underspecified	O
terms	O
,	O
while	O
indefinites	O
are	O
not	O
.	O
Here	O
we	O
propose	O
an	O
alternative	O
analysis	O
that	O
translates	O
an	O
indefinite	O
to	O
an	O
underspecified	O
term	O
of	O
the	O
form	O
#	O
A	O
where	O
A	O
is	O
a	O
Σ	O
-	O
type	O
.	O
This	O
alternative	O
analysis	O
translates	O
the	O
sentence	O
(	O
6a	O
)	O
as	O
follows	O
(	O
note	O
that	O
man	O
*	O
is	O
an	O
abbreviation	O
for	O
(	O
x	O
:	O
e	O
)	O
×	O
man	O
(	O
x	O
)	O
)	O
:	O
(	O
16	O
)	O
enter	O
(	O
π	O
1	O
(	O
#	O
man	O
*	O
)	O
)	O
For	O
underspecified	O
terms	O
#	O
,	O
we	O
use	O
the	O
following	O
rule	O
.	O
4	O
(	O
17	O
)	O
A	O
:	O
t	O
#	O
A	O
:	O
A	O

In	O
this	O
paper	O
,	O
we	O
have	O
proposed	O
an	O
analysis	O
of	O
the	O
interpretive	O
parallelism	O
for	O
anaphora	O
and	O
scope	O
in	O
the	O
so	O
-	O
called	O
Geach	O
sentences	O
involving	O
right	O
-	O
node	O
raising	O
.	O
In	O
the	O
proposed	O
analysis	O
,	O
the	O
parallel	O
interpretation	O
requirement	O
on	O
pronouns	O
and	O
indefinites	O
in	O
the	O
shared	O
right	O
periphery	O
is	O
a	O
consequence	O
of	O
the	O
way	O
underspecified	O
terms	O
are	O
interpreted	O
in	O
the	O
underspecification	O
language	O
that	O
mediates	O
the	O
compositional	O
semantic	O
representation	O
straightforwardly	O
derived	O
from	O
the	O
syntactic	O
derivation	O
and	O
the	O
fully	O
resolved	O
semantic	O
representation	O
that	O
explicitly	O
encodes	O
all	O
the	O
relevant	O
logical	O
entailment	O
relations	O
.	O
The	O
natural	O
next	O
question	O
is	O
whether	O
the	O
present	O
approach	O
can	O
be	O
extended	O
to	O
the	O
ellipsis	O
cases	O
.	O
Preliminary	O
results	O
suggest	O
a	O
positive	O
answer	O
to	O
this	O
question	O
,	O
but	O
a	O
detailed	O
analysis	O
is	O
a	O
task	O
for	O
future	O
research	O
.	O

This	O
work	O
was	O
supported	O
by	O
JSPS	O
KAKENHI	O
JP15K16732	O
,	O
the	O
NINJAL	O
collaborative	O
research	O
project	O
'	O
Cross	O
-	O
linguistic	O
Studies	O
of	O
Japanese	O
Prosody	O
and	O
Grammar	O
'	O
and	O
the	O
OSU	O
College	O
of	O
the	O
Arts	O
and	O
Sciences	O
Larger	O
Grant	O
.	O

This	O
section	O
discusses	O
each	O
dataset	O
used	O
in	O
our	O
taskoriented	O
pre	O
-	O
training	O
and	O
how	O
we	O
process	O
the	O
data	O
.	O
Then	O
we	O
introduce	O
the	O
selected	O
pre	O
-	O
training	O
base	O
model	O
and	O
its	O
objective	O
functions	O
.	O

A	O
Appendices	O

On	O
the	O
Computational	O
Power	O
of	O
Transformers	O
and	O
its	O
Implications	O
in	O
Sequence	O
Modeling	O

In	O
this	O
section	O
,	O
we	O
describe	O
the	O
specifics	O
of	O
our	O
experimental	O
setup	O
.	O
This	O
includes	O
details	O
about	O
the	O
dataset	O
,	O
models	O
,	O
setup	O
and	O
some	O
sample	O
outputs	O
.	O

We	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
constructive	O
comments	O
and	O
suggestions	O
.	O
We	O
would	O
also	O
like	O
to	O
thank	O
our	O
colleagues	O
at	O
Microsoft	O
Research	O
and	O
Michael	O
Hahn	O
for	O
their	O
valuable	O
feedback	O
and	O
helpful	O
discussions	O
.	O

We	O
conduct	O
the	O
ablation	O
experiments	O
to	O
distinguish	O
the	O
contribution	O
of	O
each	O
part	O
.	O
There	O
are	O
several	O
different	O
variants	O
of	O
our	O
model	O
.	O
SWRM	O
is	O
our	O
proposed	O
full	O
model	O
.	O
SWRM	O
w/o	O
Position	O
does	O
not	O
Table	O
2	O
shows	O
the	O
results	O
of	O
the	O
variants	O
of	O
our	O
model	O
.	O
After	O
ablating	O
the	O
sentiment	O
word	O
position	O
location	O
module	O
,	O
SWRM	O
w/o	O
Position	O
obtains	O
worse	O
results	O
than	O
SWRM	O
,	O
which	O
indicates	O
that	O
finding	O
the	O
right	O
word	O
for	O
refinement	O
is	O
very	O
important	O
.	O
The	O
comparison	O
between	O
SWRM	O
w/o	O
Attention	O
and	O
SWRM	O
w/o	O
Position	O
further	O
demonstrates	O
this	O
conclusion	O
.	O
SWRM	O
w/o	O
Attention	O
first	O
detects	O
the	O
right	O
position	O
and	O
then	O
incorporates	O
the	O
information	O
of	O
the	O
special	O
word	O
[	O
MASK	O
]	O
,	O
which	O
achieves	O
better	O
performance	O
than	O
SWRM	O
w/o	O
Position	O
.	O
But	O
SWRM	O
w/o	O
Attention	O
is	O
still	O
worse	O
than	O
SWRM	O
,	O
which	O
shows	O
using	O
the	O
attention	O
network	O
to	O
incorporating	O
extra	O
information	O
from	O
the	O
candidate	O
words	O
is	O
useful	O
for	O
refinement	O
.	O
Comparing	O
the	O
SWRM	O
w/o	O
Multi	O
-	O
modal	O
between	O
SWRM	O
,	O
we	O
can	O
find	O
that	O
the	O
model	O
benefits	O
from	O
the	O
visual	O
and	O
acoustic	O
features	O
.	O
It	O
is	O
in	O
line	O
with	O
our	O
expectations	O
since	O
the	O
sentiment	O
information	O
provided	O
by	O
the	O
multimodal	O
features	O
can	O
help	O
the	O
model	O
detect	O
the	O
sentiment	O
word	O
and	O
incorporate	O
the	O
sentiment	O
-	O
related	O
information	O
from	O
the	O
candidate	O
words	O
.	O

This	O
work	O
was	O
supported	O
by	O
the	O
following	O
Grants	O
:	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
(	O
No	O
.	O
62176078	O
)	O
,	O
National	O
Key	O
R&D	O
Program	O
of	O
China	O
(	O
No	O
.	O
2018YFB1005103	O
)	O
.	O

In	O
this	O
section	O
we	O
make	O
a	O
brief	O
description	O
of	O
the	O
system	O
submitted	O
for	O
the	O
different	O
subtasks	O
.	O
We	O
presented	O
our	O
submission	O
for	O
English	O
restaurants	O
dataset	O
for	O
subtask	O
1	O
,	O
slots	O
1	O
,	O
2	O
and	O
3	O
,	O
and	O
subtask	O
2	O
,	O
slots	O
1	O
and	O
3	O
.	O
For	O
English	O
laptops	O
dataset	O
we	O
sent	O
a	O
submission	O
for	O
subtasks	O
1	O
and	O
2	O
only	O
in	O
slot	O
3	O
.	O
Then	O
,	O
the	O
system	O
was	O
also	O
developed	O
for	O
Spanish	O
language	O
and	O
restaurants	O
dataset	O
in	O
subtasks	O
1	O
,	O
slots	O
1	O
and	O
2	O
and	O
subtask	O
2	O
,	O
slot	O
1	O
.	O
In	O
the	O
next	O
subsections	O
we	O
describe	O
the	O
different	O
stages	O
carried	O
out	O
for	O
obtaining	O
all	O
the	O
different	O
results	O
.	O

Subtask	O
2	O
is	O
similar	O
to	O
subtask	O
1	O
,	O
but	O
instead	O
of	O
implementing	O
aspect	O
detection	O
at	O
sentence	O
-	O
level	O
,	O
it	O
is	O
performed	O
at	O
text	O
-	O
level	O
.	O
Participants	O
are	O
asked	O
to	O
implement	O
slots	O
1	O
and	O
3	O
for	O
this	O
subtask	O
.	O
We	O
participate	O
in	O
slot	O
1	O
for	O
Spanish	O
and	O
English	O
language	O
,	O
following	O
the	O
same	O
procedure	O
for	O
both	O
.	O
Slot	O
3	O
is	O
just	O
implemented	O
for	O
English	O
language	O
for	O
restaurants	O
and	O
laptops	O
datasets	O
.	O

Similarly	O
to	O
slot	O
1	O
,	O
we	O
use	O
the	O
output	O
from	O
subtask	O
1	O
slot	O
3	O
as	O
input	O
for	O
this	O
slot	O
.	O
All	O
the	O
polarities	O
found	O
are	O
again	O
grouped	O
for	O
all	O
the	O
sentences	O
contained	O
in	O
the	O
review	O
and	O
added	O
them	O
to	O
text	O
-	O
level	O
.	O
If	O
there	O
are	O
different	O
polarities	O
for	O
the	O
same	O
category	O
,	O
some	O
rules	O
are	O
applied	O
:	O
if	O
polarities	O
are	O
negative	O
and	O
neutral	O
,	O
negative	O
is	O
finally	O
assigned	O
;	O
if	O
there	O
are	O
positive	O
and	O
neutral	O
opinions	O
,	O
positive	O
polarity	O
is	O
assigned	O
;	O
if	O
there	O
are	O
positive	O
and	O
negative	O
opinions	O
for	O
the	O
same	O
category	O
,	O
the	O
tag	O
"	O
conflict	O
"	O
is	O
assigned	O
to	O
that	O
category	O
at	O
review	O
-	O
level	O
.	O
Moreover	O
,	O
as	O
RESTAURANT#GENERAL	O
is	O
compulsory	O
for	O
every	O
review	O
,	O
if	O
no	O
sentence	O
has	O
this	O
category	O
assigned	O
,	O
we	O
take	O
into	O
account	O
all	O
the	O
polarities	O
of	O
the	O
other	O
categories	O
found	O
and	O
then	O
assign	O
the	O
polarity	O
for	O
this	O
category	O
.	O
Again	O
,	O
if	O
there	O
are	O
different	O
polarities	O
containing	O
positive	O
and	O
negative	O
,	O
"	O
conflict	O
"	O
tag	O
is	O
assigned	O
.	O
The	O
same	O
process	O
is	O
followed	O
for	O
laptops	O
dataset	O
,	O
with	O
the	O
LAPTOPS#GENERAL	O
category	O
.	O

This	O
work	O
was	O
supported	O
by	O
the	O
Spanish	O
Government	O
,	O
co	O
-	O
financed	O
by	O
the	O
European	O
Regional	O
Development	O
Fund	O
(	O
ERDF	O
)	O
under	O
project	O
TACTICA	O
.	O

Discretized	O
Integrated	O
Gradients	O
for	O
Explaining	O
Language	O
Models	O

In	O
this	O
section	O
,	O
we	O
first	O
describe	O
our	O
proposed	O
Discretized	O
integrated	O
gradients	O
(	O
DIG	O
)	O
and	O
the	O
desirable	O
explanation	O
axioms	O
satisfied	O
by	O
it	O
.	O
Then	O
we	O
describe	O
an	O
interpolation	O
algorithm	O
that	O
leverages	O
our	O
DIG	O
in	O
discrete	O
textual	O
domains	O
.	O
Please	O
refer	O
to	O
Appendix	O
A	O
for	O
a	O
brief	O
introduction	O
of	O
the	O
attribution	O
-	O
based	O
explanation	O
setup	O
and	O
the	O
integrated	O
gradients	O
method	O
.	O

Here	O
,	O
we	O
describe	O
our	O
proposed	O
interpolation	O
algorithm	O
that	O
searches	O
for	O
intermediate	O
interpolation	O
points	O
between	O
the	O
input	O
word	O
embedding	O
and	O
the	O
baseline	O
embedding	O
.	O
Once	O
we	O
have	O
the	O
desired	O
interpolation	O
points	O
,	O
we	O
can	O
use	O
Equation	O
3	O
to	O
compute	O
the	O
word	O
attributions	O
similar	O
to	O
the	O
IG	O
algorithm	O
.	O
Please	O
refer	O
to	O
Section	O
A.2	O
for	O
more	O
details	O
about	O
application	O
of	O
IG	O
to	O
text	O
.	O
Design	O
Consideration	O
.	O
First	O
,	O
we	O
discuss	O
the	O
key	O
design	O
considerations	O
we	O
need	O
to	O
consider	O
of	O
our	O
interpolation	O
algorithm	O
.	O
Clearly	O
,	O
our	O
interpolation	O
points	O
need	O
to	O
satisfy	O
the	O
monotonicity	O
constraints	O
defined	O
in	O
Equation	O
2	O
so	O
that	O
we	O
can	O
use	O
the	O
Riemann	O
sum	O
approximation	O
of	O
DIG	O
.	O
Hence	O
,	O
we	O
need	O
to	O
ensure	O
that	O
every	O
intermediate	O
point	O
lies	O
in	O
a	O
monotonic	O
path	O
.	O
Also	O
,	O
the	O
interpolation	O
points	O
should	O
lie	O
close	O
to	O
the	O
original	O
words	O
in	O
the	O
embedding	O
space	O
to	O
ensure	O
that	O
the	O
model	O
gradients	O
faithfully	O
define	O
the	O
model	O
behavior	O
.	O
Now	O
,	O
we	O
define	O
the	O
notion	O
of	O
closeness	O
for	O
our	O
specific	O
use	O
-	O
case	O
of	O
explaining	O
textual	O
models	O
.	O
To	O
calculate	O
how	O
far	O
the	O
interpolated	O
words	O
are	O
from	O
some	O
true	O
word	O
embedding	O
in	O
the	O
vocabulary	O
,	O
we	O
can	O
compute	O
the	O
distance	O
of	O
the	O
interpolated	O
point	O
from	O
the	O
nearest	O
word	O
in	O
the	O
vocabulary	O
.	O
We	O
define	O
this	O
as	O
the	O
word	O
-	O
approximation	O
error	O
(	O
WAE	O
)	O
.	O
More	O
specifically	O
,	O
if	O
w	O
k	O
denotes	O
the	O
k	O
th	O
interpolation	O
point	O
for	O
a	O
word	O
w	O
,	O
then	O
its	O
word	O
-	O
approximation	O
error	O
along	O
the	O
interpolated	O
path	O
is	O
defined	O
as	O
:	O
WAE	O
w	O
=	O
1	O
m	O
m	O
k=1	O
min	O
x	O
V	O
dist	O
(	O
w	O
k	O
−	O
x	O
)	O
,	O
(	O
4	O
)	O
where	O
V	O
is	O
the	O
embedding	O
matrix	O
of	O
all	O
the	O
words	O
in	O
the	O
vocabulary	O
.	O
WAE	O
of	O
a	O
sentence	O
is	O
the	O
average	O
WAE	O
of	O
all	O
words	O
in	O
the	O
sentence	O
.	O
Intuitively	O
,	O
minimizing	O
WAE	O
will	O
ensure	O
that	O
the	O
interpolated	O
points	O
are	O
close	O
to	O
some	O
real	O
word	O
embedding	O
in	O
the	O
vocabulary	O
which	O
in	O
turn	O
ensures	O
that	O
output	O
gradients	O
of	O
F	O
are	O
not	O
computed	O
for	O
some	O
out	O
-	O
ofdistribution	O
unseen	O
embedding	O
points	O
.	O
We	O
observe	O
that	O
to	O
minimize	O
WAE	O
without	O
the	O
monotonic	O
constraints	O
defined	O
in	O
Section	O
2.1	O
,	O
one	O
can	O
define	O
some	O
heuristic	O
to	O
search	O
for	O
interpolation	O
points	O
that	O
belong	O
to	O
the	O
set	O
V	O
(	O
i.e.	O
,	O
select	O
words	O
from	O
the	O
vocabulary	O
as	O
interpolation	O
points	O
)	O
,	O
leading	O
to	O
a	O
zero	O
WAE	O
.	O
Motivated	O
by	O
this	O
,	O
for	O
a	O
given	O
input	O
word	O
embedding	O
,	O
we	O
first	O
search	O
for	O
an	O
anchor	O
word	O
from	O
the	O
vocabulary	O
that	O
can	O
be	O
considered	O
as	O
the	O
next	O
interpolation	O
point	O
.	O
Since	O
the	O
anchor	O
point	O
need	O
not	O
be	O
monotonic	O
w.r.t	O
.	O
the	O
given	O
input	O
,	O
we	O
then	O
optimally	O
perturb	O
the	O
dimensions	O
of	O
the	O
anchor	O
word	O
so	O
that	O
they	O
satisfy	O
the	O
monotonicity	O
constraints	O
in	O
Equation	O
2	O
.	O
This	O
perturbed	O
point	O
becomes	O
our	O
first	O
interpolation	O
.	O
For	O
subsequent	O
interpolation	O
points	O
,	O
we	O
repeat	O
the	O
above	O
steps	O
using	O
the	O
previous	O
anchor	O
and	O
perturbed	O
points	O
.	O
Formally	O
,	O
we	O
break	O
our	O
interpolation	O
algorithm	O
into	O
two	O
parts	O
:	O
(	O
i	O
)	O
ANCHORSEARCH	O
:	O
In	O
this	O
step	O
,	O
given	O
the	O
initial	O
word	O
embedding	O
w	O
,	O
we	O
search	O
for	O
an	O
anchor	O
word	O
embedding	O
a	O
V	O
.	O
(	O
ii	O
)	O
MONOTONIZE	O
:	O
This	O
step	O
takes	O
the	O
anchor	O
embedding	O
a	O
and	O
modifies	O
its	O
dimensions	O
to	O
create	O
a	O
new	O
embedding	O
c	O
such	O
that	O
all	O
dimensions	O
of	O
c	O
are	O
monotonic	O
between	O
the	O
input	O
w	O
and	O
the	O
baseline	O
w	O
.	O
Overall	O
,	O
given	O
an	O
initial	O
input	O
word	O
embedding	O
w	O
and	O
a	O
baseline	O
embedding	O
w	O
,	O
our	O
interpolation	O
algorithm	O
interpolates	O
points	O
from	O
w	O
to	O
w	O
(	O
which	O
is	O
in	O
decreasing	O
order	O
of	O
k	O
in	O
Eq	O
.	O
3	O
)	O
.	O
It	O
proceeds	O
by	O
calling	O
ANCHORSEARCH	O
on	O
w	O
to	O
get	O
an	O
anchor	O
word	O
a.	O
Then	O
,	O
it	O
applies	O
MONOTONIZE	O
on	O
a	O
to	O
get	O
the	O
monotonic	O
embedding	O
c.	O
This	O
is	O
our	O
first	O
interpolated	O
point	O
(	O
in	O
reverse	O
order	O
)	O
,	O
i.e.	O
,	O
c	O
=	O
w	O
m−1	O
.	O
Now	O
,	O
the	O
a	O
becomes	O
the	O
new	O
w	O
for	O
the	O
next	O
iteration	O
and	O
the	O
process	O
continues	O
till	O
m	O
steps	O
.	O
Next	O
,	O
we	O
describe	O
in	O
detail	O
our	O
specific	O
formulations	O
of	O
the	O
MONOTONIZE	O
and	O
ANCHORSEARCH	O
algorithms	O
.	O

In	O
this	O
paper	O
,	O
we	O
proposed	O
Discretized	O
integrated	O
gradients	O
(	O
DIG	O
)	O
which	O
is	O
effective	O
in	O
explaining	O
models	O
working	O
with	O
discrete	O
text	O
data	O
.	O
Further	O
,	O
we	O
proposed	O
two	O
interpolation	O
strategies	O
-	O
DIG	O
-	O
GREEDY	O
and	O
DIG	O
-	O
MAXCOUNT	O
that	O
generate	O
non	O
-	O
linear	O
interpolation	O
paths	O
for	O
word	O
embedding	O
space	O
.	O
Finally	O
,	O
we	O
established	O
the	O
effectiveness	O
of	O
DIG	O
over	O
integrated	O
gradients	O
and	O
other	O
gradientbased	O
baselines	O
through	O
experiments	O
on	O
multiple	O
language	O
models	O
and	O
datasets	O
.	O
We	O
also	O
conduct	O
human	O
evaluations	O
and	O
find	O
that	O
DIG	O
enhances	O
human	O
trust	O
on	O
model	O
predictions	O
.	O

Attribution	O
-	O
based	O
explanations	O
generate	O
a	O
scalar	O
score	O
for	O
a	O
given	O
input	O
feature	O
that	O
indicates	O
the	O
contribution	O
(	O
or	O
importance	O
)	O
of	O
that	O
feature	O
towards	O
particular	O
label	O
(	O
Ancona	O
et	O
al	O
,	O
2018	O
)	O
.	O
Formally	O
,	O
let	O
x	O
=	O
[	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
N	O
]	O
R	O
N	O
be	O
an	O
input	O
to	O
a	O
model	O
which	O
produces	O
an	O
output	O
y	O
=	O
[	O
y	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
C	O
]	O
,	O
where	O
C	O
is	O
the	O
total	O
number	O
of	O
labels	O
.	O
For	O
a	O
given	O
label	O
(	O
usually	O
the	O
label	O
predicted	O
by	O
the	O
model	O
)	O
,	O
attribution	O
-	O
based	O
explanation	O
methods	O
compute	O
the	O
contribution	O
R	O
c	O
=	O
[	O
R	O
c	O
1	O
,	O
.	O
.	O
.	O
,	O
R	O
c	O
N	O
]	O
R	O
N	O
of	O
each	O
feature	O
.	O

In	O
this	O
section	O
,	O
we	O
redefine	O
the	O
evaluation	O
metrics	O
and	O
state	O
the	O
formulations	O
for	O
each	O
of	O
them	O
.	O
In	O
this	O
work	O
,	O
we	O
use	O
the	O
following	O
three	O
automated	O
metrics	O
:	O
Log	O
-	O
odds	O
(	O
LO	O
)	O
score	O
(	O
Shrikumar	O
et	O
al	O
,	O
2017	O
)	O
is	O
defined	O
as	O
the	O
average	O
difference	O
of	O
the	O
negative	O
logarithmic	O
probabilities	O
on	O
the	O
predicted	O
class	O
before	O
and	O
after	O
masking	O
the	O
top	O
k%	O
features	O
with	O
zero	O
padding	O
.	O
Given	O
the	O
attribution	O
scores	O
generated	O
by	O
an	O
explanation	O
algorithm	O
,	O
we	O
select	O
the	O
top	O
k%	O
words	O
based	O
on	O
their	O
attributions	O
replace	O
them	O
with	O
zero	O
padding	O
.	O
More	O
concretely	O
,	O
for	O
a	O
dataset	O
with	O
N	O
sentences	O
,	O
it	O
is	O
defined	O
as	O
:	O
log	O
−	O
odds	O
(	O
k	O
)	O
=	O
1	O
N	O
N	O
i=1	O
log	O
p	O
ŷ	O
|	O
x	O
(	O
k	O
)	O
i	O
p	O
(	O
ŷ	O
|	O
x	O
i	O
)	O
,	O
whereŷ	O
is	O
the	O
predicted	O
class	O
,	O
x	O
i	O
is	O
the	O
i	O
th	O
sentence	O
,	O
and	O
x	O
(	O
k	O
)	O
i	O
is	O
the	O
modified	O
sentence	O
with	O
top	O
k%	O
words	O
replaced	O
with	O
zero	O
padding	O
.	O
Lower	O
scores	O
are	O
better	O
.	O
Comprehensiveness	O
(	O
Comp	O
)	O
score	O
(	O
DeYoung	O
et	O
al	O
,	O
2020	O
)	O
is	O
the	O
average	O
difference	O
of	O
the	O
change	O
in	O
predicted	O
class	O
probability	O
before	O
and	O
after	O
removing	O
the	O
top	O
k%	O
features	O
.	O
Similar	O
to	O
Log	O
-	O
odds	O
,	O
this	O
measures	O
the	O
influence	O
of	O
the	O
top	O
-	O
attributed	O
words	O
on	O
the	O
model	O
's	O
prediction	O
.	O
It	O
is	O
defined	O
as	O
:	O
Comp	O
(	O
k	O
)	O
=	O
1	O
N	O
N	O
i=1	O
p	O
(	O
ŷ	O
|	O
x	O
(	O
k	O
)	O
i	O
)	O
−	O
p	O
(	O
ŷ	O
|	O
x	O
i	O
)	O
.	O
Here	O
x	O
(	O
k	O
)	O
i	O
denotes	O
the	O
modified	O
sentence	O
with	O
top	O
k%	O
words	O
deleted	O
from	O
the	O
sentence	O
.	O
Higher	O
scores	O
are	O
better	O
.	O
Sufficiency	O
(	O
Suff	O
)	O
score	O
(	O
DeYoung	O
et	O
al	O
,	O
2020	O
)	O
is	O
defined	O
as	O
the	O
average	O
difference	O
of	O
the	O
change	O
in	O
predicted	O
class	O
probability	O
before	O
and	O
after	O
keeping	O
only	O
the	O
top	O
k%	O
features	O
.	O
This	O
measures	O
the	O
adequacy	O
of	O
the	O
top	O
k%	O
attributions	O
for	O
model	O
's	O
prediction	O
.	O
It	O
is	O
defined	O
in	O
a	O
similar	O
fashion	O
as	O
comprehensiveness	O
,	O
except	O
the	O
x	O
(	O
k	O
)	O
i	O
is	O
defined	O
as	O
the	O
sentence	O
containing	O
only	O
the	O
top	O
k%	O
words	O
.	O
Lower	O
scores	O
are	O
better	O
.	O

Here	O
,	O
we	O
report	O
the	O
detailed	O
analysis	O
of	O
the	O
effect	O
of	O
increasing	O
m	O
and	O
f	O
in	O
Tables	O
7	O
and	O
8	O
respectively	O
.	O
In	O
Table	O
7	O
,	O
we	O
report	O
the	O
Log	O
-	O
odds	O
score	O
along	O
with	O
Delta	O
%	O
.	O
We	O
do	O
not	O
note	O
any	O
consistent	O
trend	O
in	O
Log	O
-	O
odds	O
with	O
increasing	O
m	O
for	O
both	O
IG	O
and	O
DIG	O
.	O
The	O
results	O
of	O
IG	O
suggest	O
that	O
,	O
as	O
long	O
as	O
the	O
Delta	O
%	O
is	O
sufficiently	O
low	O
,	O
decreasing	O
Delta	O
%	O
any	O
further	O
does	O
n't	O
impact	O
the	O
explanations	O
very	O
significantly	O
.	O
Further	O
,	O
in	O

In	O
this	O
section	O
,	O
we	O
study	O
the	O
effect	O
of	O
increasing	O
the	O
neighborhood	O
size	O
in	O
DIG	O
.	O
The	O
results	O
are	O
shown	O
in	O
Table	O
9	O
.	O
We	O
observe	O
a	O
clear	O
decreasing	O
trend	O
in	O
Delta	O
%	O
with	O
increasing	O
neighborhood	O
size	O
,	O
but	O
there	O
is	O
no	O
clear	O
trend	O
on	O
Log	O
-	O
odds	O
or	O
WAE	O
.	O
Hence	O
,	O
we	O
believe	O
that	O
the	O
neighborhood	O
size	O
has	O
little	O
impact	O
on	O
the	O
explanation	O
quality	O
,	O
but	O
we	O
should	O
still	O
ensure	O
sufficiently	O
low	O
Delta	O
.	O

In	O
this	O
section	O
,	O
we	O
briefly	O
discuss	O
the	O
computational	O
complexity	O
of	O
our	O
proposed	O
interpolation	O
strategies	O
.	O
The	O
algorithms	O
for	O
DIG	O
-	O
GREEDY	O
and	O
DIG	O
-	O
MAXCOUNT	O
are	O
presented	O
in	O
Algorithms	O
1	O
and	O
2	O
respectively	O
.	O
From	O
there	O
,	O
we	O
observe	O
that	O
both	O
our	O
algorithms	O
have	O
a	O
running	O
time	O
complexity	O
of	O
O	O
(	O
nmK	O
)	O
,	O
where	O
n	O
is	O
the	O
number	O
of	O
words	O
,	O
m	O
is	O
the	O
number	O
of	O
interpolation	O
points	O
,	O
and	O
K	O
is	O
the	O
KN	O
N	O
V	O
neighborhood	O
size	O
.	O
While	O
it	O
is	O
computationally	O
feasible	O
to	O
parallelize	O
the	O
loops	O
corresponding	O
to	O
n	O
and	O
K	O
,	O
the	O
same	O
can	O
not	O
be	O
said	O
for	O
the	O
loop	O
corresponding	O
to	O
m	O
because	O
we	O
select	O
the	O
interpolation	O
points	O
iteratively	O
.	O
Although	O
we	O
empirically	O
find	O
in	O
Section	O
F.1	O
that	O
a	O
small	O
number	O
of	O
interpolation	O
points	O
are	O
sufficient	O
to	O
calculate	O
the	O
explanations	O
,	O
we	O
believe	O
this	O
bottleneck	O
can	O
be	O
further	O
tackled	O
through	O
efficient	O
design	O
of	O
noniterative	O
search	O
algorithms	O
.	O
We	O
leave	O
this	O
for	O
future	O
works	O
.	O

Discovering	O
Black	O
Lives	O
Matter	O
Events	O
in	O
the	O
United	O
States	O
:	O
Shared	O
Task	O
3	O
,	O
CASE	O
2021	O

As	O
a	O
usability	O
analysis	O
,	O
no	O
training	O
data	O
were	O
provided	O
for	O
this	O
Task	O
.	O
Namely	O
,	O
the	O
event	O
definition	O
applied	O
for	O
coding	O
the	O
reference	O
event	O
data	O
set	O
is	O
the	O
same	O
as	O
the	O
one	O
adopted	O
for	O
Shared	O
Task	O
1	O
(	O
Hürriyetoglu	O
et	O
al	O
,	O
2021a	O
)	O
and	O
any	O
data	O
utilized	O
for	O
Task	O
1	O
and	O
Task	O
2	O
,	O
such	O
as	O
the	O
one	O
from	O
Hürriyetoglu	O
et	O
al	O
(	O
2021	O
)	O
,	O
or	O
any	O
additional	O
data	O
could	O
be	O
used	O
to	O
build	O
a	O
system	O
/	O
model	O
run	O
on	O
the	O
input	O
data	O
.	O

We	O
provide	O
two	O
types	O
of	O
input	O
data	O
.	O
The	O
first	O
is	O
a	O
generic	O
,	O
not	O
topic	O
filtered	O
collection	O
of	O
all	O
news	O
items	O
(	O
Title	O
and	O
Lead	O
Paragraph	O
)	O
from	O
the	O
New	O
York	O
Times	O
for	O
the	O
target	O
time	O
range	O
May	O
25th	O
-	O
June	O
30th	O
.	O
The	O
second	O
is	O
a	O
collection	O
of	O
Black	O
Lives	O
Matter	O
related	O
tweets	O
(	O
Giorgi	O
et	O
al	O
,	O
2020	O
)	O
.	O
New	O
York	O
Times	O
The	O
New	O
York	O
Times	O
(	O
NYT	O
)	O
data	O
sets	O
consists	O
of	O
5	O
,	O
347	O
articles	O
published	O
between	O
May	O
,	O
25	O
and	O
June	O
30	O
,	O
2020	O
.	O
The	O
data	O
associated	O
with	O
each	O
article	O
includes	O
published	O
date	O
,	O
print	O
headline	O
,	O
lead	O
paragraph	O
,	O
web	O
URL	O
,	O
authors	O
,	O
and	O
an	O
abstract	O
,	O
among	O
other	O
meta	O
-	O
data	O
.	O
This	O
is	O
a	O
general	O
set	O
of	O
NYT	O
articles	O
(	O
i.e.	O
,	O
articles	O
may	O
or	O
may	O
not	O
be	O
related	O
to	O
BLM	O
)	O
,	O
unlike	O
the	O
Twitter	O
data	O
set	O
which	O
only	O
contains	O
tweets	O
related	O
to	O
BLM	O
or	O
counter	O
protests	O
(	O
e.g.	O
,	O
All	O
Lives	O
Matter	O
and	O
Blue	O
Lives	O
Matter	O
)	O
.	O

We	O
used	O
an	O
open	O
source	O
data	O
set	O
of	O
tweets	O
containing	O
keywords	O
related	O
to	O
Black	O
Lives	O
Matter	O
and	O
the	O
counter	O
protests	O
:	O
All	O
Lives	O
Matter	O
and	O
Blue	O
Lives	O
Matter	O
.	O
While	O
this	O
data	O
set	O
contains	O
tweets	O
dating	O
back	O
to	O
the	O
origins	O
of	O
the	O
Black	O
Lives	O
Matter	O
movement	O
,	O
the	O
tweets	O
used	O
in	O
this	O
task	O
are	O
limited	O
to	O
the	O
date	O
range	O
:	O
May	O
25	O
,	O
2020	O
(	O
the	O
date	O
of	O
George	O
Floyd	O
's	O
murder	O
)	O
to	O
June	O
30	O
,	O
2020	O
.	O
These	O
tweets	O
were	O
pulled	O
in	O
real	O
time	O
using	O
the	O
Twitter	O
API	O
's	O
keyword	O
matching	O
with	O
the	O
following	O
three	O
keywords	O
:	O
BlackLivesMatter	O
,	O
Al	O
-	O
lLivesMatter	O
,	O
and	O
BlueLivesMatter	O
.	O
This	O
data	O
set	O
consists	O
of	O
30	O
,	O
160	O
,	O
837	O
tweets	O
.	O
Participants	O
were	O
given	O
full	O
access	O
to	O
each	O
tweet	O
's	O
meta	O
-	O
data	O
(	O
including	O
the	O
tweet	O
's	O
text	O
)	O
,	O
which	O
could	O
include	O
URLs	O
,	O
location	O
information	O
,	O
and	O
dates	O
.	O

System	O
performance	O
is	O
evaluated	O
by	O
computing	O
correlation	O
coefficients	O
on	O
event	O
counts	O
aggregated	O
on	O
cell	O
-	O
days	O
,	O
using	O
uniform	O
grid	O
cells	O
of	O
approximately	O
55	O
kilometers	O
sides	O
from	O
the	O
PRIO	O
-	O
GRID	O
data	O
set	O
(	O
Tollefsen	O
et	O
al	O
,	O
2012	O
)	O
.	O
We	O
use	O
these	O
analytical	O
measures	O
as	O
a	O
proxy	O
to	O
the	O
spatio	O
-	O
temporal	O
pattern	O
of	O
the	O
BLM	O
protest	O
movement	O
.	O

In	O
order	O
to	O
be	O
joined	O
with	O
PRIO	O
-	O
GRID	O
shapefiles	O
,	O
string	O
-	O
like	O
location	O
information	O
of	O
system	O
output	O
data	O
had	O
to	O
be	O
normalized	O
to	O
coordinate	O
pairs	O
.	O
To	O
do	O
this	O
we	O
used	O
the	O
OpenStreetMap	O
Nominatim	O
search	O
API	O
5	O
.	O
For	O
structured	O
location	O
name	O
representations	O
(	O
i.e.	O
,	O
city	O
,	O
state	O
,	O
country	O
)	O
we	O
used	O
a	O
parametric	O
search	O
,	O
otherwise	O
we	O
used	O
free	O
-	O
form	O
query	O
strings	O
.	O
We	O
note	O
that	O
geographical	O
coordinate	O
conversion	O
from	O
Nominatim	O
places	O
the	O
event	O
at	O
the	O
geographical	O
centroid	O
of	O
the	O
polygon	O
of	O
the	O
assigned	O
administrative	O
unit	O
.	O
In	O
our	O
evaluation	O
,	O
we	O
discarded	O
the	O
system	O
output	O
event	O
records	O
with	O
no	O
source	O
location	O
information	O
or	O
whose	O
string	O
-	O
like	O
location	O
attribute	O
returned	O
null	O
results	O
in	O
Nominatim	O
API	O
.	O

We	O
use	O
the	O
cell	O
-	O
days	O
counts	O
for	O
two	O
different	O
analysis	O
:	O
the	O
correlation	O
with	O
the	O
total	O
daily	O
"	O
protest	O
cell	O
"	O
counts	O
(	O
i.e.	O
,	O
time	O
trends	O
alone	O
)	O
and	O
the	O
event	O
counts	O
for	O
each	O
cell	O
-	O
day	O
(	O
i.e.	O
,	O
spatial	O
and	O
temporal	O
trends	O
together	O
)	O
.	O

As	O
a	O
baseline	O
,	O
we	O
used	O
the	O
output	O
from	O
NEXUS	O
,	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
engine	O
for	O
events	O
detection	O
from	O
news	O
(	O
Tanev	O
et	O
al	O
,	O
2008	O
)	O
that	O
has	O
been	O
used	O
in	O
the	O
area	O
of	O
security	O
and	O
disaster	O
management	O
6	O
.	O
We	O
denote	O
this	O
system	O
as	O
Baseline	O
throughout	O
.	O
Nexus	O
is	O
based	O
on	O
a	O
blend	O
of	O
rule	O
-	O
based	O
cascaded	O
grammars	O
for	O
detection	O
event	O
slots	O
(	O
i.e.	O
perpetrator	O
,	O
various	O
types	O
of	O
affected	O
people	O
,	O
infrastructure	O
and	O
vehicle	O
targets	O
and	O
weapons	O
used	O
)	O
,	O
and	O
a	O
combination	O
of	O
keyword	O
-	O
based	O
and	O
statistical	O
classifiers	O
for	O
detection	O
of	O
event	O
classes	O
.	O
The	O
dictionaries	O
underlying	O
the	O
extraction	O
grammars	O
of	O
the	O
system	O
have	O
been	O
learned	O
using	O
weakly	O
supervised	O
lexical	O
learning	O
on	O
generic	O
news	O
corpora	O
.	O
No	O
learning	O
was	O
performed	O
on	O
domain	O
corpora	O
in	O
protest	O
movements	O
or	O
related	O
themes	O
.	O
Details	O
on	O
Nexus	O
full	O
taxonomy	O
of	O
event	O
categories	O
can	O
be	O
found	O
in	O
Atkinson	O
et	O
al	O
(	O
2017	O
)	O
.	O
For	O
this	O
task	O
,	O
we	O
filter	O
the	O
events	O
belonging	O
to	O
the	O
following	O
type	O
set	O
:	O
Disorder	O
/	O
Protest	O
/	O
Mutiny	O
,	O
Boycott	O
/	O
Strike	O
,	O
Public	O
Demonstration	O
,	O
Riot	O
/	O
Turmoil	O
,	O
Sabotage	O
/	O
Impede	O
,	O
Mutiny	O
.	O
NEXUS	O
performs	O
event	O
geocoding	O
by	O
(	O
1	O
)	O
matching	O
populated	O
place	O
names	O
from	O
the	O
GeoNames	O
gazetteer	O
7	O
in	O
the	O
news	O
item	O
;	O
(	O
2	O
)	O
resolving	O
them	O
into	O
unique	O
location	O
entities	O
via	O
disambiguation	O
heuristics	O
(	O
Pouliquen	O
et	O
al	O
,	O
2006	O
)	O
;	O
and	O
(	O
3	O
)	O
selecting	O
a	O
single	O
main	O
event	O
location	O
based	O
on	O
the	O
text	O
proximity	O
with	O
the	O
matched	O
event	O
components	O
(	O
see	O
the	O
slots	O
above	O
)	O
in	O
the	O
news	O
article	O
.	O
In	O
order	O
to	O
mitigate	O
the	O
lack	O
of	O
geographical	O
context	O
in	O
the	O
tweet	O
body	O
,	O
when	O
processing	O
the	O
Twitter	O
data	O
,	O
we	O
ran	O
Nexus	O
on	O
an	O
enriched	O
text	O
,	O
which	O
included	O
the	O
String	O
value	O
of	O
the	O
full	O
name	O
field	O
in	O
the	O
Place	O
child	O
object	O
of	O
the	O
tweet	O
,	O
whenever	O
that	O
was	O
available	O
8	O
.	O
This	O
resulted	O
in	O
a	O
small	O
fraction	O
of	O
32	O
,	O
085	O
tweets	O
with	O
geographical	O
information	O
(	O
out	O
of	O
the	O
roughly	O
30	O
million	O
tweets	O
originally	O
sampled	O
)	O
.	O
For	O
the	O
sake	O
of	O
comparison	O
,	O
we	O
shared	O
with	O
participants	O
this	O
subset	O
of	O
tweets	O
,	O
together	O
with	O
the	O
assigned	O
location	O
.	O

The	O
author	O
from	O
Koc	O
University	O
was	O
funded	O
by	O
the	O
European	O
Research	O
Council	O
(	O
ERC	O
)	O
Starting	O
Grant	O
714868	O
awarded	O
to	O
Dr.	O
Erdem	O
Yörük	O
for	O
his	O
project	O
Emerging	O
Welfare	O
.	O
The	O
authors	O
from	O
the	O
National	O
Institute	O
on	O
Drug	O
Abuse	O
were	O
supported	O
in	O
part	O
by	O
the	O
Intramural	O
Research	O
Program	O
of	O
the	O
NIH	O
,	O
National	O
Institute	O
on	O
Drug	O
Abuse	O
(	O
NIDA	O
)	O
.	O

This	O
section	O
describes	O
the	O
structural	O
causal	O
model	O
(	O
SCM	O
)	O
for	O
FSED	O
,	O
illustrated	O
in	O
Figure	O
1	O
(	O
a	O
)	O
.	O
Note	O
that	O
,	O
we	O
omit	O
the	O
causal	O
structure	O
of	O
the	O
query	O
for	O
simplicity	O
since	O
it	O
is	O
the	O
same	O
as	O
the	O
support	O
set	O
.	O
Concretely	O
,	O
the	O
SCM	O
formulates	O
the	O
data	O
distribution	O
of	O
FSED	O
:	O
1	O
)	O
Starting	O
from	O
an	O
event	O
E	O
we	O
want	O
to	O
describe	O
(	O
in	O
Figure	O
1	O
(	O
a	O
)	O
is	O
an	O
Attack	O
in	O
Iraqi	O
)	O
.	O
2	O
)	O
The	O
path	O
E	O
T	O
indicates	O
the	O
trigger	O
decision	O
process	O
,	O
i.e.	O
,	O
selecting	O
words	O
or	O
phrases	O
(	O
in	O
Figure	O
1	O
(	O
a	O
)	O
is	O
fire	O
)	O
which	O
can	O
almost	O
clearly	O
express	O
the	O
event	O
occurrence	O
(	O
Doddington	O
et	O
al	O
,	O
2004	O
)	O
.	O
3	O
)	O
The	O
path	O
E	O
C	O
T	O
indicates	O
that	O
a	O
set	O
of	O
contexts	O
are	O
generated	O
depending	O
on	O
both	O
the	O
event	O
and	O
the	O
trigger	O
,	O
which	O
provides	O
background	O
information	O
and	O
organizes	O
this	O
information	O
depending	O
on	O
the	O
trigger	O
.	O
For	O
instance	O
,	O
the	O
context	O
"	O
They	O
killed	O
by	O
hostile	O
[	O
fire	O
]	O
in	O
Iraqi	O
"	O
provides	O
the	O
place	O
,	O
the	O
role	O
and	O
the	O
consequences	O
of	O
the	O
event	O
,	O
and	O
this	O
information	O
is	O
organized	O
following	O
the	O
structure	O
determined	O
by	O
fire	O
.	O
4	O
)	O
an	O
event	O
instance	O
is	O
generated	O
by	O
combining	O
one	O
of	O
the	O
contexts	O
in	O
C	O
and	O
one	O
of	O
the	O
triggers	O
in	O
T	O
via	O
the	O
path	O
C	O
S	O
T	O
.	O
5	O
)	O
Finally	O
,	O
a	O
matching	O
between	O
query	O
and	O
support	O
set	O
is	O
generated	O
through	O
S	O
Y	O
Q.	O
Conventional	O
learning	O
criteria	O
for	O
FSED	O
directly	O
optimize	O
towards	O
the	O
conditional	O
distribution	O
P	O
(	O
Y	O
|	O
S	O
,	O
Q	O
)	O
.	O
However	O
,	O
from	O
the	O
SCM	O
,	O
we	O
found	O
that	O
the	O
backdoor	O
path	O
C	O
T	O
Y	O
pass	O
on	O
associations	O
(	O
Pearl	O
et	O
al	O
,	O
2016	O
)	O
and	O
mislead	O
the	O
learning	O
with	O
spurious	O
correlation	O
.	O
Consequently	O
,	O
the	O
learning	O
procedure	O
towards	O
P	O
(	O
Y	O
|	O
S	O
,	O
Q	O
)	O
will	O
mistakenly	O
regard	O
the	O
effects	O
of	O
triggers	O
as	O
the	O
effects	O
of	O
contexts	O
,	O
and	O
therefore	O
overfit	O
the	O
trigger	O
information	O
.	O

To	O
further	O
demonstrate	O
the	O
effectiveness	O
of	O
the	O
proposed	O
method	O
,	O
we	O
also	O
conduct	O
experiments	O
under	O
different	O
FSED	O
settings	O
:	O
1	O
)	O
The	O
primal	O
episodebased	O
settings	O
(	O
Episode	O
)	O
,	O
which	O
is	O
the	O
5	O
+	O
1	O
-	O
way	O
5	O
-	O
shot	O
settings	O
in	O
Lai	O
et	O
al	O
(	O
2020	O
)	O
.	O
2	O
)	O
Episode	O
+	O
ambiguous	O
instances	O
(	O
Ambiguity	O
)	O
,	O
which	O
samples	O
some	O
additional	O
negative	O
query	O
instances	O
that	O
include	O
words	O
same	O
as	O
triggers	O
in	O
support	O
set	O
to	O
verify	O
whether	O
models	O
overfit	O
the	O
triggers	O
.	O
The	O
performance	O
of	O
different	O
models	O
with	O
different	O
settings	O
is	O
shown	O
in	O
Figure	O
2	O
.	O
We	O
can	O
see	O
that	O
:	O
1	O
)	O
Generally	O
speaking	O
,	O
all	O
models	O
can	O
achieve	O
better	O
performance	O
on	O
Episode	O
because	O
correctly	O
recognize	O
high	O
-	O
frequent	O
triggers	O
can	O
achieve	O
good	O
performance	O
in	O
this	O
setting	O
.	O
Consequently	O
,	O
the	O
performance	O
under	O
this	O
setting	O
can	O
not	O
well	O
represent	O
how	O
FSED	O
is	O
influenced	O
by	O
trigger	O
overfitting	O
.	O
2	O
)	O
The	O
performance	O
of	O
all	O
models	O
dropped	O
on	O
Ambiguity	O
setting	O
,	O
which	O
suggests	O
that	O
trigger	O
overfitting	O
has	O
a	O
significant	O
impact	O
on	O
FSED	O
.	O
3	O
)	O
Our	O
method	O
still	O
maintains	O
good	O
performance	O
on	O
Ambiguity	O
,	O
which	O
indicates	O
that	O
our	O
method	O
can	O
alleviate	O
the	O
trigger	O
curse	O
problem	O
by	O
optimizing	O
towards	O
the	O
underlying	O
causality	O
.	O

We	O
select	O
ambiguous	O
cases	O
(	O
in	O
Table	O
2	O
)	O
to	O
better	O
illustrate	O
the	O
effectiveness	O
of	O
our	O
method	O
.	O
For	O
Query	O
1	O
,	O
FS	O
-	O
Base	O
wrongly	O
detects	O
the	O
word	O
run	O
to	O
be	O
a	O
trigger	O
word	O
.	O
In	O
Support	O
set	O
1	O
,	O
run	O
means	O
nomi	O
-	O
nating	O
while	O
run	O
means	O
managing	O
in	O
Query	O
1	O
.	O
FS	O
-	O
Base	O
fails	O
to	O
recognize	O
such	O
different	O
sense	O
of	O
word	O
under	O
context	O
.	O
For	O
Query	O
2	O
,	O
FS	O
-	O
Base	O
makes	O
mistake	O
again	O
on	O
the	O
ambiguous	O
word	O
suspect	O
.	O
Even	O
though	O
suspect	O
is	O
the	O
noun	O
form	O
of	O
suspected	O
in	O
Support	O
set	O
2	O
,	O
it	O
does	O
not	O
trigger	O
a	O
Suspicion	O
event	O
in	O
Query	O
2	O
.	O
In	O
contract	O
to	O
FS	O
-	O
Base	O
,	O
our	O
approach	O
is	O
able	O
to	O
handle	O
both	O
cases	O
correctly	O
,	O
illustrating	O
its	O
effectiveness	O
.	O

This	O
paper	O
proposes	O
to	O
revisit	O
the	O
trigger	O
curse	O
in	O
FSED	O
from	O
a	O
causal	O
view	O
.	O
Specifically	O
,	O
we	O
identify	O
the	O
cause	O
of	O
the	O
trigger	O
curse	O
problem	O
from	O
a	O
structural	O
causal	O
model	O
,	O
and	O
then	O
solve	O
the	O
problem	O
through	O
casual	O
intervention	O
via	O
backdoor	O
adjustment	O
.	O
Experimental	O
results	O
demonstrate	O
the	O
effectiveness	O
and	O
robustness	O
of	O
our	O
methods	O
.	O

We	O
prove	O
the	O
backdoor	O
adjustment	O
for	O
our	O
SCM	O
using	O
the	O
rules	O
of	O
do	O
-	O
calculus	O
(	O
Pearl	O
,	O
1995	O
)	O
.	O
For	O
a	O
causal	O
graph	O
G	O
,	O
let	O
G	O
X	O
denote	O
the	O
graph	O
where	O
all	O
of	O
the	O
incoming	O
edges	O
to	O
Node	O
X	O
are	O
removed	O
.	O
let	O
G	O
X	O
denote	O
the	O
graph	O
where	O
all	O
of	O
the	O
outgoing	O
edges	O
from	O
Node	O
X	O
are	O
removed	O
.	O
⊥	O
⊥	O
G	O
denotes	O
d	O
-	O
separation	O
in	O
G.	O
D	O
-	O
separation	O
(	O
Pearl	O
,	O
2014	O
)	O
:	O
Two	O
(	O
sets	O
of	O
)	O
nodes	O
X	O
and	O
Y	O
are	O
d	O
-	O
separation	O
by	O
a	O
set	O
of	O
nodes	O
Z	O
(	O
i.e.	O
X	O
⊥	O
⊥	O
G	O
Y	O
|	O
Z	O
)	O
if	O
all	O
of	O
the	O
paths	O
between	O
(	O
any	O
node	O
in	O
)	O
X	O
and	O
(	O
any	O
node	O
in	O
)	O
Y	O
are	O
blocked	O
by	O
Z.	O
The	O
rules	O
of	O
do	O
-	O
calculus	O
are	O
:	O
Rule	O
1	O
P	O
(	O
y	O
|	O
do	O
(	O
t	O
)	O
,	O
z	O
,	O
w	O
)	O
=	O
P	O
(	O
y	O
|	O
do	O
(	O
t	O
)	O
,	O
w	O
)	O
if	O
Y	O
⊥	O
⊥	O
G	O
T	O
Z	O
|	O
T	O
,	O
W	O
Rule	O
2	O
P	O
(	O
y	O
|	O
do	O
(	O
t	O
)	O
,	O
do	O
(	O
z	O
)	O
,	O
w	O
)	O
=	O
P	O
(	O
y	O
|	O
do	O
(	O
t	O
)	O
,	O
z	O
,	O
w	O
)	O
if	O
Y	O
⊥	O
⊥	O
G	O
T	O
Z	O
Z	O
|	O
T	O
,	O
W	O
Rule	O
3	O
P	O
(	O
y	O
|	O
do	O
(	O
t	O
)	O
,	O
do	O
(	O
z	O
)	O
,	O
w	O
)	O
=	O
P	O
(	O
y	O
|	O
do	O
(	O
t	O
)	O
,	O
w	O
)	O
if	O
Y	O
⊥	O
⊥	O
G	O
T	O
Z	O
(	O
W	O
)	O
Z	O
|	O
T	O
,	O
W	O
(	O
5	O
)	O
where	O
Z	O
(	O
W	O
)	O
denotes	O
the	O
set	O
of	O
nodes	O
of	O
Z	O
that	O
are	O
n't	O
ancestors	O
of	O
any	O
node	O
of	O
W	O
in	O
G	O
T	O
.	O
We	O
can	O
prove	O
our	O
interventional	O
distribution	O
P	O
(	O
Y	O
|	O
do	O
(	O
C	O
=	O
C	O
)	O
,	O
E	O
=	O
e	O
)	O
:	O
Step	O
1	O
Using	O
the	O
law	O
of	O
total	O
probability	O
:	O
Step	O
2	O
Using	O
the	O
law	O
of	O
conditional	O
probability	O
:	O
P	O
(	O
Y	O
|	O
do	O
(	O
C	O
=	O
C	O
)	O
,	O
E	O
=	O
e	O
,	O
Q	O
=	O
q	O
)	O
=	O
t	O
T	O
P	O
(	O
Y	O
|	O
do	O
(	O
C	O
=	O
C	O
)	O
,	O
E	O
=	O
e	O
,	O
Q	O
=	O
q	O
)	O
=	O
t	O
T	O
s	O
S	O
[	O
P	O
(	O
Y	O
|	O
do	O
(	O
C	O
)	O
,	O
e	O
,	O
t	O
,	O
s	O
,	O
q	O
)	O
×	O
P	O
(	O
s	O
|	O
do	O
(	O
C	O
)	O
,	O
e	O
,	O
t	O
,	O
q	O
)	O
P	O
(	O
t	O
|	O
do	O
(	O
C	O
)	O
,	O
e	O
,	O
q	O
)	O
]	O
Step	O
3	O
Using	O
the	O
Rule	O
3	O
:	O
P	O
(	O
Y	O
|	O
do	O
(	O
C	O
=	O
C	O
)	O
,	O
E	O
=	O
e	O
,	O
Q	O
=	O
q	O
)	O
=	O
t	O
T	O
s	O
S	O
[	O
P	O
(	O
Y	O
|	O
e	O
,	O
t	O
,	O
s	O
,	O
q	O
)	O
×	O
P	O
(	O
s	O
|	O
do	O
(	O
C	O
)	O
,	O
e	O
,	O
t	O
,	O
q	O
)	O
P	O
(	O
t	O
|	O
e	O
,	O
q	O
)	O
]	O
Step	O
4	O
Using	O
the	O
Rule	O
1	O
:	O
P	O
(	O
Y	O
|	O
do	O
(	O
C	O
=	O
C	O
)	O
,	O
E	O
=	O
e	O
,	O
Q	O
=	O
q	O
)	O
=	O
t	O
T	O
s	O
S	O
P	O
(	O
Y	O
|	O
s	O
,	O
q	O
)	O
P	O
(	O
s	O
|	O
do	O
(	O
C	O
)	O
,	O
t	O
)	O
P	O
(	O
t	O
|	O
e	O
)	O
Step	O
5	O
Using	O
the	O
Rule	O
2	O
:	O
P	O
(	O
Y	O
|	O
do	O
(	O
C	O
=	O
C	O
)	O
,	O
E	O
=	O
e	O
,	O
Q	O
=	O
q	O
)	O
=	O
t	O
T	O
s	O
S	O
P	O
(	O
Y	O
|	O
s	O
,	O
q	O
)	O
P	O
(	O
s	O
|	O
C	O
,	O
t	O
)	O
P	O
(	O
t	O
|	O
e	O
)	O

The	O
hyperparameter	O
is	O
shown	O
in	O
Table	O
5	O
.	O
During	O
training	O
,	O
the	O
support	O
set	O
and	O
the	O
query	O
is	O
sampled	O
in	O
training	O
set	O
,	O
the	O
query	O
contains	O
2	O
positive	O
instances	O
and	O
10	O
negative	O
instances	O
(	O
5	O
times	O
of	O
positive	O
instances	O
)	O
.	O
During	O
validating	O
,	O
the	O
support	O
set	O
and	O
the	O
query	O
is	O
sampled	O
in	O
dev	O
set	O
,	O
the	O
query	O
contains	O
10	O
positive	O
instances	O
and	O
100	O
negative	O
instances	O
(	O
10	O
times	O
of	O
positive	O

We	O
thank	O
the	O
reviewers	O
for	O
their	O
insightful	O
comments	O
and	O
helpful	O
suggestions	O
.	O
This	O
research	O
work	O
is	O
supported	O
by	O
National	O
Key	O
R&D	O
Program	O
of	O
China	O
under	O
Grant	O
2018YFB1005100	O
,	O
the	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
under	O
Grants	O
no	O
.	O
62106251	O
and	O
62076233	O
,	O
and	O
in	O
part	O
by	O
the	O
Youth	O
Innovation	O
Promotion	O
Association	O
CAS	O
(	O
2018141	O
)	O
.	O

instances	O
)	O
.	O
The	O
results	O
of	O
dev	O
set	O
are	O
shown	O
in	O
Table	O
3	O
.	O
For	O
FS	O
-	O
Causal	O
,	O
we	O
found	O
that	O
there	O
is	O
an	O
impact	O
on	O
whether	O
backdoor	O
adjustment	O
is	O
applied	O
separately	O
to	O
the	O
support	O
set	O
and	O
query	O
,	O
as	O
shown	O
in	O
Table	O
4	O
.	O
Based	O
on	O
the	O
best	O
results	O
of	O
the	O
dev	O
set	O
,	O
we	O
evaluate	O
it	O
on	O
the	O
test	O
set	O
.	O

Context	O
-	O
aware	O
Interactive	O
Attention	O
for	O
Multi	O
-	O
modal	O
Sentiment	O
and	O
Emotion	O
Analysis	O

In	O
this	O
section	O
,	O
we	O
present	O
our	O
experimental	O
results	O
along	O
with	O
necessary	O
analysis	O
.	O
We	O
also	O
compare	O
our	O
obtained	O
results	O
with	O
several	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
systems	O
.	O

The	O
research	O
reported	O
here	O
is	O
partially	O
supported	O
by	O
SkyMap	O
Global	O
India	O
Private	O
Limited	O
.	O
Asif	O
Ekbal	O
acknowledges	O
the	O
Young	O
Faculty	O
Research	O
Fellowship	O
(	O
YFRF	O
)	O
,	O
supported	O
by	O
Visvesvaraya	O
PhD	O
scheme	O
for	O
Electronics	O
and	O
IT	O
,	O
Ministry	O
of	O
Electronics	O
and	O
Information	O
Technology	O
(	O
MeitY	O
)	O
,	O
Government	O
of	O
India	O
,	O
being	O
implemented	O
by	O
Digital	O
India	O
Corporation	O
(	O
formerly	O
Media	O
Lab	O
Asia	O
)	O
.	O

As	O
an	O
extension	O
of	O
SE	O
-	O
Graph	O
,	O
IRSE	O
-	O
Graph	O
can	O
be	O
denoted	O
as	O
G=	O
(	O
V	O
,	O
E	O
,	O
W	O
)	O
,	O
where	O
V	O
and	O
E	O
share	O
the	O
same	O
definitions	O
with	O
those	O
of	O
SE	O
-	O
Graph	O
.	O
Particularly	O
,	O
in	O
IRSE	O
-	O
Graph	O
,	O
each	O
ss	O
-	O
edge	O
e	O
i	O
,	O
i	O
is	O
a	O
directed	O
one	O
with	O
a	O
weight	O
w	O
i	O
,	O
i	O
W	O
indicating	O
the	O
probability	O
of	O
sentence	O
s	O
i	O
occurring	O
before	O
sentence	O
s	O
i	O
.	O
Meanwhile	O
,	O
there	O
must	O
exist	O
a	O
corresponding	O
ssedge	O
e	O
i	O
,	O
i	O
with	O
the	O
weight	O
w	O
i	O
,	O
i	O
=	O
1−w	O
i	O
,	O
i	O
denoting	O
the	O
probability	O
of	O
s	O
i	O
appearing	O
after	O
s	O
i	O
.	O
For	O
example	O
,	O
in	O
Figure	O
2	O
,	O
for	O
two	O
linked	O
sentence	O
nodes	O
v	O
1	O
and	O
v	O
2	O
,	O
there	O
exist	O
two	O
ss	O
-	O
edges	O
e	O
1	O
,	O
2	O
and	O
e	O
2	O
,	O
1	O
with	O
weights	O
w	O
1	O
,	O
2	O
and	O
w	O
2	O
,	O
1	O
respectively	O
,	O
both	O
of	O
which	O
are	O
iteratively	O
updated	O
during	O
constructing	O
IRSE	O
-	O
Graph	O
.	O

Since	O
pairwise	O
ordering	O
plays	O
a	O
crucial	O
role	O
in	O
our	O
proposed	O
framework	O
,	O
we	O
first	O
compare	O
the	O
performance	O
of	O
different	O
classifiers	O
on	O
various	O
datasets	O
.	O
the	O
predictions	O
of	O
pairwise	O
orderings	O
.	O

To	O
provide	O
more	O
experimental	O
results	O
,	O
we	O
summarize	O
the	O
runtime	O
on	O
the	O
validation	O
sets	O
and	O
the	O
numbers	O
of	O
parameters	O
for	O
our	O
enhanced	O
models	O
and	O
baseline	O
SE	O
-	O
GRN	O
in	O
Table	O
6	O
.	O

The	O
project	O
was	O
supported	O
by	O
National	O
Key	O
Research	O
and	O
Development	O
Program	O
of	O
China	O
(	O
No	O
.	O
2020AAA0108004	O
)	O
,	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
(	O
No	O
.	O
61672440	O
)	O
,	O
Natural	O
Science	O
Foundation	O
of	O
Fujian	O
Province	O
of	O
China	O
(	O
No	O
.	O
2020J06001	O
)	O
,	O
and	O
Youth	O
Innovation	O
Fund	O
of	O
Xiamen	O
(	O
No	O
.	O
3502Z20206059	O
)	O
.	O
We	O
also	O
thank	O
the	O
reviewers	O
for	O
their	O
insightful	O
comments	O
.	O

Who	O
wrote	O
this	O
book	O
?	O
A	O
challenge	O
for	O
e	O
-	O
commerce	O

Unlike	O
brick	O
-	O
and	O
-	O
mortar	O
stores	O
,	O
e	O
-	O
commerce	O
websites	O
can	O
list	O
hundreds	O
of	O
millions	O
of	O
products	O
,	O
with	O
thousands	O
of	O
new	O
products	O
entering	O
their	O
catalogs	O
every	O
day	O
.	O
The	O
availability	O
and	O
the	O
reliability	O
of	O
the	O
information	O
on	O
the	O
products	O
,	O
or	O
product	O
data	O
,	O
is	O
crucial	O
for	O
the	O
products	O
to	O
be	O
found	O
by	O
the	O
users	O
via	O
textual	O
or	O
visual	O
search	O
,	O
or	O
using	O
faceted	O
navigation	O
.	O
Books	O
constitute	O
a	O
prominent	O
part	O
of	O
many	O
large	O
ecommerce	O
catalogs	O
.	O
Relevant	O
book	O
properties	O
include	O
:	O
title	O
,	O
author	O
(	O
s	O
)	O
,	O
format	O
,	O
edition	O
,	O
and	O
publication	O
date	O
,	O
among	O
others	O
.	O
In	O
this	O
work	O
,	O
we	O
focus	O
on	O
the	O
names	O
of	O
book	O
authors	O
,	O
as	O
they	O
are	O
found	O
to	O
be	O
extremely	O
relevant	O
to	O
the	O
user	O
and	O
are	O
commonly	O
used	O
in	O
search	O
queries	O
on	O
e	O
-	O
commerce	O
websites	O
,	O
but	O
suffer	O
from	O
considerable	O
variability	O
and	O
noise	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
there	O
is	O
no	O
large	O
-	O
scale	O
public	O
dataset	O
for	O
books	O
that	O
captures	O
the	O
variability	O
arising	O
on	O
e	O
-	O
commerce	O
marketplaces	O
from	O
user	O
-	O
generated	O
input	O
.	O
Thus	O
,	O
in	O
this	O
work	O
we	O
use	O
product	O
data	O
from	O
Rakuten	O
France	O
(	O
RFR	O
)	O
.	O
1	O
1	O
https://fr.shopping.rakuten.com	O
The	O
variability	O
and	O
noise	O
is	O
evident	O
in	O
the	O
RFR	O
dataset	O
.	O
For	O
example	O
,	O
books	O
written	O
by	O
F.	O
Scott	O
Fitzgerald	O
are	O
also	O
listed	O
with	O
the	O
following	O
author	O
's	O
names	O
:	O
"	O
Francis	O
Scott	O
Fitzgerald	O
"	O
(	O
full	O
name	O
)	O
,	O
"	O
Fitzgerald	O
,	O
F.	O
Scott	O
"	O
(	O
inversion	O
of	O
the	O
first	O
and	O
last	O
name	O
)	O
,	O
"	O
Fitzgerald	O
"	O
(	O
last	O
name	O
only	O
)	O
,	O
"	O
F.	O
Scott	O
Fitgerald	O
"	O
(	O
misspelling	O
of	O
the	O
last	O
name	O
)	O
,	O
"	O
F	O
SCOTT	O
FITZGERALD	O
"	O
(	O
capitalization	O
and	O
different	O
typological	O
conventions	O
)	O
,	O
as	O
well	O
as	O
several	O
combinations	O
of	O
those	O
variations	O
.	O
The	O
variability	O
of	O
the	O
possible	O
spellings	O
for	O
an	O
author	O
's	O
name	O
is	O
very	O
hard	O
to	O
capture	O
using	O
rules	O
,	O
even	O
more	O
so	O
for	O
names	O
which	O
are	O
not	O
primarily	O
written	O
in	O
latin	O
alphabet	O
(	O
such	O
as	O
arabic	O
or	O
asian	O
names	O
)	O
,	O
for	O
names	O
containing	O
titles	O
(	O
such	O
as	O
"	O
Dr.	O
"	O
or	O
"	O
Pr	O
.	O
"	O
)	O
,	O
and	O
for	O
pen	O
names	O
which	O
may	O
not	O
follow	O
the	O
usual	O
conventions	O
.	O
This	O
motivated	O
us	O
to	O
explore	O
automated	O
techniques	O
for	O
normalizing	O
the	O
authors	O
'	O
names	O
to	O
their	O
best	O
known	O
(	O
"	O
canonical	O
"	O
)	O
spellings	O
.	O
Fortunately	O
,	O
a	O
wealth	O
of	O
open	O
databases	O
exist	O
for	O
books	O
,	O
making	O
it	O
possible	O
to	O
match	O
a	O
significant	O
fraction	O
of	O
the	O
books	O
listed	O
in	O
e	O
-	O
commerce	O
catalogs	O
.	O
While	O
not	O
always	O
clean	O
and	O
unambiguous	O
,	O
this	O
information	O
is	O
extremely	O
valuable	O
and	O
enables	O
us	O
to	O
build	O
datasets	O
of	O
name	O
variants	O
,	O
used	O
to	O
train	O
machine	O
learning	O
systems	O
to	O
normalize	O
authors	O
'	O
names	O
.	O
To	O
this	O
end	O
,	O
in	O
addition	O
to	O
the	O
match	O
with	O
open	O
databases	O
,	O
we	O
will	O
explore	O
two	O
different	O
approaches	O
:	O
approximate	O
match	O
with	O
known	O
authors	O
'	O
names	O
using	O
Siamese	O
neural	O
networks	O
,	O
and	O
direct	O
correction	O
of	O
the	O
provided	O
author	O
's	O
name	O
using	O
sequenceto	O
-	O
sequence	O
learning	O
with	O
neural	O
networks	O
.	O
Then	O
,	O
an	O
additional	O
machine	O
learning	O
component	O
is	O
used	O
to	O
rank	O
the	O
results	O
.	O
The	O
rest	O
of	O
the	O
paper	O
is	O
organized	O
as	O
follows	O
:	O
we	O
present	O
the	O
data	O
from	O
RFR	O
and	O
from	O
the	O
open	O
databases	O
in	O
Section	O
2	O
,	O
before	O
turning	O
to	O
the	O
experimental	O
setup	O
for	O
the	O
overall	O
system	O
and	O
for	O
each	O
of	O
its	O
components	O
in	O
Section	O
3	O
.	O
Finally	O
,	O
we	O
give	O
results	O
in	O
Section	O
4	O
,	O
we	O
present	O
related	O
works	O
in	O
Section	O
5	O
,	O
and	O
conclude	O
in	O
Section	O
6	O
.	O

The	O
RFR	O
dataset	O
contains	O
12	O
million	O
book	O
references	O
2	O
.	O
The	O
most	O
relevant	O
product	O
data	O
for	O
normalization	O
is	O
:	O
ISBN	O
3	O
in	O
10	O
digit	O
or	O
13	O
digit	O
format	O
;	O
F.	O
S.	O
product	O
title	O
,	O
which	O
includes	O
the	O
book	O
title	O
,	O
often	O
supplemented	O
with	O
extra	O
information	O
in	O
free	O
text	O
;	O
author	O
(	O
s	O
)	O
of	O
the	O
book	O
as	O
the	O
input	O
catalog	O
name	O
provided	O
by	O
the	O
seller	O
.	O
In	O
particular	O
,	O
the	O
ISBN	O
is	O
a	O
worldwide	O
unique	O
identifier	O
for	O
books	O
,	O
which	O
makes	O
it	O
a	O
prime	O
candidate	O
for	O
unambiguous	O
matching	O
with	O
external	O
sources	O
.	O
In	O
this	O
dataset	O
,	O
an	O
ISBN	O
is	O
present	O
for	O
about	O
70	O
%	O
of	O
the	O
books	O
.	O
Among	O
the	O
books	O
with	O
no	O
ISBN	O
,	O
30	O
%	O
are	O
ancient	O
books	O
which	O
are	O
not	O
expected	O
to	O
be	O
associated	O
an	O
ISBN	O
.	O

There	O
is	O
no	O
central	O
authority	O
providing	O
consistent	O
information	O
on	O
books	O
associated	O
with	O
an	O
ISBN	O
.	O
However	O
,	O
there	O
is	O
a	O
wealth	O
of	O
bibliographic	O
resources	O
and	O
open	O
databases	O
for	O
books	O
.	O
In	O
order	O
to	O
retrieve	O
the	O
author	O
's	O
name	O
(	O
s	O
)	O
associated	O
with	O
the	O
books	O
in	O
the	O
RFR	O
dataset	O
,	O
we	O
perform	O
ISBN	O
matching	O
using	O
public	O
APIs	O
on	O
eight	O
of	O
them	O
,	O
listed	O
in	O
Table	O
1	O
along	O
with	O
the	O
fraction	O
of	O
found	O
ISBNs	O
from	O
this	O
dataset	O
.	O
We	O
find	O
the	O
sources	O
to	O
be	O
highly	O
complementary	O
and	O
that	O
75	O
%	O
of	O
the	O
books	O
with	O
an	O
ISBN	O
are	O
matched	O
with	O
at	O
least	O
one	O
source	O
.	O
The	O
match	O
via	O
ISBN	O
on	O
external	O
bibliographic	O
resources	O
is	O
the	O
first	O
component	O
of	O
the	O
system	O
depicted	O
in	O
Fig	O
.	O
1	O
.	O

In	O
order	O
to	O
evaluate	O
the	O
overall	O
system	O
,	O
we	O
need	O
product	O
data	O
from	O
RFR	O
for	O
which	O
the	O
canonical	O
author	O
name	O
has	O
been	O
carefully	O
annotated	O
and	O
can	O
be	O
considered	O
as	O
the	O
ground	O
truth	O
.	O
To	O
this	O
end	O
,	O
we	O
have	O
considered	O
a	O
subset	O
of	O
1000	O
books	O
from	O
the	O
RFR	O
dataset	O
,	O
discarding	O
books	O
written	O
by	O
more	O
than	O
one	O
author	O
for	O
simplicity	O
.	O
6	O
We	O
find	O
that	O
467	O
books	O
have	O
a	O
canonical	O
author	O
name	O
that	O
differs	O
from	O
RFR	O
's	O
original	O
(	O
unnormalized	O
)	O
author	O
name	O
.	O
Also	O
,	O
310	O
do	O
not	O
have	O
an	O
ISBN	O
or	O
do	O
not	O
match	O
on	O
any	O
of	O
the	O
bibliographic	O
resources	O
listed	O
in	O
Section	O
2.2	O
.	O
Among	O
them	O
,	O
208	O
books	O
have	O
a	O
canonical	O
name	O
that	O
differs	O
from	O
the	O
input	O
catalog	O
name	O
provided	O
by	O
the	O
seller	O
.	O

The	O
overview	O
of	O
the	O
system	O
can	O
be	O
found	O
in	O
Fig	O
.	O
1	O
.	O
Its	O
first	O
component	O
,	O
the	O
matching	O
via	O
ISBN	O
against	O
external	O
databases	O
,	O
has	O
already	O
been	O
presented	O
in	O
Section	O
2.2	O
.	O
In	O
the	O
rest	O
of	O
this	O
section	O
,	O
we	O
will	O
shed	O
light	O
on	O
the	O
three	O
machine	O
learning	O
components	O
of	O
the	O
system	O
.	O

We	O
provided	O
a	O
first	O
attempt	O
at	O
solving	O
the	O
problem	O
of	O
author	O
name	O
normalization	O
in	O
the	O
context	O
of	O
books	O
sold	O
on	O
e	O
-	O
commerce	O
websites	O
.	O
To	O
this	O
end	O
,	O
we	O
used	O
a	O
composite	O
system	O
involving	O
open	O
data	O
sources	O
for	O
books	O
,	O
approximate	O
match	O
with	O
Siamese	O
networks	O
,	O
name	O
correction	O
with	O
sequence	O
-	O
to	O
-	O
sequence	O
networks	O
,	O
and	O
ranking	O
of	O
the	O
proposals	O
.	O
We	O
find	O
that	O
72	O
%	O
of	O
the	O
books	O
have	O
the	O
author	O
's	O
name	O
normalized	O
by	O
the	O
highest	O
ranked	O
proposal	O
.	O
In	O
order	O
to	O
facilitate	O
future	O
research	O
,	O
we	O
are	O
releasing	O
data	O
from	O
Rakuten	O
France	O
:	O
a	O
large	O
dataset	O
containing	O
product	O
information	O
,	O
and	O
a	O
subset	O
of	O
it	O
with	O
expert	O
human	O
annotation	O
for	O
the	O
authors	O
'	O
names	O
.	O
They	O
are	O
accessible	O
at	O
rit.rakuten.co.jp/data_release	O
.	O
Multiple	O
challenges	O
remain	O
and	O
are	O
left	O
for	O
future	O
research	O
.	O
First	O
,	O
the	O
system	O
should	O
be	O
extended	O
to	O
handle	O
the	O
case	O
of	O
books	O
with	O
multiple	O
authors	O
.	O
In	O
addition	O
,	O
the	O
book	O
title	O
could	O
be	O
used	O
to	O
help	O
disambiguate	O
between	O
authors	O
and	O
to	O
query	O
external	O
bibliographic	O
resources	O
.	O
This	O
work	O
can	O
also	O
be	O
seen	O
as	O
an	O
intermediate	O
step	O
towards	O
a	O
knowledge	O
base	O
for	O
book	O
author	O
names	O
with	O
name	O
variants	O
,	O
extending	O
public	O
ones	O
such	O
as	O
BnF	O
,	O
using	O
the	O
ISNI	O
8	O
for	O
easier	O
record	O
linkage	O
whenever	O
available	O
.	O

We	O
thank	O
Raphaël	O
Ligier	O
-	O
Tirilly	O
for	O
his	O
help	O
with	O
the	O
deployment	O
of	O
the	O
system	O
as	O
microservices	O
,	O
and	O
Laurent	O
Ach	O
for	O
his	O
support	O
.	O

Reevaluating	O
Adversarial	O
Examples	O
in	O
Natural	O
Language	O

State	O
-	O
of	O
-	O
the	O
-	O
art	O
attacks	O
on	O
NLP	O
models	O
lack	O
a	O
shared	O
definition	O
of	O
what	O
constitutes	O
a	O
successful	O
attack	O
.	O
These	O
differences	O
make	O
the	O
attacks	O
difficult	O
to	O
compare	O
and	O
hindered	O
the	O
use	O
of	O
adversarial	O
examples	O
to	O
understand	O
and	O
improve	O
NLP	O
models	O
.	O
We	O
distill	O
ideas	O
from	O
past	O
work	O
into	O
a	O
unified	O
framework	O
:	O
a	O
successful	O
natural	O
language	O
adversarial	O
example	O
is	O
a	O
perturbation	O
that	O
fools	O
the	O
model	O
and	O
follows	O
four	O
proposed	O
linguistic	O
constraints	O
.	O
We	O
categorize	O
previous	O
attacks	O
based	O
on	O
these	O
constraints	O
.	O
For	O
each	O
constraint	O
,	O
we	O
suggest	O
options	O
for	O
human	O
and	O
automatic	O
evaluation	O
methods	O
.	O
We	O
use	O
these	O
methods	O
to	O
evaluate	O
two	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
synonym	O
substitution	O
attacks	O
.	O
We	O
find	O
that	O
perturbations	O
often	O
do	O
not	O
preserve	O
semantics	O
,	O
and	O
38	O
%	O
introduce	O
grammatical	O
errors	O
.	O
Next	O
,	O
we	O
conduct	O
human	O
studies	O
to	O
find	O
a	O
threshold	O
for	O
each	O
evaluation	O
method	O
that	O
aligns	O
with	O
human	O
judgment	O
.	O
Human	O
surveys	O
reveal	O
that	O
to	O
successfully	O
preserve	O
semantics	O
,	O
we	O
need	O
to	O
significantly	O
increase	O
the	O
minimum	O
cosine	O
similarities	O
between	O
the	O
embeddings	O
of	O
swapped	O
words	O
and	O
between	O
the	O
sentence	O
encodings	O
of	O
original	O
and	O
perturbed	O
sentences	O
.	O
With	O
constraints	O
adjusted	O
to	O
better	O
preserve	O
semantics	O
and	O
grammaticality	O
,	O
the	O
attack	O
success	O
rate	O
drops	O
by	O
over	O
70	O
percentage	O
points	O
.	O
1	O

We	O
define	O
F	O
:	O
X	O
!	O
Y	O
as	O
a	O
predictive	O
model	O
,	O
for	O
example	O
,	O
a	O
deep	O
neural	O
network	O
classifier	O
.	O
X	O
is	O
the	O
input	O
space	O
and	O
Y	O
is	O
the	O
output	O
space	O
.	O
We	O
focus	O
on	O
adversarial	O
perturbations	O
which	O
perturb	O
a	O
correctly	O
predicted	O
input	O
,	O
x	O
2	O
X	O
,	O
into	O
an	O
input	O
x	O
adv	O
.	O
The	O
boolean	O
goal	O
function	O
G	O
(	O
F	O
,	O
x	O
adv	O
)	O
represents	O
whether	O
the	O
goal	O
of	O
the	O
attack	O
has	O
been	O
met	O
.	O
We	O
define	O
C	O
1	O
...	O
C	O
n	O
as	O
a	O
set	O
of	O
boolean	O
functions	O
indicating	O
whether	O
the	O
perturbation	O
satisfies	O
a	O
certain	O
constraint	O
.	O
Adversarial	O
attacks	O
search	O
for	O
a	O
perturbation	O
from	O
x	O
to	O
x	O
adv	O
which	O
fools	O
F	O
by	O
both	O
achieving	O
some	O
goal	O
,	O
as	O
represented	O
by	O
G	O
(	O
F	O
,	O
x	O
adv	O
)	O
,	O
and	O
fulfilling	O
each	O
constraint	O
C	O
i	O
(	O
x	O
,	O
x	O
adv	O
)	O
.	O
The	O
definition	O
of	O
the	O
goal	O
function	O
G	O
depends	O
on	O
the	O
purpose	O
of	O
the	O
attack	O
.	O
Attacks	O
on	O
classification	O
frequently	O
aim	O
to	O
either	O
induce	O
any	O
incorrect	O
classification	O
(	O
untargeted	O
)	O
or	O
induce	O
a	O
particular	O
classification	O
(	O
targeted	O
)	O
.	O
Attacks	O
on	O
other	O
types	O
of	O
models	O
may	O
have	O
more	O
sophisticated	O
goals	O
.	O
For	O
example	O
,	O
attacks	O
on	O
translation	O
may	O
attempt	O
to	O
change	O
every	O
word	O
of	O
a	O
translation	O
,	O
or	O
introduce	O
targeted	O
keywords	O
into	O
the	O
translation	O
(	O
Cheng	O
et	O
al	O
,	O
2018	O
)	O
.	O
In	O
addition	O
to	O
defining	O
the	O
goal	O
of	O
the	O
attack	O
,	O
the	O
attacker	O
must	O
decide	O
the	O
constraints	O
perturbations	O
must	O
meet	O
.	O
Different	O
use	O
cases	O
require	O
different	O
Input	O
,	O
x	O
:	O
"	O
Shall	O
I	O
compare	O
thee	O
to	O
a	O
summer	O
's	O
day	O
?	O
"	O
-	O
William	O
Shakespeare	O
,	O
Sonnet	O
XVIII	O

Perturbation	O
,	O
x	O
adv	O
Explanation	O
Semantics	O
Shall	O
I	O
compare	O
thee	O
to	O
a	O
winter	O
's	O
day	O
?	O
x	O
adv	O
has	O
a	O
different	O
meaning	O
than	O
x.	O
Grammaticality	O
Shall	O
I	O
compares	O
thee	O
to	O
a	O
summer	O
's	O
day	O
?	O
x	O
adv	O
is	O
less	O
grammatically	O
correct	O
than	O
x.	O
Edit	O
Distance	O
Sha1l	O
i	O
conpp$haaare	O
thee	O
to	O
a	O
5umm3r	O
's	O
day	O
?	O
x	O
and	O
x	O
adv	O
have	O
a	O
large	O
edit	O
distance	O
.	O
Non	O
-	O
suspicion	O
Am	O
I	O
gon	O
na	O
compare	O
thee	O
to	O
a	O
summer	O
's	O
day	O
?	O
A	O
human	O
reader	O
may	O
suspect	O
this	O
sentence	O
to	O
have	O
been	O
modified	O
.	O
1	O
1	O
Shakespeare	O
never	O
used	O
the	O
word	O
"	O
gon	O
na	O
"	O
.	O
Its	O
first	O
recorded	O
usage	O
was	O
n't	O
until	O
1806	O
,	O
and	O
it	O
did	O
n't	O
become	O
popular	O
until	O
the	O
20th	O
century	O
.	O
constraints	O
.	O
We	O
build	O
on	O
the	O
categorization	O
of	O
attack	O
spaces	O
introduced	O
by	O
Gilmer	O
et	O
al	O
(	O
2018	O
)	O
to	O
introduce	O
a	O
categorization	O
of	O
constraints	O
for	O
adversarial	O
examples	O
in	O
natural	O
language	O
.	O
In	O
the	O
following	O
,	O
we	O
define	O
four	O
categories	O
of	O
constraints	O
on	O
adversarial	O
perturbations	O
in	O
natural	O
language	O
:	O
semantics	O
,	O
grammatically	O
,	O
overlap	O
,	O
and	O
non	O
-	O
suspicion	O
.	O
Table	O
1	O
provides	O
examples	O
of	O
adversarial	O
perturbations	O
that	O
violate	O
each	O
constraint	O
.	O

Semantics	O
constraints	O
require	O
the	O
semantics	O
of	O
the	O
input	O
to	O
be	O
preserved	O
between	O
x	O
and	O
x	O
adv	O
.	O
Many	O
attacks	O
include	O
constraints	O
on	O
semantics	O
as	O
a	O
way	O
to	O
ensure	O
the	O
correct	O
output	O
is	O
preserved	O
(	O
Zhang	O
et	O
al	O
,	O
2019	O
)	O
.	O
As	O
long	O
as	O
the	O
semantics	O
of	O
an	O
input	O
do	O
not	O
change	O
,	O
the	O
correct	O
output	O
will	O
stay	O
the	O
same	O
.	O
There	O
are	O
exceptions	O
:	O
one	O
could	O
imagine	O
tasks	O
for	O
which	O
preserving	O
semantics	O
does	O
not	O
necessarily	O
preserve	O
the	O
correct	O
output	O
.	O
For	O
example	O
,	O
consider	O
the	O
task	O
of	O
classifying	O
passages	O
as	O
written	O
in	O
either	O
Modern	O
or	O
Early	O
Modern	O
English	O
.	O
Perturbing	O
"	O
why	O
"	O
to	O
"	O
wherefore	O
"	O
may	O
retain	O
the	O
semantics	O
of	O
the	O
passage	O
,	O
but	O
change	O
the	O
correct	O
label	O
from	O
Modern	O
to	O
Early	O
Modern	O
English	O
5	O

Grammaticality	O
constraints	O
place	O
restrictions	O
on	O
the	O
grammaticality	O
of	O
x	O
adv	O
.	O
For	O
example	O
,	O
an	O
adversary	O
attempting	O
to	O
generate	O
a	O
plagiarised	O
paper	O
which	O
fools	O
a	O
plagiarism	O
checker	O
would	O
need	O
to	O
ensure	O
that	O
the	O
paper	O
remains	O
grammatically	O
correct	O
.	O
Grammatical	O
errors	O
do	O
n't	O
necessarily	O
change	O
semantics	O
,	O
as	O
illustrated	O
in	O
Table	O
1	O
.	O

Non	O
-	O
suspicion	O
constraints	O
specify	O
that	O
x	O
adv	O
must	O
appear	O
to	O
be	O
unmodified	O
.	O
Consider	O
the	O
example	O
in	O
Table	O
1	O
.	O
While	O
the	O
perturbation	O
preserves	O
semantics	O
and	O
grammar	O
,	O
it	O
switches	O
between	O
Modern	O
and	O
Early	O
Modern	O
English	O
and	O
thus	O
may	O
seem	O
suspicious	O
to	O
readers	O
.	O
Note	O
that	O
the	O
definition	O
of	O
the	O
non	O
-	O
suspicious	O
constraint	O
is	O
context	O
-	O
dependent	O
.	O
A	O
sentence	O
that	O
is	O
non	O
-	O
suspicious	O
in	O
the	O
context	O
of	O
a	O
kindergartner	O
's	O
homework	O
assignment	O
might	O
be	O
suspicious	O
in	O
the	O
context	O
of	O
an	O
academic	O
paper	O
.	O
An	O
attack	O
scenario	O
where	O
non	O
-	O
suspicion	O
constraints	O
do	O
not	O
apply	O
is	O
illegal	O
PDF	O
distribution	O
,	O
similar	O
to	O
a	O
case	O
discussed	O
by	O
Gilmer	O
et	O
al	O
(	O
2018	O
)	O
.	O
Consumers	O
of	O
an	O
illegal	O
PDF	O
may	O
tacitly	O
collude	O
with	O
the	O
person	O
uploading	O
it	O
.	O
They	O
know	O
the	O
document	O
has	O
been	O
altered	O
,	O
but	O
do	O
not	O
care	O
as	O
long	O
as	O
semantics	O
are	O
preserved	O
.	O
Attacks	O
by	O
Synonym	O
Substitution	O
:	O
Some	O
works	O
focus	O
on	O
an	O
easier	O
way	O
to	O
generate	O
a	O
subset	O
of	O
paraphrases	O
:	O
replacing	O
words	O
from	O
the	O
input	O
with	O
synonyms	O
(	O
Alzantot	O
et	O
al	O
,	O
2018	O
;	O
Jin	O
et	O
al	O
,	O
2019	O
;	O
Kuleshov	O
et	O
al	O
,	O
2018	O
;	O
Papernot	O
et	O
al	O
,	O
2016	O
;	O
Ren	O
et	O
al	O
,	O
2019	O
)	O
.	O
Each	O
attack	O
applies	O
a	O
search	O
algorithm	O
to	O
determine	O
which	O
words	O
to	O
replace	O
with	O
which	O
synonyms	O
.	O
Like	O
the	O
general	O
paraphrase	O
case	O
,	O
they	O
aim	O
to	O
create	O
examples	O
that	O
preserve	O
semantics	O
,	O
grammaticality	O
,	O
and	O
non	O
-	O
suspicion	O
.	O
While	O
not	O
all	O
have	O
an	O
explicit	O
edit	O
distance	O
constraint	O
,	O
some	O
limit	O
the	O
number	O
of	O
words	O
perturbed	O
.	O
Attacks	O
by	O
Character	O
Substitution	O
:	O
Some	O
studies	O
have	O
proposed	O
to	O
attack	O
natural	O
language	O
classification	O
models	O
by	O
deliberately	O
misspelling	O
words	O
(	O
Ebrahimi	O
et	O
al	O
,	O
2017	O
;	O
Gao	O
et	O
al	O
,	O
2018	O
;	O
Li	O
et	O
al	O
,	O
2018	O
)	O
.	O
These	O
attacks	O
use	O
character	O
replacements	O
to	O
change	O
a	O
word	O
into	O
one	O
that	O
the	O
model	O
does	O
n't	O
recognize	O
.	O
The	O
replacements	O
are	O
designed	O
to	O
create	O
character	O
sequences	O
that	O
a	O
human	O
reader	O
would	O
easily	O
correct	O
into	O
the	O
original	O
words	O
.	O
If	O
there	O
are	O
n't	O
many	O
misspellings	O
,	O
non	O
-	O
suspicion	O
may	O
be	O
preserved	O
.	O
Semantics	O
are	O
preserved	O
as	O
long	O
as	O
human	O
readers	O
can	O
correct	O
the	O
misspellings	O
.	O
Attacks	O
by	O
Word	O
Insertion	O
or	O
Removal	O
:	O
Liang	O
et	O
al	O
(	O
2017	O
)	O
and	O
Samanta	O
and	O
Mehta	O
(	O
2017	O
)	O
devised	O
a	O
way	O
to	O
determine	O
the	O
most	O
important	O
words	O
in	O
the	O
input	O
and	O
then	O
used	O
heuristics	O
to	O
generate	O
perturbed	O
inputs	O
by	O
adding	O
or	O
removing	O
important	O
words	O
.	O
In	O
some	O
cases	O
,	O
these	O
strategies	O
are	O
combined	O
with	O
synonym	O
substitution	O
.	O
These	O
attacks	O
aim	O
to	O
follow	O
all	O
constraints	O
.	O
Using	O
constraints	O
defined	O
in	O
Section	O
2	O
we	O
categorize	O
a	O
sample	O
of	O
current	O
attacks	O
in	O
Table	O
2	O
.	O

A	O
few	O
past	O
studies	O
of	O
attacks	O
have	O
included	O
human	O
evaluation	O
of	O
semantic	O
preservation	O
(	O
Ribeiro	O
et	O
al	O
,	O
2018	O
;	O
Iyyer	O
et	O
al	O
,	O
2018	O
;	O
Alzantot	O
et	O
al	O
,	O
2018	O
;	O
Jin	O
et	O
al	O
,	O
2019	O
)	O
.	O
However	O
,	O
studies	O
often	O
simply	O
ask	O
users	O
to	O
simply	O
rate	O
the	O
"	O
similarity	O
"	O
of	O
x	O
and	O
x	O
adv	O
.	O
We	O
believe	O
this	O
phrasing	O
does	O
not	O
generate	O
an	O
accurate	O
measure	O
of	O
semantic	O
preservation	O
,	O
as	O
users	O
may	O
consider	O
two	O
sentences	O
with	O
different	O
semantics	O
"	O
similar	O
"	O
if	O
they	O
only	O
differ	O
by	O
a	O
few	O
words	O
.	O
Instead	O
,	O
users	O
should	O
be	O
explicitly	O
asked	O
whether	O
changes	O
between	O
x	O
and	O
x	O
adv	O
preserve	O
the	O
meaning	O
of	O
the	O
original	O
passage	O
.	O
We	O
propose	O
to	O
ask	O
human	O
judges	O
to	O
rate	O
if	O
meaning	O
is	O
preserved	O
on	O
a	O
Likert	O
scale	O
of	O
1	O
-	O
5	O
,	O
where	O
1	O
is	O
"	O
Strongly	O
Disagree	O
"	O
and	O
5	O
is	O
"	O
Strongly	O
Agree	O
"	O
(	O
Likert	O
,	O
1932	O
)	O
.	O
A	O
perturbation	O
is	O
semantics	O
-	O
preserving	O
if	O
the	O
average	O
score	O
is	O
at	O
least	O
✏	O
sem	O
.	O
We	O
propose	O

Input	O
,	O
x	O
Perturbation	O
,	O
x	O
adv	O
Semantics	O
Jagger	O
,	O
Stoppard	O
and	O
director	O
Michael	O
Apted	O
deliver	O
a	O
riveting	O
and	O
surprisingly	O
romantic	O
ride	O
.	O
Jagger	O
,	O
Stoppard	O
and	O
director	O
Michael	O
Apted	O
deliver	O
a	O
baffling	O
and	O
surprisingly	O
sappy	O
motorbike	O
.	O
Grammaticality	O
A	O
grating	O
,	O
emaciated	O
flick	O
.	O
A	O
grates	O
,	O
lanky	O
flick	O
.	O
Non	O
-	O
suspicion	O
Great	O
character	O
interaction	O
.	O
Gargantuan	O
character	O
interaction	O
.	O
✏	O
sem	O
=	O
4	O
as	O
a	O
general	O
rule	O
:	O
on	O
average	O
,	O
humans	O
should	O
at	O
least	O
"	O
Agree	O
"	O
that	O
x	O
and	O
x	O
adv	O
have	O
the	O
same	O
meaning	O
.	O

Both	O
Jin	O
et	O
al	O
(	O
2019	O
)	O
and	O
Iyyer	O
et	O
al	O
(	O
2018	O
)	O
reported	O
a	O
human	O
evaluation	O
of	O
grammaticality	O
,	O
but	O
neither	O
study	O
clearly	O
asked	O
if	O
any	O
errors	O
were	O
introduced	O
by	O
a	O
perturbation	O
.	O
For	O
human	O
evaluation	O
of	O
the	O
grammaticality	O
constraint	O
,	O
we	O
propose	O
presenting	O
x	O
and	O
x	O
adv	O
together	O
and	O
asking	O
judges	O
if	O
grammatical	O
errors	O
were	O
introduced	O
by	O
the	O
changes	O
made	O
.	O
However	O
,	O
due	O
to	O
the	O
rule	O
-	O
based	O
nature	O
of	O
grammar	O
,	O
automatic	O
evaluation	O
is	O
preferred	O
.	O

We	O
ran	O
each	O
of	O
the	O
generated	O
(	O
x	O
,	O
x	O
adv	O
)	O
pairs	O
through	O
LanguageTool	O
to	O
count	O
grammatical	O
errors	O
.	O
LanguageTool	O
detected	O
more	O
grammatical	O
errors	O
in	O
x	O
adv	O
than	O
x	O
for	O
50	O
%	O
of	O
perturbations	O
generated	O
by	O
TEXTFOOLER	O
,	O
and	O
32	O
%	O
of	O
perturbations	O
generated	O
by	O
GENETICATTACK	O
.	O
Additionally	O
,	O
perturbations	O
often	O
contain	O
errors	O
that	O
humans	O
rarely	O
make	O
.	O
LanguageTool	O
detected	O
6	O
categories	O
for	O
which	O
errors	O
in	O
the	O
perturbed	O
samples	O
appear	O
at	O
least	O
10	O
times	O
more	O
frequently	O
than	O
in	O
the	O
original	O
content	O
.	O
Details	O
regarding	O
these	O
error	O
categories	O
and	O
examples	O
of	O
violations	O
are	O
shown	O
in	O
Table	O
4	O
.	O

TFADJUSTED	O
generated	O
better	O
quality	O
adversarial	O
examples	O
by	O
constraining	O
its	O
search	O
to	O
exclude	O
examples	O
that	O
fail	O
to	O
meet	O
three	O
constraints	O
:	O
word	O
embedding	O
distance	O
,	O
sentence	O
encoder	O
similarity	O
,	O
and	O
grammaticality	O
.	O
We	O
performed	O
an	O
ablation	O
study	O
to	O
understand	O
the	O
relative	O
impact	O
of	O
each	O
on	O
attack	O
success	O
rate	O
.	O
We	O
reran	O
three	O
TFADJUSTED	O
attacks	O
(	O
one	O
for	O
each	O
constraint	O
removed	O
)	O
on	O
each	O
dataset	O
.	O
Table	O
6	O
shows	O
attack	O
success	O
rate	O
after	O
individually	O
removing	O
each	O
constraint	O
.	O
The	O
word	O
embedding	O
distance	O
constraint	O
was	O
the	O
greatest	O
inhibitor	O
of	O
attack	O
success	O
rate	O
,	O
followed	O
by	O
the	O
sentence	O
encoder	O
.	O

We	O
showed	O
that	O
two	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
synonym	O
substitution	O
attacks	O
,	O
TEXTFOOLER	O
and	O
GENETI	O
-	O
CATTACK	O
,	O
frequently	O
violate	O
the	O
constraints	O
they	O
claim	O
to	O
follow	O
.	O
We	O
created	O
TFADJUSTED	O
,	O
which	O
applies	O
constraints	O
that	O
produce	O
adversarial	O
examples	O
judged	O
to	O
preserve	O
semantics	O
and	O
grammaticality	O
.	O
Due	O
to	O
the	O
lack	O
of	O
a	O
shared	O
vocabulary	O
for	O
discussing	O
NLP	O
attacks	O
,	O
the	O
source	O
of	O
improvement	O
in	O
attack	O
success	O
rate	O
between	O
TEXTFOOLER	O
and	O
GENETICATTACK	O
was	O
unclear	O
.	O
Holding	O
constraint	O
application	O
constant	O
revealed	O
that	O
the	O
source	O
of	O
TEXTFOOLER	O
's	O
improvement	O
was	O
lenient	O
constraint	O
application	O
(	O
rather	O
than	O
a	O
better	O
search	O
method	O
)	O
.	O
With	O
a	O
shared	O
framework	O
for	O
defining	O
and	O
applying	O
constraints	O
,	O
future	O
research	O
can	O
focus	O
on	O
developing	O
better	O
search	O
methods	O
and	O
better	O
constraint	O
application	O
techniques	O
for	O
preserving	O
semantics	O
and	O
grammaticality	O
.	O

Let	O
V	O
denote	O
the	O
set	O
of	O
entities	O
,	O
R	O
denote	O
the	O
set	O
of	O
binary	O
relations	O
and	O
G	O
denote	O
a	O
KB	O
or	O
equivalently	O
a	O
Knowledge	O
Graph	O
(	O
KG	O
)	O
.	O
Formally	O
,	O
G	O
=	O
(	O
V	O
,	O
E	O
,	O
R	O
)	O
is	O
a	O
directed	O
labeled	O
multigraph	O
where	O
V	O
and	O
E	O
denote	O
the	O
vertices	O
and	O
edges	O
of	O
the	O
graph	O
respectively	O
.	O
Note	O
that	O
,	O
E	O
⊆	O
V	O
×	O
R	O
×	O
V	O
.	O
Let	O
(	O
e	O
1	O
,	O
r	O
,	O
e	O
2	O
)	O
denote	O
a	O
fact	O
in	O
G	O
where	O
e	O
1	O
,	O
e	O
2	O
V	O
and	O
r	O
E.	O
Also	O
,	O
following	O
previous	O
approaches	O
(	O
Bordes	O
et	O
al	O
,	O
2013	O
)	O
,	O
we	O
add	O
the	O
inverse	O
relation	O
of	O
every	O
edge	O
,	O
i.e.	O
,	O
for	O
an	O
fact	O
(	O
e	O
1	O
,	O
r	O
,	O
e	O
2	O
)	O
E	O
,	O
we	O
add	O
the	O
edge	O
(	O
e	O
2	O
,	O
r	O
−1	O
,	O
e	O
1	O
)	O
to	O
the	O
graph	O
.	O
(	O
If	O
the	O
set	O
of	O
binary	O
relations	O
R	O
does	O
not	O
contain	O
the	O
inverse	O
relation	O
r	O
−1	O
,	O
it	O
is	O
added	O
to	O
R	O
as	O
well	O
)	O
.	O
Task	O
:	O
We	O
consider	O
the	O
task	O
of	O
query	O
answering	O
on	O
KGs	O
,	O
i.e.	O
,	O
answering	O
questions	O
of	O
the	O
form	O
(	O
e	O
1q	O
,	O
r	O
q	O
,	O
?	O
)	O
,	O
where	O
answer	O
is	O
an	O
entity	O
in	O
the	O
KG	O
.	O

A	O
path	O
in	O
a	O
KG	O
between	O
two	O
entities	O
e	O
s	O
,	O
e	O
t	O
is	O
defined	O
as	O
a	O
sequence	O
of	O
alternating	O
entity	O
and	O
relations	O
that	O
connect	O
e	O
s	O
and	O
e	O
t	O
.	O
A	O
length	O
of	O
a	O
path	O
is	O
the	O
number	O
of	O
relation	O
(	O
edges	O
)	O
in	O
the	O
path	O
.	O
Formally	O
,	O
let	O
a	O
path	O
p	O
=	O
(	O
e	O
1	O
,	O
r	O
1	O
,	O
e	O
2	O
,	O
.	O
.	O
.	O
,	O
r	O
n	O
,	O
e	O
n+1	O
)	O
with	O
st	O
(	O
p	O
)	O
=	O
e	O
1	O
,	O
en	O
(	O
p	O
)	O
=	O
e	O
n+1	O
and	O
len	O
(	O
p	O
)	O
=	O
n.	O
We	O
also	O
define	O
a	O
path	O
type	O
as	O
the	O
sequence	O
of	O
the	O
relations	O
in	O
p	O
,	O
i.e.	O
,	O
type	O
(	O
p	O
)	O
=	O
(	O
r	O
1	O
,	O
r	O
2	O
,	O
.	O
.	O
.	O
,	O
r	O
n	O
)	O
.	O
Let	O
P	O
denote	O
the	O
set	O
of	O
all	O
paths	O
in	O
G.	O
Let	O
P	O
n	O
⊆	O
P	O
=	O
{	O
p	O
|	O
len	O
(	O
p	O
)	O
≤	O
n	O
}	O
be	O
the	O
set	O
of	O
all	O
paths	O
of	O
length	O
up	O
to	O
n.	O
Also	O
,	O
let	O
P	O
n	O
denote	O
the	O
set	O
of	O
all	O
path	O
types	O
with	O
length	O
up	O
to	O
n	O
,	O
i.e.	O
P	O
n	O
=	O
{	O
type	O
(	O
p	O
)	O
|	O
p	O
P	O
n	O
}	O
.	O
Let	O
P	O
n	O
(	O
e	O
1	O
,	O
r	O
)	O
⊆	O
P	O
n	O
denote	O
all	O
path	O
types	O
of	O
length	O
up	O
to	O
n	O
that	O
originate	O
at	O
e	O
1	O
and	O
end	O
at	O
the	O
entities	O
that	O
are	O
connected	O
to	O
e	O
1	O
by	O
a	O
direct	O
edge	O
of	O
type	O
r.	O
In	O
other	O
words	O
,	O
if	O
S	O
e	O
1	O
r	O
=	O
{	O
e	O
2	O
|	O
(	O
e	O
1	O
,	O
r	O
,	O
e	O
2	O
)	O
G	O
}	O
denotes	O
the	O
set	O
of	O
entities	O
that	O
are	O
connected	O
to	O
e	O
1	O
via	O
a	O
direct	O
edge	O
r	O
,	O
then	O
P	O
n	O
(	O
e	O
1	O
,	O
r	O
)	O
denotes	O
the	O
set	O
of	O
all	O
path	O
types	O
of	O
length	O
up	O
to	O
n	O
that	O
start	O
from	O
e	O
1	O
and	O
end	O
at	O
entities	O
in	O
S	O
e	O
1	O
r	O
.	O
By	O
definition	O
,	O
r	O
P	O
n	O
(	O
e	O
1	O
,	O
r	O
)	O
.	O
Similarly	O
,	O
we	O
define	O
P	O
n	O
(	O
e	O
1	O
,	O
r	O
)	O
which	O
contain	O
paths	O
instead	O
of	O
path	O
types	O
.	O

Next	O
we	O
discuss	O
how	O
to	O
estimate	O
path	O
prior	O
and	O
precision	O
terms	O
.	O
There	O
exists	O
abundant	O
modeling	O
choices	O
to	O
estimate	O
them	O
.	O
For	O
example	O
,	O
following	O
Chen	O
et	O
al	O
(	O
2018	O
)	O
,	O
we	O
could	O
train	O
a	O
neural	O
network	O
model	O
to	O
estimate	O
P	O
(	O
p	O
|	O
c	O
e	O
1q	O
,	O
r	O
q	O
)	O
.	O
However	O
,	O
with	O
our	O
original	O
goal	O
of	O
designing	O
a	O
simple	O
and	O
efficient	O
non	O
-	O
parametric	O
model	O
,	O
we	O
estimate	O
these	O
parameters	O
by	O
simple	O
count	O
statistics	O
from	O
the	O
KG	O
.	O
E.g.	O
,	O
the	O
path	O
prior	O
P	O
(	O
p	O
|	O
c	O
,	O
r	O
q	O
)	O
is	O
estimated	O
as	O
∑	O
e	O
c	O
c	O
∑	O
p	O
P	O
n	O
(	O
e	O
c	O
,	O
r	O
q	O
)	O
1	O
[	O
type	O
(	O
p	O
)	O
=	O
p	O
]	O
∑	O
e	O
c	O
c	O
∑	O
p	O
P	O
n	O
(	O
e	O
c	O
,	O
r	O
q	O
)	O
1	O
(	O
2	O
)	O
For	O
each	O
entity	O
in	O
cluster	O
c	O
,	O
we	O
consider	O
the	O
paths	O
that	O
connect	O
e	O
c	O
to	O
entities	O
it	O
is	O
directly	O
connected	O
to	O
via	O
edge	O
type	O
r	O
q	O
(	O
P	O
n	O
(	O
e	O
c	O
,	O
r	O
q	O
)	O
in	O
2.1	O
)	O
.	O
The	O
path	O
prior	O
for	O
a	O
path	O
type	O
p	O
is	O
computed	O
as	O
the	O
proportion	O
of	O
times	O
the	O
type	O
of	O
paths	O
in	O
P	O
n	O
(	O
e	O
c	O
,	O
r	O
q	O
)	O
is	O
equal	O
to	O
p.	O
Let	O
P	O
n	O
(	O
e	O
c	O
)	O
denote	O
the	O
paths	O
of	O
up	O
to	O
length	O
n	O
starting	O
from	O
the	O
entity	O
e	O
c	O
.	O
Note	O
,	O
unlike	O
P	O
n	O
(	O
e	O
c	O
,	O
r	O
q	O
)	O
,	O
the	O
paths	O
in	O
P	O
n	O
(	O
e	O
c	O
)	O
do	O
not	O
have	O
to	O
end	O
at	O
specific	O
entities	O
.	O
Also	O
from	O
2.1	O
,	O
en	O
(	O
p	O
)	O
denotes	O
the	O
end	O
entity	O
for	O
a	O
path	O
p	O
and	O
S	O
e	O
c	O
r	O
q	O
denotes	O
the	O
set	O
of	O
entities	O
that	O
are	O
connected	O
to	O
e	O
c	O
via	O
a	O
direct	O
edge	O
of	O
type	O
r	O
q	O
.	O
Equation	O
3	O
,	O
therefore	O
,	O
estimates	O
the	O
proportion	O
of	O
times	O
the	O
path	O
p	O
successfully	O
ends	O
at	O
one	O
of	O
the	O
answer	O
entities	O
when	O
starting	O
from	O
e	O
c	O
,	O
given	O
r	O
q	O
.	O
There	O
are	O
several	O
advantages	O
in	O
estimating	O
the	O
parameters	O
using	O
simple	O
count	O
statistics	O
.	O
Firstly	O
,	O
they	O
are	O
extremely	O
simple	O
,	O
and	O
statistics	O
for	O
each	O
entity	O
in	O
clusters	O
can	O
be	O
computed	O
in	O
parallel	O
making	O
them	O
extremely	O
time	O
efficient	O
.	O
Secondly	O
once	O
they	O
are	O
computed	O
,	O
our	O
approach	O
needs	O
no	O
further	O
training	O
.	O
Lastly	O
,	O
when	O
new	O
data	O
is	O
added	O
,	O
it	O
makes	O
it	O
easy	O
to	O
update	O
the	O
parameters	O
without	O
training	O
from	O
scratch	O
.	O
To	O
summarize	O
,	O
given	O
a	O
query	O
entity	O
(	O
e	O
1q	O
,	O
r	O
q	O
)	O
,	O
our	O
method	O
gathers	O
reasoning	O
paths	O
from	O
k	O
similar	O
entities	O
to	O
e	O
1q	O
.	O
These	O
reasoning	O
paths	O
are	O
then	O
traversed	O
in	O
the	O
KG	O
starting	O
from	O
e	O
1q	O
,	O
leading	O
to	O
a	O
set	O
of	O
candidate	O
answer	O
entities	O
.	O
The	O
score	O
of	O
each	O
answer	O
entity	O
candidate	O
is	O
computed	O
as	O
a	O
weighted	O
sum	O
of	O
the	O
reasoning	O
paths	O
the	O
lead	O
to	O
them	O
(	O
Equation	O
1	O
)	O
.	O
Each	O
path	O
is	O
weighed	O
with	O
an	O
estimate	O
of	O
its	O
frequency	O
(	O
Equation	O
2	O
)	O
and	O
precision	O
(	O
Equation	O
3	O
)	O
given	O
the	O
query	O
relation	O
.	O
The	O
next	O
section	O
describes	O
how	O
we	O
extend	O
our	O
model	O
for	O
open	O
-	O
world	O
setting	O
where	O
new	O
entities	O
and	O
facts	O
are	O
added	O
to	O
the	O
KB	O
.	O

In	O
this	O
section	O
,	O
we	O
evaluate	O
our	O
proposed	O
approach	O
on	O
a	O
wide	O
array	O
of	O
knowledge	O
-	O
base	O
completion	O
(	O
KBC	O
)	O
benchmarks	O
(	O
3.3	O
)	O
.	O
To	O
evaluate	O
the	O
nonparametric	O
nature	O
of	O
our	O
approach	O
,	O
we	O
also	O
evaluate	O
on	O
an	O
'	O
open	O
-	O
world	O
'	O
setting	O
(	O
2.3	O
)	O
in	O
which	O
new	O
entities	O
are	O
added	O
to	O
the	O
KG	O
.	O
We	O
demonstrate	O
our	O
proposed	O
approach	O
is	O
competitive	O
to	O
several	O
stateof	O
-	O
the	O
-	O
art	O
methods	O
on	O
benchmarks	O
in	O
the	O
standard	O
setting	O
,	O
but	O
it	O
greatly	O
outperforms	O
other	O
methods	O
in	O
the	O
online	O
setting	O
(	O
3.4	O
)	O
.	O
The	O
best	O
hyper	O
-	O
parameters	O
for	O
all	O
experiments	O
including	O
the	O
range	O
of	O
hyperparameter	O
tried	O
and	O
results	O
on	O
validation	O
set	O
are	O
noted	O
in	O
A.6	O
.	O

(	O
athlete	O
-	O
led	O
-	O
sports	O
-	O
team	O
,	O
team	O
-	O
plays	O
-	O
in	O
-	O
league	O
)	O
(	O
athlete	O
-	O
home	O
-	O
stadium	O
,	O
league	O
-	O
stadiums	O
−1	O
)	O

We	O
present	O
a	O
simple	O
yet	O
accurate	O
approach	O
for	O
probabilistic	O
case	O
-	O
based	O
reasoning	O
in	O
knowledge	O
bases	O
.	O
Our	O
method	O
is	O
non	O
-	O
parametric	O
,	O
deriving	O
reasoning	O
rules	O
dynamically	O
from	O
similar	O
entities	O
in	O
the	O
KB	O
and	O
is	O
capable	O
of	O
handling	O
new	O
entities	O
.	O
We	O
cluster	O
similar	O
entities	O
together	O
and	O
estimate	O
per	O
-	O
cluster	O
parameters	O
that	O
measures	O
the	O
prior	O
and	O
precision	O
of	O
paths	O
using	O
simple	O
count	O
statistics	O
.	O
Our	O
simple	O
approach	O
performs	O
competitively	O
to	O
the	O
best	O
embeddings	O
based	O
models	O
on	O
several	O
benchmarks	O
and	O
outperforms	O
all	O
models	O
in	O
the	O
open	O
-	O
world	O
setting	O
.	O
Algorithm	O
1	O
Select	O
a	O
flat	O
clustering	O
from	O
a	O
tree	O
structure	O
.	O
1	O
:	O
input	O
:	O
V	O
:	O
Entities	O
,	O
root	O
:	O
Root	O
of	O
tree	O
,	O
τ	O
:	O
(	O
5	O
)	O

A	O
hierarchical	O
clustering	O
T	O
over	O
the	O
entities	O
V	O
,	O
encodes	O
a	O
large	O
number	O
of	O
flat	O
partitions	O
of	O
the	O
entities	O
,	O
often	O
referred	O
to	O
as	O
tree	O
consistent	O
partitions	O
in	O
the	O
clustering	O
literature	O
.	O
We	O
select	O
one	O
of	O
these	O
tree	O
consistent	O
partitions	O
using	O
a	O
threshold	O
on	O
the	O
linkage	O
function	O
,	O
τ	O
.	O
The	O
algorithm	O
performs	O
a	O
breadth	O
first	O
search	O
starting	O
at	O
the	O
root	O
node	O
.	O
The	O
search	O
stops	O
at	O
any	O
node	O
for	O
which	O
the	O
linkage	O
is	O
above	O
the	O
given	O
value	O
τ	O
.	O
Pseudocode	O
is	O
given	O
in	O
Algorithm	O
1	O
.	O

We	O
analyze	O
the	O
number	O
of	O
entities	O
that	O
need	O
to	O
be	O
re	O
-	O
clustered	O
and	O
added	O
in	O
each	O
round	O
.	O
We	O
observe	O
that	O
it	O
is	O
significantly	O
fewer	O
than	O
the	O
number	O
of	O
entities	O
in	O
the	O
KB	O
.	O
Note	O
that	O
an	O
online	O
method	O
like	O
the	O
one	O
proposed	O
in	O
this	O
paper	O
just	O
needs	O
to	O
run	O
on	O
the	O
new	O
and	O
modified	O
entities	O
while	O
a	O
batch	O
algorithm	O
would	O
need	O
to	O
run	O
on	O
the	O
entire	O
KB	O
.	O

Proposition	O
:	O
Let	O
n	O
denote	O
the	O
maximum	O
length	O
of	O
a	O
reasoning	O
path	O
considered	O
by	O
our	O
model	O
.	O
For	O
every	O
new	O
entity	O
e	O
i	O
added	O
to	O
the	O
KG	O
,	O
we	O
need	O
to	O
recompute	O
statistics	O
for	O
entities	O
that	O
lie	O
within	O
cycles	O
of	O
length	O
up	O
to	O
(	O
n	O
+	O
1	O
)	O
starting	O
from	O
e	O
i	O
.	O
We	O
see	O
from	O
Eq	O
2	O
,	O
that	O
the	O
estimate	O
for	O
the	O
prior	O
for	O
a	O
path	O
type	O
p	O
depends	O
on	O
P	O
n	O
(	O
e	O
c	O
,	O
r	O
q	O
)	O
i.e.	O
the	O
set	O
of	O
paths	O
that	O
lead	O
from	O
e	O
c	O
to	O
entities	O
that	O
are	O
connected	O
to	O
e	O
c	O
via	O
relation	O
r	O
q	O
.	O
WLOG	O
,	O
say	O
e	O
t	O
is	O
such	O
an	O
entity	O
i.e.	O
(	O
e	O
c	O
,	O
r	O
q	O
,	O
e	O
t	O
)	O
G.	O
When	O
a	O
new	O
entity	O
/	O
edge	O
is	O
added	O
to	O
the	O
KG	O
,	O
this	O
set	O
of	O
paths	O
might	O
increase	O
.	O
It	O
is	O
easy	O
to	O
see	O
that	O
the	O
set	O
P	O
n	O
(	O
e	O
c	O
,	O
r	O
q	O
)	O
is	O
updated	O
iff	O
a	O
new	O
path	O
p	O
new	O
of	O
length	O
≤	O
n	O
appears	O
between	O
e	O
c	O
and	O
e	O
t	O
.	O
In	O
this	O
case	O
,	O
the	O
edges	O
in	O
p	O
new	O
would	O
form	O
a	O
cycle	O
with	O
the	O
edge	O
(	O
e	O
c	O
,	O
r	O
q	O
,	O
e	O
t	O
)	O
.	O
The	O
length	O
of	O
the	O
cycle	O
would	O
be	O
at	O
most	O
len	O
(	O
p	O
new	O
)	O
+	O
1	O
which	O
in	O
turn	O
is	O
at	O
most	O
of	O
length	O
n	O
+	O
1	O
.	O
This	O
,	O
to	O
find	O
entities	O
for	O
which	O
the	O
prior	O
has	O
changed	O
after	O
the	O
addition	O
of	O
a	O
new	O
edge	O
/	O
entity	O
,	O
it	O
is	O
sufficient	O
to	O
find	O
entities	O
lying	O
on	O
cycles	O
of	O
length	O
up	O
to	O
n	O
+	O
1	O
starting	O
from	O
the	O
new	O
entity	O
/	O
edge	O
.	O
This	O
mechanism	O
for	O
finding	O
entities	O
for	O
recomputation	O
is	O
only	O
approximate	O
when	O
computing	O
the	O
precision	O
.	O
We	O
see	O
from	O
Eq	O
3	O
,	O
that	O
the	O
numerator	O
depends	O
on	O
paths	O
that	O
lead	O
to	O
the	O
answer	O
entity	O
(	O
as	O
with	O
prior	O
)	O
while	O
denominator	O
depends	O
on	O
all	O
n	O
length	O
paths	O
around	O
e	O
c	O
.	O
So	O
,	O
if	O
the	O
numerator	O
is	O
ever	O
to	O
be	O
increased	O
,	O
we	O
would	O
catch	O
that	O
update	O
by	O
the	O
proposed	O
cycle	O
finding	O
method	O
.	O
However	O
,	O
even	O
if	O
an	O
entity	O
does	O
not	O
lie	O
on	O
a	O
cycle	O
with	O
the	O
new	O
edge	O
/	O
entity	O
,	O
if	O
there	O
is	O
a	O
path	O
of	O
length	O
n	O
from	O
e	O
c	O
to	O
the	O
new	O
edge	O
/	O
entity	O
,	O
the	O
denominator	O
count	O
would	O

be	O
incremented	O
.	O
Thus	O
,	O
the	O
precision	O
estimates	O
for	O
some	O
entities	O
might	O
be	O
an	O
over	O
-	O
estimate	O
of	O
the	O
path	O
precision	O
(	O
had	O
it	O
been	O
recomputed	O
after	O
new	O
edges	O
are	O
added	O
to	O
the	O
KB	O
)	O
.	O

Table	O
7	O
shows	O
some	O
example	O
of	O
new	O
entities	O
arriving	O
and	O
getting	O
assigned	O
to	O
their	O
respective	O
clusters	O
by	O
GRINCH	O
.	O

Group	O
,	O
Extract	O
and	O
Aggregate	O
:	O
Summarizing	O
a	O
Large	O
Amount	O
of	O
Finance	O
News	O
for	O
Forex	O
Movement	O
Prediction	O

In	O
this	O
part	O
,	O
we	O
introduce	O
the	O
three	O
news	O
grouping	O
methods	O
.	O
The	O
ideal	O
division	O
enables	O
news	O
groups	O
to	O
be	O
high	O
cohesion	O
and	O
low	O
coupling	O
,	O
which	O
means	O
the	O
semantic	O
information	O
of	O
finance	O
news	O
should	O
be	O
highly	O
related	O
intra	O
-	O
group	O
and	O
less	O
related	O
inter	O
-	O
groups	O
.	O
We	O
suppose	O
that	O
extracting	O
news	O
by	O
groups	O
can	O
reduce	O
the	O
extraction	O
difficulty	O
compared	O
to	O
extracting	O
from	O
all	O
news	O
directly	O
because	O
news	O
in	O
the	O
same	O
group	O
is	O
close	O
to	O
each	O
other	O
and	O
has	O
less	O
noise	O
.	O
Moreover	O
,	O
this	O
method	O
can	O
help	O
us	O
analyze	O
the	O
contributions	O
of	O
different	O
groups	O
.	O

In	O
this	O
method	O
,	O
finance	O
news	O
is	O
divided	O
into	O
groups	O
according	O
to	O
the	O
time	O
when	O
news	O
happens	O
.	O
We	O
set	O
the	O
time	O
unit	O
to	O
5	O
minutes	O
and	O
news	O
released	O
in	O
the	O
same	O
time	O
unit	O
will	O
be	O
divided	O
into	O
the	O
same	O
group	O
.	O
This	O
method	O
supposes	O
that	O
news	O
happened	O
closely	O
is	O
highly	O
correlated	O
.	O

In	O
this	O
method	O
,	O
finance	O
news	O
is	O
divided	O
into	O
groups	O
by	O
news	O
topic	O
.	O
The	O
news	O
topics	O
are	O
generated	O
by	O
unsupervised	O
news	O
clustering	O
.	O
In	O
this	O
work	O
,	O
we	O
choose	O
the	O
affinity	O
propagation	O
algorithm	O
(	O
Frey	O
and	O
Dueck	O
,	O
2007	O
)	O
to	O
generate	O
news	O
clusters	O
without	O
setting	O
the	O
number	O
of	O
clusters	O
subjectively	O
.	O
Moreover	O
,	O
we	O
choose	O
the	O
tf	O
-	O
idf	O
of	O
2	O
-	O
gram	O
features	O
from	O
news	O
headlines	O
.	O
This	O
method	O
supposes	O
that	O
finance	O
news	O
focuses	O
on	O
several	O
finance	O
event	O
topics	O
at	O
a	O
particular	O
time	O
.	O
News	O
in	O
the	O
same	O
topic	O
describes	O
this	O
topic	O
from	O
different	O
aspects	O
and	O
has	O
a	O
high	O
correlation	O
.	O

In	O
this	O
section	O
,	O
we	O
analyze	O
the	O
influence	O
of	O
finance	O
news	O
's	O
attributes	O
(	O
category	O
and	O
region	O
)	O
on	O
prediction	O
results	O
and	O
summarize	O
the	O
influence	O
patterns	O
for	O
different	O
currency	O
pairs	O
.	O
We	O
conduct	O
the	O
experiments	O
based	O
on	O
BHAM	O
-	O
Category	O
.	O

The	O
forex	O
trading	O
data	O
's	O
attention	O
weights	O
over	O
news	O
categories	O
are	O
calculated	O
by	O
Equation	O
6	O
.	O
We	O
sum	O
up	O
all	O
the	O
attention	O
weights	O
of	O
test	O
samples	O
and	O
calculate	O
the	O
proportions	O
each	O
category	O
contributes	O
.	O
As	O
shown	O
in	O
Figure	O
5	O
,	O
we	O
display	O
the	O
influence	O
patterns	O
of	O
news	O
category	O
for	O
different	O
currency	O
pairs	O
.	O
We	O
observe	O
that	O
there	O
are	O
obvious	O
differences	O
among	O
currency	O
pairs	O
.	O
USD	O
-	O
EUR	O
trading	O
pays	O
more	O
attention	O
to	O
the	O
Business	O
Sectors	O
and	O
Politics	O
/	O
International	O
Affairs	O
news	O
.	O
USD	O
-	O
JPY	O
trading	O
is	O
mostly	O
influenced	O
by	O
Business	O
Sectors	O
and	O
Science	O
/	O
Technology	O
news	O
.	O
Politics	O
/	O
International	O
Affairs	O
news	O
has	O
the	O
most	O
significant	O
impact	O
on	O
USD	O
-	O
RMB	O
trading	O
and	O
Business	O
Commodities	O
news	O
effects	O
USD	O
-	O
GBP	O
trading	O
most	O
.	O
The	O
summarized	O
influence	O
patterns	O
can	O
serve	O
as	O
decision	O
-	O
making	O
reference	O
for	O
forex	O
traders	O
when	O
facing	O
news	O
from	O
various	O
categories	O
.	O

The	O
trading	O
data	O
's	O
attention	O
weight	O
for	O
selected	O
news	O
att	O
ij	O
is	O
calculated	O
by	O
the	O
following	O
formula	O
:	O
att	O
ij	O
=	O
att	O
i	O
*	O
s	O
i	O
j	O
(	O
11	O
)	O
Where	O
att	O
i	O
is	O
the	O
trade	O
data	O
's	O
attention	O
on	O
the	O
i	O
-	O
th	O
category	O
in	O
Equation	O
6	O
and	O
s	O
i	O
j	O
in	O
Equation	O
4	O
is	O
the	O
weight	O
of	O
selected	O
news	O
in	O
group	O
.	O
We	O
sum	O
up	O
all	O
the	O
selected	O
news	O
's	O
attention	O
according	O
to	O
their	O
regions	O
and	O
access	O
the	O
region	O
influence	O
weight	O
.	O
The	O
results	O
are	O
shown	O
in	O
Figure	O
6	O
.	O
For	O
each	O
currency	O
pair	O
,	O
the	O
news	O
are	O
divided	O
into	O
three	O
classes	O
:	O
news	O
related	O
to	O
region	O
A	O
only	O
,	O
news	O
related	O
to	O
region	O
B	O
only	O
and	O
news	O
related	O
to	O
both	O
region	O
A	O
and	O
B.	O
And	O
we	O
observe	O
that	O
the	O
news	O
related	O
to	O
both	O
region	O
A	O
and	O
B	O
has	O
the	O
least	O
influence	O
on	O
all	O
currency	O
pairs	O
.	O
News	O
related	O
to	O
the	O
US	O
has	O
the	O
largest	O
influence	O
weight	O
on	O
USD	O
-	O
JPY	O
and	O
USD	O
-	O
GBP	O
trading	O
.	O
Yet	O
news	O
related	O
to	O
China	O
/	O
Europe	O
has	O
a	O
larger	O
influence	O
weight	O
than	O
news	O
related	O
to	O
US	O
in	O
USD	O
-	O
RMB	O
/	O
USD	O
-	O
EUP	O
trading	O
.	O
We	O
can	O
intuitively	O
observe	O
the	O
influence	O
weights	O
of	O
different	O
regions	O
for	O
forex	O
trading	O
,	O
which	O
is	O
helpful	O
for	O
the	O
analysis	O
and	O
forecast	O
of	O
forex	O
movement	O
.	O

The	O
selection	O
number	O
in	O
each	O
group	O
is	O
an	O
essential	O
hyper	O
-	O
parameter	O
to	O
control	O
the	O
amount	O
of	O
extracted	O
information	O
.	O
As	O
shown	O
in	O
Table	O
2	O
,	O
the	O
BHAM	O
-	O
Category	O
performs	O
best	O
when	O
the	O
selection	O
number	O
is	O
3	O
in	O
all	O
currency	O
pairs	O
.	O
When	O
the	O
selection	O
number	O
is	O
small	O
(	O
1	O
,	O
2	O
)	O
,	O
the	O
model	O
is	O
too	O
strict	O
so	O
that	O
some	O
crucial	O
information	O
will	O
be	O
missed	O
.	O
When	O
the	O
selection	O
number	O
is	O
large	O
(	O
4	O
,	O
5	O
)	O
,	O
some	O
less	O
influential	O
news	O
will	O
be	O
selected	O
and	O
interfere	O
model	O
's	O
decision	O
.	O
When	O
we	O
keep	O
all	O
news	O
in	O
the	O
group	O
,	O
the	O
model	O
's	O
performance	O
declines	O
by	O
a	O
large	O
margin	O
.	O
This	O
experiment	O
demonstrates	O
that	O
the	O
selection	O
mechanism	O
plays	O
an	O
important	O
role	O
in	O
the	O
proposed	O
model	O
.	O

This	O
work	O
is	O
supported	O
by	O
a	O
Research	O
Grant	O
from	O
Mizuho	O
Securities	O
Co.	O
,	O
Ltd.	O
Mizuho	O
Securities	O
also	O
provide	O
experiment	O
data	O
and	O
valuable	O
domain	O
experts	O
suggestions	O
.	O

Problem	O
Formulation	O
Throughout	O
the	O
paper	O
,	O
we	O
refer	O
to	O
target	O
code	O
representations	O
as	O
API	O
components	O
.	O
In	O
all	O
cases	O
,	O
components	O
will	O
consist	O
of	O
formal	O
representations	O
of	O
functions	O
,	O
or	O
function	O
signatures	O
(	O
e.g.	O
,	O
long	O
max	O
(	O
int	O
a	O
,	O
int	O
b	O
)	O
)	O
,	O
which	O
include	O
a	O
function	O
name	O
(	O
max	O
)	O
,	O
a	O
sequence	O
of	O
arguments	O
(	O
int	O
a	O
,	O
int	O
b	O
)	O
,	O
and	O
other	O
information	O
such	O
as	O
a	O
return	O
value	O
(	O
long	O
)	O
and	O
namespace	O
(	O
for	O
more	O
details	O
,	O
see	O
Richardson	O
(	O
2018	O
)	O
)	O
.	O
For	O
a	O
given	O
API	O
dataset	O
D	O
=	O
{	O
(	O
x	O
i	O
,	O
z	O
i	O
)	O
}	O
n	O
i=1	O
of	O
size	O
n	O
,	O
the	O
goal	O
is	O
to	O
learn	O
a	O
model	O
that	O
can	O
generate	O
exactly	O
a	O
correct	O
component	O
sequence	O
z	O
=	O
(	O
z	O
1	O
,	O
..	O
,	O
z	O
|	O
z	O
|	O
)	O
,	O
within	O
a	O
finite	O
space	O
C	O
of	O
signatures	O
(	O
i.e.	O
,	O
the	O
space	O
of	O
all	O
defined	O
functions	O
)	O
,	O
for	O
each	O
input	O
text	O
sequence	O
x	O
=	O
(	O
x	O
1	O
,	O
...	O
,	O
x	O
|	O
x	O
|	O
)	O
.	O
This	O
involves	O
learning	O
a	O
probability	O
distribution	O
p	O
(	O
z	O
|	O
x	O
)	O
.	O
As	O
such	O
,	O
one	O
can	O
think	O
of	O
this	O
underlying	O
problem	O
as	O
a	O
constrained	O
MT	O
task	O
.	O
In	O
this	O
section	O
,	O
we	O
describe	O
the	O
baseline	O
approach	O
of	O
Richardson	O
and	O
Kuhn	O
(	O
2017b	O
)	O
.	O
Technically	O
,	O
their	O
approach	O
has	O
two	O
components	O
:	O
a	O
simple	O
word	O
-	O
based	O
translation	O
model	O
and	O
task	O
specific	O
decoder	O
,	O
which	O
is	O
used	O
to	O
generate	O
a	O
k	O
-	O
best	O
list	O
of	O
candidate	O
component	O
representations	O
for	O
a	O
given	O
input	O
x.	O
They	O
then	O
use	O
a	O
discriminative	O
model	O
to	O
rerank	O
the	O
translation	O
output	O
using	O
additional	O
nonworld	O
level	O
features	O
.	O
The	O
goal	O
in	O
this	O
section	O
is	O
to	O
provide	O
the	O
technical	O
details	O
of	O
their	O
translation	O
approach	O
,	O
which	O
we	O
improve	O
in	O
Section	O
4	O
.	O

The	O
translation	O
models	O
investigated	O
in	O
Richardson	O
and	O
Kuhn	O
(	O
2017b	O
)	O
use	O
a	O
noisy	O
-	O
channel	O
formulation	O
where	O
p	O
(	O
z	O
|	O
x	O
)	O
∝	O
p	O
(	O
x	O
|	O
z	O
)	O
p	O
(	O
z	O
)	O
via	O
Bayes	O
rule	O
.	O
By	O
assuming	O
a	O
uniform	O
prior	O
on	O
output	O
components	O
,	O
p	O
(	O
z	O
)	O
,	O
the	O
model	O
therefore	O
involves	O
estimating	O
p	O
(	O
x	O
|	O
z	O
)	O
,	O
which	O
under	O
a	O
word	O
-	O
translation	O
model	O
is	O
computed	O
using	O
the	O
following	O
formula	O
:	O
p	O
(	O
x	O
|	O
z	O
)	O
=	O
a	O
A	O
p	O
(	O
x	O
,	O
a	O
|	O
z	O
)	O
,	O
where	O
the	O
summation	O
ranges	O
over	O
the	O
set	O
of	O
all	O
many	O
-	O
to	O
-	O
one	O
word	O
alignments	O
A	O
from	O
x	O
z	O
,	O
with	O
|	O
A	O
|	O
equal	O
to	O
(	O
|	O
z	O
|	O
+	O
1	O
)	O
|	O
x	O
|	O
.	O
They	O
investigate	O
various	O
types	O
of	O
sequence	O
-	O
based	O
alignment	O
models	O
(	O
Och	O
and	O
Ney	O
,	O
2003	O
)	O
,	O
and	O
find	O
that	O
the	O
classic	O
IBM	O
Model	O
1	O
outperforms	O
more	O
complex	O
word	O
models	O
.	O
This	O
model	O
factors	O
in	O
the	O
following	O
way	O
and	O
assumes	O
an	O
inde	O
-	O
pendent	O
word	O
generation	O
process	O
:	O
p	O
(	O
x	O
|	O
z	O
)	O
=	O
1	O
|	O
A	O
|	O
|	O
x	O
|	O
j=1	O
|	O
z	O
|	O
i=0	O
p	O
t	O
(	O
x	O
j	O
|	O
z	O
i	O
)	O
(	O
1	O
)	O
where	O
each	O
p	O
t	O
defines	O
a	O
multinomial	O
distribution	O
over	O
a	O
given	O
component	O
term	O
z	O
for	O
all	O
words	O
x.	O
The	O
decoding	O
problem	O
for	O
the	O
above	O
translation	O
model	O
involves	O
finding	O
the	O
most	O
likely	O
outputẑ	O
,	O
which	O
requires	O
solving	O
an	O
arg	O
max	O
z	O
over	O
Equation	O
1	O
.	O
In	O
the	O
general	O
case	O
,	O
this	O
problem	O
is	O
known	O
to	O
be	O
N	O
P	O
-	O
complete	O
for	O
the	O
models	O
under	O
consideration	O
(	O
Knight	O
,	O
1999	O
)	O
largely	O
due	O
to	O
the	O
large	O
space	O
of	O
possible	O
predictions	O
z.	O
Richardson	O
and	O
Kuhn	O
(	O
2017b	O
)	O
avoid	O
these	O
issues	O
by	O
exploiting	O
the	O
finiteness	O
of	O
the	O
target	O
component	O
search	O
space	O
(	O
an	O
idea	O
we	O
also	O
pursue	O
here	O
and	O
discuss	O
more	O
below	O
)	O
,	O
and	O
describe	O
a	O
constrained	O
decoding	O
algorithm	O
that	O
runs	O
in	O
time	O
O	O
(	O
|	O
C	O
|	O
log	O
|	O
C	O
|	O
)	O
.	O
While	O
this	O
works	O
well	O
for	O
small	O
APIs	O
,	O
it	O
becomes	O
less	O
feasible	O
when	O
dealing	O
with	O
large	O
sets	O
of	O
APIs	O
,	O
as	O
in	O
the	O
polyglot	O
case	O
,	O
or	O
with	O
more	O
complex	O
semantic	O
languages	O
typically	O
used	O
in	O
SP	O
(	O
Liang	O
,	O
2013	O
)	O
.	O

Our	O
framework	O
facilitates	O
both	O
monolingual	O
and	O
polyglot	O
decoding	O
.	O
In	O
the	O
first	O
case	O
,	O
the	O
decoder	O
requires	O
a	O
graph	O
associated	O
with	O
the	O
output	O
semantic	O
language	O
(	O
more	O
details	O
in	O
next	O
section	O
)	O
and	O
a	O
trained	O
translation	O
model	O
.	O
The	O
latter	O
case	O
requires	O
taking	O
the	O
union	O
of	O
all	O
datasets	O
and	O
graphs	O
(	O
with	O
artificial	O
identifier	O
tokens	O
)	O
for	O
a	O
collection	O
of	O
target	O
datasets	O
and	O
training	O
a	O
single	O
model	O
over	O
this	O
global	O
dataset	O
.	O
In	O
this	O
setting	O
,	O
we	O
can	O
then	O
decode	O
to	O
a	O
particular	O
language	O
using	O
the	O
language	O
identifiers	O
or	O
decode	O
without	O
specifying	O
the	O
output	O
language	O
.	O
The	O
main	O
focus	O
in	O
this	O
paper	O
is	O
investigating	O
polyglot	O
decoding	O
,	O
and	O
in	O
particular	O
the	O
effect	O
of	O
training	O
models	O
on	O
multiple	O
datasets	O
when	O
translating	O
to	O
individuals	O
APIs	O
or	O
SP	O
datasets	O
.	O
When	O
evaluating	O
our	O
models	O
and	O
building	O
QA	O
applications	O
,	O
it	O
is	O
important	O
to	O
be	O
able	O
to	O
generate	O
the	O
k	O
best	O
translations	O
.	O
This	O
can	O
easily	O
be	O
done	O
in	O
our	O
framework	O
by	O
applying	O
standard	O
k	O
SSSP	O
algorithms	O
(	O
Brander	O
and	O
Sinclair	O
,	O
1995	O
)	O
.	O
We	O
use	O
an	O
implementation	O
of	O
the	O
algorithm	O
of	O
Yen	O
(	O
1971	O
)	O
,	O
which	O
works	O
on	O
top	O
of	O
the	O
SSSP	O
algorithms	O
introduced	O
above	O
by	O
iteratively	O
finding	O
deviating	O
or	O
branching	O
paths	O
from	O
an	O
initial	O
SSSP	O
(	O
more	O
details	O
provided	O
in	O
supplementary	O
materials	O
)	O
.	O

We	O
experimented	O
with	O
two	O
main	O
types	O
of	O
resources	O
:	O
45	O
API	O
documentation	O
datasets	O
and	O
two	O
multilingual	O
benchmark	O
SP	O
datasets	O
.	O
In	O
the	O
former	O
case	O
,	O
our	O
main	O
objective	O
is	O
to	O
test	O
whether	O
training	O
polyglot	O
models	O
(	O
shown	O
as	O
polyglot	O
in	O
Tables	O
1	O
-	O
2	O
)	O
on	O
multiple	O
datasets	O
leads	O
to	O
an	O
improvement	O
when	O
compared	O
to	O
training	O
individual	O
monolingual	O
models	O
(	O
shown	O
as	O
monolingual	O
in	O
Tables	O
1	O
-	O
2	O
)	O
.	O
Experiments	O
involving	O
the	O
latter	O
datasets	O
are	O
meant	O
to	O
test	O
the	O
applicability	O
of	O
our	O
general	O
graph	O
and	O
polyglot	O
method	O
to	O
related	O
SP	O
tasks	O
,	O
and	O
are	O
also	O
used	O
for	O
comparison	O
against	O
our	O
main	O
technical	O
documentation	O
task	O
.	O
Figure	O
3	O
:	O
Test	O
Acc@1	O
for	O
the	O
best	O
monolingual	O
models	O
(	O
in	O
yellow	O
/	O
left	O
)	O
compared	O
with	O
the	O
best	O
lexical	O
polyglot	O
model	O
(	O
green	O
/	O
right	O
)	O
across	O
all	O
45	O
technical	O
documentation	O
datasets	O
.	O

We	O
use	O
the	O
Foma	O
finite	O
-	O
state	O
toolkit	O
of	O
Hulden	O
(	O
2009	O
)	O
to	O
construct	O
all	O
graphs	O
used	O
in	O
our	O
experiments	O
.	O
We	O
also	O
use	O
the	O
Cython	O
version	O
of	O
Dynet	O
(	O
Neubig	O
et	O
al	O
,	O
2017	O
)	O
to	O
implement	O
all	O
the	O
neural	O
models	O
(	O
see	O
supp	O
.	O
materials	O
for	O
more	O
details	O
)	O
.	O
In	O
the	O
results	O
tables	O
,	O
we	O
refer	O
to	O
the	O
lexical	O
and	O
neural	O
models	O
introduced	O
in	O
Section	O
4	O
as	O
Lexical	O
Shortest	O
Path	O
and	O
Neural	O
Shortest	O
Path	O
,	O
where	O
models	O
that	O
use	O
copying	O
(	O
+	O
copy	O
)	O
and	O
lexical	O
biasing	O
(	O
+	O
bias	O
)	O
are	O
marked	O
accordingly	O
.	O
We	O
also	O
experimented	O
with	O
adding	O
a	O
discriminative	O
reranker	O
to	O
our	O
lexical	O
models	O
(	O
+	O
rerank	O
)	O
,	O
using	O
the	O
approach	O
from	O
Richardson	O
and	O
Kuhn	O
(	O
2017b	O
)	O
,	O
which	O
uses	O
additional	O
lexical	O
(	O
e.g.	O
,	O
word	O
match	O
and	O
alignment	O
)	O
features	O
and	O
other	O
phrase	O
-	O
level	O
and	O
syntax	O
features	O
.	O
The	O
goal	O
here	O
is	O
to	O
see	O
if	O
these	O
additional	O
(	O
mostly	O
non	O
-	O
word	O
level	O
)	O
features	O
help	O
improve	O
on	O
the	O
baseline	O
lexical	O
models	O
.	O

We	O
look	O
at	O
learning	O
from	O
multiple	O
API	O
libraries	O
and	O
datasets	O
in	O
the	O
context	O
of	O
learning	O
to	O
translate	O
text	O
to	O
code	O
representations	O
and	O
other	O
SP	O
tasks	O
.	O
To	O
support	O
polyglot	O
modeling	O
of	O
this	O
type	O
,	O
we	O
developed	O
a	O
novel	O
graph	O
based	O
decoding	O
method	O
and	O
experimented	O
with	O
various	O
SMT	O
and	O
neural	O
MT	O
models	O
that	O
work	O
in	O
this	O
framework	O
.	O
We	O
report	O
a	O
mixture	O
of	O
positive	O
results	O
specific	O
to	O
each	O
task	O
and	O
set	O
of	O
models	O
,	O
some	O
of	O
which	O
reveal	O
interesting	O
limitations	O
of	O
different	O
approaches	O
to	O
SP	O
.	O
We	O
also	O
introduced	O
new	O
API	O
and	O
mixed	O
language	O
datasets	O
to	O
facilitate	O
further	O
work	O
on	O
polyglot	O
SP	O
.	O

This	O
work	O
was	O
supported	O
by	O
the	O
German	O
Research	O
Foundation	O
(	O
DFG	O
)	O
in	O
project	O
D2	O
of	O
SFB	O
732	O
.	O

Nomen	O
Omen	O
.	O
Enhancing	O
the	O
Latin	O
Morphological	O
Analyser	O
Lemlat	O
with	O
an	O
Onomasticon	O

Lemlat	O
is	O
a	O
morphological	O
analyser	O
for	O
Latin	O
,	O
which	O
shows	O
a	O
remarkably	O
wide	O
coverage	O
of	O
the	O
Latin	O
lexicon	O
.	O
However	O
,	O
the	O
performance	O
of	O
the	O
tool	O
is	O
limited	O
by	O
the	O
absence	O
of	O
proper	O
names	O
in	O
its	O
lexical	O
basis	O
.	O
In	O
this	O
paper	O
we	O
present	O
the	O
extension	O
of	O
Lemlat	O
with	O
a	O
large	O
Onomasticon	O
for	O
Latin	O
.	O
First	O
,	O
we	O
describe	O
and	O
motivate	O
the	O
automatic	O
and	O
manual	O
procedures	O
for	O
including	O
the	O
proper	O
names	O
in	O
Lemlat	O
.	O
Then	O
,	O
we	O
compare	O
the	O
new	O
version	O
of	O
Lemlat	O
with	O
the	O
previous	O
one	O
,	O
by	O
evaluating	O
their	O
lexical	O
coverage	O
of	O
four	O
Latin	O
texts	O
of	O
different	O
era	O
and	O
genre	O
.	O

We	O
evaluated	O
the	O
quality	O
of	O
the	O
rules	O
for	O
automatic	O
enhancement	O
by	O
precision	O
and	O
recall	O
(	O
Van	O
Rijsbergen	O
,	O
1979	O
)	O
.	O
Measuring	O
the	O
precision	O
of	O
our	O
rules	O
is	O
straightforward	O
.	O
As	O
said	O
,	O
while	O
writing	O
the	O
rules	O
,	O
we	O
focused	O
on	O
inflectionally	O
regular	O
groups	O
of	O
lemmas	O
.	O
As	O
a	O
consequence	O
,	O
we	O
never	O
had	O
to	O
modify	O
the	O
output	O
of	O
rules	O
neither	O
in	O
terms	O
of	O
removal	O
of	O
results	O
(	O
i.e.	O
wrong	O
results	O
due	O
to	O
overproduction	O
)	O
nor	O
in	O
terms	O
of	O
completion	O
of	O
results	O
(	O
i.e.	O
wrong	O
results	O
due	O
to	O
underproduction	O
)	O
.	O
Thus	O
,	O
the	O
precision	O
of	O
our	O
rules	O
is	O
always	O
100	O
%	O
.	O
To	O
calculate	O
recall	O
,	O
we	O
grouped	O
all	O
those	O
rules	O
that	O
treat	O
lemmas	O
of	O
the	O
same	O
inflectional	O
class	O
(	O
e.g.	O
all	O
rules	O
for	O
nouns	O
of	O
the	O
first	O
declension	O
)	O
.	O
We	O
measured	O
the	O
recall	O
of	O
such	O
groups	O
of	O
rules	O
by	O
comparing	O
the	O
number	O
of	O
lemmas	O
automatically	O
inserted	O
into	O
Lemlat	O
by	O
one	O
group	O
of	O
rules	O
with	O
the	O
total	O
number	O
of	O
lemmas	O
in	O
the	O
Onomasticon	O
of	O
Forcellini	O
belonging	O
to	O
the	O
inflectional	O
class	O
addressed	O
by	O
that	O
group	O
of	O
rules	O
.	O
The	O
most	O
problematic	O
inflectional	O
class	O
is	O
that	O
of	O
third	O
declension	O
nouns	O
.	O
2	O
As	O
mentioned	O
above	O
,	O
this	O
is	O
motivated	O
by	O
the	O
fact	O
that	O
it	O
is	O
not	O
always	O
possible	O
to	O
match	O
regularly	O
an	O
inflectional	O
paradigm	O
(	O
e.g.	O
third	O
declension	O
imparisyllable	O
nouns	O
)	O
with	O
one	O
specific	O
ending	O
.	O
Hence	O
,	O
given	O
such	O
a	O
low	O
recall	O
,	O
the	O
amount	O
of	O
manual	O
work	O
required	O
for	O
enhancing	O
Lemlat	O
with	O
third	O
declension	O
proper	O
names	O
was	O
quite	O
considerable	O
.	O
To	O
provide	O
an	O
example	O
,	O
the	O
number	O
of	O
third	O
declension	O
feminine	O
nouns	O
in	O
the	O
Onomasticon	O
is	O
1	O
,	O
200	O
.	O
Our	O
rules	O
covered	O
only	O
542	O
out	O
of	O
them	O
.	O
Thus	O
,	O
658	O
nouns	O
had	O
to	O
be	O
inserted	O
into	O
Lemlat	O
manually	O
(	O
54.833	O
%	O
of	O
the	O
total	O
for	O
that	O
class	O
)	O
.	O
There	O
are	O
also	O
entire	O
inflectional	O
classes	O
for	O
which	O
writing	O
a	O
rule	O
was	O
not	O
possible	O
,	O
like	O
for	O
instance	O
Busa	O
's	O
class	O
of	O
irregularly	O
inflected	O
nouns	O
(	O
146	O
wordforms	O
)	O
.	O
All	O
these	O
lemmas	O
were	O
inserted	O
into	O
the	O
LES	O
archive	O
manually	O
.	O
In	O
total	O
,	O
the	O
number	O
of	O
lemmas	O
transferred	O
manually	O
into	O
Lemlat	O
is	O
1	O
,	O
752	O
(	O
6.632	O
%	O
of	O
all	O
the	O
lemmas	O
of	O
the	O
Onomasticon	O
)	O
.	O

In	O
this	O
paper	O
we	O
described	O
the	O
enhancing	O
of	O
the	O
morphological	O
analyser	O
for	O
Latin	O
Lemlat	O
with	O
a	O
large	O
Onomasticon	O
provided	O
by	O
a	O
reference	O
dictionary	O
for	O
Latin	O
(	O
Forcellini	O
)	O
.	O
Although	O
we	O
have	O
included	O
most	O
of	O
the	O
words	O
of	O
the	O
Onomasticon	O
into	O
Lemlat	O
,	O
the	O
work	O
is	O
far	O
from	O
being	O
complete	O
.	O
Indeed	O
,	O
we	O
have	O
just	O
started	O
to	O
enhance	O
the	O
analyser	O
with	O
graphical	O
variants	O
.	O
Furthermore	O
,	O
around	O
2	O
,	O
000	O
words	O
of	O
the	O
Onomasticon	O
belonging	O
to	O
minor	O
and	O
irregular	O
inflectional	O
classes	O
still	O
have	O
to	O
be	O
included	O
into	O
Lemlat	O
.	O
Although	O
this	O
promises	O
to	O
be	O
a	O
largely	O
manual	O
and	O
time	O
-	O
consuming	O
work	O
,	O
it	O
is	O
worth	O
doing	O
for	O
achieving	O
the	O
lexicographically	O
motivated	O
completeness	O
of	O
the	O
tool	O
's	O
lexical	O
basis	O
.	O
Once	O
completed	O
,	O
the	O
lexical	O
look	O
-	O
up	O
table	O
of	O
the	O
Onomasticon	O
will	O
become	O
part	O
of	O
the	O
overall	O
Lemlat	O
suite	O
,	O
which	O
will	O
be	O
shortly	O
made	O
available	O
for	O
free	O
download	O
and	O
on	O
-	O
line	O
use	O
.	O

Text	O
Editing	O
by	O
Command	O

We	O
retrieve	O
grounding	O
snippets	O
for	O
the	O
edits	O
in	O
our	O
dataset	O
by	O
querying	O
a	O
commercial	O
search	O
engine	O
.	O
In	O
order	O
to	O
formulate	O
a	O
query	O
for	O
a	O
given	O
edit	O
,	O
we	O
combine	O
the	O
relevant	O
page	O
and	O
section	O
titles	O
with	O
keywords	O
5	O
from	O
the	O
target	O
sentence	O
.	O
While	O
the	O
target	O
sentence	O
is	O
not	O
available	O
at	O
test	O
time	O
,	O
we	O
make	O
the	O
assumption	O
that	O
in	O
a	O
real	O
user	O
scenario	O
the	O
relevant	O
grounding	O
would	O
be	O
provided	O
by	O
the	O
user	O
.	O
We	O
retrieve	O
the	O
top	O
200	O
returned	O
web	O
page	O
results	O
and	O
only	O
keep	O
the	O
preview	O
snippets	O
returned	O
by	O
the	O
search	O
engine	O
as	O
the	O
grounding	O
corpus	O
.	O
6	O
Because	O
Wikipedia	O
,	O
as	O
well	O
as	O
several	O
clones	O
,	O
often	O
appear	O
in	O
search	O
engine	O
results	O
,	O
we	O
check	O
for	O
4	O
-	O
gram	O
overlap	O
between	O
the	O
target	O
sentence	O
and	O
each	O
grounding	O
snippet	O
,	O
removing	O
any	O
snippet	O
with	O
more	O
than	O
50	O
%	O
overlap	O
.	O
Finally	O
,	O
we	O
rerank	O
7	O
the	O
retrieved	O
snippets	O
using	O
an	O
information	O
extraction	O
score	O
,	O
and	O
merge	O
the	O
ranked	O
snippets	O
to	O
take	O
the	O
first	O
N	O
=	O
512	O
tokens	O
.	O

Percentiles	O
Mean	O
25	O
%	O
50	O
%	O
75	O
%	O

Krishna	O
attended	O
Dartmouth	O
College	O
where	O
she	O
was	O
a	O
double	O
major	O
in	O
government	O
and	O
French	O
.	O

Krishna	O
attended	O
Dartmouth	O
College	O
where	O
she	O
was	O
a	O
double	O
major	O
in	O
government	O
and	O
French	O
and	O
graduated	O
in	O
the	O
class	O
of	O
'	O
13	O
.	O

Mountain	O
State	O
is	O
currently	O
seeing	O
alternative	O
accreditation	O
by	O
the	O
Commission	O
on	O
Collegiate	O
Nursing	O
Education	O
.	O

Mountain	O
State	O
is	O
currently	O
seeking	O
alternative	O
accreditation	O
by	O
the	O
Commission	O
on	O
Collegiate	O
Nursing	O
Education	O
.	O
Comment	O
correct	O
year	O
of	O
marriage	O
(	O
did	O
not	O
fit	O
NSW	O
records	O
)	O

He	O
married	O
Margaret	O
Frances	O
Prowse	O
Shaw	O
in	O
Sydney	O
in	O
1874	O
.	O

He	O
married	O
Margaret	O
Frances	O
Prowse	O
Shaw	O
in	O
Sydney	O
in	O
1871	O
.	O

Entitled	O
"	O
It	O
Feels	O
Like	O
Home	O
(	O
Re	O
Invented	O
)	O
Tour	O
2011	O
"	O
,	O
it	O
contained	O
his	O
songs	O
and	O
remakes	O
of	O
Alliage	O
hits	O
.	O

Johnson	O
married	O
Group	O
1	O
Crew	O
member	O
Manwell	O
Reyes	O
in	O
2011	O
.	O

Johnson	O
married	O
Group	O
1	O
Crew	O
member	O
Manwell	O
Reyes	O
in	O
2011	O
.	O
Johnson	O
married	O
Group	O
1	O
Crew	O
member	O
Manwell	O
Reyes	O
in	O
2011	O
in	O
a	O
ceremony	O
at	O
Half	O
Moon	O
Bay	O
,	O
California	O
.	O

They	O
are	O
more	O
frequent	O
than	O
primary	O
brain	O
tumors	O
.	O
They	O
are	O
more	O
frequent	O
than	O
primary	O
brain	O
tumors	O
,	O
and	O
are	O
mainly	O
a	O
problem	O
in	O
adults	O
,	O
though	O
children	O
may	O
also	O
have	O
secondary	O
tumors	O
.	O

Text	O
Geoff	O
Hinton	O
is	O
an	O
English	O
tennis	O
player	O
.	O

Geoffrey	O
Hinton	O
is	O
a	O
computer	O
science	O
professor	O
at	O
the	O
University	O
of	O
Toronto	O
.	O

Geoffrey	O
Hinton	O
is	O
an	O
English	O
-	O
Canadian	O
computer	O
science	O
professor	O
at	O
the	O
University	O
of	O
Toronto	O
.	O

Geoffrey	O
Hinton	O
(	O
born	O
1946	O
)	O
is	O
an	O
English	O
-	O
Canadian	O
computer	O
science	O
professor	O
at	O
the	O
University	O
of	O
Toronto	O
.	O

Geoffrey	O
Hinton	O
(	O
born	O
1946	O
)	O
is	O
an	O
English	O
-	O
Canadian	O
computer	O
science	O
professor	O
at	O
the	O
University	O
of	O
Toronto	O
.	O
Geoffrey	O
Hinton	O
is	O
most	O
famous	O
for	O
his	O
work	O
on	O
artificial	O
neural	O
networks	O
.	O
Table	O
9	O
:	O
An	O
example	O
of	O
a	O
multi	O
-	O
turn	O
interaction	O
with	O
our	O
model	O
.	O
At	O
each	O
turn	O
,	O
the	O
edit	O
was	O
chosen	O
among	O
the	O
top	O
3	O
outputs	O
returned	O
by	O
beam	O
-	O
search	O
.	O
See	O
table	O
12	O
in	O
the	O
appendix	O
for	O
the	O
grounding	O
used	O
in	O
this	O
example	O
.	O
This	O
paper	O
focuses	O
on	O
the	O
task	O
of	O
editing	O
individual	O
sentences	O
,	O
which	O
we	O
believe	O
to	O
be	O
a	O
challenging	O
task	O
for	O
NLP	O
,	O
as	O
it	O
involves	O
making	O
nuanced	O
changes	O
to	O
text	O
according	O
to	O
natural	O
language	O
commands	O
.	O
We	O
also	O
believe	O
this	O
task	O
has	O
useful	O
applications	O
,	O
particularly	O
in	O
speech	O
-	O
to	O
-	O
text	O
scenarios	O
,	O
where	O
it	O
may	O
be	O
more	O
convenient	O
to	O
speak	O
out	O
a	O
command	O
rather	O
than	O
edit	O
the	O
text	O
directly	O
.	O
However	O
,	O
we	O
also	O
wish	O
to	O
emphasize	O
that	O
this	O
task	O
is	O
a	O
step	O
towards	O
a	O
larger	O
goal	O
of	O
interactive	O
document	O
generation	O
,	O
and	O
that	O
there	O
are	O
many	O
interesting	O
future	O
directions	O
to	O
explore	O
in	O
this	O
space	O
.	O
While	O
this	O
paper	O
has	O
focused	O
on	O
single	O
interactions	O
(	O
i.e.	O
making	O
isolated	O
edits	O
to	O
text	O
)	O
,	O
it	O
would	O
be	O
worth	O
modeling	O
multiple	O
interactions	O
between	O
the	O
user	O
and	O
model	O
.	O
One	O
can	O
imagine	O
that	O
there	O
may	O
be	O
a	O
natural	O
order	O
in	O
which	O
to	O
make	O
edits	O
,	O
such	O
as	O
adding	O
information	O
at	O
the	O
start	O
,	O
and	O
fine	O
-	O
tuning	O
the	O
language	O
at	O
the	O
end	O
.	O
It	O
is	O
an	O
open	O
question	O
whether	O
or	O
not	O
a	O
model	O
could	O
learn	O
this	O
.	O
For	O
illustration	O
,	O
table	O
9	O
gives	O
an	O
example	O
of	O
using	O
our	O
model	O
to	O
make	O
several	O
edits	O
in	O
order	O
to	O
create	O
a	O
sentence	O
.	O
Ultimately	O
,	O
this	O
may	O
look	O
more	O
like	O
a	O
dialogue	O
than	O
a	O
sequence	O
of	O
commands	O
coming	O
from	O
the	O
user	O
.	O
Additionally	O
,	O
it	O
would	O
also	O
be	O
interesting	O
to	O
look	O
at	O
other	O
settings	O
where	O
a	O
model	O
must	O
generate	O
a	O
complex	O
,	O
structured	O
object	O
for	O
a	O
user	O
,	O
such	O
as	O
code	O
,	O
or	O
images	O
.	O
We	O
hope	O
that	O
our	O
text	O
editing	O
task	O
,	O
as	O
a	O
first	O
step	O
,	O
can	O
demonstrate	O
the	O
potential	O
for	O
interactive	O
generation	O
systems	O
,	O
and	O
that	O
it	O
will	O
encourage	O
the	O
community	O
to	O
pursue	O
more	O
ideas	O
in	O
this	O
space	O
.	O

For	O
a	O
given	O
edit	O
,	O
we	O
combine	O
the	O
relevant	O
page	O
and	O
section	O
titles	O
with	O
keywords	O
from	O
the	O
target	O
sentence	O
to	O
construct	O
a	O
query	O
that	O
we	O
use	O
to	O
retrieve	O
grounding	O
from	O
a	O
commercial	O
search	O
engine	O
.	O
In	O
order	O
to	O
identify	O
keywords	O
we	O
look	O
at	O
document	O
frequency	O
df	O
(	O
w	O
)	O
=	O
|	O
{	O
D	O
D	O
|	O
w	O
D	O
}	O
|	O
|	O
D	O
|	O
,	O
where	O
D	O
is	O
a	O
sample	O
of	O
500	O
,	O
000	O
Wikipedia	O
articles	O
taken	O
from	O
the	O
Tensorflow	O
Wikipedia	O
dataset	O
.	O
12	O
We	O
consider	O
words	O
w	O
with	O
df	O
(	O
w	O
)	O
<	O
0.01	O
to	O
be	O
keywords	O
.	O

Because	O
the	O
combined	O
length	O
of	O
the	O
grounding	O
snippets	O
we	O
retrieve	O
far	O
exceeds	O
the	O
capacity	O
of	O
our	O
model	O
,	O
we	O
rerank	O
the	O
retrieved	O
snippets	O
using	O
an	O
information	O
extraction	O
score	O
.	O
We	O
then	O
merge	O
the	O
ranked	O
snippets	O
and	O
take	O
only	O
the	O
first	O
N	O
=	O
512	O
tokens	O
.	O
Following	O
(	O
Liu	O
et	O
al	O
,	O
2018a	O
)	O
we	O
use	O
tf	O
-	O
idf	O
scores	O
to	O
rerank	O
.	O
For	O
a	O
given	O
edit	O
s	O
−	O
s	O
,	O
with	O
retrieved	O
grounding	O
documents	O
G	O
,	O
the	O
information	O
extraction	O
score	O
of	O
snippet	O
G	O
G	O
is	O
score	O
(	O
G	O
)	O
=	O
w	O
s	O
tf	O
-	O
idf	O
(	O
w	O
,	O
G	O
)	O
,	O
where	O
the	O
tf	O
-	O
idf	O
score	O
of	O
word	O
w	O
is	O
tf	O
-	O
idf	O
(	O
w	O
,	O
G	O
)	O
=	O
N	O
w	O
(	O
G	O
)	O
log	O
N	O
g	O
N	O
gw	O
,	O
where	O
N	O
w	O
(	O
G	O
)	O
is	O
the	O
number	O
of	O
occurrences	O
of	O
w	O
in	O
G	O
,	O
N	O
gw	O
is	O
the	O
number	O
of	O
documents	O
in	O
G	O
that	O
contain	O
w	O
,	O
and	O
N	O
g	O
is	O
the	O
number	O
of	O
documents	O
in	O
G.	O
.	O
%	O
Edits	O
gives	O
the	O
prevalence	O
of	O
each	O
label	O
in	O
our	O
data	O
,	O
while	O
%	O
Orig	O
.	O
gives	O
the	O
prevalence	O
in	O
the	O
hand	O
-	O
labelled	O
dataset	O
presented	O
in	O
.	O
The	O
percentages	O
do	O
not	O
total	O
100	O
because	O
edits	O
can	O
have	O
multiple	O
labels	O
.	O

Reword	O

ByteDance	O
responded	O
by	O
adding	O
a	O
kids	O
-	O
only	O
mode	O
to	O
TikTok	O
which	O
allows	O
music	O
videos	O
to	O
be	O
recorded	O
,	O
but	O
not	O
posted	O
and	O
by	O
removing	O
some	O
accounts	O
and	O
content	O
from	O
those	O
determined	O
to	O
be	O
underage	O
.	O

ByteDance	O
responded	O
by	O
adding	O
a	O
kids	O
-	O
only	O
mode	O
to	O
TikTok	O
which	O
blocks	O
the	O
upload	O
of	O
videos	O
,	O
the	O
building	O
of	O
user	O
profiles	O
,	O
direct	O
messaging	O
,	O
and	O
commenting	O
on	O
other	O
's	O
videos	O
,	O
while	O
still	O
allowing	O
the	O
viewing	O
and	O
recording	O
of	O
content	O
.	O

The	O
authors	O
would	O
like	O
to	O
thank	O
Thomas	O
Hofmann	O
,	O
as	O
well	O
as	O
Sudha	O
Rao	O
,	O
Matt	O
Richardson	O
,	O
Zhang	O
Li	O
,	O
Kosh	O
Narayanan	O
,	O
and	O
Chandra	O
Chikkareddy	O
for	O
their	O
helpful	O
suggestions	O
.	O

We	O
are	O
very	O
grateful	O
to	O
the	O
mentor	O
of	O
this	O
paper	O
for	O
her	O
meaningful	O
feedback	O
.	O
Thanks	O
three	O
anonymous	O
reviewers	O
for	O
their	O
insightful	O
comments	O
and	O
practical	O
suggestions	O
.	O

Cogent	O
:	O
A	O
Generic	O
Dialogue	O
System	O
Shell	O
Based	O
on	O
a	O
Collaborative	O
Problem	O
Solving	O
Model	O

The	O
bulk	O
of	O
current	O
research	O
in	O
dialogue	O
systems	O
is	O
focused	O
on	O
fairly	O
simple	O
task	O
models	O
,	O
primarily	O
state	O
-	O
based	O
.	O
Progress	O
on	O
developing	O
dialogue	O
systems	O
for	O
more	O
complex	O
tasks	O
has	O
been	O
limited	O
by	O
the	O
lack	O
generic	O
toolkits	O
to	O
build	O
from	O
.	O
In	O
this	O
paper	O
we	O
report	O
on	O
our	O
development	O
from	O
the	O
ground	O
up	O
of	O
a	O
new	O
dialogue	O
model	O
based	O
on	O
collaborative	O
problem	O
solving	O
.	O
We	O
implemented	O
the	O
model	O
in	O
a	O
dialogue	O
system	O
shell	O
(	O
Cogent	O
)	O
that	O
allows	O
developers	O
to	O
plug	O
in	O
problem	O
-	O
solving	O
agents	O
to	O
create	O
dialogue	O
systems	O
in	O
new	O
domains	O
.	O
The	O
Cogent	O
shell	O
has	O
now	O
been	O
used	O
by	O
several	O
independent	O
teams	O
of	O
researchers	O
to	O
develop	O
dialogue	O
systems	O
in	O
different	O
domains	O
,	O
with	O
varied	O
lexicons	O
and	O
interaction	O
style	O
,	O
each	O
with	O
their	O
own	O
problem	O
-	O
solving	O
backend	O
.	O
We	O
believe	O
this	O
to	O
be	O
the	O
first	O
practical	O
demonstration	O
of	O
the	O
feasibility	O
of	O
a	O
CPSbased	O
dialogue	O
system	O
shell	O
.	O

This	O
system	O
uses	O
a	O
computational	O
model	O
of	O
music	O
cognition	O
,	O
as	O
well	O
as	O
knowledge	O
about	O
existing	O
pieces	O
of	O
music	O
,	O
to	O
help	O
a	O
human	O
composer	O
create	O
and	O
edit	O
a	O
musical	O
score	O
(	O
Quick	O
and	O
Morrison	O
,	O
2017	O
)	O
.	O
SMILEE	O
:	O
This	O
system	O
acts	O
as	O
a	O
partner	O
for	O
playing	O
a	O
cooperative	O
game	O
(	O
Kim	O
et	O
al	O
,	O
2018	O
)	O
.	O
The	O
game	O
involves	O
placing	O
pieces	O
(	O
blocks	O
)	O
on	O
a	O
board	O
to	O
create	O
complex	O
symmetrical	O
configurations	O
.	O
Players	O
alternate	O
,	O
but	O
each	O
player	O
can	O
hold	O
their	O
turn	O
for	O
multiple	O
rounds	O
.	O
Each	O
player	O
has	O
some	O
freedom	O
to	O
be	O
creative	O
with	O
respect	O
to	O
the	O
configuration	O
being	O
pursued	O
(	O
it	O
is	O
not	O
set	O
in	O
advance	O
)	O
.	O
Thus	O
,	O
they	O
have	O
to	O
negotiate	O
turn	O
taking	O
,	O
and	O
they	O
can	O
ask	O
for	O
explanations	O
to	O
achieve	O
a	O
shared	O
understanding	O
about	O
the	O
properties	O
of	O
the	O
configuration	O
being	O
created	O
.	O
Aesop	O
:	O
A	O
system	O
for	O
building	O
animated	O
stories	O
.	O
The	O
user	O
acts	O
as	O
a	O
director	O
,	O
and	O
can	O
choose	O
scenes	O
,	O
props	O
,	O
characters	O
,	O
direct	O
them	O
what	O
to	O
do	O
,	O
etc	O
.	O
Essentially	O
,	O
the	O
system	O
provides	O
a	O
dialogue	O
interface	O
to	O
a	O
sophisticated	O
system	O
for	O
creating	O
visual	O
narratives	O
.	O
Of	O
note	O
,	O
these	O
systems	O
work	O
in	O
several	O
application	O
domains	O
,	O
with	O
varying	O
interaction	O
styles	O
.	O
Musica	O
and	O
Aesop	O
currently	O
work	O
mostly	O
in	O
fixedinitiative	O
mode	O
(	O
user	O
tells	O
the	O
system	O
what	O
to	O
do	O
)	O
.	O
All	O
others	O
involve	O
varying	O
degrees	O
of	O
mixed	O
initiative	O
.	O
While	O
Cabot	O
is	O
a	O
more	O
traditional	O
planning	O
domain	O
,	O
it	O
is	O
interesting	O
to	O
note	O
that	O
all	O
others	O
involve	O
fairly	O
open	O
-	O
ended	O
collaborative	O
tasks	O
,	O
for	O
which	O
the	O
ultimate	O
goal	O
is	O
learning	O
or	O
creating	O
something	O
new	O
.	O
BoB	O
is	O
notable	O
for	O
the	O
fact	O
that	O
it	O
is	O
helping	O
the	O
user	O
learn	O
new	O
knowledge	O
,	O
by	O
helping	O
to	O
formulate	O
and	O
evaluate	O
biological	O
hypotheses	O
(	O
which	O
may	O
even	O
lead	O
to	O
new	O
scientific	O
discoveries	O
)	O
.	O
Importantly	O
,	O
with	O
the	O
exception	O
of	O
Cabot	O
-	O
L	O
,	O
which	O
was	O
developed	O
by	O
our	O
team	O
,	O
all	O
others	O
were	O
developed	O
by	O
independent	O
teams	O
(	O
the	O
BAs	O
for	O
Cabot	O
and	O
BoB	O
were	O
developed	O
by	O
a	O
single	O
team	O
,	O
though	O
the	O
latter	O
also	O
involved	O
collaboration	O
with	O
a	O
large	O
group	O
of	O
biologists	O
and	O
bioinformaticians	O
)	O
.	O
We	O
helped	O
those	O
teams	O
understand	O
how	O
our	O
tools	O
work	O
and	O
the	O
meaning	O
of	O
the	O
CPS	O
acts	O
(	O
especially	O
to	O
the	O
early	O
adopters	O
,	O
who	O
did	O
not	O
have	O
the	O
benefit	O
of	O
much	O
documentation	O
)	O
,	O
but	O
we	O
had	O
no	O
role	O
in	O
deciding	O
what	O
problemsolving	O
behaviors	O
they	O
should	O
or	O
should	O
not	O
implement	O
,	O
how	O
to	O
implement	O
them	O
and	O
so	O
on	O
.	O
Two	O
of	O
the	O
systems	O
(	O
BoB	O
and	O
Musica	O
)	O
required	O
additions	O
to	O
our	O
surface	O
NLP	O
components	O
(	O
mainly	O
add	O
-	O

We	O
present	O
the	O
first	O
systematic	O
study	O
of	O
negative	O
interference	O
in	O
multilingual	O
models	O
and	O
shed	O
light	O
on	O
its	O
causes	O
.	O
We	O
further	O
propose	O
a	O
method	O
and	O
show	O
it	O
can	O
improve	O
cross	O
-	O
lingual	O
transferability	O
by	O
mitigating	O
negative	O
interference	O
.	O
While	O
prior	O
efforts	O
focus	O
on	O
improving	O
sharing	O
and	O
cross	O
-	O
lingual	O
alignment	O
,	O
we	O
provide	O
new	O
insights	O
and	O
a	O
different	O
perspective	O
on	O
unsharing	O
and	O
resolving	O
language	O
conflicts	O
.	O

We	O
want	O
to	O
thank	O
Jaime	O
Carbonell	O
for	O
his	O
support	O
on	O
the	O
early	O
stage	O
of	O
this	O
project	O
.	O
We	O
also	O
would	O
like	O
to	O
thank	O
Zihang	O
Dai	O
,	O
Graham	O
Neubig	O
,	O
Orhan	O
Firat	O
,	O
Yuan	O
Cao	O
,	O
Jiateng	O
Xie	O
,	O
Xinyi	O
Wang	O
,	O
Ruochen	O
Xu	O
and	O
Yiheng	O
Zhou	O
for	O
insightful	O
discussions	O
.	O
Lastly	O
,	O
we	O
thank	O
anonymous	O
reviewers	O
for	O
their	O
valueable	O
feedbacks	O
.	O

Due	O
to	O
the	O
varied	O
nature	O
of	O
the	O
input	O
presented	O
we	O
perform	O
various	O
data	O
cleaning	O
operations	O
.	O
We	O
start	O
by	O
expansion	O
of	O
common	O
contractions	O
(	O
e.g.	O
"	O
is	O
n't	O
"	O
)	O
and	O
informal	O
contractions	O
(	O
e.g.	O
"	O
howz	O
"	O
,	O
"	O
could	O
ve	O
"	O
)	O
.	O
We	O
then	O
perform	O
a	O
spell	O
check	O
and	O
hyphen	O
removal	O
,	O
which	O
are	O
conditional	O
,	O
in	O
the	O
sense	O
that	O
a	O
word	O
is	O
not	O
modified	O
unless	O
the	O
modified	O
form	O
appears	O
in	O
the	O
other	O
sentence	O
.	O
All	O
remaining	O
hyphens	O
are	O
replaced	O
by	O
spaces	O
,	O
a	O
method	O
different	O
from	O
those	O
that	O
previously	O
handled	O
hyphens	O
(	O
Han	O
et	O
al	O
,	O
2013	O
)	O
.	O
We	O
also	O
perform	O
case	O
correction	O
,	O
as	O
has	O
been	O
done	O
previously	O
(	O
Hänig	O
et	O
al	O
,	O
2015	O
)	O
,	O
since	O
we	O
observe	O
several	O
instances	O
wherein	O
sentence	O
capitalisation	O
is	O
not	O
suitable	O
for	O
parsing	O
(	O
e.g.	O
headlines	O
and	O
forums	O
)	O
.	O

We	O
use	O
two	O
measures	O
,	O
which	O
are	O
boosted	O
based	O
on	O
different	O
parameters	O
described	O
in	O
Section	O
4	O
.	O

The	O
first	O
measure	O
makes	O
use	O
of	O
the	O
aligner	O
developed	O
by	O
Sultan	O
et	O
al	O
(	O
2014a	O
)	O
,	O
which	O
was	O
used	O
to	O
achieve	O
State	O
of	O
the	O
Art	O
results	O
in	O
(	O
Sultan	O
et	O
al	O
,	O
2014bSultan	O
et	O
al	O
,	O
2015	O
)	O
.	O
Our	O
use	O
of	O
the	O
aligner	O
disregards	O
sequences	O
thus	O
making	O
use	O
of	O
the	O
aligner	O
more	O
as	O
a	O
synonym	O
finder	O
,	O
with	O
the	O
additional	O
power	O
of	O
the	O
Paraphrase	O
Database	O
(	O
PPDB	O
)	O
(	O
Ganitkevitch	O
et	O
al	O
,	O
2013	O
)	O
.	O

In	O
this	O
section	O
,	O
we	O
detail	O
the	O
variations	O
used	O
to	O
generate	O
different	O
similarity	O
measures	O
.	O
These	O
variations	O
are	O
not	O
used	O
simultaneously	O
,	O
but	O
are	O
instead	O
combined	O
as	O
described	O
in	O
Algorithm	O
1	O
(	O
Section	O
5	O
)	O
,	O
which	O
iterates	O
through	O
all	O
possible	O
variations	O
to	O
generate	O
a	O
different	O
similarity	O
score	O
associated	O
with	O
each	O
combination	O
.	O

Consider	O
the	O
following	O
sentence	O
pairs	O
with	O
relations	O
assigned	O
by	O
human	O
annotators	O
:	O
"	O
A	O
boy	O
is	O
playing	O
a	O
guitar	O
.	O
"	O
,	O
"	O
A	O
man	O
is	O
playing	O
a	O
guitar	O
.	O
"	O
,	O
rel	O
:	O
3.2	O
;	O
and	O
"	O
A	O
man	O
is	O
cutting	O
up	O
a	O
potato	O
.	O
"	O
,	O
"	O
A	O
man	O
is	O
cutting	O
up	O
carrots	O
.	O
"	O
,	O
rel	O
:	O
2.4	O
.	O
Although	O
both	O
pairs	O
of	O
sentences	O
differ	O
by	O
exactly	O
one	O
noun	O
,	O
the	O
first	O
pair	O
was	O
considered	O
to	O
be	O
more	O
closely	O
associated	O
than	O
the	O
second	O
.	O
We	O
associate	O
this	O
to	O
what	O
we	O
call	O
the	O
"	O
Surprise	O
"	O
and	O
assign	O
a	O
value	O
to	O
this	O
,	O
which	O
we	O
call	O
the	O
"	O
Surprise	O
Factor	O
"	O
.	O
Surprise	O
is	O
based	O
on	O
the	O
work	O
by	O
Dunning	O
(	O
1993	O
)	O
,	O
who	O
observed	O
that	O
the	O
assumption	O
of	O
normality	O
of	O
data	O
is	O
invalid	O
as	O
"	O
simple	O
word	O
counts	O
made	O
on	O
a	O
moderate	O
-	O
sized	O
corpus	O
show	O
that	O
words	O
that	O
have	O
a	O
frequency	O
of	O
less	O
than	O
one	O
in	O
50	O
,	O
000	O
words	O
make	O
up	O
about	O
20	O
-	O
30	O
%	O
of	O
typical	O
English	O
language	O
newswire	O
reports	O
.	O
This	O
'	O
rare	O
'	O
quarter	O
of	O
English	O
includes	O
many	O
of	O
the	O
content	O
-	O
bearing	O
words	O
.	O
.	O
.	O
"	O
We	O
define	O
the	O
Surprise	O
Factor	O
of	O
a	O
noun	O
or	O
phrase	O
to	O
be	O
proportional	O
to	O
the	O
number	O
of	O
Web	O
Search	O
Hits	O
for	O
that	O
phrase	O
or	O
term	O
,	O
while	O
inversely	O
proportional	O
to	O
the	O
Search	O
Hits	O
in	O
the	O
case	O
of	O
proper	O
nouns	O
.	O
Intuitively	O
this	O
makes	O
sense	O
,	O
as	O
words	O
that	O
are	O
more	O
common	O
will	O
generate	O
less	O
Surprise	O
,	O
carry	O
less	O
information	O
,	O
and	O
will	O
also	O
be	O
more	O
widely	O
used	O
on	O
the	O
Internet	O
.	O
We	O
incorporate	O
this	O
idea	O
of	O
Surprise	O
by	O
adding	O
the	O
option	O
of	O
additionally	O
weighting	O
nouns	O
by	O
the	O
total	O
number	O
of	O
Web	O
Search	O
Hits	O
or	O
Results	O
4	O
.	O
We	O
define	O
,	O
H	O
i	O
to	O
be	O
the	O
the	O
number	O
of	O
Web	O
Search	O
Hits	O
for	O
the	O
noun	O
i	O
,	O
HT	O
the	O
total	O
number	O
of	O
hits	O
for	O
all	O
nouns	O
HT	O
=	O
N	O
i=0	O
H	O
i	O
,	O
N	O
i	O
the	O
fraction	O
of	O
the	O
Search	O
Hits	O
that	O
noun	O
i	O
captures	O
N	O
i	O
=	O
Hn	O
HT	O
,	O
and	O
N	O
T	O
the	O
normalised	O
total	O
of	O
all	O
nouns	O
(	O
C	O
)	O
in	O
a	O
given	O
sentence	O
N	O
T	O
=	O
C	O
i=0	O
N	O
i	O
.	O
We	O
define	O
the	O
Surprise	O
of	O
word	O
i	O
in	O
terms	O
of	O
the	O
above	O
in	O
Equation	O
1	O
.	O
S	O
i	O
=	O
N	O
i	O
N	O
T	O
(	O
1	O
)	O

T	O
t=0	O
score	O
t	O
×	O
w	O
t	O
×	O
2	O
T	O
t=0	O
count	O
t	O
×	O
w	O
t	O
)	O
(	O
2	O
)	O
4	O
We	O

As	O
described	O
above	O
,	O
we	O
use	O
variations	O
to	O
generate	O
thousands	O
of	O
Similarity	O
Scores	O
,	O
each	O
of	O
which	O
we	O
call	O
a	O
"	O
Method	O
"	O
.	O
Each	O
Method	O
's	O
performance	O
varies	O
depending	O
on	O
the	O
input	O
.	O
In	O
this	O
section	O
,	O
we	O
detail	O
the	O
process	O
for	O
combining	O
these	O
Methods	O
,	O
which	O
is	O
performed	O
using	O
either	O
Support	O
Vector	O
Regression	O
(	O
SVR	O
)	O
or	O
Kernel	O
Ridge	O
Regression	O
(	O
KRR	O
)	O
.	O

In	O
addition	O
to	O
using	O
scores	O
from	O
the	O
chosen	O
Methods	O
,	O
we	O
add	O
the	O
following	O
features	O
to	O
some	O
of	O
our	O
submitted	O
runs	O
:	O
a	O
)	O
a	O
binary	O
value	O
to	O
represent	O
whether	O
each	O
of	O
the	O
sentences	O
were	O
case	O
corrected	O
,	O
b	O
)	O
the	O
length	O
of	O
each	O
of	O
the	O
sentences	O
,	O
c	O
)	O
the	O
number	O
of	O
contin	O
-	O
uous	O
aligned	O
or	O
unaligned	O
sequences	O
,	O
d	O
)	O
the	O
maximum	O
and	O
minimum	O
lengths	O
of	O
continuous	O
aligned	O
or	O
unaligned	O
sequences	O
,	O
and	O
e	O
)	O
a	O
binary	O
value	O
to	O
represent	O
alignments	O
that	O
are	O
non	O
-	O
sequential	O
.	O
It	O
should	O
be	O
noted	O
that	O
the	O
specific	O
Methods	O
we	O
choose	O
for	O
use	O
in	O
the	O
SVR	O
or	O
KRR	O
will	O
depend	O
on	O
the	O
training	O
data	O
picked	O
.	O
We	O
found	O
,	O
by	O
testing	O
our	O
system	O
using	O
several	O
different	O
combinations	O
of	O
training	O
data	O
,	O
that	O
the	O
best	O
results	O
were	O
achieved	O
when	O
our	O
system	O
was	O
trained	O
on	O
the	O
headlines	O
data	O
from	O
the	O
years	O
2015	O
,	O
2014	O
and	O
2013	O
.	O
The	O
method	O
selection	O
criterion	O
,	O
regression	O
model	O
and	O
parameters	O
used	O
for	O
each	O
of	O
the	O
runs	O
submitted	O
are	O
detailed	O
in	O
Table	O
1	O
.	O
Although	O
some	O
of	O
the	O
settings	O
are	O
very	O
similar	O
(	O
e.g.	O
run2	O
)	O
,	O
we	O
noticed	O
that	O
these	O
minor	O
changes	O
translated	O
to	O
significant	O
differences	O
in	O
performance	O
.	O

This	O
work	O
was	O
supported	O
,	O
in	O
part	O
,	O
by	O
the	O
EPSRC	O
,	O
U.K.	O
,	O
under	O
grant	O
number	O
1576491	O
,	O
and	O
is	O
also	O
partially	O
funded	O
by	O
the	O
ENDEAVOUR	O
Scholarships	O
Scheme	O
,	O
which	O
may	O
be	O
part	O
-	O
financed	O
by	O
the	O
European	O
Union	O
-	O
European	O
Social	O
Fund	O
.	O

The	O
only	O
system	O
that	O
our	O
team	O
submitted	O
for	O
the	O
SMG	O
-	O
CH	O
subtask	O
is	O
an	O
ensemble	O
model	O
based	O
on	O
the	O
XGBoost	O
meta	O
-	O
learner	O
,	O
as	O
illustrated	O
in	O
Figure	O
1	O
.	O
In	O
this	O
section	O
,	O
we	O
describe	O
the	O
three	O
machine	O
learning	O
techniques	O
that	O
provide	O
their	O
predictions	O
as	O
input	O
for	O
the	O
meta	O
-	O
learner	O
,	O
as	O
well	O
as	O
the	O
gradient	O
boosting	O
method	O
that	O
combines	O
the	O
independent	O
models	O
.	O

The	O
SMG	O
-	O
CH	O
subtask	O
(	O
Hovy	O
and	O
Purschke	O
,	O
2018	O
)	O
offers	O
,	O
as	O
support	O
,	O
a	O
training	O
set	O
of	O
25	O
,	O
261	O
Jodel	O
posts	O
,	O
provided	O
in	O
plain	O
text	O
format	O
.	O
Each	O
textual	O
input	O
is	O
associated	O
with	O
a	O
pair	O
of	O
coordinates	O
,	O
i.e.	O
latitude	O
and	O
longitude	O
,	O
representing	O
the	O
position	O
on	O
Earth	O
from	O
where	O
the	O
text	O
was	O
posted	O
.	O
The	O
development	O
set	O
is	O
provided	O
in	O
an	O
identical	O
format	O
and	O
it	O
is	O
composed	O
of	O
2	O
,	O
416	O
samples	O
that	O
we	O
use	O
to	O
perform	O
hyperparameter	O
tuning	O
and	O
validate	O
the	O
results	O
of	O
our	O
models	O
.	O
The	O
test	O
set	O
has	O
2	O
,	O
438	O
samples	O
without	O
the	O
corresponding	O
coordinates	O
,	O
in	O
order	O
to	O
avoid	O
cheating	O
or	O
overfitting	O
.	O
Additionally	O
,	O
the	O
proposed	O
evaluation	O
metric	O
is	O
the	O
median	O
distance	O
between	O
the	O
predicted	O
and	O
the	O
reference	O
coordinates	O
.	O
A	O
baseline	O
median	O
distance	O
of	O
53.13	O
km	O
is	O
also	O
included	O
in	O
the	O
specifications	O
.	O

For	O
our	O
final	O
submission	O
,	O
we	O
trained	O
all	O
the	O
individual	O
models	O
on	O
both	O
the	O
provided	O
training	O
and	O
development	O
data	O
sets	O
.	O
Then	O
,	O
we	O
also	O
retrained	O
the	O
submitted	O
ensemble	O
model	O
,	O
in	O
a	O
hope	O
that	O
this	O
will	O
give	O
an	O
even	O
smaller	O
median	O
distance	O
on	O
the	O
test	O
set	O
,	O
compared	O
to	O
what	O
we	O
have	O
obtained	O
in	O
the	O
preliminary	O
validation	O
phase	O
.	O
Table	O
2	O
shows	O
an	O
improvement	O
in	O
terms	O
of	O
the	O
median	O
distance	O
on	O
the	O
test	O
set	O
compared	O
to	O
the	O
one	O
obtain	O
on	O
the	O
development	O
data	O
.	O
We	O
can	O
not	O
be	O
sure	O
that	O
this	O
improvement	O
is	O
solely	O
due	O
to	O
the	O
retraining	O
that	O
involves	O
the	O
development	O
set	O
.	O
However	O
,	O
we	O
conjecture	O
that	O
this	O
endeavour	O
played	O
its	O
part	O
in	O
the	O
slightly	O
better	O
results	O
.	O
We	O
outperform	O
the	O
baseline	O
model	O
by	O
29.53	O
km	O
in	O
terms	O
of	O
the	O
median	O
distance	O
and	O
by	O
21.75	O
km	O
in	O
terms	O
of	O
the	O
mean	O
distance	O
,	O
obtaining	O
the	O
third	O
place	O
in	O
the	O
competition	O
.	O
The	O
constrained	O
submission	O
proposed	O
by	O
the	O
organizers	O
of	O
the	O
SMG	O
-	O
CH	O
shared	O
task	O
surpasses	O
our	O
model	O
by	O
2.9	O
km	O
in	O
terms	O
of	O
the	O
median	O
distance	O
and	O
by	O
0.13	O
km	O
in	O
terms	O
of	O
the	O
mean	O
distance	O
.	O
The	O
unconstrained	O
system	O
on	O
the	O
first	O
place	O
,	O
which	O
was	O
also	O
proposed	O
by	O
the	O
organizers	O
of	O
the	O
SMG	O
-	O
CH	O
shared	O
task	O
,	O
distances	O
itself	O
by	O
larger	O
margins	O
,	O
with	O
a	O
difference	O
of	O
6.05	O
km	O
for	O
the	O
median	O
distance	O
and	O
a	O
difference	O
of	O
3.91	O
km	O
for	O
the	O
mean	O
distance	O
.	O

The	O
authors	O
thank	O
reviewers	O
for	O
their	O
useful	O
remarks	O
.	O
This	O
work	O
was	O
supported	O
by	O
a	O
grant	O
of	O
the	O
Romanian	O
Ministry	O
of	O
Education	O
and	O
Research	O
,	O
CNCS	O
-	O
UEFISCDI	O
,	O
project	O
number	O
PN	O
-	O
III	O
-	O
P1	O
-	O
1.1	O
-	O
TE	O
-	O
2019	O
-	O
0235	O
,	O
within	O
PNCDI	O
III	O
.	O
This	O
article	O
has	O
also	O
benefited	O
from	O
the	O
support	O
of	O
the	O
Romanian	O
Young	O
Academy	O
,	O
which	O
is	O
funded	O
by	O
Stiftung	O
Mercator	O
and	O
the	O
Alexander	O
von	O
Humboldt	O
Foundation	O
for	O
the	O
period	O
2020	O
-	O
2022	O
.	O

We	O
presented	O
a	O
comparison	O
between	O
12	O
transformers	O
-	O
based	O
models	O
,	O
with	O
the	O
goal	O
of	O
"	O
prescribing	O
"	O
the	O
best	O
option	O
to	O
the	O
researchers	O
working	O
in	O
the	O
field	O
.	O
We	O
also	O
wanted	O
to	O
test	O
whether	O
the	O
span	O
-	O
based	O
objective	O
of	O
SpanBERT	O
and	O
in	O
-	O
domain	O
language	O
pretraining	O
were	O
useful	O
for	O
the	O
task	O
.	O
We	O
can	O
positively	O
answer	O
to	O
the	O
first	O
question	O
,	O
since	O
SpanBERT	O
turned	O
out	O
to	O
be	O
the	O
best	O
performing	O
model	O
on	O
both	O
datasets	O
.	O
As	O
for	O
the	O
in	O
-	O
domain	O
models	O
,	O
PubMedBERT	O
came	O
as	O
a	O
close	O
second	O
after	O
SpanBERT	O
,	O
suggesting	O
that	O
pretraining	O
from	O
scratch	O
with	O
no	O
general	O
domain	O
data	O
is	O
the	O
best	O
strategy	O
,	O
at	O
least	O
for	O
this	O
task	O
.	O
We	O
have	O
been	O
the	O
first	O
,	O
to	O
our	O
knowledge	O
,	O
to	O
test	O
these	O
two	O
models	O
in	O
a	O
systematic	O
comparison	O
on	O
ADE	O
detection	O
,	O
and	O
they	O
delivered	O
promising	O
results	O
for	O
future	O
research	O
.	O
For	O
the	O
next	O
step	O
,	O
a	O
possible	O
direction	O
would	O
be	O
to	O
combine	O
the	O
strengths	O
of	O
their	O
respective	O
representations	O
:	O
the	O
accurate	O
modeling	O
of	O
text	O
spans	O
on	O
the	O
one	O
side	O
,	O
and	O
deep	O
biomedical	O
knowledge	O
on	O
the	O
other	O
one	O
.	O

Some	O
statistics	O
for	O
the	O
texts	O
of	O
the	O
two	O
datasets	O
have	O
been	O
extracted	O
with	O
the	O
TEXTSTAT	O
Python	O
package	O
and	O
reported	O
reported	O
in	O
Table	O
A	O
:	O
we	O
extracted	O
the	O
counts	O
of	O
syllables	O
,	O
lexicon	O
(	O
how	O
many	O
different	O
word	O
types	O
are	O
being	O
used	O
)	O
,	O
sentences	O
and	O
characters	O
.	O
Difficult	O
words	O
refers	O
to	O
the	O
number	O
of	O
polysyllabic	O
words	O
with	O
Syllable	O
Count	O
>	O
2	O
that	O
are	O
not	O
included	O
in	O
the	O
list	O
of	O
words	O
of	O
common	O
usage	O
in	O
English	O
.	O

We	O
use	O
the	O
data	O
sets	O
provided	O
by	O
the	O
SemEval	O
-	O
2016	O
shared	O
task	O
6	O
(	O
Mohammad	O
et	O
al	O
,	O
2016	O
)	O
.	O

We	O
mined	O
additional	O
tweets	O
for	O
each	O
of	O
the	O
five	O
targets	O
in	O
Nov.	O
2015	O
by	O
searching	O
for	O
hashtags	O
relevant	O
to	O
the	O
targets	O
.	O
These	O
tweets	O
are	O
not	O
included	O
in	O
the	O
final	O
systems	O
since	O
they	O
increased	O
the	O
class	O
imbalance	O
.	O
We	O
will	O
investigate	O
better	O
options	O
for	O
including	O
the	O
data	O
in	O
the	O
future	O
.	O
Hashtags	O
for	O
Abortion	O
include	O
#	O
abortion	O
,	O
#	O
abortionrights	O
,	O
and	O
#	O
prolife	O
;	O
Atheism	O
includes	O
#	O
atheism	O
,	O
#	O
atheist	O
,	O
and	O
#	O
theist	O
;	O
Climate	O
includes	O
#	O
actionclimate	O
and	O
#	O
climatechange	O
;	O
Feminist	O
includes	O
#	O
feminism	O
,	O
#	O
feminist	O
,	O
#	O
heforshe	O
,	O
and	O
#	O
womensrights	O
;	O
and	O
Hillary	O
includes	O
#	O
HillaryClinton	O
.	O
Tweets	O
were	O
then	O
annotated	O
for	O
stance	O
,	O
following	O
the	O
guidelines	O
used	O
for	O
the	O
annotation	O
of	O
the	O
official	O
shared	O
task	O
data	O
3	O
.	O
Two	O
annotators	O
participated	O
in	O
the	O
annotation	O
process	O
.	O
The	O
number	O
of	O
additional	O
tweets	O
ranged	O
between	O
260	O
and	O
2	O
,	O
400	O
per	O
target	O
.	O

Since	O
the	O
ensemble	O
classifier	O
was	O
not	O
completed	O
in	O
time	O
for	O
submission	O
,	O
we	O
had	O
to	O
decide	O
which	O
individual	O
classifier	O
to	O
submit	O
.	O
The	O
random	O
forest	O
model	O
is	O
selected	O
based	O
on	O
a	O
five	O
-	O
fold	O
cross	O
validation	O
on	O
the	O
training	O
set	O
.	O
This	O
system	O
reaches	O
a	O
score	O
of	O
63.60	O
(	O
macro	O
-	O
averaged	O
F	O
)	O
,	O
as	O
shown	O
in	O
table	O
3	O
,	O
the	O
sixth	O
best	O
result	O
out	O
of	O
19	O
participating	O
systems	O
.	O
This	O
result	O
is	O
approximately	O
4	O
percent	O
points	O
lower	O
than	O
that	O
of	O
the	O
highest	O
performing	O
system	O
.	O

This	O
work	O
is	O
based	O
on	O
research	O
supported	O
by	O
the	O
U.S.	O
Office	O
of	O
Naval	O
Research	O
(	O
ONR	O
)	O
via	O
grant	O
#	O
N00014	O
-	O
10	O
-	O
1	O
-	O
0140	O
.	O

The	O
tokens	O
decoded	O
as	O
B	O
-	O
Begin	O
or	O
I	O
-	O
Inside	O
were	O
marked	O
as	O
toxic	O
.	O
The	O
character	O
spans	O
corresponding	O
to	O
these	O
toxic	O
tokens	O
were	O
added	O
to	O
the	O
predicted	O
spans	O
.	O
Two	O
consecutive	O
spans	O
were	O
merged	O
if	O
separated	O
by	O
at	O
most	O
five	O
characters	O
,	O
provided	O
all	O
of	O
them	O
are	O
non	O
-	O
alphabetic	O
.	O

Block	O
Pruning	O
For	O
Faster	O
Transformers	O

We	O
expect	O
the	O
method	O
presented	O
here	O
to	O
contribute	O
to	O
the	O
reduction	O
of	O
the	O
compute	O
resources	O
and	O
energy	O
needed	O
to	O
perform	O
natural	O
language	O
tasks	O
,	O
while	O
preserving	O
the	O
original	O
model	O
performance	O
.	O
It	O
will	O
contribute	O
additionally	O
to	O
alleviating	O
privacy	O
concerns	O
:	O
smaller	O
models	O
running	O
on	O
user	O
devices	O
instead	O
of	O
server	O
-	O
side	O
allow	O
more	O
information	O
to	O
stay	O
private	O
.	O
This	O
is	O
especially	O
relevant	O
when	O
considering	O
the	O
large	O
anticipated	O
demand	O
for	O
such	O
NLP	O
applications	O
in	O
the	O
near	O
future	O
.	O

The	O
complete	O
code	O
to	O
run	O
the	O
experiments	O
,	O
analyze	O
the	O
results	O
and	O
finally	O
create	O
the	O
figures	O
and	O
tables	O
in	O
this	O
paper	O
is	O
available	O
on	O
the	O
Hugging	O
Face	O
nn_pruning	O
repository	O
,	O
at	O
https://github.com/huggingface/nn_pruning	O
.	O

The	O
authors	O
would	O
like	O
to	O
thank	O
the	O
anonymous	O
reviewers	O
,	O
the	O
Hugging	O
Face	O
team	O
for	O
the	O
support	O
,	O
Nvidia	O
for	O
providing	O
us	O
some	O
hardware	O
for	O
evaluation	O
,	O
and	O
finally	O
the	O
open	O
-	O
source	O
community	O
for	O
the	O
numerous	O
tools	O
which	O
made	O
this	O
research	O
possible	O
.	O

Visual	O
Story	O
Post	O
-	O
Editing	O

Orthographic	O
très	O
près	O
7.5	O
très	O
ors	O
5	O
Phononetic	O
très	O
frais	O
6.67	O
très	O
traînent	O
6.67	O

The	O
training	O
set	O
for	O
the	O
CNN	O
consists	O
of	O
488	O
hours	O
of	O
French	O
Broadcast	O
News	O
with	O
manual	O
transcriptions	O
.	O
This	O
dataset	O
is	O
composed	O
of	O
data	O
coming	O
from	O
the	O
ESTER1	O
(	O
Galliano	O
et	O
al	O
,	O
2005	O
)	O
,	O
ES	O
-	O
TER2	O
(	O
Galliano	O
et	O
al	O
,	O
2009	O
)	O
and	O
EPAC	O
(	O
Estève	O
et	O
al	O
,	O
2010	O
)	O
corpora	O
.	O
It	O
contains	O
52k	O
unique	O
words	O
that	O
have	O
been	O
seen	O
at	O
least	O
twice	O
each	O
in	O
the	O
corpus	O
.	O
All	O
of	O
them	O
corresponds	O
to	O
a	O
total	O
of	O
5.75	O
millions	O
occurrences	O
.	O
In	O
French	O
language	O
,	O
many	O
words	O
have	O
the	O
same	O
pronunciation	O
without	O
sharing	O
the	O
same	O
spelling	O
,	O
and	O
they	O
can	O
have	O
different	O
meanings	O
;	O
e.g.	O
the	O
sound	O
[	O
so	O
]	O
corresponds	O
to	O
four	O
homophones	O
:	O
sot	O
(	O
fool	O
)	O
,	O
saut	O
(	O
jump	O
)	O
,	O
sceau	O
(	O
seal	O
)	O
and	O
seau	O
(	O
bucket	O
)	O
,	O
and	O
twice	O
more	O
by	O
taking	O
into	O
account	O
their	O
plural	O
forms	O
that	O
have	O
the	O
same	O
pronunciation	O
:	O
sots	O
,	O
sauts	O
,	O
sceaux	O
,	O
and	O
seaux	O
.	O
When	O
a	O
CNN	O
is	O
trained	O
to	O
predict	O
a	O
word	O
given	O
an	O
acoustic	O
sequence	O
,	O
these	O
frequent	O
homophones	O
can	O
introduce	O
a	O
bias	O
to	O
evaluate	O
the	O
recognition	O
error	O
.	O
To	O
avoid	O
this	O
,	O
we	O
merged	O
all	O
the	O
homophones	O
existing	O
among	O
the	O
52k	O
unique	O
words	O
of	O
the	O
training	O
corpus	O
.	O
As	O
a	O
result	O
,	O
we	O
obtained	O
a	O
new	O
reduced	O
dictionary	O
containing	O
45k	O
words	O
and	O
classes	O
of	O
homophones	O
.	O
Acoustic	O
features	O
provided	O
to	O
the	O
CNN	O
are	O
logfilterbanks	O
,	O
computed	O
every	O
10ms	O
over	O
a	O
25ms	O
window	O
yielding	O
a	O
23	O
-	O
dimension	O
vector	O
for	O
each	O
frame	O
.	O
A	O
forced	O
alignment	O
between	O
manual	O
transcriptions	O
and	O
speech	O
signal	O
was	O
performed	O
on	O
the	O
training	O
set	O
in	O
order	O
to	O
detect	O
word	O
boundaries	O
.	O
The	O
statistics	O
computed	O
from	O
this	O
alignment	O
reveal	O
that	O
99	O
%	O
of	O
words	O
are	O
shorter	O
than	O
1	O
second	O
.	O
Hence	O
we	O
decided	O
to	O
represent	O
each	O
word	O
by	O
100	O
frames	O
,	O
thus	O
,	O
by	O
a	O
vector	O
of	O
2300	O
dimensions	O
.	O
When	O
words	O
are	O
shorter	O
they	O
are	O
padded	O
with	O
zero	O
equally	O
on	O
both	O
ends	O
,	O
while	O
longer	O
words	O
are	O
cut	O
equally	O
on	O
both	O
ends	O
.	O
The	O
CNN	O
and	O
DNN	O
deep	O
architectures	O
are	O
trained	O
on	O
90	O
%	O
of	O
the	O
training	O
set	O
and	O
the	O
remaining	O
10	O
%	O
are	O
used	O
for	O
validation	O
.	O

This	O
work	O
was	O
partially	O
funded	O
by	O
the	O
European	O
Commission	O
through	O
the	O
EUMSSI	O
project	O
,	O
under	O
the	O
contract	O
number	O
611057	O
,	O
in	O
the	O
framework	O
of	O
the	O
FP7	O
-	O
ICT	O
-	O
2013	O
-	O
10	O
call	O
,	O
by	O
the	O
French	O
National	O
Research	O
Agency	O
(	O
ANR	O
)	O
through	O
the	O
VERA	O
project	O
,	O
under	O
the	O
contract	O
number	O
ANR	O
-	O
12	O
-	O
BS02	O
-	O
006	O
-	O
01	O
,	O
and	O
by	O
the	O
Région	O
Pays	O
de	O
la	O
Loire	O
.	O

As	O
discussed	O
in	O
(	O
Zhao	O
et	O
al	O
,	O
2019	O
)	O
,	O
the	O
routing	O
procedure	O
is	O
computational	O
expensive	O
for	O
a	O
large	O
number	O
of	O
capsules	O
.	O
Compressing	O
capsules	O
into	O
a	O
smaller	O
amount	O
can	O
not	O
only	O
relieve	O
the	O
computational	O
complexity	O
,	O
but	O
also	O
merge	O
similar	O
capsules	O
and	O
remove	O
outliers	O
.	O
Therefore	O
,	O
hyperbolic	O
compression	O
layer	O
is	O
introduced	O
.	O
Each	O
compressed	O
local	O
hyperbolic	O
capsule	O
is	O
calculated	O
as	O
a	O
weighted	O
Möbius	O
summation	O
over	O
all	O
the	O
local	O
hyperbolic	O
capsules	O
.	O
For	O
instance	O
,	O
u	O
l	O
=	O
M	O
u	O
k	O
{	O
u	O
1	O
,	O
...	O
,	O
u	O
L	O
}	O
r	O
k	O
⊗	O
u	O
k	O
B	O
d	O
,	O
(	O
9	O
)	O
where	O
r	O
k	O
is	O
a	O
learnable	O
weight	O
parameter	O
.	O
And	O
likewise	O
for	O
compressing	O
global	O
hyperbolic	O
capsules	O
.	O
Let	O
set	O
{	O
u	O
1	O
,	O
.	O
.	O
.	O
,	O
u	O
P	O
}	O
denote	O
the	O
compressed	O
local	O
and	O
global	O
hyperbolic	O
capsules	O
together	O
,	O
which	O
are	O
then	O
aggregated	O
in	O
a	O
label	O
-	O
aware	O
manner	O
via	O
HDR	O
.	O

The	O
purpose	O
of	O
Hyperbolic	O
Dynamic	O
Routing	O
(	O
HDR	O
)	O
is	O
to	O
iteratively	O
aggregate	O
local	O
and	O
global	O
hyperbolic	O
capsules	O
into	O
label	O
-	O
aware	O
hyperbolic	O
capsules	O
,	O
whose	O
activations	O
stand	O
for	O
probabilities	O
of	O
the	O
labels	O
.	O

The	O
proposed	O
HYPERCAPS	O
is	O
evaluated	O
on	O
four	O
benchmark	O
datasets	O
with	O
various	O
label	O
number	O
from	O
54	O
to	O
4271	O
.	O
We	O
compare	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
in	O
terms	O
of	O
widely	O
used	O
metrics	O
.	O
Performance	O
on	O
tail	O
labels	O
is	O
also	O
compared	O
to	O
demonstrate	O
the	O
superiority	O
of	O
HYPERCAPS	O
for	O
MLC	O
.	O
An	O
ablation	O
test	O
is	O
also	O
carried	O
out	O
to	O
analyse	O
the	O
contribution	O
of	O
each	O
component	O
of	O
HYPERCAPS	O
.	O

This	O
work	O
was	O
supported	O
in	O
part	O
by	O
the	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
under	O
Grant	O
61822601	O
,	O
61773050	O
,	O
and	O
61632004	O
;	O
the	O
Beijing	O
Natural	O
Science	O
Foundation	O
under	O
Grant	O
Z180006	O
;	O
National	O
Key	O
Research	O
and	O
Development	O
Program	O
(	O
2017YFC1703506	O
)	O
;	O
the	O
Fundamental	O
Research	O
Funds	O
for	O
the	O
Central	O
Universities	O
(	O
2019JBZ110	O
)	O
.	O
We	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
valuable	O
feedback	O
.	O

Table	O
1	O
)	O
,	O
i.e.	O
labels	O
have	O
less	O
than	O
average	O
number	O
of	O
training	O
instances	O
are	O
tail	O
labels	O
.	O
We	O
observe	O
that	O
this	O
division	O
generally	O
follows	O
the	O
Pareto	O
Principle	O
,	O
as	O
nearly	O
80	O
%	O
of	O
labels	O
are	O
divided	O
into	O
the	O
tail	O
label	O
set	O
.	O

Organizing	O
and	O
Program	O
Committees	O
Organizing	O
Committee	O

A	O
Teacher	O
-	O
Student	O
Framework	O
for	O
Maintainable	O
Dialog	O
Manager	O

Fig	O
.	O
3	O
shows	O
two	O
kinds	O
of	O
strategies	O
to	O
extend	O
the	O
original	O
system	O
.	O
The	O
first	O
strategy	O
requires	O
a	O
new	O
interaction	O
environment	O
.	O
However	O
,	O
building	O
a	O
user	O
simulator	O
or	O
hiring	O
real	O
users	O
once	O
the	O
system	O
needs	O
to	O
be	O
extended	O
is	O
costly	O
and	O
impractical	O
in	O
real	O
-	O
world	O
applications	O
.	O
By	O
contrast	O
,	O
our	O
method	O
enhances	O
the	O
reuse	O
of	O
existing	O
resources	O
.	O
The	O
basic	O
idea	O
is	O
to	O
use	O
the	O
existing	O
user	O
logs	O
,	O
original	O
dialog	O
policy	O
model	O
and	O
logic	O
rules	O
(	O
"	O
teacher	O
"	O
)	O
to	O
guide	O
the	O
learning	O
process	O
of	O
a	O
new	O
dialog	O
manager	O
(	O
"	O
student	O
"	O
)	O
.	O
Without	O
an	O
expensive	O
interaction	O
environment	O
,	O
the	O
developers	O
can	O
maintain	O
RL	O
-	O
based	O
dialog	O
systems	O
as	O
efficiently	O
and	O
straightforwardly	O
as	O
in	O
rule	O
-	O
based	O
systems	O
.	O

To	O
evaluate	O
our	O
method	O
,	O
we	O
conduct	O
experiments	O
on	O
a	O
dialog	O
system	O
extension	O
task	O
of	O
restaurant	O
domain	O
.	O

The	O
dialog	O
system	O
provides	O
restaurant	O
information	O
in	O
Beijing	O
.	O
The	O
database	O
we	O
use	O
includes	O
2988	O
restaurants	O
.	O
This	O
domain	O
consists	O
of	O
8	O
slots	O
(	O
name	O
,	O
area	O
,	O
price	O
range	O
,	O
cuisine	O
,	O
rating	O
,	O
number	O
of	O
comments	O
,	O
address	O
and	O
phone	O
number	O
)	O
in	O
which	O
the	O
first	O
four	O
slots	O
(	O
inform	O
slots	O
)	O
can	O
be	O
used	O
for	O
searching	O
the	O
desirable	O
restaurant	O
and	O
all	O
of	O
these	O
slots	O
(	O
request	O
slots	O
)	O
can	O
be	O
asked	O
by	O
users	O
.	O
In	O
each	O
dialog	O
,	O
the	O
user	O
has	O
a	O
goal	O
containing	O
a	O
set	O
of	O
slots	O
,	O
indicating	O
the	O
constraints	O
and	O
requests	O
from	O
users	O
.	O
For	O
example	O
,	O
an	O
inform	O
slot	O
,	O
such	O
as	O
"	O
inform	O
(	O
cuisine	O
=	O
Sichuan	O
cuisine	O
)	O
"	O
,	O
indicates	O
the	O
user	O
finding	O
a	O
Sichuan	O
restaurant	O
,	O
and	O
a	O
request	O
slot	O
,	O
such	O
as	O
"	O
request	O
(	O
area	O
)	O
"	O
,	O
indicates	O
the	O
user	O
asking	O
for	O
information	O
from	O
the	O
system	O
(	O
Li	O
et	O
al	O
,	O
2016	O
(	O
Li	O
et	O
al	O
,	O
,	O
2017bPeng	O
et	O
al	O
,	O
2017	O
)	O
.	O

A	O
main	O
advantage	O
of	O
our	O
approach	O
is	O
that	O
the	O
unconsidered	O
user	O
actions	O
can	O
be	O
handled	O
in	O
the	O
extended	O
system	O
.	O
In	O
addition	O
to	O
traditional	O
measurements	O
(	O
e.g.	O
,	O
success	O
rate	O
,	O
average	O
turns	O
and	O
average	O
reward	O
)	O
,	O
we	O
define	O
an	O
objective	O
measurement	O
called	O
"	O
Satis	O
.	O
"	O
(	O
user	O
satisfaction	O
)	O
to	O
verify	O
this	O
feature	O
in	O
the	O
simulated	O
evaluation	O
.	O
"	O
Satis	O
.	O
"	O
indicates	O
the	O
rate	O
at	O
which	O
the	O
system	O
takes	O
reasonable	O
actions	O
in	O
unsupported	O
dialog	O
situations	O
.	O
It	O
can	O
be	O
calculated	O
as	O
follows	O
:	O
Satis	O
.	O
=	O
d	O
D	O
|	O
d	O
|	O
t=1	O
L	O
l=1	O
1	O
{	O
ht	O
=	O
h	O
l	O
}	O
1	O
{	O
a	O
s	O
t	O
=	O
a	O
l	O
}	O
d	O
D	O
|	O
d	O
|	O
t=1	O
L	O
l=1	O
1	O
{	O
ht	O
=	O
h	O
l	O
}	O
(	O
8	O
)	O
where	O
h	O
t	O
and	O
a	O
s	O
t	O
are	O
the	O
dialog	O
history	O
and	O
system	O
action	O
in	O
the	O
t	O
-	O
th	O
turn	O
,	O
h	O
l	O
and	O
a	O
l	O
are	O
dialog	O
context	O
condition	O
and	O
corresponding	O
system	O
action	O
defined	O
in	O
the	O
l	O
-	O
th	O
rules	O
.	O
Intuitively	O
,	O
an	O
unreasonable	O
system	O
reply	O
will	O
frustrate	O
users	O
and	O
low	O
"	O
Satis	O
.	O
"	O
indicates	O
a	O
poor	O
user	O
experience	O
.	O

Training	O
RL	O
-	O
based	O
dialog	O
systems	O
requires	O
a	O
large	O
number	O
of	O
interactions	O
with	O
users	O
.	O
It	O
's	O
common	O
to	O
use	O
a	O
user	O
simulator	O
to	O
train	O
RL	O
-	O
based	O
dialog	O
systems	O
in	O
an	O
online	O
fashion	O
(	O
Pietquin	O
and	O
Dutoit	O
,	O
2006	O
;	O
Scheffler	O
and	O
Young	O
,	O
2002	O
;	O
Li	O
et	O
al	O
,	O
2016	O
)	O
.	O
As	O
a	O
consequence	O
,	O
we	O
construct	O
an	O
agenda	O
-	O
based	O
user	O
simulator	O
,	O
which	O
we	O
refer	O
to	O
as	O
Sim1	O
,	O
to	O
train	O
the	O
original	O
RL	O
-	O
based	O
system	O
.	O
The	O
user	O
action	O
set	O
of	O
Sim1	O
is	O
denoted	O
as	O
A	O
u	O
,	O
which	O
includes	O
such	O
intents	O
4	O
:	O
"	O
hello	O
"	O
,	O
"	O
bye	O
"	O
,	O
"	O
inform	O
"	O
,	O
"	O
deny	O
"	O
,	O
"	O
negate	O
"	O
,	O
"	O
affirm	O
"	O
,	O
"	O
request	O
"	O
,	O
"	O
reqalts	O
"	O
and	O
"	O
null	O
"	O
.	O
The	O
slots	O
of	O
Sim1	O
are	O
shown	O
in	O
section	O
6.1	O
.	O
In	O
each	O
turn	O
,	O
the	O
user	O
action	O
consists	O
of	O
a	O
intent	O
and	O
slots	O
and	O
we	O
append	O
the	O
value	O
of	O
slots	O
according	O
to	O
the	O
user	O
goal	O
.	O

In	O
any	O
case	O
,	O
the	O
developers	O
ca	O
n't	O
guarantee	O
all	O
user	O
actions	O
are	O
considered	O
.	O
Fortunately	O
,	O
our	O
method	O
makes	O
no	O
assumptions	O
about	O
the	O
new	O
user	O
actions	O
and	O
new	O
dialog	O
model	O
architecture	O
.	O
As	O
a	O
result	O
,	O
the	O
system	O
can	O
be	O
extended	O
over	O
multiple	O
iterations	O
.	O
To	O
evaluate	O
this	O
characteristic	O
,	O
we	O
deploy	O
the	O
extended	O
system	O
11	O
in	O
section	O
6.5	O
to	O
interact	O
with	O
real	O
human	O
users	O
.	O
Users	O
are	O
given	O
a	O
goal	O
sampled	O
from	O
our	O
corpus	O
for	O
reference	O
.	O
To	O
elicit	O
more	O
complex	O
situations	O
,	O
they	O
are	O
encouraged	O
to	O
interact	O
with	O
our	O
system	O
by	O
new	O
intents	O
and	O
slots	O
related	O
to	O
the	O
restaurant	O
domain	O
.	O
At	O
the	O
end	O
of	O
each	O
dialog	O
,	O
they	O
are	O
asked	O
to	O
give	O
a	O
subjective	O
rating	O
on	O
the	O
scale	O
from	O
1	O
to	O
5	O
based	O
on	O
the	O
naturalness	O
of	O
the	O
system	O
(	O
1	O
is	O
the	O
worst	O
,	O
5	O
is	O
the	O
best	O
.	O
)	O
.	O
After	O
filtering	O
dialog	O
sessions	O
unrelated	O
to	O
our	O
task	O
,	O
we	O
collect	O
315	O
episodes	O
in	O
total	O
.	O
Table	O
3	O
shows	O
the	O
details	O
of	O
the	O
user	O
logs	O
.	O
As	O
shown	O
in	O
Table	O
3	O
,	O
after	O
deployment	O
,	O
there	O
are	O
a	O
few	O
slots	O
11	O
The	O
extended	O
system	O
in	O
the	O
simulated	O
evaluation	O
will	O
be	O
the	O
original	O
system	O
in	O
our	O
human	O
evaluation	O
.	O

Extended	O
System	O
and	O
intents	O
unseen	O
before	O
.	O
For	O
example	O
,	O
users	O
may	O
ask	O
for	O
the	O
discount	O
information	O
or	O
take	O
a	O
taxi	O
to	O
the	O
restaurant	O
.	O
To	O
represent	O
the	O
new	O
intents	O
and	O
slots	O
,	O
the	O
dimension	O
of	O
extracted	O
dialog	O
features	O
is	O
extended	O
to	O
236	O
.	O
Meanwhile	O
,	O
the	O
number	O
of	O
system	O
actions	O
is	O
extended	O
to	O
29	O
to	O
handle	O
new	O
user	O
actions	O
.	O
To	O
deal	O
with	O
the	O
newfound	O
user	O
actions	O
,	O
we	O
define	O
14	O
rules	O
in	O
total	O
.	O
Table	O
4	O
shows	O
the	O
details	O
of	O
new	O
defined	O
logic	O
rules	O
.	O
Then	O
we	O
distill	O
the	O
knowledge	O
of	O
the	O
original	O
system	O
and	O
logic	O
rules	O
into	O
a	O
new	O
system	O
.	O
Fig	O
.	O
5	O
shows	O
the	O
comparison	O
in	O
user	O
ratings	O
.	O
The	O
extended	O
system	O
significantly	O
gets	O
a	O
higher	O
subjective	O
rating	O
than	O
the	O
original	O
one	O
.	O
It	O
proves	O
that	O
the	O
extended	O
system	O
can	O
give	O
reasonable	O
responses	O
in	O
unseen	O
dialog	O
situations	O
.	O
Table	O
5	O
shows	O
sample	O
dialogs	O
from	O
the	O
original	O
system	O
and	O
extended	O
system	O
with	O
real	O
users	O
.	O
We	O
can	O
see	O
that	O
the	O
extended	O
system	O
is	O
much	O
more	O
coherent	O
since	O
it	O
takes	O
new	O
user	O
actions	O
into	O
account	O
.	O
It	O
inspires	O
us	O
that	O
a	O
complicated	O
RL	O
-	O
based	O
dialog	O
system	O
can	O
start	O
from	O
a	O
simple	O
one	O
and	O
be	O
improved	O
incrementally	O
based	O
on	O
our	O
framework	O
.	O
This	O
design	O
pattern	O
will	O
be	O
much	O
more	O
efficient	O
if	O
the	O
interaction	O
environment	O
is	O
hard	O
to	O
access	O
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
a	O
practical	O
solution	O
to	O
maintain	O
RL	O
-	O
based	O
dialog	O
systems	O
without	O
retraining	O
from	O
scratch	O
.	O
By	O
reusing	O
existing	O
resources	O
,	O
developers	O
can	O
extend	O
the	O
RL	O
-	O
based	O
dialog	O
manager	O
to	O
deal	O
with	O
unpredictable	O
user	O
actions	O
after	O
deployment	O
.	O
Furthermore	O
,	O
our	O
method	O
holds	O
no	O
constraints	O
on	O
the	O
architecture	O
of	O
the	O
new	O
system	O
and	O
retains	O
the	O
key	O
benefit	O
of	O
data	O
-	O
driven	O
methods	O
.	O
It	O
means	O
the	O
new	O
system	O
can	O
be	O
maintained	O
iteratively	O
and	O
further	O
refined	O
by	O
RL	O
once	O
a	O
new	O
interaction	O
environment	O
is	O
available	O
.	O
The	O
sim	O
-	O

Extended	O
System	O
User	O
:	O
I	O
want	O
to	O
find	O
a	O
Sichuan	O
restaurant	O
.	O
User	O
:	O
I	O
want	O
to	O
find	O
a	O
Sichuan	O
restaurant	O
in	O
System	O
:	O
Where	O
do	O
you	O
want	O
to	O
go	O
?	O
Zhongguancun	O
area	O
.	O
User	O
:	O
Zhongguancun	O
area	O
.	O
System	O
:	O
"	O
Hot	O
Charge	O
"	O
is	O
a	O
nice	O
Sichuan	O
restaurant	O
in	O
System	O
:	O
"	O
Hot	O
Charge	O
"	O
is	O
a	O
nice	O
Sichuan	O
restaurant	O
in	O
in	O
Zhongguancun	O
area	O
.	O
Zhongguancun	O
area	O
.	O
User	O
:	O
What	O
's	O
the	O
discount	O
of	O
this	O
restaurant	O
?	O
User	O
:	O
Can	O
you	O
tell	O
me	O
the	O
discount	O
?	O
System	O
:	O
This	O
restaurant	O
is	O
offering	O
10	O
%	O
off	O
.	O
System	O
:	O
The	O
average	O
cost	O
is	O
42	O
CNY	O
.	O
User	O
:	O
Is	O
there	O
a	O
waiting	O
line	O
?	O
User	O
:	O
Tell	O
me	O
the	O
discount	O
!	O
System	O
:	O
Currently	O
there	O
are	O
5	O
parties	O
waiting	O
in	O
line	O
.	O
System	O
:	O
"	O
Hot	O
Charge	O
"	O
is	O
a	O
nice	O
Sichuan	O
restaurant	O
in	O
User	O
:	O
Can	O
you	O
book	O
a	O
table	O
for	O
me	O
?	O
Zhongguancun	O
area	O
.	O
System	O
:	O
Ok	O
.	O
User	O
:	O
Bye	O
.	O
User	O
:	O
Thanks	O
.	O
System	O
:	O
Bye	O
.	O
System	O
:	O
You	O
are	O
welcome	O
.	O
Subjective	O
Rating	O
=	O
2	O
Subjective	O
Rating	O
=	O
5	O
Table	O
5	O
:	O
Sample	O
dialogs	O
of	O
the	O
original	O
and	O
extended	O
systems	O
in	O
the	O
human	O
evaluation	O
.	O
The	O
original	O
system	O
will	O
collapse	O
when	O
encountering	O
unseen	O
slots	O
and	O
intents	O
.	O
After	O
extension	O
,	O
the	O
new	O
system	O
can	O
handle	O
more	O
user	O
actions	O
.	O
ulation	O
and	O
human	O
experiments	O
show	O
our	O
proposed	O
framework	O
guarantees	O
the	O
maintainability	O
and	O
scalability	O
in	O
RL	O
-	O
based	O
systems	O
,	O
which	O
are	O
necessary	O
for	O
any	O
industrial	O
application	O
.	O

The	O
research	O
work	O
described	O
in	O
this	O
paper	O
has	O
been	O
supported	O
by	O
the	O
National	O
Key	O
Research	O
and	O
Development	O
Program	O
of	O
China	O
under	O
Grant	O
No	O
.	O
2017YFB1002103	O
and	O
also	O
supported	O
by	O
the	O
Natural	O
Science	O
Foundation	O
of	O
China	O
under	O
Grant	O
No	O
.	O
61333018	O
.	O

Neural	O
NLG	O
for	O
Methodius	O
:	O
From	O
RST	O
Meaning	O
Representations	O
to	O
Texts	O
*	O

The	O
textual	O
output	O
of	O
Methodius	O
is	O
pseudo	O
-	O
English	O
with	O
some	O
expressions	O
replaced	O
by	O
canned	O
text	O
,	O
the	O
morpho	O
-	O
syntactic	O
descriptions	O
of	O
which	O
are	O
not	O
present	O
in	O
either	O
the	O
content	O
plan	O
or	O
in	O
the	O
logical	O
form	O
.	O
Instead	O
the	O
canned	O
text	O
is	O
retrieved	O
from	O
the	O
Methodius	O
system	O
's	O
database	O
by	O
looking	O
up	O
the	O
reference	O
given	O
in	O
the	O
content	O
plan	O
.	O
Such	O
canned	O
texts	O
might	O
occur	O
infrequently	O
in	O
a	O
relatively	O
small	O
corpus	O
.	O
To	O
avoid	O
data	O
sparsity	O
,	O
we	O
substitute	O
canned	O
texts	O
by	O
their	O
labels	O
,	O
cf	O
.	O
(	O
1b	O
)	O
,	O
(	O
1a	O
)	O
.	O
Note	O
that	O
the	O
textual	O
output	O
of	O
Methodius	O
does	O
n't	O
contain	O
nonterminal	O
symbols	O
the	O
sort	O
used	O
in	O
Balakrishnan	O
et	O
al	O
's	O
approach	O
.	O
We	O
use	O
only	O
special	O
terminal	O
symbols	O
,	O
which	O
appear	O
both	O
in	O
content	O
plans	O
(	O
decorating	O
terminal	O
nodes	O
in	O
the	O
tree	O
)	O
and	O
in	O
texts	O
(	O
representing	O
the	O
corresponding	O
chunks	O
of	O
canned	O
texts	O
)	O
.	O

We	O
anonymize	O
exhibits	O
by	O
replacing	O
them	O
with	O
entity0	O
,	O
entity1	O
,	O
etc	O
in	O
both	O
the	O
content	O
plans	O
and	O
corresponding	O
text	O
.	O
In	O
each	O
text	O
,	O
there	O
is	O
a	O
single	O
focal	O
exhibit	O
.	O
The	O
focal	O
exhibit	O
is	O
compared	O
to	O
one	O
or	O
many	O
exhibits	O
and	O
this	O
is	O
expressed	O
in	O
text	O
using	O
singular	O
and	O
plural	O
forms	O
respectively	O
(	O
e.g.	O
the	O
other	O
vessel	O
,	O
which	O
originates	O
from	O
region1	O
VS	O
the	O
other	O
coins	O
,	O
which	O
were	O
created	O
in	O
city0	O
)	O
.	O
We	O
use	O
two	O
substitution	O
forms	O
:	O
entity1	O
(	O
for	O
singular	O
)	O
and	O
entityplural	O
.	O
Content	O
plans	O
are	O
augmented	O
with	O
relevant	O
information	O
concerning	O
the	O
types	O
of	O
exhibits	O
that	O
occur	O
in	O
a	O
content	O
plan	O
.	O
The	O
type	O
predicate	O
relates	O
an	O
exhibit	O
to	O
the	O
NP	O
it	O
corresponds	O
to	O
in	O
the	O
text	O
.	O
This	O
information	O
is	O
encoded	O
within	O
the	O
Methodius	O
logical	O
form	O
and	O
thus	O
is	O
available	O
for	O
the	O
Methodius	O
system	O
when	O
it	O
comes	O
to	O
generating	O
text	O
.	O
However	O
,	O
since	O
we	O
anonymize	O
exhibits	O
and	O
we	O
ignore	O
the	O
logical	O
forms	O
,	O
we	O
need	O
to	O
explicitly	O
provide	O
the	O
type	O
information	O
of	O
each	O
exhibit	O
.	O
Methodius	O
sometimes	O
produces	O
content	O
plans	O
in	O
which	O
the	O
first	O
FACT	O
TYPE	O
is	O
missing	O
arg2	O
.	O
This	O
missing	O
position	O
corresponds	O
to	O
the	O
focal	O
exhibit	O
in	O
the	O
text	O
.	O
The	O
modified	O
corpus	O
regiments	O
the	O
input	O
by	O
ensuring	O
every	O
FACT	O
TYPE	O
includes	O
arg2	O
.	O
For	O
every	O
exhibit	O
in	O
the	O
the	O
Methodius	O
content	O
plan	O
not	O
explicitly	O
typed	O
we	O
add	O
a	O
new	O
OPTIONAL	O
TYPE	O
branch	O
to	O
the	O
tree	O
which	O
includes	O
the	O
type	O
of	O
the	O
exhibit	O
.	O
(	O
1	O
)	O
a.	O
This	O
is	O
a	O
marriage	O
cauldron	O
and	O
it	O
was	O
created	O
during	O
the	O
classical	O
period	O
in	O
between	O
420	O
and	O
410	O
2	O
.	O
The	O
output	O
of	O
Methodius	O
is	O
limited	O
with	O
respect	O
to	O
both	O
the	O
homogeneity	O
and	O
lengths	O
of	O
the	O
texts	O
-	O
Methodius	O
only	O
infrequently	O
produces	O
very	O
short	O
or	O
long	O
texts	O
,	O
e.g.	O
one	O
or	O
six	O
sentences	O
respectively	O
.	O
One	O
of	O
the	O
test	O
sets	O
,	O
which	O
is	O
described	O
below	O
,	O
is	O
explicitly	O
constructed	O
to	O
determine	O
whether	O
the	O
model	O
's	O
knowledge	O
of	O
discourse	O
structure	O
is	O
limited	O
by	O
the	O
length	O
of	O
the	O
texts	O
it	O
sees	O
.	O

In	O
the	O
training	O
set	O
,	O
there	O
are	O
around	O
4300	O
examples	O
harvested	O
by	O
using	O
the	O
Methodius	O
system	O
.	O
The	O
higher	O
number	O
of	O
inputs	O
with	O
SIMILARITY	O
(	O
2911	O
)	O
is	O
due	O
to	O
the	O
Methodius	O
system	O
.	O
This	O
proportion	O
of	O
SIMILARITY	O
persists	O
into	O
every	O
split	O
except	O
the	O
challenge	O
test	O
set	O
,	O
where	O
the	O
number	O
of	O
inputs	O
of	O
distinct	O
RST	O
types	O
is	O
more	O
homogeneous	O
.	O

We	O
have	O
two	O
splits	O
of	O
data	O
for	O
our	O
experiments	O
.	O
One	O
we	O
dub	O
the	O
'	O
challenge	O
split	O
'	O
,	O
the	O
other	O
the	O
'	O
standard	O
split	O
'	O
.	O
The	O
major	O
difference	O
between	O
them	O
is	O
their	O
average	O
lengths	O
.	O
The	O
average	O
length	O
of	O
the	O
challenge	O
split	O
items	O
are	O
roughly	O
half	O
the	O
length	O
of	O
the	O
training	O
set	O
items	O
,	O
while	O
the	O
average	O
length	O
of	O
the	O
standard	O
split	O
is	O
roughly	O
seventy	O
five	O
percent	O
of	O
the	O
training	O
set	O
items	O
.	O

In	O
the	O
standard	O
split	O
,	O
the	O
average	O
length	O
of	O
items	O
in	O
the	O
training	O
and	O
validation	O
sets	O
is	O
roughly	O
the	O
same	O
;	O
the	O
distribution	O
of	O
lengths	O
is	O
similar	O
in	O
the	O
training	O
,	O
valid	O
,	O
and	O
test	O
sets	O
but	O
the	O
training	O
set	O
still	O
includes	O
slightly	O
longer	O
sequences	O
on	O
average	O
.	O
The	O
proportion	O
of	O
items	O
with	O
distinct	O
RST	O
types	O
is	O
roughly	O
the	O
same	O
between	O
the	O
train	O
,	O
valid	O
,	O
and	O
standard	O
test	O
sets	O
.	O
This	O
test	O
set	O
does	O
n't	O
identify	O
possible	O
effects	O
of	O
item	O
length	O
on	O
correct	O
discourse	O
structure	O
production	O
.	O
Challenge	O
The	O
challenge	O
test	O
set	O
consists	O
of	O
items	O
on	O
average	O
half	O
the	O
length	O
of	O
the	O
the	O
average	O
lengths	O
of	O
items	O
in	O
the	O
train	O
and	O
valid	O
sets	O
.	O
Due	O
to	O
the	O
lower	O
frequency	O
of	O
short	O
items	O
produced	O
by	O
Methodius	O
,	O
the	O
number	O
of	O
items	O
in	O
the	O
challenge	O
set	O
is	O
reduced	O
.	O
The	O
distribution	O
of	O
items	O
with	O
CON	O
-	O
TRAST	O
and	O
SIMILARITY	O
is	O
homogeneous	O
.	O
With	O
respect	O
to	O
distinguishing	O
RST	O
types	O
,	O
the	O
challenge	O
test	O
set	O
is	O
no	O
more	O
difficult	O
than	O
the	O
standard	O
test	O
set	O
;	O
the	O
item	O
length	O
is	O
shorter	O
but	O
no	O
less	O
structured	O
.	O
Moreover	O
,	O
the	O
set	O
of	O
lexemes	O
-	O
including	O
delexicalized	O
expressions	O
-	O
which	O
occur	O
in	O
the	O
test	O
set	O
are	O
present	O
in	O
the	O
training	O
set	O
.	O
However	O
,	O
there	O
are	O
patterns	O
in	O
the	O
test	O
set	O
which	O
are	O
uncommon	O
or	O
unseen	O
in	O
the	O
training	O
set	O
,	O
e.g.	O
one	O
content	O
plan	O
in	O
the	O
challenge	O
set	O
begins	O
with	O
CONTRAST	O
but	O
no	O
such	O
items	O
are	O
found	O
in	O
training	O
.	O
This	O
distinguishes	O
possible	O
effects	O
of	O
length	O
,	O
e.g.	O
'	O
RST	O
type	O
X	O
occurs	O
in	O
the	O
third	O
sentence	O
'	O
,	O
from	O
effect	O
of	O
RST	O
tree	O
structure	O
in	O
the	O
input	O
for	O
correct	O
discourse	O
structure	O
production	O
,	O
i.e.	O
'	O
RST	O
type	O
X	O
must	O
correspond	O
to	O
lexeme	O
/	O
structure	O
Y	O
'	O
.	O
These	O
challenge	O
test	O
-	O
specific	O
content	O
plans	O
help	O
to	O
determine	O
how	O
well	O
a	O
model	O
learns	O
to	O
associate	O
certain	O
strings	O
with	O
either	O
CONTRAST	O
or	O
SIMILAR	O
-	O
ITY	O
.	O
If	O
the	O
model	O
stumbles	O
on	O
shorter	O
texts	O
then	O
its	O
knowledge	O
of	O
RST	O
structure	O
might	O
be	O
(	O
erroneously	O
)	O
conditioned	O
on	O
item	O
length	O
.	O
5	O
Evaluation	O
Methods	O

Since	O
the	O
data	O
we	O
generate	O
after	O
preprocessing	O
contains	O
certain	O
expressions	O
which	O
we	O
dub	O
'	O
special	O
terminals	O
'	O
,	O
these	O
expressions	O
can	O
be	O
tracked	O
between	O
the	O
target	O
and	O
the	O
hypothesis	O
.	O
By	O
obtaining	O
metrics	O
based	O
on	O
the	O
correspondence	O
between	O
these	O
special	O
terminals	O
,	O
we	O
get	O
a	O
picture	O
how	O
close	O
the	O
hypothesis	O
is	O
to	O
the	O
target	O
.	O
This	O
measure	O
enjoys	O
some	O
useful	O
properties	O
.	O
Firstly	O
,	O
it	O
's	O
cheap	O
-	O
it	O
is	O
defined	O
solely	O
in	O
terms	O
of	O
expressions	O
which	O
occur	O
both	O
in	O
the	O
input	O
(	O
content	O
plan	O
)	O
and	O
in	O
the	O
output	O
(	O
text	O
)	O
.	O
Second	O
,	O
the	O
special	O
terminals	O
stand	O
for	O
important	O
parts	O
of	O
the	O
text	O
-	O
those	O
ones	O
that	O
are	O
explicitly	O
provided	O
as	O
values	O
to	O
features	O
in	O
the	O
content	O
plan	O
(	O
since	O
they	O
are	O
terminals	O
)	O
.	O
Hence	O
,	O
having	O
information	O
about	O
their	O
presence	O
gives	O
us	O
a	O
good	O
hint	O
of	O
the	O
quality	O
of	O
a	O
text	O
.	O
In	O
addition	O
to	O
standard	O
evaluation	O
metrics	O
scores	O
such	O
as	O
BLEU4	O
,	O
we	O
report	O
the	O
following	O
metrics	O
for	O
each	O
test	O
item	O
:	O
2	O
Repetitions	O
:	O
A	O
special	O
terminal	O
is	O
present	O
in	O
the	O
hypothesis	O
n	O
times	O
but	O
in	O
the	O
target	O
text	O
it	O
occurs	O
m	O
times	O
,	O
where	O
m	O
<	O
n.	O
We	O
calculate	O
n−m	O
for	O
every	O
such	O
special	O
terminal	O
and	O
sum	O
up	O
.	O
Omissions	O
:	O
How	O
many	O
times	O
special	O
terminals	O
occurring	O
in	O
the	O
target	O
text	O
are	O
not	O
generated	O
at	O
all	O
in	O
the	O
hypothesis	O
.	O
Hallucinations	O
:	O
Number	O
of	O
occurrences	O
of	O
those	O
special	O
terminals	O
in	O
the	O
hypothesis	O
that	O
have	O
no	O
occurrence	O
in	O
the	O
target	O
.	O

When	O
the	O
FACT	O
-	O
LBL	O
model	O
makes	O
mistakes	O
,	O
such	O
mistakes	O
frequently	O
correspond	O
to	O
the	O
substitution	O
of	O
one	O
lexeme	O
marking	O
a	O
rhetorical	O
relation	O
for	O
another	O
marking	O
a	O
distinct	O
(	O
sometimes	O
opposite	O
)	O
relation	O
.	O
The	O
following	O
hypothesis	O
replaces	O
the	O
CON	O
-	O
TRAST	O
in	O
the	O
target	O
with	O
a	O
SIMILARITY	O
,	O
misidentifying	O
the	O
origin	O
of	O
some	O
previous	O
exhibit	O
in	O
the	O
chain	O
.	O
T	O
unlike	O
the	O
other	O
exhibits	O
you	O
recently	O
saw	O
,	O
which	O
originate	O
from	O
region0	O
,	O
this	O
coin	O
was	O
originally	O
from	O
city0	O
.	O
H	O
like	O
the	O
other	O
exhibits	O
you	O
recently	O
saw	O
,	O
this	O
coin	O
originates	O
from	O
city0	O
.	O
In	O
the	O
following	O
hypothesis	O
the	O
erroneous	O
substitution	O
of	O
SIMILARITY	O
by	O
CONTRAST	O
leads	O
to	O
an	O
outright	O
contradiction	O
:	O
T	O
like	O
the	O
other	O
exhibits	O
you	O
recently	O
saw	O
,	O
this	O
marriage	O
cauldron	O
is	O
currently	O
in	O
museum0	O
.	O
H	O
unlike	O
the	O
other	O
exhibits	O
you	O
recently	O
saw	O
,	O
which	O
are	O
located	O
in	O
museum0	O
,	O
this	O
marriage	O
cauldron	O
is	O
located	O
in	O
museum0	O
.	O
Less	O
frequently	O
the	O
insertion	O
of	O
SIMILARITY	O
or	O
CONTRAST	O
compares	O
the	O
topic	O
of	O
an	O
exhibit	O
to	O
itself	O
:	O
T	O
this	O
is	O
a	O
statue	O
and	O
it	O
was	O
created	O
during	O
historical	O
-	O
period0	O
in	O
entity0	O
-	O
creation	O
-	O
time	O
.	O
H	O
this	O
is	O
a	O
statue	O
and	O
it	O
was	O
created	O
during	O
historical	O
-	O
period0	O
in	O
entity0	O
-	O
creation	O
-	O
time	O
.	O
like	O
the	O
statue	O
,	O
this	O
statue	O
was	O
created	O
during	O
historical	O
-	O
period0	O
.	O
The	O
details	O
of	O
the	O
number	O
of	O
errors	O
and	O
successes	O
in	O
generating	O
discourse	O
connectives	O
are	O
reported	O
in	O
Appendix	O
A.	O
Fig	O
.	O
4	O
and	O
Fig	O
.	O
5	O
show	O
Fisher	O
's	O
Exact	O
Test	O
statistics	O
for	O
best	O
performing	O
RST	O
(	O
SM	O
and	O
LG	O
)	O
and	O
FACT	O
(	O
SM	O
and	O
LG	O
)	O
models	O
.	O

The	O
overall	O
conclusion	O
is	O
that	O
including	O
RST	O
relations	O
in	O
the	O
input	O
content	O
plans	O
is	O
necessary	O
to	O
achieve	O
optimum	O
performance	O
in	O
correctly	O
and	O
coherently	O
expressing	O
discourse	O
relations	O
in	O
the	O
neural	O
reimplementation	O
of	O
Methodius	O
.	O
This	O
is	O
somewhat	O
surprising	O
since	O
the	O
FACT	O
-	O
only	O
inputs	O
actually	O
have	O
all	O
the	O
information	O
necessary	O
to	O
infer	O
that	O
a	O
SIMILARITY	O
or	O
CONTRAST	O
relation	O
should	O
be	O
expressed	O
,	O
but	O
the	O
models	O
nevertheless	O
struggle	O
to	O
learn	O
the	O
desired	O
same	O
/	O
different	O
generalization	O
.	O
Moreover	O
,	O
the	O
errors	O
are	O
often	O
jarring	O
-	O
they	O
produce	O
genuine	O
incoherence	O
in	O
the	O
text	O
.	O
We	O
see	O
the	O
best	O
performance	O
from	O
the	O
RST	O
model	O
with	O
small	O
but	O
clean	O
self	O
-	O
training	O
data	O
(	O
RST	O
-	O
SM	O
)	O
,	O
as	O
it	O
comes	O
from	O
Methodius	O
and	O
thus	O
follows	O
the	O
same	O
general	O
patterns	O
as	O
the	O
ones	O
in	O
the	O
test	O
set	O
.	O
The	O
large	O
RST	O
model	O
(	O
RST	O
-	O
LG	O
)	O
had	O
similar	O
,	O
where	O
an	O
item	O
produces	O
an	O
error	O
if	O
either	O
there	O
is	O
an	O
incorrectly	O
generated	O
discourse	O
cue	O
word	O
,	O
or	O
there	O
has	O
been	O
a	O
cue	O
word	O
generated	O
while	O
the	O
target	O
has	O
none	O
,	O
or	O
no	O
cue	O
word	O
is	O
generated	O
but	O
the	O
reference	O
contains	O
one	O
.	O
The	O
dotted	O
line	O
links	O
two	O
models	O
if	O
there	O
is	O
a	O
significant	O
difference	O
between	O
their	O
performance	O
in	O
terms	O
of	O
Fisher	O
's	O
Exact	O
Test	O
statistics	O
(	O
with	O
significance	O
threshold	O
of	O
5	O
%	O
)	O
.	O
performance	O
to	O
the	O
small	O
one	O
.	O
FACT	O
models	O
,	O
both	O
small	O
and	O
large	O
,	O
show	O
significant	O
self	O
-	O
training	O
improvements	O
when	O
reranking	O
with	O
reverse	O
models	O
.	O
Because	O
the	O
RST	O
baseline	O
already	O
performs	O
relatively	O
well	O
,	O
such	O
an	O
improvement	O
is	O
not	O
observable	O
with	O
them	O
.	O
RST	O
-	O
SM	O
with	O
vanilla	O
self	O
-	O
training	O
already	O
showed	O
high	O
performance	O
.	O
In	O
the	O
case	O
of	O
the	O
FACT	O
models	O
,	O
we	O
saw	O
that	O
reranking	O
with	O
reverse	O
models	O
lowers	O
repetitions	O
,	O
omissions	O
and	O
hallucinations	O
in	O
total	O
.	O
It	O
was	O
also	O
beneficial	O
for	O
the	O
RST	O
-	O
LG	O
model	O
.	O
Despite	O
the	O
highly	O
regular	O
nature	O
of	O
the	O
rulebased	O
texts	O
,	O
even	O
our	O
best	O
models	O
do	O
not	O
get	O
close	O
to	O
zero	O
content	O
errors	O
,	O
highlighting	O
the	O
importance	O
of	O
continued	O
work	O
on	O
eliminating	O
these	O
errors	O
,	O
e.g.	O
using	O
pretrained	O
models	O
(	O
Kale	O
,	O
2020	O
;	O
Kale	O
and	O
Rastogi	O
,	O
Forthcoming	O
)	O
or	O
constrained	O
decoding	O
(	O
Balakrishnan	O
et	O
al	O
,	O
2019b	O
)	O
.	O

This	O
research	O
was	O
supported	O
by	O
a	O
collaborative	O
open	O
science	O
research	O
agreement	O
between	O
Facebook	O
and	O
The	O
Ohio	O
State	O
University	O
.	O

A	O
question	O
that	O
as	O
important	O
as	O
designing	O
a	O
subtle	O
method	O
is	O
"	O
what	O
make	O
the	O
model	O
fail	O
"	O
.	O
By	O
answering	O
this	O
question	O
we	O
can	O
gain	O
an	O
insight	O
into	O
our	O
model	O
performance	O
and	O
further	O
improve	O
it	O
.	O
The	O
method	O
used	O
for	O
analyzing	O
is	O
the	O
bottom	O
row	O
in	O
Table	O
1	O
.	O
Firstly	O
,	O
we	O
plot	O
confusion	O
matrix	O
(	O
Figure	O
3	O
)	O
,	O
observe	O
that	O
both	O
True	O
Positive	O
and	O
True	O
Negative	O
are	O
evenly	O
distributed	O
with	O
a	O
small	O
proportion	O
of	O
False	O
Negative	O
and	O
False	O
Positive	O
,	O
indicating	O
our	O
model	O
did	O
not	O
bias	O
towards	O
any	O
classes	O
.	O
Secondly	O
,	O
we	O
randomly	O
sample	O
some	O
failures	O
the	O
model	O
made	O
(	O
Table	O
2	O
)	O
.	O
It	O
seems	O
like	O
sentences	O
containing	O
more	O
numbers	O
are	O
usually	O
(	O
mis	O
)	O
classified	O
as	O
INFORMA	O
-	O
TIVE	O
while	O
the	O
ones	O
containing	O
less	O
numbers	O
are	O
classified	O
as	O
UNINFORMATIVE	O
.	O
This	O
can	O
be	O
explained	O
that	O
INFORMATIVE	O
tweets	O
provide	O
information	O
about	O
recovered	O
,	O
suspected	O
,	O
confirmed	O
and	O
death	O
cases	O
.	O
Therefore	O
,	O
numbers	O
appearance	O
is	O
inevitable	O
.	O

Lot	O
of	O
research	O
on	O
MT	O
evaluation	O
deals	O
with	O
classification	O
and	O
analysis	O
of	O
MT	O
errors	O
,	O
for	O
example	O
(	O
Vilar	O
et	O
al	O
,	O
2006	O
;	O
Farrús	O
et	O
al	O
,	O
2010	O
;	O
Stymne	O
and	O
Ahrenberg	O
,	O
2012	O
;	O
Lommel	O
et	O
al	O
,	O
2014	O
;	O
Klubička	O
et	O
al	O
,	O
2018	O
)	O
.	O
Few	O
papers	O
deal	O
with	O
human	O
perception	O
of	O
these	O
errors	O
,	O
but	O
neither	O
of	O
them	O
defines	O
precisely	O
which	O
criterion	O
is	O
the	O
translation	O
quality	O
based	O
on	O
.	O
Kirchhoff	O
et	O
al	O
(	O
2014	O
)	O
uses	O
conjoint	O
analysis	O
to	O
investigate	O
user	O
preferences	O
for	O
error	O
types	O
of	O
SMT	O
systems	O
.	O
First	O
,	O
the	O
errors	O
in	O
MT	O
outputs	O
were	O
annotated	O
,	O
and	O
then	O
MT	O
outputs	O
with	O
different	O
error	O
types	O
were	O
given	O
to	O
the	O
crowd	O
evaluators	O
.	O
They	O
were	O
asked	O
to	O
choose	O
the	O
MT	O
output	O
which	O
they	O
like	O
best	O
and	O
to	O
give	O
the	O
reason	O
for	O
their	O
preference	O
.	O
One	O
of	O
the	O
findings	O
is	O
that	O
the	O
frequencies	O
of	O
error	O
types	O
are	O
not	O
related	O
to	O
the	O
user	O
preferences	O
.	O
The	O
most	O
dispreferred	O
error	O
type	O
was	O
word	O
order	O
error	O
,	O
although	O
it	O
was	O
the	O
least	O
frequent	O
one	O
.	O
It	O
was	O
followed	O
by	O
word	O
sense	O
errors	O
(	O
ambiguity	O
)	O
,	O
then	O
morphological	O
errors	O
(	O
most	O
frequent	O
ones	O
)	O
,	O
whereas	O
errors	O
in	O
function	O
words	O
were	O
the	O
most	O
tolerable	O
.	O
A	O
similar	O
study	O
on	O
SMT	O
outputs	O
based	O
on	O
linear	O
mixed	O
-	O
effects	O
models	O
is	O
described	O
in	O
(	O
Federico	O
et	O
al	O
,	O
2014	O
)	O
,	O
aiming	O
to	O
estimate	O
the	O
impact	O
of	O
different	O
translation	O
errors	O
to	O
the	O
overall	O
translation	O
quality	O
.	O
For	O
each	O
MT	O
output	O
,	O
experts	O
were	O
asked	O
to	O
assign	O
a	O
score	O
on	O
a	O
5	O
-	O
point	O
scale	O
while	O
other	O
experts	O
annotated	O
the	O
errors	O
.	O
The	O
results	O
confirmed	O
that	O
the	O
frequency	O
of	O
errors	O
of	O
a	O
given	O
type	O
does	O
not	O
correlate	O
with	O
human	O
preferences	O
.	O
Another	O
finding	O
is	O
that	O
omissions	O
and	O
mistranslations	O
have	O
the	O
highest	O
impact	O
on	O
the	O
overall	O
translation	O
quality	O
.	O
In	O
addition	O
,	O
it	O
is	O
observed	O
that	O
certain	O
combinations	O
of	O
errors	O
have	O
less	O
impact	O
than	O
each	O
of	O
those	O
error	O
types	O
ocurring	O
in	O
isolation	O
.	O
In	O
the	O
last	O
few	O
years	O
,	O
with	O
the	O
emergence	O
of	O
NMT	O
systems	O
which	O
generate	O
much	O
more	O
fluent	O
and	O
readable	O
outputs	O
but	O
still	O
are	O
prone	O
to	O
adequacy	O
errors	O
,	O
some	O
studies	O
have	O
concentrated	O
on	O
investigating	O
adequacy	O
and	O
fluency	O
errors	O
.	O
Martindale	O
and	O
Carpuat	O
(	O
2018	O
)	O
carried	O
out	O
a	O
survey	O
to	O
determine	O
how	O
users	O
respond	O
to	O
good	O
translations	O
compared	O
to	O
translations	O
that	O
are	O
either	O
adequate	O
but	O
not	O
fluent	O
,	O
or	O
fluent	O
but	O
not	O
adequate	O
.	O
This	O
study	O
showed	O
that	O
users	O
strongly	O
disliked	O
disfluent	O
translations	O
,	O
but	O
were	O
much	O
less	O
bothered	O
with	O
adequacy	O
errors	O
.	O
Therefore	O
,	O
it	O
was	O
concluded	O
that	O
fluent	O
translations	O
with	O
adequacy	O
errors	O
can	O
mislead	O
the	O
reader	O
to	O
trust	O
an	O
incorrect	O
meaning	O
.	O
Automatic	O
identification	O
of	O
these	O
misleading	O
"	O
fluently	O
inadequate	O
"	O
translations	O
using	O
source	O
text	O
,	O
reference	O
human	O
translation	O
and	O
MT	O
output	O
was	O
proposed	O
in	O
(	O
Martindale	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
the	O
main	O
finding	O
was	O
that	O
NMT	O
systems	O
generate	O
more	O
misleading	O
translations	O
than	O
SMT	O
systems	O
.	O
However	O
,	O
the	O
question	O
about	O
how	O
many	O
adequacy	O
errors	O
are	O
actually	O
hidden	O
by	O
fluency	O
remained	O
open	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
the	O
relation	O
be	O
-	O
tween	O
adequacy	O
and	O
comprehensibility	O
has	O
not	O
been	O
investigated	O
yet	O
.	O
Comprehensibility	O
,	O
similarly	O
to	O
fluency	O
,	O
has	O
an	O
immediate	O
effect	O
on	O
the	O
reader	O
,	O
while	O
adequacy	O
problems	O
can	O
be	O
perceived	O
only	O
if	O
the	O
reader	O
has	O
access	O
to	O
the	O
source	O
text	O
or	O
to	O
a	O
correct	O
translation	O
to	O
find	O
out	O
that	O
the	O
meaning	O
is	O
wrong	O
.	O
This	O
means	O
that	O
comprehensibility	O
may	O
have	O
the	O
same	O
misleading	O
effect	O
making	O
the	O
reader	O
accept	O
an	O
incorrect	O
information	O
.	O
On	O
the	O
other	O
hand	O
,	O
because	O
comprehensibility	O
is	O
different	O
than	O
fluency	O
(	O
fluent	O
sentences	O
can	O
be	O
incomprehensible	O
and	O
vice	O
versa	O
)	O
,	O
the	O
effects	O
might	O
be	O
different	O
.	O

As	O
mentioned	O
in	O
Introduction	O
,	O
comprehensibility	O
reflects	O
the	O
degree	O
to	O
which	O
a	O
translated	O
text	O
can	O
be	O
understood	O
,	O
and	O
adequacy	O
reflects	O
the	O
degree	O
to	O
which	O
the	O
translation	O
conveys	O
the	O
meaning	O
of	O
the	O
original	O
text	O
in	O
the	O
source	O
language	O
.	O
Comprehension	O
should	O
be	O
assessed	O
without	O
access	O
to	O
the	O
original	O
text	O
in	O
the	O
source	O
language	O
(	O
or	O
a	O
correct	O
translation	O
)	O
,	O
while	O
the	O
original	O
text	O
(	O
or	O
a	O
correct	O
translation	O
)	O
is	O
mandatory	O
for	O
adequacy	O
.	O
Therefore	O
,	O
each	O
annotator	O
first	O
completed	O
the	O
annotation	O
of	O
comprehension	O
issues	O
while	O
reading	O
only	O
the	O
translation	O
.	O
After	O
completing	O
(	O
usually	O
after	O
about	O
two	O
weeks	O
)	O
,	O
they	O
annotated	O
adequacy	O
issues	O
by	O
comparing	O
the	O
translation	O
with	O
the	O
original	O
source	O
text	O
.	O
For	O
each	O
criterion	O
,	O
the	O
annotators	O
were	O
asked	O
to	O
distinguish	O
two	O
levels	O
of	O
issues	O
:	O
major	O
issues	O
and	O
minor	O
issues	O
.	O
While	O
for	O
this	O
particular	O
study	O
we	O
are	O
interested	O
only	O
in	O
major	O
issues	O
,	O
we	O
did	O
not	O
want	O
any	O
errors	O
to	O
remain	O
unannotated	O
.	O
The	O
following	O
guidelines	O
were	O
given	O
to	O
the	O
annotators	O
:	O
Comprehensibility	O
:	O
mark	O
all	O
parts	O
of	O
the	O
text	O
(	O
single	O
words	O
,	O
small	O
or	O
long	O
phrases	O
,	O
or	O
entire	O
sentences	O
)	O
which	O
are	O
not	O
understandable	O
(	O
it	O
does	O
not	O
make	O
sense	O
,	O
it	O
is	O
not	O
clear	O
what	O
it	O
is	O
about	O
,	O
etc	O
.	O
)	O
as	O
major	O
issues	O
;	O
mark	O
all	O
parts	O
of	O
the	O
text	O
(	O
again	O
:	O
words	O
,	O
phrases	O
or	O
sentences	O
)	O
which	O
seem	O
understandable	O
but	O
contain	O
grammatical	O
or	O
stylistic	O
errors	O
as	O
minor	O
issues	O
;	O
if	O
it	O
seems	O
that	O
something	O
is	O
missing	O
,	O
add	O
"	O
XXX	O
"	O
to	O
the	O
corresponding	O
position	O
.	O
Adequacy	O
:	O
mark	O
all	O
parts	O
of	O
the	O
translation	O
(	O
single	O
words	O
,	O
small	O
or	O
long	O
phrases	O
,	O
or	O
entire	O
sentences	O
)	O
which	O
have	O
different	O
meaning	O
than	O
the	O
original	O
English	O
text	O
as	O
major	O
issues	O
;	O
mark	O
all	O
parts	O
of	O
the	O
translation	O
(	O
again	O
:	O
words	O
,	O
phrases	O
or	O
sentences	O
)	O
which	O
do	O
not	O
actually	O
change	O
the	O
meaning	O
of	O
the	O
source	O
text	O
,	O
but	O
contain	O
sub	O
-	O
optimal	O
lexical	O
choices	O
or	O
grammar	O
errors	O
as	O
minor	O
issues	O
;	O
if	O
some	O
parts	O
of	O
the	O
original	O
English	O
text	O
are	O
missing	O
in	O
the	O
translation	O
,	O
add	O
"	O
XXX	O
"	O
to	O
the	O
corresponding	O
position	O
in	O
the	O
translation	O
;	O
if	O
there	O
are	O
any	O
errors	O
in	O
the	O
source	O
language	O
8	O
(	O
spelling	O
or	O
grammar	O
errors	O
,	O
etc	O
.	O
)	O
,	O
mark	O
its	O
translation	O
as	O
major	O
or	O
minor	O
issue	O
if	O
it	O
does	O
not	O
correspond	O
to	O
the	O
intented	O
English	O
word	O
even	O
though	O
it	O
is	O
a	O
correct	O
translation	O
of	O
the	O
erroneous	O
English	O
word	O
.	O
The	O
annotators	O
were	O
seeing	O
the	O
entire	O
reviews	O
during	O
the	O
process	O
,	O
not	O
only	O
isolated	O
segments	O
or	O
blocks	O
of	O
2	O
-	O
3	O
segments	O
.	O
In	O
this	O
way	O
,	O
it	O
was	O
ensured	O
that	O
the	O
annotators	O
were	O
able	O
to	O
spot	O
any	O
contextdependent	O
issues	O
.	O
We	O
wanted	O
the	O
texts	O
to	O
be	O
annotated	O
by	O
a	O
reliable	O
group	O
of	O
readers	O
which	O
is	O
neither	O
too	O
homogeneous	O
as	O
a	O
group	O
of	O
professional	O
translators	O
nor	O
too	O
heterogeneous	O
as	O
crowd	O
evaluators	O
.	O
Therefore	O
,	O
the	O
annotation	O
was	O
performed	O
by	O
computational	O
linguistics	O
researchers	O
and	O
students	O
,	O
fluent	O
in	O
the	O
source	O
language	O
and	O
native	O
speakers	O
of	O
the	O
target	O
language	O
.	O
They	O
had	O
different	O
backgrounds	O
,	O
coming	O
from	O
technical	O
studies	O
,	O
translation	O
studies	O
as	O
well	O
as	O
from	O
humanities	O
.	O
Because	O
the	O
annotators	O
were	O
not	O
asked	O
to	O
perform	O
any	O
fine	O
-	O
grained	O
categorisation	O
,	O
the	O
interannotator	O
agreement	O
was	O
high	O
-	O
annotators	O
assigned	O
identical	O
issue	O
tags	O
to	O
more	O
than	O
70	O
%	O
of	O
words	O
.	O
More	O
details	O
about	O
the	O
annotation	O
process	O
can	O
be	O
found	O
in	O
(	O
Popović	O
,	O
2020	O
)	O
.	O

Table	O
1	O
presents	O
overall	O
percentages	O
9	O
of	O
words	O
perceived	O
as	O
issues	O
,	O
separately	O
for	O
each	O
of	O
the	O
two	O
translation	O
criteria	O
.	O
In	O
total	O
(	O
including	O
both	O
target	O
languages	O
and	O
all	O
three	O
MT	O
systems	O
)	O
,	O
9.5	O
%	O
words	O
in	O
the	O
text	O
were	O
perceived	O
as	O
incomprehensible	O
,	O
and	O
the	O
meaning	O
of	O
9.9	O
%	O
words	O
was	O
changed	O
in	O
the	O
translation	O
process	O
.	O
As	O
for	O
minor	O
issues	O
,	O
13.5	O
%	O
words	O
were	O
perceived	O
as	O
slightly	O
difficult	O
to	O
understand	O
,	O
and	O
12.8	O
%	O
were	O
not	O
translated	O
in	O
the	O
optimal	O
way	O
.	O
It	O
can	O
be	O
noted	O
that	O
the	O
overall	O
amounts	O
of	O
comprehensibility	O
and	O
adequacy	O
issues	O
are	O
similar	O
.	O
However	O
,	O
it	O
does	O
not	O
necessarily	O
mean	O
that	O
the	O
majority	O
of	O
words	O
is	O
perceived	O
both	O
as	O
incomprehensible	O
and	O
inadequate	O
.	O
Therefore	O
,	O
we	O
examined	O
major	O
comprehensibility	O
and	O
adequacy	O
issues	O
in	O
depth	O
.	O
9	O
raw	O
counts	O
divided	O
by	O
the	O
total	O
number	O
of	O
words	O
in	O
the	O
text	O
including	O
those	O
without	O
issues	O
and	O
the	O
omission	O
marks	O
"	O
XXX	O
"	O

In	O
order	O
to	O
determine	O
presence	O
or	O
absence	O
of	O
misleading	O
translations	O
,	O
we	O
explored	O
the	O
following	O
cases	O
of	O
different	O
relations	O
between	O
comprehensibility	O
and	O
adequacy	O
errors	O
:	O
only	O
major	O
adequacy	O
issue	O
A	O
maj	O
(	O
comprehensible	O
inadequate	O
translation	O
)	O
-	O
incorrect	O
information	O
is	O
accepted	O
-	O
The	O
meaning	O
of	O
the	O
original	O
text	O
is	O
changed	O
but	O
the	O
translation	O
is	O
readable	O
and	O
comprehensible	O
.	O
The	O
reader	O
feels	O
comfortable	O
with	O
the	O
text	O
and	O
does	O
not	O
notice	O
any	O
problem	O
,	O
thus	O
accepting	O
the	O
incorrect	O
meaning	O
.	O
A	O
maj	O
+	O
C	O
min	O
-	O
major	O
adequacy	O
and	O
minor	O
comprehension	O
issues	O
(	O
almost	O
comprehensible	O
inadequate	O
translation	O
)	O
-	O
incorrect	O
information	O
can	O
be	O
accepted	O
-	O
The	O
meaning	O
of	O
the	O
original	O
text	O
is	O
changed	O
,	O
and	O
the	O
reader	O
finds	O
this	O
incorrect	O
meanining	O
slightly	O
difficult	O
to	O
understand	O
.	O
The	O
reader	O
is	O
therefore	O
susceptible	O
to	O
accept	O
this	O
incorrect	O
meaning	O
.	O
A	O
maj	O
+	O
C	O
maj	O
-	O
both	O
major	O
issues	O
(	O
incomprehensible	O
inadequate	O
translation	O
)	O
-	O
incorrect	O
information	O
is	O
discarded	O
-	O
The	O
meaning	O
of	O
the	O
original	O
text	O
is	O
changed	O
,	O
and	O
the	O
reader	O
is	O
not	O
able	O
to	O
understand	O
this	O
changed	O
meaning	O
.	O
The	O
reader	O
clearly	O
notices	O
that	O
there	O
is	O
something	O
wrong	O
with	O
the	O
text	O
.	O
Table	O
2	O
:	O
Raw	O
counts	O
and	O
percentages	O
of	O
words	O
(	O
normalised	O
by	O
the	O
total	O
number	O
of	O
words	O
,	O
including	O
those	O
without	O
issues	O
and	O
the	O
omission	O
marks	O
"	O
XXX	O
"	O
)	O
of	O
all	O
combinations	O
of	O
perceived	O
issue	O
types	O
.	O
For	O
cases	O
involving	O
major	O
adequacy	O
issues	O
,	O
the	O
percentages	O
normalised	O
by	O
the	O
total	O
number	O
of	O
major	O
adequacy	O
issues	O
are	O
shown	O
,	O
too	O
,	O
in	O
order	O
to	O
estimate	O
the	O
portion	O
of	O
hidden	O
adequacy	O
issues	O
.	O
For	O
the	O
sake	O
of	O
completenes	O
,	O
the	O
numbers	O
are	O
presented	O
for	O
minor	O
issues	O
,	O
too	O
,	O
although	O
they	O
were	O
not	O
further	O
analysed	O
in	O
this	O
work	O
.	O
optimal	O
way	O
,	O
but	O
the	O
reader	O
can	O
not	O
understand	O
it	O
.	O
The	O
reader	O
is	O
thus	O
missing	O
some	O
correct	O
information	O
.	O
only	O
major	O
comprehension	O
issue	O
C	O
maj	O
(	O
incomprehensible	O
adequate	O
translation	O
)	O
-	O
correct	O
information	O
is	O
discarded	O
-	O
The	O
meaning	O
of	O
the	O
original	O
text	O
is	O
correctly	O
conveyed	O
to	O
the	O
translation	O
,	O
but	O
the	O
reader	O
can	O
not	O
understand	O
it	O
.	O
The	O
reader	O
is	O
therefore	O
not	O
able	O
to	O
get	O
the	O
fully	O
correct	O
information	O
.	O
Table	O
2	O
presents	O
raw	O
counts	O
and	O
percentages	O
of	O
words	O
perceived	O
in	O
the	O
described	O
ways	O
.	O
For	O
the	O
sake	O
of	O
completeness	O
,	O
the	O
numbers	O
for	O
minor	O
issue	O
types	O
are	O
shown	O
as	O
well	O
.	O
The	O
numbers	O
are	O
generally	O
in	O
line	O
with	O
the	O
findings	O
of	O
the	O
previous	O
work	O
(	O
Kirchhoff	O
et	O
al	O
,	O
2014	O
;	O
Federico	O
et	O
al	O
,	O
2014	O
)	O
regarding	O
lack	O
of	O
correlation	O
between	O
the	O
error	O
frequency	O
and	O
perception	O
of	O
severity	O
-	O
in	O
our	O
texts	O
,	O
the	O
frequencies	O
of	O
words	O
perceived	O
only	O
as	O
minor	O
issues	O
are	O
higher	O
than	O
the	O
frequencies	O
of	O
words	O
perceived	O
as	O
major	O
issues	O
.	O
As	O
already	O
mentioned	O
,	O
minor	O
issues	O
were	O
not	O
further	O
analysed	O
in	O
this	O
work	O
,	O
because	O
by	O
definition	O
they	O
were	O
not	O
perceived	O
as	O
essential	O
:	O
either	O
the	O
meaning	O
was	O
preserved	O
although	O
not	O
conveyed	O
in	O
the	O
best	O
way	O
,	O
or	O
the	O
translation	O
was	O
slightly	O
difficult	O
to	O
understand	O
,	O
or	O
both	O
.	O
2	O
shows	O
that	O
about	O
3	O
%	O
of	O
words	O
in	O
the	O
translated	O
text	O
are	O
mis	O
-	O
leading	O
,	O
and	O
2.5	O
%	O
are	O
potentially	O
misleading	O
.	O
This	O
means	O
that	O
of	O
every	O
100	O
words	O
in	O
the	O
translation	O
,	O
3	O
are	O
fully	O
accepted	O
by	O
the	O
reader	O
although	O
their	O
meaning	O
is	O
not	O
correct	O
,	O
and	O
2	O
can	O
be	O
potentially	O
accepted	O
.	O
Furthermore	O
,	O
from	O
all	O
major	O
adequacy	O
errors	O
in	O
the	O
text	O
,	O
only	O
45.5	O
%	O
are	O
incomprehensible	O
.	O
About	O
30	O
%	O
of	O
adequacy	O
errors	O
are	O
fully	O
hidden	O
so	O
that	O
the	O
reader	O
does	O
not	O
notice	O
any	O
problem	O
,	O
and	O
about	O
25	O
%	O
are	O
partially	O
hidden	O
because	O
the	O
reader	O
is	O
not	O
fully	O
sure	O
that	O
s	O
/	O
he	O
understands	O
the	O
text	O
,	O
but	O
s	O
/	O
he	O
is	O
very	O
susceptible	O
to	O
accept	O
the	O
meaning	O
.	O

All	O
in	O
all	O
,	O
the	O
portion	O
of	O
misleading	O
translations	O
is	O
not	O
negligible	O
,	O
so	O
we	O
continued	O
our	O
analysis	O
by	O
trying	O
to	O
identify	O
error	O
types	O
associated	O
with	O
such	O
translations	O
.	O
Also	O
,	O
we	O
wanted	O
to	O
explore	O
whether	O
there	O
are	O
error	O
types	O
related	O
(	O
almost	O
)	O
exclusively	O
to	O
them	O
.	O

For	O
each	O
(	O
group	O
of	O
)	O
word	O
(	O
s	O
)	O
perceived	O
as	O
compreensibility	O
or	O
adequacy	O
major	O
issue	O
,	O
we	O
assigned	O
an	O
error	O
type	O
.	O
The	O
error	O
types	O
were	O
not	O
predefined	O
by	O
any	O
particular	O
error	O
typology	O
,	O
but	O
identified	O
while	O
looking	O
into	O
the	O
text	O
.	O
It	O
is	O
worth	O
noting	O
that	O
many	O
error	O
types	O
were	O
identified	O
,	O
but	O
most	O
of	O
them	O
are	O
ocurring	O
rarely	O
in	O
the	O
text	O
.	O
Also	O
,	O
for	O
some	O
of	O
the	O
annotated	O
words	O
no	O
particular	O
error	O
type	O
could	O
be	O
defined	O
,	O
which	O
is	O
probably	O
an	O
effect	O
of	O
annotators	O
'	O
personal	O
preferences	O
.	O
The	O
most	O
frequent	O
error	O
types	O
perceived	O
as	O
misleading	O
translations	O
can	O
be	O
defined	O
as	O
follows	O
:	O

The	O
obtained	O
translation	O
for	O
the	O
given	O
word	O
is	O
in	O
principle	O
correct	O
,	O
but	O
not	O
in	O
the	O
given	O
context	O
(	O
word	O
sense	O
error	O
)	O
.	O

The	O
obtained	O
translation	O
for	O
the	O
given	O
word	O
is	O
incorrect	O
.	O

An	O
English	O
sequence	O
consisting	O
of	O
a	O
head	O
noun	O
and	O
additional	O
nouns	O
and	O
adjectives	O
is	O
incorrectly	O
translated	O
.	O
Formation	O
rules	O
for	O
Serbian	O
and	O
Croatian	O
are	O
rather	O
different	O
than	O
for	O
English	O
and	O
there	O
is	O
often	O
no	O
unique	O
solution	O
.	O
The	O
examples	O
in	O
the	O
table	O
below	O
represent	O
two	O
English	O
noun	O
collocations	O
and	O
their	O
reference	O
translations	O
into	O
Serbian	O
and	O
Croatian	O
together	O
with	O
the	O
corresponding	O
English	O
glosses	O
.	O
This	O
type	O
of	O
issue	O
is	O
relevant	O
for	O
many	O
Slavic	O
languages	O
.	O
spelling	O
error	O
in	O
source	O
A	O
word	O
in	O
the	O
original	O
text	O
in	O
the	O
source	O
language	O
has	O
spelling	O
errors	O
which	O
result	O
in	O
incorrect	O
translation	O
.	O
This	O
type	O
of	O
issue	O
is	O
especially	O
relevant	O
for	O
user	O
-	O
generated	O
content	O
.	O
subject	O
-	O
verb	O
agreement	O
A	O
verb	O
inflection	O
in	O
the	O
translation	O
denoting	O
person	O
does	O
not	O
correspond	O
to	O
the	O
subject	O
.	O

This	O
work	O
presents	O
the	O
results	O
of	O
a	O
detailed	O
analysis	O
of	O
translation	O
errors	O
perceived	O
by	O
readers	O
as	O
major	O
comprehensibility	O
and/or	O
major	O
adequacy	O
issues	O
.	O
The	O
main	O
finding	O
is	O
that	O
good	O
comprehensibility	O
,	O
similarly	O
to	O
good	O
fluency	O
,	O
can	O
mask	O
a	O
number	O
of	O
adequacy	O
errors	O
.	O
Of	O
all	O
major	O
adequacy	O
errors	O
,	O
30	O
%	O
were	O
fully	O
comprehensible	O
,	O
thus	O
fully	O
misleading	O
the	O
reader	O
to	O
accept	O
the	O
incorrect	O
information	O
.	O
An	O
-	O
word	O
-	O
by	O
-	O
word	O
5.4	O
word	O
-	O
by	O
-	O
word	O
5.5	O
word	O
-	O
by	O
-	O
word	O
5.8	O
mistranslation	O
3.7	O
subject	O
-	O
verb	O
3.8	O
(	O
subject	O
-	O
verb	O
3.2	O
)	O
(	O
subject	O
-	O
verb	O
4.5	O
)	O
subject	O
-	O
verb	O
4.9	O
(	O
subject	O
-	O
verb	O
2.0	O
)	O
untranslated	O
3.8	O
(	O
source	O
spelling	O
2.4	O
)	O
(	O
source	O
spelling	O
3.5	O
)	O
{	O
POS	O
ambiguity	O
3.5	O
}	O
(	O
source	O
spelling	O
1.2	O
)	O
Table	O
3	O
:	O
The	O
most	O
frequent	O
error	O
types	O
perceived	O
as	O
a	O
particular	O
issue	O
combination	O
.	O
The	O
numbers	O
represent	O
percentages	O
of	O
the	O
error	O
type	O
perceived	O
as	O
the	O
issue	O
combination	O
-	O
24.0	O
%	O
of	O
all	O
comprehensible	O
inadequate	O
translations	O
(	O
accepted	O
incorrect	O
information	O
)	O
are	O
ambiguity	O
errors	O
,	O
6.0	O
%	O
are	O
mistranslations	O
,	O
etc	O
.	O
Parentheses	O
indicate	O
that	O
the	O
error	O
type	O
was	O
not	O
in	O
the	O
top	O
list	O
for	O
the	O
given	O
issue	O
combination	O
,	O
but	O
it	O
is	O
presented	O
for	O
comparison	O
because	O
it	O
is	O
in	O
the	O
top	O
list	O
for	O
misleading	O
traslations	O
.	O
other	O
25	O
%	O
of	O
major	O
adequacy	O
errors	O
were	O
perceived	O
as	O
almost	O
comprehensible	O
,	O
thus	O
being	O
potentially	O
misleading	O
.	O
In	O
addition	O
,	O
a	O
vast	O
majority	O
of	O
omissions	O
(	O
about	O
70	O
%	O
)	O
is	O
hidden	O
by	O
comprehensibility	O
.	O
Further	O
analysis	O
of	O
those	O
misleading	O
translations	O
was	O
carried	O
out	O
in	O
order	O
to	O
find	O
out	O
which	O
types	O
of	O
translation	O
errors	O
are	O
perceived	O
in	O
this	O
way	O
.	O
Ambiguous	O
words	O
,	O
mistranslations	O
,	O
noun	O
phrases	O
,	O
untranslated	O
words	O
,	O
word	O
-	O
by	O
-	O
word	O
translations	O
,	O
subject	O
-	O
verb	O
agreement	O
and	O
spelling	O
errors	O
in	O
the	O
original	O
text	O
were	O
identified	O
as	O
the	O
most	O
frequent	O
error	O
types	O
in	O
misleading	O
translations	O
.	O
Although	O
noun	O
phrase	O
problems	O
are	O
typical	O
for	O
Slavic	O
languages	O
and	O
errors	O
in	O
the	O
source	O
text	O
are	O
typical	O
for	O
user	O
generated	O
content	O
,	O
the	O
rest	O
of	O
the	O
error	O
types	O
is	O
rather	O
general	O
.	O
However	O
,	O
none	O
of	O
these	O
error	O
types	O
is	O
exclusively	O
related	O
to	O
misleading	O
translations	O
,	O
but	O
are	O
also	O
frequent	O
in	O
fully	O
incorrect	O
(	O
incomprehensible	O
inadequate	O
)	O
and	O
discarded	O
correct	O
(	O
incomprehensible	O
adequate	O
)	O
translations	O
.	O
Deeper	O
analysis	O
is	O
needed	O
to	O
potentially	O
detect	O
underlying	O
phenomena	O
specifically	O
related	O
to	O
misleading	O
translations	O
.	O
Apart	O
from	O
the	O
obvious	O
directions	O
for	O
future	O
work	O
such	O
as	O
analysing	O
more	O
texts	O
and	O
including	O
more	O
language	O
pairs	O
and	O
domains	O
,	O
the	O
presented	O
analysis	O
can	O
be	O
expanded	O
in	O
the	O
following	O
directions	O
:	O
including	O
fluency	O
in	O
the	O
analysis	O
,	O
including	O
all	O
minor	O
issues	O
in	O
the	O
analysis	O
,	O
further	O
analysis	O
of	O
omissions	O
,	O
and	O
investigating	O
co	O
-	O
ocurrences	O
of	O
different	O
error	O
types	O
.	O
Another	O
experiment	O
could	O
include	O
monolingual	O
annotators	O
for	O
comprehensibility	O
in	O
order	O
to	O
completely	O
eliminate	O
potential	O
influence	O
of	O
knowlegde	O
of	O
the	O
source	O
language	O
.	O
source	O
For	O
the	O
kind	O
of	O
shipping	O
they	O
want	O
it	O
would	O
be	O
reasonable	O
to	O
expect	O
a	O
better	O
presentation	O
.	O
MT	O
Za	O
vrstu	O
dostave	O
XXXžele	O
da	O
bi	O
bilo	O
razumno	O
očekivati	O
bolju	O
prezentaciju	O
.	O
gloss	O
For	O
the	O
kind	O
of	O
shipping	O
{	O
which	O
}	O
they	O
want	O
that	O
would	O
be	O
reasonable	O
to	O
expect	O
better	O
presentation	O
.	O

We	O
found	O
an	O
issue	O
during	O
the	O
prediction	O
where	O
some	O
words	O
are	O
labeled	O
with	O
O	O
,	O
in	O
between	O
B	O
-	O
label	O
and	O
I	O
-	O
label	O
tags	O
.	O
Our	O
solution	O
is	O
to	O
insert	O
I	O
-	O
label	O
tag	O
if	O
the	O
tag	O
is	O
surrounded	O
by	O
B	O
-	O
label	O
and	O
I	O
-	O
label	O
tags	O
with	O
the	O
same	O
entity	O
category	O
.	O
Another	O
problem	O
we	O
found	O
that	O
many	O
I	O
-	O
label	O
tags	O
are	O
paired	O
with	O
B	O
-	O
label	O
in	O
different	O
categories	O
.	O
So	O
,	O
we	O
replace	O
B	O
-	O
label	O
category	O
tag	O
with	O
corresponding	O
I	O
-	O
label	O
category	O
tag	O
.	O
This	O
step	O
improves	O
the	O
result	O
of	O
the	O
pre	O
-	O

GOT	O
:	O
Testing	O
for	O
Originality	O
in	O
Natural	O
Language	O
Generation	O

Our	O
proposed	O
generation	O
originality	O
test	O
(	O
GOT	O
)	O
determines	O
whether	O
:	O
1	O
.	O
any	O
fragment	O
in	O
a	O
generated	O
sentence	O
equals	O
an	O
"	O
original	O
"	O
fragment	O
in	O
the	O
ground	O
truth	O
,	O
in	O
which	O
case	O
the	O
generation	O
may	O
be	O
in	O
violation	O
of	O
a	O
copyright	O
law	O
,	O
if	O
no	O
citation	O
of	O
the	O
original	O
source	O
is	O
included	O
;	O
or	O
,	O
2	O
.	O
the	O
generated	O
sentence	O
is	O
"	O
original	O
"	O
,	O
per	O
Definition	O
1	O
,	O
below	O
.	O
Definition	O
1	O
(	O
Original	O
Sentence	O
)	O
.	O
A	O
sentence	O
,	O
whether	O
generated	O
or	O
in	O
the	O
ground	O
truth	O
,	O
of	O
n	O
tokens	O
is	O
original	O
if	O
there	O
exists	O
an	O
original	O
k	O
-	O
gram	O
within	O
the	O
sentence	O
for	O
some	O
k≤n	O
.	O
The	O
originality	O
of	O
k	O
-	O
grams	O
is	O
defined	O
next	O
.	O
The	O
definition	O
of	O
originality	O
of	O
a	O
fragment	O
(	O
or	O
k	O
-	O
gram	O
)	O
depends	O
on	O
whether	O
we	O
are	O
referring	O
to	O
a	O
generated	O
fragment	O
or	O
to	O
a	O
fragment	O
in	O
the	O
ground	O
truth	O
.	O
Generated	O
fragments	O
are	O
tested	O
against	O
the	O
ground	O
truth	O
.	O
If	O
the	O
generated	O
fragment	O
does	O
not	O
appear	O
in	O
the	O
ground	O
truth	O
,	O
then	O
the	O
generated	O
fragment	O
is	O
considered	O
original	O
.	O
If	O
it	O
appears	O
once	O
in	O
the	O
ground	O
truth	O
,	O
then	O
it	O
is	O
considered	O
not	O
original	O
and	O
so	O
a	O
citation	O
may	O
be	O
needed	O
.	O
See	O
Table	O
1	O
for	O
a	O
summary	O
of	O
the	O
criterion	O
for	O
each	O
type	O
of	O
fragment	O
to	O
be	O
true	O
.	O
In	O
Table	O
1	O
,	O
C	O
equals	O
the	O
number	O
of	O
times	O
that	O
fragment	O
appears	O
in	O
the	O
ground	O
truth	O
.	O

Generated	O
Metaphor	O
tears	O
The	O
arrested	O
waters	O
shone	O
and	O
danced	O
.	O
fathers	O
Expectations	O
are	O
premeditated	O
resentments	O
.	O
character	O
Today	O
is	O
the	O
companion	O
of	O
genius	O
.	O
friends	O
Assumptions	O
are	O
the	O
termites	O
of	O
relationships	O
.	O
writers	O
The	O
writer	O
is	O
the	O
lengthened	O
shadow	O
of	O
a	O
man	O
.	O
world	O
This	O
world	O
is	O
the	O
rainbow	O
of	O
us	O
.	O
truth	O
The	O
brain	O
is	O
the	O
eden	O
of	O
a	O
star	O
.	O
innocence	O
The	O
cure	O
for	O
silence	O
is	O
the	O
salt	O
of	O
speech	O
.	O
imagination	O
Success	O
is	O
the	O
only	O
deadline	O
.	O

Our	O
approach	O
to	O
originality	O
testing	O
includes	O
two	O
contributions	O
:	O
An	O
automatic	O
test	O
,	O
where	O
no	O
standard	O
existed	O
,	O
for	O
originality	O
in	O
generated	O
language	O
An	O
automatic	O
test	O
,	O
where	O
no	O
standard	O
existed	O
,	O
for	O
identifying	O
where	O
generators	O
are	O
in	O
violation	O
of	O
copying	O
an	O
original	O
use	O
of	O
language	O
without	O
attribution	O
The	O
first	O
contribution	O
tells	O
us	O
whether	O
a	O
generation	O
is	O
an	O
original	O
use	O
of	O
language	O
.	O
The	O
second	O
contribution	O
tells	O
us	O
whether	O
a	O
generation	O
is	O
,	O
at	O
least	O
,	O
not	O
at	O
risk	O
of	O
committing	O
plagiarism	O
.	O
For	O
example	O
,	O
the	O
sentence	O
"	O
A	O
bird	O
built	O
a	O
nest	O
"	O
is	O
not	O
an	O
original	O
use	O
of	O
language	O
;	O
however	O
,	O
it	O
is	O
at	O
least	O
probably	O
not	O
in	O
violation	O
of	O
plagiarism	O
since	O
it	O
does	O
not	O
contain	O
a	O
fragment	O
that	O
is	O
so	O
rare	O
that	O
it	O
should	O
be	O
protected	O
as	O
an	O
original	O
use	O
of	O
language	O
.	O

On	O
the	O
application	O
of	O
Transformers	O
for	O
estimating	O
the	O
difficulty	O
of	O
Multiple	O
-	O
Choice	O
Questions	O
from	O
text	O

ASSISTments	O
2	O
is	O
an	O
online	O
tutoring	O
system	O
that	O
provides	O
instructional	O
assistance	O
while	O
assessing	O
students	O
(	O
Feng	O
et	O
al	O
,	O
2009	O
)	O
.	O
In	O
practice	O
,	O
this	O
means	O
that	O
questions	O
-	O
called	O
problems	O
-	O
can	O
be	O
broken	O
down	O
into	O
steps	O
:	O
if	O
the	O
student	O
does	O
not	O
get	O
the	O
original	O
problem	O
correctly	O
,	O
he	O
has	O
to	O
answer	O
a	O
sequence	O
of	O
scaffolding	O
questions	O
that	O
break	O
the	O
problem	O
down	O
into	O
steps	O
.	O
In	O
the	O
current	O
work	O
,	O
we	O
consider	O
both	O
original	O
and	O
scaffolding	O
problems	O
for	O
QDE	O
from	O
text	O
.	O
An	O
example	O
problem	O
and	O
the	O
corresponding	O
scaffolding	O
questions	O
are	O
shown	O
in	O
Appendix	O
B.	O
We	O
filter	O
the	O
dataset	O
to	O
keep	O
only	O
questions	O
that	O
are	O
answered	O
by	O
at	O
least	O
50	O
students	O
,	O
to	O
improve	O
the	O
reliability	O
of	O
the	O
estimation	O
of	O
ground	O
truth	O
latent	O
traits	O
with	O
IRT	O
;	O
on	O
average	O
,	O
each	O
item	O
is	O
answered	O
by	O
151	O
students	O
and	O
each	O
student	O
answers	O
to	O
64	O
different	O
items	O
.	O
We	O
also	O
remove	O
the	O
questions	O
that	O
require	O
external	O
resources	O
and	O
the	O
system	O
messages	O
(	O
e.g.	O
"	O
Submit	O
your	O
answer	O
from	O
the	O
textbook	O
.	O
"	O
,	O
"	O
Sorry	O
,	O
that	O
is	O
incorrect	O
.	O
Let	O
's	O
go	O
to	O
the	O
next	O
question	O
!	O
"	O
)	O
.	O
After	O
removal	O
of	O
the	O
unsuitable	O
questions	O
,	O
the	O
final	O
dataset	O
used	O
for	O
QDE	O
from	O
text	O
contains	O
11	O
,	O
393	O
different	O
items	O
.	O
A	O
is	O
publicly	O
available	O
for	O
download	O
3	O
;	O
Q	O
is	O
publicly	O
available	O
under	O
request	O
4	O
.	O

CloudAcademy	O
5	O
is	O
an	O
e	O
-	O
learning	O
provider	O
offering	O
online	O
courses	O
about	O
IT	O
technologies	O
.	O
All	O
the	O
questions	O
are	O
MCQ	O
and	O
we	O
have	O
access	O
to	O
the	O
text	O
of	O
the	O
possible	O
choices	O
.	O
An	O
example	O
question	O
is	O
shown	O
in	O
Appendix	O
B.	O
The	O
dataset	O
used	O
in	O
our	O
experiments	O
is	O
a	O
sub	O
-	O
sample	O
of	O
the	O
CloudAcademy	O
data	O
collection	O
and	O
it	O
was	O
generated	O
in	O
order	O
to	O
have	O
only	O
questions	O
answered	O
by	O
at	O
least	O
50	O
students	O
.	O
A	O
contains	O
7	O
,	O
323	O
,	O
502	O
interactions	O
,	O
involving	O
34	O
,	O
696	O
students	O
and	O
13	O
,	O
603	O
unique	O
questions	O
;	O
on	O
average	O
,	O
each	O
item	O
is	O
answered	O
by	O
304	O
students	O
and	O
each	O
student	O
answers	O
to	O
115	O
different	O
items	O
.	O
The	O
overall	O
correctness	O
is	O
66	O
%	O
.	O
L	O
contains	O
the	O
transcript	O
of	O
some	O
of	O
the	O
online	O
lectures	O
offered	O
by	O
CloudAcademy	O
about	O
the	O
same	O
topics	O
(	O
i.e.	O
cloud	O
technologies	O
)	O
assessed	O
by	O
the	O
questions	O
.	O
L	O
contains	O
a	O
total	O
of	O
159	O
,	O
563	O
sentences	O
and	O
3	O
,	O
228	O
,	O
038	O
words	O
.	O

The	O
computer	O
game	O
Peter	O
wants	O
to	O
buy	O
will	O
cost	O
at	O
least	O
$	O
50	O
and	O
not	O
more	O
than	O
$	O
70	O
.	O
He	O
earns	O
$	O
3	O
an	O
hour	O
running	O
errands	O
for	O
his	O
grandmother	O
.	O
Which	O
inequality	O
shows	O
the	O
number	O
of	O
hours	O
,	O
n	O
,	O
he	O
will	O
have	O
to	O
work	O
to	O
pay	O
for	O
the	O
game	O
?	O
Original	O
326	O
What	O
is	O
the	O
minimum	O
cost	O
of	O
the	O
game	O
?	O
Scaffolding	O
327	O
What	O
is	O
the	O
maximum	O
cost	O
of	O
the	O
game	O
?	O
Scaffolding	O
328	O
Write	O
an	O
expression	O
that	O
represents	O
the	O
amount	O
of	O
money	O
Peter	O
earns	O
in	O
n	O
hours	O
.	O
Scaffolding	O
329	O
Which	O
inequality	O
shows	O
the	O
number	O
of	O
hours	O
,	O
n	O
,	O
Peter	O
will	O
have	O
to	O
work	O
to	O
pay	O
for	O
the	O
game	O
?	O
Scaffolding	O

Question	O
A	O
user	O
has	O
launched	O
an	O
EBS	O
backed	O
EC2	O
instance	O
in	O
the	O
US	O
-	O
East	O
-	O
1	O
region	O
.	O
The	O
user	O
wants	O
to	O
implement	O
a	O
disaster	O
recovery	O
(	O
DR	O
)	O
plan	O
for	O
that	O
instance	O
by	O
creating	O
another	O
instance	O
in	O
a	O
European	O
region	O
.	O
How	O
can	O
the	O
user	O
accomplish	O
this	O
?	O
Correct	O
choice	O
Create	O
an	O
AMI	O
of	O
the	O
instance	O
and	O
copy	O
the	O
AMI	O
to	O
the	O
EU	O
region	O
.	O
Then	O
launch	O
the	O
instance	O
from	O
the	O
EU	O
AMI	O
.	O

Use	O
the	O
"	O
Launch	O
more	O
like	O
this	O
"	O
option	O
to	O
copy	O
the	O
instance	O
from	O
one	O
region	O
to	O
another	O
.	O

Copy	O
the	O
instance	O
from	O
the	O
US	O
East	O
region	O
to	O
the	O
EU	O
region	O
.	O

An	O
example	O
problem	O
from	O
ASSISTments	O
and	O
the	O
corresponding	O
scaffolding	O
questions	O
are	O
shown	O
in	O
Table	O
4	O
.	O
An	O
example	O
question	O
from	O
CloudAcademy	O
,	O
with	O
its	O
correct	O
answer	O
and	O
distractors	O
,	O
is	O
given	O
in	O
Table	O
5	O
.	O

Tandem	O
Anchoring	O
:	O
a	O
Multiword	O
Anchor	O
Approach	O
for	O
Interactive	O
Topic	O
Modeling	O

Single	O
word	O
anchors	O
can	O
be	O
opaque	O
to	O
users	O
.	O
For	O
an	O
example	O
of	O
bewildering	O
anchor	O
words	O
,	O
consider	O
a	O
camera	O
bag	O
topic	O
from	O
a	O
collection	O
of	O
Amazon	O
product	O
reviews	O
(	O
Table	O
1	O
)	O
.	O
The	O
anchor	O
word	O
"	O
backpack	O
"	O
may	O
seem	O
strange	O
.	O
However	O
,	O
this	O
dataset	O
contains	O
nothing	O
about	O
regular	O
backpacks	O
;	O
thus	O
,	O
"	O
backpack	O
"	O
is	O
unique	O
to	O
camera	O
bags	O
.	O
Bizarre	O
,	O
low	O
-	O
to	O
-	O
mid	O
frequency	O
words	O
are	O
often	O
anchors	O
because	O
anchor	O
words	O
must	O
be	O
unique	O
to	O
a	O
topic	O
;	O
intuitive	O
or	O
high	O
-	O
frequency	O
words	O
can	O
not	O
be	O
anchors	O
if	O
they	O
have	O
probability	O
in	O
any	O
other	O
topic	O
.	O
The	O
anchor	O
selection	O
strategy	O
can	O
mitigate	O
this	O
problem	O
to	O
some	O
degree	O
.	O
For	O
example	O
,	O
rather	O
than	O
selecting	O
anchors	O
using	O
an	O
approximate	O
convex	O
hull	O
in	O
high	O
-	O
dimensional	O
space	O
,	O
we	O
can	O
find	O
an	O
exact	O
convex	O
hull	O
in	O
a	O
low	O
-	O
dimensional	O
embedding	O
(	O
Lee	O
and	O
Mimno	O
,	O
2014	O
)	O
.	O
This	O
strategy	O
will	O
produce	O
more	O
salient	O
topics	O
but	O
still	O
makes	O
it	O
difficult	O
for	O
users	O
to	O
manually	O
choose	O
unique	O
anchor	O
words	O
for	O
interactive	O
topic	O
modeling	O
.	O
If	O
we	O
instead	O
ask	O
users	O
to	O
give	O
us	O
representative	O
words	O
for	O
this	O
topic	O
,	O
we	O
would	O
expect	O
combinations	O
of	O
words	O
like	O
"	O
camera	O
"	O
and	O
"	O
bag	O
.	O
"	O
However	O
,	O
with	O
single	O
word	O
anchors	O
we	O
must	O
choose	O
a	O
single	O
word	O
to	O
anchor	O
each	O
topic	O
.	O
Unfortunately	O
,	O
because	O
these	O
words	O
might	O
appear	O
in	O
multiple	O
topics	O
,	O
individually	O
they	O
are	O
not	O
suitable	O
as	O
anchor	O
words	O
.	O
The	O
anchor	O
word	O
"	O
camera	O
"	O
generates	O
a	O
general	O
camera	O
topic	O
instead	O
of	O
camera	O
bags	O
,	O
and	O
the	O
topic	O
anchored	O
by	O
"	O
bag	O
"	O
includes	O
bags	O
for	O
diaper	O
pails	O
(	O
Table	O
1	O
)	O
.	O
Instead	O
,	O
we	O
need	O
to	O
use	O
sets	O
of	O
representative	O
terms	O
as	O
an	O
interpretable	O
,	O
parsimonious	O
description	O
of	O
a	O
topic	O
.	O
This	O
section	O
discusses	O
strategies	O
to	O
build	O
anchors	O
from	O
multiple	O
words	O
and	O
the	O
implications	O
of	O
using	O
multiword	O
anchors	O
to	O
recover	O
topics	O
.	O
This	O
extension	O
not	O
only	O
makes	O
anchors	O
more	O
interpretable	O
but	O
also	O
enables	O
users	O
to	O
manually	O
construct	O
effective	O
anchors	O
in	O
interactive	O
topic	O
modeling	O
settings	O
.	O

We	O
first	O
need	O
to	O
turn	O
words	O
into	O
an	O
anchor	O
.	O
If	O
we	O
interpret	O
the	O
anchor	O
algorithm	O
geometrically	O
,	O
each	O
row	O
of	O
Q	O
represents	O
a	O
word	O
as	O
a	O
point	O
in	O
V	O
-	O
dimensional	O
space	O
.	O
We	O
then	O
model	O
each	O
point	O
as	O
a	O
convex	O
combination	O
of	O
anchor	O
words	O
to	O
reconstruct	O
the	O
topic	O
matrix	O
A	O
(	O
Equation	O
1	O
)	O
.	O
Instead	O
of	O
individual	O
anchor	O
words	O
(	O
one	O
anchor	O
word	O
per	O
topic	O
)	O
,	O
we	O
use	O
anchor	O
facets	O
,	O
or	O
sets	O
of	O
words	O
that	O
describe	O
a	O
topic	O
.	O
The	O
facets	O
for	O
each	O
anchor	O
form	O
a	O
new	O
pseudoword	O
,	O
or	O
an	O
invented	O
point	O
in	O
V	O
-	O
dimensional	O
space	O
(	O
described	O
in	O
more	O
detail	O
in	O
Section	O
2.2	O
)	O
.	O
While	O
these	O
new	O
points	O
do	O
not	O
correspond	O
to	O
words	O
in	O
the	O
vocabulary	O
,	O
we	O
can	O
express	O
nonanchor	O
words	O
as	O
convex	O
combinations	O
of	O
pseudowords	O
.	O
To	O
construct	O
these	O
pseudowords	O
from	O
their	O
facets	O
,	O
we	O
combine	O
the	O
co	O
-	O
occurrence	O
profiles	O
of	O
the	O
facets	O
.	O
These	O
pseudowords	O
then	O
augment	O
the	O
original	O
cooccurrence	O
matrix	O
Q	O
with	O
K	O
additional	O
rows	O
corresponding	O
to	O
synthetic	O
pseudowords	O
forming	O
each	O
of	O
K	O
multiword	O
anchors	O
.	O
We	O
refer	O
to	O
this	O
augmented	O
matrix	O
as	O
S.	O
The	O
rest	O
of	O
the	O
anchor	O
algorithm	O
proceeds	O
unmodified	O
.	O
Our	O
augmented	O
matrix	O
S	O
is	O
therefore	O
a	O
(	O
V	O
+	O
K	O
)	O
×	O
V	O
matrix	O
.	O
As	O
before	O
,	O
V	O
is	O
the	O
number	O
of	O
token	O
types	O
in	O
the	O
data	O
and	O
K	O
is	O
the	O
number	O
of	O
topics	O
.	O
The	O
first	O
V	O
rows	O
of	O
S	O
correspond	O
to	O
the	O
V	O
token	O
types	O
observed	O
in	O
the	O
data	O
,	O
while	O
the	O
additional	O
K	O
rows	O
correspond	O
to	O
the	O
pseudowords	O
constructed	O
from	O
anchor	O
facets	O
.	O
Each	O
entry	O
of	O
S	O
en	O
-	O
codes	O
conditional	O
probabilities	O
so	O
that	O
S	O
i	O
,	O
j	O
is	O
equal	O
to	O
p	O
(	O
w	O
i	O
|	O
w	O
j	O
)	O
.	O
For	O
the	O
additional	O
K	O
rows	O
,	O
we	O
invent	O
a	O
cooccurrence	O
pattern	O
that	O
can	O
effectively	O
explain	O
the	O
other	O
words	O
'	O
conditional	O
probabilities	O
.	O
This	O
modification	O
is	O
similar	O
in	O
spirit	O
to	O
supervised	O
anchor	O
words	O
(	O
Nguyen	O
et	O
al	O
,	O
2015	O
)	O
.	O
This	O
supervised	O
extension	O
of	O
the	O
anchor	O
words	O
algorithm	O
adds	O
columns	O
corresponding	O
to	O
conditional	O
probabilities	O
of	O
metadata	O
values	O
after	O
having	O
seen	O
a	O
particular	O
word	O
.	O
By	O
extending	O
the	O
vector	O
-	O
space	O
representation	O
of	O
each	O
word	O
,	O
anchor	O
words	O
corresponding	O
to	O
metadata	O
values	O
can	O
be	O
found	O
.	O
In	O
contrast	O
,	O
our	O
extension	O
does	O
not	O
add	O
dimensions	O
to	O
the	O
representation	O
,	O
but	O
simply	O
places	O
additional	O
points	O
corresponding	O
to	O
pseudoword	O
words	O
in	O
the	O
vectorspace	O
representation	O
.	O

After	O
constructing	O
the	O
pseudowords	O
of	O
S	O
we	O
then	O
need	O
to	O
find	O
the	O
coefficients	O
C	O
i	O
,	O
k	O
which	O
describe	O
each	O
word	O
in	O
our	O
vocabulary	O
as	O
a	O
convex	O
combination	O
of	O
the	O
multiword	O
anchors	O
.	O
Like	O
standard	O
anchor	O
methods	O
,	O
we	O
solve	O
the	O
following	O
for	O
each	O
token	O
type	O
:	O
C	O
*	O
i	O
,	O
=	O
argmin	O
C	O
i	O
,	O
D	O
KL	O
S	O
i	O
,	O
K	O
k=1	O
C	O
i	O
,	O
k	O
S	O
g	O
k	O
,	O
.	O
(	O
6	O
)	O
Finally	O
,	O
we	O
appeal	O
to	O
Bayes	O
'	O
rule	O
,	O
we	O
recover	O
the	O
topic	O
-	O
word	O
matrix	O
A	O
from	O
the	O
coefficients	O
of	O
C.	O
The	O
correctness	O
of	O
the	O
topic	O
recovery	O
algorithm	O
hinges	O
upon	O
the	O
assumption	O
of	O
separability	O
.	O
Separability	O
means	O
that	O
the	O
occurrence	O
pattern	O
across	O
documents	O
of	O
the	O
anchor	O
words	O
across	O
the	O
data	O
mirrors	O
that	O
of	O
the	O
topics	O
themselves	O
.	O
For	O
single	O
word	O
anchors	O
,	O
this	O
has	O
been	O
observed	O
to	O
hold	O
for	O
a	O
wide	O
variety	O
of	O
data	O
(	O
Arora	O
et	O
al	O
,	O
2012b	O
)	O
.	O
With	O
our	O
tandem	O
anchor	O
extension	O
,	O
we	O
make	O
similar	O
assumptions	O
as	O
the	O
vanilla	O
algorithm	O
,	O
except	O
with	O
pseudowords	O
constructed	O
from	O
anchor	O
facets	O
.	O
So	O
long	O
as	O
the	O
occurrence	O
pattern	O
of	O
our	O
tandem	O
anchors	O
mirrors	O
that	O
of	O
the	O
underlying	O
topics	O
,	O
we	O
can	O
use	O
the	O
same	O
reasoning	O
as	O
Arora	O
et	O
al	O
(	O
2012a	O
)	O
to	O
assert	O
that	O
we	O
can	O
provably	O
recover	O
the	O
topic	O
-	O
word	O
matrix	O
A	O
with	O
all	O
of	O
the	O
same	O
theoretical	O
guarantees	O
of	O
complexity	O
and	O
robustness	O
.	O
Furthermore	O
,	O
we	O
runtime	O
analysis	O
given	O
by	O
Arora	O
et	O
al	O
(	O
2013	O
)	O
applies	O
to	O
tandem	O
anchors	O
.	O
If	O
desired	O
,	O
we	O
can	O
also	O
add	O
further	O
robustness	O
and	O
extensibility	O
to	O
tandem	O
anchors	O
by	O
adding	O
regularization	O
to	O
Equation	O
6	O
.	O
Regularization	O
allows	O
us	O
to	O
add	O
something	O
which	O
is	O
mathematically	O
similar	O
to	O
priors	O
,	O
and	O
has	O
been	O
shown	O
to	O
improve	O
the	O
vanilla	O
anchor	O
word	O
algorithm	O
(	O
Nguyen	O
et	O
al	O
,	O
2014	O
)	O
.	O
We	O
leave	O
the	O
question	O
of	O
the	O
best	O
regularization	O
for	O
tandem	O
anchors	O
as	O
future	O
work	O
,	O
and	O
focus	O
our	O
efforts	O
on	O
solving	O
the	O
problem	O
of	O
interactive	O
topic	O
modeling	O
.	O

Before	O
addressing	O
interactivity	O
,	O
we	O
apply	O
tandem	O
anchors	O
to	O
real	O
world	O
data	O
,	O
but	O
with	O
anchors	O
gleaned	O
from	O
metadata	O
.	O
Our	O
purpose	O
is	O
twofold	O
.	O
First	O
,	O
we	O
determine	O
which	O
combiner	O
from	O
Section	O
2.2	O
to	O
use	O
in	O
our	O
interactive	O
experiments	O
in	O
Section	O
4	O
and	O
second	O
,	O
we	O
confirm	O
that	O
well	O
-	O
chosen	O
tandem	O
anchors	O
can	O
improve	O
topics	O
.	O
In	O
addition	O
,	O
we	O
examine	O
the	O
runtime	O
of	O
tandem	O
anchors	O
and	O
compare	O
to	O
traditional	O
model	O
-	O
based	O
interactive	O
topic	O
modeling	O
techniques	O
.	O
We	O
can	O
not	O
assume	O
that	O
we	O
will	O
have	O
metadata	O
available	O
to	O
build	O
tandem	O
anchors	O
,	O
but	O
we	O
use	O
them	O
here	O
because	O
they	O
provide	O
a	O
high	O
water	O
mark	O
without	O
the	O
variance	O
introduced	O
by	O
study	O
participants	O
.	O

We	O
examine	O
the	O
qualitative	O
differences	O
between	O
how	O
users	O
select	O
multiword	O
anchor	O
facets	O
versus	O
single	O
word	O
anchors	O
.	O
Table	O
2	O
gives	O
examples	O
of	O
topics	O
generated	O
using	O
different	O
anchor	O
strategies	O
.	O
In	O
a	O
follow	O
-	O
up	O
survey	O
with	O
our	O
users	O
,	O
75	O
%	O
find	O
it	O
easier	O
to	O
affect	O
individual	O
changes	O
in	O
the	O
topics	O
using	O
tandem	O
anchors	O
compared	O
to	O
single	O
word	O
anchors	O
.	O
Users	O
who	O
prefer	O
editing	O
multiword	O
anchors	O
over	O
single	O
word	O
anchors	O
often	O
report	O
that	O
In	O
all	O
cases	O
higher	O
is	O
better	O
.	O
Multiword	O
anchors	O
produce	O
topics	O
which	O
are	O
more	O
significant	O
than	O
single	O
word	O
anchors	O
.	O
multiword	O
anchors	O
make	O
it	O
easier	O
to	O
merge	O
similar	O
topics	O
into	O
a	O
single	O
focused	O
topic	O
by	O
combining	O
anchors	O
.	O
For	O
example	O
,	O
by	O
combining	O
multiple	O
words	O
related	O
to	O
Christianity	O
,	O
users	O
were	O
able	O
to	O
create	O
a	O
topic	O
which	O
is	O
highly	O
specific	O
,	O
and	O
differentiated	O
from	O
general	O
religion	O
themes	O
which	O
included	O
terms	O
about	O
Atheism	O
and	O
Judaism	O
.	O
While	O
users	O
find	O
that	O
use	O
tandem	O
anchors	O
is	O
easier	O
,	O
only	O
55	O
%	O
of	O
our	O
users	O
say	O
that	O
they	O
prefer	O
the	O
final	O
topics	O
produced	O
by	O
tandem	O
anchors	O
compared	O
to	O
single	O
word	O
anchors	O
.	O
This	O
is	O
in	O
harmony	O
with	O
our	O
quantitative	O
measurements	O
of	O
topic	O
coherence	O
,	O
and	O
may	O
be	O
the	O
result	O
of	O
our	O
stopping	O
criteria	O
:	O
when	O
users	O
judged	O
the	O
topics	O
to	O
be	O
useful	O
.	O
However	O
,	O
100	O
%	O
of	O
our	O
users	O
feel	O
that	O
the	O
topics	O
created	O
through	O
interaction	O
were	O
better	O
than	O
those	O
generated	O
from	O
Gram	O
-	O
Schmidt	O
anchors	O
.	O
This	O
was	O
true	O
regardless	O
of	O
whether	O
we	O
used	O
tandem	O
anchors	O
or	O
single	O
word	O
anchors	O
.	O
Our	O
participants	O
also	O
produce	O
fewer	O
topics	O
when	O
using	O
multiword	O
anchors	O
.	O
The	O
mean	O
difference	O
between	O
topics	O
under	O
single	O
word	O
anchors	O
and	O
multiple	O
word	O
anchors	O
is	O
9.35	O
.	O
In	O
follow	O
up	O
interviews	O
,	O
participants	O
indicate	O
that	O
the	O
easiest	O
way	O
to	O
resolve	O
an	O
ambiguous	O
topic	O
with	O
single	O
word	O
anchors	O
was	O
to	O
create	O
a	O
new	O
anchor	O
for	O
each	O
of	O
the	O
ambiguous	O
terms	O
,	O
thus	O
explaining	O
the	O
proliferation	O
of	O
topics	O
for	O
single	O
word	O
anchors	O
.	O
In	O
contrast	O
,	O
fixing	O
an	O
ambiguous	O
tandem	O
anchor	O
is	O
simple	O
:	O
users	O
just	O
add	O
more	O
terms	O
to	O
the	O
anchor	O
facet	O
.	O

This	O
work	O
was	O
supported	O
by	O
the	O
collaborative	O
NSF	O
Grant	O
IIS	O
-	O
1409287	O
(	O
UMD	O
)	O
and	O
IIS	O
-	O
1409739	O
(	O
BYU	O
)	O
.	O
Boyd	O
-	O
Graber	O
is	O
also	O
supported	O
by	O
NSF	O
grants	O
IIS	O
-	O
1320538	O
and	O
NCSE	O
-	O
1422492	O
.	O

A	O
second	O
experiment	O
to	O
validate	O
that	O
retention	O
is	O
used	O
as	O
a	O
heuristic	O
in	O
models	O
'	O
predictions	O
is	O
to	O
modify	O
their	O
input	O
sentences	O
in	O
a	O
controlled	O
manner	O

We	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
thoughtful	O
comments	O
.	O
Part	O
of	O
this	O
work	O
was	O
performed	O
while	O
Bruno	O
Taillé	O
was	O
an	O
employee	O
of	O
BNP	O
Paribas	O
and	O
supported	O
by	O
the	O
French	O
Ministry	O
of	O
Higher	O
Education	O
,	O
Research	O
and	O
Innovation	O
under	O
the	O
CIFRE	O
convention	O
2018/0327	O
.	O

In	O
this	O
section	O
,	O
we	O
first	O
compare	O
our	O
HacRED	O
with	O
existing	O
datasets	O
.	O
Then	O
we	O
re	O
-	O
evaluate	O
the	O
SOTA	O
RE	O
models	O
on	O
HacRED	O
and	O
systematically	O
analyze	O
their	O
abilities	O
on	O
different	O
experiment	O
settings	O
.	O
At	O
last	O
,	O
we	O
demonstrate	O
the	O
effectiveness	O
of	O
HacRED	O
via	O
a	O
case	O
study	O
.	O

As	O
shown	O
in	O
Figure	O
4	O
,	O
the	O
text	O
mentions	O
multiple	O
organization	O
entities	O
and	O
similar	O
relations	O
including	O
graduate_from	O
and	O
affiliation_of	O
.	O
The	O
incorrect	O
triple	O
(	O
Lu	O
,	O
graduate_from	O
,	O
Yanjing	O
University	O
)	O
extracted	O
by	O
CasRel	O
represents	O
that	O
models	O
struggle	O
to	O
capture	O
fine	O
-	O
grained	O
semantic	O
information	O
.	O
The	O
distractive	O
phrases	O
study	O
for	O
a	O
doctorate	O
could	O
result	O
in	O
the	O
incorrect	O
extraction	O
(	O
Wu	O
,	O
graduate_from	O
,	O
University	O
of	O
Chicago	O
)	O
,	O
which	O
can	O
be	O
rectified	O
by	O
comprehending	O
the	O
context	O
of	O
before	O
finishing	O
his	O
doctoral	O
dissertation	O
.	O
Reasoning	O
is	O
needed	O
to	O
extract	O
the	O
triple	O
(	O
Wu	O
,	O
af	O
filiation_of	O
,	O
Yanjing	O
University	O
)	O
since	O
he	O
worked	O
as	O
a	O
professor	O
in	O
the	O
organization	O
.	O

In	O
order	O
to	O
effectively	O
evaluate	O
the	O
RE	O
models	O
and	O
accelerate	O
the	O
research	O
of	O
practical	O
RE	O
,	O
we	O
first	O
analyze	O
the	O
performance	O
gap	O
between	O
popular	O
datasets	O
and	O
practical	O
applications	O
.	O
Therefore	O
,	O
we	O
construct	O
a	O
large	O
-	O
scale	O
and	O
high	O
-	O
quality	O
Ha	O
-	O
cRED	O
with	O
reasonable	O
data	O
distribution	O
and	O
sufficient	O
hard	O
cases	O
.	O
To	O
focus	O
on	O
the	O
practical	O
challenging	O
cases	O
,	O
we	O
propose	O
a	O
case	O
-	O
oriented	O
construction	O
framework	O
.	O
We	O
also	O
design	O
a	O
novel	O
annotation	O
method	O
to	O
guarantee	O
the	O
quality	O
of	O
Ha	O
-	O
cRED	O
.	O
Finally	O
,	O
we	O
conduct	O
extensive	O
experiments	O
and	O
analyze	O
the	O
abilities	O
of	O
SOTA	O
models	O
from	O
various	O
aspects	O
,	O
which	O
provides	O
a	O
deeper	O
understanding	O
of	O
RE	O
models	O
and	O
inspiration	O
for	O
further	O
improvement	O
.	O

In	O

We	O
would	O
like	O
to	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
thoughtful	O
and	O
constructive	O
comments	O
.	O
This	O

XL	O
-	O
NBT	O
:	O
A	O
Cross	O
-	O
lingual	O
Neural	O
Belief	O
Tracking	O
Framework	O

The	O
final	O
component	O
is	O
a	O
slot	O
-	O
value	O
decoder	O
,	O
which	O
predicts	O
the	O
score	O
y	O
of	O
a	O
given	O
slot	O
-	O
value	O
pair	O
using	O
the	O
filtered	O
information	O
from	O
the	O
utterance	O
representation	O
r	O
as	O
:	O
y	O
(	O
c	O
s	O
,	O
c	O
v	O
,	O
u	O
t	O
,	O
a	O
t	O
)	O
=	O
W	O
T	O
y	O
[	O
r	O
(	O
u	O
t	O
)	O
g	O
(	O
c	O
s	O
,	O
c	O
v	O
,	O
a	O
t	O
)	O
]	O
(	O
3	O
)	O
where	O
W	O
y	O
R	O
H×1	O
is	O
the	O
weight	O
vector	O
.	O
The	O
above	O
expression	O
computes	O
the	O
score	O
for	O
the	O
slotvalue	O
pair	O
based	O
on	O
the	O
information	O
from	O
the	O
current	O
turn	O
.	O
We	O
combine	O
it	O
with	O
the	O
information	O
from	O
previous	O
turns	O
to	O
get	O
the	O
final	O
score	O
:	O
y	O
(	O
cv	O
|	O
ut	O
,	O
at	O
,	O
cs	O
)	O
=	O
λy	O
(	O
cs	O
,	O
cv	O
,	O
ut	O
,	O
at	O
)	O
+	O
(	O
1	O
−	O
λ	O
)	O
ŷ	O
(	O
cs	O
,	O
cv	O
,	O
ut−1	O
,	O
at−1	O
)	O
(	O
4	O
)	O
here	O
λ	O
is	O
a	O
combination	O
weight	O
.	O
For	O
each	O
given	O
slot	O
c	O
s	O
,	O
NBT	O
selects	O
the	O
single	O
highest	O
value	O
for	O
informable	O
slots	O
and	O
selects	O
all	O
values	O
above	O
a	O
certain	O
threshold	O
for	O
request	O
slots	O
.	O
Here	O
we	O
replace	O
the	O
multi	O
-	O
layer	O
perceptron	O
in	O
the	O
orginal	O
NBT	O
by	O
a	O
linear	O
output	O
layer	O
(	O
to	O
be	O
explained	O
in	O
section	O
5	O
)	O
.	O

We	O
would	O
like	O
to	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
thoughtful	O
comments	O
and	O
insightful	O
feedback	O
.	O
This	O
work	O
was	O
supported	O
by	O
the	O
National	O
Key	O
Research	O
and	O
Development	O
Program	O
of	O
China	O
(	O
2016YFB100101	O
)	O
.	O

We	O
divide	O
the	O
100	O
K	O
SwissText	O
dataset	O
(	O
downloaded	O
from	O
SwissText	O
2019	O
website	O
)	O
into	O
three	O
subsets	O
:	O
train	O
,	O
dev	O
,	O
and	O
test	O
in	O
90:5:5	O
ratio	O
(	O
i.	O
the	O
test	O
data	O
)	O
.	O
The	O
experiments	O
performed	O
over	O
these	O
datasets	O
are	O
described	O
in	O
Section	O
4.3	O
(	O
denoted	O
as	O
S1	O
experimental	O
setup	O
)	O
.	O

The	O
preprocess	O
step	O
involves	O
preprocessing	O
the	O
dataset	O
such	O
that	O
source	O
and	O
target	O
are	O
aligned	O
and	O
use	O
the	O
same	O
dictionary	O
.	O
Additionally	O
,	O
we	O
truncate	O
the	O
source	O
length	O
at	O
400	O
tokens	O
and	O
the	O
target	O
length	O
at	O
100	O
tokens	O
to	O
expedite	O
training	O
(	O
See	O
et	O
al	O
,	O
2017	O
)	O
.	O

In	O
our	O
first	O
experiment	O
we	O
studied	O
how	O
irrelevant	O
visual	O
cues	O
performed	O
compared	O
to	O
relevant	O
ones	O
.	O
We	O
fine	O
-	O
tune	O
the	O
model	O
with	O
irrelevant	O
cues	O
defined	O
as	O
:	O
S	O
irrelevant	O
:	O
=	O
(	O
1	O
−	O
S	O
h	O
)	O
,	O
where	O
,	O
S	O
h	O
represents	O
the	O
human	O
-	O
based	O
importance	O
scores	O
.	O
As	O
shown	O
in	O
the	O
'	O
Grounding	O
using	O
irrelevant	O
cues	O
'	O
section	O
of	O
Table	O
1	O
,	O
both	O
HINT	O
and	O
SCR	O
are	O
within	O
0.3	O
%	O
of	O
the	O
results	O
obtained	O
from	O
looking	O
at	O
relevant	O
regions	O
,	O
which	O
indicates	O
the	O
gains	O
for	O
HINT	O
and	O
SCR	O
are	O
not	O
necessarily	O
from	O
looking	O
at	O
relevant	O
regions	O
.	O

In	O
order	O
to	O
truly	O
assess	O
if	O
existing	O
methods	O
are	O
using	O
relevant	O
regions	O
to	O
produce	O
correct	O
answers	O
,	O
we	O
use	O
our	O
proposed	O
metric	O
:	O
Correctly	O
Predicted	O
but	O
Improperly	O
Grounded	O
(	O
CPIG	O
)	O
.	O
If	O
the	O
CPIG	O
values	O
are	O
large	O
,	O
then	O
it	O
implies	O
that	O
large	O
portion	O
of	O
correctly	O
predicted	O
samples	O
were	O
not	O
properly	O
grounded	O
.	O
Fig	O
.	O
A4	O
shows	O
%	O
CPIG	O
for	O
different	O
variants	O
of	O
HINT	O
trained	O
on	O
human	O
attention	O
-	O
based	O
cues	O
,	O
whereas	O
Fig	O
.	O
A5	O
shows	O
the	O
metric	O
for	O
different	O
variants	O
of	O
SCR	O
trained	O
on	O
textual	O
explanationbased	O
cues	O
.	O
We	O
observe	O
that	O
HINT	O
and	O
SCR	O
trained	O
on	O
relevant	O
regions	O
have	O
the	O
lowest	O
%	O
CPIG	O
values	O
(	O
70.24	O
%	O
and	O
80.22	O
%	O
respectively	O
)	O
,	O
indicating	O
that	O
they	O
are	O
better	O
than	O
other	O
variants	O
in	O
finding	O
relevant	O
regions	O
.	O
However	O
,	O
only	O
a	O
small	O
percentage	O
of	O
correctly	O
predicted	O
samples	O
were	O
properly	O
grounded	O
(	O
29.76	O
%	O
and	O
19.78	O
%	O
for	O
HINT	O
and	O
SCR	O
respectively	O
)	O
,	O
even	O
when	O
trained	O
on	O
relevant	O
cues	O
.	O

Acknowledgement	O
.	O
This	O
work	O
was	O
supported	O
in	O
part	O
by	O
AFOSR	O
grant	O
[	O
FA9550	O
-	O
18	O
-	O
1	O
-	O
0121	O
]	O
,	O
NSF	O
award	O
#	O
1909696	O
,	O
and	O
a	O
gift	O
from	O
Adobe	O
Research	O
.	O
We	O
thank	O
NVIDIA	O
for	O
the	O
GPU	O
donation	O
.	O
The	O
views	O
and	O
conclusions	O
contained	O
herein	O
are	O
those	O
of	O
the	O
authors	O
and	O
should	O
not	O
be	O
interpreted	O
as	O
representing	O
the	O
official	O
policies	O
or	O
endorsements	O
of	O
any	O
sponsor	O
.	O
We	O
are	O
grateful	O
to	O
Tyler	O
Hayes	O
for	O
agreeing	O
to	O
review	O
the	O
paper	O
at	O
short	O
notice	O
and	O
suggesting	O
valuable	O
edits	O
and	O
corrections	O
for	O
the	O
paper	O
.	O

Adhering	O
to	O
UDv2	O
guidelines	O
provided	O
an	O
opportunity	O
to	O
make	O
a	O
consistent	O
decision	O
about	O
topics	O
under	O
debate	O
,	O
and	O
to	O
generally	O
revise	O
inconsistencies	O
in	O
the	O
system	O
.	O
Our	O
revisions	O
typically	O
fall	O
under	O
one	O
of	O
the	O
following	O
three	O
categories	O
:	O
predicate	O
/	O
argument	O
types	O
distinctions	O
(	O
3.3.1	O
)	O
,	O
morphological	O
vs.	O
syntactic	O
distinctions	O
(	O
3.3.2	O
)	O
,	O
and	O
Hebrew	O
-	O
specific	O
vs.	O
universal	O
distinctions	O
(	O
3.3.3	O
)	O
.	O

Some	O
inconsistencies	O
in	O
the	O
treebank	O
were	O
spotted	O
but	O
not	O
yet	O
fixed	O
as	O
their	O
automatic	O
full	O
retrieval	O
and	O
change	O
is	O
more	O
complicated	O
12	O
.	O
For	O
example	O
,	O
it	O
-	O
extraposition	O
construction	O
is	O
represented	O
in	O
UDv2	O
by	O
a	O
combination	O
of	O
nsubj	O
and	O
ccomp	O
or	O
advcl	O
,	O
but	O
should	O
be	O
a	O
combination	O
of	O
expl+csubj	O
,	O
as	O
defined	O
in	O
the	O
guidelines	O
(	O
see	O
example	O
9	O
in	O
the	O
supplements	O
)	O
.	O
In	O
another	O
case	O
,	O
lack	O
of	O
congruence	O
was	O
found	O
between	O
our	O
treatment	O
of	O
participles	O
and	O
Adler	O
et	O
al	O
(	O
2008	O
)	O
.	O
The	O
feature	O
of	O
VerbForm	O
=	O
Part	O
marks	O
both	O
deverbal	O
nouns	O
and	O
present	O
tense	O
clauses	O
,	O
as	O
in	O
the	O
following	O
sentence	O
.	O
Hebrew	O
makes	O
various	O
uses	O
of	O
the	O
dative	O
case	O
,	O
some	O
of	O
them	O
fulfill	O
purely	O
discursive	O
functionality	O
(	O
Borer	O
and	O
Grodzinsky	O
,	O
1986	O
)	O
.	O
The	O
current	O
representation	O
of	O
the	O
dative	O
case	O
marker	O
in	O
UDv2New	O
does	O
not	O
give	O
way	O
to	O
all	O
possible	O
meanings	O
,	O
including	O
experiencer	O
dative	O
(	O
Berman	O
,	O
1982	O
)	O
as	O
opposed	O
to	O
ethical	O
dative	O
,	O
the	O
regular	O
dative	O
where	O
the	O
dative	O
argument	O
is	O
subcategorized	O
by	O
the	O
verb	O
.	O
The	O
current	O
UDv2	O
guidelines	O
do	O
not	O
distinguish	O
between	O
the	O
different	O
types	O
of	O
dative	O
,	O
so	O
an	O
educated	O
decision	O
must	O
be	O
made	O
locally	O
as	O
for	O
how	O
to	O
tell	O
them	O
apart	O
.	O
12	O
For	O
reasons	O
of	O
brevity	O
we	O
do	O
not	O
discuss	O
all	O
of	O
them	O
in	O
this	O
work	O
.	O

We	O
thank	O
the	O
ONLP	O
team	O
at	O
the	O
Open	O
University	O
of	O
Israel	O
for	O
fruitful	O
discussions	O
throughout	O
the	O
process	O
.	O
We	O
further	O
thank	O
two	O
anonymous	O
reviewers	O
for	O
their	O
detailed	O
and	O
insightful	O
comments	O
.	O
This	O
research	O
is	O
supported	O
by	O
the	O
European	O
Research	O
Council	O
,	O
ERC	O
-	O
StG	O
-	O
2015	O
scheme	O
,	O
Grant	O
number	O
677352	O
,	O
and	O
by	O
the	O
Israel	O
Science	O
Foundation	O
(	O
ISF	O
)	O
,	O
Grant	O
number	O
1739/26	O
,	O
for	O
which	O
we	O
are	O
grateful	O
.	O

Enriching	O
Language	O
Models	O
with	O
Visually	O
-	O
grounded	O
Word	O
Vectors	O
and	O
the	O
Lancaster	O
Sensorimotor	O
Norms	O

Acknowledgements	O
Thanks	O
to	O
the	O
anonymous	O
reviewers	O
whose	O
comments	O
really	O
helped	O
strengthen	O
the	O
paper	O
.	O
Also	O
thanks	O
to	O
NVIDIA	O
for	O
donating	O
that	O
GPU	O
that	O
was	O
used	O
for	O
the	O
experiments	O
.	O
than	O
baseline	O
when	O
frozen	O
for	O
2	O
epochs	O
,	O
but	O
the	O
results	O
of	O
wac	O
-	O
aoa	O
are	O
significantly	O
better	O
.	O

Clause	O
Final	O
Verb	O
Prediction	O
in	O
Hindi	O
:	O
Evidence	O
for	O
Noisy	O
Channel	O
Model	O
of	O
Communication	O

Research	O
on	O
sentence	O
comprehension	O
has	O
conclusively	O
established	O
the	O
widespread	O
role	O
of	O
prediction	O
during	O
online	O
processing	O
(	O
e.g.	O
,	O
Marslen	O
-	O
Wilson	O
,	O
1973	O
;	O
Altmann	O
and	O
Kamide	O
,	O
1999	O
;	O
Staub	O
and	O
Clifton	O
,	O
2006	O
;	O
Kutas	O
and	O
Hillyard	O
,	O
1984	O
)	O
.	O
It	O
is	O
known	O
that	O
comprehenders	O
actively	O
anticipate	O
the	O
upcoming	O
linguistic	O
material	O
prior	O
to	O
receiving	O
that	O
information	O
during	O
listening	O
or	O
reading	O
(	O
Luke	O
and	O
Christianson	O
,	O
2016	O
;	O
Staub	O
,	O
2015	O
)	O
.	O
The	O
role	O
of	O
active	O
prediction	O
during	O
comprehension	O
has	O
particularly	O
been	O
emphasized	O
for	O
processing	O
of	O
SOV	O
languages	O
(	O
e.g.	O
,	O
Konieczny	O
,	O
2000	O
;	O
Yamashita	O
,	O
1997	O
;	O
*	O
Equal	O
contribution	O
by	O
KS	O
and	O
NB	O
.	O
Friederici	O
and	O
Frisch	O
,	O
2000	O
)	O
.	O
In	O
particular	O
,	O
it	O
has	O
been	O
argued	O
that	O
preverbal	O
nominal	O
features	O
such	O
as	O
case	O
-	O
markers	O
are	O
effectively	O
used	O
to	O
make	O
precise	O
prediction	O
regarding	O
the	O
clause	O
final	O
verb	O
.	O
Indeed	O
,	O
the	O
ADAPTABILITY	O
HYPOTHESIS	O
states	O
that	O
owing	O
to	O
the	O
typological	O
properties	O
,	O
the	O
prediction	O
system	O
in	O
SOV	O
languages	O
is	O
particularly	O
adapted	O
to	O
make	O
effective	O
use	O
of	O
preverbal	O
linguistic	O
material	O
to	O
make	O
robust	O
clause	O
final	O
verbal	O
prediction	O
(	O
Vasishth	O
et	O
al	O
,	O
2010	O
;	O
Levy	O
and	O
Keller	O
,	O
2013	O
)	O
.	O
Evidence	O
for	O
the	O
adaptability	O
hypothesis	O
come	O
from	O
various	O
behavioral	O
experiments	O
that	O
show	O
effective	O
use	O
of	O
case	O
-	O
markers	O
to	O
make	O
clause	O
final	O
verbal	O
prediction	O
(	O
e.g.	O
,	O
Husain	O
et	O
al	O
,	O
2014	O
)	O
,	O
facilitation	O
at	O
the	O
verb	O
when	O
the	O
distance	O
between	O
the	O
verb	O
and	O
its	O
prior	O
dependent	O
increase	O
(	O
e.g.	O
,	O
Konieczny	O
,	O
2000	O
)	O
,	O
and	O
lack	O
of	O
structural	O
forgetting	O
in	O
the	O
face	O
of	O
complex	O
linguistic	O
environment	O
(	O
e.g.	O
,	O
Vasishth	O
et	O
al	O
,	O
2010	O
)	O
.	O
On	O
the	O
other	O
hand	O
,	O
the	O
NOISY	O
CHAN	O
-	O
NEL	O
HYPOTHESIS	O
assumes	O
that	O
prediction	O
during	O
comprehension	O
is	O
required	O
to	O
accommodate	O
uncertainty	O
in	O
the	O
input	O
(	O
Gibson	O
et	O
al	O
,	O
2013	O
;	O
Kurumada	O
and	O
Jaeger	O
,	O
2015	O
)	O
.	O
In	O
other	O
words	O
,	O
the	O
hypothesis	O
posits	O
that	O
comprehenders	O
have	O
the	O
knowledge	O
that	O
speakers	O
make	O
mistakes	O
during	O
production	O
,	O
hence	O
,	O
comprehenders	O
need	O
to	O
reconstruct	O
the	O
received	O
input	O
(	O
Ferreira	O
and	O
Patson	O
,	O
2007	O
)	O
.	O
The	O
two	O
hypotheses	O
stated	O
above	O
make	O
distinct	O
assumptions	O
regarding	O
the	O
utilization	O
of	O
pre	O
-	O
verbal	O
context	O
towards	O
making	O
clause	O
final	O
verbal	O
predictions	O
in	O
SOV	O
languages	O
.	O
One	O
way	O
to	O
operationalize	O
the	O
predictions	O
of	O
the	O
adaptability	O
hypothesis	O
is	O
to	O
assume	O
that	O
the	O
preverbal	O
linguistic	O
material	O
will	O
be	O
faithfully	O
used	O
to	O
make	O
verbal	O
prediction	O
,	O
the	O
noisy	O
channel	O
hypothesis	O
on	O
the	O
other	O
hand	O
,	O
assumes	O
that	O
the	O
preverbal	O
context	O
is	O
noisy	O
and	O
therefore	O
subject	O
to	O
reconstruction	O
.	O
One	O
consequence	O
of	O
this	O
would	O
be	O
that	O
the	O
adaptability	O
hypothesis	O
would	O
predict	O
that	O
verbal	O
prediction	O
should	O
be	O
robust	O
while	O
the	O
noisy	O
channel	O
hypothesis	O
would	O
predict	O
that	O
verbal	O
prediction	O
should	O
be	O
susceptible	O
to	O
errors	O
.	O
In	O
addition	O
,	O
the	O
two	O
hypotheses	O
would	O
make	O
distinct	O
prediction	O
regarding	O
the	O
nature	O
of	O
errors	O
that	O
might	O
occur	O
during	O
clause	O
final	O
verbal	O
prediction	O
.	O
In	O
order	O
to	O
probe	O
the	O
two	O
hypotheses	O
stated	O
earlier	O
,	O
in	O
this	O
work	O
,	O
we	O
investigate	O
various	O
incremental	O
models	O
that	O
use	O
local	O
linguistic	O
features	O
to	O
predict	O
clause	O
final	O
verbal	O
prediction	O
in	O
Hindi	O
(	O
an	O
SOV	O
language	O
)	O
.	O
The	O
distribution	O
of	O
these	O
model	O
predictions	O
is	O
compared	O
with	O
human	O
data	O
.	O
In	O
particular	O
,	O
we	O
investigate	O
to	O
what	O
extent	O
the	O
models	O
are	O
able	O
to	O
capture	O
the	O
nature	O
of	O
both	O
grammatical	O
as	O
well	O
as	O
ungrammatical	O
verbal	O
predictions	O
when	O
compared	O
to	O
data	O
collected	O
from	O
native	O
speakers	O
of	O
Hindi	O
.	O
Further	O
,	O
in	O
order	O
to	O
probe	O
the	O
assumptions	O
of	O
the	O
noisy	O
channel	O
hypothesis	O
more	O
closely	O
,	O
we	O
probe	O
multiple	O
noise	O
functions	O
to	O
investigate	O
the	O
nature	O
of	O
preverbal	O
context	O
reconstruction	O
during	O
prediction	O
.	O
The	O
paper	O
is	O
arranged	O
as	O
follow	O
,	O
in	O
Section	O
2	O
we	O
briefly	O
describe	O
the	O
experimental	O
results	O
that	O
we	O
model	O
.	O
Section	O
3	O
provide	O
the	O
necessary	O
details	O
regarding	O
methodology	O
(	O
data	O
/	O
tools	O
,	O
model	O
evaluation	O
,	O
etc	O
.	O
)	O
.	O
In	O
Sections	O
4	O
and	O
5	O
we	O
respectively	O
discuss	O
the	O
n	O
-	O
gram	O
surprisal	O
and	O
the	O
lossy	O
-	O
surprisal	O
models	O
.	O
Section	O
6	O
presents	O
the	O
results	O
.	O
Section	O
7	O
discusses	O
the	O
current	O
findings	O
and	O
its	O
implications	O
.	O
We	O
conclude	O
the	O
paper	O
in	O
Section	O
8	O
.	O

Given	O
the	O
abstract	O
nominals	O
and	O
their	O
case	O
-	O
marker	O
,	O
a	O
model	O
's	O
task	O
is	O
to	O
complete	O
the	O
input	O
string	O
with	O
an	O
appropriate	O
verb	O
phrase	O
.	O
For	O
example	O
,	O
if	O
the	O
model	O
is	O
given	O
3	O
noun	O
tokens	O
(	O
each	O
with	O
a	O
unique	O
case	O
-	O
marker	O
)	O
with	O
the	O
lexical	O
item	O
replaced	O
with	O
a	O
label	O
A	O
denoting	O
animate	O
,	O
the	O
task	O
is	O
to	O
predict	O
a	O
verb	O
phrase	O
from	O
this	O
context	O
.	O
End	O
of	O
prediction	O
is	O
signalled	O
as	O
a	O
punctuation	O
.	O
We	O
note	O
that	O
,	O
given	O
a	O
context	O
,	O
the	O
model	O
makes	O
the	O
prediction	O
in	O
an	O
incremental	O
fashion	O
,	O
rather	O
than	O
producing	O
a	O
one	O
-	O
shot	O
phrase	O
.	O
This	O
means	O
that	O
once	O
a	O
word	O
is	O
predicted	O
,	O
the	O
model	O
considers	O
it	O
as	O
part	O
of	O
the	O
context	O
for	O
the	O
prediction	O
of	O
the	O
next	O
word	O
.	O
For	O
example	O
,	O
given	O
"	O
A	O
-	O
ne	O
A	O
-	O
ko	O
A	O
-	O
se	O
"	O
,	O
the	O
model	O
completes	O
the	O
sentence	O
with	O
w	O
1	O
w	O
2	O
w	O
3	O
in	O
the	O
following	O
manner	O
:	O
A	O
-	O
ne	O
A	O
-	O
ko	O
A	O
-	O
se	O
⇒	O
w	O
1	O
A	O
-	O
ne	O
A	O
-	O
ko	O
A	O
-	O
se	O
w	O
1	O
⇒	O
w	O
2	O
A	O
-	O
ne	O
A	O
-	O
ko	O
A	O
-	O
se	O
w	O
1	O
w	O
2	O
⇒	O
w	O
3	O
All	O
implemented	O
models	O
discussed	O
in	O
Section	O
4	O
and	O
Section	O
5	O
,	O
use	O
the	O
1/2/3	O
preverbal	O
arguments	O
as	O
context	O
.	O
The	O
rationale	O
for	O
use	O
of	O
local	O
context	O
is	O
driven	O
by	O
the	O
goal	O
to	O
model	O
the	O
role	O
of	O
argument	O
structure	O
in	O
verbal	O
prediction	O
(	O
see	O
Section	O
2	O
)	O
.	O
Interestingly	O
,	O
the	O
automatically	O
parsed	O
Hindi	O
corpus	O
(	O
Bojar	O
et	O
al	O
,	O
2014	O
)	O
shows	O
that	O
arguments	O
(	O
when	O
compared	O
to	O
adjuncts	O
)	O
tend	O
to	O
be	O
closer	O
to	O
the	O
verb	O
6	O
suggesting	O
that	O
the	O
critical	O
information	O
needed	O
to	O
predict	O
the	O
verb	O
should	O
be	O
accessible	O
locally	O
.	O
In	O
addition	O
,	O
we	O
place	O
an	O
upper	O
limit	O
on	O
the	O
no	O
.	O
of	O
predicted	O
words	O
-	O
2	O
words	O
for	O
2	O
-	O
NP	O
conditions	O
and	O
3	O
for	O
3	O
-	O
NP	O
.	O
7	O
Given	O
the	O
cognitive	O
validity	O
of	O
limited	O
beam	O
-	O
size	O
(	O
e.g.	O
,	O
Boston	O
et	O
al	O
,	O
2011	O
)	O
,	O
we	O
only	O
pick	O
the	O
top	O
50	O
predictions	O
for	O
further	O
analyses	O
.	O
Both	O
human	O
and	O
model	O
completions	O
are	O
manually	O
annotated	O
with	O
verb	O
classes	O
based	O
on	O
the	O
valency	O
of	O
the	O
predicted	O
verb	O
.	O
In	O
addition	O
,	O
any	O
nominal	O
argument	O
prediction	O
was	O
also	O
annotated	O
.	O
Verb	O
classes	O
were	O
labeled	O
as	O
IN	O
(	O
intransitive	O
)	O
,	O
T	O
(	O
transitive	O
)	O
,	O
DT	O
(	O
ditransitive	O
)	O
,	O
CAUS	O
(	O
causative	O
)	O
,	O
or	O
combinations	O
of	O
the	O
above	O
in	O
case	O
a	O
combination	O
of	O
non	O
-	O
finite	O
and	O
matrix	O
verbs	O
is	O
predicted	O
.	O
For	O
example	O
,	O
the	O
following	O
phrase	O
contains	O
a	O
transitive	O
verb	O
preceded	O
by	O
its	O
object	O
noun	O
:	O
(	O
2	O
)	O
khaana	O
food	O
khaaya	O
eat	O
-	O
PT	O
−	O
N	O
T	O
Verb	O
classes	O
are	O
used	O
for	O
comparing	O
model	O
output	O
with	O
human	O
data	O
as	O
predictions	O
are	O
known	O
to	O
be	O
graded	O
rather	O
than	O
all	O
-	O
or	O
-	O
nothing	O
lexical	O
prediction	O
(	O
Luke	O
and	O
Christianson	O
,	O
2016	O
;	O
Staub	O
,	O
2015	O
)	O
.	O
Additionally	O
,	O
we	O
do	O
n't	O
predict	O
the	O
verb	O
classes	O
directly	O
to	O
keep	O
the	O
model	O
output	O
consistent	O
with	O
the	O
human	O
data	O
.	O
These	O
completions	O
are	O
then	O
labelled	O
for	O
grammaticality	O
automatically	O
;	O
given	O
the	O
prompt	O
condition	O
and	O
the	O
verb	O
class	O
of	O
the	O
completion	O
,	O
we	O
can	O
infer	O
the	O
grammaticality	O
of	O
the	O
sentence	O
.	O
8	O
7	O
No	O
significant	O
change	O
in	O
the	O
set	O
of	O
predictions	O
was	O
observed	O
on	O
increasing	O
these	O
numbers	O
any	O
further	O
.	O
8	O
We	O
use	O
information	O
from	O
our	O
human	O
-	O
annotated	O
completion	O
data	O
as	O
well	O
as	O
native	O
speaker	O
knowledge	O
to	O
construct	O
an	O
exhaustive	O
list	O
of	O
valid	O
completions	O
per	O
condition	O
for	O
this	O
purpose	O
.	O

Pred	O
-	O
Bias	O
)	O
We	O
first	O
consider	O
a	O
noise	O
distribution	O
such	O
that	O
the	O
context	O
is	O
reconstructed	O
based	O
on	O
the	O
predictability	O
of	O
a	O
sub	O
-	O
context	O
.	O
This	O
is	O
driven	O
by	O
the	O
idea	O
that	O
reconstruction	O
of	O
context	O
given	O
a	O
noisy	O
input	O
will	O
be	O
influenced	O
by	O
prior	O
linguistic	O
exposure	O
(	O
Futrell	O
et	O
al	O
,	O
2020	O
)	O
.	O
When	O
the	O
input	O
is	O
less	O
frequent	O
,	O
its	O
reconstruction	O
will	O
be	O
influenced	O
by	O
frequent	O
linguistic	O
patterns	O
in	O
the	O
language	O
.	O
Note	O
,	O
however	O
,	O
that	O
a	O
single	O
word	O
is	O
obviously	O
more	O
frequent	O
than	O
two	O
.	O
Hence	O
,	O
we	O
needed	O
to	O
control	O
for	O
the	O
reduction	O
in	O
the	O
size	O
of	O
the	O
context	O
that	O
may	O
arise	O
due	O
to	O
this	O
predictability	O
bias	O
.	O
We	O
do	O
this	O
by	O
selecting	O
sub	O
-	O
contexts	O
based	O
on	O
their	O
size	O
with	O
a	O
preference	O
to	O
a	O
larger	O
size	O
.	O
Starting	O
from	O
the	O
complete	O
context	O
,	O
we	O
thus	O
iteratively	O
reduce	O
the	O
size	O
by	O
1	O
with	O
a	O
high	O
probability	O
(	O
d	O
=	O
0.8	O
)	O
.	O
10	O
Thus	O
,	O
a	O
sub	O
-	O
context	O
of	O
size	O
m	O
is	O
considered	O
with	O
a	O
probability	O
d	O
n−m	O
where	O
m	O
is	O
the	O
size	O
of	O
the	O
corresponding	O
context	O
.	O
Hence	O
,	O
p	O
M	O
(	O
r	O
|	O
c	O
)	O
∝	O
d	O
n−m	O
p	O
L	O
(	O
r	O
)	O
(	O
3	O
)	O

We	O
next	O
consider	O
a	O
noise	O
distribution	O
which	O
exploits	O
both	O
predictability	O
bias	O
as	O
well	O
as	O
recency	O
.	O
It	O
is	O
well	O
attested	O
that	O
recent	O
input	O
is	O
easier	O
to	O
retrieve	O
from	O
memory	O
compared	O
to	O
non	O
-	O
recent	O
input	O
(	O
e.g.	O
,	O
Lewis	O
and	O
Vasishth	O
,	O
2005	O
)	O
.	O
The	O
function	O
therefore	O
is	O
motivated	O
by	O
the	O
fact	O
that	O
while	O
previous	O
linguistic	O
exposure	O
should	O
influence	O
context	O
reconstruction	O
(	O
Futrell	O
et	O
al	O
,	O
2020	O
)	O
,	O
this	O
reconstruction	O
should	O
bias	O
recent	O
linguistic	O
material	O
.	O
In	O
a	O
way	O
,	O
this	O
model	O
combines	O
the	O
properties	O
of	O
the	O
Predictability	O
bias	O
noise	O
model	O
and	O
the	O
n	O
-	O
gram	O
surprisal	O
model	O
.	O
The	O
conditional	O
probability	O
p	O
(	O
r	O
|	O
c	O
)	O
,	O
here	O
,	O
thus	O
can	O
be	O
seen	O
as	O
the	O
multiplication	O
of	O
two	O
parts	O
-	O
(	O
a	O
)	O
predictability	O
of	O
r	O
,	O
p	O
L	O
(	O
r	O
)	O
;	O
and	O
(	O
b	O
)	O
decaying	O
erasure	O
factor	O
,	O
p	O
rec	O
(	O
r	O
|	O
c	O
)	O
.	O
Let	O
c	O
=	O
w	O
1	O
w	O
2	O
w	O
n	O
,	O
r	O
=	O
w	O
i	O
1	O
w	O
i	O
2	O
w	O
i	O
k	O
for	O
some	O
n	O
,	O
k	O
,	O
then	O
p	O
M	O
(	O
r	O
|	O
c	O
)	O
∝	O
n−k	O
j=1	O
f	O
n−i	O
j	O
p	O
L	O
(	O
r	O
)	O
,	O
(	O
4	O
)	O
where	O
f	O
is	O
a	O
constant	O
fixed	O
at	O
0.8	O
.	O
11	O
Thus	O
,	O
a	O
context	O
which	O
is	O
both	O
predictable	O
and	O
can	O
be	O
formed	O
from	O
a	O
recent	O
subcontext	O
is	O
favored	O
.	O
The	O
further	O
a	O
word	O
is	O
from	O
the	O
last	O
uttered	O
word	O
,	O
the	O
lesser	O
its	O
likelihood	O
of	O
being	O
a	O
part	O
of	O
the	O
reduced	O
context	O
r.	O

Table	O
4	O
compares	O
the	O
verb	O
class	O
results	O
for	O
the	O
three	O
models	O
discussed	O
above	O
.	O
The	O
key	O
finding	O
is	O
that	O
the	O
values	O
of	O
KLp	O
for	O
the	O
LC	O
-	O
Surp	O
Pred	O
-	O
Rec	O
model	O
is	O
lower	O
than	O
the	O
other	O
models	O
for	O
most	O
of	O
the	O
conditions	O
.	O
This	O
suggests	O
that	O
the	O
model	O
performs	O
better	O
in	O
capturing	O
the	O
verb	O
class	O
distribution	O
found	O
in	O
the	O
human	O
data	O
.	O
10	O
We	O
also	O
evaluated	O
the	O
model	O
with	O
d	O
=	O
0.9	O
but	O
the	O
model	O
with	O
d	O
=	O
0.8	O
gave	O
better	O
results	O
.	O
11	O
Following	O
the	O
value	O
fixed	O
for	O
d	O
in	O
Section	O
5.1	O
.	O
In	O
order	O
to	O
test	O
if	O
the	O
improvement	O
seen	O
in	O
the	O
LC	O
-	O
Surp	O
Pred	O
-	O
Rec	O
model	O
is	O
indeed	O
significant	O
,	O
we	O
also	O
performed	O
the	O
chi	O
-	O
square	O
test	O
to	O
see	O
if	O
the	O
categories	O
of	O
verb	O
class	O
predicted	O
in	O
the	O
LC	O
-	O
Surp	O
Pred	O
-	O
Rec	O
model	O
were	O
significantly	O
different	O
from	O
other	O
models	O
.	O
Results	O
showed	O
that	O
this	O
was	O
indeed	O
true	O
-	O
categories	O
of	O
verb	O
classes	O
in	O
the	O
LC	O
-	O
Surp	O
Pred	O
-	O
Rec	O
model	O
were	O
significantly	O
different	O
(	O
p	O
<	O
0.05	O
)	O
from	O
both	O
4	O
-	O
gram	O
model	O
and	O
the	O
LC	O
-	O
Surp	O
Predbias	O
model	O
.	O
12	O
KLp	O
provides	O
a	O
measure	O
to	O
quantify	O
the	O
divergence	O
between	O
the	O
human	O
and	O
model	O
prediction	O
distributions	O
.	O
However	O
,	O
the	O
nature	O
of	O
this	O
divergence	O
is	O
still	O
unclear	O
.	O
In	O
order	O
to	O
understand	O
the	O
output	O
of	O
the	O
models	O
better	O
,	O
we	O
evaluate	O
them	O
on	O
some	O
additional	O
metrics	O
.	O
Finally	O
,	O
we	O
report	O
a	O
qualitative	O
analysis	O
of	O
the	O
model	O
output	O
.	O

We	O
implemented	O
three	O
models	O
to	O
predict	O
clause	O
final	O
verbs	O
in	O
Hindi	O
.	O
Model	O
outputs	O
were	O
compared	O
with	O
verb	O
predictions	O
of	O
native	O
speakers	O
of	O
Hindi	O
using	O
quantitative	O
measures	O
as	O
well	O
as	O
qualitatively	O
.	O
Results	O
show	O
that	O
the	O
model	O
that	O
uses	O
limited	O
preverbal	O
context	O
with	O
a	O
predictability	O
recency	O
bias	O
noise	O
function	O
captures	O
the	O
distribution	O
of	O
human	O
data	O
best	O
.	O
The	O
success	O
of	O
this	O
model	O
is	O
consistent	O
with	O
the	O
idea	O
that	O
the	O
reconstruction	O
of	O
the	O
noisy	O
context	O
during	O
prediction	O
is	O
influenced	O
by	O
prior	O
linguistic	O
exposure	O
and	O
that	O
this	O
process	O
interacts	O
with	O
recency	O
of	O
input	O
.	O
These	O
results	O
support	O
the	O
noisy	O
channel	O
hypothesis	O
to	O
language	O
comprehension	O
.	O

Evaluating	O
Attribution	O
Methods	O
using	O
White	O
-	O
Box	O
LSTMs	O

Formal	O
languages	O
are	O
often	O
used	O
to	O
evaluate	O
the	O
expressive	O
power	O
of	O
RNNs	O
.	O
Here	O
,	O
we	O
focus	O
on	O
formal	O
languages	O
that	O
have	O
been	O
recently	O
used	O
to	O
probe	O
LSTMs	O
'	O
ability	O
to	O
capture	O
three	O
kinds	O
of	O
dependencies	O
:	O
counting	O
,	O
long	O
-	O
distance	O
,	O
and	O
hierarchical	O
dependencies	O
.	O
We	O
define	O
a	O
classification	O
task	O
based	O
on	O
each	O
of	O
these	O
formal	O
languages	O
.	O

Strictly	O
piecewise	O
(	O
SP	O
,	O
Heinz	O
,	O
2007	O
)	O
languages	O
were	O
used	O
by	O
Avcu	O
et	O
al	O
(	O
2017	O
)	O
and	O
Kelleher	O
(	O
2018	O
,	O
2019a	O
,	O
b	O
)	O
to	O
test	O
the	O
propensity	O
of	O
LSTMs	O
to	O
learn	O
long	O
-	O
distance	O
dependencies	O
,	O
compared	O
to	O
Elman	O
's	O
(	O
1990	O
)	O
simple	O
recurrent	O
networks	O
.	O
SP	O
languages	O
are	O
regular	O
languages	O
whose	O
membership	O
is	O
defined	O
by	O
the	O
presence	O
or	O
absence	O
of	O
certain	O
subsequences	O
,	O
which	O
may	O
or	O
may	O
not	O
be	O
contiguous	O
.	O
For	O
example	O
,	O
ad	O
is	O
a	O
subsequence	O
of	O
abcde	O
,	O
since	O
both	O
letters	O
of	O
ad	O
occur	O
in	O
abcde	O
,	O
in	O
the	O
same	O
order	O
.	O
Based	O
on	O
these	O
ideas	O
,	O
we	O
define	O
the	O
SP	O
task	O
as	O
follows	O
.	O
Task	O
3	O
(	O
SP	O
Task	O
)	O
.	O
Given	O
x	O
{	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
}	O
*	O
,	O
determine	O
whether	O
or	O
not	O
x	O
contains	O
at	O
least	O
one	O
of	O
the	O
following	O
subsequences	O
:	O
ab	O
,	O
bc	O
,	O
cd	O
,	O
dc	O
.	O
Example	O
4	O
.	O
In	O
the	O
SP	O
task	O
,	O
aab	O
is	O
classified	O
as	O
True	O
,	O
since	O
it	O
contains	O
the	O
subsequence	O
ab	O
.	O
Similarly	O
,	O
acb	O
is	O
classified	O
as	O
True	O
,	O
since	O
it	O
contains	O
ab	O
non	O
-	O
contiguously	O
.	O
The	O
string	O
aaa	O
is	O
classified	O
as	O
False	O
.	O
The	O
choice	O
of	O
SP	O
languages	O
as	O
a	O
test	O
for	O
longdistance	O
dependencies	O
is	O
motivated	O
by	O
the	O
fact	O
that	O
symbols	O
in	O
a	O
non	O
-	O
contiguous	O
subsequence	O
may	O
occur	O
arbitrarily	O
far	O
from	O
one	O
another	O
.	O
The	O
SP	O
task	O
yields	O
a	O
variant	O
of	O
the	O
pointing	O
game	O
task	O
in	O
the	O
sense	O
that	O
the	O
input	O
string	O
may	O
or	O
may	O
not	O
contain	O
an	O
"	O
object	O
"	O
(	O
one	O
of	O
the	O
four	O
subsequences	O
)	O
that	O
the	O
network	O
must	O
identify	O
.	O
Therefore	O
,	O
we	O
expect	O
an	O
input	O
symbol	O
to	O
receive	O
a	O
nonzero	O
attribution	O
score	O
if	O
and	O
only	O
if	O
it	O
comprises	O
a	O
subsequence	O
.	O

To	O
evaluate	O
attribution	O
methods	O
under	O
our	O
framework	O
,	O
we	O
begin	O
with	O
a	O
qualitative	O
description	O
of	O
the	O
heatmaps	O
that	O
are	O
computed	O
for	O
our	O
whitebox	O
networks	O
,	O
based	O
on	O
the	O
illustrative	O
sample	O
of	O
heatmaps	O
appearing	O
in	O
Table	O
3	O
.	O

The	O
heatmaps	O
for	O
the	O
PDA	O
-	O
based	O
network	O
also	O
differ	O
strikingly	O
from	O
those	O
of	O
the	O
other	O
networks	O
,	O
in	O
that	O
the	O
gradient	O
-	O
based	O
methods	O
never	O
assign	O
nonzero	O
scores	O
.	O
This	O
is	O
because	O
equation	O
(	O
1	O
)	O
causes	O
g	O
(	O
t	O
)	O
to	O
be	O
highly	O
saturated	O
,	O
resulting	O
in	O
zero	O
gradients	O
.	O
In	O
the	O
case	O
of	O
LRP	O
,	O
the	O
matching	O
bracket	O
is	O
highlighted	O
when	O
c	O
̸	O
=	O
None	O
.	O
When	O
the	O
matching	O
bracket	O
is	O
not	O
the	O
last	O
symbol	O
of	O
the	O
input	O
,	O
the	O
other	O
unclosed	O
brackets	O
are	O
also	O
highlighted	O
,	O
with	O
progressively	O
smaller	O
magnitudes	O
,	O
and	O
with	O
brackets	O
of	O
the	O
opposite	O
type	O
from	O
c	O
receiving	O
negative	O
scores	O
.	O
This	O
pattern	O
reflects	O
the	O
mechanism	O
of	O
(	O
1	O
)	O
,	O
in	O
which	O
progressively	O
larger	O
powers	O
of	O
2	O
are	O
used	O
to	O
determine	O
the	O
content	O
copied	O
to	O
c	O
(	O
t	O
)	O
k	O
.	O
When	O
the	O
relevance	O
output	O
class	O
is	O
c	O
=	O
None	O
,	O
LRP	O
assigns	O
opening	O
brackets	O
a	O
negative	O
score	O
,	O
revealing	O
the	O
fact	O
that	O
those	O
input	O
symbols	O
set	O
the	O
bit	O
c	O
(	O
t	O
)	O
2k+1	O
to	O
indicate	O
that	O
the	O
stack	O
is	O
not	O
empty	O
.	O
Although	O
occlusion	O
sometimes	O
highlights	O
the	O
matching	O
bracket	O
,	O
it	O
does	O
not	O
appear	O
to	O
be	O
consistent	O
in	O
doing	O
so	O
.	O
For	O
example	O
,	O
it	O
fails	O
to	O
highlight	O
the	O
matching	O
bracket	O
(	O
[	O
[	O
(	O
[	O
(	O
[	O
[	O
(	O
[	O
(	O
[	O
[	O
(	O
[	O
(	O
[	O
[	O
(	O
[	O
22	O
)	O
)	O
(	O
[	O
[	O
(	O
[	O
]	O
(	O
[	O
[	O
(	O
[	O
]	O
(	O
[	O
[	O
(	O
[	O
]	O
(	O
[	O
[	O
(	O
[	O
]	O
(	O
[	O
[	O
(	O
[	O
]	O
23	O
None	O
None	O
(	O
[	O
[	O
]	O
]	O
)	O
(	O
[	O
[	O
]	O
]	O
)	O
(	O
[	O
[	O
]	O
]	O
)	O
(	O
[	O
[	O
]	O
]	O
)	O
(	O
[	O
[	O
]	O
]	O
)	O
24	O
]	O
]	O
[	O
(	O
[	O
]	O
[	O
(	O
)	O
[	O
(	O
[	O
]	O
[	O
(	O
)	O
[	O
(	O
[	O
]	O
[	O
(	O
)	O
[	O
(	O
[	O
]	O
[	O
(	O
)	O
[	O
(	O
[	O
]	O
[	O
(	O
)	O
25	O
)	O
]	O
[	O
(	O
[	O
]	O
[	O
(	O
)	O
[	O
(	O
[	O
]	O
[	O
(	O
)	O
[	O
(	O
[	O
]	O
[	O
(	O
)	O
[	O
(	O
[	O
]	O
[	O
(	O
)	O
[	O
(	O
[	O
]	O
[	O
(	O
)	O

We	O
now	O
turn	O
to	O
focused	O
investigations	O
of	O
particular	O
phenomena	O
that	O
attribution	O
methods	O
exhibit	O
when	O
applied	O
to	O
white	O
-	O
box	O
networks	O
.	O
Subsection	O
7.1	O
begins	O
by	O
discussing	O
the	O
effect	O
of	O
network	O
saturation	O
on	O
the	O
gradient	O
-	O
based	O
methods	O
and	O
LRP	O
.	O
In	O
Subsection	O
7.2	O
we	O
apply	O
Bach	O
et	O
al	O
's	O
(	O
2015	O
)	O
ablation	O
test	O
to	O
our	O
attribution	O
methods	O
for	O
the	O
SP	O
task	O
.	O

So	O
far	O
,	O
we	O
have	O
primarily	O
compared	O
attribution	O
methods	O
via	O
visual	O
inspection	O
of	O
individual	O
examples	O
.	O
To	O
compare	O
the	O
five	O
methods	O
quantitatively	O
,	O
we	O
apply	O
the	O
ablation	O
test	O
of	O
Bach	O
et	O
al	O
(	O
2015	O
)	O
to	O
our	O
two	O
white	O
-	O
box	O
networks	O
for	O
the	O
SP	O
task	O
.	O
5	O
Given	O
an	O
input	O
string	O
classified	O
as	O
True	O
,	O
we	O
iteratively	O
remove	O
the	O
symbol	O
with	O
the	O
highest	O
relevance	O
score	O
,	O
recomputing	O
heatmaps	O
at	O
each	O
iteration	O
,	O
until	O
the	O
string	O
no	O
longer	O
contains	O
any	O
of	O
the	O
four	O
subsequences	O
.	O
We	O
apply	O
the	O
ablation	O
test	O
to	O
100	O
randomly	O
generated	O
input	O
strings	O
,	O
and	O
report	O
the	O
average	O
percentage	O
of	O
each	O
string	O
that	O
is	O
ablated	O
in	O
Table	O
6	O
.	O
A	O
peculiar	O
property	O
of	O
the	O
SP	O
task	O
is	O
that	O
removing	O
a	O
symbol	O
preserves	O
the	O
validity	O
of	O
input	O
strings	O
.	O
This	O
means	O
that	O
,	O
unlike	O
in	O
NLP	O
settings	O
,	O
our	O
ablation	O
test	O
does	O
not	O
suffer	O
from	O
the	O
issue	O
that	O
ablation	O
produces	O
invalid	O
inputs	O
.	O
Saliency	O
,	O
G	O
×	O
I	O
,	O
and	O
LRP	O
perform	O
close	O
to	O
the	O
random	O
baseline	O
on	O
the	O
FSA	O
network	O
;	O
this	O
is	O
unsurprising	O
,	O
since	O
these	O
methods	O
only	O
assign	O
nonzero	O
scores	O
to	O
the	O
last	O
input	O
symbol	O
.	O
While	O
Table	O
3	O
shows	O
some	O
variation	O
in	O
the	O
IG	O
heatmaps	O
,	O
IG	O
also	O
performs	O
close	O
to	O
the	O
random	O
baseline	O
.	O
Only	O
occlusion	O
performs	O
considerably	O
better	O
,	O
since	O
it	O
is	O
able	O
to	O
identify	O
symbols	O
whose	O
ablation	O
would	O
destroy	O
subsequences	O
.	O
On	O
the	O
counter	O
-	O
based	O
SP	O
network	O
,	O
IG	O
performs	O
remarkably	O
close	O
to	O
the	O
optimal	O
benchmark	O
,	O
which	O
represents	O
the	O
best	O
possible	O
performance	O
on	O
this	O
task	O
.	O
Occlusion	O
,	O
G	O
×	O
I	O
,	O
and	O
LRP	O
achieve	O
a	O
similar	O
level	O
of	O
performance	O
to	O
one	O
another	O
,	O
while	O
saliency	O
performs	O
worse	O
than	O
the	O
random	O
baseline	O
.	O

Of	O
all	O
the	O
heatmaps	O
considered	O
in	O
this	O
paper	O
,	O
only	O
those	O
computed	O
by	O
G	O
×	O
I	O
and	O
IG	O
for	O
the	O
counting	O
task	O
fully	O
matched	O
our	O
expectations	O
.	O
In	O
other	O
cases	O
,	O
all	O
attribution	O
methods	O
fail	O
to	O
identify	O
at	O
least	O
some	O
of	O
the	O
input	O
features	O
that	O
should	O
be	O
considered	O
relevant	O
,	O
or	O
assign	O
relevance	O
to	O
input	O
features	O
that	O
do	O
not	O
affect	O
the	O
model	O
's	O
behavior	O
.	O
Among	O
the	O
five	O
methods	O
,	O
saliency	O
achieves	O
the	O
worst	O
performance	O
:	O
it	O
never	O
assigns	O
nonzero	O
scores	O
for	O
the	O
counting	O
and	O
bracket	O
prediction	O
tasks	O
,	O
and	O
it	O
does	O
not	O
identify	O
the	O
relevant	O
symbols	O
for	O
either	O
of	O
the	O
two	O
SP	O
networks	O
.	O
Saliency	O
also	O
achieves	O
the	O
worst	O
performance	O
on	O
the	O
ablation	O
test	O
for	O
both	O
the	O
counterbased	O
and	O
the	O
FSA	O
-	O
based	O
SP	O
networks	O
.	O
Among	O
the	O
four	O
white	O
-	O
box	O
networks	O
,	O
the	O
two	O
automatabased	O
networks	O
proved	O
to	O
be	O
much	O
more	O
challenging	O
for	O
the	O
attribution	O
methods	O
than	O
the	O
counterbased	O
networks	O
.	O
While	O
the	O
LRP	O
heatmaps	O
for	O
the	O
PDA	O
network	O
correctly	O
identify	O
the	O
matching	O
bracket	O
when	O
available	O
,	O
no	O
other	O
method	O
produces	O
reasonable	O
heatmaps	O
for	O
the	O
PDA	O
network	O
,	O
and	O
all	O
five	O
methods	O
fail	O
to	O
interpret	O
the	O
FSA	O
network	O
.	O
Taken	O
together	O
,	O
our	O
results	O
suggest	O
that	O
attribution	O
heatmaps	O
should	O
be	O
viewed	O
with	O
skepticism	O
.	O
This	O
paper	O
has	O
identified	O
cases	O
in	O
which	O
heatmaps	O
fail	O
to	O
highlight	O
relevant	O
features	O
,	O
as	O
well	O
as	O
cases	O
in	O
which	O
heatmaps	O
incorrectly	O
highlight	O
irrelevant	O
features	O
.	O
Although	O
most	O
of	O
the	O
methods	O
perform	O
better	O
for	O
the	O
counter	O
-	O
based	O
networks	O
than	O
the	O
automaton	O
-	O
based	O
networks	O
,	O
in	O
practical	O
settings	O
we	O
do	O
not	O
know	O
what	O
kinds	O
of	O
computations	O
are	O
implemented	O
by	O
a	O
trained	O
network	O
,	O
making	O
it	O
impossible	O
to	O
determine	O
whether	O
the	O
network	O
under	O
analysis	O
is	O
compatible	O
with	O
the	O
attribution	O
method	O
being	O
used	O
.	O
In	O
future	O
work	O
,	O
we	O
encourage	O
the	O
use	O
of	O
our	O
four	O
white	O
-	O
box	O
models	O
as	O
qualitative	O
benchmarks	O
for	O
evaluating	O
interpretability	O
methods	O
.	O
For	O
example	O
,	O
the	O
style	O
of	O
evaluation	O
we	O
have	O
developed	O
can	O
be	O
replicated	O
for	O
attribution	O
methods	O
not	O
covered	O
in	O
this	O
paper	O
,	O
including	O
DeepLIFT	O
(	O
Shrikumar	O
et	O
al	O
,	O
2017	O
)	O
and	O
contextual	O
decomposition	O
(	O
Murdoch	O
et	O
al	O
,	O
2018	O
)	O
.	O
We	O
believe	O
that	O
insights	O
gleaned	O
from	O
white	O
-	O
box	O
analysis	O
can	O
help	O
researchers	O
choose	O
between	O
different	O
attribution	O
methods	O
and	O
identify	O
areas	O
of	O
improvement	O
in	O
current	O
techniques	O
.	O

This	O
appendix	O
provides	O
detailed	O
descriptions	O
of	O
our	O
four	O
white	O
-	O
box	O
networks	O
.	O

I	O
would	O
like	O
to	O
thank	O
Dana	O
Angluin	O
and	O
Robert	O
Frank	O
for	O
their	O
advice	O
and	O
mentorship	O
on	O
this	O
project	O
.	O
I	O
would	O
also	O
like	O
to	O
thank	O
Yoav	O
Goldberg	O
,	O
John	O
Lafferty	O
,	O
Tal	O
Linzen	O
,	O
R.	O
Thomas	O
Mc	O
-	O
Coy	O
,	O
Aaron	O
Mueller	O
,	O
Karl	O
Mulligan	O
,	O
Shauli	O
Ravfogel	O
,	O
Jason	O
Shaw	O
,	O
and	O
the	O
reviewers	O
for	O
their	O
helpful	O
feedback	O
and	O
discussion	O
.	O

EDTC	O
:	O
A	O
Corpus	O
for	O
Discourse	O
-	O
Level	O
Topic	O
Chain	O
Parsing	O

Discourse	O
analysis	O
has	O
long	O
been	O
known	O
to	O
be	O
fundamental	O
in	O
natural	O
language	O
processing	O
.	O
In	O
this	O
research	O
,	O
we	O
present	O
our	O
insight	O
on	O
discourse	O
-	O
level	O
topic	O
chain	O
(	O
DTC	O
)	O
parsing	O
which	O
aims	O
at	O
discovering	O
new	O
topics	O
and	O
investigating	O
how	O
these	O
topics	O
evolve	O
over	O
time	O
within	O
an	O
article	O
.	O
To	O
address	O
the	O
lack	O
of	O
data	O
,	O
we	O
contribute	O
a	O
new	O
discourse	O
corpus	O
with	O
DTC	O
-	O
style	O
dependency	O
graphs	O
annotated	O
upon	O
news	O
articles	O
.	O
In	O
particular	O
,	O
we	O
ensure	O
the	O
high	O
reliability	O
of	O
the	O
corpus	O
by	O
utilizing	O
a	O
two	O
-	O
step	O
annotation	O
strategy	O
to	O
build	O
the	O
data	O
and	O
filtering	O
out	O
the	O
annotations	O
with	O
low	O
confidence	O
scores	O
.	O
Based	O
on	O
the	O
annotated	O
corpus	O
,	O
we	O
introduce	O
a	O
simple	O
yet	O
robust	O
system	O
for	O
automatic	O
discourse	O
-	O
level	O
topic	O
chain	O
parsing	O
.	O

[	O
u1	O
]	O
ALBERTA	O
ENERGY	O
Co.	O
,	O
Calgary	O
,	O
said	O
it	O
filed	O
a	O
preliminary	O
prospectus	O
for	O
an	O
offering	O
of	O
common	O
shares	O
.	O
[	O
u2	O
]	O
The	O
natural	O
resources	O
development	O
concern	O
said	O
proceeds	O
will	O
be	O
used	O
to	O
repay	O
long	O
-	O
term	O
debt	O
,	O
which	O
stood	O
at	O
598	O
million	O
Canadian	O
dollars	O
(	O
US$	O
510.6	O
million	O
)	O
at	O
the	O
end	O
of	O
1988	O
.	O
[	O
u3	O
]	O
The	O
company	O
plans	O
to	O
raise	O
between	O
C$	O
75	O
million	O
and	O
C$	O
100	O
million	O
from	O
the	O
offering	O
,	O
according	O
to	O
a	O
spokeswoman	O
at	O
Richardson	O
Greenshields	O
of	O
Canada	O
Ltd.	O
,	O
lead	O
underwriter	O
.	O
[	O
u4	O
]	O
The	O
shares	O
will	O
be	O
priced	O
in	O
early	O
November	O
,	O
she	O
said	O
.	O
wsj_1183	O

The	O
authors	O
would	O
like	O
to	O
thank	O
Yuqing	O
Xing	O
,	O
Jialong	O
Xie	O
,	O
and	O
the	O
other	O
annotators	O
for	O
their	O
valuable	O
discussion	O
and	O
advice	O
on	O
this	O
research	O
.	O
This	O
work	O
was	O
supported	O
by	O
the	O
National	O
Key	O
R&D	O
Program	O
of	O
China	O
under	O
Grant	O
No	O
.	O
2020AAA0108600	O
,	O
Projects	O
61876118	O
and	O
61976146	O
under	O
the	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
and	O
the	O
Priority	O
Academic	O
Program	O
Development	O
of	O
Jiangsu	O
Higher	O
Education	O
Institutions	O
.	O

Referring	O
to	O
our	O
system	O
outputs	O
,	O
we	O
find	O
that	O
the	O
automatically	O
parsed	O
DTC	O
structures	O
are	O
highly	O
consistent	O
with	O
human	O
annotations	O
.	O
Here	O
,	O
we	O
present	O
some	O
automatic	O
DTC	O
structures	O
constructed	O
by	O
the	O
Bert	O
-	O
large	O
-	O
based	O
system	O
for	O
reference	O
.	O

Moody	O
's	O
Investors	O
Service	O
said	O
it	O
reduced	O
its	O
rating	O
on	O
$	O
165	O
million	O
of	O
subordinated	O
debt	O
of	O
this	O
Beverly	O
Hills	O
,	O
Calif.	O
,	O
thrift	O
,	O
citing	O
turmoil	O
in	O
the	O
market	O
for	O
low	O
-	O
grade	O
,	O
high	O
-	O
yield	O
securities	O
.	O
[	O
u2	O
]	O
The	O
agency	O
said	O
it	O
reduced	O
its	O
rating	O
on	O
the	O
thrift	O
's	O
subordinated	O
debt	O
to	O
B	O
-	O
2	O
from	O
Ba	O
-	O
2	O
and	O
will	O
keep	O
the	O
debt	O
under	O
review	O
for	O
possible	O
further	O
downgrade	O
.	O
[	O
u3	O
]	O
Columbia	O
Savings	O
is	O
a	O
major	O
holder	O
of	O
so	O
-	O
called	O
junk	O
bonds	O
.	O
[	O
u4	O
]	O
New	O
federal	O
legislation	O
requires	O
that	O
all	O
thrifts	O
divest	O
themselves	O
of	O
such	O
speculative	O
securities	O
over	O
a	O
period	O
of	O
years	O
.	O
[	O
u5	O
]	O
Columbia	O
Savings	O
officials	O
were	O
n't	O
available	O
for	O
comment	O
on	O
the	O
downgrade	O
.	O
[	O
u6	O
]	O
FRANKLIN	O
SAVINGS	O
ASSOCIA	O
-	O
TION	O
(	O
Ottawa	O
,	O
Kan.	O
)	O
-	O
Moody	O
's	O
Investors	O
Service	O

We	O
apply	O
back	O
-	O
translation	O
(	O
Sennrich	O
et	O
al	O
,	O
2016b	O
)	O
method	O
to	O
use	O
monolingual	O
data	O
.	O
For	O
English	O
-	O
German	O
and	O
Chinese	O
-	O
English	O
translation	O
,	O
we	O
sample	O
monolingual	O
data	O
from	O
the	O
NewsCrawl2016	O
corpora	O
.	O
For	O
English	O
-	O
Chinese	O
translation	O
,	O
we	O
sample	O
monolingual	O
data	O
from	O
the	O
XinhuaNet2011	O
corpus	O
.	O

For	O
Chinese	O
-	O
English	O
translation	O
,	O
we	O
also	O
use	O
a	O
target	O
-	O
bidirectional	O
model	O
(	O
Liu	O
et	O
al	O
,	O
2016	O
;	O
Sennrich	O
et	O
al	O
,	O
2016a	O
)	O
to	O
rescore	O
the	O
hypotheses	O
.	O
To	O
train	O
a	O
target	O
-	O
bidirectional	O
model	O
,	O
we	O
reverse	O
the	O
target	O
side	O
of	O
bilingual	O
pairs	O
from	O
left	O
-	O
to	O
-	O
right	O
(	O
L2R	O
)	O
to	O
right	O
-	O
to	O
-	O
left	O
(	O
R2L	O
)	O
.	O
We	O
first	O
output	O
50	O
candidates	O
from	O
the	O
ensemble	O
of	O
4	O
L2R	O
models	O
.	O
Then	O
we	O
rescore	O
candidates	O
by	O
interpolating	O
L2R	O
score	O
and	O
R2L	O
score	O
with	O
uniform	O
weights	O
.	O
3	O
https://github.com/rsennrich/	O
subword	O
-	O
nmt	O

Table	O
4	O
shows	O
the	O
ranking	O
of	O
our	O
submitted	O
systems	O
at	O
the	O
WMT17	O
shared	O
news	O
translation	O
task	O
.	O
Our	O
submissions	O
are	O
ranked	O
(	O
tied	O
)	O
first	O
for	O
2	O
out	O
of	O
3	O
translation	O
directions	O
in	O
which	O
we	O
participated	O
:	O
EN↔ZH	O
.	O

Or	O
bends	O
with	O
the	O
remover	O
to	O
remove	O
.	O

We	O
can	O
assign	O
any	O
format	O
and	O
rhyming	O
symbols	O
C	O
to	O
control	O
the	O
generation	O
.	O
Given	O
C	O
,	O
we	O
will	O
obtain	O
P	O
and	O
S	O
automatically	O
.	O
And	O
the	O
model	O
can	O
conduct	O
generation	O
starting	O
from	O
the	O
special	O
token	O
bos	O
iteratively	O
until	O
meet	O
the	O
ending	O
marker	O
eos	O
.	O
Both	O
beam	O
-	O
search	O
algorithm	O
(	O
Koehn	O
,	O
2004	O
)	O
and	O
truncated	O
top	O
-	O
k	O
sampling	O
(	O
Fan	O
et	O
al	O
,	O
2018	O
;	O
Radford	O
et	O
al	O
,	O
2019	O
)	O
method	O
are	O
utilized	O
to	O
conduct	O
the	O
decoding	O
.	O
4	O
Experimental	O
Setup	O

We	O
conduct	O
ablation	O
study	O
on	O
corpus	O
SongCi	O
and	O
the	O
experimental	O
results	O
are	O
depicted	O
in	O
Table	O
4	O
.	O
It	O
should	O
note	O
that	O
all	O
the	O
models	O
are	O
purely	O
trained	O
on	O
SongCi	O
corpus	O
without	O
any	O
pre	O
-	O
training	O
stages	O
.	O
From	O
the	O
results	O
we	O
can	O
conclude	O
that	O
the	O
introduced	O
symbols	O
C	O
,	O
P	O
,	O
and	O
S	O
indeed	O
play	O
crucial	O
roles	O
in	O
improving	O
the	O
overall	O
performance	O
especially	O
on	O
the	O
metrics	O
of	O
format	O
,	O
rhyme	O
,	O
and	O
sentence	O
integrity	O
.	O
Even	O
though	O
some	O
of	O
the	O
components	O
can	O
not	O
improve	O
the	O
performance	O
simultaneously	O
on	O
all	O
the	O
metrics	O
,	O
the	O
combination	O
of	O
them	O
can	O
obtain	O
the	O
best	O
performance	O
.	O

A	O
Report	O
on	O
the	O
2020	O
VUA	O
and	O
TOEFL	O
Metaphor	O
Detection	O
Shared	O
Task	O

Metaphor	O
use	O
in	O
everyday	O
language	O
is	O
a	O
way	O
to	O
relate	O
our	O
physical	O
and	O
familiar	O
social	O
experiences	O
to	O
a	O
multitude	O
of	O
other	O
subjects	O
and	O
contexts	O
(	O
Lakoff	O
and	O
Johnson	O
,	O
2008	O
)	O
;	O
it	O
is	O
a	O
fundamental	O
way	O
to	O
structure	O
our	O
understanding	O
of	O
the	O
world	O
even	O
without	O
our	O
conscious	O
realization	O
of	O
its	O
presence	O
as	O
we	O
speak	O
and	O
write	O
.	O
It	O
highlights	O
the	O
unknown	O
using	O
the	O
known	O
,	O
explains	O
the	O
complex	O
using	O
the	O
simple	O
,	O
and	O
helps	O
us	O
to	O
emphasize	O
the	O
relevant	O
aspects	O
of	O
meaning	O
resulting	O
in	O
effective	O
communication	O
.	O
Metaphor	O
has	O
been	O
studied	O
in	O
the	O
context	O
of	O
political	O
communication	O
,	O
marketing	O
,	O
mental	O
health	O
,	O
teaching	O
,	O
assessment	O
of	O
English	O
proficiency	O
,	O
among	O
others	O
Gutierrez	O
et	O
al	O
,	O
2017	O
;	O
Littlemore	O
et	O
al	O
,	O
2013	O
;	O
Thibodeau	O
and	O
Boroditsky	O
,	O
2011	O
;	O
Kaviani	O
and	O
Hamedi	O
,	O
2011	O
;	O
Kathpalia	O
and	O
Carmel	O
,	O
2011	O
;	O
Landau	O
et	O
al	O
,	O
2009	O
;	O
Beigman	O
Klebanov	O
et	O
al	O
,	O
2008	O
;	O
Zaltman	O
and	O
Zaltman	O
,	O
2008	O
;	O
Littlemore	O
and	O
Low	O
,	O
2006	O
;	O
Cameron	O
,	O
2003	O
;	O
Lakoff	O
,	O
2010	O
;	O
Billow	O
et	O
al	O
,	O
1997	O
;	O
Bosman	O
,	O
1987	O
)	O
;	O
see	O
chapter	O
7	O
in	O
Veale	O
et	O
al	O
(	O
2016	O
)	O
for	O
a	O
recent	O
review	O
.	O
We	O
report	O
on	O
the	O
second	O
shared	O
task	O
on	O
automatic	O
metaphor	O
detection	O
,	O
following	O
up	O
on	O
the	O
first	O
shared	O
task	O
held	O
in	O
2018	O
.	O
We	O
present	O
the	O
shared	O
task	O
and	O
provide	O
a	O
brief	O
description	O
of	O
each	O
of	O
the	O
participating	O
systems	O
,	O
a	O
comparative	O
evaluation	O
of	O
the	O
systems	O
,	O
and	O
our	O
observations	O
about	O
trends	O
in	O
designs	O
and	O
performance	O
of	O
the	O
systems	O
that	O
participated	O
in	O
the	O
shared	O
task	O
.	O

We	O
use	O
the	O
VU	O
Amsterdam	O
Metaphor	O
Corpus	O
(	O
VUA	O
)	O
(	O
Steen	O
et	O
al	O
,	O
2010	O
)	O
.	O
The	O
dataset	O
consists	O
of	O
117	O
fragments	O
sampled	O
across	O
four	O
genres	O
from	O
the	O
British	O
National	O
Corpus	O
:	O
Academic	O
,	O
News	O
,	O
Conversation	O
,	O
and	O
Fiction	O
.	O
The	O
data	O
is	O
annotated	O
using	O
the	O
MIPVU	O
procedure	O
with	O
a	O
strong	O
interannotator	O
reliability	O
of	O
κ	O
>	O
0.8	O
(	O
Steen	O
et	O
al	O
,	O
2010	O
)	O
.	O
The	O
VUA	O
dataset	O
and	O
annotations	O
is	O
the	O
same	O
as	O
the	O
one	O
used	O
in	O
the	O
first	O
shared	O
task	O
on	O
metaphor	O
detection	O
,	O
where	O
the	O
reader	O
is	O
referred	O
for	O
further	O
details	O
.	O

This	O
data	O
labeled	O
for	O
metaphor	O
was	O
sampled	O
from	O
the	O
publicly	O
available	O
ETS	O
Corpus	O
of	O
Non	O
-	O
Native	O
Written	O
English	O
1	O
and	O
was	O
first	O
introduced	O
by	O
(	O
Beigman	O
.	O
The	O
annotated	O
data	O
comprises	O
essay	O
responses	O
to	O
eight	O
persuasive	O
/	O
argumentative	O
prompts	O
,	O
for	O
three	O
native	O
languages	O
of	O
the	O
writer	O
(	O
Japanese	O
,	O
Italian	O
,	O
Arabic	O
)	O
,	O
and	O
for	O
two	O
proficiency	O
levels	O
-	O
medium	O
and	O
high	O
.	O
The	O
data	O
was	O
annotated	O
using	O
the	O
protocol	O
in	O
Beigman	O
Klebanov	O
and	O
Flor	O
(	O
2013	O
)	O
,	O
that	O
emphasized	O
argumentation	O
-	O
relevant	O
metaphors	O
:	O
"	O
Argumentation	O
-	O
relevant	O
metaphors	O
are	O
,	O
briefly	O
,	O
those	O
that	O
help	O
the	O
author	O
advance	O
her	O
argument	O
.	O
For	O
example	O
,	O
if	O
you	O
are	O
arguing	O
against	O
some	O
action	O
because	O
it	O
would	O
drain	O
resources	O
,	O
drain	O
1	O
https://catalog.ldc.upenn.edu/LDC2014T06	O
is	O
a	O
metaphor	O
that	O
helps	O
you	O
advance	O
your	O
argument	O
,	O
because	O
it	O
presents	O
the	O
expenditure	O
in	O
a	O
very	O
negative	O
way	O
,	O
suggesting	O
that	O
resources	O
would	O
disappear	O
very	O
quickly	O
and	O
without	O
control	O
.	O
"	O
Beigman	O
Klebanov	O
and	O
Flor	O
(	O
2013	O
)	O
Average	O
inter	O
-	O
annotator	O
agreement	O
was	O
κ	O
=	O
0.56	O
-	O
0.62	O
,	O
for	O
multiple	O
passes	O
of	O
the	O
annotation	O
(	O
see	O
for	O
more	O
details	O
)	O
.	O
We	O
use	O
the	O
data	O
partition	O
from	O
Beigman	O
,	O
with	O
180	O
essays	O
as	O
training	O
data	O
and	O
60	O
essays	O
as	O
testing	O
data	O
.	O
Tables	O
1	O
and	O
2	O
show	O
some	O
descriptive	O
characteristics	O
of	O
the	O
data	O
:	O
the	O
number	O
of	O
texts	O
,	O
sentences	O
,	O
tokens	O
,	O
and	O
class	O
distribution	O
information	O
for	O
Verbs	O
and	O
AllPOS	O
tracks	O
for	O
the	O
two	O
datasets	O
.	O
To	O
facilitate	O
the	O
use	O
of	O
the	O
datasets	O
and	O
evaluation	O
scripts	O
beyond	O
this	O
shared	O
task	O
in	O
future	O
research	O
,	O
the	O
complete	O
set	O
of	O
task	O
instructions	O
and	O
scripts	O
are	O
published	O
on	O
Github	O
2	O
.	O
We	O
also	O
provide	O
a	O
set	O
of	O
features	O
used	O
to	O
construct	O
one	O
of	O
the	O
baseline	O
classification	O
models	O
for	O
prediction	O
of	O
metaphor	O
/	O
non	O
-	O
metaphor	O
classes	O
at	O
the	O
word	O
level	O
,	O
and	O
instructions	O
on	O
how	O
to	O
replicate	O
that	O
baseline	O
.	O

In	O
this	O
first	O
phase	O
,	O
data	O
is	O
released	O
for	O
training	O
and/or	O
development	O
of	O
metaphor	O
detection	O
models	O
.	O
Participants	O
can	O
elect	O
to	O
perform	O
crossvalidation	O
on	O
the	O
training	O
data	O
,	O
or	O
partition	O
the	O
training	O
data	O
further	O
to	O
have	O
a	O
held	O
-	O
out	O
set	O
for	O
preliminary	O
evaluations	O
,	O
and/or	O
set	O
apart	O
a	O
subset	O
of	O
the	O
data	O
for	O
development	O
/	O
tuning	O
of	O
hyperparameters	O
.	O
However	O
the	O
training	O
data	O
is	O
used	O
,	O
the	O
goal	O
is	O
to	O
have	O
N	O
final	O
systems	O
(	O
or	O
versions	O
of	O
a	O
system	O
)	O
ready	O
for	O
evaluation	O
when	O
the	O
test	O
data	O
is	O
released	O
.	O

We	O
first	O
describe	O
the	O
baseline	O
systems	O
.	O
Next	O
,	O
we	O
briefly	O
describe	O
the	O
general	O
approach	O
taken	O
by	O
every	O
team	O
.	O
Interested	O
readers	O
can	O
refer	O
to	O
the	O
teams	O
'	O
papers	O
for	O
more	O
details	O
.	O

Table	O
4	O
present	O
the	O
results	O
for	O
All	O
POS	O
and	O
Verbs	O
tracks	O
for	O
VUA	O
data	O
.	O
Table	O
5	O
present	O
the	O
results	O
for	O
All	O
POS	O
and	O
Verbs	O
tracks	O
for	O
TOEFL	O
data	O
.	O

As	O
organizers	O
of	O
the	O
shared	O
task	O
,	O
we	O
would	O
like	O
to	O
thank	O
all	O
the	O
teams	O
for	O
their	O
interest	O
and	O
participation	O
.	O
We	O
would	O
also	O
like	O
to	O
thank	O
Ton	O
Veale	O
,	O
Eyal	O
Sagi	O
,	O
Debanjan	O
Ghosh	O
,	O
Xinhao	O
Wang	O
,	O
and	O
Keelan	O
Evanini	O
for	O
their	O
helpful	O
comments	O
on	O
the	O
paper	O
,	O
and	O
Verna	O
Dankers	O
for	O
pointing	O
out	O
an	O
error	O
in	O
the	O
original	O
paper	O
that	O
has	O
since	O
been	O
fixed	O
.	O

We	O
now	O
consider	O
the	O
second	O
subtask	O
of	O
INTSUMM	O
:	O
generating	O
lists	O
of	O
suggested	O
queries	O
.	O
The	O
list	O
is	O
regenerated	O
after	O
every	O
interaction	O
,	O
to	O
yield	O
queries	O
that	O
focus	O
on	O
sub	O
-	O
topics	O
that	O
were	O
not	O
yet	O
explored	O
.	O
Reusing	O
the	O
notations	O
of	O
M	O
Summ	O
in	O
3	O
,	O
we	O
define	O
a	O
model	O
,	O
M	O
Sugg	O
,	O
for	O
suggested	O
queries	O
list	O
generation	O
,	O
that	O
receives	O
an	O
input	O
tuple	O
(	O
D	O
,	O
E	O
in	O
,	O
m	O
)	O
(	O
notice	O
that	O
a	O
query	O
is	O
not	O
needed	O
here	O
)	O
.	O
Here	O
,	O
the	O
jth	O
phrase	O
in	O
D	O
is	O
denoted	O
ρ	O
j	O
,	O
when	O
the	O
documents	O
in	O
D	O
are	O
concatenated	O
,	O
and	O
accordingly	O
,	O
history	O
E	O
in	O
is	O
a	O
list	O
of	O
phrases	O
extracted	O
from	O
the	O
session	O
's	O
current	O
accumulated	O
summary	O
.	O
m	O
is	O
the	O
number	O
of	O
suggested	O
queries	O
to	O
output	O
.	O
The	O
model	O
outputs	O
phrase	O
sequence	O
E	O
out	O
=	O
{	O
e	O
out	O
1	O
,	O
e	O
out	O
2	O
,	O
...	O
,	O
e	O
out	O
m	O
}	O
from	O
D	O
,	O
accounting	O
for	O
history	O
E	O
in	O
.	O
As	O
in	O
M	O
Summ	O
's	O
setting	O
,	O
D	O
is	O
paired	O
with	O
a	O
set	O
of	O
generic	O
reference	O
summaries	O
R.	O

We	O
ran	O
several	O
experiments	O
for	O
the	O
assessment	O
of	O
our	O
M	O
Summ	O
and	O
M	O
Sugg	O
models	O
,	O
applying	O
the	O
INTSUMM	O
evaluation	O
framework	O
of	O
Shapira	O
et	O
al	O
(	O
2021b	O
)	O
.	O
The	O
goals	O
of	O
the	O
experiments	O
are	O
to	O
compare	O
varying	O
configurations	O
of	O
our	O
models	O
and	O
to	O
evaluate	O
against	O
an	O
INTSUMM	O
baseline	O
system	O
.	O
The	O
experiments	O
include	O
both	O
simulations	O
and	O
interactive	O
sessions	O
with	O
human	O
users	O
.	O

The	O
M	O
Summ	O
model	O
architecture	O
(	O
3.2	O
)	O
has	O
several	O
configurable	O
components	O
:	O
encoding	O
the	O
query	O
into	O
sentences	O
,	O
considering	O
the	O
query	O
in	O
the	O
MMR	O
function	O
(	O
both	O
at	O
train	O
and	O
inference	O
time	O
)	O
,	O
and	O
the	O
dual	O
reward	O
mechanism	O
.	O
We	O
compared	O
several	O
variations	O
of	O
these	O
using	O
simulations	O
,	O
presented	O
in	O
5.2	O
.	O
In	O
addition	O
,	O
we	O
compare	O
,	O
both	O
via	O
simulations	O
(	O
5.2	O
)	O
and	O
real	O
sessions	O
(	O
5.3	O
)	O
,	O
against	O
the	O
(	O
betterperforming	O
)	O
baseline	O
system	O
in	O
(	O
Shapira	O
et	O
al	O
,	O
2021b	O
)	O
,	O
named	O
S	O
2	O
.	O
S	O
2	O
's	O
initial	O
summary	O
algorithm	O
is	O
TextRank	O
,	O
and	O
the	O
query	O
-	O
response	O
generator	O
extracts	O
sentences	O
via	O
lexical+semantic	O
similarity	O
to	O
the	O
query	O
,	O
somewhat	O
resembling	O
QSIM	O
in	O
Equation	O
10	O
,	O
fully	O
neglecting	O
the	O
summary	O
-	O
so	O
-	O
far	O
,	O
in	O
contrast	O
to	O
M	O
Summ	O
.	O
S	O
2	O
's	O
suggested	O
queries	O
list	O
contains	O
TextRank	O
's	O
top	O
salient	O
topic	O
phrases	O
.	O
Since	O
these	O
too	O
do	O
not	O
account	O
for	O
the	O
summary	O
-	O
so	O
-	O
far	O
,	O
they	O
are	O
computed	O
at	O
the	O
session	O
beginning	O
and	O
are	O
not	O
updated	O
along	O
the	O
session	O
,	O
in	O
contrast	O
to	O
M	O
Sugg	O
.	O

We	O
analyzed	O
the	O
types	O
of	O
queries	O
users	O
submitted	O
throughout	O
their	O
sessions	O
,	O
to	O
assess	O
the	O
utility	O
of	O
updating	O
suggested	O
queries	O
,	O
with	O
M	O
Sugg	O
,	O
as	O
opposed	O
to	O
a	O
static	O
list	O
of	O
suggestions	O
,	O
with	O
S	O
2	O
.	O
To	O
that	O
end	O
,	O
we	O
tallied	O
suggested	O
query	O
clicks	O
and	O
query	O
submissions	O
via	O
other	O
modes	O
,	O
binning	O
the	O
tallies	O
to	O
three	O
sequential	O
temporal	O
segments	O
within	O
their	O
respective	O
sessions	O
(	O
Appendix	O
E.3	O
)	O
.	O
We	O
found	O
that	O
,	O
on	O
average	O
,	O
the	O
usage	O
of	O
suggested	O
query	O
clicks	O
increased	O
by~13	O
%	O
when	O
nearing	O
the	O
end	O
of	O
a	O
session	O
with	O
M	O
Sugg	O
,	O
and	O
conversely	O
decreased	O
by~24	O
%	O
with	O
S	O
2	O
.	O
While	O
the	O
decrease	O
in	O
use	O
of	O
the	O
static	O
list	O
is	O
expected	O
,	O
since	O
appealing	O
queries	O
are	O
likely	O
exhausted	O
earlier	O
in	O
a	O
session	O
,	O
it	O
is	O
encouraging	O
to	O
witness	O
the	O
usefulness	O
of	O
updated	O
queries	O
as	O
the	O
session	O
progresses	O
.	O
This	O
behavior	O
suggests	O
that	O
the	O
updated	O
list	O
contains	O
suggested	O
queries	O
that	O
are	O
indeed	O
engaging	O
for	O
learning	O
more	O
about	O
the	O
topic	O
.	O

Systems	O
that	O
are	O
made	O
for	O
interacting	O
with	O
humans	O
must	O
respond	O
quickly	O
in	O
order	O
to	O
keep	O
the	O
user	O
's	O
engagement	O
.	O
The	O
exact	O
amount	O
of	O
time	O
does	O
not	O
affect	O
the	O
user	O
experience	O
as	O
long	O
as	O
it	O
does	O
not	O
surpass	O
some	O
limit	O
,	O
after	O
which	O
the	O
user	O
starts	O
losing	O
interest	O
or	O
feeling	O
irritated	O
(	O
Attig	O
et	O
al	O
,	O
2017	O
;	O
Anderson	O
,	O
2020	O
)	O
.	O
As	O
mentioned	O
in	O
Appendix	O
D	O
,	O
M	O
Summ	O
generates	O
summaries	O
in	O
under	O
a	O
second	O
and	O
M	O
Sugg	O
prepares	O
the	O
list	O
in	O
a	O
few	O
seconds	O
.	O
The	O
baseline	O
summarizer	O
also	O
responds	O
in	O
under	O
a	O
second	O
.	O
The	O
difference	O
between	O
the	O
systems	O
is	O
virtually	O
unperceivable	O
during	O
interaction	O
.	O
There	O
were	O
no	O
comments	O
from	O
the	O
users	O
in	O
our	O
experiments	O
that	O
stated	O
any	O
issue	O
with	O
execution	O
time	O
.	O
Figure	O
3	O
:	O
Averaged	O
recall	O
curves	O
of	O
our	O
system	O
and	O
the	O
S	O
2	O
baseline	O
system	O
in	O
the	O
experiment	O
described	O
in	O
5.3	O
and	O
Table	O
2	O
(	O
using	O
M	O
Summ	O
configuration	O
v	O
from	O
Table	O
1	O
)	O
.	O
The	O
intersecting	O
range	O
is	O
bounded	O
by	O
dashed	O
lines	O
(	O
between	O
106	O
and	O
250	O
tokens	O
)	O
.	O

In	O
this	O
analysis	O
,	O
we	O
assessed	O
what	O
modes	O
of	O
query	O
submission	O
users	O
relied	O
on	O
over	O
the	O
course	O
of	O
a	O
session	O
.	O
To	O
that	O
end	O
,	O
(	O
1	O
)	O
we	O
divided	O
each	O
session	O
to	O
three	O
segments	O
(	O
first	O
,	O
second	O
and	O
third	O
part	O
of	O
the	O
session	O
)	O
,	O
and	O
counted	O
the	O
types	O
of	O
queries	O
.	O
The	O
types	O
are	O
"	O
suggested	O
query	O
"	O
,	O
"	O
free	O
-	O
text	O
"	O
,	O
"	O
highlight	O
"	O
(	O
a	O
span	O
from	O
the	O
summary	O
text	O
)	O
and	O
"	O
repeat	O
"	O
(	O
repeating	O
the	O
last	O
submitted	O
query	O
)	O
.	O
(	O
2	O
)	O
We	O
then	O
computed	O
the	O
percentage	O
of	O
each	O
mode	O
in	O
each	O
segment	O
.	O
(	O
3	O
)	O
The	O
percentages	O
over	O
all	O
sessions	O
and	O
all	O
topics	O
were	O
computed	O
for	O
each	O
of	O
the	O
three	O
segments	O
.	O
This	O
process	O
was	O
conducted	O
only	O
for	O
sessions	O
between	O
4	O
and	O
20	O
interactions	O
,	O
as	O
the	O
few	O
long	O
and	O
short	O
sessions	O
often	O
show	O
different	O
behavior	O
.	O
For	O
the	O
first	O
experiment	O
,	O
this	O
left	O
43	O
sessions	O
with	O
avg	O
.	O
8.63	O
(	O
std	O
.	O
2.32	O
)	O
interactions	O
for	O
our	O
system	O
,	O
and	O
50	O
sessions	O
with	O
8.44	O
(	O
2.48	O
)	O
interaction	O
for	O
S	O
2	O
.	O
For	O
the	O
second	O
experiment	O
,	O
it	O
left	O
72	O
sessions	O
with	O
10.24	O
(	O
4.82	O
)	O
interactions	O
for	O
our	O
system	O
,	O
and	O
74	O
sessions	O
with	O
9.59	O
(	O
4.42	O
)	O
interactions	O
for	O
S	O
2	O
.	O
We	O
focus	O
here	O
on	O
the	O
use	O
of	O
suggested	O
queries	O
versus	O
all	O
other	O
query	O
types	O
.	O
In	O
the	O
first	O
experiment	O
we	O
observe	O
a	O
change	O
of	O
+9	O
%	O
from	O
the	O
first	O
to	O
the	O
third	O
segment	O
in	O
our	O
system	O
,	O
and	O
-	O
20	O
%	O
in	O
S	O
2	O
.	O
In	O
the	O
second	O
experiment	O
we	O
see	O
+18	O
%	O
and	O
-	O
28	O
%	O
in	O
S	O
2	O
.	O
As	O
discussed	O
in	O
5.3	O
,	O
this	O
suggests	O
the	O
effectiveness	O
of	O
updated	O
suggested	O
queries	O
,	O
especially	O
by	O
the	O
end	O
of	O
a	O
session	O
.	O
Figure	O
4	O
:	O
Averaged	O
recall	O
curves	O
of	O
our	O
system	O
and	O
the	O
S	O
2	O
baseline	O
system	O
in	O
the	O
experiment	O
described	O
here	O
in	O
Appendix	O
E.1	O
and	O
Table	O
3	O
(	O
using	O
M	O
Summ	O
configuration	O
i	O
from	O
Table	O
1	O
)	O
.	O
The	O
intersecting	O
range	O
is	O
bounded	O
by	O
dashed	O
lines	O
(	O
between	O
106	O
and	O
250	O
tokens	O
)	O
.	O

We	O
show	O
in	O
Figure	O
5	O
an	O
example	O
of	O
an	O
INTSUMM	O
system	O
using	O
the	O
web	O
application	O
of	O
Shapira	O
et	O
al	O
(	O
2021b	O
)	O
and	O
our	O
our	O
M	O
Summ	O
(	O
configuration	O
i	O
from	O
Table	O
1	O
)	O
and	O
M	O
Sugg	O
models	O
in	O
the	O
backend	O
.	O
shows	O
the	O
result	O
of	O
clicking	O
the	O
"	O
carbon	O
dioxide	O
gas	O
"	O
suggested	O
query	O
(	O
with	O
the	O
query	O
response	O
and	O
updated	O
suggested	O
queries	O
list	O
)	O
.	O
Sub	O
-	O
figure	O
(	O
c	O
)	O
shows	O
the	O
result	O
of	O
subsequently	O
submitting	O
the	O
query	O
"	O
water	O
level	O
"	O
.	O
Query	O
responses	O
should	O
be	O
informative	O
for	O
the	O
general	O
topic	O
,	O
while	O
also	O
complying	O
to	O
the	O
user	O
queries	O
.	O
System	O
summaries	O
and	O
expansions	O
must	O
be	O
output	O
fast	O
in	O
order	O
to	O
allow	O
smooth	O
interaction	O
and	O
human	O
engagement	O
.	O

We	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
constructive	O
comments	O
and	O
suggestions	O
.	O
This	O
work	O
was	O
supported	O
in	O
part	O
by	O
Intel	O
Labs	O
;	O
by	O
the	O
Israel	O
Science	O
Foundation	O
(	O
grants	O
no	O
.	O
2827/21	O
and	O
2015/21	O
)	O
;	O
by	O
a	O
grant	O
from	O
the	O
Israel	O
Ministry	O
of	O
Science	O
and	O
Technology	O
;	O
by	O
the	O
NSF	O
-	O
CAREER	O
Award	O
#	O
1846185	O
;	O
and	O
by	O
a	O
Microsoft	O
PhD	O
Fellowship	O
.	O

Interpreting	O
the	O
Robustness	O
of	O
Neural	O
NLP	O
Models	O
to	O
Textual	O
Perturbations	O

Natural	O
noise	O
(	O
simulated	O
by	O
perturbations	O
in	O
this	O
work	O
)	O
usually	O
co	O
-	O
occurs	O
with	O
latent	O
features	O
in	O
an	O
example	O
.	O
If	O
we	O
did	O
not	O
assign	O
random	O
labels	O
and	O
simply	O
perturbed	O
one	O
of	O
the	O
original	O
groups	O
,	O
there	O
would	O
be	O
confounding	O
latent	O
features	O
that	O
would	O
prevent	O
us	O
from	O
estimating	O
the	O
causal	O
effect	O
of	O
the	O
perturbation	O
.	O
Figure	O
1a	O
illustrates	O
this	O
scenario	O
.	O
Both	O
perturbation	O
P	O
and	O
latent	O
feature	O
T	O
may	O
affect	O
the	O
outcome	O
Y	O
,	O
2	O
while	O
the	O
latent	O
feature	O
is	O
predictive	O
of	O
label	O
L.	O
Since	O
we	O
make	O
the	O
perturbation	O
P	O
on	O
examples	O
with	O
the	O
same	O
label	O
,	O
P	O
is	O
decided	O
by	O
L.	O
It	O
therefore	O
follows	O
that	O
T	O
is	O
a	O
confounder	O
of	O
the	O
effect	O
of	O
P	O
on	O
Y	O
,	O
resulting	O
in	O
non	O
-	O
causal	O
association	O
flowing	O
along	O
the	O
path	O
P	O
L	O
T	O
Y	O
.	O
However	O
,	O
if	O
we	O
do	O
randomize	O
the	O
labels	O
,	O
P	O
no	O
longer	O
has	O
any	O
causal	O
parents	O
(	O
i.e.	O
,	O
incoming	O
edges	O
)	O
(	O
Figure	O
1b	O
)	O
.	O
This	O
is	O
because	O
perturbation	O
is	O
purely	O
2	O
Y	O
is	O
later	O
defined	O
in	O
Section	O
3.2	O
the	O
language	O
of	O
causality	O
,	O
this	O
is	O
"	O
correlation	O
is	O
not	O
causation	O
"	O
.	O
Causality	O
provides	O
insight	O
on	O
how	O
to	O
fully	O
decouple	O
the	O
effect	O
of	O
perturbation	O
and	O
other	O
latent	O
features	O
.	O
We	O
introduce	O
the	O
causal	O
motivations	O
for	O
step	O
1	O
and	O
3	O
of	O
learnability	O
estimation	O
in	O
the	O
following	O
Section	O
3.1	O
and	O
3.2	O
respectively	O
.	O

Natural	O
noise	O
(	O
simulated	O
by	O
perturbations	O
in	O
this	O
work	O
)	O
usually	O
co	O
-	O
occurs	O
with	O
latent	O
features	O
in	O
an	O
example	O
.	O
If	O
we	O
did	O
not	O
assign	O
random	O
labels	O
and	O
simply	O
perturbed	O
one	O
of	O
the	O
original	O
groups	O
,	O
there	O
would	O
be	O
confounding	O
latent	O
features	O
that	O
would	O
prevent	O
us	O
from	O
estimating	O
the	O
causal	O
effect	O
of	O
the	O
perturbation	O
.	O
Figure	O
4a	O
illustrates	O
this	O
scenario	O
.	O
Both	O
perturbation	O
P	O
and	O
latent	O
feature	O
T	O
may	O
affect	O
the	O
outcome	O
Y	O
,	O
3	O
while	O
the	O
latent	O
feature	O
is	O
predictive	O
of	O
label	O
L.	O
Since	O
we	O
make	O
perturbation	O
P	O
on	O
examples	O
with	O
the	O
same	O
label	O
,	O
P	O
is	O
decided	O
by	O
L.	O
It	O
therefore	O
follows	O
that	O
T	O
is	O
a	O
confounder	O
of	O
the	O
effect	O
of	O
P	O
on	O
Y	O
,	O
resulting	O
in	O
non	O
-	O
causal	O
association	O
flowing	O
along	O
the	O
path	O
P	O
L	O
T	O
Y	O
.	O
However	O
,	O
if	O
we	O
do	O
randomize	O
the	O
labels	O
,	O
P	O
no	O
longer	O
has	O
any	O
causal	O
parents	O
(	O
i.e.	O
,	O
incoming	O
edges	O
)	O
(	O
Figure	O
4b	O
)	O
.	O
This	O
is	O
because	O
perturbation	O
is	O
purely	O
random	O
.	O
Without	O
the	O
path	O
represented	O
by	O
P	O
L	O
,	O
all	O
of	O
the	O
association	O
that	O
flows	O
from	O
P	O
to	O
Y	O
is	O
causal	O
.	O
As	O
a	O
result	O
,	O
we	O
can	O
directly	O
calculate	O
the	O
causal	O
effect	O
from	O
the	O
observed	O
outcomes	O
(	O
Section	O
3.2	O
)	O
.	O
Our	O
randomization	O
experiments	O
allow	O
us	O
to	O
dis	O
-	O
of	O
the	O
association	O
that	O
flows	O
from	O
P	O
to	O
Y	O
is	O
causal	O
.	O

As	O
a	O
result	O
,	O
we	O
can	O
directly	O
calculate	O
the	O
causal	O
315	O
effect	O
from	O
the	O
observed	O
outcomes	O
(	O
Section	O
3.2	O
)	O
.	O

Our	O
randomization	O
experiments	O
allow	O
us	O
to	O
dis	O
-	O
Figure	O
1	O
:	O
Causal	O
graph	O
explanation	O
for	O
decoupling	O
perturbation	O
and	O
latent	O
feature	O
with	O
randomization	O
.	O
P	O
is	O
the	O
perturbation	O
and	O
T	O
is	O
the	O
latent	O
feature	O
.	O
L	O
is	O
the	O
original	O
label	O
and	O
Y	O
is	O
the	O
correctness	O
of	O
the	O
predicted	O
label	O
.	O
random	O
.	O
Without	O
the	O
path	O
represented	O
by	O
P	O
L	O
,	O
all	O
of	O
the	O
association	O
that	O
flows	O
from	O
P	O
to	O
Y	O
is	O
causal	O
.	O
As	O
a	O
result	O
,	O
we	O
can	O
directly	O
calculate	O
the	O
causal	O
effect	O
from	O
the	O
observed	O
outcomes	O
.	O

Criteria	O
for	O
Perturbations	O
.	O
We	O
select	O
various	O
character	O
-	O
level	O
and	O
word	O
-	O
level	O
perturbation	O
methods	O
in	O
existing	O
literature	O
that	O
simulate	O
different	O
types	O
of	O
noise	O
an	O
NLP	O
model	O
may	O
encounter	O
in	O
real	O
-	O
world	O
situations	O
.	O
These	O
perturbations	O
are	O
nonadversarial	O
,	O
label	O
-	O
consistent	O
,	O
and	O
can	O
be	O
automatically	O
generated	O
at	O
scale	O
.	O
We	O
note	O
that	O
our	O
perturbations	O
do	O
not	O
require	O
access	O
to	O
the	O
model	O
internal	O
structure	O
.	O
We	O
also	O
assume	O
that	O
the	O
feature	O
of	O
perturbation	O
does	O
not	O
exist	O
in	O
the	O
original	O
data	O
.	O
Not	O
all	O
perturbations	O
in	O
the	O
existing	O
literature	O
are	O
suitable	O
for	O
our	O
task	O
.	O
For	O
example	O
,	O
a	O
perturbation	O
that	O
swaps	O
gender	O
words	O
(	O
i.e.	O
,	O
female	O
male	O
,	O
male	O
female	O
)	O
is	O
not	O
suitable	O
for	O
our	O
experiments	O
since	O
we	O
can	O
not	O
distinguish	O
the	O
perturbed	O
text	O
from	O
an	O
unperturbed	O
one	O
.	O
In	O
other	O
words	O
,	O
the	O
perturbation	O
function	O
g	O
(	O
⋅	O
)	O
should	O
be	O
asymmetric	O
,	O
such	O
that	O
g	O
(	O
g	O
(	O
x	O
)	O
)	O
≠	O
x.	O
Figure	O
2	O
shows	O
an	O
example	O
sentence	O
with	O
different	O
perturbations	O
.	O
Perturbation	O
of	O
"	O
dupli	O
-	O
cate_punctuation	O
"	O
doubles	O
the	O
punctuation	O
by	O
appending	O
a	O
duplicate	O
after	O
each	O
punctuation	O
,	O
e.g.	O
,	O
"	O
,	O
"	O
"	O
"	O
"	O
;	O
"	O
butter_fingers_perturbation	O
"	O
misspells	O
some	O
words	O
with	O
noise	O
erupting	O
from	O
keyboard	O
typos	O
;	O
"	O
shuffle_word	O
"	O
randomly	O
changes	O
the	O
order	O
of	O
word	O
in	O
the	O
text	O
(	O
Moradi	O
and	O
Samwald	O
,	O
2021	O
)	O
;	O
"	O
random_upper_transformation	O
"	O
randomly	O
adds	O
upper	O
cased	O
letters	O
(	O
Wei	O
and	O
Zou	O
,	O
2019	O
)	O
;	O
"	O
in	O
-	O
sert_abbreviation	O
"	O
implements	O
a	O
rule	O
system	O
that	O
encodes	O
word	O
sequences	O
associated	O
with	O
the	O
replaced	O
abbreviations	O
;	O
"	O
whitespace_perturbation	O
"	O
randomly	O
removes	O
or	O
adds	O
whitespaces	O
to	O
text	O
;	O
"	O
vi	O
-	O
sual_attack_letters	O
"	O
replaces	O
letters	O
with	O
visually	O
similar	O
,	O
but	O
different	O
,	O
letters	O
(	O
Eger	O
et	O
al	O
,	O
2019	O
)	O
;	O
"	O
leet_letters	O
"	O
replaces	O
letters	O
with	O
leet	O
,	O
a	O
common	O
encoding	O
used	O
in	O
gaming	O
(	O
Eger	O
et	O
al	O
,	O
2019	O
)	O
.	O

This	O
work	O
targets	O
at	O
an	O
open	O
question	O
in	O
NLP	O
:	O
why	O
models	O
are	O
less	O
robust	O
to	O
some	O
textual	O
perturbations	O
than	O
others	O
?	O
We	O
find	O
that	O
learnability	O
,	O
which	O
causally	O
quantifies	O
how	O
well	O
a	O
model	O
learns	O
to	O
identify	O
a	O
perturbation	O
,	O
is	O
predictive	O
of	O
the	O
model	O
robustness	O
to	O
the	O
perturbation	O
.	O
In	O
future	O
work	O
,	O
we	O
will	O
investigate	O
whether	O
these	O
findings	O
can	O
generalize	O
to	O
other	O
domains	O
,	O
including	O
computer	O
vision	O
.	O

Computing	O
average	O
learnability	O
requires	O
training	O
a	O
model	O
for	O
multiple	O
times	O
at	O
different	O
perturbation	O
probabilities	O
,	O
which	O
can	O
be	O
computationally	O
intensive	O
if	O
the	O
sizes	O
of	O
the	O
datasets	O
and	O
models	O
are	O
large	O
.	O
This	O
can	O
be	O
a	O
non	O
-	O
trivial	O
problem	O
for	O
NLP	O
practitioners	O
with	O
limited	O
computational	O
resources	O
.	O
We	O
hope	O
that	O
our	O
benchmark	O
results	O
of	O
typical	O
perturbations	O
for	O
NLP	O
models	O
work	O
as	O
a	O
reference	O
for	O
potential	O
users	O
.	O
Collaboratively	O
sharing	O
the	O
results	O
of	O
such	O
metrics	O
on	O
popular	O
models	O
and	O
perturbations	O
in	O
public	O
fora	O
can	O
also	O
help	O
reduce	O
duplicate	O
investigation	O
and	O
coordinate	O
efforts	O
across	O
teams	O
.	O
To	O
alleviate	O
the	O
computational	O
efficiency	O
issue	O
of	O
average	O
learnability	O
estimation	O
,	O
using	O
learnability	O
at	O
selected	O
perturbation	O
probabilities	O
may	O
help	O
at	O
the	O
cost	O
of	O
reduced	O
precision	O
(	O
Appendix	O
D	O
)	O
.	O
We	O
are	O
not	O
alone	O
in	O
facing	O
this	O
issue	O
:	O
two	O
similar	O
metrics	O
for	O
interpreting	O
model	O
inductive	O
bias	O
,	O
extractability	O
and	O
s	O
-	O
only	O
error	O
)	O
also	O
require	O
training	O
the	O
model	O
repeatedly	O
over	O
the	O
whole	O
dataset	O
.	O
Therefore	O
,	O
finding	O
an	O
efficient	O
proxy	O
for	O
average	O
learnability	O
is	O
promising	O
for	O
more	O
practical	O
use	O
of	O
learnability	O
in	O
model	O
interpretation	O
.	O

This	O
research	O
is	O
supported	O
by	O
the	O
National	O
Research	O
Foundation	O
,	O
Singapore	O
under	O
its	O
International	O
Research	O
Centres	O
in	O
Singapore	O
Funding	O
Initiative	O
.	O
Any	O
opinions	O
,	O
findings	O
and	O
conclusions	O
or	O
recommendations	O
expressed	O
in	O
this	O
material	O
are	O
those	O
of	O
the	O
author	O
(	O
s	O
)	O
and	O
do	O
not	O
reflect	O
the	O
views	O
of	O
National	O
Research	O
Foundation	O
,	O
Singapore	O
.	O
We	O
acknowledge	O
the	O
support	O
of	O
NVIDIA	O
Corporation	O
for	O
their	O
donation	O
of	O
the	O
GeForce	O
RTX	O
3090	O
GPU	O
that	O
facilitated	O
this	O
research	O
.	O

In	O
order	O
to	O
enable	O
the	O
model	O
to	O
learn	O
from	O
explicit	O
hard	O
negatives	O
,	O
we	O
construct	O
three	O
diverse	O
types	O
of	O
graphs	O
-	O
synthetically	O
constructed	O
structural	O
negatives	O
for	O
learning	O
graph	O
constraints	O
and	O
synthetic	O
and	O
human	O
-	O
created	O
semantic	O
negatives	O
to	O
capture	O
a	O
fairly	O
large	O
space	O
of	O
semantically	O
incorrect	O
graphs	O
.	O
Below	O
we	O
discuss	O
the	O
construction	O
of	O
these	O
graphs	O
.	O

As	O
shown	O
previously	O
,	O
one	O
common	O
source	O
of	O
errors	O
in	O
the	O
generated	O
explanation	O
graphs	O
is	O
the	O
violation	O
of	O
structural	O
constraints	O
.	O
To	O
enable	O
learning	O
these	O
constraints	O
,	O
we	O
generate	O
four	O
types	O
of	O
negative	O
graphs	O
by	O
performing	O
the	O
following	O
perturbations	O
on	O
each	O
ground	O
-	O
truth	O
graph	O
:	O
(	O
1	O
)	O
removing	O
an	O
edge	O
at	O
random	O
such	O
that	O
the	O
resultant	O
graph	O
becomes	O
disconnected	O
,	O
(	O
2	O
)	O
adding	O
an	O
edge	O
between	O
two	O
randomly	O
chosen	O
nodes	O
such	O
that	O
the	O
resultant	O
graph	O
becomes	O
cyclic	O
,	O
(	O
3	O
)	O
adding	O
and	O
removing	O
one	O
edge	O
at	O
random	O
such	O
that	O
the	O
resultant	O
graph	O
becomes	O
both	O
disconnected	O
and	O
cyclic	O
,	O
(	O
4	O
)	O
removing	O
a	O
node	O
randomly	O
such	O
that	O
the	O
resultant	O
graph	O
contains	O
less	O
than	O
two	O
concepts	O
from	O
the	O
belief	O
or	O
argument	O
.	O
Fig	O
.	O
2	O
shows	O
an	O
example	O
of	O
a	O
disconnected	O
graph	O
created	O
as	O
part	O
of	O
the	O
structurally	O
negative	O
graphs	O
.	O

Generate	O
&	O
Refine	O
model	O
(	O
Sec	O
.	O
5.3	O
)	O
improves	O
all	O
metrics	O
;	O
however	O
the	O
gains	O
are	O
small	O
.	O
Note	O
that	O
this	O
model	O
refines	O
all	O
graphs	O
(	O
correct	O
or	O
not	O
)	O
and	O
can	O
lead	O
to	O
already	O
correct	O
graphs	O
becoming	O
incorrect	O
after	O
refinement	O
.	O
In	O
practice	O
,	O
we	O
observe	O
that	O
most	O
graphs	O
do	O
not	O
change	O
much	O
after	O
refinement	O
which	O
we	O
believe	O
stems	O
from	O
the	O
model	O
's	O
inability	O
to	O
distinguish	O
between	O
correct	O
and	O
incorrect	O
graphs	O
.	O
graphs	O
.	O
This	O
can	O
potentially	O
be	O
improved	O
by	O
incorporating	O
more	O
structurally	O
diverse	O
graphs	O
.	O
Finally	O
,	O
our	O
best	O
SeCA	O
is	O
far	O
from	O
perfect	O
and	O
significant	O
future	O
work	O
can	O
be	O
done	O
in	O
improving	O
the	O
graph	O
semantics	O
.	O
Further	O
ablations	O
of	O
negative	O
graphs	O
and	O
human	O
evaluation	O
are	O
done	O
on	O
the	O
Max	O
-	O
Margin	O
model	O
,	O
due	O
to	O
its	O
slightly	O
higher	O
SeCA	O
.	O

We	O
create	O
a	O
total	O
of	O
11k	O
negative	O
graphs	O
.	O
Table	O
6	O
shows	O
the	O
respective	O
counts	O
of	O
the	O
negative	O
graphs	O
belonging	O
to	O
synthetic	O
structural	O
(	O
SySt	O
)	O
,	O
synthetic	O
semantic	O
(	O
SySe	O
)	O
and	O
human	O
-	O
created	O
semantic	O
(	O
HuSe	O
)	O
categories	O
.	O

In	O
Fig	O
.	O
5	O
,	O
we	O
show	O
the	O
interface	O
for	O
human	O
verification	O
of	O
commonsense	O
explanation	O
graphs	O
on	O
Amazon	O
Mechanical	O
Turk	O
.	O
We	O
select	O
crowdworkers	O
who	O
are	O
located	O
in	O
the	O
US	O
with	O
a	O
HIT	O
approval	O
rate	O
higher	O
than	O
96	O
%	O
and	O
at	O
least	O
1000	O
HITs	O
approved	O
.	O
Since	O
graph	O
evaluation	O
is	O
a	O
challenging	O
task	O
,	O
we	O
first	O
explain	O
how	O
to	O
read	O
the	O
graphs	O
and	O
also	O
provide	O
clear	O
guidelines	O
for	O
comparing	O
the	O
quality	O
of	O
the	O
two	O
graphs	O
.	O
9	O

We	O
use	O
all	O
publicly	O
available	O
literature	O
from	O
PubMed	O
and	O
PubMed	O
Central	O
Open	O
Access	O
subset	O
,	O
which	O
cover	O
most	O
of	O
the	O
relevant	O
literature	O
and	O
are	O
commonly	O
used	O
as	O
the	O
prime	O
source	O
of	O
data	O
in	O
biomedical	O
text	O
mining	O
knowledge	O
bases	O
.	O
PubMed	O
provides	O
titles	O
and	O
abstracts	O
in	O
XML	O
format	O
in	O
a	O
collection	O
of	O
baseline	O
release	O
and	O
subsequent	O
updates	O
.	O
The	O
former	O
is	O
available	O
at	O
the	O
end	O
of	O
each	O
year	O
whereas	O
the	O
latter	O
is	O
updated	O
daily	O
.	O
As	O
this	O
project	O
was	O
started	O
during	O
2015	O
,	O
we	O
have	O
first	O
processed	O
the	O
baseline	O
release	O
from	O
the	O
end	O
of	O
2014	O
and	O
this	O
data	O
has	O
then	O
been	O
extended	O
with	O
the	O
new	O
publications	O
from	O
the	O
end	O
of	O
2015	O
baseline	O
release	O
.	O
The	O
rest	O
of	O
the	O
data	O
up	O
to	O
date	O
has	O
been	O
collected	O
from	O
the	O
daily	O
updates	O
.	O
The	O
full	O
articles	O
in	O
PMC	O
Open	O
Access	O
subset	O
(	O
PMCOA	O
)	O
are	O
retrieved	O
via	O
the	O
PMC	O
FTP	O
service	O
.	O
Multiple	O
types	O
of	O
data	O
format	O
are	O
provided	O
in	O
PM	O
-	O
COA	O
,	O
including	O
NXML	O
and	O
TXT	O
formats	O
which	O
are	O
suitable	O
for	O
text	O
processing	O
.	O
We	O
use	O
the	O
provided	O
NXML	O
format	O
as	O
it	O
is	O
compatible	O
with	O
our	O
processing	O
pipeline	O
.	O
This	O
service	O
does	O
not	O
provide	O
distinct	O
incremental	O
updates	O
,	O
but	O
a	O
list	O
of	O
all	O
indexed	O
articles	O
updated	O
weekly	O
.	O

We	O
have	O
introduced	O
a	O
new	O
resource	O
which	O
provides	O
the	O
basic	O
linguistic	O
analyses	O
,	O
essential	O
in	O
the	O
development	O
of	O
text	O
mining	O
knowledge	O
bases	O
,	O
for	O
the	O
whole	O
of	O
PubMed	O
and	O
PubMed	O
Central	O
Open	O
Access	O
section	O
,	O
thus	O
drastically	O
reducing	O
the	O
amount	O
of	O
required	O
preprocessing	O
efforts	O
.	O
In	O
addition	O
,	O
we	O
provide	O
named	O
entity	O
tagging	O
for	O
several	O
biologically	O
relevant	O
entity	O
types	O
and	O
show	O
that	O
the	O
models	O
we	O
have	O
used	O
are	O
comparable	O
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
approaches	O
,	O
although	O
our	O
focus	O
has	O
been	O
on	O
retaining	O
the	O
processing	O
pipeline	O
as	O
simple	O
as	O
possible	O
for	O
easier	O
maintenance	O
.	O
The	O
resource	O
is	O
periodically	O
updated	O
with	O
an	O
automated	O
pipeline	O
,	O
and	O
currently	O
includes	O
over	O
26	O
M	O
documents	O
fully	O
parsed	O
with	O
526	O
M	O
named	O
entity	O
mentions	O
detected	O
.	O
The	O
data	O
is	O
available	O
for	O
download	O
in	O
XML	O
format	O
.	O

Computational	O
resources	O
were	O
provided	O
by	O
CSC	O
-	O
IT	O
Center	O
For	O
Science	O
Ltd.	O
,	O
Espoo	O
,	O
Finland	O
.	O
This	O
work	O
was	O
supported	O
by	O
ATT	O
Tieto	O
käyttöön	O
grant	O
.	O

Among	O
the	O
previous	O
series	O
(	O
2011	O
,	O
2013	O
,	O
2016	O
)	O
of	O
the	O
BioNLP	O
Shared	O
Task	O
,	O
the	O
Bacteria	O
Biotope	O
Task	O
in	O
2013	O
is	O
the	O
first	O
shared	O
task	O
that	O
addressed	O
the	O
problem	O
of	O
normalization	O
of	O
the	O
entities	O
in	O
the	O
bacteria	O
biotopes	O
domain	O
.	O
In	O
2013	O
,	O
the	O
participant	O
teams	O
proposed	O
rule	O
-	O
based	O
methods	O
and	O
similarity	O
-	O
based	O
methods	O
.	O
According	O
to	O
the	O
official	O
results	O
of	O
the	O
Bacteria	O
Biotope	O
Task	O
of	O
2013	O
,	O
for	O
the	O
habitat	O
mention	O
normalization	O
,	O
the	O
best	O
precision	O
was	O
obtained	O
by	O
the	O
BOUN	O
system	O
,	O
which	O
utilized	O
syntactic	O
rules	O
and	O
shallow	O
linguistic	O
knowledge	O
(	O
Karadeniz	O
and	O
Ozgür	O
,	O
2013	O
;	O
Karadeniz	O
andÖzgür	O
,	O
2015	O
)	O
.	O
In	O
the	O
following	O
series	O
of	O
the	O
Bacteria	O
Biotopes	O
task	O
,	O
the	O
habitat	O
mention	O
normalization	O
sub	O
-	O
task	O
continued	O
to	O
attract	O
the	O
attention	O
of	O
the	O
researchers	O
.	O
In	O
the	O
Bacteria	O
Biotope	O
task	O
of	O
the	O
BioNLP	O
Shared	O
Task	O
2016	O
,	O
the	O
best	O
precision	O
for	O
the	O
habitat	O
normalization	O
task	O
was	O
obtained	O
by	O
the	O
BOUN	O
system	O
,	O
which	O
utilized	O
both	O
approximate	O
string	O
matching	O
and	O
cosine	O
similarity	O
of	O
word	O
-	O
vectors	O
weighted	O
with	O
Term	O
Frequency	O
-	O
Inverse	O
Document	O
Frequency	O
(	O
TF	O
-	O
IDF	O
)	O
(	O
Tiftikci	O
et	O
al	O
,	O
2016	O
)	O
.	O
After	O
the	O
Shared	O
Tasks	O
,	O
the	O
researchers	O
continued	O
to	O
search	O
for	O
a	O
solution	O
for	O
the	O
problem	O
of	O
Bacteria	O
Biotopes	O
normalization	O
(	O
Ferré	O
et	O
al	O
,	O
2017	O
;	O
Mehryary	O
et	O
al	O
,	O
2017	O
;	O
Karadeniz	O
andÖzgür	O
,	O
2019	O
)	O
.	O
Although	O
promising	O
results	O
have	O
been	O
obtained	O
by	O
these	O
approaches	O
,	O
the	O
results	O
showed	O
that	O
there	O
is	O
still	O
room	O
for	O
improvement	O
for	O
the	O
normalization	O
task	O
of	O
bacteria	O
biotopes	O
.	O
Besides	O
the	O
bacteria	O
biotopes	O
,	O
there	O
exist	O
a	O
significant	O
amount	O
of	O
prior	O
work	O
on	O
biomedical	O
named	O
entity	O
normalization	O
for	O
different	O
types	O
of	O
biomedical	O
entities	O
including	O
genes	O
/	O
proteins	O
(	O
Morgan	O
et	O
al	O
,	O
2008	O
;	O
Wermter	O
et	O
al	O
,	O
2009	O
;	O
Lu	O
et	O
al	O
,	O
2011	O
;	O
and	O
diseases	O
(	O
Leaman	O
et	O
al	O
,	O
2013	O
;	O
Li	O
et	O
al	O
,	O
2017	O
)	O
.	O
However	O
,	O
the	O
need	O
for	O
manually	O
annotated	O
training	O
data	O
makes	O
the	O
adaptation	O
of	O
such	O
methods	O
to	O
new	O
entities	O
difficult	O
.	O

(	O
w	O
*	O
S	O
S	O
(	O
m	O
head	O
,	O
c	O
head	O
)	O
)	O
+	O
(	O
(	O
1	O
-	O
w	O
)	O
*	O
S	O
S	O
(	O
m	O
,	O
c	O
)	O
)	O
(	O
1	O
)	O

Similar	O
to	O
the	O
extraction	O
of	O
localization	O
relations	O
,	O
for	O
the	O
extraction	O
of	O
Exhibits	O
relations	O
,	O
all	O
the	O
sentences	O
are	O
searched	O
for	O
whether	O
there	O
exist	O
a	O
Microorganism	O
entity	O
and	O
a	O
Phenotype	O
entity	O
.	O
The	O
same	O
rules	O
that	O
are	O
explained	O
in	O
the	O
previous	O
subsection	O
are	O
applied	O
for	O
the	O
extraction	O
of	O
the	O
Exhibits	O
relations	O
.	O

The	O
official	O
results	O
obtained	O
by	O
our	O
system	O
and	O
the	O
other	O
participants	O
for	O
the	O
BB	O
-	O
rel	O
task	O
are	O
demonstrated	O
in	O
Table	O
5	O
.	O

We	O
would	O
like	O
to	O
thank	O
the	O
BioNLP	O
shared	O
task	O
organizers	O
,	O
especially	O
,	O
Robert	O
Bossy	O
for	O
their	O
help	O
with	O
the	O
questions	O
.	O

Our	O
learner	O
trace	O
data	O
comes	O
from	O
Duolingo	O
:	O
a	O
free	O
,	O
award	O
-	O
winning	O
,	O
online	O
language	O
-	O
learning	O
platform	O
.	O
Since	O
launching	O
in	O
2012	O
,	O
more	O
than	O
200	O
million	O
learners	O
worldwide	O
have	O
enrolled	O
in	O
Duolingo	O
's	O
game	O
-	O
like	O
courses	O
,	O
either	O
via	O
the	O
website	O
1	O
or	O
mobile	O
apps	O
.	O
Figure	O
1	O
(	O
a	O
)	O
is	O
a	O
screen	O
-	O
shot	O
of	O
the	O
home	O
screen	O
,	O
which	O
specifies	O
the	O
game	O
-	O
like	O
curriculum	O
.	O
Each	O
icon	O
represents	O
a	O
skill	O
,	O
aimed	O
at	O
teaching	O
thematically	O
or	O
grammatically	O
grouped	O
words	O
or	O
concepts	O
.	O
Learners	O
can	O
tap	O
an	O
icon	O
to	O
access	O
lessons	O
of	O
new	O
material	O
,	O
or	O
to	O
review	O
material	O
once	O
all	O
lessons	O
are	O
completed	O
.	O
Learners	O
can	O
also	O
choose	O
to	O
get	O
a	O
personalized	O
practice	O
session	O
that	O
reviews	O
previouslylearned	O
material	O
from	O
anywhere	O
in	O
the	O
course	O
by	O
tapping	O
the	O
"	O
practice	O
weak	O
skills	O
"	O
button	O
.	O

To	O
create	O
the	O
SLA	O
modeling	O
corpus	O
,	O
we	O
sampled	O
from	O
Duolingo	O
users	O
who	O
registered	O
for	O
a	O
course	O
and	O
reached	O
at	O
least	O
the	O
tenth	O
row	O
of	O
skill	O
icons	O
within	O
the	O
month	O
of	O
November	O
2015	O
.	O
By	O
limiting	O
the	O
data	O
to	O
new	O
users	O
who	O
reach	O
this	O
level	O
of	O
the	O
course	O
,	O
we	O
hope	O
to	O
better	O
capture	O
beginners	O
'	O
broader	O
language	O
-	O
learning	O
process	O
,	O
including	O
repeated	O
interaction	O
with	O
vocabulary	O
and	O
grammar	O
over	O
time	O
.	O
Note	O
that	O
we	O
excluded	O
all	O
learners	O
who	O
took	O
a	O
placement	O
test	O
to	O
skip	O
ahead	O
in	O
the	O
course	O
,	O
since	O
these	O
learners	O
are	O
likely	O
more	O
advanced	O
.	O

An	O
important	O
question	O
for	O
SLA	O
modeling	O
is	O
:	O
to	O
what	O
extent	O
does	O
an	O
approach	O
generalize	O
across	O
languages	O
?	O
While	O
the	O
majority	O
of	O
Duolingo	O
users	O
learn	O
English	O
-	O
which	O
can	O
significantly	O
improve	O
job	O
prospects	O
and	O
quality	O
of	O
life	O
(	O
Pinon	O
and	O
Haydon	O
,	O
2010	O
)	O
-	O
Spanish	O
and	O
French	O
are	O
the	O
second	O
and	O
third	O
most	O
popular	O
courses	O
.	O
To	O
encourage	O
researchers	O
to	O
explore	O
language	O
-	O
agnostic	O
features	O
,	O
or	O
unified	O
cross	O
-	O
lingual	O
modeling	O
approaches	O
,	O
we	O
created	O
three	O
tracks	O
:	O
English	O
learners	O
(	O
who	O
speak	O
Spanish	O
)	O
,	O
Spanish	O
learners	O
(	O
who	O
speak	O
English	O
)	O
,	O
and	O
French	O
learners	O
(	O
who	O
speak	O
English	O
)	O
.	O

The	O
goal	O
of	O
the	O
task	O
is	O
as	O
follows	O
:	O
given	O
a	O
history	O
of	O
token	O
-	O
level	O
errors	O
made	O
by	O
the	O
learner	O
in	O
the	O
learning	O
language	O
(	O
L2	O
)	O
,	O
accurately	O
predict	O
the	O
errors	O
they	O
will	O
make	O
in	O
the	O
future	O
.	O
In	O
particular	O
,	O
we	O
focus	O
on	O
three	O
Duolingo	O
exercise	O
formats	O
that	O
require	O
the	O
learners	O
to	O
engage	O
in	O
active	O
recall	O
,	O
that	O
is	O
,	O
they	O
must	O
construct	O
answers	O
in	O
the	O
L2	O
through	O
translation	O
or	O
transcription	O
.	O
Figure	O
1	O
(	O
b	O
)	O
illustrates	O
a	O
reverse	O
translate	O
item	O
,	O
where	O
learners	O
are	O
given	O
a	O
prompt	O
in	O
the	O
language	O
they	O
know	O
(	O
e.g.	O
,	O
their	O
L1	O
or	O
native	O
language	O
)	O
,	O
and	O
translate	O
it	O
into	O
the	O
L2	O
.	O
Figure	O
1	O
(	O
c	O
)	O
illustrates	O
a	O
reverse	O
tap	O
item	O
,	O
which	O
is	O
a	O
simpler	O
version	O
of	O
the	O
same	O
format	O
:	O
learners	O
construct	O
an	O
answer	O
using	O
a	O
bank	O
of	O
words	O
and	O
distractors	O
.	O
Figure	O
1	O
(	O
d	O
)	O
is	O
a	O
listen	O
item	O
,	O
where	O
learners	O
hear	O
an	O
utterance	O
in	O
the	O
L2	O
they	O
are	O
learning	O
,	O
and	O
must	O
transcribe	O
it	O
.	O
Duolingo	O
does	O
include	O
many	O
other	O
exercise	O
formats	O
,	O
but	O
we	O
focus	O
on	O
these	O
three	O
in	O
the	O
current	O
work	O
,	O
since	O
constructing	O
L2	O
responses	O
through	O
translation	O
or	O
transcription	O
is	O
associated	O
with	O
deeper	O
levels	O
of	O
processing	O
,	O
which	O
in	O
turn	O
is	O
more	O
strongly	O
associated	O
with	O
learning	O
(	O
Craik	O
and	O
Tulving	O
,	O
1975	O
)	O
.	O
Since	O
each	O
exercise	O
can	O
have	O
multiple	O
correct	O
answers	O
(	O
due	O
to	O
synonyms	O
,	O
homophones	O
,	O
or	O
ambiguities	O
in	O
tense	O
,	O
number	O
,	O
formality	O
,	O
etc	O
.	O
)	O
,	O
Duolingo	O
uses	O
a	O
finite	O
-	O
state	O
machine	O
to	O
align	O
the	O
learner	O
's	O
response	O
to	O
the	O
most	O
similar	O
reference	O
answer	O
form	O
a	O
large	O
set	O
of	O
acceptable	O
responses	O
,	O
based	O
on	O
token	O
string	O
edit	O
distance	O
(	O
Levenshtein	O
,	O
1966	O
)	O
.	O
For	O
example	O
,	O
Figure	O
1	O
(	O
b	O
)	O
shows	O
an	O
example	O
of	O
corrective	O
feedback	O
based	O
on	O
such	O
an	O
alignment	O
.	O
Figure	O
2	O
shows	O
how	O
we	O
use	O
these	O
alignments	O
to	O
generate	O
labels	O
for	O
the	O
SLA	O
modeling	O
task	O
.	O
In	O
this	O
case	O
,	O
an	O
English	O
(	O
from	O
Spanish	O
)	O
learner	O
was	O
asked	O
to	O
translate	O
,	O
"	O
¿	O
Cuándo	O
puedo	O
ayudar	O
?	O
"	O
and	O
wrote	O
"	O
wen	O
can	O
help	O
"	O
instead	O
of	O
"	O
When	O
can	O
I	O
help	O
?	O
"	O
This	O
produces	O
two	O
errors	O
(	O
a	O
typo	O
and	O
a	O
missing	O
pronoun	O
)	O
.	O
We	O
ignore	O
capitalization	O
,	O
punctuation	O
,	O
and	O
accents	O
when	O
matching	O
tokens	O
.	O

The	O
data	O
were	O
released	O
in	O
two	O
phases	O
.	O
In	O
phase	O
1	O
(	O
8	O
weeks	O
)	O
,	O
TRAIN	O
and	O
DEV	O
partitions	O
were	O
released	O
with	O
labels	O
,	O
along	O
with	O
a	O
baseline	O
system	O
and	O
evaluation	O
script	O
,	O
for	O
system	O
development	O
.	O
In	O
phase	O
2	O
(	O
10	O
days	O
)	O
,	O
the	O
TEST	O
partition	O
was	O
released	O
without	O
labels	O
,	O
and	O
teams	O
submitted	O
predictions	O
to	O
CodaLab	O
3	O
for	O
blind	O
evaluation	O
.	O
To	O
allow	O
teams	O
to	O
compare	O
different	O
system	O
parameters	O
or	O
features	O
,	O
they	O
were	O
allowed	O
to	O
submit	O
up	O
to	O
10	O
predictions	O
total	O
(	O
up	O
to	O
2	O
per	O
day	O
)	O
during	O
this	O
phase	O
.	O
Table	O
1	O
reports	O
summary	O
statistics	O
for	O
each	O
of	O
the	O
data	O
partitions	O
for	O
all	O
three	O
tracks	O
.	O
We	O
created	O
TRAIN	O
,	O
DEV	O
,	O
and	O
TEST	O
partitions	O
as	O
follows	O
.	O
For	O
each	O
user	O
,	O
the	O
first	O
80	O
%	O
of	O
their	O
exercises	O
were	O
placed	O
in	O
the	O
TRAIN	O
set	O
,	O
the	O
subsequent	O
10	O
%	O
in	O
DEV	O
,	O
and	O
the	O
final	O
10	O
%	O
in	O
TEST	O
.	O
Hence	O
the	O
three	O
data	O
partitions	O
are	O
sequential	O
,	O
and	O
contain	O
ordered	O
observations	O
for	O
all	O
users	O
.	O
Note	O
that	O
because	O
the	O
three	O
data	O
partitions	O
are	O
sequential	O
,	O
and	O
the	O
DEV	O
set	O
contains	O
observations	O
that	O
are	O
potentially	O
valuable	O
for	O
making	O
TEST	O
set	O
predictions	O
,	O
most	O
teams	O
opted	O
to	O
combine	O
the	O
TRAIN	O
and	O
DEV	O
sets	O
to	O
train	O
their	O
systems	O
in	O
final	O
phase	O
2	O
evaluations	O
.	O

We	O
would	O
also	O
like	O
to	O
get	O
a	O
sense	O
of	O
which	O
features	O
,	O
if	O
any	O
,	O
significantly	O
affect	O
system	O
performance	O
.	O
Table	O
4	O
lists	O
features	O
provided	O
with	O
the	O
SLA	O
modeling	O
data	O
set	O
,	O
as	O
well	O
as	O
several	O
newlyengineered	O
feature	O
types	O
that	O
were	O
employed	O
by	O
at	O
least	O
three	O
teams	O
(	O
note	O
that	O
the	O
precise	O
details	O
may	O
vary	O
from	O
team	O
to	O
team	O
,	O
but	O
in	O
our	O
view	O
aim	O
to	O
cap	O
-	O
4	O
Interestingly	O
,	O
the	O
only	O
linear	O
model	O
to	O
rank	O
among	O
the	O
top	O
5	O
(	O
CECL	O
)	O
relied	O
on	O
combinatorial	O
feature	O
conjunctionswhich	O
effectively	O
alter	O
the	O
decision	O
surface	O
to	O
be	O
non	O
-	O
linear	O
with	O
respect	O
to	O
the	O
original	O
features	O
.	O
The	O
RNN	O
hidden	O
nodes	O
and	O
GBDT	O
constituent	O
trees	O
from	O
other	O
top	O
systems	O
may	O
in	O
fact	O
be	O
learning	O
to	O
represent	O
these	O
same	O
feature	O
conjunctions	O
.	O

The	O
authors	O
would	O
like	O
to	O
acknowledge	O
Bożena	O
Pająk	O
,	O
Joseph	O
Rollinson	O
,	O
and	O
Hideki	O
Shima	O
for	O
their	O
help	O
planning	O
and	O
co	O
-	O
organizing	O
the	O
shared	O
task	O
.	O
Eleanor	O
Avrunin	O
and	O
Natalie	O
Glance	O
made	O
significant	O
contributions	O
to	O
early	O
versions	O
of	O
the	O
SLA	O
modeling	O
data	O
set	O
,	O
and	O
Anastassia	O
Loukina	O
and	O
Kristen	O
K.	O
Reyher	O
provided	O
helpful	O
advice	O
regarding	O
mixed	O
-	O
effects	O
modeling	O
.	O
Finally	O
,	O
we	O
would	O
like	O
to	O
thank	O
the	O
organizers	O
of	O
the	O
NAACL	O
-	O
HLT	O
2018	O
Workshop	O
on	O
Innovative	O
Use	O
of	O
NLP	O
for	O
Building	O
Educational	O
Applications	O
(	O
BEA	O
)	O
for	O
providing	O
a	O
forum	O
for	O
this	O
work	O
.	O

English	O
Spanish	O
French	O
by	O
NYU	O
(	O
+1.632	O
)	O
,	O
whereas	O
the	O
top	O
-	O
performing	O
team	O
SanaLabs	O
had	O
a	O
surprisingly	O
lower	O
weight	O
(	O
+0.841	O
)	O
.	O
This	O
could	O
be	O
due	O
to	O
the	O
fact	O
that	O
their	O
system	O
was	O
itself	O
an	O
ensemble	O
of	O
an	O
RNN	O
and	O
GBDT	O
models	O
,	O
which	O
were	O
used	O
(	O
in	O
isolation	O
)	O
by	O
each	O
of	O
the	O
other	O
two	O
teams	O
.	O
This	O
seems	O
to	O
add	O
further	O
support	O
for	O
the	O
effectiveness	O
of	O
combining	O
these	O
algorithms	O
for	O
the	O
task	O
.	O

Higher	O
-	O
order	O
Comparisons	O
of	O
Sentence	O
Encoder	O
Representations	O

Representational	O
Similarity	O
Analysis	O
(	O
RSA	O
)	O
is	O
a	O
technique	O
developed	O
by	O
neuroscientists	O
for	O
comparing	O
activity	O
patterns	O
of	O
different	O
measurement	O
modalities	O
(	O
e.g.	O
,	O
fMRI	O
,	O
electrophysiology	O
,	O
behavior	O
)	O
.	O
As	O
a	O
framework	O
,	O
RSA	O
has	O
several	O
advantages	O
over	O
existing	O
approaches	O
to	O
interpretation	O
of	O
language	O
encoders	O
based	O
on	O
probing	O
or	O
diagnostic	O
classification	O
:	O
namely	O
,	O
it	O
does	O
not	O
require	O
large	O
training	O
samples	O
,	O
is	O
not	O
prone	O
to	O
overfitting	O
,	O
and	O
it	O
enables	O
a	O
more	O
transparent	O
comparison	O
between	O
the	O
representational	O
geometries	O
of	O
different	O
models	O
and	O
modalities	O
.	O
We	O
demonstrate	O
the	O
utility	O
of	O
RSA	O
by	O
establishing	O
a	O
previously	O
unknown	O
correspondence	O
between	O
widely	O
-	O
employed	O
pretrained	O
language	O
encoders	O
and	O
human	O
processing	O
difficulty	O
via	O
eye	O
-	O
tracking	O
data	O
,	O
showcasing	O
its	O
potential	O
in	O
the	O
interpretability	O
toolbox	O
for	O
neural	O
models	O
.	O

We	O
presented	O
a	O
framework	O
for	O
analyzing	O
neural	O
network	O
representations	O
(	O
RSA	O
)	O
that	O
allowed	O
us	O
to	O
relate	O
human	O
sentence	O
processing	O
data	O
with	O
language	O
encoder	O
representations	O
.	O
In	O
experiments	O
conducted	O
on	O
two	O
widely	O
used	O
encoders	O
,	O
our	O
findings	O
show	O
that	O
sentences	O
which	O
are	O
difficult	O
for	O
humans	O
to	O
process	O
have	O
more	O
divergent	O
representations	O
both	O
intra	O
-	O
encoder	O
and	O
between	O
different	O
encoders	O
.	O
Furthermore	O
,	O
we	O
lend	O
modest	O
support	O
to	O
the	O
intuition	O
that	O
a	O
model	O
's	O
middle	O
layers	O
encode	O
comparatively	O
more	O
syntax	O
.	O
Our	O
framework	O
offers	O
insight	O
that	O
is	O
complimentary	O
to	O
decoding	O
or	O
probing	O
approaches	O
,	O
and	O
is	O
particularly	O
useful	O
to	O
compare	O
representations	O
from	O
across	O
modalities	O
.	O

We	O
empirically	O
analyse	O
the	O
OAAG	O
model	O
with	O
dynamic	O
fusion	O
and	O
CHIME	O
model	O
to	O
answer	O
the	O
following	O
research	O
questions	O
:	O
RQ1	O
:	O
Are	O
the	O
retrieved	O
review	O
snippets	O
significantly	O
utilized	O
for	O
generating	O
the	O
answers	O
?	O
RQ2	O
:	O
Is	O
the	O
model	O
performing	O
similarly	O
for	O
a	O
heterogeneous	O
group	O
of	O
questions	O
?	O
RQ3	O
:	O
Is	O
the	O
generative	O
model	O
biased	O
towards	O
more	O
frequently	O
occurring	O
phrases	O
?	O
RQ4	O
:	O
Can	O
ROUGE	O
capture	O
the	O
correctness	O
of	O
generated	O
answers	O
?	O

For	O
answering	O
RQ1	O
,	O
at	O
inference	O
time	O
,	O
we	O
replace	O
these	O
reviews	O
with	O
four	O
sets	O
of	O
review	O
snippets	O
:	O
(	O
i	O
)	O
TrainA	O
:	O
We	O
use	O
BM25	O
to	O
find	O
the	O
closest	O
question	O
to	O
the	O
test	O
question	O
in	O
the	O
train	O
data	O
,	O
and	O
we	O
take	O
the	O
answer	O
of	O
it	O
as	O
the	O
generated	O
answer	O
.	O
(	O
ii	O
)	O
Ran	O
-	O
domOD	O
:	O
We	O
randomly	O
choose	O
the	O
review	O
snippets	O
from	O
any	O
other	O
product	O
of	O
that	O
category	O
except	O
the	O
product	O
for	O
which	O
the	O
question	O
is	O
asked	O
.	O
(	O
iii	O
)	O
Ran	O
-	O
domID	O
:	O
We	O
randomly	O
select	O
review	O
snippets	O
from	O
the	O
review	O
sentences	O
of	O
that	O
particular	O
product	O
.	O
(	O
iv	O
)	O
BM25QA	O
:	O
We	O
retrieve	O
the	O
review	O
snippets	O
using	O
the	O
BM25	O
algorithm	O
that	O
uses	O
the	O
question	O
and	O
reference	O
answer	O
in	O
the	O
test	O
dataset	O
.	O
OAAG	O
uses	O
the	O
opinion	O
along	O
with	O
the	O
reviews	O
.	O
We	O
also	O
select	O
the	O
opinion	O
of	O
the	O
corresponding	O
review	O
sentence	O
while	O
replacing	O
the	O
reviews	O
.	O
Both	O
the	O
models	O
utilize	O
the	O
top	O
10	O
reviews	O
for	O
training	O
and	O
evaluation	O
.	O
Table	O
1	O
shows	O
the	O
result	O
of	O
this	O
experiment	O
.	O
The	O
TrainA	O
does	O
not	O
utilize	O
either	O
of	O
the	O
models	O
to	O
generate	O
the	O
answer	O
.	O
It	O
shows	O
the	O
answer	O
from	O
the	O
most	O
similar	O
train	O
question	O
,	O
and	O
its	O
performance	O
is	O
competitive	O
with	O
other	O
methods	O
,	O
especially	O
in	O
Home	O
.	O
In	O
both	O
the	O
categories	O
,	O
the	O
performance	O
of	O
both	O
the	O
models	O
is	O
almost	O
similar	O
in	O
RandomOD	O
and	O
RandomID	O
.	O
RandomID	O
shows	O
marginally	O
better	O
performance	O
than	O
RandomOD	O
for	O
OAAG	O
.	O
For	O
CHIME	O
,	O
BM25Q	O
performs	O
the	O
best	O
in	O
both	O
categories	O
.	O
For	O
OAAG	O
,	O
BM25QA	O
performs	O
the	O
best	O
in	O
Home	O
while	O
in	O
Sports	O
,	O
BM25QA	O
performs	O
the	O
best	O
in	O
R1	O
,	O
and	O
BM25Q	O
performs	O
the	O
best	O
in	O
RL	O
,	O
but	O
the	O
difference	O
is	O
minute	O
.	O
The	O
results	O
are	O
quite	O
surprising	O
:	O
the	O
performance	O
of	O
the	O
models	O
is	O
very	O
similar	O
when	O
the	O
answers	O
are	O
generated	O
with	O
random	O
reviews	O
vs.	O
when	O
the	O
answers	O
are	O
generated	O
with	O
the	O
reviews	O
obtained	O
from	O
BM25	O
.	O
Hence	O
,	O
it	O
is	O
not	O
clear	O
if	O
the	O
model	O
is	O
effectively	O
utilizing	O
the	O
retrieved	O
review	O
snippets	O
.	O

We	O
observe	O
that	O
some	O
phrases	O
are	O
frequently	O
occurring	O
in	O
the	O
reference	O
answers	O
as	O
well	O
as	O
in	O
the	O
generated	O
answers	O
.	O
We	O
find	O
that	O
in	O
the	O
training	O
data	O
of	O
both	O
categories	O
,	O
∼2.4	O
%	O
of	O
the	O
reference	O
answers	O
start	O
with	O
the	O
phrase	O
"	O
I	O
do	O
n't	O
think	O
so	O
"	O
,	O
but	O
12.29	O
%	O
of	O
responses	O
in	O
Sports	O
and	O
35.64	O
%	O
responses	O
in	O
Home	O
begin	O
with	O
this	O
phrase	O
.	O
This	O
∼2.4	O
%	O
repetition	O
of	O
the	O
same	O
phrase	O
in	O
the	O
training	O
data	O
makes	O
the	O
generative	O
model	O
biased	O
towards	O
this	O
phrase	O
.	O
Many	O
of	O
the	O
reference	O
answers	O
in	O
the	O
training	O
data	O
contain	O
"	O
I	O
do	O
n't	O
know	O
"	O
,	O
"	O
I	O
have	O
no	O
idea	O
"	O
,	O
"	O
I	O
ca	O
n't	O
say	O
"	O
.	O
These	O
kinds	O
of	O
answers	O
do	O
not	O
give	O
any	O
meaningful	O
information	O
to	O
the	O
user	O
.	O
Together	O
,	O
we	O
denote	O
these	O
phrases	O
as	O
IDK	O
.	O
On	O
analysis	O
of	O
the	O
dataset	O
,	O
we	O
find	O
that	O
in	O
Sports	O
,	O
there	O
are	O
3.04	O
%	O
,	O
2.9	O
%	O
,	O
and	O
6.9	O
%	O
IDK	O
phrases	O
in	O
train	O
dataset	O
answers	O
,	O
test	O
dataset	O
answers	O
,	O
and	O
generated	O
answers	O
,	O
respectively	O
.	O
In	O
Home	O
,	O
the	O
answers	O
in	O
the	O
train	O
and	O
test	O
dataset	O
contain	O
3.64	O
%	O
and	O
3.60	O
%	O
IDK	O
phrases	O
,	O
respectively	O
,	O
but	O
16.31	O
%	O
of	O
the	O
answers	O
are	O
generated	O
as	O
IDK	O
phrases	O
.	O
So	O
,	O
in	O
the	O
generated	O
answers	O
,	O
the	O
appearance	O
of	O
IDK	O
phrases	O
has	O
increased	O
by	O
three	O
to	O
five	O
folds	O
which	O
clearly	O
shows	O
that	O
the	O
model	O
is	O
biased	O
towards	O
these	O
frequent	O
phrases	O
.	O
To	O
see	O
the	O
effect	O
of	O
these	O
phrases	O
on	O
the	O
models	O
,	O
we	O
remove	O
the	O
questions	O
from	O
the	O
training	O
dataset	O
which	O
have	O
IDK	O
in	O
their	O
reference	O
answer	O
4	O
and	O
retrain	O
the	O
models	O
.	O
We	O
denote	O
this	O
model	O
as	O
BM25Q	O
-	O
IDK	O
.	O
Table	O
3	O
shows	O
the	O
result	O
of	O
BM25Q	O
,	O
the	O
model	O
trained	O
on	O
the	O
original	O
training	O
data	O
,	O
and	O
BM25Q	O
-	O
IDK	O
.	O
Home	O
had	O
16.31	O
%	O
and	O
Sports	O
had	O
only	O
6.9	O
%	O
IDK	O
phrases	O
,	O
and	O
consequently	O
,	O
when	O
the	O
IDK	O
phrases	O
are	O
removed	O
,	O
it	O
has	O
more	O
impact	O
on	O
Home	O
which	O
drops	O
the	O
bias	O
towards	O
these	O
phrases	O
and	O
improves	O
the	O
ROUGE	O
score	O
,	O
whereas	O
,	O
in	O
Sports	O
,	O
BM25Q	O
and	O
BM25Q	O
-	O
IDK	O
have	O
close	O
ROUGE	O
scores	O
due	O
to	O
lesser	O
IDK	O
phrases	O
in	O
the	O
generated	O
answer	O
.	O

For	O
answering	O
RQ4	O
,	O
we	O
look	O
into	O
the	O
generated	O
response	O
with	O
high	O
R1	O
scores	O
and	O
check	O
their	O
correct	O
-	O
Question	O
is	O
this	O
box	O
made	O
of	O
polypropylene	O
?	O
can	O
photos	O
and	O
mementos	O
be	O
safely	O
stored	O
in	O
it	O
?	O
Ref	O
.	O
Ans	O
.	O
i	O
ca	O
n't	O
comment	O
on	O
the	O
material	O
it	O
is	O
made	O
of	O
,	O
but	O
the	O
top	O
does	O
not	O
stay	O
on	O
tight	O
.	O
it	O
is	O
not	O
a	O
decent	O
storage	O
container	O
for	O
anything	O
requiring	O
a	O
top	O
or	O
to	O
be	O
airtight	O
.	O
OAAG	O
.	O
it	O
says	O
"	O
made	O
in	O
china	O
"	O
on	O
the	O
bottom	O
of	O
the	O
box	O
.	O
it	O
says	O
"	O
made	O
in	O
china	O
"	O
on	O
the	O
bottom	O
of	O
the	O
box	O
.	O
hope	O
that	O
helps	O
.	O
(	O
26.22	O
)	O
CHIME	O
.	O
yes	O
,	O
it	O
is	O
polypropylene	O
and	O
can	O
be	O
stored	O
in	O
the	O
box	O
if	O
you	O
want	O
to	O
store	O
them	O
in	O
a	O
safe	O
environment	O
that	O
is	O
not	O
toxic	O
to	O
you	O
and/or	O
your	O
pets	O
or	O
food	O
they	O
will	O
be	O
exposed	O
(	O
27.77	O
)	O

What	O
is	O
the	O
width	O
at	O
the	O
base	O
?	O
Ref	O
.	O
Ans	O
.	O
Width	O
across	O
the	O
bottom	O
/	O
base	O
is	O
approximately	O
3	O
inches	O
.	O
OAAG	O
.	O
The	O
width	O
of	O
the	O
top	O
of	O
the	O
base	O
is	O
about	O
1.5	O
inches	O
.	O
the	O
base	O
of	O
the	O
pitcher	O
is	O
9	O
inches	O
.	O
the	O
top	O
of	O
the	O
pitcher	O
is	O
about	O
1.5	O
inche	O
.	O
'	O
(	O
25.00	O
)	O
CHIME	O
.	O
itś	O
about	O
12	O
"	O
wide	O
at	O
the	O
base	O
and	O
about	O
10.5	O
inches	O
deep	O
(	O
26.08	O
)	O
ness	O
with	O
respect	O
to	O
the	O
reference	O
answer	O
.	O
In	O
OAAG	O
model	O
,	O
15.36	O
%	O
predictions	O
in	O
Home	O
and	O
13.34	O
%	O
predictions	O
in	O
Sports	O
have	O
R1	O
score	O
above	O
20	O
.	O
We	O
manually	O
analyse	O
the	O
reference	O
and	O
generated	O
answers	O
of	O
randomly	O
chosen	O
100	O
question	O
-	O
answers	O
with	O
a	O
high	O
ROUGE	O
score	O
(	O
>	O
20	O
)	O
,	O
and	O
we	O
find	O
that	O
54	O
%	O
are	O
answered	O
incorrectly	O
.	O
In	O
CHIME	O
model	O
,	O
46.87	O
%	O
predictions	O
in	O
Home	O
and	O
46.15	O
%	O
predictions	O
in	O
Sports	O
have	O
R1	O
score	O
above	O
20	O
and	O
56	O
%	O
of	O
100	O
randomly	O
chosen	O
question	O
-	O
answer	O
pairs	O
(	O
whose	O
ROUGE	O
score	O
>	O
20	O
)	O
turn	O
out	O
to	O
be	O
incorrect	O
.	O
Table	O
4	O
shows	O
two	O
examples	O
where	O
the	O
generated	O
responses	O
result	O
in	O
high	O
R1	O
scores	O
,	O
but	O
the	O
answers	O
are	O
incorrect	O
.	O
Both	O
models	O
predict	O
irrelevant	O
answers	O
in	O
the	O
first	O
question	O
,	O
and	O
the	O
predicted	O
dimension	O
is	O
incorrect	O
in	O
the	O
second	O
question	O
.	O
It	O
shows	O
that	O
it	O
is	O
not	O
possible	O
to	O
infer	O
from	O
ROUGE	O
scores	O
if	O
the	O
generated	O
answer	O
is	O
accurate	O
to	O
the	O
reference	O
answer	O
,	O
i.e.	O
,	O
the	O
word	O
count	O
overlap	O
is	O
not	O
an	O
indicator	O
of	O
an	O
accurate	O
answer	O
.	O
We	O
show	O
some	O
more	O
cases	O
with	O
high	O
R1	O
scores	O
in	O
Tables	O
A.2	O
and	O
A.3	O
in	O
the	O
Appendix	O
.	O

Similarly	O
,	O
in	O
the	O
case	O
of	O
Table	O
A.3	O
,	O
the	O
second	O
and	O
the	O
fifth	O
examples	O
have	O
high	O
R1	O
scores	O
,	O
but	O
the	O
generated	O
answers	O
are	O
exactly	O
opposite	O
of	O
the	O
reference	O
answers	O
.	O
In	O
the	O
first	O
question	O
,	O
the	O
model	O
wrongly	O
predicts	O
that	O
it	O
would	O
melt	O
and	O
bubble	O
up	O
a	O
little	O
in	O
the	O
microwave	O
,	O
and	O
in	O
the	O
third	O
question	O
,	O
it	O
predicts	O
an	O
entirely	O
different	O
answer	O
.	O
The	O
response	O
to	O
the	O
fourth	O
question	O
is	O
"	O
I	O
do	O
n't	O
know	O
,	O
"	O
which	O
is	O
a	O
frequently	O
occurring	O
phrase	O
.	O

it	O
's	O
about	O
3	O
oz	O
.	O
and	O
it	O
's	O
just	O
as	O
strong	O
as	O
any	O
other	O
clip	O
i	O
'	O
ve	O
seen	O
OAAG	O
:	O
it	O
's	O
about	O
7.5	O
′′	O
long	O
.	O
it	O
's	O
a	O
great	O
knife	O
.	O
it	O
's	O
a	O
great	O
deal	O
.	O
it	O
's	O
a	O
great	O
knife	O
and	O
i	O
love	O
it	O
.	O
it	O
's	O
a	O
great	O
deal	O
.	O
CHIME	O
:	O
i	O
'm	O
not	O
sure	O
about	O
the	O
exact	O
weight	O
but	O
it	O
is	O
very	O
light	O
and	O
is	O
very	O
easy	O
to	O
use	O
with	O
ease	O
.	O
Question	O
:	O
does	O
this	O
fit	O
a	O
2	O
year	O
old	O
?	O
Ref	O
.	O
Ans	O
.	O
:	O
it	O
fit	O
my	O
two	O
year	O
old	O
who	O
has	O
a	O
pretty	O
normal	O
size	O
head	O
.	O
the	O
multiple	O
thicknesses	O
of	O
pads	O
that	O
are	O
included	O
really	O
make	O
it	O
a	O
great	O
long	O
term	O
helmet	O
!	O
OAAG	O
:	O
yes	O
it	O
will	O
,	O
it	O
's	O
a	O
very	O
tight	O
fit	O
.	O
i	O
do	O
n't	O
think	O
it	O
would	O
be	O
too	O
big	O
for	O
a	O
2	O
′′	O
2	O
′′	O
.	O
it	O
's	O
a	O
great	O
helmet	O
.	O

i	O
bought	O
this	O
for	O
my	O
son	O
and	O
he	O
loves	O
it	O
so	O
much	O
he	O
bought	O
another	O
one	O
for	O
his	O
2	O
year	O
-	O
bean	O
.	O

We	O
present	O
some	O
examples	O
of	O
numerical	O
questions	O
with	O
their	O
answers	O
in	O
Table	O
A.4	O
.	O
In	O
the	O
first	O
example	O
,	O
the	O
generated	O
answer	O
is	O
right	O
,	O
but	O
none	O
of	O
the	O
answers	O
are	O
correct	O
for	O
the	O
rest	O
of	O
the	O
questions	O
.	O

The	O
AFRL	O
WMT19	O
Systems	O
:	O
Old	O
Favorites	O
and	O
New	O
Tricks	O

Six	O
Attributes	O
of	O
Unhealthy	O
Conversations	O

We	O
present	O
a	O
new	O
dataset	O
of	O
approximately	O
44000	O
comments	O
labeled	O
by	O
crowdworkers	O
.	O
Each	O
comment	O
is	O
labelled	O
as	O
either	O
'	O
healthy	O
'	O
or	O
'	O
unhealthy	O
'	O
,	O
in	O
addition	O
to	O
binary	O
labels	O
for	O
the	O
presence	O
of	O
six	O
potentially	O
'	O
unhealthy	O
'	O
sub	O
-	O
attributes	O
:	O
(	O
1	O
)	O
hostile	O
;	O
(	O
2	O
)	O
antagonistic	O
,	O
insulting	O
,	O
provocative	O
or	O
trolling	O
;	O
(	O
3	O
)	O
dismissive	O
;	O
(	O
4	O
)	O
condescending	O
or	O
patronising	O
;	O
(	O
5	O
)	O
sarcastic	O
;	O
and/or	O
(	O
6	O
)	O
an	O
unfair	O
generalisation	O
.	O
Each	O
label	O
also	O
has	O
an	O
associated	O
confidence	O
score	O
.	O
We	O
argue	O
that	O
there	O
is	O
a	O
need	O
for	O
datasets	O
which	O
enable	O
research	O
based	O
on	O
a	O
broad	O
notion	O
of	O
'	O
unhealthy	O
online	O
conversation	O
'	O
.	O
We	O
build	O
this	O
typology	O
to	O
encompass	O
a	O
substantial	O
proportion	O
of	O
the	O
individual	O
comments	O
which	O
contribute	O
to	O
unhealthy	O
online	O
conversation	O
.	O
For	O
some	O
of	O
these	O
attributes	O
,	O
this	O
is	O
the	O
first	O
publicly	O
available	O
dataset	O
of	O
this	O
scale	O
.	O
We	O
explore	O
the	O
quality	O
of	O
the	O
dataset	O
,	O
present	O
some	O
summary	O
statistics	O
and	O
initial	O
models	O
to	O
illustrate	O
the	O
utility	O
of	O
this	O
data	O
,	O
and	O
highlight	O
limitations	O
and	O
directions	O
for	O
further	O
research	O
.	O

In	O
this	O
paper	O
,	O
we	O
broadly	O
characterise	O
a	O
healthy	O
online	O
public	O
conversation	O
as	O
one	O
where	O
posts	O
and	O
comments	O
are	O
made	O
in	O
good	O
faith	O
,	O
are	O
not	O
overly	O
hostile	O
or	O
destructive	O
,	O
and	O
generally	O
invite	O
engagement	O
.	O
Such	O
a	O
conversation	O
may	O
include	O
robust	O
engagement	O
and	O
debate	O
,	O
and	O
is	O
generally	O
(	O
though	O
not	O
always	O
)	O
focused	O
on	O
substance	O
and	O
ideas	O
.	O
Importantly	O
,	O
though	O
,	O
healthy	O
contributions	O
to	O
online	O
conversations	O
are	O
not	O
necessarily	O
friendly	O
,	O
grammatically	O
correct	O
,	O
well	O
constructed	O
,	O
intellectual	O
,	O
substantive	O
,	O
or	O
even	O
free	O
of	O
any	O
vulgarity	O
.	O
Some	O
harmful	O
contributions	O
to	O
conversations	O
are	O
obviously	O
derogatory	O
,	O
threatening	O
,	O
violent	O
,	O
or	O
insulting	O
(	O
Anderson	O
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
these	O
are	O
the	O
sorts	O
of	O
comments	O
which	O
have	O
been	O
the	O
primary	O
focus	O
of	O
research	O
in	O
algorithmic	O
moderation	O
assistance	O
and	O
related	O
areas	O
.	O
However	O
,	O
many	O
of	O
those	O
comments	O
which	O
deter	O
people	O
from	O
engagement	O
or	O
create	O
downward	O
spirals	O
in	O
interactions	O
can	O
be	O
more	O
subtle	O
(	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
.	O
This	O
is	O
especially	O
the	O
case	O
with	O
conversations	O
online	O
,	O
many	O
of	O
which	O
(	O
i	O
)	O
take	O
place	O
in	O
a	O
'	O
public	O
'	O
forum	O
that	O
is	O
visible	O
to	O
thousands	O
of	O
others	O
,	O
and	O
(	O
ii	O
)	O
involve	O
strangers	O
who	O
have	O
never	O
met	O
and	O
know	O
little	O
about	O
one	O
another	O
(	O
Santana	O
,	O
2014	O
)	O
.	O
These	O
two	O
features	O
of	O
online	O
conversations	O
can	O
sometimes	O
enhance	O
commenters	O
'	O
sensitivity	O
to	O
subtler	O
forms	O
of	O
toxicity	O
like	O
sarcasm	O
,	O
condescension	O
,	O
or	O
dismissiveness	O
,	O
amplifying	O
their	O
negative	O
impact	O
on	O
conversations	O
despite	O
the	O
fact	O
that	O
these	O
attributes	O
may	O
be	O
less	O
(	O
or	O
not	O
at	O
all	O
)	O
harmful	O
in	O
other	O
specific	O
contexts	O
.	O
Identifying	O
subtle	O
indicators	O
of	O
problematic	O
online	O
comments	O
is	O
a	O
difficult	O
task	O
.	O
There	O
are	O
at	O
least	O
three	O
reasons	O
for	O
this	O
.	O
First	O
,	O
they	O
are	O
less	O
extreme	O
and	O
therefore	O
less	O
likely	O
to	O
use	O
clearly	O
identifiable	O
explicit	O
or	O
inflammatory	O
language	O
.	O
Second	O
,	O
a	O
substantive	O
point	O
might	O
be	O
made	O
in	O
an	O
inflammatory	O
way	O
,	O
or	O
a	O
remark	O
may	O
be	O
perceived	O
differently	O
depending	O
on	O
the	O
context	O
,	O
norms	O
,	O
and	O
expectations	O
of	O
the	O
reader	O
.	O
Third	O
,	O
there	O
is	O
an	O
even	O
greater	O
risk	O
of	O
identifying	O
'	O
false	O
positives	O
'	O
and	O
'	O
false	O
negatives	O
'	O
,	O
since	O
many	O
of	O
the	O
expressions	O
used	O
in	O
subtle	O
forms	O
of	O
toxicity	O
can	O
also	O
be	O
deployed	O
for	O
positive	O
contributions	O
.	O
For	O
example	O
,	O
sarcasm	O
is	O
often	O
used	O
in	O
derisive	O
or	O
bullying	O
ways	O
,	O
but	O
it	O
can	O
also	O
be	O
used	O
for	O
humour	O
or	O
to	O
express	O
a	O
substantive	O
,	O
inoffensive	O
point	O
(	O
Vidgen	O
et	O
al	O
,	O
2019	O
)	O
.	O
The	O
challenge	O
is	O
to	O
identify	O
the	O
subtle	O
characteristics	O
of	O
harmful	O
comments	O
online	O
despite	O
their	O
ambiguity	O
,	O
without	O
falsely	O
identifying	O
healthy	O
comments	O
.	O
We	O
differentiate	O
between	O
two	O
categories	O
.	O
The	O
first	O
,	O
which	O
is	O
the	O
most	O
well	O
studied	O
to	O
date	O
,	O
are	O
those	O
whose	O
explicit	O
intention	O
is	O
to	O
insult	O
,	O
threaten	O
,	O
or	O
abuse	O
.	O
The	O
second	O
category	O
,	O
are	O
comments	O
which	O
engage	O
with	O
others	O
,	O
share	O
an	O
opinion	O
,	O
or	O
contribute	O
to	O
the	O
conversation	O
,	O
but	O
are	O
written	O
in	O
a	O
way	O
which	O
is	O
likely	O
to	O
antagonise	O
,	O
hurt	O
,	O
or	O
deter	O
others	O
.	O
We	O
found	O
these	O
comments	O
to	O
be	O
at	O
least	O
as	O
prevalent	O
in	O
the	O
sample	O
data	O
(	O
Table	O
1	O
)	O
.	O
Our	O
typology	O
of	O
unhealthy	O
attributes	O
aims	O
to	O
include	O
this	O
second	O
category	O
of	O
comments	O
,	O
and	O
determine	O
whether	O
annotators	O
believe	O
they	O
belong	O
in	O
a	O
healthy	O
online	O
conversation	O
.	O
Our	O
hypothesis	O
was	O
that	O
together	O
these	O
6	O
attributes	O
account	O
for	O
the	O
majority	O
of	O
'	O
unhealthy	O
'	O
comments	O
online	O
,	O
but	O
that	O
there	O
will	O
still	O
be	O
some	O
comments	O
that	O
are	O
'	O
unhealthy	O
'	O
but	O
do	O
not	O
display	O
any	O
sub	O
-	O
attribute	O
,	O
and	O
also	O
some	O
which	O
are	O
'	O
healthy	O
'	O
despite	O
representing	O
one	O
or	O
more	O
sub	O
-	O
attributes	O
(	O
see	O
Figure	O
1	O
)	O
.	O
In	O
general	O
,	O
whether	O
the	O
presence	O
of	O
these	O
attributes	O
indicates	O
healthy	O
or	O
unhealthy	O
conversation	O
will	O
also	O
depend	O
significantly	O
on	O
the	O
nature	O
of	O
the	O
forum	O
and	O
users	O
.	O
Nonetheless	O
,	O
the	O
combination	O
of	O
an	O
abstract	O
'	O
health	O
'	O
rating	O
with	O
the	O
other	O
6	O
attributes	O
provides	O
a	O
useful	O
dataset	O
for	O
investigating	O
nuanced	O
comments	O
,	O
and	O
could	O
be	O
used	O
to	O
help	O
develop	O
a	O
broader	O
range	O
of	O
models	O
that	O
are	O
customised	O
for	O
specific	O
production	O
environments	O
.	O

One	O
further	O
challenge	O
which	O
comes	O
with	O
annotating	O
more	O
subtle	O
unhealthy	O
attributes	O
is	O
the	O
potential	O
to	O
encode	O
unintended	O
societal	O
biases	O
and	O
value	O
judgements	O
in	O
models	O
trained	O
on	O
this	O
data	O
.	O
For	O
example	O
,	O

In	O
this	O
job	O
,	O
you	O
will	O
be	O
asked	O
to	O
read	O
a	O
comment	O
and	O
to	O
express	O
an	O
overall	O
opinion	O
about	O
whether	O
you	O
think	O
it	O
has	O
a	O
place	O
in	O
a	O
healthy	O
conversation	O
online	O
.	O
You	O
will	O
also	O
be	O
asked	O
to	O
identify	O
whether	O
it	O
displays	O
a	O
range	O
of	O
characteristics	O
that	O
may	O
lead	O
to	O
unhealthy	O
conversations	O
.	O
These	O
characteristics	O
include	O
:	O
sarcasm	O
,	O
gross	O
generalisations	O
,	O
hostility	O
,	O
aggression	O
,	O
dismissiveness	O
,	O
condescension	O
and	O
patronization	O
.	O
All	O
of	O
the	O
comments	O
you	O
will	O
see	O
are	O
real	O
comments	O
posted	O
by	O
users	O
in	O
online	O
conversations	O
.	O
Most	O
of	O
them	O
will	O
have	O
been	O
posted	O
in	O
response	O
to	O
one	O
or	O
more	O
comments	O
made	O
by	O
others	O
(	O
which	O
you	O
are	O
not	O
given	O
)	O
.	O
However	O
,	O
the	O
questions	O
are	O
designed	O
in	O
such	O
a	O
way	O
that	O
you	O
should	O
be	O
able	O
to	O
answer	O
them	O
without	O
seeing	O
these	O
other	O
comments	O
.	O
The	O
data	O
collected	O
here	O
will	O
be	O
used	O
to	O
help	O
build	O
tools	O
which	O
promote	O
healthier	O
conversations	O
online	O
.	O

Please	O
bear	O
in	O
mind	O
that	O
the	O
questions	O
do	O
not	O
ask	O
whether	O
you	O
agree	O
or	O
disagree	O
with	O
the	O
substance	O
of	O
each	O
comment	O
.	O
Do	O
your	O
best	O
to	O
ignore	O
your	O
own	O
opinion	O
on	O
the	O
substantive	O
idea	O
or	O
claim	O
made	O
in	O
the	O
comment	O
when	O
answering	O
the	O
questions	O
.	O
Please	O
be	O
sure	O
to	O
read	O
the	O
full	O
text	O
of	O
the	O
comment	O
before	O
answering	O
the	O
questions	O
.	O
Sometimes	O
the	O
part	O
of	O
a	O
comment	O
which	O
displays	O
one	O
or	O
more	O
of	O
the	O
attributes	O
you	O
will	O
be	O
asked	O
about	O
,	O
appears	O
close	O
to	O
the	O
end	O
of	O
the	O
comment	O
.	O

What	O
are	O
the	O
characteristics	O
of	O
a	O
healthy	O
conversation	O
?	O
Posts	O
and	O
comments	O
are	O
made	O
in	O
good	O
faith	O
Posts	O
and	O
comments	O
are	O
not	O
overly	O
hostile	O
,	O
and	O
are	O
not	O
destructive	O
The	O
comments	O
in	O
the	O
conversation	O
generally	O
invite	O
engagement	O
The	O
conversation	O
may	O
include	O
robust	O
engagement	O
and	O
debate	O
The	O
conversation	O
is	O
generally	O
focused	O
on	O
substance	O
and	O
ideas	O
A	O
healthy	O
conversation	O
does	O
not	O
necessarily	O
require	O
all	O
posts	O
and	O
comments	O
to	O
be	O
:	O
Is	O
this	O
comment	O
sarcastic	O
?	O
3	O
.	O
Does	O
this	O
comment	O
make	O
a	O
generalisation	O
about	O
a	O
specific	O
group	O
of	O
people	O
?	O
4	O
.	O
If	O
yes	O
,	O
would	O
a	O
member	O
of	O
that	O
group	O
feel	O
that	O
the	O
generalisation	O
is	O
unfair	O
?	O
5	O
.	O
Is	O
this	O
comment	O
needlessly	O
hostile	O
?	O
6	O
.	O
Is	O
the	O
intention	O
of	O
this	O
comment	O
to	O
insult	O
,	O
antagonize	O
,	O
provoke	O
,	O
or	O
troll	O
other	O
users	O
?	O
7	O
.	O
A	O
comment	O
with	O
a	O
condescending	O
or	O
patronising	O
tone	O
will	O
generally	O
assume	O
an	O
attitude	O
of	O
superiority	O
,	O
and	O
imply	O
that	O
the	O
other	O
commenter	O
(	O
s	O
)	O
is	O
ignorant	O
,	O
child	O
-	O
like	O
,	O
naive	O
,	O
or	O
unintelligent	O
.	O
Such	O
comments	O
will	O
usually	O
imply	O
that	O
the	O
other	O
commenter	O
should	O
n't	O
be	O
taken	O
seriously	O
.	O
Is	O
this	O
comment	O
condescending	O
and/or	O
patronising	O
?	O
8	O
.	O
A	O
comment	O
is	O
dismissive	O
if	O
it	O
rejects	O
or	O
ridicules	O
another	O
comment	O
without	O
good	O
reason	O
,	O
or	O
tries	O
to	O
push	O
another	O
commenter	O
and	O
their	O
ideas	O
out	O
of	O
the	O
conversations	O
.	O
Note	O
:	O
A	O
comment	O
which	O
expresses	O
disagreement	O
is	O
not	O
necessarily	O
dismissive	O
.	O
Is	O
this	O
comment	O
dismissive	O
?	O

The	O
LAIX	O
Systems	O
in	O
the	O
BEA	O
-	O
2019	O
GEC	O
Shared	O
Task	O

We	O
have	O
implemented	O
the	O
following	O
GEC	O
rules	O
.	O
(	O
1	O
)	O
'	O
a	O
'	O
and	O
'	O
an	O
'	O
substitution	O
.	O
For	O
this	O
problem	O
,	O
we	O
made	O
rules	O
based	O
on	O
the	O
first	O
phoneme	O
of	O
the	O
following	O
word	O
.	O
(	O
2	O
)	O
Comma	O
deletion	O
.	O
After	O
a	O
prepositional	O
phrase	O
at	O
the	O
beginning	O
of	O
a	O
sentence	O
,	O
we	O
add	O
a	O
comma	O
.	O
For	O
example	O
,	O
"	O
Despite	O
our	O
differences	O
we	O
collaborate	O
well	O
.	O
"	O
A	O
comma	O
should	O
be	O
added	O
after	O
Despite	O
our	O
differences	O
.	O
(	O
3	O
)	O
Orthography	O
mistakes	O
.	O
We	O
obtain	O
statistics	O
of	O
named	O
entities	O
that	O
require	O
initial	O
capitalization	O
and	O
make	O
a	O
white	O
list	O
using	O
the	O
Wikipedia	O
corpus	O
.	O
If	O
a	O
word	O
is	O
on	O
the	O
white	O
list	O
,	O
we	O
will	O
force	O
the	O
conversion	O
to	O
the	O
initial	O
capitalization	O
form	O
.	O
In	O
addition	O
,	O
we	O
use	O
Pyenchant	O
as	O
our	O
spell	O
checker	O
(	O
Kelly	O
,	O
2006	O
)	O
.	O
The	O
top	O
candidate	O
is	O
considered	O
to	O
be	O
the	O
correction	O
.	O

We	O
use	O
one	O
conflict	O
solver	O
to	O
combine	O
the	O
outputs	O
from	O
all	O
of	O
the	O
systems	O
in	O
this	O
task	O
.	O
Parameters	O
for	O
this	O
ensemble	O
system	O
are	O
shown	O
in	O
Table	O
7	O
.	O
sults	O
on	O
the	O
test	O
set	O
.	O
We	O
can	O
see	O
that	O
the	O
base	O
systems	O
are	O
not	O
very	O
strong	O
,	O
and	O
the	O
ensemble	O
system	O
significantly	O
improves	O
the	O
performance	O
.	O
The	O
difference	O
between	O
the	O
development	O
set	O
and	O
test	O
set	O
can	O
still	O
be	O
observed	O
in	O
this	O
task	O
.	O

Unsupervised	O
Detection	O
of	O
Argumentative	O
Units	O
though	O
Topic	O
Modeling	O
Techniques	O

Given	O
a	O
document	O
corpus	O
,	O
topic	O
modeling	O
techniques	O
can	O
be	O
employed	O
to	O
discover	O
the	O
most	O
representative	O
topics	O
throughout	O
the	O
corpus	O
,	O
and	O
to	O
provide	O
an	O
assignment	O
of	O
documents	O
to	O
topics	O
,	O
meaning	O
that	O
the	O
higher	O
is	O
the	O
assignment	O
value	O
of	O
a	O
document	O
to	O
a	O
certain	O
topic	O
,	O
the	O
higher	O
is	O
the	O
probability	O
that	O
the	O
document	O
is	O
"	O
focused	O
"	O
on	O
that	O
topic	O
.	O
The	O
idea	O
of	O
A2	O
T	O
is	O
that	O
an	O
argumentative	O
unit	O
is	O
a	O
sentence	O
highly	O
focused	O
on	O
a	O
specific	O
topic	O
,	O
namely	O
a	O
sentence	O
with	O
high	O
assignment	O
value	O
to	O
a	O
certain	O
topic	O
and	O
low	O
assignment	O
value	O
to	O
the	O
other	O
topics	O
.	O
To	O
this	O
end	O
,	O
A2	O
T	O
introduces	O
the	O
notion	O
of	O
attraction	O
with	O
the	O
aim	O
at	O
recognizing	O
the	O
sentences	O
highly	O
focused	O
on	O
specific	O
topics	O
,	O
that	O
represent	O
the	O
recognized	O
argumentative	O
units	O
.	O
In	O
the	O
following	O
,	O
the	O
A2	O
T	O
approach	O
and	O
related	O
techniques	O
are	O
described	O
in	O
detail	O
.	O

The	O
schema	O
of	O
the	O
A2	O
T	O
approach	O
is	O
shown	O
in	O
Figure	O
1	O
.	O
Consider	O
a	O
corpus	O
of	O
texts	O
C	O
=	O
{	O
c	O
1	O
,	O
.	O
.	O
.	O
,	O
c	O
n	O
}	O
,	O
where	O
a	O
text	O
c	O
i	O
C	O
is	O
a	O
sequence	O
of	O
sentences	O
,	O
like	O
for	O
example	O
an	O
essay	O
,	O
a	O
web	O
page	O
/	O
post	O
,	O
or	O
a	O
scientific	O
paper	O
.	O
The	O
ultimate	O
goal	O
of	O
the	O
A2	O
T	O
approach	O
is	O
to	O
derive	O
a	O
set	O
of	O
argumentative	O
units	O
U	O
=	O
{	O
s	O
1	O
,	O
c	O
,	O
l	O
,	O
.	O
.	O
.	O
,	O
s	O
h	O
,	O
c	O
,	O
l	O
}	O
,	O
where	O

Units	O
(	O
U	O
)	O

As	O
a	O
second	O
experiment	O
,	O
we	O
exploited	O
probabilities	O
associated	O
with	O
sentences	O
to	O
perform	O
a	O
ranked	O
evaluation	O
.	O
In	O
particular	O
,	O
we	O
calculated	O
two	O
measures	O
,	O
namely	O
P	O
that	O
is	O
the	O
area	O
the	O
under	O
the	O
precision	O
-	O
recall	O
curve	O
and	O
R	O
that	O
is	O
the	O
area	O
under	O
the	O
receiver	O
operating	O
characteristic	O
(	O
ROC	O
)	O
curve	O
.	O
In	O
this	O
experiments	O
,	O
we	O
used	O
different	O
criteria	O
for	O
defining	O
the	O
true	O
labels	O
:	O
in	O
P	O
CM	O
,	O
an	O
annotated	O
sentence	O
in	O
the	O
corpus	O
is	O
considered	O
a	O
true	O
argumentative	O
unit	O
if	O
it	O
is	O
either	O
a	O
premise	O
,	O
a	O
claim	O
,	O
or	O
a	O
major	O
claim	O
;	O
in	O
CM	O
only	O
claims	O
and	O
major	O
claims	O
are	O
taken	O
as	O
valid	O
au	O
;	O
in	O
M	O
only	O
major	O
claims	O
are	O
taken	O
into	O
account	O
.	O
Results	O
are	O
reported	O
in	O
Table	O
1	O
.	O

A	O
first	O
evidence	O
emerging	O
from	O
the	O
analysis	O
of	O
confusion	O
matrices	O
for	O
both	O
corpora	O
C1	O
and	O
C2	O
is	O
that	O
the	O
role	O
of	O
sentences	O
is	O
strictly	O
dependent	O
on	O
the	O
type	O
of	O
documents	O
.	O
C1	O
contains	O
structured	O
essays	O
of	O
various	O
topics	O
,	O
while	O
C2	O
provides	O
conversational	O
texts	O
extracted	O
from	O
blogs	O
and	O
chats	O
.	O
In	O
the	O
first	O
case	O
,	O
the	O
number	O
of	O
argumentative	O
units	O
is	O
higher	O
than	O
in	O
the	O
second	O
one	O
.	O
In	O
particular	O
,	O
for	O
C2	O
we	O
overestimated	O
the	O
probability	O
of	O
sentences	O
to	O
be	O
an	O
argumentative	O
unit	O
.	O
This	O
is	O
mainly	O
due	O
to	O
the	O
fact	O
that	O
those	O
sentences	O
contain	O
words	O
that	O
are	O
semantically	O
related	O
to	O
the	O
main	O
topic	O
of	O
the	O
conversation	O
although	O
they	O
are	O
not	O
playing	O
a	O
role	O
in	O
the	O
argumentation	O
.	O
An	O
example	O
is	O
the	O
following	O
sentence	O
,	O
taken	O
from	O
a	O
document	O
associated	O
with	O
the	O
topic	O
"	O
school	O
"	O
:	O
"	O
why	O
do	O
some	O
parents	O
not	O
think	O
their	O
kids	O
can	O
attain	O
?	O
"	O
.	O
The	O
sentence	O
is	O
clearly	O
part	O
of	O
a	O
conversation	O
and	O
it	O
has	O
been	O
annotated	O
as	O
a	O
non	O
argumentative	O
unit	O
because	O
it	O
is	O
a	O
question	O
.	O
However	O
,	O
since	O
it	O
contains	O
words	O
that	O
are	O
relevant	O
for	O
the	O
topic	O
(	O
i.e.	O
,	O
parents	O
,	O
kids	O
,	O
attain	O
)	O
,	O
A2	O
T	O
associates	O
the	O
sentence	O
with	O
a	O
good	O
level	O
of	O
attraction	O
,	O
labeling	O
it	O
as	O
a	O
premise	O
.	O
In	O
order	O
to	O
address	O
this	O
kind	O
of	O
false	O
positives	O
,	O
we	O
aim	O
in	O
our	O
future	O
work	O
to	O
study	O
the	O
dependency	O
relations	O
among	O
sentences	O
in	O
text	O
(	O
such	O
as	O
questionanswers	O
)	O
to	O
the	O
goal	O
of	O
achieving	O
a	O
better	O
insight	O
of	O
the	O
sentences	O
role	O
.	O
A	O
second	O
lesson	O
learned	O
from	O
error	O
analysis	O
concerns	O
the	O
distinction	O
between	O
claims	O
and	O
premises	O
.	O
This	O
confusion	O
is	O
evident	O
especially	O
when	O
dealing	O
with	O
corpus	O
C1	O
.	O
An	O
example	O
is	O
given	O
by	O
the	O
following	O
two	O
sentences	O
,	O
taken	O
from	O
an	O
essay	O
about	O
the	O
role	O
of	O
sports	O
in	O
favor	O
of	O
peace	O
.	O
(	O
s1	O
)	O
for	O
example	O
,	O
when	O
Irak	O
was	O
hardly	O
struck	O
by	O
the	O
second	O
gulf	O
war	O
,	O
its	O
citizens	O
tried	O
to	O
catch	O
any	O
incoming	O
news	O
about	O
the	O
footballworld	O
cup	O
through	O
their	O
portable	O
receivers	O
.	O
(	O
s2	O
)	O
thus	O
,	O
world	O
sports	O
events	O
strongly	O
participate	O
in	O
eventually	O
pulling	O
back	O
people	O
towards	O
friendship	O
and	O
peace	O
The	O
sentence	O
(	O
s1	O
)	O
has	O
been	O
annotated	O
as	O
a	O
premise	O
,	O
while	O
(	O
s2	O
)	O
as	O
a	O
claim	O
.	O
In	O
our	O
classification	O
,	O
they	O
are	O
both	O
claims	O
.	O
The	O
reason	O
is	O
that	O
they	O
both	O
contain	O
topic	O
-	O
related	O
words	O
and	O
their	O
position	O
in	O
text	O
is	O
similar	O
.	O
The	O
main	O
distinction	O
is	O
the	O
presence	O
of	O
the	O
expression	O
"	O
for	O
example	O
"	O
in	O
the	O
first	O
sentence	O
which	O
qualifies	O
it	O
as	O
a	O
premise	O
.	O
To	O
this	O
end	O
,	O
in	O
our	O
future	O
work	O
we	O
aim	O
at	O
adding	O
some	O
special	O
words	O
(	O
such	O
as	O
"	O
for	O
example	O
"	O
,	O
"	O
therefore	O
"	O
)	O
in	O
the	O
background	O
knowledge	O
of	O
the	O
classifier	O
,	O
in	O
order	O
to	O
improve	O
the	O
capability	O
of	O
discriminating	O
premises	O
and	O
claims	O
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
the	O
"	O
Attraction	O
to	O
Topics	O
"	O
-	O
A2	O
T	O
unsupervised	O
approach	O
for	O
detecting	O
argumentative	O
discourse	O
units	O
,	O
at	O
sentence	O
-	O
level	O
granularity	O
.	O
Motivated	O
by	O
the	O
observation	O
that	O
topic	O
information	O
is	O
frequently	O
employed	O
as	O
a	O
sub	O
-	O
task	O
in	O
the	O
process	O
of	O
manual	O
annotation	O
of	O
arguments	O
,	O
we	O
propose	O
an	O
approach	O
that	O
exploits	O
topic	O
modeling	O
techniques	O
in	O
order	O
to	O
identify	O
argumentative	O
units	O
.	O
Since	O
manual	O
supervision	O
is	O
not	O
required	O
,	O
A2	O
T	O
has	O
the	O
potential	O
to	O
be	O
applicable	O
on	O
documents	O
of	O
various	O
genres	O
and	O
domains	O
.	O
Preliminary	O
evaluation	O
results	O
on	O
two	O
different	O
corpora	O
are	O
promising	O
.	O
First	O
,	O
A2	O
T	O
performs	O
significantly	O
better	O
than	O
the	O
baseline	O
on	O
argumentative	O
sentence	O
detection	O
on	O
both	O
corpora	O
.	O
Second	O
,	O
A2	O
T	O
exhibits	O
good	O
results	O
for	O
classifying	O
argumentative	O
sentences	O
as	O
major	O
claims	O
,	O
claims	O
,	O
premises	O
,	O
and	O
non	O
-	O
argumentative	O
units	O
,	O
at	O
least	O
for	O
the	O
first	O
corpus	O
,	O
which	O
has	O
a	O
low	O
rate	O
of	O
non	O
-	O
argumentative	O
sentences	O
(	O
20	O
%	O
)	O
.	O
Regarding	O
directions	O
for	O
further	O
research	O
,	O
there	O
are	O
several	O
axes	O
that	O
can	O
be	O
explored	O
.	O
Evaluation	O
on	O
a	O
larger	O
set	O
of	O
annotation	O
corpora	O
will	O
provide	O
enhanced	O
insights	O
about	O
the	O
performance	O
of	O
the	O
proposed	O
approach	O
on	O
different	O
document	O
types	O
.	O
Our	O
preliminary	O
results	O
showed	O
that	O
despite	O
good	O
recall	O
on	O
multiple	O
corpora	O
,	O
achieving	O
also	O
good	O
precision	O
can	O
be	O
a	O
challenging	O
task	O
in	O
documents	O
where	O
argumentative	O
units	O
are	O
sparse	O
,	O
and	O
false	O
positives	O
can	O
be	O
an	O
issue	O
.	O
In	O
this	O
context	O
,	O
we	O
would	O
like	O
to	O
also	O
exploit	O
other	O
types	O
of	O
relations	O
,	O
and	O
extend	O
our	O
method	O
with	O
other	O
kinds	O
of	O
similarities	O
over	O
sentences	O
.	O

In	O
this	O
section	O
,	O
we	O
further	O
analyse	O
UDapter	O
to	O
understand	O
its	O
impact	O
on	O
different	O
languages	O
,	O
and	O
the	O
importance	O
of	O
its	O
various	O
components	O
.	O

UDapter	O
learns	O
language	O
embeddings	O
from	O
syntactic	O
,	O
phonological	O
and	O
phonetic	O
inventory	O
features	O
.	O
A	O
natural	O
alternative	O
to	O
this	O
choice	O
is	O
to	O
learn	O
language	O
embeddings	O
from	O
scratch	O
.	O
For	O
a	O
comparison	O
,	O
we	O
train	O
a	O
model	O
where	O
,	O
for	O
each	O
in	O
-	O
training	O
language	O
,	O
a	O
separate	O
language	O
embedding	O
(	O
of	O
the	O
same	O
size	O
:	O
32	O
)	O
is	O
initialized	O
randomly	O
and	O
learned	O
end	O
-	O
toend	O
.	O
For	O
the	O
zero	O
-	O
shot	O
languages	O
we	O
use	O
the	O
average	O
,	O
or	O
centroid	O
,	O
of	O
all	O
in	O
-	O
training	O
language	O
embeddings	O
.	O
As	O
shown	O
in	O
Figure	O
4a	O
,	O
on	O
the	O
high	O
-	O
resource	O
set	O
,	O
the	O
models	O
with	O
and	O
without	O
typological	O
features	O
achieve	O
very	O
similar	O
average	O
LAS	O
(	O
87.3	O
and	O
87.1	O
respectively	O
)	O
.	O
On	O
zero	O
-	O
shot	O
languages	O
,	O
however	O
,	O
the	O
use	O
of	O
centroid	O
embedding	O
performs	O
very	O
poorly	O
:	O
9.0	O
vs	O
36.5	O
average	O
LAS	O
score	O
over	O
30	O
languages	O
.	O
As	O
already	O
discussed	O
in	O
4.1	O
(	O
Table	O
2	O
)	O
,	O
using	O
a	O
proxy	O
language	O
embedding	O
belonging	O
to	O
the	O
same	O
family	O
as	O
the	O
test	O
language	O
,	O
when	O
available	O
,	O
also	O
clearly	O
underperforms	O
UDapter	O
.	O
These	O
results	O
confirm	O
our	O
expectation	O
that	O
a	O
model	O
can	O
learn	O
reliable	O
language	O
embeddings	O
for	O
in	O
-	O
training	O
languages	O
,	O
however	O
typological	O
signals	O
are	O
required	O
to	O
obtain	O
a	O
robust	O
parsing	O
quality	O
on	O
zero	O
-	O
shot	O
languages	O
.	O

Details	O
of	O
training	O
and	O
zero	O
-	O
shot	O
languages	O
such	O
as	O
language	O
code	O
,	O
data	O
size	O
(	O
number	O
of	O
sentences	O
)	O
,	O
and	O
family	O
are	O
given	O
in	O
Table	O
5	O
and	O
Table	O
6	O
.	O

Arianna	O
Bisazza	O
was	O
partly	O
funded	O
by	O
the	O
Netherlands	O
Organization	O
for	O
Scientific	O
Research	O
(	O
NWO	O
)	O
under	O
project	O
number	O
639.021.646	O
.	O
We	O
would	O
like	O
to	O
thank	O
the	O
Center	O
for	O
Information	O
Technology	O
of	O
the	O
University	O
of	O
Groningen	O
for	O
providing	O
access	O
to	O
the	O
Peregrine	O
HPC	O
cluster	O
and	O
the	O
anonymous	O
reviewers	O
for	O
their	O
helpful	O
comments	O
.	O

In	O
Section	O
[	O
1	O
]	O
we	O
provided	O
a	O
brief	O
summary	O
of	O
each	O
sub	O
-	O
task	O
in	O
which	O
we	O
participated	O
.	O
For	O
each	O
of	O
them	O
,	O
participants	O
were	O
given	O
access	O
to	O
a	O
labeled	O
training	O
and	O
validation	O
set	O
,	O
as	O
well	O
as	O
an	O
unlabeled	O
evaluation	O
set	O
that	O
was	O
used	O
to	O
determined	O
the	O
final	O
performance	O
of	O
the	O
submitted	O
systems	O
.	O

In	O
this	O
section	O
,	O
we	O
give	O
a	O
brief	O
description	O
of	O
the	O
system	O
we	O
used	O
to	O
conduct	O
our	O
experiments	O
,	O
share	O
our	O
results	O
and	O
provide	O
a	O
brief	O
discussion	O
.	O

The	O
model	O
was	O
developed	O
using	O
the	O
PyTorch	O
(	O
Paszke	O
et	O
al	O
,	O
2019	O
)	O
machine	O
with	O
an	O
Intel	O
Core	O
i9	O
-	O
9820X	O
CPU	O
@	O
3.30GHz	O
and	O
a	O
NVIDIA	O
GeForce	O
RTX	O
2080	O
Ti	O
GPU	O
with	O
11	O
GB	O
of	O
memory	O
.	O

We	O
predict	O
complex	O
SQL	O
clause	O
-	O
wisely	O
as	O
described	O
in	O
Figure	O
1	O
.	O
Each	O
clause	O
is	O
predicted	O
consecutively	O
by	O
at	O
most	O
three	O
different	O
types	O
of	O
modules	O
(	O
sketch	O
,	O
column	O
,	O
operator	O
)	O
.	O
The	O
same	O
architecture	O
recursively	O
predicts	O
nested	O
queries	O
with	O
temporal	O
predicted	O
SQL	O
as	O
an	O
additional	O
input	O
.	O

After	O
the	O
predictions	O
of	O
all	O
the	O
other	O
clauses	O
,	O
we	O
use	O
a	O
heuristic	O
to	O
generate	O
the	O
FROM	O
clause	O
.	O
We	O
first	O
collect	O
all	O
the	O
columns	O
that	O
appear	O
in	O
the	O
predicted	O
SQL	O
,	O
and	O
then	O
we	O
JOIN	O
tables	O
that	O
include	O
these	O
predicted	O
columns	O
.	O

To	O
predict	O
the	O
presence	O
of	O
a	O
sub	O
-	O
query	O
,	O
we	O
train	O
another	O
module	O
that	O
has	O
the	O
same	O
architecture	O
as	O
the	O
operator	O
prediction	O
module	O
.	O
Instead	O
of	O
predicting	O
corresponding	O
operators	O
for	O
each	O
column	O
,	O
it	O
predicts	O
whether	O
each	O
column	O
is	O
compared	O
to	O
a	O
variable	O
(	O
e.g.	O
,	O
WHERE	O
age	O
>	O
3	O
)	O
or	O
to	O
a	O
sub	O
-	O
query	O
(	O
e.g.	O
,	O
WHERE	O
age	O
>	O
(	O
SELECT	O
avg	O
(	O
age	O
)	O
..	O
)	O
)	O
.	O
This	O
input	O
is	O
encoded	O
in	O
the	O
same	O
way	O
as	O
question	O
encoding	O
described	O
in	O
Section	O
3.1	O
.	O
Then	O
,	O
the	O
rest	O
of	O
the	O
SQL	O
generation	O
process	O
is	O
identical	O
to	O
that	O
described	O
in	O
Section	O
3.2	O
-	O
3.4	O
.	O
After	O
the	O
sub	O
-	O
query	O
is	O
predicted	O
,	O
it	O
replaces	O
the	O
[	O
SUB	O
QUERY	O
]	O
token	O
to	O
form	O
the	O
final	O
query	O
.	O

We	O
thank	O
Yongsik	O
Lee	O
,	O
Jaesik	O
Yoon	O
,	O
and	O
Donghun	O
Lee	O
(	O
SAP	O
)	O
for	O
their	O
reviews	O
and	O
support	O
.	O
We	O
also	O
thank	O
professor	O
Sungroh	O
Yoon	O
,	O
Jongyun	O
Song	O
,	O
and	O
Taeuk	O
Kim	O
(	O
Seoul	O
National	O
University	O
)	O
for	O
their	O
insightful	O
feedback	O
and	O
three	O
anonymous	O
reviewers	O
for	O
their	O
helpful	O
comments	O
.	O

In	O
Table	O
3	O
,	O
we	O
show	O
some	O
examples	O
of	O
predicted	O
SQL	O
queries	O
from	O
different	O
models	O
.	O
We	O
compare	O
the	O
result	O
of	O
our	O
model	O
with	O
two	O
of	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
:	O
SyntaxSQLNet	O
(	O
Yu	O
et	O
al	O
,	O
2018b	O
)	O
and	O
the	O
modified	O
version	O
of	O
SQLNet	O
(	O
Xu	O
et	O
al	O
,	O
2017	O
)	O
by	O
Yu	O
et	O
al	O
(	O
2018c	O
)	O
to	O
support	O
complex	O
SQL	O
queries	O
.	O

Psycholinguistic	O
Models	O
of	O
Sentence	O
Processing	O
Improve	O
Sentence	O
Readability	O
Ranking	O

While	O
previous	O
research	O
on	O
readability	O
has	O
typically	O
focused	O
on	O
document	O
-	O
level	O
measures	O
,	O
recent	O
work	O
in	O
areas	O
such	O
as	O
natural	O
language	O
generation	O
has	O
pointed	O
out	O
the	O
need	O
of	O
sentence	O
-	O
level	O
readability	O
measures	O
.	O
Much	O
of	O
psycholinguistics	O
has	O
focused	O
for	O
many	O
years	O
on	O
processing	O
measures	O
that	O
provide	O
difficulty	O
estimates	O
on	O
a	O
word	O
-	O
by	O
-	O
word	O
basis	O
.	O
However	O
,	O
these	O
psycholinguistic	O
measures	O
have	O
not	O
yet	O
been	O
tested	O
on	O
sentence	O
readability	O
ranking	O
tasks	O
.	O
In	O
this	O
paper	O
,	O
we	O
use	O
four	O
psycholinguistic	O
measures	O
:	O
idea	O
density	O
,	O
surprisal	O
,	O
integration	O
cost	O
,	O
and	O
embedding	O
depth	O
to	O
test	O
whether	O
these	O
features	O
are	O
predictive	O
of	O
readability	O
levels	O
.	O
We	O
find	O
that	O
psycholinguistic	O
features	O
significantly	O
improve	O
performance	O
by	O
up	O
to	O
3	O
percentage	O
points	O
over	O
a	O
standard	O
document	O
-	O
level	O
readability	O
metric	O
baseline	O
.	O

We	O
have	O
already	O
explained	O
the	O
experimental	O
findings	O
of	O
Kintsch	O
&	O
Keenan	O
(	O
1973	O
)	O
with	O
respect	O
to	O
idea	O
density	O
,	O
but	O
what	O
behavioral	O
evidence	O
is	O
there	O
to	O
suggest	O
that	O
the	O
remaining	O
theories	O
are	O
valid	O
?	O
Demberg	O
&	O
Keller	O
(	O
2008	O
)	O
examined	O
the	O
relationship	O
between	O
both	O
surprisal	O
and	O
integration	O
cost	O
and	O
eye	O
-	O
tracking	O
times	O
in	O
the	O
Dundee	O
corpus	O
(	O
Kennedy	O
and	O
Pynte	O
,	O
2005	O
)	O
Demberg	O
&	O
Keller	O
found	O
that	O
increased	O
surprisal	O
significantly	O
correlated	O
with	O
reading	O
times	O
.	O
Although	O
they	O
found	O
that	O
integration	O
cost	O
did	O
not	O
significantly	O
contribute	O
to	O
predicting	O
eye	O
-	O
tracking	O
reading	O
times	O
in	O
general	O
,	O
its	O
contribution	O
was	O
significant	O
when	O
restricted	O
to	O
nouns	O
and	O
verbs	O
.	O
They	O
also	O
found	O
that	O
surprisal	O
and	O
integration	O
cost	O
were	O
uncorrelated	O
,	O
suggesting	O
that	O
they	O
should	O
be	O
considered	O
complementary	O
factors	O
in	O
a	O
model	O
of	O
reading	O
times	O
.	O
Another	O
eyetracking	O
study	O
divided	O
surprisal	O
into	O
lexical	O
and	O
synactic	O
components	O
,	O
finding	O
that	O
lexical	O
surprisal	O
was	O
a	O
significant	O
factor	O
but	O
not	O
syntactic	O
surprisal	O
(	O
Roark	O
et	O
al	O
,	O
2009	O
)	O
.	O
Wu	O
et	O
al	O
(	O
2010	O
)	O
examined	O
surprisal	O
,	O
entropy	O
reduction	O
,	O
and	O
embedding	O
depth	O
in	O
a	O
study	O
of	O
psycholinguistic	O
complexity	O
metrics	O
.	O
Their	O
study	O
of	O
the	O
reading	O
times	O
of	O
23	O
native	O
English	O
speakers	O
reading	O
four	O
narratives	O
indicated	O
that	O
embedding	O
difference	O
was	O
a	O
significant	O
predictor	O
of	O
reading	O
times	O
for	O
closed	O
class	O
words	O
.	O
Moreover	O
,	O
this	O
contribution	O
was	O
independent	O
of	O
the	O
contribution	O
of	O
surprisal	O
,	O
indicating	O
that	O
the	O
two	O
measures	O
are	O
capturing	O
different	O
components	O
of	O
the	O
variance	O
in	O
reading	O
times	O
.	O
Since	O
integration	O
cost	O
was	O
a	O
significant	O
predictor	O
of	O
reading	O
times	O
for	O
nouns	O
and	O
verbs	O
(	O
i.e.	O
not	O
closed	O
class	O
words	O
)	O
and	O
embedding	O
depth	O
was	O
a	O
significant	O
predictor	O
of	O
reading	O
times	O
for	O
closed	O
class	O
words	O
,	O
integration	O
cost	O
and	O
embedding	O
depth	O
should	O
also	O
be	O
complementary	O
to	O
each	O
other	O
.	O

We	O
examined	O
features	O
for	O
the	O
ranking	O
of	O
sentences	O
by	O
their	O
complexity	O
,	O
training	O
linear	O
models	O
on	O
two	O
corpora	O
using	O
features	O
derived	O
from	O
psycholinguistic	O
theories	O
of	O
online	O
sentence	O
processing	O
:	O
idea	O
density	O
,	O
surprisal	O
,	O
integration	O
cost	O
,	O
and	O
embedding	O
depth	O
.	O
Surprisal	O
coupled	O
with	O
embedding	O
depth	O
and	O
our	O
baseline	O
features	O
(	O
average	O
word	O
length	O
&	O
sentence	O
length	O
)	O
performed	O
as	O
well	O
as	O
the	O
full	O
model	O
across	O
all	O
subsets	O
of	O
the	O
OSE	O
corpus	O
.	O
Integration	O
cost	O
and	O
idea	O
density	O
were	O
less	O
effective	O
,	O
suggesting	O
that	O
the	O
gain	O
in	O
speed	O
from	O
running	O
a	O
faster	O
dependency	O
parser	O
may	O
not	O
be	O
worth	O
it	O
.	O
Instead	O
,	O
it	O
is	O
necessary	O
to	O
use	O
the	O
slower	O
ModelBlocks	O
parser	O
to	O
extract	O
the	O
more	O
useful	O
features	O
.	O
Overall	O
,	O
our	O
strongest	O
model	O
combined	O
the	O
baseline	O
features	O
and	O
the	O
online	O
psycholinguistic	O
features	O
.	O
Because	O
these	O
features	O
are	O
complementary	O
to	O
features	O
which	O
have	O
been	O
explored	O
in	O
other	O
work	O
(	O
Vajjala	O
and	O
Meurers	O
,	O
2014	O
;	O
Ambati	O
et	O
al	O
,	O
2016	O
)	O
,	O
the	O
next	O
step	O
in	O
future	O
work	O
is	O
to	O
combine	O
all	O
of	O
these	O
features	O
and	O
conduct	O
a	O
more	O
comparison	O
between	O
the	O
features	O
proposed	O
here	O
and	O
those	O
examined	O
in	O
earlier	O
work	O
.	O
In	O
the	O
meantime	O
,	O
we	O
have	O
demonstrated	O
that	O
features	O
derived	O
from	O
psycholinguistic	O
theories	O
of	O
sentence	O
processing	O
can	O
be	O
used	O
to	O
improve	O
models	O
for	O
ranking	O
sentences	O
by	O
readability	O
.	O

Thanks	O
are	O
due	O
to	O
Matthew	O
Crocker	O
,	O
Michael	O
White	O
,	O
Eric	O
Fosler	O
-	O
Lussier	O
,	O
William	O
Schuler	O
,	O
Detmar	O
Meurers	O
,	O
Marten	O
van	O
Schijndel	O
,	O
and	O
Sowmya	O
Vajjala	O
for	O
discussions	O
and	O
guidance	O
during	O
the	O
development	O
of	O
this	O
work	O
.	O
We	O
are	O
supported	O
by	O
DFG	O
collaborative	O
research	O
center	O
SFB	O
1102	O
'	O
Information	O
Density	O
and	O
Linguistic	O
Encoding	O
'	O
.	O

Dense	O
Procedure	O
Captioning	O
in	O
Narrated	O
Instructional	O
Videos	O

Understanding	O
narrated	O
instructional	O
videos	O
is	O
important	O
for	O
both	O
research	O
and	O
real	O
-	O
world	O
web	O
applications	O
.	O
Motivated	O
by	O
video	O
dense	O
captioning	O
,	O
we	O
propose	O
a	O
model	O
to	O
generate	O
procedure	O
captions	O
from	O
narrated	O
instructional	O
videos	O
which	O
are	O
a	O
sequence	O
of	O
stepwise	O
clips	O
with	O
description	O
.	O
Previous	O
works	O
on	O
video	O
dense	O
captioning	O
learn	O
video	O
segments	O
and	O
generate	O
captions	O
without	O
considering	O
transcripts	O
.	O
We	O
argue	O
that	O
transcripts	O
in	O
narrated	O
instructional	O
videos	O
can	O
enhance	O
video	O
representation	O
by	O
providing	O
fine	O
-	O
grained	O
complimentary	O
and	O
semantic	O
textual	O
information	O
.	O
In	O
this	O
paper	O
,	O
we	O
introduce	O
a	O
framework	O
to	O
(	O
1	O
)	O
extract	O
procedures	O
by	O
a	O
cross	O
-	O
modality	O
module	O
,	O
which	O
fuses	O
video	O
content	O
with	O
the	O
entire	O
transcript	O
;	O
and	O
(	O
2	O
)	O
generate	O
captions	O
by	O
encoding	O
video	O
frames	O
as	O
well	O
as	O
a	O
snippet	O
of	O
transcripts	O
within	O
each	O
extracted	O
procedure	O
.	O
Experiments	O
show	O
that	O
our	O
model	O
can	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
procedure	O
extraction	O
and	O
captioning	O
,	O
and	O
the	O
ablation	O
studies	O
demonstrate	O
that	O
both	O
the	O
video	O
frames	O
and	O
the	O
transcripts	O
are	O
important	O
for	O
the	O
task	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
our	O
framework	O
and	O
model	O
details	O
as	O
shown	O
in	O
Figure	O
2	O
.	O
First	O
,	O
we	O
adopt	O
a	O
context	O
-	O
aware	O
video	O
-	O
transcript	O
fusion	O
module	O
to	O
generate	O
features	O
by	O
fusing	O
video	O
information	O
and	O
transcript	O
embedding	O
;	O
Then	O
the	O
procedure	O
extraction	O
module	O
takes	O
the	O
embedded	O
features	O
and	O
predicts	O
procedures	O
with	O
various	O
lengths	O
;	O
Finally	O
,	O
the	O
procedure	O
captioning	O
module	O
generates	O
captions	O
for	O
each	O
procedure	O
by	O
an	O
encoder	O
-	O
decoder	O
based	O
model	O
.	O

We	O
take	O
the	O
encoded	O
T	O
feature	O
vectors	O
F	O
of	O
each	O
video	O
as	O
the	O
elementary	O
units	O
to	O
generate	O
procedure	O
proposals	O
.	O
We	O
follow	O
the	O
idea	O
in	O
(	O
Zhou	O
et	O
al	O
,	O
2018a	O
;	O
Krishna	O
et	O
al	O
,	O
2017	O
)	O
that	O
(	O
1	O
)	O
generate	O
a	O
lot	O
of	O
anchors	O
,	O
i.e.	O
proposals	O
,	O
with	O
different	O
lengths	O
and	O
(	O
2	O
)	O
use	O
the	O
frame	O
features	O
within	O
a	O
proposal	O
span	O
to	O
predict	O
plausible	O
scores	O
.	O

In	O
order	O
to	O
generate	O
different	O
-	O
sized	O
procedure	O
proposals	O
,	O
we	O
adopt	O
a	O
1D	O
(	O
temporal	O
)	O
convolutional	O
layer	O
with	O
the	O
setting	O
of	O
K	O
different	O
kernels	O
;	O
three	O
output	O
channels	O
and	O
zero	O
padding	O
to	O
generate	O
procedure	O
candidates	O
.	O
The	O
layer	O
takes	O
F	O
R	O
T	O
×f	O
as	O
input	O
and	O
outputs	O
a	O
list	O
of	O
M	O
(	O
k	O
)	O
R	O
T	O
×3	O
for	O
each	O
k	O
-	O
th	O
kernel	O
.	O
All	O
these	O
results	O
are	O
stacked	O
as	O
a	O
tensor	O
M	O
R	O
K×T	O
×3	O
.	O
Next	O
,	O
the	O
tensor	O
M	O
is	O
divided	O
into	O
three	O
matrices	O
:	O
M	O
=	O
M	O
m	O
,	O
M	O
l	O
,	O
M	O
s	O
whereM	O
m	O
,	O
M	O
l	O
,	O
M	O
s	O
R	O
K×T	O
,	O
They	O
are	O
designed	O
to	O
represent	O
the	O
offset	O
of	O
the	O
proposal	O
's	O
midpoint	O
;	O
the	O
offset	O
of	O
the	O
proposal	O
's	O
length	O
and	O
the	O
prediction	O
score	O
.	O
We	O
calculate	O
the	O
starting	O
and	O
ending	O
timestamp	O
of	O
each	O
proposal	O
by	O
the	O
offset	O
of	O
midpoint	O
and	O
length	O
.	O
Finally	O
,	O
a	O
non	O
-	O
linear	O
projection	O
is	O
applied	O
on	O
each	O
matrix	O
:	O
M	O
m	O
=	O
tanh	O
(	O
M	O
m	O
)	O
,	O
M	O
l	O
=	O
tanh	O
(	O
M	O
l	O
)	O
,	O
M	O
s	O
=	O
σ	O
(	O
M	O
s	O
)	O
where	O
σ	O
is	O
the	O
Sigmoid	O
projection	O
.	O

We	O
conduct	O
the	O
ablation	O
experiments	O
to	O
show	O
the	O
effectiveness	O
of	O
utilizing	O
transcripts	O
.	O
Table	O
3	O
lists	O
the	O
results	O
.	O
The	O
Video	O
Only	O
Model	O
only	O
relies	O
on	O
video	O
information	O
for	O
all	O
modules	O
.	O
The	O
Captioning	O
by	O
Video	O
Model	O
fuses	O
transcripts	O
during	O
the	O
procedure	O
extraction	O
which	O
shows	O
the	O
transcript	O
is	O
effective	O
for	O
the	O
extracting	O
procedure	O
.	O
The	O
Caption	O
by	O
Transcript	O
Model	O
only	O
uses	O
transcripts	O
for	O
captioning	O
.	O
Compared	O
with	O
the	O
Caption	O
by	O
Video	O
Model	O
,	O
we	O
find	O
that	O
only	O
using	O
transcripts	O
for	O
captioning	O
decreases	O
performance	O
.	O
The	O
reason	O
is	O
that	O
only	O
using	O
transcripts	O
for	O
captioning	O
will	O
miss	O
several	O
actions	O
appearing	O
in	O
the	O
video	O
but	O
not	O
mentioned	O
in	O
the	O
transcript	O
.	O
The	O
full	O
Model	O
achieves	O
state	O
-	O

(	O
1.1	O
)	O
mix	O
the	O
eggs	O
and	O
mix	O
in	O
a	O
bowl	O
(	O
1.2	O
)	O
mix	O
the	O
eggs	O
in	O
a	O
bowl	O
(	O
1.3	O
)	O
cut	O
the	O
meat	O
into	O
pieces	O
(	O
1.4	O
)	O
mix	O
some	O
olive	O
oil	O
in	O
a	O
bowl	O
(	O
1.5	O
)	O
add	O
salt	O
and	O
pepper	O
and	O
pepper	O
to	O
the	O
bowl	O
(	O
1.6	O
)	O
mix	O
the	O
sauce	O
and	O
mix	O
(	O
1.7	O
)	O
pour	O
the	O
sauce	O
in	O
the	O
pan	O
and	O
s	O
�	O
r	O
(	O
1.8	O
)	O
add	O
the	O
pasta	O
and	O
mix	O
it	O
with	O
the	O
sauce	O

(	O
2.1	O
)	O
add	O
some	O
oil	O
in	O
a	O
pan	O
and	O
add	O
some	O
water	O
(	O
2.2	O
)	O
add	O
a	O
li	O
�	O
le	O
of	O
oil	O
and	O
add	O
a	O
pan	O
and	O
add	O
some	O
oil	O
(	O
2.3	O
)	O
add	O
oil	O
and	O
add	O
to	O
a	O
pan	O
and	O
add	O
some	O
oil	O
(	O
2.4	O
)	O
add	O
salt	O
and	O
pepper	O
to	O
the	O
pan	O
and	O
s	O
�	O
r	O
(	O
2.5	O
)	O
add	O
the	O
chicken	O
to	O
the	O
pan	O
and	O
s	O
�	O
r	O
(	O
2.6	O
)	O
add	O
the	O
sauce	O
to	O
the	O
pan	O
and	O
s	O
�	O
r	O
(	O
2.7	O
)	O
add	O
the	O
pasta	O
and	O
add	O
the	O
sauce	O
and	O
mix	O

(	O
a	O
)	O
cut	O
the	O
potatoes	O
into	O
a	O
bowl	O
and	O
add	O
some	O
oil	O
and	O
pepper	O
(	O
b	O
)	O
cut	O
a	O
pan	O
and	O
add	O
some	O
oil	O
and	O
add	O
the	O
pan	O
(	O
c	O
)	O
cut	O
the	O
potatoes	O
into	O
a	O
bowl	O
and	O
add	O
them	O
(	O
d	O
)	O
heat	O
some	O
oil	O
in	O
a	O
pan	O
and	O
add	O
some	O
chopped	O
onions	O
and	O
add	O
some	O
chopped	O
onions	O
and	O
pepper	O
(	O
e	O
)	O
add	O
chopped	O
garlic	O
and	O
garlic	O
and	O
garlic	O
and	O
add	O
to	O
the	O
pot	O
(	O
f	O
)	O
add	O
the	O
sauce	O
and	O
cook	O
in	O
the	O
pan	O
and	O
s	O
�	O
r	O
(	O
g	O
)	O
add	O
the	O
sauce	O
and	O
add	O
the	O
sauce	O
and	O
s	O
�	O
r	O

(	O
5.1	O
)	O
blend	O
the	O
pepper	O
and	O
a	O
small	O
pieces	O
(	O
5.2	O
)	O
mix	O
cheese	O
bread	O
crumbs	O
parmesan	O
cheese	O
egg	O
yolks	O
a	O
bowl	O
and	O
whisk	O
the	O
mixture	O
(	O
5.3	O
)	O
add	O
sugar	O
cream	O
ketchup	O
and	O
worcestershire	O
sauce	O
on	O
a	O
pan	O
(	O
5.4	O
)	O
add	O
some	O
tomato	O
into	O
a	O
bowl	O
(	O
5.5	O
)	O
add	O
salt	O
and	O
black	O
pepper	O
to	O
the	O
salad	O
and	O
mix	O
(	O
5.6	O
)	O
mix	O
the	O
cabbage	O
and	O
salt	O
in	O
a	O
bowl	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
procedure	O
extraction	O
and	O
captioning	O
,	O
while	O
Caption	O
by	O
Video	O
Model	O
gets	O
better	O
results	O
on	O
captioning	O
for	O
the	O
ground	O
-	O
truth	O
procedure	O
.	O
To	O
sum	O
up	O
,	O
both	O
video	O
frame	O
frames	O
and	O
transcripts	O
are	O
important	O
for	O
the	O
task	O
.	O

We	O
also	O
present	O
a	O
qualitative	O
analysis	O
based	O
on	O
the	O
case	O
study	O
shown	O
in	O
Figures	O
3	O
and	O
4	O
(	O
best	O
viewed	O
in	O
color	O
)	O
.	O
Figure	O
3	O
visualizes	O
the	O
ground	O
-	O
truth	O
procedures	O
and	O
the	O
predicted	O
procedures	O
.	O
The	O
horizontal	O
axis	O
is	O
the	O
time	O
and	O
the	O
number	O
on	O
each	O
small	O
ribbon	O
is	O
the	O
ID	O
of	O
the	O
procedure	O
.	O
We	O
have	O
slightly	O
shifted	O
the	O
overlapping	O
procedures	O
in	O
order	O
to	O
show	O
the	O
results	O
more	O
clearly	O
.	O
It	O
can	O
be	O
seen	O
that	O
the	O
extracted	O
procedures	O
by	O
our	O
full	O
model	O
have	O
the	O
most	O
similar	O
trend	O
with	O
the	O
ground	O
-	O
truth	O
procedures	O
.	O
Figure	O
4	O
presents	O
the	O
generated	O
captions	O
on	O
extracted	O
procedures	O
(	O
Fig	O
.	O
4a	O
)	O
and	O
ground	O
-	O
truth	O
procedures	O
(	O
Fig	O
.	O
4b	O
)	O
separately	O
.	O
Each	O
column	O
shows	O
captioning	O
results	O
from	O
one	O
model	O
,	O
and	O
the	O
first	O
column	O
is	O
the	O
ground	O
-	O
truth	O
result	O
.	O
On	O
one	O
hand	O
,	O
only	O
the	O
full	O
model	O
can	O
generate	O
eggs	O
in	O
the	O
procedure	O
(	O
1.1	O
)	O
and	O
(	O
1.2	O
)	O
,	O
which	O
is	O
also	O
an	O
important	O
ingredient	O
entity	O
in	O
the	O
ground	O
-	O
truth	O
captions	O
.	O
On	O
the	O
other	O
hand	O
,	O
the	O
ingredient	O
bacon	O
in	O
groundtruth	O
caption	O
(	O
c	O
)	O
is	O
ignored	O
by	O
all	O
models	O
.	O
In	O
fact	O
,	O
our	O
Full	O
Model	O
predicts	O
meat	O
synonyms	O
of	O
bacon	O
.	O
Besides	O
,	O
the	O
Full	O
Model	O
can	O
also	O
generate	O
the	O
action	O
cut	O
and	O
the	O
final	O
state	O
of	O
ingredient	O
pieces	O
mentioned	O
in	O
transcript	O
,	O
while	O
it	O
is	O
hard	O
to	O
recognize	O
using	O
only	O
video	O
signals	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
framework	O
for	O
procedure	O
extraction	O
and	O
captioning	O
modeling	O
in	O
instructional	O
videos	O
.	O
Our	O
model	O
use	O
narrated	O
tran	O
-	O
scripts	O
of	O
each	O
video	O
as	O
the	O
supplementary	O
information	O
and	O
can	O
help	O
to	O
predict	O
and	O
caption	O
procedures	O
better	O
.	O
The	O
extensive	O
experiments	O
demonstrate	O
that	O
our	O
model	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
the	O
YouCookII	O
dataset	O
,	O
and	O
ablation	O
studies	O
indicate	O
the	O
effectiveness	O
of	O
utilizing	O
transcripts	O
.	O

We	O
thank	O
the	O
reviewers	O
for	O
their	O
carefully	O
reading	O
and	O
suggestions	O
.	O
This	O
work	O
was	O
supported	O
by	O
the	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
(	O
No	O
.	O
61370137	O
)	O
,	O
the	O
National	O
Basic	O
Research	O
Program	O
of	O
China	O
(	O
No.2012CB7207002	O
)	O
,	O
the	O
Ministry	O
of	O
Education	O
-	O
China	O
Mobile	O
Research	O
Foundation	O
Project	O
(	O
2016/2	O
-	O
7	O
)	O
.	O

The	O
likelihood	O
of	O
the	O
keyword	O
in	O
the	O
target	O
domain	O
is	O
l	O
x	O
,	O
and	O
l	O
x	O
is	O
set	O
as	O
the	O
value	O
of	O
IDF	O
in	O
the	O
target	O
domain	O
of	O
w	O
:	O
l	O
x	O
=	O
log	O
(	O
N	O
d	O
i	O
)	O
+	O
1	O
Here	O
N	O
is	O
the	O
number	O
of	O
articles	O
in	O
the	O
article	O
collection	O
in	O
the	O
target	O
domain	O
,	O
and	O
d	O
i	O
is	O
the	O
number	O
of	O
articles	O
containing	O
the	O
word	O
w	O
in	O
the	O
article	O
collection	O
in	O
the	O
target	O
domain	O
.	O

In	O
this	O
section	O
we	O
describe	O
(	O
1	O
)	O
our	O
baseline	O
sequence	O
-	O
to	O
-	O
sequence	O
model	O
,	O
(	O
2	O
)	O
our	O
pointergenerator	O
model	O
,	O
and	O
(	O
3	O
)	O
our	O
coverage	O
mechanism	O
that	O
can	O
be	O
added	O
to	O
either	O
of	O
the	O
first	O
two	O
models	O
.	O
The	O
code	O
for	O
our	O
models	O
is	O
available	O
online	O
.	O
1	O

In	O
this	O
work	O
we	O
presented	O
a	O
hybrid	O
pointergenerator	O
architecture	O
with	O
coverage	O
,	O
and	O
showed	O
that	O
it	O
reduces	O
inaccuracies	O
and	O
repetition	O
.	O
We	O
applied	O
our	O
model	O
to	O
a	O
new	O
and	O
challenging	O
longtext	O
dataset	O
,	O
and	O
significantly	O
outperformed	O
the	O
abstractive	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
result	O
.	O
Our	O
model	O
exhibits	O
many	O
abstractive	O
abilities	O
,	O
but	O
attaining	O
higher	O
levels	O
of	O
abstraction	O
remains	O
an	O
open	O
research	O
question	O
.	O

There	O
are	O
two	O
broad	O
meta	O
-	O
evaluation	O
strategies	O
:	O
summary	O
-	O
level	O
and	O
system	O
-	O
level	O
.	O
Setup	O
:	O
For	O
each	O
document	O
d	O
i	O
,	O
i	O
{	O
1	O
.	O
.	O
.	O
n	O
}	O
in	O
a	O
dataset	O
D	O
,	O
we	O
have	O
J	O
system	O
outputs	O
,	O
where	O
the	O
outputs	O
can	O
come	O
from	O
(	O
1	O
)	O
extractive	O
systems	O
(	O
Ext	O
)	O
,	O
(	O
2	O
)	O
abstractive	O
systems	O
(	O
Abs	O
)	O
or	O
(	O
3	O
)	O
a	O
union	O
of	O
both	O
(	O
Mix	O
)	O
.	O
Let	O
s	O
ij	O
,	O
j	O
{	O
1	O
.	O
.	O
.	O
J	O
}	O
be	O
the	O
j	O
th	O
summary	O
of	O
the	O
i	O
th	O
document	O
,	O
m	O
i	O
be	O
a	O
specific	O
metric	O
and	O
K	O
be	O
a	O
correlation	O
measure	O
.	O

Summary	O
-	O
level	O
correlation	O
is	O
calculated	O
as	O
follows	O
:	O
K	O
sum	O
m	O
1	O
m	O
2	O
=	O
1	O
n	O
n	O
i=1	O
K	O
[	O
m	O
1	O
(	O
s	O
i1	O
)	O
.	O
.	O
.	O
m	O
1	O
(	O
s	O
iJ	O
)	O
]	O
,	O
[	O
m	O
2	O
(	O
s	O
i1	O
)	O
.	O
.	O
.	O
m	O
2	O
(	O
s	O
iJ	O
)	O
]	O
.	O
(	O
1	O
)	O
Here	O
,	O
correlation	O
is	O
calculated	O
for	O
each	O
document	O
,	O
among	O
the	O
different	O
system	O
outputs	O
of	O
that	O
document	O
,	O
and	O
the	O
mean	O
value	O
is	O
reported	O
.	O

System	O
-	O
level	O
correlation	O
is	O
calculated	O
as	O
follows	O
:	O
K	O
sys	O
m	O
1	O
m	O
2	O
=	O
K	O
1	O
n	O
n	O
i=1	O
m1	O
(	O
si1	O
)	O
.	O
.	O
.	O
1	O
n	O
n	O
i=1	O
m1	O
(	O
siJ	O
)	O
,	O
1	O
n	O
n	O
i=1	O
m2	O
(	O
si1	O
)	O
.	O
.	O
.	O
1	O
n	O
n	O
i=1	O
m2	O
(	O
siJ	O
)	O
.	O
(	O
2	O
)	O
Additionally	O
,	O
the	O
"	O
quality	O
"	O
of	O
a	O
system	O
sys	O
j	O
is	O
defined	O
as	O
the	O
mean	O
human	O
score	O
received	O
by	O
it	O
i.e.	O
HScore	O
sys	O
j	O
mean	O
=	O
1	O
n	O
n	O
i=1	O
humanScore	O
(	O
s	O
ij	O
)	O
.	O
(	O
3	O
)	O

We	O
collect	O
the	O
system	O
-	O
generated	O
summaries	O
from	O
25	O
top	O
-	O
scoring	O
systems	O
,	O
9	O
covering	O
11	O
extractive	O
and	O
14	O
abstractive	O
systems	O
(	O
Sec	O
.	O
2.2	O
)	O
on	O
the	O
CNNDM	O
dataset	O
.	O
We	O
organize	O
our	O
collected	O
generated	O
summaries	O
into	O
three	O
groups	O
based	O
on	O
system	O
type	O
:	O
CNNDM	O
Abs	O
denotes	O
collected	O
output	O
summaries	O
from	O
abstractive	O
systems	O
.	O
CNNDM	O
Ext	O
denotes	O
collected	O
output	O
summaries	O
from	O
extractive	O
systems	O
.	O
CNNDM	O
Mix	O
is	O
the	O
union	O
of	O
the	O
two	O
.	O

Since	O
collecting	O
human	O
annotations	O
is	O
costly	O
,	O
we	O
sample	O
100	O
documents	O
from	O
CNNDM	O
test	O
set	O
(	O
11	O
,	O
490	O
samples	O
)	O
and	O
evaluate	O
system	O
generated	O
summaries	O
of	O
these	O
100	O
documents	O
.	O
We	O
aim	O
to	O
include	O
documents	O
of	O
varying	O
difficulties	O
in	O
the	O
representative	O
sample	O
.	O
As	O
a	O
proxy	O
to	O
the	O
difficulty	O
of	O
summarizing	O
a	O
document	O
,	O
we	O
use	O
the	O
mean	O
score	O
received	O
by	O
the	O
system	O
generated	O
summaries	O
for	O
the	O
document	O
.	O
Based	O
on	O
this	O
,	O
we	O
partition	O
the	O
CNNDM	O
test	O
set	O
into	O
5	O
equal	O
sized	O
bins	O
and	O
sample	O
4	O
documents	O
from	O
each	O
bin	O
.	O
We	O
repeat	O
this	O
process	O
for	O
5	O
metrics	O
(	O
BERTScore	O
,	O
MoverScore	O
,	O
R	O
-	O
1	O
,	O
R	O
-	O
2	O
,	O
R	O
-	O
L	O
)	O
obtaining	O
a	O
sample	O
of	O
100	O
documents	O
.	O
This	O
methodology	O
is	O
detailed	O
in	O
Alg	O
.	O
1	O
in	O
Sec	O
.	O
A.1	O
.	O

Most	O
papers	O
that	O
propose	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
system	O
often	O
use	O
automatic	O
metrics	O
as	O
a	O
proxy	O
to	O
human	O
judgments	O
to	O
compare	O
their	O
proposed	O
method	O
against	O
other	O
top	O
scoring	O
systems	O
.	O
However	O
,	O
can	O
metrics	O
reliably	O
quantify	O
the	O
improvements	O
that	O
one	O
high	O
quality	O
system	O
makes	O
over	O
other	O
competitive	O
systems	O
?	O
To	O
answer	O
this	O
,	O
instead	O
of	O
focusing	O
on	O
all	O
of	O
the	O
collected	O
systems	O
,	O
we	O
evaluate	O
the	O
correlation	O
between	O
automatic	O
metrics	O
and	O
human	O
judg	O
-	O
ments	O
in	O
comparing	O
the	O
top	O
-	O
k	O
systems	O
,	O
where	O
top	O
-	O
k	O
are	O
chosen	O
based	O
on	O
a	O
system	O
's	O
mean	O
human	O
score	O
(	O
Eqn	O
.	O
3	O
)	O
.	O
14	O
Our	O
observations	O
are	O
presented	O
in	O
Fig	O
.	O
3	O
.	O
We	O
find	O
that	O
:	O
(	O
1	O
)	O
As	O
k	O
becomes	O
smaller	O
,	O
metrics	O
de	O
-	O
correlate	O
with	O
humans	O
on	O
the	O
TAC	O
-	O
2008	O
and	O
CNNDM	O
Mix	O
datasets	O
,	O
even	O
getting	O
negative	O
correlations	O
for	O
small	O
values	O
of	O
k	O
(	O
Fig	O
.	O
8a	O
,	O
8c	O
)	O
.	O
Interestingly	O
,	O
SMS	O
,	O
R	O
-	O
1	O
,	O
R	O
-	O
2	O
and	O
R	O
-	O
L	O
improve	O
in	O
performance	O
as	O
k	O
becomes	O
smaller	O
on	O
CNNDM	O
Ext	O
.	O
(	O
2	O
)	O
R	O
-	O
2	O
had	O
negative	O
correlations	O
with	O
human	O
judgments	O
on	O
TAC	O
-	O
2009	O
for	O
k	O
<	O
50	O
,	O
however	O
it	O
remains	O
highly	O
correlated	O
with	O
human	O
judgments	O
on	O
CNNDM	O
Abs	O
for	O
all	O
values	O
of	O
k.	O
Takeaway	O
:	O
Metrics	O
can	O
not	O
reliably	O
quantify	O
the	O
improvements	O
made	O
by	O
one	O
system	O
over	O
others	O
,	O
especially	O
for	O
the	O
top	O
few	O
systems	O
across	O
all	O
datasets	O
.	O
Some	O
metrics	O
,	O
however	O
,	O
are	O
well	O
suited	O
for	O
specific	O
datasets	O
,	O
e.g.	O
JS	O
-	O
2	O
and	O
R	O
-	O
2	O
are	O
reliable	O
indicators	O
of	O
improvements	O
on	O
TAC	O
-	O
2009	O
and	O
CNNDM	O
Abs	O
respectively	O
.	O

We	O
sincerely	O
thank	O
all	O
authors	O
of	O
the	O
systems	O
that	O
we	O
used	O
in	O
this	O
work	O
for	O
sharing	O
their	O
systems	O
'	O
outputs	O
.	O

We	O
are	O
grateful	O
to	O
the	O
DataLEASH	O
project	O
and	O
Helse	O
Nord	O
for	O
funding	O
this	O
research	O
work	O
.	O

The	O
need	O
to	O
detect	O
events	O
that	O
could	O
lead	O
to	O
protests	O
is	O
of	O
prime	O
interest	O
to	O
sociologists	O
and	O
governments	O
(	O
Danilova	O
et	O
al	O
,	O
2016	O
)	O
.	O
There	O
are	O
several	O
active	O
ongoing	O
projects	O
for	O
socio	O
-	O
political	O
event	O
systems	O
such	O
as	O
KEDS	O
(	O
Kansas	O
Event	O
Data	O
System	O
)	O
(	O
Schrodt	O
and	O
Hall	O
,	O
2006	O
)	O
,	O
CAMEO	O
(	O
Conflict	O
and	O
Mediation	O
Event	O
Observation	O
)	O
(	O
Gerner	O
et	O
al	O
,	O
2002	O
)	O
,	O
and	O
several	O
other	O
databases	O
for	O
protest	O
de	O
-	O
tection	O
systems	O
(	O
Danilova	O
,	O
2015	O
)	O
.	O
These	O
methods	O
have	O
focused	O
on	O
news	O
data	O
as	O
they	O
have	O
traditionally	O
been	O
the	O
most	O
reliant	O
source	O
of	O
events	O
.	O
Protest	O
detection	O
has	O
been	O
one	O
of	O
the	O
major	O
issues	O
in	O
the	O
context	O
of	O
social	O
and	O
political	O
(	O
Ettinger	O
et	O
al	O
,	O
2017	O
)	O
.	O
Papanikolaou	O
and	O
Papageorgiou	O
(	O
2020	O
)	O
presented	O
a	O
computational	O
social	O
science	O
methodology	O
to	O
analyse	O
protests	O
in	O
Greece	O
.	O
constructed	O
a	O
corpus	O
of	O
protest	O
events	O
comprising	O
various	O
language	O
sources	O
from	O
various	O
countries	O
.	O
Several	O
systems	O
were	O
submitted	O
to	O
the	O
CLEF	O
ProtestNews	O
Track	O
that	O
consisted	O
of	O
three	O
shared	O
tasks	O
,	O
primarily	O
aimed	O
at	O
identifying	O
and	O
extracting	O
event	O
information	O
spanning	O
to	O
multiple	O
countries	O
(	O
Hürriyetoglu	O
et	O
al	O
,	O
2019b	O
.	O

This	O
dataset	O
comprises	O
26	O
,	O
208	O
sentences	O
in	O
three	O
languages	O
,	O
namely	O
English	O
,	O
Spanish	O
,	O
and	O
Portuguese	O
.	O
The	O
dataset	O
consists	O
of	O
two	O
classes	O
:	O
Event	O
:	O
The	O
sentence	O
indicates	O
an	O
event	O
of	O
the	O
past	O
.	O
Not	O
-	O
event	O
:	O
The	O
sentence	O
does	O
not	O
talk	O
about	O
any	O
event	O
.	O
The	O
volume	O
of	O
sequences	O
indicating	O
Not	O
-	O
event	O
is	O
higher	O
in	O
contrast	O
to	O
that	O
of	O
the	O
Event	O
label	O
.	O
Therefore	O
,	O
the	O
dataset	O
distribution	O
is	O
quite	O
imbalanced	O
.	O
We	O
can	O
also	O
notice	O
that	O
the	O
number	O
of	O
English	O
samples	O
exceeds	O
that	O
of	O
Spanish	O
and	O
Portuguese	O
ones	O
.	O
Refer	O
to	O

EmoNet	O
:	O
Fine	O
-	O
Grained	O
Emotion	O
Detection	O
with	O
Gated	O
Recurrent	O
Neural	O
Networks	O

Learning	O
and	O
Evaluating	O
Emotion	O
Lexicons	O
for	O
91	O
Languages	O

The	O
exact	O
design	O
of	O
the	O
Source	O
train	O
-	O
dev	O
-	O
test	O
split	O
is	O
as	O
follows	O
:	O
All	O
entries	O
(	O
words	O
plus	O
ratings	O
)	O
from	O
all	O
splits	O
are	O
taken	O
from	O
Warriner	O
et	O
al	O
(	O
2013	O
)	O
.	O
The	O
data	O
was	O
then	O
partitioned	O
based	O
on	O
the	O
overlap	O
with	O
the	O
two	O
precursory	O
versions	O
by	O
Bradley	O
and	O
Lang	O
(	O
1999	O
)	O
(	O
the	O
original	O
ANEW	O
)	O
and	O
Bradley	O
and	O
Lang	O
(	O
2010	O
)	O
(	O
an	O
early	O
extended	O
version	O
of	O
ANEW	O
roughly	O
twice	O
as	O
large	O
)	O
.	O
Source	O
-	O
test	O
was	O
built	O
by	O
intersecting	O
the	O
lexicon	O
from	O
Warriner	O
et	O
al	O
(	O
2013	O
)	O
with	O
the	O
original	O
ANEW	O
.	O
A	O
similar	O
process	O
was	O
applied	O
for	O
Source	O
-	O
dev	O
:	O
we	O
intersected	O
the	O
words	O
from	O
Warriner	O
et	O
al	O
(	O
2013	O
)	O
and	O
Bradley	O
and	O
Lang	O
(	O
2010	O
)	O
and	O
removed	O
the	O
ones	O
present	O
in	O
Source	O
-	O
test	O
.	O
Lastly	O
,	O
Source	O
-	O
train	O
is	O
made	O
up	O
by	O
all	O
words	O
from	O
Warriner	O
et	O
al	O
(	O
2013	O
)	O
which	O
are	O
neither	O
in	O
Source	O
-	O
test	O
nor	O
in	O
Source	O
-	O
dev	O
.	O
The	O
reason	O
why	O
the	O
ratings	O
in	O
Source	O
are	O
taken	O
exclusively	O
from	O
Warriner	O
et	O
al	O
(	O
2013	O
)	O
is	O
that	O
these	O
are	O
distributed	O
under	O
a	O
more	O
permissive	O
license	O
compared	O
to	O
their	O
precursors	O
.	O
We	O
removed	O
multi	O
-	O
token	O
entries	O
(	O
e.g.	O
,	O
boa	O
constrictor	O
)	O
and	O
entries	O
with	O
upper	O
case	O
characters	O
(	O
e.g.	O
,	O
Budweiser	O
)	O
from	O
all	O
data	O
splits	O
of	O
Source	O
,	O
thus	O
restricting	O
the	O
lexicon	O
to	O
single	O
-	O
token	O
,	O
nonproper	O
noun	O
entries	O
to	O
make	O
it	O
more	O
suitable	O
for	O
word	O
embedding	O
-	O
based	O
research	O
.	O
All	O
splits	O
combined	O
have	O
13	O
,	O
791	O
entries	O
(	O
train	O
:	O
11	O
,	O
463	O
,	O
dev	O
:	O
1	O
,	O
296	O
,	O
test	O
:	O
1	O
,	O
032	O
)	O
,	O
thus	O
removing	O
less	O
than	O
1	O
%	O
from	O
the	O
original	O
lexicon	O
.	O
5	O
Regarding	O
the	O
remaining	O
gold	O
standards	O
,	O
the	O
only	O
cases	O
which	O
needed	O
additional	O
preparation	O
or	O
cleansing	O
steps	O
were	O
zh1	O
and	O
zh2	O
(	O
Yao	O
et	O
al	O
,	O
2017	O
)	O
.	O
zh1	O
was	O
created	O
and	O
is	O
distributed	O
using	O
traditional	O
Chinese	O
characters	O
,	O
whereas	O
the	O
embedding	O
model	O
by	O
Grave	O
et	O
al	O
(	O
2018	O
)	O
employs	O
simplified	O
ones	O
.	O
Therefore	O
,	O
we	O
converted	O
zh1	O
into	O
simplified	O
characters	O
using	O
GOOGLE	O
TRANSLATE	O
6	O
prior	O
to	O
evaluation	O
.	O
While	O
manually	O
examining	O
the	O
zh2	O
lexicon	O
,	O
we	O
noticed	O
several	O
cases	O
where	O
the	O
ratings	O
seemed	O
rather	O
counter	O
-	O
intuitive	O
(	O
e.g.	O
,	O
seemingly	O
positive	O
words	O
which	O
received	O
very	O
negative	O
ratings	O
)	O
.	O
We	O
contacted	O
the	O
authors	O
who	O
confirmed	O
the	O
problem	O
and	O
sent	O
us	O
a	O
corrected	O
version	O
.	O
We	O
did	O
not	O
find	O
any	O
such	O
problems	O
in	O
the	O
second	O
version	O
.	O
We	O
consulted	O
with	O
a	O
Chinese	O
native	O
speaker	O
for	O
both	O
of	O
these	O
procedures	O
regarding	O
the	O
zh1	O
and	O
zh2	O
lexicons	O
.	O

We	O
would	O
like	O
to	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
helpful	O
suggestions	O
and	O
comments	O
,	O
and	O
Tinghui	O
Duan	O
,	O
JULIE	O
LAB	O
,	O
for	O
assisting	O
us	O
with	O
the	O
Chinese	O
gold	O
data	O
.	O
This	O
work	O
was	O
partially	O
funded	O
by	O
the	O
German	O
Federal	O
Ministry	O
for	O
Economic	O
Affairs	O
and	O
Energy	O
(	O
funding	O
line	O
"	O
Big	O
Data	O
in	O
der	O
makroökonomischen	O
Analyse	O
"	O
[	O
Big	O
data	O
in	O
macroeconomic	O
analysis	O
]	O
;	O
Fachlos	O
2	O
;	O
GZ	O
23305/003#002	O
)	O
.	O

Paraphrases	O
How	O
do	O
I	O
improve	O
my	O
English	O
What	O
is	O
the	O
best	O
way	O
to	O
learn	O
English	O

In	O
this	O
section	O
,	O
some	O
traditional	O
approaches	O
without	O
neural	O
models	O
will	O
be	O
introduced	O
.	O

This	O
approach	O
usually	O
generates	O
paraphrases	O
by	O
substituting	O
some	O
words	O
in	O
the	O
source	O
sentences	O
with	O
their	O
synonyms	O
extracted	O
from	O
a	O
thesaurus	O
(	O
Bolshakov	O
and	O
Gelbukh	O
,	O
2004	O
;	O
Kauchak	O
and	O
Barzilay	O
,	O
2006	O
)	O
.	O
Thesaurus	O
-	O
based	O
approaches	O
proceed	O
by	O
first	O
extracting	O
all	O
synonyms	O
from	O
a	O
thesaurus	O
for	O
the	O
words	O
to	O
be	O
replaced	O
.	O
Then	O
the	O
optimal	O
candidate	O
is	O
selected	O
according	O
to	O
the	O
context	O
in	O
the	O
source	O
sentence	O
.	O
Although	O
simple	O
and	O
effective	O
,	O
this	O
approach	O
is	O
severely	O
limited	O
by	O
the	O
diversity	O
of	O
the	O
generated	O
paraphrases	O
.	O

Restoring	O
Hebrew	O
Diacritics	O
Without	O
a	O
Dictionary	O

The	O
input	O
to	O
the	O
dotting	O
task	O
consists	O
of	O
a	O
sequence	O
of	O
characters	O
.	O
Each	O
of	O
the	O
characters	O
is	O
assigned	O
three	O
values	O
,	O
from	O
three	O
separate	O
diacritic	O
categories	O
:	O
one	O
category	O
for	O
the	O
dot	O
distinguishing	O
shin	O
(	O
‫	O
)	O
שׁ‬	O
from	O
sin	O
(	O
‫	O
,	O
)	O
שׂ‬	O
two	O
consonants	O
sharing	O
a	O
base	O
character	O
‫	O
;	O
ש‬	O
another	O
for	O
the	O
presence	O
of	O
dagesh	O
/	O
mappiq	O
,	O
a	O
central	O
dot	O
affecting	O
pronunciation	O
of	O
some	O
consonants	O
,	O
e.g.	O
‫פּ‬	O
/p/	O
from	O
‫פ‬	O
/f/	O
,	O
but	O
also	O
present	O
elsewhere	O
;	O
and	O
one	O
for	O
all	O
other	O
diacritic	O
marks	O
,	O
which	O
mostly	O
determine	O
vocalization	O
,	O
e.g.	O
‫ד‬	O
/da/	O
vs.	O
‫ד‬	O
/de/.	O
Diacritics	O
of	O
different	O
categories	O
may	O
co	O
-	O
occur	O
on	O
single	O
letters	O
,	O
e.g.	O
,	O
or	O
may	O
be	O
absent	O
altogether	O
.	O
Full	O
script	O
Hebrew	O
script	O
written	O
without	O
intention	O
of	O
dotting	O
typically	O
employs	O
a	O
compensatory	O
variant	O
known	O
colloquially	O
as	O
full	O
script	O
(	O
ktiv	O
male	O
,	O
‫מלא‬	O
‫	O
,	O
)	O
כתיב‬	O
which	O
adds	O
instances	O
of	O
the	O
letters	O
‫י‬	O
and	O
‫ו‬	O
in	O
some	O
places	O
where	O
they	O
can	O
aid	O
pronunciation	O
,	O
but	O
are	O
incompatible	O
with	O
the	O
rules	O
for	O
dotted	O
script	O
.	O
In	O
our	O
formulation	O
of	O
dotting	O
as	O
a	O
sequence	O
tagging	O
problem	O
,	O
and	O
in	O
collecting	O
our	O
test	O
set	O
from	O
raw	O
text	O
,	O
these	O
added	O
letters	O
may	O
conflict	O
with	O
the	O
dotting	O
standard	O
.	O
For	O
the	O
sake	O
of	O
input	O
integrity	O
,	O
and	O
unlike	O
some	O
other	O
systems	O
,	O
we	O
opt	O
not	O
to	O
remove	O
these	O
characters	O
,	O
but	O
instead	O
employ	O
a	O
dotting	O
policy	O
consistent	O
with	O
full	O
script	O
.	O
See	O
Appendix	O
A	O
for	O
further	O
details	O
.	O

In	O
Table	O
4	O
we	O
present	O
examples	O
of	O
words	O
dotted	O
incorrectly	O
,	O
or	O
correctly	O
,	O
only	O
by	O
NAKDIMON	O
,	O
compared	O
with	O
Morfix	O
and	O
Dicta	O
.	O
The	O
largest	O
category	O
for	O
NAKDIMON	O
-	O
only	O
errors	O
(	O
∼18	O
%	O
of	O
90	O
sampled	O
)	O
are	O
ones	O
where	O
a	O
fused	O
preposition+determiner	O
character	O
is	O
dotted	O
to	O
only	O
include	O
the	O
preposition	O
,	O
perhaps	O
due	O
to	O
its	O
inability	O
to	O
detect	O
the	O
explicit	O
determiner	O
clitic	O
‫ה‬	O
in	O
neighboring	O
words	O
,	O
on	O
which	O
the	O
complex	O
systems	O
apply	O
morphological	O
segmentation	O
.	O
In	O
other	O
cases	O
(	O
∼15	O
%	O
)	O
,	O
NAKDIMON	O
creates	O
11	O
These	O
are	O
:	O
the	O
sin	O
/	O
shin	O
dot	O
,	O
vowel	O
distinctions	O
across	O
the	O
a	O
/	O
e	O
/	O
i	O
/	O
o	O
/	O
u	O
/	O
null	O
sets	O
,	O
and	O
dagesh	O
in	O
the	O
‫/ב‬	O
‫/כ‬	O
‫פ‬	O
characters	O
.	O
We	O
do	O
not	O
distinguish	O
between	O
kamatz	O
gadol	O
/	O
kamatz	O
katan	O
,	O
and	O
schwa	O
is	O
assumed	O
to	O
always	O
be	O
null	O
.	O
unreadable	O
vocalization	O
sequences	O
,	O
as	O
it	O
has	O
no	O
lexical	O
component	O
and	O
is	O
decoded	O
greedily	O
.	O
These	O
types	O
of	O
errors	O
are	O
more	O
friendly	O
to	O
the	O
typical	O
use	O
cases	O
of	O
a	O
dotting	O
system	O
,	O
as	O
they	O
are	O
likely	O
to	O
stand	O
out	O
to	O
a	O
reader	O
.	O
In	O
contrast	O
,	O
a	O
large	O
portion	O
of	O
cases	O
where	O
only	O
NAKDIMON	O
was	O
correct	O
(	O
∼13	O
%	O
of	O
152	O
)	O
are	O
foreign	O
names	O
and	O
terms	O
.	O
This	O
may	O
be	O
the	O
result	O
of	O
such	O
words	O
not	O
yet	O
appearing	O
in	O
dictionaries	O
,	O
or	O
not	O
being	O
easily	O
separable	O
from	O
an	O
adjoining	O
clitic	O
,	O
while	O
character	O
-	O
level	O
information	O
can	O
capture	O
pronunciation	O
patterns	O
from	O
similar	O
words	O
(	O
e.g.	O
‫ֶפוֹ‬	O
‫ֶל‬	O
‫ט‬	O
'	O
telephone	O
'	O
,	O
for	O
the	O
example	O
‫.	O
)	O
האייפו‬	O
OOVs	O
To	O
further	O
quantify	O
the	O
strengths	O
of	O
NAKDIMON	O
's	O
architecture	O
and	O
training	O
abilities	O
,	O
we	O
evaluate	O
the	O
systems	O
'	O
results	O
pertaining	O
only	O
to	O
those	O
words	O
in	O
the	O
test	O
set	O
which	O
do	O
not	O
appear	O
in	O
our	O
training	O
sets	O
.	O
We	O
follow	O
common	O
practice	O
by	O
calling	O
them	O
OOVs	O
(	O
"	O
out	O
of	O
vocabulary	O
"	O
)	O
,	O
but	O
emphasize	O
that	O
NAKDIMON	O
does	O
not	O
consult	O
an	O
explicit	O
vocabulary	O
,	O
and	O
the	O
other	O
systems	O
are	O
not	O
evaluated	O
against	O
their	O
own	O
vocabularies	O
(	O
which	O
are	O
unknown	O
to	O
us	O
)	O
.	O
We	O
find	O
that	O
NAKDIMON	O
's	O
performance	O
on	O
this	O
subset	O
is	O
substantially	O
worse	O
compared	O
with	O
the	O
other	O
systems	O
than	O
on	O
the	O
full	O
set	O
:	O
15	O
percentage	O
points	O
below	O
Dicta	O
and	O
seven	O
below	O
Morfix	O
on	O
the	O
VOC	O
metric	O
(	O
see	O
full	O
results	O
in	O
Appendix	O
C	O
)	O
.	O
These	O
results	O
might	O
be	O
counter	O
-	O
intuitive	O
considering	O
the	O
proven	O
utility	O
of	O
character	O
-	O
level	O
models	O
in	O
OOV	O
contexts	O
(	O
e.g.	O
,	O
Plank	O
et	O
al	O
,	O
2016	O
)	O
,	O
and	O
so	O
we	O
offer	O
several	O
possible	O
explanations	O
:	O
First	O
,	O
many	O
"	O
OOVs	O
"	O
consist	O
in	O
fact	O
of	O
known	O
words	O
coupled	O
with	O
an	O
unseen	O
combination	O
of	O
prefix	O
clitics	O
and/or	O
suffix	O
possessive	O
markers	O
,	O
which	O
other	O
systems	O
explicitly	O
remove	O
using	O
morphological	O
analyzers	O
before	O
dotting	O
.	O
Second	O
,	O
mirroring	O
the	O
last	O
finding	O
from	O
the	O
overall	O
analysis	O
,	O
some	O
"	O
OOVs	O
"	O
are	O
proper	O
names	O
which	O
appear	O
in	O
dictionaries	O
but	O
are	O
absent	O
from	O
the	O
training	O
set	O
,	O
due	O
to	O
corpus	O
effects	O
such	O
as	O
time	O
and	O
domain	O
,	O
or	O
simply	O
chance	O
.	O

We	O
collected	O
the	O
data	O
for	O
our	O
training	O
set	O
and	O
test	O
sets	O
from	O
open	O
online	O
sources	O
,	O
while	O
making	O
sure	O
their	O
terms	O
allow	O
research	O
application	O
and	O
privacy	O
is	O
not	O
impugned	O
.	O
NAKDIMON	O
's	O
architecture	O
does	O
not	O
encourage	O
memorization	O
of	O
training	O
data	O
and	O
the	O
system	O
is	O
not	O
trained	O
for	O
generating	O
text	O
.	O
We	O
consider	O
a	O
main	O
use	O
case	O
for	O
our	O
system	O
to	O
be	O
assisting	O
Hebrew	O
learners	O
in	O
reading	O
.	O
We	O
therefore	O
expect	O
NAKDIMON	O
to	O
facilitate	O
life	O
in	O
Israel	O
for	O
immigrants	O
still	O
struggling	O
with	O
Hebrew	O
,	O
among	O
other	O
underprivileged	O
groups	O
.	O
Automatic	O
dotting	O
can	O
increase	O
inclusion	O
in	O
Hebrew	O
-	O
prominent	O
societies	O
for	O
literacy	O
-	O
challenged	O
individuals	O
,	O
and	O
derivative	O
improvements	O
in	O
text	O
-	O
to	O
-	O
speech	O
applications	O
can	O
assist	O
those	O
with	O
impaired	O
vision	O
.	O
Lastly	O
,	O
dotting	O
can	O
help	O
researchers	O
with	O
limited	O
understanding	O
of	O
Hebrew	O
access	O
resources	O
in	O
the	O
language	O
.	O
Hebrew	O
is	O
a	O
gendered	O
language	O
.	O
Orthographically	O
,	O
in	O
many	O
cases	O
the	O
lack	O
of	O
dots	O
masks	O
gender	O
ambiguity	O
,	O
allowing	O
both	O
masculine	O
and	O
feminine	O
readings	O
for	O
a	O
given	O
word	O
(	O
e.g.	O
ְ	O
‫ְתּ‬	O
‫ַח‬	O
‫ל‬	O
‫שׁ‬	O
/	O
ָ	O
‫ְתּ‬	O
‫ַח‬	O
‫ל‬	O
‫שׁ‬	O
'	O
you.fem	O
sent	O
'	O
/	O
'	O
you.masc	O
sent	O
'	O
)	O
.	O
While	O
wellperforming	O
automatic	O
dotting	O
can	O
help	O
alleviate	O
these	O
ambiguities	O
and	O
reduce	O
the	O
amount	O
of	O
potentially	O
prejudiced	O
readings	O
,	O
we	O
recognize	O
the	O
large	O
body	O
of	O
work	O
on	O
gender	O
bias	O
in	O
NLP	O
(	O
Blodgett	O
et	O
al	O
,	O
2020	O
)	O
,	O
including	O
in	O
Hebrew	O
NLP	O
(	O
Moryossef	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
the	O
findings	O
that	O
an	O
imbalanced	O
training	O
set	O
may	O
result	O
in	O
an	O
even	O
more	O
skewed	O
distribution	O
of	O
gender	O
bias	O
in	O
applications	O
(	O
Zhao	O
et	O
al	O
,	O
2017	O
)	O
.	O
We	O
believe	O
our	O
unlexicalized	O
approach	O
is	O
more	O
robust	O
to	O
such	O
bias	O
compared	O
with	O
other	O
systems	O
,	O
and	O
have	O
already	O
started	O
quantifying	O
and	O
addressing	O
these	O
issues	O
as	O
we	O
find	O
them	O
in	O
ongoing	O
work	O
.	O
In	O
the	O
meantime	O
,	O
we	O
offer	O
this	O
paragraph	O
as	O
a	O
disclaimer	O
.	O

We	O
apply	O
the	O
following	O
resolution	O
tactics	O
for	O
added	O
letters	O
in	O
undotted	O
text	O
:	O
(	O
a	O
)	O
We	O
almost	O
never	O
remove	O
or	O
add	O
letters	O
to	O
the	O
original	O
text	O
(	O
unless	O
it	O
is	O
completely	O
undiacritizable	O
)	O
.	O
(	O
b	O
)	O
We	O
keep	O
dagesh	O
in	O
letters	O
that	O
follow	O
a	O
shuruk	O
which	O
replaces	O
a	O
kubuts	O
,	O
and	O
similarly	O
for	O
yod	O
(	O
hirik	O
male	O
replacing	O
hirik	O
haser	O
)	O
.	O
(	O
c	O
)	O
When	O
we	O
have	O
double	O
vav	O
or	O
double	O
yod	O
,	O
the	O
second	O
letter	O
is	O
usually	O
left	O
undotted	O
,	O
except	O
when	O
it	O
is	O
impossible	O
to	O
have	O
the	O
correct	O
vocalization	O
this	O
way	O
.	O
Resolving	O
ktiv	O
haser	O
discrepancies	O
from	O
Morfix	O
outputs	O
is	O
done	O
by	O
adding	O
missing	O
vowel	O
letters	O
,	O
or	O
removing	O
superfluous	O
vowel	O
letters	O
,	O
in	O
such	O
a	O
way	O
that	O
would	O
not	O
count	O
as	O
an	O
error	O
if	O
it	O
is	O
correct	O
according	O
to	O
Academy	O
regulations	O
.	O

We	O
would	O
like	O
to	O
thank	O
Avi	O
Shmidman	O
for	O
details	O
about	O
Dicta	O
's	O
Nakdan	O
and	O
other	O
suggestions	O
.	O
We	O
thank	O
Sara	O
Gershuni	O
for	O
lengthy	O
and	O
fruitful	O
discussions	O
,	O
and	O
for	O
her	O
linguistic	O
insights	O
and	O
advice	O
.	O
We	O
thank	O
Yoav	O
Goldberg	O
,	O
Reut	O
Tsarfaty	O
,	O
Ian	O
Stewart	O
,	O
Sarah	O
Wiegreffe	O
,	O
Kyle	O
Gorman	O
and	O
many	O
anonymous	O
reviewers	O
for	O
their	O
comments	O
and	O
suggestions	O
in	O
discussions	O
and	O
on	O
earlier	O
drafts	O
.	O

SUPERB	O
-	O
SG	O
:	O
Enhanced	O
Speech	O
processing	O
Universal	O
PERformance	O
Benchmark	O
for	O
Semantic	O
and	O
Generative	O
Capabilities	O

Upstream	O
Model	O
(	O
eg	O
.	O
FBANK	O
,	O
TERA	O
,	O
etc	O
.	O
)	O
The	O
introduction	O
of	O
these	O
new	O
tasks	O
of	O
varying	O
difficulty	O
takes	O
us	O
closer	O
to	O
a	O
more	O
comprehensive	O
unified	O
standard	O
speech	O
benchmark	O
.	O
We	O
hope	O
that	O
this	O
will	O
motivate	O
the	O
development	O
of	O
more	O
powerful	O
,	O
generalizable	O
,	O
and	O
reusable	O
pre	O
-	O
trained	O
models	O
to	O
democratize	O
the	O
advancement	O
of	O
speech	O
research	O
.	O
To	O
facilitate	O
this	O
,	O
we	O
released	O
the	O
codes	O
1	O
and	O
integrated	O
the	O
tasks	O
with	O
the	O
SUPERB	O
benchmark	O
.	O

This	O
section	O
introduces	O
the	O
tasks	O
in	O
SUPERB	O
-	O
SG	O
,	O
including	O
why	O
we	O
choose	O
these	O
tasks	O
and	O
how	O
we	O
design	O
the	O
task	O
-	O
specific	O
heads	O
for	O
fine	O
-	O
tuning	O
.	O
Following	O
SUPERB	O
's	O
methodology	O
,	O
we	O
use	O
a	O
lightweight	O
fine	O
-	O
tuning	O
approach	O
wherein	O
we	O
freeze	O
the	O
pre	O
-	O
trained	O
model	O
parameters	O
and	O
only	O
keep	O
the	O
task	O
-	O
specific	O
head	O
's	O
parameters	O
trainable	O
.	O
This	O
setting	O
serves	O
the	O
dual	O
purpose	O
of	O
evaluating	O
the	O
robustness	O
as	O
well	O
as	O
the	O
generalizability	O
of	O
the	O
speech	O
representations	O
,	O
and	O
provides	O
a	O
resourceefficient	O
way	O
of	O
fine	O
-	O
tuning	O
the	O
models	O
that	O
is	O
inclusive	O
of	O
participants	O
with	O
constrained	O
compute	O
resources	O
.	O
We	O
call	O
the	O
pre	O
-	O
trained	O
model	O
as	O
upstream	O
model	O
and	O
the	O
task	O
-	O
specific	O
heads	O
as	O
downstream	O
model	O
.	O
We	O
now	O
discuss	O
the	O
newly	O
added	O
tasks	O
in	O
SUPERB	O
-	O
SG	O
in	O
the	O
following	O
sub	O
-	O
sections	O
.	O

Following	O
SUPERB	O
,	O
we	O
fix	O
upstream	O
models	O
parameters	O
for	O
all	O
downstream	O
tasks	O
'	O
training	O
.	O
We	O
extract	O
the	O
frame	O
-	O
level	O
representations	O
for	O
each	O
hidden	O
layer	O
of	O
the	O
upstream	O
models	O
from	O
raw	O
waveform	O
,	O
and	O
use	O
a	O
trainable	O
task	O
-	O
specific	O
weightedsum	O
mechanism	O
to	O
summarize	O
all	O
layers	O
'	O
representations	O
into	O
a	O
sequence	O
of	O
vectors	O
.	O
The	O
summarized	O
representations	O
are	O
then	O
used	O
as	O
the	O
downstream	O
model	O
's	O
input	O
.	O
An	O
overview	O
of	O
the	O
training	O
procedure	O
is	O
demonstrated	O
in	O
Figure	O
1	O
.	O
Each	O
experiment	O
is	O
done	O
by	O
one	O
single	O
run	O
with	O
the	O
same	O
seed	O
.	O
This	O
procedure	O
is	O
consistent	O
for	O
all	O
experiments	O
,	O
offering	O
a	O
fair	O
and	O
simple	O
evaluation	O
strategy	O
for	O
all	O
upstream	O
models	O
.	O

This	O
work	O
fully	O
adheres	O
to	O
the	O
ACL	O
code	O
of	O
ethics	O
.	O
For	O
more	O
details	O
,	O
we	O
provide	O
a	O
checklist	O
in	O
Appendix	O
B.	O

Here	O
we	O
answer	O
the	O
ethics	O
questions	O
to	O
show	O
our	O
ethics	O
statement	O
.	O

Yes	O
,	O
we	O
discussed	O
the	O
constrains	O
on	O
the	O
frozen	O
upstreams	O
and	O
simple	O
task	O
specific	O
heads	O
in	O
abstract	O
and	O
Section	O
3	O
.	O
Yes	O
,	O
we	O
used	O
public	O
datasets	O
and	O
pre	O
-	O
trained	O
models	O
mentioned	O
in	O
Section	O
3	O
.	O

UniConv	O
:	O
A	O
Unified	O
Conversational	O
Neural	O
Architecture	O
for	O
Multi	O
-	O
domain	O
Task	O
-	O
oriented	O
Dialogues	O

DST	O
.	O
For	O
state	O
tracking	O
,	O
the	O
metrics	O
are	O
calculated	O
for	O
domain	O
-	O
specific	O
slots	O
of	O
the	O
corresponding	O
domain	O
at	O
each	O
dialogue	O
turn	O
.	O
We	O
also	O
report	O
the	O
DST	O
separately	O
for	O
multi	O
-	O
domain	O
and	O
single	O
-	O
domain	O
dialogues	O
to	O
evaluate	O
the	O
challenges	O
in	O
multi	O
-	O
domain	O
dialogues	O
and	O
our	O
DST	O
performance	O
gap	O
as	O
compared	O
to	O
single	O
-	O
domain	O
dialogues	O
.	O
From	O
our	O
DST	O
performs	O
consistently	O
well	O
in	O
the	O
3	O
domains	O
attraction	O
,	O
restaurant	O
,	O
and	O
train	O
domains	O
.	O
However	O
,	O
the	O
performance	O
drops	O
in	O
the	O
taxi	O
and	O
hotel	O
domain	O
,	O
significantly	O
in	O
the	O
taxi	O
domain	O
.	O
We	O
note	O
that	O
dialogues	O
with	O
the	O
taxi	O
domain	O
is	O
usually	O
not	O
single	O
-	O
domain	O
but	O
typically	O
entangled	O
with	O
other	O
domains	O
.	O
Secondly	O
,	O
we	O
observe	O
that	O
there	O
is	O
a	O
significant	O
performance	O
gap	O
of	O
about	O
10	O
points	O
absolute	O
score	O
between	O
DST	O
performances	O
in	O
singledomain	O
and	O
multi	O
-	O
domain	O
dialogues	O
.	O
State	O
tracking	O
in	O
multi	O
-	O
domain	O
dialogues	O
is	O
,	O
hence	O
,	O
could	O
be	O
further	O
improved	O
to	O
boost	O
the	O
overall	O
performance	O
.	O
Additionally	O
,	O
we	O
report	O
qualitative	O
analysis	O
and	O
the	O
insights	O
can	O
be	O
seen	O
in	O
Appendix	O
C.	O

We	O
thank	O
all	O
reviewers	O
for	O
their	O
insightful	O
feedback	O
to	O
the	O
manuscript	O
of	O
this	O
paper	O
.	O
The	O
first	O
author	O
of	O
this	O
paper	O
is	O
supported	O
by	O
the	O
Agency	O
for	O
Science	O
,	O
Technology	O
and	O
Research	O
(	O
A*STAR	O
)	O
Computing	O
and	O
Information	O
Science	O
scholarship	O
.	O

De	O
-	O
Biased	O
Court	O
's	O
View	O
Generation	O
with	O
Causality	O

Court	O
's	O
view	O
generation	O
is	O
a	O
novel	O
but	O
essential	O
task	O
for	O
legal	O
AI	O
,	O
aiming	O
at	O
improving	O
the	O
interpretability	O
of	O
judgment	O
prediction	O
results	O
and	O
enabling	O
automatic	O
legal	O
document	O
generation	O
.	O
While	O
prior	O
text	O
-	O
to	O
-	O
text	O
natural	O
language	O
generation	O
(	O
NLG	O
)	O
approaches	O
can	O
be	O
used	O
to	O
address	O
this	O
problem	O
,	O
neglecting	O
the	O
confounding	O
bias	O
from	O
the	O
data	O
generation	O
mechanism	O
can	O
limit	O
the	O
model	O
performance	O
,	O
and	O
the	O
bias	O
may	O
pollute	O
the	O
learning	O
outcomes	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
Attentional	O
and	O
Counterfactual	O
based	O
Natural	O
Language	O
Generation	O
(	O
AC	O
-	O
NLG	O
)	O
method	O
,	O
consisting	O
of	O
an	O
attentional	O
encoder	O
and	O
a	O
pair	O
of	O
innovative	O
counterfactual	O
decoders	O
.	O
The	O
attentional	O
encoder	O
leverages	O
the	O
plaintiff	O
's	O
claim	O
and	O
fact	O
description	O
as	O
input	O
to	O
learn	O
a	O
claim	O
-	O
aware	O
encoder	O
from	O
which	O
the	O
claim	O
-	O
related	O
information	O
in	O
fact	O
description	O
can	O
be	O
emphasized	O
.	O
The	O
counterfactual	O
decoders	O
are	O
employed	O
to	O
eliminate	O
the	O
confounding	O
bias	O
in	O
data	O
and	O
generate	O
judgmentdiscriminative	O
court	O
's	O
views	O
(	O
both	O
supportive	O
and	O
non	O
-	O
supportive	O
views	O
)	O
by	O
incorporating	O
with	O
a	O
synergistic	O
judgment	O
predictive	O
model	O
.	O
Comprehensive	O
experiments	O
show	O
the	O
effectiveness	O
of	O
our	O
method	O
under	O
both	O
quantitative	O
and	O
qualitative	O
evaluation	O
metrics	O
.	O

Owing	O
to	O
the	O
prosperity	O
of	O
machine	O
learning	O
,	O
especially	O
the	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
techniques	O
,	O
many	O
legal	O
assistant	O
systems	O
have	O
been	O
proposed	O
to	O
improve	O
the	O
effectiveness	O
and	O
efficiency	O
of	O
a	O
judge	O
from	O
different	O
aspects	O
,	O
such	O
as	O
relevant	O
case	O
retrieval	O
(	O
Chen	O
et	O
al	O
,	O
2013	O
)	O
,	O
applicable	O
law	O
articles	O
recommendation	O
(	O
Chen	O
et	O
al	O
,	O
2019	O
)	O
,	O
controversy	O
focus	O
mining	O
(	O
Duan	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
judgment	O
prediction	O
(	O
Lin	O
et	O
al	O
,	O
2012	O
;	O
Zhong	O
et	O
al	O
,	O
2018	O
;	O
Hu	O
et	O
al	O
,	O
2018	O
;	O
Jiang	O
et	O
al	O
,	O
2018	O
;	O
Chalkidis	O
et	O
al	O
,	O
*	O
Corresponding	O
Authors	O
.	O

In	O
this	O
section	O
,	O
we	O
first	O
introduce	O
the	O
effect	O
of	O
mechanism	O
confounding	O
bias	O
on	O
the	O
court	O
's	O
view	O
generation	O
and	O
propose	O
a	O
backdoor	O
-	O
inspired	O
method	O
to	O
eliminate	O
that	O
bias	O
.	O
Then	O
,	O
we	O
describe	O
our	O
Attentional	O
and	O
Counterfactual	O
based	O
Natural	O
Language	O
Generation	O
(	O
AC	O
-	O
NLG	O
)	O
model	O
in	O
detail	O
.	O
Fig	O
.	O
3	O
shows	O
the	O
overall	O
framework	O
.	O

As	O
shown	O
in	O
Fig	O
.	O
3	O
,	O
to	O
optimize	O
Eq	O
.	O
3	O
,	O
we	O
use	O
a	O
pair	O
of	O
counterfactual	O
decoders	O
to	O
learn	O
the	O
likelihood	O
P	O
(	O
V	O
|	O
I	O
,	O
j	O
)	O
for	O
each	O
j.	O
At	O
inference	O
,	O
we	O
propose	O
to	O
use	O
a	O
predictor	O
to	O
approximate	O
P	O
(	O
j	O
)	O
.	O
Note	O
that	O
our	O
implementation	O
on	O
backdoor	O
-	O
adjustment	O
can	O
be	O
easily	O
applied	O
for	O
multi	O
-	O
valued	O
confounding	O
with	O
multiple	O
counterfactual	O
decoders	O
.	O

We	O
conduct	O
a	O
human	O
evaluation	O
to	O
better	O
analyze	O
the	O
quality	O
of	O
the	O
generated	O
court	O
's	O
view	O
.	O
First	O
,	O
we	O
randomly	O
sample	O
500	O
test	O
cases	O
,	O
where	O
the	O
ratio	O
of	O
the	O
supported	O
and	O
nonsupported	O
cases	O
are	O
1:1	O
.	O
For	O
each	O
case	O
,	O
we	O
present	O
the	O
generated	O
court	O
's	O
views	O
from	O
each	O
method	O
7	O
with	O
the	O
ground	O
truth	O
to	O
5	O
human	O
annotators	O
with	O
legal	O
backgrounds	O
.	O
The	O
evaluation	O
is	O
conducted	O
following	O
three	O
perspectives	O
:	O
(	O
1	O
)	O
Judgment	O
level	O
.	O
Annotators	O
are	O
asked	O
to	O
give	O
a	O
score	O
(	O
1	O
-	O
5	O
)	O
on	O
the	O
judgment	O
in	O
the	O
generated	O
court	O
's	O
view	O
.	O
1	O
for	O
totally	O
wrong	O
and	O
5	O
for	O
totally	O
correct	O
.	O
(	O
2	O
)	O
Rational	O
level	O
.	O
Annotators	O
are	O
asked	O
to	O
give	O
a	O
score	O
(	O
1	O
-	O
5	O
)	O
on	O
the	O
rationals	O
in	O
the	O
generated	O
court	O
's	O
view	O
.	O
1	O
for	O
the	O
worst	O
and	O
5	O
for	O
the	O
best	O
.	O
(	O
3	O
)	O
Fluency	O
level	O
.	O
Annotators	O
are	O
asked	O
to	O
give	O
a	O
score	O
(	O
1	O
-	O
5	O
)	O
on	O
the	O
fluency	O
of	O
the	O
generated	O
court	O
's	O
view	O
.	O
1	O
for	O
the	O
worst	O
and	O
5	O
for	O
the	O
best	O
.	O

The	O
defendant	O
B	O
return	O
the	O
loan	O
of	O
$	O
495	O
,	O
000	O
.	O
The	O
defendant	O
C	O
return	O
the	O
loan	O
together	O
.	O

After	O
the	O
hearing	O
,	O
the	O
court	O
held	O
the	O
facts	O
as	O
follows	O
:	O
From	O
November	O
20	O
,	O
2010	O
to	O
March	O
23	O
,	O
2011	O
,	O
the	O
defendant	O
B	O
successively	O
borrowed	O
a	O
total	O
of	O
$	O
495	O
,	O
000	O
from	O
the	O
plaintiff	O
A	O
and	O
issued	O
four	O
separate	O
borrowings	O
.	O
The	O
defendant	O
B	O
has	O
not	O
repaid	O
the	O
above	O
loan	O
.	O

The	O
court	O
concluded	O
that	O
:	O
The	O
private	O
lending	O
relationship	O
between	O
Plaintiff	O
A	O
and	O
Defendant	O
B	O
,	O
where	O
the	O
subject	O
was	O
appropriate	O
,	O
the	O
content	O
was	O
legitimate	O
,	O
and	O
the	O
meaning	O
was	O
true	O
,	O
should	O
be	O
confirmed	O
to	O
be	O
legal	O
and	O
valid	O
.	O
The	O
two	O
sides	O
did	O
not	O
agreed	O
in	O
writing	O
on	O
a	O
loan	O
period	O
,	O
so	O
the	O
defendant	O
should	O
return	O
the	O
plaintiff	O
's	O
loan	O
in	O
a	O
timely	O
manner	O
within	O
a	O
reasonable	O
period	O
after	O
the	O
plaintiff	O
urged	O
.	O
The	O
defendant	O
's	O
failure	O
to	O
return	O
the	O
loan	O
timely	O
constituted	O
a	O
breach	O
of	O
contract	O
and	O
should	O
bear	O
corresponding	O
civil	O
liabilities	O
.	O
Therefore	O
,	O
the	O
plaintiff	O
's	O
claim	O
was	O
reasonable	O
and	O
legal	O
,	O
and	O
the	O
court	O
supported	O
it	O
Acceptance	O
.	O

The	O
court	O
concluded	O
that	O
the	O
subject	O
of	O
the	O
private	O
lending	O
relationship	O
between	O
Plaintiff	O
A	O
and	O
Defendant	O
B	O
was	O
qualified	O
,	O
the	O
content	O
was	O
legal	O
,	O
and	O
the	O
meaning	O
was	O
true	O
.	O
It	O
should	O
be	O
deemed	O
valid	O
.	O
The	O
two	O
sides	O
did	O
not	O
agreed	O
in	O
writing	O
on	O
a	O
loan	O
period	O
,	O
the	O
defendant	O
shall	O
return	O
the	O
loan	O
within	O
a	O
reasonable	O
period	O
after	O
the	O
plaintiff	O
urged	O
.	O
The	O
plaintiff	O
's	O
claim	O
requesting	O
the	O
defendant	O
to	O
return	O
the	O
loan	O
of	O
$	O
495	O
,	O
000	O
was	O
in	O
compliance	O
with	O
the	O
law	O
and	O
the	O
court	O
supported	O
it	O
Acceptance	O
.	O
However	O
,	O
the	O
court	O
did	O
not	O
support	O
the	O
claim	O
requesting	O
the	O
defendant	O
C	O
to	O
bear	O
the	O
guarantee	O
liability	O
because	O
the	O
evidence	O
was	O
insufficient	O

.	O

Figure	O
4	O
shows	O
three	O
court	O
's	O
views	O
for	O
a	O
certain	O
case	O
:	O
the	O
court	O
's	O
view	O
generated	O
by	O
PGN	O
,	O
by	O
the	O
proposed	O
AC	O
-	O
NLG	O
method	O
,	O
and	O
the	O
real	O
court	O
's	O
view	O
.	O
We	O
find	O
that	O
the	O
one	O
generated	O
by	O
PGN	O
accepts	O
the	O
claim	O
for	O
principal	O
,	O
but	O
ignores	O
other	O
claims	O
such	O
as	O
issue	O
related	O
to	O
guarantee	O
.	O
Compared	O
with	O
the	O
real	O
court	O
's	O
view	O
,	O
our	O
model	O
accu	O
-	O
rately	O
responds	O
to	O
both	O
claims	O
and	O
produces	O
the	O
correct	O
judgment	O
.	O

The	O
defendant	O
B	O
should	O
return	O
to	O
the	O
defendant	O
$	O
20	O
,	O
000	O
and	O
pay	O
litigation	O
costs	O
of	O
this	O
case	O
,	O
and	O
the	O
defendant	O
C	O
shall	O
undertake	O
joint	O
and	O
several	O
liability	O
.	O

After	O
the	O
hearing	O
,	O
the	O
court	O
held	O
the	O
facts	O
as	O
follows	O
:	O
Plaintiff	O
A	O
supported	O
the	O
facts	O
of	O
his	O
claim	O
and	O
provided	O
the	O
court	O
with	O
a	O
receipt	O
of	O
the	O
loan	O
issued	O
by	O
the	O
defendant	O
B	O
on	O
September	O
22	O
,	O
2008	O
.	O
Although	O
the	O
evidence	O
was	O
not	O
cross	O
-	O
examined	O
by	O
the	O
two	O
defendants	O
in	O
court	O
,	O
it	O
was	O
considered	O
by	O
the	O
court	O
that	O
the	O
evidence	O
was	O
legal	O
,	O
true	O
and	O
relevant	O
to	O
the	O
facts	O
of	O
this	O
case	O
,	O
so	O
the	O
validity	O
of	O
the	O
evidence	O
was	O
confirmed	O
.	O
The	O
facts	O
confirmed	O
by	O
the	O
court	O
are	O
consistent	O
with	O
the	O
facts	O
claimed	O
by	O
the	O
plaintiff	O
A.	O

The	O
court	O
concluded	O
that	O
:	O
The	O
loan	O
relationship	O
between	O
the	O
plaintiff	O
and	O
the	O
defendant	O
is	O
legal	O
and	O
effective	O
,	O
it	O
should	O
be	O
protected	O
by	O
law	O
.	O
The	O
defendant	O
did	O
not	O
return	O
the	O
loan	O
within	O
the	O
agreed	O
time	O
limit	O
,	O
which	O
constituted	O
a	O
breach	O
of	O
contract	O
and	O
should	O
bear	O
the	O
corresponding	O
liability	O
.	O
The	O
plaintiff	O
is	O
now	O
claiming	O
the	O
defendant	O
to	O
return	O
the	O
loan	O
of	O
$	O
20	O
,	O
000	O
,	O
which	O
complies	O
with	O
the	O
law	O
and	O
the	O
court	O
will	O
support	O
it	O
Acceptance	O
.	O
The	O
defendant	O
was	O
legally	O
summoned	O
by	O
the	O
court	O
and	O
failed	O
to	O
appear	O
in	O
court	O
without	O
justifiable	O
reasons	O
.	O

The	O
court	O
concluded	O
that	O
:	O
The	O
loan	O
relationship	O
between	O
the	O
plaintiff	O
and	O
the	O
defendant	O
is	O
legal	O
and	O
valid	O
.	O
The	O
defendant	O
still	O
owes	O
the	O
plaintiff	O
a	O
loan	O
of	O
$	O
20	O
,	O
000	O
and	O
has	O
not	O
returned	O
.	O
The	O
plaintiff	O
's	O
request	O
for	O
the	O
defendant	O
to	O
return	O
the	O
money	O
complies	O
with	O
the	O
law	O
and	O
the	O
court	O
supports	O
it	O
Acceptance	O
.	O
The	O
defendant	O
B	O
was	O
legally	O
summoned	O
by	O
the	O
court	O
and	O
failed	O
to	O
appear	O
in	O
court	O
without	O
justifiable	O
reasons	O
.	O
It	O
was	O
deemed	O
to	O
have	O
waived	O
his	O
right	O
to	O
defend	O
the	O
facts	O
and	O
claims	O
by	O
the	O
plaintiff	O
.	O

The	O
court	O
concluded	O
that	O
:	O
The	O
guaranteed	O
loan	O
relationship	O
between	O
the	O
plaintiff	O
and	O
the	O
defendant	O
is	O
legal	O
and	O
effective	O
.	O
The	O
defendant	O
B	O
still	O
owes	O
the	O
plaintiff	O
a	O
loan	O
of	O
$	O
20	O
,	O
000	O
and	O
has	O
not	O
returned	O
.	O
The	O
plaintiff	O
's	O
claim	O
for	O
the	O
defendant	O
B	O
to	O
return	O
the	O
loan	O
complies	O
with	O
the	O
law	O
and	O
the	O
court	O
supports	O
it	O
Acceptance	O
.	O
Defendant	O
C	O
voluntarily	O
provided	O
guarantee	O
for	O
this	O
loan	O
and	O
did	O
not	O
stipulate	O
the	O
guarantee	O
method	O
and	O
period	O
.	O
According	O
to	O
law	O
,	O
he	O
should	O
bear	O
joint	O
and	O
several	O
liability	O
for	O
the	O
above	O
debt	O
within	O
six	O
months	O
from	O
the	O
date	O
of	O
maturity	O
of	O
the	O
main	O
debt	O
.	O
The	O
main	O
contract	O
in	O
this	O
case	O
did	O
not	O
stipulate	O
the	O
time	O
limit	O
for	O
the	O
performance	O
of	O
the	O
main	O
debt	O
,	O
and	O
the	O
guarantee	O
period	O
should	O
be	O
calculated	O
from	O
the	O
date	O
when	O
the	O
plaintiff	O
claimed	O
the	O
rights	O
.	O
The	O
plaintiff	O
's	O
claim	O
that	O
the	O
defendant	O
C	O
bears	O
joint	O
and	O
several	O
liability	O
for	O
the	O
settlement	O
of	O
the	O
above	O
debts	O
complies	O
with	O
the	O
law	O
,	O
and	O
the	O
court	O
also	O
supports	O
it	O
Acceptance	O
.	O
The	O
two	O
defendants	O
were	O
legally	O
summoned	O
by	O
the	O
court	O
and	O
failed	O
to	O
appear	O
in	O
court	O
without	O
justifiable	O
reasons	O
.	O
It	O
was	O
deemed	O
to	O
have	O
waived	O
his	O
right	O
to	O
defend	O
the	O
facts	O
and	O
claims	O
by	O
the	O
plaintiff	O
.	O

This	O
work	O
was	O
supported	O
by	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
(	O
No	O
.	O
62006207	O
,	O
61625107	O
)	O
,	O
National	O
Key	O
R&D	O
Program	O
of	O
China	O
(	O
No	O
.	O
2018AAA0101900	O
,	O
2018YFC0830200	O
,	O
2018YFC0830206	O
,	O
2020YFC0832500	O
)	O
,	O
the	O
Fundamental	O
Research	O
Funds	O
for	O
the	O
Central	O
Universities	O
.	O
Finally	O
,	O
we	O
would	O
like	O
to	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
helpful	O
feedback	O
and	O
suggestions	O
.	O

All	O
models	O
are	O
trained	O
on	O
2	O
V100	O
GPU	O
(	O
16	O
GB	O
)	O
.	O

1.The	O
defendant	O
B	O
shall	O
return	O
the	O
plaintiff	O
's	O
loan	O
of	O
$	O
30	O
,	O
000	O
and	O
pay	O
the	O
overdue	O
interest	O
at	O
the	O
interest	O
rate	O
of	O
2.4	O
%	O
per	O
month	O
from	O
the	O
date	O
of	O
prosecution	O
to	O
the	O
date	O
of	O
repayment	O
.	O
2	O
.	O
The	O
defendant	O
B	O
shall	O
pay	O
the	O
litigation	O
costs	O
of	O
this	O
case	O
.	O

After	O
the	O
hearing	O
,	O
the	O
court	O
held	O
the	O
facts	O
as	O
follows	O
:	O
On	O
December	O
11	O
,	O
2013	O
,	O
the	O
defendant	O
B	O
borrowed	O
$	O
30	O
,	O
000	O
from	O
the	O
plaintiff	O
A.	O
The	O
defendant	O
B	O
received	O
the	O
loan	O
and	O
gave	O
a	O
receipt	O
of	O
this	O
loan	O
.	O
Note	O
:	O
I	O
have	O
borrowed	O
$	O
30	O
,	O
000	O
from	O
A	O
today	O
.	O

The	O
court	O
concluded	O
that	O
:	O
The	O
facts	O
that	O
defendant	O
B	O
borrowed	O
$	O
30	O
,	O
000	O
from	O
the	O
plaintiff	O
A	O
are	O
clear	O
.	O
The	O
private	O
lending	O
relationship	O
between	O
Plaintiff	O
A	O
and	O
Defendant	O
B	O
is	O
legitimate	O
and	O
valid	O
,	O
it	O
shall	O
be	O
protected	O
by	O
the	O
law	O
.	O
The	O
plaintiff	O
now	O
demands	O
that	O
the	O
defendant	O
repay	O
the	O
loan	O
of	O
$	O
30	O
,	O
000	O
.	O
This	O
demand	O
is	O
justified	O
and	O
should	O
be	O
supported	O
Acceptance	O
.	O
Defendant	O
B	O
refused	O
to	O
appear	O
in	O
court	O
without	O
justification	O
after	O
being	O
legally	O
summoned	O
by	O
the	O
court	O
.	O

The	O
court	O
concluded	O
that	O
:	O
The	O
legitimate	O
private	O
lending	O
relationships	O
are	O
protected	O
by	O
law	O
.	O
The	O
act	O
of	O
borrowing	O
between	O
the	O
plaintiff	O
A	O
and	O
the	O
defendant	O
B	O
did	O
not	O
violate	O
the	O
prohibitive	O
provisions	O
of	O
state	O
laws	O
and	O
regulations	O
,	O
so	O
it	O
should	O
be	O
valid	O
.	O
The	O
fact	O
that	O
the	O
defendant	O
B	O
owed	O
the	O
plaintiff	O
A	O
a	O
loan	O
of	O
$	O
30	O
,	O
000	O
is	O
clear	O
,	O
and	O
the	O
evidence	O
is	O
sufficient	O
.	O
The	O
defendant	O
was	O
supposed	O
to	O
repay	O
the	O
loan	O
in	O
time	O
,	O
and	O
his	O
failure	O
to	O
repay	O
in	O
time	O
constituted	O
a	O
breach	O
of	O
contract	O
,	O
and	O
he	O
shall	O
assume	O
corresponding	O
civil	O
liabilities	O
according	O
to	O
law	O
Acceptance	O
.	O
The	O
receipt	O
of	O
this	O
loan	O
provided	O
by	O
the	O
plaintiff	O
A	O
does	O
not	O
have	O
agreed	O
interest	O
,	O
as	O
not	O
to	O
pay	O
interest	O
.	O
So	O
the	O
court	O
does	O
not	O
support	O
the	O
claim	O
that	O
the	O
plaintiff	O
A	O
asked	O
the	O
defendant	O
B	O
to	O
calculate	O
the	O
interest	O
from	O
the	O
date	O
of	O
the	O
loan	O

.	O
The	O
defendant	O
was	O
summoned	O
by	O
the	O
court	O
and	O
refused	O
to	O
appear	O
in	O
court	O
without	O
justification	O
.	O

The	O
court	O
concluded	O
that	O
:	O
the	O
lending	O
agreement	O
between	O
the	O
plaintiff	O
A	O
and	O
the	O
defendant	O
B	O
contains	O
the	O
true	O
meaning	O
and	O
does	O
not	O
violate	O
the	O
prohibitive	O
provisions	O
of	O
state	O
laws	O
and	O
regulations	O
,	O
it	O
is	O
legal	O
and	O
valid	O
.	O
Although	O
the	O
plaintiff	O
and	O
the	O
defendant	O
did	O
not	O
specifically	O
agree	O
on	O
the	O
time	O
for	O
repayment	O
,	O
after	O
the	O
defendant	O
received	O
the	O
loan	O
,	O
it	O
shall	O
be	O
returned	O
within	O
a	O
reasonable	O
period	O
after	O
being	O
appealed	O
by	O
the	O
plaintiff	O
.	O
If	O
the	O
defendant	O
fails	O
to	O
return	O
it	O
within	O
a	O
reasonable	O
period	O
after	O
being	O
called	O
,	O
the	O
defendant	O
shall	O
be	O
responsible	O
to	O
pay	O
the	O
overdue	O
interest	O
from	O
the	O
date	O
of	O
prosecution	O
Acceptance	O
.	O
For	O
the	O
calculation	O
standard	O
for	O
overdue	O
interest	O
,	O
the	O
plaintiff	O
claimed	O
that	O
the	O
monthly	O
interest	O
rate	O
was	O
2.4	O
%	O
,	O
but	O
it	O
did	O
not	O
provide	O
a	O
corresponding	O
evidence	O
.	O
Therefore	O
,	O
the	O
court	O
does	O
not	O
support	O
this	O
claim	O
of	O
overdue	O
interest	O

.	O
With	O
reference	O
to	O
the	O
loan	O
interest	O
rate	O
announced	O
by	O
the	O
People	O
's	O
Bank	O
of	O
China	O
for	O
the	O
same	O
period	O
,	O
the	O
court	O
determined	O
that	O
overdue	O
interest	O
is	O
calculated	O
at	O
an	O
annual	O
interest	O
rate	O
of	O
5.6	O
%	O
.	O
The	O
fact	O
that	O
the	O
defendant	O
has	O
not	O
returned	O
the	O
loan	O
of	O
$	O
30	O
,	O
000	O
is	O
clear	O
.	O
So	O
the	O
court	O
supports	O
the	O
reasonable	O
part	O
of	O
the	O
plaintiff	O
's	O
claim	O
requesting	O
the	O
defendant	O
to	O
return	O
the	O
loan	O
and	O
pay	O
the	O
overdue	O
interest	O
.	O
Defendant	O
B	O
refused	O
to	O
appear	O
in	O
court	O
without	O
justification	O
after	O
being	O
legally	O
summoned	O
by	O
the	O
court	O
.	O

The	O
two	O
defendants	O
B	O
and	O
C	O
return	O
the	O
loan	O
principal	O
of	O
$	O
2	O
,	O
000	O
,	O
000	O
and	O
interest	O
(	O
The	O
interest	O
will	O
be	O
calculated	O
as	O
four	O
times	O
the	O
interest	O
rate	O
of	O
similar	O
loans	O
of	O
the	O
bank	O
from	O
February	O
28	O
,	O
2014	O
to	O
the	O
date	O
when	O
the	O
judgment	O
is	O
confirmed	O
,	O
it	O
is	O
$	O
40	O
,	O
000	O
temporarily	O
calculated	O
to	O
the	O
date	O
of	O
prosecution	O
)	O
.	O

After	O
the	O
hearing	O
,	O
the	O
court	O
held	O
the	O
facts	O
as	O
follows	O
:	O
The	O
two	O
defendants	O
B	O
and	O
C	O
have	O
spousal	O
relationship	O
.	O
On	O
February	O
28	O
,	O
2014	O
,	O
the	O
defendant	O
B	O
borrowed	O
$	O
2	O
,	O
000	O
,	O
000	O
from	O
the	O
plaintiff	O
A	O
and	O
signed	O
a	O
loan	O
contract	O
,	O
stipulating	O
that	O
the	O
defendant	O
borrowed	O
2	O
million	O
(	O
$	O
2	O
,	O
000	O
,	O
000.00	O
)	O
from	O
the	O
plaintiff	O
,	O
and	O
the	O
loan	O
period	O
is	O
from	O
the	O
date	O
of	O
signing	O
to	O
March	O
27	O
,	O
2014	O
,	O
the	O
interest	O
is	O
calculated	O
at	O
four	O
times	O
the	O
interest	O
rate	O
of	O
similar	O
loans	O
of	O
the	O
People	O
's	O
Bank	O
of	O
China	O
over	O
the	O
same	O
period	O
.	O
The	O
loan	O
period	O
has	O
expired	O
and	O
the	O
defendant	O
refused	O
to	O
return	O
the	O
loan	O
.	O
For	O
this	O
reason	O
,	O
the	O
plaintiff	O
A	O
claimed	O
in	O
court	O
.	O

The	O
court	O
concluded	O
that	O
:	O
The	O
legal	O
loan	O
relationship	O
is	O
protected	O
by	O
law	O
.	O
The	O
fact	O
that	O
the	O
defendant	O
B	O
borrowed	O
$	O
2	O
,	O
000	O
,	O
000	O
from	O
the	O
plaintiff	O
A	O
is	O
clear	O
,	O
and	O
the	O
evidence	O
is	O
indeed	O
sufficient	O
.	O
The	O
defendant	O
B	O
did	O
not	O
return	O
the	O
loan	O
in	O
time	O
according	O
to	O
the	O
contract	O
,	O
which	O
was	O
a	O
breach	O
of	O
contract	O
and	O
should	O
assume	O
the	O
corresponding	O
liabilities	O
for	O
breach	O
of	O
contract	O
according	O
to	O
law	O
.	O
The	O
plaintiff	O
's	O
claim	O
was	O
accepted	O
and	O
the	O
court	O
supports	O
it	O
Acceptance	O
.	O
The	O
defendants	O
failed	O
to	O
appear	O
in	O
court	O
after	O
being	O
legally	O
summoned	O
by	O
the	O
court	O
.	O
The	O
court	O
can	O
judge	O
the	O
case	O
in	O
absentia	O
according	O
to	O
law	O
.	O

The	O
court	O
concluded	O
that	O
:	O
The	O
civil	O
loan	O
relationship	O
formed	O
by	O
the	O
defendant	O
B	O
borrowing	O
money	O
from	O
the	O
plaintiff	O
A	O
and	O
the	O
act	O
of	O
giving	O
a	O
receipt	O
of	O
this	O
loan	O
are	O
based	O
on	O
the	O
true	O
intention	O
of	O
them	O
,	O
they	O
did	O
not	O
violate	O
the	O
mandatory	O
provisions	O
of	O
the	O
laws	O
and	O
regulations	O
,	O
it	O
is	O
legal	O
and	O
valid	O
,	O
and	O
it	O
should	O
be	O
protected	O
by	O
law	O
.	O
The	O
defendant	O
B	O
did	O
not	O
repay	O
the	O
plaintiff	O
's	O
loan	O
of	O
$	O
2	O
million	O
,	O
which	O
constituted	O
a	O
breach	O
of	O
contract	O
,	O
and	O
he	O
should	O
assume	O
the	O
civil	O
liabilities	O
for	O
returning	O
the	O
loan	O
and	O
paying	O
interest	O
.	O
The	O
defendant	O
B	O
and	O
C	O
have	O
spousal	O
relationship	O
.	O
The	O
debt	O
in	O
this	O
case	O
occurred	O
during	O
the	O
marriage	O
,	O
so	O
it	O
should	O
be	O
treated	O
as	O
joint	O
debts	O
and	O
paid	O
by	O
the	O
two	O
defendants	O
together	O
.	O
In	O
summary	O
,	O
the	O
plaintiff	O
's	O
claim	O
is	O
supported	O
by	O
law	O
,	O
and	O
the	O
court	O
supports	O
it	O
Acceptance	O
.	O
The	O
defendants	O
B	O
and	O
C	O
was	O
legally	O
summoned	O
by	O
the	O
court	O
and	O
refused	O
to	O
appear	O
in	O
court	O
without	O
justifiable	O
reasons	O
.	O

The	O
court	O
concluded	O
that	O
:	O
The	O
defendant	O
B	O
has	O
not	O
repaid	O
the	O
loan	O
of	O
$	O
2	O
million	O
from	O
the	O
plaintiff	O
A	O
,	O
and	O
should	O
return	O
it	O
in	O
time	O
and	O
pay	O
interest	O
according	O
to	O
the	O
agreed	O
time	O
limit	O
.	O
The	O
debt	O
occurred	O
during	O
the	O
marriage	O
of	O
the	O
defendants	O
B	O
and	O
C	O
,	O
so	O
it	O
should	O
be	O
treated	O
as	O
joint	O
debts	O
,	O
the	O
two	O
defendants	O
should	O
jointly	O
take	O
the	O
responsibility	O
for	O
repayment	O
.	O
The	O
plaintiff	O
A	O
's	O
claim	O
is	O
legal	O
,	O
and	O
the	O
court	O
supports	O
it	O
Acceptance	O
.	O
The	O
defendants	O
B	O
and	O
C	O
were	O
legally	O
summoned	O
by	O
the	O
court	O
and	O
refused	O
to	O
appear	O
in	O
court	O
without	O
justifiable	O
reasons	O
.	O

After	O
the	O
hearing	O
,	O
the	O
court	O
held	O
the	O
facts	O
as	O
follows	O
:	O
On	O
June	O
28	O
,	O
2013	O
,	O
the	O
defendant	O
B	O
gave	O
a	O
receipt	O
of	O
loan	O
.	O
The	O
defendant	O
needed	O
a	O
loan	O
of	O
$	O
50	O
,	O
000	O
.	O
The	O
loan	O
period	O
was	O
from	O
June	O
28	O
,	O
2013	O
to	O
July	O
27	O
,	O
2013	O
.	O
There	O
was	O
no	O
agreed	O
interest	O
on	O
the	O
loan	O
.	O
After	O
the	O
due	O
date	O
,	O
the	O
defendant	O
agreed	O
to	O
calculate	O
the	O
interest	O
on	O
the	O
unrefunded	O
principal	O
at	O
four	O
times	O
the	O
bank	O
's	O
loan	O
interest	O
during	O
the	O
same	O
period	O
.	O
On	O
the	O
same	O
day	O
,	O
the	O
plaintiff	O
A	O
made	O
a	O
payment	O
of	O
$	O
50	O
,	O
000	O
from	O
his	O
bank	O
account	O
to	O
the	O
defendant	O
B	O
's	O
bank	O
account	O
.	O
Then	O
the	O
defendant	O
B	O
issued	O
a	O
receipt	O
confirming	O
that	O
the	O
loan	O
of	O
$	O
50	O
,	O
000	O
was	O
received	O
.	O
However	O
,	O
the	O
defendant	O
B	O
has	O
not	O
returned	O
the	O
loan	O
principal	O
and	O
interest	O
.	O

The	O
court	O
concluded	O
that	O
:	O
The	O
loan	O
relationship	O
between	O
the	O
plaintiff	O
and	O
the	O
defendant	O
does	O
not	O
violate	O
the	O
compulsory	O
provisions	O
of	O
state	O
laws	O
and	O
administrative	O
regulations	O
,	O
and	O
should	O
be	O
deemed	O
as	O
legal	O
and	O
effective	O
.	O
The	O
defendant	O
B	O
failed	O
to	O
repay	O
the	O
interest	O
according	O
to	O
the	O
receipt	O
,	O
and	O
the	O
plaintiff	O
's	O
claim	O
to	O
return	O
the	O
principal	O
and	O
pay	O
the	O
overdue	O
interest	O
should	O
be	O
supported	O
according	O
to	O
law	O
Acceptance	O
.	O
Defendant	O
B	O
refused	O
to	O
appear	O
in	O
court	O
without	O
justification	O
after	O
being	O
legally	O
summoned	O
by	O
the	O
court	O
,	O
and	O
is	O
deemed	O
to	O
have	O
waived	O
his	O
right	O
to	O
litigation	O
.	O
The	O
court	O
can	O
judge	O
the	O
case	O
in	O
absentia	O
according	O
to	O
law	O
.	O

The	O
court	O
concluded	O
that	O
:	O
The	O
loan	O
between	O
the	O
plaintiff	O
A	O
and	O
the	O
defendant	O
B	O
did	O
not	O
violate	O
the	O
legal	O
provisions	O
,	O
and	O
was	O
based	O
on	O
the	O
true	O
intentions	O
of	O
them	O
,	O
and	O
this	O
case	O
has	O
the	O
loan	O
agreement	O
,	O
receipt	O
and	O
bank	O
statement	O
issued	O
by	O
the	O
defendant	O
.	O
The	O
loan	O
relationship	O
is	O
legal	O
and	O
valid	O
.	O
The	O
plaintiff	O
requested	O
the	O
defendant	O
to	O
return	O
the	O
loan	O
principal	O
of	O
$	O
50	O
,	O
000	O
in	O
compliance	O
with	O
the	O
law	O
,	O
and	O
the	O
court	O
will	O
support	O
it	O
.	O
Acceptance	O
.	O
In	O
this	O
case	O
,	O
the	O
interest	O
was	O
not	O
agreed	O
during	O
the	O
loan	O
period	O
,	O
the	O
court	O
does	O
not	O
support	O
the	O
interest	O
during	O
the	O
loan	O
period	O
in	O
the	O
plaintiff	O
's	O
claim	O

.	O
But	O
the	O
court	O
supports	O
the	O
calculation	O
of	O
the	O
overdue	O
interest	O
from	O
July	O
28	O
,	O
2013	O
to	O
March	O
31	O
,	O
2014	O
based	O
on	O
the	O
four	O
times	O
bank	O
's	O
loan	O
interest	O
rate	O
during	O
the	O
same	O
period	O
.	O

After	O
the	O
hearing	O
,	O
the	O
court	O
held	O
the	O
facts	O
as	O
follows	O
:	O
On	O
November	O
3	O
,	O
2011	O
,	O
the	O
defendant	O
B	O
borrowed	O
$	O
20	O
,	O
000	O
from	O
the	O
plaintiff	O
A	O
,	O
and	O
the	O
defendant	O
B	O
issued	O
a	O
receipt	O
for	O
this	O
loan	O
of	O
$	O
20	O
,	O
000	O
to	O
the	O
plaintiff	O
.	O
The	O
loan	O
receipt	O
did	O
not	O
specify	O
the	O
loan	O
interest	O
and	O
repayment	O
date	O
.	O
The	O
plaintiff	O
stated	O
in	O
court	O
that	O
the	O
defendant	O
B	O
paid	O
about	O
$	O
500	O
but	O
less	O
than	O
$	O
1	O
,	O
000	O
.	O
It	O
was	O
also	O
found	O
that	O
the	O
defendant	O
B	O
and	O
the	O
defendant	O
C	O
registered	O
their	O
marriage	O
on	O
September	O
1	O
,	O
2006	O
.	O

The	O
court	O
concluded	O
that	O
:	O
The	O
legal	O
loan	O
relationship	O
is	O
protected	O
by	O
law	O
.	O
The	O
defendant	O
B	O
borrowed	O
$	O
20	O
,	O
000	O
from	O
the	O
plaintiff	O
A.	O
This	O
case	O
has	O
the	O
evidence	O
of	O
the	O
loan	O
receipt	O
and	O
the	O
plaintiff	O
's	O
statement	O
in	O
court	O
.	O
The	O
facts	O
were	O
clear	O
and	O
the	O
evidence	O
was	O
true	O
and	O
sufficient	O
.	O
The	O
legal	O
loan	O
relationship	O
is	O
protected	O
by	O
law	O
,	O
and	O
the	O
loan	O
principal	O
and	O
interest	O
should	O
be	O
repaid	O
Acceptance	O
.	O

The	O
court	O
concluded	O
that	O
:	O
The	O
legal	O
loan	O
relationship	O
is	O
protected	O
by	O
law	O
.	O
The	O
fact	O
that	O
the	O
defendant	O
B	O
owed	O
the	O
plaintiff	O
A	O
a	O
loan	O
of	O
$	O
20	O
,	O
000	O
was	O
based	O
on	O
a	O
loan	O
receipt	O
and	O
the	O
plaintiff	O
's	O
statement	O
in	O
court	O
.	O
The	O
facts	O
are	O
clear	O
and	O
the	O
evidence	O
is	O
true	O
and	O
sufficient	O
.	O
The	O
loan	O
repayment	O
period	O
does	O
not	O
stipulate	O
the	O
repayment	O
period	O
,	O
the	O
plaintiff	O
can	O
urge	O
the	O
defendant	O
to	O
repay	O
within	O
a	O
reasonable	O
period	O
.	O
Now	O
the	O
plaintiff	O
claims	O
that	O
the	O
defendants	O
repay	O
the	O
loan	O
principal	O
of	O
$	O
20	O
,	O
000	O
,	O
it	O
complies	O
with	O
the	O
law	O
and	O
the	O
court	O
supports	O
it	O
.	O
The	O
defendant	O
B	O
and	O
the	O
defendant	O
C	O
have	O
spousal	O
relationship	O
.	O
In	O
this	O
case	O
,	O
the	O
debt	O
in	O
this	O
case	O
occurred	O
during	O
the	O
marriage	O
,	O
so	O
it	O
should	O
be	O
treated	O
as	O
joint	O
debts	O
and	O
paid	O
by	O
the	O
two	O
defendants	O
together	O
Acceptance	O
.	O

The	O
court	O
concluded	O
that	O
:	O
The	O
legal	O
loan	O
relationship	O
is	O
protected	O
by	O
law	O
.	O
The	O
fact	O
that	O
the	O
defendant	O
B	O
borrowed	O
money	O
from	O
the	O
plaintiff	O
A	O
was	O
based	O
on	O
a	O
loan	O
receipt	O
and	O
the	O
plaintiff	O
's	O
statement	O
.	O
The	O
facts	O
are	O
clear	O
,	O
and	O
the	O
evidence	O
is	O
true	O
and	O
sufficient	O
.	O
The	O
loan	O
interest	O
rate	O
is	O
not	O
agreed	O
on	O
the	O
receipt	O
,	O
it	O
shall	O
be	O
deemed	O
as	O
non	O
-	O
payment	O
of	O
interest	O
.	O
The	O
plaintiff	O
's	O
opinion	O
that	O
the	O
amount	O
paid	O
by	O
the	O
defendant	O
B	O
is	O
interest	O
has	O
no	O
factual	O
basis	O
and	O
the	O
court	O
will	O
not	O
approve	O
it	O
.	O
Because	O
the	O
plaintiff	O
could	O
not	O
determine	O
the	O
specific	O
amount	O
paid	O
by	O
the	O
defendant	O
B	O
,	O
the	O
court	O
determined	O
the	O
amount	O
paid	O
by	O
the	O
defendant	O
B	O
as	O
$	O
500	O
at	O
his	O
discretion	O
,	O
and	O
the	O
$	O
500	O
should	O
be	O
deducted	O
from	O
the	O
loan	O
principal	O
.	O
If	O
the	O
loan	O
does	O
not	O
agree	O
on	O
the	O
repayment	O
period	O
,	O
the	O
debtor	O
shall	O
return	O
the	O
loan	O
if	O
the	O
creditor	O
requests	O
it	O
to	O
be	O
returned	O
according	O
to	O
trading	O
habits	O
.	O
The	O
defendant	O
B	O
and	O
the	O
defendant	O
C	O
registered	O
their	O
marriage	O
on	O
September	O
1	O
,	O
2006	O
.	O
The	O
debt	O
in	O
this	O
case	O
occurred	O
during	O
their	O
marriage	O
,	O
so	O
it	O
is	O
the	O
joint	O
debt	O
of	O
the	O
two	O
defendants	O
and	O
should	O
be	O
repaid	O
together	O
Acceptance	O
.	O
The	O
defendants	O
B	O
and	O
C	O
were	O
legally	O
summoned	O
by	O
the	O
court	O
and	O
refused	O
to	O
appear	O
in	O
court	O
without	O
justifiable	O
reasons	O
.	O

The	O
two	O
defendants	O
B	O
and	O
C	O
are	O
required	O
to	O
return	O
the	O
loan	O
of	O
$	O
180	O
,	O
000	O
and	O
pay	O
the	O
overdue	O
interest	O
of	O
the	O
loan	O
of	O
$	O
100	O
,	O
000	O
(	O
from	O
March	O
9	O
,	O
2013	O
,	O
the	O
monthly	O
interest	O
rate	O
is	O
calculated	O
at	O
1.87	O
%	O
to	O
the	O
date	O
that	O
the	O
judgment	O
is	O
confirmed	O
)	O

After	O
the	O
hearing	O
,	O
the	O
court	O
held	O
the	O
facts	O
as	O
follows	O
:	O
In	O
support	O
of	O
the	O
facts	O
claimed	O
by	O
the	O
plaintiff	O
,	O
the	O
following	O
evidence	O
materials	O
were	O
provided	O
to	O
this	O
court	O
within	O
the	O
proof	O
period	O
:	O
1	O
.	O
Two	O
loan	O
agreements	O
to	O
prove	O
the	O
fact	O
that	O
the	O
defendant	O
B	O
borrowed	O
$	O
180	O
,	O
000	O
from	O
the	O
plaintiff	O
.	O
2	O
.	O
One	O
piece	O
of	O
marriage	O
registration	O
information	O
,	O
to	O
prove	O
that	O
the	O
loan	O
in	O
the	O
case	O
occurred	O
during	O
the	O
marriage	O
of	O
the	O
two	O
defendants	O
,	O
and	O
should	O
be	O
the	O
joint	O
debts	O
of	O
the	O
two	O
defendants	O
.	O
Although	O
the	O
evidence	O
provided	O
by	O
the	O
plaintiff	O
has	O
not	O
been	O
cross	O
-	O
examined	O
by	O
the	O
two	O
defendants	O
,	O
the	O
court	O
found	O
that	O
the	O
content	O
of	O
the	O
abovementioned	O
evidence	O
was	O
objective	O
and	O
clear	O
,	O
the	O
source	O
form	O
was	O
legal	O
,	O
and	O
was	O
related	O
to	O
the	O
facts	O
of	O
the	O
case	O
,	O
so	O
it	O
was	O
accepted	O
.	O
Based	O
on	O
the	O
evidence	O
adopted	O
above	O
and	O
the	O
court	O
investigation	O
,	O
the	O
facts	O
confirmed	O
by	O
the	O
court	O
are	O
consistent	O
with	O
the	O
facts	O
claimed	O
by	O
the	O
plaintiff	O
.	O

The	O
court	O
concluded	O
that	O
:	O
The	O
private	O
lending	O
relationship	O
between	O
the	O
plaintiff	O
and	O
the	O
defendant	O
B	O
is	O
established	O
and	O
legally	O
valid	O
.	O
As	O
the	O
borrower	O
,	O
the	O
defendant	O
B	O
failed	O
to	O
repay	O
the	O
loan	O
,	O
which	O
constituted	O
a	O
breach	O
of	O
contract	O
and	O
should	O
bear	O
corresponding	O
civil	O
liabilities	O
.	O
The	O
plaintiff	O
's	O
claim	O
has	O
sufficient	O
evidence	O
and	O
complies	O
with	O
the	O
law	O
,	O
and	O
the	O
court	O
supports	O
it	O
Acceptance	O
.	O
The	O
two	O
defendants	O
were	O
legally	O
summoned	O
by	O
the	O
court	O
and	O
failed	O
to	O
appear	O
in	O
court	O
without	O
justifiable	O
reasons	O
.	O
It	O
was	O
deemed	O
to	O
have	O
waived	O
his	O
right	O
to	O
defend	O
the	O
facts	O
and	O
claims	O
by	O
the	O
plaintiff	O
.	O

The	O
court	O
concluded	O
that	O
:	O
The	O
private	O
lending	O
relationship	O
between	O
the	O
plaintiff	O
and	O
the	O
defendant	O
B	O
is	O
established	O
and	O
legally	O
valid	O
.	O
As	O
the	O
borrower	O
,	O
the	O
defendant	O
B	O
failed	O
to	O
perform	O
the	O
repayment	O
obligations	O
in	O
time	O
and	O
should	O
bear	O
corresponding	O
civil	O
liabilities	O
.	O
The	O
plaintiff	O
's	O
changed	O
claim	O
has	O
sufficient	O
evidence	O
and	O
complies	O
with	O
the	O
law	O
,	O
and	O
the	O
court	O
will	O
support	O
it	O
Acceptance	O
.	O
The	O
two	O
defendants	O
were	O
legally	O
summon	O
by	O
the	O
court	O
and	O
failed	O
to	O
appear	O
in	O
court	O
without	O
justifiable	O
reasons	O
.	O
It	O
was	O
deemed	O
to	O
have	O
waived	O
his	O
right	O
to	O
defend	O
the	O
facts	O
and	O
claims	O
by	O
the	O
plaintiff	O
.	O

The	O
court	O
concluded	O
that	O
:	O
The	O
civil	O
lending	O
relationship	O
between	O
the	O
plaintiff	O
A	O
and	O
the	O
defendant	O
B	O
was	O
established	O
and	O
legally	O
valid	O
.	O
As	O
the	O
borrower	O
,	O
the	O
defendant	O
B	O
failed	O
to	O
fully	O
perform	O
the	O
repayment	O
obligations	O
as	O
agreed	O
,	O
which	O
constituted	O
a	O
breach	O
of	O
contract	O
and	O
should	O
bear	O
corresponding	O
civil	O
liabilities	O
.	O
Because	O
the	O
loan	O
in	O
this	O
case	O
was	O
formed	O
during	O
the	O
marriage	O
of	O
the	O
two	O
defendants	O
,	O
in	O
view	O
of	O
the	O
fact	O
that	O
the	O
defendant	O
C	O
did	O
not	O
respond	O
to	O
the	O
claim	O
and	O
did	O
not	O
appear	O
in	O
court	O
to	O
participate	O
in	O
the	O
litigation	O
,	O
the	O
debt	O
owned	O
by	O
the	O
defendant	O
B	O
personally	O
should	O
be	O
regarded	O
as	O
the	O
joint	O
debts	O
of	O
the	O
defendant	O
B	O
and	O
C.	O
The	O
plaintiff	O
's	O
changed	O
claim	O
has	O
sufficient	O
evidence	O
and	O
complies	O
with	O
the	O
law	O
,	O
and	O
the	O
court	O
will	O
support	O
it	O
Acceptance	O
.	O
The	O
two	O
defendants	O
were	O
legally	O
summon	O
by	O
the	O
court	O
and	O
failed	O
to	O
appear	O
in	O
court	O
without	O
justifiable	O
reasons	O
.	O
It	O
was	O
deemed	O
to	O
have	O
waived	O
his	O
right	O
to	O
defend	O
the	O
facts	O
and	O
claims	O
by	O
the	O
plaintiff	O
.	O

After	O
the	O
hearing	O
,	O
the	O
court	O
held	O
the	O
facts	O
as	O
follows	O
:	O
On	O
September	O
30	O
,	O
2013	O
and	O
August	O
25	O
,	O
2014	O
,	O
the	O
defendant	O
B	O
borrowed	O
$	O
10	O
,	O
000	O
each	O
time	O
from	O
the	O
plaintiff	O
A.	O
The	O
defendant	O
issued	O
a	O
loan	O
receipt	O
to	O
the	O
plaintiff	O
for	O
each	O
of	O
the	O
two	O
loans	O
.	O
There	O
was	O
no	O
written	O
agreement	O
on	O
the	O
interest	O
and	O
loan	O
period	O
.	O
Later	O
,	O
the	O
defendant	O
did	O
not	O
return	O
the	O
loan	O
,	O
then	O
it	O
caused	O
a	O
dispute	O
.	O
The	O
above	O
facts	O
are	O
proved	O
by	O
two	O
receipts	O
of	O
the	O
loan	O
provided	O
by	O
the	O
plaintiff	O
and	O
the	O
plaintiff	O
's	O
statement	O
in	O
the	O
court	O
.	O

The	O
court	O
concluded	O
that	O
:	O
The	O
private	O
lending	O
relationship	O
between	O
the	O
plaintiff	O
and	O
the	O
defendant	O
is	O
established	O
according	O
to	O
law	O
,	O
and	O
is	O
effective	O
from	O
the	O
date	O
the	O
plaintiff	O
provides	O
the	O
defendant	O
with	O
the	O
loan	O
.	O
After	O
the	O
plaintiff	O
provided	O
the	O
loan	O
to	O
the	O
defendant	O
,	O
the	O
defendant	O
failed	O
to	O
return	O
the	O
loan	O
as	O
agreed	O
,	O
it	O
was	O
obviously	O
a	O
breach	O
of	O
contract	O
.	O
Therefore	O
,	O
the	O
plaintiff	O
's	O
claim	O
requesting	O
the	O
defendant	O
to	O
return	O
the	O
loan	O
principal	O
of	O
$	O
28	O
,	O
000	O
was	O
justified	O
,	O
and	O
the	O
court	O
supports	O
it	O
Acceptance	O
.	O
The	O
defendant	O
was	O
legally	O
summoned	O
by	O
the	O
court	O
and	O
refused	O
to	O
appear	O
in	O
court	O
without	O
justifiable	O
reasons	O
to	O
participate	O
in	O
the	O
proceedings	O
.	O

The	O
court	O
concluded	O
that	O
:	O
The	O
private	O
lending	O
relationship	O
between	O
the	O
plaintiff	O
and	O
defendant	O
is	O
established	O
according	O
to	O
law	O
and	O
should	O
be	O
protected	O
by	O
law	O
.	O
The	O
defendant	O
borrowed	O
$	O
10	O
,	O
000	O
from	O
the	O
plaintiff	O
.	O
The	O
facts	O
were	O
clear	O
and	O
the	O
evidence	O
was	O
sufficient	O
.	O
The	O
plaintiff	O
now	O
requires	O
the	O
defendant	O
to	O
repay	O
the	O
loan	O
of	O
$	O
10	O
,	O
000	O
.	O
The	O
reasons	O
are	O
justified	O
,	O
and	O
the	O
court	O
supports	O
it	O
Acceptance	O
.	O
But	O
the	O
court	O
does	O
not	O
support	O
the	O
plaintiff	O
's	O
claim	O
requesting	O
the	O
defendant	O
to	O
pay	O
interest	O
on	O
the	O
loan	O
because	O
the	O
plaintiff	O
failed	O
to	O
provide	O
evidence	O
to	O
prove	O
the	O
fact	O
that	O
both	O
of	O
them	O
agreed	O
on	O
the	O
interest	O
of	O
the	O
loan	O

.	O
The	O
defendant	O
was	O
legally	O
summoned	O
by	O
this	O
court	O
and	O
refused	O
to	O
appear	O
in	O
court	O
without	O
justifiable	O
reasons	O
to	O
participate	O
in	O
the	O
proceedings	O
.	O

Target	O
-	O
specified	O
Sequence	O
Labeling	O
with	O
Multi	O
-	O
head	O
Self	O
-	O
attention	O
for	O
Target	O
-	O
oriented	O
Opinion	O
Words	O
Extraction	O

Given	O
a	O
sentence	O
s	O
=	O
{	O
w	O
1	O
,	O
w	O
2	O
,	O
...	O
,	O
w	O
n	O
}	O
consisting	O
of	O
n	O
words	O
,	O
an	O
aspect	O
(	O
opinion	O
target	O
)	O
a	O
=	O
{	O
w	O
i	O
,	O
w	O
i+1	O
,	O
...	O
,	O
w	O
i+k	O
}	O
,	O
and	O
an	O
opinion	O
term	O
o	O
=	O
{	O
w	O
j	O
,	O
w	O
j+1	O
,	O
...	O
,	O
w	O
j+m	O
}	O
(	O
a	O
and	O
o	O
are	O
substrings	O
of	O
s	O
)	O
,	O
the	O
probabilities	O
of	O
target	O
-	O
oriented	O
opinion	O
terms	O
are	O
defined	O
as	O
p	O
(	O
o	O
|	O
s	O
,	O
a	O
)	O
in	O
the	O
TOWE	O
task	O
and	O
the	O
probabilities	O
of	O
aspect	O
-	O
opinion	O
pairs	O
are	O
defined	O
as	O
p	O
(	O
a	O
,	O
o	O
|	O
s	O
)	O
=	O
p	O
(	O
a	O
|	O
s	O
)	O
×	O
p	O
(	O
o	O
|	O
s	O
,	O
a	O
)	O
in	O
the	O
AOPE	O
task	O
.	O
The	O
BIO	O
tagging	O
scheme	O
(	O
Ramshaw	O
and	O
Marcus	O
,	O
1995	O
)	O
and	O
a	O
special	O
symbol	O
"	O
[	O
SEP	O
]	O
"	O
are	O
applied	O
to	O
this	O
task	O
,	O
where	O
each	O
word	O
w	O
i	O
in	O
the	O
sentence	O
s	O
is	O
tagged	O
as	O
y	O
i	O
{	O
B	O
,	O
I	O
,	O
O	O
,	O
[	O
SEP	O
]	O
}	O
(	O
B	O
:	O
Beginning	O
,	O
I	O
:	O
Inside	O
,	O
O	O
:	O
Others	O
,	O
[	O
SEP	O
]	O
:	O
the	O
tag	O
of	O
an	O
aspect	O
)	O
.	O

To	O
evaluate	O
the	O
performance	O
of	O
our	O
model	O
2	O
,	O
we	O
conduct	O
experiments	O
on	O
two	O
public	O
datasets	O
from	O
laptop	O
and	O
restaurant	O
domains	O
.	O
These	O
two	O
datasets	O
were	O
respectively	O
built	O
by	O
Fan	O
et	O
al	O
(	O
2019	O
)	O
for	O
TOWE	O
and	O
Chen	O
et	O
al	O
(	O
2020	O
)	O
for	O
AOPE	O
based	O
on	O
SemEval	O
Challenge	O
2014	O
Task	O
4	O
,	O
SemEval	O
Challenge	O
2015	O
Task	O
12	O
,	O
and	O
SemEval	O
Challenge	O
2016	O
Task	O
5	O
(	O
Pontiki	O
et	O
al	O
,	O
2014	O
(	O
Pontiki	O
et	O
al	O
,	O
,	O
2015	O
(	O
Pontiki	O
et	O
al	O
,	O
,	O
2016	O
.	O
For	O
the	O
first	O
dataset	O
,	O
every	O
sentence	O
was	O
annotated	O
by	O
two	O
people	O
,	O
and	O
the	O
conflicts	O
were	O
checked	O
and	O
eliminated	O
manually	O
.	O
The	O
second	O
dataset	O
was	O
developed	O
by	O
extending	O
the	O
first	O
one	O
.	O
The	O
statistics	O
of	O
these	O
benchmark	O
datasets	O
are	O
shown	O
in	O
Table	O
2	O
,	O
from	O
which	O
we	O
can	O
observe	O
that	O
the	O
second	O
dataset	O
includes	O
many	O
negative	O
samples	O
for	O
AOPE	O
(	O
i.e.	O
,	O
the	O
sentences	O
only	O
contain	O
aspects	O
and	O
opinion	O
terms	O
,	O
without	O
any	O
aspect	O
-	O
opinion	O
pairs	O
)	O
.	O
Note	O
that	O
these	O
negative	O
samples	O
will	O
also	O
be	O
considered	O
when	O
testing	O
our	O
model	O
on	O
AOPE	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
target	O
-	O
specified	O
sequence	O
labeling	O
method	O
based	O
on	O
multi	O
-	O
head	O
selfattention	O
(	O
TSMSA	O
)	O
and	O
a	O
multi	O
-	O
task	O
version	O
(	O
MT	O
-	O
TSMSA	O
)	O
to	O
deal	O
with	O
TOWE	O
and	O
AOPE	O
,	O
respectively	O
.	O
In	O
our	O
methods	O
,	O
the	O
encoder	O
is	O
capable	O
of	O
capturing	O
the	O
information	O
of	O
the	O
specific	O
aspect	O
which	O
is	O
labeled	O
by	O
a	O
special	O
symbol	O
"	O
[	O
SEP	O
]	O
"	O
.	O
Experimental	O
results	O
demonstrate	O
that	O
TSMSA	O
and	O
MT	O
-	O
TSMSA	O
achieve	O
quite	O
competitive	O
performance	O
in	O
most	O
cases	O
.	O
When	O
combining	O
aspect	O
and	O
opinion	O
words	O
extraction	O
with	O
TOWE	O
,	O
our	O
MT	O
-	O
TSMSA	O
can	O
slightly	O
improve	O
the	O
performance	O
as	O
compared	O
with	O
TSMSA	O
.	O
In	O
the	O
future	O
,	O
we	O
plan	O
to	O
extend	O
our	O
approaches	O
to	O
sentiment	O
classification	O
of	O
pairs	O
and	O
explore	O
an	O
efficient	O
model	O
with	O
a	O
one	O
-	O
stage	O
inference	O
process	O
to	O
reduce	O
the	O
time	O
complexity	O
on	O
AOPE	O
.	O

We	O
are	O
grateful	O
to	O
the	O
reviewers	O
for	O
their	O
constructive	O
comments	O
and	O
suggestions	O
on	O
this	O
study	O
.	O
This	O
work	O
has	O
been	O
supported	O
by	O
the	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
(	O
61972426	O
)	O
and	O
Guangdong	O
Basic	O
and	O
Applied	O
Basic	O
Research	O
Foundation	O
(	O
2020A1515010536	O
)	O
.	O

Modeling	O
Framing	O
in	O
Immigration	O
Discourse	O
on	O
Social	O
Media	O

Framing	O
selects	O
particular	O
aspects	O
of	O
an	O
issue	O
and	O
makes	O
them	O
salient	O
in	O
communicating	O
a	O
message	O
(	O
Entman	O
,	O
1993	O
)	O
.	O
Framing	O
can	O
impact	O
how	O
people	O
understand	O
issues	O
,	O
attribute	O
responsibility	O
(	O
Iyengar	O
,	O
1991	O
)	O
,	O
and	O
endorse	O
possible	O
solutions	O
,	O
thus	O
having	O
major	O
implications	O
for	O
public	O
opinion	O
and	O
policy	O
decisions	O
(	O
Chong	O
and	O
Druckman	O
,	O
2007	O
)	O
.	O
While	O
past	O
work	O
has	O
studied	O
framing	O
by	O
the	O
news	O
media	O
and	O
the	O
political	O
elite	O
,	O
little	O
is	O
known	O
about	O
how	O
ordinary	O
people	O
frame	O
political	O
issues	O
.	O
Yet	O
,	O
framing	O
by	O
ordinary	O
people	O
can	O
influence	O
others	O
'	O
perspectives	O
and	O
may	O
even	O
shape	O
elites	O
'	O
rhetoric	O
(	O
Russell	O
Neuman	O
et	O
al	O
,	O
2014	O
)	O
.	O
To	O
shed	O
light	O
on	O
this	O
important	O
topic	O
,	O
we	O
focus	O
on	O
one	O
issue	O
-	O
immigration	O
-	O
and	O
develop	O
a	O
new	O
methodology	O
to	O
computationally	O
analyze	O
its	O
framing	O
on	O
Twitter	O
.	O
Our	O
work	O
highlights	O
unique	O
insights	O
that	O
social	O
media	O
data	O
offers	O
.	O
The	O
massive	O
amount	O
of	O
available	O
social	O
media	O
content	O
enables	O
us	O
to	O
compare	O
framing	O
strategies	O
across	O
countries	O
and	O
political	O
ideologies	O
.	O
Furthermore	O
,	O
social	O
media	O
provides	O
unique	O
insights	O
into	O
how	O
messages	O
resonate	O
with	O
audiences	O
through	O
interactive	O
signals	O
such	O
as	O
retweets	O
and	O
favorites	O
.	O
By	O
jointly	O
analyzing	O
the	O
production	O
and	O
reception	O
of	O
frames	O
on	O
Twitter	O
,	O
we	O
provide	O
an	O
in	O
-	O
depth	O
analysis	O
of	O
immigration	O
framing	O
by	O
and	O
on	O
the	O
public	O
.	O
Political	O
communications	O
research	O
has	O
identified	O
numerous	O
typologies	O
of	O
frames	O
,	O
such	O
as	O
issuegeneric	O
policy	O
,	O
immigration	O
-	O
specific	O
,	O
and	O
narrative	O
.	O
Each	O
of	O
these	O
frame	O
types	O
can	O
significantly	O
shape	O
the	O
audience	O
's	O
perceptions	O
of	O
an	O
issue	O
(	O
Iyengar	O
,	O
1991	O
;	O
Chong	O
and	O
Druckman	O
,	O
2007	O
;	O
Lecheler	O
et	O
al	O
,	O
2015	O
)	O
,	O
but	O
prior	O
NLP	O
work	O
seeking	O
to	O
detect	O
frames	O
in	O
mass	O
media	O
(	O
e.g.	O
Card	O
et	O
al	O
,	O
2016	O
;	O
Field	O
et	O
al	O
,	O
2018	O
;	O
Kwak	O
et	O
al	O
,	O
2020	O
)	O
has	O
largely	O
been	O
limited	O
to	O
a	O
single	O
issue	O
-	O
generic	O
policy	O
typology	O
.	O
Multiple	O
dimensions	O
of	O
framing	O
must	O
be	O
considered	O
in	O
order	O
to	O
better	O
understand	O
the	O
structure	O
of	O
immigration	O
discourse	O
and	O
its	O
effect	O
on	O
public	O
opinion	O
and	O
attitudes	O
.	O
We	O
thus	O
create	O
a	O
novel	O
dataset	O
of	O
immigration	O
-	O
related	O
tweets	O
containing	O
labels	O
for	O
each	O
typology	O
to	O
facilitate	O
more	O
nuanced	O
computational	O
analyses	O
of	O
framing	O
.	O
This	O
work	O
combines	O
political	O
communication	O
theory	O
with	O
NLP	O
to	O
model	O
multiple	O
framing	O
strategies	O
and	O
analyze	O
how	O
the	O
public	O
on	O
Twitter	O
frames	O
immigration	O
.	O
Our	O
contributions	O
are	O
as	O
follows	O
:	O
(	O
1	O
)	O
We	O
create	O
a	O
novel	O
dataset	O
of	O
immigrationrelated	O
tweets	O
labeled	O
for	O
issue	O
-	O
generic	O
policy	O
,	O
immigration	O
-	O
specific	O
,	O
and	O
narrative	O
frames	O
.	O
(	O
2	O
)	O
We	O
develop	O
and	O
evaluate	O
multiple	O
methods	O
to	O
detect	O
each	O
type	O
of	O
frame	O
.	O
(	O
3	O
)	O
We	O
illustrate	O
how	O
a	O
message	O
's	O
framing	O
is	O
influenced	O
by	O
its	O
author	O
's	O
ideology	O
and	O
country	O
.	O
(	O
4	O
)	O
We	O
show	O
how	O
a	O
message	O
's	O
framing	O
affects	O
its	O
audience	O
by	O
analyzing	O
favoriting	O
and	O
retweeting	O
behaviors	O
.	O
Finally	O
,	O
our	O
work	O
highlights	O
the	O
need	O
to	O
consider	O
multiple	O
framing	O
typologies	O
and	O
their	O
effects	O
.	O

Framing	O
serves	O
four	O
functions	O
:	O
(	O
i	O
)	O
defining	O
problems	O
,	O
(	O
ii	O
)	O
diagnosing	O
causes	O
,	O
(	O
ii	O
)	O
making	O
evaluative	O
judgments	O
,	O
and	O
(	O
iv	O
)	O
suggesting	O
solutions	O
(	O
Entman	O
,	O
1993	O
)	O
.	O
Framing	O
impacts	O
what	O
people	O
notice	O
about	O
an	O
issue	O
,	O
making	O
it	O
a	O
key	O
mechanism	O
by	O
which	O
a	O
text	O
influences	O
its	O
audience	O
.	O
Framing	O
Typologies	O
We	O
draw	O
upon	O
distinct	O
typologies	O
of	O
frames	O
that	O
can	O
be	O
applied	O
to	O
the	O
issue	O
of	O
immigration	O
:	O
(	O
1	O
)	O
issue	O
-	O
specific	O
,	O
which	O
identify	O
aspects	O
of	O
a	O
particular	O
issue	O
,	O
or	O
(	O
2	O
)	O
issue	O
-	O
generic	O
,	O
which	O
appear	O
across	O
a	O
variety	O
of	O
issues	O
and	O
facilitate	O
cross	O
-	O
issue	O
comparison	O
(	O
de	O
Vreese	O
,	O
2005	O
)	O
.	O
Issue	O
-	O
generic	O
frames	O
include	O
policy	O
frames	O
that	O
focus	O
on	O
aspects	O
of	O
issues	O
important	O
for	O
policymaking	O
,	O
such	O
as	O
economic	O
consequences	O
or	O
fairness	O
and	O
equality	O
(	O
Boydstun	O
et	O
al	O
,	O
2013	O
)	O
.	O
Other	O
generic	O
frames	O
focus	O
on	O
a	O
text	O
's	O
narrative	O
;	O
news	O
articles	O
use	O
both	O
episodic	O
frames	O
,	O
which	O
highlight	O
specific	O
events	O
or	O
individuals	O
,	O
and	O
thematic	O
frames	O
,	O
which	O
place	O
issues	O
within	O
a	O
broader	O
social	O
context	O
.	O
The	O
use	O
of	O
episodic	O
versus	O
thematic	O
frames	O
can	O
influence	O
the	O
audience	O
's	O
attitudes	O
.	O
For	O
example	O
,	O
episodic	O
frames	O
lead	O
audiences	O
to	O
attribute	O
responsibility	O
for	O
issues	O
such	O
as	O
poverty	O
to	O
individual	O
citizens	O
while	O
thematic	O
frames	O
lead	O
them	O
to	O
hold	O
the	O
government	O
responsible	O
(	O
Iyengar	O
,	O
1991	O
)	O
.	O
Issue	O
-	O
specific	O
frames	O
for	O
immigration	O
focus	O
on	O
the	O
portrayal	O
of	O
immigrants	O
.	O
Our	O
analysis	O
uses	O
Benson	O
(	O
2013	O
)	O
's	O
set	O
of	O
issue	O
-	O
specific	O
frames	O
,	O
which	O
represent	O
immigrants	O
as	O
heroes	O
(	O
cultural	O
diversity	O
,	O
integration	O
,	O
good	O
workers	O
)	O
,	O
victims	O
(	O
humanitarian	O
,	O
global	O
economy	O
,	O
discrimination	O
)	O
,	O
and	O
threats	O
(	O
to	O
jobs	O
,	O
public	O
order	O
,	O
taxpayers	O
,	O
cultural	O
values	O
)	O
.	O
Both	O
issue	O
-	O
specific	O
and	O
generic	O
frames	O
provide	O
unique	O
insights	O
but	O
present	O
advantages	O
and	O
drawbacks	O
.	O
While	O
issue	O
-	O
specific	O
frames	O
analysis	O
are	O
specific	O
and	O
detailed	O
,	O
they	O
are	O
hard	O
to	O
generalize	O
and	O
replicate	O
across	O
studies	O
,	O
which	O
is	O
a	O
key	O
advantage	O
for	O
generic	O
frames	O
(	O
de	O
Vreese	O
,	O
2005	O
)	O
.	O
Framing	O
effects	O
Studies	O
of	O
framing	O
typically	O
focus	O
on	O
either	O
frame	O
-	O
building	O
or	O
framesetting	O
(	O
Scheufele	O
,	O
1999	O
;	O
de	O
Vreese	O
,	O
2005	O
)	O
.	O
Frame	O
-	O
building	O
is	O
the	O
process	O
by	O
which	O
external	O
factors	O
,	O
such	O
as	O
a	O
journalist	O
's	O
ideology	O
or	O
economic	O
pressures	O
,	O
influence	O
what	O
frames	O
are	O
used	O
;	O
frame	O
-	O
building	O
studies	O
thus	O
treat	O
framing	O
as	O
the	O
dependent	O
variable	O
.	O
Frame	O
-	O
setting	O
studies	O
treat	O
frames	O
as	O
independent	O
variables	O
that	O
impact	O
how	O
an	O
audience	O
interprets	O
and	O
evaluates	O
issues	O
.	O
Prior	O
analyses	O
of	O
frame	O
-	O
building	O
in	O
immigration	O
news	O
highlight	O
region	O
and	O
ideology	O
as	O
particularly	O
important	O
factors	O
.	O
Right	O
-	O
leaning	O
media	O
from	O
conservative	O
regions	O
are	O
more	O
likely	O
to	O
frame	O
immigrants	O
as	O
intruders	O
(	O
van	O
Gorp	O
,	O
2005	O
)	O
,	O
and	O
as	O
threats	O
to	O
the	O
economy	O
and	O
public	O
safety	O
(	O
Fryberg	O
et	O
al	O
,	O
2012	O
)	O
.	O
Framing	O
also	O
differs	O
across	O
countries	O
;	O
while	O
the	O
US	O
press	O
emphasizes	O
public	O
order	O
,	O
discrimination	O
,	O
and	O
humanitarian	O
concerns	O
,	O
the	O
French	O
press	O
more	O
frequently	O
frames	O
immigrants	O
as	O
victims	O
of	O
global	O
inequality	O
(	O
Benson	O
,	O
2013	O
)	O
.	O
Frame	O
-	O
setting	O
has	O
also	O
been	O
studied	O
in	O
the	O
context	O
of	O
immigration	O
.	O
For	O
example	O
,	O
experimental	O
work	O
has	O
shown	O
that	O
frames	O
eliciting	O
angry	O
or	O
enthusiastic	O
emotions	O
impact	O
participants	O
'	O
opinions	O
on	O
immigration	O
(	O
Lecheler	O
et	O
al	O
,	O
2015	O
)	O
.	O
While	O
past	O
work	O
has	O
analyzed	O
linguistic	O
framing	O
in	O
Twitter	O
immigration	O
discourse	O
(	O
e.g.	O
,	O
de	O
Saint	O
Laurent	O
et	O
al	O
,	O
2020	O
)	O
,	O
little	O
is	O
known	O
about	O
how	O
such	O
framing	O
affects	O
users	O
'	O
interactive	O
behaviors	O
such	O
as	O
resharing	O
content	O
,	O
which	O
is	O
a	O
key	O
objective	O
of	O
frame	O
setting	O
.	O

These	O
instances	O
highlight	O
the	O
challenges	O
of	O
annotation	O
;	O
there	O
are	O
convincing	O
arguments	O
that	O
model	O
's	O
predicted	O
frames	O
can	O
be	O
appropriate	O
labels	O
.	O
Interestingly	O
,	O
the	O
criteria	O
to	O
which	O
immigrants	O
would	O
be	O
held	O
would	O
not	O
be	O
met	O
by	O
a	O
large	O
number	O
of	O
the	O
'	O
British	O
'	O
people	O
either	O
.	O

Model	O
predicts	O
frames	O
that	O
may	O
capture	O
an	O
author	O
's	O
intention	O
but	O
without	O
sufficient	O
evidence	O
from	O
the	O
text	O

Model	O
erroneously	O
predicted	O
Threat	O
:	O
Public	O
Order	O
Missing	O
necessary	O
contextual	O
knowledge	O
Some	O
frames	O
are	O
directly	O
cued	O
by	O
lexical	O
items	O
(	O
e.g.	O
politicians	O
'	O
names	O
cue	O
Political	O
frame	O
)	O
,	O
but	O
model	O
lacks	O
real	O
-	O
world	O
knowledge	O
required	O
to	O
identify	O
these	O
frames	O
@EricTrump	O
Eric	O
I	O
have	O
been	O
alive	O
longer	O
than	O
your	O
immigrant	O
mother	O
in	O
law	O
and	O
you	O
.	O
I	O
paid	O
more	O
in	O
taxes	O
than	O
you	O
did	O
and	O
your	O
immigrant	O
mother	O
in	O
law	O
combined	O
...	O

In	O
writing	O
about	O
an	O
issue	O
,	O
individuals	O
are	O
known	O
to	O
select	O
particular	O
frames	O
-	O
a	O
process	O
known	O
as	O
frame	O
-	O
building	O
-	O
based	O
on	O
numerous	O
factors	O
,	O
such	O
as	O
exposure	O
to	O
politicians	O
'	O
rhetoric	O
or	O
their	O
own	O
identity	O
(	O
Scheufele	O
,	O
1999	O
)	O
.	O
Here	O
,	O
we	O
focus	O
on	O
two	O
specific	O
identity	O
attributes	O
affecting	O
frame	O
building	O
:	O
(	O
i	O
)	O
political	O
ideology	O
and	O
(	O
ii	O
)	O
country	O
/	O
region	O
.	O
The	O
political	O
,	O
social	O
,	O
and	O
historical	O
contexts	O
of	O
an	O
one	O
's	O
nation	O
-	O
state	O
can	O
impact	O
how	O
they	O
frame	O
immigration	O
(	O
Helbling	O
,	O
2014	O
)	O
.	O
Immigration	O
has	O
a	O
long	O
history	O
in	O
the	O
USA	O
relative	O
to	O
Europe	O
,	O
and	O
former	O
European	O
colonial	O
powers	O
(	O
e.g.	O
the	O
UK	O
)	O
have	O
longer	O
immigration	O
histories	O
than	O
other	O
countries	O
(	O
e.g.	O
Norway	O
)	O
(	O
Thorbjørnsrud	O
,	O
2015	O
;	O
Eberl	O
et	O
al	O
,	O
2018	O
)	O
.	O
Cross	O
-	O
country	O
variation	O
in	O
news	O
framing	O
also	O
arise	O
from	O
differences	O
in	O
immigration	O
policies	O
(	O
Helbling	O
,	O
2014	O
;	O
Lawlor	O
,	O
2015	O
)	O
,	O
media	O
systems	O
(	O
Thorbjørnsrud	O
,	O
2015	O
)	O
,	O
journalis	O
-	O
tic	O
norms	O
(	O
Papacharissi	O
and	O
De	O
Fatima	O
Oliveira	O
,	O
2008	O
)	O
,	O
geographic	O
proximity	O
to	O
immigrant	O
populations	O
or	O
points	O
of	O
entry	O
(	O
Grimm	O
and	O
Andsager	O
,	O
2011	O
;	O
Fryberg	O
et	O
al	O
,	O
2012	O
)	O
,	O
and	O
immigrants	O
'	O
race	O
/	O
ethnicity	O
(	O
Grimm	O
and	O
Andsager	O
,	O
2011	O
)	O
.	O
At	O
the	O
same	O
time	O
,	O
increased	O
globalization	O
may	O
result	O
in	O
a	O
uniform	O
transnational	O
immigration	O
discourse	O
(	O
Helbling	O
,	O
2014	O
)	O
.	O
Framing	O
variations	O
across	O
countries	O
has	O
implications	O
for	O
government	O
policies	O
and	O
initiatives	O
,	O
particularly	O
in	O
determining	O
what	O
solutions	O
could	O
be	O
applied	O
internationally	O
or	O
tailored	O
to	O
each	O
country	O
(	O
Caviedes	O
,	O
2015	O
)	O
.	O
Prior	O
studies	O
on	O
the	O
role	O
of	O
ideology	O
in	O
framebuilding	O
have	O
focused	O
on	O
the	O
newspapers	O
or	O
political	O
movements	O
,	O
showing	O
patterns	O
in	O
frames	O
like	O
morality	O
and	O
security	O
by	O
political	O
affiliation	O
in	O
European	O
immigration	O
discourse	O
(	O
Helbling	O
,	O
2014	O
;	O
Hogan	O
and	O
Haltinner	O
,	O
2015	O
)	O
or	O
in	O
use	O
of	O
economic	O
frames	O
by	O
American	O
newspapers	O
(	O
Fryberg	O
et	O
al	O
,	O
2012	O
;	O
Abrajano	O
et	O
al	O
,	O
2017	O
)	O
.	O
However	O
,	O
it	O
remains	O
unclear	O
whether	O
these	O
patterns	O
observed	O
for	O
elite	O
groups	O
can	O
generalize	O
to	O
the	O
effect	O
of	O
individual	O
people	O
's	O
political	O
dispositions	O
.	O

Chong	O
and	O
Druckman	O
(	O
2007	O
,	O
p.	O
116	O
)	O
assert	O
that	O
a	O
"	O
challenge	O
for	O
future	O
work	O
concerns	O
the	O
identification	O
of	O
factors	O
that	O
make	O
a	O
frame	O
strong	O
.	O
"	O
Studies	O
of	O
frame	O
-	O
setting	O
-	O
i.e.	O
,	O
how	O
a	O
message	O
's	O
framing	O
affects	O
its	O
audience	O
's	O
emotions	O
,	O
beliefs	O
,	O
and	O
opinions	O
-	O
have	O
largely	O
been	O
restricted	O
to	O
smallscale	O
experimental	O
studies	O
because	O
responses	O
to	O
news	O
media	O
framing	O
can	O
not	O
be	O
directly	O
observed	O
(	O
Eberl	O
et	O
al	O
,	O
2018	O
)	O
.	O
However	O
,	O
Twitter	O
provides	O
insight	O
into	O
the	O
frame	O
-	O
setting	O
process	O
via	O
interactive	O
signals	O
:	O
favorites	O
and	O
retweets	O
.	O
While	O
related	O
,	O
these	O
two	O
actions	O
can	O
have	O
distinct	O
underlying	O
motivations	O
:	O
favoriting	O
often	O
indicates	O
positive	O
alignment	O
between	O
the	O
author	O
and	O
the	O
reader	O
;	O
in	O
contrast	O
,	O
retweeting	O
may	O
also	O
be	O
driven	O
by	O
other	O
motivations	O
,	O
such	O
as	O
the	O
desire	O
to	O
inform	O
or	O
entertain	O
others	O
(	O
boyd	O
et	O
al	O
,	O
2010	O
tweets	O
with	O
detected	O
author	O
ideology	O
.	O
The	O
presence	O
of	O
a	O
frame	O
is	O
treated	O
as	O
a	O
binary	O
fixed	O
effect	O
.	O
We	O
control	O
for	O
all	O
temporal	O
,	O
user	O
-	O
level	O
and	O
tweet	O
-	O
level	O
features	O
as	O
in	O
the	O
prior	O
section	O
,	O
as	O
well	O
as	O
ideology	O
.	O
Results	O
The	O
framing	O
of	O
immigration	O
has	O
a	O
significant	O
impact	O
on	O
how	O
users	O
engage	O
with	O
the	O
content	O
via	O
retweets	O
and	O
favorites	O
(	O
Figure	O
4	O
)	O
.	O
Many	O
issuespecific	O
frames	O
have	O
a	O
stronger	O
effect	O
on	O
audience	O
responses	O
than	O
either	O
of	O
the	O
other	O
typologies	O
.	O
As	O
recent	O
NLP	O
approaches	O
have	O
adopted	O
issue	O
-	O
generic	O
frames	O
for	O
analysis	O
(	O
e.g.	O
,	O
Kwak	O
et	O
al	O
,	O
2020	O
)	O
,	O
the	O
strength	O
of	O
issue	O
-	O
specific	O
frames	O
highlights	O
the	O
importance	O
of	O
expanding	O
computational	O
analyses	O
beyond	O
issue	O
-	O
generic	O
frames	O
,	O
as	O
other	O
frames	O
may	O
have	O
larger	O
consequences	O
for	O
public	O
opinion	O
.	O
Most	O
frames	O
impact	O
favorites	O
and	O
retweets	O
differently	O
,	O
suggesting	O
that	O
the	O
strength	O
of	O
a	O
frame	O
's	O
effects	O
is	O
tied	O
to	O
the	O
specific	O
engagement	O
behavior	O
.	O
Cultural	O
frames	O
(	O
e.g.	O
hero	O
:	O
integration	O
)	O
and	O
frames	O
oriented	O
around	O
human	O
interest	O
(	O
e.g.	O
morality	O
,	O
victim	O
:	O
discrimination	O
)	O
are	O
particularly	O
associated	O
with	O
more	O
endorsements	O
(	O
favorites	O
)	O
,	O
perhaps	O
due	O
to	O
their	O
increased	O
emotional	O
appeal	O
to	O
readers	O
(	O
Semetko	O
and	O
Valkenburg	O
,	O
2000	O
)	O
.	O
On	O
the	O
other	O
hand	O
,	O
political	O
factors	O
&	O
implications	O
is	O
most	O
highly	O
associated	O
with	O
increased	O
retweets	O
.	O
As	O
the	O
political	O
frame	O
emphasizes	O
competition	O
and	O
strategy	O
(	O
Boydstun	O
et	O
al	O
,	O
2013	O
)	O
,	O
this	O
result	O
mirrors	O
similar	O
links	O
between	O
the	O
"	O
horse	O
-	O
race	O
"	O
frame	O
in	O
news	O
reports	O
and	O
engagement	O
(	O
Iyengar	O
et	O
al	O
,	O
2004	O
)	O
;	O
users	O
may	O
prefer	O
amplifying	O
political	O
messages	O
via	O
retweeting	O
to	O
help	O
their	O
side	O
win	O
.	O
Similarly	O
,	O
frames	O
about	O
security	O
and	O
safety	O
(	O
e.g.	O
crime	O
&	O
punishment	O
,	O
victim	O
:	O
humanitarian	O
)	O
are	O
highly	O
associated	O
with	O
more	O
retweets	O
,	O
but	O
not	O
necessarily	O
favorites	O
.	O
While	O
security	O
and	O
safety	O
frames	O
may	O
not	O
lead	O
audience	O
members	O
to	O
endorse	O
such	O
messages	O
,	O
perhaps	O
they	O
are	O
more	O
likely	O
to	O
amplify	O
these	O
messages	O
due	O
to	O
perceived	O
urgency	O
or	O
the	O
desire	O
to	O
persuade	O
others	O
of	O
such	O
concerns	O
.	O
Finally	O
,	O
Figure	O
4	O
shows	O
how	O
a	O
message	O
's	O
narrative	O
framing	O
impacts	O
audience	O
response	O
,	O
even	O
after	O
controlling	O
for	O
all	O
other	O
frames	O
.	O
Both	O
episodic	O
and	O
thematic	O
frames	O
are	O
significantly	O
associated	O
with	O
increased	O
engagement	O
(	O
retweets	O
)	O
,	O
but	O
less	O
strongly	O
than	O
issue	O
frames	O
.	O
Having	O
a	O
clear	O
narrative	O
is	O
important	O
for	O
messages	O
to	O
spread	O
,	O
but	O
the	O
underlying	O
mechanisms	O
driving	O
engagement	O
behaviors	O
may	O
differ	O
for	O
episodic	O
and	O
thematic	O
frames	O
;	O
prior	O
work	O
on	O
mainstream	O
media	O
has	O
found	O
that	O
news	O
stories	O
using	O
episodic	O
frames	O
tend	O
to	O
be	O
more	O
emotionally	O
engaging	O
,	O
while	O
thematic	O
frames	O
can	O
be	O
more	O
persuasive	O
(	O
Iyengar	O
,	O
1991	O
;	O
Gross	O
,	O
2008	O
)	O
.	O

Users	O
'	O
exposure	O
to	O
political	O
information	O
on	O
social	O
media	O
can	O
have	O
immense	O
consequences	O
.	O
By	O
leveraging	O
multiple	O
theory	O
-	O
informed	O
typologies	O
,	O
our	O
computational	O
analysis	O
of	O
framing	O
enables	O
us	O
to	O
better	O
understand	O
public	O
discourses	O
surrounding	O
immigration	O
.	O
We	O
furthermore	O
show	O
that	O
framing	O
on	O
Twitter	O
affects	O
how	O
audience	O
interactions	O
with	O
messages	O
via	O
favoriting	O
and	O
retweeting	O
behaviors	O
.	O
This	O
work	O
has	O
implications	O
for	O
social	O
media	O
platforms	O
,	O
who	O
may	O
wish	O
to	O
improve	O
users	O
'	O
experiences	O
by	O
enabling	O
them	O
to	O
discover	O
content	O
with	O
a	O
diversity	O
of	O
frames	O
.	O
By	O
exposing	O
users	O
to	O
a	O
wide	O
range	O
of	O
perspectives	O
,	O
this	O
work	O
can	O
help	O
lay	O
foundations	O
for	O
more	O
cooperative	O
and	O
effective	O
online	O
discussions	O
.	O
All	O
code	O
,	O
data	O
,	O
annotation	O
guidelines	O
,	O
and	O
pretrained	O
models	O
are	O
available	O
at	O
https	O
:	O
//github.com	O
/	O
juliamendelsohn	O
/	O
framing	O
.	O

Figure	O
5	O
shows	O
the	O
distribution	O
of	O
frames	O
as	O
a	O
fraction	O
of	O
total	O
tweets	O
in	O
the	O
annotated	O
data	O
.	O

Tables	O
9	O
-	O
35	O
show	O
independent	O
variable	O
coefficients	O
for	O
logit	O
regressions	O
predicting	O
frames	O
from	O
region	O
.	O

Tables	O
36	O
-	O
62	O
show	O
independent	O
variable	O
coefficients	O
for	O
logit	O
regressions	O
predicting	O
frames	O
from	O
ideology	O
.	O

We	O
thank	O
Anoop	O
Kotha	O
,	O
Shiqi	O
Sheng	O
,	O
Guoxin	O
Yin	O
,	O
and	O
Hongting	O
Zhu	O
for	O
their	O
contributions	O
to	O
the	O
data	O
annotation	O
effort	O
.	O
We	O
also	O
thank	O
Libby	O
Hemphill	O
and	O
Stuart	O
Soroka	O
for	O
their	O
valuable	O
comments	O
and	O
feedback	O
.	O
This	O
work	O
was	O
supported	O
in	O
part	O
through	O
funding	O
from	O
the	O
Volkswagen	O
Foundation	O
.	O

Aggressive	O
language	O
in	O
an	O
online	O
hacking	O
forum	O

We	O
have	O
an	O
inter	O
-	O
corpus	O
experimental	O
design	O
,	O
in	O
which	O
a	O
document	O
classifier	O
is	O
trained	O
on	O
one	O
dataset	O
and	O
tested	O
on	O
other	O
datasets	O
.	O
Our	O
training	O
data	O
come	O
from	O
the	O
Wikipedia	O
Comments	O
Corpus	O
(	O
WikiComments	O
)	O
(	O
Wulczyn	O
et	O
al	O
,	O
2017	O
)	O
,	O
which	O
contains	O
115	O
,	O
864	O
discussion	O
posts	O
extracted	O
from	O
an	O
English	O
Wikipedia	O
dump	O
,	O
judged	O
as	O
personal	O
attacks	O
or	O
harassment	O
by	O
crowdworkers	O
.	O
Ten	O
judgements	O
were	O
collected	O
for	O
each	O
post	O
;	O
hence	O
we	O
have	O
an	O
attack	O
score	O
from	O
zero	O
to	O
ten	O
for	O
every	O
post	O
2	O
,	O
and	O
we	O
assume	O
that	O
the	O
higher	O
the	O
attack	O
score	O
the	O
greater	O
the	O
linguistic	O
aggression	O
shown	O
in	O
writing	O
.	O
This	O
assumption	O
may	O
be	O
challenged	O
,	O
as	O
we	O
accept	O
that	O
there	O
are	O
many	O
reasons	O
why	O
a	O
text	O
may	O
not	O
be	O
unanimously	O
judged	O
to	O
be	O
an	O
attack	O
or	O
ha	O
-	O
rassment	O
-	O
properties	O
of	O
the	O
text	O
such	O
as	O
poor	O
grammar	O
which	O
obfuscates	O
meaning	O
,	O
use	O
of	O
slang	O
insults	O
which	O
are	O
not	O
universally	O
known	O
,	O
or	O
sarcastic	O
phrasing	O
which	O
is	O
not	O
interpreted	O
as	O
an	O
attack	O
by	O
all	O
annotators	O
.	O
On	O
the	O
other	O
hand	O
,	O
properties	O
of	O
the	O
annotator	O
,	O
such	O
as	O
fatigue	O
or	O
inattention	O
,	O
inexperience	O
with	O
English	O
or	O
the	O
terminology	O
used	O
,	O
or	O
idiosyncratic	O
linguistic	O
thresholds	O
for	O
attacks	O
and	O
harassment	O
,	O
could	O
all	O
play	O
a	O
part	O
in	O
judgement	O
variation	O
as	O
well	O
.	O
However	O
,	O
over	O
such	O
a	O
large	O
dataset	O
we	O
assume	O
that	O
in	O
terms	O
of	O
aggressive	O
language	O
the	O
texts	O
will	O
be	O
broadly	O
well	O
ordered	O
by	O
their	O
attack	O
scores	O
.	O
Table	O
1	O
shows	O
examples	O
randomly	O
drawn	O
from	O
each	O
attack	O
score	O
,	O
zero	O
to	O
ten	O
,	O
along	O
with	O
the	O
number	O
of	O
posts	O
in	O
each	O
class	O
,	O
and	O
the	O
cumulative	O
size	O
of	O
the	O
corpus	O
in	O
reverse	O
order	O
from	O
attack	O
score	O
ten	O
to	O
zero	O
.	O
The	O
curators	O
of	O
WikiComments	O
used	O
these	O
annotated	O
discussion	O
posts	O
to	O
train	O
a	O
classifier	O
and	O
further	O
label	O
unseen	O
posts	O
in	O
a	O
larger	O
collection	O
of	O
63	O
million	O
discussion	O
posts	O
,	O
with	O
a	O
view	O
to	O
largescale	O
analyses	O
of	O
attacks	O
by	O
unregistered	O
users	O
,	O
moderator	O
actions	O
in	O
response	O
to	O
attacks	O
,	O
and	O
more	O
(	O
Wulczyn	O
et	O
al	O
,	O
2017	O
)	O
.	O
They	O
experimented	O
with	O
different	O
thresholds	O
t	O
where	O
attack	O
scores	O
at	O
or	O
above	O
t	O
would	O
be	O
labelled	O
as	O
attacks	O
,	O
and	O
those	O
below	O
t	O
would	O
not	O
be	O
attacks	O
.	O
They	O
found	O
that	O
the	O
optimal	O
value	O
for	O
t	O
balancing	O
precision	O
and	O
recall	O
was	O
4.25	O
.	O
Our	O
intention	O
is	O
to	O
take	O
the	O
texts	O
and	O
attack	O
scores	O
from	O
WikiComments	O
to	O
train	O
a	O
binary	O
aggression	O
classifier	O
for	O
use	O
with	O
other	O
corpora	O
.	O
The	O
question	O
with	O
such	O
a	O
classifier	O
is	O
how	O
to	O
partition	O
the	O
training	O
data	O
for	O
true	O
/	O
false	O
aggression	O
labels	O
:	O
the	O
cut	O
-	O
off	O
could	O
be	O
any	O
attack	O
score	O
value	O
from	O
one	O
to	O
ten	O
.	O
In	O
the	O
following	O
sections	O
we	O
report	O
on	O
classification	O
experiments	O
with	O
each	O
attack	O
score	O
cut	O
-	O
off	O
value	O
and	O
a	O
test	O
corpus	O
sourced	O
from	O
Internet	O
forums	O
.	O
Our	O
test	O
data	O
come	O
from	O
the	O
CrimeBB	O
Corpus	O
3	O
,	O
a	O
dataset	O
harvested	O
from	O
several	O
hackingrelated	O
websites	O
including	O
HackForums	O
,	O
Antichat	O
and	O
Greysec	O
(	O
Pastrana	O
et	O
al	O
,	O
2018	O
(	O
Fleiss	O
,	O
1971	O
;	O
Landis	O
and	O
Koch	O
,	O
1977	O
)	O
-	O
i.e.	O
κ	O
=	O
0.4	O
to	O
0.6	O
.	O
We	O
did	O
not	O
attempt	O
to	O
settle	O
on	O
single	O
annotations	O
for	O
each	O
post	O
,	O
but	O
instead	O
treated	O
all	O
judgements	O
equally	O
,	O
allowing	O
multiple	O
labels	O
both	O
by	O
individual	O
annotators	O
and	O
across	O
different	O
annotators	O
.	O
A	O
single	O
annotator	O
further	O
labelled	O
the	O
remaining	O
1923	O
posts	O
.	O
Posts	O
with	O
aggressive	O
intent	O
are	O
uncommon	O
on	O
HackForums	O
,	O
with	O
only	O
100	O
aggressive	O
posts	O
judged	O
to	O
be	O
aggressive	O
by	O
at	O
least	O
one	O
annotator	O
in	O
the	O
total	O
corpus	O
of	O
4123	O
posts	O
(	O
2.4	O
%	O
)	O
.	O
Note	O
that	O
profane	O
language	O
is	O
more	O
commonly	O
found	O
-	O
which	O
is	O
unsurprising	O
given	O
the	O
casual	O
linguistic	O
register	O
-	O
with	O
201	O
posts	O
in	O
this	O
dataset	O
featuring	O
at	O
least	O
one	O
of	O
'	O
fuck	O
,	O
shit	O
,	O
cunt	O
,	O
jerk	O
,	O
crap	O
,	O
dick	O
'	O
(	O
or	O
derived	O
forms	O
)	O
.	O
However	O
,	O
the	O
profanity	O
is	O
often	O
used	O
for	O
humorous	O
purposes	O
,	O
or	O
to	O
defuse	O
potentially	O
confrontational	O
conversations	O
,	O
or	O
simply	O
in	O
a	O
casual	O
way	O
for	O
no	O
purpose	O
at	O
all	O
;	O
it	O
is	O
not	O
always	O
used	O
aggressively	O
(	O
hence	O
the	O
need	O
for	O
manual	O
annotation	O
)	O
.	O
This	O
observation	O
underlines	O
the	O
distinction	O
between	O
offensive	O
and	O
aggressive	O
language	O
.	O
Table	O
2	O
shows	O
the	O
size	O
of	O
the	O
CrimeBB	O
Corpus	O
,	O
the	O
HackForums	O
subset	O
,	O
and	O
the	O
annotated	O
posts	O
,	O
along	O
with	O
examples	O
of	O
aggressive	O
and	O
nonaggressive	O
posts	O
from	O
HackForums	O
.	O

We	O
trained	O
a	O
binary	O
aggression	O
classifier	O
on	O
the	O
WikiComments	O
Corpus	O
setting	O
the	O
true	O
/	O
false	O
threshold	O
t	O
at	O
each	O
attack	O
score	O
from	O
1	O
to	O
10	O
and	O
testing	O
the	O
classifier	O
on	O
our	O
annotated	O
set	O
of	O
4123	O
HackForums	O
posts	O
from	O
the	O
CrimeBB	O
Corpus	O
.	O

Refers	O
to	O
disability	O
7	O

Alludes	O
to	O
violence	O
2	O

Includes	O
racism	O
1	O
ability	O
'	O
label	O
always	O
involves	O
the	O
words	O
'	O
retard	O
'	O
and	O
'	O
retarded	O
'	O
in	O
this	O
100	O
post	O
sample	O
.	O
Finally	O
,	O
direct	O
threats	O
of	O
violence	O
are	O
very	O
rare	O
,	O
with	O
only	O
two	O
examples	O
found	O
in	O
this	O
subcorpus	O
.	O

This	O
work	O
was	O
supported	O
by	O
The	O
Alan	O
Turing	O
Institute	O
's	O
Defence	O
&	O
Security	O
Programme	O
,	O
and	O
the	O
U.K.	O
Engineering	O
&	O
Physical	O
Sciences	O
Research	O
Council	O
.	O
We	O
thank	O
Emma	O
Lenton	O
,	O
Dr	O
Alastair	O
Beresford	O
,	O
and	O
the	O
anonymous	O
reviewers	O
for	O
their	O
support	O
and	O
advice	O
.	O

Our	O
work	O
builds	O
upon	O
the	O
notion	O
of	O
a	O
noisy	O
knowledge	O
graph	O
(	O
NKG	O
)	O
,	O
which	O
consists	O
of	O
a	O
directed	O
graph	O
G	O
=	O
(	O
V	O
,	O
E	O
)	O
where	O
V	O
is	O
a	O
set	O
of	O
concepts	O
and	O
E	O
the	O
set	O
of	O
labelled	O
binary	O
semantic	O
relations	O
-	O
e.g.	O
,	O
those	O
found	O
between	O
synsets	O
like	O
,	O
for	O
instance	O
,	O
hypernymy	O
or	O
meronymy	O
within	O
a	O
semantic	O
network	O
like	O
WordNet	O
.	O
In	O
a	O
NKG	O
we	O
assume	O
both	O
V	O
and	O
E	O
to	O
have	O
been	O
acquired	O
automatically	O
,	O
e.g.	O
,	O
in	O
order	O
to	O
induce	O
a	O
domain	O
-	O
aware	O
or	O
a	O
general	O
purpose	O
knowledge	O
base	O
.	O
Additionally	O
,	O
we	O
consider	O
for	O
our	O
purposes	O
the	O
hypernymy	O
graph	O
T	O
=	O
(	O
T	O
V	O
,	O
T	O
E	O
)	O
of	O
G	O
,	O
the	O
subgraph	O
made	O
up	O
of	O
the	O
hypernymy	O
(	O
i.e.	O
,	O
isa	O
-	O
labeled	O
)	O
edges	O
of	O
E.	O
Since	O
T	O
is	O
a	O
subgraph	O
of	O
G	O
,	O
we	O
can	O
expect	O
that	O
the	O
former	O
inherits	O
a	O
certain	O
amount	O
of	O
noise	O
from	O
the	O
latter	O
.	O
Noise	O
within	O
hypernymy	O
graphs	O
can	O
be	O
further	O
classified	O
into	O
:	O
i	O
)	O
noisy	O
nodes	O
,	O
the	O
concepts	O
that	O
do	O
not	O
belong	O
to	O
a	O
specific	O
target	O
vocabulary	O
,	O
e.g.	O
,	O
domain	O
concepts	O
for	O
domain	O
-	O
specific	O
KBs	O
,	O
such	O
as	O
Jaguar	O
Cars	O
within	O
a	O
zoological	O
taxonomy	O
;	O
ii	O
)	O
noisy	O
edges	O
,	O
the	O
wrongly	O
-	O
acquired	O
relations	O
between	O
unrelated	O
concepts	O
or	O
out	O
-	O
of	O
-	O
domain	O
relations	O
,	O
e.g.	O
,	O
Jaguar	O
Cars	O
isa	O
Feline	O
;	O
iii	O
)	O
cycles	O
of	O
hypernymy	O
relations	O
,	O
such	O
as	O
those	O
derived	O
from	O
counts	O
over	O
very	O
large	O
corpora	O
(	O
Seitner	O
et	O
al	O
,	O
2016	O
)	O
,	O
e.g.	O
,	O
jaguar	O
(	O
Panthera	O
onca	O
)	O
feline	O
animal	O
jaguar	O
(	O
Panthera	O
onca	O
)	O
.	O
We	O
accordingly	O
define	O
the	O
task	O
of	O
extracting	O
a	O
clean	O
taxonomy	O
from	O
a	O
NKG	O
as	O
that	O
of	O
pruning	O
the	O
cycles	O
,	O
as	O
well	O
as	O
the	O
noisy	O
edges	O
and	O
nodes	O
,	O
from	O
the	O
hypernymy	O
subgraph	O
T	O
of	O
G.	O

In	O
order	O
to	O
enable	O
end	O
-	O
to	O
-	O
end	O
taxonomy	O
induction	O
from	O
scratch	O
,	O
we	O
combine	O
our	O
general	O
approach	O
with	O
existing	O
KBs	O
that	O
have	O
been	O
automatically	O
induced	O
from	O
text	O
and	O
linked	O
to	O
reference	O
lexical	O
knowledge	O
bases	O
on	O
the	O
basis	O
of	O
unsuper	O
-	O
vised	O
methods	O
.	O
To	O
this	O
end	O
,	O
we	O
use	O
the	O
linked	O
disambiguated	O
distributional	O
KBs	O
from	O
1	O
,	O
which	O
are	O
built	O
in	O
three	O
steps	O
:	O
1	O
)	O
Learning	O
a	O
JoBimText	O
model	O
.	O
Initially	O
,	O
a	O
sense	O
inventory	O
is	O
created	O
from	O
a	O
large	O
text	O
collection	O
using	O
the	O
pipeline	O
of	O
the	O
JoBimText	O
project	O
(	O
Biemann	O
and	O
Riedl	O
,	O
2013	O
)	O
.	O
2	O
The	O
resulting	O
structure	O
contains	O
disambiguated	O
protoconcepts	O
(	O
i.e.	O
,	O
senses	O
)	O
,	O
their	O
similar	O
and	O
related	O
terms	O
,	O
as	O
well	O
as	O
aggregated	O
contextual	O
clues	O
per	O
proto	O
-	O
concept	O
.	O
2	O
)	O
Disambiguation	O
of	O
related	O
terms	O
.	O
Similar	O
terms	O
and	O
hypernyms	O
associated	O
with	O
a	O
protoconcept	O
are	O
fully	O
disambiguated	O
based	O
on	O
the	O
partial	O
disambiguation	O
from	O
step	O
(	O
1	O
)	O
.	O
The	O
result	O
is	O
a	O
proto	O
-	O
conceptualization	O
(	O
PCZ	O
)	O
,	O
where	O
all	O
terms	O
have	O
a	O
sense	O
identifier	O
.	O
3	O
)	O
Linking	O
to	O
a	O
lexical	O
resource	O
.	O
The	O
PCZ	O
is	O
automatically	O
aligned	O
with	O
an	O
existing	O
lexical	O
resource	O
(	O
LR	O
)	O
such	O
as	O
WordNet	O
or	O
BabelNet	O
.	O
For	O
example	O
,	O
bridge	O
:	O
NN:3	O
is	O
linked	O
to	O
the	O
Babel	O
synset	O
bn:00013077n	O
(	O
the	O
'	O
infrastructure	O
'	O
sense	O
)	O
.	O
That	O
is	O
,	O
a	O
mapping	O
between	O
the	O
two	O
sense	O
inventories	O
is	O
created	O
to	O
combine	O
them	O
into	O
a	O
new	O
extended	O
sense	O
inventory	O
,	O
a	O
hybrid	O
aligned	O
resource	O
.	O
Table	O
1	O
shows	O
the	O
proto	O
-	O
conceptualization	O
entries	O
for	O
the	O
polysemous	O
terms	O
bridge	O
and	O
link	O
,	O
namely	O
their	O
figurative	O
(	O
"	O
bridge	O
:	O
NN:2	O
"	O
and	O
"	O
link	O
:	O
NN:1	O
"	O
)	O
and	O
concrete	O
'	O
infrastructure	O
'	O
(	O
"	O
bridge	O
:	O
NN:3	O
"	O
and	O
"	O
link	O
:	O
NN:0	O
"	O
)	O
senses	O
,	O
respectively	O
.	O
JoBimText	O
models	O
provide	O
sense	O
distinctions	O
that	O
are	O
only	O
partially	O
disambiguated	O
:	O
the	O
list	O
of	O
similar	O
and	O
hypernyms	O
terms	O
of	O
each	O
sense	O
,	O
in	O
fact	O
,	O
does	O
not	O
carry	O
sense	O
information	O
.	O
Consequently	O
,	O
a	O
semantic	O
closure	O
procedure	O
is	O
applied	O
in	O
order	O
to	O
obtain	O
a	O
PCZ	O
and	O
arrive	O
at	O
sense	O
representation	O
in	O
which	O
all	O
terms	O
get	O
assigned	O
a	O
unique	O
,	O
best	O
-	O
fitting	O
sense	O
identifier	O
(	O
see	O
for	O
details	O
)	O
.	O
PCZs	O
consist	O
of	O
a	O
rich	O
,	O
yet	O
noisy	O
,	O
disambiguated	O
semantic	O
network	O
automatically	O
induced	O
from	O
large	O
amounts	O
of	O
text	O
:	O
links	O
to	O
existing	O
lexical	O
resources	O
provide	O
us	O
a	O
source	O
of	O
external	O
supervision	O
that	O
can	O
be	O
leveraged	O
to	O
clean	O
them	O
and	O
turn	O
them	O
into	O
full	O
-	O
fledged	O
taxonomies	O
.	O
Steps	O
1	O
-	O
3	O
are	O
unsupervised	O
by	O
nature	O
.	O
Consequently	O
,	O
when	O
combined	O
with	O
our	O
algorithm	O
they	O
provide	O
a	O
complete	O
framework	O
for	O
fully	O
unsupervised	O
taxonomy	O
induction	O
from	O
scratch	O
.	O
Note	O
,	O
however	O
,	O
that	O
our	O
approach	O
offers	O
a	O
general	O
solution	O
to	O
the	O
problem	O
of	O
taxonomy	O
cleaning	O
.	O
In	O
an	O
additional	O
set	O
of	O
experiments	O
,	O
we	O
apply	O
it	O
to	O
different	O
automatically	O
generated	O
taxonomies	O
from	O
a	O
SemEval	O
task	O
in	O
a	O
more	O
controlled	O
setting	O
where	O
we	O
rely	O
on	O
a	O
few	O
manually	O
created	O
KB	O
links	O
only	O
.	O

We	O
next	O
evaluate	O
the	O
overall	O
impact	O
of	O
our	O
approach	O
within	O
an	O
existing	O
benchmark	O
for	O
the	O
taxonomy	O
induction	O
task	O
.	O
Intuitively	O
,	O
most	O
of	O
the	O
benefits	O
from	O
our	O
method	O
derive	O
from	O
the	O
"	O
gold	O
standard	O
"	O
information	O
of	O
the	O
companion	O
KB	O
,	O
and	O
its	O
linking	O
to	O
the	O
NKG	O
,	O
which	O
act	O
as	O
a	O
source	O
of	O
supervision	O
.	O
Consequently	O
,	O
we	O
address	O
the	O
research	O
question	O
of	O
how	O
much	O
(	O
pseudo	O
-	O
)	O
supervision	O
our	O
method	O
needs	O
in	O
terms	O
of	O
KB	O
links	O
,	O
and	O
whether	O
it	O
can	O
be	O
used	O
to	O
improve	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
the	O
task	O
of	O
taxonomy	O
induction	O
.	O

In	O
Table	O
3	O
,	O
we	O
report	O
the	O
performance	O
on	O
the	O
Sem	O
-	O
Eval	O
task	O
for	O
the	O
two	O
selected	O
input	O
taxonomies	O
.	O
Results	O
on	O
the	O
structural	O
similarities	O
of	O
the	O
pruned	O
taxonomies	O
with	O
the	O
gold	O
standard	O
ones	O
,	O
computed	O
using	O
the	O
CF&M	O
measure	O
,	O
indicate	O
that	O
,	O
thanks	O
to	O
ContrastMedium	O
and	O
with	O
a	O
minimal	O
human	O
effort	O
-	O
the	O
creation	O
of	O
just	O
a	O
few	O
KB	O
links	O
(	O
up	O
to	O
10	O
)	O
,	O
which	O
are	O
needed	O
only	O
when	O
automatic	O
linking	O
is	O
not	O
available	O
-	O
it	O
is	O
possible	O
to	O
boost	O
the	O
quality	O
of	O
taxonomies	O
using	O
state	O
-	O
of	O
-	O
art	O
methods	O
by	O
a	O
large	O
margin	O
.	O
For	O
instance	O
,	O
in	O
the	O
case	O
of	O
the	O
Equipments	O
taxonomy	O
,	O
we	O
improve	O
up	O
to	O
7	O
points	O
.	O
The	O
baseline	O
,	O
which	O
only	O
breaks	O
cycles	O
,	O
is	O
not	O
able	O
to	O
reassess	O
the	O
graph	O
structure	O
and	O
only	O
provides	O
very	O
small	O
improvements	O
to	O
the	O
submitted	O
NKGs	O
.	O
Overall	O
,	O
the	O
results	O
show	O
that	O
ContrastMedium	O
leads	O
to	O
competitive	O
performance	O
on	O
a	O
hard	O
,	O
realistic	O
benchmark	O
such	O
as	O
TExEval	O
,	O
achieving	O
the	O
best	O
overall	O
results	O
for	O
both	O
taxonomies	O
.	O
That	O
is	O
,	O
our	O
algorithm	O
is	O
able	O
to	O
improve	O
the	O
state	O
-	O
of	O
-	O
theart	O
on	O
taxonomy	O
induction	O
by	O
additionally	O
boosting	O
the	O
quality	O
of	O
existing	O
top	O
-	O
performing	O
systems	O
for	O
this	O
task	O
:	O
this	O
is	O
achieved	O
on	O
the	O
basis	O
of	O
a	O
minimally	O
supervised	O
approach	O
that	O
only	O
requires	O
a	O
few	O
links	O
to	O
a	O
reference	O
KB	O
,	O
which	O
is	O
used	O
to	O
provide	O
ground	O
-	O
truth	O
taxonomic	O
relations	O
and	O
guide	O
the	O
cleaning	O
process	O
.	O

We	O
acknowledge	O
the	O
support	O
of	O
the	O
Deutsche	O
Forschungsgemeinschaft	O
(	O
DFG	O
)	O
under	O
the	O
JOIN	O
-	O
T	O
project	O
.	O

Computational	O
Argumentation	O
Synthesis	O
as	O
a	O
Language	O
Modeling	O
Task	O

Synthesis	O
approaches	O
in	O
computational	O
argumentation	O
so	O
far	O
are	O
restricted	O
to	O
generating	O
claim	O
-	O
like	O
argument	O
units	O
or	O
short	O
summaries	O
of	O
debates	O
.	O
Ultimately	O
,	O
however	O
,	O
we	O
expect	O
computers	O
to	O
generate	O
whole	O
new	O
arguments	O
for	O
a	O
given	O
stance	O
towards	O
some	O
topic	O
,	O
backing	O
up	O
claims	O
following	O
argumentative	O
and	O
rhetorical	O
considerations	O
.	O
In	O
this	O
paper	O
,	O
we	O
approach	O
such	O
an	O
argumentation	O
synthesis	O
as	O
a	O
language	O
modeling	O
task	O
.	O
In	O
our	O
language	O
model	O
,	O
argumentative	O
discourse	O
units	O
are	O
the	O
"	O
words	O
"	O
,	O
and	O
arguments	O
represent	O
the	O
"	O
sentences	O
"	O
.	O
Given	O
a	O
pool	O
of	O
units	O
for	O
any	O
unseen	O
topic	O
-	O
stance	O
pair	O
,	O
the	O
model	O
selects	O
a	O
set	O
of	O
unit	O
types	O
according	O
to	O
a	O
basic	O
rhetorical	O
strategy	O
(	O
logos	O
vs.	O
pathos	O
)	O
,	O
arranges	O
the	O
structure	O
of	O
the	O
types	O
based	O
on	O
the	O
units	O
'	O
argumentative	O
roles	O
,	O
and	O
finally	O
"	O
phrases	O
"	O
an	O
argument	O
by	O
instantiating	O
the	O
structure	O
with	O
semantically	O
coherent	O
units	O
from	O
the	O
pool	O
.	O
Our	O
evaluation	O
suggests	O
that	O
the	O
model	O
can	O
,	O
to	O
some	O
extent	O
,	O
mimic	O
the	O
human	O
synthesis	O
of	O
strategy	O
-	O
specific	O
arguments	O
.	O

Existing	O
research	O
on	O
computational	O
argumentation	O
largely	O
focuses	O
on	O
the	O
analysis	O
side	O
.	O
Various	O
analysis	O
tasks	O
are	O
widely	O
studied	O
including	O
identifying	O
the	O
claims	O
along	O
with	O
their	O
supporting	O
premises	O
(	O
Stab	O
and	O
Gurevych	O
,	O
2014	O
)	O
,	O
finding	O
the	O
relation	O
between	O
argumentative	O
units	O
(	O
Cocarascu	O
and	O
Toni	O
,	O
2017	O
)	O
,	O
and	O
assessing	O
the	O
persuasiveness	O
of	O
arguments	O
(	O
Habernal	O
and	O
Gurevych	O
,	O
2016	O
)	O
.	O
Diverse	O
downstream	O
applications	O
,	O
however	O
,	O
necessitate	O
the	O
development	O
of	O
argumentation	O
synthesis	O
technologies	O
.	O
For	O
example	O
,	O
synthesis	O
is	O
needed	O
to	O
produce	O
a	O
summary	O
of	O
arguments	O
for	O
a	O
given	O
topic	O
(	O
Wang	O
and	O
Ling	O
,	O
2016	O
)	O
or	O
to	O
build	O
a	O
debating	O
system	O
where	O
new	O
arguments	O
are	O
exchanged	O
between	O
the	O
users	O
and	O
the	O
system	O
(	O
Le	O
et	O
al	O
,	O
2018	O
)	O
.	O
As	O
a	O
result	O
,	O
a	O
number	O
of	O
recent	O
studies	O
addresses	O
the	O
argumentation	O
synthesis	O
task	O
.	O
These	O
studies	O
have	O
proposed	O
different	O
approaches	O
to	O
generating	O
claims	O
or	O
reasons	O
for	O
a	O
given	O
topic	O
,	O
partly	O
with	O
a	O
particular	O
stance	O
towards	O
the	O
topic	O
(	O
Bilu	O
and	O
Slonim	O
,	O
2016	O
;	O
Hua	O
and	O
Wang	O
,	O
2018	O
)	O
.	O
However	O
,	O
the	O
next	O
important	O
synthesis	O
step	O
is	O
still	O
missing	O
in	O
the	O
literature	O
,	O
namely	O
,	O
to	O
generate	O
complete	O
texts	O
including	O
both	O
argumentative	O
and	O
rhetorical	O
considerations	O
.	O
With	O
the	O
latter	O
,	O
we	O
refer	O
to	O
Aristotle	O
's	O
three	O
means	O
of	O
persuasion	O
:	O
logos	O
(	O
providing	O
logical	O
arguments	O
)	O
,	O
ethos	O
(	O
demonstrating	O
credibility	O
)	O
,	O
and	O
pathos	O
(	O
evoking	O
emotions	O
)	O
.	O
As	O
discussed	O
by	O
Wachsmuth	O
et	O
al	O
(	O
2018	O
)	O
,	O
following	O
a	O
rhetorical	O
strategy	O
is	O
key	O
to	O
achieving	O
persuasion	O
with	O
argumentative	O
texts	O
.	O
This	O
paper	O
proposes	O
a	O
new	O
computational	O
approach	O
that	O
synthesizes	O
argumentative	O
texts	O
following	O
a	O
rhetorical	O
strategy	O
.	O
We	O
do	O
not	O
tackle	O
this	O
task	O
immediately	O
"	O
in	O
the	O
wild	O
"	O
,	O
i.e.	O
,	O
generating	O
an	O
entirely	O
new	O
argumentative	O
text	O
for	O
a	O
freely	O
-	O
chosen	O
topic	O
and	O
a	O
possibly	O
complex	O
strategy	O
.	O
Rather	O
,	O
we	O
consider	O
a	O
"	O
controlled	O
"	O
synthesis	O
setting	O
,	O
with	O
the	O
goal	O
of	O
successively	O
creating	O
models	O
that	O
are	O
able	O
to	O
deal	O
with	O
more	O
complex	O
settings	O
later	O
on	O
.	O
In	O
particular	O
,	O
given	O
a	O
pool	O
of	O
argumentative	O
discourse	O
units	O
(	O
ADUs	O
)	O
,	O
our	O
approach	O
generates	O
arguments	O
for	O
any	O
unseen	O
pair	O
of	O
topic	O
and	O
stance	O
(	O
e.g.	O
,	O
"	O
con	O
abortion	O
"	O
)	O
as	O
well	O
as	O
a	O
basic	O
rhetorical	O
strategy	O
(	O
i.e.	O
,	O
logos	O
-	O
oriented	O
vs.	O
pathos	O
-	O
oriented	O
)	O
.	O
1	O
To	O
abstract	O
from	O
the	O
arguments	O
'	O
topics	O
during	O
training	O
,	O
we	O
first	O
identify	O
different	O
ADU	O
types	O
using	O
clustering	O
.	O
Our	O
approach	O
then	O
learns	O
to	O
select	O
unit	O
types	O
matching	O
the	O
given	O
strategy	O
and	O
to	O
arrange	O
them	O
according	O
to	O
their	O
argumentative	O
roles	O
.	O
Both	O
steps	O
are	O
realized	O
as	O
a	O
language	O
model	O
where	O
ADUs	O
represent	O
words	O
and	O
arguments	O
are	O
sentences	O
.	O
Finally	O
,	O
our	O
approach	O
"	O
phrases	O
"	O
an	O
argument	O
by	O
predicting	O
the	O
best	O
set	O
of	O
semantically	O
related	O
ADUs	O
for	O
the	O
arranged	O
structure	O
using	O
supervised	O
regression	O
.	O
Thereby	O
,	O
we	O
ensure	O
that	O
the	O
synthesized	O
texts	O
are	O
composed	O
of	O
meaningful	O
units	O
,	O
a	O
property	O
that	O
neural	O
generation	O
methods	O
barely	O
achieve	O
so	O
far	O
.	O
In	O
our	O
evaluation	O
,	O
we	O
utilize	O
the	O
dataset	O
of	O
Wachsmuth	O
et	O
al	O
(	O
2018	O
)	O
.	O
This	O
dataset	O
contains	O
260	O
argumentative	O
texts	O
on	O
10	O
topic	O
-	O
stance	O
pairs	O
,	O
where	O
each	O
text	O
composes	O
five	O
ADUs	O
in	O
a	O
logos	O
-	O
oriented	O
or	O
pathos	O
-	O
oriented	O
manner	O
.	O
In	O
our	O
experiments	O
,	O
we	O
train	O
our	O
approach	O
on	O
nine	O
topic	O
-	O
stance	O
pairs	O
and	O
then	O
generate	O
an	O
argument	O
for	O
the	O
tenth	O
.	O
The	O
results	O
demonstrate	O
that	O
our	O
approach	O
successfully	O
manages	O
to	O
combine	O
pairs	O
of	O
ADUs	O
,	O
but	O
its	O
performance	O
on	O
longer	O
sequences	O
of	O
ADUs	O
is	O
limited	O
.	O
Altogether	O
,	O
our	O
contribution	O
is	O
three	O
-	O
fold	O
:	O
1	O
.	O
A	O
new	O
view	O
of	O
argumentation	O
synthesis	O
that	O
represents	O
argumentative	O
and	O
rhetorical	O
considerations	O
with	O
language	O
modeling	O
.	O
2	O
.	O
A	O
novel	O
approach	O
that	O
selects	O
,	O
arranges	O
,	O
and	O
phrases	O
ADUs	O
to	O
synthesize	O
strategy	O
-	O
specific	O
arguments	O
for	O
any	O
topic	O
and	O
stance	O
.	O
3	O
.	O
First	O
experimental	O
evidence	O
that	O
arguments	O
with	O
basic	O
rhetorical	O
strategies	O
can	O
be	O
synthesized	O
computationally	O
.	O
2	O

To	O
develop	O
our	O
model	O
for	O
argumentation	O
synthesis	O
,	O
we	O
exploit	O
the	O
dataset	O
recently	O
developed	O
by	O
Wachsmuth	O
et	O
al	O
(	O
2018	O
)	O
.	O
The	O
dataset	O
comprises	O
260	O
manually	O
generated	O
argumentative	O
texts	O
.	O
The	O
generation	O
of	O
each	O
text	O
,	O
for	O
one	O
topic	O
-	O
stance	O
pair	O
,	O
has	O
been	O
conducted	O
in	O
a	O
systematic	O
fashion	O
following	O
the	O
three	O
canons	O
of	O
rhetoric	O
(	O
Aristotle	O
,	O
2007	O
)	O
:	O
1	O
.	O
Inventio	O
∼	O
Selecting	O
a	O
subset	O
of	O
argumentative	O
discourse	O
units	O
(	O
ADUs	O
)	O
from	O
a	O
pool	O
of	O
given	O
ADUs	O
for	O
a	O
topic	O
-	O
stance	O
pair	O
.	O
2	O
.	O
Dispositio	O
∼	O
Arranging	O
the	O
selected	O
ADUs	O
in	O
a	O
sequential	O
order	O
.	O
3	O
.	O
Elocutio	O
∼	O
Phrasing	O
the	O
arranged	O
ADUs	O
by	O
adding	O
connectives	O
at	O
unit	O
-	O
initial	O
or	O
unit	O
-	O
final	O
positions	O
.	O
Specifically	O
,	O
Wachsmuth	O
et	O
al	O
(	O
2018	O
)	O
selected	O
a	O
pool	O
of	O
200	O
ADUs	O
for	O
10	O
pairs	O
of	O
controversial	O
topic	O
and	O
stance	O
from	O
the	O
English	O
version	O
of	O
the	O
arg	O
-	O
microtexts	O
corpus	O
(	O
Peldszus	O
and	O
Stede	O
,	O
2016	O
)	O
.	O
As	O
a	O
preprocessing	O
step	O
,	O
they	O
"	O
decontextualized	O
"	O
these	O
ADUs	O
manually	O
by	O
removing	O
connectives	O
,	O
resolving	O
pronouns	O
,	O
and	O
similar	O
.	O
Each	O
topic	O
-	O
stance	O
pair	O
comes	O
with	O
20	O
such	O
ADUs	O
:	O
four	O
theses	O
,	O
four	O
con	O
units	O
,	O
and	O
12	O
pro	O
units	O
.	O
Table	O
1	O
shows	O
the	O
ADU	O
list	O
for	O
one	O
topic	O
-	O
stance	O
pair	O
.	O
26	O
participants	O
were	O
asked	O
by	O
Wachsmuth	O
et	O
al	O
(	O
2018	O
)	O
to	O
create	O
short	O
argumentative	O
texts	O
for	O
each	O
topic	O
-	O
stance	O
pair	O
following	O
one	O
of	O
two	O
basic	O
rhetorical	O
strategies	O
:	O
(	O
1	O
)	O
logos	O
-	O
oriented	O
,	O
i.e.	O
,	O
arguing	O
logically	O
,	O
and	O
(	O
2	O
)	O
pathos	O
-	O
oriented	O
,	O
i.e.	O
,	O
arguing	O
based	O
on	O
emotional	O
appeals	O
.	O
For	O
each	O
topic	O
-	O
stance	O
pair	O
they	O
created	O
an	O
argument	O
by	O
selecting	O
one	O
thesis	O
,	O
one	O
con	O
and	O
three	O
pro	O
units	O
that	O
they	O
thought	O
could	O
best	O
form	O
a	O
persuasive	O
argument	O
following	O
the	O
given	O
strategies	O
.	O
Table	O
2	O
shows	O
two	O
samples	O
of	O
generated	O
arguments	O
in	O
the	O
dataset	O
.	O
The	O
dataset	O
contains	O
130	O
logos	O
-	O
oriented	O
and	O
130	O

This	O
section	O
presents	O
our	O
computational	O
approach	O
to	O
synthesize	O
arguments	O
for	O
any	O
pair	O
of	O
topic	O
and	O
stance	O
,	O
following	O
one	O
of	O
two	O
basic	O
rhetorical	O
strategies	O
:	O
arguing	O
logically	O
(	O
logos	O
-	O
oriented	O
)	O
or	O
arguing	O
emotionally	O
(	O
pathos	O
-	O
oriented	O
)	O
.	O
A	O
black	O
-	O
box	O
view	O
of	O
the	O
approach	O
is	O
shown	O
in	O
Figure	O
1	O
.	O
As	O
input	O
,	O
our	O
approach	O
takes	O
a	O
strategy	O
as	O
well	O
as	O
a	O
pool	O
of	O
argumentative	O
discourse	O
units	O
(	O
ADUs	O
)	O
for	O
any	O
specific	O
topic	O
-	O
stance	O
pair	O
x.	O
Each	O
ADU	O
has	O
the	O
role	O
of	O
a	O
thesis	O
(	O
in	O
terms	O
of	O
claim	O
with	O
a	O
stance	O
on	O
the	O
topic	O
)	O
,	O
a	O
con	O
point	O
(	O
objecting	O
the	O
thesis	O
)	O
,	O
or	O
a	O
pro	O
point	O
(	O
supporting	O
the	O
thesis	O
)	O
.	O
The	O
approach	O
then	O
imitates	O
the	O
human	O
selection	O
,	O
arrangement	O
,	O
and	O
"	O
phrasing	O
"	O
of	O
a	O
sequence	O
of	O
n	O
ADUs	O
,	O
in	O
order	O
to	O
synthesize	O
an	O
argument	O
.	O
Phrasing	O
is	O
done	O
only	O
in	O
terms	O
of	O
picking	O
semantically	O
coherent	O
ADUs	O
for	O
the	O
arranged	O
sequence	O
;	O
the	O
addition	O
of	O
connectives	O
between	O
ADUs	O
is	O
left	O
to	O
future	O
work	O
.	O
Below	O
,	O
we	O
detail	O
how	O
we	O
realize	O
each	O
step	O
(	O
selection	O
,	O
arrangement	O
,	O
and	O
phrasing	O
)	O
with	O
a	O
topicindependent	O
model	O
.	O
For	O
each	O
step	O
,	O
we	O
explain	O
how	O
it	O
is	O
trained	O
(	O
illustrated	O
in	O
Figure	O
2	O
)	O
and	O
how	O
it	O
is	O
applied	O
to	O
an	O
unseen	O
topic	O
-	O
stance	O
pair	O
(	O
Figure	O
3	O
)	O
.	O

This	O
model	O
handles	O
the	O
selection	O
of	O
a	O
set	O
of	O
n	O
ADUs	O
for	O
a	O
topic	O
-	O
stance	O
pair	O
x	O
and	O
a	O
rhetorical	O
strategy	O
.	O
We	O
approach	O
the	O
selection	O
as	O
a	O
language	O
modeling	O
task	O
where	O
each	O
ADU	O
is	O
a	O
"	O
word	O
"	O
of	O
our	O
language	O
model	O
and	O
each	O
argument	O
a	O
"	O
sentence	O
"	O
.	O
To	O
abstract	O
from	O
topic	O
,	O
the	O
model	O
actually	O
selects	O
ADU	O
types	O
,	O
as	O
explained	O
in	O
the	O
following	O
.	O

Output	O
Strategy	O
-	O
specific	O
arg	O
'	O

Figure	O
1	O
:	O
Black	O
-	O
box	O
view	O
of	O
our	O
argumentation	O
synthesis	O
approach	O
.	O
The	O
input	O
is	O
a	O
rhetorical	O
strategy	O
as	O
well	O
as	O
a	O
pool	O
of	O
thesis	O
,	O
con	O
,	O
and	O
pro	O
ADUs	O
for	O
some	O
topicstance	O
pair	O
x.	O
The	O
approach	O
outputs	O
a	O
strategy	O
-	O
specific	O
sequence	O
of	O
n	O
ADUs	O
as	O
an	O
argument	O
for	O
x	O
(	O
here	O
,	O
n	O
=	O
5	O
)	O
.	O

We	O
use	O
the	O
NRC	O
lexicon	O
of	O
Mohammad	O
and	O
Turney	O
(	O
2013	O
)	O
.	O
The	O
lexicon	O
has	O
been	O
compiled	O
manually	O
using	O
crowdsourcing	O
and	O
contains	O
a	O
set	O
of	O
English	O
words	O
and	O
their	O
associations	O
with	O
(	O
1	O
)	O
sentiment	O
,	O
i.e.	O
,	O
negative	O
and	O
positive	O
polarities	O
,	O
and	O
(	O
2	O
)	O
emotions	O
,	O
i.e.	O
,	O
the	O
eight	O
basic	O
emotions	O
defined	O
by	O
Plutchik	O
(	O
1980	O
)	O
:	O
anger	O
,	O
anticipation	O
,	O
disgust	O
,	O
fear	O
,	O
joy	O
,	O
surprise	O
,	O
sadness	O
,	O
and	O
trust	O
.	O
These	O
features	O
are	O
represented	O
as	O
the	O
count	O
of	O
words	O
associated	O
with	O
each	O
category	O
(	O
e.g.	O
,	O
the	O
count	O
of	O
sad	O
words	O
in	O
an	O
ADU	O
)	O
.	O
Somasundaran	O
et	O
al	O
(	O
2007	O
)	O
constructed	O
a	O
lexicon	O
that	O
includes	O
the	O
following	O
arguing	O
patterns	O
:	O
assessments	O
,	O
doubt	O
,	O
authority	O
,	O
emphasis	O
,	O
necessity	O
,	O
causation	O
,	O
generalization	O
,	O
structure	O
,	O
conditionals	O
,	O
inconsistency	O
,	O
possibility	O
,	O
wants	O
,	O
contrast	O
,	O
priority	O
,	O
difficulty	O
,	O
inyour	O
-	O
From	O
the	O
type	O
sequence	O
,	O
a	O
set	O
of	O
candidate	O
arguments	O
is	O
decoded	O
.	O
(	O
2	O
)	O
The	O
arrangement	O
filters	O
out	O
candidates	O
not	O
matching	O
the	O
most	O
probable	O
ADU	O
role	O
sequence	O
,	O
(	O
T	O
hesis	O
,	O
Con	O
,	O
P	O
ro	O
,	O
P	O
ro	O
,	O
P	O
ro	O
)	O
.	O
(	O
3	O
)	O
Phrasing	O
scores	O
each	O
remaining	O
argument	O
and	O
outputs	O
the	O
top	O
argument	O
.	O

shoes	O
,	O
rhetorical	O
question	O
.	O
We	O
use	O
the	O
count	O
of	O
each	O
arguing	O
pattern	O
in	O
text	O
as	O
one	O
feature	O
(	O
e.g.	O
,	O
number	O
of	O
assessments	O
patterns	O
in	O
an	O
ADU	O
)	O
.	O

As	O
shown	O
in	O
Figure	O
3	O
,	O
the	O
selection	O
language	O
model	O
takes	O
the	O
ADUs	O
of	O
an	O
unseen	O
topic	O
-	O
stance	O
x	O
as	O
input	O
.	O
It	O
then	O
outputs	O
a	O
set	O
of	O
candidate	O
arguments	O
,	O
in	O
terms	O
of	O
sequences	O
of	O
ADUs	O
.	O
Each	O
ADU	O
is	O
encoded	O
into	O
a	O
cluster	O
label	O
(	O
representing	O
an	O
ADU	O
type	O
)	O
.	O
For	O
example	O
,	O
one	O
might	O
have	O
the	O
following	O
mappings	O
,	O
given	O
the	O
six	O
labels	O
A	O
-	O
F	O
from	O
Figure	O
2	O
:	O
A	O
{	O
T	O
he	O
x	O
,	O
1	O
,	O
T	O
he	O
x	O
,	O
2	O
,	O
Con	O
x	O
,	O
3	O
}	O
B	O
{	O
Con	O
x	O
,	O
2	O
,	O
P	O
ro	O
x	O
,	O
1	O
}	O
C	O
{	O
T	O
he	O
x	O
,	O
3	O
,	O
Con	O
x	O
,	O
c	O
,	O
P	O
ro	O
x	O
,	O
2	O
,	O
P	O
ro	O
x	O
,	O
3	O
}	O
D	O
{	O
P	O
ro	O
x	O
,	O
p	O
,	O
Con	O
x	O
,	O
1	O
}	O
E	O
{	O
T	O
he	O
x	O
,	O
t	O
}	O
F	O
{	O
T	O
he	O
x	O
,	O
4	O
,	O
Con	O
x	O
,	O
4	O
,	O
P	O
ro	O
x	O
,	O
4	O
}	O
The	O
language	O
model	O
for	O
either	O
of	O
the	O
two	O
rhetorical	O
strategies	O
generates	O
a	O
set	O
of	O
arguments	O
where	O
each	O
argument	O
is	O
composed	O
of	O
n	O
cluster	O
labels	O
,	O
e.g.	O
,	O
(	O
A	O
,	O
B	O
,	O
C	O
,	O
D	O
,	O
C	O
)	O
for	O
n	O
=	O
5	O
in	O
Figure	O
3	O
.	O
This	O
set	O
is	O
ranked	O
by	O
probability	O
of	O
the	O
associated	O
sequence	O
.	O
For	O
example	O
,	O
assume	O
that	O
(	O
A	O
,	O
B	O
,	O
C	O
,	O
D	O
,	O
C	O
)	O
is	O
most	O
probable	O
.	O
Then	O
we	O
decode	O
all	O
possible	O
ADU	O
sequences	O
for	O
topic	O
-	O
stance	O
x	O
from	O
(	O
A	O
,	O
B	O
,	O
C	O
,	O
D	O
,	O
C	O
)	O
to	O
a	O
set	O
of	O
candidate	O
arguments	O
:	O
(	O
A	O
,	O
B	O
,	O
C	O
,	O
D	O
,	O
C	O
)	O
{	O
T	O
he	O
x	O
,	O
1	O
,	O
T	O
he	O
x	O
,	O
2	O
,	O
Con	O
x	O
,	O
3	O
}	O
×	O
{	O
Con	O
x	O
,	O
2	O
,	O
P	O
ro	O
x	O
,	O
1	O
}	O
×	O
{	O
T	O
he	O
x	O
,	O
3	O
,	O
Con	O
x	O
,	O
c	O
,	O
P	O
ro	O
x	O
,	O
2	O
,	O
P	O
ro	O
x	O
,	O
3	O
}	O
×	O
{	O
P	O
ro	O
x	O
,	O
p	O
,	O
Con	O
x	O
,	O
1	O
}	O
×	O
{	O
T	O
he	O
x	O
,	O
3	O
,	O
Con	O
x	O
,	O
c	O
,	O
P	O
ro	O
x	O
,	O
2	O
,	O
P	O
ro	O
x	O
,	O
3	O
}	O
The	O
output	O
of	O
the	O
model	O
is	O
a	O
set	O
of	O
candidate	O
arguments	O
,	O
which	O
becomes	O
the	O
input	O
of	O
the	O
arrangement	O
language	O
model	O
.	O

In	O
the	O
arrangement	O
process	O
,	O
we	O
aim	O
to	O
imitate	O
the	O
human	O
behavior	O
of	O
arranging	O
ADUs	O
for	O
a	O
specific	O
topic	O
-	O
stance	O
following	O
a	O
rhetorical	O
strategy	O
(	O
here	O
,	O
logos	O
or	O
pathos	O
)	O
.	O
Again	O
,	O
we	O
approach	O
this	O
problem	O
as	O
a	O
language	O
modeling	O
task	O
.	O
Each	O
ADU	O
role	O
(	O
thesis	O
,	O
pro	O
,	O
or	O
con	O
)	O
is	O
a	O
word	O
of	O
the	O
language	O
model	O
and	O
each	O
argument	O
a	O
sentence	O
.	O

As	O
sketched	O
in	O
Figure	O
2	O
,	O
we	O
first	O
convert	O
the	O
humangenerated	O
arguments	O
from	O
a	O
sequence	O
of	O
ADUs	O
to	O
a	O
sequence	O
of	O
ADU	O
roles	O
.	O
Then	O
,	O
we	O
use	O
these	O
sequences	O
to	O
train	O
a	O
language	O
model	O
for	O
each	O
strategy	O
.	O

As	O
shown	O
in	O
Figure	O
3	O
,	O
the	O
arrangement	O
language	O
model	O
takes	O
as	O
input	O
the	O
candidate	O
arguments	O
that	O
we	O
get	O
from	O
the	O
selection	O
language	O
model	O
and	O
outputs	O
a	O
set	O
of	O
filtered	O
candidate	O
arguments	O
.	O
The	O
language	O
model	O
for	O
a	O
specific	O
strategy	O
generates	O
a	O
set	O
of	O
argument	O
structures	O
where	O
each	O
such	O
structure	O
is	O
a	O
sequence	O
of	O
n	O
ADU	O
roles	O
,	O
e.g.	O
,	O
(	O
T	O
hesis	O
,	O
Con	O
,	O
P	O
ro	O
,	O
P	O
ro	O
,	O
P	O
ro	O
)	O
for	O
n	O
=	O
5	O
in	O
Figure	O
3	O
.	O
This	O
set	O
is	O
ranked	O
by	O
the	O
probability	O
of	O
the	O
sequences	O
.	O
For	O
example	O
,	O
assume	O
that	O
the	O
most	O
frequent	O
sequence	O
is	O
(	O
T	O
hesis	O
,	O
Con	O
,	O
P	O
ro	O
,	O
P	O
ro	O
,	O
P	O
ro	O
)	O
.	O
Using	O
the	O
output	O
from	O
the	O
selection	O
language	O
model	O
,	O
we	O
filter	O
out	O
all	O
candidate	O
arguments	O
that	O
do	O
not	O
match	O
(	O
T	O
hesis	O
,	O
Con	O
,	O
P	O
ro	O
,	O
P	O
ro	O
,	O
P	O
ro	O
)	O
,	O
ending	O
up	O
with	O
the	O
following	O
filtered	O
arguments	O
:	O
{	O
T	O
he	O
x	O
,	O
1	O
,	O
T	O
he	O
x	O
,	O
2	O
}	O
×	O
{	O
Con	O
x	O
,	O
2	O
}	O
×	O
{	O
P	O
ro	O
x	O
,	O
2	O
,	O
P	O
ro	O
x	O
,	O
3	O
}	O
×	O
{	O
P	O
ro	O
x	O
,	O
p	O
}	O
×	O
{	O
P	O
ro	O
x	O
,	O
2	O
,	O
P	O
ro	O
x	O
,	O
3	O
}	O
The	O
output	O
of	O
the	O
model	O
is	O
a	O
filtered	O
set	O
of	O
candidate	O
arguments	O
,	O
which	O
becomes	O
the	O
input	O
of	O
the	O
phrasing	O
regression	O
model	O
.	O

The	O
set	O
of	O
arguments	O
resulting	O
from	O
the	O
selection	O
and	O
arrangement	O
language	O
models	O
are	O
based	O
on	O
topic	O
-	O
independent	O
features	O
.	O
The	O
missing	O
step	O
is	O
to	O
entail	O
the	O
topical	O
relationship	O
between	O
the	O
ADUs	O
in	O
each	O
generated	O
argument	O
.	O
We	O
approach	O
this	O
task	O
with	O
supervised	O
regression	O
.	O
As	O
indicated	O
above	O
,	O
our	O
model	O
does	O
not	O
really	O
phrase	O
an	O
argument	O
.	O
Rather	O
,	O
it	O
aims	O
to	O
choose	O
the	O
best	O
among	O
the	O
given	O
set	O
of	O
candidates	O
in	O
terms	O
of	O
semantic	O
coherence	O
.	O

At	O
this	O
point	O
,	O
the	O
phrasing	O
model	O
is	O
provided	O
by	O
the	O
filtered	O
arguments	O
from	O
the	O
arrangement	O
model	O
.	O
For	O
each	O
filtered	O
argument	O
,	O
we	O
extract	O
the	O
bigram	O
features	O
(	O
semantic	O
similarities	O
)	O
.	O
Next	O
,	O
using	O
the	O
phrasing	O
model	O
,	O
we	O
predict	O
the	O
score	O
of	O
each	O
sequence	O
.	O
The	O
sequence	O
with	O
the	O
highest	O
score	O
is	O
the	O
generated	O
argument	O
.	O
In	O
Figure	O
3	O
,	O
this	O
is	O
:	O
(	O
T	O
he	O
x	O
,	O
2	O
,	O
Con	O
x	O
,	O
2	O
,	O
P	O
ro	O
x	O
,	O
2	O
,	O
P	O
ro	O
x	O
,	O
p	O
,	O
P	O
ro	O
x	O
,	O
3	O
)	O

In	O
this	O
section	O
,	O
we	O
report	O
the	O
results	O
of	O
evaluating	O
the	O
introduced	O
approach	O
to	O
argumentation	O
synthesis	O
based	O
on	O
the	O
dataset	O
described	O
in	O
Section	O
3	O
.	O

This	O
paper	O
has	O
presented	O
a	O
topic	O
-	O
independent	O
computational	O
approach	O
to	O
imitate	O
the	O
process	O
of	O
selecting	O
,	O
arranging	O
,	O
and	O
phrasing	O
argumentative	O
discourse	O
units	O
(	O
ADUs	O
)	O
-	O
so	O
to	O
speak	O
,	O
to	O
synthesize	O
arguments	O
.	O
We	O
have	O
proposed	O
to	O
operationalize	O
the	O
necessary	O
synthesis	O
knowledge	O
in	O
the	O
form	O
of	O
a	O
combined	O
language	O
and	O
regression	O
model	O
that	O
predicts	O
ADU	O
sequences	O
.	O
So	O
far	O
,	O
we	O
have	O
evaluated	O
our	O
approach	O
on	O
a	O
small	O
dataset	O
only	O
that	O
contains	O
260	O
argumentative	O
texts	O
following	O
either	O
of	O
two	O
rhetorical	O
strategies	O
.	O
For	O
a	O
controlled	O
experiment	O
setting	O
based	O
on	O
this	O
data	O
,	O
we	O
have	O
reported	O
preliminary	O
results	O
of	O
medium	O
effectiveness	O
regarding	O
the	O
imitation	O
of	O
human	O
-	O
generated	O
arguments	O
.	O
A	O
big	O
challenge	O
for	O
the	O
future	O
is	O
to	O
move	O
from	O
such	O
a	O
controlled	O
setting	O
to	O
a	O
real	O
-	O
world	O
scenario	O
,	O
where	O
arguments	O
have	O
to	O
be	O
formed	O
for	O
a	O
freelychosen	O
topic	O
from	O
material	O
that	O
is	O
mined	O
from	O
the	O
web	O
.	O
Still	O
,	O
our	O
topic	O
-	O
independent	O
approach	O
defines	O
a	O
first	O
substantial	O
step	O
in	O
this	O
direction	O
.	O

IMPLI	O
:	O
Investigating	O
NLI	O
Models	O
'	O
Performance	O
on	O
Figurative	O
Language	O

Jamie	O
was	O
pissed	O
off	O
this	O
afternoon	O
.	O
Jamie	O
was	O
irritated	O
this	O
afternoon	O
There	O
's	O
a	O
marina	O
down	O
in	O
the	O
docks	O
.	O
There	O
's	O
a	O
marina	O
down	O
under	O
scrutiny	O
.	O

The	O
hearts	O
of	O
men	O
were	O
softened	O
.	O
The	O
men	O
were	O
made	O
kindler	O
and	O
gentler	O
.	O
The	O
gun	O
kicked	O
into	O
my	O
shoulder	O
.	O
The	O
mule	O
kicked	O
into	O
my	O
shoulder	O
.	O
Metaphors	O
involve	O
linking	O
conceptual	O
properties	O
of	O
two	O
or	O
more	O
domains	O
,	O
and	O
are	O
known	O
to	O
be	O
pervasive	O
in	O
everyday	O
language	O
(	O
Lakoff	O
and	O
Johnson	O
,	O
1980	O
;	O
Stefanowitsch	O
and	O
Gries	O
,	O
2008	O
;	O
Steen	O
et	O
al	O
,	O
2010	O
)	O
.	O
Recent	O
work	O
has	O
shown	O
that	O
these	O
types	O
of	O
figurative	O
language	O
are	O
impactful	O
across	O
a	O
broad	O
array	O
of	O
NLP	O
tasks	O
(	O
see	O
2.1	O
)	O
.	O
Large	O
-	O
scale	O
pre	O
-	O
training	O
and	O
transformer	O
-	O
based	O
architectures	O
have	O
yielded	O
increasingly	O
powerful	O
language	O
models	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
;	O
Devlin	O
et	O
al	O
,	O
2019	O
;	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
.	O
However	O
,	O
relatively	O
little	O
work	O
has	O
explored	O
these	O
models	O
'	O
representations	O
of	O
figurative	O
and	O
creative	O
language	O
.	O
NLI	O
datasets	O
have	O
widely	O
been	O
used	O
for	O
evaluating	O
the	O
performance	O
of	O
language	O
models	O
(	O
Dagan	O
et	O
al	O
,	O
2006	O
;	O
Bowman	O
et	O
al	O
,	O
2015a	O
;	O
Williams	O
et	O
al	O
,	O
2018	O
)	O
,	O
but	O
there	O
are	O
insufficient	O
figurative	O
language	O
datasets	O
in	O
which	O
a	O
literal	O
sentence	O
is	O
linked	O
to	O
a	O
corresponding	O
figurative	O
counterpart	O
that	O
are	O
large	O
enough	O
to	O
be	O
suitable	O
for	O
evaluating	O
NLI	O
.	O
Due	O
to	O
the	O
creative	O
nature	O
of	O
human	O
language	O
,	O
creating	O
a	O
dataset	O
of	O
diverse	O
,	O
high	O
-	O
quality	O
literal	O
/	O
figurative	O
pairs	O
is	O
time	O
-	O
consuming	O
and	O
difficult	O
.	O
To	O
address	O
this	O
gap	O
,	O
we	O
build	O
a	O
new	O
English	O
dataset	O
of	O
paired	O
expressions	O
designed	O
to	O
be	O
leveraged	O
to	O
explore	O
model	O
performance	O
via	O
NLI	O
.	O
Our	O
dataset	O
,	O
IMPLI	O
(	O
Idiomatic	O
/	O
Metaphoric	O
Paired	O
Language	O
Inference	O
)	O
,	O
is	O
comprised	O
of	O
both	O
silver	O
pairs	O
,	O
which	O
are	O
built	O
using	O
semi	O
-	O
automated	O
methods	O
(	O
3.1	O
)	O
,	O
as	O
well	O
as	O
hand	O
-	O
written	O
gold	O
pairs	O
(	O
3.4	O
)	O
,	O
crafted	O
to	O
reflect	O
both	O
entailment	O
and	O
nonentailment	O
scenarios	O
.	O
Each	O
pair	O
consists	O
of	O
a	O
sentence	O
containing	O
a	O
figurative	O
expression	O
(	O
idioms	O
/	O
metaphors	O
)	O
and	O
a	O
literal	O
counterpart	O
,	O
designed	O
to	O
be	O
either	O
entailed	O
or	O
non	O
-	O
entailed	O
by	O
the	O
figurative	O
expression	O
(	O
Table	O
1	O
shows	O
some	O
examples	O
)	O
.	O
Our	O
contribution	O
thus	O
consists	O
of	O
three	O
key	O
parts	O
:	O
We	O
create	O
a	O
new	O
IMPLI	O
dataset	O
consisting	O
of	O
24	O
,	O
029	O
silver	O
and	O
1	O
,	O
831	O
gold	O
sentence	O
pairs	O
consisting	O
of	O
idiomatic	O
and	O
metaphoric	O
phrases	O
that	O
result	O
in	O
both	O
entailment	O
and	O
nonentailment	O
relationship	O
(	O
see	O
Table	O
2	O
)	O
.	O
We	O
evaluate	O
language	O
models	O
in	O
an	O
NLI	O
setup	O
,	O
showing	O
that	O
metaphoric	O
language	O
is	O
surprisingly	O
easy	O
,	O
while	O
non	O
-	O
entailing	O
idiomatic	O
relationships	O
remain	O
extremely	O
difficult	O
.	O
We	O
evaluate	O
model	O
performance	O
in	O
a	O
number	O
of	O
experiments	O
,	O
showing	O
that	O
incorporating	O
idiomatic	O
expressions	O
into	O
the	O
training	O
data	O
is	O
less	O
helpful	O
than	O
expected	O
,	O
and	O
that	O
idioms	O
that	O
can	O
occur	O
more	O
in	O
more	O
flexible	O
syntactic	O
contexts	O
tend	O
to	O
be	O
easier	O
to	O
classify	O
.	O

Our	O
IMPLI	O
dataset	O
is	O
built	O
from	O
idiomatic	O
and	O
metaphoric	O
sentences	O
paired	O
with	O
entailing	O
and	O
nonentailing	O
counterparts	O
,	O
from	O
both	O
silver	O
pairs	O
(	O
3.1	O
)	O
and	O
manually	O
written	O
sentences	O
(	O
3.4	O
)	O
.	O
For	O
our	O
purposes	O
,	O
we	O
follow	O
McCoy	O
et	O
al	O
(	O
2019	O
)	O
in	O
conflating	O
the	O
neutral	O
and	O
contradiction	O
categories	O
into	O
a	O
nonentailment	O
label	O
.	O
We	O
then	O
label	O
every	O
pair	O
as	O
either	O
entailment	O
(	O
)	O
or	O
non	O
-	O
entailment	O
(	O
)	O
.	O
Due	O
to	O
the	O
difficult	O
nature	O
of	O
the	O
task	O
and	O
to	O
avoid	O
issues	O
with	O
crowdsourcing	O
(	O
Bowman	O
et	O
al	O
,	O
2020	O
)	O
,	O
we	O
employed	O
expert	O
annotators	O
.	O
We	O
used	O
two	O
fluent	O
English	O
speakers	O
,	O
both	O
graduate	O
students	O
in	O
linguistics	O
with	O
strong	O
knowledge	O
in	O
figurative	O
language	O
,	O
paid	O
at	O
a	O
rate	O
of	O
$	O
20	O
/	O
hr	O
.	O
For	O
each	O
method	O
below	O
,	O
we	O
ran	O
pilot	O
studies	O
,	O
incorporated	O
annotator	O
feedback	O
and	O
iteratively	O
assessed	O
the	O
viability	O
of	O
identifying	O
and	O
generating	O
appropriate	O
expressions	O
.	O
As	O
the	O
annotators	O
were	O
working	O
on	O
generating	O
new	O
expressions	O
,	O
agreement	O
was	O
not	O
calculated	O
:	O
we	O
instead	O
assessed	O
the	O
quality	O
of	O
the	O
resulting	O
expressions	O
(	O
see	O
Section	O
3.3	O
)	O
.	O
Table	O
2	O
contains	O
an	O
overview	O
of	O
the	O
different	O
entailment	O
and	O
non	O
-	O
entailment	O
types	O
collected	O
(	O
Detail	O
examples	O
are	O
also	O
provided	O
in	O
Appendix	O
D	O
)	O
.	O

First	O
,	O
we	O
explore	O
a	O
method	O
for	O
generating	O
silver	O
pairs	O
using	O
annotators	O
to	O
create	O
phrase	O
definitions	O
which	O
can	O
be	O
inserted	O
automatically	O
into	O
relevant	O
contexts	O
,	O
yielding	O
a	O
large	O
number	O
of	O
possible	O
entailment	O
and	O
non	O
-	O
entailment	O
pairs	O
that	O
differ	O
only	O
with	O
regard	O
to	O
the	O
relevant	O
phrase	O
.	O
Our	O
procedure	O
hinges	O
on	O
a	O
key	O
assumption	O
:	O
for	O
any	O
given	O
figurative	O
phrase	O
,	O
we	O
can	O
generate	O
a	O
contextually	O
independent	O
literal	O
paraphrase	O
.	O
We	O
then	O
replace	O
the	O
original	O
expression	O
with	O
the	O
literal	O
paraphrase	O
,	O
following	O
the	O
assumption	O
that	O
the	O
figurative	O
expression	O
necessarily	O
entails	O
its	O
literal	O
paraphrase	O
:	O
He	O
's	O
stuck	O
in	O
bed	O
,	O
which	O
is	O
his	O
hard	O
cheese	O
.	O
He	O
's	O
stuck	O
in	O
bed	O
,	O
which	O
is	O
his	O
bad	O
luck	O
.	O
Conversely	O
,	O
in	O
contexts	O
where	O
the	O
original	O
phrase	O
is	O
used	O
literally	O
,	O
replacing	O
it	O
with	O
the	O
literal	O
paraphrase	O
should	O
yield	O
a	O
non	O
-	O
entailment	O
relation	O
.	O
Switzerland	O
is	O
famous	O
for	O
six	O
cheeses	O
,	O
sometimes	O
referred	O
to	O
as	O
hard	O
cheeses	O
.	O
The	O
sailors	O
all	O
worked	O
under	O
scrutiny	O
.	O
The	O
sailors	O
all	O
worked	O
in	O
the	O
docks	O
Switzerland	O
is	O
famous	O
for	O
six	O
cheeses	O
,	O
sometimes	O
referred	O
to	O
as	O
bad	O
luck	O
.	O

As	O
a	O
second	O
method	O
for	O
generating	O
non	O
-	O
entailment	O
pairs	O
,	O
we	O
asked	O
annotators	O
to	O
write	O
novel	O
,	O
adversarial	O
definitions	O
for	O
IEs	O
.	O
Given	O
a	O
particular	O
phrase	O
,	O
they	O
were	O
instructed	O
to	O
invent	O
a	O
new	O
meaning	O
for	O
the	O
IE	O
that	O
was	O
not	O
entailed	O
by	O
the	O
true	O
meaning	O
,	O
but	O
which	O
seemed	O
reasonable	O
presuming	O
they	O
had	O
never	O
heard	O
the	O
original	O
IE	O
.	O
Some	O
examples	O
of	O
this	O
process	O
are	O
shown	O
in	O
Table	O
3	O
.	O
We	O
then	O
replace	O
these	O
adversarial	O
definitions	O
into	O
figurative	O
sentences	O
from	O
the	O
corpora	O
.	O
This	O
yields	O
pairs	O
where	O
the	O
premise	O
is	O
an	O
idiom	O
used	O
figuratively	O
,	O
and	O
the	O
hypothesis	O
is	O
a	O
sentence	O
that	O
attempts	O
to	O
rephrase	O
the	O
idiom	O
literally	O
,	O
but	O
does	O
so	O
incorrectly	O
,	O
thus	O
yielding	O
non	O
-	O
entailments	O
(	O
Figure	O
2	O
)	O
.	O

Metaphors	O
are	O
handled	O
in	O
a	O
similar	O
way	O
:	O
we	O
start	O
with	O
a	O
collection	O
of	O
minimal	O
metaphoric	O
expressions	O
(	O
MEs	O
)	O
.	O
These	O
are	O
subject	O
-	O
verb	O
-	O
object	O
and	O
adjective	O
-	O
noun	O
constructions	O
from	O
Tsvetkov	O
et	O
al	O
(	O
2014	O
)	O
.	O
Each	O
is	O
annotated	O
as	O
being	O
either	O
literal	O
or	O
metaphoric	O
,	O
along	O
with	O
an	O
example	O
sentence	O
.	O
We	O
passed	O
these	O
MEs	O
directly	O
to	O
annotators	O
,	O
who	O
were	O
then	O
instructed	O
to	O
replace	O
a	O
word	O
in	O
the	O
ME	O
so	O
that	O
it	O
would	O
be	O
considered	O
literal	O
in	O
a	O
neutral	O
context	O
.	O
They	O
ran	O
through	O
the	O
airport	O
to	O
board	O
their	O
flight	O
.	O

In	O
implementing	O
and	O
analyzing	O
this	O
procedure	O
,	O
we	O
noted	O
a	O
number	O
of	O
practical	O
issues	O
.	O
First	O
,	O
a	O
large	O
number	O
of	O
the	O
MEs	O
provided	O
are	O
actually	O
idiomatic	O
or	O
proverbial	O
:	O
the	O
focus	O
word	O
does	O
not	O
actually	O
contribute	O
to	O
the	O
metaphor	O
,	O
but	O
rather	O
the	O
entire	O
expression	O
is	O
necessary	O
.	O
Similarly	O
,	O
we	O
found	O
that	O
replacing	O
individual	O
parts	O
of	O
MEs	O
is	O
often	O
insufficient	O
to	O
fully	O
remove	O
the	O
metaphoric	O
meaning	O
.	O
We	O
iterated	O
over	O
possible	O
solutions	O
to	O
circumvent	O
these	O
issues	O
and	O
found	O
that	O
it	O
is	O
best	O
to	O
simply	O
skip	O
instances	O
for	O
which	O
a	O
replacement	O
does	O
not	O
yield	O
a	O
feasible	O
literal	O
interpretation	O
.	O

To	O
create	O
gold	O
pairs	O
,	O
annotators	O
were	O
given	O
a	O
figurative	O
sentence	O
along	O
with	O
the	O
focus	O
of	O
the	O
figurative	O
expression	O
:	O
for	O
idioms	O
,	O
this	O
is	O
the	O
IE	O
;	O
for	O
metaphors	O
,	O
the	O
focus	O
word	O
of	O
the	O
metaphor	O
.	O
For	O
idioms	O
,	O
we	O
used	O
the	O
MAGPIE	O
dataset	O
to	O
collect	O
contextually	O
figurative	O
expressions	O
.	O
For	O
metaphors	O
,	O
we	O
collected	O
metaphoric	O
sentences	O
from	O
the	O
VUA	O
Metaphor	O
Corpus	O
(	O
Steen	O
et	O
al	O
,	O
2010	O
)	O
,	O
the	O
metaphor	O
dataset	O
of	O
(	O
Mohammad	O
et	O
al	O
,	O
2016	O
)	O
,	O
and	O
instances	O
from	O
the	O
Gutenberg	O
poetry	O
corpus	O
(	O
Jacobs	O
,	O
2018	O
)	O
annotated	O
for	O
metaphoricity	O
(	O
Chakrabarty	O
et	O
al	O
,	O
2021b	O
;	O
Stowe	O
et	O
al	O
,	O
2021	O
)	O
.	O
Annotators	O
were	O
instructed	O
to	O
rewrite	O
the	O
sentence	O
literally	O
.	O
This	O
was	O
done	O
by	O
removing	O
or	O
rephrasing	O
the	O
figurative	O
component	O
of	O
the	O
sentence	O
.	O
This	O
yields	O
gold	O
standard	O
paraphrases	O
for	O
idiomatic	O
and	O
metaphoric	O
contexts	O
.	O
We	O
then	O
asked	O
annotators	O
to	O
write	O
non	O
-	O
entailed	O
hypotheses	O
for	O
each	O
premise	O
.	O
They	O
were	O
encour	O
-	O
aged	O
to	O
keep	O
as	O
much	O
of	O
the	O
original	O
utterance	O
as	O
possible	O
,	O
ensuring	O
high	O
lexical	O
overlap	O
,	O
while	O
removing	O
the	O
main	O
figurative	O
element	O
of	O
the	O
sentence	O
.	O
For	O
idioms	O
,	O
this	O
comes	O
from	O
adding	O
or	O
adjusting	O
words	O
to	O
force	O
a	O
literal	O
reading	O
of	O
the	O
idiom	O
:	O
The	O
old	O
girl	O
finally	O
kicked	O
the	O
bucket	O
.	O
The	O
girl	O
kicked	O
the	O
bucket	O
on	O
the	O
right	O
.	O
For	O
metaphors	O
,	O
this	O
typically	O
involves	O
keeping	O
the	O
same	O
phrasing	O
while	O
adapting	O
the	O
sentence	O
to	O
have	O
a	O
different	O
,	O
non	O
-	O
metaphoric	O
meaning	O
.	O
You	O
must	O
adhere	O
to	O
the	O
rules	O
.	O
You	O
must	O
adhere	O
the	O
rules	O
to	O
the	O
wall	O
.	O

Previous	O
work	O
in	O
NLI	O
has	O
employed	O
the	O
technique	O
of	O
replacing	O
words	O
in	O
the	O
literal	O
sentences	O
with	O
their	O
antonyms	O
to	O
yield	O
non	O
-	O
entailing	O
pairs	O
(	O
Chakrabarty	O
et	O
al	O
,	O
2021a	O
)	O
.	O
We	O
replicate	O
this	O
process	O
for	O
idioms	O
:	O
for	O
the	O
manually	O
elicited	O
definitions	O
,	O
we	O
replace	O
key	O
words	O
as	O
determined	O
by	O
annotators	O
with	O
their	O
antonyms	O
.	O
This	O
yields	O
sentences	O
which	O
negate	O
the	O
original	O
figurative	O
meaning	O
and	O
are	O
thus	O
suitable	O
non	O
-	O
entailment	O
pairs	O
.	O
Previous	O
work	O
found	O
this	O
antonym	O
replacement	O
for	O
figurative	O
language	O
remains	O
relatively	O
easy	O
for	O
NLI	O
systems	O
,	O
which	O
we	O
can	O
additionally	O
explore	O
with	O
regard	O
to	O
idioms	O
.	O
These	O
manual	O
annotations	O
provide	O
a	O
number	O
of	O
concrete	O
benefits	O
.	O
First	O
,	O
they	O
are	O
not	O
restricted	O
to	O
individual	O
words	O
or	O
phrases	O
(	O
excluding	O
antonyms	O
)	O
:	O
the	O
figurative	O
components	O
can	O
be	O
rewritten	O
freely	O
,	O
allowing	O
for	O
diverse	O
,	O
interesting	O
pairs	O
.	O
Second	O
,	O
they	O
are	O
written	O
by	O
experts	O
,	O
ensuring	O
higher	O
quality	O
than	O
the	O
automatic	O
annotations	O
,	O
which	O
may	O
be	O
noisy	O
.	O

Figure	O
6	O
shows	O
correlations	O
between	O
ICE	O
scores	O
(	O
determined	O
by	O
frequency	O
of	O
occurences	O
of	O
a	O
given	O
IE	O
outside	O
of	O
its	O
normal	O
form	O
)	O
and	O
roberta	O
-	O
base	O
model	O
performance	O
on	O
that	O
IE	O
.	O

It	O
would	O
be	O
good	O
to	O
roll	O
in	O
a	O
difficult	O
situation	O
all	O
over	O
.	O
Pour	O
in	O
the	O
soup	O
.	O
Pour	O
in	O
trouble	O
.	O
There	O
's	O
a	O
marina	O
down	O
in	O
the	O
docks	O
.	O
There	O
's	O
a	O
marina	O
down	O
under	O
scrutiny	O
.	O
(	O
The	O
mule	O
kicked	O
back	O
into	O
my	O
shoulder	O
.	O
This	O
was	O
conveniently	O
encapsulated	O
on	O
the	O
first	O
try	O
.	O
This	O
was	O
conveniently	O
encapsulated	O
in	O
the	O
first	O
battle	O
.	O
On	O
their	O
tracks	O
his	O
eyes	O
were	O
fastened	O
.	O
On	O
their	O
tracks	O
his	O
hands	O
were	O
fastened	O
.	O

In	O
this	O
section	O
,	O
we	O
first	O
describe	O
representing	O
a	O
query	O
,	O
sentence	O
and	O
document	O
using	O
local	O
and	O
distributed	O
representation	O
schemes	O
.	O
We	O
further	O
describe	O
enhanced	O
query	O
-	O
document	O
(	O
query	O
-	O
title	O
and	O
query	O
-	O
content	O
)	O
and	O
query	O
-	O
sentence	O
interactions	O
to	O
compute	O
query	O
-	O
aware	O
document	O
or	O
sentence	O
representations	O
for	O
Task	O
-	O
1	O
and	O
Task	O
-	O
2	O
,	O
respectively	O
.	O
Finally	O
,	O
we	O
discuss	O
the	O
application	O
of	O
supervised	O
neural	O
topic	O
modeling	O
in	O
ranking	O
documents	O
for	O
task	O
1	O
and	O
introduce	O
unsupervised	O
and	O
supervised	O
sentence	O
rankers	O
for	O
Task	O
-	O
2	O
.	O

The	O
RDoC	O
Task	O
-	O
2	O
aims	O
at	O
extracting	O
the	O
most	O
relevant	O
sentence	O
from	O
each	O
of	O
the	O
PubMed	O
abstract	O
for	O
the	O
corresponding	O
RDoC	O
construct	O
.	O
Each	O
abstract	O
consists	O
of	O
title	O
t	O
and	O
sentences	O
s	O
with	O
an	O
RDoC	O
construct	O
q.	O
To	O
address	O
RDoc	O
Task	O
-	O
2	O
,	O
we	O
first	O
compute	O
multi	O
-	O
view	O
representation	O
:	O
BoW	O
,	O
TF	O
-	O
IDF	O
and	O
QAR	O
(	O
i.e.	O
,	O
Φ	O
q	O
(	O
s	O
j	O
)	O
)	O
for	O
each	O
sentence	O
s	O
j	O
in	O
an	O
abstract	O
d.	O
On	O
other	O
hand	O
,	O
we	O
compute	O
ESR	O
representation	O
for	O
RDoC	O
construct	O
(	O
query	O
q	O
)	O
and	O
title	O
t	O
of	O
the	O
abstract	O
d	O
to	O
obtain	O
q	O
and	O
t	O
,	O
respectively	O
.	O
Figure	O
2	O
and	O
section	O
3.1	O
describe	O
the	O
computation	O
of	O
these	O
representations	O
.	O
We	O
then	O
use	O
the	O
representations	O
(	O
Φ	O
q	O
(	O
s	O
j	O
)	O
,	O
t	O
and	O
q	O
)	O
to	O
compute	O
a	O
relevance	O
scores	O
of	O
a	O
sentence	O
s	O
j	O
relative	O
to	O
q	O
and/or	O
t	O
via	O
unsupervised	O
and	O
supervised	O
ranking	O
schemes	O
,	O
discussed	O
in	O
the	O
following	O
section	O
.	O

As	O
shown	O
in	O
Figure	O
2	O
,	O
we	O
first	O
extract	O
representations	O
:	O
Φ	O
q	O
(	O
s	O
j	O
)	O
,	O
t	O
and	O
q	O
for	O
the	O
sentence	O
s	O
j	O
query	O
q	O
and	O
title	O
t.	O
During	O
ranking	O
sentences	O
within	O
an	O
abstract	O
for	O
the	O
given	O
RDoC	O
construct	O
q	O
,	O
we	O
also	O
consider	O
title	O
t	O
in	O
computing	O
the	O
relevance	O
score	O
for	O
each	O
sentence	O
relative	O
to	O
q	O
and	O
t.	O
It	O
is	O
inspired	O
from	O
the	O
fact	O
that	O
the	O
title	O
often	O
contains	O
relevant	O
terms	O
(	O
or	O
words	O
)	O
appearing	O
in	O
sentence	O
(	O
s	O
)	O
of	O
the	O
document	O
(	O
or	O
abstract	O
)	O
.	O
On	O
top	O
,	O
we	O
observe	O
that	O
q	O
is	O
a	O
very	O
short	O
text	O
and	O
non	O
-	O
descriptive	O
,	O
leading	O
to	O
minimal	O
text	O
overlap	O
with	O
s.	O
We	O
compute	O
two	O
relevance	O
scores	O
:	O
r	O
q	O
and	O
r	O
t	O
for	O
a	O
sentence	O
s	O
j	O
with	O
respect	O
to	O
a	O
query	O
q	O
and	O
title	O
t	O
,	O
respectively	O
.	O
r	O
q	O
=	O
sim	O
(	O
q	O
,	O
Φ	O
q	O
(	O
s	O
j	O
)	O
)	O
and	O
r	O
t	O
=	O
sim	O
(	O
t	O
,	O
Φ	O
q	O
(	O
s	O
j	O
)	O
)	O
Now	O
,	O
we	O
devise	O
two	O
ways	O
to	O
combine	O
the	O
rele	O
-	O
vance	O
scores	O
r	O
q	O
and	O
r	O
t	O
in	O
unsupervised	O
paradigm	O
:	O
version1	O
:	O
r	O
unsup	O
1	O
=	O
r	O
q	O
r	O
q	O
+	O
r	O
t	O
r	O
t	O
Observe	O
that	O
the	O
relevance	O
scores	O
are	O
weighted	O
by	O
itself	O
.	O
However	O
,	O
the	O
task	O
-	O
2	O
expects	O
a	O
higher	O
importance	O
to	O
the	O
relevance	O
score	O
r	O
q	O
over	O
q	O
t	O
.	O
Therefore	O
,	O
we	O
coin	O
the	O
following	O
weighting	O
scheme	O
to	O
give	O
higher	O
importance	O
to	O
r	O
q	O
only	O
if	O
it	O
is	O
higher	O
than	O
r	O
t	O
otherwise	O
we	O
compute	O
a	O
weight	O
factor	O
r	O
t	O
for	O
r	O
t	O
.	O
version2	O
:	O
r	O
unsup	O
2	O
=	O
r	O
q	O
r	O
q	O
+	O
r	O
t	O
r	O
t	O
where	O
r	O
t	O
is	O
compute	O
as	O
:	O
r	O
t	O
=	O
(	O
r	O
t	O
>	O
r	O
q	O
)	O
|	O
r	O
t	O
−	O
r	O
q	O
|	O
The	O
relevance	O
score	O
r	O
unsup	O
2	O
is	O
effective	O
in	O
ranking	O
sentences	O
when	O
a	O
query	O
and	O
sentence	O
does	O
not	O
overlap	O
.	O
In	O
such	O
a	O
scenario	O
,	O
a	O
sentence	O
is	O
scored	O
by	O
title	O
,	O
penalized	O
by	O
a	O
factor	O
of	O
|	O
r	O
t	O
−	O
r	O
q	O
|	O
.	O
At	O
the	O
end	O
,	O
we	O
obtain	O
a	O
final	O
relevance	O
score	O
r	O
unsup	O
f	O
for	O
a	O
sentence	O
s	O
j	O
by	O
summing	O
the	O
relevance	O
scores	O
of	O
BM25	O
-	O
Extra	O
and	O
r	O
unsup	O
1	O
or	O
r	O
unsup	O
2	O
.	O

Table	O
7	O
:	O
RDoC	O
Task	O
-	O
2	O
analysis	O
:	O
This	O
table	O
shows	O
that	O
the	O
most	O
relevant	O
sentence	O
predicted	O
using	O
reRank	O
(	O
BM25	O
-	O
Extra	O
)	O
is	O
actually	O
not	O
a	O
relevant	O
sentence	O
,	O
but	O
Ensemble	O
{	O
#	O
1	O
,	O
#	O
2	O
,	O
#	O
4	O
}	O
(	O
Table	O
5	O
)	O
predicts	O
the	O
correct	O
sentence	O
as	O
the	O
most	O
relevant	O
.	O
els	O
(	O
except	O
[	O
#	O
3	O
]	O
)	O
outperform	O
tranditional	O
ranking	O
models	O
,	O
e.g.	O
,	O
reRank	O
(	O
BM25	O
-	O
Extra	O
)	O
in	O
terms	O
of	O
query	O
-	O
document	O
relevance	O
score	O
.	O

Table	O
7	O
shows	O
that	O
the	O
most	O
relevant	O
sentence	O
predicted	O
by	O
reRank	O
(	O
BM25	O
-	O
Extra	O
)	O
is	O
actually	O
a	O
non	O
-	O
relevant	O
sentence	O
.	O
But	O
an	O
ensemble	O
of	O
predictions	O
from	O
both	O
unsupervised	O
and	O
supervised	O
ranker	O
models	O
correctly	O
predicts	O
the	O
relevant	O
sentence	O
.	O
This	O
suggests	O
that	O
complementary	O
knowledge	O
of	O
different	O
models	O
is	O
able	O
to	O
capture	O
the	O
relevance	O
of	O
sentences	O
on	O
different	O
scales	O
and	O
majority	O
voting	O
among	O
them	O
is	O
,	O
evidently	O
,	O
a	O
robust	O
sentence	O
ranking	O
technique	O
.	O

This	O
research	O
was	O
supported	O
by	O
Bundeswirtschaftsministerium	O
(	O
bmwi.de	O
)	O
,	O
grant	O
01MD19003E	O
(	O
PLASS	O
(	O
plass.io	O
)	O
)	O
at	O
Siemens	O
AG	O
-	O
CT	O
Machine	O
Intelligence	O
,	O
Munich	O
Germany	O
.	O

Invited	O
Speaker	O

This	O
special	O
theme	O
considers	O
the	O
marking	O
of	O
information	O
quality	O
in	O
discourse	O
,	O
i.e.	O
,	O
annotations	O
that	O
mark	O
how	O
the	O
speaker	O
/	O
writer	O
expresses	O
assessments	O
.	O
These	O
assessments	O
may	O
be	O
explicit	O
and/or	O
implicit	O
in	O
discourse	O
,	O
and	O
may	O
reflect	O
positions	O
,	O
beliefs	O
,	O
opinions	O
,	O
appraisals	O
and/or	O
assessments	O
about	O
written	O
or	O
spoken	O
propositions	O
,	O
for	O
example	O
,	O
how	O
a	O
politician	O
shows	O
in	O
discourse	O
the	O
degree	O
of	O
truthfulness	O
in	O
one	O
of	O
his	O
/	O
her	O
electoral	O
promises	O
,	O
or	O
how	O
a	O
reporter	O
shows	O
his	O
/	O
her	O
degree	O
of	O
belief	O
in	O
what	O
the	O
politician	O
stated	O
.	O
This	O
might	O
include	O
the	O
annotation	O
of	O
devices	O
such	O
as	O
hedges	O
(	O
"	O
Donald	O
claims	O
that	O
the	O
crowd	O
size	O
,	O
if	O
you	O
can	O
really	O
trust	O
him	O
to	O
measure	O
it	O
,	O
was	O
enormous	O
.	O
"	O
)	O
,	O
committed	O
belief	O
(	O
"	O
The	O
winners	O
of	O
the	O
contest	O
will	O
be	O
announced	O
tomorrow	O
.	O
"	O
)	O
or	O
attitudes	O
(	O
"	O
It	O
is	O
with	O
great	O
sadness	O
that	O
we	O
have	O
learnt	O
about	O
the	O
death	O
of	O
6	O
people	O
in	O
the	O
accident	O
.	O
"	O
)	O
.	O

Workshop	O
chairs	O

Hybrid	O
Code	O
Networks	O
:	O
practical	O
and	O
efficient	O
end	O
-	O
to	O
-	O
end	O
dialog	O
control	O
with	O
supervised	O
and	O
reinforcement	O
learning	O

End	O
-	O
to	O
-	O
end	O
learning	O
of	O
recurrent	O
neural	O
networks	O
(	O
RNNs	O
)	O
is	O
an	O
attractive	O
solution	O
for	O
dialog	O
systems	O
;	O
however	O
,	O
current	O
techniques	O
are	O
data	O
-	O
intensive	O
and	O
require	O
thousands	O
of	O
dialogs	O
to	O
learn	O
simple	O
behaviors	O
.	O
We	O
introduce	O
Hybrid	O
Code	O
Networks	O
(	O
HCNs	O
)	O
,	O
which	O
combine	O
an	O
RNN	O
with	O
domain	O
-	O
specific	O
knowledge	O
encoded	O
as	O
software	O
and	O
system	O
action	O
templates	O
.	O
Compared	O
to	O
existing	O
end	O
-	O
toend	O
approaches	O
,	O
HCNs	O
considerably	O
reduce	O
the	O
amount	O
of	O
training	O
data	O
required	O
,	O
while	O
retaining	O
the	O
key	O
benefit	O
of	O
inferring	O
a	O
latent	O
representation	O
of	O
dialog	O
state	O
.	O
In	O
addition	O
,	O
HCNs	O
can	O
be	O
optimized	O
with	O
supervised	O
learning	O
,	O
reinforcement	O
learning	O
,	O
or	O
a	O
mixture	O
of	O
both	O
.	O
HCNs	O
attain	O
stateof	O
-	O
the	O
-	O
art	O
performance	O
on	O
the	O
bAbI	O
dialog	O
dataset	O
(	O
Bordes	O
and	O
Weston	O
,	O
2016	O
)	O
,	O
and	O
outperform	O
two	O
commercially	O
deployed	O
customer	O
-	O
facing	O
dialog	O
systems	O
.	O

Task	O
-	O
oriented	O
dialog	O
systems	O
help	O
a	O
user	O
to	O
accomplish	O
some	O
goal	O
using	O
natural	O
language	O
,	O
such	O
as	O
making	O
a	O
restaurant	O
reservation	O
,	O
getting	O
technical	O
support	O
,	O
or	O
placing	O
a	O
phonecall	O
.	O
Historically	O
,	O
these	O
dialog	O
systems	O
have	O
been	O
built	O
as	O
a	O
pipeline	O
,	O
with	O
modules	O
for	O
language	O
understanding	O
,	O
state	O
tracking	O
,	O
action	O
selection	O
,	O
and	O
language	O
generation	O
.	O
However	O
,	O
dependencies	O
between	O
modules	O
introduce	O
considerable	O
complexity	O
-	O
for	O
example	O
,	O
it	O
is	O
often	O
unclear	O
how	O
to	O
define	O
the	O
dialog	O
state	O
and	O
what	O
history	O
to	O
maintain	O
,	O
yet	O
action	O
selection	O
relies	O
exclusively	O
on	O
the	O
state	O
for	O
input	O
.	O
Moreover	O
,	O
training	O
each	O
module	O
requires	O
specialized	O
labels	O
.	O
*	O
Currently	O
at	O
JPMorgan	O
Chase	O
Recently	O
,	O
end	O
-	O
to	O
-	O
end	O
approaches	O
have	O
trained	O
recurrent	O
neural	O
networks	O
(	O
RNNs	O
)	O
directly	O
on	O
text	O
transcripts	O
of	O
dialogs	O
.	O
A	O
key	O
benefit	O
is	O
that	O
the	O
RNN	O
infers	O
a	O
latent	O
representation	O
of	O
state	O
,	O
obviating	O
the	O
need	O
for	O
state	O
labels	O
.	O
However	O
,	O
end	O
-	O
to	O
-	O
end	O
methods	O
lack	O
a	O
general	O
mechanism	O
for	O
injecting	O
domain	O
knowledge	O
and	O
constraints	O
.	O
For	O
example	O
,	O
simple	O
operations	O
like	O
sorting	O
a	O
list	O
of	O
database	O
results	O
or	O
updating	O
a	O
dictionary	O
of	O
entities	O
can	O
expressed	O
in	O
a	O
few	O
lines	O
of	O
software	O
,	O
yet	O
may	O
take	O
thousands	O
of	O
dialogs	O
to	O
learn	O
.	O
Moreover	O
,	O
in	O
some	O
practical	O
settings	O
,	O
programmed	O
constraints	O
are	O
essential	O
-	O
for	O
example	O
,	O
a	O
banking	O
dialog	O
system	O
would	O
require	O
that	O
a	O
user	O
is	O
logged	O
in	O
before	O
they	O
can	O
retrieve	O
account	O
information	O
.	O
This	O
paper	O
presents	O
a	O
model	O
for	O
end	O
-	O
to	O
-	O
end	O
learning	O
,	O
called	O
Hybrid	O
Code	O
Networks	O
(	O
HCNs	O
)	O
which	O
addresses	O
these	O
problems	O
.	O
In	O
addition	O
to	O
learning	O
an	O
RNN	O
,	O
HCNs	O
also	O
allow	O
a	O
developer	O
to	O
express	O
domain	O
knowledge	O
via	O
software	O
and	O
action	O
templates	O
.	O
Experiments	O
show	O
that	O
,	O
compared	O
to	O
existing	O
recurrent	O
end	O
-	O
to	O
-	O
end	O
techniques	O
,	O
HCNs	O
achieve	O
the	O
same	O
performance	O
with	O
considerably	O
less	O
training	O
data	O
,	O
while	O
retaining	O
the	O
key	O
benefit	O
of	O
end	O
-	O
to	O
-	O
end	O
trainability	O
.	O
Moreover	O
,	O
the	O
neural	O
network	O
can	O
be	O
trained	O
with	O
supervised	O
learning	O
or	O
reinforcement	O
learning	O
,	O
by	O
changing	O
the	O
gradient	O
update	O
applied	O
.	O
This	O
paper	O
is	O
organized	O
as	O
follows	O
.	O
Section	O
2	O
describes	O
the	O
model	O
,	O
and	O
Section	O
3	O
compares	O
the	O
model	O
to	O
related	O
work	O
.	O
Section	O
4	O
applies	O
HCNs	O
to	O
the	O
bAbI	O
dialog	O
dataset	O
(	O
Bordes	O
and	O
Weston	O
,	O
2016	O
)	O
.	O
Section	O
5	O
then	O
applies	O
the	O
method	O
to	O
real	O
customer	O
support	O
domains	O
at	O
our	O
company	O
.	O
Section	O
6	O
illustrates	O
how	O
HCNs	O
can	O
be	O
optimized	O
with	O
reinforcement	O
learning	O
,	O
and	O
Section	O
7	O
concludes	O
.	O

Broadly	O
there	O
are	O
two	O
lines	O
of	O
work	O
applying	O
machine	O
learning	O
to	O
dialog	O
control	O
.	O
The	O
first	O
decomposes	O
a	O
dialog	O
system	O
into	O
a	O
pipeline	O
,	O
typically	O
including	O
language	O
understanding	O
,	O
dialog	O
state	O
tracking	O
,	O
action	O
selection	O
policy	O
,	O
and	O
language	O
generation	O
(	O
Levin	O
et	O
al	O
,	O
2000	O
;	O
Singh	O
et	O
al	O
,	O
2002	O
;	O
Williams	O
and	O
Young	O
,	O
2007	O
;	O
Williams	O
,	O
2008	O
;	O
Hori	O
et	O
al	O
,	O
2009	O
;	O
Lee	O
et	O
al	O
,	O
2009	O
;	O
Griol	O
et	O
al	O
,	O
2008	O
;	O
Young	O
et	O
al	O
,	O
2013	O
;	O
Li	O
et	O
al	O
,	O
2014	O
)	O
.	O
Specifically	O
related	O
to	O
HCNs	O
,	O
past	O
work	O
has	O
implemented	O
the	O
policy	O
as	O
feed	O
-	O
forward	O
neural	O
networks	O
,	O
trained	O
with	O
supervised	O
learning	O
followed	O
by	O
reinforcement	O
learning	O
.	O
In	O
these	O
works	O
,	O
the	O
policy	O
has	O
not	O
been	O
recurrent	O
-	O
i.e.	O
,	O
the	O
policy	O
depends	O
on	O
the	O
state	O
tracker	O
to	O
summarize	O
observable	O
dialog	O
history	O
into	O
state	O
features	O
,	O
which	O
requires	O
design	O
and	O
specialized	O
labeling	O
.	O
By	O
contrast	O
,	O
HCNs	O
use	O
an	O
RNN	O
which	O
automatically	O
infers	O
a	O
representation	O
of	O
state	O
.	O
For	O
learning	O
efficiency	O
,	O
HCNs	O
use	O
an	O
external	O
lightweight	O
process	O
for	O
tracking	O
entity	O
values	O
,	O
but	O
the	O
policy	O
is	O
not	O
strictly	O
dependent	O
on	O
it	O
:	O
as	O
an	O
illustration	O
,	O
in	O
Section	O
5	O
below	O
,	O
we	O
demonstrate	O
an	O
HCNbased	O
dialog	O
system	O
which	O
has	O
no	O
external	O
state	O
tracker	O
.	O
If	O
there	O
is	O
context	O
which	O
is	O
not	O
apparent	O
in	O
the	O
text	O
in	O
the	O
dialog	O
,	O
such	O
as	O
database	O
status	O
,	O
this	O
can	O
be	O
encoded	O
as	O
a	O
context	O
feature	O
to	O
the	O
RNN	O
.	O
The	O
second	O
,	O
more	O
recent	O
line	O
of	O
work	O
applies	O
recurrent	O
neural	O
networks	O
(	O
RNNs	O
)	O
to	O
learn	O
"	O
endto	O
-	O
end	O
"	O
models	O
,	O
which	O
map	O
from	O
an	O
observable	O
dialog	O
history	O
directly	O
to	O
a	O
sequence	O
of	O
output	O
words	O
(	O
Sordoni	O
et	O
al	O
,	O
2015	O
;	O
Shang	O
et	O
al	O
,	O
2015	O
;	O
Vinyals	O
and	O
Le	O
,	O
2015	O
;	O
Yao	O
et	O
al	O
,	O
2015	O
;	O
Serban	O
et	O
al	O
,	O
2016	O
;	O
Li	O
et	O
al	O
,	O
2016a	O
,	O
c	O
;	O
Luan	O
et	O
al	O
,	O
2016	O
;	O
Xu	O
et	O
al	O
,	O
2016	O
;	O
Li	O
et	O
al	O
,	O
2016b	O
;	O
Mei	O
et	O
al	O
,	O
2016	O
;	O
.	O
These	O
systems	O
can	O
be	O
applied	O
to	O
task	O
-	O
oriented	O
domains	O
by	O
adding	O
special	O
"	O
API	O
call	O
"	O
actions	O
,	O
enumerating	O
database	O
output	O
as	O
a	O
sequence	O
of	O
tokens	O
(	O
Bordes	O
and	O
Weston	O
,	O
2016	O
)	O
,	O
then	O
learning	O
an	O
RNN	O
using	O
Memory	O
Networks	O
(	O
Sukhbaatar	O
et	O
al	O
,	O
2015	O
)	O
,	O
gated	O
memory	O
networks	O
(	O
Liu	O
and	O
Perez	O
,	O
2016	O
)	O
,	O
query	O
reduction	O
networks	O
(	O
Seo	O
et	O
al	O
,	O
2016	O
)	O
,	O
and	O
copyaugmented	O
networks	O
(	O
Eric	O
and	O
Manning	O
,	O
2017	O
)	O
.	O
In	O
each	O
of	O
these	O
architectures	O
,	O
the	O
RNN	O
learns	O
to	O
manipulate	O
entity	O
values	O
,	O
for	O
example	O
by	O
saving	O
them	O
in	O
a	O
memory	O
.	O
Output	O
is	O
produced	O
by	O
generating	O
a	O
sequence	O
of	O
tokens	O
(	O
or	O
ranking	O
all	O
possible	O
surface	O
forms	O
)	O
,	O
which	O
can	O
also	O
draw	O
from	O
this	O
memory	O
.	O
HCNs	O
also	O
use	O
an	O
RNN	O
to	O
accumulate	O
dialog	O
state	O
and	O
choose	O
actions	O
.	O
However	O
,	O
HCNs	O
differ	O
in	O
that	O
they	O
use	O
developer	O
-	O
provided	O
action	O
templates	O
,	O
which	O
can	O
contain	O
entity	O
references	O
,	O
such	O
as	O
"	O
<	O
city	O
>	O
,	O
right	O
?	O
"	O
.	O
This	O
design	O
reduce	O
learning	O
complexity	O
,	O
and	O
also	O
enable	O
the	O
software	O
to	O
limit	O
which	O
actions	O
are	O
available	O
via	O
an	O
action	O
mask	O
,	O
at	O
the	O
expense	O
of	O
developer	O
effort	O
.	O
To	O
further	O
reduce	O
learning	O
complexity	O
in	O
a	O
practical	O
system	O
,	O
entities	O
are	O
tracked	O
separately	O
,	O
outside	O
the	O
the	O
RNN	O
,	O
which	O
also	O
allows	O
them	O
to	O
be	O
substituted	O
into	O
action	O
templates	O
.	O
Also	O
,	O
past	O
end	O
-	O
to	O
-	O
end	O
recurrent	O
models	O
have	O
been	O
trained	O
using	O
supervised	O
learning	O
,	O
whereas	O
we	O
show	O
how	O
HCNs	O
can	O
also	O
be	O
trained	O
with	O
reinforcement	O
learning	O
.	O

The	O
RNN	O
was	O
specified	O
using	O
Keras	O
version	O
0.3.3	O
,	O
with	O
back	O
-	O
end	O
computation	O
in	O
Theano	O
version	O
0.8.0.dev0	O
(	O
Theano	O
Development	O
Team	O
,	O
2016	O
;	O
Chollet	O
,	O
2015	O
)	O
.	O
The	O
Keras	O
model	O
specification	O
is	O
given	O
below	O
.	O
The	O
input	O
variable	O
obs	O
includes	O
all	O
features	O
from	O
Figure	O
1	O
step	O
6	O
except	O
for	O
the	O
previous	O
action	O
(	O
step	O
18	O
)	O
and	O
the	O
action	O
mask	O
(	O
step	O
6	O
,	O
top	O
-	O
most	O
vector	O
)	O
.	O
Model	O
sizes	O
are	O
given	O
in	O
Table	O
3	O
.	O
Example	O
dialogs	O
are	O
given	O
below	O
for	O
each	O
of	O
the	O
5	O
dialog	O
systems	O
.	O
For	O
space	O
and	O
readability	O
,	O
the	O
entity	O
tags	O
that	O
appear	O
in	O
the	O
user	O
and	O
system	O
sides	O
of	O
the	O
dialogs	O
have	O
been	O
removed	O
-	O
for	O
example	O
,	O
Call	O
<	O
name	O
>	O
Joan</name	O
>	O
is	O
shown	O
as	O
Call	O
Joan	O
.	O

This	O
paper	O
proposes	O
to	O
adapt	O
self	O
-	O
attention	O
to	O
discourse	O
level	O
for	O
modeling	O
discourse	O
elements	O
in	O
argumentative	O
student	O
essays	O
.	O
Specifically	O
,	O
we	O
focus	O
on	O
two	O
issues	O
.	O
First	O
,	O
we	O
propose	O
structural	O
sentence	O
positional	O
encodings	O
to	O
explicitly	O
represent	O
sentence	O
positions	O
.	O
Second	O
,	O
we	O
propose	O
to	O
use	O
inter	O
-	O
sentence	O
attentions	O
to	O
capture	O
sentence	O
interactions	O
and	O
enhance	O
sentence	O
representation	O
.	O
We	O
conduct	O
experiments	O
on	O
two	O
datasets	O
:	O
a	O
Chinese	O
dataset	O
and	O
an	O
English	O
dataset	O
.	O
We	O
find	O
that	O
(	O
i	O
)	O
sentence	O
positional	O
encodings	O
can	O
lead	O
to	O
a	O
large	O
improvement	O
for	O
identifying	O
discourse	O
elements	O
;	O
(	O
ii	O
)	O
a	O
structural	O
relative	O
positional	O
encoding	O
of	O
sentences	O
shows	O
to	O
be	O
most	O
effective	O
;	O
(	O
iii	O
)	O
inter	O
-	O
sentence	O
attention	O
vectors	O
are	O
useful	O
as	O
a	O
kind	O
of	O
sentence	O
representation	O
for	O
identifying	O
discourse	O
elements	O
.	O

Attention	O
mechanism	O
was	O
first	O
introduced	O
by	O
(	O
Bahdanau	O
et	O
al	O
,	O
2015	O
)	O
in	O
the	O
encoder	O
-	O
decoder	O
framework	O
.	O
Attention	O
has	O
the	O
ability	O
to	O
learn	O
important	O
regions	O
within	O
a	O
context	O
and	O
has	O
been	O
widely	O
adopted	O
in	O
deep	O
learning	O
.	O
Liu	O
and	O
Lapata	O
(	O
2018	O
)	O
proposed	O
a	O
structured	O
attention	O
mechanism	O
to	O
derive	O
a	O
tree	O
over	O
a	O
text	O
,	O
akin	O
to	O
an	O
RST	O
discourse	O
tree	O
.	O
Ferracane	O
et	O
al	O
(	O
2019	O
)	O
evaluated	O
the	O
model	O
,	O
however	O
,	O
found	O
multiple	O
negative	O
results	O
.	O
Attention	O
mechanism	O
has	O
also	O
been	O
applied	O
for	O
RST	O
parsing	O
and	O
its	O
applications	O
(	O
Li	O
et	O
al	O
,	O
2016	O
;	O
Ji	O
and	O
Smith	O
,	O
2017	O
;	O
Huber	O
and	O
Carenini	O
,	O
2019	O
)	O
but	O
it	O
is	O
mostly	O
used	O
for	O
capturing	O
local	O
semantic	O
interactions	O
.	O

The	O
construction	O
of	O
the	O
Chinese	O
Dataset	O
mainly	O
follows	O
the	O
definition	O
and	O
taxonomy	O
of	O
discourse	O
elements	O
proposed	O
by	O
Burstein	O
et	O
al	O
(	O
2003	O
)	O
.	O
Specifically	O
,	O
we	O
consider	O
the	O
following	O
discourse	O
elements	O
:	O
Introduction	O
The	O
role	O
of	O
introduction	O
is	O
to	O
introduce	O
background	O
or	O
attract	O
readers	O
'	O
attention	O
before	O
making	O
claims	O
.	O

We	O
presented	O
a	O
method	O
DiSA	O
to	O
identify	O
discourse	O
elements	O
in	O
argumentative	O
student	O
essays	O
by	O
explicitly	O
modeling	O
structural	O
positions	O
and	O
inter	O
-	O
sentence	O
relations	O
.	O
The	O
structural	O
positional	O
encoding	O
considers	O
relative	O
positions	O
of	O
the	O
sentence	O
and	O
its	O
paragraph	O
.	O
Moreover	O
,	O
we	O
use	O
inter	O
-	O
sentence	O
attention	O
vectors	O
to	O
capture	O
sentence	O
relations	O
in	O
content	O
and	O
function	O
.	O
Experiments	O
on	O
a	O
Chinese	O
dataset	O
and	O
an	O
English	O
dataset	O
show	O
that	O
(	O
i	O
)	O
although	O
it	O
is	O
simple	O
,	O
the	O
positional	O
encoding	O
largely	O
improves	O
the	O
performance	O
.	O
This	O
indicates	O
that	O
modeling	O
structural	O
positions	O
is	O
feasible	O
and	O
important	O
to	O
analyze	O
the	O
role	O
of	O
sentences	O
;	O
(	O
ii	O
)	O
discourse	O
elements	O
could	O
be	O
better	O
identified	O
with	O
the	O
help	O
of	O
inter	O
-	O
sentence	O
attention	O
vectors	O
,	O
especially	O
the	O
minority	O
ones	O
and	O
the	O
ones	O
that	O
have	O
distinct	O
relation	O
patterns	O
to	O
other	O
sentences	O
.	O
In	O
future	O
,	O
we	O
plan	O
to	O
evaluate	O
DiSA	O
on	O
other	O
discourse	O
analysis	O
tasks	O
.	O

This	O
work	O
is	O
supported	O
by	O
the	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
(	O
Nos	O
.	O
61876113	O
,	O
61876112	O
)	O
,	O
Beijing	O
Natural	O
Science	O
Foundation	O
(	O
No	O
.	O
4192017	O
)	O
,	O
Support	O
Project	O
of	O
Highlevel	O
Teachers	O
in	O
Beijing	O
Municipal	O
Universities	O
in	O
the	O
Period	O
of	O
13th	O
Five	O
-	O
year	O
Plan	O
(	O
CIT&TCD20170322	O
)	O
and	O
Capital	O
Building	O
for	O
Sci	O
-	O
Tech	O
Innovation	O
-	O
Fundamental	O
Scientific	O
Research	O
Funds	O
.	O
Lizhen	O
Liu	O
is	O
the	O
corresponding	O
author	O
.	O

Instead	O
of	O
treating	O
attention	O
as	O
a	O
by	O
-	O
product	O
of	O
model	O
training	O
,	O
the	O
following	O
work	O
explored	O
how	O
machine	O
/	O
human	O
can	O
consume	O
attention	O
for	O
model	O
improvement	O
or	O
explanation	O
,	O
respectively	O
.	O
Machine	O
/	O
human	O
may	O
also	O
provide	O
supervision	O
.	O
We	O
thus	O
categorize	O
existing	O
work	O
by	O
machine	O
/	O
human	O
consumption	O
and	O
supervision	O
.	O
Our	O
work	O
falls	O
into	O
human	O
providing	O
supervision	O
(	O
with	O
machine	O
augmenting	O
supervision	O
)	O
for	O
machine	O
consumption	O
.	O

Overview	O
of	O
Open	O
-	O
Source	O
Morphology	O
Development	O
for	O
the	O
Komi	O
-	O
Zyrian	O
Language	O
:	O
Past	O
and	O
Future	O

This	O
study	O
describes	O
the	O
on	O
-	O
going	O
development	O
of	O
the	O
finite	O
-	O
state	O
description	O
for	O
an	O
endangered	O
minority	O
language	O
,	O
Komi	O
-	O
Zyrian	O
.	O
This	O
work	O
is	O
located	O
in	O
the	O
context	O
where	O
large	O
written	O
and	O
spoken	O
language	O
corpora	O
are	O
available	O
,	O
which	O
creates	O
a	O
set	O
of	O
unique	O
challenges	O
that	O
have	O
to	O
be	O
,	O
and	O
can	O
be	O
,	O
addressed	O
.	O
We	O
describe	O
how	O
we	O
have	O
designed	O
the	O
transducer	O
so	O
that	O
it	O
can	O
benefit	O
from	O
existing	O
open	O
-	O
source	O
infrastructures	O
and	O
therefore	O
be	O
as	O
reusable	O
as	O
possible	O
.	O
Тайӧ	O
гижӧдын	O
сёрни	O
мунӧ	O
канму	O
коми	O
кыв	O
технология	O
йылысь	O
,	O
кӧні	O
сетӧмаӧсь	O
коми	O
морфологиялы	O
помысь	O
-	O
помӧдз	O
автомат	O
.	O
Уджыс	O
сэтшӧм	O
контекстын	O
,	O
кӧні	O
ыджыд	O
гижан	O
да	O
сёрнисикас	O
корпусъяс	O
босьтанног	O
.	O
Та	O
вӧсна	O
чужӧны	O
торйӧн	O
юалӧмъяс	O
,	O
кодлы	O
выль	O
воча	O
кывъяс	O
коланаӧсь	O
.	O
Петкӧдлам	O
,	O
мый	O
эм	O
кыдзи	O
аддзыны	O
колана	O
воча	O
кывъяс	O
.	O
Серпасалам	O
анализаторавтоматлысь	O
сӧвмӧдӧм	O
процесс	O
да	O
вӧзйӧмным	O
ӧтлаӧдны	O
анализаторсӧ	O
паськыдджык	O
восся	O
кодъяса	O
ӧтувтечасӧинфраструктураӧ	O
,	O
медым	O
уджыс	O
уналаздоръясын	O
вӧдитчыны	O
.	O

This	O
study	O
discusses	O
open	O
-	O
source	O
morphology	O
development	O
,	O
which	O
has	O
greatly	O
benefited	O
from	O
opensource	O
projects	O
most	O
notably	O
achievements	O
attributed	O
to	O
the	O
GiellaLT	O
infrastructure	O
(	O
Moshagen	O
et	O
al	O
,	O
2014	O
)	O
,	O
i.e.	O
Giellatekno	O
&	O
Divvun	O
at	O
the	O
Norwegian	O
Arctic	O
University	O
in	O
Tromsø	O
,	O
Norway	O
.	O
Specifically	O
we	O
discuss	O
the	O
infrastructure	O
for	O
the	O
Komi	O
-	O
Zyrian	O
language	O
.	O
We	O
describe	O
the	O
work	O
done	O
up	O
until	O
now	O
,	O
and	O
delineate	O
some	O
of	O
the	O
tasks	O
we	O
deem	O
necessary	O
in	O
the	O
future	O
.	O
There	O
are	O
features	O
of	O
Komi	O
morphosyntax	O
that	O
need	O
special	O
attention	O
,	O
in	O
regard	O
to	O
both	O
of	O
their	O
linguistic	O
and	O
computational	O
descriptions	O
.	O
This	O
contribution	O
aims	O
to	O
bring	O
that	O
discussion	O
forward	O
,	O
and	O
delineate	O
the	O
current	O
status	O
of	O
the	O
work	O
.	O
Rueter	O
(	O
2000	O
)	O
describes	O
the	O
initial	O
creation	O
of	O
the	O
transducer	O
,	O
and	O
the	O
work	O
discussed	O
here	O
continues	O
that	O
same	O
undertaking	O
,	O
essentially	O
providing	O
an	O
update	O
of	O
the	O
changes	O
done	O
in	O
the	O
last	O
decade	O
,	O
and	O
a	O
plan	O
for	O
the	O
future	O
.	O
The	O
transducer	O
is	O
available	O
on	O
GitHub	O
for	O
Komi	O
-	O
Zyrian.¹	O
The	O
nightly	O
builds	O
are	O
available	O
through	O
a	O
Python	O
library	O
called	O
Ural	O
-	O
icNLP²	O
(	O
Hämäläinen	O
,	O
2019	O
)	O
.	O
Easy	O
and	O
efficient	O
access	O
to	O
the	O
traducers	O
and	O
their	O
lexical	O
materials	O
has	O
been	O
the	O
main	O
designing	O
principle	O
,	O
and	O
we	O
consider	O
current	O
approach	O
very	O
successful	O
.	O
Komi	O
-	O
Zyrian	O
has	O
a	O
growing	O
representation	O
in	O
online	O
corpora	O
.	O
There	O
is	O
a	O
large	O
written	O
corpus	O
that	O
is	O
accessible	O
online³	O
;	O
it	O
has	O
been	O
created	O
by	O
FU	O
-	O
Lab	O
in	O
Syktyvkar	O
.	O
The	O
Giellatekno	O
infrastructure	O
provides	O
a	O
Korp	O
implementation	O
(	O
Ahlberg	O
et	O
al	O
,	O
2013	O
)	O
hosting	O
numerous	O
Uralic	O
Wikipedia	O
corpora	O
,	O
among	O
which	O
Komi	O
can	O
also	O
be	O
found⁴.	O
At	O
the	O
Language	O
Bank	O
of	O
Finland	O
,	O
parallel	O
Bible	O
corpora	O
are	O
available	O
with	O
possibilities	O
for	O
comparing	O
different	O
translations	O
(	O
Rueter	O
and	O
Axelson	O
,	O
2020	O
)	O
.	O
While	O
literary	O
language	O
often	O
reflects	O
astute	O
professional	O
language	O
users	O
,	O
social	O
media	O
provides	O
written	O
language	O
that	O
may	O
be	O
more	O
closely	O
related	O
to	O
the	O
vernacular	O
,	O
this	O
type	O
of	O
Komi	O
is	O
found	O
with	O
minority	O
languages	O
of	O
the	O
adjacent	O
Volga	O
-	O
Kama	O
region⁵	O
and	O
as	O
described	O
in	O
Arkhangelskiy	O
(	O
2019	O
)	O
.	O
In	O
a	O
simi	O
-	O
lar	O
vein	O
,	O
a	O
Spoken	O
Komi	O
corpus	O
containing	O
mainly	O
Izhma	O
dialect	O
has	O
been	O
created	O
in	O
a	O
Kone	O
Foundation	O
funded	O
research	O
project	O
(	O
Blokland	O
et	O
al	O
,	O
2014	O
(	O
Blokland	O
et	O
al	O
,	O
-	O
2016	O
,	O
and	O
it	O
is	O
also	O
available	O
online	O
for	O
community	O
and	O
research	O
access.⁶	O
Written	O
and	O
spoken	O
language	O
corpora	O
are	O
different	O
in	O
many	O
ways	O
,	O
but	O
together	O
they	O
form	O
a	O
large	O
and	O
representative	O
description	O
of	O
the	O
Komi	O
language	O
.	O
Thereby	O
they	O
both	O
need	O
to	O
be	O
accounted	O
for	O
when	O
the	O
transducer	O
is	O
further	O
developed	O
.	O
Electronic	O
corpora	O
have	O
an	O
important	O
role	O
in	O
the	O
research	O
of	O
Komi	O
in	O
general	O
,	O
and	O
their	O
significance	O
most	O
certainly	O
will	O
only	O
grow	O
when	O
access	O
and	O
practices	O
improve	O
(	O
for	O
discussion	O
about	O
the	O
use	O
of	O
electronic	O
corpora	O
,	O
see	O
Федина	O
,	O
2019	O
;	O
Чупров	O
,	O
2018	O
;	O
Блокланд	O
et	O
al	O
,	O
2014	O
)	O
.	O
There	O
are	O
also	O
numerous	O
dialect	O
materials	O
in	O
Komi	O
,	O
and	O
their	O
progressing	O
digitization	O
gives	O
us	O
access	O
to	O
an	O
increasing	O
number	O
of	O
materials	O
hitherto	O
unavailable	O
in	O
digital	O
format	O
.	O
When	O
this	O
process	O
advances	O
and	O
we	O
inevitably	O
encounter	O
more	O
dialectal	O
texts	O
,	O
we	O
must	O
also	O
consider	O
wider	O
dialectal	O
features	O
of	O
Komi	O
when	O
we	O
develop	O
the	O
transducer	O
.	O
Additionally	O
,	O
as	O
there	O
are	O
two	O
main	O
Komi	O
varieties	O
with	O
written	O
standards	O
and	O
their	O
dialects	O
,	O
Zyrian	O
and	O
Permyak	O
,	O
we	O
must	O
acknowledge	O
that	O
infrastructures	O
for	O
these	O
languages	O
can	O
not	O
be	O
developed	O
in	O
isolation	O
,	O
but	O
rather	O
that	O
both	O
language	O
variants	O
must	O
be	O
taken	O
into	O
consideration	O
in	O
different	O
ways	O
(	O
Rueter	O
et	O
al	O
,	O
2020c	O
)	O
.	O
At	O
the	O
same	O
time	O
,	O
the	O
respective	O
written	O
standards	O
have	O
needs	O
for	O
their	O
own	O
tools	O
and	O
resources	O
that	O
are	O
still	O
independent	O
,	O
so	O
the	O
whole	O
question	O
of	O
how	O
to	O
best	O
handle	O
pluricentric	O
language	O
varieties	O
such	O
as	O
Komi	O
still	O
needs	O
additional	O
planning	O
.	O
The	O
study	O
is	O
structured	O
so	O
that	O
we	O
first	O
describe	O
the	O
work	O
that	O
has	O
been	O
done	O
for	O
modeling	O
the	O
morphosyntax	O
of	O
the	O
Komi	O
-	O
Zyrian	O
language	O
.	O
Then	O
we	O
discuss	O
individual	O
features	O
and	O
their	O
role	O
in	O
the	O
description	O
,	O
and	O
aim	O
to	O
illustrate	O
the	O
types	O
of	O
challenges	O
they	O
present	O
.	O
As	O
we	O
believe	O
that	O
computational	O
modeling	O
of	O
the	O
language	O
is	O
directly	O
connected	O
to	O
the	O
linguistic	O
description	O
itself	O
,	O
we	O
also	O
discuss	O
different	O
phenomena	O
and	O
the	O
ways	O
our	O
description	O
is	O
directly	O
connected	O
to	O
the	O
grammar	O
.	O

Komi	O
regular	O
morphology	O
affects	O
word	O
forms	O
in	O
several	O
parts	O
of	O
speech	O
.	O
In	O
addition	O
to	O
verbal	O
conjugation	O
and	O
nominal	O
declension	O
,	O
there	O
is	O
an	O
abundance	O
of	O
regular	O
morpheme	O
-	O
sememe	O
alignment	O
in	O
derivation	O
.	O
Whereas	O
verbal	O
conjugation	O
is	O
,	O
indeed	O
,	O
limited	O
to	O
the	O
indicative	O
(	O
in	O
four	O
synthetic	O
tenses	O
)	O
and	O
imperative	O
moods	O
,	O
the	O
complex	O
noun	O
-	O
phrase	O
head	O
is	O
associated	O
with	O
the	O
categories	O
of	O
number	O
(	O
singular	O
and	O
plural	O
)	O
,	O
possessive	O
marking	O
for	O
three	O
persons	O
and	O
two	O
numbers	O
as	O
well	O
as	O
nearly	O
thirty	O
syntactic	O
entity	O
markers	O
or	O
cases	O
.	O
Regular	O
derivation	O
can	O
be	O
observed	O
in	O
aspect	O
,	O
mediopassive	O
and	O
causative	O
marking	O
of	O
verbs	O
,	O
as	O
well	O
as	O
comparative	O
and	O
diminutive	O
marking	O
of	O
nominals	O
.	O
There	O
is	O
a	O
plethora	O
of	O
single	O
-	O
syllable	O
nouns	O
and	O
derivational	O
suffixes	O
,	O
and	O
,	O
at	O
times	O
,	O
the	O
boundary	O
between	O
compounding	O
and	O
derivation	O
becomes	O
obscure	O
.	O

As	O
mentioned	O
above	O
,	O
there	O
are	O
nearly	O
thirty	O
syntactic	O
entity	O
markers	O
or	O
cases	O
associated	O
with	O
complex	O
noun	O
phrases	O
.	O
The	O
distinction	O
drawn	O
here	O
of	O
cases	O
versus	O
derivations	O
lies	O
in	O
the	O
complexity	O
of	O
the	O
noun	O
phrase	O
,	O
i.e.	O
compatibility	O
with	O
the	O
category	O
of	O
number	O
or	O
presence	O
of	O
modifiers	O
has	O
been	O
underlined	O
as	O
a	O
possible	O
boundary	O
(	O
see	O
Rueter	O
,	O
2010	O
,	O
74	O
-	O
75	O
;	O
cf	O
.	O
Ylikoski	O
(	O
2020	O
)	O
)	O
.	O
If	O
a	O
denominal	O
adverbial	O
derivation	O
does	O
not	O
take	O
adjectival	O
or	O
determiner	O
modifiers	O
,	O
there	O
is	O
no	O
syntactic	O
need	O
to	O
distinguish	O
it	O
from	O
other	O
opaque	O
adverbials	O
.	O
On	O
the	O
contrary	O
,	O
it	O
may	O
be	O
noted	O
,	O
syntactic	O
elements	O
that	O
can	O
take	O
this	O
kind	O
of	O
modifiers	O
should	O
be	O
classified	O
according	O
to	O
their	O
syntactic	O
merits	O
.	O
(	O
The	O
term	O
CASE	O
should	O
not	O
be	O
reguarded	O
as	O
a	O
title	O
of	O
estate	O
but	O
as	O
a	O
useful	O
indication	O
of	O
syntactic	O
class	O
membership	O
.	O
)	O
Here	O
,	O
we	O
will	O
further	O
note	O
that	O
according	O
to	O
the	O
SIL	O
Glossary	O
of	O
Linguistic	O
Terms⁸	O
case	O
is	O
defined	O
as	O
a	O
grammatical	O
category	O
determined	O
by	O
the	O
syntactic	O
or	O
semantic	O
function	O
of	O
a	O
noun	O
or	O
pronoun	O
.	O
If	O
we	O
apply	O
this	O
to	O
a	O
regular	O
morphological	O
description	O
of	O
the	O
Komi	O
languages	O
,	O
we	O
may	O
choose	O
to	O
distinguish	O
between	O
derivational	O
endings	O
applied	O
to	O
simple	O
NP	O
heads	O
and	O
inflectional	O
endings	O
applied	O
to	O
complex	O
NP	O
heads	O
.	O
By	O
distinguishing	O
these	O
two	O
varieties	O
of	O
inflection	O
,	O
we	O
can	O
arrive	O
at	O
a	O
syntactic	O
criterion	O
for	O
classifying	O
different	O
types	O
of	O
inflection	O
,	O
whereas	O
the	O
complex	O
NP	O
,	O
which	O
also	O
takes	O
marking	O
for	O
number	O
,	O
might	O
be	O
readily	O
integrated	O
into	O
the	O
enumeration	O
of	O
nominal	O
modifiers	O
,	O
i.e.	O
cases	O
.	O
For	O
nearly	O
one	O
and	O
a	O
half	O
centuries	O
,	O
the	O
16	O
and	O
1	O
dependent	O
cases	O
as	O
defined	O
by	O
Castrén	O
(	O
1844	O
)	O
have	O
represented	O
the	O
canonical	O
cases	O
addressed	O
in	O
grammars	O
of	O
the	O
Komi	O
-	O
Zyrian	O
language	O
.	O
The	O
seventeenth	O
case	O
,	O
the	O
comitative	O
,	O
is	O
addressed	O
as	O
a	O
postposition	O
,	O
but	O
all	O
examples	O
of	O
it	O
show	O
it	O
as	O
integrated	O
morphology	O
in	O
the	O
noun	O
.	O
Некрасова	O
(	O
2000	O
)	O
(	O
'	O
The	O
Modern	O
Komi	O
Language	O
'	O
,	O
ÖKK	O
)	O
,	O
published	O
in	O
2000	O
broke	O
with	O
this	O
tradition	O
by	O
including	O
a	O
set	O
of	O
compounded	O
cases	O
(	O
seven	O
)	O
.	O
The	O
26	O
cases	O
shown	O
by	O
the	O
latest	O
Komi	O
grammar	O
,	O
may	O
be	O
further	O
augmented	O
to	O
29	O
by	O
introducing	O
the	O
PROPRIETIVE	O
,	O
ABESSIVE	O
and	O
LOCATIVE	O
cases	O
,	O
in	O
-	O
a	O
,	O
тӧм	O
and	O
-	O
са	O
,	O
respectively	O
.	O
The	O
TEMPORAL	O
in	O
-	O
ся	O
might	O
,	O
as	O
a	O
function	O
,	O
be	O
simply	O
attributed	O
to	O
the	O
already	O
existing	O
COMPARATIVE	O
case	O
.	O
Similar	O
questions	O
of	O
case	O
definition	O
have	O
been	O
treated	O
by	O
one	O
of	O
the	O
authors	O
,	O
Rueter	O
(	O
2010	O
)	O
,	O
where	O
he	O
regards	O
syntactic	O
entity	O
complexity	O
as	O
sufficient	O
grounds	O
for	O
casehood	O
,	O
(	O
see	O
also	O
Ylikoski	O
,	O
2020	O
)	O
.	O
Tauli	O
(	O
1956	O
)	O
,	O
it	O
should	O
be	O
noted	O
,	O
provides	O
numerous	O
references	O
to	O
researchers	O
dealing	O
with	O
affixes	O
,	O
inclusive	O
derivation	O
and	O
case	O
,	O
there	O
does	O
not	O
seem	O
to	O
be	O
any	O
standards	O
for	O
distinction	O
between	O
case	O
and	O
derivation	O
.	O
The	O
Komi	O
-	O
Zyrian	O
PROPRIETIVE	O
refered	O
to	O
also	O
as	O
a	O
nomen	O
possessoris	O
suffix	O
а	O
,	O
which	O
occurs	O
as	O
a	O
"	O
comitative	O
"	O
(	O
Tauli	O
,	O
1956	O
)	O
,	O
provides	O
a	O
challenge	O
for	O
the	O
those	O
wishing	O
to	O
distinguish	O
Kom	O
proprietive	O
-	O
а	O
,	O
comitative	O
-	O
кӧд	O
and	O
instrumental	O
-	O
ӧн	O
.	O
Not	O
unlike	O
the	O
PROPRIETIVE	O
,	O
the	O
ABESSIVE	O
,	O
LOCA	O
-	O
TIVE	O
and	O
even	O
the	O
temporal	O
function	O
of	O
the	O
COMPAR	O
-	O
ATIVE	O
case	O
are	O
almost	O
entirely	O
limited	O
in	O
use	O
to	O
the	O
adnominal	O
range	O
.	O
The	O
ABESSIVE	O
has	O
a	O
predicative	O
counterpart	O
in	O
the	O
CARATIVE	O
-	O
тӧг	O
,	O
while	O
the	O
LOCA	O
-	O
TIVE	O
has	O
a	O
predicative	O
counterpart	O
in	O
the	O
INESSIVEын	O
.	O
Perhaps	O
this	O
range	O
distinction	O
has	O
also	O
played	O
⁸https://glossary.sil.org/term/case	O
a	O
part	O
so	O
-	O
called	O
case	O
classification	O
.	O
The	O
adnominal	O
TEMPORAL	O
marker	O
,	O
however	O
,	O
seems	O
to	O
have	O
no	O
morphological	O
counterpart	O
for	O
use	O
in	O
the	O
predicative	O
.	O

One	O
of	O
the	O
dilemmas	O
in	O
Komi	O
morphosyntax	O
is	O
where	O
to	O
introduce	O
the	O
object	O
of	O
a	O
sentence	O
.	O
Actual	O
non	O
-	O
ambiguous	O
accusative	O
forms	O
are	O
attested	O
for	O
pronouns	O
and	O
other	O
NP	O
heads	O
,	O
but	O
the	O
accusative	O
is	O
not	O
the	O
only	O
case	O
used	O
for	O
indicating	O
the	O
object	O
,	O
the	O
ZERO	O
marker	O
strategy	O
is	O
also	O
used	O
for	O
this	O
purpose	O
.	O
Hence	O
,	O
one	O
might	O
readily	O
speak	O
of	O
object	O
marking	O
with	O
the	O
nominative	O
.	O
Canonic	O
practice	O
in	O
the	O
Komi	O
grammaticography	O
has	O
been	O
to	O
include	O
the	O
nominative	O
,	O
ZERO	O
form	O
,	O
as	O
an	O
additional	O
accusative	O
case	O
form	O
.	O
If	O
we	O
introduce	O
ZERO	O
as	O
an	O
accusative	O
case	O
marker	O
as	O
well	O
,	O
we	O
,	O
essentially	O
,	O
be	O
introducing	O
ambiguity	O
on	O
the	O
text	O
on	O
the	O
analysis	O
level	O
.	O
Komi	O
is	O
known	O
for	O
its	O
use	O
of	O
singular	O
possessive	O
suffixes	O
in	O
the	O
accusative	O
for	O
marking	O
different	O
degrees	O
of	O
identifiability	O
;	O
zero	O
,	O
i.e.	O
nominative	O
marking	O
,	O
is	O
also	O
a	O
possibility	O
.	O
When	O
we	O
also	O
have	O
the	O
full	O
syntactic	O
dependency	O
tree	O
,	O
the	O
ambiguity	O
between	O
nominatives	O
and	O
unmarked	O
accusative	O
is	O
resolved	O
,	O
as	O
the	O
object	O
relation	O
is	O
unambiguously	O
marked	O
and	O
connected	O
to	O
the	O
root	O
verb	O
.	O
The	O
current	O
solution	O
in	O
the	O
morphological	O
modeling	O
has	O
been	O
to	O
resolve	O
all	O
unmarked	O
wordforms	O
as	O
nominatives	O
,	O
and	O
to	O
leave	O
the	O
nominative	O
-	O
accusative	O
distinction	O
into	O
a	O
later	O
step	O
of	O
the	O
analysis	O
.	O
None	O
the	O
less	O
,	O
we	O
recognize	O
this	O
is	O
only	O
one	O
of	O
the	O
various	O
ways	O
this	O
can	O
be	O
analysed	O
,	O
and	O
when	O
the	O
full	O
analysis	O
comes	O
,	O
we	O
essentially	O
have	O
all	O
the	O
information	O
to	O
transform	O
the	O
material	O
to	O
match	O
various	O
existing	O
traditions	O
.	O

This	O
section	O
will	O
investigate	O
the	O
ordering	O
of	O
morphological	O
constituents	O
typically	O
associated	O
with	O
nominals	O
and	O
convey	O
meaning	O
associated	O
with	O
the	O
categories	O
of	O
number	O
,	O
possession	O
and	O
case	O
.	O
In	O
initial	O
collaboration	O
with	O
FU	O
-	O
Lab	O
,	O
a	O
singular	O
set	O
of	O
morpheme	O
ordering	O
was	O
adopted	O
for	O
each	O
individual	O
combination	O
of	O
possessor	O
&	O
case	O
marking	O
.	O
Hence	O
,	O
it	O
was	O
determined	O
that	O
the	O
word	O
form	O
батьӧйлӧн	O
«	O
бать	O
-	O
ӧй	O
-	O
лӧн	O
'	O
father	O
.	O
N	O
-	O
PxSg1	O
-	O
Gen	O
'	O
featuring	O
the	O
ӧй	O
marking	O
for	O
the	O
first	O
person	O
singular	O
possessor	O
could	O
be	O
distinguished	O
from	O
the	O
possessive	O
suffix	O
ым	O
in	O
гортӧдзым	O
«	O
горт	O
-	O
ӧдз	O
-	O
ым	O
'	O
home	O
.	O
N	O
-	O
Ter	O
-	O
PxSg1	O
'	O
on	O
the	O
basis	O
of	O
complementary	O
distribution	O
,	O
i.e.	O
there	O
was	O
no	O
need	O
to	O
label	O
the	O
possessive	O
suffixes	O
as	O
separate	O
entities	O
.	O
In	O
later	O
development	O
,	O
however	O
,	O
a	O
different	O
issue	O
was	O
observed	O
in	O
which	O
case	O
and	O
possessive	O
formatives	O
might	O
show	O
varied	O
ordering	O
.	O
Although	O
this	O
phenomenon	O
is	O
not	O
as	O
prevalent	O
as	O
in	O
the	O
Meadow	O
and	O
Eastern	O
Mari	O
language	O
(	O
cf	O
.	O
Luutonen	O
(	O
1997	O
)	O
)	O
,	O
it	O
did	O
merit	O
recognition	O
and	O
distinction	O
for	O
the	O
facilitation	O
of	O
further	O
resource	O
.	O
The	O
distinguishing	O
tags	O
strategy	O
implemented	O
for	O
Meadow	O
Mari	O
and	O
Hill	O
Mari	O
has	O
been	O
adapted	O
for	O
use	O
with	O
Komi	O
-	O
Zyrian	O
with	O
two	O
tags	O
.	O
One	O
tag	O
indicates	O
segment	O
ordering	O
where	O
the	O
possessive	O
marker	O
precedes	O
the	O
case	O
marker	O
(	O
+	O
So	O
/	O
PC	O
)	O
,	O
and	O
the	O
other	O
indicates	O
the	O
case	O
marker	O
precedes	O
the	O
possessive	O
marker	O
(	O
+	O
So	O
/	O
CP	O
)	O
,	O
e.g.	O
кӧзяиныслань	O
кӧзяиныс	O
-	O
лань	O
'	O
owner	O
.	O
N	O
-	O
PxSg3	O
-	O
Apr	O
'	O
кӧзяинланьыс	O
кӧзяин	O
-	O
лань	O
-	O
ыс	O
'	O
owner	O
.	O
N	O
-	O
Apr	O
-	O
PxSg3	O
'	O
.	O
In	O
addition	O
to	O
this	O
relatively	O
infrequent	O
type	O
of	O
ordering	O
variation	O
of	O
cases	O
versus	O
possessive	O
suffixes	O
,	O
there	O
also	O
appears	O
to	O
be	O
use	O
of	O
the	O
accusative	O
possessive	O
suffix	O
markers	O
for	O
second	O
-	O
тӧ	O
and	O
third	O
-	O
сӧ	O
person	O
on	O
noun	O
and	O
adjective	O
phrase	O
heads	O
,	O
where	O
the	O
accusative	O
case	O
would	O
not	O
be	O
syntactically	O
compatible	O
.	O
In	O
fact	O
,	O
these	O
same	O
endings	O
are	O
found	O
in	O
connection	O
with	O
other	O
parts	O
of	O
speech	O
as	O
well	O
.	O
It	O
has	O
been	O
maintained	O
that	O
these	O
morphological	O
constituents	O
convey	O
discourse	O
meaning	O
,	O
but	O
there	O
is	O
still	O
much	O
to	O
investigate	O
and	O
establishing	O
tagging	O
practices	O
for	O
these	O
features	O
will	O
contribute	O
to	O
better	O
research	O
materials	O
in	O
the	O
future	O
.	O

In	O
Komi	O
,	O
numerals	O
are	O
regularly	O
derived	O
to	O
form	O
subgroups	O
in	O
cardinals	O
ZERO	O
,	O
ordinals	O
-	O
ӧд	O
,	O
distributives	O
-	O
ӧн	O
,	O
iteratives	O
-	O
ысь	O
,	O
ordinal	O
iteratives	O
-	O
ӧдысь	O
and	O
distributional	O
iteratives	O
-	O
ысьӧн	O
(	O
Rueter	O
et	O
al	O
,	O
2020c	O
)	O
.	O
As	O
such	O
,	O
it	O
is	O
often	O
novel	O
or	O
even	O
confounding	O
that	O
we	O
find	O
the	O
syntactic	O
adverbial	O
role	O
found	O
across	O
languages	O
is	O
attributed	O
to	O
a	O
regularly	O
derived	O
adverb	O
кыкысь	O
'	O
twice	O
'	O
on	O
the	O
Komi	O
side	O
,	O
on	O
the	O
one	O
hand	O
,	O
and	O
a	O
noun	O
phrase	O
fifty	O
times	O
'	O
ветымынысь	O
'	O
,	O
on	O
the	O
other	O
.	O
Like	O
other	O
adnominal	O
modifiers	O
,	O
it	O
should	O
be	O
noted	O
,	O
numerals	O
may	O
also	O
be	O
promoted	O
to	O
NP	O
head	O
position	O
in	O
instances	O
of	O
contextually	O
motivated	O
ellipsis	O
.	O

We	O
have	O
recently	O
moved	O
into	O
primarily	O
data	O
-	O
driven	O
development	O
practice	O
for	O
Komi	O
,	O
where	O
new	O
lexicon	O
and	O
morphology	O
is	O
described	O
primarily	O
based	O
on	O
gaps	O
we	O
find	O
through	O
analysed	O
language	O
materi	O
-	O
als	O
.	O
At	O
the	O
same	O
time	O
we	O
have	O
developed	O
further	O
tests	O
to	O
check	O
the	O
validity	O
of	O
the	O
output	O
,	O
and	O
in	O
the	O
long	O
term	O
these	O
approaches	O
naturally	O
will	O
live	O
on	O
in	O
parallel	O
.	O
Needless	O
to	O
say	O
,	O
using	O
more	O
natural	O
texts	O
has	O
also	O
forced	O
us	O
to	O
take	O
into	O
account	O
more	O
spoken	O
language	O
and	O
dialect	O
phenomena	O
,	O
which	O
moves	O
the	O
work	O
into	O
quite	O
new	O
directions	O
,	O
which	O
we	O
have	O
already	O
discussed	O
partially	O
above	O
.	O
After	O
reporting	O
our	O
experiments	O
with	O
the	O
written	O
corpus	O
data	O
,	O
we	O
discuss	O
our	O
plan	O
to	O
integrate	O
the	O
dialectal	O
materials	O
and	O
tags	O
better	O
to	O
the	O
currently	O
discussed	O
Komi	O
analyser	O
.	O

From	O
a	O
corpus	O
of	O
1	O
,	O
415	O
,	O
210	O
unique	O
word	O
forms	O
(	O
2020	O
-	O
11	O
-	O
11	O
)	O
520	O
,	O
180	O
were	O
not	O
recognized	O
by	O
the	O
analyzer	O
.	O
Aside	O
from	O
the	O
Russian	O
words	O
,	O
apparently	O
from	O
quoted	O
text	O
,	O
and	O
words	O
written	O
entirely	O
in	O
upper	O
case	O
,	O
the	O
most	O
frequent	O
words	O
not	O
to	O
be	O
recognized	O
by	O
the	O
FST	O
seem	O
to	O
all	O
involve	O
hyphens	O
.	O
The	O
use	O
of	O
hyphenation	O
is	O
best	O
illiustrated	O
by	O
Рытыв	O
-	O
Войвыв	O
the	O
preposed	O
modifier	O
for	O
direction	O
'	O
north	O
northwest	O
'	O
(	O
1377	O
times	O
)	O
,	O
a	O
drawn	O
out	O
pronunciation	O
Но	O
-	O
о	O
'	O
Well	O
-	O
l	O
'	O
(	O
1177	O
times	O
)	O
,	O
and	O
the	O
orthographic	O
practice	O
of	O
adding	O
-	O
мӧд	O
'	O
another	O
'	O
in	O
здук	O
-	O
мӧд	O
'	O
yet	O
another	O
moment	O
'	O
(	O
942	O
times	O
)	O
.	O
Since	O
over	O
a	O
third	O
of	O
the	O
unique	O
word	O
forms	O
had	O
gone	O
unrecognized	O
,	O
a	O
strategy	O
was	O
developed	O
for	O
improving	O
the	O
model	O
.	O
This	O
would	O
be	O
carried	O
out	O
for	O
nominals	O
initially	O
and	O
subsequently	O
verbs	O
.	O
As	O
described	O
below	O
,	O
a	O
very	O
large	O
portion	O
of	O
unrecognized	O
forms	O
involved	O
various	O
plurals	O
.	O
How	O
they	O
were	O
dealt	O
with	O
is	O
described	O
below	O
,	O
as	O
it	O
illustrates	O
well	O
the	O
challenges	O
we	O
have	O
encountered	O
and	O
their	O
possible	O
solutions	O
.	O
In	O
the	O
Komi	O
-	O
Zyrian	O
morphology	O
there	O
are	O
two	O
separate	O
plural	O
markers	O
associated	O
with	O
nominal	O
declension	O
.	O
One	O
is	O
the	O
NP	O
plural	O
marker	O
яс	O
and	O
the	O
other	O
is	O
the	O
copula	O
complement	O
plural	O
marker	O
ӧсь	O
.	O
20604	O
unrecognized	O
word	O
forms	O
ended	O
in	O
яс	O
,	O
and	O
in	O
11441	O
of	O
these	O
the	O
plural	O
marker	O
was	O
preceded	O
by	O
a	O
Cyrillic	O
hard	O
sign	O
ъ	O
.	O
This	O
number	O
was	O
was	O
further	O
delimited	O
by	O
removing	O
all	O
instances	O
of	O
hyphenation	O
and	O
v	O
followed	O
by	O
Cyrillic	O
hard	O
sign	O
and	O
wordfinal	O
яс	O
.	O
Where	O
the	O
hyphen	O
may	O
have	O
meant	O
compound	O
words	O
for	O
simple	O
hyphenation	O
in	O
the	O
text	O
,	O
the	O
removal	O
of	O
v	O
meant	O
we	O
could	O
automatically	O
avoid	O
the	O
problem	O
of	O
determining	O
whether	O
the	O
word	O
stem	O
contained	O
the	O
notorious	O
l	O
/	O
v	O
variation	O
or	O
not	O
.	O
Our	O
resulting	O
figure	O
was	O
8766	O
.	O
After	O
entering	O
15	O
,	O
101	O
new	O
stems	O
the	O
number	O
of	O
unrecognized	O
unique	O
word	O
forms	O
dropped	O
to	O
422	O
,	O
227	O
,	O
which	O
was	O
nearly	O
a	O
nineteen	O
per	O
cent	O
improvement	O
over	O
the	O
previous	O
520	O
,	O
180	O
.	O
In	O
the	O
future	O
we	O
plan	O
to	O
go	O
further	O
through	O
the	O
frequency	O
list	O
of	O
unknown	O
word	O
forms	O
and	O
improve	O
the	O
analyzer	O
so	O
that	O
individual	O
yet	O
frequent	O
phenomena	O
is	O
adequately	O
described	O
and	O
addressed	O
.	O

Currently	O
the	O
FST	O
is	O
designed	O
so	O
that	O
dialectal	O
elements	O
are	O
recognized	O
,	O
but	O
they	O
come	O
with	O
an	O
additional	O
error	O
or	O
dialect	O
tag	O
which	O
prevents	O
them	O
being	O
suggested	O
in	O
tools	O
such	O
as	O
spellcheckers	O
.	O
We	O
have	O
also	O
experimented	O
with	O
approaches	O
where	O
Zyrian	O
,	O
Permyak	O
and	O
Russian	O
analysers	O
are	O
run	O
on	O
top	O
of	O
one	O
another	O
,	O
so	O
that	O
unknown	O
forms	O
may	O
be	O
captured	O
by	O
one	O
of	O
the	O
systems	O
with	O
appropriate	O
language	O
tags	O
returned	O
.	O
Since	O
some	O
Zyrian	O
dialectal	O
phenomena	O
is	O
also	O
present	O
in	O
Permyak	O
standard	O
language	O
,	O
already	O
this	O
solution	O
helps	O
to	O
improve	O
the	O
coverage	O
.	O
Eventually	O
,	O
however	O
,	O
we	O
consider	O
it	O
important	O
that	O
the	O
analyser	O
could	O
capture	O
nuances	O
of	O
individual	O
dialects	O
.	O
In	O
principle	O
this	O
could	O
be	O
accompanied	O
with	O
dialect	O
specific	O
tags	O
,	O
but	O
this	O
approach	O
is	O
also	O
problematic	O
.	O
Many	O
of	O
the	O
features	O
are	O
not	O
strictly	O
found	O
in	O
singular	O
dialects	O
,	O
but	O
cover	O
larger	O
regions	O
.	O
At	O
the	O
same	O
time	O
the	O
speech	O
of	O
any	O
individual	O
is	O
not	O
necessarily	O
limited	O
to	O
any	O
specific	O
variety	O
.	O
Moreover	O
,	O
we	O
believe	O
that	O
further	O
research	O
in	O
Komi	O
dialect	O
isoglosses	O
may	O
be	O
necessary	O
to	O
exactly	O
point	O
for	O
each	O
feature	O
where	O
they	O
definitely	O
occur	O
.	O
Some	O
rough	O
areal	O
boundaries	O
,	O
however	O
,	O
are	O
well	O
known	O
and	O
clear	O
cut	O
,	O
which	O
would	O
make	O
some	O
areal	O
tags	O
potentially	O
useful	O
.	O
Features	O
that	O
currently	O
are	O
not	O
included	O
are	O
especially	O
those	O
found	O
from	O
southern	O
and	O
eastern	O
Zyrian	O
dialects	O
,	O
mainly	O
because	O
nobody	O
has	O
attempted	O
to	O
use	O
an	O
FST	O
with	O
those	O
varieties	O
yet	O
.	O
We	O
must	O
also	O
recognize	O
that	O
Permyak	O
and	O
Zyrian	O
dialects	O
overlap	O
in	O
their	O
features	O
in	O
various	O
ways	O
,	O
and	O
especially	O
the	O
creation	O
of	O
infrastructure	O
that	O
handless	O
all	O
Komi	O
varieties	O
and	O
both	O
standards	O
remains	O
a	O
challenge	O
.	O

As	O
described	O
in	O
the	O
study	O
above	O
,	O
this	O
work	O
on	O
Komi	O
has	O
been	O
funded	O
by	O
Alfred	O
Kordelin	O
Foundation	O
and	O
Kone	O
Foundation	O
.	O
Matias	O
Grioni	O
,	O
Loïc	O
Grobol	O
,	O
Normunds	O
Grūzītis	O
,	O
Bruno	O
Guillaume	O
,	O
Céline	O
Guillot	O
-	O
Barbance	O
,	O
Tunga	O
Güngör	O
,	O
Nizar	O
Habash	O
,	O
Hinrik	O
Hafsteinsson	O
,	O
Jan	O
Hajič	O
,	O
Jan	O
Hajič	O
jr	O
.	O
,	O
Mika	O
Hämäläinen	O
,	O
Linh	O
Hà	O
Mỹ	O
,	O
Na	O
-	O
Rae	O
Han	O
,	O
Muhammad	O
Yudistira	O
Hanifmuti	O
,	O
Sam	O
Hardwick	O
,	O
Kim	O
Harris	O
,	O
Dag	O
Haug	O
,	O
Johannes	O
Heinecke	O
,	O
Oliver	O
Hellwig	O
,	O
Felix	O
Hennig	O
,	O
Barbora	O
Hladká	O
,	O
Jaroslava	O
Hlaváčová	O
,	O
Florinel	O
Hociung	O

SMARTies	O
:	O
Sentiment	O
Models	O
for	O
Arabic	O
Target	O
entities	O

We	O
use	O
the	O
Arabic	O
Opinion	O
Target	O
dataset	O
developed	O
by	O
Farra	O
et	O
al	O
(	O
2015	O
)	O
,	O
which	O
is	O
publicly	O
available	O
1	O
.	O
The	O
data	O
consists	O
of	O
1177	O
online	O
comments	O
posted	O
in	O
response	O
to	O
Aljazeera	O
Arabic	O
newspaper	O
articles	O
and	O
is	O
part	O
of	O
the	O
Qatar	O
Arabic	O
Language	O
Bank	O
(	O
QALB	O
)	O
corpus	O
(	O
Habash	O
et	O
al	O
,	O
2013	O
;	O
Zaghouani	O
et	O
al	O
,	O
2014	O
)	O
.	O
The	O
comments	O
are	O
1	O
-	O
3	O
sentences	O
long	O
with	O
an	O
average	O
length	O
of	O
51	O
words	O
.	O
They	O
were	O
selected	O
such	O
that	O
they	O
included	O
topics	O
from	O
three	O
domains	O
:	O
politics	O
,	O
culture	O
,	O
and	O
sports	O
.	O
Targets	O
are	O
always	O
noun	O
phrases	O
and	O
they	O
are	O
either	O
labeled	O
positive	O
if	O
a	O
positive	O
opinion	O
is	O
expressed	O
about	O
them	O
and	O
negative	O
if	O
a	O
negative	O
opinion	O
is	O
expressed	O
(	O
as	O
shown	O
in	O
Figure	O
1	O
)	O
.	O
Targets	O
were	O
identified	O
using	O
an	O
incremental	O
process	O
where	O
first	O
important	O
entities	O
were	O
identified	O
,	O
and	O
then	O
entities	O
agreed	O
to	O
be	O
neutral	O
were	O
discarded	O
(	O
the	O
annotation	O
does	O
not	O
distinguish	O
between	O
neutral	O
and	O
subjective	O
neutral	O
)	O
.	O
The	O
data	O
also	O
contains	O
ambiguous	O
or	O
'	O
undeter	O
-	O
1	O
www.cs.columbia.edu/~noura/Resources.html	O
The	O
dictator	O
is	O
destroying	O
his	O
country	O
T	O
T	O
O	O
O	O
O	O
O	O
N	O
N	O

This	O
model	O
predicts	O
a	O
sequence	O
of	O
labels	O
S	O
for	O
the	O
sequence	O
x	O
,	O
S	O
i	O
{	O
P	O
(	O
pos	O
)	O
,	O
N	O
(	O
neg	O
)	O
,	O
(	O
neutral	O
)	O
}	O
and	O
each	O
token	O
x	O
i	O
is	O
represented	O
by	O
a	O
feature	O
vector	O
:	O
(	O
f	O
is	O
,	O
E	O
i	O
)	O
;	O
E	O
i	O
{	O
T	O
,	O
O	O
}	O
Additionally	O
,	O
this	O
model	O
has	O
the	O
constraint	O
:	O
if	O
E	O
i	O
=	O
T	O
,	O
S	O
i	O
{	O
P	O
,	O
N	O
}	O
and	O
otherwise	O
S	O
i	O
=	O
The	O
last	O
constraint	O
indicating	O
that	O
sentiment	O
is	O
either	O
positive	O
or	O
negative	O
is	O
ensured	O
by	O
the	O
training	O
data	O
,	O
where	O
we	O
have	O
no	O
examples	O
of	O
target	O
tokens	O
having	O
neutral	O
sentiment	O
.	O
The	O
two	O
models	O
are	O
trained	O
independently	O
.	O
Thus	O
,	O
if	O
target	O
words	O
are	O
already	O
available	O
for	O
the	O
data	O
,	O
the	O
sentiment	O
model	O
can	O
be	O
run	O
without	O
training	O
or	O
running	O
the	O
target	O
model	O
.	O
Otherwise	O
,	O
the	O
sentiment	O
model	O
can	O
be	O
run	O
on	O
the	O
output	O
of	O
the	O
target	O
predictor	O
.	O
The	O
sentiment	O
model	O
uses	O
knowledge	O
of	O
whether	O
a	O
word	O
is	O
a	O
target	O
and	O
utilizes	O
context	O
from	O
neighboring	O
words	O
whereby	O
the	O
entire	O
sequence	O
is	O
optimized	O
to	O
predict	O
sentiment	O
polarities	O
for	O
the	O
targets	O
.	O
An	O
example	O
sequence	O
is	O
shown	O
in	O
Table	O
1	O
,	O
where	O
the	O
dictator	O
is	O
an	O
entity	O
target	O
towards	O
which	O
the	O
writer	O
implicitly	O
expresses	O
negative	O
sentiment	O
.	O
5	O
Arabic	O
Morphology	O
and	O
Linguistics	O

We	O
experiment	O
with	O
the	O
following	O
classifiers	O
:	O
Maximum	O
Entropy	O
Classifier	O
(	O
Berger	O
et	O
al	O
,	O
1996	O
)	O
,	O
Support	O
Vector	O
Machines	O
Classifier	O
(	O
Cortes	O
and	O
Vapnik	O
,	O
1995	O
)	O
,	O
Multilayer	O
perceptron	O
and	O
Voted	O
perceptron	O
neural	O
networks	O
(	O
Freund	O
and	O
Schapire	O
,	O
1999	O
)	O
and	O
with	O
Decision	O
/	O
regression	O
tree	O
learning	O
(	O
Breiman	O
et	O
al	O
,	O
1984	O
)	O
.	O
We	O
employ	O
the	O
following	O
two	O
frameworks	O
:	O
Brainy	O
(	O
Konkol	O
,	O
2014	O
)	O
and	O
Weka	O
(	O
Hall	O
et	O
al	O
,	O
2009	O
)	O
.	O

The	O
alignment	O
of	O
chunks	O
is	O
generated	O
by	O
the	O
binary	O
classification	O
of	O
all	O
possible	O
chunk	O
pairs	O
.	O
If	O
one	O
chunk	O
is	O
aligned	O
with	O
multiple	O
chunks	O
in	O
the	O
other	O
sentence	O
,	O
these	O
chunks	O
should	O
be	O
merged	O
into	O
one	O
chunk	O
.	O
Also	O
,	O
impossible	O
multiple	O
chunks	O
to	O
multiple	O
chunks	O
alignments	O
are	O
generated	O
in	O
some	O
cases	O
(	O
e.g.	O
two	O
chunks	O
from	O
the	O
first	O
sentence	O
belong	O
a	O
chunk	O
in	O
the	O
second	O
sentence	O
but	O
one	O
of	O
the	O
two	O
chunks	O
from	O
the	O
first	O
sentence	O
belong	O
also	O
to	O
a	O
different	O
chunk	O
in	O
the	O
second	O
sentence	O
)	O
.	O
These	O
cases	O
are	O
resolved	O
with	O
few	O
hand	O
crafted	O
rules	O
.	O

This	O
publication	O
was	O
supported	O
by	O
the	O
project	O
LO1506	O
of	O
the	O
Czech	O
Ministry	O
of	O
Education	O
,	O
Youth	O
and	O
Sports	O
and	O
by	O
Grant	O
No	O
.	O
SGS	O
-	O
2016	O
-	O
018	O
Data	O
and	O
Software	O
Engineering	O
for	O
Advanced	O
Applications	O
.	O
Computational	O
resources	O
were	O
provided	O
by	O
the	O
CESNET	O
LM2015042	O
and	O
the	O
CERIT	O
Scientific	O
Cloud	O
LM2015085	O
,	O
provided	O
under	O
the	O
programme	O
"	O
Projects	O
of	O
Large	O
Research	O
,	O
Development	O
,	O
and	O
Innovations	O
Infrastructures	O
"	O
.	O

We	O
investigate	O
a	O
new	O
commonsense	O
inference	O
task	O
:	O
given	O
an	O
event	O
described	O
in	O
a	O
short	O
free	O
-	O
form	O
text	O
(	O
"	O
X	O
drinks	O
coffee	O
in	O
the	O
morning	O
"	O
)	O
,	O
a	O
system	O
reasons	O
about	O
the	O
likely	O
intents	O
(	O
"	O
X	O
wants	O
to	O
stay	O
awake	O
"	O
)	O
and	O
reactions	O
(	O
"	O
X	O
feels	O
alert	O
"	O
)	O
of	O
the	O
event	O
's	O
participants	O
.	O
To	O
support	O
this	O
study	O
,	O
we	O
construct	O
a	O
new	O
crowdsourced	O
corpus	O
of	O
25	O
,	O
000	O
event	O
phrases	O
covering	O
a	O
diverse	O
range	O
of	O
everyday	O
events	O
and	O
situations	O
.	O
We	O
report	O
baseline	O
performance	O
on	O
this	O
task	O
,	O
demonstrating	O
that	O
neural	O
encoder	O
-	O
decoder	O
models	O
can	O
successfully	O
compose	O
embedding	O
representations	O
of	O
previously	O
unseen	O
events	O
and	O
reason	O
about	O
the	O
likely	O
intents	O
and	O
reactions	O
of	O
the	O
event	O
participants	O
.	O
In	O
addition	O
,	O
we	O
demonstrate	O
how	O
commonsense	O
inference	O
on	O
people	O
's	O
intents	O
and	O
reactions	O
can	O
help	O
unveil	O
the	O
implicit	O
gender	O
inequality	O
prevalent	O
in	O
modern	O
movie	O
scripts	O
.	O

Understanding	O
a	O
narrative	O
requires	O
commonsense	O
reasoning	O
about	O
the	O
mental	O
states	O
of	O
people	O
in	O
relation	O
to	O
events	O
.	O
For	O
example	O
,	O
if	O
"	O
Alex	O
is	O
dragging	O
his	O
feet	O
at	O
work	O
"	O
,	O
pragmatic	O
implications	O
about	O
Alex	O
's	O
intent	O
are	O
that	O
"	O
Alex	O
wants	O
to	O
avoid	O
doing	O
things	O
"	O
(	O
Figure	O
1	O
)	O
.	O
We	O
can	O
also	O
infer	O
that	O
Alex	O
's	O
emotional	O
reaction	O
might	O
be	O
feeling	O
"	O
lazy	O
"	O
or	O
"	O
bored	O
"	O
.	O
Furthermore	O
,	O
while	O
not	O
explicitly	O
mentioned	O
,	O
we	O
can	O
infer	O
that	O
people	O
other	O
than	O
Alex	O
are	O
affected	O
by	O
the	O
situation	O
,	O
and	O
these	O
people	O
are	O
likely	O
to	O
feel	O
"	O
frustrated	O
"	O
or	O
"	O
impatient	O
"	O
.	O
This	O
type	O
of	O
pragmatic	O
inference	O
can	O
potentially	O
be	O
useful	O
for	O
a	O
wide	O
range	O
of	O
NLP	O
applications	O
⇤	O
These	O
two	O
authors	O
contributed	O
equally	O
.	O

PersonX	O
reads	O
PersonY	O
's	O
diary	O
to	O
avoid	O
doing	O
things	O
lazy	O
,	O
bored	O
frustrated	O
,	O
impatient	O
to	O
impress	O
their	O
family	O
tired	O
,	O
a	O
sense	O
of	O
belonging	O
impressed	O
to	O
be	O
nosey	O
,	O
know	O
secrets	O
guilty	O
,	O
curious	O
angry	O
,	O
violated	O
,	O
betrayed	O
X	O
's	O
intent	O
X	O
's	O
reaction	O
Y	O
's	O
reaction	O
X	O
's	O
intent	O
X	O
's	O
reaction	O
Y	O
's	O
reaction	O
X	O
's	O
intent	O
X	O
's	O
reaction	O
Y	O
's	O
reaction	O
Figure	O
1	O
:	O
Examples	O
of	O
commonsense	O
inference	O
on	O
mental	O
states	O
of	O
event	O
participants	O
.	O
In	O
the	O
third	O
example	O
event	O
,	O
common	O
sense	O
tells	O
us	O
that	O
Y	O
is	O
likely	O
to	O
feel	O
betrayed	O
as	O
a	O
result	O
of	O
X	O
reading	O
their	O
diary	O
.	O
that	O
require	O
accurate	O
anticipation	O
of	O
people	O
's	O
intents	O
and	O
emotional	O
reactions	O
,	O
even	O
when	O
they	O
are	O
not	O
explicitly	O
mentioned	O
.	O
For	O
example	O
,	O
an	O
ideal	O
dialogue	O
system	O
should	O
react	O
in	O
empathetic	O
ways	O
by	O
reasoning	O
about	O
the	O
human	O
user	O
's	O
mental	O
state	O
based	O
on	O
the	O
events	O
the	O
user	O
has	O
experienced	O
,	O
without	O
the	O
user	O
explicitly	O
stating	O
how	O
they	O
are	O
feeling	O
.	O
Similarly	O
,	O
advertisement	O
systems	O
on	O
social	O
media	O
should	O
be	O
able	O
to	O
reason	O
about	O
the	O
emotional	O
reactions	O
of	O
people	O
after	O
events	O
such	O
as	O
mass	O
shootings	O
and	O
remove	O
ads	O
for	O
guns	O
which	O
might	O
increase	O
social	O
distress	O
(	O
Goel	O
and	O
Isaac	O
,	O
2016	O
)	O
.	O
Also	O
,	O
pragmatic	O
inference	O
is	O
a	O
necessary	O
step	O
toward	O
automatic	O
narrative	O
understanding	O
and	O
generation	O
(	O
Tomai	O
and	O
Forbus	O
,	O
2010	O
;	O
Ding	O
and	O
Riloff	O
,	O
2016	O
;	O
Ding	O
et	O
al	O
,	O
2017	O
)	O
.	O
However	O
,	O
this	O
type	O
of	O
social	O
commonsense	O
reasoning	O
goes	O
far	O
beyond	O
the	O
widely	O
studied	O
entailment	O
tasks	O
(	O
Bowman	O
et	O
al	O
,	O
2015	O
;	O
Dagan	O
et	O
al	O
,	O
2006	O
)	O
and	O
thus	O
falls	O
outside	O
the	O
scope	O
of	O
existing	O
benchmarks	O
.	O
In	O
this	O
paper	O
,	O
we	O
introduce	O
a	O
new	O
task	O
,	O
corpus	O
,	O
and	O
model	O
,	O
supporting	O
commonsense	O
inference	O
on	O
events	O
with	O
a	O
specific	O
focus	O
on	O
modeling	O
stereotypical	O
intents	O
and	O
reactions	O
of	O
people	O
,	O
described	O
in	O
short	O
free	O
-	O
form	O
text	O
.	O
Our	O
study	O
is	O
in	O
a	O
similar	O
spirit	O
to	O
recent	O
efforts	O
of	O
Ding	O
and	O
Riloff	O
(	O
2016	O
)	O
and	O
Zhang	O
et	O
al	O
(	O
2017	O
)	O
,	O
in	O
that	O
we	O
aim	O
to	O
model	O
aspects	O
of	O
commonsense	O
inference	O
via	O
natural	O
language	O
descriptions	O
.	O
Our	O
new	O
contributions	O
are	O
:	O
(	O
1	O
)	O
a	O
new	O
corpus	O
that	O
supports	O
commonsense	O
inference	O
about	O
people	O
's	O
intents	O
and	O
reactions	O
over	O
a	O
diverse	O
range	O
of	O
everyday	O
events	O
and	O
situations	O
,	O
(	O
2	O
)	O
inference	O
about	O
even	O
those	O
people	O
who	O
are	O
not	O
directly	O
mentioned	O
by	O
the	O
event	O
phrase	O
,	O
and	O
(	O
3	O
)	O
a	O
task	O
formulation	O
that	O
aims	O
to	O
generate	O
the	O
textual	O
descriptions	O
of	O
intents	O
and	O
reactions	O
,	O
instead	O
of	O
classifying	O
their	O
polarities	O
or	O
classifying	O
the	O
inference	O
relations	O
between	O
two	O
given	O
textual	O
descriptions	O
.	O
Our	O
work	O
establishes	O
baseline	O
performance	O
on	O
this	O
new	O
task	O
,	O
demonstrating	O
that	O
,	O
given	O
the	O
phrase	O
-	O
level	O
inference	O
dataset	O
,	O
neural	O
encoderdecoder	O
models	O
can	O
successfully	O
compose	O
phrasal	O
embeddings	O
for	O
previously	O
unseen	O
events	O
and	O
reason	O
about	O
the	O
mental	O
states	O
of	O
their	O
participants	O
.	O
Furthermore	O
,	O
in	O
order	O
to	O
showcase	O
the	O
practical	O
implications	O
of	O
commonsense	O
inference	O
on	O
events	O
and	O
people	O
's	O
mental	O
states	O
,	O
we	O
apply	O
our	O
model	O
to	O
modern	O
movie	O
scripts	O
,	O
which	O
provide	O
a	O
new	O
insight	O
into	O
the	O
gender	O
bias	O
in	O
modern	O
films	O
beyond	O
what	O
previous	O
studies	O
have	O
offered	O
(	O
England	O
et	O
al	O
,	O
2011	O
;	O
Agarwal	O
et	O
al	O
,	O
2015	O
;	O
Ramakrishna	O
et	O
al	O
,	O
2017	O
;	O
Sap	O
et	O
al	O
,	O
2017	O
)	O
.	O
The	O
resulting	O
corpus	O
includes	O
around	O
25	O
,	O
000	O
event	O
phrases	O
,	O
which	O
combine	O
automatically	O
extracted	O
phrases	O
from	O
stories	O
and	O
blogs	O
with	O
all	O
idiomatic	O
verb	O
phrases	O
listed	O
in	O
the	O
Wiktionary	O
.	O
Our	O
corpus	O
is	O
publicly	O
available	O
.	O
1	O

One	O
goal	O
of	O
our	O
investigation	O
is	O
to	O
probe	O
whether	O
it	O
is	O
feasible	O
to	O
build	O
computational	O
models	O
that	O
can	O
perform	O
limited	O
,	O
but	O
well	O
-	O
scoped	O
commonsense	O
inference	O
on	O
short	O
free	O
-	O
form	O
text	O
,	O
which	O
we	O
refer	O
to	O
as	O
event	O
phrases	O
.	O
While	O
there	O
has	O
been	O
much	O
prior	O
research	O
on	O
phrase	O
-	O
level	O
paraphrases	O
(	O
Pavlick	O
et	O
al	O
,	O
2015	O
)	O
and	O
phrase	O
-	O
level	O
entailment	O
(	O
Dagan	O
et	O
al	O
,	O
2006	O
)	O
,	O
relatively	O
little	O
prior	O
work	O
focused	O
on	O
phrase	O
-	O
level	O
inference	O
that	O
requires	O
prag	O
-	O
matic	O
or	O
commonsense	O
interpretation	O
.	O
We	O
scope	O
our	O
study	O
to	O
two	O
distinct	O
types	O
of	O
inference	O
:	O
given	O
a	O
phrase	O
that	O
describes	O
an	O
event	O
,	O
we	O
want	O
to	O
reason	O
about	O
the	O
likely	O
intents	O
and	O
emotional	O
reactions	O
of	O
people	O
who	O
caused	O
or	O
affected	O
by	O
the	O
event	O
.	O
This	O
complements	O
prior	O
work	O
on	O
more	O
general	O
commonsense	O
inference	O
(	O
Speer	O
and	O
Havasi	O
,	O
2012	O
;	O
Li	O
et	O
al	O
,	O
2016	O
;	O
Zhang	O
et	O
al	O
,	O
2017	O
)	O
,	O
by	O
focusing	O
on	O
the	O
causal	O
relations	O
between	O
events	O
and	O
people	O
's	O
mental	O
states	O
,	O
which	O
are	O
not	O
well	O
covered	O
by	O
most	O
existing	O
resources	O
.	O
We	O
collect	O
a	O
wide	O
range	O
of	O
phrasal	O
event	O
descriptions	O
from	O
stories	O
,	O
blogs	O
,	O
and	O
Wiktionary	O
idioms	O
.	O
Compared	O
to	O
prior	O
work	O
on	O
phrasal	O
embeddings	O
(	O
Wieting	O
et	O
al	O
,	O
2015	O
;	O
Pavlick	O
et	O
al	O
,	O
2015	O
)	O
,	O
our	O
work	O
generalizes	O
the	O
phrases	O
by	O
introducing	O
(	O
typed	O
)	O
variables	O
.	O
In	O
particular	O
,	O
we	O
replace	O
words	O
that	O
correspond	O
to	O
entity	O
mentions	O
or	O
pronouns	O
with	O
typed	O
variables	O
such	O
as	O
PersonX	O
or	O
PersonY	O
,	O
as	O
shown	O
in	O
examples	O
in	O
Table	O
1	O
.	O
More	O
formally	O
,	O
the	O
phrases	O
we	O
extract	O
are	O
a	O
combination	O
of	O
a	O
verb	O
predicate	O
with	O
partially	O
instantiated	O
arguments	O
.	O
We	O
keep	O
specific	O
arguments	O
together	O
with	O
the	O
predicate	O
,	O
if	O
they	O
appear	O
frequently	O
enough	O
(	O
e.g.	O
,	O
PersonX	O
eats	O
pasta	O
for	O
dinner	O
)	O
.	O
Otherwise	O
,	O
the	O
arguments	O
are	O
replaced	O
with	O
an	O
untyped	O
blank	O
(	O
e.g.	O
,	O
PersonX	O
eats	O
for	O
dinner	O
)	O
.	O
In	O
our	O
work	O
,	O
only	O
person	O
mentions	O
are	O
replaced	O
with	O
typed	O
variables	O
,	O
leaving	O
other	O
types	O
to	O
future	O
research	O
.	O

To	O
prune	O
the	O
set	O
of	O
events	O
that	O
will	O
be	O
annotated	O
for	O
intent	O
and	O
reaction	O
,	O
we	O
ran	O
a	O
preliminary	O
annotation	O
to	O
filter	O
out	O
candidate	O
events	O
that	O
have	O
implausible	O
coreferences	O
.	O
In	O
this	O
preliminary	O
task	O
,	O
annotators	O
were	O
shown	O
a	O
combinatorial	O
list	O
of	O
coreferences	O
for	O
an	O
event	O
(	O
e.g.	O
,	O
PersonX	O
punches	O
PersonX	O
's	O
lights	O
out	O
,	O
PersonX	O
punches	O
PersonY	O
's	O
lights	O
out	O
)	O
and	O
were	O
asked	O
to	O
select	O
only	O
the	O
plausible	O
ones	O
(	O
e.g.	O
,	O
PersonX	O
punches	O
PersonY	O
's	O
lights	O
out	O
)	O
.	O
Each	O
set	O
of	O
coreferences	O
was	O
annotated	O
by	O
3	O
workers	O
,	O
yielding	O
an	O
overall	O
agreement	O
of	O
	O
=	O
0.4	O
.	O
This	O
annotation	O
excluded	O
8	O
,	O
406	O
events	O
with	O
implausible	O
coreference	O
from	O
our	O
set	O
(	O
out	O
of	O
17	O
,	O
806	O
events	O
)	O
.	O

Table	O
3	O
summarizes	O
the	O
performance	O
of	O
different	O
encoding	O
models	O
on	O
the	O
dev	O
and	O
test	O
set	O
in	O
terms	O
of	O
cross	O
-	O
entropy	O
and	O
recall	O
at	O
10	O
predicted	O
intents	O
and	O
reactions	O
.	O
As	O
expected	O
,	O
we	O
see	O
a	O
moderate	O
improvement	O
in	O
recall	O
and	O
cross	O
-	O
entropy	O
when	O
using	O
the	O
more	O
compositional	O
encoder	O
models	O
(	O
Con	O
-	O
vNet	O
and	O
BiRNN	O
;	O
both	O
n	O
-	O
gram	O
and	O
sequence	O
de	O
-	O
Table	O
3	O
:	O
Average	O
cross	O
-	O
entropy	O
(	O
lower	O
is	O
better	O
)	O
and	O
recall	O
@10	O
(	O
percentage	O
of	O
times	O
the	O
gold	O
falls	O
within	O
the	O
top	O
10	O
decoded	O
;	O
higher	O
is	O
better	O
)	O
on	O
development	O
and	O
test	O
sets	O
for	O
different	O
modeling	O
variations	O
.	O
We	O
show	O
recall	O
values	O
for	O
PersonX	O
's	O
intent	O
,	O
PersonX	O
's	O
reaction	O
and	O
others	O
'	O
reaction	O
(	O
denoted	O
as	O
"	O
Intent	O
"	O
,	O
"	O
XReact	O
"	O
,	O
and	O
"	O
OReact	O
"	O
)	O
.	O
Note	O
that	O
because	O
of	O
two	O
different	O
decoding	O
setups	O
,	O
cross	O
-	O
entropy	O
between	O
n	O
-	O
gram	O
and	O
sequence	O
decoding	O
are	O
not	O
directly	O
comparable	O
.	O
coding	O
setups	O
)	O
.	O
Additionally	O
,	O
BiRNN	O
models	O
outperform	O
ConvNets	O
on	O
cross	O
-	O
entropy	O
in	O
both	O
decoding	O
setups	O
.	O
Looking	O
at	O
the	O
recall	O
split	O
across	O
intent	O
vs.	O
reaction	O
labels	O
(	O
"	O
Intent	O
"	O
,	O
"	O
XReact	O
"	O
and	O
"	O
OReact	O
"	O
columns	O
)	O
,	O
we	O
see	O
that	O
much	O
of	O
the	O
improvement	O
in	O
using	O
these	O
two	O
models	O
is	O
within	O
the	O
prediction	O
of	O
PersonX	O
's	O
intents	O
.	O
Note	O
that	O
recall	O
for	O
"	O
OReact	O
"	O
is	O
much	O
higher	O
,	O
since	O
a	O
majority	O
of	O
events	O
do	O
not	O
involve	O
other	O
people	O
.	O
Human	O
evaluation	O
To	O
further	O
assess	O
the	O
quality	O
of	O
our	O
models	O
,	O
we	O
randomly	O
select	O
100	O
events	O
from	O
our	O
test	O
set	O
and	O
ask	O
crowd	O
-	O
workers	O
to	O
rate	O
generated	O
intents	O
and	O
reactions	O
.	O
We	O
present	O
5	O
workers	O
with	O
an	O
event	O
's	O
top	O
10	O
most	O
likely	O
intents	O
and	O
reactions	O
according	O
to	O
our	O
model	O
and	O
ask	O
them	O
to	O
select	O
all	O
those	O
that	O
make	O
sense	O
to	O
them	O
.	O
We	O
evaluate	O
each	O
model	O
's	O
precision	O
@10	O
by	O
computing	O
the	O
average	O
number	O
of	O
generated	O
responses	O
that	O
make	O
sense	O
to	O
annotators	O
.	O
Figure	O
4	O
summarizes	O
the	O
results	O
of	O
this	O
evaluation	O
.	O
In	O
most	O
cases	O
,	O
the	O
performance	O
is	O
higher	O
for	O
the	O
sequential	O
decoder	O
than	O
the	O
corresponding	O
n	O
-	O
gram	O
decoder	O
.	O
The	O
biggest	O
gain	O
from	O
using	O
sequence	O
decoders	O
is	O
in	O
intent	O
prediction	O
,	O
possibly	O
because	O
intent	O
explanations	O
are	O
more	O
likely	O
to	O
be	O
longer	O
.	O
The	O
BiRNN	O
and	O
ConvNet	O
encoders	O
consistently	O
have	O
higher	O
precision	O
than	O
the	O
mean	O
-	O
pooling	O
with	O
the	O
BiRNN	O
-	O
seq	O
setup	O
slightly	O
outperforming	O
other	O
models	O
.	O
Unless	O
otherwise	O
specified	O
,	O
this	O
is	O
the	O
model	O
we	O
employ	O
in	O
further	O
sections	O
.	O
ilar	O
for	O
all	O
three	O
sets	O
of	O
events	O
,	O
it	O
is	O
10	O
%	O
behind	O
intent	O
prediction	O
on	O
the	O
full	O
development	O
set	O
.	O
Additionally	O
,	O
predicting	O
other	O
people	O
's	O
reactions	O
is	O
more	O
difficult	O
for	O
the	O
model	O
when	O
other	O
people	O
are	O
explicitly	O
mentioned	O
.	O
Unsurprisingly	O
,	O
idioms	O
are	O
particularly	O
difficult	O
for	O
commonsense	O
inference	O
,	O
perhaps	O
due	O
to	O
the	O
difficulty	O
in	O
composing	O
meaning	O
over	O
nonliteral	O
or	O
noncompositional	O
event	O
descriptions	O
.	O
To	O
further	O
evaluate	O
the	O
geometry	O
of	O
the	O
embedding	O
space	O
,	O
we	O
analyze	O
interpolations	O
between	O
pairs	O
of	O
event	O
phrases	O
(	O
from	O
outside	O
the	O
train	O
set	O
)	O
,	O
similar	O
to	O
the	O
homotopic	O
analysis	O
of	O
Bowman	O
et	O
al	O
(	O
2016	O
)	O
.	O
For	O
a	O
handful	O
of	O
event	O
pairs	O
,	O
we	O
decode	O
intents	O
,	O
reactions	O
for	O
PersonX	O
,	O
and	O
reactions	O
for	O
other	O
people	O
from	O
points	O
sampled	O
at	O
equal	O
inter	O
-	O
vals	O
on	O
the	O
interpolated	O
line	O
between	O
two	O
event	O
phrases	O
.	O
We	O
show	O
examples	O
in	O
Figure	O
5	O
.	O
The	O
embedding	O
space	O
distinguishes	O
changes	O
from	O
generally	O
positive	O
to	O
generally	O
negative	O
words	O
and	O
is	O
also	O
able	O
to	O
capture	O
small	O
differences	O
between	O
event	O
phrases	O
(	O
such	O
as	O
"	O
washes	O
"	O
versus	O
"	O
cuts	O
"	O
)	O
.	O

Our	O
approach	O
decodes	O
nuanced	O
implications	O
into	O
more	O
explicit	O
statements	O
,	O
helping	O
to	O
identify	O
and	O
explain	O
gender	O
bias	O
that	O
is	O
prevalent	O
in	O
modern	O
literature	O
and	O
media	O
.	O
Specifically	O
,	O
our	O
results	O
indicate	O
that	O
modern	O
movies	O
have	O
the	O
bias	O
to	O
portray	O
female	O
characters	O
as	O
having	O
pro	O
-	O
social	O
attitudes	O
,	O
whereas	O
male	O
characters	O
are	O
portrayed	O
as	O
being	O
competitive	O
or	O
pro	O
-	O
achievement	O
.	O
This	O
is	O
consistent	O
with	O
gender	O
stereotypes	O
that	O
have	O
been	O
studied	O
in	O
movies	O
in	O
both	O
NLP	O
and	O
psychology	O
literature	O
(	O
Agarwal	O
et	O
al	O
,	O
2015	O
;	O
Madaan	O
et	O
al	O
,	O
2017	O
;	O
Prentice	O
and	O
Carranza	O
,	O
2002	O
;	O
England	O
et	O
al	O
,	O
2011	O
)	O
.	O

We	O
introduced	O
a	O
new	O
corpus	O
,	O
task	O
,	O
and	O
model	O
for	O
performing	O
commonsense	O
inference	O
on	O
textuallydescribed	O
everyday	O
events	O
,	O
focusing	O
on	O
stereotypical	O
intents	O
and	O
reactions	O
of	O
people	O
involved	O
in	O
the	O
events	O
.	O
Our	O
corpus	O
supports	O
learning	O
representations	O
over	O
a	O
diverse	O
range	O
of	O
events	O
and	O
reasoning	O
about	O
the	O
likely	O
intents	O
and	O
reactions	O
of	O
previously	O
unseen	O
events	O
.	O
We	O
also	O
demonstrate	O
that	O
such	O
inference	O
can	O
help	O
reveal	O
implicit	O
gender	O
bias	O
in	O
movie	O
scripts	O
.	O

We	O
would	O
like	O
to	O
thank	O
all	O
of	O
the	O
anonymous	O
reviewers	O
(	O
during	O
ARR	O
Oct.	O
and	O
ARR	O
Dec.	O
)	O
for	O
the	O
helpful	O
comments	O
.	O
We	O
also	O
thank	O
Baosong	O
Yang	O
and	O
Dayiheng	O
Liu	O
for	O
their	O
instructive	O
suggestions	O
and	O
invaluable	O
help	O
.	O

Semantic	O
Content	O
Prediction	O
for	O
Generating	O
Interviewing	O
Dialogues	O
to	O
Elicit	O
Users	O
'	O
Food	O
Preferences	O

This	O
study	O
aims	O
to	O
generate	O
interview	O
dialogues	O
that	O
elicit	O
information	O
about	O
users	O
'	O
food	O
preferences	O
.	O
For	O
this	O
purpose	O
,	O
we	O
collected	O
role	O
-	O
play	O
conversations	O
between	O
an	O
interviewer	O
and	O
a	O
customer	O
and	O
constructed	O
a	O
corpus	O
from	O
the	O
collected	O
conversations	O
.	O

Subject	O
pairs	O
were	O
created	O
with	O
participants	O
recruited	O
by	O
crowdsourcing	O
.	O
One	O
subject	O
was	O
assigned	O
the	O
role	O
of	O
an	O
interviewer	O
and	O
the	O
other	O
,	O
the	O
role	O
of	O
a	O
customer	O
.	O
They	O
conducted	O
a	O
text	O
-	O
based	O
chat	O
session	O
in	O
Japanese	O
on	O
the	O
web	O
.	O
After	O
typing	O
an	O
utterance	O
and	O
pressing	O
the	O
send	O
button	O
,	O
the	O
message	O
was	O
added	O
to	O
the	O
chat	O
screen	O
.	O
They	O
were	O
also	O
instructed	O
to	O
take	O
turns	O
sending	O
the	O
messages	O
.	O
The	O
participants	O
playing	O
as	O
interviewers	O
were	O
requested	O
to	O
engage	O
in	O
conversations	O
to	O
elicit	O
food	O
preferences	O
from	O
customers	O
.	O
The	O
participants	O
playing	O
as	O
customers	O
were	O
asked	O
to	O
indicate	O
their	O
food	O
preferences	O
.	O
We	O
allowed	O
the	O
customers	O
to	O
respond	O
to	O
their	O
real	O
preferences	O
or	O
to	O
pretend	O
to	O
be	O
someone	O
else	O
.	O
After	O
the	O
dialogue	O
,	O
each	O
participant	O
answered	O
a	O
questionnaire	O
.	O
The	O
interviewers	O
were	O
asked	O
to	O
describe	O
the	O
client	O
's	O
food	O
preferences	O
obtained	O
from	O
the	O
conversation	O
,	O
and	O
the	O
dishes	O
they	O
would	O
like	O
to	O
recommend	O
to	O
the	O
customer	O
.	O
The	O
customers	O
were	O
asked	O
to	O
describe	O
the	O
food	O
preferences	O
they	O
expressed	O
in	O
the	O
dialogue	O
.	O
They	O
were	O
also	O
asked	O
to	O
describe	O
the	O
dishes	O
they	O
would	O
like	O
the	O
interviewer	O
to	O
recommend	O
to	O
them	O
.	O
To	O
create	O
a	O
dialogue	O
model	O
capable	O
of	O
generating	O
responses	O
that	O
considered	O
the	O
interviewer	O
's	O
dialogue	O
strategy	O
and	O
dialogue	O
history	O
,	O
we	O
requested	O
the	O
participants	O
to	O
input	O
at	O
least	O
20	O
turns	O
from	O
each	O
party	O
and	O
40	O
turns	O
in	O
total	O
.	O
This	O
was	O
a	O
task	O
completion	O
requirement	O
.	O

Structured	O
semantic	O
labels	O
were	O
assigned	O
to	O
classify	O
the	O
interviewees	O
'	O
utterances	O
and	O
understand	O
their	O
semantic	O
content	O
.	O
Following	O
the	O
idea	O
of	O
structured	O
semantic	O
labels	O
discussed	O
in	O
the	O
Dialogue	O
Act	O
annotation	O
(	O
Bunt	O
et	O
al	O
,	O
2012	O
)	O
,	O
we	O
represented	O
each	O
utterance	O
as	O
a	O
combination	O
of	O
communicative	O
function	O
and	O
semantic	O
content	O
.	O
More	O
specifically	O
,	O
a	O
dialog	O
consists	O
of	O
messages	O
sent	O
by	O
the	O
user	O
in	O
the	O
chat	O
,	O
and	O
one	O
message	O
may	O
include	O
multiple	O
sentences	O
.	O
We	O
annotated	O
each	O
sentence	O
in	O
interviewer	O
's	O
message	O
.	O
To	O
annotate	O
sentences	O
in	O
the	O
interviewer	O
's	O
message	O
in	O
our	O
corpus	O
collected	O
in	O
Section	O
3.1	O
,	O
we	O
first	O
defined	O
labels	O
for	O
communicative	O
function	O
and	O
semantic	O
content	O
.	O
Communicative	O
Function	O
:	O
We	O
defined	O
32	O
labels	O
for	O
the	O
communicative	O
functions	O
based	O
on	O
those	O
for	O
SWBD	O
-	O
DAMSL	O
(	O
Jurafsky	O
,	O
1997	O
)	O
and	O
Meguro	O
et	O
al	O
(	O
2014	O
)	O
.	O
We	O
used	O
SWBD	O
-	O
DAMSL	O
to	O
label	O
backward	O
utterances	O
,	O
including	O
understanding	O
,	O
answer	O
,	O
and	O
agreement	O
(	O
Appendix	O
A	O
)	O
.	O
For	O
self	O
-	O
disclosure	O
(	O
SD	O
)	O
and	O
questions	O
(	O
Q	O
)	O
,	O
we	O
used	O
labels	O
defined	O
in	O
the	O
Meguro	O
et	O
al	O
(	O
2014	O
)	O
as	O
references	O
and	O
added	O
new	O
labels	O
such	O
as	O
preferences	O
,	O
experiences	O
,	O
and	O
habits	O
.	O
For	O
the	O
preference	O
labels	O
,	O
we	O
added	O
the	O
polarity	O
:	O
positive	O
,	O
negative	O
,	O
and	O
neutral	O
.	O

The	O
semantic	O
content	O
expresses	O
the	O
meaning	O
of	O
a	O
sentence	O
,	O
whereas	O
the	O
communicative	O
function	O
specifies	O
the	O
intention	O
of	O
a	O
sentence	O
,	O
as	O
discussed	O
above	O
.	O
In	O
our	O
corpus	O
,	O
many	O
of	O
the	O
interviewer	O
's	O
questions	O
referred	O
to	O
the	O
name	O
of	O
the	O
dish	O
and	O
its	O
ingredients	O
,	O
tastes	O
,	O
recipes	O
,	O
and	O
how	O
to	O
eat	O
.	O
Based	O
on	O
this	O
observation	O
,	O
we	O
defined	O
semantic	O
content	O
as	O
a	O
combination	O
of	O
utterance	O
objects	O
(	O
e.g.	O
,	O
dishes	O
and	O
ingredients	O
)	O
and	O
their	O
attributes	O
(	O
e.g.	O
,	O
tastes	O
and	O
cooking	O
methods	O
)	O
.	O
Figure	O
2	O
shows	O
the	O
structure	O
of	O
the	O
semantic	O
content	O
and	O
list	O
of	O
values	O
for	O
<	O
verb	O
>	O
,	O
<	O
ObjectType	O
>	O
,	O
and	O
<	O
ObjectAttribute	O
>	O
.	O
Two	O
examples	O
of	O
semantic	O
content	O
were	O
assigned	O
to	O
an	O
interviewer	O
sentence	O
.	O
In	O
Example	O
A	O
"	O
I	O
ate	O
hot	O
curry	O
"	O
in	O
Figure	O
2	O
,	O
the	O
verb	O
is	O
"	O
eat	O
"	O
and	O
its	O
object	O
is	O
"	O
hot	O
curry	O
"	O
.	O
The	O
object	O
is	O
the	O
first	O
argument	O
(	O
argument_1	O
)	O
of	O
the	O
verb	O
:	O
eat	O
,	O
and	O
the	O
relationship	O
between	O
this	O
verb	O
and	O
the	O
object	O
is	O
expressed	O
as	O
a	O
verb	O
frame	O
.	O
verb	O
frame	O
:	O
<	O
verb	O
>	O
:	O
We	O
defined	O
five	O
verbs	O
that	O
are	O
frequently	O
used	O
in	O
conversations	O
regarding	O
food	O
.	O
They	O
consider	O
direct	O
objects	O
as	O
arguments	O
.	O
We	O
also	O
defined	O
negative	O
forms	O
for	O
them	O
by	O
adding	O
"	O
!	O
"	O
.	O
For	O
example	O
,	O
the	O
negative	O
form	O
for	O
"	O
like	O
"	O
is	O
"	O
!	O
like	O
.	O
"	O
In	O
addition	O
to	O
these	O
10	O
verbs	O
,	O
"	O
think	O
"	O
and	O
"	O
other	O
"	O
were	O
added	O
,	O
and	O
12	O
verbs	O
were	O
defined	O
in	O
total	O
.	O
object	O
-	O
features	O
:	O
We	O
defined	O
four	O
types	O
of	O
features	O
for	O
an	O
object	O
.	O
These	O
are	O
ObjectType	O
,	O
ObjectName	O
,	O
Ob	O
-	O
jectAttribute	O
,	O
and	O
AttributeValue	O
.	O
These	O
are	O
called	O
the	O
object	O
features	O
.	O
The	O
"	O
hot	O
curry	O
"	O
is	O
an	O
object	O
of	O
the	O
verb	O
'	O
eat	O
'	O
.	O
It	O
contains	O
a	O
set	O
of	O
features	O
:	O
ObjectType='Dish	O
'	O
,	O
Object	O
-	O
Name='curry	O
'	O
,	O
ObjectAttribute='taste	O
'	O
,	O
and	O
At	O
-	O
tributeValue='hot	O
'	O
.	O
We	O
simply	O
expressed	O
this	O
set	O
as	O
(	O
Dish	O
,	O
curry	O
,	O
taste	O
,	O
hot	O
)	O
.	O
Details	O
of	O
the	O
object	O
features	O
are	O
presented	O
below	O
.	O
<	O
ObjectType	O
>	O
:	O
We	O
defined	O
10	O
object	O
types	O
:	O
Dish	O
,	O
Ingredient	O
,	O
and	O
Drink	O
.	O
Each	O
name	O
begins	O
with	O
a	O
capital	O
letter	O
.	O
For	O
example	O
,	O
"	O
Dish	O
"	O
is	O
assigned	O
as	O
the	O
ObjectType	O
value	O
for	O
curry	O
,	O
"	O
Ingredient	O
"	O
for	O
carrot	O
,	O
and	O
"	O
Genre+Cuisine	O
"	O
for	O
Indian	O
food	O
.	O
<	O
ObjectName	O
>	O
:	O
This	O
feature	O
indicates	O
the	O
name	O
of	O
the	O
target	O
object	O
in	O
an	O
interviewer	O
's	O
sentence	O
.	O
<	O
ObjectAttribute	O
>	O
:	O
As	O
shown	O
in	O
Example	O
-	O
A	O
in	O
Figure	O
2	O
,	O
there	O
are	O
many	O
detailed	O
questions	O
and	O
utterances	O
about	O
the	O
target	O
object	O
,	O
such	O
as	O
the	O
taste	O
of	O
the	O
food	O
,	O
its	O
recipe	O
,	O
and	O
how	O
to	O
eat	O
it	O
.	O
We	O
believe	O
that	O
such	O
information	O
is	O
important	O
for	O
food	O
preferences	O
.	O
To	O
include	O
it	O
in	O
the	O
semantic	O
content	O
,	O
we	O
defined	O
the	O
attributes	O
of	O
objects	O
with	O
a	O
specific	O
ObjectType	O
.	O
The	O
values	O
of	O
these	O
attributes	O
are	O
described	O
later	O
in	O
this	O
study	O
.	O
<	O
AttributeValue	O
>	O
:	O
The	O
value	O
for	O
the	O
ObjectAttribute	O
is	O
specified	O
in	O
this	O
section	O
.	O
A	O
set	O
of	O
possible	O
values	O
is	O
not	O
defined	O
,	O
and	O
the	O
value	O
is	O
freely	O
specified	O
,	O
as	O
in	O
ObjectName	O
.	O
For	O
example	O
,	O
the	O
ObjectType	O
of	O
"	O
hot	O
curry	O
"	O
is	O
a	O
'	O
Dish	O
'	O
,	O
and	O
ObjectType='Dish	O
'	O
can	O
take	O
an	O
Objec	O
-	O
tAttribute	O
(	O
see	O
Figure	O
2	O
,	O
Allowed	O
to	O
take	O
<	O
Objec	O
-	O
tAttribute	O
>	O
?	O
:	O
Yes	O
)	O
.	O
Then	O
,	O
"	O
hot	O
"	O
belongs	O
to	O
"	O
taste	O
"	O
,	O
which	O
is	O
defined	O
as	O
an	O
ObjectAttribute	O
.	O
As	O
a	O
result	O
,	O
"	O
hot	O
curry	O
"	O
is	O
interpreted	O
as	O
an	O
object	O
feature	O
.	O
ObjectType='Dish	O
'	O
,	O
ObjectName='curry	O
'	O
,	O
Objec	O
-	O
tAttribute='taste'，AttributeValue='hot	O
'	O
.	O
When	O
the	O
interviewer	O
's	O
utterance	O
is	O
a	O
question	O
,	O
such	O
as	O
a	O
Yes	O
/	O
No	O
question	O
or	O
WH	O
question	O
,	O
the	O
object	O
of	O
the	O
question	O
is	O
indicated	O
as	O
a	O
'	O
?	O
'	O
.	O
For	O
example	O
,	O
in	O
the	O
WH	O
question	O
,	O
"	O
What	O
taste	O
of	O
curry	O
do	O
you	O
like	O
?	O
"	O
,	O
the	O
AttributeValue	O
for	O
ObjectAt	O
-	O
tribute='taste	O
'	O
is	O
the	O
target	O
of	O
this	O
question	O
.	O
In	O
this	O
case	O
,	O
the	O
semantic	O
content	O
is	O
described	O
as	O
[	O
like	O
,	O
[	O
(	O
Dish	O
,	O
curry	O
,	O
taste	O
,	O
?	O
)	O
]	O
]	O
.	O
For	O
a	O
Yes	O
/	O
No	O
question	O
,	O
where	O
(	O
default	O
)	O
values	O
are	O
already	O
assigned	O
,	O
the	O
features	O
are	O
described	O
as	O
ObjectName+	O
?	O
and	O
AttributeValue+	O
?	O
.	O
For	O
example	O
,	O
the	O
semantic	O
content	O
for	O
"	O
Do	O
you	O
like	O
curry	O
hot	O
?	O
"	O
is	O
described	O
as	O
[	O
like	O
,	O
[	O
(	O
Dish	O
,	O
curry	O
,	O
taste	O
,	O
hot	O
?	O
)	O
]	O
]	O
Some	O
sentences	O
,	O
such	O
as	O
"	O
Steak	O
is	O
good	O
"	O
(	O
Example	O
-	O
B	O
in	O
Figure	O
2	O
)	O
,	O
express	O
an	O
evaluation	O
of	O
the	O
target	O
object	O
.	O
In	O
such	O
a	O
case	O
,	O
"	O
think	O
"	O
is	O
assigned	O
to	O
(	O
<	O
verb	O
>	O
)	O
,	O
and	O
two	O
arguments	O
are	O
used	O
;	O
the	O
object	O
information	O
is	O
described	O
in	O
argument_1	O
and	O
the	O
evaluation	O
in	O
(	O
argument_2	O
)	O
.	O
In	O
this	O
example	O
,	O
ar	O
-	O
gument_2	O
describes	O
a	O
pair	O
of	O
values	O
:	O
"	O
Evaluation	O
"	O
and	O
the	O
(	O
<	O
EvaluationValue	O
>	O
)	O
denoting	O
the	O
value	O
of	O
the	O
evaluation	O
.	O
Thus	O
,	O
(	O
argument_2	O
)	O
is	O
[	O
Evaluation	O
,	O
good	O
]	O
.	O

With	O
the	O
goal	O
of	O
building	O
a	O
dialogue	O
system	O
that	O
generates	O
the	O
interviewer	O
's	O
appropriate	O
questions	O
to	O
acquire	O
the	O
customer	O
's	O
food	O
preferences	O
,	O
we	O
present	O
two	O
machine	O
learning	O
models	O
in	O
this	O
section	O
for	O
communicative	O
function	O
prediction	O
and	O
semantic	O
content	O
generation	O
.	O

Table	O
1	O
(	O
top	O
)	O
lists	O
the	O
details	O
of	O
the	O
corpus	O
collected	O
in	O
Section	O
3	O
.	O
Table	O
1	O
(	O
bottom	O
)	O
shows	O
the	O
number	O
of	O
instances	O
4	O
that	O
was	O
used	O
to	O
train	O
the	O
CFP	O
and	O
SCG	O
models	O
.	O
The	O
dataset	O
was	O
divided	O
into	O
train	O
/	O
valid	O
/	O
test	O
sets	O
at	O
a	O
ratio	O
of	O
7:1:2	O
.	O
Although	O
we	O
defined	O
32	O
communication	O
function	O
labels	O
in	O
the	O
original	O
dataset	O
,	O
many	O
of	O
them	O
were	O
not	O
frequently	O
observed	O
.	O
Thus	O
,	O
we	O
merged	O
the	O
labels	O
whose	O
frequency	O
was	O
lower	O
than	O
20	O
%	O
of	O
all	O
samples	O
and	O
used	O
the	O
seven	O
labels	O
listed	O
in	O
Table	O
2	O
in	O
this	O
experiment	O
.	O
We	O
calculated	O
the	O
inter	O
-	O
coder	O
reliability	O
using	O
three	O
dialogues	O
annotated	O
by	O
two	O
coders	O
.	O
For	O
the	O
seven	O
labels	O
of	O
communicative	O
function	O
,	O
Cohen	O
's	O
kappa	O
was	O
0.75	O
,	O
which	O
indicated	O
substantial	O
agreement	O
.	O
For	O
semantic	O
content	O
,	O
which	O
is	O
a	O
combination	O
of	O
verb	O
and	O
object	O
-	O
features	O
,	O
the	O
percentage	O
of	O
agreement	O
was	O
0.72	O
.	O
Because	O
we	O
achieved	O
a	O
sufficient	O
agreement	O
level	O
,	O
the	O
remaining	O
data	O
were	O
annotated	O
by	O
either	O
coder	O
.	O

In	O
this	O
section	O
,	O
we	O
present	O
examples	O
of	O
the	O
responses	O
generated	O
by	O
our	O
interview	O
system	O
.	O
We	O
first	O
describe	O
the	O
template	O
-	O
based	O
responsegeneration	O
mechanism	O
and	O
then	O
discuss	O
examples	O
of	O
interview	O
generation	O
.	O

As	O
shown	O
in	O
Figure	O
1	O
,	O
the	O
system	O
receives	O
outputs	O
from	O
the	O
SCG	O
and	O
CFP	O
models	O
and	O
generates	O
the	O
interviewer	O
's	O
responses	O
using	O
the	O
templatebased	O
generation	O
method	O
.	O
Suppose	O
that	O
the	O
outputs	O
from	O
the	O
two	O
prediction	O
models	O
are	O
as	O
follows	O
:	O
communicative	O
function	O
label	O
:	O
Q	O
-	O
Preference	O
-	O
Positive	O
semantic	O
content	O
:	O
like	O
[	O
SEP	O
]	O
Dish	O
[	O
SEP	O
]	O
pasta	O
[	O
SEP	O
]	O
type	O
-	O
of	O
[	O
SEP	O
]	O
?	O
By	O
referring	O
to	O
this	O
information	O
:	O
communicative	O
function='Q	O
-	O
Preference	O
-	O
Positive	O
'	O
,	O
verb='like	O
'	O
,	O
ObjectAttribute='type	O
-	O
of	O
'	O
,	O
and	O
AttributeValue=	O
'	O
?	O
'	O
,	O
the	O
system	O
selects	O
a	O
template	O
:	O
"	O
{	O
ObjectName	O
}	O
no	O
Shurui	O
de	O
Nani	O
ga	O
Sukidesuka	O
?	O
"	O
(	O
in	O
English	O
,	O
"	O
What	O
kind	O
of	O
{	O
ObjectName	O
}	O
do	O
you	O
like	O
?	O
"	O
)	O
.	O
Then	O
,	O
a	O
response	O
sentence	O
is	O
generated	O
by	O
replacing	O
{	O
Object	O
-	O
Name	O
}	O
with	O
the	O
value	O
'	O
pasta	O
'	O
.	O

This	O
work	O
was	O
supported	O
by	O
JST	O
Moonshot	O
R&D	O
Grant	O
Number	O
JPMJMS2011	O
and	O
JST	O
AIP	O
Trilateral	O
AI	O
Research	O
(	O
PANORAMA	O
project	O
,	O
grant	O
no	O
.	O
JPMJCR20G6	O
)	O
and	O
JSPS	O
KAKENHI	O
(	O
grant	O
numbers	O
JP19H01120	O
and	O
JP19H04159	O
)	O
.	O

Provide	O
own	O
information	O
and	O
opinions	O
about	O
food	O
.	O
SD	O
-	O
Fact&Experience	O
e.g.	O
,	O
I	O
ate	O
pasta	O
yesterday	O
.	O
We	O
defined	O
the	O
labels	O
with	O
reference	O
SWBD	O
-	O
DAMSL	O
(	O
Jurafsky	O
,	O
1997	O
)	O
and	O
Meguro	O
et	O
al	O
(	O
2014	O
)	O
's	O
dialogue	O
acts	O
.	O

Multitask	O
Learning	O
for	O
Emotionally	O
Analyzing	O
Sexual	O
Abuse	O
Disclosures	O

Analyzing	O
social	O
media	O
data	O
of	O
individuals	O
discussing	O
sexual	O
harassment	O
disclosures	O
and	O
exploitation	O
in	O
public	O
spheres	O
necessitates	O
the	O
need	O
to	O
safeguard	O
the	O
ethics	O
and	O
privacy	O
of	O
individuals	O
(	O
Tusinski	O
Berg	O
,	O
2019	O
)	O
.	O
We	O
address	O
these	O
:	O
Generalization	O
We	O
acknowledge	O
that	O
the	O
limitations	O
of	O
the	O
experiments	O
might	O
get	O
amplified	O
due	O
to	O
the	O
highly	O
subjective	O
nature	O
of	O
this	O
challenging	O
problem	O
.	O
Therefore	O
it	O
would	O
not	O
be	O
fair	O
to	O
conduct	O
a	O
population	O
-	O
centric	O
analysis	O
based	O
on	O
inferences	O
from	O
this	O
work	O
.	O
Confidentiality	O
Individual	O
consent	O
was	O
not	O
sought	O
from	O
social	O
media	O
users	O
as	O
the	O
data	O
was	O
publicly	O
available	O
.	O
Disclosure	O
of	O
sexual	O
harassment	O
information	O
on	O
public	O
forums	O
may	O
have	O
been	O
met	O
with	O
public	O
backlash	O
and	O
apathy	O
.	O
Therefore	O
the	O
social	O
reputation	O
of	O
the	O
accuser	O
and	O
the	O
accused	O
would	O
be	O
at	O
a	O
peril	O
(	O
McDonald	O
,	O
2019	O
)	O
.	O
Hence	O
,	O
the	O
authors	O
were	O
aware	O
not	O
to	O
make	O
any	O
automated	O
interventions	O
,	O
as	O
any	O
attempts	O
to	O
contact	O
individuals	O
could	O
be	O
seen	O
as	O
personally	O
intrusive	O
and	O
might	O
also	O
repeal	O
their	O
social	O
information	O
(	O
Fiesler	O
and	O
Proferes	O
,	O
2018	O
)	O
.	O
Bias	O
&	O
Discrimination	O
Social	O
support	O
discussions	O
on	O
social	O
media	O
platforms	O
gave	O
victims	O
the	O
liberty	O
to	O
describe	O
their	O
instances	O
of	O
sexual	O
exploitation	O
and	O
abuse	O
(	O
Manikonda	O
et	O
al	O
,	O
2018a	O
)	O
.	O
The	O
authors	O
are	O
aware	O
of	O
the	O
potential	O
inevitable	O
sampling	O
biases	O
that	O
may	O
be	O
present	O
in	O
the	O
data	O
.	O
Importance	O
has	O
to	O
be	O
placed	O
on	O
mitigating	O
the	O
bias	O
against	O
certain	O
minority	O
groups	O
,	O
which	O
might	O
get	O
amplified	O
due	O
to	O
the	O
sensitive	O
nature	O
of	O
social	O
discussions	O
(	O
Hellwig	O
and	O
Sinno	O
,	O
2017	O
)	O
.	O

Rajiv	O
Ratn	O
Shah	O
is	O
partly	O
supported	O
by	O
the	O
Infosys	O
Center	O
for	O
AI	O
and	O
Center	O
for	O
Design	O
and	O
New	O
Media	O
at	O
IIIT	O
Delhi	O
.	O

Overnight	O
uses	O
a	O
context	O
-	O
free	O
synchronous	O
grammar	O
to	O
generate	O
canonical	O
representations	O
for	O
the	O
logical	O
forms	O
.	O
As	O
can	O
be	O
seen	O
in	O
Fig	O
.	O
2	O
,	O
these	O
canonical	O
representations	O
resemble	O
natural	O
language	O
.	O

We	O
provide	O
training	O
details	O
and	O
hyperparameters	O
for	O
all	O
models	O
in	O
Appendix	O
A.	O
Below	O
,	O
we	O
briefly	O
explain	O
the	O
prompt	O
-	O
tuning	O
methodology	O
.	O

There	O
are	O
two	O
main	O
limitations	O
of	O
this	O
work	O
.	O
The	O
first	O
is	O
the	O
limited	O
analysis	O
of	O
the	O
learned	O
prompts	O
.	O
While	O
concurrent	O
work	O
has	O
shown	O
that	O
interpreting	O
prompts	O
is	O
a	O
difficult	O
task	O
,	O
it	O
is	O
still	O
an	O
important	O
consideration	O
and	O
left	O
for	O
future	O
work	O
(	O
Khashabi	O
et	O
al	O
,	O
2021	O
)	O
.	O
Secondly	O
,	O
training	O
prompts	O
on	O
meaning	O
representations	O
requires	O
substantially	O
more	O
compute	O
than	O
fine	O
-	O
tuning	O
.	O
This	O
may	O
exacerbate	O
inequalities	O
in	O
regions	O
where	O
access	O
to	O
data	O
and	O
compute	O
are	O
similarly	O
limited	O
(	O
Ahia	O
et	O
al	O
,	O
2021	O
)	O
.	O

For	O
completeness	O
,	O
we	O
provide	O
all	O
Overnight	O
results	O
in	O
Table	O
5	O
.	O

Sentiment	O
Tagging	O
with	O
Partial	O
Labels	O
using	O
Modular	O
Architectures	O

In	O
the	O
previous	O
section	O
we	O
described	O
a	O
way	O
of	O
infusing	O
information	O
from	O
other	O
modules	O
naively	O
by	O
simply	O
concatenating	O
them	O
.	O
But	O
intuitively	O
,	O
the	O
hidden	O
representation	O
from	O
the	O
decision	O
module	O
plays	O
an	O
important	O
role	O
as	O
it	O
is	O
directly	O
related	O
to	O
the	O
final	O
task	O
we	O
are	O
interested	O
in	O
.	O
To	O
effectively	O
use	O
the	O
information	O
from	O
other	O
modules	O
forming	O
sub	O
-	O
tasks	O
,	O
we	O
design	O
a	O
gating	O
mechanism	O
to	O
dynamically	O
control	O
the	O
amount	O
of	O
information	O
flowing	O
from	O
other	O
modules	O
by	O
infusing	O
the	O
expedient	O
part	O
while	O
excluding	O
the	O
irrelevant	O
part	O
,	O
as	O
shown	O
in	O
Figure	O
2c	O
.	O
This	O
gating	O
mechanism	O
uses	O
the	O
information	O
from	O
the	O
decision	O
module	O
to	O
guide	O
the	O
information	O
from	O
other	O
modules	O
,	O
thus	O
we	O
name	O
it	O
as	O
guided	O
gating	O
infusion	O
,	O
which	O
we	O
describe	O
formally	O
as	O
follows	O
:	O
I	O
seg	O
t	O
=	O
σ	O
(	O
W	O
1	O
h	O
t	O
+	O
b	O
1	O
)	O
⊗	O
(	O
W	O
seg	O
h	O
seg	O
t	O
+	O
b	O
seg	O
)	O
,	O
I	O
typ	O
t	O
=	O
σ	O
(	O
W	O
2	O
h	O
t	O
+	O
b	O
2	O
)	O
⊗	O
(	O
W	O
typ	O
h	O
typ	O
t	O
+	O
b	O
typ	O
)	O
,	O
S	O
t	O
=	O
W	O
[	O
h	O
t	O
;	O
I	O
seg	O
t	O
;	O
I	O
typ	O
t	O
]	O
+	O
b	O
,	O
where	O
σ	O
is	O
the	O
logistic	O
sigmoid	O
function	O
and	O
⊗	O
is	O
the	O
element	O
-	O
wise	O
multiplication	O
.	O
The	O
{	O
W	O
1	O
,	O
W	O
2	O
,	O
b	O
1	O
,	O
b	O
2	O
}	O
are	O
the	O
parameters	O
of	O
these	O
guided	O
gating	O
,	O
which	O
are	O
updated	O
during	O
the	O
training	O
to	O
maximize	O
the	O
overall	O
sequence	O
labeling	O
performance	O
.	O

We	O
used	O
the	O
Restaurants	O
dataset	O
provided	O
by	O
Se	O
-	O
mEval	O
2016	O
Task	O
5	O
subtask	O
1	O
,	O
consisting	O
of	O
opinion	O
target	O
(	O
aspect	O
)	O
expression	O
segmentation	O
,	O
aspect	O
classification	O
and	O
matching	O
sentiment	O
prediction	O
.	O
In	O
the	O
original	O
task	O
definition	O
,	O
the	O
three	O
tasks	O
were	O
designed	O
as	O
a	O
pipeline	O
,	O
and	O
assumed	O
gold	O
aspect	O
labels	O
when	O
predicting	O
the	O
matching	O
sentiment	O
labels	O
.	O
Instead	O
,	O
our	O
model	O
deals	O
with	O
the	O
challenging	O
end	O
-	O
to	O
-	O
end	O
setting	O
by	O
casting	O
the	O
problem	O
as	O
a	O
sequence	O
labeling	O
task	O
,	O
labeling	O
each	O
aspect	O
segment	O
with	O
the	O
aspect	O
label	O
and	O
sentiment	O
polarity	O
2	O
.	O

Our	O
modular	O
architecture	O
is	O
a	O
natural	O
fit	O
for	O
learning	O
with	O
partial	O
labels	O
.	O
Since	O
the	O
modular	O
architecture	O
decomposes	O
the	O
final	O
task	O
into	O
sub	O
-	O
tasks	O
,	O
the	O
absence	O
of	O
certain	O
partial	O
labels	O
is	O
permitted	O
.	O
In	O
this	O
case	O
,	O
only	O
the	O
module	O
corresponding	O
to	O
the	O
available	O
partial	O
labels	O
will	O
be	O
updated	O
while	O
the	O
other	O
parts	O
of	O
the	O
model	O
stay	O
fixed	O
.	O
This	O
property	O
can	O
be	O
exploited	O
to	O
reduce	O
the	O
supervision	O
effort	O
by	O
defining	O
semi	O
-	O
supervised	O
learning	O
protocols	O
that	O
use	O
partial	O
-	O
labels	O
when	O
the	O
full	O
labels	O
are	O
not	O
available	O
,	O
or	O
too	O
costly	O
to	O
annotate	O
.	O
E.g.	O
,	O
in	O
the	O
target	O
sentiment	O
task	O
,	O
segmentation	O
labels	O
are	O
significantly	O
easier	O
to	O
annotate	O
.	O
To	O
demonstrate	O
this	O
property	O
we	O
conducted	O
two	O
sets	O
of	O
experiments	O
.	O
The	O
first	O
investigates	O
how	O
the	O
decision	O
module	O
can	O
effectively	O
integrate	O
the	O
knowledge	O
independently	O
learned	O
by	O
sub	O
-	O
tasks	O
modules	O
using	O
different	O
partial	O
labels	O
.	O
We	O
quantify	O
this	O
ability	O
by	O
providing	O
varying	O
amounts	O
of	O
full	O
labels	O
to	O
support	O
the	O
integration	O
process	O
.	O
The	O
second	O
set	O
studies	O
the	O
traditional	O
semi	O
-	O
supervised	O
settings	O
,	O
where	O
we	O
have	O
a	O
handful	O
of	O
full	O
labels	O
,	O
but	O
we	O
have	O
a	O
larger	O
amount	O
of	O
partial	O
labels	O
.	O
Modular	O
Knowledge	O
Integration	O
The	O
modular	O
architecture	O
allows	O
us	O
to	O
train	O
each	O
model	O
using	O
data	O
obtained	O
separately	O
for	O
each	O
task	O
,	O
and	O
only	O
use	O
a	O
handful	O
of	O
examples	O
annotated	O
for	O
the	O
final	O
task	O
in	O
order	O
to	O
integrate	O
the	O
knowledge	O
learned	O
by	O
each	O
module	O
into	O
a	O
unified	O
decision	O
.	O
We	O
simulated	O
these	O
settings	O
by	O
dividing	O
the	O
training	O
data	O
into	O
three	O
folds	O
.	O
We	O
associated	O
each	O
one	O
of	O
the	O
first	O
two	O
folds	O
with	O
the	O
two	O
sub	O
-	O
task	O
modules	O
.	O
Each	O
one	O
of	O
the	O
these	O
folds	O
only	O
included	O
the	O
partial	O
labels	O
relevant	O
for	O
that	O
sub	O
-	O
task	O
.	O
We	O
then	O
used	O
gradually	O
increasing	O
amounts	O
of	O
the	O
third	O
fold	O
,	O
consisting	O
of	O
the	O
full	O
labels	O
,	O
for	O
training	O
the	O
decision	O
module	O
.	O
Fig	O
.	O
3	O
describes	O
the	O
outcome	O
for	O
targetsentiment	O
,	O
comparing	O
a	O
non	O
-	O
modular	O
model	O
using	O
only	O
the	O
full	O
labels	O
,	O
with	O
the	O
modular	O
approach	O
,	O
which	O
uses	O
the	O
full	O
labels	O
for	O
knowledge	O
integration	O
.	O
Results	O
show	O
that	O
even	O
when	O
very	O
little	O
full	O
data	O
is	O
available	O
results	O
significantly	O
improve	O
.	O
Additional	O
results	O
show	O
the	O
same	O
pattern	O
for	O
subjective	O
phrase	O
identification	O
and	O
classification	O
are	O
included	O
in	O
the	O
Appendix	O
.	O

Partially	O
-	O
labeled	O
data	O
can	O
be	O
cheaper	O
and	O
easier	O
to	O
obtain	O
,	O
especially	O
for	O
low	O
-	O
resource	O
languages	O
.	O
In	O
this	O
set	O
of	O
experiments	O
,	O
we	O
model	O
these	O
settings	O
over	O
the	O
target	O
-	O
sentiment	O
task	O
.	O
The	O
results	O
are	O
summarized	O
in	O
Fig	O
.	O
4	O
.	O
We	O
fixed	O
the	O
amount	O
of	O
full	O
labels	O
to	O
20	O
%	O
of	O
the	O
training	O
set	O
,	O
and	O
gradually	O
increased	O
the	O
amount	O
of	O
partially	O
labeled	O
data	O
.	O
We	O
studied	O
adding	O
segmentation	O
and	O
type	O
separately	O
.	O
After	O
the	O
model	O
is	O
trained	O
in	O
this	O
routine	O
,	O
it	O
was	O
tested	O
on	O
predicting	O
the	O
full	O
labels	O
jointly	O
on	O
the	O
test	O
set	O
.	O

In	O
our	O
final	O
analysis	O
we	O
considered	O
a	O
novel	O
domain	O
-	O
adaptation	O
settings	O
,	O
where	O
we	O
have	O
a	O
small	O
amount	O
of	O
fully	O
labeled	O
in	O
-	O
domain	O
data	O
from	O
aspect	O
sentiment	O
and	O
more	O
out	O
-	O
of	O
-	O
domain	O
data	O
from	O
target	O
sentiment	O
.	O
However	O
unlike	O
the	O
traditional	O
domain	O
-	O
adaptation	O
settings	O
,	O
the	O
out	O
-	O
ofdomain	O
data	O
is	O
labeled	O
for	O
a	O
different	O
task	O
,	O
and	O
only	O
shares	O
one	O
module	O
with	O
the	O
original	O
task	O
.	O
In	O
our	O
experiments	O
we	O
fixed	O
20	O
%	O
of	O
the	O
fully	O
labeled	O
data	O
for	O
the	O
aspect	O
sentiment	O
task	O
,	O
and	O
gradually	O
added	O
out	O
-	O
of	O
-	O
domain	O
data	O
,	O
consisting	O
of	O
partial	O
sentiment	O
labels	O
from	O
the	O
target	O
sentiment	O
task	O
.	O
Our	O
model	O
successfully	O
utilized	O
the	O
out	O
-	O
ofdomain	O
data	O
and	O
improved	O
performance	O
on	O
the	O
indomain	O
task	O
.	O
The	O
results	O
are	O
shown	O
on	O
Fig	O
5	O
.	O

For	O
all	O
our	O
experiments	O
,	O
to	O
show	O
efficacy	O
of	O
our	O
approach	O
we	O
kept	O
the	O
preprocessing	O
as	O
minimal	O
as	O
possible	O
.	O
Apart	O
from	O
word	O
lowerization	O
,	O
tokenization	O
,	O
and	O
punctuation	O
removal	O
we	O
did	O
n't	O
perform	O
any	O
other	O
activity	O
.	O

Convolutional	O
Neural	O
Networks	O
are	O
known	O
to	O
perform	O
well	O
on	O
short	O
texts	O
(	O
Yin	O
et	O
al	O
,	O
2017	O
)	O
,	O
in	O
(	O
Conneau	O
et	O
al	O
,	O
2017a	O
)	O
authors	O
proposed	O
to	O
concatenate	O
representation	O
at	O
different	O
levels	O
of	O
input	O
sentence	O
.	O
The	O
model	O
was	O
claimed	O
to	O
capture	O
hierarchical	O
abstractions	O
of	O
the	O
input	O
sentence	O
.	O
For	O
our	O
experiments	O
,	O
we	O
fixed	O
128	O
kernels	O
of	O
size	O
2	O
,	O
3	O
,	O
4	O
,	O
4	O
at	O
4	O
different	O
levels	O
.	O
These	O
values	O
were	O
decided	O
after	O
the	O
experiments	O
with	O
different	O
number	O
of	O
kernels	O
and	O
their	O
sizes	O
.	O

We	O

Bing	O
Liu	O
's	O
work	O
was	O
partially	O
supported	O
by	O
the	O
National	O
Science	O
Foundation	O
(	O
NSF	O
IIS	O
1838770	O
)	O
and	O
by	O
a	O
research	O
gift	O
from	O
Huawei	O
.	O

A	O
Modular	O
Architecture	O
for	O
Unsupervised	O
Sarcasm	O
Generation	O

Generation	O
of	O
sarcasm	O
,	O
unlike	O
other	O
language	O
generation	O
tasks	O
,	O
is	O
highly	O
nuanced	O
.	O
If	O
we	O
reconsider	O
the	O
example	O
in	O
the	O
introductory	O
section	O
,	O
the	O
output	O
sentence	O
is	O
sarcastic	O
as	O
it	O
presents	O
an	O
unusual	O
situation	O
where	O
the	O
opinion	O
holder	O
has	O
liked	O
the	O
rather	O
boring	O
act	O
of	O
waiting	O
for	O
a	O
bus	O
.	O
The	O
unusualness	O
(	O
and	O
hence	O
,	O
the	O
sarcasm	O
)	O
arises	O
from	O
two	O
implicitly	O
opposing	O
(	O
incongruous	O
)	O
contexts	O
:	O
love	O
and	O
waiting	O
for	O
the	O
bus	O
.	O
Such	O
a	O
form	O
of	O
sarcasm	O
,	O
based	O
on	O
the	O
context	O
incongruity	O
theory	O
(	O
Campbell	O
and	O
Katz	O
,	O
2012	O
)	O
,	O
is	O
more	O
common	O
in	O
text	O
than	O
other	O
forms	O
such	O
as	O
prepositional	O
,	O
embedded	O
or	O
illocutionary	O
sarcasm	O
(	O
Camp	O
,	O
2012	O
)	O
.	O
For	O
any	O
textual	O
sarcasm	O
generator	O
,	O
figuring	O
out	O
contextually	O
incongruous	O
phrases	O
will	O
be	O
as	O
difficult	O
as	O
generating	O
a	O
fluent	O
sentence	O
.	O
Moreover	O
,	O
most	O
of	O
the	O
existing	O
language	O
generators	O
are	O
known	O
to	O
work	O
on	O
large	O
scale	O
literal	O
/	O
non	O
-	O
sarcastic	O
texts	O
(	O
e.g.	O
,	O
language	O
models	O
trained	O
on	O
Wikipedia	O
articles	O
)	O
,	O
and	O
are	O
agnostic	O
of	O
the	O
possible	O
collocations	O
of	O
contextually	O
incongruous	O
phrases	O
(	O
Joshi	O
et	O
al	O
,	O
2017a	O
)	O
.	O
We	O
try	O
to	O
overcome	O
these	O
challenges	O
through	O
our	O
modular	O
system	O
design	O
,	O
discussed	O
as	O
follows	O
.	O

The	O
overall	O
system	O
architecture	O
is	O
presented	O
in	O
Figure	O
1	O
.	O
For	O
development	O
of	O
the	O
modules	O
three	O
corpora	O
are	O
needed	O
:	O
(	O
a	O
)	O
a	O
corpus	O
of	O
positive	O
sentiment	O
sentences	O
(	O
P	O
)	O
,	O
(	O
b	O
)	O
a	O
corpus	O
of	O
negative	O
sentiment	O
sentences	O
(	O
N	O
)	O
,	O
and	O
(	O
c	O
)	O
a	O
corpus	O
of	O
sarcastic	O
sentences	O
(	O
S	O
)	O
.	O
The	O
framework	O
performs	O
transformation	O
of	O
literal	O
text	O
into	O
sarcastic	O
ones	O
in	O
four	O
stages	O
as	O
given	O
below	O
:	O

This	O
is	O
a	O
one	O
-	O
time	O
process	O
and	O
is	O
done	O
using	O
the	O
unsupervised	O
bootstrapping	O
technique	O
similar	O
to	O
Riloff	O
et	O
al	O
(	O
2013	O
)	O
.	O
For	O
each	O
sentence	O
in	O
the	O
sarcasm	O
corpus	O
S	O
,	O
a	O
candidate	O
negative	O
situation	O
phrase	O
is	O
extracted	O
.	O
A	O
candidate	O
negative	O
situation	O
phrase	O
is	O
a	O
word	O
n	O
-	O
gram	O
(	O
n	O
≤	O
5	O
)	O
that	O
follows	O
a	O
positive	O
sentiment	O
phrase	O
in	O
a	O
sarcastic	O
sentence	O
3	O
.	O
After	O
the	O
candidates	O
for	O
a	O
positive	O
phrase	O
are	O
obtained	O
,	O
their	O
Part	O
of	O
Speech	O
tags	O
are	O
extracted	O
with	O
the	O
help	O
of	O
a	O
POS	O
tagger	O
.	O
Specific	O
patterns	O
of	O
n	O
-	O
gram	O
are	O
then	O
obtained	O
using	O
the	O
POS	O
tags	O
.	O
This	O
ensures	O
that	O
the	O
phrases	O
extracted	O
are	O
mostly	O
verb	O
phrases	O
,	O
noun	O
-	O
phrases	O
,	O
and	O
to	O
-	O
infinitive	O
verb	O
phrases	O
that	O
describe	O
situations	O
.	O
In	O
our	O
setting	O
we	O
use	O
30	O
predefined	O
POS	O
n	O
-	O
gram	O
patterns	O
following	O
Riloff	O
et	O
al	O
(	O
2013	O
)	O
.	O
Once	O
the	O
candidate	O
negative	O
situation	O
phrases	O
are	O
extracted	O
,	O
they	O
are	O
filtered	O
based	O
on	O
a	O
scoring	O
function	O
as	O
given	O
below	O
:	O
score	O
i	O
=	O
#	O
ns	O
i	O
in	O
S	O
#	O
ns	O
i	O
in	O
S	O
,	O
P	O
,	O
N	O
(	O
2	O
)	O
where	O
ns	O
i	O
is	O
the	O
i	O
th	O
negative	O
situation	O
extracted	O
for	O
a	O
certain	O
positive	O
phrase	O
.	O
The	O
scoring	O
function	O
returns	O
a	O
real	O
value	O
indicating	O
the	O
exclusiveness	O
of	O
the	O
negative	O
situation	O
w.r.t	O
the	O
sarcastic	O
sentences	O
.	O
If	O
the	O
score	O
exceeds	O
3	O
the	O
word	O
"	O
love	O
"	O
is	O
considered	O
as	O
the	O
seed	O
positive	O
sentiment	O
phrase	O
to	O
begin	O
the	O
bootstrapping	O
procedure	O
a	O
threshold	O
(	O
i.e.	O
,	O
p	O
>	O
0.5	O
)	O
,	O
the	O
candidate	O
phrase	O
is	O
added	O
to	O
the	O
gazetteer	O
.	O
Once	O
all	O
the	O
possible	O
negative	O
situation	O
phrases	O
are	O
extracted	O
,	O
each	O
phrase	O
is	O
used	O
to	O
extract	O
more	O
positive	O
sentiment	O
phrases	O
similarly	O
as	O
above	O
.	O
This	O
process	O
of	O
positive	O
phrase	O
and	O
negative	O
situation	O
extraction	O
is	O
repeated	O
until	O
no	O
new	O
phrases	O
are	O
found	O
.	O
Table	O
1	O
shows	O
some	O
example	O
negative	O
situation	O
phrases	O
extracted	O
from	O
our	O
dataset	O
.	O

Since	O
no	O
dataset	O
containing	O
paired	O
examples	O
of	O
literal	O
and	O
sarcastic	O
utterances	O
are	O
available	O
,	O
we	O
created	O
a	O
small	O
test	O
-	O
set	O
for	O
evaluating	O
our	O
system	O
.	O
From	O
the	O
test	O
split	O
of	O
the	O
sarcasm	O
corpus	O
S	O
,	O
250	O
sentences	O
on	O
diverse	O
topics	O
are	O
selected	O
and	O
are	O
manually	O
translated	O
into	O
literal	O
versions	O
by	O
two	O
linguists	O
.	O
From	O
this	O
,	O
only	O
203	O
sentences	O
could	O
be	O
selected	O
by	O
the	O
linguists	O
who	O
mutually	O
decided	O
whether	O
the	O
sentences	O
were	O
sarcastic	O
enough	O
to	O
keep	O
in	O
the	O
test	O
dataset	O
or	O
not	O
.	O

by	O
Artetxe	O
et	O
al	O
(	O
2017	O
)	O
,	O
which	O
can	O
be	O
extended	O
to	O
any	O
translation	O
task	O
.	O
In	O
our	O
setting	O
,	O
the	O
source	O
and	O
target	O
side	O
are	O
literal	O
and	O
sarcastic	O
utterances	O
,	O
i.e.	O
the	O
direction	O
of	O
translation	O
is	O
non	O
-	O
sarcastic	O
to	O
sarcastic	O
.	O

Tables	O
2	O
and	O
3	O
present	O
evaluation	O
results	O
.	O
While	O
it	O
was	O
expected	O
that	O
the	O
automatic	O
metrics	O
may	O
not	O
be	O
able	O
to	O
capture	O
the	O
subtleties	O
of	O
sarcasm	O
,	O
the	O
WL	O
measure	O
indicates	O
that	O
a	O
carefully	O
designed	O
modular	O
approach	O
like	O
ours	O
often	O
generates	O
longer	O
sentences	O
with	O
more	O
context	O
.	O
This	O
is	O
also	O
corroborated	O
by	O
the	O
human	O
evaluation	O
where	O
annotators	O
have	O
judged	O
that	O
the	O
output	O
generated	O
from	O
our	O
system	O
are	O
more	O
sarcastic	O
than	O
the	O
comparison	O
systems	O
.	O
SarcasmBot	O
,	O
being	O
a	O
heuristic	O
driven	O
sarcasm	O
generator	O
produces	O
sarcastic	O
responses	O
but	O
is	O
not	O
related	O
to	O
the	O
input	O
topic	O
.	O
Moreover	O
,	O
it	O
ends	O
up	O
generating	O
only	O
20	O
different	O
responses	O
for	O
our	O
entire	O
test	O
dataset	O
making	O
its	O
output	O
redundant	O
and	O
unrelated	O
to	O
the	O
input	O
.	O
Other	O
existing	O
systems	O
such	O
as	O
UNMT	O
and	O
Monoses	O
converge	O
to	O
autoencoding	O
and	O
end	O
up	O
replicating	O
the	O
input	O
as	O
output	O
.	O
FLIP	O
,	O
performs	O
transformations	O
at	O
lexical	O
level	O
,	O
hence	O
achieves	O
better	O
fluency	O
but	O
certainly	O
fails	O
to	O
induce	O
sarcasm	O
in	O
most	O
of	O
the	O
cases	O
.	O
Table	O
4	O
presents	O
example	O
generations	O
from	O
different	O
systems	O
.	O
It	O
is	O
quite	O
interesting	O
to	O
note	O
that	O
due	O
to	O
the	O
RL	O
,	O
the	O
model	O
tends	O
to	O
produce	O
longer	O
sentences	O
and	O
brings	O
additional	O
context	O
necessary	O
for	O
sarcasm	O
.	O
The	O
fluency	O
is	O
however	O
compromised	O
.	O
A	O
close	O
inspection	O
of	O
the	O
outputs	O
from	O
each	O
module	O
suggests	O
that	O
the	O
overall	O
error	O
committed	O
by	O
the	O
system	O
is	O
due	O
to	O
accumulation	O
of	O
different	O
types	O
of	O
errors	O
,	O
mainly	O
(	O
a	O
)	O
error	O
during	O
neutralization	O
due	O
to	O
inappropriate	O
assignment	O
of	O
weights	O
to	O
the	O
words	O
in	O
the	O
input	O
,	O
(	O
b	O
)	O
dropping	O
of	O
words	O
and/or	O
insertion	O
of	O
spurious	O
words	O
during	O
positive	O
sentiment	O
induction	O
,	O
and	O
(	O
c	O
)	O
error	O
in	O
scoring	O
the	O
sarcasm	O
content	O
in	O
the	O
RL	O
setting	O
.	O
These	O
can	O
be	O
addressed	O
through	O
better	O
hyper	O
-	O
parameter	O
tuning	O
,	O
gathering	O
more	O
training	O
data	O
for	O
training	O
the	O
individual	O
modules	O
(	O
especially	O
the	O
sarcasm	O
synthe	O
-	O
Input	O
:	O
worrying	O
because	O
did	O
not	O
finish	O
my	O
homework	O
.	O
Reference	O
:	O
did	O
not	O
finish	O
any	O
homework	O
&	O
i	O
still	O
need	O
to	O
shower	O
!	O
lol	O
!	O
love	O
stressing	O
out	O
.	O
SarcasmBot	O
:	O
How	O
exciting	O
!	O
rolls	O
eyes	O
.	O
Monoses	O
:	O
whining	O
because	O
actually	O
finish	O
my	O
homework	O
.	O
UNMT	O
:	O
worrying	O
because	O
i	O
did	O
not	O
finish	O
my	O
homework	O
.	O
ST	O
:	O
if	O
do	O
not	O
work	O
my	O
mom	O
hurts	O
.	O
FLIP	O
:	O
reassuring	O
because	O
did	O
not	O
finish	O
my	O
homework	O
.	O
SG	O
(	O
NORMAL	O
)	O
:	O
i	O
am	O
worrying	O
about	O
the	O
worrying	O
of	O
homework	O
.	O
SG	O
(	O
RL	O
)	O
:	O
worrying	O
about	O
finish	O
homework	O
.	O
ALL	O
(	O
NORMAL	O
)	O
:	O
no	O
worrying	O
,	O
i	O
finish	O
doing	O
homework	O
great	O
.	O
ALL	O
(	O
RL	O
)	O
:	O
worrying	O
about	O
finish	O
homework	O
is	O
great	O
.	O
Input	O
:	O
swimming	O
lessons	O
are	O
very	O
costly	O
in	O
nyc	O
.	O
Reference	O
:	O
you	O
have	O
to	O
sacrifice	O
your	O
first	O
born	O
child	O
for	O
swimming	O
lessons	O
in	O
nyc	O
.	O
SarcasmBot	O
:	O
That	O
is	O
a	O
very	O
useful	O
piece	O
of	O
information	O
!	O
Yawn	O
!	O
Monoses	O
:	O
Dry	O
lessons	O
are	O
very	O
costly	O
financially	O
.	O
UNMT	O
:	O
swimming	O
lessons	O
are	O
very	O
costly	O
in	O
nyc	O
.	O
ST	O
:	O
a	O
will	O
was	O
in	O
bed	O
.	O
FLIP	O
:	O
swimming	O
lessons	O
are	O
very	O
costly	O
in	O
nyc	O
.	O
SG	O
(	O
NORMAL	O
)	O
:	O
this	O
is	O
so	O
costly	O
to	O
me	O
swimming	O
lessons	O
in	O
nyc	O
.	O
SG	O
(	O
RL	O
)	O
:	O
swimming	O
lessons	O
is	O
so	O
costly	O
in	O
nyc	O
.	O
ALL	O
(	O
NORMAL	O
)	O
:	O
loving	O
the	O
swimming	O
lessons	O
in	O
nyc	O
ch	O
.	O
ALL	O
(	O
RL	O
)	O
:	O
i	O
am	O
loving	O
the	O
swimming	O
lessons	O
.	O
going	O
to	O
be	O
a	O
very	O
costly	O
in	O
nyc	O
ch	O
.	O

Twitter	O
is	O
an	O
ever	O
-	O
growing	O
store	O
of	O
daily	O
generated	O
data	O
.	O
Given	O
the	O
huge	O
number	O
of	O
tweets	O
talking	O
about	O
drug	O
-	O
related	O
issues	O
,	O
social	O
media	O
mining	O
is	O
applicable	O
to	O
areas	O
such	O
as	O
pharmacovigilance	O
(	O
Lee	O
et	O
al	O
,	O
2017	O
;	O
Nikfarjam	O
et	O
al	O
,	O
2015	O
;	O
Ginn	O
et	O
al	O
,	O
2014	O
;	O
Freifeld	O
et	O
al	O
,	O
2014	O
;	O
Bian	O
et	O
al	O
,	O
2012	O
)	O
.	O
Tasks	O
1	O
and	O
2	O
focuses	O
on	O
detecting	O
tweets	O
with	O
ADR	O
and	O
identifying	O
location	O
of	O
mentions	O
.	O
We	O
are	O
provided	O
with	O
25	O
,	O
672	O
tweets	O
(	O
2	O
,	O
374	O
positive	O
and	O
23	O
,	O
298	O
negative	O
)	O
and	O
approximately	O
5	O
,	O
000	O
unlabeled	O
tweets	O
as	O
a	O
validation	O
set	O
.	O
For	O
the	O
second	O
task	O
,	O
a	O
subset	O
of	O
2	O
,	O
367	O
tweets	O
from	O
the	O
first	O
task	O
was	O
provided	O
(	O
1	O
,	O
212	O
positive	O
and	O
1	O
,	O
155	O
negative	O
)	O
.	O
The	O
evaluation	O
data	O
comprises	O
1	O
,	O
000	O
tweets	O
(	O
~500	O
positive	O
,	O
~500	O
negative	O
)	O
.	O

Stop	O
words	O
and	O
punctuations	O
were	O
removed	O
from	O
tweets	O
and	O
all	O
drug	O
names	O
found	O
in	O
the	O
FDA	O
's	O
Approved	O
Drug	O
Products	O
list	O
1	O
were	O
replaced	O
by	O
the	O
word	O
"	O
drug	O
"	O
.	O
Word	O
stemming	O
and	O
tokenization	O
were	O
performed	O
using	O
nltk	O
python	O
library	O
.	O

To	O
identify	O
the	O
text	O
spans	O
of	O
ADR	O
mentions	O
,	O
first	O
the	O
model	O
developed	O
for	O
task	O
1	O
was	O
used	O
to	O
determine	O
whether	O
each	O
tweet	O
mentions	O
an	O
ADR	O
.	O
Then	O
the	O
similarity	O
between	O
each	O
tweet	O
and	O
3	O
different	O
lexicon	O
sets	O
(	O
Nikfarjam	O
et	O
al	O
3	O
,	O
MedDRA	O
(	O
Medical	O
Dictionary	O
for	O
Regulatory	O
Activities	O
)	O
4	O
,	O
and	O
CHV	O
(	O
Consumer	O
Health	O
Vocabulary	O
)	O
5	O
)	O
was	O
measured	O
.	O
To	O
calculate	O
similarity	O
,	O
each	O
tweet	O
and	O
lexicon	O
was	O
converted	O
to	O
a	O
set	O
of	O
word	O
stems	O
.	O
Since	O
similarity	O
measures	O
such	O
as	O
cosine	O
or	O
Jaccard	O
are	O
highly	O
affected	O
by	O
other	O
non	O
-	O
ADR	O
words	O
,	O
we	O
defined	O
similarity	O
as	O
the	O
percent	O
of	O
word	O
stems	O
of	O
a	O
lexicon	O
that	O
exist	O
in	O
a	O
tweet	O
.	O
For	O
each	O
tweet	O
,	O
only	O
lexicons	O
with	O
a	O
100	O
%	O
match	O
were	O
kept	O
.	O

The	O
sentiment	O
polarity	O
of	O
price	O
is	O
positive	O
(	O
scoring	O
:	O
0.1	O
)	O
The	O
sentiment	O
polarity	O
of	O
price	O
is	O
neutral	O
(	O
scoring	O
:	O
0.2	O
)	O
The	O
sentiment	O
polarity	O
of	O
price	O
is	O
negative	O
(	O
scoring	O
:	O
0.7	O
)	O

given	O
the	O
crowd	O
,	O
but	O
for	O
the	O
price	O
I	O
was	O
disappointed	O
.	O
<	O
miscellaneous	O
:	O
neutral	O
>	O
<	O
incorrect	O
output	O
:	O
negative	O
>	O
The	O
kids	O
really	O
enjoyed	O
their	O
food	O
and	O
the	O
value	O
on	O
the	O
kids	O
menu	O
is	O
good	O
.	O
<	O
menu	O
:	O
neutral	O
>	O
<	O
incorrect	O
output	O
:	O
positive	O
>	O
The	O
decor	O
could	O
be	O
a	O
bit	O
better	O
,	O
and	O
if	O
there	O
was	O
a	O
small	O
bar	O
the	O
overall	O
atmosphere	O
would	O
be	O
a	O
bit	O
more	O
inviting	O
.	O
<	O
place	O
:	O
negative	O
>	O
<	O
incorrect	O
output	O
:	O
neutral	O
>	O

A.1	O
Sentence	O
-	O
Level	O
Datasets	O
Rest14	O
(	O
Pontiki	O
et	O
al	O
,	O
2014a	O
)	O
Following	O
previous	O
work	O
(	O
Cheng	O
et	O
al	O
,	O
2017	O
;	O
Tay	O
et	O
al	O
,	O
2018	O
;	O
Hu	O
et	O
al	O
,	O
2019	O
)	O
,	O
we	O
remove	O
samples	O
with	O
conflict	O
polarities	O
.	O
Since	O
there	O
is	O
no	O
official	O
development	O
set	O
for	O
Rest14	O
,	O
we	O
use	O
the	O
split	O
offered	O
by	O
Tay	O
et	O
al	O
(	O
2018	O

Zhiyang	O
Teng	O
is	O
the	O
corresponding	O
author	O
.	O
We	O
would	O
like	O
to	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
insightful	O
comments	O
.	O
We	O
gratefully	O
acknowledge	O
funding	O
from	O
the	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
(	O
NSFC	O
No.61976180	O
)	O
.	O

Seen2Unseen	O
at	O
PARSEME	O
Shared	O
Task	O
2020	O
:	O
All	O
Roads	O
do	O
not	O
Lead	O
to	O
Unseen	O
Verb	O
-	O
Noun	O
VMWEs	O

We	O
describe	O
the	O
Seen2Unseen	O
system	O
that	O
participated	O
in	O
edition	O
1.2	O
of	O
the	O
PARSEME	O
shared	O
task	O
on	O
automatic	O
identification	O
of	O
verbal	O
multiword	O
expressions	O
(	O
VMWEs	O
)	O
.	O
The	O
identification	O
of	O
VMWEs	O
that	O
do	O
not	O
appear	O
in	O
the	O
provided	O
training	O
corpora	O
(	O
called	O
unseen	O
VMWEs	O
)	O
-	O
with	O
a	O
focus	O
here	O
on	O
verb	O
-	O
noun	O
VMWEs	O
-	O
is	O
based	O
on	O
mutual	O
information	O
and	O
lexical	O
substitution	O
or	O
translation	O
of	O
seen	O
VMWEs	O
.	O
We	O
present	O
the	O
architecture	O
of	O
the	O
system	O
,	O
report	O
results	O
for	O
14	O
languages	O
,	O
and	O
propose	O
an	O
error	O
analysis	O
.	O

The	O
identification	O
of	O
multiword	O
expressions	O
(	O
MWEs	O
)	O
such	O
as	O
spill	O
the	O
beans	O
is	O
a	O
challenging	O
problem	O
(	O
Baldwin	O
and	O
Kim	O
,	O
2010	O
;	O
Constant	O
et	O
al	O
,	O
2017	O
)	O
,	O
all	O
the	O
more	O
so	O
for	O
verbal	O
MWEs	O
(	O
VMWEs	O
)	O
subject	O
to	O
morphological	O
(	O
spill	O
the	O
bean	O
)	O
and	O
syntactic	O
variability	O
(	O
the	O
beans	O
were	O
spilled	O
)	O
.	O
The	O
PARSEME	O
shared	O
task	O
(	O
PST	O
)	O
provided	O
training	O
,	O
development	O
and	O
test	O
corpora	O
(	O
hereafter	O
Train	O
,	O
Dev	O
,	O
and	O
Test	O
)	O
manually	O
annotated	O
for	O
VMWEs	O
.	O
1	O
Our	O
system	O
aimed	O
at	O
identifying	O
every	O
VMWE	O
in	O
Test	O
which	O
also	O
appears	O
in	O
Train	O
or	O
Dev	O
,	O
including	O
possible	O
morphological	O
or	O
syntactic	O
variants	O
(	O
henceforth	O
seen	O
VMWEs	O
)	O
or	O
not	O
present	O
in	O
Train	O
/	O
Dev	O
(	O
unseen	O
VMWEs	O
)	O
.	O
Unseen	O
VMWE	O
identification	O
,	O
the	O
main	O
focus	O
of	O
this	O
PST	O
edition	O
,	O
is	O
harder	O
than	O
seen	O
VMWE	O
identification	O
,	O
as	O
shown	O
by	O
previous	O
results	O
(	O
Ramisch	O
et	O
al	O
,	O
2018	O
)	O
.	O
We	O
submitted	O
two	O
systems	O
:	O
Seen2Seen	O
(	O
closed	O
track	O
)	O
and	O
Seen2Unseen	O
(	O
open	O
track	O
)	O
.	O
Seen2Unseen	O
relies	O
on	O
Seen2Seen	O
for	O
the	O
identification	O
of	O
seen	O
VMWEs	O
and	O
has	O
an	O
additional	O
module	O
for	O
unseen	O
ones	O
.	O
Its	O
best	O
global	O
unseen	O
F	O
-	O
score	O
(	O
i.e.	O
not	O
only	O
for	O
verb	O
-	O
noun	O
constructions	O
)	O
was	O
obtained	O
for	O
Hindi	O
(	O
42.66	O
)	O
and	O
it	O
reached	O
25.36	O
in	O
French	O
,	O
which	O
was	O
our	O
main	O
focus	O
.	O
Despite	O
the	O
lower	O
global	O
MWE	O
-	O
based	O
F1score	O
of	O
Seen2Unseen	O
(	O
63.02	O
)	O
compared	O
to	O
Seen2Seen	O
(	O
66.23	O
)	O
,	O
we	O
describe	O
the	O
former	O
(	O
Sec	O
.	O
2	O
)	O
,	O
analyse	O
its	O
interesting	O
negative	O
results	O
(	O
Sec	O
.	O
3	O
)	O
,	O
and	O
conclude	O
with	O
ideas	O
for	O
future	O
work	O
(	O
Sec	O
.	O
4	O
)	O
.	O

This	O
work	O
was	O
funded	O
by	O
the	O
French	O
PARSEME	O
-	O
FR	O
grant	O
(	O
ANR	O
-	O
14	O
-	O
CERA	O
-	O
0001	O
)	O
.	O
We	O
are	O
grateful	O
to	O
Guillaume	O
Vidal	O
for	O
his	O
prototype	O
,	O
and	O
to	O
the	O
anonymous	O
reviewers	O
for	O
their	O
useful	O
comments	O
.	O

Do	O
n't	O
sweat	O
the	O
small	O
stuff	O
,	O
classify	O
the	O
rest	O
:	O
Sample	O
Shielding	O
to	O
protect	O
text	O
classifiers	O
against	O
adversarial	O
attacks	O

The	O
typical	O
attack	O
strategy	O
perturbing	O
texts	O
with	O
word	O
synonyms	O
or	O
character	O
substitutions	O
assumes	O
to	O
have	O
query	O
access	O
to	O
the	O
target	O
web	O
site	O
's	O
classifier	O
(	O
W	O
)	O
(	O
Yoo	O
and	O
Qi	O
,	O
2021	O
;	O
Li	O
et	O
al	O
,	O
2021a	O
;	O
Ren	O
et	O
al	O
,	O
2019	O
;	O
Li	O
et	O
al	O
,	O
2020	O
;	O
Garg	O
and	O
Ramakrishnan	O
,	O
2020	O
;	O
Jia	O
et	O
al	O
,	O
2019	O
;	O
Li	O
et	O
al	O
,	O
2019	O
)	O
.	O
The	O
text	O
is	O
modified	O
by	O
querying	O
W	O
hundreds	O
or	O
thousands	O
of	O
times	O
,	O
each	O
time	O
with	O
a	O
text	O
version	O
differing	O
only	O
slightly	O
from	O
the	O
previouseven	O
by	O
just	O
a	O
single	O
word	O
(	O
Li	O
et	O
al	O
,	O
2020	O
;	O
.	O
Such	O
a	O
querying	O
pattern	O
can	O
be	O
easily	O
identified	O
as	O
adversarial	O
by	O
the	O
website	O
and	O
countered	O
.	O
Thus	O
,	O
practically	O
the	O
only	O
way	O
in	O
which	O
such	O
an	O
attack	O
can	O
take	O
place	O
is	O
when	O
the	O
attacker	O
owns	O
a	O
local	O
classifier	O
W	O
′	O
which	O
is	O
either	O
an	O
exact	O
copy	O
of	O
W	O
or	O
a	O
close	O
enough	O
approximation	O
.	O
We	O
adopt	O
this	O
more	O
realistic	O
threat	O
model	O
,	O
shown	O
in	O
Figure	O
1	O
.	O
In	O
our	O
threat	O
model	O
the	O
attacker	O
uses	O
feedback	O
from	O
its	O
local	O
W	O
′	O
to	O
generate	O
a	O
final	O
perturbed	O
version	O
that	O
defeats	O
W	O
′	O
or	O
is	O
close	O
enough	O
to	O
do	O
so	O
.	O
The	O
attacker	O
submits	O
only	O
this	O
final	O
version	O
to	O
the	O
website	O
,	O
expecting	O
W	O
to	O
make	O
the	O
same	O
error	O
.	O
However	O
,	O
the	O
website	O
defends	O
W	O
using	O
Sample	O
Shielding	O
:	O
sample	O
based	O
pre	O
-	O
processing	O
on	O
the	O
input	O
text	O
,	O
prior	O
to	O
applying	O
W	O
.	O
The	O
attacker	O
may	O
or	O
may	O
not	O
be	O
aware	O
of	O
this	O
fact	O
.	O
Keeping	O
W	O
=	O
W	O
′	O
which	O
is	O
consistent	O
with	O
other	O
defenses	O
,	O
we	O
evaluate	O
our	O
defense	O
under	O
two	O
conditions	O
:	O
1	O
)	O
The	O
attacker	O
does	O
not	O
know	O
that	O
the	O
website	O
employs	O
Sample	O
Shielding	O
pre	O
-	O
processing	O
when	O
classifying	O
text	O
using	O
W	O
.	O
2	O
)	O
The	O
Sample	O
Shielding	O
step	O
is	O
leaked	O
and	O
the	O
attacker	O
incorporates	O
it	O
locally	O
when	O
using	O
W	O
′	O
to	O
generate	O
the	O
final	O
perturbed	O
text	O
.	O
We	O
present	O
results	O
from	O
experiments	O
exploring	O
both	O
of	O
these	O
attack	O
conditions	O
.	O

Intuition	O
.	O
Current	O
adversarial	O
attackers	O
have	O
two	O
goals	O
:	O
fool	O
the	O
classifier	O
and	O
maintain	O
the	O
original	O
meaning	O
.	O
Since	O
they	O
make	O
minimal	O
changes	O
,	O
the	O
extent	O
of	O
perturbation	O
is	O
in	O
fact	O
one	O
of	O
the	O
reported	O
statistics	O
.	O
For	O
example	O
,	O
(	O
Li	O
et	O
al	O
,	O
2020	O
)	O
note	O
that	O
their	O
10	O
%	O
perturbation	O
rate	O
is	O
far	O
less	O
than	O
in	O
previous	O
attacks	O
.	O
(	O
Li	O
et	O
al	O
,	O
2019	O
)	O
also	O
focus	O
on	O
minimal	O
changes	O
(	O
4	O
%	O
)	O
needed	O
in	O
support	O
of	O
their	O
attack	O
success	O
rate	O
.	O
Our	O
defense	O
approach	O
capitalizes	O
on	O
this	O
drive	O
to	O
make	O
minimal	O
changes	O
.	O
Specifically	O
,	O
in	O
Sample	O
Shielding	O
,	O
we	O
take	O
k	O
samples	O
each	O
composed	O
of	O
p%	O
of	O
the	O
text	O
.	O
We	O
choose	O
a	O
p	O
which	O
minimizes	O
the	O
chance	O
of	O
a	O
sample	O
including	O
attacked	O
(	O
modified	O
)	O
words	O
,	O
while	O
maximizing	O
the	O
content	O
available	O
for	O
the	O
classifier	O
to	O
make	O
a	O
correct	O
classification	O
.	O
We	O
choose	O
a	O
k	O
which	O
is	O
large	O
enough	O
to	O
cover	O
key	O
information	O
but	O
small	O
enough	O
to	O
reduce	O
redundancy	O
.	O
We	O
classify	O
each	O
sample	O
and	O
combine	O
>	O
0.5	O
k	O
sample	O
predictions	O
…	O
I	O
enjoyed	O
this	O
movie	O
more	O
than	O
I	O
thought	O
I	O
would	O
.	O
From	O
multiple	O
viewings	O
it	O
becomes	O
especially	O
clear	O
how	O
much	O
time	O
and	O
energy	O
the	O
director	O
put	O
into	O
this	O
film	O
.	O
The	O
choice	O
for	O
lead	O
actor	O
had	O
me	O
worried	O
but	O
it	O
worked	O
well	O
.	O
The	O
twist	O
was	O
what	O
really	O
had	O
me	O
hooked	O
.	O
...	O
their	O
decisions	O
for	O
the	O
final	O
classification	O
.	O
We	O
explore	O
two	O
sampling	O
and	O
three	O
decision	O
combining	O
methods	O
.	O

Random	O
Sampling	O
.	O
We	O
randomly	O
sample	O
p	O
portions	O
of	O
the	O
text	O
.	O
We	O
explore	O
both	O
sentences	O
and	O
words	O
as	O
sampled	O
units	O
.	O
A	O
visualization	O
of	O
random	O
sampling	O
is	O
in	O
Figure	O
2	O
.	O
Shifting	O
Sampling	O
.	O
We	O
sample	O
the	O
text	O
using	O
a	O
moving	O
window	O
of	O
length	O
p	O
×	O
length_of	O
_	O
text	O
.	O
The	O
first	O
starts	O
at	O
the	O
beginning	O
of	O
the	O
text	O
.	O
The	O
next	O
window	O
starts	O
right	O
after	O
the	O
previous	O
window	O
ends	O
.	O
If	O
there	O
is	O
insufficient	O
text	O
for	O
the	O
last	O
window	O
,	O
then	O
it	O
wraps	O
back	O
to	O
include	O
the	O
beginning	O
text	O
.	O

Majority	O
voting	O
.	O
This	O
is	O
a	O
simple	O
majority	O
vote	O
across	O
the	O
k	O
samples	O
(	O
Figure	O
2	O
)	O
.	O
Classifier	O
trained	O
on	O
sample	O
scores	O
from	O
original	O
texts	O
(	O
NN	O
)	O
.	O
We	O
train	O
a	O
neural	O
network	O
summarizer	O
to	O
make	O
a	O
final	O
class	O
prediction	O
based	O
on	O
the	O
k	O
sample	O
probabilities	O
.	O
Since	O
sample	O
ID	O
does	O
not	O
carry	O
any	O
information	O
,	O
the	O
input	O
to	O
the	O
neural	O
network	O
is	O
a	O
sorted	O
list	O
of	O
sample	O
probabilities	O
.	O
The	O
intent	O
is	O
to	O
see	O
if	O
the	O
neural	O
network	O
picks	O
up	O
on	O
latent	O
patterns	O
in	O
the	O
probabilities	O
that	O
are	O
not	O
captured	O
by	O
majority	O
voting	O
(	O
see	O
Figure	O
2	O
)	O
.	O
It	O
should	O
be	O
emphasized	O
that	O
the	O
neural	O
network	O
summarizer	O
is	O
trained	O
only	O
on	O
probabilities	O
generated	O
from	O
original	O
texts	O
and	O
does	O
not	O
consider	O
probabilities	O
from	O
attacker	O
modified	O
texts	O
.	O
We	O
use	O
a	O
simple	O
feed	O
forward	O
neural	O
net	O
composed	O
of	O
2	O
linear	O
layers	O
(	O
size	O
500	O
and	O
300	O
)	O
as	O
classification	O
summarizer	O
.	O
Classifier	O
trained	O
on	O
sample	O
scores	O
from	O
original	O
and	O
attacked	O
texts	O
(	O
NN	O
-	O
BB	O
)	O
.	O
This	O
is	O
similar	O
to	O
the	O
previous	O
strategy	O
except	O
that	O
the	O
training	O
data	O
includes	O
scores	O
from	O
original	O
texts	O
and	O
texts	O
that	O
have	O
been	O
modified	O
by	O
the	O
attacker	O
.	O
Because	O
this	O
assumes	O
more	O
knowledge	O
of	O
the	O
attacker	O
we	O
expect	O
NN	O
-	O
BB	O
to	O
perform	O
better	O
than	O
NN	O
.	O
The	O
ground	O
truth	O
label	O
for	O
these	O
modified	O
texts	O
is	O
the	O
original	O
correct	O
class	O
label	O
.	O
3	O
Experimental	O
Setup	O

We	O
submit	O
constrained	O
systems	O
to	O
both	O
German	O
to	O
French	O
and	O
French	O
to	O
German	O
translations	O
,	O
with	O
the	O
same	O
techniques	O
.	O

Acquisition	O
,	O
Representation	O
and	O
Usage	O
of	O
Conceptual	O
Hierarchies	O

Roadblocks	O
in	O
Gender	O
Bias	O
Measurement	O
for	O
Diachronic	O
Corpora	O

Building	O
on	O
both	O
Bolukbasi	O
et	O
al	O
(	O
2016	O
)	O
and	O
Chen	O
et	O
al	O
(	O
2021	O
)	O
,	O
we	O
consider	O
how	O
the	O
sets	O
of	O
profession	O
words	O
required	O
by	O
the	O
Bolukbasi	O
et	O
al	O
's	O
method	O
would	O
need	O
to	O
change	O
over	O
time	O
in	O
Arabic	O
.	O
We	O
begin	O
by	O
describing	O
two	O
diachronic	O
datasets	O
that	O
we	O
used	O
and	O
how	O
we	O
processed	O
these	O
datasets	O
,	O
then	O
we	O
describe	O
the	O
changes	O
in	O
the	O
profession	O
word	O
usage	O
over	O
time	O
.	O

In	O
the	O
religion	O
of	O
Islam	O
,	O
some	O
professions	O
are	O
forbidden	O
,	O
for	O
example	O
,	O
all	O
types	O
of	O
usury	O
,	O
and	O
serving	O
,	O
selling	O
,	O
or	O
drinking	O
alcohol	O
.	O
We	O
examined	O
a	O
set	O
of	O
illegal	O
/	O
religiously	O
forbidden	O
profession	O
words	O
in	O
Islam	O
across	O
the	O
11	O
ages	O
of	O
the	O
Arabic	O
poems	O
,	O
such	O
as	O
male	O
usurer	O
(	O
)	O
,	O
female	O
usurer	O
(	O
)	O
,	O
male	O
bartender	O
(	O
)	O
,	O
and	O
female	O
bartender	O
(	O
)	O
.	O
Specifically	O
,	O
we	O
closely	O
focused	O
on	O
the	O
diachronic	O
semantic	O
meaning	O
change	O
of	O
the	O
bartending	O
profession	O
words	O
in	O
the	O
parallel	O
eras	O
of	O
the	O
APCD	O
dataset	O
.	O
Interestingly	O
,	O
we	O
found	O
that	O
bartending	O
profession	O
words	O
in	O
the	O
early	O
ages	O
of	O
the	O
Arabic	O
poems	O
like	O
Pre	O
-	O
Islamic	O
,	O
Islamic	O
,	O
and	O
Umayyad	O
only	O
point	O
to	O
providing	O
water	O
to	O
people	O
but	O
not	O
serving	O
wine	O
even	O
though	O
the	O
wine	O
does	O
exist	O
.	O
Those	O
bartending	O
profession	O
words	O
are	O
polysemous	O
and	O
could	O
carry	O
other	O
meanings	O
like	O
the	O
male	O
bartender	O
(	O
)	O
could	O
have	O
a	O
meaning	O
of	O
the	O
phrase	O
'	O
my	O
leg	O
'	O
(	O
)	O
,	O
while	O
the	O
female	O
bartender	O
(	O
)	O
could	O
have	O
as	O
well	O
the	O
)	O
across	O
the	O
11	O
ages	O
of	O
the	O
Arabic	O
poems	O
in	O
the	O
APCD	O
dataset	O
,	O
showing	O
the	O
related	O
meanings	O
like	O
serving	O
water	O
,	O
wine	O
,	O
or	O
could	O
be	O
entirely	O
meaning	O
something	O
that	O
entirely	O
unrelated	O
to	O
the	O
profession	O
word	O
's	O
meaning	O
of	O
serving	O
drinks	O
.	O
meaning	O
of	O
'	O
a	O
water	O
creek	O
or	O
an	O
aqueduct	O
'	O
(	O
)	O
.	O
To	O
thoroughly	O
investigate	O
the	O
occurrence	O
of	O
those	O
profession	O
words	O
regarding	O
their	O
correlation	O
with	O
water	O
-	O
the	O
allowed	O
/	O
halal	O
drink	O
,	O
and	O
the	O
wine	O
-	O
-	O
the	O
forbidden	O
/	O
haram	O
drink	O
in	O
Islam	O
,	O
we	O
manually	O
analyzed	O
the	O
Arabic	O
poems	O
of	O
each	O
age	O
and	O
decided	O
whether	O
that	O
word	O
occurrence	O
is	O
a	O
waterrelated	O
meaning	O
,	O
wine	O
-	O
related	O
meaning	O
,	O
or	O
other	O
unrelated	O
meanings	O
to	O
both	O
of	O
the	O
drinks	O
.	O
Figure	O
2a	O
shows	O
that	O
the	O
male	O
bartender	O
(	O
)	O
profession	O
word	O
started	O
to	O
appear	O
in	O
the	O
Arabic	O
poems	O
as	O
a	O
profession	O
of	O
serving	O
alcohol	O
generally	O
,	O
wine	O
exclusively	O
,	O
as	O
a	O
symbol	O
of	O
love	O
,	O
passion	O
,	O
and	O
adoration	O
for	O
women	O
from	O
the	O
age	O
of	O
between	O
Umayyad	O
and	O
Abbasid	O
until	O
the	O
Modern	O
age	O
.	O
Similarly	O
,	O
in	O
Figure	O
2b	O
,	O
the	O
female	O
bartender	O
(	O
)	O
started	O
to	O
appear	O
as	O
a	O
profession	O
of	O
serving	O
wine	O
from	O
the	O
age	O
of	O
between	O
Umayyad	O
and	O
Abbasid	O
until	O
the	O
Modern	O
age	O
as	O
same	O
as	O
the	O
male	O
bartender	O
(	O
)	O
profession	O
word	O
,	O
except	O
they	O
did	O
not	O
appear	O
in	O
the	O
two	O
ages	O
of	O
Ayyubid	O
and	O
Ottoman	O
.	O
While	O
the	O
female	O
and	O
male	O
bartender	O
(	O
)	O
surprisingly	O
appeared	O
in	O
correlation	O
with	O
wine	O
in	O
the	O
Arabic	O
poems	O
despite	O
its	O
religious	O
forbiddance	O
,	O
both	O
of	O
the	O
two	O
profession	O
words	O
also	O
refer	O
to	O
water	O
-	O
related	O
words	O
.	O
For	O
example	O
,	O
the	O
female	O
bartender	O
(	O
)	O
refers	O
to	O
the	O
'	O
water	O
creek	O
or	O
aqueduct	O
.	O
'	O
One	O
example	O
to	O
show	O
that	O
is	O
when	O
the	O
Modern	O
Arabic	O
poet	O
,	O
Rashid	O
Ayoub	O
(	O
)	O
,	O
said	O
in	O
his	O
poem	O
:	O
"	O
I	O
sat	O
in	O
the	O
meadow	O
alone	O
at	O
the	O
water	O
creek	O
,	O
in	O
which	O
the	O
water	O
echoed	O
the	O
sound	O
of	O
my	O
melodies	O
"	O
,	O

We	O
'd	O
like	O
to	O
thank	O
the	O
Clarkson	O
Open	O
Source	O
Institute	O
for	O
their	O
help	O
and	O
support	O
with	O
infrastructure	O
and	O
hosting	O
of	O
our	O
experiments	O
.	O
We	O
'd	O
like	O
to	O
thank	O
Abigail	O
Matthews	O
and	O
Thomas	O
Middleton	O
for	O
their	O
help	O
and	O
support	O
in	O
writing	O
and	O
reviewing	O
the	O
manuscript	O
.	O

Specializing	O
Multilingual	O
Language	O
Models	O
:	O
An	O
Empirical	O
Study	O

Research	O
in	O
natural	O
language	O
processing	O
is	O
increasingly	O
carried	O
out	O
in	O
languages	O
beyond	O
English	O
.	O
This	O
includes	O
high	O
-	O
resource	O
languages	O
with	O
abundant	O
data	O
,	O
as	O
well	O
as	O
low	O
-	O
resource	O
languages	O
,	O
for	O
which	O
labeled	O
(	O
and	O
unlabeled	O
)	O
data	O
is	O
scarce	O
.	O
In	O
fact	O
,	O
many	O
of	O
the	O
world	O
's	O
languages	O
fall	O
into	O
the	O
latter	O
category	O
,	O
even	O
some	O
with	O
a	O
high	O
number	O
of	O
speakers	O
.	O
This	O
presents	O
unique	O
challenges	O
compared	O
to	O
high	O
-	O
resource	O
languages	O
:	O
effectively	O
modeling	O
low	O
-	O
resource	O
languages	O
involves	O
both	O
accurately	O
tokenizing	O
text	O
in	O
such	O
languages	O
and	O
maximally	O
leveraging	O
the	O
limited	O
available	O
data	O
.	O
One	O
common	O
approach	O
to	O
low	O
-	O
resource	O
NLP	O
is	O
the	O
multilingual	O
paradigm	O
,	O
in	O
which	O
methods	O
that	O
have	O
shown	O
success	O
in	O
English	O
are	O
applied	O
to	O
the	O
union	O
of	O
many	O
languages	O
'	O
data	O
,	O
1	O
enabling	O
transfer	O
between	O
languages	O
.	O
For	O
instance	O
,	O
multilingual	O
contextual	O
word	O
representations	O
(	O
CWRs	O
)	O
from	O
language	O
models	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
;	O
Huang	O
et	O
al	O
,	O
2019	O
;	O
Lample	O
and	O
Conneau	O
,	O
2019	O
,	O
inter	O
alia	O
)	O
are	O
conventionally	O
"	O
pretrained	O
"	O
on	O
large	O
multilingual	O
1	O
Within	O
the	O
multilingual	O
paradigm	O
,	O
a	O
distinction	O
is	O
sometimes	O
made	O
between	O
massively	O
multilingual	O
methods	O
,	O
which	O
consider	O
tens	O
or	O
hundreds	O
of	O
languages	O
;	O
and	O
polyglot	O
methods	O
,	O
which	O
use	O
only	O
a	O
handful	O
.	O
In	O
this	O
paper	O
,	O
all	O
mentions	O
of	O
"	O
multilingual	O
"	O
refer	O
to	O
the	O
former	O
.	O
corpora	O
before	O
being	O
"	O
finetuned	O
"	O
directly	O
on	O
supervised	O
tasks	O
;	O
this	O
pretraining	O
-	O
finetuning	O
approach	O
is	O
derived	O
from	O
analogous	O
monolingual	O
models	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
;	O
Liu	O
et	O
al	O
,	O
2019	O
;	O
.	O
However	O
,	O
considering	O
the	O
diversity	O
of	O
the	O
world	O
's	O
languages	O
and	O
the	O
great	O
data	O
imbalance	O
among	O
them	O
,	O
it	O
is	O
natural	O
to	O
question	O
whether	O
the	O
current	O
multilingual	O
paradigm	O
can	O
be	O
improved	O
upon	O
for	O
low	O
-	O
resource	O
languages	O
.	O
Indeed	O
,	O
past	O
work	O
has	O
demonstrated	O
that	O
it	O
can	O
.	O
For	O
instance	O
,	O
Wu	O
and	O
Dredze	O
(	O
2020	O
)	O
find	O
that	O
multilingual	O
models	O
often	O
lag	O
behind	O
non	O
-	O
contextualized	O
baselines	O
for	O
the	O
lowest	O
-	O
resource	O
languages	O
in	O
their	O
training	O
data	O
,	O
drawing	O
into	O
question	O
their	O
utility	O
in	O
such	O
settings	O
.	O
Conneau	O
et	O
al	O
(	O
2020a	O
)	O
posit	O
that	O
this	O
phenomenon	O
is	O
a	O
result	O
of	O
limited	O
model	O
capacity	O
,	O
which	O
proves	O
to	O
be	O
a	O
bottleneck	O
for	O
sufficient	O
transfer	O
to	O
low	O
-	O
resource	O
languages	O
.	O
In	O
fact	O
,	O
with	O
multilingual	O
models	O
only	O
being	O
pretrained	O
on	O
a	O
limited	O
set	O
of	O
languages	O
,	O
most	O
of	O
the	O
world	O
's	O
languages	O
are	O
unseen	O
by	O
the	O
model	O
.	O
For	O
such	O
languages	O
,	O
the	O
performance	O
of	O
such	O
models	O
is	O
even	O
worse	O
(	O
Chau	O
et	O
al	O
,	O
2020	O
)	O
,	O
due	O
in	O
part	O
to	O
the	O
diversity	O
of	O
scripts	O
across	O
the	O
world	O
's	O
languages	O
(	O
Muller	O
et	O
al	O
,	O
2021	O
;	O
Pfeiffer	O
et	O
al	O
,	O
2021b	O
;	O
Rust	O
et	O
al	O
,	O
2021	O
)	O
as	O
compared	O
to	O
the	O
models	O
'	O
Latin	O
-	O
centricity	O
(	O
Ács	O
,	O
2019	O
)	O
.	O
Nonetheless	O
,	O
there	O
have	O
been	O
multiple	O
attempts	O
to	O
remedy	O
this	O
discrepancy	O
by	O
specializing	O
2	O
a	O
multilingual	O
model	O
to	O
a	O
given	O
target	O
low	O
-	O
resource	O
language	O
,	O
from	O
which	O
we	O
take	O
inspiration	O
.	O
Among	O
them	O
,	O
Chau	O
et	O
al	O
(	O
2020	O
)	O
augment	O
the	O
model	O
's	O
vocabulary	O
to	O
more	O
effectively	O
tokenize	O
text	O
,	O
then	O
pretrain	O
on	O
a	O
small	O
amount	O
of	O
data	O
in	O
the	O
target	O
language	O
;	O
they	O
report	O
significant	O
performance	O
improvements	O
on	O
a	O
small	O
set	O
of	O
low	O
-	O
resource	O
languages	O
.	O
In	O
a	O
similar	O
vein	O
,	O
Muller	O
et	O
al	O
(	O
2021	O
)	O
propose	O
to	O
transliterate	O
text	O
in	O
the	O
target	O
language	O
to	O
Latin	O
script	O
to	O
be	O
better	O
tokenized	O
by	O
the	O
existing	O
model	O
,	O
followed	O
by	O
additional	O
pretraining	O
;	O
they	O
observe	O
mixed	O
results	O
and	O
note	O
that	O
transliteration	O
quality	O
may	O
be	O
a	O
confounding	O
factor	O
.	O
We	O
hypothesize	O
that	O
these	O
two	O
methods	O
can	O
serve	O
as	O
the	O
basis	O
for	O
improvements	O
in	O
modeling	O
a	O
broad	O
set	O
of	O
low	O
-	O
resource	O
languages	O
.	O
In	O
this	O
work	O
,	O
we	O
study	O
the	O
effectiveness	O
,	O
extensibility	O
,	O
and	O
interaction	O
of	O
these	O
two	O
approaches	O
to	O
specialization	O
:	O
the	O
vocabulary	O
augmentation	O
technique	O
of	O
Chau	O
et	O
al	O
(	O
2020	O
)	O
and	O
the	O
script	O
transliteration	O
method	O
of	O
Muller	O
et	O
al	O
(	O
2021	O
)	O
.	O
We	O
verify	O
the	O
performance	O
of	O
vocabulary	O
augmentation	O
on	O
three	O
tasks	O
in	O
a	O
diverse	O
set	O
of	O
nine	O
low	O
-	O
resource	O
languages	O
across	O
three	O
different	O
scripts	O
,	O
especially	O
on	O
non	O
-	O
Latin	O
scripts	O
(	O
2	O
)	O
and	O
find	O
that	O
these	O
gains	O
are	O
associated	O
with	O
improved	O
vocabulary	O
coverage	O
of	O
the	O
target	O
language	O
.	O
We	O
further	O
observe	O
a	O
negative	O
interaction	O
between	O
vocabulary	O
augmentation	O
and	O
transliteration	O
in	O
light	O
of	O
a	O
broader	O
framework	O
for	O
specializing	O
multilingual	O
models	O
,	O
while	O
noting	O
that	O
vocabulary	O
augmentation	O
offers	O
an	O
appealing	O
balance	O
of	O
performance	O
and	O
cost	O
(	O
3	O
)	O
.	O
Overall	O
,	O
our	O
results	O
highlight	O
several	O
possible	O
directions	O
for	O
future	O
study	O
in	O
the	O
low	O
-	O
resource	O
setting	O
.	O
Our	O
code	O
,	O
data	O
,	O
and	O
hyperparameters	O
are	O
publicly	O
available	O
.	O
3	O

We	O
begin	O
by	O
revisiting	O
the	O
Vocabulary	O
Augmentation	O
method	O
of	O
Chau	O
et	O
al	O
(	O
2020	O
)	O
,	O
which	O
we	O
recast	O
more	O
generally	O
in	O
light	O
of	O
recent	O
work	O
(	O
2.1	O
)	O
.	O
We	O
evaluate	O
their	O
claims	O
on	O
three	O
different	O
tasks	O
,	O
using	O
a	O
diverse	O
set	O
of	O
languages	O
in	O
multiple	O
scripts	O
(	O
2.2	O
)	O
,	O
and	O
find	O
that	O
the	O
results	O
hold	O
to	O
an	O
even	O
more	O
pronounced	O
degree	O
in	O
unseen	O
low	O
-	O
resource	O
languages	O
with	O
non	O
-	O
Latin	O
scripts	O
(	O
2.3	O
)	O
.	O

To	O
test	O
this	O
research	O
question	O
,	O
we	O
apply	O
transliteration	O
and	O
VA	O
in	O
succession	O
and	O
evaluate	O
their	O
compatibility	O
.	O
Given	O
unlabeled	O
data	O
in	O
the	O
target	O
language	O
,	O
we	O
first	O
transliterate	O
it	O
into	O
Latin	O
script	O
,	O
which	O
decreases	O
but	O
does	O
not	O
fully	O
eliminate	O
the	O
issue	O
of	O
unseen	O
wordpieces	O
.	O
We	O
then	O
perform	O
VA	O
,	O
generating	O
the	O
vocabulary	O
for	O
augmentation	O
based	O
on	O
the	O
transliterated	O
data	O
.	O
We	O
evaluate	O
on	O
Meadow	O
Mari	O
and	O
Uyghur	O
,	O
which	O
are	O
Type	O
2	O
languages	O
where	O
transliteration	O
was	O
successfully	O
applied	O
by	O
Muller	O
et	O
al	O
(	O
2021	O
)	O
.	O
To	O
transliterate	O
the	O
data	O
,	O
we	O
use	O
the	O
same	O
methods	O
as	O
Muller	O
et	O
al	O
(	O
2021	O
)	O
:	O
Meadow	O
Mari	O
uses	O
the	O
transliterate	O
8	O
package	O
,	O
while	O
Uyghur	O
uses	O
a	O
linguistically	O
-	O
motivated	O
transliteration	O
scheme	O
9	O
aimed	O
at	O
associating	O
Uyghur	O
with	O
Turkish	O
.	O
We	O
use	O
the	O
same	O
training	O
scheme	O
,	O
model	O
architectures	O
,	O
and	O
baselines	O
as	O
in	O
2.2	O
,	O
the	O
only	O
difference	O
being	O
the	O
use	O
of	O
transliterated	O
data	O
.	O
This	O
includes	O
directly	O
pretraining	O
on	O
the	O
unlabeled	O
data	O
(	O
LAPT	O
)	O
,	O
which	O
is	O
comparable	O
to	O
the	O
highest	O
-	O
performing	O
transliteration	O
models	O
of	O
Muller	O
et	O
al	O
(	O
2021	O
)	O
.	O
Although	O
our	O
initial	O
investigation	O
of	O
VA	O
in	O
2	O
also	O
included	O
non	O
-	O
Type	O
2	O
languages	O
of	O
other	O
scripts	O
,	O
we	O
omit	O
them	O
from	O
our	O
investigation	O
based	O
on	O
the	O
finding	O
of	O
Muller	O
et	O
al	O
(	O
2021	O
)	O
that	O
transliterating	O
higherresource	O
languages	O
into	O
Latin	O
scripts	O
is	O
not	O
beneficial	O
.	O

We	O
thank	O
Jungo	O
Kasai	O
,	O
Phoebe	O
Mulcaire	O
,	O
and	O
members	O
of	O
UW	O
NLP	O
for	O
their	O
helpful	O
comments	O
on	O
preliminary	O
versions	O
of	O
this	O
paper	O
.	O
We	O
also	O
thank	O
Benjamin	O
Muller	O
for	O
insightful	O
discussions	O
and	O
providing	O
details	O
about	O
transliteration	O
methods	O
and	O
baselines	O
.	O
Finally	O
,	O
we	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
helpful	O
remarks	O
.	O

Learning	O
to	O
Predict	O
Denotational	O
Probabilities	O
For	O
Modeling	O
Entailment	O

In	O
order	O
to	O
bridge	O
the	O
gap	O
between	O
vector	O
-	O
based	O
distributional	O
approaches	O
to	O
lexical	O
semantics	O
that	O
are	O
intended	O
to	O
capture	O
which	O
words	O
occur	O
in	O
similar	O
contexts	O
,	O
and	O
logic	O
-	O
based	O
approaches	O
to	O
compositional	O
semantics	O
that	O
are	O
intended	O
to	O
capture	O
the	O
truth	O
conditions	O
under	O
which	O
statements	O
hold	O
,	O
Young	O
et	O
al	O
(	O
2014	O
)	O
introduced	O
the	O
concept	O
of	O
"	O
denotational	O
similarity	O
.	O
"	O
Denotational	O
similarity	O
is	O
intended	O
to	O
measure	O
the	O
similarity	O
of	O
simple	O
,	O
declarative	O
statements	O
in	O
terms	O
of	O
the	O
similarity	O
of	O
their	O
truth	O
conditions	O
.	O
From	O
classical	O
truth	O
-	O
conditional	O
semantics	O
,	O
Young	O
et	O
al	O
borrowed	O
the	O
notion	O
of	O
the	O
denotation	O
of	O
a	O
declarative	O
sentence	O
s	O
,	O
s	O
,	O
as	O
the	O
set	O
of	O
possible	O
worlds	O
in	O
which	O
the	O
sentence	O
is	O
true	O
.	O
Young	O
et	O
al	O
apply	O
this	O
concept	O
to	O
the	O
domain	O
of	O
image	O
descriptions	O
by	O
defining	O
the	O
visual	O
denotation	O
of	O
a	O
sentence	O
s	O
as	O
the	O
set	O
of	O
images	O
that	O
s	O
describes	O
.	O
The	O
denotational	O
probability	O
of	O
s	O
,	O
P	O
(	O
s	O
)	O
,	O
is	O
the	O
number	O
of	O
images	O
in	O
the	O
visual	O
denotation	O
of	O
s	O
over	O
the	O
size	O
of	O
the	O
corpus	O
.	O
Two	O
sentences	O
are	O
denotationally	O
similar	O
if	O
the	O
sets	O
of	O
images	O
(	O
possible	O
worlds	O
)	O
they	O
describe	O
have	O
a	O
large	O
overlap	O
.	O
For	O
example	O
,	O
"	O
A	O
woman	O
is	O
jogging	O
on	O
a	O
beach	O
"	O
and	O
"	O
A	O
woman	O
is	O
running	O
on	O
a	O
sandy	O
shore	O
"	O
can	O
often	O
be	O
used	O
to	O
describe	O
the	O
same	O
scenario	O
,	O
so	O
they	O
will	O
have	O
a	O
large	O
image	O
overlap	O
that	O
corresponds	O
to	O
high	O
denotational	O
similarity	O
.	O
Given	O
the	O
above	O
definitions	O
,	O
Young	O
et	O
al	O
estimate	O
the	O
denotational	O
probabilities	O
of	O
phrases	O
from	O
FLICKR30	O
K	O
,	O
a	O
corpus	O
of	O
30	O
,	O
000	O
images	O
,	O
each	O
paired	O
with	O
five	O
descriptive	O
captions	O
.	O
Young	O
et	O
al	O
(	O
2014	O
)	O
and	O
showed	O
that	O
these	O
similarities	O
are	O
complementary	O
to	O
standard	O
distributional	O
similarities	O
,	O
and	O
potentially	O
more	O
useful	O
for	O
semantic	O
tasks	O
that	O
involve	O
entailment	O
.	O
However	O
,	O
the	O
systems	O
presented	O
in	O
these	O
papers	O
were	O
restricted	O
to	O
looking	O
up	O
the	O
denotational	O
similarities	O
of	O
frequent	O
phrases	O
in	O
the	O
training	O
data	O
.	O
In	O
this	O
paper	O
,	O
we	O
go	O
beyond	O
this	O
prior	O
work	O
and	O
define	O
a	O
model	O
that	O
can	O
predict	O
the	O
denotational	O
probabilities	O
of	O
novel	O
phrases	O
and	O
sentences	O
.	O
Our	O
experimental	O
results	O
indicate	O
that	O
these	O
predicted	O
denotational	O
probabilities	O
are	O
useful	O
for	O
several	O
textual	O
entailment	O
datasets	O
.	O

Section	O
7	O
has	O
demonstrated	O
that	O
we	O
can	O
successfully	O
learn	O
to	O
predict	O
denotational	O
probabilities	O
for	O
phrases	O
that	O
we	O
have	O
not	O
encountered	O
during	O
training	O
and	O
for	O
longer	O
sentences	O
.	O
We	O
examine	O
examples	O
of	O
predicted	O
conditional	O
probabilities	O
for	O
phrase	O
and	O
sentence	O
pairs	O
to	O
analyze	O
our	O
model	O
's	O
strengths	O
and	O
weaknesses	O
.	O
Table	O
4	O
has	O
example	O
predictions	O
from	O
the	O
denotation	O
phrase	O
development	O
data	O
.	O
Our	O
model	O
correctly	O
predicts	O
high	O
conditional	O
probability	O
for	O
entailed	O
phrase	O
pairs	O
even	O
when	O
there	O
is	O
no	O
direct	O
hypernym	O
involved	O
,	O
as	O
in	O
example	O
2	O
,	O
and	O
for	O
closely	O
related	O
phrases	O
that	O
are	O
not	O
strictly	O
entailing	O
,	O
as	O
in	O
example	O
3	O
.	O
Our	O
model	O
also	O
predicts	O
reasonable	O
probabilities	O
for	O
events	O
that	O
frequently	O
co	O
-	O
occur	O
but	O
are	O
not	O
required	O
to	O
do	O
so	O
,	O
such	O
as	O
example	O
7	O
.	O
A	O
person	O
is	O
on	O
a	O
beach	O
.	O
0.88	O
2	O
Two	O
women	O
having	O
drinks	O
and	O
smoking	O
cigarettes	O
at	O
the	O
bar	O
.	O
Two	O
women	O
are	O
at	O
a	O
bar	O
.	O
0.86	O
3	O
A	O
senior	O
is	O
waiting	O
at	O
the	O
window	O
of	O
a	O
restaurant	O
that	O
serves	O
sandwiches	O
.	O
A	O
person	O
waits	O
to	O
be	O
served	O
his	O
food	O
.	O
0.61	O
John	O
Deere	O
equipment	O
is	O
being	O
worked	O
on	O
by	O
two	O
farmers	O
.	O

We	O
have	O
presented	O
a	O
framework	O
for	O
representing	O
denotational	O
probabilities	O
in	O
a	O
vector	O
space	O
,	O
and	O
demonstrated	O
that	O
we	O
can	O
successfully	O
train	O
a	O
neural	O
network	O
model	O
to	O
predict	O
these	O
probabilities	O
for	O
new	O
phrases	O
.	O
We	O
have	O
shown	O
that	O
when	O
also	O
trained	O
on	O
longer	O
sentences	O
with	O
approximate	O
probabilities	O
,	O
our	O
model	O
can	O
learn	O
reasonable	O
representations	O
for	O
these	O
longer	O
sentences	O
.	O
We	O
have	O
also	O
shown	O
that	O
our	O
model	O
's	O
predicted	O
probabilities	O
are	O
useful	O
for	O
textual	O
entailment	O
,	O
and	O
provide	O
additional	O
gains	O
in	O
performance	O
when	O
added	O
to	O
existing	O
competitive	O
textual	O
entailment	O
classifiers	O
.	O
Future	O
work	O
will	O
examine	O
whether	O
the	O
embeddings	O
our	O
model	O
learns	O
can	O
be	O
used	O
directly	O
by	O
these	O
classifiers	O
,	O
and	O
explore	O
how	O
to	O
incorporate	O
negation	O
into	O
our	O
model	O
.	O

Learning	O
and	O
Reasoning	O
for	O
Robot	O
Dialog	O
and	O
Navigation	O
Tasks	O

Reinforcement	O
learning	O
and	O
probabilistic	O
reasoning	O
algorithms	O
aim	O
at	O
learning	O
from	O
interaction	O
experiences	O
and	O
reasoning	O
with	O
probabilistic	O
contextual	O
knowledge	O
respectively	O
.	O
In	O
this	O
research	O
,	O
we	O
develop	O
algorithms	O
for	O
robot	O
task	O
completions	O
,	O
while	O
looking	O
into	O
the	O
complementary	O
strengths	O
of	O
reinforcement	O
learning	O
and	O
probabilistic	O
reasoning	O
techniques	O
.	O
The	O
robots	O
learn	O
from	O
trial	O
-	O
and	O
-	O
error	O
experiences	O
to	O
augment	O
their	O
declarative	O
knowledge	O
base	O
,	O
and	O
the	O
augmented	O
knowledge	O
can	O
be	O
used	O
for	O
speeding	O
up	O
the	O
learning	O
process	O
in	O
potentially	O
different	O
tasks	O
.	O
We	O
have	O
implemented	O
and	O
evaluated	O
the	O
developed	O
algorithms	O
using	O
mobile	O
robots	O
conducting	O
dialog	O
and	O
navigation	O
tasks	O
.	O
From	O
the	O
results	O
,	O
we	O
see	O
that	O
our	O
robot	O
's	O
performance	O
can	O
be	O
improved	O
by	O
both	O
reasoning	O
with	O
human	O
knowledge	O
and	O
learning	O
from	O
task	O
-	O
completion	O
experience	O
.	O
More	O
interestingly	O
,	O
the	O
robot	O
was	O
able	O
to	O
learn	O
from	O
navigation	O
tasks	O
to	O
improve	O
its	O
dialog	O
strategies	O
.	O

Knowledge	O
representation	O
and	O
reasoning	O
(	O
KRR	O
)	O
and	O
reinforcement	O
learning	O
(	O
RL	O
)	O
are	O
two	O
important	O
research	O
areas	O
in	O
artificial	O
intelligence	O
(	O
AI	O
)	O
and	O
have	O
been	O
applied	O
to	O
a	O
variety	O
of	O
problems	O
in	O
robotics	O
.	O
On	O
the	O
one	O
hand	O
,	O
KRR	O
research	O
aims	O
to	O
concisely	O
represent	O
knowledge	O
,	O
and	O
robustly	O
draw	O
conclusions	O
with	O
the	O
knowledge	O
(	O
or	O
generate	O
new	O
knowledge	O
)	O
.	O
Knowledge	O
in	O
KRR	O
is	O
typically	O
provided	O
by	O
human	O
experts	O
in	O
the	O
form	O
of	O
declarative	O
rules	O
.	O
Although	O
KRR	O
paradigms	O
are	O
strong	O
in	O
representing	O
and	O
reasoning	O
with	O
knowledge	O
in	O
a	O
variety	O
of	O
forms	O
,	O
they	O
are	O
not	O
designed	O
for	O
(	O
and	O
hence	O
not	O
good	O
at	O
)	O
learning	O
from	O
experiences	O
of	O
accomplishing	O
the	O
tasks	O
.	O
On	O
the	O
other	O
hand	O
,	O
RL	O
algorithms	O
enable	O
agents	O
to	O
learn	O
by	O
interacting	O
with	O
an	O
environment	O
,	O
and	O
RL	O
agents	O
are	O
good	O
at	O
learning	O
action	O
policies	O
from	O
trial	O
-	O
and	O
-	O
error	O
experiences	O
toward	O
maximizing	O
long	O
-	O
term	O
rewards	O
un	O
-	O
der	O
uncertainty	O
,	O
but	O
they	O
are	O
ill	O
-	O
equipped	O
to	O
utilize	O
declarative	O
knowledge	O
from	O
human	O
experts	O
.	O
Motivated	O
by	O
the	O
complementary	O
features	O
of	O
KRR	O
and	O
RL	O
,	O
we	O
aim	O
at	O
a	O
framework	O
that	O
integrates	O
both	O
paradigms	O
to	O
enable	O
agents	O
(	O
robots	O
in	O
our	O
case	O
)	O
to	O
simultaneously	O
reason	O
with	O
declarative	O
knowledge	O
and	O
learn	O
by	O
interacting	O
with	O
an	O
environment	O
.	O
Most	O
KRR	O
paradigms	O
support	O
the	O
representation	O
and	O
reasoning	O
of	O
knowledge	O
in	O
logical	O
form	O
,	O
e.g.	O
,	O
Prolog	O
-	O
style	O
.	O
More	O
recently	O
,	O
researchers	O
have	O
developed	O
hybrid	O
KRR	O
paradigms	O
that	O
support	O
both	O
logical	O
and	O
probabilistic	O
knowledge	O
(	O
Richardson	O
and	O
Domingos	O
,	O
2006	O
;	O
Bach	O
et	O
al	O
,	O
2017	O
;	O
Wang	O
et	O
al	O
,	O
2019	O
)	O
.	O
Such	O
logical	O
-	O
probabilistic	O
KRR	O
paradigms	O
can	O
be	O
used	O
for	O
a	O
variety	O
of	O
reasoning	O
tasks	O
.	O
We	O
use	O
P	O
-	O
log	O
(	O
Baral	O
et	O
al	O
,	O
2009	O
)	O
in	O
this	O
work	O
to	O
represent	O
and	O
reason	O
with	O
both	O
human	O
knowledge	O
and	O
the	O
knowledge	O
from	O
RL	O
.	O
The	O
reasoning	O
results	O
are	O
then	O
used	O
by	O
our	O
robot	O
to	O
compute	O
action	O
policies	O
at	O
runtime	O
.	O
Reinforcement	O
learning	O
(	O
RL	O
)	O
algorithms	O
can	O
be	O
used	O
to	O
help	O
robots	O
learn	O
action	O
policies	O
from	O
the	O
experience	O
of	O
interacting	O
with	O
the	O
real	O
world	O
(	O
Sutton	O
and	O
Barto	O
,	O
2018	O
)	O
.	O
We	O
use	O
model	O
-	O
based	O
RL	O
in	O
this	O
work	O
,	O
because	O
the	O
learned	O
world	O
model	O
can	O
be	O
used	O
to	O
update	O
the	O
robot	O
's	O
declarative	O
knowledge	O
base	O
and	O
combined	O
with	O
human	O
knowledge	O
.	O
Theoretical	O
Contribution	O
:	O
In	O
this	O
paper	O
,	O
we	O
develop	O
a	O
learning	O
and	O
reasoning	O
framework	O
(	O
called	O
KRR	O
-	O
RL	O
)	O
that	O
integrates	O
logical	O
-	O
probabilistic	O
KRR	O
and	O
model	O
-	O
based	O
RL	O
.	O
The	O
KRR	O
component	O
reasons	O
with	O
the	O
qualitative	O
knowledge	O
from	O
humans	O
(	O
e.g.	O
,	O
it	O
is	O
difficult	O
for	O
a	O
robot	O
to	O
navigate	O
through	O
a	O
busy	O
area	O
)	O
and	O
the	O
quantitative	O
knowledge	O
from	O
modelbased	O
RL	O
(	O
e.g.	O
,	O
a	O
navigation	O
action	O
's	O
success	O
rate	O
in	O
the	O
form	O
of	O
a	O
probability	O
)	O
.	O
The	O
hybrid	O
knowledge	O
is	O
then	O
used	O
for	O
computing	O
action	O
policies	O
at	O
runtime	O
by	O
planning	O
with	O
task	O
-	O
oriented	O
partial	O
world	O
models	O
.	O
KRR	O
-	O
RL	O
enables	O
a	O
robot	O
to	O
:	O
i	O
)	O
represent	O
the	O
probabilistic	O
knowledge	O
(	O
i.e.	O
,	O
world	O
dynamics	O
)	O
learned	O
from	O
RL	O
in	O
declarative	O
form	O
;	O
ii	O
)	O
unify	O
and	O
reason	O
with	O
both	O
human	O
knowledge	O
and	O
the	O
knowledge	O
from	O
RL	O
;	O
and	O
iii	O
)	O
compute	O
policies	O
at	O
runtime	O
by	O
dynamically	O
constructing	O
task	O
-	O
oriented	O
partial	O
world	O
models	O
.	O
Application	O
Domain	O
:	O
We	O
use	O
a	O
robot	O
delivery	O
domain	O
for	O
demonstration	O
and	O
evaluation	O
purposes	O
,	O
where	O
the	O
robot	O
needs	O
to	O
dialog	O
with	O
people	O
to	O
figure	O
out	O
the	O
delivery	O
task	O
's	O
goal	O
location	O
,	O
and	O
then	O
physically	O
take	O
navigation	O
actions	O
to	O
complete	O
the	O
delivery	O
task	O
Veloso	O
,	O
2018	O
)	O
.	O
A	O
delivery	O
is	O
deemed	O
successful	O
only	O
if	O
both	O
the	O
dialog	O
and	O
navigation	O
subtasks	O
are	O
successfully	O
conducted	O
.	O
We	O
have	O
conducted	O
experiments	O
using	O
a	O
simulated	O
mobile	O
robot	O
,	O
as	O
well	O
as	O
demonstrated	O
the	O
system	O
using	O
a	O
real	O
mobile	O
robot	O
.	O
Results	O
show	O
that	O
the	O
robot	O
is	O
able	O
to	O
learn	O
world	O
dynamics	O
from	O
navigation	O
tasks	O
through	O
model	O
-	O
based	O
RL	O
,	O
and	O
apply	O
the	O
learned	O
knowledge	O
to	O
both	O
navigation	O
tasks	O
(	O
with	O
different	O
goals	O
)	O
and	O
delivery	O
tasks	O
(	O
that	O
require	O
subtasks	O
of	O
navigation	O
and	O
dialog	O
)	O
through	O
logical	O
-	O
probabilistic	O
reasoning	O
.	O
In	O
particular	O
,	O
we	O
observed	O
that	O
the	O
robot	O
is	O
able	O
to	O
adjust	O
its	O
dialog	O
strategy	O
through	O
learning	O
from	O
navigation	O
behaviors	O
.	O

We	O
briefly	O
describe	O
the	O
two	O
most	O
important	O
building	O
blocks	O
of	O
this	O
research	O
,	O
namely	O
model	O
-	O
based	O
RL	O
and	O
hybrid	O
KRR	O
.	O

Cold	O
start	O
is	O
a	O
common	O
problem	O
in	O
recommender	O
systems	O
(	O
Schein	O
et	O
al	O
,	O
2002	O
;	O
Zhang	O
et	O
al	O
,	O
2014	O
;	O
.	O
This	O
also	O
applies	O
to	O
dialogue	O
systems	O
,	O
as	O
the	O
partner	O
personas	O
are	O
commonly	O
missing	O
in	O
early	O
turns	O
.	O
We	O
conduct	O
an	O
analysis	O
on	O
the	O
baselines	O
and	O
our	O
framework	O
when	O
N	O
turns	O
are	O
available	O
where	O
N=	O
{	O
1	O
,	O
2	O
,	O
3	O
}	O
,	O
using	O
PERSONACHAT	O
-	O
ORI	O
.	O
As	O
demonstrated	O
in	O
Figure	O
3	O
,	O
all	O
the	O
methods	O
attain	O
a	O
better	O
PPL	O
when	O
N	O
increases	O
,	O
which	O
indicates	O
the	O
existence	O
of	O
the	O
cold	O
start	O
.	O
This	O
is	O
also	O
the	O
case	O
for	O
the	O
baseline	O
with	O
ground	O
truth	O
personas	O
,	O
and	O
we	O
postulate	O
that	O
it	O
fails	O
to	O
learn	O
how	O
to	O
use	O
partner	O
personas	O
during	O
cold	O
start	O
due	O
to	O
the	O
lack	O
of	O
clues	O
.	O
Our	O
framework	O
effectively	O
mitigates	O
the	O
cold	O
start	O
problem	O
and	O
attains	O
the	O
best	O
among	O
them	O
for	O
all	O
N.	O
our	O
framework	O
successfully	O
recognizes	O
that	O
the	O
partner	O
is	O
asking	O
specifically	O
for	O
metallica	O
.	O
It	O
then	O
conditions	O
on	O
the	O
generated	O
personas	O
to	O
generate	O
a	O
much	O
more	O
entailed	O
response	O
than	O
the	O
baseline	O
.	O
The	O
human	O
response	O
expresses	O
negatively	O
and	O
thus	O
seems	O
less	O
engaging	O
.	O
In	O
the	O
second	O
case	O
,	O
our	O
framework	O
recognizes	O
that	O
the	O
partner	O
has	O
a	O
garden	O
.	O
It	O
then	O
talks	O
about	O
the	O
garden	O
rather	O
than	O
the	O
irrelevant	O
response	O
from	O
the	O
baseline	O
that	O
we	O
postulate	O
is	O
misled	O
by	O
the	O
'	O
large	O
'	O
adjective	O
in	O
the	O
dialogue	O
context	O
.	O
The	O
human	O
response	O
is	O
potentially	O
sarcastic	O
if	O
the	O
partner	O
is	O
not	O
joking	O
,	O
while	O
our	O
generation	O
does	O
not	O
have	O
such	O
issue	O
.	O
For	O
the	O
third	O
case	O
,	O
the	O
baseline	O
produces	O
a	O
response	O
that	O
could	O
be	O
potentially	O
offensive	O
,	O
which	O
could	O
be	O
biased	O
by	O
the	O
word	O
'	O
violent	O
'	O
in	O
the	O
dialogue	O
context	O
.	O
In	O
contrast	O
,	O
our	O
framework	O
recognizes	O
the	O
identity	O
of	O
the	O
partner	O
to	O
generate	O
a	O
response	O
without	O
such	O
an	O
issue	O
.	O
The	O
human	O
response	O
tends	O
to	O
raise	O
a	O
new	O
topic	O
and	O
is	O
less	O
relevant	O
.	O
For	O
the	O
fourth	O
case	O
,	O
we	O
observe	O
that	O
the	O
annotator	O
sometimes	O
converses	O
based	O
on	O
the	O
partner	O
profile	O
rather	O
than	O
his	O
own	O
traits	O
.	O
In	O
this	O
case	O
,	O
the	O
annotator	O
(	O
Dialogue	O
Context	O
)	O
said	O
that	O
he	O
has	O
many	O
pets	O
,	O
which	O
is	O
not	O
in	O
his	O
own	O
traits	O
(	O
Gold	O
Partner	O
)	O
.	O
Rather	O
,	O
his	O
conversation	O
partner	O
expressed	O
his	O
passion	O
for	O
animals	O
in	O
previous	O
dialogue	O
contexts	O
.	O
We	O
postulate	O
that	O
the	O
annotator	O
attempted	O
to	O
engage	O
the	O
conversation	O
by	O
conditioning	O
his	O
partner	O
personas	O
and	O
telling	O
a	O
relevant	O
joke	O
.	O
Our	O
PPG	O
can	O
recognize	O
this	O
,	O
which	O
further	O
tweaks	O
the	O
model	O
output	O
to	O
talk	O
about	O
dogs	O
and	O
cats	O
rather	O
than	O
the	O
dog	O
only	O
.	O
These	O
cases	O
validate	O
that	O
leveraging	O
partner	O
personas	O
is	O
beneficial	O
,	O
and	O
our	O
framework	O
can	O
generate	O
reasonable	O
partner	O
personas	O
,	O
which	O
is	O
not	O
even	O
in	O
the	O
ground	O
truth	O
.	O

Table	O
3	O
presents	O
generated	O
partner	O
personas	O
using	O
PERSONACHAT	O
-	O
ORI	O
.	O
As	O
depicted	O
,	O
our	O
PPG	O
can	O
generate	O
reasonable	O
partner	O
personas	O
which	O
are	O
relevant	O
to	O
the	O
ground	O
truth	O
partner	O
personas	O
.	O
It	O
sometimes	O
gives	O
a	O
reasonable	O
generation	O
which	O
is	O
even	O
not	O
in	O
the	O
ground	O
truth	O
partner	O
personas	O
.	O
In	O
the	O
first	O
case	O
,	O
the	O
generator	O
successfully	O
identifies	O
the	O
partner	O
as	O
being	O
an	O
army	O
ranger	O
.	O
It	O
then	O
becomes	O
rather	O
positive	O
than	O
a	O
violent	O
person	O
as	O
given	O
in	O
the	O
ground	O
truth	O
personas	O
.	O
Conditioning	O
on	O
such	O
positive	O
contents	O
can	O
give	O
a	O
positive	O
response	O
.	O
In	O
the	O
second	O
case	O
,	O
it	O
recognizes	O
the	O
partner	O
as	O
a	O
gym	O
person	O
,	O
and	O
imagines	O
that	O
the	O
partner	O
drinks	O
protein	O
and	O
life	O
weights	O
,	O
which	O
is	O
not	O
in	O
the	O
ground	O
truth	O
personas	O
.	O
In	O
the	O
third	O
case	O
,	O
the	O
generator	O
generates	O
coherent	O
personas	O
,	O
saying	O
that	O
the	O
partner	O
would	O
drink	O
beer	O
and	O
eat	O
food	O
while	O
watching	O
football	O
,	O
which	O
is	O
also	O
not	O
in	O
the	O
ground	O
truth	O
.	O
We	O
postulate	O
that	O
personas	O
could	O
be	O
semantically	O
closer	O
to	O
each	O
other	O
when	O
they	O
frequently	O
co	O
-	O
occur	O
in	O
the	O
training	O
set	O
.	O
Our	O
PPG	O
then	O
tends	O
to	O
generate	O
more	O
coherent	O
personas	O
by	O
learning	O
such	O
semantical	O
relationship	O
.	O
Since	O
our	O
generated	O
personas	O
are	O
relevant	O
and	O
coherent	O
,	O
we	O
postulate	O
it	O
as	O
the	O
underlying	O
reason	O
why	O
our	O
method	O
gives	O
a	O
better	O
generalization	O
to	O
DRG	O
.	O
In	O
contrast	O
,	O
as	O
demonstrated	O
by	O
Table	O
3	O
,	O
ground	O
truth	O
personas	O
tend	O
to	O
be	O
more	O
like	O
discrete	O
collections	O
of	O
traits	O
.	O
This	O
could	O
be	O
the	O
reason	O
why	O
some	O
of	O
our	O
generated	O
partner	O
personas	O
could	O
beat	O
the	O
ground	O
truth	O
,	O
which	O
is	O
also	O
supported	O
by	O
our	O
human	O
evaluation	O
in	O
Section	O
5.6	O
.	O
This	O
is	O
a	O
potential	O
benefit	O
of	O
our	O
approach	O
compared	O
to	O
sentence	O
-	O
level	O
user	O
profile	O
extraction	O
(	O
Li	O
et	O
al	O
,	O
2014	O
;	O
Wu	O
et	O
al	O
,	O
2020b	O
;	O
that	O
is	O
upper	O
bounded	O
by	O
the	O
discrete	O
ground	O
truth	O
.	O
We	O
present	O
more	O
examples	O
in	O
Table	O
8	O
in	O
the	O
Appendix	O
.	O

The	O
PERSONACHAT	O
dataset	O
used	O
in	O
this	O
work	O
is	O
well	O
-	O
known	O
and	O
widely	O
used	O
.	O
In	O
our	O
view	O
,	O
there	O
is	O
no	O
known	O
ethical	O
issue	O
with	O
its	O
usage	O
.	O
Large	O
-	O
scale	O
pre	O
-	O
trained	O
models	O
are	O
also	O
employed	O
,	O
but	O
they	O
are	O
widely	O
known	O
to	O
be	O
subject	O
to	O
potential	O
problems	O
such	O
as	O
generating	O
offensiveness	O
context	O
.	O
With	O
its	O
use	O
,	O
our	O
partner	O
personas	O
generator	O
could	O
generate	O
unseen	O
personas	O
,	O
which	O
are	O
also	O
subject	O
to	O
potential	O
offensive	O
generation	O
.	O
An	O
offensiveness	O
check	O
can	O
be	O
incorporated	O
to	O
alleviate	O
this	O
problem	O
for	O
actual	O
usage	O
(	O
Baheti	O
et	O
al	O
,	O
2021	O
)	O
.	O

This	O
research	O
/	O
paper	O
was	O
supported	O
by	O
the	O
Center	O
for	O
Perceptual	O
and	O
Interactive	O
Intelligence	O
(	O
CPII	O
)	O
Ltd	O
under	O
the	O
Innovation	O
and	O
Technology	O
Commission	O
's	O
InnoHK	O
scheme	O
.	O

To	O
assure	O
a	O
seamless	O
annotation	O
procedure	O
,	O
the	O
supply	O
of	O
new	O
instances	O
has	O
to	O
be	O
reasonably	O
fast	O
.	O
The	O
generation	O
and	O
selection	O
of	O
the	O
next	O
instance	O
is	O
dependant	O
on	O
the	O
label	O
of	O
the	O
previous	O
instance	O
.	O
Because	O
of	O
this	O
,	O
there	O
is	O
no	O
way	O
to	O
pre	O
-	O
fetch	O
the	O
next	O
instance	O
in	O
the	O
background	O
and	O
the	O
annotator	O
has	O
to	O
wait	O
for	O
the	O
selection	O
/	O
generation	O
process	O
to	O
finish	O
before	O
the	O
next	O
instance	O
is	O
presented	O
for	O
annotation	O
.	O
However	O
,	O
the	O
runtime	O
for	O
pool	O
-	O
based	O
AL	O
methods	O
is	O
increasing	O
with	O
the	O
pool	O
's	O
size	O
.	O
In	O
contrast	O
,	O
the	O
generation	O
method	O
presented	O
in	O
this	O
work	O
does	O
not	O
have	O
this	O
limitation	O
.	O
The	O
least	O
confidence	O
baseline	O
has	O
a	O
complexity	O
of	O
O	O
(	O
n	O
)	O
where	O
n	O
is	O
the	O
number	O
of	O
instances	O
in	O
the	O
pool	O
.	O
The	O
complexity	O
of	O
nearest	O
neighbor	O
search	O
without	O
any	O
approximation	O
techniques	O
like	O
preclustering	O
is	O
also	O
O	O
(	O
n	O
)	O
.	O
Query	O
generation	O
from	O
an	O
exact	O
point	O
with	O
the	O
decoder	O
has	O
a	O
complexity	O
of	O
O	O
(	O
m	O
)	O
where	O
m	O
is	O
the	O
length	O
of	O
the	O
sentence	O
and	O
n	O
>	O
>	O
m.	O
Because	O
sentences	O
have	O
a	O
natural	O
length	O
limit	O
and	O
in	O
this	O
work	O
are	O
capped	O
to	O
15	O
words	O
,	O
one	O
could	O
argue	O
that	O
the	O
complexity	O
is	O
O	O
(	O
1	O
)	O
.	O
prototypical	O
positive	O
and	O
negative	O
instances	O
.	O
Example	O
7	O
is	O
ambiguous	O
,	O
caused	O
by	O
the	O
decoder	O
generating	O
an	O
unknown	O
(	O
UNK	O
)	O
token	O
at	O
the	O
position	O
where	O
one	O
would	O
normally	O
expect	O
an	O
evaluative	O
adjective	O
.	O
We	O
see	O
this	O
as	O
an	O
indicator	O
that	O
the	O
point	O
is	O
positioned	O
close	O
to	O
the	O
hyperplane	O
and	O
thus	O
the	O
sentiment	O
of	O
the	O
latent	O
variable	O
is	O
ambiguous	O
.	O
We	O
also	O
observe	O
instances	O
with	O
UNK	O
token	O
which	O
still	O
express	O
a	O
sentiment	O
,	O
as	O
seen	O
in	O
Example	O
8	O
an	O
9	O
.	O
This	O
can	O
be	O
interpreted	O
as	O
a	O
placeholder	O
for	O
a	O
named	O
entity	O
or	O
,	O
in	O
other	O
cases	O
,	O
a	O
specifier	O
like	O
movie	O
genre	O
and	O
does	O
not	O
impact	O
the	O
annotation	O
process	O
.	O

Part	O
of	O
this	O
research	O
has	O
been	O
conducted	O
within	O
the	O
Leibniz	O
Science	O
Campus	O
"	O
Empirical	O
Linguistics	O
and	O
Computational	O
Modeling	O
"	O
,	O
funded	O
by	O
the	O
Leibniz	O
Association	O
under	O
grant	O
no	O
.	O
SAS	O
-	O
2015	O
-	O
IDS	O
-	O
LWC	O
and	O
by	O
the	O
Ministry	O
of	O
Science	O
,	O
Research	O
,	O
and	O
Art	O
(	O
MWK	O
)	O
of	O
the	O
state	O
of	O
Baden	O
-	O
Württemberg	O
.	O

In	O
this	O
section	O
,	O
we	O
use	O
language	O
codes	O
8	O
to	O
represent	O
languages	O
,	O
and	O
use	O
MULTI	O
and	O
MIX	O
to	O
represent	O
multilingual	O
and	O
code	O
-	O
mixed	O
tracks	O
respectively	O
9	O
.	O

We	O
compare	O
with	O
some	O
variants	O
of	O
our	O
system	O
that	O
we	O
designed	O
but	O
did	O
not	O
use	O
in	O
the	O
test	O
phase	O
.	O

In	O
Table	O
8	O
,	O
we	O
show	O
the	O
effectiveness	O
of	O
multistage	O
fine	O
-	O
tuning	O
on	O
the	O
development	O
set	O
for	O
our	O
baseline	O
system	O
.	O
The	O
result	O
shows	O
that	O
multi	O
-	O
stage	O
fine	O
-	O
tuning	O
can	O
significantly	O
improve	O
the	O
model	O
performance	O
for	O
all	O
the	O
tracks	O
.	O

Performance	O
?	O
In	O
the	O
multilingual	O
test	O
set	O
,	O
we	O
can	O
find	O
304	O
,	O
905	O
sentences	O
in	O
the	O
other	O
monolingual	O
test	O
sets	O
while	O
there	O
are	O
167	O
,	O
006	O
sentences	O
that	O
can	O
not	O
be	O
found	O
.	O
For	O
these	O
sentences	O
,	O
we	O
can	O
either	O
search	O
on	O
the	O
whole	O
KB	O
of	O
all	O
languages	O
or	O
first	O
detect	O
the	O
language	O
of	O
the	O
input	O
sentence	O
and	O
then	O
search	O
in	O
the	O
specific	O
language	O
KB	O
14	O
.	O
Moreover	O
,	O
as	O
we	O
discussed	O
in	O
Section	O
5.4	O
,	O
using	O
different	O
kinds	O
of	O
retrieved	O
knowledge	O
affects	O
the	O
model	O
performance	O
.	O
As	O
a	O
result	O
,	O
we	O
train	O
two	O
types	O
of	O
multilingual	O
models	O
.	O
One	O
is	O
only	O
using	O
the	O
PARA	O
contexts	O
for	O
all	O
language	O
and	O
another	O
is	O
using	O
the	O
best	O
option	O
for	O
each	O
language	O
based	O
on	O
Table	O
3	O
.	O
From	O
the	O
results	O
in	O
Table	O
13	O
,	O
we	O
can	O
observe	O
that	O
:	O
1	O
)	O
searching	O
over	O
the	O
language	O
specific	O
KB	O
performs	O
better	O
than	O
searching	O
the	O
whole	O
KB	O
,	O
2	O
)	O
using	O
the	O
language	O
specific	O
context	O
option	O
can	O
not	O
improve	O
the	O
model	O
performance	O
.	O
Therefore	O
,	O
we	O
ensemble	O
both	O
types	O
of	O
the	O
model	O
for	O
the	O
final	O
submission	O
.	O

This	O
work	O
was	O
supported	O
by	O
Alibaba	O
Group	O
through	O
Alibaba	O
Innovative	O
Research	O
Program	O
.	O

It	O
should	O
be	O
noted	O
that	O
we	O
have	O
detected	O
a	O
few	O
flaws	O
in	O
the	O
provided	O
data	O
,	O
namely	O
several	O
sentences	O
incorrectly	O
considered	O
as	O
parallel	O
,	O
as	O
well	O
as	O
the	O
existence	O
of	O
many	O
spelling	O
errors	O
,	O
not	O
only	O
in	O
the	O
training	O
data	O
,	O
but	O
also	O
in	O
the	O
testing	O
documents	O
.	O
We	O
believe	O
that	O
many	O
of	O
the	O
typos	O
result	O
from	O
PDF	O
extraction	O
and/or	O
OCR	O
processes	O
,	O
which	O
are	O
never	O
perfect	O
,	O
having	O
found	O
and	O
corrected	O
a	O
total	O
of	O
127	O
,	O
198	O
misspellings	O
.	O
Yet	O
,	O
it	O
should	O
be	O
noted	O
that	O
some	O
misspelling	O
errors	O
are	O
easy	O
to	O
correct	O
,	O
but	O
errors	O
which	O
still	O
produce	O
correct	O
words	O
require	O
sentence	O
analysis	O
which	O
was	O
not	O
carried	O
out	O
.	O
Some	O
of	O
the	O
parallel	O
problems	O
are	O
illustrated	O
,	O
for	O
instance	O
,	O
by	O
having	O
the	O
first	O
Portuguese	O
line	O
from	O
medline	O
-	O
pubmed	O
"	O
ERRATA	O
.	O
"	O
aligned	O
with	O
the	O
first	O
English	O
line	O
"	O
Inequalities	O
in	O
self	O
-	O
rated	O
health	O
:	O
an	O
analysis	O
of	O
the	O
Brazilian	O
and	O
Portuguese	O
populations	O
.	O
"	O
,	O
which	O
should	O
be	O
"	O
ER	O
-	O
RATA	O
.	O
"	O
instead	O
.	O
Filtering	O
wrong	O
translation	O
units	O
as	O
the	O
one	O
above	O
,	O
as	O
well	O
as	O
translation	O
units	O
which	O
the	O
language	O
was	O
not	O
Portuguese	O
,	O
reduced	O
this	O
corpora	O
by	O
almost	O
2	O
,	O
000	O
translation	O
units	O
.	O
Some	O
errors	O
were	O
simply	O
detected	O
by	O
chance	O
,	O
like	O
first	O
and	O
last	O
entries	O
of	O
medline	O
-	O
pubmed	O
,	O
while	O
other	O
errors	O
were	O
detected	O
by	O
looking	O
at	O
the	O
untranslated	O
terms	O
in	O
the	O
initial	O
testing	O
3	O
and	O
realizing	O
that	O
some	O
terms	O
were	O
misspellings	O
,	O
as	O
well	O
as	O
spelling	O
and	O
vocabulary	O
differences	O
between	O
European	O
and	O
Brazilian	O
Portuguese	O
.	O
Table	O
3	O
shows	O
the	O
differences	O
between	O
the	O
original	O
version	O
medline	O
-	O
pubmed	O
and	O
its	O
revised	O
version	O
medline	O
-	O
pubmed	O
-	O
rev	O
.	O
The	O
reduction	O
in	O
size	O
towards	O
the	O
revised	O
version	O
is	O
mainly	O
due	O
to	O
the	O
removal	O
of	O
non	O
-	O
parallel	O
sentences	O
.	O
However	O
,	O
efforts	O
to	O
correct	O
such	O
situations	O
were	O
only	O
made	O
over	O
the	O
mentioned	O
medline	O
-	O
pubmed	O
parallel	O
document	O
set	O
,	O
since	O
the	O
other	O
sets	O
were	O
significantly	O
larger	O
,	O
as	O
shown	O
in	O
Table	O
1	O
.	O
Also	O
,	O
no	O
corrections	O
were	O
applied	O
to	O
the	O
testing	O
documents	O
because	O
we	O
assumed	O
they	O
were	O
not	O
supposed	O
to	O
be	O
edited	O
.	O
Yet	O
,	O
another	O
"	O
noise	O
"	O
element	O
was	O
the	O
already	O
mentioned	O
difference	O
in	O
spelling	O
and	O
vocabulary	O
between	O
European	O
Portuguese	O
(	O
which	O
has	O
been	O
our	O
main	O
focus	O
of	O
attention	O
throughout	O
our	O
research	O
experience	O
)	O
and	O
Brazilian	O
Portuguese	O
(	O
the	O
version	O
of	O
the	O
provided	O
biomedical	O
data	O
)	O
,	O
which	O
can	O
also	O
impact	O
results	O
negatively	O
.	O

Text	O
tokenization	O
ensures	O
that	O
words	O
are	O
properly	O
separated	O
by	O
a	O
single	O
blank	O
space	O
,	O
while	O
normalization	O
ensures	O
that	O
they	O
are	O
represented	O
by	O
a	O
"	O
standard	O
"	O
version	O
.	O
In	O
English	O
,	O
this	O
means	O
that	O
cases	O
like	O
"	O
was	O
n't	O
"	O
or	O
"	O
is	O
n't	O
"	O
are	O
going	O
to	O
be	O
replaced	O
by	O
"	O
was	O
not	O
"	O
and	O
"	O
is	O
not	O
"	O
,	O
respectively	O
.	O
In	O
Portuguese	O
,	O
this	O
means	O
that	O
cases	O
like	O
"	O
do	O
"	O
(	O
of	O
the	O
)	O
or	O
"	O
nas	O
"	O
(	O
in	O
the	O
)	O
are	O
going	O
to	O
be	O
replaced	O
by	O
"	O
de	O
o	O
"	O
(	O
of	O
the	O
)	O
and	O
"	O
em	O
as	O
"	O
(	O
in	O
the	O
)	O
,	O
respectively	O
.	O
These	O
tokenization	O
and	O
normalization	O
changes	O
are	O
reverted	O
when	O
presenting	O
the	O
final	O
translation	O
results	O
.	O
Whipple	O
disease	O
and	O
central	O
nervous	O
system	O
.	O
Doença	O
de	O
Whipple	O
e	O
sistema	O
nervoso	O
central	O
.	O

Phrase	O
-	O
level	O
alignment	O
was	O
obtained	O
with	O
a	O
modified	O
version	O
of	O
the	O
lexicon	O
-	O
based	O
aligner	O
proposed	O
by	O
Gomes	O
(	O
2009	O
)	O
.	O
The	O
aligner	O
matches	O
bilingual	O
phrase	O
pairs	O
provided	O
in	O
an	O
input	O
lexicon	O
(	O
described	O
ahead	O
in	O
2.3.2	O
)	O
and	O
selects	O
a	O
maximalcoverage	O
1	O
subset	O
of	O
coherent	O
alignments	O
.	O
While	O
the	O
original	O
method	O
imposed	O
a	O
monotonicity	O
constraint	O
,	O
i.e.	O
it	O
selected	O
a	O
maximal	O
-	O
coverage	O
chain	O
of	O
phrase	O
alignments	O
without	O
allowing	O
phrase	O
reorderings	O
,	O
the	O
new	O
method	O
applied	O
has	O
a	O
more	O
relaxed	O
coherency	O
criteria	O
:	O
it	O
only	O
requires	O
that	O
a	O
source	O
-	O
language	O
phrase	O
is	O
not	O
simultaneously	O
aligned	O
with	O
two	O
distinct	O
target	O
-	O
language	O
phrases	O
.	O
Therefore	O
,	O
it	O
allows	O
phrase	O
reordering	O
as	O
shown	O
in	O
the	O
example	O
in	O
Figure	O
1	O
.	O

Similar	O
to	O
the	O
ILP	O
(	O
Integer	O
Linear	O
Programming	O
)	O
solution	O
proposed	O
by	O
(	O
DeNero	O
and	O
Klein	O
,	O
2008	O
)	O
,	O
we	O
treat	O
the	O
alignment	O
problem	O
as	O
an	O
optimization	O
problem	O
,	O
but	O
we	O
employ	O
a	O
greedy	O
optimization	O
algorithm	O
which	O
allows	O
us	O
to	O
align	O
longer	O
sentences	O
with	O
reasonable	O
time	O
and	O
memory	O
.	O
The	O
algorithm	O
1	O
Maximal	O
-	O
coverage	O
means	O
that	O
the	O
selected	O
phrase	O
alignments	O
cover	O
as	O
much	O
text	O
as	O
possible	O
from	O
both	O
sentences	O
constructs	O
a	O
solution	O
(	O
a	O
set	O
of	O
coherent	O
alignments	O
)	O
incrementally	O
.	O
It	O
starts	O
by	O
settling	O
alignments	O
of	O
longer	O
phrases	O
,	O
which	O
tend	O
to	O
be	O
more	O
reliable	O
,	O
and	O
progresses	O
towards	O
shorter	O
phrases	O
or	O
words	O
,	O
which	O
are	O
allowed	O
to	O
align	O
only	O
if	O
they	O
are	O
coherent	O
with	O
previously	O
settled	O
alignments	O
.	O

Our	O
lexicon	O
covers	O
59.5	O
%	O
of	O
the	O
EN	O
corpus	O
tokens	O
and	O
55.4	O
%	O
of	O
the	O
PT	O
corpus	O
tokens	O
.	O
There	O
were	O
143	O
,	O
317	O
unique	O
phrasal	O
translations	O
matched	O
out	O
of	O
931	O
,	O
568	O
in	O
our	O
lexicon	O
.	O
The	O
cognaticity	O
-	O
based	O
matching	O
was	O
responsible	O
for	O
aligning	O
8	O
%	O
of	O
the	O
EN	O
corpus	O
and	O
7.2	O
%	O
of	O
the	O
PT	O
corpus	O
3	O
.	O
The	O
remainder	O
32.5	O
%	O
of	O
the	O
EN	O
corpus	O
and	O
37.4	O
%	O
of	O
the	O
PT	O
corpus	O
were	O
left	O
unaligned	O
.	O
These	O
unaligned	O
tokens	O
are	O
handled	O
as	O
gaps	O
by	O
the	O
phrase	O
table	O
extraction	O
algorithm	O
described	O
in	O
(	O
Aires	O
et	O
al	O
,	O
2009	O
)	O
.	O

The	O
language	O
model	O
used	O
is	O
supported	O
by	O
the	O
indexation	O
of	O
the	O
texts	O
in	O
each	O
language	O
of	O
the	O
provided	O
corpora	O
.	O
Such	O
indexation	O
will	O
support	O
determining	O
the	O
likelihood	O
of	O
the	O
occurrence	O
of	O
phrases	O
in	O
the	O
target	O
language	O
for	O
the	O
several	O
adjacent	O
translation	O
fragments	O
in	O
decoding	O
,	O
a	O
process	O
based	O
on	O
the	O
structures	O
presented	O
in	O
(	O
Aires	O
et	O
al	O
,	O
2008	O
)	O
.	O

The	O
translation	O
model	O
depends	O
on	O
the	O
alignment	O
to	O
determine	O
phrase	O
translation	O
equivalents	O
by	O
establishing	O
phrase	O
relations	O
between	O
source	O
and	O
target	O
languages	O
,	O
as	O
well	O
as	O
to	O
determine	O
a	O
degree	O
of	O
likelihood	O
of	O
those	O
same	O
relations	O
,	O
to	O
be	O
used	O
in	O
decoding	O
to	O
produce	O
new	O
translations	O
,	O
a	O
process	O
based	O
on	O
the	O
methodology	O
presented	O
in	O
(	O
Aires	O
et	O
al	O
,	O
2009	O
)	O
.	O

The	O
decoding	O
stage	O
is	O
the	O
one	O
that	O
will	O
finally	O
produce	O
the	O
actual	O
translations	O
.	O
First	O
,	O
an	O
original	O
text	O
is	O
fragmented	O
into	O
smaller	O
pieces	O
of	O
text	O
,	O
which	O
will	O
then	O
be	O
used	O
to	O
retrieve	O
their	O
corresponding	O
translations	O
.	O
The	O
several	O
combinations	O
of	O
the	O
translations	O
of	O
those	O
smaller	O
pieces	O
will	O
represent	O
many	O
possible	O
translations	O
and	O
the	O
purpose	O
of	O
decoding	O
is	O
to	O
find	O
the	O
most	O
likely	O
one	O
,	O
according	O
to	O
the	O
provided	O
scores	O
from	O
the	O
language	O
and	O
the	O
translation	O
models	O
.	O
As	O
mentioned	O
before	O
,	O
separate	O
models	O
can	O
be	O
obtained	O
from	O
separate	O
corpora	O
and	O
be	O
assigned	O
with	O
different	O
relevances	O
or	O
weights	O
,	O
according	O
to	O
their	O
importance	O
to	O
the	O
translation	O
in	O
question	O
.	O
As	O
such	O
,	O
and	O
as	O
explained	O
in	O
,	O
decoding	O
is	O
carried	O
out	O
as	O
a	O
best	O
path	O
finding	O
in	O
a	O
directed	O
acyclic	O
graph	O
,	O
where	O
its	O
edges	O
are	O
weighed	O
by	O
:	O
the	O
translation	O
model	O
score	O
between	O
source	O
and	O
target	O
phrases	O
;	O
and	O
the	O
language	O
model	O
scores	O
between	O
adjacent	O
target	O
phrases	O
.	O
Each	O
complete	O
path	O
will	O
represent	O
a	O
possible	O
translation	O
in	O
which	O
the	O
final	O
score	O
is	O
a	O
composition	O
of	O
the	O
scores	O
of	O
the	O
several	O
edges	O
that	O
compose	O
the	O
given	O
path	O
.	O
An	O
additional	O
penalty	O
is	O
introduced	O
to	O
provide	O
lower	O
scores	O
to	O
larger	O
paths	O
,	O
which	O
are	O
known	O
to	O
produce	O
worse	O
results	O
.	O

Considering	O
that	O
the	O
test	O
documents	O
to	O
be	O
translated	O
,	O
provided	O
by	O
the	O
shared	O
task	O
organization	O
,	O
share	O
their	O
domain	O
with	O
the	O
training	O
data	O
,	O
we	O
decided	O
to	O
propose	O
for	O
submission	O
the	O
three	O
possible	O
translation	O
runs	O
for	O
each	O
document	O
according	O
to	O
the	O
criteria	O
described	O
in	O
each	O
of	O
the	O
following	O
subsections	O
.	O

This	O
run	O
uses	O
the	O
medline	O
-	O
pubmed	O
,	O
biological	O
and	O
health	O
training	O
corpora	O
with	O
the	O
same	O
relevance	O
to	O
translate	O
every	O
translation	O
test	O
document	O
.	O
These	O
can	O
be	O
considered	O
our	O
simplest	O
set	O
of	O
tests	O
since	O
the	O
possible	O
model	O
relevance	O
difference	O
is	O
not	O
explored	O
and	O
no	O
additional	O
sources	O
are	O
included	O
.	O
In	O
this	O
case	O
we	O
achieved	O
a	O
total	O
of	O
7228	O
unique	O
untranslated	O
terms	O
5	O
.	O

This	O
run	O
also	O
uses	O
the	O
medline	O
-	O
pubmed	O
,	O
biological	O
and	O
health	O
training	O
corpora	O
,	O
but	O
assigns	O
a	O
higher	O
4	O
http://www.statmt.org/europarl/	O
5	O
Terms	O
can	O
have	O
one	O
or	O
more	O
words	O
relevance	O
to	O
the	O
biological	O
corpora	O
to	O
translate	O
the	O
biological	O
test	O
documents	O
and	O
then	O
assigns	O
a	O
higher	O
relevance	O
to	O
the	O
health	O
corpora	O
to	O
translate	O
the	O
health	O
test	O
documents	O
.	O
Because	O
the	O
changes	O
introduced	O
in	O
this	O
set	O
of	O
tests	O
only	O
concerned	O
the	O
relevance	O
of	O
the	O
models	O
,	O
the	O
total	O
of	O
7228	O
unique	O
untranslated	O
terms	O
did	O
not	O
change	O
.	O

UID	O
/	O
CEC/04516/2013	O
)	O
.	O
We	O
would	O
also	O
like	O
to	O
thank	O
Hugo	O
Delgado	O
for	O
his	O
support	O
.	O

Table	O
3	O
shows	O
the	O
performance	O
of	O
CLSC	O
with	O
onestep	O
transition	O
(	O
L	O
1−step	O
)	O
and	O
with	O
Markov	O
Chains	O
(	O
L	O
clsc	O
)	O
as	O
described	O
in	O
Section	O
3.2	O
.	O
Results	O
show	O
that	O
the	O
use	O
of	O
Markov	O
Chains	O
does	O
bring	O
improvement	O
to	O
the	O
overall	O
performance	O
,	O
which	O
is	O
consistent	O
with	O
the	O
model	O
intuition	O
.	O

Given	O
a	O
question	O
q	O
and	O
a	O
mixture	O
set	O
of	O
paragraphs	O
P	O
=	O
P	O
+	O
∪	O
P	O
−	O
with	O
some	O
paragraphs	O
p	O
P	O
+	O
relevant	O
to	O
q	O
and	O
some	O
p	O
P	O
−	O
irrelevant	O
.	O
Our	O
goal	O
is	O
to	O
select	O
a	O
small	O
subset	O
of	O
paragraphs	O
P	O
sel	O
⊂	O
P	O
,	O
such	O
that	O
every	O
p	O
P	O
sel	O
satisfies	O
p	O
P	O
+	O
(	O
relevancy	O
)	O
,	O
and	O
all	O
p	O
P	O
sel	O
can	O
jointly	O
cover	O
all	O
the	O
information	O
asked	O
by	O
q	O
(	O
complementary	O
)	O
.	O
The	O
off	O
-	O
the	O
-	O
shelf	O
models	O
select	O
relevant	O
paragraphs	O
independently	O
,	O
thus	O
usually	O
can	O
not	O
deal	O
with	O
the	O
complementary	O
property	O
.	O
The	O
inner	O
dependency	O
among	O
the	O
selected	O
P	O
sel	O
needs	O
to	O
be	O
considered	O
,	O
which	O
will	O
be	O
modeled	O
in	O
the	O
remaining	O
of	O
the	O
section	O
.	O

For	O
efficient	O
inference	O
when	O
L	O
=	O
2	O
,	O
we	O
start	O
to	O
select	O
the	O
top	O
-	O
N	O
(	O
N	O
K	O
)	O
most	O
relevant	O
passages	O
.	O
Then	O
we	O
score	O
the	O
combinations	O
between	O
each	O
passage	O
pair	O
in	O
the	O
top	O
-	O
N	O
set	O
and	O
another	O
top	O
-	O
M	O
set	O
.	O
This	O
reduces	O
the	O
complexity	O
from	O
O	O
(	O
K	O
2	O
)	O
to	O
O	O
(	O
M	O
N	O
)	O
.	O
M	O
is	O
a	O
hyperparameter	O
corresponding	O
to	O
the	O
beam	O
size	O
.	O
In	O
a	O
more	O
general	O
setting	O
with	O
L	O
≥	O
2	O
,	O
we	O
have	O
an	O
algorithm	O
with	O
the	O
complexity	O
of	O
O	O
(	O
(	O
L	O
−	O
1	O
)	O
M	O
N	O
)	O
instead	O
of	O
O	O
(	O
K	O
L	O
)	O
,	O
which	O
is	O
shown	O
in	O
algorithm	O
1	O
.	O

Research	O
reported	O
in	O
this	O
publication	O
was	O
supported	O
by	O
the	O
National	O
Library	O
Of	O
Medicine	O
of	O
the	O
National	O
Institutes	O
of	O
Health	O
under	O
Award	O
Numbers	O
R01LM012918	O
and	O
R01LM012973	O
.	O
The	O
content	O
is	O
solely	O
the	O
responsibility	O
of	O
the	O
authors	O
and	O
does	O
not	O
necessarily	O
represent	O
the	O
official	O
views	O
of	O
the	O
National	O
Institutes	O
of	O
Health	O
.	O

We	O
collect	O
two	O
corpora	O
,	O
Small	O
Corpus	O
and	O
Large	O
Corpus	O
,	O
with	O
different	O
sizes	O
for	O
cross	O
-	O
lingual	O
pretraining	O
.	O
Table	O
1	O
lists	O
the	O
data	O
statistics	O
.	O

For	O
tasks	O
QADSM	O
,	O
WPR	O
,	O
QAM	O
and	O
QG	O
,	O
we	O
label	O
the	O
data	O
on	O
an	O
Microsoft	O
internal	O
crowdsourcing	O
platform	O
.	O
Each	O
labeler	O
must	O
learn	O
the	O
guideline	O
and	O
pass	O
the	O
labeling	O
test	O
.	O
Each	O
sample	O
is	O
labeled	O
by	O
three	O
labeler	O
.	O
We	O
only	O
keep	O
the	O
samples	O
with	O
two	O
or	O
three	O
labeler	O
have	O
same	O
label	O
.	O
For	O
tasks	O
NC	O
and	O
NTG	O
,	O
we	O
directly	O
use	O
the	O
category	O
label	O
on	O
MSN	O
website	O
.	O
All	O
the	O
category	O
label	O
on	O
MSN	O
is	O
review	O
by	O
human	O
.	O

We	O
investigate	O
the	O
impacts	O
of	O
different	O
text	O
noising	O
strategies	O
(	O
Section	O
4.1	O
)	O
in	O
Unicoder	O
xDAE	O
SC	O
,	O
and	O
list	O
comparison	O
results	O
in	O
Table	O
10	O
,	O
where	O
(	O
1	O
)	O
+	O
(	O
2	O
)	O
+	O
(	O
3	O
)	O
denotes	O
the	O
result	O
of	O
using	O
the	O
first	O
three	O
strategies	O
in	O
pre	O
-	O
training	O
,	O
(	O
4	O
)	O
denotes	O
the	O
result	O
of	O
using	O
the	O
last	O
strategy	O
in	O
pre	O
-	O
training	O
,	O
(	O
1	O
)	O
+	O
(	O
2	O
)	O
+	O
(	O
3	O
)	O
+	O
(	O
4	O
)	O
denotes	O
the	O
result	O
of	O
using	O
all	O
strategies	O
in	O
pretraining	O
.	O
To	O
reduce	O
experiment	O
cost	O
,	O
we	O
set	O
max	O
sequence	O
length	O
to	O
256	O
and	O
only	O
train	O
60	O
K	O
steps	O
.	O
We	O
find	O
that	O
(	O
4	O
)	O
can	O
achieve	O
the	O
best	O
average	O
result	O
on	O
NTG	O
.	O
So	O
all	O
results	O
of	O
Unicoder	O
xDAE	O
SC	O
reported	O
in	O
this	O
paper	O
is	O
pre	O
-	O
trained	O
using	O
(	O
4	O
)	O
only	O
.	O

In	O
this	O
section	O
we	O
first	O
present	O
some	O
properties	O
of	O
our	O
dataset	O
,	O
and	O
then	O
describe	O
the	O
process	O
that	O
we	O
used	O
to	O
create	O
it	O
.	O

We	O
follow	O
three	O
principles	O
when	O
designing	O
the	O
dataset	O
construction	O
process	O
:	O
1	O
.	O
The	O
text	O
part	O
of	O
the	O
data	O
should	O
be	O
directly	O
comparable	O
in	O
complexity	O
to	O
the	O
capability	O
of	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
text	O
generative	O
models	O
.	O
2	O
.	O
The	O
graph	O
part	O
of	O
the	O
data	O
should	O
be	O
constructed	O
in	O
an	O
automatic	O
and	O
scalable	O
way	O
.	O
3	O
.	O
The	O
graph	O
part	O
of	O
the	O
data	O
should	O
be	O
relevant	O
for	O
the	O
paired	O
text	O
data	O
.	O
Note	O
that	O
our	O
process	O
is	O
general	O
,	O
and	O
can	O
be	O
applied	O
to	O
any	O
set	O
of	O
Wikipedia	O
articles	O
.	O
We	O
have	O
tried	O
to	O
pair	O
a	O
full	O
dump	O
of	O
English	O
Wikipedia	O
with	O
Freebase	O
and	O
managed	O
to	O
get	O
over	O
3	O
million	O
graphtext	O
pairs	O
.	O
Here	O
we	O
restrict	O
the	O
process	O
to	O
the	O
set	O
of	O
articles	O
from	O
the	O
WikiText	O
-	O
103	O
dataset	O
.	O
We	O
try	O
to	O
map	O
each	O
Wikipedia	O
article	O
to	O
a	O
relevant	O
subgraph	O
of	O
the	O
existing	O
large	O
scale	O
KG	O
Freebase	O
(	O
Bollacker	O
et	O
al	O
,	O
2008	O
)	O
.	O
We	O
used	O
the	O
last	O
public	O
dump	O
of	O
Freebase	O
2	O
,	O
which	O
contains	O
1.9B	O
triples	O
and	O
a	O
total	O
of	O
250	O
GB	O
of	O
data	O
.	O
We	O
filtered	O
the	O
data	O
by	O
keeping	O
only	O
the	O
entities	O
with	O
at	O
least	O
4	O
string	O
attributes	O
(	O
otherwise	O
the	O
entities	O
are	O
less	O
interpretable	O
)	O
,	O
and	O
keeping	O
only	O
the	O
top	O
1024	O
most	O
frequent	O
relation	O
types	O
and	O
restricting	O
the	O
relations	O
to	O
only	O
those	O
among	O
the	O
retained	O
entities	O
and	O
between	O
the	O
entities	O
and	O
string	O
attributes	O
.	O
We	O
also	O
simplified	O
the	O
entity	O
and	O
relation	O
names	O
by	O
stripping	O
off	O
the	O
irrelevant	O
"	O
http://rdf.freebase.com/	O
"	O
and	O
further	O
removed	O
duplicates	O
.	O
This	O
gives	O
us	O
a	O
significantly	O
cleaner	O
and	O
smaller	O
backbone	O
graph	O
for	O
Freebase	O
,	O
with	O
about	O
20	O
M	O
nodes	O
.	O
Finding	O
the	O
relevant	O
subgraph	O
for	O
an	O
article	O
in	O
such	O
a	O
cleaned	O
up	O
but	O
still	O
large	O
KG	O
remains	O
nontrivial	O
.	O
Our	O
process	O
for	O
this	O
contains	O
3	O
stages	O
:	O
mapping	O
,	O
expansion	O
,	O
and	O
filtering	O
.	O
Mapping	O
In	O
the	O
first	O
stage	O
of	O
the	O
process	O
,	O
we	O
map	O
each	O
article	O
into	O
an	O
entity	O
in	O
our	O
processed	O
Freebase	O
KG	O
.	O
This	O
is	O
made	O
possible	O
through	O
triples	O
from	O
Freebase	O
like	O
the	O
following	O
:	O
ns	O
/	O
g.11b6jbqpt4	O
key	O
/	O
wikipedia.en	O
"	O
Madunnella	O
"	O
where	O
ns	O
/	O
g.11b6jbqpt4	O
refers	O
to	O
an	O
entity	O
in	O
the	O
KG	O
,	O
key	O
/	O
wikipedia.en	O
is	O
the	O
type	O
of	O
the	O
edge	O
,	O
which	O
indicates	O
that	O
this	O
entity	O
is	O
linked	O
to	O
a	O
Wikipedia	O
article	O
and	O
"	O
Madunnella	O
"	O
is	O
the	O
title	O
of	O
that	O
article	O
.	O
We	O
normalize	O
the	O
title	O
string	O
(	O
and	O
in	O
general	O
any	O
string	O
literals	O
)	O
from	O
Freebase	O
by	O
replacing	O
"	O
_	O
"	O
with	O
white	O
space	O
and	O
handle	O
unicode	O
characters	O
properly	O
.	O
We	O
extract	O
the	O
titles	O
from	O
the	O
Wikipedia	O
article	O
through	O
string	O
matching	O
,	O
where	O
titles	O
are	O
enclosed	O
in	O
a	O
"	O
=	O
[	O
title	O
]	O
=	O
"	O
pattern	O
.	O
In	O
this	O
step	O
we	O
managed	O
to	O
map	O
24	O
,	O
345	O
out	O
of	O
28	O
,	O
475	O
(	O
85.5	O
%	O
)	O
article	O
titles	O
from	O
WikiText	O
-	O
103	O
to	O
an	O
entity	O
in	O
our	O
KG	O
.	O
Expansion	O
We	O
treat	O
each	O
of	O
the	O
mapped	O
entities	O
as	O
the	O
center	O
node	O
of	O
a	O
subgraph	O
,	O
and	O
expand	O
1	O
hop	O
out	O
in	O
the	O
entire	O
filtered	O
Freebase	O
graph	O
to	O
include	O
all	O
the	O
neighboring	O
entities	O
that	O
are	O
the	O
most	O
relevant	O
to	O
the	O
center	O
entity	O
.	O
We	O
then	O
expand	O
further	O
from	O
this	O
1	O
-	O
hop	O
graph	O
out	O
to	O
include	O
all	O
the	O
relations	O
that	O
connect	O
the	O
selected	O
entities	O
to	O
string	O
attributes	O
as	O
well	O
as	O
between	O
these	O
entities	O
themselves	O
.	O
Note	O
that	O
because	O
of	O
these	O
edges	O
between	O
the	O
1	O
-	O
hop	O
neighbor	O
entities	O
the	O
graphs	O
are	O
typically	O
not	O
star	O
structured	O
.	O
This	O
gives	O
us	O
a	O
relevant	O
but	O
compact	O
graph	O
for	O
each	O
article	O
.	O
We	O
have	O
also	O
investigated	O
the	O
possibility	O
of	O
a	O
2	O
-	O
hop	O
neighborhood	O
from	O
the	O
center	O
node	O
,	O
and	O
found	O
that	O
2	O
-	O
hop	O
neighborhoods	O
are	O
significantly	O
larger	O
than	O
1	O
-	O
hop	O
and	O
through	O
some	O
"	O
hub	O
"	O
nodes	O
like	O
"	O
Male	O
"	O
or	O
"	O
Female	O
"	O
a	O
2	O
-	O
hop	O
neighborhood	O
from	O
an	O
entity	O
can	O
easily	O
include	O
many	O
other	O
irrelevant	O
entities	O
.	O
Based	O
on	O
such	O
observations	O
we	O
decided	O
to	O
use	O
the	O
1	O
-	O
hop	O
neighborhood	O
to	O
keep	O
the	O
relevance	O
of	O
the	O
subgraph	O
high	O
.	O
Filtering	O
The	O
last	O
stage	O
of	O
the	O
process	O
involves	O
more	O
filtering	O
and	O
cleaning	O
up	O
of	O
the	O
data	O
.	O
We	O
noticed	O
that	O
in	O
Freebase	O
it	O
is	O
common	O
for	O
one	O
entity	O
to	O
have	O
multiple	O
relations	O
of	O
the	O
same	O
type	O
pointing	O
to	O
different	O
string	O
attributes	O
,	O
like	O
the	O
following	O
:	O
ns	O
/	O
m.07c72	O
key	O
/	O
wikipedia.en	O
"	O
The	O
SImpsons	O
"	O
ns	O
/	O
m.07c72	O
key	O
/	O
wikipedia.en	O
"	O
The	O
Simpson	O
"	O
ns	O
/	O
m.07c72	O
key	O
/	O
wikipedia.en	O
"	O
The	O
simsons	O
"	O
ns	O
/	O
m.07c72	O
key	O
/	O
wikipedia.en	O
"	O
Thr	O
Simpsons	O
"	O
ns	O
/	O
m.07c72	O
key	O
/	O
wikipedia.en	O
"	O
The	O
Simpson	O
's	O
"	O
It	O
is	O
clear	O
that	O
there	O
is	O
a	O
lot	O
of	O
redundancy	O
in	O
this	O
data	O
.	O
We	O
reduced	O
all	O
such	O
edges	O
(	O
from	O
the	O
same	O
entity	O
with	O
the	O
same	O
edge	O
type	O
to	O
string	O
attributes	O
)	O
to	O
a	O
single	O
edge	O
by	O
picking	O
the	O
most	O
"	O
canonical	O
"	O
one	O
.	O
This	O
was	O
done	O
by	O
fitting	O
a	O
unigram	O
model	O
to	O
the	O
characters	O
in	O
the	O
collection	O
of	O
strings	O
and	O
using	O
that	O
model	O
to	O
pick	O
the	O
most	O
likely	O
string	O
.	O
We	O
also	O
filtered	O
the	O
graphs	O
based	O
on	O
size	O
and	O
created	O
three	O
versions	O
of	O
the	O
data	O
with	O
maximum	O
graph	O
size	O
capped	O
at	O
256	O
,	O
512	O
,	O
and	O
1024	O
nodes	O
,	O
respectively	O
.	O
All	O
the	O
statistics	O
and	O
results	O
in	O
the	O
rest	O
of	O
the	O
paper	O
are	O
based	O
on	O
graphs	O
with	O
a	O
maximum	O
size	O
of	O
256	O
,	O
but	O
all	O
versions	O
of	O
the	O
data	O
are	O
made	O
available	O
online	O
.	O

In	O
this	O
last	O
task	O
,	O
we	O
evaluate	O
the	O
performance	O
of	O
graph	O
retrieval	O
given	O
a	O
text	O
query	O
.	O
We	O
use	O
exactly	O
the	O
same	O
setting	O
and	O
scores	O
as	O
Section	O
4.3	O
,	O
but	O
instead	O
rank	O
the	O
graphs	O
for	O
each	O
text	O
article	O
using	O
the	O
likelihood	O
scores	O
.	O
The	O
results	O
are	O
shown	O
in	O
Table	O
6	O
.	O
Note	O
that	O
this	O
task	O
is	O
quite	O
easy	O
with	O
our	O
data	O
and	O
setup	O
,	O
potentially	O
because	O
the	O
graphs	O
are	O
much	O
more	O
distinguishable	O
than	O
the	O
text	O
articles	O
.	O
All	O
the	O
graph	O
-	O
conditioned	O
models	O
perform	O
almost	O
perfectly	O
,	O
with	O
the	O
GNN	O
model	O
again	O
outperforming	O
the	O
others	O
.	O

A.1	O
Graph	O
visualization	O
Some	O
example	O
visualizations	O
of	O
the	O
KG	O
structures	O
are	O
shown	O
in	O
Figure	O
7	O
and	O
Figure	O
8	O
.	O
The	O
corresponding	O
graph	O
truth	O
texts	O
are	O
shown	O
in	O
Table	O
7	O
.	O

The	O
generated	O
texts	O
based	O
on	O
the	O
graph	O
shown	O
in	O
Figure	O
7	O
and	O
Figure	O
8	O
are	O
listed	O
in	O
Table	O
8	O
and	O
Table	O
9	O
,	O
respectively	O
.	O

The	O
order	O
of	O
merging	O
these	O
results	O
is	O
decided	O
by	O
the	O
evaluation	O
scores	O
from	O
these	O
modules	O
for	O
training	O
data	O
.	O
The	O
same	O
order	O
is	O
applied	O
to	O
the	O
test	O
data	O
.	O

Once	O
the	O
UMBC	O
corpus	O
is	O
pre	O
-	O
processed	O
and	O
the	O
three	O
required	O
corpora	O
and	O
an	O
embedding	O
matrix	O
are	O
derived	O
,	O
candidate	O
hypernyms	O
are	O
acquired	O
by	O
applying	O
the	O
below	O
processes	O
.	O
Co	O
-	O
occurrence	O
frequency	O
from	O
Normalized	O
Corpus	O
:	O
With	O
this	O
module	O
,	O
we	O
hypothesized	O
that	O
a	O
hyponym	O
and	O
its	O
possible	O
hypernyms	O
are	O
more	O
likely	O
to	O
co	O
-	O
occur	O
within	O
a	O
context	O
-	O
window	O
.	O
The	O
context	O
window	O
of	O
a	O
term	O
is	O
its	O
own	O
paragraph	O
.	O
We	O
start	O
by	O
creating	O
a	O
map	O
for	O
all	O
the	O
input	O
terms	O
.	O
If	O
a	O
normalized	O
paragraph	O
2.2.1	O
contains	O
any	O
of	O
the	O
input	O
terms	O
,	O
then	O
all	O
the	O
words	O
in	O
the	O
context	O
are	O
added	O
to	O
the	O
map	O
of	O
this	O
particular	O
term	O
which	O
considers	O
them	O
to	O
be	O
hypernyms	O
for	O
this	O
input	O
hyponym	O
term	O
.	O
Every	O
time	O
a	O
hypernym	O
-	O
hyponym	O
pair	O
co	O
-	O
occurs	O
in	O
one	O
line	O
,	O
their	O
co	O
-	O
occurrence	O
count	O
is	O
increased	O
by	O
one	O
.	O
Finally	O
,	O
the	O
candidate	O
hypernyms	O
are	O
ranked	O
in	O
descending	O
order	O
of	O
their	O
co	O
-	O
occurrence	O
frequencies	O
.	O

In	O
the	O
pre	O
-	O
processing	O
step	O
2.2.1	O
,	O
we	O
extracted	O
possible	O
hypernym	O
-	O
hyponym	O
mapping	O
data	O
using	O
Hearst	O
Patterns	O
.	O
Each	O
line	O
of	O
the	O
data	O
is	O
of	O
the	O
form	O
hypernym	O
:	O
hyponym	O
-	O
1	O
,	O
hyponym	O
-	O
2	O
,	O
,	O
hyponymn	O
.	O
In	O
this	O
module	O
,	O
we	O
created	O
a	O
map	O
where	O
each	O
hyponym	O
is	O
a	O
key	O
mapped	O
to	O
hypernyms	O
occurring	O
with	O
that	O
hyponym	O
and	O
their	O
co	O
-	O
occurrence	O
frequencies	O
.	O
For	O
example	O
,	O
values	O
for	O
keys	O
hyponym	O
-	O
1	O
,	O
hyponym	O
-	O
2	O
,	O
and	O
hyponym	O
-	O
n	O
are	O
updated	O
with	O
hypernym	O
and	O
the	O
frequencies	O
are	O
increased	O
by	O
1	O
.	O
Finally	O
the	O
top	O
15	O
hyponyms	O
(	O
based	O
on	O
frequencies	O
)	O
for	O
each	O
key	O
are	O
reported	O
as	O
the	O
result	O
hypernyms	O
.	O

We	O
need	O
a	O
general	O
distance	O
vector	O
which	O
represents	O
a	O
hypernym	O
-	O
hyponym	O
distance	O
in	O
the	O
UMBC	O
Embedding	O
.	O
We	O
use	O
training	O
data	O
input	O
term	O
(	O
x	O
)	O
and	O
the	O
gold	O
data	O
hypernyms	O
(	O
y	O
)	O
to	O
calculate	O
this	O
distance	O
(	O
Φ	O
*	O
)	O
which	O
is	O
calculated	O
by	O
:	O
Φ	O
*	O
=	O
argmin	O
Φ	O
1	O
N	O
(	O
x	O
,	O
y	O
)	O
Φ	O
x	O
−	O
y	O
2	O
(	O
1	O
)	O
Φ	O
is	O
used	O
to	O
get	O
candidate	O
hypernyms	O
from	O
the	O
UMBC	O
word	O
embedding	O
matrix	O
for	O
the	O
input	O
terms	O
(	O
test	O
data	O
)	O
.	O

We	O
define	O
large	O
-	O
scale	O
sense	O
induction	O
as	O
deriving	O
sense	O
clusters	O
for	O
all	O
words	O
in	O
a	O
large	O
vocabulary	O
and	O
assigning	O
a	O
sense	O
cluster	O
to	O
each	O
corpus	O
occurrence	O
of	O
these	O
words	O
.	O
2	O

We	O
evaluate	O
our	O
method	O
on	O
large	O
corpora	O
by	O
randomly	O
sampling	O
2000	O
instances	O
from	O
the	O
senseinduced	O
Wikipedia	O
,	O
focusing	O
on	O
frequent	O
words	O
with	O
many	O
senses	O
.	O
We	O
manually	O
annotate	O
the	O
samples	O
'	O
senses	O
without	O
access	O
to	O
the	O
automatically	O
induced	O
senses	O
,	O
and	O
then	O
compare	O
our	O
annotations	O
to	O
the	O
system	O
's	O
sense	O
assignments	O
.	O
We	O
publicly	O
release	O
our	O
manual	O
sense	O
annotations	O
.	O

Due	O
to	O
limit	O
of	O
space	O
we	O
provide	O
additional	O
examples	O
in	O
the	O
appendix	O
.	O
We	O
start	O
with	O
the	O
senses	O
found	O
for	O
the	O
word	O
face	O
:	O
The	O
face	O
senses	O
refer	O
to	O
meeting	O
/	O
confronting	O
,	O
the	O
body	O
part	O
,	O
turn	O
/	O
look	O
and	O
side	O
,	O
respectively	O
.	O
Here	O
we	O
present	O
two	O
senses	O
of	O
the	O
word	O
orange	O
,	O
corresponding	O
to	O
the	O
color	O
and	O
fruit	O
:	O
For	O
each	O
of	O
the	O
target	O
words	O
you	O
labeled	O
,	O
you	O
will	O
now	O
receive	O
a	O
short	O
list	O
of	O
indirect	O
wordmeaning	O
definitions	O
.	O
Indirect	O
word	O
-	O
meanings	O
are	O
composed	O
of	O
:	O
(	O
a	O
)	O
A	O
list	O
of	O
10	O
words	O
that	O
may	O
appear	O
instead	O
of	O
the	O
target	O
word	O
in	O
specific	O
contexts	O
(	O
b	O
)	O
A	O
list	O
of	O
5	O
sentences	O
in	O
which	O
the	O
target	O
word	O
has	O
this	O
specific	O
word	O
-	O
meaning	O
.	O
For	O
example	O
,	O
this	O
is	O
a	O
possible	O
indirect	O
wordmeaning	O
for	O
the	O
target	O
word	O
"	O
Apple	O
"	O
,	O
representing	O
the	O
fruit	O
,	O
as	O
opposed	O
to	O
the	O
tech	O
company	O
:	O
Alternatives	O
:	O
orange	O
,	O
olive	O
,	O
cherry	O
,	O
lime	O
,	O
banana	O
,	O
emerald	O
,	O
lemon	O
,	O
tomato	O
,	O
oak	O
,	O
arrow	O
,	O
Sentences	O
in	O
which	O
Apple	O
appears	O
in	O
this	O
word	O
-	O
meaning	O
:	O
"	O
He	O
and	O
his	O
new	O
bride	O
planted	O
apple	O
trees	O
to	O
celebrate	O
their	O
marriage	O
.	O
"	O
"	O
While	O
visiting	O
,	O
Luther	O
offers	O
Alice	O
an	O
apple	O
.	O
"	O
"	O
When	O
she	O
picks	O
the	O
apple	O
up	O
,	O
it	O
is	O
revealed	O
that	O
Luther	O
has	O
stolen	O
a	O
swipe	O
card	O
and	O
given	O
it	O
to	O
Alice	O
to	O
help	O
her	O
escape	O
.	O
"	O
You	O
will	O
be	O
asked	O
to	O
label	O
the	O
indirect	O
wordmeanings	O
with	O
one	O
of	O
the	O
labels	O
you	O
used	O
in	O
step	O
1	O
.	O
If	O
no	O
label	O
matches	O
the	O
indirect	O
word	O
-	O
meaning	O
you	O
are	O
allowed	O
to	O
propose	O
a	O
new	O
label	O
or	O
define	O
it	O
to	O
be	O
"	O
Unknown	O
"	O
.	O
Additionally	O
,	O
if	O
you	O
find	O
several	O
indirect	O
word	O
-	O
meanings	O
too	O
close	O
in	O
meaning	O
,	O
label	O
them	O
the	O
same	O
.	O

Finally	O
we	O
present	O
the	O
senses	O
for	O
Jordan	O
:	O
Josh0	O
Tyropoeon	O
Here	O
the	O
clusters	O
correspond	O
to	O
Jordan	O
the	O
surname	O
,	O
the	O
country	O
,	O
first	O
name	O
and	O
the	O
Jordan	O
River	O
,	O
respectively	O
.	O

The	O
objective	O
of	O
this	O
task	O
is	O
to	O
annotate	O
wordmeanings	O
of	O
20	O
ambiguous	O
words	O
in	O
a	O
total	O
of	O
2000	O
different	O
contexts	O
.	O
What	O
is	O
word	O
-	O
meaning	O
?	O
Words	O
have	O
different	O
meanings	O
in	O
different	O
contexts	O
,	O
for	O
example	O
,	O
in	O
the	O
sentence	O
:	O
"	O
there	O
is	O
a	O
light	O
that	O
never	O
goes	O
out	O
"	O
,	O
the	O
word	O
"	O
light	O
"	O
refers	O
to	O
any	O
device	O
serving	O
as	O
a	O
source	O
of	O
illumination	O
.	O
While	O
"	O
light	O
"	O
in	O
the	O
sentence	O
"	O
light	O
as	O
a	O
feather	O
"	O
refers	O
to	O
the	O
comparatively	O
little	O
physical	O
weight	O
or	O
density	O
of	O
an	O
object	O
.	O
Step	O
1	O
:	O
In	O
this	O
dataset	O
we	O
examine	O
20	O
ambiguous	O
words	O
as	O
targets	O
.	O
For	O
each	O
of	O
these	O
words	O
we	O
collected	O
100	O
sentences	O
in	O
which	O
the	O
target	O
word	O
appears	O
.	O
For	O
every	O
sentence	O
in	O
the	O
100	O
set	O
per	O
target	O
word	O
,	O
you	O
will	O
be	O
asked	O
to	O
write	O
a	O
short	O
label	O
expressing	O
the	O
meaning	O
of	O
the	O
target	O
word	O
in	O
that	O
particular	O
context	O
.	O
For	O
example	O
,	O
here	O
are	O
three	O
sentences	O
with	O
the	O
target	O
word	O
"	O
light	O
"	O
,	O
each	O
with	O
its	O
possible	O
annotation	O
.	O
1	O
.	O
"	O
there	O
is	O
a	O
light	O
that	O
never	O
goes	O
out	O
"	O
visible	O
light	O
.	O
2	O
.	O
"	O
light	O
as	O
a	O
feather	O
"	O
light	O
as	O
in	O
weight	O
.	O
3	O
.	O
"	O
magnesium	O
is	O
a	O
light	O
metal	O
"	O
light	O
as	O
in	O
weight	O
.	O
Note	O
that	O
in	O
this	O
example	O
the	O
annotator	O
found	O
the	O
second	O
and	O
third	O
meanings	O
of	O
the	O
word	O
"	O
light	O
"	O
to	O
be	O
the	O
same	O
and	O
therefore	O
labeled	O
them	O
with	O
the	O
same	O
label	O
.	O
13	O
While	O
some	O
annotations	O
are	O
indeed	O
intuitive	O
,	O
labeling	O
word	O
-	O
meanings	O
when	O
the	O
target	O
word	O
is	O
part	O
of	O
a	O
name	O
can	O
be	O
challenging	O
.	O
Here	O
are	O
a	O
few	O
guidelines	O
for	O
such	O
use	O
case	O
:	O
Whenever	O
a	O
target	O
word	O
appeared	O
as	O
part	O
of	O
a	O
name	O
(	O
Person	O
,	O
Organization	O
etc	O
.	O
)	O
,	O
one	O
of	O
three	O
heuristics	O
should	O
be	O
used	O
14	O
:	O
1	O
.	O
If	O
the	O
target	O
word	O
is	O
the	O
surname	O
of	O
a	O
person	O
,	O
the	O
example	O
should	O
be	O
tagged	O
surname	O
.	O
15	O
2	O
.	O
If	O
the	O
entity	O
(	O
as	O
a	O
whole	O
)	O
refers	O
to	O
one	O
of	O
the	O
word	O
-	O
meanings	O
,	O
it	O
should	O
be	O
labeled	O
as	O
such	O
.	O
For	O
example	O
,	O
Quitobaquito	O
Springs	O
label	O
should	O
refer	O
to	O
a	O
natural	O
source	O
of	O
water	O
.	O
3	O
.	O
If	O
the	O
target	O
word	O
is	O
part	O
of	O
a	O
name	O
different	O
from	O
the	O
original	O
word	O
-	O
meaning	O
,	O
it	O
should	O
be	O
tagged	O
as	O
Part	O
of	O
Name	O
.	O
This	O
includes	O
song	O
names	O
,	O
companies	O
(	O
Cold	O
Spring	O
Ice	O
)	O
,	O
restaurants	O
etc	O
.	O
Possible	O
exceptions	O
for	O
this	O
case	O
are	O
when	O
a	O
specific	O
named	O
entity	O
is	O
significantly	O
frequent	O
.	O
Step	O

This	O
research	O
was	O
supported	O
in	O
part	O
by	O
the	O
Canada	O
First	O
Research	O
Excellence	O
Fund	O
and	O
the	O
Natural	O
Sciences	O
and	O
Engineering	O
Research	O
Council	O
(	O
NSERC	O
)	O
of	O
Canada	O
.	O

Morphotactics	O
as	O
Tier	O
-	O
Based	O
Strictly	O
Local	O
Dependencies	O

It	O
is	O
commonly	O
accepted	O
that	O
morphological	O
dependencies	O
are	O
finite	O
-	O
state	O
in	O
nature	O
.	O
We	O
argue	O
that	O
the	O
upper	O
bound	O
on	O
morphological	O
expressivity	O
is	O
much	O
lower	O
.	O
Drawing	O
on	O
technical	O
results	O
from	O
computational	O
phonology	O
,	O
we	O
show	O
that	O
a	O
variety	O
of	O
morphotactic	O
phenomena	O
are	O
tierbased	O
strictly	O
local	O
and	O
do	O
not	O
fall	O
into	O
weaker	O
subclasses	O
such	O
as	O
the	O
strictly	O
local	O
or	O
strictly	O
piecewise	O
languages	O
.	O
Since	O
the	O
tier	O
-	O
based	O
strictly	O
local	O
languages	O
are	O
learnable	O
in	O
the	O
limit	O
from	O
positive	O
texts	O
,	O
this	O
marks	O
a	O
first	O
important	O
step	O
towards	O
general	O
machine	O
learning	O
algorithms	O
for	O
morphology	O
.	O
Furthermore	O
,	O
the	O
limitation	O
to	O
tier	O
-	O
based	O
strictly	O
local	O
languages	O
explains	O
typological	O
gaps	O
that	O
are	O
puzzling	O
from	O
a	O
purely	O
linguistic	O
perspective	O
.	O

Different	O
aspects	O
of	O
language	O
have	O
different	O
levels	O
of	O
complexity	O
.	O
A	O
lot	O
of	O
recent	O
work	O
in	O
phonology	O
(	O
see	O
Graf	O
(	O
2010	O
)	O
,	O
Heinz	O
(	O
2011a	O
;	O
2011b	O
;	O
,	O
Chandlee	O
(	O
2014	O
)	O
,	O
Jardine	O
(	O
2015	O
)	O
and	O
references	O
therein	O
)	O
argues	O
that	O
phonological	O
well	O
-	O
formedness	O
conditions	O
are	O
subregular	O
and	O
hence	O
do	O
not	O
require	O
the	O
full	O
power	O
of	O
finite	O
-	O
state	O
automata	O
.	O
This	O
is	O
particularly	O
noteworthy	O
because	O
computational	O
phonology	O
still	O
relies	O
heavily	O
on	O
finite	O
-	O
state	O
methods	O
(	O
Kaplan	O
and	O
Kay	O
,	O
1994	O
;	O
Frank	O
and	O
Satta	O
,	O
1998	O
;	O
Riggle	O
,	O
2004	O
)	O
.	O
A	O
similar	O
trend	O
can	O
be	O
observed	O
in	O
computational	O
syntax	O
,	O
where	O
the	O
original	O
characterization	O
as	O
mildly	O
context	O
-	O
sensitive	O
string	O
languages	O
(	O
Huybregts	O
,	O
1984	O
;	O
Shieber	O
,	O
1985	O
)	O
is	O
now	O
being	O
reinterpreted	O
in	O
terms	O
of	O
subregular	O
tree	O
languages	O
(	O
Graf	O
,	O
2012	O
;	O
Graf	O
and	O
Heinz	O
,	O
2015	O
)	O
.	O
Curiously	O
missing	O
from	O
these	O
investigations	O
is	O
morphology	O
.	O
While	O
linguistic	O
theories	O
sometimes	O
consider	O
morphology	O
a	O
part	O
of	O
syntax	O
,	O
computational	O
morphology	O
recognizes	O
that	O
the	O
weak	O
generative	O
capacity	O
of	O
morphology	O
is	O
much	O
closer	O
to	O
phonology	O
than	O
syntax	O
.	O
Consequently	O
,	O
computational	O
morphology	O
involves	O
largely	O
the	O
same	O
finite	O
-	O
state	O
methods	O
as	O
computational	O
phonology	O
(	O
Koskenniemi	O
,	O
1983	O
;	O
Karttunen	O
et	O
al	O
,	O
1992	O
)	O
.	O
This	O
raises	O
the	O
question	O
whether	O
morphology	O
,	O
just	O
like	O
phonology	O
,	O
uses	O
only	O
a	O
fraction	O
of	O
the	O
power	O
furnished	O
by	O
these	O
tools	O
.	O
A	O
positive	O
answer	O
would	O
have	O
important	O
repercussions	O
for	O
linguistics	O
as	O
well	O
as	O
natural	O
language	O
processing	O
.	O
The	O
subregular	O
classes	O
identified	O
in	O
computational	O
phonology	O
are	O
learnable	O
in	O
the	O
limit	O
from	O
positive	O
text	O
(	O
Heinz	O
et	O
al	O
,	O
2012	O
)	O
,	O
so	O
a	O
subregular	O
theory	O
of	O
morphology	O
would	O
greatly	O
simplify	O
machine	O
learning	O
while	O
also	O
explaining	O
how	O
morphological	O
dependencies	O
can	O
be	O
acquired	O
by	O
the	O
child	O
from	O
very	O
little	O
input	O
.	O
A	O
subregular	O
model	O
of	O
morphology	O
would	O
also	O
be	O
much	O
more	O
restricted	O
with	O
respect	O
to	O
what	O
processes	O
are	O
predicted	O
to	O
arise	O
in	O
natural	O
languages	O
.	O
It	O
thus	O
provides	O
a	O
much	O
tighter	O
typological	O
fit	O
than	O
the	O
regular	O
languages	O
.	O
In	O
this	O
paper	O
,	O
we	O
argue	O
that	O
the	O
subregular	O
view	O
of	O
morphology	O
is	O
indeed	O
correct	O
,	O
at	O
least	O
for	O
morphotactics	O
.	O
Morphotactics	O
describes	O
the	O
syntax	O
of	O
morphemes	O
,	O
that	O
is	O
to	O
say	O
,	O
their	O
linear	O
order	O
in	O
the	O
word	O
and	O
the	O
conditions	O
that	O
license	O
their	O
presence	O
or	O
enforce	O
their	O
absence	O
.	O
One	O
can	O
distinguish	O
surface	O
morphotactics	O
from	O
underlying	O
morphotactics	O
.	O
The	O
former	O
regulates	O
the	O
shape	O
of	O
the	O
pronounced	O
surface	O
strings	O
,	O
whereas	O
the	O
latter	O
is	O
only	O
concerned	O
with	O
the	O
arrangements	O
of	O
the	O
morphemes	O
in	O
the	O
initial	O
representation	O
rather	O
than	O
how	O
said	O
morphemes	O
are	O
realized	O
in	O
specific	O
environments	O
.	O
We	O
only	O
consider	O
underlying	O
morphotactics	O
in	O
this	O
paper	O
.	O
The	O
following	O
example	O
may	O
clarify	O
the	O
distinction	O
.	O
In	O
German	O
,	O
the	O
past	O
participle	O
of	O
a	O
verb	O
is	O
formed	O
via	O
a	O
circumfix	O
.	O
The	O
first	O
part	O
of	O
the	O
circumfix	O
is	O
always	O
the	O
prefix	O
ge	O
-	O
,	O
whereas	O
the	O
second	O
part	O
may	O
be	O
the	O
suffix	O
-	O
en	O
or	O
-	O
t	O
depending	O
on	O
the	O
verb	O
stem	O
.	O
In	O
addition	O
,	O
the	O
suffixes	O
can	O
also	O
occur	O
on	O
their	O
own	O
,	O
e.g.	O
on	O
infinitives	O
or	O
the	O
third	O
person	O
singular	O
form	O
of	O
the	O
verb	O
.	O
Surface	O
morphotactics	O
thus	O
has	O
to	O
ensure	O
that	O
ge	O
-	O
always	O
appears	O
with	O
one	O
of	O
these	O
two	O
suffixes	O
,	O
and	O
that	O
the	O
form	O
of	O
the	O
suffix	O
matches	O
the	O
stem	O
.	O
At	O
the	O
same	O
time	O
,	O
it	O
does	O
not	O
need	O
to	O
worry	O
about	O
matching	O
-	O
en	O
or	O
-	O
t	O
with	O
ge	O
-	O
since	O
these	O
forms	O
can	O
occur	O
independently	O
as	O
realizations	O
of	O
different	O
morphemes	O
.	O
Underlying	O
morphotactics	O
,	O
on	O
the	O
other	O
hand	O
,	O
is	O
unaware	O
of	O
the	O
surface	O
realizations	O
and	O
only	O
knows	O
that	O
some	O
abstract	O
prefix	O
GE	O
-	O
must	O
always	O
occur	O
with	O
the	O
abstract	O
suffix	O
-	O
EN	O
,	O
and	O
the	O
other	O
way	O
round	O
.	O
The	O
fact	O
that	O
-	O
EN	O
has	O
a	O
surface	O
realization	O
that	O
is	O
indistinguishable	O
from	O
the	O
infinitival	O
marker	O
,	O
which	O
does	O
not	O
require	O
the	O
prefix	O
GE	O
-	O
,	O
is	O
irrelevant	O
for	O
underlying	O
morphotactics	O
.	O
More	O
succinctly	O
:	O
underlying	O
morphotactics	O
regulates	O
the	O
distribution	O
of	O
morphemes	O
,	O
surface	O
morphotactics	O
the	O
distribution	O
of	O
allomorphs	O
.	O
This	O
paper	O
considers	O
a	O
variety	O
of	O
phenomenacircumfixation	O
,	O
variable	O
affix	O
ordering	O
,	O
unbounded	O
prefixation	O
-	O
and	O
concludes	O
that	O
they	O
all	O
belong	O
to	O
the	O
class	O
of	O
tier	O
-	O
based	O
strictly	O
local	O
languages	O
.	O
We	O
first	O
show	O
that	O
even	O
though	O
many	O
morphotactic	O
dependencies	O
are	O
strictly	O
local	O
,	O
that	O
is	O
not	O
the	O
case	O
for	O
all	O
of	O
them	O
(	O
Sec	O
.	O
2.1	O
)	O
.	O
While	O
some	O
of	O
these	O
outliers	O
are	O
strictly	O
piecewise	O
(	O
Sec	O
.	O
2.2	O
)	O
,	O
tier	O
-	O
based	O
strictly	O
local	O
grammars	O
are	O
needed	O
to	O
handle	O
the	O
full	O
range	O
of	O
data	O
points	O
(	O
Sec	O
.	O
2.3	O
)	O
.	O
This	O
prompts	O
our	O
conjecture	O
that	O
all	O
dependencies	O
that	O
are	O
part	O
of	O
underlying	O
morphotactics	O
stay	O
within	O
the	O
class	O
of	O
tier	O
-	O
based	O
strictly	O
local	O
languages	O
.	O
We	O
then	O
use	O
this	O
hypothesis	O
in	O
Sec	O
.	O
3	O
to	O
explain	O
two	O
typological	O
gaps	O
with	O
respect	O
to	O
compounding	O
markers	O
and	O
circumfixation	O
.	O

The	O
regular	O
languages	O
are	O
one	O
of	O
the	O
best	O
understood	O
language	O
classes	O
,	O
with	O
many	O
attractive	O
properties	O
.	O
Yet	O
it	O
is	O
often	O
forgotten	O
that	O
this	O
class	O
properly	O
includes	O
many	O
weaker	O
ones	O
(	O
McNaughton	O
and	O
Pappert	O
,	O
1971	O
)	O
,	O
some	O
of	O
which	O
have	O
recently	O
attracted	O
much	O
interest	O
in	O
computational	O
phonology	O
.	O
At	O
the	O
very	O
bottom	O
of	O
the	O
hierarchy	O
one	O
finds	O
strictly	O
local	O
and	O
strictly	O
piecewise	O
languages	O
(	O
Rogers	O
et	O
al	O
,	O
2010	O
)	O
,	O
and	O
a	O
little	O
bit	O
higher	O
up	O
the	O
tier	O
-	O
based	O
strictly	O
local	O
languages	O
(	O
Heinz	O
et	O
al	O
,	O
Regular	O
Star	O
-	O
Free	O
LTT	O
LT	O
SL	O
PT	O
SP	O
TSL	O
Figure	O
1	O
:	O
The	O
subregular	O
hierarchy	O
as	O
given	O
in	O
Heinz	O
et	O
al	O
(	O
2011	O
)	O
;	O
language	O
classes	O
in	O
dashed	O
boxes	O
are	O
studied	O
in	O
this	O
paper	O
2011	O
)	O
.	O
The	O
subregular	O
hierarchy	O
includes	O
many	O
other	O
classes	O
(	O
see	O
Fig	O
.	O
1	O
)	O
,	O
but	O
the	O
previous	O
three	O
are	O
noteworthy	O
because	O
they	O
are	O
conceptually	O
simple	O
and	O
efficiently	O
learnable	O
in	O
the	O
limit	O
from	O
positive	O
data	O
(	O
Heinz	O
et	O
al	O
,	O
2012	O
;	O
Jardine	O
and	O
Heinz	O
,	O
2016	O
)	O
while	O
also	O
furnishing	O
sufficient	O
power	O
for	O
a	O
wide	O
range	O
of	O
phonological	O
phenomena	O
(	O
Heinz	O
,	O
2015	O
;	O
Jardine	O
,	O
2015	O
)	O
.	O
In	O
this	O
section	O
,	O
we	O
investigate	O
the	O
role	O
of	O
strictly	O
local	O
,	O
strictly	O
piecewise	O
and	O
tier	O
-	O
based	O
strictly	O
local	O
patterns	O
in	O
morphotactics	O
.	O
We	O
show	O
that	O
some	O
but	O
not	O
all	O
patterns	O
are	O
strictly	O
local	O
or	O
strictly	O
piecewise	O
,	O
whereas	O
all	O
typologically	O
instantiated	O
patterns	O
seem	O
to	O
fit	O
in	O
the	O
class	O
of	O
tier	O
-	O
based	O
strictly	O
local	O
languages	O
.	O

While	O
SL	O
covers	O
a	O
wide	O
range	O
of	O
phenomena	O
,	O
it	O
is	O
n't	O
just	O
circumfixes	O
that	O
require	O
more	O
power	O
.	O
Problems	O
arise	O
whenever	O
a	O
dependency	O
involves	O
both	O
the	O
domain	O
of	O
prefixes	O
and	O
the	O
domain	O
of	O
suffixes	O
-	O
because	O
they	O
can	O
be	O
separated	O
by	O
arbitrarily	O
many	O
symbols	O
-	O
and	O
such	O
configurations	O
are	O
not	O
limited	O
to	O
circumfixes	O
.	O
In	O
most	O
languages	O
the	O
ordering	O
of	O
affixes	O
tends	O
to	O
be	O
fixed	O
,	O
but	O
there	O
are	O
languages	O
in	O
which	O
affixes	O
are	O
ordered	O
relatively	O
freely	O
and	O
do	O
not	O
follow	O
a	O
strict	O
template	O
,	O
thereby	O
creating	O
non	O
-	O
local	O
dependencies	O
.	O
Let	O
us	O
consider	O
the	O
following	O
data	O
from	O
Swahili	O
:	O
(	O
5	O
)	O
a.	O
a	O
-	O
This	O
data	O
is	O
taken	O
from	O
Stump	O
(	O
2016	O
)	O
.	O
Based	O
on	O
his	O
discussion	O
of	O
vyo	O
,	O
the	O
following	O
forms	O
are	O
ungrammatical	O
.	O
Different	O
generalizations	O
can	O
be	O
drawn	O
from	O
these	O
data	O
sets	O
,	O
some	O
of	O
which	O
are	O
more	O
complex	O
than	O
others	O
.	O
The	O
first	O
generalization	O
states	O
that	O
vyo	O
is	O
only	O
licensed	O
if	O
it	O
follows	O
either	O
a	O
segment	O
that	O
is	O
part	O
of	O
a	O
stem	O
or	O
the	O
prefix	O
si	O
.	O
This	O
is	O
a	O
strictly	O
2	O
-	O
local	O
pattern	O
,	O
and	O
it	O
explains	O
both	O
(	O
6a	O
)	O
and	O
(	O
6b	O
)	O
.	O
Alternatively	O
,	O
one	O
may	O
conclude	O
that	O
(	O
6b	O
)	O
is	O
ill	O
-	O
formed	O
because	O
there	O
is	O
more	O
than	O
one	O
occurrence	O
of	O
vyo	O
.	O
Such	O
a	O
ban	O
against	O
two	O
instances	O
of	O
vyo	O
is	O
also	O
supported	O
by	O
the	O
ill	O
-	O
formedness	O
of	O
(	O
6c	O
)	O
,	O
which	O
is	O
unexpected	O
under	O
the	O
first	O
generalization	O
.	O
Preventing	O
the	O
presence	O
of	O
two	O
instances	O
of	O
vyo	O
is	O
beyond	O
the	O
power	O
of	O
any	O
SL	O
grammar	O
G	O
:	O
if	O
uvx	O
+	O
c	O
⊂	O
L	O
(	O
G	O
)	O
and	O
uwcvx	O
+	O
⊂	O
L	O
(	O
G	O
)	O
,	O
then	O
L	O
(	O
G	O
)	O
must	O
also	O
contain	O
strings	O
of	O
the	O
form	O
uwcvx	O
+	O
c	O
(	O
due	O
to	O
suffix	O
substitution	O
closure	O
)	O
.	O
The	O
second	O
generalization	O
is	O
similar	O
to	O
the	O
phonological	O
requirement	O
that	O
no	O
word	O
may	O
contain	O
more	O
than	O
one	O
primary	O
stress	O
,	O
which	O
is	O
strictly	O
piecewise	O
(	O
SP	O
;	O
Heinz	O
(	O
2010	O
)	O
,	O
Rogers	O
et	O
al	O
(	O
2010	O
)	O
)	O
.	O
SP	O
grammars	O
work	O
exactly	O
the	O
same	O
as	O
SL	O
grammar	O
except	O
that	O
instead	O
of	O
illicit	O
substrings	O
they	O
list	O
illicit	O
subsequences	O
.	O
Given	O
a	O
string	O
w	O
,	O
its	O
set	O
of	O
k	O
-	O
sequences	O
is	O
k	O
-	O
seqs	O
(	O
w	O
)	O
:	O
=	O
s	O
|	O
s	O
is	O
a	O
subsequence	O
ofŵ	O
k−1	O
of	O
length	O
k	O
.	O
A	O
strictly	O
k	O
-	O
piecewise	O
grammar	O
G	O
is	O
a	O
finite	O
set	O
of	O
k	O
-	O
grams	O
over	O
Σ	O
∪	O
{	O
,	O
}	O
,	O
and	O
the	O
language	O
generated	O
by	O
G	O
is	O
L	O
:	O
=	O
{	O
w	O
|	O
k	O
-	O
seqs	O
(	O
w	O
)	O
∩	O
G	O
=	O
}	O
.	O
The	O
ban	O
against	O
two	O
occurrences	O
of	O
vyo	O
is	O
strictly	O
2	O
-	O
piecewise	O
-	O
the	O
grammar	O
only	O
need	O
to	O
contain	O
the	O
bigram	O
vyo	O
vyo	O
.	O
The	O
intersection	O
of	O
the	O
strictly	O
2	O
-	O
local	O
and	O
strictly	O
2	O
-	O
piecewise	O
languages	O
does	O
not	O
contain	O
(	O
6a	O
)	O
-	O
(	O
6c	O
)	O
,	O
as	O
desired	O
.	O
But	O
it	O
does	O
contain	O
(	O
6d	O
)	O
.	O
Both	O
generalizations	O
miss	O
that	O
even	O
though	O
vyo	O
can	O
occur	O
as	O
a	O
prefix	O
and	O
as	O
a	O
suffix	O
,	O
it	O
is	O
a	O
prefix	O
if	O
and	O
only	O
if	O
si	O
is	O
present	O
.	O
This	O
kind	O
of	O
conditional	O
positioning	O
can	O
not	O
be	O
captured	O
by	O
SL	O
grammars	O
,	O
and	O
the	O
culprit	O
is	O
once	O
again	O
suffix	O
substitution	O
closure	O
.	O
But	O
SP	O
grammars	O
by	O
themselves	O
are	O
not	O
sufficient	O
,	O
either	O
.	O
Suppose	O
we	O
increase	O
the	O
locality	O
rank	O
from	O
2	O
to	O
3	O
and	O
include	O
si	O
x	O
vyo	O
as	O
an	O
illicit	O
subsequence	O
in	O
our	O
SP	O
grammar	O
.	O
This	O
forces	O
vyo	O
to	O
be	O
a	O
prefix	O
in	O
the	O
presence	O
of	O
si	O
.	O
However	O
,	O
it	O
still	O
incorrectly	O
allows	O
for	O
vyo	O
to	O
be	O
a	O
prefix	O
in	O
the	O
absence	O
of	O
si	O
.	O
No	O
SP	O
grammar	O
can	O
prevent	O
this	O
outcome	O
.	O
The	O
problem	O
is	O
that	O
any	O
word	O
of	O
the	O
form	O
u	O
vyo	O
v	O
x	O
contains	O
only	O
subsequences	O
that	O
also	O
occur	O
in	O
the	O
well	O
-	O
formed	O
u	O
si	O
vyo	O
v	O
x.	O
Consequently	O
,	O
any	O
SP	O
grammar	O
that	O
generates	O
the	O
latter	O
also	O
generates	O
the	O
former	O
.	O
It	O
is	O
only	O
in	O
combination	O
with	O
the	O
SL	O
grammar	O
that	O
we	O
can	O
correctly	O
rule	O
out	O
prefix	O
vyo	O
without	O
a	O
preceding	O
si	O
.	O
Swahili	O
's	O
inflectional	O
morphology	O
thus	O
provides	O
evidence	O
that	O
SL	O
is	O
not	O
enough	O
to	O
handle	O
all	O
aspects	O
of	O
morphotactics	O
and	O
must	O
be	O
supplemented	O
by	O
some	O
mechanism	O
to	O
handle	O
long	O
-	O
distance	O
dependencies	O
,	O
with	O
SP	O
being	O
one	O
option	O
.	O
But	O
even	O
the	O
combination	O
of	O
SL	O
and	O
SP	O
can	O
not	O
capture	O
all	O
non	O
-	O
local	O
dependencies	O
.	O
In	O
Swahili	O
,	O
the	O
inability	O
of	O
SP	O
mechanisms	O
to	O
enforce	O
the	O
presence	O
of	O
si	O
with	O
prefix	O
vyo	O
could	O
be	O
remedied	O
by	O
the	O
strictly	O
local	O
requirement	O
that	O
vyo	O
may	O
only	O
occur	O
after	O
si	O
or	O
a	O
stem	O
.	O
This	O
elegant	O
interaction	O
of	O
SL	O
and	O
SP	O
is	O
not	O
always	O
possible	O
,	O
however	O
.	O
The	O
most	O
noteworthy	O
case	O
are	O
circumfixes	O
.	O
Consider	O
some	O
arbitrary	O
circumfix	O
u	O
-	O
-	O
v.	O
Clearly	O
all	O
subsequences	O
of	O
ux	O
+	O
are	O
subsequences	O
of	O
ux	O
+	O
v	O
,	O
so	O
if	O
the	O
latter	O
is	O
generated	O
by	O
some	O
SP	O
grammar	O
then	O
by	O
definition	O
the	O
former	O
must	O
be	O
,	O
too	O
.	O
The	O
underlying	O
problem	O
is	O
that	O
SP	O
grammars	O
can	O
only	O
enforce	O
the	O
absence	O
of	O
an	O
affix	O
,	O
not	O
its	O
presence	O
.	O
Circumfixes	O
where	O
the	O
presence	O
of	O
one	O
affix	O
entails	O
the	O
presence	O
of	O
the	O
other	O
affix	O
thus	O
are	O
not	O
SP	O
.	O
It	O
seems	O
that	O
we	O
must	O
move	O
higher	O
up	O
the	O
subregular	O
hierarchy	O
in	O
order	O
to	O
accommodate	O
circumfixes	O
,	O
which	O
will	O
also	O
have	O
the	O
welcome	O
side	O
-	O
effect	O
of	O
providing	O
a	O
simpler	O
account	O
for	O
the	O
distribution	O
of	O
Swahili	O
vyo	O
.	O

If	O
the	O
subregular	O
hypothesis	O
is	O
correct	O
,	O
then	O
no	O
morphological	O
pattern	O
may	O
exceed	O
the	O
computational	O
power	O
furnished	O
by	O
tier	O
-	O
based	O
strictly	O
local	O
grammars	O
.	O
In	O
particular	O
,	O
whenever	O
the	O
combination	O
of	O
two	O
attested	O
TSL	O
patterns	O
is	O
not	O
TSL	O
,	O
that	O
combination	O
should	O
not	O
be	O
attested	O
.	O
The	O
subreg	O
-	O
ular	O
hypothesis	O
thus	O
provides	O
a	O
principled	O
explanation	O
for	O
typological	O
gaps	O
.	O
In	O
this	O
section	O
we	O
consider	O
two	O
such	O
cases	O
related	O
to	O
compounding	O
markers	O
and	O
the	O
limits	O
of	O
circumfixation	O
.	O

Compounding	O
describes	O
the	O
combination	O
of	O
two	O
or	O
more	O
stems	O
to	O
form	O
a	O
compound	O
lexeme	O
,	O
where	O
the	O
stems	O
may	O
belong	O
to	O
different	O
categories	O
.	O
Languages	O
differ	O
with	O
respect	O
to	O
whether	O
compounding	O
is	O
(	O
at	O
least	O
sometimes	O
)	O
explicitly	O
marked	O
.	O
In	O
the	O
following	O
we	O
exhibit	O
two	O
TSL	O
compounding	O
patterns	O
from	O
Turkish	O
and	O
Russian	O
whose	O
combination	O
is	O
not	O
typologically	O
attested	O
.	O
We	O
then	O
explain	O
why	O
this	O
combined	O
pattern	O
is	O
not	O
TSL	O
,	O
deriving	O
the	O
otherwise	O
puzzling	O
typological	O
gap	O
.	O
Turkish	O
possessive	O
compounds	O
(	O
see	O
Aslan	O
and	O
Altan	O
(	O
1998	O
)	O
for	O
a	O
detailed	O
description	O
)	O
obey	O
the	O
general	O
pattern	O
stem	O
-	O
stem	O
+	O
-	O
o	O
,	O
where	O
o	O
stands	O
for	O
the	O
compounding	O
marker	O
-	O
sI.	O
Assuming	O
once	O
again	O
the	O
presence	O
of	O
the	O
special	O
symbol	O
#	O
-	O
which	O
marked	O
the	O
edges	O
of	O
stems	O
in	O
the	O
previous	O
section	O
-	O
we	O
can	O
show	O
this	O
pattern	O
to	O
also	O
be	O
tier	O
-	O
based	O
strictly	O
2	O
-	O
local	O
.	O
In	O
this	O
case	O
,	O
the	O
illicit	O
bigrams	O
are	O
#	O
#	O
,	O
oo	O
,	O
o	O
,	O
and	O
o	O
.	O
Observe	O
that	O
we	O
can	O
remove	O
the	O
first	O
one	O
of	O
these	O
bigrams	O
to	O
allow	O
for	O
cases	O
where	O
the	O
compounding	O
marker	O
is	O
optional	O
.	O
One	O
may	O
wonder	O
now	O
whether	O
it	O
is	O
possible	O
for	O
natural	O
languages	O
to	O
display	O
a	O
combination	O
of	O
the	O
compounding	O
patterns	O
seen	O
with	O
Russian	O
and	O
Turkish	O
.	O
From	O
a	O
linguistic	O
perspective	O
,	O
the	O
expected	O
answer	O
is	O
yes	O
.	O
If	O
compounding	O
can	O
be	O
marked	O
by	O
a	O
suffix	O
as	O
in	O
Turkish	O
,	O
and	O
compounding	O
can	O
introduce	O
a	O
marker	O
with	O
each	O
step	O
as	O
in	O
Russian	O
,	O
then	O
it	O
should	O
be	O
possible	O
to	O
introduce	O
a	O
suffix	O
with	O
each	O
compounding	O
step	O
.	O
But	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
no	O
language	O
instantiates	O
this	O
system	O
.	O
From	O
a	O
computational	O
perspective	O
,	O
on	O
the	O
other	O
hand	O
,	O
this	O
typological	O
gap	O
is	O
expected	O
because	O
the	O
described	O
system	O
is	O
not	O
TSL	O
-	O
as	O
a	O
matter	O
of	O
fact	O
,	O
it	O
is	O
n't	O
even	O
regular	O
.	O
A	O
language	O
L	O
that	O
suffixes	O
a	O
marker	O
to	O
a	O
compound	O
with	O
each	O
compounding	O
step	O
would	O
produce	O
compounds	O
where	O
the	O
number	O
of	O
compound	O
markers	O
is	O
proportional	O
to	O
the	O
number	O
of	O
stems	O
.	O
Let	O
h	O
be	O
a	O
map	O
that	O
replaces	O
all	O
stems	O
by	O
s	O
,	O
all	O
compound	O
markers	O
by	O
o	O
,	O
and	O
all	O
other	O
material	O
by	O
some	O
other	O
symbol	O
.	O
Intersecting	O
h	O
(	O
L	O
)	O
with	O
the	O
regular	O
language	O
s	O
+	O
o	O
+	O
yields	O
the	O
language	O
s	O
m	O
o	O
n	O
,	O
m	O
>	O
n.	O
This	O
string	O
set	O
is	O
easily	O
shown	O
to	O
be	O
context	O
-	O
free	O
(	O
e.g.	O
via	O
the	O
Myhill	O
-	O
Nerode	O
theorem	O
)	O
,	O
and	O
since	O
regular	O
languages	O
are	O
closed	O
under	O
homomorphisms	O
and	O
intersection	O
,	O
it	O
follows	O
that	O
L	O
can	O
not	O
be	O
regular	O
.	O
But	O
every	O
TSL	O
language	O
is	O
regular	O
,	O
so	O
the	O
combination	O
of	O
Russian	O
and	O
Turkish	O
outlined	O
above	O
is	O
not	O
TSL	O
.	O
The	O
absence	O
of	O
this	O
compounding	O
pattern	O
in	O
the	O
typology	O
of	O
natural	O
languages	O
thus	O
lends	O
further	O
support	O
to	O
our	O
conjecture	O
that	O
natural	O
language	O
morphotactics	O
is	O
limited	O
to	O
TSL	O
dependencies	O
.	O

Circumfixation	O
already	O
played	O
an	O
important	O
role	O
in	O
motivating	O
TSL	O
as	O
a	O
reasonable	O
lower	O
bound	O
on	O
how	O
much	O
power	O
is	O
required	O
for	O
natural	O
lan	O
-	O
guage	O
morphotactics	O
.	O
We	O
now	O
show	O
that	O
just	O
like	O
compounding	O
markers	O
,	O
circumfixation	O
also	O
suggests	O
that	O
TSL	O
marks	O
the	O
upper	O
bound	O
on	O
required	O
expressivity	O
.	O
In	O
particular	O
,	O
unbounded	O
affixation	O
is	O
widely	O
attested	O
across	O
languages	O
,	O
whereas	O
unbounded	O
circumfixation	O
is	O
not	O
.	O
A	O
number	O
of	O
languages	O
allow	O
some	O
of	O
their	O
affixes	O
to	O
occur	O
multiple	O
times	O
.	O
For	O
instance	O
,	O
the	O
Russian	O
temporal	O
prefix	O
posle	O
-	O
can	O
appear	O
iteratively	O
in	O
the	O
beginning	O
of	O
a	O
word	O
like	O
zavtra	O
'	O
tomorrow	O
'	O
.	O
The	O
very	O
same	O
pattern	O
is	O
also	O
found	O
in	O
German	O
,	O
with	O
morgen	O
'	O
tomorrow	O
'	O
,	O
über	O
-	O
morgen	O
'	O
the	O
day	O
after	O
tomorrow	O
'	O
,	O
über	O
-	O
über	O
-	O
morgen	O
'	O
the	O
day	O
after	O
the	O
day	O
after	O
tomorrow	O
'	O
,	O
and	O
so	O
on	O
.	O
German	O
also	O
has	O
the	O
pattern	O
ur	O
-	O
groß	O
-	O
vater	O
,	O
ur	O
-	O
ur	O
-	O
groß	O
-	O
vater	O
,	O
which	O
is	O
the	O
analog	O
of	O
English	O
great	O
grandfather	O
,	O
great	O
great	O
grandfather	O
and	O
its	O
iterations	O
(	O
various	O
linguistic	O
diagnostics	O
show	O
that	O
these	O
are	O
morphological	O
words	O
rather	O
than	O
phrases	O
)	O
.	O
Note	O
that	O
in	O
all	O
those	O
cases	O
it	O
is	O
impossible	O
to	O
insert	O
other	O
prefixes	O
between	O
the	O
iterated	O
prefix	O
:	O
*	O
ur	O
-	O
groß	O
-	O
ur	O
-	O
ur	O
-	O
großvater	O
.	O
In	O
sum	O
,	O
some	O
affixes	O
can	O
be	O
freely	O
iterated	O
as	O
long	O
as	O
no	O
other	O
affixes	O
intervene	O
.	O
These	O
patterns	O
are	O
all	O
TSL	O
by	O
virtue	O
of	O
being	O
strictly	O
2	O
-	O
local	O
.	O
We	O
illustrate	O
this	O
claim	O
with	O
German	O
.	O
We	O
ignore	O
the	O
problem	O
of	O
how	O
groß	O
can	O
be	O
restricted	O
to	O
occur	O
only	O
with	O
specific	O
stems	O
(	O
if	O
stems	O
are	O
atomic	O
symbols	O
,	O
this	O
is	O
trivial	O
,	O
otherwise	O
it	O
requires	O
a	O
strictly	O
k	O
-	O
local	O
grammar	O
over	O
the	O
string	O
of	O
phonological	O
segments	O
where	O
k	O
is	O
large	O
enough	O
to	O
contain	O
both	O
groß	O
and	O
the	O
relevant	O
stems	O
)	O
.	O
We	O
also	O
assume	O
,	O
as	O
before	O
,	O
that	O
there	O
is	O
some	O
marker	O
#	O
that	O
marks	O
the	O
edges	O
of	O
stems	O
.	O
Then	O
to	O
ensure	O
that	O
the	O
relevant	O
strings	O
of	O
prefixes	O
follow	O
the	O
pattern	O
ur	O
*	O
groß	O
#	O
,	O
the	O
sequences	O
großur	O
,	O
großgroß	O
,	O
and	O
ur	O
#	O
are	O
marked	O
as	O
illicit	O
.	O
Unbounded	O
prefixation	O
thus	O
stays	O
within	O
the	O
class	O
of	O
TSL	O
dependencies	O
.	O
An	O
interesting	O
counterpart	O
to	O
Russian	O
and	O
German	O
is	O
Ilocano	O
(	O
Galvez	O
Rubino	O
,	O
1998	O
)	O
,	O
which	O
uses	O
the	O
circumfix	O
ka	O
-	O
-	O
an	O
with	O
a	O
function	O
similar	O
to	O
posle	O
andüber	O
.	O
As	O
before	O
,	O
there	O
is	O
little	O
linguistic	O
reason	O
why	O
unbounded	O
circumfixation	O
should	O
be	O
blocked	O
.	O
If	O
affixation	O
can	O
be	O
unbounded	O
to	O
construct	O
more	O
and	O
more	O
complex	O
versions	O
of	O
day	O
after	O
tomorrow	O
,	O
and	O
day	O
after	O
tomorrow	O
can	O
be	O
derived	O
via	O
circumfixation	O
,	O
then	O
one	O
would	O
expect	O
unbounded	O
circumfixation	O
to	O
be	O
a	O
viable	O
option	O
.	O
But	O
once	O
again	O
there	O
is	O
a	O
clear	O
computational	O
reason	O
as	O
to	O
why	O
this	O
does	O
not	O
happen	O
:	O
the	O
corresponding	O
morphotactic	O
system	O
would	O
no	O
longer	O
be	O
TSL	O
.	O
Let	O
L	O
be	O
some	O
minor	O
variant	O
of	O
Russian	O
where	O
posle	O
-	O
is	O
instead	O
a	O
circumfix	O
pos	O
-	O
-	O
le	O
.	O
As	O
before	O
we	O
let	O
h	O
be	O
a	O
homomorphism	O
that	O
replaces	O
all	O
stems	O
by	O
s	O
,	O
the	O
two	O
parts	O
of	O
the	O
circumfix	O
by	O
o	O
,	O
and	O
all	O
other	O
material	O
by	O
some	O
distinct	O
symbol	O
.	O
The	O
intersection	O
of	O
h	O
(	O
L	O
)	O
with	O
the	O
regular	O
language	O
o	O
+	O
so	O
+	O
is	O
the	O
context	O
-	O
free	O
string	O
set	O
o	O
n	O
so	O
n	O
.	O
Therefore	O
L	O
is	O
supra	O
-	O
regular	O
and	O
can	O
not	O
be	O
tier	O
-	O
based	O
strictly	O
local	O
.	O
Unbounded	O
circumfixation	O
simply	O
can	O
not	O
be	O
reconciled	O
with	O
the	O
subregular	O
hypothesis	O
.	O

The	O
received	O
view	O
is	O
that	O
all	O
of	O
morphology	O
is	O
easily	O
modeled	O
with	O
finite	O
-	O
state	O
machines	O
(	O
Koskenniemi	O
,	O
1983	O
;	O
Karttunen	O
et	O
al	O
,	O
1992	O
)	O
.	O
We	O
contend	O
that	O
this	O
view	O
is	O
overly	O
generous	O
and	O
that	O
tighter	O
bounds	O
can	O
be	O
established	O
,	O
at	O
least	O
for	O
specific	O
subparts	O
of	O
morphology	O
.	O
Morphotactics	O
defines	O
the	O
restrictions	O
on	O
the	O
possible	O
orderings	O
of	O
morphological	O
units	O
,	O
and	O
we	O
argued	O
based	O
on	O
data	O
from	O
typologically	O
diverse	O
languages	O
that	O
the	O
power	O
of	O
natural	O
language	O
morphotactics	O
is	O
severely	O
restricted	O
:	O
Subregular	O
Morphotactics	O
All	O
morphotactic	O
dependencies	O
are	O
tier	O
-	O
based	O
strictly	O
local	O
.	O
In	O
contrast	O
to	O
regular	O
languages	O
,	O
tier	O
-	O
based	O
strictly	O
local	O
languages	O
are	O
efficiently	O
learnable	O
in	O
the	O
limit	O
from	O
positive	O
text	O
(	O
Heinz	O
et	O
al	O
,	O
2012	O
;	O
Jardine	O
and	O
Heinz	O
,	O
2016	O
)	O
.	O
Our	O
result	O
thus	O
marks	O
a	O
first	O
step	O
towards	O
provably	O
correct	O
machine	O
learning	O
algorithms	O
for	O
natural	O
language	O
morphology	O
.	O
Admittedly	O
,	O
morphotactics	O
is	O
just	O
one	O
of	O
several	O
parts	O
of	O
morphology	O
.	O
We	O
put	O
aside	O
allomorphy	O
and	O
only	O
considered	O
the	O
distribution	O
of	O
morphemes	O
in	O
the	O
underlying	O
forms	O
.	O
Even	O
within	O
that	O
narrow	O
area	O
we	O
did	O
not	O
thoroughly	O
explore	O
all	O
facets	O
,	O
for	O
instance	O
infixation	O
and	O
incorporation	O
.	O
Nonetheless	O
our	O
results	O
show	O
that	O
the	O
success	O
of	O
the	O
subregular	O
perspective	O
need	O
not	O
be	O
limited	O
to	O
phonology	O
.	O
At	O
least	O
morphotactics	O
can	O
be	O
insightfully	O
studied	O
through	O
this	O
lens	O
,	O
too	O
.	O
In	O
addition	O
,	O
there	O
has	O
been	O
a	O
lot	O
of	O
progress	O
in	O
extending	O
the	O
subregular	O
hierarchy	O
from	O
languages	O
to	O
transductions	O
(	O
see	O
Chandlee	O
(	O
2014	O
)	O
and	O
references	O
therein	O
)	O
,	O
and	O
we	O
are	O
confident	O
that	O
these	O
results	O
will	O
allow	O
us	O
to	O
expand	O
the	O
focus	O
of	O
investigation	O
from	O
morphotactics	O
to	O
morphology	O
at	O
large	O
.	O
It	O
will	O
also	O
be	O
interesting	O
to	O
see	O
how	O
uniform	O
the	O
complexity	O
bounds	O
are	O
across	O
different	O
modules	O
of	O
morphology	O
.	O
In	O
phonology	O
,	O
suprasegmental	O
dependencies	O
tend	O
to	O
be	O
more	O
complex	O
than	O
segmental	O
ones	O
(	O
Jardine	O
,	O
2015	O
)	O
.	O
Most	O
constructions	O
in	O
this	O
paper	O
involve	O
derivational	O
morphology	O
,	O
but	O
the	O
affix	O
vyo	O
in	O
Swahili	O
is	O
related	O
to	O
inflectional	O
morphology	O
and	O
turned	O
out	O
to	O
have	O
a	O
distribution	O
that	O
is	O
neither	O
SL	O
nor	O
SP	O
(	O
although	O
it	O
can	O
be	O
captured	O
with	O
a	O
combination	O
of	O
the	O
two	O
)	O
.	O
So	O
both	O
derivational	O
and	O
inflectional	O
morphotactics	O
occupy	O
points	O
in	O
TSL	O
\	O
(	O
SL	O
∪	O
SP	O
)	O
.	O
In	O
this	O
regard	O
it	O
is	O
also	O
worth	O
noting	O
that	O
some	O
phonological	O
processes	O
such	O
as	O
tone	O
plateauing	O
belong	O
to	O
SP	O
\	O
TSL	O
,	O
whereas	O
no	O
morphological	O
dependencies	O
seem	O
to	O
be	O
part	O
of	O
this	O
subclass	O
.	O
We	O
hope	O
to	O
address	O
these	O
and	O
related	O
issues	O
in	O
future	O
work	O
.	O

There	O
is	O
a	O
long	O
history	O
of	O
pre	O
-	O
training	O
general	O
language	O
representations	O
,	O
and	O
we	O
briefly	O
review	O
the	O
most	O
widely	O
-	O
used	O
approaches	O
in	O
this	O
section	O
.	O

NovelPerspective	O
:	O
Identifying	O
Point	O
of	O
View	O
Characters	O

We	O
present	O
NovelPerspective	O
:	O
a	O
tool	O
to	O
allow	O
consumers	O
to	O
subset	O
their	O
digital	O
literature	O
,	O
based	O
on	O
point	O
of	O
view	O
(	O
POV	O
)	O
character	O
.	O
Many	O
novels	O
have	O
multiple	O
main	O
characters	O
each	O
with	O
their	O
own	O
storyline	O
running	O
in	O
parallel	O
.	O
A	O
well	O
-	O
known	O
example	O
is	O
George	O
R.	O
R.	O
Martin	O
's	O
novel	O
:	O
"	O
A	O
Game	O
of	O
Thrones	O
"	O
,	O
and	O
others	O
from	O
that	O
series	O
.	O
Our	O
tool	O
detects	O
the	O
main	O
character	O
that	O
each	O
section	O
is	O
from	O
the	O
POV	O
of	O
,	O
and	O
allows	O
the	O
user	O
to	O
generate	O
a	O
new	O
ebook	O
with	O
only	O
those	O
sections	O
.	O
This	O
gives	O
consumers	O
new	O
options	O
in	O
how	O
they	O
consume	O
their	O
media	O
;	O
allowing	O
them	O
to	O
pursue	O
the	O
storylines	O
sequentially	O
,	O
or	O
skip	O
chapters	O
about	O
characters	O
they	O
find	O
boring	O
.	O
We	O
present	O
two	O
heuristic	O
-	O
based	O
baselines	O
,	O
and	O
two	O
machine	O
learning	O
based	O
methods	O
for	O
the	O
detection	O
of	O
the	O
main	O
character	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
no	O
systems	O
have	O
been	O
developed	O
for	O
this	O
task	O
before	O
.	O
As	O
such	O
,	O
we	O
have	O
developed	O
two	O
deterministic	O
baseline	O
character	O
classifiers	O
.	O
These	O
are	O
both	O
potentially	O
useful	O
to	O
the	O
end	O
-	O
user	O
in	O
our	O
deployed	O
system	O
(	O
Section	O
5	O
)	O
,	O
and	O
used	O
to	O
gauge	O
the	O
performance	O
of	O
the	O
more	O
complicated	O
systems	O
in	O
the	O
evaluations	O
presented	O
in	O
Section	O
4	O
.	O
It	O
should	O
be	O
noted	O
that	O
the	O
baseline	O
systems	O
,	O
while	O
not	O
using	O
machine	O
learning	O
for	O
the	O
character	O
classification	O
steps	O
,	O
do	O
make	O
extensive	O
use	O
of	O
machine	O
learning	O
-	O
based	O
systems	O
during	O
the	O
preprocessing	O
stages	O
.	O

An	O
obvious	O
way	O
to	O
determine	O
the	O
main	O
character	O
of	O
the	O
section	O
is	O
to	O
select	O
the	O
first	O
named	O
entity	O
.	O
We	O
use	O
this	O
to	O
define	O
the	O
"	O
First	O
Mentioned	O
"	O
baseline	O
In	O
this	O
system	O
,	O
the	O
Feature	O
Extraction	O
step	O
is	O
simply	O
retrieving	O
the	O
position	O
of	O
the	O
first	O
use	O
of	O
each	O
name	O
;	O
and	O
the	O
Character	O
Scoring	O
step	O
assigns	O
each	O
a	O
score	O
such	O
that	O
earlier	O
is	O
higher	O
.	O
This	O
works	O
for	O
many	O
examples	O
:	O
"	O
One	O
dark	O
and	O
stormy	O
night	O
,	O
Bill	O
heard	O
a	O
knock	O
at	O
the	O
door	O
.	O
"	O
;	O
however	O
it	O
fails	O
for	O
many	O
others	O
:	O
"	O
'	O
Is	O
that	O
Tom	O
?	O
'	O
called	O
out	O
Bill	O
,	O
after	O
hearing	O
a	O
knock	O
.	O
''	O
.	O
Sometimes	O
a	O
section	O
may	O
go	O
several	O
paragraphs	O
describing	O
events	O
before	O
it	O
even	O
mentions	O
the	O
character	O
who	O
is	O
perceiving	O
them	O
.	O
This	O
is	O
a	O
varying	O
element	O
of	O
style	O
.	O

A	O
more	O
robust	O
method	O
to	O
determine	O
the	O
main	O
character	O
,	O
is	O
to	O
use	O
the	O
occurrence	O
counts	O
.	O
We	O
call	O
this	O
the	O
"	O
Most	O
Mentioned	O
"	O
baseline	O
.	O
The	O
Feature	O
Extraction	O
step	O
is	O
to	O
count	O
how	O
often	O
the	O
name	O
is	O
used	O
.	O
The	O
Character	O
Scoring	O
step	O
assigns	O
each	O
a	O
score	O
based	O
what	O
proportional	O
of	O
all	O
names	O
used	O
were	O
for	O
this	O
entity	O
.	O
This	O
works	O
well	O
for	O
many	O
books	O
.	O
The	O
more	O
important	O
a	O
character	O
is	O
,	O
the	O
more	O
often	O
their	O
name	O
occurs	O
.	O
However	O
,	O
it	O
is	O
fooled	O
,	O
for	O
example	O
,	O
by	O
book	O
chapters	O
that	O
are	O
about	O
the	O
POV	O
character	O
's	O
relationship	O
with	O
a	O
secondary	O
character	O
.	O
In	O
such	O
cases	O
the	O
secondary	O
character	O
may	O
be	O
mentioned	O
more	O
often	O
.	O

In	O
the	O
evaluation	O
,	O
the	O
systems	O
are	O
given	O
the	O
body	O
text	O
and	O
asked	O
to	O
predict	O
the	O
character	O
names	O
.	O
During	O
evaluation	O
,	O
we	O
sum	O
the	O
scores	O
of	O
the	O
characters	O
alternative	O
aliases	O
/	O
nick	O
-	O
names	O
used	O
in	O
the	O
books	O
.	O
For	O
example	O
merging	O
Ned	O
into	O
Eddard	O
in	O
ASOIAF	O
.	O
This	O
roughly	O
corresponds	O
to	O
the	O
case	O
that	O
a	O
normal	O
user	O
can	O
enter	O
multiple	O
aliases	O
into	O
our	O
application	O
when	O
selecting	O
sections	O
to	O
keep	O
.	O
We	O
do	O
not	O
use	O
these	O
aliases	O
during	O
training	O
,	O
though	O
that	O
is	O
an	O
option	O
that	O
could	O
be	O
investigated	O
in	O
a	O
future	O
work	O
.	O

The	O
full	O
source	O
code	O
is	O
available	O
on	O
GitHub	O
.	O
3	O
Scikit	O
-	O
Learn	O
(	O
Pedregosa	O
et	O
al	O
,	O
2011	O
)	O
is	O
used	O
for	O
the	O
machine	O
learning	O
and	O
evaluations	O
,	O
and	O
NLTK	O
(	O
Bird	O
and	O
Loper	O
,	O
2004	O
)	O
is	O
used	O
for	O
textual	O
preprocessing	O
.	O
The	O
text	O
is	O
tokenised	O
,	O
and	O
tagged	O
with	O
POS	O
and	O
named	O
entities	O
using	O
NLTK	O
's	O
default	O
methods	O
.	O
Specifically	O
,	O
these	O
are	O
the	O
Punkt	O
sentence	O
tokenizer	O
,	O
the	O
regex	O
-	O
based	O
improved	O
Tree	O
-	O
Bank	O
word	O
tokenizer	O
,	O
greedy	O
averaged	O
perceptron	O
POS	O
tagger	O
,	O
and	O
the	O
max	O
-	O
entropy	O
binary	O
named	O
entity	O
chunker	O
.	O
The	O
use	O
of	O
a	O
binary	O
,	O
rather	O
than	O
a	O
multi	O
-	O
class	O
,	O
named	O
entity	O
chunker	O
is	O
significant	O
.	O
Fantasy	O
novels	O
often	O
use	O
"	O
exotic	O
"	O
names	O
for	O
characters	O
,	O
we	O
found	O
that	O
this	O
often	O
resulted	O
in	O
character	O
named	O
entities	O
being	O
misclassified	O
as	O
organisations	O
or	O
places	O
.	O
Note	O
that	O
this	O
is	O
particularly	O
disadvantageous	O
to	O
the	O
First	O
Mentioned	O
baseline	O
,	O
as	O
any	O
kind	O
of	O
named	O
entity	O
will	O
steal	O
the	O
place	O
.	O
Nevertheless	O
,	O
it	O
is	O
required	O
to	O
ensure	O
that	O
all	O
character	O
names	O
are	O
a	O
possibility	O
to	O
be	O
selected	O
.	O

The	O
demonstration	O
system	O
is	O
deployed	O
online	O
at	O
https://white.ucc.asn.au/tools/np	O
.	O
A	O
video	O
demonstrating	O
its	O
use	O
can	O
be	O
found	O
at	O
https://youtu.be/iu41pUF4wTY	O
.	O
This	O
web	O
-	O
app	O
,	O
made	O
using	O
the	O
CherryPy	O
framework	O
,	O
4	O
allows	O
the	O
user	O
to	O
apply	O
any	O
of	O
the	O
model	O
discussed	O
to	O
their	O
own	O
novels	O
.	O
The	O
web	O
-	O
app	O
functions	O
as	O
shown	O
in	O
Figure	O
1	O
.	O
The	O
user	O
uploads	O
an	O
ebook	O
,	O
and	O
selects	O
one	O
of	O
the	O
character	O
classification	O
systems	O
that	O
we	O
have	O
discussed	O
above	O
.	O
They	O
are	O
then	O
presented	O
with	O
a	O
page	O
displaying	O
a	O
list	O
of	O
sections	O
,	O
with	O
the	O
predicted	O
main	O
character	O
(	O
/s	O
)	O
paired	O
with	O
an	O
excerpt	O
from	O
the	O
beginning	O
of	O
the	O
section	O
.	O
The	O
user	O
can	O
adjust	O
to	O
show	O
the	O
top	O
-	O
k	O
most	O
-	O
likely	O
characters	O
on	O
this	O
screen	O
,	O
to	O
allow	O
for	O
additional	O
recall	O
.	O
The	O
user	O
can	O
select	O
sections	O
to	O
retain	O
.	O
They	O
can	O
use	O
a	O
regular	O
expression	O
to	O
match	O
the	O
character	O
names	O
(	O
/s	O
)	O
they	O
are	O
interested	O
in	O
.	O
The	O
sections	O
with	O
matching	O
predicted	O
character	O
names	O
will	O
be	O
selected	O
.	O
As	O
none	O
of	O
the	O
models	O
is	O
perfect	O
,	O
some	O
mistakes	O
are	O
likely	O
.	O
The	O
user	O
can	O
manually	O
correct	O
the	O
selection	O
before	O
downloading	O
the	O
book	O
.	O

Acknowledgements	O
This	O
research	O
was	O
partially	O
funded	O
by	O
Australian	O
Research	O
Council	O
grants	O
DP150102405	O
and	O
LP110100050	O
.	O

We	O
classified	O
the	O
schemes	O
according	O
to	O
the	O
annotation	O
level	O
they	O
address	O
,	O
either	O
on	O
the	O
sentence	O
,	O
entity	O
or	O
relation	O
-	O
level	O
.	O
We	O
present	O
a	O
summary	O
of	O
all	O
schemes	O
that	O
we	O
found	O
,	O
but	O
give	O
a	O
more	O
detailed	O
description	O
for	O
(	O
selected	O
)	O
schemes	O
for	O
which	O
an	O
annotated	O
corpus	O
is	O
available	O
(	O
cf	O
.	O
Table	O
1	O
)	O
.	O

We	O
would	O
like	O
to	O
thank	O
Arnaldo	O
Candido	O
Junior	O
and	O
Sandra	O
Maria	O
Aluísio	O
from	O
the	O
MAZEA	O
tool	O
for	O
kindly	O
processing	O
our	O
documents	O
.	O
We	O
also	O
would	O
like	O
to	O
thank	O
Animesh	O
Prasad	O
and	O
Min	O
-	O
Yen	O
Kan	O
for	O
their	O
support	O
when	O
using	O
their	O
tool	O
.	O

IST	O
-	O
Unbabel	O
2021	O
Submission	O
for	O
the	O
Quality	O
Estimation	O
Shared	O
Task	O

We	O
present	O
the	O
joint	O
contribution	O
of	O
IST	O
and	O
Unbabel	O
to	O
the	O
WMT	O
2021	O
Shared	O
Task	O
on	O
Quality	O
Estimation	O
.	O
Our	O
team	O
participated	O
on	O
two	O
tasks	O
:	O
Direct	O
Assessment	O
and	O
Post	O
-	O
Editing	O
Effort	O
,	O
encompassing	O
a	O
total	O
of	O
35	O
submissions	O
.	O
For	O
all	O
submissions	O
,	O
our	O
efforts	O
focused	O
on	O
training	O
multilingual	O
models	O
on	O
top	O
of	O
OpenKiwi	O
predictor	O
-	O
estimator	O
architecture	O
,	O
using	O
pre	O
-	O
trained	O
multilingual	O
encoders	O
combined	O
with	O
adapters	O
.	O
We	O
further	O
experiment	O
with	O
and	O
uncertainty	O
-	O
related	O
objectives	O
and	O
features	O
as	O
well	O
as	O
training	O
on	O
out	O
-	O
ofdomain	O
direct	O
assessment	O
data	O
.	O

Quality	O
estimation	O
(	O
QE	O
)	O
is	O
the	O
task	O
of	O
evaluating	O
a	O
translation	O
system	O
's	O
quality	O
without	O
access	O
to	O
reference	O
translations	O
(	O
Blatz	O
et	O
al	O
,	O
2004	O
;	O
Specia	O
et	O
al	O
,	O
2018	O
)	O
.	O
This	O
paper	O
describes	O
the	O
joint	O
contribution	O
of	O
Instituto	O
Superior	O
Técnico	O
(	O
IST	O
)	O
and	O
Unbabel	O
to	O
the	O
WMT21	O
Quality	O
Estimation	O
shared	O
task	O
(	O
Specia	O
et	O
al	O
,	O
2021	O
)	O
,	O
where	O
systems	O
were	O
submitted	O
to	O
two	O
tasks	O
:	O
1	O
)	O
sentence	O
-	O
level	O
direct	O
assessment	O
;	O
2	O
)	O
word	O
-	O
and	O
sentence	O
-	O
level	O
post	O
-	O
editing	O
effort	O
.	O
This	O
year	O
's	O
submission	O
combines	O
several	O
ideas	O
built	O
on	O
top	O
of	O
the	O
OpenKiwi	O
framework	O
.	O
Motivated	O
by	O
the	O
mixture	O
of	O
blind	O
and	O
seen	O
language	O
pairs	O
in	O
the	O
test	O
sets	O
,	O
we	O
experimented	O
with	O
extensions	O
that	O
would	O
allow	O
us	O
to	O
train	O
multilingual	O
models	O
that	O
maintain	O
good	O
generalization	O
ability	O
and	O
are	O
robust	O
to	O
the	O
presence	O
of	O
epistemic	O
and	O
aleatoric	O
uncertainty	O
.	O
For	O
both	O
tasks	O
we	O
trained	O
and	O
submitted	O
an	O
ensemble	O
of	O
multilingual	O
models	O
.	O
All	O
submitted	O
models	O
follow	O
the	O
predictor	O
-	O
estimator	O
architecture	O
(	O
Kim	O
and	O
Lee	O
,	O
2016	O
;	O
Kim	O
et	O
al	O
,	O
2017	O
)	O
and	O
use	O
pretrained	O
models	O
for	O
feature	O
extraction	O
.	O
Also	O
,	O
we	O
fine	O
-	O
tune	O
all	O
models	O
on	O
the	O
provided	O
QE	O
data	O
using	O
stacked	O
adapter	O
layers	O
(	O
Pfeiffer	O
et	O
al	O
,	O
2020	O
)	O
.	O
*	O
The	O
first	O
three	O
authors	O
have	O
equal	O
contribution	O
.	O
We	O
show	O
that	O
we	O
can	O
thus	O
achieve	O
comparable	O
performance	O
across	O
language	O
pairs	O
while	O
minimising	O
the	O
number	O
of	O
trainable	O
parameters	O
(	O
see	O
Table	O
1	O
)	O
.	O
Furthermore	O
,	O
we	O
experimented	O
with	O
different	O
types	O
of	O
uncertainty	O
-	O
related	O
information	O
to	O
leverage	O
it	O
's	O
benefits	O
,	O
improving	O
performance	O
and	O
robustness	O
of	O
the	O
submitted	O
systems	O
(	O
see	O
3.1.1	O
)	O
.	O
All	O
related	O
code	O
extensions	O
will	O
be	O
publicly	O
available	O
.	O
Our	O
main	O
contributions	O
are	O
:	O
We	O
build	O
on	O
our	O
OpenKiwi	O
architecture	O
by	O
exploring	O
adapter	O
layers	O
(	O
Houlsby	O
et	O
al	O
,	O
2019	O
;	O
Pfeiffer	O
et	O
al	O
,	O
2020	O
)	O
for	O
quality	O
estimation	O
as	O
these	O
demonstrated	O
to	O
be	O
less	O
amenable	O
to	O
overfitting	O
while	O
presenting	O
the	O
same	O
or	O
superior	O
quality	O
performance	O
than	O
fine	O
-	O
tuning	O
the	O
whole	O
base	O
pre	O
-	O
trained	O
model	O
for	O
different	O
NLP	O
tasks	O
(	O
He	O
et	O
al	O
,	O
2021	O
)	O
.	O
We	O
incorporate	O
different	O
types	O
of	O
uncertainty	O
into	O
our	O
architectures	O
.	O
We	O
make	O
use	O
of	O
the	O
glass	O
-	O
box	O
features	O
(	O
Fomicheva	O
et	O
al	O
,	O
2020	O
)	O
extracted	O
from	O
the	O
NMT	O
models	O
,	O
the	O
aleatoric	O
(	O
data	O
)	O
uncertainty	O
derived	O
from	O
the	O
human	O
annotations	O
and	O
the	O
epistemic	O
(	O
model	O
)	O
uncertainty	O
(	O
Hora	O
,	O
1996	O
;	O
Kiureghian	O
and	O
Ditlevsen	O
,	O
2009	O
;	O
Huellermeier	O
and	O
Waegeman	O
,	O
2021	O
)	O
that	O
originates	O
from	O
the	O
QE	O
model	O
.	O
We	O
show	O
that	O
training	O
the	O
QE	O
models	O
on	O
additional	O
out	O
-	O
of	O
-	O
domain	O
direct	O
assessment	O
(	O
DA	O
)	O
data	O
gives	O
considerable	O
gains	O
in	O
performance	O
for	O
the	O
new	O
language	O
pairs	O
from	O
the	O
blind	O
test	O
sets	O
.	O

In	O
this	O
year	O
's	O
shared	O
task	O
edition	O
we	O
submitted	O
models	O
for	O
the	O
first	O
two	O
tasks	O
:	O
1	O
.	O
Task	O
1	O
:	O
sentence	O
-	O
level	O
direct	O
assessment	O
2	O
.	O
Task	O
2	O
:	O
word	O
-	O
and	O
sentence	O
level	O
post	O
-	O
editing	O
effort	O
,	O
comprising	O
of	O
two	O
subtasks	O
:	O
a	O
)	O
predicting	O
the	O
HTER	O
score	O
of	O
the	O
translated	O
sentence	O
(	O
hypothesis	O
)	O
;	O
and	O
b	O
)	O
predicting	O
OK	O
/	O
BAD	O
tags	O
for	O
the	O
words	O
and	O
gaps	O
(	O
both	O
in	O
source	O
and	O
translation	O
)	O
We	O
note	O
that	O
this	O
year	O
,	O
both	O
tasks	O
1	O
and	O
2	O
provided	O
additional	O
blind	O
test	O
sets	O
with	O
language	O
pairs	O
that	O
were	O
not	O
included	O
in	O
the	O
data	O
made	O
available	O
for	O
training	O
/	O
development	O
,	O
providing	O
an	O
interesting	O
challenge	O
and	O
motivating	O
multilingual	O
and	O
generalisable	O
approaches	O
.	O
3	O
Implemented	O
Systems	O

We	O
present	O
the	O
performance	O
of	O
the	O
implemented	O
models	O
on	O
the	O
test	O
-	O
20	O
dataset	O
.	O

We	O
presented	O
a	O
joint	O
contribution	O
of	O
IST	O
and	O
Unbabel	O
to	O
the	O
WMT	O
2021	O
QE	O
shared	O
task	O
.	O
Our	O
submissions	O
are	O
ensembles	O
of	O
multilingual	O
checkpoints	O
extending	O
the	O
OpenKiwi	O
framework	O
.	O
We	O
found	O
adapter	O
-	O
tuning	O
to	O
be	O
suitable	O
for	O
fine	O
-	O
tuning	O
OpenKiwi	O
on	O
the	O
QE	O
tasks	O
data	O
and	O
less	O
prone	O
to	O
overfitting	O
.	O
We	O
showed	O
that	O
pre	O
-	O
training	O
on	O
large	O
,	O
out	O
-	O
of	O
-	O
domain	O
annotated	O
data	O
can	O
prove	O
beneficial	O
both	O
for	O
the	O
direct	O
assessment	O
and	O
the	O
postediting	O
QE	O
tasks	O
.	O
We	O
also	O
demonstrated	O
that	O
handling	O
uncertainty	O
-	O
related	O
sources	O
of	O
information	O
improves	O
the	O
performance	O
when	O
integrated	O
into	O
the	O
QE	O
system	O
.	O
For	O
Task	O
2	O
we	O
do	O
multi	O
-	O
task	O
training	O
based	O
on	O
the	O
models	O
from	O
the	O
previous	O
task	O
and	O
use	O
multiple	O
checkpoints	O
to	O
create	O
the	O
submitted	O
ensemble	O
.	O

In	O

We	O
present	O
the	O
performance	O
of	O
the	O
submitted	O
ensembles	O
on	O
the	O
TEST	O
-	O
21	O
dataset	O
as	O
calculated	O
in	O
the	O
official	O
QE	O
results	O
6	O
for	O
each	O
task	O
and	O
sub	O
-	O
task	O
.	O
We	O
also	O
provide	O
the	O
comparison	O
with	O
the	O
organisers	O
'	O
baseline	O
.	O
The	O
results	O
for	O
Task1	O
on	O
TEST	O
-	O
21	O
are	O
presented	O
in	O
Table	O
7	O
.	O

The	O
results	O
for	O
Task2	O
on	O
TEST	O
-	O
21TEST	O
-	O
21	O
are	O
presented	O
in	O
Table	O
8	O
,	O
showing	O
the	O
performance	O
for	O
the	O
sentence	O
level	O
,	O
HTER	O
score	O
predictions	O
.	O

The	O
results	O
for	O
Task2	O
on	O
TEST	O
-	O
21	O
are	O
presented	O
in	O
Table	O
9	O
,	O
showing	O
the	O
performance	O
for	O
the	O
word	O
tag	O
predictions	O
.	O

We	O
present	O
below	O
(	O
Tables	O
10	O
and	O
11	O
)	O
the	O
statistics	O
on	O
the	O
Metrics	O
data	O
used	O
to	O
train	O
the	O
M1	O
M	O
model	O
on	O
direct	O
assessments	O
.	O

To	O
generate	O
abstractive	O
and	O
factual	O
summaries	O
from	O
unstructured	O
conversations	O
,	O
we	O
propose	O
to	O
model	O
structural	O
signals	O
in	O
conversations	O
by	O
first	O
constructing	O
discourse	O
relation	O
graphs	O
and	O
action	O
graphs	O
(	O
Section	O
3.1	O
)	O
,	O
and	O
then	O
encoding	O
the	O
graphs	O
together	O
with	O
conversations	O
(	O
Section	O
3.2	O
)	O
as	O
well	O
as	O
incorporating	O
these	O
different	O
levels	O
of	O
information	O
in	O
the	O
decoding	O
stage	O
through	O
a	O
multi	O
-	O
granularity	O
decoder	O
(	O
Section	O
3.3	O
)	O
to	O
summarize	O
given	O
conversations	O
.	O
The	O
overall	O
architecture	O
is	O
shown	O
in	O
Figure	O
2	O
.	O

Given	O
a	O
conversation	O
and	O
its	O
corresponding	O
discourse	O
relation	O
graph	O
and	O
action	O
graph	O
,	O
we	O
utilize	O
an	O
utterance	O
encoder	O
and	O
two	O
graph	O
encoders	O
,	O
to	O
obtain	O
its	O
hidden	O
representations	O
shown	O
in	O
Figure	O
2	O
(	O
a	O
)	O
.	O

To	O
fine	O
-	O
tune	O
on	O
sequence	O
labeling	O
tasks	O
,	O
a	O
dropout	O
layer	O
(	O
p	O
=	O
0.1	O
)	O
and	O
a	O
linear	O
(	O
token	O
-	O
level	O
)	O
classification	O
layer	O
is	O
built	O
upon	O
the	O
pre	O
-	O
trained	O
model	O
.	O

Table	O
4	O
to	O
Table	O
6	O
shows	O
the	O
model	O
performance	O
on	O
the	O
validation	O
set	O
.	O
The	O
data	O
usage	O
in	O
these	O
tables	O

We	O
implement	O
our	O
system	O
on	O
Ubuntu	O
18.04.3	O
LTS	O
system	O
.	O
We	O
run	O
our	O
experiments	O
on	O
an	O
Intel	O
(	O
R	O
)	O
Xeon	O
(	O
R	O
)	O
CPU	O
@	O
2.30GHz	O
and	O
NVIDIA	O
Tesla	O
P100	O
-	O
PCIe	O
with	O
16	O
GB	O
HBM2	O
memory	O
.	O
The	O
NVIDIA	O
-	O
SMI	O
version	O
is	O
418.67	O
and	O
the	O
CUDA	O
version	O
is	O
10.1	O
.	O

Semi	O
-	O
Supervised	O
Iterative	O
Approach	O
for	O
Domain	O
-	O
Specific	O
Complaint	O
Detection	O
in	O
Social	O
Media	O

In	O
this	O
paper	O
,	O
we	O
present	O
a	O
semi	O
-	O
supervised	O
bootstrapping	O
approach	O
to	O
detect	O
product	O
or	O
service	O
related	O
complaints	O
in	O
social	O
media	O
.	O
Our	O
approach	O
begins	O
with	O
a	O
small	O
collection	O
of	O
annotated	O
samples	O
which	O
are	O
used	O
to	O
identify	O
a	O
preliminary	O
set	O
of	O
linguistic	O
indicators	O
pertinent	O
to	O
complaints	O
.	O
These	O
indicators	O
are	O
then	O
used	O
to	O
expand	O
the	O
dataset	O
.	O
The	O
expanded	O
dataset	O
is	O
again	O
used	O
to	O
extract	O
more	O
indicators	O
.	O
This	O
process	O
is	O
applied	O
for	O
several	O
iterations	O
until	O
we	O
can	O
no	O
longer	O
find	O
any	O
new	O
indicators	O
.	O
We	O
evaluated	O
this	O
approach	O
on	O
a	O
Twitter	O
corpus	O
specifically	O
to	O
detect	O
complaints	O
about	O
transportation	O
services	O
.	O
We	O
started	O
with	O
an	O
annotated	O
set	O
of	O
326	O
samples	O
of	O
transportation	O
complaints	O
,	O
and	O
after	O
four	O
iterations	O
of	O
the	O
approach	O
,	O
we	O
collected	O
2	O
,	O
840	O
indicators	O
and	O
over	O
3	O
,	O
700	O
tweets	O
.	O
We	O
annotated	O
a	O
random	O
sample	O
of	O
700	O
tweets	O
from	O
the	O
final	O
dataset	O
and	O
observed	O
that	O
nearly	O
half	O
the	O
samples	O
were	O
actual	O
transportation	O
complaints	O
.	O
Lastly	O
,	O
we	O
also	O
studied	O
how	O
different	O
features	O
based	O
on	O
semantics	O
,	O
orthographic	O
properties	O
,	O
and	O
sentiment	O
contribute	O
towards	O
the	O
prediction	O
of	O
complaints	O
.	O

Our	O
proposed	O
approach	O
begins	O
with	O
a	O
large	O
corpus	O
of	O
transport	O
-	O
related	O
tweets	O
and	O
a	O
small	O
set	O
of	O
annotated	O
complaints	O
.	O
We	O
use	O
this	O
labeled	O
data	O
to	O
create	O
a	O
set	O
of	O
seed	O
indicators	O
that	O
drive	O
the	O
rest	O
of	O
our	O
iterative	O
complaint	O
detection	O
process	O
.	O

We	O
focused	O
our	O
experimentation	O
over	O
the	O
period	O
of	O
November	O
2018	O
to	O
December	O
2018	O
.	O
Our	O
first	O
step	O
towards	O
creating	O
a	O
corpus	O
of	O
transportrelated	O
tweets	O
is	O
to	O
identify	O
linguistic	O
markers	O
related	O
to	O
the	O
transport	O
domain	O
.	O
To	O
this	O
end	O
,	O
we	O
scraped	O
random	O
posts	O
from	O
transport	O
-	O
related	O
web	O
forums	O
2	O
.	O
These	O
forums	O
involve	O
users	O
discussing	O
their	O
grievances	O
and	O
raising	O
awareness	O
about	O
a	O
wide	O
array	O
of	O
transportation	O
-	O
related	O
issues	O
.	O
We	O
then	O
processed	O
this	O
data	O
to	O
extract	O
words	O
and	O
phrases	O
(	O
unigrams	O
,	O
bigrams	O
,	O
and	O
trigrams	O
)	O
with	O
high	O
tf	O
-	O
idf	O
scores	O
.	O
We	O
then	O
had	O
human	O
annotators	O
prune	O
them	O
further	O
to	O
remove	O
duplicates	O
and	O
irrelevant	O
items	O
.	O
This	O
resulted	O
in	O
a	O
lexicon	O
of	O
75	O
unique	O
phrases	O
.	O
Some	O
examples	O
include	O
cabs	O
,	O
discount	O
,	O
tickets	O
,	O
underground	O
,	O
luggage	O
,	O
transit	O
,	O
parking	O
,	O
neighborhood	O
,	O
downtown	O
,	O
traffic	O
,	O
Uber	O
.	O
We	O
used	O
Twitter	O
's	O
public	O
streaming	O
API	O
to	O
query	O
for	O
tweets	O
that	O
contained	O
any	O
of	O
the	O
75	O
phrases	O
over	O
the	O
chosen	O
time	O
range	O
.	O
We	O
then	O
excluded	O
non	O
-	O
English	O
tweets	O
and	O
any	O
tweets	O
with	O
less	O
than	O
two	O
tokens	O
.	O
This	O
resulted	O
in	O
a	O
collection	O
of	O
19	O
,	O
300	O
tweets	O
.	O
We	O
will	O
refer	O
to	O
this	O
collection	O
as	O
corpus	O
C.	O
We	O
chose	O
a	O
random	O
sample	O
of	O
1	O
,	O
500	O
tweets	O
from	O
this	O
collection	O
for	O
human	O
annotation	O
.	O
We	O
employed	O
two	O
human	O
annotators	O
to	O
identify	O
traffic	O
-	O
related	O
complaints	O
from	O
these	O
1	O
,	O
500	O
tweets	O
.	O
Following	O
are	O
some	O
high	O
-	O
level	O
details	O
of	O
the	O
annotation	O
task	O
.	O
We	O
instructed	O
the	O
annotators	O
to	O
identify	O
any	O
tweets	O
that	O
contain	O
first	O
-	O
hand	O
accounts	O
of	O
a	O
complaint	O
or	O
a	O
grievance	O
related	O
to	O
a	O
public	O
/	O
private	O
mode	O
of	O
transport	O
.	O
Following	O
is	O
a	O
sample	O
tweet	O
from	O
this	O
instruction	O
:	O
"	O
@	O
[	O
UserHandle	O
]	O
can	O
you	O
please	O
make	O
sure	O
that	O
compartment	O
A	O
-	O
6	O
is	O
at	O
least	O
clean	O
before	O
public	O
use	O
.	O
"	O
We	O
also	O
instructed	O
them	O
to	O
identify	O
tweets	O
that	O
provide	O
verifiable	O
sources	O
of	O
information	O
(	O
news	O
)	O
about	O
transport	O
-	O
related	O
services	O
.	O
Sample	O
tweet	O
:	O
"	O
4	O
hour	O
jam	O
in	O
[	O
place	O
]	O
area	O
due	O
to	O
rain	O
and	O
poor	O
management	O
of	O
traffic	O
police	O
.	O
"	O
.	O
Lastly	O
,	O
we	O
also	O
explicitly	O
asked	O
them	O
to	O
exclude	O
tweets	O
that	O
contain	O
announcements	O
or	O
advertisements	O
about	O
transportation	O
services	O
.	O
Sample	O
tweet	O
:	O
"	O
Please	O
use	O
[	O
name	O
]	O
cabs	O
,	O
you	O
will	O
get	O
60	O
%	O
discount	O
on	O
your	O
first	O
3	O
rides	O
.	O
"	O
The	O
two	O
annotators	O
worked	O
independently	O
,	O
and	O
when	O
we	O
finally	O
tallied	O
their	O
responses	O
,	O
we	O
observed	O
that	O
they	O
had	O
an	O
inter	O
-	O
annotator	O
agreement	O
rate	O
of	O
κ	O
=	O
0.81	O
(	O
Cohen	O
kappa	O
)	O
.	O
In	O
cases	O
where	O
the	O
annotators	O
disagreed	O
,	O
the	O
labels	O
were	O
resolved	O
through	O
a	O
discussion	O
.	O
After	O
the	O
disagreements	O
were	O
resolved	O
,	O
the	O
final	O
seed	O
dataset	O
had	O
326	O
samples	O
of	O
traffic	O
-	O
related	O
complaints	O
.	O
We	O
will	O
refer	O
to	O
this	O
as	O
T	O
s	O
.	O
Table	O
1	O
shows	O
some	O
examples	O
of	O
tweets	O
that	O
were	O
annotated	O
as	O
complaints	O
.	O

Our	O
proposed	O
iterative	O
approach	O
is	O
summarized	O
in	O
Algorithm	O
1	O
.	O
First	O
,	O
we	O
use	O
the	O
seed	O
data	O
T	O
s	O
to	O
build	O
a	O
set	O
of	O
linguistic	O
indicators	O
I	O
for	O
complaints	O
.	O
We	O
then	O
use	O
these	O
indicators	O
to	O
get	O
potential	O
new	O
complaints	O
T	O
l	O
from	O
the	O
corpus	O
C.	O
We	O
merge	O
T	O
s	O
and	O
T	O
l	O
to	O
build	O
our	O
new	O
dataset	O
.	O
We	O
then	O
use	O
this	O
new	O
dataset	O
to	O
extract	O
a	O
new	O
set	O
of	O
indicators	O
I	O
l	O
.	O
The	O
indicators	O
are	O
combined	O
with	O
the	O
original	O
indicators	O
I	O
to	O
extract	O
the	O
next	O
version	O
of	O
T	O
l	O
.	O
This	O
process	O
is	O
repeated	O
until	O
we	O
can	O
no	O
longer	O
find	O
any	O
new	O
indicators	O
.	O

As	O
shown	O
in	O
Algorithm	O
1	O
,	O
extracting	O
linguistic	O
indicators	O
(	O
n	O
-	O
grams	O
)	O
is	O
one	O
of	O
the	O
most	O
important	O
steps	O
in	O
the	O
process	O
.	O
These	O
indicators	O
are	O
critical	O
to	O
identifying	O
tweets	O
that	O
are	O
most	O
likely	O
domainspecific	O
complaints	O
.	O
We	O
employ	O
two	O
different	O
approaches	O
for	O
extracting	O
these	O
indicators	O
.	O
For	O
seed	O
data	O
,	O
T	O
s	O
,	O
which	O
is	O
annotated	O
,	O
we	O
just	O
select	O
n	O
-	O
grams	O
with	O
the	O
highest	O
tf	O
-	O
idf	O
scores	O
.	O
In	O
our	O
experimental	O
work	O
,	O
T	O
s	O
had	O
326	O
annotated	O
tweets	O
.	O
We	O
identified	O
50	O
n	O
-	O
grams	O
with	O
the	O
highest	O
tf	O
-	O
idf	O
scores	O
to	O
initialize	O
I.	O
Some	O
examples	O
include	O
:	O
problem	O
,	O
station	O
,	O
services	O
,	O
toll	O
-	O
fee	O
,	O
reply	O
,	O
fault	O
,	O
provide	O
information	O
,	O
driver	O
,	O
district	O
,	O
passenger	O
.	O
In	O
subsequent	O
iterations	O
,	O
when	O
we	O
are	O
handling	O
unannotated	O
samples	O
,	O
we	O
use	O
a	O
more	O
advanced	O
domain	O
relevance	O
criterion	O
for	O
extracting	O
the	O
indicators	O
.	O
When	O
extracting	O
indicators	O
from	O
T	O
l	O
,	O
which	O
is	O
not	O
annotated	O
,	O
it	O
is	O
possible	O
that	O
there	O
could	O
be	O
frequently	O
occurring	O
phrases	O
that	O
are	O
not	O
necessarily	O
indicative	O
of	O
complaints	O
.	O
These	O
phrases	O
could	O
lead	O
to	O
a	O
concept	O
drift	O
in	O
subsequent	O
iterations	O
.	O
To	O
avoid	O
these	O
digressions	O
,	O
we	O
use	O
a	O
measure	O
of	O
domain	O
relevance	O
when	O
selecting	O
indicators	O
.	O
This	O
is	O
defined	O
as	O
the	O
ratio	O
of	O
the	O
frequency	O
of	O
an	O
n	O
-	O
gram	O
in	O
T	O
l	O
to	O
that	O
of	O
in	O
T	O
r	O
.	O
T	O
r	O
is	O
a	O
collection	O
of	O
randomly	O
chosen	O
tweets	O
that	O
do	O
not	O
intersect	O
with	O
C.	O
In	O
our	O
experimental	O
work	O
,	O
we	O
defined	O
T	O
r	O
as	O
a	O
random	O
sample	O
of	O
5	O
,	O
000	O
tweets	O
from	O
a	O
different	O
time	O
range	O
than	O
that	O
of	O
C.	O
We	O
also	O
wanted	O
to	O
quantitatively	O
en	O
-	O
Samples	O
of	O
transport	O
-	O
related	O
complaints	O
.	O
1	O
.	O
No	O
metro	O
fares	O
will	O
be	O
reduced	O
,	O
but	O
proper	O
fare	O
structure	O
needs	O
to	O
be	O
introduced	O
....	O
right	O
?	O
.	O
2	O
.	O
It	O
takes	O
[	O
name	O
]	O
govt	O
.	O
longer	O
to	O
refund	O
charges	O
,	O
but	O
it	O
took	O
them	O
a	O
few	O
mins	O
to	O
remove	O
that	O
bus	O
stop	O
.	O
You	O
ca	O
n't	O
erase	O
the	O
problem	O
[	O
name	O
]	O
.	O
3	O
.	O
I	O
tried	O
to	O
lodge	O
a	O
complaint	O
on	O
[	O
url	O
]	O
but	O
see	O
the	O
results	O
.	O
Sir	O
if	O
8	O
A.C	O
's	O
are	O
not	O
working	O
in	O
this	O
coach	O
,	O
why	O
have	O
you	O
attached	O
that	O
coach	O
.	O

Given	O
a	O
set	O
of	O
indicators	O
I	O
,	O
the	O
process	O
of	O
selecting	O
tweets	O
from	O
corpus	O
C	O
is	O
fairly	O
straightforward	O
.	O
It	O
only	O
requires	O
to	O
identify	O
all	O
the	O
tweet	O
that	O
contains	O
any	O
of	O
the	O
indicators	O
.	O
The	O
only	O
caveat	O
here	O
is	O
to	O
reduce	O
the	O
redundancy	O
in	O
the	O
dataset	O
.	O
For	O
this	O
,	O
we	O
just	O
filtered	O
out	O
tweets	O
that	O
have	O
a	O
cosine	O
similarity	O
of	O
more	O
than	O
0.85	O
with	O
any	O
other	O
tweet	O
in	O
the	O
tf	O
-	O
idf	O
space	O
(	O
Albakour	O
et	O
al	O
,	O
2013	O
)	O
.	O
This	O
process	O
also	O
helped	O
remove	O
tweets	O
,	O
which	O
are	O
exact	O
matches	O
,	O
sub	O
-	O
strings	O
,	O
or	O
differing	O
by	O
some	O
punctuation	O
.	O
Removal	O
of	O
these	O
redundant	O
tweets	O
also	O
helps	O
in	O
diversifying	O
the	O
lexicon	O
for	O
subsequent	O
iterations	O
.	O

Our	O
iterative	O
approach	O
converged	O
in	O
four	O
rounds	O
,	O
after	O
which	O
it	O
did	O
not	O
extract	O
any	O
new	O
indicators	O
.	O
Figure	O
1	O
shows	O
the	O
counts	O
of	O
indicators	O
and	O
the	O
number	O
of	O
tweets	O
after	O
each	O
iteration	O
.	O
After	O
four	O
iterations	O
,	O
this	O
approach	O
chose	O
3	O
,	O
732	O
tweets	O
and	O
generated	O
2	O
,	O
840	O
unique	O
indicators	O
.	O
We	O
also	O
manually	O
inspected	O
the	O
indicators	O
chosen	O
during	O
the	O
process	O
.	O
We	O
observed	O
that	O
only	O
indicators	O
with	O
a	O
domain	O
relevance	O
score	O
of	O
≥	O
2.5	O
were	O
chosen	O
for	O
subsequent	O
iterations	O
.	O
Table	O
2	O
provides	O
a	O
few	O
examples	O
of	O
strong	O
and	O
weak	O
indicators	O
acquired	O
after	O
the	O
first	O
iteration	O
.	O
In	O
this	O
figure	O
,	O
strong	O
indicators	O
are	O
those	O
with	O
a	O
domain	O
relevance	O
score	O
≥	O
2.5	O
.	O
We	O
chose	O
a	O
random	O
set	O
of	O
700	O
tweets	O
from	O
the	O
final	O
complaints	O
dataset	O
T	O
and	O
annotated	O
them	O
manually	O
to	O
help	O
understand	O
the	O
quality	O
.	O
We	O
used	O
the	O
same	O
guidelines	O
as	O
discussed	O
in	O
section	O
3.1	O
and	O
also	O
employed	O
the	O
same	O
annotators	O
as	O
before	O
.	O
The	O
anno	O
-	O
tators	O
once	O
again	O
obtained	O
a	O
high	O
agreement	O
score	O
of	O
κ	O
=	O
0.83	O
.	O
After	O
resolving	O
the	O
disagreements	O
,	O
we	O
observed	O
that	O
332	O
tweets	O
were	O
labeled	O
as	O
complaints	O
.	O
This	O
accounts	O
for	O
47.4	O
%	O
of	O
the	O
sampled	O
700	O
tweets	O
.	O
This	O
demonstrates	O
that	O
nearly	O
half	O
the	O
tweets	O
selected	O
by	O
our	O
semi	O
-	O
supervised	O
approach	O
were	O
traffic	O
-	O
related	O
complaints	O
.	O
This	O
is	O
a	O
significantly	O
higher	O
proportion	O
in	O
the	O
original	O
seed	O
data	O
T	O
s	O
,	O
where	O
only	O
21.7	O
%	O
were	O
actual	O
complaints	O
.	O

We	O
conducted	O
a	O
series	O
of	O
experiments	O
to	O
understand	O
if	O
we	O
can	O
automatically	O
build	O
simple	O
machine	O
learning	O
models	O
to	O
detect	O
complaints	O
.	O
These	O
experiments	O
also	O
helped	O
us	O
evaluate	O
the	O
quality	O
of	O
the	O
final	O
dataset	O
.	O
Additionally	O
,	O
this	O
experimental	O
work	O
also	O
studies	O
how	O
different	O
types	O
of	O
linguistic	O
features	O
contribute	O
to	O
the	O
detection	O
of	O
social	O
media	O
complaints	O
.	O
For	O
these	O
experiments	O
,	O
we	O
used	O
the	O
annotated	O
sample	O
of	O
700	O
posts	O
as	O
a	O
test	O
dataset	O
.	O
We	O
built	O
our	O
training	O
dataset	O
by	O
selecting	O
another	O
2	O
,	O
000	O
posts	O
from	O
the	O
original	O
corpus	O
C	O
,	O
and	O
anno	O
-	O
tated	O
them	O
once	O
again	O
per	O
guidelines	O
discussed	O
in	O
section	O
3.1	O
.	O
In	O
this	O
sample	O
,	O
we	O
observed	O
that	O
the	O
annotators	O
had	O
similar	O
agreements	O
scores	O
of	O
κ	O
=	O
0.79	O
,	O
and	O
there	O
were	O
702	O
instances	O
of	O
complaints	O
.	O

We	O
experimented	O
with	O
four	O
different	O
semantic	O
features	O
:	O
Unigrams	O
:	O
Each	O
tweet	O
(	O
Wallach	O
,	O
2006	O
)	O
is	O
represented	O
as	O
sparse	O
vector	O
of	O
tf	O
-	O
idf	O
values	O
correspond	O
-	O
ing	O
to	O
the	O
constituent	O
tokens	O
.	O
Word2Vec	O
Clusters	O
:	O
We	O
follow	O
the	O
same	O
approach	O
as	O
in	O
(	O
Preoţiuc	O
-	O
Pietro	O
et	O
al	O
,	O
2015	O
)	O
,	O
where	O
words	O
are	O
clustered	O
using	O
pair	O
-	O
wise	O
similarities	O
in	O
Word2Vec	O
space	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
.	O
Each	O
tweet	O
is	O
then	O
represented	O
as	O
a	O
distribution	O
over	O
these	O
clusters	O
;	O
the	O
values	O
are	O
proportional	O
to	O
the	O
number	O
of	O
tokens	O
belonging	O
to	O
a	O
cluster	O
.	O
These	O
clusters	O
have	O
previously	O
been	O
demonstrated	O
to	O
have	O
great	O
interpretability	O
(	O
Preoţiuc	O
-	O
Pietro	O
et	O
al	O
,	O
2015Zou	O
et	O
al	O
,	O
2016	O
)	O
.	O
POS	O
Tags	O
:	O
We	O
used	O
the	O
Stanford	O
POS	O
Tagger	O
(	O
Manning	O
et	O
al	O
,	O
2014	O
)	O
to	O
represent	O
tweets	O
as	O
a	O
dense	O
frequency	O
vector	O
over	O
five	O
main	O
POS	O
tags	O
:	O
nouns	O
,	O
adjectives	O
,	O
adverbs	O
,	O
verbs	O
,	O
pronouns	O
.	O
Pronoun	O
Types	O
:	O
Pronouns	O
are	O
often	O
used	O
in	O
complaints	O
and	O
suggestions	O
to	O
reveal	O
personal	O
involvement	O
or	O
to	O
add	O
intensity	O
to	O
an	O
opinion	O
(	O
Claridge	O
,	O
2007	O
;	O
Meinl	O
,	O
2013	O
)	O
.	O
We	O
identify	O
various	O
pronoun	O
types	O
(	O
first	O
person	O
,	O
second	O
person	O
,	O
third	O
person	O
,	O
demonstrative	O
,	O
indefinite	O
)	O
using	O
dictionaries	O
and	O
use	O
their	O
counts	O
as	O
features	O
.	O

Our	O
first	O
set	O
of	O
orthographic	O
feature	O
uses	O
counts	O
of	O
URLs	O
,	O
hashtags	O
,	O
user	O
mentions	O
,	O
and	O
special	O
symbols	O
used	O
in	O
the	O
post	O
.	O
The	O
second	O
set	O
of	O
orthographic	O
features	O
try	O
to	O
identify	O
potential	O
intensifiers	O
such	O
as	O
capitalization	O
and	O
repeated	O
use	O
of	O
exclamation	O
or	O
question	O
marks	O
.	O
These	O
types	O
of	O
intensifiers	O
are	O
often	O
used	O
to	O
express	O
anger	O
or	O
strong	O
opinions	O
(	O
Meinl	O
,	O
2013	O
)	O
.	O

A	O
request	O
is	O
a	O
speech	O
act	O
very	O
closely	O
related	O
to	O
complaints	O
.	O
Often	O
,	O
the	O
main	O
motivation	O
behind	O
a	O
complaint	O
on	O
a	O
social	O
media	O
platform	O
is	O
to	O
get	O
a	O
correction	O
or	O
reparation	O
from	O
the	O
service	O
providers	O
(	O
Blum	O
-	O
Kulka	O
and	O
Olshtain	O
,	O
1984	O
)	O
.	O
We	O
use	O
the	O
model	O
presented	O
in	O
(	O
Danescu	O
-	O
Niculescu	O
-	O
Mizil	O
et	O
al	O
,	O
2013	O
)	O
to	O
detect	O
if	O
a	O
given	O
tweet	O
is	O
a	O
request	O
.	O
Requests	O
might	O
also	O
often	O
include	O
polite	O
phrases	O
in	O
expectation	O
of	O
better	O
service	O
.	O
They	O
are	O
coded	O
using	O
various	O
dictionaries	O
e.g	O
,	O
downgraders	O
(	O
little	O
)	O
,	O
down	O
-	O
toners	O
(	O
just	O
)	O
,	O
hedges	O
(	O
somewhat	O
)	O
.	O
Apology	O
markers	O
have	O
the	O
same	O
effect	O
as	O
politeness	O
markers	O
,	O
they	O
may	O
include	O
greetings	O
at	O
the	O
start	O
(	O
Good	O
Morning	O
)	O
,	O
direct	O
start	O
(	O
e.g	O
so	O
)	O
,	O
subjunctive	O
phrases	O
(	O
could	O
you	O
)	O
(	O
Švárová	O
,	O
2008	O
)	O
.	O
We	O
utilize	O
pre	O
-	O
defined	O
dictionaries	O
to	O
determine	O
the	O
presence	O
of	O
politeness	O
identifiers	O
along	O
with	O
the	O
politeness	O
score	O
of	O
the	O
tweet	O
based	O
on	O
the	O
model	O
in	O
(	O
Danescu	O
-	O
Niculescu	O
-	O
Mizil	O
et	O
al	O
,	O
2013	O
)	O
.	O

In	O
this	O
paper	O
,	O
we	O
presented	O
a	O
semi	O
-	O
supervised	O
iterative	O
approach	O
for	O
the	O
detection	O
of	O
complaints	O
in	O
social	O
media	O
platforms	O
.	O
The	O
process	O
begins	O
with	O
a	O
small	O
sample	O
of	O
annotated	O
examples	O
,	O
and	O
then	O
iteratively	O
builds	O
more	O
linguistic	O
identifiers	O
to	O
expand	O
the	O
dataset	O
.	O
We	O
evaluated	O
this	O
approach	O
on	O
the	O
domain	O
of	O
transportation	O
on	O
Twitter	O
,	O
starting	O
with	O
a	O
sample	O
of	O
326	O
annotated	O
tweets	O
.	O
After	O
four	O
iterations	O
,	O
we	O
were	O
able	O
to	O
construct	O
a	O
corpus	O
with	O
over	O
3	O
,	O
700	O
tweets	O
.	O
Annotation	O
of	O
random	O
samples	O
established	O
that	O
nearly	O
half	O
the	O
tweets	O
were	O
actual	O
complaints	O
.	O
We	O
evaluated	O
the	O
predictive	O
power	O
based	O
on	O
semantic	O
,	O
orthographic	O
,	O
and	O
sentiment	O
features	O
.	O
We	O
observed	O
that	O
complaint	O
is	O
a	O
complex	O
speech	O
act	O
,	O
which	O
is	O
related	O
to	O
many	O
other	O
linguistic	O
properties	O
.	O
Automatic	O
detection	O
of	O
complaints	O
is	O
not	O
only	O
useful	O
to	O
service	O
providers	O
as	O
feedback	O
;	O
it	O
could	O
also	O
prove	O
helpful	O
in	O
improving	O
service	O
providers	O
'	O
operations	O
and	O
in	O
downstream	O
applications	O
such	O
as	O
developing	O
chat	O
-	O
bots	O
.	O
Additionally	O
,	O
it	O
could	O
also	O
be	O
of	O
interest	O
to	O
linguists	O
in	O
understanding	O
how	O
humans	O
express	O
grievances	O
and	O
criticism	O
.	O
This	O
proposed	O
methodology	O
could	O
be	O
applied	O
to	O
many	O
other	O
products	O
or	O
services	O
to	O
detect	O
complaints	O
.	O
This	O
would	O
only	O
additionally	O
require	O
some	O
lexicons	O
and	O
a	O
small	O
annotated	O
dataset	O
.	O
We	O
also	O
expect	O
it	O
would	O
be	O
fairly	O
straightforward	O
to	O
adapt	O
this	O
technique	O
to	O
many	O
other	O
types	O
of	O
speech	O
acts	O
.	O
Further	O
investigation	O
is	O
necessary	O
to	O
understand	O
how	O
this	O
method	O
compares	O
against	O
supervised	O
or	O
completely	O
unsupervised	O
techniques	O
.	O

nlpUP	O
at	O
SemEval	O
-	O
2019	O
Task	O
6	O
:	O
A	O
Deep	O
Neural	O
Language	O
Model	O
for	O
Offensive	O
Language	O
Detection	O

Tweets	O
are	O
first	O
tokenized	O
and	O
converted	O
to	O
lowercase	O
.	O
We	O
constrain	O
repeated	O
character	O
sequences	O
to	O
length	O
3	O
and	O
replace	O
all	O
longer	O
character	O
sequences	O
.	O
HTML	O
character	O
encodings	O
are	O
replaced	O
by	O
their	O
corresponding	O
literal	O
or	O
token	O
representation	O
(	O
e.g.	O
'	O
&	O
amp	O
;	O
'	O
translates	O
to	O
'	O
and	O
'	O
)	O
.	O
Tokens	O
are	O
further	O
split	O
if	O
they	O
enclose	O
a	O
set	O
of	O
special	O
characters	O
(	O
'	O
\	O
'	O
,	O
'	O
/	O
'	O
,	O
'	O
&	O
'	O
,	O
'	O
-	O
'	O
)	O
.	O
Since	O
hashtags	O
are	O
often	O
used	O
to	O
replace	O
contextually	O
important	O
words	O
mid	O
-	O
sentence	O
,	O
we	O
split	O
hashtags	O
in	O
the	O
actual	O
hashsymbol	O
and	O
the	O
following	O
string	O
to	O
keep	O
the	O
semantic	O
information	O
of	O
a	O
hashtag	O
(	O
e.g.	O
'	O
Brainless	O
#	O
Liberal	O
Stooge	O
Ocasio	O
-	O
Cortez	O
'	O
)	O
.	O

Quantifying	O
the	O
Effects	O
of	O
COVID	O
-	O
19	O
on	O
Mental	O
Health	O
Support	O
Forums	O

There	O
is	O
a	O
considerable	O
body	O
of	O
research	O
that	O
examines	O
the	O
relationship	O
between	O
language	O
use	O
and	O
mental	O
health	O
,	O
including	O
work	O
dating	O
back	O
several	O
decades	O
.	O
For	O
example	O
,	O
Bucci	O
and	O
Freedman	O
(	O
1981	O
)	O
and	O
Weintraub	O
(	O
1981	O
)	O
observed	O
an	O
increased	O
usage	O
of	O
first	O
person	O
singular	O
pronouns	O
in	O
individuals	O
with	O
depression	O
.	O
Oxman	O
et	O
al	O
(	O
1982	O
)	O
showed	O
that	O
they	O
could	O
distinguish	O
between	O
paranoia	O
and	O
depression	O
by	O
applying	O
linguistic	O
analysis	O
to	O
speech	O
.	O
Since	O
then	O
,	O
advances	O
in	O
tools	O
for	O
text	O
analytics	O
have	O
led	O
to	O
increased	O
research	O
in	O
this	O
area	O
.	O
Notably	O
,	O
the	O
Linguistic	O
Inquiry	O
and	O
Word	O
Count	O
(	O
LIWC	O
)	O
)	O
is	O
a	O
widely	O
used	O
computerized	O
text	O
analysis	O
tool	O
that	O
has	O
been	O
validated	O
for	O
psycholinguistic	O
analysis	O
.	O
Some	O
of	O
the	O
earliest	O
studies	O
using	O
LIWC	O
analyzed	O
written	O
text	O
.	O
For	O
instance	O
,	O
researchers	O
have	O
used	O
LIWC	O
to	O
study	O
linguistic	O
patterns	O
in	O
essays	O
written	O
by	O
college	O
students	O
with	O
and	O
without	O
depression	O
(	O
Rude	O
et	O
al	O
,	O
2004	O
)	O
or	O
in	O
poems	O
written	O
by	O
suicidal	O
vs	O
non	O
-	O
suicidal	O
poets	O
(	O
Stirman	O
and	O
Pennebaker	O
,	O
2001	O
)	O
.	O
More	O
recently	O
,	O
there	O
has	O
been	O
a	O
proliferation	O
of	O
studies	O
applying	O
LIWC	O
to	O
online	O
text	O
,	O
including	O
social	O
media	O
data	O
.	O
LIWC	O
has	O
been	O
used	O
to	O
study	O
language	O
patterns	O
on	O
social	O
media	O
for	O
a	O
variety	O
of	O
mental	O
health	O
disorders	O
,	O
including	O
depression	O
,	O
anxiety	O
,	O
suicidality	O
,	O
and	O
bipolar	O
disorder	O
(	O
De	O
Choudhury	O
et	O
al	O
,	O
2013	O
;	O
Shen	O
and	O
Rudzicz	O
,	O
2017	O
;	O
Coppersmith	O
et	O
al	O
,	O
2014	O
.	O
In	O
addition	O
to	O
LIWC	O
,	O
other	O
methods	O
used	O
to	O
study	O
the	O
linguistic	O
patterns	O
of	O
mental	O
illness	O
include	O
character	O
and	O
word	O
models	O
(	O
Coppersmith	O
et	O
al	O
,	O
2014	O
;	O
Tsugawa	O
et	O
al	O
,	O
2013	O
)	O
and	O
topic	O
modeling	O
(	O
Resnik	O
et	O
al	O
,	O
2015	O
;	O
Preoţiuc	O
-	O
Pietro	O
et	O
al	O
,	O
2015	O
)	O
.	O

We	O
report	O
the	O
daily	O
number	O
of	O
unique	O
users	O
who	O
posted	O
in	O
each	O
subreddit	O
in	O
Figure	O
2	O
.	O
We	O
observe	O
an	O
increase	O
in	O
the	O
number	O
of	O
users	O
who	O
posted	O
in	O
the	O
r	O
/	O
Anxiety	O
subreddit	O
in	O
the	O
post	O
-	O
COVID	O
period	O
.	O
Meanwhile	O
,	O
in	O
both	O
r	O
/	O
depression	O
and	O
r	O
/	O
SuicideWatch	O
,	O
we	O
find	O
significant	O
decreases	O
in	O
the	O
number	O
of	O
users	O
who	O
posted	O
.	O
In	O
r	O
/	O
depression	O
,	O
we	O
observe	O
a	O
substantial	O
drop	O
in	O
posting	O
rates	O
around	O
mid	O
-	O
March	O
.	O
Activity	O
in	O
this	O
subreddit	O
remains	O
abnormally	O
low	O
into	O
late	O
-	O
April	O
,	O
when	O
it	O
starts	O
to	O
revert	O
back	O
towards	O
the	O
forecasted	O
values	O
.	O
In	O
r	O
/	O
SuicideWatch	O
,	O
the	O
drop	O
in	O
user	O
activity	O
is	O
less	O
extreme	O
,	O
and	O
we	O
see	O
that	O
the	O
activity	O
levels	O
eventually	O
return	O
to	O
their	O
predicted	O
values	O
.	O

The	O
increase	O
in	O
users	O
posting	O
on	O
r	O
/	O
Anxiety	O
is	O
consistent	O
with	O
prior	O
work	O
that	O
has	O
found	O
that	O
epidemics	O
often	O
lead	O
to	O
increased	O
rates	O
of	O
anxiety	O
(	O
Torales	O
et	O
al	O
,	O
2020	O
)	O
.	O
One	O
explanation	O
for	O
the	O
reduction	O
of	O
activity	O
within	O
r	O
/	O
depression	O
could	O
be	O
that	O
fewer	O
users	O
are	O
depressed	O
and	O
do	O
n't	O
feel	O
the	O
need	O
to	O
post	O
on	O
the	O
support	O
forum	O
.	O
If	O
this	O
is	O
the	O
case	O
,	O
our	O
findings	O
contrast	O
with	O
prior	O
work	O
that	O
found	O
that	O
depressive	O
symptoms	O
are	O
commonly	O
observed	O
during	O
pandemics	O
(	O
Chew	O
et	O
al	O
,	O
2020	O
)	O
.	O
However	O
,	O
there	O
are	O
multiple	O
possible	O
alternatives	O
;	O
for	O
example	O
,	O
depression	O
can	O
also	O
cause	O
people	O
to	O
socially	O
withdraw	O
(	O
Mayo	O
Clinic	O
,	O
2018	O
)	O
,	O
so	O
an	O
increase	O
in	O
depression	O
rates	O
could	O
lead	O
to	O
a	O
reduction	O
in	O
posting	O
activity	O
.	O
Another	O
finding	O
from	O
prior	O
work	O
is	O
that	O
delayed	O
depression	O
is	O
common	O
following	O
disaster	O
events	O
(	O
Pennebaker	O
and	O
Harber	O
,	O
1993	O
;	O
Nandi	O
et	O
al	O
,	O
2009	O
)	O
.	O
Our	O
analysis	O
covers	O
only	O
the	O
beginning	O
of	O
the	O
pandemic	O
,	O
so	O
it	O
likely	O
would	O
n't	O
capture	O
this	O
phenomenon	O
.	O
Additional	O
analysis	O
focused	O
on	O
the	O
causes	O
driving	O
the	O
reduction	O
in	O
activity	O
and	O
how	O
this	O
pattern	O
changes	O
in	O
the	O
long	O
-	O
term	O
is	O
needed	O
to	O
make	O
a	O
more	O
conclusive	O
statement	O
about	O
the	O
effects	O
of	O
COVID	O
-	O
19	O
on	O
depression	O
.	O

In	O
Table	O
2	O
,	O
we	O
show	O
the	O
ten	O
LIWC	O
categories	O
with	O
the	O
most	O
outliers	O
(	O
outside	O
of	O
the	O
95	O
%	O
prediction	O
interval	O
)	O
from	O
March	O
to	O
May	O
of	O
2020	O
.	O
We	O
observe	O
a	O
lack	O
of	O
consistency	O
between	O
the	O
subreddits	O
,	O
both	O
in	O
the	O
number	O
and	O
direction	O
of	O
the	O
outliers	O
.	O
We	O
see	O
decreases	O
in	O
r	O
/	O
Anxiety	O
and	O
r	O
/	O
depression	O
in	O
the	O
MOTION	O
category	O
.	O
Categories	O
such	O
as	O
BIO	O
and	O
BODY	O
tend	O
to	O
increase	O
in	O
r	O
/	O
Anxiety	O
;	O
however	O
,	O
this	O
pattern	O
is	O
not	O
present	O
in	O
other	O
subreddits	O
.	O
We	O
see	O
consistent	O
changes	O
in	O
time	O
orientation	O
(	O
e.g.	O
,	O
FOCUSPAST	O
,	O
FOCUSFUTURE	O
)	O
across	O
subreddits	O
;	O
a	O
higher	O
focus	O
on	O
the	O
past	O
in	O
r	O
/	O
depression	O
,	O
and	O
a	O
lower	O
focus	O
on	O
the	O
future	O
in	O
r	O
/	O
SuicideWatch	O
.	O
While	O
it	O
is	O
not	O
among	O
the	O
categories	O
with	O
the	O
most	O
outliers	O
,	O
there	O
is	O
a	O
statistically	O
significant	O
drop	O
in	O
FO	O
-	O
CUSFUTURE	O
on	O
r	O
/	O
Anxiety	O
and	O
r	O
/	O
depression	O
.	O
We	O
also	O
see	O
changes	O
in	O
pronoun	O
usage	O
;	O
the	O
most	O
notable	O
and	O
consistent	O
change	O
across	O
the	O
subreddits	O
is	O
that	O
the	O
usage	O
of	O
WE	O
increases	O
significantly	O
,	O
especially	O
in	O
the	O
early	O
period	O
of	O
COVID	O
-	O
19	O
(	O
Figure	O
3a	O
)	O
.	O
While	O
there	O
is	O
a	O
significant	O
decrease	O
in	O
I	O
words	O
in	O
r	O
/	O
Anxiety	O
,	O
there	O
is	O
in	O
fact	O
an	O
increase	O
in	O
r	O
/	O
depression	O
(	O
Figure	O
3b	O
)	O
.	O
Finally	O
,	O
we	O
see	O
a	O
notable	O
drop	O
in	O
the	O
WORK	O
category	O
(	O
Figure	O
3c	O
)	O
.	O

Finding	O
Good	O
Conversations	O
Online	O
:	O
The	O
Yahoo	O
News	O
Annotated	O
Comments	O
Corpus	O

This	O
work	O
presents	O
a	O
dataset	O
and	O
annotation	O
scheme	O
for	O
the	O
new	O
task	O
of	O
identifying	O
"	O
good	O
"	O
conversations	O
that	O
occur	O
online	O
,	O
which	O
we	O
call	O
ERICs	O
:	O
Engaging	O
,	O
Respectful	O
,	O
and/or	O
Informative	O
Conversations	O
.	O
We	O
develop	O
a	O
taxonomy	O
to	O
reflect	O
features	O
of	O
entire	O
threads	O
and	O
individual	O
comments	O
which	O
we	O
believe	O
contribute	O
to	O
identifying	O
ERICs	O
;	O
code	O
a	O
novel	O
dataset	O
of	O
Yahoo	O
News	O
comment	O
threads	O
(	O
2.4k	O
threads	O
and	O
10k	O
comments	O
)	O
and	O
1k	O
threads	O
from	O
the	O
Internet	O
Argument	O
Corpus	O
;	O
and	O
analyze	O
the	O
features	O
characteristic	O
of	O
ERICs	O
.	O
This	O
is	O
one	O
of	O
the	O
largest	O
annotated	O
corpora	O
of	O
online	O
human	O
dialogues	O
,	O
with	O
the	O
most	O
detailed	O
set	O
of	O
annotations	O
.	O
It	O
will	O
be	O
valuable	O
for	O
identifying	O
ERICs	O
and	O
other	O
aspects	O
of	O
argumentation	O
,	O
dialogue	O
,	O
and	O
discourse	O
.	O

Automatically	O
curating	O
online	O
comments	O
has	O
been	O
a	O
large	O
focus	O
in	O
recent	O
NLP	O
and	O
social	O
media	O
work	O
,	O
as	O
popular	O
news	O
outlets	O
can	O
receive	O
millions	O
of	O
comments	O
on	O
their	O
articles	O
each	O
month	O
(	O
Warzel	O
,	O
2012	O
)	O
.	O
Comment	O
threads	O
often	O
range	O
from	O
vacuous	O
to	O
hateful	O
,	O
but	O
good	O
discussions	O
do	O
occur	O
online	O
,	O
with	O
people	O
expressing	O
different	O
viewpoints	O
and	O
attempting	O
to	O
inform	O
,	O
convince	O
,	O
or	O
better	O
understand	O
the	O
other	O
side	O
,	O
but	O
they	O
can	O
get	O
lost	O
among	O
the	O
multitude	O
of	O
unconstructive	O
comments	O
.	O
We	O
hypothesize	O
that	O
identifying	O
and	O
promoting	O
these	O
types	O
of	O
conversations	O
(	O
ERICs	O
)	O
will	O
cultivate	O
a	O
more	O
civil	O
and	O
constructive	O
atmosphere	O
in	O
online	O
communities	O
and	O
potentially	O
encourage	O
participation	O
from	O
more	O
users	O
.	O
ERICs	O
are	O
characterized	O
by	O
:	O
A	O
respectful	O
exchange	O
of	O
ideas	O
,	O
opinions	O
,	O
and/or	O
information	O
in	O
response	O
to	O
a	O
given	O
topic	O
(	O
s	O
)	O
.	O
Opinions	O
expressed	O
as	O
an	O
attempt	O
to	O
elicit	O
a	O
dialogue	O
or	O
persuade	O
.	O
Comments	O
that	O
seek	O
to	O
contribute	O
some	O
new	O
information	O
or	O
perspective	O
on	O
the	O
relevant	O
topic	O
.	O
ERICs	O
have	O
no	O
single	O
identifying	O
attribute	O
:	O
for	O
instance	O
,	O
an	O
exchange	O
where	O
communicants	O
are	O
in	O
total	O
agreement	O
throughout	O
can	O
be	O
an	O
ERIC	O
,	O
as	O
can	O
an	O
exchange	O
with	O
heated	O
disagreement	O
.	O
Figures	O
1	O
and	O
2	O
contain	O
two	O
threads	O
that	O
are	O
characterized	O
by	O
continual	O
disagreement	O
,	O
but	O
one	O
is	O
an	O
ERIC	O
and	O
the	O
other	O
is	O
not	O
.	O
We	O
have	O
developed	O
a	O
new	O
coding	O
scheme	O
to	O
label	O
ERICs	O
and	O
identify	O
six	O
dimensions	O
of	O
comments	O
and	O
three	O
dimensions	O
of	O
threads	O
that	O
are	O
frequently	O
seen	O
in	O
the	O
comments	O
section	O
.	O
Many	O
of	O
these	O
labels	O
are	O
for	O
characteristics	O
of	O
online	O
conversations	O
not	O
captured	O
by	O
traditional	O
argumentation	O
or	O
dialogue	O
features	O
.	O
Some	O
of	O
the	O
labels	O
we	O
collect	O
have	O
been	O
annotated	O
in	O
previous	O
work	O
(	O
2	O
)	O
,	O
but	O
this	O
is	O
the	O
first	O
time	O
they	O
are	O
aggregated	O
in	O
a	O
single	O
corpus	O
at	O
the	O
dialogue	O
level	O
.	O
In	O
this	O
paper	O
,	O
we	O
present	O
the	O
Yahoo	O
News	O
Annotated	O
Comments	O
Corpus	O
(	O
YNACC	O
)	O
,	O
which	O
contains	O
2.4k	O
threads	O
and	O
10k	O
comments	O
from	O
the	O
comments	O
sections	O
of	O
Yahoo	O
News	O
articles	O
.	O
We	O
additionally	O
collect	O
annotations	O
on	O
1k	O
threads	O
from	O
the	O
Internet	O
Argument	O
Corpus	O
(	O
Abbott	O
et	O
al	O
,	O
2016	O
)	O
,	O
representing	O
another	O
domain	O
of	O
online	O
debates	O
.	O
We	O
contrast	O
annotations	O
of	O
Yahoo	O
and	O
IAC	O
threads	O
,	O
explore	O
ways	O
in	O
which	O
threads	O
perceived	O
to	O
be	O
ERICs	O
differ	O
in	O
this	O
two	O
venues	O
,	O
and	O
identify	O
some	O
unanticipated	O
characteristics	O
of	O
ERICs	O
.	O
This	O
is	O
the	O
first	O
exploration	O
of	O
how	O
characteristics	O
of	O
individual	O
comments	O
contribute	O
to	O
the	O
dialogue	O
-	O
level	O
classification	O
of	O
an	O
exchange	O
.	O
YNACC	O
will	O
facilitate	O
research	O
to	O
understand	O
ERICS	O
and	O
other	O
aspects	O
of	O
dialogue	O
.	O
The	O
corpus	O
and	O
annotations	O
will	O
be	O
available	O
at	O
https	O
:	O
//github.com	O
/	O
cnap	O
/	O
ynacc	O
.	O
.	O
Controversial	O
Controversial	O
Sarcastic	O
Sarcastic	O
$	O
'	O
☹	O
%	O
'	O
☹	O
"	O
%	O
&	O
)	O
✋	O
%	O
&	O
)	O
✋	O
%	O
'	O
☹	O
☹	O
"	O
%	O
&	O
☹	O
☹	O
"	O
when	O
a	O
country	O
has	O
to	O
use	O
force	O
to	O
keep	O
it	O
's	O
businesses	O
behind	O
a	O
wall	O
.	O
.	O
.	O
something	O
is	O
very	O
wrong	O
.	O
will	O
the	O
next	O
step	O
be	O
forcing	O
the	O
talented	O
and	O
wealthy	O
to	O
remain	O
?	O
this	O
strategy	O
did	O
not	O
work	O
well	O
for	O
the	O
soviet	O
union	O
.	O
A	O
your	O
solution	O
is	O
?	O
B	O
@B	O
,	O
lower	O
the	O
govt	O
imposed	O
costs	O
and	O
businesses	O
will	O
stay	O
voluntarily	O
.	O

just	O
because	O
a	O
company	O
was	O
started	O
in	O
us	O
,	O
given	O
large	O
tax	O
breakes	O
in	O
the	O
us	O
and	O
makes	O
most	O
of	O
its	O
profits	O
in	O
the	O
us	O
does	O
not	O
mean	O
it	O
owes	O
loyalty	O
right	O
?	O
they	O
have	O
to	O
appease	O
the	O
shareholders	O
who	O
want	O
more	O
value	O
so	O
lower	O
your	O
cost	O
of	O
business	O
by	O
lowering	O
taxes	O
while	O
still	O
getting	O
all	O
the	O
perks	O
is	O
one	O
way	O
of	O
doing	O
it	O
.	O

@D	O
-	O
in	O
your	O
world	O
who	O
eventually	O
pays	O
the	O
taxes	O
that	O
our	O
gov't	O
charges	O
business	O
?	O

@C	O
lowering	O
corporate	O
taxes	O
does	O
not	O
equate	O
to	O
more	O
jobs	O
,	O
its	O
only	O
equates	O
to	O
corporations	O
making	O
more	O
money	O
.	O
did	O
you	O
think	O
they	O
take	O
their	O
profits	O
and	O
make	O
high	O
paying	O
jobs	O
with	O
them	O
?	O
lol	O
wake	O
up	O
!	O

Headline	O
:	O
Allergan	O
CEO	O
:	O
Feds	O
blindsided	O
us	O
on	O
Pfizer	O
deal	O
Figure	O
1	O
:	O
An	O
ERIC	O
that	O
is	O
labeled	O
argumentative	O
,	O
positive	O
/	O
respectful	O
,	O
and	O
having	O
continual	O
disagreement	O
.	O
Controversial	O
Mean	O
!	O
"	O
☹	O
Controversial	O
$	O
%	O
&	O
'	O
$	O
"	O
☹	O
'	O
Controversial	O
Informative	O
$	O
%	O
(	O
'	O
Informative	O
$	O
%	O
(	O
'	O
Controversial	O
$	O
%	O
☹	O
'	O
$	O
"	O
(	O
)	O
'	O
$	O
"	O
(	O
)	O
'	O
$	O
"	O
(	O
)	O
'	O
$	O
"	O
(	O
'	O
)	O
quit	O
your	O
whining	O
you	O
are	O
in	O
america	O
assimilate	O
into	O
american	O
society	O
.	O
or	O
go	O
back	O
where	O
you	O
came	O
from	O
.	O

american	O
society	O
is	O
that	O
of	O
immigrants	O
and	O
the	O
freedom	O
to	O
practice	O
whatever	O
religion	O
you	O
wish	O
.	O
you	O
anti	O
american	O
?	O
F	O
@F	O
,	O
you	O
may	O
be	O
an	O
immigrant	O
,	O
but	O
i	O
'm	O
not	O
G	O
the	O
only	O
reason	O
you	O
are	O
an	O
american	O
is	O
because	O
of	O
immigrants	O
.	O

that	O
can	O
be	O
said	O
of	O
all	O
humans	O
.	O
humans	O
migrated	O
from	O
africa	O
.	O
everyone	O
in	O
germany	O
is	O
an	O
immigrant	O
.	O

then	O
any	O
statement	O
about	O
they	O
need	O
to	O
"	O
go	O
back	O
"	O
is	O
irrelevant	O
and	O
wrong	O
.	O
thanks	O
for	O
proving	O
my	O
point	O
.	O

floridians	O
tell	O
new	O
yorkers	O
to	O
go	O
back	O
.	O
you	O
have	O
no	O
point	O
.	O

just	O
because	O
someone	O
says	O
something	O
does	O
nt	O
make	O
it	O
valid	O
.	O
your	O
point	O
has	O
no	O
point	O
.	O

just	O
because	O
someone	O
says	O
something	O
does	O
nt	O
make	O
it	O
valid	O
.	O
nothing	O
you	O
say	O
is	O
valid	O
.	O

that	O
's	O
your	O
opinion	O
.	O
but	O
it	O
's	O
not	O
valid	O
.	O
my	O
factual	O
statement	O
is	O
.	O

Headline	O
:	O
'	O
The	O
Daily	O
Show	O
'	O
Nailed	O
How	O
Islamophobia	O
Hurts	O
the	O
Sikh	O
Community	O
Too	O

This	O
section	O
outlines	O
our	O
coding	O
scheme	O
for	O
identifying	O
ERICs	O
,	O
with	O
labels	O
for	O
comment	O
threads	O
and	O
each	O
comment	O
contained	O
therein	O
.	O
Starting	O
with	O
the	O
annotation	O
categories	O
from	O
the	O
IAC	O
and	O
the	O
curation	O
criteria	O
of	O
Diakopoulos	O
(	O
2015	O
)	O
,	O
we	O
have	O
adapted	O
these	O
schemes	O
and	O
identified	O
new	O
characteristics	O
that	O
have	O
broad	O
coverage	O
over	O
100	O
comment	O
threads	O
(	O
4	O
)	O
that	O
we	O
manually	O
examined	O
.	O
Annotations	O
are	O
made	O
at	O
the	O
thread	O
-	O
level	O
and	O
the	O
comment	O
-	O
level	O
.	O
Thread	O
-	O
level	O
annotations	O
capture	O
the	O
qualities	O
of	O
a	O
thread	O
on	O
the	O
whole	O
,	O
while	O
comment	O
-	O
level	O
annotations	O
reflect	O
the	O
characteristics	O
of	O
each	O
comment	O
.	O
The	O
labels	O
for	O
each	O
dimension	O
are	O
described	O
below	O
.	O
Only	O
one	O
label	O
per	O
dimension	O
is	O
allowed	O
unless	O
otherwise	O
specified	O
.	O

Agreement	O
The	O
overall	O
agreement	O
present	O
in	O
a	O
thread	O
.	O
Agreement	O
throughout	O
Continual	O
disagreement	O
Agreement	O
disagreement	O
:	O
Begins	O
with	O
agreement	O
which	O
turns	O
into	O
disagreement	O
.	O
Disagreement	O
agreement	O
:	O
Starts	O
with	O
disagreement	O
that	O
converges	O
into	O
agreement	O
.	O
Constructiveness	O
A	O
binary	O
label	O
indicating	O
when	O
a	O
conversation	O
is	O
an	O
ERIC	O
,	O
or	O
has	O
a	O
clear	O
exchange	O
of	O
ideas	O
,	O
opinions	O
,	O
and/or	O
information	O
done	O
so	O
somewhat	O
respectfully	O
.	O
1	O
Constructive	O
Not	O
constructive	O
Type	O
The	O
overall	O
type	O
or	O
tone	O
of	O
the	O
conversation	O
,	O
describing	O
the	O
majority	O
of	O
comments	O
.	O
Two	O
labels	O
can	O
be	O
chosen	O
if	O
conversations	O
exhibit	O
more	O
than	O
one	O
dominant	O
feature	O
.	O
Argumentative	O
:	O
Contains	O
a	O
lot	O
of	O
"	O
back	O
and	O
forth	O
"	O
between	O
participants	O
that	O
does	O
not	O
necessarily	O
reach	O
a	O
conclusion	O
.	O
Flamewar	O
:	O
Contains	O
insults	O
,	O
users	O
"	O
yelling	O
"	O
at	O
each	O
other	O
,	O
and	O
no	O
information	O
exchanged	O
.	O
Off	O
-	O
Topic	O
/	O
digression	O
:	O
Comments	O
are	O
completely	O
irrelevant	O
to	O
the	O
article	O
or	O
each	O
other	O
,	O
or	O
the	O
conversation	O
starts	O
on	O
topic	O
but	O
veers	O
off	O
into	O
another	O
direction	O
.	O
Personal	O
stories	O
:	O
Participants	O
exchange	O
personal	O
anecdotes	O
.	O
Positive	O
/	O
respectful	O
:	O
Consists	O
primarily	O
of	O
comments	O
expressing	O
opinions	O
in	O
a	O
respectful	O
,	O
potentially	O
empathetic	O
manner	O
.	O
Snarky	O
/	O
humorous	O
:	O
Participants	O
engage	O
with	O
each	O
other	O
using	O
humor	O
rather	O
than	O
argue	O
or	O
sympathize	O
.	O
May	O
be	O
on	O
-	O
or	O
off	O
-	O
topic	O
.	O

The	O
corpus	O
was	O
coded	O
by	O
two	O
groups	O
of	O
annotators	O
:	O
professional	O
trained	O
editors	O
and	O
untrained	O
crowdsourced	O
workers	O
.	O
Three	O
separate	O
annotators	O
coded	O
each	O
thread	O
.	O
The	O
trained	O
editors	O
were	O
paid	O
contractors	O
who	O
received	O
two	O
30	O
-	O
45	O
-	O
minute	O
training	O
sessions	O
,	O
editorial	O
guidelines	O
(	O
2	O
,	O
000	O
-	O
word	O
document	O
)	O
,	O
and	O
two	O
sample	O
annotated	O
threads	O
.	O
The	O
training	O
sessions	O
were	O
recorded	O
and	O
available	O
to	O
the	O
annotators	O
during	O
annotation	O
,	O
as	O
were	O
the	O
guidelines	O
.	O
They	O
could	O
communicate	O
their	O
questions	O
to	O
the	O
trainers	O
,	O
who	O
were	O
two	O
authors	O
of	O
this	O
paper	O
,	O
and	O
receive	O
feedback	O
during	O
the	O
training	O
and	O
annotation	O
phases	O
.	O
Because	O
training	O
is	O
expensive	O
and	O
time	O
consuming	O
,	O
we	O
also	O
collected	O
annotations	O
from	O
untrained	O
coders	O
on	O
Amazon	O
Mechanical	O
Turk	O
(	O
AMT	O
)	O
.	O
To	O
simplify	O
the	O
task	O
for	O
AMT	O
,	O
we	O
only	O
solicited	O
thread	O
-	O
level	O
labels	O
,	O
paying	O
$	O
0.75	O
per	O
thread	O
.	O
For	O
quality	O
assurance	O
,	O
only	O
workers	O
located	O
in	O
the	O
United	O
States	O
or	O
Canada	O
with	O
a	O
minimum	O
HIT	O
acceptance	O
rate	O
of	O
95	O
%	O
could	O
participate	O
,	O
and	O
the	O
annotations	O
were	O
spot	O
-	O
checked	O
by	O
the	O
authors	O
.	O
Trained	O
annotators	O
coded	O
1	O
,	O
300	O
Yahoo	O
threads	O
and	O
the	O
100	O
-	O
thread	O
test	O
set	O
on	O
the	O
comment	O
-	O
and	O
thread	O
-	O
levels	O
;	O
untrained	O
annotators	O
coded	O
threadlevel	O
labels	O
of	O
1	O
,	O
300	O
Yahoo	O
threads	O
(	O
300	O
of	O
which	O
overlapped	O
with	O
the	O
trained	O
annotations	O
)	O
and	O
1	O
,	O
000	O
IAC	O
threads	O
(	O
Table	O
1	O
)	O
.	O
In	O
total	O
,	O
26	O
trained	O
and	O
495	O
untrained	O
annotators	O
worked	O
on	O
this	O
task	O
.	O

To	O
assess	O
the	O
difficulty	O
of	O
the	O
task	O
,	O
we	O
also	O
collected	O
a	O
rating	O
for	O
each	O
thread	O
from	O
the	O
trained	O
annotators	O
describing	O
how	O
confident	O
they	O
were	O
with	O
their	O
judgments	O
of	O
each	O
thread	O
and	O
the	O
comments	O
it	O
comprises	O
.	O
Ratings	O
were	O
made	O
on	O
a	O
5	O
-	O
level	O
Likert	O
scale	O
,	O
with	O
1	O
being	O
not	O
at	O
all	O
confident	O
and	O
5	O
fully	O
confident	O
.	O
The	O
levels	O
of	O
confidence	O
were	O
high	O
(	O
3.9	O
±	O
0.7	O
)	O
,	O
indicating	O
that	O
coders	O
were	O
able	O
to	O
distinguish	O
the	O
thread	O
and	O
comment	O
codes	O
with	O
relative	O
ease	O
.	O

To	O
understand	O
what	O
makes	O
a	O
thread	O
constructive	O
,	O
we	O
explore	O
the	O
following	O
research	O
questions	O
:	O
1	O
.	O
How	O
does	O
the	O
overall	O
thread	O
categorization	O
differ	O
between	O
ERICs	O
and	O
non	O
-	O
ERICs	O
?	O
(	O
5.1	O
)	O
2	O
.	O
What	O
types	O
of	O
comments	O
make	O
up	O
ERICs	O
compared	O
to	O
non	O
-	O
ERICs	O
?	O
(	O
5.2	O
)	O
3	O
.	O
Are	O
social	O
signals	O
related	O
to	O
whether	O
a	O
thread	O
is	O
an	O
ERIC	O
?	O
(	O
5.3	O
)	O

Before	O
examining	O
what	O
types	O
of	O
threads	O
are	O
ERICs	O
,	O
we	O
first	O
compare	O
the	O
threads	O
coded	O
by	O
different	O
sets	O
of	O
annotators	O
(	O
trained	O
or	O
untrained	O
)	O
and	O
from	O
different	O
sources	O
(	O
IAC	O
or	O
Yahoo	O
)	O
.	O
We	O
measure	O
the	O
significance	O
of	O
annotation	O
group	O
for	O
each	O
label	O
with	O
a	O
test	O
of	O
equal	O
proportions	O
for	O
binary	O
categories	O
(	O
constructiveness	O
and	O
each	O
thread	O
type	O
)	O
and	O
a	O
chi	O
-	O
squared	O
test	O
of	O
independence	O
for	O
the	O
agreement	O
label	O
.	O
Overall	O
,	O
annotations	O
by	O
the	O
trained	O
and	O
untrained	O
annotators	O
on	O
Yahoo	O
threads	O
are	O
very	O
similar	O
,	O
with	O
significant	O
differences	O
only	O
between	O
some	O
of	O
the	O
thread	O
type	O
labels	O
(	O
Figure	O
3	O
)	O
.	O
We	O
posit	O
that	O
the	O
discrepancies	O
between	O
the	O
trained	O
and	O
untrained	O
annotators	O
is	O
due	O
to	O
the	O
former	O
's	O
training	O
sessions	O
and	O
ability	O
to	O
communicate	O
with	O
the	O
authors	O
,	O
which	O
could	O
have	O
swayed	O
annotators	O
to	O
make	O
inferences	O
into	O
the	O
coding	O
scheme	O
that	O
were	O
not	O
overtly	O
stated	O
in	O
the	O
instructions	O
.	O
The	O
differences	O
between	O
Yahoo	O
and	O
IAC	O
threads	O
are	O
more	O
pronounced	O
.	O
The	O
only	O
label	O
for	O
which	O
there	O
is	O
no	O
significant	O
difference	O
is	O
personal	O
stories	O
(	O
p	O
=	O
0.41	O
,	O
between	O
the	O
IAC	O
and	O
trained	O
Yahoo	O
labels	O
)	O
.	O
All	O
other	O
IAC	O
labels	O
are	O
significantly	O
different	O
from	O
both	O
trained	O
and	O
untrained	O
Yahoo	O
labels	O
(	O
p	O
<	O
0.001	O
)	O
.	O
ERICs	O
are	O
more	O
prevalent	O
in	O
the	O
IAC	O
,	O
with	O
70	O
%	O
of	O
threads	O
labeled	O
constructive	O
,	O
compared	O
to	O
roughly	O
half	O
of	O
Yahoo	O
threads	O
.	O
On	O
the	O
whole	O
,	O
threads	O
from	O
the	O
IAC	O
are	O
more	O
concordant	O
and	O
positive	O
than	O
from	O
Yahoo	O
:	O
they	O
have	O
more	O
agreement	O
and	O
less	O
disagreement	O
,	O
more	O
than	O
twice	O
as	O
many	O
positive	O
/	O
respectful	O
threads	O
,	O
and	O
fewer	O
than	O
half	O
the	O
flamewars	O
.	O
For	O
Yahoo	O
threads	O
,	O
there	O
is	O
no	O
significant	O
difference	O
between	O
trained	O
and	O
untrained	O
coders	O
for	O
constructiveness	O
(	O
p	O
=	O
0.11	O
)	O
and	O
the	O
argumentative	O
thread	O
type	O
(	O
p	O
=	O
0.07	O
;	O
all	O
other	O
thread	O
types	O
are	O
significant	O
with	O
p	O
<	O
10	O
−5	O
)	O
.	O
There	O
is	O
no	O
significant	O
difference	O
between	O
the	O
agreement	O
labels	O
,	O
either	O
(	O
p	O
=	O
1.00	O
)	O
.	O
Untrained	O
coders	O
are	O
more	O
likely	O
than	O
trained	O
to	O
classify	O
threads	O
using	O
emotional	O
labels	O
like	O
snarky	O
,	O
flamewar	O
,	O
and	O
positive	O
/	O
respectful	O
,	O
while	O
trained	O
annotators	O
more	O
frequently	O
recognize	O
off	O
-	O
topic	O
threads	O
.	O
These	O
differences	O
should	O
be	O
taken	O
into	O
consideration	O
for	O
evaluating	O
the	O
IAC	O
codes	O
,	O
and	O
for	O
future	O
efforts	O
collecting	O
subjective	O
annotations	O
through	O
crowdsourcing	O
.	O
We	O
measure	O
the	O
strength	O
of	O
relationships	O
between	O
labels	O
with	O
the	O
phi	O
coefficient	O
(	O
Figure	O
4	O
)	O
.	O
There	O
is	O
a	O
positive	O
association	O
between	O
ERICs	O
and	O
all	O
agreement	O
labels	O
in	O
both	O
Yahoo	O
(	O
trained	O
)	O
and	O
IAC	O
threads	O
,	O
which	O
indicates	O
that	O
concord	O
is	O
not	O
necessary	O
for	O
threads	O
to	O
be	O
constructive	O
.	O
The	O
example	O
in	O
Figure	O
1	O
is	O
a	O
constructive	O
thread	O
that	O
is	O
argumentative	O
and	O
contains	O
disagreement	O
.	O
Thread	O
types	O
associated	O
with	O
non	O
-	O
ERICs	O
are	O
flamewars	O
,	O
off	O
-	O
topic	O
digressions	O
,	O
and	O
snarky	O
/	O
humorous	O
exchanges	O
,	O
which	O
is	O
consistent	O
across	O
data	O
sources	O
.	O
The	O
labels	O
from	O
untrained	O
annotators	O
show	O
a	O
stronger	O
correlation	O
between	O
flamewars	O
and	O
not	O
constructive	O
compared	O
to	O
the	O
trained	O
annotators	O
,	O
but	O
the	O
former	O
also	O
identified	O
more	O
flamewars	O
.	O
Some	O
correlations	O
are	O
expected	O
:	O
across	O
all	O
annotating	O
groups	O
,	O
there	O
is	O
a	O
positive	O
correlation	O
between	O
threads	O
labeled	O
with	O
agreement	O
throughout	O
and	O
positive	O
/	O
respectful	O
,	O
and	O
disagreement	O
throughout	O
is	O
correlated	O
with	O
argumentative	O
(	O
Figures	O
1	O
and	O
2	O
)	O
and	O
,	O
to	O
a	O
lesser	O
degree	O
,	O
flamewar	O
.	O
The	O
greatest	O
difference	O
between	O
the	O
IAC	O
and	O
Yahoo	O
are	O
the	O
thread	O
types	O
associated	O
with	O
ERICs	O
.	O
In	O
the	O
IAC	O
,	O
the	O
positive	O
/	O
respectful	O
label	O
has	O
a	O
much	O
stronger	O
positive	O
relationship	O
with	O
constructive	O
than	O
the	O
trained	O
Yahoo	O
labels	O
,	O
but	O
this	O
could	O
be	O
due	O
to	O
the	O
difference	O
between	O
trained	O
and	O
untrained	O
coders	O
.	O
Argumentative	O
has	O
a	O
positive	O
correlation	O
with	O
constructive	O
in	O
the	O
Yahoo	O
threads	O
,	O
but	O
a	O
weak	O
negative	O
relationship	O
is	O
found	O
in	O
the	O
IAC	O
.	O
In	O
both	O
domains	O
,	O
threads	O
characterized	O
as	O
offtopic	O
,	O
snarky	O
,	O
or	O
flamewars	O
are	O
more	O
likely	O
to	O
be	O
non	O
-	O
ERICs	O
.	O
Threads	O
with	O
some	O
level	O
of	O
agreement	O
characterized	O
as	O
positive	O
/	O
respectful	O
are	O
commonly	O
ERICs	O
.	O
A	O
two	O
-	O
tailed	O
z	O
-	O
test	O
shows	O
a	O
significant	O
difference	O
between	O
the	O
number	O
of	O
ERICs	O
and	O
non	O
-	O
ERICs	O
in	O
Yahoo	O
articles	O
in	O
the	O
Arts	O
&	O
Entertainment	O
,	O
Finance	O
,	O
and	O
Lifestyle	O
categories	O
(	O
p	O
<	O
0.005	O
;	O
Figure	O
5	O
)	O
.	O

We	O
next	O
consider	O
the	O
codes	O
assigned	O
by	O
trained	O
annotators	O
to	O
Yahoo	O
comments	O
(	O
Figure	O
6	O
)	O
.	O
The	O
majority	O
of	O
comments	O
are	O
not	O
persuasive	O
,	O
reply	O
to	O
a	O
previous	O
comment	O
,	O
express	O
disagreement	O
,	O
or	O
have	O
negative	O
sentiment	O
.	O
More	O
than	O
three	O
times	O
as	O
many	O
comments	O
express	O
disagreement	O
than	O
agreement	O
,	O
and	O
comments	O
are	O
labeled	O
negative	O
seven	O
times	O
as	O
frequently	O
as	O
positive	O
.	O
Approximately	O
half	O
of	O
the	O
comments	O
express	O
disagreement	O
or	O
a	O
negative	O
sentiment	O
.	O
Very	O
few	O
comments	O
are	O
funny	O
,	O
positive	O
,	O
sympathetic	O
,	O
or	O
contain	O
a	O
personal	O
story	O
(	O
<	O
10	O
%	O
)	O
.	O
Encouragingly	O
,	O
only	O
6	O
%	O
of	O
comments	O
are	O
off	O
-	O
topic	O
with	O
the	O
conversation	O
,	O
suggesting	O
that	O
participants	O
are	O
attuned	O
to	O
and	O
respectful	O
of	O
the	O
topic	O
.	O
Only	O
20	O
%	O
of	O
comments	O
are	O
informative	O
,	O
indicating	O
that	O
participants	O
infrequently	O
introduce	O
new	O
information	O
to	O
complement	O
the	O
article	O
or	O
discussion	O
.	O
The	O
only	O
strong	O
correlations	O
are	O
between	O
the	O
binary	O
labels	O
,	O
but	O
the	O
moderate	O
correlations	O
provide	O
insight	O
into	O
the	O
Yahoo	O
threads	O
(	O
Figure	O
7	O
)	O
.	O
Some	O
relationships	O
accord	O
with	O
intuition	O
.	O
For	O
instance	O
,	O
participants	O
tend	O
to	O
go	O
off	O
-	O
topic	O
with	O
the	O
article	O
when	O
they	O
are	O
responding	O
to	O
others	O
and	O
not	O
during	O
broadcast	O
messages	O
;	O
comments	O
expressing	O
disagreement	O
with	O
a	O
commenter	O
are	O
frequently	O
posted	O
in	O
a	O
reply	O
to	O
a	O
commenter	O
;	O
comments	O
expressing	O
agreement	O
tend	O
to	O
be	O
sympathetic	O
and	O
have	O
positive	O
sentiment	O
;	O
and	O
mean	O
comments	O
correlate	O
with	O
negative	O
sentiment	O
.	O
Commenters	O
in	O
this	O
domain	O
also	O
express	O
disagreement	O
without	O
particular	O
nastiness	O
,	O
since	O
there	O
is	O
no	O
correlation	O
between	O
disagreement	O
and	O
mean	O
or	O
sarcastic	O
comments	O
.	O
The	O
informative	O
label	O
is	O
moderately	O
correlated	O
with	O
persuasiveness	O
,	O
suggesting	O
that	O
comments	O
containing	O
facts	O
and	O
new	O
information	O
are	O
more	O
convincing	O
than	O
those	O
without	O
.	O
The	O
correlation	O
between	O
comment	O
and	O
thread	O
labels	O
is	O
shown	O
in	O
Figure	O
7	O
.	O
Many	O
of	O
the	O
relationships	O
are	O
unsurprising	O
,	O
like	O
off	O
-	O
topic	O
threads	O
tend	O
to	O
have	O
off	O
-	O
topic	O
comments	O
,	O
personal	O
-	O
story	O
threads	O
have	O
personal	O
-	O
story	O
comments	O
;	O
thread	O
agreement	O
levels	O
correlate	O
with	O
comment	O
-	O
level	O
In	O
accord	O
with	O
our	O
definition	O
of	O
ERICs	O
,	O
constructiveness	O
is	O
positively	O
correlated	O
with	O
informative	O
and	O
persuasive	O
comments	O
and	O
negatively	O
correlated	O
with	O
negative	O
and	O
mean	O
comments	O
.	O
From	O
these	O
correlations	O
one	O
can	O
infer	O
that	O
argumenta	O
-	O
tive	O
threads	O
are	O
generally	O
respectful	O
because	O
,	O
while	O
they	O
are	O
strongly	O
correlated	O
with	O
comments	O
that	O
are	O
controversial	O
or	O
express	O
disagreement	O
or	O
a	O
mixed	O
sentiment	O
,	O
there	O
is	O
no	O
correlation	O
with	O
mean	O
and	O
very	O
little	O
with	O
negative	O
sentiment	O
.	O
More	O
surprising	O
is	O
the	O
positive	O
correlation	O
between	O
controversial	O
comments	O
and	O
constructive	O
threads	O
.	O
Controversial	O
comments	O
are	O
more	O
associated	O
with	O
ERICs	O
,	O
not	O
non	O
-	O
ERICs	O
,	O
even	O
though	O
the	O
controversial	O
label	O
also	O
positively	O
correlates	O
with	O
flamewars	O
,	O
which	O
are	O
negatively	O
correlated	O
with	O
constructiveness	O
.	O
The	O
examples	O
in	O
Figures	O
1	O
-	O
2	O
both	O
have	O
controversial	O
comments	O
expressing	O
disagreement	O
,	O
but	O
comments	O
in	O
the	O
second	O
half	O
of	O
the	O
non	O
-	O
ERIC	O
veer	O
off	O
-	O
topic	O
and	O
are	O
not	O
persuasive	O
,	O
where	O
the	O
ERIC	O
stays	O
on	O
-	O
topic	O
and	O
persuasive	O
.	O

Previous	O
work	O
has	O
taken	O
social	O
signals	O
to	O
be	O
a	O
proxy	O
for	O
thread	O
quality	O
,	O
using	O
some	O
function	O
of	O
the	O
total	O
number	O
of	O
votes	O
received	O
by	O
comments	O
within	O
a	O
thread	O
(	O
e.g.	O
,	O
Lee	O
et	O
al	O
(	O
2014	O
)	O
)	O
.	O
Because	O
earlier	O
research	O
has	O
indicated	O
that	O
user	O
votes	O
are	O
not	O
completely	O
independent	O
or	O
objective	O
(	O
Sipos	O
et	O
al	O
,	O
2014	O
;	O
Danescu	O
-	O
Niculescu	O
-	O
Mizil	O
et	O
al	O
,	O
2009	O
)	O
,	O
we	O
take	O
the	O
use	O
of	O
votes	O
as	O
a	O
proxy	O
for	O
quality	O
skeptically	O
ad	O
perform	O
our	O
own	O
exploration	O
of	O
the	O
relationship	O
between	O
social	O
signals	O
and	O
the	O
presence	O
of	O
ERICs	O
.	O
On	O
Yahoo	O
,	O
users	O
reacted	O
to	O
comments	O
with	O
a	O
thumbs	O
up	O
or	O
thumbs	O
down	O
and	O
we	O
collected	O
the	O
total	O
number	O
of	O
such	O
reactions	O
for	O
each	O
comment	O
in	O
our	O
corpus	O
.	O
First	O
,	O
we	O
compare	O
the	O
total	O
number	O
of	O
thumbs	O
up	O
(	O
TU	O
)	O
and	O
thumbs	O
down	O
(	O
TD	O
)	O
received	O
by	O
comments	O
in	O
a	O
thread	O
to	O
the	O
coded	O
labels	O
to	O
determine	O
whether	O
there	O
are	O
any	O
relationships	O
between	O
social	O
signals	O
and	O
threads	O
qualities	O
.	O
We	O
calculate	O
the	O
relationship	O
between	O
labels	O
in	O
each	O
category	O
with	O
TU	O
and	O
TD	O
with	O
Pearson	O
's	O
coefficient	O
for	O
the	O
binary	O
labels	O
and	O
a	O
one	O
-	O
way	O
ANOVA	O
for	O
the	O
agreement	O
category	O
.	O
The	O
strongest	O
correlation	O
is	O
between	O
TD	O
and	O
untrained	O
annotators	O
'	O
perception	O
of	O
flamewars	O
(	O
r	O
=	O
0.21	O
)	O
,	O
and	O
there	O
is	O
a	O
very	O
weak	O
to	O
no	O
correlation	O
(	O
positive	O
or	O
negative	O
)	O
between	O
the	O
other	O
labels	O
and	O
TU	O
,	O
TD	O
,	O
or	O
TU−TD	O
.	O
There	O
is	O
moderate	O
correlation	O
between	O
TU	O
and	O
TD	O
(	O
r	O
=	O
0.46	O
)	O
,	O
suggesting	O
that	O
threads	O
that	O
elicit	O
reactions	O
tend	O
to	O
receive	O
both	O
thumbs	O
up	O
and	O
down	O
.	O
The	O
correlation	O
between	O
TU	O
and	O
TD	O
received	O
by	O
each	O
comment	O
is	O
weaker	O
(	O
r	O
=	O
0.23	O
)	O
.	O
Comparing	O
the	O
comment	O
labels	O
to	O
the	O
TU	O
and	O
TD	O
received	O
by	O
each	O
comment	O
also	O
show	O
little	O
correlation	O
.	O
Comments	O
that	O
reply	O
to	O
a	O
specific	O
commenter	O
are	O
negatively	O
correlated	O
with	O
TU	O
,	O
TD	O
,	O
and	O
TU−TD	O
(	O
r	O
=	O
0.30	O
,	O
-	O
0.25	O
,	O
and	O
-	O
0.22	O
,	O
respectively	O
)	O
.	O
The	O
only	O
other	O
label	O
with	O
a	O
non	O
-	O
negligible	O
correlation	O
is	O
disagreement	O
with	O
a	O
commenter	O
,	O
which	O
negatively	O
correlates	O
with	O
TU	O
(	O
r	O
=	O
−0.21	O
)	O
.	O
There	O
is	O
no	O
correlation	O
between	O
social	O
signal	O
and	O
the	O
presence	O
of	O
ERICs	O
or	O
non	O
-	O
ERICs	O
.	O
These	O
results	O
support	O
the	O
findings	O
of	O
previous	O
work	O
and	O
indicate	O
that	O
thumbs	O
up	O
or	O
thumbs	O
down	O
alone	O
(	O
and	O
,	O
presumably	O
,	O
up	O
/	O
down	O
votes	O
)	O
are	O
inappropriate	O
proxies	O
for	O
quality	O
measurements	O
of	O
comments	O
or	O
threads	O
in	O
this	O
domain	O
.	O

We	O
have	O
developed	O
a	O
coding	O
scheme	O
for	O
labeling	O
"	O
good	O
"	O
online	O
conversations	O
(	O
ERICs	O
)	O
and	O
created	O
the	O
Yahoo	O
News	O
Annotated	O
Comments	O
Corpus	O
,	O
a	O
new	O
corpus	O
of	O
2.4k	O
coded	O
comment	O
threads	O
posted	O
in	O
response	O
to	O
Yahoo	O
News	O
articles	O
.	O
Additionally	O
,	O
we	O
have	O
annotated	O
1k	O
debate	O
threads	O
from	O
the	O
IAC	O
.	O
These	O
annotations	O
reflect	O
several	O
different	O
characteristics	O
of	O
comments	O
and	O
threads	O
,	O
and	O
we	O
have	O
explored	O
their	O
relationships	O
with	O
each	O
other	O
.	O
ERICs	O
are	O
characterized	O
by	O
argumentative	O
,	O
respectful	O
exchanges	O
containing	O
persuasive	O
,	O
informative	O
,	O
and/or	O
sympathetic	O
comments	O
.	O
They	O
tend	O
to	O
stay	O
on	O
topic	O
with	O
the	O
original	O
article	O
and	O
not	O
to	O
contain	O
funny	O
,	O
mean	O
,	O
or	O
sarcastic	O
comments	O
.	O
We	O
found	O
differences	O
between	O
the	O
distribution	O
of	O
annotations	O
made	O
by	O
trained	O
and	O
untrained	O
annotators	O
,	O
but	O
high	O
levels	O
of	O
agreement	O
within	O
each	O
group	O
,	O
suggesting	O
that	O
crowdsourcing	O
annotations	O
for	O
this	O
task	O
is	O
reliable	O
.	O
YNACC	O
will	O
be	O
a	O
valuable	O
resource	O
for	O
researchers	O
in	O
multiple	O
areas	O
of	O
discourse	O
analysis	O
.	O

We	O
are	O
grateful	O
to	O
Danielle	O
Lottridge	O
,	O
Smaranda	O
Muresan	O
,	O
and	O
Amanda	O
Stent	O
for	O
their	O
valuable	O
input	O
.	O
We	O
also	O
wish	O
to	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
feedback	O
.	O

Linguistic	O
Reflexes	O
of	O
Well	O
-	O
Being	O
and	O
Happiness	O
in	O
Echo	O

Different	O
theories	O
posit	O
different	O
sources	O
for	O
feelings	O
of	O
well	O
-	O
being	O
and	O
happiness	O
.	O
Appraisal	O
theory	O
grounds	O
our	O
emotional	O
responses	O
in	O
our	O
goals	O
and	O
desires	O
and	O
their	O
fulfillment	O
,	O
or	O
lack	O
of	O
fulfillment	O
.	O
Self	O
-	O
Determination	O
theory	O
posits	O
that	O
the	O
basis	O
for	O
well	O
-	O
being	O
rests	O
on	O
our	O
assessments	O
of	O
our	O
competence	O
,	O
autonomy	O
and	O
social	O
connection	O
.	O
And	O
surveys	O
that	O
measure	O
happiness	O
empirically	O
note	O
that	O
people	O
require	O
their	O
basic	O
needs	O
to	O
be	O
met	O
for	O
food	O
and	O
shelter	O
,	O
but	O
beyond	O
that	O
tend	O
to	O
be	O
happiest	O
when	O
socializing	O
,	O
eating	O
or	O
having	O
sex	O
.	O
We	O
analyze	O
a	O
corpus	O
of	O
private	O
micro	O
-	O
blogs	O
from	O
a	O
well	O
-	O
being	O
application	O
called	O
ECHO	O
,	O
where	O
users	O
label	O
each	O
written	O
post	O
about	O
daily	O
events	O
with	O
a	O
happiness	O
score	O
between	O
1	O
and	O
9	O
.	O
Our	O
goal	O
is	O
to	O
ground	O
the	O
linguistic	O
descriptions	O
of	O
events	O
that	O
users	O
experience	O
in	O
theories	O
of	O
well	O
-	O
being	O
and	O
happiness	O
,	O
and	O
then	O
examine	O
the	O
extent	O
to	O
which	O
different	O
theoretical	O
accounts	O
can	O
explain	O
the	O
variance	O
in	O
the	O
happiness	O
scores	O
.	O
We	O
show	O
that	O
recurrent	O
event	O
types	O
,	O
such	O
as	O
OBLIGATION	O
and	O
IN	O
-	O
COMPETENCE	O
,	O
which	O
affect	O
people	O
's	O
feelings	O
of	O
well	O
-	O
being	O
are	O
not	O
captured	O
in	O
current	O
lexical	O
or	O
semantic	O
resources	O
.	O

This	O
research	O
was	O
partially	O
supported	O
by	O
NSF	O
Robust	O
Intelligence	O
#	O
IIS	O
-	O
1302668	O
-	O
002	O
and	O
NSF	O
HCC	O
#	O
IIS	O
-	O
1321102	O
.	O

After	O
preprocessing	O
and	O
extracting	O
the	O
vocabulary	O
from	O
our	O
training	O
documents	O
,	O
each	O
word	O
type	O
is	O
converted	O
to	O
its	O
embedding	O
representation	O
(	O
averaging	O
all	O
of	O
its	O
tokens	O
for	O
contextualized	O
embeddings	O
;	O
details	O
in	O
5.3	O
)	O
.	O
Following	O
this	O
we	O
apply	O
the	O
various	O
clustering	O
algorithms	O
on	O
the	O
entire	O
training	O
corpus	O
vocabulary	O
to	O
obtain	O
k	O
clusters	O
,	O
using	O
weighted	O
(	O
3.2	O
)	O
or	O
unweighted	O
word	O
types	O
.	O
After	O
the	O
clustering	O
algorithm	O
has	O
converged	O
,	O
we	O
obtain	O
the	O
top	O
J	O
words	O
(	O
3.1	O
)	O
from	O
each	O
cluster	O
for	O
evaluation	O
.	O
Note	O
that	O
one	O
potential	O
shortcoming	O
of	O
our	O
approach	O
is	O
the	O
possibility	O
of	O
outliers	O
forming	O
their	O
own	O
cluster	O
,	O
which	O
we	O
leave	O
to	O
future	O
work	O
.	O
Figure	O
1	O
:	O
The	O
figure	O
on	O
the	O
left	O
shows	O
the	O
cluster	O
center	O
(	O
)	O
without	O
weighting	O
,	O
while	O
the	O
figure	O
on	O
the	O
right	O
shows	O
that	O
after	O
weighting	O
(	O
larger	O
points	O
have	O
higher	O
weight	O
)	O
a	O
hopefully	O
more	O
representative	O
cluster	O
center	O
is	O
found	O
.	O
Note	O
that	O
top	O
words	O
based	O
on	O
distance	O
from	O
the	O
cluster	O
center	O
could	O
still	O
very	O
well	O
be	O
low	O
frequency	O
word	O
types	O
,	O
motivating	O
reranking	O
(	O
3.3	O
)	O
.	O

When	O
obtaining	O
the	O
top	O
-	O
J	O
words	O
that	O
make	O
up	O
a	O
cluster	O
's	O
topic	O
,	O
we	O
also	O
consider	O
reranking	O
terms	O
,	O
as	O
there	O
is	O
no	O
guarantee	O
that	O
words	O
closest	O
to	O
cluster	O
centers	O
are	O
important	O
word	O
types	O
.	O
We	O
will	O
show	O
in	O
Table	O
2	O
that	O
without	O
reranking	O
,	O
clustering	O
yields	O
"	O
sensible	O
"	O
topics	O
but	O
low	O
NPMI	O
scores	O
.	O

To	O
incorporate	O
corpus	O
statistics	O
into	O
the	O
clustering	O
algorithm	O
,	O
we	O
examine	O
three	O
different	O
schemes	O
2	O
to	O
assign	O
weights	O
to	O
word	O
types	O
,	O
where	O
n	O
t	O
is	O
the	O
count	O
of	O
word	O
type	O
t	O
in	O
corpus	O
D	O
,	O
and	O
d	O
is	O
a	O
document	O
:	O
tf	O
=	O
n	O
t	O
t	O
n	O
t	O
(	O
1	O
)	O
tf	O
-	O
df	O
=	O
tf	O
|	O
{	O
d	O
D	O
|	O
t	O
d	O
}	O
|	O
|	O
D	O
|	O
(	O
2	O
)	O
tf	O
-	O
idf	O
=	O
tf	O
log	O
|	O
D	O
|	O
|	O
{	O
d	O
D	O
|	O
t	O
d	O
}	O
|	O
+	O
1	O
(	O
3	O
)	O
These	O
scores	O
can	O
now	O
be	O
used	O
for	O
weighting	O
word	O
types	O
when	O
clustering	O
(	O
w	O
)	O
,	O
reranking	O
top	O
100	O
words	O
(	O
r	O
)	O
after	O
,	O
both	O
(	O
w	O
r	O
)	O
,	O
or	O
neither	O
(	O
simply	O
)	O
.	O
We	O
find	O
that	O
simply	O
using	O
tf	O
outperforms	O
the	O
other	O
weighting	O
schemes	O
(	O
App	O
.	O
C	O
)	O
.	O
Our	O
results	O
and	O
subsequent	O
analysis	O
in	O
6	O
uses	O
tf	O
for	O
weighting	O
and	O
reranking	O
.	O

Our	O
implementation	O
is	O
freely	O
available	O
online	O
.	O
4	O

We	O
use	O
the	O
20	O
newsgroup	O
dataset	O
(	O
20NG	O
)	O
which	O
contains	O
around	O
18000	O
documents	O
and	O
20	O
categories	O
,	O
5	O
and	O
a	O
subset	O
of	O
Reuters21578	O
6	O
which	O
contains	O
around	O
10000	O
documents	O
.	O

Our	O
main	O
results	O
are	O
shown	O
in	O
Table	O
1	O
.	O

To	O
further	O
understand	O
the	O
effect	O
of	O
other	O
centroid	O
based	O
algorithms	O
on	O
topic	O
coherence	O
,	O
we	O
also	O
applied	O
the	O
k	O
-	O
medoids	O
(	O
KD	O
)	O
clustering	O
algorithm	O
.	O
KD	O
is	O
a	O
hard	O
clustering	O
algorithm	O
similar	O
to	O
KM	O
but	O
less	O
sensitive	O
to	O
outliers	O
.	O
As	O
we	O
can	O
see	O
in	O
Table	O
3	O
,	O
in	O
all	O
cases	O
KD	O
usually	O
did	O
as	O
well	O
or	O
worse	O
than	O
KM	O
.	O
KD	O
also	O
did	O
relatively	O
poorly	O
after	O
frequency	O
reranking	O
.	O
Where	O
KD	O
did	O
do	O
better	O
than	O
KM	O
,	O
the	O
difference	O
is	O
not	O
very	O
striking	O
and	O
the	O
NPMI	O
scores	O
were	O
still	O
quite	O
below	O
the	O
other	O
top	O
performing	O
models	O
.	O

Mises	O
-	O
Fisher	O
Mixture	O

Self	O
-	O
Alignment	O
Pretraining	O
for	O
Biomedical	O
Entity	O
Representations	O

We	O
divide	O
our	O
experimental	O
datasets	O
into	O
two	O
categories	O
(	O
1	O
)	O
scientific	O
language	O
datasests	O
where	O
the	O
data	O
is	O
extracted	O
from	O
scientific	O
papers	O
and	O
(	O
2	O
)	O
social	O
media	O
language	O
datasets	O
where	O
the	O
data	O
is	O
coming	O
from	O
social	O
media	O
forums	O
like	O
Reddit.com	O
.	O
For	O
an	O
overview	O
of	O
the	O
key	O
statistics	O
,	O
see	O
Tab	O
.	O
3	O
.	O

All	O
our	O
experiments	O
are	O
conducted	O
on	O
a	O
server	O
with	O
specifications	O
listed	O
in	O
Tab	O
.	O
8	O
.	O
C	O
Other	O
Details	O

The	O
full	O
table	O
of	O
supervised	O
baseline	O
models	O
is	O
provided	O
in	O
Tab	O
.	O
4	O
.	O

Tab	O
.	O
9	O
lists	O
hyper	O
-	O
parameter	O
search	O
space	O
for	O
obtaining	O
the	O
set	O
of	O
used	O
numbers	O
.	O
Note	O
that	O
the	O
chosen	O
hyper	O
-	O
parameters	O
yield	O
the	O
overall	O
best	O
performance	O
but	O
might	O
be	O
sub	O
-	O
optimal	O
on	O
any	O
single	O
dataset	O
.	O
Also	O
,	O
we	O
balanced	O
the	O
memory	O
limit	O
and	O
model	O
performance	O
.	O
C.3	O
A	O
High	O
-	O
Resolution	O
Version	O
of	O
Fig	O
.	O
1	O
We	O
show	O
a	O
clearer	O
version	O
of	O
t	O
-	O
SNE	O
embedding	O
visualisation	O
in	O
Fig	O
.	O
3	O
.	O

PUDMEDBERT	O
Figure	O
3	O
:	O
Same	O
as	O
Fig	O
.	O
1	O
in	O
the	O
main	O
text	O
,	O
but	O
generated	O
with	O
a	O
higher	O
resolution	O
.	O

Evaluating	O
Multiple	O
System	O
Summary	O
Lengths	O
:	O
A	O
Case	O
Study	O

We	O
would	O
like	O
to	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
constructive	O
comments	O
.	O
We	O
thank	O
Yinfei	O
Yang	O
for	O
his	O
assistance	O
in	O
producing	O
the	O
IC	O
-	O
SISumm	O
summaries	O
that	O
we	O
utilized	O
in	O
our	O
analysis	O
.	O
This	O
work	O
was	O
supported	O
in	O
part	O
by	O
grants	O
from	O
the	O
MAGNET	O
program	O
of	O
the	O
Israel	O
Innovation	O
Authority	O
;	O
by	O
the	O
German	O
Research	O
Foundation	O
through	O
the	O
German	O
-	O
Israeli	O
Project	O
Cooperation	O
(	O
DIP	O
,	O
grant	O
DA	O
1600/1	O
-	O
1	O
)	O
;	O
by	O
the	O
BIU	O
Center	O
for	O
Research	O
in	O
Applied	O
Cryptography	O
and	O
Cyber	O
Security	O
in	O
conjunction	O
with	O
the	O
Israel	O
National	O
Cyber	O
Bureau	O
in	O
the	O
Prime	O
Ministers	O
Office	O
;	O
and	O
by	O
the	O
Israel	O
Science	O
Foundation	O
(	O
grants	O
1157/16	O
and	O
1951/17	O
)	O
.	O

MovieChats	O
:	O
Chat	O
like	O
Humans	O
in	O
a	O
Closed	O
Domain	O

[	O
context	O
]	O
dialogue	O
context	O
ation	O
.	O
Unlike	O
in	O
traditional	O
task	O
-	O
oriented	O
systems	O
where	O
subtasks	O
are	O
decomposed	O
separately	O
,	O
we	O
opt	O
for	O
a	O
simple	O
and	O
unified	O
approach	O
by	O
casting	O
all	O
subtasks	O
into	O
sequence	O
prediction	O
.	O
A	O
special	O
token	O
is	O
injected	O
in	O
the	O
beginning	O
to	O
indicate	O
which	O
subtask	O
to	O
perform	O
(	O
Hosseini	O
-	O
Asl	O
et	O
al	O
,	O
2020	O
;	O
Peng	O
et	O
al	O
,	O
2020	O
)	O
.	O
Table	O
3	O
shows	O
the	O
schema	O
representation	O
for	O
different	O
components	O
.	O
The	O
condition	O
and	O
the	O
target	O
are	O
concatenated	O
into	O
a	O
single	O
sequence	O
and	O
then	O
fed	O
into	O
the	O
language	O
model	O
to	O
train	O
.	O
For	O
example	O
,	O
the	O
task	O
of	O
predicting	O
the	O
intent	O
given	O
the	O
dialogue	O
context	O
will	O
be	O
transformed	O
into	O
"	O

We	O
tokenize	O
the	O
Text	O
in	O
the	O
unit	O
of	O
Chinese	O
characters	O
and	O
keep	O
all	O
unique	O
non	O
-	O
Chinese	O
unique	O
tokens	O
appearing	O
for	O
more	O
than	O
5	O
times	O
.	O
The	O
whole	O
vocabulary	O
contains	O
13	O
,	O
317	O
words	O
.	O
We	O
train	O
our	O
model	O
on	O
24	O
Nvidia	O
V100	O
GPUs	O
(	O
32	O
GB	O
)	O
with	O
three	O
different	O
model	O
sizes	O
as	O
shown	O
in	O
PyTorch	O
(	O
Paszke	O
et	O
al	O
,	O
2019	O
)	O
.	O

A	O
:	O
威尔史密斯演技真的很棒	O
(	O
Will	O
Smith	O
's	O
acting	O
skill	O
is	O
really	O
good	O
)	O
.	O
B	O
:	O
他的当幸福来敲门太经典了	O
(	O
His	O
The	O
Pursuit	O
of	O
Happyness	O
is	O
a	O
classic	O
)	O
.	O
A	O
:	O
一直都挂在电影排行榜靠前的位置	O
(	O
That	O
'	O
always	O
among	O
top	O
ranked	O
movies	O
)	O
.	O
B	O
:	O
嗯嗯，这部电影真的很励志啊	O
(	O
Yes	O
,	O
it	O
's	O
really	O
motivational	O
)	O
.	O
A	O
:	O
威尔史密斯也演出了很惨的感觉了	O
(	O
Will	O
Smith	O
plays	O
like	O
he	O
is	O
a	O
real	O
tragedy	O
)	O
.	O
B	O
:	O
演技特别好	O
(	O
Yes	O
,	O
he	O
acts	O
pretty	O
well	O
)	O
.	O

Table	O
10	O
shows	O
examples	O
comparing	O
our	O
dataset	O
and	O
the	O
others	O
.	O
As	O
observed	O
,	O
forum	O
conversations	O
are	O
mostly	O
single	O
-	O
turn	O
QA	O
or	O
comments	O
.	O
Current	O
crowd	O
-	O
sourced	O
datasets	O
are	O
either	O
collected	O
on	O
constrained	O
scenarios	O
(	O
the	O
scenario	O
in	O
(	O
Zhou	O
et	O
al	O
,	O
2018b	O
)	O
fixed	O
the	O
roles	O
in	O
a	O
conversation	O
as	O
one	O
introducer	O
and	O
one	O
listener	O
)	O
,	O
or	O
unconstrained	O
but	O
prompting	O
people	O
to	O
deliberately	O
connect	O
knowledge	O
.	O
Our	O
dataset	O
simulates	O
real	O
-	O
life	O
conversations	O
to	O
the	O
largest	O
extent	O
.	O
We	O
classify	O
the	O
utterances	O
into	O
one	O
of	O
15	O
aspects	O
.	O

As	O
for	O
the	O
four	O
human	O
evaluation	O
metrics	O
.	O
The	O
first	O
two	O
will	O
focus	O
only	O
on	O
the	O
conversational	O
backbones	O
without	O
considering	O
domain	O
knowledge	O
.	O
The	O
second	O
two	O
will	O
check	O
if	O
the	O
responses	O
can	O
provide	O
informative	O
and	O
correct	O
responses	O
powered	O
by	O
domain	O
knowledge	O
.	O
The	O
detailed	O
definitions	O
of	O
them	O
are	O
:	O

All	O
the	O
three	O
metrics	O
are	O
evaluated	O
by	O
three	O
crow	O
-	O
workers	O
each	O
except	O
for	O
factuality	O
.	O
As	O
evaluating	O
factuality	O
requires	O
in	O
-	O
depth	O
knowledge	O
about	O
one	O
movie	O
,	O
it	O
is	O
quite	O
difficult	O
for	O
random	O
human	O
evaluators	O
to	O
judge	O
them	O
.	O
Even	O
if	O
we	O
filter	O
to	O
only	O
keep	O
people	O
who	O
have	O
watched	O
a	O
movie	O
,	O
it	O
is	O
hard	O
to	O
guarantee	O
they	O
can	O
recall	O
all	O
the	O
scenes	O
in	O
the	O
movie	O
.	O
Therefore	O
,	O
the	O
factuality	O
check	O
is	O
only	O
done	O
by	O
the	O
person	O
who	O
performed	O
this	O
dialogue	O
.	O
In	O
the	O
static	O
evaluation	O
,	O
it	O
is	O
evaluated	O
by	O
the	O
annotator	O
who	O
produced	O
the	O
reference	O
response	O
.	O
In	O
the	O
interactive	O
evaluation	O
,	O
it	O
is	O
evaluated	O
by	O
the	O
person	O
who	O
chat	O
with	O
the	O
bot	O
.	O
It	O
is	O
nevertheless	O
not	O
accurate	O
though	O
.	O
However	O
,	O
if	O
the	O
bot	O
can	O
cheat	O
the	O
human	O
into	O
believing	O
its	O
false	O
information	O
,	O
it	O
can	O
also	O
somehow	O
be	O
considered	O
a	O
"	O
success	O
"	O
.	O
We	O
provide	O
examples	O
for	O
guiding	O
the	O
human	O
evaluators	O
in	O
Table	O
12	O
.	O
Table	O
13	O
shows	O
some	O
interactive	O
examples	O
with	O
humans	O
.	O
We	O
observe	O
Mitsuku	O
can	O
XiaoIce	O
perform	O
decently	O
in	O
single	O
-	O
turn	O
exchanges	O
but	O
strongly	O
struggle	O
at	O
understanding	O
multi	O
-	O
turn	O
user	O
intents	O
.	O
Most	O
conversations	O
stop	O
at	O
turn	O
4	O
and	O
will	O
not	O
move	O
on	O
.	O

Determining	O
a	O
Person	O
's	O
Suicide	O
Risk	O
by	O
Voting	O
on	O
the	O
Short	O
-	O
Term	O
History	O
of	O
Tweets	O
for	O
the	O
CLPsych	O
2021	O
Shared	O
Task	O

In	O
this	O
shared	O
task	O
,	O
we	O
accept	O
the	O
challenge	O
of	O
constructing	O
models	O
to	O
identify	O
Twitter	O
users	O
who	O
attempted	O
suicide	O
based	O
on	O
their	O
tweets	O
30	O
and	O
182	O
days	O
before	O
the	O
adverse	O
event	O
's	O
occurrence	O
.	O
We	O
explore	O
multiple	O
machine	O
learning	O
and	O
deep	O
learning	O
methods	O
to	O
identify	O
a	O
person	O
's	O
suicide	O
risk	O
based	O
on	O
the	O
short	O
-	O
term	O
history	O
of	O
their	O
tweets	O
.	O
Taking	O
the	O
real	O
-	O
life	O
applicability	O
of	O
the	O
model	O
into	O
account	O
,	O
we	O
make	O
the	O
design	O
choice	O
of	O
classifying	O
on	O
the	O
tweet	O
level	O
.	O
By	O
voting	O
the	O
tweet	O
-	O
level	O
suicide	O
risk	O
scores	O
through	O
an	O
ensemble	O
of	O
classifiers	O
,	O
we	O
predict	O
the	O
suicidal	O
users	O
30	O
-	O
days	O
before	O
the	O
event	O
with	O
an	O
81.8	O
%	O
true	O
-	O
positives	O
rate	O
.	O
Meanwhile	O
,	O
the	O
tweet	O
-	O
level	O
voting	O
falls	O
short	O
on	O
the	O
six	O
-	O
month	O
-	O
long	O
data	O
as	O
the	O
number	O
of	O
tweets	O
with	O
weak	O
suicidal	O
ideation	O
levels	O
weakens	O
the	O
overall	O
suicidal	O
signals	O
in	O
the	O
long	O
term	O
.	O

Pre	O
-	O
processing	O
:	O
We	O
clean	O
the	O
tweets	O
by	O
removing	O
user	O
mentions	O
,	O
URLs	O
,	O
punctuation	O
,	O
and	O
non	O
-	O
ASCII	O
characters	O
,	O
then	O
normalize	O
hashtags	O
into	O
words	O
using	O
a	O
probabilistic	O
splitting	O
tool	O
based	O
on	O
English	O
Wikipedia	O
unigram	O
frequencies	O
(	O
Anderson	O
,	O
2019	O
)	O
.	O
We	O
maintain	O
stopwords	O
and	O
emojis	O
,	O
as	O
they	O
might	O
provide	O
clues	O
regarding	O
the	O
suicidal	O
ideation	O
of	O
the	O
users	O
.	O
Experimentation	O
Framework	O
:	O
Before	O
designing	O
the	O
experiments	O
,	O
we	O
face	O
a	O
critical	O
choice	O
:	O
Should	O
we	O
merge	O
all	O
tweets	O
per	O
user	O
,	O
or	O
should	O
we	O
perform	O
the	O
assessment	O
per	O
tweet	O
and	O
then	O
aggregate	O
the	O
scores	O
?	O
To	O
answer	O
this	O
,	O
we	O
consider	O
a	O
real	O
-	O
life	O
risk	O
assessment	O
system	O
.	O
The	O
system	O
should	O
provide	O
a	O
score	O
every	O
time	O
someone	O
posts	O
a	O
tweet	O
.	O
Some	O
social	O
media	O
domains	O
already	O
implement	O
these	O
systems	O
(	O
Ji	O
et	O
al	O
,	O
2020	O
)	O
.	O
Hence	O
,	O
we	O
select	O
to	O
train	O
the	O
models	O
to	O
classify	O
tweets	O
,	O
then	O
apply	O
majority	O
voting	O
(	O
MV	O
)	O
per	O
user	O
to	O
compute	O
a	O
risk	O
score	O
based	O
on	O
the	O
tweet	O
scores	O
.	O
Our	O
framework	O
is	O
described	O
in	O
Figure	O
1	O
.	O

Secure	O
access	O
to	O
the	O
shared	O
task	O
dataset	O
was	O
provided	O
with	O
IRB	O
approval	O
under	O
University	O
of	O
Maryland	O
,	O
College	O
Park	O
protocol	O
1642625	O
.	O

The	O
organizers	O
are	O
particularly	O
grateful	O
to	O
the	O
users	O
who	O
donated	O
data	O
to	O
the	O
OurDataHelps	O
project	O
without	O
whom	O
this	O
work	O
would	O
not	O
be	O
possible	O
,	O
to	O
Qntfy	O
for	O
supporting	O
the	O
OurDataHelps	O
project	O
and	O
making	O
the	O
data	O
available	O
,	O
to	O
NORC	O
for	O
creating	O
and	O
administering	O
the	O
secure	O
infrastructure	O
,	O
and	O
to	O
Amazon	O
for	O
supporting	O
this	O
research	O
with	O
computational	O
resources	O
on	O
AWS	O
.	O
The	O
authors	O
are	O
thankful	O
to	O
the	O
anonymous	O
reviewers	O
for	O
their	O
constructive	O
comments	O
and	O
valuable	O
suggestions	O
.	O

Welcome	O
to	O
ACL	O
2017	O
in	O
Vancouver	O
,	O
Canada	O
!	O
This	O
is	O
the	O
55th	O
annual	O
meeting	O
of	O
the	O
Association	O
for	O
Computational	O
Linguistics	O
.	O
A	O
tremendous	O
amount	O
of	O
knowledge	O
has	O
been	O
presented	O
at	O
more	O
than	O
half	O
a	O
century	O
's	O
worth	O
of	O
our	O
conferences	O
.	O
Hopefully	O
,	O
some	O
of	O
it	O
is	O
still	O
relevant	O
now	O
that	O
deep	O
learning	O
has	O
solved	O
language	O
.	O
We	O
are	O
anticipating	O
one	O
of	O
the	O
largest	O
ACL	O
conferences	O
ever	O
.	O
We	O
had	O
a	O
record	O
number	O
of	O
papers	O
submitted	O
to	O
the	O
conference	O
,	O
and	O
a	O
record	O
number	O
of	O
industry	O
partners	O
joining	O
us	O
as	O
sponsors	O
of	O
the	O
conference	O
.	O
We	O
are	O
on	O
track	O
to	O
be	O
one	O
of	O
the	O
best	O
attended	O
ACL	O
conferences	O
to	O
date	O
.	O
I	O
hope	O
that	O
this	O
year	O
's	O
conference	O
is	O
intellectually	O
stimulating	O
and	O
that	O
you	O
take	O
home	O
many	O
new	O
ideas	O
and	O
techniques	O
that	O
will	O
help	O
extend	O
your	O
own	O
research	O
.	O

Revisiting	O
Pretraining	O
with	O
Adapters	O

Details	O
of	O
hyperparameter	O
setting	O
including	O
the	O
learning	O
rates	O
for	O
the	O
best	O
performing	O
results	O
are	O
provided	O
in	O
Table	O
4	O
,	O
5	O
,	O
and	O
6	O
.	O

We	O
present	O
validation	O
performance	O
in	O
Table	O
7	O
and	O
Figure	O
3	O
and	O
8	O
.	O

We	O
would	O
like	O
to	O
thank	O
Zsolt	O
Kira	O
,	O
Mandeep	O
Baines	O
,	O
Shruti	O
Bhosale	O
,	O
and	O
Siddharth	O
Goyal	O
for	O
helpful	O
feedback	O
and	O
suggestions	O
.	O
We	O
also	O
would	O
like	O
to	O
thank	O
anonymous	O
reviewers	O
for	O
their	O
insightful	O
comments	O
on	O
the	O
earlier	O
version	O
of	O
the	O
paper	O
.	O

The	O
work	O
presented	O
in	O
this	O
paper	O
has	O
been	O
funded	O
by	O
the	O
Slovenian	O
Research	O
Agency	O
national	O
basic	O
research	O
project	O
"	O
Resources	O
,	O
methods	O
and	O
tools	O
for	O
the	O
understanding	O
,	O
identification	O
and	O
classification	O
of	O
various	O
forms	O
of	O
socially	O
unacceptable	O
discourse	O
in	O
the	O
information	O
society	O
"	O
(	O
ARRS	O
J7	O
-	O
8280	O
,	O
2017	O
-	O
2020	O
)	O
,	O
and	O
by	O
the	O
Slovenian	O
research	O
infrastructure	O
CLARIN.SI	O
.	O

This	O
work	O
is	O
part	O
of	O
the	O
FoTran	O
project	O
,	O
funded	O
by	O
the	O
European	O
Research	O
Council	O
(	O
ERC	O
)	O
under	O
the	O
European	O
Union	O
's	O
Horizon	O
2020	O
research	O
and	O
innovation	O
programme	O
(	O
grant	O
agreement	O
No	O
771113	O
)	O
.	O
The	O
authors	O
gratefully	O
acknowledge	O
the	O
support	O
of	O
the	O
CSC	O
-	O
IT	O
Center	O
for	O
Science	O
,	O
Finland	O
,	O
for	O
computational	O
resources	O
.	O
Finally	O
,	O
We	O
would	O
also	O
like	O
to	O
acknowledge	O
NVIDIA	O
and	O
their	O
GPU	O
grant	O
.	O

Team	O
JARS	O
:	O
DialDoc	O
Subtask	O
1	O
-	O
Improved	O
Knowledge	O
Identification	O
with	O
Supervised	O
Out	O
-	O
of	O
-	O
Domain	O
Pretraining	O

As	O
discussed	O
earlier	O
,	O
subtask	O
1	O
of	O
DialDoc	O
shared	O
task	O
is	O
formulated	O
as	O
a	O
span	O
selection	O
problem	O
.	O
Therefore	O
,	O
in	O
order	O
to	O
learn	O
to	O
predict	O
the	O
correct	O
span	O
,	O
we	O
use	O
an	O
extractive	O
question	O
-	O
answering	O
setup	O
.	O

In	O
this	O
section	O
,	O
we	O
discuss	O
our	O
experimental	O
setup	O
in	O
detail	O
.	O

We	O
use	O
default	O
parameters	O
set	O
by	O
the	O
subtask	O
baseline	O
provided	O
by	O
the	O
authors	O
.	O
4	O
However	O
,	O
we	O
reduce	O
the	O
training	O
per	O
-	O
device	O
batch	O
-	O
size	O
to	O
2	O
to	O
accommodate	O
the	O
large	O
models	O
on	O
an	O
Nvidia	O
Geforce	O
GTX	O
1080	O
Ti	O
12	O
GB	O
GPU	O
.	O
We	O
stop	O
the	O
continual	O
out	O
-	O
ofdomain	O
supervised	O
pretraining	O
after	O
2	O
epochs	O
.	O

We	O
now	O
present	O
the	O
results	O
for	O
different	O
experimental	O
setups	O
we	O
tried	O
for	O
DialDoc	O
subtask	O
1	O
.	O

The	O
final	O
dataset	O
contains	O
7	O
,	O
953	O
annotated	O
tweets	O
for	O
"	O
Donald	O
Trump	O
"	O
,	O
7	O
,	O
296	O
for	O
"	O
Joe	O
Biden	O
"	O
and	O
6	O
,	O
325	O
for	O
"	O
Bernie	O
Sanders	O
"	O
,	O
respectively	O
.	O
The	O
label	O
distribution	O
of	O
each	O
target	O
is	O
shown	O
in	O
Table	O
6	O
.	O
Each	O
tweet	O
is	O
annotated	O
with	O
a	O
stance	O
label	O
"	O
Favor	O
"	O
or	O
"	O
Against	O
"	O
.	O
We	O
created	O
the	O
training	O
,	O
validation	O
and	O
testing	O
sets	O
following	O
an	O
80/10/10	O
split	O
.	O
We	O
note	O
that	O
P	O
-	O
STANCE	O
is	O
more	O
than	O
3	O
times	O
larger	O
than	O
the	O
previous	O
benchmark	O
(	O
Mohammad	O
et	O
al	O
,	O
2016a	O
)	O
.	O

We	O
thank	O
the	O
National	O
Science	O
Foundation	O
and	O
Amazon	O
Web	O
Services	O
for	O
support	O
from	O
grants	O
IIS	O
-	O
1912887	O
and	O
IIS	O
-	O
1903963	O
which	O
supported	O
the	O
research	O
and	O
the	O
computation	O
in	O
this	O
study	O
.	O
We	O
also	O
thank	O
our	O
reviewers	O
for	O
their	O
insightful	O
comments	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
three	O
methods	O
for	O
generating	O
pseudo	O
data	O
.	O
In	O
Section	O
4	O
,	O
we	O
experimentally	O
compare	O
these	O
methods	O
.	O

The	O
goal	O
of	O
our	O
experiments	O
is	O
to	O
investigate	O
aspect	O
(	O
i	O
)	O
-	O
(	O
iii	O
)	O
introduced	O
in	O
Section	O
2	O
.	O
To	O
ensure	O
that	O
the	O
experimental	O
findings	O
are	O
applicable	O
to	O
GEC	O
in	O
general	O
,	O
we	O
design	O
our	O
experiments	O
by	O
using	O
the	O
following	O
two	O
strategies	O
:	O
(	O
i	O
)	O
we	O
use	O
an	O
off	O
-	O
the	O
-	O
shelf	O
EncDec	O
model	O
without	O
any	O
task	O
-	O
specific	O
architecture	O
or	O
techniques	O
;	O
(	O
ii	O
)	O
we	O
conduct	O
hyper	O
-	O
parameter	O
tuning	O
,	O
evaluation	O
and	O
comparison	O
of	O
each	O
method	O
or	O
setting	O
on	O
the	O
validation	O
set	O
.	O
At	O
the	O
end	O
of	O
experiments	O
(	O
Section	O
4.5	O
)	O
,	O
we	O
summarize	O
our	O
findings	O
and	O
propose	O
suitable	O
settings	O
.	O
We	O
then	O
perform	O
a	O
single	O
-	O
shot	O
evaluation	O
of	O
their	O
performance	O
on	O
the	O
test	O
set	O
.	O

We	O
investigate	O
the	O
effectiveness	O
of	O
the	O
seed	O
corpus	O
T	O
for	O
generating	O
pseudo	O
data	O
D	O
p	O
.	O
The	O
three	O
corpora	O
(	O
Wikipedia	O
,	O
SimpleWiki	O
and	O
Gigaword	O
)	O
are	O
compared	O
in	O
Table	O
3	O
.	O
We	O
set	O
|	O
D	O
p	O
|	O
=	O
1.4M.	O
The	O
difference	O
in	O
F	O
0.5	O
is	O
small	O
,	O
which	O
implies	O
that	O
the	O
seed	O
corpus	O
T	O
has	O
only	O
a	O
minor	O
effect	O
on	O
the	O
model	O
performance	O
.	O
Nevertheless	O
,	O
Gigaword	O
consistently	O
outperforms	O
the	O
other	O
two	O
corpora	O
.	O
In	O
particular	O
,	O
DIRECTNOISE	O
with	O
Gigaword	O
achieves	O
the	O
best	O
value	O
of	O
F	O
0.5	O
among	O
all	O
the	O
configurations	O
.	O

We	O
compare	O
the	O
JOINT	O
and	O
PRETRAIN	O
optimization	O
settings	O
.	O
We	O
are	O
interested	O
in	O
how	O
each	O
setting	O
performs	O
when	O
the	O
scale	O
of	O
the	O
pseudo	O
data	O
D	O
p	O
compared	O
with	O
that	O
of	O
the	O
genuine	O
parallel	O
data	O
D	O
g	O
is	O
(	O
i	O
)	O
approximately	O
the	O
same	O
(	O
|	O
D	O
p	O
|	O
=	O
1.4	O
M	O
)	O
and	O
(	O
ii	O
)	O
substantially	O
bigger	O
(	O
|	O
D	O
p	O
|	O
=	O
14	O
M	O
)	O
.	O
Here	O
,	O
we	O
use	O
Wikipedia	O
as	O
the	O
seed	O
corpus	O
T	O
instead	O
of	O
SimpleWiki	O
or	O
Gigaword	O
for	O
two	O
reasons	O
.	O
First	O
,	O
SimpleWiki	O
is	O
too	O
small	O
for	O
the	O
experiment	O
(	O
b	O
)	O
(	O
see	O
Table	O
1	O
)	O
.	O
Second	O
,	O
the	O
fact	O
that	O
Gigaword	O
is	O
not	O
freely	O
available	O
makes	O
it	O
difficult	O
for	O
other	O
researchers	O
to	O
replicate	O
our	O
results	O
.	O
(	O
a	O
)	O
Joint	O
Training	O
or	O
Pretraining	O
Table	O
4	O
presents	O
the	O
results	O
.	O
The	O
most	O
notable	O
result	O
here	O
is	O
that	O
PRETRAIN	O
demonstrates	O
the	O
properties	O
of	O
more	O
pseudo	O
data	O
and	O
better	O
performance	O
,	O
whereas	O
JOINT	O
does	O
not	O
.	O
For	O
example	O
,	O
in	O
BACKTRANS	O
(	O
NOISY	O
)	O
,	O
increasing	O
|	O
D	O
p	O
|	O
(	O
1.4	O
M	O
14	O
M	O
)	O
improves	O
F	O
0.5	O
on	O
PRETRAIN	O
(	O
41.1	O
44.5	O
)	O
.	O
By	O
contrast	O
,	O
F	O
0.5	O
does	O
not	O
improve	O
on	O
JOINT	O
(	O
40.4	O
40.3	O
)	O
.	O
An	O
intuitive	O
explanation	O
for	O
this	O
case	O
is	O
that	O
when	O
pseudo	O
data	O
D	O
p	O
are	O
substantially	O
more	O
than	O
genuine	O
data	O
D	O
g	O
,	O
the	O
teaching	O
signal	O
from	O
D	O
p	O
becomes	O
dominant	O
in	O
JOINT	O
.	O
PRETRAIN	O
alleviates	O
this	O
problem	O
because	O
the	O
model	O
is	O
trained	O
with	O
only	O
D	O
g	O
during	O
fine	O
-	O
tuning	O
.	O
We	O
therefore	O
suppose	O
that	O
PRETRAIN	O
is	O
crucial	O
for	O
utilizing	O
extensive	O
pseudo	O
data	O
.	O
F	O
0.5	O
=	O
45.9	O
.	O

In	O
this	O
study	O
,	O
we	O
investigated	O
several	O
aspects	O
of	O
incorporating	O
pseudo	O
data	O
for	O
GEC	O
.	O
Through	O
extensive	O
experiments	O
,	O
we	O
found	O
the	O
following	O
to	O
be	O
effective	O
:	O
(	O
i	O
)	O
utilizing	O
Gigaword	O
as	O
the	O
seed	O
corpus	O
,	O
and	O
(	O
ii	O
)	O
pretraining	O
the	O
model	O
with	O
BACKTRANS	O
(	O
NOISY	O
)	O
data	O
.	O
Based	O
on	O
these	O
findings	O
,	O
we	O
proposed	O
suitable	O
settings	O
for	O
GEC	O
.	O
We	O
demonstrated	O
the	O
effectiveness	O
of	O
our	O
proposal	O
by	O
achieving	O
stateof	O
-	O
the	O
-	O
art	O
performance	O
on	O
the	O
CoNLL	O
-	O
2014	O
test	O
set	O
and	O
the	O
BEA	O
-	O
2019	O
test	O
set	O
.	O

We	O
thank	O
the	O
three	O
anonymous	O
reviewers	O
for	O
their	O
insightful	O
comments	O
.	O
We	O
are	O
deeply	O
grateful	O
to	O
Takumi	O
Ito	O
and	O
Tatsuki	O
Kuribayashi	O
for	O
kindly	O
sharing	O
the	O
re	O
-	O
implementation	O
of	O
BACKTRANS	O
(	O
NOISY	O
)	O
.	O
The	O
work	O
of	O
Jun	O
Suzuki	O
was	O
supported	O
in	O
part	O
by	O
JSPS	O
KAKENHI	O
Grant	O
Number	O
JP19104418	O
and	O
AIRPF	O
Grant	O
Number	O
30AI036	O
-	O
8	O
.	O

The	O
author	O
would	O
like	O
to	O
thank	O
his	O
manager	O
for	O
supporting	O
this	O
project	O
,	O
and	O
the	O
anonymous	O
reviewers	O
for	O
their	O
thoughtful	O
comments	O
which	O
helped	O
improve	O
the	O
presentation	O
of	O
this	O
work	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
the	O
details	O
of	O
our	O
method	O
,	O
which	O
is	O
illustrated	O
in	O
Figure	O
1	O
.	O
We	O
feed	O
different	O
word	O
embedding	O
sets	O
into	O
neural	O
networks	O
and	O
train	O
these	O
neural	O
networks	O
separately	O
.	O
When	O
predicting	O
the	O
labels	O
of	O
tweets	O
in	O
testing	O
set	O
,	O
we	O
sum	O
label	O
probabilities	O
of	O
all	O
neural	O
network	O
to	O
make	O
final	O
decisions	O
.	O

We	O
learn	O
different	O
RCNN	O
models	O
with	O
different	O
embedding	O
sets	O
as	O
input	O
.	O
Formally	O
,	O
we	O
have	O
s	O
embedding	O
sets	O
which	O
are	O
denoted	O
as	O
{	O
E	O
1	O
,	O
E	O
2	O
,	O
,	O
E	O
s	O
}	O
,	O
p	O
1	O
=	O
RCNN	O
1	O
(	O
{	O
x	O
i	O
}	O
l	O
i=1	O
,	O
E	O
1	O
)	O
,	O
p	O
2	O
=	O
RCNN	O
2	O
(	O
{	O
x	O
i	O
}	O
l	O
i=1	O
,	O
E	O
2	O
)	O
,	O
.	O
.	O
.	O
,	O
p	O
s	O
=	O
RCNN	O
s	O
(	O
{	O
x	O
i	O
}	O
l	O
i=1	O
,	O
E	O
s	O
)	O
,	O
p	O
=	O
1≤i≤s	O
p	O
i	O
.	O
y	O
=	O
argmax	O
1≤i≤	O
|	O
Y	O
|	O
p	O
i	O
,	O
where	O
y	O
is	O
the	O
predicted	O
label	O
.	O

In	O
this	O
section	O
,	O
we	O
analyze	O
the	O
incorrect	O
predictions	O
of	O
our	O
system	O
in	O
SemEval	O
2017	O
.	O
We	O
summarize	O
four	O
kinds	O
of	O
errors	O
in	O
our	O
system	O
.	O
The	O
first	O
one	O
is	O
that	O
some	O
decisions	O
need	O
domain	O
knowledge	O
,	O
which	O
our	O
method	O
only	O
can	O
learn	O
from	O
the	O
labeled	O
datasets	O
.	O
The	O
instances	O
are	O
as	O
follows	O
:	O
Messi	O
's	O
100	O
international	O
goals	O
for	O
Barcelona	O
#	O
fcblive	O
https://t.co/fMkglvusL1	O
[	O
via	O
@thereisagenius	O
]	O
.	O
Predicted	O
label	O
:	O
neutral	O
,	O
golden	O
label	O
:	O
positive	O
#	O
Trudeau	O
gives	O
your	O
cash	O
to	O
#	O
Terrorist	O
#	O
Hamas	O
-	O
influenced	O
group	O
-	O
#	O
UNRWA	O
-	O
@Can	O
-	O
diceMalcolm	O
https://t.co/5i5o2qwRWl	O
Predicted	O
label	O
:	O
neutral	O
,	O
golden	O
label	O
:	O
negative	O
Messis	O
9	O
goals	O
in	O
CL	O
are	O
more	O
than	O
20	O
of	O
the	O
32	O
teams	O
in	O
the	O
competition	O
have	O
scored	O
in	O
total	O
,	O
and	O
he	O
s	O
tied	O
with	O
five	O
other	O
sides	O
#	O
fcblive	O
Predicted	O
label	O
:	O
neutral	O
,	O
golden	O
label	O
:	O
positive	O
The	O
second	O
one	O
is	O
emoticons	O
in	O
tweet	O
,	O
as	O
most	O
of	O
word	O
embedding	O
sets	O
do	O
not	O
include	O
emoticon	O
embeddings	O
and	O
emoticons	O
are	O
always	O
with	O
senti	O
-	O
ments	O
.	O
The	O
instances	O
are	O
described	O
in	O
Figure	O
2	O
The	O
third	O
one	O
is	O
that	O
sentiments	O
are	O
not	O
consistent	O
in	O
sentences	O
.	O
For	O
example	O
,	O
the	O
first	O
half	O
part	O
is	O
positive	O
,	O
while	O
the	O
second	O
half	O
part	O
is	O
negative	O
,	O
in	O
this	O
case	O
,	O
our	O
system	O
would	O
predict	O
'	O
positive	O
'	O
or	O
'	O
negative	O
'	O
,	O
the	O
golden	O
label	O
is	O
neutral	O
.	O
@jimmyfallon	O
1	O
.	O
Emily	O
2	O
.	O
Michel	O
3	O
.	O
Kirk	O
4	O
.	O
TJ	O
.	O
Love	O
the	O
quirky	O
ones	O
and	O
Emily	O
coz	O
she	O
's	O
such	O
a	O
BIATCH	O
!	O
#	O
gilmoregirlstop4	O
.	O
predicted	O
label	O
positive	O
,	O
golden	O
label	O
:	O
neutral	O
The	O
fourth	O
one	O
is	O
the	O
sarcasm	O
,	O
such	O
as	O
:	O
#	O
Hamas	O
leader	O
:	O
#	O
Trump	O
may	O
be	O
a	O
#	O
Jew	O
https://t.co/jGFZTvj2pF.	O
predicted	O
label	O
positive	O
,	O
golden	O
label	O
:	O
negative	O

We	O
propose	O
a	O
simple	O
and	O
effective	O
ensemble	O
method	O
to	O
boost	O
the	O
neural	O
twitter	O
sentiment	O
classification	O
.	O
By	O
using	O
different	O
embedding	O
sets	O
,	O
the	O
system	O
can	O
cover	O
more	O
words	O
and	O
encode	O
more	O
sentiment	O
information	O
.	O
The	O
results	O
on	O
datasets	O
of	O
previous	O
SemEval	O
and	O
SemEval	O
2017	O
show	O
the	O
effectiveness	O
of	O
our	O
method	O
.	O
Moreover	O
,	O
error	O
analysis	O
is	O
conducted	O
to	O
propose	O
the	O
main	O
challenges	O
for	O
our	O
method	O
.	O
We	O
release	O
our	O
code	O
for	O
system	O
duplicability	O
.	O

Machine	O
Comprehension	O
Improves	O
Domain	O
-	O
Specific	O
Japanese	O
Predicate	O
-	O
Argument	O
Structure	O
Analysis	O

We	O
construct	O
PAS	O
-	O
QA	O
and	O
RC	O
-	O
QA	O
datasets	O
in	O
the	O
driving	O
domain	O
.	O
Both	O
the	O
QA	O
datasets	O
consist	O
of	O
triplets	O
of	O
a	O
document	O
,	O
a	O
question	O
and	O
its	O
answer	O
as	O
in	O
existing	O
RC	O
-	O
QA	O
datasets	O
.	O
We	O
employ	O
crowdsourcing	O
to	O
create	O
large	O
-	O
scale	O
datasets	O
in	O
a	O
short	O
time	O
.	O
Figure	O
1	O
and	O
Figure	O
2	O
show	O
examples	O
of	O
our	O
PAS	O
-	O
QA	O
and	O
RC	O
-	O
QA	O
datasets	O
.	O

We	O
construct	O
a	O
PAS	O
-	O
QA	O
dataset	O
in	O
which	O
a	O
question	O
asks	O
an	O
omitted	O
argument	O
for	O
a	O
predicate	O
.	O
We	O
focus	O
on	O
the	O
ga	O
case	O
(	O
nominative	O
)	O
,	O
the	O
wo	O
case	O
(	O
accusative	O
)	O
,	O
and	O
the	O
ni	O
case	O
(	O
dative	O
)	O
,	O
which	O
are	O
targeted	O
in	O
the	O
Japanese	O
PAS	O
analysis	O
literature	O
(	O
Shibata	O
et	O
al	O
,	O
2016	O
;	O
Shibata	O
and	O
Kurohashi	O
,	O
2018	O
;	O
Kurita	O
et	O
al	O
,	O
2018	O
;	O
Ouchi	O
et	O
al	O
,	O
2017	O
)	O
.	O
As	O
a	O
source	O
corpus	O
,	O
we	O
use	O
blog	O
articles	O
included	O
in	O
the	O
Driving	O
Experience	O
Corpus	O
(	O
Iwai	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
first	O
detect	O
a	O
predicate	O
that	O
has	O
an	O
omitted	O
argument	O
of	O
either	O
of	O
the	O
target	O
three	O
cases	O
by	O
applying	O
the	O
existing	O
PAS	O
analyzer	O
KNP	O
1	O
to	O
the	O
corpus	O
.	O
KNP	O
tends	O
to	O
overgenerate	O
such	O
predicates	O
,	O
but	O
most	O
erroneous	O
ones	O
are	O
filtered	O
out	O
by	O
the	O
following	O
crowdsourcing	O
step	O
.	O
We	O
extract	O
the	O
sentence	O
that	O
contains	O
the	O
predicate	O
and	O
preceding	O
three	O
sentences	O
as	O
a	O
document	O
.	O
Then	O
,	O
we	O
automatically	O
generate	O
a	O
question	O
using	O
the	O
following	O
template	O
for	O
nominative	O
.	O
"	O
author	O
,	O
"	O
"	O
other	O
,	O
"	O
and	O
"	O
not	O
sure	O
.	O
"	O
The	O
details	O
of	O
this	O
procedure	O
are	O
described	O
in	O
the	O
appendix	O
.	O
We	O
generated	O
questions	O
from	O
2	O
,	O
146	O
blog	O
articles	O
.	O
We	O
asked	O
five	O
crowdworkers	O
per	O
question	O
using	O
Yahoo	O
!	O
crowdsourcing	O
2	O
.	O
We	O
adopted	O
triplets	O
with	O
three	O
or	O
more	O
votes	O
if	O
they	O
are	O
not	O
"	O
not	O
sure	O
.	O
"	O
For	O
accusative	O
and	O
dative	O
PAS	O
-	O
QA	O
questions	O
,	O
we	O
adopted	O
triplets	O
if	O
they	O
are	O
"	O
other	O
.	O
"	O
In	O
this	O
case	O
,	O
there	O
is	O
not	O
any	O
antecedent	O
of	O
a	O
zero	O
pronoun	O
in	O
a	O
document	O
,	O
and	O
the	O
answer	O
is	O
"	O
NULL	O
.	O
"	O
For	O
nominative	O
PAS	O
-	O
QA	O
questions	O
,	O
we	O
did	O
not	O
adopt	O
triplets	O
if	O
they	O
are	O
"	O
other	O
"	O
because	O
a	O
nominative	O
always	O
exists	O
as	O
a	O
noun	O
in	O
a	O
document	O
or	O
"	O
author	O
.	O
"	O
In	O
addition	O
,	O
because	O
"	O
author	O
"	O
is	O
not	O
explicitly	O
expressed	O
in	O
the	O
document	O
,	O
we	O
add	O
a	O
sentence	O
"	O
"	O
(	O
The	O
author	O
wrote	O
the	O
following	O
document	O
.	O
)	O
to	O
the	O
beginning	O
of	O
the	O
document	O
to	O
deal	O
with	O
"	O
author	O
"	O
in	O
MC	O
models	O
.	O
We	O
record	O
the	O
answers	O
as	O
spans	O
in	O
a	O
document	O
or	O
NULL	O
.	O
We	O
randomly	O
extracted	O
100	O
questions	O
for	O
each	O
case	O
from	O
the	O
PAS	O
-	O
QA	O
dataset	O
and	O
judged	O
whether	O
we	O
can	O
answer	O
them	O
.	O
As	O
a	O
result	O
,	O
97	O
%	O
nominative	O
,	O
87	O
%	O
accusative	O
and	O
68	O
%	O
dative	O
questions	O
were	O
answerable	O
.	O
For	O
accusative	O
and	O
dative	O
,	O
we	O
checked	O
all	O
the	O
questions	O
and	O
chose	O
answerable	O
ones	O
.	O
Finally	O
,	O
we	O
created	O
12	O
,	O
468	O
nominative	O
,	O
3	O
,	O
151	O
accusative	O
and	O
1	O
,	O
069	O
dative	O
triplets	O
including	O
476	O
accusative	O
and	O
126	O
dative	O
questions	O
whose	O
answers	O
are	O
NULL	O
.	O
It	O
took	O
approximately	O
32	O
hours	O
and	O
approximately	O
210	O
,	O
000	O
JPY	O
to	O
create	O
this	O
dataset	O
.	O

We	O

"	O
(	O
the	O
slope	O
)	O
,	O
looked	O
correct	O
although	O
"	O
"	O
(	O
the	O
slope	O
)	O
was	O
the	O
only	O
answer	O
from	O
crowdsourcing	O
.	O
Supplying	O
multiple	O
answers	O
considering	O
coreference	O
relations	O
is	O
our	O
future	O
work	O
.	O
From	O
these	O
results	O
,	O
we	O
think	O
that	O
it	O
is	O
important	O
to	O
use	O
an	O
RC	O
-	O
QA	O
dataset	O
to	O
acquire	O
domain	O
knowledge	O
,	O
and	O
suggest	O
that	O
it	O
is	O
better	O
to	O
construct	O
both	O
PAS	O
-	O
QA	O
and	O
RC	O
-	O
QA	O
datasets	O
to	O
develop	O
a	O
PAS	O
analyzer	O
for	O
a	O
new	O
domain	O
.	O

LightningDOT	O
:	O
Pre	O
-	O
training	O
Visual	O
-	O
Semantic	O
Embeddings	O
for	O
Real	O
-	O
Time	O
Image	O
-	O
Text	O
Retrieval	O

In	O
this	O
section	O
,	O
we	O
present	O
the	O
proposed	O
Light	O
-	O
ningDOT	O
framework	O
,	O
which	O
consists	O
of	O
two	O
deep	O
Transformers	O
as	O
image	O
and	O
language	O
encoders	O
.	O
We	O
first	O
introduce	O
three	O
tasks	O
designed	O
to	O
pre	O
-	O
train	O
the	O
model	O
,	O
then	O
present	O
our	O
inference	O
pipeline	O
from	O
offline	O
feature	O
extraction	O
to	O
online	O
instant	O
retrieval	O
.	O

This	O
section	O
discusses	O
our	O
experiments	O
on	O
pretraining	O
and	O
evaluating	O
LightningDOT	O
on	O
downstream	O
ITR	O
benchmarks	O
.	O

Analysing	O
the	O
Integration	O
of	O
Semantic	O
Web	O
Features	O
for	O
Document	O
Planning	O
across	O
Genres	O

Language	O
is	O
usually	O
studied	O
and	O
analysed	O
from	O
different	O
disciplines	O
generally	O
on	O
the	O
premise	O
that	O
it	O
constitutes	O
a	O
form	O
of	O
communication	O
which	O
pursues	O
a	O
specific	O
objective	O
.	O
The	O
discourse	O
,	O
in	O
that	O
sense	O
,	O
can	O
be	O
understood	O
as	O
a	O
text	O
which	O
is	O
constructed	O
to	O
express	O
such	O
objective	O
.	O
When	O
a	O
discourse	O
is	O
created	O
,	O
its	O
production	O
is	O
related	O
to	O
some	O
textual	O
genre	O
,	O
usually	O
connected	O
with	O
some	O
pragmatic	O
features	O
,	O
like	O
the	O
intention	O
of	O
the	O
writer	O
or	O
the	O
audience	O
to	O
whom	O
is	O
addressed	O
,	O
both	O
conditioning	O
the	O
use	O
of	O
language	O
.	O
But	O
genres	O
can	O
be	O
considered	O
as	O
well	O
as	O
compounds	O
of	O
different	O
pieces	O
of	O
text	O
with	O
a	O
certain	O
degree	O
of	O
order	O
,	O
each	O
one	O
seeking	O
for	O
more	O
concrete	O
objectives	O
.	O
This	O
paper	O
presents	O
a	O
proposal	O
to	O
learn	O
such	O
features	O
as	O
a	O
way	O
to	O
generate	O
richer	O
document	O
plans	O
,	O
applying	O
clustering	O
techniques	O
over	O
annotated	O
documents	O
.	O

The	O
current	O
research	O
is	O
carried	O
out	O
from	O
a	O
conception	O
of	O
Natural	O
Language	O
Generation	O
(	O
NLG	O
)	O
for	O
which	O
the	O
creation	O
of	O
a	O
text	O
requires	O
an	O
intermediate	O
output	O
called	O
a	O
document	O
plan	O
.	O
It	O
is	O
by	O
the	O
macroplanning	O
stage	O
that	O
the	O
system	O
provides	O
this	O
plan	O
of	O
selected	O
and	O
ordered	O
content	O
.	O
At	O
present	O
,	O
our	O
work	O
is	O
focused	O
on	O
how	O
to	O
elaborate	O
that	O
plan	O
in	O
order	O
to	O
meet	O
some	O
requisites	O
regarding	O
flexibility	O
of	O
the	O
system	O
:	O
it	O
should	O
be	O
able	O
to	O
produce	O
different	O
outcomes	O
conditioned	O
by	O
the	O
communicative	O
goal	O
,	O
the	O
audience	O
,	O
...	O
the	O
context	O
,	O
on	O
the	O
whole	O
.	O
Henceforth	O
,	O
the	O
main	O
aim	O
of	O
our	O
current	O
research	O
is	O
to	O
enrich	O
the	O
pragmatic	O
facet	O
of	O
the	O
NLG	O
process	O
.	O
The	O
expected	O
outcome	O
is	O
a	O
scheme	O
or	O
ordering	O
of	O
the	O
ideas	O
that	O
should	O
be	O
realised	O
in	O
a	O
set	O
of	O
cohesive	O
and	O
coherent	O
sentences	O
and	O
paragraphs	O
.	O
According	O
to	O
some	O
theories	O
of	O
the	O
discourse	O
(	O
Bakhtin	O
,	O
2010	O
;	O
Halliday	O
et	O
al	O
,	O
2014	O
)	O
,	O
genres	O
can	O
be	O
understood	O
as	O
social	O
constructions	O
that	O
settle	O
a	O
connection	O
between	O
the	O
discourse	O
and	O
the	O
situation	O
in	O
which	O
it	O
is	O
produced	O
,	O
reflected	O
both	O
in	O
its	O
structure	O
and	O
its	O
content	O
.	O
According	O
to	O
Swavels	O
(	O
1990	O
)	O
:	O
"	O
A	O
genre	O
comprises	O
a	O
class	O
of	O
communicative	O
events	O
,	O
the	O
members	O
of	O
which	O
share	O
some	O
set	O
of	O
communicative	O
purposes	O
.	O
These	O
purposes	O
are	O
recognised	O
by	O
the	O
expert	O
members	O
of	O
the	O
parent	O
discourse	O
community	O
,	O
and	O
thereby	O
constitute	O
the	O
rationale	O
for	O
the	O
genre	O
.	O
This	O
rationale	O
shapes	O
the	O
schematic	O
structure	O
of	O
the	O
discourse	O
and	O
influences	O
and	O
constraints	O
choice	O
of	O
content	O
and	O
style	O
.	O
"	O
Besides	O
,	O
genres	O
become	O
interesting	O
because	O
they	O
are	O
related	O
to	O
communicative	O
purposes	O
in	O
different	O
manners	O
,	O
from	O
a	O
global	O
viewpoint	O
to	O
fine	O
-	O
grained	O
levels	O
.	O
As	O
an	O
example	O
,	O
we	O
can	O
think	O
on	O
the	O
case	O
of	O
a	O
person	O
who	O
is	O
looking	O
for	O
recommendation	O
in	O
review	O
pages	O
.	O
Recommending	O
would	O
be	O
the	O
main	O
,	O
global	O
purpose	O
of	O
the	O
text	O
he	O
consults	O
when	O
it	O
was	O
created	O
.	O
But	O
it	O
is	O
possible	O
that	O
the	O
writer	O
also	O
wanted	O
to	O
explain	O
the	O
motivation	O
of	O
the	O
journeynarrative	O
,	O
personal	O
experience	O
-	O
or	O
to	O
describe	O
the	O
facilities	O
in	O
order	O
to	O
complete	O
his	O
review	O
.	O
Narration	O
,	O
description	O
,	O
recommendation	O
,	O
...	O
they	O
represent	O
low	O
-	O
level	O
functions	O
of	O
the	O
text	O
related	O
to	O
the	O
intention	O
of	O
the	O
writer	O
and	O
,	O
in	O
some	O
cases	O
,	O
they	O
can	O
be	O
identified	O
as	O
different	O
sets	O
of	O
sentences	O
.	O
This	O
lead	O
us	O
to	O
the	O
possibility	O
of	O
learning	O
the	O
structure	O
of	O
the	O
text	O
and	O
its	O
features	O
,	O
which	O
differs	O
from	O
one	O
genre	O
to	O
another	O
.	O
In	O
reviews	O
,	O
the	O
presence	O
and	O
order	O
of	O
the	O
parts	O
is	O
not	O
strict	O
.	O
Maybe	O
one	O
traveller	O
does	O
not	O
share	O
his	O
personal	O
story	O
,	O
but	O
also	O
he	O
describes	O
the	O
room	O
and	O
recom	O
-	O
mends	O
the	O
brand	O
,	O
while	O
another	O
one	O
first	O
evaluates	O
and	O
then	O
describes	O
.	O
An	O
example	O
to	O
illustrate	O
this	O
can	O
be	O
found	O
in	O
table	O
1	O
.	O
Conversely	O
,	O
it	O
would	O
make	O
no	O
sense	O
to	O
write	O
a	O
scientific	O
article	O
that	O
reports	O
the	O
results	O
before	O
explaining	O
the	O
methodology	O
or	O
not	O
explaining	O
it	O
at	O
all	O
,	O
for	O
example	O
.	O

Personal	O
Experience	O
:	O
On	O
our	O
last	O
trip	O
to	O
Hawaii	O
my	O
husband	O
and	O
I	O
...	O
As	O
an	O
added	O
bonus	O
,	O
we	O
were	O
given	O
...	O
We	O
decided	O
to	O
take	O
advantage	O
of	O
...	O

The	O
lobby	O
is	O
adorned	O
with	O
lush	O
gardens	O
...	O
Alongside	O
the	O
gardens	O
are	O
tropical	O
birds	O
...	O
The	O
rooms	O
are	O
spacious	O
.	O

If	O
you	O
are	O
ever	O
fortunate	O
enough	O
to	O
visit	O
the	O
beautiful	O
island	O
of	O
Kauai	O
,	O
try	O
to	O
stay	O
at	O
the	O
H	O
Regency	O
,	O
you	O
wo	O
n't	O
be	O
disappointed	O
.	O

The	O
W	O
New	O
York	O
is	O
on	O
Lexington	O
right	O
...	O
The	O
rooms	O
are	O
just	O
as	O
small	O
as	O
before	O
...	O
The	O
lobby	O
of	O
the	O
hotel	O
is	O
also	O
...	O

Being	O
a	O
corporate	O
lawyer	O
I	O
travel	O
...	O
The	O
first	O
time	O
I	O
was	O
in	O
a	O
small	O
room	O
...	O
The	O
second	O
time	O
I	O
could	O
not	O
believe	O
...	O

Having	O
pointed	O
out	O
the	O
expanse	O
of	O
the	O
related	O
work	O
,	O
our	O
approach	O
wants	O
to	O
overcome	O
its	O
limitations	O
.	O
On	O
the	O
one	O
hand	O
,	O
in	O
the	O
sense	O
of	O
being	O
suitable	O
for	O
any	O
genre	O
,	O
not	O
a	O
particular	O
one	O
.	O
On	O
the	O
other	O
hand	O
,	O
focusing	O
on	O
several	O
types	O
of	O
features	O
at	O
the	O
same	O
time	O
,	O
in	O
order	O
to	O
propose	O
a	O
more	O
comprehensive	O
description	O
of	O
the	O
parts	O
of	O
a	O
discourse	O
.	O
With	O
regard	O
to	O
accomplish	O
such	O
a	O
project	O
,	O
the	O
selection	O
and	O
design	O
of	O
the	O
proper	O
features	O
becomes	O
a	O
challenging	O
task	O
itself	O
,	O
strongly	O
related	O
to	O
the	O
aim	O
of	O
the	O
investigation	O
.	O
Specifically	O
,	O
we	O
try	O
to	O
detect	O
the	O
features	O
that	O
may	O
reveal	O
links	O
with	O
the	O
functionality	O
or	O
purpose	O
of	O
the	O
paragraph	O
that	O
includes	O
them	O
.	O
We	O
have	O
begun	O
annotating	O
several	O
aspects	O
by	O
means	O
of	O
linguistic	O
tools	O
and	O
resources	O
:	O
Freeling	O
(	O
Padró	O
and	O
Stanilovsky	O
,	O
2012	O
)	O
for	O
PoS	O
annotation	O
and	O
Entity	O
Recognition	O
and	O
ADESSE	O
(	O
García	O
-	O
Miguel	O
et	O
al	O
,	O
2010	O
)	O
as	O
a	O
source	O
of	O
verb	O
senses	O
from	O
a	O
semantic	O
perspective	O
.	O

Until	O
now	O
,	O
some	O
experiments	O
have	O
been	O
performed	O
over	O
a	O
corpus	O
of	O
Spanish	O
reviews	O
extracted	O
from	O
Tripadvisor	O
.	O
The	O
reviews	O
were	O
segmented	O
into	O
sentences	O
,	O
and	O
some	O
figures	O
regarding	O
semantic	O
and	O
morphological	O
features	O
were	O
computed	O
after	O
dividing	O
each	O
document	O
in	O
regions	O
(	O
sets	O
of	O
sentences	O
)	O
,	O
increasing	O
their	O
number	O
from	O
one	O
block	O
up	O
to	O
four	O
blocks	O
of	O
sentences	O
.	O
Table	O
3	O
shows	O
some	O
statistics	O
of	O
the	O
corpus	O
employed	O
.	O
In	O
order	O
to	O
strengthen	O
the	O
results	O
,	O
corpora	O
of	O
other	O
genres	O
with	O
different	O
degree	O
of	O
flexibility	O
in	O
their	O
structure	O
are	O
being	O
analysed	O
:	O
tales	O
,	O
news	O
and	O
Wikipedia	O
articles	O
are	O
to	O
be	O
compared	O
with	O
the	O
for	O
-	O
mer	O
outcomes	O
.	O
The	O
length	O
of	O
the	O
blocks	O
is	O
the	O
result	O
of	O
a	O
proportional	O
division	O
of	O
the	O
length	O
of	O
the	O
document	O
for	O
now	O
.	O
As	O
the	O
research	O
advances	O
,	O
new	O
experiments	O
will	O
be	O
developed	O
to	O
determine	O
a	O
more	O
accurate	O
size	O
for	O
the	O
pseudo	O
-	O
paragraphs	O
.	O
With	O
the	O
ideas	O
introduced	O
in	O
the	O
section	O
4	O
,	O
our	O
next	O
step	O
and	O
proposal	O
,	O
includes	O
improving	O
the	O
significance	O
of	O
the	O
features	O
with	O
which	O
the	O
clustering	O
algorithms	O
have	O
to	O
work	O
,	O
trying	O
to	O
reveal	O
an	O
inner	O
structure	O
of	O
the	O
text	O
related	O
to	O
its	O
genre	O
and	O
purposes	O
.	O
The	O
better	O
our	O
features	O
are	O
,	O
the	O
more	O
precise	O
the	O
descriptions	O
we	O
can	O
do	O
of	O
the	O
discourse	O
areas	O
.	O

This	O
research	O
work	O
has	O
been	O
supported	O
by	O
the	O
Generalitat	O
Valenciana	O
by	O
the	O
grant	O
ACIF/2016/501.It	O
has	O
also	O
funded	O
by	O
the	O
University	O
of	O
Alicante	O
,	O
Spanish	O
Government	O
and	O
the	O
European	O
Commission	O
through	O
the	O
projects	O
,	O
"	O
Explotación	O
y	O
tratamiento	O
de	O
la	O
información	O
disponible	O
en	O
Internet	O
para	O
la	O
anotación	O
y	O
generación	O
de	O
textos	O
adaptados	O
al	O
usuario	O
"	O
(	O
GRE13	O
-	O
15	O
)	O
and	O
"	O
DIIM2.0	O
:	O
Desarrollo	O
de	O
técnicas	O
Inteligentes	O
e	O
Interactivas	O
de	O
Minería	O
y	O
generación	O
de	O
información	O
sobre	O
la	O
web	O
2.0	O
"	O
(	O
PROMETEOII/2014/001	O
)	O
,	O
TIN2015	O
-	O
65100	O
-	O
R	O
,	O
TIN2015	O
-	O
65136	O
-	O
C2	O
-	O
2	O
-	O
R	O
,	O
and	O
SAM	O
(	O
FP7	O
-	O
611312	O
)	O
,	O
respectively	O
.	O

Distilling	O
Relation	O
Embeddings	O
from	O
Pre	O
-	O
trained	O
Language	O
Models	O

In	O
this	O
section	O
,	O
we	O
describe	O
our	O
proposed	O
relation	O
embedding	O
model	O
(	O
RelBERT	O
henceforth	O
)	O
.	O
To	O
obtain	O
a	O
relation	O
embedding	O
for	O
given	O
a	O
word	O
pair	O
(	O
h	O
,	O
t	O
)	O
,	O
we	O
first	O
convert	O
it	O
into	O
a	O
sentence	O
s	O
,	O
called	O
the	O
prompt	O
.	O
We	O
then	O
feed	O
the	O
prompt	O
through	O
the	O
LM	O
and	O
average	O
the	O
contextualized	O
embeddings	O
(	O
i.e.	O
the	O
output	O
vectors	O
)	O
to	O
get	O
the	O
relation	O
embedding	O
of	O
(	O
h	O
,	O
t	O
)	O
.	O
These	O
steps	O
are	O
illustrated	O
in	O
Figure	O
1	O
and	O
explained	O
in	O
more	O
detail	O
in	O
the	O
following	O
.	O

To	O
better	O
understand	O
how	O
relation	O
embeddings	O
are	O
learned	O
,	O
in	O
this	O
section	O
we	O
analyze	O
the	O
model	O
's	O
performance	O
in	O
more	O
detail	O
.	O

In	O
this	O
section	O
,	O
we	O
show	O
additional	O
experimental	O
results	O
that	O
complement	O
the	O
main	O
results	O
of	O
the	O
paper	O
.	O

In	O
this	O
section	O
,	O
we	O
explain	O
models	O
'	O
configuration	O
in	O
the	O
experiments	O
,	O
and	O
details	O
on	O
RelBERT	O
's	O
training	O
time	O
.	O

Training	O
a	O
single	O
RelBERT	O
model	O
with	O
a	O
custom	O
prompt	O
takes	O
about	O
half	O
a	O
day	O
on	O
two	O
V100	O
GPUs	O
.	O
Additionally	O
,	O
to	O
achieve	O
prompt	O
by	O
AutoPrompt	O
technique	O
takes	O
about	O
a	O
week	O
on	O
a	O
single	O
V100	O
,	O
while	O
P	O
-	O
tuning	O
takes	O
3	O
to	O
4	O
hours	O
,	O
also	O
on	O
a	O
single	O
V100	O
.	O

In	O
this	O
section	O
,	O
we	O
analyze	O
our	O
experimental	O
results	O
based	O
on	O
prediction	O
breakdown	O
and	O
provide	O
an	O
extended	O
qualitative	O
analysis	O
.	O

Tables	O
13	O
shows	O
the	O
nearest	O
neighbors	O
of	O
a	O
number	O
of	O
selected	O
word	O
pairs	O
,	O
in	O
terms	O
of	O
their	O
RelBERT	O
and	O
RELATIVE	O
embeddings	O
.	O
In	O
both	O
cases	O
cosine	O
similarity	O
is	O
used	O
to	O
compare	O
the	O
embeddings	O
and	O
the	O
pair	O
vocabulary	O
of	O
the	O
RELATIVE	O
model	O
is	O
used	O
to	O
determine	O
the	O
universe	O
of	O
candidate	O
neighbors	O
.	O
The	O
results	O
for	O
the	O
RelBERT	O
embeddings	O
show	O
their	O
ability	O
to	O
capture	O
a	O
wide	O
range	O
of	O
relations	O
.	O
In	O
most	O
cases	O
the	O
neighbors	O
make	O
sense	O
,	O
despite	O
the	O
fact	O
that	O
many	O
of	O
these	O
relations	O
are	O
quite	O
different	O
from	O
those	O
in	O
the	O
SemEval	O
dataset	O
that	O
was	O
used	O
for	O
training	O
RelBERT	O
.	O
The	O
results	O
for	O
RELATIVE	O
are	O
in	O
general	O
much	O
noisier	O
,	O
suggesting	O
that	O
REL	O
-	O
ATIVE	O
embeddings	O
fail	O
to	O
capture	O
many	O
types	O
of	O
relations	O
.	O
This	O
is	O
in	O
particular	O
the	O
case	O
for	O
the	O
morphological	O
examples	O
,	O
although	O
various	O
issues	O
can	O
be	O
observed	O
for	O
the	O
other	O
relations	O
as	O
well	O
.	O

Jose	O
Camacho	O
-	O
Collados	O
acknowledges	O
support	O
from	O
the	O
UKRI	O
Future	O
Leaders	O
Fellowship	O
scheme	O
.	O

Once	O
documents	O
and	O
labels	O
have	O
been	O
processed	O
as	O
described	O
previously	O
,	O
we	O
assign	O
a	O
label	O
to	O
a	O
document	O
by	O
identifying	O
the	O
label	O
to	O
which	O
it	O
is	O
most	O
similar	O
.	O
Our	O
evaluation	O
of	O
similarity	O
is	O
based	O
on	O
Latent	O
Semantic	O
Analysis	O
(	O
to	O
avoid	O
the	O
pitfalls	O
of	O
literal	O
term	O
matching	O
)	O
and	O
cosine	O
similarity	O
on	O
the	O
output	O
LSA	O
vectors	O
.	O
Before	O
applying	O
LSA	O
,	O
we	O
start	O
by	O
stemming	O
all	O
the	O
words	O
using	O
Porter	O
stemmer	O
.	O
We	O
feel	O
that	O
similarities	O
between	O
documents	O
and	O
labels	O
can	O
be	O
more	O
reliably	O
estimated	O
in	O
the	O
reduced	O
latent	O
space	O
representation	O
than	O
in	O
the	O
original	O
representation	O
.	O
The	O
rationale	O
is	O
that	O
documents	O
which	O
share	O
frequently	O
co	O
-	O
occurring	O
terms	O
will	O
have	O
a	O
similar	O
representation	O
in	O
the	O
latent	O
space	O
,	O
even	O
if	O
they	O
have	O
no	O
terms	O
in	O
common	O
.	O
LSA	O
thus	O
performs	O
some	O
sort	O
of	O
noise	O
reduction	O
and	O
has	O
the	O
potential	O
benefit	O
to	O
detect	O
synonyms	O
as	O
well	O
as	O
words	O
that	O
refer	O
to	O
the	O
same	O
topic	O
.	O

As	O
we	O
described	O
previously	O
,	O
the	O
proposed	O
method	O
stemmed	O
from	O
a	O
specific	O
need	O
in	O
the	O
banking	O
industry	O
where	O
a	O
large	O
number	O
of	O
incidents	O
had	O
to	O
be	O
mapped	O
to	O
a	O
newly	O
defined	O
taxonomy	O
of	O
operational	O
risks	O
.	O
Specifically	O
,	O
it	O
was	O
designed	O
to	O
avoid	O
the	O
tedious	O
and	O
time	O
consuming	O
effort	O
of	O
asking	O
experts	O
to	O
manually	O
review	O
thousands	O
of	O
incidents	O
.	O
An	O
automated	O
-	O
or	O
more	O
precisely	O
assisted	O
-	O
approach	O
also	O
presented	O
the	O
additional	O
benefit	O
of	O
ensuring	O
a	O
higher	O
degree	O
of	O
consistency	O
in	O
the	O
mapping	O
than	O
would	O
have	O
been	O
achieved	O
a	O
team	O
of	O
annotators	O
.	O
In	O
this	O
section	O
,	O
we	O
provide	O
some	O
additional	O
context	O
into	O
this	O
specific	O
task	O
,	O
report	O
the	O
observed	O
performance	O
of	O
our	O
method	O
and	O
discuss	O
some	O
of	O
the	O
specificities	O
of	O
the	O
context	O
.	O

For	O
the	O
purpose	O
of	O
experiment	O
,	O
operational	O
teams	O
(	O
not	O
experts	O
)	O
were	O
asked	O
to	O
provide	O
manual	O
tags	O
for	O
a	O
sample	O
of	O
989	O
operational	O
incidents	O
.	O
Table	O
4	O
provide	O
the	O
classification	O
results	O
of	O
our	O
approach	O
when	O
compared	O
to	O
those	O
manual	O
annotations	O
,	O
considering	O
all	O
three	O
levels	O
of	O
the	O
taxonomy	O
.	O
In	O
a	O
second	O
step	O
in	O
the	O
evaluation	O
,	O
an	O
expert	O
was	O
given	O
the	O
difficult	O
task	O
to	O
challenge	O
each	O
time	O
they	O
disagreed	O
the	O
computer	O
and	O
human	O
annotation	O
and	O
determine	O
which	O
was	O
ultimately	O
correct	O
.	O
This	O
exercise	O
indicated	O
that	O
in	O
32	O
cases	O
out	O
of	O
989	O
operational	O
incidents	O
under	O
consideration	O
for	O
the	O
Level	O
1	O
classification	O
,	O
the	O
machine	O
generated	O
category	O
were	O
more	O
relevant	O
(	O
hence	O
correct	O
)	O
than	O
those	O
identified	O
by	O
the	O
operational	O
team	O
.	O

Given	O
the	O
number	O
of	O
categories	O
,	O
we	O
were	O
satisfied	O
with	O
the	O
level	O
of	O
performance	O
that	O
we	O
observed	O
,	O
especially	O
for	O
Level	O
1	O
and	O
Level	O
2	O
of	O
the	O
taxonomy	O
.	O
More	O
importantly	O
,	O
as	O
we	O
progress	O
with	O
the	O
follow	O
up	O
exercise	O
of	O
mapping	O
internal	O
incident	O
descriptions	O
,	O
we	O
have	O
evolved	O
from	O
a	O
point	O
where	O
users	O
always	O
mistrust	O
the	O
outcome	O
of	O
the	O
automated	O
classification	O
to	O
a	O
point	O
where	O
users	O
see	O
the	O
suggested	O
mapping	O
from	O
our	O
algorithm	O
as	O
a	O
rele	O
-	O
vant	O
recommendation	O
.	O
Our	O
perspective	O
on	O
the	O
success	O
of	O
this	O
method	O
in	O
this	O
particular	O
context	O
is	O
that	O
operational	O
risk	O
is	O
a	O
textbook	O
case	O
where	O
domain	O
specific	O
labels	O
and	O
vocabulary	O
prevail	O
.	O
For	O
instance	O
,	O
technical	O
words	O
such	O
as	O
forge	O
,	O
fictitious	O
,	O
bogus	O
,	O
ersatz	O
,	O
or	O
counterfeit	O
indicate	O
almost	O
surely	O
that	O
a	O
Fraudulent	O
Account	O
Opening	O
operation	O
happened	O
.	O
Most	O
of	O
operational	O
incidents	O
must	O
contain	O
a	O
combination	O
of	O
technical	O
keywords	O
due	O
to	O
their	O
highly	O
operational	O
nature	O
.	O
What	O
the	O
method	O
brings	O
is	O
the	O
ability	O
to	O
combine	O
human	O
expertise	O
through	O
seed	O
words	O
with	O
the	O
strength	O
of	O
the	O
machine	O
which	O
can	O
process	O
and	O
memorize	O
large	O
corpus	O
and	O
derive	O
distributional	O
semantics	O
from	O
it	O
.	O
In	O
this	O
way	O
,	O
the	O
cognitive	O
burden	O
of	O
being	O
exhaustive	O
is	O
lifted	O
from	O
the	O
experts	O
shoulders	O
.	O

ICE	O
:	O
Idiom	O
and	O
Collocation	O
Extractor	O
for	O
Research	O
and	O
Education	O

Collocation	O
and	O
idiom	O
extraction	O
are	O
wellknown	O
challenges	O
with	O
many	O
potential	O
applications	O
in	O
Natural	O
Language	O
Processing	O
(	O
NLP	O
)	O
.	O
Our	O
experimental	O
,	O
open	O
-	O
source	O
software	O
system	O
,	O
called	O
ICE	O
,	O
is	O
a	O
python	O
package	O
for	O
flexibly	O
extracting	O
collocations	O
and	O
idioms	O
,	O
currently	O
in	O
English	O
.	O
It	O
also	O
has	O
a	O
competitive	O
POS	O
tagger	O
that	O
can	O
be	O
used	O
alone	O
or	O
as	O
part	O
of	O
collocation	O
/	O
idiom	O
extraction	O
.	O
ICE	O
is	O
available	O
free	O
of	O
cost	O
for	O
research	O
and	O
educational	O
uses	O
in	O
two	O
user	O
-	O
friendly	O
formats	O
.	O
This	O
paper	O
gives	O
an	O
overview	O
of	O
ICE	O
and	O
its	O
performance	O
,	O
and	O
briefly	O
describes	O
the	O
research	O
underlying	O
the	O
extraction	O
algorithms	O
.	O

ICE	O
is	O
a	O
tool	O
for	O
extracting	O
idioms	O
and	O
collocations	O
,	O
but	O
it	O
also	O
has	O
functions	O
for	O
part	O
of	O
speech	O

Lightweight	O
Models	O
for	O
Multimodal	O
Sequential	O
Data	O

The	O
Interaction	O
Canonical	O
Correlation	O
Network	O
(	O
ICCN	O
)	O
[	O
Sun	O
et	O
al	O
,	O
2020	O
]	O
implements	O
Deep	O
Canonical	O
Correlation	O
Analysis	O
(	O
DCCA	O
)	O
[	O
Andrew	O
et	O
al	O
,	O
2013	O
]	O
to	O
extract	O
bimodal	O
features	O
from	O
the	O
outer	O
product	O
matrix	O
of	O
a	O
pair	O
of	O
modalities	O
.	O
3	O
Models	O

We	O
use	O
T	O
,	O
A	O
,	O
and	O
V	O
,	O
to	O
represent	O
the	O
three	O
modalities	O
:	O
text	O
,	O
audio	O
,	O
and	O
video	O
,	O
respectively	O
.	O
Following	O
the	O
notation	O
in	O
[	O
Tsai	O
et	O
al	O
,	O
2019	O
]	O
and	O
,	O
we	O
denote	O
the	O
input	O
as	O
X	O
T	O
,	O
A	O
,	O
V	O
=	O
{	O
x	O
T	O
,	O
x	O
A	O
,	O
x	O
V	O
}	O
where	O
x	O
i	O
=	O
[	O
x	O
t	O
,	O
i	O
]	O
for	O
i	O
[	O
T	O
,	O
A	O
,	O
V	O
]	O
and	O
t	O
[	O
1	O
,	O
τ	O
]	O
and	O
τ	O
is	O
the	O
length	O
of	O
the	O
input	O
sentence	O
.	O
Each	O
of	O
the	O
three	O
modalities	O
has	O
its	O
own	O
lowlevel	O
features	O
,	O
such	O
as	O
the	O
Mel	O
spectrogram	O
for	O
audio	O
or	O
facial	O
landmarks	O
for	O
video	O
.	O
These	O
features	O
are	O
extracted	O
at	O
different	O
sampling	O
rates	O
-	O
one	O
set	O
of	O
features	O
per	O
word	O
or	O
character	O
for	O
text	O
,	O
per	O
millisecond	O
for	O
audio	O
,	O
and	O
per	O
frame	O
for	O
video	O
-	O
and	O
thus	O
the	O
input	O
sequences	O
for	O
the	O
three	O
modalities	O
are	O
often	O
different	O
.	O
A	O
five	O
-	O
thousand	O
-	O
millisecond	O
audio	O
sequence	O
,	O
for	O
example	O
,	O
may	O
be	O
only	O
a	O
three	O
-	O
word	O
sequence	O
from	O
a	O
textual	O
perspective	O
and	O
a	O
50	O
-	O
frame	O
sequence	O
from	O
a	O
video	O
perspective	O
.	O
We	O
align	O
the	O
audio	O
and	O
video	O
to	O
the	O
text	O
using	O
the	O
timestamps	O
provided	O
in	O
the	O
text	O
transcripts	O
.	O
The	O
set	O
of	O
audio	O
or	O
video	O
samples	O
that	O
correspond	O
to	O
a	O
word	O
in	O
the	O
transcript	O
are	O
combined	O
using	O
a	O
series	O
of	O
1D	O
convolutional	O
layers	O
:	O
X	O
{	O
T	O
,	O
A	O
,	O
V	O
}	O
=	O
conv1D	O
X	O
{	O
T	O
,	O
A	O
,	O
V	O
}	O
R	O
d	O
where	O
d	O
is	O
a	O
common	O
feature	O
dimension	O
size	O
.	O
This	O
procedure	O
ensures	O
that	O
the	O
input	O
sequence	O
length	O
is	O
the	O
same	O
across	O
modalities	O
.	O

The	O
effect	O
of	O
direction	O
on	O
our	O
Round	O
Robin	O
model	O
is	O
shown	O
in	O
Table	O
9	O
;	O
this	O
experiment	O
shows	O
the	O
impact	O
of	O
the	O
direction	O
of	O
information	O
flow	O
across	O
modalities	O
within	O
the	O
model	O
.	O
Comparing	O
our	O
results	O
to	O
those	O
of	O
MuLT	O
and	O
MuLT	O
*	O
,	O
we	O
see	O
that	O
capturing	O
information	O
flow	O
in	O
one	O
direction	O
,	O
text	O
to	O
audio	O
to	O
video	O
and	O
back	O
to	O
text	O
,	O
is	O
enough	O
for	O
a	O
model	O
to	O
give	O
good	O
predictions	O
,	O
without	O
requiring	O
the	O
additional	O
overhead	O
of	O
handling	O
both	O
directions	O
.	O
We	O
can	O
also	O
see	O
that	O
the	O
direction	O
does	O
matter	O
;	O
the	O
performance	O
of	O
the	O
Round	O
Robin	O
model	O
with	O
information	O
flowing	O
in	O
the	O
opposite	O
direction	O
,	O
from	O
video	O
to	O
audio	O
to	O
text	O
and	O
back	O
to	O
video	O
,	O
is	O
relatively	O
poor	O
.	O
These	O
results	O
suggest	O
that	O
the	O
interactions	O
between	O
pairs	O
of	O
modalities	O
are	O
directed	O
.	O

IIT	O
DHANBAD	O
CODECHAMPS	O
at	O
SemEval	O
-	O
2022	O
Task	O
5	O
:	O
MAMI	O
-	O
Multimedia	O
Automatic	O
Misogyny	O
Identification	O

With	O
the	O
growth	O
of	O
the	O
internet	O
,	O
the	O
use	O
of	O
social	O
media	O
based	O
on	O
images	O
has	O
drastically	O
increased	O
like	O
Twitter	O
,	O
Instagram	O
,	O
etc	O
.	O
In	O
these	O
social	O
media	O
,	O
women	O
have	O
a	O
very	O
high	O
contribution	O
as	O
of	O
75	O
%	O
women	O
use	O
social	O
media	O
multiple	O
times	O
compared	O
to	O
men	O
which	O
is	O
only	O
65	O
%	O
of	O
men	O
uses	O
social	O
media	O
multiple	O
times	O
a	O
day	O
.	O
However	O
,	O
with	O
this	O
much	O
contribution	O
,	O
it	O
also	O
increases	O
systematic	O
inequality	O
and	O
discrimination	O
offline	O
is	O
replicated	O
in	O
online	O
spaces	O
in	O
the	O
form	O
of	O
MEMEs	O
.	O
A	O
meme	O
is	O
essentially	O
an	O
image	O
characterized	O
by	O
pictorial	O
content	O
with	O
an	O
overlaying	O
text	O
a	O
posteriori	O
introduced	O
by	O
humans	O
,	O
with	O
the	O
main	O
goal	O
of	O
being	O
funny	O
and/or	O
ironic	O
.	O
Although	O
most	O
of	O
them	O
are	O
created	O
with	O
the	O
intent	O
of	O
making	O
funny	O
jokes	O
,	O
in	O
a	O
short	O
time	O
people	O
started	O
to	O
use	O
them	O
as	O
a	O
form	O
of	O
hate	O
and	O
prejudice	O
against	O
women	O
,	O
landing	O
to	O
sexist	O
and	O
aggressive	O
messages	O
in	O
online	O
environments	O
that	O
subsequently	O
amplify	O
the	O
sexual	O
stereotyping	O
and	O
gender	O
inequality	O
of	O
the	O
offline	O
world	O
.	O
This	O
leads	O
to	O
the	O
need	O
for	O
automatic	O
detection	O
of	O
Misogyny	O
MEMEs	O
.	O
Specifically	O
,	O
I	O
described	O
the	O
model	O
submitted	O
for	O
the	O
shared	O
task	O
on	O
Multimedia	O
Automatic	O
Misogyny	O
Identification	O
(	O
MAMI	O
(	O
Fersini	O
et	O
al	O
,	O
2022	O
)	O
)	O
and	O
my	O
team	O
name	O
is	O
IIT	O
DHANBAD	O
CODECHAMPS	O
.	O

Here	O
we	O
have	O
described	O
the	O
dataset	O
and	O
task	O
provided	O
by	O
Multimedia	O
Automatic	O
Misogyny	O
Identification	O
(	O
MAMI	O
(	O
Fersini	O
et	O
al	O
,	O
2022	O
)	O
)	O
challenge	O
.	O
Multimedia	O
Automatic	O
Misogyny	O
Identification	O
(	O
MAMI	O
)	O
task	O
is	O
divided	O
into	O
two	O
sub	O
task	O
.	O
Sub	O
-	O
task	O
A	O
:	O
a	O
basic	O
task	O
about	O
misogynous	O
meme	O
identification	O
,	O
where	O
a	O
meme	O
should	O
be	O
categorized	O
either	O
as	O
misogynous	O
or	O
not	O
misogynous	O
(	O
shown	O
in	O
Table	O
1	O
)	O
.	O
Sub	O
-	O
task	O
B	O
:	O
an	O
advanced	O
task	O
,	O
where	O
the	O
type	O
of	O
misogyny	O
should	O
be	O
recognized	O
among	O
potential	O
overlapping	O
categories	O
such	O
as	O
stereotype	O
,	O
shaming	O
,	O
objectification	O
and	O
violence	O
.	O
e.g.	O
umn	O
represent	O
Text	O
Transcription	O
of	O
the	O
meme	O
.	O

We	O
pose	O
a	O
new	O
,	O
multivalent	O
interpretation	O
of	O
the	O
DIH	O
(	O
the	O
MDIH	O
)	O
which	O
models	O
the	O
entailment	O
of	O
predicates	O
across	O
valencies	O
.	O
The	O
intuition	O
comes	O
from	O
observing	O
eventualities	O
(	O
Vendler	O
,	O
1967	O
)	O
which	O
occur	O
in	O
the	O
world	O
.	O
Neo	O
-	O
Davidsonian	O
semantics	O
(	O
Davidson	O
,	O
1967	O
;	O
Maienborn	O
,	O
2011	O
)	O
explains	O
that	O
a	O
textual	O
predicate	O
,	O
its	O
arguments	O
,	O
and	O
adjuncts	O
,	O
are	O
all	O
properties	O
of	O
an	O
underlying	O
event	O
variable	O
.	O
En	O
-	O
tailments	O
about	O
one	O
or	O
more	O
of	O
the	O
arguments	O
arise	O
from	O
their	O
roles	O
in	O
this	O
eventuality	O
.	O
We	O
may	O
infer	O
that	O
"	O
Mr.	O
Boddy	O
died	O
"	O
due	O
to	O
his	O
role	O
as	O
a	O
direct	O
object	O
in	O
the	O
killing	O
/	O
murdering	O
event	O
.	O
No	O
other	O
information	O
is	O
needed	O
,	O
including	O
who	O
murdered	O
Mr.	O
Boddy	O
,	O
where	O
,	O
or	O
with	O
what	O
instrument	O
.	O
Boddy	O
is	O
dead	O
simply	O
because	O
he	O
was	O
murdered	O
.	O
We	O
build	O
on	O
this	O
insight	O
to	O
develop	O
the	O
MDIH	O
.	O
Here	O
,	O
a	O
predicate	O
is	O
represented	O
(	O
as	O
in	O
2	O
)	O
by	O
features	O
which	O
are	O
the	O
argument	O
tuples	O
it	O
appears	O
with	O
.	O
We	O
recognize	O
a	O
tuple	O
as	O
a	O
proxy	O
for	O
a	O
world	O
event	O
,	O
e.g.	O
VISIT	O
(	O
Obama	O
,	O
Hawaii	O
)	O
identifies	O
one	O
instance	O
of	O
a	O
real	O
VISIT	O
event	O
.	O
Our	O
method	O
learns	O
by	O
tracking	O
entity	O
tuples	O
across	O
events	O
in	O
the	O
world	O
.	O
The	O
MDIH	O
signals	O
an	O
entailment	O
from	O
a	O
premise	O
p	O
to	O
hypothesis	O
h	O
if	O
,	O
distributionally	O
,	O
subtuples	O
of	O
p	O
are	O
always	O
found	O
amongst	O
tuples	O
of	O
h.	O
Crucially	O
,	O
we	O
allow	O
h	O
to	O
drop	O
in	O
valency	O
so	O
that	O
we	O
learn	O
entailments	O
about	O
subsets	O
of	O
p	O
's	O
arguments	O
.	O
We	O
now	O
formalize	O
the	O
MDIH	O
and	O
then	O
illustrate	O
with	O
an	O
example	O
.	O
We	O
define	O
the	O
argument	O
tuple	O
structures	O
for	O
a	O
premise	O
and	O
hypothesis	O
predicate	O
:	O
P	O
=	O
{	O
(	O
a	O
k	O
,	O
1	O
,	O
.	O
.	O
.	O
,	O
a	O
k	O
,	O
I	O
)	O
|	O
k	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
M	O
}	O
}	O
H	O
=	O
{	O
(	O
b	O
k	O
,	O
1	O
,	O
.	O
.	O
.	O
,	O
b	O
k	O
,	O
J	O
)	O
|	O
k	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
N	O
}	O
}	O
P	O
is	O
a	O
set	O
of	O
M	O
argument	O
tuples	O
(	O
each	O
of	O
size	O
I	O
)	O
which	O
correspond	O
to	O
instances	O
of	O
a	O
premise	O
predicate	O
p.	O
H	O
is	O
a	O
set	O
of	O
N	O
argument	O
tuples	O
(	O
each	O
of	O
size	O
J	O
)	O
representing	O
the	O
same	O
for	O
hypothesis	O
h.	O
We	O
limit	O
J	O
≤	O
I	O
,	O
e.g.	O
we	O
learn	O
about	O
relations	O
on	O
realized	O
entity	O
subsets	O
.	O
We	O
do	O
not	O
learn	O
entailments	O
to	O
higher	O
valencies	O
(	O
such	O
as	O
a	O
unary	O
entailing	O
a	O
binary	O
)	O
because	O
additional	O
arguments	O
must	O
be	O
existential	O
,	O
not	O
real	O
.	O
We	O
leave	O
this	O
to	O
future	O
work	O
.	O
To	O
select	O
argument	O
subtuples	O
from	O
tuples	O
in	O
P	O
,	O
we	O
define	O
a	O
vector	O
of	O
indices	O
j	O
with	O
length	O
J	O
,	O
which	O
selects	O
arguments	O
by	O
position	O
.	O
For	O
example	O
,	O
with	O
j	O
=	O
[	O
2	O
,	O
3	O
]	O
,	O
perform	O
P	O
[	O
:	O
,	O
j	O
]	O
.	O
For	O
each	O
argument	O
tuple	O
in	O
P	O
,	O
select	O
just	O
the	O
2nd	O
and	O
3rd	O
arguments	O
,	O
forming	O
a	O
new	O
set	O
of	O
2	O
-	O
tuples	O
.	O
We	O
define	O
the	O
Multivalent	O
Distributional	O
Inclusion	O
Hypothesis	O
:	O
If	O
P	O
[	O
:	O
,	O
j	O
]	O
⊆	O
H	O
[	O
:	O
,	O
m	O
(	O
j	O
)	O
]	O
,	O
then	O
p	O
h	O
Here	O
m	O
:	O
N	O
J	O
N	O
J	O
is	O
a	O
simple	O
bijective	O
mapping	O
from	O
argument	O
indices	O
of	O
p	O
to	O
h.	O
An	O
example	O
where	O
m	O
is	O
needed	O
for	O
argument	O
swapping	O
is	O
"	O
x	O
bought	O
y	O
"	O
entails	O
"	O
y	O
sold	O
to	O
x.	O
"	O
We	O
illustrate	O
by	O
working	O
the	O
kill	O
/	O
die	O
example	O
on	O
a	O
hypothetical	O
corpus	O
.	O
We	O
might	O
find	O
that	O
KILL	O
(	O
x	O
,	O
y	O
)	O
DIE	O
(	O
y	O
)	O
by	O
trying	O
j	O
=	O
[	O
2	O
]	O
and	O
m	O
(	O
[	O
2	O
]	O
)	O
=	O
[	O
1	O
]	O
.	O
We	O
start	O
with	O
P	O
,	O
all	O
2	O
-	O
tuples	O
of	O
killings	O
,	O
and	O
H	O
,	O
all	O
1	O
-	O
tuples	O
of	O
dyings	O
and	O
apply	O
j	O
and	O
m.	O
We	O
may	O
find	O
that	O
selecting	O
arg	O
2	O
from	O
all	O
tuples	O
in	O
P	O
forms	O
a	O
subset	O
of	O
the	O
selection	O
of	O
arg	O
1	O
from	O
tuples	O
in	O
H.	O
Though	O
dyings	O
may	O
happen	O
in	O
many	O
ways	O
,	O
we	O
observe	O
that	O
arg	O
2	O
of	O
a	O
killing	O
often	O
occurs	O
elsewhere	O
in	O
the	O
corpus	O
with	O
a	O
dying	O
,	O
and	O
thus	O
we	O
infer	O
the	O
entailment	O
between	O
predicates	O
.	O
Intuitively	O
this	O
is	O
true	O
for	O
arbitrarily	O
large	O
valencies	O
:	O
MURDER	O
(	O
Mustard	O
,	O
Boddy	O
,	O
kitchen	O
,	O
candlestick	O
)	O
entails	O
KILL	O
(	O
Mustard	O
,	O
Boddy	O
)	O
and	O
both	O
entail	O
DIE	O
(	O
Boddy	O
)	O
.	O
Though	O
arguments	O
may	O
be	O
dropped	O
from	O
the	O
premise	O
,	O
they	O
still	O
influence	O
entailments	O
.	O
This	O
is	O
because	O
the	O
MDIH	O
tracks	O
eventualities	O
.	O
"	O
Person	O
writing	O
a	O
book	O
"	O
is	O
a	O
different	O
kind	O
of	O
event	O
than	O
"	O
person	O
writing	O
software	O
"	O
with	O
a	O
different	O
distribution	O
of	O
argument	O
tuples	O
,	O
so	O
we	O
learn	O
that	O
the	O
former	O
entails	O
being	O
an	O
author	O
,	O
while	O
the	O
latter	O
entails	O
being	O
a	O
programmer	O
.	O

Local	O
learning	O
of	O
entailments	O
suffers	O
from	O
sparsity	O
issues	O
which	O
can	O
be	O
improved	O
by	O
further	O
learning	O
of	O
"	O
global	O
"	O
graphs	O
.	O
We	O
use	O
the	O
soft	O
constraint	O
method	O
of	O
Hosseini	O
et	O
al	O
(	O
2018	O
)	O
which	O
has	O
two	O
optimizations	O
.	O
The	O
paraphrase	O
resolution	O
constraint	O
encourages	O
predicates	O
within	O
the	O
same	O
typed	O
graphs	O
that	O
entail	O
each	O
other	O
to	O
have	O
similar	O
entailment	O
patterns	O
.	O
The	O
cross	O
-	O
graph	O
constraint	O
additionally	O
encourages	O
compatible	O
predicates	O
across	O
different	O
typed	O
graphs	O
to	O
share	O
entailment	O
patterns	O
.	O
We	O
apply	O
global	O
learning	O
to	O
bivalent	O
graphs	O
and	O
separately	O
to	O
univalent	O
graphs	O
.	O
Globalization	O
is	O
valency	O
-	O
agnostic	O
,	O
using	O
just	O
the	O
common	O
structures	O
between	O
predicates	O
,	O
so	O
bivalent	O
graphs	O
can	O
use	O
BB	O
and	O
BU	O
edges	O
to	O
optimize	O
binary	O
predicate	O
entailments	O
.	O
Final	O
graph	O
size	O
statistics	O
are	O
in	O
Table	O
1	O
.	O

Vertices	O
Edges	O
Bivalent	O
938	O
K	O
Binary	O
94	O
M	O
BB	O
/	O
30	O
M	O
BU	O
Univalent	O
36	O
K	O
Unary	O
3.6	O
M	O
UU	O

Since	O
the	O
annual	O
examination	O
papers	O
are	O
designed	O
by	O
a	O
team	O
of	O
healthcare	O
experts	O
who	O
try	O
to	O
follow	O
the	O
similar	O
reasoning	O
types	O
distribution	O
.	O
To	O
better	O
understand	O
our	O
dataset	O
,	O
we	O
manually	O
inspected	O
10	O
sets	O
of	O
examination	O
papers	O
(	O
2	O
sets	O
for	O
each	O
subfield	O
)	O
,	O
and	O
summarize	O
the	O
most	O
frequent	O
reasoning	O
types	O
of	O
the	O
questions	O
from	O
MLEC	O
-	O
QA	O
and	O
previous	O
works	O
(	O
Lai	O
et	O
al	O
,	O
2017	O
;	O
Zhong	O
et	O
al	O
,	O
2020	O
)	O
.	O
The	O
examples	O
are	O
shown	O
in	O
Table	O
6	O
.	O
Notably	O
,	O
the	O
"	O
Evidence	O
"	O
is	O
well	O
-	O
organized	O
by	O
us	O
to	O
show	O
how	O
models	O
need	O
to	O
handle	O
these	O
reasoning	O
issues	O
to	O
achieve	O
promising	O
performance	O
in	O
MLEC	O
-	O
QA	O
.	O
The	O
definition	O
of	O
reasoning	O
types	O
of	O
the	O
questions	O
are	O
as	O
follows	O
:	O
Lexical	O
Matching	O
This	O
type	O
of	O
question	O
is	O
common	O
and	O
the	O
simplest	O
.	O
The	O
retrieved	O
documents	O
are	O
highly	O
matched	O
with	O
the	O
question	O
,	O
the	O
correct	O
answer	O
exactly	O
matches	O
a	O
span	O
in	O
the	O
document	O
.	O
As	O
shown	O
in	O
the	O
example	O
,	O
the	O
model	O
only	O
needs	O
to	O
check	O
which	O
option	O
is	O
matched	O
with	O
.	O
Multi	O
-	O
Sentence	O
Reading	O
Unlike	O
lexical	O
matching	O
,	O
where	O
questions	O
and	O
correct	O
answers	O
can	O
be	O
found	O
within	O
a	O
single	O
sentence	O
,	O
multi	O
-	O
sentence	O
reading	O
requires	O
models	O
reading	O
multiple	O
sentences	O
to	O
gather	O
enough	O
information	O
to	O
generate	O
answers	O
.	O

The	O
correct	O
options	O
for	O
this	O
type	O
of	O
question	O
do	O
not	O
appear	O
directly	O
in	O
the	O
documents	O
.	O
It	O
requires	O
the	O
model	O
to	O
understand	O
and	O
summarize	O
the	O
question	O
relevant	O
concepts	O
after	O
reading	O
the	O
documents	O
.	O
As	O
shown	O
in	O
the	O
example	O
,	O
the	O
model	O
needs	O
to	O
understand	O
and	O
summarize	O
the	O
relevant	O
mechanism	O
of	O
"	O
Thermoregulation	O
"	O
,	O
and	O
infer	O
that	O
when	O
an	O
obstacle	O
arises	O
in	O
thermoregulation	O
,	O
the	O
body	O
temperature	O
will	O
not	O
be	O
able	O
to	O
maintain	O
a	O
relatively	O
constant	O
level	O
,	O
that	O
is	O
,	O
it	O
will	O
rise	O
with	O
the	O
increase	O
of	O
ambient	O
temperature	O
.	O
Numerical	O
Calculation	O
This	O
type	O
of	O
question	O
involves	O
logical	O
reasoning	O
and	O
arithmetic	O
operations	O
related	O
to	O
mathematics	O
.	O
As	O
shown	O
in	O
the	O
example	O
,	O
the	O
model	O
first	O
needs	O
to	O
judge	O
the	O
approximate	O
age	O
of	O
month	O
according	O
to	O
the	O
height	O
of	O
the	O
infant	O
,	O
and	O
then	O
reverse	O
calculate	O
the	O
age	O
of	O
months	O
according	O
to	O
the	O
height	O
formula	O
of	O
infants	O
7~12	O
months	O
old	O
to	O
obtain	O
the	O
age	O
in	O
months	O
:	O
(	O
68	O
-	O
65	O
)	O
/	O
1.5	O
+	O
6	O
=	O
8	O
.	O
Multi	O
-	O
Hop	O
Reasoning	O
This	O
type	O
of	O
question	O
requires	O
several	O
steps	O
of	O
logical	O
reasoning	O
over	O
mul	O
-	O
tiple	O
documents	O
to	O
answer	O
.	O
As	O
shown	O
in	O
the	O
example	O
,	O
the	O
patient	O
's	O
hemoglobin	O
(	O
HB	O
)	O
value	O
is	O
low	O
,	O
indicating	O
that	O
the	O
patient	O
has	O
anemia	O
,	O
and	O
the	O
supply	O
of	O
iron	O
should	O
be	O
increased	O
in	O
their	O
diet	O
.	O
The	O
model	O
needs	O
to	O
compare	O
the	O
iron	O
content	O
of	O
each	O
option	O
:	O
the	O
iron	O
content	O
of	O
C	O
,	O
D	O
and	O
E	O
is	O
low	O
and	O
that	O
of	O
A	O
,	O
B	O
is	O
high	O
,	O
but	O
B	O
is	O
not	O
easily	O
absorbed	O
,	O
so	O
the	O
best	O
answer	O
is	O
A.	O
Reasoning	O
Type	O
Example	O
(	O
*	O
represents	O
the	O
correct	O
answer	O
)	O

The	O
main	O
hallmark	O
of	O
peritonitis	O
is	O
:	O
A.	O
Significant	O
abdominal	O
distension	O
B.	O
Abdominal	O
mobility	O
dullness	O
C.	O
Bowel	O
sounds	O
were	O
reduced	O
or	O
absent	O
D.	O
Severe	O
abdominal	O
cramping	O
*	O
E.	O
Peritoneal	O
irritation	O
signs	O
Evidence	O
:	O
The	O
hallmark	O
signs	O
of	O
peritonitis	O
are	O
peritoneal	O
irritation	O
signs	O
,	O
i.e.	O
,	O
tenderness	O
,	O
muscle	O
tension	O
,	O
and	O
rebound	O
tenderness	O
.	O

Which	O
is	O
wrong	O
in	O
the	O
following	O
narrative	O
relating	O
to	O
the	O
appendix	O
:	O
The	O
purpose	O
of	O
thermoregulation	O
is	O
to	O
maintain	O
body	O
temperature	O
in	O
the	O
normal	O
range	O
.	O
In	O
hyperthermic	O
environments	O
,	O
the	O
thermoregulatory	O
center	O
is	O
dysfunctional	O
and	O
can	O
not	O
maintain	O
the	O
body	O
's	O
balance	O
of	O
heat	O
production	O
and	O
heat	O
dissipation	O
,	O
so	O
the	O
body	O
temperature	O
is	O
increased	O
by	O
the	O
influence	O
of	O
ambient	O
temperature	O
.	O
A.	O

A	O
normal	O
infant	O
,	O
weighing	O
7.5	O
kg	O
and	O
measuring	O
68	O
cm	O
in	O
length	O
.	O
Bregma	O
1.0	O
cm	O
,	O
head	O
circumference	O
44	O
cm	O
.	O
Teething	O
4	O
.	O
Can	O
sit	O
alone	O
and	O
can	O
pick	O
up	O
pellets	O
with	O
a	O
hallux	O
and	O
forefinger	O
.	O
The	O
most	O
likely	O
age	O
of	O
the	O
infant	O
is	O
:	O
*	O
A.	O
8	O
months	O
B.	O
24	O
months	O
C.	O
18	O
months	O
D.	O
12	O
months	O
E.	O
5	O
months	O
Evidence	O
:	O
A	O
normal	O
infant	O
measured	O
65	O
cm	O
at	O
6	O
months	O
and	O
75	O
cm	O
at	O
1	O
year	O
of	O
age	O
.	O
The	O
infant	O
's	O
7	O
to	O
12	O
month	O
length	O
is	O
calculated	O
as	O
:	O
length	O
=	O
65	O
+	O
(	O
months	O
of	O
age	O
-	O
6	O
)	O
x	O
1.5	O
.	O

6	O
-	O
month	O
-	O
old	O
female	O
infant	O
,	O
artificial	O
feeding	O
mainly	O
,	O
physical	O
examination	O
revealed	O
a	O
low	O
hemoglobin	O
(	O
HB	O
)	O
value	O
,	O
the	O
dietary	O
supplement	O
that	O
should	O
be	O
mainly	O
added	O
is	O
:	O
*	O
A.	O
Liver	O
paste	O
B.	O
Egg	O
yolk	O
paste	O
C.	O
Tomato	O
paste	O
D.	O
Rice	O
paste	O
E.	O
Apple	O
puree	O
Evidence	O
:	O
(	O
1	O
)	O
Low	O
HB	O
value	O
indicates	O
anemia	O
tendency	O
.	O
Iron	O
deficiency	O
anemia	O
is	O
the	O
most	O
important	O
and	O
common	O
type	O
of	O
anemia	O
in	O
China	O
.	O
(	O
2	O
)	O
Iron	O
supply	O
should	O
be	O
increased	O
in	O
diet	O
.	O
(	O
3	O
)	O
Liver	O
paste	O
is	O
rich	O
in	O
iron	O
.	O
(	O
4	O
)	O
The	O
iron	O
content	O
of	O
egg	O
yolk	O
paste	O
is	O
lower	O
than	O
that	O
of	O
liver	O
paste	O
,	O
and	O
it	O
is	O
not	O
easy	O
to	O
be	O
absorbed	O
.	O
(	O
5	O
)	O
The	O
iron	O
content	O
of	O
tomato	O
paste	O
,	O
rice	O
paste	O
and	O
apple	O
puree	O
is	O
lower	O
than	O
that	O
of	O
liver	O
paste	O
.	O

Given	O
that	O
we	O
use	O
the	O
Chinese	O
Wikipedia	O
database	O
as	O
our	O
information	O
sources	O
and	O
apply	O
a	O
two	O
-	O
stage	O
retriever	O
-	O
reader	O
framework	O
,	O
the	O
reason	O
for	O
such	O
poor	O
baseline	O
performance	O
could	O
come	O
from	O
both	O
our	O
information	O
sources	O
and	O
the	O
retriever	O
-	O
reader	O
framework	O
.	O

The	O
data	O
is	O
represented	O
in	O
simplified	O
Chinese	O
(	O
zh	O
-	O
Hans	O
-	O
CN	O
)	O
,	O
and	O
collect	O
from	O
the	O
2006	O
to	O
2020	O
NM	O
-	O
LEC	O
,	O
as	O
well	O
as	O
practice	O
exercises	O
from	O
the	O
Internet	O
.	O

The	O
experiments	O
involve	O
annotations	O
from	O
5	O
medical	O
experts	O
with	O
at	O
least	O
have	O
a	O
master	O
's	O
degree	O
and	O
have	O
passed	O
the	O
NMLEC	O
.	O
They	O
ranged	O
in	O
age	O
from	O
28	O
45	O
years	O
,	O
included	O
3	O
men	O
and	O
2	O
women	O
,	O
all	O
come	O
from	O
China	O
and	O
speak	O
Chinese	O
as	O
a	O
native	O
language	O
.	O

All	O
questions	O
in	O
MLEC	O
-	O
QA	O
are	O
collected	O
from	O
the	O
National	O
Medical	O
Licensing	O
Examination	O
in	O
China	O
(	O
NMLEC	O
)	O
,	O
which	O
are	O
carefully	O
designed	O
by	O
human	O
experts	O
to	O
evaluate	O
professional	O
knowledge	O
and	O
skills	O
for	O
those	O
who	O
want	O
to	O
be	O
medical	O
practitioners	O
in	O
China	O
.	O

We	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
insightful	O
comments	O
and	O
suggestions	O
.	O
This	O
work	O
is	O
supported	O
by	O
the	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
(	O
NSFC	O
No	O
.	O
61972187	O
)	O
.	O

The	O
data	O
structure	O
below	O
describe	O
the	O
JSON	O
file	O
representation	O
in	O
MLEC	O
-	O
QA	O
.	O

"	O
qid":The	O
question	O
ID	O
,	O
"	O
qtype	O
"	O
:	O
[	O
"	O
A1	O
"	O
,	O
"	O
B1	O
"	O
,	O
"	O
A2	O
"	O
,	O
"	O
A3	O
/	O
A4	O
"	O
]	O
,	O
"	O
qtext":Description	O
of	O
the	O
question	O
,	O
"	O
qimage":Image	O
or	O
table	O
path	O
(	O
if	O
any	O
)	O
,	O
"	O
options	O
"	O
:	O
{	O
"	O
A":Description	O
of	O
the	O
option	O
A	O
,	O
"	O
B":Description	O
of	O
the	O
option	O
B	O
,	O
"	O
C":Description	O
of	O
the	O
option	O
C	O
,	O
"	O
D":Description	O
of	O
the	O
option	O
D	O
,	O
"	O
E":Description	O
of	O
the	O
option	O
E	O
}	O
,	O
"	O
answer	O
"	O
:	O
[	O
"	O
A	O
"	O
,	O
"	O
B	O
"	O
,	O
"	O
C	O
"	O
,	O
"	O
D	O
"	O
,	O
"	O
E	O
"	O
]	O
}	O

The	O
BM25	O
algorithm	O
is	O
defined	O
as	O
:	O
where	O
q	O
i	O
is	O
the	O
i	O
th	O
query	O
term	O
of	O
a	O
query	O
Q	O
,	O
f	O
(	O
q	O
i	O
,	O
D	O
)	O
is	O
q	O
i	O
's	O
term	O
frequency	O
in	O
the	O
document	O
D	O
,	O
|	O
D	O
|	O
is	O
the	O
length	O
of	O
the	O
document	O
D	O
in	O
words	O
,	O
and	O
avgdl	O
is	O
the	O
average	O
document	O
length	O
in	O
the	O
text	O
collection	O
from	O
which	O
documents	O
are	O
drawn	O
.	O
b	O
determines	O
the	O
effects	O
of	O
the	O
length	O
of	O
the	O
document	O
on	O
the	O
average	O
length	O
.	O
k1	O
is	O
a	O
variable	O
which	O
helps	O
determine	O
term	O
frequency	O
saturation	O
characteristics	O
.	O
By	O
default	O
,	O
b	O
,	O
k1	O
has	O
a	O
value	O
of	O
0.75	O
,	O
1.2	O
in	O
Elasticsearch	O
,	O
respectively	O
.	O
IDF	O
(	O
q	O
i	O
)	O
is	O
the	O
Inverse	O
Document	O
Frequency	O
(	O
IDF	O
)	O
weight	O
of	O
the	O
query	O
term	O
q	O
i	O
.	O
It	O
is	O
usually	O
computed	O
as	O
:	O
where	O
N	O
is	O
the	O
total	O
number	O
of	O
documents	O
in	O
the	O
collection	O
,	O
and	O
n	O
(	O
q	O
i	O
)	O
is	O
the	O
number	O
of	O
documents	O
containing	O
q	O
i	O
.	O

Parsing	O
Graphs	O
with	O
Regular	O
Graph	O
Grammars	O

Recently	O
,	O
several	O
datasets	O
have	O
become	O
available	O
which	O
represent	O
natural	O
language	O
phenomena	O
as	O
graphs	O
.	O
Hyperedge	O
Replacement	O
Languages	O
(	O
HRL	O
)	O
have	O
been	O
the	O
focus	O
of	O
much	O
attention	O
as	O
a	O
formalism	O
to	O
represent	O
the	O
graphs	O
in	O
these	O
datasets	O
.	O
Chiang	O
et	O
al	O
(	O
2013	O
)	O
prove	O
that	O
HRL	O
graphs	O
can	O
be	O
parsed	O
in	O
polynomial	O
time	O
with	O
respect	O
to	O
the	O
size	O
of	O
the	O
input	O
graph	O
.	O
We	O
believe	O
that	O
HRL	O
are	O
more	O
expressive	O
than	O
is	O
necessary	O
to	O
represent	O
semantic	O
graphs	O
and	O
we	O
propose	O
the	O
use	O
of	O
Regular	O
Graph	O
Languages	O
(	O
RGL	O
;	O
Courcelle	O
1991	O
)	O
,	O
which	O
is	O
a	O
subfamily	O
of	O
HRL	O
,	O
as	O
a	O
possible	O
alternative	O
.	O
We	O
provide	O
a	O
topdown	O
parsing	O
algorithm	O
for	O
RGL	O
that	O
runs	O
in	O
time	O
linear	O
in	O
the	O
size	O
of	O
the	O
input	O
graph	O
.	O

Definition	O
3	O
.	O
A	O
hyperedge	O
replacement	O
grammar	O
G	O
=	O
(	O
N	O
G	O
,	O
T	O
G	O
,	O
P	O
G	O
,	O
S	O
G	O
)	O
consists	O
of	O
ranked	O
(	O
disjoint	O
)	O
alphabets	O
N	O
G	O
and	O
T	O
G	O
of	O
nonterminal	O
and	O
terminal	O
symbols	O
,	O
respectively	O
,	O
a	O
finite	O
set	O
P	O
G	O
of	O
productions	O
,	O
and	O
a	O
start	O
symbol	O
S	O
G	O
N	O
G	O
.	O
Every	O
production	O
in	O
P	O
G	O
is	O
of	O
the	O
form	O
X	O
G	O
where	O
G	O
is	O
a	O
hypergraph	O
over	O
N	O
G	O
∪	O
T	O
G	O
and	O
rank	O
(	O
G	O
)	O
=	O
rank	O
(	O
X	O
)	O
.	O
For	O
each	O
production	O
p	O
:	O
X	O
G	O
,	O
we	O
use	O
L	O
(	O
p	O
)	O
to	O
refer	O
to	O
X	O
(	O
the	O
left	O
-	O
hand	O
side	O
of	O
p	O
)	O
and	O
R	O
(	O
p	O
)	O
to	O
refer	O
to	O
G	O
(	O
the	O
right	O
-	O
hand	O
side	O
of	O
p	O
)	O
.	O
An	O
edge	O
is	O
a	O
terminal	O
edge	O
if	O
its	O
label	O
is	O
terminal	O
and	O
a	O
nonterminal	O
edge	O
if	O
its	O
label	O
is	O
nonterminal	O
.	O
A	O
graph	O
is	O
a	O
terminal	O
graph	O
if	O
all	O
of	O
its	O
edges	O
are	O
terminal	O
.	O
The	O
terminal	O
subgraph	O
of	O
a	O
graph	O
is	O
the	O
subgraph	O
consisting	O
of	O
all	O
terminal	O
edges	O
and	O
their	O
incident	O
nodes	O
.	O
Given	O
a	O
HRG	O
G	O
,	O
we	O
say	O
that	O
graph	O
G	O
immediately	O
derives	O
graph	O
G	O
,	O
denoted	O
G	O
G	O
,	O
iff	O
there	O
is	O
an	O
edge	O
e	O
E	O
G	O
and	O
a	O
nonterminal	O
X	O
N	O
G	O
such	O
that	O
lab	O
G	O
(	O
e	O
)	O
=	O
X	O
and	O
G	O
=	O
G	O
[	O
e	O
/	O
H	O
]	O
,	O
where	O
X	O
H	O
is	O
in	O
P	O
G	O
.	O
We	O
extend	O
the	O
idea	O
of	O
immediate	O
derivation	O
to	O
its	O
transitive	O
closure	O
G	O
*	O
G	O
,	O
and	O
say	O
here	O
that	O
G	O
derives	O
G	O
.	O
For	O
every	O
X	O
N	O
G	O
we	O
also	O
use	O
X	O
to	O
de	O
-	O
X	O
a	O
b	O
(	O
1	O
)	O
(	O
2	O
)	O
G	O
(	O
2	O
)	O
c	O
(	O
1	O
)	O
a	O
(	O
3	O
)	O
d	O
H	O
(	O
1	O
)	O
c	O
d	O
a	O
(	O
2	O
)	O
b	O
a	O
G	O
[	O
e	O
/	O
H	O
]	O

(	O
1	O
)	O
1	O
go	O
1	O
2	O
I	O
arg0	O
Y	O
Z	O
s	O
:	O
(	O
1	O
)	O
(	O
2	O
)	O
1	O
2	O
1	O
arg0	O
arg1	O
X	O
q	O
:	O
W	O
Y	O
(	O
2	O
)	O
(	O
1	O
)	O
1	O
2	O
1	O
1	O
2	O
arg1	O
arg0	O
W	O
t	O
:	O
(	O
1	O
)	O
1	O
want	O
Y	O
r	O
:	O
Z	O
X	O
(	O
2	O
)	O
(	O
1	O
)	O
1	O
2	O
1	O
1	O
2	O
arg1	O
arg0	O
Z	O
u	O
:	O
(	O
1	O
)	O
1	O
need	O
Table	O
1	O
:	O
Productions	O
of	O
a	O
HRG	O
.	O
The	O
labels	O
p	O
,	O
q	O
,	O
r	O
,	O
s	O
,	O
t	O
,	O
and	O
u	O
label	O
the	O
productions	O
so	O
that	O
we	O
can	O
refer	O
to	O
them	O
in	O
the	O
text	O
.	O
Note	O
that	O
Y	O
can	O
rewrite	O
in	O
two	O
ways	O
,	O
either	O
via	O
production	O
r	O
or	O
s.	O
note	O
the	O
graph	O
consisting	O
of	O
a	O
single	O
edge	O
e	O
with	O
lab	O
(	O
e	O
)	O
=	O
X	O
and	O
nodes	O
(	O
v	O
1	O
,	O
.	O
.	O
.	O
,	O
v	O
rank	O
(	O
X	O
)	O
)	O
such	O
that	O
att	O
G	O
(	O
e	O
)	O
=	O
(	O
v	O
1	O
,	O
.	O
.	O
.	O
,	O
v	O
rank	O
(	O
X	O
)	O
)	O
,	O
and	O
we	O
define	O
the	O
language	O
L	O
X	O
(	O
G	O
)	O
as	O
{	O
G	O
|	O
X	O
*	O
G	O
G	O
is	O
terminal	O
}	O
.	O
The	O
language	O
of	O
G	O
is	O
L	O
(	O
G	O
)	O
=	O
L	O
S	O
G	O
(	O
G	O
)	O
.	O
We	O
call	O
the	O
family	O
of	O
languages	O
that	O
can	O
be	O
produced	O
by	O
any	O
HRG	O
the	O
hyperedge	O
replacement	O
languages	O
(	O
HRL	O
)	O
.	O
We	O
assume	O
that	O
terminal	O
edges	O
are	O
always	O
of	O
rank	O
2	O
,	O
and	O
depict	O
them	O
as	O
directed	O
edges	O
where	O
the	O
direction	O
is	O
determined	O
by	O
the	O
tentacle	O
labels	O
:	O
the	O
tentacle	O
labeled	O
1	O
attaches	O
to	O
the	O
source	O
of	O
the	O
edge	O
and	O
the	O
tentacle	O
labeled	O
2	O
attaches	O
to	O
the	O
target	O
of	O
the	O
edge	O
.	O
Example	O
3	O
.	O
Table	O
1	O
shows	O
a	O
HRG	O
deriving	O
AMR	O
graphs	O
for	O
sentences	O
of	O
the	O
form	O
'	O
I	O
need	O
to	O
want	O
to	O
need	O
to	O
want	O
to	O
...	O
to	O
want	O
to	O
go	O
'	O
.	O
Figure	O
3	O
is	O
a	O
graph	O
derived	O
by	O
the	O
grammar	O
.	O
The	O
grammar	O
is	O
somewhat	O
unnatural	O
,	O
a	O
point	O
we	O
will	O
return	O
to	O
(	O
4	O
)	O
.	O
We	O
can	O
use	O
HRGs	O
to	O
generate	O
chain	O
graphs	O
(	O
strings	O
)	O
by	O
restricting	O
the	O
form	O
of	O
the	O
productions	O
in	O
the	O
grammars	O
.	O
Figure	O
4	O
shows	O
a	O
HRG	O
that	O
produces	O
the	O
context	O
-	O
free	O
string	O
language	O
a	O
n	O
b	O
n	O
.	O
HRGs	O
can	O
simulate	O
the	O
class	O
of	O
mildly	O
context	O
-	O
sensitive	O
languages	O
that	O
is	O
characterized	O
,	O
e.g.	O
,	O
by	O
linear	O
context	O
-	O
free	O
rewriting	O
systems	O
(	O
LCFRS	O
;	O
Vijay	O
-	O
Shanker	O
et	O
al	O
1987	O
)	O
,	O
where	O
the	O
fan	O
-	O
out	O
of	O
the	O
LCFRS	O
will	O
influence	O
the	O
maximum	O
rank	O
of	O
nonterminal	O
required	O
in	O
the	O
HRG	O
,	O
see	O
(	O
Engelfriet	O
and	O
Heyker	O
,	O
1991	O
)	O
.	O

(	O
1	O
)	O
Y	O
Z	O
We	O
call	O
the	O
family	O
of	O
languages	O
generated	O
by	O
RGGs	O
the	O
regular	O
graph	O
languages	O
(	O
RGLs	O
)	O
.	O

In	O
this	O
section	O
we	O
report	O
details	O
on	O
the	O
database	O
,	O
training	O
parameters	O
and	O
results	O
.	O

The	O
system	O
was	O
implemented	O
using	O
OpenNMT	O
in	O
PyTorch	O
(	O
Klein	O
et	O
al	O
,	O
2017	O
)	O

Utilizando	O
la	O
base	O
de	O
datos	O
Epistemonikos	O
,	O
la	O
cual	O
es	O
mantenida	O
mediante	O
bsquedas	O
realizadas	O
en	O
30	O
bases	O
de	O
datos	O
,	O
identificamos	O
seis	O
revisiones	O
sistemticas	O
que	O
en	O
conjunto	O
incluyen	O
36	O
estudios	O
aleatorizados	O
pertinentes	O
a	O
la	O
pregunta	O
.	O

Using	O
the	O
Epistemonikos	O
database	O
,	O
which	O
is	O
maintained	O
through	O
searches	O
in	O
30	O
databases	O
,	O
we	O
identified	O
six	O
systematic	O
reviews	O
including	O
36	O
randomized	O
studies	O
relevant	O
to	O
the	O
question	O
.	O

Using	O
the	O
Epistemonikos	O
database	O
,	O
which	O
is	O
maintained	O
through	O
searches	O
in	O
30	O
databases	O
,	O
we	O
identified	O
six	O
systematic	O
reviews	O
that	O
altogether	O
include	O
36	O
randomized	O
studies	O
relevant	O
to	O
the	O
question	O
.	O

Os	O
resultados	O
dos	O
modelos	O
de	O
regresso	O
mostraram	O
associao	O
entre	O
os	O
fatores	O
de	O
correo	O
estimados	O
e	O
os	O
indicadores	O
de	O
adequao	O
propostos	O

Regression	O
models	O
showed	O
an	O
association	O
between	O
estimated	O
correction	O
factors	O
and	O
the	O
proposed	O
adequacy	O
indicators	O
.	O

Authors	O
would	O
like	O
to	O
thank	O
Noe	O
Casas	O
for	O
his	O
valuable	O
comments	O
.	O
This	O
work	O
is	O
supported	O
in	O

We	O
start	O
with	O
the	O
problem	O
definition	O
.	O
Let	O
G	O
=	O
(	O
E	O
,	O
R	O
)	O
be	O
an	O
existing	O
KG	O
where	O
E	O
and	O
R	O
are	O
the	O
sets	O
of	O
entities	O
and	O
relationships	O
(	O
predicates	O
)	O
in	O
G	O
,	O
respectively	O
.	O
We	O
consider	O
a	O
sentence	O
S	O
=	O
w	O
1	O
,	O
w	O
2	O
,	O
...	O
,	O
w	O
i	O
as	O
the	O
input	O
,	O
where	O
w	O
i	O
is	O
a	O
token	O
at	O
position	O
i	O
in	O
the	O
sentence	O
.	O
We	O
aim	O
to	O
extract	O
a	O
set	O
of	O
triples	O
O	O
=	O
{	O
o	O
1	O
,	O
o	O
2	O
,	O
...	O
,	O
o	O
j	O
}	O
from	O
the	O
sentence	O
,	O
where	O
o	O
j	O
=	O
h	O
j	O
,	O
r	O
j	O
,	O
t	O
j	O
,	O
h	O
j	O
,	O
t	O
j	O
E	O
,	O
and	O
r	O
j	O
R.	O
Table	O
1	O
illustrates	O
the	O
input	O
and	O
target	O
output	O
of	O
our	O
problem	O
.	O

In	O
the	O
training	O
phase	O
,	O
in	O
addition	O
to	O
the	O
sentencetriple	O
pairs	O
collected	O
using	O
distant	O
supervision	O
(	O
see	O
Section	O
3.2	O
)	O
,	O
we	O
also	O
add	O
pairs	O
of	O
Entity	O
-	O
name	O
,	O
Entity	O
-	O
ID	O
of	O
all	O
entities	O
in	O
the	O
KB	O
to	O
the	O
training	O
data	O
,	O
e.g.	O
,	O
New	O
York	O
University	O
,	O
Q49210	O
.	O
This	O
allows	O
the	O
model	O
to	O
learn	O
the	O
mapping	O
between	O
entity	O
names	O
and	O
entity	O
IDs	O
,	O
especially	O
for	O
the	O
unseen	O
entities	O
.	O

We	O
further	O
perform	O
manual	O
error	O
analysis	O
.	O
We	O
found	O
that	O
the	O
incorrect	O
output	O
of	O
our	O
model	O
is	O
caused	O
by	O
the	O
same	O
entity	O
name	O
of	O
two	O
different	O
entities	O
(	O
e.g.	O
,	O
the	O
name	O
of	O
Michael	O
Jordan	O
that	O
refers	O
to	O
the	O
American	O
basketball	O
player	O
or	O
the	O
English	O
footballer	O
)	O
.	O
The	O
modified	O
beam	O
search	O
can	O
not	O
disambiguate	O
those	O
entities	O
as	O
it	O
only	O
considers	O
the	O
lexical	O
similarity	O
.	O
We	O
consider	O
using	O
context	O
-	O
based	O
similarity	O
as	O
future	O
work	O
.	O

We	O
begin	O
by	O
surveying	O
existing	O
KGC	O
benchmarks	O
.	O
Table	O
8	O
in	O
Appendix	O
A	O
provides	O
an	O
overview	O
of	O
evaluation	O
datasets	O
and	O
tasks	O
on	O
a	O
per	O
-	O
paper	O
basis	O
across	O
the	O
artificial	O
intelligence	O
,	O
machine	O
learning	O
,	O
and	O
natural	O
language	O
processing	O
communities	O
.	O
Note	O
that	O
we	O
focus	O
on	O
data	O
rather	O
than	O
models	O
,	O
so	O
we	O
only	O
overview	O
relevant	O
evaluation	O
benchmarks	O
here	O
.	O
For	O
more	O
on	O
existing	O
KGC	O
models	O
,	O
both	O
neural	O
and	O
symbolic	O
,	O
we	O
refer	O
the	O
reader	O
to	O
(	O
Meilicke	O
et	O
al	O
,	O
2018	O
)	O
and	O
(	O
Ji	O
et	O
al	O
,	O
2020	O
)	O
.	O

These	O
datasets	O
,	O
extracted	O
from	O
the	O
Freebase	O
knowledge	O
graph	O
(	O
Bollacker	O
et	O
al	O
,	O
2008	O
)	O
,	O
are	O
the	O
most	O
popular	O
for	O
KGC	O
(	O
see	O
Table	O
8	O
in	O
Appendix	O
A	O
)	O
.	O
Bordes	O
et	O
al	O
(	O
2013	O
)	O
.	O
It	O
contains	O
14	O
,	O
951	O
entities	O
,	O
1	O
,	O
345	O
relations	O
,	O
and	O
592	O
,	O
213	O
triples	O
covering	O
several	O
domains	O
,	O
with	O
a	O
strong	O
focus	O
on	O
awards	O
,	O
entertainment	O
,	O
and	O
sports	O
.	O

FB15	O
K	O
-	O
237	O
was	O
introduced	O
by	O
to	O
remedy	O
data	O
leakage	O
in	O
FB15	O
K	O
,	O
which	O
contains	O
many	O
test	O
triples	O
that	O
invert	O
triples	O
in	O
the	O
training	O
set	O
.	O
FB15	O
K	O
-	O
237	O
contains	O
14	O
,	O
541	O
entities	O
,	O
237	O
relations	O
,	O
and	O
310	O
,	O
116	O
triples	O
.	O
We	O
compare	O
FB15	O
K	O
-	O
237	O
to	O
CODEX	O
in	O
6	O
to	O
assess	O
each	O
dataset	O
's	O
content	O
and	O
relative	O
difficulty	O
.	O

In	O
this	O
section	O
we	O
describe	O
the	O
pipeline	O
used	O
to	O
construct	O
CODEX	O
.	O
For	O
reference	O
,	O
we	O
define	O
a	O
knowledge	O
graph	O
G	O
as	O
a	O
multi	O
-	O
relational	O
graph	O
consisting	O
of	O
a	O
set	O
of	O
entities	O
E	O
,	O
relations	O
R	O
,	O
and	O
factual	O
statements	O
in	O
the	O
form	O
of	O
(	O
head	O
,	O
relation	O
,	O
tail	O
)	O
triples	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
E	O
×	O
R	O
×	O
E.	O

An	O
advantage	O
of	O
Wikidata	O
is	O
that	O
it	O
links	O
entities	O
and	O
relations	O
to	O
various	O
sources	O
of	O
rich	O
auxiliary	O
information	O
.	O
To	O
enable	O
tasks	O
that	O
involve	O
joint	O
learning	O
over	O
knowledge	O
graph	O
structure	O
and	O
such	O
additional	O
information	O
,	O
we	O
collected	O
:	O
Entity	O
types	O
for	O
each	O
entity	O
as	O
given	O
by	O
Wikidata	O
's	O
instance	O
of	O
and	O
subclass	O
of	O
relations	O
;	O
Wikidata	O
labels	O
and	O
descriptions	O
for	O
entities	O
,	O
relations	O
,	O
and	O
entity	O
types	O
;	O
and	O
Wikipedia	O
page	O
extracts	O
(	O
introduction	O
sections	O
)	O
for	O
entities	O
and	O
entity	O
types	O
.	O
For	O
the	O
latter	O
two	O
,	O
we	O
collected	O
text	O
where	O
available	O
in	O
Arabic	O
,	O
German	O
,	O
English	O
,	O
Spanish	O
,	O
Russian	O
,	O
and	O
Chinese	O
.	O
We	O
chose	O
these	O
languages	O
because	O
they	O
are	O
all	O
relatively	O
well	O
-	O
represented	O
on	O
Wikidata	O
(	O
Kaffee	O
et	O
al	O
,	O
2017	O
)	O
.	O
Table	O
2	O
provides	O
the	O
coverage	O
by	O
language	O
for	O
each	O
CODEX	O
dataset	O
.	O

We	O
manually	O
labeled	O
all	O
candidate	O
negative	O
triples	O
generated	O
for	O
CODEX	O
-	O
S	O
and	O
CODEX	O
-	O
M	O
as	O
true	O
or	O
false	O
using	O
the	O
guidelines	O
provided	O
in	O
Appendix	O
C.	O
3	O
We	O
randomly	O
selected	O
among	O
the	O
triples	O
labeled	O
as	O
false	O
to	O
create	O
validation	O
and	O
test	O
negatives	O
for	O
CODEX	O
-	O
S	O
and	O
CODEX	O
-	O
M	O
,	O
examples	O
of	O
which	O
are	O
given	O
in	O
Ta	O
-	O
3	O
We	O
are	O
currently	O
investigating	O
methods	O
for	O
obtaining	O
highquality	O
crowdsourced	O
annotations	O
of	O
negatives	O
for	O
CODEX	O
-	O
L.	O
ble	O
3	O
.	O
To	O
assess	O
the	O
quality	O
of	O
our	O
annotations	O
,	O
we	O
gathered	O
judgments	O
from	O
two	O
independent	O
native	O
English	O
speakers	O
on	O
a	O
random	O
selection	O
of	O
100	O
candidate	O
negatives	O
.	O
The	O
annotators	O
were	O
provided	O
the	O
instructions	O
from	O
Appendix	O
C.	O
On	O
average	O
,	O
our	O
labels	O
agreed	O
with	O
those	O
of	O
the	O
annotators	O
89.5	O
%	O
of	O
the	O
time	O
.	O
Among	O
the	O
disagreements	O
,	O
81	O
%	O
of	O
the	O
time	O
we	O
assigned	O
the	O
label	O
true	O
whereas	O
the	O
annotator	O
assigned	O
the	O
label	O
false	O
,	O
meaning	O
that	O
we	O
were	O
comparatively	O
conservative	O
in	O
labeling	O
negatives	O
.	O

Symmetric	O
relations	O
are	O
relations	O
r	O
for	O
which	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
G	O
implies	O
(	O
t	O
,	O
r	O
,	O
h	O
)	O
G.	O
For	O
each	O
relation	O
,	O
we	O
compute	O
the	O
number	O
of	O
its	O
(	O
head	O
,	O
tail	O
)	O
pairs	O
that	O
overlap	O
with	O
its	O
(	O
tail	O
,	O
head	O
)	O
pairs	O
,	O
divided	O
by	O
the	O
total	O
number	O
of	O
pairs	O
,	O
and	O
take	O
those	O
with	O
50	O
%	O
overlap	O
or	O
higher	O
as	O
symmetric	O
.	O
CODEX	O
datasets	O
have	O
five	O
such	O
relations	O
:	O
diplomatic	O
relation	O
,	O
shares	O
border	O
with	O
,	O
sibling	O
,	O
spouse	O
,	O
and	O
unmarried	O
partner	O
.	O
Table	O
4	O
gives	O
the	O
proportion	O
of	O
triples	O
containing	O
symmetric	O
relations	O
per	O
dataset	O
.	O
Symmetric	O
patterns	O
are	O
more	O
prevalent	O
in	O
CODEX	O
-	O
S	O
,	O
whereas	O
the	O
larger	O
datasets	O
are	O
mostly	O
antisymmetric	O
,	O
i.e.	O
,	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
G	O
implies	O
(	O
t	O
,	O
r	O
,	O
h	O
)	O
G.	O

Finally	O
,	O
we	O
conduct	O
a	O
comparative	O
analysis	O
between	O
CODEX	O
-	O
M	O
and	O
FB15	O
K	O
-	O
237	O
(	O
2.1	O
)	O
to	O
demonstrate	O
the	O
unique	O
value	O
of	O
CODEX	O
.	O
We	O
choose	O
FB15	O
K	O
-	O
237	O
because	O
it	O
is	O
the	O
most	O
popular	O
encyclopedic	O
KGC	O
benchmark	O
after	O
FB15	O
K	O
,	O
which	O
was	O
already	O
shown	O
to	O
be	O
an	O
easy	O
dataset	O
by	O
.	O
We	O
choose	O
CODEX	O
-	O
M	O
because	O
it	O
is	O
the	O
closest	O
in	O
size	O
to	O
FB15	O
K	O
-	O
237	O
.	O

Table	O
9	O
provides	O
all	O
seed	O
entity	O
and	O
relation	O
types	O
used	O
to	O
collect	O
CODEX	O
.	O
Each	O
type	O
is	O
given	O
first	O
by	O
its	O
natural	O
language	O
label	O
and	O
then	O
by	O
its	O
Wikidata	O
unique	O
ID	O
:	O
Entity	O
IDs	O
begin	O
with	O
Q	O
,	O
whereas	O
relation	O
(	O
property	O
)	O
IDs	O
begin	O
with	O
P.	O
For	O
the	O
entity	O
types	O
that	O
apply	O
to	O
people	O
(	O
e.g.	O
,	O
actor	O
,	O
musician	O
,	O
journalist	O
)	O
,	O
we	O
retrieved	O
seed	O
entities	O
by	O
querying	O
Wikidata	O
using	O
the	O
occupation	O
relation	O
.	O
For	O
the	O
entity	O
types	O
that	O
apply	O
to	O
things	O
(	O
e.g.	O
,	O
airline	O
,	O
disease	O
,	O
tourist	O
attraction	O
)	O
,	O
we	O
retrieved	O
seed	O
entities	O
by	O
querying	O
Wikidata	O
using	O
the	O
instance	O
of	O
and	O
subclass	O
of	O
relations	O
.	O

We	O
provide	O
additional	O
comparison	O
of	O
the	O
contents	O
in	O
CODEX	O
-	O
M	O
and	O
FB15	O
K	O
-	O
237	O
.	O
Figure	O
5	O
,	O
which	O
plots	O
the	O
top	O
-	O
30	O
entities	O
by	O
frequency	O
in	O
the	O
two	O
benchmarks	O
,	O
demonstrates	O
that	O
both	O
dataset	O
are	O
biased	O
toward	O
developed	O
Western	O
countries	O
and	O
cultures	O
.	O
However	O
,	O
CODEX	O
-	O
M	O
is	O
more	O
diverse	O
in	O
domain	O
.	O
It	O
covers	O
academia	O
,	O
entertainment	O
,	O
journalism	O
,	O
politics	O
,	O
science	O
,	O
and	O
writing	O
,	O
whereas	O
FB15	O
K	O
-	O
237	O
covers	O
mostly	O
entertaiment	O
and	O
sports	O
.	O
FB15	O
K	O
-	O
237	O
is	O
also	O
much	O
more	O
biased	O
toward	O
the	O
United	O
States	O
in	O
particular	O
,	O
as	O
five	O
of	O
its	O
top	O
-	O
30	O
entities	O
are	O
specific	O
to	O
the	O
US	O
:	O
United	O
States	O
of	O
America	O
,	O
United	O
States	O
dollar	O
,	O
New	O
York	O
City	O
,	O
Figure	O
6	O
compares	O
the	O
top	O
-	O
15	O
entity	O
types	O
in	O
CODEX	O
-	O
M	O
and	O
FB15	O
K	O
-	O
237	O
.	O
Again	O
,	O
CODEX	O
-	O
M	O
is	O
diverse	O
,	O
covering	O
people	O
,	O
places	O
,	O
organizations	O
,	O
movies	O
,	O
and	O
abstract	O
concepts	O
,	O
whereas	O
FB15	O
K	O
-	O
237	O
has	O
many	O
overlapping	O
entity	O
types	O
mostly	O
about	O
entertainment	O
.	O

The	O
authors	O
thank	O
Michał	O
Rybak	O
and	O
Xinyi	O
(	O
Carol	O
)	O
Zheng	O
for	O
their	O
contributions	O
.	O
This	O
material	O
is	O
supported	O
by	O
the	O
National	O
Science	O
Foundation	O
under	O
Grant	O
No	O
.	O
IIS	O
1845491	O
,	O
Army	O
Young	O
Investigator	O
Award	O
No	O
.	O
W911NF1810397	O
,	O
and	O
an	O
NSF	O
Graduate	O
Research	O
Fellowship	O
.	O

Table	O
10	O
:	O
Our	O
hyperparameter	O
search	O
space	O
.	O
We	O
follow	O
the	O
naming	O
conventions	O
and	O
ranges	O
given	O
by	O
Ruffinelli	O
et	O
al	O
(	O
2020	O
)	O
,	O
and	O
explain	O
the	O
meanings	O
of	O
selected	O
hyperparameter	O
settings	O
in	O
Appendix	O
F.	O
As	O
most	O
KGC	O
embedding	O
models	O
have	O
a	O
wide	O
range	O
of	O
configuration	O
options	O
,	O
we	O
encourage	O
future	O
work	O
to	O
follow	O
this	O
tabular	O
scheme	O
for	O
transparent	O
reporting	O
of	O
implementation	O
details	O
.	O
1.0	O
-	O
See	O
Tab	O
.	O
12	O
-	O
-	O
See	O
Tab	O
.	O
12	O
Interval	O
(	O
Unif	O
)	O
-	O
See	O
Tab	O
.	O
12	O
−0.8133	O
-	O
See	O
Tab	O
.	O
12	O
Gain	O
(	O
XvNorm	O
)	O
1.0	O
See	O
Tab	O
.	O
12	O
-	O
-	O
See	O
Tab	O
.	O
12	O
Gain	O
(	O
XvUnif	O
)	O
-	O
See	O
Tab	O
.	O
12	O
-	O
1.0	O
See	O
Tab	O
.	O
12	O

Scalable	O
Construction	O
and	O
Reasoning	O
of	O
Massive	O
Knowledge	O
Bases	O
-	O
Proposal	O
for	O
a	O
Tutorial	O
at	O
NAACL	O
2018	O

In	O
today	O
's	O
information	O
-	O
based	O
society	O
,	O
there	O
is	O
abundant	O
knowledge	O
out	O
there	O
carried	O
in	O
the	O
form	O
of	O
natural	O
language	O
texts	O
(	O
e.g.	O
,	O
news	O
articles	O
,	O
social	O
media	O
posts	O
,	O
scientific	O
publications	O
)	O
,	O
which	O
spans	O
across	O
various	O
domains	O
(	O
e.g.	O
,	O
corporate	O
documents	O
,	O
advertisements	O
,	O
legal	O
acts	O
,	O
medical	O
reports	O
)	O
,	O
and	O
grows	O
at	O
an	O
astonishing	O
rate	O
.	O
How	O
to	O
turn	O
such	O
massive	O
and	O
unstructured	O
text	O
data	O
into	O
structured	O
,	O
actionable	O
knowledge	O
for	O
computational	O
machines	O
,	O
and	O
furthermore	O
,	O
how	O
to	O
teach	O
machines	O
learn	O
to	O
reason	O
and	O
complete	O
the	O
extracted	O
knowledge	O
is	O
a	O
grand	O
challenge	O
to	O
the	O
research	O
community	O
.	O

This	O
tutorial	O
presents	O
a	O
comprehensive	O
overview	O
of	O
techniques	O
for	O
automatic	O
knowledge	O
base	O
construction	O
from	O
text	O
data	O
(	O
especially	O
from	O
a	O
large	O
,	O
domain	O
-	O
specific	O
text	O
corpora	O
)	O
,	O
and	O
techniques	O
for	O
reasoning	O
over	O
large	O
-	O
scale	O
knowledge	O
bases	O
.	O
We	O
will	O
discuss	O
the	O
following	O
key	O
issues	O
:	O

